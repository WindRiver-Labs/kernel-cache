From edf554cb7e554fa24ca8ce32248e460a326a2439 Mon Sep 17 00:00:00 2001
From: Paul Gortmaker <paul.gortmaker@windriver.com>
Date: Mon, 2 Nov 2015 13:39:14 -0500
Subject: [PATCH] Revert "sched/preempt: Fix cond_resched_lock() and
 cond_resched_softirq()"

This reverts commit e211cb68dd3c951b104ff0b47dbaed2c8b8d2399.

The cherry pick of fe32d3cd5e8eb0f82e459763374aa80797023403 causes fallout
in the -rt patches, esp. in the PREEMPT_LAZY area.  Defer integration of
this commit until stable-rt has moved ahead to 3.14.56 itself so we can
then just re-export the impacted patches from the rt-rebase stream.

It has been this way from 3.13 to 4.3 w/o any significant fallout, so 
it really is not a big concern.

Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/x86/include/asm/preempt.h b/arch/x86/include/asm/preempt.h
index 999b4a3e65f5..b39e194f6c8d 100644
--- a/arch/x86/include/asm/preempt.h
+++ b/arch/x86/include/asm/preempt.h
@@ -105,9 +105,9 @@ static __always_inline bool __preempt_count_dec_and_test(void)
 /*
  * Returns true when we need to resched and can (barring IRQ state).
  */
-static __always_inline bool should_resched(int preempt_offset)
+static __always_inline bool should_resched(void)
 {
-	return unlikely(__this_cpu_read_4(__preempt_count) == preempt_offset);
+	return unlikely(!__this_cpu_read_4(__preempt_count));
 }
 
 #ifdef CONFIG_PREEMPT
diff --git a/include/asm-generic/preempt.h b/include/asm-generic/preempt.h
index 54352f4dde1a..1cd3f5d767a8 100644
--- a/include/asm-generic/preempt.h
+++ b/include/asm-generic/preempt.h
@@ -74,10 +74,9 @@ static __always_inline bool __preempt_count_dec_and_test(void)
 /*
  * Returns true when we need to resched and can (barring IRQ state).
  */
-static __always_inline bool should_resched(int preempt_offset)
+static __always_inline bool should_resched(void)
 {
-	return unlikely(preempt_count() == preempt_offset &&
-			tif_need_resched());
+	return unlikely(!preempt_count() && tif_need_resched());
 }
 
 #ifdef CONFIG_PREEMPT
diff --git a/include/linux/preempt.h b/include/linux/preempt.h
index 411a5c6371da..1841b58cf173 100644
--- a/include/linux/preempt.h
+++ b/include/linux/preempt.h
@@ -22,8 +22,7 @@
 #if defined(CONFIG_DEBUG_PREEMPT) || defined(CONFIG_PREEMPT_TRACER)
 extern void preempt_count_add(int val);
 extern void preempt_count_sub(int val);
-#define preempt_count_dec_and_test() \
-	({ preempt_count_sub(1); should_resched(0); })
+#define preempt_count_dec_and_test() ({ preempt_count_sub(1); should_resched(); })
 #else
 #define preempt_count_add(val)	__preempt_count_add(val)
 #define preempt_count_sub(val)	__preempt_count_sub(val)
@@ -62,7 +61,7 @@ do { \
 
 #define preempt_check_resched() \
 do { \
-	if (should_resched(0)) \
+	if (should_resched()) \
 		__preempt_schedule(); \
 } while (0)
 
diff --git a/include/linux/preempt_mask.h b/include/linux/preempt_mask.h
index 5cb25f17331a..1f654ee836b7 100644
--- a/include/linux/preempt_mask.h
+++ b/include/linux/preempt_mask.h
@@ -71,21 +71,13 @@
  */
 #define in_nmi()	(preempt_count() & NMI_MASK)
 
-/*
- * The preempt_count offset after preempt_disable();
- */
 #if defined(CONFIG_PREEMPT_COUNT)
-# define PREEMPT_DISABLE_OFFSET	PREEMPT_OFFSET
+# define PREEMPT_DISABLE_OFFSET 1
 #else
-# define PREEMPT_DISABLE_OFFSET	0
+# define PREEMPT_DISABLE_OFFSET 0
 #endif
 
 /*
- * The preempt_count offset after spin_lock()
- */
-#define PREEMPT_LOCK_OFFSET	PREEMPT_DISABLE_OFFSET
-
-/*
  * The preempt_count offset needed for things like:
  *
  *  spin_lock_bh()
@@ -98,7 +90,7 @@
  *
  * Work as expected.
  */
-#define SOFTIRQ_LOCK_OFFSET (SOFTIRQ_DISABLE_OFFSET + PREEMPT_LOCK_OFFSET)
+#define SOFTIRQ_LOCK_OFFSET (SOFTIRQ_DISABLE_OFFSET + PREEMPT_DISABLE_OFFSET)
 
 /*
  * Are we running in atomic context?  WARNING: this macro cannot
diff --git a/include/linux/sched.h b/include/linux/sched.h
index ec6000f66e75..91fe6a38b307 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2647,6 +2647,12 @@ extern int _cond_resched(void);
 
 extern int __cond_resched_lock(spinlock_t *lock);
 
+#ifdef CONFIG_PREEMPT_COUNT
+#define PREEMPT_LOCK_OFFSET	PREEMPT_OFFSET
+#else
+#define PREEMPT_LOCK_OFFSET	0
+#endif
+
 #define cond_resched_lock(lock) ({				\
 	__might_sleep(__FILE__, __LINE__, PREEMPT_LOCK_OFFSET);	\
 	__cond_resched_lock(lock);				\
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index bbe957762ace..a19262a7d70b 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -4113,7 +4113,7 @@ static void __cond_resched(void)
 
 int __sched _cond_resched(void)
 {
-	if (should_resched(0)) {
+	if (should_resched()) {
 		__cond_resched();
 		return 1;
 	}
@@ -4131,7 +4131,7 @@ EXPORT_SYMBOL(_cond_resched);
  */
 int __cond_resched_lock(spinlock_t *lock)
 {
-	int resched = should_resched(PREEMPT_LOCK_OFFSET);
+	int resched = should_resched();
 	int ret = 0;
 
 	lockdep_assert_held(lock);
@@ -4153,7 +4153,7 @@ int __sched __cond_resched_softirq(void)
 {
 	BUG_ON(!in_softirq());
 
-	if (should_resched(SOFTIRQ_DISABLE_OFFSET)) {
+	if (should_resched()) {
 		local_bh_enable();
 		__cond_resched();
 		local_bh_disable();
-- 
2.1.4

