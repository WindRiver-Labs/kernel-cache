Rework SPARC64 stack tracing to make it compatible with LatencyTop.
Enable LatencyTop support for SPARC64.

Signed-off-by: Hong H. Pham <hong.pham@windriver.com>

---
 arch/sparc64/Kconfig             |    3 ++
 arch/sparc64/kernel/stacktrace.c |   42 ++++++++++++++++++++++++++++++++-----
 2 files changed, 39 insertions(+), 6 deletions(-)

diff --git a/arch/sparc64/Kconfig b/arch/sparc64/Kconfig
index 36b4b7a..1fa250c 100644
--- a/arch/sparc64/Kconfig
+++ b/arch/sparc64/Kconfig
@@ -45,16 +45,19 @@ config IOMMU_HELPER
 config QUICKLIST
 	bool
 	default y
 
 config STACKTRACE_SUPPORT
 	bool
 	default y
 
+config HAVE_LATENCYTOP_SUPPORT
+        def_bool y
+
 config LOCKDEP_SUPPORT
 	bool
 	default y
 
 config ARCH_MAY_HAVE_PC_FDC
 	bool
 	default y
 
diff --git a/arch/sparc64/kernel/stacktrace.c b/arch/sparc64/kernel/stacktrace.c
index 4e21d4a..f339c91 100644
--- a/arch/sparc64/kernel/stacktrace.c
+++ b/arch/sparc64/kernel/stacktrace.c
@@ -2,27 +2,38 @@
 #include <linux/stacktrace.h>
 #include <linux/thread_info.h>
 #include <linux/module.h>
 #include <asm/ptrace.h>
 #include <asm/stacktrace.h>
 
 #include "kstack.h"
 
-void save_stack_trace(struct stack_trace *trace)
+static void __save_stack_trace(struct task_struct *task,
+                               struct stack_trace *trace,
+                               int no_sched)
 {
-	struct thread_info *tp = task_thread_info(current);
+	struct thread_info *tp;
 	unsigned long ksp, fp;
 
+	if (!task)
+		task = current;
+
+	tp = task_thread_info(task);
+
 	stack_trace_flush();
 
-	__asm__ __volatile__(
-		"mov	%%fp, %0"
-		: "=r" (ksp)
-	);
+	if (task == current) {
+		__asm__ __volatile__(
+			"mov	%%fp, %0"
+			: "=r" (ksp)
+		);
+	} else {
+		ksp = tp->ksp;
+	}
 
 	fp = ksp + STACK_BIAS;
 	do {
 		struct sparc_stackf *sf;
 		struct pt_regs *regs;
 		unsigned long pc;
 
 		if (!kstack_valid(tp, fp))
@@ -36,15 +47,34 @@ void save_stack_trace(struct stack_trace *trace)
 				break;
 			pc = regs->tpc;
 			fp = regs->u_regs[UREG_I6] + STACK_BIAS;
 		} else {
 			pc = sf->callers_pc;
 			fp = (unsigned long)sf->fp + STACK_BIAS;
 		}
 
+		if (no_sched) {
+			if (in_sched_functions(pc))
+				continue;
+		}
+
 		if (trace->skip > 0)
 			trace->skip--;
 		else
 			trace->entries[trace->nr_entries++] = pc;
 	} while (trace->nr_entries < trace->max_entries);
 }
+
+void save_stack_trace(struct stack_trace *trace)
+{
+	__save_stack_trace(current, trace, 0);
+}
+
+void save_stack_trace_tsk(struct task_struct *tsk, struct stack_trace *trace)
+{
+	__save_stack_trace(tsk, trace, 1);
+	if (trace->nr_entries < trace->max_entries)
+		trace->entries[trace->nr_entries++] = ULONG_MAX;
+}
+
 EXPORT_SYMBOL_GPL(save_stack_trace);
+EXPORT_SYMBOL_GPL(save_stack_trace_tsk);
