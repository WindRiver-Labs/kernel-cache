From 10d290f8bd322954ba3607c1709695d3c3ca5d00 Mon Sep 17 00:00:00 2001
From: David Daney <david.daney@cavium.com>
Date: Wed, 31 Oct 2012 12:47:38 -0700
Subject: [PATCH 085/382] MIPS: OCTEON: Add /proc/octeon_perf support.

Based on SDK octeon3_3.10.

Warning: Doesn't play well with 'perf' tool.

Signed-off-by: David Daney <david.daney@cavium.com>
Signed-off-by: Leonid Rosenboim <lrosenboim@caviumnetworks.com>
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 arch/mips/cavium-octeon/Makefile         |    1 +
 arch/mips/cavium-octeon/perf_counters.c  |  612 ++++++++++++++++++++++++++++++
 arch/mips/include/asm/octeon/cvmx-core.h |  189 +++++++++
 3 files changed, 802 insertions(+), 0 deletions(-)
 create mode 100644 arch/mips/cavium-octeon/perf_counters.c
 create mode 100644 arch/mips/include/asm/octeon/cvmx-core.h

diff --git a/arch/mips/cavium-octeon/Makefile b/arch/mips/cavium-octeon/Makefile
index 74c0e37..784798d 100644
--- a/arch/mips/cavium-octeon/Makefile
+++ b/arch/mips/cavium-octeon/Makefile
@@ -16,6 +16,7 @@ obj-y := cpu.o setup.o serial.o octeon-platform.o octeon-irq.o csrc-octeon.o
 obj-y += dma-octeon.o
 obj-y += csrc-octeon-ptp.o
 obj-y += octeon-pci-console.o
+obj-y += perf_counters.o
 obj-y += octeon-memcpy.o
 obj-y += executive/
 
diff --git a/arch/mips/cavium-octeon/perf_counters.c b/arch/mips/cavium-octeon/perf_counters.c
new file mode 100644
index 0000000..cf23d80
--- /dev/null
+++ b/arch/mips/cavium-octeon/perf_counters.c
@@ -0,0 +1,612 @@
+/*
+ * Simple /proc interface to the Octeon Performance Counters
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2004-2012 Cavium, Inc.
+ */
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/fs.h>
+#include <linux/seq_file.h>
+#include <linux/proc_fs.h>
+#include <asm/octeon/octeon.h>
+#include <asm/octeon/cvmx-core.h>
+#include <asm/octeon/cvmx-l2c.h>
+#include <asm/octeon/cvmx-l2c-defs.h>
+#include <asm/octeon/cvmx-lmcx-defs.h>
+
+/*
+ * Module parameters used to control the counters. Can be
+ * changed on the fly through sysfs.
+ */
+static char counter0[32] = "sissue";
+module_param_string(counter0, counter0, sizeof(counter0), S_IWUSR | S_IRUGO);
+
+static char counter1[32] = "dissue";
+module_param_string(counter1, counter1, sizeof(counter1), S_IWUSR | S_IRUGO);
+
+static char l2counter0[32] = "imiss";
+module_param_string(l2counter0, l2counter0, sizeof(l2counter0), S_IWUSR | S_IRUGO);
+
+static char l2counter1[32] = "ihit";
+module_param_string(l2counter1, l2counter1, sizeof(l2counter1), S_IWUSR | S_IRUGO);
+
+static char l2counter2[32] = "dmiss";
+module_param_string(l2counter2, l2counter2, sizeof(l2counter2), S_IWUSR | S_IRUGO);
+
+static char l2counter3[32] = "dhit";
+module_param_string(l2counter3, l2counter3, sizeof(l2counter3), S_IWUSR | S_IRUGO);
+
+static struct proc_dir_entry *proc_perf_entry;
+static uint64_t proc_perf_counter_control[2];
+static uint64_t proc_perf_counter_data[NR_CPUS][2];
+static uint64_t proc_perf_l2counter_control[4];
+static uint64_t proc_perf_l2counter_data[4];
+static const char *proc_perf_label[CVMX_CORE_PERF_MAX];
+static const char *proc_perf_l2label[CVMX_L2C_EVENT_MAX];
+static uint64_t proc_perf_dram_clocks;
+static uint64_t proc_perf_dram_operations;
+static int proc_perf_in_use;
+static uint64_t start_cycle, end_cycle;
+static struct proc_perf_l2tad_label
+{
+	/* type of the event */
+	enum cvmx_l2c_tad_event type;
+	/* unique name of each event */
+	const char *name;
+	/*
+	 * Based on the type of the event, print the counter value
+	 * differently
+	 */
+	int info;
+} proc_perf_l2tad_label[] = {
+	{ CVMX_L2C_TAD_EVENT_NONE, "none", 0 },
+	{ CVMX_L2C_TAD_EVENT_TAG_HIT, "hit", 0 },
+	{ CVMX_L2C_TAD_EVENT_TAG_MISS, "miss", 0 },
+	{ CVMX_L2C_TAD_EVENT_TAG_NOALLOC, "no-alloc", 0 },
+	{ CVMX_L2C_TAD_EVENT_TAG_VICTIM, "victim", 0 },
+	{ CVMX_L2C_TAD_EVENT_SC_FAIL, "sc-fail", 0 },
+	{ CVMX_L2C_TAD_EVENT_SC_PASS, "sc-pass", 0 },
+	{ CVMX_L2C_TAD_EVENT_LFB_VALID, "lfb-valid", 1 },
+	{ CVMX_L2C_TAD_EVENT_LFB_WAIT_LFB, "lfb-wait-lfb", 1 },
+	{ CVMX_L2C_TAD_EVENT_LFB_WAIT_VAB, "lfb-wait-vab", 1 },
+	{ CVMX_L2C_TAD_EVENT_QUAD0_INDEX, "quad0-index", 2 },
+	{ CVMX_L2C_TAD_EVENT_QUAD0_READ, "quad0-read", 2 },
+	{ CVMX_L2C_TAD_EVENT_QUAD0_BANK, "quad0-bank", 1 },
+	{ CVMX_L2C_TAD_EVENT_QUAD0_WDAT, "quad0-wdat", 2 },
+	{ CVMX_L2C_TAD_EVENT_QUAD1_INDEX, "quad1-index", 2 },
+	{ CVMX_L2C_TAD_EVENT_QUAD1_READ, "quad1-read", 2 },
+	{ CVMX_L2C_TAD_EVENT_QUAD1_BANK, "quad1-bank", 1 },
+	{ CVMX_L2C_TAD_EVENT_QUAD1_WDAT, "quad1-wdat", 2 },
+	{ CVMX_L2C_TAD_EVENT_QUAD2_INDEX, "quad2-index", 2 },
+	{ CVMX_L2C_TAD_EVENT_QUAD2_READ, "quad2-read", 2 },
+	{ CVMX_L2C_TAD_EVENT_QUAD2_BANK, "quad2-bank", 1 },
+	{ CVMX_L2C_TAD_EVENT_QUAD2_WDAT, "quad2-wdat", 2 },
+	{ CVMX_L2C_TAD_EVENT_QUAD3_INDEX, "quad3-index", 2 },
+	{ CVMX_L2C_TAD_EVENT_QUAD3_READ, "quad3-read", 2 },
+	{ CVMX_L2C_TAD_EVENT_QUAD3_BANK, "quad3-bank", 1 },
+	{ CVMX_L2C_TAD_EVENT_QUAD3_WDAT, "quad3-wdat", 2 },
+	{ CVMX_L2C_TAD_EVENT_MAX, NULL, 0}
+};
+
+/*
+ * Setup the core counters. Called on each core
+ */
+static void proc_perf_setup_counters(void *arg)
+{
+	union cvmx_core_perf_control control;
+	uint64_t cvmctl;
+
+	if (proc_perf_in_use) {
+		/*
+		 * Disable the issue and exec conditional clock
+		 * support so we get better results
+		 */
+		cvmctl = __read_64bit_c0_register($9, 7);
+		cvmctl |= 3 << 16;
+		__write_64bit_c0_register($9, 7, cvmctl);
+	}
+
+	control.u32 = 0;
+	control.s.event = proc_perf_counter_control[0];
+	control.s.u = 1;
+	control.s.s = 1;
+	control.s.k = 1;
+	control.s.ex = 1;
+	__write_32bit_c0_register($25, 0, control.u32);
+
+	control.s.event = proc_perf_counter_control[1];
+	__write_32bit_c0_register($25, 2, control.u32);
+
+	__write_32bit_c0_register($25, 1, 0);
+	__write_32bit_c0_register($25, 3, 0);
+}
+
+/*
+ * Update the counters for each core.
+ */
+static void proc_perf_update_counters(void *arg)
+{
+	int cpu = smp_processor_id();
+
+	proc_perf_counter_data[cpu][0] = __read_64bit_c0_register($25, 1);
+	proc_perf_counter_data[cpu][1] = __read_64bit_c0_register($25, 3);
+	mb();
+}
+
+/*
+ * Cleanup the input of sysfs
+ */
+static inline void clean_string(char *str, int len)
+{
+	int i;
+	for (i = 0; i < len; i++)
+		if (str[i] <= 32)
+			str[i] = 0;
+}
+
+/*
+ * Setup the counters using the current config
+ */
+static void proc_perf_setup(void)
+{
+	int i;
+
+	proc_perf_counter_control[0] = 0;
+	proc_perf_counter_control[1] = 0;
+	proc_perf_l2counter_control[0] = 0;
+	proc_perf_l2counter_control[1] = 0;
+	proc_perf_l2counter_control[2] = 0;
+	proc_perf_l2counter_control[3] = 0;
+
+	/* Cleanup junk on end of param strings */
+	clean_string(counter0, sizeof(counter0));
+	clean_string(counter1, sizeof(counter1));
+	clean_string(l2counter0, sizeof(l2counter0));
+	clean_string(l2counter1, sizeof(l2counter1));
+	clean_string(l2counter2, sizeof(l2counter2));
+	clean_string(l2counter3, sizeof(l2counter3));
+
+	/* Set the core counters to match the string parameters */
+	for (i = 0; i < CVMX_CORE_PERF_MAX; i++) {
+		if (proc_perf_label[i]) {
+			if (strcmp(proc_perf_label[i], counter0) == 0)
+				proc_perf_counter_control[0] = i;
+			if (strcmp(proc_perf_label[i], counter1) == 0)
+				proc_perf_counter_control[1] = i;
+		}
+	}
+	if (OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN3XXX)) {
+		if (proc_perf_counter_control[0] & ~0x3f) {
+			pr_err("WARNING: Invalid Core Performance Counter0 Event(%s), resetting to 'none'.\n",
+			       proc_perf_label[proc_perf_counter_control[0]]);
+			proc_perf_counter_control[0] = 0;
+		}
+		if (proc_perf_counter_control[1] & ~0x3f) {
+			pr_err("WARNING: Invalid Core Performance Counter1 Event(%s), resetting to 'none'.\n",
+			       proc_perf_label[proc_perf_counter_control[1]]);
+			proc_perf_counter_control[1] = 0;
+		}
+	}
+
+	/* Set the L2 counters to match the string parameters */
+	if (OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN3XXX)) {
+		for (i = 0; i < CVMX_L2C_EVENT_MAX; i++) {
+			if (proc_perf_l2label[i]) {
+				if (strcmp(proc_perf_l2label[i], l2counter0) == 0)
+					proc_perf_l2counter_control[0] = i;
+				if (strcmp(proc_perf_l2label[i], l2counter1) == 0)
+					proc_perf_l2counter_control[1] = i;
+				if (strcmp(proc_perf_l2label[i], l2counter2) == 0)
+					proc_perf_l2counter_control[2] = i;
+				if (strcmp(proc_perf_l2label[i], l2counter3) == 0)
+					proc_perf_l2counter_control[3] = i;
+			}
+		}
+	} else {
+		for (i = 0; proc_perf_l2tad_label[i].name; i++) {
+			if (strcmp(proc_perf_l2tad_label[i].name, l2counter0) == 0)
+				proc_perf_l2counter_control[0] = i;
+			if (strcmp(proc_perf_l2tad_label[i].name, l2counter1) == 0)
+				proc_perf_l2counter_control[1] = i;
+			if (strcmp(proc_perf_l2tad_label[i].name, l2counter2) == 0)
+				proc_perf_l2counter_control[2] = i;
+			if (strcmp(proc_perf_l2tad_label[i].name, l2counter3) == 0)
+				proc_perf_l2counter_control[3] = i;
+		}
+	}
+
+	/* Update strings to match final config */
+	strcpy(counter0, proc_perf_label[proc_perf_counter_control[0]]);
+	strcpy(counter1, proc_perf_label[proc_perf_counter_control[1]]);
+
+	if (OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN3XXX)) {
+		strcpy(l2counter0,
+		       proc_perf_l2label[proc_perf_l2counter_control[0]]);
+		strcpy(l2counter1,
+		       proc_perf_l2label[proc_perf_l2counter_control[1]]);
+		strcpy(l2counter2,
+		       proc_perf_l2label[proc_perf_l2counter_control[2]]);
+		strcpy(l2counter3,
+		       proc_perf_l2label[proc_perf_l2counter_control[3]]);
+	} else {
+		strcpy(l2counter0,
+		       proc_perf_l2tad_label[proc_perf_l2counter_control[0]].name);
+		strcpy(l2counter1,
+		       proc_perf_l2tad_label[proc_perf_l2counter_control[1]].name);
+		strcpy(l2counter2,
+		       proc_perf_l2tad_label[proc_perf_l2counter_control[2]].name);
+		strcpy(l2counter3,
+		       proc_perf_l2tad_label[proc_perf_l2counter_control[3]].name);
+	}
+
+	on_each_cpu(proc_perf_setup_counters, NULL, 1);
+
+	if (OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN3XXX)) {
+		union cvmx_l2c_pfctl l2control;
+
+		l2control.u64 = 0;
+		l2control.s.cnt3ena = 1;
+		l2control.s.cnt3clr = 1;
+		l2control.s.cnt3sel = proc_perf_l2counter_control[3];
+		l2control.s.cnt2ena = 1;
+		l2control.s.cnt2clr = 1;
+		l2control.s.cnt2sel = proc_perf_l2counter_control[2];
+		l2control.s.cnt1ena = 1;
+		l2control.s.cnt1clr = 1;
+		l2control.s.cnt1sel = proc_perf_l2counter_control[1];
+		l2control.s.cnt0ena = 1;
+		l2control.s.cnt0clr = 1;
+		l2control.s.cnt0sel = proc_perf_l2counter_control[0];
+
+		cvmx_write_csr(CVMX_L2C_PFCTL, l2control.u64);
+	} else {
+		union cvmx_l2c_tadx_prf l2c_tadx_prf;
+		int tad;
+
+		l2c_tadx_prf.u64 = 0;
+		l2c_tadx_prf.s.cnt3sel = proc_perf_l2counter_control[3];
+		l2c_tadx_prf.s.cnt2sel = proc_perf_l2counter_control[2];
+		l2c_tadx_prf.s.cnt1sel = proc_perf_l2counter_control[1];
+		l2c_tadx_prf.s.cnt0sel = proc_perf_l2counter_control[0];
+
+		for (tad = 0; tad < CVMX_L2C_TADS; tad++)
+			cvmx_write_csr(CVMX_L2C_TADX_PRF(tad), l2c_tadx_prf.u64);
+
+		start_cycle = cvmx_read_csr(CVMX_IPD_CLK_COUNT);
+	}
+}
+
+static void proc_perf_update(void)
+{
+	on_each_cpu(proc_perf_update_counters, NULL, 1);
+	mb();
+
+	if (OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN3XXX)) {
+		proc_perf_l2counter_data[0] = cvmx_read_csr(CVMX_L2C_PFC0);
+		proc_perf_l2counter_data[1] = cvmx_read_csr(CVMX_L2C_PFC1);
+		proc_perf_l2counter_data[2] = cvmx_read_csr(CVMX_L2C_PFC2);
+		proc_perf_l2counter_data[3] = cvmx_read_csr(CVMX_L2C_PFC3);
+	} else {
+		int tad;
+		proc_perf_l2counter_data[0] = 0;
+		proc_perf_l2counter_data[1] = 0;
+		proc_perf_l2counter_data[2] = 0;
+		proc_perf_l2counter_data[3] = 0;
+		for (tad = 0; tad < CVMX_L2C_TADS; tad++) {
+			proc_perf_l2counter_data[0] +=
+				cvmx_read_csr(CVMX_L2C_TADX_PFC0(tad));
+			cvmx_write_csr(CVMX_L2C_TADX_PFC0(tad), 0);
+			proc_perf_l2counter_data[1] +=
+				cvmx_read_csr(CVMX_L2C_TADX_PFC1(tad));
+			cvmx_write_csr(CVMX_L2C_TADX_PFC1(tad), 0);
+			proc_perf_l2counter_data[2] +=
+				cvmx_read_csr(CVMX_L2C_TADX_PFC2(tad));
+			cvmx_write_csr(CVMX_L2C_TADX_PFC2(tad), 0);
+			proc_perf_l2counter_data[3] +=
+				cvmx_read_csr(CVMX_L2C_TADX_PFC3(tad));
+			cvmx_write_csr(CVMX_L2C_TADX_PFC3(tad), 0);
+		}
+		end_cycle = cvmx_read_csr(CVMX_IPD_CLK_COUNT);
+	}
+}
+
+/*
+ * Show the counters to the user
+ */
+static int proc_perf_show(struct seq_file *m, void *v)
+{
+	int cpu;
+	int i;
+	uint64_t dram_clocks;
+	uint64_t dram_operations;
+	union cvmx_core_perf_control control0;
+	union cvmx_core_perf_control control1;
+
+	proc_perf_update();
+
+	control0.u32 = __read_32bit_c0_register($25, 0);
+	control1.u32 = __read_32bit_c0_register($25, 2);
+	seq_printf(m, "       %16s %16s\n",
+		   proc_perf_label[control0.s.event],
+		   proc_perf_label[control1.s.event]);
+	for_each_online_cpu (cpu) {
+		seq_printf(m, "CPU%2d: %16llu %16llu\n", cpu,
+			   (unsigned long long)proc_perf_counter_data[cpu][0],
+			   (unsigned long long)proc_perf_counter_data[cpu][1]);
+	}
+
+	seq_printf(m, "\n");
+	if (OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN3XXX)) {
+		for (i = 0; i < 4; i++)
+			seq_printf(m, "%s: %llu\n",
+				   proc_perf_l2label[proc_perf_l2counter_control[i]],
+				   (unsigned long long) proc_perf_l2counter_data[i]);
+	} else {
+		uint64_t cycles_used = end_cycle - start_cycle;
+		for (i = 0; i < 4; i++) {
+			if (proc_perf_l2tad_label[proc_perf_l2counter_control[i]].info == 1)
+				seq_printf(m, "%s: %llu, average: %6lu\n",
+					   proc_perf_l2tad_label
+					   [proc_perf_l2counter_control[i]].name,
+					   (unsigned long long)proc_perf_l2counter_data[i],
+					   (long int)(proc_perf_l2counter_data[i] / (cycles_used * CVMX_L2C_TADS)));
+			else if (proc_perf_l2tad_label[proc_perf_l2counter_control[i]].info == 2)
+				seq_printf(m, "%s bus utilization: %4lu%%\n",
+					   proc_perf_l2tad_label
+					   [proc_perf_l2counter_control[i]].name,
+					   (long int)((proc_perf_l2counter_data[i]*100) / (cycles_used * CVMX_L2C_TADS)));
+			else
+				seq_printf(m, "%s: %llu\n",
+					   proc_perf_l2tad_label
+					   [proc_perf_l2counter_control[i]].name,
+					   (unsigned long long)proc_perf_l2counter_data[i]);
+		}
+	}
+
+	/* Compute DRAM utilization */
+	if (OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN3XXX)) {
+		dram_operations =
+			(cvmx_read_csr(CVMX_LMCX_OPS_CNT_HI(0)) << 32) |
+			cvmx_read_csr(CVMX_LMCX_OPS_CNT_LO(0));
+		dram_clocks =
+			(cvmx_read_csr(CVMX_LMCX_DCLK_CNT_HI(0)) << 32) |
+			cvmx_read_csr(CVMX_LMCX_DCLK_CNT_LO(0));
+	} else {
+		int tad;
+		dram_operations = 0;
+		dram_clocks = 0;
+		for (tad = 0; tad < 1; tad++) {
+			dram_operations += cvmx_read_csr
+					(CVMX_LMCX_OPS_CNT(tad));
+			dram_clocks += cvmx_read_csr
+					(CVMX_LMCX_DCLK_CNT(tad));
+		}
+	}
+
+	if (dram_clocks > proc_perf_dram_clocks) {
+		uint64_t delta_clocks = dram_clocks - proc_perf_dram_clocks;
+		uint64_t delta_operations =
+			dram_operations - proc_perf_dram_operations;
+		uint64_t percent_x100 = 10000 * delta_operations / delta_clocks;
+		seq_printf(m,
+			   "\nDRAM ops count: %lu, dclk count: %lu, utilization: %lu.%02lu%%\n",
+			   (long int)delta_operations, (long int)delta_clocks,
+			   (long int)percent_x100 / 100,
+			   (long int)percent_x100 % 100);
+	}
+
+	proc_perf_dram_operations = dram_operations;
+	proc_perf_dram_clocks = dram_clocks;
+
+	seq_printf(m,
+		   "\n"
+		   "Configuration of the performance counters is controlled by writing\n"
+		   "one of the following values to:\n"
+		   "    /sys/module/perf_counters/parameters/counter{0,1}\n"
+		   "    /sys/module/perf_counters/parameters/l2counter{0-3}\n"
+		   "\n"
+		   "Possible CPU counters:");
+	for (i = 0; i < CVMX_CORE_PERF_MAX; i++) {
+		if ((i & 7) == 0)
+			seq_printf(m, "\n    ");
+		if (proc_perf_label[i])
+			seq_printf(m, "%s ", proc_perf_label[i]);
+	}
+
+	seq_printf(m, "\nPossible L2 counters:");
+	if (OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN3XXX)) {
+		for (i = 0; i < CVMX_L2C_EVENT_MAX; i++) {
+			if ((i & 3) == 0)
+				seq_printf(m, "\n    ");
+			if (proc_perf_l2label[i])
+				seq_printf(m, "%s ", proc_perf_l2label[i]);
+		}
+	} else {
+		for (i = 0; proc_perf_l2tad_label[i].name; i++) {
+			if ((i & 3) == 0)
+				seq_printf(m, "\n    ");
+			seq_printf(m, "%s ", proc_perf_l2tad_label[i].name);
+		}
+	}
+
+	seq_printf(m,
+		   "\nWarning: Counter configuration doesn't update till you access /proc/octeon_perf.\n");
+
+	proc_perf_setup();
+	return 0;
+}
+
+/*
+ * /proc/octeon_perf was openned. Use the single_open iterator
+ */
+static int proc_perf_open(struct inode *inode, struct file *file)
+{
+	proc_perf_in_use = 1;
+	return single_open(file, proc_perf_show, NULL);
+}
+
+static const struct file_operations proc_perf_operations = {
+	.open = proc_perf_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+/*
+ * Module initialization
+ */
+static int __init proc_perf_init(void)
+{
+	pr_notice("/proc/octeon_perf: Octeon performance counter interface loaded\n");
+
+	proc_perf_label[CVMX_CORE_PERF_NONE] = "none";
+	proc_perf_label[CVMX_CORE_PERF_CLK] = "clk";
+	proc_perf_label[CVMX_CORE_PERF_ISSUE] = "issue";
+	proc_perf_label[CVMX_CORE_PERF_RET] = "ret";
+	proc_perf_label[CVMX_CORE_PERF_NISSUE] = "nissue";
+	proc_perf_label[CVMX_CORE_PERF_SISSUE] = "sissue";
+	proc_perf_label[CVMX_CORE_PERF_DISSUE] = "dissue";
+	proc_perf_label[CVMX_CORE_PERF_IFI] = "ifi";
+	proc_perf_label[CVMX_CORE_PERF_BR] = "br";
+	proc_perf_label[CVMX_CORE_PERF_BRMIS] = "brmis";
+	proc_perf_label[CVMX_CORE_PERF_J] = "j";
+	proc_perf_label[CVMX_CORE_PERF_JMIS] = "jmis";
+	proc_perf_label[CVMX_CORE_PERF_REPLAY] = "replay";
+	proc_perf_label[CVMX_CORE_PERF_IUNA] = "iuna";
+	proc_perf_label[CVMX_CORE_PERF_TRAP] = "trap";
+	proc_perf_label[CVMX_CORE_PERF_UULOAD] = "uuload";
+	proc_perf_label[CVMX_CORE_PERF_UUSTORE] = "uustore";
+	proc_perf_label[CVMX_CORE_PERF_ULOAD] = "uload";
+	proc_perf_label[CVMX_CORE_PERF_USTORE] = "ustore";
+	proc_perf_label[CVMX_CORE_PERF_EC] = "ec";
+	proc_perf_label[CVMX_CORE_PERF_MC] = "mc";
+	proc_perf_label[CVMX_CORE_PERF_CC] = "cc";
+	proc_perf_label[CVMX_CORE_PERF_CSRC] = "csrc";
+	proc_perf_label[CVMX_CORE_PERF_CFETCH] = "cfetch";
+	proc_perf_label[CVMX_CORE_PERF_CPREF] = "cpref";
+	proc_perf_label[CVMX_CORE_PERF_ICA] = "ica";
+	proc_perf_label[CVMX_CORE_PERF_II] = "ii";
+	proc_perf_label[CVMX_CORE_PERF_IP] = "ip";
+	proc_perf_label[CVMX_CORE_PERF_CIMISS] = "cimiss";
+	proc_perf_label[CVMX_CORE_PERF_WBUF] = "wbuf";
+	proc_perf_label[CVMX_CORE_PERF_WDAT] = "wdat";
+	proc_perf_label[CVMX_CORE_PERF_WBUFLD] = "wbufld";
+	proc_perf_label[CVMX_CORE_PERF_WBUFFL] = "wbuffl";
+	proc_perf_label[CVMX_CORE_PERF_WBUFTR] = "wbuftr";
+	proc_perf_label[CVMX_CORE_PERF_BADD] = "badd";
+	proc_perf_label[CVMX_CORE_PERF_BADDL2] = "baddl2";
+	proc_perf_label[CVMX_CORE_PERF_BFILL] = "bfill";
+	proc_perf_label[CVMX_CORE_PERF_DDIDS] = "ddids";
+	proc_perf_label[CVMX_CORE_PERF_IDIDS] = "idids";
+	proc_perf_label[CVMX_CORE_PERF_DIDNA] = "didna";
+	proc_perf_label[CVMX_CORE_PERF_LDS] = "lds";
+	proc_perf_label[CVMX_CORE_PERF_LMLDS] = "lmlds";
+	proc_perf_label[CVMX_CORE_PERF_IOLDS] = "iolds";
+	proc_perf_label[CVMX_CORE_PERF_DMLDS] = "dmlds";
+	proc_perf_label[CVMX_CORE_PERF_STS] = "sts";
+	proc_perf_label[CVMX_CORE_PERF_LMSTS] = "lmsts";
+	proc_perf_label[CVMX_CORE_PERF_IOSTS] = "iosts";
+	proc_perf_label[CVMX_CORE_PERF_IOBDMA] = "iobdma";
+	proc_perf_label[CVMX_CORE_PERF_DTLB] = "dtlb";
+	proc_perf_label[CVMX_CORE_PERF_DTLBAD] = "dtlbad";
+	proc_perf_label[CVMX_CORE_PERF_ITLB] = "itlb";
+	proc_perf_label[CVMX_CORE_PERF_SYNC] = "sync";
+	proc_perf_label[CVMX_CORE_PERF_SYNCIOB] = "synciob";
+	proc_perf_label[CVMX_CORE_PERF_SYNCW] = "syncw";
+	if (!OCTEON_IS_MODEL(OCTEON_CN5XXX) && !OCTEON_IS_MODEL(OCTEON_CN3XXX)) {
+		proc_perf_label[CVMX_CORE_PERF_ERETMIS] = "eretmis";
+		proc_perf_label[CVMX_CORE_PERF_LIKMIS] = "likmis";
+		proc_perf_label[CVMX_CORE_PERF_HAZTR] = "hazard-trap";
+	}
+
+	if (OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN3XXX)) {
+		proc_perf_l2label[CVMX_L2C_EVENT_CYCLES] = "cycles";
+		proc_perf_l2label[CVMX_L2C_EVENT_INSTRUCTION_MISS] = "imiss";
+		proc_perf_l2label[CVMX_L2C_EVENT_INSTRUCTION_HIT] = "ihit";
+		proc_perf_l2label[CVMX_L2C_EVENT_DATA_MISS] = "dmiss";
+		proc_perf_l2label[CVMX_L2C_EVENT_DATA_HIT] = "dhit";
+		proc_perf_l2label[CVMX_L2C_EVENT_MISS] = "miss";
+		proc_perf_l2label[CVMX_L2C_EVENT_HIT] = "hit";
+		proc_perf_l2label[CVMX_L2C_EVENT_VICTIM_HIT] = "victim-buffer-hit";
+		proc_perf_l2label[CVMX_L2C_EVENT_INDEX_CONFLICT] = "lfb-nq-index-conflict";
+		proc_perf_l2label[CVMX_L2C_EVENT_TAG_PROBE] = "tag-probe";
+		proc_perf_l2label[CVMX_L2C_EVENT_TAG_UPDATE] = "tag-update";
+		proc_perf_l2label[CVMX_L2C_EVENT_TAG_COMPLETE] = "tag-probe-completed";
+		proc_perf_l2label[CVMX_L2C_EVENT_TAG_DIRTY] = "tag-dirty-victim";
+		proc_perf_l2label[CVMX_L2C_EVENT_DATA_STORE_NOP] = "data-store-nop";
+		proc_perf_l2label[CVMX_L2C_EVENT_DATA_STORE_READ] = "data-store-read";
+		proc_perf_l2label[CVMX_L2C_EVENT_DATA_STORE_WRITE] = "data-store-write";
+		proc_perf_l2label[CVMX_L2C_EVENT_FILL_DATA_VALID] = "memory-fill-data-valid";
+		proc_perf_l2label[CVMX_L2C_EVENT_WRITE_REQUEST] = "memory-write-request";
+		proc_perf_l2label[CVMX_L2C_EVENT_READ_REQUEST] = "memory-read-request";
+		proc_perf_l2label[CVMX_L2C_EVENT_WRITE_DATA_VALID] = "memory-write-data-valid";
+		proc_perf_l2label[CVMX_L2C_EVENT_XMC_NOP] = "xmc-nop";
+		proc_perf_l2label[CVMX_L2C_EVENT_XMC_LDT] = "xmc-ldt";
+		proc_perf_l2label[CVMX_L2C_EVENT_XMC_LDI] = "xmc-ldi";
+		proc_perf_l2label[CVMX_L2C_EVENT_XMC_LDD] = "xmc-ldd";
+		proc_perf_l2label[CVMX_L2C_EVENT_XMC_STF] = "xmc-stf";
+		proc_perf_l2label[CVMX_L2C_EVENT_XMC_STT] = "xmc-stt";
+		proc_perf_l2label[CVMX_L2C_EVENT_XMC_STP] = "xmc-stp";
+		proc_perf_l2label[CVMX_L2C_EVENT_XMC_STC] = "xmc-stc";
+		proc_perf_l2label[CVMX_L2C_EVENT_XMC_DWB] = "xmc-dwb";
+		proc_perf_l2label[CVMX_L2C_EVENT_XMC_PL2] = "xmc-pl2";
+		proc_perf_l2label[CVMX_L2C_EVENT_XMC_PSL1] = "xmc-psl1";
+		proc_perf_l2label[CVMX_L2C_EVENT_XMC_IOBLD] = "xmc-iobld";
+		proc_perf_l2label[CVMX_L2C_EVENT_XMC_IOBST] = "xmc-iobst";
+		proc_perf_l2label[CVMX_L2C_EVENT_XMC_IOBDMA] = "xmc-iobdma";
+		proc_perf_l2label[CVMX_L2C_EVENT_XMC_IOBRSP] = "xmc-iobrsp";
+		proc_perf_l2label[CVMX_L2C_EVENT_XMC_BUS_VALID] = "xmd-bus-valid";
+		proc_perf_l2label[CVMX_L2C_EVENT_XMC_MEM_DATA] = "xmd-bus-valid-dst-l2c";
+		proc_perf_l2label[CVMX_L2C_EVENT_XMC_REFL_DATA] = "xmd-bus-valid-dst-iob";
+		proc_perf_l2label[CVMX_L2C_EVENT_XMC_IOBRSP_DATA] = "xmd-bus-valid-dst-pp";
+		proc_perf_l2label[CVMX_L2C_EVENT_RSC_NOP] = "rsc-nop";
+		proc_perf_l2label[CVMX_L2C_EVENT_RSC_STDN] = "rsc-stdn";
+		proc_perf_l2label[CVMX_L2C_EVENT_RSC_FILL] = "rsc-fill";
+		proc_perf_l2label[CVMX_L2C_EVENT_RSC_REFL] = "rsc-refl";
+		proc_perf_l2label[CVMX_L2C_EVENT_RSC_STIN] = "rsc-stin";
+		proc_perf_l2label[CVMX_L2C_EVENT_RSC_SCIN] = "rsc-scin";
+		proc_perf_l2label[CVMX_L2C_EVENT_RSC_SCFL] = "rsc-scfl";
+		proc_perf_l2label[CVMX_L2C_EVENT_RSC_SCDN] = "rsc-scdn";
+		proc_perf_l2label[CVMX_L2C_EVENT_RSC_DATA_VALID] = "rsd-data-valid";
+		proc_perf_l2label[CVMX_L2C_EVENT_RSC_VALID_FILL] = "rsd-data-valid-fill";
+		proc_perf_l2label[CVMX_L2C_EVENT_RSC_VALID_STRSP] = "rsd-data-valid-strsp";
+		proc_perf_l2label[CVMX_L2C_EVENT_RSC_VALID_REFL] = "rsd-data-valid-refl";
+		proc_perf_l2label[CVMX_L2C_EVENT_LRF_REQ] = "lrf-req";
+		proc_perf_l2label[CVMX_L2C_EVENT_DT_RD_ALLOC] = "dt-rd-alloc";
+		proc_perf_l2label[CVMX_L2C_EVENT_DT_WR_INVAL] = "dt-wr-inva";
+	}
+
+	proc_perf_entry = proc_create("octeon_perf", S_IRUGO, NULL,
+				&proc_perf_operations);
+
+	/* Octeon2 has different L2C performance counters */
+	if (!(OCTEON_IS_MODEL(OCTEON_CN5XXX) ||
+	      OCTEON_IS_MODEL(OCTEON_CN3XXX))) {
+		strcpy(l2counter0, "hit");
+		strcpy(l2counter1, "miss");
+		strcpy(l2counter2, "lfb-wait-lfb");
+		strcpy(l2counter3, "lfb-wait-vab");
+	}
+
+	proc_perf_setup();
+	return 0;
+}
+
+/*
+ * Module cleanup
+ */
+static void __exit proc_perf_cleanup(void)
+{
+	if (proc_perf_entry)
+		remove_proc_entry("octeon_perf", NULL);
+}
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Cavium Inc. <support@cavium.com>");
+MODULE_DESCRIPTION("Cavium Inc. OCTEON performance counter interface.");
+module_init(proc_perf_init);
+module_exit(proc_perf_cleanup);
diff --git a/arch/mips/include/asm/octeon/cvmx-core.h b/arch/mips/include/asm/octeon/cvmx-core.h
new file mode 100644
index 0000000..f7b4814
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-core.h
@@ -0,0 +1,189 @@
+/***********************license start***************
+ * Author: Cavium Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2010 Cavium Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium Inc. for more information
+ ***********************license end**************************************/
+
+/*
+ * Module to support operations on core such as TLB config, etc.
+ */
+
+#ifndef __CVMX_CORE_H__
+#define __CVMX_CORE_H__
+
+/*
+ * The types of performance counters supported per cpu
+ */
+enum cvmx_core_perf {
+	CVMX_CORE_PERF_NONE = 0,
+				     /* Turn off the performance counter */
+	CVMX_CORE_PERF_CLK = 1,
+				     /* Conditionally clocked cycles (as opposed to count/cvm_count which count even with no clocks) */
+	CVMX_CORE_PERF_ISSUE = 2,
+				     /* Instructions issued but not retired */
+	CVMX_CORE_PERF_RET = 3,
+				     /* Instructions retired */
+	CVMX_CORE_PERF_NISSUE = 4,
+				     /* Cycles no issue */
+	CVMX_CORE_PERF_SISSUE = 5,
+				     /* Cycles single issue */
+	CVMX_CORE_PERF_DISSUE = 6,
+				     /* Cycles dual issue */
+	CVMX_CORE_PERF_IFI = 7,
+				     /* Cycle ifetch issued (but not necessarily commit to pp_mem) */
+	CVMX_CORE_PERF_BR = 8,
+				     /* Branches retired */
+	CVMX_CORE_PERF_BRMIS = 9,
+				     /* Branch mispredicts */
+	CVMX_CORE_PERF_J = 10,
+				     /* Jumps retired */
+	CVMX_CORE_PERF_JMIS = 11,
+				     /* Jumps mispredicted */
+	CVMX_CORE_PERF_REPLAY = 12,
+				     /* Mem Replays */
+	CVMX_CORE_PERF_IUNA = 13,
+				     /* Cycles idle due to unaligned_replays */
+	CVMX_CORE_PERF_TRAP = 14,
+				     /* trap_6a signal */
+	CVMX_CORE_PERF_UULOAD = 16,
+				     /* Unexpected unaligned loads (REPUN=1) */
+	CVMX_CORE_PERF_UUSTORE = 17,
+				     /* Unexpected unaligned store (REPUN=1) */
+	CVMX_CORE_PERF_ULOAD = 18,
+				     /* Unaligned loads (REPUN=1 or USEUN=1) */
+	CVMX_CORE_PERF_USTORE = 19,
+				     /* Unaligned store (REPUN=1 or USEUN=1) */
+	CVMX_CORE_PERF_EC = 20,
+				     /* Exec clocks(must set CvmCtl[DISCE] for accurate timing) */
+	CVMX_CORE_PERF_MC = 21,
+				     /* Mul clocks(must set CvmCtl[DISCE] for accurate timing) */
+	CVMX_CORE_PERF_CC = 22,
+				     /* Crypto clocks(must set CvmCtl[DISCE] for accurate timing) */
+	CVMX_CORE_PERF_CSRC = 23,
+				     /* Issue_csr clocks(must set CvmCtl[DISCE] for accurate timing) */
+	CVMX_CORE_PERF_CFETCH = 24,
+				     /* Icache committed fetches (demand+prefetch) */
+	CVMX_CORE_PERF_CPREF = 25,
+				     /* Icache committed prefetches */
+	CVMX_CORE_PERF_ICA = 26,
+				     /* Icache aliases */
+	CVMX_CORE_PERF_II = 27,
+				     /* Icache invalidates */
+	CVMX_CORE_PERF_IP = 28,
+				     /* Icache parity error */
+	CVMX_CORE_PERF_CIMISS = 29,
+				     /* Cycles idle due to imiss (must set CvmCtl[DISCE] for accurate timing) */
+	CVMX_CORE_PERF_WBUF = 32,
+				     /* Number of write buffer entries created */
+	CVMX_CORE_PERF_WDAT = 33,
+				     /* Number of write buffer data cycles used (may need to set CvmCtl[DISCE] for accurate counts) */
+	CVMX_CORE_PERF_WBUFLD = 34,
+				     /* Number of write buffer entries forced out by loads */
+	CVMX_CORE_PERF_WBUFFL = 35,
+				     /* Number of cycles that there was no available write buffer entry (may need to set CvmCtl[DISCE] and CvmMemCtl[MCLK] for accurate counts) */
+	CVMX_CORE_PERF_WBUFTR = 36,
+				     /* Number of stores that found no available write buffer entries */
+	CVMX_CORE_PERF_BADD = 37,
+				     /* Number of address bus cycles used (may need to set CvmCtl[DISCE] for accurate counts) */
+	CVMX_CORE_PERF_BADDL2 = 38,
+				     /* Number of address bus cycles not reflected (i.e. destined for L2) (may need to set CvmCtl[DISCE] for accurate counts) */
+	CVMX_CORE_PERF_BFILL = 39,
+				     /* Number of fill bus cycles used (may need to set CvmCtl[DISCE] for accurate counts) */
+	CVMX_CORE_PERF_DDIDS = 40,
+				     /* Number of Dstream DIDs created */
+	CVMX_CORE_PERF_IDIDS = 41,
+				     /* Number of Istream DIDs created */
+	CVMX_CORE_PERF_DIDNA = 42,
+				     /* Number of cycles that no DIDs were available (may need to set CvmCtl[DISCE] and CvmMemCtl[MCLK] for accurate counts) */
+	CVMX_CORE_PERF_LDS = 43,
+				     /* Number of load issues */
+	CVMX_CORE_PERF_LMLDS = 44,
+				     /* Number of local memory load */
+	CVMX_CORE_PERF_IOLDS = 45,
+				     /* Number of I/O load issues */
+	CVMX_CORE_PERF_DMLDS = 46,
+				     /* Number of loads that were not prefetches and missed in the cache */
+	CVMX_CORE_PERF_STS = 48,
+				     /* Number of store issues */
+	CVMX_CORE_PERF_LMSTS = 49,
+				     /* Number of local memory store issues */
+	CVMX_CORE_PERF_IOSTS = 50,
+				     /* Number of I/O store issues */
+	CVMX_CORE_PERF_IOBDMA = 51,
+				     /* Number of IOBDMAs */
+	CVMX_CORE_PERF_DTLB = 53,
+				     /* Number of dstream TLB refill, invalid, or modified exceptions */
+	CVMX_CORE_PERF_DTLBAD = 54,
+				     /* Number of dstream TLB address errors */
+	CVMX_CORE_PERF_ITLB = 55,
+				     /* Number of istream TLB refill, invalid, or address error exceptions */
+	CVMX_CORE_PERF_SYNC = 56,
+				     /* Number of SYNC stall cycles (may need to set CvmCtl[DISCE] for accurate counts) */
+	CVMX_CORE_PERF_SYNCIOB = 57,
+				     /* Number of SYNCIOBDMA stall cycles (may need to set CvmCtl[DISCE] for accurate counts) */
+	CVMX_CORE_PERF_SYNCW = 58,
+				     /* Number of SYNCWs */
+	/* Added in CN63XX */
+	CVMX_CORE_PERF_ERETMIS = 64,
+				     /* D/eret mispredicts */
+	CVMX_CORE_PERF_LIKMIS = 65,
+				     /* Branch likely mispredicts */
+	CVMX_CORE_PERF_HAZTR = 66,
+				     /* Hazard traps due to *MTC0 to CvmCtl, Perf counter control, EntryHi, or CvmMemCtl registers */
+	CVMX_CORE_PERF_MAX	     /* This not a counter, just a marker for the highest number */
+};
+/**
+ * Bit description of the COP0 counter control register
+ */
+union cvmx_core_perf_control {
+	uint32_t u32;
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint32_t m:1;	     /* Set to 1 for sel 0 and 0 for sel 2, indicating there are two performance counters */
+		uint32_t w:1;	     /* Set to 1 indicating counters are 64 bit */
+		uint32_t reserved_11_29:15;
+		uint32_t event:10;
+				     /* Selects the event to be counted by the corresponding Counter Register */
+		uint32_t ie:1;
+				     /* Interrupt Enable */
+		uint32_t u:1;	     /* Count in user mode */
+		uint32_t s:1;	     /* Count in supervisor mode */
+		uint32_t k:1;	     /* Count in kernel mode */
+		uint32_t ex:1;
+				     /* Count in exception context */
+#else
+		uint32_t ex:1;
+		uint32_t k:1;
+		uint32_t s:1;
+		uint32_t u:1;
+		uint32_t ie:1;
+		uint32_t event:10;
+		uint32_t reserved_11_29:15;
+		uint32_t w:1;
+		uint32_t m:1;
+#endif
+	} s;
+};
+
+#endif /* __CVMX_CORE_H__ */
-- 
1.7.0.4

