From 12ce05114f88eb3cea575cdd9d4b9bb783ddf3cf Mon Sep 17 00:00:00 2001
From: David Daney <david.daney@cavium.com>
Date: Fri, 18 Jul 2014 18:05:50 -0700
Subject: [PATCH 671/974] MIPS: OCTEON: Update S.E. to r104679

Signed-off-by: David Daney <david.daney@cavium.com>
[Original patch taken from Cavium SDK 3.1.2-568]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
---
 arch/mips/cavium-octeon/executive/cvmx-bootmem.c   |  24 +-
 .../executive/cvmx-global-resources.c              |  14 +-
 .../mips/cavium-octeon/executive/cvmx-helper-bgx.c | 337 +++++++--------------
 .../cavium-octeon/executive/cvmx-helper-board.c    |   2 +
 .../cavium-octeon/executive/cvmx-helper-pko3.c     |  10 +-
 .../cavium-octeon/executive/cvmx-helper-util.c     | 154 +++++++---
 .../cavium-octeon/executive/cvmx-pki-resources.c   |  84 +++--
 arch/mips/cavium-octeon/executive/octeon-feature.c |   2 +-
 arch/mips/include/asm/octeon/cvmx-coremask.h       |  40 ++-
 arch/mips/include/asm/octeon/cvmx-fpa.h            |   9 +-
 arch/mips/include/asm/octeon/cvmx-fpa1.h           |   4 +-
 arch/mips/include/asm/octeon/cvmx-fpa3.h           |  91 ++++--
 arch/mips/include/asm/octeon/cvmx-helper-fpa.h     |   2 -
 arch/mips/include/asm/octeon/cvmx-helper-util.h    |  44 ++-
 arch/mips/include/asm/octeon/cvmx-hwpko.h          |   3 +-
 arch/mips/include/asm/octeon/cvmx-pki.h            |  16 +-
 arch/mips/include/asm/octeon/cvmx-pow.h            |   2 +
 arch/mips/include/asm/octeon/octeon-feature.h      |   6 +
 18 files changed, 471 insertions(+), 373 deletions(-)

diff --git a/arch/mips/cavium-octeon/executive/cvmx-bootmem.c b/arch/mips/cavium-octeon/executive/cvmx-bootmem.c
index 5d2c94a..86b61db 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-bootmem.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-bootmem.c
@@ -43,7 +43,7 @@
  * Simple allocate only memory allocator.  Used to allocate memory at
  * application start time.
  *
- * <hr>$Revision: 103461 $<hr>
+ * <hr>$Revision: 104081 $<hr>
  *
  */
 
@@ -1377,7 +1377,8 @@ int64_t cvmx_bootmem_phy_mem_list_init_multi(uint8_t node_mask,
 	uint64_t mem_size;
 	int64_t addr;
 	int i;
-	uint64_t node;	/* Make u64 to reduce type casting */
+	int node;
+	uint64_t node_base;	/* Make u64 to reduce type casting */
 
 #if defined(CVMX_BUILD_FOR_UBOOT)
 	mem_sizes[0] = gd->ram_size / (1024 * 1024);
@@ -1395,9 +1396,8 @@ int64_t cvmx_bootmem_phy_mem_list_init_multi(uint8_t node_mask,
 		cvmx_dprintf("ERROR: no memory for cvmx_bootmem descriptor provided\n");
 		return 0;
 	}
-	for (node = 0; node < CVMX_MAX_NODES; node++ ) {
-		if ((node_mask & (1 << node)) &&
-		    (mem_sizes[node] * (1024*1024)) > OCTEON_MAX_PHY_MEM_SIZE) {
+	cvmx_coremask_for_each_node(node, node_mask) {
+		if ((mem_sizes[node] * (1024*1024)) > OCTEON_MAX_PHY_MEM_SIZE) {
 			mem_sizes[node] = OCTEON_MAX_PHY_MEM_SIZE / (1024*1024);
 			cvmx_dprintf("ERROR node#%lld: requested memory size too large, truncating to maximum size\n",
 				     CAST_ULL(node));
@@ -1423,10 +1423,7 @@ int64_t cvmx_bootmem_phy_mem_list_init_multi(uint8_t node_mask,
 	CVMX_BOOTMEM_DESC_SET_FIELD(app_data_addr, 0);
 	CVMX_BOOTMEM_DESC_SET_FIELD(app_data_size, 0);
 
-	for (node = 0; node < CVMX_MAX_NODES; node++ ) {
-		if (!(node_mask & (1 << node)))
-			continue;	/* skip undetected nodes */
-
+	cvmx_coremask_for_each_node(node, node_mask) {
 		if (node != 0)	/* do not reserve memory on remote nodes */
 			low_reserved_bytes = 0;
 		mem_size = (uint64_t)mem_sizes[node] * (1024*1024);	/* MBytes */
@@ -1435,8 +1432,9 @@ int64_t cvmx_bootmem_phy_mem_list_init_multi(uint8_t node_mask,
 		* vectors, space for global descriptor
 		*/
 
+		node_base = (uint64_t)node << CVMX_NODE_MEM_SHIFT;
 		cur_block_addr = (OCTEON_DDR0_BASE + low_reserved_bytes) |
-					(node << CVMX_NODE_MEM_SHIFT);
+				  node_base;
 
 		if (mem_size <= OCTEON_DDR0_SIZE) {
 			__cvmx_bootmem_phy_free(cur_block_addr,
@@ -1454,14 +1452,14 @@ int64_t cvmx_bootmem_phy_mem_list_init_multi(uint8_t node_mask,
 		/* Add DDR2 block next if present */
 		if (mem_size > OCTEON_DDR1_SIZE) {
 			__cvmx_bootmem_phy_free(OCTEON_DDR1_BASE |
-						(node << CVMX_NODE_MEM_SHIFT),
+						node_base,
 						OCTEON_DDR1_SIZE, 0);
 			__cvmx_bootmem_phy_free(OCTEON_DDR2_BASE |
-						(node << CVMX_NODE_MEM_SHIFT),
+						node_base,
 						mem_size - OCTEON_DDR1_SIZE, 0);
 		} else {
 			__cvmx_bootmem_phy_free(OCTEON_DDR1_BASE |
-						(node << CVMX_NODE_MEM_SHIFT),
+						node_base,
 						mem_size, 0);
 		}
 	}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-global-resources.c b/arch/mips/cavium-octeon/executive/cvmx-global-resources.c
index 153730f..65f5581 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-global-resources.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-global-resources.c
@@ -248,7 +248,7 @@ static uint64_t __cvmx_global_resources_init(void)
 						      CVMX_BOOTMEM_FLAG_NO_LOCKING);
 	if (!block_desc) {
 		if (dbg)
-			cvmx_dprintf("%s: allocating global resources\n", __FUNCTION__);
+			cvmx_dprintf("%s: allocating global resources\n", __func__);
 
 		tmp_phys = cvmx_bootmem_phy_named_block_alloc(sz, 0, 0, CVMX_CACHE_LINE_SIZE,
 							      CVMX_GLOBAL_RESOURCES_DATA_NAME,
@@ -260,7 +260,7 @@ static uint64_t __cvmx_global_resources_init(void)
 		__cvmx_global_resources_addr = (uint64_t) tmp_phys;
 
 		if (dbg)
-			cvmx_dprintf("%s: memset global resources %llu\n", __FUNCTION__,
+			cvmx_dprintf("%s: memset global resources %llu\n", __func__,
 				     CAST_ULL(__cvmx_global_resources_addr));
 
 		base = (1ull << 63) | __cvmx_global_resources_addr;
@@ -270,7 +270,7 @@ static uint64_t __cvmx_global_resources_init(void)
 		}
 	} else {
 		if (dbg)
-			cvmx_dprintf("%s:found global resource\n", __FUNCTION__);
+			cvmx_dprintf("%s:found global resource\n", __func__);
 		__cvmx_global_resources_addr = block_desc->base_addr;
 	}
  end:
@@ -302,7 +302,7 @@ uint64_t cvmx_get_global_resource(struct global_resource_tag tag, int no_lock)
 
 		if (tag_lo == tag.lo && tag_hi == tag.hi) {
 			if (dbg)
-				cvmx_dprintf("%s: Found global resource entry\n", __FUNCTION__);
+				cvmx_dprintf("%s: Found global resource entry\n", __func__);
 			break;
 		}
 		entry_cnt--;
@@ -311,7 +311,7 @@ uint64_t cvmx_get_global_resource(struct global_resource_tag tag, int no_lock)
 
 	if (entry_cnt == 0) {
 		if (dbg)
-			cvmx_dprintf("%s: no matching global resource entry found\n", __FUNCTION__);
+			cvmx_dprintf("%s: no matching global resource entry found\n", __func__);
 		if (!no_lock)
 			__cvmx_global_resource_unlock();
 		return 0;
@@ -531,7 +531,7 @@ int free_global_resources(void)
 		/* free the resource */
 		rc = __cvmx_bootmem_phy_free(phys_addr, size, 0);
 		if (!rc) {
-			cvmx_dprintf("ERROR: %s: could not free memory to bootmem\n", __FUNCTION__);
+			cvmx_dprintf("ERROR: %s: could not free memory to bootmem\n", __func__);
 		}
 	}
 
@@ -571,7 +571,7 @@ void cvmx_global_resources_show(void)
 	memset (tagname, 0, MAX_RESOURCE_TAG_LEN + 1);
 
 	if (dbg)
-		cvmx_dprintf("%s: cvmx-global-resources: \n",__FUNCTION__);
+		cvmx_dprintf("%s: cvmx-global-resources: \n", __func__);
 	for (count = 0; count < entry_cnt; count++) {
 		p = CVMX_GET_RESOURCE_ENTRY(count);
 		phys_addr = CVMX_RESOURCE_ENTRY_GET_FIELD(p, phys_addr);
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-bgx.c b/arch/mips/cavium-octeon/executive/cvmx-helper-bgx.c
index 6d8bca1..29fd6e3 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-bgx.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-bgx.c
@@ -126,7 +126,7 @@ void cvmx_helper_bgx_disable(int xipd_port)
  *
  * @param mode      Mode to configure the bgx mac as
  */
-static void __cvmx_bgx_common_init(int xiface)
+static void __cvmx_helper_bgx_common_init(int xiface)
 {
 	cvmx_bgxx_cmrx_config_t	cmr_config;
 	cvmx_bgxx_cmr_rx_lmacs_t bgx_cmr_rx_lmacs;
@@ -191,44 +191,38 @@ static void __cvmx_bgx_common_init(int xiface)
 
 }
 
-static void __cvmx_bgx_common_init_pknd(int xiface)
+static void __cvmx_helper_bgx_common_init_pknd(int xiface, int index)
 {
-	int num_ports;
-	int index;
+	cvmx_bgxx_cmrx_rx_bp_on_t bgx_rx_bp_on;
+	cvmx_bgxx_cmrx_rx_id_map_t cmr_rx_id_map;
+        cvmx_bgxx_cmr_chan_msk_and_t chan_msk_and;
+	cvmx_bgxx_cmr_chan_msk_or_t chan_msk_or;
 	int num_chl = 16; /*modify it to 64 for xlaui and xaui*/
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
 	int interface = xi.interface;
 	int node = xi.node;
-	cvmx_bgxx_cmrx_rx_bp_on_t bgx_rx_bp_on;
-
-	num_ports = cvmx_helper_ports_on_interface(xiface);
+	int pknd = cvmx_helper_get_pknd(xiface, index);
+	int num_ports = cvmx_helper_ports_on_interface(xiface);
 
 	/* Modify bp_on mark, depending on number of LMACS on that interface
 	and write it for every port */
 	bgx_rx_bp_on.u64 = 0;
 	bgx_rx_bp_on.s.mark = (CVMX_BGX_RX_FIFO_SIZE / (num_ports * 4 * 16));
 
-	for (index = 0; index < num_ports; index++) {
-		/* Setup pkind */
-		int pknd = cvmx_helper_get_pknd(xiface, index);
-		cvmx_bgxx_cmrx_rx_id_map_t cmr_rx_id_map;
-                cvmx_bgxx_cmr_chan_msk_and_t chan_msk_and;
-	        cvmx_bgxx_cmr_chan_msk_or_t chan_msk_or;
-
-		cmr_rx_id_map.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_RX_ID_MAP(index, interface));
-		cmr_rx_id_map.s.pknd = pknd;
-		cvmx_write_csr_node(node, CVMX_BGXX_CMRX_RX_ID_MAP(index, interface),
+	/* Setup pkind */
+	cmr_rx_id_map.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_RX_ID_MAP(index, interface));
+	cmr_rx_id_map.s.pknd = pknd;
+	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_RX_ID_MAP(index, interface),
 			       cmr_rx_id_map.u64);
-                 /* Set backpressure channel mask AND/OR registers */
-                chan_msk_and.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMR_CHAN_MSK_AND(interface));
-                chan_msk_or.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMR_CHAN_MSK_OR(interface));
-                chan_msk_and.s.msk_and |= ((1 << num_chl) - 1) << (16 * index);
-                chan_msk_or.s.msk_or |= ((1 << num_chl) - 1) << (16 * index);
-                cvmx_write_csr_node(node, CVMX_BGXX_CMR_CHAN_MSK_AND(interface), chan_msk_and.u64);
-                cvmx_write_csr_node(node, CVMX_BGXX_CMR_CHAN_MSK_OR(interface), chan_msk_or.u64);
-		/* set rx back pressure (bp_on) on value */
-		cvmx_write_csr_node(node, CVMX_BGXX_CMRX_RX_BP_ON(index, interface), bgx_rx_bp_on.u64);
-	}
+        /* Set backpressure channel mask AND/OR registers */
+        chan_msk_and.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMR_CHAN_MSK_AND(interface));
+        chan_msk_or.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMR_CHAN_MSK_OR(interface));
+        chan_msk_and.s.msk_and |= ((1 << num_chl) - 1) << (16 * index);
+        chan_msk_or.s.msk_or |= ((1 << num_chl) - 1) << (16 * index);
+        cvmx_write_csr_node(node, CVMX_BGXX_CMR_CHAN_MSK_AND(interface), chan_msk_and.u64);
+        cvmx_write_csr_node(node, CVMX_BGXX_CMR_CHAN_MSK_OR(interface), chan_msk_or.u64);
+	/* set rx back pressure (bp_on) on value */
+	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_RX_BP_ON(index, interface), bgx_rx_bp_on.u64);
 }
 
 /**
@@ -243,7 +237,7 @@ static void __cvmx_bgx_common_init_pknd(int xiface)
  */
 int __cvmx_helper_bgx_probe(int xiface)
 {
-	__cvmx_bgx_common_init(xiface);	
+	__cvmx_helper_bgx_common_init(xiface);	
 	return __cvmx_helper_bgx_enumerate(xiface);
 }
 
@@ -256,31 +250,68 @@ int __cvmx_helper_bgx_probe(int xiface)
  *
  * @return Zero on success, negative on failure
  */
-static int __cvmx_helper_bgx_sgmii_hardware_init_one_time(int xiface, int index)
+static int __cvmx_helper_bgx_sgmii_init(int xiface, int index)
 {
+	cvmx_bgxx_gmp_pcs_mrx_control_t gmp_control;
+	cvmx_bgxx_gmp_pcs_miscx_ctl_t gmp_misc_ctl;
+	cvmx_bgxx_gmp_pcs_linkx_timer_t gmp_timer;
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
 	int interface = xi.interface;
 	int node = xi.node;
 	const uint64_t clock_mhz = cvmx_clock_get_rate_node(node, CVMX_CLOCK_SCLK) / 1000000;
-	cvmx_bgxx_gmp_pcs_miscx_ctl_t gmp_misc_ctl;
-	cvmx_bgxx_gmp_pcs_linkx_timer_t gmp_timer;
+	int phy_mode, mode_1000x;
 
 	if (!cvmx_helper_is_port_valid(interface, index))
 		return 0;
 
-	/*
-	 * Write PCS*_LINK*_TIMER_COUNT_REG[COUNT] with the
-	 * appropriate value. 1000BASE-X specifies a 10ms
-	 * interval. SGMII specifies a 1.6ms interval.
-	 */
+	/* Take PCS through a reset sequence */
+	if (cvmx_sysinfo_get()->board_type != CVMX_BOARD_TYPE_SIM) {
+		gmp_control.s.reset = 1;
+		cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface),
+		       					     gmp_control.u64);
+
+		/* Wait until GMP_PCS_MRX_CONTROL[reset] comes out of reset */
+		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface),
+				cvmx_bgxx_gmp_pcs_mrx_control_t, reset, ==, 0, 10000)) {
+			cvmx_dprintf("SGMII%d: Timeout waiting for port %d to finish reset\n", interface, index);
+			return -1;
+		}
+	}
+
+	/* Write GMP_PCS_MR*_CONTROL[RST_AN]=1 to ensure a fresh SGMII
+	   negotiation starts. */
+	gmp_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface));
+	gmp_control.s.rst_an = 1;
+	gmp_control.s.an_en = 1;
+	gmp_control.s.pwr_dn = 0;
+	cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface),
+		       gmp_control.u64);
+
+	phy_mode = cvmx_helper_get_mac_phy_mode(xiface, index);
+	mode_1000x = cvmx_helper_get_1000x_mode(xiface, index);
+
 	gmp_misc_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface));
-	/* Adjust the MAC mode if requested by device tree */
-	gmp_misc_ctl.s.mac_phy =
-		cvmx_helper_get_mac_phy_mode(xiface, index);
-	gmp_misc_ctl.s.mode =
-		cvmx_helper_get_1000x_mode(xiface, index);
+	gmp_misc_ctl.s.mac_phy = phy_mode;
+	gmp_misc_ctl.s.mode = mode_1000x;
 	cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface), gmp_misc_ctl.u64);
 
+	if (phy_mode)
+		/* In PHY mode we can't query the link status so we just
+		   assume that the link is up */
+		return 0;
+
+	/* Wait for GMP_PCS_MRX_CONTROL[an_cpt] to be set, indicating that
+	   SGMII autonegotiation is complete. In MAC mode this isn't an
+	   ethernet link, but a link between OCTEON and PHY. */
+
+	if ((cvmx_sysinfo_get()->board_type != CVMX_BOARD_TYPE_SIM) &&
+	     CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_GMP_PCS_MRX_STATUS(index, interface),
+				   cvmx_bgxx_gmp_pcs_mrx_status_t, an_cpt,
+				   ==, 1, 10000)) {
+		cvmx_dprintf("SGMII%d: Port %d link timeout\n", interface, index);
+		return -1;
+	}
+
 	gmp_timer.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_LINKX_TIMER(index, interface));
 	if (gmp_misc_ctl.s.mode)
 		/* 1000BASE-X */
@@ -288,7 +319,6 @@ static int __cvmx_helper_bgx_sgmii_hardware_init_one_time(int xiface, int index)
 	else
 		/* SGMII */
 		gmp_timer.s.count = (1600ull * clock_mhz) >> 10;
-
 	cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_LINKX_TIMER(index, interface), gmp_timer.u64);
 
 	/*
@@ -327,59 +357,6 @@ static int __cvmx_helper_bgx_sgmii_hardware_init_one_time(int xiface, int index)
 
 /**
  * @INTERNAL
- * Bring up the SGMII interface to be ready for packet I/O but
- * leave I/O disabled using the GMX override. This function
- * follows the bringup documented in 10.6.3 of the manual.
- *
- * @param interface Interface to bringup
- * @param num_ports Number of ports on the interface
- *
- * @return Zero on success, negative on failure
- */
-static int __cvmx_helper_bgx_sgmii_hardware_init(int xiface, int num_ports)
-{
-	int index;
-	int do_link_set = 1;
-	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
-	int interface = xi.interface;
-	int node = xi.node;
-
-	__cvmx_bgx_common_init(xiface);
-	__cvmx_bgx_common_init_pknd(xiface);
-
-	for (index = 0; index < num_ports; index++) {
-		int xipd_port = cvmx_helper_get_ipd_port(xiface, index);
-		cvmx_bgxx_gmp_gmi_txx_thresh_t gmi_tx_thresh;
-
-		if (!cvmx_helper_is_port_valid(xiface, index))
-			continue;
-		__cvmx_helper_bgx_sgmii_hardware_init_one_time(xiface, index);
-
-		/* Set TX Threshold */
-		gmi_tx_thresh.u64 = 0;
-		gmi_tx_thresh.s.cnt = 0x20;
-		cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_THRESH(index, interface),
-					gmi_tx_thresh.u64);
-#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
-		/*
-		 * Linux kernel driver will call ....link_set with the
-		 * proper link state. In the simulator there is no
-		 * link state polling and hence it is set from
-		 * here.
-		 */
-		if (!(cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_SIM))
-			do_link_set = 0;
-#endif
-		if (do_link_set)
-			__cvmx_helper_bgx_sgmii_link_set(xipd_port,
-					__cvmx_helper_bgx_sgmii_link_get(xipd_port));
-	}
-
-	return 0;
-}
-
-/**
- * @INTERNAL
  * Bringup and enable a SGMII interface. After this call packet
  * I/O should be fully functional. This is called with IPD
  * enabled but PKO disabled. This is used by interfaces using
@@ -389,116 +366,56 @@ static int __cvmx_helper_bgx_sgmii_hardware_init(int xiface, int num_ports)
  *
  * @return Zero on success, negative on failure
  */
-int __cvmx_helper_bgx_sgmii_enable(int xiface)
+int __cvmx_helper_bgx_sgmii_enable_port(int xiface, int index)
 {
-	cvmx_bgxx_cmrx_config_t cmr_config;
 	cvmx_bgxx_gmp_gmi_txx_append_t gmp_txx_append;
 	cvmx_bgxx_gmp_gmi_txx_sgmii_ctl_t gmp_sgmii_ctl;
-	int num_ports;
-	int index;
+	cvmx_bgxx_gmp_gmi_txx_thresh_t gmi_tx_thresh;
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
 	int interface = xi.interface;
 	int node = xi.node;
 
-	num_ports = cvmx_helper_ports_on_interface(xiface);
-
-	__cvmx_helper_bgx_sgmii_hardware_init(xiface, num_ports);
+	__cvmx_helper_bgx_common_init_pknd(xiface, index);
 
-	for (index = 0; index < num_ports; index++) {
-		gmp_txx_append.u64 = cvmx_read_csr_node(node,
+	/* Set TX Threshold */
+	gmp_txx_append.u64 = cvmx_read_csr_node(node,
 					CVMX_BGXX_GMP_GMI_TXX_APPEND(index, interface));
-#if 0
-		/* FCS/PAD options are set in cvmx_helper_bgx_tx_options() */
-		gmp_txx_append.s.fcs = 0;
-		gmp_txx_append.s.pad = 0;
-		cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_APPEND(index, interface), gmp_txx_append.u64);
-#endif
-
-		gmp_sgmii_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_SGMII_CTL(index, interface));
-		gmp_sgmii_ctl.s.align = gmp_txx_append.s.preamble ? 0 : 1;
-		cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_SGMII_CTL(index, interface),
+	gmi_tx_thresh.u64 = 0;
+	gmi_tx_thresh.s.cnt = 0x20;
+	cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_THRESH(index, interface),
+			    gmi_tx_thresh.u64);
+
+	gmp_sgmii_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_SGMII_CTL(index, interface));
+	gmp_sgmii_ctl.s.align = gmp_txx_append.s.preamble ? 0 : 1;
+	cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_SGMII_CTL(index, interface),
 				gmp_sgmii_ctl.u64);
 
-		cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface));
-		cmr_config.s.enable = 1;
-		cmr_config.s.data_pkt_tx_en = 1;
-		cmr_config.s.data_pkt_rx_en = 1;
-		cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
-	}
 
 	return 0;
 }
 
-/**
- * @INTERNAL
- * Initialize the SERTES link for the first time or after a loss
- * of link.
- *
- * @param interface Interface to init
- * @param index     Index of prot on the interface
- *
- * @return Zero on success, negative on failure
- */
-static int __cvmx_helper_bgx_sgmii_hardware_init_link(int xiface, int index)
+int __cvmx_helper_bgx_sgmii_enable(int xiface)
 {
-	cvmx_bgxx_gmp_pcs_mrx_control_t gmp_control;
-	cvmx_bgxx_gmp_pcs_miscx_ctl_t gmp_misc_ctl;
-	int phy_mode, mode_1000x;
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
 	int interface = xi.interface;
 	int node = xi.node;
+	int num_ports;
+	int index;
 
-	if (!cvmx_helper_is_port_valid(xiface, index))
-		return 0;
-
-	gmp_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface));
-	/* Take PCS through a reset sequence */
-	if (cvmx_sysinfo_get()->board_type != CVMX_BOARD_TYPE_SIM) {
-		gmp_control.s.reset = 1;
-		cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface),
-		       					     gmp_control.u64);
-
-		/* Wait until GMP_PCS_MRX_CONTROL[reset] comes out of reset */
-		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface),
-				cvmx_bgxx_gmp_pcs_mrx_control_t, reset, ==, 0, 10000)) {
-			cvmx_dprintf("SGMII%d: Timeout waiting for port %d to finish reset\n", interface, index);
-			return -1;
-		}
-	}
-
-	/* Write GMP_PCS_MR*_CONTROL[RST_AN]=1 to ensure a fresh SGMII
-	   negotiation starts. */
-	gmp_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface));
-	gmp_control.s.rst_an = 1;
-	gmp_control.s.an_en = 1;
-	gmp_control.s.pwr_dn = 0;
-	cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface),
-		       gmp_control.u64);
-
-
-	phy_mode = cvmx_helper_get_mac_phy_mode(xiface, index);
-	mode_1000x = cvmx_helper_get_1000x_mode(xiface, index);
-
-	gmp_misc_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface));
-	gmp_misc_ctl.s.mac_phy = phy_mode;
-	gmp_misc_ctl.s.mode = mode_1000x;
-	cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface), gmp_misc_ctl.u64);
+	num_ports = cvmx_helper_ports_on_interface(xiface);
+	__cvmx_helper_bgx_common_init(xiface);
 
-	if (phy_mode)
-		/* In PHY mode we can't query the link status so we just
-		   assume that the link is up */
-		return 0;
+	for (index = 0; index < num_ports; index++) {
+		cvmx_bgxx_cmrx_config_t cmr_config;
 
-	/* Wait for GMP_PCS_MRX_CONTROL[an_cpt] to be set, indicating that
-	   SGMII autonegotiation is complete. In MAC mode this isn't an
-	   ethernet link, but a link between OCTEON and PHY. */
+		__cvmx_helper_bgx_sgmii_init(xiface, index);
+		__cvmx_helper_bgx_sgmii_enable_port(xiface, index);
 
-	if ((cvmx_sysinfo_get()->board_type != CVMX_BOARD_TYPE_SIM) &&
-	     CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_GMP_PCS_MRX_STATUS(index, interface),
-				   cvmx_bgxx_gmp_pcs_mrx_status_t, an_cpt,
-				   ==, 1, 10000)) {
-		cvmx_dprintf("SGMII%d: Port %d link timeout\n", interface, index);
-		return -1;
+		cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface));
+		cmr_config.s.enable = 1;
+		cmr_config.s.data_pkt_tx_en = 1;
+		cmr_config.s.data_pkt_rx_en = 1;
+		cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
 	}
 
 	return 0;
@@ -515,9 +432,8 @@ static int __cvmx_helper_bgx_sgmii_hardware_init_link(int xiface, int index)
  *
  * @return Zero on success, negative on failure
  */
-static int __cvmx_helper_bgx_sgmii_hardware_init_link_speed(int xiface,
-							    int index,
-							    cvmx_helper_link_info_t link_info)
+static int __cvmx_helper_bgx_sgmii_speed(int xiface, int index,
+					 cvmx_helper_link_info_t link_info)
 {
 	int is_enabled;
 	cvmx_bgxx_cmrx_config_t cmr_config;
@@ -642,6 +558,7 @@ cvmx_helper_link_info_t __cvmx_helper_bgx_sgmii_link_get(int xipd_port)
 	int node = xi.node;
 	int index = cvmx_helper_get_interface_index_num(xp.port);
 	int speed = 1000;
+	int qlm = cvmx_qlm_interface(xiface);
 
 	result.u64 = 0;
 
@@ -656,7 +573,7 @@ cvmx_helper_link_info_t __cvmx_helper_bgx_sgmii_link_get(int xipd_port)
 		return result;
 	}
 
-	speed = cvmx_qlm_get_gbaud_mhz(0) * 8 / 10;
+	speed = cvmx_qlm_get_gbaud_mhz(qlm) * 8 / 10;
 
 	gmp_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface));
 	if (gmp_control.s.loopbck1) {
@@ -700,50 +617,18 @@ cvmx_helper_link_info_t __cvmx_helper_bgx_sgmii_link_get(int xipd_port)
 int __cvmx_helper_bgx_sgmii_link_set(int xipd_port,
 				 cvmx_helper_link_info_t link_info)
 {
-	cvmx_bgxx_cmrx_config_t cmr_config;
 	int xiface = cvmx_helper_get_interface_num(xipd_port);
-	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
 	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(xipd_port);
-	int interface = xi.interface;
-	int node = xi.node;
 	int index = cvmx_helper_get_interface_index_num(xp.port);
+	int status;
 
 	if (!cvmx_helper_is_port_valid(xiface, index))
 		return 0;
 
-	cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface));
-	if (link_info.s.link_up) {
-		cmr_config.s.enable = 1;
-		cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
-
-		__cvmx_helper_bgx_sgmii_hardware_init_link(xiface, index);
-	} else {
-		cvmx_bgxx_gmp_pcs_miscx_ctl_t gmp_misc_ctl;
-
-		cmr_config.s.enable = 0;
-		cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
-		gmp_misc_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface));
-
-		/* Disable autonegotiation only when MAC mode. */
-		if (gmp_misc_ctl.s.mac_phy == 0) {
-			cvmx_bgxx_gmp_pcs_mrx_control_t gmp_control;
-
-			gmp_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface));
-			gmp_control.s.an_en = 0;
-			cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface), gmp_control.u64);
-			cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface));
-		}
-		/*
-		 * Use GMXENO to force the link down it will get
-		 * reenabled later...
-		 */
-		gmp_misc_ctl.s.gmxeno = 1;
-		cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface),
-			       gmp_misc_ctl.u64);
-		cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface));
-		return 0;
-	}
-	return __cvmx_helper_bgx_sgmii_hardware_init_link_speed(xiface, index, link_info);
+	status = __cvmx_helper_bgx_sgmii_init(xiface, index);
+	if (status == 0)
+		return __cvmx_helper_bgx_sgmii_speed(xiface, index, link_info);
+	return 0;
 }
 
 /**
@@ -784,7 +669,7 @@ int __cvmx_helper_bgx_sgmii_configure_loopback(int xipd_port, int enable_interna
 	gmp_misc_ctl.s.loopbck2 = enable_external;
 	cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface), gmp_misc_ctl.u64);
 
-	__cvmx_helper_bgx_sgmii_hardware_init_link(xiface, index);
+	__cvmx_helper_bgx_sgmii_init(xiface, index);
 
 	return 0;
 }
@@ -1138,11 +1023,11 @@ int __cvmx_helper_bgx_xaui_enable(int xiface)
 
 	mode = cvmx_helper_interface_get_mode(xiface);
 
-	__cvmx_bgx_common_init(xiface);
-	__cvmx_bgx_common_init_pknd(xiface);
+	__cvmx_helper_bgx_common_init(xiface);
 
 	for (index = 0; index < num_ports; index++) {
 		int res = __cvmx_helper_bgx_xaui_init(index, xiface);
+		__cvmx_helper_bgx_common_init_pknd(xiface, index);
 		if (res == -1) {
 			cvmx_dprintf("Failed to enable XAUI for BGX(%d,%d)\n", interface, index);
 			return res;
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-board.c b/arch/mips/cavium-octeon/executive/cvmx-helper-board.c
index a607c4a..364fbc9 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-board.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-board.c
@@ -798,6 +798,7 @@ int __cvmx_helper_parse_78xx_bgx_dt(void *fdt_addr)
 					     fdt_get_name(fdt_addr,
 							  fdt_phy_node, NULL));
 			}
+			cvmx_helper_set_port_phy_present(xiface, port_index, true);
 
 		} else {
 			cvmx_helper_set_phy_fdt_node_offset(xiface, port_index,
@@ -805,6 +806,7 @@ int __cvmx_helper_parse_78xx_bgx_dt(void *fdt_addr)
 			if (dbg)
 				cvmx_dprintf("%s: No PHY fdt node offset for interface 0x%x, port %d to %d\n",
 					     __func__, xiface, port_index, fdt_phy_node);
+			cvmx_helper_set_port_phy_present(xiface, port_index, false);
 		}
 	}
 	return 0;
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-pko3.c b/arch/mips/cavium-octeon/executive/cvmx-helper-pko3.c
index 28ec36e..a2bab6c 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-pko3.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-pko3.c
@@ -72,7 +72,7 @@
 
 /* bootloader has limited memory, not much traffic */
 #if defined(__U_BOOT__)
-#define CVMX_PKO3_POOL_BUFFERS (1024*4+1024)
+#define CVMX_PKO3_POOL_BUFFERS (1024)
 #endif
 
 /*
@@ -90,7 +90,7 @@
 /* Number of command words per buffer */
 #define _NUMW	(4*1024/8)
 /* Assume 8 concurrently active, fully loaded queues */
-#define _NUMQ	8
+#define _NUMQ	1024
 /* Assume worst case of 16 words per command per DQ */
 #define _DQ_SZ	16
 /* Combine the above guesstimates into the total buffer count */
@@ -582,13 +582,17 @@ int __cvmx_pko3_config_gen_interface(int xiface, uint8_t subif,
 	cvmx_xiface_t xi = cvmx_helper_xiface_to_node_interface(xiface);
 	int static_pri;
 
+#if defined(__U_BOOT__)
+	num_queues = 1;
+#endif
+
 	if (num_queues == 0) {
 		num_queues = 1;
 		cvmx_dprintf("%s: WARNING xiface %#x misconfigured\n",
 			__func__, xiface);
 	}
 
-	if(debug)
+	if (debug)
 		cvmx_dprintf("%s: configuring xiface %u:%u/%u nq=%u %s\n",
 			     __FUNCTION__, xi.node, xi.interface, subif,
 			    num_queues, (prioritized)?"qos":"fair");
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-util.c b/arch/mips/cavium-octeon/executive/cvmx-helper-util.c
index f06198a..025f397 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-util.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-util.c
@@ -363,6 +363,73 @@ static int __cvmx_ipd_mode_no_wptr(void)
 	return 0;
 }
 
+static CVMX_TLS cvmx_buf_ptr_t __cvmx_packet_short_ptr[4];
+static CVMX_TLS uint8_t __cvmx_wqe_pool;
+
+/**
+ * @INTERNAL
+ * Prepare packet pointer templace for dynamic short
+ * packets.
+ */
+static void cvmx_packet_short_ptr_calculate(void)
+{
+	unsigned i, off;
+	union cvmx_pip_gbl_cfg pip_gbl_cfg;
+	union cvmx_pip_ip_offset pip_ip_offset;
+
+	/* Fill in the common values for all cases */
+	for(i = 0; i < 4; i++) {
+		if (__cvmx_ipd_mode_no_wptr())
+			/* packet pool, set to 0 in hardware */
+			__cvmx_wqe_pool = 0;
+		else 
+			/* WQE pool as configured */
+			__cvmx_wqe_pool =
+				cvmx_read_csr(CVMX_IPD_WQE_FPA_QUEUE) & 7;
+
+		__cvmx_packet_short_ptr[i].s.pool = __cvmx_wqe_pool;
+		__cvmx_packet_short_ptr[i].s.size =
+			cvmx_fpa_get_block_size(__cvmx_wqe_pool);
+		__cvmx_packet_short_ptr[i].s.size -= 32;
+		__cvmx_packet_short_ptr[i].s.addr = 32;
+	}
+
+	pip_gbl_cfg.u64 = cvmx_read_csr(CVMX_PIP_GBL_CFG);
+	pip_ip_offset.u64 = cvmx_read_csr(CVMX_PIP_IP_OFFSET);
+
+	/* RAW_FULL: index = 0 */
+	i = 0;
+	off = pip_gbl_cfg.s.raw_shf;
+	__cvmx_packet_short_ptr[i].s.addr += off;
+	__cvmx_packet_short_ptr[i].s.size -= off;
+	__cvmx_packet_short_ptr[i].s.back += off >> 7;
+
+	/* NON-IP: index = 1 */
+	i = 1;
+	off = pip_gbl_cfg.s.nip_shf;
+	__cvmx_packet_short_ptr[i].s.addr += off;
+	__cvmx_packet_short_ptr[i].s.size -= off;
+	__cvmx_packet_short_ptr[i].s.back += off >> 7;
+
+	/* IPv4: index = 2 */
+	i = 2;
+	off = (pip_ip_offset.s.offset << 3) + 4;
+	__cvmx_packet_short_ptr[i].s.addr += off;
+	__cvmx_packet_short_ptr[i].s.size -= off;
+	__cvmx_packet_short_ptr[i].s.back += off >> 7;
+
+	/* IPv6: index = 3 */
+	i = 3;
+	off = (pip_ip_offset.s.offset << 3) + 0;
+	__cvmx_packet_short_ptr[i].s.addr += off;
+	__cvmx_packet_short_ptr[i].s.size -= off;
+	__cvmx_packet_short_ptr[i].s.back += off >> 7;
+
+	/* For IPv4/IPv6: subtract work->word2.s.ip_offset
+	 * to addr, if it is smaller than IP_OFFSET[OFFSET]*8
+	 * which is stored in __cvmx_packet_short_ptr[3].s.addr
+	 */
+}
 
 /**
  * Extract packet data buffer pointer from work queue entry.
@@ -440,55 +507,52 @@ cvmx_buf_ptr_t cvmx_wqe_get_packet_ptr(cvmx_wqe_t *work)
 		return optr;
 
 	} else {
+		unsigned i;
+		unsigned off = 0;
 		cvmx_buf_ptr_t bptr;
 
 		if (cvmx_likely(work->word2.s.bufs > 0))
 			return work->packet_ptr;
 
-		if (work->word2.s.software)
+		if (cvmx_unlikely(work->word2.s.software))
 			return work->packet_ptr;
 
-		/* data is only in WQE, convert it into a buf_ptr */
-		bptr.u64 = 0;
-		bptr.s.size = cvmx_wqe_get_len(work);
-		bptr.s.addr = cvmx_ptr_to_phys(work) + 32;
-
-		/* For CN68XX it could be NO_WPTR or Dynamic-Short cause */
-		if (__cvmx_ipd_mode_no_wptr()) {
-			/* Packet pool is hardwired to 0 in relevant SoCs */
-			bptr.s.pool = 0;
-		} else {
-			bptr.s.pool = cvmx_read_csr(CVMX_IPD_WQE_FPA_QUEUE) & 7;
+		/* first packet, precalculate packet_ptr templaces */
+		if (cvmx_unlikely(__cvmx_packet_short_ptr[0].u64 == 0)) {
+			cvmx_packet_short_ptr_calculate();
 		}
 
-		/* FIXME- RAWFULL case not handled yet */
 
-		if (work->word2.s_cn38xx.not_IP ||
-		    work->word2.s_cn38xx.rcv_error) {
-			/* Adjust data offset for non-IP packets */
-			union cvmx_pip_gbl_cfg pip_gbl_cfg;
-			pip_gbl_cfg.u64 = cvmx_read_csr(CVMX_PIP_GBL_CFG);
-			bptr.s.addr += pip_gbl_cfg.s.nip_shf;
+		/* calculate templace index */
+		i = work->word2.s_cn38xx.not_IP | work->word2.s_cn38xx.rcv_error;
+		i = 2 ^ (i << 1);
+
+		/* IPv4/IPv6: Adjust IP offset */
+		if (cvmx_likely(i & 2)) {
+			i |= work->word2.s.is_v6;
+			off = work->word2.s.ip_offset;
 		} else {
-			/* Adjust data start address for IP protocols */
-			union cvmx_pip_ip_offset pip_ip_offset;
-			pip_ip_offset.u64 = cvmx_read_csr(CVMX_PIP_IP_OFFSET);
-			bptr.s.addr += (pip_ip_offset.s.offset << 3) -
-				work->word2.s.ip_offset;
-			bptr.s.addr += (work->word2.s.is_v6 ^ 1) << 2;
+			/* RAWFULL/RAWSCHED should be handled here */
+			i = 1;	/* not-IP */
+			off = 0;
 		}
 
-		/* Calculate the "back" offset in 64-bit words */
-		bptr.s.back = (bptr.s.addr -cvmx_ptr_to_phys(work)) >> 7;
+		/* Get the right templace */
+		bptr = __cvmx_packet_short_ptr[i];
+		bptr.s.addr -= off;
+		bptr.s.back = bptr.s.addr >> 7;
 
-		/* Store the new buffer pointer back into WQE */
-		work->packet_ptr = bptr;
+		/* Add actual WQE paddr to the templace offset */
+		bptr.s.addr += cvmx_ptr_to_phys(work);
 
 		/* Adjust word2.bufs so that _free_data() handles it
 		 * in the same way as PKO
 		 */
 		work->word2.s.bufs = 1;
 
+		/* Store the new buffer pointer back into WQE */
+		work->packet_ptr = bptr;
+
 		/* Returned the synthetic buffer_pointer */
 		return bptr;
 	}
@@ -496,8 +560,7 @@ cvmx_buf_ptr_t cvmx_wqe_get_packet_ptr(cvmx_wqe_t *work)
 
 void cvmx_wqe_free(cvmx_wqe_t *work)
 {
-	unsigned ncl = 0;
-	unsigned pool;
+	unsigned ncl = 1;
 	cvmx_wqe_78xx_t * wqe = (void *) work;
 
 	/* Free native untranslated 78xx WQE */
@@ -511,9 +574,10 @@ void cvmx_wqe_free(cvmx_wqe_t *work)
 		if (!bptr.packet_outside_wqe)
 			return;
 	} else {
-		/* determine if packet is inside WQE the old way */
-		if (cvmx_likely(work->word2.s_cn38xx.bufs > 0)) {
-			uint64_t paddr, paddr1;
+		uint64_t paddr, paddr1;
+
+		/* check for unconverted RS */
+		if (cvmx_likely(work->word2.s_cn38xx.bufs != 0)) {
 
 			/* Check if the first data buffer is inside WQE */
 			paddr = (work->packet_ptr.s.addr & (~0x7full)) -
@@ -524,6 +588,9 @@ void cvmx_wqe_free(cvmx_wqe_t *work)
 			if (paddr == paddr1)
 				return;
 		}
+
+		cvmx_fpa1_free(work, __cvmx_wqe_pool, ncl);
+		return;
 	}
 
 	/* At this point it is clear the WQE needs to be freed */
@@ -536,18 +603,6 @@ void cvmx_wqe_free(cvmx_wqe_t *work)
 		/* Only a few words have been touched, not entire buf */
 		ncl = 1;
 		cvmx_fpa3_free(work, aura, ncl);
-	} else {
-		/* Determine FPA pool the WQE buffer belongs to */
-		if (__cvmx_ipd_mode_no_wptr()) {
-			pool = 0 ; /* Hardwired packet FPA pool */
-			ncl = (cvmx_wqe_get_len(work) + 127) >> 7;
-			ncl += 4;
-		} else {
-			pool = cvmx_read_csr(CVMX_IPD_WQE_FPA_QUEUE) & 7;
-			ncl = 1;
-		}
-
-		cvmx_fpa1_free(work, pool, ncl);
 	}
 }
 
@@ -582,11 +637,11 @@ void cvmx_helper_free_packet_data(cvmx_wqe_t *work)
 
 	if (number_buffers == 0)
 		return;
-	cvmx_wqe_pki_errata_20776(work);
 
 	/* Interpret PKI-style bufptr unless it has been translated */
 	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE) &&
 	    !wqe->pki_wqe_translated) {
+		cvmx_wqe_pki_errata_20776(work);
 		bptr.u64 = buffer_ptr.u64;
 		next_buffer_ptr = *(uint64_t *)
 			cvmx_phys_to_ptr(bptr.addr - 8);
@@ -1050,8 +1105,11 @@ int cvmx_helper_get_interface_index_num(int ipd_port)
 
 		if (OCTEON_IS_MODEL(OCTEON_CN68XX))
 			port_map = ipd_port_map_68xx;
-		else if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+		else if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+			struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(ipd_port);
 			port_map = ipd_port_map_78xx;
+			ipd_port = xp.port;
+		}
 		else
 			return -1;
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pki-resources.c b/arch/mips/cavium-octeon/executive/cvmx-pki-resources.c
index 5ab9abd..5138aaf 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pki-resources.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pki-resources.c
@@ -49,6 +49,7 @@
 #include <asm/octeon/cvmx-pki.h>
 #include "asm/octeon/cvmx-global-resources.h"
 #include "asm/octeon/cvmx-range.h"
+#include "asm/octeon/cvmx-atomic.h"
 #else
 #include "cvmx.h"
 #include "cvmx-version.h"
@@ -56,8 +57,12 @@
 #include "cvmx-pki.h"
 #include "cvmx-global-resources.h"
 #include "cvmx-range.h"
+#include "cvmx-atomic.h"
 #endif
 
+static CVMX_SHARED int32_t cvmx_pki_style_refcnt
+	[CVMX_MAX_NODES][CVMX_PKI_NUM_INTERNAL_STYLE];
+
 /**
  * This function allocates/reserves a style from pool of global styles per node.
  * @param node	node to allocate style from.
@@ -73,23 +78,38 @@ int cvmx_pki_style_alloc(int node, int style)
 	int rs;
 
 	if (cvmx_create_global_resource_range(CVMX_GR_TAG_STYLE(node), CVMX_PKI_NUM_INTERNAL_STYLE)) {
-		cvmx_dprintf("ERROR: Failed to create styles global resource\n");
+		cvmx_printf("ERROR: Failed to create styles global resource\n");
 		return -1;
 	}
 	if (style >= 0) {
+		/* Reserving specific style, use refcnt for sharing */
+		rs = cvmx_atomic_fetch_and_add32(
+			&cvmx_pki_style_refcnt[node][style], 1);
+		if (rs > 0)
+			return CVMX_RESOURCE_ALREADY_RESERVED;
+
 		rs = cvmx_reserve_global_resource_range(CVMX_GR_TAG_STYLE(node), style, style, 1);
 		if (rs == -1) {
-			cvmx_dprintf("INFO: style %d is already reserved\n", (int)style);
-			return CVMX_RESOURCE_ALREADY_RESERVED;
+			/* This means the style is taken by another app */
+			cvmx_printf("ERROR: style %d is reserved by another app\n",
+				style);
+			cvmx_atomic_fetch_and_add32(
+				&cvmx_pki_style_refcnt[node][style], -1);
+			return CVMX_RESOURCE_ALLOC_FAILED;
 		}
 	} else {
+		/* Allocate first available style */
 		rs = cvmx_allocate_global_resource_range(CVMX_GR_TAG_STYLE(node), style, 1, 1);
-		if (rs == -1) {
-			cvmx_dprintf("ERROR: Failed to allocate style\n");
+		if (rs < 0) {
+			cvmx_printf("ERROR: Failed to allocate style, "
+				"none available\n");
 			return CVMX_RESOURCE_ALLOC_FAILED;
 		}
+		style = rs;
+		/* Increment refcnt for newly created style */
+		cvmx_atomic_fetch_and_add32(
+			&cvmx_pki_style_refcnt[node][style], 1);
 	}
-	style = rs;
 	return style;
 }
 
@@ -97,12 +117,20 @@ int cvmx_pki_style_alloc(int node, int style)
  * This function frees a style from pool of global styles per node.
  * @param node	 node to free style from.
  * @param style	 style to free
- * @return 	 0 on success, -1 on failure.
+ * @return 	 0 on success, -1 on failure or
+ * if the style is shared a positive count of remaining users for this style.
  */
 int cvmx_pki_style_free(int node, int style)
 {
+	int rs;
+
+	rs = cvmx_atomic_fetch_and_add32(
+		&cvmx_pki_style_refcnt[node][style], -1);
+	if (rs > 1)
+		return rs-1;
+
 	if (cvmx_free_global_resource_range_with_base(CVMX_GR_TAG_STYLE(node), style, 1) == -1) {
-		cvmx_dprintf("ERROR Failed to release style %d\n", (int)style);
+		cvmx_printf("ERROR Failed to release style %d\n", (int)style);
 		return -1;
 	}
 	return 0;
@@ -124,11 +152,11 @@ int cvmx_pki_cluster_grp_alloc(int node, int cl_grp)
 	int rs;
 
 	if (node >= CVMX_MAX_NODES) {
-		cvmx_dprintf("Invalid node number %d\n", node);
+		cvmx_printf("ERROR: Invalid node number %d\n", node);
 		return -1;
 	}
 	if (cvmx_create_global_resource_range(CVMX_GR_TAG_CLUSTER_GRP(node), CVMX_PKI_NUM_CLUSTER_GROUP)) {
-		cvmx_dprintf("Failed to create Cluster group global resource\n");
+		cvmx_printf("ERROR: Failed to create Cluster group global resource\n");
 		return -1;
 	}
 	if (cl_grp >= 0) {
@@ -160,7 +188,7 @@ int cvmx_pki_cluster_grp_alloc(int node, int cl_grp)
 int cvmx_pki_cluster_grp_free(int node, int cl_grp)
 {
 	if (cvmx_free_global_resource_range_with_base(CVMX_GR_TAG_CLUSTER_GRP(node), cl_grp, 1) == -1) {
-		cvmx_dprintf("ERROR Failed to release cluster group %d\n", (int)cl_grp);
+		cvmx_printf("ERROR Failed to release cluster group %d\n", (int)cl_grp);
 		return -1;
 	}
 	return 0;
@@ -180,18 +208,18 @@ int cvmx_pki_cluster_alloc(int node, int num_clusters, uint64_t *cluster_mask)
 	int clusters[CVMX_PKI_NUM_CLUSTER];
 
 	if (node >= CVMX_MAX_NODES) {
-		cvmx_dprintf("Invalid node number %d\n", node);
+		cvmx_printf("ERROR: Invalid node number %d\n", node);
 		return -1;
 	}
 	if (cvmx_create_global_resource_range(CVMX_GR_TAG_CLUSTERS(node), CVMX_PKI_NUM_CLUSTER)) {
-		cvmx_dprintf("Failed to create Clusters global resource\n");
+		cvmx_printf("ERROR: Failed to create Clusters global resource\n");
 		return -1;
 	}
 	if (*cluster_mask > 0) {
 		while (cluster < CVMX_PKI_NUM_CLUSTER) {
 			if (*cluster_mask & (0x01L << cluster)) {
 				if (cvmx_reserve_global_resource_range(CVMX_GR_TAG_CLUSTERS(node), 0, cluster, 1) == -1) {
-					cvmx_dprintf("ERROR: allocating cluster %d\n", cluster);
+					cvmx_printf("ERROR: allocating cluster %d\n", cluster);
 					return -1;
 				}
 			}
@@ -199,7 +227,7 @@ int cvmx_pki_cluster_alloc(int node, int num_clusters, uint64_t *cluster_mask)
 		}
 	} else {
 		if (cvmx_resource_alloc_many(CVMX_GR_TAG_CLUSTERS(node), 0, num_clusters, clusters) == -1) {
-			   cvmx_dprintf("ERROR: allocating clusters\n");
+			   cvmx_printf("ERROR: allocating clusters\n");
 			   return -1;
 		}
 		*cluster_mask = 0;
@@ -224,7 +252,7 @@ int cvmx_pki_cluster_free(int node, uint64_t cluster_mask)
 			if (cluster_mask & (0x01L << cluster)) {
 				if (cvmx_free_global_resource_range_with_base(
 						CVMX_GR_TAG_CLUSTERS(node), cluster, 1) == -1) {
-					cvmx_dprintf("ERROR: freeing cluster %d\n", cluster);
+					cvmx_printf("ERROR: freeing cluster %d\n", cluster);
 					return -1;
 				}
 			}
@@ -252,7 +280,7 @@ int cvmx_pki_pcam_entry_alloc(int node, int index, int bank, uint64_t cluster_ma
 		if (cluster_mask & (0x01L << cluster)) {
 			if (cvmx_create_global_resource_range(CVMX_GR_TAG_PCAM(node, cluster, bank),
 				CVMX_PKI_TOTAL_PCAM_ENTRY)) {
-				cvmx_dprintf("Failed to create pki pcam global resource\n");
+				cvmx_printf("ERROR: Failed to create pki pcam global resource\n");
 				return -1;
 			}
 			if (index >= 0)
@@ -262,7 +290,7 @@ int cvmx_pki_pcam_entry_alloc(int node, int index, int bank, uint64_t cluster_ma
 				rs = cvmx_allocate_global_resource_range(CVMX_GR_TAG_PCAM(node, cluster, bank),
 						cluster, 1, 1);
 			if (rs == -1) {
-				cvmx_dprintf("Error:index %d not available in cluster %d bank %d",
+				cvmx_printf("ERROR: PCAM :index %d not available in cluster %d bank %d",
 						(int)index, (int)cluster, bank);
 				return -1;
 			}
@@ -291,7 +319,7 @@ int cvmx_pki_pcam_entry_free(int node, int index, int bank, uint64_t cluster_mas
 		if (cluster_mask & (0x01L << cluster)) {
 			if (cvmx_free_global_resource_range_with_base (
 						CVMX_GR_TAG_PCAM(node, cluster, bank), index, 1) == -1) {
-				cvmx_dprintf("ERROR: freeing cluster %d\n", (int)cluster);
+				cvmx_printf("ERROR: freeing cluster %d\n", (int)cluster);
 				return -1;
 			}
 			cluster++;
@@ -318,20 +346,20 @@ int cvmx_pki_qpg_entry_alloc(int node, int base_offset, int count)
 	int rs;
 
 	if (cvmx_create_global_resource_range(CVMX_GR_TAG_QPG_ENTRY(node), CVMX_PKI_NUM_QPG_ENTRY)) {
-		cvmx_dprintf("\nERROR: Failed to create qpg_entry global resource\n");
+		cvmx_printf("ERROR: Failed to create qpg_entry global resource\n");
 		return -1;
 	}
 	if (base_offset >= 0) {
 		rs = cvmx_reserve_global_resource_range(CVMX_GR_TAG_QPG_ENTRY(node),
 				base_offset, base_offset, count);
 		if (rs == -1) {
-			cvmx_dprintf("\nINFO: qpg entry %d is already reserved\n", (int)base_offset);
+			cvmx_dprintf("INFO: qpg entry %d is already reserved\n", (int)base_offset);
 			return CVMX_RESOURCE_ALREADY_RESERVED;
 		}
 	} else {
 		rs = cvmx_allocate_global_resource_range(CVMX_GR_TAG_QPG_ENTRY(node), base_offset, count, 1);
 		if (rs == -1) {
-			cvmx_dprintf("ERROR: Failed to allocate qpg entry\n");
+			cvmx_printf("ERROR: Failed to allocate qpg entry\n");
 			return CVMX_RESOURCE_ALLOC_FAILED;
 		}
 	}
@@ -352,7 +380,7 @@ int cvmx_pki_qpg_entry_alloc(int node, int base_offset, int count)
 int cvmx_pki_qpg_entry_free(int node, int base_offset, int count)
 {
 	if (cvmx_free_global_resource_range_with_base(CVMX_GR_TAG_QPG_ENTRY(node), base_offset, count) == -1) {
-		cvmx_dprintf("\nERROR Failed to release qpg offset %d", (int)base_offset);
+		cvmx_printf("ERROR Failed to release qpg offset %d", (int)base_offset);
 		return -1;
 	}
 	return 0;
@@ -369,24 +397,24 @@ void __cvmx_pki_global_rsrc_free(int node)
 
 	cnt = CVMX_PKI_NUM_CLUSTER_GROUP;
 	if (cvmx_free_global_resource_range_with_base(CVMX_GR_TAG_CLUSTER_GRP(node), 0, cnt) == -1) {
-		cvmx_dprintf("pki-rsrc:ERROR Failed to release all styles\n");
+		cvmx_printf("ERROR pki-rsrc: Failed to release all styles\n");
 	}
 
 #if 0
 	cnt = CVMX_PKI_NUM_CLUSTER;
 	if (cvmx_free_global_resource_range_with_base(CVMX_GR_TAG_CLUSTERS(node), 0, cnt) == -1) {
-		cvmx_dprintf("pki-rsrc:ERROR Failed to release all clusters\n");
+		cvmx_printf("ERROR pki-rsrc:Failed to release all clusters\n");
 	}
 #endif
 
 	cnt = CVMX_PKI_NUM_FINAL_STYLE;
 	if (cvmx_free_global_resource_range_with_base(CVMX_GR_TAG_STYLE(node), 0, cnt) == -1) {
-		cvmx_dprintf("pki-rsrc:ERROR Failed to release all styles\n");
+		cvmx_printf("ERROR pki-rsrc:Failed to release all styles\n");
 	}
 
 	cnt = CVMX_PKI_NUM_QPG_ENTRY;
 	if (cvmx_free_global_resource_range_with_base(CVMX_GR_TAG_QPG_ENTRY(node), 0, cnt) == -1) {
-		cvmx_dprintf("pki-rsrc:ERROR Failed to release all qpg entries\n");
+		cvmx_printf("ERROR pki-rsrc:Failed to release all qpg entries\n");
 	}
 
 	cnt = CVMX_PKI_NUM_PCAM_ENTRY;
@@ -394,7 +422,7 @@ void __cvmx_pki_global_rsrc_free(int node)
 		for (bank = 0; bank < CVMX_PKI_NUM_PCAM_BANK; bank++) {
 			if (cvmx_free_global_resource_range_with_base (
 				CVMX_GR_TAG_PCAM(node, cluster, bank), 0, cnt) == -1) {
-				cvmx_dprintf("pki-rsrc:ERROR Failed to release all pcan entries\n");
+				cvmx_printf("ERROR pki-rsrc:Failed to release all pcan entries\n");
 			}
 		}
 	}
diff --git a/arch/mips/cavium-octeon/executive/octeon-feature.c b/arch/mips/cavium-octeon/executive/octeon-feature.c
index 4752804..ad97437 100644
--- a/arch/mips/cavium-octeon/executive/octeon-feature.c
+++ b/arch/mips/cavium-octeon/executive/octeon-feature.c
@@ -142,7 +142,7 @@ void __init octeon_feature_init(void)
 	OCTEON_FEATURE_SET(OCTEON_FEATURE_PKO3);
 	OCTEON_FEATURE_SET(OCTEON_FEATURE_HNA);
 	OCTEON_FEATURE_SET(OCTEON_FEATURE_BGX_MIX);
-
+	OCTEON_FEATURE_SET(OCTEON_FEATURE_OCX);
 	val = OCTEON_FEATURE_SUCCESS;
 
 feature_check:
diff --git a/arch/mips/include/asm/octeon/cvmx-coremask.h b/arch/mips/include/asm/octeon/cvmx-coremask.h
index 57d5c84..bfb65ba 100644
--- a/arch/mips/include/asm/octeon/cvmx-coremask.h
+++ b/arch/mips/include/asm/octeon/cvmx-coremask.h
@@ -60,7 +60,7 @@
  * provide future compatibility if more cores are added to future processors
  * or more nodes are supported.
  *
- * <hr>$Revision: 101121 $<hr>
+ * <hr>$Revision: 104294 $<hr>
  *
  */
 
@@ -165,6 +165,38 @@ typedef struct cvmx_coremask cvmx_coremask_t;
 		(core) >= 0; )
 
 /**
+ * Given a node and node mask, return the next available node.
+ *
+ * @param node		starting node number
+ * @param node_mask	node mask to use to find the next node
+ *
+ * @return next node number or -1 if no more nodes are available
+ */
+static inline int cvmx_coremask_next_node(unsigned node, uint8_t node_mask)
+{
+	int next_offset;
+
+	next_offset = __builtin_ffs(node_mask >> (node + 1));
+	if (next_offset == 0)
+		return -1;
+	else
+		return node + next_offset;
+}
+
+/**
+ * Iterate through all nodes in a node mask
+ *
+ * @param node		node iterator variable
+ * @param node_mask	mask to use for iterating
+ *
+ * Use this like a for statement
+ */
+#define cvmx_coremask_for_each_node(node, node_mask)		\
+	for ((node) = __builtin_ffs(node_mask) - 1;		\
+	     (node) >= 0 && (node) < CVMX_MAX_NODES;		\
+	     (node) = cvmx_coremask_next_node(node, node_mask))
+
+/**
  * Is ``core'' set in the coremask?
  *
  * @param pcm is the pointer to the coremask.
@@ -319,7 +351,7 @@ static inline void cvmx_coremask_set64_node(cvmx_coremask_t *pcm,
 					    uint8_t node,
 					    uint64_t coremask_64)
 {
-	
+
 	pcm->coremask_bitmap[CVMX_COREMASK_BMP_NODE_CORE_IDX(node, 0)] =
 								coremask_64;
 }
@@ -641,7 +673,9 @@ cvmx_coremask_is_core_first_core(const cvmx_coremask_t *pcm,
 	for (i = 0; i < n; i++)
 		if (pcm->coremask_bitmap[i] != 0)
 			return 0;
-	if (__builtin_ffsll(pcm->coremask_bitmap[n]) < core + 1)
+	/* From now on we only care about the core number within an entry */
+	core &= (CVMX_COREMASK_HLDRSZ - 1);
+	if (__builtin_ffsll(pcm->coremask_bitmap[n]) < (core + 1))
 		return 0;
 	return (__builtin_ffsll(pcm->coremask_bitmap[n]) == core + 1);
 }
diff --git a/arch/mips/include/asm/octeon/cvmx-fpa.h b/arch/mips/include/asm/octeon/cvmx-fpa.h
index a60cb44..a6adbbd 100644
--- a/arch/mips/include/asm/octeon/cvmx-fpa.h
+++ b/arch/mips/include/asm/octeon/cvmx-fpa.h
@@ -42,7 +42,7 @@
  *
  * Interface to the hardware Free Pool Allocator.
  *
- * <hr>$Revision: 103836 $<hr>
+ * <hr>$Revision: 104152 $<hr>
  *
  */
 
@@ -121,7 +121,7 @@ static inline void cvmx_fpa_enable(void)
 	if (!octeon_has_feature(OCTEON_FEATURE_FPA3))
 		cvmx_fpa1_enable();
 	else
-		cvmx_fpa_global_init_node(0);
+		cvmx_fpa_global_init_node(cvmx_get_node_num());
 }
 
 /**
@@ -142,7 +142,7 @@ static inline void cvmx_fpa_disable(void)
  */
 static inline void cvmx_fpa_global_initialize(void)
 {
-	cvmx_fpa_global_init_node(0);
+	cvmx_fpa_global_init_node(cvmx_get_node_num());
 }
 
 
@@ -156,7 +156,8 @@ static inline cvmx_fpa3_gaura_t
 cvmx_fpa1_pool_to_fpa3_aura(cvmx_fpa1_pool_t pool)
 {
 	if ((octeon_has_feature(OCTEON_FEATURE_FPA3))) {
-		cvmx_fpa3_gaura_t aura = __cvmx_fpa3_gaura(0, pool);
+		unsigned node = cvmx_get_node_num();
+		cvmx_fpa3_gaura_t aura = __cvmx_fpa3_gaura(node, pool);
 		return aura;
 	}
 	return CVMX_FPA3_INVALID_GAURA;
diff --git a/arch/mips/include/asm/octeon/cvmx-fpa1.h b/arch/mips/include/asm/octeon/cvmx-fpa1.h
index a2cfc7b..0158622 100644
--- a/arch/mips/include/asm/octeon/cvmx-fpa1.h
+++ b/arch/mips/include/asm/octeon/cvmx-fpa1.h
@@ -43,7 +43,7 @@
  * Interface to the hardware Free Pool Allocator on Octeon chips.
  * These are the legacy models, i.e. prior to CN78XX/CN76XX.
  *
- * <hr>$Revision: 103822 $<hr>
+ * <hr>$Revision: 104152 $<hr>
  *
  */
 
@@ -67,7 +67,7 @@ extern "C" {
 
 /* Legacy pool range is 0..7 and 8 on CN68XX */
 typedef	int cvmx_fpa1_pool_t;
-#define CVMX_FPA1_NUM_POOLS      9
+#define CVMX_FPA1_NUM_POOLS      8
 #define CVMX_FPA1_INVALID_POOL ((cvmx_fpa1_pool_t)-1)
 #define	CVMX_FPA1_NAME_SIZE	16
 
diff --git a/arch/mips/include/asm/octeon/cvmx-fpa3.h b/arch/mips/include/asm/octeon/cvmx-fpa3.h
index c98ef0a..224b756 100644
--- a/arch/mips/include/asm/octeon/cvmx-fpa3.h
+++ b/arch/mips/include/asm/octeon/cvmx-fpa3.h
@@ -42,7 +42,7 @@
  *
  * Interface to the CN78XX Free Pool Allocator, a.k.a. FPA3
  *
- * <hr>$Revision: 103836 $<hr>
+ * <hr>$Revision: 104195 $<hr>
  *
  */
 
@@ -437,38 +437,6 @@ static inline int cvmx_fpa3_config_red_params(unsigned node, int qos_avg_en,
 	return 0;
 }
 
-static inline void cvmx_fpa3_setup_aura_qos(cvmx_fpa3_gaura_t aura,
-			bool ena_red,
-			uint64_t pass_thresh,
-			uint64_t drop_thresh,
-			bool ena_bp, uint64_t bp_thresh)
-{
-	uint64_t shift = 0;
-	uint64_t shift_thresh;
-	cvmx_fpa_aurax_cnt_levels_t aura_level;
-
-	if (!__cvmx_fpa3_aura_valid(aura))
-		return;
-
-	shift_thresh = (bp_thresh > drop_thresh) ? bp_thresh : drop_thresh;
-
-	while ((shift_thresh & (uint64_t)(~(0xff)))) {
-		shift_thresh = shift_thresh >> 1;
-		shift++;
-	};
-
-	aura_level.u64 = cvmx_read_csr_node(aura.node,
-				    CVMX_FPA_AURAX_CNT_LEVELS(aura.laura));
-	aura_level.s.pass = pass_thresh >> shift;
-	aura_level.s.drop = drop_thresh >> shift;
-	aura_level.s.bp = bp_thresh >> shift;
-	aura_level.s.shift = shift;
-	aura_level.s.red_ena = ena_red;
-	aura_level.s.bp_ena = ena_bp;
-	cvmx_write_csr_node(aura.node, CVMX_FPA_AURAX_CNT_LEVELS(aura.laura),
-			    aura_level.u64);
-}
-
 /**
  * Get the FPA3 POOL underneath FPA3 AURA, containing all its buffers
  *
@@ -558,6 +526,63 @@ static inline long long cvmx_fpa3_get_available(cvmx_fpa3_gaura_t aura)
 	return ret;
 }
 
+/**
+ * Configure the QoS parameters of an FPA3 AURA
+ *
+ * @para aura is the FPA3 AURA handle
+ * @param ena_bp enables backpressure when outstanding count exceeds 'bp_thresh'
+ * @param ena_red enables random early discard when outstanding count
+ * exceeds 'pass_thresh'
+ * @param pass_thresh is the maximum count to invoke flow control
+ * @param drop_thresh is the count threshold to begin dropping packets
+ * @para, bp_thresh is the back-pressure threshold
+ *
+ */
+static inline void cvmx_fpa3_setup_aura_qos(cvmx_fpa3_gaura_t aura, bool ena_red,
+	uint64_t pass_thresh, uint64_t drop_thresh,
+	bool ena_bp, uint64_t bp_thresh)
+{
+	unsigned shift = 0;
+	uint64_t shift_thresh;
+	cvmx_fpa_aurax_cnt_limit_t limit_reg;
+	cvmx_fpa_aurax_cnt_levels_t aura_level;
+
+	if (!__cvmx_fpa3_aura_valid(aura))
+		return;
+
+	/* Get AURAX count limit for validation */
+	limit_reg.u64 = cvmx_read_csr_node(aura.node,
+		CVMX_FPA_AURAX_CNT_LIMIT(aura.laura));
+
+	if (pass_thresh < 256)
+		pass_thresh = 255;
+
+	if (drop_thresh <= pass_thresh || drop_thresh > limit_reg.cn78xx.limit)
+		drop_thresh = limit_reg.cn78xx.limit;
+
+	if (bp_thresh < 256 || bp_thresh > limit_reg.cn78xx.limit)
+		bp_thresh = limit_reg.cn78xx.limit >> 1;
+
+	shift_thresh = (bp_thresh > drop_thresh) ? bp_thresh : drop_thresh;
+
+	/* Calculate shift so that the largest threshold fits in 8 bits */
+	for (shift = 0; shift < (1 << 6); shift++) {
+		if (0 == ((shift_thresh >> shift) & ~0xffull))
+			break;
+	};
+
+	aura_level.u64 = cvmx_read_csr_node(aura.node,
+				    CVMX_FPA_AURAX_CNT_LEVELS(aura.laura));
+	aura_level.s.pass = pass_thresh >> shift;
+	aura_level.s.drop = drop_thresh >> shift;
+	aura_level.s.bp = bp_thresh >> shift;
+	aura_level.s.shift = shift;
+	aura_level.s.red_ena = ena_red;
+	aura_level.s.bp_ena = ena_bp;
+	cvmx_write_csr_node(aura.node, CVMX_FPA_AURAX_CNT_LEVELS(aura.laura),
+			    aura_level.u64);
+}
+
 extern cvmx_fpa3_gaura_t cvmx_fpa3_reserve_aura(int node, int desired_aura_num);
 extern int cvmx_fpa3_release_aura(cvmx_fpa3_gaura_t aura);
 extern cvmx_fpa3_pool_t cvmx_fpa3_reserve_pool(int node, int desired_pool_num);
diff --git a/arch/mips/include/asm/octeon/cvmx-helper-fpa.h b/arch/mips/include/asm/octeon/cvmx-helper-fpa.h
index b29757e..c0899a5 100644
--- a/arch/mips/include/asm/octeon/cvmx-helper-fpa.h
+++ b/arch/mips/include/asm/octeon/cvmx-helper-fpa.h
@@ -47,8 +47,6 @@
 #ifndef __CVMX_HELPER_H_FPA__
 #define __CVMX_HELPER_H_FPA__
 
-extern int fpa_helper_debug;
-
 /**
  * Allocate memory and initialize the FPA pools using memory
  * from cvmx-bootmem. Sizes of each element in the pools is
diff --git a/arch/mips/include/asm/octeon/cvmx-helper-util.h b/arch/mips/include/asm/octeon/cvmx-helper-util.h
index 0b0e379..1d607e9 100644
--- a/arch/mips/include/asm/octeon/cvmx-helper-util.h
+++ b/arch/mips/include/asm/octeon/cvmx-helper-util.h
@@ -42,7 +42,7 @@
  *
  * Small helper utilities.
  *
- * <hr>$Revision: 103836 $<hr>
+ * <hr>$Revision: 104282 $<hr>
  */
 
 #ifndef __CVMX_HELPER_UTIL_H__
@@ -142,6 +142,48 @@ static inline int cvmx_helper_node_interface_to_xiface(int node, int interface)
 }
 
 /**
+ * Free the pip packet buffers contained in a work queue entry.
+ * The work queue entry is not freed.
+ *
+ * @param work   Work queue entry with packet to free
+ */
+static inline void cvmx_helper_free_pip_pkt_data(cvmx_wqe_t *work)
+{
+	uint64_t        number_buffers;
+	cvmx_buf_ptr_t  buffer_ptr;
+	cvmx_buf_ptr_t  next_buffer_ptr;
+	uint64_t        start_of_buffer;
+
+	number_buffers = work->word2.s.bufs;
+	if (number_buffers == 0)
+		return;
+	buffer_ptr = work->packet_ptr;
+
+    /* Since the number of buffers is not zero, we know this is not a dynamic
+	short packet. We need to check if it is a packet received with
+	IPD_CTL_STATUS[NO_WPTR]. If this is true, we need to free all buffers
+	except for the first one. The caller doesn't expect their WQE pointer
+	to be freed */
+	start_of_buffer = ((buffer_ptr.s.addr >> 7) - buffer_ptr.s.back) << 7;
+	if (cvmx_ptr_to_phys(work) == start_of_buffer)
+	{
+		next_buffer_ptr = *(cvmx_buf_ptr_t*)cvmx_phys_to_ptr(buffer_ptr.s.addr - 8);
+		buffer_ptr = next_buffer_ptr;
+		number_buffers--;
+	}
+
+	while (number_buffers--)
+	{
+		/* Remember the back pointer is in cache lines, not 64bit words */
+		start_of_buffer = ((buffer_ptr.s.addr >> 7) - buffer_ptr.s.back) << 7;
+		/* Read pointer to next buffer before we free the current buffer. */
+		next_buffer_ptr = *(cvmx_buf_ptr_t*)cvmx_phys_to_ptr(buffer_ptr.s.addr - 8);
+		cvmx_fpa_free(cvmx_phys_to_ptr(start_of_buffer), buffer_ptr.s.pool, 0);
+		buffer_ptr = next_buffer_ptr;
+	}
+}
+
+/**
  * Free the pki packet buffers contained in a work queue entry.
  * If first packet buffer contains wqe, wqe gets freed too so do not access
  * wqe after calling this function.
diff --git a/arch/mips/include/asm/octeon/cvmx-hwpko.h b/arch/mips/include/asm/octeon/cvmx-hwpko.h
index 8a1f56d..f5d93b0 100644
--- a/arch/mips/include/asm/octeon/cvmx-hwpko.h
+++ b/arch/mips/include/asm/octeon/cvmx-hwpko.h
@@ -341,7 +341,8 @@ static inline void cvmx_pko_doorbell(uint64_t ipd_port, uint64_t queue, uint64_t
  * @param queue  Queue to use
  * @param use_locking
  *               CVMX_PKO_LOCK_NONE, CVMX_PKO_LOCK_ATOMIC_TAG, or CVMX_PKO_LOCK_CMD_QUEUE
- */ static inline void cvmx_pko_send_packet_prepare(uint64_t port __attribute__ ((unused)), uint64_t queue, cvmx_pko_lock_t use_locking)
+ */
+static inline void cvmx_pko_send_packet_prepare(uint64_t port __attribute__ ((unused)), uint64_t queue, cvmx_pko_lock_t use_locking)
 {
 	if (use_locking == CVMX_PKO_LOCK_ATOMIC_TAG) {
 		/* Must do a full switch here to handle all cases.  We use a fake WQE pointer, as the POW does
diff --git a/arch/mips/include/asm/octeon/cvmx-pki.h b/arch/mips/include/asm/octeon/cvmx-pki.h
index 5d0f908..09d55b6 100644
--- a/arch/mips/include/asm/octeon/cvmx-pki.h
+++ b/arch/mips/include/asm/octeon/cvmx-pki.h
@@ -56,6 +56,7 @@
 #include "cvmx-fpa3.h"
 #include "cvmx-helper-util.h"
 #include "cvmx-helper-cfg.h"
+#include "cvmx-error.h"
 #endif
 
 #ifdef	__cplusplus
@@ -719,7 +720,7 @@ static inline void cvmx_pki_enable_backpressure(int node)
 	while (value >= (1ull << 48) && cnt++ < 20) \
 		value = cvmx_read_csr_node(node, addr); \
 	if (cnt >= 20)  \
-		cvmx_dprintf("count stuck for 0x%llx\n", addr); }
+		cvmx_dprintf("count stuck for 0x%llx\n", (long long unsigned int)addr); }
 
 
 /**
@@ -886,6 +887,13 @@ static inline void cvmx_pki_get_stats(int node, int index, struct cvmx_pki_port_
 	cvmx_pki_pkndx_inb_stat2_t pki_pknd_inb_stat2;
 	int cnt;
 
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+	/* Accessing PKI stat registers can timeout based on the Errata
+	   PKI-20775, disable SLI_INT_SUM[RML_TO] before reading the stats
+	   enable back after clearing the interrupt. */
+	cvmx_error_intsn_disable_v3(0x1f000);
+#endif
+
 	stat0.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT0(index));
 	READCORRECT(cnt, node, stat0.u64, CVMX_PKI_STATX_STAT0(index));
 
@@ -985,6 +993,12 @@ static inline void cvmx_pki_get_stats(int node, int index, struct cvmx_pki_port_
 	status->inb_packets = pki_pknd_inb_stat0.s.pkts;
 	status->inb_octets = pki_pknd_inb_stat1.s.octs;
 	status->inb_errors = pki_pknd_inb_stat2.s.errs;
+
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+	/* Enable SLI_INT_SUM[RML_TO] interrupt after clear the pending interrupt. */
+	cvmx_write_csr_node(node, CVMX_CIU3_ISCX_W1C(0x1f000), 1);
+	cvmx_error_intsn_enable_v3(0x1f000);
+#endif
 }
 
 /**
diff --git a/arch/mips/include/asm/octeon/cvmx-pow.h b/arch/mips/include/asm/octeon/cvmx-pow.h
index f4b32c8..1df6093 100644
--- a/arch/mips/include/asm/octeon/cvmx-pow.h
+++ b/arch/mips/include/asm/octeon/cvmx-pow.h
@@ -2402,6 +2402,7 @@ static inline void cvmx_pow_work_submit(cvmx_wqe_t * wqp, uint32_t tag, cvmx_pow
 		tag_req.s_cn78xx_other.op = CVMX_POW_TAG_OP_ADDWQ;
 		tag_req.s_cn78xx_other.type = tag_type;
 		tag_req.s_cn78xx_other.wqp = cvmx_ptr_to_phys(wqp);
+		tag_req.s_cn78xx_other.grp = xgrp;
 
 		ptr.s_cn78xx.did = 0x66; //CVMX_OCT_DID_TAG_TAG6;
 		ptr.s_cn78xx.mem_region = CVMX_IO_SEG;
@@ -2500,6 +2501,7 @@ static inline void cvmx_pow_work_submit_node(cvmx_wqe_t * wqp, uint32_t tag, cvm
 		tag_req.s_cn78xx_other.op = CVMX_POW_TAG_OP_ADDWQ;
 		tag_req.s_cn78xx_other.type = tag_type;
 		tag_req.s_cn78xx_other.wqp = cvmx_ptr_to_phys(wqp);
+		tag_req.s_cn78xx_other.grp = xgrp;
 
 		ptr.s_cn78xx.did = 0x66; //CVMX_OCT_DID_TAG_TAG6;
 		ptr.s_cn78xx.mem_region = CVMX_IO_SEG;
diff --git a/arch/mips/include/asm/octeon/octeon-feature.h b/arch/mips/include/asm/octeon/octeon-feature.h
index b1615b1..ddb9825 100644
--- a/arch/mips/include/asm/octeon/octeon-feature.h
+++ b/arch/mips/include/asm/octeon/octeon-feature.h
@@ -174,6 +174,8 @@ typedef enum {
 				/**<  Octeon has FAU */
 	OCTEON_FEATURE_HNA,
 				/**<  Octeon has HNA */
+	OCTEON_FEATURE_OCX,
+				/**<  Octeon has OCX */
 	OCTEON_MAX_FEATURE
 } octeon_feature_t;
 
@@ -478,6 +480,10 @@ static inline int octeon_has_feature_OCTEON_FEATURE_BGX_MIX(void)
 	return OCTEON_IS_MODEL(OCTEON_CN78XX);
 }
 
+static inline int octeon_has_feature_OCTEON_FEATURE_OCX(void)
+{
+	return OCTEON_IS_MODEL(OCTEON_CN78XX) && !OCTEON_IS_MODEL(OCTEON_CN76XX);
+}
 /*
  * bit map for octeon features
  */
-- 
2.6.2

