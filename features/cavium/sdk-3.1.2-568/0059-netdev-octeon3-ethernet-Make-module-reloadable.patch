From d3ac6fcc60189c8be6b6c0817e4a6f2e5eceb810 Mon Sep 17 00:00:00 2001
From: Carlos Munoz <cmunoz@caviumnetworks.com>
Date: Wed, 28 Jan 2015 17:51:05 -0800
Subject: [PATCH 059/184] netdev: octeon3-ethernet: Make module reloadable.

Source: Cavium Networks, Inc.
MR: 12636
Type: Integration
Disposition: Merged from Octeon Tree
ChangeID: b8df6b59dc01ea11752f43fd41ba17622f7df729
Description:

Signed-off-by: Carlos Munoz <cmunoz@caviumnetworks.com>
Signed-off-by: Corey Minyard <cminyard@mvista.com>
[Original patch taken from octeon-linux-kernel-patches-SDK-3.1.2-release]
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 drivers/net/ethernet/octeon/octeon-bgx-port.c  |   8 +-
 drivers/net/ethernet/octeon/octeon3-ethernet.c | 225 ++++++++++++++++++++++---
 2 files changed, 207 insertions(+), 26 deletions(-)

diff --git a/drivers/net/ethernet/octeon/octeon-bgx-port.c b/drivers/net/ethernet/octeon/octeon-bgx-port.c
index 6e06e64..80aabfd 100644
--- a/drivers/net/ethernet/octeon/octeon-bgx-port.c
+++ b/drivers/net/ethernet/octeon/octeon-bgx-port.c
@@ -72,9 +72,13 @@ static struct bgx_port_priv *bgx_port_netdev2priv(struct net_device *netdev)
 
 void bgx_port_set_netdev(struct device *dev, struct net_device *netdev)
 {
-	struct bgx_port_netdev_priv *nd_priv = netdev_priv(netdev);
 	struct bgx_port_priv *priv = dev_get_drvdata(dev);
-	nd_priv->bgx_priv = priv;
+
+	if (netdev) {
+		struct bgx_port_netdev_priv *nd_priv = netdev_priv(netdev);
+		nd_priv->bgx_priv = priv;
+	}
+
 	priv->netdev = netdev;
 }
 EXPORT_SYMBOL(bgx_port_set_netdev);
diff --git a/drivers/net/ethernet/octeon/octeon3-ethernet.c b/drivers/net/ethernet/octeon/octeon3-ethernet.c
index 76c2565..d2ce358 100644
--- a/drivers/net/ethernet/octeon/octeon3-ethernet.c
+++ b/drivers/net/ethernet/octeon/octeon3-ethernet.c
@@ -244,6 +244,9 @@ struct octeon3_ethernet_node {
 	cvmx_fpa3_pool_t  pki_packet_pool;
 	cvmx_fpa3_pool_t sso_pool;
 	cvmx_fpa3_pool_t pko_pool;
+	void *sso_pool_stack;
+	void *pko_pool_stack;
+	void *pki_packet_pool_stack;
 	cvmx_fpa3_gaura_t sso_aura;
 	cvmx_fpa3_gaura_t pko_aura;
 	int tx_complete_grp;
@@ -394,9 +397,10 @@ static void octeon3_eth_gen_affinity(int node, cpumask_t *mask)
 	cpumask_set_cpu(cpu, mask);
 }
 
-static int octeon3_eth_fpa_pool_init(cvmx_fpa3_pool_t pool, int num_ptrs)
+static int octeon3_eth_fpa_pool_init(cvmx_fpa3_pool_t	pool,
+				     void		**pool_stack,
+				     int		num_ptrs)
 {
-	void *pool_stack;
 	u64 pool_stack_start, pool_stack_end;
 	union cvmx_fpa_poolx_end_addr limit_addr;
 	union cvmx_fpa_poolx_cfg cfg;
@@ -408,13 +412,13 @@ static int octeon3_eth_fpa_pool_init(cvmx_fpa3_pool_t pool, int num_ptrs)
 	limit_addr.cn78xx.addr = ~0ll;
 	cvmx_write_csr_node(pool.node, CVMX_FPA_POOLX_END_ADDR(pool.lpool), limit_addr.u64);
 
-	pool_stack = kmalloc_node(stack_size, GFP_KERNEL, pool.node);
-	if (!pool_stack)
+	*pool_stack = kmalloc_node(stack_size, GFP_KERNEL, pool.node);
+	if (!*pool_stack)
 		return -ENOMEM;
 
 	kmemleak_not_leak(pool_stack);
 
-	pool_stack_start = virt_to_phys(pool_stack);
+	pool_stack_start = virt_to_phys(*pool_stack);
 	pool_stack_end = round_down(pool_stack_start + stack_size, 128);
 	pool_stack_start = round_up(pool_stack_start, 128);
 
@@ -495,7 +499,9 @@ static int octeon3_eth_sso_init(unsigned int node, int aura)
 		kmemleak_not_leak(mem);
 		phys = virt_to_phys(mem);
 		cvmx_write_csr_node(node, CVMX_SSO_XAQX_HEAD_PTR(i), phys);
+		cvmx_write_csr_node(node, CVMX_SSO_XAQX_HEAD_NEXT(i), phys);
 		cvmx_write_csr_node(node, CVMX_SSO_XAQX_TAIL_PTR(i), phys);
+		cvmx_write_csr_node(node, CVMX_SSO_XAQX_TAIL_NEXT(i), phys);
 		/* SSO-18678 */
 		cvmx_write_csr_node(node, CVMX_SSO_GRPX_PRI(i), grp_pri.u64);
 	}
@@ -510,6 +516,67 @@ err:
 	return rv;
 }
 
+/* octeon3_eth_sso_shutdown:		Shutdown the sso. It undoes what
+ *					octeon3_eth_sso_init() did.
+ *
+ *  node:				Node where sso to disable is.
+ */
+static void octeon3_eth_sso_shutdown(unsigned int node)
+{
+	struct octeon3_ethernet_node	*oen;
+	union cvmx_sso_aw_cfg		aw_cfg;
+	cvmx_sso_grpx_aq_cnt_t		aq_cnt;
+	cvmx_sso_aw_status_t		aw_status;
+	int				i;
+
+	oen = octeon3_eth_node + node;
+
+	/* Disable sso */
+	aw_cfg.u64 = cvmx_read_csr_node(node, CVMX_SSO_AW_CFG);
+	aw_cfg.s.xaq_byp_dis = 1;
+	aw_cfg.s.xaq_alloc_dis = 1;
+	aw_cfg.s.rwen = 0;
+	cvmx_write_csr_node(node, CVMX_SSO_AW_CFG, aw_cfg.u64);
+
+	/* Extract the fpa buffers */
+	for (i = 0; i < 256; i++) {
+		cvmx_sso_xaqx_head_ptr_t	head;
+		cvmx_sso_xaqx_tail_ptr_t	tail;
+		u64				addr;
+		void				*ptr;
+
+		head.u64 = cvmx_read_csr_node(node, CVMX_SSO_XAQX_HEAD_PTR(i));
+		tail.u64 = cvmx_read_csr_node(node, CVMX_SSO_XAQX_TAIL_PTR(i));
+		aq_cnt.u64 = cvmx_read_csr_node(node, CVMX_SSO_GRPX_AQ_CNT(i));
+
+		/* Verify pointers */
+		if (head.s.ptr != tail.s.ptr) {
+			pr_err("WARNING: octeon3_eth_sso_shutdown bad ptr\n");
+			continue;
+		}
+
+		/* This sso group should have no pending entries */
+		if (aq_cnt.s.aq_cnt != 0)
+			pr_err("WARNING: octeon3_eth_sso_shutdown not empty\n");
+
+		addr = head.s.ptr;
+		addr <<= 7;
+		ptr = phys_to_virt(addr);
+		cvmx_fpa3_free(ptr, oen->sso_aura, 0);
+
+		/* Clear pointers */
+		cvmx_write_csr_node(node, CVMX_SSO_XAQX_HEAD_PTR(i), 0);
+		cvmx_write_csr_node(node, CVMX_SSO_XAQX_HEAD_NEXT(i), 0);
+		cvmx_write_csr_node(node, CVMX_SSO_XAQX_TAIL_PTR(i), 0);
+		cvmx_write_csr_node(node, CVMX_SSO_XAQX_TAIL_NEXT(i), 0);
+	}
+
+	/* Make sure all buffers drained */
+	do
+		aw_status.u64 = cvmx_read_csr_node(node, CVMX_SSO_AW_STATUS);
+	while (aw_status.s.xaq_buf_cached);
+}
+
 static void octeon3_eth_sso_irq_set_armed(int node, int grp, bool v)
 {
 	union cvmx_sso_grpx_int_thr grp_int_thr;
@@ -625,7 +692,7 @@ static void octeon3_eth_replenish_rx(struct octeon3_ethernet *priv, int count)
 
 static bool octeon3_eth_tx_complete_runnable(struct octeon3_ethernet_worker *worker)
 {
-	return atomic_read(&worker->kick) != 0;
+	return atomic_read(&worker->kick) != 0 || kthread_should_stop();
 }
 
 static int octeon3_eth_replenish_all(struct octeon3_ethernet_node *oen)
@@ -658,7 +725,7 @@ static int octeon3_eth_tx_complete_worker(void *data)
 	int backlog_stop_thresh = order == 0 ? 31 : order * 80;
 	int i;
 
-	for (;;) {
+	while (!kthread_should_stop()) {
 		/*
 		 * replaced by wait_event to avoid warnings like
 		 * "task oct3_eth/0:2:1250 blocked for more than 120 seconds."
@@ -773,9 +840,11 @@ static int octeon3_eth_global_init(unsigned int node)
 	       oen->sso_pool.lpool, oen->sso_aura.laura,
 	       oen->pko_pool.lpool, oen->pko_aura.laura);
 
-	octeon3_eth_fpa_pool_init(oen->sso_pool, 40960);
-	octeon3_eth_fpa_pool_init(oen->pko_pool, 40960);
-	octeon3_eth_fpa_pool_init(oen->pki_packet_pool, 64 * num_packet_buffers);
+	octeon3_eth_fpa_pool_init(oen->sso_pool, &oen->sso_pool_stack, 40960);
+	octeon3_eth_fpa_pool_init(oen->pko_pool, &oen->pko_pool_stack, 40960);
+	octeon3_eth_fpa_pool_init(oen->pki_packet_pool,
+				  &oen->pki_packet_pool_stack,
+				  64 * num_packet_buffers);
 	octeon3_eth_fpa_aura_init(oen->sso_pool, oen->sso_aura, 20480);
 	octeon3_eth_fpa_aura_init(oen->pko_pool, oen->pko_aura, 20480);
 
@@ -1486,7 +1555,7 @@ static int octeon3_eth_ndo_init(struct net_device *netdev)
 		&priv->buffers_needed;
 
 	base_rx_grp = -1;
-	r = cvmx_sso_allocate_group_range(priv->numa_node, &base_rx_grp, rx_contexts);
+	r = cvmx_sso_reserve_group_range(priv->numa_node, &base_rx_grp, rx_contexts);
 	if (r) {
 		dev_err(netdev->dev.parent, "Failed to allocated SSO group\n");
 		return -ENODEV;
@@ -1523,6 +1592,7 @@ static int octeon3_eth_ndo_init(struct net_device *netdev)
 	prt_schd->qpg_qos = CVMX_PKI_QPG_QOS_NONE;
 
 	cvmx_helper_pki_init_port(ipd_port, prt_schd);
+	kfree(prt_schd);
 	cvmx_pki_get_port_config(ipd_port, &pki_prt_cfg);
 
 	pki_prt_cfg.style_cfg.parm_cfg.ip6_udp_opt = false;
@@ -1623,8 +1693,6 @@ static int octeon3_eth_ndo_init(struct net_device *netdev)
 	/* Register ethtool methods */
 	netdev->ethtool_ops = &octeon3_ethtool_ops;
 
-	__cvmx_export_config();
-
 	return 0;
 err:
 	kfree(prt_schd);
@@ -1633,6 +1701,23 @@ err:
 
 static void octeon3_eth_ndo_uninit(struct net_device *netdev)
 {
+	struct octeon3_ethernet	*priv = netdev_priv(netdev);
+	int			ipd_port;
+
+	/* Shutdwon pki for this interface */
+	ipd_port = cvmx_helper_get_ipd_port(priv->xiface, priv->port_index);
+	cvmx_helper_pki_port_shutdown(ipd_port);
+	cvmx_fpa3_release_aura(__cvmx_fpa3_gaura(priv->numa_node,
+						 priv->pki_laura));
+	aura2bufs_needed[priv->numa_node][priv->pki_laura] = NULL;
+
+	/* Shutdown pko for this interface */
+	cvmx_helper_pko3_shut_interface(priv->xiface);
+
+	/* Free the receive contexts sso groups */
+	cvmx_sso_release_group_range(priv->numa_node, priv->rx_cxt[0].rx_grp,
+				     rx_contexts);
+
 	return;
 }
 
@@ -1710,7 +1795,6 @@ static int octeon3_eth_ndo_open(struct net_device *netdev)
 	octeon3_eth_replenish_rx(priv, priv->rx_buf_count);
 
 	r = bgx_port_enable(netdev);
-	__cvmx_export_config();
 
 	return r;
 
@@ -1747,24 +1831,29 @@ static int octeon3_eth_ndo_stop(struct net_device *netdev)
 	if (r)
 		goto err;
 
+	/* Allow enough time for ingress in transit packets to be drained */
 	msleep(20);
 
+	/* Wait until sso has no more work for this interface */
 	for (i = 0; i < priv->num_rx_cxt; i++) {
 		rx = priv->rx_cxt + i;
-		/* Wait for SSO to drain */
 		while (cvmx_read_csr_node(priv->numa_node, CVMX_SSO_GRPX_AQ_CNT(rx->rx_grp)))
 			msleep(20);
 	}
 
+	/* Free the irq and napi context for each rx context */
 	for (i = 0; i < priv->num_rx_cxt; i++) {
 		rx = priv->rx_cxt + i;
 		octeon3_eth_sso_irq_set_armed(priv->numa_node, rx->rx_grp, false);
-
 		irq_set_affinity_hint(rx->rx_irq, NULL);
 		free_irq(rx->rx_irq, rx);
+		irq_dispose_mapping(rx->rx_irq);
 		rx->rx_irq = 0;
+
+		octeon3_rm_napi_from_cxt(priv->numa_node, rx->napiw);
+		rx->napiw = NULL;
+		BUG_ON(!__bitmap_empty(rx->napi_idx_bitmap, CVMX_MAX_CORES));
 	}
-	msleep(20);
 
 	/* Free the packet buffers */
 	for (;;) {
@@ -1775,13 +1864,6 @@ static int octeon3_eth_ndo_stop(struct net_device *netdev)
 		dev_kfree_skb(skb);
 	}
 
-	/* Free the napis */
-	for (i = 0; i < priv->num_rx_cxt; i++) {
-		octeon3_rm_napi_from_cxt(priv->numa_node,
-					 priv->rx_cxt[i].napiw);
-		priv->rx_cxt[i].napiw = NULL;
-	}
-
 err:
 	return r;
 }
@@ -2200,8 +2282,103 @@ static int octeon3_eth_probe(struct platform_device *pdev)
 	return 0;
 }
 
+/*
+ * octeon3_eth_global_exit:	Free all the used resources and restore the
+ *				hardware to the default state.
+ *
+ *  node:			Node to free/reset.
+ *
+ *  Returns:			Zero on success, error otherwise.
+ */
+static int octeon3_eth_global_exit(int node)
+{
+	struct octeon3_ethernet_node	*oen = octeon3_eth_node + node;
+	int				i;
+
+	/* Free the tx_complete irq */
+	octeon3_eth_sso_irq_set_armed(node, oen->tx_complete_grp, false);
+	irq_set_affinity_hint(oen->tx_irq, NULL);
+	free_irq(oen->tx_irq, oen);
+	irq_dispose_mapping(oen->tx_irq);
+	oen->tx_irq = 0;
+
+	/* Stop the worker threads */
+	for (i = 0; i < ARRAY_SIZE(oen->workers); i++)
+		kthread_stop(oen->workers[i].task);
+
+	/* Shutdown pki */
+	cvmx_helper_pki_shutdown(node);
+	cvmx_fpa3_release_pool(oen->pki_packet_pool);
+	kfree(oen->pki_packet_pool_stack);
+
+	/* Shutdown pko */
+	cvmx_helper_pko3_shutdown(node);
+	for (;;) {
+		void **w;
+
+		w = cvmx_fpa3_alloc(oen->pko_aura);
+		if (!w)
+			break;
+		kmem_cache_free(octeon3_eth_sso_pko_cache, w);
+	}
+	cvmx_fpa3_release_aura(oen->pko_aura);
+	cvmx_fpa3_release_pool(oen->pko_pool);
+	kfree(oen->pko_pool_stack);
+
+	/* Shutdown sso */
+	cvmx_sso_release_group(node, oen->tx_complete_grp);
+	octeon3_eth_sso_shutdown(node);
+	for (;;) {
+		void **w;
+
+		w = cvmx_fpa3_alloc(oen->sso_aura);
+		if (!w)
+			break;
+		kmem_cache_free(octeon3_eth_sso_pko_cache, w);
+	}
+	cvmx_fpa3_release_aura(oen->sso_aura);
+	cvmx_fpa3_release_pool(oen->sso_pool);
+	kfree(oen->sso_pool_stack);
+
+	/* Destroy the memory cache used by sso and pko */
+	kmem_cache_destroy(octeon3_eth_sso_pko_cache);
+
+	return 0;
+}
+
 static int octeon3_eth_remove(struct platform_device *pdev)
 {
+	struct net_device		*netdev = dev_get_drvdata(&pdev->dev);
+	struct octeon3_ethernet		*priv = netdev_priv(netdev);
+	int				node = priv->numa_node;
+	struct octeon3_ethernet_node	*oen = octeon3_eth_node + node;
+
+	unregister_netdev(netdev);
+	bgx_port_set_netdev(pdev->dev.parent, NULL);
+	dev_set_drvdata(&pdev->dev, NULL);
+	free_netdev(netdev);
+
+	/* Free all resources when there are no more devices */
+	mutex_lock(&octeon3_eth_init_mutex);
+	mutex_lock(&oen->device_list_lock);
+	list_del_rcu(&priv->list);
+	if (oen->init_done && list_empty(&oen->device_list)) {
+		int	i;
+
+		octeon3_eth_global_exit(node);
+
+		for (i = 0; i < MAX_NAPIS_PER_NODE; i++) {
+			napi_disable(&napi_wrapper[node][i].napi);
+			netif_napi_del(&napi_wrapper[node][i].napi);
+		}
+
+		oen->init_done = false;
+		oen->napi_init_done = false;
+	}
+
+	mutex_unlock(&oen->device_list_lock);
+	mutex_unlock(&octeon3_eth_init_mutex);
+
 	return 0;
 }
 
-- 
1.9.1

