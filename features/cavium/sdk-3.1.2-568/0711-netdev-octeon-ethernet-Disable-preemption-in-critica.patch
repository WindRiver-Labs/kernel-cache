From 9aec634d368faffbc7298fdedc26826cb8d25ddc Mon Sep 17 00:00:00 2001
From: David Daney <david.daney@cavium.com>
Date: Thu, 10 Jul 2014 18:05:50 -0700
Subject: [PATCH 711/974] netdev: octeon-ethernet: Disable preemption in
 critical sections of xmit.

After a QOS/queue is selected, we cannot allow scheduling to a
different CPU, as that can lead to PKO command queue corruption.  Add
preempt_disable()/preempt_enable() to protect the critical section.

Move some code out of this section to minimize time with preemption
disabled.  Also fix possible buffer leak on wqe pool depletion.

Signed-off-by: David Daney <david.daney@cavium.com>
[Original patch taken from Cavium SDK 3.1.2-568]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
---
 drivers/net/ethernet/octeon/ethernet-xmit.c | 46 +++++++++++++++++------------
 1 file changed, 27 insertions(+), 19 deletions(-)

diff --git a/drivers/net/ethernet/octeon/ethernet-xmit.c b/drivers/net/ethernet/octeon/ethernet-xmit.c
index 9af0965..397bef7 100644
--- a/drivers/net/ethernet/octeon/ethernet-xmit.c
+++ b/drivers/net/ethernet/octeon/ethernet-xmit.c
@@ -55,7 +55,7 @@ CVM_OCT_XMIT
 	int qos;
 	int i;
 	int frag_count;
-	enum {QUEUE_HW, QUEUE_WQE, QUEUE_DROP} queue_type;
+	enum {QUEUE_HW, QUEUE_WQE, QUEUE_DROP, QUEUE_DROP_NO_DEC} queue_type;
 	struct octeon_ethernet *priv = netdev_priv(dev);
 	s32 queue_depth;
 	s32 buffers_to_free;
@@ -84,6 +84,27 @@ CVM_OCT_XMIT
 					       0);
 	}
 
+	frag_count = 0;
+	if (skb_has_frag_list(skb))
+		skb_walk_frags(skb, skb_tmp)
+			frag_count++;
+	/* We have space for 12 segment pointers, If there will be
+	 * more than that, we must linearize.  The count is: 1 (base
+	 * SKB) + frag_count + nr_frags.
+	 */
+	if (unlikely(skb_shinfo(skb)->nr_frags + frag_count > 11)) {
+		if (unlikely(__skb_linearize(skb))) {
+			dev_kfree_skb_any(skb);
+			dev->stats.tx_dropped++;
+			goto post_preempt_out;
+		}
+		frag_count = 0;
+	}
+
+	/* We cannot move to a different CPU once we determine our
+	 * queue number/qos
+	 */
+	preempt_disable();
 #ifdef CVM_OCT_LOCKLESS
 	qos = cvmx_get_core_num();
 #else
@@ -105,22 +126,6 @@ CVM_OCT_XMIT
 					       priv->tx_queue[qos].fau, 1);
 	}
 
-	frag_count = 0;
-	if (skb_has_frag_list(skb))
-		skb_walk_frags(skb, skb_tmp)
-			frag_count++;
-	/* We have space for 12 segment pointers, If there will be
-	 * more than that, we must linearize.  The count is: 1 (base
-	 * SKB) + frag_count + nr_frags.
-	 */
-	if (unlikely(skb_shinfo(skb)->nr_frags + frag_count > 11)) {
-		if (unlikely(__skb_linearize(skb))) {
-			queue_type = QUEUE_DROP;
-			goto skip_xmit;
-		}
-		frag_count = 0;
-	}
-
 #ifndef CVM_OCT_LOCKLESS
 	/* The CN3XXX series of parts has an errata (GMX-401) which
 	 * causes the GMX block to hang if a collision occurs towards
@@ -172,7 +177,7 @@ CVM_OCT_XMIT
 		work = cvmx_fpa1_alloc(wqe_pool);
 		if (unlikely(!work)) {
 			netdev_err(dev, "Failed WQE allocate\n");
-			queue_type = QUEUE_DROP;
+			queue_type = USE_ASYNC_IOBDMA ? QUEUE_DROP : QUEUE_DROP_NO_DEC;
 			goto skip_xmit;
 		}
 		hw_buffer_list = (u64 *)work->packet_data;
@@ -359,6 +364,8 @@ skip_xmit:
 	switch (queue_type) {
 	case QUEUE_DROP:
 		cvmx_hwfau_atomic_add32(priv->tx_queue[qos].fau, -1);
+		/* Fall through */
+	case QUEUE_DROP_NO_DEC:
 		dev_kfree_skb_any(skb);
 		dev->stats.tx_dropped++;
 		if (work)
@@ -373,7 +380,8 @@ skip_xmit:
 	default:
 		BUG();
 	}
-
+	preempt_enable();
+post_preempt_out:
 	if (USE_ASYNC_IOBDMA) {
 		CVMX_SYNCIOBDMA;
 		/* Restore the scratch area */
-- 
2.6.2

