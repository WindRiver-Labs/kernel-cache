From 82ebde585009d3ccc27c1adbdc998faf42434150 Mon Sep 17 00:00:00 2001
From: Carlos Munoz <cmunoz@caviumnetworks.com>
Date: Tue, 15 Jul 2014 18:05:50 -0700
Subject: [PATCH 888/974] MIPS: OCTEON: Create fpa3 standalone driver.

Signed-off-by: Carlos Munoz <cmunoz@caviumnetworks.com>
[Original patch taken from Cavium SDK 3.1.2-568]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
---
 arch/mips/cavium-octeon/Kconfig                |  11 ++
 arch/mips/cavium-octeon/Makefile               |   1 +
 arch/mips/cavium-octeon/octeon-fpa3.c          | 234 +++++++++++++++++++++++++
 arch/mips/include/asm/octeon/octeon.h          |  13 ++
 drivers/net/ethernet/octeon/Kconfig            |   1 +
 drivers/net/ethernet/octeon/octeon3-ethernet.c | 151 ++++------------
 6 files changed, 295 insertions(+), 116 deletions(-)
 create mode 100644 arch/mips/cavium-octeon/octeon-fpa3.c

diff --git a/arch/mips/cavium-octeon/Kconfig b/arch/mips/cavium-octeon/Kconfig
index 8fc2ef2..eb0b8c3 100644
--- a/arch/mips/cavium-octeon/Kconfig
+++ b/arch/mips/cavium-octeon/Kconfig
@@ -189,6 +189,7 @@ config CAVIUM_OCTEON_RAPIDIO
 	bool "Enable support for Octeon Serial Rapid IO"
 	select RAPIDIO
 	select OCTEON_ETHERNET_MEM
+	select OCTEON_FPA3
 	help
 	  Connect the SRIO interfaces available in the Octeon II series of
 	  processors to the kernel's RapidIO subsystem. The existence of the
@@ -221,6 +222,16 @@ config CAVIUM_OCTEON_PERF
 	help
 	  support extra performance counters, including L2 cache & DRAM controller
 
+config OCTEON_FPA3
+	tristate "Octeon III fpa driver"
+	default "n"
+	depends on CPU_CAVIUM_OCTEON
+	help
+	  This option enables a Octeon III driver for the Free Pool Unit (FPA).
+	  The FPA is a hardware unit that manages pools of pointers to free
+	  L2/DRAM memory. This driver provides an interface to reserve,
+	  initialize, and fill fpa pools.
+
 config ARCH_SPARSEMEM_ENABLE
 	def_bool y
 	select SPARSEMEM_STATIC
diff --git a/arch/mips/cavium-octeon/Makefile b/arch/mips/cavium-octeon/Makefile
index b0ffbe5..3fb62a5 100644
--- a/arch/mips/cavium-octeon/Makefile
+++ b/arch/mips/cavium-octeon/Makefile
@@ -32,6 +32,7 @@ obj-$(CONFIG_CAVIUM_OCTEON_ERROR_TREE)	+= octeon-error-tree.o octeon-78xx-errors
 obj-$(CONFIG_CAVIUM_OCTEON_KERNEL_CRYPTO) += octeon-crypto.o
 obj-$(CONFIG_OCTEON_ERROR_INJECTOR)	+= octeon-error-injector.o
 obj-$(CONFIG_CAVIUM_OCTEON_RAPIDIO)	+= octeon-rapidio.o
+obj-$(CONFIG_OCTEON_FPA3)		+= octeon-fpa3.o
 
 obj-$(CONFIG_PERF_EVENTS)		+= perf_counters.o
 obj-$(CONFIG_CAVIUM_OCTEON_PERF)	+= perf_uncore.o
diff --git a/arch/mips/cavium-octeon/octeon-fpa3.c b/arch/mips/cavium-octeon/octeon-fpa3.c
new file mode 100644
index 0000000..8c6c4a5
--- /dev/null
+++ b/arch/mips/cavium-octeon/octeon-fpa3.c
@@ -0,0 +1,234 @@
+/*
+ * Driver for the Octeon III Free Pool Unit (fpa).
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2015 Cavium Networks, Inc.
+ */
+
+#include <linux/module.h>
+
+#include <asm/octeon/octeon.h>
+
+static DEFINE_MUTEX(octeon_fpa3_lock);
+
+/*
+ * octeon_fpa3_init:		Initialize the fpa to default values.
+ *
+ *  node:			Node of fpa to initialize.
+ *
+ *  Returns:			Zero on success, error otherwise.
+ */
+int octeon_fpa3_init(int node)
+{
+	union cvmx_fpa_gen_cfg	fpa_cfg;
+	static int		init_done;
+	int			i;
+	int			aura_cnt;
+
+	mutex_lock(&octeon_fpa3_lock);
+
+	if (init_done)
+		goto done;
+
+	aura_cnt = cvmx_fpa3_num_auras();
+	for (i = 0; i < aura_cnt; i++) {
+		cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT(i),
+				    0x100000000ull);
+		cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT_LIMIT(i),
+				    0xfffffffffull);
+		cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT_THRESHOLD(i),
+				    0xffffffffeull);
+	}
+
+	fpa_cfg.u64 = cvmx_read_csr_node(node, CVMX_FPA_GEN_CFG);
+	fpa_cfg.s.lvl_dly = 3;
+	cvmx_write_csr_node(node, CVMX_FPA_GEN_CFG, fpa_cfg.u64);
+
+	init_done = 1;
+ done:
+	mutex_unlock(&octeon_fpa3_lock);
+	return 0;
+}
+EXPORT_SYMBOL(octeon_fpa3_init);
+
+/*
+ * octeon_fpa3_pool_init:	Initialize a pool.
+ *
+ *  node:			Node to initialize pool on.
+ *  pool_num:			Requested pool number (-1 for don't care).
+ *  pool:			Updated with the initialized pool number.
+ *  pool_stack:			Updated with the base of the memory allocated
+ *				for the pool stack.
+ *  num_ptrs:			Number of pointers to allocated on the stack.
+ *
+ *  Returns:			Zero on success, error otherwise.
+ */
+int octeon_fpa3_pool_init(int			node,
+			  int			pool_num,
+			  cvmx_fpa3_pool_t	*pool,
+			  void			**pool_stack,
+			  int			num_ptrs)
+{
+	u64				pool_stack_start;
+	u64				pool_stack_end;
+	union cvmx_fpa_poolx_end_addr	limit_addr;
+	union cvmx_fpa_poolx_cfg	cfg;
+	int				stack_size;
+	int				rc = 0;
+
+	mutex_lock(&octeon_fpa3_lock);
+
+	*pool = cvmx_fpa3_reserve_pool(node, pool_num);
+	if (!__cvmx_fpa3_pool_valid(*pool)) {
+		pr_err("Failed to reserve pool=%d\n", pool_num);
+		rc = -ENODEV;
+		goto error;
+	}
+
+	stack_size = (DIV_ROUND_UP(num_ptrs, 29) + 1) * 128;
+
+	cvmx_write_csr_node(node, CVMX_FPA_POOLX_CFG(pool->lpool), 0);
+	cvmx_write_csr_node(node, CVMX_FPA_POOLX_START_ADDR(pool->lpool), 128);
+	limit_addr.u64 = 0;
+	limit_addr.cn78xx.addr = ~0ll;
+	cvmx_write_csr_node(node, CVMX_FPA_POOLX_END_ADDR(pool->lpool),
+			    limit_addr.u64);
+
+	*pool_stack = kmalloc_node(stack_size, GFP_KERNEL, node);
+	if (!*pool_stack) {
+		pr_err("Failed to allocate pool stack memory pool=%d\n",
+		       pool_num);
+		rc = -ENOMEM;
+		goto error_stack;
+	}
+
+	pool_stack_start = virt_to_phys(*pool_stack);
+	pool_stack_end = round_down(pool_stack_start + stack_size, 128);
+	pool_stack_start = round_up(pool_stack_start, 128);
+
+	cvmx_write_csr_node(node, CVMX_FPA_POOLX_STACK_BASE(pool->lpool),
+			    pool_stack_start);
+	cvmx_write_csr_node(node, CVMX_FPA_POOLX_STACK_ADDR(pool->lpool),
+			    pool_stack_start);
+	cvmx_write_csr_node(node, CVMX_FPA_POOLX_STACK_END(pool->lpool),
+			    pool_stack_end);
+
+	cfg.u64 = 0;
+	cfg.s.l_type = 2; /* Load with DWB */
+	cfg.s.ena = 1;
+	cvmx_write_csr_node(node, CVMX_FPA_POOLX_CFG(pool->lpool), cfg.u64);
+
+ error_stack:
+	cvmx_fpa3_release_pool(*pool);
+ error:
+	mutex_unlock(&octeon_fpa3_lock);
+	return rc;
+}
+EXPORT_SYMBOL(octeon_fpa3_pool_init);
+
+/*
+ * octeon_fpa3_aura_init:	Initialize an aura.
+ *
+ *  pool:			Pool the aura belongs to.
+ *  aura_num:			Requested aura number (-1 for don't care).
+ *  aura:			Updated with the initialized aura number.
+ *  num_bufs:			Number of buffers in the aura.
+ *  limit:			Limit for the aura.
+ *
+ *  Returns:			Zero on success, error otherwise.
+ */
+int octeon_fpa3_aura_init(cvmx_fpa3_pool_t	pool,
+			  int			aura_num,
+			  cvmx_fpa3_gaura_t	*aura,
+			  int			num_bufs,
+			  unsigned int		limit)
+{
+	union cvmx_fpa_aurax_cnt_levels	cnt_levels;
+	int				shift;
+	unsigned int			drop;
+	unsigned int			pass;
+	int				rc = 0;
+
+	mutex_lock(&octeon_fpa3_lock);
+
+	*aura = cvmx_fpa3_reserve_aura(pool.node, aura_num);
+	if (!__cvmx_fpa3_aura_valid(*aura)) {
+		pr_err("Failed to reserved aura=%d\n", aura_num);
+		rc = -ENODEV;
+		goto error;
+	}
+
+	limit *= 2; /* Allow twice the limit before saturation at zero. */
+
+	cvmx_write_csr_node(aura->node, CVMX_FPA_AURAX_CFG(aura->laura), 0);
+	cvmx_write_csr_node(aura->node, CVMX_FPA_AURAX_CNT_LIMIT(aura->laura),
+			    limit);
+	cvmx_write_csr_node(aura->node, CVMX_FPA_AURAX_CNT(aura->laura), limit);
+	cvmx_write_csr_node(aura->node, CVMX_FPA_AURAX_POOL(aura->laura),
+			    pool.lpool);
+	cvmx_write_csr_node(aura->node, CVMX_FPA_AURAX_POOL_LEVELS(aura->laura),
+			    0); /* No per-pool RED/Drop */
+
+	shift = 0;
+	while ((limit >> shift) > 255)
+		shift++;
+
+	drop = (limit - num_bufs / 20) >> shift;	/* 95% */
+	pass = (limit - (num_bufs * 3) / 20) >> shift;	/* 85% */
+
+	cnt_levels.u64 = 0;
+	cnt_levels.s.shift = shift;
+	cnt_levels.s.red_ena = 1;
+	cnt_levels.s.drop = drop;
+	cnt_levels.s.pass = pass;
+	cvmx_write_csr_node(aura->node, CVMX_FPA_AURAX_CNT_LEVELS(aura->laura),
+			    cnt_levels.u64);
+
+ error:
+	mutex_unlock(&octeon_fpa3_lock);
+	return rc;
+}
+EXPORT_SYMBOL(octeon_fpa3_aura_init);
+
+/*
+ * octeon_mem_fill_fpa3:	Add buffers to an aura.
+ *
+ *  node:			Node to get memory from.
+ *  cache:			Memory cache to allocate from.
+ *  aura:			Aura to add buffers to.
+ *  num_bufs:			Number of buffers to add to the aura.
+ *
+ *  Returns:			Zero on success, error otherwise.
+ */
+int octeon_mem_fill_fpa3(int			node,
+			 struct kmem_cache	*cache,
+			 cvmx_fpa3_gaura_t	aura,
+			 int			num_bufs)
+{
+	void	*mem;
+	int	i;
+	int	rc = 0;
+
+	mutex_lock(&octeon_fpa3_lock);
+
+	for (i = 0; i < num_bufs; i++) {
+		mem = kmem_cache_alloc_node(cache, GFP_KERNEL, node);
+		if (!mem) {
+			pr_err("Failed to allocate memory for aura=%d\n",
+			       aura.laura);
+			rc = -ENOMEM;
+			break;
+		}
+		cvmx_fpa3_free(mem, aura, 0);
+	}
+
+	mutex_unlock(&octeon_fpa3_lock);
+	return rc;
+}
+EXPORT_SYMBOL(octeon_mem_fill_fpa3);
+
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Cavium, Inc. Octeon III FPA manager.");
diff --git a/arch/mips/include/asm/octeon/octeon.h b/arch/mips/include/asm/octeon/octeon.h
index 0a2665a..b565f5e 100644
--- a/arch/mips/include/asm/octeon/octeon.h
+++ b/arch/mips/include/asm/octeon/octeon.h
@@ -12,7 +12,9 @@
 #include <linux/irqflags.h>
 #include <linux/notifier.h>
 #include <asm/octeon/cvmx.h>
+#include <asm/octeon/cvmx-fpa3.h>
 #include <linux/irq.h>
+#include <linux/slab.h>
 
 extern int octeon_is_simulation(void);
 extern int octeon_is_pci_host(void);
@@ -482,4 +484,15 @@ void octeon_mult_restore3_end(void);
 void octeon_mult_restore2(void);
 void octeon_mult_restore2_end(void);
 
+#if CONFIG_OCTEON_FPA3
+int octeon_fpa3_init(int node);
+int octeon_fpa3_pool_init(int node, int pool_num, cvmx_fpa3_pool_t *pool,
+			  void **pool_stack, int num_ptrs);
+int octeon_fpa3_aura_init(cvmx_fpa3_pool_t pool, int aura_num,
+			  cvmx_fpa3_gaura_t *aura, int num_bufs,
+			  unsigned int limit);
+int octeon_mem_fill_fpa3(int node, struct kmem_cache *cache,
+			  cvmx_fpa3_gaura_t aura, int num_bufs);
+#endif
+
 #endif /* __ASM_OCTEON_OCTEON_H */
diff --git a/drivers/net/ethernet/octeon/Kconfig b/drivers/net/ethernet/octeon/Kconfig
index 28f6f7b..6b9e510 100644
--- a/drivers/net/ethernet/octeon/Kconfig
+++ b/drivers/net/ethernet/octeon/Kconfig
@@ -15,6 +15,7 @@ config OCTEON3_ETHERNET
 	tristate "Cavium Inc. OCTEON III PKI/PKO Ethernet support (not cn70xx)"
 	depends on CAVIUM_OCTEON_SOC
 	select OCTEON_BGX_PORT
+	select OCTEON_FPA3
 	help
 	  Support for 'BGX' Ethernet via PKI/PKO units.  No support
 	  for cn70xx chips (use OCTEON_ETHERNET for cn70xx)
diff --git a/drivers/net/ethernet/octeon/octeon3-ethernet.c b/drivers/net/ethernet/octeon/octeon3-ethernet.c
index 682186c..50beae6 100644
--- a/drivers/net/ethernet/octeon/octeon3-ethernet.c
+++ b/drivers/net/ethernet/octeon/octeon3-ethernet.c
@@ -287,6 +287,7 @@ MODULE_PARM_DESC(rx_contexts, "Number of RX threads per port.");
 static struct octeon3_ethernet_node octeon3_eth_node[OCTEON3_ETH_MAX_NUMA_NODES];
 static struct kmem_cache *octeon3_eth_sso_pko_cache;
 
+
 /**
  * octeon3_register_callback:	Register a intercept callback for the named
  *				device.
@@ -396,73 +397,6 @@ static void octeon3_eth_gen_affinity(int node, cpumask_t *mask)
 	cpumask_set_cpu(cpu, mask);
 }
 
-static int octeon3_eth_fpa_pool_init(cvmx_fpa3_pool_t	pool,
-				     void		**pool_stack,
-				     int		num_ptrs)
-{
-	u64 pool_stack_start, pool_stack_end;
-	union cvmx_fpa_poolx_end_addr limit_addr;
-	union cvmx_fpa_poolx_cfg cfg;
-	int stack_size = (DIV_ROUND_UP(num_ptrs, 29) + 1) * 128;
-
-	cvmx_write_csr_node(pool.node, CVMX_FPA_POOLX_CFG(pool.lpool), 0);
-	cvmx_write_csr_node(pool.node, CVMX_FPA_POOLX_START_ADDR(pool.lpool), 128);
-	limit_addr.u64 = 0;
-	limit_addr.cn78xx.addr = ~0ll;
-	cvmx_write_csr_node(pool.node, CVMX_FPA_POOLX_END_ADDR(pool.lpool), limit_addr.u64);
-
-	*pool_stack = kmalloc_node(stack_size, GFP_KERNEL, pool.node);
-	if (!*pool_stack)
-		return -ENOMEM;
-
-	pool_stack_start = virt_to_phys(*pool_stack);
-	pool_stack_end = round_down(pool_stack_start + stack_size, 128);
-	pool_stack_start = round_up(pool_stack_start, 128);
-
-	cvmx_write_csr_node(pool.node, CVMX_FPA_POOLX_STACK_BASE(pool.lpool), pool_stack_start);
-	cvmx_write_csr_node(pool.node, CVMX_FPA_POOLX_STACK_ADDR(pool.lpool), pool_stack_start);
-	cvmx_write_csr_node(pool.node, CVMX_FPA_POOLX_STACK_END(pool.lpool), pool_stack_end);
-
-	cfg.u64 = 0;
-	cfg.s.l_type = 2; /* Load with DWB */
-	cfg.s.ena = 1;
-	cvmx_write_csr_node(pool.node, CVMX_FPA_POOLX_CFG(pool.lpool), cfg.u64);
-	return 0;
-}
-
-static int octeon3_eth_fpa_aura_init(cvmx_fpa3_pool_t pool, cvmx_fpa3_gaura_t aura, unsigned int limit)
-{
-	int shift;
-	union cvmx_fpa_aurax_cnt_levels cnt_levels;
-	unsigned int drop, pass;
-
-	BUG_ON(aura.node != pool.node);
-
-	limit *= 2; /* allow twice the limit before saturation at zero. */
-
-	cvmx_write_csr_node(aura.node, CVMX_FPA_AURAX_CFG(aura.laura), 0);
-	cvmx_write_csr_node(aura.node, CVMX_FPA_AURAX_CNT_LIMIT(aura.laura), limit);
-	cvmx_write_csr_node(aura.node, CVMX_FPA_AURAX_CNT(aura.laura), limit);
-	cvmx_write_csr_node(aura.node, CVMX_FPA_AURAX_POOL(aura.laura), pool.lpool);
-	cvmx_write_csr_node(aura.node, CVMX_FPA_AURAX_POOL_LEVELS(aura.laura), 0); /* No per-pool RED/Drop */
-
-	shift = 0;
-	while ((limit >> shift) > 255)
-		shift++;
-
-	drop = (limit - num_packet_buffers / 20) >> shift;	  /* 95% */
-	pass = (limit - (num_packet_buffers * 3) / 20) >> shift;  /* 85% */
-
-	cnt_levels.u64 = 0;
-	cnt_levels.s.shift = shift;
-	cnt_levels.s.red_ena = 1;
-	cnt_levels.s.drop = drop;
-	cnt_levels.s.pass = pass;
-	cvmx_write_csr_node(aura.node, CVMX_FPA_AURAX_CNT_LEVELS(aura.laura), cnt_levels.u64);
-
-	return 0;
-}
-
 static int octeon3_eth_sso_init(unsigned int node, int aura)
 {
 	union cvmx_sso_aw_cfg cfg;
@@ -788,7 +722,6 @@ static int octeon3_eth_global_init(unsigned int node)
 	struct irq_domain *d;
 	unsigned int sso_intsn;
 	struct octeon3_ethernet_node *oen;
-	union cvmx_fpa_gen_cfg fpa_cfg;
 
 	mutex_lock(&octeon3_eth_init_mutex);
 
@@ -811,46 +744,38 @@ static int octeon3_eth_global_init(unsigned int node)
 	spin_lock_init(&oen->napi_alloc_lock);
 
 	oen->numa_node = node;
-	for (i = 0; i < 1024; i++) {
-		cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT(i), 0x100000000ull);
-		cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT_LIMIT(i), 0xfffffffffull);
-		cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT_THRESHOLD(i), 0xffffffffeull);
-	}
 
-	fpa_cfg.u64 = cvmx_read_csr_node(node, CVMX_FPA_GEN_CFG);
-	fpa_cfg.s.lvl_dly = 3;
-	cvmx_write_csr_node(node, CVMX_FPA_GEN_CFG, fpa_cfg.u64);
+	octeon_fpa3_init(node);
+	rv = octeon_fpa3_pool_init(node, -1, &oen->sso_pool,
+				   &oen->sso_pool_stack, 40960);
+	if (rv)
+		goto done;
 
-	oen->sso_pool = cvmx_fpa3_reserve_pool(node, -1);
-	if (!__cvmx_fpa3_pool_valid(oen->sso_pool)) {
-		rv = -ENODEV;
+	rv = octeon_fpa3_pool_init(node, -1, &oen->pko_pool,
+				   &oen->pko_pool_stack, 40960);
+	if (rv)
 		goto done;
-	}
-	oen->pko_pool = cvmx_fpa3_reserve_pool(node, -1);
-	if (!__cvmx_fpa3_pool_valid(oen->pko_pool)) {
-		rv = -ENODEV;
+
+	rv = octeon_fpa3_pool_init(node, -1, &oen->pki_packet_pool,
+				   &oen->pki_packet_pool_stack,
+				   64 * num_packet_buffers);
+	if (rv)
 		goto done;
-	}
-	oen->pki_packet_pool = cvmx_fpa3_reserve_pool(node, -1);
-	if (!__cvmx_fpa3_pool_valid(oen->pki_packet_pool)) {
-		rv = -ENODEV;
+
+	rv = octeon_fpa3_aura_init(oen->sso_pool, -1, &oen->sso_aura,
+				   num_packet_buffers, 20480);
+	if (rv)
+		goto done;
+
+	rv = octeon_fpa3_aura_init(oen->pko_pool, -1, &oen->pko_aura,
+				   num_packet_buffers, 20480);
+	if (rv)
 		goto done;
-	}
-	oen->sso_aura = cvmx_fpa3_reserve_aura(node, -1);
-	oen->pko_aura = cvmx_fpa3_reserve_aura(node, -1);
 
 	pr_err("octeon3_eth_global_init  SSO:%d:%d, PKO:%d:%d\n",
 	       oen->sso_pool.lpool, oen->sso_aura.laura,
 	       oen->pko_pool.lpool, oen->pko_aura.laura);
 
-	octeon3_eth_fpa_pool_init(oen->sso_pool, &oen->sso_pool_stack, 40960);
-	octeon3_eth_fpa_pool_init(oen->pko_pool, &oen->pko_pool_stack, 40960);
-	octeon3_eth_fpa_pool_init(oen->pki_packet_pool,
-				  &oen->pki_packet_pool_stack,
-				  64 * num_packet_buffers);
-	octeon3_eth_fpa_aura_init(oen->sso_pool, oen->sso_aura, 20480);
-	octeon3_eth_fpa_aura_init(oen->pko_pool, oen->pko_aura, 20480);
-
 	if (!octeon3_eth_sso_pko_cache) {
 		octeon3_eth_sso_pko_cache = kmem_cache_create("sso_pko", 4096, 128, 0, NULL);
 		if (!octeon3_eth_sso_pko_cache) {
@@ -858,21 +783,16 @@ static int octeon3_eth_global_init(unsigned int node)
 			goto done;
 		}
 	}
-	for (i = 0; i < 1024; i++) {
-		void *mem;
-		mem = kmem_cache_alloc_node(octeon3_eth_sso_pko_cache, GFP_KERNEL, node);
-		if (!mem) {
-			rv = -ENOMEM;
-			goto done;
-		}
-		cvmx_fpa3_free(mem, oen->sso_aura, 0);
-		mem = kmem_cache_alloc_node(octeon3_eth_sso_pko_cache, GFP_KERNEL, node);
-		if (!mem) {
-			rv = -ENOMEM;
-			goto done;
-		}
-		cvmx_fpa3_free(mem, oen->pko_aura, 0);
-	}
+
+	rv = octeon_mem_fill_fpa3(node, octeon3_eth_sso_pko_cache,
+				  oen->sso_aura, 1024);
+	if (rv)
+		goto done;
+
+	rv = octeon_mem_fill_fpa3(node, octeon3_eth_sso_pko_cache,
+				   oen->pko_aura, 1024);
+	if (rv)
+		goto done;
 
 	BUG_ON(node != oen->sso_aura.node);
 	rv = octeon3_eth_sso_init(node, oen->sso_aura.laura);
@@ -1559,10 +1479,9 @@ static int octeon3_eth_ndo_init(struct net_device *netdev)
 	xdq = cvmx_helper_ipd_port_to_xport(node_dq);
 
 	priv->pko_queue = xdq.port;
-	aura = cvmx_fpa3_reserve_aura(priv->numa_node, -1);
+	octeon_fpa3_aura_init(oen->pki_packet_pool, -1, &aura,
+			      num_packet_buffers, num_packet_buffers * 2);
 	priv->pki_laura = aura.laura;
-	octeon3_eth_fpa_aura_init(oen->pki_packet_pool, aura,
-				  num_packet_buffers * 2);
 	aura2bufs_needed[priv->numa_node][priv->pki_laura] =
 		&priv->buffers_needed;
 
-- 
2.6.2

