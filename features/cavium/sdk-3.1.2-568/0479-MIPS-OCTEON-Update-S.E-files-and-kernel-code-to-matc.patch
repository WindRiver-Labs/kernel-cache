From c1c9d5db4d63d2e0066567f889894eaea7b5326a Mon Sep 17 00:00:00 2001
From: David Daney <david.daney@cavium.com>
Date: Thu, 20 Feb 2014 13:46:59 -0800
Subject: [PATCH 479/974] MIPS/OCTEON: Update S.E files and kernel code to
 match.

Signed-off-by: David Daney <david.daney@cavium.com>
[Original patch taken from Cavium SDK 3.1.2-568]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
---
 arch/mips/cavium-octeon/executive/Makefile         |   2 +-
 .../executive/cvmx-appcfg-transport.c              |   4 +-
 arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c |  16 +-
 .../mips/cavium-octeon/executive/cvmx-dma-engine.c |   3 +-
 .../cavium-octeon/executive/cvmx-fpa-resource.c    |  51 +-
 .../executive/cvmx-global-resources.c              |   4 -
 .../cavium-octeon/executive/cvmx-helper-board.c    |  14 +-
 .../mips/cavium-octeon/executive/cvmx-helper-cfg.c |  12 +-
 .../cavium-octeon/executive/cvmx-helper-errata.c   |  10 +-
 .../mips/cavium-octeon/executive/cvmx-helper-ilk.c |   4 +-
 .../mips/cavium-octeon/executive/cvmx-helper-ipd.c | 299 ++++++++
 .../mips/cavium-octeon/executive/cvmx-helper-npi.c |   6 +-
 .../mips/cavium-octeon/executive/cvmx-helper-pki.c |  31 +-
 .../mips/cavium-octeon/executive/cvmx-helper-pko.c |  47 +-
 .../cavium-octeon/executive/cvmx-helper-pko3.c     |   9 +-
 .../cavium-octeon/executive/cvmx-helper-rgmii.c    |   6 +-
 .../cavium-octeon/executive/cvmx-helper-util.c     |  60 +-
 arch/mips/cavium-octeon/executive/cvmx-helper.c    | 266 +------
 arch/mips/cavium-octeon/executive/cvmx-ilk.c       |   4 +-
 arch/mips/cavium-octeon/executive/cvmx-ipd.c       |  32 +-
 arch/mips/cavium-octeon/executive/cvmx-nand.c      |   1 -
 arch/mips/cavium-octeon/executive/cvmx-pcie.c      | 490 +++++++-----
 arch/mips/cavium-octeon/executive/cvmx-pki.c       |  48 +-
 arch/mips/cavium-octeon/executive/cvmx-pko.c       |   8 +-
 arch/mips/cavium-octeon/executive/cvmx-pko3.c      | 699 ++++++++++++++++--
 arch/mips/cavium-octeon/executive/cvmx-qlm.c       |   6 +-
 arch/mips/cavium-octeon/executive/cvmx-spi.c       |   6 +-
 arch/mips/cavium-octeon/octeon-rapidio.c           |   4 +-
 arch/mips/include/asm/octeon/cvmx-asm.h            | 228 +++---
 arch/mips/include/asm/octeon/cvmx-cmd-queue.h      |  21 +-
 arch/mips/include/asm/octeon/cvmx-dma-engine.h     |   6 +-
 arch/mips/include/asm/octeon/cvmx-fau.h            | 648 ----------------
 arch/mips/include/asm/octeon/cvmx-fpa-defs.h       |  93 +++
 arch/mips/include/asm/octeon/cvmx-fpa.h            | 818 ---------------------
 arch/mips/include/asm/octeon/cvmx-fpa1.h           | 177 +++++
 arch/mips/include/asm/octeon/cvmx-fpa3.h           | 427 +++++++++++
 arch/mips/include/asm/octeon/cvmx-helper-fpa.h     |  75 +-
 arch/mips/include/asm/octeon/cvmx-helper-ipd.h     |  70 ++
 arch/mips/include/asm/octeon/cvmx-helper-pki.h     |   8 +-
 arch/mips/include/asm/octeon/cvmx-helper-pko.h     |  33 +-
 arch/mips/include/asm/octeon/cvmx-helper-util.h    |  12 +-
 arch/mips/include/asm/octeon/cvmx-helper.h         |   3 +-
 arch/mips/include/asm/octeon/cvmx-hwfau.h          | 648 ++++++++++++++++
 arch/mips/include/asm/octeon/cvmx-hwpko.h          | 665 +++++++++++++++++
 arch/mips/include/asm/octeon/cvmx-ipd.h            |   5 +-
 arch/mips/include/asm/octeon/cvmx-mdio.h           |  90 ++-
 arch/mips/include/asm/octeon/cvmx-pip.h            |   3 +-
 arch/mips/include/asm/octeon/cvmx-pki.h            |  14 +-
 arch/mips/include/asm/octeon/cvmx-pko.h            | 665 -----------------
 arch/mips/include/asm/octeon/cvmx-pko3-queue.h     |   2 +-
 arch/mips/include/asm/octeon/cvmx-pko3.h           |  67 +-
 arch/mips/include/asm/octeon/cvmx-pow.h            | 105 ++-
 arch/mips/include/asm/octeon/cvmx-wqe.h            |  94 ++-
 drivers/net/ethernet/octeon/ethernet-mem.c         |  10 +-
 drivers/net/ethernet/octeon/ethernet-napi.c        |   4 +-
 drivers/net/ethernet/octeon/ethernet-rx.c          |   2 +-
 drivers/net/ethernet/octeon/ethernet-tx.c          |   6 +-
 drivers/net/ethernet/octeon/ethernet-xmit.c        |   6 +-
 drivers/net/ethernet/octeon/ethernet.c             |  10 +-
 drivers/net/ethernet/octeon/octeon-ethernet.h      |   2 +-
 drivers/net/ethernet/octeon/octeon-pow-ethernet.c  |  14 +-
 61 files changed, 4213 insertions(+), 2960 deletions(-)
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-helper-ipd.c
 delete mode 100644 arch/mips/include/asm/octeon/cvmx-fau.h
 delete mode 100644 arch/mips/include/asm/octeon/cvmx-fpa.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-fpa1.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-fpa3.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-helper-ipd.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-hwfau.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-hwpko.h
 delete mode 100644 arch/mips/include/asm/octeon/cvmx-pko.h

diff --git a/arch/mips/cavium-octeon/executive/Makefile b/arch/mips/cavium-octeon/executive/Makefile
index 166b521..79b18f7 100644
--- a/arch/mips/cavium-octeon/executive/Makefile
+++ b/arch/mips/cavium-octeon/executive/Makefile
@@ -21,7 +21,7 @@ obj-y += cvmx-pko.o cvmx-spi.o cvmx-cmd-queue.o cvmx-helper-cfg.o	\
 	cvmx-helper-rgmii.o cvmx-helper-sgmii.o cvmx-helper-npi.o \
 	cvmx-helper-loop.o cvmx-helper-spi.o cvmx-helper-util.o	\
 	cvmx-pki-resources.o cvmx-gser.o cvmx-bgx.o cvmx-pko3-queue.o cvmx-helper-bgx.o \
-	cvmx-pko3.o cvmx-helper-pki.o cvmx-helper-pko3.o cvmx-pko3-resources.o cvmx-helper-pko.o
+	cvmx-pko3.o cvmx-helper-pki.o cvmx-helper-pko3.o cvmx-pko3-resources.o cvmx-helper-pko.o cvmx-helper-ipd.o
 
 obj-y += cvmx-helper-errata.o cvmx-helper-jtag.o
 obj-y += cvmx-pcie.o
diff --git a/arch/mips/cavium-octeon/executive/cvmx-appcfg-transport.c b/arch/mips/cavium-octeon/executive/cvmx-appcfg-transport.c
index 3b14fc6..cb8c875 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-appcfg-transport.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-appcfg-transport.c
@@ -41,7 +41,7 @@
 #include <linux/export.h>
 #include <asm/octeon/cvmx.h>
 #include <asm/octeon/cvmx-bootmem.h>
-#include <asm/octeon/cvmx-fpa.h>
+#include <asm/octeon/cvmx-fpa1.h>
 #include <asm/octeon/cvmx-ipd.h>
 #include <asm/octeon/cvmx-helper.h>
 #include <asm/octeon/cvmx-helper-cfg.h>
@@ -50,7 +50,7 @@
 #else
 #include "cvmx.h"
 #include "cvmx-bootmem.h"
-#include "cvmx-fpa.h"
+#include "cvmx-fpa1.h"
 #include "cvmx-ipd.h"
 #include "cvmx-helper.h"
 #include "cvmx-helper-cfg.h"
diff --git a/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c b/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c
index 2c474f3..b60ec23 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c
@@ -43,7 +43,7 @@
  * Support functions for managing command queues used for
  * various hardware blocks.
  *
- * <hr>$Revision: 90763 $<hr>
+ * <hr>$Revision: 95258 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <linux/export.h>
@@ -53,12 +53,12 @@
 #include <asm/octeon/cvmx-pexp-defs.h>
 #include <asm/octeon/cvmx-dpi-defs.h>
 #include <asm/octeon/cvmx-pko-defs.h>
-#include <asm/octeon/cvmx-fpa.h>
+#include <asm/octeon/cvmx-fpa1.h>
 #include <asm/octeon/cvmx-cmd-queue.h>
 #else
 #include "cvmx.h"
 #include "cvmx-bootmem.h"
-#include "cvmx-fpa.h"
+#include "cvmx-fpa1.h"
 #include "cvmx-cmd-queue.h"
 #endif
 
@@ -183,7 +183,7 @@ cvmx_cmd_queue_result_t cvmx_cmd_queue_initialize(cvmx_cmd_queue_id_t queue_id,
 				return CVMX_CMD_QUEUE_NO_MEMORY;
 			}
 		}
-		buffer = cvmx_fpa_alloc(fpa_pool);
+		buffer = __cvmx_cmd_queue_alloc_buffer(fpa_pool);
 		if (buffer == NULL) {
 			cvmx_dprintf("ERROR: cvmx_cmd_queue_initialize: Unable to allocate initial buffer.\n");
 			return CVMX_CMD_QUEUE_NO_MEMORY;
@@ -228,8 +228,12 @@ cvmx_cmd_queue_result_t cvmx_cmd_queue_shutdown(cvmx_cmd_queue_id_t queue_id)
 
 	__cvmx_cmd_queue_lock(queue_id, qptr);
 	if (qptr->base_ptr_div128) {
-		cvmx_fpa_free(cvmx_phys_to_ptr((uint64_t) qptr->base_ptr_div128 << 7),
-			      qptr->fpa_pool, 0);
+		if (octeon_has_feature(OCTEON_FEATURE_FPA3))
+			cvmx_fpa_free_aura(cvmx_phys_to_ptr((uint64_t) qptr->base_ptr_div128 << 7),
+					   0, qptr->fpa_pool, 0);
+		else
+			cvmx_fpa1_free(cvmx_phys_to_ptr((uint64_t) qptr->base_ptr_div128 << 7),
+				       qptr->fpa_pool, 0);
 		qptr->base_ptr_div128 = 0;
 	}
 	__cvmx_cmd_queue_unlock(qptr);
diff --git a/arch/mips/cavium-octeon/executive/cvmx-dma-engine.c b/arch/mips/cavium-octeon/executive/cvmx-dma-engine.c
index c931ca5..a15b31a 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-dma-engine.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-dma-engine.c
@@ -43,7 +43,7 @@
  * Interface to the PCI / PCIe DMA engines. These are only avialable
  * on chips with PCI / PCIe.
  *
- * <hr>$Revision: 94747 $<hr>
+ * <hr>$Revision: 95258 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <linux/export.h>
@@ -64,6 +64,7 @@
 #include "cvmx-dma-engine.h"
 #include "cvmx-helper-cfg.h"
 #include "cvmx-helper-fpa.h"
+#include "cvmx-fpa.h"
 #endif
 
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-fpa-resource.c b/arch/mips/cavium-octeon/executive/cvmx-fpa-resource.c
index 282962b..87af418 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-fpa-resource.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-fpa-resource.c
@@ -40,11 +40,13 @@
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include "linux/export.h"
 #include "asm/octeon/cvmx.h"
-#include "asm/octeon/cvmx-fpa.h"
+#include "asm/octeon/cvmx-fpa1.h"
+#include "asm/octeon/cvmx-fpa3.h"
 #include "asm/octeon/cvmx-global-resources.h"
 #else
 #include "cvmx.h"
-#include "cvmx-fpa.h"
+#include "cvmx-fpa1.h"
+#include "cvmx-fpa3.h"
 #include "cvmx-global-resources.h"
 #include "cvmx-sysinfo.h"
 #endif
@@ -57,7 +59,7 @@
   */
 int cvmx_fpa_alloc_pool(int pool)
 {
-	if (cvmx_create_global_resource_range(CVMX_GR_TAG_FPA, CVMX_FPA_NUM_POOLS)) {
+	if (cvmx_create_global_resource_range(CVMX_GR_TAG_FPA, CVMX_FPA3_NUM_POOLS)) {
 		cvmx_dprintf("\nFailed to create FPA global resource");
 		return -1;
 	}
@@ -111,25 +113,6 @@ static inline struct global_resource_tag get_aura_resourse_tag(int node)
 	}
 }
 
-int cvmx_fpa_reserve_compat(int node)
-{
-	int pool_num = 1, aura_num = 1;
-
-	if (!octeon_has_feature(OCTEON_FEATURE_FPA3))
-		return 0;
-
-	if (node == -1) node = cvmx_get_node_num();
-
-	if (cvmx_sysinfo_get()->init_core != cvmx_get_core_num())
-		return 0;
-	cvmx_fpa_allocate_fpa_pools(node, &pool_num, 4);
-	cvmx_fpa_allocate_auras(node, &aura_num, 4);
-
-	return 0;
-}
-
-
-
 /**
  * This will allocate/reserve count number of FPA pools on the specified node to the
  * calling application. These pools will be for exclusive use of the application
@@ -142,7 +125,7 @@ int cvmx_fpa_reserve_compat(int node)
  */
 int cvmx_fpa_allocate_fpa_pools(int node, int pools_allocated[], int count)
 {
-	int num_pools = CVMX_FPA_NUM_POOLS;
+	int num_pools = CVMX_FPA1_NUM_POOLS;
 	uint64_t owner = 0;
 	int rv = 0;
 	struct global_resource_tag tag = get_fpa_resourse_tag(node);
@@ -167,6 +150,22 @@ int cvmx_fpa_allocate_fpa_pools(int node, int pools_allocated[], int count)
 	return rv;
 }
 
+int cvmx_fpa_reserve_compat(int node)
+{
+	int pool_num = 1, aura_num = 1;
+
+	if (!octeon_has_feature(OCTEON_FEATURE_FPA3))
+		return 0;
+
+	if (node == -1) node = cvmx_get_node_num();
+
+	if (cvmx_sysinfo_get()->init_core != cvmx_get_core_num())
+		return 0;
+	cvmx_fpa_allocate_fpa_pools(node, &pool_num, 4);
+	cvmx_fpa3_allocate_auras(node, &aura_num, 4);
+
+	return 0;
+}
 
 /** Release/Frees the specified pool
   * @param pool	    Pool to free
@@ -181,9 +180,9 @@ int cvmx_fpa_release_pool(int pool)
 	return 0;
 }
 
-int cvmx_fpa_allocate_auras(int node, int auras_allocated[], int count)
+int cvmx_fpa3_allocate_auras(int node, int auras_allocated[], int count)
 {
-	int num_aura = CVMX_FPA_AURA_NUM;
+	int num_aura = CVMX_FPA3_AURA_NUM;
 	uint64_t owner = 0;
 	int rv = 0;
 	struct global_resource_tag tag = get_aura_resourse_tag(node);
@@ -208,7 +207,7 @@ int cvmx_fpa_allocate_auras(int node, int auras_allocated[], int count)
 
 }
 
-int cvmx_fpa_free_auras(int node, int *pools_allocated, int count)
+int cvmx_fpa3_free_auras(int node, int *pools_allocated, int count)
 {
 	int rv;
 	struct global_resource_tag tag = get_aura_resourse_tag(node);
diff --git a/arch/mips/cavium-octeon/executive/cvmx-global-resources.c b/arch/mips/cavium-octeon/executive/cvmx-global-resources.c
index 9a870e0..355dffe 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-global-resources.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-global-resources.c
@@ -5,8 +5,6 @@
 #include "asm/octeon/cvmx-bootmem.h"
 #include "asm/octeon/cvmx.h"
 #include "asm/octeon/cvmx-helper-cfg.h"
-#include "asm/octeon/cvmx-fpa.h"
-#include "asm/octeon/cvmx-fau.h"
 #include "asm/octeon/cvmx-range.h"
 #else
 #include "cvmx.h"
@@ -14,8 +12,6 @@
 #include "cvmx-global-resources.h"
 #include "cvmx-bootmem.h"
 #include "cvmx-helper-cfg.h"
-#include "cvmx-fpa.h"
-#include "cvmx-fau.h"
 #include "cvmx-range.h"
 #endif
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-board.c b/arch/mips/cavium-octeon/executive/cvmx-helper-board.c
index a91c1fb..f62371a 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-board.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-board.c
@@ -406,20 +406,24 @@ int __cvmx_helper_board_get_port_from_dt(void *fdt_addr, int ipd_port)
 
 	aliases = fdt_path_offset(fdt_addr, "/aliases");
 	if (aliases < 0) {
-		cvmx_dprintf("%s: Error: /aliases not found in device tree fdt_addr=%p\n",
-			     __func__, fdt_addr);
+		cvmx_dprintf("%s: ERROR: "
+			"/aliases not found in device tree fdt_addr=%p\n",
+			__func__, fdt_addr);
 		return -1;
 	}
 
 	pip_path = fdt_getprop(fdt_addr, aliases, "pip", NULL);
 	if (!pip_path) {
-		cvmx_dprintf("%s: Error: pip path not found in device tree\n",
-			     __func__);
+		cvmx_dprintf("%s: ERROR: "
+			"interface %u pip path not found in device tree\n",
+		         __func__, interface_num);
 		return -1;
 	}
 	pip = fdt_path_offset(fdt_addr, pip_path);
 	if (pip < 0) {
-		cvmx_dprintf("%s: Error: pip not found in device tree\n", __func__);
+		cvmx_dprintf("%s: ERROR: "
+			"interface %u pip not found in device tree\n",
+			__func__, interface_num);
 		return -1;
 	}
 	snprintf(name_buffer, sizeof(name_buffer), "interface@%d",
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c b/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c
index 7a75784..45d6682 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c
@@ -465,6 +465,8 @@ void cvmx_helper_cfg_set_jabber_and_frame_max()
 	int interface, port;
 
 	if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+		int node = cvmx_get_node_num();
+
 		/*Set the frame max size and jabber size to 65535. */
 		for (interface = 0; interface < cvmx_helper_get_number_of_interfaces(); interface++) {
 			/* Set the frame max size and jabber size to 65535, as the defaults
@@ -474,17 +476,19 @@ void cvmx_helper_cfg_set_jabber_and_frame_max()
 
 			switch (imode) {
 			case CVMX_HELPER_INTERFACE_MODE_SGMII:
-				cvmx_pip_set_frame_check(interface, -1);
-				for (port = 0; port < num_ports; port++)
+				for (port = 0; port < num_ports; port++) {
+					cvmx_pki_set_max_frm_len(node, port, -1);
 					cvmx_write_csr(CVMX_BGXX_GMP_GMI_RXX_JABBER(port, interface), 65535);
+				}
 				break;
 			case CVMX_HELPER_INTERFACE_MODE_XAUI:
 			case CVMX_HELPER_INTERFACE_MODE_RXAUI:
 			case CVMX_HELPER_INTERFACE_MODE_XLAUI:
 			case CVMX_HELPER_INTERFACE_MODE_XFI:
-				cvmx_pip_set_frame_check(interface, -1);
-				for (port = 0; port < num_ports; port++)
+				for (port = 0; port < num_ports; port++) {
+					cvmx_pki_set_max_frm_len(node, port, -1);
 					cvmx_write_csr(CVMX_BGXX_SMUX_RX_JABBER(port, interface), 65535);
+				}
 				break;
 			default:
 				break;
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-errata.c b/arch/mips/cavium-octeon/executive/cvmx-helper-errata.c
index 4052c51..0ff373d 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-errata.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-errata.c
@@ -45,13 +45,13 @@
  * chip errata. For the most part, code doesn't need to call
  * these functions directly.
  *
- * <hr>$Revision: 94793 $<hr>
+ * <hr>$Revision: 95258 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
 #include <asm/octeon/cvmx-helper.h>
 #include <asm/octeon/cvmx-helper-jtag.h>
-#include <asm/octeon/cvmx-pko.h>
+#include <asm/octeon/cvmx-hwpko.h>
 #include <asm/octeon/cvmx-asxx-defs.h>
 #include <asm/octeon/cvmx-gmxx-defs.h>
 #include <asm/octeon/cvmx-ipd.h>
@@ -61,7 +61,7 @@
 
 #include "cvmx-fpa.h"
 #include "cvmx-pip.h"
-#include "cvmx-pko.h"
+#include "cvmx-hwpko.h"
 #include "cvmx-ipd.h"
 #include "cvmx-spi.h"
 #include "cvmx-pow.h"
@@ -137,7 +137,7 @@ int __cvmx_helper_errata_fix_ipd_ptr_alignment(void)
 		CVMX_SYNC;
 
 		g_buffer.u64 = 0;
-		g_buffer.s.addr = cvmx_ptr_to_phys(cvmx_fpa_alloc(wqe_pool));
+		g_buffer.s.addr = cvmx_ptr_to_phys(cvmx_fpa1_alloc(wqe_pool));
 		if (g_buffer.s.addr == 0) {
 			cvmx_dprintf("WARNING: FIX_IPD_PTR_ALIGNMENT buffer allocation failure.\n");
 			goto fix_ipd_exit;
@@ -147,7 +147,7 @@ int __cvmx_helper_errata_fix_ipd_ptr_alignment(void)
 		g_buffer.s.size = num_segs;
 
 		pkt_buffer.u64 = 0;
-		pkt_buffer.s.addr = cvmx_ptr_to_phys(cvmx_fpa_alloc(cvmx_fpa_get_packet_pool()));
+		pkt_buffer.s.addr = cvmx_ptr_to_phys(cvmx_fpa1_alloc(cvmx_fpa_get_packet_pool()));
 		if (pkt_buffer.s.addr == 0) {
 			cvmx_dprintf("WARNING: FIX_IPD_PTR_ALIGNMENT buffer allocation failure.\n");
 			goto fix_ipd_exit;
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-ilk.c b/arch/mips/cavium-octeon/executive/cvmx-helper-ilk.c
index 19c6e7d..4fd15cd 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-ilk.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-ilk.c
@@ -55,7 +55,7 @@
 #include <asm/octeon/cvmx-helper-cfg.h>
 #include <asm/octeon/cvmx-ilk.h>
 #include <asm/octeon/cvmx-bootmem.h>
-#include <asm/octeon/cvmx-pko.h>
+#include <asm/octeon/cvmx-hwpko.h>
 #include <asm/octeon/cvmx-qlm.h>
 #include <asm/octeon/cvmx-ilk-defs.h>
 #else
@@ -65,7 +65,7 @@
 #include "cvmx-helper-cfg.h"
 #include "cvmx-ilk.h"
 #include "cvmx-bootmem.h"
-#include "cvmx-pko.h"
+#include "cvmx-hwpko.h"
 #include "cvmx-qlm.h"
 #endif
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-ipd.c b/arch/mips/cavium-octeon/executive/cvmx-helper-ipd.c
new file mode 100644
index 0000000..f7c9edb
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-ipd.c
@@ -0,0 +1,299 @@
+/***********************license start***************
+ * Copyright (c) 2003-2014  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * IPD helper functions.
+ */
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include <linux/module.h>
+#include <asm/octeon/cvmx.h>
+#include <asm/octeon/cvmx-ipd-defs.h>
+#include <asm/octeon/cvmx-ipd.h>
+#include <asm/octeon/cvmx-fpa1.h>
+#include <asm/octeon/cvmx-ipd.h>
+#include <asm/octeon/cvmx-pip.h>
+#include <asm/octeon/cvmx-helper-pki.h>
+#else
+#include "cvmx.h"
+#include "cvmx-version.h"
+#include "cvmx-error.h"
+#include "cvmx-fpa.h"
+#include "cvmx-ipd.h"
+#include "cvmx-pip.h"
+#include "cvmx-helper-pki.h"
+#endif
+
+/** It allocate pools for packet and wqe pools
+ * and sets up the FPA hardware
+ */
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+int __cvmx_helper_ipd_setup_fpa_pools(void)
+{
+	cvmx_fpa_global_initialize();
+	if (cvmx_ipd_cfg.packet_pool.buffer_count == 0)
+		return 0;
+	__cvmx_helper_initialize_fpa_pool(cvmx_ipd_cfg.packet_pool.pool_num, cvmx_ipd_cfg.packet_pool.buffer_size,
+					  cvmx_ipd_cfg.packet_pool.buffer_count, "Packet Buffers");
+	if (cvmx_ipd_cfg.wqe_pool.buffer_count == 0)
+		return 0;
+	__cvmx_helper_initialize_fpa_pool(cvmx_ipd_cfg.wqe_pool.pool_num, cvmx_ipd_cfg.wqe_pool.buffer_size,
+					  cvmx_ipd_cfg.wqe_pool.buffer_count, "WQE Buffers");
+	return 0;
+}
+#endif
+
+/**
+ * @INTERNAL
+ * Setup global setting for IPD/PIP not related to a specific
+ * interface or port. This must be called before IPD is enabled.
+ *
+ * @return Zero on success, negative on failure.
+ */
+int __cvmx_helper_ipd_global_setup(void)
+{
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+	/* Setup the packet and wqe pools*/
+	__cvmx_helper_ipd_setup_fpa_pools();
+#endif
+	/* Setup the global packet input options */
+	cvmx_ipd_config(cvmx_ipd_cfg.packet_pool.buffer_size / 8,
+			cvmx_ipd_cfg.first_mbuf_skip / 8,
+			cvmx_ipd_cfg.not_first_mbuf_skip / 8,
+			/* The +8 is to account for the next ptr */
+			(cvmx_ipd_cfg.first_mbuf_skip + 8) / 128,
+			/* The +8 is to account for the next ptr */
+			(cvmx_ipd_cfg.not_first_mbuf_skip + 8) / 128,
+			cvmx_ipd_cfg.wqe_pool.pool_num,
+			(cvmx_ipd_mode_t)(cvmx_ipd_cfg.cache_mode), 1);
+	return 0;
+}
+
+/**
+ * Enable or disable FCS stripping for all the ports on an interface.
+ *
+ * @param interface
+ * @param nports number of ports
+ * @param has_fcs 0 for disable and !0 for enable
+ */
+static int cvmx_helper_fcs_op(int interface, int nports, int has_fcs)
+{
+	uint64_t port_bit;
+	int index;
+	int pknd;
+	union cvmx_pip_sub_pkind_fcsx pkind_fcsx;
+	union cvmx_pip_prt_cfgx port_cfg;
+	unsigned int node = cvmx_get_node_num();
+
+	if (!octeon_has_feature(OCTEON_FEATURE_PKND))
+		return 0;
+	if (octeon_has_feature(OCTEON_FEATURE_PKI)) {
+		cvmx_helper_pki_set_fcs_op(node, interface, nports, has_fcs);
+		return 0;
+	}
+
+	port_bit = 0;
+	for (index = 0; index < nports; index++)
+		port_bit |= ((uint64_t) 1 << cvmx_helper_get_pknd(interface, index));
+
+	pkind_fcsx.u64 = cvmx_read_csr(CVMX_PIP_SUB_PKIND_FCSX(0));
+	if (has_fcs)
+		pkind_fcsx.s.port_bit |= port_bit;
+	else
+		pkind_fcsx.s.port_bit &= ~port_bit;
+	cvmx_write_csr(CVMX_PIP_SUB_PKIND_FCSX(0), pkind_fcsx.u64);
+
+	for (pknd = 0; pknd < 64; pknd++) {
+		if ((1ull << pknd) & port_bit) {
+			port_cfg.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(pknd));
+			port_cfg.s.crc_en = (has_fcs) ? 1 : 0;
+			cvmx_write_csr(CVMX_PIP_PRT_CFGX(pknd), port_cfg.u64);
+		}
+	}
+
+	return 0;
+}
+
+/**
+ * @INTERNAL
+ * Configure the IPD/PIP tagging and QoS options for a specific
+ * port. This function determines the POW work queue entry
+ * contents for a port. The setup performed here is controlled by
+ * the defines in executive-config.h.
+ *
+ * @param ipd_port Port/Port kind to configure. This follows the IPD numbering,
+ *                 not the per interface numbering
+ *
+ * @return Zero on success, negative on failure
+ */
+static int __cvmx_helper_ipd_port_setup(int ipd_port)
+{
+	union cvmx_pip_prt_cfgx port_config;
+	union cvmx_pip_prt_tagx tag_config;
+
+	if (octeon_has_feature(OCTEON_FEATURE_PKND)) {
+		int interface, index, pknd;
+		union cvmx_pip_prt_cfgbx prt_cfgbx;
+
+		interface = cvmx_helper_get_interface_num(ipd_port);
+		index = cvmx_helper_get_interface_index_num(ipd_port);
+		pknd = cvmx_helper_get_pknd(interface, index);
+
+		port_config.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(pknd));
+		tag_config.u64 = cvmx_read_csr(CVMX_PIP_PRT_TAGX(pknd));
+
+		port_config.s.qos = pknd & 0x7;
+
+		/* Default BPID to use for packets on this port-kind */
+		prt_cfgbx.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGBX(pknd));
+		prt_cfgbx.s.bpid = pknd;
+		cvmx_write_csr(CVMX_PIP_PRT_CFGBX(pknd), prt_cfgbx.u64);
+	} else {
+		port_config.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(ipd_port));
+		tag_config.u64 = cvmx_read_csr(CVMX_PIP_PRT_TAGX(ipd_port));
+
+		/* Have each port go to a different POW queue */
+		port_config.s.qos = ipd_port & 0x7;
+	}
+
+	/* Process the headers and place the IP header in the work queue */
+	port_config.s.mode = (cvmx_pip_port_parse_mode_t)cvmx_ipd_cfg.port_config.parse_mode;
+
+	tag_config.s.ip6_src_flag  = cvmx_ipd_cfg.port_config.tag_fields.ipv6_src_ip;
+	tag_config.s.ip6_dst_flag  = cvmx_ipd_cfg.port_config.tag_fields.ipv6_dst_ip;
+	tag_config.s.ip6_sprt_flag = cvmx_ipd_cfg.port_config.tag_fields.ipv6_src_port;
+	tag_config.s.ip6_dprt_flag = cvmx_ipd_cfg.port_config.tag_fields.ipv6_dst_port;
+	tag_config.s.ip6_nxth_flag = cvmx_ipd_cfg.port_config.tag_fields.ipv6_next_header;
+	tag_config.s.ip4_src_flag  = cvmx_ipd_cfg.port_config.tag_fields.ipv4_src_ip;
+	tag_config.s.ip4_dst_flag  = cvmx_ipd_cfg.port_config.tag_fields.ipv4_dst_ip;
+	tag_config.s.ip4_sprt_flag = cvmx_ipd_cfg.port_config.tag_fields.ipv4_src_port;
+	tag_config.s.ip4_dprt_flag = cvmx_ipd_cfg.port_config.tag_fields.ipv4_dst_port;
+	tag_config.s.ip4_pctl_flag = cvmx_ipd_cfg.port_config.tag_fields.ipv4_protocol;
+	tag_config.s.inc_prt_flag  = cvmx_ipd_cfg.port_config.tag_fields.input_port;
+	tag_config.s.tcp6_tag_type = (cvmx_pow_tag_type_t)cvmx_ipd_cfg.port_config.tag_type;
+	tag_config.s.tcp4_tag_type = (cvmx_pow_tag_type_t)cvmx_ipd_cfg.port_config.tag_type;
+	tag_config.s.ip6_tag_type  = (cvmx_pow_tag_type_t)cvmx_ipd_cfg.port_config.tag_type;
+	tag_config.s.ip4_tag_type  = (cvmx_pow_tag_type_t)cvmx_ipd_cfg.port_config.tag_type;
+	tag_config.s.non_tag_type  = (cvmx_pow_tag_type_t)cvmx_ipd_cfg.port_config.tag_type;
+
+	/* Put all packets in group 0. Other groups can be used by the app */
+	tag_config.s.grp = 0;
+
+	cvmx_pip_config_port(ipd_port, port_config, tag_config);
+
+	/* Give the user a chance to override our setting for each port */
+	if (cvmx_override_ipd_port_setup)
+		cvmx_override_ipd_port_setup(ipd_port);
+
+	return 0;
+}
+
+/**
+ * @INTERNAL
+ * Setup the IPD/PIP for the ports on an interface. Packet
+ * classification and tagging are set for every port on the
+ * interface. The number of ports on the interface must already
+ * have been probed.
+ *
+ * @param interface Interface to setup IPD/PIP for
+ *
+ * @return Zero on success, negative on failure
+ */
+int __cvmx_helper_ipd_setup_interface(int interface)
+{
+	cvmx_helper_interface_mode_t mode;
+	int ipd_port = cvmx_helper_get_ipd_port(interface, 0);
+	int num_ports = cvmx_helper_ports_on_interface(interface);
+	int delta;
+	unsigned int node = cvmx_get_node_num();
+
+	if (num_ports == CVMX_HELPER_CFG_INVALID_VALUE)
+		return 0;
+
+	mode = cvmx_helper_interface_get_mode(interface);
+
+	if (!octeon_has_feature(OCTEON_FEATURE_PKI)) {
+		if (mode == CVMX_HELPER_INTERFACE_MODE_LOOP)
+			__cvmx_helper_loop_enable(interface);
+	}
+
+	delta = 1;
+	if (octeon_has_feature(OCTEON_FEATURE_PKND)) {
+		if (mode == CVMX_HELPER_INTERFACE_MODE_SGMII)
+			delta = 16;
+	}
+
+	while (num_ports--) {
+		if (!cvmx_helper_is_port_valid(interface, num_ports))
+			continue;
+		if (octeon_has_feature(OCTEON_FEATURE_PKI))
+			__cvmx_helper_pki_port_setup(node, ipd_port);
+		else
+			__cvmx_helper_ipd_port_setup(ipd_port);
+		ipd_port += delta;
+	}
+	/* FCS settings */
+	cvmx_helper_fcs_op(interface,
+			   cvmx_helper_ports_on_interface(interface),
+			   __cvmx_helper_get_has_fcs(interface));
+
+	return 0;
+}
+
+void cvmx_helper_ipd_set_wqe_no_ptr_mode(bool mode)
+{
+	if (!octeon_has_feature(OCTEON_FEATURE_PKI)) {
+		cvmx_ipd_ctl_status_t ipd_ctl_status;
+		ipd_ctl_status.u64 = cvmx_read_csr(CVMX_IPD_CTL_STATUS);
+		ipd_ctl_status.s.no_wptr = mode;
+		cvmx_write_csr(CVMX_IPD_CTL_STATUS, ipd_ctl_status.u64);
+	}
+}
+
+void cvmx_helper_ipd_pkt_wqe_le_mode(bool mode)
+{
+	if (!octeon_has_feature(OCTEON_FEATURE_PKI)) {
+		cvmx_ipd_ctl_status_t ipd_ctl_status;
+		ipd_ctl_status.u64 = cvmx_read_csr(CVMX_IPD_CTL_STATUS);
+		ipd_ctl_status.s.pkt_lend = mode;
+		ipd_ctl_status.s.wqe_lend = mode;
+		cvmx_write_csr(CVMX_IPD_CTL_STATUS, ipd_ctl_status.u64);
+	}
+}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-npi.c b/arch/mips/cavium-octeon/executive/cvmx-helper-npi.c
index 1ae9423..70687d4 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-npi.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-npi.c
@@ -43,19 +43,19 @@
  * Functions for NPI initialization, configuration,
  * and monitoring.
  *
- * <hr>$Revision: 94257 $<hr>
+ * <hr>$Revision: 95258 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
 #include <asm/octeon/cvmx-helper.h>
-#include <asm/octeon/cvmx-pko.h>
+#include <asm/octeon/cvmx-hwpko.h>
 #include <asm/octeon/cvmx-pexp-defs.h>
 #include <asm/octeon/cvmx-sli-defs.h>
 #include <asm/octeon/cvmx-pip-defs.h>
 #include <asm/octeon/cvmx-pki.h>
 #else
 #include "cvmx.h"
-#include "cvmx-pko.h"
+#include "cvmx-hwpko.h"
 #include "cvmx-helper.h"
 #include "cvmx-pki.h"
 #endif
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-pki.c b/arch/mips/cavium-octeon/executive/cvmx-helper-pki.c
index c86f6ad..5625c90 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-pki.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-pki.c
@@ -103,7 +103,15 @@ struct cvmx_pki_style_config pki_dflt_style[CVMX_MAX_NODES] = {
 struct cvmx_pki_sso_grp_config pki_dflt_sso_grp[CVMX_MAX_NODES];
 struct cvmx_pki_qpg_config pki_dflt_qpg[CVMX_MAX_NODES];
 struct cvmx_pki_pkind_config pki_dflt_pkind[CVMX_MAX_NODES];
-uint64_t pkind_style_map[CVMX_MAX_NODES][CVMX_PKI_NUM_PKIND];
+uint64_t pkind_style_map[CVMX_MAX_NODES][CVMX_PKI_NUM_PKIND] = {
+	{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,
+	16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,
+	32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47,
+	48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63},
+	{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,
+	16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,
+	32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47,
+	48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63} };
 
 int __cvmx_helper_setup_pki_cluster_groups(int node)
 {
@@ -197,14 +205,14 @@ int __cvmx_helper_pki_setup_fpa_pools(int node)
 				cvmx_dprintf("pki-helper: set fpa pool %d with \
 					buffer size %d buffer cnt %d\n",
 			pki_dflt_pool[node].pool_num, (int)buffer_size, (int)buffer_count);
-			cvmx_fpa_pool_stack_init(node, pki_dflt_pool[node].pool_num, "PKI Pools", 0,
+			cvmx_fpa3_pool_stack_init(node, pki_dflt_pool[node].pool_num, "PKI Pool0", 0,
 						 buffer_count, FPA_NATURAL_ALIGNMENT,
 						 buffer_size);
 		}
 	}
 	buffer_count = pki_dflt_aura[node].buffer_count;
 	if (buffer_count != 0) {
-		rs = cvmx_fpa_allocate_auras(node, &pki_dflt_aura[node].aura_num, 1);
+		rs = cvmx_fpa3_allocate_auras(node, &pki_dflt_aura[node].aura_num, 1);
 		if (rs == -1) {
 			if (pki_dflt_aura[node].aura_num == -1) {
 				cvmx_dprintf("ERROR: Failed to allocate aura %d\n", pki_dflt_aura[node].aura_num);
@@ -217,8 +225,8 @@ int __cvmx_helper_pki_setup_fpa_pools(int node)
 			cvmx_dprintf("pki-helper: set fpa aura %d in pool %d with buffer cnt %d\n",
 				     pki_dflt_aura[node].aura_num, pki_dflt_aura[node].pool_num,
 				     (int)buffer_count);
-		cvmx_fpa_assign_aura(node, pki_dflt_aura[node].aura_num, pki_dflt_pool[node].pool_num);
-		cvmx_fpa_aura_init(node, pki_dflt_aura[node].aura_num, "PKI Aura", 0, NULL, buffer_count, 0);
+		cvmx_fpa3_assign_aura(node, pki_dflt_aura[node].aura_num, pki_dflt_pool[node].pool_num);
+		cvmx_fpa3_aura_init(node, pki_dflt_aura[node].aura_num, "PKI Aura0", 0, NULL, buffer_count, 0);
 #endif
 	}
 	return 0;
@@ -334,7 +342,7 @@ int __cvmx_helper_pki_install_default_vlan(int node)
 	return 0;
 }
 
-int __cvmx_helper_global_setup_pki(int node)
+int __cvmx_helper_pki_global_setup(int node)
 {
 #ifndef CVMX_BUILD_FOR_LINUX_KERNEL
 	/* Setup the packet pools*/
@@ -371,7 +379,7 @@ int cvmx_helper_pki_get_num_qos_entry(enum cvmx_pki_qpg_qos qpg_qos)
 	/* vinita_to_do add port_sh and port_msb too */
 }
 
-int __cvmx_helper_port_setup_pki(int node, int ipd_port)
+int __cvmx_helper_pki_port_setup(int node, int ipd_port)
 {
 	int interface, index;
 	int pknd, style_num;
@@ -483,6 +491,11 @@ void cvmx_helper_pki_get_dflt_qpg(int node, struct cvmx_pki_qpg_config *qpg_cfg)
 	*qpg_cfg = pki_dflt_qpg[node];
 }
 
+void cvmx_helper_pki_set_dflt_pkind_map(int node, int pkind, int style)
+{
+	pkind_style_map[node][pkind] = style;
+}
+
 /**
  * This function sets up aura QOS for RED, backpressure and tail-drop.
  *
@@ -522,7 +535,7 @@ int cvmx_helper_setup_aura_qos(int node, int aura, bool ena_red, bool ena_drop,
  * @param bpid          bpid to map
  * @return Zero on success. Negative on failure
  */
-int cvmx_helper_map_aura_channel_bpid(int node, int aura_map[], int aura_cnt,
+int cvmx_helper_pki_map_aura_chl_bpid(int node, int aura_map[], int aura_cnt,
 				      int chl_map[], int chl_cnt, int bpid)
 {
 	while (aura_cnt--)
@@ -531,3 +544,5 @@ int cvmx_helper_map_aura_channel_bpid(int node, int aura_map[], int aura_cnt,
 		cvmx_pki_write_channel_bpid(node, chl_map[chl_cnt], bpid);
 	return 0;
 }
+
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-pko.c b/arch/mips/cavium-octeon/executive/cvmx-helper-pko.c
index 64b3c76..fcdc009 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-pko.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-pko.c
@@ -42,13 +42,13 @@
  *
  * Helper Functions for the PKO
  *
- * $Id: cvmx-helper-pko.c 94829 2014-03-06 19:36:22Z ddaney $
+ * $Id: cvmx-helper-pko.c 95258 2014-03-18 23:43:53Z ddaney $
  */
 
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/octeon.h>
-#include <asm/octeon/cvmx-pko.h>
-#include <asm/octeon/cvmx-fpa.h>
+#include <asm/octeon/cvmx-hwpko.h>
+#include <asm/octeon/cvmx-fpa1.h>
 #include <asm/octeon/cvmx-clock.h>
 #include <asm/octeon/cvmx-helper-cfg.h>
 #else
@@ -57,12 +57,16 @@
 #include "cvmx-helper.h"
 #include "cvmx-helper-ilk.h"
 #include "cvmx-ipd.h"
-#include "cvmx-pko.h"
+#include "cvmx-fpa.h"
+#include "cvmx-hwpko.h"
 #include "cvmx-global-resources.h"
 #endif
 
 //XXX- these config data structures will go away soon!
-CVMX_SHARED cvmx_fpa_pool_config_t pko_fpa_config = {2,1024,0};
+static CVMX_SHARED int64_t pko_fpa_config_pool = 2;
+static CVMX_SHARED uint64_t pko_fpa_config_size = 1024;
+static CVMX_SHARED uint64_t pko_fpa_config_count = 0;
+
 
 /**
  * cvmx_override_pko_queue_priority(int pko_port, uint64_t
@@ -78,22 +82,45 @@ EXPORT_SYMBOL(cvmx_override_pko_queue_priority);
 void cvmx_pko_set_cmd_que_pool_config(int64_t pool, uint64_t buffer_size,
 				    uint64_t buffer_count)
 {
-	pko_fpa_config.pool_num = pool;
-	pko_fpa_config.buffer_size = buffer_size;
-	pko_fpa_config.buffer_count = buffer_count;
+	pko_fpa_config_pool = pool;
+	pko_fpa_config_size = buffer_size;
+	pko_fpa_config_count = buffer_count;
+	
 }
 EXPORT_SYMBOL(cvmx_pko_set_cmd_que_pool_config);
 
 void cvmx_pko_set_cmd_queue_pool_buffer_count(uint64_t buffer_count)
 {
-	pko_fpa_config.buffer_count = buffer_count;
+	pko_fpa_config_count = buffer_count;
 }
 
 void cvmx_pko_get_cmd_que_pool_config(cvmx_fpa_pool_config_t *pko_pool)
 {
-	*pko_pool = pko_fpa_config;
+	pko_pool->pool_num = pko_fpa_config_pool;
+	pko_pool->buffer_size = pko_fpa_config_size;
+	pko_pool->buffer_count = pko_fpa_config_count;
+}
+
+int64_t cvmx_fpa_get_pko_pool(void)
+{
+	return pko_fpa_config_pool;
 }
 
+/**
+ * Gets the buffer size of pko pool
+ */
+uint64_t cvmx_fpa_get_pko_pool_block_size(void)
+{
+	return pko_fpa_config_size;
+}
+
+/**
+ * Gets the buffer size  of pko pool
+ */
+uint64_t cvmx_fpa_get_pko_pool_buffer_count(void)
+{
+	return pko_fpa_config_count;
+}
 
 #ifndef CVMX_BUILD_FOR_LINUX_KERNEL
 /**
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-pko3.c b/arch/mips/cavium-octeon/executive/cvmx-helper-pko3.c
index a1992f1..305cea8 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-pko3.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-pko3.c
@@ -56,6 +56,7 @@
 #else
 #include "cvmx.h"
 #include "cvmx-ilk.h"
+#include "cvmx-fpa.h"
 #include "cvmx-pko3.h"
 #include "cvmx-pko3-resources.h"
 #include "cvmx-helper.h"
@@ -136,20 +137,20 @@ static int __cvmx_pko3_config_memory(unsigned node)
 	if(res < 0)
 		return res;
 
-	res = cvmx_fpa_allocate_auras(node, &aura_num, 1);
+	res = cvmx_fpa3_allocate_auras(node, &aura_num, 1);
 	if(res < 0)
 		return res;
 
 	/* fpa pool intialization for pko command buffers */
-	res = cvmx_fpa_pool_stack_init(node, pool_num,
+	res = cvmx_fpa3_pool_stack_init(node, pool_num,
 				"PKO Pool", 0, //XXX- use local memory ?
 				CVMX_PKO3_POOL_BUFFERS,
 				FPA_NATURAL_ALIGNMENT,
 				CVMX_PKO3_POOL_BUFFER_SIZE);
 
-	res = cvmx_fpa_assign_aura(node, aura_num, pool_num);
+	res = cvmx_fpa3_assign_aura(node, aura_num, pool_num);
 
-	res = cvmx_fpa_aura_init(node, aura_num,"PKO Aura", 0, NULL,
+	res = cvmx_fpa3_aura_init(node, aura_num,"PKO Aura", 0, NULL,
 			   CVMX_PKO3_POOL_BUFFERS, 0);
 
 	/* Store numbers e.g. for destruction */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-rgmii.c b/arch/mips/cavium-octeon/executive/cvmx-helper-rgmii.c
index f02873d..e236f56 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-rgmii.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-rgmii.c
@@ -43,11 +43,11 @@
  * Functions for RGMII/GMII/MII initialization, configuration,
  * and monitoring.
  *
- * <hr>$Revision: 86586 $<hr>
+ * <hr>$Revision: 95258 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
-#include <asm/octeon/cvmx-pko.h>
+#include <asm/octeon/cvmx-hwpko.h>
 #include <asm/octeon/cvmx-helper.h>
 #include <asm/octeon/cvmx-helper-board.h>
 #include <asm/octeon/cvmx-asxx-defs.h>
@@ -61,7 +61,7 @@
 #include "cvmx.h"
 #include "cvmx-sysinfo.h"
 #include "cvmx-mdio.h"
-#include "cvmx-pko.h"
+#include "cvmx-hwpko.h"
 #include "cvmx-helper.h"
 #include "cvmx-helper-board.h"
 #endif
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-util.c b/arch/mips/cavium-octeon/executive/cvmx-helper-util.c
index a255ffe..82bb49a 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-util.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-util.c
@@ -53,7 +53,7 @@
 #include <asm/octeon/cvmx-helper.h>
 #include <asm/octeon/cvmx-gmxx-defs.h>
 #include <asm/octeon/cvmx-pko-defs.h>
-#include <asm/octeon/cvmx-pko.h>
+#include <asm/octeon/cvmx-hwpko.h>
 #include <asm/octeon/cvmx-sli-defs.h>
 #include <asm/octeon/cvmx-pexp-defs.h>
 #include <asm/octeon/cvmx-helper-cfg.h>
@@ -64,7 +64,7 @@
 #include "cvmx-bootmem.h"
 #include "cvmx-fpa.h"
 #include "cvmx-pip.h"
-#include "cvmx-pko.h"
+#include "cvmx-hwpko.h"
 #include "cvmx-ilk.h"
 #include "cvmx-ipd.h"
 #include "cvmx-spi.h"
@@ -355,6 +355,60 @@ int cvmx_helper_dump_packet(cvmx_wqe_t *work)
 	return 0;
 }
 
+void cvmx_helper_setup_legacy_red(int pass_thresh, int drop_thresh)
+{
+	unsigned int node = cvmx_get_node_num();
+	int aura, bpid;
+	bool ena_red = 0, ena_drop = 0, ena_bp = 0;
+
+#define FPA_RED_AVG_DLY	1
+#define FPA_RED_LVL_DLY	 3
+#define FPA_QOS_AVRG 0
+	/* Trying to make it backward compatible with older chips */
+
+	/* Setting up avg_dly and prb_dly, enable bits */
+	if (octeon_has_feature(OCTEON_FEATURE_FPA3))
+		cvmx_fpa3_config_red_params(node, FPA_QOS_AVRG, FPA_RED_LVL_DLY, FPA_RED_AVG_DLY);
+
+	/* Disable backpressure on queued buffers which is aura in 78xx*/
+	/* Assumption is that all packets from all interface and ports goes in same poolx/aurax
+	for backward compatibility*/
+	aura = cvmx_fpa_get_packet_pool();
+	/* Map aura to bpid 0*/
+	bpid = 0;
+	cvmx_pki_write_aura_bpid(node, aura, bpid);
+	/* Don't enable back pressure */
+	ena_bp = 0;
+	/* enable RED */
+	ena_red = 1;
+	ena_drop = 1;
+	/* This will enable RED on all interfaces since
+	they all have packet buffer coming from  same aura */
+	cvmx_helper_setup_aura_qos(node, aura, ena_red, pass_thresh,
+				   drop_thresh, ena_bp, 0, ena_drop);
+}
+
+/**
+ * Setup Random Early Drop to automatically begin dropping packets.
+ *
+ * @param pass_thresh
+ *               Packets will begin slowly dropping when there are less than
+ *               this many packet buffers free in FPA 0.
+ * @param drop_thresh
+ *               All incomming packets will be dropped when there are less
+ *               than this many free packet buffers in FPA 0.
+ * @return Zero on success. Negative on failure
+ */
+int cvmx_helper_setup_red(int pass_thresh, int drop_thresh)
+{
+	if (octeon_has_feature(OCTEON_FEATURE_PKI))
+		cvmx_helper_setup_legacy_red(pass_thresh, drop_thresh);
+	else
+		cvmx_ipd_setup_red(pass_thresh, drop_thresh);
+	return 0;
+}
+EXPORT_SYMBOL(cvmx_helper_setup_red);
+
 /**
  * @INTERNAL
  * Setup the common GMX settings that determine the number of
@@ -596,6 +650,7 @@ int __cvmx_helper_init_interface(int interface, int num_ipd_ports,
 	int sz;
 #ifndef CVMX_BUILD_FOR_LINUX_KERNEL
 	uint64_t addr;
+	char name[32];
 #endif
 
 	if (interface >= cvmx_helper_get_number_of_interfaces())
@@ -618,7 +673,6 @@ int __cvmx_helper_init_interface(int interface, int num_ipd_ports,
 	if (ZERO_OR_NULL_PTR(piface->cvif_ipd_port_link_info))
 		panic("Cannot allocate memory in __cvmx_helper_init_interface.");
 #else
-	char name[32];
 	snprintf(name, sizeof(name), "__int_%d_link_info", interface);
 	addr = CAST64(cvmx_bootmem_alloc_named_range_once(sz, 0, 0, 128, name, NULL));
 	piface->cvif_ipd_port_link_info = (cvmx_helper_link_info_t *) __cvmx_phys_addr_to_ptr(addr, sz);
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper.c b/arch/mips/cavium-octeon/executive/cvmx-helper.c
index 6e8db91..084e0b0 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper.c
@@ -61,9 +61,9 @@
 
 #include <asm/octeon/cvmx-agl.h>
 #include <asm/octeon/cvmx-gmx.h>
-#include <asm/octeon/cvmx-fpa.h>
+#include <asm/octeon/cvmx-fpa1.h>
 #include <asm/octeon/cvmx-pip.h>
-#include <asm/octeon/cvmx-pko.h>
+#include <asm/octeon/cvmx-hwpko.h>
 #include <asm/octeon/cvmx-pko3.h>
 #include <asm/octeon/cvmx-ipd.h>
 #include <asm/octeon/cvmx-qlm.h>
@@ -78,6 +78,7 @@
 #include <asm/octeon/cvmx-pki.h>
 #include <asm/octeon/cvmx-helper-pko.h>
 #include <asm/octeon/cvmx-helper-pko3.h>
+#include <asm/octeon/cvmx-helper-ipd.h>
 #else
 #include "cvmx.h"
 #include "cvmx-sysinfo.h"
@@ -102,6 +103,7 @@
 #include "cvmx-pki.h"
 #include "cvmx-helper-pko.h"
 #include "cvmx-helper-pko3.h"
+#include "cvmx-helper-ipd.h"
 #endif
 
 
@@ -936,125 +938,6 @@ cvmx_helper_interface_mode_t cvmx_helper_interface_get_mode(int interface)
 EXPORT_SYMBOL(cvmx_helper_interface_get_mode);
 
 /**
- * @INTERNAL
- * Configure the IPD/PIP tagging and QoS options for a specific
- * port. This function determines the POW work queue entry
- * contents for a port. The setup performed here is controlled by
- * the defines in executive-config.h.
- *
- * @param ipd_port Port/Port kind to configure. This follows the IPD numbering,
- *                 not the per interface numbering
- *
- * @return Zero on success, negative on failure
- */
-static int __cvmx_helper_port_setup_ipd(int ipd_port)
-{
-	union cvmx_pip_prt_cfgx port_config;
-	union cvmx_pip_prt_tagx tag_config;
-
-	if (octeon_has_feature(OCTEON_FEATURE_PKND)) {
-		int interface, index, pknd;
-		union cvmx_pip_prt_cfgbx prt_cfgbx;
-
-		interface = cvmx_helper_get_interface_num(ipd_port);
-		index = cvmx_helper_get_interface_index_num(ipd_port);
-		pknd = cvmx_helper_get_pknd(interface, index);
-
-		port_config.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(pknd));
-		tag_config.u64 = cvmx_read_csr(CVMX_PIP_PRT_TAGX(pknd));
-
-		port_config.s.qos = pknd & 0x7;
-
-		/* Default BPID to use for packets on this port-kind */
-		prt_cfgbx.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGBX(pknd));
-		prt_cfgbx.s.bpid = pknd;
-		cvmx_write_csr(CVMX_PIP_PRT_CFGBX(pknd), prt_cfgbx.u64);
-	} else {
-		port_config.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(ipd_port));
-		tag_config.u64 = cvmx_read_csr(CVMX_PIP_PRT_TAGX(ipd_port));
-
-		/* Have each port go to a different POW queue */
-		port_config.s.qos = ipd_port & 0x7;
-	}
-
-	/* Process the headers and place the IP header in the work queue */
-	port_config.s.mode = (cvmx_pip_port_parse_mode_t)cvmx_ipd_cfg.port_config.parse_mode;
-
-	tag_config.s.ip6_src_flag  = cvmx_ipd_cfg.port_config.tag_fields.ipv6_src_ip;
-	tag_config.s.ip6_dst_flag  = cvmx_ipd_cfg.port_config.tag_fields.ipv6_dst_ip;
-	tag_config.s.ip6_sprt_flag = cvmx_ipd_cfg.port_config.tag_fields.ipv6_src_port;
-	tag_config.s.ip6_dprt_flag = cvmx_ipd_cfg.port_config.tag_fields.ipv6_dst_port;
-	tag_config.s.ip6_nxth_flag = cvmx_ipd_cfg.port_config.tag_fields.ipv6_next_header;
-	tag_config.s.ip4_src_flag  = cvmx_ipd_cfg.port_config.tag_fields.ipv4_src_ip;
-	tag_config.s.ip4_dst_flag  = cvmx_ipd_cfg.port_config.tag_fields.ipv4_dst_ip;
-	tag_config.s.ip4_sprt_flag = cvmx_ipd_cfg.port_config.tag_fields.ipv4_src_port;
-	tag_config.s.ip4_dprt_flag = cvmx_ipd_cfg.port_config.tag_fields.ipv4_dst_port;
-	tag_config.s.ip4_pctl_flag = cvmx_ipd_cfg.port_config.tag_fields.ipv4_protocol;
-	tag_config.s.inc_prt_flag  = cvmx_ipd_cfg.port_config.tag_fields.input_port;
-	tag_config.s.tcp6_tag_type = (cvmx_pow_tag_type_t)cvmx_ipd_cfg.port_config.tag_type;
-	tag_config.s.tcp4_tag_type = (cvmx_pow_tag_type_t)cvmx_ipd_cfg.port_config.tag_type;
-	tag_config.s.ip6_tag_type  = (cvmx_pow_tag_type_t)cvmx_ipd_cfg.port_config.tag_type;
-	tag_config.s.ip4_tag_type  = (cvmx_pow_tag_type_t)cvmx_ipd_cfg.port_config.tag_type;
-	tag_config.s.non_tag_type  = (cvmx_pow_tag_type_t)cvmx_ipd_cfg.port_config.tag_type;
-
-	/* Put all packets in group 0. Other groups can be used by the app */
-	tag_config.s.grp = 0;
-
-	cvmx_pip_config_port(ipd_port, port_config, tag_config);
-
-	/* Give the user a chance to override our setting for each port */
-	if (cvmx_override_ipd_port_setup)
-		cvmx_override_ipd_port_setup(ipd_port);
-
-	return 0;
-}
-
-/**
- * Enable or disable FCS stripping for all the ports on an interface.
- *
- * @param interface
- * @param nports number of ports
- * @param has_fcs 0 for disable and !0 for enable
- */
-static int cvmx_helper_fcs_op(int interface, int nports, int has_fcs)
-{
-	uint64_t port_bit;
-	int index;
-	int pknd;
-	union cvmx_pip_sub_pkind_fcsx pkind_fcsx;
-	union cvmx_pip_prt_cfgx port_cfg;
-	unsigned int node = cvmx_get_node_num();
-
-	if (!octeon_has_feature(OCTEON_FEATURE_PKND))
-		return 0;
-	if (octeon_has_feature(OCTEON_FEATURE_PKI)) {
-		cvmx_helper_pki_set_fcs_op(node, interface, nports, has_fcs);
-		return 0;
-	}
-
-	port_bit = 0;
-	for (index = 0; index < nports; index++)
-		port_bit |= ((uint64_t) 1 << cvmx_helper_get_pknd(interface, index));
-
-	pkind_fcsx.u64 = cvmx_read_csr(CVMX_PIP_SUB_PKIND_FCSX(0));
-	if (has_fcs)
-		pkind_fcsx.s.port_bit |= port_bit;
-	else
-		pkind_fcsx.s.port_bit &= ~port_bit;
-	cvmx_write_csr(CVMX_PIP_SUB_PKIND_FCSX(0), pkind_fcsx.u64);
-
-	for (pknd = 0; pknd < 64; pknd++) {
-		if ((1ull << pknd) & port_bit) {
-			port_cfg.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(pknd));
-			port_cfg.s.crc_en = (has_fcs) ? 1 : 0;
-			cvmx_write_csr(CVMX_PIP_PRT_CFGX(pknd), port_cfg.u64);
-		}
-	}
-
-	return 0;
-}
-
-/**
  * Determine the actual number of hardware ports connected to an
  * interface. It doesn't setup the ports or enable them.
  *
@@ -1180,105 +1063,6 @@ int cvmx_helper_interface_probe(int interface)
 
 /**
  * @INTERNAL
- * Setup the IPD/PIP for the ports on an interface. Packet
- * classification and tagging are set for every port on the
- * interface. The number of ports on the interface must already
- * have been probed.
- *
- * @param interface Interface to setup IPD/PIP for
- *
- * @return Zero on success, negative on failure
- */
-static int __cvmx_helper_interface_setup_ipd(int interface)
-{
-	cvmx_helper_interface_mode_t mode;
-	int ipd_port = cvmx_helper_get_ipd_port(interface, 0);
-	int num_ports = cvmx_helper_ports_on_interface(interface);
-	int delta;
-	unsigned int node = cvmx_get_node_num();
-
-	if (num_ports == CVMX_HELPER_CFG_INVALID_VALUE)
-		return 0;
-
-	mode = cvmx_helper_interface_get_mode(interface);
-
-	if (!octeon_has_feature(OCTEON_FEATURE_PKI)) {
-		if (mode == CVMX_HELPER_INTERFACE_MODE_LOOP)
-			__cvmx_helper_loop_enable(interface);
-	}
-
-	delta = 1;
-	if (octeon_has_feature(OCTEON_FEATURE_PKND)) {
-		if (mode == CVMX_HELPER_INTERFACE_MODE_SGMII)
-			delta = 16;
-	}
-
-	while (num_ports--) {
-		if (!cvmx_helper_is_port_valid(interface, num_ports))
-			continue;
-		if (octeon_has_feature(OCTEON_FEATURE_PKI))
-			__cvmx_helper_port_setup_pki(node, ipd_port);
-		else
-			__cvmx_helper_port_setup_ipd(ipd_port);
-		ipd_port += delta;
-	}
-
-	/* FCS settings */
-	cvmx_helper_fcs_op(interface,
-	    cvmx_helper_ports_on_interface(interface),
-	    __cvmx_helper_get_has_fcs(interface));
-
-	return 0;
-}
-
-/** It allocate pools for packet and wqe pools
-  * and sets up the FPA hardware
-  */
-#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
-int __cvmx_helper_ipd_setup_fpa_pools(void)
-{
-	cvmx_fpa_global_initialize();
-	if (cvmx_ipd_cfg.packet_pool.buffer_count == 0)
-		return 0;
-		__cvmx_helper_initialize_fpa_pool(cvmx_ipd_cfg.packet_pool.pool_num, cvmx_ipd_cfg.packet_pool.buffer_size,
-					  cvmx_ipd_cfg.packet_pool.buffer_count, "Packet Buffers");
-	if (cvmx_ipd_cfg.wqe_pool.buffer_count == 0)
-		return 0;
-		__cvmx_helper_initialize_fpa_pool(cvmx_ipd_cfg.wqe_pool.pool_num, cvmx_ipd_cfg.wqe_pool.buffer_size,
-			cvmx_ipd_cfg.wqe_pool.buffer_count, "WQE Buffers");
-	return 0;
-}
-#endif
-
-/**
- * @INTERNAL
- * Setup global setting for IPD/PIP not related to a specific
- * interface or port. This must be called before IPD is enabled.
- *
- * @return Zero on success, negative on failure.
- */
-static int __cvmx_helper_global_setup_ipd(void)
-{
-#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
-	/* Setup the packet and wqe pools*/
-	__cvmx_helper_ipd_setup_fpa_pools();
-#endif
-	/* Setup the global packet input options */
-	cvmx_ipd_config(cvmx_ipd_cfg.packet_pool.buffer_size / 8,
-			cvmx_ipd_cfg.first_mbuf_skip / 8,
-			cvmx_ipd_cfg.not_first_mbuf_skip / 8,
-			/* The +8 is to account for the next ptr */
-			(cvmx_ipd_cfg.first_mbuf_skip + 8) / 128,
-			/* The +8 is to account for the next ptr */
-			(cvmx_ipd_cfg.not_first_mbuf_skip + 8) / 128,
-			cvmx_ipd_cfg.wqe_pool.pool_num,
-			(cvmx_ipd_mode_t)(cvmx_ipd_cfg.cache_mode), 1);
-	return 0;
-}
-
-
-/**
- * @INTERNAL
  * Setup global backpressure setting.
  *
  * @return Zero on success, negative on failure
@@ -1604,7 +1388,7 @@ int cvmx_helper_initialize_packet_io_global(void)
 		//FIXME- ILK needs this config data for now - must fix!
 		__cvmx_helper_init_port_config_data();
 		cvmx_helper_pko3_init_global();
-	} 
+	}
 	else {
 		cvmx_helper_pko_init();
 	}
@@ -1618,7 +1402,7 @@ int cvmx_helper_initialize_packet_io_global(void)
 		     cvmx_helper_ports_on_interface(interface),
 		     cvmx_helper_interface_mode_to_string(cvmx_helper_interface_get_mode(interface)));
 
-		result |= __cvmx_helper_interface_setup_ipd(interface);/* vinita_to_do separate pki */
+		result |= __cvmx_helper_ipd_setup_interface(interface);/* vinita_to_do separate pki */
 		if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE))
 			result |= cvmx_helper_pko3_init_interface(interface);
 		else
@@ -1626,9 +1410,9 @@ int cvmx_helper_initialize_packet_io_global(void)
 	}
 
 	if (octeon_has_feature(OCTEON_FEATURE_PKI))
-		result |= __cvmx_helper_global_setup_pki(node);
+		result |= __cvmx_helper_pki_global_setup(node);
 	else
-		result |= __cvmx_helper_global_setup_ipd();
+		result |= __cvmx_helper_ipd_global_setup();
 
 	/* Enable any flow control and backpressure */
 	result |= __cvmx_helper_global_setup_backpressure();
@@ -1651,6 +1435,9 @@ EXPORT_SYMBOL(cvmx_helper_initialize_packet_io_global);
  */
 int cvmx_helper_initialize_packet_io_local(void)
 {
+	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
+		__cvmx_pko3_dq_table_setup();
+	}
 	return 0;
 }
 struct cvmx_buffer_list {
@@ -2055,7 +1842,7 @@ step2:
 	/* Step 4: Retrieve all packets from the POW and free them */
 	while ((work = cvmx_pow_work_request_sync(CVMX_POW_WAIT))) {
 		cvmx_helper_free_packet_data(work);
-		cvmx_fpa_free(work, wqe_pool, 0);
+		cvmx_fpa1_free(work, wqe_pool, 0);
 	}
 
 	/* Step 4b: Special workaround for pass 2 errata */
@@ -2073,7 +1860,7 @@ step2:
 				port = 16;
 
 			if (port != -1) {
-				void *buffer = cvmx_fpa_alloc(packet_pool);
+				void *buffer = cvmx_fpa1_alloc(packet_pool);
 				if (buffer) {
 					int queue = cvmx_pko_get_base_queue(port);
 					cvmx_pko_command_word0_t pko_command;
@@ -2099,7 +1886,7 @@ step2:
 							break;
 						}
 					}
-					cvmx_fpa_free(buffer, packet_pool, 0);
+					cvmx_fpa1_free(buffer, packet_pool, 0);
 
 					/* Wait for the packets to loop back */
 					start_cycle = cvmx_get_cycle();
@@ -2191,7 +1978,7 @@ step2:
 	pool0_buffers = NULL;
 	pool0_buffers_tail = NULL;
 	while (1) {
-		struct cvmx_buffer_list *buffer = cvmx_fpa_alloc(0);
+		struct cvmx_buffer_list *buffer = cvmx_fpa1_alloc(0);
 		if (buffer) {
 			buffer->next = NULL;
 
@@ -2262,7 +2049,7 @@ step2:
 	/* Step 11: Restore the FPA buffers into pool 0 */
 	while (pool0_buffers) {
 		struct cvmx_buffer_list *n = pool0_buffers->next;
-		cvmx_fpa_free(pool0_buffers, 0, 0);
+		cvmx_fpa1_free(pool0_buffers, 0, 0);
 		pool0_buffers = n;
 	}
 
@@ -2281,7 +2068,7 @@ int cvmx_helper_shutdown_fpa_pools(int node)
 	uint64_t pool;
 
 	if (!octeon_has_feature(OCTEON_FEATURE_FPA3)) { /*vinita_to_do implement for 78xx */
-		for (pool = 0; pool < CVMX_FPA_NUM_POOLS; pool++) {
+		for (pool = 0; pool < CVMX_FPA1_NUM_POOLS; pool++) {
 			if (cvmx_fpa_get_block_size(pool) > 0)
 				result |= cvmx_fpa_shutdown_pool(pool);
 		}
@@ -2463,23 +2250,4 @@ void cvmx_helper_setup_simulator_io_buffer_counts(int node, int num_packet_buffe
 	}
 }
 
-void cvmx_helper_set_wqe_no_ptr_mode(bool mode)
-{
-	if (!octeon_has_feature(OCTEON_FEATURE_PKI)) {
-		cvmx_ipd_ctl_status_t ipd_ctl_status;
-		ipd_ctl_status.u64 = cvmx_read_csr(CVMX_IPD_CTL_STATUS);
-		ipd_ctl_status.s.no_wptr = mode;
-		cvmx_write_csr(CVMX_IPD_CTL_STATUS, ipd_ctl_status.u64);
-	}
-}
 
-void cvmx_helper_set_pkt_wqe_le_mode(bool mode)
-{
-	if (!octeon_has_feature(OCTEON_FEATURE_PKI)) {
-		cvmx_ipd_ctl_status_t ipd_ctl_status;
-		ipd_ctl_status.u64 = cvmx_read_csr(CVMX_IPD_CTL_STATUS);
-		ipd_ctl_status.s.pkt_lend = mode;
-		ipd_ctl_status.s.wqe_lend = mode;
-		cvmx_write_csr(CVMX_IPD_CTL_STATUS, ipd_ctl_status.u64);
-	}
-}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-ilk.c b/arch/mips/cavium-octeon/executive/cvmx-ilk.c
index f24fcac..7371b2d 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-ilk.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-ilk.c
@@ -48,7 +48,7 @@
 #include <linux/export.h>
 #include <asm/octeon/cvmx.h>
 #include <asm/octeon/cvmx-sysinfo.h>
-#include <asm/octeon/cvmx-pko.h>
+#include <asm/octeon/cvmx-hwpko.h>
 #include <asm/octeon/cvmx-ilk.h>
 #include <asm/octeon/cvmx-qlm.h>
 #include <asm/octeon/cvmx-ilk-defs.h>
@@ -59,7 +59,7 @@
 #else
 #include "cvmx.h"
 #include "cvmx-sysinfo.h"
-#include "cvmx-pko.h"
+#include "cvmx-hwpko.h"
 #include "cvmx-ilk.h"
 #include "cvmx-qlm.h"
 #include "cvmx-helper-util.h"
diff --git a/arch/mips/cavium-octeon/executive/cvmx-ipd.c b/arch/mips/cavium-octeon/executive/cvmx-ipd.c
index ca8e479..7bdb397 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-ipd.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-ipd.c
@@ -51,7 +51,7 @@
 #include <asm/octeon/cvmx-dbg-defs.h>
 #include <asm/octeon/cvmx-sso-defs.h>
 
-#include <asm/octeon/cvmx-fpa.h>
+#include <asm/octeon/cvmx-fpa1.h>
 #include <asm/octeon/cvmx-wqe.h>
 #include <asm/octeon/cvmx-ipd.h>
 #include <asm/octeon/cvmx-clock.h>
@@ -64,7 +64,7 @@
 #include "cvmx-bootmem.h"
 #include "cvmx-version.h"
 #include "cvmx-error.h"
-#include "cvmx-fpa.h"
+#include "cvmx-fpa1.h"
 #include "cvmx-wqe.h"
 #include "cvmx-ipd.h"
 #include "cvmx-helper-pki.h"
@@ -206,7 +206,7 @@ static void __cvmx_ipd_free_ptr_v1(void)
 	if (ptr_count.s.wqev_cnt) {
 		union cvmx_ipd_wqe_ptr_valid wqe_ptr_valid;
 		wqe_ptr_valid.u64 = cvmx_read_csr(CVMX_IPD_WQE_PTR_VALID);
-		cvmx_fpa_free(cvmx_phys_to_ptr((uint64_t)wqe_ptr_valid.s.ptr << 7),
+		cvmx_fpa1_free(cvmx_phys_to_ptr((uint64_t)wqe_ptr_valid.s.ptr << 7),
 			      wqe_pool, 0);
 	}
 
@@ -220,7 +220,7 @@ static void __cvmx_ipd_free_ptr_v1(void)
 			pwp_fifo.s.raddr = pwp_fifo.s.max_cnts + (pwp_fifo.s.wraddr + i) % pwp_fifo.s.max_cnts;
 			cvmx_write_csr(CVMX_IPD_PWP_PTR_FIFO_CTL, pwp_fifo.u64);
 			pwp_fifo.u64 = cvmx_read_csr(CVMX_IPD_PWP_PTR_FIFO_CTL);
-			cvmx_fpa_free(cvmx_phys_to_ptr((uint64_t)pwp_fifo.s.ptr << 7),
+			cvmx_fpa1_free(cvmx_phys_to_ptr((uint64_t)pwp_fifo.s.ptr << 7),
 				      wqe_pool, 0);
 		}
 		pwp_fifo.s.cena = 1;
@@ -231,7 +231,7 @@ static void __cvmx_ipd_free_ptr_v1(void)
 	if (ptr_count.s.pktv_cnt) {
 		union cvmx_ipd_pkt_ptr_valid pkt_ptr_valid;
 		pkt_ptr_valid.u64 = cvmx_read_csr(CVMX_IPD_PKT_PTR_VALID);
-		cvmx_fpa_free(cvmx_phys_to_ptr((uint64_t)pkt_ptr_valid.s.ptr << 7),
+		cvmx_fpa1_free(cvmx_phys_to_ptr((uint64_t)pkt_ptr_valid.s.ptr << 7),
 			      packet_pool, 0);
 	}
 
@@ -244,7 +244,7 @@ static void __cvmx_ipd_free_ptr_v1(void)
 		cvmx_write_csr(CVMX_IPD_PRC_PORT_PTR_FIFO_CTL,
 			       prc_port_fifo.u64);
 		prc_port_fifo.u64 = cvmx_read_csr(CVMX_IPD_PRC_PORT_PTR_FIFO_CTL);
-		cvmx_fpa_free(cvmx_phys_to_ptr((uint64_t)prc_port_fifo.s.ptr << 7),
+		cvmx_fpa1_free(cvmx_phys_to_ptr((uint64_t)prc_port_fifo.s.ptr << 7),
 			      packet_pool, 0);
 	}
 	prc_port_fifo.s.cena = 1;
@@ -263,7 +263,7 @@ static void __cvmx_ipd_free_ptr_v1(void)
 			cvmx_write_csr(CVMX_IPD_PRC_HOLD_PTR_FIFO_CTL,
 				       prc_hold_fifo.u64);
 			prc_hold_fifo.u64 = cvmx_read_csr(CVMX_IPD_PRC_HOLD_PTR_FIFO_CTL);
-			cvmx_fpa_free(cvmx_phys_to_ptr((uint64_t)prc_hold_fifo.s.ptr << 7),
+			cvmx_fpa1_free(cvmx_phys_to_ptr((uint64_t)prc_hold_fifo.s.ptr << 7),
 				      packet_pool, 0);
 		}
 		prc_hold_fifo.s.cena = 1;
@@ -282,7 +282,7 @@ static void __cvmx_ipd_free_ptr_v1(void)
 			pwp_fifo.s.raddr = (pwp_fifo.s.praddr + i) % pwp_fifo.s.max_cnts;
 			cvmx_write_csr(CVMX_IPD_PWP_PTR_FIFO_CTL, pwp_fifo.u64);
 			pwp_fifo.u64 = cvmx_read_csr(CVMX_IPD_PWP_PTR_FIFO_CTL);
-			cvmx_fpa_free(cvmx_phys_to_ptr((uint64_t)pwp_fifo.s.ptr << 7),
+			cvmx_fpa1_free(cvmx_phys_to_ptr((uint64_t)pwp_fifo.s.ptr << 7),
 				      packet_pool, 0);
 		}
 		pwp_fifo.s.cena = 1;
@@ -313,10 +313,10 @@ static void __cvmx_ipd_free_ptr_v2(void)
 		union cvmx_ipd_next_wqe_ptr next_wqe_ptr;
 		next_wqe_ptr.u64 = cvmx_read_csr(CVMX_IPD_NEXT_WQE_PTR);
 		if (no_wptr)
-			cvmx_fpa_free(cvmx_phys_to_ptr((uint64_t)next_wqe_ptr.s.ptr << 7),
+			cvmx_fpa1_free(cvmx_phys_to_ptr((uint64_t)next_wqe_ptr.s.ptr << 7),
 				      packet_pool, 0);
 		else
-			cvmx_fpa_free(cvmx_phys_to_ptr((uint64_t)next_wqe_ptr.s.ptr << 7),
+			cvmx_fpa1_free(cvmx_phys_to_ptr((uint64_t)next_wqe_ptr.s.ptr << 7),
 				      wqe_pool, 0);
 	}
 
@@ -333,10 +333,10 @@ static void __cvmx_ipd_free_ptr_v2(void)
 			free_fifo.u64 = cvmx_read_csr(CVMX_IPD_FREE_PTR_FIFO_CTL);
 			free_ptr_value.u64 = cvmx_read_csr(CVMX_IPD_FREE_PTR_VALUE);
 			if (no_wptr)
-				cvmx_fpa_free(cvmx_phys_to_ptr((uint64_t)free_ptr_value.s.ptr << 7),
+				cvmx_fpa1_free(cvmx_phys_to_ptr((uint64_t)free_ptr_value.s.ptr << 7),
 					      packet_pool, 0);
 			else
-				cvmx_fpa_free(cvmx_phys_to_ptr((uint64_t)free_ptr_value.s.ptr << 7),
+				cvmx_fpa1_free(cvmx_phys_to_ptr((uint64_t)free_ptr_value.s.ptr << 7),
 					      wqe_pool, 0);
 		}
 		free_fifo.s.cena = 1;
@@ -347,7 +347,7 @@ static void __cvmx_ipd_free_ptr_v2(void)
 	if (ptr_count.s.pktv_cnt) {
 		union cvmx_ipd_next_pkt_ptr next_pkt_ptr;
 		next_pkt_ptr.u64 = cvmx_read_csr(CVMX_IPD_NEXT_PKT_PTR);
-		cvmx_fpa_free(cvmx_phys_to_ptr((uint64_t)next_pkt_ptr.s.ptr << 7),
+		cvmx_fpa1_free(cvmx_phys_to_ptr((uint64_t)next_pkt_ptr.s.ptr << 7),
 			      packet_pool, 0);
 	}
 
@@ -359,7 +359,7 @@ static void __cvmx_ipd_free_ptr_v2(void)
 		port_ptr_fifo.s.raddr = i % port_ptr_fifo.s.max_pkt;
 		cvmx_write_csr(CVMX_IPD_PORT_PTR_FIFO_CTL, port_ptr_fifo.u64);
 		port_ptr_fifo.u64 = cvmx_read_csr(CVMX_IPD_PORT_PTR_FIFO_CTL);
-		cvmx_fpa_free(cvmx_phys_to_ptr((uint64_t)port_ptr_fifo.s.ptr << 7),
+		cvmx_fpa1_free(cvmx_phys_to_ptr((uint64_t)port_ptr_fifo.s.ptr << 7),
 			      packet_pool, 0);
 	}
 	port_ptr_fifo.s.cena = 1;
@@ -377,7 +377,7 @@ static void __cvmx_ipd_free_ptr_v2(void)
 			cvmx_write_csr(CVMX_IPD_HOLD_PTR_FIFO_CTL,
 				       hold_ptr_fifo.u64);
 			hold_ptr_fifo.u64 = cvmx_read_csr(CVMX_IPD_HOLD_PTR_FIFO_CTL);
-			cvmx_fpa_free(cvmx_phys_to_ptr((uint64_t)hold_ptr_fifo.s.ptr << 7),
+			cvmx_fpa1_free(cvmx_phys_to_ptr((uint64_t)hold_ptr_fifo.s.ptr << 7),
 				      packet_pool, 0);
 		}
 		hold_ptr_fifo.s.cena = 1;
@@ -397,7 +397,7 @@ static void __cvmx_ipd_free_ptr_v2(void)
 				       free_fifo.u64);
 			free_fifo.u64 = cvmx_read_csr(CVMX_IPD_FREE_PTR_FIFO_CTL);
 			free_ptr_value.u64 = cvmx_read_csr(CVMX_IPD_FREE_PTR_VALUE);
-			cvmx_fpa_free(cvmx_phys_to_ptr((uint64_t)free_ptr_value.s.ptr << 7),
+			cvmx_fpa1_free(cvmx_phys_to_ptr((uint64_t)free_ptr_value.s.ptr << 7),
 				      packet_pool, 0);
 		}
 		free_fifo.s.cena = 1;
diff --git a/arch/mips/cavium-octeon/executive/cvmx-nand.c b/arch/mips/cavium-octeon/executive/cvmx-nand.c
index 8c48ebc..80c230c 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-nand.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-nand.c
@@ -863,7 +863,6 @@ cvmx_nand_status_t cvmx_nand_initialize(cvmx_nand_initialize_flags_t flags,
 						if (mode_mask & (1 << i))
 							mode = i;
 					}
-					mode = 0;
 					cvmx_nand_state[chip].onfi_timing = mode;
 				} else {
 					nand_debug("%s: Invalid timing mode (%d) in ONFI parameter page, ignoring\n",
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pcie.c b/arch/mips/cavium-octeon/executive/cvmx-pcie.c
index 2870cc2..9058eea 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pcie.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pcie.c
@@ -42,7 +42,7 @@
  *
  * Interface to PCIe as a host(RC) or target(EP)
  *
- * <hr>$Revision: 93891 $<hr>
+ * <hr>$Revision: 95179 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
@@ -187,7 +187,8 @@ static void __cvmx_pcie_rc_initialize_config_space(int pcie_port)
 	/* Error Message Enables (PCIE*_CFG030[CE_EN,NFE_EN,FE_EN,UR_EN]) */
 	{
 		cvmx_pciercx_cfg030_t pciercx_cfg030;
-		pciercx_cfg030.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG030(pcie_port));
+		pciercx_cfg030.u32 = cvmx_pcie_cfgx_read(pcie_port,
+							 CVMX_PCIERCX_CFG030(pcie_port));
 		if (OCTEON_IS_MODEL(OCTEON_CN5XXX)) {
 			pciercx_cfg030.s.mps = MPS_CN5XXX;
 			pciercx_cfg030.s.mrrs = MRRS_CN5XXX;
@@ -201,7 +202,8 @@ static void __cvmx_pcie_rc_initialize_config_space(int pcie_port)
 		pciercx_cfg030.s.nfe_en = 1;	/* Non-fatal error reporting enable. */
 		pciercx_cfg030.s.fe_en = 1;	/* Fatal error reporting enable. */
 		pciercx_cfg030.s.ur_en = 1;	/* Unsupported request reporting enable. */
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG030(pcie_port), pciercx_cfg030.u32);
+		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG030(pcie_port),
+				     pciercx_cfg030.u32);
 	}
 
 	if (octeon_has_feature(OCTEON_FEATURE_NPEI)) {
@@ -237,10 +239,12 @@ static void __cvmx_pcie_rc_initialize_config_space(int pcie_port)
 	/* ECRC Generation (PCIE*_CFG070[GE,CE]) */
 	{
 		cvmx_pciercx_cfg070_t pciercx_cfg070;
-		pciercx_cfg070.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG070(pcie_port));
+		pciercx_cfg070.u32 = cvmx_pcie_cfgx_read(pcie_port,
+							 CVMX_PCIERCX_CFG070(pcie_port));
 		pciercx_cfg070.s.ge = 1;	/* ECRC generation enable. */
 		pciercx_cfg070.s.ce = 1;	/* ECRC check enable. */
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG070(pcie_port), pciercx_cfg070.u32);
+		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG070(pcie_port),
+				     pciercx_cfg070.u32);
 	}
 
 	/* Access Enables (PCIE*_CFG001[MSAE,ME]) */
@@ -249,12 +253,14 @@ static void __cvmx_pcie_rc_initialize_config_space(int pcie_port)
 	/* System Error Message Enable (PCIE*_CFG001[SEE]) */
 	{
 		cvmx_pciercx_cfg001_t pciercx_cfg001;
-		pciercx_cfg001.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG001(pcie_port));
+		pciercx_cfg001.u32 = cvmx_pcie_cfgx_read(pcie_port,
+							 CVMX_PCIERCX_CFG001(pcie_port));
 		pciercx_cfg001.s.msae = 1;	/* Memory space enable. */
 		pciercx_cfg001.s.me = 1;	/* Bus master enable. */
 		pciercx_cfg001.s.i_dis = 1;	/* INTx assertion disable. */
 		pciercx_cfg001.s.see = 1;	/* SERR# enable */
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG001(pcie_port), pciercx_cfg001.u32);
+		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG001(pcie_port),
+				     pciercx_cfg001.u32);
 	}
 
 	/* Advanced Error Recovery Message Enables */
@@ -266,21 +272,28 @@ static void __cvmx_pcie_rc_initialize_config_space(int pcie_port)
 	/* Active State Power Management (PCIE*_CFG032[ASLPC]) */
 	{
 		cvmx_pciercx_cfg032_t pciercx_cfg032;
-		pciercx_cfg032.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG032(pcie_port));
+		pciercx_cfg032.u32 = cvmx_pcie_cfgx_read(pcie_port,
+							 CVMX_PCIERCX_CFG032(pcie_port));
 		pciercx_cfg032.s.aslpc = 0;	/* Active state Link PM control. */
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG032(pcie_port), pciercx_cfg032.u32);
+		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG032(pcie_port),
+				     pciercx_cfg032.u32);
 	}
 
-	/* Link Width Mode (PCIERCn_CFG452[LME]) - Set during cvmx_pcie_rc_initialize_link() */
+	/* Link Width Mode (PCIERCn_CFG452[LME]) - Set during
+	 * cvmx_pcie_rc_initialize_link()
+	 */
 	/* Primary Bus Number (PCIERCn_CFG006[PBNUM]) */
 	{
-		/* We set the primary bus number to 1 so IDT bridges are happy. They don't like zero */
+		/* We set the primary bus number to 1 so IDT bridges are happy.
+		 * They don't like zero
+		 */
 		cvmx_pciercx_cfg006_t pciercx_cfg006;
 		pciercx_cfg006.u32 = 0;
 		pciercx_cfg006.s.pbnum = 1;
 		pciercx_cfg006.s.sbnum = 1;
 		pciercx_cfg006.s.subbnum = 1;
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG006(pcie_port), pciercx_cfg006.u32);
+		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG006(pcie_port),
+				     pciercx_cfg006.u32);
 	}
 
 	/* Memory-mapped I/O BAR (PCIERCn_CFG008) */
@@ -291,7 +304,8 @@ static void __cvmx_pcie_rc_initialize_config_space(int pcie_port)
 		pciercx_cfg008.u32 = 0;
 		pciercx_cfg008.s.mb_addr = 0x100;
 		pciercx_cfg008.s.ml_addr = 0;
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG008(pcie_port), pciercx_cfg008.u32);
+		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG008(pcie_port),
+				     pciercx_cfg008.u32);
 	}
 
 	/* Prefetchable BAR (PCIERCn_CFG009,PCIERCn_CFG010,PCIERCn_CFG011) */
@@ -302,50 +316,62 @@ static void __cvmx_pcie_rc_initialize_config_space(int pcie_port)
 		cvmx_pciercx_cfg009_t pciercx_cfg009;
 		cvmx_pciercx_cfg010_t pciercx_cfg010;
 		cvmx_pciercx_cfg011_t pciercx_cfg011;
-		pciercx_cfg009.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG009(pcie_port));
-		pciercx_cfg010.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG010(pcie_port));
-		pciercx_cfg011.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG011(pcie_port));
+		pciercx_cfg009.u32 = cvmx_pcie_cfgx_read(pcie_port,
+							 CVMX_PCIERCX_CFG009(pcie_port));
+		pciercx_cfg010.u32 = cvmx_pcie_cfgx_read(pcie_port,
+							 CVMX_PCIERCX_CFG010(pcie_port));
+		pciercx_cfg011.u32 = cvmx_pcie_cfgx_read(pcie_port,
+							 CVMX_PCIERCX_CFG011(pcie_port));
 		pciercx_cfg009.s.lmem_base = 0x100;
 		pciercx_cfg009.s.lmem_limit = 0;
 		pciercx_cfg010.s.umem_base = 0x100;
 		pciercx_cfg011.s.umem_limit = 0;
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG009(pcie_port), pciercx_cfg009.u32);
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG010(pcie_port), pciercx_cfg010.u32);
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG011(pcie_port), pciercx_cfg011.u32);
+		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG009(pcie_port),
+				     pciercx_cfg009.u32);
+		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG010(pcie_port),
+				     pciercx_cfg010.u32);
+		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG011(pcie_port),
+				     pciercx_cfg011.u32);
 	}
 
 	/* System Error Interrupt Enables (PCIERCn_CFG035[SECEE,SEFEE,SENFEE]) */
 	/* PME Interrupt Enables (PCIERCn_CFG035[PMEIE]) */
 	{
 		cvmx_pciercx_cfg035_t pciercx_cfg035;
-		pciercx_cfg035.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG035(pcie_port));
+		pciercx_cfg035.u32 = cvmx_pcie_cfgx_read(pcie_port,
+							 CVMX_PCIERCX_CFG035(pcie_port));
 		pciercx_cfg035.s.secee = 1;	/* System error on correctable error enable. */
 		pciercx_cfg035.s.sefee = 1;	/* System error on fatal error enable. */
 		pciercx_cfg035.s.senfee = 1;	/* System error on non-fatal error enable. */
 		pciercx_cfg035.s.pmeie = 1;	/* PME interrupt enable. */
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG035(pcie_port), pciercx_cfg035.u32);
+		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG035(pcie_port),
+				     pciercx_cfg035.u32);
 	}
 
 	/* Advanced Error Recovery Interrupt Enables */
 	/* (PCIERCn_CFG075[CERE,NFERE,FERE]) */
 	{
 		cvmx_pciercx_cfg075_t pciercx_cfg075;
-		pciercx_cfg075.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG075(pcie_port));
+		pciercx_cfg075.u32 = cvmx_pcie_cfgx_read(pcie_port,
+							 CVMX_PCIERCX_CFG075(pcie_port));
 		pciercx_cfg075.s.cere = 1;	/* Correctable error reporting enable. */
 		pciercx_cfg075.s.nfere = 1;	/* Non-fatal error reporting enable. */
 		pciercx_cfg075.s.fere = 1;	/* Fatal error reporting enable. */
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG075(pcie_port), pciercx_cfg075.u32);
+		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG075(pcie_port),
+				     pciercx_cfg075.u32);
 	}
 
 	/* HP Interrupt Enables (PCIERCn_CFG034[HPINT_EN], */
 	/* PCIERCn_CFG034[DLLS_EN,CCINT_EN]) */
 	{
 		cvmx_pciercx_cfg034_t pciercx_cfg034;
-		pciercx_cfg034.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG034(pcie_port));
+		pciercx_cfg034.u32 = cvmx_pcie_cfgx_read(pcie_port,
+							 CVMX_PCIERCX_CFG034(pcie_port));
 		pciercx_cfg034.s.hpint_en = 1;	/* Hot-plug interrupt enable. */
 		pciercx_cfg034.s.dlls_en = 1;	/* Data Link Layer state changed enable */
 		pciercx_cfg034.s.ccint_en = 1;	/* Command completed interrupt enable. */
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG034(pcie_port), pciercx_cfg034.u32);
+		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG034(pcie_port),
+				     pciercx_cfg034.u32);
 	}
 }
 
@@ -368,7 +394,8 @@ static int __cvmx_pcie_rc_initialize_link_gen1(int pcie_port)
 	cvmx_pciercx_cfg448_t pciercx_cfg448;
 
 	/* Set the lane width */
-	pciercx_cfg452.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG452(pcie_port));
+	pciercx_cfg452.u32 = cvmx_pcie_cfgx_read(pcie_port,
+						 CVMX_PCIERCX_CFG452(pcie_port));
 	pescx_ctl_status.u64 = cvmx_read_csr(CVMX_PESCX_CTL_STATUS(pcie_port));
 	if (pescx_ctl_status.s.qlm_cfg == 0) {
 		/* We're in 8 lane (56XX) or 4 lane (54XX) mode */
@@ -377,22 +404,27 @@ static int __cvmx_pcie_rc_initialize_link_gen1(int pcie_port)
 		/* We're in 4 lane (56XX) or 2 lane (52XX) mode */
 		pciercx_cfg452.s.lme = 0x7;
 	}
-	cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG452(pcie_port), pciercx_cfg452.u32);
+	cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG452(pcie_port),
+			     pciercx_cfg452.u32);
 
-	/* CN52XX pass 1.x has an errata where length mismatches on UR responses can
-	   cause bus errors on 64bit memory reads. Turning off length error
-	   checking fixes this */
+	/* CN52XX pass 1.x has an errata where length mismatches on UR responses
+	 * can cause bus errors on 64bit memory reads. Turning off length error
+	 * checking fixes this
+	 */
 	if (OCTEON_IS_MODEL(OCTEON_CN52XX_PASS1_X)) {
 		cvmx_pciercx_cfg455_t pciercx_cfg455;
-		pciercx_cfg455.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG455(pcie_port));
+		pciercx_cfg455.u32 = cvmx_pcie_cfgx_read(pcie_port,
+							 CVMX_PCIERCX_CFG455(pcie_port));
 		pciercx_cfg455.s.m_cpl_len_err = 1;
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG455(pcie_port), pciercx_cfg455.u32);
+		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG455(pcie_port),
+				     pciercx_cfg455.u32);
 	}
 
 	/* Lane swap needs to be manually enabled for CN52XX */
 	if (OCTEON_IS_MODEL(OCTEON_CN52XX) && (pcie_port == 1)) {
 		pescx_ctl_status.s.lane_swp = 1;
-		cvmx_write_csr(CVMX_PESCX_CTL_STATUS(pcie_port), pescx_ctl_status.u64);
+		cvmx_write_csr(CVMX_PESCX_CTL_STATUS(pcie_port),
+			       pescx_ctl_status.u64);
 	}
 
 	/* Bring up the link */
@@ -400,7 +432,9 @@ static int __cvmx_pcie_rc_initialize_link_gen1(int pcie_port)
 	pescx_ctl_status.s.lnk_enb = 1;
 	cvmx_write_csr(CVMX_PESCX_CTL_STATUS(pcie_port), pescx_ctl_status.u64);
 
-	/* CN52XX pass 1.0: Due to a bug in 2nd order CDR, it needs to be disabled */
+	/* CN52XX pass 1.0: Due to a bug in 2nd order CDR, it needs to be
+	 * disabled
+	 */
 	if (OCTEON_IS_MODEL(OCTEON_CN52XX_PASS1_0))
 		__cvmx_helper_errata_qlm_disable_2nd_order_cdr(0);
 
@@ -412,18 +446,21 @@ static int __cvmx_pcie_rc_initialize_link_gen1(int pcie_port)
 			return -1;
 		}
 		cvmx_wait(10000);
-		pciercx_cfg032.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG032(pcie_port));
+		pciercx_cfg032.u32 = cvmx_pcie_cfgx_read(pcie_port,
+							 CVMX_PCIERCX_CFG032(pcie_port));
 	} while ((pciercx_cfg032.s.dlla == 0) || (pciercx_cfg032.s.lt == 1));
 
 	/* Clear all pending errors */
-	cvmx_write_csr(CVMX_PEXP_NPEI_INT_SUM, cvmx_read_csr(CVMX_PEXP_NPEI_INT_SUM));
+	cvmx_write_csr(CVMX_PEXP_NPEI_INT_SUM,
+		       cvmx_read_csr(CVMX_PEXP_NPEI_INT_SUM));
 
 	/* Update the Replay Time Limit. Empirically, some PCIe devices take a
 	   little longer to respond than expected under load. As a workaround for
 	   this we configure the Replay Time Limit to the value expected for a 512
 	   byte MPS instead of our actual 256 byte MPS. The numbers below are
 	   directly from the PCIe spec table 3-4 */
-	pciercx_cfg448.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG448(pcie_port));
+	pciercx_cfg448.u32 = cvmx_pcie_cfgx_read(pcie_port,
+						 CVMX_PCIERCX_CFG448(pcie_port));
 	switch (pciercx_cfg032.s.nlw) {
 	case 1:		/* 1 lane */
 		pciercx_cfg448.s.rtl = 1677;
@@ -438,7 +475,8 @@ static int __cvmx_pcie_rc_initialize_link_gen1(int pcie_port)
 		pciercx_cfg448.s.rtl = 258;
 		break;
 	}
-	cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG448(pcie_port), pciercx_cfg448.u32);
+	cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG448(pcie_port),
+			     pciercx_cfg448.u32);
 
 	return 0;
 }
@@ -481,14 +519,18 @@ static int __cvmx_pcie_rc_initialize_gen1(int pcie_port)
 	}
 
 retry:
-	/* Make sure we aren't trying to setup a target mode interface in host mode */
+	/* Make sure we aren't trying to setup a target mode interface in host
+	 * mode
+	 */
 	npei_ctl_status.u64 = cvmx_read_csr(CVMX_PEXP_NPEI_CTL_STATUS);
 	if ((pcie_port == 0) && !npei_ctl_status.s.host_mode) {
 		cvmx_dprintf("PCIe: Port %d in endpoint mode\n", pcie_port);
 		return -1;
 	}
 
-	/* Make sure a CN52XX isn't trying to bring up port 1 when it is disabled */
+	/* Make sure a CN52XX isn't trying to bring up port 1 when it is
+	 * disabled
+	 */
 	if (OCTEON_IS_MODEL(OCTEON_CN52XX)) {
 		npei_dbg_data.u64 = cvmx_read_csr(CVMX_PEXP_NPEI_DBG_DATA);
 		if ((pcie_port == 1) && npei_dbg_data.cn52xx.qlm0_link_width) {
@@ -497,11 +539,15 @@ retry:
 		}
 	}
 
-	/* PCIe switch arbitration mode. '0' == fixed priority NPEI, PCIe0, then PCIe1. '1' == round robin. */
+	/* PCIe switch arbitration mode. '0' == fixed priority NPEI, PCIe0,
+	 * then PCIe1. '1' == round robin.
+	 */
 	npei_ctl_status.s.arb = 1;
 	/* Allow up to 0x20 config retries */
 	npei_ctl_status.s.cfg_rtry = 0x20;
-	/* CN52XX pass1.x has an errata where P0_NTAGS and P1_NTAGS don't reset */
+	/* CN52XX pass1.x has an errata where P0_NTAGS and P1_NTAGS don't
+	 * reset
+	 */
 	if (OCTEON_IS_MODEL(OCTEON_CN52XX_PASS1_X)) {
 		npei_ctl_status.s.p0_ntags = 0x20;
 		npei_ctl_status.s.p1_ntags = 0x20;
@@ -510,22 +556,27 @@ retry:
 
 	/* Bring the PCIe out of reset */
 	if (cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_EBH5200) {
-		/* The EBH5200 board swapped the PCIe reset lines on the board. As a
-		   workaround for this bug, we bring both PCIe ports out of reset at
-		   the same time instead of on separate calls. So for port 0, we bring
-		   both out of reset and do nothing on port 1 */
+		/* The EBH5200 board swapped the PCIe reset lines on the board.
+		 * As a workaround for this bug, we bring both PCIe ports out
+		 * of reset at the same time instead of on separate calls.  So
+		 * for port 0, we bring both out of reset and do nothing on
+		 * port 1
+		 */
 		if (pcie_port == 0) {
 			ciu_soft_prst.u64 = cvmx_read_csr(CVMX_CIU_SOFT_PRST);
-			/* After a chip reset the PCIe will also be in reset. If it isn't,
-			   most likely someone is trying to init it again without a proper
-			   PCIe reset */
+			/* After a chip reset the PCIe will also be in reset.
+			 * If it isn't, most likely someone is trying to init
+			 * it again without a proper PCIe reset
+			 */
 			if (ciu_soft_prst.s.soft_prst == 0) {
 				/* Reset the ports */
 				ciu_soft_prst.s.soft_prst = 1;
-				cvmx_write_csr(CVMX_CIU_SOFT_PRST, ciu_soft_prst.u64);
+				cvmx_write_csr(CVMX_CIU_SOFT_PRST,
+					       ciu_soft_prst.u64);
 				ciu_soft_prst.u64 = cvmx_read_csr(CVMX_CIU_SOFT_PRST1);
 				ciu_soft_prst.s.soft_prst = 1;
-				cvmx_write_csr(CVMX_CIU_SOFT_PRST1, ciu_soft_prst.u64);
+				cvmx_write_csr(CVMX_CIU_SOFT_PRST1,
+					       ciu_soft_prst.u64);
 				/* Wait until pcie resets the ports. */
 				cvmx_wait_usec(2000);
 			}
@@ -537,22 +588,26 @@ retry:
 			cvmx_write_csr(CVMX_CIU_SOFT_PRST, ciu_soft_prst.u64);
 		}
 	} else {
-		/* The normal case: The PCIe ports are completely separate and can be
-		   brought out of reset independently */
+		/* The normal case: The PCIe ports are completely separate and
+		 * can be brought out of reset independently
+		 */
 		if (pcie_port)
 			ciu_soft_prst.u64 = cvmx_read_csr(CVMX_CIU_SOFT_PRST1);
 		else
 			ciu_soft_prst.u64 = cvmx_read_csr(CVMX_CIU_SOFT_PRST);
-		/* After a chip reset the PCIe will also be in reset. If it isn't,
-		   most likely someone is trying to init it again without a proper
-		   PCIe reset */
+		/* After a chip reset the PCIe will also be in reset.  If it
+		 * isn't, most likely someone is trying to init it again
+		 * without a proper PCIe reset
+		 */
 		if (ciu_soft_prst.s.soft_prst == 0) {
 			/* Reset the port */
 			ciu_soft_prst.s.soft_prst = 1;
 			if (pcie_port)
-				cvmx_write_csr(CVMX_CIU_SOFT_PRST1, ciu_soft_prst.u64);
+				cvmx_write_csr(CVMX_CIU_SOFT_PRST1,
+					       ciu_soft_prst.u64);
 			else
-				cvmx_write_csr(CVMX_CIU_SOFT_PRST, ciu_soft_prst.u64);
+				cvmx_write_csr(CVMX_CIU_SOFT_PRST,
+					       ciu_soft_prst.u64);
 			/* Wait until pcie resets the ports. */
 			cvmx_wait_usec(2000);
 		}
@@ -567,53 +622,68 @@ retry:
 		}
 	}
 
-	/* Wait for PCIe reset to complete. Due to errata PCIE-700, we don't poll
-	   PESCX_CTL_STATUS2[PCIERST], but simply wait a fixed number of cycles */
+	/* Wait for PCIe reset to complete. Due to errata PCIE-700, we don't
+	 * poll PESCX_CTL_STATUS2[PCIERST], but simply wait a fixed number of
+	 * cycles
+	 */
 	cvmx_wait(400000);
 
 	/* PESCX_BIST_STATUS2[PCLK_RUN] was missing on pass 1 of CN56XX and
-	   CN52XX, so we only probe it on newer chips */
-	if (!OCTEON_IS_MODEL(OCTEON_CN56XX_PASS1_X) && !OCTEON_IS_MODEL(OCTEON_CN52XX_PASS1_X)) {
+	 * CN52XX, so we only probe it on newer chips
+	 */
+	if (!OCTEON_IS_MODEL(OCTEON_CN56XX_PASS1_X) &&
+	    !OCTEON_IS_MODEL(OCTEON_CN52XX_PASS1_X)) {
 		/* Clear PCLK_RUN so we can check if the clock is running */
 		pescx_ctl_status2.u64 = cvmx_read_csr(CVMX_PESCX_CTL_STATUS2(pcie_port));
 		pescx_ctl_status2.s.pclk_run = 1;
-		cvmx_write_csr(CVMX_PESCX_CTL_STATUS2(pcie_port), pescx_ctl_status2.u64);
-		/* Now that we cleared PCLK_RUN, wait for it to be set again telling
-		   us the clock is running */
-		if (CVMX_WAIT_FOR_FIELD64(CVMX_PESCX_CTL_STATUS2(pcie_port), cvmx_pescx_ctl_status2_t, pclk_run, ==, 1, 10000)) {
-			cvmx_dprintf("PCIe: Port %d isn't clocked, skipping.\n", pcie_port);
+		cvmx_write_csr(CVMX_PESCX_CTL_STATUS2(pcie_port),
+			       pescx_ctl_status2.u64);
+		/* Now that we cleared PCLK_RUN, wait for it to be set again
+		 * telling us the clock is running
+		 */
+		if (CVMX_WAIT_FOR_FIELD64(CVMX_PESCX_CTL_STATUS2(pcie_port),
+					  cvmx_pescx_ctl_status2_t, pclk_run,
+					  ==, 1, 10000)) {
+			cvmx_dprintf("PCIe: Port %d isn't clocked, skipping.\n",
+				     pcie_port);
 			return -1;
 		}
 	}
 
 	/* Check and make sure PCIe came out of reset. If it doesn't the board
-	   probably hasn't wired the clocks up and the interface should be
-	   skipped */
+	 * probably hasn't wired the clocks up and the interface should be
+	 * skipped
+	 */
 	pescx_ctl_status2.u64 = cvmx_read_csr(CVMX_PESCX_CTL_STATUS2(pcie_port));
 	if (pescx_ctl_status2.s.pcierst) {
-		cvmx_dprintf("PCIe: Port %d stuck in reset, skipping.\n", pcie_port);
+		cvmx_dprintf("PCIe: Port %d stuck in reset, skipping.\n",
+			     pcie_port);
 		return -1;
 	}
 
-	/* Check BIST2 status. If any bits are set skip this interface. This
-	   is an attempt to catch PCIE-813 on pass 1 parts */
+	/* Check BIST2 status. If any bits are set skip this interface.  This
+	 * is an attempt to catch PCIE-813 on pass 1 parts
+	 */
 	pescx_bist_status2.u64 = cvmx_read_csr(CVMX_PESCX_BIST_STATUS2(pcie_port));
 	if (pescx_bist_status2.u64) {
-		cvmx_dprintf("PCIe: Port %d BIST2 failed. Most likely this port isn't hooked up, skipping.\n", pcie_port);
+		cvmx_dprintf("PCIe: Port %d BIST2 failed. Most likely this port isn't hooked up, skipping.\n",
+			     pcie_port);
 		return -1;
 	}
 
 	/* Check BIST status */
 	pescx_bist_status.u64 = cvmx_read_csr(CVMX_PESCX_BIST_STATUS(pcie_port));
 	if (pescx_bist_status.u64)
-		cvmx_dprintf("PCIe: BIST FAILED for port %d (0x%016llx)\n", pcie_port, CAST64(pescx_bist_status.u64));
+		cvmx_dprintf("PCIe: BIST FAILED for port %d (0x%016llx)\n",
+			     pcie_port, CAST64(pescx_bist_status.u64));
 
 	/* Initialize the config space CSRs */
 	__cvmx_pcie_rc_initialize_config_space(pcie_port);
 
 	/* Bring the link up */
 	if (__cvmx_pcie_rc_initialize_link_gen1(pcie_port)) {
-		cvmx_dprintf("PCIe: Failed to initialize port %d, probably the slot is empty\n", pcie_port);
+		cvmx_dprintf("PCIe: Failed to initialize port %d, probably the slot is empty\n",
+			     pcie_port);
 		return -1;
 	}
 
@@ -635,9 +705,12 @@ retry:
 	mem_access_subid.s.row = 0;	/* Disable Relaxed Ordering for Writes. */
 	mem_access_subid.s.ba = 0;	/* PCIe Adddress Bits <63:34>. */
 
-	/* Setup mem access 12-15 for port 0, 16-19 for port 1, supplying 36 bits of address space */
+	/* Setup mem access 12-15 for port 0, 16-19 for port 1, supplying 36
+	 * bits of address space
+	 */
 	for (i = 12 + pcie_port * 4; i < 16 + pcie_port * 4; i++) {
-		cvmx_write_csr(CVMX_PEXP_NPEI_MEM_ACCESS_SUBIDX(i), mem_access_subid.u64);
+		cvmx_write_csr(CVMX_PEXP_NPEI_MEM_ACCESS_SUBIDX(i),
+			       mem_access_subid.u64);
 		mem_access_subid.s.ba += 1;	/* Set each SUBID to extend the addressable range */
 	}
 
@@ -653,7 +726,8 @@ retry:
 	cvmx_write_csr(CVMX_PESCX_P2N_BAR0_START(pcie_port), 0);
 
 	/* BAR1 follows BAR2 with a gap so it has the same address as for gen2. */
-	cvmx_write_csr(CVMX_PESCX_P2N_BAR1_START(pcie_port), CVMX_PCIE_BAR1_RC_BASE);
+	cvmx_write_csr(CVMX_PESCX_P2N_BAR1_START(pcie_port),
+		       CVMX_PCIE_BAR1_RC_BASE);
 
 	bar1_index.u32 = 0;
 	bar1_index.s.addr_idx = (CVMX_PCIE_BAR1_PHYS_BASE >> 22);
@@ -711,34 +785,43 @@ retry:
 	}
 
 	/* Both pass 1 and pass 2 of CN52XX and CN56XX have an errata that causes
-	   TLP ordering to not be preserved after multiple PCIe port resets. This
-	   code detects this fault and corrects it by aligning the TLP counters
-	   properly. Another link reset is then performed. See PCIE-13340 */
-	if (OCTEON_IS_MODEL(OCTEON_CN56XX_PASS2_X) || OCTEON_IS_MODEL(OCTEON_CN52XX_PASS2_X) || OCTEON_IS_MODEL(OCTEON_CN56XX_PASS1_X) || OCTEON_IS_MODEL(OCTEON_CN52XX_PASS1_X)) {
+	 * TLP ordering to not be preserved after multiple PCIe port resets. This
+	 * code detects this fault and corrects it by aligning the TLP counters
+	 * properly. Another link reset is then performed. See PCIE-13340
+	 */
+	if (OCTEON_IS_MODEL(OCTEON_CN56XX_PASS2_X) ||
+	    OCTEON_IS_MODEL(OCTEON_CN52XX_PASS2_X) ||
+	    OCTEON_IS_MODEL(OCTEON_CN56XX_PASS1_X) ||
+	    OCTEON_IS_MODEL(OCTEON_CN52XX_PASS1_X)) {
 		cvmx_npei_dbg_data_t dbg_data;
 		int old_in_fif_p_count;
 		int in_fif_p_count;
 		int out_p_count;
-		int in_p_offset = (OCTEON_IS_MODEL(OCTEON_CN52XX_PASS1_X) || OCTEON_IS_MODEL(OCTEON_CN56XX_PASS1_X)) ? 4 : 1;
+		int in_p_offset = (OCTEON_IS_MODEL(OCTEON_CN52XX_PASS1_X) ||
+				   OCTEON_IS_MODEL(OCTEON_CN56XX_PASS1_X)) ? 4 : 1;
 		int i;
 
-		/* Choose a write address of 1MB. It should be harmless as all bars
-		   haven't been setup */
+		/* Choose a write address of 1MB. It should be harmless as all
+		 * bars haven't been setup
+		 */
 		uint64_t write_address = (cvmx_pcie_get_mem_base_address(pcie_port) + 0x100000) | (1ull << 63);
 
-		/* Make sure at least in_p_offset have been executed before we try and
-		   read in_fif_p_count */
+		/* Make sure at least in_p_offset have been executed before we
+		 * try and read in_fif_p_count
+		 */
 		i = in_p_offset;
 		while (i--) {
 			cvmx_write64_uint32(write_address, 0);
 			cvmx_wait(10000);
 		}
 
-		/* Read the IN_FIF_P_COUNT from the debug select. IN_FIF_P_COUNT can be
-		   unstable sometimes so read it twice with a write between the reads.
-		   This way we can tell the value is good as it will increment by one
-		   due to the write */
-		cvmx_write_csr(CVMX_PEXP_NPEI_DBG_SELECT, (pcie_port) ? 0xd7fc : 0xcffc);
+		/* Read the IN_FIF_P_COUNT from the debug select.
+		 * IN_FIF_P_COUNT can be unstable sometimes so read it twice
+		 * with a write between the reads.  This way we can tell the
+		 * value is good as it will increment by one due to the write
+		 */
+		cvmx_write_csr(CVMX_PEXP_NPEI_DBG_SELECT,
+			       (pcie_port) ? 0xd7fc : 0xcffc);
 		cvmx_read_csr(CVMX_PEXP_NPEI_DBG_SELECT);
 		do {
 			dbg_data.u64 = cvmx_read_csr(CVMX_PEXP_NPEI_DBG_DATA);
@@ -749,28 +832,35 @@ retry:
 			in_fif_p_count = dbg_data.s.data & 0xff;
 		} while (in_fif_p_count != ((old_in_fif_p_count + 1) & 0xff));
 
-		/* Update in_fif_p_count for it's offset with respect to out_p_count */
+		/* Update in_fif_p_count for it's offset with respect to
+		 * out_p_count
+		 */
 		in_fif_p_count = (in_fif_p_count + in_p_offset) & 0xff;
 
 		/* Read the OUT_P_COUNT from the debug select */
-		cvmx_write_csr(CVMX_PEXP_NPEI_DBG_SELECT, (pcie_port) ? 0xd00f : 0xc80f);
+		cvmx_write_csr(CVMX_PEXP_NPEI_DBG_SELECT,
+			       (pcie_port) ? 0xd00f : 0xc80f);
 		cvmx_read_csr(CVMX_PEXP_NPEI_DBG_SELECT);
 		dbg_data.u64 = cvmx_read_csr(CVMX_PEXP_NPEI_DBG_DATA);
 		out_p_count = (dbg_data.s.data >> 1) & 0xff;
 
 		/* Check that the two counters are aligned */
 		if (out_p_count != in_fif_p_count) {
-			cvmx_dprintf("PCIe: Port %d aligning TLP counters as workaround to maintain ordering\n", pcie_port);
+			cvmx_dprintf("PCIe: Port %d aligning TLP counters as workaround to maintain ordering\n",
+				     pcie_port);
 			while (in_fif_p_count != 0) {
 				cvmx_write64_uint32(write_address, 0);
 				cvmx_wait(10000);
 				in_fif_p_count = (in_fif_p_count + 1) & 0xff;
 			}
-			/* The EBH5200 board swapped the PCIe reset lines on the board. This
-			   means we must bring both links down and up, which will cause the
-			   PCIe0 to need alignment again. Lots of messages will be displayed,
-			   but everything should work */
-			if ((cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_EBH5200) && (pcie_port == 1))
+			/* The EBH5200 board swapped the PCIe reset lines on
+			 * the board.  This means we must bring both links down
+			 * and up, which will cause the PCIe0 to need alignment
+			 * again. Lots of messages will be displayed, but
+			 * everything should work
+			 */
+			if ((cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_EBH5200)
+			    && (pcie_port == 1))
 				cvmx_pcie_rc_initialize(0);
 			/* Rety bringing this port up */
 			goto retry;
@@ -778,8 +868,10 @@ retry:
 	}
 
 	/* Display the link status */
-	pciercx_cfg032.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG032(pcie_port));
-	cvmx_dprintf("PCIe: Port %d link active, %d lanes\n", pcie_port, pciercx_cfg032.s.nlw);
+	pciercx_cfg032.u32 = cvmx_pcie_cfgx_read(pcie_port,
+						 CVMX_PCIERCX_CFG032(pcie_port));
+	cvmx_dprintf("PCIe: Port %d link active, %d lanes\n",
+		     pcie_port, pciercx_cfg032.s.nlw);
 
 	return 0;
 }
@@ -825,15 +917,18 @@ static int __cvmx_pcie_rc_initialize_link_gen2(int pcie_port)
 		if (cvmx_get_cycle() - start_cycle > cvmx_clock_get_rate(CVMX_CLOCK_CORE))
 			return -1;
 		cvmx_wait(10000);
-		pciercx_cfg032.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG032(pcie_port));
+		pciercx_cfg032.u32 = cvmx_pcie_cfgx_read(pcie_port,
+							 CVMX_PCIERCX_CFG032(pcie_port));
 	} while ((pciercx_cfg032.s.dlla == 0) || (pciercx_cfg032.s.lt == 1));
 
-	/* Update the Replay Time Limit. Empirically, some PCIe devices take a
-	   little longer to respond than expected under load. As a workaround for
-	   this we configure the Replay Time Limit to the value expected for a 512
-	   byte MPS instead of our actual 256 byte MPS. The numbers below are
-	   directly from the PCIe spec table 3-4 */
-	pciercx_cfg448.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG448(pcie_port));
+	/* Update the Replay Time Limit.  Empirically, some PCIe devices take a
+	 * little longer to respond than expected under load. As a workaround
+	 * for this we configure the Replay Time Limit to the value expected
+	 * for a 512 byte MPS instead of our actual 256 byte MPS. The numbers
+	 * below are directly from the PCIe spec table 3-4
+	 */
+	pciercx_cfg448.u32 = cvmx_pcie_cfgx_read(pcie_port,
+						 CVMX_PCIERCX_CFG448(pcie_port));
 	switch (pciercx_cfg032.s.nlw) {
 	case 1:		/* 1 lane */
 		pciercx_cfg448.s.rtl = 1677;
@@ -848,7 +943,8 @@ static int __cvmx_pcie_rc_initialize_link_gen2(int pcie_port)
 		pciercx_cfg448.s.rtl = 258;
 		break;
 	}
-	cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG448(pcie_port), pciercx_cfg448.u32);
+	cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG448(pcie_port),
+			     pciercx_cfg448.u32);
 
 	return 0;
 }
@@ -891,7 +987,7 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 	if (OCTEON_IS_MODEL(OCTEON_CN70XX)) {
 		if (cvmx_qlm_get_dlm_mode(1, pcie_port) == CVMX_QLM_MODE_DISABLED) {
 			cvmx_dprintf("PCIe: Port %d not in PCIe mode, skipping\n",
-						pcie_port);
+				     pcie_port);
 			return -1;
 		}
 	} else if (octeon_has_feature(OCTEON_FEATURE_PCIE)) {
@@ -906,7 +1002,8 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 				qlm = 1;
 		}
 		/* PCIe is allowed only in QLM1, 1 PCIe port in x2 or
-		   2 PCIe ports in x1 */
+		 * 2 PCIe ports in x1
+		 */
 		else if (OCTEON_IS_MODEL(OCTEON_CNF71XX))
 			qlm = 1;
 
@@ -914,23 +1011,28 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 		if (mode == CVMX_QLM_MODE_SRIO_1X4 ||
 		    mode == CVMX_QLM_MODE_SRIO_2X2 ||
 		    mode == CVMX_QLM_MODE_SRIO_4X1) {
-			cvmx_dprintf("PCIe: Port %d is SRIO, skipping.\n", pcie_port);
+			cvmx_dprintf("PCIe: Port %d is SRIO, skipping.\n",
+				     pcie_port);
 			return -1;
 		} else if (mode == CVMX_QLM_MODE_SGMII) {
-			cvmx_dprintf("PCIe: Port %d is SGMII, skipping.\n", pcie_port);
+			cvmx_dprintf("PCIe: Port %d is SGMII, skipping.\n",
+				     pcie_port);
 			return -1;
 		} else if (mode == CVMX_QLM_MODE_XAUI ||
 			   mode == CVMX_QLM_MODE_RXAUI) {
-			cvmx_dprintf("PCIe: Port %d is XAUI, skipping.\n", pcie_port);
+			cvmx_dprintf("PCIe: Port %d is XAUI, skipping.\n",
+				     pcie_port);
 			return -1;
 		} else if (mode == CVMX_QLM_MODE_ILK) {
-			cvmx_dprintf("PCIe: Port %d is ILK, skipping.\n", pcie_port);
+			cvmx_dprintf("PCIe: Port %d is ILK, skipping.\n",
+				     pcie_port);
 			return -1;
 		} else if (mode != CVMX_QLM_MODE_PCIE &&
 			   mode != CVMX_QLM_MODE_PCIE_1X2 &&
 			   mode != CVMX_QLM_MODE_PCIE_2X1 &&
 			   mode != CVMX_QLM_MODE_PCIE_1X1) {
-			cvmx_dprintf("PCIe: Port %d is unknown, skipping.\n", pcie_port);
+			cvmx_dprintf("PCIe: Port %d is unknown, skipping.\n",
+				     pcie_port);
 			return -1;
 		}
 	}
@@ -953,7 +1055,9 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 	cvmx_helper_qlm_jtag_update(pcie_port);
 #endif
 
-	/* Make sure we aren't trying to setup a target mode interface in host mode */
+	/* Make sure we aren't trying to setup a target mode interface in host
+	 * mode
+	 */
 	if (OCTEON_IS_OCTEON3()) {
 		ciu_soft_prst_reg = CVMX_RST_SOFT_PRSTX(pcie_port);
 		rst_ctl_reg = CVMX_RST_CTLX(pcie_port);
@@ -996,7 +1100,9 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 	__cvmx_qlm_pcie_cfg_rxd_set_tweak(qlm, -1);
 #endif
 
-	/* CN63XX Pass 1.0 errata G-14395 requires the QLM De-emphasis be programmed */
+	/* CN63XX Pass 1.0 errata G-14395 requires the QLM De-emphasis be
+	 * programmed
+	 */
 	if (OCTEON_IS_MODEL(OCTEON_CN63XX_PASS1_0)) {
 		if (pcie_port) {
 			cvmx_ciu_qlm1_t ciu_qlm;
@@ -1131,15 +1237,18 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 	/* Check and make sure PCIe came out of reset. If it doesn't the board
 	   probably hasn't wired the clocks up and the interface should be
 	   skipped */
-	if (CVMX_WAIT_FOR_FIELD64(rst_ctl_reg, cvmx_mio_rst_ctlx_t, rst_done, ==, 1, 10000)) {
-		cvmx_dprintf("PCIe: Port %d stuck in reset, skipping.\n", pcie_port);
+	if (CVMX_WAIT_FOR_FIELD64(rst_ctl_reg, cvmx_mio_rst_ctlx_t,
+				  rst_done, ==, 1, 10000)) {
+		cvmx_dprintf("PCIe: Port %d stuck in reset, skipping.\n",
+			     pcie_port);
 		return -1;
 	}
 
 	/* Check BIST status */
 	pemx_bist_status.u64 = cvmx_read_csr(CVMX_PEMX_BIST_STATUS(pcie_port));
 	if (pemx_bist_status.u64)
-		cvmx_dprintf("PCIe: BIST FAILED for port %d (0x%016llx)\n", pcie_port, CAST64(pemx_bist_status.u64));
+		cvmx_dprintf("PCIe: BIST FAILED for port %d (0x%016llx)\n",
+			     pcie_port, CAST64(pemx_bist_status.u64));
 	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
 		pemx_bist_status2.u64 = 0;
 	else
@@ -1148,26 +1257,33 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 	if (OCTEON_IS_MODEL(OCTEON_CN63XX_PASS1_X))
 		pemx_bist_status2.u64 &= ~0x3full;
 	if (pemx_bist_status2.u64)
-		cvmx_dprintf("PCIe: BIST2 FAILED for port %d (0x%016llx)\n", pcie_port, CAST64(pemx_bist_status2.u64));
+		cvmx_dprintf("PCIe: BIST2 FAILED for port %d (0x%016llx)\n",
+			     pcie_port, CAST64(pemx_bist_status2.u64));
 
 	/* Initialize the config space CSRs */
 	__cvmx_pcie_rc_initialize_config_space(pcie_port);
 
 	/* Enable gen2 speed selection */
-	pciercx_cfg515.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG515(pcie_port));
+	pciercx_cfg515.u32 = cvmx_pcie_cfgx_read(pcie_port,
+						 CVMX_PCIERCX_CFG515(pcie_port));
 	pciercx_cfg515.s.dsc = 1;
-	cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG515(pcie_port), pciercx_cfg515.u32);
+	cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG515(pcie_port),
+			     pciercx_cfg515.u32);
 
 	/* Bring the link up */
 	if (__cvmx_pcie_rc_initialize_link_gen2(pcie_port)) {
-		/* Some gen1 devices don't handle the gen 2 training correctly. Disable
-		   gen2 and try again with only gen1 */
+		/* Some gen1 devices don't handle the gen 2 training correctly.
+		 * Disable gen2 and try again with only gen1
+		 */
 		cvmx_pciercx_cfg031_t pciercx_cfg031;
-		pciercx_cfg031.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG031(pcie_port));
+		pciercx_cfg031.u32 = cvmx_pcie_cfgx_read(pcie_port,
+							 CVMX_PCIERCX_CFG031(pcie_port));
 		pciercx_cfg031.s.mls = 1;
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG031(pcie_port), pciercx_cfg031.u32);
+		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG031(pcie_port),
+				     pciercx_cfg031.u32);
 		if (__cvmx_pcie_rc_initialize_link_gen2(pcie_port)) {
-			cvmx_dprintf("PCIe: Link timeout on port %d, probably the slot is empty\n", pcie_port);
+			cvmx_dprintf("PCIe: Link timeout on port %d, probably the slot is empty\n",
+				     pcie_port);
 			return -1;
 		}
 	}
@@ -1192,9 +1308,12 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 	else
 		mem_access_subid.cn63xx.ba = 0;
 
-	/* Setup mem access 12-15 for port 0, 16-19 for port 1, supplying 36 bits of address space */
+	/* Setup mem access 12-15 for port 0, 16-19 for port 1, supplying 36
+	 * bits of address space
+	 */
 	for (i = 12 + pcie_port * 4; i < 16 + pcie_port * 4; i++) {
-		cvmx_write_csr(CVMX_PEXP_SLI_MEM_ACCESS_SUBIDX(i), mem_access_subid.u64);
+		cvmx_write_csr(CVMX_PEXP_SLI_MEM_ACCESS_SUBIDX(i),
+			       mem_access_subid.u64);
 		/* Set each SUBID to extend the addressable range */
 		__cvmx_increment_ba(&mem_access_subid);
 	}
@@ -1203,9 +1322,10 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 	    OCTEON_IS_MODEL(OCTEON_CN66XX) ||
 	    OCTEON_IS_MODEL(OCTEON_CN68XX) ||
 	    OCTEON_IS_MODEL(OCTEON_CN78XX)) {
-		/* Disable the peer to peer forwarding register. This must be setup
-		   by the OS after it enumerates the bus and assigns addresses to the
-		   PCIe busses */
+		/* Disable the peer to peer forwarding register. This must be
+		 * setup by the OS after it enumerates the bus and assigns
+		 * addresses to the PCIe busses
+		 */
 		for (i = 0; i < 4; i++) {
 			cvmx_write_csr(CVMX_PEMX_P2P_BARX_START(i, pcie_port), -1);
 			cvmx_write_csr(CVMX_PEMX_P2P_BARX_END(i, pcie_port), -1);
@@ -1216,8 +1336,9 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 	cvmx_write_csr(CVMX_PEMX_P2N_BAR0_START(pcie_port), 0);
 
 	/* Set Octeon's BAR2 to decode 0-2^41. Bar0 and Bar1 take precedence
-	   where they overlap. It also overlaps with the device addresses, so
-	   make sure the peer to peer forwarding is set right */
+	 * where they overlap. It also overlaps with the device addresses, so
+	 * make sure the peer to peer forwarding is set right
+	 */
 	cvmx_write_csr(CVMX_PEMX_P2N_BAR2_START(pcie_port), 0);
 
 	/* Setup BAR2 attributes */
@@ -1239,7 +1360,8 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 	cvmx_write_csr(CVMX_PEXP_SLI_CTL_PORTX(pcie_port), sli_ctl_portx.u64);
 
 	/* BAR1 follows BAR2 */
-	cvmx_write_csr(CVMX_PEMX_P2N_BAR1_START(pcie_port), CVMX_PCIE_BAR1_RC_BASE);
+	cvmx_write_csr(CVMX_PEMX_P2N_BAR1_START(pcie_port),
+		       CVMX_PCIE_BAR1_RC_BASE);
 
 	bar1_index.u64 = 0;
 	bar1_index.s.addr_idx = (CVMX_PCIE_BAR1_PHYS_BASE >> 22);
@@ -1248,7 +1370,8 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 	bar1_index.s.addr_v = 1;	/* Valid entry */
 
 	for (i = 0; i < 16; i++) {
-		cvmx_write_csr(CVMX_PEMX_BAR1_INDEXX(i, pcie_port), bar1_index.u64);
+		cvmx_write_csr(CVMX_PEMX_BAR1_INDEXX(i, pcie_port),
+			       bar1_index.u64);
 		/* 256MB / 16 >> 22 == 4 */
 		bar1_index.s.addr_idx += (((1ull << 28) / 16ull) >> 22);
 	}
@@ -1260,8 +1383,10 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 	cvmx_write_csr(CVMX_PEMX_CTL_STATUS(pcie_port), pemx_ctl_status.u64);
 
 	/* Display the link status */
-	pciercx_cfg032.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG032(pcie_port));
-	cvmx_dprintf("PCIe: Port %d link active, %d lanes, speed gen%d\n", pcie_port, pciercx_cfg032.s.nlw, pciercx_cfg032.s.ls);
+	pciercx_cfg032.u32 = cvmx_pcie_cfgx_read(pcie_port,
+						 CVMX_PCIERCX_CFG032(pcie_port));
+	cvmx_dprintf("PCIe: Port %d link active, %d lanes, speed gen%d\n",
+		     pcie_port, pciercx_cfg032.s.nlw, pciercx_cfg032.s.ls);
 
 	return 0;
 }
@@ -1301,11 +1426,16 @@ int cvmx_pcie_rc_shutdown(int pcie_port)
 #endif
 	/* Wait for all pending operations to complete */
 	if (octeon_has_feature(OCTEON_FEATURE_NPEI)) {
-		if (CVMX_WAIT_FOR_FIELD64(CVMX_PESCX_CPL_LUT_VALID(pcie_port), cvmx_pescx_cpl_lut_valid_t, tag, ==, 0, 2000))
-			cvmx_dprintf("PCIe: Port %d shutdown timeout\n", pcie_port);
+		if (CVMX_WAIT_FOR_FIELD64(CVMX_PESCX_CPL_LUT_VALID(pcie_port),
+			cvmx_pescx_cpl_lut_valid_t, tag, ==, 0, 2000))
+			cvmx_dprintf("PCIe: Port %d shutdown timeout\n",
+				     pcie_port);
 	} else {
-		if (CVMX_WAIT_FOR_FIELD64(CVMX_PEMX_CPL_LUT_VALID(pcie_port), cvmx_pemx_cpl_lut_valid_t, tag, ==, 0, 2000))
-			cvmx_dprintf("PCIe: Port %d shutdown timeout\n", pcie_port);
+		if (CVMX_WAIT_FOR_FIELD64(CVMX_PEMX_CPL_LUT_VALID(pcie_port),
+					  cvmx_pemx_cpl_lut_valid_t, tag, ==,
+					  0, 2000))
+			cvmx_dprintf("PCIe: Port %d shutdown timeout\n",
+				     pcie_port);
 	}
 
 	/* Force reset */
@@ -1335,12 +1465,14 @@ int cvmx_pcie_rc_shutdown(int pcie_port)
  *
  * @return 64bit Octeon IO address
  */
-static inline uint64_t __cvmx_pcie_build_config_addr(int pcie_port, int bus, int dev, int fn, int reg)
+static inline uint64_t __cvmx_pcie_build_config_addr(int pcie_port, int bus,
+						     int dev, int fn, int reg)
 {
 	cvmx_pcie_address_t pcie_addr;
 	cvmx_pciercx_cfg006_t pciercx_cfg006;
 
-	pciercx_cfg006.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG006(pcie_port));
+	pciercx_cfg006.u32 = cvmx_pcie_cfgx_read(pcie_port,
+						 CVMX_PCIERCX_CFG006(pcie_port));
 	if ((bus <= pciercx_cfg006.s.pbnum) && (dev != 0))
 		return 0;
 
@@ -1372,7 +1504,8 @@ static inline uint64_t __cvmx_pcie_build_config_addr(int pcie_port, int bus, int
  */
 uint8_t cvmx_pcie_config_read8(int pcie_port, int bus, int dev, int fn, int reg)
 {
-	uint64_t address = __cvmx_pcie_build_config_addr(pcie_port, bus, dev, fn, reg);
+	uint64_t address = __cvmx_pcie_build_config_addr(pcie_port, bus, dev,
+							 fn, reg);
 	if (address)
 		return cvmx_read64_uint8(address);
 	else
@@ -1390,9 +1523,11 @@ uint8_t cvmx_pcie_config_read8(int pcie_port, int bus, int dev, int fn, int reg)
  *
  * @return Result of the read
  */
-uint16_t cvmx_pcie_config_read16(int pcie_port, int bus, int dev, int fn, int reg)
+uint16_t cvmx_pcie_config_read16(int pcie_port, int bus, int dev,
+				 int fn, int reg)
 {
-	uint64_t address = __cvmx_pcie_build_config_addr(pcie_port, bus, dev, fn, reg);
+	uint64_t address = __cvmx_pcie_build_config_addr(pcie_port, bus, dev,
+							 fn, reg);
 	if (address)
 		return cvmx_le16_to_cpu(cvmx_read64_uint16(address));
 	else
@@ -1410,9 +1545,11 @@ uint16_t cvmx_pcie_config_read16(int pcie_port, int bus, int dev, int fn, int re
  *
  * @return Result of the read
  */
-uint32_t cvmx_pcie_config_read32(int pcie_port, int bus, int dev, int fn, int reg)
+uint32_t cvmx_pcie_config_read32(int pcie_port, int bus, int dev,
+				 int fn, int reg)
 {
-	uint64_t address = __cvmx_pcie_build_config_addr(pcie_port, bus, dev, fn, reg);
+	uint64_t address = __cvmx_pcie_build_config_addr(pcie_port, bus, dev,
+							 fn, reg);
 	if (address)
 		return cvmx_le32_to_cpu(cvmx_read64_uint32(address));
 	else
@@ -1429,9 +1566,11 @@ uint32_t cvmx_pcie_config_read32(int pcie_port, int bus, int dev, int fn, int re
  * @param reg       Register to access
  * @param val       Value to write
  */
-void cvmx_pcie_config_write8(int pcie_port, int bus, int dev, int fn, int reg, uint8_t val)
+void cvmx_pcie_config_write8(int pcie_port, int bus, int dev, int fn,
+			     int reg, uint8_t val)
 {
-	uint64_t address = __cvmx_pcie_build_config_addr(pcie_port, bus, dev, fn, reg);
+	uint64_t address = __cvmx_pcie_build_config_addr(pcie_port, bus, dev,
+							 fn, reg);
 	if (address)
 		cvmx_write64_uint8(address, val);
 }
@@ -1446,9 +1585,11 @@ void cvmx_pcie_config_write8(int pcie_port, int bus, int dev, int fn, int reg, u
  * @param reg       Register to access
  * @param val       Value to write
  */
-void cvmx_pcie_config_write16(int pcie_port, int bus, int dev, int fn, int reg, uint16_t val)
+void cvmx_pcie_config_write16(int pcie_port, int bus, int dev, int fn,
+			      int reg, uint16_t val)
 {
-	uint64_t address = __cvmx_pcie_build_config_addr(pcie_port, bus, dev, fn, reg);
+	uint64_t address = __cvmx_pcie_build_config_addr(pcie_port, bus, dev,
+							 fn, reg);
 	if (address)
 		cvmx_write64_uint16(address, cvmx_cpu_to_le16(val));
 }
@@ -1463,9 +1604,11 @@ void cvmx_pcie_config_write16(int pcie_port, int bus, int dev, int fn, int reg,
  * @param reg       Register to access
  * @param val       Value to write
  */
-void cvmx_pcie_config_write32(int pcie_port, int bus, int dev, int fn, int reg, uint32_t val)
+void cvmx_pcie_config_write32(int pcie_port, int bus, int dev, int fn,
+			      int reg, uint32_t val)
 {
-	uint64_t address = __cvmx_pcie_build_config_addr(pcie_port, bus, dev, fn, reg);
+	uint64_t address = __cvmx_pcie_build_config_addr(pcie_port, bus, dev,
+							 fn, reg);
 	if (address)
 		cvmx_write64_uint32(address, cvmx_cpu_to_le32(val));
 }
@@ -1537,7 +1680,9 @@ int cvmx_pcie_ep_initialize(int pcie_port)
 	if (!cvmx_pcie_is_host_mode(pcie_port))
 		return -1;
 
-	/* CN63XX Pass 1.0 errata G-14395 requires the QLM De-emphasis be programmed */
+	/* CN63XX Pass 1.0 errata G-14395 requires the QLM De-emphasis be
+	 * programmed
+	 */
 	if (OCTEON_IS_MODEL(OCTEON_CN63XX_PASS1_0)) {
 		if (pcie_port) {
 			cvmx_ciu_qlm1_t ciu_qlm;
@@ -1565,7 +1710,8 @@ int cvmx_pcie_ep_initialize(int pcie_port)
 	/* Error Message Enables (PCIE*_CFG030[CE_EN,NFE_EN,FE_EN,UR_EN]) */
 	{
 		cvmx_pcieepx_cfg030_t pcieepx_cfg030;
-		pcieepx_cfg030.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIEEPX_CFG030(pcie_port));
+		pcieepx_cfg030.u32 = cvmx_pcie_cfgx_read(pcie_port,
+							 CVMX_PCIEEPX_CFG030(pcie_port));
 		if (OCTEON_IS_MODEL(OCTEON_CN5XXX)) {
 			pcieepx_cfg030.s.mps = MPS_CN5XXX;
 			pcieepx_cfg030.s.mrrs = MRRS_CN5XXX;
@@ -1579,20 +1725,29 @@ int cvmx_pcie_ep_initialize(int pcie_port)
 		pcieepx_cfg030.s.nfe_en = 1;	/* Non-fatal error reporting enable. */
 		pcieepx_cfg030.s.fe_en = 1;	/* Fatal error reporting enable. */
 		pcieepx_cfg030.s.ur_en = 1;	/* Unsupported request reporting enable. */
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIEEPX_CFG030(pcie_port), pcieepx_cfg030.u32);
+		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIEEPX_CFG030(pcie_port),
+				     pcieepx_cfg030.u32);
 	}
 
 	if (octeon_has_feature(OCTEON_FEATURE_NPEI)) {
-		/* Max Payload Size (NPEI_CTL_STATUS2[MPS]) must match PCIE*_CFG030[MPS] */
-		/* Max Read Request Size (NPEI_CTL_STATUS2[MRRS]) must not exceed PCIE*_CFG030[MRRS] */
+		/* Max Payload Size (NPEI_CTL_STATUS2[MPS]) must match
+		 * PCIE*_CFG030[MPS]
+		 */
+		/* Max Read Request Size (NPEI_CTL_STATUS2[MRRS]) must not
+		 * exceed PCIE*_CFG030[MRRS]
+		 */
 		cvmx_npei_ctl_status2_t npei_ctl_status2;
 		npei_ctl_status2.u64 = cvmx_read_csr(CVMX_PEXP_NPEI_CTL_STATUS2);
 		npei_ctl_status2.s.mps = MPS_CN5XXX;	/* Max payload size = 128 bytes (Limit of most PCs) */
 		npei_ctl_status2.s.mrrs = MRRS_CN5XXX;	/* Max read request size = 128 bytes for best Octeon DMA performance */
 		cvmx_write_csr(CVMX_PEXP_NPEI_CTL_STATUS2, npei_ctl_status2.u64);
 	} else {
-		/* Max Payload Size (DPI_SLI_PRTX_CFG[MPS]) must match PCIE*_CFG030[MPS] */
-		/* Max Read Request Size (DPI_SLI_PRTX_CFG[MRRS]) must not exceed PCIE*_CFG030[MRRS] */
+		/* Max Payload Size (DPI_SLI_PRTX_CFG[MPS]) must match
+		 * PCIE*_CFG030[MPS]
+		 */
+		/* Max Read Request Size (DPI_SLI_PRTX_CFG[MRRS]) must not
+		 * exceed PCIE*_CFG030[MRRS]
+		 */
 		cvmx_dpi_sli_prtx_cfg_t prt_cfg;
 		cvmx_sli_s2m_portx_ctl_t sli_s2m_portx_ctl;
 		prt_cfg.u64 = cvmx_read_csr(CVMX_DPI_SLI_PRTX_CFG(pcie_port));
@@ -1657,8 +1812,9 @@ void cvmx_pcie_wait_for_pending(int pcie_port)
 		int b;
 		int c;
 
-		/* See section 9.8, PCIe Core-initiated Requests, in the manual for a
-		   description of how this code works */
+		/* See section 9.8, PCIe Core-initiated Requests, in the manual
+		 * for a description of how this code works
+		 */
 		npei_data_out_cnt.u64 = cvmx_read_csr(CVMX_PEXP_NPEI_DATA_OUT_CNT);
 		if (pcie_port) {
 			if (!npei_data_out_cnt.s.p1_fcnt)
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pki.c b/arch/mips/cavium-octeon/executive/cvmx-pki.c
index b636b31..6fcb3e1 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pki.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pki.c
@@ -47,7 +47,7 @@
 #include <asm/octeon/cvmx.h>
 #include <asm/octeon/cvmx-pki-defs.h>
 #include <asm/octeon/cvmx-pki.h>
-#include <asm/octeon/cvmx-fpa.h>
+#include <asm/octeon/cvmx-fpa3.h>
 #include <asm/octeon/cvmx-pki-cluster.h>
 #include <asm/octeon/cvmx-pki-resources.h>
 #else
@@ -56,7 +56,7 @@
 #include "cvmx-error.h"
 #include "cvmx-pki-defs.h"
 #include "cvmx-pki.h"
-#include "cvmx-fpa.h"
+#include "cvmx-fpa3.h"
 #include "cvmx-pki-resources.h"
 #include "cvmx-pki-cluster.h"
 #endif
@@ -482,6 +482,17 @@ int cvmx_pki_get_pkind_config(int node, int pkind, struct cvmx_pki_pkind_config
 	return 0;
 }
 
+int cvmx_pki_get_pkind_style(int node, int pkind)
+{
+	int cluster = 0;
+
+	cvmx_pki_clx_pkindx_style_t pkind_style;
+
+	pkind_style.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_STYLE(pkind, cluster));
+
+	return pkind_style.s.style;
+}
+
 void cvmx_pki_config_port(int node, int ipd_port, struct cvmx_pki_port_config *port_cfg)
 {
 	int interface, index, pknd;
@@ -749,6 +760,39 @@ void cvmx_pki_dis_frame_len_chk(int node, int pknd)
 	cvmx_write_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(style, cluster), style_cfg.u64);
 }
 
+/**
+ * Modifies maximum frame length to check.
+ * It modifies the global frame length set used by this port, any other
+ * port using the same set will get affected too.
+ * @param node		node number
+ * @param ipd_port	ipd port for which to modify max len.
+ * @param max_size	maximum frame length
+ */
+void cvmx_pki_set_max_frm_len(int node, int ipd_port, uint32_t max_size)
+{
+	/* On CN78XX frame check is enabled for a style n and
+	PKI_CLX_STYLE_CFG[minmax_sel] selects which set of
+	MAXLEN/MINLEN to use. */
+	int interface, index, pknd;
+	cvmx_pki_clx_stylex_cfg_t style_cfg;
+	cvmx_pki_frm_len_chkx_t frame_len;
+	int cluster = 0;
+	int style;
+	int sel;
+
+	/* get the pkind used by this ipd port */
+	interface = cvmx_helper_get_interface_num(ipd_port);
+	index = cvmx_helper_get_interface_index_num(ipd_port);
+	pknd = cvmx_helper_get_pknd(interface, index);
+
+	style = cvmx_pki_get_pkind_style(node, pknd);
+	style_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(style, cluster));
+	sel = style_cfg.s.minmax_sel;
+	frame_len.u64 = cvmx_read_csr(CVMX_PKI_FRM_LEN_CHKX(sel));
+	frame_len.s.maxlen = max_size;
+	cvmx_write_csr_node(node, CVMX_PKI_FRM_LEN_CHKX(sel), frame_len.u64);
+}
+
 
 /**
  * This function shows the pcam table in raw format,
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pko.c b/arch/mips/cavium-octeon/executive/cvmx-pko.c
index 90bcebe..5c74ca1 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pko.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pko.c
@@ -45,13 +45,14 @@
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <linux/export.h>
 #include <asm/octeon/cvmx.h>
-#include <asm/octeon/cvmx-pko.h>
+#include <asm/octeon/cvmx-hwpko.h>
 #include <asm/octeon/cvmx-pko3.h>
 #include <asm/octeon/cvmx-pko3-queue.h>
 #include <asm/octeon/cvmx-helper.h>
 #include <asm/octeon/cvmx-helper-cfg.h>
 #include <asm/octeon/cvmx-helper-util.h>
-#include <asm/octeon/cvmx-fpa.h>
+#include <asm/octeon/cvmx-fpa1.h>
+#include <asm/octeon/cvmx-fpa3.h>
 #include <asm/octeon/cvmx-clock.h>
 #else
 #include "cvmx.h"
@@ -60,7 +61,8 @@
 #include "cvmx-helper.h"
 #include "cvmx-helper-cfg.h"
 #include "cvmx-helper-util.h"
-#include "cvmx-fpa.h"
+#include "cvmx-fpa1.h"
+#include "cvmx-fpa3.h"
 #ifndef __U_BOOT__
 #include "cvmx-error.h"
 #endif
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pko3.c b/arch/mips/cavium-octeon/executive/cvmx-pko3.c
index eee8e82..6849f9a 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pko3.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pko3.c
@@ -40,16 +40,16 @@
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <linux/module.h>
 #include <asm/octeon/cvmx.h>
-#include <asm/octeon/cvmx-fpa.h>
+#include <asm/octeon/cvmx-fpa3.h>
 #include <asm/octeon/cvmx-clock.h>
-#include <asm/octeon/cvmx-pko.h>
+#include <asm/octeon/cvmx-hwpko.h>
 #include <asm/octeon/cvmx-pko3.h>
 #include <asm/octeon/cvmx-helper-pko3.h>
 #else
 #include "cvmx.h"
-#include "cvmx-pko.h"	/* For legacy support */
+#include "cvmx-hwpko.h"	/* For legacy support */
 #include "cvmx-pko3.h"
-#include "cvmx-fpa.h"
+#include "cvmx-fpa3.h"
 #include "cvmx-helper-pko3.h"
 #endif
 
@@ -113,6 +113,7 @@ int cvmx_pko3_hw_init_global(int node, uint16_t aura)
 	cvmx_pko_dpfi_fpa_aura_t pko_aura;
 	cvmx_pko_dpfi_ena_t dpfi_enable;
 	cvmx_pko_ptf_iobp_cfg_t ptf_iobp_cfg;
+	cvmx_pko_pdm_cfg_t pko_pdm_cfg;
 	cvmx_pko_enable_t pko_enable;
 
 	/* Clear FLUSH command to be sure */
@@ -123,7 +124,7 @@ int cvmx_pko3_hw_init_global(int node, uint16_t aura)
 	/* set the aura number in pko, use aura node from parameter */
 	pko_aura.u64 = 0;
 	pko_aura.s.node = aura >> 10;
-	pko_aura.s.laura = aura & (CVMX_FPA_AURA_NUM-1);
+	pko_aura.s.laura = aura & (CVMX_FPA3_AURA_NUM-1);
 	cvmx_write_csr_node(node, CVMX_PKO_DPFI_FPA_AURA, pko_aura.u64);
 
 	dpfi_enable.u64 = 0;
@@ -132,9 +133,14 @@ int cvmx_pko3_hw_init_global(int node, uint16_t aura)
 
 	/* set max outstanding requests in IOBP for any FIFO */
 	ptf_iobp_cfg.u64 = 0;
-	ptf_iobp_cfg.s.max_read_size = 72;	//HRM: typical=0x48
+	ptf_iobp_cfg.s.max_read_size = 72;	/* HRM: typical=0x48 */
 	cvmx_write_csr_node(node, CVMX_PKO_PTF_IOBP_CFG, ptf_iobp_cfg.u64);
 
+	/* Set minimum packet size per Ethernet standard */
+	pko_pdm_cfg.u64 = 0;
+	pko_pdm_cfg.s.pko_pad_minlen = 0x3c;	/* 60 bytes before FCS */
+	cvmx_write_csr_node(node, CVMX_PKO_PDM_CFG, pko_pdm_cfg.u64);
+
 	/* Initialize MACs and FIFOs */
 	cvmx_pko_setup_macs(node);
 
@@ -199,7 +205,7 @@ int cvmx_pko3_hw_disable(int node)
 
 	return 0;
 }
-
+#if 0 //deprecated
 /*
  * Transmit packets through pko on specified node and queue.
  *
@@ -244,6 +250,7 @@ int cvmx_pko_transmit_packet(int dq, cvmx_buf_ptr_pki_t bufptr,
 	}
 	return 0;
 }
+#endif
 
  /** Open configured descriptor queues before queueing packets into them.
  *
@@ -715,6 +722,10 @@ int cvmx_pko3_interface_options(int node, int interface, int port,
 	pko_mac_cfg.s.fcs_ena = fcs_enable;
 	pko_mac_cfg.s.fcs_sop_off = fcs_sop_off;
 
+	if (debug)
+		cvmx_dprintf("%s: PKO_MAC[%u]CFG=%#x\n",__func__,
+		mac_num, (unsigned) pko_mac_cfg.u64);
+
 	cvmx_write_csr_node(node, CVMX_PKO_MACX_CFG(mac_num), pko_mac_cfg.u64);
 
 	return 0;
@@ -736,82 +747,678 @@ void cvmx_pko3_dq_options(unsigned node, unsigned dq, bool min_pad)
 	cvmx_write_csr_node(node, CVMX_PKO_PDM_DQX_MINPAD(dq), reg.u64);
 }
 
-//
-// Add new native API:
-//
-// define packet descriptor, one cache line, 16 words, simple, opaque
-typedef struct cvmx_pko3_pdesc_s {
-	// PKO3 command buffer:
-	uint64_t word[16];
-	// Bookkeeping fields:
-	unsigned num_words	:4,
-		last_aura	:12,
-		flags		:3;
-} cvmx_pko3_pdesc_t;
-
-// function to create a pkt_desc from WQE
+/******************************************************************************
+*
+* New PKO3 API - Experimental
+*
+******************************************************************************/
+
+/**
+ * Initialize packet descriptor
+ *
+ * Desciptor storage is provided by the caller,
+ * use this function to initialize the descriptor to a known
+ * empty state.
+ *
+ * @param pdesc Packet Desciptor.
+ *
+ * Do not use this function when creating a descriptor from a
+ * Work Queue Entry.
+ */
+void cvmx_pko3_pdesc_init(cvmx_pko3_pdesc_t *pdesc)
+{
+	cvmx_pko_send_aura_t *ext_s;
+
+	memset(pdesc, 0, sizeof(*pdesc));
+
+	/* Start with HDR_S and HDR_EXT_S in first two words, all 0's */
+	pdesc->num_words = 2;
+
+	ext_s = (void *) &pdesc->word[1];
+	ext_s->s.subdc4 = CVMX_PKO_SENDSUBDC_EXT;
+
+	pdesc->last_aura = -1;
+	pdesc->jb_aura = -1;
+}
+
+/**
+ * Create a packet descriptor from WQE
+ *
+ * Populate a packet descriptor with a packet data and meta-data
+ * located in the Work Queue Entry.
+ * After this function, it is safe to call 'cvmx-wqe-free()'
+ * to release the WQE buffer if separate from data buffers.
+ * This function discards any data or meta-data that may have
+ * been present in the packet descriptor previously, and does not
+ * require the call to 'cvmx_pko3_pdesc_init()'.
+ *
+ * @param pdesc Packet Desciptor.
+ * @param wqe Work Queue Entry as returned from `cvmx_get_work()'
+ * @param free_bufs Automatically free data buffers when transmission complete.
+ *
+ * This function is the quickes way to prepare a received packet
+ * represented by a WQE for transmission via any output queue to
+ * an output port.
+ * If the packet data is to be transmitted unmodified, call
+ * 'cvmx_pko_pdesc_transmit()' immediately after this function
+ * returns.
+ */
 int cvmx_pko3_pdesc_from_wqe(cvmx_pko3_pdesc_t *pdesc, cvmx_wqe_78xx_t *wqe,
 	bool free_bufs)
 {
-return -1;
+	unsigned node;
+	cvmx_pko_send_hdr_t *hdr_s;
+	cvmx_pko_send_aura_t *ext_s;
+	cvmx_pko_buf_ptr_t *buf_s;
+	cvmx_buf_ptr_pki_t pki_bptr;
+        cvmx_pki_stylex_buf_t     style_buf_reg;
+
+
+	/* Verufy the WQE is legit */
+	if (cvmx_unlikely(wqe->word2.pki.software || wqe->pki_wqe_translated)) {
+		cvmx_dprintf("%s: ERROR: invalid WQE\n", __func__);
+		return -1;
+	}
+
+	/* descriptor provided by caller, reset state */
+	memset(pdesc, 0, sizeof(*pdesc));
+	pdesc->jb_aura = -1;
+
+	/* 1st word is SEND_HDR_S header */
+	hdr_s = (void *) &pdesc->word[0];
+	/* 2nd word is the SEND_EXT_S header */
+	ext_s = (void *) &pdesc->word[1];
+	ext_s->s.subdc4 = CVMX_PKO_SENDSUBDC_EXT;
+	pdesc->num_words = 2;
+
+	hdr_s->s.format = 0;	/* Only 0 works for Pass1 */
+	hdr_s->s.ds = 0;	/* don't send, never used */
+
+	/* TODO: n2 is not currently supported in simulator */
+	hdr_s->s.n2 = 0;	/* No L2 allocate */
+
+	/* Default buffer freeing setting, may be overriden by "i" */
+	hdr_s->s.df = !free_bufs;
+
+	/* Inherit GAURA */
+	pdesc->last_aura =
+	hdr_s->s.aura = wqe->word0.pki.aura;
+
+	/* Get the NODE on which this packet was received */
+	node = pdesc->last_aura >> 10;
+
+	/* Import total packet length */
+	hdr_s->s.total = wqe->word1.cn78xx.len ;
+
+	/* Read the PKI_STYLEX_BUF register for this packet style */
+        style_buf_reg.u64 = cvmx_read_csr_node(node,
+		CVMX_PKI_STYLEX_BUF(wqe->word0.pki.style));
+
+	/* mirror PKI endianness state: */
+	hdr_s->s.le = style_buf_reg.s.pkt_lend;
+#if CVMX_ENABLE_PARAMETER_CHECKING
+#ifdef __BIG_ENDIAN_BITFIELD
+	if (hdr_s->s.le)
+#else
+	if (!hdr_s->s.le)
+#endif
+		cvmx_dprintf("%s: WARNING: packet endianness mismatch\n", __func__);
+#endif
+#if 0
+	// WQE fields not used (yet?)
+		wqe->word0.pki.pknd
+		wqe->word0.pki.channel
+
+		wqe->word1.cn78xx.tag
+		wqe->word1.cn78xx.tag_type
+		wqe->word1.cn78xx.grp
+#endif
+
+	/* Carry-over layer protocol detection from PKI */
+	pdesc->pki_word2 = wqe->word2.pki;
+
+	/* check if WQE WORD4 is present */
+	if (style_buf_reg.s.wqe_hsz != 0 || style_buf_reg.s.first_skip > 4) {
+		pdesc->pki_word4_present = 1;
+		/* Carry-over protocol header offsets */
+		pdesc->pki_word4 = wqe->word4;
+	}
+
+
+	/* Checksum recalculation is not needed, until headers get modified */
+	/* NOTE: Simulator does not support CKL3/CKL4, so this is not tested */
+	hdr_s->s.ckl4 = CKL4ALG_NONE;
+	hdr_s->s.ckl3 = 0;
+
+	/* Convert WQE buffer ptr to LINK_S or GATHER_S bufptr in descriptor */
+	pki_bptr = wqe->packet_ptr;
+	buf_s = (void *) &pdesc->word[pdesc->num_words++];
+	buf_s->u64 = 0;
+	buf_s->s.addr = pki_bptr.s_cn78xx.addr;
+	buf_s->s.size = pki_bptr.s_cn78xx.size;
+
+	/* use LINK_S if more than one buf present, calculate headroom */
+	if (cvmx_unlikely(wqe->word0.pki.bufs > 1)) {
+		pdesc->headroom =  (style_buf_reg.s.first_skip) << 3;
+		buf_s->s.subdc3 = CVMX_PKO_SENDSUBDC_LINK;
+	} else {
+		pdesc->headroom =  (1 + style_buf_reg.s.first_skip) << 3;
+		buf_s->s.subdc3 = CVMX_PKO_SENDSUBDC_GATHER;
+	}
+	pdesc->headroom += wqe->word0.pki.apad;
+
+	return 0;
 }
 
-// function to prepend data to packet (i.e. push header
-// data bytes will be copied into pdesc
-int cvmx_pko3_pdesc_prepend(cvmx_pko3_pdesc_t *pdesc, const void *p_data, unsigned data_bytes)
+/**
+ * @INTERNAL
+ *
+ * Add arbitrary subcommand to a packet desciptor.
+ *
+ * This function will also allocate a jump buffer when
+ * the primary LTDMA buffer is exhausted.
+ * The jump buffer is allocated from the internal PKO3 aura
+ * on the node where this function is running.
+ */
+static int cvmx_pko3_pdesc_subdc_add(cvmx_pko3_pdesc_t *pdesc,
+		uint64_t subdc)
 {
+	cvmx_pko_send_hdr_t *hdr_s;
+	cvmx_pko_send_aura_t *ext_s;
+	cvmx_pko_buf_ptr_t *jump_s;
+	const unsigned jump_buf_size = 4*1024 / sizeof(uint64_t);
+	unsigned i;
+
+	/* Simple handling while fitting the command buffer */
+	if (cvmx_likely(pdesc->num_words <= 15)) {
+		pdesc->word[ pdesc->num_words ] = subdc;
+		pdesc->num_words ++;
+		return pdesc->num_words;
+	}
+
+        /* SEND_JUMP_S broken on Pass1 */
+        if(OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_0)) {
+                cvmx_dprintf("%s: ERROR: too many segments\n",__func__);
+                return -__LINE__;
+        }
+
+	hdr_s = (void *) &pdesc->word[0];
+	ext_s = (void *) &pdesc->word[1];
+
+	/* Allocate jump buffer */
+	if (cvmx_unlikely(pdesc->jump_buf == NULL)) {
+		unsigned pko_gaura, pko_anode, pko_laura;
+		unsigned fpa_node = cvmx_get_node_num();
+
+		/* Allocate jump buffer from PKO internal FPA AURA, size=4KiB */
+		pko_gaura = __cvmx_pko3_aura_get(fpa_node);
+		pko_anode = pko_gaura >> 10;
+		pko_laura = pko_gaura & (CVMX_FPA3_AURA_NUM-1);
+
+		pdesc->jump_buf = cvmx_fpa_alloc_aura(pko_anode, pko_laura);
+                if(pdesc->jump_buf == NULL)
+                        return -__LINE__;
+
+		/* Save the JB aura for later */
+		pdesc->jb_aura = pko_gaura;
+
+		/* Move most of the command to the jump buffer */
+		memcpy(pdesc->jump_buf, &pdesc->word[2],
+			(pdesc->num_words-2)*sizeof(uint64_t));
+		jump_s = (void *) &pdesc->word[2];
+		jump_s->u64 = 0;
+		jump_s->s.addr = cvmx_ptr_to_phys(pdesc->jump_buf);
+		jump_s->s.i = hdr_s->s.df;	/* force PKO3 to free JB */
+		jump_s->s.size = pdesc->num_words - 2;
+		jump_s->s.subdc3 = CVMX_PKO_SENDSUBDC_JUMP;
+
+		/* Now the LMTDMA buffer has only HDR_S, EXT_S, JUMP_S */
+		pdesc->num_words = 3;
+	}
+
+	/* Add the new subcommand to the jump buffer */
+	jump_s = (void *) &pdesc->word[2];
+	i = jump_s->s.size;
+
+	/* Avoid overrunning jump buffer */
+	if (i >= (jump_buf_size-2)) {
+                cvmx_dprintf("%s: ERROR: too many segments\n",__func__);
+		return -__LINE__;
+	}
 
+	pdesc->jump_buf[i] = subdc;
+	jump_s->s.size++;
 
-return -1;
+	(void) ext_s;
+
+	return(i + pdesc->num_words);
 }
 
-// function to append a gather buffer to end of packet
-// the buffer can be optionally freed by PKO
-int cvmx_pko3_pdesc_buf_append(cvmx_pko3_pdesc_t *pdesc, const void *p_data,
-		unsigned data_bytes, unsigned buffer_aura, bool free_buf)
+/**
+ * Send a packet in a desciptor to an output port via an output queue.
+ *
+ * A call to this function must follow all other functions that
+ * create a packet descriptor from WQE, or after initializing an
+ * empty descriptor and filling it with one or more data fragments.
+ * After this function is called, the content of the packet descriptor
+ * can no longer be used, and are undefined.
+ *
+ * @param pdesc Packet Desciptor.
+ * @param dq Descriptor Queue associated with the desired output port
+ * @return Returns 0 on success, -1 on error.
+ *
+ */
+int cvmx_pko3_pdesc_transmit(cvmx_pko3_pdesc_t *pdesc, uint16_t dq)
 {
+        cvmx_pko_query_rtn_s_t pko_status;
+	cvmx_pko_send_aura_t aura_s;
+	uint8_t port_node;
+	int rc;
+
+	/* Add last AURA_S for jump_buf, if present */
+	if (cvmx_unlikely(pdesc->jump_buf != NULL)) {
+		/* The last AURA_S subdc refers to the jump_buf itself */
+		aura_s.s.aura = pdesc->jb_aura;
+		aura_s.s.offset = 0;
+		aura_s.s.alg = AURAALG_NOP;
+		aura_s.s.subdc4 = CVMX_PKO_SENDSUBDC_AURA;
+		pdesc->last_aura = pdesc->jb_aura;
+
+		rc = cvmx_pko3_pdesc_subdc_add(pdesc, aura_s.u64);
+		if (rc < 0)
+			return -1;
+	}
 
-return -1;
-}
+	/* SEND_WORK_S must be the very last subdc */
+	if (cvmx_unlikely(pdesc->send_work_s != 0ULL)) {
+		rc = cvmx_pko3_pdesc_subdc_add(pdesc, pdesc->send_work_s);
+		if (rc < 0)
+			return -1;
+	}
 
+        /* Derive destination node from dq */
+        port_node = dq >> 14;
+        dq &= (1<<10)-1;
 
-// function to remote data from head of packet (i.e. pop header)
-int cvmx_pko3_pdesc_trim(cvmx_pko3_pdesc_t *pdesc, unsigned num_bytes)
+        /* Send the PKO3 command into the Descriptor Queue */
+        pko_status = __cvmx_pko3_do_dma(port_node, dq,
+                pdesc->word, pdesc->num_words, CVMX_PKO_DQ_SEND);
+
+        /* Map PKO3 result codes to legacy return values */
+        if (pko_status.s.dqstatus == PKO_DQSTATUS_PASS)
+                return 0;
+
+        cvmx_dprintf("%s: ERROR: failed to enqueue: %s\n",
+                                __FUNCTION__,
+                                pko_dqstatus_error(pko_status.s.dqstatus));
+
+	return -1;
+}
+
+/**
+ * Append a packet segment to a packet descriptor
+ *
+ * After a packet descriptor is initialized, one or more
+ * packet data segments can be added to the packet,
+ * in the order in which they should be transmitted.
+ *
+ * The size of the resulting packet will be equal to the
+ * sum of the segments appended by thus function.
+ * Every segment may be contained in a buffer that belongs
+ * to a different FPA 'aura', and may be automatically
+ * released back to that aura, if required.
+ *
+ * @param pdesc Packet Desciptor.
+ * @param p_data Address of the segment first byte (virtual).
+ * @param data_bytes Size of the data segment (in bytes).
+ * @param gaura A global FPA 'aura' where the packet buffer was allocated from.
+ * @param free_buf Cause the PKO to release the buffer on completion.
+ *
+ * The 'gaura' parameter contaisn the node number where the buffer pool
+ * is located, and has only a meaning if the 'free_buf' argument is 'true'.
+ *
+ * @return Returns 0 on success, -1 on error.
+ */
+int cvmx_pko3_pdesc_buf_append(cvmx_pko3_pdesc_t *pdesc, void *p_data,
+		unsigned data_bytes, unsigned gaura, bool free_buf)
 {
+	cvmx_pko_send_hdr_t *hdr_s;
+	cvmx_pko_buf_ptr_t gather_s;
+	cvmx_pko_send_aura_t aura_s;
+	int rc;
+
+	if (pdesc->mem_s_ix > 0) {
+		cvmx_dprintf("%s: subcommand restriction violated\n", __func__);
+		return -1;
+	}
+	
+	hdr_s = (void *) &pdesc->word[0];
+
+	if(pdesc->last_aura == -1) {
+		unsigned buf_sz = 128;
+
+		/* First mbuf, calculate headroom */
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+                buf_sz = cvmx_fpa_get_aura_buf_size(gaura);
+#endif
+		pdesc->headroom = (unsigned long)p_data & (buf_sz-1);
+		pdesc->last_aura = hdr_s->s.aura = gaura;
+	} else if(pdesc->last_aura != (short) gaura && free_buf) {
+		aura_s.s.aura = gaura;
+		aura_s.s.offset = 0;
+		aura_s.s.alg = AURAALG_NOP;
+		aura_s.s.subdc4 = CVMX_PKO_SENDSUBDC_AURA;
+		pdesc->last_aura = gaura;
+
+		rc = cvmx_pko3_pdesc_subdc_add(pdesc, aura_s.u64);
+		if (rc < 0)
+			return -1;
+	}
 
+	gather_s.u64 = 0;
+	gather_s.s.addr = cvmx_ptr_to_phys(p_data);
+	gather_s.s.size = data_bytes;
+	hdr_s->s.total += data_bytes;
+	gather_s.s.i = free_buf ^ ~hdr_s->s.df;
+	gather_s.s.subdc3 = CVMX_PKO_SENDSUBDC_GATHER;
 
-return -1;
+	rc = cvmx_pko3_pdesc_subdc_add(pdesc, gather_s.u64);
+	if (rc < 0)
+		return -1;
+
+	return rc;
 }
 
-// function to add completion indication via SSO
+/**
+ * Add a Work Entry for packet transmission notification
+ *
+ * Add a subcommand to notify of packet transmission completion
+ * via a Work Queue entry over the SSO.
+ * The Work Queue entry may be a 'software' event, or the content
+ * of a packet.
+ *
+ * @param pdesc Packet Desciptor, memory provided by caller.
+ * @param wqe Work Queue Entry in a model-native format.
+ * @param node The OCI node of the SSO where the WQE will be delivered.
+ * @param group The SSO group where the WQE is delivered.
+ * @param tt The SSO Tag Type for the WQE. If tt is not NULL, WQE should 
+ * contain a valid tag value for the work entry.
+ *
+ * @return Returns 0 on success, -1 on error.
+ *
+ * Restrictions:
+ * There can be only one such notification per packet descriptor,
+ * but this function may be called at any time after the descriptor
+ * is first created from WQE or initialized, and before 
+ * starting transmission.
+ *
+ */
 int cvmx_pko3_pdesc_notify_wqe(cvmx_pko3_pdesc_t *pdesc, cvmx_wqe_78xx_t *wqe,
-	unsigned node, unsigned group, unsigned tag, unsigned tt)
+	uint8_t node, uint8_t group, uint8_t tt, uint32_t tag)
 {
+	cvmx_pko_send_work_t work_s;
+
+	/*
+	 * There can be only one SEND_WORK_S emtry in the command
+	 * and it must be the very last subcommand
+	 */
+	if (pdesc->send_work_s != 0) {
+		cvmx_dprintf("%s: Only one SEND_WORK_S is allowed\n", __func__);
+		return -1;
+	}
+
+	work_s.u64 = 0;
+	work_s.s.subdc4 = CVMX_PKO_SENDSUBDC_WORK;
+	work_s.s.addr = cvmx_ptr_to_phys(wqe);
+	work_s.s.grp = (group & 0xff) | (node  << 8);
+	work_s.s.tt = tt;
+
+	wqe->word1.cn78xx.rsvd_0 = 0;
+	wqe->word1.cn78xx.rsvd_1 = 0;
+	wqe->word1.cn78xx.tag = tag;
+	wqe->word1.cn78xx.tag_type = tt;
+	wqe->word1.cn78xx.grp = work_s.s.grp;
 
+	/* Store in descriptor for now, apply just before LTDMA-ing */
+	pdesc->send_work_s = work_s.u64;
 
-return -1;
+	return 0;
 }
 
-// function to add completion indication via atomic counter
-// p_count type should be __atomic__
+/**
+ * Request atomic memory decrement at transmission completion
+ *
+ * Each packet descriptor may contain several decrement notification
+ * requests, but these request must only be made after all of the
+ * packet data segments have been added, and before packet transmission
+ * commences.
+ *
+ * Only decrement of a 64-bit memory location is supported.
+ *
+ * @param pdesc Packet Descriptor.
+ * @param p_counter A pointer to an atomic 64-bit memory location.
+ *
+ * @return Returns 0 on success, -1 on failure.
+ */
 int cvmx_pko3_pdesc_notify_decrement(cvmx_pko3_pdesc_t *pdesc,
 	volatile uint64_t *p_counter)
 {
+	int rc;
+	/* 640bit decrement is the only supported operation */
+	cvmx_pko_send_mem_t mem_s = {.s={
+		.subdc4 = CVMX_PKO_SENDSUBDC_MEM,
+		.dsz = MEMDSZ_B64, .alg = MEMALG_SUB,
+		.offset = 1,
+#ifdef	_NOT_IN_SIM_
+		/* Enforce MEM nefore SSO submission if both present */
+		.wmem = 1
+#endif
+		}};
+
+	mem_s.s.addr = cvmx_ptr_to_phys(CASTPTR(void,p_counter));
+
+
+
+	rc = cvmx_pko3_pdesc_subdc_add(pdesc, mem_s.u64);
 
-return -1;
+	/*
+	 * SEND_MEM_S must be after all LINK_S/FATHER_S/IMM_S
+	 * subcommands, set the index to prevent further data
+	 * subcommands.
+	 */
+	if (rc > 0)
+		pdesc->mem_s_ix = rc;
+
+	return rc;
 }
 
-// function to initiate transmission of a pkt_desc
-int cvmx_pko3_pdesc_transmit(cvmx_pko3_pdesc_t *pdesc, uint16_t dq)
+/**
+ * Request atomic memory clear at transmission completion
+ *
+ * Each packet descriptor may contain several notification
+ * requests, but these request must only be made after all of the
+ * packet data segments have been added, and before packet transmission
+ * commences.
+ *
+ * Clearing of a single byte is requested by this function.
+ *
+ * @param pdesc Packet Descriptor.
+ * @param p_counter A pointer to a byte location.
+ *
+ * @return Returns 0 on success, -1 on failure.
+ */
+int cvmx_pko3_pdesc_notify_memclr(cvmx_pko3_pdesc_t *pdesc,
+	volatile uint8_t *p_mem)
+{
+	int rc;
+	/* 640bit decrement is the only supported operation */
+	cvmx_pko_send_mem_t mem_s = {.s={
+		.subdc4 = CVMX_PKO_SENDSUBDC_MEM,
+		.dsz = MEMDSZ_B8, .alg = MEMALG_SET,
+		.offset = 0,
+		}};
+
+	mem_s.s.addr = cvmx_ptr_to_phys(CASTPTR(void,p_mem));
+
+	rc = cvmx_pko3_pdesc_subdc_add(pdesc, mem_s.u64);
+
+	/*
+	 * SEND_MEM_S must be after all LINK_S/FATHER_S/IMM_S
+	 * subcommands, set the index to prevent further data
+	 * subcommands.
+	 */
+	if (rc > 0)
+		pdesc->mem_s_ix = rc;
+
+	return rc;
+}
+
+/**
+ * Prepend a data segment to the packet descriptor
+ *
+ * Useful for pushing additiona headers
+ *
+ * The initial implementation is confined by the size of the
+ * "headroom" in the first packet buffer attached to the descriptor.
+ * Future version may prepend additional buffers when this head room
+ * is insufficient, but currently will return -1 when headrom is
+ * insufficient.
+ *
+ * On success, the function returns the remaining headroom in the buffer.
+ *
+ * FIXME: Not tested yet.
+ */
+int cvmx_pko3_pdesc_prepend(cvmx_pko3_pdesc_t *pdesc,
+	const void *p_data, uint8_t data_bytes)
 {
+	cvmx_pko_send_hdr_t *hdr_s;
+	cvmx_pko_buf_ptr_t *gather_s;
+	short headroom;
+	void *p;	/* old data location */
+	void *q;	/* new data location */
 
-return -1;
+	if ((int)data_bytes < (headroom = pdesc->headroom))
+		return -1;
+
+	hdr_s = (void *)&pdesc->word[0];
+
+	/* Get GATTHER_S/LINK_S subcommand location */
+	if (cvmx_likely(pdesc->jump_buf == NULL))
+		/* Without JB, first data buf is in 3rd comand word */
+		gather_s = (void *)&pdesc->word[2];
+	else
+		/* With JB, its first word is the first buffer */
+		gather_s = (void *)pdesc->jump_buf;
+
+	/* adjust  address and size values */
+	p = cvmx_phys_to_ptr(gather_s->s.addr);
+	q			= p - data_bytes;
+	gather_s->s.addr	-= data_bytes;
+	gather_s->s.size	+= data_bytes;
+	hdr_s->s.total		+= data_bytes;
+	headroom		-= data_bytes;
+
+	/* Move link pointer if the descriptor is SEND_LINK_S */
+	if (gather_s->s.subdc3 == CVMX_PKO_SENDSUBDC_LINK)
+		memcpy(q-8, p-8, 8);
+	memcpy(q, p_data, data_bytes);
+
+	return pdesc->headroom = headroom;
 }
 
-// function to create an empty pkt_desc
-void cvmx_pko3_pdesc_init(cvmx_pko3_pdesc_t *pdesc)
+
+/**
+ * Remove some bytes from start of packet
+ *
+ * Useful for popping a header from a packet.
+ * It only eeds to find the first segment, and adjust its address,
+ * as well as segment and total sizes.
+ *
+ * Returns new packet size, or -1 if the trimmed size exceeds the
+ * size of the first data segment.
+ *
+ * FIXME: Not tested yet.
+ */
+int cvmx_pko3_pdesc_trim(cvmx_pko3_pdesc_t *pdesc, unsigned num_bytes)
 {
+	cvmx_pko_send_hdr_t *hdr_s;
+	cvmx_pko_buf_ptr_t *gather_s;
+	short headroom;
+	void *p;
+	void *q;
+
+	headroom = pdesc->headroom;
+
+	hdr_s = (void *)&pdesc->word[0];
+
+	/* Get GATTHER_S/LINK_S subcommand location */
+	if (cvmx_likely(pdesc->jump_buf == NULL))
+		/* Without JB, first data buf is in 3rd comand word */
+		gather_s = (void *)&pdesc->word[2];
+	else
+		/* With JB, its first word is the first buffer */
+		gather_s = (void *)pdesc->jump_buf;
+
+	/* Can't trim more than the content of the first buffer */
+	if (gather_s->s.size < num_bytes)
+		return -1;
 
+	/* adjust  address and size values */
+	p = cvmx_phys_to_ptr(gather_s->s.addr);
+	q			= p + num_bytes;
+	gather_s->s.addr	+= num_bytes;
+	gather_s->s.size	-= num_bytes;
+	hdr_s->s.total		-= num_bytes;
+	headroom		+= num_bytes;
 
+	/* Move link pointer if the descriptor is SEND_LINK_S */
+	if (gather_s->s.subdc3 == CVMX_PKO_SENDSUBDC_LINK)
+		memcpy(q-8, p-8, 8);
+
+	return hdr_s->s.total;
 }
 
+/**
+ * Decode packet header and calculate protocol header offsets
+ *
+ * The protocol information and layer offset is derived
+ * from the results if decoding done by the PKI,
+ * and the appropriate PKO fields are filled.
+ *
+ * The function assumes the headers have not been modified
+ * since converted from WQE, and does not (yet) implement
+ * software-based decoding to handle modified or originated
+ * packets correctly.
+ *
+ * FIXME:
+ * Add simple accessors to read the decoded protocol fields.
+ */
+int cvmx_pko3_pdesc_hdr_offsets(cvmx_pko3_pdesc_t *pdesc)
+{
+	cvmx_pko_send_hdr_t *hdr_s;
+
+	if (!pdesc->pki_word4_present)
+		return -1;
+
+	hdr_s = (void *) &pdesc->word[0];
+
+	/* Match IPv5/IPv6 protocols with/without options */
+	if ((pdesc->pki_word2.lc_hdr_type & 0x1c) 
+		== CVMX_PKI_LTYPE_E_IP4) {
+		hdr_s->s.l3ptr = pdesc->pki_word4.ptr_layer_c;
+
+		/* Match TCP/UDP/SCTP group */
+		if ((pdesc->pki_word2.lf_hdr_type & 0x18) == CVMX_PKI_LTYPE_E_TCP)
+			hdr_s->s.l4ptr = pdesc->pki_word4.ptr_layer_f;
+
+		if (pdesc->pki_word2.lf_hdr_type == CVMX_PKI_LTYPE_E_UDP)
+			pdesc->ckl4_alg = CKL4ALG_UDP;
+		if (pdesc->pki_word2.lf_hdr_type == CVMX_PKI_LTYPE_E_TCP)
+			pdesc->ckl4_alg = CKL4ALG_TCP;
+		if (pdesc->pki_word2.lf_hdr_type == CVMX_PKI_LTYPE_E_SCTP)
+			pdesc->ckl4_alg = CKL4ALG_SCTP;
+	}
+	/* FIXME: consider ARP as L3 too ? what about IPfrag ? */
+
+	return 0;
+}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-qlm.c b/arch/mips/cavium-octeon/executive/cvmx-qlm.c
index 9b24d05..7184162 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-qlm.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-qlm.c
@@ -42,7 +42,7 @@
  *
  * Helper utilities for qlm.
  *
- * <hr>$Revision: 94747 $<hr>
+ * <hr>$Revision: 95250 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
@@ -1207,7 +1207,11 @@ int cvmx_qlm_measure_clock(int qlm)
 	uint64_t count;
 	uint64_t start_cycle, stop_cycle;
 	int evcnt_offset = 0x10;
+#ifdef CVMX_BUILD_FOR_UBOOT
+	int ref_clock[16] = {0};
+#else
 	static int ref_clock[16] = {0};
+#endif
 
 	if (ref_clock[qlm])
 		return ref_clock[qlm];
diff --git a/arch/mips/cavium-octeon/executive/cvmx-spi.c b/arch/mips/cavium-octeon/executive/cvmx-spi.c
index c468b41..fc9714a 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-spi.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-spi.c
@@ -42,7 +42,7 @@
  *
  * Support library for the SPI
  *
- * <hr>$Revision: 78972 $<hr>
+ * <hr>$Revision: 95258 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <linux/export.h>
@@ -51,13 +51,13 @@
 #include <asm/octeon/cvmx-spxx-defs.h>
 #include <asm/octeon/cvmx-stxx-defs.h>
 #include <asm/octeon/cvmx-srxx-defs.h>
-#include <asm/octeon/cvmx-pko.h>
+#include <asm/octeon/cvmx-hwpko.h>
 #include <asm/octeon/cvmx-spi.h>
 #include <asm/octeon/cvmx-clock.h>
 #else
 #include "cvmx.h"
 #include "cvmx-sysinfo.h"
-#include "cvmx-pko.h"
+#include "cvmx-hwpko.h"
 #include "cvmx-spi.h"
 #include "cvmx-clock.h"
 #endif
diff --git a/arch/mips/cavium-octeon/octeon-rapidio.c b/arch/mips/cavium-octeon/octeon-rapidio.c
index 2976984..40721d7 100644
--- a/arch/mips/cavium-octeon/octeon-rapidio.c
+++ b/arch/mips/cavium-octeon/octeon-rapidio.c
@@ -22,7 +22,7 @@
 #include <asm/octeon/cvmx-pexp-defs.h>
 #include <asm/octeon/cvmx-sriomaintx-defs.h>
 #include <asm/octeon/cvmx-dma-engine.h>
-#include <asm/octeon/cvmx-fpa.h>
+#include <asm/octeon/cvmx-fpa1.h>
 #include <asm/octeon/cvmx-config.h>
 #include <asm/octeon/cvmx-helper.h>
 #include <asm/octeon/cvmx-qlm.h>
@@ -816,7 +816,7 @@ static int __init octeon_rio_init(void)
 	}
 	if (count) {
 		int r;
-		cvmx_fpa_enable();
+		cvmx_fpa1_enable();
 		r = cvm_oct_alloc_fpa_pool(CVMX_FPA_OUTPUT_BUFFER_POOL,
 					   CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE);
 		if (r < 0)
diff --git a/arch/mips/include/asm/octeon/cvmx-asm.h b/arch/mips/include/asm/octeon/cvmx-asm.h
index 914e924..2a79a42 100644
--- a/arch/mips/include/asm/octeon/cvmx-asm.h
+++ b/arch/mips/include/asm/octeon/cvmx-asm.h
@@ -42,7 +42,7 @@
  *
  * This is file defines ASM primitives for the executive.
 
- * <hr>$Revision: 90510 $<hr>
+ * <hr>$Revision: 95100 $<hr>
  *
  *
  */
@@ -221,44 +221,53 @@ extern "C" {
 #define CVMX_SYNCI(address, offset) asm volatile ("synci " CVMX_TMP_STR(offset) "(%[rbase])" : : [rbase] "d" (address) )
 #define CVMX_PREFETCH0(address) CVMX_PREFETCH(address, 0)
 #define CVMX_PREFETCH128(address) CVMX_PREFETCH(address, 128)
-// a normal prefetch
+
+/** a normal prefetch */
 #define CVMX_PREFETCH(address, offset) CVMX_PREFETCH_PREF0(address, offset)
-// normal prefetches that use the pref instruction
+
+/** normal prefetches that use the pref instruction */
 #define CVMX_PREFETCH_PREFX(X, address, offset) asm volatile ("pref %[type], %[off](%[rbase])" : : [rbase] "d" (address), [off] "I" (offset), [type] "n" (X))
 #define CVMX_PREFETCH_PREF0(address, offset) CVMX_PREFETCH_PREFX(0, address, offset)
 #define CVMX_PREFETCH_PREF1(address, offset) CVMX_PREFETCH_PREFX(1, address, offset)
 #define CVMX_PREFETCH_PREF6(address, offset) CVMX_PREFETCH_PREFX(6, address, offset)
 #define CVMX_PREFETCH_PREF7(address, offset) CVMX_PREFETCH_PREFX(7, address, offset)
-// prefetch into L1, do not put the block in the L2
+
+/** prefetch into L1, do not put the block in the L2 */
 #define CVMX_PREFETCH_NOTL2(address, offset) CVMX_PREFETCH_PREFX(4, address, offset)
 #define CVMX_PREFETCH_NOTL22(address, offset) CVMX_PREFETCH_PREFX(5, address, offset)
-// prefetch into L2, do not put the block in the L1
+
+/** prefetch into L2, do not put the block in the L1 */
 #define CVMX_PREFETCH_L2(address, offset) CVMX_PREFETCH_PREFX(28, address, offset)
-// CVMX_PREPARE_FOR_STORE makes each byte of the block unpredictable (actually old value or zero) until
-// that byte is stored to (by this or another processor. Note that the value of each byte is not only
-// unpredictable, but may also change again - up until the point when one of the cores stores to the
-// byte.
+
+/**
+ * CVMX_PREPARE_FOR_STORE makes each byte of the block unpredictable (actually old value or zero) until
+ * that byte is stored to (by this or another processor. Note that the value of each byte is not only
+ * unpredictable, but may also change again - up until the point when one of the cores stores to the
+ * byte.
+ */
 #define CVMX_PREPARE_FOR_STORE(address, offset) CVMX_PREFETCH_PREFX(30, address, offset)
-// This is a command headed to the L2 controller to tell it to clear its dirty bit for a
-// block. Basically, SW is telling HW that the current version of the block will not be
-// used.
+/**
+ * This is a command headed to the L2 controller to tell it to clear its dirty bit for a
+ * block. Basically, SW is telling HW that the current version of the block will not be
+ * used.
+ */
 #define CVMX_DONT_WRITE_BACK(address, offset) CVMX_PREFETCH_PREFX(29, address, offset)
 
-#define CVMX_ICACHE_INVALIDATE  { CVMX_SYNC; asm volatile ("synci 0($0)" : : ); }	// flush stores, invalidate entire icache
-#define CVMX_ICACHE_INVALIDATE2 { CVMX_SYNC; asm volatile ("cache 0, 0($0)" : : ); }	// flush stores, invalidate entire icache
-#define CVMX_DCACHE_INVALIDATE  { CVMX_SYNC; asm volatile ("cache 9, 0($0)" : : ); }	// complete prefetches, invalidate entire dcache
+#define CVMX_ICACHE_INVALIDATE  { CVMX_SYNC; asm volatile ("synci 0($0)" : : ); }	/* flush stores, invalidate entire icache */
+#define CVMX_ICACHE_INVALIDATE2 { CVMX_SYNC; asm volatile ("cache 0, 0($0)" : : ); }	/* flush stores, invalidate entire icache */
+#define CVMX_DCACHE_INVALIDATE  { CVMX_SYNC; asm volatile ("cache 9, 0($0)" : : ); }	/* complete prefetches, invalidate entire dcache */
 
 #define CVMX_CACHE(op, address, offset) asm volatile ("cache " CVMX_TMP_STR(op) ", " CVMX_TMP_STR(offset) "(%[rbase])" : : [rbase] "d" (address) )
-#define CVMX_CACHE_LCKL2(address, offset) CVMX_CACHE(31, address, offset)	// fetch and lock the state.
-#define CVMX_CACHE_WBIL2(address, offset) CVMX_CACHE(23, address, offset)	// unlock the state.
-#define CVMX_CACHE_WBIL2I(address, offset) CVMX_CACHE(3, address, offset)	// invalidate the cache block and clear the USED bits for the block
-#define CVMX_CACHE_LTGL2I(address, offset) CVMX_CACHE(7, address, offset)	// load virtual tag and data for the L2 cache block into L2C_TAD0_TAG register
-#define CVMX_CACHE_L2HWB(address, offset) CVMX_CACHE(27, address, offset)	// L2 cache hit writeback (no invalidate)
+#define CVMX_CACHE_LCKL2(address, offset) CVMX_CACHE(31, address, offset)	/* fetch and lock the state. */
+#define CVMX_CACHE_WBIL2(address, offset) CVMX_CACHE(23, address, offset)	/* unlock the state. */
+#define CVMX_CACHE_WBIL2I(address, offset) CVMX_CACHE(3, address, offset)	/* invalidate the cache block and clear the USED bits for the block */
+#define CVMX_CACHE_LTGL2I(address, offset) CVMX_CACHE(7, address, offset)	/* load virtual tag and data for the L2 cache block into L2C_TAD0_TAG register */
+#define CVMX_CACHE_L2HWB(address, offset) CVMX_CACHE(27, address, offset)	/* L2 cache hit writeback (no invalidate) */
 
-/* new instruction to make RC4 run faster */
+/** new instruction to make RC4 run faster */
 #define CVMX_BADDU(result, input1, input2) asm ("baddu %[rd],%[rs],%[rt]" : [rd] "=d" (result) : [rs] "d" (input1) , [rt] "d" (input2))
 
-// misc v2 stuff
+/** misc v2 stuff */
 #define CVMX_ROTR(result, input1, shiftconst) asm ("rotr %[rd],%[rs]," CVMX_TMP_STR(shiftconst) : [rd] "=d" (result) : [rs] "d" (input1))
 #define CVMX_ROTRV(result, input1, input2) asm ("rotrv %[rd],%[rt],%[rs]" : [rd] "=d" (result) : [rt] "d" (input1) , [rs] "d" (input2))
 #define CVMX_DROTR(result, input1, shiftconst) asm ("drotr %[rd],%[rs]," CVMX_TMP_STR(shiftconst) : [rd] "=d" (result) : [rs] "d" (input1))
@@ -269,7 +278,7 @@ extern "C" {
 #define CVMX_DSHD(result, input1) asm ("dshd %[rd],%[rt]" : [rd] "=d" (result) : [rt] "d" (input1))
 #define CVMX_WSBH(result, input1) asm ("wsbh %[rd],%[rt]" : [rd] "=d" (result) : [rt] "d" (input1))
 
-// Endian swap
+/** Endian swap */
 #define CVMX_ES64(result, input) \
         do {\
         CVMX_DSBH(result, input); \
@@ -293,9 +302,10 @@ extern "C" {
 #define CVMX_EXT(result,input,pos,len) asm ("ext %[rt],%[rs]," CVMX_TMP_STR(pos) "," CVMX_TMP_STR(len) : [rt] "=d" (result) : [rs] "d" (input))
 #define CVMX_EXTM1(result,input,pos,lenm1) CVMX_EXT(result,input,pos,(lenm1)+1)
 
-// removed
-// #define CVMX_EXTU(result,input,pos,lenm1) asm ("extu %[rt],%[rs]," CVMX_TMP_STR(pos) "," CVMX_TMP_STR(lenm1) : [rt] "=d" (result) : [rs] "d" (input))
-// #define CVMX_EXTUP(result,input,pos,len) CVMX_EXTU(result,input,pos,(len)-1)
+/* removed
+ * #define CVMX_EXTU(result,input,pos,lenm1) asm ("extu %[rt],%[rs]," CVMX_TMP_STR(pos) "," CVMX_TMP_STR(lenm1) : [rt] "=d" (result) : [rs] "d" (input))
+ * #define CVMX_EXTUP(result,input,pos,len) CVMX_EXTU(result,input,pos,(len)-1)
+ */
 
 #define CVMX_CINS(result,input,pos,lenm1) asm ("cins %[rt],%[rs]," CVMX_TMP_STR(pos) "," CVMX_TMP_STR(lenm1) : [rt] "=d" (result) : [rs] "d" (input))
 #define CVMX_CINSP(result,input,pos,len) CVMX_CINS(result,input,pos,(len)-1)
@@ -310,11 +320,12 @@ extern "C" {
 #define CVMX_INSC(result,pos,len) asm ("ins %[rt],$0," CVMX_TMP_STR(pos) "," CVMX_TMP_STR(len): [rt] "=d" (result): "[rt]" (result))
 #define CVMX_INSCM1(result,pos,lenm1) CVMX_INSC(result,pos,(lenm1)+1)
 
-// removed
-// #define CVMX_INS0(result,input,pos,lenm1) asm("ins0 %[rt],%[rs]," CVMX_TMP_STR(pos) "," CVMX_TMP_STR(lenm1): [rt] "=d" (result): [rs] "d" (input), "[rt]" (result))
-// #define CVMX_INS0P(result,input,pos,len) CVMX_INS0(result,input,pos,(len)-1)
-// #define CVMX_INS0C(result,pos,lenm1) asm ("ins0 %[rt],$0," CVMX_TMP_STR(pos) "," CVMX_TMP_STR(lenm1) : [rt] "=d" (result) : "[rt]" (result))
-// #define CVMX_INS0CP(result,pos,len) CVMX_INS0C(result,pos,(len)-1)
+/* removed
+ * #define CVMX_INS0(result,input,pos,lenm1) asm("ins0 %[rt],%[rs]," CVMX_TMP_STR(pos) "," CVMX_TMP_STR(lenm1): [rt] "=d" (result): [rs] "d" (input), "[rt]" (result))
+ * #define CVMX_INS0P(result,input,pos,len) CVMX_INS0(result,input,pos,(len)-1)
+ * #define CVMX_INS0C(result,pos,lenm1) asm ("ins0 %[rt],$0," CVMX_TMP_STR(pos) "," CVMX_TMP_STR(lenm1) : [rt] "=d" (result) : "[rt]" (result))
+ * #define CVMX_INS0CP(result,pos,len) CVMX_INS0C(result,pos,(len)-1)
+ */
 
 #define CVMX_CLZ(result, input) asm ("clz %[rd],%[rs]" : [rd] "=d" (result) : [rs] "d" (input))
 #define CVMX_DCLZ(result, input) asm ("dclz %[rd],%[rs]" : [rd] "=d" (result) : [rs] "d" (input))
@@ -357,7 +368,7 @@ extern "C" {
 #define CVMX_RDHWRNV(result, regstr) asm ("rdhwr %[rt],$" CVMX_TMP_STR(regstr) : [rt] "=d" (result))
 #endif
 
-// some new cop0-like stuff
+/** some new cop0-like stuff */
 #define CVMX_DI(result) asm volatile ("di %[rt]" : [rt] "=d" (result))
 #define CVMX_DI_NULL asm volatile ("di")
 #define CVMX_EI(result) asm volatile ("ei %[rt]" : [rt] "=d" (result))
@@ -388,19 +399,21 @@ extern "C" {
 #define CVMX_V3MULU(dest,mpcand,accum) asm volatile ("v3mulu %[rd],%[rs],%[rt]" : [rd] "=d" (dest) : [rs] "d" (mpcand), [rt] "d" (accum))
 
 /* branch stuff */
-// these are hard to make work because the compiler does not realize that the
-// instruction is a branch so may optimize away the label
-// the labels to these next two macros must not include a ":" at the end
+/* these are hard to make work because the compiler does not realize that the
+ * instruction is a branch so may optimize away the label
+ * the labels to these next two macros must not include a ":" at the end
+ */
 #define CVMX_BBIT1(var, pos, label) asm volatile ("bbit1 %[rs]," CVMX_TMP_STR(pos) "," CVMX_TMP_STR(label) : : [rs] "d" (var))
 #define CVMX_BBIT0(var, pos, label) asm volatile ("bbit0 %[rs]," CVMX_TMP_STR(pos) "," CVMX_TMP_STR(label) : : [rs] "d" (var))
-// the label to this macro must include a ":" at the end
+
+/** the label to this macro must include a ":" at the end */
 #define CVMX_ASM_LABEL(label) label \
                              asm volatile (CVMX_TMP_STR(label) : : )
 
-//
-// Low-latency memory stuff
-//
-// set can be 0-1
+/*
+ * Low-latency memory stuff
+ */
+/* set can be 0-1 */
 #define CVMX_MT_LLM_READ_ADDR(set,val)    asm volatile ("dmtc2 %[rt],0x0400+(8*(" CVMX_TMP_STR(set) "))" : : [rt] "d" (val))
 #define CVMX_MT_LLM_WRITE_ADDR_INTERNAL(set,val)   asm volatile ("dmtc2 %[rt],0x0401+(8*(" CVMX_TMP_STR(set) "))" : : [rt] "d" (val))
 #define CVMX_MT_LLM_READ64_ADDR(set,val)  asm volatile ("dmtc2 %[rt],0x0404+(8*(" CVMX_TMP_STR(set) "))" : : [rt] "d" (val))
@@ -408,13 +421,13 @@ extern "C" {
 #define CVMX_MT_LLM_DATA(set,val)         asm volatile ("dmtc2 %[rt],0x0402+(8*(" CVMX_TMP_STR(set) "))" : : [rt] "d" (val))
 #define CVMX_MF_LLM_DATA(set,val)         asm volatile ("dmfc2 %[rt],0x0402+(8*(" CVMX_TMP_STR(set) "))" : [rt] "=d" (val) : )
 
-// load linked, store conditional
+/* load linked, store conditional */
 #define CVMX_LL(dest, address, offset) asm volatile ("ll %[rt], " CVMX_TMP_STR(offset) "(%[rbase])" : [rt] "=d" (dest) : [rbase] "d" (address) )
 #define CVMX_LLD(dest, address, offset) asm volatile ("lld %[rt], " CVMX_TMP_STR(offset) "(%[rbase])" : [rt] "=d" (dest) : [rbase] "d" (address) )
 #define CVMX_SC(srcdest, address, offset) asm volatile ("sc %[rt], " CVMX_TMP_STR(offset) "(%[rbase])" : [rt] "=d" (srcdest) : [rbase] "d" (address), "[rt]" (srcdest) )
 #define CVMX_SCD(srcdest, address, offset) asm volatile ("scd %[rt], " CVMX_TMP_STR(offset) "(%[rbase])" : [rt] "=d" (srcdest) : [rbase] "d" (address), "[rt]" (srcdest) )
 
-// load/store word left/right
+/* load/store word left/right */
 #define CVMX_LWR(srcdest, address, offset) asm volatile ("lwr %[rt], " CVMX_TMP_STR(offset) "(%[rbase])" : [rt] "=d" (srcdest) : [rbase] "d" (address), "[rt]" (srcdest) )
 #define CVMX_LWL(srcdest, address, offset) asm volatile ("lwl %[rt], " CVMX_TMP_STR(offset) "(%[rbase])" : [rt] "=d" (srcdest) : [rbase] "d" (address), "[rt]" (srcdest) )
 #define CVMX_LDR(srcdest, address, offset) asm volatile ("ldr %[rt], " CVMX_TMP_STR(offset) "(%[rbase])" : [rt] "=d" (srcdest) : [rbase] "d" (address), "[rt]" (srcdest) )
@@ -425,11 +438,10 @@ extern "C" {
 #define CVMX_SDR(src, address, offset) asm volatile ("sdr %[rt], " CVMX_TMP_STR(offset) "(%[rbase])" : : [rbase] "d" (address), [rt] "d" (src) )
 #define CVMX_SDL(src, address, offset) asm volatile ("sdl %[rt], " CVMX_TMP_STR(offset) "(%[rbase])" : : [rbase] "d" (address), [rt] "d" (src) )
 
-//
-// Useful crypto ASM's
-//
+/* Useful crypto ASM's */
+
 
-// CRC
+/* CRC */
 
 #define CVMX_MT_CRC_POLYNOMIAL(val)         asm volatile ("dmtc2 %[rt],0x4200" : : [rt] "d" (val))
 #define CVMX_MT_CRC_IV(val)                 asm volatile ("dmtc2 %[rt],0x0201" : : [rt] "d" (val))
@@ -452,12 +464,12 @@ extern "C" {
 #define CVMX_MF_CRC_IV_REFLECT(val)         asm volatile ("dmfc2 %[rt],0x0203" : [rt] "=d" (val) : )
 #define CVMX_MF_CRC_LEN(val)                asm volatile ("dmfc2 %[rt],0x0202" : [rt] "=d" (val) : )
 
-// MD5 and SHA-1
+/* MD5 and SHA-1 */
 
-// pos can be 0-6
+/* pos can be 0-6 */
 #define CVMX_MT_HSH_DAT(val,pos)    asm volatile ("dmtc2 %[rt],0x0040+" CVMX_TMP_STR(pos) :                 : [rt] "d" (val))
 #define CVMX_MT_HSH_DATZ(pos)       asm volatile ("dmtc2    $0,0x0040+" CVMX_TMP_STR(pos) :                 :               )
-// pos can be 0-14
+/* pos can be 0-14 */
 #define CVMX_MT_HSH_DATW(val,pos)   asm volatile ("dmtc2 %[rt],0x0240+" CVMX_TMP_STR(pos) :                 : [rt] "d" (val))
 #define CVMX_MT_HSH_DATWZ(pos)      asm volatile ("dmtc2    $0,0x0240+" CVMX_TMP_STR(pos) :                 :               )
 #define CVMX_MT_HSH_STARTMD5(val)   asm volatile ("dmfc2 $0,0x0250 \n"     \
@@ -468,23 +480,23 @@ extern "C" {
                                                      "dmtc2 %[rt],0x404f \n"                :                 : [rt] "d" (val))
 #define CVMX_MT_HSH_STARTSHA512(val)   asm volatile ("dmfc2 $0,0x0250 \n"     \
                                                      "dmtc2 %[rt],0x424f \n"                :                 : [rt] "d" (val))
-// pos can be 0-3
+/* pos can be 0-3 */
 #define CVMX_MT_HSH_IV(val,pos)     asm volatile ("dmtc2 %[rt],0x0048+" CVMX_TMP_STR(pos) :                 : [rt] "d" (val))
-// pos can be 0-7
+/* pos can be 0-7 */
 #define CVMX_MT_HSH_IVW(val,pos)     asm volatile ("dmtc2 %[rt],0x0250+" CVMX_TMP_STR(pos) :                 : [rt] "d" (val))
 
-// pos can be 0-6
+/* pos can be 0-6 */
 #define CVMX_MF_HSH_DAT(val,pos)    asm volatile ("dmfc2 %[rt],0x0040+" CVMX_TMP_STR(pos) : [rt] "=d" (val) :               )
-// pos can be 0-14
+/* pos can be 0-14 */
 #define CVMX_MF_HSH_DATW(val,pos)   asm volatile ("dmfc2 %[rt],0x0240+" CVMX_TMP_STR(pos) : [rt] "=d" (val) :               )
-// pos can be 0-3
+/* pos can be 0-3 */
 #define CVMX_MF_HSH_IV(val,pos)     asm volatile ("dmfc2 %[rt],0x0048+" CVMX_TMP_STR(pos) : [rt] "=d" (val) :               )
-// pos can be 0-7
+/* pos can be 0-7 */
 #define CVMX_MF_HSH_IVW(val,pos)     asm volatile ("dmfc2 %[rt],0x0250+" CVMX_TMP_STR(pos) : [rt] "=d" (val) :               )
 
-// 3DES
+/* 3DES */
 
-// pos can be 0-2
+/* pos can be 0-2 */
 #define CVMX_MT_3DES_KEY(val,pos)   asm volatile ("dmtc2 %[rt],0x0080+" CVMX_TMP_STR(pos) :                 : [rt] "d" (val))
 #define CVMX_MT_3DES_IV(val)        asm volatile ("dmtc2 %[rt],0x0084"                   :                 : [rt] "d" (val))
 #define CVMX_MT_3DES_ENC_CBC(val)   asm volatile ("dmtc2 %[rt],0x4088"                   :                 : [rt] "d" (val))
@@ -493,24 +505,24 @@ extern "C" {
 #define CVMX_MT_3DES_DEC(val)       asm volatile ("dmtc2 %[rt],0x408e"                   :                 : [rt] "d" (val))
 #define CVMX_MT_3DES_RESULT(val)    asm volatile ("dmtc2 %[rt],0x0098"                   :                 : [rt] "d" (val))
 
-// pos can be 0-2
+/* pos can be 0-2 */
 #define CVMX_MF_3DES_KEY(val,pos)   asm volatile ("dmfc2 %[rt],0x0080+" CVMX_TMP_STR(pos) : [rt] "=d" (val) :               )
 #define CVMX_MF_3DES_IV(val)        asm volatile ("dmfc2 %[rt],0x0084"                   : [rt] "=d" (val) :               )
 #define CVMX_MF_3DES_RESULT(val)    asm volatile ("dmfc2 %[rt],0x0088"                   : [rt] "=d" (val) :               )
 
-// KASUMI
+/* KASUMI */
 
-// pos can be 0-1
+/* pos can be 0-1 */
 #define CVMX_MT_KAS_KEY(val,pos)    CVMX_MT_3DES_KEY(val,pos)
 #define CVMX_MT_KAS_ENC_CBC(val)    asm volatile ("dmtc2 %[rt],0x4089"                   :                 : [rt] "d" (val))
 #define CVMX_MT_KAS_ENC(val)        asm volatile ("dmtc2 %[rt],0x408b"                   :                 : [rt] "d" (val))
 #define CVMX_MT_KAS_RESULT(val)     CVMX_MT_3DES_RESULT(val)
 
-// pos can be 0-1
+/* pos can be 0-1 */
 #define CVMX_MF_KAS_KEY(val,pos)    CVMX_MF_3DES_KEY(val,pos)
 #define CVMX_MF_KAS_RESULT(val)     CVMX_MF_3DES_RESULT(val)
 
-// AES
+/* AES */
 
 #define CVMX_MT_AES_ENC_CBC0(val)   asm volatile ("dmtc2 %[rt],0x0108"                   :                 : [rt] "d" (val))
 #define CVMX_MT_AES_ENC_CBC1(val)   asm volatile ("dmtc2 %[rt],0x3109"                   :                 : [rt] "d" (val))
@@ -520,68 +532,68 @@ extern "C" {
 #define CVMX_MT_AES_DEC_CBC1(val)   asm volatile ("dmtc2 %[rt],0x310d"                   :                 : [rt] "d" (val))
 #define CVMX_MT_AES_DEC0(val)       asm volatile ("dmtc2 %[rt],0x010e"                   :                 : [rt] "d" (val))
 #define CVMX_MT_AES_DEC1(val)       asm volatile ("dmtc2 %[rt],0x310f"                   :                 : [rt] "d" (val))
-// pos can be 0-3
+/* pos can be 0-3 */
 #define CVMX_MT_AES_KEY(val,pos)    asm volatile ("dmtc2 %[rt],0x0104+" CVMX_TMP_STR(pos) :                 : [rt] "d" (val))
-// pos can be 0-1
+/* pos can be 0-1 */
 #define CVMX_MT_AES_IV(val,pos)     asm volatile ("dmtc2 %[rt],0x0102+" CVMX_TMP_STR(pos) :                 : [rt] "d" (val))
-#define CVMX_MT_AES_KEYLENGTH(val)  asm volatile ("dmtc2 %[rt],0x0110"                   :                 : [rt] "d" (val))	// write the keylen
-// pos can be 0-1
+#define CVMX_MT_AES_KEYLENGTH(val)  asm volatile ("dmtc2 %[rt],0x0110"                   :                 : [rt] "d" (val))	/* write the keylen */
+/* pos can be 0-1 */
 #define CVMX_MT_AES_RESULT(val,pos) asm volatile ("dmtc2 %[rt],0x0100+" CVMX_TMP_STR(pos) :                 : [rt] "d" (val))
 
-// pos can be 0-1
+/* pos can be 0-1 */
 #define CVMX_MF_AES_RESULT(val,pos) asm volatile ("dmfc2 %[rt],0x0100+" CVMX_TMP_STR(pos) : [rt] "=d" (val) :               )
-// pos can be 0-1
+/* pos can be 0-1 */
 #define CVMX_MF_AES_IV(val,pos)     asm volatile ("dmfc2 %[rt],0x0102+" CVMX_TMP_STR(pos) : [rt] "=d" (val) :               )
-// pos can be 0-3
+/* pos can be 0-3 */
 #define CVMX_MF_AES_KEY(val,pos)    asm volatile ("dmfc2 %[rt],0x0104+" CVMX_TMP_STR(pos) : [rt] "=d" (val) :               )
-#define CVMX_MF_AES_KEYLENGTH(val)  asm volatile ("dmfc2 %[rt],0x0110"                   : [rt] "=d" (val) :               )	// read the keylen
-#define CVMX_MF_AES_DAT0(val)       asm volatile ("dmfc2 %[rt],0x0111"                   : [rt] "=d" (val) :               )	// first piece of input data
+#define CVMX_MF_AES_KEYLENGTH(val)  asm volatile ("dmfc2 %[rt],0x0110"                   : [rt] "=d" (val) :               )	/* read the keylen */
+#define CVMX_MF_AES_DAT0(val)       asm volatile ("dmfc2 %[rt],0x0111"                   : [rt] "=d" (val) :               )	/* first piece of input data */
 
-// GFM
+/* GFM */
 
-// pos can be 0-1
+/* pos can be 0-1 */
 #define CVMX_MF_GFM_MUL(val,pos)             asm volatile ("dmfc2 %[rt],0x0258+" CVMX_TMP_STR(pos) : [rt] "=d" (val) :               )
 #define CVMX_MF_GFM_POLY(val)                asm volatile ("dmfc2 %[rt],0x025e"                    : [rt] "=d" (val) :               )
-// pos can be 0-1
+/* pos can be 0-1 */
 #define CVMX_MF_GFM_RESINP(val,pos)          asm volatile ("dmfc2 %[rt],0x025a+" CVMX_TMP_STR(pos) : [rt] "=d" (val) :               )
-// pos can be 0-1
+/* pos can be 0-1 */
 #define CVMX_MF_GFM_RESINP_REFLECT(val,pos)  asm volatile ("dmfc2 %[rt],0x005a+" CVMX_TMP_STR(pos) : [rt] "=d" (val) :               )
 
-// pos can be 0-1
+/* pos can be 0-1 */
 #define CVMX_MT_GFM_MUL(val,pos)             asm volatile ("dmtc2 %[rt],0x0258+" CVMX_TMP_STR(pos) :                 : [rt] "d" (val))
 #define CVMX_MT_GFM_POLY(val)                asm volatile ("dmtc2 %[rt],0x025e"                    :                 : [rt] "d" (val))
-// pos can be 0-1
+/* pos can be 0-1 */
 #define CVMX_MT_GFM_RESINP(val,pos)          asm volatile ("dmtc2 %[rt],0x025a+" CVMX_TMP_STR(pos) :                 : [rt] "d" (val))
 #define CVMX_MT_GFM_XOR0(val)                asm volatile ("dmtc2 %[rt],0x025c"                    :                 : [rt] "d" (val))
 #define CVMX_MT_GFM_XORMUL1(val)             asm volatile ("dmtc2 %[rt],0x425d"                    :                 : [rt] "d" (val))
-// pos can be 0-1
+/* pos can be 0-1 */
 #define CVMX_MT_GFM_MUL_REFLECT(val,pos)     asm volatile ("dmtc2 %[rt],0x0058+" CVMX_TMP_STR(pos) : [rt] "=d" (val) :               )
 #define CVMX_MT_GFM_XOR0_REFLECT(val)        asm volatile ("dmtc2 %[rt],0x005c"                    :                 : [rt] "d" (val))
 #define CVMX_MT_GFM_XORMUL1_REFLECT(val)     asm volatile ("dmtc2 %[rt],0x405d"                    :                 : [rt] "d" (val))
 
-// SNOW 3G
+/* SNOW 3G */
 
-// pos can be 0-7
+/* pos can be 0-7 */
 #define CVMX_MF_SNOW3G_LFSR(val,pos)    asm volatile ("dmfc2 %[rt],0x0240+" CVMX_TMP_STR(pos) : [rt] "=d" (val) :               )
-// pos can be 0-2
+/* pos can be 0-2 */
 #define CVMX_MF_SNOW3G_FSM(val,pos)     asm volatile ("dmfc2 %[rt],0x0251+" CVMX_TMP_STR(pos) : [rt] "=d" (val) :               )
 #define CVMX_MF_SNOW3G_RESULT(val)      asm volatile ("dmfc2 %[rt],0x0250"                    : [rt] "=d" (val) :               )
 
-// pos can be 0-7
+/* pos can be 0-7 */
 #define CVMX_MT_SNOW3G_LFSR(val,pos)    asm volatile ("dmtc2 %[rt],0x0240+" CVMX_TMP_STR(pos) : : [rt] "d" (val))
-// pos can be 0-2
+/* pos can be 0-2 */
 #define CVMX_MT_SNOW3G_FSM(val,pos)     asm volatile ("dmtc2 %[rt],0x0251+" CVMX_TMP_STR(pos) : : [rt] "d" (val))
 #define CVMX_MT_SNOW3G_RESULT(val)      asm volatile ("dmtc2 %[rt],0x0250"                    : : [rt] "d" (val))
 #define CVMX_MT_SNOW3G_START(val)       asm volatile ("dmtc2 %[rt],0x404d"                    : : [rt] "d" (val))
 #define CVMX_MT_SNOW3G_MORE(val)        asm volatile ("dmtc2 %[rt],0x404e"                    : : [rt] "d" (val))
 
-// SMS4
+/* SMS4 */
 
-// pos can be 0-1
+/* pos can be 0-1 */
 #define CVMX_MF_SMS4_IV(val,pos)	asm volatile ("dmfc2 %[rt],0x0102+"CVMX_TMP_STR(pos) : [rt] "=d" (val) :               )
-// pos can be 0-1
+/* pos can be 0-1 */
 #define CVMX_MF_SMS4_KEY(val,pos)	asm volatile ("dmfc2 %[rt],0x0104+"CVMX_TMP_STR(pos) : [rt] "=d" (val) :               )
-// pos can be 0-1
+/* pos can be 0-1 */
 #define CVMX_MF_SMS4_RESINP(val,pos)	asm volatile ("dmfc2 %[rt],0x0100+"CVMX_TMP_STR(pos) : [rt] "=d" (val) :               )
 #define CVMX_MT_SMS4_DEC_CBC0(val)	asm volatile ("dmtc2 %[rt],0x010c"                   : : [rt] "d" (val))
 #define CVMX_MT_SMS4_DEC_CBC1(val)	asm volatile ("dmtc2 %[rt],0x311d"      : : [rt] "d" (val))
@@ -591,48 +603,48 @@ extern "C" {
 #define CVMX_MT_SMS4_ENC_CBC1(val)	asm volatile ("dmtc2 %[rt],0x3119"      : : [rt] "d" (val))
 #define CVMX_MT_SMS4_ENC0(val)		asm volatile ("dmtc2 %[rt],0x010a"      : : [rt] "d" (val))
 #define CVMX_MT_SMS4_ENC1(val)		asm volatile ("dmtc2 %[rt],0x311b"      : : [rt] "d" (val))
-// pos can be 0-1
+/* pos can be 0-1 */
 #define CVMX_MT_SMS4_IV(val,pos)	asm volatile ("dmtc2 %[rt],0x0102+"CVMX_TMP_STR(pos) : : [rt] "d" (val))
-// pos can be 0-1
+/* pos can be 0-1 */
 #define CVMX_MT_SMS4_KEY(val,pos)	asm volatile ("dmtc2 %[rt],0x0104+"CVMX_TMP_STR(pos) : : [rt] "d" (val))
-// pos can be 0-1
+/* pos can be 0-1 */
 #define CVMX_MT_SMS4_RESINP(val,pos)	asm volatile ("dmtc2 %[rt],0x0100+"CVMX_TMP_STR(pos) : : [rt] "d" (val))
 
-// CAMELLIA
+/* CAMELLIA */
 
-// pos can be 0-1
+/* pos can be 0-1 */
 #define CVMX_MT_CAMELLIA_RESINP(val,pos) CVMX_MT_AES_RESULT(val,pos)
 #define CVMX_MT_CAMELLIA_ROUND(val)      asm volatile ("dmtc2 %[rt],0x3114" : : [rt] "d" (val))
 #define CVMX_MT_CAMELLIA_FL(val)         asm volatile ("dmtc2 %[rt],0x115" : : [rt] "d" (val))
 #define CVMX_MT_CAMELLIA_FLINV(val)      asm volatile ("dmtc2 %[rt],0x116" : : [rt] "d" (val))
 
-// pos can be 0-1
+/* pos can be 0-1 */
 #define CVMX_MF_CAMELLIA_RESINP(val,pos) CVMX_MF_AES_RESULT(val,pos)
 
-// ZUC
+/* ZUC */
 
-// pos can be 0-7  (0-6 normally used) pos=i has LFSR_s(2*i) in msbs,
-// LFSR_s(2*i + 1) in lsbs, <63> and <31> are unused
+/* pos can be 0-7  (0-6 normally used) pos=i has LFSR_s(2*i) in msbs, */
+/* LFSR_s(2*i + 1) in lsbs, <63> and <31> are unused */
 #define CVMX_MT_ZUC_LFSR(val,pos) CVMX_MT_HSH_DATW(val,pos)
-// pos can be 0-1  (not normally used) pos=0 has F_R1 in lsbs, pos=1 has F_R2 in lsbs
+/* pos can be 0-1  (not normally used) pos=0 has F_R1 in lsbs, pos=1 has F_R2 in lsbs */
 #define CVMX_MT_ZUC_FSM(val,pos)  CVMX_MT_HSH_IVW(val,(pos)+1)
 #define CVMX_MT_ZUC_RESULT(val)   CVMX_MT_HSH_IVW(val,0)
 #define CVMX_MT_ZUC_START(val)    asm volatile ("dmtc2 %[rt],0x4055" : : [rt] "d" (val))
 #define CVMX_MT_ZUC_MORE_NO_T     asm volatile ("dmtc2    $0,0x4056" : :)
 #define CVMX_MT_ZUC_MORE(val)     asm volatile ("dmtc2 %[rt],0x4056" : : [rt] "d" (val))
 
-// pos can be 0-7  (not normally used) pos=i has LFSR_s(2*i) in msbs,
-// LFSR_s(2*i + 1) in lsbs, <63> and <31> are unpredictable after ops
+/* pos can be 0-7  (not normally used) pos=i has LFSR_s(2*i) in msbs, */
+/* LFSR_s(2*i + 1) in lsbs, <63> and <31> are unpredictable after ops */
 #define CVMX_MF_ZUC_LFSR(val,pos) CVMX_MF_HSH_DATW(val,pos)
-// pos can be 0-1  (not normally used) pos=0 has F_R1 in lsbs, pos=1 has F_R2 in lsbs
+/* pos can be 0-1  (not normally used) pos=0 has F_R1 in lsbs, pos=1 has F_R2 in lsbs */
 #define CVMX_MF_ZUC_FSM(val,pos)  CVMX_MF_HSH_IVW(val,(pos)+1)
-// when T can be used, it is in <31:0> and <63:32> are zero
+/* when T can be used, it is in <31:0> and <63:32> are zero */
 #define CVMX_MF_ZUC_TRESULT(val)  CVMX_MF_HSH_IVW(val,3)
 #define CVMX_MF_ZUC_RESULT(val)   CVMX_MF_HSH_IVW(val,0)
 
-// SHA3 (Keccak)
+/* SHA3 (Keccak) */
 
-// pos can be 0-24
+/* pos can be 0-24 */
 #define CVMX_MT_SHA3_DAT(val,pos)   {                                        \
     if(pos <= 14)                                                           \
         CVMX_MT_HSH_DATW(val,pos);                                           \
@@ -644,13 +656,13 @@ extern "C" {
         asm volatile ("dmtc2 %[rt],0x50" : : [rt] "d" (val));               \
     }
 
-// pos can be 0-17 (use 0-8 for SHA3-512, 0-12 for SHA3-384, 0-16 for
-// SHA3-256, and 0-17 for SHA3-224)
+/* pos can be 0-17 (use 0-8 for SHA3-512, 0-12 for SHA3-384, 0-16 for */
+/* SHA3-256, and 0-17 for SHA3-224) */
 #define CVMX_MT_SHA3_XORDAT(val,pos) asm volatile ("dmtc2 %[rt],0x02C0+" CVMX_TMP_STR(pos) : : [rt] "d" (val))
-// works for all of SHA3-512, SHA3-384, SHA3-256, and SHA3-224
+/* works for all of SHA3-512, SHA3-384, SHA3-256, and SHA3-224 */
 #define CVMX_MT_SHA3_STARTOP         asm volatile ("dmtc2 $0,0x4052")
 
-// pos can be 0-24
+/* pos can be 0-24 */
 #define CVMX_MF_SHA3_DAT(val,pos)   {                                        \
     if(pos <= 14)                                                           \
         CVMX_MF_HSH_DATW(val,pos);                                           \
@@ -670,7 +682,7 @@ extern "C" {
 #endif
 
 #if 0
-#define CVMX_MF_CYCLE(dest)         asm volatile ("dmfc0 %[rt],$9,6" : [rt] "=d" (dest) : )	// Use (64-bit) CvmCount register rather than Count
+#define CVMX_MF_CYCLE(dest)         asm volatile ("dmfc0 %[rt],$9,6" : [rt] "=d" (dest) : )	/* Use (64-bit) CvmCount register rather than Count */
 #else
 #define CVMX_MF_CYCLE(dest)         CVMX_RDHWR(dest, 31)	/* reads the current (64-bit) CvmCount value */
 #endif
diff --git a/arch/mips/include/asm/octeon/cvmx-cmd-queue.h b/arch/mips/include/asm/octeon/cvmx-cmd-queue.h
index f7f780b..b7d938e 100644
--- a/arch/mips/include/asm/octeon/cvmx-cmd-queue.h
+++ b/arch/mips/include/asm/octeon/cvmx-cmd-queue.h
@@ -82,13 +82,14 @@
  * internal cycle counter to completely eliminate any causes of
  * bus traffic.
  *
- * <hr> $Revision: 90195 $ <hr>
+ * <hr> $Revision: 95258 $ <hr>
  */
 
 #ifndef __CVMX_CMD_QUEUE_H__
 #define __CVMX_CMD_QUEUE_H__
 
-#include "cvmx-fpa.h"
+#include "cvmx-fpa1.h"
+#include "cvmx-fpa3.h"
 
 #ifdef	__cplusplus
 /* *INDENT-OFF* */
@@ -329,6 +330,16 @@ __cvmx_cmd_queue_get_state(cvmx_cmd_queue_id_t queue_id)
 	return &__cvmx_cmd_queue_state_ptr->state[__cvmx_cmd_queue_get_index(queue_id)];
 }
 
+static inline uint64_t *__cvmx_cmd_queue_alloc_buffer(int pool)
+{
+	uint64_t *new_buffer;
+	if (octeon_has_feature(OCTEON_FEATURE_FPA3))
+		new_buffer = cvmx_fpa_alloc_aura(0, pool);
+	else
+		new_buffer = cvmx_fpa1_alloc(pool);
+	return new_buffer;
+}
+
 /**
  * Write an arbitrary number of command words to a command queue.
  * This is a generic function; the fixed number of command word
@@ -385,7 +396,7 @@ cvmx_cmd_queue_write(cvmx_cmd_queue_id_t queue_id, int use_locking,
 		uint64_t *ptr;
 		int count;
 		/* We need a new command buffer. Fail if there isn't one available */
-		uint64_t *new_buffer = (uint64_t *) cvmx_fpa_alloc(qptr->fpa_pool);
+		uint64_t *new_buffer = __cvmx_cmd_queue_alloc_buffer(qptr->fpa_pool);
 		if (cvmx_unlikely(new_buffer == NULL)) {
 			if (cvmx_likely(use_locking))
 				__cvmx_cmd_queue_unlock(qptr);
@@ -470,7 +481,7 @@ cvmx_cmd_queue_write2(cvmx_cmd_queue_id_t queue_id, int use_locking,
 		   location will be needed for the next buffer pointer */
 		int count = qptr->pool_size_m1 - qptr->index;
 		/* We need a new command buffer. Fail if there isn't one available */
-		uint64_t *new_buffer = (uint64_t *) cvmx_fpa_alloc(qptr->fpa_pool);
+		uint64_t *new_buffer = __cvmx_cmd_queue_alloc_buffer(qptr->fpa_pool);
 		if (cvmx_unlikely(new_buffer == NULL)) {
 			if (cvmx_likely(use_locking))
 				__cvmx_cmd_queue_unlock(qptr);
@@ -555,7 +566,7 @@ cvmx_cmd_queue_write3(cvmx_cmd_queue_id_t queue_id, int use_locking,
 		 */
 		int count = qptr->pool_size_m1 - qptr->index;
 		/* We need a new command buffer. Fail if there isn't one available */
-		uint64_t *new_buffer = (uint64_t *) cvmx_fpa_alloc(qptr->fpa_pool);
+		uint64_t *new_buffer = __cvmx_cmd_queue_alloc_buffer(qptr->fpa_pool);
 		if (cvmx_unlikely(new_buffer == NULL)) {
 			if (cvmx_likely(use_locking))
 				__cvmx_cmd_queue_unlock(qptr);
diff --git a/arch/mips/include/asm/octeon/cvmx-dma-engine.h b/arch/mips/include/asm/octeon/cvmx-dma-engine.h
index 079a61a..210d796 100644
--- a/arch/mips/include/asm/octeon/cvmx-dma-engine.h
+++ b/arch/mips/include/asm/octeon/cvmx-dma-engine.h
@@ -43,7 +43,7 @@
  * Interface to the PCI / PCIe DMA engines. These are only avialable
  * on chips with PCI / PCIe.
  *
- * <hr>$Revision: 93682 $<hr>
+ * <hr>$Revision: 95258 $<hr>
  */
 
 #ifndef __CVMX_DMA_ENGINES_H__
@@ -51,10 +51,10 @@
 
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx-dpi-defs.h>
-#include <asm/octeon/cvmx-fpa.h>
+#include <asm/octeon/cvmx-fpa1.h>
 #else
 #include "cvmx-dpi-defs.h"
-#include "cvmx-fpa.h"
+#include "cvmx-fpa1.h"
 #endif
 
 #ifdef	__cplusplus
diff --git a/arch/mips/include/asm/octeon/cvmx-fau.h b/arch/mips/include/asm/octeon/cvmx-fau.h
deleted file mode 100644
index 0613c9c..0000000
--- a/arch/mips/include/asm/octeon/cvmx-fau.h
+++ /dev/null
@@ -1,648 +0,0 @@
-/***********************license start***************
- * Copyright (c) 2003-2010  Cavium Inc. (support@cavium.com). All rights
- * reserved.
- *
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are
- * met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *
- *   * Redistributions in binary form must reproduce the above
- *     copyright notice, this list of conditions and the following
- *     disclaimer in the documentation and/or other materials provided
- *     with the distribution.
-
- *   * Neither the name of Cavium Inc. nor the names of
- *     its contributors may be used to endorse or promote products
- *     derived from this software without specific prior written
- *     permission.
-
- * This Software, including technical data, may be subject to U.S. export  control
- * laws, including the U.S. Export Administration Act and its  associated
- * regulations, and may be subject to export or import  regulations in other
- * countries.
-
- * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
- * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
- * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
- * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
- * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
- * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
- * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
- * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
- * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
- * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
- ***********************license end**************************************/
-
-/**
- * @file
- *
- * Interface to the hardware Fetch and Add Unit.
- *
- */
-
-#ifndef __CVMX_FAU_H__
-#define __CVMX_FAU_H__
-
-typedef int cvmx_fau_reg64_t;
-typedef int cvmx_fau_reg32_t;
-typedef int cvmx_fau_reg16_t;
-typedef int cvmx_fau_reg8_t;
-
-#define CVMX_FAU_REG_ANY 	-1
-
-#ifdef	__cplusplus
-/* *INDENT-OFF* */
-extern "C" {
-/* *INDENT-ON* */
-#endif
-
-/*
- * Octeon Fetch and Add Unit (FAU)
- */
-
-#define CVMX_FAU_LOAD_IO_ADDRESS    cvmx_build_io_address(0x1e, 0)
-#define CVMX_FAU_BITS_SCRADDR       63,56
-#define CVMX_FAU_BITS_LEN           55,48
-#define CVMX_FAU_BITS_INEVAL        35,14
-#define CVMX_FAU_BITS_TAGWAIT       13,13
-#define CVMX_FAU_BITS_NOADD         13,13
-#define CVMX_FAU_BITS_SIZE          12,11
-#define CVMX_FAU_BITS_REGISTER      10,0
-
-#define CVMX_FAU_MAX_REGISTERS_8  (2048)
-
-typedef enum {
-	CVMX_FAU_OP_SIZE_8 = 0,
-	CVMX_FAU_OP_SIZE_16 = 1,
-	CVMX_FAU_OP_SIZE_32 = 2,
-	CVMX_FAU_OP_SIZE_64 = 3
-} cvmx_fau_op_size_t;
-
-/**
- * Tagwait return definition. If a timeout occurs, the error
- * bit will be set. Otherwise the value of the register before
- * the update will be returned.
- */
-typedef struct {
-	uint64_t error:1;
-	int64_t value:63;
-} cvmx_fau_tagwait64_t;
-
-/**
- * Tagwait return definition. If a timeout occurs, the error
- * bit will be set. Otherwise the value of the register before
- * the update will be returned.
- */
-typedef struct {
-	uint64_t error:1;
-	int32_t value:31;
-} cvmx_fau_tagwait32_t;
-
-/**
- * Tagwait return definition. If a timeout occurs, the error
- * bit will be set. Otherwise the value of the register before
- * the update will be returned.
- */
-typedef struct {
-	uint64_t error:1;
-	int16_t value:15;
-} cvmx_fau_tagwait16_t;
-
-/**
- * Tagwait return definition. If a timeout occurs, the error
- * bit will be set. Otherwise the value of the register before
- * the update will be returned.
- */
-typedef struct {
-	uint64_t error:1;
-	int8_t value:7;
-} cvmx_fau_tagwait8_t;
-
-/**
- * Asynchronous tagwait return definition. If a timeout occurs,
- * the error bit will be set. Otherwise the value of the
- * register before the update will be returned.
- */
-typedef union {
-	uint64_t u64;
-	struct {
-		uint64_t invalid:1;
-		uint64_t data:63;	/* unpredictable if invalid is set */
-	} s;
-} cvmx_fau_async_tagwait_result_t;
-
-#ifdef __LITTLE_ENDIAN_BITFIELD
-#define SWIZZLE_8  0x7
-#define SWIZZLE_16 0x6
-#define SWIZZLE_32 0x4
-#else
-#define SWIZZLE_8  0
-#define SWIZZLE_16 0
-#define SWIZZLE_32 0
-#endif
-
-/**
- * @INTERNAL
- * Builds a store I/O address for writing to the FAU
- *
- * @param noadd  0 = Store value is atomically added to the current value
- *               1 = Store value is atomically written over the current value
- * @param reg    FAU atomic register to access. 0 <= reg < 2048.
- *               - Step by 2 for 16 bit access.
- *               - Step by 4 for 32 bit access.
- *               - Step by 8 for 64 bit access.
- * @return Address to store for atomic update
- */
-static inline uint64_t __cvmx_hwfau_store_address(uint64_t noadd, uint64_t reg)
-{
-	return (CVMX_ADD_IO_SEG(CVMX_FAU_LOAD_IO_ADDRESS) |
-		cvmx_build_bits(CVMX_FAU_BITS_NOADD, noadd) |
-		cvmx_build_bits(CVMX_FAU_BITS_REGISTER, reg));
-}
-
-/**
- * @INTERNAL
- * Builds a I/O address for accessing the FAU
- *
- * @param tagwait Should the atomic add wait for the current tag switch
- *                operation to complete.
- *                - 0 = Don't wait
- *                - 1 = Wait for tag switch to complete
- * @param reg     FAU atomic register to access. 0 <= reg < 2048.
- *                - Step by 2 for 16 bit access.
- *                - Step by 4 for 32 bit access.
- *                - Step by 8 for 64 bit access.
- * @param value   Signed value to add.
- *                Note: When performing 32 and 64 bit access, only the low
- *                22 bits are available.
- * @return Address to read from for atomic update
- */ static inline uint64_t __cvmx_hwfau_atomic_address(uint64_t tagwait, uint64_t reg, int64_t value)
-{
-	return (CVMX_ADD_IO_SEG(CVMX_FAU_LOAD_IO_ADDRESS) |
-		cvmx_build_bits(CVMX_FAU_BITS_INEVAL, value) |
-		cvmx_build_bits(CVMX_FAU_BITS_TAGWAIT, tagwait) |
-		cvmx_build_bits(CVMX_FAU_BITS_REGISTER, reg));
-}
-
-/**
- * Perform an atomic 64 bit add
- *
- * @param reg     FAU atomic register to access. 0 <= reg < 2048.
- *                - Step by 8 for 64 bit access.
- * @param value   Signed value to add.
- *                Note: Only the low 22 bits are available.
- * @return Value of the register before the update
- */
-static inline int64_t cvmx_hwfau_fetch_and_add64(cvmx_fau_reg64_t reg, int64_t value)
-{
-	return cvmx_read64_int64(__cvmx_hwfau_atomic_address(0, reg, value));
-}
-
-/**
- * Perform an atomic 32 bit add
- *
- * @param reg     FAU atomic register to access. 0 <= reg < 2048.
- *                - Step by 4 for 32 bit access.
- * @param value   Signed value to add.
- *                Note: Only the low 22 bits are available.
- * @return Value of the register before the update
- */
-static inline int32_t cvmx_hwfau_fetch_and_add32(cvmx_fau_reg32_t reg, int32_t value)
-{
-	reg ^= SWIZZLE_32;
-	return cvmx_read64_int32(__cvmx_hwfau_atomic_address(0, reg, value));
-}
-
-/**
- * Perform an atomic 16 bit add
- *
- * @param reg     FAU atomic register to access. 0 <= reg < 2048.
- *                - Step by 2 for 16 bit access.
- * @param value   Signed value to add.
- * @return Value of the register before the update
- */
-static inline int16_t cvmx_hwfau_fetch_and_add16(cvmx_fau_reg16_t reg, int16_t value)
-{
-	reg ^= SWIZZLE_16;
-	return cvmx_read64_int16(__cvmx_hwfau_atomic_address(0, reg, value));
-}
-
-/**
- * Perform an atomic 8 bit add
- *
- * @param reg     FAU atomic register to access. 0 <= reg < 2048.
- * @param value   Signed value to add.
- * @return Value of the register before the update
- */
-static inline int8_t cvmx_hwfau_fetch_and_add8(cvmx_fau_reg8_t reg, int8_t value)
-{
-	reg ^= SWIZZLE_8;
-	return cvmx_read64_int8(__cvmx_hwfau_atomic_address(0, reg, value));
-}
-
-/**
- * Perform an atomic 64 bit add after the current tag switch
- * completes
- *
- * @param reg    FAU atomic register to access. 0 <= reg < 2048.
- *               - Step by 8 for 64 bit access.
- * @param value  Signed value to add.
- *               Note: Only the low 22 bits are available.
- * @return If a timeout occurs, the error bit will be set. Otherwise
- *         the value of the register before the update will be
- *         returned
- */
-static inline cvmx_fau_tagwait64_t cvmx_hwfau_tagwait_fetch_and_add64(cvmx_fau_reg64_t reg, int64_t value)
-{
-	union {
-		uint64_t i64;
-		cvmx_fau_tagwait64_t t;
-	} result;
-	result.i64 = cvmx_read64_int64(__cvmx_hwfau_atomic_address(1, reg, value));
-		
-	return result.t;
-}
-
-/**
- * Perform an atomic 32 bit add after the current tag switch
- * completes
- *
- * @param reg    FAU atomic register to access. 0 <= reg < 2048.
- *               - Step by 4 for 32 bit access.
- * @param value  Signed value to add.
- *               Note: Only the low 22 bits are available.
- * @return If a timeout occurs, the error bit will be set. Otherwise
- *         the value of the register before the update will be
- *         returned
- */
-static inline cvmx_fau_tagwait32_t cvmx_hwfau_tagwait_fetch_and_add32(cvmx_fau_reg32_t reg, int32_t value)
-{
-	union {
-		uint64_t i32;
-		cvmx_fau_tagwait32_t t;
-	} result;
-	reg ^= SWIZZLE_32;
-	result.i32 = cvmx_read64_int32(__cvmx_hwfau_atomic_address(1, reg, value));
-	return result.t;
-}
-
-/**
- * Perform an atomic 16 bit add after the current tag switch
- * completes
- *
- * @param reg    FAU atomic register to access. 0 <= reg < 2048.
- *               - Step by 2 for 16 bit access.
- * @param value  Signed value to add.
- * @return If a timeout occurs, the error bit will be set. Otherwise
- *         the value of the register before the update will be
- *         returned
- */
-static inline cvmx_fau_tagwait16_t cvmx_hwfau_tagwait_fetch_and_add16(cvmx_fau_reg16_t reg, int16_t value)
-{
-	union {
-		uint64_t i16;
-		cvmx_fau_tagwait16_t t;
-	} result;
-	reg ^= SWIZZLE_16;
-	result.i16 = cvmx_read64_int16(__cvmx_hwfau_atomic_address(1, reg, value));
-	return result.t;
-}
-
-/**
- * Perform an atomic 8 bit add after the current tag switch
- * completes
- *
- * @param reg    FAU atomic register to access. 0 <= reg < 2048.
- * @param value  Signed value to add.
- * @return If a timeout occurs, the error bit will be set. Otherwise
- *         the value of the register before the update will be
- *         returned
- */
-static inline cvmx_fau_tagwait8_t cvmx_hwfau_tagwait_fetch_and_add8(cvmx_fau_reg8_t reg, int8_t value)
-{
-	union {
-		uint64_t i8;
-		cvmx_fau_tagwait8_t t;
-	} result;
-	reg ^= SWIZZLE_8;
-	result.i8 = cvmx_read64_int8(__cvmx_hwfau_atomic_address(1, reg, value));
-	return result.t;
-}
-
-/**
- * @INTERNAL
- * Builds I/O data for async operations
- *
- * @param scraddr Scratch pad byte addres to write to.  Must be 8 byte aligned
- * @param value   Signed value to add.
- *                Note: When performing 32 and 64 bit access, only the low
- *                22 bits are available.
- * @param tagwait Should the atomic add wait for the current tag switch
- *                operation to complete.
- *                - 0 = Don't wait
- *                - 1 = Wait for tag switch to complete
- * @param size    The size of the operation:
- *                - CVMX_FAU_OP_SIZE_8  (0) = 8 bits
- *                - CVMX_FAU_OP_SIZE_16 (1) = 16 bits
- *                - CVMX_FAU_OP_SIZE_32 (2) = 32 bits
- *                - CVMX_FAU_OP_SIZE_64 (3) = 64 bits
- * @param reg     FAU atomic register to access. 0 <= reg < 2048.
- *                - Step by 2 for 16 bit access.
- *                - Step by 4 for 32 bit access.
- *                - Step by 8 for 64 bit access.
- * @return Data to write using cvmx_send_single
- */
-static inline uint64_t __cvmx_fau_iobdma_data(uint64_t scraddr, int64_t value, uint64_t tagwait, cvmx_fau_op_size_t size, uint64_t reg)
-{
-	return (CVMX_FAU_LOAD_IO_ADDRESS |
-		cvmx_build_bits(CVMX_FAU_BITS_SCRADDR, scraddr >> 3) |
-		cvmx_build_bits(CVMX_FAU_BITS_LEN, 1) |
-		cvmx_build_bits(CVMX_FAU_BITS_INEVAL, value) |
-		cvmx_build_bits(CVMX_FAU_BITS_TAGWAIT, tagwait) |
-		cvmx_build_bits(CVMX_FAU_BITS_SIZE, size) |
-		cvmx_build_bits(CVMX_FAU_BITS_REGISTER, reg));
-}
-
-/**
- * Perform an async atomic 64 bit add. The old value is
- * placed in the scratch memory at byte address scraddr.
- *
- * @param scraddr Scratch memory byte address to put response in.
- *                Must be 8 byte aligned.
- * @param reg     FAU atomic register to access. 0 <= reg < 2048.
- *                - Step by 8 for 64 bit access.
- * @param value   Signed value to add.
- *                Note: Only the low 22 bits are available.
- * @return Placed in the scratch pad register
- */
-static inline void cvmx_hwfau_async_fetch_and_add64(uint64_t scraddr, cvmx_fau_reg64_t reg, int64_t value)
-{
-	cvmx_send_single(__cvmx_fau_iobdma_data(scraddr, value, 0, CVMX_FAU_OP_SIZE_64, reg));
-}
-
-/**
- * Perform an async atomic 32 bit add. The old value is
- * placed in the scratch memory at byte address scraddr.
- *
- * @param scraddr Scratch memory byte address to put response in.
- *                Must be 8 byte aligned.
- * @param reg     FAU atomic register to access. 0 <= reg < 2048.
- *                - Step by 4 for 32 bit access.
- * @param value   Signed value to add.
- *                Note: Only the low 22 bits are available.
- * @return Placed in the scratch pad register
- */
-static inline void cvmx_hwfau_async_fetch_and_add32(uint64_t scraddr, cvmx_fau_reg32_t reg, int32_t value)
-{
-	cvmx_send_single(__cvmx_fau_iobdma_data(scraddr, value, 0, CVMX_FAU_OP_SIZE_32, reg));
-}
-
-/**
- * Perform an async atomic 16 bit add. The old value is
- * placed in the scratch memory at byte address scraddr.
- *
- * @param scraddr Scratch memory byte address to put response in.
- *                Must be 8 byte aligned.
- * @param reg     FAU atomic register to access. 0 <= reg < 2048.
- *                - Step by 2 for 16 bit access.
- * @param value   Signed value to add.
- * @return Placed in the scratch pad register
- */
-static inline void cvmx_hwfau_async_fetch_and_add16(uint64_t scraddr, cvmx_fau_reg16_t reg, int16_t value)
-{
-	cvmx_send_single(__cvmx_fau_iobdma_data(scraddr, value, 0, CVMX_FAU_OP_SIZE_16, reg));
-}
-
-/**
- * Perform an async atomic 8 bit add. The old value is
- * placed in the scratch memory at byte address scraddr.
- *
- * @param scraddr Scratch memory byte address to put response in.
- *                Must be 8 byte aligned.
- * @param reg     FAU atomic register to access. 0 <= reg < 2048.
- * @param value   Signed value to add.
- * @return Placed in the scratch pad register
- */
-static inline void cvmx_hwfau_async_fetch_and_add8(uint64_t scraddr, cvmx_fau_reg8_t reg, int8_t value)
-{
-	cvmx_send_single(__cvmx_fau_iobdma_data(scraddr, value, 0, CVMX_FAU_OP_SIZE_8, reg));
-}
-
-/**
- * Perform an async atomic 64 bit add after the current tag
- * switch completes.
- *
- * @param scraddr Scratch memory byte address to put response in.
- *                Must be 8 byte aligned.
- *                If a timeout occurs, the error bit (63) will be set. Otherwise
- *                the value of the register before the update will be
- *                returned
- * @param reg     FAU atomic register to access. 0 <= reg < 2048.
- *                - Step by 8 for 64 bit access.
- * @param value   Signed value to add.
- *                Note: Only the low 22 bits are available.
- * @return Placed in the scratch pad register
- */
-static inline void cvmx_hwfau_async_tagwait_fetch_and_add64(uint64_t scraddr, cvmx_fau_reg64_t reg, int64_t value)
-{
-	cvmx_send_single(__cvmx_fau_iobdma_data(scraddr, value, 1, CVMX_FAU_OP_SIZE_64, reg));
-}
-
-/**
- * Perform an async atomic 32 bit add after the current tag
- * switch completes.
- *
- * @param scraddr Scratch memory byte address to put response in.
- *                Must be 8 byte aligned.
- *                If a timeout occurs, the error bit (63) will be set. Otherwise
- *                the value of the register before the update will be
- *                returned
- * @param reg     FAU atomic register to access. 0 <= reg < 2048.
- *                - Step by 4 for 32 bit access.
- * @param value   Signed value to add.
- *                Note: Only the low 22 bits are available.
- * @return Placed in the scratch pad register
- */
-static inline void cvmx_hwfau_async_tagwait_fetch_and_add32(uint64_t scraddr, cvmx_fau_reg32_t reg, int32_t value)
-{
-	cvmx_send_single(__cvmx_fau_iobdma_data(scraddr, value, 1, CVMX_FAU_OP_SIZE_32, reg));
-}
-
-/**
- * Perform an async atomic 16 bit add after the current tag
- * switch completes.
- *
- * @param scraddr Scratch memory byte address to put response in.
- *                Must be 8 byte aligned.
- *                If a timeout occurs, the error bit (63) will be set. Otherwise
- *                the value of the register before the update will be
- *                returned
- * @param reg     FAU atomic register to access. 0 <= reg < 2048.
- *                - Step by 2 for 16 bit access.
- * @param value   Signed value to add.
- * @return Placed in the scratch pad register
- */
-static inline void cvmx_hwfau_async_tagwait_fetch_and_add16(uint64_t scraddr, cvmx_fau_reg16_t reg, int16_t value)
-{
-	cvmx_send_single(__cvmx_fau_iobdma_data(scraddr, value, 1, CVMX_FAU_OP_SIZE_16, reg));
-}
-
-/**
- * Perform an async atomic 8 bit add after the current tag
- * switch completes.
- *
- * @param scraddr Scratch memory byte address to put response in.
- *                Must be 8 byte aligned.
- *                If a timeout occurs, the error bit (63) will be set. Otherwise
- *                the value of the register before the update will be
- *                returned
- * @param reg     FAU atomic register to access. 0 <= reg < 2048.
- * @param value   Signed value to add.
- * @return Placed in the scratch pad register
- */
-static inline void cvmx_hwfau_async_tagwait_fetch_and_add8(uint64_t scraddr, cvmx_fau_reg8_t reg, int8_t value)
-{
-	cvmx_send_single(__cvmx_fau_iobdma_data(scraddr, value, 1, CVMX_FAU_OP_SIZE_8, reg));
-}
-
-/**
- * Perform an atomic 64 bit add
- *
- * @param reg     FAU atomic register to access. 0 <= reg < 2048.
- *                - Step by 8 for 64 bit access.
- * @param value   Signed value to add.
- */
-static inline void cvmx_hwfau_atomic_add64(cvmx_fau_reg64_t reg, int64_t value)
-{
-	cvmx_write64_int64(__cvmx_hwfau_store_address(0, reg), value);
-}
-
-/**
- * Perform an atomic 32 bit add
- *
- * @param reg     FAU atomic register to access. 0 <= reg < 2048.
- *                - Step by 4 for 32 bit access.
- * @param value   Signed value to add.
- */
-static inline void cvmx_hwfau_atomic_add32(cvmx_fau_reg32_t reg, int32_t value)
-{
-	reg ^= SWIZZLE_32;
-	cvmx_write64_int32(__cvmx_hwfau_store_address(0, reg), value);
-}
-
-/**
- * Perform an atomic 16 bit add
- *
- * @param reg     FAU atomic register to access. 0 <= reg < 2048.
- *                - Step by 2 for 16 bit access.
- * @param value   Signed value to add.
- */
-static inline void cvmx_hwfau_atomic_add16(cvmx_fau_reg16_t reg, int16_t value)
-{
-	reg ^= SWIZZLE_16;
-	cvmx_write64_int16(__cvmx_hwfau_store_address(0, reg), value);
-}
-
-/**
- * Perform an atomic 8 bit add
- *
- * @param reg     FAU atomic register to access. 0 <= reg < 2048.
- * @param value   Signed value to add.
- */
-static inline void cvmx_hwfau_atomic_add8(cvmx_fau_reg8_t reg, int8_t value)
-{
-	reg ^= SWIZZLE_8;
-	cvmx_write64_int8(__cvmx_hwfau_store_address(0, reg), value);
-}
-
-/**
- * Perform an atomic 64 bit write
- *
- * @param reg     FAU atomic register to access. 0 <= reg < 2048.
- *                - Step by 8 for 64 bit access.
- * @param value   Signed value to write.
- */
-static inline void cvmx_hwfau_atomic_write64(cvmx_fau_reg64_t reg, int64_t value)
-{
-	cvmx_write64_int64(__cvmx_hwfau_store_address(1, reg), value);
-}
-
-/**
- * Perform an atomic 32 bit write
- *
- * @param reg     FAU atomic register to access. 0 <= reg < 2048.
- *                - Step by 4 for 32 bit access.
- * @param value   Signed value to write.
- */
-static inline void cvmx_hwfau_atomic_write32(cvmx_fau_reg32_t reg, int32_t value)
-{
-	reg ^= SWIZZLE_32;
-	cvmx_write64_int32(__cvmx_hwfau_store_address(1, reg), value);
-}
-
-/**
- * Perform an atomic 16 bit write
- *
- * @param reg     FAU atomic register to access. 0 <= reg < 2048.
- *                - Step by 2 for 16 bit access.
- * @param value   Signed value to write.
- */
-static inline void cvmx_hwfau_atomic_write16(cvmx_fau_reg16_t reg, int16_t value)
-{
-	reg ^= SWIZZLE_16;
-	cvmx_write64_int16(__cvmx_hwfau_store_address(1, reg), value);
-}
-
-/**
- * Perform an atomic 8 bit write
- *
- * @param reg     FAU atomic register to access. 0 <= reg < 2048.
- * @param value   Signed value to write.
- */
-static inline void cvmx_hwfau_atomic_write8(cvmx_fau_reg8_t reg, int8_t value)
-{
-	reg ^= SWIZZLE_8;
-	cvmx_write64_int8(__cvmx_hwfau_store_address(1, reg), value);
-}
-
-/** Allocates 64bit FAU register.
- *  @return value is the base address of allocated FAU register
- */
-extern int cvmx_fau64_alloc(int reserve);
-
-/** Allocates 32bit FAU register.
- *  @return value is the base address of allocated FAU register
- */
-extern int cvmx_fau32_alloc(int reserve);
-
-/** Allocates 16bit FAU register.
- *  @return value is the base address of allocated FAU register
- */
-extern int cvmx_fau16_alloc(int reserve);
-
-/** Allocates 8bit FAU register.
- *  @return value is the base address of allocated FAU register
- */
-extern int cvmx_fau8_alloc(int reserve);
-
-/** Frees the specified FAU register.
- *  @param Base address of register to release.
- *  @return 0 on success; -1 on failure
- */
-extern int cvmx_fau_free(int address);
-
-/** Display the fau registers array
- */
-extern void cvmx_fau_show(void);
-
-#ifdef	__cplusplus
-/* *INDENT-OFF* */
-}
-/* *INDENT-ON* */
-#endif
-
-#endif /* __CVMX_FAU_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-fpa-defs.h b/arch/mips/include/asm/octeon/cvmx-fpa-defs.h
index 0fe81af..015fb2f 100644
--- a/arch/mips/include/asm/octeon/cvmx-fpa-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-fpa-defs.h
@@ -3834,4 +3834,97 @@ union cvmx_fpa_wqe_threshold {
 };
 typedef union cvmx_fpa_wqe_threshold cvmx_fpa_wqe_threshold_t;
 
+/**
+ * Structure describing the data format used for stores to the FPA.
+ */
+typedef union {
+	uint64_t u64;
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t scraddr:8;	/**
+					 * the (64-bit word) location in
+					 * scratchpad to write to (if len != 0)
+					 */
+		uint64_t len:8;		/**
+					 * the number of words in the response
+					 * (0 => no response)
+					 */
+		uint64_t did:8;		/**
+					 * the ID of the device on the
+					 * non-coherent bus
+					 */
+		uint64_t addr:40;	/**
+					 * the address that will appear in the
+					 * first tick on the NCB bus
+					 */
+#else
+		uint64_t addr:40;	/**
+					 * the address that will appear in the
+					 * first tick on the NCB bus
+					 */
+		uint64_t did:8;		/**
+					 * the ID of the device on the
+					 * non-coherent bus
+					 */
+		uint64_t len:8;		/**
+					 * the number of words in the response
+					 * (0 => no response)
+					 */
+		uint64_t scraddr:8;	/**
+					 * the (64-bit word) location in
+					 * scratchpad to write to (if len != 0)
+					 */
+#endif
+	} s;
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t scraddr:8;     /**
+					 * the (64-bit word) location in
+					 * scratchpad to write to (if len != 0)
+					 */
+		uint64_t len:8;		/**
+					 * length of return in workds, must be
+					 * one.  If the pool has no availale pts
+					 * in the selected pool then the ptr
+					 * returned for the IOBDMA operation are
+					 * 0s, indicating that the pool does not
+					 * have an adequate number of ptrs to
+					 * satisfy the IOBDMA.
+					 */
+		uint64_t did:8;		/**
+					 * the ID of the device on the
+					 * non-coherent bus
+					 */
+		uint64_t node:4;	/** OCI node number */
+		uint64_t red:1;		/** Perform RED on allocation */
+		uint64_t reserved2:9;   /** Reserved */
+		uint64_t aura:10;	/** Aura number */
+		uint64_t reserved3:16;	/** Reserved */
+#else
+		uint64_t reserved3:16;	/** Reserved */
+		uint64_t aura:10;	/** Aura number */
+		uint64_t reserved2:9;   /** Reserved */
+		uint64_t red:1;		/** Perform RED on allocation */
+		uint64_t node:4;	/** OCI node number */
+		uint64_t did:8;		/**
+					 * the ID of the device on the
+					 * non-coherent bus
+					 */
+		uint64_t len:8;		/**
+					 * length of return in workds, must be
+					 * one.  If the pool has no availale pts
+					 * in the selected pool then the ptr
+					 * returned for the IOBDMA operation are
+					 * 0s, indicating that the pool does not
+					 * have an adequate number of ptrs to
+					 * satisfy the IOBDMA.
+					 */
+		uint64_t scraddr:8;	/**
+					 * the (64-bit word) location in
+					 * scratchpad to write to (if len != 0)
+					 */
+#endif
+	} cn78xx;
+} cvmx_fpa_iobdma_data_t;
+
 #endif
diff --git a/arch/mips/include/asm/octeon/cvmx-fpa.h b/arch/mips/include/asm/octeon/cvmx-fpa.h
deleted file mode 100644
index cf9b50c..0000000
--- a/arch/mips/include/asm/octeon/cvmx-fpa.h
+++ /dev/null
@@ -1,818 +0,0 @@
-/***********************license start***************
- * Copyright (c) 2003-2010  Cavium Inc. (support@cavium.com). All rights
- * reserved.
- *
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are
- * met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *
- *   * Redistributions in binary form must reproduce the above
- *     copyright notice, this list of conditions and the following
- *     disclaimer in the documentation and/or other materials provided
- *     with the distribution.
-
- *   * Neither the name of Cavium Inc. nor the names of
- *     its contributors may be used to endorse or promote products
- *     derived from this software without specific prior written
- *     permission.
-
- * This Software, including technical data, may be subject to U.S. export  control
- * laws, including the U.S. Export Administration Act and its  associated
- * regulations, and may be subject to export or import  regulations in other
- * countries.
-
- * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
- * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
- * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
- * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
- * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
- * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
- * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
- * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
- * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
- * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
- ***********************license end**************************************/
-
-/**
- * @file
- *
- * Interface to the hardware Free Pool Allocator.
- *
- * <hr>$Revision: 94697 $<hr>
- *
- */
-
-#ifndef __CVMX_FPA_H__
-#define __CVMX_FPA_H__
-
-#include "cvmx-scratch.h"
-#include "cvmx.h"
-//#include "cvmx-spinlock.h"
-
-
-#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
-#include "cvmx-fpa-defs.h"
-#endif
-
-#ifdef	__cplusplus
-/* *INDENT-OFF* */
-extern "C" {
-/* *INDENT-ON* */
-#endif
-
-#define CVMX_FPA_NUM_POOLS      8
-#define CVMX_FPA3_NUM_POOLS	64
-#define CVMX_FPA_AURA_NUM       1024
-#define CVMX_FPA_MIN_BLOCK_SIZE 128
-#define CVMX_FPA_ALIGNMENT      128
-#define CVMX_FPA_POOL_NAME_LEN  16
-#define CVMX_FPA_AURA_NAME_LEN  16
-
-/**
- * Structure describing the data format used for stores to the FPA.
- */
-typedef union {
-	uint64_t u64;
-	struct {
-#ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t scraddr:8;	/**
-					 * the (64-bit word) location in
-					 * scratchpad to write to (if len != 0)
-					 */
-		uint64_t len:8;		/**
-					 * the number of words in the response
-					 * (0 => no response)
-					 */
-		uint64_t did:8;		/**
-					 * the ID of the device on the
-					 * non-coherent bus
-					 */
-		uint64_t addr:40;	/**
-					 * the address that will appear in the
-					 * first tick on the NCB bus
-					 */
-#else
-		uint64_t addr:40;	/**
-					 * the address that will appear in the
-					 * first tick on the NCB bus
-					 */
-		uint64_t did:8;		/**
-					 * the ID of the device on the
-					 * non-coherent bus
-					 */
-		uint64_t len:8;		/**
-					 * the number of words in the response
-					 * (0 => no response)
-					 */
-		uint64_t scraddr:8;	/**
-					 * the (64-bit word) location in
-					 * scratchpad to write to (if len != 0)
-					 */
-#endif
-	} s;
-	struct {
-#ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t scraddr:8;     /**
-					 * the (64-bit word) location in
-					 * scratchpad to write to (if len != 0)
-					 */
-		uint64_t len:8;		/**
-					 * length of return in workds, must be
-					 * one.  If the pool has no availale pts
-					 * in the selected pool then the ptr
-					 * returned for the IOBDMA operation are
-					 * 0s, indicating that the pool does not
-					 * have an adequate number of ptrs to
-					 * satisfy the IOBDMA.
-					 */
-		uint64_t did:8;		/**
-					 * the ID of the device on the
-					 * non-coherent bus
-					 */
-		uint64_t node:4;	/** OCI node number */
-		uint64_t red:1;		/** Perform RED on allocation */
-		uint64_t reserved2:9;   /** Reserved */
-		uint64_t aura:10;	/** Aura number */
-		uint64_t reserved3:16;	/** Reserved */
-#else
-		uint64_t reserved3:16;	/** Reserved */
-		uint64_t aura:10;	/** Aura number */
-		uint64_t reserved2:9;   /** Reserved */
-		uint64_t red:1;		/** Perform RED on allocation */
-		uint64_t node:4;	/** OCI node number */
-		uint64_t did:8;		/**
-					 * the ID of the device on the
-					 * non-coherent bus
-					 */
-		uint64_t len:8;		/**
-					 * length of return in workds, must be
-					 * one.  If the pool has no availale pts
-					 * in the selected pool then the ptr
-					 * returned for the IOBDMA operation are
-					 * 0s, indicating that the pool does not
-					 * have an adequate number of ptrs to
-					 * satisfy the IOBDMA.
-					 */
-		uint64_t scraddr:8;	/**
-					 * the (64-bit word) location in
-					 * scratchpad to write to (if len != 0)
-					 */
-#endif
-	} cn78xx;
-} cvmx_fpa_iobdma_data_t;
-
-/**
- * Struct describing load allocate operation addresses for FPA pool.
- */
-typedef union {
-	uint64_t u64;
-	struct {
-#ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved1:15;
-		uint64_t io:1;		/** Indicates I/O space */
-		uint64_t did:8;		/**
-					 * the ID of the device on the
-					 * non-coherent bus
-					 */
-		uint64_t node:4;	/** OCI node number */
-		uint64_t red:1;		/** Perform RED on allocation */
-		uint64_t reserved2:9;   /** Reserved */
-		uint64_t aura:10;	/** Aura number */
-		uint64_t reserved3:16;	/** Reserved */
-#else
-		uint64_t reserved3:16;	/** Reserved */
-		uint64_t aura:10;	/** Aura number */
-		uint64_t reserved2:9;   /** Reserved */
-		uint64_t red:1;		/** Perform RED on allocation */
-		uint64_t node:4;	/** OCI node number */
-		uint64_t did:8;		/**
-					 * the ID of the device on the
-					 * non-coherent bus
-					 */
-		uint64_t io:1;		/** Indicates I/O space */
-		uint64_t reserved1:15;
-
-#endif
-	} cn78xx;
-} cvmx_fpa_load_data_t;
-
-/**
- * Struct describing store free operation addresses from FPA pool.
- */
-typedef union {
-	uint64_t u64;
-	struct {
-#ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved1:15;
-		uint64_t io:1;		/** Indicates I/O space */
-		uint64_t did:8;		/**
-					 * the ID of the device on the
-					 * non-coherent bus
-					 */
-		uint64_t node:4;	/** OCI node number */
-		uint64_t reserved2:10;  /** Reserved */
-		uint64_t aura:10;	/** Aura number */
-		uint64_t fabs:1;	/** free absolute */
-		uint64_t reserved3:3;	/** Reserved */
-		uint64_t dwb_count:9;	/**
-					 * Number of cache lines for which the
-					 * hardware should try to execute
-					 * 'don't-write-back' commands.
-					 */
-		uint64_t reserved4:3;	/** Reserved */
-#else
-		uint64_t reserved4:3;	/** Reserved */
-		uint64_t dwb_count:9;	/**
-					 * Number of cache lines for which the
-					 * hardware should try to execute
-					 * 'don't-write-back' commands.
-					 */
-		uint64_t reserved3:3;	/** Reserved */
-		uint64_t fabs:1;	/** free absolute */
-		uint64_t aura:10;	/** Aura number */
-		uint64_t reserved2:10;  /** Reserved */
-		uint64_t node:4;	/** OCI node number */
-		uint64_t did:8;		/**
-					 * the ID of the device on the
-					 * non-coherent bus
-					 */
-		uint64_t io:1;		/** Indicates I/O space */
-		uint64_t reserved1:15;
-#endif
-	} cn78xx;
-} cvmx_fpa_store_addr_t;
-
-enum fpa_pool_alignment {
-	FPA_NATURAL_ALIGNMENT,
-	FPA_OFFSET_ALIGNMENT,
-	FPA_OPAQUE_ALIGNMENT
-};
-
-/**
- * Structure describing the current state of a FPA pool.
- */
-typedef struct {
-	char name[CVMX_FPA_POOL_NAME_LEN];	/** FPA Pool Name */
-	uint64_t size;				/** Size of each block */
-	void *base;				/**
-						 * The base memory address of
-						 * whole block
-						 */
-	uint64_t stack_base;			/**
-						 * Base address of stack of FPA
-						 * pool
-						 */
-	uint64_t starting_element_count;	/**
-						 * The number of elements in the
-						 * pool at creation
-						 */
-	uint64_t max_buffer_cnt;		/**
-						 * Maximum amount of buffers
-						 * that can be held in this
-						 * FPA pool
-						 */
-} cvmx_fpa_pool_info_t;
-
-
-/**
- * Structure which contains information on auras.
- */
-typedef struct {
-	char name[CVMX_FPA_AURA_NAME_LEN];
-	int pool_num;
-} cvmx_fpa_aura_info_t;
-
-/**
- * Structure to store FPA pool configuration parameters.
- */
-typedef struct cvmx_fpa_pool_config
-{
-	int64_t pool_num;
-	uint64_t buffer_size;
-	uint64_t buffer_count;
-}cvmx_fpa_pool_config_t;
-
-int cvmx_fpa_allocate_auras(int node, int auras_allocated[], int count);
-int cvmx_fpa_free_auras(int node, int *pools_allocated, int count);
-/**
- * Current state of all the pools. Use access functions
- * instead of using it directly.
- */
-extern CVMX_SHARED cvmx_fpa_pool_info_t
-cvmx_fpa_pool_info[CVMX_MAX_NODES][CVMX_FPA3_NUM_POOLS];
-
-/* CSR typedefs have been moved to cvmx-fpa-defs.h */
-
-/**
- * Get a new block from the FPA Aura
- *
- * @param node  - node number
- * @param aura  - aura to get the block from
- * @return pointer to the block or NULL on failure
- */
-static inline void *cvmx_fpa_alloc_aura(int node, int aura)
-{
-	uint64_t address;
-	cvmx_fpa_load_data_t load_addr;
-
-	load_addr.u64 = 0;
-	load_addr.cn78xx.io = 1;
-	load_addr.cn78xx.did = 0x29;    /* Device ID. Indicates FPA. */
-	load_addr.cn78xx.node = node;   /* OCI node number */
-	load_addr.cn78xx.red = 0;       /* Perform RED on allocation.
-					 * FIXME to use config option
-					 */
-	load_addr.cn78xx.aura = aura;   /* Aura number */
-
-	address = cvmx_read_csr((CVMX_ADD_SEG(CVMX_MIPS_SPACE_XKPHYS,
-					      load_addr.u64)));
-	if (!address)
-		return NULL;
-	return cvmx_phys_to_ptr(address);
-}
-
-/**
- * Free a pointer back to the aura.
- *
- * @param node   node number
- * @param aura   aura number
- * @param ptr    physical address of block to free.
- * @param num_cache_lines Cache lines to invalidate
- */
-static inline void cvmx_fpa_free_aura(void *ptr, uint64_t node, int aura,
-				      uint64_t num_cache_lines)
-{
-	cvmx_fpa_store_addr_t newptr;
-	cvmx_addr_t newdata;
-
-	newdata.u64 = cvmx_ptr_to_phys(ptr);
-
-	newptr.u64 = 2ull<<62;
-	newptr.cn78xx.io = 1;
-	newptr.cn78xx.did = 0x29;    /* Device id, indicates FPA */
-	newptr.cn78xx.node = node;   /* OCI node number. */
-	newptr.cn78xx.aura = aura;   /* Aura number */
-	newptr.cn78xx.fabs = 0;	/* Free absolute. FIXME to use config option */
-	newptr.cn78xx.dwb_count = num_cache_lines;
-
-	/*cvmx_dprintf("aura = %d ptr_to_phys(ptr) = 0x%llx newptr.u64 = 0x%llx"
-		     " ptr = %p \n", ptr, aura, (ULL) newptr.u64
-		     (ULL) cvmx_ptr_to_phys(ptr)); */
-	/* Make sure that any previous writes to memory go out before we free
-	   this buffer. This also serves as a barrier to prevent GCC from
-	   reordering operations to after the free. */
-	CVMX_SYNCWS;
-        cvmx_write_io(newptr.u64, newdata.u64);
-}
-
-/**
- * Return the name of the pool
- *
- * @param pool   Pool to get the name of
- * @return The name
- */
-static inline const char *cvmx_fpa_get_name(uint64_t pool)
-{
-	return cvmx_fpa_pool_info[0][pool].name;
-}
-
-/**
- * Return the base of the pool
- *
- * @param pool   Pool to get the base of
- * @return The base
- */ static inline void *cvmx_fpa_get_base(uint64_t pool)
-{
-	return cvmx_fpa_pool_info[0][pool].base;
-}
-
-/**
- * Check if a pointer belongs to an FPA pool. Return non-zero
- * if the supplied pointer is inside the memory controlled by
- * an FPA pool.
- *
- * @param pool   Pool to check
- * @param ptr    Pointer to check
- * @return Non-zero if pointer is in the pool. Zero if not
- */
-static inline int cvmx_fpa_is_member(uint64_t pool, void *ptr)
-{
-	return ((ptr >= cvmx_fpa_pool_info[0][pool].base) &&
-		((char *)ptr < ((char *)(cvmx_fpa_pool_info[0][pool].base))
-		 + (cvmx_fpa_pool_info[0][pool].size
-		    * cvmx_fpa_pool_info[0][pool].starting_element_count)));
-}
-
-/**
- * Enable the FPA for use. Must be performed after any CSR
- * configuration but before any other FPA functions.
- */
-static inline void cvmx_fpa_enable(void)
-{
-	cvmx_fpa_ctl_status_t status;
-
-	if (!octeon_has_feature(OCTEON_FEATURE_FPA3)) {
-		status.u64 = cvmx_read_csr(CVMX_FPA_CTL_STATUS);
-		if (status.s.enb) {
-			/*
-		 	* CN68XXP1 should not reset the FPA (doing so may break
-			* the SSO, so we may end up enabling it more than once.
-			* Just return and don't spew messages.
-		 	*/
-			return;
-		}
-
-		status.u64 = 0;
-		status.s.enb = 1;
-		cvmx_write_csr(CVMX_FPA_CTL_STATUS, status.u64);
-	}
-}
-
-/**
- * Reset FPA to disable. Make sure buffers from all FPA pools are freed
- * before disabling FPA.
- */
-static inline void cvmx_fpa_disable(void)
-{
-	cvmx_fpa_ctl_status_t status;
-
-	status.u64 = cvmx_read_csr(CVMX_FPA_CTL_STATUS);
-	status.s.reset = 1;
-	cvmx_write_csr(CVMX_FPA_CTL_STATUS, status.u64);
-}
-
-/**
- * Get a new block from the FPA
- *
- * @param pool   Pool to get the block from
- * @return Pointer to the block or NULL on failure
- */
-static inline void *cvmx_fpa_alloc(uint64_t pool)
-{
-	uint64_t address;
-
-	/* FPA3 is handled differently */
-	if ((octeon_has_feature(OCTEON_FEATURE_FPA3))) {
-		/* We use the pool as the aura */
-		return cvmx_fpa_alloc_aura(0, (int)pool);
-	}
-
-	for (;;) {
-		address = cvmx_read_csr(CVMX_ADDR_DID(CVMX_FULL_DID(CVMX_OCT_DID_FPA,
-								    pool)));
-		if (cvmx_likely(address)) {
-			return cvmx_phys_to_ptr(address);
-		} else {
-			/* If pointers are available, continuously retry.  */
-			if (cvmx_read_csr(CVMX_FPA_QUEX_AVAILABLE(pool)) > 0)
-				cvmx_wait(50);
-			else
-				return NULL;
-		}
-	}
-}
-
-/**
- * Asynchronously get a new block from the FPA
- *
- * The result of cvmx_fpa_async_alloc() may be retrieved using
- * cvmx_fpa_async_alloc_finish().
- *
- * @param scr_addr Local scratch address to put response in.  This is a byte
- *		   address but must be 8 byte aligned.
- * @param pool      Pool to get the block from
- */
-static inline void cvmx_fpa_async_alloc(uint64_t scr_addr, uint64_t pool)
-{
-	cvmx_fpa_iobdma_data_t data;
-
-	/* Hardware only uses 64 bit aligned locations, so convert from byte
-	 * address to 64-bit index
-	 */
-	data.u64 = 0ull;
-	if (octeon_has_feature(OCTEON_FEATURE_FPA3)) {
-		data.cn78xx.scraddr = scr_addr >> 3;
-		data.cn78xx.len = 1;
-		data.cn78xx.did = 0x29;
-		data.cn78xx.node = cvmx_get_node_num();
-		data.cn78xx.aura = pool;
-	} else {
-		data.s.scraddr = scr_addr >> 3;
-		data.s.len = 1;
-		data.s.did = CVMX_FULL_DID(CVMX_OCT_DID_FPA, pool);
-		data.s.addr = 0;
-	}
-	cvmx_scratch_write64(scr_addr, 0ull);
-	CVMX_SYNCW;
-	cvmx_send_single(data.u64);
-}
-
-/**
- * Retrieve the result of cvmx_fpa_async_alloc
- *
- * @param scr_addr The Local scratch address.  Must be the same value
- * passed to cvmx_fpa_async_alloc().
- *
- * @param pool Pool the block came from.  Must be the same value
- * passed to cvmx_fpa_async_alloc.
- *
- * @return Pointer to the block or NULL on failure
- */
-static inline void *cvmx_fpa_async_alloc_finish(uint64_t scr_addr, uint64_t pool)
-{
-	uint64_t address;
-
-	CVMX_SYNCIOBDMA;
-
-	address = cvmx_scratch_read64(scr_addr);
-	if (cvmx_likely(address))
-		return cvmx_phys_to_ptr(address);
-	else
-		return cvmx_fpa_alloc(pool);
-}
-
-/**
- * Free a block allocated with a FPA pool.
- * Does NOT provide memory ordering in cases where the memory block was
- * modified by the core.
- *
- * @param ptr    Block to free
- * @param pool   Pool to put it in
- * @param num_cache_lines
- *               Cache lines to invalidate
- */
-static inline void cvmx_fpa_free_nosync(void *ptr, uint64_t pool,
-					uint64_t num_cache_lines)
-{
-	cvmx_addr_t newptr;
-	newptr.u64 = cvmx_ptr_to_phys(ptr);
-	newptr.sfilldidspace.didspace =
-		CVMX_ADDR_DIDSPACE(CVMX_FULL_DID(CVMX_OCT_DID_FPA, pool));
-	/* Prevent GCC from reordering around free */
-	asm volatile ("":::"memory");
-	/* value written is number of cache lines not written back */
-	cvmx_write_io(newptr.u64, num_cache_lines);
-}
-
-/**
- * Free a block allocated with a FPA pool.  Provides required memory
- * ordering in cases where memory block was modified by core.
- *
- * @param ptr    Block to free
- * @param pool   Pool to put it in
- * @param num_cache_lines
- *               Cache lines to invalidate
- */
-static inline void cvmx_fpa_free(void *ptr, uint64_t pool,
-				 uint64_t num_cache_lines)
-{
-	cvmx_addr_t newptr;
-
-	/* FPA3 is handled differently */
-	if ((octeon_has_feature(OCTEON_FEATURE_FPA3))) {
-		/* We use the pool as the aura, assume node=0 */
-		cvmx_fpa_free_aura(ptr, 0, (int)pool, num_cache_lines);
-		return;
-	}
-
-	newptr.u64 = cvmx_ptr_to_phys(ptr);
-	newptr.sfilldidspace.didspace =
-		CVMX_ADDR_DIDSPACE(CVMX_FULL_DID(CVMX_OCT_DID_FPA, pool));
-	/* Make sure that any previous writes to memory go out before we free
-	 * this buffer.  This also serves as a barrier to prevent GCC from
-	 * reordering operations to after the free.
-	 */
-	CVMX_SYNCWS;
-	/* value written is number of cache lines not written back */
-	cvmx_write_io(newptr.u64, num_cache_lines);
-}
-
-
-static inline void __cvmx_fpa_aura_cfg(int node, int aura, int pool,
-				       int buffers_cnt, int ptr_dis)
-{
-       cvmx_fpa_aurax_cfg_t aura_cfg;
-       uint64_t pool64 = pool;
-
-       aura_cfg.u64 = cvmx_read_csr_node(node, CVMX_FPA_AURAX_CFG(aura));
-       aura_cfg.s.ptr_dis = ptr_dis;
-        /* Configure CVMX_FPA_AURAX_CNT_LEVELS, CVMX_FPA_AURAX_POOL_LEVELS  */
-       cvmx_write_csr_node(node, CVMX_FPA_AURAX_CFG(aura), aura_cfg.u64);
-       cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT(aura), buffers_cnt);
-       cvmx_write_csr_node(node, CVMX_FPA_AURAX_POOL(aura), pool64);
-
-       /* TODO : config back pressure, RED */
-}
-
-/**
- * This function sets up QOS related parameter for specified Aura.
- *
- * @param node       node number.
- * @param aura       aura to configure.
- * @param ena_red       enable RED based on [DROP] and [PASS] levels
-                        1: enable 0:disable
- * @param pass_thresh   pass threshold for RED.
- * @param drop_thresh   drop threshold for RED
- * @param ena_bp        enable backpressure based on [BP] level.
-                        1:enable 0:disable
- * @param bp_thresh     backpressure threshold.
- *
- */
-static inline void cvmx_fpa_setup_aura_qos(int node, int aura, bool ena_red,
-					   uint64_t pass_thresh,
-					   uint64_t drop_thresh,
-					   bool ena_bp,uint64_t bp_thresh)
-{
-	uint64_t shift=0;
-	uint64_t shift_thresh;
-	cvmx_fpa_aurax_cnt_levels_t aura_level;
-
-	shift_thresh = bp_thresh > drop_thresh ? bp_thresh:drop_thresh;
-
-	while ( (shift_thresh & (uint64_t)(~(0xff)))) {
-		shift_thresh = shift_thresh >> 1;
-		shift++;
-	};
-
-	aura_level.u64 = cvmx_read_csr_node(node,CVMX_FPA_AURAX_CNT_LEVELS(aura));
-	aura_level.s.pass = pass_thresh >> shift;
-	aura_level.s.drop = drop_thresh >> shift;
-	aura_level.s.bp = bp_thresh >> shift;
-	aura_level.s.shift = shift;
-	aura_level.s.red_ena = ena_red;
-	aura_level.s.bp_ena = ena_bp;
-	cvmx_write_csr_node(node,CVMX_FPA_AURAX_CNT_LEVELS(aura),aura_level.u64);
-}
-
-/**
- * Setup a FPA pool to control a new block of memory.
- * This can only be called once per pool. Make sure proper
- * locking enforces this.
- *
- * @param pool       Pool to initialize
- *                   0 <= pool < 8
- * @param name       Constant character string to name this pool.
- *                   String is not copied.
- * @param buffer     Pointer to the block of memory to use. This must be
- *                   accessable by all processors and external hardware.
- * @param block_size Size for each block controlled by the FPA
- * @param num_blocks Number of blocks
- *
- * @return 0 on Success,
- *         -1 on failure
- */
-extern int cvmx_fpa_setup_pool(uint64_t pool, const char *name, void *buffer,
-			       uint64_t block_size, uint64_t num_blocks);
-
-/**
- * Shutdown a Memory pool and validate that it had all of
- * the buffers originally placed in it. This should only be
- * called by one processor after all hardware has finished
- * using the pool. Most like you will want to have called
- * cvmx_helper_shutdown_packet_io_global() before this
- * function to make sure all FPA buffers are out of the packet
- * IO hardware.
- *
- * @param pool   Pool to shutdown
- *
- * @return Zero on success
- *         - Positive is count of missing buffers
- *         - Negative is too many buffers or corrupted pointers
- */
-extern uint64_t cvmx_fpa_shutdown_pool(uint64_t pool);
-
-/**
- * Get the size of blocks controlled by the pool
- * This is resolved to a constant at compile time.
- *
- * @param pool   Pool to access
- * @return Size of the block in bytes
- */
-extern uint64_t cvmx_fpa_get_block_size(uint64_t pool);
-
-/**
- * Initialize FPA block global configuration.
- */
-int cvmx_fpa_global_initialize(void);
-
-/**
- * Initialize global configuration for FPA block for specified node.
- */
-int cvmx_fpa_global_init_node(int node);
-
-/**
- * Allocate or reserve  the specified fpa pool.
- *
- * @param pool	  FPA pool to allocate/reserve. If -1 it
- *                finds an empty pool to allocate.
- * @return        Alloctaed pool number or -1 if fails to allocate
-                  the pool
- */
-int cvmx_fpa_alloc_pool(int pool);
-
-/**
- * Free the specified fpa pool.
- * @param pool	   Pool to free
- * @return         0 for success -1 failure
- */
-int cvmx_fpa_release_pool(int pool);
-
-/**
- * This call will initialise the stack of the specified pool. Only the stack
- * memory which is the memory that holds the buffer pointers is allocated.
- * For now assume that natural alignment is used. When using natural alignment
- * the hardware needs to be initialised with the buffer size and hence it is
- * specified at the time of pool initialisation. The value will be buffered and
- * used later during cvmx_fpa_aura_init() to allocate buffers.
- * Before invoking this call the application should already have ownership of
- * the pool, the ownership is obtained when pool is one of the values in the
- * pools_allocated array after invocation of cvmx_allocate_fpa_pool()
- * The linux device driver chooses to not use this call as it would initialise
- * the FPA pool with kernel memory as opposed to using bootmem.
- * @param node     - specifies the node of FPA pool.
- * @parma pool     - Specifies the FPA pool number.
- * @param name     - Specifies the FPA pool name.
- * @param mem_node - specifies the node from which the memory for the stack
- *                   is allocated.
- * @param max_buffer_cnt - specifies the maximum buffers that FPA pool can hold.
- * @parm  align          - specifies the alignment type.
- * @param buffer_sz      - Only when the alignment is natural this field is used
- *                         to specify the size of each buffer in the FPA .
- *
- */
-int cvmx_fpa_pool_stack_init(int node, int pool, const char *name, int mem_node,
-			     int max_buffer_cnt, enum fpa_pool_alignment align,
-			     int buffer_sz);
-
-/**
- * This call will allocated buffers_cnt number of buffers from  the bootmemory
- * of bootmem_node and populate the aura specified with the allocated buffers.
- * The size of the buffers is obtained from the buffer_sz used to initialise the
- * stack of the FPA pool associated with the aura. This also means that the aura
- * has already been mapped to an FPA pool. More parameters is possible to
- * specify RED and buffer level parameters. This is another that would not be
- * used by the Linux device driver, the driver would populate the buffers
- * in the pool using it's own allocation mechanism.
- * @param node     - specifies the node of aura to be initialized
- * @parma aura     - specifies the aura to be initalized.
- * @param name     - specifies the name of aura to be initalized.
- * @param mem_node - specifies the node from which the memory for the buffers
- *                   is allocated.
- * @param buffers  - Block of memory to use for the aura buffers. If NULL,
- *                   aura memory is allocated.
- * @param ptr_dis - Need to look into this more but is on the lines of of
- *		    whether the hardware checks double frees.
- */
-int cvmx_fpa_aura_init(int node, int aura, const char *name, int mem_node,
-		       void *buffers, int buffers_cnt, int ptr_dis);
-int cvmx_fpa_pool_init(int pool_id, int num_blocks, int block_size,
-		       void *buffer, const char *name);
-int cvmx_fpa_config_red_params(int node, int qos_avg_en, int red_lvl_dly,
-			       int avg_dly);
-
-#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
-/**
- * This will map auras specified in the auras_index[] array to specified
- * FPA pool_index.
- * The array is assumed to contain count number of entries.
- * @param count is the number of entries in the auras_index array.
- * @pool_index is the index of the fpa pool.
- * @return 0 on success
- */
-int cvmx_fpa_assign_auras(int node, int auras_index[], int count,
-			  int pool_index);
-
-static inline int cvmx_fpa_assign_aura(int node, int aura, int pool_index)
-{
-	int auras[1];
-
-	auras[0] = aura;
-	return cvmx_fpa_assign_auras(node, auras, 1, pool_index);
-}
-#endif
-
-/**
- * This will allocate count number of FPA pools on the specified node to the
- * calling application. These pools will be for exclusive use of the application
- * until they are freed.
- * @param pools_allocated is an array of length count allocated by
- *			  the application before invoking the
- *			  cvmx_allocate_fpa_pool call.  On return it will
- *			  contain the index numbers of the pools allocated.
- * @return 0 on success and -1 on failure.
- */
-int cvmx_fpa_allocate_fpa_pools(int node, int pools_allocated[], int count);
-
-int cvmx_fpa_reserve_compat(int node);
-
-#ifdef	__cplusplus
-/* *INDENT-OFF* */
-}
-/* *INDENT-ON* */
-#endif
-
-#endif /*  __CVM_FPA_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-fpa1.h b/arch/mips/include/asm/octeon/cvmx-fpa1.h
new file mode 100644
index 0000000..ce06039
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-fpa1.h
@@ -0,0 +1,177 @@
+/***********************license start***************
+ * Copyright (c) 2003-2010  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Interface to the hardware Free Pool Allocator on the 78xx.
+ *
+ * <hr>$Revision: 94697 $<hr>
+ *
+ */
+
+#ifndef __CVMX_FPA1_H__
+#define __CVMX_FPA1_H__
+
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include <asm/octeon/cvmx-fpa-defs.h>
+#endif
+
+#define CVMX_FPA1_NUM_POOLS      8
+#define CVMX_FPA1_POOL_NAME_LEN  16
+#define CVMX_FPA1_MIN_BLOCK_SIZE 128
+#define CVMX_FPA1_ALIGNMENT      128
+
+/**
+ * Structure to store FPA pool configuration parameters.
+ */
+typedef struct cvmx_fpa_pool_config
+{
+	int64_t pool_num;
+	uint64_t buffer_size;
+	uint64_t buffer_count;
+}cvmx_fpa_pool_config_t;
+
+/**
+ * Structure describing the current state of a FPA pool.
+ */
+typedef struct {
+	char name[CVMX_FPA1_POOL_NAME_LEN];	/** FPA Pool Name */
+	uint64_t size;				/** Size of each block */
+	void *base;				/**
+						 * The base memory address of
+						 * whole block
+						 */
+	uint64_t stack_base;			/**
+						 * Base address of stack of FPA
+						 * pool
+						 */
+	uint64_t starting_element_count;	/**
+						 * The number of elements in the
+						 * pool at creation
+						 */
+	uint64_t max_buffer_cnt;		/**
+						 * Maximum amount of buffers
+						 * that can be held in this
+						 * FPA pool
+						 */
+} cvmx_fpa1_pool_info_t;
+
+extern CVMX_SHARED cvmx_fpa1_pool_info_t
+cvmx_fpa1_pool_info[CVMX_FPA1_NUM_POOLS];
+
+/**
+ * Allocate or reserve  the specified fpa pool.
+ *
+ * @param pool	  FPA pool to allocate/reserve. If -1 it
+ *                finds an empty pool to allocate.
+ * @return        Alloctaed pool number or -1 if fails to allocate
+                  the pool
+ */
+int cvmx_fpa_alloc_pool(int pool);
+
+/**
+ * Free the specified fpa pool.
+ * @param pool	   Pool to free
+ * @return         0 for success -1 failure
+ */
+int cvmx_fpa_release_pool(int pool);
+
+int cvmx_fpa1_pool_init(int pool_id, int num_blocks, int block_size,
+		       void *buffer, const char *name);
+
+static inline void cvmx_fpa1_free(void *ptr, uint64_t pool,
+	uint64_t num_cache_lines)
+{
+	cvmx_addr_t newptr;
+
+	newptr.u64 = cvmx_ptr_to_phys(ptr);
+	newptr.sfilldidspace.didspace =
+		CVMX_ADDR_DIDSPACE(CVMX_FULL_DID(CVMX_OCT_DID_FPA, pool));
+	/* Make sure that any previous writes to memory go out before we free
+	 * this buffer.  This also serves as a barrier to prevent GCC from
+	 * reordering operations to after the free.
+	 */
+	CVMX_SYNCWS;
+	/* value written is number of cache lines not written back */
+	cvmx_write_io(newptr.u64, num_cache_lines);
+}
+
+/**
+ * Enable the FPA for use. Must be performed after any CSR
+ * configuration but before any other FPA functions.
+ */
+static inline void cvmx_fpa1_enable(void)
+{
+	cvmx_fpa_ctl_status_t status;
+
+	status.u64 = cvmx_read_csr(CVMX_FPA_CTL_STATUS);
+	if (status.s.enb) {
+		/*
+	 	* CN68XXP1 should not reset the FPA (doing so may break
+		* the SSO, so we may end up enabling it more than once.
+		* Just return and don't spew messages.
+	 	*/
+		return;
+	}
+
+	status.u64 = 0;
+	status.s.enb = 1;
+	cvmx_write_csr(CVMX_FPA_CTL_STATUS, status.u64);
+}
+
+static inline void *cvmx_fpa1_alloc(uint64_t pool)
+{
+	uint64_t address;
+
+	for (;;) {
+		address = cvmx_read_csr(CVMX_ADDR_DID(CVMX_FULL_DID(CVMX_OCT_DID_FPA,
+					pool)));
+		if (cvmx_likely(address)) {
+			return cvmx_phys_to_ptr(address);
+		} else {
+			if (cvmx_read_csr(CVMX_FPA_QUEX_AVAILABLE(pool)) > 0)
+				cvmx_wait(50);
+			else
+				return NULL;
+		}
+	}
+}
+
+#endif
diff --git a/arch/mips/include/asm/octeon/cvmx-fpa3.h b/arch/mips/include/asm/octeon/cvmx-fpa3.h
new file mode 100644
index 0000000..c59cbba
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-fpa3.h
@@ -0,0 +1,427 @@
+/***********************license start***************
+ * Copyright (c) 2003-2010  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Interface to the hardware Free Pool Allocator on the 78xx.
+ *
+ * <hr>$Revision: 94697 $<hr>
+ *
+ */
+
+#ifndef __CVMX_FPA3_H__
+#define __CVMX_FPA3_H__
+
+#include "cvmx.h"
+
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include "cvmx-fpa-defs.h"
+#endif
+
+enum fpa_pool_alignment {
+	FPA_NATURAL_ALIGNMENT,
+	FPA_OFFSET_ALIGNMENT,
+	FPA_OPAQUE_ALIGNMENT
+};
+
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+extern "C" {
+/* *INDENT-ON* */
+#endif
+
+#define CVMX_FPA3_AURA_NUM       1024
+#define CVMX_FPA3_MIN_BLOCK_SIZE 128
+#define CVMX_FPA3_ALIGNMENT      128
+#define CVMX_FPA3_POOL_NAME_LEN  16
+#define CVMX_FPA3_NUM_POOLS	64
+#define CVMX_FPA3_AURA_NAME_LEN  16
+
+/**
+ * Struct describing load allocate operation addresses for FPA pool.
+ */
+typedef union {
+	uint64_t u64;
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t reserved1:15;
+		uint64_t io:1;		/** Indicates I/O space */
+		uint64_t did:8;		/**
+					 * the ID of the device on the
+					 * non-coherent bus
+					 */
+		uint64_t node:4;	/** OCI node number */
+		uint64_t red:1;		/** Perform RED on allocation */
+		uint64_t reserved2:9;   /** Reserved */
+		uint64_t aura:10;	/** Aura number */
+		uint64_t reserved3:16;	/** Reserved */
+#else
+		uint64_t reserved3:16;	/** Reserved */
+		uint64_t aura:10;	/** Aura number */
+		uint64_t reserved2:9;   /** Reserved */
+		uint64_t red:1;		/** Perform RED on allocation */
+		uint64_t node:4;	/** OCI node number */
+		uint64_t did:8;		/**
+					 * the ID of the device on the
+					 * non-coherent bus
+					 */
+		uint64_t io:1;		/** Indicates I/O space */
+		uint64_t reserved1:15;
+
+#endif
+	} cn78xx;
+} cvmx_fpa_load_data_t;
+
+/**
+ * Struct describing store free operation addresses from FPA pool.
+ */
+typedef union {
+	uint64_t u64;
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t reserved1:15;
+		uint64_t io:1;		/** Indicates I/O space */
+		uint64_t did:8;		/**
+					 * the ID of the device on the
+					 * non-coherent bus
+					 */
+		uint64_t node:4;	/** OCI node number */
+		uint64_t reserved2:10;  /** Reserved */
+		uint64_t aura:10;	/** Aura number */
+		uint64_t fabs:1;	/** free absolute */
+		uint64_t reserved3:3;	/** Reserved */
+		uint64_t dwb_count:9;	/**
+					 * Number of cache lines for which the
+					 * hardware should try to execute
+					 * 'don't-write-back' commands.
+					 */
+		uint64_t reserved4:3;	/** Reserved */
+#else
+		uint64_t reserved4:3;	/** Reserved */
+		uint64_t dwb_count:9;	/**
+					 * Number of cache lines for which the
+					 * hardware should try to execute
+					 * 'don't-write-back' commands.
+					 */
+		uint64_t reserved3:3;	/** Reserved */
+		uint64_t fabs:1;	/** free absolute */
+		uint64_t aura:10;	/** Aura number */
+		uint64_t reserved2:10;  /** Reserved */
+		uint64_t node:4;	/** OCI node number */
+		uint64_t did:8;		/**
+					 * the ID of the device on the
+					 * non-coherent bus
+					 */
+		uint64_t io:1;		/** Indicates I/O space */
+		uint64_t reserved1:15;
+#endif
+	} cn78xx;
+} cvmx_fpa_store_addr_t;
+/**
+ * Structure describing the current state of a FPA pool.
+ */
+typedef struct {
+	char name[CVMX_FPA3_POOL_NAME_LEN];	/** FPA Pool Name */
+	uint64_t size;				/** Size of each block */
+	void *base;				/**
+						 * The base memory address of
+						 * whole block
+						 */
+	uint64_t stack_base;			/**
+						 * Base address of stack of FPA
+						 * pool
+						 */
+	uint64_t starting_element_count;	/**
+						 * The number of elements in the
+						 * pool at creation
+						 */
+	uint64_t max_buffer_cnt;		/**
+						 * Maximum amount of buffers
+						 * that can be held in this
+						 * FPA pool
+						 */
+} cvmx_fpa3_pool_info_t;
+
+/**
+ * Structure which contains information on auras.
+ */
+typedef struct {
+	char name[CVMX_FPA3_AURA_NAME_LEN];
+	int pool_num;
+	void *base; /** Base of aura if allocated separately */
+	uint64_t size;
+} cvmx_fpa3_aura_info_t;
+
+/**
+ * Current state of all the pools. Use access functions
+ * instead of using it directly.
+ */
+extern CVMX_SHARED cvmx_fpa3_pool_info_t
+cvmx_fpa3_pool_info[CVMX_MAX_NODES][CVMX_FPA3_NUM_POOLS];
+extern CVMX_SHARED cvmx_fpa3_aura_info_t
+cvmx_fpa3_aura_info[CVMX_MAX_NODES][CVMX_FPA3_AURA_NUM];
+
+int cvmx_fpa3_allocate_auras(int node, int auras_allocated[], int count);
+int cvmx_fpa3_free_auras(int node, int *pools_allocated, int count);
+
+/**
+ * Get a new block from the FPA Aura
+ *
+ * @param node  - node number
+ * @param aura  - aura to get the block from
+ * @return pointer to the block or NULL on failure
+ */
+static inline void *cvmx_fpa_alloc_aura(int node, int aura)
+{
+	uint64_t address;
+	cvmx_fpa_load_data_t load_addr;
+
+	load_addr.u64 = 0;
+	load_addr.cn78xx.io = 1;
+	load_addr.cn78xx.did = 0x29;    /* Device ID. Indicates FPA. */
+	load_addr.cn78xx.node = node;   /* OCI node number */
+	load_addr.cn78xx.red = 0;       /* Perform RED on allocation.
+					 * FIXME to use config option
+					 */
+	load_addr.cn78xx.aura = aura;   /* Aura number */
+
+	address = cvmx_read_csr((CVMX_ADD_SEG(CVMX_MIPS_SPACE_XKPHYS,
+					      load_addr.u64)));
+	if (!address)
+		return NULL;
+	return cvmx_phys_to_ptr(address);
+}
+
+/**
+ * Free a pointer back to the aura.
+ *
+ * @param node   node number
+ * @param aura   aura number
+ * @param ptr    physical address of block to free.
+ * @param num_cache_lines Cache lines to invalidate
+ */
+static inline void cvmx_fpa_free_aura(void *ptr, uint64_t node, int aura,
+				      uint64_t num_cache_lines)
+{
+	cvmx_fpa_store_addr_t newptr;
+	cvmx_addr_t newdata;
+
+	newdata.u64 = cvmx_ptr_to_phys(ptr);
+
+	newptr.u64 = 2ull<<62;
+	newptr.cn78xx.io = 1;
+	newptr.cn78xx.did = 0x29;    /* Device id, indicates FPA */
+	newptr.cn78xx.node = node;   /* OCI node number. */
+	newptr.cn78xx.aura = aura;   /* Aura number */
+	newptr.cn78xx.fabs = 0;	/* Free absolute. FIXME to use config option */
+	newptr.cn78xx.dwb_count = num_cache_lines;
+
+	/*cvmx_dprintf("aura = %d ptr_to_phys(ptr) = 0x%llx newptr.u64 = 0x%llx"
+		     " ptr = %p \n", ptr, aura, (ULL) newptr.u64
+		     (ULL) cvmx_ptr_to_phys(ptr)); */
+	/* Make sure that any previous writes to memory go out before we free
+	   this buffer. This also serves as a barrier to prevent GCC from
+	   reordering operations to after the free. */
+	CVMX_SYNCWS;
+        cvmx_write_io(newptr.u64, newdata.u64);
+}
+
+static inline void __cvmx_fpa_aura_cfg(int node, int aura, int pool,
+				       int buffers_cnt, int ptr_dis)
+{
+       cvmx_fpa_aurax_cfg_t aura_cfg;
+       uint64_t pool64 = pool;
+
+       aura_cfg.u64 = cvmx_read_csr_node(node, CVMX_FPA_AURAX_CFG(aura));
+       aura_cfg.s.ptr_dis = ptr_dis;
+        /* Configure CVMX_FPA_AURAX_CNT_LEVELS, CVMX_FPA_AURAX_POOL_LEVELS  */
+       cvmx_write_csr_node(node, CVMX_FPA_AURAX_CFG(aura), aura_cfg.u64);
+       cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT(aura), buffers_cnt);
+       cvmx_write_csr_node(node, CVMX_FPA_AURAX_POOL(aura), pool64);
+
+       /* TODO : config back pressure, RED */
+}
+
+/**
+ * This function sets up QOS related parameter for specified Aura.
+ *
+ * @param node       node number.
+ * @param aura       aura to configure.
+ * @param ena_red       enable RED based on [DROP] and [PASS] levels
+                        1: enable 0:disable
+ * @param pass_thresh   pass threshold for RED.
+ * @param drop_thresh   drop threshold for RED
+ * @param ena_bp        enable backpressure based on [BP] level.
+                        1:enable 0:disable
+ * @param bp_thresh     backpressure threshold.
+ *
+ */
+static inline void cvmx_fpa_setup_aura_qos(int node, int aura, bool ena_red,
+					   uint64_t pass_thresh,
+					   uint64_t drop_thresh,
+					   bool ena_bp,uint64_t bp_thresh)
+{
+	uint64_t shift=0;
+	uint64_t shift_thresh;
+	cvmx_fpa_aurax_cnt_levels_t aura_level;
+
+	shift_thresh = bp_thresh > drop_thresh ? bp_thresh:drop_thresh;
+
+	while ( (shift_thresh & (uint64_t)(~(0xff)))) {
+		shift_thresh = shift_thresh >> 1;
+		shift++;
+	};
+
+	aura_level.u64 = cvmx_read_csr_node(node,CVMX_FPA_AURAX_CNT_LEVELS(aura));
+	aura_level.s.pass = pass_thresh >> shift;
+	aura_level.s.drop = drop_thresh >> shift;
+	aura_level.s.bp = bp_thresh >> shift;
+	aura_level.s.shift = shift;
+	aura_level.s.red_ena = ena_red;
+	aura_level.s.bp_ena = ena_bp;
+	cvmx_write_csr_node(node,CVMX_FPA_AURAX_CNT_LEVELS(aura),aura_level.u64);
+}
+
+/**
+ * This call will initialise the stack of the specified pool. Only the stack
+ * memory which is the memory that holds the buffer pointers is allocated.
+ * For now assume that natural alignment is used. When using natural alignment
+ * the hardware needs to be initialised with the buffer size and hence it is
+ * specified at the time of pool initialisation. The value will be buffered and
+ * used later during cvmx_fpa_aura_init() to allocate buffers.
+ * Before invoking this call the application should already have ownership of
+ * the pool, the ownership is obtained when pool is one of the values in the
+ * pools_allocated array after invocation of cvmx_allocate_fpa_pool()
+ * The linux device driver chooses to not use this call as it would initialise
+ * the FPA pool with kernel memory as opposed to using bootmem.
+ * @param node     - specifies the node of FPA pool.
+ * @parma pool     - Specifies the FPA pool number.
+ * @param name     - Specifies the FPA pool name.
+ * @param mem_node - specifies the node from which the memory for the stack
+ *                   is allocated.
+ * @param max_buffer_cnt - specifies the maximum buffers that FPA pool can hold.
+ * @parm  align          - specifies the alignment type.
+ * @param buffer_sz      - Only when the alignment is natural this field is used
+ *                         to specify the size of each buffer in the FPA .
+ *
+ */
+int cvmx_fpa3_pool_stack_init(int node, int pool, const char *name, int mem_node,
+			     int max_buffer_cnt, enum fpa_pool_alignment align,
+			     int buffer_sz);
+
+/**
+ * This call will allocate buffers_cnt number of buffers from  the bootmemory
+ * of bootmem_node and populate the aura specified with the allocated buffers.
+ * The size of the buffers is obtained from the buffer_sz used to initialise the
+ * stack of the FPA pool associated with the aura. This also means that the aura
+ * has already been mapped to an FPA pool. More parameters is possible to
+ * specify RED and buffer level parameters. This is another that would not be
+ * used by the Linux device driver, the driver would populate the buffers
+ * in the pool using it's own allocation mechanism.
+ * @param node     - specifies the node of aura to be initialized
+ * @parma aura     - specifies the aura to be initalized.
+ * @param name     - specifies the name of aura to be initalized.
+ * @param mem_node - specifies the node from which the memory for the buffers
+ *                   is allocated.
+ * @param buffers  - Block of memory to use for the aura buffers. If NULL,
+ *                   aura memory is allocated.
+ * @param ptr_dis - Need to look into this more but is on the lines of of
+ *		    whether the hardware checks double frees.
+ */
+int cvmx_fpa3_aura_init(int node, int aura, const char *name, int mem_node,
+		       void *buffers, int buffers_cnt, int ptr_dis);
+
+static inline int cvmx_fpa3_config_red_params(int node, int qos_avg_en, int red_lvl_dly,
+			       int avg_dly)
+{
+	cvmx_fpa_gen_cfg_t fpa_cfg;
+	cvmx_fpa_red_delay_t red_delay;
+
+	fpa_cfg.u64 = cvmx_read_csr_node(node,CVMX_FPA_GEN_CFG);
+	fpa_cfg.s.avg_en = qos_avg_en;
+	fpa_cfg.s.lvl_dly = red_lvl_dly;
+	cvmx_write_csr_node(node,CVMX_FPA_GEN_CFG,fpa_cfg.u64);
+
+	red_delay.u64 = cvmx_read_csr_node(node,CVMX_FPA_RED_DELAY);
+	red_delay.s.avg_dly = avg_dly;
+	cvmx_write_csr_node(node,CVMX_FPA_RED_DELAY,red_delay.u64);
+	return 0;
+}
+
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+
+/**
+ * Gets the buffer size of the specified AURA,
+ * which is a 12-bit quantity, with the upper 2 bits
+ * representing the OCI node number, a.k.a. GAURA.
+ *
+ * @param gaura Global aura number
+ * @return Returns size of the buffers in the specified aura.
+ */
+static inline unsigned cvmx_fpa_get_aura_buf_size(uint16_t gaura)
+{
+	unsigned node = gaura >> 10;
+	unsigned pool;
+
+	pool = cvmx_fpa3_aura_info[node][gaura & ((1<<10)-1)].pool_num;
+	return (cvmx_fpa3_pool_info[node][pool].size);
+}
+
+/**
+ * This will map auras specified in the auras_index[] array to specified
+ * FPA pool_index.
+ * The array is assumed to contain count number of entries.
+ * @param count is the number of entries in the auras_index array.
+ * @pool_index is the index of the fpa pool.
+ * @return 0 on success
+ */
+int cvmx_fpa3_assign_auras(int node, int auras_index[], int count,
+			  int pool_index);
+
+static inline int cvmx_fpa3_assign_aura(int node, int aura, int pool_index)
+{
+	int auras[1];
+
+	auras[0] = aura;
+	return cvmx_fpa3_assign_auras(node, auras, 1, pool_index);
+}
+#endif
+
+#endif
diff --git a/arch/mips/include/asm/octeon/cvmx-helper-fpa.h b/arch/mips/include/asm/octeon/cvmx-helper-fpa.h
index 91e915a..76a5f60 100644
--- a/arch/mips/include/asm/octeon/cvmx-helper-fpa.h
+++ b/arch/mips/include/asm/octeon/cvmx-helper-fpa.h
@@ -74,22 +74,6 @@ extern int cvmx_helper_initialize_fpa(int packet_buffers,
 				      int work_queue_entries, int pko_buffers,
 				      int tim_buffers, int dfa_buffers);
 
-/**
- * @INTERNAL
- * Setup a FPA3 pool and aura to control a new block of memory. This
- * function is called by legacy code.
- *
- * @param pool       Pool to initialize
- * @param name       Pool name
- * @param buffers    Pointer to block of memory to use for the aura buffers
- * @param block_size Size of the aura buffers
- * @param num_blocks Number of aura buffers
- *
- * @return 0 on Success, -1 on failure
- */
-/*int __cvmx_fpa3_setup_pool(uint64_t pool, const char *name, void *buffer,
-			   uint64_t block_size, uint64_t num_blocks);*/
-
 extern int __cvmx_helper_initialize_fpa_pool(int pool, uint64_t buffer_size,
 					     uint64_t buffers,
 					     const char *name);
@@ -97,18 +81,61 @@ extern int __cvmx_helper_initialize_fpa_pool(int pool, uint64_t buffer_size,
 extern void cvmx_fpa_show_stats(void);
 
 /**
- * Setup an FPA pool (and/or aura) to control a block of memory.
+ * Function to create a simple 1:1 pool/aura configuration.
+ * Meant to make pool/aura initialization easy for SE apps.
+ *
+ * @param node - configure fpa on this node
+ * @param pool_num - pointer to pool to use, set to -1 to allocate
+ * @param aura_id - pointer to aura to use, set to -1 to allocate
+ * @param block_size - size of aura buffers to use
+ * @param num_blocks - number of buffers to allocate
+ * @param name - name to assign pool
+ * @param buffers - pointer to buffer memory to use, NULL to allocate
  *
- * @param pool_num 		Pointer to pool id to initialize or -1 to allocate
- * @param aura_id		Pointer to aura id to initialize or -1 to allocate (ignored for legacy hardware)
- * @param block_size 	Size of the aura buffers
- * @param num_blocks 	Number of aura buffers
- * @param name			Name to assign fpa pool
- * @param buffers		Pointer to buffer memory to use
+ * @return -1 on error, 0 on success with buffers containing allocated memory if passed NULL
  */
+
 extern int cvmx_helper_fpa_init(int node, int *pool_num, int *aura_id, int block_size,
-						int num_blocks, const char *name, void **buffers);
+				int num_blocks, const char *name, void **buffers);
+
+/**
+ * Function to setup and initialize a pool.
+ *
+ * @param node - configure fpa on this node
+ * @param mem_node - if memory should be allocated from a different node
+ * @param pool_num - pointer to pool to use, or -1 to allocate
+ * @param block_size - size of buffers in pool
+ * @param num_blocks - max number of buffers allowed
+ * @param name - name to register
+ */
 
+extern int cvmx_helper_fpa_init_pool(int node, int mem_node, int *pool_num, int block_size, int num_blocks, const char *name);
+
+/**
+ * Function to add an aura to an existing pool
+ *
+ * @param node - configure fpa on this node
+ * @param pool_num - pool to attach aura to
+ * @param aura_id - pointer to aura to use, set to -1 to allocate
+ * @param block_size - size of buffers to use
+ * @param num_blocks - number of blocks to allocate
+ * @param buffer - pointer to buffer memory to use, NULL to allocate
+ *
+ * @return -1 on error, 0 on success with buffers containing allocated memory if passed NULL
+ */
+
+extern int cvmx_helper_fpa_add_aura_to_pool(int node, int pool_num, int *aura_id, int num_blocks, void **buffer, const char *name);
+
+/**
+ * Function to fill a pre-78xx fpa pool with memory
+ *
+ * @param node - configure fpa on this node
+ * @param mem_node - if memory should be allocated from a different node
+ * @param pool_num - pool to fill
+ * @param num_blocks - number of blocks to add (of pool's block_size)
+ * @param buffer - buffer to add blocks from
+ */
 
+extern int cvmx_helper_fpa_fill_pool(int pool_num, int num_blocks, void **buffer);
 
 #endif /* __CVMX_HELPER_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-helper-ipd.h b/arch/mips/include/asm/octeon/cvmx-helper-ipd.h
new file mode 100644
index 0000000..afb914f
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-helper-ipd.h
@@ -0,0 +1,70 @@
+/***********************license start***************
+ * Copyright (c) 2003-2014  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Helper functions for IPD
+ */
+
+#ifndef __CVMX_HELPER_IPD_H__
+#define __CVMX_HELPER_IPD_H__
+
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include <asm/octeon/cvmx.h>
+#include <asm/octeon/cvmx-ipd.h>
+#else
+#include "cvmx-ipd.h"
+#endif
+
+#ifdef __cplusplus
+/* *INDENT-OFF* */
+extern "C" {
+/* *INDENT-ON* */
+#endif
+void cvmx_helper_ipd_set_wqe_no_ptr_mode(bool mode);
+void cvmx_helper_ipd_pkt_wqe_le_mode(bool mode);
+int __cvmx_helper_ipd_global_setup(void);
+int __cvmx_helper_ipd_setup_interface(int interface);
+#ifdef __cplusplus
+/* *INDENT-OFF* */
+}
+/* *INDENT-ON* */
+#endif
+#endif /* __CVMX_HELPER_PKI_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-helper-pki.h b/arch/mips/include/asm/octeon/cvmx-helper-pki.h
index 2c93981..28716ad 100644
--- a/arch/mips/include/asm/octeon/cvmx-helper-pki.h
+++ b/arch/mips/include/asm/octeon/cvmx-helper-pki.h
@@ -70,12 +70,11 @@ extern struct cvmx_pki_pkind_config pki_dflt_pkind[CVMX_MAX_NODES];
 extern uint64_t pkind_style_map[CVMX_MAX_NODES][CVMX_PKI_NUM_PKIND];
 
 int cvmx_helper_setup_pki_port(int node, int pknd);
-int cvmx_helper_global_setup_pki(int node);
 int cvmx_helper_pki_setup_qpg_table(int node, int num_entries, int port_addr[],
 				    uint64_t aura[], uint64_t sso_grp_ok[], uint64_t sso_grp_bad[]);
 void cvmx_helper_pki_set_fcs_op(int node, int interface, int nports, int has_fcs);
-int __cvmx_helper_port_setup_pki(int node, int pknd);
-int __cvmx_helper_global_setup_pki(int node);
+int __cvmx_helper_pki_port_setup(int node, int ipd_port);
+int __cvmx_helper_pki_global_setup(int node);
 void cvmx_helper_pki_set_dflt_pool(int node, int pool,
 				   int buffer_size, int buffer_count);
 void cvmx_helper_pki_set_dflt_aura(int node, int aura,
@@ -102,8 +101,9 @@ void cvmx_helper_pki_set_dflt_aura_buffer(int node, int buffer_count);
 int cvmx_helper_setup_aura_qos(int node, int aura, bool ena_red, bool ena_drop,
 			       uint64_t pass_thresh, uint64_t drop_thresh,
 			       bool ena_bp, uint64_t bp_thresh);
-int cvmx_helper_map_aura_channel_bpid(int node, int aura_map[], int aura_cnt,
+int cvmx_helper_pki_map_aura_chl_bpid(int node, int aura_map[], int aura_cnt,
 				      int chl_map[], int chl_cnt, int bpid);
+void cvmx_helper_pki_set_dflt_pkind_map(int node, int pkind, int style);
 
 #ifdef __cplusplus
 /* *INDENT-OFF* */
diff --git a/arch/mips/include/asm/octeon/cvmx-helper-pko.h b/arch/mips/include/asm/octeon/cvmx-helper-pko.h
index f7f4c63..eff9cfd 100644
--- a/arch/mips/include/asm/octeon/cvmx-helper-pko.h
+++ b/arch/mips/include/asm/octeon/cvmx-helper-pko.h
@@ -66,47 +66,20 @@ typedef struct {
  */
 extern CVMX_SHARED void (*cvmx_override_pko_queue_priority) (int ipd_port, uint8_t * priorities);
 
-
-extern CVMX_SHARED cvmx_fpa_pool_config_t pko_fpa_config;
-
 /**
  * Gets the fpa pool number of pko pool
  */
-static inline int64_t cvmx_fpa_get_pko_pool(void)
-{
-	return (pko_fpa_config.pool_num);
-}
+int64_t cvmx_fpa_get_pko_pool(void);
 
 /**
  * Gets the buffer size of pko pool
  */
-static inline uint64_t cvmx_fpa_get_pko_pool_block_size(void)
-{
-	return (pko_fpa_config.buffer_size);
-}
+uint64_t cvmx_fpa_get_pko_pool_block_size(void);
 
 /**
  * Gets the buffer size  of pko pool
  */
-static inline uint64_t cvmx_fpa_get_pko_pool_buffer_count(void)
-{
-	return (pko_fpa_config.buffer_count);
-}
-
-/**
- * Sets the internal PKO pool data structure for command queue pool.
- * @param pool	fpa pool number yo use
- * @param buffer_size	buffer size of pool
- * @param buffer_count	number of buufers to allocate to pool
- */
-void cvmx_helper_set_pko_fpa_config(int64_t pool, uint64_t buffer_size,
-				    uint64_t buffer_count);
-
-/**
- * Gets up the pko FPA pool data from internal data structure
- * @param pko_pool pointer to the fpa data structure to copy data
- */
-void cvmx_pko_get_cmd_que_pool_config(cvmx_fpa_pool_config_t *pko_pool);
+uint64_t cvmx_fpa_get_pko_pool_buffer_count(void);
 
 
 int cvmx_helper_pko_init(void);
diff --git a/arch/mips/include/asm/octeon/cvmx-helper-util.h b/arch/mips/include/asm/octeon/cvmx-helper-util.h
index 9dfd2c5..6476ed4 100644
--- a/arch/mips/include/asm/octeon/cvmx-helper-util.h
+++ b/arch/mips/include/asm/octeon/cvmx-helper-util.h
@@ -42,7 +42,7 @@
  *
  * Small helper utilities.
  *
- * <hr>$Revision: 94787 $<hr>
+ * <hr>$Revision: 95258 $<hr>
  */
 
 #ifndef __CVMX_HELPER_UTIL_H__
@@ -317,8 +317,12 @@ static inline void cvmx_helper_free_packet_data(cvmx_wqe_t *work)
 			next_buffer_ptr = *(uint64_t *)
 				cvmx_phys_to_ptr(buffer_ptr.s.addr - 8);
 			/* FPA pool comes from buf_ptr itself */
-			cvmx_fpa_free(cvmx_phys_to_ptr(start_of_buffer),
-				buffer_ptr.s.pool, ncl);
+			if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE))
+				/* FIXME:  Which node is it? */
+				cvmx_fpa_free_aura(cvmx_phys_to_ptr(start_of_buffer), 0, (int)buffer_ptr.s.pool, ncl);
+			else
+				cvmx_fpa1_free(cvmx_phys_to_ptr(start_of_buffer),
+					      buffer_ptr.s.pool, ncl);
 		}
 		buffer_ptr.u64 = next_buffer_ptr;
 	}
@@ -370,7 +374,7 @@ extern int cvmx_helper_get_bpid(int interface, int port);
  */
 extern int __cvmx_helper_post_init_interfaces(void);
 extern void __cvmx_helper_shutdown_interfaces(void);
-
+extern int cvmx_helper_setup_red(int pass_thresh, int drop_thresh);
 extern void cvmx_helper_show_stats(int port);
 
 #endif /* __CVMX_HELPER_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-helper.h b/arch/mips/include/asm/octeon/cvmx-helper.h
index 90c3029..dce77b0 100644
--- a/arch/mips/include/asm/octeon/cvmx-helper.h
+++ b/arch/mips/include/asm/octeon/cvmx-helper.h
@@ -42,7 +42,7 @@
  *
  * Helper functions for common, but complicated tasks.
  *
- * <hr>$Revision: 94800 $<hr>
+ * <hr>$Revision: 95258 $<hr>
  */
 
 #ifndef __CVMX_HELPER_H__
@@ -52,7 +52,6 @@
 #include <asm/octeon/cvmx.h>
 #endif
 
-#include "cvmx-fpa.h"
 #include "cvmx-wqe.h"
 
 #ifdef  __cplusplus
diff --git a/arch/mips/include/asm/octeon/cvmx-hwfau.h b/arch/mips/include/asm/octeon/cvmx-hwfau.h
new file mode 100644
index 0000000..cb2a068
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-hwfau.h
@@ -0,0 +1,648 @@
+/***********************license start***************
+ * Copyright (c) 2003-2010  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Interface to the hardware Fetch and Add Unit.
+ *
+ */
+
+#ifndef __CVMX_HWFAU_H__
+#define __CVMX_HWFAU_H__
+
+typedef int cvmx_fau_reg64_t;
+typedef int cvmx_fau_reg32_t;
+typedef int cvmx_fau_reg16_t;
+typedef int cvmx_fau_reg8_t;
+
+#define CVMX_FAU_REG_ANY 	-1
+
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+extern "C" {
+/* *INDENT-ON* */
+#endif
+
+/*
+ * Octeon Fetch and Add Unit (FAU)
+ */
+
+#define CVMX_FAU_LOAD_IO_ADDRESS    cvmx_build_io_address(0x1e, 0)
+#define CVMX_FAU_BITS_SCRADDR       63,56
+#define CVMX_FAU_BITS_LEN           55,48
+#define CVMX_FAU_BITS_INEVAL        35,14
+#define CVMX_FAU_BITS_TAGWAIT       13,13
+#define CVMX_FAU_BITS_NOADD         13,13
+#define CVMX_FAU_BITS_SIZE          12,11
+#define CVMX_FAU_BITS_REGISTER      10,0
+
+#define CVMX_FAU_MAX_REGISTERS_8  (2048)
+
+typedef enum {
+	CVMX_FAU_OP_SIZE_8 = 0,
+	CVMX_FAU_OP_SIZE_16 = 1,
+	CVMX_FAU_OP_SIZE_32 = 2,
+	CVMX_FAU_OP_SIZE_64 = 3
+} cvmx_fau_op_size_t;
+
+/**
+ * Tagwait return definition. If a timeout occurs, the error
+ * bit will be set. Otherwise the value of the register before
+ * the update will be returned.
+ */
+typedef struct {
+	uint64_t error:1;
+	int64_t value:63;
+} cvmx_fau_tagwait64_t;
+
+/**
+ * Tagwait return definition. If a timeout occurs, the error
+ * bit will be set. Otherwise the value of the register before
+ * the update will be returned.
+ */
+typedef struct {
+	uint64_t error:1;
+	int32_t value:31;
+} cvmx_fau_tagwait32_t;
+
+/**
+ * Tagwait return definition. If a timeout occurs, the error
+ * bit will be set. Otherwise the value of the register before
+ * the update will be returned.
+ */
+typedef struct {
+	uint64_t error:1;
+	int16_t value:15;
+} cvmx_fau_tagwait16_t;
+
+/**
+ * Tagwait return definition. If a timeout occurs, the error
+ * bit will be set. Otherwise the value of the register before
+ * the update will be returned.
+ */
+typedef struct {
+	uint64_t error:1;
+	int8_t value:7;
+} cvmx_fau_tagwait8_t;
+
+/**
+ * Asynchronous tagwait return definition. If a timeout occurs,
+ * the error bit will be set. Otherwise the value of the
+ * register before the update will be returned.
+ */
+typedef union {
+	uint64_t u64;
+	struct {
+		uint64_t invalid:1;
+		uint64_t data:63;	/* unpredictable if invalid is set */
+	} s;
+} cvmx_fau_async_tagwait_result_t;
+
+#ifdef __LITTLE_ENDIAN_BITFIELD
+#define SWIZZLE_8  0x7
+#define SWIZZLE_16 0x6
+#define SWIZZLE_32 0x4
+#else
+#define SWIZZLE_8  0
+#define SWIZZLE_16 0
+#define SWIZZLE_32 0
+#endif
+
+/**
+ * @INTERNAL
+ * Builds a store I/O address for writing to the FAU
+ *
+ * @param noadd  0 = Store value is atomically added to the current value
+ *               1 = Store value is atomically written over the current value
+ * @param reg    FAU atomic register to access. 0 <= reg < 2048.
+ *               - Step by 2 for 16 bit access.
+ *               - Step by 4 for 32 bit access.
+ *               - Step by 8 for 64 bit access.
+ * @return Address to store for atomic update
+ */
+static inline uint64_t __cvmx_hwfau_store_address(uint64_t noadd, uint64_t reg)
+{
+	return (CVMX_ADD_IO_SEG(CVMX_FAU_LOAD_IO_ADDRESS) |
+		cvmx_build_bits(CVMX_FAU_BITS_NOADD, noadd) |
+		cvmx_build_bits(CVMX_FAU_BITS_REGISTER, reg));
+}
+
+/**
+ * @INTERNAL
+ * Builds a I/O address for accessing the FAU
+ *
+ * @param tagwait Should the atomic add wait for the current tag switch
+ *                operation to complete.
+ *                - 0 = Don't wait
+ *                - 1 = Wait for tag switch to complete
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 2 for 16 bit access.
+ *                - Step by 4 for 32 bit access.
+ *                - Step by 8 for 64 bit access.
+ * @param value   Signed value to add.
+ *                Note: When performing 32 and 64 bit access, only the low
+ *                22 bits are available.
+ * @return Address to read from for atomic update
+ */ static inline uint64_t __cvmx_hwfau_atomic_address(uint64_t tagwait, uint64_t reg, int64_t value)
+{
+	return (CVMX_ADD_IO_SEG(CVMX_FAU_LOAD_IO_ADDRESS) |
+		cvmx_build_bits(CVMX_FAU_BITS_INEVAL, value) |
+		cvmx_build_bits(CVMX_FAU_BITS_TAGWAIT, tagwait) |
+		cvmx_build_bits(CVMX_FAU_BITS_REGISTER, reg));
+}
+
+/**
+ * Perform an atomic 64 bit add
+ *
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 8 for 64 bit access.
+ * @param value   Signed value to add.
+ *                Note: Only the low 22 bits are available.
+ * @return Value of the register before the update
+ */
+static inline int64_t cvmx_hwfau_fetch_and_add64(cvmx_fau_reg64_t reg, int64_t value)
+{
+	return cvmx_read64_int64(__cvmx_hwfau_atomic_address(0, reg, value));
+}
+
+/**
+ * Perform an atomic 32 bit add
+ *
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 4 for 32 bit access.
+ * @param value   Signed value to add.
+ *                Note: Only the low 22 bits are available.
+ * @return Value of the register before the update
+ */
+static inline int32_t cvmx_hwfau_fetch_and_add32(cvmx_fau_reg32_t reg, int32_t value)
+{
+	reg ^= SWIZZLE_32;
+	return cvmx_read64_int32(__cvmx_hwfau_atomic_address(0, reg, value));
+}
+
+/**
+ * Perform an atomic 16 bit add
+ *
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 2 for 16 bit access.
+ * @param value   Signed value to add.
+ * @return Value of the register before the update
+ */
+static inline int16_t cvmx_hwfau_fetch_and_add16(cvmx_fau_reg16_t reg, int16_t value)
+{
+	reg ^= SWIZZLE_16;
+	return cvmx_read64_int16(__cvmx_hwfau_atomic_address(0, reg, value));
+}
+
+/**
+ * Perform an atomic 8 bit add
+ *
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ * @param value   Signed value to add.
+ * @return Value of the register before the update
+ */
+static inline int8_t cvmx_hwfau_fetch_and_add8(cvmx_fau_reg8_t reg, int8_t value)
+{
+	reg ^= SWIZZLE_8;
+	return cvmx_read64_int8(__cvmx_hwfau_atomic_address(0, reg, value));
+}
+
+/**
+ * Perform an atomic 64 bit add after the current tag switch
+ * completes
+ *
+ * @param reg    FAU atomic register to access. 0 <= reg < 2048.
+ *               - Step by 8 for 64 bit access.
+ * @param value  Signed value to add.
+ *               Note: Only the low 22 bits are available.
+ * @return If a timeout occurs, the error bit will be set. Otherwise
+ *         the value of the register before the update will be
+ *         returned
+ */
+static inline cvmx_fau_tagwait64_t cvmx_hwfau_tagwait_fetch_and_add64(cvmx_fau_reg64_t reg, int64_t value)
+{
+	union {
+		uint64_t i64;
+		cvmx_fau_tagwait64_t t;
+	} result;
+	result.i64 = cvmx_read64_int64(__cvmx_hwfau_atomic_address(1, reg, value));
+		
+	return result.t;
+}
+
+/**
+ * Perform an atomic 32 bit add after the current tag switch
+ * completes
+ *
+ * @param reg    FAU atomic register to access. 0 <= reg < 2048.
+ *               - Step by 4 for 32 bit access.
+ * @param value  Signed value to add.
+ *               Note: Only the low 22 bits are available.
+ * @return If a timeout occurs, the error bit will be set. Otherwise
+ *         the value of the register before the update will be
+ *         returned
+ */
+static inline cvmx_fau_tagwait32_t cvmx_hwfau_tagwait_fetch_and_add32(cvmx_fau_reg32_t reg, int32_t value)
+{
+	union {
+		uint64_t i32;
+		cvmx_fau_tagwait32_t t;
+	} result;
+	reg ^= SWIZZLE_32;
+	result.i32 = cvmx_read64_int32(__cvmx_hwfau_atomic_address(1, reg, value));
+	return result.t;
+}
+
+/**
+ * Perform an atomic 16 bit add after the current tag switch
+ * completes
+ *
+ * @param reg    FAU atomic register to access. 0 <= reg < 2048.
+ *               - Step by 2 for 16 bit access.
+ * @param value  Signed value to add.
+ * @return If a timeout occurs, the error bit will be set. Otherwise
+ *         the value of the register before the update will be
+ *         returned
+ */
+static inline cvmx_fau_tagwait16_t cvmx_hwfau_tagwait_fetch_and_add16(cvmx_fau_reg16_t reg, int16_t value)
+{
+	union {
+		uint64_t i16;
+		cvmx_fau_tagwait16_t t;
+	} result;
+	reg ^= SWIZZLE_16;
+	result.i16 = cvmx_read64_int16(__cvmx_hwfau_atomic_address(1, reg, value));
+	return result.t;
+}
+
+/**
+ * Perform an atomic 8 bit add after the current tag switch
+ * completes
+ *
+ * @param reg    FAU atomic register to access. 0 <= reg < 2048.
+ * @param value  Signed value to add.
+ * @return If a timeout occurs, the error bit will be set. Otherwise
+ *         the value of the register before the update will be
+ *         returned
+ */
+static inline cvmx_fau_tagwait8_t cvmx_hwfau_tagwait_fetch_and_add8(cvmx_fau_reg8_t reg, int8_t value)
+{
+	union {
+		uint64_t i8;
+		cvmx_fau_tagwait8_t t;
+	} result;
+	reg ^= SWIZZLE_8;
+	result.i8 = cvmx_read64_int8(__cvmx_hwfau_atomic_address(1, reg, value));
+	return result.t;
+}
+
+/**
+ * @INTERNAL
+ * Builds I/O data for async operations
+ *
+ * @param scraddr Scratch pad byte addres to write to.  Must be 8 byte aligned
+ * @param value   Signed value to add.
+ *                Note: When performing 32 and 64 bit access, only the low
+ *                22 bits are available.
+ * @param tagwait Should the atomic add wait for the current tag switch
+ *                operation to complete.
+ *                - 0 = Don't wait
+ *                - 1 = Wait for tag switch to complete
+ * @param size    The size of the operation:
+ *                - CVMX_FAU_OP_SIZE_8  (0) = 8 bits
+ *                - CVMX_FAU_OP_SIZE_16 (1) = 16 bits
+ *                - CVMX_FAU_OP_SIZE_32 (2) = 32 bits
+ *                - CVMX_FAU_OP_SIZE_64 (3) = 64 bits
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 2 for 16 bit access.
+ *                - Step by 4 for 32 bit access.
+ *                - Step by 8 for 64 bit access.
+ * @return Data to write using cvmx_send_single
+ */
+static inline uint64_t __cvmx_fau_iobdma_data(uint64_t scraddr, int64_t value, uint64_t tagwait, cvmx_fau_op_size_t size, uint64_t reg)
+{
+	return (CVMX_FAU_LOAD_IO_ADDRESS |
+		cvmx_build_bits(CVMX_FAU_BITS_SCRADDR, scraddr >> 3) |
+		cvmx_build_bits(CVMX_FAU_BITS_LEN, 1) |
+		cvmx_build_bits(CVMX_FAU_BITS_INEVAL, value) |
+		cvmx_build_bits(CVMX_FAU_BITS_TAGWAIT, tagwait) |
+		cvmx_build_bits(CVMX_FAU_BITS_SIZE, size) |
+		cvmx_build_bits(CVMX_FAU_BITS_REGISTER, reg));
+}
+
+/**
+ * Perform an async atomic 64 bit add. The old value is
+ * placed in the scratch memory at byte address scraddr.
+ *
+ * @param scraddr Scratch memory byte address to put response in.
+ *                Must be 8 byte aligned.
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 8 for 64 bit access.
+ * @param value   Signed value to add.
+ *                Note: Only the low 22 bits are available.
+ * @return Placed in the scratch pad register
+ */
+static inline void cvmx_hwfau_async_fetch_and_add64(uint64_t scraddr, cvmx_fau_reg64_t reg, int64_t value)
+{
+	cvmx_send_single(__cvmx_fau_iobdma_data(scraddr, value, 0, CVMX_FAU_OP_SIZE_64, reg));
+}
+
+/**
+ * Perform an async atomic 32 bit add. The old value is
+ * placed in the scratch memory at byte address scraddr.
+ *
+ * @param scraddr Scratch memory byte address to put response in.
+ *                Must be 8 byte aligned.
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 4 for 32 bit access.
+ * @param value   Signed value to add.
+ *                Note: Only the low 22 bits are available.
+ * @return Placed in the scratch pad register
+ */
+static inline void cvmx_hwfau_async_fetch_and_add32(uint64_t scraddr, cvmx_fau_reg32_t reg, int32_t value)
+{
+	cvmx_send_single(__cvmx_fau_iobdma_data(scraddr, value, 0, CVMX_FAU_OP_SIZE_32, reg));
+}
+
+/**
+ * Perform an async atomic 16 bit add. The old value is
+ * placed in the scratch memory at byte address scraddr.
+ *
+ * @param scraddr Scratch memory byte address to put response in.
+ *                Must be 8 byte aligned.
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 2 for 16 bit access.
+ * @param value   Signed value to add.
+ * @return Placed in the scratch pad register
+ */
+static inline void cvmx_hwfau_async_fetch_and_add16(uint64_t scraddr, cvmx_fau_reg16_t reg, int16_t value)
+{
+	cvmx_send_single(__cvmx_fau_iobdma_data(scraddr, value, 0, CVMX_FAU_OP_SIZE_16, reg));
+}
+
+/**
+ * Perform an async atomic 8 bit add. The old value is
+ * placed in the scratch memory at byte address scraddr.
+ *
+ * @param scraddr Scratch memory byte address to put response in.
+ *                Must be 8 byte aligned.
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ * @param value   Signed value to add.
+ * @return Placed in the scratch pad register
+ */
+static inline void cvmx_hwfau_async_fetch_and_add8(uint64_t scraddr, cvmx_fau_reg8_t reg, int8_t value)
+{
+	cvmx_send_single(__cvmx_fau_iobdma_data(scraddr, value, 0, CVMX_FAU_OP_SIZE_8, reg));
+}
+
+/**
+ * Perform an async atomic 64 bit add after the current tag
+ * switch completes.
+ *
+ * @param scraddr Scratch memory byte address to put response in.
+ *                Must be 8 byte aligned.
+ *                If a timeout occurs, the error bit (63) will be set. Otherwise
+ *                the value of the register before the update will be
+ *                returned
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 8 for 64 bit access.
+ * @param value   Signed value to add.
+ *                Note: Only the low 22 bits are available.
+ * @return Placed in the scratch pad register
+ */
+static inline void cvmx_hwfau_async_tagwait_fetch_and_add64(uint64_t scraddr, cvmx_fau_reg64_t reg, int64_t value)
+{
+	cvmx_send_single(__cvmx_fau_iobdma_data(scraddr, value, 1, CVMX_FAU_OP_SIZE_64, reg));
+}
+
+/**
+ * Perform an async atomic 32 bit add after the current tag
+ * switch completes.
+ *
+ * @param scraddr Scratch memory byte address to put response in.
+ *                Must be 8 byte aligned.
+ *                If a timeout occurs, the error bit (63) will be set. Otherwise
+ *                the value of the register before the update will be
+ *                returned
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 4 for 32 bit access.
+ * @param value   Signed value to add.
+ *                Note: Only the low 22 bits are available.
+ * @return Placed in the scratch pad register
+ */
+static inline void cvmx_hwfau_async_tagwait_fetch_and_add32(uint64_t scraddr, cvmx_fau_reg32_t reg, int32_t value)
+{
+	cvmx_send_single(__cvmx_fau_iobdma_data(scraddr, value, 1, CVMX_FAU_OP_SIZE_32, reg));
+}
+
+/**
+ * Perform an async atomic 16 bit add after the current tag
+ * switch completes.
+ *
+ * @param scraddr Scratch memory byte address to put response in.
+ *                Must be 8 byte aligned.
+ *                If a timeout occurs, the error bit (63) will be set. Otherwise
+ *                the value of the register before the update will be
+ *                returned
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 2 for 16 bit access.
+ * @param value   Signed value to add.
+ * @return Placed in the scratch pad register
+ */
+static inline void cvmx_hwfau_async_tagwait_fetch_and_add16(uint64_t scraddr, cvmx_fau_reg16_t reg, int16_t value)
+{
+	cvmx_send_single(__cvmx_fau_iobdma_data(scraddr, value, 1, CVMX_FAU_OP_SIZE_16, reg));
+}
+
+/**
+ * Perform an async atomic 8 bit add after the current tag
+ * switch completes.
+ *
+ * @param scraddr Scratch memory byte address to put response in.
+ *                Must be 8 byte aligned.
+ *                If a timeout occurs, the error bit (63) will be set. Otherwise
+ *                the value of the register before the update will be
+ *                returned
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ * @param value   Signed value to add.
+ * @return Placed in the scratch pad register
+ */
+static inline void cvmx_hwfau_async_tagwait_fetch_and_add8(uint64_t scraddr, cvmx_fau_reg8_t reg, int8_t value)
+{
+	cvmx_send_single(__cvmx_fau_iobdma_data(scraddr, value, 1, CVMX_FAU_OP_SIZE_8, reg));
+}
+
+/**
+ * Perform an atomic 64 bit add
+ *
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 8 for 64 bit access.
+ * @param value   Signed value to add.
+ */
+static inline void cvmx_hwfau_atomic_add64(cvmx_fau_reg64_t reg, int64_t value)
+{
+	cvmx_write64_int64(__cvmx_hwfau_store_address(0, reg), value);
+}
+
+/**
+ * Perform an atomic 32 bit add
+ *
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 4 for 32 bit access.
+ * @param value   Signed value to add.
+ */
+static inline void cvmx_hwfau_atomic_add32(cvmx_fau_reg32_t reg, int32_t value)
+{
+	reg ^= SWIZZLE_32;
+	cvmx_write64_int32(__cvmx_hwfau_store_address(0, reg), value);
+}
+
+/**
+ * Perform an atomic 16 bit add
+ *
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 2 for 16 bit access.
+ * @param value   Signed value to add.
+ */
+static inline void cvmx_hwfau_atomic_add16(cvmx_fau_reg16_t reg, int16_t value)
+{
+	reg ^= SWIZZLE_16;
+	cvmx_write64_int16(__cvmx_hwfau_store_address(0, reg), value);
+}
+
+/**
+ * Perform an atomic 8 bit add
+ *
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ * @param value   Signed value to add.
+ */
+static inline void cvmx_hwfau_atomic_add8(cvmx_fau_reg8_t reg, int8_t value)
+{
+	reg ^= SWIZZLE_8;
+	cvmx_write64_int8(__cvmx_hwfau_store_address(0, reg), value);
+}
+
+/**
+ * Perform an atomic 64 bit write
+ *
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 8 for 64 bit access.
+ * @param value   Signed value to write.
+ */
+static inline void cvmx_hwfau_atomic_write64(cvmx_fau_reg64_t reg, int64_t value)
+{
+	cvmx_write64_int64(__cvmx_hwfau_store_address(1, reg), value);
+}
+
+/**
+ * Perform an atomic 32 bit write
+ *
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 4 for 32 bit access.
+ * @param value   Signed value to write.
+ */
+static inline void cvmx_hwfau_atomic_write32(cvmx_fau_reg32_t reg, int32_t value)
+{
+	reg ^= SWIZZLE_32;
+	cvmx_write64_int32(__cvmx_hwfau_store_address(1, reg), value);
+}
+
+/**
+ * Perform an atomic 16 bit write
+ *
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ *                - Step by 2 for 16 bit access.
+ * @param value   Signed value to write.
+ */
+static inline void cvmx_hwfau_atomic_write16(cvmx_fau_reg16_t reg, int16_t value)
+{
+	reg ^= SWIZZLE_16;
+	cvmx_write64_int16(__cvmx_hwfau_store_address(1, reg), value);
+}
+
+/**
+ * Perform an atomic 8 bit write
+ *
+ * @param reg     FAU atomic register to access. 0 <= reg < 2048.
+ * @param value   Signed value to write.
+ */
+static inline void cvmx_hwfau_atomic_write8(cvmx_fau_reg8_t reg, int8_t value)
+{
+	reg ^= SWIZZLE_8;
+	cvmx_write64_int8(__cvmx_hwfau_store_address(1, reg), value);
+}
+
+/** Allocates 64bit FAU register.
+ *  @return value is the base address of allocated FAU register
+ */
+extern int cvmx_fau64_alloc(int reserve);
+
+/** Allocates 32bit FAU register.
+ *  @return value is the base address of allocated FAU register
+ */
+extern int cvmx_fau32_alloc(int reserve);
+
+/** Allocates 16bit FAU register.
+ *  @return value is the base address of allocated FAU register
+ */
+extern int cvmx_fau16_alloc(int reserve);
+
+/** Allocates 8bit FAU register.
+ *  @return value is the base address of allocated FAU register
+ */
+extern int cvmx_fau8_alloc(int reserve);
+
+/** Frees the specified FAU register.
+ *  @param Base address of register to release.
+ *  @return 0 on success; -1 on failure
+ */
+extern int cvmx_fau_free(int address);
+
+/** Display the fau registers array
+ */
+extern void cvmx_fau_show(void);
+
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+}
+/* *INDENT-ON* */
+#endif
+
+#endif /* __CVMX_HWFAU_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-hwpko.h b/arch/mips/include/asm/octeon/cvmx-hwpko.h
new file mode 100644
index 0000000..0f5eba7
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-hwpko.h
@@ -0,0 +1,665 @@
+/***********************license start***************
+ * Copyright (c) 2003-2010  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Interface to the hardware Packet Output unit.
+ *
+ * Starting with SDK 1.7.0, the PKO output functions now support
+ * two types of locking. CVMX_PKO_LOCK_ATOMIC_TAG continues to
+ * function similarly to previous SDKs by using POW atomic tags
+ * to preserve ordering and exclusivity. As a new option, you
+ * can now pass CVMX_PKO_LOCK_CMD_QUEUE which uses a ll/sc
+ * memory based locking instead. This locking has the advantage
+ * of not affecting the tag state but doesn't preserve packet
+ * ordering. CVMX_PKO_LOCK_CMD_QUEUE is appropriate in most
+ * generic code while CVMX_PKO_LOCK_CMD_QUEUE should be used
+ * with hand tuned fast path code.
+ *
+ * Some of other SDK differences visible to the command command
+ * queuing:
+ * - PKO indexes are no longer stored in the FAU. A large
+ *   percentage of the FAU register block used to be tied up
+ *   maintaining PKO queue pointers. These are now stored in a
+ *   global named block.
+ * - The PKO <b>use_locking</b> parameter can now have a global
+ *   effect. Since all application use the same named block,
+ *   queue locking correctly applies across all operating
+ *   systems when using CVMX_PKO_LOCK_CMD_QUEUE.
+ * - PKO 3 word commands are now supported. Use
+ *   cvmx_pko_send_packet_finish3().
+ *
+ */
+
+#ifndef __CVMX_HWPKO_H__
+#define __CVMX_HWPKO_H__
+
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include "cvmx-pko-defs.h"
+#include <asm/octeon/cvmx-hwfau.h>
+#include <asm/octeon/cvmx-fpa1.h>
+#include <asm/octeon/cvmx-pow.h>
+#include <asm/octeon/cvmx-cmd-queue.h>
+#include <asm/octeon/cvmx-helper.h>
+#include <asm/octeon/cvmx-helper-cfg.h>
+#include <asm/octeon/cvmx-helper-pko.h>
+#else
+#include "cvmx-hwfau.h"
+#include "cvmx-fpa1.h"
+#include "cvmx-pow.h"
+#include "cvmx-pko3.h"	/* for back-comp */
+#include "cvmx-cmd-queue.h"
+#include "cvmx-helper.h"
+#include "cvmx-helper-util.h"
+#include "cvmx-helper-cfg.h"
+#include "cvmx-helper-pko.h"
+#endif
+
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+extern "C" {
+/* *INDENT-ON* */
+#endif
+
+/* Adjust the command buffer size by 1 word so that in the case of using only
+** two word PKO commands no command words stradle buffers.  The useful values
+** for this are 0 and 1. */
+#define CVMX_PKO_COMMAND_BUFFER_SIZE_ADJUST (1)
+
+
+#define CVMX_PKO_MAX_OUTPUT_QUEUES_STATIC 256
+#define CVMX_PKO_MAX_OUTPUT_QUEUES      ((OCTEON_IS_MODEL(OCTEON_CN31XX) || \
+					  OCTEON_IS_MODEL(OCTEON_CN3010) || \
+					  OCTEON_IS_MODEL(OCTEON_CN3005) || \
+					  OCTEON_IS_MODEL(OCTEON_CN50XX)) ? \
+					  32 :				    \
+					 (OCTEON_IS_MODEL(OCTEON_CN58XX) || \
+					  OCTEON_IS_MODEL(OCTEON_CN56XX) || \
+					  OCTEON_IS_MODEL(OCTEON_CN52XX) || \
+					  OCTEON_IS_OCTEON2() || \
+					  OCTEON_IS_MODEL(OCTEON_CN70XX)) ? \
+					  256 : 128)
+#define CVMX_PKO_NUM_OUTPUT_PORTS       ((OCTEON_IS_MODEL(OCTEON_CN63XX)) ? 44 : (OCTEON_IS_MODEL(OCTEON_CN66XX) ? 48 : 40))
+#define CVMX_PKO_MEM_QUEUE_PTRS_ILLEGAL_PID 63	/* use this for queues that are not used */
+#define CVMX_PKO_QUEUE_STATIC_PRIORITY  9
+#define CVMX_PKO_ILLEGAL_QUEUE  0xFFFF
+#define CVMX_PKO_MAX_QUEUE_DEPTH 0
+
+typedef enum {
+	CVMX_PKO_SUCCESS,
+	CVMX_PKO_INVALID_PORT,
+	CVMX_PKO_INVALID_QUEUE,
+	CVMX_PKO_INVALID_PRIORITY,
+	CVMX_PKO_NO_MEMORY,
+	CVMX_PKO_PORT_ALREADY_SETUP,
+	CVMX_PKO_CMD_QUEUE_INIT_ERROR
+} cvmx_pko_return_value_t;
+
+/**
+ * This enumeration represents the differnet locking modes supported by PKO.
+ */
+typedef enum {
+	CVMX_PKO_LOCK_NONE = 0,
+				    /**< PKO doesn't do any locking. It is the responsibility
+                                        of the application to make sure that no other core is
+                                        accessing the same queue at the same time */
+	CVMX_PKO_LOCK_ATOMIC_TAG = 1,
+				    /**< PKO performs an atomic tagswitch to insure exclusive
+                                        access to the output queue. This will maintain
+                                        packet ordering on output */
+	CVMX_PKO_LOCK_CMD_QUEUE = 2,
+				    /**< PKO uses the common command queue locks to insure
+                                        exclusive access to the output queue. This is a memory
+                                        based ll/sc. This is the most portable locking
+                                        mechanism */
+} cvmx_pko_lock_t;
+
+typedef struct cvmx_pko_port_status {
+	uint32_t packets;
+	uint64_t octets;
+	uint64_t doorbell;
+} cvmx_pko_port_status_t;
+
+/**
+ * This structure defines the address to use on a packet enqueue
+ */
+typedef union {
+	uint64_t u64;
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		cvmx_mips_space_t mem_space:2;
+						/**< Must CVMX_IO_SEG */
+		uint64_t reserved:13;		/**< Must be zero */
+		uint64_t is_io:1;		/**< Must be one */
+		uint64_t did:8;			/**< The ID of the device on the non-coherent bus */
+		uint64_t reserved2:4;		/**< Must be zero */
+		uint64_t reserved3:15;		/**< Must be zero */
+		uint64_t port:9;		/**< The hardware must have the output port in addition to the output queue */
+		uint64_t queue:9;		/**< The output queue to send the packet to (0-127 are legal) */
+		uint64_t reserved4:3;		/**< Must be zero */
+#else
+		uint64_t reserved4:3;
+		uint64_t queue:9;
+		uint64_t port:9;
+		uint64_t reserved3:15;
+		uint64_t reserved2:4;
+		uint64_t did:8;
+		uint64_t is_io:1;
+		uint64_t reserved:13;
+		cvmx_mips_space_t mem_space:2;
+#endif
+	} s;
+} cvmx_pko_doorbell_address_t;
+
+/**
+ * Structure of the first packet output command word.
+ */
+typedef union {
+	uint64_t u64;
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		cvmx_fau_op_size_t size1:2;
+					     /**< The size of the reg1 operation - could be 8, 16, 32, or 64 bits */
+		cvmx_fau_op_size_t size0:2;
+					     /**< The size of the reg0 operation - could be 8, 16, 32, or 64 bits */
+		uint64_t subone1:1;	     /**< If set, subtract 1, if clear, subtract packet size */
+		uint64_t reg1:11;	     /**< The register, subtract will be done if reg1 is non-zero */
+		uint64_t subone0:1;	     /**< If set, subtract 1, if clear, subtract packet size */
+		uint64_t reg0:11;	     /**< The register, subtract will be done if reg0 is non-zero */
+		uint64_t le:1;		     /**< When set, interpret segment pointer and segment bytes in little endian order */
+		uint64_t n2:1;		     /**< When set, packet data not allocated in L2 cache by PKO */
+		uint64_t wqp:1;		     /**< If set and rsp is set, word3 contains a pointer to a work queue entry */
+		uint64_t rsp:1;		     /**< If set, the hardware will send a response when done */
+		uint64_t gather:1;	     /**< If set, the supplied pkt_ptr is really a pointer to a list of pkt_ptr's */
+		uint64_t ipoffp1:7;	     /**< If ipoffp1 is non zero, (ipoffp1-1) is the number of bytes to IP header,
+                                                and the hardware will calculate and insert the  UDP/TCP checksum */
+		uint64_t ignore_i:1;	     /**< If set, ignore the I bit (force to zero) from all pointer structures */
+		uint64_t dontfree:1;	     /**< If clear, the hardware will attempt to free the buffers containing the packet */
+		uint64_t segs:6;	     /**< The total number of segs in the packet, if gather set, also gather list length */
+		uint64_t total_bytes:16;
+					     /**< Including L2, but no trailing CRC */
+#else
+		uint64_t total_bytes:16;
+		uint64_t segs:6;
+		uint64_t dontfree:1;
+		uint64_t ignore_i:1;
+		uint64_t ipoffp1:7;
+		uint64_t gather:1;
+		uint64_t rsp:1;
+		uint64_t wqp:1;
+		uint64_t n2:1;
+		uint64_t le:1;
+		uint64_t reg0:11;
+		uint64_t subone0:1;
+		uint64_t reg1:11;
+		uint64_t subone1:1;
+		cvmx_fau_op_size_t size0:2;
+		cvmx_fau_op_size_t size1:2;
+#endif
+	} s;
+} cvmx_pko_command_word0_t;
+
+/**
+ * Call before any other calls to initialize the packet
+ * output system.
+ */
+// extern void cvmx_pko_initialize_global(void);
+// extern int cvmx_pko_initialize_local(void);
+
+extern void cvmx_pko_hw_init(uint8_t pool, unsigned bufsize);
+
+/**
+ * Enables the packet output hardware. It must already be
+ * configured.
+ */
+extern void cvmx_pko_enable(void);
+
+/**
+ * Disables the packet output. Does not affect any configuration.
+ */
+extern void cvmx_pko_disable(void);
+
+/**
+ * Shutdown and free resources required by packet output.
+ */
+
+extern void cvmx_pko_shutdown(void);
+
+/**
+ * Configure a output port and the associated queues for use.
+ *
+ * @param port       Port to configure.
+ * @param base_queue First queue number to associate with this port.
+ * @param num_queues Number of queues t oassociate with this port
+ * @param priority   Array of priority levels for each queue. Values are
+ *                   allowed to be 1-8. A value of 8 get 8 times the traffic
+ *                   of a value of 1. There must be num_queues elements in the
+ *                   array.
+ */
+extern cvmx_pko_return_value_t cvmx_pko_config_port(int port, int base_queue, int num_queues, const uint8_t priority[]);
+
+/**
+ * Ring the packet output doorbell. This tells the packet
+ * output hardware that "len" command words have been added
+ * to its pending list.  This command includes the required
+ * CVMX_SYNCWS before the doorbell ring.
+ *
+ * WARNING: This function may have to look up the proper PKO port in
+ * the IPD port to PKO port map, and is thus slower than calling
+ * cvmx_pko_doorbell_pkoid() directly if the PKO port identifier is
+ * known.
+ *
+ * @param ipd_port   The IPD port corresponding the to pko port the packet is for
+ * @param queue  Queue the packet is for
+ * @param len    Length of the command in 64 bit words
+ */
+static inline void cvmx_pko_doorbell(uint64_t ipd_port, uint64_t queue, uint64_t len)
+{
+	cvmx_pko_doorbell_address_t ptr;
+	uint64_t pko_port;
+
+	pko_port = ipd_port;
+	if (octeon_has_feature(OCTEON_FEATURE_PKND))
+		pko_port = cvmx_helper_cfg_ipd2pko_port_base(ipd_port);
+
+	ptr.u64 = 0;
+	ptr.s.mem_space = CVMX_IO_SEG;
+	ptr.s.did = CVMX_OCT_DID_PKT_SEND;
+	ptr.s.is_io = 1;
+	ptr.s.port = pko_port;
+	ptr.s.queue = queue;
+	CVMX_SYNCWS;		/* Need to make sure output queue data is in DRAM before doorbell write */
+	cvmx_write_io(ptr.u64, len);
+}
+
+/**
+ * Prepare to send a packet.  This may initiate a tag switch to
+ * get exclusive access to the output queue structure, and
+ * performs other prep work for the packet send operation.
+ *
+ * cvmx_pko_send_packet_finish() MUST be called after this function is called,
+ * and must be called with the same port/queue/use_locking arguments.
+ *
+ * The use_locking parameter allows the caller to use three
+ * possible locking modes.
+ * - CVMX_PKO_LOCK_NONE
+ *      - PKO doesn't do any locking. It is the responsibility
+ *          of the application to make sure that no other core
+ *          is accessing the same queue at the same time.
+ * - CVMX_PKO_LOCK_ATOMIC_TAG
+ *      - PKO performs an atomic tagswitch to insure exclusive
+ *          access to the output queue. This will maintain
+ *          packet ordering on output.
+ * - CVMX_PKO_LOCK_CMD_QUEUE
+ *      - PKO uses the common command queue locks to insure
+ *          exclusive access to the output queue. This is a
+ *          memory based ll/sc. This is the most portable
+ *          locking mechanism.
+ *
+ * NOTE: If atomic locking is used, the POW entry CANNOT be
+ * descheduled, as it does not contain a valid WQE pointer.
+ *
+ * @param port   Port to send it on, this can be either IPD port or PKO
+ * 		 port.
+ * @param queue  Queue to use
+ * @param use_locking
+ *               CVMX_PKO_LOCK_NONE, CVMX_PKO_LOCK_ATOMIC_TAG, or CVMX_PKO_LOCK_CMD_QUEUE
+ */ static inline void cvmx_pko_send_packet_prepare(uint64_t port __attribute__ ((unused)), uint64_t queue, cvmx_pko_lock_t use_locking)
+{
+	if (use_locking == CVMX_PKO_LOCK_ATOMIC_TAG) {
+		/* Must do a full switch here to handle all cases.  We use a fake WQE pointer, as the POW does
+		 ** not access this memory.  The WQE pointer and group are only used if this work is descheduled,
+		 ** which is not supported by the cvmx_pko_send_packet_prepare/cvmx_pko_send_packet_finish combination.
+		 ** Note that this is a special case in which these fake values can be used - this is not a general technique.
+		 */
+		uint32_t tag = CVMX_TAG_SW_BITS_INTERNAL << CVMX_TAG_SW_SHIFT | CVMX_TAG_SUBGROUP_PKO << CVMX_TAG_SUBGROUP_SHIFT | (CVMX_TAG_SUBGROUP_MASK & queue);
+		cvmx_pow_tag_sw_full((cvmx_wqe_t *) cvmx_phys_to_ptr(0x80), tag, CVMX_POW_TAG_TYPE_ATOMIC, 0);
+	}
+}
+
+#define cvmx_pko_send_packet_prepare_pkoid	cvmx_pko_send_packet_prepare
+
+/**
+ * Complete packet output. cvmx_pko_send_packet_prepare() must be called exactly once before this,
+ * and the same parameters must be passed to both cvmx_pko_send_packet_prepare() and
+ * cvmx_pko_send_packet_finish().
+ *
+ * WARNING: This function may have to look up the proper PKO port in
+ * the IPD port to PKO port map, and is thus slower than calling
+ * cvmx_pko_send_packet_finish_pkoid() directly if the PKO port
+ * identifier is known.
+ *
+ * @param ipd_port   The IPD port corresponding the to pko port the packet is for
+ * @param queue  Queue to use
+ * @param pko_command
+ *               PKO HW command word
+ * @param packet Packet to send
+ * @param use_locking
+ *               CVMX_PKO_LOCK_NONE, CVMX_PKO_LOCK_ATOMIC_TAG, or CVMX_PKO_LOCK_CMD_QUEUE
+ *
+ * @return returns CVMX_PKO_SUCCESS on success, or error code on failure of output
+ */
+static inline cvmx_pko_return_value_t cvmx_hwpko_send_packet_finish(uint64_t ipd_port, uint64_t queue,
+								    cvmx_pko_command_word0_t pko_command, cvmx_buf_ptr_t packet, cvmx_pko_lock_t use_locking)
+{
+	cvmx_cmd_queue_result_t result;
+
+	if (use_locking == CVMX_PKO_LOCK_ATOMIC_TAG)
+		cvmx_pow_tag_sw_wait();
+
+	result = cvmx_cmd_queue_write2(CVMX_CMD_QUEUE_PKO(queue), (use_locking == CVMX_PKO_LOCK_CMD_QUEUE), pko_command.u64, packet.u64);
+	if (cvmx_likely(result == CVMX_CMD_QUEUE_SUCCESS)) {
+		cvmx_pko_doorbell(ipd_port, queue, 2);
+		return CVMX_PKO_SUCCESS;
+	} else if ((result == CVMX_CMD_QUEUE_NO_MEMORY) || (result == CVMX_CMD_QUEUE_FULL)) {
+		return CVMX_PKO_NO_MEMORY;
+	} else {
+		return CVMX_PKO_INVALID_QUEUE;
+	}
+}
+
+/**
+ * Complete packet output. cvmx_pko_send_packet_prepare() must be called exactly once before this,
+ * and the same parameters must be passed to both cvmx_pko_send_packet_prepare() and
+ * cvmx_pko_send_packet_finish().
+ *
+ * WARNING: This function may have to look up the proper PKO port in
+ * the IPD port to PKO port map, and is thus slower than calling
+ * cvmx_pko_send_packet_finish3_pkoid() directly if the PKO port
+ * identifier is known.
+ *
+ * @param ipd_port   The IPD port corresponding the to pko port the packet is for
+ * @param queue  Queue to use
+ * @param pko_command
+ *               PKO HW command word
+ * @param packet Packet to send
+ * @param addr   Plysical address of a work queue entry or physical address to zero on complete.
+ * @param use_locking
+ *               CVMX_PKO_LOCK_NONE, CVMX_PKO_LOCK_ATOMIC_TAG, or CVMX_PKO_LOCK_CMD_QUEUE
+ *
+ * @return returns CVMX_PKO_SUCCESS on success, or error code on failure of output
+ */
+static inline cvmx_pko_return_value_t cvmx_hwpko_send_packet_finish3(uint64_t ipd_port, uint64_t queue,
+							     cvmx_pko_command_word0_t pko_command, cvmx_buf_ptr_t packet, uint64_t addr, cvmx_pko_lock_t use_locking)
+{
+	cvmx_cmd_queue_result_t result;
+	if (use_locking == CVMX_PKO_LOCK_ATOMIC_TAG)
+		cvmx_pow_tag_sw_wait();
+
+	result = cvmx_cmd_queue_write3(CVMX_CMD_QUEUE_PKO(queue), (use_locking == CVMX_PKO_LOCK_CMD_QUEUE), pko_command.u64, packet.u64, addr);
+	if (cvmx_likely(result == CVMX_CMD_QUEUE_SUCCESS)) {
+		cvmx_pko_doorbell(ipd_port, queue, 3);
+		return CVMX_PKO_SUCCESS;
+	} else if ((result == CVMX_CMD_QUEUE_NO_MEMORY) || (result == CVMX_CMD_QUEUE_FULL)) {
+		return CVMX_PKO_NO_MEMORY;
+	} else {
+		return CVMX_PKO_INVALID_QUEUE;
+	}
+}
+
+/**
+ * Get the first pko_port for the (interface, index)
+ *
+ * @param interface
+ * @param index
+ */
+extern int cvmx_pko_get_base_pko_port(int interface, int index);
+
+/**
+ * Get the number of pko_ports for the (interface, index)
+ *
+ * @param interface
+ * @param index
+ */
+extern int cvmx_pko_get_num_pko_ports(int interface, int index);
+
+/**
+ * For a given port number, return the base pko output queue
+ * for the port.
+ *
+ * @param port   IPD port number
+ * @return Base output queue
+ */
+extern int cvmx_pko_get_base_queue(int port);
+
+/**
+ * For a given port number, return the number of pko output queues.
+ *
+ * @param port   IPD port number
+ * @return Number of output queues
+ */
+extern int cvmx_pko_get_num_queues(int port);
+
+/**
+ * Sets the internal FPA pool data structure for PKO comamnd queue.
+ * @param pool	fpa pool number yo use
+ * @param buffer_size	buffer size of pool
+ * @param buffer_count	number of buufers to allocate to pool
+ */
+void cvmx_pko_set_cmd_que_pool_config(int64_t pool, uint64_t buffer_size,
+					   uint64_t buffer_count);
+//FIXME- reconsider:
+// Helper should setup the pool, then call PKO (pko_init?) with the
+// pool number to use.
+
+/**
+ * Get the status counters for a port.
+ *
+ * @param ipd_port Port number (ipd_port) to get statistics for.
+ * @param clear    Set to 1 to clear the counters after they are read
+ * @param status   Where to put the results.
+ *
+ * Note:
+ *     - Only the doorbell for the base queue of the ipd_port is
+ *       collected.
+ *     - Retrieving the stats involves writing the index through
+ *       CVMX_PKO_REG_READ_IDX and reading the stat CSRs, in that
+ *       order. It is not MP-safe and caller should guarantee
+ *       atomicity.
+ */
+void cvmx_pko_get_port_status(uint64_t ipd_port, uint64_t clear, cvmx_pko_port_status_t * status);
+
+/**
+ * Rate limit a PKO port to a max packets/sec. This function is only
+ * supported on CN57XX, CN56XX, CN55XX, and CN54XX.
+ *
+ * @param port      Port to rate limit
+ * @param packets_s Maximum packet/sec
+ * @param burst     Maximum number of packets to burst in a row before rate
+ *                  limiting cuts in.
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int cvmx_pko_rate_limit_packets(int port, int packets_s, int burst);
+
+/**
+ * Rate limit a PKO port to a max bits/sec. This function is only
+ * supported on CN57XX, CN56XX, CN55XX, and CN54XX.
+ *
+ * @param port   Port to rate limit
+ * @param bits_s PKO rate limit in bits/sec
+ * @param burst  Maximum number of bits to burst before rate
+ *               limiting cuts in.
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int cvmx_pko_rate_limit_bits(int port, uint64_t bits_s, int burst);
+
+/**
+ * @INTERNAL
+ *
+ * Retrieve the PKO pipe number for a port
+ *
+ * @param interface
+ * @param index
+ *
+ * @return negative on error.
+ *
+ * This applies only to the non-loopback interfaces.
+ *
+ */
+extern int __cvmx_pko_get_pipe(int interface, int index);
+
+/**
+ * For a given PKO port number, return the base output queue
+ * for the port.
+ *
+ * @param pko_port   PKO port number
+ * @return           Base output queue
+ */
+extern int cvmx_pko_get_base_queue_pkoid(int pko_port);
+
+/**
+ * For a given PKO port number, return the number of output queues
+ * for the port.
+ *
+ * @param pko_port	PKO port number
+ * @return		the number of output queues
+ */
+extern int cvmx_pko_get_num_queues_pkoid(int pko_port);
+
+/**
+ * Ring the packet output doorbell. This tells the packet
+ * output hardware that "len" command words have been added
+ * to its pending list.  This command includes the required
+ * CVMX_SYNCWS before the doorbell ring.
+ *
+ * @param pko_port   Port the packet is for
+ * @param queue  Queue the packet is for
+ * @param len    Length of the command in 64 bit words
+ */
+static inline void cvmx_pko_doorbell_pkoid(uint64_t pko_port, uint64_t queue, uint64_t len)
+{
+	cvmx_pko_doorbell_address_t ptr;
+
+	ptr.u64 = 0;
+	ptr.s.mem_space = CVMX_IO_SEG;
+	ptr.s.did = CVMX_OCT_DID_PKT_SEND;
+	ptr.s.is_io = 1;
+	ptr.s.port = pko_port;
+	ptr.s.queue = queue;
+	CVMX_SYNCWS;		/* Need to make sure output queue data is in DRAM before doorbell write */
+	cvmx_write_io(ptr.u64, len);
+}
+
+/**
+ * Complete packet output. cvmx_pko_send_packet_prepare() must be called exactly once before this,
+ * and the same parameters must be passed to both cvmx_pko_send_packet_prepare() and
+ * cvmx_pko_send_packet_finish_pkoid().
+ *
+ * @param pko_port   Port to send it on
+ * @param queue  Queue to use
+ * @param pko_command
+ *               PKO HW command word
+ * @param packet Packet to send
+ * @param use_locking
+ *               CVMX_PKO_LOCK_NONE, CVMX_PKO_LOCK_ATOMIC_TAG, or CVMX_PKO_LOCK_CMD_QUEUE
+ *
+ * @return returns CVMX_PKO_SUCCESS on success, or error code on failure of output
+ */
+static inline cvmx_pko_return_value_t cvmx_hwpko_send_packet_finish_pkoid(int pko_port, uint64_t queue,
+								  cvmx_pko_command_word0_t pko_command, cvmx_buf_ptr_t packet, cvmx_pko_lock_t use_locking)
+{
+	cvmx_cmd_queue_result_t result;
+	if (use_locking == CVMX_PKO_LOCK_ATOMIC_TAG)
+		cvmx_pow_tag_sw_wait();
+
+	result = cvmx_cmd_queue_write2(CVMX_CMD_QUEUE_PKO(queue), (use_locking == CVMX_PKO_LOCK_CMD_QUEUE), pko_command.u64, packet.u64);
+	if (cvmx_likely(result == CVMX_CMD_QUEUE_SUCCESS)) {
+		cvmx_pko_doorbell_pkoid(pko_port, queue, 2);
+		return CVMX_PKO_SUCCESS;
+	} else if ((result == CVMX_CMD_QUEUE_NO_MEMORY) || (result == CVMX_CMD_QUEUE_FULL)) {
+		return CVMX_PKO_NO_MEMORY;
+	} else {
+		return CVMX_PKO_INVALID_QUEUE;
+	}
+}
+
+/**
+ * Complete packet output. cvmx_pko_send_packet_prepare() must be called exactly once before this,
+ * and the same parameters must be passed to both cvmx_pko_send_packet_prepare() and
+ * cvmx_pko_send_packet_finish_pkoid().
+ *
+ * @param pko_port   The PKO port the packet is for
+ * @param queue  Queue to use
+ * @param pko_command
+ *               PKO HW command word
+ * @param packet Packet to send
+ * @param addr   Plysical address of a work queue entry or physical address to zero on complete.
+ * @param use_locking
+ *               CVMX_PKO_LOCK_NONE, CVMX_PKO_LOCK_ATOMIC_TAG, or CVMX_PKO_LOCK_CMD_QUEUE
+ *
+ * @return returns CVMX_PKO_SUCCESS on success, or error code on failure of output
+ */
+static inline cvmx_pko_return_value_t cvmx_hwpko_send_packet_finish3_pkoid(uint64_t pko_port, uint64_t queue,
+								   cvmx_pko_command_word0_t pko_command, cvmx_buf_ptr_t packet, uint64_t addr, cvmx_pko_lock_t use_locking)
+{
+	cvmx_cmd_queue_result_t result;
+	if (use_locking == CVMX_PKO_LOCK_ATOMIC_TAG)
+		cvmx_pow_tag_sw_wait();
+
+	result = cvmx_cmd_queue_write3(CVMX_CMD_QUEUE_PKO(queue), (use_locking == CVMX_PKO_LOCK_CMD_QUEUE), pko_command.u64, packet.u64, addr);
+	if (cvmx_likely(result == CVMX_CMD_QUEUE_SUCCESS)) {
+		cvmx_pko_doorbell_pkoid(pko_port, queue, 3);
+		return CVMX_PKO_SUCCESS;
+	} else if ((result == CVMX_CMD_QUEUE_NO_MEMORY) || (result == CVMX_CMD_QUEUE_FULL)) {
+		return CVMX_PKO_NO_MEMORY;
+	} else {
+		return CVMX_PKO_INVALID_QUEUE;
+	}
+}
+
+/*
+ * Obtain the number of PKO commands pending in a queue
+ *
+ * @param queue is the queue identifier to be queried
+ * @return the number of commands pending transmission or -1 on error
+ */
+int cvmx_pko_queue_pend_count( cvmx_cmd_queue_id_t queue);
+
+void cvmx_pko_set_cmd_queue_pool_buffer_count(uint64_t buffer_count);
+
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+}
+/* *INDENT-ON* */
+#endif
+
+#endif /* __CVMX_HWPKO_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-ipd.h b/arch/mips/include/asm/octeon/cvmx-ipd.h
index fac5406..67f9cc1 100644
--- a/arch/mips/include/asm/octeon/cvmx-ipd.h
+++ b/arch/mips/include/asm/octeon/cvmx-ipd.h
@@ -42,7 +42,7 @@
  *
  * Interface to the hardware Input Packet Data unit.
  *
- * <hr>$Revision: 94747 $<hr>
+ * <hr>$Revision: 95049 $<hr>
  */
 
 #ifndef __CVMX_IPD_H__
@@ -270,9 +270,6 @@ extern int cvmx_ipd_setup_red_queue(int queue, int pass_thresh, int drop_thresh)
  */
 extern int cvmx_ipd_setup_red(int pass_thresh, int drop_thresh);
 
-/* Legacy name */
-#define cvmx_helper_setup_red(p,d) cvmx_ipd_setup_red(p, d)
-
 #ifdef	__cplusplus
 /* *INDENT-OFF* */
 }
diff --git a/arch/mips/include/asm/octeon/cvmx-mdio.h b/arch/mips/include/asm/octeon/cvmx-mdio.h
index 1842129..a1a9c30 100644
--- a/arch/mips/include/asm/octeon/cvmx-mdio.h
+++ b/arch/mips/include/asm/octeon/cvmx-mdio.h
@@ -1,5 +1,5 @@
 /***********************license start***************
- * Copyright (c) 2003-2010  Cavium Inc. (support@cavium.com). All rights
+ * Copyright (c) 2003-2014  Cavium Inc. (support@cavium.com). All rights
  * reserved.
  *
  *
@@ -43,7 +43,7 @@
  * Interface to the SMI/MDIO hardware, including support for both IEEE 802.3
  * clause 22 and clause 45 operations.
  *
- * <hr>$Revision: 79232 $<hr>
+ * <hr>$Revision: 95160 $<hr>
  */
 
 #ifndef __CVMX_MIO_H__
@@ -291,25 +291,47 @@ typedef union {
 
 #define CVMX_MDIO_TIMEOUT   100000	/* 100 millisec */
 
+static inline int cvmx_mdio_bus_id_to_node(int bus_id)
+{
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+		return (bus_id >> 2) & CVMX_NODE_MASK;
+	else
+		return 0;
+}
+
+static inline int cvmx_mdio_bus_id_to_bus(int bus_id)
+{
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+		return bus_id & 3;
+	else
+		return bus_id;
+}
+
 /* Helper function to put MDIO interface into clause 45 mode */
 static inline void __cvmx_mdio_set_clause45_mode(int bus_id)
 {
 	cvmx_smix_clk_t smi_clk;
+	int node = cvmx_mdio_bus_id_to_node(bus_id);
+	int bus = cvmx_mdio_bus_id_to_bus(bus_id);
+
 	/* Put bus into clause 45 mode */
-	smi_clk.u64 = cvmx_read_csr(CVMX_SMIX_CLK(bus_id));
+	smi_clk.u64 = cvmx_read_csr_node(node, CVMX_SMIX_CLK(bus));
 	smi_clk.s.mode = 1;
 	smi_clk.s.preamble = 1;
-	cvmx_write_csr(CVMX_SMIX_CLK(bus_id), smi_clk.u64);
+	cvmx_write_csr_node(node, CVMX_SMIX_CLK(bus), smi_clk.u64);
 }
 
 /* Helper function to put MDIO interface into clause 22 mode */
 static inline void __cvmx_mdio_set_clause22_mode(int bus_id)
 {
 	cvmx_smix_clk_t smi_clk;
+	int node = cvmx_mdio_bus_id_to_node(bus_id);
+	int bus = cvmx_mdio_bus_id_to_bus(bus_id);
+
 	/* Put bus into clause 22 mode */
-	smi_clk.u64 = cvmx_read_csr(CVMX_SMIX_CLK(bus_id));
+	smi_clk.u64 = cvmx_read_csr_node(node, CVMX_SMIX_CLK(bus));
 	smi_clk.s.mode = 0;
-	cvmx_write_csr(CVMX_SMIX_CLK(bus_id), smi_clk.u64);
+	cvmx_write_csr_node(node, CVMX_SMIX_CLK(bus), smi_clk.u64);
 }
 
 /**
@@ -325,11 +347,19 @@ static inline void __cvmx_mdio_set_clause22_mode(int bus_id)
 static inline cvmx_smix_rd_dat_t __cvmx_mdio_read_rd_dat(int bus_id)
 {
 	cvmx_smix_rd_dat_t smi_rd;
-	uint64_t done = cvmx_get_cycle() + (uint64_t) CVMX_MDIO_TIMEOUT * cvmx_clock_get_rate(CVMX_CLOCK_CORE) / 1000000;
+	int node = cvmx_mdio_bus_id_to_node(bus_id);
+	int bus = cvmx_mdio_bus_id_to_bus(bus_id);
+	uint64_t done;
+
+	done = (uint64_t)CVMX_MDIO_TIMEOUT * cvmx_clock_get_rate(CVMX_CLOCK_CORE);
+	done /= 1000000;
+	done += cvmx_get_cycle();
+
 	do {
 		cvmx_wait(1000);
-		smi_rd.u64 = cvmx_read_csr(CVMX_SMIX_RD_DAT(bus_id));
+		smi_rd.u64 = cvmx_read_csr_node(node, CVMX_SMIX_RD_DAT(bus));
 	} while (smi_rd.s.pending && (cvmx_get_cycle() < done));
+
 	return smi_rd;
 }
 
@@ -349,6 +379,8 @@ static inline int cvmx_mdio_read(int bus_id, int phy_id, int location)
 #if defined(CVMX_BUILD_FOR_LINUX_KERNEL) && defined(CONFIG_PHYLIB)
 	return -1;
 #else
+	int node = cvmx_mdio_bus_id_to_node(bus_id);
+	int bus = cvmx_mdio_bus_id_to_bus(bus_id);
 	cvmx_smix_cmd_t smi_cmd;
 	cvmx_smix_rd_dat_t smi_rd;
 
@@ -359,7 +391,7 @@ static inline int cvmx_mdio_read(int bus_id, int phy_id, int location)
 	smi_cmd.s.phy_op = MDIO_CLAUSE_22_READ;
 	smi_cmd.s.phy_adr = phy_id;
 	smi_cmd.s.reg_adr = location;
-	cvmx_write_csr(CVMX_SMIX_CMD(bus_id), smi_cmd.u64);
+	cvmx_write_csr_node(node, CVMX_SMIX_CMD(bus), smi_cmd.u64);
 
 	smi_rd = __cvmx_mdio_read_rd_dat(bus_id);
 	if (smi_rd.s.val)
@@ -387,6 +419,8 @@ static inline int cvmx_mdio_write(int bus_id, int phy_id, int location, int val)
 #if defined(CVMX_BUILD_FOR_LINUX_KERNEL) && defined(CONFIG_PHYLIB)
 	return -1;
 #else
+	int node = cvmx_mdio_bus_id_to_node(bus_id);
+	int bus = cvmx_mdio_bus_id_to_bus(bus_id);
 	cvmx_smix_cmd_t smi_cmd;
 	cvmx_smix_wr_dat_t smi_wr;
 
@@ -395,15 +429,17 @@ static inline int cvmx_mdio_write(int bus_id, int phy_id, int location, int val)
 
 	smi_wr.u64 = 0;
 	smi_wr.s.dat = val;
-	cvmx_write_csr(CVMX_SMIX_WR_DAT(bus_id), smi_wr.u64);
+	cvmx_write_csr_node(node, CVMX_SMIX_WR_DAT(bus), smi_wr.u64);
 
 	smi_cmd.u64 = 0;
 	smi_cmd.s.phy_op = MDIO_CLAUSE_22_WRITE;
 	smi_cmd.s.phy_adr = phy_id;
 	smi_cmd.s.reg_adr = location;
-	cvmx_write_csr(CVMX_SMIX_CMD(bus_id), smi_cmd.u64);
+	cvmx_write_csr_node(node, CVMX_SMIX_CMD(bus), smi_cmd.u64);
 
-	if (CVMX_WAIT_FOR_FIELD64(CVMX_SMIX_WR_DAT(bus_id), cvmx_smix_wr_dat_t, pending, ==, 0, CVMX_MDIO_TIMEOUT))
+	if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_SMIX_WR_DAT(bus),
+				       cvmx_smix_wr_dat_t, pending, ==, 0,
+				       CVMX_MDIO_TIMEOUT))
 		return -1;
 
 	return 0;
@@ -432,6 +468,8 @@ static inline int cvmx_mdio_45_read(int bus_id, int phy_id, int device, int loca
 	cvmx_smix_cmd_t smi_cmd;
 	cvmx_smix_rd_dat_t smi_rd;
 	cvmx_smix_wr_dat_t smi_wr;
+	int node = cvmx_mdio_bus_id_to_node(bus_id);
+	int bus = cvmx_mdio_bus_id_to_bus(bus_id);
 
 	if (!octeon_has_feature(OCTEON_FEATURE_MDIO_CLAUSE_45))
 		return -1;
@@ -440,15 +478,17 @@ static inline int cvmx_mdio_45_read(int bus_id, int phy_id, int device, int loca
 
 	smi_wr.u64 = 0;
 	smi_wr.s.dat = location;
-	cvmx_write_csr(CVMX_SMIX_WR_DAT(bus_id), smi_wr.u64);
+	cvmx_write_csr_node(node, CVMX_SMIX_WR_DAT(bus), smi_wr.u64);
 
 	smi_cmd.u64 = 0;
 	smi_cmd.s.phy_op = MDIO_CLAUSE_45_ADDRESS;
 	smi_cmd.s.phy_adr = phy_id;
 	smi_cmd.s.reg_adr = device;
-	cvmx_write_csr(CVMX_SMIX_CMD(bus_id), smi_cmd.u64);
+	cvmx_write_csr_node(node, CVMX_SMIX_CMD(bus), smi_cmd.u64);
 
-	if (CVMX_WAIT_FOR_FIELD64(CVMX_SMIX_WR_DAT(bus_id), cvmx_smix_wr_dat_t, pending, ==, 0, CVMX_MDIO_TIMEOUT)) {
+	if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_SMIX_WR_DAT(bus),
+				       cvmx_smix_wr_dat_t, pending, ==, 0,
+				       CVMX_MDIO_TIMEOUT)) {
 		cvmx_dprintf("cvmx_mdio_45_read: bus_id %d phy_id %2d device %2d register %2d   TIME OUT(address)\n", bus_id, phy_id, device, location);
 		return -1;
 	}
@@ -457,7 +497,7 @@ static inline int cvmx_mdio_45_read(int bus_id, int phy_id, int device, int loca
 	smi_cmd.s.phy_op = MDIO_CLAUSE_45_READ;
 	smi_cmd.s.phy_adr = phy_id;
 	smi_cmd.s.reg_adr = device;
-	cvmx_write_csr(CVMX_SMIX_CMD(bus_id), smi_cmd.u64);
+	cvmx_write_csr_node(node, CVMX_SMIX_CMD(bus), smi_cmd.u64);
 
 	smi_rd = __cvmx_mdio_read_rd_dat(bus_id);
 	if (smi_rd.s.pending) {
@@ -495,6 +535,8 @@ static inline int cvmx_mdio_45_write(int bus_id, int phy_id, int device, int loc
 #else
 	cvmx_smix_cmd_t smi_cmd;
 	cvmx_smix_wr_dat_t smi_wr;
+	int node = cvmx_mdio_bus_id_to_node(bus_id);
+	int bus = cvmx_mdio_bus_id_to_bus(bus_id);
 
 	if (!octeon_has_feature(OCTEON_FEATURE_MDIO_CLAUSE_45))
 		return -1;
@@ -503,28 +545,32 @@ static inline int cvmx_mdio_45_write(int bus_id, int phy_id, int device, int loc
 
 	smi_wr.u64 = 0;
 	smi_wr.s.dat = location;
-	cvmx_write_csr(CVMX_SMIX_WR_DAT(bus_id), smi_wr.u64);
+	cvmx_write_csr_node(node, CVMX_SMIX_WR_DAT(bus), smi_wr.u64);
 
 	smi_cmd.u64 = 0;
 	smi_cmd.s.phy_op = MDIO_CLAUSE_45_ADDRESS;
 	smi_cmd.s.phy_adr = phy_id;
 	smi_cmd.s.reg_adr = device;
-	cvmx_write_csr(CVMX_SMIX_CMD(bus_id), smi_cmd.u64);
+	cvmx_write_csr_node(node, CVMX_SMIX_CMD(bus), smi_cmd.u64);
 
-	if (CVMX_WAIT_FOR_FIELD64(CVMX_SMIX_WR_DAT(bus_id), cvmx_smix_wr_dat_t, pending, ==, 0, CVMX_MDIO_TIMEOUT))
+	if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_SMIX_WR_DAT(bus),
+				       cvmx_smix_wr_dat_t, pending, ==, 0,
+				       CVMX_MDIO_TIMEOUT))
 		return -1;
 
 	smi_wr.u64 = 0;
 	smi_wr.s.dat = val;
-	cvmx_write_csr(CVMX_SMIX_WR_DAT(bus_id), smi_wr.u64);
+	cvmx_write_csr_node(node, CVMX_SMIX_WR_DAT(bus), smi_wr.u64);
 
 	smi_cmd.u64 = 0;
 	smi_cmd.s.phy_op = MDIO_CLAUSE_45_WRITE;
 	smi_cmd.s.phy_adr = phy_id;
 	smi_cmd.s.reg_adr = device;
-	cvmx_write_csr(CVMX_SMIX_CMD(bus_id), smi_cmd.u64);
+	cvmx_write_csr_node(node, CVMX_SMIX_CMD(bus), smi_cmd.u64);
 
-	if (CVMX_WAIT_FOR_FIELD64(CVMX_SMIX_WR_DAT(bus_id), cvmx_smix_wr_dat_t, pending, ==, 0, CVMX_MDIO_TIMEOUT))
+	if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_SMIX_WR_DAT(bus),
+				       cvmx_smix_wr_dat_t, pending, ==, 0,
+				       CVMX_MDIO_TIMEOUT))
 		return -1;
 
 	return 0;
diff --git a/arch/mips/include/asm/octeon/cvmx-pip.h b/arch/mips/include/asm/octeon/cvmx-pip.h
index 0b1d321..dd1a6d8 100644
--- a/arch/mips/include/asm/octeon/cvmx-pip.h
+++ b/arch/mips/include/asm/octeon/cvmx-pip.h
@@ -42,14 +42,13 @@
  *
  * Interface to the hardware Packet Input Processing unit.
  *
- * <hr>$Revision: 94787 $<hr>
+ * <hr>$Revision: 95258 $<hr>
  */
 
 #ifndef __CVMX_PIP_H__
 #define __CVMX_PIP_H__
 
 #include "cvmx-wqe.h"
-#include "cvmx-fpa.h"
 #include "cvmx-pki.h"
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include "cvmx-pip-defs.h"
diff --git a/arch/mips/include/asm/octeon/cvmx-pki.h b/arch/mips/include/asm/octeon/cvmx-pki.h
index 5d885fa..826333a 100644
--- a/arch/mips/include/asm/octeon/cvmx-pki.h
+++ b/arch/mips/include/asm/octeon/cvmx-pki.h
@@ -49,11 +49,11 @@
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
 #include <asm/octeon/cvmx-pki-defs.h>
-#include <asm/octeon/cvmx-fpa.h>
+#include <asm/octeon/cvmx-fpa3.h>
 #include <asm/octeon/cvmx-helper-util.h>
 #include <asm/octeon/cvmx-helper-cfg.h>
 #else
-#include "cvmx-fpa.h"
+#include "cvmx-fpa3.h"
 #include "cvmx-helper-util.h"
 #include "cvmx-helper-cfg.h"
 #endif
@@ -1043,11 +1043,21 @@ void cvmx_pki_endis_l2_errs(int node, int pknd, bool l2len_err,
  *			0 -- Do not strip L2 FCS.
  */
 void cvmx_pki_endis_fcs_check(int node, int pknd, bool fcs_chk, bool fcs_strip);
+/**
+ * Modifies maximum frame length to check.
+ * It modifies the global frame length set used by this port, any other
+ * port using the same set will get affected too.
+ * @param node   node number
+ * @param port	 ipd port for which to modify max len.
+ * @param max_size	maximum frame length
+ */
+void cvmx_pki_set_max_frm_len(int node, int port, uint32_t max_size);
 void cvmx_pki_get_style_config(int node, int style, uint64_t cl_mask,
 			       struct cvmx_pki_style_config *style_cfg);
 void cvmx_pki_config_port(int node, int ipd_port, struct cvmx_pki_port_config *port_cfg);
 void cvmx_pki_get_port_config(int node, int ipd_port, struct cvmx_pki_port_config *port_cfg);
 void cvmx_pki_reset(int node);
+int cvmx_pki_get_pkind_style(int node, int pkind);
 void __cvmx_pki_free_ptr(int node);
 
 #ifdef	__cplusplus
diff --git a/arch/mips/include/asm/octeon/cvmx-pko.h b/arch/mips/include/asm/octeon/cvmx-pko.h
deleted file mode 100644
index 5b61029..0000000
--- a/arch/mips/include/asm/octeon/cvmx-pko.h
+++ /dev/null
@@ -1,665 +0,0 @@
-/***********************license start***************
- * Copyright (c) 2003-2010  Cavium Inc. (support@cavium.com). All rights
- * reserved.
- *
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are
- * met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *
- *   * Redistributions in binary form must reproduce the above
- *     copyright notice, this list of conditions and the following
- *     disclaimer in the documentation and/or other materials provided
- *     with the distribution.
-
- *   * Neither the name of Cavium Inc. nor the names of
- *     its contributors may be used to endorse or promote products
- *     derived from this software without specific prior written
- *     permission.
-
- * This Software, including technical data, may be subject to U.S. export  control
- * laws, including the U.S. Export Administration Act and its  associated
- * regulations, and may be subject to export or import  regulations in other
- * countries.
-
- * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
- * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
- * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
- * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
- * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
- * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
- * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
- * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
- * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
- * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
- ***********************license end**************************************/
-
-/**
- * @file
- *
- * Interface to the hardware Packet Output unit.
- *
- * Starting with SDK 1.7.0, the PKO output functions now support
- * two types of locking. CVMX_PKO_LOCK_ATOMIC_TAG continues to
- * function similarly to previous SDKs by using POW atomic tags
- * to preserve ordering and exclusivity. As a new option, you
- * can now pass CVMX_PKO_LOCK_CMD_QUEUE which uses a ll/sc
- * memory based locking instead. This locking has the advantage
- * of not affecting the tag state but doesn't preserve packet
- * ordering. CVMX_PKO_LOCK_CMD_QUEUE is appropriate in most
- * generic code while CVMX_PKO_LOCK_CMD_QUEUE should be used
- * with hand tuned fast path code.
- *
- * Some of other SDK differences visible to the command command
- * queuing:
- * - PKO indexes are no longer stored in the FAU. A large
- *   percentage of the FAU register block used to be tied up
- *   maintaining PKO queue pointers. These are now stored in a
- *   global named block.
- * - The PKO <b>use_locking</b> parameter can now have a global
- *   effect. Since all application use the same named block,
- *   queue locking correctly applies across all operating
- *   systems when using CVMX_PKO_LOCK_CMD_QUEUE.
- * - PKO 3 word commands are now supported. Use
- *   cvmx_pko_send_packet_finish3().
- *
- */
-
-#ifndef __CVMX_PKO_H__
-#define __CVMX_PKO_H__
-
-#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
-#include "cvmx-pko-defs.h"
-#include <asm/octeon/cvmx-fau.h>
-#include <asm/octeon/cvmx-fpa.h>
-#include <asm/octeon/cvmx-pow.h>
-#include <asm/octeon/cvmx-cmd-queue.h>
-#include <asm/octeon/cvmx-helper.h>
-#include <asm/octeon/cvmx-helper-cfg.h>
-#include <asm/octeon/cvmx-helper-pko.h>
-#else
-#include "cvmx-fau.h"
-#include "cvmx-fpa.h"
-#include "cvmx-pow.h"
-#include "cvmx-pko3.h"	/* for back-comp */
-#include "cvmx-cmd-queue.h"
-#include "cvmx-helper.h"
-#include "cvmx-helper-util.h"
-#include "cvmx-helper-cfg.h"
-#include "cvmx-helper-pko.h"
-#endif
-
-#ifdef	__cplusplus
-/* *INDENT-OFF* */
-extern "C" {
-/* *INDENT-ON* */
-#endif
-
-/* Adjust the command buffer size by 1 word so that in the case of using only
-** two word PKO commands no command words stradle buffers.  The useful values
-** for this are 0 and 1. */
-#define CVMX_PKO_COMMAND_BUFFER_SIZE_ADJUST (1)
-
-
-#define CVMX_PKO_MAX_OUTPUT_QUEUES_STATIC 256
-#define CVMX_PKO_MAX_OUTPUT_QUEUES      ((OCTEON_IS_MODEL(OCTEON_CN31XX) || \
-					  OCTEON_IS_MODEL(OCTEON_CN3010) || \
-					  OCTEON_IS_MODEL(OCTEON_CN3005) || \
-					  OCTEON_IS_MODEL(OCTEON_CN50XX)) ? \
-					  32 :				    \
-					 (OCTEON_IS_MODEL(OCTEON_CN58XX) || \
-					  OCTEON_IS_MODEL(OCTEON_CN56XX) || \
-					  OCTEON_IS_MODEL(OCTEON_CN52XX) || \
-					  OCTEON_IS_OCTEON2() || \
-					  OCTEON_IS_MODEL(OCTEON_CN70XX)) ? \
-					  256 : 128)
-#define CVMX_PKO_NUM_OUTPUT_PORTS       ((OCTEON_IS_MODEL(OCTEON_CN63XX)) ? 44 : (OCTEON_IS_MODEL(OCTEON_CN66XX) ? 48 : 40))
-#define CVMX_PKO_MEM_QUEUE_PTRS_ILLEGAL_PID 63	/* use this for queues that are not used */
-#define CVMX_PKO_QUEUE_STATIC_PRIORITY  9
-#define CVMX_PKO_ILLEGAL_QUEUE  0xFFFF
-#define CVMX_PKO_MAX_QUEUE_DEPTH 0
-
-typedef enum {
-	CVMX_PKO_SUCCESS,
-	CVMX_PKO_INVALID_PORT,
-	CVMX_PKO_INVALID_QUEUE,
-	CVMX_PKO_INVALID_PRIORITY,
-	CVMX_PKO_NO_MEMORY,
-	CVMX_PKO_PORT_ALREADY_SETUP,
-	CVMX_PKO_CMD_QUEUE_INIT_ERROR
-} cvmx_pko_return_value_t;
-
-/**
- * This enumeration represents the differnet locking modes supported by PKO.
- */
-typedef enum {
-	CVMX_PKO_LOCK_NONE = 0,
-				    /**< PKO doesn't do any locking. It is the responsibility
-                                        of the application to make sure that no other core is
-                                        accessing the same queue at the same time */
-	CVMX_PKO_LOCK_ATOMIC_TAG = 1,
-				    /**< PKO performs an atomic tagswitch to insure exclusive
-                                        access to the output queue. This will maintain
-                                        packet ordering on output */
-	CVMX_PKO_LOCK_CMD_QUEUE = 2,
-				    /**< PKO uses the common command queue locks to insure
-                                        exclusive access to the output queue. This is a memory
-                                        based ll/sc. This is the most portable locking
-                                        mechanism */
-} cvmx_pko_lock_t;
-
-typedef struct cvmx_pko_port_status {
-	uint32_t packets;
-	uint64_t octets;
-	uint64_t doorbell;
-} cvmx_pko_port_status_t;
-
-/**
- * This structure defines the address to use on a packet enqueue
- */
-typedef union {
-	uint64_t u64;
-	struct {
-#ifdef __BIG_ENDIAN_BITFIELD
-		cvmx_mips_space_t mem_space:2;
-						/**< Must CVMX_IO_SEG */
-		uint64_t reserved:13;		/**< Must be zero */
-		uint64_t is_io:1;		/**< Must be one */
-		uint64_t did:8;			/**< The ID of the device on the non-coherent bus */
-		uint64_t reserved2:4;		/**< Must be zero */
-		uint64_t reserved3:15;		/**< Must be zero */
-		uint64_t port:9;		/**< The hardware must have the output port in addition to the output queue */
-		uint64_t queue:9;		/**< The output queue to send the packet to (0-127 are legal) */
-		uint64_t reserved4:3;		/**< Must be zero */
-#else
-		uint64_t reserved4:3;
-		uint64_t queue:9;
-		uint64_t port:9;
-		uint64_t reserved3:15;
-		uint64_t reserved2:4;
-		uint64_t did:8;
-		uint64_t is_io:1;
-		uint64_t reserved:13;
-		cvmx_mips_space_t mem_space:2;
-#endif
-	} s;
-} cvmx_pko_doorbell_address_t;
-
-/**
- * Structure of the first packet output command word.
- */
-typedef union {
-	uint64_t u64;
-	struct {
-#ifdef __BIG_ENDIAN_BITFIELD
-		cvmx_fau_op_size_t size1:2;
-					     /**< The size of the reg1 operation - could be 8, 16, 32, or 64 bits */
-		cvmx_fau_op_size_t size0:2;
-					     /**< The size of the reg0 operation - could be 8, 16, 32, or 64 bits */
-		uint64_t subone1:1;	     /**< If set, subtract 1, if clear, subtract packet size */
-		uint64_t reg1:11;	     /**< The register, subtract will be done if reg1 is non-zero */
-		uint64_t subone0:1;	     /**< If set, subtract 1, if clear, subtract packet size */
-		uint64_t reg0:11;	     /**< The register, subtract will be done if reg0 is non-zero */
-		uint64_t le:1;		     /**< When set, interpret segment pointer and segment bytes in little endian order */
-		uint64_t n2:1;		     /**< When set, packet data not allocated in L2 cache by PKO */
-		uint64_t wqp:1;		     /**< If set and rsp is set, word3 contains a pointer to a work queue entry */
-		uint64_t rsp:1;		     /**< If set, the hardware will send a response when done */
-		uint64_t gather:1;	     /**< If set, the supplied pkt_ptr is really a pointer to a list of pkt_ptr's */
-		uint64_t ipoffp1:7;	     /**< If ipoffp1 is non zero, (ipoffp1-1) is the number of bytes to IP header,
-                                                and the hardware will calculate and insert the  UDP/TCP checksum */
-		uint64_t ignore_i:1;	     /**< If set, ignore the I bit (force to zero) from all pointer structures */
-		uint64_t dontfree:1;	     /**< If clear, the hardware will attempt to free the buffers containing the packet */
-		uint64_t segs:6;	     /**< The total number of segs in the packet, if gather set, also gather list length */
-		uint64_t total_bytes:16;
-					     /**< Including L2, but no trailing CRC */
-#else
-		uint64_t total_bytes:16;
-		uint64_t segs:6;
-		uint64_t dontfree:1;
-		uint64_t ignore_i:1;
-		uint64_t ipoffp1:7;
-		uint64_t gather:1;
-		uint64_t rsp:1;
-		uint64_t wqp:1;
-		uint64_t n2:1;
-		uint64_t le:1;
-		uint64_t reg0:11;
-		uint64_t subone0:1;
-		uint64_t reg1:11;
-		uint64_t subone1:1;
-		cvmx_fau_op_size_t size0:2;
-		cvmx_fau_op_size_t size1:2;
-#endif
-	} s;
-} cvmx_pko_command_word0_t;
-
-/**
- * Call before any other calls to initialize the packet
- * output system.
- */
-// extern void cvmx_pko_initialize_global(void);
-// extern int cvmx_pko_initialize_local(void);
-
-extern void cvmx_pko_hw_init(uint8_t pool, unsigned bufsize);
-
-/**
- * Enables the packet output hardware. It must already be
- * configured.
- */
-extern void cvmx_pko_enable(void);
-
-/**
- * Disables the packet output. Does not affect any configuration.
- */
-extern void cvmx_pko_disable(void);
-
-/**
- * Shutdown and free resources required by packet output.
- */
-
-extern void cvmx_pko_shutdown(void);
-
-/**
- * Configure a output port and the associated queues for use.
- *
- * @param port       Port to configure.
- * @param base_queue First queue number to associate with this port.
- * @param num_queues Number of queues t oassociate with this port
- * @param priority   Array of priority levels for each queue. Values are
- *                   allowed to be 1-8. A value of 8 get 8 times the traffic
- *                   of a value of 1. There must be num_queues elements in the
- *                   array.
- */
-extern cvmx_pko_return_value_t cvmx_pko_config_port(int port, int base_queue, int num_queues, const uint8_t priority[]);
-
-/**
- * Ring the packet output doorbell. This tells the packet
- * output hardware that "len" command words have been added
- * to its pending list.  This command includes the required
- * CVMX_SYNCWS before the doorbell ring.
- *
- * WARNING: This function may have to look up the proper PKO port in
- * the IPD port to PKO port map, and is thus slower than calling
- * cvmx_pko_doorbell_pkoid() directly if the PKO port identifier is
- * known.
- *
- * @param ipd_port   The IPD port corresponding the to pko port the packet is for
- * @param queue  Queue the packet is for
- * @param len    Length of the command in 64 bit words
- */
-static inline void cvmx_pko_doorbell(uint64_t ipd_port, uint64_t queue, uint64_t len)
-{
-	cvmx_pko_doorbell_address_t ptr;
-	uint64_t pko_port;
-
-	pko_port = ipd_port;
-	if (octeon_has_feature(OCTEON_FEATURE_PKND))
-		pko_port = cvmx_helper_cfg_ipd2pko_port_base(ipd_port);
-
-	ptr.u64 = 0;
-	ptr.s.mem_space = CVMX_IO_SEG;
-	ptr.s.did = CVMX_OCT_DID_PKT_SEND;
-	ptr.s.is_io = 1;
-	ptr.s.port = pko_port;
-	ptr.s.queue = queue;
-	CVMX_SYNCWS;		/* Need to make sure output queue data is in DRAM before doorbell write */
-	cvmx_write_io(ptr.u64, len);
-}
-
-/**
- * Prepare to send a packet.  This may initiate a tag switch to
- * get exclusive access to the output queue structure, and
- * performs other prep work for the packet send operation.
- *
- * cvmx_pko_send_packet_finish() MUST be called after this function is called,
- * and must be called with the same port/queue/use_locking arguments.
- *
- * The use_locking parameter allows the caller to use three
- * possible locking modes.
- * - CVMX_PKO_LOCK_NONE
- *      - PKO doesn't do any locking. It is the responsibility
- *          of the application to make sure that no other core
- *          is accessing the same queue at the same time.
- * - CVMX_PKO_LOCK_ATOMIC_TAG
- *      - PKO performs an atomic tagswitch to insure exclusive
- *          access to the output queue. This will maintain
- *          packet ordering on output.
- * - CVMX_PKO_LOCK_CMD_QUEUE
- *      - PKO uses the common command queue locks to insure
- *          exclusive access to the output queue. This is a
- *          memory based ll/sc. This is the most portable
- *          locking mechanism.
- *
- * NOTE: If atomic locking is used, the POW entry CANNOT be
- * descheduled, as it does not contain a valid WQE pointer.
- *
- * @param port   Port to send it on, this can be either IPD port or PKO
- * 		 port.
- * @param queue  Queue to use
- * @param use_locking
- *               CVMX_PKO_LOCK_NONE, CVMX_PKO_LOCK_ATOMIC_TAG, or CVMX_PKO_LOCK_CMD_QUEUE
- */ static inline void cvmx_pko_send_packet_prepare(uint64_t port __attribute__ ((unused)), uint64_t queue, cvmx_pko_lock_t use_locking)
-{
-	if (use_locking == CVMX_PKO_LOCK_ATOMIC_TAG) {
-		/* Must do a full switch here to handle all cases.  We use a fake WQE pointer, as the POW does
-		 ** not access this memory.  The WQE pointer and group are only used if this work is descheduled,
-		 ** which is not supported by the cvmx_pko_send_packet_prepare/cvmx_pko_send_packet_finish combination.
-		 ** Note that this is a special case in which these fake values can be used - this is not a general technique.
-		 */
-		uint32_t tag = CVMX_TAG_SW_BITS_INTERNAL << CVMX_TAG_SW_SHIFT | CVMX_TAG_SUBGROUP_PKO << CVMX_TAG_SUBGROUP_SHIFT | (CVMX_TAG_SUBGROUP_MASK & queue);
-		cvmx_pow_tag_sw_full((cvmx_wqe_t *) cvmx_phys_to_ptr(0x80), tag, CVMX_POW_TAG_TYPE_ATOMIC, 0);
-	}
-}
-
-#define cvmx_pko_send_packet_prepare_pkoid	cvmx_pko_send_packet_prepare
-
-/**
- * Complete packet output. cvmx_pko_send_packet_prepare() must be called exactly once before this,
- * and the same parameters must be passed to both cvmx_pko_send_packet_prepare() and
- * cvmx_pko_send_packet_finish().
- *
- * WARNING: This function may have to look up the proper PKO port in
- * the IPD port to PKO port map, and is thus slower than calling
- * cvmx_pko_send_packet_finish_pkoid() directly if the PKO port
- * identifier is known.
- *
- * @param ipd_port   The IPD port corresponding the to pko port the packet is for
- * @param queue  Queue to use
- * @param pko_command
- *               PKO HW command word
- * @param packet Packet to send
- * @param use_locking
- *               CVMX_PKO_LOCK_NONE, CVMX_PKO_LOCK_ATOMIC_TAG, or CVMX_PKO_LOCK_CMD_QUEUE
- *
- * @return returns CVMX_PKO_SUCCESS on success, or error code on failure of output
- */
-static inline cvmx_pko_return_value_t cvmx_hwpko_send_packet_finish(uint64_t ipd_port, uint64_t queue,
-								    cvmx_pko_command_word0_t pko_command, cvmx_buf_ptr_t packet, cvmx_pko_lock_t use_locking)
-{
-	cvmx_cmd_queue_result_t result;
-
-	if (use_locking == CVMX_PKO_LOCK_ATOMIC_TAG)
-		cvmx_pow_tag_sw_wait();
-
-	result = cvmx_cmd_queue_write2(CVMX_CMD_QUEUE_PKO(queue), (use_locking == CVMX_PKO_LOCK_CMD_QUEUE), pko_command.u64, packet.u64);
-	if (cvmx_likely(result == CVMX_CMD_QUEUE_SUCCESS)) {
-		cvmx_pko_doorbell(ipd_port, queue, 2);
-		return CVMX_PKO_SUCCESS;
-	} else if ((result == CVMX_CMD_QUEUE_NO_MEMORY) || (result == CVMX_CMD_QUEUE_FULL)) {
-		return CVMX_PKO_NO_MEMORY;
-	} else {
-		return CVMX_PKO_INVALID_QUEUE;
-	}
-}
-
-/**
- * Complete packet output. cvmx_pko_send_packet_prepare() must be called exactly once before this,
- * and the same parameters must be passed to both cvmx_pko_send_packet_prepare() and
- * cvmx_pko_send_packet_finish().
- *
- * WARNING: This function may have to look up the proper PKO port in
- * the IPD port to PKO port map, and is thus slower than calling
- * cvmx_pko_send_packet_finish3_pkoid() directly if the PKO port
- * identifier is known.
- *
- * @param ipd_port   The IPD port corresponding the to pko port the packet is for
- * @param queue  Queue to use
- * @param pko_command
- *               PKO HW command word
- * @param packet Packet to send
- * @param addr   Plysical address of a work queue entry or physical address to zero on complete.
- * @param use_locking
- *               CVMX_PKO_LOCK_NONE, CVMX_PKO_LOCK_ATOMIC_TAG, or CVMX_PKO_LOCK_CMD_QUEUE
- *
- * @return returns CVMX_PKO_SUCCESS on success, or error code on failure of output
- */
-static inline cvmx_pko_return_value_t cvmx_hwpko_send_packet_finish3(uint64_t ipd_port, uint64_t queue,
-							     cvmx_pko_command_word0_t pko_command, cvmx_buf_ptr_t packet, uint64_t addr, cvmx_pko_lock_t use_locking)
-{
-	cvmx_cmd_queue_result_t result;
-	if (use_locking == CVMX_PKO_LOCK_ATOMIC_TAG)
-		cvmx_pow_tag_sw_wait();
-
-	result = cvmx_cmd_queue_write3(CVMX_CMD_QUEUE_PKO(queue), (use_locking == CVMX_PKO_LOCK_CMD_QUEUE), pko_command.u64, packet.u64, addr);
-	if (cvmx_likely(result == CVMX_CMD_QUEUE_SUCCESS)) {
-		cvmx_pko_doorbell(ipd_port, queue, 3);
-		return CVMX_PKO_SUCCESS;
-	} else if ((result == CVMX_CMD_QUEUE_NO_MEMORY) || (result == CVMX_CMD_QUEUE_FULL)) {
-		return CVMX_PKO_NO_MEMORY;
-	} else {
-		return CVMX_PKO_INVALID_QUEUE;
-	}
-}
-
-/**
- * Get the first pko_port for the (interface, index)
- *
- * @param interface
- * @param index
- */
-extern int cvmx_pko_get_base_pko_port(int interface, int index);
-
-/**
- * Get the number of pko_ports for the (interface, index)
- *
- * @param interface
- * @param index
- */
-extern int cvmx_pko_get_num_pko_ports(int interface, int index);
-
-/**
- * For a given port number, return the base pko output queue
- * for the port.
- *
- * @param port   IPD port number
- * @return Base output queue
- */
-extern int cvmx_pko_get_base_queue(int port);
-
-/**
- * For a given port number, return the number of pko output queues.
- *
- * @param port   IPD port number
- * @return Number of output queues
- */
-extern int cvmx_pko_get_num_queues(int port);
-
-/**
- * Sets the internal FPA pool data structure for PKO comamnd queue.
- * @param pool	fpa pool number yo use
- * @param buffer_size	buffer size of pool
- * @param buffer_count	number of buufers to allocate to pool
- */
-void cvmx_pko_set_cmd_que_pool_config(int64_t pool, uint64_t buffer_size,
-					   uint64_t buffer_count);
-//FIXME- reconsider:
-// Helper should setup the pool, then call PKO (pko_init?) with the
-// pool number to use.
-
-/**
- * Get the status counters for a port.
- *
- * @param ipd_port Port number (ipd_port) to get statistics for.
- * @param clear    Set to 1 to clear the counters after they are read
- * @param status   Where to put the results.
- *
- * Note:
- *     - Only the doorbell for the base queue of the ipd_port is
- *       collected.
- *     - Retrieving the stats involves writing the index through
- *       CVMX_PKO_REG_READ_IDX and reading the stat CSRs, in that
- *       order. It is not MP-safe and caller should guarantee
- *       atomicity.
- */
-void cvmx_pko_get_port_status(uint64_t ipd_port, uint64_t clear, cvmx_pko_port_status_t * status);
-
-/**
- * Rate limit a PKO port to a max packets/sec. This function is only
- * supported on CN57XX, CN56XX, CN55XX, and CN54XX.
- *
- * @param port      Port to rate limit
- * @param packets_s Maximum packet/sec
- * @param burst     Maximum number of packets to burst in a row before rate
- *                  limiting cuts in.
- *
- * @return Zero on success, negative on failure
- */
-extern int cvmx_pko_rate_limit_packets(int port, int packets_s, int burst);
-
-/**
- * Rate limit a PKO port to a max bits/sec. This function is only
- * supported on CN57XX, CN56XX, CN55XX, and CN54XX.
- *
- * @param port   Port to rate limit
- * @param bits_s PKO rate limit in bits/sec
- * @param burst  Maximum number of bits to burst before rate
- *               limiting cuts in.
- *
- * @return Zero on success, negative on failure
- */
-extern int cvmx_pko_rate_limit_bits(int port, uint64_t bits_s, int burst);
-
-/**
- * @INTERNAL
- *
- * Retrieve the PKO pipe number for a port
- *
- * @param interface
- * @param index
- *
- * @return negative on error.
- *
- * This applies only to the non-loopback interfaces.
- *
- */
-extern int __cvmx_pko_get_pipe(int interface, int index);
-
-/**
- * For a given PKO port number, return the base output queue
- * for the port.
- *
- * @param pko_port   PKO port number
- * @return           Base output queue
- */
-extern int cvmx_pko_get_base_queue_pkoid(int pko_port);
-
-/**
- * For a given PKO port number, return the number of output queues
- * for the port.
- *
- * @param pko_port	PKO port number
- * @return		the number of output queues
- */
-extern int cvmx_pko_get_num_queues_pkoid(int pko_port);
-
-/**
- * Ring the packet output doorbell. This tells the packet
- * output hardware that "len" command words have been added
- * to its pending list.  This command includes the required
- * CVMX_SYNCWS before the doorbell ring.
- *
- * @param pko_port   Port the packet is for
- * @param queue  Queue the packet is for
- * @param len    Length of the command in 64 bit words
- */
-static inline void cvmx_pko_doorbell_pkoid(uint64_t pko_port, uint64_t queue, uint64_t len)
-{
-	cvmx_pko_doorbell_address_t ptr;
-
-	ptr.u64 = 0;
-	ptr.s.mem_space = CVMX_IO_SEG;
-	ptr.s.did = CVMX_OCT_DID_PKT_SEND;
-	ptr.s.is_io = 1;
-	ptr.s.port = pko_port;
-	ptr.s.queue = queue;
-	CVMX_SYNCWS;		/* Need to make sure output queue data is in DRAM before doorbell write */
-	cvmx_write_io(ptr.u64, len);
-}
-
-/**
- * Complete packet output. cvmx_pko_send_packet_prepare() must be called exactly once before this,
- * and the same parameters must be passed to both cvmx_pko_send_packet_prepare() and
- * cvmx_pko_send_packet_finish_pkoid().
- *
- * @param pko_port   Port to send it on
- * @param queue  Queue to use
- * @param pko_command
- *               PKO HW command word
- * @param packet Packet to send
- * @param use_locking
- *               CVMX_PKO_LOCK_NONE, CVMX_PKO_LOCK_ATOMIC_TAG, or CVMX_PKO_LOCK_CMD_QUEUE
- *
- * @return returns CVMX_PKO_SUCCESS on success, or error code on failure of output
- */
-static inline cvmx_pko_return_value_t cvmx_hwpko_send_packet_finish_pkoid(int pko_port, uint64_t queue,
-								  cvmx_pko_command_word0_t pko_command, cvmx_buf_ptr_t packet, cvmx_pko_lock_t use_locking)
-{
-	cvmx_cmd_queue_result_t result;
-	if (use_locking == CVMX_PKO_LOCK_ATOMIC_TAG)
-		cvmx_pow_tag_sw_wait();
-
-	result = cvmx_cmd_queue_write2(CVMX_CMD_QUEUE_PKO(queue), (use_locking == CVMX_PKO_LOCK_CMD_QUEUE), pko_command.u64, packet.u64);
-	if (cvmx_likely(result == CVMX_CMD_QUEUE_SUCCESS)) {
-		cvmx_pko_doorbell_pkoid(pko_port, queue, 2);
-		return CVMX_PKO_SUCCESS;
-	} else if ((result == CVMX_CMD_QUEUE_NO_MEMORY) || (result == CVMX_CMD_QUEUE_FULL)) {
-		return CVMX_PKO_NO_MEMORY;
-	} else {
-		return CVMX_PKO_INVALID_QUEUE;
-	}
-}
-
-/**
- * Complete packet output. cvmx_pko_send_packet_prepare() must be called exactly once before this,
- * and the same parameters must be passed to both cvmx_pko_send_packet_prepare() and
- * cvmx_pko_send_packet_finish_pkoid().
- *
- * @param pko_port   The PKO port the packet is for
- * @param queue  Queue to use
- * @param pko_command
- *               PKO HW command word
- * @param packet Packet to send
- * @param addr   Plysical address of a work queue entry or physical address to zero on complete.
- * @param use_locking
- *               CVMX_PKO_LOCK_NONE, CVMX_PKO_LOCK_ATOMIC_TAG, or CVMX_PKO_LOCK_CMD_QUEUE
- *
- * @return returns CVMX_PKO_SUCCESS on success, or error code on failure of output
- */
-static inline cvmx_pko_return_value_t cvmx_hwpko_send_packet_finish3_pkoid(uint64_t pko_port, uint64_t queue,
-								   cvmx_pko_command_word0_t pko_command, cvmx_buf_ptr_t packet, uint64_t addr, cvmx_pko_lock_t use_locking)
-{
-	cvmx_cmd_queue_result_t result;
-	if (use_locking == CVMX_PKO_LOCK_ATOMIC_TAG)
-		cvmx_pow_tag_sw_wait();
-
-	result = cvmx_cmd_queue_write3(CVMX_CMD_QUEUE_PKO(queue), (use_locking == CVMX_PKO_LOCK_CMD_QUEUE), pko_command.u64, packet.u64, addr);
-	if (cvmx_likely(result == CVMX_CMD_QUEUE_SUCCESS)) {
-		cvmx_pko_doorbell_pkoid(pko_port, queue, 3);
-		return CVMX_PKO_SUCCESS;
-	} else if ((result == CVMX_CMD_QUEUE_NO_MEMORY) || (result == CVMX_CMD_QUEUE_FULL)) {
-		return CVMX_PKO_NO_MEMORY;
-	} else {
-		return CVMX_PKO_INVALID_QUEUE;
-	}
-}
-
-/*
- * Obtain the number of PKO commands pending in a queue
- *
- * @param queue is the queue identifier to be queried
- * @return the number of commands pending transmission or -1 on error
- */
-int cvmx_pko_queue_pend_count( cvmx_cmd_queue_id_t queue);
-
-void cvmx_pko_set_cmd_queue_pool_buffer_count(uint64_t buffer_count);
-
-#ifdef	__cplusplus
-/* *INDENT-OFF* */
-}
-/* *INDENT-ON* */
-#endif
-
-#endif /* __CVMX_PKO_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-pko3-queue.h b/arch/mips/include/asm/octeon/cvmx-pko3-queue.h
index 8e8d3fa..20caa1e 100644
--- a/arch/mips/include/asm/octeon/cvmx-pko3-queue.h
+++ b/arch/mips/include/asm/octeon/cvmx-pko3-queue.h
@@ -98,7 +98,7 @@ static inline int cvmx_pko3_get_queue_base(uint16_t ipd_port)
 	unsigned node;
 
 	/* get per-node table */
-	if(__cvmx_pko3_dq_table == NULL)
+	if(cvmx_unlikely(__cvmx_pko3_dq_table == NULL))
 		__cvmx_pko3_dq_table_setup();
 
 	/* extract node # from portID */
diff --git a/arch/mips/include/asm/octeon/cvmx-pko3.h b/arch/mips/include/asm/octeon/cvmx-pko3.h
index 85172fd..20f32c7 100644
--- a/arch/mips/include/asm/octeon/cvmx-pko3.h
+++ b/arch/mips/include/asm/octeon/cvmx-pko3.h
@@ -57,11 +57,13 @@ extern "C" {
 #include <asm/octeon/cvmx-helper.h>
 #include <asm/octeon/cvmx-pko3-queue.h>
 #include <asm/octeon/cvmx-ilk.h>
+#include <asm/octeon/cvmx-scratch.h>
 #else
 #include "cvmx-pko-defs.h"
 #include "cvmx-pko3-queue.h"
 #include "cvmx-helper.h"
 #include "cvmx-ilk.h"
+#include "cvmx-scratch.h"
 #endif
 
 /* dwords are from 1-16 */
@@ -159,6 +161,16 @@ enum pko_auraalg_e {
 };
 
 /**
+ * PKO_CKL4ALG_E
+ */
+enum pko_clk4alg_e {
+	CKL4ALG_NONE = 0x0,	/* No checksum. */
+	CKL4ALG_UDP = 0x1,	/* UDP L4 checksum. */
+	CKL4ALG_TCP = 0x2,	/* TCP L4 checksum. */
+	CKL4ALG_SCTP = 0x3,	/* SCTP L4 checksum. */
+};
+
+/**
  * pko_send_aura
  */
 union cvmx_pko_send_aura {
@@ -175,13 +187,13 @@ union cvmx_pko_send_aura {
 };
 typedef union cvmx_pko_send_aura cvmx_pko_send_aura_t;
 
-/* pko command descriptor */
+/* PKO_SEND_HDR_S - PKO header subcommand */
 union cvmx_pko_send_hdr {
 	uint64_t u64;
 	struct {
 		CVMX_BITFIELD_FIELD(uint64_t rsvd_60_63	:4,
 		CVMX_BITFIELD_FIELD(uint64_t aura	:12,
-		CVMX_BITFIELD_FIELD(uint64_t ckl4	:2,
+		CVMX_BITFIELD_FIELD(uint64_t ckl4	:2, /* PKO_CKL4ALG_E */
 		CVMX_BITFIELD_FIELD(uint64_t ckl3	:1,
 		CVMX_BITFIELD_FIELD(uint64_t ds		:1,
 		CVMX_BITFIELD_FIELD(uint64_t le		:1,
@@ -198,6 +210,24 @@ union cvmx_pko_send_hdr {
 };
 typedef union cvmx_pko_send_hdr cvmx_pko_send_hdr_t;
 
+/* PKO_SEND_EXT_S - extended header subcommand */
+union cvmx_pko_send_ext {
+	uint64_t u64;
+	struct {
+		CVMX_BITFIELD_FIELD(uint64_t rsvd_48_63	:16,
+		CVMX_BITFIELD_FIELD(uint64_t subdc4	:4, /* _SENDSUBDC_EXT */
+		CVMX_BITFIELD_FIELD(uint64_t col	:2, /* _COLORALG_E */
+		CVMX_BITFIELD_FIELD(uint64_t ra		:2, /* _REDALG_E */
+		CVMX_BITFIELD_FIELD(uint64_t tstmp	:1,
+		CVMX_BITFIELD_FIELD(uint64_t rsvd_24_38:15,
+		CVMX_BITFIELD_FIELD(uint64_t markptr	:8,
+		CVMX_BITFIELD_FIELD(uint64_t rsvd_9_15	:7,
+		CVMX_BITFIELD_FIELD(uint64_t shapechg	:9,
+			)))))))));
+	} s;
+};
+typedef union cvmx_pko_send_ext cvmx_pko_send_ext_t;
+
 /* PKO_MEMDSZ_E */
 enum cvmx_pko_memdsz_e {
 	MEMDSZ_B64 = 0,
@@ -487,6 +517,7 @@ int cvmx_pko3_hw_init_global(int node, uint16_t aura);
  */
 int cvmx_pko3_hw_disable(int node);
 
+#if 0 //deprecated
 /*
  * Transmit packets through pko on specified node and queue.
  *
@@ -500,6 +531,7 @@ int cvmx_pko3_hw_disable(int node);
  */
 int cvmx_pko_transmit_packet(int dq, cvmx_buf_ptr_pki_t bufptr,
 			     int packet_len, int aura_free);
+#endif
 
 /* Define legacy type here to break circular dependency */
 typedef struct cvmx_pko_port_status cvmx_pko_port_status_t;
@@ -542,6 +574,37 @@ extern int cvmx_pko3_interface_options(int node, int interface, int port,
  */
 extern void cvmx_pko3_dq_options(unsigned node, unsigned dq, bool min_pad);
 
+/* Packet descriptor - PKO3 command buffer + internal state */
+typedef struct cvmx_pko3_pdesc_s {
+	uint64_t *jump_buf;	/**< jump buffer vaddr */
+	int16_t last_aura;	/**< AURA of the latest LINK_S/GATHER_S */
+	unsigned
+		num_words:5,	/**< valid words in word array 2..16 */
+		headroom:10,	/**< free bytes at start of 1st buf */
+		pki_word4_present : 1;
+	/* PKO3 command buffer: */
+	uint64_t word[16];	/**< header and subcommands buffer */
+	/* Bookkeeping fields: */
+	uint64_t send_work_s;	/**< SEND_WORK_S must be the very last subdc */
+	int16_t jb_aura;	/**< AURA where the jump buffer belongs */
+	uint16_t mem_s_ix;	/**< index of first MEM_S subcommand */
+	uint8_t ckl4_alg;	/**< L3/L4 alg to use if recalc is needed */
+	/* Fields saved from WQE for later inspection */
+	cvmx_wqe_word4_t pki_word4;
+	cvmx_pki_wqe_word2_t pki_word2;
+} cvmx_pko3_pdesc_t;
+
+void cvmx_pko3_pdesc_init(cvmx_pko3_pdesc_t *pdesc);
+int cvmx_pko3_pdesc_from_wqe(cvmx_pko3_pdesc_t *pdesc, cvmx_wqe_78xx_t *wqe,
+	bool free_bufs);
+int cvmx_pko3_pdesc_transmit(cvmx_pko3_pdesc_t *pdesc, uint16_t dq);
+int cvmx_pko3_pdesc_notify_decrement(cvmx_pko3_pdesc_t *pdesc,
+        volatile uint64_t *p_counter);
+int cvmx_pko3_pdesc_notify_wqe(cvmx_pko3_pdesc_t *pdesc, cvmx_wqe_78xx_t *wqe,
+	uint8_t node, uint8_t group, uint8_t tt, uint32_t tag);
+int cvmx_pko3_pdesc_buf_append(cvmx_pko3_pdesc_t *pdesc, void *p_data,
+		unsigned data_bytes, unsigned gaura, bool free_buf);
+
 #ifdef	__cplusplus
 /* *INDENT-OFF* */
 }
diff --git a/arch/mips/include/asm/octeon/cvmx-pow.h b/arch/mips/include/asm/octeon/cvmx-pow.h
index e0d6fe7..2d60178 100644
--- a/arch/mips/include/asm/octeon/cvmx-pow.h
+++ b/arch/mips/include/asm/octeon/cvmx-pow.h
@@ -2329,6 +2329,104 @@ static inline void cvmx_pow_work_submit(cvmx_wqe_t * wqp, uint32_t tag, cvmx_pow
 }
 
 /**
+ * Submits work to an SSO group on any OCI node.
+ * This function updates the work queue entry in DRAM to match
+ * the arguments given.
+ * Note that the tag provided is for the work queue entry submitted,
+ * and is unrelated to the tag that the core currently holds.
+ *
+ * @param wqp pointer to work queue entry to submit. 
+ * This entry is updated to match the other parameters
+ * @param tag tag value to be assigned to work queue entry
+ * @param tag_type type of tag
+ * @param group group value for the work queue entry, in the range 0..255
+ * on the CN78XX model.
+ * @param node The OCI node number for the target group
+ *
+ * When this function is called on a model prior to CN78XX, which does
+ * not support OCI nodes, the 'node' argument is ignored, and the 'group'
+ * parameter is converted into 'qps' (the lower 3 bits) and 'grp' (the higher
+ * 5 bits), following the backward-compatibility scheme of translating
+ * between new and old style group numbers.
+ */
+static inline void cvmx_pow_work_submit_node(cvmx_wqe_t * wqp, uint32_t tag, cvmx_pow_tag_type_t tag_type, uint8_t  group, uint8_t node)
+{
+	union cvmx_pow_tag_req_addr ptr;
+	cvmx_pow_tag_req_t tag_req;
+
+	tag_req.u64 = 0;
+	ptr.u64 = 0;
+
+	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
+		unsigned xgrp;
+
+		xgrp = group & 0xff;
+		xgrp |= 0x300 & (node << 8);
+ 
+		wqp->word1.cn78xx.rsvd_0 = 0;
+		wqp->word1.cn78xx.rsvd_1 = 0;
+		wqp->word1.cn78xx.tag = tag;
+		wqp->word1.cn78xx.tag_type = tag_type;
+		wqp->word1.cn78xx.grp = xgrp;
+
+		tag_req.s_cn78xx_other.op = CVMX_POW_TAG_OP_ADDWQ;
+		tag_req.s_cn78xx_other.type = tag_type;
+		tag_req.s_cn78xx_other.wqp = cvmx_ptr_to_phys(wqp);
+
+		ptr.s_cn78xx.did = 0x66; //CVMX_OCT_DID_TAG_TAG6;
+		ptr.s_cn78xx.mem_region = CVMX_IO_SEG;
+		ptr.s_cn78xx.is_io = 1;
+		ptr.s_cn78xx.node = node;
+		ptr.s_cn78xx.tag = tag;
+	} else if (octeon_has_feature(OCTEON_FEATURE_CN68XX_WQE)) {
+		/* Reset all reserved bits */
+		wqp->word1.cn68xx.zero_0 = 0;
+		wqp->word1.cn68xx.zero_1 = 0;
+		wqp->word1.cn68xx.zero_2 = 0;
+		wqp->word1.cn68xx.qos = group & 0x7;
+		wqp->word1.cn68xx.grp = group >> 3;
+
+		wqp->word1.tag = tag;
+		wqp->word1.tag_type = tag_type;
+
+		tag_req.s_cn68xx_add.op = CVMX_POW_TAG_OP_ADDWQ;
+		tag_req.s_cn68xx_add.type = tag_type;
+		tag_req.s_cn68xx_add.tag = tag;
+		tag_req.s_cn68xx_add.qos = group & 0x7;
+		tag_req.s_cn68xx_add.grp = group >> 3;
+
+		ptr.s.mem_region = CVMX_IO_SEG;
+		ptr.s.is_io = 1;
+		ptr.s.did = CVMX_OCT_DID_TAG_TAG1;
+		ptr.s.addr = cvmx_ptr_to_phys(wqp);
+	} else {
+		/* Reset all reserved bits */
+		wqp->word1.cn38xx.zero_2 = 0;
+		wqp->word1.cn38xx.qos = group & 0x7;
+		wqp->word1.cn38xx.grp = group >> 3;
+
+		wqp->word1.tag = tag;
+		wqp->word1.tag_type = tag_type;
+
+		tag_req.s_cn38xx.op = CVMX_POW_TAG_OP_ADDWQ;
+		tag_req.s_cn38xx.type = tag_type;
+		tag_req.s_cn38xx.tag = tag;
+		tag_req.s_cn38xx.qos = group & 0x7;
+		tag_req.s_cn38xx.grp = group >> 3;
+
+		ptr.s.mem_region = CVMX_IO_SEG;
+		ptr.s.is_io = 1;
+		ptr.s.did = CVMX_OCT_DID_TAG_TAG1;
+		ptr.s.addr = cvmx_ptr_to_phys(wqp);
+	}
+
+	/* SYNC write to memory before the work submit.  This is necessary
+	 ** as POW may read values from DRAM at this time */
+	CVMX_SYNCWS;
+	cvmx_write_io(ptr.u64, tag_req.u64);
+}
+
+/**
  * This function sets the group mask for a core.  The group mask
  * indicates which groups each core will accept work from. There are
  * 16 groups.
@@ -2970,11 +3068,4 @@ extern int cvmx_pow_get_num_entries(void);
 /* *INDENT-ON* */
 #endif
 
-/*
- * TODO:
- *
- * WQE_ALLOC - make use of cn78xx hardware-supported feature
- * PREP_WORK - prefetch next WQE,
- */
-
 #endif /* __CVMX_POW_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-wqe.h b/arch/mips/include/asm/octeon/cvmx-wqe.h
index 1cb63c1..3db30b0 100644
--- a/arch/mips/include/asm/octeon/cvmx-wqe.h
+++ b/arch/mips/include/asm/octeon/cvmx-wqe.h
@@ -51,7 +51,7 @@
  * This file must not depend on any other header files, except for cvmx.h!!!
  *
  *
- * <hr>$Revision: 94735 $<hr>
+ * <hr>$Revision: 95258 $<hr>
  *
  *
  */
@@ -59,9 +59,10 @@
 #ifndef __CVMX_WQE_H__
 #define __CVMX_WQE_H__
 
-#include "cvmx-fpa.h"
 #include "cvmx-pki-defs.h"
 #include "cvmx-pip-defs.h"
+#include "cvmx-fpa3.h"
+#include "cvmx-fpa1.h"
 
 #ifdef	__cplusplus
 /* *INDENT-OFF* */
@@ -1622,10 +1623,97 @@ static inline void cvmx_wqe_free(cvmx_wqe_t *work)
 			ncl = 1;
 		}
 
-		cvmx_fpa_free(work, pool, ncl);
+		cvmx_fpa1_free(work, pool, ncl);
 	}
 }
 
+/**
+ * Check if a work entry has been intiated by software
+ *
+ */
+static inline bool cvmx_wqe_is_soft(cvmx_wqe_t *work)
+{
+	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
+		cvmx_wqe_78xx_t* wqe = (void *)work;
+		return wqe->word2.pki.software;
+	} else
+		return work->word2.s.software;
+}
+
+#if 0  /* What is this for?  there are no users.  Move to .c file if needed. */
+/**
+ * Allocate a work-queue entry for delivering software-initiated
+ * event notifications (as opposed to packet data).
+ * The application data is copied into the work-queue entry,
+ * if the space is sufficient.
+ */
+static inline cvmx_wqe_t * cvmx_wqe_soft_create(void *data_p, unsigned data_sz)
+{
+	cvmx_wqe_t *work = NULL;
+	unsigned pool = 0, gaura = 0;
+	unsigned buf_sz = 128;
+
+	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
+		cvmx_pki_qpg_tblx_t qpg_tbl;
+		unsigned node = cvmx_get_node_num();
+
+		/* Use PKI default AURA for these work entries */
+		qpg_tbl.u64 = cvmx_read_csr_node(node, CVMX_PKI_QPG_TBLX(0));
+		gaura = qpg_tbl.s.laura;
+		gaura |= node << 10;
+
+		/* Check if WQE space is large enough */
+#ifndef	CVMX_BUILD_FOR_LINUX_KERNEL
+		buf_sz = cvmx_fpa_get_aura_buf_size(gaura);
+#endif
+		buf_sz -= sizeof(cvmx_wqe_78xx_t);
+		if(buf_sz >= data_sz)
+			work = cvmx_fpa_alloc_aura(node, gaura & ((1<<10)-1));
+	} else {
+		pool = __cvmx_ipd_wqe_pool();
+#ifndef	CVMX_BUILD_FOR_LINUX_KERNEL
+		buf_sz = cvmx_fpa_get_block_size(pool);
+#endif
+		buf_sz -= offsetof(cvmx_wqe_t, packet_data);
+		if(buf_sz >= data_sz)
+			work = cvmx_fpa1_alloc(pool);
+	}
+
+	if(work == NULL)
+		return work;
+
+	work->word0.u64 = 0;
+	work->word1.u64 = 0;
+	work->word2.u64 = 0;
+
+	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
+		cvmx_wqe_78xx_t* wqe = (void *)work;
+		wqe->word2.pki.software = 1;
+		wqe->word0.pki.aura = gaura;
+		wqe->packet_ptr.u64 = 0;
+		if (data_sz > 0) {
+			wqe->word0.pki.bufs = 1;
+			wqe->packet_ptr.s_cn78xx.size = data_sz;
+			wqe->packet_ptr.s_cn78xx.addr = cvmx_ptr_to_phys(wqe+1);
+			memcpy(wqe+1, data_p, data_sz);
+		}
+	} else {
+		work->word2.s.software = 1;
+		work->packet_ptr.u64 = 0;
+		work->packet_ptr.s.pool = pool;
+		if (data_sz > 0) {
+			work->packet_ptr.s.size = data_sz;
+			work->packet_ptr.s.addr =
+				cvmx_ptr_to_phys(&work->packet_data);
+			memcpy(&work->packet_data, data_p, data_sz);
+		}
+	}
+
+	cvmx_wqe_set_len(work, data_sz);
+
+	return work;
+}
+#endif
 
 /**
  * @INTERNAL
diff --git a/drivers/net/ethernet/octeon/ethernet-mem.c b/drivers/net/ethernet/octeon/ethernet-mem.c
index 6e0a22f..78dc46d 100644
--- a/drivers/net/ethernet/octeon/ethernet-mem.c
+++ b/drivers/net/ethernet/octeon/ethernet-mem.c
@@ -30,7 +30,7 @@
 #include <linux/slab.h>
 
 #include <asm/octeon/octeon.h>
-#include <asm/octeon/cvmx-fpa.h>
+#include <asm/octeon/cvmx-fpa1.h>
 
 #include "ethernet-defines.h"
 #include "octeon-ethernet.h"
@@ -78,7 +78,7 @@ static int cvm_oct_fill_hw_skbuff(struct fpa_pool *pool, int elements)
 		extra_reserve = desired_data - skb->data;
 		skb_reserve(skb, extra_reserve);
 		*(struct sk_buff **)(skb->data - sizeof(void *)) = skb;
-		cvmx_fpa_free(skb->data, pool_num, DONT_WRITEBACK(size / 128));
+		cvmx_fpa1_free(skb->data, pool_num, DONT_WRITEBACK(size / 128));
 		freed--;
 	}
 	return elements - freed;
@@ -97,7 +97,7 @@ static int cvm_oct_free_hw_skbuff(struct fpa_pool *pool, int elements)
 	int pool_num = pool->pool;
 
 	while (elements) {
-		memory = cvmx_fpa_alloc(pool_num);
+		memory = cvmx_fpa1_alloc(pool_num);
 		if (!memory)
 			break;
 		skb = *cvm_oct_packet_to_skb(memory);
@@ -132,7 +132,7 @@ static int cvm_oct_fill_hw_kmem(struct fpa_pool *pool, int elements)
 			       elements * pool->size, pool->pool);
 			break;
 		}
-		cvmx_fpa_free(memory, pool->pool, 0);
+		cvmx_fpa1_free(memory, pool->pool, 0);
 		freed--;
 	}
 	return elements - freed;
@@ -148,7 +148,7 @@ static int cvm_oct_free_hw_kmem(struct fpa_pool *pool, int elements)
 {
 	char *fpa;
 	while (elements) {
-		fpa = cvmx_fpa_alloc(pool->pool);
+		fpa = cvmx_fpa1_alloc(pool->pool);
 		if (!fpa)
 			break;
 		elements--;
diff --git a/drivers/net/ethernet/octeon/ethernet-napi.c b/drivers/net/ethernet/octeon/ethernet-napi.c
index bf4ff2f..d0d5c83 100644
--- a/drivers/net/ethernet/octeon/ethernet-napi.c
+++ b/drivers/net/ethernet/octeon/ethernet-napi.c
@@ -170,7 +170,7 @@ static int CVM_OCT_NAPI_POLL(struct napi_struct *napi, int budget)
 			}
 			dev_kfree_skb_any(skb);
 
-			cvmx_fpa_free(work, wqe_pool, DONT_WRITEBACK(1));
+			cvmx_fpa1_free(work, wqe_pool, DONT_WRITEBACK(1));
 
 			/* We are done with this one, adjust the queue
 			 * depth.
@@ -443,7 +443,7 @@ static int CVM_OCT_NAPI_POLL(struct napi_struct *napi, int budget)
 				cvmx_hwfau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE,
 						      packets_to_replace);
 
-				cvmx_fpa_free(work, wqe_pool, DONT_WRITEBACK(1));
+				cvmx_fpa1_free(work, wqe_pool, DONT_WRITEBACK(1));
 			} else {
 				cvm_oct_free_work(work);
 			}
diff --git a/drivers/net/ethernet/octeon/ethernet-rx.c b/drivers/net/ethernet/octeon/ethernet-rx.c
index ac647ec..45eeab0 100644
--- a/drivers/net/ethernet/octeon/ethernet-rx.c
+++ b/drivers/net/ethernet/octeon/ethernet-rx.c
@@ -51,7 +51,7 @@
 
 #include <asm/octeon/cvmx-helper.h>
 #include <asm/octeon/cvmx-wqe.h>
-#include <asm/octeon/cvmx-fau.h>
+#include <asm/octeon/cvmx-hwfau.h>
 #include <asm/octeon/cvmx-pow.h>
 #include <asm/octeon/cvmx-pip.h>
 #include <asm/octeon/cvmx-ipd.h>
diff --git a/drivers/net/ethernet/octeon/ethernet-tx.c b/drivers/net/ethernet/octeon/ethernet-tx.c
index de47cd7..26f9340 100644
--- a/drivers/net/ethernet/octeon/ethernet-tx.c
+++ b/drivers/net/ethernet/octeon/ethernet-tx.c
@@ -42,10 +42,10 @@
 #include "octeon-ethernet.h"
 
 #include <asm/octeon/cvmx-wqe.h>
-#include <asm/octeon/cvmx-fau.h>
+#include <asm/octeon/cvmx-hwfau.h>
 #include <asm/octeon/cvmx-ipd.h>
 #include <asm/octeon/cvmx-pip.h>
-#include <asm/octeon/cvmx-pko.h>
+#include <asm/octeon/cvmx-hwpko.h>
 #include <asm/octeon/cvmx-helper.h>
 
 #include <asm/octeon/cvmx-gmxx-defs.h>
@@ -222,7 +222,7 @@ int cvm_oct_transmit_qos(struct net_device *dev,
 		dev->stats.tx_dropped++;
 	} else
 	if (do_free)
-		cvmx_fpa_free(work, wqe_pool, DONT_WRITEBACK(1));
+		cvmx_fpa1_free(work, wqe_pool, DONT_WRITEBACK(1));
 
 	return dropped;
 }
diff --git a/drivers/net/ethernet/octeon/ethernet-xmit.c b/drivers/net/ethernet/octeon/ethernet-xmit.c
index 4ecf66b..9af0965 100644
--- a/drivers/net/ethernet/octeon/ethernet-xmit.c
+++ b/drivers/net/ethernet/octeon/ethernet-xmit.c
@@ -169,7 +169,7 @@ CVM_OCT_XMIT
 	} else {
 		u64 *hw_buffer_list;
 
-		work = cvmx_fpa_alloc(wqe_pool);
+		work = cvmx_fpa1_alloc(wqe_pool);
 		if (unlikely(!work)) {
 			netdev_err(dev, "Failed WQE allocate\n");
 			queue_type = QUEUE_DROP;
@@ -298,7 +298,7 @@ CVM_OCT_XMIT
 
 	if (queue_type == QUEUE_WQE) {
 		if (!work) {
-			work = cvmx_fpa_alloc(wqe_pool);
+			work = cvmx_fpa1_alloc(wqe_pool);
 			if (unlikely(!work)) {
 				netdev_err(dev, "Failed WQE allocate\n");
 				queue_type = QUEUE_DROP;
@@ -362,7 +362,7 @@ skip_xmit:
 		dev_kfree_skb_any(skb);
 		dev->stats.tx_dropped++;
 		if (work)
-			cvmx_fpa_free(work, wqe_pool, DONT_WRITEBACK(1));
+			cvmx_fpa1_free(work, wqe_pool, DONT_WRITEBACK(1));
 		break;
 	case QUEUE_HW:
 		cvmx_hwfau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, -buffers_being_recycled);
diff --git a/drivers/net/ethernet/octeon/ethernet.c b/drivers/net/ethernet/octeon/ethernet.c
index ae3949e..aee934d 100644
--- a/drivers/net/ethernet/octeon/ethernet.c
+++ b/drivers/net/ethernet/octeon/ethernet.c
@@ -41,8 +41,8 @@
 #include "octeon-ethernet.h"
 
 #include <asm/octeon/cvmx-pip.h>
-#include <asm/octeon/cvmx-pko.h>
-#include <asm/octeon/cvmx-fau.h>
+#include <asm/octeon/cvmx-hwpko.h>
+#include <asm/octeon/cvmx-hwfau.h>
 #include <asm/octeon/cvmx-ipd.h>
 #include <asm/octeon/cvmx-srio.h>
 #include <asm/octeon/cvmx-helper.h>
@@ -270,7 +270,7 @@ static void cvm_oct_set_pko_multiqueue(void)
 static int cvm_oct_configure_common_hw(void)
 {
 	/* Setup the FPA */
-	cvmx_fpa_enable();
+	cvmx_fpa1_enable();
 
 	/* allocate packet pool */
 	packet_pool = cvm_oct_alloc_fpa_pool(packet_pool, FPA_PACKET_POOL_SIZE);
@@ -386,12 +386,12 @@ int cvm_oct_free_work(void *work_queue_entry)
 	while (segments--) {
 		union cvmx_buf_ptr next_ptr = *(union cvmx_buf_ptr *)phys_to_virt(segment_ptr.s.addr - 8);
 		if (!segment_ptr.s.i)
-			cvmx_fpa_free(cvm_oct_get_buffer_ptr(segment_ptr),
+			cvmx_fpa1_free(cvm_oct_get_buffer_ptr(segment_ptr),
 				      segment_ptr.s.pool,
 				      DONT_WRITEBACK(FPA_PACKET_POOL_SIZE / 128));
 		segment_ptr = next_ptr;
 	}
-	cvmx_fpa_free(work, wqe_pool, DONT_WRITEBACK(1));
+	cvmx_fpa1_free(work, wqe_pool, DONT_WRITEBACK(1));
 
 	return 0;
 }
diff --git a/drivers/net/ethernet/octeon/octeon-ethernet.h b/drivers/net/ethernet/octeon/octeon-ethernet.h
index 5d00e5e..10ffdea 100644
--- a/drivers/net/ethernet/octeon/octeon-ethernet.h
+++ b/drivers/net/ethernet/octeon/octeon-ethernet.h
@@ -36,7 +36,7 @@
 #include "octeon_common.h"
 
 #include <asm/octeon/cvmx-helper.h>
-#include <asm/octeon/cvmx-fau.h>
+#include <asm/octeon/cvmx-hwfau.h>
 #include <asm/octeon/octeon-ethernet-user.h>
 
 /**
diff --git a/drivers/net/ethernet/octeon/octeon-pow-ethernet.c b/drivers/net/ethernet/octeon/octeon-pow-ethernet.c
index c6039a4..c2ab83b 100644
--- a/drivers/net/ethernet/octeon/octeon-pow-ethernet.c
+++ b/drivers/net/ethernet/octeon/octeon-pow-ethernet.c
@@ -19,7 +19,7 @@
 
 #include <asm/octeon/octeon.h>
 #include <asm/octeon/cvmx.h>
-#include <asm/octeon/cvmx-fpa.h>
+#include <asm/octeon/cvmx-fpa1.h>
 #include <asm/octeon/cvmx-pow.h>
 #include <asm/octeon/cvmx-wqe.h>
 #include <asm/octeon/cvmx-pow-defs.h>
@@ -134,11 +134,11 @@ static int octeon_pow_free_work(cvmx_wqe_t *work)
 		union cvmx_buf_ptr next_ptr =
 			*(union cvmx_buf_ptr *)phys_to_virt(segment_ptr.s.addr - 8);
 		if (unlikely(!segment_ptr.s.i))
-			cvmx_fpa_free(get_buffer_ptr(segment_ptr),
+			cvmx_fpa1_free(get_buffer_ptr(segment_ptr),
 				 segment_ptr.s.pool, 0);
 		segment_ptr = next_ptr;
 	}
-	cvmx_fpa_free(work, fpa_wqe_pool, 0);
+	cvmx_fpa1_free(work, fpa_wqe_pool, 0);
 
 	return 0;
 }
@@ -175,7 +175,7 @@ static int octeon_pow_xmit(struct sk_buff *skb, struct net_device *dev)
 			continue;
 
 		/* Get a work queue entry */
-		work = cvmx_fpa_alloc(fpa_wqe_pool);
+		work = cvmx_fpa1_alloc(fpa_wqe_pool);
 		if (unlikely(work == NULL)) {
 			DEBUGPRINT("%s: Failed to allocate a work queue entry\n",
 				   dev->name);
@@ -183,7 +183,7 @@ static int octeon_pow_xmit(struct sk_buff *skb, struct net_device *dev)
 		}
 
 		/* Get a packet buffer */
-		packet_buffer = cvmx_fpa_alloc(fpa_packet_pool);
+		packet_buffer = cvmx_fpa1_alloc(fpa_packet_pool);
 		if (unlikely(packet_buffer == NULL)) {
 			DEBUGPRINT("%s: Failed to allocate a packet buffer\n",
 				   dev->name);
@@ -321,9 +321,9 @@ static int octeon_pow_xmit(struct sk_buff *skb, struct net_device *dev)
 
 fail:
 	if (work)
-		cvmx_fpa_free(work, fpa_wqe_pool, 0);
+		cvmx_fpa1_free(work, fpa_wqe_pool, 0);
 	if (packet_buffer)
-		cvmx_fpa_free(packet_buffer, fpa_packet_pool, 0);
+		cvmx_fpa1_free(packet_buffer, fpa_packet_pool, 0);
 	dev->stats.tx_dropped++;
 	dev_kfree_skb(skb);
 	return NETDEV_TX_OK;
-- 
2.6.2

