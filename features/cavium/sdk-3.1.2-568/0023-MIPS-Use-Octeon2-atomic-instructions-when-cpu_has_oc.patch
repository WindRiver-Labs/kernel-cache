From 0745f079b3aeacffd708df96928e80d4afab52ad Mon Sep 17 00:00:00 2001
From: David Daney <ddaney@caviumnetworks.com>
Date: Fri, 31 May 2013 15:21:12 -0700
Subject: [PATCH 023/974] MIPS: Use Octeon2 atomic instructions when
 cpu_has_octeon2_isa.

Signed-off-by: David Daney <ddaney@caviumnetworks.com>
Signed-off-by: Leonid Rosenboim <lrosenboim@caviumnetworks.com>
[Original patch taken from Cavium SDK 3.1.2-568]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
---
 arch/mips/include/asm/atomic.h  | 120 +++++++++++++++++++++++++++++++++++++---
 arch/mips/include/asm/cmpxchg.h |  36 +++++++++++-
 2 files changed, 146 insertions(+), 10 deletions(-)

diff --git a/arch/mips/include/asm/atomic.h b/arch/mips/include/asm/atomic.h
index 08b6079..e3266ce 100644
--- a/arch/mips/include/asm/atomic.h
+++ b/arch/mips/include/asm/atomic.h
@@ -49,7 +49,15 @@
  */
 static __inline__ void atomic_add(int i, atomic_t * v)
 {
-	if (kernel_uses_llsc && R10000_LLSC_WAR) {
+	if (cpu_has_saa) {
+		__asm__ __volatile__(
+		".set	push\n\t"
+		".set	arch=octeon\n\t"
+		"saa    %1, (%2)\t# atomic_add (%0)\n\t"
+		".set	pop"
+		: "+m" (v->counter)
+		: "r" (i), "r" (&v->counter));
+	} else if (kernel_uses_llsc && R10000_LLSC_WAR) {
 		int temp;
 
 		__asm__ __volatile__(
@@ -92,7 +100,15 @@ static __inline__ void atomic_add(int i, atomic_t * v)
  */
 static __inline__ void atomic_sub(int i, atomic_t * v)
 {
-	if (kernel_uses_llsc && R10000_LLSC_WAR) {
+	if (cpu_has_saa) {
+		__asm__ __volatile__(
+		".set	push\n\t"
+		".set	arch=octeon\n\t"
+		"saa    %1, (%2)\t# atomic_sub(%0)\n\t"
+		".set	pop"
+		: "+m" (v->counter)
+		: "r" (-i), "r" (&v->counter));
+	} else if (kernel_uses_llsc && R10000_LLSC_WAR) {
 		int temp;
 
 		__asm__ __volatile__(
@@ -135,7 +151,25 @@ static __inline__ int atomic_add_return(int i, atomic_t * v)
 
 	smp_mb__before_llsc();
 
-	if (kernel_uses_llsc && R10000_LLSC_WAR) {
+	if (cpu_has_octeon2_isa && kernel_uses_llsc) {
+		/*
+		 * For proper barrier semantics, the preceding
+		 * smp_mb__before_llsc() must expand to syncw.
+		 */
+		if (__builtin_constant_p(i) && i == 1)
+			__asm__ __volatile__("lai\t%0,(%2)\t# atomic_add_return (%1)"
+					: "=r" (result), "+m" (v->counter)
+					: "r" (&v->counter));
+		else if (__builtin_constant_p(i) && i == -1)
+			__asm__ __volatile__("lad\t%0,(%2)\t# atomic_add_return (%1)"
+					: "=r" (result), "+m" (v->counter)
+					: "r" (&v->counter));
+		else
+			__asm__ __volatile__("laa\t%0,(%2),%3\t# atomic_add_return (%1)"
+					: "=r" (result), "+m" (v->counter)
+					: "r" (&v->counter), "r" (i));
+		result += i;
+	} else if (kernel_uses_llsc && R10000_LLSC_WAR) {
 		int temp;
 
 		__asm__ __volatile__(
@@ -184,7 +218,25 @@ static __inline__ int atomic_sub_return(int i, atomic_t * v)
 
 	smp_mb__before_llsc();
 
-	if (kernel_uses_llsc && R10000_LLSC_WAR) {
+	if (cpu_has_octeon2_isa && kernel_uses_llsc) {
+		/*
+		 * For proper barrier semantics, the preceding
+		 * smp_mb__before_llsc() must expand to syncw.
+		 */
+		if (__builtin_constant_p(i) && i == -1)
+			__asm__ __volatile__("lai\t%0,(%2)\t# atomic_sub_return (%1)"
+					: "=r" (result), "+m" (v->counter)
+					: "r" (&v->counter));
+		else if (__builtin_constant_p(i) && i == 1)
+			__asm__ __volatile__("lad\t%0,(%2)\t# atomic_sub_return (%1)"
+					: "=r" (result), "+m" (v->counter)
+					: "r" (&v->counter));
+		else
+			__asm__ __volatile__("laa\t%0,(%2),%3\t# atomic_sub_return (%1)"
+					: "=r" (result), "+m" (v->counter)
+					: "r" (&v->counter), "r" (-i));
+		result -= i;
+	} else if (kernel_uses_llsc && R10000_LLSC_WAR) {
 		int temp;
 
 		__asm__ __volatile__(
@@ -416,7 +468,15 @@ static __inline__ int __atomic_add_unless(atomic_t *v, int a, int u)
  */
 static __inline__ void atomic64_add(long i, atomic64_t * v)
 {
-	if (kernel_uses_llsc && R10000_LLSC_WAR) {
+	if (cpu_has_saa) {
+		__asm__ __volatile__(
+		".set	push\n\t"
+		".set	arch=octeon\n\t"
+		"saad   %1, (%2)\t# atomic64_add (%0)\n\t"
+		".set	pop"
+		: "+m" (v->counter)
+		: "r" (i), "r" (v));
+	} else if (kernel_uses_llsc && R10000_LLSC_WAR) {
 		long temp;
 
 		__asm__ __volatile__(
@@ -459,7 +519,15 @@ static __inline__ void atomic64_add(long i, atomic64_t * v)
  */
 static __inline__ void atomic64_sub(long i, atomic64_t * v)
 {
-	if (kernel_uses_llsc && R10000_LLSC_WAR) {
+	if (cpu_has_saa) {
+		__asm__ __volatile__(
+		".set	push\n\t"
+		".set	arch=octeon\n\t"
+		"saad    %1, (%2)\t# atomic64_sub (%0)\n\t"
+		".set	pop"
+		: "+m" (v->counter)
+		: "r" (-i), "r" (v));
+	} else if (kernel_uses_llsc && R10000_LLSC_WAR) {
 		long temp;
 
 		__asm__ __volatile__(
@@ -502,7 +570,25 @@ static __inline__ long atomic64_add_return(long i, atomic64_t * v)
 
 	smp_mb__before_llsc();
 
-	if (kernel_uses_llsc && R10000_LLSC_WAR) {
+	if (cpu_has_octeon2_isa && kernel_uses_llsc) {
+		/*
+		 * For proper barrier semantics, the preceding
+		 * smp_mb__before_llsc() must expand to syncw.
+		 */
+		if (__builtin_constant_p(i) && i == 1)
+			__asm__ __volatile__("laid\t%0,(%2)\t# atomic64_add_return (%1)"
+					: "=r" (result), "+m" (v->counter)
+					: "r" (&v->counter));
+		else if (__builtin_constant_p(i) && i == -1)
+			__asm__ __volatile__("ladd\t%0,(%2)\t# atomic64_add_return (%1)"
+					: "=r" (result), "+m" (v->counter)
+					: "r" (&v->counter));
+		else
+			__asm__ __volatile__("laad\t%0,(%2),%3\t# atomic64_add_return (%1)"
+					: "=r" (result), "+m" (v->counter)
+					: "r" (&v->counter), "r" (i));
+		result += i;
+	} else if (kernel_uses_llsc && R10000_LLSC_WAR) {
 		long temp;
 
 		__asm__ __volatile__(
@@ -552,7 +638,25 @@ static __inline__ long atomic64_sub_return(long i, atomic64_t * v)
 
 	smp_mb__before_llsc();
 
-	if (kernel_uses_llsc && R10000_LLSC_WAR) {
+	if (cpu_has_octeon2_isa && kernel_uses_llsc) {
+		/*
+		 * For proper barrier semantics, the preceding
+		 * smp_mb__before_llsc() must expand to syncw.
+		 */
+		if (__builtin_constant_p(i) && i == -1)
+			__asm__ __volatile__("laid\t%0,(%2)\t# atomic64_sub_return (%1)"
+					: "=r" (result), "+m" (v->counter)
+					: "r" (&v->counter));
+		else if (__builtin_constant_p(i) && i == 1)
+			__asm__ __volatile__("ladd\t%0,(%2)\t# atomic64_sub_return (%1)"
+					: "=r" (result), "+m" (v->counter)
+					: "r" (&v->counter));
+		else
+			__asm__ __volatile__("laad\t%0,(%2),%3\t# atomic64_sub_return (%1)"
+					: "=r" (result), "+m" (v->counter)
+					: "r" (&v->counter), "r" (-i));
+		result -= i;
+	} else if (kernel_uses_llsc && R10000_LLSC_WAR) {
 		long temp;
 
 		__asm__ __volatile__(
diff --git a/arch/mips/include/asm/cmpxchg.h b/arch/mips/include/asm/cmpxchg.h
index 466069b..7a61366 100644
--- a/arch/mips/include/asm/cmpxchg.h
+++ b/arch/mips/include/asm/cmpxchg.h
@@ -18,7 +18,23 @@ static inline unsigned long __xchg_u32(volatile int * m, unsigned int val)
 
 	smp_mb__before_llsc();
 
-	if (kernel_uses_llsc && R10000_LLSC_WAR) {
+	if (cpu_has_octeon2_isa && kernel_uses_llsc) {
+		if (__builtin_constant_p(val) && val == 0)
+			__asm__ __volatile__("lac\t%0,(%1)\t# xchg_u32"
+					: "=r" (retval)
+					: "r" (m)
+					: "memory");
+		else if (__builtin_constant_p(val) && val == 0xffffffffu)
+			__asm__ __volatile__("las\t%0,(%1)\t# xchg_u32"
+					: "=r" (retval)
+					: "r" (m)
+					: "memory");
+		else
+			__asm__ __volatile__("law\t%0,(%1),%2\t# xchg_u32"
+					: "=r" (retval)
+					: "r" (m), "r" (val)
+					: "memory");
+	} else if (kernel_uses_llsc && R10000_LLSC_WAR) {
 		unsigned long dummy;
 
 		__asm__ __volatile__(
@@ -70,7 +86,23 @@ static inline __u64 __xchg_u64(volatile __u64 * m, __u64 val)
 
 	smp_mb__before_llsc();
 
-	if (kernel_uses_llsc && R10000_LLSC_WAR) {
+	if (cpu_has_octeon2_isa && kernel_uses_llsc) {
+		if (__builtin_constant_p(val) && val == 0)
+			__asm__ __volatile__("lacd\t%0,(%1)\t# xchg_u64"
+					: "=r" (retval)
+					: "r" (m)
+					: "memory");
+		else if (__builtin_constant_p(val) && val == 0xffffffffffffffffull)
+			__asm__ __volatile__("lasd\t%0,(%1)\t# xchg_u64"
+					: "=r" (retval)
+					: "r" (m)
+					: "memory");
+		else
+			__asm__ __volatile__("lawd\t%0,(%1),%2\t# xchg_u64"
+					: "=r" (retval)
+					: "r" (m), "r" (val)
+					: "memory");
+	} else if (kernel_uses_llsc && R10000_LLSC_WAR) {
 		unsigned long dummy;
 
 		__asm__ __volatile__(
-- 
2.6.2

