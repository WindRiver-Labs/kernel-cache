From 2749507bb45fca6fe9e13458e42c023b9ffcef3e Mon Sep 17 00:00:00 2001
From: Yanjiang Jin <yanjiang.jin@windriver.com>
Date: Fri, 4 Aug 2017 12:32:26 +0800
Subject: [PATCH 173/184] cav-octeon3: align octeon-fpa3.c with SDK

This file is different between Cavium's SDK formatted patches and SDK's
final image(OCTEON-SDK-3.1.2-568 .i386.rpm + sdk_3.1.2_update_p10 .tgz),
align it with SDK's final image.

Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 arch/mips/cavium-octeon/octeon-fpa3.c          | 292 +++++++++++++++++++------
 arch/mips/cavium-octeon/octeon-rapidio.c       |  10 +-
 arch/mips/include/asm/octeon/octeon.h          |  17 +-
 drivers/net/ethernet/octeon/octeon3-ethernet.c |  60 +++--
 4 files changed, 264 insertions(+), 115 deletions(-)

diff --git a/arch/mips/cavium-octeon/octeon-fpa3.c b/arch/mips/cavium-octeon/octeon-fpa3.c
index 5e347fe..b5761fe 100644
--- a/arch/mips/cavium-octeon/octeon-fpa3.c
+++ b/arch/mips/cavium-octeon/octeon-fpa3.c
@@ -12,8 +12,65 @@
 
 #include <asm/octeon/octeon.h>
 
+
+#define GENMASK_ULL(h, l) \
+	(((~0ULL) << (l)) & (~0ULL >> (BITS_PER_LONG_LONG - 1 - (h))))
+
+/* Registers are accessed via xkphys */
+#define SET_XKPHYS			(1ull << 63)
+#define NODE_OFFSET			0x1000000000ull
+#define SET_NODE(node)			((node) * NODE_OFFSET)
+
+#define FPA_BASE			0x1280000000000ull
+#define SET_FPA_BASE(node)		(SET_XKPHYS + SET_NODE(node) + FPA_BASE)
+
+#define FPA_GEN_CFG(n)			(SET_FPA_BASE(n)           + 0x00000050)
+
+#define FPA_POOLX_CFG(n, p)		(SET_FPA_BASE(n) + (p<<3)  + 0x10000000)
+#define FPA_POOLX_START_ADDR(n, p)	(SET_FPA_BASE(n) + (p<<3)  + 0x10500000)
+#define FPA_POOLX_END_ADDR(n, p)	(SET_FPA_BASE(n) + (p<<3)  + 0x10600000)
+#define FPA_POOLX_STACK_BASE(n, p)	(SET_FPA_BASE(n) + (p<<3)  + 0x10700000)
+#define FPA_POOLX_STACK_END(n, p)	(SET_FPA_BASE(n) + (p<<3)  + 0x10800000)
+#define FPA_POOLX_STACK_ADDR(n, p)	(SET_FPA_BASE(n) + (p<<3)  + 0x10900000)
+
+#define FPA_AURAX_POOL(n, a)		(SET_FPA_BASE(n) + (a<<3)  + 0x20000000)
+#define FPA_AURAX_CFG(n, a)		(SET_FPA_BASE(n) + (a<<3)  + 0x20100000)
+#define FPA_AURAX_CNT(n, a)		(SET_FPA_BASE(n) + (a<<3)  + 0x20200000)
+#define FPA_AURAX_CNT_LIMIT(n, a)	(SET_FPA_BASE(n) + (a<<3)  + 0x20400000)
+#define FPA_AURAX_CNT_THRESHOLD(n, a)	(SET_FPA_BASE(n) + (a<<3)  + 0x20500000)
+#define FPA_AURAX_POOL_LEVELS(n, a)	(SET_FPA_BASE(n) + (a<<3)  + 0x20700000)
+#define FPA_AURAX_CNT_LEVELS(n, a)	(SET_FPA_BASE(n) + (a<<3)  + 0x20800000)
+
+static inline u64 oct_csr_read(u64 addr)
+{
+	return __raw_readq((void __iomem *)addr);
+}
+
+static inline void oct_csr_write(u64 data, u64 addr)
+{
+	__raw_writeq(data, (void __iomem *)addr);
+}
+
 static DEFINE_MUTEX(octeon_fpa3_lock);
 
+static int get_num_pools(void)
+{
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+		return 64;
+	if (OCTEON_IS_MODEL(OCTEON_CNF75XX) || OCTEON_IS_MODEL(OCTEON_CN73XX))
+		return 32;
+	return 0;
+}
+
+static int get_num_auras(void)
+{
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+		return 1024;
+	if (OCTEON_IS_MODEL(OCTEON_CNF75XX) || OCTEON_IS_MODEL(OCTEON_CN73XX))
+		return 512;
+	return 0;
+}
+
 /*
  * octeon_fpa3_init:		Initialize the fpa to default values.
  *
@@ -23,31 +80,29 @@ static DEFINE_MUTEX(octeon_fpa3_lock);
  */
 int octeon_fpa3_init(int node)
 {
-	union cvmx_fpa_gen_cfg	fpa_cfg;
-	static int		init_done;
+	static bool	init_done[2];
+	u64		data;
 	int			i;
 	int			aura_cnt;
 
 	mutex_lock(&octeon_fpa3_lock);
 
-	if (init_done)
+	if (init_done[node])
 		goto done;
 
-	aura_cnt = cvmx_fpa3_num_auras();
+	aura_cnt = get_num_auras();
 	for (i = 0; i < aura_cnt; i++) {
-		cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT(i),
-				    0x100000000ull);
-		cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT_LIMIT(i),
-				    0xfffffffffull);
-		cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT_THRESHOLD(i),
-				    0xffffffffeull);
+		oct_csr_write(0x100000000ull, FPA_AURAX_CNT(node, i));
+		oct_csr_write(0xfffffffffull, FPA_AURAX_CNT_LIMIT(node, i));
+		oct_csr_write(0xffffffffeull, FPA_AURAX_CNT_THRESHOLD(node, i));
 	}
 
-	fpa_cfg.u64 = cvmx_read_csr_node(node, CVMX_FPA_GEN_CFG);
-	fpa_cfg.s.lvl_dly = 3;
-	cvmx_write_csr_node(node, CVMX_FPA_GEN_CFG, fpa_cfg.u64);
+	data = oct_csr_read(FPA_GEN_CFG(node));
+	data &= ~GENMASK_ULL(9, 4);
+	data |= 3 << 4;
+	oct_csr_write(data, FPA_GEN_CFG(node));
 
-	init_done = 1;
+	init_done[node] = 1;
  done:
 	mutex_unlock(&octeon_fpa3_lock);
 	return 0;
@@ -68,35 +123,36 @@ EXPORT_SYMBOL(octeon_fpa3_init);
  */
 int octeon_fpa3_pool_init(int			node,
 			  int			pool_num,
-			  cvmx_fpa3_pool_t	*pool,
+			  int	*pool,
 			  void			**pool_stack,
 			  int			num_ptrs)
 {
+	struct global_resource_tag	tag;
+	char				buf[16];
 	u64				pool_stack_start;
 	u64				pool_stack_end;
-	union cvmx_fpa_poolx_end_addr	limit_addr;
-	union cvmx_fpa_poolx_cfg	cfg;
+	u64				data;
 	int				stack_size;
 	int				rc = 0;
 
 	mutex_lock(&octeon_fpa3_lock);
 
-	*pool = cvmx_fpa3_reserve_pool(node, pool_num);
-	if (!__cvmx_fpa3_pool_valid(*pool)) {
-		pr_err("Failed to reserve pool=%d\n", pool_num);
+	strncpy((char *)&tag.lo, "cvm_pool", 8);
+	snprintf(buf, 16, "_%d......", node);
+	memcpy(&tag.hi, buf, 8);
+
+	res_mgr_create_resource(tag, get_num_pools());
+	*pool = res_mgr_alloc(tag, pool_num, true);
+	if (*pool < 0) {
 		rc = -ENODEV;
 		goto error;
 	}
 
-	stack_size = (DIV_ROUND_UP(num_ptrs, 29) + 1) * 128;
-
-	cvmx_write_csr_node(node, CVMX_FPA_POOLX_CFG(pool->lpool), 0);
-	cvmx_write_csr_node(node, CVMX_FPA_POOLX_START_ADDR(pool->lpool), 128);
-	limit_addr.u64 = 0;
-	limit_addr.cn78xx.addr = ~0ll;
-	cvmx_write_csr_node(node, CVMX_FPA_POOLX_END_ADDR(pool->lpool),
-			    limit_addr.u64);
+	oct_csr_write(0, FPA_POOLX_CFG(node, *pool));
+	oct_csr_write(128, FPA_POOLX_START_ADDR(node, *pool));
+	oct_csr_write(GENMASK_ULL(41, 7), FPA_POOLX_END_ADDR(node, *pool));
 
+	stack_size = (DIV_ROUND_UP(num_ptrs, 29) + 1) * 128;
 	*pool_stack = kmalloc_node(stack_size, GFP_KERNEL, node);
 	if (!*pool_stack) {
 		pr_err("Failed to allocate pool stack memory pool=%d\n",
@@ -108,24 +164,18 @@ int octeon_fpa3_pool_init(int			node,
 	pool_stack_start = virt_to_phys(*pool_stack);
 	pool_stack_end = round_down(pool_stack_start + stack_size, 128);
 	pool_stack_start = round_up(pool_stack_start, 128);
+	oct_csr_write(pool_stack_start, FPA_POOLX_STACK_BASE(node, *pool));
+	oct_csr_write(pool_stack_start, FPA_POOLX_STACK_ADDR(node, *pool));
+	oct_csr_write(pool_stack_end, FPA_POOLX_STACK_END(node, *pool));
 
-	cvmx_write_csr_node(node, CVMX_FPA_POOLX_STACK_BASE(pool->lpool),
-			    pool_stack_start);
-	cvmx_write_csr_node(node, CVMX_FPA_POOLX_STACK_ADDR(pool->lpool),
-			    pool_stack_start);
-	cvmx_write_csr_node(node, CVMX_FPA_POOLX_STACK_END(pool->lpool),
-			    pool_stack_end);
-
-	cfg.u64 = 0;
-	cfg.s.l_type = 2; /* Load with DWB */
-	cfg.s.ena = 1;
-	cvmx_write_csr_node(node, CVMX_FPA_POOLX_CFG(pool->lpool), cfg.u64);
+	data = (2 << 3) | BIT(0);
+	oct_csr_write(data, FPA_POOLX_CFG(node, *pool));
 
 	mutex_unlock(&octeon_fpa3_lock);
 	return 0;
 
  error_stack:
-	cvmx_fpa3_release_pool(*pool);
+	res_mgr_free(tag, *pool);
  error:
 	mutex_unlock(&octeon_fpa3_lock);
 	return rc;
@@ -133,8 +183,32 @@ int octeon_fpa3_pool_init(int			node,
 EXPORT_SYMBOL(octeon_fpa3_pool_init);
 
 /*
+ * octeon_fpa3_release_pool:	Release a pool.
+ *
+ *  node:			Node pool is on.
+ *  pool:			Pool to release.
+ */
+void octeon_fpa3_release_pool(int node, int pool)
+{
+	struct global_resource_tag	tag;
+	char				buf[16];
+
+	mutex_lock(&octeon_fpa3_lock);
+
+	strncpy((char *)&tag.lo, "cvm_pool", 8);
+	snprintf(buf, 16, "_%d......", node);
+	memcpy(&tag.hi, buf, 8);
+
+	res_mgr_free(tag, pool);
+
+	mutex_unlock(&octeon_fpa3_lock);
+}
+EXPORT_SYMBOL(octeon_fpa3_release_pool);
+
+/*
  * octeon_fpa3_aura_init:	Initialize an aura.
  *
+ *  node:			Node to initialize aura on.
  *  pool:			Pool the aura belongs to.
  *  aura_num:			Requested aura number (-1 for don't care).
  *  aura:			Updated with the initialized aura number.
@@ -143,37 +217,45 @@ EXPORT_SYMBOL(octeon_fpa3_pool_init);
  *
  *  Returns:			Zero on success, error otherwise.
  */
-int octeon_fpa3_aura_init(cvmx_fpa3_pool_t	pool,
+int octeon_fpa3_aura_init(int		node,
+			  int		pool,
 			  int			aura_num,
-			  cvmx_fpa3_gaura_t	*aura,
+			  int		*aura,
 			  int			num_bufs,
 			  unsigned int		limit)
 {
-	union cvmx_fpa_aurax_cnt_levels	cnt_levels;
-	int				shift;
+	struct global_resource_tag	tag;
+	char				buf[16];
+	u64				data;
+	u64				shift;
 	unsigned int			drop;
 	unsigned int			pass;
 	int				rc = 0;
 
 	mutex_lock(&octeon_fpa3_lock);
 
-	*aura = cvmx_fpa3_reserve_aura(pool.node, aura_num);
-	if (!__cvmx_fpa3_aura_valid(*aura)) {
-		pr_err("Failed to reserved aura=%d\n", aura_num);
+	strncpy((char *)&tag.lo, "cvm_aura", 8);
+	snprintf(buf, 16, "_%d......", node);
+	memcpy(&tag.hi, buf, 8);
+
+	res_mgr_create_resource(tag, get_num_auras());
+	*aura = res_mgr_alloc(tag, aura_num, true);
+	if (*aura < 0) {
 		rc = -ENODEV;
 		goto error;
 	}
 
-	limit *= 2; /* Allow twice the limit before saturation at zero. */
+	oct_csr_write(0, FPA_AURAX_CFG(node, *aura));
+
+	/* Allow twice the limit before saturation at zero */
+	data = limit * 2;
+	oct_csr_write(data, FPA_AURAX_CNT_LIMIT(node, *aura));
+
+	oct_csr_write(limit, FPA_AURAX_CNT(node, *aura));
+	oct_csr_write(pool, FPA_AURAX_POOL(node, *aura));
 
-	cvmx_write_csr_node(aura->node, CVMX_FPA_AURAX_CFG(aura->laura), 0);
-	cvmx_write_csr_node(aura->node, CVMX_FPA_AURAX_CNT_LIMIT(aura->laura),
-			    limit);
-	cvmx_write_csr_node(aura->node, CVMX_FPA_AURAX_CNT(aura->laura), limit);
-	cvmx_write_csr_node(aura->node, CVMX_FPA_AURAX_POOL(aura->laura),
-			    pool.lpool);
-	cvmx_write_csr_node(aura->node, CVMX_FPA_AURAX_POOL_LEVELS(aura->laura),
-			    0); /* No per-pool RED/Drop */
+	/* No per-pool RED/Drop */
+	oct_csr_write(0, FPA_AURAX_POOL_LEVELS(node, *aura));
 
 	shift = 0;
 	while ((limit >> shift) > 255)
@@ -182,13 +264,9 @@ int octeon_fpa3_aura_init(cvmx_fpa3_pool_t	pool,
 	drop = (limit - num_bufs / 20) >> shift;	/* 95% */
 	pass = (limit - (num_bufs * 3) / 20) >> shift;	/* 85% */
 
-	cnt_levels.u64 = 0;
-	cnt_levels.s.shift = shift;
-	cnt_levels.s.red_ena = 1;
-	cnt_levels.s.drop = drop;
-	cnt_levels.s.pass = pass;
-	cvmx_write_csr_node(aura->node, CVMX_FPA_AURAX_CNT_LEVELS(aura->laura),
-			    cnt_levels.u64);
+	/* Enable per aura RED/drop */
+	data = BIT(38) | (shift << 32) | (drop << 16) | (pass << 8);
+	oct_csr_write(data, FPA_AURAX_CNT_LEVELS(node, *aura));
 
  error:
 	mutex_unlock(&octeon_fpa3_lock);
@@ -197,7 +275,82 @@ int octeon_fpa3_aura_init(cvmx_fpa3_pool_t	pool,
 EXPORT_SYMBOL(octeon_fpa3_aura_init);
 
 /*
- * octeon_mem_fill_fpa3:	Add buffers to an aura.
+ * octeon_fpa3_release_aura:	Release an aura.
+ *
+ *  node:			Node to aura is on.
+ *  aura:			Aura to release.
+ */
+void octeon_fpa3_release_aura(int node, int aura)
+{
+	struct global_resource_tag	tag;
+	char				buf[16];
+
+	mutex_lock(&octeon_fpa3_lock);
+
+	strncpy((char *)&tag.lo, "cvm_aura", 8);
+	snprintf(buf, 16, "_%d......", node);
+	memcpy(&tag.hi, buf, 8);
+
+	res_mgr_free(tag, aura);
+
+	mutex_unlock(&octeon_fpa3_lock);
+}
+EXPORT_SYMBOL(octeon_fpa3_release_aura);
+
+/*
+ * octeon_fpa3_alloc:		Get a buffer from a aura's pool.
+ *
+ *  node:			Node to free memory to.
+ *  aura:			Aura to free memory to.
+ *
+ *  returns:			Allocated buffer pointer, or NULL on error.
+ */
+void *octeon_fpa3_alloc(u64 node, int aura)
+{
+	u64	addr;
+	u64	buf_phys;
+	void	*buf = NULL;
+
+	/* Buffer pointers are obtained using load operations */
+	addr = BIT(63) | BIT(48) | (0x29ull << 40) | (node << 36) |
+		(aura << 16);
+	buf_phys = *(u64 *)addr;
+
+	if (buf_phys)
+		buf = phys_to_virt(buf_phys);
+
+	return buf;
+}
+EXPORT_SYMBOL(octeon_fpa3_alloc);
+
+/*
+ * octeon_fpa3_free:		Add a buffer back to the aura's pool.
+ *
+ *  node:			Node to free memory to.
+ *  aura:			Aura to free memory to.
+ *  buf:			Address of buffer to free to the aura's pool.
+ */
+void octeon_fpa3_free(u64 node, int aura, const void *buf)
+{
+	u64	buf_phys;
+	u64	addr;
+
+	buf_phys = virt_to_phys(buf);
+
+	/* Make sure that any previous writes to memory go out before we free
+	   this buffer. This also serves as a barrier to prevent GCC from
+	   reordering operations to after the free. */
+	wmb();
+
+	/* Buffers are added to fpa pools using store operations */
+	addr = BIT(63) | BIT(48) | (0x29ull << 40) | (node << 36) |
+		(aura << 16);
+	*(u64 *)addr = buf_phys;
+}
+EXPORT_SYMBOL(octeon_fpa3_free);
+
+/*
+ * octeon_fpa3_mem_fill:	Add buffers to an aura.
  *
  *  node:			Node to get memory from.
  *  cache:			Memory cache to allocate from.
@@ -206,9 +359,9 @@ EXPORT_SYMBOL(octeon_fpa3_aura_init);
  *
  *  Returns:			Zero on success, error otherwise.
  */
-int octeon_mem_fill_fpa3(int			node,
+int octeon_fpa3_mem_fill(int			node,
 			 struct kmem_cache	*cache,
-			 cvmx_fpa3_gaura_t	aura,
+			 int			aura,
 			 int			num_bufs)
 {
 	void	*mem;
@@ -220,18 +373,17 @@ int octeon_mem_fill_fpa3(int			node,
 	for (i = 0; i < num_bufs; i++) {
 		mem = kmem_cache_alloc_node(cache, GFP_KERNEL, node);
 		if (!mem) {
-			pr_err("Failed to allocate memory for aura=%d\n",
-			       aura.laura);
+			pr_err("Failed to allocate memory for aura=%d\n", aura);
 			rc = -ENOMEM;
 			break;
 		}
-		cvmx_fpa3_free(mem, aura, 0);
+		octeon_fpa3_free(node, aura, mem);
 	}
 
 	mutex_unlock(&octeon_fpa3_lock);
 	return rc;
 }
-EXPORT_SYMBOL(octeon_mem_fill_fpa3);
+EXPORT_SYMBOL(octeon_fpa3_mem_fill);
 
 MODULE_LICENSE("GPL");
 MODULE_DESCRIPTION("Cavium, Inc. Octeon III FPA manager.");
diff --git a/arch/mips/cavium-octeon/octeon-rapidio.c b/arch/mips/cavium-octeon/octeon-rapidio.c
index f1b5785..d3fee50 100644
--- a/arch/mips/cavium-octeon/octeon-rapidio.c
+++ b/arch/mips/cavium-octeon/octeon-rapidio.c
@@ -64,8 +64,8 @@ struct octeon_srio_port {
 static struct octeon_srio_port srio_ports[MAX_SRIO_PORTS];
 
 /* Pool/aura used by the dma engine */
-static cvmx_fpa3_pool_t cmd_pool;
-static cvmx_fpa3_gaura_t cmd_aura;
+static int cmd_pool;
+static int cmd_aura;
 static void *cmd_pool_stack;
 static struct kmem_cache *cmd_pool_cache;
 
@@ -1106,8 +1106,8 @@ static int octeon_rio_dma_cmd_pool_init(void)
 	octeon_fpa3_init(node);
 	octeon_fpa3_pool_init(node, CVMX_FPA_OUTPUT_BUFFER_POOL, &cmd_pool,
 			      &cmd_pool_stack, 4096);
-	octeon_fpa3_aura_init(cmd_pool, CVMX_FPA_OUTPUT_BUFFER_POOL, &cmd_aura,
-			      128, 20480);
+	octeon_fpa3_aura_init(node, cmd_pool, CVMX_FPA_OUTPUT_BUFFER_POOL,
+			      &cmd_aura, 128, 20480);
 
 	cmd_pool_cache = kmem_cache_create("dma cmd",
 					   CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE,
@@ -1115,7 +1115,7 @@ static int octeon_rio_dma_cmd_pool_init(void)
 	if (!cmd_pool_cache)
 		return -ENOMEM;
 
-	return octeon_mem_fill_fpa3(node, cmd_pool_cache, cmd_aura, 128);
+	return octeon_fpa3_mem_fill(node, cmd_pool_cache, cmd_aura, 128);
 }
 
 
diff --git a/arch/mips/include/asm/octeon/octeon.h b/arch/mips/include/asm/octeon/octeon.h
index 3675ce0..bf26bdb 100644
--- a/arch/mips/include/asm/octeon/octeon.h
+++ b/arch/mips/include/asm/octeon/octeon.h
@@ -511,13 +511,16 @@ int res_mgr_create_resource(struct global_resource_tag tag, int inst_cnt);
 
 #if IS_ENABLED(CONFIG_OCTEON_FPA3)
 int octeon_fpa3_init(int node);
-int octeon_fpa3_pool_init(int node, int pool_num, cvmx_fpa3_pool_t *pool,
-			  void **pool_stack, int num_ptrs);
-int octeon_fpa3_aura_init(cvmx_fpa3_pool_t pool, int aura_num,
-			  cvmx_fpa3_gaura_t *aura, int num_bufs,
-			  unsigned int limit);
-int octeon_mem_fill_fpa3(int node, struct kmem_cache *cache,
-			  cvmx_fpa3_gaura_t aura, int num_bufs);
+int octeon_fpa3_pool_init(int node, int pool_num, int *pool, void **pool_stack,
+			  int num_ptrs);
+int octeon_fpa3_aura_init(int node, int pool, int aura_num, int *aura,
+			  int num_bufs, unsigned int limit);
+int octeon_fpa3_mem_fill(int node, struct kmem_cache *cache, int aura,
+			 int num_bufs);
+void octeon_fpa3_free(u64 node, int aura, const void *buf);
+void *octeon_fpa3_alloc(u64 node, int aura);
+void octeon_fpa3_release_pool(int node, int pool);
+void octeon_fpa3_release_aura(int node, int aura);
 #endif
 
 #endif /* __ASM_OCTEON_OCTEON_H */
diff --git a/drivers/net/ethernet/octeon/octeon3-ethernet.c b/drivers/net/ethernet/octeon/octeon3-ethernet.c
index 91e8354..9ba3118 100644
--- a/drivers/net/ethernet/octeon/octeon3-ethernet.c
+++ b/drivers/net/ethernet/octeon/octeon3-ethernet.c
@@ -266,14 +266,14 @@ struct octeon3_ethernet_node {
 	bool napi_init_done;
 	int next_cpu_irq_affinity;
 	int numa_node;
-	cvmx_fpa3_pool_t  pki_packet_pool;
-	cvmx_fpa3_pool_t sso_pool;
-	cvmx_fpa3_pool_t pko_pool;
+	int pki_packet_pool;
+	int sso_pool;
+	int pko_pool;
 	void *sso_pool_stack;
 	void *pko_pool_stack;
 	void *pki_packet_pool_stack;
-	cvmx_fpa3_gaura_t sso_aura;
-	cvmx_fpa3_gaura_t pko_aura;
+	int sso_aura;
+	int pko_aura;
 	int tx_complete_grp;
 	int tx_irq;
 	cpumask_t tx_affinity_hint;
@@ -455,7 +455,7 @@ static int octeon3_eth_sso_init(unsigned int node, int aura)
 	max_grps = cvmx_sso_num_xgrp();
 	for (i = 0; i < max_grps; i++) {
 		u64 phys;
-		void *mem = cvmx_fpa3_alloc(__cvmx_fpa3_gaura(node, aura));
+		void *mem = octeon_fpa3_alloc(node, aura);
 		if (!mem) {
 			rv = -ENOMEM;
 			goto err;
@@ -528,7 +528,7 @@ static void octeon3_eth_sso_shutdown(unsigned int node)
 		addr = head.s.ptr;
 		addr <<= 7;
 		ptr = phys_to_virt(addr);
-		cvmx_fpa3_free(ptr, oen->sso_aura, 0);
+		octeon_fpa3_free(node, oen->sso_aura, ptr);
 
 		/* Clear pointers */
 		cvmx_write_csr_node(node, CVMX_SSO_XAQX_HEAD_PTR(i), 0);
@@ -650,9 +650,7 @@ static void octeon3_eth_replenish_rx(struct octeon3_ethernet *priv, int count)
 		kmemleak_not_leak(skb);
 		buf = (void **)PTR_ALIGN(skb->head, 128);
 		buf[SKB_PTR_OFFSET] = skb;
-		cvmx_fpa3_free(buf,
-			__cvmx_fpa3_gaura(priv->numa_node, priv->pki_laura),
-			0);
+		octeon_fpa3_free(priv->numa_node, priv->pki_laura, buf);
 	}
 }
 
@@ -818,19 +816,18 @@ static int octeon3_eth_global_init(unsigned int node)
 	if (rv)
 		goto done;
 
-	rv = octeon_fpa3_aura_init(oen->sso_pool, -1, &oen->sso_aura,
+	rv = octeon_fpa3_aura_init(node, oen->sso_pool, -1, &oen->sso_aura,
 				   num_packet_buffers, 20480);
 	if (rv)
 		goto done;
 
-	rv = octeon_fpa3_aura_init(oen->pko_pool, -1, &oen->pko_aura,
+	rv = octeon_fpa3_aura_init(node, oen->pko_pool, -1, &oen->pko_aura,
 				   num_packet_buffers, 20480);
 	if (rv)
 		goto done;
 
 	pr_err("octeon3_eth_global_init  SSO:%d:%d, PKO:%d:%d\n",
-	       oen->sso_pool.lpool, oen->sso_aura.laura,
-	       oen->pko_pool.lpool, oen->pko_aura.laura);
+	       oen->sso_pool, oen->sso_aura, oen->pko_pool, oen->pko_aura);
 
 	if (!octeon3_eth_sso_pko_cache) {
 		octeon3_eth_sso_pko_cache = kmem_cache_create("sso_pko", 4096, 128, 0, NULL);
@@ -840,18 +837,17 @@ static int octeon3_eth_global_init(unsigned int node)
 		}
 	}
 
-	rv = octeon_mem_fill_fpa3(node, octeon3_eth_sso_pko_cache,
+	rv = octeon_fpa3_mem_fill(node, octeon3_eth_sso_pko_cache,
 				  oen->sso_aura, 1024);
 	if (rv)
 		goto done;
 
-	rv = octeon_mem_fill_fpa3(node, octeon3_eth_sso_pko_cache,
+	rv = octeon_fpa3_mem_fill(node, octeon3_eth_sso_pko_cache,
 				   oen->pko_aura, 1024);
 	if (rv)
 		goto done;
 
-	BUG_ON(node != oen->sso_aura.node);
-	rv = octeon3_eth_sso_init(node, oen->sso_aura.laura);
+	rv = octeon3_eth_sso_init(node, oen->sso_aura);
 	if (rv)
 		goto done;
 
@@ -864,8 +860,7 @@ static int octeon3_eth_global_init(unsigned int node)
 		goto done;
 	}
 
-	__cvmx_helper_init_port_config_data();
-	rv = __cvmx_helper_pko3_init_global(node, oen->pko_aura.laura | (node << 10));
+	rv = __cvmx_helper_pko3_init_global(node, oen->pko_aura | (node << 10));
 	if (rv) {
 		pr_err("cvmx_helper_pko3_init_global failed\n");
 		rv = -ENODEV;
@@ -1539,7 +1534,7 @@ static int octeon3_eth_common_ndo_init(struct net_device	*netdev,
 	int first_skip, later_skip;
 	struct cvmx_xport xdq;
 	int r, i;
-	cvmx_fpa3_gaura_t aura;
+	int aura;
 
 	netif_carrier_off(netdev);
 
@@ -1580,9 +1575,9 @@ static int octeon3_eth_common_ndo_init(struct net_device	*netdev,
 	xdq = cvmx_helper_ipd_port_to_xport(node_dq);
 
 	priv->pko_queue = xdq.port;
-	octeon_fpa3_aura_init(oen->pki_packet_pool, -1, &aura,
+	octeon_fpa3_aura_init(priv->numa_node, oen->pki_packet_pool, -1, &aura,
 			      num_packet_buffers, num_packet_buffers * 2);
-	priv->pki_laura = aura.laura;
+	priv->pki_laura = aura;
 	aura2bufs_needed[priv->numa_node][priv->pki_laura] =
 		&priv->buffers_needed;
 
@@ -1760,8 +1755,7 @@ static void octeon3_eth_ndo_uninit(struct net_device *netdev)
 	/* Shutdwon pki for this interface */
 	ipd_port = cvmx_helper_get_ipd_port(priv->xiface, priv->port_index);
 	cvmx_helper_pki_port_shutdown(ipd_port);
-	cvmx_fpa3_release_aura(__cvmx_fpa3_gaura(priv->numa_node,
-						 priv->pki_laura));
+	octeon_fpa3_release_aura(priv->numa_node, priv->pki_laura);
 	aura2bufs_needed[priv->numa_node][priv->pki_laura] = NULL;
 
 	/* Shutdown pko for this interface */
@@ -1914,7 +1908,7 @@ static int octeon3_eth_common_ndo_stop(struct net_device *netdev)
 
 	/* Free the packet buffers */
 	for (;;) {
-		w = cvmx_fpa3_alloc(__cvmx_fpa3_gaura(priv->numa_node, priv->pki_laura));
+		w = octeon_fpa3_alloc(priv->numa_node, priv->pki_laura);
 		if (!w)
 			break;
 		skb = w[0];
@@ -2926,7 +2920,7 @@ static int octeon3_eth_global_exit(int node)
 
 	/* Shutdown pki */
 	cvmx_helper_pki_shutdown(node);
-	cvmx_fpa3_release_pool(oen->pki_packet_pool);
+	octeon_fpa3_release_pool(node, oen->pki_packet_pool);
 	kfree(oen->pki_packet_pool_stack);
 
 	/* Shutdown pko */
@@ -2934,13 +2928,13 @@ static int octeon3_eth_global_exit(int node)
 	for (;;) {
 		void **w;
 
-		w = cvmx_fpa3_alloc(oen->pko_aura);
+		w = octeon_fpa3_alloc(node, oen->pko_aura);
 		if (!w)
 			break;
 		kmem_cache_free(octeon3_eth_sso_pko_cache, w);
 	}
-	cvmx_fpa3_release_aura(oen->pko_aura);
-	cvmx_fpa3_release_pool(oen->pko_pool);
+	octeon_fpa3_release_aura(node, oen->pko_aura);
+	octeon_fpa3_release_pool(node, oen->pko_pool);
 	kfree(oen->pko_pool_stack);
 
 	/* Shutdown sso */
@@ -2949,13 +2943,13 @@ static int octeon3_eth_global_exit(int node)
 	for (;;) {
 		void **w;
 
-		w = cvmx_fpa3_alloc(oen->sso_aura);
+		w = octeon_fpa3_alloc(node, oen->sso_aura);
 		if (!w)
 			break;
 		kmem_cache_free(octeon3_eth_sso_pko_cache, w);
 	}
-	cvmx_fpa3_release_aura(oen->sso_aura);
-	cvmx_fpa3_release_pool(oen->sso_pool);
+	octeon_fpa3_release_aura(node, oen->sso_aura);
+	octeon_fpa3_release_pool(node, oen->sso_pool);
 	kfree(oen->sso_pool_stack);
 
 	return 0;
-- 
1.9.1

