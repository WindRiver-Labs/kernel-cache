From be17bbf546b33b6cbe3f248c75538b09f942a1df Mon Sep 17 00:00:00 2001
From: David Daney <david.daney@cavium.com>
Date: Tue, 25 Feb 2014 13:46:59 -0800
Subject: [PATCH 498/974] netdev: octeon3-ethernet: Add Preliminary device
 statistics.

Signed-off-by: David Daney <david.daney@cavium.com>
[Original patch taken from Cavium SDK 3.1.2-568]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
---
 drivers/net/ethernet/octeon/octeon3-ethernet.c | 69 +++++++++++++++++++++++++-
 1 file changed, 67 insertions(+), 2 deletions(-)

diff --git a/drivers/net/ethernet/octeon/octeon3-ethernet.c b/drivers/net/ethernet/octeon/octeon3-ethernet.c
index f0e6646..2036793 100644
--- a/drivers/net/ethernet/octeon/octeon3-ethernet.c
+++ b/drivers/net/ethernet/octeon/octeon3-ethernet.c
@@ -56,6 +56,7 @@ struct octeon3_ethernet {
 	struct net_device *netdev;
 	struct napi_struct napi;
 	int pki_laura;
+	int pki_pkind;
 	int pko_queue;
 	int numa_node;
 	int xiface;
@@ -66,6 +67,15 @@ struct octeon3_ethernet {
 	int rx_irq;
 	int tx_complete_grp;
 	atomic64_t tx_backlog;
+	spinlock_t stat_lock;
+	u64 last_packets;
+	u64 last_octets;
+	u64 last_dropped;
+	atomic64_t rx_packets;
+	atomic64_t rx_octets;
+	atomic64_t rx_dropped;
+	atomic64_t tx_packets;
+	atomic64_t tx_octets;
 };
 
 static DEFINE_MUTEX(octeon3_eth_init_mutex);
@@ -325,6 +335,8 @@ static int octeon3_eth_global_init(unsigned int node)
 	cvmx_pki_enable_backpressure(node);
 	cvmx_pki_parse_enable(node, 0);
 	cvmx_pki_enable(node);
+	/* Statistics per pkind */
+	cvmx_write_csr_node(node, CVMX_PKI_STAT_CTL, 0);
 
 	init_kthread_worker(&nd->tx_complete_worker);
 	init_kthread_work(&nd->tx_complete_work, octeon3_eth_tx_complete_worker);
@@ -544,8 +556,9 @@ static int octeon3_eth_ndo_init(struct net_device *netdev)
 
 	priv->rx_grp = cvmx_sso_allocate_group(priv->numa_node);
 	priv->tx_complete_grp = nd->tx_complete_grp;
+	priv->pki_pkind = cvmx_helper_get_pknd(priv->xiface, priv->port_index);
 	dev_err(netdev->dev.parent, "rx sso grp:%d aura:%d pknd:%d\n",
-		priv->rx_grp, priv->pki_laura, cvmx_helper_get_pknd(priv->xiface, priv->port_index));
+		priv->rx_grp, priv->pki_laura, priv->pki_pkind);
 	if (priv->rx_grp < 0) {
 		dev_err(netdev->dev.parent, "Failed to allocated SSO group\n");
 		return -ENODEV;
@@ -600,6 +613,14 @@ static int octeon3_eth_ndo_init(struct net_device *netdev)
 		cvmx_fpa3_free_aura(mem, priv->numa_node, laura, 0);
 	}
 
+	cvmx_write_csr_node(priv->numa_node, CVMX_PKI_STATX_STAT0(priv->pki_pkind), 0);
+	priv->last_packets = 0;
+
+	cvmx_write_csr_node(priv->numa_node, CVMX_PKI_STATX_STAT1(priv->pki_pkind), 0);
+	priv->last_octets = 0;
+
+	cvmx_write_csr_node(priv->numa_node, CVMX_PKI_STATX_STAT3(priv->pki_pkind), 0);
+	priv->last_dropped = 0;
 
 	mac = bgx_port_get_mac(priv->bgx_dev);
 	if (mac && is_valid_ether_addr(mac)) {
@@ -708,6 +729,14 @@ static int octeon3_eth_ndo_start_xmit(struct sk_buff *skb, struct net_device *ne
 	backlog = atomic64_inc_return(&priv->tx_backlog);
 	if (backlog > MAX_TX_QUEUE_DEPTH)
 		netif_stop_queue(netdev);
+
+	/* Adjust the port statistics. */
+	atomic64_inc(&priv->tx_packets);
+	atomic64_add(skb->len, &priv->tx_octets);
+
+	/* Make sure packet data writes are committed before
+	 * submitting the command below
+	 */
 	wmb();
 
 	send_hdr.u64 = 0;
@@ -818,11 +847,47 @@ skip_xmit:
 	return NETDEV_TX_OK;
 }
 
+static struct rtnl_link_stats64 *octeon3_eth_ndo_get_stats64(struct net_device *netdev,
+							     struct rtnl_link_stats64 *s)
+{
+	struct octeon3_ethernet *priv = netdev_priv(netdev);
+	u64 packets, octets, dropped;
+	u64 delta_packets, delta_octets, delta_dropped;
+
+	spin_lock(&priv->stat_lock);
+
+	packets = cvmx_read_csr_node(priv->numa_node, CVMX_PKI_STATX_STAT0(priv->pki_pkind));
+	octets = cvmx_read_csr_node(priv->numa_node, CVMX_PKI_STATX_STAT1(priv->pki_pkind));
+	dropped = cvmx_read_csr_node(priv->numa_node, CVMX_PKI_STATX_STAT3(priv->pki_pkind));
+
+	delta_packets = (packets - priv->last_packets) & ((1ull << 48) - 1);
+	delta_octets = (octets - priv->last_octets) & ((1ull << 48) - 1);
+	delta_dropped = (dropped - priv->last_dropped) & ((1ull << 48) - 1);
+
+	priv->last_packets = packets;
+	priv->last_octets = octets;
+	priv->last_dropped = dropped;
+
+	spin_unlock(&priv->stat_lock);
+
+	atomic64_add(delta_packets, &priv->rx_packets);
+	atomic64_add(delta_octets, &priv->rx_octets);
+	atomic64_add(delta_dropped, &priv->rx_dropped);
+
+	s->rx_packets = atomic64_read(&priv->rx_packets);
+	s->rx_bytes = atomic64_read(&priv->rx_octets);
+	s->rx_dropped = atomic64_read(&priv->rx_dropped);
+	s->tx_packets = atomic64_read(&priv->tx_packets);
+	s->tx_bytes = atomic64_read(&priv->tx_octets);
+	return s;
+}
+
 static const struct net_device_ops octeon3_eth_netdev_ops = {
 	.ndo_init		= octeon3_eth_ndo_init,
 	.ndo_uninit		= octeon3_eth_ndo_uninit,
 	.ndo_open		= octeon3_eth_ndo_open,
 	.ndo_start_xmit		= octeon3_eth_ndo_start_xmit,
+	.ndo_get_stats64	= octeon3_eth_ndo_get_stats64,
 };
 
 static int octeon3_eth_probe(struct platform_device *pdev)
@@ -853,7 +918,7 @@ static int octeon3_eth_probe(struct platform_device *pdev)
 	priv->numa_node = pd->numa_node;
 	priv->xiface = cvmx_helper_node_interface_to_xiface(pd->numa_node, pd->interface);
 	priv->port_index = pd->port;
-
+	spin_lock_init(&priv->stat_lock);
 	netdev->netdev_ops = &octeon3_eth_netdev_ops;
 
 	if (register_netdev(netdev) < 0) {
-- 
2.6.2

