From e7514eef1b0eee9fcf79f8343186013a14763180 Mon Sep 17 00:00:00 2001
From: Carlos Munoz <cmunoz@caviumnetworks.com>
Date: Sun, 6 Jul 2014 18:05:50 -0700
Subject: [PATCH 815/974] netdev: octeon3-ethernet: Performance improvements.

Signed-off-by: Carlos Munoz <cmunoz@caviumnetworks.com>
[Original patch taken from Cavium SDK 3.1.2-568]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
---
 drivers/net/ethernet/octeon/octeon3-ethernet.c | 54 +++++++++++++++++---------
 1 file changed, 35 insertions(+), 19 deletions(-)

diff --git a/drivers/net/ethernet/octeon/octeon3-ethernet.c b/drivers/net/ethernet/octeon/octeon3-ethernet.c
index 70b1e7f..6201373 100644
--- a/drivers/net/ethernet/octeon/octeon3-ethernet.c
+++ b/drivers/net/ethernet/octeon/octeon3-ethernet.c
@@ -252,7 +252,7 @@ struct octeon3_ethernet_node {
 };
 
 /* This array keeps track of the number of napis running on each cpu */
-static u8 octeon3_cpu_napi_cnt[NR_CPUS];
+static u8 octeon3_cpu_napi_cnt[NR_CPUS] __cacheline_aligned_in_smp;
 
 static int recycle_skbs = 1;
 module_param(recycle_skbs, int, 0644);
@@ -836,22 +836,29 @@ static struct sk_buff *octeon3_eth_work_to_skb(void *w)
  *				called with the napi_alloc_lock lock held.
  *
  *  node:			Node to allocate cpu from.
+ *  cpu:			Cpu to bind the napi to:
+ *					<  0: use any cpu.
+ *					>= 0: use requested cpu.
  *
  *  returns:			cpu number or a negative error code.
  */
-static int octeon3_napi_alloc_cpu(int	node)
+static int octeon3_napi_alloc_cpu(int	node,
+				  int	cpu)
 {
-	int				cpu;
 	int				min_cnt = MAX_NAPIS_PER_NODE;
 	int				min_cpu = -EBUSY;
 
-	for_each_cpu(cpu, cpumask_of_node(node)) {
-		if (octeon3_cpu_napi_cnt[cpu] == 0) {
-			min_cpu = cpu;
-			break;
-		} else if (octeon3_cpu_napi_cnt[cpu] < min_cnt) {
-			min_cnt = octeon3_cpu_napi_cnt[cpu];
-			min_cpu = cpu;
+	if (cpu >= 0)
+		min_cpu = cpu;
+	else {
+		for_each_cpu(cpu, cpumask_of_node(node)) {
+			if (octeon3_cpu_napi_cnt[cpu] == 0) {
+				min_cpu = cpu;
+				break;
+			} else if (octeon3_cpu_napi_cnt[cpu] < min_cnt) {
+				min_cnt = octeon3_cpu_napi_cnt[cpu];
+				min_cpu = cpu;
+			}
 		}
 	}
 
@@ -867,17 +874,20 @@ static int octeon3_napi_alloc_cpu(int	node)
  *
  *  cxt:			Receive context the napi will be added to.
  *  idx:			Napi index within the receive context.
+ *  cpu:			Cpu to bind the napi to:
+ *					<  0: use any cpu.
+ *					>= 0: use requested cpu.
  *
  *  Returns:			Pointer to napi wrapper or NULL on error.
  */
 static struct octeon3_napi_wrapper *octeon3_napi_alloc(struct octeon3_rx *cxt,
-						       int		  idx)
+						       int		  idx,
+						       int		  cpu)
 {
 	struct octeon3_ethernet_node	*oen;
 	struct octeon3_ethernet		*priv = cxt->parent;
 	int				node = priv->numa_node;
 	unsigned long			flags;
-	int				cpu;
 	int				i;
 
 	oen = octeon3_eth_node + node;
@@ -887,7 +897,7 @@ static struct octeon3_napi_wrapper *octeon3_napi_alloc(struct octeon3_rx *cxt,
 	for (i = 0; i < MAX_NAPIS_PER_NODE; i++) {
 		if (napi_wrapper[node][i].available) {
 			/* Allocate a cpu to use */
-			cpu = octeon3_napi_alloc_cpu(node);
+			cpu = octeon3_napi_alloc_cpu(node, cpu);
 			if (cpu < 0)
 				break;
 
@@ -977,7 +987,7 @@ static int octeon3_add_napi_to_cxt(struct octeon3_rx *cxt)
 	spin_unlock_irqrestore(&cxt->napi_idx_lock, flags);
 
 	/* Get a free napi block */
-	napiw = octeon3_napi_alloc(cxt, idx);
+	napiw = octeon3_napi_alloc(cxt, idx, -1);
 	if (unlikely(napiw == NULL)) {
 		spin_lock_irqsave(&cxt->napi_idx_lock, flags);
 		bitmap_clear(cxt->napi_idx_bitmap, idx, 1);
@@ -1530,7 +1540,8 @@ static int octeon3_eth_ndo_open(struct net_device *netdev)
 	int r;
 
 	for (i = 0; i < priv->num_rx_cxt; i++) {
-		unsigned int sso_intsn;
+		unsigned int	sso_intsn;
+		int		cpu;
 
 		rx = priv->rx_cxt + i;
 		sso_intsn = SSO_INTSN_EXE << 12 | rx->rx_grp;
@@ -1566,9 +1577,10 @@ static int octeon3_eth_ndo_open(struct net_device *netdev)
 			goto err3;
 		}
 		bitmap_set(priv->rx_cxt[i].napi_idx_bitmap, idx, 1);
+		cpu = cpumask_first(&rx->rx_affinity_hint);
 
 		priv->rx_cxt[i].napiw = octeon3_napi_alloc(&priv->rx_cxt[i],
-							   idx);
+							   idx, cpu);
 		if (priv->rx_cxt[i].napiw == NULL) {
 			r = -ENOMEM;
 			goto err4;
@@ -1739,11 +1751,15 @@ static int octeon3_eth_ndo_start_xmit(struct sk_buff *skb, struct net_device *ne
 		}
 	}
 
+	/* SSO can only fall behind when the skb is not recycled */
+	if (can_recycle_skb == false) {
+		oen = octeon3_eth_node + priv->numa_node;
+		aq_cnt.u64 = cvmx_read_csr_node(oen->numa_node,
+				CVMX_SSO_GRPX_AQ_CNT(oen->tx_complete_grp));
+	}
+
 	/* Drop the packet if pko or sso are not keeping up */
-	oen = octeon3_eth_node + priv->numa_node;
 	backlog = atomic64_inc_return(&priv->tx_backlog);
-	aq_cnt.u64 = cvmx_read_csr_node(oen->numa_node,
-				CVMX_SSO_GRPX_AQ_CNT(oen->tx_complete_grp));
 	if (unlikely(backlog > MAX_TX_QUEUE_DEPTH) ||
 	    (can_recycle_skb == false && aq_cnt.s.aq_cnt > 100000)) {
 		if (use_tx_queues) {
-- 
2.6.2

