From 9e21feccb2aae5a3314c3f954d898cf213eb4882 Mon Sep 17 00:00:00 2001
From: David Daney <david.daney@cavium.com>
Date: Sat, 5 Jul 2014 18:05:50 -0700
Subject: [PATCH 833/974] netdev: octeon-mgmt: Add cn78XX support.

The MAC for the cn78XX MIX unit is the BGX shared with the
octeon3-ethernet driver.  So, we use the BGX driver code for the MAC
related functions instead of the in-driver GMX management done for
earlier MIX support.

The interrupt routing and in-memory ring structure layout also
changes, so that has to be made general enough to adapt for either
case.

Bug #13457

Signed-off-by: David Daney <david.daney@cavium.com>
[Original patch taken from Cavium SDK 3.1.2-568]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
---
 drivers/net/ethernet/octeon/Kconfig       |   1 +
 drivers/net/ethernet/octeon/octeon_mgmt.c | 550 ++++++++++++++++++++++++------
 2 files changed, 440 insertions(+), 111 deletions(-)

diff --git a/drivers/net/ethernet/octeon/Kconfig b/drivers/net/ethernet/octeon/Kconfig
index 1dd804e..28f6f7b 100644
--- a/drivers/net/ethernet/octeon/Kconfig
+++ b/drivers/net/ethernet/octeon/Kconfig
@@ -59,6 +59,7 @@ config OCTEON_MGMT_ETHERNET
 	select MDIO_OCTEON
 	select NET_VENDOR_OCTEON
 	select OCTEON_ETHERNET_COMMON
+	select OCTEON_BGX_PORT
 	default y
 	---help---
 	  This option enables the ethernet driver for the management
diff --git a/drivers/net/ethernet/octeon/octeon_mgmt.c b/drivers/net/ethernet/octeon/octeon_mgmt.c
index 665683a..a8c7d3b 100644
--- a/drivers/net/ethernet/octeon/octeon_mgmt.c
+++ b/drivers/net/ethernet/octeon/octeon_mgmt.c
@@ -26,6 +26,8 @@
 #include <asm/octeon/octeon.h>
 #include <asm/octeon/cvmx-mixx-defs.h>
 #include <asm/octeon/cvmx-agl-defs.h>
+
+#include "octeon-bgx.h"
 #include "octeon_common.h"
 
 #define DRV_NAME "octeon_mgmt"
@@ -41,23 +43,14 @@
 #define OCTEON_MGMT_RX_RING_SIZE 512
 #define OCTEON_MGMT_TX_RING_SIZE 128
 
-union mgmt_port_ring_entry {
-	u64 d64;
-	struct {
+/* 14 bits of length */
+#define OCTEON_MGMT_RE_LEN_MASK 0x3fffULL
+
+/* 7 or 8 bits of code. 8th bit is always zero, so just look at 7 bits. */
+#define OCTEON_MGMT_RE_CODE_MASK 0x7fULL
+
 #define RING_ENTRY_CODE_DONE 0xf
 #define RING_ENTRY_CODE_MORE 0x10
-		__BITFIELD_FIELD(u64 reserved_62_63:2,
-		/* Length of the buffer/packet in bytes */
-		__BITFIELD_FIELD(u64 len:14,
-		/* For TX, signals that the packet should be timestamped */
-		 __BITFIELD_FIELD(u64 tstamp:1,
-		/* The RX error code */
-		__BITFIELD_FIELD(u64 code:7,
-		/* Physical address of the buffer */
-		__BITFIELD_FIELD(u64 addr:40,
-		;)))))
-	} s;
-};
 
 #define MIX_ORING1	0x0
 #define MIX_ORING2	0x8
@@ -96,13 +89,21 @@ union mgmt_port_ring_entry {
 #define AGL_GMX_TX_STAT9		0x2c8
 
 struct octeon_mgmt {
+	/* bgx_priv Must be first element.  May be unused in the case
+	 * where the MAC is not BGX
+	 */
+	struct bgx_port_netdev_priv bgx_priv;
 	struct net_device *netdev;
 	u64 mix;
 	u64 agl;
 	u64 agl_prt_ctl;
 	int port;
+	int numa_node;
 	int irq;
+	int irq_orthresh;
+	int irq_irthresh;
 	bool has_rx_tstamp;
+	bool has_o3_structs;
 	u64 *tx_ring;
 	dma_addr_t tx_ring_handle;
 	unsigned int tx_next;
@@ -114,6 +115,10 @@ struct octeon_mgmt {
 	/* RX variables only touched in napi_poll.  No locking necessary. */
 	u64 *rx_ring;
 	dma_addr_t rx_ring_handle;
+	u64 re_addr_mask;
+	u8 re_len_shift;
+	u8 re_code_shift;
+	u8 re_ts_bit;
 	unsigned int rx_next;
 	unsigned int rx_next_fill;
 	unsigned int rx_current_fill;
@@ -162,22 +167,34 @@ static void octeon_mgmt_set_tx_irq(struct octeon_mgmt *p, int enable)
 
 static void octeon_mgmt_enable_rx_irq(struct octeon_mgmt *p)
 {
-	octeon_mgmt_set_rx_irq(p, 1);
+	if (p->has_o3_structs)
+		enable_irq(p->irq_irthresh);
+	else
+		octeon_mgmt_set_rx_irq(p, 1);
 }
 
 static void octeon_mgmt_disable_rx_irq(struct octeon_mgmt *p)
 {
-	octeon_mgmt_set_rx_irq(p, 0);
+	if (p->has_o3_structs)
+		disable_irq_nosync(p->irq_irthresh);
+	else
+		octeon_mgmt_set_rx_irq(p, 0);
 }
 
 static void octeon_mgmt_enable_tx_irq(struct octeon_mgmt *p)
 {
-	octeon_mgmt_set_tx_irq(p, 1);
+	if (p->has_o3_structs)
+		enable_irq(p->irq_orthresh);
+	else
+		octeon_mgmt_set_tx_irq(p, 1);
 }
 
 static void octeon_mgmt_disable_tx_irq(struct octeon_mgmt *p)
 {
-	octeon_mgmt_set_tx_irq(p, 0);
+	if (p->has_o3_structs)
+		disable_irq_nosync(p->irq_orthresh);
+	else
+		octeon_mgmt_set_tx_irq(p, 0);
 }
 
 static unsigned int ring_max_fill(unsigned int ring_size)
@@ -187,7 +204,7 @@ static unsigned int ring_max_fill(unsigned int ring_size)
 
 static unsigned int ring_size_to_bytes(unsigned int ring_size)
 {
-	return ring_size * sizeof(union mgmt_port_ring_entry);
+	return ring_size * sizeof(u64);
 }
 
 static void octeon_mgmt_rx_fill_ring(struct net_device *netdev)
@@ -195,8 +212,8 @@ static void octeon_mgmt_rx_fill_ring(struct net_device *netdev)
 	struct octeon_mgmt *p = netdev_priv(netdev);
 
 	while (p->rx_current_fill < ring_max_fill(OCTEON_MGMT_RX_RING_SIZE)) {
-		unsigned int size;
-		union mgmt_port_ring_entry re;
+		u64 size;
+		u64 raw_re;
 		struct sk_buff *skb;
 
 		/* CN56XX pass 1 needs 8 bytes of padding.  */
@@ -208,14 +225,13 @@ static void octeon_mgmt_rx_fill_ring(struct net_device *netdev)
 		skb_reserve(skb, NET_IP_ALIGN);
 		__skb_queue_tail(&p->rx_list, skb);
 
-		re.d64 = 0;
-		re.s.len = size;
-		re.s.addr = dma_map_single(p->dev, skb->data,
-					   size,
-					   DMA_FROM_DEVICE);
+		raw_re = dma_map_single(p->dev, skb->data,
+					size,
+					DMA_FROM_DEVICE);
+		raw_re |= (size << p->re_len_shift);
 
 		/* Put it in the ring.  */
-		p->rx_ring[p->rx_next_fill] = re.d64;
+		p->rx_ring[p->rx_next_fill] = raw_re;
 		dma_sync_single_for_device(p->dev, p->rx_ring_handle,
 					   ring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),
 					   DMA_BIDIRECTIONAL);
@@ -252,12 +268,13 @@ static ktime_t ptp_to_ktime(u64 ptptime)
 static void octeon_mgmt_clean_tx_buffers(struct octeon_mgmt *p)
 {
 	union cvmx_mixx_orcnt mix_orcnt;
-	union mgmt_port_ring_entry re;
+	u64 re;
 	struct sk_buff *skb;
 	int cleaned = 0;
 	unsigned long flags;
 
 	mix_orcnt.u64 = cvmx_read_csr(p->mix + MIX_ORCNT);
+
 	while (mix_orcnt.s.orcnt) {
 		spin_lock_irqsave(&p->tx_list.lock, flags);
 
@@ -272,7 +289,7 @@ static void octeon_mgmt_clean_tx_buffers(struct octeon_mgmt *p)
 					ring_size_to_bytes(OCTEON_MGMT_TX_RING_SIZE),
 					DMA_BIDIRECTIONAL);
 
-		re.d64 = p->tx_ring[p->tx_next_clean];
+		re = p->tx_ring[p->tx_next_clean];
 		p->tx_next_clean =
 			(p->tx_next_clean + 1) % OCTEON_MGMT_TX_RING_SIZE;
 		skb = __skb_dequeue(&p->tx_list);
@@ -286,11 +303,12 @@ static void octeon_mgmt_clean_tx_buffers(struct octeon_mgmt *p)
 
 		spin_unlock_irqrestore(&p->tx_list.lock, flags);
 
-		dma_unmap_single(p->dev, re.s.addr, re.s.len,
+		dma_unmap_single(p->dev, re & p->re_addr_mask,
+				 (re >> p->re_len_shift) & OCTEON_MGMT_RE_LEN_MASK,
 				 DMA_TO_DEVICE);
 
 		/* Read the hardware TX timestamp if one was recorded */
-		if (unlikely(re.s.tstamp)) {
+		if (unlikely((re >> p->re_ts_bit) & 1)) {
 			struct skb_shared_hwtstamps ts;
 			/* Read the timestamp */
 			u64 ns = cvmx_read_csr(CVMX_MIXX_TSTAMP(p->port));
@@ -307,6 +325,12 @@ static void octeon_mgmt_clean_tx_buffers(struct octeon_mgmt *p)
 
 		mix_orcnt.u64 = cvmx_read_csr(p->mix + MIX_ORCNT);
 	}
+	if (p->has_o3_structs) {
+		union cvmx_mixx_isr mixx_isr;
+		mixx_isr.u64 = 0;
+		mixx_isr.s.orthresh = 1;
+		cvmx_write_csr(p->mix + MIX_ISR, mixx_isr.u64);
+	}
 
 	if (cleaned && netif_queue_stopped(p->netdev))
 		netif_wake_queue(p->netdev);
@@ -325,6 +349,9 @@ static void octeon_mgmt_update_rx_stats(struct net_device *netdev)
 	unsigned long flags;
 	u64 drop, bad;
 
+	if (p->has_o3_structs)
+		return;
+
 	/* These reads also clear the count registers.  */
 	drop = cvmx_read_csr(p->agl + AGL_GMX_RX_STATS_PKTS_DRP);
 	bad = cvmx_read_csr(p->agl + AGL_GMX_RX_STATS_PKTS_BAD);
@@ -366,22 +393,22 @@ static void octeon_mgmt_update_tx_stats(struct net_device *netdev)
 static u64 octeon_mgmt_dequeue_rx_buffer(struct octeon_mgmt *p,
 					 struct sk_buff **pskb)
 {
-	union mgmt_port_ring_entry re;
+	u64 entry;
 
 	dma_sync_single_for_cpu(p->dev, p->rx_ring_handle,
 				ring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),
 				DMA_BIDIRECTIONAL);
 
-	re.d64 = p->rx_ring[p->rx_next];
+	entry = p->rx_ring[p->rx_next];
 	p->rx_next = (p->rx_next + 1) % OCTEON_MGMT_RX_RING_SIZE;
 	p->rx_current_fill--;
 	*pskb = __skb_dequeue(&p->rx_list);
 
-	dma_unmap_single(p->dev, re.s.addr,
+	dma_unmap_single(p->dev, entry & p->re_addr_mask,
 			 ETH_FRAME_LEN + OCTEON_FRAME_HEADER_LEN,
 			 DMA_FROM_DEVICE);
 
-	return re.d64;
+	return entry;
 }
 
 
@@ -389,18 +416,22 @@ static int octeon_mgmt_receive_one(struct octeon_mgmt *p)
 {
 	struct net_device *netdev = p->netdev;
 	union cvmx_mixx_ircnt mix_ircnt;
-	union mgmt_port_ring_entry re;
+	u64 re;
+	int code, code2;
+	int len, len2;
 	struct sk_buff *skb;
 	struct sk_buff *skb2;
 	struct sk_buff *skb_new;
-	union mgmt_port_ring_entry re2;
+	u64 re2;
 	int rc = 1;
 
 
-	re.d64 = octeon_mgmt_dequeue_rx_buffer(p, &skb);
-	if (likely(re.s.code == RING_ENTRY_CODE_DONE)) {
+	re = octeon_mgmt_dequeue_rx_buffer(p, &skb);
+	code = (re >> p->re_code_shift) & OCTEON_MGMT_RE_CODE_MASK;
+	len = (re >> p->re_len_shift) & OCTEON_MGMT_RE_LEN_MASK;
+	if (likely(code == RING_ENTRY_CODE_DONE)) {
 		/* A good packet, send it up. */
-		skb_put(skb, re.s.len);
+		skb_put(skb, len);
 good:
 		/* Process the RX timestamp if it was recorded */
 		if (p->has_rx_tstamp) {
@@ -417,7 +448,7 @@ good:
 		netdev->stats.rx_bytes += skb->len;
 		netif_receive_skb(skb);
 		rc = 0;
-	} else if (re.s.code == RING_ENTRY_CODE_MORE) {
+	} else if (code == RING_ENTRY_CODE_MORE) {
 		/* Packet split across skbs.  This can happen if we
 		 * increase the MTU.  Buffers that are already in the
 		 * rx ring can then end up being too small.  As the rx
@@ -425,13 +456,15 @@ good:
 		 * will be used and we should go back to the normal
 		 * non-split case.
 		 */
-		skb_put(skb, re.s.len);
+		skb_put(skb, len);
 		do {
-			re2.d64 = octeon_mgmt_dequeue_rx_buffer(p, &skb2);
-			if (re2.s.code != RING_ENTRY_CODE_MORE
-				&& re2.s.code != RING_ENTRY_CODE_DONE)
+			re2 = octeon_mgmt_dequeue_rx_buffer(p, &skb2);
+			code2 = (re2 >> p->re_code_shift) & OCTEON_MGMT_RE_CODE_MASK;
+			len2 = (re2 >> p->re_len_shift) & OCTEON_MGMT_RE_LEN_MASK;
+			if (code2 != RING_ENTRY_CODE_MORE
+				&& code2 != RING_ENTRY_CODE_DONE)
 				goto split_error;
-			skb_put(skb2,  re2.s.len);
+			skb_put(skb2,  len2);
 			skb_new = skb_copy_expand(skb, 0, skb2->len,
 						  GFP_ATOMIC);
 			if (!skb_new)
@@ -443,7 +476,7 @@ good:
 			dev_kfree_skb_any(skb);
 			dev_kfree_skb_any(skb2);
 			skb = skb_new;
-		} while (re2.s.code == RING_ENTRY_CODE_MORE);
+		} while (code2 == RING_ENTRY_CODE_MORE);
 		goto good;
 	} else {
 		/* Some other error, discard it. */
@@ -457,8 +490,9 @@ split_error:
 	/* Discard the whole mess. */
 	dev_kfree_skb_any(skb);
 	dev_kfree_skb_any(skb2);
-	while (re2.s.code == RING_ENTRY_CODE_MORE) {
-		re2.d64 = octeon_mgmt_dequeue_rx_buffer(p, &skb2);
+	while (code2 == RING_ENTRY_CODE_MORE) {
+		re2 = octeon_mgmt_dequeue_rx_buffer(p, &skb2);
+		code2 = (re2 >> p->re_code_shift) & OCTEON_MGMT_RE_CODE_MASK;
 		dev_kfree_skb_any(skb2);
 	}
 	netdev->stats.rx_errors++;
@@ -504,6 +538,12 @@ static int octeon_mgmt_napi_poll(struct napi_struct *napi, int budget)
 	if (work_done < budget) {
 		/* We stopped because no more packets were available. */
 		napi_complete(napi);
+		if (p->has_o3_structs) {
+			union cvmx_mixx_isr mixx_isr;
+			mixx_isr.u64 = 0;
+			mixx_isr.s.irthresh = 1;
+			cvmx_write_csr(p->mix + MIX_ISR, mixx_isr.u64);
+		}
 		octeon_mgmt_enable_rx_irq(p);
 	}
 	octeon_mgmt_update_rx_stats(netdev);
@@ -512,11 +552,10 @@ static int octeon_mgmt_napi_poll(struct napi_struct *napi, int budget)
 }
 
 /* Reset the hardware to clean state.  */
-static void octeon_mgmt_reset_hw(struct octeon_mgmt *p)
+static void octeon_mgmt_common_reset_hw(struct octeon_mgmt *p)
 {
 	union cvmx_mixx_ctl mix_ctl;
 	union cvmx_mixx_bist mix_bist;
-	union cvmx_agl_gmx_bist agl_gmx_bist;
 
 	mix_ctl.u64 = 0;
 	cvmx_write_csr(p->mix + MIX_CTL, mix_ctl.u64);
@@ -533,6 +572,15 @@ static void octeon_mgmt_reset_hw(struct octeon_mgmt *p)
 		dev_warn(p->dev, "MIX failed BIST (0x%016llx)\n",
 			(unsigned long long)mix_bist.u64);
 
+}
+
+/* Reset the hardware to clean state.  */
+static void octeon_mgmt_reset_hw(struct octeon_mgmt *p)
+{
+	union cvmx_agl_gmx_bist agl_gmx_bist;
+
+	octeon_mgmt_common_reset_hw(p);
+
 	agl_gmx_bist.u64 = cvmx_read_csr(CVMX_AGL_GMX_BIST);
 	if (agl_gmx_bist.u64)
 		dev_warn(p->dev, "AGL failed BIST (0x%016llx)\n",
@@ -594,6 +642,26 @@ static irqreturn_t octeon_mgmt_interrupt(int cpl, void *dev_id)
 	return IRQ_HANDLED;
 }
 
+static irqreturn_t octeon_mgmt_o3_or_handler(int irq, void *dev_id)
+{
+	struct octeon_mgmt *p = dev_id;
+
+	octeon_mgmt_disable_tx_irq(p);
+	tasklet_schedule(&p->tx_clean_tasklet);
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t octeon_mgmt_o3_ir_handler(int irq, void *dev_id)
+{
+	struct octeon_mgmt *p = dev_id;
+
+	octeon_mgmt_disable_rx_irq(p);
+	napi_schedule(&p->napi);
+
+	return IRQ_HANDLED;
+}
+
 static int octeon_mgmt_ioctl_hwtstamp(struct net_device *netdev,
 				      struct ifreq *rq, int cmd)
 {
@@ -704,6 +772,16 @@ static int octeon_mgmt_ioctl(struct net_device *netdev,
 	}
 }
 
+static int octeon_mgmt_o3_ioctl(struct net_device *netdev,
+				struct ifreq *rq, int cmd)
+{
+	struct octeon_mgmt *p = netdev_priv(netdev);
+
+	if (p->phydev)
+		return phy_mii_ioctl(p->phydev, rq, cmd);
+	return -EINVAL;
+}
+
 static void octeon_mgmt_disable_link(struct octeon_mgmt *p)
 {
 	union cvmx_agl_gmx_prtx_cfg prtx_cfg;
@@ -874,6 +952,27 @@ static int octeon_mgmt_init_phy(struct net_device *netdev)
 	return 0;
 }
 
+static int octeon_mgmt_release_mem(struct octeon_mgmt *p)
+{
+	/* dma_unmap is a nop on Octeon, so just free everything.  */
+	skb_queue_purge(&p->tx_list);
+	skb_queue_purge(&p->rx_list);
+
+	if (p->rx_ring_handle)
+		dma_unmap_single(p->dev, p->rx_ring_handle,
+				 ring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),
+				 DMA_BIDIRECTIONAL);
+	kfree(p->rx_ring);
+
+	if (p->tx_ring_handle)
+		dma_unmap_single(p->dev, p->tx_ring_handle,
+				 ring_size_to_bytes(OCTEON_MGMT_TX_RING_SIZE),
+				 DMA_BIDIRECTIONAL);
+	kfree(p->tx_ring);
+
+	return 0;
+}
+
 static int octeon_mgmt_stop(struct net_device *netdev)
 {
 	struct octeon_mgmt *p = netdev_priv(netdev);
@@ -892,39 +991,32 @@ static int octeon_mgmt_stop(struct net_device *netdev)
 	if (p->irq)
 		free_irq(p->irq, netdev);
 
-	/* dma_unmap is a nop on Octeon, so just free everything.  */
-	skb_queue_purge(&p->tx_list);
-	skb_queue_purge(&p->rx_list);
+	return octeon_mgmt_release_mem(p);
+}
 
-	if (p->rx_ring_handle)
-		dma_unmap_single(p->dev, p->rx_ring_handle,
-				 ring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),
-				 DMA_BIDIRECTIONAL);
-	if (p->rx_ring)
-		kfree(p->rx_ring);
+static int octeon_mgmt_o3_stop(struct net_device *netdev)
+{
+	struct octeon_mgmt *p = netdev_priv(netdev);
 
-	if (p->tx_ring_handle)
-		dma_unmap_single(p->dev, p->tx_ring_handle,
-				 ring_size_to_bytes(OCTEON_MGMT_TX_RING_SIZE),
-				 DMA_BIDIRECTIONAL);
-	if (p->tx_ring)
-		kfree(p->tx_ring);
+	napi_disable(&p->napi);
+	netif_stop_queue(netdev);
 
-	return 0;
+	bgx_port_disable(netdev);
+
+	octeon_mgmt_reset_hw(p);
+
+
+	if (p->irq_orthresh)
+		free_irq(p->irq_orthresh, p);
+	if (p->irq_irthresh)
+		free_irq(p->irq_irthresh, p);
+
+	return octeon_mgmt_release_mem(p);
 }
 
-static int octeon_mgmt_open(struct net_device *netdev)
+static int octeon_mgmt_alloc_rings(struct net_device *netdev)
 {
 	struct octeon_mgmt *p = netdev_priv(netdev);
-	union cvmx_mixx_ctl mix_ctl;
-	union cvmx_agl_gmx_inf_mode agl_gmx_inf_mode;
-	union cvmx_mixx_oring1 oring1;
-	union cvmx_mixx_iring1 iring1;
-	union cvmx_agl_gmx_rxx_frm_ctl rxx_frm_ctl;
-	union cvmx_mixx_irhwm mix_irhwm;
-	union cvmx_mixx_orhwm mix_orhwm;
-	union cvmx_mixx_intena mix_intena;
-	struct sockaddr sa;
 
 	/* Allocate ring buffers.  */
 	p->tx_ring = kzalloc(ring_size_to_bytes(OCTEON_MGMT_TX_RING_SIZE),
@@ -943,7 +1035,7 @@ static int octeon_mgmt_open(struct net_device *netdev)
 	p->rx_ring = kzalloc(ring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),
 			     GFP_KERNEL);
 	if (!p->rx_ring)
-		goto err_nomem;
+		return -ENOMEM;
 	p->rx_ring_handle =
 		dma_map_single(p->dev, p->rx_ring,
 			       ring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),
@@ -953,6 +1045,125 @@ static int octeon_mgmt_open(struct net_device *netdev)
 	p->rx_next_fill = 0;
 	p->rx_current_fill = 0;
 
+	return 0;
+}
+
+static int octeon_mgmt_o3_open(struct net_device *netdev)
+{
+	struct octeon_mgmt *p = netdev_priv(netdev);
+	union cvmx_mixx_ctl mix_ctl;
+	union cvmx_mixx_oring1 oring1;
+	union cvmx_mixx_iring1 iring1;
+	union cvmx_mixx_irhwm mix_irhwm;
+	union cvmx_mixx_orhwm mix_orhwm;
+	struct irq_domain *domain;
+	int intsn_orthresh;
+	int intsn_irthresh;
+
+	if (octeon_mgmt_alloc_rings(netdev))
+		goto err_nomem;
+
+	bgx_port_mix_assert_reset(netdev, p->port, true);
+	octeon_mgmt_common_reset_hw(p);
+
+	mix_ctl.u64 = cvmx_read_csr(p->mix + MIX_CTL);
+
+	/* Bring it out of reset if needed. */
+	if (mix_ctl.s.reset) {
+		mix_ctl.s.reset = 0;
+		cvmx_write_csr(p->mix + MIX_CTL, mix_ctl.u64);
+		do {
+			mix_ctl.u64 = cvmx_read_csr(p->mix + MIX_CTL);
+		} while (mix_ctl.s.reset);
+	}
+	bgx_port_mix_assert_reset(netdev, p->port, false);
+
+	oring1.u64 = 0;
+	oring1.cn78xx.obase = p->tx_ring_handle >> 3;
+	oring1.cn78xx.osize = OCTEON_MGMT_TX_RING_SIZE;
+	cvmx_write_csr(p->mix + MIX_ORING1, oring1.u64);
+
+	iring1.u64 = 0;
+	iring1.cn78xx.ibase = p->rx_ring_handle >> 3;
+	iring1.cn78xx.isize = OCTEON_MGMT_RX_RING_SIZE;
+	cvmx_write_csr(p->mix + MIX_IRING1, iring1.u64);
+
+	/* Enable the port HW. Packets are not allowed until
+	 * cvmx_mgmt_port_enable() is called.
+	 */
+	mix_ctl.u64 = 0;
+	mix_ctl.s.crc_strip = 1;    /* Strip the ending CRC */
+	mix_ctl.s.en = 1;           /* Enable the port */
+	mix_ctl.s.nbtarb = 0;       /* Arbitration mode */
+	/* MII CB-request FIFO programmable high watermark */
+	mix_ctl.s.mrq_hwm = 1;
+#ifdef __LITTLE_ENDIAN
+	mix_ctl.s.lendian = 1;
+#endif
+	cvmx_write_csr(p->mix + MIX_CTL, mix_ctl.u64);
+
+	octeon_mgmt_rx_fill_ring(netdev);
+
+	/* Clear any pending interrupts */
+	cvmx_write_csr(p->mix + MIX_ISR, cvmx_read_csr(p->mix + MIX_ISR));
+
+	/* Interrupt every single RX packet */
+	mix_irhwm.u64 = 0;
+	mix_irhwm.s.irhwm = 0;
+	cvmx_write_csr(p->mix + MIX_IRHWM, mix_irhwm.u64);
+
+	/* Interrupt when we have 1 or more packets to clean.  */
+	mix_orhwm.u64 = 0;
+	mix_orhwm.s.orhwm = 0;
+	cvmx_write_csr(p->mix + MIX_ORHWM, mix_orhwm.u64);
+
+	napi_enable(&p->napi);
+
+	intsn_orthresh = 0x10002 + 0x10 * p->port;
+	intsn_irthresh = 0x10003 + 0x10 * p->port;
+
+	domain = octeon_irq_get_block_domain(p->numa_node, intsn_orthresh >> 12);
+	p->irq_orthresh = irq_create_mapping(domain, intsn_orthresh);
+	p->irq_irthresh = irq_create_mapping(domain, intsn_irthresh);
+
+	if (request_irq(p->irq_orthresh, octeon_mgmt_o3_or_handler, 0, netdev->name, p)) {
+		netdev_err(netdev, "request_irq(%d) failed.\n", p->irq_orthresh);
+		goto err_noirq;
+	}
+	if (request_irq(p->irq_irthresh, octeon_mgmt_o3_ir_handler, 0, netdev->name, p)) {
+		netdev_err(netdev, "request_irq(%d) failed.\n", p->irq_irthresh);
+		goto err_noirq;
+	}
+
+	p->last_link = 0;
+	p->last_speed = 0;
+
+	netif_wake_queue(netdev);
+
+	return bgx_port_enable(netdev);
+
+err_noirq:
+err_nomem:
+	octeon_mgmt_o3_stop(netdev);
+	return -ENOMEM;
+}
+
+static int octeon_mgmt_open(struct net_device *netdev)
+{
+	struct octeon_mgmt *p = netdev_priv(netdev);
+	union cvmx_mixx_ctl mix_ctl;
+	union cvmx_agl_gmx_inf_mode agl_gmx_inf_mode;
+	union cvmx_mixx_oring1 oring1;
+	union cvmx_mixx_iring1 iring1;
+	union cvmx_agl_gmx_rxx_frm_ctl rxx_frm_ctl;
+	union cvmx_mixx_irhwm mix_irhwm;
+	union cvmx_mixx_orhwm mix_orhwm;
+	union cvmx_mixx_intena mix_intena;
+	struct sockaddr sa;
+
+	if (octeon_mgmt_alloc_rings(netdev))
+		goto err_nomem;
+
 	octeon_mgmt_reset_hw(p);
 
 	mix_ctl.u64 = cvmx_read_csr(p->mix + MIX_CTL);
@@ -1115,6 +1326,8 @@ static int octeon_mgmt_open(struct net_device *netdev)
 	mix_orhwm.s.orhwm = 0;
 	cvmx_write_csr(p->mix + MIX_ORHWM, mix_orhwm.u64);
 
+	napi_enable(&p->napi);
+
 	/* Enable receive and transmit interrupts */
 	mix_intena.u64 = 0;
 	mix_intena.s.ithena = 1;
@@ -1167,7 +1380,6 @@ static int octeon_mgmt_open(struct net_device *netdev)
 	}
 
 	netif_wake_queue(netdev);
-	napi_enable(&p->napi);
 
 	return 0;
 
@@ -1180,16 +1392,18 @@ err_nomem:
 static int octeon_mgmt_xmit(struct sk_buff *skb, struct net_device *netdev)
 {
 	struct octeon_mgmt *p = netdev_priv(netdev);
-	union mgmt_port_ring_entry re;
 	unsigned long flags;
 	int rv = NETDEV_TX_BUSY;
+	u64 re;
+	u64 addr;
+
+	addr = dma_map_single(p->dev, skb->data,
+			      skb->len,
+			      DMA_TO_DEVICE);
 
-	re.d64 = 0;
-	re.s.tstamp = ((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) != 0);
-	re.s.len = skb->len;
-	re.s.addr = dma_map_single(p->dev, skb->data,
-				   skb->len,
-				   DMA_TO_DEVICE);
+	re = addr | (((u64)skb->len & OCTEON_MGMT_RE_LEN_MASK) << p->re_len_shift);
+	if ((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP))
+		re |= 1ull << p->re_ts_bit;
 
 	spin_lock_irqsave(&p->tx_list.lock, flags);
 
@@ -1202,7 +1416,7 @@ static int octeon_mgmt_xmit(struct sk_buff *skb, struct net_device *netdev)
 	if (unlikely(p->tx_current_fill >=
 		     ring_max_fill(OCTEON_MGMT_TX_RING_SIZE))) {
 		spin_unlock_irqrestore(&p->tx_list.lock, flags);
-		dma_unmap_single(p->dev, re.s.addr, re.s.len,
+		dma_unmap_single(p->dev, addr, skb->len,
 				 DMA_TO_DEVICE);
 		goto out;
 	}
@@ -1210,7 +1424,7 @@ static int octeon_mgmt_xmit(struct sk_buff *skb, struct net_device *netdev)
 	__skb_queue_tail(&p->tx_list, skb);
 
 	/* Put it in the ring.  */
-	p->tx_ring[p->tx_next] = re.d64;
+	p->tx_ring[p->tx_next] = re;
 	p->tx_next = (p->tx_next + 1) % OCTEON_MGMT_TX_RING_SIZE;
 	p->tx_current_fill++;
 
@@ -1229,7 +1443,8 @@ static int octeon_mgmt_xmit(struct sk_buff *skb, struct net_device *netdev)
 	netdev->trans_start = jiffies;
 	rv = NETDEV_TX_OK;
 out:
-	octeon_mgmt_update_tx_stats(netdev);
+	if (!p->has_o3_structs)
+		octeon_mgmt_update_tx_stats(netdev);
 	return rv;
 }
 
@@ -1243,6 +1458,34 @@ static void octeon_mgmt_poll_controller(struct net_device *netdev)
 }
 #endif
 
+static int octeon_mgmt_o3_init(struct net_device *netdev)
+{
+	const u8 *mac;
+
+	mac = bgx_port_get_mac(netdev);
+	if (mac && is_valid_ether_addr(mac)) {
+		memcpy(netdev->dev_addr, mac, ETH_ALEN);
+		netdev->addr_assign_type &= ~NET_ADDR_RANDOM;
+	} else {
+		eth_hw_addr_random(netdev);
+	}
+	bgx_port_set_rx_filtering(netdev);
+
+	return bgx_port_change_mtu(netdev, netdev->mtu);
+}
+
+static int octeon_mgmt_o3_set_mac_address(struct net_device *netdev, void *addr)
+{
+	int r = eth_mac_addr(netdev, addr);
+
+	if (r)
+		return r;
+
+	bgx_port_set_rx_filtering(netdev);
+
+	return 0;
+}
+
 static void octeon_mgmt_get_drvinfo(struct net_device *netdev,
 				    struct ethtool_drvinfo *info)
 {
@@ -1315,6 +1558,20 @@ static const struct net_device_ops octeon_mgmt_ops = {
 #endif
 };
 
+static const struct net_device_ops octeon_mgmt_o3_ops = {
+	.ndo_init		= octeon_mgmt_o3_init,
+	.ndo_open		= octeon_mgmt_o3_open,
+	.ndo_stop		= octeon_mgmt_o3_stop,
+	.ndo_start_xmit		= octeon_mgmt_xmit,
+	.ndo_set_rx_mode	= bgx_port_set_rx_filtering,
+	.ndo_set_mac_address	= octeon_mgmt_o3_set_mac_address,
+	.ndo_do_ioctl		= octeon_mgmt_o3_ioctl,
+	.ndo_change_mtu		= bgx_port_change_mtu,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= octeon_mgmt_poll_controller,
+#endif
+};
+
 static int octeon_mgmt_remove(struct platform_device *pdev)
 {
 	struct net_device *netdev = dev_get_drvdata(&pdev->dev);
@@ -1346,6 +1603,87 @@ static int octeon_mgmt_remove(struct platform_device *pdev)
 	return 0;
 }
 
+static void octeon_mgmt_probe_common(struct net_device *netdev,
+				     struct platform_device *pdev)
+{
+	struct octeon_mgmt *p = netdev_priv(netdev);
+
+	SET_NETDEV_DEV(netdev, &pdev->dev);
+	dev_set_drvdata(&pdev->dev, netdev);
+
+	netif_napi_add(netdev, &p->napi, octeon_mgmt_napi_poll,
+		       OCTEON_MGMT_NAPI_WEIGHT);
+
+	p->netdev = netdev;
+	p->dev = &pdev->dev;
+	p->has_rx_tstamp = false;
+
+	spin_lock_init(&p->lock);
+
+	skb_queue_head_init(&p->tx_list);
+	skb_queue_head_init(&p->rx_list);
+	tasklet_init(&p->tx_clean_tasklet,
+		     octeon_mgmt_clean_tx_tasklet, (unsigned long)p);
+
+	netdev->priv_flags |= IFF_UNICAST_FLT;
+
+	pdev->dev.coherent_dma_mask = DMA_BIT_MASK(64);
+	pdev->dev.dma_mask = &pdev->dev.coherent_dma_mask;
+
+	netif_carrier_off(netdev);
+}
+
+static int octeon_mgmt_o3_probe(struct platform_device *pdev)
+{
+	struct net_device *netdev;
+	struct octeon_mgmt *p;
+	struct bgx_platform_data *pd = dev_get_platdata(&pdev->dev);
+	int result = -EINVAL;
+
+	dev_notice(&pdev->dev, "Probed %d:%d:%d\n", pd->numa_node, pd->interface, pd->port);
+	netdev = alloc_etherdev(sizeof(struct octeon_mgmt));
+	if (netdev == NULL)
+		return -ENOMEM;
+
+	octeon_mgmt_probe_common(netdev, pdev);
+
+	bgx_port_set_netdev(pdev->dev.parent, netdev);
+
+	p = netdev_priv(netdev);
+	p->has_o3_structs = true;
+	p->re_len_shift = 50;
+	p->re_code_shift = 44;
+	p->re_ts_bit = 49;
+	/* 42-bits */
+	p->re_addr_mask = 0x3ffffffffffull;
+	p->port = pd->port;
+	p->numa_node = pd->numa_node;
+	netdev->netdev_ops = &octeon_mgmt_o3_ops;
+
+	p->mix_phys = 0x1070000100000ull + 0x800 * p->port + (1ull << 36) * p->numa_node;
+	p->mix_size = 0x100;
+
+	snprintf(netdev->name, IFNAMSIZ, "mgmt%d%d", p->numa_node, p->port);
+
+	if (!devm_request_mem_region(&pdev->dev, p->mix_phys, p->mix_size,
+				     "MIX")) {
+		p->mix_phys = 0;
+		dev_err(&pdev->dev, "ERROR: request_mem_region MIX failed\n");
+		goto err;
+	}
+
+	p->mix = (u64)devm_ioremap(&pdev->dev, p->mix_phys, p->mix_size);
+
+	result = register_netdev(netdev);
+	if (result)
+		goto err;
+
+	return 0;
+err:
+	octeon_mgmt_remove(pdev);
+	return result;
+}
+
 static int octeon_mgmt_probe(struct platform_device *pdev)
 {
 	struct net_device *netdev;
@@ -1358,20 +1696,23 @@ static int octeon_mgmt_probe(struct platform_device *pdev)
 	int len;
 	int result;
 
+	/* If it isn't the old MIX, it must be the new one */
+	if (!of_device_is_compatible(pdev->dev.of_node, "cavium,octeon-5750-mix"))
+		return octeon_mgmt_o3_probe(pdev);
+
 	netdev = alloc_etherdev(sizeof(struct octeon_mgmt));
 	if (netdev == NULL)
 		return -ENOMEM;
 
-	SET_NETDEV_DEV(netdev, &pdev->dev);
+	octeon_mgmt_probe_common(netdev, pdev);
 
-	dev_set_drvdata(&pdev->dev, netdev);
 	p = netdev_priv(netdev);
-	netif_napi_add(netdev, &p->napi, octeon_mgmt_napi_poll,
-		       OCTEON_MGMT_NAPI_WEIGHT);
-
-	p->netdev = netdev;
-	p->dev = &pdev->dev;
-	p->has_rx_tstamp = false;
+	p->has_o3_structs = false;
+	p->re_len_shift = 48;
+	p->re_code_shift = 40;
+	p->re_ts_bit = 47;
+	/* 40-bits */
+	p->re_addr_mask = 0xffffffffffull;
 
 	data = of_get_property(pdev->dev.of_node, "cell-index", &len);
 	if (data && len == sizeof(*data)) {
@@ -1446,15 +1787,6 @@ static int octeon_mgmt_probe(struct platform_device *pdev)
 	p->agl = (u64)devm_ioremap(&pdev->dev, p->agl_phys, p->agl_size);
 	p->agl_prt_ctl = (u64)devm_ioremap(&pdev->dev, p->agl_prt_ctl_phys,
 					   p->agl_prt_ctl_size);
-	spin_lock_init(&p->lock);
-
-	skb_queue_head_init(&p->tx_list);
-	skb_queue_head_init(&p->rx_list);
-	tasklet_init(&p->tx_clean_tasklet,
-		     octeon_mgmt_clean_tx_tasklet, (unsigned long)p);
-
-	netdev->priv_flags |= IFF_UNICAST_FLT;
-
 	netdev->netdev_ops = &octeon_mgmt_ops;
 	netdev->ethtool_ops = &octeon_mgmt_ethtool_ops;
 
@@ -1467,10 +1799,6 @@ static int octeon_mgmt_probe(struct platform_device *pdev)
 
 	p->phy_np = of_parse_phandle(pdev->dev.of_node, "phy-handle", 0);
 
-	pdev->dev.coherent_dma_mask = DMA_BIT_MASK(64);
-	pdev->dev.dma_mask = &pdev->dev.coherent_dma_mask;
-
-	netif_carrier_off(netdev);
 	result = register_netdev(netdev);
 	if (result)
 		goto err;
-- 
2.6.2

