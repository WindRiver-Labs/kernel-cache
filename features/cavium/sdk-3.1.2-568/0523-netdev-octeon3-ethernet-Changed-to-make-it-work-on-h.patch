From ad740409cff428d15f3adbde59b14fcc9dd37f4b Mon Sep 17 00:00:00 2001
From: David Daney <david.daney@cavium.com>
Date: Fri, 7 Feb 2014 13:46:59 -0800
Subject: [PATCH 523/974] netdev: octeon3-ethernet: Changed to make it work on
 hardware.

o Disable the ingress CAMs.

o Errata workarounds.

Signed-off-by: David Daney <david.daney@cavium.com>
[Original patch taken from Cavium SDK 3.1.2-568]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
---
 drivers/net/ethernet/octeon/octeon-bgx-port.c  |  2 +
 drivers/net/ethernet/octeon/octeon3-ethernet.c | 94 +++++++++++++++++++++++---
 2 files changed, 85 insertions(+), 11 deletions(-)

diff --git a/drivers/net/ethernet/octeon/octeon-bgx-port.c b/drivers/net/ethernet/octeon/octeon-bgx-port.c
index 1b11a1f..07cdd5b 100644
--- a/drivers/net/ethernet/octeon/octeon-bgx-port.c
+++ b/drivers/net/ethernet/octeon/octeon-bgx-port.c
@@ -100,6 +100,8 @@ void bgx_port_set_rx_filtering(struct net_device *netdev)
 	int available_cam_entries, current_cam_entry;
 	struct netdev_hw_addr *ha;
 
+	if(true)
+		return;
 	available_cam_entries = 8;
 	adr_ctl.u64 = 0;
 	adr_ctl.s.bcst_accept = 1; /* Accept all Broadcast*/
diff --git a/drivers/net/ethernet/octeon/octeon3-ethernet.c b/drivers/net/ethernet/octeon/octeon3-ethernet.c
index d5d5ebc..4df455e 100644
--- a/drivers/net/ethernet/octeon/octeon3-ethernet.c
+++ b/drivers/net/ethernet/octeon/octeon3-ethernet.c
@@ -76,6 +76,7 @@ struct octeon3_ethernet {
 	atomic64_t rx_dropped;
 	atomic64_t tx_packets;
 	atomic64_t tx_octets;
+	atomic64_t tx_dropped;
 };
 
 static DEFINE_MUTEX(octeon3_eth_init_mutex);
@@ -93,9 +94,10 @@ struct octeon3_ethernet_node {
 	struct task_struct *tx_complete_task;
 	struct kthread_worker tx_complete_worker;
 	struct kthread_work tx_complete_work;
+	u8 *sixty_zeros;
 };
 
-static int num_packet_buffers = 128;
+static int num_packet_buffers = 512;
 module_param(num_packet_buffers, int, 0444);
 MODULE_PARM_DESC(num_packet_buffers, "Number of packet buffers to allocate per port.");
 
@@ -155,9 +157,13 @@ static int octeon3_eth_sso_init(unsigned int node, int aura)
 	union cvmx_sso_aw_cfg cfg;
 	union cvmx_sso_xaq_aura xaq_aura;
 	union cvmx_sso_err0 err0;
+	union cvmx_sso_grpx_pri grp_pri;
 	int i;
 	int rv = 0;
 
+	grp_pri.u64 = 0;
+	grp_pri.s.weight = 0x3f;
+
 	cfg.u64 = 0;
 	cfg.s.stt = 1;
 	cfg.s.ldt = 1;
@@ -179,6 +185,8 @@ static int octeon3_eth_sso_init(unsigned int node, int aura)
 		phys = virt_to_phys(mem);
 		cvmx_write_csr_node(node, CVMX_SSO_XAQX_HEAD_PTR(i), phys);
 		cvmx_write_csr_node(node, CVMX_SSO_XAQX_TAIL_PTR(i), phys);
+		/* SSO-18678 */
+		cvmx_write_csr_node(node, CVMX_SSO_GRPX_PRI(i), grp_pri.u64);
 	}
 	err0.u64 = 0;
 	err0.s.fpe = 1;
@@ -206,6 +214,38 @@ static void octeon3_eth_sso_irq_set_armed(int node, int grp, bool v)
 	cvmx_write_csr_node(node, CVMX_SSO_GRPX_INT(grp), grp_int.u64);
 }
 
+struct wr_ret {
+	void *work;
+	u16 grp;
+};
+
+static inline struct wr_ret octeon3_eth_work_request_grp_sync(unsigned int lgrp, cvmx_pow_wait_t wait)
+{
+	cvmx_pow_load_addr_t ptr;
+	cvmx_pow_tag_load_resp_t result;
+	struct wr_ret r;
+	unsigned int node = cvmx_get_node_num() & 3;
+
+	ptr.u64 = 0;
+
+	ptr.swork_78xx.mem_region = CVMX_IO_SEG;
+	ptr.swork_78xx.is_io = 1;
+	ptr.swork_78xx.did = CVMX_OCT_DID_TAG_SWTAG;
+	ptr.swork_78xx.node = node;
+	ptr.swork_78xx.rtngrp = 1;
+	ptr.swork_78xx.grouped = 1;
+	ptr.swork_78xx.index = (lgrp & 0xff) | node << 8;
+	ptr.swork_78xx.wait = wait;
+
+	result.u64 = cvmx_read_csr(ptr.u64);
+	r.grp = result.s_work.grp;
+	if (result.s_work.no_work)
+		r.work = NULL;
+	else
+		r.work = cvmx_phys_to_ptr(result.s_work.addr);
+	return r;
+}
+
 static void octeon3_eth_tx_complete_worker(struct kthread_work *txcw)
 {
 	struct octeon3_ethernet_node *oen = container_of(txcw, struct octeon3_ethernet_node, tx_complete_work);
@@ -215,7 +255,10 @@ static void octeon3_eth_tx_complete_worker(struct kthread_work *txcw)
 		void **work;
 		struct sk_buff *skb;
 		struct octeon3_ethernet *priv;
-		work = cvmx_sso_work_request_grp_sync_nocheck(oen->tx_complete_grp, CVMX_POW_NO_WAIT);
+		struct wr_ret r;
+
+		r = octeon3_eth_work_request_grp_sync(oen->tx_complete_grp, CVMX_POW_NO_WAIT);
+		work = r.work;
 		if (work == NULL)
 			break;
 		skb = work[2];
@@ -253,6 +296,16 @@ static int octeon3_eth_global_init(unsigned int node)
 		goto done;
 
 	nd->numa_node = node;
+	nd->sixty_zeros = kzalloc_node(ETH_ZLEN, GFP_KERNEL, node);
+	if (!nd->sixty_zeros) {
+		rv = -ENOMEM;
+		goto done;
+	}
+	for (i = 0; i < 1024; i++) {
+		cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT(i), 0x100000000ull);
+		cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT_LIMIT(i), 0xfffffffffull);
+		cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT_THRESHOLD(i), 0xffffffffeull);
+	}
 	nd->sso_pko_pool = cvmx_fpa_alloc_pool(-1);
 	if (nd->sso_pko_pool < 0) {
 		rv = -ENODEV;
@@ -275,7 +328,7 @@ static int octeon3_eth_global_init(unsigned int node)
 	       nd->sso_pko_pool, nd->sso_aura, nd->pko_aura);
 
 	octeon3_eth_fpa_pool_init(node, nd->sso_pko_pool, 40960);
-	octeon3_eth_fpa_pool_init(node, nd->pki_packet_pool, 40960);
+	octeon3_eth_fpa_pool_init(node, nd->pki_packet_pool, 64 * num_packet_buffers);
 	octeon3_eth_fpa_pool_init(node, nd->tx_complete_pool, 40960);
 	octeon3_eth_fpa_aura_init(node, nd->sso_pko_pool, nd->sso_aura, 20480);
 	octeon3_eth_fpa_aura_init(node, nd->sso_pko_pool, nd->pko_aura, 20480);
@@ -381,15 +434,15 @@ static int octeon3_eth_rx_one(struct octeon3_ethernet *priv)
 	u8 *data;
 	struct sk_buff *skb;
 	union cvmx_buf_ptr_pki packet_ptr;
+	struct wr_ret r;
 
-	work = cvmx_sso_work_request_grp_sync_nocheck(priv->rx_grp, CVMX_POW_NO_WAIT);
-	prefetch(work);
+	r = octeon3_eth_work_request_grp_sync(priv->rx_grp, CVMX_POW_NO_WAIT);
+	work = r.work;
 	if (!work)
 		return 0;
 	skb = octeon3_eth_work_to_skb(work);
 	segments = work->word0.bufs;
 	ret = segments;
-
 	if (unlikely(work->word2.err_level <= CVMX_PKI_ERRLEV_LA &&
 		     work->word2.err_code != CVMX_PKI_OPCODE_RE_NONE))
 		goto drop; /* FIXME:  Free chained buffers in this case */
@@ -552,7 +605,8 @@ static int octeon3_eth_ndo_init(struct net_device *netdev)
 
 	priv->pko_queue = xdq.port;
 	priv->pki_laura = cvmx_fpa3_allocate_aura(priv->numa_node);
-	octeon3_eth_fpa_aura_init(priv->numa_node, nd->pki_packet_pool, priv->pki_laura, 1024);
+	octeon3_eth_fpa_aura_init(priv->numa_node, nd->pki_packet_pool, priv->pki_laura,
+				  num_packet_buffers * 2);
 
 	priv->rx_grp = cvmx_sso_allocate_group(priv->numa_node);
 	priv->tx_complete_grp = nd->tx_complete_grp;
@@ -681,7 +735,8 @@ static int octeon3_eth_ndo_open(struct net_device *netdev)
 
 	octeon3_eth_replentish_rx(priv, priv->rx_buf_count);
 
-	return bgx_port_enable(netdev);
+	r = bgx_port_enable(netdev);
+	return r;
 err:
 	/* Cleanup mapping ?? */
 	return r;
@@ -704,9 +759,17 @@ static int octeon3_eth_ndo_start_xmit(struct sk_buff *skb, struct net_device *ne
 	int frag_count;
 	int head_len, i;
 	u64 dma_addr;
+	int zero_pad;
 	void **work;
 
-	frag_count = 0;
+	/* PKO-20715, must manually pad. */
+	if (skb->len < ETH_ZLEN) {
+		zero_pad = ETH_ZLEN - skb->len;
+		frag_count = 1;
+	} else {
+		zero_pad = 0;
+		frag_count = 0;
+	}
 	if (skb_has_frag_list(skb))
 		skb_walk_frags(skb, skb_tmp)
 			frag_count++;
@@ -742,9 +805,9 @@ static int octeon3_eth_ndo_start_xmit(struct sk_buff *skb, struct net_device *ne
 #ifdef __LITTLE_ENDIAN
 	send_hdr.s.le = 1;
 #endif
-// broken in sim	send_hdr.s.n2 = 1; /* Don't allocate to L2 */
+/* broken in sim */	send_hdr.s.n2 = 1; /* Don't allocate to L2 */
 	send_hdr.s.df = 1; /* Don't automatically free to FPA */
-	send_hdr.s.total = skb->len;
+	send_hdr.s.total = skb->len + zero_pad;
 
 #ifndef BROKEN_SIMULATOR_CSUM
 	switch (skb->protocol) {
@@ -802,6 +865,13 @@ static int octeon3_eth_ndo_start_xmit(struct sk_buff *skb, struct net_device *ne
 		scr_off += sizeof(buf_ptr);
 	}
 
+	if (zero_pad) {
+		buf_ptr.s.addr = virt_to_phys(octeon3_eth_node[priv->numa_node].sixty_zeros);
+		buf_ptr.s.size = zero_pad;
+		cvmx_scratch_write64(scr_off, buf_ptr.u64);
+		scr_off += sizeof(buf_ptr);
+	}
+
 	/* Subtract 1 from the tx_backlog. */
 	send_mem.u64 = 0;
 	send_mem.s.subdc4 = CVMX_PKO_SENDSUBDC_MEM;
@@ -842,6 +912,7 @@ static int octeon3_eth_ndo_start_xmit(struct sk_buff *skb, struct net_device *ne
 
 	return NETDEV_TX_OK;
 skip_xmit:
+	atomic64_inc(&priv->tx_dropped);
 	dev_kfree_skb_any(skb);
 	return NETDEV_TX_OK;
 }
@@ -878,6 +949,7 @@ static struct rtnl_link_stats64 *octeon3_eth_ndo_get_stats64(struct net_device *
 	s->rx_dropped = atomic64_read(&priv->rx_dropped);
 	s->tx_packets = atomic64_read(&priv->tx_packets);
 	s->tx_bytes = atomic64_read(&priv->tx_octets);
+	s->tx_dropped = atomic64_read(&priv->tx_dropped);
 	return s;
 }
 
-- 
2.6.2

