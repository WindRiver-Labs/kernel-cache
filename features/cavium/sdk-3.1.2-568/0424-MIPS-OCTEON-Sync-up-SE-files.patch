From b98c6fda32aca9f2a9ad18cc6da37814b9afb66e Mon Sep 17 00:00:00 2001
From: Chandrakala Chavva <cchavva@caviumnetworks.com>
Date: Tue, 3 Dec 2013 13:15:35 -0800
Subject: [PATCH 424/974] MIPS: OCTEON: Sync-up SE files.

Signed-off-by: Chandrakala Chavva <cchavva@caviumnetworks.com>
[Original patch taken from Cavium SDK 3.1.2-568]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
---
 arch/mips/cavium-octeon/executive/cvmx-agl.c       |   2 +-
 arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c |  20 ++--
 arch/mips/cavium-octeon/executive/cvmx-debug.c     | 121 ++++++++++++---------
 arch/mips/cavium-octeon/executive/cvmx-pcie.c      |   6 +-
 arch/mips/include/asm/octeon/cvmx-debug.h          |   1 +
 arch/mips/include/asm/octeon/cvmx-pow.h            |  95 ++++++++++++++--
 6 files changed, 167 insertions(+), 78 deletions(-)

diff --git a/arch/mips/cavium-octeon/executive/cvmx-agl.c b/arch/mips/cavium-octeon/executive/cvmx-agl.c
index c24c4d3..c8d8884 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-agl.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-agl.c
@@ -193,7 +193,7 @@ int cvmx_agl_link_set(int port, cvmx_helper_link_info_t link_info, int mode)
 		/* MII (both speeds) and RGMII 1000 setting */
 		agl_clk.s.clk_cnt = 1;
 		/* Check other speeds for RGMII mode */
-		if (mode == CVMX_MGMT_PORT_RGMII_MODE) {
+		if ((mode == CVMX_MGMT_PORT_RGMII_MODE) || OCTEON_IS_OCTEON3()) {
 			if (link_info.s.speed == 10)
 				agl_clk.s.clk_cnt = 50;
 			else if (link_info.s.speed == 100)
diff --git a/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c b/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c
index 4466f29..8d98a6a 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c
@@ -43,7 +43,7 @@
  * Support functions for managing command queues used for
  * various hardware blocks.
  *
- * <hr>$Revision: 90196 $<hr>
+ * <hr>$Revision: 91009 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <linux/export.h>
@@ -89,25 +89,19 @@ static cvmx_cmd_queue_result_t __cvmx_cmd_queue_init_state_ptr(void)
 #if defined(CONFIG_CAVIUM_RESERVE32) && CONFIG_CAVIUM_RESERVE32
 	if (octeon_reserve32_memory)
 		__cvmx_cmd_queue_state_ptr = cvmx_bootmem_alloc_named_range(sizeof(*__cvmx_cmd_queue_state_ptr),
-					       octeon_reserve32_memory,
-					       (octeon_reserve32_memory
-					        + (CONFIG_CAVIUM_RESERVE32 << 20)
-					        - 1),
-					      128, alloc_name);
+									    octeon_reserve32_memory,
+									    octeon_reserve32_memory + (CONFIG_CAVIUM_RESERVE32 << 20) - 1,
+									    128, alloc_name);
 	else
 #endif
-		__cvmx_cmd_queue_state_ptr =
-			cvmx_bootmem_alloc_named(sizeof(*__cvmx_cmd_queue_state_ptr),
-						 128, alloc_name);
+		__cvmx_cmd_queue_state_ptr = cvmx_bootmem_alloc_named(sizeof(*__cvmx_cmd_queue_state_ptr), 128, alloc_name);
 #else
-	__cvmx_cmd_queue_state_ptr = cvmx_bootmem_alloc_named(sizeof(*__cvmx_cmd_queue_state_ptr),
-							      128, alloc_name);
+	__cvmx_cmd_queue_state_ptr = cvmx_bootmem_alloc_named(sizeof(*__cvmx_cmd_queue_state_ptr), 128, alloc_name);
 #endif
 	if (__cvmx_cmd_queue_state_ptr)
 		memset(__cvmx_cmd_queue_state_ptr, 0, sizeof(*__cvmx_cmd_queue_state_ptr));
 	else {
-		const cvmx_bootmem_named_block_desc_t *block_desc =
-				cvmx_bootmem_find_named_block(alloc_name);
+		const cvmx_bootmem_named_block_desc_t *block_desc = cvmx_bootmem_find_named_block(alloc_name);
 		if (block_desc)
 			__cvmx_cmd_queue_state_ptr = cvmx_phys_to_ptr(block_desc->base_addr);
 		else {
diff --git a/arch/mips/cavium-octeon/executive/cvmx-debug.c b/arch/mips/cavium-octeon/executive/cvmx-debug.c
index 8b8b1f8..548d2ae 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-debug.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-debug.c
@@ -174,7 +174,7 @@ static void cvmx_debug_memcpy_align(void *dest, const void *src, int size)
 	long long *dest1 = (long long *)dest;
 	const long long *src1 = (const long long *)src;
 	int i;
-	if (size == 40) {
+	if (size == 80) {
 		long long a0, a1, a2, a3, a4;
 		a0 = src1[0];
 		a1 = src1[1];
@@ -186,6 +186,16 @@ static void cvmx_debug_memcpy_align(void *dest, const void *src, int size)
 		dest1[2] = a2;
 		dest1[3] = a3;
 		dest1[4] = a4;
+		a0 = src1[5 + 0];
+		a1 = src1[5 + 1];
+		a2 = src1[5 + 2];
+		a3 = src1[5 + 3];
+		a4 = src1[5 + 4];
+		dest1[5 + 0] = a0;
+		dest1[5 + 1] = a1;
+		dest1[5 + 2] = a2;
+		dest1[5 + 3] = a3;
+		dest1[5 + 4] = a4;
 		return;
 	}
 	for (i = 0; i < size; i += 8) {
@@ -205,16 +215,14 @@ static inline cvmx_coremask_t *cvmx_debug_core_mask(void)
 #endif
 }
 
-static inline void cvmx_debug_update_state(cvmx_debug_state_t state)
+static inline void cvmx_debug_update_state(cvmx_debug_state_t *state)
 {
-	cvmx_debug_memcpy_align(cvmx_debug_globals->state, &state, sizeof(cvmx_debug_state_t));
+	cvmx_debug_memcpy_align(cvmx_debug_globals->state, state, sizeof(cvmx_debug_state_t));
 }
 
-static inline cvmx_debug_state_t cvmx_debug_get_state(void)
+static inline void cvmx_debug_get_state(cvmx_debug_state_t *state)
 {
-	cvmx_debug_state_t state;
-	cvmx_debug_memcpy_align(&state, cvmx_debug_globals->state, sizeof(cvmx_debug_state_t));
-	return state;
+	cvmx_debug_memcpy_align(state, cvmx_debug_globals->state, sizeof(cvmx_debug_state_t));
 }
 
 static void cvmx_debug_printf(char *format, ...) __attribute__ ((format(__printf__, 1, 2)));
@@ -230,9 +238,9 @@ static void cvmx_debug_printf(char *format, ...)
 	va_end(ap);
 }
 
-static inline int __cvmx_debug_in_focus(cvmx_debug_state_t state, unsigned core)
+static inline int __cvmx_debug_in_focus(cvmx_debug_state_t *state, unsigned core)
 {
-	return state.focus_core == core;
+	return state->focus_core == core;
 }
 
 static void cvmx_debug_install_handler(unsigned core)
@@ -360,7 +368,7 @@ void cvmx_debug_init(void)
 
 	{
 		cvmx_spinlock_lock(lock);
-		state = cvmx_debug_get_state();
+		cvmx_debug_get_state(&state);
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 		/* The Linux kernel only calls this once for the init core,
 		   setup the known cores to be all of the cores that Linux knows about. */
@@ -373,7 +381,7 @@ void cvmx_debug_init(void)
 		state.known_cores |= (1ull << core);
 		state.core_finished &= ~(1ull << core);
 #endif
-		cvmx_debug_update_state(state);
+		cvmx_debug_update_state(&state);
 		cvmx_spinlock_unlock(lock);
 	}
 
@@ -385,7 +393,7 @@ void cvmx_debug_init(void)
 #endif
 	{
 		cvmx_debug_printf("cvmx_debug_init core: %d\n", core);
-		state = cvmx_debug_get_state();
+		cvmx_debug_get_state(&state);
 		state.focus_core = core;
 		state.active_cores = state.known_cores;
 		state.focus_switch = 1;
@@ -393,7 +401,7 @@ void cvmx_debug_init(void)
 		/* COMMAND_NOP might not be 0. */
 		state.command = COMMAND_NOP;
 		cvmx_debug_printf("Known cores at init: 0x%llx\n", (long long)state.known_cores);
-		cvmx_debug_update_state(state);
+		cvmx_debug_update_state(&state);
 
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 		/* Initialize __cvmx_debug_stack_top_all for Linux kernel, all cores share the same address space. */
@@ -496,9 +504,9 @@ static int cvmx_debug_putpacket_hexint(char *buf, uint64_t value)
 	return cvmx_debug_putpacket_noformat(packet);
 }
 
-static int cvmx_debug_active_core(cvmx_debug_state_t state, unsigned core)
+static int cvmx_debug_active_core(cvmx_debug_state_t *state, unsigned core)
 {
-	return state.active_cores & (1ull << core);
+	return state->active_cores & (1ull << core);
 }
 
 static volatile cvmx_debug_core_context_t *cvmx_debug_core_context(void)
@@ -651,7 +659,8 @@ static cvmx_debug_command_t cvmx_debug_process_packet(const char *packet)
 {
 	const char *buf = packet;
 	cvmx_debug_command_t result = COMMAND_NOP;
-	cvmx_debug_state_t state = cvmx_debug_get_state();
+	cvmx_debug_state_t state;
+	cvmx_debug_get_state(&state);
 
 	/* A one letter command code represents what to do.  */
 	switch (*buf++) {
@@ -679,7 +688,7 @@ static cvmx_debug_command_t cvmx_debug_process_packet(const char *packet)
 				cvmx_debug_putpacket_hexint("F", core);
 				cvmx_debug_comms[cvmx_debug_globals->comm_type]->change_core(state.focus_core, core);
 				state.focus_core = core;
-				cvmx_debug_update_state(state);
+				cvmx_debug_update_state(&state);
 				break;
 			} else
 				cvmx_debug_putpacket_noformat("!Core is not in the exception handler. Focus not changed.");
@@ -696,7 +705,7 @@ static cvmx_debug_command_t cvmx_debug_process_packet(const char *packet)
 				state.step_isr = 1;	/* Step in ISR */
 			else
 				state.step_isr = 0;	/* Step over ISR */
-			cvmx_debug_update_state(state);
+			cvmx_debug_update_state(&state);
 		}
 		/* Fall through. The reply to the set step-isr command is the
 		   same as the get step-isr command */
@@ -723,7 +732,7 @@ static cvmx_debug_command_t cvmx_debug_process_packet(const char *packet)
 				state.active_cores |= 1ull << state.focus_core;
 			}
 
-			cvmx_debug_update_state(state);
+			cvmx_debug_update_state(&state);
 		}
 		/* Fall through. The reply to the set active cores command is the
 		   same as the get active cores command */
@@ -738,7 +747,7 @@ static cvmx_debug_command_t cvmx_debug_process_packet(const char *packet)
 				state.step_all = 1;	/* A step or continue will start all cores */
 			else
 				state.step_all = 0;	/* A step or continue only affects the focus core */
-			cvmx_debug_update_state(state);
+			cvmx_debug_update_state(&state);
 		}
 		/* Fall through. The reply to the set step-all command is the
 		   same as the get step-all command */
@@ -1065,14 +1074,15 @@ static cvmx_debug_command_t cvmx_debug_process_next_packet(int needs_proxy, vola
    that when the cores in active core mask are done executing the program, the
    focus will not be transfered to this core.  */
 
-static int cvmx_debug_stop_core(cvmx_debug_state_t state, unsigned core, cvmx_debug_register_t * debug_reg, int proxy)
+static int cvmx_debug_stop_core(cvmx_debug_state_t *state, unsigned core,
+		 cvmx_debug_register_t *debug_reg, int proxy)
 {
 	if (!cvmx_debug_active_core(state, core) && !debug_reg->s.dbp && !debug_reg->s.dss && (debug_reg->s.dint != 1)) {
 		debug_reg->s.sst = 0;
 		cvmx_debug_printf("Core #%d not in active cores, continuing.\n", core);
 		return 0;
 	}
-	if ((state.core_finished & (1ull << core)) && proxy)
+	if ((state->core_finished & (1ull << core)) && proxy)
 		return 0;
 	return 1;
 }
@@ -1141,18 +1151,18 @@ static void cvmx_debug_sync_up_cores(void)
 	/* NOTE this reads directly from the state array for speed reasons
 	   and we don't change the array. */
 	do {
-asm("": : :	"memory");
+		asm("" : : : "memory");
 	} while (cvmx_debug_globals->state[offsetof(cvmx_debug_state_t, step_all) / sizeof(uint64_t)]
 		 && cvmx_debug_globals->state[offsetof(cvmx_debug_state_t, handler_cores) / sizeof(uint64_t)] != 0);
 }
 
 /* Delay the focus core a little if it is likely another core needs to steal
    focus. Once we enter the main loop focus can't be stolen */
-static void cvmx_debug_delay_focus_core(cvmx_debug_state_t state, unsigned core, cvmx_debug_register_t * debug_reg)
+static void cvmx_debug_delay_focus_core(cvmx_debug_state_t *state, unsigned core, cvmx_debug_register_t *debug_reg)
 {
 	volatile int i;
 	/* Don't delay for single stepping or a breakpoint was hit.  */
-	if (debug_reg->s.dss || debug_reg->s.dbp || core != state.focus_core)
+	if (debug_reg->s.dss || debug_reg->s.dbp || core != state->focus_core)
 		return;
 
 	for (i = 0; i < 4800; i++) {
@@ -1171,19 +1181,19 @@ static void cvmx_debug_delay_focus_core(cvmx_debug_state_t state, unsigned core,
    && it was not the last focus-core,
    && last focus-core happens to be inside an ISR, blocking focus-switch
    then burn some cycles, to avoid unnecessary focus toggles. */
-static void cvmx_debug_delay_isr_core(unsigned core, uint32_t depc, int single_stepped_exc_only, cvmx_debug_state_t state)
+static void cvmx_debug_delay_isr_core(unsigned core, uint32_t depc, int single_stepped_exc_only, cvmx_debug_state_t *state)
 {
 	volatile uint64_t i;
-	if (!single_stepped_exc_only || state.step_isr || core == state.focus_core || state.focus_switch)
+	if (!single_stepped_exc_only || state->step_isr || core == state->focus_core || state->focus_switch)
 		return;
 
 	cvmx_debug_printf("Core #%u spinning for focus at 0x%x\n", core, (unsigned int)depc);
 
 	for (i = ISR_DELAY_COUNTER; i > 0; i--) {
-		state = cvmx_debug_get_state();
+		cvmx_debug_get_state(state);
 		/* Spin giving the focus core time to service ISR */
 		/* But cut short the loop, if we can.  Shrink down i, only once. */
-		if (i > 600000 && state.focus_switch)
+		if (i > 600000 && state->focus_switch)
 			i = 500000;
 	}
 
@@ -1192,10 +1202,12 @@ static void cvmx_debug_delay_isr_core(unsigned core, uint32_t depc, int single_s
 static int cvmx_debug_event_loop(cvmx_debug_register_t * debug_reg, volatile cvmx_debug_core_context_t * context, int needs_proxy)
 {
 	unsigned core = cvmx_get_core_num();
-	cvmx_debug_state_t state = cvmx_debug_get_state();
+	cvmx_debug_state_t state;
 	cvmx_debug_command_t command = COMMAND_NOP;
 	int single_stepped_exc_only = cvmx_debug_single_step_exc(debug_reg);
 
+	cvmx_debug_get_state(&state);
+
 	/* All cores should respect the focus core if it has to
 	   stop focus switching while servicing an interrupt.
 	   If the system is single-stepping, then the following
@@ -1206,7 +1218,7 @@ static int cvmx_debug_event_loop(cvmx_debug_register_t * debug_reg, volatile cvm
 	   the debugger exception stub fully. */
 	if (!state.step_isr && (cvmx_interrupt_in_isr || (context->cop0.status & 0x2ULL)) && single_stepped_exc_only) {
 		cvmx_spinlock_lock(&cvmx_debug_globals->lock);
-		state = cvmx_debug_get_state();
+		cvmx_debug_get_state(&state);
 		/* If this is the focus core, switch off focus switching
 		   till ISR_DELAY_COUNTER. This will let focus core
 		   keep the focus until the ISR is completed. */
@@ -1222,7 +1234,7 @@ static int cvmx_debug_event_loop(cvmx_debug_register_t * debug_reg, volatile cvm
 					  (unsigned long long)context->cop0.depc);
 			state.focus_switch = 1;
 		}
-		cvmx_debug_update_state(state);
+		cvmx_debug_update_state(&state);
 		cvmx_spinlock_unlock(&cvmx_debug_globals->lock);
 		cvmx_debug_printf("Core #%u resumed skipping isr.\n", core);
 		return 0;
@@ -1230,9 +1242,9 @@ static int cvmx_debug_event_loop(cvmx_debug_register_t * debug_reg, volatile cvm
 
 	/* Delay the focus core a little if it is likely another core needs to
 	   steal focus. Once we enter the main loop focus can't be stolen */
-	cvmx_debug_delay_focus_core(state, core, debug_reg);
+	cvmx_debug_delay_focus_core(&state, core, debug_reg);
 
-	cvmx_debug_delay_isr_core(core, context->cop0.depc, single_stepped_exc_only, state);
+	cvmx_debug_delay_isr_core(core, context->cop0.depc, single_stepped_exc_only, &state);
 
 	/* The following section of code does two critical things. First, it
 	   populates the handler_cores bitmask of all cores in the exception
@@ -1241,24 +1253,24 @@ static int cvmx_debug_event_loop(cvmx_debug_register_t * debug_reg, volatile cvm
 	{
 		cvmx_debug_printf("Core #%d stopped\n", core);
 		cvmx_spinlock_lock(&cvmx_debug_globals->lock);
-		state = cvmx_debug_get_state();
+		cvmx_debug_get_state(&state);
 
 		state.handler_cores |= (1ull << core);
 		cvmx_debug_may_elect_as_focus_core(&state, core, debug_reg);
 
 		/* Push all updates before exiting the critical section */
 		state.focus_switch = 1;
-		cvmx_debug_update_state(state);
+		cvmx_debug_update_state(&state);
 		cvmx_spinlock_unlock(&cvmx_debug_globals->lock);
 	}
-	if (__cvmx_debug_in_focus(state, core))
+	if (__cvmx_debug_in_focus(&state, core))
 		cvmx_debug_send_stop_reason(debug_reg, context);
 
 	do {
 		unsigned oldfocus = state.focus_core;
-		state = cvmx_debug_get_state();
+		cvmx_debug_get_state(&state);
 		/* Note the focus core can change in this loop. */
-		if (__cvmx_debug_in_focus(state, core)) {
+		if (__cvmx_debug_in_focus(&state, core)) {
 			if (context->remote_controlled == COMMAND_NOT_FOCUS && !needs_proxy)
 				context->remote_controlled = COMMAND_NOP;
 			CVMX_SYNCW;
@@ -1270,7 +1282,9 @@ static int cvmx_debug_event_loop(cvmx_debug_register_t * debug_reg, volatile cvm
 				cvmx_debug_send_stop_reason(debug_reg, context);
 
 			command = cvmx_debug_process_next_packet(needs_proxy, context);
-			state = cvmx_debug_get_state();
+
+			cvmx_spinlock_lock(&cvmx_debug_globals->lock);
+			cvmx_debug_get_state(&state);
 			if (command == COMMAND_NOT_FOCUS && state.focus_core != core) {
 				cvmx_debug_printf("change focus recieved to %d.\n", (int)state.focus_core);
 				command = COMMAND_NOP;
@@ -1279,8 +1293,9 @@ static int cvmx_debug_event_loop(cvmx_debug_register_t * debug_reg, volatile cvm
 			   step-all.  */
 			if (command != COMMAND_NOP && state.step_all) {
 				state.command = command;
-				cvmx_debug_update_state(state);
+				cvmx_debug_update_state(&state);
 			}
+			cvmx_spinlock_unlock(&cvmx_debug_globals->lock);
 		} else {
 			volatile int i;
 			/* Do a small sleep (around 2000 cycles) just so there is some time to process a focus change. */
@@ -1318,19 +1333,19 @@ static int cvmx_debug_event_loop(cvmx_debug_register_t * debug_reg, volatile cvm
 
 	{
 		cvmx_spinlock_lock(&cvmx_debug_globals->lock);
-		state = cvmx_debug_get_state();
+		cvmx_debug_get_state(&state);
 		state.handler_cores ^= (1ull << core);
-		cvmx_debug_update_state(state);
+		cvmx_debug_update_state(&state);
 		cvmx_spinlock_unlock(&cvmx_debug_globals->lock);
 	}
 
 	cvmx_debug_sync_up_cores();
 	/* Now that all cores are out, reset the command.  */
-	if (__cvmx_debug_in_focus(state, core)) {
+	if (__cvmx_debug_in_focus(&state, core)) {
 		cvmx_spinlock_lock(&cvmx_debug_globals->lock);
-		state = cvmx_debug_get_state();
+		cvmx_debug_get_state(&state);
 		state.command = COMMAND_NOP;
-		cvmx_debug_update_state(state);
+		cvmx_debug_update_state(&state);
 		cvmx_spinlock_unlock(&cvmx_debug_globals->lock);
 	}
 	return 0;
@@ -1496,9 +1511,9 @@ void __cvmx_debug_handler_stage3(uint64_t lo, uint64_t hi)
 	{
 		cvmx_debug_state_t state;
 		cvmx_spinlock_lock(&cvmx_debug_globals->lock);
-		state = cvmx_debug_get_state();
+		cvmx_debug_get_state(&state);
 		state.ever_been_in_debug = 1;
-		cvmx_debug_update_state(state);
+		cvmx_debug_update_state(&state);
 		cvmx_spinlock_unlock(&cvmx_debug_globals->lock);
 	}
 	cvmx_debug_print_cause(context);
@@ -1533,12 +1548,12 @@ void __cvmx_debug_handler_stage3(uint64_t lo, uint64_t hi)
 			cvmx_debug_state_t state;
 			unsigned core = cvmx_get_core_num();
 
-			state = cvmx_debug_get_state();
+			cvmx_debug_get_state(&state);
 			debug_reg.u64 = context->cop0.debug;
 			/* All cores stop on any exception.  See if we want nothing from this and
 			   it should resume.  This needs to be done for non proxy based debugging
 			   so that some non active-cores can control the other cores.  */
-			if (!cvmx_debug_stop_core(state, core, &debug_reg, needs_proxy)) {
+			if (!cvmx_debug_stop_core(&state, core, &debug_reg, needs_proxy)) {
 				context->cop0.debug = debug_reg.u64;
 				break;
 			}
@@ -1591,10 +1606,10 @@ void cvmx_debug_finish(void)
 #endif
 
 	cvmx_spinlock_lock(&cvmx_debug_globals->lock);
-	state = cvmx_debug_get_state();
+	cvmx_debug_get_state(&state);
 	state.known_cores ^= (1ull << coreid);
 	state.core_finished |= (1ull << coreid);
-	cvmx_debug_update_state(state);
+	cvmx_debug_update_state(&state);
 
 	/* Tell the user the core has finished. */
 	if (state.ever_been_in_debug)
@@ -1618,7 +1633,7 @@ void cvmx_debug_finish(void)
 			if (state.known_cores & (1ull << newcore)) {
 				cvmx_debug_printf("Routing uart interrupts to Core #%u.\n", newcore);
 				cvmx_debug_set_focus_core(&state, newcore);
-				cvmx_debug_update_state(state);
+				cvmx_debug_update_state(&state);
 				break;
 			}
 		}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pcie.c b/arch/mips/cavium-octeon/executive/cvmx-pcie.c
index 181fcf0..21ae773 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pcie.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pcie.c
@@ -42,7 +42,7 @@
  *
  * Interface to PCIe as a host(RC) or target(EP)
  *
- * <hr>$Revision: 90514 $<hr>
+ * <hr>$Revision: 91009 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
@@ -1204,8 +1204,8 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 		   by the OS after it enumerates the bus and assigns addresses to the
 		   PCIe busses */
 		for (i = 0; i < 4; i++) {
-			cvmx_write_csr(CVMX_PEMX_P2P_BARX_START(pcie_port, i), -1);
-			cvmx_write_csr(CVMX_PEMX_P2P_BARX_END(pcie_port, i), -1);
+			cvmx_write_csr(CVMX_PEMX_P2P_BARX_START(i, pcie_port), -1);
+			cvmx_write_csr(CVMX_PEMX_P2P_BARX_END(i, pcie_port), -1);
 		}
 	}
 
diff --git a/arch/mips/include/asm/octeon/cvmx-debug.h b/arch/mips/include/asm/octeon/cvmx-debug.h
index cb59b16..0902f8e 100644
--- a/arch/mips/include/asm/octeon/cvmx-debug.h
+++ b/arch/mips/include/asm/octeon/cvmx-debug.h
@@ -284,6 +284,7 @@ typedef struct cvmx_debug_globals_s {
 	uint64_t tlb_entries;
 	uint64_t state[sizeof(cvmx_debug_state_t) / sizeof(uint64_t)];
 	cvmx_spinlock_t lock;
+	uint32_t pad;
 
 	volatile cvmx_debug_core_context_t contextes[CVMX_MAX_CORES];
 } cvmx_debug_globals_t;
diff --git a/arch/mips/include/asm/octeon/cvmx-pow.h b/arch/mips/include/asm/octeon/cvmx-pow.h
index c04a601..4fcd6da 100644
--- a/arch/mips/include/asm/octeon/cvmx-pow.h
+++ b/arch/mips/include/asm/octeon/cvmx-pow.h
@@ -1784,12 +1784,59 @@ static inline void cvmx_pow_work_request_async_nocheck(int scr_addr, cvmx_pow_wa
 
 	/* scr_addr must be 8 byte aligned */
 	data.u64 = 0;
-	data.s.scraddr = scr_addr >> 3;
-	data.s.len = 1;
-	data.s.did = CVMX_OCT_DID_TAG_SWTAG;
-	data.s.wait = wait;
-	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE))
+	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
 		data.s_cn78xx.node = cvmx_get_node_num();
+		data.s_cn78xx.scraddr = scr_addr >> 3;
+		data.s_cn78xx.len = 1;
+		data.s_cn78xx.did = CVMX_OCT_DID_TAG_SWTAG;
+		data.s_cn78xx.wait = wait;
+	} else {
+		data.s.scraddr = scr_addr >> 3;
+		data.s.len = 1;
+		data.s.did = CVMX_OCT_DID_TAG_SWTAG;
+		data.s.wait = wait;
+	}
+	cvmx_send_single(data.u64);
+}
+
+/**
+ * Asynchronous work request.  Work is requested from the POW unit, and should later
+ * be checked with function cvmx_pow_work_response_async.
+ * This function does NOT wait for previous tag switches to complete,
+ * so the caller must ensure that there is not a pending tag switch.
+ *
+ * @param scr_addr Scratch memory address that response will be returned to,
+ *                  which is either a valid WQE, or a response with the invalid bit set.
+ *                  Byte address, must be 8 byte aligned.
+ * @param group     group to receive work for.
+ * @param wait      1 to cause response to wait for work to become available (or timeout)
+ *                  0 to cause response to return immediately
+ */
+static inline void cvmx_sso_work_request_grp_async_nocheck(int scr_addr, unsigned group, cvmx_pow_wait_t wait)
+{
+	cvmx_pow_iobdma_store_t data;
+	if (CVMX_ENABLE_POW_CHECKS)
+		__cvmx_pow_warn_if_pending_switch(__func__);
+
+	/* scr_addr must be 8 byte aligned */
+	data.u64 = 0;
+	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
+		unsigned int node = cvmx_get_node_num();
+		data.s_cn78xx.scraddr = scr_addr >> 3;
+		data.s_cn78xx.len = 1;
+		data.s_cn78xx.did = CVMX_OCT_DID_TAG_SWTAG;
+		data.s_cn78xx.grouped = 1;
+		data.s_cn78xx.index_grp_mask = (node << 8) | (group & 0xff);
+		data.s_cn78xx.wait = wait;
+		data.s_cn78xx.node = node;
+	} else {
+		/* GRP not supported on older chips, ignore it */
+		data.s.scraddr = scr_addr >> 3;
+		data.s.len = 1;
+		data.s.did = CVMX_OCT_DID_TAG_SWTAG;
+		data.s.wait = wait;
+	}
+
 	cvmx_send_single(data.u64);
 }
 
@@ -1848,7 +1895,7 @@ static inline cvmx_wqe_t *cvmx_pow_work_response_async(int scr_addr)
  */
 static inline uint64_t cvmx_pow_work_invalid(cvmx_wqe_t * wqe_ptr)
 {
-	return (wqe_ptr == NULL);
+	return (wqe_ptr == NULL);	/* FIXME: improve */
 }
 
 /**
@@ -1895,6 +1942,7 @@ static inline void cvmx_pow_tag_sw_nocheck(uint32_t tag, cvmx_pow_tag_type_t tag
 	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
 		tag_req.s_cn78xx_other.op   = CVMX_POW_TAG_OP_SWTAG;
 		tag_req.s_cn78xx_other.type = tag_type;
+		/* FIXME: tag */
 	} else if (octeon_has_feature(OCTEON_FEATURE_CN68XX_WQE)) {
 		tag_req.s_cn68xx_other.op = CVMX_POW_TAG_OP_SWTAG;
 		tag_req.s_cn68xx_other.tag = tag;
@@ -2006,7 +2054,7 @@ static inline void cvmx_pow_tag_sw_full_nocheck(cvmx_wqe_t * wqp, uint32_t tag,
 		tag_req.s_cn78xx_other.op = CVMX_POW_TAG_OP_SWTAG_FULL;
 		tag_req.s_cn78xx_other.type = tag_type;
 		tag_req.s_cn78xx_other.grp = group;
-		tag_req.s_cn78xx_other.wqp = CAST64(wqp);
+		tag_req.s_cn78xx_other.wqp = CAST64(wqp); /* FIXME: phys ? */
 	}
 	else if (octeon_has_feature(OCTEON_FEATURE_CN68XX_WQE)) {
 		tag_req.s_cn68xx_other.op = CVMX_POW_TAG_OP_SWTAG_FULL;
@@ -2571,7 +2619,7 @@ static inline void cvmx_pow_desched(uint64_t no_sched)
 	CVMX_SYNCWS;
 
 	tag_req.u64 = 0;
-	if (octeon_has_feature(OCTEON_FEATURE_CN68XX_WQE)) {
+	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
 		tag_req.s_cn78xx_other.op = CVMX_POW_TAG_OP_DESCH;
 		tag_req.s_cn78xx_other.no_sched = no_sched;
 	} else if (octeon_has_feature(OCTEON_FEATURE_CN68XX_WQE)) {
@@ -2667,6 +2715,37 @@ static inline uint32_t cvmx_pow_tag_get_hw_bits(uint64_t tag)
 	return (tag & cvmx_build_mask(32 - CVMX_TAG_SW_BITS));
 }
 
+static inline uint64_t cvmx_sso_get_total_wqe_count(void)
+{
+	if(OCTEON_IS_MODEL(OCTEON_CN78XX))
+	{
+		cvmx_sso_grpx_aq_cnt_t sso_iq_com_cnt;
+		int grp = 0;
+		uint64_t cnt = 0;
+
+		for( grp = 0; grp < CVMX_SSO_NUM_GROUPS_78XX; grp++) {
+			sso_iq_com_cnt.u64 = cvmx_read_csr_node(0,CVMX_SSO_GRPX_AQ_CNT(grp));
+			cnt += sso_iq_com_cnt.u64;
+		}
+		return cnt;
+	}
+	else if (OCTEON_IS_MODEL(OCTEON_CN68XX))
+	{
+		cvmx_sso_iq_com_cnt_t sso_iq_com_cnt;
+
+		sso_iq_com_cnt.u64 = cvmx_read_csr(CVMX_SSO_IQ_COM_CNT);
+
+		return (sso_iq_com_cnt.s.iq_cnt);
+	}
+	else
+	{
+		cvmx_pow_iq_com_cnt_t pow_iq_com_cnt;
+
+		pow_iq_com_cnt.u64 = cvmx_read_csr(CVMX_POW_IQ_COM_CNT);
+		return(pow_iq_com_cnt.s.iq_cnt);
+	}
+}
+
 /**
  * Store the current POW internal state into the supplied
  * buffer. It is recommended that you pass a buffer of at least
-- 
2.6.2

