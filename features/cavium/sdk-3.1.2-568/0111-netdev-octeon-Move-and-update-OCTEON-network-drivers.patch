From 785d0bdc317f4eb4447950bf17e6de810463b558 Mon Sep 17 00:00:00 2001
From: David Daney <david.daney@cavium.com>
Date: Wed, 12 Jun 2013 16:23:12 -0700
Subject: [PATCH 111/974] netdev: octeon: Move and update OCTEON network
 drivers from staging.

Signed-off-by: David Daney <david.daney@cavium.com>
[Original patch taken from Cavium SDK 3.1.2-568]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
---
 drivers/net/ethernet/Makefile                     |    2 +-
 drivers/net/ethernet/octeon/Kconfig               |   35 +
 drivers/net/ethernet/octeon/Makefile              |   14 +
 drivers/net/ethernet/octeon/ethernet-defines.h    |  109 ++
 drivers/net/ethernet/octeon/ethernet-mdio.c       |  414 +++++++
 drivers/net/ethernet/octeon/ethernet-mem.c        |  325 ++++++
 drivers/net/ethernet/octeon/ethernet-napi.c       |  473 ++++++++
 drivers/net/ethernet/octeon/ethernet-rgmii.c      |  300 +++++
 drivers/net/ethernet/octeon/ethernet-rx.c         |  593 ++++++++++
 drivers/net/ethernet/octeon/ethernet-sgmii.c      |  241 ++++
 drivers/net/ethernet/octeon/ethernet-spi.c        |  239 ++++
 drivers/net/ethernet/octeon/ethernet-srio.c       |  252 +++++
 drivers/net/ethernet/octeon/ethernet-tx.c         |  233 ++++
 drivers/net/ethernet/octeon/ethernet-xmit.c       |  395 +++++++
 drivers/net/ethernet/octeon/ethernet.c            | 1221 +++++++++++++++++++++
 drivers/net/ethernet/octeon/octeon-ethernet.h     |  223 ++++
 drivers/net/ethernet/octeon/octeon-pow-ethernet.c |  653 +++++++++++
 drivers/staging/Kconfig                           |    2 -
 drivers/staging/Makefile                          |    1 -
 drivers/staging/octeon/Kconfig                    |   13 -
 drivers/staging/octeon/Makefile                   |   22 -
 drivers/staging/octeon/ethernet-defines.h         |  106 --
 drivers/staging/octeon/ethernet-mdio.c            |  203 ----
 drivers/staging/octeon/ethernet-mem.c             |  177 ---
 drivers/staging/octeon/ethernet-mem.h             |   29 -
 drivers/staging/octeon/ethernet-rgmii.c           |  415 -------
 drivers/staging/octeon/ethernet-rx.c              |  563 ----------
 drivers/staging/octeon/ethernet-rx.h              |   52 -
 drivers/staging/octeon/ethernet-sgmii.c           |  128 ---
 drivers/staging/octeon/ethernet-spi.c             |  323 ------
 drivers/staging/octeon/ethernet-tx.c              |  728 ------------
 drivers/staging/octeon/ethernet-tx.h              |   34 -
 drivers/staging/octeon/ethernet-util.h            |   72 --
 drivers/staging/octeon/ethernet.c                 |  878 ---------------
 drivers/staging/octeon/octeon-ethernet.h          |  110 --
 35 files changed, 5721 insertions(+), 3857 deletions(-)
 create mode 100644 drivers/net/ethernet/octeon/ethernet-defines.h
 create mode 100644 drivers/net/ethernet/octeon/ethernet-mdio.c
 create mode 100644 drivers/net/ethernet/octeon/ethernet-mem.c
 create mode 100644 drivers/net/ethernet/octeon/ethernet-napi.c
 create mode 100644 drivers/net/ethernet/octeon/ethernet-rgmii.c
 create mode 100644 drivers/net/ethernet/octeon/ethernet-rx.c
 create mode 100644 drivers/net/ethernet/octeon/ethernet-sgmii.c
 create mode 100644 drivers/net/ethernet/octeon/ethernet-spi.c
 create mode 100644 drivers/net/ethernet/octeon/ethernet-srio.c
 create mode 100644 drivers/net/ethernet/octeon/ethernet-tx.c
 create mode 100644 drivers/net/ethernet/octeon/ethernet-xmit.c
 create mode 100644 drivers/net/ethernet/octeon/ethernet.c
 create mode 100644 drivers/net/ethernet/octeon/octeon-ethernet.h
 create mode 100644 drivers/net/ethernet/octeon/octeon-pow-ethernet.c
 delete mode 100644 drivers/staging/octeon/Kconfig
 delete mode 100644 drivers/staging/octeon/Makefile
 delete mode 100644 drivers/staging/octeon/ethernet-defines.h
 delete mode 100644 drivers/staging/octeon/ethernet-mdio.c
 delete mode 100644 drivers/staging/octeon/ethernet-mem.c
 delete mode 100644 drivers/staging/octeon/ethernet-mem.h
 delete mode 100644 drivers/staging/octeon/ethernet-rgmii.c
 delete mode 100644 drivers/staging/octeon/ethernet-rx.c
 delete mode 100644 drivers/staging/octeon/ethernet-rx.h
 delete mode 100644 drivers/staging/octeon/ethernet-sgmii.c
 delete mode 100644 drivers/staging/octeon/ethernet-spi.c
 delete mode 100644 drivers/staging/octeon/ethernet-tx.c
 delete mode 100644 drivers/staging/octeon/ethernet-tx.h
 delete mode 100644 drivers/staging/octeon/ethernet-util.h
 delete mode 100644 drivers/staging/octeon/ethernet.c
 delete mode 100644 drivers/staging/octeon/octeon-ethernet.h

diff --git a/drivers/net/ethernet/Makefile b/drivers/net/ethernet/Makefile
index 8268d85..4c3ce5b 100644
--- a/drivers/net/ethernet/Makefile
+++ b/drivers/net/ethernet/Makefile
@@ -47,7 +47,7 @@ obj-$(CONFIG_NET_NETX) += netx-eth.o
 obj-$(CONFIG_NET_VENDOR_NUVOTON) += nuvoton/
 obj-$(CONFIG_NET_VENDOR_NVIDIA) += nvidia/
 obj-$(CONFIG_LPC_ENET) += nxp/
-obj-$(CONFIG_OCTEON_MGMT_ETHERNET) += octeon/
+obj-$(CONFIG_NET_VENDOR_OCTEON) += octeon/
 obj-$(CONFIG_NET_VENDOR_OKI) += oki-semi/
 obj-$(CONFIG_KGDBOE) += kgdboe.o
 obj-$(CONFIG_ETHOC) += ethoc.o
diff --git a/drivers/net/ethernet/octeon/Kconfig b/drivers/net/ethernet/octeon/Kconfig
index d35ef22..2402fff 100644
--- a/drivers/net/ethernet/octeon/Kconfig
+++ b/drivers/net/ethernet/octeon/Kconfig
@@ -1,14 +1,49 @@
 #
 # Cavium network device configuration
 #
+config OCTEON_ETHERNET
+	tristate "Cavium Inc. OCTEON Ethernet support"
+	depends on CPU_CAVIUM_OCTEON
+	select PHYLIB
+	select MDIO_OCTEON
+	select NET_VENDOR_OCTEON
+	select OCTEON_ETHERNET_MEM
+	help
+	  This driver supports the builtin ethernet ports on Cavium
+	  Inc.' products in the Octeon family. This driver supports the
+	  CN3XXX, CN5XXX, CN6XXX and CNF7XXX OCTEON processors.
+
+	  To compile this driver as a module, choose M here.  The module
+	  will be called octeon-ethernet.
+
+config OCTEON_ETHERNET_MEM
+	tristate
+	depends on CPU_CAVIUM_OCTEON
+
+config OCTEON_POW_ONLY_ETHERNET
+	tristate "POW based internal only ethernet driver"
+	depends on  CPU_CAVIUM_OCTEON
+	depends on  OCTEON_ETHERNET
+	help
+	  This option enables a very simple ethernet driver for internal core
+	  to core traffic. It relies on another driver, octeon-ethernet,
+	  to perform all hardware setup. This driver's purpose is to supply
+	  basic networking between different Linux images running on the same
+	  chip. A single core loads the octeon-ethernet module, all other cores
+	  load this driver. On load, the driver waits for some other core to
+	  perform hardware setup.
 
 config OCTEON_MGMT_ETHERNET
 	tristate "Octeon Management port ethernet driver (CN5XXX, CN6XXX)"
 	depends on  CAVIUM_OCTEON_SOC
 	select PHYLIB
 	select MDIO_OCTEON
+	select NET_VENDOR_OCTEON
 	default y
 	---help---
 	  This option enables the ethernet driver for the management
 	  port on Cavium Networks' Octeon CN57XX, CN56XX, CN55XX,
 	  CN54XX, CN52XX, and CN6XXX chips.
+
+config NET_VENDOR_OCTEON
+	bool
diff --git a/drivers/net/ethernet/octeon/Makefile b/drivers/net/ethernet/octeon/Makefile
index efa41c1..9c1c7f8 100644
--- a/drivers/net/ethernet/octeon/Makefile
+++ b/drivers/net/ethernet/octeon/Makefile
@@ -3,3 +3,17 @@
 #
 
 obj-$(CONFIG_OCTEON_MGMT_ETHERNET)	+= octeon_mgmt.o
+obj-$(CONFIG_OCTEON_POW_ONLY_ETHERNET)	+= octeon-pow-ethernet.o
+obj-$(CONFIG_OCTEON_ETHERNET) +=  octeon-ethernet.o
+obj-$(CONFIG_OCTEON_ETHERNET_MEM) += ethernet-mem.o
+
+octeon-ethernet-objs := ethernet.o
+octeon-ethernet-objs += ethernet-mdio.o
+octeon-ethernet-objs += ethernet-rgmii.o
+octeon-ethernet-objs += ethernet-rx.o
+octeon-ethernet-objs += ethernet-sgmii.o
+octeon-ethernet-objs += ethernet-spi.o
+octeon-ethernet-objs += ethernet-tx.o
+ifdef CONFIG_RAPIDIO
+octeon-ethernet-objs += ethernet-srio.o
+endif
diff --git a/drivers/net/ethernet/octeon/ethernet-defines.h b/drivers/net/ethernet/octeon/ethernet-defines.h
new file mode 100644
index 0000000..c6aedfa
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-defines.h
@@ -0,0 +1,109 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+
+/*
+ * A few defines are used to control the operation of this driver:
+ *  CONFIG_CAVIUM_RESERVE32
+ *      This kernel config options controls the amount of memory configured
+ *      in a wired TLB entry for all processes to share. If this is set, the
+ *      driver will use this memory instead of kernel memory for pools. This
+ *      allows 32bit userspace application to access the buffers, but also
+ *      requires all received packets to be copied.
+ *  USE_SKBUFFS_IN_HW
+ *      Tells the driver to populate the packet buffers with kernel skbuffs.
+ *      This allows the driver to receive packets without copying them. It also
+ *      means that 32bit userspace can't access the packet buffers.
+ *  USE_HW_TCPUDP_CHECKSUM
+ *      Controls if the Octeon TCP/UDP checksum engine is used for packet
+ *      output. If this is zero, the kernel will perform the checksum in
+ *      software.
+ *  USE_ASYNC_IOBDMA
+ *      Use asynchronous IO access to hardware. This uses Octeon's asynchronous
+ *      IOBDMAs to issue IO accesses without stalling. Set this to zero
+ *      to disable this. Note that IOBDMAs require CVMSEG.
+ *  REUSE_SKBUFFS_WITHOUT_FREE
+ *      Allows the TX path to free an skbuff into the FPA hardware pool. This
+ *      can significantly improve performance for forwarding and bridging, but
+ *      may be somewhat dangerous. Checks are made, but if any buffer is reused
+ *      without the proper Linux cleanup, the networking stack may have very
+ *      bizarre bugs.
+ */
+#ifndef __ETHERNET_DEFINES_H__
+#define __ETHERNET_DEFINES_H__
+
+#define OCTEON_ETHERNET_VERSION "2.0"
+
+/* FAU */
+#define FAU_REG_END (2048)
+
+/* FPA defines */
+#define FPA_WQE_POOL_SIZE (1 * CVMX_CACHE_LINE_SIZE)
+#define FPA_PACKET_POOL_SIZE (16 * CVMX_CACHE_LINE_SIZE)
+#define FPA_OUTPUT_BUFFER_POOL_SIZE (8 * CVMX_CACHE_LINE_SIZE)
+
+/* TODO: replace this */
+#define CVMX_SCR_SCRATCH (0)
+
+#ifndef CONFIG_CAVIUM_RESERVE32
+#define CONFIG_CAVIUM_RESERVE32 0
+#endif
+
+#define USE_SKBUFFS_IN_HW           1
+#ifdef CONFIG_NETFILTER
+#define REUSE_SKBUFFS_WITHOUT_FREE  0
+#else
+#define REUSE_SKBUFFS_WITHOUT_FREE  0
+#endif
+
+#define USE_HW_TCPUDP_CHECKSUM      1
+
+/* Enable Random Early Dropping under load */
+#define USE_RED                     1
+#define USE_ASYNC_IOBDMA            (CONFIG_CAVIUM_OCTEON_CVMSEG_SIZE > 0)
+
+/*
+ * Allow SW based preamble removal at 10Mbps to workaround PHYs giving
+ * us bad preambles.
+ */
+#define USE_10MBPS_PREAMBLE_WORKAROUND 1
+/*
+ * Use this to have all FPA frees also tell the L2 not to write data
+ * to memory.
+ */
+#define DONT_WRITEBACK(x)           (x)
+/* Use this to not have FPA frees control L2 */
+/*#define DONT_WRITEBACK(x)         0   */
+
+/* Maximum number of SKBs to try to free per xmit packet. */
+#define MAX_OUT_QUEUE_DEPTH 1000
+
+#define FAU_NUM_PACKET_BUFFERS_TO_FREE (FAU_REG_END - sizeof(u32))
+
+#define TOTAL_NUMBER_OF_PORTS       (CVMX_PIP_NUM_INPUT_PORTS+1)
+
+
+#endif /* __ETHERNET_DEFINES_H__ */
diff --git a/drivers/net/ethernet/octeon/ethernet-mdio.c b/drivers/net/ethernet/octeon/ethernet-mdio.c
new file mode 100644
index 0000000..bbb6452
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-mdio.c
@@ -0,0 +1,414 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+#include <linux/kernel.h>
+#include <linux/ethtool.h>
+#include <linux/phy.h>
+#include <linux/ratelimit.h>
+#include <linux/of_mdio.h>
+#include <linux/net_tstamp.h>
+
+#include <net/dst.h>
+
+#include <asm/octeon/octeon.h>
+#include <asm/octeon/cvmx-srio.h>
+#include <asm/octeon/octeon-ethernet-user.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+
+#include <asm/octeon/cvmx-helper-board.h>
+
+#include <asm/octeon/cvmx-smix-defs.h>
+#include <asm/octeon/cvmx-gmxx-defs.h>
+#include <asm/octeon/cvmx-pip-defs.h>
+#include <asm/octeon/cvmx-pko-defs.h>
+
+static void cvm_oct_get_drvinfo(struct net_device *dev,
+				struct ethtool_drvinfo *info)
+{
+	strcpy(info->driver, "octeon-ethernet");
+	strcpy(info->version, OCTEON_ETHERNET_VERSION);
+	strcpy(info->bus_info, "Builtin");
+}
+
+static int cvm_oct_get_settings(struct net_device *dev, struct ethtool_cmd *cmd)
+{
+	cvmx_helper_link_info_t link_info;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	if (priv->phydev)
+		return phy_ethtool_gset(priv->phydev, cmd);
+
+	link_info = cvmx_helper_link_get(priv->ipd_port);
+	cmd->speed = link_info.s.link_up ? link_info.s.speed : 0;
+	cmd->duplex = link_info.s.full_duplex ? DUPLEX_FULL : DUPLEX_HALF;
+
+	return 0;
+}
+
+static int cvm_oct_set_settings(struct net_device *dev, struct ethtool_cmd *cmd)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	if (priv->phydev)
+		return phy_ethtool_sset(priv->phydev, cmd);
+
+	return -EOPNOTSUPP;
+}
+
+static int cvm_oct_nway_reset(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	if (priv->phydev)
+		return phy_start_aneg(priv->phydev);
+
+	return -EOPNOTSUPP;
+}
+
+const struct ethtool_ops cvm_oct_ethtool_ops = {
+	.get_drvinfo = cvm_oct_get_drvinfo,
+	.get_settings = cvm_oct_get_settings,
+	.set_settings = cvm_oct_set_settings,
+	.nway_reset = cvm_oct_nway_reset,
+	.get_link = ethtool_op_get_link,
+};
+
+/**
+ * cvm_oct_ioctl_hwtstamp - IOCTL support for timestamping
+ * @dev:    Device to change
+ * @rq:     the request
+ * @cmd:    the command
+ *
+ * Returns Zero on success
+ */
+static int cvm_oct_ioctl_hwtstamp(struct net_device *dev,
+				  struct ifreq *rq, int cmd)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	struct hwtstamp_config config;
+	union cvmx_mio_ptp_clock_cfg ptp;
+	union cvmx_gmxx_rxx_frm_ctl frm_ctl;
+	union cvmx_pip_prt_cfgx prt_cfg;
+
+	if (copy_from_user(&config, rq->ifr_data, sizeof(config)))
+		return -EFAULT;
+
+	if (config.flags) /* reserved for future extensions */
+		return -EINVAL;
+
+	/* Check the status of hardware for tiemstamps */
+	if (OCTEON_IS_MODEL(OCTEON_CN6XXX) || OCTEON_IS_MODEL(OCTEON_CNF7XXX)) {
+		/* Write TX timestamp into word 4 */
+		cvmx_write_csr(CVMX_PKO_REG_TIMESTAMP, 4);
+
+		switch (priv->imode) {
+		case CVMX_HELPER_INTERFACE_MODE_XAUI:
+		case CVMX_HELPER_INTERFACE_MODE_RXAUI:
+		case CVMX_HELPER_INTERFACE_MODE_SGMII:
+			break;
+		default:
+			/* No timestamp support*/
+			return -EOPNOTSUPP;
+		}
+
+		ptp.u64 = octeon_read_ptp_csr(CVMX_MIO_PTP_CLOCK_CFG);
+		if (!ptp.s.ptp_en) {
+			/* It should have been enabled by csrc-octeon-ptp */
+			netdev_err(dev, "Error: PTP clock not enabled\n");
+			/* No timestamp support*/
+			return -EOPNOTSUPP;
+		}
+	} else {
+			/* No timestamp support*/
+			return -EOPNOTSUPP;
+	}
+
+	switch (config.tx_type) {
+	case HWTSTAMP_TX_OFF:
+		priv->tx_timestamp_hw = 0;
+		break;
+	case HWTSTAMP_TX_ON:
+		priv->tx_timestamp_hw = 1;
+		break;
+	default:
+		return -ERANGE;
+	}
+
+	switch (config.rx_filter) {
+	case HWTSTAMP_FILTER_NONE:
+		priv->rx_timestamp_hw = 0;
+
+		frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface));
+		frm_ctl.s.ptp_mode = 0;
+		cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface), frm_ctl.u64);
+
+		prt_cfg.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(priv->ipd_pkind));
+		prt_cfg.s.skip = 0;
+		cvmx_write_csr(CVMX_PIP_PRT_CFGX(priv->ipd_pkind), prt_cfg.u64);
+		break;
+	case HWTSTAMP_FILTER_ALL:
+	case HWTSTAMP_FILTER_SOME:
+	case HWTSTAMP_FILTER_PTP_V1_L4_EVENT:
+	case HWTSTAMP_FILTER_PTP_V1_L4_SYNC:
+	case HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ:
+	case HWTSTAMP_FILTER_PTP_V2_L4_EVENT:
+	case HWTSTAMP_FILTER_PTP_V2_L4_SYNC:
+	case HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ:
+	case HWTSTAMP_FILTER_PTP_V2_L2_EVENT:
+	case HWTSTAMP_FILTER_PTP_V2_L2_SYNC:
+	case HWTSTAMP_FILTER_PTP_V2_L2_DELAY_REQ:
+	case HWTSTAMP_FILTER_PTP_V2_EVENT:
+	case HWTSTAMP_FILTER_PTP_V2_SYNC:
+	case HWTSTAMP_FILTER_PTP_V2_DELAY_REQ:
+		priv->rx_timestamp_hw = 1;
+		config.rx_filter = HWTSTAMP_FILTER_ALL;
+		frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface));
+		frm_ctl.s.ptp_mode = 1;
+		cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface), frm_ctl.u64);
+
+		prt_cfg.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(priv->ipd_pkind));
+		prt_cfg.s.skip = 8;
+		cvmx_write_csr(CVMX_PIP_PRT_CFGX(priv->ipd_pkind), prt_cfg.u64);
+		break;
+	default:
+		return -ERANGE;
+	}
+
+	if (copy_to_user(rq->ifr_data, &config, sizeof(config)))
+		return -EFAULT;
+
+	return 0;
+}
+
+/**
+ * cvm_oct_ioctl - IOCTL support for PHY control
+ * @dev:    Device to change
+ * @rq:     the request
+ * @cmd:    the command
+ *
+ * Returns Zero on success
+ */
+int cvm_oct_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	union cvmx_srio_tx_message_header tx_header;
+	int ivalue;
+
+	switch (cmd) {
+	case CAVIUM_NET_IOCTL_SETPRIO:
+		ivalue = rq->ifr_ifru.ifru_ivalue;
+		if ((ivalue >= 0) && (ivalue < 4)) {
+			tx_header.u64 = priv->srio_tx_header;
+			tx_header.s.prio = ivalue;
+			priv->srio_tx_header = tx_header.u64;
+			return 0;
+		}
+		return -EINVAL;
+
+	case CAVIUM_NET_IOCTL_GETPRIO:
+		tx_header.u64 = priv->srio_tx_header;
+		rq->ifr_ifru.ifru_ivalue = tx_header.s.prio;
+		return 0;
+
+	case CAVIUM_NET_IOCTL_SETIDSIZE:
+		ivalue = rq->ifr_ifru.ifru_ivalue;
+		if ((ivalue >= 0) && (ivalue < 2)) {
+			tx_header.u64 = priv->srio_tx_header;
+			tx_header.s.tt = ivalue;
+			priv->srio_tx_header = tx_header.u64;
+			return 0;
+		}
+		return -EINVAL;
+
+	case CAVIUM_NET_IOCTL_GETIDSIZE:
+		tx_header.u64 = priv->srio_tx_header;
+		rq->ifr_ifru.ifru_ivalue = tx_header.s.tt;
+		return 0;
+
+	case CAVIUM_NET_IOCTL_SETSRCID:
+		ivalue = rq->ifr_ifru.ifru_ivalue;
+		if ((ivalue >= 0) && (ivalue < 2)) {
+			tx_header.u64 = priv->srio_tx_header;
+			tx_header.s.sis = ivalue;
+			priv->srio_tx_header = tx_header.u64;
+			return 0;
+		}
+		return -EINVAL;
+
+	case CAVIUM_NET_IOCTL_GETSRCID:
+		tx_header.u64 = priv->srio_tx_header;
+		rq->ifr_ifru.ifru_ivalue = tx_header.s.sis;
+		return 0;
+
+	case CAVIUM_NET_IOCTL_SETLETTER:
+		ivalue = rq->ifr_ifru.ifru_ivalue;
+		if ((ivalue >= -1) && (ivalue < 4)) {
+			tx_header.u64 = priv->srio_tx_header;
+			tx_header.s.lns = (ivalue == -1);
+			if (tx_header.s.lns)
+				tx_header.s.letter = 0;
+			else
+				tx_header.s.letter = ivalue;
+			priv->srio_tx_header = tx_header.u64;
+			return 0;
+		}
+		return -EINVAL;
+
+	case CAVIUM_NET_IOCTL_GETLETTER:
+		tx_header.u64 = priv->srio_tx_header;
+		if (tx_header.s.lns)
+			rq->ifr_ifru.ifru_ivalue = -1;
+		else
+			rq->ifr_ifru.ifru_ivalue = tx_header.s.letter;
+		return 0;
+
+	case SIOCSHWTSTAMP:
+		return cvm_oct_ioctl_hwtstamp(dev, rq, cmd);
+
+	default:
+		if (priv->phydev)
+			return phy_mii_ioctl(priv->phydev, rq, cmd);
+	}
+	return -EOPNOTSUPP;
+}
+
+static void cvm_oct_note_carrier(struct octeon_ethernet *priv,
+				 cvmx_helper_link_info_t li)
+{
+	if (li.s.link_up) {
+		pr_notice_ratelimited("%s: %u Mbps %s duplex, port %d\n",
+				      netdev_name(priv->netdev), li.s.speed,
+				      (li.s.full_duplex) ? "Full" : "Half",
+				      priv->ipd_port);
+	} else {
+		pr_notice_ratelimited("%s: Link down\n",
+				      netdev_name(priv->netdev));
+	}
+}
+
+/**
+ * cvm_oct_set_carrier - common wrapper of netif_carrier_{on,off}
+ *
+ * @priv: Device struct.
+ * @link_info: Current state.
+ */
+void cvm_oct_set_carrier(struct octeon_ethernet *priv,
+			 cvmx_helper_link_info_t link_info)
+{
+	cvm_oct_note_carrier(priv, link_info);
+	if (link_info.s.link_up) {
+		if (!netif_carrier_ok(priv->netdev))
+			netif_carrier_on(priv->netdev);
+	} else {
+		if (netif_carrier_ok(priv->netdev))
+			netif_carrier_off(priv->netdev);
+	}
+}
+
+void cvm_oct_adjust_link(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	cvmx_helper_link_info_t link_info;
+
+	if (priv->last_link != priv->phydev->link) {
+		priv->last_link = priv->phydev->link;
+		link_info.u64 = 0;
+		link_info.s.link_up = priv->last_link ? 1 : 0;
+		link_info.s.full_duplex = priv->phydev->duplex ? 1 : 0;
+		link_info.s.speed = priv->phydev->speed;
+
+		cvmx_helper_link_set(priv->ipd_port, link_info);
+
+		if (priv->link_change)
+			priv->link_change(priv, link_info);
+
+		cvm_oct_note_carrier(priv, link_info);
+	}
+}
+
+int cvm_oct_common_stop(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	cvmx_helper_link_info_t link_info;
+
+	if (priv->last_link) {
+		link_info.u64 = 0;
+		priv->last_link = 0;
+
+		cvmx_helper_link_set(priv->ipd_port, link_info);
+
+		if (priv->link_change)
+			priv->link_change(priv, link_info);
+
+		cvm_oct_note_carrier(priv, link_info);
+	}
+	return 0;
+}
+
+/**
+ * cvm_oct_phy_setup_device - setup the PHY
+ *
+ * @dev:    Device to setup
+ *
+ * Returns Zero on success, negative on failure
+ */
+int cvm_oct_phy_setup_device(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	struct device_node *phy_node;
+
+	if (!priv->of_node)
+		goto no_phy;
+
+	phy_node = of_parse_phandle(priv->of_node, "phy-handle", 0);
+	if (!phy_node)
+		goto no_phy;
+
+	priv->phydev = of_phy_connect(dev, phy_node, cvm_oct_adjust_link, 0,
+				      PHY_INTERFACE_MODE_GMII);
+
+	if (priv->phydev == NULL)
+		return -ENODEV;
+
+	priv->last_link = 0;
+	phy_start_aneg(priv->phydev);
+
+	return 0;
+no_phy:
+	/* If there is no phy, assume a direct MAC connection and that
+	 * the link is up.
+	 */
+	netif_carrier_on(dev);
+	return 0;
+}
diff --git a/drivers/net/ethernet/octeon/ethernet-mem.c b/drivers/net/ethernet/octeon/ethernet-mem.c
new file mode 100644
index 0000000..67d907d
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-mem.c
@@ -0,0 +1,325 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+#include <linux/netdevice.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/slab.h>
+
+#include <asm/octeon/octeon.h>
+#include <asm/octeon/cvmx-fpa.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+
+struct fpa_pool {
+	int pool;
+	int users;
+	int size;
+	char kmem_name[20];
+	struct kmem_cache *kmem;
+	int (*fill)(struct fpa_pool *p, int num);
+	int (*empty)(struct fpa_pool *p, int num);
+};
+
+static DEFINE_SPINLOCK(cvm_oct_pools_lock);
+/* Eight pools. */
+static struct fpa_pool cvm_oct_pools[] = {
+	{-1}, {-1}, {-1}, {-1}, {-1}, {-1}, {-1}, {-1}
+};
+
+/**
+ * cvm_oct_fill_hw_skbuff - fill the supplied hardware pool with skbuffs
+ * @pool:     Pool to allocate an skbuff for
+ * @size:     Size of the buffer needed for the pool
+ * @elements: Number of buffers to allocate
+ *
+ * Returns the actual number of buffers allocated.
+ */
+static int cvm_oct_fill_hw_skbuff(struct fpa_pool *pool, int elements)
+{
+	int freed = elements;
+	int size = pool->size;
+	int pool_num = pool->pool;
+	while (freed) {
+		int extra_reserve;
+		u8 *desired_data;
+		struct sk_buff *skb = alloc_skb(size + CVM_OCT_SKB_TO_FPA_PADDING,
+						GFP_ATOMIC);
+		if (skb == NULL) {
+			pr_err("Failed to allocate skb for hardware pool %d\n",
+			       pool_num);
+			break;
+		}
+		desired_data = cvm_oct_get_fpa_head(skb);
+		extra_reserve = desired_data - skb->data;
+		skb_reserve(skb, extra_reserve);
+		*(struct sk_buff **)(skb->data - sizeof(void *)) = skb;
+		cvmx_fpa_free(skb->data, pool_num, DONT_WRITEBACK(size / 128));
+		freed--;
+	}
+	return elements - freed;
+}
+
+/**
+ * cvm_oct_free_hw_skbuff- free hardware pool skbuffs
+ * @pool:     Pool to allocate an skbuff for
+ * @size:     Size of the buffer needed for the pool
+ * @elements: Number of buffers to allocate
+ */
+static int cvm_oct_free_hw_skbuff(struct fpa_pool *pool, int elements)
+{
+	struct sk_buff *skb;
+	char *memory;
+	int pool_num = pool->pool;
+
+	while (elements) {
+		memory = cvmx_fpa_alloc(pool_num);
+		if (!memory)
+			break;
+		skb = *cvm_oct_packet_to_skb(memory);
+		elements--;
+		dev_kfree_skb(skb);
+	}
+
+	if (elements > 0)
+		pr_err("Freeing of pool %u is missing %d skbuffs\n",
+		       pool_num, elements);
+
+	return elements;
+}
+
+/**
+ * cvm_oct_fill_hw_memory - fill a hardware pool with memory.
+ * @pool:     Pool to populate
+ * @size:     Size of each buffer in the pool
+ * @elements: Number of buffers to allocate
+ *
+ * Returns the actual number of buffers allocated.
+ */
+static int cvm_oct_fill_hw_kmem(struct fpa_pool *pool, int elements)
+{
+	char *memory;
+	int freed = elements;
+
+	while (freed) {
+		memory = kmem_cache_alloc(pool->kmem, GFP_KERNEL);
+		if (unlikely(memory == NULL)) {
+			pr_err("Unable to allocate %u bytes for FPA pool %d\n",
+			       elements * pool->size, pool->pool);
+			break;
+		}
+		cvmx_fpa_free(memory, pool->pool, 0);
+		freed--;
+	}
+	return elements - freed;
+}
+
+/**
+ * cvm_oct_free_hw_memory - Free memory allocated by cvm_oct_fill_hw_memory
+ * @pool:     FPA pool to free
+ * @size:     Size of each buffer in the pool
+ * @elements: Number of buffers that should be in the pool
+ */
+static int cvm_oct_free_hw_kmem(struct fpa_pool *pool, int elements)
+{
+	char *fpa;
+	while (elements) {
+		fpa = cvmx_fpa_alloc(pool->pool);
+		if (!fpa)
+			break;
+		elements--;
+		kmem_cache_free(pool->kmem, fpa);
+	}
+
+	if (elements > 0)
+		pr_err("Warning: Freeing of pool %u is missing %d buffers\n",
+		       pool->pool, elements);
+	return elements;
+}
+
+int cvm_oct_mem_fill_fpa(int pool, int elements)
+{
+	struct fpa_pool *p;
+
+	if (pool < 0 || pool >= ARRAY_SIZE(cvm_oct_pools))
+		return -EINVAL;
+
+	p = cvm_oct_pools + pool;
+
+	return p->fill(p, elements);
+}
+EXPORT_SYMBOL(cvm_oct_mem_fill_fpa);
+
+int cvm_oct_mem_empty_fpa(int pool, int elements)
+{
+	struct fpa_pool *p;
+
+	if (pool < 0 || pool >= ARRAY_SIZE(cvm_oct_pools))
+		return -EINVAL;
+
+	p = cvm_oct_pools + pool;
+	if (p->empty)
+		return p->empty(p, elements);
+
+	return 0;
+}
+EXPORT_SYMBOL(cvm_oct_mem_empty_fpa);
+
+void cvm_oct_mem_cleanup(void)
+{
+	int i;
+
+	spin_lock(&cvm_oct_pools_lock);
+
+	for (i = 0; i < ARRAY_SIZE(cvm_oct_pools); i++)
+		if (cvm_oct_pools[i].kmem)
+			kmem_cache_shrink(cvm_oct_pools[i].kmem);
+	spin_unlock(&cvm_oct_pools_lock);
+}
+EXPORT_SYMBOL(cvm_oct_mem_cleanup);
+
+/**
+ * cvm_oct_alloc_fpa_pool() - Allocate an FPA pool of the given size
+ * @pool:  Requested pool number (-1 for don't care).
+ * @size:  The size of the pool elements.
+ *
+ * Returns the pool number or a negative number on error.
+ */
+int cvm_oct_alloc_fpa_pool(int pool, int size)
+{
+	int i;
+	int ret = 0;
+
+	if (pool >= (int)ARRAY_SIZE(cvm_oct_pools) || size < 128)
+		return -EINVAL;
+
+	spin_lock(&cvm_oct_pools_lock);
+
+	if (pool >= 0) {
+		if (cvm_oct_pools[pool].pool != -1) {
+			if (cvm_oct_pools[pool].size == size) {
+				/* Already allocated */
+				cvm_oct_pools[pool].users++;
+				ret = pool;
+				goto out;
+			} else {
+				/* conflict */
+				ret = -EINVAL;
+				goto out;
+			}
+		}
+		/* reserve/alloc fpa pool */
+		pool = cvmx_fpa_alloc_pool(pool);
+		if (pool < 0) {
+			ret = -EINVAL;
+			goto out;
+		}
+	} else {
+		/* Find an established pool */
+		for (i = 0; i < ARRAY_SIZE(cvm_oct_pools); i++)
+			if (cvm_oct_pools[i].pool >= 0 &&
+			    cvm_oct_pools[i].size == size) {
+				cvm_oct_pools[i].users++;
+				ret = i;
+				goto out;
+			}
+
+		/* Alloc fpa pool */
+		pool = cvmx_fpa_alloc_pool(pool);
+		if (pool < 0) {
+			/* No empties. */
+			ret = -EINVAL;
+			goto out;
+		}
+	}
+
+	/* Setup the pool */
+	cvm_oct_pools[pool].pool = pool;
+	cvm_oct_pools[pool].users++;
+	cvm_oct_pools[pool].size = size;
+	if (USE_SKBUFFS_IN_HW && pool == 0) {
+		/* Special packet pool */
+		cvm_oct_pools[pool].fill = cvm_oct_fill_hw_skbuff;
+		cvm_oct_pools[pool].empty = cvm_oct_free_hw_skbuff;
+	} else {
+		snprintf(cvm_oct_pools[pool].kmem_name,
+			 sizeof(cvm_oct_pools[pool].kmem_name),
+			 "oct-fpa-%d", size);
+		cvm_oct_pools[pool].fill = cvm_oct_fill_hw_kmem;
+		cvm_oct_pools[pool].empty = cvm_oct_free_hw_kmem;
+		cvm_oct_pools[pool].kmem =
+			kmem_cache_create(cvm_oct_pools[pool].kmem_name,
+					  size, 128, 0, NULL);
+		if (!cvm_oct_pools[pool].kmem) {
+			ret = -ENOMEM;
+			cvm_oct_pools[pool].pool = -1;
+			cvmx_fpa_release_pool(pool);
+			goto out;
+		}
+	}
+	ret = pool;
+out:
+	spin_unlock(&cvm_oct_pools_lock);
+	return ret;
+}
+EXPORT_SYMBOL(cvm_oct_alloc_fpa_pool);
+
+/**
+ * cvm_oct_release_fpa_pool() - Releases an FPA pool
+ * @pool:  Pool number.
+ *
+ * This undoes the action of cvm_oct_alloc_fpa_pool().
+ *
+ * Returns zero on success.
+ */
+int cvm_oct_release_fpa_pool(int pool)
+{
+	int ret = -EINVAL;
+
+	if (pool < 0 || pool >= (int)ARRAY_SIZE(cvm_oct_pools))
+		return ret;
+
+	spin_lock(&cvm_oct_pools_lock);
+
+	if (cvm_oct_pools[pool].users <= 0) {
+		pr_err("Error: Unbalanced FPA pool allocation\n");
+		goto out;
+	}
+	cvm_oct_pools[pool].users--;
+
+	if (cvm_oct_pools[pool].users == 0)
+		cvmx_fpa_release_pool(pool);
+
+	ret = 0;
+out:
+	spin_unlock(&cvm_oct_pools_lock);
+	return ret;
+}
+EXPORT_SYMBOL(cvm_oct_release_fpa_pool);
+
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Cavium, Inc. Ethernet/FPA memory allocator.");
diff --git a/drivers/net/ethernet/octeon/ethernet-napi.c b/drivers/net/ethernet/octeon/ethernet-napi.c
new file mode 100644
index 0000000..89d8f59
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-napi.c
@@ -0,0 +1,473 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+
+/* This file is included in ethernet-rx.c *twice* */
+
+#undef CVM_OCT_NAPI_POLL
+#undef CVM_OCT_NAPI_HAS_CN68XX_SSO
+
+#ifdef CVM_OCT_NAPI_68
+#define CVM_OCT_NAPI_POLL cvm_oct_napi_poll_68
+#define CVM_OCT_NAPI_HAS_CN68XX_SSO 1
+#else
+#define CVM_OCT_NAPI_POLL cvm_oct_napi_poll_38
+#define CVM_OCT_NAPI_HAS_CN68XX_SSO 0
+#endif
+
+/**
+ * cvm_oct_napi_poll - the NAPI poll function.
+ * @napi: The NAPI instance, or null if called from cvm_oct_poll_controller
+ * @budget: Maximum number of packets to receive.
+ *
+ * Returns the number of packets processed.
+ */
+static int CVM_OCT_NAPI_POLL(struct napi_struct *napi, int budget)
+{
+	const int	coreid = cvmx_get_core_num();
+	int		no_work_count = 0;
+	u64		old_group_mask;
+	u64		old_scratch;
+	int		rx_count = 0;
+	bool		did_work_request = false;
+	bool		packet_copied;
+
+	char		*p = (char *)cvm_oct_by_pkind;
+
+	/* Prefetch cvm_oct_device since we know we need it soon */
+	prefetch(&p[0]);
+	prefetch(&p[SMP_CACHE_BYTES]);
+	prefetch(&p[2 * SMP_CACHE_BYTES]);
+
+	if (USE_ASYNC_IOBDMA) {
+		/* Save scratch in case userspace is using it */
+		CVMX_SYNCIOBDMA;
+		old_scratch = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
+	}
+
+	/* Only allow work for our group (and preserve priorities) */
+	if (CVM_OCT_NAPI_HAS_CN68XX_SSO) {
+		old_group_mask = cvmx_read_csr(CVMX_SSO_PPX_GRP_MSK(coreid));
+		cvmx_write_csr(CVMX_SSO_PPX_GRP_MSK(coreid),
+			       1ull << pow_receive_group);
+		/* Read it back so it takes effect before we request work */
+		cvmx_read_csr(CVMX_SSO_PPX_GRP_MSK(coreid));
+	} else {
+		old_group_mask = cvmx_read_csr(CVMX_POW_PP_GRP_MSKX(coreid));
+		cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(coreid),
+			       (old_group_mask & ~0xFFFFull) | 1 << pow_receive_group);
+	}
+
+	if (USE_ASYNC_IOBDMA) {
+		cvmx_pow_work_request_async(CVMX_SCR_SCRATCH, CVMX_POW_NO_WAIT);
+		did_work_request = true;
+	}
+
+	while (rx_count < budget) {
+		struct sk_buff *skb = NULL;
+		struct sk_buff **pskb = NULL;
+		struct octeon_ethernet *priv;
+		enum cvm_oct_callback_result callback_result;
+		bool skb_in_hw;
+		cvmx_wqe_t *work;
+		int port;
+		unsigned int segments;
+		int packets_to_replace = 0;
+		unsigned int packet_len;
+
+		union cvmx_buf_ptr  packet_ptr;
+
+		if (USE_ASYNC_IOBDMA && did_work_request)
+			work = cvmx_pow_work_response_async(CVMX_SCR_SCRATCH);
+		else
+			work = cvmx_pow_work_request_sync(CVMX_POW_NO_WAIT);
+
+		prefetch(work);
+		did_work_request = false;
+		if (unlikely(work == NULL)) {
+			/* It takes so long to get here, so lets wait
+			 * around a little to see if another packet
+			 * comes in.
+			 */
+			if (no_work_count >= 2)
+				break;
+			no_work_count++;
+			ndelay(500);
+			continue;
+		}
+		packet_ptr = work->packet_ptr;
+		pskb = cvm_oct_packet_to_skb(cvm_oct_get_buffer_ptr(packet_ptr));
+		prefetch(pskb);
+
+		if (USE_ASYNC_IOBDMA && rx_count < (budget - 1)) {
+			cvmx_pow_work_request_async_nocheck(CVMX_SCR_SCRATCH, CVMX_POW_NO_WAIT);
+			did_work_request = true;
+		}
+
+		if (rx_count == 0) {
+			/* First time through, see if there is enough
+			 * work waiting to merit waking another
+			 * CPU.
+			 */
+			int backlog;
+			int cores_in_use = core_state.active_cores;
+			if (CVM_OCT_NAPI_HAS_CN68XX_SSO) {
+				union cvmx_sso_wq_int_cntx counts;
+				counts.u64 = cvmx_read_csr(CVMX_SSO_WQ_INT_CNTX(pow_receive_group));
+				backlog = counts.s.iq_cnt + counts.s.ds_cnt;
+			} else {
+				union cvmx_pow_wq_int_cntx counts;
+				counts.u64 = cvmx_read_csr(CVMX_POW_WQ_INT_CNTX(pow_receive_group));
+				backlog = counts.s.iq_cnt + counts.s.ds_cnt;
+			}
+			if (backlog > rx_cpu_factor * cores_in_use &&
+			    napi != NULL &&
+			    cores_in_use < core_state.baseline_cores)
+				cvm_oct_enable_one_cpu();
+		}
+
+		/* If WORD2[SOFTWARE] then this WQE is a complete for
+		 * a TX packet.
+		 */
+		if (work->word2.s.software) {
+			struct octeon_ethernet *priv;
+			int packet_qos = work->word0.raw.unused;
+
+			skb = (struct sk_buff *)packet_ptr.u64;
+			priv = netdev_priv(skb->dev);
+			if (!netif_running(skb->dev))
+				netif_wake_queue(skb->dev);
+			if (unlikely((skb_shinfo(skb)->tx_flags | SKBTX_IN_PROGRESS) != 0 &&
+				     priv->tx_timestamp_hw)) {
+					u64 ns = *(u64 *)work->packet_data;
+					struct skb_shared_hwtstamps ts;
+					ts.syststamp = cvm_oct_ptp_to_ktime(ns);
+					ts.hwtstamp = ns_to_ktime(ns);
+					skb_tstamp_tx(skb, &ts);
+			}
+			dev_kfree_skb_any(skb);
+
+			cvmx_fpa_free(work, wqe_pool, DONT_WRITEBACK(1));
+
+			/* We are done with this one, adjust the queue
+			 * depth.
+			 */
+			cvmx_fau_atomic_add32(priv->tx_queue[packet_qos].fau, -1);
+			continue;
+		}
+		segments = work->word2.s.bufs;
+		skb_in_hw = USE_SKBUFFS_IN_HW && segments > 0;
+		if (likely(skb_in_hw)) {
+			skb = *pskb;
+			prefetch(&skb->head);
+			prefetch(&skb->len);
+		}
+
+		if (CVM_OCT_NAPI_HAS_CN68XX_SSO)
+			port = work->word0.pip.cn68xx.pknd;
+		else
+			port = work->word1.cn38xx.ipprt;
+
+		prefetch(cvm_oct_by_pkind[port]);
+
+		/* Immediately throw away all packets with receive errors */
+		if (unlikely(work->word2.snoip.rcv_error)) {
+			if (cvm_oct_check_rcv_error(work))
+				continue;
+		}
+
+		if (CVM_OCT_NAPI_HAS_CN68XX_SSO) {
+			if (unlikely(cvm_oct_by_pkind[port] == NULL))
+				priv = cvm_oct_dev_for_port(work->word2.s_cn68xx.port);
+			else
+				priv = cvm_oct_by_pkind[port];
+		} else {
+			/* srio priv is based on mbox, not port */
+			if (port >= 40 && port <= 47)
+				priv = NULL;
+			else
+				priv = cvm_oct_by_pkind[port];
+		}
+
+		if (likely(priv) && priv->rx_strip_fcs)
+			work->word1.len -= 4;
+
+		packet_len = work->word1.len;
+		/* We can only use the zero copy path if skbuffs are
+		 * in the FPA pool and the packet fits in a single
+		 * buffer.
+		 */
+		if (likely(skb_in_hw)) {
+			skb->data = phys_to_virt(packet_ptr.s.addr);
+			prefetch(skb->data);
+			skb->len = packet_len;
+			packets_to_replace = segments;
+			if (likely(segments == 1)) {
+				skb_set_tail_pointer(skb, skb->len);
+			} else {
+				struct sk_buff *current_skb = skb;
+				struct sk_buff *next_skb = NULL;
+				unsigned int segment_size;
+				bool first_frag = true;
+
+				skb_frag_list_init(skb);
+				/* Multi-segment packet. */
+				for (;;) {
+					/* Octeon Errata PKI-100: The segment size is
+					 * wrong. Until it is fixed, calculate the
+					 * segment size based on the packet pool
+					 * buffer size. When it is fixed, the
+					 * following line should be replaced with this
+					 * one: int segment_size =
+					 * segment_ptr.s.size;
+					 */
+					segment_size = FPA_PACKET_POOL_SIZE -
+						(packet_ptr.s.addr - (((packet_ptr.s.addr >> 7) - packet_ptr.s.back) << 7));
+					if (segment_size > packet_len)
+						segment_size = packet_len;
+					if (!first_frag) {
+						current_skb->len = segment_size;
+						skb->data_len += segment_size;
+						skb->truesize += current_skb->truesize;
+					}
+					skb_set_tail_pointer(current_skb, segment_size);
+					packet_len -= segment_size;
+					segments--;
+					if (segments == 0)
+						break;
+					packet_ptr = *(union cvmx_buf_ptr *)phys_to_virt(packet_ptr.s.addr - 8);
+					next_skb = *cvm_oct_packet_to_skb(cvm_oct_get_buffer_ptr(packet_ptr));
+					if (first_frag) {
+						skb_frag_add_head(current_skb, next_skb);
+					} else {
+						current_skb->next = next_skb;
+						next_skb->next = NULL;
+					}
+					current_skb = next_skb;
+					first_frag = false;
+					current_skb->data = phys_to_virt(packet_ptr.s.addr);
+				}
+			}
+			packet_copied = false;
+		} else {
+			/* We have to copy the packet. First allocate
+			 * an skbuff for it.
+			 */
+			skb = dev_alloc_skb(packet_len);
+			if (!skb) {
+				printk_ratelimited("Port %d failed to allocate skbuff, packet dropped\n",
+						   port);
+				cvm_oct_free_work(work);
+				continue;
+			}
+
+			/* Check if we've received a packet that was
+			 * entirely stored in the work entry.
+			 */
+			if (unlikely(work->word2.s.bufs == 0)) {
+				u8 *ptr = work->packet_data;
+
+				if (likely(!work->word2.s.not_IP)) {
+					/* The beginning of the packet
+					 * moves for IP packets.
+					 */
+					if (work->word2.s.is_v6)
+						ptr += 2;
+					else
+						ptr += 6;
+				}
+				memcpy(skb_put(skb, packet_len), ptr, packet_len);
+				/* No packet buffers to free */
+			} else {
+				int segments = work->word2.s.bufs;
+				union cvmx_buf_ptr segment_ptr = work->packet_ptr;
+
+				while (segments--) {
+					union cvmx_buf_ptr next_ptr =
+					    *(union cvmx_buf_ptr *)phys_to_virt(segment_ptr.s.addr - 8);
+
+			/* Octeon Errata PKI-100: The segment size is
+			 * wrong. Until it is fixed, calculate the
+			 * segment size based on the packet pool
+			 * buffer size. When it is fixed, the
+			 * following line should be replaced with this
+			 * one: int segment_size =
+			 * segment_ptr.s.size;
+			 */
+					int segment_size = FPA_PACKET_POOL_SIZE -
+						(segment_ptr.s.addr - (((segment_ptr.s.addr >> 7) - segment_ptr.s.back) << 7));
+					/* Don't copy more than what
+					 * is left in the packet.
+					 */
+					if (segment_size > packet_len)
+						segment_size = packet_len;
+					/* Copy the data into the packet */
+					memcpy(skb_put(skb, segment_size),
+					       phys_to_virt(segment_ptr.s.addr),
+					       segment_size);
+					packet_len -= segment_size;
+					segment_ptr = next_ptr;
+				}
+			}
+			packet_copied = true;
+		}
+		/* srio priv is based on mbox, not port */
+		if (!CVM_OCT_NAPI_HAS_CN68XX_SSO && unlikely(priv == NULL)) {
+			const struct cvmx_srio_rx_message_header *rx_header =
+				(const struct cvmx_srio_rx_message_header *)skb->data;
+			priv = cvm_oct_by_srio_mbox[(port - 40) >> 1][rx_header->word0.s.mbox];
+		}
+
+		if (likely(priv)) {
+#ifdef CONFIG_RAPIDIO
+			if (unlikely(priv->imode == CVMX_HELPER_INTERFACE_MODE_SRIO)) {
+				__skb_pull(skb, sizeof(struct cvmx_srio_rx_message_header));
+
+				atomic64_add(1, (atomic64_t *)&priv->netdev->stats.rx_packets);
+				atomic64_add(skb->len, (atomic64_t *)&priv->netdev->stats.rx_bytes);
+			}
+#endif
+			/* Only accept packets for devices that are
+			 * currently up.
+			 */
+			if (likely(priv->netdev->flags & IFF_UP)) {
+				if (priv->rx_timestamp_hw) {
+					/* The first 8 bytes are the timestamp */
+					u64 ns = *(u64 *)skb->data;
+					struct skb_shared_hwtstamps *ts;
+					ts = skb_hwtstamps(skb);
+					ts->hwtstamp = ns_to_ktime(ns);
+					ts->syststamp = cvm_oct_ptp_to_ktime(ns);
+					__skb_pull(skb, 8);
+				}
+				skb->protocol = eth_type_trans(skb, priv->netdev);
+				skb->dev = priv->netdev;
+
+				if (unlikely(work->word2.s.not_IP || work->word2.s.IP_exc ||
+					work->word2.s.L4_error || !work->word2.s.tcp_or_udp))
+					skb->ip_summed = CHECKSUM_NONE;
+				else
+					skb->ip_summed = CHECKSUM_UNNECESSARY;
+
+				/* Increment RX stats for virtual ports */
+				if (port >= CVMX_PIP_NUM_INPUT_PORTS) {
+					atomic64_add(1, (atomic64_t *)&priv->netdev->stats.rx_packets);
+					atomic64_add(skb->len, (atomic64_t *)&priv->netdev->stats.rx_bytes);
+				}
+				if (priv->intercept_cb) {
+					callback_result = priv->intercept_cb(priv->netdev, work, skb);
+					switch (callback_result) {
+					case CVM_OCT_PASS:
+						netif_receive_skb(skb);
+						rx_count++;
+						break;
+					case CVM_OCT_DROP:
+						dev_kfree_skb_any(skb);
+						atomic64_add(1, (atomic64_t *)&priv->netdev->stats.rx_dropped);
+						break;
+					case CVM_OCT_TAKE_OWNERSHIP_WORK:
+						/*
+						 * Interceptor took
+						 * our work, but we
+						 * need to free the
+						 * skbuff
+						 */
+						if (USE_SKBUFFS_IN_HW && likely(!packet_copied)) {
+							/*
+							 * We can't free the skbuff since its data is
+							 * the same as the work. In this case we don't
+							 * do anything
+							 */
+						} else {
+							dev_kfree_skb_any(skb);
+						}
+						break;
+					case CVM_OCT_TAKE_OWNERSHIP_SKB:
+						/* Interceptor took our packet */
+						break;
+					}
+				} else {
+					netif_receive_skb(skb);
+					callback_result = CVM_OCT_PASS;
+					rx_count++;
+				}
+			} else {
+				/* Drop any packet received for a device that isn't up */
+				atomic64_add(1, (atomic64_t *)&priv->netdev->stats.rx_dropped);
+				dev_kfree_skb_any(skb);
+				callback_result = CVM_OCT_DROP;
+
+			}
+		} else {
+			/* Drop any packet received for a device that
+			 * doesn't exist.
+			 */
+			printk_ratelimited("Port %d not controlled by Linux, packet dropped\n",
+					   port);
+			dev_kfree_skb_any(skb);
+			callback_result = CVM_OCT_DROP;
+		}
+		/* We only need to free the work if the interceptor didn't
+		   take over ownership of it */
+		if (callback_result != CVM_OCT_TAKE_OWNERSHIP_WORK) {
+			/* Check to see if the skbuff and work share the same
+			 * packet buffer.
+			 */
+			if (USE_SKBUFFS_IN_HW && likely(!packet_copied)) {
+				/* This buffer needs to be replaced, increment
+				 * the number of buffers we need to free by
+				 * one.
+				 */
+				cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE,
+						      packets_to_replace);
+
+				cvmx_fpa_free(work, wqe_pool, DONT_WRITEBACK(1));
+			} else {
+				cvm_oct_free_work(work);
+			}
+		}
+	}
+	/* Restore the original POW group mask */
+	if (CVM_OCT_NAPI_HAS_CN68XX_SSO) {
+		cvmx_write_csr(CVMX_SSO_PPX_GRP_MSK(coreid), old_group_mask);
+		/* Read it back so it takes effect before ?? */
+		cvmx_read_csr(CVMX_SSO_PPX_GRP_MSK(coreid));
+	} else {
+		cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(coreid), old_group_mask);
+	}
+	if (USE_ASYNC_IOBDMA) {
+		/* Restore the scratch area */
+		cvmx_scratch_write64(CVMX_SCR_SCRATCH, old_scratch);
+	}
+	cvm_oct_rx_refill_pool(0);
+
+	if (rx_count < budget && napi != NULL) {
+		/* No more work */
+		napi_complete(napi);
+		cvm_oct_no_more_work(napi);
+	}
+	return rx_count;
+}
diff --git a/drivers/net/ethernet/octeon/ethernet-rgmii.c b/drivers/net/ethernet/octeon/ethernet-rgmii.c
new file mode 100644
index 0000000..92a7407
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-rgmii.c
@@ -0,0 +1,300 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/interrupt.h>
+#include <linux/phy.h>
+#include <linux/ratelimit.h>
+#include <net/dst.h>
+
+#include <asm/octeon/octeon-hw-status.h>
+#include <asm/octeon/octeon.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+
+#include <asm/octeon/cvmx-helper.h>
+
+#include <asm/octeon/cvmx-ipd-defs.h>
+#include <asm/octeon/cvmx-npi-defs.h>
+#include <asm/octeon/cvmx-gmxx-defs.h>
+
+#define INT_BIT_PHY_LINK 16
+#define INT_BIT_PHY_SPD 17
+#define INT_BIT_PHY_DUPX 18
+
+DEFINE_SPINLOCK(global_register_lock);
+
+static void cvm_oct_rgmii_poll(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	cvmx_helper_link_info_t link_info;
+
+	if (priv->phydev) {
+		link_info.u64 = 0;
+		link_info.s.link_up = priv->last_link ? 1 : 0;
+		link_info.s.full_duplex = priv->phydev->duplex ? 1 : 0;
+		link_info.s.speed = priv->phydev->speed;
+	} else {
+		link_info = cvmx_helper_link_get(priv->ipd_port);
+	}
+	if (link_info.u64 == priv->link_info) {
+		/* If the 10Mbps preamble workaround is supported and we're
+		 * at 10Mbps we may need to do some special checking.
+		 */
+		if (USE_10MBPS_PREAMBLE_WORKAROUND && (link_info.s.speed == 10)) {
+			/* Read the GMXX_RXX_INT_REG[PCTERR] bit and
+			 * see if we are getting preamble errors.
+			 */
+			union cvmx_gmxx_rxx_int_reg gmxx_rxx_int_reg;
+			gmxx_rxx_int_reg.u64 =
+				cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(priv->interface_port, priv->interface));
+			if (gmxx_rxx_int_reg.s.pcterr) {
+				/* We are getting preamble errors at
+				 * 10Mbps.  Most likely the PHY is
+				 * giving us packets with mis aligned
+				 * preambles. In order to get these
+				 * packets we need to disable preamble
+				 * checking and do it in software.
+				 */
+				union cvmx_gmxx_rxx_frm_ctl gmxx_rxx_frm_ctl;
+
+				/* Disable preamble checking */
+				gmxx_rxx_frm_ctl.u64 =
+					cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface));
+				gmxx_rxx_frm_ctl.s.pre_chk = 0;
+				cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface),
+					       gmxx_rxx_frm_ctl.u64);
+
+				/* Clear any error bits */
+				cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(priv->interface_port, priv->interface),
+					       gmxx_rxx_int_reg.u64);
+				printk_ratelimited("%s: Using 10Mbps with software preamble removal\n",
+						   dev->name);
+			}
+		}
+		return;
+	}
+
+	/* If the 10Mbps preamble workaround is allowed we need to on
+	 * preamble checking, FCS stripping, and clear error bits on
+	 * every speed change. If errors occur during 10Mbps operation
+	 * the above code will change this stuff.
+	 */
+	if (USE_10MBPS_PREAMBLE_WORKAROUND) {
+		union cvmx_gmxx_rxx_frm_ctl gmxx_rxx_frm_ctl;
+		union cvmx_gmxx_rxx_int_reg gmxx_rxx_int_reg;
+
+		/* Enable preamble checking */
+		gmxx_rxx_frm_ctl.u64 =
+		    cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface));
+		gmxx_rxx_frm_ctl.s.pre_chk = 1;
+		cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface),
+			       gmxx_rxx_frm_ctl.u64);
+		/* Clear any error bits */
+		gmxx_rxx_int_reg.u64 =
+			cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(priv->interface_port, priv->interface));
+		cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(priv->interface_port, priv->interface),
+			       gmxx_rxx_int_reg.u64);
+	}
+	if (priv->phydev == NULL) {
+		link_info = cvmx_helper_link_autoconf(priv->ipd_port);
+		priv->link_info = link_info.u64;
+	}
+
+	if (priv->phydev == NULL)
+		cvm_oct_set_carrier(priv, link_info);
+}
+
+static int cvm_oct_rgmii_hw_status(struct notifier_block *nb, unsigned long val, void *v)
+{
+	struct octeon_ethernet *priv = container_of(nb, struct octeon_ethernet, hw_status_notifier);
+
+	if (val == OCTEON_HW_STATUS_SOURCE_ASSERTED) {
+		struct octeon_hw_status_data *d = v;
+		if (d->reg == CVMX_GMXX_RXX_INT_REG(priv->interface_port, priv->interface) &&
+		    (d->bit == INT_BIT_PHY_LINK ||
+		     d->bit == INT_BIT_PHY_SPD ||
+		     d->bit == INT_BIT_PHY_DUPX)) {
+			if (!atomic_read(&cvm_oct_poll_queue_stopping))
+				queue_work(cvm_oct_poll_queue, &priv->port_work);
+			return NOTIFY_STOP;
+		}
+	}
+	return NOTIFY_DONE;
+}
+
+static void cvm_oct_rgmii_immediate_poll(struct work_struct *work)
+{
+	struct octeon_ethernet *priv = container_of(work, struct octeon_ethernet, port_work);
+	cvm_oct_rgmii_poll(priv->netdev);
+}
+
+int cvm_oct_rgmii_open(struct net_device *dev)
+{
+	union cvmx_ipd_sub_port_fcs ipd_sub_port_fcs;
+	struct octeon_hw_status_reg sr[3];
+	u64 en_mask;
+	union cvmx_gmxx_prtx_cfg gmx_cfg;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	cvmx_helper_link_info_t link_info;
+	int rv;
+
+	rv = cvm_oct_phy_setup_device(dev);
+	if (rv)
+		return rv;
+
+
+	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
+	gmx_cfg.s.en = 1;
+	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface), gmx_cfg.u64);
+
+	if (!octeon_is_simulation()) {
+		if (priv->phydev) {
+			int r = phy_read_status(priv->phydev);
+			if (r == 0 && priv->phydev->link == 0)
+				netif_carrier_off(dev);
+			cvm_oct_adjust_link(dev);
+		} else {
+			link_info = cvmx_helper_link_get(priv->ipd_port);
+			if (!link_info.s.link_up)
+				netif_carrier_off(dev);
+			spin_lock(&priv->poll_lock);
+			priv->poll = cvm_oct_rgmii_poll;
+			spin_unlock(&priv->poll_lock);
+		}
+	}
+
+	INIT_WORK(&priv->port_work, cvm_oct_rgmii_immediate_poll);
+
+
+	/* Only true RGMII ports need to be polled. In GMII mode, port
+	 * 0 is really a RGMII port.
+	 */
+	if ((priv->imode == CVMX_HELPER_INTERFACE_MODE_GMII && priv->ipd_port != 0) ||
+	    octeon_is_simulation())
+		return 0;
+
+	/* Due to GMX errata in CN3XXX series chips, it is necessary
+	 * to take the link down immediately when the PHY changes
+	 * state. In order to do this we call the poll function every
+	 * time the RGMII inband status changes.  This may cause
+	 * problems if the PHY doesn't implement inband status
+	 * properly.
+	 */
+	priv->hw_status_notifier.priority = 10;
+	priv->hw_status_notifier.notifier_call = cvm_oct_rgmii_hw_status;
+	octeon_hw_status_notifier_register(&priv->hw_status_notifier);
+
+	en_mask = 0;
+	memset(sr, 0, sizeof(sr));
+	sr[0].reg = 46; /* RML */
+	sr[0].reg_is_hwint = 1;
+	sr[0].has_child = 1;
+	sr[1].reg = CVMX_NPI_RSL_INT_BLOCKS;
+	sr[1].bit = priv->interface + 1;
+	sr[1].has_child = 1;
+	sr[2].reg = CVMX_GMXX_RXX_INT_REG(priv->interface_port, priv->interface);
+	sr[2].mask_reg = CVMX_GMXX_RXX_INT_EN(priv->interface_port, priv->interface);
+	sr[2].ack_w1c = 1;
+
+	sr[2].bit = INT_BIT_PHY_LINK;
+	en_mask |= 1ull << sr[2].bit;
+	octeon_hw_status_add_source(sr);
+
+	sr[2].bit = INT_BIT_PHY_SPD;
+	en_mask |= 1ull << sr[2].bit;
+	octeon_hw_status_add_source(sr);
+
+	sr[2].bit = INT_BIT_PHY_DUPX;
+	en_mask |= 1ull << sr[2].bit;
+	octeon_hw_status_add_source(sr);
+
+	octeon_hw_status_enable(sr[2].reg, en_mask);
+
+
+	/* Disable FCS stripping for PKI-602*/
+	ipd_sub_port_fcs.u64 = cvmx_read_csr(CVMX_IPD_SUB_PORT_FCS);
+	ipd_sub_port_fcs.s.port_bit &= 0xffffffffull ^ (1ull << priv->ipd_port);
+	cvmx_write_csr(CVMX_IPD_SUB_PORT_FCS, ipd_sub_port_fcs.u64);
+	priv->rx_strip_fcs = 1;
+
+	return 0;
+}
+
+int cvm_oct_rgmii_stop(struct net_device *dev)
+{
+	union cvmx_ipd_sub_port_fcs ipd_sub_port_fcs;
+	union cvmx_gmxx_prtx_cfg gmx_cfg;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
+	gmx_cfg.s.en = 0;
+	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface), gmx_cfg.u64);
+
+
+	if (priv->hw_status_notifier.notifier_call) {
+		struct octeon_hw_status_reg sr;
+		memset(&sr, 0, sizeof(sr));
+
+		sr.reg = CVMX_GMXX_RXX_INT_REG(priv->interface_port, priv->interface);
+		sr.mask_reg = CVMX_GMXX_RXX_INT_EN(priv->interface_port, priv->interface);
+		sr.ack_w1c = 1;
+		sr.bit = INT_BIT_PHY_LINK;
+		octeon_hw_status_remove_source(&sr);
+		sr.bit = INT_BIT_PHY_SPD;
+		octeon_hw_status_remove_source(&sr);
+		sr.bit = INT_BIT_PHY_DUPX;
+		octeon_hw_status_remove_source(&sr);
+		octeon_hw_status_notifier_unregister(&priv->hw_status_notifier);
+		priv->hw_status_notifier.notifier_call = NULL;
+	}
+
+	cancel_work_sync(&priv->port_work);
+
+	/* re-Enable FCS stripping */
+	ipd_sub_port_fcs.u64 = cvmx_read_csr(CVMX_IPD_SUB_PORT_FCS);
+	ipd_sub_port_fcs.s.port_bit |= 1ull << priv->ipd_port;
+	cvmx_write_csr(CVMX_IPD_SUB_PORT_FCS, ipd_sub_port_fcs.u64);
+	priv->rx_strip_fcs = 0;
+
+	return 0;
+}
+
+int cvm_oct_rgmii_init(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	union cvmx_gmxx_prtx_cfg gmx_cfg;
+
+	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
+	gmx_cfg.s.en = 0;
+	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface), gmx_cfg.u64);
+
+	cvm_oct_common_init(dev);
+
+	return 0;
+}
diff --git a/drivers/net/ethernet/octeon/ethernet-rx.c b/drivers/net/ethernet/octeon/ethernet-rx.c
new file mode 100644
index 0000000..aa74fb8
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-rx.c
@@ -0,0 +1,593 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/cache.h>
+#include <linux/cpumask.h>
+#include <linux/netdevice.h>
+#include <linux/init.h>
+#include <linux/etherdevice.h>
+#include <linux/ip.h>
+#include <linux/string.h>
+#include <linux/prefetch.h>
+#include <linux/ratelimit.h>
+#include <linux/smp.h>
+#include <linux/interrupt.h>
+#include <net/dst.h>
+#ifdef CONFIG_XFRM
+#include <linux/xfrm.h>
+#include <net/xfrm.h>
+#endif /* CONFIG_XFRM */
+
+#include <asm/octeon/octeon.h>
+#include <asm/octeon/octeon-hw-status.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+
+#include <asm/octeon/cvmx-helper.h>
+#include <asm/octeon/cvmx-wqe.h>
+#include <asm/octeon/cvmx-fau.h>
+#include <asm/octeon/cvmx-pow.h>
+#include <asm/octeon/cvmx-pip.h>
+#include <asm/octeon/cvmx-ipd.h>
+#include <asm/octeon/cvmx-srio.h>
+#include <asm/octeon/cvmx-scratch.h>
+
+#include <asm/octeon/cvmx-gmxx-defs.h>
+#include <asm/octeon/cvmx-sso-defs.h>
+
+struct cvm_napi_wrapper {
+	struct napi_struct napi;
+	int available;
+} ____cacheline_aligned_in_smp;
+
+static struct cvm_napi_wrapper cvm_oct_napi[NR_CPUS] __cacheline_aligned_in_smp;
+
+struct cvm_oct_core_state {
+	int baseline_cores;
+	/* We want to read this without having to acquire the lock,
+	 * make it volatile so we are likely to get a fairly current
+	 * value.
+	 */
+	volatile int active_cores;
+	/* cvm_napi_wrapper.available and active_cores must be kept
+	 * consistent with this lock.
+	 */
+	spinlock_t lock;
+} ____cacheline_aligned_in_smp;
+
+static struct cvm_oct_core_state core_state __cacheline_aligned_in_smp;
+
+#ifdef CONFIG_SMP
+static int cvm_oct_enable_one_message;
+#endif
+
+static void cvm_oct_enable_napi(void)
+{
+	int cpu = smp_processor_id();
+	napi_schedule(&cvm_oct_napi[cpu].napi);
+}
+
+static void cvm_oct_enable_one_cpu(void)
+{
+	int cpu;
+	unsigned long flags;
+	spin_lock_irqsave(&core_state.lock, flags);
+	/* ... if a CPU is available, Turn on NAPI polling for that CPU.  */
+	for_each_online_cpu(cpu) {
+		if (cvm_oct_napi[cpu].available > 0) {
+			cvm_oct_napi[cpu].available--;
+			core_state.active_cores++;
+			spin_unlock_irqrestore(&core_state.lock, flags);
+			if (cpu == smp_processor_id()) {
+				cvm_oct_enable_napi();
+			} else {
+#ifdef CONFIG_SMP
+				octeon_send_ipi_single(cpu, cvm_oct_enable_one_message);
+#else
+				BUG();
+#endif
+			}
+			goto out;
+		}
+	}
+	spin_unlock_irqrestore(&core_state.lock, flags);
+out:
+	return;
+}
+
+static void cvm_oct_no_more_work(struct napi_struct *napi)
+{
+	struct cvm_napi_wrapper *nr = container_of(napi, struct cvm_napi_wrapper, napi);
+	int current_active;
+	unsigned long flags;
+
+	spin_lock_irqsave(&core_state.lock, flags);
+
+	core_state.active_cores--;
+	current_active = core_state.active_cores;
+	nr->available++;
+	BUG_ON(nr->available != 1);
+
+	spin_unlock_irqrestore(&core_state.lock, flags);
+
+	if (current_active == 0) {
+		/* No more CPUs doing processing, enable interrupts so
+		 * we can start processing again when there is
+		 * something to do.
+		 */
+		if (OCTEON_IS_MODEL(OCTEON_CN68XX)) {
+			union cvmx_sso_wq_int_thrx int_thr;
+			int_thr.u64 = 0;
+			int_thr.s.iq_thr = 1;
+			int_thr.s.ds_thr = 1;
+			/*
+			 * Enable SSO interrupt when our port has at
+			 * least one packet.
+			 */
+			cvmx_write_csr(CVMX_SSO_WQ_INT_THRX(pow_receive_group),
+				       int_thr.u64);
+		} else {
+			union cvmx_pow_wq_int_thrx int_thr;
+			int_thr.u64 = 0;
+			int_thr.s.iq_thr = 1;
+			int_thr.s.ds_thr = 1;
+			/* Enable POW interrupt when our port has at
+			 * least one packet.
+			 */
+			cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group),
+				       int_thr.u64);
+		}
+	}
+}
+
+/**
+ * cvm_oct_do_interrupt - interrupt handler.
+ *
+ * The interrupt occurs whenever the POW has packets in our group.
+ *
+ */
+static irqreturn_t cvm_oct_do_interrupt(int cpl, void *dev_id)
+{
+	int cpu = smp_processor_id();
+	unsigned long flags;
+
+	/* Disable the IRQ and start napi_poll. */
+	if (OCTEON_IS_MODEL(OCTEON_CN68XX)) {
+		cvmx_write_csr(CVMX_SSO_WQ_INT_THRX(pow_receive_group), 0);
+		cvmx_write_csr(CVMX_SSO_WQ_INT, 1ULL << pow_receive_group);
+	} else {
+		union cvmx_pow_wq_int wq_int;
+
+		cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group), 0);
+
+		wq_int.u64 = 0;
+		wq_int.s.wq_int = 1 << pow_receive_group;
+		cvmx_write_csr(CVMX_POW_WQ_INT, wq_int.u64);
+	}
+
+	spin_lock_irqsave(&core_state.lock, flags);
+
+	/* ... and NAPI better not be running on this CPU.  */
+	BUG_ON(cvm_oct_napi[cpu].available != 1);
+	cvm_oct_napi[cpu].available--;
+
+	/* There better be cores available...  */
+	core_state.active_cores++;
+	BUG_ON(core_state.active_cores > core_state.baseline_cores);
+
+	spin_unlock_irqrestore(&core_state.lock, flags);
+
+	cvm_oct_enable_napi();
+
+	return IRQ_HANDLED;
+}
+
+/**
+ * cvm_oct_check_rcv_error - process receive errors
+ * @work: Work queue entry pointing to the packet.
+ *
+ * Returns Non-zero if the packet can be dropped, zero otherwise.
+ */
+static int cvm_oct_check_rcv_error(cvmx_wqe_t *work)
+{
+	bool err  = false;
+	int port = cvmx_wqe_get_port(work);
+
+	if ((work->word2.snoip.err_code == 10) && (work->word1.len <= 64)) {
+		/* Ignore length errors on min size packets. Some
+		 * equipment incorrectly pads packets to 64+4FCS
+		 * instead of 60+4FCS.  Note these packets still get
+		 * counted as frame errors.
+		 */
+	} else if (USE_10MBPS_PREAMBLE_WORKAROUND &&
+		   ((work->word2.snoip.err_code == 5)
+		    || (work->word2.snoip.err_code == 7))) {
+
+		/* We received a packet with either an alignment error
+		 * or a FCS error. This may be signalling that we are
+		 * running 10Mbps with GMXX_RXX_FRM_CTL[PRE_CHK}
+		 * off. If this is the case we need to parse the
+		 * packet to determine if we can remove a non spec
+		 * preamble and generate a correct packet.
+		 */
+		int interface = cvmx_helper_get_interface_num(port);
+		int index = cvmx_helper_get_interface_index_num(port);
+		union cvmx_gmxx_rxx_frm_ctl gmxx_rxx_frm_ctl;
+		gmxx_rxx_frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface));
+		if (gmxx_rxx_frm_ctl.s.pre_chk == 0) {
+
+			u8 *ptr = phys_to_virt(work->packet_ptr.s.addr);
+			int i = 0;
+
+			while (i < work->word1.len - 1) {
+				if (*ptr != 0x55)
+					break;
+				ptr++;
+				i++;
+			}
+
+			if (*ptr == 0xd5) {
+				work->packet_ptr.s.addr += i + 1;
+				work->word1.len -= i + 5;
+			} else if ((*ptr & 0xf) == 0xd) {
+				work->packet_ptr.s.addr += i;
+				work->word1.len -= i + 4;
+				for (i = 0; i < work->word1.len; i++) {
+					*ptr = ((*ptr & 0xf0) >> 4) | ((*(ptr + 1) & 0xf) << 4);
+					ptr++;
+				}
+			} else {
+				printk_ratelimited("Port %d unknown preamble, packet dropped\n",
+						   port);
+				/* cvmx_helper_dump_packet(work); */
+				cvm_oct_free_work(work);
+				return 1;
+			}
+		} else {
+			err  = true;
+		}
+	} else {
+			err  = true;
+	}
+	if (err) {
+		printk_ratelimited("Port %d receive error code %d, packet dropped\n",
+				   port, work->word2.snoip.err_code);
+		cvm_oct_free_work(work);
+		return 1;
+	}
+
+	return 0;
+}
+
+/**
+ * cvm_oct_ptp_to_ktime - Convert a hardware PTP timestamp into a
+ * kernel timestamp.
+ *
+ * @ptptime: 64 bit PTP timestamp, normally in nanoseconds
+ *
+ * Return ktime_t
+ */
+static ktime_t cvm_oct_ptp_to_ktime(u64 ptptime)
+{
+	ktime_t ktimebase;
+	u64 ptpbase;
+	unsigned long flags;
+
+	local_irq_save(flags);
+	/* Fill the icache with the code */
+	ktime_get_real();
+	/* Flush all pending operations */
+	mb();
+	/* Read the time and PTP clock as close together as
+	 * possible. It is important that this sequence take the same
+	 * amount of time to reduce jitter
+	 */
+	ktimebase = ktime_get_real();
+	ptpbase = octeon_read_ptp_csr(CVMX_MIO_PTP_CLOCK_HI);
+	local_irq_restore(flags);
+
+	return ktime_sub_ns(ktimebase, ptpbase - ptptime);
+}
+
+#undef CVM_OCT_NAPI_68
+#include "ethernet-napi.c"
+
+#define CVM_OCT_NAPI_68
+#include "ethernet-napi.c"
+
+static int (*cvm_oct_napi_poll)(struct napi_struct *, int);
+
+#ifdef CONFIG_NET_POLL_CONTROLLER
+
+/**
+ * cvm_oct_poll_controller - poll for receive packets
+ * device.
+ *
+ * @dev:    Device to poll. Unused
+ */
+void cvm_oct_poll_controller(struct net_device *dev)
+{
+	cvm_oct_napi_poll(NULL, 16);
+}
+#endif
+
+static struct kmem_cache *cvm_oct_kmem_sso;
+static int cvm_oct_sso_fptr_count;
+
+static int cvm_oct_sso_initialize(int num_wqe)
+{
+	union cvmx_sso_cfg sso_cfg;
+	union cvmx_fpa_fpfx_marks fpa_marks;
+	int i;
+	int rwq_bufs;
+
+	if (!OCTEON_IS_MODEL(OCTEON_CN68XX))
+		return 0;
+
+	rwq_bufs = 48 + DIV_ROUND_UP(num_wqe, 26);
+	cvm_oct_sso_fptr_count = rwq_bufs;
+	cvm_oct_kmem_sso = kmem_cache_create("octeon_ethernet_sso", 256, 128, 0, NULL);
+	if (cvm_oct_kmem_sso == NULL) {
+		pr_err("cannot create kmem_cache for octeon_ethernet_sso\n");
+		return -ENOMEM;
+	}
+
+	/*
+	 * CN68XX-P1 may reset with the wrong values, put in
+	 * the correct values.
+	 */
+	fpa_marks.u64 = 0;
+	fpa_marks.s.fpf_wr = 0xa4;
+	fpa_marks.s.fpf_rd = 0x40;
+	cvmx_write_csr(CVMX_FPA_FPF8_MARKS, fpa_marks.u64);
+
+	/* Make sure RWI/RWO is disabled. */
+	sso_cfg.u64 = cvmx_read_csr(CVMX_SSO_CFG);
+	sso_cfg.s.rwen = 0;
+	cvmx_write_csr(CVMX_SSO_CFG, sso_cfg.u64);
+
+	while (rwq_bufs) {
+		union cvmx_sso_rwq_psh_fptr fptr;
+		void *mem;
+
+		mem = kmem_cache_alloc(cvm_oct_kmem_sso, GFP_KERNEL);
+		if (mem == NULL) {
+			pr_err("cannot allocate memory from octeon_ethernet_sso\n");
+			return -ENOMEM;
+		}
+		for (;;) {
+			fptr.u64 = cvmx_read_csr(CVMX_SSO_RWQ_PSH_FPTR);
+			if (!fptr.s.full)
+				break;
+			__delay(1000);
+		}
+		fptr.s.fptr = virt_to_phys(mem) >> 7;
+		cvmx_write_csr(CVMX_SSO_RWQ_PSH_FPTR, fptr.u64);
+		rwq_bufs--;
+	}
+	for (i = 0; i < 8; i++) {
+		union cvmx_sso_rwq_head_ptrx head_ptr;
+		union cvmx_sso_rwq_tail_ptrx tail_ptr;
+		void *mem;
+
+		mem = kmem_cache_alloc(cvm_oct_kmem_sso, GFP_KERNEL);
+		if (mem == NULL) {
+			pr_err("cannot allocate memory from octeon_ethernet_sso\n");
+			return -ENOMEM;
+		}
+
+		head_ptr.u64 = 0;
+		tail_ptr.u64 = 0;
+		head_ptr.s.ptr = virt_to_phys(mem) >> 7;
+		tail_ptr.s.ptr = head_ptr.s.ptr;
+		cvmx_write_csr(CVMX_SSO_RWQ_HEAD_PTRX(i), head_ptr.u64);
+		cvmx_write_csr(CVMX_SSO_RWQ_TAIL_PTRX(i), tail_ptr.u64);
+	}
+	/* Now enable the SS0  RWI/RWO */
+	sso_cfg.u64 = cvmx_read_csr(CVMX_SSO_CFG);
+	sso_cfg.s.rwen = 1;
+	sso_cfg.s.rwq_byp_dis = 0;
+	sso_cfg.s.rwio_byp_dis = 0;
+	cvmx_write_csr(CVMX_SSO_CFG, sso_cfg.u64);
+
+	return 0;
+}
+
+void cvm_oct_rx_initialize(int num_wqe)
+{
+	int i;
+	struct net_device *dev_for_napi = NULL;
+
+	if (list_empty(&cvm_oct_list))
+		panic("No net_devices were allocated.");
+
+#ifdef CONFIG_SMP
+	cvm_oct_enable_one_message = octeon_request_ipi_handler(cvm_oct_enable_napi);
+	if (cvm_oct_enable_one_message < 0)
+		panic("cvm_oct_rx_initialize: No IPI handler handles available\n");
+#endif
+
+	if (OCTEON_IS_MODEL(OCTEON_CN68XX))
+		cvm_oct_napi_poll = cvm_oct_napi_poll_68;
+	else
+		cvm_oct_napi_poll = cvm_oct_napi_poll_38;
+
+	dev_for_napi = list_first_entry(&cvm_oct_list,
+					struct octeon_ethernet,
+					list)->netdev;
+
+	if (max_rx_cpus >= 1  && max_rx_cpus < num_online_cpus())
+		core_state.baseline_cores = max_rx_cpus;
+	else
+		core_state.baseline_cores = num_online_cpus();
+
+	for_each_possible_cpu(i) {
+		cvm_oct_napi[i].available = 1;
+		netif_napi_add(dev_for_napi, &cvm_oct_napi[i].napi,
+			       cvm_oct_napi_poll, rx_napi_weight);
+		napi_enable(&cvm_oct_napi[i].napi);
+	}
+	/* Before interrupts are enabled, no RX processing will occur,
+	 * so we can initialize all those things out side of the
+	 * lock.
+	 */
+	spin_lock_init(&core_state.lock);
+
+	/* Register an IRQ hander for to receive POW interrupts */
+	i = request_irq(OCTEON_IRQ_WORKQ0 + pow_receive_group,
+			cvm_oct_do_interrupt, 0, dev_for_napi->name, &cvm_oct_list);
+
+	if (i)
+		panic("Could not acquire Ethernet IRQ %d\n",
+		      OCTEON_IRQ_WORKQ0 + pow_receive_group);
+
+	if (cvm_oct_sso_initialize(num_wqe))
+		goto err;
+
+	/* Scheduld NAPI now.  This will indirectly enable interrupts. */
+	preempt_disable();
+	cvm_oct_enable_one_cpu();
+	preempt_enable();
+	return;
+err:
+	free_irq(OCTEON_IRQ_WORKQ0 + pow_receive_group, &cvm_oct_list);
+	return;
+}
+
+void cvm_oct_rx_shutdown0(void)
+{
+	int i;
+
+	/* Disable POW/SSO interrupt */
+	if (OCTEON_IS_MODEL(OCTEON_CN68XX))
+		cvmx_write_csr(CVMX_SSO_WQ_INT_THRX(pow_receive_group), 0);
+	else
+		cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group), 0);
+
+	/* Free the interrupt handler */
+	free_irq(OCTEON_IRQ_WORKQ0 + pow_receive_group, &cvm_oct_list);
+
+#ifdef CONFIG_SMP
+	octeon_release_ipi_handler(cvm_oct_enable_one_message);
+#endif
+
+	/* Shutdown all of the NAPIs */
+	for_each_possible_cpu(i)
+		netif_napi_del(&cvm_oct_napi[i].napi);
+}
+
+void cvm_oct_rx_shutdown1(void)
+{
+	union cvmx_fpa_quex_available queue_available;
+	union cvmx_sso_cfg sso_cfg;
+	union cvmx_sso_rwq_pop_fptr pop_fptr;
+	union cvmx_sso_rwq_psh_fptr fptr;
+	union cvmx_sso_fpage_cnt fpage_cnt;
+	int num_to_transfer, count, i;
+	void *mem;
+	const int sso_fpe_bit = 45;
+
+	if (!OCTEON_IS_MODEL(OCTEON_CN68XX))
+		return;
+
+	/* Spurious FPE errors will happen doing this cleanup.
+	 * Disable the indication.
+	 */
+	octeon_hw_status_disable(CVMX_SSO_ERR, 1ull << sso_fpe_bit);
+
+	sso_cfg.u64 = cvmx_read_csr(CVMX_SSO_CFG);
+	sso_cfg.s.rwen = 0;
+	sso_cfg.s.rwq_byp_dis = 1;
+	cvmx_write_csr(CVMX_SSO_CFG, sso_cfg.u64);
+	cvmx_read_csr(CVMX_SSO_CFG);
+	queue_available.u64 = cvmx_read_csr(CVMX_FPA_QUEX_AVAILABLE(8));
+
+	/* Make CVMX_FPA_QUEX_AVAILABLE(8) % 16 == 0*/
+	for (num_to_transfer = (16 - queue_available.s.que_siz) % 16;
+	     num_to_transfer > 0; num_to_transfer--) {
+		do {
+			pop_fptr.u64 = cvmx_read_csr(CVMX_SSO_RWQ_POP_FPTR);
+		} while (!pop_fptr.s.val);
+		for (;;) {
+			fptr.u64 = cvmx_read_csr(CVMX_SSO_RWQ_PSH_FPTR);
+			if (!fptr.s.full)
+				break;
+			__delay(1000);
+		}
+		fptr.s.fptr = pop_fptr.s.fptr;
+		cvmx_write_csr(CVMX_SSO_RWQ_PSH_FPTR, fptr.u64);
+	}
+	cvmx_read_csr(CVMX_SSO_CFG);
+
+	do {
+		queue_available.u64 = cvmx_read_csr(CVMX_FPA_QUEX_AVAILABLE(8));
+	} while (queue_available.s.que_siz % 16);
+
+	sso_cfg.s.rwen = 1;
+	sso_cfg.s.rwq_byp_dis = 0;
+	cvmx_write_csr(CVMX_SSO_CFG, sso_cfg.u64);
+
+	for (i = 0; i < 8; i++) {
+		union cvmx_sso_rwq_head_ptrx head_ptr;
+		union cvmx_sso_rwq_tail_ptrx tail_ptr;
+
+		head_ptr.u64 = cvmx_read_csr(CVMX_SSO_RWQ_HEAD_PTRX(i));
+		tail_ptr.u64 = cvmx_read_csr(CVMX_SSO_RWQ_TAIL_PTRX(i));
+		WARN_ON(head_ptr.s.ptr != tail_ptr.s.ptr);
+
+		mem = phys_to_virt(((u64)head_ptr.s.ptr) << 7);
+		kmem_cache_free(cvm_oct_kmem_sso, mem);
+	}
+
+	count = 0;
+
+	do {
+		do {
+			pop_fptr.u64 = cvmx_read_csr(CVMX_SSO_RWQ_POP_FPTR);
+			if (pop_fptr.s.val) {
+				mem = phys_to_virt(((u64)pop_fptr.s.fptr) << 7);
+				kmem_cache_free(cvm_oct_kmem_sso, mem);
+				count++;
+			}
+		} while (pop_fptr.s.val);
+		fpage_cnt.u64 = cvmx_read_csr(CVMX_SSO_FPAGE_CNT);
+	} while (fpage_cnt.s.fpage_cnt);
+
+	WARN_ON(count != cvm_oct_sso_fptr_count);
+
+	sso_cfg.s.rwen = 0;
+	sso_cfg.s.rwq_byp_dis = 0;
+	cvmx_write_csr(CVMX_SSO_CFG, sso_cfg.u64);
+	kmem_cache_destroy(cvm_oct_kmem_sso);
+	cvm_oct_kmem_sso = NULL;
+
+	/* Clear any FPE indicators, and reenable. */
+	cvmx_write_csr(CVMX_SSO_ERR, 1ull << sso_fpe_bit);
+	octeon_hw_status_enable(CVMX_SSO_ERR, 1ull << sso_fpe_bit);
+}
diff --git a/drivers/net/ethernet/octeon/ethernet-sgmii.c b/drivers/net/ethernet/octeon/ethernet-sgmii.c
new file mode 100644
index 0000000..00d5f7a
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-sgmii.c
@@ -0,0 +1,241 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+**********************************************************************/
+#include <linux/phy.h>
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/ratelimit.h>
+#include <net/dst.h>
+
+#include <asm/octeon/octeon.h>
+#include <asm/octeon/octeon-hw-status.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+
+#include <asm/octeon/cvmx-helper.h>
+
+#include <asm/octeon/cvmx-gmxx-defs.h>
+#include <asm/octeon/cvmx-npei-defs.h>
+
+#define INT_BIT_LOC_FAULT 20
+#define INT_BIT_REM_FAULT 21
+
+/* Although these functions are called cvm_oct_sgmii_*, they also
+ * happen to be used for the XAUI ports as well.
+ */
+
+static void cvm_oct_sgmii_poll(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	cvmx_helper_link_info_t link_info;
+
+	link_info = cvmx_helper_link_get(priv->ipd_port);
+	if (link_info.u64 == priv->link_info)
+		return;
+
+	link_info = cvmx_helper_link_autoconf(priv->ipd_port);
+	priv->link_info = link_info.u64;
+
+	/* Tell the core */
+	cvm_oct_set_carrier(priv, link_info);
+}
+static int cvm_oct_sgmii_hw_status(struct notifier_block *nb,
+				   unsigned long val, void *v)
+{
+	struct octeon_ethernet *priv = container_of(nb, struct octeon_ethernet,
+						    hw_status_notifier);
+
+	if (val == OCTEON_HW_STATUS_SOURCE_ASSERTED) {
+		struct octeon_hw_status_data *d = v;
+		if (d->reg == CVMX_GMXX_RXX_INT_REG(priv->interface_port,
+						    priv->interface) &&
+		    (d->bit == INT_BIT_LOC_FAULT ||
+		     d->bit == INT_BIT_REM_FAULT)) {
+			cvmx_helper_link_autoconf(priv->ipd_port);
+			return NOTIFY_STOP;
+		}
+	}
+	return NOTIFY_DONE;
+}
+
+int cvm_oct_sgmii_open(struct net_device *dev)
+{
+	struct octeon_hw_status_reg sr[3];
+	union cvmx_gmxx_prtx_cfg gmx_cfg;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	cvmx_helper_link_info_t link_info;
+	cvmx_helper_interface_mode_t imode;
+	int rv, i;
+	u64 en_mask;
+
+	rv = cvm_oct_phy_setup_device(dev);
+	if (rv)
+		return rv;
+
+	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
+	gmx_cfg.s.en = 1;
+	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface), gmx_cfg.u64);
+
+	if (octeon_is_simulation())
+		return 0;
+
+	if (priv->phydev) {
+		int r = phy_read_status(priv->phydev);
+		if (r == 0 && priv->phydev->link == 0)
+			netif_carrier_off(dev);
+		cvm_oct_adjust_link(dev);
+	} else {
+		link_info = cvmx_helper_link_get(priv->ipd_port);
+		if (!link_info.s.link_up)
+			netif_carrier_off(dev);
+		spin_lock(&priv->poll_lock);
+		priv->poll = cvm_oct_sgmii_poll;
+		spin_unlock(&priv->poll_lock);
+		cvm_oct_sgmii_poll(dev);
+	}
+	imode = cvmx_helper_interface_get_mode(priv->interface);
+	switch (imode) {
+	case CVMX_HELPER_INTERFACE_MODE_XAUI:
+	case CVMX_HELPER_INTERFACE_MODE_RXAUI:
+		/* Handle GMXX_RXX_INT_REG[LOC_FAULT,REM_FAULT]*/
+		priv->hw_status_notifier.priority = 10;
+		priv->hw_status_notifier.notifier_call = cvm_oct_sgmii_hw_status;
+		octeon_hw_status_notifier_register(&priv->hw_status_notifier);
+		memset(sr, 0, sizeof(sr));
+		i = 0;
+		en_mask = 0;
+		if (OCTEON_IS_OCTEONPLUS()) {
+			sr[i].reg = 46; /* RML */
+			sr[i].reg_is_hwint = 1;
+			sr[i].has_child = 1;
+			i++;
+			sr[i].reg = CVMX_NPEI_RSL_INT_BLOCKS;
+			/* GMX[priv->interface]*/
+			sr[i].bit = priv->interface + 1;
+			sr[i].has_child = 1;
+			i++;
+		} else if (octeon_has_feature(OCTEON_FEATURE_CIU2)) {
+			/* PKT[AGX[priv->interface]]*/
+			sr[i].reg = (6 << 6) | priv->interface;
+			sr[i].reg_is_hwint = 1;
+			sr[i].has_child = 1;
+			i++;
+		} else {
+			/* INT_SUM1[AGX[priv->interface]]*/
+			sr[i].reg = (1 << 6) | (priv->interface + 36);
+			sr[i].reg_is_hwint = 1;
+			sr[i].has_child = 1;
+			i++;
+		}
+		sr[i].reg = CVMX_GMXX_RXX_INT_REG(priv->interface_port,
+						  priv->interface);
+		sr[i].mask_reg = CVMX_GMXX_RXX_INT_EN(priv->interface_port,
+						      priv->interface);
+		sr[i].ack_w1c = 1;
+
+		sr[i].bit = INT_BIT_LOC_FAULT;
+		en_mask |= 1ull << sr[i].bit;
+		octeon_hw_status_add_source(sr);
+
+		sr[i].bit = INT_BIT_REM_FAULT;
+		en_mask |= 1ull << sr[i].bit;
+		octeon_hw_status_add_source(sr);
+
+		octeon_hw_status_enable(sr[i].reg, en_mask);
+		break;
+	default:
+		break;
+	}
+	return 0;
+}
+
+int cvm_oct_sgmii_stop(struct net_device *dev)
+{
+	union cvmx_gmxx_prtx_cfg gmx_cfg;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
+	gmx_cfg.s.en = 0;
+	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface), gmx_cfg.u64);
+
+	if (priv->hw_status_notifier.notifier_call) {
+		struct octeon_hw_status_reg sr;
+		memset(&sr, 0, sizeof(sr));
+
+		sr.reg = CVMX_GMXX_RXX_INT_REG(priv->interface_port,
+					       priv->interface);
+		sr.mask_reg = CVMX_GMXX_RXX_INT_EN(priv->interface_port,
+						   priv->interface);
+		sr.ack_w1c = 1;
+		sr.bit = INT_BIT_LOC_FAULT;
+		octeon_hw_status_remove_source(&sr);
+		sr.bit = INT_BIT_REM_FAULT;
+		octeon_hw_status_remove_source(&sr);
+		octeon_hw_status_notifier_unregister(&priv->hw_status_notifier);
+		priv->hw_status_notifier.notifier_call = NULL;
+	}
+
+	spin_lock(&priv->poll_lock);
+	priv->poll = NULL;
+	spin_unlock(&priv->poll_lock);
+
+	if (priv->phydev)
+		phy_disconnect(priv->phydev);
+	priv->phydev = NULL;
+
+	return cvm_oct_common_stop(dev);
+}
+
+static void cvm_oct_sgmii_link_change(struct octeon_ethernet *priv,
+				      cvmx_helper_link_info_t link_info)
+{
+	if (link_info.s.link_up)
+		octeon_error_tree_enable(CVMX_ERROR_GROUP_ETHERNET,
+					 priv->ipd_port);
+	else
+		octeon_error_tree_disable(CVMX_ERROR_GROUP_ETHERNET,
+					  priv->ipd_port);
+}
+
+int cvm_oct_sgmii_init(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	cvm_oct_common_init(dev);
+	dev->netdev_ops->ndo_stop(dev);
+	priv->link_change = cvm_oct_sgmii_link_change;
+
+	return 0;
+}
+
+void cvm_oct_sgmii_uninit(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	octeon_error_tree_disable(CVMX_ERROR_GROUP_ETHERNET,
+				  priv->ipd_port);
+}
diff --git a/drivers/net/ethernet/octeon/ethernet-spi.c b/drivers/net/ethernet/octeon/ethernet-spi.c
new file mode 100644
index 0000000..64e30b2
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-spi.c
@@ -0,0 +1,239 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/interrupt.h>
+#include <net/dst.h>
+
+#include <asm/octeon/octeon-hw-status.h>
+#include <asm/octeon/octeon.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+
+#include <asm/octeon/cvmx-spi.h>
+
+#include <asm/octeon/cvmx-npi-defs.h>
+#include <asm/octeon/cvmx-spxx-defs.h>
+#include <asm/octeon/cvmx-stxx-defs.h>
+
+#define MASK_FOR_BIT(_X) (1ull << (_X))
+
+#define INT_BIT_SPX_PRTNXA 0
+#define INT_BIT_SPX_ABNORM 1
+#define INT_BIT_SPX_SPIOVR 4
+#define INT_BIT_SPX_CLSERR 5
+#define INT_BIT_SPX_DRWNNG 6
+#define INT_BIT_SPX_RSVERR 7
+#define INT_BIT_SPX_TPAOVR 8
+#define INT_BIT_SPX_DIPERR 9
+#define INT_BIT_SPX_SYNCERR 10
+#define INT_BIT_SPX_CALERR 11
+
+#define SPX_MASK (				\
+	MASK_FOR_BIT(INT_BIT_SPX_PRTNXA) |	\
+	MASK_FOR_BIT(INT_BIT_SPX_ABNORM) |	\
+	MASK_FOR_BIT(INT_BIT_SPX_SPIOVR) |	\
+	MASK_FOR_BIT(INT_BIT_SPX_CLSERR) |	\
+	MASK_FOR_BIT(INT_BIT_SPX_DRWNNG) |	\
+	MASK_FOR_BIT(INT_BIT_SPX_RSVERR) |	\
+	MASK_FOR_BIT(INT_BIT_SPX_TPAOVR) |	\
+	MASK_FOR_BIT(INT_BIT_SPX_DIPERR) |	\
+	MASK_FOR_BIT(INT_BIT_SPX_CALERR))
+
+#define INT_BIT_SPX_SPF 31
+
+#define INT_BIT_STX_CALPAR0 0
+#define INT_BIT_STX_CALPAR1 1
+#define INT_BIT_STX_OVRBST 2
+#define INT_BIT_STX_DATOVR 3
+#define INT_BIT_STX_DIPERR 4
+#define INT_BIT_STX_NOSYNC 5
+#define INT_BIT_STX_UNXFRM 6
+#define INT_BIT_STX_FRMERR 7
+#define INT_BIT_STX_SYNCERR 8
+#define STX_MASK (					\
+		MASK_FOR_BIT(INT_BIT_STX_CALPAR0) |	\
+		MASK_FOR_BIT(INT_BIT_STX_CALPAR1) |	\
+		MASK_FOR_BIT(INT_BIT_STX_OVRBST) |	\
+		MASK_FOR_BIT(INT_BIT_STX_DATOVR) |	\
+		MASK_FOR_BIT(INT_BIT_STX_DIPERR) |	\
+		MASK_FOR_BIT(INT_BIT_STX_NOSYNC) |	\
+		MASK_FOR_BIT(INT_BIT_STX_UNXFRM) |	\
+		MASK_FOR_BIT(INT_BIT_STX_FRMERR) |	\
+		MASK_FOR_BIT(INT_BIT_STX_SYNCERR))
+
+static int need_retrain[2] = { 0, 0 };
+
+static int cvm_oct_spi_hw_status(struct notifier_block *nb, unsigned long val, void *v)
+{
+	struct octeon_ethernet *priv = container_of(nb, struct octeon_ethernet, hw_status_notifier);
+
+	if (val == OCTEON_HW_STATUS_SOURCE_ASSERTED) {
+		struct octeon_hw_status_data *d = v;
+		if (d->reg == CVMX_SPXX_INT_REG(priv->interface) ||
+		    d->reg == CVMX_STXX_INT_REG(priv->interface)) {
+			if (need_retrain[priv->interface])
+				return NOTIFY_STOP;
+			need_retrain[priv->interface] = 1;
+			octeon_hw_status_disable(CVMX_SPXX_INT_REG(priv->interface), SPX_MASK);
+			octeon_hw_status_disable(CVMX_STXX_INT_REG(priv->interface), STX_MASK);
+		}
+	}
+	return NOTIFY_DONE;
+}
+static void cvm_oct_spi_poll(struct net_device *dev)
+{
+	static int spi4000_port;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	int interface;
+
+	for (interface = 0; interface < 2; interface++) {
+		if ((priv->ipd_port == interface * 16) && need_retrain[interface]) {
+			if (cvmx_spi_restart_interface(interface, CVMX_SPI_MODE_DUPLEX, 10) == 0) {
+				need_retrain[interface] = 0;
+				octeon_hw_status_enable(CVMX_SPXX_INT_REG(priv->interface), SPX_MASK);
+				octeon_hw_status_enable(CVMX_STXX_INT_REG(priv->interface), STX_MASK);
+			}
+		}
+
+		/* The SPI4000 TWSI interface is very slow. In order
+		 * not to bring the system to a crawl, we only poll a
+		 * single port every second. This means negotiation
+		 * speed changes take up to 10 seconds, but at least
+		 * we don't waste absurd amounts of time waiting for
+		 * TWSI.
+		 */
+		if (priv->ipd_port == spi4000_port) {
+			/* This function does nothing if it is called on an
+			 * interface without a SPI4000.
+			 */
+			cvmx_spi4000_check_speed(interface, priv->ipd_port);
+			/* Normal ordering increments. By decrementing
+			 * we only match once per iteration.
+			 */
+			spi4000_port--;
+			if (spi4000_port < 0)
+				spi4000_port = 10;
+		}
+	}
+}
+
+int cvm_oct_spi_init(struct net_device *dev)
+{
+	int r, i;
+	struct octeon_hw_status_reg sr[3];
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	if ((priv->ipd_port == 0) || (priv->ipd_port == 16)) {
+
+		priv->hw_status_notifier.priority = 10;
+		priv->hw_status_notifier.notifier_call = cvm_oct_spi_hw_status;
+		r = octeon_hw_status_notifier_register(&priv->hw_status_notifier);
+		if (r)
+			return r;
+
+		memset(sr, 0, sizeof(sr));
+		sr[0].reg = 46; /* RML */
+		sr[0].reg_is_hwint = 1;
+		sr[0].has_child = 1;
+
+		sr[1].reg = CVMX_NPI_RSL_INT_BLOCKS;
+		sr[1].bit = priv->interface + 18;
+		sr[1].has_child = 1;
+
+		sr[2].reg = CVMX_SPXX_INT_REG(priv->interface);
+		sr[2].mask_reg = CVMX_SPXX_INT_MSK(priv->interface);
+		sr[2].ack_w1c = 1;
+
+		for (i = 0; i < 32; i++) {
+			if ((1ull << i) & SPX_MASK) {
+				sr[2].bit = i;
+				octeon_hw_status_add_source(sr);
+			}
+		}
+
+		sr[2].bit = INT_BIT_SPX_SPF;
+		sr[2].ack_w1c = 0;
+		octeon_hw_status_add_source(sr);
+		octeon_hw_status_enable(sr[2].reg, SPX_MASK);
+
+		sr[2].reg = CVMX_STXX_INT_REG(priv->interface);
+		sr[2].mask_reg = CVMX_STXX_INT_MSK(priv->interface);
+		sr[2].ack_w1c = 1;
+
+		for (i = 0; i < 32; i++) {
+			if ((1ull << i) & STX_MASK) {
+				sr[2].bit = i;
+				octeon_hw_status_add_source(sr);
+			}
+		}
+		octeon_hw_status_enable(sr[2].reg, STX_MASK);
+
+		priv->poll = cvm_oct_spi_poll;
+	}
+	cvm_oct_common_init(dev);
+	return 0;
+}
+
+void cvm_oct_spi_uninit(struct net_device *dev)
+{
+	int i;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	if (priv->hw_status_notifier.notifier_call) {
+		struct octeon_hw_status_reg sr;
+		memset(&sr, 0, sizeof(sr));
+
+		sr.reg = CVMX_SPXX_INT_REG(priv->interface);
+		sr.mask_reg = CVMX_SPXX_INT_MSK(priv->interface);
+		sr.ack_w1c = 1;
+
+		for (i = 0; i < 32; i++) {
+			if ((1ull << i) & SPX_MASK) {
+				sr.bit = i;
+				octeon_hw_status_remove_source(&sr);
+			}
+		}
+		sr.bit = INT_BIT_SPX_SPF;
+		sr.ack_w1c = 0;
+		octeon_hw_status_remove_source(&sr);
+
+		sr.reg = CVMX_STXX_INT_REG(priv->interface);
+		sr.mask_reg = CVMX_STXX_INT_MSK(priv->interface);
+		sr.ack_w1c = 1;
+		for (i = 0; i < 32; i++) {
+			if ((1ull << i) & STX_MASK) {
+				sr.bit = i;
+				octeon_hw_status_remove_source(&sr);
+			}
+		}
+
+		octeon_hw_status_notifier_unregister(&priv->hw_status_notifier);
+		priv->hw_status_notifier.notifier_call = NULL;
+	}
+}
diff --git a/drivers/net/ethernet/octeon/ethernet-srio.c b/drivers/net/ethernet/octeon/ethernet-srio.c
new file mode 100644
index 0000000..23786852
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-srio.c
@@ -0,0 +1,252 @@
+/*************************************************************************
+ *
+ * Author: Cavium Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2010 - 2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ *************************************************************************/
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <net/dst.h>
+#include <net/sock.h>
+#include <linux/rio.h>
+#include <linux/rio_drv.h>
+#include <linux/rio_ids.h>
+#include <linux/if_vlan.h>
+
+#include <asm/octeon/octeon.h>
+#include <asm/octeon/cvmx-srio.h>
+#include <asm/octeon/cvmx-pip-defs.h>
+#include <asm/octeon/cvmx-sriox-defs.h>
+#include <asm/octeon/cvmx-sriomaintx-defs.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+
+struct net_device_stats *cvm_oct_srio_get_stats(struct net_device *dev)
+{
+	return &dev->stats;
+}
+
+int cvm_oct_srio_set_mac_address(struct net_device *dev, void *addr)
+{
+	/* FIXME: Should this be allowed? Should it change our device ID? */
+	memcpy(dev->dev_addr, addr + 2, 6);
+	return 0;
+}
+
+int cvm_oct_srio_change_mtu(struct net_device *dev, int new_mtu)
+{
+	union cvmx_pip_frm_len_chkx pip_frm_len_chkx;
+#if defined(CONFIG_VLAN_8021Q) || defined(CONFIG_VLAN_8021Q_MODULE)
+	int vlan_bytes = VLAN_HLEN;
+#else
+	int vlan_bytes = 0;
+#endif
+	unsigned int max_mtu;
+
+	/*
+	 * Limit the MTU to make sure the ethernet packets are between
+	 * 68 bytes and 4096 - ethernet header, fcs and optional VLAN bytes.
+	 */
+	max_mtu = RIO_MAX_MSG_SIZE - ETH_HLEN - vlan_bytes - ETH_FCS_LEN;
+	if ((new_mtu < 68) || (new_mtu > max_mtu)) {
+		netdev_warn(dev, "MTU must be between %d and %d.\n",
+			    68, max_mtu);
+		return -EINVAL;
+	}
+	dev->mtu = new_mtu;
+
+	/* set up pip. other interfaces prefer to disable the pip check. */
+	pip_frm_len_chkx.u64 = cvmx_read_csr(CVMX_PIP_FRM_LEN_CHKX(0));
+	pip_frm_len_chkx.s.maxlen = (new_mtu + 256) & ~0xff;
+	cvmx_write_csr(CVMX_PIP_FRM_LEN_CHKX(0), pip_frm_len_chkx.u64);
+
+	return 0;
+}
+
+int cvm_oct_xmit_srio(struct sk_buff *skb, struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	union cvmx_srio_tx_message_header tx_header;
+	u64 dest_mac;
+
+	if (unlikely(skb->len > 4096)) {
+		dev_kfree_skb(skb);
+		netdev_dbg(dev, "TX packet larger than 4096 bytes. Dropped.\n");
+		return 0;
+	}
+
+	/* srio message length needs to be a multiple of 8 */
+	if (unlikely(skb_tailroom(skb) < 8))
+		/* can optionally allocate a larger sk_buff and do a copy */
+		skb->len = skb->len;
+	else
+		skb->len = ((skb->len >> 3) + 1) << 3;
+
+	tx_header.u64 = priv->srio_tx_header;
+	/* Use the socket priority if it is available */
+	if (skb->sk) {
+		if (skb->sk->sk_priority < 0)
+			tx_header.s.prio = 0;
+		else if (skb->sk->sk_priority > 3)
+			tx_header.s.prio = 3;
+		else
+			tx_header.s.prio = skb->sk->sk_priority;
+	}
+
+	/* Extract the destination MAC address from the packet */
+	dest_mac = *(u64 *)skb->data >> 16;
+
+	/* If this is a broadcast/multicast we must manually send to everyone */
+	if (dest_mac>>40) {
+		struct list_head *pos;
+		struct sk_buff *new_skb;
+
+		list_for_each(pos, &priv->srio_bcast) {
+			struct octeon_ethernet_srio_bcast_target *t;
+
+			t = container_of(pos, struct octeon_ethernet_srio_bcast_target, list);
+			/* Create a new SKB since each packet will have different data */
+			new_skb = skb_copy(skb, GFP_ATOMIC);
+			if (new_skb) {
+				tx_header.s.did = t->destid;
+				*(u64 *)__skb_push(new_skb, 8) = tx_header.u64;
+				cvm_oct_xmit(new_skb, dev);
+			} else {
+				netdev_dbg(dev, "SKB allocation failed\n");
+				break;
+			}
+		}
+
+		dev->stats.tx_packets++;
+		dev->stats.tx_bytes += skb->len;
+		dev_kfree_skb(skb);
+		return NETDEV_TX_OK;
+	} else {
+		/* Use the low two bytes of the destination MAC as the SRIO
+		 * destination */
+		/* tx_header.s.did = *(u16 *)(skb->data + 4); */
+		tx_header.s.did = *(u8 *)(skb->data + 5);
+		if (unlikely(skb_headroom(skb) < 8)) {
+			struct sk_buff *new_skb = skb_copy(skb, GFP_ATOMIC);
+			dev_kfree_skb(skb);
+			if (!new_skb) {
+				netdev_dbg(dev,
+					   "SKB didn't have room for SRIO header and allocation failed\n");
+				return NETDEV_TX_OK;
+			}
+			skb = new_skb;
+		}
+
+		dev->stats.tx_packets++;
+		dev->stats.tx_bytes += skb->len;
+		*(u64 *)__skb_push(skb, 8) = tx_header.u64;
+		return cvm_oct_xmit(skb, dev);
+	}
+}
+
+int cvm_oct_srio_init(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	int srio_port = (priv->ipd_port - 40) >> 1;
+	union cvmx_sriox_status_reg srio_status_reg;
+
+	dev->features |= NETIF_F_LLTX; /* We do our own locking, Linux doesn't need to */
+
+	SET_ETHTOOL_OPS(dev, &cvm_oct_ethtool_ops);
+
+	/* Make sure register access is allowed */
+	srio_status_reg.u64 = cvmx_read_csr(CVMX_SRIOX_STATUS_REG(srio_port));
+	if (!srio_status_reg.s.access)
+		return 0;
+
+	netif_carrier_on(dev);
+
+	dev->netdev_ops->ndo_change_mtu(dev, dev->mtu);
+
+	return 0;
+}
+
+int cvm_oct_srio_open(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	int srio_port = (priv->ipd_port - 40) >> 1;
+	struct rio_dev *rdev;
+	struct sockaddr sa;
+	u32 devid;
+
+	cvmx_srio_config_read32(srio_port, 0, -1, 1, 0, CVMX_SRIOMAINTX_PRI_DEV_ID(srio_port), &devid);
+
+	sa.sa_data[0] = 0;
+	sa.sa_data[1] = 0;
+	sa.sa_data[2] = 0;
+	sa.sa_data[3] = 0;
+	if (devid >> 16) {
+		sa.sa_data[4] = 0;
+		sa.sa_data[5] = (devid >> 16) & 0xff;
+	} else {
+		sa.sa_data[4] = (devid >> 8) & 0xff;
+		sa.sa_data[5] = devid & 0xff;
+	}
+
+	dev->netdev_ops->ndo_set_mac_address(dev, &sa);
+
+	rdev = NULL;
+	for (;;) {
+		struct octeon_ethernet_srio_bcast_target *target;
+		rdev = rio_get_device(RIO_ANY_ID, RIO_ANY_ID, rdev);
+		if (!rdev)
+			break;
+		/* Skip devices not on my rio port */
+		if (rdev->net->hport->id != srio_port)
+			continue;
+		/* Skip switches */
+		if (rdev->destid == 0xffff)
+			continue;
+		target = kmalloc(sizeof(*target), GFP_KERNEL);
+		if (!target) {
+			WARN(1, "No memory");
+			return -ENOMEM;
+		}
+		target->destid = rdev->destid;
+		list_add(&target->list, &priv->srio_bcast);
+	}
+	return 0;
+}
+
+int cvm_oct_srio_stop(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	struct list_head *pos;
+	struct list_head *n;
+
+	list_for_each_safe(pos, n, &priv->srio_bcast) {
+		struct octeon_ethernet_srio_bcast_target *t;
+		list_del(pos);
+		t = container_of(pos, struct octeon_ethernet_srio_bcast_target,
+				 list);
+		kfree(t);
+	}
+	return 0;
+}
diff --git a/drivers/net/ethernet/octeon/ethernet-tx.c b/drivers/net/ethernet/octeon/ethernet-tx.c
new file mode 100644
index 0000000..9c700f9
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-tx.c
@@ -0,0 +1,233 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/init.h>
+#include <linux/etherdevice.h>
+#include <linux/ip.h>
+#include <linux/ratelimit.h>
+#include <linux/string.h>
+#include <linux/interrupt.h>
+#include <net/dst.h>
+
+#include <asm/octeon/octeon.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+
+#include <asm/octeon/cvmx-wqe.h>
+#include <asm/octeon/cvmx-fau.h>
+#include <asm/octeon/cvmx-ipd.h>
+#include <asm/octeon/cvmx-pip.h>
+#include <asm/octeon/cvmx-pko.h>
+#include <asm/octeon/cvmx-helper.h>
+
+#include <asm/octeon/cvmx-gmxx-defs.h>
+
+/*
+ * You can define GET_SKBUFF_QOS() to override how the skbuff output
+ * function determines which output queue is used. The default
+ * implementation always uses the base queue for the port. If, for
+ * example, you wanted to use the skb->priority fieid, define
+ * GET_SKBUFF_QOS as: #define GET_SKBUFF_QOS(skb) ((skb)->priority)
+ */
+#ifndef GET_SKBUFF_QOS
+#define GET_SKBUFF_QOS(skb) 0
+#endif
+
+#if REUSE_SKBUFFS_WITHOUT_FREE
+static bool cvm_oct_skb_ok_for_reuse(struct sk_buff *skb)
+{
+	unsigned char *fpa_head = cvm_oct_get_fpa_head(skb);
+
+	if (unlikely(skb->data < fpa_head))
+		return false;
+
+	if (unlikely(fpa_head - skb->head < sizeof(void *)))
+		return false;
+
+	if (unlikely((skb_end_pointer(skb) - fpa_head) < FPA_PACKET_POOL_SIZE))
+		return false;
+
+	if (unlikely(skb_shared(skb)) ||
+	    unlikely(skb_cloned(skb)) ||
+	    unlikely(skb->fclone != SKB_FCLONE_UNAVAILABLE))
+		return false;
+
+	return true;
+}
+
+static void cvm_oct_skb_prepare_for_reuse(struct sk_buff *skb)
+{
+	int r;
+	unsigned char *fpa_head = cvm_oct_get_fpa_head(skb);
+
+	skb->data_len = 0;
+	skb_frag_list_init(skb);
+
+	/* The check also resets all the fields. */
+	r = skb_recycle_check(skb, FPA_PACKET_POOL_SIZE);
+	WARN(!r, "SKB recycle logic fail\n");
+
+	*(struct sk_buff **)(fpa_head - sizeof(void *)) = skb;
+	skb->truesize = sizeof(*skb) + skb_end_pointer(skb) - skb->head;
+}
+
+static inline void cvm_oct_set_back(struct sk_buff *skb,
+				    union cvmx_buf_ptr *hw_buffer)
+{
+	unsigned char *fpa_head = cvm_oct_get_fpa_head(skb);
+
+	hw_buffer->s.back = ((unsigned long)skb->data >> 7) - ((unsigned long)fpa_head >> 7);
+}
+#else
+static bool cvm_oct_skb_ok_for_reuse(struct sk_buff *skb)
+{
+	return false;
+}
+static void cvm_oct_skb_prepare_for_reuse(struct sk_buff *skb)
+{
+	/* Do nothing */
+}
+
+static inline void cvm_oct_set_back(struct sk_buff *skb,
+				    union cvmx_buf_ptr *hw_buffer)
+{
+	/* Do nothing. */
+}
+
+#endif
+
+#define CVM_OCT_LOCKLESS 1
+#include "ethernet-xmit.c"
+
+#undef CVM_OCT_LOCKLESS
+#include "ethernet-xmit.c"
+
+/**
+ * cvm_oct_transmit_qos - transmit a work queue entry out of the ethernet port.
+ *
+ * Both the work queue entry and the packet data can optionally be
+ * freed. The work will be freed on error as well.
+ *
+ * @dev: Device to transmit out.
+ * @work_queue_entry: Work queue entry to send
+ * @do_free: True if the work queue entry and packet data should be
+ *           freed. If false, neither will be freed.
+ * @qos: Index into the queues for this port to transmit on. This is
+ *       used to implement QoS if their are multiple queues per
+ *       port. This parameter must be between 0 and the number of
+ *       queues per port minus 1. Values outside of this range will be
+ *       change to zero.
+ *
+ * Returns Zero on success, negative on failure.
+ */
+int cvm_oct_transmit_qos(struct net_device *dev,
+			 void *work_queue_entry,
+			 int do_free,
+			 int qos)
+{
+	unsigned long			flags;
+	cvmx_buf_ptr_t			hw_buffer;
+	cvmx_pko_command_word0_t	pko_command;
+	int				dropped;
+	struct octeon_ethernet		*priv = netdev_priv(dev);
+	cvmx_wqe_t			*work = work_queue_entry;
+	cvmx_pko_lock_t lock_type;
+
+	if (!(dev->flags & IFF_UP)) {
+		netdev_err(dev, "Error: Device not up\n");
+		if (do_free)
+			cvm_oct_free_work(work);
+		return -1;
+	}
+
+	if (priv->tx_lockless) {
+		qos = cvmx_get_core_num();
+		lock_type = CVMX_PKO_LOCK_NONE;
+	} else {
+		/*
+		 * The check on CVMX_PKO_QUEUES_PER_PORT_* is designed to
+		 * completely remove "qos" in the event neither interface
+		 * supports multiple queues per port
+		 */
+		if (priv->tx_multiple_queues) {
+			if (qos <= 0)
+				qos = 0;
+			else if (qos >= priv->num_tx_queues)
+				qos = 0;
+		} else
+			qos = 0;
+		lock_type = CVMX_PKO_LOCK_CMD_QUEUE;
+	}
+
+	/* Start off assuming no drop */
+	dropped = 0;
+
+	local_irq_save(flags);
+
+	cvmx_pko_send_packet_prepare_pkoid(priv->pko_port, priv->tx_queue[qos].queue, lock_type);
+
+	/* Build the PKO buffer pointer */
+	hw_buffer.u64 = 0;
+	hw_buffer.s.addr = work->packet_ptr.s.addr;
+	hw_buffer.s.pool = packet_pool;
+	hw_buffer.s.size = FPA_PACKET_POOL_SIZE;
+	hw_buffer.s.back = work->packet_ptr.s.back;
+
+	/* Build the PKO command */
+	pko_command.u64 = 0;
+	pko_command.s.n2 = 1; /* Don't pollute L2 with the outgoing packet */
+	pko_command.s.dontfree = !do_free;
+	pko_command.s.segs = work->word2.s.bufs;
+	pko_command.s.total_bytes = work->word1.len;
+
+	/* Check if we can use the hardware checksumming */
+	if (unlikely(work->word2.s.not_IP || work->word2.s.IP_exc))
+		pko_command.s.ipoffp1 = 0;
+	else
+		pko_command.s.ipoffp1 = sizeof(struct ethhdr) + 1;
+
+	/* Send the packet to the output queue */
+	if (unlikely(cvmx_pko_send_packet_finish_pkoid(priv->pko_port, priv->tx_queue[qos].queue, pko_command, hw_buffer, lock_type))) {
+		netdev_err(dev, "Error: Failed to send the packet\n");
+		dropped = -1;
+	}
+	local_irq_restore(flags);
+
+	if (unlikely(dropped)) {
+		if (do_free)
+			cvm_oct_free_work(work);
+		dev->stats.tx_dropped++;
+	} else
+	if (do_free)
+		cvmx_fpa_free(work, wqe_pool, DONT_WRITEBACK(1));
+
+	return dropped;
+}
+EXPORT_SYMBOL(cvm_oct_transmit_qos);
diff --git a/drivers/net/ethernet/octeon/ethernet-xmit.c b/drivers/net/ethernet/octeon/ethernet-xmit.c
new file mode 100644
index 0000000..864c793
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-xmit.c
@@ -0,0 +1,395 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+
+#undef CVM_OCT_XMIT
+#undef CVM_OCT_PKO_LOCK_TYPE
+
+#ifdef CVM_OCT_LOCKLESS
+#define CVM_OCT_XMIT cvm_oct_xmit_lockless
+#define CVM_OCT_PKO_LOCK_TYPE CVMX_PKO_LOCK_NONE
+#else
+#define CVM_OCT_XMIT cvm_oct_xmit
+#define CVM_OCT_PKO_LOCK_TYPE CVMX_PKO_LOCK_CMD_QUEUE
+#endif
+
+/**
+ * cvm_oct_xmit - transmit a packet
+ * @skb:    Packet to send
+ * @dev:    Device info structure
+ *
+ * Returns Always returns NETDEV_TX_OK
+ */
+int
+CVM_OCT_XMIT
+(struct sk_buff *skb, struct net_device *dev)
+{
+	struct sk_buff *skb_tmp;
+	cvmx_pko_command_word0_t pko_command;
+	union cvmx_buf_ptr hw_buffer;
+	u64 old_scratch;
+	u64 old_scratch2;
+	int qos;
+	int i;
+	int frag_count;
+	enum {QUEUE_HW, QUEUE_WQE, QUEUE_DROP} queue_type;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	s32 queue_depth;
+	s32 buffers_to_free;
+	s32 buffers_being_recycled;
+	unsigned long flags;
+	cvmx_wqe_t *work = NULL;
+	bool timestamp_this_skb = false;
+
+	/* Prefetch the private data structure.  It is larger than one
+	 * cache line.
+	 */
+	prefetch(priv);
+
+	if (USE_ASYNC_IOBDMA) {
+		/* Save scratch in case userspace is using it */
+		CVMX_SYNCIOBDMA;
+		old_scratch = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
+		old_scratch2 = cvmx_scratch_read64(CVMX_SCR_SCRATCH + 8);
+
+		/* Fetch and increment the number of packets to be
+		 * freed.
+		 */
+		cvmx_fau_async_fetch_and_add32(CVMX_SCR_SCRATCH + 8,
+					       FAU_NUM_PACKET_BUFFERS_TO_FREE,
+					       0);
+	}
+
+#ifdef CVM_OCT_LOCKLESS
+	qos = cvmx_get_core_num();
+#else
+	/* The check on CVMX_PKO_QUEUES_PER_PORT_* is designed to
+	 * completely remove "qos" in the event neither interface
+	 * supports multiple queues per port.
+	 */
+	if (priv->tx_multiple_queues) {
+		qos = GET_SKBUFF_QOS(skb);
+		if (qos <= 0)
+			qos = 0;
+		else if (qos >= priv->num_tx_queues)
+			qos = 0;
+	} else
+		qos = 0;
+#endif
+	if (USE_ASYNC_IOBDMA) {
+		cvmx_fau_async_fetch_and_add32(CVMX_SCR_SCRATCH,
+					       priv->tx_queue[qos].fau, 1);
+	}
+
+	frag_count = 0;
+	if (skb_has_frag_list(skb))
+		skb_walk_frags(skb, skb_tmp)
+			frag_count++;
+	/* We have space for 12 segment pointers, If there will be
+	 * more than that, we must linearize.  The count is: 1 (base
+	 * SKB) + frag_count + nr_frags.
+	 */
+	if (unlikely(skb_shinfo(skb)->nr_frags + frag_count > 11)) {
+		if (unlikely(__skb_linearize(skb))) {
+			queue_type = QUEUE_DROP;
+			goto skip_xmit;
+		}
+		frag_count = 0;
+	}
+
+#ifndef CVM_OCT_LOCKLESS
+	/* The CN3XXX series of parts has an errata (GMX-401) which
+	 * causes the GMX block to hang if a collision occurs towards
+	 * the end of a <68 byte packet. As a workaround for this, we
+	 * pad packets to be 68 bytes whenever we are in half duplex
+	 * mode. We don't handle the case of having a small packet but
+	 * no room to add the padding.  The kernel should always give
+	 * us at least a cache line
+	 */
+	if ((skb->len < 64) && OCTEON_IS_MODEL(OCTEON_CN3XXX)) {
+		union cvmx_gmxx_prtx_cfg gmx_prt_cfg;
+
+		if (priv->interface < 2) {
+			/* We only need to pad packet in half duplex mode */
+			gmx_prt_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
+			if (gmx_prt_cfg.s.duplex == 0) {
+				int add_bytes = 64 - skb->len;
+				if ((skb_tail_pointer(skb) + add_bytes) <= skb_end_pointer(skb))
+					memset(__skb_put(skb, add_bytes), 0, add_bytes);
+			}
+		}
+	}
+#endif
+	/* Build the PKO command */
+	pko_command.u64 = 0;
+#ifdef __LITTLE_ENDIAN
+	pko_command.s.le = 1;
+#endif
+	/* Don't pollute L2 with the outgoing packet */
+	pko_command.s.n2 = 1;
+	pko_command.s.segs = 1;
+	pko_command.s.total_bytes = skb->len;
+	/* Use fau0 to decrement the number of packets queued */
+	pko_command.s.size0 = CVMX_FAU_OP_SIZE_32;
+	pko_command.s.subone0 = 1;
+	pko_command.s.reg0 = priv->tx_queue[qos].fau;
+	pko_command.s.dontfree = 1;
+
+	/* Build the PKO buffer pointer */
+	hw_buffer.u64 = 0; /* Implies pool == 0, i == 0 */
+	if (skb_shinfo(skb)->nr_frags == 0 && frag_count == 0) {
+		hw_buffer.s.addr = virt_to_phys(skb->data);
+		hw_buffer.s.size = skb->len;
+		cvm_oct_set_back(skb, &hw_buffer);
+		buffers_being_recycled = 1;
+	} else {
+		u64 *hw_buffer_list;
+		bool can_do_reuse = true;
+
+		work = cvmx_fpa_alloc(wqe_pool);
+		if (unlikely(!work)) {
+			netdev_err(dev, "Failed WQE allocate\n");
+			queue_type = QUEUE_DROP;
+			goto skip_xmit;
+		}
+		hw_buffer_list = (u64 *)work->packet_data;
+		hw_buffer.s.addr = virt_to_phys(skb->data);
+		hw_buffer.s.size = skb_headlen(skb);
+		if (skb_shinfo(skb)->nr_frags == 0 && cvm_oct_skb_ok_for_reuse(skb)) {
+			cvm_oct_set_back(skb, &hw_buffer);
+		} else {
+			hw_buffer.s.back = 0;
+			can_do_reuse = false;
+		}
+		hw_buffer_list[0] = hw_buffer.u64;
+		for (i = 1; i <= skb_shinfo(skb)->nr_frags; i++) {
+			struct skb_frag_struct *fs = skb_shinfo(skb)->frags + i - 1;
+			hw_buffer.s.addr = virt_to_phys((u8 *)page_address(fs->page.p) + fs->page_offset);
+			hw_buffer.s.size = fs->size;
+			hw_buffer_list[i] = hw_buffer.u64;
+			can_do_reuse = false;
+		}
+		skb_walk_frags(skb, skb_tmp) {
+			hw_buffer.s.addr = virt_to_phys(skb_tmp->data);
+			hw_buffer.s.size = skb_tmp->len;
+			if (cvm_oct_skb_ok_for_reuse(skb_tmp)) {
+				cvm_oct_set_back(skb_tmp, &hw_buffer);
+			} else {
+				hw_buffer.s.back = 0;
+				can_do_reuse = false;
+			}
+			hw_buffer_list[i] = hw_buffer.u64;
+			i++;
+		}
+		hw_buffer.s.addr = virt_to_phys(hw_buffer_list);
+		hw_buffer.s.size = i;
+		hw_buffer.s.back = 0;
+		hw_buffer.s.pool = wqe_pool;
+		buffers_being_recycled = i;
+		pko_command.s.segs = hw_buffer.s.size;
+		pko_command.s.gather = 1;
+		if (!can_do_reuse)
+			goto dont_put_skbuff_in_hw;
+	}
+
+	if (unlikely(priv->tx_timestamp_hw &&
+		     (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP))) {
+		timestamp_this_skb = true;
+		skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+		goto dont_put_skbuff_in_hw;
+	}
+	/* See if we can put this skb in the FPA pool. Any strange
+	 * behavior from the Linux networking stack will most likely
+	 * be caused by a bug in the following code. If some field is
+	 * in use by the network stack and get carried over when a
+	 * buffer is reused, bad thing may happen.  If in doubt and
+	 * you dont need the absolute best performance, disable the
+	 * define REUSE_SKBUFFS_WITHOUT_FREE. The reuse of buffers has
+	 * shown a 25% increase in performance under some loads.
+	 */
+#if REUSE_SKBUFFS_WITHOUT_FREE
+	if (!cvm_oct_skb_ok_for_reuse(skb))
+		goto dont_put_skbuff_in_hw;
+	if (unlikely(skb_header_cloned(skb)))
+		goto dont_put_skbuff_in_hw;
+	if (unlikely(skb->destructor))
+		goto dont_put_skbuff_in_hw;
+
+
+	/* We can use this buffer in the FPA.  We don't need the FAU
+	 * update anymore
+	 */
+	pko_command.s.dontfree = 0;
+
+#endif /* REUSE_SKBUFFS_WITHOUT_FREE */
+
+dont_put_skbuff_in_hw:
+
+	/* Check if we can use the hardware checksumming */
+	if (USE_HW_TCPUDP_CHECKSUM && (skb->protocol == htons(ETH_P_IP)) &&
+	    (ip_hdr(skb)->version == 4) && (ip_hdr(skb)->ihl == 5) &&
+	    ((ip_hdr(skb)->frag_off == 0) || (ip_hdr(skb)->frag_off == htons(1 << 14)))
+	    && ((ip_hdr(skb)->protocol == IPPROTO_TCP) || (ip_hdr(skb)->protocol == IPPROTO_UDP))) {
+		/* Use hardware checksum calc */
+		pko_command.s.ipoffp1 = sizeof(struct ethhdr) + 1;
+		if (unlikely(priv->imode == CVMX_HELPER_INTERFACE_MODE_SRIO))
+			pko_command.s.ipoffp1 += 8;
+	}
+
+	if (USE_ASYNC_IOBDMA) {
+		/* Get the number of skbuffs in use by the hardware */
+		CVMX_SYNCIOBDMA;
+		queue_depth = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
+		buffers_to_free = cvmx_scratch_read64(CVMX_SCR_SCRATCH + 8);
+	} else {
+		/* Get the number of skbuffs in use by the hardware */
+		queue_depth = cvmx_fau_fetch_and_add32(priv->tx_queue[qos].fau, 1);
+		buffers_to_free = cvmx_fau_fetch_and_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);
+	}
+
+	/* If we're sending faster than the receive can free them then
+	 * don't do the HW free.
+	 */
+	if (unlikely(buffers_to_free < -100))
+		pko_command.s.dontfree = 1;
+
+	/* Drop this packet if we have too many already queued to the HW */
+	if (unlikely(queue_depth >= MAX_OUT_QUEUE_DEPTH)) {
+		if (dev->tx_queue_len != 0) {
+			netif_stop_queue(dev);
+		} else {
+			/* If not using normal queueing.  */
+			queue_type = QUEUE_DROP;
+			goto skip_xmit;
+		}
+	}
+
+	if (pko_command.s.dontfree) {
+		queue_type = QUEUE_WQE;
+	} else {
+		queue_type = QUEUE_HW;
+		if (buffers_being_recycled > 1) {
+			struct sk_buff *tskb, *nskb;
+			/* We are committed to use hardware free, restore the
+			 * frag list to empty on the first SKB
+			 */
+			tskb = skb_shinfo(skb)->frag_list;
+			while (tskb) {
+				nskb = tskb->next;
+				cvm_oct_skb_prepare_for_reuse(tskb);
+				tskb = nskb;
+			}
+		}
+		cvm_oct_skb_prepare_for_reuse(skb);
+	}
+
+	if (queue_type == QUEUE_WQE) {
+		if (!work) {
+			work = cvmx_fpa_alloc(wqe_pool);
+			if (unlikely(!work)) {
+				netdev_err(dev, "Failed WQE allocate\n");
+				queue_type = QUEUE_DROP;
+				goto skip_xmit;
+			}
+		}
+
+		pko_command.s.rsp = 1;
+		pko_command.s.wqp = 1;
+		/* work->unused will carry the qos for this packet,
+		 * this allows us to find the proper FAU when freeing
+		 * the packet.  We decrement the FAU when the WQE is
+		 * replaced in the pool.
+		 */
+		pko_command.s.reg0 = 0;
+		work->word0.u64 = 0;
+		work->word0.raw.unused = (u8)qos;
+
+		work->word1.u64 = 0;
+		work->word1.tag_type = CVMX_POW_TAG_TYPE_NULL;
+		work->word1.tag = 0;
+		work->word2.u64 = 0;
+		work->word2.s.software = 1;
+		cvmx_wqe_set_grp(work, pow_receive_group);
+		work->packet_ptr.u64 = (unsigned long)skb;
+	}
+
+	local_irq_save(flags);
+
+	cvmx_pko_send_packet_prepare_pkoid(priv->pko_port,
+					   priv->tx_queue[qos].queue,
+					   CVM_OCT_PKO_LOCK_TYPE);
+
+	/* Send the packet to the output queue */
+	if (queue_type == QUEUE_WQE) {
+		u64 word2 = virt_to_phys(work);
+		if (timestamp_this_skb)
+			word2 |= 1ull << 40; /* Bit 40 controls timestamps */
+
+		if (unlikely(cvmx_pko_send_packet_finish3_pkoid(priv->pko_port,
+							  priv->tx_queue[qos].queue, pko_command, hw_buffer,
+							  word2, CVM_OCT_PKO_LOCK_TYPE))) {
+				queue_type = QUEUE_DROP;
+				netdev_err(dev, "Failed to send the packet with wqe\n");
+		}
+	} else {
+		if (unlikely(cvmx_pko_send_packet_finish_pkoid(priv->pko_port,
+							 priv->tx_queue[qos].queue,
+							 pko_command, hw_buffer,
+							 CVM_OCT_PKO_LOCK_TYPE))) {
+			netdev_err(dev, "Failed to send the packet\n");
+			queue_type = QUEUE_DROP;
+		}
+	}
+	local_irq_restore(flags);
+
+skip_xmit:
+	switch (queue_type) {
+	case QUEUE_DROP:
+		cvmx_fau_atomic_add32(priv->tx_queue[qos].fau, -1);
+		dev_kfree_skb_any(skb);
+		dev->stats.tx_dropped++;
+		if (work)
+			cvmx_fpa_free(work, wqe_pool, DONT_WRITEBACK(1));
+		break;
+	case QUEUE_HW:
+		cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, -buffers_being_recycled);
+		break;
+	case QUEUE_WQE:
+		/* Cleanup is done on the RX path when the WQE returns */
+		break;
+	default:
+		BUG();
+	}
+
+	if (USE_ASYNC_IOBDMA) {
+		CVMX_SYNCIOBDMA;
+		/* Restore the scratch area */
+		cvmx_scratch_write64(CVMX_SCR_SCRATCH, old_scratch);
+		cvmx_scratch_write64(CVMX_SCR_SCRATCH + 8, old_scratch2);
+	}
+
+	return NETDEV_TX_OK;
+}
diff --git a/drivers/net/ethernet/octeon/ethernet.c b/drivers/net/ethernet/octeon/ethernet.c
new file mode 100644
index 0000000..840f582
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet.c
@@ -0,0 +1,1221 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+#include <linux/platform_device.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/phy.h>
+#include <linux/slab.h>
+#include <linux/of_net.h>
+#include <linux/interrupt.h>
+
+#include <net/dst.h>
+
+#include <asm/octeon/octeon.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+
+#include <asm/octeon/cvmx-pip.h>
+#include <asm/octeon/cvmx-pko.h>
+#include <asm/octeon/cvmx-fau.h>
+#include <asm/octeon/cvmx-ipd.h>
+#include <asm/octeon/cvmx-srio.h>
+#include <asm/octeon/cvmx-helper.h>
+#include <asm/octeon/cvmx-helper-util.h>
+#include <asm/octeon/cvmx-pko-internal-ports-range.h>
+#include <asm/octeon/cvmx-app-config.h>
+
+#include <asm/octeon/cvmx-gmxx-defs.h>
+#include <asm/octeon/cvmx-smix-defs.h>
+
+int rx_cpu_factor = 8;
+module_param(rx_cpu_factor, int, S_IRUGO | S_IWUSR | S_IWGRP);
+MODULE_PARM_DESC(rx_cpu_factor, "Control how many CPUs are used for packet reception.\n"
+		 "\tLarger numbers result in fewer CPUs used.");
+
+int num_packet_buffers = 1024;
+module_param(num_packet_buffers, int, 0444);
+MODULE_PARM_DESC(num_packet_buffers, "\n"
+	"\tNumber of packet buffers to allocate and store in the\n"
+	"\tFPA. By default, 1024 packet buffers are used.");
+
+int pow_receive_group = 15;
+module_param(pow_receive_group, int, 0444);
+MODULE_PARM_DESC(pow_receive_group, "\n"
+	"\tPOW group to receive packets from. All ethernet hardware\n"
+	"\twill be configured to send incomming packets to this POW\n"
+	"\tgroup. Also any other software can submit packets to this\n"
+	"\tgroup for the kernel to process.");
+
+static int disable_core_queueing = 1;
+module_param(disable_core_queueing, int, S_IRUGO);
+MODULE_PARM_DESC(disable_core_queueing, "\n"
+		"\t\tWhen set the networking core's tx_queue_len is set to zero.  This\n"
+		"\t\tallows packets to be sent without lock contention in the packet scheduler\n"
+		"\t\tresulting in some cases in improved throughput.");
+
+int max_rx_cpus = -1;
+module_param(max_rx_cpus, int, 0444);
+MODULE_PARM_DESC(max_rx_cpus, "\n"
+	"\t\tThe maximum number of CPUs to use for packet reception.\n"
+	"\t\tUse -1 to use all available CPUs.");
+
+int rx_napi_weight = 32;
+module_param(rx_napi_weight, int, 0444);
+MODULE_PARM_DESC(rx_napi_weight, "The NAPI WEIGHT parameter.");
+
+static int disable_lockless_pko;
+module_param(disable_lockless_pko, int, S_IRUGO);
+MODULE_PARM_DESC(disable_lockless_pko, "Disable lockless PKO access (use locking for queues instead).");
+
+/* internal ports count for each port in a interface */
+int iport_count = 1;
+/* pko queue count for each port in a interface */
+int queues_count = 1;
+/* packet pool */
+int packet_pool = 0;
+/* wqe pool */
+int wqe_pool = -1;
+/* output pool */
+int output_pool = -1;
+
+/**
+ * cvm_oct_poll_queue - Workqueue for polling operations.
+ */
+struct workqueue_struct *cvm_oct_poll_queue;
+
+/**
+ * cvm_oct_poll_queue_stopping - flag to indicate polling should stop.
+ *
+ * Set to one right before cvm_oct_poll_queue is destroyed.
+ */
+atomic_t cvm_oct_poll_queue_stopping = ATOMIC_INIT(0);
+
+/* cvm_oct_by_pkind is an array of every ethernet device owned by this
+ * driver indexed by the IPD pkind/port_number.  If an entry is empty
+ * (NULL) it either doesn't exist, or there was a collision.  The two
+ * cases can be distinguished by trying to look up via
+ * cvm_oct_dev_for_port();
+ */
+struct octeon_ethernet *cvm_oct_by_pkind[64] __cacheline_aligned;
+
+/*
+ * cvm_oct_by_srio_mbox is indexed by the SRIO mailbox.
+ */
+struct octeon_ethernet *cvm_oct_by_srio_mbox[4][4];
+
+/* cvm_oct_list is a list of all cvm_oct_private_t created by this driver. */
+LIST_HEAD(cvm_oct_list);
+
+static void cvm_oct_rx_refill_worker(struct work_struct *work);
+static DECLARE_DELAYED_WORK(cvm_oct_rx_refill_work, cvm_oct_rx_refill_worker);
+
+static void cvm_oct_rx_refill_worker(struct work_struct *work)
+{
+	/* FPA 0 may have been drained, try to refill it if we need
+	 * more than num_packet_buffers / 2, otherwise normal receive
+	 * processing will refill it.  If it were drained, no packets
+	 * could be received so cvm_oct_napi_poll would never be
+	 * invoked to do the refill.
+	 */
+	cvm_oct_rx_refill_pool(num_packet_buffers / 2);
+
+	if (!atomic_read(&cvm_oct_poll_queue_stopping))
+		queue_delayed_work(cvm_oct_poll_queue,
+				   &cvm_oct_rx_refill_work, HZ);
+}
+
+static void cvm_oct_periodic_worker(struct work_struct *work)
+{
+	struct octeon_ethernet *priv = container_of(work,
+						    struct octeon_ethernet,
+						    port_periodic_work.work);
+	void (*poll_fn) (struct net_device *);
+
+	spin_lock(&priv->poll_lock);
+	poll_fn = priv->poll;
+	spin_unlock(&priv->poll_lock);
+
+	if (poll_fn)
+		poll_fn(priv->netdev);
+
+	priv->netdev->netdev_ops->ndo_get_stats(priv->netdev);
+
+	if (!atomic_read(&cvm_oct_poll_queue_stopping))
+		queue_delayed_work(cvm_oct_poll_queue, &priv->port_periodic_work, HZ);
+}
+
+static int cvm_oct_num_output_buffers;
+
+static int cvm_oct_get_total_pko_queues(void)
+{
+	if (OCTEON_IS_MODEL(OCTEON_CN38XX))
+		return 128;
+	else if (OCTEON_IS_MODEL(OCTEON_CN3XXX))
+		return 32;
+	else if (OCTEON_IS_MODEL(OCTEON_CN50XX))
+		return 32;
+	else
+		return 256;
+}
+
+static bool cvm_oct_pko_lockless(void)
+{
+	int interface, num_interfaces;
+	int queues = 0;
+
+	if (disable_lockless_pko)
+		return false;
+
+	/* CN3XXX require workarounds in xmit.  Disable lockless for
+	 * CN3XXX to optimize the lockless case with out the
+	 * workarounds.
+	 */
+	if (OCTEON_IS_MODEL(OCTEON_CN3XXX))
+		return false;
+
+	num_interfaces = cvmx_helper_get_number_of_interfaces();
+	for (interface = 0; interface < num_interfaces; interface++) {
+		int num_ports, port;
+		cvmx_helper_interface_mode_t imode = cvmx_helper_interface_get_mode(interface);
+
+		num_ports = cvmx_helper_interface_enumerate(interface);
+		for (port = 0; port < num_ports; port++) {
+			if (!cvmx_helper_is_port_valid(interface, port))
+				continue;
+			switch (imode) {
+			case CVMX_HELPER_INTERFACE_MODE_XAUI:
+			case CVMX_HELPER_INTERFACE_MODE_RXAUI:
+			case CVMX_HELPER_INTERFACE_MODE_SGMII:
+			case CVMX_HELPER_INTERFACE_MODE_RGMII:
+			case CVMX_HELPER_INTERFACE_MODE_GMII:
+			case CVMX_HELPER_INTERFACE_MODE_SPI:
+				queues += num_possible_cpus();
+				break;
+			case CVMX_HELPER_INTERFACE_MODE_NPI:
+			case CVMX_HELPER_INTERFACE_MODE_LOOP:
+#ifdef CONFIG_RAPIDIO
+			case CVMX_HELPER_INTERFACE_MODE_SRIO:
+#endif
+				queues += 1;
+				break;
+			default:
+				break;
+			}
+		}
+	}
+	return queues <= cvm_oct_get_total_pko_queues();
+}
+
+static void cvm_oct_set_pko_multiqueue(void)
+{
+	int interface, num_interfaces, rv;
+
+	num_interfaces = cvmx_helper_get_number_of_interfaces();
+	for (interface = 0; interface < num_interfaces; interface++) {
+		int num_ports, port;
+		cvmx_helper_interface_mode_t imode = cvmx_helper_interface_get_mode(interface);
+
+		num_ports = cvmx_helper_interface_enumerate(interface);
+		for (port = 0; port < num_ports; port++) {
+			if (!cvmx_helper_is_port_valid(interface, port))
+				continue;
+			switch (imode) {
+			case CVMX_HELPER_INTERFACE_MODE_XAUI:
+			case CVMX_HELPER_INTERFACE_MODE_RXAUI:
+			case CVMX_HELPER_INTERFACE_MODE_SGMII:
+			case CVMX_HELPER_INTERFACE_MODE_RGMII:
+			case CVMX_HELPER_INTERFACE_MODE_GMII:
+			case CVMX_HELPER_INTERFACE_MODE_SPI:
+				rv = cvmx_pko_alloc_iport_and_queues(interface, port, 1,
+								     num_possible_cpus());
+				WARN(rv, "cvmx_pko_alloc_iport_and_queues failed");
+				if (rv)
+					return;
+				break;
+			default:
+				break;
+			}
+		}
+	}
+}
+
+static int cvm_oct_configure_common_hw(void)
+{
+	/* Setup the FPA */
+	cvmx_fpa_enable();
+
+	/* allocate packet pool */
+	packet_pool = cvm_oct_alloc_fpa_pool(packet_pool, FPA_PACKET_POOL_SIZE);
+	if (packet_pool < 0) {
+		pr_err("cvm_oct_alloc_fpa_pool(%d, FPA_PACKET_POOL_SIZE) failed.\n", packet_pool);
+		return -ENOMEM;
+	}
+	cvm_oct_mem_fill_fpa(packet_pool, num_packet_buffers);
+
+	/* communicate packet pool number to ipd */
+	cvmx_ipd_set_packet_pool_config(packet_pool, FPA_PACKET_POOL_SIZE,
+					num_packet_buffers);
+
+	/* allocate wqe pool */
+	wqe_pool = cvm_oct_alloc_fpa_pool(-1, FPA_WQE_POOL_SIZE);
+	if (wqe_pool < 0) {
+		pr_err("cvm_oct_alloc_fpa_pool(-1, FPA_WQE_POOL_SIZE) failed.\n");
+		return -ENOMEM;;
+	}
+	cvm_oct_mem_fill_fpa(wqe_pool, num_packet_buffers);
+
+	/* communicate wqe pool to ipd */
+	cvmx_ipd_set_wqe_pool_config(wqe_pool, FPA_WQE_POOL_SIZE,
+				     num_packet_buffers);
+
+	if (cvm_oct_pko_lockless()) {
+		cvm_oct_set_pko_multiqueue();
+		cvm_oct_num_output_buffers = 4 * cvm_oct_get_total_pko_queues();
+	} else {
+		cvm_oct_num_output_buffers = 128;
+	}
+
+	/* alloc fpa pool for output buffers */
+	output_pool = cvm_oct_alloc_fpa_pool(-1, FPA_OUTPUT_BUFFER_POOL_SIZE);
+	if (output_pool < 0) {
+		pr_err("cvm_oct_alloc_fpa_pool(-1, FPA_OUTPUT_BUFFER_POOL_SIZE) failed.\n");
+		return -ENOMEM;;
+	}
+	cvm_oct_mem_fill_fpa(output_pool, cvm_oct_num_output_buffers);
+
+	/* communicate output pool no. to pko */
+	cvmx_pko_set_cmd_que_pool_config(output_pool,
+					 FPA_OUTPUT_BUFFER_POOL_SIZE,
+					 cvm_oct_num_output_buffers);
+
+	/* more configuration needs to be done, so enable ipd seperately */
+	cvmx_ipd_cfg.ipd_enable = 0;
+
+	__cvmx_export_app_config_to_named_block(CVMX_APP_CONFIG);
+
+	cvmx_helper_initialize_packet_io_global();
+
+#ifdef __LITTLE_ENDIAN
+	{
+		union cvmx_ipd_ctl_status ipd_ctl_status;
+		ipd_ctl_status.u64 = cvmx_read_csr(CVMX_IPD_CTL_STATUS);
+		ipd_ctl_status.s.pkt_lend = 1;
+		ipd_ctl_status.s.wqe_lend = 1;
+		cvmx_write_csr(CVMX_IPD_CTL_STATUS, ipd_ctl_status.u64);
+	}
+#endif
+	/* Enable red after interface is initialized */
+	if (USE_RED)
+		cvmx_helper_setup_red(num_packet_buffers / 4,
+				      num_packet_buffers / 8);
+
+	return 0;
+}
+
+/**
+ * cvm_oct_register_callback -  Register a intercept callback for the named device.
+ *
+ * It returns the net_device structure for the ethernet port. Usign a
+ * callback of NULL will remove the callback. Note that this callback
+ * must not disturb scratch. It will be called with SYNCIOBDMAs in
+ * progress and userspace may be using scratch. It also must not
+ * disturb the group mask.
+ *
+ * @device_name: Device name to register for. (Example: "eth0")
+ * @callback: Intercept callback to set.
+ *
+ * Returns the net_device structure for the ethernet port or NULL on failure.
+ */
+struct net_device *cvm_oct_register_callback(const char *device_name, cvm_oct_callback_t callback)
+{
+	struct octeon_ethernet *priv;
+
+	list_for_each_entry(priv, &cvm_oct_list, list) {
+		if (strcmp(device_name, priv->netdev->name) == 0) {
+			priv->intercept_cb = callback;
+			wmb();
+			return priv->netdev;
+		}
+	}
+	return NULL;
+}
+EXPORT_SYMBOL(cvm_oct_register_callback);
+
+/**
+ * cvm_oct_free_work- Free a work queue entry
+ *
+ * @work_queue_entry: Work queue entry to free
+ *
+ * Returns Zero on success, Negative on failure.
+ */
+int cvm_oct_free_work(void *work_queue_entry)
+{
+	cvmx_wqe_t *work = work_queue_entry;
+
+	int segments = work->word2.s.bufs;
+	union cvmx_buf_ptr segment_ptr = work->packet_ptr;
+
+	while (segments--) {
+		union cvmx_buf_ptr next_ptr = *(union cvmx_buf_ptr *)phys_to_virt(segment_ptr.s.addr - 8);
+		if (!segment_ptr.s.i)
+			cvmx_fpa_free(cvm_oct_get_buffer_ptr(segment_ptr),
+				      segment_ptr.s.pool,
+				      DONT_WRITEBACK(FPA_PACKET_POOL_SIZE / 128));
+		segment_ptr = next_ptr;
+	}
+	cvmx_fpa_free(work, wqe_pool, DONT_WRITEBACK(1));
+
+	return 0;
+}
+EXPORT_SYMBOL(cvm_oct_free_work);
+
+/* Lock to protect racy cvmx_pko_get_port_status() */
+static DEFINE_SPINLOCK(cvm_oct_tx_stat_lock);
+
+/**
+ * cvm_oct_common_get_stats - get the low level ethernet statistics
+ * @dev:    Device to get the statistics from
+ *
+ * Returns Pointer to the statistics
+ */
+static struct net_device_stats *cvm_oct_common_get_stats(struct net_device *dev)
+{
+	unsigned long flags;
+	cvmx_pip_port_status_t rx_status;
+	cvmx_pko_port_status_t tx_status;
+	u64 current_tx_octets;
+	u32 current_tx_packets;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	if (octeon_is_simulation()) {
+		/* The simulator doesn't support statistics */
+		memset(&rx_status, 0, sizeof(rx_status));
+		memset(&tx_status, 0, sizeof(tx_status));
+	} else {
+		cvmx_pip_get_port_status(priv->ipd_port, 1, &rx_status);
+
+		spin_lock_irqsave(&cvm_oct_tx_stat_lock, flags);
+		cvmx_pko_get_port_status(priv->ipd_port, 0, &tx_status);
+		current_tx_packets = tx_status.packets;
+		current_tx_octets = tx_status.octets;
+		/* The tx_packets counter is 32-bits as are all these
+		 * variables.  No truncation necessary.
+		 */
+		tx_status.packets = current_tx_packets - priv->last_tx_packets;
+		/* The tx_octets counter is only 48-bits, so we need
+		 * to truncate in case there was a wrap-around
+		 */
+		tx_status.octets = (current_tx_octets - priv->last_tx_octets) & 0xffffffffffffull;
+		priv->last_tx_packets = current_tx_packets;
+		priv->last_tx_octets = current_tx_octets;
+		spin_unlock_irqrestore(&cvm_oct_tx_stat_lock, flags);
+	}
+
+	dev->stats.rx_packets += rx_status.inb_packets;
+	dev->stats.tx_packets += tx_status.packets;
+	dev->stats.rx_bytes += rx_status.inb_octets;
+	dev->stats.tx_bytes += tx_status.octets;
+	dev->stats.multicast += rx_status.multicast_packets;
+	dev->stats.rx_crc_errors += rx_status.inb_errors;
+	dev->stats.rx_frame_errors += rx_status.fcs_align_err_packets;
+
+	/* The drop counter must be incremented atomically since the
+	 * RX tasklet also increments it.
+	 */
+	atomic64_add(rx_status.dropped_packets,
+		     (atomic64_t *)&dev->stats.rx_dropped);
+
+	return &dev->stats;
+}
+
+/**
+ * cvm_oct_common_change_mtu - change the link MTU
+ * @dev:     Device to change
+ * @new_mtu: The new MTU
+ *
+ * Returns Zero on success
+ */
+static int cvm_oct_common_change_mtu(struct net_device *dev, int new_mtu)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+#if IS_ENABLED(CONFIG_VLAN_8021Q)
+	int vlan_bytes = 4;
+#else
+	int vlan_bytes = 0;
+#endif
+
+	/* Limit the MTU to make sure the ethernet packets are between
+	 * 64 bytes and 65535 bytes.
+	 */
+	if ((new_mtu + 14 + 4 + vlan_bytes < 64)
+	    || (new_mtu + 14 + 4 + vlan_bytes > 65392)) {
+		netdev_err(dev, "MTU must be between %d and %d.\n",
+			   64 - 14 - 4 - vlan_bytes,
+			   65392 - 14 - 4 - vlan_bytes);
+		return -EINVAL;
+	}
+	dev->mtu = new_mtu;
+
+	if (priv->has_gmx_regs) {
+		/* Add ethernet header and FCS, and VLAN if configured. */
+		int max_packet = new_mtu + 14 + 4 + vlan_bytes;
+
+		if (OCTEON_IS_MODEL(OCTEON_CN3XXX)
+		    || OCTEON_IS_MODEL(OCTEON_CN58XX)) {
+			/* Signal errors on packets larger than the MTU */
+			cvmx_write_csr(CVMX_GMXX_RXX_FRM_MAX(priv->interface_port, priv->interface),
+				       max_packet);
+		} else {
+			union cvmx_pip_prt_cfgx port_cfg;
+
+			port_cfg.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(priv->ipd_pkind));
+			if (port_cfg.s.maxerr_en) {
+				/* Disable the PIP check as it can
+				 * only be controlled over a group of
+				 * ports, let the check be done in the
+				 * GMX instead.
+				 */
+				port_cfg.s.maxerr_en = 0;
+				cvmx_write_csr(CVMX_PIP_PRT_CFGX(priv->ipd_pkind), port_cfg.u64);
+			}
+		}
+		/* Set the hardware to truncate packets larger than
+		 * the MTU. The jabber register must be set to a
+		 * multiple of 8 bytes, so round up.
+		 */
+		cvmx_write_csr(CVMX_GMXX_RXX_JABBER(priv->interface_port, priv->interface),
+			       (max_packet + 7) & ~7u);
+	}
+	return 0;
+}
+
+/**
+ * cvm_oct_common_set_multicast_list - set the multicast list
+ * @dev:    Device to work on
+ */
+static void cvm_oct_common_set_multicast_list(struct net_device *dev)
+{
+	union cvmx_gmxx_prtx_cfg gmx_cfg;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	if (priv->has_gmx_regs) {
+		union cvmx_gmxx_rxx_adr_ctl control;
+		control.u64 = 0;
+		control.s.bcst = 1;	/* Allow broadcast MAC addresses */
+
+		if (!netdev_mc_empty(dev) || (dev->flags & IFF_ALLMULTI) ||
+		    (dev->flags & IFF_PROMISC))
+			/* Force accept multicast packets */
+			control.s.mcst = 2;
+		else
+			/* Force reject multicat packets */
+			control.s.mcst = 1;
+
+		if (dev->flags & IFF_PROMISC)
+			/* Reject matches if promisc. Since CAM is
+			 * shut off, should accept everything.
+			 */
+			control.s.cam_mode = 0;
+		else
+			/* Filter packets based on the CAM */
+			control.s.cam_mode = 1;
+
+		gmx_cfg.u64 =
+		    cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
+		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface),
+			       gmx_cfg.u64 & ~1ull);
+
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CTL(priv->interface_port, priv->interface),
+			       control.u64);
+		if (dev->flags & IFF_PROMISC)
+			cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM_EN(priv->interface_port, priv->interface), 0);
+		else
+			cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM_EN(priv->interface_port, priv->interface), 1);
+
+		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface),
+			       gmx_cfg.u64);
+	}
+}
+
+/**
+ * cvm_oct_common_set_mac_address - set the hardware MAC address for a device
+ * @dev:    The device in question.
+ * @addr:   Address structure to change it too.
+ *
+ * Returns Zero on success
+ */
+static int cvm_oct_set_mac_filter(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	union cvmx_gmxx_prtx_cfg gmx_cfg;
+
+	if (priv->has_gmx_regs) {
+		int i;
+		u8 *ptr = dev->dev_addr;
+		u64 mac = 0;
+		for (i = 0; i < 6; i++)
+			mac = (mac << 8) | (u64)ptr[i];
+
+		gmx_cfg.u64 =
+		    cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
+		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface),
+			       gmx_cfg.u64 & ~1ull);
+
+		cvmx_write_csr(CVMX_GMXX_SMACX(priv->interface_port, priv->interface), mac);
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM0(priv->interface_port, priv->interface),
+			       ptr[0]);
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM1(priv->interface_port, priv->interface),
+			       ptr[1]);
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM2(priv->interface_port, priv->interface),
+			       ptr[2]);
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM3(priv->interface_port, priv->interface),
+			       ptr[3]);
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM4(priv->interface_port, priv->interface),
+			       ptr[4]);
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM5(priv->interface_port, priv->interface),
+			       ptr[5]);
+		cvm_oct_common_set_multicast_list(dev);
+		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface),
+			       gmx_cfg.u64);
+	}
+	return 0;
+}
+
+static int cvm_oct_common_set_mac_address(struct net_device *dev, void *addr)
+{
+	int r = eth_mac_addr(dev, addr);
+
+	if (r)
+		return r;
+	return cvm_oct_set_mac_filter(dev);
+}
+
+/**
+ * cvm_oct_common_init - per network device initialization
+ * @dev:    Device to initialize
+ *
+ * Returns Zero on success
+ */
+int cvm_oct_common_init(struct net_device *dev)
+{
+	unsigned long flags;
+	cvmx_pko_port_status_t tx_status;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	const u8 *mac = NULL;
+
+	if (priv->of_node)
+		mac = of_get_mac_address(priv->of_node);
+
+	if (mac && is_valid_ether_addr(mac)) {
+		memcpy(dev->dev_addr, mac, ETH_ALEN);
+		dev->addr_assign_type &= ~NET_ADDR_RANDOM;
+	} else {
+		eth_hw_addr_random(dev);
+	}
+
+	if (priv->num_tx_queues != -1) {
+		dev->features |= NETIF_F_SG | NETIF_F_FRAGLIST;
+		if (USE_HW_TCPUDP_CHECKSUM)
+			dev->features |= NETIF_F_IP_CSUM;
+	}
+
+	/* We do our own locking, Linux doesn't need to */
+	dev->features |= NETIF_F_LLTX;
+	SET_ETHTOOL_OPS(dev, &cvm_oct_ethtool_ops);
+
+	cvm_oct_set_mac_filter(dev);
+	dev->netdev_ops->ndo_change_mtu(dev, dev->mtu);
+
+	spin_lock_irqsave(&cvm_oct_tx_stat_lock, flags);
+	cvmx_pko_get_port_status(priv->ipd_port, 0, &tx_status);
+	priv->last_tx_packets = tx_status.packets;
+	priv->last_tx_octets = tx_status.octets;
+	/* Zero out stats for port so we won't mistakenly show
+	 * counters from the bootloader.
+	 */
+	memset(&dev->stats, 0, sizeof(struct net_device_stats));
+	spin_unlock_irqrestore(&cvm_oct_tx_stat_lock, flags);
+
+	return 0;
+}
+
+static const struct net_device_ops cvm_oct_npi_netdev_ops = {
+	.ndo_init		= cvm_oct_common_init,
+	.ndo_start_xmit		= cvm_oct_xmit,
+	.ndo_set_rx_mode	= cvm_oct_common_set_multicast_list,
+	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_common_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+
+/* SGMII and XAUI handled the same so they both use this. */
+static const struct net_device_ops cvm_oct_sgmii_netdev_ops = {
+	.ndo_init		= cvm_oct_sgmii_init,
+	.ndo_uninit		= cvm_oct_sgmii_uninit,
+	.ndo_open		= cvm_oct_sgmii_open,
+	.ndo_stop		= cvm_oct_sgmii_stop,
+	.ndo_start_xmit		= cvm_oct_xmit,
+	.ndo_set_rx_mode	= cvm_oct_common_set_multicast_list,
+	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_common_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+static const struct net_device_ops cvm_oct_sgmii_lockless_netdev_ops = {
+	.ndo_init		= cvm_oct_sgmii_init,
+	.ndo_uninit		= cvm_oct_sgmii_uninit,
+	.ndo_open		= cvm_oct_sgmii_open,
+	.ndo_stop		= cvm_oct_sgmii_stop,
+	.ndo_start_xmit		= cvm_oct_xmit_lockless,
+	.ndo_set_rx_mode	= cvm_oct_common_set_multicast_list,
+	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_common_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+static const struct net_device_ops cvm_oct_spi_netdev_ops = {
+	.ndo_init		= cvm_oct_spi_init,
+	.ndo_uninit		= cvm_oct_spi_uninit,
+	.ndo_open		= cvm_oct_phy_setup_device,
+	.ndo_start_xmit		= cvm_oct_xmit,
+	.ndo_set_rx_mode	= cvm_oct_common_set_multicast_list,
+	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_common_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+static const struct net_device_ops cvm_oct_spi_lockless_netdev_ops = {
+	.ndo_init		= cvm_oct_spi_init,
+	.ndo_uninit		= cvm_oct_spi_uninit,
+	.ndo_open		= cvm_oct_phy_setup_device,
+	.ndo_start_xmit		= cvm_oct_xmit_lockless,
+	.ndo_set_rx_mode	= cvm_oct_common_set_multicast_list,
+	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_common_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+static const struct net_device_ops cvm_oct_rgmii_netdev_ops = {
+	.ndo_init		= cvm_oct_rgmii_init,
+	.ndo_open		= cvm_oct_rgmii_open,
+	.ndo_stop		= cvm_oct_rgmii_stop,
+	.ndo_start_xmit		= cvm_oct_xmit,
+	.ndo_set_rx_mode	= cvm_oct_common_set_multicast_list,
+	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_common_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+static const struct net_device_ops cvm_oct_rgmii_lockless_netdev_ops = {
+	.ndo_init		= cvm_oct_rgmii_init,
+	.ndo_open		= cvm_oct_rgmii_open,
+	.ndo_stop		= cvm_oct_rgmii_stop,
+	.ndo_start_xmit		= cvm_oct_xmit_lockless,
+	.ndo_set_rx_mode	= cvm_oct_common_set_multicast_list,
+	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_common_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+#ifdef CONFIG_RAPIDIO
+static const struct net_device_ops cvm_oct_srio_netdev_ops = {
+	.ndo_init		= cvm_oct_srio_init,
+	.ndo_open		= cvm_oct_srio_open,
+	.ndo_stop		= cvm_oct_srio_stop,
+	.ndo_start_xmit		= cvm_oct_xmit_srio,
+	.ndo_set_mac_address	= cvm_oct_srio_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_srio_change_mtu,
+	.ndo_get_stats		= cvm_oct_srio_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+#endif
+
+extern void octeon_mdiobus_force_mod_depencency(void);
+
+static int num_devices_extra_wqe;
+#define PER_DEVICE_EXTRA_WQE (MAX_OUT_QUEUE_DEPTH)
+
+static struct rb_root cvm_oct_ipd_tree = RB_ROOT;
+
+void cvm_oct_add_ipd_port(struct octeon_ethernet *port)
+{
+	struct rb_node **link = &cvm_oct_ipd_tree.rb_node;
+	struct rb_node *parent = NULL;
+	struct octeon_ethernet *n;
+	int value = port->key;
+
+	while (*link) {
+		parent = *link;
+		n = rb_entry(parent, struct octeon_ethernet, ipd_tree);
+
+		if (value < n->key)
+			link = &(*link)->rb_left;
+		else if (value > n->key)
+			link = &(*link)->rb_right;
+		else
+			BUG();
+	}
+	rb_link_node(&port->ipd_tree, parent, link);
+	rb_insert_color(&port->ipd_tree, &cvm_oct_ipd_tree);
+}
+
+struct octeon_ethernet *cvm_oct_dev_for_port(int port_number)
+{
+	struct rb_node *n = cvm_oct_ipd_tree.rb_node;
+	while (n) {
+		struct octeon_ethernet *s = rb_entry(n, struct octeon_ethernet, ipd_tree);
+
+		if (s->key > port_number)
+			n = n->rb_left;
+		else if (s->key < port_number)
+			n = n->rb_left;
+		else
+			return s;
+	}
+	return NULL;
+}
+
+static struct device_node *cvm_oct_of_get_child(const struct device_node *parent,
+						int reg_val)
+{
+	struct device_node *node = NULL;
+	int size;
+	const __be32 *addr;
+
+	for (;;) {
+		node = of_get_next_child(parent, node);
+		if (!node)
+			break;
+		addr = of_get_property(node, "reg", &size);
+		if (addr && (be32_to_cpu(*addr) == reg_val))
+			break;
+	}
+	return node;
+}
+
+static struct device_node *cvm_oct_node_for_port(struct device_node *pip,
+						 int interface, int port)
+{
+	struct device_node *ni, *np;
+
+	ni = cvm_oct_of_get_child(pip, interface);
+	if (!ni)
+		return NULL;
+
+	np = cvm_oct_of_get_child(ni, port);
+	of_node_put(ni);
+
+	return np;
+}
+
+static int __init cvm_oct_get_port_status(struct device_node *pip)
+{
+	int i, j;
+	int num_interfaces = cvmx_helper_get_number_of_interfaces();
+
+	for (i = 0; i < num_interfaces; i++) {
+		int num_ports = cvmx_helper_interface_enumerate(i);
+		int mode = cvmx_helper_interface_get_mode(i);
+
+		for (j = 0; j < num_ports; j++) {
+			if (mode == CVMX_HELPER_INTERFACE_MODE_RGMII
+			    || mode == CVMX_HELPER_INTERFACE_MODE_GMII
+			    || mode == CVMX_HELPER_INTERFACE_MODE_XAUI
+			    || mode == CVMX_HELPER_INTERFACE_MODE_RXAUI
+			    || mode == CVMX_HELPER_INTERFACE_MODE_SGMII
+			    || mode == CVMX_HELPER_INTERFACE_MODE_SPI) {
+				if (cvm_oct_node_for_port(pip, i, j) != NULL)
+					cvmx_helper_set_port_valid(i, j, true);
+				else
+					cvmx_helper_set_port_valid(i, j, false);
+			} else
+				cvmx_helper_set_port_valid(i, j, true);
+		}
+	}
+	return 0;
+}
+
+static int cvm_oct_probe(struct platform_device *pdev)
+{
+	int num_interfaces;
+	int interface;
+	int fau = FAU_NUM_PACKET_BUFFERS_TO_FREE;
+	int qos, r;
+	struct device_node *pip;
+
+	octeon_mdiobus_force_mod_depencency();
+	pr_notice("octeon-ethernet %s\n", OCTEON_ETHERNET_VERSION);
+
+	pip = pdev->dev.of_node;
+	if (!pip) {
+		dev_err(&pdev->dev, "No of_node.\n");
+		return -EINVAL;
+	}
+
+	cvm_oct_get_port_status(pip);
+
+	cvm_oct_poll_queue = create_singlethread_workqueue("octeon-ethernet");
+	if (cvm_oct_poll_queue == NULL) {
+		dev_err(&pdev->dev, "Cannot create workqueue");
+		return -ENOMEM;
+	}
+
+	r = cvm_oct_configure_common_hw();
+	if (r)
+		return r;
+
+	/* Change the input group for all ports before input is enabled */
+	num_interfaces = cvmx_helper_get_number_of_interfaces();
+	for (interface = 0; interface < num_interfaces; interface++) {
+		int num_ports = cvmx_helper_ports_on_interface(interface);
+		int index;
+
+		for (index = 0; index < num_ports; index++) {
+			union cvmx_pip_prt_tagx pip_prt_tagx;
+			int port = cvmx_helper_get_ipd_port(interface, index);
+
+			if (octeon_has_feature(OCTEON_FEATURE_PKND))
+				port = cvmx_helper_get_pknd(interface, index);
+
+			if (!cvmx_helper_is_port_valid(interface, index))
+				continue;
+			pip_prt_tagx.u64 = cvmx_read_csr(CVMX_PIP_PRT_TAGX(port));
+			pip_prt_tagx.s.grp = pow_receive_group;
+			cvmx_write_csr(CVMX_PIP_PRT_TAGX(port), pip_prt_tagx.u64);
+		}
+	}
+
+	cvmx_helper_ipd_and_packet_input_enable();
+
+	/* Initialize the FAU used for counting packet buffers that
+	 * need to be freed.
+	 */
+	cvmx_fau_atomic_write32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);
+
+	num_interfaces = cvmx_helper_get_number_of_interfaces();
+	for (interface = 0; interface < num_interfaces; interface++) {
+		cvmx_helper_interface_mode_t imode = cvmx_helper_interface_get_mode(interface);
+		int num_ports = cvmx_helper_ports_on_interface(interface);
+		int interface_port;
+
+		if (imode == CVMX_HELPER_INTERFACE_MODE_SRIO)
+			num_ports = 2; /* consistent with se apps. could be 4 */
+
+		for (interface_port = 0; interface_port < num_ports;
+		     interface_port++) {
+			struct octeon_ethernet *priv;
+			int base_queue;
+			struct net_device *dev;
+
+			if (!cvmx_helper_is_port_valid(interface,
+						      interface_port))
+				continue;
+
+			dev = alloc_etherdev(sizeof(struct octeon_ethernet));
+			if (!dev) {
+				dev_err(&pdev->dev,
+					"Failed to allocate ethernet device for port %d:%d\n",
+					interface, interface_port);
+				continue;
+			}
+
+			if (disable_core_queueing)
+				dev->tx_queue_len = 0;
+
+			/* Initialize the device private structure. */
+			priv = netdev_priv(dev);
+			INIT_LIST_HEAD(&priv->srio_bcast);
+			priv->of_node = cvm_oct_node_for_port(pip, interface, interface_port);
+			priv->netdev = dev;
+			priv->interface = interface;
+			priv->interface_port = interface_port;
+			spin_lock_init(&priv->poll_lock);
+			INIT_DELAYED_WORK(&priv->port_periodic_work,
+					  cvm_oct_periodic_worker);
+			priv->imode = imode;
+
+			if (imode == CVMX_HELPER_INTERFACE_MODE_SRIO) {
+				int mbox = cvmx_helper_get_ipd_port(interface, interface_port) - cvmx_helper_get_ipd_port(interface, 0);
+				union cvmx_srio_tx_message_header tx_header;
+				tx_header.u64 = 0;
+				tx_header.s.tt = 0;
+				tx_header.s.ssize = 0xe;
+				tx_header.s.mbox = mbox;
+				tx_header.s.lns = 1;
+				tx_header.s.intr = 1;
+				priv->srio_tx_header = tx_header.u64;
+				priv->ipd_port = cvmx_helper_get_ipd_port(interface, mbox >> 1);
+				priv->pko_port = priv->ipd_port;
+				priv->key = priv->ipd_port + (0x10000 * mbox);
+				base_queue = cvmx_pko_get_base_queue(priv->ipd_port) + (mbox & 1);
+				priv->num_tx_queues = 1;
+				cvm_oct_by_srio_mbox[interface - 4][mbox] = priv;
+			} else {
+				priv->ipd_port = cvmx_helper_get_ipd_port(interface, interface_port);
+				priv->key = priv->ipd_port;
+				priv->pko_port = cvmx_helper_get_pko_port(interface, interface_port);
+				base_queue = cvmx_pko_get_base_queue(priv->ipd_port);
+				priv->num_tx_queues = cvmx_pko_get_num_queues(priv->ipd_port);
+			}
+
+			BUG_ON(priv->num_tx_queues < 1);
+			BUG_ON(priv->num_tx_queues > 32);
+
+			if (octeon_has_feature(OCTEON_FEATURE_PKND))
+				priv->ipd_pkind = cvmx_helper_get_pknd(interface, interface_port);
+			else
+				priv->ipd_pkind = priv->ipd_port;
+
+			for (qos = 0; qos < priv->num_tx_queues; qos++) {
+				priv->tx_queue[qos].queue = base_queue + qos;
+				fau = fau - sizeof(u32);
+				priv->tx_queue[qos].fau = fau;
+				cvmx_fau_atomic_write32(priv->tx_queue[qos].fau, 0);
+			}
+
+			/* Cache the fact that there may be multiple queues */
+			priv->tx_multiple_queues = (queues_count > 1);
+
+			switch (priv->imode) {
+			/* These types don't support ports to IPD/PKO */
+			case CVMX_HELPER_INTERFACE_MODE_DISABLED:
+			case CVMX_HELPER_INTERFACE_MODE_PCIE:
+			case CVMX_HELPER_INTERFACE_MODE_PICMG:
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_NPI:
+				dev->netdev_ops = &cvm_oct_npi_netdev_ops;
+				strcpy(dev->name, "npi%d");
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_XAUI:
+			case CVMX_HELPER_INTERFACE_MODE_RXAUI:
+				priv->tx_lockless = priv->tx_multiple_queues && !disable_lockless_pko;
+				dev->netdev_ops = priv->tx_lockless ?
+					&cvm_oct_sgmii_lockless_netdev_ops : &cvm_oct_sgmii_netdev_ops;
+				priv->has_gmx_regs = 1;
+				strcpy(dev->name, "xaui%d");
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_LOOP:
+				dev->netdev_ops = &cvm_oct_npi_netdev_ops;
+				strcpy(dev->name, "loop%d");
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_SGMII:
+			case CVMX_HELPER_INTERFACE_MODE_QSGMII:
+				priv->tx_lockless = priv->tx_multiple_queues && !disable_lockless_pko;
+				dev->netdev_ops = priv->tx_lockless ?
+					&cvm_oct_sgmii_lockless_netdev_ops : &cvm_oct_sgmii_netdev_ops;
+				priv->has_gmx_regs = 1;
+				strcpy(dev->name, "eth%d");
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_SPI:
+				priv->tx_lockless = priv->tx_multiple_queues && !disable_lockless_pko;
+				dev->netdev_ops = priv->tx_lockless ?
+					&cvm_oct_spi_lockless_netdev_ops : &cvm_oct_spi_netdev_ops;
+				strcpy(dev->name, "spi%d");
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_RGMII:
+			case CVMX_HELPER_INTERFACE_MODE_GMII:
+				priv->tx_lockless = priv->tx_multiple_queues && !disable_lockless_pko;
+				dev->netdev_ops = priv->tx_lockless ?
+					&cvm_oct_rgmii_lockless_netdev_ops : &cvm_oct_rgmii_netdev_ops;
+				priv->has_gmx_regs = 1;
+				strcpy(dev->name, "eth%d");
+				break;
+#ifdef CONFIG_RAPIDIO
+			case CVMX_HELPER_INTERFACE_MODE_SRIO:
+				dev->netdev_ops = &cvm_oct_srio_netdev_ops;
+				strcpy(dev->name, "rio%d");
+				break;
+#endif
+			}
+
+			netif_carrier_off(dev);
+			if (!dev->netdev_ops) {
+				free_netdev(dev);
+			} else if (register_netdev(dev) < 0) {
+				dev_err(&pdev->dev,
+					"Failed to register ethernet device for interface %d, port %d\n",
+					interface, priv->ipd_port);
+				free_netdev(dev);
+			} else {
+				list_add_tail(&priv->list, &cvm_oct_list);
+				if (cvm_oct_by_pkind[priv->ipd_pkind] == NULL)
+					cvm_oct_by_pkind[priv->ipd_pkind] = priv;
+				else
+					cvm_oct_by_pkind[priv->ipd_pkind] = (void *)-1L;
+
+				cvm_oct_add_ipd_port(priv);
+				/* Each transmit queue will need its
+				 * own MAX_OUT_QUEUE_DEPTH worth of
+				 * WQE to track the transmit skbs.
+				 */
+				cvm_oct_mem_fill_fpa(wqe_pool,
+						     PER_DEVICE_EXTRA_WQE);
+				num_devices_extra_wqe++;
+				queue_delayed_work(cvm_oct_poll_queue,
+						   &priv->port_periodic_work, HZ);
+			}
+		}
+	}
+
+	cvm_oct_rx_initialize(num_packet_buffers + num_devices_extra_wqe * PER_DEVICE_EXTRA_WQE);
+
+	queue_delayed_work(cvm_oct_poll_queue, &cvm_oct_rx_refill_work, HZ);
+
+	return 0;
+}
+
+static int cvm_oct_remove(struct platform_device *pdev)
+{
+	struct octeon_ethernet *priv;
+	struct octeon_ethernet *tmp;
+
+	/* Put both taking the interface down and unregistering it
+	 * under the lock.  That way the devices cannot be taken back
+	 * up in the middle of everything.
+	 */
+	rtnl_lock();
+
+	/* Take down all the interfaces, this disables the GMX and
+	 * prevents it from getting into a Bad State when IPD is
+	 * disabled.
+	 */
+	list_for_each_entry(priv, &cvm_oct_list, list) {
+		unsigned int f = dev_get_flags(priv->netdev);
+		dev_change_flags(priv->netdev, f & ~IFF_UP);
+	}
+
+	mdelay(10);
+
+	cvmx_ipd_disable();
+
+	mdelay(10);
+
+	atomic_inc_return(&cvm_oct_poll_queue_stopping);
+	cancel_delayed_work_sync(&cvm_oct_rx_refill_work);
+
+	cvm_oct_rx_shutdown0();
+
+	/* unregister the ethernet devices */
+	list_for_each_entry(priv, &cvm_oct_list, list) {
+		cancel_delayed_work_sync(&priv->port_periodic_work);
+		unregister_netdevice(priv->netdev);
+	}
+
+	rtnl_unlock();
+
+	/* Free the ethernet devices */
+	list_for_each_entry_safe_reverse(priv, tmp, &cvm_oct_list, list) {
+		list_del(&priv->list);
+		free_netdev(priv->netdev);
+	}
+
+	cvmx_helper_shutdown_packet_io_global();
+
+	cvm_oct_rx_shutdown1();
+
+	destroy_workqueue(cvm_oct_poll_queue);
+
+	/* Free the HW pools */
+	cvm_oct_mem_empty_fpa(packet_pool, num_packet_buffers);
+	cvm_oct_release_fpa_pool(packet_pool);
+
+	cvm_oct_mem_empty_fpa(wqe_pool,
+			      num_packet_buffers + num_devices_extra_wqe * PER_DEVICE_EXTRA_WQE);
+	cvm_oct_release_fpa_pool(wqe_pool);
+
+	cvm_oct_mem_empty_fpa(output_pool,
+				cvm_oct_num_output_buffers);
+	cvm_oct_release_fpa_pool(output_pool);
+
+	cvm_oct_mem_cleanup();
+
+	cvmx_pko_queue_free_all();
+	cvmx_pko_internal_ports_range_free_all();
+
+	__cvmx_export_app_config_cleanup();
+
+	return 0;
+}
+
+static struct of_device_id cvm_oct_match[] = {
+	{
+		.compatible = "cavium,octeon-3860-pip",
+	},
+	{},
+};
+MODULE_DEVICE_TABLE(of, cvm_oct_match);
+
+static struct platform_driver cvm_oct_driver = {
+	.probe		= cvm_oct_probe,
+	.remove		= cvm_oct_remove,
+	.driver		= {
+		.owner	= THIS_MODULE,
+		.name	= KBUILD_MODNAME,
+		.of_match_table = cvm_oct_match,
+	},
+};
+
+module_platform_driver(cvm_oct_driver);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Cavium Networks <support@caviumnetworks.com>");
+MODULE_DESCRIPTION("Cavium Networks Octeon ethernet driver.");
diff --git a/drivers/net/ethernet/octeon/octeon-ethernet.h b/drivers/net/ethernet/octeon/octeon-ethernet.h
new file mode 100644
index 0000000..a7bfe57
--- /dev/null
+++ b/drivers/net/ethernet/octeon/octeon-ethernet.h
@@ -0,0 +1,223 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+
+/*
+ * External interface for the Cavium Octeon ethernet driver.
+ */
+#ifndef OCTEON_ETHERNET_H
+#define OCTEON_ETHERNET_H
+
+#include <linux/of.h>
+
+#include <asm/octeon/cvmx-helper.h>
+#include <asm/octeon/cvmx-fau.h>
+#include <asm/octeon/octeon-ethernet-user.h>
+
+/**
+ * This is the definition of the Ethernet driver's private
+ * driver state stored in netdev_priv(dev).
+ */
+struct octeon_ethernet {
+	struct rb_node ipd_tree;
+	int key;
+	int ipd_port;
+	int pko_port;
+	int ipd_pkind;
+	int interface;
+	int interface_port;
+
+	/* My netdev. */
+	struct net_device *netdev;
+	/* My location in the cvm_oct_list */
+	struct list_head list;
+
+	/* Type of port. This is one of the enums in
+	 * cvmx_helper_interface_mode_t
+	 */
+	int imode;
+
+	unsigned int rx_strip_fcs:1;
+	unsigned int has_gmx_regs:1;
+	unsigned int tx_timestamp_hw:1;
+	unsigned int rx_timestamp_hw:1;
+	unsigned int tx_multiple_queues:1;
+	unsigned int tx_lockless:1;
+
+	/* Optional intecept callback defined above */
+	cvm_oct_callback_t      intercept_cb;
+
+	/* Number of elements in tx_queue below */
+	int                     num_tx_queues;
+
+	/* SRIO ports add this header for the SRIO block */
+	u64 srio_tx_header;
+
+	struct {
+		/* PKO hardware queue for the port */
+		int	queue;
+		/* Hardware fetch and add to count outstanding tx buffers */
+		int	fau;
+	} tx_queue[32];
+
+	struct phy_device *phydev;
+	int last_link;
+	/* Last negotiated link state */
+	u64 link_info;
+	/* Called periodically to check link status */
+	spinlock_t poll_lock;
+	void (*poll) (struct net_device *dev);
+	void (*link_change) (struct octeon_ethernet *, cvmx_helper_link_info_t);
+	struct delayed_work	port_periodic_work;
+	struct work_struct	port_work;	/* may be unused. */
+	struct device_node	*of_node;
+	u64 last_tx_octets;
+	u32 last_tx_packets;
+	struct list_head srio_bcast;
+	struct notifier_block	hw_status_notifier;
+};
+
+struct octeon_ethernet_srio_bcast_target {
+	struct list_head list;
+	u16 destid;
+};
+
+struct octeon_ethernet *cvm_oct_dev_for_port(int);
+
+int cvm_oct_free_work(void *work_queue_entry);
+
+int cvm_oct_rgmii_init(struct net_device *dev);
+int cvm_oct_rgmii_open(struct net_device *dev);
+int cvm_oct_rgmii_stop(struct net_device *dev);
+
+int cvm_oct_sgmii_init(struct net_device *dev);
+void cvm_oct_sgmii_uninit(struct net_device *dev);
+int cvm_oct_sgmii_open(struct net_device *dev);
+int cvm_oct_sgmii_stop(struct net_device *dev);
+
+int cvm_oct_spi_init(struct net_device *dev);
+void cvm_oct_spi_uninit(struct net_device *dev);
+
+int cvm_oct_xaui_init(struct net_device *dev);
+int cvm_oct_xaui_open(struct net_device *dev);
+int cvm_oct_xaui_stop(struct net_device *dev);
+
+int cvm_oct_srio_init(struct net_device *dev);
+int cvm_oct_srio_open(struct net_device *dev);
+int cvm_oct_srio_stop(struct net_device *dev);
+int cvm_oct_xmit_srio(struct sk_buff *skb, struct net_device *dev);
+int cvm_oct_srio_set_mac_address(struct net_device *dev, void *addr);
+int cvm_oct_srio_change_mtu(struct net_device *dev, int new_mtu);
+struct net_device_stats *cvm_oct_srio_get_stats(struct net_device *dev);
+
+int cvm_oct_common_init(struct net_device *dev);
+int cvm_oct_common_stop(struct net_device *dev);
+
+void cvm_oct_set_carrier(struct octeon_ethernet *priv,
+				cvmx_helper_link_info_t link_info);
+void cvm_oct_adjust_link(struct net_device *dev);
+
+int cvm_oct_ioctl(struct net_device *dev, struct ifreq *rq, int cmd);
+int cvm_oct_phy_setup_device(struct net_device *dev);
+
+int cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev);
+int cvm_oct_xmit_lockless(struct sk_buff *skb, struct net_device *dev);
+
+void cvm_oct_poll_controller(struct net_device *dev);
+void cvm_oct_rx_initialize(int);
+void cvm_oct_rx_shutdown0(void);
+void cvm_oct_rx_shutdown1(void);
+
+int cvm_oct_mem_fill_fpa(int pool, int elements);
+int cvm_oct_mem_empty_fpa(int pool, int elements);
+int cvm_oct_alloc_fpa_pool(int pool, int size);
+int cvm_oct_release_fpa_pool(int pool);
+void cvm_oct_mem_cleanup(void);
+
+extern const struct ethtool_ops cvm_oct_ethtool_ops;
+
+extern int rx_cpu_factor;
+extern int packet_pool;
+extern int wqe_pool;
+extern int output_pool;
+extern int always_use_pow;
+extern int pow_send_group;
+extern int pow_receive_group;
+extern char pow_send_list[];
+extern struct list_head cvm_oct_list;
+extern struct octeon_ethernet *cvm_oct_by_pkind[];
+extern struct octeon_ethernet *cvm_oct_by_srio_mbox[4][4];
+
+extern struct workqueue_struct *cvm_oct_poll_queue;
+extern atomic_t cvm_oct_poll_queue_stopping;
+
+extern int max_rx_cpus;
+extern int rx_napi_weight;
+
+static inline void cvm_oct_rx_refill_pool(int fill_threshold)
+{
+	int number_to_free;
+	int num_freed;
+	/* Refill the packet buffer pool */
+	number_to_free =
+		cvmx_fau_fetch_and_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);
+
+	if (number_to_free > fill_threshold) {
+		cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE,
+				      -number_to_free);
+		num_freed = cvm_oct_mem_fill_fpa(packet_pool, number_to_free);
+		if (num_freed != number_to_free) {
+			cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE,
+					number_to_free - num_freed);
+		}
+	}
+}
+
+/**
+ * cvm_oct_get_buffer_ptr - convert packet data address to pointer
+ * @pd: Packet data hardware address
+ *
+ * Returns Packet buffer pointer
+ */
+static inline void *cvm_oct_get_buffer_ptr(union cvmx_buf_ptr pd)
+{
+	return phys_to_virt(((pd.s.addr >> 7) - pd.s.back) << 7);
+}
+
+static inline struct sk_buff **cvm_oct_packet_to_skb(void *packet)
+{
+	char *p = packet;
+	return (struct sk_buff **)(p - sizeof(void *));
+}
+
+#define CVM_OCT_SKB_TO_FPA_PADDING (128 + sizeof(void *) - 1)
+
+static inline u8 *cvm_oct_get_fpa_head(struct sk_buff *skb)
+{
+	return (u8 *)((unsigned long)(skb->head + CVM_OCT_SKB_TO_FPA_PADDING) & ~0x7ful);
+}
+
+#endif
diff --git a/drivers/net/ethernet/octeon/octeon-pow-ethernet.c b/drivers/net/ethernet/octeon/octeon-pow-ethernet.c
new file mode 100644
index 0000000..a80a7c9
--- /dev/null
+++ b/drivers/net/ethernet/octeon/octeon-pow-ethernet.c
@@ -0,0 +1,653 @@
+/*
+ *   Octeon POW Ethernet Driver
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2005-2012 Cavium, Inc.
+ */
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/interrupt.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/in.h>
+#include <linux/ip.h>
+#include <linux/string.h>
+#include <linux/delay.h>
+
+#include <asm/octeon/octeon.h>
+#include <asm/octeon/cvmx.h>
+#include <asm/octeon/cvmx-fpa.h>
+#include <asm/octeon/cvmx-pow.h>
+#include <asm/octeon/cvmx-wqe.h>
+#include <asm/octeon/cvmx-pow-defs.h>
+#include <asm/octeon/cvmx-sso-defs.h>
+
+#define VIRTUAL_PORT    63	/* Value to put in work->ipprt */
+
+#define DEBUGPRINT(format, ...) do {					\
+		if (printk_ratelimit())					\
+			printk(format, ##__VA_ARGS__);			\
+	} while (0)
+
+#define DEV_NAME "octeon-pow-ethernet"
+
+static int receive_group = -1;
+module_param(receive_group, int, 0444);
+MODULE_PARM_DESC(receive_group,
+		 " 0-16 POW group to receive packets from. This must be unique in\n"
+		 "\t\tthe system. If you don't specify a value, the core ID will\n"
+		 "\t\tbe used.");
+
+static int broadcast_groups;
+module_param(broadcast_groups, int, 0644);
+MODULE_PARM_DESC(broadcast_groups,
+		 " Bitmask of groups to send broadcasts to. This MUST be specified.\n"
+		 "\t\tWARNING: Be careful to not send broadcasts to groups that aren't\n"
+		 "\t\tread otherwise you may fill the POW and stop receiving packets.\n");
+
+
+
+static int ptp_rx_group = -1;
+module_param(ptp_rx_group, int, 0444);
+MODULE_PARM_DESC(ptp_rx_group,
+		 "For the PTP POW device, 0-64 POW group to receive packets from.\n"
+		 "\t\tIf you don't specify a value, the 'pow0' device will not be created\n.");
+
+static int ptp_tx_group = -1;
+module_param(ptp_tx_group, int, 0444);
+MODULE_PARM_DESC(ptp_tx_group,
+		 "For the PTP POW device, 0-64 POW group to transmit packets to.\n"
+		 "\t\tIf you don't specify a value, the 'pow0' device will not be created\n.");
+
+
+/*
+ * This is the definition of the Ethernet driver's private
+ * driver state.
+ */
+struct octeon_pow {
+	u64 tx_mask;
+	int rx_group;
+	bool is_ptp;
+};
+
+static int fpa_wqe_pool = 1;	/* HW FPA pool to use for work queue entries */
+static int fpa_packet_pool;	/* HW FPA pool to use for packet buffers */
+static int fpa_packet_pool_size = 2048;	/* Size of the packet buffers */
+static struct net_device *octeon_pow_oct_dev;
+static struct net_device *octeon_pow_ptp_dev;
+static int octeon_pow_num_groups;
+
+/*
+ * Given a packet data address, return a pointer to the
+ * beginning of the packet buffer.
+ */
+static void *get_buffer_ptr(union cvmx_buf_ptr packet_ptr)
+{
+	return phys_to_virt(((packet_ptr.s.addr >> 7) -
+			     packet_ptr.s.back) << 7);
+}
+
+static int octeon_pow_free_work(cvmx_wqe_t *work)
+{
+	int segments = work->word2.s.bufs;
+	union cvmx_buf_ptr segment_ptr = work->packet_ptr;
+
+	while (segments--) {
+		union cvmx_buf_ptr next_ptr =
+			*(union cvmx_buf_ptr *)phys_to_virt(segment_ptr.s.addr - 8);
+		if (unlikely(!segment_ptr.s.i))
+			cvmx_fpa_free(get_buffer_ptr(segment_ptr),
+				 segment_ptr.s.pool, 0);
+		segment_ptr = next_ptr;
+	}
+	cvmx_fpa_free(work, fpa_wqe_pool, 0);
+
+	return 0;
+}
+
+static int octeon_pow_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+	struct octeon_pow *priv;
+	cvmx_wqe_t *work = NULL;
+	void *packet_buffer = NULL;
+	void *copy_location;
+	u64 send_group_mask;
+	int send_group;
+
+	priv = netdev_priv(dev);
+
+	/* Any unknown MAC address goes to all groups in the module
+	 * param broadcast_groups. Known MAC addresses use the low
+	 * order dest mac byte as the group number.
+	 */
+	if (!priv->is_ptp && ((*(uint64_t *) (skb->data) >> 16) < 0x01ff))
+		send_group_mask = 1ull << (skb->data[5] & (octeon_pow_num_groups - 1));
+	else
+		send_group_mask = priv->tx_mask;
+
+	send_group_mask &= ~(1 << priv->rx_group);
+
+	/* It is ugly, but we need to send multiple times for
+	 * broadcast packets. The hardware doesn't support submitting
+	 * work to multiple groups
+	 */
+	for (send_group = 0; send_group < octeon_pow_num_groups; send_group++) {
+		/* Don't transmit to groups not in our send_group_mask */
+		if (likely((send_group_mask & (1ULL << send_group)) == 0))
+			continue;
+
+		/* Get a work queue entry */
+		work = cvmx_fpa_alloc(fpa_wqe_pool);
+		if (unlikely(work == NULL)) {
+			DEBUGPRINT("%s: Failed to allocate a work queue entry\n",
+				   dev->name);
+			goto fail;
+		}
+
+		/* Get a packet buffer */
+		packet_buffer = cvmx_fpa_alloc(fpa_packet_pool);
+		if (unlikely(packet_buffer == NULL)) {
+			DEBUGPRINT("%s: Failed to allocate a packet buffer\n",
+				   dev->name);
+			goto fail;
+		}
+
+		/* Calculate where we need to copy the data to. We
+		 * need to leave 8 bytes for a next pointer
+		 * (unused). Then we need to align the IP packet src
+		 * and dest into the same 64bit word.
+		 */
+		copy_location = packet_buffer + sizeof(uint64_t) + 6;
+
+		/* Fail if the packet won't fit in a single buffer */
+		if (unlikely
+		    (copy_location + skb->len >
+		     packet_buffer + fpa_packet_pool_size)) {
+			DEBUGPRINT("%s: Packet too large for FPA buffer\n",
+				   dev->name);
+			goto fail;
+		}
+
+		memcpy(copy_location, skb->data, skb->len);
+
+		/* Fill in some of the work queue fields. We may need
+		 * to add more if the software at the other end needs
+		 * them.
+		 */
+		work->word0.u64 = 0;
+		work->word2.u64 = 0;	/* Default to zero. Sets of zero later
+					   are commented out */
+#if 0
+		work->hw_chksum = skb->csum;
+#endif
+		work->word1.len = skb->len;
+		cvmx_wqe_set_port(work, VIRTUAL_PORT);
+		cvmx_wqe_set_qos(work, 0);
+		cvmx_wqe_set_grp(work, send_group);
+		work->word1.tag_type = 2;
+		work->word1.tag = 0;
+
+		work->word2.s.bufs = 1;
+		work->packet_ptr.u64 = 0;
+		work->packet_ptr.s.addr = virt_to_phys(copy_location);
+		work->packet_ptr.s.pool = fpa_packet_pool;
+		work->packet_ptr.s.size = fpa_packet_pool_size;
+		work->packet_ptr.s.back = (copy_location - packet_buffer) >> 7;
+
+		if (skb->protocol == htons(ETH_P_IP)) {
+			work->word2.s.ip_offset = 14;
+			#if 0
+			work->word2.s.vlan_valid = 0;	/* FIXME */
+			work->word2.s.vlan_cfi = 0;	/* FIXME */
+			work->word2.s.vlan_id = 0;	/* FIXME */
+			work->word2.s.dec_ipcomp = 0;	/* FIXME */
+			#endif
+			work->word2.s.tcp_or_udp =
+				(ip_hdr(skb)->protocol == IPPROTO_TCP) ||
+				(ip_hdr(skb)->protocol == IPPROTO_UDP);
+			#if 0
+			work->word2.s.dec_ipsec = 0; /* FIXME */
+			work->word2.s.is_v6 = 0; /* We only support IPv4
+						    right now */
+			work->word2.s.software = 0; /* Hardware would set to
+						       zero */
+			work->word2.s.L4_error = 0; /* No error, packet is
+						       internal */
+			#endif
+			work->word2.s.is_frag = !((ip_hdr(skb)->frag_off == 0)
+						  || (ip_hdr(skb)->frag_off ==
+						      1 << 14));
+			#if 0
+			work->word2.s.IP_exc = 0; /* Assume Linux is sending
+						     a good packet */
+			#endif
+			work->word2.s.is_bcast =
+				(skb->pkt_type == PACKET_BROADCAST);
+			work->word2.s.is_mcast =
+				(skb->pkt_type == PACKET_MULTICAST);
+			#if 0
+			work->word2.s.not_IP = 0; /* This is an IP packet */
+			work->word2.s.rcv_error = 0; /* No error, packet is
+							internal */
+			work->word2.s.err_code = 0;  /* No error, packet is
+							internal */
+			#endif
+
+			/* When copying the data, include 4 bytes of the
+			   ethernet header to align the same way hardware does */
+			memcpy(work->packet_data, skb->data + 10,
+			       sizeof(work->packet_data));
+		} else {
+			#if 0
+			work->word2.snoip.vlan_valid = 0; /* FIXME */
+			work->word2.snoip.vlan_cfi = 0;   /* FIXME */
+			work->word2.snoip.vlan_id = 0;    /* FIXME */
+			work->word2.snoip.software = 0;   /* Hardware would
+							     set to zero */
+			#endif
+			work->word2.snoip.is_rarp =
+				skb->protocol == htons(ETH_P_RARP);
+			work->word2.snoip.is_arp =
+				skb->protocol == htons(ETH_P_ARP);
+			work->word2.snoip.is_bcast =
+				(skb->pkt_type == PACKET_BROADCAST);
+			work->word2.snoip.is_mcast =
+				(skb->pkt_type == PACKET_MULTICAST);
+			work->word2.snoip.not_IP = 1;	/* IP was done up above */
+			#if 0
+			work->word2.snoip.rcv_error = 0; /* No error, packet
+							    is internal */
+			work->word2.snoip.err_code = 0;  /* No error, packet
+							    is internal */
+			#endif
+			memcpy(work->packet_data, skb->data,
+			       sizeof(work->packet_data));
+		}
+
+		/*
+		 * Submit the packet to the POW
+		 * tag: 0
+		 * tag_type: 2
+		 * qos: 0
+		 * grp: send_group
+		 */
+		cvmx_pow_work_submit(work, 0, 2, 0, send_group);
+		work = NULL;
+		packet_buffer = NULL;
+	}
+
+	dev->stats.tx_packets++;
+	dev->stats.tx_bytes += skb->len;
+	dev_kfree_skb(skb);
+	return NETDEV_TX_OK;
+
+fail:
+	if (work)
+		cvmx_fpa_free(work, fpa_wqe_pool, 0);
+	if (packet_buffer)
+		cvmx_fpa_free(packet_buffer, fpa_packet_pool, 0);
+	dev->stats.tx_dropped++;
+	dev_kfree_skb(skb);
+	return NETDEV_TX_OK;
+}
+
+
+/**
+ * Interrupt handler. The interrupt occurs whenever the POW
+ * transitions from 0->1 packets in our group.
+ *
+ * @param cpl
+ * @param dev_id
+ * @param regs
+ * @return
+ */
+static irqreturn_t octeon_pow_interrupt(int cpl, void *dev_id)
+{
+	const uint64_t coreid = cvmx_get_core_num();
+	struct net_device *dev = (struct net_device *) dev_id;
+	struct octeon_pow *priv;
+	uint64_t old_group_mask;
+	cvmx_wqe_t *work;
+	struct sk_buff *skb;
+
+	priv = netdev_priv(dev);
+
+	/* Make sure any userspace operations are complete */
+	asm volatile ("synciobdma" : : : "memory");
+
+	if (OCTEON_IS_MODEL(OCTEON_CN68XX)) {
+		cvmx_write_csr(CVMX_SSO_WQ_INT, 1ull << priv->rx_group);
+		cvmx_write_csr(CVMX_SSO_WQ_IQ_DIS, 1ull << priv->rx_group);
+
+		old_group_mask = cvmx_read_csr(CVMX_SSO_PPX_GRP_MSK(coreid));
+		cvmx_write_csr(CVMX_SSO_PPX_GRP_MSK(coreid),
+			       1ull << priv->rx_group);
+		/* Read it back so it takes effect before we request work */
+		cvmx_read_csr(CVMX_SSO_PPX_GRP_MSK(coreid));
+	} else {
+		/* Acknowledge the interrupt */
+		cvmx_write_csr(CVMX_POW_WQ_INT, 0x10001 << priv->rx_group);
+
+		/* Only allow work for our group */
+		old_group_mask = cvmx_read_csr(CVMX_POW_PP_GRP_MSKX(coreid));
+		cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(coreid), 1 << priv->rx_group);
+	}
+	while (1) {
+		work = cvmx_pow_work_request_sync(0);
+		if (work == NULL)
+			break;
+
+		/* Silently drop packets if we aren't up */
+		if ((dev->flags & IFF_UP) == 0) {
+			octeon_pow_free_work(work);
+			continue;
+		}
+
+		/* Throw away all packets with receive errors */
+		if (unlikely(work->word2.snoip.rcv_error)) {
+			DEBUGPRINT("%s: Receive error code %d, packet dropped\n",
+				   dev->name, work->word2.snoip.err_code);
+			octeon_pow_free_work(work);
+			dev->stats.rx_errors++;
+			continue;
+		}
+
+		/* We have to copy the packet. First allocate an skbuff for it */
+		skb = dev_alloc_skb(work->word1.len);
+		if (!skb) {
+			DEBUGPRINT("%s: Failed to allocate skbuff, packet dropped\n",
+				   dev->name);
+			octeon_pow_free_work(work);
+			dev->stats.rx_dropped++;
+			continue;
+		}
+
+		/* Check if we've received a packet that was entirely
+		 * stored the work entry. This is untested
+		 */
+		if (unlikely(work->word2.s.bufs == 0)) {
+			DEBUGPRINT("%s: Received a work with work->word2.s.bufs=0, untested\n",
+				   dev->name);
+			memcpy(skb_put(skb, work->word1.len), work->packet_data,
+			       work->word1.len);
+		} else {
+			int segments = work->word2.s.bufs;
+			union cvmx_buf_ptr segment_ptr = work->packet_ptr;
+			int len = work->word1.len;
+			while (segments--) {
+				union cvmx_buf_ptr next_ptr =
+					*(union cvmx_buf_ptr *)phys_to_virt(segment_ptr.s.addr - 8);
+				/* Octeon Errata PKI-100: The segment size is
+				   wrong. Until it is fixed, calculate the
+				   segment size based on the packet pool buffer
+				   size. When it is fixed, the following line
+				   should be replaced with this one: int
+				   segment_size = segment_ptr.s.size; */
+				int segment_size =
+					fpa_packet_pool_size -
+					(segment_ptr.s.addr -
+					 (((segment_ptr.s.addr >> 7) -
+					   segment_ptr.s.back) << 7));
+				/* Don't copy more than what is left in the
+				   packet */
+				if (segment_size > len)
+					segment_size = len;
+				/* Copy the data into the packet */
+				memcpy(skb_put(skb, segment_size),
+				       phys_to_virt(segment_ptr.s.addr),
+				       segment_size);
+				/* Reduce the amount of bytes left to copy */
+				len -= segment_size;
+				segment_ptr = next_ptr;
+			}
+		}
+		octeon_pow_free_work(work);
+		skb->protocol = eth_type_trans(skb, dev);
+		skb->dev = dev;
+		skb->ip_summed = CHECKSUM_NONE;
+		dev->stats.rx_bytes += skb->len;
+		dev->stats.rx_packets++;
+		netif_rx(skb);
+	}
+
+	/* Restore the original POW group mask */
+	if (OCTEON_IS_MODEL(OCTEON_CN68XX)) {
+		cvmx_write_csr(CVMX_SSO_PPX_GRP_MSK(coreid), old_group_mask);
+		/* Read it back so it takes effect before ?? */
+		cvmx_read_csr(CVMX_SSO_PPX_GRP_MSK(coreid));
+	} else {
+		cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(coreid), old_group_mask);
+	}
+	return IRQ_HANDLED;
+}
+
+#ifdef CONFIG_NET_POLL_CONTROLLER
+
+static void octeon_pow_poll(struct net_device *dev)
+{
+	octeon_pow_interrupt(0, dev);
+}
+#endif
+
+static int octeon_pow_open(struct net_device *dev)
+{
+	int r;
+	struct octeon_pow *priv = netdev_priv(dev);
+	/* Clear the statistics whenever the interface is brought up */
+	memset(&dev->stats, 0, sizeof(dev->stats));
+
+	/* Register an IRQ hander for to receive POW interrupts */
+	r = request_irq(OCTEON_IRQ_WORKQ0 + priv->rx_group, octeon_pow_interrupt, 0, dev->name, dev);
+	if (r)
+		return r;
+
+	/* Enable POW interrupt when our port has at least one packet */
+	if (OCTEON_IS_MODEL(OCTEON_CN68XX)) {
+		union cvmx_sso_wq_int_thrx thr;
+		thr.u64 = 0;
+		thr.s.iq_thr = 1;
+		thr.s.ds_thr = 1;
+		cvmx_write_csr(CVMX_SSO_WQ_INT_THRX(priv->rx_group), thr.u64);
+	} else {
+		union cvmx_pow_wq_int_thrx thr;
+		thr.u64 = 0;
+		thr.s.iq_thr = 1;
+		thr.s.ds_thr = 1;
+		cvmx_write_csr(CVMX_POW_WQ_INT_THRX(priv->rx_group), thr.u64);
+	}
+
+	return 0;
+}
+
+static int octeon_pow_stop(struct net_device *dev)
+{
+	struct octeon_pow *priv = netdev_priv(dev);
+
+	/* Disable POW interrupt */
+	if (OCTEON_IS_MODEL(OCTEON_CN68XX))
+		cvmx_write_csr(CVMX_SSO_WQ_INT_THRX(priv->rx_group), 0);
+	else
+		cvmx_write_csr(CVMX_POW_WQ_INT_THRX(priv->rx_group), 0);
+
+	/* Free the interrupt handler */
+	free_irq(OCTEON_IRQ_WORKQ0 + priv->rx_group, dev);
+	return 0;
+}
+
+/**
+ * Per network device initialization
+ *
+ * @param dev    Device to initialize
+ * @return Zero on success
+ */
+static int octeon_pow_init(struct net_device *dev)
+{
+	struct octeon_pow *priv = netdev_priv(dev);
+
+	dev->features |= NETIF_F_LLTX;	/* We do our own locking, Linux doesn't
+					   need to */
+	dev->dev_addr[0] = 0;
+	dev->dev_addr[1] = 0;
+	dev->dev_addr[2] = 0;
+	dev->dev_addr[3] = 0;
+	dev->dev_addr[4] = priv->is_ptp ? 3 : 1;
+	dev->dev_addr[5] = priv->rx_group;
+	return 0;
+}
+
+static const struct net_device_ops octeon_pow_netdev_ops = {
+	.ndo_init = octeon_pow_init,
+	.ndo_open = octeon_pow_open,
+	.ndo_stop = octeon_pow_stop,
+	.ndo_start_xmit = octeon_pow_xmit,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller =  octeon_pow_poll,
+#endif
+};
+
+static int __init octeon_pow_mod_init(void)
+{
+	struct octeon_pow *priv;
+	u64 allowed_group_mask;
+	if (OCTEON_IS_MODEL(OCTEON_CN68XX)) {
+		octeon_pow_num_groups = 64;
+		allowed_group_mask = 0xffffffffffffffffull;
+	} else {
+		octeon_pow_num_groups = 16;
+		allowed_group_mask = 0xffffull;
+	}
+
+	/* If a receive group isn't specified, default to the core id */
+	if (receive_group < 0)
+		receive_group = cvmx_get_core_num();
+
+
+	if ((receive_group > octeon_pow_num_groups)) {
+		pr_err(DEV_NAME " ERROR: Invalid receive group. Must be 0-%d\n",
+		       octeon_pow_num_groups - 1);
+		return -1;
+	}
+
+	if (!broadcast_groups) {
+		pr_err(DEV_NAME " ERROR: You must specify a broadcast group mask.\n");
+		return -1;
+	}
+
+	if ((broadcast_groups & allowed_group_mask) != broadcast_groups) {
+		pr_err(DEV_NAME " ERROR: Invalid broadcast group mask.\n");
+		return -1;
+	}
+
+	if ((ptp_rx_group >= 0 && ptp_tx_group < 0) || (ptp_rx_group < 0 && ptp_tx_group >= 0)) {
+		pr_err(DEV_NAME " ERROR: Both ptp_rx_group AND ptp_tx_group must be set.\n");
+		return -1;
+	}
+
+	if (ptp_rx_group >= 0 && ptp_tx_group == ptp_rx_group) {
+		pr_err(DEV_NAME " ERROR: ptp_rx_group and ptp_tx_group must differ.\n");
+		return -1;
+	}
+
+	if (ptp_rx_group >= octeon_pow_num_groups || ptp_tx_group >= octeon_pow_num_groups) {
+		pr_err(DEV_NAME " ERROR: ptp_rx_group and ptp_tx_group. Must be 0-%d\n",
+		       octeon_pow_num_groups - 1);
+		return -1;
+	}
+
+	if (receive_group == ptp_rx_group) {
+		pr_err(DEV_NAME " ERROR: ptp_rx_group(%d) and  receive_group(%d) must differ.\n",
+			ptp_rx_group, receive_group);
+		return -1;
+	}
+
+	pr_info("Octeon POW only ethernet driver\n");
+
+	/* Setup is complete, create the virtual ethernet devices */
+	octeon_pow_oct_dev = alloc_etherdev(sizeof(struct octeon_pow));
+	if (octeon_pow_oct_dev == NULL) {
+		pr_err(DEV_NAME " ERROR: Failed to allocate ethernet device\n");
+		return -1;
+	}
+
+	octeon_pow_oct_dev->netdev_ops = &octeon_pow_netdev_ops;
+	strcpy(octeon_pow_oct_dev->name, "oct%d");
+
+	/* Initialize the device private structure. */
+	priv = netdev_priv(octeon_pow_oct_dev);
+	priv->rx_group = receive_group;
+	priv->tx_mask = broadcast_groups;
+
+	/* Spin waiting for another core to setup all the hardware */
+	printk("Waiting for another core to setup the IPD hardware...");
+	while ((cvmx_read_csr(CVMX_IPD_CTL_STATUS) & 1) == 0)
+		mdelay(100);
+
+	printk("Done\n");
+
+	/* Read the configured size of the FPA packet buffers. This
+	 * way we don't need changes if someone chooses to use a
+	 * different buffer size
+	 */
+	fpa_packet_pool_size = (cvmx_read_csr(CVMX_IPD_PACKET_MBUFF_SIZE) & 0xfff) * 8;
+
+	/* Read the work queue pool */
+	fpa_wqe_pool = cvmx_read_csr(CVMX_IPD_WQE_FPA_QUEUE) & 7;
+
+	if (register_netdev(octeon_pow_oct_dev) < 0) {
+		pr_err(DEV_NAME " ERROR: Failed to register ethernet device\n");
+		free_netdev(octeon_pow_oct_dev);
+		return -1;
+	}
+
+	if (ptp_rx_group < 0)
+		return 0;
+
+	/* Else create a ptp device. */
+	octeon_pow_ptp_dev = alloc_etherdev(sizeof(struct octeon_pow));
+	if (octeon_pow_ptp_dev == NULL) {
+		pr_err(DEV_NAME " ERROR: Failed to allocate ethernet device\n");
+		return -1;
+	}
+
+	octeon_pow_ptp_dev->netdev_ops = &octeon_pow_netdev_ops;
+	strcpy(octeon_pow_ptp_dev->name, "pow%d");
+
+	/* Initialize the device private structure. */
+	priv = netdev_priv(octeon_pow_ptp_dev);
+	priv->rx_group = ptp_rx_group;
+	priv->tx_mask = 1ull << ptp_tx_group;
+	priv->is_ptp = true;
+
+	if (register_netdev(octeon_pow_ptp_dev) < 0) {
+		pr_err(DEV_NAME " ERROR: Failed to register ethernet device\n");
+		free_netdev(octeon_pow_ptp_dev);
+		return -1;
+	}
+
+	return 0;
+}
+
+/**
+ * Module / driver shutdown
+ *
+ * @return Zero on success
+ */
+static void __exit octeon_pow_mod_exit(void)
+{
+	/* Free the ethernet devices */
+	unregister_netdev(octeon_pow_oct_dev);
+	free_netdev(octeon_pow_oct_dev);
+	if (octeon_pow_ptp_dev) {
+		unregister_netdev(octeon_pow_ptp_dev);
+		free_netdev(octeon_pow_ptp_dev);
+	}
+}
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Cavium Inc. <support@cavium.com>");
+MODULE_DESCRIPTION("Cavium Inc. OCTEON internal only POW ethernet driver.");
+module_init(octeon_pow_mod_init);
+module_exit(octeon_pow_mod_exit);
diff --git a/drivers/staging/Kconfig b/drivers/staging/Kconfig
index 8217471..6259cff 100644
--- a/drivers/staging/Kconfig
+++ b/drivers/staging/Kconfig
@@ -60,8 +60,6 @@ source "drivers/staging/phison/Kconfig"
 
 source "drivers/staging/line6/Kconfig"
 
-source "drivers/staging/octeon/Kconfig"
-
 source "drivers/staging/serqt_usb2/Kconfig"
 
 source "drivers/staging/vt6655/Kconfig"
diff --git a/drivers/staging/Makefile b/drivers/staging/Makefile
index 137e521..42f3f00 100644
--- a/drivers/staging/Makefile
+++ b/drivers/staging/Makefile
@@ -24,7 +24,6 @@ obj-$(CONFIG_IDE_PHISON)	+= phison/
 obj-$(CONFIG_LINE6_USB)		+= line6/
 obj-$(CONFIG_NETLOGIC_XLR_NET)	+= netlogic/
 obj-$(CONFIG_USB_SERIAL_QUATECH2)	+= serqt_usb2/
-obj-$(CONFIG_OCTEON_ETHERNET)	+= octeon/
 obj-$(CONFIG_VT6655)		+= vt6655/
 obj-$(CONFIG_VT6656)		+= vt6656/
 obj-$(CONFIG_VME_BUS)		+= vme/
diff --git a/drivers/staging/octeon/Kconfig b/drivers/staging/octeon/Kconfig
deleted file mode 100644
index 6e1d5f8..0000000
--- a/drivers/staging/octeon/Kconfig
+++ /dev/null
@@ -1,13 +0,0 @@
-config OCTEON_ETHERNET
-	tristate "Cavium Networks Octeon Ethernet support"
-	depends on CAVIUM_OCTEON_SOC && NETDEVICES
-	select PHYLIB
-	select MDIO_OCTEON
-	help
-	  This driver supports the builtin ethernet ports on Cavium
-	  Networks' products in the Octeon family. This driver supports the
-	  CN3XXX and CN5XXX Octeon processors.
-
-	  To compile this driver as a module, choose M here.  The module
-	  will be called octeon-ethernet.
-
diff --git a/drivers/staging/octeon/Makefile b/drivers/staging/octeon/Makefile
deleted file mode 100644
index 23f37e5..0000000
--- a/drivers/staging/octeon/Makefile
+++ /dev/null
@@ -1,22 +0,0 @@
-# This file is subject to the terms and conditions of the GNU General Public
-# License.  See the file "COPYING" in the main directory of this archive
-# for more details.
-#
-# Copyright (C) 2005-2009 Cavium Networks
-#
-
-#
-# Makefile for Cavium OCTEON on-board ethernet driver
-#
-
-obj-${CONFIG_OCTEON_ETHERNET} :=  octeon-ethernet.o
-
-octeon-ethernet-y := ethernet.o
-octeon-ethernet-y += ethernet-mdio.o
-octeon-ethernet-y += ethernet-mem.o
-octeon-ethernet-y += ethernet-rgmii.o
-octeon-ethernet-y += ethernet-rx.o
-octeon-ethernet-y += ethernet-sgmii.o
-octeon-ethernet-y += ethernet-spi.o
-octeon-ethernet-y += ethernet-tx.o
-
diff --git a/drivers/staging/octeon/ethernet-defines.h b/drivers/staging/octeon/ethernet-defines.h
deleted file mode 100644
index bdaec8d..0000000
--- a/drivers/staging/octeon/ethernet-defines.h
+++ /dev/null
@@ -1,106 +0,0 @@
-/**********************************************************************
- * Author: Cavium Networks
- *
- * Contact: support@caviumnetworks.com
- * This file is part of the OCTEON SDK
- *
- * Copyright (c) 2003-2007 Cavium Networks
- *
- * This file is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, Version 2, as
- * published by the Free Software Foundation.
- *
- * This file is distributed in the hope that it will be useful, but
- * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
- * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
- * NONINFRINGEMENT.  See the GNU General Public License for more
- * details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this file; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- * or visit http://www.gnu.org/licenses/.
- *
- * This file may also be available under a different license from Cavium.
- * Contact Cavium Networks for more information
-**********************************************************************/
-
-/*
- * A few defines are used to control the operation of this driver:
- *  CONFIG_CAVIUM_RESERVE32
- *      This kernel config options controls the amount of memory configured
- *      in a wired TLB entry for all processes to share. If this is set, the
- *      driver will use this memory instead of kernel memory for pools. This
- *      allows 32bit userspace application to access the buffers, but also
- *      requires all received packets to be copied.
- *  CONFIG_CAVIUM_OCTEON_NUM_PACKET_BUFFERS
- *      This kernel config option allows the user to control the number of
- *      packet and work queue buffers allocated by the driver. If this is zero,
- *      the driver uses the default from below.
- *  USE_SKBUFFS_IN_HW
- *      Tells the driver to populate the packet buffers with kernel skbuffs.
- *      This allows the driver to receive packets without copying them. It also
- *      means that 32bit userspace can't access the packet buffers.
- *  USE_HW_TCPUDP_CHECKSUM
- *      Controls if the Octeon TCP/UDP checksum engine is used for packet
- *      output. If this is zero, the kernel will perform the checksum in
- *      software.
- *  USE_ASYNC_IOBDMA
- *      Use asynchronous IO access to hardware. This uses Octeon's asynchronous
- *      IOBDMAs to issue IO accesses without stalling. Set this to zero
- *      to disable this. Note that IOBDMAs require CVMSEG.
- *  REUSE_SKBUFFS_WITHOUT_FREE
- *      Allows the TX path to free an skbuff into the FPA hardware pool. This
- *      can significantly improve performance for forwarding and bridging, but
- *      may be somewhat dangerous. Checks are made, but if any buffer is reused
- *      without the proper Linux cleanup, the networking stack may have very
- *      bizarre bugs.
- */
-#ifndef __ETHERNET_DEFINES_H__
-#define __ETHERNET_DEFINES_H__
-
-#include <asm/octeon/cvmx-config.h>
-
-
-#define OCTEON_ETHERNET_VERSION "1.9"
-
-#ifndef CONFIG_CAVIUM_RESERVE32
-#define CONFIG_CAVIUM_RESERVE32 0
-#endif
-
-#define USE_SKBUFFS_IN_HW           1
-#ifdef CONFIG_NETFILTER
-#define REUSE_SKBUFFS_WITHOUT_FREE  0
-#else
-#define REUSE_SKBUFFS_WITHOUT_FREE  1
-#endif
-
-#define USE_HW_TCPUDP_CHECKSUM      1
-
-/* Enable Random Early Dropping under load */
-#define USE_RED                     1
-#define USE_ASYNC_IOBDMA            (CONFIG_CAVIUM_OCTEON_CVMSEG_SIZE > 0)
-
-/*
- * Allow SW based preamble removal at 10Mbps to workaround PHYs giving
- * us bad preambles.
- */
-#define USE_10MBPS_PREAMBLE_WORKAROUND 1
-/*
- * Use this to have all FPA frees also tell the L2 not to write data
- * to memory.
- */
-#define DONT_WRITEBACK(x)           (x)
-/* Use this to not have FPA frees control L2 */
-/*#define DONT_WRITEBACK(x)         0   */
-
-/* Maximum number of SKBs to try to free per xmit packet. */
-#define MAX_OUT_QUEUE_DEPTH 1000
-
-#define FAU_TOTAL_TX_TO_CLEAN (CVMX_FAU_REG_END - sizeof(uint32_t))
-#define FAU_NUM_PACKET_BUFFERS_TO_FREE (FAU_TOTAL_TX_TO_CLEAN - sizeof(uint32_t))
-
-#define TOTAL_NUMBER_OF_PORTS       (CVMX_PIP_NUM_INPUT_PORTS+1)
-
-
-#endif /* __ETHERNET_DEFINES_H__ */
diff --git a/drivers/staging/octeon/ethernet-mdio.c b/drivers/staging/octeon/ethernet-mdio.c
deleted file mode 100644
index 445e6ab..0000000
--- a/drivers/staging/octeon/ethernet-mdio.c
+++ /dev/null
@@ -1,203 +0,0 @@
-/**********************************************************************
- * Author: Cavium Networks
- *
- * Contact: support@caviumnetworks.com
- * This file is part of the OCTEON SDK
- *
- * Copyright (c) 2003-2007 Cavium Networks
- *
- * This file is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, Version 2, as
- * published by the Free Software Foundation.
- *
- * This file is distributed in the hope that it will be useful, but
- * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
- * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
- * NONINFRINGEMENT.  See the GNU General Public License for more
- * details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this file; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- * or visit http://www.gnu.org/licenses/.
- *
- * This file may also be available under a different license from Cavium.
- * Contact Cavium Networks for more information
-**********************************************************************/
-#include <linux/kernel.h>
-#include <linux/ethtool.h>
-#include <linux/phy.h>
-#include <linux/ratelimit.h>
-#include <linux/of_mdio.h>
-
-#include <net/dst.h>
-
-#include <asm/octeon/octeon.h>
-
-#include "ethernet-defines.h"
-#include "octeon-ethernet.h"
-#include "ethernet-util.h"
-
-#include <asm/octeon/cvmx-helper-board.h>
-
-#include <asm/octeon/cvmx-smix-defs.h>
-
-static void cvm_oct_get_drvinfo(struct net_device *dev,
-				struct ethtool_drvinfo *info)
-{
-	strlcpy(info->driver, "cavium-ethernet", sizeof(info->driver));
-	strlcpy(info->version, OCTEON_ETHERNET_VERSION, sizeof(info->version));
-	strlcpy(info->bus_info, "Builtin", sizeof(info->bus_info));
-}
-
-static int cvm_oct_get_settings(struct net_device *dev, struct ethtool_cmd *cmd)
-{
-	struct octeon_ethernet *priv = netdev_priv(dev);
-
-	if (priv->phydev)
-		return phy_ethtool_gset(priv->phydev, cmd);
-
-	return -EINVAL;
-}
-
-static int cvm_oct_set_settings(struct net_device *dev, struct ethtool_cmd *cmd)
-{
-	struct octeon_ethernet *priv = netdev_priv(dev);
-
-	if (!capable(CAP_NET_ADMIN))
-		return -EPERM;
-
-	if (priv->phydev)
-		return phy_ethtool_sset(priv->phydev, cmd);
-
-	return -EINVAL;
-}
-
-static int cvm_oct_nway_reset(struct net_device *dev)
-{
-	struct octeon_ethernet *priv = netdev_priv(dev);
-
-	if (!capable(CAP_NET_ADMIN))
-		return -EPERM;
-
-	if (priv->phydev)
-		return phy_start_aneg(priv->phydev);
-
-	return -EINVAL;
-}
-
-const struct ethtool_ops cvm_oct_ethtool_ops = {
-	.get_drvinfo = cvm_oct_get_drvinfo,
-	.get_settings = cvm_oct_get_settings,
-	.set_settings = cvm_oct_set_settings,
-	.nway_reset = cvm_oct_nway_reset,
-	.get_link = ethtool_op_get_link,
-};
-
-/**
- * cvm_oct_ioctl - IOCTL support for PHY control
- * @dev:    Device to change
- * @rq:     the request
- * @cmd:    the command
- *
- * Returns Zero on success
- */
-int cvm_oct_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)
-{
-	struct octeon_ethernet *priv = netdev_priv(dev);
-
-	if (!netif_running(dev))
-		return -EINVAL;
-
-	if (!priv->phydev)
-		return -EINVAL;
-
-	return phy_mii_ioctl(priv->phydev, rq, cmd);
-}
-
-static void cvm_oct_note_carrier(struct octeon_ethernet *priv,
-				 cvmx_helper_link_info_t li)
-{
-	if (li.s.link_up) {
-		pr_notice_ratelimited("%s: %u Mbps %s duplex, port %d\n",
-				      priv->netdev->name, li.s.speed,
-				      (li.s.full_duplex) ? "Full" : "Half",
-				      priv->port);
-	} else {
-		pr_notice_ratelimited("%s: Link down\n", priv->netdev->name);
-	}
-}
-
-/**
- * cvm_oct_set_carrier - common wrapper of netif_carrier_{on,off}
- *
- * @priv: Device struct.
- * @link_info: Current state.
- */
-void cvm_oct_set_carrier(struct octeon_ethernet *priv,
-			 cvmx_helper_link_info_t link_info)
-{
-	cvm_oct_note_carrier(priv, link_info);
-	if (link_info.s.link_up) {
-		if (!netif_carrier_ok(priv->netdev))
-			netif_carrier_on(priv->netdev);
-	} else {
-		if (netif_carrier_ok(priv->netdev))
-			netif_carrier_off(priv->netdev);
-	}
-}
-
-void cvm_oct_adjust_link(struct net_device *dev)
-{
-	struct octeon_ethernet *priv = netdev_priv(dev);
-	cvmx_helper_link_info_t link_info;
-
-	if (priv->last_link != priv->phydev->link) {
-		priv->last_link = priv->phydev->link;
-		link_info.u64 = 0;
-		link_info.s.link_up = priv->last_link ? 1 : 0;
-		link_info.s.full_duplex = priv->phydev->duplex ? 1 : 0;
-		link_info.s.speed = priv->phydev->speed;
-
-		cvmx_helper_link_set(priv->port, link_info);
-
-		cvm_oct_note_carrier(priv, link_info);
-	}
-}
-
-/**
- * cvm_oct_phy_setup_device - setup the PHY
- *
- * @dev:    Device to setup
- *
- * Returns Zero on success, negative on failure
- */
-int cvm_oct_phy_setup_device(struct net_device *dev)
-{
-	struct octeon_ethernet *priv = netdev_priv(dev);
-	struct device_node *phy_node;
-
-	if (!priv->of_node)
-		goto no_phy;
-
-	phy_node = of_parse_phandle(priv->of_node, "phy-handle", 0);
-	if (!phy_node)
-		goto no_phy;
-
-	priv->phydev = of_phy_connect(dev, phy_node, cvm_oct_adjust_link, 0,
-				      PHY_INTERFACE_MODE_GMII);
-
-	if (priv->phydev == NULL)
-		return -ENODEV;
-
-	priv->last_link = 0;
-	phy_start_aneg(priv->phydev);
-
-no_phy:
-	/*
-	 * If there is no phy, assume a direct MAC connection and that
-	 * the link is up.
-	 */
-	netif_carrier_on(dev);
-	return 0;
-}
diff --git a/drivers/staging/octeon/ethernet-mem.c b/drivers/staging/octeon/ethernet-mem.c
deleted file mode 100644
index 78b6cb7..0000000
--- a/drivers/staging/octeon/ethernet-mem.c
+++ /dev/null
@@ -1,177 +0,0 @@
-/**********************************************************************
- * Author: Cavium Networks
- *
- * Contact: support@caviumnetworks.com
- * This file is part of the OCTEON SDK
- *
- * Copyright (c) 2003-2010 Cavium Networks
- *
- * This file is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, Version 2, as
- * published by the Free Software Foundation.
- *
- * This file is distributed in the hope that it will be useful, but
- * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
- * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
- * NONINFRINGEMENT.  See the GNU General Public License for more
- * details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this file; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- * or visit http://www.gnu.org/licenses/.
- *
- * This file may also be available under a different license from Cavium.
- * Contact Cavium Networks for more information
-**********************************************************************/
-#include <linux/kernel.h>
-#include <linux/netdevice.h>
-#include <linux/slab.h>
-
-#include <asm/octeon/octeon.h>
-
-#include "ethernet-defines.h"
-
-#include <asm/octeon/cvmx-fpa.h>
-
-/**
- * cvm_oct_fill_hw_skbuff - fill the supplied hardware pool with skbuffs
- * @pool:     Pool to allocate an skbuff for
- * @size:     Size of the buffer needed for the pool
- * @elements: Number of buffers to allocate
- *
- * Returns the actual number of buffers allocated.
- */
-static int cvm_oct_fill_hw_skbuff(int pool, int size, int elements)
-{
-	int freed = elements;
-	while (freed) {
-
-		struct sk_buff *skb = dev_alloc_skb(size + 256);
-		if (unlikely(skb == NULL)) {
-			pr_warning
-			    ("Failed to allocate skb for hardware pool %d\n",
-			     pool);
-			break;
-		}
-
-		skb_reserve(skb, 256 - (((unsigned long)skb->data) & 0x7f));
-		*(struct sk_buff **)(skb->data - sizeof(void *)) = skb;
-		cvmx_fpa_free(skb->data, pool, DONT_WRITEBACK(size / 128));
-		freed--;
-	}
-	return elements - freed;
-}
-
-/**
- * cvm_oct_free_hw_skbuff- free hardware pool skbuffs
- * @pool:     Pool to allocate an skbuff for
- * @size:     Size of the buffer needed for the pool
- * @elements: Number of buffers to allocate
- */
-static void cvm_oct_free_hw_skbuff(int pool, int size, int elements)
-{
-	char *memory;
-
-	do {
-		memory = cvmx_fpa_alloc(pool);
-		if (memory) {
-			struct sk_buff *skb =
-			    *(struct sk_buff **)(memory - sizeof(void *));
-			elements--;
-			dev_kfree_skb(skb);
-		}
-	} while (memory);
-
-	if (elements < 0)
-		pr_warning("Freeing of pool %u had too many skbuffs (%d)\n",
-		     pool, elements);
-	else if (elements > 0)
-		pr_warning("Freeing of pool %u is missing %d skbuffs\n",
-		       pool, elements);
-}
-
-/**
- * cvm_oct_fill_hw_memory - fill a hardware pool with memory.
- * @pool:     Pool to populate
- * @size:     Size of each buffer in the pool
- * @elements: Number of buffers to allocate
- *
- * Returns the actual number of buffers allocated.
- */
-static int cvm_oct_fill_hw_memory(int pool, int size, int elements)
-{
-	char *memory;
-	char *fpa;
-	int freed = elements;
-
-	while (freed) {
-		/*
-		 * FPA memory must be 128 byte aligned.  Since we are
-		 * aligning we need to save the original pointer so we
-		 * can feed it to kfree when the memory is returned to
-		 * the kernel.
-		 *
-		 * We allocate an extra 256 bytes to allow for
-		 * alignment and space for the original pointer saved
-		 * just before the block.
-		 */
-		memory = kmalloc(size + 256, GFP_ATOMIC);
-		if (unlikely(memory == NULL)) {
-			pr_warning("Unable to allocate %u bytes for FPA pool %d\n",
-				   elements * size, pool);
-			break;
-		}
-		fpa = (char *)(((unsigned long)memory + 256) & ~0x7fUL);
-		*((char **)fpa - 1) = memory;
-		cvmx_fpa_free(fpa, pool, 0);
-		freed--;
-	}
-	return elements - freed;
-}
-
-/**
- * cvm_oct_free_hw_memory - Free memory allocated by cvm_oct_fill_hw_memory
- * @pool:     FPA pool to free
- * @size:     Size of each buffer in the pool
- * @elements: Number of buffers that should be in the pool
- */
-static void cvm_oct_free_hw_memory(int pool, int size, int elements)
-{
-	char *memory;
-	char *fpa;
-	do {
-		fpa = cvmx_fpa_alloc(pool);
-		if (fpa) {
-			elements--;
-			fpa = (char *)phys_to_virt(cvmx_ptr_to_phys(fpa));
-			memory = *((char **)fpa - 1);
-			kfree(memory);
-		}
-	} while (fpa);
-
-	if (elements < 0)
-		pr_warning("Freeing of pool %u had too many buffers (%d)\n",
-			pool, elements);
-	else if (elements > 0)
-		pr_warning("Warning: Freeing of pool %u is missing %d buffers\n",
-			pool, elements);
-}
-
-int cvm_oct_mem_fill_fpa(int pool, int size, int elements)
-{
-	int freed;
-	if (USE_SKBUFFS_IN_HW && pool == CVMX_FPA_PACKET_POOL)
-		freed = cvm_oct_fill_hw_skbuff(pool, size, elements);
-	else
-		freed = cvm_oct_fill_hw_memory(pool, size, elements);
-	return freed;
-}
-
-void cvm_oct_mem_empty_fpa(int pool, int size, int elements)
-{
-	if (USE_SKBUFFS_IN_HW && pool == CVMX_FPA_PACKET_POOL)
-		cvm_oct_free_hw_skbuff(pool, size, elements);
-	else
-		cvm_oct_free_hw_memory(pool, size, elements);
-}
diff --git a/drivers/staging/octeon/ethernet-mem.h b/drivers/staging/octeon/ethernet-mem.h
deleted file mode 100644
index 713f2ed..0000000
--- a/drivers/staging/octeon/ethernet-mem.h
+++ /dev/null
@@ -1,29 +0,0 @@
-/*********************************************************************
- * Author: Cavium Networks
- *
- * Contact: support@caviumnetworks.com
- * This file is part of the OCTEON SDK
- *
- * Copyright (c) 2003-2007 Cavium Networks
- *
- * This file is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, Version 2, as
- * published by the Free Software Foundation.
- *
- * This file is distributed in the hope that it will be useful, but
- * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
- * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
- * NONINFRINGEMENT.  See the GNU General Public License for more
- * details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this file; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- * or visit http://www.gnu.org/licenses/.
- *
- * This file may also be available under a different license from Cavium.
- * Contact Cavium Networks for more information
-********************************************************************/
-
-int cvm_oct_mem_fill_fpa(int pool, int size, int elements);
-void cvm_oct_mem_empty_fpa(int pool, int size, int elements);
diff --git a/drivers/staging/octeon/ethernet-rgmii.c b/drivers/staging/octeon/ethernet-rgmii.c
deleted file mode 100644
index 29940fd1c..0000000
--- a/drivers/staging/octeon/ethernet-rgmii.c
+++ /dev/null
@@ -1,415 +0,0 @@
-/*********************************************************************
- * Author: Cavium Networks
- *
- * Contact: support@caviumnetworks.com
- * This file is part of the OCTEON SDK
- *
- * Copyright (c) 2003-2007 Cavium Networks
- *
- * This file is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, Version 2, as
- * published by the Free Software Foundation.
- *
- * This file is distributed in the hope that it will be useful, but
- * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
- * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
- * NONINFRINGEMENT.  See the GNU General Public License for more
- * details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this file; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- * or visit http://www.gnu.org/licenses/.
- *
- * This file may also be available under a different license from Cavium.
- * Contact Cavium Networks for more information
-**********************************************************************/
-#include <linux/kernel.h>
-#include <linux/netdevice.h>
-#include <linux/interrupt.h>
-#include <linux/phy.h>
-#include <linux/ratelimit.h>
-#include <net/dst.h>
-
-#include <asm/octeon/octeon.h>
-
-#include "ethernet-defines.h"
-#include "octeon-ethernet.h"
-#include "ethernet-util.h"
-
-#include <asm/octeon/cvmx-helper.h>
-
-#include <asm/octeon/cvmx-ipd-defs.h>
-#include <asm/octeon/cvmx-npi-defs.h>
-#include <asm/octeon/cvmx-gmxx-defs.h>
-
-DEFINE_SPINLOCK(global_register_lock);
-
-static int number_rgmii_ports;
-
-static void cvm_oct_rgmii_poll(struct net_device *dev)
-{
-	struct octeon_ethernet *priv = netdev_priv(dev);
-	unsigned long flags = 0;
-	cvmx_helper_link_info_t link_info;
-	int use_global_register_lock = (priv->phydev == NULL);
-
-	BUG_ON(in_interrupt());
-	if (use_global_register_lock) {
-		/*
-		 * Take the global register lock since we are going to
-		 * touch registers that affect more than one port.
-		 */
-		spin_lock_irqsave(&global_register_lock, flags);
-	} else {
-		mutex_lock(&priv->phydev->bus->mdio_lock);
-	}
-
-	link_info = cvmx_helper_link_get(priv->port);
-	if (link_info.u64 == priv->link_info) {
-
-		/*
-		 * If the 10Mbps preamble workaround is supported and we're
-		 * at 10Mbps we may need to do some special checking.
-		 */
-		if (USE_10MBPS_PREAMBLE_WORKAROUND && (link_info.s.speed == 10)) {
-
-			/*
-			 * Read the GMXX_RXX_INT_REG[PCTERR] bit and
-			 * see if we are getting preamble errors.
-			 */
-			int interface = INTERFACE(priv->port);
-			int index = INDEX(priv->port);
-			union cvmx_gmxx_rxx_int_reg gmxx_rxx_int_reg;
-			gmxx_rxx_int_reg.u64 =
-			    cvmx_read_csr(CVMX_GMXX_RXX_INT_REG
-					  (index, interface));
-			if (gmxx_rxx_int_reg.s.pcterr) {
-
-				/*
-				 * We are getting preamble errors at
-				 * 10Mbps.  Most likely the PHY is
-				 * giving us packets with mis aligned
-				 * preambles. In order to get these
-				 * packets we need to disable preamble
-				 * checking and do it in software.
-				 */
-				union cvmx_gmxx_rxx_frm_ctl gmxx_rxx_frm_ctl;
-				union cvmx_ipd_sub_port_fcs ipd_sub_port_fcs;
-
-				/* Disable preamble checking */
-				gmxx_rxx_frm_ctl.u64 =
-				    cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL
-						  (index, interface));
-				gmxx_rxx_frm_ctl.s.pre_chk = 0;
-				cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL
-					       (index, interface),
-					       gmxx_rxx_frm_ctl.u64);
-
-				/* Disable FCS stripping */
-				ipd_sub_port_fcs.u64 =
-				    cvmx_read_csr(CVMX_IPD_SUB_PORT_FCS);
-				ipd_sub_port_fcs.s.port_bit &=
-				    0xffffffffull ^ (1ull << priv->port);
-				cvmx_write_csr(CVMX_IPD_SUB_PORT_FCS,
-					       ipd_sub_port_fcs.u64);
-
-				/* Clear any error bits */
-				cvmx_write_csr(CVMX_GMXX_RXX_INT_REG
-					       (index, interface),
-					       gmxx_rxx_int_reg.u64);
-				printk_ratelimited("%s: Using 10Mbps with software "
-						   "preamble removal\n",
-						   dev->name);
-			}
-		}
-
-		if (use_global_register_lock)
-			spin_unlock_irqrestore(&global_register_lock, flags);
-		else
-			mutex_unlock(&priv->phydev->bus->mdio_lock);
-		return;
-	}
-
-	/* If the 10Mbps preamble workaround is allowed we need to on
-	   preamble checking, FCS stripping, and clear error bits on
-	   every speed change. If errors occur during 10Mbps operation
-	   the above code will change this stuff */
-	if (USE_10MBPS_PREAMBLE_WORKAROUND) {
-
-		union cvmx_gmxx_rxx_frm_ctl gmxx_rxx_frm_ctl;
-		union cvmx_ipd_sub_port_fcs ipd_sub_port_fcs;
-		union cvmx_gmxx_rxx_int_reg gmxx_rxx_int_reg;
-		int interface = INTERFACE(priv->port);
-		int index = INDEX(priv->port);
-
-		/* Enable preamble checking */
-		gmxx_rxx_frm_ctl.u64 =
-		    cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface));
-		gmxx_rxx_frm_ctl.s.pre_chk = 1;
-		cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface),
-			       gmxx_rxx_frm_ctl.u64);
-		/* Enable FCS stripping */
-		ipd_sub_port_fcs.u64 = cvmx_read_csr(CVMX_IPD_SUB_PORT_FCS);
-		ipd_sub_port_fcs.s.port_bit |= 1ull << priv->port;
-		cvmx_write_csr(CVMX_IPD_SUB_PORT_FCS, ipd_sub_port_fcs.u64);
-		/* Clear any error bits */
-		gmxx_rxx_int_reg.u64 =
-		    cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(index, interface));
-		cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(index, interface),
-			       gmxx_rxx_int_reg.u64);
-	}
-	if (priv->phydev == NULL) {
-		link_info = cvmx_helper_link_autoconf(priv->port);
-		priv->link_info = link_info.u64;
-	}
-
-	if (use_global_register_lock)
-		spin_unlock_irqrestore(&global_register_lock, flags);
-	else {
-		mutex_unlock(&priv->phydev->bus->mdio_lock);
-	}
-
-	if (priv->phydev == NULL)
-		cvm_oct_set_carrier(priv, link_info);
-}
-
-static irqreturn_t cvm_oct_rgmii_rml_interrupt(int cpl, void *dev_id)
-{
-	union cvmx_npi_rsl_int_blocks rsl_int_blocks;
-	int index;
-	irqreturn_t return_status = IRQ_NONE;
-
-	rsl_int_blocks.u64 = cvmx_read_csr(CVMX_NPI_RSL_INT_BLOCKS);
-
-	/* Check and see if this interrupt was caused by the GMX0 block */
-	if (rsl_int_blocks.s.gmx0) {
-
-		int interface = 0;
-		/* Loop through every port of this interface */
-		for (index = 0;
-		     index < cvmx_helper_ports_on_interface(interface);
-		     index++) {
-
-			/* Read the GMX interrupt status bits */
-			union cvmx_gmxx_rxx_int_reg gmx_rx_int_reg;
-			gmx_rx_int_reg.u64 =
-			    cvmx_read_csr(CVMX_GMXX_RXX_INT_REG
-					  (index, interface));
-			gmx_rx_int_reg.u64 &=
-			    cvmx_read_csr(CVMX_GMXX_RXX_INT_EN
-					  (index, interface));
-			/* Poll the port if inband status changed */
-			if (gmx_rx_int_reg.s.phy_dupx
-			    || gmx_rx_int_reg.s.phy_link
-			    || gmx_rx_int_reg.s.phy_spd) {
-
-				struct net_device *dev =
-				    cvm_oct_device[cvmx_helper_get_ipd_port
-						   (interface, index)];
-				struct octeon_ethernet *priv = netdev_priv(dev);
-
-				if (dev && !atomic_read(&cvm_oct_poll_queue_stopping))
-					queue_work(cvm_oct_poll_queue, &priv->port_work);
-
-				gmx_rx_int_reg.u64 = 0;
-				gmx_rx_int_reg.s.phy_dupx = 1;
-				gmx_rx_int_reg.s.phy_link = 1;
-				gmx_rx_int_reg.s.phy_spd = 1;
-				cvmx_write_csr(CVMX_GMXX_RXX_INT_REG
-					       (index, interface),
-					       gmx_rx_int_reg.u64);
-				return_status = IRQ_HANDLED;
-			}
-		}
-	}
-
-	/* Check and see if this interrupt was caused by the GMX1 block */
-	if (rsl_int_blocks.s.gmx1) {
-
-		int interface = 1;
-		/* Loop through every port of this interface */
-		for (index = 0;
-		     index < cvmx_helper_ports_on_interface(interface);
-		     index++) {
-
-			/* Read the GMX interrupt status bits */
-			union cvmx_gmxx_rxx_int_reg gmx_rx_int_reg;
-			gmx_rx_int_reg.u64 =
-			    cvmx_read_csr(CVMX_GMXX_RXX_INT_REG
-					  (index, interface));
-			gmx_rx_int_reg.u64 &=
-			    cvmx_read_csr(CVMX_GMXX_RXX_INT_EN
-					  (index, interface));
-			/* Poll the port if inband status changed */
-			if (gmx_rx_int_reg.s.phy_dupx
-			    || gmx_rx_int_reg.s.phy_link
-			    || gmx_rx_int_reg.s.phy_spd) {
-
-				struct net_device *dev =
-				    cvm_oct_device[cvmx_helper_get_ipd_port
-						   (interface, index)];
-				struct octeon_ethernet *priv = netdev_priv(dev);
-
-				if (dev && !atomic_read(&cvm_oct_poll_queue_stopping))
-					queue_work(cvm_oct_poll_queue, &priv->port_work);
-
-				gmx_rx_int_reg.u64 = 0;
-				gmx_rx_int_reg.s.phy_dupx = 1;
-				gmx_rx_int_reg.s.phy_link = 1;
-				gmx_rx_int_reg.s.phy_spd = 1;
-				cvmx_write_csr(CVMX_GMXX_RXX_INT_REG
-					       (index, interface),
-					       gmx_rx_int_reg.u64);
-				return_status = IRQ_HANDLED;
-			}
-		}
-	}
-	return return_status;
-}
-
-static void cvm_oct_rgmii_immediate_poll(struct work_struct *work)
-{
-	struct octeon_ethernet *priv = container_of(work, struct octeon_ethernet, port_work);
-	cvm_oct_rgmii_poll(priv->netdev);
-}
-
-int cvm_oct_rgmii_open(struct net_device *dev)
-{
-	union cvmx_gmxx_prtx_cfg gmx_cfg;
-	struct octeon_ethernet *priv = netdev_priv(dev);
-	int interface = INTERFACE(priv->port);
-	int index = INDEX(priv->port);
-	cvmx_helper_link_info_t link_info;
-	int rv;
-
-	rv = cvm_oct_phy_setup_device(dev);
-	if (rv)
-		return rv;
-
-
-	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
-	gmx_cfg.s.en = 1;
-	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
-
-	if (!octeon_is_simulation()) {
-		if (priv->phydev) {
-			int r = phy_read_status(priv->phydev);
-			if (r == 0 && priv->phydev->link == 0)
-				netif_carrier_off(dev);
-			cvm_oct_adjust_link(dev);
-		} else {
-			link_info = cvmx_helper_link_get(priv->port);
-			if (!link_info.s.link_up)
-				netif_carrier_off(dev);
-			spin_lock(&priv->poll_lock);
-			priv->poll = cvm_oct_rgmii_poll;
-			spin_unlock(&priv->poll_lock);
-		}
-	}
-
-	INIT_WORK(&priv->port_work, cvm_oct_rgmii_immediate_poll);
-	/*
-	 * Due to GMX errata in CN3XXX series chips, it is necessary
-	 * to take the link down immediately when the PHY changes
-	 * state. In order to do this we call the poll function every
-	 * time the RGMII inband status changes.  This may cause
-	 * problems if the PHY doesn't implement inband status
-	 * properly.
-	 */
-	if (number_rgmii_ports == 0) {
-		rv = request_irq(OCTEON_IRQ_RML, cvm_oct_rgmii_rml_interrupt,
-				 IRQF_SHARED, "RGMII", &number_rgmii_ports);
-		if (rv != 0)
-			return rv;
-	}
-	number_rgmii_ports++;
-
-	/*
-	 * Only true RGMII ports need to be polled. In GMII mode, port
-	 * 0 is really a RGMII port.
-	 */
-	if (((priv->imode == CVMX_HELPER_INTERFACE_MODE_GMII)
-	     && (priv->port == 0))
-	    || (priv->imode == CVMX_HELPER_INTERFACE_MODE_RGMII)) {
-
-		if (!octeon_is_simulation()) {
-
-			union cvmx_gmxx_rxx_int_en gmx_rx_int_en;
-			int interface = INTERFACE(priv->port);
-			int index = INDEX(priv->port);
-
-			/*
-			 * Enable interrupts on inband status changes
-			 * for this port.
-			 */
-			gmx_rx_int_en.u64 =
-			    cvmx_read_csr(CVMX_GMXX_RXX_INT_EN
-					  (index, interface));
-			gmx_rx_int_en.s.phy_dupx = 1;
-			gmx_rx_int_en.s.phy_link = 1;
-			gmx_rx_int_en.s.phy_spd = 1;
-			cvmx_write_csr(CVMX_GMXX_RXX_INT_EN(index, interface),
-				       gmx_rx_int_en.u64);
-		}
-	}
-
-	return 0;
-}
-
-int cvm_oct_rgmii_stop(struct net_device *dev)
-{
-	union cvmx_gmxx_prtx_cfg gmx_cfg;
-	struct octeon_ethernet *priv = netdev_priv(dev);
-	int interface = INTERFACE(priv->port);
-	int index = INDEX(priv->port);
-
-	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
-	gmx_cfg.s.en = 0;
-	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
-
-	/*
-	 * Only true RGMII ports need to be polled. In GMII mode, port
-	 * 0 is really a RGMII port.
-	 */
-	if (((priv->imode == CVMX_HELPER_INTERFACE_MODE_GMII)
-	     && (priv->port == 0))
-	    || (priv->imode == CVMX_HELPER_INTERFACE_MODE_RGMII)) {
-
-		if (!octeon_is_simulation()) {
-
-			union cvmx_gmxx_rxx_int_en gmx_rx_int_en;
-			int interface = INTERFACE(priv->port);
-			int index = INDEX(priv->port);
-
-			/*
-			 * Disable interrupts on inband status changes
-			 * for this port.
-			 */
-			gmx_rx_int_en.u64 =
-			    cvmx_read_csr(CVMX_GMXX_RXX_INT_EN
-					  (index, interface));
-			gmx_rx_int_en.s.phy_dupx = 0;
-			gmx_rx_int_en.s.phy_link = 0;
-			gmx_rx_int_en.s.phy_spd = 0;
-			cvmx_write_csr(CVMX_GMXX_RXX_INT_EN(index, interface),
-				       gmx_rx_int_en.u64);
-		}
-	}
-
-	/* Remove the interrupt handler when the last port is removed. */
-	number_rgmii_ports--;
-	if (number_rgmii_ports == 0)
-		free_irq(OCTEON_IRQ_RML, &number_rgmii_ports);
-	cancel_work_sync(&priv->port_work);
-
-	return 0;
-}
-
-int cvm_oct_rgmii_init(struct net_device *dev)
-{
-	cvm_oct_common_init(dev);
-	dev->netdev_ops->ndo_stop(dev);
-
-	return 0;
-}
diff --git a/drivers/staging/octeon/ethernet-rx.c b/drivers/staging/octeon/ethernet-rx.c
deleted file mode 100644
index 34afc16b..0000000
--- a/drivers/staging/octeon/ethernet-rx.c
+++ /dev/null
@@ -1,563 +0,0 @@
-/**********************************************************************
- * Author: Cavium Networks
- *
- * Contact: support@caviumnetworks.com
- * This file is part of the OCTEON SDK
- *
- * Copyright (c) 2003-2010 Cavium Networks
- *
- * This file is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, Version 2, as
- * published by the Free Software Foundation.
- *
- * This file is distributed in the hope that it will be useful, but
- * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
- * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
- * NONINFRINGEMENT.  See the GNU General Public License for more
- * details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this file; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- * or visit http://www.gnu.org/licenses/.
- *
- * This file may also be available under a different license from Cavium.
- * Contact Cavium Networks for more information
-**********************************************************************/
-#include <linux/module.h>
-#include <linux/kernel.h>
-#include <linux/cache.h>
-#include <linux/cpumask.h>
-#include <linux/netdevice.h>
-#include <linux/init.h>
-#include <linux/etherdevice.h>
-#include <linux/ip.h>
-#include <linux/string.h>
-#include <linux/prefetch.h>
-#include <linux/ratelimit.h>
-#include <linux/smp.h>
-#include <linux/interrupt.h>
-#include <net/dst.h>
-#ifdef CONFIG_XFRM
-#include <linux/xfrm.h>
-#include <net/xfrm.h>
-#endif /* CONFIG_XFRM */
-
-#include <linux/atomic.h>
-
-#include <asm/octeon/octeon.h>
-
-#include "ethernet-defines.h"
-#include "ethernet-mem.h"
-#include "ethernet-rx.h"
-#include "octeon-ethernet.h"
-#include "ethernet-util.h"
-
-#include <asm/octeon/cvmx-helper.h>
-#include <asm/octeon/cvmx-wqe.h>
-#include <asm/octeon/cvmx-fau.h>
-#include <asm/octeon/cvmx-pow.h>
-#include <asm/octeon/cvmx-pip.h>
-#include <asm/octeon/cvmx-scratch.h>
-
-#include <asm/octeon/cvmx-gmxx-defs.h>
-
-struct cvm_napi_wrapper {
-	struct napi_struct napi;
-} ____cacheline_aligned_in_smp;
-
-static struct cvm_napi_wrapper cvm_oct_napi[NR_CPUS] __cacheline_aligned_in_smp;
-
-struct cvm_oct_core_state {
-	int baseline_cores;
-	/*
-	 * The number of additional cores that could be processing
-	 * input packtes.
-	 */
-	atomic_t available_cores;
-	cpumask_t cpu_state;
-} ____cacheline_aligned_in_smp;
-
-static struct cvm_oct_core_state core_state __cacheline_aligned_in_smp;
-
-static void cvm_oct_enable_napi(void *_)
-{
-	int cpu = smp_processor_id();
-	napi_schedule(&cvm_oct_napi[cpu].napi);
-}
-
-static void cvm_oct_enable_one_cpu(void)
-{
-	int v;
-	int cpu;
-
-	/* Check to see if more CPUs are available for receive processing... */
-	v = atomic_sub_if_positive(1, &core_state.available_cores);
-	if (v < 0)
-		return;
-
-	/* ... if a CPU is available, Turn on NAPI polling for that CPU.  */
-	for_each_online_cpu(cpu) {
-		if (!cpu_test_and_set(cpu, core_state.cpu_state)) {
-			v = smp_call_function_single(cpu, cvm_oct_enable_napi,
-						     NULL, 0);
-			if (v)
-				panic("Can't enable NAPI.");
-			break;
-		}
-	}
-}
-
-static void cvm_oct_no_more_work(void)
-{
-	int cpu = smp_processor_id();
-
-	/*
-	 * CPU zero is special.  It always has the irq enabled when
-	 * waiting for incoming packets.
-	 */
-	if (cpu == 0) {
-		enable_irq(OCTEON_IRQ_WORKQ0 + pow_receive_group);
-		return;
-	}
-
-	cpu_clear(cpu, core_state.cpu_state);
-	atomic_add(1, &core_state.available_cores);
-}
-
-/**
- * cvm_oct_do_interrupt - interrupt handler.
- *
- * The interrupt occurs whenever the POW has packets in our group.
- *
- */
-static irqreturn_t cvm_oct_do_interrupt(int cpl, void *dev_id)
-{
-	/* Disable the IRQ and start napi_poll. */
-	disable_irq_nosync(OCTEON_IRQ_WORKQ0 + pow_receive_group);
-	cvm_oct_enable_napi(NULL);
-
-	return IRQ_HANDLED;
-}
-
-/**
- * cvm_oct_check_rcv_error - process receive errors
- * @work: Work queue entry pointing to the packet.
- *
- * Returns Non-zero if the packet can be dropped, zero otherwise.
- */
-static inline int cvm_oct_check_rcv_error(cvmx_wqe_t *work)
-{
-	if ((work->word2.snoip.err_code == 10) && (work->len <= 64)) {
-		/*
-		 * Ignore length errors on min size packets. Some
-		 * equipment incorrectly pads packets to 64+4FCS
-		 * instead of 60+4FCS.  Note these packets still get
-		 * counted as frame errors.
-		 */
-	} else
-	    if (USE_10MBPS_PREAMBLE_WORKAROUND
-		&& ((work->word2.snoip.err_code == 5)
-		    || (work->word2.snoip.err_code == 7))) {
-
-		/*
-		 * We received a packet with either an alignment error
-		 * or a FCS error. This may be signalling that we are
-		 * running 10Mbps with GMXX_RXX_FRM_CTL[PRE_CHK]
-		 * off. If this is the case we need to parse the
-		 * packet to determine if we can remove a non spec
-		 * preamble and generate a correct packet.
-		 */
-		int interface = cvmx_helper_get_interface_num(work->ipprt);
-		int index = cvmx_helper_get_interface_index_num(work->ipprt);
-		union cvmx_gmxx_rxx_frm_ctl gmxx_rxx_frm_ctl;
-		gmxx_rxx_frm_ctl.u64 =
-		    cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface));
-		if (gmxx_rxx_frm_ctl.s.pre_chk == 0) {
-
-			uint8_t *ptr =
-			    cvmx_phys_to_ptr(work->packet_ptr.s.addr);
-			int i = 0;
-
-			while (i < work->len - 1) {
-				if (*ptr != 0x55)
-					break;
-				ptr++;
-				i++;
-			}
-
-			if (*ptr == 0xd5) {
-				/*
-				  printk_ratelimited("Port %d received 0xd5 preamble\n", work->ipprt);
-				 */
-				work->packet_ptr.s.addr += i + 1;
-				work->len -= i + 5;
-			} else if ((*ptr & 0xf) == 0xd) {
-				/*
-				  printk_ratelimited("Port %d received 0x?d preamble\n", work->ipprt);
-				 */
-				work->packet_ptr.s.addr += i;
-				work->len -= i + 4;
-				for (i = 0; i < work->len; i++) {
-					*ptr =
-					    ((*ptr & 0xf0) >> 4) |
-					    ((*(ptr + 1) & 0xf) << 4);
-					ptr++;
-				}
-			} else {
-				printk_ratelimited("Port %d unknown preamble, packet "
-						   "dropped\n",
-						   work->ipprt);
-				/*
-				   cvmx_helper_dump_packet(work);
-				 */
-				cvm_oct_free_work(work);
-				return 1;
-			}
-		}
-	} else {
-		printk_ratelimited("Port %d receive error code %d, packet dropped\n",
-				   work->ipprt, work->word2.snoip.err_code);
-		cvm_oct_free_work(work);
-		return 1;
-	}
-
-	return 0;
-}
-
-/**
- * cvm_oct_napi_poll - the NAPI poll function.
- * @napi: The NAPI instance, or null if called from cvm_oct_poll_controller
- * @budget: Maximum number of packets to receive.
- *
- * Returns the number of packets processed.
- */
-static int cvm_oct_napi_poll(struct napi_struct *napi, int budget)
-{
-	const int	coreid = cvmx_get_core_num();
-	uint64_t	old_group_mask;
-	uint64_t	old_scratch;
-	int		rx_count = 0;
-	int		did_work_request = 0;
-	int		packet_not_copied;
-
-	/* Prefetch cvm_oct_device since we know we need it soon */
-	prefetch(cvm_oct_device);
-
-	if (USE_ASYNC_IOBDMA) {
-		/* Save scratch in case userspace is using it */
-		CVMX_SYNCIOBDMA;
-		old_scratch = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
-	}
-
-	/* Only allow work for our group (and preserve priorities) */
-	old_group_mask = cvmx_read_csr(CVMX_POW_PP_GRP_MSKX(coreid));
-	cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(coreid),
-		       (old_group_mask & ~0xFFFFull) | 1 << pow_receive_group);
-
-	if (USE_ASYNC_IOBDMA) {
-		cvmx_pow_work_request_async(CVMX_SCR_SCRATCH, CVMX_POW_NO_WAIT);
-		did_work_request = 1;
-	}
-
-	while (rx_count < budget) {
-		struct sk_buff *skb = NULL;
-		struct sk_buff **pskb = NULL;
-		int skb_in_hw;
-		cvmx_wqe_t *work;
-
-		if (USE_ASYNC_IOBDMA && did_work_request)
-			work = cvmx_pow_work_response_async(CVMX_SCR_SCRATCH);
-		else
-			work = cvmx_pow_work_request_sync(CVMX_POW_NO_WAIT);
-
-		prefetch(work);
-		did_work_request = 0;
-		if (work == NULL) {
-			union cvmx_pow_wq_int wq_int;
-			wq_int.u64 = 0;
-			wq_int.s.iq_dis = 1 << pow_receive_group;
-			wq_int.s.wq_int = 1 << pow_receive_group;
-			cvmx_write_csr(CVMX_POW_WQ_INT, wq_int.u64);
-			break;
-		}
-		pskb = (struct sk_buff **)(cvm_oct_get_buffer_ptr(work->packet_ptr) - sizeof(void *));
-		prefetch(pskb);
-
-		if (USE_ASYNC_IOBDMA && rx_count < (budget - 1)) {
-			cvmx_pow_work_request_async_nocheck(CVMX_SCR_SCRATCH, CVMX_POW_NO_WAIT);
-			did_work_request = 1;
-		}
-
-		if (rx_count == 0) {
-			/*
-			 * First time through, see if there is enough
-			 * work waiting to merit waking another
-			 * CPU.
-			 */
-			union cvmx_pow_wq_int_cntx counts;
-			int backlog;
-			int cores_in_use = core_state.baseline_cores - atomic_read(&core_state.available_cores);
-			counts.u64 = cvmx_read_csr(CVMX_POW_WQ_INT_CNTX(pow_receive_group));
-			backlog = counts.s.iq_cnt + counts.s.ds_cnt;
-			if (backlog > budget * cores_in_use && napi != NULL)
-				cvm_oct_enable_one_cpu();
-		}
-
-		skb_in_hw = USE_SKBUFFS_IN_HW && work->word2.s.bufs == 1;
-		if (likely(skb_in_hw)) {
-			skb = *pskb;
-			prefetch(&skb->head);
-			prefetch(&skb->len);
-		}
-		prefetch(cvm_oct_device[work->ipprt]);
-
-		/* Immediately throw away all packets with receive errors */
-		if (unlikely(work->word2.snoip.rcv_error)) {
-			if (cvm_oct_check_rcv_error(work))
-				continue;
-		}
-
-		/*
-		 * We can only use the zero copy path if skbuffs are
-		 * in the FPA pool and the packet fits in a single
-		 * buffer.
-		 */
-		if (likely(skb_in_hw)) {
-			skb->data = skb->head + work->packet_ptr.s.addr - cvmx_ptr_to_phys(skb->head);
-			prefetch(skb->data);
-			skb->len = work->len;
-			skb_set_tail_pointer(skb, skb->len);
-			packet_not_copied = 1;
-		} else {
-			/*
-			 * We have to copy the packet. First allocate
-			 * an skbuff for it.
-			 */
-			skb = dev_alloc_skb(work->len);
-			if (!skb) {
-				printk_ratelimited("Port %d failed to allocate "
-						   "skbuff, packet dropped\n",
-						   work->ipprt);
-				cvm_oct_free_work(work);
-				continue;
-			}
-
-			/*
-			 * Check if we've received a packet that was
-			 * entirely stored in the work entry.
-			 */
-			if (unlikely(work->word2.s.bufs == 0)) {
-				uint8_t *ptr = work->packet_data;
-
-				if (likely(!work->word2.s.not_IP)) {
-					/*
-					 * The beginning of the packet
-					 * moves for IP packets.
-					 */
-					if (work->word2.s.is_v6)
-						ptr += 2;
-					else
-						ptr += 6;
-				}
-				memcpy(skb_put(skb, work->len), ptr, work->len);
-				/* No packet buffers to free */
-			} else {
-				int segments = work->word2.s.bufs;
-				union cvmx_buf_ptr segment_ptr = work->packet_ptr;
-				int len = work->len;
-
-				while (segments--) {
-					union cvmx_buf_ptr next_ptr =
-					    *(union cvmx_buf_ptr *)cvmx_phys_to_ptr(segment_ptr.s.addr - 8);
-
-			/*
-			 * Octeon Errata PKI-100: The segment size is
-			 * wrong. Until it is fixed, calculate the
-			 * segment size based on the packet pool
-			 * buffer size. When it is fixed, the
-			 * following line should be replaced with this
-			 * one: int segment_size =
-			 * segment_ptr.s.size;
-			 */
-					int segment_size = CVMX_FPA_PACKET_POOL_SIZE -
-						(segment_ptr.s.addr - (((segment_ptr.s.addr >> 7) - segment_ptr.s.back) << 7));
-					/*
-					 * Don't copy more than what
-					 * is left in the packet.
-					 */
-					if (segment_size > len)
-						segment_size = len;
-					/* Copy the data into the packet */
-					memcpy(skb_put(skb, segment_size),
-					       cvmx_phys_to_ptr(segment_ptr.s.addr),
-					       segment_size);
-					len -= segment_size;
-					segment_ptr = next_ptr;
-				}
-			}
-			packet_not_copied = 0;
-		}
-
-		if (likely((work->ipprt < TOTAL_NUMBER_OF_PORTS) &&
-			   cvm_oct_device[work->ipprt])) {
-			struct net_device *dev = cvm_oct_device[work->ipprt];
-			struct octeon_ethernet *priv = netdev_priv(dev);
-
-			/*
-			 * Only accept packets for devices that are
-			 * currently up.
-			 */
-			if (likely(dev->flags & IFF_UP)) {
-				skb->protocol = eth_type_trans(skb, dev);
-				skb->dev = dev;
-
-				if (unlikely(work->word2.s.not_IP || work->word2.s.IP_exc ||
-					work->word2.s.L4_error || !work->word2.s.tcp_or_udp))
-					skb->ip_summed = CHECKSUM_NONE;
-				else
-					skb->ip_summed = CHECKSUM_UNNECESSARY;
-
-				/* Increment RX stats for virtual ports */
-				if (work->ipprt >= CVMX_PIP_NUM_INPUT_PORTS) {
-#ifdef CONFIG_64BIT
-					atomic64_add(1, (atomic64_t *)&priv->stats.rx_packets);
-					atomic64_add(skb->len, (atomic64_t *)&priv->stats.rx_bytes);
-#else
-					atomic_add(1, (atomic_t *)&priv->stats.rx_packets);
-					atomic_add(skb->len, (atomic_t *)&priv->stats.rx_bytes);
-#endif
-				}
-				netif_receive_skb(skb);
-				rx_count++;
-			} else {
-				/* Drop any packet received for a device that isn't up */
-				/*
-				  printk_ratelimited("%s: Device not up, packet dropped\n",
-					   dev->name);
-				*/
-#ifdef CONFIG_64BIT
-				atomic64_add(1, (atomic64_t *)&priv->stats.rx_dropped);
-#else
-				atomic_add(1, (atomic_t *)&priv->stats.rx_dropped);
-#endif
-				dev_kfree_skb_irq(skb);
-			}
-		} else {
-			/*
-			 * Drop any packet received for a device that
-			 * doesn't exist.
-			 */
-			printk_ratelimited("Port %d not controlled by Linux, packet dropped\n",
-				   work->ipprt);
-			dev_kfree_skb_irq(skb);
-		}
-		/*
-		 * Check to see if the skbuff and work share the same
-		 * packet buffer.
-		 */
-		if (USE_SKBUFFS_IN_HW && likely(packet_not_copied)) {
-			/*
-			 * This buffer needs to be replaced, increment
-			 * the number of buffers we need to free by
-			 * one.
-			 */
-			cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE,
-					      1);
-
-			cvmx_fpa_free(work, CVMX_FPA_WQE_POOL,
-				      DONT_WRITEBACK(1));
-		} else {
-			cvm_oct_free_work(work);
-		}
-	}
-	/* Restore the original POW group mask */
-	cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(coreid), old_group_mask);
-	if (USE_ASYNC_IOBDMA) {
-		/* Restore the scratch area */
-		cvmx_scratch_write64(CVMX_SCR_SCRATCH, old_scratch);
-	}
-	cvm_oct_rx_refill_pool(0);
-
-	if (rx_count < budget && napi != NULL) {
-		/* No more work */
-		napi_complete(napi);
-		cvm_oct_no_more_work();
-	}
-	return rx_count;
-}
-
-#ifdef CONFIG_NET_POLL_CONTROLLER
-/**
- * cvm_oct_poll_controller - poll for receive packets
- * device.
- *
- * @dev:    Device to poll. Unused
- */
-void cvm_oct_poll_controller(struct net_device *dev)
-{
-	cvm_oct_napi_poll(NULL, 16);
-}
-#endif
-
-void cvm_oct_rx_initialize(void)
-{
-	int i;
-	struct net_device *dev_for_napi = NULL;
-	union cvmx_pow_wq_int_thrx int_thr;
-	union cvmx_pow_wq_int_pc int_pc;
-
-	for (i = 0; i < TOTAL_NUMBER_OF_PORTS; i++) {
-		if (cvm_oct_device[i]) {
-			dev_for_napi = cvm_oct_device[i];
-			break;
-		}
-	}
-
-	if (NULL == dev_for_napi)
-		panic("No net_devices were allocated.");
-
-	if (max_rx_cpus > 1  && max_rx_cpus < num_online_cpus())
-		atomic_set(&core_state.available_cores, max_rx_cpus);
-	else
-		atomic_set(&core_state.available_cores, num_online_cpus());
-	core_state.baseline_cores = atomic_read(&core_state.available_cores);
-
-	core_state.cpu_state = CPU_MASK_NONE;
-	for_each_possible_cpu(i) {
-		netif_napi_add(dev_for_napi, &cvm_oct_napi[i].napi,
-			       cvm_oct_napi_poll, rx_napi_weight);
-		napi_enable(&cvm_oct_napi[i].napi);
-	}
-	/* Register an IRQ hander for to receive POW interrupts */
-	i = request_irq(OCTEON_IRQ_WORKQ0 + pow_receive_group,
-			cvm_oct_do_interrupt, 0, "Ethernet", cvm_oct_device);
-
-	if (i)
-		panic("Could not acquire Ethernet IRQ %d\n",
-		      OCTEON_IRQ_WORKQ0 + pow_receive_group);
-
-	disable_irq_nosync(OCTEON_IRQ_WORKQ0 + pow_receive_group);
-
-	int_thr.u64 = 0;
-	int_thr.s.tc_en = 1;
-	int_thr.s.tc_thr = 1;
-	/* Enable POW interrupt when our port has at least one packet */
-	cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group), int_thr.u64);
-
-	int_pc.u64 = 0;
-	int_pc.s.pc_thr = 5;
-	cvmx_write_csr(CVMX_POW_WQ_INT_PC, int_pc.u64);
-
-
-	/* Scheduld NAPI now.  This will indirectly enable interrupts. */
-	cvm_oct_enable_one_cpu();
-}
-
-void cvm_oct_rx_shutdown(void)
-{
-	int i;
-	/* Shutdown all of the NAPIs */
-	for_each_possible_cpu(i)
-		netif_napi_del(&cvm_oct_napi[i].napi);
-}
diff --git a/drivers/staging/octeon/ethernet-rx.h b/drivers/staging/octeon/ethernet-rx.h
deleted file mode 100644
index 9240c85..0000000
--- a/drivers/staging/octeon/ethernet-rx.h
+++ /dev/null
@@ -1,52 +0,0 @@
-/*********************************************************************
- * Author: Cavium Networks
- *
- * Contact: support@caviumnetworks.com
- * This file is part of the OCTEON SDK
- *
- * Copyright (c) 2003-2007 Cavium Networks
- *
- * This file is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, Version 2, as
- * published by the Free Software Foundation.
- *
- * This file is distributed in the hope that it will be useful, but
- * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
- * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
- * NONINFRINGEMENT.  See the GNU General Public License for more
- * details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this file; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- * or visit http://www.gnu.org/licenses/.
- *
- * This file may also be available under a different license from Cavium.
- * Contact Cavium Networks for more information
-*********************************************************************/
-#include <asm/octeon/cvmx-fau.h>
-
-void cvm_oct_poll_controller(struct net_device *dev);
-void cvm_oct_rx_initialize(void);
-void cvm_oct_rx_shutdown(void);
-
-static inline void cvm_oct_rx_refill_pool(int fill_threshold)
-{
-	int number_to_free;
-	int num_freed;
-	/* Refill the packet buffer pool */
-	number_to_free =
-		cvmx_fau_fetch_and_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);
-
-	if (number_to_free > fill_threshold) {
-		cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE,
-				      -number_to_free);
-		num_freed = cvm_oct_mem_fill_fpa(CVMX_FPA_PACKET_POOL,
-						 CVMX_FPA_PACKET_POOL_SIZE,
-						 number_to_free);
-		if (num_freed != number_to_free) {
-			cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE,
-					number_to_free - num_freed);
-		}
-	}
-}
diff --git a/drivers/staging/octeon/ethernet-sgmii.c b/drivers/staging/octeon/ethernet-sgmii.c
deleted file mode 100644
index fb7e441..0000000
--- a/drivers/staging/octeon/ethernet-sgmii.c
+++ /dev/null
@@ -1,128 +0,0 @@
-/**********************************************************************
- * Author: Cavium Networks
- *
- * Contact: support@caviumnetworks.com
- * This file is part of the OCTEON SDK
- *
- * Copyright (c) 2003-2012 Cavium, Inc.
- *
- * This file is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, Version 2, as
- * published by the Free Software Foundation.
- *
- * This file is distributed in the hope that it will be useful, but
- * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
- * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
- * NONINFRINGEMENT.  See the GNU General Public License for more
- * details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this file; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- * or visit http://www.gnu.org/licenses/.
- *
- * This file may also be available under a different license from Cavium.
- * Contact Cavium, Inc. for more information
-**********************************************************************/
-#include <linux/phy.h>
-#include <linux/kernel.h>
-#include <linux/netdevice.h>
-#include <linux/ratelimit.h>
-#include <net/dst.h>
-
-#include <asm/octeon/octeon.h>
-
-#include "ethernet-defines.h"
-#include "octeon-ethernet.h"
-#include "ethernet-util.h"
-
-#include <asm/octeon/cvmx-helper.h>
-
-#include <asm/octeon/cvmx-gmxx-defs.h>
-
-/*
- * Although these functions are called cvm_oct_sgmii_*, they also
- * happen to be used for the XAUI ports as well.
- */
-
-static void cvm_oct_sgmii_poll(struct net_device *dev)
-{
-	struct octeon_ethernet *priv = netdev_priv(dev);
-	cvmx_helper_link_info_t link_info;
-
-	link_info = cvmx_helper_link_get(priv->port);
-	if (link_info.u64 == priv->link_info)
-		return;
-
-	link_info = cvmx_helper_link_autoconf(priv->port);
-	priv->link_info = link_info.u64;
-
-	/* Tell the core */
-	cvm_oct_set_carrier(priv, link_info);
-}
-
-int cvm_oct_sgmii_open(struct net_device *dev)
-{
-	union cvmx_gmxx_prtx_cfg gmx_cfg;
-	struct octeon_ethernet *priv = netdev_priv(dev);
-	int interface = INTERFACE(priv->port);
-	int index = INDEX(priv->port);
-	cvmx_helper_link_info_t link_info;
-	int rv;
-
-	rv = cvm_oct_phy_setup_device(dev);
-	if (rv)
-		return rv;
-
-	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
-	gmx_cfg.s.en = 1;
-	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
-
-	if (!octeon_is_simulation()) {
-		if (priv->phydev) {
-			int r = phy_read_status(priv->phydev);
-			if (r == 0 && priv->phydev->link == 0)
-				netif_carrier_off(dev);
-			cvm_oct_adjust_link(dev);
-		} else {
-			link_info = cvmx_helper_link_get(priv->port);
-			if (!link_info.s.link_up)
-				netif_carrier_off(dev);
-			spin_lock(&priv->poll_lock);
-			priv->poll = cvm_oct_sgmii_poll;
-			spin_unlock(&priv->poll_lock);
-			cvm_oct_sgmii_poll(dev);
-		}
-	}
-	return 0;
-}
-
-int cvm_oct_sgmii_stop(struct net_device *dev)
-{
-	union cvmx_gmxx_prtx_cfg gmx_cfg;
-	struct octeon_ethernet *priv = netdev_priv(dev);
-	int interface = INTERFACE(priv->port);
-	int index = INDEX(priv->port);
-
-	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
-	gmx_cfg.s.en = 0;
-	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
-
-	spin_lock(&priv->poll_lock);
-	priv->poll = NULL;
-	spin_unlock(&priv->poll_lock);
-
-	if (priv->phydev)
-		phy_disconnect(priv->phydev);
-	priv->phydev = NULL;
-
-	return 0;
-}
-
-int cvm_oct_sgmii_init(struct net_device *dev)
-{
-	cvm_oct_common_init(dev);
-	dev->netdev_ops->ndo_stop(dev);
-
-	return 0;
-}
diff --git a/drivers/staging/octeon/ethernet-spi.c b/drivers/staging/octeon/ethernet-spi.c
deleted file mode 100644
index 3cb75f9..0000000
--- a/drivers/staging/octeon/ethernet-spi.c
+++ /dev/null
@@ -1,323 +0,0 @@
-/**********************************************************************
- * Author: Cavium Networks
- *
- * Contact: support@caviumnetworks.com
- * This file is part of the OCTEON SDK
- *
- * Copyright (c) 2003-2007 Cavium Networks
- *
- * This file is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, Version 2, as
- * published by the Free Software Foundation.
- *
- * This file is distributed in the hope that it will be useful, but
- * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
- * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
- * NONINFRINGEMENT.  See the GNU General Public License for more
- * details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this file; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- * or visit http://www.gnu.org/licenses/.
- *
- * This file may also be available under a different license from Cavium.
- * Contact Cavium Networks for more information
-**********************************************************************/
-#include <linux/kernel.h>
-#include <linux/netdevice.h>
-#include <linux/interrupt.h>
-#include <net/dst.h>
-
-#include <asm/octeon/octeon.h>
-
-#include "ethernet-defines.h"
-#include "octeon-ethernet.h"
-#include "ethernet-util.h"
-
-#include <asm/octeon/cvmx-spi.h>
-
-#include <asm/octeon/cvmx-npi-defs.h>
-#include <asm/octeon/cvmx-spxx-defs.h>
-#include <asm/octeon/cvmx-stxx-defs.h>
-
-static int number_spi_ports;
-static int need_retrain[2] = { 0, 0 };
-
-static irqreturn_t cvm_oct_spi_rml_interrupt(int cpl, void *dev_id)
-{
-	irqreturn_t return_status = IRQ_NONE;
-	union cvmx_npi_rsl_int_blocks rsl_int_blocks;
-
-	/* Check and see if this interrupt was caused by the GMX block */
-	rsl_int_blocks.u64 = cvmx_read_csr(CVMX_NPI_RSL_INT_BLOCKS);
-	if (rsl_int_blocks.s.spx1) {	/* 19 - SPX1_INT_REG & STX1_INT_REG */
-
-		union cvmx_spxx_int_reg spx_int_reg;
-		union cvmx_stxx_int_reg stx_int_reg;
-
-		spx_int_reg.u64 = cvmx_read_csr(CVMX_SPXX_INT_REG(1));
-		cvmx_write_csr(CVMX_SPXX_INT_REG(1), spx_int_reg.u64);
-		if (!need_retrain[1]) {
-
-			spx_int_reg.u64 &= cvmx_read_csr(CVMX_SPXX_INT_MSK(1));
-			if (spx_int_reg.s.spf)
-				pr_err("SPI1: SRX Spi4 interface down\n");
-			if (spx_int_reg.s.calerr)
-				pr_err("SPI1: SRX Spi4 Calendar table "
-				       "parity error\n");
-			if (spx_int_reg.s.syncerr)
-				pr_err("SPI1: SRX Consecutive Spi4 DIP4 "
-				       "errors have exceeded "
-				       "SPX_ERR_CTL[ERRCNT]\n");
-			if (spx_int_reg.s.diperr)
-				pr_err("SPI1: SRX Spi4 DIP4 error\n");
-			if (spx_int_reg.s.tpaovr)
-				pr_err("SPI1: SRX Selected port has hit "
-				       "TPA overflow\n");
-			if (spx_int_reg.s.rsverr)
-				pr_err("SPI1: SRX Spi4 reserved control "
-				       "word detected\n");
-			if (spx_int_reg.s.drwnng)
-				pr_err("SPI1: SRX Spi4 receive FIFO "
-				       "drowning/overflow\n");
-			if (spx_int_reg.s.clserr)
-				pr_err("SPI1: SRX Spi4 packet closed on "
-				       "non-16B alignment without EOP\n");
-			if (spx_int_reg.s.spiovr)
-				pr_err("SPI1: SRX Spi4 async FIFO overflow\n");
-			if (spx_int_reg.s.abnorm)
-				pr_err("SPI1: SRX Abnormal packet "
-				       "termination (ERR bit)\n");
-			if (spx_int_reg.s.prtnxa)
-				pr_err("SPI1: SRX Port out of range\n");
-		}
-
-		stx_int_reg.u64 = cvmx_read_csr(CVMX_STXX_INT_REG(1));
-		cvmx_write_csr(CVMX_STXX_INT_REG(1), stx_int_reg.u64);
-		if (!need_retrain[1]) {
-
-			stx_int_reg.u64 &= cvmx_read_csr(CVMX_STXX_INT_MSK(1));
-			if (stx_int_reg.s.syncerr)
-				pr_err("SPI1: STX Interface encountered a "
-				       "fatal error\n");
-			if (stx_int_reg.s.frmerr)
-				pr_err("SPI1: STX FRMCNT has exceeded "
-				       "STX_DIP_CNT[MAXFRM]\n");
-			if (stx_int_reg.s.unxfrm)
-				pr_err("SPI1: STX Unexpected framing "
-				       "sequence\n");
-			if (stx_int_reg.s.nosync)
-				pr_err("SPI1: STX ERRCNT has exceeded "
-				       "STX_DIP_CNT[MAXDIP]\n");
-			if (stx_int_reg.s.diperr)
-				pr_err("SPI1: STX DIP2 error on the Spi4 "
-				       "Status channel\n");
-			if (stx_int_reg.s.datovr)
-				pr_err("SPI1: STX Spi4 FIFO overflow error\n");
-			if (stx_int_reg.s.ovrbst)
-				pr_err("SPI1: STX Transmit packet burst "
-				       "too big\n");
-			if (stx_int_reg.s.calpar1)
-				pr_err("SPI1: STX Calendar Table Parity "
-				       "Error Bank1\n");
-			if (stx_int_reg.s.calpar0)
-				pr_err("SPI1: STX Calendar Table Parity "
-				       "Error Bank0\n");
-		}
-
-		cvmx_write_csr(CVMX_SPXX_INT_MSK(1), 0);
-		cvmx_write_csr(CVMX_STXX_INT_MSK(1), 0);
-		need_retrain[1] = 1;
-		return_status = IRQ_HANDLED;
-	}
-
-	if (rsl_int_blocks.s.spx0) {	/* 18 - SPX0_INT_REG & STX0_INT_REG */
-		union cvmx_spxx_int_reg spx_int_reg;
-		union cvmx_stxx_int_reg stx_int_reg;
-
-		spx_int_reg.u64 = cvmx_read_csr(CVMX_SPXX_INT_REG(0));
-		cvmx_write_csr(CVMX_SPXX_INT_REG(0), spx_int_reg.u64);
-		if (!need_retrain[0]) {
-
-			spx_int_reg.u64 &= cvmx_read_csr(CVMX_SPXX_INT_MSK(0));
-			if (spx_int_reg.s.spf)
-				pr_err("SPI0: SRX Spi4 interface down\n");
-			if (spx_int_reg.s.calerr)
-				pr_err("SPI0: SRX Spi4 Calendar table "
-				       "parity error\n");
-			if (spx_int_reg.s.syncerr)
-				pr_err("SPI0: SRX Consecutive Spi4 DIP4 "
-				       "errors have exceeded "
-				       "SPX_ERR_CTL[ERRCNT]\n");
-			if (spx_int_reg.s.diperr)
-				pr_err("SPI0: SRX Spi4 DIP4 error\n");
-			if (spx_int_reg.s.tpaovr)
-				pr_err("SPI0: SRX Selected port has hit "
-				       "TPA overflow\n");
-			if (spx_int_reg.s.rsverr)
-				pr_err("SPI0: SRX Spi4 reserved control "
-				       "word detected\n");
-			if (spx_int_reg.s.drwnng)
-				pr_err("SPI0: SRX Spi4 receive FIFO "
-				       "drowning/overflow\n");
-			if (spx_int_reg.s.clserr)
-				pr_err("SPI0: SRX Spi4 packet closed on "
-				       "non-16B alignment without EOP\n");
-			if (spx_int_reg.s.spiovr)
-				pr_err("SPI0: SRX Spi4 async FIFO overflow\n");
-			if (spx_int_reg.s.abnorm)
-				pr_err("SPI0: SRX Abnormal packet "
-				       "termination (ERR bit)\n");
-			if (spx_int_reg.s.prtnxa)
-				pr_err("SPI0: SRX Port out of range\n");
-		}
-
-		stx_int_reg.u64 = cvmx_read_csr(CVMX_STXX_INT_REG(0));
-		cvmx_write_csr(CVMX_STXX_INT_REG(0), stx_int_reg.u64);
-		if (!need_retrain[0]) {
-
-			stx_int_reg.u64 &= cvmx_read_csr(CVMX_STXX_INT_MSK(0));
-			if (stx_int_reg.s.syncerr)
-				pr_err("SPI0: STX Interface encountered a "
-				       "fatal error\n");
-			if (stx_int_reg.s.frmerr)
-				pr_err("SPI0: STX FRMCNT has exceeded "
-				       "STX_DIP_CNT[MAXFRM]\n");
-			if (stx_int_reg.s.unxfrm)
-				pr_err("SPI0: STX Unexpected framing "
-				       "sequence\n");
-			if (stx_int_reg.s.nosync)
-				pr_err("SPI0: STX ERRCNT has exceeded "
-				       "STX_DIP_CNT[MAXDIP]\n");
-			if (stx_int_reg.s.diperr)
-				pr_err("SPI0: STX DIP2 error on the Spi4 "
-				       "Status channel\n");
-			if (stx_int_reg.s.datovr)
-				pr_err("SPI0: STX Spi4 FIFO overflow error\n");
-			if (stx_int_reg.s.ovrbst)
-				pr_err("SPI0: STX Transmit packet burst "
-				       "too big\n");
-			if (stx_int_reg.s.calpar1)
-				pr_err("SPI0: STX Calendar Table Parity "
-				       "Error Bank1\n");
-			if (stx_int_reg.s.calpar0)
-				pr_err("SPI0: STX Calendar Table Parity "
-				       "Error Bank0\n");
-		}
-
-		cvmx_write_csr(CVMX_SPXX_INT_MSK(0), 0);
-		cvmx_write_csr(CVMX_STXX_INT_MSK(0), 0);
-		need_retrain[0] = 1;
-		return_status = IRQ_HANDLED;
-	}
-
-	return return_status;
-}
-
-static void cvm_oct_spi_enable_error_reporting(int interface)
-{
-	union cvmx_spxx_int_msk spxx_int_msk;
-	union cvmx_stxx_int_msk stxx_int_msk;
-
-	spxx_int_msk.u64 = cvmx_read_csr(CVMX_SPXX_INT_MSK(interface));
-	spxx_int_msk.s.calerr = 1;
-	spxx_int_msk.s.syncerr = 1;
-	spxx_int_msk.s.diperr = 1;
-	spxx_int_msk.s.tpaovr = 1;
-	spxx_int_msk.s.rsverr = 1;
-	spxx_int_msk.s.drwnng = 1;
-	spxx_int_msk.s.clserr = 1;
-	spxx_int_msk.s.spiovr = 1;
-	spxx_int_msk.s.abnorm = 1;
-	spxx_int_msk.s.prtnxa = 1;
-	cvmx_write_csr(CVMX_SPXX_INT_MSK(interface), spxx_int_msk.u64);
-
-	stxx_int_msk.u64 = cvmx_read_csr(CVMX_STXX_INT_MSK(interface));
-	stxx_int_msk.s.frmerr = 1;
-	stxx_int_msk.s.unxfrm = 1;
-	stxx_int_msk.s.nosync = 1;
-	stxx_int_msk.s.diperr = 1;
-	stxx_int_msk.s.datovr = 1;
-	stxx_int_msk.s.ovrbst = 1;
-	stxx_int_msk.s.calpar1 = 1;
-	stxx_int_msk.s.calpar0 = 1;
-	cvmx_write_csr(CVMX_STXX_INT_MSK(interface), stxx_int_msk.u64);
-}
-
-static void cvm_oct_spi_poll(struct net_device *dev)
-{
-	static int spi4000_port;
-	struct octeon_ethernet *priv = netdev_priv(dev);
-	int interface;
-
-	for (interface = 0; interface < 2; interface++) {
-
-		if ((priv->port == interface * 16) && need_retrain[interface]) {
-
-			if (cvmx_spi_restart_interface
-			    (interface, CVMX_SPI_MODE_DUPLEX, 10) == 0) {
-				need_retrain[interface] = 0;
-				cvm_oct_spi_enable_error_reporting(interface);
-			}
-		}
-
-		/*
-		 * The SPI4000 TWSI interface is very slow. In order
-		 * not to bring the system to a crawl, we only poll a
-		 * single port every second. This means negotiation
-		 * speed changes take up to 10 seconds, but at least
-		 * we don't waste absurd amounts of time waiting for
-		 * TWSI.
-		 */
-		if (priv->port == spi4000_port) {
-			/*
-			 * This function does nothing if it is called on an
-			 * interface without a SPI4000.
-			 */
-			cvmx_spi4000_check_speed(interface, priv->port);
-			/*
-			 * Normal ordering increments. By decrementing
-			 * we only match once per iteration.
-			 */
-			spi4000_port--;
-			if (spi4000_port < 0)
-				spi4000_port = 10;
-		}
-	}
-}
-
-int cvm_oct_spi_init(struct net_device *dev)
-{
-	int r;
-	struct octeon_ethernet *priv = netdev_priv(dev);
-
-	if (number_spi_ports == 0) {
-		r = request_irq(OCTEON_IRQ_RML, cvm_oct_spi_rml_interrupt,
-				IRQF_SHARED, "SPI", &number_spi_ports);
-		if (r)
-			return r;
-	}
-	number_spi_ports++;
-
-	if ((priv->port == 0) || (priv->port == 16)) {
-		cvm_oct_spi_enable_error_reporting(INTERFACE(priv->port));
-		priv->poll = cvm_oct_spi_poll;
-	}
-	cvm_oct_common_init(dev);
-	return 0;
-}
-
-void cvm_oct_spi_uninit(struct net_device *dev)
-{
-	int interface;
-
-	number_spi_ports--;
-	if (number_spi_ports == 0) {
-		for (interface = 0; interface < 2; interface++) {
-			cvmx_write_csr(CVMX_SPXX_INT_MSK(interface), 0);
-			cvmx_write_csr(CVMX_STXX_INT_MSK(interface), 0);
-		}
-		free_irq(OCTEON_IRQ_RML, &number_spi_ports);
-	}
-}
diff --git a/drivers/staging/octeon/ethernet-tx.c b/drivers/staging/octeon/ethernet-tx.c
deleted file mode 100644
index d063641..0000000
--- a/drivers/staging/octeon/ethernet-tx.c
+++ /dev/null
@@ -1,728 +0,0 @@
-/*********************************************************************
- * Author: Cavium Networks
- *
- * Contact: support@caviumnetworks.com
- * This file is part of the OCTEON SDK
- *
- * Copyright (c) 2003-2010 Cavium Networks
- *
- * This file is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, Version 2, as
- * published by the Free Software Foundation.
- *
- * This file is distributed in the hope that it will be useful, but
- * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
- * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
- * NONINFRINGEMENT.  See the GNU General Public License for more
- * details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this file; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- * or visit http://www.gnu.org/licenses/.
- *
- * This file may also be available under a different license from Cavium.
- * Contact Cavium Networks for more information
-*********************************************************************/
-#include <linux/module.h>
-#include <linux/kernel.h>
-#include <linux/netdevice.h>
-#include <linux/init.h>
-#include <linux/etherdevice.h>
-#include <linux/ip.h>
-#include <linux/ratelimit.h>
-#include <linux/string.h>
-#include <linux/interrupt.h>
-#include <net/dst.h>
-#ifdef CONFIG_XFRM
-#include <linux/xfrm.h>
-#include <net/xfrm.h>
-#endif /* CONFIG_XFRM */
-
-#include <linux/atomic.h>
-
-#include <asm/octeon/octeon.h>
-
-#include "ethernet-defines.h"
-#include "octeon-ethernet.h"
-#include "ethernet-tx.h"
-#include "ethernet-util.h"
-
-#include <asm/octeon/cvmx-wqe.h>
-#include <asm/octeon/cvmx-fau.h>
-#include <asm/octeon/cvmx-pip.h>
-#include <asm/octeon/cvmx-pko.h>
-#include <asm/octeon/cvmx-helper.h>
-
-#include <asm/octeon/cvmx-gmxx-defs.h>
-
-#define CVM_OCT_SKB_CB(skb)	((u64 *)((skb)->cb))
-
-/*
- * You can define GET_SKBUFF_QOS() to override how the skbuff output
- * function determines which output queue is used. The default
- * implementation always uses the base queue for the port. If, for
- * example, you wanted to use the skb->priority field, define
- * GET_SKBUFF_QOS as: #define GET_SKBUFF_QOS(skb) ((skb)->priority)
- */
-#ifndef GET_SKBUFF_QOS
-#define GET_SKBUFF_QOS(skb) 0
-#endif
-
-static void cvm_oct_tx_do_cleanup(unsigned long arg);
-static DECLARE_TASKLET(cvm_oct_tx_cleanup_tasklet, cvm_oct_tx_do_cleanup, 0);
-
-/* Maximum number of SKBs to try to free per xmit packet. */
-#define MAX_SKB_TO_FREE (MAX_OUT_QUEUE_DEPTH * 2)
-
-static inline int32_t cvm_oct_adjust_skb_to_free(int32_t skb_to_free, int fau)
-{
-	int32_t undo;
-	undo = skb_to_free > 0 ? MAX_SKB_TO_FREE : skb_to_free + MAX_SKB_TO_FREE;
-	if (undo > 0)
-		cvmx_fau_atomic_add32(fau, -undo);
-	skb_to_free = -skb_to_free > MAX_SKB_TO_FREE ? MAX_SKB_TO_FREE : -skb_to_free;
-	return skb_to_free;
-}
-
-static void cvm_oct_kick_tx_poll_watchdog(void)
-{
-	union cvmx_ciu_timx ciu_timx;
-	ciu_timx.u64 = 0;
-	ciu_timx.s.one_shot = 1;
-	ciu_timx.s.len = cvm_oct_tx_poll_interval;
-	cvmx_write_csr(CVMX_CIU_TIMX(1), ciu_timx.u64);
-}
-
-void cvm_oct_free_tx_skbs(struct net_device *dev)
-{
-	int32_t skb_to_free;
-	int qos, queues_per_port;
-	int total_freed = 0;
-	int total_remaining = 0;
-	unsigned long flags;
-	struct octeon_ethernet *priv = netdev_priv(dev);
-
-	queues_per_port = cvmx_pko_get_num_queues(priv->port);
-	/* Drain any pending packets in the free list */
-	for (qos = 0; qos < queues_per_port; qos++) {
-		if (skb_queue_len(&priv->tx_free_list[qos]) == 0)
-			continue;
-		skb_to_free = cvmx_fau_fetch_and_add32(priv->fau+qos*4, MAX_SKB_TO_FREE);
-		skb_to_free = cvm_oct_adjust_skb_to_free(skb_to_free, priv->fau+qos*4);
-
-
-		total_freed += skb_to_free;
-		if (skb_to_free > 0) {
-			struct sk_buff *to_free_list = NULL;
-			spin_lock_irqsave(&priv->tx_free_list[qos].lock, flags);
-			while (skb_to_free > 0) {
-				struct sk_buff *t = __skb_dequeue(&priv->tx_free_list[qos]);
-				t->next = to_free_list;
-				to_free_list = t;
-				skb_to_free--;
-			}
-			spin_unlock_irqrestore(&priv->tx_free_list[qos].lock, flags);
-			/* Do the actual freeing outside of the lock. */
-			while (to_free_list) {
-				struct sk_buff *t = to_free_list;
-				to_free_list = to_free_list->next;
-				dev_kfree_skb_any(t);
-			}
-		}
-		total_remaining += skb_queue_len(&priv->tx_free_list[qos]);
-	}
-	if (total_freed >= 0 && netif_queue_stopped(dev))
-		netif_wake_queue(dev);
-	if (total_remaining)
-		cvm_oct_kick_tx_poll_watchdog();
-}
-
-/**
- * cvm_oct_xmit - transmit a packet
- * @skb:    Packet to send
- * @dev:    Device info structure
- *
- * Returns Always returns NETDEV_TX_OK
- */
-int cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev)
-{
-	cvmx_pko_command_word0_t pko_command;
-	union cvmx_buf_ptr hw_buffer;
-	uint64_t old_scratch;
-	uint64_t old_scratch2;
-	int qos;
-	int i;
-	enum {QUEUE_CORE, QUEUE_HW, QUEUE_DROP} queue_type;
-	struct octeon_ethernet *priv = netdev_priv(dev);
-	struct sk_buff *to_free_list;
-	int32_t skb_to_free;
-	int32_t buffers_to_free;
-	u32 total_to_clean;
-	unsigned long flags;
-#if REUSE_SKBUFFS_WITHOUT_FREE
-	unsigned char *fpa_head;
-#endif
-
-	/*
-	 * Prefetch the private data structure.  It is larger than the
-	 * one cache line.
-	 */
-	prefetch(priv);
-
-	/*
-	 * The check on CVMX_PKO_QUEUES_PER_PORT_* is designed to
-	 * completely remove "qos" in the event neither interface
-	 * supports multiple queues per port.
-	 */
-	if ((CVMX_PKO_QUEUES_PER_PORT_INTERFACE0 > 1) ||
-	    (CVMX_PKO_QUEUES_PER_PORT_INTERFACE1 > 1)) {
-		qos = GET_SKBUFF_QOS(skb);
-		if (qos <= 0)
-			qos = 0;
-		else if (qos >= cvmx_pko_get_num_queues(priv->port))
-			qos = 0;
-	} else
-		qos = 0;
-
-	if (USE_ASYNC_IOBDMA) {
-		/* Save scratch in case userspace is using it */
-		CVMX_SYNCIOBDMA;
-		old_scratch = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
-		old_scratch2 = cvmx_scratch_read64(CVMX_SCR_SCRATCH + 8);
-
-		/*
-		 * Fetch and increment the number of packets to be
-		 * freed.
-		 */
-		cvmx_fau_async_fetch_and_add32(CVMX_SCR_SCRATCH + 8,
-					       FAU_NUM_PACKET_BUFFERS_TO_FREE,
-					       0);
-		cvmx_fau_async_fetch_and_add32(CVMX_SCR_SCRATCH,
-					       priv->fau + qos * 4,
-					       MAX_SKB_TO_FREE);
-	}
-
-	/*
-	 * We have space for 6 segment pointers, If there will be more
-	 * than that, we must linearize.
-	 */
-	if (unlikely(skb_shinfo(skb)->nr_frags > 5)) {
-		if (unlikely(__skb_linearize(skb))) {
-			queue_type = QUEUE_DROP;
-			if (USE_ASYNC_IOBDMA) {
-				/* Get the number of skbuffs in use by the hardware */
-				CVMX_SYNCIOBDMA;
-				skb_to_free = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
-			} else {
-				/* Get the number of skbuffs in use by the hardware */
-				skb_to_free = cvmx_fau_fetch_and_add32(priv->fau + qos * 4,
-								       MAX_SKB_TO_FREE);
-			}
-			skb_to_free = cvm_oct_adjust_skb_to_free(skb_to_free, priv->fau + qos * 4);
-			spin_lock_irqsave(&priv->tx_free_list[qos].lock, flags);
-			goto skip_xmit;
-		}
-	}
-
-	/*
-	 * The CN3XXX series of parts has an errata (GMX-401) which
-	 * causes the GMX block to hang if a collision occurs towards
-	 * the end of a <68 byte packet. As a workaround for this, we
-	 * pad packets to be 68 bytes whenever we are in half duplex
-	 * mode. We don't handle the case of having a small packet but
-	 * no room to add the padding.  The kernel should always give
-	 * us at least a cache line
-	 */
-	if ((skb->len < 64) && OCTEON_IS_MODEL(OCTEON_CN3XXX)) {
-		union cvmx_gmxx_prtx_cfg gmx_prt_cfg;
-		int interface = INTERFACE(priv->port);
-		int index = INDEX(priv->port);
-
-		if (interface < 2) {
-			/* We only need to pad packet in half duplex mode */
-			gmx_prt_cfg.u64 =
-			    cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
-			if (gmx_prt_cfg.s.duplex == 0) {
-				int add_bytes = 64 - skb->len;
-				if ((skb_tail_pointer(skb) + add_bytes) <=
-				    skb_end_pointer(skb))
-					memset(__skb_put(skb, add_bytes), 0,
-					       add_bytes);
-			}
-		}
-	}
-
-	/* Build the PKO command */
-	pko_command.u64 = 0;
-#ifdef __LITTLE_ENDIAN
-	pko_command.s.le = 1;
-#endif
-	pko_command.s.n2 = 1;	/* Don't pollute L2 with the outgoing packet */
-	pko_command.s.segs = 1;
-	pko_command.s.total_bytes = skb->len;
-	pko_command.s.size0 = CVMX_FAU_OP_SIZE_32;
-	pko_command.s.subone0 = 1;
-
-	pko_command.s.dontfree = 1;
-
-	/* Build the PKO buffer pointer */
-	hw_buffer.u64 = 0;
-	if (skb_shinfo(skb)->nr_frags == 0) {
-		hw_buffer.s.addr = XKPHYS_TO_PHYS((u64)skb->data);
-		hw_buffer.s.pool = 0;
-		hw_buffer.s.size = skb->len;
-	} else {
-		hw_buffer.s.addr = XKPHYS_TO_PHYS((u64)skb->data);
-		hw_buffer.s.pool = 0;
-		hw_buffer.s.size = skb_headlen(skb);
-		CVM_OCT_SKB_CB(skb)[0] = hw_buffer.u64;
-		for (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {
-			struct skb_frag_struct *fs = skb_shinfo(skb)->frags + i;
-			hw_buffer.s.addr = XKPHYS_TO_PHYS((u64)(page_address(fs->page.p) + fs->page_offset));
-			hw_buffer.s.size = fs->size;
-			CVM_OCT_SKB_CB(skb)[i + 1] = hw_buffer.u64;
-		}
-		hw_buffer.s.addr = XKPHYS_TO_PHYS((u64)CVM_OCT_SKB_CB(skb));
-		hw_buffer.s.size = skb_shinfo(skb)->nr_frags + 1;
-		pko_command.s.segs = skb_shinfo(skb)->nr_frags + 1;
-		pko_command.s.gather = 1;
-		goto dont_put_skbuff_in_hw;
-	}
-
-	/*
-	 * See if we can put this skb in the FPA pool. Any strange
-	 * behavior from the Linux networking stack will most likely
-	 * be caused by a bug in the following code. If some field is
-	 * in use by the network stack and gets carried over when a
-	 * buffer is reused, bad things may happen.  If in doubt and
-	 * you dont need the absolute best performance, disable the
-	 * define REUSE_SKBUFFS_WITHOUT_FREE. The reuse of buffers has
-	 * shown a 25% increase in performance under some loads.
-	 */
-#if REUSE_SKBUFFS_WITHOUT_FREE
-	fpa_head = skb->head + 256 - ((unsigned long)skb->head & 0x7f);
-	if (unlikely(skb->data < fpa_head)) {
-		/*
-		 * printk("TX buffer beginning can't meet FPA
-		 * alignment constraints\n");
-		 */
-		goto dont_put_skbuff_in_hw;
-	}
-	if (unlikely
-	    ((skb_end_pointer(skb) - fpa_head) < CVMX_FPA_PACKET_POOL_SIZE)) {
-		/*
-		   printk("TX buffer isn't large enough for the FPA\n");
-		 */
-		goto dont_put_skbuff_in_hw;
-	}
-	if (unlikely(skb_shared(skb))) {
-		/*
-		   printk("TX buffer sharing data with someone else\n");
-		 */
-		goto dont_put_skbuff_in_hw;
-	}
-	if (unlikely(skb_cloned(skb))) {
-		/*
-		   printk("TX buffer has been cloned\n");
-		 */
-		goto dont_put_skbuff_in_hw;
-	}
-	if (unlikely(skb_header_cloned(skb))) {
-		/*
-		   printk("TX buffer header has been cloned\n");
-		 */
-		goto dont_put_skbuff_in_hw;
-	}
-	if (unlikely(skb->destructor)) {
-		/*
-		   printk("TX buffer has a destructor\n");
-		 */
-		goto dont_put_skbuff_in_hw;
-	}
-	if (unlikely(skb_shinfo(skb)->nr_frags)) {
-		/*
-		   printk("TX buffer has fragments\n");
-		 */
-		goto dont_put_skbuff_in_hw;
-	}
-	if (unlikely
-	    (skb->truesize !=
-	     sizeof(*skb) + skb_end_offset(skb))) {
-		/*
-		   printk("TX buffer truesize has been changed\n");
-		 */
-		goto dont_put_skbuff_in_hw;
-	}
-
-	/*
-	 * We can use this buffer in the FPA.  We don't need the FAU
-	 * update anymore
-	 */
-	pko_command.s.dontfree = 0;
-
-	hw_buffer.s.back = ((unsigned long)skb->data >> 7) - ((unsigned long)fpa_head >> 7);
-	*(struct sk_buff **)(fpa_head - sizeof(void *)) = skb;
-
-	/*
-	 * The skbuff will be reused without ever being freed. We must
-	 * cleanup a bunch of core things.
-	 */
-	dst_release(skb_dst(skb));
-	skb_dst_set(skb, NULL);
-#ifdef CONFIG_XFRM
-	secpath_put(skb->sp);
-	skb->sp = NULL;
-#endif
-	nf_reset(skb);
-
-#ifdef CONFIG_NET_SCHED
-	skb->tc_index = 0;
-#ifdef CONFIG_NET_CLS_ACT
-	skb->tc_verd = 0;
-#endif /* CONFIG_NET_CLS_ACT */
-#endif /* CONFIG_NET_SCHED */
-#endif /* REUSE_SKBUFFS_WITHOUT_FREE */
-
-dont_put_skbuff_in_hw:
-
-	/* Check if we can use the hardware checksumming */
-	if (USE_HW_TCPUDP_CHECKSUM && (skb->protocol == htons(ETH_P_IP)) &&
-	    (ip_hdr(skb)->version == 4) && (ip_hdr(skb)->ihl == 5) &&
-	    ((ip_hdr(skb)->frag_off == 0) || (ip_hdr(skb)->frag_off == 1 << 14))
-	    && ((ip_hdr(skb)->protocol == IPPROTO_TCP)
-		|| (ip_hdr(skb)->protocol == IPPROTO_UDP))) {
-		/* Use hardware checksum calc */
-		pko_command.s.ipoffp1 = sizeof(struct ethhdr) + 1;
-	}
-
-	if (USE_ASYNC_IOBDMA) {
-		/* Get the number of skbuffs in use by the hardware */
-		CVMX_SYNCIOBDMA;
-		skb_to_free = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
-		buffers_to_free = cvmx_scratch_read64(CVMX_SCR_SCRATCH + 8);
-	} else {
-		/* Get the number of skbuffs in use by the hardware */
-		skb_to_free = cvmx_fau_fetch_and_add32(priv->fau + qos * 4,
-						       MAX_SKB_TO_FREE);
-		buffers_to_free =
-		    cvmx_fau_fetch_and_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);
-	}
-
-	skb_to_free = cvm_oct_adjust_skb_to_free(skb_to_free, priv->fau+qos*4);
-
-	/*
-	 * If we're sending faster than the receive can free them then
-	 * don't do the HW free.
-	 */
-	if ((buffers_to_free < -100) && !pko_command.s.dontfree)
-		pko_command.s.dontfree = 1;
-
-	if (pko_command.s.dontfree) {
-		queue_type = QUEUE_CORE;
-		pko_command.s.reg0 = priv->fau+qos*4;
-	} else {
-		queue_type = QUEUE_HW;
-	}
-	if (USE_ASYNC_IOBDMA)
-		cvmx_fau_async_fetch_and_add32(CVMX_SCR_SCRATCH, FAU_TOTAL_TX_TO_CLEAN, 1);
-
-	spin_lock_irqsave(&priv->tx_free_list[qos].lock, flags);
-
-	/* Drop this packet if we have too many already queued to the HW */
-	if (unlikely(skb_queue_len(&priv->tx_free_list[qos]) >= MAX_OUT_QUEUE_DEPTH)) {
-		if (dev->tx_queue_len != 0) {
-			/* Drop the lock when notifying the core.  */
-			spin_unlock_irqrestore(&priv->tx_free_list[qos].lock, flags);
-			netif_stop_queue(dev);
-			spin_lock_irqsave(&priv->tx_free_list[qos].lock, flags);
-		} else {
-			/* If not using normal queueing.  */
-			queue_type = QUEUE_DROP;
-			goto skip_xmit;
-		}
-	}
-
-	cvmx_pko_send_packet_prepare(priv->port, priv->queue + qos,
-				     CVMX_PKO_LOCK_NONE);
-
-	/* Send the packet to the output queue */
-	if (unlikely(cvmx_pko_send_packet_finish(priv->port,
-						 priv->queue + qos,
-						 pko_command, hw_buffer,
-						 CVMX_PKO_LOCK_NONE))) {
-		printk_ratelimited("%s: Failed to send the packet\n", dev->name);
-		queue_type = QUEUE_DROP;
-	}
-skip_xmit:
-	to_free_list = NULL;
-
-	switch (queue_type) {
-	case QUEUE_DROP:
-		skb->next = to_free_list;
-		to_free_list = skb;
-		priv->stats.tx_dropped++;
-		break;
-	case QUEUE_HW:
-		cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, -1);
-		break;
-	case QUEUE_CORE:
-		__skb_queue_tail(&priv->tx_free_list[qos], skb);
-		break;
-	default:
-		BUG();
-	}
-
-	while (skb_to_free > 0) {
-		struct sk_buff *t = __skb_dequeue(&priv->tx_free_list[qos]);
-		t->next = to_free_list;
-		to_free_list = t;
-		skb_to_free--;
-	}
-
-	spin_unlock_irqrestore(&priv->tx_free_list[qos].lock, flags);
-
-	/* Do the actual freeing outside of the lock. */
-	while (to_free_list) {
-		struct sk_buff *t = to_free_list;
-		to_free_list = to_free_list->next;
-		dev_kfree_skb_any(t);
-	}
-
-	if (USE_ASYNC_IOBDMA) {
-		CVMX_SYNCIOBDMA;
-		total_to_clean = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
-		/* Restore the scratch area */
-		cvmx_scratch_write64(CVMX_SCR_SCRATCH, old_scratch);
-		cvmx_scratch_write64(CVMX_SCR_SCRATCH + 8, old_scratch2);
-	} else {
-		total_to_clean = cvmx_fau_fetch_and_add32(FAU_TOTAL_TX_TO_CLEAN, 1);
-	}
-
-	if (total_to_clean & 0x3ff) {
-		/*
-		 * Schedule the cleanup tasklet every 1024 packets for
-		 * the pathological case of high traffic on one port
-		 * delaying clean up of packets on a different port
-		 * that is blocked waiting for the cleanup.
-		 */
-		tasklet_schedule(&cvm_oct_tx_cleanup_tasklet);
-	}
-
-	cvm_oct_kick_tx_poll_watchdog();
-
-	return NETDEV_TX_OK;
-}
-
-/**
- * cvm_oct_xmit_pow - transmit a packet to the POW
- * @skb:    Packet to send
- * @dev:    Device info structure
-
- * Returns Always returns zero
- */
-int cvm_oct_xmit_pow(struct sk_buff *skb, struct net_device *dev)
-{
-	struct octeon_ethernet *priv = netdev_priv(dev);
-	void *packet_buffer;
-	void *copy_location;
-
-	/* Get a work queue entry */
-	cvmx_wqe_t *work = cvmx_fpa_alloc(CVMX_FPA_WQE_POOL);
-	if (unlikely(work == NULL)) {
-		printk_ratelimited("%s: Failed to allocate a work "
-				   "queue entry\n", dev->name);
-		priv->stats.tx_dropped++;
-		dev_kfree_skb(skb);
-		return 0;
-	}
-
-	/* Get a packet buffer */
-	packet_buffer = cvmx_fpa_alloc(CVMX_FPA_PACKET_POOL);
-	if (unlikely(packet_buffer == NULL)) {
-		printk_ratelimited("%s: Failed to allocate a packet buffer\n",
-				   dev->name);
-		cvmx_fpa_free(work, CVMX_FPA_WQE_POOL, DONT_WRITEBACK(1));
-		priv->stats.tx_dropped++;
-		dev_kfree_skb(skb);
-		return 0;
-	}
-
-	/*
-	 * Calculate where we need to copy the data to. We need to
-	 * leave 8 bytes for a next pointer (unused). We also need to
-	 * include any configure skip. Then we need to align the IP
-	 * packet src and dest into the same 64bit word. The below
-	 * calculation may add a little extra, but that doesn't
-	 * hurt.
-	 */
-	copy_location = packet_buffer + sizeof(uint64_t);
-	copy_location += ((CVMX_HELPER_FIRST_MBUFF_SKIP + 7) & 0xfff8) + 6;
-
-	/*
-	 * We have to copy the packet since whoever processes this
-	 * packet will free it to a hardware pool. We can't use the
-	 * trick of counting outstanding packets like in
-	 * cvm_oct_xmit.
-	 */
-	memcpy(copy_location, skb->data, skb->len);
-
-	/*
-	 * Fill in some of the work queue fields. We may need to add
-	 * more if the software at the other end needs them.
-	 */
-	work->hw_chksum = skb->csum;
-	work->len = skb->len;
-	work->ipprt = priv->port;
-	work->qos = priv->port & 0x7;
-	work->grp = pow_send_group;
-	work->tag_type = CVMX_HELPER_INPUT_TAG_TYPE;
-	work->tag = pow_send_group;	/* FIXME */
-	/* Default to zero. Sets of zero later are commented out */
-	work->word2.u64 = 0;
-	work->word2.s.bufs = 1;
-	work->packet_ptr.u64 = 0;
-	work->packet_ptr.s.addr = cvmx_ptr_to_phys(copy_location);
-	work->packet_ptr.s.pool = CVMX_FPA_PACKET_POOL;
-	work->packet_ptr.s.size = CVMX_FPA_PACKET_POOL_SIZE;
-	work->packet_ptr.s.back = (copy_location - packet_buffer) >> 7;
-
-	if (skb->protocol == htons(ETH_P_IP)) {
-		work->word2.s.ip_offset = 14;
-#if 0
-		work->word2.s.vlan_valid = 0;	/* FIXME */
-		work->word2.s.vlan_cfi = 0;	/* FIXME */
-		work->word2.s.vlan_id = 0;	/* FIXME */
-		work->word2.s.dec_ipcomp = 0;	/* FIXME */
-#endif
-		work->word2.s.tcp_or_udp =
-		    (ip_hdr(skb)->protocol == IPPROTO_TCP)
-		    || (ip_hdr(skb)->protocol == IPPROTO_UDP);
-#if 0
-		/* FIXME */
-		work->word2.s.dec_ipsec = 0;
-		/* We only support IPv4 right now */
-		work->word2.s.is_v6 = 0;
-		/* Hardware would set to zero */
-		work->word2.s.software = 0;
-		/* No error, packet is internal */
-		work->word2.s.L4_error = 0;
-#endif
-		work->word2.s.is_frag = !((ip_hdr(skb)->frag_off == 0)
-					  || (ip_hdr(skb)->frag_off ==
-					      1 << 14));
-#if 0
-		/* Assume Linux is sending a good packet */
-		work->word2.s.IP_exc = 0;
-#endif
-		work->word2.s.is_bcast = (skb->pkt_type == PACKET_BROADCAST);
-		work->word2.s.is_mcast = (skb->pkt_type == PACKET_MULTICAST);
-#if 0
-		/* This is an IP packet */
-		work->word2.s.not_IP = 0;
-		/* No error, packet is internal */
-		work->word2.s.rcv_error = 0;
-		/* No error, packet is internal */
-		work->word2.s.err_code = 0;
-#endif
-
-		/*
-		 * When copying the data, include 4 bytes of the
-		 * ethernet header to align the same way hardware
-		 * does.
-		 */
-		memcpy(work->packet_data, skb->data + 10,
-		       sizeof(work->packet_data));
-	} else {
-#if 0
-		work->word2.snoip.vlan_valid = 0;	/* FIXME */
-		work->word2.snoip.vlan_cfi = 0;	/* FIXME */
-		work->word2.snoip.vlan_id = 0;	/* FIXME */
-		work->word2.snoip.software = 0;	/* Hardware would set to zero */
-#endif
-		work->word2.snoip.is_rarp = skb->protocol == htons(ETH_P_RARP);
-		work->word2.snoip.is_arp = skb->protocol == htons(ETH_P_ARP);
-		work->word2.snoip.is_bcast =
-		    (skb->pkt_type == PACKET_BROADCAST);
-		work->word2.snoip.is_mcast =
-		    (skb->pkt_type == PACKET_MULTICAST);
-		work->word2.snoip.not_IP = 1;	/* IP was done up above */
-#if 0
-		/* No error, packet is internal */
-		work->word2.snoip.rcv_error = 0;
-		/* No error, packet is internal */
-		work->word2.snoip.err_code = 0;
-#endif
-		memcpy(work->packet_data, skb->data, sizeof(work->packet_data));
-	}
-
-	/* Submit the packet to the POW */
-	cvmx_pow_work_submit(work, work->tag, work->tag_type, work->qos,
-			     work->grp);
-	priv->stats.tx_packets++;
-	priv->stats.tx_bytes += skb->len;
-	dev_kfree_skb(skb);
-	return 0;
-}
-
-/**
- * cvm_oct_tx_shutdown_dev - free all skb that are currently queued for TX.
- * @dev:    Device being shutdown
- *
- */
-void cvm_oct_tx_shutdown_dev(struct net_device *dev)
-{
-	struct octeon_ethernet *priv = netdev_priv(dev);
-	unsigned long flags;
-	int qos;
-
-	for (qos = 0; qos < 16; qos++) {
-		spin_lock_irqsave(&priv->tx_free_list[qos].lock, flags);
-		while (skb_queue_len(&priv->tx_free_list[qos]))
-			dev_kfree_skb_any(__skb_dequeue
-					  (&priv->tx_free_list[qos]));
-		spin_unlock_irqrestore(&priv->tx_free_list[qos].lock, flags);
-	}
-}
-
-static void cvm_oct_tx_do_cleanup(unsigned long arg)
-{
-	int port;
-
-	for (port = 0; port < TOTAL_NUMBER_OF_PORTS; port++) {
-		if (cvm_oct_device[port]) {
-			struct net_device *dev = cvm_oct_device[port];
-			cvm_oct_free_tx_skbs(dev);
-		}
-	}
-}
-
-static irqreturn_t cvm_oct_tx_cleanup_watchdog(int cpl, void *dev_id)
-{
-	/* Disable the interrupt.  */
-	cvmx_write_csr(CVMX_CIU_TIMX(1), 0);
-	/* Do the work in the tasklet.  */
-	tasklet_schedule(&cvm_oct_tx_cleanup_tasklet);
-	return IRQ_HANDLED;
-}
-
-void cvm_oct_tx_initialize(void)
-{
-	int i;
-
-	/* Disable the interrupt.  */
-	cvmx_write_csr(CVMX_CIU_TIMX(1), 0);
-	/* Register an IRQ hander for to receive CIU_TIMX(1) interrupts */
-	i = request_irq(OCTEON_IRQ_TIMER1,
-			cvm_oct_tx_cleanup_watchdog, 0,
-			"Ethernet", cvm_oct_device);
-
-	if (i)
-		panic("Could not acquire Ethernet IRQ %d\n", OCTEON_IRQ_TIMER1);
-}
-
-void cvm_oct_tx_shutdown(void)
-{
-	/* Free the interrupt handler */
-	free_irq(OCTEON_IRQ_TIMER1, cvm_oct_device);
-}
diff --git a/drivers/staging/octeon/ethernet-tx.h b/drivers/staging/octeon/ethernet-tx.h
deleted file mode 100644
index 547680c..0000000
--- a/drivers/staging/octeon/ethernet-tx.h
+++ /dev/null
@@ -1,34 +0,0 @@
-/*********************************************************************
- * Author: Cavium Networks
- *
- * Contact: support@caviumnetworks.com
- * This file is part of the OCTEON SDK
- *
- * Copyright (c) 2003-2007 Cavium Networks
- *
- * This file is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, Version 2, as
- * published by the Free Software Foundation.
- *
- * This file is distributed in the hope that it will be useful, but
- * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
- * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
- * NONINFRINGEMENT.  See the GNU General Public License for more
- * details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this file; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- * or visit http://www.gnu.org/licenses/.
- *
- * This file may also be available under a different license from Cavium.
- * Contact Cavium Networks for more information
-*********************************************************************/
-
-int cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev);
-int cvm_oct_xmit_pow(struct sk_buff *skb, struct net_device *dev);
-int cvm_oct_transmit_qos(struct net_device *dev, void *work_queue_entry,
-			 int do_free, int qos);
-void cvm_oct_tx_initialize(void);
-void cvm_oct_tx_shutdown(void);
-void cvm_oct_tx_shutdown_dev(struct net_device *dev);
diff --git a/drivers/staging/octeon/ethernet-util.h b/drivers/staging/octeon/ethernet-util.h
deleted file mode 100644
index 2da5ce1..0000000
--- a/drivers/staging/octeon/ethernet-util.h
+++ /dev/null
@@ -1,72 +0,0 @@
-/**********************************************************************
- * Author: Cavium Networks
- *
- * Contact: support@caviumnetworks.com
- * This file is part of the OCTEON SDK
- *
- * Copyright (c) 2003-2007 Cavium Networks
- *
- * This file is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, Version 2, as
- * published by the Free Software Foundation.
- *
- * This file is distributed in the hope that it will be useful, but
- * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
- * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
- * NONINFRINGEMENT.  See the GNU General Public License for more
- * details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this file; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- * or visit http://www.gnu.org/licenses/.
- *
- * This file may also be available under a different license from Cavium.
- * Contact Cavium Networks for more information
-*********************************************************************/
-
-/**
- * cvm_oct_get_buffer_ptr - convert packet data address to pointer
- * @packet_ptr: Packet data hardware address
- *
- * Returns Packet buffer pointer
- */
-static inline void *cvm_oct_get_buffer_ptr(union cvmx_buf_ptr packet_ptr)
-{
-	return cvmx_phys_to_ptr(((packet_ptr.s.addr >> 7) - packet_ptr.s.back)
-				<< 7);
-}
-
-/**
- * INTERFACE - convert IPD port to logical interface
- * @ipd_port: Port to check
- *
- * Returns Logical interface
- */
-static inline int INTERFACE(int ipd_port)
-{
-	if (ipd_port < 32)	/* Interface 0 or 1 for RGMII,GMII,SPI, etc */
-		return ipd_port >> 4;
-	else if (ipd_port < 36)	/* Interface 2 for NPI */
-		return 2;
-	else if (ipd_port < 40)	/* Interface 3 for loopback */
-		return 3;
-	else if (ipd_port == 40)	/* Non existent interface for POW0 */
-		return 4;
-	else
-		panic("Illegal ipd_port %d passed to INTERFACE\n", ipd_port);
-}
-
-/**
- * INDEX - convert IPD/PKO port number to the port's interface index
- * @ipd_port: Port to check
- *
- * Returns Index into interface port list
- */
-static inline int INDEX(int ipd_port)
-{
-	if (ipd_port < 32)
-		return ipd_port & 15;
-	else
-		return ipd_port & 3;
-}
diff --git a/drivers/staging/octeon/ethernet.c b/drivers/staging/octeon/ethernet.c
deleted file mode 100644
index 3ba38e5..0000000
--- a/drivers/staging/octeon/ethernet.c
+++ /dev/null
@@ -1,878 +0,0 @@
-/**********************************************************************
- * Author: Cavium Networks
- *
- * Contact: support@caviumnetworks.com
- * This file is part of the OCTEON SDK
- *
- * Copyright (c) 2003-2007 Cavium Networks
- *
- * This file is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, Version 2, as
- * published by the Free Software Foundation.
- *
- * This file is distributed in the hope that it will be useful, but
- * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
- * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
- * NONINFRINGEMENT.  See the GNU General Public License for more
- * details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this file; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- * or visit http://www.gnu.org/licenses/.
- *
- * This file may also be available under a different license from Cavium.
- * Contact Cavium Networks for more information
-**********************************************************************/
-#include <linux/platform_device.h>
-#include <linux/kernel.h>
-#include <linux/init.h>
-#include <linux/module.h>
-#include <linux/netdevice.h>
-#include <linux/etherdevice.h>
-#include <linux/phy.h>
-#include <linux/slab.h>
-#include <linux/interrupt.h>
-#include <linux/of_net.h>
-
-#include <net/dst.h>
-
-#include <asm/octeon/octeon.h>
-
-#include "ethernet-defines.h"
-#include "octeon-ethernet.h"
-#include "ethernet-mem.h"
-#include "ethernet-rx.h"
-#include "ethernet-tx.h"
-#include "ethernet-util.h"
-
-#include <asm/octeon/cvmx-pip.h>
-#include <asm/octeon/cvmx-pko.h>
-#include <asm/octeon/cvmx-fau.h>
-#include <asm/octeon/cvmx-ipd.h>
-#include <asm/octeon/cvmx-helper.h>
-
-#include <asm/octeon/cvmx-gmxx-defs.h>
-#include <asm/octeon/cvmx-smix-defs.h>
-
-#if defined(CONFIG_CAVIUM_OCTEON_NUM_PACKET_BUFFERS) \
-	&& CONFIG_CAVIUM_OCTEON_NUM_PACKET_BUFFERS
-int num_packet_buffers = CONFIG_CAVIUM_OCTEON_NUM_PACKET_BUFFERS;
-#else
-int num_packet_buffers = 1024;
-#endif
-module_param(num_packet_buffers, int, 0444);
-MODULE_PARM_DESC(num_packet_buffers, "\n"
-	"\tNumber of packet buffers to allocate and store in the\n"
-	"\tFPA. By default, 1024 packet buffers are used unless\n"
-	"\tCONFIG_CAVIUM_OCTEON_NUM_PACKET_BUFFERS is defined.");
-
-int pow_receive_group = 15;
-module_param(pow_receive_group, int, 0444);
-MODULE_PARM_DESC(pow_receive_group, "\n"
-	"\tPOW group to receive packets from. All ethernet hardware\n"
-	"\twill be configured to send incoming packets to this POW\n"
-	"\tgroup. Also any other software can submit packets to this\n"
-	"\tgroup for the kernel to process.");
-
-int pow_send_group = -1;
-module_param(pow_send_group, int, 0644);
-MODULE_PARM_DESC(pow_send_group, "\n"
-	"\tPOW group to send packets to other software on. This\n"
-	"\tcontrols the creation of the virtual device pow0.\n"
-	"\talways_use_pow also depends on this value.");
-
-int always_use_pow;
-module_param(always_use_pow, int, 0444);
-MODULE_PARM_DESC(always_use_pow, "\n"
-	"\tWhen set, always send to the pow group. This will cause\n"
-	"\tpackets sent to real ethernet devices to be sent to the\n"
-	"\tPOW group instead of the hardware. Unless some other\n"
-	"\tapplication changes the config, packets will still be\n"
-	"\treceived from the low level hardware. Use this option\n"
-	"\tto allow a CVMX app to intercept all packets from the\n"
-	"\tlinux kernel. You must specify pow_send_group along with\n"
-	"\tthis option.");
-
-char pow_send_list[128] = "";
-module_param_string(pow_send_list, pow_send_list, sizeof(pow_send_list), 0444);
-MODULE_PARM_DESC(pow_send_list, "\n"
-	"\tComma separated list of ethernet devices that should use the\n"
-	"\tPOW for transmit instead of the actual ethernet hardware. This\n"
-	"\tis a per port version of always_use_pow. always_use_pow takes\n"
-	"\tprecedence over this list. For example, setting this to\n"
-	"\t\"eth2,spi3,spi7\" would cause these three devices to transmit\n"
-	"\tusing the pow_send_group.");
-
-int max_rx_cpus = -1;
-module_param(max_rx_cpus, int, 0444);
-MODULE_PARM_DESC(max_rx_cpus, "\n"
-	"\t\tThe maximum number of CPUs to use for packet reception.\n"
-	"\t\tUse -1 to use all available CPUs.");
-
-int rx_napi_weight = 32;
-module_param(rx_napi_weight, int, 0444);
-MODULE_PARM_DESC(rx_napi_weight, "The NAPI WEIGHT parameter.");
-
-/**
- * cvm_oct_poll_queue - Workqueue for polling operations.
- */
-struct workqueue_struct *cvm_oct_poll_queue;
-
-/**
- * cvm_oct_poll_queue_stopping - flag to indicate polling should stop.
- *
- * Set to one right before cvm_oct_poll_queue is destroyed.
- */
-atomic_t cvm_oct_poll_queue_stopping = ATOMIC_INIT(0);
-
-/**
- * Array of every ethernet device owned by this driver indexed by
- * the ipd input port number.
- */
-struct net_device *cvm_oct_device[TOTAL_NUMBER_OF_PORTS];
-
-u64 cvm_oct_tx_poll_interval;
-
-static void cvm_oct_rx_refill_worker(struct work_struct *work);
-static DECLARE_DELAYED_WORK(cvm_oct_rx_refill_work, cvm_oct_rx_refill_worker);
-
-static void cvm_oct_rx_refill_worker(struct work_struct *work)
-{
-	/*
-	 * FPA 0 may have been drained, try to refill it if we need
-	 * more than num_packet_buffers / 2, otherwise normal receive
-	 * processing will refill it.  If it were drained, no packets
-	 * could be received so cvm_oct_napi_poll would never be
-	 * invoked to do the refill.
-	 */
-	cvm_oct_rx_refill_pool(num_packet_buffers / 2);
-
-	if (!atomic_read(&cvm_oct_poll_queue_stopping))
-		queue_delayed_work(cvm_oct_poll_queue,
-				   &cvm_oct_rx_refill_work, HZ);
-}
-
-static void cvm_oct_periodic_worker(struct work_struct *work)
-{
-	struct octeon_ethernet *priv = container_of(work,
-						    struct octeon_ethernet,
-						    port_periodic_work.work);
-	void (*poll_fn) (struct net_device *);
-
-	spin_lock(&priv->poll_lock);
-	poll_fn = priv->poll;
-	spin_unlock(&priv->poll_lock);
-
-	if (poll_fn)
-		poll_fn(priv->netdev);
-
-	priv->netdev->netdev_ops->ndo_get_stats(priv->netdev);
-
-	if (!atomic_read(&cvm_oct_poll_queue_stopping))
-		queue_delayed_work(cvm_oct_poll_queue, &priv->port_periodic_work, HZ);
- }
-
-static void cvm_oct_configure_common_hw(void)
-{
-	union cvmx_ipd_ctl_status ipd_ctl_status;
-
-	/* Setup the FPA */
-	cvmx_fpa_enable();
-	cvm_oct_mem_fill_fpa(CVMX_FPA_PACKET_POOL, CVMX_FPA_PACKET_POOL_SIZE,
-			     num_packet_buffers);
-	cvm_oct_mem_fill_fpa(CVMX_FPA_WQE_POOL, CVMX_FPA_WQE_POOL_SIZE,
-			     num_packet_buffers);
-	if (CVMX_FPA_OUTPUT_BUFFER_POOL != CVMX_FPA_PACKET_POOL)
-		cvm_oct_mem_fill_fpa(CVMX_FPA_OUTPUT_BUFFER_POOL,
-				     CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE, 128);
-
-#ifdef __LITTLE_ENDIAN
-	ipd_ctl_status.u64 = cvmx_read_csr(CVMX_IPD_CTL_STATUS);
-	ipd_ctl_status.s.pkt_lend = 1;
-	ipd_ctl_status.s.wqe_lend = 1;
-	cvmx_write_csr(CVMX_IPD_CTL_STATUS, ipd_ctl_status.u64);
-#else
-	ipd_ctl_status.u64 = 0LL;
-#endif
-	if (USE_RED)
-		cvmx_helper_setup_red(num_packet_buffers / 4,
-				      num_packet_buffers / 8);
-
-}
-
-/**
- * cvm_oct_free_work- Free a work queue entry
- *
- * @work_queue_entry: Work queue entry to free
- *
- * Returns Zero on success, Negative on failure.
- */
-int cvm_oct_free_work(void *work_queue_entry)
-{
-	cvmx_wqe_t *work = work_queue_entry;
-
-	int segments = work->word2.s.bufs;
-	union cvmx_buf_ptr segment_ptr = work->packet_ptr;
-
-	while (segments--) {
-		union cvmx_buf_ptr next_ptr = *(union cvmx_buf_ptr *)
-			cvmx_phys_to_ptr(segment_ptr.s.addr - 8);
-		if (unlikely(!segment_ptr.s.i))
-			cvmx_fpa_free(cvm_oct_get_buffer_ptr(segment_ptr),
-				      segment_ptr.s.pool,
-				      DONT_WRITEBACK(CVMX_FPA_PACKET_POOL_SIZE /
-						     128));
-		segment_ptr = next_ptr;
-	}
-	cvmx_fpa_free(work, CVMX_FPA_WQE_POOL, DONT_WRITEBACK(1));
-
-	return 0;
-}
-EXPORT_SYMBOL(cvm_oct_free_work);
-
-/**
- * cvm_oct_common_get_stats - get the low level ethernet statistics
- * @dev:    Device to get the statistics from
- *
- * Returns Pointer to the statistics
- */
-static struct net_device_stats *cvm_oct_common_get_stats(struct net_device *dev)
-{
-	cvmx_pip_port_status_t rx_status;
-	cvmx_pko_port_status_t tx_status;
-	struct octeon_ethernet *priv = netdev_priv(dev);
-
-	if (priv->port < CVMX_PIP_NUM_INPUT_PORTS) {
-		if (octeon_is_simulation()) {
-			/* The simulator doesn't support statistics */
-			memset(&rx_status, 0, sizeof(rx_status));
-			memset(&tx_status, 0, sizeof(tx_status));
-		} else {
-			cvmx_pip_get_port_status(priv->port, 1, &rx_status);
-			cvmx_pko_get_port_status(priv->port, 1, &tx_status);
-		}
-
-		priv->stats.rx_packets += rx_status.inb_packets;
-		priv->stats.tx_packets += tx_status.packets;
-		priv->stats.rx_bytes += rx_status.inb_octets;
-		priv->stats.tx_bytes += tx_status.octets;
-		priv->stats.multicast += rx_status.multicast_packets;
-		priv->stats.rx_crc_errors += rx_status.inb_errors;
-		priv->stats.rx_frame_errors += rx_status.fcs_align_err_packets;
-
-		/*
-		 * The drop counter must be incremented atomically
-		 * since the RX tasklet also increments it.
-		 */
-#ifdef CONFIG_64BIT
-		atomic64_add(rx_status.dropped_packets,
-			     (atomic64_t *)&priv->stats.rx_dropped);
-#else
-		atomic_add(rx_status.dropped_packets,
-			     (atomic_t *)&priv->stats.rx_dropped);
-#endif
-	}
-
-	return &priv->stats;
-}
-
-/**
- * cvm_oct_common_change_mtu - change the link MTU
- * @dev:     Device to change
- * @new_mtu: The new MTU
- *
- * Returns Zero on success
- */
-static int cvm_oct_common_change_mtu(struct net_device *dev, int new_mtu)
-{
-	struct octeon_ethernet *priv = netdev_priv(dev);
-	int interface = INTERFACE(priv->port);
-	int index = INDEX(priv->port);
-#if defined(CONFIG_VLAN_8021Q) || defined(CONFIG_VLAN_8021Q_MODULE)
-	int vlan_bytes = 4;
-#else
-	int vlan_bytes = 0;
-#endif
-
-	/*
-	 * Limit the MTU to make sure the ethernet packets are between
-	 * 64 bytes and 65535 bytes.
-	 */
-	if ((new_mtu + 14 + 4 + vlan_bytes < 64)
-	    || (new_mtu + 14 + 4 + vlan_bytes > 65392)) {
-		pr_err("MTU must be between %d and %d.\n",
-		       64 - 14 - 4 - vlan_bytes, 65392 - 14 - 4 - vlan_bytes);
-		return -EINVAL;
-	}
-	dev->mtu = new_mtu;
-
-	if ((interface < 2)
-	    && (cvmx_helper_interface_get_mode(interface) !=
-		CVMX_HELPER_INTERFACE_MODE_SPI)) {
-		/* Add ethernet header and FCS, and VLAN if configured. */
-		int max_packet = new_mtu + 14 + 4 + vlan_bytes;
-
-		if (OCTEON_IS_MODEL(OCTEON_CN3XXX)
-		    || OCTEON_IS_MODEL(OCTEON_CN58XX)) {
-			/* Signal errors on packets larger than the MTU */
-			cvmx_write_csr(CVMX_GMXX_RXX_FRM_MAX(index, interface),
-				       max_packet);
-		} else {
-			/*
-			 * Set the hardware to truncate packets larger
-			 * than the MTU and smaller the 64 bytes.
-			 */
-			union cvmx_pip_frm_len_chkx frm_len_chk;
-			frm_len_chk.u64 = 0;
-			frm_len_chk.s.minlen = 64;
-			frm_len_chk.s.maxlen = max_packet;
-			cvmx_write_csr(CVMX_PIP_FRM_LEN_CHKX(interface),
-				       frm_len_chk.u64);
-		}
-		/*
-		 * Set the hardware to truncate packets larger than
-		 * the MTU. The jabber register must be set to a
-		 * multiple of 8 bytes, so round up.
-		 */
-		cvmx_write_csr(CVMX_GMXX_RXX_JABBER(index, interface),
-			       (max_packet + 7) & ~7u);
-	}
-	return 0;
-}
-
-/**
- * cvm_oct_common_set_multicast_list - set the multicast list
- * @dev:    Device to work on
- */
-static void cvm_oct_common_set_multicast_list(struct net_device *dev)
-{
-	union cvmx_gmxx_prtx_cfg gmx_cfg;
-	struct octeon_ethernet *priv = netdev_priv(dev);
-	int interface = INTERFACE(priv->port);
-	int index = INDEX(priv->port);
-
-	if ((interface < 2)
-	    && (cvmx_helper_interface_get_mode(interface) !=
-		CVMX_HELPER_INTERFACE_MODE_SPI)) {
-		union cvmx_gmxx_rxx_adr_ctl control;
-		control.u64 = 0;
-		control.s.bcst = 1;	/* Allow broadcast MAC addresses */
-
-		if (!netdev_mc_empty(dev) || (dev->flags & IFF_ALLMULTI) ||
-		    (dev->flags & IFF_PROMISC))
-			/* Force accept multicast packets */
-			control.s.mcst = 2;
-		else
-			/* Force reject multicast packets */
-			control.s.mcst = 1;
-
-		if (dev->flags & IFF_PROMISC)
-			/*
-			 * Reject matches if promisc. Since CAM is
-			 * shut off, should accept everything.
-			 */
-			control.s.cam_mode = 0;
-		else
-			/* Filter packets based on the CAM */
-			control.s.cam_mode = 1;
-
-		gmx_cfg.u64 =
-		    cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
-		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface),
-			       gmx_cfg.u64 & ~1ull);
-
-		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CTL(index, interface),
-			       control.u64);
-		if (dev->flags & IFF_PROMISC)
-			cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM_EN
-				       (index, interface), 0);
-		else
-			cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM_EN
-				       (index, interface), 1);
-
-		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface),
-			       gmx_cfg.u64);
-	}
-}
-
-/**
- * cvm_oct_common_set_mac_address - set the hardware MAC address for a device
- * @dev:    The device in question.
- * @addr:   Address structure to change it too.
-
- * Returns Zero on success
- */
-static int cvm_oct_set_mac_filter(struct net_device *dev)
-{
-	struct octeon_ethernet *priv = netdev_priv(dev);
-	union cvmx_gmxx_prtx_cfg gmx_cfg;
-	int interface = INTERFACE(priv->port);
-	int index = INDEX(priv->port);
-
-	if ((interface < 2)
-	    && (cvmx_helper_interface_get_mode(interface) !=
-		CVMX_HELPER_INTERFACE_MODE_SPI)) {
-		int i;
-		uint8_t *ptr = dev->dev_addr;
-		uint64_t mac = 0;
-		for (i = 0; i < 6; i++)
-			mac = (mac << 8) | (uint64_t)ptr[i];
-
-		gmx_cfg.u64 =
-		    cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
-		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface),
-			       gmx_cfg.u64 & ~1ull);
-
-		cvmx_write_csr(CVMX_GMXX_SMACX(index, interface), mac);
-		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM0(index, interface),
-			       ptr[0]);
-		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM1(index, interface),
-			       ptr[1]);
-		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM2(index, interface),
-			       ptr[2]);
-		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM3(index, interface),
-			       ptr[3]);
-		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM4(index, interface),
-			       ptr[4]);
-		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM5(index, interface),
-			       ptr[5]);
-		cvm_oct_common_set_multicast_list(dev);
-		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface),
-			       gmx_cfg.u64);
-	}
-	return 0;
-}
-
-static int cvm_oct_common_set_mac_address(struct net_device *dev, void *addr)
-{
-	int r = eth_mac_addr(dev, addr);
-
-	if (r)
-		return r;
-	return cvm_oct_set_mac_filter(dev);
-}
-
-/**
- * cvm_oct_common_init - per network device initialization
- * @dev:    Device to initialize
- *
- * Returns Zero on success
- */
-int cvm_oct_common_init(struct net_device *dev)
-{
-	struct octeon_ethernet *priv = netdev_priv(dev);
-	const u8 *mac = NULL;
-
-	if (priv->of_node)
-		mac = of_get_mac_address(priv->of_node);
-
-	if (mac && is_valid_ether_addr(mac))
-		memcpy(dev->dev_addr, mac, ETH_ALEN);
-	else
-		eth_hw_addr_random(dev);
-
-	/*
-	 * Force the interface to use the POW send if always_use_pow
-	 * was specified or it is in the pow send list.
-	 */
-	if ((pow_send_group != -1)
-	    && (always_use_pow || strstr(pow_send_list, dev->name)))
-		priv->queue = -1;
-
-	if (priv->queue != -1) {
-		dev->features |= NETIF_F_SG;
-		if (USE_HW_TCPUDP_CHECKSUM)
-			dev->features |= NETIF_F_IP_CSUM;
-	}
-
-	/* We do our own locking, Linux doesn't need to */
-	dev->features |= NETIF_F_LLTX;
-	SET_ETHTOOL_OPS(dev, &cvm_oct_ethtool_ops);
-
-	cvm_oct_set_mac_filter(dev);
-	dev->netdev_ops->ndo_change_mtu(dev, dev->mtu);
-
-	/*
-	 * Zero out stats for port so we won't mistakenly show
-	 * counters from the bootloader.
-	 */
-	memset(dev->netdev_ops->ndo_get_stats(dev), 0,
-	       sizeof(struct net_device_stats));
-
-	return 0;
-}
-
-static const struct net_device_ops cvm_oct_npi_netdev_ops = {
-	.ndo_init		= cvm_oct_common_init,
-	.ndo_start_xmit		= cvm_oct_xmit,
-	.ndo_set_rx_mode	= cvm_oct_common_set_multicast_list,
-	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
-	.ndo_do_ioctl		= cvm_oct_ioctl,
-	.ndo_change_mtu		= cvm_oct_common_change_mtu,
-	.ndo_get_stats		= cvm_oct_common_get_stats,
-#ifdef CONFIG_NET_POLL_CONTROLLER
-	.ndo_poll_controller	= cvm_oct_poll_controller,
-#endif
-};
-
-/* SGMII and XAUI handled the same so they both use this. */
-static const struct net_device_ops cvm_oct_sgmii_netdev_ops = {
-	.ndo_init		= cvm_oct_sgmii_init,
-	.ndo_open		= cvm_oct_sgmii_open,
-	.ndo_stop		= cvm_oct_sgmii_stop,
-	.ndo_start_xmit		= cvm_oct_xmit,
-	.ndo_set_rx_mode	= cvm_oct_common_set_multicast_list,
-	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
-	.ndo_do_ioctl		= cvm_oct_ioctl,
-	.ndo_change_mtu		= cvm_oct_common_change_mtu,
-	.ndo_get_stats		= cvm_oct_common_get_stats,
-#ifdef CONFIG_NET_POLL_CONTROLLER
-	.ndo_poll_controller	= cvm_oct_poll_controller,
-#endif
-};
-static const struct net_device_ops cvm_oct_spi_netdev_ops = {
-	.ndo_init		= cvm_oct_spi_init,
-	.ndo_uninit		= cvm_oct_spi_uninit,
-	.ndo_start_xmit		= cvm_oct_xmit,
-	.ndo_set_rx_mode	= cvm_oct_common_set_multicast_list,
-	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
-	.ndo_do_ioctl		= cvm_oct_ioctl,
-	.ndo_change_mtu		= cvm_oct_common_change_mtu,
-	.ndo_get_stats		= cvm_oct_common_get_stats,
-#ifdef CONFIG_NET_POLL_CONTROLLER
-	.ndo_poll_controller	= cvm_oct_poll_controller,
-#endif
-};
-static const struct net_device_ops cvm_oct_rgmii_netdev_ops = {
-	.ndo_init		= cvm_oct_rgmii_init,
-	.ndo_open		= cvm_oct_rgmii_open,
-	.ndo_stop		= cvm_oct_rgmii_stop,
-	.ndo_start_xmit		= cvm_oct_xmit,
-	.ndo_set_rx_mode	= cvm_oct_common_set_multicast_list,
-	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
-	.ndo_do_ioctl		= cvm_oct_ioctl,
-	.ndo_change_mtu		= cvm_oct_common_change_mtu,
-	.ndo_get_stats		= cvm_oct_common_get_stats,
-#ifdef CONFIG_NET_POLL_CONTROLLER
-	.ndo_poll_controller	= cvm_oct_poll_controller,
-#endif
-};
-static const struct net_device_ops cvm_oct_pow_netdev_ops = {
-	.ndo_init		= cvm_oct_common_init,
-	.ndo_start_xmit		= cvm_oct_xmit_pow,
-	.ndo_set_rx_mode	= cvm_oct_common_set_multicast_list,
-	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
-	.ndo_do_ioctl		= cvm_oct_ioctl,
-	.ndo_change_mtu		= cvm_oct_common_change_mtu,
-	.ndo_get_stats		= cvm_oct_common_get_stats,
-#ifdef CONFIG_NET_POLL_CONTROLLER
-	.ndo_poll_controller	= cvm_oct_poll_controller,
-#endif
-};
-
-extern void octeon_mdiobus_force_mod_depencency(void);
-
-static struct device_node *cvm_oct_of_get_child(const struct device_node *parent,
-							   int reg_val)
-{
-	struct device_node *node = NULL;
-	int size;
-	const __be32 *addr;
-
-	for (;;) {
-		node = of_get_next_child(parent, node);
-		if (!node)
-			break;
-		addr = of_get_property(node, "reg", &size);
-		if (addr && (be32_to_cpu(*addr) == reg_val))
-			break;
-	}
-	return node;
-}
-
-static struct device_node *cvm_oct_node_for_port(struct device_node *pip,
-							    int interface, int port)
-{
-	struct device_node *ni, *np;
-
-	ni = cvm_oct_of_get_child(pip, interface);
-	if (!ni)
-		return NULL;
-
-	np = cvm_oct_of_get_child(ni, port);
-	of_node_put(ni);
-
-	return np;
-}
-
-static int cvm_oct_probe(struct platform_device *pdev)
-{
-	int num_interfaces;
-	int interface;
-	int fau = FAU_NUM_PACKET_BUFFERS_TO_FREE;
-	int qos;
-	struct device_node *pip;
-
-	octeon_mdiobus_force_mod_depencency();
-	pr_notice("cavium-ethernet %s\n", OCTEON_ETHERNET_VERSION);
-
-	pip = pdev->dev.of_node;
-	if (!pip) {
-		pr_err("Error: No 'pip' in /aliases\n");
-		return -EINVAL;
-	}
-
-	cvm_oct_poll_queue = create_singlethread_workqueue("octeon-ethernet");
-	if (cvm_oct_poll_queue == NULL) {
-		pr_err("octeon-ethernet: Cannot create workqueue");
-		return -ENOMEM;
-	}
-
-	cvm_oct_configure_common_hw();
-
-	cvmx_helper_initialize_packet_io_global();
-
-	/* Change the input group for all ports before input is enabled */
-	num_interfaces = cvmx_helper_get_number_of_interfaces();
-	for (interface = 0; interface < num_interfaces; interface++) {
-		int num_ports = cvmx_helper_ports_on_interface(interface);
-		int port;
-
-		for (port = cvmx_helper_get_ipd_port(interface, 0);
-		     port < cvmx_helper_get_ipd_port(interface, num_ports);
-		     port++) {
-			union cvmx_pip_prt_tagx pip_prt_tagx;
-			pip_prt_tagx.u64 =
-			    cvmx_read_csr(CVMX_PIP_PRT_TAGX(port));
-			pip_prt_tagx.s.grp = pow_receive_group;
-			cvmx_write_csr(CVMX_PIP_PRT_TAGX(port),
-				       pip_prt_tagx.u64);
-		}
-	}
-
-	cvmx_helper_ipd_and_packet_input_enable();
-
-	memset(cvm_oct_device, 0, sizeof(cvm_oct_device));
-
-	/*
-	 * Initialize the FAU used for counting packet buffers that
-	 * need to be freed.
-	 */
-	cvmx_fau_atomic_write32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);
-
-	/* Initialize the FAU used for counting tx SKBs that need to be freed */
-	cvmx_fau_atomic_write32(FAU_TOTAL_TX_TO_CLEAN, 0);
-
-	if ((pow_send_group != -1)) {
-		struct net_device *dev;
-		pr_info("\tConfiguring device for POW only access\n");
-		dev = alloc_etherdev(sizeof(struct octeon_ethernet));
-		if (dev) {
-			/* Initialize the device private structure. */
-			struct octeon_ethernet *priv = netdev_priv(dev);
-
-			dev->netdev_ops = &cvm_oct_pow_netdev_ops;
-			priv->imode = CVMX_HELPER_INTERFACE_MODE_DISABLED;
-			priv->port = CVMX_PIP_NUM_INPUT_PORTS;
-			priv->queue = -1;
-			strcpy(dev->name, "pow%d");
-			for (qos = 0; qos < 16; qos++)
-				skb_queue_head_init(&priv->tx_free_list[qos]);
-
-			if (register_netdev(dev) < 0) {
-				pr_err("Failed to register ethernet device for POW\n");
-				free_netdev(dev);
-			} else {
-				cvm_oct_device[CVMX_PIP_NUM_INPUT_PORTS] = dev;
-				pr_info("%s: POW send group %d, receive group %d\n",
-					dev->name, pow_send_group,
-					pow_receive_group);
-			}
-		} else {
-			pr_err("Failed to allocate ethernet device for POW\n");
-		}
-	}
-
-	num_interfaces = cvmx_helper_get_number_of_interfaces();
-	for (interface = 0; interface < num_interfaces; interface++) {
-		cvmx_helper_interface_mode_t imode =
-		    cvmx_helper_interface_get_mode(interface);
-		int num_ports = cvmx_helper_ports_on_interface(interface);
-		int port;
-		int port_index;
-
-		for (port_index = 0, port = cvmx_helper_get_ipd_port(interface, 0);
-		     port < cvmx_helper_get_ipd_port(interface, num_ports);
-		     port_index++, port++) {
-			struct octeon_ethernet *priv;
-			struct net_device *dev =
-			    alloc_etherdev(sizeof(struct octeon_ethernet));
-			if (!dev) {
-				pr_err("Failed to allocate ethernet device for port %d\n", port);
-				continue;
-			}
-
-			/* Initialize the device private structure. */
-			priv = netdev_priv(dev);
-			priv->of_node = cvm_oct_node_for_port(pip, interface, port_index);
-			priv->netdev = dev;
-			spin_lock_init(&priv->poll_lock);
-			INIT_DELAYED_WORK(&priv->port_periodic_work,
-					  cvm_oct_periodic_worker);
-			priv->imode = imode;
-			priv->port = port;
-			priv->queue = cvmx_pko_get_base_queue(priv->port);
-			priv->fau = fau - cvmx_pko_get_num_queues(port) * 4;
-			for (qos = 0; qos < 16; qos++)
-				skb_queue_head_init(&priv->tx_free_list[qos]);
-			for (qos = 0; qos < cvmx_pko_get_num_queues(port);
-			     qos++)
-				cvmx_fau_atomic_write32(priv->fau + qos * 4, 0);
-
-			switch (priv->imode) {
-
-			/* These types don't support ports to IPD/PKO */
-			case CVMX_HELPER_INTERFACE_MODE_DISABLED:
-			case CVMX_HELPER_INTERFACE_MODE_PCIE:
-			case CVMX_HELPER_INTERFACE_MODE_PICMG:
-				break;
-
-			case CVMX_HELPER_INTERFACE_MODE_NPI:
-				dev->netdev_ops = &cvm_oct_npi_netdev_ops;
-				strcpy(dev->name, "npi%d");
-				break;
-
-			case CVMX_HELPER_INTERFACE_MODE_XAUI:
-				dev->netdev_ops = &cvm_oct_sgmii_netdev_ops;
-				strcpy(dev->name, "xaui%d");
-				break;
-
-			case CVMX_HELPER_INTERFACE_MODE_LOOP:
-				dev->netdev_ops = &cvm_oct_npi_netdev_ops;
-				strcpy(dev->name, "loop%d");
-				break;
-
-			case CVMX_HELPER_INTERFACE_MODE_SGMII:
-				dev->netdev_ops = &cvm_oct_sgmii_netdev_ops;
-				strcpy(dev->name, "eth%d");
-				break;
-
-			case CVMX_HELPER_INTERFACE_MODE_SPI:
-				dev->netdev_ops = &cvm_oct_spi_netdev_ops;
-				strcpy(dev->name, "spi%d");
-				break;
-
-			case CVMX_HELPER_INTERFACE_MODE_RGMII:
-			case CVMX_HELPER_INTERFACE_MODE_GMII:
-				dev->netdev_ops = &cvm_oct_rgmii_netdev_ops;
-				strcpy(dev->name, "eth%d");
-				break;
-			}
-
-			netif_carrier_off(dev);
-			if (!dev->netdev_ops) {
-				free_netdev(dev);
-			} else if (register_netdev(dev) < 0) {
-				pr_err("Failed to register ethernet device "
-					 "for interface %d, port %d\n",
-					 interface, priv->port);
-				free_netdev(dev);
-			} else {
-				cvm_oct_device[priv->port] = dev;
-				fau -=
-				    cvmx_pko_get_num_queues(priv->port) *
-				    sizeof(uint32_t);
-				queue_delayed_work(cvm_oct_poll_queue,
-						   &priv->port_periodic_work, HZ);
-			}
-		}
-	}
-
-	cvm_oct_tx_initialize();
-	cvm_oct_rx_initialize();
-
-	/*
-	 * 150 uS: about 10 1500-byte packtes at 1GE.
-	 */
-	cvm_oct_tx_poll_interval = 150 * (octeon_get_clock_rate() / 1000000);
-
-	queue_delayed_work(cvm_oct_poll_queue, &cvm_oct_rx_refill_work, HZ);
-
-	return 0;
-}
-
-static int cvm_oct_remove(struct platform_device *pdev)
-{
-	int port;
-
-	/* Disable POW interrupt */
-	cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group), 0);
-
-	cvmx_ipd_disable();
-
-	/* Free the interrupt handler */
-	free_irq(OCTEON_IRQ_WORKQ0 + pow_receive_group, cvm_oct_device);
-
-	atomic_inc_return(&cvm_oct_poll_queue_stopping);
-	cancel_delayed_work_sync(&cvm_oct_rx_refill_work);
-
-	cvm_oct_rx_shutdown();
-	cvm_oct_tx_shutdown();
-
-	cvmx_pko_disable();
-
-	/* Free the ethernet devices */
-	for (port = 0; port < TOTAL_NUMBER_OF_PORTS; port++) {
-		if (cvm_oct_device[port]) {
-			struct net_device *dev = cvm_oct_device[port];
-			struct octeon_ethernet *priv = netdev_priv(dev);
-			cancel_delayed_work_sync(&priv->port_periodic_work);
-
-			cvm_oct_tx_shutdown_dev(dev);
-			unregister_netdev(dev);
-			free_netdev(dev);
-			cvm_oct_device[port] = NULL;
-		}
-	}
-
-	destroy_workqueue(cvm_oct_poll_queue);
-
-	cvmx_pko_shutdown();
-
-	cvmx_ipd_free_ptr();
-
-	/* Free the HW pools */
-	cvm_oct_mem_empty_fpa(CVMX_FPA_PACKET_POOL, CVMX_FPA_PACKET_POOL_SIZE,
-			      num_packet_buffers);
-	cvm_oct_mem_empty_fpa(CVMX_FPA_WQE_POOL, CVMX_FPA_WQE_POOL_SIZE,
-			      num_packet_buffers);
-	if (CVMX_FPA_OUTPUT_BUFFER_POOL != CVMX_FPA_PACKET_POOL)
-		cvm_oct_mem_empty_fpa(CVMX_FPA_OUTPUT_BUFFER_POOL,
-				      CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE, 128);
-	return 0;
-}
-
-static struct of_device_id cvm_oct_match[] = {
-	{
-		.compatible = "cavium,octeon-3860-pip",
-	},
-	{},
-};
-MODULE_DEVICE_TABLE(of, cvm_oct_match);
-
-static struct platform_driver cvm_oct_driver = {
-	.probe		= cvm_oct_probe,
-	.remove		= cvm_oct_remove,
-	.driver		= {
-		.owner	= THIS_MODULE,
-		.name	= KBUILD_MODNAME,
-		.of_match_table = cvm_oct_match,
-	},
-};
-
-module_platform_driver(cvm_oct_driver);
-
-MODULE_LICENSE("GPL");
-MODULE_AUTHOR("Cavium Networks <support@caviumnetworks.com>");
-MODULE_DESCRIPTION("Cavium Networks Octeon ethernet driver.");
diff --git a/drivers/staging/octeon/octeon-ethernet.h b/drivers/staging/octeon/octeon-ethernet.h
deleted file mode 100644
index 58a95f1..0000000
--- a/drivers/staging/octeon/octeon-ethernet.h
+++ /dev/null
@@ -1,110 +0,0 @@
-/**********************************************************************
- * Author: Cavium Networks
- *
- * Contact: support@caviumnetworks.com
- * This file is part of the OCTEON SDK
- *
- * Copyright (c) 2003-2010 Cavium Networks
- *
- * This file is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, Version 2, as
- * published by the Free Software Foundation.
- *
- * This file is distributed in the hope that it will be useful, but
- * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
- * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
- * NONINFRINGEMENT.  See the GNU General Public License for more
- * details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this file; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- * or visit http://www.gnu.org/licenses/.
- *
- * This file may also be available under a different license from Cavium.
- * Contact Cavium Networks for more information
-**********************************************************************/
-
-/*
- * External interface for the Cavium Octeon ethernet driver.
- */
-#ifndef OCTEON_ETHERNET_H
-#define OCTEON_ETHERNET_H
-
-#include <linux/of.h>
-
-#include <asm/octeon/cvmx-helper.h>
-/**
- * This is the definition of the Ethernet driver's private
- * driver state stored in netdev_priv(dev).
- */
-struct octeon_ethernet {
-	/* PKO hardware output port */
-	int port;
-	/* My netdev. */
-	struct net_device *netdev;
-	/* PKO hardware queue for the port */
-	int queue;
-	/* Hardware fetch and add to count outstanding tx buffers */
-	int fau;
-	/*
-	 * Type of port. This is one of the enums in
-	 * cvmx_helper_interface_mode_t
-	 */
-	int imode;
-	/* List of outstanding tx buffers per queue */
-	struct sk_buff_head tx_free_list[16];
-	/* Device statistics */
-	struct net_device_stats stats;
-	struct phy_device *phydev;
-	unsigned int last_link;
-	/* Last negotiated link state */
-	uint64_t link_info;
-	/* Called periodically to check link status */
-	spinlock_t poll_lock;
-	void (*poll) (struct net_device *dev);
-	struct delayed_work	port_periodic_work;
-	struct work_struct	port_work;	/* may be unused. */
-	struct device_node	*of_node;
-};
-
-int cvm_oct_free_work(void *work_queue_entry);
-
-extern int cvm_oct_rgmii_init(struct net_device *dev);
-extern int cvm_oct_rgmii_open(struct net_device *dev);
-extern int cvm_oct_rgmii_stop(struct net_device *dev);
-
-extern int cvm_oct_sgmii_init(struct net_device *dev);
-extern int cvm_oct_sgmii_open(struct net_device *dev);
-extern int cvm_oct_sgmii_stop(struct net_device *dev);
-
-extern int cvm_oct_spi_init(struct net_device *dev);
-extern void cvm_oct_spi_uninit(struct net_device *dev);
-
-extern int cvm_oct_xaui_init(struct net_device *dev);
-extern int cvm_oct_xaui_open(struct net_device *dev);
-extern int cvm_oct_xaui_stop(struct net_device *dev);
-
-extern int cvm_oct_common_init(struct net_device *dev);
-
-extern void cvm_oct_set_carrier(struct octeon_ethernet *priv,
-				cvmx_helper_link_info_t link_info);
-extern void cvm_oct_adjust_link(struct net_device *dev);
-
-extern const struct ethtool_ops cvm_oct_ethtool_ops;
-int cvm_oct_ioctl(struct net_device *dev, struct ifreq *rq, int cmd);
-int cvm_oct_phy_setup_device(struct net_device *dev);
-
-extern int always_use_pow;
-extern int pow_send_group;
-extern int pow_receive_group;
-extern char pow_send_list[];
-extern struct net_device *cvm_oct_device[];
-extern struct workqueue_struct *cvm_oct_poll_queue;
-extern atomic_t cvm_oct_poll_queue_stopping;
-extern u64 cvm_oct_tx_poll_interval;
-
-extern int max_rx_cpus;
-extern int rx_napi_weight;
-
-#endif
-- 
2.6.2

