From 66aed676e6b808c46bf259451721eec08d393242 Mon Sep 17 00:00:00 2001
From: David Daney <david.daney@cavium.com>
Date: Fri, 30 Aug 2013 12:27:09 -0700
Subject: [PATCH 283/974] MIPS: OCTEON: Sync S.E. files to r87484

Signed-off-by: David Daney <david.daney@cavium.com>
[Original patch taken from Cavium SDK 3.1.2-568]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
---
 arch/mips/cavium-octeon/executive/Makefile         |    2 +-
 arch/mips/cavium-octeon/executive/cvmx-bgx.c       |  255 ++++
 arch/mips/cavium-octeon/executive/cvmx-debug.c     |   12 +-
 .../cavium-octeon/executive/cvmx-error-trees.c     |  497 ++++---
 .../cavium-octeon/executive/cvmx-fpa-resource.c    |   12 +-
 .../executive/cvmx-global-resources.c              |    9 +-
 arch/mips/cavium-octeon/executive/cvmx-gser.c      |  277 ++++
 .../mips/cavium-octeon/executive/cvmx-helper-agl.c |    2 +-
 .../cavium-octeon/executive/cvmx-helper-board.c    |  107 +-
 .../mips/cavium-octeon/executive/cvmx-helper-cfg.c |   11 +-
 .../mips/cavium-octeon/executive/cvmx-helper-ilk.c |   16 +-
 .../mips/cavium-octeon/executive/cvmx-helper-npi.c |    3 +-
 .../cavium-octeon/executive/cvmx-helper-rgmii.c    |   29 +-
 .../cavium-octeon/executive/cvmx-helper-sgmii.c    |  167 ++-
 .../cavium-octeon/executive/cvmx-helper-xaui.c     |  156 ++-
 arch/mips/cavium-octeon/executive/cvmx-helper.c    |  906 +++++++------
 arch/mips/cavium-octeon/executive/cvmx-ilk.c       |    2 +-
 arch/mips/cavium-octeon/executive/cvmx-pcie.c      |    4 +-
 arch/mips/cavium-octeon/executive/cvmx-pki.c       |   75 +-
 arch/mips/cavium-octeon/executive/cvmx-qlm.c       |  450 +++----
 arch/mips/cavium-octeon/executive/cvmx-usb.c       | 1231 +++++++++++------
 arch/mips/cavium-octeon/executive/octeon-feature.c |    4 +-
 arch/mips/include/asm/octeon/cvmx-app-hotplug.h    |    2 +-
 arch/mips/include/asm/octeon/cvmx-app-init.h       |    2 +-
 arch/mips/include/asm/octeon/cvmx-ase-defs.h       |   14 +-
 arch/mips/include/asm/octeon/cvmx-bgx.h            |   64 +
 arch/mips/include/asm/octeon/cvmx-bgxx-defs.h      |   57 +-
 arch/mips/include/asm/octeon/cvmx-ciu3-defs.h      |   12 +-
 arch/mips/include/asm/octeon/cvmx-clock.h          |    1 +
 arch/mips/include/asm/octeon/cvmx-coremask.h       |   17 +-
 arch/mips/include/asm/octeon/cvmx-dpi-defs.h       |   60 +-
 .../include/asm/octeon/cvmx-global-resources.h     |   20 +-
 arch/mips/include/asm/octeon/cvmx-gser.h           |   63 +
 arch/mips/include/asm/octeon/cvmx-gserx-defs.h     |  961 +++++++++++---
 arch/mips/include/asm/octeon/cvmx-helper-board.h   |   15 +-
 arch/mips/include/asm/octeon/cvmx-helper-rgmii.h   |   15 +-
 arch/mips/include/asm/octeon/cvmx-helper-sgmii.h   |   77 +-
 arch/mips/include/asm/octeon/cvmx-helper-xaui.h    |   73 +-
 arch/mips/include/asm/octeon/cvmx-ila-defs.h       |   12 +-
 arch/mips/include/asm/octeon/cvmx-ilk-defs.h       |   85 +-
 arch/mips/include/asm/octeon/cvmx-l2c-defs.h       |  425 +++---
 arch/mips/include/asm/octeon/cvmx-lapx-defs.h      |    9 +-
 arch/mips/include/asm/octeon/cvmx-lmcx-defs.h      |  931 ++++++-------
 arch/mips/include/asm/octeon/cvmx-mio-defs.h       |    7 +-
 arch/mips/include/asm/octeon/cvmx-ocx-defs.h       |   14 +-
 arch/mips/include/asm/octeon/cvmx-osm-defs.h       |   93 +-
 arch/mips/include/asm/octeon/cvmx-pemx-defs.h      |  563 +++++---
 arch/mips/include/asm/octeon/cvmx-pexp-defs.h      |    5 +-
 arch/mips/include/asm/octeon/cvmx-pki-defs.h       |   89 +-
 arch/mips/include/asm/octeon/cvmx-pki.h            |   27 +-
 arch/mips/include/asm/octeon/cvmx-pko-defs.h       | 1383 ++++++++++++++------
 arch/mips/include/asm/octeon/cvmx-pow.h            |  124 +-
 arch/mips/include/asm/octeon/cvmx-qlm.h            |   39 +-
 arch/mips/include/asm/octeon/cvmx-sli-defs.h       |   49 +-
 arch/mips/include/asm/octeon/cvmx-sso-defs.h       |    8 +-
 arch/mips/include/asm/octeon/cvmx-sysinfo.h        |   32 +-
 arch/mips/include/asm/octeon/cvmx-uctlx-defs.h     |   73 +-
 arch/mips/include/asm/octeon/octeon-boot-info.h    |    4 +-
 arch/mips/include/asm/octeon/octeon-feature.h      |    4 +-
 59 files changed, 6472 insertions(+), 3184 deletions(-)
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-bgx.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-gser.c
 create mode 100644 arch/mips/include/asm/octeon/cvmx-bgx.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-gser.h

diff --git a/arch/mips/cavium-octeon/executive/Makefile b/arch/mips/cavium-octeon/executive/Makefile
index 4871387..97bf781 100644
--- a/arch/mips/cavium-octeon/executive/Makefile
+++ b/arch/mips/cavium-octeon/executive/Makefile
@@ -20,7 +20,7 @@ obj-y += cvmx-pko.o cvmx-spi.o cvmx-cmd-queue.o cvmx-helper-cfg.o	\
 	cvmx-helper-board.o cvmx-helper.o cvmx-helper-xaui.o \
 	cvmx-helper-rgmii.o cvmx-helper-sgmii.o cvmx-helper-npi.o \
 	cvmx-helper-loop.o cvmx-helper-spi.o cvmx-helper-util.o	\
-	cvmx-pki-resources.o
+	cvmx-pki-resources.o cvmx-gser.o cvmx-bgx.o
 
 obj-y += cvmx-helper-errata.o cvmx-helper-jtag.o
 obj-y += cvmx-pcie.o
diff --git a/arch/mips/cavium-octeon/executive/cvmx-bgx.c b/arch/mips/cavium-octeon/executive/cvmx-bgx.c
new file mode 100644
index 0000000..8e3f6e1
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-bgx.c
@@ -0,0 +1,255 @@
+/***********************license start***************
+ * Copyright (c) 2013  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Functions to configure the BGX MAC.
+ *
+ * <hr>$Revision$<hr>
+ */
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include <asm/octeon/cvmx.h>
+#include <asm/octeon/cvmx-helper.h>
+#include <asm/octeon/cvmx-qlm.h>
+#include <asm/octeon/cvmx-bgxx-defs.h>
+#else
+#include "cvmx.h"
+#include "cvmx-helper.h"
+#include "cvmx-qlm.h"
+#endif
+
+
+/**
+ * @INTERNAL
+ * Configure the bgx mac for xaui.
+ *
+ * @param interface Interface to initialize
+ *
+ * @param num_ports Number of ports on interface
+ *
+ * @return Zero on success, negative on failure
+ */
+static int bgx_xaui_init(int 	interface,
+			 int	num_ports)
+{
+	cvmx_bgxx_smux_tx_thresh_t	smu_tx_thresh;
+	cvmx_bgxx_spux_control1_t	spu_control1;
+	cvmx_bgxx_spux_misc_control_t	spu_misc_control;
+	cvmx_bgxx_cmrx_config_t		cmr_config;
+	int				val;
+	int				i;
+
+	smu_tx_thresh.u64 = 0;
+	smu_tx_thresh.s.cnt = 0x30;
+	cvmx_write_csr(CVMX_BGXX_SMUX_TX_THRESH(0, interface),
+		       smu_tx_thresh.u64);
+
+	spu_control1.u64 = cvmx_read_csr(CVMX_BGXX_SPUX_CONTROL1(0, interface));
+	spu_control1.s.lo_pwr = 0;
+	cvmx_write_csr(CVMX_BGXX_SPUX_CONTROL1(0, interface), spu_control1.u64);
+
+	spu_misc_control.u64 =
+		cvmx_read_csr(CVMX_BGXX_SPUX_MISC_CONTROL(0, interface));
+	spu_misc_control.s.rx_packet_dis = 0;
+	cvmx_write_csr(CVMX_BGXX_SPUX_MISC_CONTROL(0, interface),
+		       spu_misc_control.u64);
+
+	/* Check if interface 0 or 1 must be routed to the mix */
+	if ((interface == 0 && MUX_78XX_IFACE0) ||
+	    (interface == 1 && MUX_78XX_IFACE1))
+		val = 1;
+	else
+		val = 0;
+
+	/* Enable bgx */
+	for (i = 0; i < num_ports; i++) {
+		cmr_config.u64 =
+			cvmx_read_csr(CVMX_BGXX_CMRX_CONFIG(i, interface));
+		cmr_config.s.lmac_type = 1;
+		cmr_config.s.mix_en = val;
+		cmr_config.s.lane_to_sds = 0xe4;
+		cmr_config.s.data_pkt_rx_en = 1;
+		cmr_config.s.data_pkt_tx_en = 1;
+		cmr_config.s.enable = 1;
+		cvmx_write_csr(CVMX_BGXX_CMRX_CONFIG(i, interface),
+			       cmr_config.u64);
+	}
+
+	return 0;
+}
+
+/**
+ * @INTERNAL
+ * Configure the bgx mac for sgmii.
+ *
+ * @param interface Interface to initialize
+ *
+ * @param num_ports Number of ports on interface
+ *
+ * @return Zero on success, negative on failure
+ */
+static int bgx_sgmii_init(int 	interface,
+			  int	num_ports)
+{
+	cvmx_bgxx_gmp_gmi_txx_thresh_t	gmp_gmi_tx_thresh;
+	cvmx_bgxx_gmp_gmi_prtx_cfg_t	gmp_gmi_prt_cfg;
+	cvmx_bgxx_gmp_pcs_mrx_control_t	gmp_pcs_mr_control;
+	cvmx_bgxx_gmp_pcs_miscx_ctl_t	gmp_pcs_misc_ctl;
+	cvmx_bgxx_cmrx_config_t		cmr_config;
+	int				val;
+	int				i;
+
+	for (i = 0; i < num_ports; i++) {
+		/* Configure gmp */
+		gmp_gmi_tx_thresh.u64 = 0;
+		gmp_gmi_tx_thresh.s.cnt = 0x30;
+		cvmx_write_csr(CVMX_BGXX_GMP_GMI_TXX_THRESH(i, interface),
+			       gmp_gmi_tx_thresh.u64);
+
+		gmp_gmi_prt_cfg.u64 = 0;
+		gmp_gmi_prt_cfg.s.speed = 1;
+		gmp_gmi_prt_cfg.s.speed_msb = 0;
+		gmp_gmi_prt_cfg.s.duplex = 1;
+		gmp_gmi_prt_cfg.s.slottime = 1;
+		cvmx_write_csr(CVMX_BGXX_GMP_GMI_PRTX_CFG(i, interface),
+			       gmp_gmi_prt_cfg.u64);
+
+		gmp_pcs_mr_control.u64 = 
+			cvmx_read_csr(CVMX_BGXX_GMP_PCS_MRX_CONTROL(i, interface));
+		gmp_pcs_mr_control.s.pwr_dn = 0;
+		cvmx_write_csr(CVMX_BGXX_GMP_PCS_MRX_CONTROL(i, interface),
+			       gmp_pcs_mr_control.u64);
+
+		gmp_pcs_misc_ctl.u64 = 
+			cvmx_read_csr(CVMX_BGXX_GMP_PCS_MISCX_CTL(i, interface));
+		gmp_pcs_misc_ctl.s.gmxeno = 0;
+		gmp_pcs_misc_ctl.s.mac_phy = 0;
+		gmp_pcs_misc_ctl.s.mode = 0;
+		cvmx_write_csr(CVMX_BGXX_GMP_PCS_MISCX_CTL(i, interface),
+			       gmp_pcs_misc_ctl.u64);
+	}
+
+	/* Check if interface 0 or 1 must be routed to the mix */
+	if ((interface == 0 && MUX_78XX_IFACE0) ||
+	    (interface == 1 && MUX_78XX_IFACE1))
+		val = 1;
+	else
+		val = 0;
+
+	/* Enable bgx */
+	for (i = 0; i < num_ports; i++) {
+		cmr_config.u64 =
+			cvmx_read_csr(CVMX_BGXX_CMRX_CONFIG(i, interface));
+		cmr_config.s.lmac_type = 0;
+		cmr_config.s.mix_en = val;
+		cmr_config.s.lane_to_sds = i;
+		cmr_config.s.data_pkt_rx_en = 1;
+		cmr_config.s.data_pkt_tx_en = 1;
+		cmr_config.s.enable = 1;
+		cvmx_write_csr(CVMX_BGXX_CMRX_CONFIG(i, interface),
+			       cmr_config.u64);
+	}
+
+	return 0;
+}
+
+/**
+ * @INTERNAL
+ * Configure the bgx mac.
+ *
+ * @param interface Interface to bring up
+ *
+ * @param mode      Mode to configure the bgx mac as
+ *
+ * @return Zero on success, negative on failure
+ */
+int bgx_init(int				interface,
+	     cvmx_helper_interface_mode_t	mode)
+{
+	cvmx_bgxx_cmrx_config_t		cmr_config;
+	cvmx_bgxx_cmr_rx_lmacs_t	bgx_cmr_rx_lmacs;
+	cvmx_bgxx_cmr_tx_lmacs_t	bgx_cmr_tx_lmacs;
+	cvmx_bgxx_cmr_global_config_t	bgx_cmr_global_config;
+	int				num_ports;
+	int				val;
+	int				i;
+
+	num_ports = cvmx_helper_ports_on_interface(interface);
+
+	/* Disable cmr for all interface ports */
+	for (i = 0; i < num_ports; i++) {
+		cmr_config.u64 =
+			cvmx_read_csr(CVMX_BGXX_CMRX_CONFIG(i, interface));
+		cmr_config.s.enable = 0;
+		cvmx_write_csr(CVMX_BGXX_CMRX_CONFIG(i, interface),
+			       cmr_config.u64);
+	}
+
+	bgx_cmr_rx_lmacs.u64 = 0;
+	bgx_cmr_rx_lmacs.s.lmacs = num_ports;
+	cvmx_write_csr(CVMX_BGXX_CMR_RX_LMACS(interface),
+		       bgx_cmr_rx_lmacs.u64);
+
+	bgx_cmr_tx_lmacs.u64 = 0;
+	bgx_cmr_tx_lmacs.s.lmacs = num_ports;
+	cvmx_write_csr(CVMX_BGXX_CMR_TX_LMACS(interface),
+		       bgx_cmr_tx_lmacs.u64);
+
+	/* Check if interface 0 or 1 must be routed to the mix */
+	if ((interface == 0 && MUX_78XX_IFACE0) ||
+	    (interface == 1 && MUX_78XX_IFACE1))
+		val = 1;
+	else
+		val = 0;
+
+	bgx_cmr_global_config.u64 = 0;
+	bgx_cmr_global_config.s.pmux_sds_sel = val;
+	cvmx_write_csr(CVMX_BGXX_CMR_GLOBAL_CONFIG(interface),
+		       bgx_cmr_global_config.u64);
+
+	if (mode == CVMX_HELPER_INTERFACE_MODE_XAUI)
+		bgx_xaui_init(interface, num_ports);
+	else if (mode == CVMX_HELPER_INTERFACE_MODE_SGMII)
+		bgx_sgmii_init(interface, num_ports);
+	else
+		return -1;
+
+	return 0;
+}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-debug.c b/arch/mips/cavium-octeon/executive/cvmx-debug.c
index 6974935..c17d025 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-debug.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-debug.c
@@ -370,7 +370,7 @@ void cvmx_debug_init(void)
 	// Put a barrier until all cores have got to this point.
 	cvmx_coremask_barrier_sync(pcm);
 
-	if (cvmx_coremask_is_first_core(pcm))
+	if (cvmx_is_init_core())
 #endif
 	{
 		cvmx_debug_printf("cvmx_debug_init core: %d\n", core);
@@ -405,7 +405,7 @@ void cvmx_debug_init(void)
 
 	/*  Install the break handler after might tripper the debugger exception. */
 #ifndef CVMX_BUILD_FOR_LINUX_KERNEL
-	if (cvmx_coremask_is_first_core(pcm))
+	if (cvmx_is_init_core())
 #endif
 	{
 		if (comm->install_break_handler)
@@ -1291,10 +1291,14 @@ static int cvmx_debug_event_loop(cvmx_debug_register_t * debug_reg, volatile cvm
 		/* If we did not get a command and the communication changed return,
 		   we are changing the communications. */
 		if (command == COMMAND_NOP && cvmx_debug_globals->comm_changed) {
-			/* FIXME, this should a sync not based on cvmx_coremask_barrier_sync.  */
 #ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+			cvmx_coremask_t cm;
+
+			/* FIXME: Debugger is limited at 64 cores */
+			cvmx_coremask_set64(&cm, state.handler_cores);
+			/* FIXME, this should a sync not based on cvmx_coremask_barrier_sync.  */
 			/* Sync up.  */
-			cvmx_coremask_barrier_sync(state.handler_cores);
+			cvmx_coremask_barrier_sync(&cm);
 #endif
 			return 1;
 		}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-error-trees.c b/arch/mips/cavium-octeon/executive/cvmx-error-trees.c
index e5ea2cf..cb7e8f3 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-error-trees.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-error-trees.c
@@ -3698,8 +3698,6 @@ static struct cvmx_error_muxchild error_tree_cn70xx =
 						NULL /*cvmx_error_childbit*/
 					},
 					{0}}},
-				{1, 51 /* sum2 */, (struct cvmx_error_muxchild[]){
-					{0}}},
 				{1, 57 /* pcm */, (struct cvmx_error_muxchild[]){
 					{CVMX_ADD_IO_SEG(0x0001070000010028ull) + ((0) & 3) * 16384 /* CVMX_PCMX_INT_SUM(0) */, CVMX_ADD_IO_SEG(0x0001070000010020ull) + ((0) & 3) * 16384 /* CVMX_PCMX_INT_ENA(0) */, (struct cvmx_error_regbit[]){
 							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 0, 0, "PCMX_INT_SUM(0)[FSYNCMISSED]"},
@@ -3736,7 +3734,159 @@ static struct cvmx_error_muxchild error_tree_cn70xx =
 					{0}}},
 				{0}}
 			},
-			{CVMX_ADD_IO_SEG(0x0001070000008000ull) + ((0) & 3) * 8 /* CVMX_CIU_SUM1_PPX_IP2(0) */, 0, NULL /* cvmx_error_regbit */, (struct cvmx_error_childbit[]){
+			{CVMX_ADD_IO_SEG(0x0001070000000108ull) /* CVMX_CIU_INT_SUM1 */, 0, NULL /* cvmx_error_regbit */, (struct cvmx_error_childbit[]){
+				{1, 24 /* l2c */, (struct cvmx_error_muxchild[]){
+					{CVMX_ADD_IO_SEG(0x000107000000E000ull) /* CVMX_CIU_CIB_L2C_RAWX(0) */, CVMX_ADD_IO_SEG(0x000107000000E100ull) /* CVMX_CIU_CIB_L2C_ENX(0) */, (struct cvmx_error_regbit[]){
+							{1, 1, CVMX_ERROR_GROUP_L2C, 0, 0, "CIU_CIB_L2C_RAWX(0)[TADX_INT_L2DSBE]"},
+							{1, 1, CVMX_ERROR_GROUP_L2C, 1, 0, "CIU_CIB_L2C_RAWX(0)[TADX_INT_L2DDBE]"},
+							{1, 1, CVMX_ERROR_GROUP_L2C, 2, 0, "CIU_CIB_L2C_RAWX(0)[TADX_INT_SBFSBE]"},
+							{1, 1, CVMX_ERROR_GROUP_L2C, 3, 0, "CIU_CIB_L2C_RAWX(0)[TADX_INT_SBFDBE]"},
+							{1, 1, CVMX_ERROR_GROUP_L2C, 4, 0, "CIU_CIB_L2C_RAWX(0)[TADX_INT_FBFSBE]"},
+							{1, 1, CVMX_ERROR_GROUP_L2C, 5, 0, "CIU_CIB_L2C_RAWX(0)[TADX_INT_FBFDBE]"},
+							{1, 1, CVMX_ERROR_GROUP_L2C, 6, 0, "CIU_CIB_L2C_RAWX(0)[TADX_INT_TAGSBE]"},
+							{1, 1, CVMX_ERROR_GROUP_L2C, 7, 0, "CIU_CIB_L2C_RAWX(0)[TADX_INT_TAGDBE]"},
+							{1, 1, CVMX_ERROR_GROUP_L2C, 8, 0, "CIU_CIB_L2C_RAWX(0)[TADX_INT_NOWAY]"},
+							{1, 1, CVMX_ERROR_GROUP_L2C, 9, 0, "CIU_CIB_L2C_RAWX(0)[TADX_INT_HOLEWR]"},
+							{1, 1, CVMX_ERROR_GROUP_L2C, 10, 0, "CIU_CIB_L2C_RAWX(0)[TADX_INT_HOLERD]"},
+							{1, 1, CVMX_ERROR_GROUP_L2C, 11, 0, "CIU_CIB_L2C_RAWX(0)[TADX_INT_BIGWR]"},
+							{1, 1, CVMX_ERROR_GROUP_L2C, 12, 0, "CIU_CIB_L2C_RAWX(0)[TADX_INT_BIGRD]"},
+							{1, 1, CVMX_ERROR_GROUP_L2C, 13, 0, "CIU_CIB_L2C_RAWX(0)[TADX_INT_WRDISLMC]"},
+							{1, 1, CVMX_ERROR_GROUP_L2C, 14, 0, "CIU_CIB_L2C_RAWX(0)[TADX_INT_RDDISLMC]"},
+							{1, 1, CVMX_ERROR_GROUP_L2C, 15, 0, "CIU_CIB_L2C_RAWX(0)[TADX_INT_RTGSBE]"},
+							{1, 1, CVMX_ERROR_GROUP_L2C, 16, 0, "CIU_CIB_L2C_RAWX(0)[TADX_INT_RTGDBE]"},
+							{1, 1, CVMX_ERROR_GROUP_L2C, 17, 0, "CIU_CIB_L2C_RAWX(0)[MCIX_INT_VBFSBE]"},
+							{1, 1, CVMX_ERROR_GROUP_L2C, 18, 0, "CIU_CIB_L2C_RAWX(0)[MCIX_INT_VBFDBE]"},
+							{1, 1, CVMX_ERROR_GROUP_L2C, 19, 0, "CIU_CIB_L2C_RAWX(0)[CBCX_INT_RSDSBE]"},
+							{1, 1, CVMX_ERROR_GROUP_L2C, 20, 0, "CIU_CIB_L2C_RAWX(0)[CBCX_INT_RSDDBE]"},
+							{1, 1, CVMX_ERROR_GROUP_L2C, 21, 0, "CIU_CIB_L2C_RAWX(0)[CBCX_INT_IOCCMDSBE]"},
+							{1, 1, CVMX_ERROR_GROUP_L2C, 22, 0, "CIU_CIB_L2C_RAWX(0)[CBCX_INT_IOCCMDDBE]"},
+							{0}},
+						NULL /*cvmx_error_childbit*/
+					},
+					{0}}},
+				{1, 25 /* ipd */, (struct cvmx_error_muxchild[]){
+					{CVMX_ADD_IO_SEG(0x00014F0000000168ull) /* CVMX_IPD_INT_SUM */, CVMX_ADD_IO_SEG(0x00014F0000000160ull) /* CVMX_IPD_INT_ENB */, (struct cvmx_error_regbit[]){
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 0, 0, "IPD_INT_SUM[PRC_PAR0]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 1, 0, "IPD_INT_SUM[PRC_PAR1]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 2, 0, "IPD_INT_SUM[PRC_PAR2]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 3, 0, "IPD_INT_SUM[PRC_PAR3]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 4, 0, "IPD_INT_SUM[BP_SUB]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 5, 0, "IPD_INT_SUM[DC_OVR]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 6, 0, "IPD_INT_SUM[CC_OVR]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 7, 0, "IPD_INT_SUM[C_COLL]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 8, 0, "IPD_INT_SUM[D_COLL]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 9, 0, "IPD_INT_SUM[BC_OVR]"},
+							{0}},
+						NULL /*cvmx_error_childbit*/
+					},
+					{0}}},
+				{1, 23 /* pow */, (struct cvmx_error_muxchild[]){
+					{CVMX_ADD_IO_SEG(0x0001670000000218ull) /* CVMX_POW_ECC_ERR */, CVMX_ADD_IO_SEG(0x0001670000000218ull) /* CVMX_POW_ECC_ERR */, (struct cvmx_error_regbit[]){
+							{1, 0, CVMX_ERROR_GROUP_INTERNAL, 0, 0, "POW_ECC_ERR[SBE]"},
+							{1, 0, CVMX_ERROR_GROUP_INTERNAL, 1, 0, "POW_ECC_ERR[DBE]"},
+							{1, 0, CVMX_ERROR_GROUP_INTERNAL, 12, 0, "POW_ECC_ERR[RPE]"},
+							{1, 0, CVMX_ERROR_GROUP_INTERNAL, 16, 0, "POW_ECC_ERR[IOP]"},
+							{0}},
+						NULL /*cvmx_error_childbit*/
+					},
+					{0}}},
+				{1, 30 /* rad */, (struct cvmx_error_muxchild[]){
+					{CVMX_ADD_IO_SEG(0x0001180070000088ull) /* CVMX_RAD_REG_ERROR */, CVMX_ADD_IO_SEG(0x0001180070000090ull) /* CVMX_RAD_REG_INT_MASK */, (struct cvmx_error_regbit[]){
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 0, 0, "RAD_REG_ERROR[DOORBELL]"},
+							{0}},
+						NULL /*cvmx_error_childbit*/
+					},
+					{0}}},
+				{1, 26 /* pip */, (struct cvmx_error_muxchild[]){
+					{CVMX_ADD_IO_SEG(0x00011800A0000008ull) /* CVMX_PIP_INT_REG */, CVMX_ADD_IO_SEG(0x00011800A0000010ull) /* CVMX_PIP_INT_EN */, (struct cvmx_error_regbit[]){
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 3, 0, "PIP_INT_REG[PRTNXA]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 4, 0, "PIP_INT_REG[BADTAG]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 5, 0, "PIP_INT_REG[SKPRUNT]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 6, 0, "PIP_INT_REG[TODOOVR]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 7, 0, "PIP_INT_REG[FEPERR]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 8, 0, "PIP_INT_REG[BEPERR]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 12, 0, "PIP_INT_REG[PUNYERR]"},
+							{0}},
+						NULL /*cvmx_error_childbit*/
+					},
+					{0}}},
+				{1, 27 /* pko */, (struct cvmx_error_muxchild[]){
+					{CVMX_ADD_IO_SEG(0x0001180050000088ull) /* CVMX_PKO_REG_ERROR */, CVMX_ADD_IO_SEG(0x0001180050000090ull) /* CVMX_PKO_REG_INT_MASK */, (struct cvmx_error_regbit[]){
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 0, 0, "PKO_REG_ERROR[PARITY]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 1, 0, "PKO_REG_ERROR[DOORBELL]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 2, 0, "PKO_REG_ERROR[CURRZERO]"},
+							{0}},
+						NULL /*cvmx_error_childbit*/
+					},
+					{0}}},
+				{1, 50 /* pem2 */, (struct cvmx_error_muxchild[]){
+					{CVMX_ADD_IO_SEG(0x00011800C0000428ull) + ((2) & 3) * 0x1000000ull /* CVMX_PEMX_INT_SUM(2) */, CVMX_ADD_IO_SEG(0x00011800C0000430ull) + ((2) & 3) * 0x1000000ull /* CVMX_PEMX_INT_ENB(2) */, (struct cvmx_error_regbit[]){
+							{1, 1, CVMX_ERROR_GROUP_PCI, 1, 2, "PEMX_INT_SUM(2)[SE]"},
+							{1, 1, CVMX_ERROR_GROUP_PCI, 4, 2, "PEMX_INT_SUM(2)[UP_B1]"},
+							{1, 1, CVMX_ERROR_GROUP_PCI, 5, 2, "PEMX_INT_SUM(2)[UP_B2]"},
+							{1, 1, CVMX_ERROR_GROUP_PCI, 6, 2, "PEMX_INT_SUM(2)[UP_BX]"},
+							{1, 1, CVMX_ERROR_GROUP_PCI, 7, 2, "PEMX_INT_SUM(2)[UN_B1]"},
+							{1, 1, CVMX_ERROR_GROUP_PCI, 8, 2, "PEMX_INT_SUM(2)[UN_B2]"},
+							{1, 1, CVMX_ERROR_GROUP_PCI, 9, 2, "PEMX_INT_SUM(2)[UN_BX]"},
+							{1, 1, CVMX_ERROR_GROUP_PCI, 11, 2, "PEMX_INT_SUM(2)[RDLK]"},
+							{1, 1, CVMX_ERROR_GROUP_PCI, 12, 2, "PEMX_INT_SUM(2)[CRS_ER]"},
+							{1, 1, CVMX_ERROR_GROUP_PCI, 13, 2, "PEMX_INT_SUM(2)[CRS_DR]"},
+							{0}},
+						(struct cvmx_error_childbit[]){
+						{1, 10 /* exc */, (struct cvmx_error_muxchild[]){
+							{CVMX_ADD_IO_SEG(0x00011800C00000D0ull) + ((2) & 3) * 0x1000000ull /* CVMX_PEMX_DBG_INFO(2) */, CVMX_ADD_IO_SEG(0x00011800C00000A0ull) + ((2) & 3) * 0x1000000ull /* CVMX_PEMX_DBG_INFO_EN(2) */, (struct cvmx_error_regbit[]){
+									{1, 1, CVMX_ERROR_GROUP_PCI, 0, 2, "PEMX_DBG_INFO(2)[SPOISON]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 2, 2, "PEMX_DBG_INFO(2)[RTLPLLE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 3, 2, "PEMX_DBG_INFO(2)[RECRCE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 4, 2, "PEMX_DBG_INFO(2)[RPOISON]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 5, 2, "PEMX_DBG_INFO(2)[RCEMRC]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 6, 2, "PEMX_DBG_INFO(2)[RNFEMRC]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 7, 2, "PEMX_DBG_INFO(2)[RFEMRC]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 8, 2, "PEMX_DBG_INFO(2)[RPMERC]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 9, 2, "PEMX_DBG_INFO(2)[RPTAMRC]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 10, 2, "PEMX_DBG_INFO(2)[RUMEP]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 11, 2, "PEMX_DBG_INFO(2)[RVDM]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 12, 2, "PEMX_DBG_INFO(2)[ACTO]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 13, 2, "PEMX_DBG_INFO(2)[RTE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 14, 2, "PEMX_DBG_INFO(2)[MRE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 15, 2, "PEMX_DBG_INFO(2)[RDWDLE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 16, 2, "PEMX_DBG_INFO(2)[RTWDLE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 17, 2, "PEMX_DBG_INFO(2)[DPEOOSD]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 18, 2, "PEMX_DBG_INFO(2)[FCPVWT]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 19, 2, "PEMX_DBG_INFO(2)[RPE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 20, 2, "PEMX_DBG_INFO(2)[FCUV]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 21, 2, "PEMX_DBG_INFO(2)[RQO]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 22, 2, "PEMX_DBG_INFO(2)[RAUC]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 23, 2, "PEMX_DBG_INFO(2)[RACUR]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 24, 2, "PEMX_DBG_INFO(2)[RACCA]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 25, 2, "PEMX_DBG_INFO(2)[CAAR]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 26, 2, "PEMX_DBG_INFO(2)[RARWDNS]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 27, 2, "PEMX_DBG_INFO(2)[RAMTLP]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 28, 2, "PEMX_DBG_INFO(2)[RACPP]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 29, 2, "PEMX_DBG_INFO(2)[RAWWPP]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 30, 2, "PEMX_DBG_INFO(2)[ECRC_E]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 31, 2, "PEMX_DBG_INFO(2)[RTRY_PE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 32, 2, "PEMX_DBG_INFO(2)[HDRQ_PE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 33, 2, "PEMX_DBG_INFO(2)[DATQ_PE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 34, 2, "PEMX_DBG_INFO(2)[P_D_SBE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 35, 2, "PEMX_DBG_INFO(2)[P_D_DBE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 36, 2, "PEMX_DBG_INFO(2)[P_C_SBE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 37, 2, "PEMX_DBG_INFO(2)[P_C_DBE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 38, 2, "PEMX_DBG_INFO(2)[N_D_SBE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 39, 2, "PEMX_DBG_INFO(2)[N_D_DBE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 40, 2, "PEMX_DBG_INFO(2)[N_C_SBE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 41, 2, "PEMX_DBG_INFO(2)[N_C_DBE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 42, 2, "PEMX_DBG_INFO(2)[C_D_SBE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 43, 2, "PEMX_DBG_INFO(2)[C_D_DBE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 44, 2, "PEMX_DBG_INFO(2)[C_C_SBE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 45, 2, "PEMX_DBG_INFO(2)[C_C_DBE]"},
+									{0}},
+								NULL /*cvmx_error_childbit*/
+							},
+							{0}}},
+						{0}}
+					},
+					{0}}},
 				{1, 48 /* pem0 */, (struct cvmx_error_muxchild[]){
 					{CVMX_ADD_IO_SEG(0x00011800C0000428ull) + ((0) & 3) * 0x1000000ull /* CVMX_PEMX_INT_SUM(0) */, CVMX_ADD_IO_SEG(0x00011800C0000430ull) + ((0) & 3) * 0x1000000ull /* CVMX_PEMX_INT_ENB(0) */, (struct cvmx_error_regbit[]){
 							{1, 1, CVMX_ERROR_GROUP_PCI, 1, 0, "PEMX_INT_SUM(0)[SE]"},
@@ -3805,27 +3955,72 @@ static struct cvmx_error_muxchild error_tree_cn70xx =
 						{0}}
 					},
 					{0}}},
-				{1, 46 /* agl */, (struct cvmx_error_muxchild[]){
-					{CVMX_ADD_IO_SEG(0x00011800E0000518ull) /* CVMX_AGL_GMX_BAD_REG */, 0, (struct cvmx_error_regbit[]){
-							{1, 1, CVMX_ERROR_GROUP_MGMT_PORT, 32, 0, "AGL_GMX_BAD_REG[OVRFLW]"},
-							{1, 1, CVMX_ERROR_GROUP_MGMT_PORT, 33, 0, "AGL_GMX_BAD_REG[TXPOP]"},
-							{1, 1, CVMX_ERROR_GROUP_MGMT_PORT, 34, 0, "AGL_GMX_BAD_REG[TXPSH]"},
-							{1, 1, CVMX_ERROR_GROUP_MGMT_PORT, 2, 0, "AGL_GMX_BAD_REG[OUT_OVR]"},
-							{1, 1, CVMX_ERROR_GROUP_MGMT_PORT, 22, 0, "AGL_GMX_BAD_REG[LOSTSTAT]"},
-							{0}},
-						NULL /*cvmx_error_childbit*/
-					},
-					{CVMX_ADD_IO_SEG(0x00011800E0000000ull) + ((0) & 0) * 2048 /* CVMX_AGL_GMX_RXX_INT_REG(0) */, CVMX_ADD_IO_SEG(0x00011800E0000008ull) + ((0) & 0) * 2048 /* CVMX_AGL_GMX_RXX_INT_EN(0) */, (struct cvmx_error_regbit[]){
-							{1, 1, CVMX_ERROR_GROUP_MGMT_PORT, 8, 0, "AGL_GMX_RXX_INT_REG(0)[SKPERR]"},
-							{1, 1, CVMX_ERROR_GROUP_MGMT_PORT, 10, 0, "AGL_GMX_RXX_INT_REG(0)[OVRERR]"},
-							{0}},
-						NULL /*cvmx_error_childbit*/
-					},
-					{CVMX_ADD_IO_SEG(0x00011800E0000500ull) /* CVMX_AGL_GMX_TX_INT_REG */, CVMX_ADD_IO_SEG(0x00011800E0000508ull) /* CVMX_AGL_GMX_TX_INT_EN */, (struct cvmx_error_regbit[]){
-							{1, 1, CVMX_ERROR_GROUP_MGMT_PORT, 0, 0, "AGL_GMX_TX_INT_REG[PKO_NXA]"},
-							{1, 1, CVMX_ERROR_GROUP_MGMT_PORT, 2, 0, "AGL_GMX_TX_INT_REG[UNDFLW]"},
+				{1, 49 /* pem1 */, (struct cvmx_error_muxchild[]){
+					{CVMX_ADD_IO_SEG(0x00011800C0000428ull) + ((1) & 3) * 0x1000000ull /* CVMX_PEMX_INT_SUM(1) */, CVMX_ADD_IO_SEG(0x00011800C0000430ull) + ((1) & 3) * 0x1000000ull /* CVMX_PEMX_INT_ENB(1) */, (struct cvmx_error_regbit[]){
+							{1, 1, CVMX_ERROR_GROUP_PCI, 1, 1, "PEMX_INT_SUM(1)[SE]"},
+							{1, 1, CVMX_ERROR_GROUP_PCI, 4, 1, "PEMX_INT_SUM(1)[UP_B1]"},
+							{1, 1, CVMX_ERROR_GROUP_PCI, 5, 1, "PEMX_INT_SUM(1)[UP_B2]"},
+							{1, 1, CVMX_ERROR_GROUP_PCI, 6, 1, "PEMX_INT_SUM(1)[UP_BX]"},
+							{1, 1, CVMX_ERROR_GROUP_PCI, 7, 1, "PEMX_INT_SUM(1)[UN_B1]"},
+							{1, 1, CVMX_ERROR_GROUP_PCI, 8, 1, "PEMX_INT_SUM(1)[UN_B2]"},
+							{1, 1, CVMX_ERROR_GROUP_PCI, 9, 1, "PEMX_INT_SUM(1)[UN_BX]"},
+							{1, 1, CVMX_ERROR_GROUP_PCI, 11, 1, "PEMX_INT_SUM(1)[RDLK]"},
+							{1, 1, CVMX_ERROR_GROUP_PCI, 12, 1, "PEMX_INT_SUM(1)[CRS_ER]"},
+							{1, 1, CVMX_ERROR_GROUP_PCI, 13, 1, "PEMX_INT_SUM(1)[CRS_DR]"},
 							{0}},
-						NULL /*cvmx_error_childbit*/
+						(struct cvmx_error_childbit[]){
+						{1, 10 /* exc */, (struct cvmx_error_muxchild[]){
+							{CVMX_ADD_IO_SEG(0x00011800C00000D0ull) + ((1) & 3) * 0x1000000ull /* CVMX_PEMX_DBG_INFO(1) */, CVMX_ADD_IO_SEG(0x00011800C00000A0ull) + ((1) & 3) * 0x1000000ull /* CVMX_PEMX_DBG_INFO_EN(1) */, (struct cvmx_error_regbit[]){
+									{1, 1, CVMX_ERROR_GROUP_PCI, 0, 1, "PEMX_DBG_INFO(1)[SPOISON]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 2, 1, "PEMX_DBG_INFO(1)[RTLPLLE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 3, 1, "PEMX_DBG_INFO(1)[RECRCE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 4, 1, "PEMX_DBG_INFO(1)[RPOISON]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 5, 1, "PEMX_DBG_INFO(1)[RCEMRC]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 6, 1, "PEMX_DBG_INFO(1)[RNFEMRC]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 7, 1, "PEMX_DBG_INFO(1)[RFEMRC]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 8, 1, "PEMX_DBG_INFO(1)[RPMERC]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 9, 1, "PEMX_DBG_INFO(1)[RPTAMRC]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 10, 1, "PEMX_DBG_INFO(1)[RUMEP]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 11, 1, "PEMX_DBG_INFO(1)[RVDM]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 12, 1, "PEMX_DBG_INFO(1)[ACTO]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 13, 1, "PEMX_DBG_INFO(1)[RTE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 14, 1, "PEMX_DBG_INFO(1)[MRE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 15, 1, "PEMX_DBG_INFO(1)[RDWDLE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 16, 1, "PEMX_DBG_INFO(1)[RTWDLE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 17, 1, "PEMX_DBG_INFO(1)[DPEOOSD]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 18, 1, "PEMX_DBG_INFO(1)[FCPVWT]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 19, 1, "PEMX_DBG_INFO(1)[RPE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 20, 1, "PEMX_DBG_INFO(1)[FCUV]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 21, 1, "PEMX_DBG_INFO(1)[RQO]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 22, 1, "PEMX_DBG_INFO(1)[RAUC]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 23, 1, "PEMX_DBG_INFO(1)[RACUR]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 24, 1, "PEMX_DBG_INFO(1)[RACCA]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 25, 1, "PEMX_DBG_INFO(1)[CAAR]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 26, 1, "PEMX_DBG_INFO(1)[RARWDNS]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 27, 1, "PEMX_DBG_INFO(1)[RAMTLP]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 28, 1, "PEMX_DBG_INFO(1)[RACPP]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 29, 1, "PEMX_DBG_INFO(1)[RAWWPP]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 30, 1, "PEMX_DBG_INFO(1)[ECRC_E]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 31, 1, "PEMX_DBG_INFO(1)[RTRY_PE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 32, 1, "PEMX_DBG_INFO(1)[HDRQ_PE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 33, 1, "PEMX_DBG_INFO(1)[DATQ_PE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 34, 1, "PEMX_DBG_INFO(1)[P_D_SBE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 35, 1, "PEMX_DBG_INFO(1)[P_D_DBE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 36, 1, "PEMX_DBG_INFO(1)[P_C_SBE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 37, 1, "PEMX_DBG_INFO(1)[P_C_DBE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 38, 1, "PEMX_DBG_INFO(1)[N_D_SBE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 39, 1, "PEMX_DBG_INFO(1)[N_D_DBE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 40, 1, "PEMX_DBG_INFO(1)[N_C_SBE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 41, 1, "PEMX_DBG_INFO(1)[N_C_DBE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 42, 1, "PEMX_DBG_INFO(1)[C_D_SBE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 43, 1, "PEMX_DBG_INFO(1)[C_D_DBE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 44, 1, "PEMX_DBG_INFO(1)[C_C_SBE]"},
+									{1, 1, CVMX_ERROR_GROUP_PCI, 45, 1, "PEMX_DBG_INFO(1)[C_C_DBE]"},
+									{0}},
+								NULL /*cvmx_error_childbit*/
+							},
+							{0}}},
+						{0}}
 					},
 					{0}}},
 				{1, 22 /* fpa */, (struct cvmx_error_muxchild[]){
@@ -3887,27 +4082,93 @@ static struct cvmx_error_muxchild error_tree_cn70xx =
 						NULL /*cvmx_error_childbit*/
 					},
 					{0}}},
-				{1, 25 /* ipd */, (struct cvmx_error_muxchild[]){
-					{CVMX_ADD_IO_SEG(0x00014F0000000168ull) /* CVMX_IPD_INT_SUM */, CVMX_ADD_IO_SEG(0x00014F0000000160ull) /* CVMX_IPD_INT_ENB */, (struct cvmx_error_regbit[]){
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 0, 0, "IPD_INT_SUM[PRC_PAR0]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 1, 0, "IPD_INT_SUM[PRC_PAR1]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 2, 0, "IPD_INT_SUM[PRC_PAR2]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 3, 0, "IPD_INT_SUM[PRC_PAR3]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 4, 0, "IPD_INT_SUM[BP_SUB]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 5, 0, "IPD_INT_SUM[DC_OVR]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 6, 0, "IPD_INT_SUM[CC_OVR]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 7, 0, "IPD_INT_SUM[C_COLL]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 8, 0, "IPD_INT_SUM[D_COLL]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 9, 0, "IPD_INT_SUM[BC_OVR]"},
+				{1, 32 /* dfa */, (struct cvmx_error_muxchild[]){
+					{CVMX_ADD_IO_SEG(0x0001180037000028ull) /* CVMX_DFA_ERROR */, CVMX_ADD_IO_SEG(0x0001180037000030ull) /* CVMX_DFA_INTMSK */, (struct cvmx_error_regbit[]){
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 0, 0, "DFA_ERROR[DBLOVF]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 1, 0, "DFA_ERROR[DC0PERR]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 13, 0, "DFA_ERROR[DLC0_OVFERR]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 17, 0, "DFA_ERROR[DFANXM]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 18, 0, "DFA_ERROR[REPLERR]"},
+							{0}},
+						NULL /*cvmx_error_childbit*/
+					},
+					{0}}},
+				{1, 19 /* nand */, (struct cvmx_error_muxchild[]){
+					{CVMX_ADD_IO_SEG(0x0001070001000020ull) /* CVMX_NDF_INT */, CVMX_ADD_IO_SEG(0x0001070001000028ull) /* CVMX_NDF_INT_EN */, (struct cvmx_error_regbit[]){
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 2, 0, "NDF_INT[WDOG]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 3, 0, "NDF_INT[SM_BAD]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 4, 0, "NDF_INT[ECC_1BIT]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 5, 0, "NDF_INT[ECC_MULT]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 6, 0, "NDF_INT[OVRF]"},
+							{0}},
+						NULL /*cvmx_error_childbit*/
+					},
+					{0}}},
+				{1, 29 /* tim */, (struct cvmx_error_muxchild[]){
+					{CVMX_ADD_IO_SEG(0x0001180058000088ull) /* CVMX_TIM_REG_ERROR */, CVMX_ADD_IO_SEG(0x0001180058000090ull) /* CVMX_TIM_REG_INT_MASK */, (struct cvmx_error_regbit[]){
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 0, 0, "TIM_REG_ERROR[MASK]"},
 							{0}},
 						NULL /*cvmx_error_childbit*/
 					},
 					{0}}},
 				{1, 52 /* lmc0 */, (struct cvmx_error_muxchild[]){
-					{CVMX_ADD_IO_SEG(0x00011800880001F0ull) + ((0) & 0) * 0x1000000ull /* CVMX_LMCX_INT(0) */, CVMX_ADD_IO_SEG(0x00011800880001E8ull) + ((0) & 0) * 0x1000000ull /* CVMX_LMCX_INT_EN(0) */, (struct cvmx_error_regbit[]){
-							{1, 1, CVMX_ERROR_GROUP_LMC, 1, 0, "LMCX_INT(0)[SEC_ERR]"},
-							{1, 1, CVMX_ERROR_GROUP_LMC, 0, 0, "LMCX_INT(0)[NXM_WR_ERR]"},
-							{1, 1, CVMX_ERROR_GROUP_LMC, 5, 0, "LMCX_INT(0)[DED_ERR]"},
+					{CVMX_ADD_IO_SEG(0x000107000000E200ull) /* CVMX_CIU_CIB_LMCX_RAWX(0,0) */, CVMX_ADD_IO_SEG(0x000107000000E300ull) /* CVMX_CIU_CIB_LMCX_ENX(0,0) */, (struct cvmx_error_regbit[]){
+							{1, 1, CVMX_ERROR_GROUP_LMC, 1, 0, "CIU_CIB_LMCX_RAWX(0,0)[INT_SEC_ERRX]"},
+							{1, 1, CVMX_ERROR_GROUP_LMC, 5, 0, "CIU_CIB_LMCX_RAWX(0,0)[INT_DED_ERRX]"},
+							{1, 1, CVMX_ERROR_GROUP_LMC, 0, 0, "CIU_CIB_LMCX_RAWX(0,0)[INT_NXM_WR_ERR]"},
+							{0}},
+						NULL /*cvmx_error_childbit*/
+					},
+					{0}}},
+				{1, 31 /* key */, (struct cvmx_error_muxchild[]){
+					{CVMX_ADD_IO_SEG(0x0001180020000000ull) /* CVMX_KEY_INT_SUM */, CVMX_ADD_IO_SEG(0x0001180020000008ull) /* CVMX_KEY_INT_ENB */, (struct cvmx_error_regbit[]){
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 0, 0, "KEY_INT_SUM[KEY_SBE]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 1, 0, "KEY_INT_SUM[KEY_DBE]"},
+							{0}},
+						NULL /*cvmx_error_childbit*/
+					},
+					{0}}},
+				{1, 63 /* rst */, (struct cvmx_error_muxchild[]){
+					{CVMX_ADD_IO_SEG(0x000107000000E400ull) /* CVMX_CIU_CIB_RST_RAWX(0) */, CVMX_ADD_IO_SEG(0x000107000000E500ull) /* CVMX_CIU_CIB_RST_ENX(0) */, (struct cvmx_error_regbit[]){
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 0, 0, "CIU_CIB_RST_RAWX(0)[INT_LINKX]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 3, 0, "CIU_CIB_RST_RAWX(0)[INT_PERSTX]"},
+							{0}},
+						NULL /*cvmx_error_childbit*/
+					},
+					{0}}},
+				{1, 21 /* iob */, (struct cvmx_error_muxchild[]){
+					{CVMX_ADD_IO_SEG(0x00011800F0000058ull) /* CVMX_IOB_INT_SUM */, CVMX_ADD_IO_SEG(0x00011800F0000060ull) /* CVMX_IOB_INT_ENB */, (struct cvmx_error_regbit[]){
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 0, 0, "IOB_INT_SUM[NP_SOP]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 1, 0, "IOB_INT_SUM[NP_EOP]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 2, 0, "IOB_INT_SUM[P_SOP]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 3, 0, "IOB_INT_SUM[P_EOP]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 4, 0, "IOB_INT_SUM[NP_DAT]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 5, 0, "IOB_INT_SUM[P_DAT]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 6, 0, "IOB_INT_SUM[INB_MAT]"},
+							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 7, 0, "IOB_INT_SUM[OUTB_MAT]"},
+							{0}},
+						NULL /*cvmx_error_childbit*/
+					},
+					{0}}},
+				{1, 46 /* agl */, (struct cvmx_error_muxchild[]){
+					{CVMX_ADD_IO_SEG(0x00011800E0000518ull) /* CVMX_AGL_GMX_BAD_REG */, 0, (struct cvmx_error_regbit[]){
+							{1, 1, CVMX_ERROR_GROUP_MGMT_PORT, 32, 0, "AGL_GMX_BAD_REG[OVRFLW]"},
+							{1, 1, CVMX_ERROR_GROUP_MGMT_PORT, 33, 0, "AGL_GMX_BAD_REG[TXPOP]"},
+							{1, 1, CVMX_ERROR_GROUP_MGMT_PORT, 34, 0, "AGL_GMX_BAD_REG[TXPSH]"},
+							{1, 1, CVMX_ERROR_GROUP_MGMT_PORT, 2, 0, "AGL_GMX_BAD_REG[OUT_OVR]"},
+							{1, 1, CVMX_ERROR_GROUP_MGMT_PORT, 22, 0, "AGL_GMX_BAD_REG[LOSTSTAT]"},
+							{0}},
+						NULL /*cvmx_error_childbit*/
+					},
+					{CVMX_ADD_IO_SEG(0x00011800E0000000ull) + ((0) & 0) * 2048 /* CVMX_AGL_GMX_RXX_INT_REG(0) */, CVMX_ADD_IO_SEG(0x00011800E0000008ull) + ((0) & 0) * 2048 /* CVMX_AGL_GMX_RXX_INT_EN(0) */, (struct cvmx_error_regbit[]){
+							{1, 1, CVMX_ERROR_GROUP_MGMT_PORT, 8, 0, "AGL_GMX_RXX_INT_REG(0)[SKPERR]"},
+							{1, 1, CVMX_ERROR_GROUP_MGMT_PORT, 10, 0, "AGL_GMX_RXX_INT_REG(0)[OVRERR]"},
+							{0}},
+						NULL /*cvmx_error_childbit*/
+					},
+					{CVMX_ADD_IO_SEG(0x00011800E0000500ull) /* CVMX_AGL_GMX_TX_INT_REG */, CVMX_ADD_IO_SEG(0x00011800E0000508ull) /* CVMX_AGL_GMX_TX_INT_EN */, (struct cvmx_error_regbit[]){
+							{1, 1, CVMX_ERROR_GROUP_MGMT_PORT, 0, 0, "AGL_GMX_TX_INT_REG[PKO_NXA]"},
+							{1, 1, CVMX_ERROR_GROUP_MGMT_PORT, 2, 0, "AGL_GMX_TX_INT_REG[UNDFLW]"},
 							{0}},
 						NULL /*cvmx_error_childbit*/
 					},
@@ -4208,101 +4469,6 @@ static struct cvmx_error_muxchild error_tree_cn70xx =
 						NULL /*cvmx_error_childbit*/
 					},
 					{0}}},
-				{1, 19 /* nand */, (struct cvmx_error_muxchild[]){
-					{CVMX_ADD_IO_SEG(0x0001070001000020ull) /* CVMX_NDF_INT */, CVMX_ADD_IO_SEG(0x0001070001000028ull) /* CVMX_NDF_INT_EN */, (struct cvmx_error_regbit[]){
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 2, 0, "NDF_INT[WDOG]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 3, 0, "NDF_INT[SM_BAD]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 4, 0, "NDF_INT[ECC_1BIT]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 5, 0, "NDF_INT[ECC_MULT]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 6, 0, "NDF_INT[OVRF]"},
-							{0}},
-						NULL /*cvmx_error_childbit*/
-					},
-					{0}}},
-				{1, 27 /* pko */, (struct cvmx_error_muxchild[]){
-					{CVMX_ADD_IO_SEG(0x0001180050000088ull) /* CVMX_PKO_REG_ERROR */, CVMX_ADD_IO_SEG(0x0001180050000090ull) /* CVMX_PKO_REG_INT_MASK */, (struct cvmx_error_regbit[]){
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 0, 0, "PKO_REG_ERROR[PARITY]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 1, 0, "PKO_REG_ERROR[DOORBELL]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 2, 0, "PKO_REG_ERROR[CURRZERO]"},
-							{0}},
-						NULL /*cvmx_error_childbit*/
-					},
-					{0}}},
-				{1, 29 /* tim */, (struct cvmx_error_muxchild[]){
-					{CVMX_ADD_IO_SEG(0x0001180058000088ull) /* CVMX_TIM_REG_ERROR */, CVMX_ADD_IO_SEG(0x0001180058000090ull) /* CVMX_TIM_REG_INT_MASK */, (struct cvmx_error_regbit[]){
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 0, 0, "TIM_REG_ERROR[MASK]"},
-							{0}},
-						NULL /*cvmx_error_childbit*/
-					},
-					{0}}},
-				{1, 49 /* pem1 */, (struct cvmx_error_muxchild[]){
-					{CVMX_ADD_IO_SEG(0x00011800C0000428ull) + ((1) & 3) * 0x1000000ull /* CVMX_PEMX_INT_SUM(1) */, CVMX_ADD_IO_SEG(0x00011800C0000430ull) + ((1) & 3) * 0x1000000ull /* CVMX_PEMX_INT_ENB(1) */, (struct cvmx_error_regbit[]){
-							{1, 1, CVMX_ERROR_GROUP_PCI, 1, 1, "PEMX_INT_SUM(1)[SE]"},
-							{1, 1, CVMX_ERROR_GROUP_PCI, 4, 1, "PEMX_INT_SUM(1)[UP_B1]"},
-							{1, 1, CVMX_ERROR_GROUP_PCI, 5, 1, "PEMX_INT_SUM(1)[UP_B2]"},
-							{1, 1, CVMX_ERROR_GROUP_PCI, 6, 1, "PEMX_INT_SUM(1)[UP_BX]"},
-							{1, 1, CVMX_ERROR_GROUP_PCI, 7, 1, "PEMX_INT_SUM(1)[UN_B1]"},
-							{1, 1, CVMX_ERROR_GROUP_PCI, 8, 1, "PEMX_INT_SUM(1)[UN_B2]"},
-							{1, 1, CVMX_ERROR_GROUP_PCI, 9, 1, "PEMX_INT_SUM(1)[UN_BX]"},
-							{1, 1, CVMX_ERROR_GROUP_PCI, 11, 1, "PEMX_INT_SUM(1)[RDLK]"},
-							{1, 1, CVMX_ERROR_GROUP_PCI, 12, 1, "PEMX_INT_SUM(1)[CRS_ER]"},
-							{1, 1, CVMX_ERROR_GROUP_PCI, 13, 1, "PEMX_INT_SUM(1)[CRS_DR]"},
-							{0}},
-						(struct cvmx_error_childbit[]){
-						{1, 10 /* exc */, (struct cvmx_error_muxchild[]){
-							{CVMX_ADD_IO_SEG(0x00011800C00000D0ull) + ((1) & 3) * 0x1000000ull /* CVMX_PEMX_DBG_INFO(1) */, CVMX_ADD_IO_SEG(0x00011800C00000A0ull) + ((1) & 3) * 0x1000000ull /* CVMX_PEMX_DBG_INFO_EN(1) */, (struct cvmx_error_regbit[]){
-									{1, 1, CVMX_ERROR_GROUP_PCI, 0, 1, "PEMX_DBG_INFO(1)[SPOISON]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 2, 1, "PEMX_DBG_INFO(1)[RTLPLLE]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 3, 1, "PEMX_DBG_INFO(1)[RECRCE]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 4, 1, "PEMX_DBG_INFO(1)[RPOISON]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 5, 1, "PEMX_DBG_INFO(1)[RCEMRC]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 6, 1, "PEMX_DBG_INFO(1)[RNFEMRC]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 7, 1, "PEMX_DBG_INFO(1)[RFEMRC]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 8, 1, "PEMX_DBG_INFO(1)[RPMERC]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 9, 1, "PEMX_DBG_INFO(1)[RPTAMRC]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 10, 1, "PEMX_DBG_INFO(1)[RUMEP]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 11, 1, "PEMX_DBG_INFO(1)[RVDM]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 12, 1, "PEMX_DBG_INFO(1)[ACTO]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 13, 1, "PEMX_DBG_INFO(1)[RTE]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 14, 1, "PEMX_DBG_INFO(1)[MRE]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 15, 1, "PEMX_DBG_INFO(1)[RDWDLE]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 16, 1, "PEMX_DBG_INFO(1)[RTWDLE]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 17, 1, "PEMX_DBG_INFO(1)[DPEOOSD]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 18, 1, "PEMX_DBG_INFO(1)[FCPVWT]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 19, 1, "PEMX_DBG_INFO(1)[RPE]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 20, 1, "PEMX_DBG_INFO(1)[FCUV]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 21, 1, "PEMX_DBG_INFO(1)[RQO]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 22, 1, "PEMX_DBG_INFO(1)[RAUC]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 23, 1, "PEMX_DBG_INFO(1)[RACUR]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 24, 1, "PEMX_DBG_INFO(1)[RACCA]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 25, 1, "PEMX_DBG_INFO(1)[CAAR]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 26, 1, "PEMX_DBG_INFO(1)[RARWDNS]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 27, 1, "PEMX_DBG_INFO(1)[RAMTLP]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 28, 1, "PEMX_DBG_INFO(1)[RACPP]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 29, 1, "PEMX_DBG_INFO(1)[RAWWPP]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 30, 1, "PEMX_DBG_INFO(1)[ECRC_E]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 31, 1, "PEMX_DBG_INFO(1)[RTRY_PE]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 32, 1, "PEMX_DBG_INFO(1)[HDRQ_PE]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 33, 1, "PEMX_DBG_INFO(1)[DATQ_PE]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 34, 1, "PEMX_DBG_INFO(1)[P_D_SBE]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 35, 1, "PEMX_DBG_INFO(1)[P_D_DBE]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 36, 1, "PEMX_DBG_INFO(1)[P_C_SBE]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 37, 1, "PEMX_DBG_INFO(1)[P_C_DBE]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 38, 1, "PEMX_DBG_INFO(1)[N_D_SBE]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 39, 1, "PEMX_DBG_INFO(1)[N_D_DBE]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 40, 1, "PEMX_DBG_INFO(1)[N_C_SBE]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 41, 1, "PEMX_DBG_INFO(1)[N_C_DBE]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 42, 1, "PEMX_DBG_INFO(1)[C_D_SBE]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 43, 1, "PEMX_DBG_INFO(1)[C_D_DBE]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 44, 1, "PEMX_DBG_INFO(1)[C_C_SBE]"},
-									{1, 1, CVMX_ERROR_GROUP_PCI, 45, 1, "PEMX_DBG_INFO(1)[C_C_DBE]"},
-									{0}},
-								NULL /*cvmx_error_childbit*/
-							},
-							{0}}},
-						{0}}
-					},
-					{0}}},
 				{1, 34 /* sli */, (struct cvmx_error_muxchild[]){
 					{CVMX_ADD_IO_SEG(0x00011F0000010330ull) /* CVMX_PEXP_SLI_INT_SUM */, CVMX_ADD_IO_SEG(0x00011F0000013CD0ull) /* CVMX_PEXP_SLI_INT_ENB_CIU */, (struct cvmx_error_regbit[]){
 							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 0, 0, "PEXP_SLI_INT_SUM[RML_TO]"},
@@ -4343,62 +4509,6 @@ static struct cvmx_error_muxchild error_tree_cn70xx =
 						NULL /*cvmx_error_childbit*/
 					},
 					{0}}},
-				{1, 31 /* key */, (struct cvmx_error_muxchild[]){
-					{CVMX_ADD_IO_SEG(0x0001180020000000ull) /* CVMX_KEY_INT_SUM */, CVMX_ADD_IO_SEG(0x0001180020000008ull) /* CVMX_KEY_INT_ENB */, (struct cvmx_error_regbit[]){
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 0, 0, "KEY_INT_SUM[KEY_SBE]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 1, 0, "KEY_INT_SUM[KEY_DBE]"},
-							{0}},
-						NULL /*cvmx_error_childbit*/
-					},
-					{0}}},
-				{1, 23 /* pow */, (struct cvmx_error_muxchild[]){
-					{CVMX_ADD_IO_SEG(0x0001670000000218ull) /* CVMX_POW_ECC_ERR */, CVMX_ADD_IO_SEG(0x0001670000000218ull) /* CVMX_POW_ECC_ERR */, (struct cvmx_error_regbit[]){
-							{1, 0, CVMX_ERROR_GROUP_INTERNAL, 0, 0, "POW_ECC_ERR[SBE]"},
-							{1, 0, CVMX_ERROR_GROUP_INTERNAL, 1, 0, "POW_ECC_ERR[DBE]"},
-							{1, 0, CVMX_ERROR_GROUP_INTERNAL, 12, 0, "POW_ECC_ERR[RPE]"},
-							{1, 0, CVMX_ERROR_GROUP_INTERNAL, 16, 0, "POW_ECC_ERR[IOP]"},
-							{0}},
-						NULL /*cvmx_error_childbit*/
-					},
-					{0}}},
-				{1, 26 /* pip */, (struct cvmx_error_muxchild[]){
-					{CVMX_ADD_IO_SEG(0x00011800A0000008ull) /* CVMX_PIP_INT_REG */, CVMX_ADD_IO_SEG(0x00011800A0000010ull) /* CVMX_PIP_INT_EN */, (struct cvmx_error_regbit[]){
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 3, 0, "PIP_INT_REG[PRTNXA]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 4, 0, "PIP_INT_REG[BADTAG]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 5, 0, "PIP_INT_REG[SKPRUNT]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 6, 0, "PIP_INT_REG[TODOOVR]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 7, 0, "PIP_INT_REG[FEPERR]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 8, 0, "PIP_INT_REG[BEPERR]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 12, 0, "PIP_INT_REG[PUNYERR]"},
-							{0}},
-						NULL /*cvmx_error_childbit*/
-					},
-					{0}}},
-				{1, 32 /* dfa */, (struct cvmx_error_muxchild[]){
-					{CVMX_ADD_IO_SEG(0x0001180037000028ull) /* CVMX_DFA_ERROR */, CVMX_ADD_IO_SEG(0x0001180037000030ull) /* CVMX_DFA_INTMSK */, (struct cvmx_error_regbit[]){
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 0, 0, "DFA_ERROR[DBLOVF]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 1, 0, "DFA_ERROR[DC0PERR]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 13, 0, "DFA_ERROR[DLC0_OVFERR]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 17, 0, "DFA_ERROR[DFANXM]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 18, 0, "DFA_ERROR[REPLERR]"},
-							{0}},
-						NULL /*cvmx_error_childbit*/
-					},
-					{0}}},
-				{1, 21 /* iob */, (struct cvmx_error_muxchild[]){
-					{CVMX_ADD_IO_SEG(0x00011800F0000058ull) /* CVMX_IOB_INT_SUM */, CVMX_ADD_IO_SEG(0x00011800F0000060ull) /* CVMX_IOB_INT_ENB */, (struct cvmx_error_regbit[]){
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 0, 0, "IOB_INT_SUM[NP_SOP]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 1, 0, "IOB_INT_SUM[NP_EOP]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 2, 0, "IOB_INT_SUM[P_SOP]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 3, 0, "IOB_INT_SUM[P_EOP]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 4, 0, "IOB_INT_SUM[NP_DAT]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 5, 0, "IOB_INT_SUM[P_DAT]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 6, 0, "IOB_INT_SUM[INB_MAT]"},
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 7, 0, "IOB_INT_SUM[OUTB_MAT]"},
-							{0}},
-						NULL /*cvmx_error_childbit*/
-					},
-					{0}}},
 				{1, 35 /* dpi */, (struct cvmx_error_muxchild[]){
 					{CVMX_ADD_IO_SEG(0x0001DF0000000008ull) /* CVMX_DPI_INT_REG */, CVMX_ADD_IO_SEG(0x0001DF0000000010ull) /* CVMX_DPI_INT_EN */, (struct cvmx_error_regbit[]){
 							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 0, 0, "DPI_INT_REG[NDERR]"},
@@ -4434,13 +4544,6 @@ static struct cvmx_error_muxchild error_tree_cn70xx =
 						NULL /*cvmx_error_childbit*/
 					},
 					{0}}},
-				{1, 30 /* rad */, (struct cvmx_error_muxchild[]){
-					{CVMX_ADD_IO_SEG(0x0001180070000088ull) /* CVMX_RAD_REG_ERROR */, CVMX_ADD_IO_SEG(0x0001180070000090ull) /* CVMX_RAD_REG_INT_MASK */, (struct cvmx_error_regbit[]){
-							{1, 1, CVMX_ERROR_GROUP_INTERNAL, 0, 0, "RAD_REG_ERROR[DOORBELL]"},
-							{0}},
-						NULL /*cvmx_error_childbit*/
-					},
-					{0}}},
 				{0}}
 			},
 			{0}}},
diff --git a/arch/mips/cavium-octeon/executive/cvmx-fpa-resource.c b/arch/mips/cavium-octeon/executive/cvmx-fpa-resource.c
index 8b67bce..30ba22f 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-fpa-resource.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-fpa-resource.c
@@ -136,9 +136,9 @@ int cvmx_fpa_allocate_fpa_pools(int node, int pools_allocated[], int count)
 			     " node=%d\n", node);
 		return -1;
 	}
-	rv = cvmx_allocate_global_resource_range_non_contiguous(tag, owner,
-							       count,
-							       pools_allocated);
+	rv = cvmx_resource_alloc_many(tag, owner,
+				      count,
+				      pools_allocated);
 	return rv;
 }
 
@@ -173,9 +173,9 @@ int cvmx_fpa_allocate_auras(int node, int auras_allocated[], int count)
 			     " node=%d\n", node);
 		return -1;
 	}
-	rv = cvmx_allocate_global_resource_range_non_contiguous(tag, owner,
-							       count,
-							       auras_allocated);
+	rv = cvmx_resource_alloc_many(tag, owner,
+				      count,
+				      auras_allocated);
 	return rv;
 
 }
diff --git a/arch/mips/cavium-octeon/executive/cvmx-global-resources.c b/arch/mips/cavium-octeon/executive/cvmx-global-resources.c
index 153a42c..7f08f55 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-global-resources.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-global-resources.c
@@ -415,10 +415,10 @@ int cvmx_allocate_global_resource_range(struct global_resource_tag tag, uint64_t
 	return base;
 }
 
-int cvmx_allocate_global_resource_range_non_contiguous(struct global_resource_tag tag,
-						      uint64_t owner,
-						      int nelements,
-						      int allocated_elements[]) {
+int cvmx_resource_alloc_many(struct global_resource_tag tag,
+			     uint64_t owner,
+			     int nelements,
+			     int allocated_elements[]) {
 	uint64_t addr = cvmx_get_global_resource(tag,1);
 	int rv;
 
@@ -432,7 +432,6 @@ int cvmx_allocate_global_resource_range_non_contiguous(struct global_resource_ta
 	rv = cvmx_range_alloc_non_contiguos(addr, owner, nelements, allocated_elements);
 	__cvmx_global_resource_unlock();
 	return rv;
-
 }
 
 int cvmx_reserve_global_resource_range(struct global_resource_tag tag,
diff --git a/arch/mips/cavium-octeon/executive/cvmx-gser.c b/arch/mips/cavium-octeon/executive/cvmx-gser.c
new file mode 100644
index 0000000..89fafcf
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-gser.c
@@ -0,0 +1,277 @@
+/***********************license start***************
+ * Copyright (c) 2013  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Helper utilities for qlm.
+ *
+ * <hr>$Revision$<hr>
+ */
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include <asm/octeon/cvmx.h>
+#include <asm/octeon/cvmx-helper.h>
+#include <asm/octeon/cvmx-qlm.h>
+#include <asm/octeon/cvmx-gserx-defs.h>
+#else
+#include "cvmx.h"
+#include "cvmx-helper.h"
+#include "cvmx-qlm.h"
+#endif
+
+
+/**
+ * @INTERNAL
+ * Configure the gser pll registers.
+ *
+ * @param interface Interface to bring up
+ *
+ * @param mode      Mode to configure the gser as
+ *
+ * @param qlm       QLM attached to this interface
+ *
+ * @parma num_ports Number of ports on this interface
+ *
+ * @return Zero on success, negative on failure
+ */
+static int gser_pll_init(int				interface,
+			 cvmx_helper_interface_mode_t	mode,
+			 int				qlm,
+			 int				num_ports)
+{
+	cvmx_gserx_pll_px_mode_0_t	gser_pll_p_mode_0;
+	cvmx_gserx_pll_px_mode_1_t	gser_pll_p_mode_1;
+	cvmx_gserx_lanex_px_mode_0_t	gser_lane_p_mode_0;
+	int				lane_mode;
+	int				i;
+
+	/* Figure out the lane mode */
+	switch (mode) {
+	case CVMX_HELPER_INTERFACE_MODE_SGMII:
+		lane_mode = 0x6;
+		break;
+
+	case CVMX_HELPER_INTERFACE_MODE_XAUI:
+		lane_mode = 0x4;
+		break;
+
+	default:
+		lane_mode = 0;
+		break;
+	}
+
+	/* Configure pll_p_mode_0 */
+	gser_pll_p_mode_0.u64 = 0;
+	switch (mode) {
+	case CVMX_HELPER_INTERFACE_MODE_SGMII:
+		gser_pll_p_mode_0.s.pll_icp = 1;
+		gser_pll_p_mode_0.s.pll_rloop = 3;
+		gser_pll_p_mode_0.s.pll_pcs_div = 0x28;
+		break;
+
+	case CVMX_HELPER_INTERFACE_MODE_XAUI:
+		gser_pll_p_mode_0.s.pll_icp = 1;
+		gser_pll_p_mode_0.s.pll_rloop = 3;
+		gser_pll_p_mode_0.s.pll_pcs_div = 0x24;
+		break;
+
+	default:
+		break;
+	}
+	cvmx_write_csr(CVMX_GSERX_PLL_PX_MODE_0(qlm, lane_mode),
+		       gser_pll_p_mode_0.u64);
+
+	/* Configure pll_p_mode_1 */
+	gser_pll_p_mode_1.u64 = 0;
+	switch (mode) {
+	case CVMX_HELPER_INTERFACE_MODE_SGMII:
+		gser_pll_p_mode_1.s.pll_div = 16;
+		gser_pll_p_mode_1.s.pll_opr = 0;
+		gser_pll_p_mode_1.s.pll_pcie3en = 0;
+		gser_pll_p_mode_1.s.pll_cpadj = 3;
+		gser_pll_p_mode_1.s.pll_16p5en = 1;
+		break;
+
+	case CVMX_HELPER_INTERFACE_MODE_XAUI:
+		gser_pll_p_mode_1.s.pll_div = 20;
+		gser_pll_p_mode_1.s.pll_opr = 0;
+		gser_pll_p_mode_1.s.pll_pcie3en = 0;
+		gser_pll_p_mode_1.s.pll_cpadj = 2;
+		gser_pll_p_mode_1.s.pll_16p5en = 1;
+		break;
+
+	default:
+		break;
+	}
+	cvmx_write_csr(CVMX_GSERX_PLL_PX_MODE_1(qlm, lane_mode),
+		       gser_pll_p_mode_1.u64);
+
+	/* Configure lane_p_mode */
+	for (i = 0; i < num_ports; i++) {
+		gser_lane_p_mode_0.u64 = 0;
+		gser_lane_p_mode_0.s.srate = 0;
+		gser_lane_p_mode_0.s.rx_mode = 3;
+		gser_lane_p_mode_0.s.tx_mode = 3;
+
+		switch (mode) {
+		case CVMX_HELPER_INTERFACE_MODE_SGMII:
+			gser_lane_p_mode_0.s.ctle = 0;
+			gser_lane_p_mode_0.s.pcie = 0;
+			gser_lane_p_mode_0.s.tx_ldiv = 2;
+			gser_lane_p_mode_0.s.rx_ldiv = 2;
+			break;
+
+		case CVMX_HELPER_INTERFACE_MODE_XAUI:
+			gser_lane_p_mode_0.s.ctle = 0;
+			gser_lane_p_mode_0.s.pcie = 0;
+			gser_lane_p_mode_0.s.tx_ldiv = 2;
+			gser_lane_p_mode_0.s.rx_ldiv = 2;
+			break;
+
+		default:
+			break;
+		}
+		cvmx_write_csr(CVMX_GSERX_LANEX_PX_MODE_0(qlm, i, lane_mode),
+			       gser_lane_p_mode_0.u64);
+	}
+
+	return 0;
+}
+
+/**
+ * @INTERNAL
+ * Configure the gser.
+ *
+ * @param interface Interface to bring up
+ *
+ * @param mode      Mode to configure the gser as
+ *
+ * @return Zero on success, negative on failure
+ */
+int gser_init(int				interface,
+	      cvmx_helper_interface_mode_t	mode)
+{
+	cvmx_gserx_phy_ctl_t		gser_phy_ctl;
+	cvmx_gserx_cfg_t		gser_cfg;
+	cvmx_gserx_rx_coast_t		gser_rx_coast;
+	cvmx_gserx_rx_eie_deten_t	gser_rx_eie_deten;
+	cvmx_gserx_lane_mode_t		gser_lane_mode;
+	cvmx_gserx_qlm_stat_t		gser_qlm_stat;
+	cvmx_gserx_pll_stat_t		gser_pll_stat;
+	cvmx_gserx_rx_eie_detsts_t	gser_rx_eie_detsts;
+	int				lane_mode;
+	int				qlm;
+	int				num_ports;
+
+	qlm = cvmx_qlm_interface(interface);
+	num_ports = cvmx_helper_ports_on_interface(interface);
+
+	/* Figure out the lane mode */
+	switch (mode) {
+	case CVMX_HELPER_INTERFACE_MODE_SGMII:
+		lane_mode = 0x6;
+		break;
+
+	case CVMX_HELPER_INTERFACE_MODE_XAUI:
+		lane_mode = 0x4;
+		break;
+
+	default:
+		lane_mode = 0;
+		break;
+	}
+
+	/* Power up phy, but keek it in reset */
+	gser_phy_ctl.u64 = 0;
+	gser_phy_ctl.s.phy_pd = 0;
+	gser_phy_ctl.s.phy_reset = 1;
+	cvmx_write_csr(CVMX_GSERX_PHY_CTL(qlm), gser_phy_ctl.u64);
+
+	/* Set gser for the interface mode */
+	gser_cfg.u64 = 0;
+	gser_cfg.s.ila = mode == CVMX_HELPER_INTERFACE_MODE_ILK ? 1 : 0;
+	gser_cfg.s.bgx = mode == CVMX_HELPER_INTERFACE_MODE_ILK ? 0 : 1;
+	gser_cfg.s.bgx_quad = mode == CVMX_HELPER_INTERFACE_MODE_XAUI ? 1 : 0;
+	gser_cfg.s.bgx_dual = 0;
+	gser_cfg.s.pcie = 0;
+	cvmx_write_csr(CVMX_GSERX_CFG(qlm), gser_cfg.u64);
+
+	/* Enable the port lanes */
+	gser_rx_coast.u64 = cvmx_read_csr(CVMX_GSERX_RX_COAST(qlm));
+	gser_rx_coast.s.coast |= ((1 << num_ports) - 1);
+	cvmx_write_csr(CVMX_GSERX_RX_COAST(qlm), gser_rx_coast.u64);
+
+	gser_rx_eie_deten.u64 = cvmx_read_csr(CVMX_GSERX_RX_EIE_DETEN(qlm));
+	gser_rx_eie_deten.s.eiede |= ((1 << num_ports) - 1);
+	cvmx_write_csr(CVMX_GSERX_RX_EIE_DETEN(qlm), gser_rx_eie_deten.u64);
+
+	/* Lane mode */
+	gser_lane_mode.u64 = 0;
+	gser_lane_mode.s.lmode = lane_mode;
+	cvmx_write_csr(CVMX_GSERX_LANE_MODE(qlm), gser_lane_mode.u64);
+
+	/* Bring phy out of reset */
+	gser_phy_ctl.u64 = cvmx_read_csr(CVMX_GSERX_PHY_CTL(qlm));
+	gser_phy_ctl.s.phy_reset = 0;
+	cvmx_write_csr(CVMX_GSERX_PHY_CTL(qlm), gser_phy_ctl.u64);
+	gser_phy_ctl.u64 = cvmx_read_csr(CVMX_GSERX_PHY_CTL(qlm));
+
+	/*
+	 * Wait 250 ns until the managment interface is ready to accept
+	 * read/write commands.
+	 */
+	cvmx_wait_usec(3);
+
+	/* Configure the gser pll */
+	gser_pll_init(interface, mode, qlm, num_ports);
+
+	/* Wait for reset to complete and the PLL to lock */
+	do {
+		gser_qlm_stat.u64 = cvmx_read_csr(CVMX_GSERX_QLM_STAT(qlm));
+		gser_pll_stat.u64 = cvmx_read_csr(CVMX_GSERX_PLL_STAT(qlm));
+	} while(!gser_qlm_stat.s.rst_rdy || !gser_pll_stat.s.pll_lock);
+
+	/* Wait for cdrlock */
+	do {
+		gser_rx_eie_detsts.u64 =
+			cvmx_read_csr(CVMX_GSERX_RX_EIE_DETSTS(qlm));
+	} while((gser_rx_eie_detsts.s.cdrlock & 0xf) != 0xf);
+
+	return 0;
+}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-agl.c b/arch/mips/cavium-octeon/executive/cvmx-helper-agl.c
index 45a7bb3..5ae133b 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-agl.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-agl.c
@@ -200,7 +200,7 @@ int __cvmx_helper_agl_enable(int interface)
 		pko_mem_port_ptrs.u64 = cvmx_read_csr(CVMX_PKO_MEM_PORT_PTRS);
 		if (pko_mem_port_ptrs.s.pid == 24) {
 			pko_mem_port_ptrs.s.eid = 10;
-			pko_mem_port_ptrs.s.bp_port = 63;
+			pko_mem_port_ptrs.s.bp_port = 40;
 			cvmx_write_csr(CVMX_PKO_MEM_PORT_PTRS, pko_mem_port_ptrs.u64);
 			break;
 		}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-board.c b/arch/mips/cavium-octeon/executive/cvmx-helper-board.c
index 9c73d02..fdf89c5 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-board.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-board.c
@@ -429,6 +429,20 @@ int __cvmx_helper_board_get_port_from_dt(void *fdt_addr, int ipd_port)
 	if (dbg)
 		cvmx_dprintf("%s: eth subnode offset %d from %s\n",
 			     __func__, eth, name_buffer);
+
+	if (eth < 0) 
+		return -1;
+
+	if (fdt_getprop(fdt_addr, eth, "cavium,sgmii-mac-phy-mode", NULL))
+		cvmx_helper_set_mac_phy_mode(interface_num, port_index, true);
+	else
+		cvmx_helper_set_mac_phy_mode(interface_num, port_index, false);
+
+	if (fdt_getprop(fdt_addr, eth, "cavium,sgmii-mac-1000x-mode", NULL))
+		cvmx_helper_set_1000x_mode(interface_num, port_index, true);
+	else
+		cvmx_helper_set_1000x_mode(interface_num, port_index, false);
+
 	return (eth >= 0);
 }
 
@@ -460,6 +474,7 @@ int __get_phy_info_from_dt(cvmx_phy_info_t *phy_info, int ipd_port)
 	uint64_t *smi_addrp;
 	uint64_t smi_addr = 0;
 	int dbg = device_tree_dbg;
+	int interface;
 
 	phy_info->phy_addr = -1;
 	phy_info->direct_connect = -1;
@@ -493,15 +508,15 @@ int __get_phy_info_from_dt(cvmx_phy_info_t *phy_info, int ipd_port)
 				     "ipd_port=%d\n", ipd_port);
 		return -1;
 	}
+
+	interface = cvmx_helper_get_interface_num(ipd_port);
 	/* Get handle to phy */
 	phy_handle = (uint32_t *) fdt_getprop(fdt_addr, eth, "phy-handle", NULL);
 	if (!phy_handle) {
-		int interface;
 		cvmx_helper_interface_mode_t if_mode;
 		/* Note that it's OK for RXAUI and ILK to not have a PHY
 		 * connected (i.e. EBB boards in loopback).
 		 */
-		interface = cvmx_helper_get_interface_num(ipd_port);
 		if_mode = cvmx_helper_interface_get_mode(interface);
 		if (if_mode != CVMX_HELPER_INTERFACE_MODE_RXAUI &&
 		    if_mode != CVMX_HELPER_INTERFACE_MODE_ILK) {
@@ -1919,91 +1934,3 @@ int __cvmx_helper_board_usb_get_num_ports(int supported_ports)
 
 	return supported_ports;
 }
-
-#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
-/**
- * @INTERNAL
- * This function outputs the port flags for the specified interface and port.
- *
- * @param interface interface to get the port flags for
- * @param index     port on interface to get the port flags for
- * @param[out] pflags port flags for the specified port.  Not modified if the
- *		      data is unavailable.
- *
- * @return 0 for success, -1 if info no available.
- */
-int __cvmx_helper_board_get_port_flags(int interface, int index)
-{
-	static void *fdt_addr;
-	int ipd_port;
-	int aliases, eth;
-	cvmx_helper_interface_mode_t interface_mode =
-				cvmx_helper_interface_get_mode(interface);
-
-	switch (interface_mode) {
-	case CVMX_HELPER_INTERFACE_MODE_ILK:
-	case CVMX_HELPER_INTERFACE_MODE_LOOP:
-	case CVMX_HELPER_INTERFACE_MODE_SRIO:
-	case CVMX_HELPER_INTERFACE_MODE_PCIE:
-	case CVMX_HELPER_INTERFACE_MODE_NPI:
-		/* These interface types have no device tree entries */
-		return 0;
-	case CVMX_HELPER_INTERFACE_MODE_AGL:
-		/* Their is no device tree for agl interface when running
-		   on simulator. */
-		if (cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_SIM)
-			return 0;
-		break;
-	default:
-		break;
-	}
-
-	if (fdt_addr == 0)
-		fdt_addr = __cvmx_phys_addr_to_ptr(cvmx_sysinfo_get()->fdt_addr,
-						   OCTEON_FDT_MAX_SIZE);
-
-	if (!fdt_addr) {
-		cvmx_dprintf("No device tree found.\n");
-		return -1;
-	}
-
-	aliases = fdt_path_offset(fdt_addr, "/aliases");
-	if (aliases < 0) {
-		cvmx_dprintf("Error: no /aliases node in device tree.\n");
-		return -1;
-	}
-
-	ipd_port = cvmx_helper_get_ipd_port(interface, index);
-	eth = __pip_eth_node(fdt_addr, aliases, ipd_port);
-	if (eth < 0)
-		return 0;
-
-	if (fdt_getprop(fdt_addr, eth, "cavium,sgmii-mac-phy-mode", NULL))
-		cvmx_helper_set_mac_phy_mode(interface, index, true);
-	else
-		cvmx_helper_set_mac_phy_mode(interface, index, false);
-
-	if (fdt_getprop(fdt_addr, eth, "cavium,sgmii-mac-1000x-mode", NULL))
-		cvmx_helper_set_1000x_mode(interface, index, true);
-	else
-		cvmx_helper_set_1000x_mode(interface, index, false);
-
-	return 0;
-}
-#endif
-/**
- * @INTERNAL
- * This function outputs the port flags for the specified interface and port.
- *
- * @param interface interface to get the port flags for
- * @param[out] iflags interface flags for the specified port.  Not modified if
- *		      the data is unavailable.
- *
- * @return 0 for success, -1 if info no available.
- *
- * NOTE: As of 6/21/2013 no interface flags have been defined.
- */
-int __cvmx_helper_board_get_interface_flags(int interface, uint32_t *iflags)
-{
-	return 0;
-}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c b/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c
index 6f52a70..1396c91 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c
@@ -83,8 +83,8 @@ CVMX_SHARED struct cvmx_cfg_port_param cvmx_cfg_port[CVMX_HELPER_MAX_IFACE][CVMX
 	{[0 ... CVMX_HELPER_MAX_IFACE - 1] = {[0 ... CVMX_HELPER_CFG_MAX_PORT_PER_IFACE - 1] =
 					      { CVMX_HELPER_CFG_INVALID_VALUE, CVMX_HELPER_CFG_INVALID_VALUE,
 						CVMX_HELPER_CFG_INVALID_VALUE, CVMX_HELPER_CFG_INVALID_VALUE,
-						CVMX_HELPER_CFG_INVALID_VALUE, CVMX_HELPER_CFG_INVALID_VALUE,
-						CVMX_HELPER_CFG_INVALID_VALUE}}};
+						CVMX_HELPER_CFG_INVALID_VALUE, 0,
+						0}}};
 /*
  * Indexed by the pko_port number
  */
@@ -504,7 +504,7 @@ void cvmx_helper_cfg_set_jabber_and_frame_max()
 			cvmx_pip_set_frame_check(interface, -1);
 			for (port = 0; port < num_ports; port++) {
 				int ipd_port = cvmx_helper_get_ipd_port(interface, port);
-				cvmx_ilk_enable_la_header(ipd_port, 1);
+				cvmx_ilk_enable_la_header(ipd_port, 0);
 			}
 			break;
 		case CVMX_HELPER_INTERFACE_MODE_SRIO:
@@ -867,11 +867,6 @@ int __cvmx_helper_init_port_config_data(void)
 		for (port = 0; port < num_ports; port++) {
 			bool init_req = false;
 
-#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
-
-			if (__cvmx_helper_board_get_port_flags(interface, port))
-				continue;
-#endif
 			if (octeon_has_feature(OCTEON_FEATURE_PKND)) {
 				port_base = __cvmx_helper_cfg_pko_port_base(interface, port);
 				if (port_base == CVMX_HELPER_CFG_INVALID_VALUE)
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-ilk.c b/arch/mips/cavium-octeon/executive/cvmx-helper-ilk.c
index dbb6cf9..5950725 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-ilk.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-ilk.c
@@ -130,16 +130,16 @@ void __cvmx_ilk_init_cal(int interface)
 
 	/* Set state to xoff for all entries */
 	rx_cal0.u64 = 0;
-	rx_cal0.s.entry_ctl0 = XOFF;
-	rx_cal0.s.entry_ctl1 = XOFF;
-	rx_cal0.s.entry_ctl2 = XOFF;
-	rx_cal0.s.entry_ctl3 = XOFF;
+	rx_cal0.s.entry_ctl0 = XON;
+	rx_cal0.s.entry_ctl1 = XON;
+	rx_cal0.s.entry_ctl2 = XON;
+	rx_cal0.s.entry_ctl3 = XON;
 
 	rx_cal1.u64 = 0;
-	rx_cal1.s.entry_ctl4 = XOFF;
-	rx_cal1.s.entry_ctl5 = XOFF;
-	rx_cal1.s.entry_ctl6 = XOFF;
-	rx_cal1.s.entry_ctl7 = XOFF;
+	rx_cal1.s.entry_ctl4 = XON;
+	rx_cal1.s.entry_ctl5 = XON;
+	rx_cal1.s.entry_ctl6 = XON;
+	rx_cal1.s.entry_ctl7 = XON;
 
 	/* Write all 288 entries */
 	for (i = 0; i < CVMX_ILK_MAX_CAL_IDX; i++) {
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-npi.c b/arch/mips/cavium-octeon/executive/cvmx-helper-npi.c
index a8d5f33..b5ae333 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-npi.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-npi.c
@@ -43,7 +43,7 @@
  * Functions for NPI initialization, configuration,
  * and monitoring.
  *
- * <hr>$Revision: 78654 $<hr>
+ * <hr>$Revision: 87074 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
@@ -73,7 +73,6 @@ int __cvmx_helper_npi_probe(int interface)
 {
 	/* TODO: When using config language what do we return? */
 	if (OCTEON_IS_MODEL(OCTEON_CN68XX)) {
-		if (__cvmx_pko_queue_static_config.pknd.pko_queues_per_port_pci > 0)
 			return 32;
 	}
 	else if (!(OCTEON_IS_MODEL(OCTEON_CN52XX_PASS1_X) ||
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-rgmii.c b/arch/mips/cavium-octeon/executive/cvmx-helper-rgmii.c
index 27e5e23..f02873d 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-rgmii.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-rgmii.c
@@ -43,7 +43,7 @@
  * Functions for RGMII/GMII/MII initialization, configuration,
  * and monitoring.
  *
- * <hr>$Revision: 75409 $<hr>
+ * <hr>$Revision: 86586 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
@@ -255,6 +255,33 @@ cvmx_helper_link_info_t __cvmx_helper_rgmii_link_get(int ipd_port)
 
 /**
  * @INTERNAL
+ * Return the link state of an IPD/PKO port as returned by
+ * auto negotiation. The result of this function may not match
+ * Octeon's link config if auto negotiation has changed since
+ * the last call to cvmx_helper_link_set().
+ *
+ * @param ipd_port IPD/PKO port to query
+ *
+ * @return Link state
+ */
+cvmx_helper_link_info_t __cvmx_helper_gmii_link_get(int ipd_port)
+{
+	cvmx_helper_link_info_t result;
+	int index = cvmx_helper_get_interface_index_num(ipd_port);
+
+	if (index == 0)
+		result = __cvmx_helper_rgmii_link_get(ipd_port);
+	else {
+		result.s.full_duplex = 1;
+		result.s.link_up = 1;
+		result.s.speed = 1000;
+	}
+
+	return result;
+}
+
+/**
+ * @INTERNAL
  * Configure an IPD/PKO port for the specified link state. This
  * function does not influence auto negotiation at the PHY level.
  * The passed link state must always match the link state returned
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.c b/arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.c
index 2a45fe4..6692113 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.c
@@ -1,5 +1,5 @@
 /***********************license start***************
- * Copyright (c) 2003-2010  Cavium Inc. (support@cavium.com). All rights
+ * Copyright (c) 2003-2013  Cavium Inc. (support@cavium.com). All rights
  * reserved.
  *
  *
@@ -43,7 +43,7 @@
  * Functions for SGMII initialization, configuration,
  * and monitoring.
  *
- * <hr>$Revision: 86446 $<hr>
+ * <hr>$Revision: 87025 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
@@ -52,9 +52,14 @@
 #include <asm/octeon/cvmx-helper.h>
 #include <asm/octeon/cvmx-helper-board.h>
 #include <asm/octeon/cvmx-helper-cfg.h>
+#include <asm/octeon/cvmx-bgxx-defs.h>
+#include <asm/octeon/cvmx-gserx-defs.h>
 #include <asm/octeon/cvmx-pcsx-defs.h>
 #include <asm/octeon/cvmx-gmxx-defs.h>
 #include <asm/octeon/cvmx-ciu-defs.h>
+#include <asm/octeon/cvmx-bgxx-defs.h>
+#include <asm/octeon/cvmx-gser.h>
+#include <asm/octeon/cvmx-bgx.h>
 #else
 
 #include "cvmx.h"
@@ -64,6 +69,8 @@
 #include "cvmx-helper-board.h"
 #include "cvmx-helper-cfg.h"
 #include "cvmx-qlm.h"
+#include "cvmx-gser.h"
+#include "cvmx-bgx.h"
 #endif
 
 
@@ -435,14 +442,13 @@ int __cvmx_helper_sgmii_enumerate(int interface)
 	if (OCTEON_IS_MODEL(OCTEON_CNF71XX))
 		return 2;
 	if (OCTEON_IS_MODEL(OCTEON_CN70XX)) {
-		cvmx_gmxx_inf_mode_t inf_mode;
-		inf_mode.u64 = cvmx_read_csr(CVMX_GMXX_INF_MODE(interface));
-		if (inf_mode.s.mode == 1)	/* SGMII */
+		enum cvmx_qlm_mode qlm_mode = cvmx_qlm_get_dlm_mode(0, interface);
+
+		if (qlm_mode == CVMX_QLM_MODE_SGMII)
 			return 1;
-		else if (inf_mode.s.mode == 2)	/* QSGMII */
+		else if (qlm_mode == CVMX_QLM_MODE_QSGMII)
 			return 4;
-		else
-			return 0;
+		return 0;
 	}
 	return 4;
 }
@@ -460,6 +466,7 @@ int __cvmx_helper_sgmii_enumerate(int interface)
 int __cvmx_helper_sgmii_probe(int interface)
 {
 	union cvmx_gmxx_inf_mode mode;
+	int ports;
 
 	/*
 	 * Check if QLM is configured correct for SGMII, verify the
@@ -471,6 +478,13 @@ int __cvmx_helper_sgmii_probe(int interface)
 		if (cvmx_qlm_get_mode(qlm) != CVMX_QLM_MODE_SGMII)
 			return 0;
 	}
+
+	/* Do not enable the interface if is not in SGMII mode */
+	ports = __cvmx_helper_sgmii_enumerate(interface);
+
+	if (ports <= 0)
+		return 0;
+
 	/*
 	 * Due to errata GMX-700 on CN56XXp1.x and CN52XXp1.x, the
 	 * interface needs to be enabled before IPD otherwise per port
@@ -480,7 +494,7 @@ int __cvmx_helper_sgmii_probe(int interface)
 	mode.s.en = 1;
 	cvmx_write_csr(CVMX_GMXX_INF_MODE(interface), mode.u64);
 
-	return __cvmx_helper_sgmii_enumerate(interface);
+	return ports;
 }
 
 /**
@@ -606,7 +620,7 @@ cvmx_helper_link_info_t __cvmx_helper_sgmii_link_get(int ipd_port)
 		return result;
 	}
 
-	if (OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN70XX)) {
+	if (OCTEON_IS_MODEL(OCTEON_CN66XX)) {
 		union cvmx_gmxx_inf_mode inf_mode;
 		inf_mode.u64 = cvmx_read_csr(CVMX_GMXX_INF_MODE(interface));
 		if (inf_mode.s.rate & (1 << index))
@@ -738,3 +752,136 @@ int __cvmx_helper_sgmii_configure_loopback(int ipd_port, int enable_internal,
 	__cvmx_helper_sgmii_hardware_init_link(interface, index);
 	return 0;
 }
+
+/**
+ * @INTERNAL
+ * Probe a SGMII interface and determine the number of ports
+ * connected to it. The SGMII interface should still be down after
+ * this call. This is used by interfaces using the bgx mac.
+ *
+ * @param interface Interface to probe
+ *
+ * @return Number of ports on the interface. Zero to disable.
+ */
+int __cvmx_helper_bgx_sgmii_probe(int interface)
+{
+	int	qlm;
+
+	/*
+	 * Check the QLM is configured correctly for SGMII, verify the
+	 * speed as well as the mode.
+	 */
+	qlm = cvmx_qlm_interface(interface);
+	if (cvmx_qlm_get_mode(qlm) != CVMX_QLM_MODE_SGMII)
+		return 0;
+
+	return __cvmx_helper_sgmii_enumerate(interface);
+}
+
+/**
+ * @INTERNAL
+ * Bringup and enable a SGMII interface. After this call packet
+ * I/O should be fully functional. This is called with IPD
+ * enabled but PKO disabled. This is used by interfaces using
+ * the bgx mac.
+ *
+ * @param interface Interface to bring up
+ *
+ * @return Zero on success, negative on failure
+ */
+int __cvmx_helper_bgx_sgmii_enable(int interface)
+{
+	cvmx_bgxx_cmrx_rx_id_map_t	bgx_cmr_rx_id_map;
+	int				num_ports;
+	int				pknd;
+	int				i;
+
+	num_ports = cvmx_helper_ports_on_interface(interface);
+
+	/* Configure the gser */
+	gser_init(interface, CVMX_HELPER_INTERFACE_MODE_SGMII);
+
+	/* Configure the bgx mac */
+	bgx_init(interface, CVMX_HELPER_INTERFACE_MODE_SGMII);
+
+	/*
+	 * Must hardcode the port kind here until the pko initializion is
+	 * complete. This must be removed once the pko initialization is
+	 * working. TODO
+	 */
+	pknd = 10 + (num_ports * interface);
+	for (i = 0; i < num_ports; i++) {
+		bgx_cmr_rx_id_map.u64 = 0;
+		bgx_cmr_rx_id_map.s.rid = 2 + i;
+		bgx_cmr_rx_id_map.s.pknd = pknd + i;
+		cvmx_write_csr(CVMX_BGXX_CMRX_RX_ID_MAP(i, interface),
+			       bgx_cmr_rx_id_map.u64);
+	}
+
+	return 0;
+}
+
+/**
+ * @INTERNAL
+ * Return the link state of an IPD/PKO port as returned by
+ * auto negotiation. The result of this function may not match
+ * Octeon's link config if auto negotiation has changed since
+ * the last call to cvmx_helper_link_set(). This is used by
+ * interfaces using the bgx mac.
+ *
+ * @param ipd_port IPD/PKO port to query
+ *
+ * @return Link state
+ */
+cvmx_helper_link_info_t __cvmx_helper_bgx_sgmii_link_get(int ipd_port)
+{
+	cvmx_helper_link_info_t result;
+
+	/* Hardcoded for now. TODO */
+	result.s.link_up = 1;
+	result.s.full_duplex = 1;
+	result.s.speed = 1000;
+
+	return result;
+}
+
+/**
+ * @INTERNAL
+ * Configure an IPD/PKO port for the specified link state. This
+ * function does not influence auto negotiation at the PHY level.
+ * The passed link state must always match the link state returned
+ * by cvmx_helper_link_get(). It is normally best to use
+ * cvmx_helper_link_autoconf() instead. This is used by interfaces
+ * using the bgx mac.
+ *
+ * @param ipd_port  IPD/PKO port to configure
+ * @param link_info The new link state
+ *
+ * @return Zero on success, negative on failure
+ */
+int __cvmx_helper_bgx_sgmii_link_set(int ipd_port,
+				 cvmx_helper_link_info_t link_info)
+{
+	return 0;
+}
+
+/**
+ * @INTERNAL
+ * Configure a port for internal and/or external loopback. Internal loopback
+ * causes packets sent by the port to be received by Octeon. External loopback
+ * causes packets received from the wire to sent out again. This is used by
+ * interfaces using the bgx mac.
+ *
+ * @param ipd_port IPD/PKO port to loopback.
+ * @param enable_internal
+ *                 Non zero if you want internal loopback
+ * @param enable_external
+ *                 Non zero if you want external loopback
+ *
+ * @return Zero on success, negative on failure.
+ */
+int __cvmx_helper_bgx_sgmii_configure_loopback(int ipd_port, int enable_internal,
+					   int enable_external)
+{
+	return 0;
+}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-xaui.c b/arch/mips/cavium-octeon/executive/cvmx-helper-xaui.c
index ee77ae9..cf92f1a 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-xaui.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-xaui.c
@@ -43,7 +43,7 @@
  * Functions for XAUI initialization, configuration,
  * and monitoring.
  *
- * <hr>$Revision: 83639 $<hr>
+ * <hr>$Revision: 87025 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
@@ -55,12 +55,17 @@
 #include <asm/octeon/cvmx-pcsx-defs.h>
 #include <asm/octeon/cvmx-pcsxx-defs.h>
 #include <asm/octeon/cvmx-ciu-defs.h>
+#include <asm/octeon/cvmx-bgxx-defs.h>
+#include <asm/octeon/cvmx-gser.h>
+#include <asm/octeon/cvmx-bgx.h>
 #else
 
 #include "cvmx.h"
 #include "cvmx-helper.h"
 #include "cvmx-helper-cfg.h"
 #include "cvmx-qlm.h"
+#include "cvmx-gser.h"
+#include "cvmx-bgx.h"
 #endif
 
 
@@ -69,12 +74,14 @@ int __cvmx_helper_xaui_enumerate(int interface)
 	union cvmx_gmxx_hg2_control gmx_hg2_control;
 
 	if (OCTEON_IS_MODEL(OCTEON_CN70XX)) {
-		cvmx_gmxx_inf_mode_t inf_mode;
-		inf_mode.u64 = cvmx_read_csr(CVMX_GMXX_INF_MODE(0));
-		if (inf_mode.s.mode == 3)	/* RXAUI */
+		enum cvmx_qlm_mode qlm_mode = cvmx_qlm_get_dlm_mode(0, interface);
+
+		if (qlm_mode == CVMX_QLM_MODE_RXAUI)
 			return 1;
 		return 0;
 		/* FIXME for higig2 */
+	} else if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+		return 1;
 	}
 	/* If HiGig2 is enabled return 16 ports, otherwise return 1 port */
 	gmx_hg2_control.u64 = cvmx_read_csr(CVMX_GMXX_HG2_CONTROL(interface));
@@ -96,7 +103,7 @@ int __cvmx_helper_xaui_enumerate(int interface)
  */
 int __cvmx_helper_xaui_probe(int interface)
 {
-	int i;
+	int i, ports;
 	union cvmx_gmxx_inf_mode mode;
 
 	/*
@@ -144,6 +151,11 @@ int __cvmx_helper_xaui_probe(int interface)
 			return 0;
 	}
 
+	ports =  __cvmx_helper_xaui_enumerate(interface);
+
+	if (ports <= 0)
+		return 0;
+
 	/*
 	 * Due to errata GMX-700 on CN56XXp1.x and CN52XXp1.x, the
 	 * interface needs to be enabled before IPD otherwise per port
@@ -155,7 +167,7 @@ int __cvmx_helper_xaui_probe(int interface)
 
 	__cvmx_helper_setup_gmx(interface, 1);
 
-	if (!OCTEON_IS_MODEL(OCTEON_CN68XX)) {
+	if (!OCTEON_IS_MODEL(OCTEON_CN68XX) && !OCTEON_IS_MODEL(OCTEON_CN70XX)) {
 		/*
 		 * Setup PKO to support 16 ports for HiGig2 virtual
 		 * ports. We're pointing all of the PKO packet ports
@@ -180,7 +192,7 @@ int __cvmx_helper_xaui_probe(int interface)
 		}
 	}
 
-	return __cvmx_helper_xaui_enumerate(interface);
+	return ports;
 }
 
 /**
@@ -512,3 +524,133 @@ extern int __cvmx_helper_xaui_configure_loopback(int ipd_port,
 	/* Take the link through a reset */
 	return __cvmx_helper_xaui_link_init(interface);
 }
+
+/**
+ * @INTERNAL
+ * Probe a XAUI interface and determine the number of ports
+ * connected to it. The XAUI interface should still be down
+ * after this call.
+ *
+ * @param interface Interface to probe
+ *
+ * @return Number of ports on the interface. Zero to disable.
+ */
+int __cvmx_helper_bgx_xaui_probe(int interface)
+{
+	int	qlm;
+
+	/*
+	 * Check the QLM is configured correctly for XAUI, verify the
+	 * speed as well as the mode.
+	 */
+	qlm = cvmx_qlm_interface(interface);
+	if (cvmx_qlm_get_mode(qlm) != CVMX_QLM_MODE_XAUI)
+		return 0;
+
+	return __cvmx_helper_sgmii_enumerate(interface);
+}
+
+/**
+ * @INTERNAL
+ * Bringup and enable a XAUI interface. After this call packet
+ * I/O should be fully functional. This is called with IPD
+ * enabled but PKO disabled.
+ *
+ * @param interface Interface to bring up
+ *
+ * @return Zero on success, negative on failure
+ */
+int __cvmx_helper_bgx_xaui_enable(int interface)
+{
+	cvmx_bgxx_cmrx_rx_id_map_t	bgx_cmr_rx_id_map;
+	int				num_ports;
+	int				pknd;
+	int				i;
+
+	num_ports = cvmx_helper_ports_on_interface(interface);
+
+	/* Configure the gser */
+	gser_init(interface, CVMX_HELPER_INTERFACE_MODE_XAUI);
+
+	/* Configure the bgx mac */
+	bgx_init(interface, CVMX_HELPER_INTERFACE_MODE_XAUI);
+
+	/*
+	 * Must hardcode the port kind here until the pko initializion is
+	 * complete. This must be removed once the pko initialization is
+	 * working. TODO
+	 */
+	pknd = 50 + (num_ports * interface);
+	for (i = 0; i < num_ports; i++) {
+		bgx_cmr_rx_id_map.u64 = 0;
+		bgx_cmr_rx_id_map.s.rid = 2 + i;
+		bgx_cmr_rx_id_map.s.pknd = pknd + i;
+		cvmx_write_csr(CVMX_BGXX_CMRX_RX_ID_MAP(i, interface),
+			       bgx_cmr_rx_id_map.u64);
+	}
+
+	return 0;
+}
+
+/**
+ * @INTERNAL
+ * Return the link state of an IPD/PKO port as returned by
+ * auto negotiation. The result of this function may not match
+ * Octeon's link config if auto negotiation has changed since
+ * the last call to cvmx_helper_link_set().
+ *
+ * @param ipd_port IPD/PKO port to query
+ *
+ * @return Link state
+ */
+cvmx_helper_link_info_t __cvmx_helper_bgx_xaui_link_get(int ipd_port)
+{
+	cvmx_helper_link_info_t result;
+
+	/* Hardcoded for now. TODO */
+	result.s.link_up = 1;
+	result.s.full_duplex = 1;
+	result.s.speed = 10000;
+
+	return result;
+}
+
+/**
+ * @INTERNAL
+ * Configure an IPD/PKO port for the specified link state. This
+ * function does not influence auto negotiation at the PHY level.
+ * The passed link state must always match the link state returned
+ * by cvmx_helper_link_get(). It is normally best to use
+ * cvmx_helper_link_autoconf() instead.
+ *
+ * @param ipd_port  IPD/PKO port to configure
+ * @param link_info The new link state
+ *
+ * @return Zero on success, negative on failure
+ */
+int __cvmx_helper_bgx_xaui_link_set(int				ipd_port,
+				    cvmx_helper_link_info_t	link_info)
+{
+	return 0;
+}
+
+/**
+ * @INTERNAL
+ * Configure a port for internal and/or external loopback. Internal loopback
+ * causes packets sent by the port to be received by Octeon. External loopback
+ * causes packets received from the wire to sent out again.
+ *
+ * @param ipd_port IPD/PKO port to loopback.
+ * @param enable_internal
+ *                 Non zero if you want internal loopback
+ * @param enable_external
+ *                 Non zero if you want external loopback
+ *
+ * @return Zero on success, negative on failure.
+ */
+extern int __cvmx_helper_bgx_xaui_configure_loopback(int ipd_port,
+						     int enable_internal,
+						     int enable_external)
+{
+	return 0;
+}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper.c b/arch/mips/cavium-octeon/executive/cvmx-helper.c
index d100d39..6afe4ef 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper.c
@@ -64,6 +64,7 @@
 #include <asm/octeon/cvmx-pip.h>
 #include <asm/octeon/cvmx-pko.h>
 #include <asm/octeon/cvmx-ipd.h>
+#include <asm/octeon/cvmx-qlm.h>
 #include <asm/octeon/cvmx-spi.h>
 #include <asm/octeon/cvmx-clock.h>
 #include <asm/octeon/cvmx-helper.h>
@@ -82,6 +83,7 @@
 #include "cvmx-pip.h"
 #include "cvmx-pko.h"
 #include "cvmx-ipd.h"
+#include "cvmx-qlm.h"
 #include "cvmx-spi.h"
 #include "cvmx-helper.h"
 #include "cvmx-helper-board.h"
@@ -90,6 +92,264 @@
 #endif
 
 
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by an interface.
+ *
+ * @param mode		Interface mode.
+ *
+ * @param enumerate	Method the get number of interface ports.
+ *
+ * @param probe		Method to probe an interface to get the number of
+ *			connected ports.
+ *
+ * @param enable	Method to enable an interface
+ *
+ * @param link_get	Method to get the state of an interface link.
+ *
+ * @param link_set	Method to configure an interface link to the specified
+ *			state.
+ *
+ * @param loopback	Method to configure a port in loopback.
+ */
+struct iface_ops_s {
+	cvmx_helper_interface_mode_t	mode;
+	int				(*enumerate)(int interface);
+	int				(*probe)(int interface);
+	int				(*enable)(int interface);
+	cvmx_helper_link_info_t		(*link_get)(int ipd_port);
+	int				(*link_set)(int ipd_port,
+					     cvmx_helper_link_info_t link_info);
+	int				(*loopback)(int ipd_port,
+						    int en_in, int en_ex);
+};
+
+/**
+ * @INTERNAL
+ * This structure is used by disabled interfaces.
+ */
+static const struct iface_ops_s iface_ops_dis = {
+	.mode		= CVMX_HELPER_INTERFACE_MODE_DISABLED,
+};
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as gmii.
+ */
+static const struct iface_ops_s iface_ops_gmii = {
+	.mode		= CVMX_HELPER_INTERFACE_MODE_GMII,
+	.enumerate	= __cvmx_helper_rgmii_enumerate,
+	.probe		= __cvmx_helper_rgmii_probe,
+	.enable		= __cvmx_helper_rgmii_enable,
+	.link_get	= __cvmx_helper_gmii_link_get,
+	.link_set	= __cvmx_helper_rgmii_link_set,
+	.loopback	= __cvmx_helper_rgmii_configure_loopback,
+};
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as rgmii.
+ */
+static const struct iface_ops_s iface_ops_rgmii = {
+	.mode		= CVMX_HELPER_INTERFACE_MODE_RGMII,
+	.enumerate	= __cvmx_helper_rgmii_enumerate,
+	.probe		= __cvmx_helper_rgmii_probe,
+	.enable		= __cvmx_helper_rgmii_enable,
+	.link_get	= __cvmx_helper_rgmii_link_get,
+	.link_set	= __cvmx_helper_rgmii_link_set,
+	.loopback	= __cvmx_helper_rgmii_configure_loopback,
+};
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as sgmii that use the gmx mac.
+ */
+static const struct iface_ops_s iface_ops_sgmii = {
+	.mode		= CVMX_HELPER_INTERFACE_MODE_SGMII,
+	.enumerate	= __cvmx_helper_sgmii_enumerate,
+	.probe		= __cvmx_helper_sgmii_probe,
+	.enable		= __cvmx_helper_sgmii_enable,
+	.link_get	= __cvmx_helper_sgmii_link_get,
+	.link_set	= __cvmx_helper_sgmii_link_set,
+	.loopback	= __cvmx_helper_sgmii_configure_loopback,
+};
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as sgmii that use the bgx mac.
+ */
+static const struct iface_ops_s iface_ops_bgx_sgmii = {
+	.mode		= CVMX_HELPER_INTERFACE_MODE_SGMII,
+	.enumerate	= __cvmx_helper_sgmii_enumerate,
+	.probe		= __cvmx_helper_bgx_sgmii_probe,
+	.enable		= __cvmx_helper_bgx_sgmii_enable,
+	.link_get	= __cvmx_helper_bgx_sgmii_link_get,
+	.link_set	= __cvmx_helper_bgx_sgmii_link_set,
+	.loopback	= __cvmx_helper_bgx_sgmii_configure_loopback,
+};
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as qsgmii.
+ */
+static const struct iface_ops_s iface_ops_qsgmii = {
+	.mode		= CVMX_HELPER_INTERFACE_MODE_QSGMII,
+	.enumerate	= __cvmx_helper_sgmii_enumerate,
+	.probe		= __cvmx_helper_sgmii_probe,
+	.enable		= __cvmx_helper_sgmii_enable,
+	.link_get	= __cvmx_helper_sgmii_link_get,
+	.link_set	= __cvmx_helper_sgmii_link_set,
+	.loopback	= __cvmx_helper_sgmii_configure_loopback,
+};
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as xaui using the gmx mac.
+ */
+static const struct iface_ops_s iface_ops_xaui = {
+	.mode		= CVMX_HELPER_INTERFACE_MODE_XAUI,
+	.enumerate	= __cvmx_helper_xaui_enumerate,
+	.probe		= __cvmx_helper_xaui_probe,
+	.enable		= __cvmx_helper_xaui_enable,
+	.link_get	= __cvmx_helper_xaui_link_get,
+	.link_set	= __cvmx_helper_xaui_link_set,
+	.loopback	= __cvmx_helper_xaui_configure_loopback,
+};
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as xaui using the bgx mac.
+ */
+static const struct iface_ops_s iface_ops_bgx_xaui = {
+	.mode		= CVMX_HELPER_INTERFACE_MODE_XAUI,
+	.enumerate	= __cvmx_helper_xaui_enumerate,
+	.probe		= __cvmx_helper_bgx_xaui_probe,
+	.enable		= __cvmx_helper_bgx_xaui_enable,
+	.link_get	= __cvmx_helper_bgx_xaui_link_get,
+	.link_set	= __cvmx_helper_bgx_xaui_link_set,
+	.loopback	= __cvmx_helper_bgx_xaui_configure_loopback,
+};
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as rxaui.
+ */
+static const struct iface_ops_s iface_ops_rxaui = {
+	.mode		= CVMX_HELPER_INTERFACE_MODE_RXAUI,
+	.enumerate	= __cvmx_helper_xaui_enumerate,
+	.probe		= __cvmx_helper_xaui_probe,
+	.enable		= __cvmx_helper_xaui_enable,
+	.link_get	= __cvmx_helper_xaui_link_get,
+	.link_set	= __cvmx_helper_xaui_link_set,
+	.loopback	= __cvmx_helper_xaui_configure_loopback,
+};
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as ilk.
+ */
+static const struct iface_ops_s iface_ops_ilk = {
+	.mode		= CVMX_HELPER_INTERFACE_MODE_ILK,
+	.enumerate	= __cvmx_helper_ilk_enumerate,
+	.probe		= __cvmx_helper_ilk_probe,
+	.enable		= __cvmx_helper_ilk_enable,
+	.link_get	= __cvmx_helper_ilk_link_get,
+	.link_set	= __cvmx_helper_ilk_link_set,
+};
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as npi.
+ */
+static const struct iface_ops_s iface_ops_npi = {
+	.mode		= CVMX_HELPER_INTERFACE_MODE_NPI,
+	.enumerate	= __cvmx_helper_npi_enumerate,
+	.probe		= __cvmx_helper_npi_probe,
+	.enable		= __cvmx_helper_npi_enable,
+};
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as srio.
+ */
+static const struct iface_ops_s iface_ops_srio = {
+	.mode		= CVMX_HELPER_INTERFACE_MODE_SRIO,
+	.enumerate	= __cvmx_helper_srio_enumerate,
+	.probe		= __cvmx_helper_srio_probe,
+	.enable		= __cvmx_helper_srio_enable,
+	.link_get	= __cvmx_helper_srio_link_get,
+	.link_set	= __cvmx_helper_srio_link_set,
+};
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as agl.
+ */
+static const struct iface_ops_s iface_ops_agl = {
+	.mode		= CVMX_HELPER_INTERFACE_MODE_AGL,
+	.enumerate	= __cvmx_helper_agl_enumerate,
+	.probe		= __cvmx_helper_agl_probe,
+	.enable		= __cvmx_helper_agl_enable,
+	.link_get	= __cvmx_helper_agl_link_get,
+	.link_set	= __cvmx_helper_agl_link_set,
+};
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as picmg.
+ */
+static const struct iface_ops_s iface_ops_picmg = {
+	.mode		= CVMX_HELPER_INTERFACE_MODE_PICMG,
+	.enumerate	= __cvmx_helper_sgmii_enumerate,
+	.probe		= __cvmx_helper_sgmii_probe,
+	.enable		= __cvmx_helper_sgmii_enable,
+	.link_get	= __cvmx_helper_sgmii_link_get,
+	.link_set	= __cvmx_helper_sgmii_link_set,
+	.loopback	= __cvmx_helper_sgmii_configure_loopback,
+};
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as spi.
+ */
+static const struct iface_ops_s iface_ops_spi = {
+	.mode		= CVMX_HELPER_INTERFACE_MODE_SPI,
+	.enumerate	= __cvmx_helper_spi_enumerate,
+	.probe		= __cvmx_helper_spi_probe,
+	.enable		= __cvmx_helper_spi_enable,
+	.link_get	= __cvmx_helper_spi_link_get,
+	.link_set	= __cvmx_helper_spi_link_set,
+};
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as loop.
+ */
+static const struct iface_ops_s iface_ops_loop = {
+	.mode		= CVMX_HELPER_INTERFACE_MODE_LOOP,
+	.enumerate	= __cvmx_helper_loop_enumerate,
+	.probe		= __cvmx_helper_loop_probe,
+};
+
+CVMX_SHARED const struct iface_ops_s *iface_ops[CVMX_HELPER_MAX_IFACE] = {
+	[0 ... CVMX_HELPER_MAX_IFACE - 1] = NULL
+};
+
 CVMX_SHARED uint64_t  cvmx_rgmii_backpressure_dis=1;
 
 typedef int (*cvmx_export_config_t)(void);
@@ -208,46 +468,35 @@ EXPORT_SYMBOL(cvmx_helper_ports_on_interface);
  */
 static cvmx_helper_interface_mode_t __cvmx_get_mode_cn70xx(int interface)
 {
-	union cvmx_gmxx_inf_mode inf_mode;
-	static int first_time = 1;
-
-	/* Hack for SGMII interface to work on simulator. Will remove it
-	   once DLM is configured properly.
-	 */
-	if (first_time) {
-		inf_mode.u64 = cvmx_read_csr(CVMX_GMXX_INF_MODE(interface));
-		inf_mode.s.mode = 1;
-		inf_mode.s.en = 0;
-		cvmx_write_csr(CVMX_GMXX_INF_MODE(interface), inf_mode.u64);
-		first_time = 0;
-	}
-
-	/* SGMII/XAUI/QSGMII */
+	/* SGMII/RXAUI/QSGMII */
 	if (interface < 2) {
-		inf_mode.u64 = cvmx_read_csr(CVMX_GMXX_INF_MODE(interface));
-
-		switch(inf_mode.s.mode) {
-		case 1:
-			return CVMX_HELPER_INTERFACE_MODE_SGMII;
-		case 2:
-			return CVMX_HELPER_INTERFACE_MODE_QSGMII;
-		case 3:
-			return CVMX_HELPER_INTERFACE_MODE_RXAUI;
-		default:
-			return CVMX_HELPER_INTERFACE_MODE_DISABLED;
-		}
+		enum cvmx_qlm_mode qlm_mode = cvmx_qlm_get_dlm_mode(0, interface);
+
+		if (qlm_mode == CVMX_QLM_MODE_SGMII)
+			iface_ops[interface] = &iface_ops_sgmii;
+		else if (qlm_mode == CVMX_QLM_MODE_QSGMII)
+			iface_ops[interface] = &iface_ops_qsgmii;
+		else if (qlm_mode == CVMX_QLM_MODE_RXAUI)
+			iface_ops[interface] = &iface_ops_rxaui;
+		else
+			iface_ops[interface] = &iface_ops_dis;
 	}
-	if (interface == 2) /* DPI */
-		return CVMX_HELPER_INTERFACE_MODE_NPI;
+	else if (interface == 2) /* DPI */
+		iface_ops[interface] = &iface_ops_npi;
 	else if (interface == 3) /* LOOP */
-		return CVMX_HELPER_INTERFACE_MODE_LOOP;
+		iface_ops[interface] = &iface_ops_loop;
 	else if (interface == 4) { /* RGMII (AGL) */
 		cvmx_agl_prtx_ctl_t prtx_ctl;
 		prtx_ctl.u64 = cvmx_read_csr(CVMX_AGL_PRTX_CTL(0));
 		if (prtx_ctl.s.mode == 0)
-			return CVMX_HELPER_INTERFACE_MODE_AGL;
+			iface_ops[interface] = &iface_ops_agl;
+		else 
+			iface_ops[interface] = &iface_ops_dis;
 	}
-	return CVMX_HELPER_INTERFACE_MODE_DISABLED;
+	else
+		iface_ops[interface] = &iface_ops_dis;
+
+	return iface_ops[interface]->mode;
 }
 
 /**
@@ -256,28 +505,51 @@ static cvmx_helper_interface_mode_t __cvmx_get_mode_cn70xx(int interface)
  */
 static cvmx_helper_interface_mode_t __cvmx_get_mode_cn78xx(int interface)
 {
+	/*
+	 * Until gser configuration is in place, we hard code the interface
+	 * mode here. This means that for the time being, this function and 
+	 * cvmx_qlm_get_mode() have to be in sync since they are both hard
+	 * coded.
+	 */
 	switch (interface) {
 	case 0:
 	case 1:
+		iface_ops[interface] = &iface_ops_bgx_sgmii;
+		break;
+
 	case 2:
+		/*
+		 * Interface 2's qlm is used by ilk so it can never be
+		 * connected to a bgx mac. Disable it for now.
+		 */
+		iface_ops[interface] = &iface_ops_dis;
+		break;
+
 	case 3:
 	case 4:
 	case 5:
-		return CVMX_HELPER_INTERFACE_MODE_DISABLED;
+		iface_ops[interface] = &iface_ops_bgx_xaui;
+		break;
 
 	case 6:
 	case 7:
-		return CVMX_HELPER_INTERFACE_MODE_ILK;
+		iface_ops[interface] = &iface_ops_ilk;
+		break;
 
 	case 8:
-		return CVMX_HELPER_INTERFACE_MODE_DISABLED;
+		iface_ops[interface] = &iface_ops_dis;
+		break;
 
 	case 9:
-		return CVMX_HELPER_INTERFACE_MODE_LOOP;
+		iface_ops[interface] = &iface_ops_loop;
+		break;
 
 	default:
-		return CVMX_HELPER_INTERFACE_MODE_DISABLED;
+		iface_ops[interface] = &iface_ops_dis;
+		break;
 	}
+
+	return iface_ops[interface]->mode;
 }
 
 /**
@@ -287,72 +559,87 @@ static cvmx_helper_interface_mode_t __cvmx_get_mode_cn78xx(int interface)
 static cvmx_helper_interface_mode_t __cvmx_get_mode_cn68xx(int interface)
 {
 	union cvmx_mio_qlmx_cfg qlm_cfg;
+
 	switch (interface) {
 	case 0:
 		qlm_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(0));
 		/* QLM is disabled when QLM SPD is 15. */
 		if (qlm_cfg.s.qlm_spd == 15)
-			return CVMX_HELPER_INTERFACE_MODE_DISABLED;
-
-		if (qlm_cfg.s.qlm_cfg == 7)
-			return CVMX_HELPER_INTERFACE_MODE_RXAUI;
+			iface_ops[interface] = &iface_ops_dis;
+		else if (qlm_cfg.s.qlm_cfg == 7)
+			iface_ops[interface] = &iface_ops_rxaui;
 		else if (qlm_cfg.s.qlm_cfg == 2)
-			return CVMX_HELPER_INTERFACE_MODE_SGMII;
+			iface_ops[interface] = &iface_ops_sgmii;
 		else if (qlm_cfg.s.qlm_cfg == 3)
-			return CVMX_HELPER_INTERFACE_MODE_XAUI;
+			iface_ops[interface] = &iface_ops_xaui;
 		else
-			return CVMX_HELPER_INTERFACE_MODE_DISABLED;
+			iface_ops[interface] = &iface_ops_dis;
+		break;
+
 	case 1:
 		qlm_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(0));
 		/* QLM is disabled when QLM SPD is 15. */
 		if (qlm_cfg.s.qlm_spd == 15)
-			return CVMX_HELPER_INTERFACE_MODE_DISABLED;
-
-		if (qlm_cfg.s.qlm_cfg == 7)
-			return CVMX_HELPER_INTERFACE_MODE_RXAUI;
+			iface_ops[interface] = &iface_ops_dis;
+		else if (qlm_cfg.s.qlm_cfg == 7)
+			iface_ops[interface] = &iface_ops_rxaui;
 		else
-			return CVMX_HELPER_INTERFACE_MODE_DISABLED;
+			iface_ops[interface] = &iface_ops_dis;
+		break;
+
 	case 2:
 	case 3:
 	case 4:
 		qlm_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(interface));
 		/* QLM is disabled when QLM SPD is 15. */
 		if (qlm_cfg.s.qlm_spd == 15)
-			return CVMX_HELPER_INTERFACE_MODE_DISABLED;
-
-		if (qlm_cfg.s.qlm_cfg == 2)
-			return CVMX_HELPER_INTERFACE_MODE_SGMII;
+			iface_ops[interface] = &iface_ops_dis;
+		else if (qlm_cfg.s.qlm_cfg == 2)
+			iface_ops[interface] = &iface_ops_sgmii;
 		else if (qlm_cfg.s.qlm_cfg == 3)
-			return CVMX_HELPER_INTERFACE_MODE_XAUI;
+			iface_ops[interface] = &iface_ops_xaui;
 		else
-			return CVMX_HELPER_INTERFACE_MODE_DISABLED;
+			iface_ops[interface] = &iface_ops_dis;
+		break;
+
 	case 5:
 	case 6:
 		qlm_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(interface - 4));
 		/* QLM is disabled when QLM SPD is 15. */
 		if (qlm_cfg.s.qlm_spd == 15)
-			return CVMX_HELPER_INTERFACE_MODE_DISABLED;
-
-		if (qlm_cfg.s.qlm_cfg == 1)
-			return CVMX_HELPER_INTERFACE_MODE_ILK;
+			iface_ops[interface] = &iface_ops_dis;
+		else if (qlm_cfg.s.qlm_cfg == 1)
+			iface_ops[interface] = &iface_ops_ilk;
 		else
-			return CVMX_HELPER_INTERFACE_MODE_DISABLED;
+			iface_ops[interface] = &iface_ops_dis;
+		break;
+
 	case 7:
 		qlm_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(3));
 		/* QLM is disabled when QLM SPD is 15. */
 		if (qlm_cfg.s.qlm_spd == 15) {
-			return CVMX_HELPER_INTERFACE_MODE_DISABLED;
+			iface_ops[interface] = &iface_ops_dis;
 		} else if (qlm_cfg.s.qlm_cfg != 0) {
 			qlm_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(1));
 			if (qlm_cfg.s.qlm_cfg != 0)
-				return CVMX_HELPER_INTERFACE_MODE_DISABLED;
+				iface_ops[interface] = &iface_ops_dis;
+			else
+				iface_ops[interface] = &iface_ops_npi;
 		}
-		return CVMX_HELPER_INTERFACE_MODE_NPI;
+		else
+			iface_ops[interface] = &iface_ops_npi;
+		break;
+
 	case 8:
-		return CVMX_HELPER_INTERFACE_MODE_LOOP;
+		iface_ops[interface] = &iface_ops_loop;
+		break;
+
 	default:
-		return CVMX_HELPER_INTERFACE_MODE_DISABLED;
+		iface_ops[interface] = &iface_ops_dis;
+		break;
 	}
+
+	return iface_ops[interface]->mode;
 }
 
 /**
@@ -367,37 +654,36 @@ static cvmx_helper_interface_mode_t __cvmx_get_mode_octeon2(int interface)
 		return __cvmx_get_mode_cn68xx(interface);
 
 	if (interface == 2)
-		return CVMX_HELPER_INTERFACE_MODE_NPI;
-
-	if (interface == 3)
-		return CVMX_HELPER_INTERFACE_MODE_LOOP;
-
-	/* Only present in CN63XX & CN66XX Octeon model */
-	if ((OCTEON_IS_MODEL(OCTEON_CN63XX) &&
-	     (interface == 4 || interface == 5)) ||
-	    (OCTEON_IS_MODEL(OCTEON_CN66XX) &&
-	     interface >= 4 && interface <= 7)) {
+		iface_ops[interface] = &iface_ops_npi;
+	else if (interface == 3)
+		iface_ops[interface] = &iface_ops_loop;
+	else if ((OCTEON_IS_MODEL(OCTEON_CN63XX) &&
+		  (interface == 4 || interface == 5)) ||
+		 (OCTEON_IS_MODEL(OCTEON_CN66XX) &&
+		  interface >= 4 && interface <= 7)) {
+		/* Only present in CN63XX & CN66XX Octeon model */
 		union cvmx_sriox_status_reg sriox_status_reg;
 
 		/* cn66xx pass1.0 has only 2 SRIO interfaces. */
 		if ((interface == 5 || interface == 7) &&
 		    OCTEON_IS_MODEL(OCTEON_CN66XX_PASS1_0))
-			return CVMX_HELPER_INTERFACE_MODE_DISABLED;
-		/*
-		 * Later passes of cn66xx support SRIO0 - x4/x2/x1,
-		 * SRIO2 - x2/x1, SRIO3 - x1
-		 */
+			iface_ops[interface] = &iface_ops_dis;
 		else if (interface == 5 && OCTEON_IS_MODEL(OCTEON_CN66XX))
-			return CVMX_HELPER_INTERFACE_MODE_DISABLED;
-
-		sriox_status_reg.u64 = cvmx_read_csr(CVMX_SRIOX_STATUS_REG(interface - 4));
-		if (sriox_status_reg.s.srio)
-			return CVMX_HELPER_INTERFACE_MODE_SRIO;
-		else
-			return CVMX_HELPER_INTERFACE_MODE_DISABLED;
+			/*
+			 * Later passes of cn66xx support SRIO0 - x4/x2/x1,
+			 * SRIO2 - x2/x1, SRIO3 - x1
+			 */
+			iface_ops[interface] = &iface_ops_dis;
+		else {
+			sriox_status_reg.u64 = 
+				cvmx_read_csr(CVMX_SRIOX_STATUS_REG(interface - 4));
+			if (sriox_status_reg.s.srio)
+				iface_ops[interface] = &iface_ops_srio;
+			else
+				iface_ops[interface] = &iface_ops_dis;
+		}
 	}
-
-	if (OCTEON_IS_MODEL(OCTEON_CN66XX)) {
+	else if (OCTEON_IS_MODEL(OCTEON_CN66XX)) {
 		union cvmx_mio_qlmx_cfg mio_qlm_cfg;
 
 		/* QLM2 is SGMII0 and QLM1 is SGMII1 */
@@ -405,71 +691,81 @@ static cvmx_helper_interface_mode_t __cvmx_get_mode_octeon2(int interface)
 			mio_qlm_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(2));
 		else if (interface == 1)
 			mio_qlm_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(1));
-		else
-			return CVMX_HELPER_INTERFACE_MODE_DISABLED;
+		else {
+			iface_ops[interface] = &iface_ops_dis;
+			return iface_ops[interface]->mode;
+		}
 
 		if (mio_qlm_cfg.s.qlm_spd == 15)
-			return CVMX_HELPER_INTERFACE_MODE_DISABLED;
-
-		if (mio_qlm_cfg.s.qlm_cfg == 9)
-			return CVMX_HELPER_INTERFACE_MODE_SGMII;
+			iface_ops[interface] = &iface_ops_dis;
+		else if (mio_qlm_cfg.s.qlm_cfg == 9)
+			iface_ops[interface] = &iface_ops_sgmii;
 		else if (mio_qlm_cfg.s.qlm_cfg == 11)
-			return CVMX_HELPER_INTERFACE_MODE_XAUI;
+			iface_ops[interface] = &iface_ops_xaui;
 		else
-			return CVMX_HELPER_INTERFACE_MODE_DISABLED;
+			iface_ops[interface] = &iface_ops_dis;
 	} else if (OCTEON_IS_MODEL(OCTEON_CN61XX)) {
 		union cvmx_mio_qlmx_cfg qlm_cfg;
 
-		if (interface == 0) {
+		if (interface == 0)
 			qlm_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(2));
-			if (qlm_cfg.s.qlm_cfg == 2)
-				return CVMX_HELPER_INTERFACE_MODE_SGMII;
-			else if (qlm_cfg.s.qlm_cfg == 3)
-				return CVMX_HELPER_INTERFACE_MODE_XAUI;
-			else
-				return CVMX_HELPER_INTERFACE_MODE_DISABLED;
-		} else if (interface == 1) {
+		else if (interface == 1)
 			qlm_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(0));
-			if (qlm_cfg.s.qlm_cfg == 2)
-				return CVMX_HELPER_INTERFACE_MODE_SGMII;
-			else if (qlm_cfg.s.qlm_cfg == 3)
-				return CVMX_HELPER_INTERFACE_MODE_XAUI;
-			else
-				return CVMX_HELPER_INTERFACE_MODE_DISABLED;
+		else {
+			iface_ops[interface] = &iface_ops_dis;
+			return iface_ops[interface]->mode;
 		}
+
+		if (qlm_cfg.s.qlm_spd == 15)
+			iface_ops[interface] = &iface_ops_dis;
+		else if (qlm_cfg.s.qlm_cfg == 2)
+			iface_ops[interface] = &iface_ops_sgmii;
+		else if (qlm_cfg.s.qlm_cfg == 3)
+			iface_ops[interface] = &iface_ops_xaui;
+		else
+			iface_ops[interface] = &iface_ops_dis;
 	} else if (OCTEON_IS_MODEL(OCTEON_CNF71XX)) {
 		if (interface == 0) {
 			union cvmx_mio_qlmx_cfg qlm_cfg;
 			qlm_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(0));
 			if (qlm_cfg.s.qlm_cfg == 2)
-				return CVMX_HELPER_INTERFACE_MODE_SGMII;
+				iface_ops[interface] = &iface_ops_sgmii;
+			else
+				iface_ops[interface] = &iface_ops_dis;
 		}
-		return CVMX_HELPER_INTERFACE_MODE_DISABLED;
+		else
+			iface_ops[interface] = &iface_ops_dis;
 	}
+	else if (interface == 1 && OCTEON_IS_MODEL(OCTEON_CN63XX))
+		iface_ops[interface] = &iface_ops_dis;
+	else {
+		mode.u64 = cvmx_read_csr(CVMX_GMXX_INF_MODE(interface));
 
-	if (interface == 1 && OCTEON_IS_MODEL(OCTEON_CN63XX))
-		return CVMX_HELPER_INTERFACE_MODE_DISABLED;
+		if (OCTEON_IS_MODEL(OCTEON_CN63XX)) {
+			switch (mode.cn63xx.mode) {
+			case 0:
+				iface_ops[interface] = &iface_ops_sgmii;
+				break;
 
-	mode.u64 = cvmx_read_csr(CVMX_GMXX_INF_MODE(interface));
+			case 1:
+				iface_ops[interface] = &iface_ops_xaui;
+				break;
 
-	if (OCTEON_IS_MODEL(OCTEON_CN63XX)) {
-		switch (mode.cn63xx.mode) {
-		case 0:
-			return CVMX_HELPER_INTERFACE_MODE_SGMII;
-		case 1:
-			return CVMX_HELPER_INTERFACE_MODE_XAUI;
-		default:
-			return CVMX_HELPER_INTERFACE_MODE_DISABLED;
+			default:
+				iface_ops[interface] = &iface_ops_dis;
+				break;
+			}
+		} else {
+			if (!mode.s.en)
+				iface_ops[interface] = &iface_ops_dis;
+			else if (mode.s.type)
+				iface_ops[interface] = &iface_ops_gmii;
+			else
+				iface_ops[interface] = &iface_ops_rgmii;
 		}
-	} else {
-		if (!mode.s.en)
-			return CVMX_HELPER_INTERFACE_MODE_DISABLED;
-
-		if (mode.s.type)
-			return CVMX_HELPER_INTERFACE_MODE_GMII;
-		else
-			return CVMX_HELPER_INTERFACE_MODE_RGMII;
 	}
+
+	return iface_ops[interface]->mode;
 }
 
 /**
@@ -491,6 +787,14 @@ cvmx_helper_interface_mode_t cvmx_helper_interface_get_mode(int interface)
 		return CVMX_HELPER_INTERFACE_MODE_DISABLED;
 
 	/*
+	 * Check if the interface mode has been already cached. If it has,
+	 * simply return it. Otherwise, fall through the rest of the code to
+	 * determine the interface mode and cache it in iface_ops.
+	 */
+	if (iface_ops[interface] != NULL)
+		return iface_ops[interface]->mode;
+
+	/*
 	 * OCTEON III models
 	 */
 	if (OCTEON_IS_MODEL(OCTEON_CN70XX))
@@ -509,17 +813,15 @@ cvmx_helper_interface_mode_t cvmx_helper_interface_get_mode(int interface)
 	 * Octeon and Octeon Plus models
 	 */
 	if (interface == 2)
-		return CVMX_HELPER_INTERFACE_MODE_NPI;
-
-	if (interface == 3) {
+		iface_ops[interface] = &iface_ops_npi;
+	else if (interface == 3) {
 		if (OCTEON_IS_MODEL(OCTEON_CN56XX)
 		    || OCTEON_IS_MODEL(OCTEON_CN52XX))
-			return CVMX_HELPER_INTERFACE_MODE_LOOP;
+			iface_ops[interface] = &iface_ops_loop;
 		else
-			return CVMX_HELPER_INTERFACE_MODE_DISABLED;
+			iface_ops[interface] = &iface_ops_dis;
 	}
-
-	if (interface == 0 &&
+	else if (interface == 0 &&
 	    cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_CN3005_EVB_HS5 &&
 	    cvmx_sysinfo_get()->board_rev_major == 1) {
 		/*
@@ -532,45 +834,56 @@ cvmx_helper_interface_mode_t cvmx_helper_interface_get_mode(int interface)
 		 * output of this function) there is no difference in
 		 * setup between GMII and RGMII modes.
 		 */
-		return CVMX_HELPER_INTERFACE_MODE_GMII;
+		iface_ops[interface] = &iface_ops_gmii;
 	}
-
-	/* Interface 1 is always disabled on CN31XX and CN30XX */
-	if ((interface == 1)
+	else if ((interface == 1)
 	    && (OCTEON_IS_MODEL(OCTEON_CN31XX)
 		|| OCTEON_IS_MODEL(OCTEON_CN30XX)
 		|| OCTEON_IS_MODEL(OCTEON_CN50XX)
 		|| OCTEON_IS_MODEL(OCTEON_CN52XX)))
-		return CVMX_HELPER_INTERFACE_MODE_DISABLED;
+		/* Interface 1 is always disabled on CN31XX and CN30XX */
+		iface_ops[interface] = &iface_ops_dis;
+	else {
+		mode.u64 = cvmx_read_csr(CVMX_GMXX_INF_MODE(interface));
 
-	mode.u64 = cvmx_read_csr(CVMX_GMXX_INF_MODE(interface));
-
-	if (OCTEON_IS_MODEL(OCTEON_CN56XX) || OCTEON_IS_MODEL(OCTEON_CN52XX)) {
-		switch (mode.cn56xx.mode) {
-		case 0:
-			return CVMX_HELPER_INTERFACE_MODE_DISABLED;
-		case 1:
-			return CVMX_HELPER_INTERFACE_MODE_XAUI;
-		case 2:
-			return CVMX_HELPER_INTERFACE_MODE_SGMII;
-		case 3:
-			return CVMX_HELPER_INTERFACE_MODE_PICMG;
-		default:
-			return CVMX_HELPER_INTERFACE_MODE_DISABLED;
-		}
-	} else {
-		if (!mode.s.en)
-			return CVMX_HELPER_INTERFACE_MODE_DISABLED;
+		if (OCTEON_IS_MODEL(OCTEON_CN56XX) ||
+		    OCTEON_IS_MODEL(OCTEON_CN52XX)) {
+			switch (mode.cn56xx.mode) {
+			case 0:
+				iface_ops[interface] = &iface_ops_dis;
+				break;
 
-		if (mode.s.type) {
-			if (OCTEON_IS_MODEL(OCTEON_CN38XX) ||
-			    OCTEON_IS_MODEL(OCTEON_CN58XX))
-				return CVMX_HELPER_INTERFACE_MODE_SPI;
-			else
-				return CVMX_HELPER_INTERFACE_MODE_GMII;
-		} else
-			return CVMX_HELPER_INTERFACE_MODE_RGMII;
+			case 1:
+				iface_ops[interface] = &iface_ops_xaui;
+				break;
+
+			case 2:
+				iface_ops[interface] = &iface_ops_sgmii;
+				break;
+
+			case 3:
+				iface_ops[interface] = &iface_ops_picmg;
+				break;
+
+			default:
+				iface_ops[interface] = &iface_ops_dis;
+				break;
+			}
+		} else {
+			if (!mode.s.en)
+				iface_ops[interface] = &iface_ops_dis;
+			else if (mode.s.type) {
+				if (OCTEON_IS_MODEL(OCTEON_CN38XX) ||
+				    OCTEON_IS_MODEL(OCTEON_CN58XX))
+					iface_ops[interface] = &iface_ops_spi;
+				else
+					iface_ops[interface] = &iface_ops_gmii;
+			} else
+				iface_ops[interface] = &iface_ops_rgmii;
+		}
 	}
+
+	return iface_ops[interface]->mode;
 }
 EXPORT_SYMBOL(cvmx_helper_interface_get_mode);
 
@@ -698,56 +1011,13 @@ static int cvmx_helper_fcs_op(int interface, int nports, int has_fcs)
  */
 int cvmx_helper_interface_enumerate(int interface)
 {
-	switch (cvmx_helper_interface_get_mode(interface)) {
-		/* XAUI is a single high speed port */
-	case CVMX_HELPER_INTERFACE_MODE_XAUI:
-	case CVMX_HELPER_INTERFACE_MODE_RXAUI:
-		return __cvmx_helper_xaui_enumerate(interface);
-		/*
-		 * RGMII/GMII/MII are all treated about the same. Most
-		 * functions refer to these ports as RGMII
-		 */
-	case CVMX_HELPER_INTERFACE_MODE_RGMII:
-	case CVMX_HELPER_INTERFACE_MODE_GMII:
-		return __cvmx_helper_rgmii_enumerate(interface);
-		/*
-		 * SPI4 can have 1-16 ports depending on the device at
-		 * the other end.
-		 */
-	case CVMX_HELPER_INTERFACE_MODE_SPI:
-		return __cvmx_helper_spi_enumerate(interface);
-		/*
-		 * SGMII can have 1-4 ports depending on how many are
-		 * hooked up.
-		 */
-	case CVMX_HELPER_INTERFACE_MODE_SGMII:
-	case CVMX_HELPER_INTERFACE_MODE_QSGMII:
-	case CVMX_HELPER_INTERFACE_MODE_PICMG:
-		return __cvmx_helper_sgmii_enumerate(interface);
-		/* PCI target Network Packet Interface */
-	case CVMX_HELPER_INTERFACE_MODE_NPI:
-		return __cvmx_helper_npi_enumerate(interface);
-		/*
-		 * Special loopback only ports. These are not the same
-		 * as other ports in loopback mode.
-		 */
-	case CVMX_HELPER_INTERFACE_MODE_LOOP:
-		return __cvmx_helper_loop_enumerate(interface);
-		/* SRIO has 2^N ports, where N is number of interfaces */
-	case CVMX_HELPER_INTERFACE_MODE_SRIO:
-		return __cvmx_helper_srio_enumerate(interface);
+	int	result = 0;
 
-	case CVMX_HELPER_INTERFACE_MODE_ILK:
-		return __cvmx_helper_ilk_enumerate(interface);
+	cvmx_helper_interface_get_mode(interface);
+	if (iface_ops[interface]->enumerate)
+		result = iface_ops[interface]->enumerate(interface);
 
-	case CVMX_HELPER_INTERFACE_MODE_AGL:
-		return __cvmx_helper_agl_enumerate(interface);
-		/* These types don't support ports to IPD/PKO */
-	case CVMX_HELPER_INTERFACE_MODE_DISABLED:
-	case CVMX_HELPER_INTERFACE_MODE_PCIE:
-	default:
-		return 0;
-	}
+	return result;
 }
 EXPORT_SYMBOL(cvmx_helper_interface_enumerate);
 
@@ -776,7 +1046,12 @@ int cvmx_helper_interface_probe(int interface)
 
 	nports = -1;
 	has_fcs = 0;
-	switch (cvmx_helper_interface_get_mode(interface)) {
+
+	cvmx_helper_interface_get_mode(interface);
+	if (iface_ops[interface]->probe)
+		nports = iface_ops[interface]->probe(interface);
+
+	switch (iface_ops[interface]->mode) {
 		/* These types don't support ports to IPD/PKO */
 	case CVMX_HELPER_INTERFACE_MODE_DISABLED:
 	case CVMX_HELPER_INTERFACE_MODE_PCIE:
@@ -785,7 +1060,6 @@ int cvmx_helper_interface_probe(int interface)
 		/* XAUI is a single high speed port */
 	case CVMX_HELPER_INTERFACE_MODE_XAUI:
 	case CVMX_HELPER_INTERFACE_MODE_RXAUI:
-		nports = __cvmx_helper_xaui_probe(interface);
 		has_fcs = 1;
 		padding = CVMX_PKO_PADDING_60;
 		break;
@@ -795,7 +1069,6 @@ int cvmx_helper_interface_probe(int interface)
 		 */
 	case CVMX_HELPER_INTERFACE_MODE_RGMII:
 	case CVMX_HELPER_INTERFACE_MODE_GMII:
-		nports = __cvmx_helper_rgmii_probe(interface);
 		padding = CVMX_PKO_PADDING_60;
 		break;
 		/*
@@ -803,7 +1076,6 @@ int cvmx_helper_interface_probe(int interface)
 		 * the other end.
 		 */
 	case CVMX_HELPER_INTERFACE_MODE_SPI:
-		nports = __cvmx_helper_spi_probe(interface);
 		padding = CVMX_PKO_PADDING_60;
 		break;
 		/*
@@ -814,31 +1086,25 @@ int cvmx_helper_interface_probe(int interface)
 	case CVMX_HELPER_INTERFACE_MODE_QSGMII:
 		padding = CVMX_PKO_PADDING_60;
 	case CVMX_HELPER_INTERFACE_MODE_PICMG:
-		nports = __cvmx_helper_sgmii_probe(interface);
 		has_fcs = 1;
 		break;
 		/* PCI target Network Packet Interface */
 	case CVMX_HELPER_INTERFACE_MODE_NPI:
-		nports = __cvmx_helper_npi_probe(interface);
 		break;
 		/*
 		 * Special loopback only ports. These are not the same
 		 * as other ports in loopback mode.
 		 */
 	case CVMX_HELPER_INTERFACE_MODE_LOOP:
-		nports = __cvmx_helper_loop_probe(interface);
 		break;
 		/* SRIO has 2^N ports, where N is number of interfaces */
 	case CVMX_HELPER_INTERFACE_MODE_SRIO:
-		nports = __cvmx_helper_srio_probe(interface);
 		break;
 	case CVMX_HELPER_INTERFACE_MODE_ILK:
-		nports = __cvmx_helper_ilk_probe(interface);
 		padding = CVMX_PKO_PADDING_60;
 		has_fcs = 1;
 		break;
 	case CVMX_HELPER_INTERFACE_MODE_AGL:
-		nports = __cvmx_helper_agl_probe(interface);
 		has_fcs = 1;
 		break;
 	}
@@ -1210,57 +1476,10 @@ int __cvmx_helper_backpressure_is_misaligned(void)
 static int __cvmx_helper_packet_hardware_enable(int interface)
 {
 	int result = 0;
-	switch (cvmx_helper_interface_get_mode(interface)) {
-		/* These types don't support ports to IPD/PKO */
-	case CVMX_HELPER_INTERFACE_MODE_DISABLED:
-	case CVMX_HELPER_INTERFACE_MODE_PCIE:
-	case CVMX_HELPER_INTERFACE_MODE_LOOP:
-		/* Nothing to do */
-		break;
-		/* XAUI is a single high speed port */
-	case CVMX_HELPER_INTERFACE_MODE_XAUI:
-	case CVMX_HELPER_INTERFACE_MODE_RXAUI:
-		result = __cvmx_helper_xaui_enable(interface);
-		break;
-		/*
-		 * RGMII/GMII/MII are all treated about the same. Most
-		 * functions refer to these ports as RGMII.
-		 */
-	case CVMX_HELPER_INTERFACE_MODE_RGMII:
-	case CVMX_HELPER_INTERFACE_MODE_GMII:
-		result = __cvmx_helper_rgmii_enable(interface);
-		break;
-		/*
-		 * SPI4 can have 1-16 ports depending on the device at
-		 * the other end.
-		 */
-	case CVMX_HELPER_INTERFACE_MODE_SPI:
-		result = __cvmx_helper_spi_enable(interface);
-		break;
-		/*
-		 * SGMII can have 1-4 ports depending on how many are
-		 * hooked up.
-		 */
-	case CVMX_HELPER_INTERFACE_MODE_SGMII:
-	case CVMX_HELPER_INTERFACE_MODE_QSGMII:
-	case CVMX_HELPER_INTERFACE_MODE_PICMG:
-		result = __cvmx_helper_sgmii_enable(interface);
-		break;
-		/* PCI target Network Packet Interface */
-	case CVMX_HELPER_INTERFACE_MODE_NPI:
-		result = __cvmx_helper_npi_enable(interface);
-		break;
-		/* SRIO has 2^N ports, where N is number of interfaces */
-	case CVMX_HELPER_INTERFACE_MODE_SRIO:
-		result = __cvmx_helper_srio_enable(interface);
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_ILK:
-		result = __cvmx_helper_ilk_enable(interface);
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_AGL:
-		result = __cvmx_helper_agl_enable(interface);
-		break;
-	}
+
+	cvmx_helper_interface_get_mode(interface);
+	if (iface_ops[interface]->enable)
+		result = iface_ops[interface]->enable(interface);
 	result |= __cvmx_helper_board_hardware_enable(interface);
 	return result;
 }
@@ -1455,7 +1674,6 @@ int cvmx_helper_uninitialize_sso(void)
 	cvmx_sso_rwq_psh_fptr_t fptr;
 	cvmx_sso_fpage_cnt_t fpage_cnt;
 	int num_to_transfer, i;
-	char *mem;
 
 	if (!OCTEON_IS_MODEL(OCTEON_CN68XX))
 		return 0;
@@ -1501,7 +1719,7 @@ int cvmx_helper_uninitialize_sso(void)
 			cvmx_dprintf("head_ptr.s.ptr != tail_ptr.s.ptr, idx: %d\n", i);
 		}
 
-		mem = cvmx_phys_to_ptr(((uint64_t) head_ptr.s.ptr) << 7);
+		cvmx_phys_to_ptr(((uint64_t) head_ptr.s.ptr) << 7);
 		/* Leak the memory */
 	}
 
@@ -1509,7 +1727,7 @@ int cvmx_helper_uninitialize_sso(void)
 		do {
 			pop_fptr.u64 = cvmx_read_csr(CVMX_SSO_RWQ_POP_FPTR);
 			if (pop_fptr.s.val) {
-				mem = cvmx_phys_to_ptr(((uint64_t) pop_fptr.s.fptr) << 7);
+				cvmx_phys_to_ptr(((uint64_t) pop_fptr.s.fptr) << 7);
 				/* Leak the memory */
 			}
 		} while (pop_fptr.s.val);
@@ -2211,49 +2429,10 @@ cvmx_helper_link_info_t cvmx_helper_link_get(int ipd_port)
 	    index >= cvmx_helper_ports_on_interface(interface))
 		return result;
 
-	switch (cvmx_helper_interface_get_mode(interface)) {
-	case CVMX_HELPER_INTERFACE_MODE_DISABLED:
-	case CVMX_HELPER_INTERFACE_MODE_PCIE:
-		/* Network links are not supported */
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_XAUI:
-	case CVMX_HELPER_INTERFACE_MODE_RXAUI:
-		result = __cvmx_helper_xaui_link_get(ipd_port);
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_GMII:
-		if (index == 0)
-			result = __cvmx_helper_rgmii_link_get(ipd_port);
-		else {
-			result.s.full_duplex = 1;
-			result.s.link_up = 1;
-			result.s.speed = 1000;
-		}
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_RGMII:
-		result = __cvmx_helper_rgmii_link_get(ipd_port);
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_SPI:
-		result = __cvmx_helper_spi_link_get(ipd_port);
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_SGMII:
-	case CVMX_HELPER_INTERFACE_MODE_QSGMII:
-	case CVMX_HELPER_INTERFACE_MODE_PICMG:
-		result = __cvmx_helper_sgmii_link_get(ipd_port);
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_SRIO:
-		result = __cvmx_helper_srio_link_get(ipd_port);
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_ILK:
-		result = __cvmx_helper_ilk_link_get(ipd_port);
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_AGL:
-		result = __cvmx_helper_agl_link_get(ipd_port);
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_NPI:
-	case CVMX_HELPER_INTERFACE_MODE_LOOP:
-		/* Network links are not supported */
-		break;
-	}
+	cvmx_helper_interface_get_mode(interface);
+	if (iface_ops[interface]->link_get)
+		result = iface_ops[interface]->link_get(ipd_port);
+
 	return result;
 }
 EXPORT_SYMBOL(cvmx_helper_link_get);
@@ -2280,43 +2459,10 @@ int cvmx_helper_link_set(int ipd_port, cvmx_helper_link_info_t link_info)
 	    index >= cvmx_helper_ports_on_interface(interface))
 		return -1;
 
-	switch (cvmx_helper_interface_get_mode(interface)) {
-	case CVMX_HELPER_INTERFACE_MODE_DISABLED:
-	case CVMX_HELPER_INTERFACE_MODE_PCIE:
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_XAUI:
-	case CVMX_HELPER_INTERFACE_MODE_RXAUI:
-		result = __cvmx_helper_xaui_link_set(ipd_port, link_info);
-		break;
-		/*
-		 * RGMII/GMII/MII are all treated about the same. Most
-		 * functions refer to these ports as RGMII.
-		 */
-	case CVMX_HELPER_INTERFACE_MODE_RGMII:
-	case CVMX_HELPER_INTERFACE_MODE_GMII:
-		result = __cvmx_helper_rgmii_link_set(ipd_port, link_info);
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_SPI:
-		result = __cvmx_helper_spi_link_set(ipd_port, link_info);
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_SGMII:
-	case CVMX_HELPER_INTERFACE_MODE_QSGMII:
-	case CVMX_HELPER_INTERFACE_MODE_PICMG:
-		result = __cvmx_helper_sgmii_link_set(ipd_port, link_info);
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_SRIO:
-		result = __cvmx_helper_srio_link_set(ipd_port, link_info);
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_ILK:
-		result = __cvmx_helper_ilk_link_set(ipd_port, link_info);
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_AGL:
-		result = __cvmx_helper_agl_link_set(ipd_port, link_info);
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_NPI:
-	case CVMX_HELPER_INTERFACE_MODE_LOOP:
-		break;
-	}
+	cvmx_helper_interface_get_mode(interface);
+	if (iface_ops[interface]->link_set)
+		result = iface_ops[interface]->link_set(ipd_port, link_info);
+
 	/*
 	 * Set the port_link_info here so that the link status is
 	 * updated no matter how cvmx_helper_link_set is called. We
@@ -2351,36 +2497,12 @@ int cvmx_helper_configure_loopback(int ipd_port, int enable_internal,
 	if (index >= cvmx_helper_ports_on_interface(interface))
 		return -1;
 
-	switch (cvmx_helper_interface_get_mode(interface)) {
-	case CVMX_HELPER_INTERFACE_MODE_DISABLED:
-	case CVMX_HELPER_INTERFACE_MODE_PCIE:
-	case CVMX_HELPER_INTERFACE_MODE_SRIO:
-	case CVMX_HELPER_INTERFACE_MODE_ILK:
-	case CVMX_HELPER_INTERFACE_MODE_SPI:
-	case CVMX_HELPER_INTERFACE_MODE_NPI:
-	case CVMX_HELPER_INTERFACE_MODE_LOOP:
-	case CVMX_HELPER_INTERFACE_MODE_AGL:
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_XAUI:
-	case CVMX_HELPER_INTERFACE_MODE_RXAUI:
-		result = __cvmx_helper_xaui_configure_loopback(ipd_port,
-							       enable_internal,
-							       enable_external);
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_RGMII:
-	case CVMX_HELPER_INTERFACE_MODE_GMII:
-		result = __cvmx_helper_rgmii_configure_loopback(ipd_port,
-								enable_internal,
-								enable_external);
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_SGMII:
-	case CVMX_HELPER_INTERFACE_MODE_QSGMII:
-	case CVMX_HELPER_INTERFACE_MODE_PICMG:
-		result = __cvmx_helper_sgmii_configure_loopback(ipd_port,
-								enable_internal,
-								enable_external);
-		break;
-	}
+	cvmx_helper_interface_get_mode(interface);
+	if (iface_ops[interface]->loopback)
+		result = iface_ops[interface]->loopback(ipd_port,
+							enable_internal,
+							enable_external);
+
 	return result;
 }
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-ilk.c b/arch/mips/cavium-octeon/executive/cvmx-ilk.c
index ae85e7c..e924068 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-ilk.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-ilk.c
@@ -1219,7 +1219,7 @@ int cvmx_ilk_disable(int interface)
 #endif
 
 	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX)) &&
-	    !(OCTEON_IS_MODEL(OCTEON_CN68XX)))
+	    !(OCTEON_IS_MODEL(OCTEON_CN78XX)))
 		return res;
 
 	if (interface >= CVMX_NUM_ILK_INTF)
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pcie.c b/arch/mips/cavium-octeon/executive/cvmx-pcie.c
index fa2672d..0b97e40 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pcie.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pcie.c
@@ -42,7 +42,7 @@
  *
  * Interface to PCIe as a host(RC) or target(EP)
  *
- * <hr>$Revision: 82360 $<hr>
+ * <hr>$Revision: 87128 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
@@ -1202,7 +1202,7 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 	/* Allow config retries for 250ms. Count is based off the 5Ghz SERDES
 	   clock */
 	pemx_ctl_status.u64 = cvmx_read_csr(CVMX_PEMX_CTL_STATUS(pcie_port));
-	pemx_ctl_status.s.cfg_rtry = 250 * 5000000 / 0x10000;
+	pemx_ctl_status.cn63xx.cfg_rtry = 250 * 5000000 / 0x10000;
 	cvmx_write_csr(CVMX_PEMX_CTL_STATUS(pcie_port), pemx_ctl_status.u64);
 
 	/* Display the link status */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pki.c b/arch/mips/cavium-octeon/executive/cvmx-pki.c
index d91be0e..bda29bd 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pki.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pki.c
@@ -57,8 +57,8 @@
 #include "cvmx-pki-defs.h"
 #include "cvmx-pki.h"
 #include "cvmx-fpa.h"
-#include "cvmx-pki-cluster.h"
 #include "cvmx-pki-resources.h"
+#include "cvmx-pki-cluster.h"
 #endif
 
 CVMX_SHARED struct cvmx_pki_config pki_config[CVMX_MAX_NODES];
@@ -599,11 +599,11 @@ int cvmx_pki_find_aura(int node, char *name)
  */
 int cvmx_pki_group_profile_exist(int node, char *name)
 {
-	int index = pki_profiles[node].group_profile_list.index;
+	int index = pki_profiles[node].sso_grp_profile_list.index;
 
 	while(index--)
 	{
-		if(strcmp(name,pki_profiles[node].group_profile_list.group_profile[index].group_name) == 0)
+		if(strcmp(name,pki_profiles[node].sso_grp_profile_list.grp_profile[index].grp_name) == 0)
 			return index;
 	}
 	return -1;
@@ -623,7 +623,7 @@ int cvmx_pki_find_group(int node, char *name)
 
 	if((index = cvmx_pki_group_profile_exist(node,name)) == -1)
 		return -1;
-	return pki_profiles[node].group_profile_list.group_profile[index].group_num;
+	return pki_profiles[node].sso_grp_profile_list.grp_profile[index].grp_num;
 }
 
 /**
@@ -914,22 +914,20 @@ int cvmx_pki_set_aura_config(int node, char* aura_name, int aura_num, int pool,
  * This function stores the group configuration in data structure
  * which is then used to program the hardware.
  * @param node  	node number
- * @param aura_name  	name associated with this config
- * @param group		SSO group number (-1 if needs to be allocated)
+ * @param grp_profile	struct to SSO group profile to configure
  * @return 		0 on SUCCESS
                         -1 on failure
  */
-int cvmx_pki_set_group_config(int node, char *name, int group)
+int cvmx_pki_set_sso_group_config(int node, struct cvmx_pki_sso_grp_profile grp_profile)
 {
 	uint64_t index;
-	struct cvmx_pki_group_profile* group_profile;
 
 	if(node >= CVMX_MAX_NODES) {
 		cvmx_dprintf("Invalid node number %d",node);
 		return -1;
 	}
-	if(cvmx_pki_group_profile_exist(node, name) >= 0) {
-		cvmx_dprintf("ERROR:group profile already exist with name %s",name);
+	if(cvmx_pki_group_profile_exist(node, grp_profile.grp_name) >= 0) {
+		cvmx_dprintf("ERROR:group profile already exist with name %s",grp_profile.grp_name);
 		return -1;
 	}
 #if 0 //vinita_to_do uncomment when group_alloc is ready
@@ -940,18 +938,16 @@ int cvmx_pki_set_group_config(int node, char *name, int group)
 #endif
 
 	//spinlock it
-	index = pki_profiles[node].group_profile_list.index;
-	if(index >= CVMX_PKI_MAX_GROUP_PROFILES) {
+	index = pki_profiles[node].sso_grp_profile_list.index;
+	if(index >= CVMX_PKI_MAX_SSO_GROUP_PROFILES) {
 		cvmx_dprintf("ERROR: Max group profile %d reached\n", (int)index);
 		return -1;
 
 	}
-	pki_profiles[node].group_profile_list.index++;
+	pki_profiles[node].sso_grp_profile_list.index++;
 	//spinlock free
 
-	group_profile = &pki_profiles[node].group_profile_list.group_profile[index];
-	strcpy(group_profile->group_name, name);
-	group_profile->group_num = group;
+	pki_profiles[node].sso_grp_profile_list.grp_profile[index] = grp_profile;
 	return 0;
 
 }
@@ -1245,4 +1241,51 @@ void cvmx_pki_show_valid_pcam_entries(int node)
 	}
 }
 
+/**
+ * This function shows the pkind attributes in readable format,
+ * read directly from hardware.
+ * @param node    node number
+ */
+void cvmx_pki_show_pkind_attributes(int node, int pkind)
+{
+	int cluster=0;
+	int index;
+	cvmx_pki_pkindx_icgsel_t pkind_clsel;
+	cvmx_pki_clx_pkindx_style_t pkind_cfg_style;
+	cvmx_pki_icgx_cfg_t pki_cl_grp;
+	cvmx_pki_clx_stylex_cfg_t style_cfg;
+	cvmx_pki_clx_stylex_alg_t style_alg;
+
+	if(pkind >= CVMX_PKI_NUM_PKIND) {
+		cvmx_dprintf("ERROR: PKIND %d is beyond range\n", pkind);
+		return;
+	}
+	pkind_clsel.u64 = cvmx_read_csr_node(node, CVMX_PKI_PKINDX_ICGSEL(pkind));
+	cvmx_dprintf("cluster group:	%d\n", pkind_clsel.s.icg);
+	pki_cl_grp.u64 = cvmx_read_csr_node(node, CVMX_PKI_ICGX_CFG(pkind_clsel.s.icg));
+	cvmx_dprintf("cluster mask of the group:	0x%x\n",pki_cl_grp.s.clusters);
 
+	while( cluster < CVMX_PKI_NUM_CLUSTERS) {
+		if(pki_cl_grp.s.clusters & (0x01L << cluster)) {
+			//vinita_to_do later modify in human readble format or now just print register value
+			pkind_cfg_style.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_STYLE(pkind, cluster));
+			cvmx_dprintf("initial parse Mode: %d\n",pkind_cfg_style.s.pm);
+			cvmx_dprintf("initial_style: %d\n", pkind_cfg_style.s.style);
+			style_alg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_ALG(pkind_cfg_style.s.style, cluster));
+			cvmx_dprintf("style_alg: 0x%llx\n", (unsigned long long)style_alg.u64);
+			style_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(pkind_cfg_style.s.style, cluster));
+			cvmx_dprintf("style_cfg: 0x%llx\n", (unsigned long long)style_cfg.u64);
+			cvmx_dprintf("style_cfg2: 0x%llx\n",
+				     (unsigned long long)cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG2(pkind_cfg_style.s.style, cluster)));
+			cvmx_dprintf("style_buf: 0x%llx\n",
+				     (unsigned long long)cvmx_read_csr_node(node, CVMX_PKI_STYLEX_BUF(pkind_cfg_style.s.style)));
+			break;
+		}
+	}
+	cvmx_dprintf("qpg base: %d\n",style_cfg.s.qpg_base);
+	cvmx_dprintf("qpg qos: %d\n",style_alg.s.qpg_qos);
+	for(index=0; index < 8; index++) {
+		cvmx_dprintf("qpg index %d: 0x%llx\n", (index+style_cfg.s.qpg_base),
+			     (unsigned long long)cvmx_read_csr_node(node, CVMX_PKI_QPG_TBLX(style_cfg.s.qpg_base+index)));
+	}
+}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-qlm.c b/arch/mips/cavium-octeon/executive/cvmx-qlm.c
index 85e80a1..73dfc94 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-qlm.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-qlm.c
@@ -1,5 +1,5 @@
 /***********************license start***************
- * Copyright (c) 2011  Cavium Inc. (support@cavium.com). All rights
+ * Copyright (c) 2011-2013  Cavium Inc. (support@cavium.com). All rights
  * reserved.
  *
  *
@@ -42,7 +42,7 @@
  *
  * Helper utilities for qlm.
  *
- * <hr>$Revision: 84617 $<hr>
+ * <hr>$Revision: 87438 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
@@ -51,6 +51,7 @@
 #include <asm/octeon/cvmx-qlm.h>
 #include <asm/octeon/cvmx-clock.h>
 #include <asm/octeon/cvmx-gmxx-defs.h>
+#include <asm/octeon/cvmx-gserx-defs.h>
 #include <asm/octeon/cvmx-sriox-defs.h>
 #include <asm/octeon/cvmx-sriomaintx-defs.h>
 #include <asm/octeon/cvmx-pciercx-defs.h>
@@ -133,6 +134,20 @@ int cvmx_qlm_interface(int interface)
 			return 0;
 		else
 			cvmx_dprintf("Warning: cvmx_qlm_interface: Invalid interface %d\n", interface);
+	} else if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+		switch (interface) {
+		case 0:
+			return MUX_78XX_IFACE0 ? 2 : 0;
+		case 1:
+			return MUX_78XX_IFACE1 ? 3 : 1;
+		case 2:
+		case 3:
+		case 4:
+		case 5:
+			return interface + 2;
+		default:
+			break;
+		}
 	} else {
 		/* Must be cn68XX */
 		switch (interface) {
@@ -678,14 +693,131 @@ int cvmx_qlm_get_gbaud_mhz(int qlm)
 		default:
 			return 0;	/* Disabled */
 		}
+	} else if (OCTEON_IS_MODEL(OCTEON_CN70XX)) {
+		cvmx_gserx_dlmx_mpll_status_t mpll_status;
+		cvmx_gserx_dlmx_ref_clkdiv2_t ref_clkdiv2;
+		cvmx_gserx_dlmx_mpll_multiplier_t mpll_multiplier;
+		uint64_t meas_refclock;
+		uint64_t mhz;
+
+		/* Return zero if the PLL hasn't locked */
+		mpll_status.u64 = cvmx_read_csr(CVMX_GSERX_DLMX_MPLL_STATUS(qlm, 0));
+#ifdef CVMX_BUILD_FOR_LINUX_HOST
+		if (mpll_status.s.mpll_status == 0)
+			return 0;
+#else
+		if ((cvmx_sysinfo_get()->board_type != CVMX_BOARD_TYPE_SIM)
+	    	    && (mpll_status.s.mpll_status == 0))
+			return 0;
+#endif
+		meas_refclock = cvmx_qlm_measure_clock(qlm);
+		mhz = meas_refclock / 1000000;
+		/* Measure the reference clock */
+		/* Divide it by two if the DLM is configure that way */
+		ref_clkdiv2.u64 = cvmx_read_csr(CVMX_GSERX_DLMX_REF_CLKDIV2(qlm, 0));
+		if (ref_clkdiv2.s.ref_clkdiv2)
+			mhz /= 2;
+		/* Multiply to get the final frequency */
+		mpll_multiplier.u64 = cvmx_read_csr(CVMX_GSERX_DLMX_MPLL_MULTIPLIER(qlm, 0));
+		mhz *= mpll_multiplier.s.mpll_multiplier;
+		return mhz;
 	}
 	return 0;
 }
 
-/*
- * Read QLM and return mode.
- */
-enum cvmx_qlm_mode cvmx_qlm_get_mode(int qlm)
+static enum cvmx_qlm_mode __cvmx_qlm_get_mode_cn70xx(int qlm)
+{
+#ifndef CVMX_BUILD_FOR_LINUX_HOST
+	if (cvmx_sysinfo_get()->board_type != CVMX_BOARD_TYPE_SIM) {
+		union cvmx_gserx_dlmx_phy_reset phy_reset;
+
+		phy_reset.u64 = cvmx_read_csr(CVMX_GSERX_DLMX_PHY_RESET(0, qlm));
+		if (phy_reset.s.phy_reset)
+			return CVMX_QLM_MODE_DISABLED;
+
+	}
+#endif
+
+	switch(qlm) {
+	case 0: /* DLM0/DLM1 - SGMII/QSGMII/RXAUI */
+		{
+			union cvmx_gmxx_inf_mode inf_mode0, inf_mode1;
+
+			inf_mode0.u64 = cvmx_read_csr(CVMX_GMXX_INF_MODE(0));
+			inf_mode1.u64 = cvmx_read_csr(CVMX_GMXX_INF_MODE(1));
+
+			/* SGMII0 SGMII1 */
+			switch (inf_mode0.s.mode) {
+			case CVMX_GMX_INF_MODE_SGMII:
+				switch (inf_mode1.s.mode) {
+				case CVMX_GMX_INF_MODE_SGMII:
+					return CVMX_QLM_MODE_SGMII_SGMII;
+				case CVMX_GMX_INF_MODE_QSGMII:
+					return CVMX_QLM_MODE_SGMII_QSGMII;
+				default:
+					return CVMX_QLM_MODE_SGMII_DISABLED;
+				}
+			case CVMX_GMX_INF_MODE_QSGMII:
+				switch (inf_mode1.s.mode) {
+				case CVMX_GMX_INF_MODE_SGMII:
+					return CVMX_QLM_MODE_QSGMII_SGMII;
+				case CVMX_GMX_INF_MODE_QSGMII:
+					return CVMX_QLM_MODE_QSGMII_QSGMII;
+				default:
+					return CVMX_QLM_MODE_QSGMII_DISABLED;
+				}
+			case CVMX_GMX_INF_MODE_RXAUI:
+				return CVMX_QLM_MODE_RXAUI_1X2;
+			default:
+				return CVMX_QLM_MODE_DISABLED;
+			}
+		}
+	default:
+		return CVMX_QLM_MODE_DISABLED;
+	}
+
+	return CVMX_QLM_MODE_DISABLED;
+}
+
+enum cvmx_qlm_mode cvmx_qlm_get_dlm_mode(int qlm, int interface)
+{
+	enum cvmx_qlm_mode qlm_mode = __cvmx_qlm_get_mode_cn70xx(qlm);
+
+	switch (interface) {
+	case 0:
+		switch (qlm_mode) {
+		case CVMX_QLM_MODE_SGMII_SGMII:
+		case CVMX_QLM_MODE_SGMII_DISABLED:
+		case CVMX_QLM_MODE_SGMII_QSGMII:
+			return CVMX_QLM_MODE_SGMII;
+		case CVMX_QLM_MODE_QSGMII_QSGMII:
+		case CVMX_QLM_MODE_QSGMII_DISABLED:
+		case CVMX_QLM_MODE_QSGMII_SGMII:
+			return CVMX_QLM_MODE_QSGMII;
+		case CVMX_QLM_MODE_RXAUI_1X2:
+			return CVMX_QLM_MODE_RXAUI;
+		default:
+			return CVMX_QLM_MODE_DISABLED;
+		}
+	case 1:
+		switch (qlm_mode) {
+		case CVMX_QLM_MODE_SGMII_SGMII:
+		case CVMX_QLM_MODE_DISABLED_SGMII:
+		case CVMX_QLM_MODE_QSGMII_SGMII:
+			return CVMX_QLM_MODE_SGMII;
+		case CVMX_QLM_MODE_QSGMII_QSGMII:
+		case CVMX_QLM_MODE_DISABLED_QSGMII:
+		case CVMX_QLM_MODE_SGMII_QSGMII:
+			return CVMX_QLM_MODE_QSGMII;
+		default:
+			return CVMX_QLM_MODE_DISABLED;
+		}
+	default:
+		return CVMX_QLM_MODE_DISABLED;
+	}
+}
+
+static enum cvmx_qlm_mode __cvmx_qlm_get_mode_cn6xxx(int qlm)
 {
 	cvmx_mio_qlmx_cfg_t qlmx_cfg;
 
@@ -822,6 +954,44 @@ enum cvmx_qlm_mode cvmx_qlm_get_mode(int qlm)
 	return CVMX_QLM_MODE_DISABLED;
 }
 
+static enum cvmx_qlm_mode __cvmx_qlm_get_mode_cn78xx(int qlm)
+{
+	/*
+	 * Until gser configuration is in place, we hard code the
+	 * qlm mode here. This means that for the time being, this
+	 * function and __cvmx_get_mode_cn78xx() have to be in sync
+	 * since they are both hard coded. Note that register 
+	 * CVMX_MIO_QLMX_CFG is not yet modeled by the simulator.
+	 */
+	switch(qlm) {
+	case 0:
+	case 1:
+		return CVMX_QLM_MODE_SGMII;
+	case 4:
+		return CVMX_QLM_MODE_ILK;
+	case 5:
+	case 6:
+	case 7:
+		return CVMX_QLM_MODE_XAUI;
+	}
+	return CVMX_QLM_MODE_DISABLED;
+}
+
+/*
+ * Read QLM and return mode.
+ */
+enum cvmx_qlm_mode cvmx_qlm_get_mode(int qlm)
+{
+	if (OCTEON_IS_OCTEON2())
+		return __cvmx_qlm_get_mode_cn6xxx(qlm);
+	else if (OCTEON_IS_MODEL(OCTEON_CN70XX))
+		return __cvmx_qlm_get_mode_cn70xx(qlm);
+	else if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+		return __cvmx_qlm_get_mode_cn78xx(qlm);
+
+	return CVMX_QLM_MODE_DISABLED;
+}
+
 /**
  * Measure the reference clock of a QLM
  *
@@ -829,7 +999,7 @@ enum cvmx_qlm_mode cvmx_qlm_get_mode(int qlm)
  *
  * @return Clock rate in Hz
  *       */
-static int __cvmx_qlm_measure_clock(int qlm)
+int cvmx_qlm_measure_clock(int qlm)
 {
 	cvmx_mio_ptp_clock_cfg_t ptp_clock;
 	uint64_t count;
@@ -844,7 +1014,6 @@ static int __cvmx_qlm_measure_clock(int qlm)
 	if (cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_SIM)
 		return 156250000;
 #endif
-
 	/* Disable the PTP event counter while we configure it */
 	ptp_clock.u64 = cvmx_read_csr(CVMX_MIO_PTP_CLOCK_CFG);	/* For CN63XXp1 errata */
 	ptp_clock.s.evcnt_en = 0;
@@ -881,271 +1050,6 @@ static int __cvmx_qlm_measure_clock(int qlm)
 	return count * cvmx_clock_get_rate(CVMX_CLOCK_CORE) / (stop_cycle - start_cycle);
 }
 
-static int __cvmx_qlm_is_ref_clock(int qlm, int reference_mhz)
-{
-	int ref_clock = __cvmx_qlm_measure_clock(qlm);
-	int mhz = ref_clock / 1000000;
-	int range = reference_mhz / 10;
-	return ((mhz >= reference_mhz - range) && (mhz <= reference_mhz + range));
-}
-
-static int __cvmx_qlm_get_qlm_spd(int qlm, int speed)
-{
-	int qlm_spd = 0xf;
-
-	if (OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN5XXX))
-		return -1;
-
-	if (__cvmx_qlm_is_ref_clock(qlm, 100)) {
-		if (speed == 1250)
-			qlm_spd = 0x3;
-		else if (speed == 2500)
-			qlm_spd = 0x2;
-		else if (speed == 5000)
-			qlm_spd = 0x0;
-		else {
-			//cvmx_dprintf("Invalide speed(%d) for QLM(%d)\n", speed, qlm);
-			qlm_spd = 0xf;
-		}
-	} else if (__cvmx_qlm_is_ref_clock(qlm, 125)) {
-		if (speed == 1250)
-			qlm_spd = 0xa;
-		else if (speed == 2500)
-			qlm_spd = 0x9;
-		else if (speed == 3125)
-			qlm_spd = 0x8;
-		else if (speed == 5000)
-			qlm_spd = 0x6;
-		else if (speed == 6250)
-			qlm_spd = 0x5;
-		else {
-			//cvmx_dprintf("Invalide speed(%d) for QLM(%d)\n", speed, qlm);
-			qlm_spd = 0xf;
-		}
-	} else if (__cvmx_qlm_is_ref_clock(qlm, 156)) {
-		if (speed == 1250)
-			qlm_spd = 0x4;
-		else if (speed == 2500)
-			qlm_spd = 0x7;
-		else if (speed == 3125)
-			qlm_spd = 0xe;
-		else if (speed == 3750)
-			qlm_spd = 0xd;
-		else if (speed == 5000)
-			qlm_spd = 0xb;
-		else if (speed == 6250)
-			qlm_spd = 0xc;
-		else {
-			//cvmx_dprintf("Invalide speed(%d) for QLM(%d)\n", speed, qlm);
-			qlm_spd = 0xf;
-		}
-	}
-	return qlm_spd;
-}
-
-static void __cvmx_qlm_set_qlm_pcie_mode(int pcie_port, int root_complex)
-{
-	int rc = root_complex ? 1 : 0;
-	int ep = root_complex ? 0 : 1;
-	cvmx_ciu_soft_prst1_t soft_prst1;
-	cvmx_ciu_soft_prst_t soft_prst;
-	cvmx_mio_rst_ctlx_t rst_ctl;
-
-	if (pcie_port) {
-		soft_prst1.u64 = cvmx_read_csr(CVMX_CIU_SOFT_PRST1);
-		soft_prst1.s.soft_prst = 1;
-		cvmx_write_csr(CVMX_CIU_SOFT_PRST1, soft_prst1.u64);
-	} else {
-		soft_prst.u64 = cvmx_read_csr(CVMX_CIU_SOFT_PRST);
-		soft_prst.s.soft_prst = 1;
-		cvmx_write_csr(CVMX_CIU_SOFT_PRST, soft_prst.u64);
-	}
-
-	rst_ctl.u64 = cvmx_read_csr(CVMX_MIO_RST_CTLX(pcie_port));
-
-	rst_ctl.s.prst_link = rc;
-	rst_ctl.s.rst_link = ep;
-	rst_ctl.s.prtmode = rc;
-	rst_ctl.s.rst_drv = rc;
-	rst_ctl.s.rst_rcv = 0;
-	rst_ctl.s.rst_chip = ep;
-	cvmx_write_csr(CVMX_MIO_RST_CTLX(pcie_port), rst_ctl.u64);
-
-	if (root_complex == 0) {
-		if (pcie_port) {
-			soft_prst1.u64 = cvmx_read_csr(CVMX_CIU_SOFT_PRST1);
-			soft_prst1.s.soft_prst = 0;
-			cvmx_write_csr(CVMX_CIU_SOFT_PRST1, soft_prst1.u64);
-		} else {
-			soft_prst.u64 = cvmx_read_csr(CVMX_CIU_SOFT_PRST);
-			soft_prst.s.soft_prst = 0;
-			cvmx_write_csr(CVMX_CIU_SOFT_PRST, soft_prst.u64);
-		}
-	}
-}
-
-/**
- * Configure qlm speed and mode. MIO_QLMX_CFG[speed,mode] are not set
- * for CN61XX.
- *
- * @param qlm     The QLM to configure
- * @param speed   The speed the QLM needs to be configured in Mhz.
- * @param mode    The QLM to be configured as SGMII/XAUI/PCIe.
- *                  QLM 0: 0 = PCIe0 1X4, 1 = Reserved, 2 = SGMII1, 3 = XAUI1
- *                  QLM 1: 0 = PCIe1 1x2, 1 = PCIe(0/1) 2x1, 2 - 3 = Reserved
- *                  QLM 2: 0 - 1 = Reserved, 2 = SGMII0, 3 = XAUI0
- * @param rc      Only used for PCIe, rc = 1 for root complex mode, 0 for EP mode.
- * @param pcie2x1 Only used when QLM1 is in PCIE2x1 mode. The QLM_SPD has different
- *                value on how PEMx needs to be configured:
- *                   0x0 - both PEM0 & PEM1 are in gen1 mode.
- *                   0x1 - PEM0 in gen2 and PEM1 in gen1 mode.
- *                   0x2 - PEM0 in gen1 and PEM1 in gen2 mode.
- *                   0x3 - both PEM0 & PEM1 are in gen2 mode.
- *               SPEED value is ignored in this mode. QLM_SPD is set based on
- *               pcie2x1 value in this mode.
- *
- * @return       Return 0 on success or -1.
- */
-int cvmx_qlm_configure_qlm(int qlm, int speed, int mode, int rc, int pcie2x1)
-{
-	cvmx_mio_qlmx_cfg_t qlm_cfg;
-
-	/* The QLM speed varies for SGMII/XAUI and PCIe mode. And depends on
-	   reference clock. */
-	if (!OCTEON_IS_MODEL(OCTEON_CN61XX))
-		return -1;
-
-	if (qlm < 3)
-		qlm_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(qlm));
-	else {
-		cvmx_dprintf("WARNING: Invalid QLM(%d) passed\n", qlm);
-		return -1;
-	}
-
-	switch (qlm) {
-		/* SGMII/XAUI mode */
-	case 2:
-		{
-			if (mode < 2) {
-				//cvmx_dprintf("Invalide mode(%d) for QLM(%d)\n", mode, qlm);
-				qlm_cfg.s.qlm_spd = 0xf;
-				break;
-			}
-			qlm_cfg.s.qlm_spd = __cvmx_qlm_get_qlm_spd(qlm, speed);
-			qlm_cfg.s.qlm_cfg = mode;
-			break;
-		}
-	case 1:
-		{
-			if (mode == 1) {	/* 2x1 mode */
-				cvmx_mio_qlmx_cfg_t qlm0;
-
-				/* When QLM0 is configured as PCIe(QLM_CFG=0x0) and enabled
-				   (QLM_SPD != 0xf), QLM1 cannot be configured as PCIe 2x1 mode
-				   (QLM_CFG=0x1) and enabled (QLM_SPD != 0xf). */
-				qlm0.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(0));
-				if (qlm0.s.qlm_spd != 0xf && qlm0.s.qlm_cfg == 0) {
-					cvmx_dprintf("Invalid mode(%d) for QLM(%d) as QLM1 is PCIe mode\n", mode, qlm);
-					qlm_cfg.s.qlm_spd = 0xf;
-					break;
-				}
-
-				/* Set QLM_SPD based on reference clock and mode */
-				if (__cvmx_qlm_is_ref_clock(qlm, 100)) {
-					if (pcie2x1 == 0x3)
-						qlm_cfg.s.qlm_spd = 0x0;
-					else if (pcie2x1 == 0x1)
-						qlm_cfg.s.qlm_spd = 0x2;
-					else if (pcie2x1 == 0x2)
-						qlm_cfg.s.qlm_spd = 0x1;
-					else if (pcie2x1 == 0x0)
-						qlm_cfg.s.qlm_spd = 0x3;
-					else
-						qlm_cfg.s.qlm_spd = 0xf;
-				} else if (__cvmx_qlm_is_ref_clock(qlm, 125)) {
-					if (pcie2x1 == 0x3)
-						qlm_cfg.s.qlm_spd = 0x4;
-					else if (pcie2x1 == 0x1)
-						qlm_cfg.s.qlm_spd = 0x6;
-					else if (pcie2x1 == 0x2)
-						qlm_cfg.s.qlm_spd = 0x9;
-					else if (pcie2x1 == 0x0)
-						qlm_cfg.s.qlm_spd = 0x7;
-					else
-						qlm_cfg.s.qlm_spd = 0xf;
-				}
-				qlm_cfg.s.qlm_cfg = mode;
-				cvmx_write_csr(CVMX_MIO_QLMX_CFG(qlm), qlm_cfg.u64);
-
-				/* Set PCIe mode bits */
-				__cvmx_qlm_set_qlm_pcie_mode(0, rc);
-				__cvmx_qlm_set_qlm_pcie_mode(1, rc);
-				return 0;
-			} else if (mode > 1) {
-				cvmx_dprintf("Invalid mode(%d) for QLM(%d).\n", mode, qlm);
-				qlm_cfg.s.qlm_spd = 0xf;
-				break;
-			}
-
-			/* Set speed and mode for PCIe 1x2 mode. */
-			if (__cvmx_qlm_is_ref_clock(qlm, 100)) {
-				if (speed == 5000)
-					qlm_cfg.s.qlm_spd = 0x1;
-				else if (speed == 2500)
-					qlm_cfg.s.qlm_spd = 0x2;
-				else
-					qlm_cfg.s.qlm_spd = 0xf;
-			} else if (__cvmx_qlm_is_ref_clock(qlm, 125)) {
-				if (speed == 5000)
-					qlm_cfg.s.qlm_spd = 0x4;
-				else if (speed == 2500)
-					qlm_cfg.s.qlm_spd = 0x6;
-				else
-					qlm_cfg.s.qlm_spd = 0xf;
-			} else
-				qlm_cfg.s.qlm_spd = 0xf;
-
-			qlm_cfg.s.qlm_cfg = mode;
-			cvmx_write_csr(CVMX_MIO_QLMX_CFG(qlm), qlm_cfg.u64);
-
-			/* Set PCIe mode bits */
-			__cvmx_qlm_set_qlm_pcie_mode(1, rc);
-			return 0;
-		}
-	case 0:
-		{
-			/* QLM_CFG = 0x1 - Reserved */
-			if (mode == 1) {
-				//cvmx_dprintf("Invalid mode(%d) for QLM(%d)\n", mode, qlm);
-				qlm_cfg.s.qlm_spd = 0xf;
-				break;
-			}
-			/* QLM_CFG = 0x0 - PCIe 1x4(PEM0) */
-			if (mode == 0 && speed != 5000 && speed != 2500) {
-				//cvmx_dprintf("Invalid speed(%d) for QLM(%d) for PCIe mode.\n", speed, qlm);
-				qlm_cfg.s.qlm_spd = 0xf;
-				break;
-			}
-
-			/* Set speed and mode */
-			qlm_cfg.s.qlm_spd = __cvmx_qlm_get_qlm_spd(qlm, speed);
-			qlm_cfg.s.qlm_cfg = mode;
-			cvmx_write_csr(CVMX_MIO_QLMX_CFG(qlm), qlm_cfg.u64);
-
-			/* Set PCIe mode bits */
-			if (mode == 0)
-				__cvmx_qlm_set_qlm_pcie_mode(0, rc);
-
-			return 0;
-		}
-	default:
-		cvmx_dprintf("WARNING: Invalid QLM(%d) passed\n", qlm);
-		qlm_cfg.s.qlm_spd = 0xf;
-	}
-	cvmx_write_csr(CVMX_MIO_QLMX_CFG(qlm), qlm_cfg.u64);
-	return 0;
-}
-
 void cvmx_qlm_display_registers(int qlm)
 {
 	int num_lanes = cvmx_qlm_get_lanes(qlm);
diff --git a/arch/mips/cavium-octeon/executive/cvmx-usb.c b/arch/mips/cavium-octeon/executive/cvmx-usb.c
index 45dd000..79558be 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-usb.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-usb.c
@@ -283,7 +283,8 @@ typedef struct {
     } while (0)
 
 /* Returns the IO address to push/pop stuff data from the FIFOs */
-#define USB_FIFO_ADDRESS(channel, usb_index) (CVMX_USBCX_GOTGCTL(usb_index) + ((channel)+1)*0x1000)
+#define USB_FIFO_ADDRESS(channel, usb_index)	\
+	(CVMX_USBCX_GOTGCTL(usb_index) + ((channel)+1)*0x1000)
 
 /**
  * @INTERNAL
@@ -296,7 +297,8 @@ typedef struct {
  *
  * @return Result of the read
  */
-static inline uint32_t __cvmx_usb_read_csr32(cvmx_usb_internal_state_t * usb, uint64_t address)
+static inline uint32_t __cvmx_usb_read_csr32(cvmx_usb_internal_state_t * usb,
+					     uint64_t address)
 {
 	uint32_t result = cvmx_read64_uint32(address ^ 4);
 #if ALLOW_CSR_DECODES
@@ -318,7 +320,8 @@ static inline uint32_t __cvmx_usb_read_csr32(cvmx_usb_internal_state_t * usb, ui
  * @param address 64bit address to write
  * @param value   Value to write
  */
-static inline void __cvmx_usb_write_csr32(cvmx_usb_internal_state_t * usb, uint64_t address, uint32_t value)
+static inline void __cvmx_usb_write_csr32(cvmx_usb_internal_state_t * usb,
+					  uint64_t address, uint32_t value)
 {
 #if ALLOW_CSR_DECODES
 	if (cvmx_unlikely(usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_CSRS)) {
@@ -340,7 +343,8 @@ static inline void __cvmx_usb_write_csr32(cvmx_usb_internal_state_t * usb, uint6
  *
  * @return Result of the read
  */
-static inline uint64_t __cvmx_usb_read_csr64(cvmx_usb_internal_state_t * usb, uint64_t address)
+static inline uint64_t __cvmx_usb_read_csr64(cvmx_usb_internal_state_t * usb,
+					     uint64_t address)
 {
 	uint64_t result = cvmx_read64_uint64(address);
 #if ALLOW_CSR_DECODES
@@ -361,7 +365,8 @@ static inline uint64_t __cvmx_usb_read_csr64(cvmx_usb_internal_state_t * usb, ui
  * @param address 64bit address to write
  * @param value   Value to write
  */
-static inline void __cvmx_usb_write_csr64(cvmx_usb_internal_state_t * usb, uint64_t address, uint64_t value)
+static inline void __cvmx_usb_write_csr64(cvmx_usb_internal_state_t * usb,
+					  uint64_t address, uint64_t value)
 {
 #if ALLOW_CSR_DECODES
 	if (cvmx_unlikely(usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_CSRS)) {
@@ -416,9 +421,11 @@ static const char *__cvmx_usb_complete_to_string(cvmx_usb_complete_t complete_co
  *
  * @return Non zero if we need to do split transactions
  */
-static inline int __cvmx_usb_pipe_needs_split(cvmx_usb_internal_state_t * usb, cvmx_usb_pipe_t * pipe)
+static inline int __cvmx_usb_pipe_needs_split(cvmx_usb_internal_state_t * usb,
+					      cvmx_usb_pipe_t * pipe)
 {
-	return ((pipe->device_speed != CVMX_USB_SPEED_HIGH) && (usb->usbcx_hprt.s.prtspd == CVMX_USB_SPEED_HIGH));
+	return ((pipe->device_speed != CVMX_USB_SPEED_HIGH) &&
+		(usb->usbcx_hprt.s.prtspd == CVMX_USB_SPEED_HIGH));
 }
 
 /**
@@ -481,7 +488,8 @@ EXPORT_SYMBOL(cvmx_usb_get_num_ports);
  *
  * @return Transaction or NULL
  */
-static inline cvmx_usb_transaction_t *__cvmx_usb_alloc_transaction(cvmx_usb_internal_state_t * usb)
+static inline cvmx_usb_transaction_t *
+__cvmx_usb_alloc_transaction(cvmx_usb_internal_state_t * usb)
 {
 	cvmx_usb_transaction_t *t;
 	t = usb->free_transaction_head;
@@ -507,7 +515,9 @@ static inline cvmx_usb_transaction_t *__cvmx_usb_alloc_transaction(cvmx_usb_inte
  * @param transaction
  *               Transaction to free
  */
-static inline void __cvmx_usb_free_transaction(cvmx_usb_internal_state_t * usb, cvmx_usb_transaction_t * transaction)
+static inline void
+__cvmx_usb_free_transaction(cvmx_usb_internal_state_t * usb,
+			    cvmx_usb_transaction_t * transaction)
 {
 	transaction->flags = 0;
 	transaction->prev = NULL;
@@ -525,7 +535,8 @@ static inline void __cvmx_usb_free_transaction(cvmx_usb_internal_state_t * usb,
  * @param list   List to add pipe to
  * @param pipe   Pipe to add
  */
-static inline void __cvmx_usb_append_pipe(cvmx_usb_pipe_list_t * list, cvmx_usb_pipe_t * pipe)
+static inline void __cvmx_usb_append_pipe(cvmx_usb_pipe_list_t * list,
+					  cvmx_usb_pipe_t * pipe)
 {
 	pipe->next = NULL;
 	pipe->prev = list->tail;
@@ -542,7 +553,8 @@ static inline void __cvmx_usb_append_pipe(cvmx_usb_pipe_list_t * list, cvmx_usb_
  * @param list   List to remove pipe from
  * @param pipe   Pipe to remove
  */
-static inline void __cvmx_usb_remove_pipe(cvmx_usb_pipe_list_t * list, cvmx_usb_pipe_t * pipe)
+static inline void __cvmx_usb_remove_pipe(cvmx_usb_pipe_list_t * list,
+					  cvmx_usb_pipe_t * pipe)
 {
 	if (list->head == pipe) {
 		list->head = pipe->next;
@@ -581,7 +593,9 @@ static inline void __cvmx_usb_remove_pipe(cvmx_usb_pipe_list_t * list, cvmx_usb_
  * @return CVMX_USB_SUCCESS or a negative error code defined in
  *         cvmx_usb_status_t.
  */
-cvmx_usb_status_t cvmx_usb_initialize(cvmx_usb_state_t * state, int usb_port_number, cvmx_usb_initialize_flags_t flags)
+cvmx_usb_status_t cvmx_usb_initialize(cvmx_usb_state_t * state,
+				      int usb_port_number,
+				      cvmx_usb_initialize_flags_t flags)
 {
 	cvmx_usbnx_clk_ctl_t usbn_clk_ctl;
 	cvmx_usbnx_usbp_ctl_status_t usbn_usbp_ctl_status;
@@ -651,7 +665,8 @@ cvmx_usb_status_t cvmx_usb_initialize(cvmx_usb_state_t * state, int usb_port_num
 	/* 1. Wait for DCOK to assert (nothing to do) */
 	/* 2a. Write USBN0/1_CLK_CTL[POR] = 1 and
 	   USBN0/1_CLK_CTL[HRST,PRST,HCLK_RST] = 0 */
-	usbn_clk_ctl.u64 = __cvmx_usb_read_csr64(usb, CVMX_USBNX_CLK_CTL(usb->index));
+	usbn_clk_ctl.u64 = __cvmx_usb_read_csr64(usb,
+						 CVMX_USBNX_CLK_CTL(usb->index));
 	usbn_clk_ctl.s.por = 1;
 	usbn_clk_ctl.s.hrst = 0;
 	usbn_clk_ctl.s.prst = 0;
@@ -666,7 +681,8 @@ cvmx_usb_status_t cvmx_usb_initialize(cvmx_usb_state_t * state, int usb_port_num
 		if (OCTEON_IS_MODEL(OCTEON_CN3XXX)) {
 			usbn_clk_ctl.cn31xx.p_rclk = 1;	/* From CN31XX,CN30XX manual */
 			usbn_clk_ctl.cn31xx.p_xenbn = 0;
-		} else if (OCTEON_IS_MODEL(OCTEON_CN56XX) || OCTEON_IS_MODEL(OCTEON_CN50XX))
+		} else if (OCTEON_IS_MODEL(OCTEON_CN56XX) ||
+			   OCTEON_IS_MODEL(OCTEON_CN50XX))
 			usbn_clk_ctl.cn56xx.p_rtype = 2;	/* From CN56XX,CN50XX manual */
 		else
 			usbn_clk_ctl.cn52xx.p_rtype = 1;	/* From CN52XX manual */
@@ -688,7 +704,8 @@ cvmx_usb_status_t cvmx_usb_initialize(cvmx_usb_state_t * state, int usb_port_num
 		if (OCTEON_IS_MODEL(OCTEON_CN3XXX)) {
 			usbn_clk_ctl.cn31xx.p_rclk = 1;	/* From CN31XX,CN30XX manual */
 			usbn_clk_ctl.cn31xx.p_xenbn = 1;
-		} else if (OCTEON_IS_MODEL(OCTEON_CN56XX) || OCTEON_IS_MODEL(OCTEON_CN50XX))
+		} else if (OCTEON_IS_MODEL(OCTEON_CN56XX) ||
+			   OCTEON_IS_MODEL(OCTEON_CN50XX))
 			usbn_clk_ctl.cn56xx.p_rtype = 0;	/* From CN56XX,CN50XX manual */
 		else
 			usbn_clk_ctl.cn52xx.p_rtype = 0;	/* From CN52XX manual */
@@ -705,47 +722,65 @@ cvmx_usb_status_t cvmx_usb_initialize(cvmx_usb_state_t * state, int usb_port_num
 		usbn_clk_ctl.s.divide = divisor;
 		usbn_clk_ctl.s.divide2 = 0;
 	}
-	__cvmx_usb_write_csr64(usb, CVMX_USBNX_CLK_CTL(usb->index), usbn_clk_ctl.u64);
+	__cvmx_usb_write_csr64(usb, CVMX_USBNX_CLK_CTL(usb->index),
+			       usbn_clk_ctl.u64);
 	/* 2d. Write USBN0/1_CLK_CTL[HCLK_RST] = 1 */
 	usbn_clk_ctl.s.hclk_rst = 1;
-	__cvmx_usb_write_csr64(usb, CVMX_USBNX_CLK_CTL(usb->index), usbn_clk_ctl.u64);
+	__cvmx_usb_write_csr64(usb, CVMX_USBNX_CLK_CTL(usb->index),
+			       usbn_clk_ctl.u64);
 	/* 2e.  Wait 64 core-clock cycles for HCLK to stabilize */
 	cvmx_wait(64);
 	/* 3. Program the power-on reset field in the USBN clock-control register:
 	   USBN_CLK_CTL[POR] = 0 */
 	usbn_clk_ctl.s.por = 0;
-	__cvmx_usb_write_csr64(usb, CVMX_USBNX_CLK_CTL(usb->index), usbn_clk_ctl.u64);
+	__cvmx_usb_write_csr64(usb, CVMX_USBNX_CLK_CTL(usb->index),
+			       usbn_clk_ctl.u64);
 	/* 4. Wait 1 ms for PHY clock to start */
 	cvmx_wait_usec(1000);
-	/* 5. Program the Reset input from automatic test equipment field in the
-	   USBP control and status register: USBN_USBP_CTL_STATUS[ATE_RESET] = 1 */
-	usbn_usbp_ctl_status.u64 = __cvmx_usb_read_csr64(usb, CVMX_USBNX_USBP_CTL_STATUS(usb->index));
+	/* 5. Program the Reset input from automatic test equipment field in
+	 *    the USBP control and status register:
+	 *	 USBN_USBP_CTL_STATUS[ATE_RESET] = 1
+	 */
+	usbn_usbp_ctl_status.u64 = __cvmx_usb_read_csr64(usb,
+							 CVMX_USBNX_USBP_CTL_STATUS(usb->index));
 	usbn_usbp_ctl_status.s.ate_reset = 1;
-	__cvmx_usb_write_csr64(usb, CVMX_USBNX_USBP_CTL_STATUS(usb->index), usbn_usbp_ctl_status.u64);
+	__cvmx_usb_write_csr64(usb,
+			       CVMX_USBNX_USBP_CTL_STATUS(usb->index),
+			       usbn_usbp_ctl_status.u64);
 	/* 6. Wait 10 cycles */
 	cvmx_wait(10);
 	/* 7. Clear ATE_RESET field in the USBN clock-control register:
 	   USBN_USBP_CTL_STATUS[ATE_RESET] = 0 */
 	usbn_usbp_ctl_status.s.ate_reset = 0;
-	__cvmx_usb_write_csr64(usb, CVMX_USBNX_USBP_CTL_STATUS(usb->index), usbn_usbp_ctl_status.u64);
+	__cvmx_usb_write_csr64(usb,
+			       CVMX_USBNX_USBP_CTL_STATUS(usb->index),
+			       usbn_usbp_ctl_status.u64);
 	/* 8. Program the PHY reset field in the USBN clock-control register:
 	   USBN_CLK_CTL[PRST] = 1 */
 	usbn_clk_ctl.s.prst = 1;
-	__cvmx_usb_write_csr64(usb, CVMX_USBNX_CLK_CTL(usb->index), usbn_clk_ctl.u64);
+	__cvmx_usb_write_csr64(usb,
+			       CVMX_USBNX_CLK_CTL(usb->index),
+			       usbn_clk_ctl.u64);
 	/* 9. Program the USBP control and status register to select host or
 	   device mode. USBN_USBP_CTL_STATUS[HST_MODE] = 0 for host, = 1 for
 	   device */
 	usbn_usbp_ctl_status.s.hst_mode = 0;
-	__cvmx_usb_write_csr64(usb, CVMX_USBNX_USBP_CTL_STATUS(usb->index), usbn_usbp_ctl_status.u64);
+	__cvmx_usb_write_csr64(usb,
+			       CVMX_USBNX_USBP_CTL_STATUS(usb->index),
+			       usbn_usbp_ctl_status.u64);
 	/* 10. Wait 1 s */
 	cvmx_wait_usec(1);
 	/* 11. Program the hreset_n field in the USBN clock-control register:
 	   USBN_CLK_CTL[HRST] = 1 */
 	usbn_clk_ctl.s.hrst = 1;
-	__cvmx_usb_write_csr64(usb, CVMX_USBNX_CLK_CTL(usb->index), usbn_clk_ctl.u64);
+	__cvmx_usb_write_csr64(usb,
+			       CVMX_USBNX_CLK_CTL(usb->index),
+			       usbn_clk_ctl.u64);
 	/* 12. Proceed to USB core initialization */
 	usbn_clk_ctl.s.enable = 1;
-	__cvmx_usb_write_csr64(usb, CVMX_USBNX_CLK_CTL(usb->index), usbn_clk_ctl.u64);
+	__cvmx_usb_write_csr64(usb,
+			       CVMX_USBNX_CLK_CTL(usb->index),
+			       usbn_clk_ctl.u64);
 	cvmx_wait_usec(1);
 
 	/* USB Core Initialization */
@@ -779,7 +814,9 @@ cvmx_usb_status_t cvmx_usb_initialize(cvmx_usb_state_t * state, int usb_port_num
 		usbcx_gahbcfg.s.nptxfemplvl = 1;
 		usbcx_gahbcfg.s.ptxfemplvl = 1;
 		usbcx_gahbcfg.s.glblintrmsk = 1;
-		__cvmx_usb_write_csr32(usb, CVMX_USBCX_GAHBCFG(usb->index), usbcx_gahbcfg.u32);
+		__cvmx_usb_write_csr32(usb,
+				       CVMX_USBCX_GAHBCFG(usb->index),
+				       usbcx_gahbcfg.u32);
 	}
 	/* 3. Program the following fields in USBC_GUSBCFG register.
 	   HS/FS timeout calibration, USBC_GUSBCFG[TOUTCAL] = 0
@@ -788,12 +825,15 @@ cvmx_usb_status_t cvmx_usb_initialize(cvmx_usb_state_t * state, int usb_port_num
 	   PHY low-power clock select, USBC_GUSBCFG[PHYLPWRCLKSEL] = 0 */
 	{
 		cvmx_usbcx_gusbcfg_t usbcx_gusbcfg;
-		usbcx_gusbcfg.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_GUSBCFG(usb->index));
+		usbcx_gusbcfg.u32 = __cvmx_usb_read_csr32(usb,
+							  CVMX_USBCX_GUSBCFG(usb->index));
 		usbcx_gusbcfg.s.toutcal = 0;
 		usbcx_gusbcfg.s.ddrsel = 0;
 		usbcx_gusbcfg.s.usbtrdtim = 0x5;
 		usbcx_gusbcfg.s.phylpwrclksel = 0;
-		__cvmx_usb_write_csr32(usb, CVMX_USBCX_GUSBCFG(usb->index), usbcx_gusbcfg.u32);
+		__cvmx_usb_write_csr32(usb,
+				       CVMX_USBCX_GUSBCFG(usb->index),
+				       usbcx_gusbcfg.u32);
 	}
 	/* 4. The software must unmask the following bits in the USBC_GINTMSK
 	   register.
@@ -803,7 +843,8 @@ cvmx_usb_status_t cvmx_usb_initialize(cvmx_usb_state_t * state, int usb_port_num
 		cvmx_usbcx_gintmsk_t usbcx_gintmsk;
 		int channel;
 
-		usbcx_gintmsk.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_GINTMSK(usb->index));
+		usbcx_gintmsk.u32 = __cvmx_usb_read_csr32(usb,
+							  CVMX_USBCX_GINTMSK(usb->index));
 		usbcx_gintmsk.s.otgintmsk = 1;
 		usbcx_gintmsk.s.modemismsk = 1;
 		usbcx_gintmsk.s.hchintmsk = 1;
@@ -811,34 +852,47 @@ cvmx_usb_status_t cvmx_usb_initialize(cvmx_usb_state_t * state, int usb_port_num
 		/* We need RX FIFO interrupts if we don't have DMA */
 		if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_NO_DMA)
 			usbcx_gintmsk.s.rxflvlmsk = 1;
-		__cvmx_usb_write_csr32(usb, CVMX_USBCX_GINTMSK(usb->index), usbcx_gintmsk.u32);
+		__cvmx_usb_write_csr32(usb,
+				       CVMX_USBCX_GINTMSK(usb->index),
+				       usbcx_gintmsk.u32);
 
 		/* Disable all channel interrupts. We'll enable them per channel later */
 		for (channel = 0; channel < 8; channel++)
-			__cvmx_usb_write_csr32(usb, CVMX_USBCX_HCINTMSKX(channel, usb->index), 0);
+			__cvmx_usb_write_csr32(usb,
+					       CVMX_USBCX_HCINTMSKX(channel,
+								    usb->index),
+					       0);
 	}
 
 	{
 		/* Host Port Initialization */
 		if (cvmx_unlikely(usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_INFO))
-			cvmx_dprintf("%s: USB%d is in host mode\n", __func__, usb->index);
+			cvmx_dprintf("%s: USB%d is in host mode\n",
+				     __func__, usb->index);
 
 		/* 1. Program the host-port interrupt-mask field to unmask,
 		   USBC_GINTMSK[PRTINT] = 1 */
-		USB_SET_FIELD32(CVMX_USBCX_GINTMSK(usb->index), cvmx_usbcx_gintmsk_t, prtintmsk, 1);
-		USB_SET_FIELD32(CVMX_USBCX_GINTMSK(usb->index), cvmx_usbcx_gintmsk_t, disconnintmsk, 1);
-		/* 2. Program the USBC_HCFG register to select full-speed host or
-		   high-speed host. */
+		USB_SET_FIELD32(CVMX_USBCX_GINTMSK(usb->index),
+				cvmx_usbcx_gintmsk_t, prtintmsk, 1);
+		USB_SET_FIELD32(CVMX_USBCX_GINTMSK(usb->index),
+				cvmx_usbcx_gintmsk_t, disconnintmsk, 1);
+		/* 2. Program the USBC_HCFG register to select full-speed host
+		 *    or high-speed host.
+		 */
 		{
 			cvmx_usbcx_hcfg_t usbcx_hcfg;
-			usbcx_hcfg.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HCFG(usb->index));
+			usbcx_hcfg.u32 = __cvmx_usb_read_csr32(usb,
+							       CVMX_USBCX_HCFG(usb->index));
 			usbcx_hcfg.s.fslssupp = 0;
 			usbcx_hcfg.s.fslspclksel = 0;
-			__cvmx_usb_write_csr32(usb, CVMX_USBCX_HCFG(usb->index), usbcx_hcfg.u32);
+			__cvmx_usb_write_csr32(usb,
+					       CVMX_USBCX_HCFG(usb->index),
+					       usbcx_hcfg.u32);
 		}
 		/* 3. Program the port power bit to drive VBUS on the USB,
 		   USBC_HPRT[PRTPWR] = 1 */
-		USB_SET_FIELD32(CVMX_USBCX_HPRT(usb->index), cvmx_usbcx_hprt_t, prtpwr, 1);
+		USB_SET_FIELD32(CVMX_USBCX_HPRT(usb->index),
+				cvmx_usbcx_hprt_t, prtpwr, 1);
 
 		/* Steps 4-15 from the manual are done later in the port enable */
 	}
@@ -873,7 +927,9 @@ cvmx_usb_status_t cvmx_usb_shutdown(cvmx_usb_state_t * state)
 	/* Make sure all pipes are closed */
 	if (usb->idle_pipes.head ||
 	    usb->active_pipes[CVMX_USB_TRANSFER_ISOCHRONOUS].head ||
-	    usb->active_pipes[CVMX_USB_TRANSFER_INTERRUPT].head || usb->active_pipes[CVMX_USB_TRANSFER_CONTROL].head || usb->active_pipes[CVMX_USB_TRANSFER_BULK].head)
+	    usb->active_pipes[CVMX_USB_TRANSFER_INTERRUPT].head ||
+	    usb->active_pipes[CVMX_USB_TRANSFER_CONTROL].head ||
+	    usb->active_pipes[CVMX_USB_TRANSFER_BULK].head)
 		CVMX_USB_RETURN(CVMX_USB_BUSY);
 
 #ifdef __CVMX_ERROR_H__
@@ -881,13 +937,15 @@ cvmx_usb_status_t cvmx_usb_shutdown(cvmx_usb_state_t * state)
 #endif
 
 	/* Disable the clocks and put them in power on reset */
-	usbn_clk_ctl.u64 = __cvmx_usb_read_csr64(usb, CVMX_USBNX_CLK_CTL(usb->index));
+	usbn_clk_ctl.u64 = __cvmx_usb_read_csr64(usb,
+						 CVMX_USBNX_CLK_CTL(usb->index));
 	usbn_clk_ctl.s.enable = 1;
 	usbn_clk_ctl.s.por = 1;
 	usbn_clk_ctl.s.hclk_rst = 1;
 	usbn_clk_ctl.s.prst = 0;
 	usbn_clk_ctl.s.hrst = 0;
-	__cvmx_usb_write_csr64(usb, CVMX_USBNX_CLK_CTL(usb->index), usbn_clk_ctl.u64);
+	__cvmx_usb_write_csr64(usb, CVMX_USBNX_CLK_CTL(usb->index),
+			       usbn_clk_ctl.u64);
 	CVMX_USB_RETURN(CVMX_USB_SUCCESS);
 }
 
@@ -911,7 +969,8 @@ cvmx_usb_status_t cvmx_usb_enable(cvmx_usb_state_t * state)
 	CVMX_USB_LOG_CALLED();
 	CVMX_USB_LOG_PARAM("%p", state);
 
-	usb->usbcx_hprt.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HPRT(usb->index));
+	usb->usbcx_hprt.u32 = __cvmx_usb_read_csr32(usb,
+						    CVMX_USBCX_HPRT(usb->index));
 
 	/* If the port is already enabled the just return. We don't need to do
 	   anything */
@@ -926,58 +985,85 @@ cvmx_usb_status_t cvmx_usb_enable(cvmx_usb_state_t * state)
 	}
 
 	/* Program the port reset bit to start the reset process */
-	USB_SET_FIELD32(CVMX_USBCX_HPRT(usb->index), cvmx_usbcx_hprt_t, prtrst, 1);
+	USB_SET_FIELD32(CVMX_USBCX_HPRT(usb->index),
+			cvmx_usbcx_hprt_t, prtrst, 1);
 
 	/* Wait at least 50ms (high speed), or 10ms (full speed) for the reset
 	   process to complete. */
 	cvmx_wait_usec(50000);
 
 	/* Program the port reset bit to 0, USBC_HPRT[PRTRST] = 0 */
-	USB_SET_FIELD32(CVMX_USBCX_HPRT(usb->index), cvmx_usbcx_hprt_t, prtrst, 0);
+	USB_SET_FIELD32(CVMX_USBCX_HPRT(usb->index),
+			cvmx_usbcx_hprt_t,
+			prtrst, 0);
 
 	/* Wait for the USBC_HPRT[PRTENA]. */
-	if (CVMX_WAIT_FOR_FIELD32(CVMX_USBCX_HPRT(usb->index), cvmx_usbcx_hprt_t, prtena, ==, 1, 100000)) {
+	if (CVMX_WAIT_FOR_FIELD32(CVMX_USBCX_HPRT(usb->index),
+				  cvmx_usbcx_hprt_t, prtena, ==, 1, 100000)) {
 		if (cvmx_unlikely(usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_INFO))
-			cvmx_dprintf("%s: Timeout waiting for the port to finish reset\n", __func__);
+			cvmx_dprintf("%s: Timeout waiting for the port to finish reset\n",
+				     __func__);
 		CVMX_USB_RETURN(CVMX_USB_TIMEOUT);
 	}
 
-	/* Read the port speed field to get the enumerated speed, USBC_HPRT[PRTSPD]. */
-	usb->usbcx_hprt.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HPRT(usb->index));
+	/* Read the port speed field to get the enumerated speed,
+	 * USBC_HPRT[PRTSPD].
+	 */
+	usb->usbcx_hprt.u32 = __cvmx_usb_read_csr32(usb,
+						    CVMX_USBCX_HPRT(usb->index));
 	if (cvmx_unlikely(usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_INFO))
-		cvmx_dprintf("%s: USB%d is in %s speed mode\n", __func__, usb->index,
-			     (usb->usbcx_hprt.s.prtspd == CVMX_USB_SPEED_HIGH) ? "high" : (usb->usbcx_hprt.s.prtspd == CVMX_USB_SPEED_FULL) ? "full" : "low");
-
-	usbcx_ghwcfg3.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_GHWCFG3(usb->index));
-
-	/* 13. Program the USBC_GRXFSIZ register to select the size of the receive
-	   FIFO (25%). */
-	USB_SET_FIELD32(CVMX_USBCX_GRXFSIZ(usb->index), cvmx_usbcx_grxfsiz_t, rxfdep, usbcx_ghwcfg3.s.dfifodepth / 4);
+		cvmx_dprintf("%s: USB%d is in %s speed mode\n",
+			     __func__, usb->index,
+			     (usb->usbcx_hprt.s.prtspd == CVMX_USB_SPEED_HIGH)
+			     ? "high"
+			     : (usb->usbcx_hprt.s.prtspd == CVMX_USB_SPEED_FULL)
+			        ? "full" : "low");
+
+	usbcx_ghwcfg3.u32 = __cvmx_usb_read_csr32(usb,
+						  CVMX_USBCX_GHWCFG3(usb->index));
+
+	/* 13. Program the USBC_GRXFSIZ register to select the size of the
+	 *     receive FIFO (25%).
+	 */
+	USB_SET_FIELD32(CVMX_USBCX_GRXFSIZ(usb->index), cvmx_usbcx_grxfsiz_t,
+			rxfdep, usbcx_ghwcfg3.s.dfifodepth / 4);
 	/* 14. Program the USBC_GNPTXFSIZ register to select the size and the
-	   start address of the non- periodic transmit FIFO for nonperiodic
-	   transactions (50%). */
+	 *     start address of the non- periodic transmit FIFO for nonperiodic
+	 *     transactions (50%).
+	 */
 	{
 		cvmx_usbcx_gnptxfsiz_t siz;
-		siz.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_GNPTXFSIZ(usb->index));
+		siz.u32 = __cvmx_usb_read_csr32(usb,
+						CVMX_USBCX_GNPTXFSIZ(usb->index));
 		siz.s.nptxfdep = usbcx_ghwcfg3.s.dfifodepth / 2;
 		siz.s.nptxfstaddr = usbcx_ghwcfg3.s.dfifodepth / 4;
-		__cvmx_usb_write_csr32(usb, CVMX_USBCX_GNPTXFSIZ(usb->index), siz.u32);
+		__cvmx_usb_write_csr32(usb,
+				       CVMX_USBCX_GNPTXFSIZ(usb->index),
+				       siz.u32);
 	}
 	/* 15. Program the USBC_HPTXFSIZ register to select the size and start
-	   address of the periodic transmit FIFO for periodic transactions (25%). */
+	 *     address of the periodic transmit FIFO for periodic transactions
+	 *     (25%).
+	 */
 	{
 		cvmx_usbcx_hptxfsiz_t siz;
-		siz.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HPTXFSIZ(usb->index));
+		siz.u32 = __cvmx_usb_read_csr32(usb,
+						CVMX_USBCX_HPTXFSIZ(usb->index));
 		siz.s.ptxfsize = usbcx_ghwcfg3.s.dfifodepth / 4;
 		siz.s.ptxfstaddr = 3 * usbcx_ghwcfg3.s.dfifodepth / 4;
 		__cvmx_usb_write_csr32(usb, CVMX_USBCX_HPTXFSIZ(usb->index), siz.u32);
 	}
 	/* Flush all FIFOs */
-	USB_SET_FIELD32(CVMX_USBCX_GRSTCTL(usb->index), cvmx_usbcx_grstctl_t, txfnum, 0x10);
-	USB_SET_FIELD32(CVMX_USBCX_GRSTCTL(usb->index), cvmx_usbcx_grstctl_t, txfflsh, 1);
-	CVMX_WAIT_FOR_FIELD32(CVMX_USBCX_GRSTCTL(usb->index), cvmx_usbcx_grstctl_t, txfflsh, ==, 0, 100);
-	USB_SET_FIELD32(CVMX_USBCX_GRSTCTL(usb->index), cvmx_usbcx_grstctl_t, rxfflsh, 1);
-	CVMX_WAIT_FOR_FIELD32(CVMX_USBCX_GRSTCTL(usb->index), cvmx_usbcx_grstctl_t, rxfflsh, ==, 0, 100);
+	USB_SET_FIELD32(CVMX_USBCX_GRSTCTL(usb->index),
+			cvmx_usbcx_grstctl_t, txfnum, 0x10);
+	USB_SET_FIELD32(CVMX_USBCX_GRSTCTL(usb->index),
+			cvmx_usbcx_grstctl_t, txfflsh, 1);
+	CVMX_WAIT_FOR_FIELD32(CVMX_USBCX_GRSTCTL(usb->index),
+			      cvmx_usbcx_grstctl_t, txfflsh, ==, 0, 100);
+	USB_SET_FIELD32(CVMX_USBCX_GRSTCTL(usb->index),
+			cvmx_usbcx_grstctl_t, rxfflsh, 1);
+	CVMX_WAIT_FOR_FIELD32(CVMX_USBCX_GRSTCTL(usb->index),
+			      cvmx_usbcx_grstctl_t, rxfflsh, ==, 0, 100);
 
 	CVMX_USB_RETURN(CVMX_USB_SUCCESS);
 }
@@ -1004,7 +1090,8 @@ cvmx_usb_status_t cvmx_usb_disable(cvmx_usb_state_t * state)
 	CVMX_USB_LOG_PARAM("%p", state);
 
 	/* Disable the port */
-	USB_SET_FIELD32(CVMX_USBCX_HPRT(usb->index), cvmx_usbcx_hprt_t, prtena, 1);
+	USB_SET_FIELD32(CVMX_USBCX_HPRT(usb->index), cvmx_usbcx_hprt_t,
+			prtena, 1);
 	CVMX_USB_RETURN(CVMX_USB_SUCCESS);
 }
 
@@ -1046,7 +1133,9 @@ cvmx_usb_port_status_t cvmx_usb_get_status(cvmx_usb_state_t * state)
 	if (cvmx_unlikely(usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_CALLS))
 		cvmx_dprintf("%*s%s: returned port enabled=%d, over_current=%d, powered=%d, speed=%d, connected=%d, connect_change=%d\n",
 			     2 * (--usb->indent), "", __func__,
-			     result.port_enabled, result.port_over_current, result.port_powered, result.port_speed, result.connected, result.connect_change);
+			     result.port_enabled, result.port_over_current,
+			     result.port_powered, result.port_speed,
+			     result.connected, result.connect_change);
 	return result;
 }
 
@@ -1064,7 +1153,8 @@ EXPORT_SYMBOL(cvmx_usb_get_status);
  * @param port_status
  *               Port status to set, most like returned by cvmx_usb_get_status()
  */
-void cvmx_usb_set_status(cvmx_usb_state_t * state, cvmx_usb_port_status_t port_status)
+void cvmx_usb_set_status(cvmx_usb_state_t * state,
+			 cvmx_usb_port_status_t port_status)
 {
 	cvmx_usb_internal_state_t *usb = (cvmx_usb_internal_state_t *) state;
 	CVMX_USB_LOG_CALLED();
@@ -1086,9 +1176,11 @@ EXPORT_SYMBOL(cvmx_usb_set_status);
  *
  * @return Handle
  */
-static inline int __cvmx_usb_get_submit_handle(cvmx_usb_internal_state_t * usb, cvmx_usb_transaction_t * transaction)
+static inline int __cvmx_usb_get_submit_handle(cvmx_usb_internal_state_t * usb,
+					       cvmx_usb_transaction_t * transaction)
 {
-	return ((unsigned long)transaction - (unsigned long)usb->transaction) / sizeof(*transaction);
+	return ((unsigned long)transaction - (unsigned long)usb->transaction)
+				/ sizeof(*transaction);
 }
 
 /**
@@ -1101,7 +1193,8 @@ static inline int __cvmx_usb_get_submit_handle(cvmx_usb_internal_state_t * usb,
  *
  * @return Handle
  */
-static inline int __cvmx_usb_get_pipe_handle(cvmx_usb_internal_state_t * usb, cvmx_usb_pipe_t * pipe)
+static inline int __cvmx_usb_get_pipe_handle(cvmx_usb_internal_state_t * usb,
+					     cvmx_usb_pipe_t * pipe)
 {
 	return ((unsigned long)pipe - (unsigned long)usb->pipe) / sizeof(*pipe);
 }
@@ -1165,7 +1258,9 @@ static inline int __cvmx_usb_get_pipe_handle(cvmx_usb_internal_state_t * usb, cv
 int cvmx_usb_open_pipe(cvmx_usb_state_t * state, cvmx_usb_pipe_flags_t flags,
 		       int device_addr, int endpoint_num,
 		       cvmx_usb_speed_t device_speed, int max_packet,
-		       cvmx_usb_transfer_t transfer_type, cvmx_usb_direction_t transfer_dir, int interval, int multi_count, int hub_device_addr, int hub_port)
+		       cvmx_usb_transfer_t transfer_type,
+		       cvmx_usb_direction_t transfer_dir, int interval,
+		       int multi_count, int hub_device_addr, int hub_port)
 {
 	cvmx_usb_pipe_t *pipe;
 	cvmx_usb_internal_state_t *usb = (cvmx_usb_internal_state_t *) state;
@@ -1184,9 +1279,11 @@ int cvmx_usb_open_pipe(cvmx_usb_state_t * state, cvmx_usb_pipe_flags_t flags,
 	CVMX_USB_LOG_PARAM("%d", hub_device_addr);
 	CVMX_USB_LOG_PARAM("%d", hub_port);
 
-	if (cvmx_unlikely((device_addr < 0) || (device_addr > MAX_USB_ADDRESS)))
+	if (cvmx_unlikely((device_addr < 0) ||
+			  (device_addr > MAX_USB_ADDRESS)))
 		CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
-	if (cvmx_unlikely((endpoint_num < 0) || (endpoint_num > MAX_USB_ENDPOINT)))
+	if (cvmx_unlikely((endpoint_num < 0) ||
+			  (endpoint_num > MAX_USB_ENDPOINT)))
 		CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
 	if (cvmx_unlikely(device_speed > CVMX_USB_SPEED_LOW))
 		CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
@@ -1194,7 +1291,8 @@ int cvmx_usb_open_pipe(cvmx_usb_state_t * state, cvmx_usb_pipe_flags_t flags,
 		CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
 	if (cvmx_unlikely(transfer_type > CVMX_USB_TRANSFER_INTERRUPT))
 		CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
-	if (cvmx_unlikely((transfer_dir != CVMX_USB_DIRECTION_OUT) && (transfer_dir != CVMX_USB_DIRECTION_IN)))
+	if (cvmx_unlikely((transfer_dir != CVMX_USB_DIRECTION_OUT) &&
+			  (transfer_dir != CVMX_USB_DIRECTION_IN)))
 		CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
 	if (cvmx_unlikely(interval < 0))
 		CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
@@ -1202,9 +1300,11 @@ int cvmx_usb_open_pipe(cvmx_usb_state_t * state, cvmx_usb_pipe_flags_t flags,
 		CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
 	if (cvmx_unlikely(multi_count < 0))
 		CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
-	if (cvmx_unlikely((device_speed != CVMX_USB_SPEED_HIGH) && (multi_count != 0)))
+	if (cvmx_unlikely((device_speed != CVMX_USB_SPEED_HIGH) &&
+			  (multi_count != 0)))
 		CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
-	if (cvmx_unlikely((hub_device_addr < 0) || (hub_device_addr > MAX_USB_ADDRESS)))
+	if (cvmx_unlikely((hub_device_addr < 0) ||
+			  (hub_device_addr > MAX_USB_ADDRESS)))
 		CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
 	if (cvmx_unlikely((hub_port < 0) || (hub_port > MAX_USB_HUB_PORT)))
 		CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
@@ -1215,7 +1315,9 @@ int cvmx_usb_open_pipe(cvmx_usb_state_t * state, cvmx_usb_pipe_flags_t flags,
 		CVMX_USB_RETURN(CVMX_USB_NO_MEMORY);
 	__cvmx_usb_remove_pipe(&usb->free_pipes, pipe);
 	pipe->flags = flags | __CVMX_USB_PIPE_FLAGS_OPEN;
-	if ((device_speed == CVMX_USB_SPEED_HIGH) && (transfer_dir == CVMX_USB_DIRECTION_OUT) && (transfer_type == CVMX_USB_TRANSFER_BULK))
+	if ((device_speed == CVMX_USB_SPEED_HIGH) &&
+	    (transfer_dir == CVMX_USB_DIRECTION_OUT) &&
+	    (transfer_type == CVMX_USB_TRANSFER_BULK))
 		pipe->flags |= __CVMX_USB_PIPE_FLAGS_NEED_PING;
 	pipe->device_addr = device_addr;
 	pipe->endpoint_num = endpoint_num;
@@ -1270,7 +1372,8 @@ static void __cvmx_usb_poll_rx_fifo(cvmx_usb_internal_state_t * usb)
 	CVMX_USB_LOG_CALLED();
 	CVMX_USB_LOG_PARAM("%p", usb);
 
-	rx_status.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_GRXSTSPH(usb->index));
+	rx_status.u32 = __cvmx_usb_read_csr32(usb,
+					      CVMX_USBCX_GRXSTSPH(usb->index));
 	/* Only read data if IN data is there */
 	if (rx_status.s.pktsts != 2)
 		CVMX_USB_RETURN_NOTHING();
@@ -1284,13 +1387,20 @@ static void __cvmx_usb_poll_rx_fifo(cvmx_usb_internal_state_t * usb)
 		CVMX_USB_RETURN_NOTHING();
 
 	/* Get where the DMA engine would have written this data */
-	address = __cvmx_usb_read_csr64(usb, CVMX_USBNX_DMA0_INB_CHN0(usb->index) + channel * 8);
+	address = __cvmx_usb_read_csr64(usb,
+					CVMX_USBNX_DMA0_INB_CHN0(usb->index)
+						+ channel * 8);
 	ptr = cvmx_phys_to_ptr(address);
-	__cvmx_usb_write_csr64(usb, CVMX_USBNX_DMA0_INB_CHN0(usb->index) + channel * 8, address + bytes);
+	__cvmx_usb_write_csr64(usb,
+			       CVMX_USBNX_DMA0_INB_CHN0(usb->index)
+					+ channel * 8,
+			       address + bytes);
 
 	/* Loop writing the FIFO data for this packet into memory */
 	while (bytes > 0) {
-		*ptr++ = __cvmx_usb_read_csr32(usb, USB_FIFO_ADDRESS(channel, usb->index));
+		*ptr++ = __cvmx_usb_read_csr32(usb,
+					       USB_FIFO_ADDRESS(channel,
+								usb->index));
 		bytes -= 4;
 	}
 	CVMX_SYNCW;
@@ -1310,7 +1420,8 @@ static void __cvmx_usb_poll_rx_fifo(cvmx_usb_internal_state_t * usb)
  * @return Non zero if the hardware fifo was too small and needs
  *         to be serviced again.
  */
-static int __cvmx_usb_fill_tx_hw(cvmx_usb_internal_state_t * usb, cvmx_usb_tx_fifo_t * fifo, int available)
+static int __cvmx_usb_fill_tx_hw(cvmx_usb_internal_state_t * usb,
+				 cvmx_usb_tx_fifo_t * fifo, int available)
 {
 	CVMX_USB_LOG_CALLED();
 	CVMX_USB_LOG_PARAM("%p", usb);
@@ -1322,7 +1433,8 @@ static int __cvmx_usb_fill_tx_hw(cvmx_usb_internal_state_t * usb, cvmx_usb_tx_fi
 	while (available && (fifo->head != fifo->tail)) {
 		int i = fifo->tail;
 		const uint32_t *ptr = cvmx_phys_to_ptr(fifo->entry[i].address);
-		uint64_t csr_address = USB_FIFO_ADDRESS(fifo->entry[i].channel, usb->index) ^ 4;
+		uint64_t csr_address = USB_FIFO_ADDRESS(fifo->entry[i].channel,
+							usb->index) ^ 4;
 		int words = available;
 
 		/* Limit the amount of data to waht the SW fifo has */
@@ -1371,20 +1483,28 @@ static void __cvmx_usb_poll_tx_fifo(cvmx_usb_internal_state_t * usb)
 
 	if (usb->periodic.head != usb->periodic.tail) {
 		cvmx_usbcx_hptxsts_t tx_status;
-		tx_status.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HPTXSTS(usb->index));
-		if (__cvmx_usb_fill_tx_hw(usb, &usb->periodic, tx_status.s.ptxfspcavail))
-			USB_SET_FIELD32(CVMX_USBCX_GINTMSK(usb->index), cvmx_usbcx_gintmsk_t, ptxfempmsk, 1);
+		tx_status.u32 = __cvmx_usb_read_csr32(usb,
+						      CVMX_USBCX_HPTXSTS(usb->index));
+		if (__cvmx_usb_fill_tx_hw(usb, &usb->periodic,
+					  tx_status.s.ptxfspcavail))
+			USB_SET_FIELD32(CVMX_USBCX_GINTMSK(usb->index),
+					cvmx_usbcx_gintmsk_t, ptxfempmsk, 1);
 		else
-			USB_SET_FIELD32(CVMX_USBCX_GINTMSK(usb->index), cvmx_usbcx_gintmsk_t, ptxfempmsk, 0);
+			USB_SET_FIELD32(CVMX_USBCX_GINTMSK(usb->index),
+					cvmx_usbcx_gintmsk_t, ptxfempmsk, 0);
 	}
 
 	if (usb->nonperiodic.head != usb->nonperiodic.tail) {
 		cvmx_usbcx_gnptxsts_t tx_status;
-		tx_status.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_GNPTXSTS(usb->index));
-		if (__cvmx_usb_fill_tx_hw(usb, &usb->nonperiodic, tx_status.s.nptxfspcavail))
-			USB_SET_FIELD32(CVMX_USBCX_GINTMSK(usb->index), cvmx_usbcx_gintmsk_t, nptxfempmsk, 1);
+		tx_status.u32 = __cvmx_usb_read_csr32(usb,
+						      CVMX_USBCX_GNPTXSTS(usb->index));
+		if (__cvmx_usb_fill_tx_hw(usb, &usb->nonperiodic,
+					  tx_status.s.nptxfspcavail))
+			USB_SET_FIELD32(CVMX_USBCX_GINTMSK(usb->index),
+					cvmx_usbcx_gintmsk_t, nptxfempmsk, 1);
 		else
-			USB_SET_FIELD32(CVMX_USBCX_GINTMSK(usb->index), cvmx_usbcx_gintmsk_t, nptxfempmsk, 0);
+			USB_SET_FIELD32(CVMX_USBCX_GINTMSK(usb->index),
+					cvmx_usbcx_gintmsk_t, nptxfempmsk, 0);
 	}
 
 	CVMX_USB_RETURN_NOTHING();
@@ -1398,7 +1518,8 @@ static void __cvmx_usb_poll_tx_fifo(cvmx_usb_internal_state_t * usb)
  *                cvmx_usb_initialize().
  * @param channel Channel number to get packet from
  */
-static void __cvmx_usb_fill_tx_fifo(cvmx_usb_internal_state_t * usb, int channel)
+static void __cvmx_usb_fill_tx_fifo(cvmx_usb_internal_state_t * usb,
+				    int channel)
 {
 	cvmx_usbcx_hccharx_t hcchar;
 	cvmx_usbcx_hcspltx_t usbc_hcsplt;
@@ -1410,27 +1531,39 @@ static void __cvmx_usb_fill_tx_fifo(cvmx_usb_internal_state_t * usb, int channel
 	CVMX_USB_LOG_PARAM("%d", channel);
 
 	/* We only need to fill data on outbound channels */
-	hcchar.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HCCHARX(channel, usb->index));
+	hcchar.u32 = __cvmx_usb_read_csr32(usb,
+					   CVMX_USBCX_HCCHARX(channel,
+							      usb->index));
 	if (hcchar.s.epdir != CVMX_USB_DIRECTION_OUT)
 		CVMX_USB_RETURN_NOTHING();
 
 	/* OUT Splits only have data on the start and not the complete */
-	usbc_hcsplt.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HCSPLTX(channel, usb->index));
+	usbc_hcsplt.u32 = __cvmx_usb_read_csr32(usb,
+						CVMX_USBCX_HCSPLTX(channel,
+								   usb->index));
 	if (usbc_hcsplt.s.spltena && usbc_hcsplt.s.compsplt)
 		CVMX_USB_RETURN_NOTHING();
 
-	/* Find out how many bytes we need to fill and convert it into 32bit words */
-	usbc_hctsiz.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HCTSIZX(channel, usb->index));
+	/* Find out how many bytes we need to fill and convert it into 32bit
+	 * words
+	 */
+	usbc_hctsiz.u32 = __cvmx_usb_read_csr32(usb,
+						CVMX_USBCX_HCTSIZX(channel,
+								   usb->index));
 	if (!usbc_hctsiz.s.xfersize)
 		CVMX_USB_RETURN_NOTHING();
 
-	if ((hcchar.s.eptype == CVMX_USB_TRANSFER_INTERRUPT) || (hcchar.s.eptype == CVMX_USB_TRANSFER_ISOCHRONOUS))
+	if ((hcchar.s.eptype == CVMX_USB_TRANSFER_INTERRUPT) ||
+	    (hcchar.s.eptype == CVMX_USB_TRANSFER_ISOCHRONOUS))
 		fifo = &usb->periodic;
 	else
 		fifo = &usb->nonperiodic;
 
 	fifo->entry[fifo->head].channel = channel;
-	fifo->entry[fifo->head].address = __cvmx_usb_read_csr64(usb, CVMX_USBNX_DMA0_OUTB_CHN0(usb->index) + channel * 8);
+	fifo->entry[fifo->head].address =
+		__cvmx_usb_read_csr64(usb,
+				      CVMX_USBNX_DMA0_OUTB_CHN0(usb->index)
+						+ channel * 8);
 	fifo->entry[fifo->head].size = (usbc_hctsiz.s.xfersize + 3) >> 2;
 	fifo->head++;
 	if (fifo->head > MAX_CHANNELS)
@@ -1452,11 +1585,15 @@ static void __cvmx_usb_fill_tx_fifo(cvmx_usb_internal_state_t * usb, int channel
  * @param channel Channel to setup
  * @param pipe    Pipe for control transaction
  */
-static void __cvmx_usb_start_channel_control(cvmx_usb_internal_state_t * usb, int channel, cvmx_usb_pipe_t * pipe)
+static void __cvmx_usb_start_channel_control(cvmx_usb_internal_state_t * usb,
+					     int channel,
+					     cvmx_usb_pipe_t * pipe)
 {
 	cvmx_usb_transaction_t *transaction = pipe->head;
-	cvmx_usb_control_header_t *header = cvmx_phys_to_ptr(transaction->control_header);
-	int bytes_to_transfer = transaction->buffer_length - transaction->actual_bytes;
+	cvmx_usb_control_header_t *header =
+			cvmx_phys_to_ptr(transaction->control_header);
+	int bytes_to_transfer =
+			transaction->buffer_length - transaction->actual_bytes;
 	int packets_to_transfer;
 	cvmx_usbcx_hctsizx_t usbc_hctsiz;
 
@@ -1465,7 +1602,9 @@ static void __cvmx_usb_start_channel_control(cvmx_usb_internal_state_t * usb, in
 	CVMX_USB_LOG_PARAM("%d", channel);
 	CVMX_USB_LOG_PARAM("%p", pipe);
 
-	usbc_hctsiz.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HCTSIZX(channel, usb->index));
+	usbc_hctsiz.u32 = __cvmx_usb_read_csr32(usb,
+						CVMX_USBCX_HCTSIZX(channel,
+								   usb->index));
 
 	switch (transaction->stage) {
 	case CVMX_USB_STAGE_NON_CONTROL:
@@ -1476,17 +1615,25 @@ static void __cvmx_usb_start_channel_control(cvmx_usb_internal_state_t * usb, in
 		usbc_hctsiz.s.pid = 3;	/* Setup */
 		bytes_to_transfer = sizeof(*header);
 		/* All Control operations start with a setup going OUT */
-		USB_SET_FIELD32(CVMX_USBCX_HCCHARX(channel, usb->index), cvmx_usbcx_hccharx_t, epdir, CVMX_USB_DIRECTION_OUT);
+		USB_SET_FIELD32(CVMX_USBCX_HCCHARX(channel, usb->index),
+				cvmx_usbcx_hccharx_t, epdir,
+				CVMX_USB_DIRECTION_OUT);
 		/* Setup send the control header instead of the buffer data. The
 		   buffer data will be used in the next stage */
-		__cvmx_usb_write_csr64(usb, CVMX_USBNX_DMA0_OUTB_CHN0(usb->index) + channel * 8, transaction->control_header);
+		__cvmx_usb_write_csr64(usb,
+				       CVMX_USBNX_DMA0_OUTB_CHN0(usb->index) +
+						channel * 8,
+				       transaction->control_header);
 		break;
 	case CVMX_USB_STAGE_SETUP_SPLIT_COMPLETE:
 		usbc_hctsiz.s.pid = 3;	/* Setup */
 		bytes_to_transfer = 0;
 		/* All Control operations start with a setup going OUT */
-		USB_SET_FIELD32(CVMX_USBCX_HCCHARX(channel, usb->index), cvmx_usbcx_hccharx_t, epdir, CVMX_USB_DIRECTION_OUT);
-		USB_SET_FIELD32(CVMX_USBCX_HCSPLTX(channel, usb->index), cvmx_usbcx_hcspltx_t, compsplt, 1);
+		USB_SET_FIELD32(CVMX_USBCX_HCCHARX(channel, usb->index),
+				cvmx_usbcx_hccharx_t, epdir,
+				CVMX_USB_DIRECTION_OUT);
+		USB_SET_FIELD32(CVMX_USBCX_HCSPLTX(channel, usb->index),
+				cvmx_usbcx_hcspltx_t, compsplt, 1);
 		break;
 	case CVMX_USB_STAGE_DATA:
 		usbc_hctsiz.s.pid = __cvmx_usb_get_data_pid(pipe);
@@ -1497,33 +1644,48 @@ static void __cvmx_usb_start_channel_control(cvmx_usb_internal_state_t * usb, in
 				bytes_to_transfer = pipe->max_packet;
 		}
 		USB_SET_FIELD32(CVMX_USBCX_HCCHARX(channel, usb->index),
-				cvmx_usbcx_hccharx_t, epdir, ((header->s.request_type & 0x80) ? CVMX_USB_DIRECTION_IN : CVMX_USB_DIRECTION_OUT));
+				cvmx_usbcx_hccharx_t, epdir,
+				((header->s.request_type & 0x80)
+					? CVMX_USB_DIRECTION_IN
+					: CVMX_USB_DIRECTION_OUT));
 		break;
 	case CVMX_USB_STAGE_DATA_SPLIT_COMPLETE:
 		usbc_hctsiz.s.pid = __cvmx_usb_get_data_pid(pipe);
 		if (!(header->s.request_type & 0x80))
 			bytes_to_transfer = 0;
 		USB_SET_FIELD32(CVMX_USBCX_HCCHARX(channel, usb->index),
-				cvmx_usbcx_hccharx_t, epdir, ((header->s.request_type & 0x80) ? CVMX_USB_DIRECTION_IN : CVMX_USB_DIRECTION_OUT));
-		USB_SET_FIELD32(CVMX_USBCX_HCSPLTX(channel, usb->index), cvmx_usbcx_hcspltx_t, compsplt, 1);
+				cvmx_usbcx_hccharx_t, epdir,
+				((header->s.request_type & 0x80)
+				? CVMX_USB_DIRECTION_IN
+				: CVMX_USB_DIRECTION_OUT));
+		USB_SET_FIELD32(CVMX_USBCX_HCSPLTX(channel, usb->index),
+				cvmx_usbcx_hcspltx_t, compsplt, 1);
 		break;
 	case CVMX_USB_STAGE_STATUS:
 		usbc_hctsiz.s.pid = __cvmx_usb_get_data_pid(pipe);
 		bytes_to_transfer = 0;
-		USB_SET_FIELD32(CVMX_USBCX_HCCHARX(channel, usb->index), cvmx_usbcx_hccharx_t, epdir,
-				((header->s.request_type & 0x80) ? CVMX_USB_DIRECTION_OUT : CVMX_USB_DIRECTION_IN));
+		USB_SET_FIELD32(CVMX_USBCX_HCCHARX(channel, usb->index),
+				cvmx_usbcx_hccharx_t, epdir,
+				((header->s.request_type & 0x80)
+				? CVMX_USB_DIRECTION_OUT
+				: CVMX_USB_DIRECTION_IN));
 		break;
 	case CVMX_USB_STAGE_STATUS_SPLIT_COMPLETE:
 		usbc_hctsiz.s.pid = __cvmx_usb_get_data_pid(pipe);
 		bytes_to_transfer = 0;
-		USB_SET_FIELD32(CVMX_USBCX_HCCHARX(channel, usb->index), cvmx_usbcx_hccharx_t, epdir,
-				((header->s.request_type & 0x80) ? CVMX_USB_DIRECTION_OUT : CVMX_USB_DIRECTION_IN));
-		USB_SET_FIELD32(CVMX_USBCX_HCSPLTX(channel, usb->index), cvmx_usbcx_hcspltx_t, compsplt, 1);
+		USB_SET_FIELD32(CVMX_USBCX_HCCHARX(channel, usb->index),
+				cvmx_usbcx_hccharx_t, epdir,
+				((header->s.request_type & 0x80)
+				? CVMX_USB_DIRECTION_OUT
+				: CVMX_USB_DIRECTION_IN));
+		USB_SET_FIELD32(CVMX_USBCX_HCSPLTX(channel, usb->index),
+				cvmx_usbcx_hcspltx_t, compsplt, 1);
 		break;
 	}
 
 	/* Make sure the transfer never exceeds the byte limit of the hardware.
-	   Further bytes will be sent as continued transactions */
+	 * Further bytes will be sent as continued transactions
+	 */
 	if (bytes_to_transfer > MAX_TRANSFER_BYTES) {
 		/* Round MAX_TRANSFER_BYTES to a multiple of out packet size */
 		bytes_to_transfer = MAX_TRANSFER_BYTES / pipe->max_packet;
@@ -1532,18 +1694,22 @@ static void __cvmx_usb_start_channel_control(cvmx_usb_internal_state_t * usb, in
 
 	/* Calculate the number of packets to transfer. If the length is zero
 	   we still need to transfer one packet */
-	packets_to_transfer = (bytes_to_transfer + pipe->max_packet - 1) / pipe->max_packet;
+	packets_to_transfer =
+		(bytes_to_transfer + pipe->max_packet - 1) / pipe->max_packet;
 	if (packets_to_transfer == 0)
 		packets_to_transfer = 1;
-	else if ((packets_to_transfer > 1) && (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_NO_DMA)) {
-		/* Limit to one packet when not using DMA. Channels must be restarted
-		   between every packet for IN transactions, so there is no reason to
-		   do multiple packets in a row */
+	else if ((packets_to_transfer > 1) &&
+		 (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_NO_DMA)) {
+		/* Limit to one packet when not using DMA. Channels must be
+		 * restarted between every packet for IN transactions, so there
+		 * is no reason to do multiple packets in a row
+		 */
 		packets_to_transfer = 1;
 		bytes_to_transfer = packets_to_transfer * pipe->max_packet;
 	} else if (packets_to_transfer > MAX_TRANSFER_PACKETS) {
 		/* Limit the number of packet and data transferred to what the
-		   hardware can handle */
+		 * hardware can handle
+		 */
 		packets_to_transfer = MAX_TRANSFER_PACKETS;
 		bytes_to_transfer = packets_to_transfer * pipe->max_packet;
 	}
@@ -1551,7 +1717,8 @@ static void __cvmx_usb_start_channel_control(cvmx_usb_internal_state_t * usb, in
 	usbc_hctsiz.s.xfersize = bytes_to_transfer;
 	usbc_hctsiz.s.pktcnt = packets_to_transfer;
 
-	__cvmx_usb_write_csr32(usb, CVMX_USBCX_HCTSIZX(channel, usb->index), usbc_hctsiz.u32);
+	__cvmx_usb_write_csr32(usb, CVMX_USBCX_HCTSIZX(channel, usb->index),
+			       usbc_hctsiz.u32);
 	CVMX_USB_RETURN_NOTHING();
 }
 
@@ -1564,7 +1731,8 @@ static void __cvmx_usb_start_channel_control(cvmx_usb_internal_state_t * usb, in
  * @param channel Channel to setup
  * @param pipe    Pipe to start
  */
-static void __cvmx_usb_start_channel(cvmx_usb_internal_state_t * usb, int channel, cvmx_usb_pipe_t * pipe)
+static void __cvmx_usb_start_channel(cvmx_usb_internal_state_t * usb,
+				     int channel, cvmx_usb_pipe_t * pipe)
 {
 	cvmx_usb_transaction_t *transaction = pipe->head;
 
@@ -1573,9 +1741,13 @@ static void __cvmx_usb_start_channel(cvmx_usb_internal_state_t * usb, int channe
 	CVMX_USB_LOG_PARAM("%d", channel);
 	CVMX_USB_LOG_PARAM("%p", pipe);
 
-	if (cvmx_unlikely((usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_TRANSFERS) || (pipe->flags & CVMX_USB_PIPE_FLAGS_DEBUG_TRANSFERS)))
+	if (cvmx_unlikely((usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_TRANSFERS)
+			  || (pipe->flags & CVMX_USB_PIPE_FLAGS_DEBUG_TRANSFERS)))
 		cvmx_dprintf("%s: Channel %d started. Pipe %d transaction %d stage %d\n",
-			     __func__, channel, __cvmx_usb_get_pipe_handle(usb, pipe), __cvmx_usb_get_submit_handle(usb, transaction), transaction->stage);
+			     __func__, channel,
+			     __cvmx_usb_get_pipe_handle(usb, pipe),
+			     __cvmx_usb_get_submit_handle(usb, transaction),
+			     transaction->stage);
 
 	/* Make sure all writes to the DMA region get flushed */
 	CVMX_SYNCW;
@@ -1595,19 +1767,27 @@ static void __cvmx_usb_start_channel(cvmx_usb_internal_state_t * usb, int channe
 		cvmx_usbcx_haintmsk_t usbc_haintmsk;
 
 		/* Clear all channel status bits */
-		usbc_hcint.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HCINTX(channel, usb->index));
-		__cvmx_usb_write_csr32(usb, CVMX_USBCX_HCINTX(channel, usb->index), usbc_hcint.u32);
+		usbc_hcint.u32 = __cvmx_usb_read_csr32(usb,
+						       CVMX_USBCX_HCINTX(channel,
+									 usb->index));
+		__cvmx_usb_write_csr32(usb,
+				       CVMX_USBCX_HCINTX(channel, usb->index),
+				       usbc_hcint.u32);
 
 		usbc_hcintmsk.u32 = 0;
 		usbc_hcintmsk.s.chhltdmsk = 1;
 		if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_NO_DMA) {
-			/* Channels need these extra interrupts when we aren't in DMA mode */
+			/* Channels need these extra interrupts when we aren't
+			 * in DMA mode
+			 */
 			usbc_hcintmsk.s.datatglerrmsk = 1;
 			usbc_hcintmsk.s.frmovrunmsk = 1;
 			usbc_hcintmsk.s.bblerrmsk = 1;
 			usbc_hcintmsk.s.xacterrmsk = 1;
 			if (__cvmx_usb_pipe_needs_split(usb, pipe)) {
-				/* Splits don't generate xfercompl, so we need ACK and NYET */
+				/* Splits don't generate xfercompl, so we need
+				 * ACK and NYET
+				 */
 				usbc_hcintmsk.s.nyetmsk = 1;
 				usbc_hcintmsk.s.ackmsk = 1;
 			}
@@ -1615,12 +1795,16 @@ static void __cvmx_usb_start_channel(cvmx_usb_internal_state_t * usb, int channe
 			usbc_hcintmsk.s.stallmsk = 1;
 			usbc_hcintmsk.s.xfercomplmsk = 1;
 		}
-		__cvmx_usb_write_csr32(usb, CVMX_USBCX_HCINTMSKX(channel, usb->index), usbc_hcintmsk.u32);
+		__cvmx_usb_write_csr32(usb,
+				       CVMX_USBCX_HCINTMSKX(channel, usb->index),
+				       usbc_hcintmsk.u32);
 
 		/* Enable the channel interrupt to propagate */
-		usbc_haintmsk.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HAINTMSK(usb->index));
+		usbc_haintmsk.u32 = __cvmx_usb_read_csr32(usb,
+							  CVMX_USBCX_HAINTMSK(usb->index));
 		usbc_haintmsk.s.haintmsk |= 1 << channel;
-		__cvmx_usb_write_csr32(usb, CVMX_USBCX_HAINTMSK(usb->index), usbc_haintmsk.u32);
+		__cvmx_usb_write_csr32(usb, CVMX_USBCX_HAINTMSK(usb->index),
+				       usbc_haintmsk.u32);
 	}
 
 	/* Setup the locations the DMA engines use  */
@@ -1628,8 +1812,12 @@ static void __cvmx_usb_start_channel(cvmx_usb_internal_state_t * usb, int channe
 		uint64_t dma_address = transaction->buffer + transaction->actual_bytes;
 		if (transaction->type == CVMX_USB_TRANSFER_ISOCHRONOUS)
 			dma_address = transaction->buffer + transaction->iso_packets[0].offset + transaction->actual_bytes;
-		__cvmx_usb_write_csr64(usb, CVMX_USBNX_DMA0_OUTB_CHN0(usb->index) + channel * 8, dma_address);
-		__cvmx_usb_write_csr64(usb, CVMX_USBNX_DMA0_INB_CHN0(usb->index) + channel * 8, dma_address);
+		__cvmx_usb_write_csr64(usb,
+				       CVMX_USBNX_DMA0_OUTB_CHN0(usb->index) + channel * 8,
+				       dma_address);
+		__cvmx_usb_write_csr64(usb,
+				       CVMX_USBNX_DMA0_INB_CHN0(usb->index) + channel * 8,
+				       dma_address);
 	}
 
 	/* Setup both the size of the transfer and the SPLIT characteristics */
@@ -1637,24 +1825,31 @@ static void __cvmx_usb_start_channel(cvmx_usb_internal_state_t * usb, int channe
 		cvmx_usbcx_hcspltx_t usbc_hcsplt = {.u32 = 0 };
 		cvmx_usbcx_hctsizx_t usbc_hctsiz = {.u32 = 0 };
 		int packets_to_transfer;
-		int bytes_to_transfer = transaction->buffer_length - transaction->actual_bytes;
+		int bytes_to_transfer = transaction->buffer_length
+						- transaction->actual_bytes;
 
 		/* ISOCHRONOUS transactions store each individual transfer size in the
 		   packet structure, not the global buffer_length */
 		if (transaction->type == CVMX_USB_TRANSFER_ISOCHRONOUS)
-			bytes_to_transfer = transaction->iso_packets[0].length - transaction->actual_bytes;
+			bytes_to_transfer = transaction->iso_packets[0].length
+						- transaction->actual_bytes;
 
-		/* We need to do split transactions when we are talking to non high
-		   speed devices that are behind a high speed hub */
+		/* We need to do split transactions when we are talking to non
+		 * high speed devices that are behind a high speed hub
+		 */
 		if (__cvmx_usb_pipe_needs_split(usb, pipe)) {
-			/* On the start split phase (stage is even) record the frame number we
-			   will need to send the split complete. We only store the lower two bits
-			   since the time ahead can only be two frames */
+			/* On the start split phase (stage is even) record the
+			 * frame number we will need to send the split complete.
+			 * We only store the lower two bits since the time ahead
+			 * can only be two frames
+			 */
 			if ((transaction->stage & 1) == 0) {
 				if (transaction->type == CVMX_USB_TRANSFER_BULK)
-					pipe->split_sc_frame = (usb->frame_number + 1) & 0x7f;
+					pipe->split_sc_frame =
+						(usb->frame_number + 1) & 0x7f;
 				else
-					pipe->split_sc_frame = (usb->frame_number + 2) & 0x7f;
+					pipe->split_sc_frame =
+						(usb->frame_number + 2) & 0x7f;
 			} else
 				pipe->split_sc_frame = -1;
 
@@ -1663,29 +1858,39 @@ static void __cvmx_usb_start_channel(cvmx_usb_internal_state_t * usb, int channe
 			usbc_hcsplt.s.prtaddr = pipe->hub_port;
 			usbc_hcsplt.s.compsplt = (transaction->stage == CVMX_USB_STAGE_NON_CONTROL_SPLIT_COMPLETE);
 
-			/* SPLIT transactions can only ever transmit one data packet so
-			   limit the transfer size to the max packet size */
+			/* SPLIT transactions can only ever transmit one data
+			 * packet so limit the transfer size to the max packet
+			 * size
+			 */
 			if (bytes_to_transfer > pipe->max_packet)
 				bytes_to_transfer = pipe->max_packet;
 
 			/* ISOCHRONOUS OUT splits are unique in that they limit
 			   data transfers to 188 byte chunks representing the
 			   begin/middle/end of the data or all */
-			if (!usbc_hcsplt.s.compsplt && (pipe->transfer_dir == CVMX_USB_DIRECTION_OUT) && (pipe->transfer_type == CVMX_USB_TRANSFER_ISOCHRONOUS)) {
-				/* Clear the split complete frame number as there isn't going
-				   to be a split complete */
+			if (!usbc_hcsplt.s.compsplt &&
+			    (pipe->transfer_dir == CVMX_USB_DIRECTION_OUT) &&
+			    (pipe->transfer_type == CVMX_USB_TRANSFER_ISOCHRONOUS)) {
+				/* Clear the split complete frame number as
+				 * there isn't going to be a split complete
+				 */
 				pipe->split_sc_frame = -1;
-				/* See if we've started this transfer and sent data */
+				/* See if we've started this transfer and sent
+				 * data
+				 */
 				if (transaction->actual_bytes == 0) {
-					/* Nothing sent yet, this is either a begin or the
-					   entire payload */
+					/* Nothing sent yet, this is either a
+					 * begin or the entire payload
+					 */
 					if (bytes_to_transfer <= 188)
 						usbc_hcsplt.s.xactpos = 3;	/* Entire payload in one go */
 					else
 						usbc_hcsplt.s.xactpos = 2;	/* First part of payload */
 				} else {
-					/* Continuing the previous data, we must either be
-					   in the middle or at the end */
+					/* Continuing the previous data, we
+					 * must either be in the middle or at
+					 * the end
+					 */
 					if (bytes_to_transfer <= 188)
 						usbc_hcsplt.s.xactpos = 1;	/* End of payload */
 					else
@@ -1697,23 +1902,30 @@ static void __cvmx_usb_start_channel(cvmx_usb_internal_state_t * usb, int channe
 			}
 		}
 
-		/* Make sure the transfer never exceeds the byte limit of the hardware.
-		   Further bytes will be sent as continued transactions */
+		/* Make sure the transfer never exceeds the byte limit of the
+		 * hardware.  Further bytes will be sent as continued
+		 * transactions
+		 */
 		if (bytes_to_transfer > MAX_TRANSFER_BYTES) {
 			/* Round MAX_TRANSFER_BYTES to a multiple of out packet size */
 			bytes_to_transfer = MAX_TRANSFER_BYTES / pipe->max_packet;
 			bytes_to_transfer *= pipe->max_packet;
 		}
 
-		/* Calculate the number of packets to transfer. If the length is zero
-		   we still need to transfer one packet */
-		packets_to_transfer = (bytes_to_transfer + pipe->max_packet - 1) / pipe->max_packet;
+		/* Calculate the number of packets to transfer. If the length
+		 * is zero we still need to transfer one packet
+		 */
+		packets_to_transfer =
+			(bytes_to_transfer + pipe->max_packet - 1) / pipe->max_packet;
 		if (packets_to_transfer == 0)
 			packets_to_transfer = 1;
-		else if ((packets_to_transfer > 1) && (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_NO_DMA)) {
-			/* Limit to one packet when not using DMA. Channels must be restarted
-			   between every packet for IN transactions, so there is no reason to
-			   do multiple packets in a row */
+		else if ((packets_to_transfer > 1) &&
+			 (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_NO_DMA)) {
+			/* Limit to one packet when not using DMA. Channels must
+			 * be restarted between every packet for IN
+			 * transactions, so there is no reason to do
+			 * multiple packets in a row
+			 */
 			packets_to_transfer = 1;
 			bytes_to_transfer = packets_to_transfer * pipe->max_packet;
 		} else if (packets_to_transfer > MAX_TRANSFER_PACKETS) {
@@ -1732,21 +1944,28 @@ static void __cvmx_usb_start_channel(cvmx_usb_internal_state_t * usb, int channe
 		if (pipe->flags & __CVMX_USB_PIPE_FLAGS_NEED_PING)
 			usbc_hctsiz.s.dopng = 1;
 
-		__cvmx_usb_write_csr32(usb, CVMX_USBCX_HCSPLTX(channel, usb->index), usbc_hcsplt.u32);
-		__cvmx_usb_write_csr32(usb, CVMX_USBCX_HCTSIZX(channel, usb->index), usbc_hctsiz.u32);
+		__cvmx_usb_write_csr32(usb,
+				       CVMX_USBCX_HCSPLTX(channel, usb->index),
+				       usbc_hcsplt.u32);
+		__cvmx_usb_write_csr32(usb,
+				       CVMX_USBCX_HCTSIZX(channel, usb->index),
+				       usbc_hctsiz.u32);
 	}
 
 	/* Setup the Host Channel Characteristics Register */
 	{
 		cvmx_usbcx_hccharx_t usbc_hcchar = {.u32 = 0 };
 
-		/* Set the startframe odd/even properly. This is only used for periodic */
+		/* Set the startframe odd/even properly. This is only used for
+		 * periodic
+		 */
 		usbc_hcchar.s.oddfrm = usb->frame_number & 1;
 
-		/* Set the number of back to back packets allowed by this endpoint.
-		   Split transactions interpret "ec" as the number of immediate
-		   retries of failure. These retries happen too quickly, so we
-		   disable these entirely for splits */
+		/* Set the number of back to back packets allowed by this
+		 * endpoint.  Split transactions interpret "ec" as the number
+		 * of immediate retries of failure. These retries happen too
+		 * quickly, so we disable these entirely for splits
+		 */
 		if (__cvmx_usb_pipe_needs_split(usb, pipe))
 			usbc_hcchar.s.ec = 1;
 		else if (pipe->multi_count < 1)
@@ -1763,7 +1982,9 @@ static void __cvmx_usb_start_channel(cvmx_usb_internal_state_t * usb, int channe
 		usbc_hcchar.s.epdir = pipe->transfer_dir;
 		usbc_hcchar.s.epnum = pipe->endpoint_num;
 		usbc_hcchar.s.mps = pipe->max_packet;
-		__cvmx_usb_write_csr32(usb, CVMX_USBCX_HCCHARX(channel, usb->index), usbc_hcchar.u32);
+		__cvmx_usb_write_csr32(usb,
+				       CVMX_USBCX_HCCHARX(channel, usb->index),
+				       usbc_hcchar.u32);
 	}
 
 	/* Do transaction type specific fixups as needed */
@@ -1776,26 +1997,38 @@ static void __cvmx_usb_start_channel(cvmx_usb_internal_state_t * usb, int channe
 		break;
 	case CVMX_USB_TRANSFER_ISOCHRONOUS:
 		if (!__cvmx_usb_pipe_needs_split(usb, pipe)) {
-			/* ISO transactions require different PIDs depending on direction
-			   and how many packets are needed */
+			/* ISO transactions require different PIDs depending on
+			 * direction and how many packets are needed
+			 */
 			if (pipe->transfer_dir == CVMX_USB_DIRECTION_OUT) {
 				if (pipe->multi_count < 2)	/* Need DATA0 */
-					USB_SET_FIELD32(CVMX_USBCX_HCTSIZX(channel, usb->index), cvmx_usbcx_hctsizx_t, pid, 0);
+					USB_SET_FIELD32(CVMX_USBCX_HCTSIZX(channel,
+									   usb->index),
+							cvmx_usbcx_hctsizx_t,
+							pid, 0);
 				else	/* Need MDATA */
-					USB_SET_FIELD32(CVMX_USBCX_HCTSIZX(channel, usb->index), cvmx_usbcx_hctsizx_t, pid, 3);
+					USB_SET_FIELD32(CVMX_USBCX_HCTSIZX(channel,
+									   usb->index),
+							cvmx_usbcx_hctsizx_t,
+							pid, 3);
 			}
 		}
 		break;
 	}
 	{
-		cvmx_usbcx_hctsizx_t usbc_hctsiz = {.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HCTSIZX(channel, usb->index)) };
+		cvmx_usbcx_hctsizx_t usbc_hctsiz = {
+			.u32 = __cvmx_usb_read_csr32(usb,
+						     CVMX_USBCX_HCTSIZX(channel,
+									usb->index))
+		};
 		transaction->xfersize = usbc_hctsiz.s.xfersize;
 		transaction->pktcnt = usbc_hctsiz.s.pktcnt;
 	}
 	/* Remeber when we start a split transaction */
 	if (__cvmx_usb_pipe_needs_split(usb, pipe))
 		usb->active_split = transaction;
-	USB_SET_FIELD32(CVMX_USBCX_HCCHARX(channel, usb->index), cvmx_usbcx_hccharx_t, chena, 1);
+	USB_SET_FIELD32(CVMX_USBCX_HCCHARX(channel, usb->index),
+			cvmx_usbcx_hccharx_t, chena, 1);
 	if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_NO_DMA)
 		__cvmx_usb_fill_tx_fifo(usb, channel);
 	CVMX_USB_RETURN_NOTHING();
@@ -1812,13 +2045,18 @@ static void __cvmx_usb_start_channel(cvmx_usb_internal_state_t * usb, int channe
  *
  * @return Pipe or NULL if none are ready
  */
-static cvmx_usb_pipe_t *__cvmx_usb_find_ready_pipe(cvmx_usb_internal_state_t * usb, cvmx_usb_pipe_list_t * list, uint64_t current_frame)
+static cvmx_usb_pipe_t *__cvmx_usb_find_ready_pipe(cvmx_usb_internal_state_t *usb,
+						   cvmx_usb_pipe_list_t *list,
+						   uint64_t current_frame)
 {
 	cvmx_usb_pipe_t *pipe = list->head;
 	while (pipe) {
-		if (!(pipe->flags & __CVMX_USB_PIPE_FLAGS_SCHEDULED) && pipe->head &&
+		if (!(pipe->flags & __CVMX_USB_PIPE_FLAGS_SCHEDULED) &&
+		    pipe->head &&
 		    (pipe->next_tx_frame <= current_frame) &&
-		    ((pipe->split_sc_frame == -1) || ((((int)current_frame - (int)pipe->split_sc_frame) & 0x7f) < 0x40)) &&
+		    ((pipe->split_sc_frame == -1) ||
+		     ((((int)current_frame - (int)pipe->split_sc_frame) & 0x7f) < 0x40))
+		    &&
 		    (!usb->active_split || (usb->active_split == pipe->head))) {
 			CVMX_PREFETCH(pipe, 128);
 			CVMX_PREFETCH(pipe->head, 0);
@@ -1847,11 +2085,20 @@ static void __cvmx_usb_schedule(cvmx_usb_internal_state_t * usb, int is_sof)
 
 	CVMX_USB_LOG_CALLED();
 	CVMX_USB_LOG_PARAM("%p", usb);
+	CVMX_USB_LOG_PARAM("%d", is_sof);
 
 	if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_NO_DMA) {
-		/* Without DMA we need to be careful to not schedule something at the end of a frame and cause an overrun */
-		cvmx_usbcx_hfnum_t hfnum = {.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HFNUM(usb->index)) };
-		cvmx_usbcx_hfir_t hfir = {.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HFIR(usb->index)) };
+		/* Without DMA we need to be careful to not schedule something
+		 * at the end of a frame and cause an overrun
+		 */
+		cvmx_usbcx_hfnum_t hfnum = {
+			.u32 = __cvmx_usb_read_csr32(usb,
+						     CVMX_USBCX_HFNUM(usb->index))
+		};
+		cvmx_usbcx_hfir_t hfir = {
+			.u32 = __cvmx_usb_read_csr32(usb,
+						     CVMX_USBCX_HFIR(usb->index))
+		};
 		if (hfnum.s.frrem < hfir.s.frint / 4)
 			goto done;
 	}
@@ -1862,24 +2109,34 @@ static void __cvmx_usb_schedule(cvmx_usb_internal_state_t * usb, int is_sof)
 		channel = 31 - channel;
 		if (cvmx_unlikely(channel > 7)) {
 			if (cvmx_unlikely(usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_INFO))
-				cvmx_dprintf("%s: Idle hardware channels has a channel higher than 7. This is wrong\n", __func__);
+				cvmx_dprintf("%s: Idle hardware channels has a channel higher than 7. This is wrong\n",
+					     __func__);
 			break;
 		}
 
 		/* Find a pipe needing service */
 		pipe = NULL;
 		if (is_sof) {
-			/* Only process periodic pipes on SOF interrupts. This way we are
-			   sure that the periodic data is sent in the beginning of the
-			   frame */
-			pipe = __cvmx_usb_find_ready_pipe(usb, usb->active_pipes + CVMX_USB_TRANSFER_ISOCHRONOUS, usb->frame_number);
+			/* Only process periodic pipes on SOF interrupts.  This
+			 * way we are sure that the periodic data is sent in the
+			 * beginning of the frame
+			 */
+			pipe = __cvmx_usb_find_ready_pipe(usb,
+							  usb->active_pipes + CVMX_USB_TRANSFER_ISOCHRONOUS,
+							  usb->frame_number);
 			if (cvmx_likely(!pipe))
-				pipe = __cvmx_usb_find_ready_pipe(usb, usb->active_pipes + CVMX_USB_TRANSFER_INTERRUPT, usb->frame_number);
+				pipe = __cvmx_usb_find_ready_pipe(usb,
+								  usb->active_pipes + CVMX_USB_TRANSFER_INTERRUPT,
+								  usb->frame_number);
 		}
 		if (cvmx_likely(!pipe)) {
-			pipe = __cvmx_usb_find_ready_pipe(usb, usb->active_pipes + CVMX_USB_TRANSFER_CONTROL, usb->frame_number);
+			pipe = __cvmx_usb_find_ready_pipe(usb,
+							  usb->active_pipes + CVMX_USB_TRANSFER_CONTROL,
+							  usb->frame_number);
 			if (cvmx_likely(!pipe))
-				pipe = __cvmx_usb_find_ready_pipe(usb, usb->active_pipes + CVMX_USB_TRANSFER_BULK, usb->frame_number);
+				pipe = __cvmx_usb_find_ready_pipe(usb,
+								  usb->active_pipes + CVMX_USB_TRANSFER_BULK,
+								  usb->frame_number);
 		}
 		if (!pipe)
 			break;
@@ -1887,19 +2144,24 @@ static void __cvmx_usb_schedule(cvmx_usb_internal_state_t * usb, int is_sof)
 		CVMX_USB_LOG_PARAM("%d", channel);
 		CVMX_USB_LOG_PARAM("%p", pipe);
 
-		if (cvmx_unlikely((usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_TRANSFERS) || (pipe->flags & CVMX_USB_PIPE_FLAGS_DEBUG_TRANSFERS))) {
+		if (cvmx_unlikely((usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_TRANSFERS)
+				  || (pipe->flags & CVMX_USB_PIPE_FLAGS_DEBUG_TRANSFERS))) {
 			cvmx_usb_transaction_t *transaction = pipe->head;
-			const cvmx_usb_control_header_t *header = (transaction->control_header) ? cvmx_phys_to_ptr(transaction->control_header) : NULL;
-			const char *dir = (pipe->transfer_dir == CVMX_USB_DIRECTION_IN) ? "IN" : "OUT";
+			const cvmx_usb_control_header_t *header = (transaction->control_header)
+				? cvmx_phys_to_ptr(transaction->control_header) : NULL;
+			const char *dir = (pipe->transfer_dir == CVMX_USB_DIRECTION_IN)
+						? "IN" : "OUT";
 			const char *type;
 			switch (pipe->transfer_type) {
 			case CVMX_USB_TRANSFER_CONTROL:
 				type = "SETUP";
 				if (!header) {
-					cvmx_dprintf("%s: fatal error (header == NULL)\n", __func__);
+					cvmx_dprintf("%s: fatal error (header == NULL)\n",
+						     __func__);
 					dir = "INVALID";
 				} else
-					dir = (header->s.request_type & 0x80) ? "IN" : "OUT";
+					dir = (header->s.request_type & 0x80)
+							? "IN" : "OUT";
 				break;
 			case CVMX_USB_TRANSFER_ISOCHRONOUS:
 				type = "ISOCHRONOUS";
@@ -1912,18 +2174,23 @@ static void __cvmx_usb_schedule(cvmx_usb_internal_state_t * usb, int is_sof)
 				break;
 			}
 			cvmx_dprintf("%s: Starting pipe %d, transaction %d on channel %d. %s %s len=%d header=0x%llx\n",
-				     __func__, __cvmx_usb_get_pipe_handle(usb, pipe),
+				     __func__,
+				     __cvmx_usb_get_pipe_handle(usb, pipe),
 				     __cvmx_usb_get_submit_handle(usb, transaction),
-				     channel, type, dir, transaction->buffer_length, (header) ? (unsigned long long)header->u64 : 0ull);
+				     channel, type, dir,
+				     transaction->buffer_length,
+				     (header) ? (unsigned long long)header->u64 : 0ull);
 		}
 		__cvmx_usb_start_channel(usb, channel, pipe);
 	}
 
 done:
 	/* Only enable SOF interrupts when we have transactions pending in the
-	   future that might need to be scheduled */
+	 * future that might need to be scheduled
+	 */
 	need_sof = 0;
-	for (ttype = CVMX_USB_TRANSFER_CONTROL; ttype <= CVMX_USB_TRANSFER_INTERRUPT; ttype++) {
+	for (ttype = CVMX_USB_TRANSFER_CONTROL;
+	     ttype <= CVMX_USB_TRANSFER_INTERRUPT; ttype++) {
 		pipe = usb->active_pipes[ttype].head;
 		while (pipe) {
 			if (pipe->next_tx_frame > usb->frame_number) {
@@ -1933,7 +2200,8 @@ done:
 			pipe = pipe->next;
 		}
 	}
-	USB_SET_FIELD32(CVMX_USBCX_GINTMSK(usb->index), cvmx_usbcx_gintmsk_t, sofmsk, need_sof);
+	USB_SET_FIELD32(CVMX_USBCX_GINTMSK(usb->index), cvmx_usbcx_gintmsk_t,
+			sofmsk, need_sof);
 	CVMX_USB_RETURN_NOTHING();
 }
 
@@ -1951,7 +2219,10 @@ done:
  *               Completion code for the transaction, if any
  */
 static void __cvmx_usb_perform_callback(cvmx_usb_internal_state_t * usb,
-					cvmx_usb_pipe_t * pipe, cvmx_usb_transaction_t * transaction, cvmx_usb_callback_t reason, cvmx_usb_complete_t complete_code)
+					cvmx_usb_pipe_t * pipe,
+					cvmx_usb_transaction_t * transaction,
+					cvmx_usb_callback_t reason,
+					cvmx_usb_complete_t complete_code)
 {
 	cvmx_usb_callback_func_t callback = usb->callback[reason];
 	void *user_data = usb->callback_data[reason];
@@ -1966,7 +2237,8 @@ static void __cvmx_usb_perform_callback(cvmx_usb_internal_state_t * usb,
 		submit_handle = __cvmx_usb_get_submit_handle(usb, transaction);
 		bytes_transferred = transaction->actual_bytes;
 		/* Transactions are allowed to override the default callback */
-		if ((reason == CVMX_USB_CALLBACK_TRANSFER_COMPLETE) && transaction->callback) {
+		if ((reason == CVMX_USB_CALLBACK_TRANSFER_COMPLETE) &&
+		    transaction->callback) {
 			callback = transaction->callback;
 			user_data = transaction->callback_data;
 		}
@@ -1978,12 +2250,17 @@ static void __cvmx_usb_perform_callback(cvmx_usb_internal_state_t * usb,
 	if (cvmx_unlikely(usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_CALLBACKS))
 		cvmx_dprintf("%*s%s: calling callback %p(usb=%p, complete_code=%s, "
 			     "pipe_handle=%d, submit_handle=%d, bytes_transferred=%d, user_data=%p);\n",
-			     2 * usb->indent, "", __func__, callback, usb, __cvmx_usb_complete_to_string(complete_code), pipe_handle, submit_handle, bytes_transferred, user_data);
+			     2 * usb->indent, "", __func__, callback, usb,
+			     __cvmx_usb_complete_to_string(complete_code),
+			     pipe_handle, submit_handle, bytes_transferred,
+			     user_data);
 
-	callback((cvmx_usb_state_t *) usb, reason, complete_code, pipe_handle, submit_handle, bytes_transferred, user_data);
+	callback((cvmx_usb_state_t *) usb, reason, complete_code, pipe_handle,
+		 submit_handle, bytes_transferred, user_data);
 
 	if (cvmx_unlikely(usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_CALLBACKS))
-		cvmx_dprintf("%*s%s: callback %p complete\n", 2 * usb->indent, "", __func__, callback);
+		cvmx_dprintf("%*s%s: callback %p complete\n", 2 * usb->indent,
+			     "", __func__, callback);
 }
 
 /**
@@ -1999,7 +2276,10 @@ static void __cvmx_usb_perform_callback(cvmx_usb_internal_state_t * usb,
  * @param complete_code
  *               Completion code
  */
-static void __cvmx_usb_perform_complete(cvmx_usb_internal_state_t * usb, cvmx_usb_pipe_t * pipe, cvmx_usb_transaction_t * transaction, cvmx_usb_complete_t complete_code)
+static void __cvmx_usb_perform_complete(cvmx_usb_internal_state_t * usb,
+					cvmx_usb_pipe_t * pipe,
+					cvmx_usb_transaction_t * transaction,
+					cvmx_usb_complete_t complete_code)
 {
 	CVMX_USB_LOG_CALLED();
 	CVMX_USB_LOG_PARAM("%p", usb);
@@ -2011,19 +2291,25 @@ static void __cvmx_usb_perform_complete(cvmx_usb_internal_state_t * usb, cvmx_us
 	if (usb->active_split == transaction)
 		usb->active_split = NULL;
 
-	/* Isochronous transactions need extra processing as they might not be done
-	   after a single data transfer */
+	/* Isochronous transactions need extra processing as they might not be
+	 * done after a single data transfer
+	 */
 	if (cvmx_unlikely(transaction->type == CVMX_USB_TRANSFER_ISOCHRONOUS)) {
 		/* Update the number of bytes transferred in this ISO packet */
 		transaction->iso_packets[0].length = transaction->actual_bytes;
 		transaction->iso_packets[0].status = complete_code;
 
-		/* If there are more ISOs pending and we succeeded, schedule the next
-		   one */
-		if ((transaction->iso_number_packets > 1) && (complete_code == CVMX_USB_COMPLETE_SUCCESS)) {
-			transaction->actual_bytes = 0;	/* No bytes transferred for this packet as of yet */
-			transaction->iso_number_packets--;	/* One less ISO waiting to transfer */
-			transaction->iso_packets++;	/* Increment to the next location in our packet array */
+		/* If there are more ISOs pending and we succeeded, schedule
+		 * the next one
+		 */
+		if ((transaction->iso_number_packets > 1) &&
+		    (complete_code == CVMX_USB_COMPLETE_SUCCESS)) {
+			/* No bytes transferred for this packet as of yet */
+			transaction->actual_bytes = 0;
+			/* One less ISO waiting to transfer */
+			transaction->iso_number_packets--;
+			/* Increment to the next location in our packet array */
+			transaction->iso_packets++;
 			transaction->stage = CVMX_USB_STAGE_NON_CONTROL;
 			goto done;
 		}
@@ -2039,11 +2325,14 @@ static void __cvmx_usb_perform_complete(cvmx_usb_internal_state_t * usb, cvmx_us
 	else
 		pipe->head = transaction->next;
 	if (!pipe->head) {
-		__cvmx_usb_remove_pipe(usb->active_pipes + pipe->transfer_type, pipe);
+		__cvmx_usb_remove_pipe(usb->active_pipes + pipe->transfer_type,
+				       pipe);
 		__cvmx_usb_append_pipe(&usb->idle_pipes, pipe);
 
 	}
-	__cvmx_usb_perform_callback(usb, pipe, transaction, CVMX_USB_CALLBACK_TRANSFER_COMPLETE, complete_code);
+	__cvmx_usb_perform_callback(usb, pipe, transaction,
+				    CVMX_USB_CALLBACK_TRANSFER_COMPLETE,
+				    complete_code);
 	__cvmx_usb_free_transaction(usb, transaction);
 done:
 	CVMX_USB_RETURN_NOTHING();
@@ -2083,7 +2372,11 @@ static int __cvmx_usb_submit_transaction(cvmx_usb_internal_state_t * usb,
 					 uint64_t buffer,
 					 int buffer_length,
 					 uint64_t control_header,
-					 int iso_start_frame, int iso_number_packets, cvmx_usb_iso_packet_t * iso_packets, cvmx_usb_callback_func_t callback, void *user_data)
+					 int iso_start_frame,
+					 int iso_number_packets,
+					 cvmx_usb_iso_packet_t * iso_packets,
+					 cvmx_usb_callback_func_t callback,
+					 void *user_data)
 {
 	int submit_handle;
 	cvmx_usb_transaction_t *transaction;
@@ -2123,11 +2416,13 @@ static int __cvmx_usb_submit_transaction(cvmx_usb_internal_state_t * usb,
 		transaction->prev->next = transaction;
 	} else {
 		if (pipe->next_tx_frame < usb->frame_number)
-			pipe->next_tx_frame = usb->frame_number + pipe->interval - (usb->frame_number - pipe->next_tx_frame) % pipe->interval;
+			pipe->next_tx_frame = usb->frame_number + pipe->interval -
+				(usb->frame_number - pipe->next_tx_frame) % pipe->interval;
 		transaction->prev = NULL;
 		pipe->head = transaction;
 		__cvmx_usb_remove_pipe(&usb->idle_pipes, pipe);
-		__cvmx_usb_append_pipe(usb->active_pipes + pipe->transfer_type, pipe);
+		__cvmx_usb_append_pipe(usb->active_pipes + pipe->transfer_type,
+				       pipe);
 	}
 	pipe->tail = transaction;
 
@@ -2171,7 +2466,9 @@ static int __cvmx_usb_submit_transaction(cvmx_usb_internal_state_t * usb,
  *         failure. Negative values are failure codes from
  *         cvmx_usb_status_t.
  */
-int cvmx_usb_submit_bulk(cvmx_usb_state_t * state, int pipe_handle, uint64_t buffer, int buffer_length, cvmx_usb_callback_func_t callback, void *user_data)
+int cvmx_usb_submit_bulk(cvmx_usb_state_t * state, int pipe_handle,
+			 uint64_t buffer, int buffer_length,
+			 cvmx_usb_callback_func_t callback, void *user_data)
 {
 	int submit_handle;
 	cvmx_usb_internal_state_t *usb = (cvmx_usb_internal_state_t *) state;
@@ -2188,8 +2485,11 @@ int cvmx_usb_submit_bulk(cvmx_usb_state_t * state, int pipe_handle, uint64_t buf
 	if (cvmx_unlikely(buffer_length < 0))
 		CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
 
-	submit_handle = __cvmx_usb_submit_transaction(usb, pipe_handle, CVMX_USB_TRANSFER_BULK, 0,	/* flags */
-						      buffer, buffer_length, 0,	/* control_header */
+	submit_handle = __cvmx_usb_submit_transaction(usb, pipe_handle,
+						      CVMX_USB_TRANSFER_BULK,
+						      0,	/* flags */
+						      buffer, buffer_length,
+						      0,	/* control_header */
 						      0,	/* iso_start_frame */
 						      0,	/* iso_number_packets */
 						      NULL,	/* iso_packets */
@@ -2230,7 +2530,10 @@ EXPORT_SYMBOL(cvmx_usb_submit_bulk);
  *         failure. Negative values are failure codes from
  *         cvmx_usb_status_t.
  */
-int cvmx_usb_submit_interrupt(cvmx_usb_state_t * state, int pipe_handle, uint64_t buffer, int buffer_length, cvmx_usb_callback_func_t callback, void *user_data)
+int cvmx_usb_submit_interrupt(cvmx_usb_state_t * state, int pipe_handle,
+			      uint64_t buffer, int buffer_length,
+			      cvmx_usb_callback_func_t callback,
+			      void *user_data)
 {
 	int submit_handle;
 	cvmx_usb_internal_state_t *usb = (cvmx_usb_internal_state_t *) state;
@@ -2247,8 +2550,11 @@ int cvmx_usb_submit_interrupt(cvmx_usb_state_t * state, int pipe_handle, uint64_
 	if (cvmx_unlikely(buffer_length < 0))
 		CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
 
-	submit_handle = __cvmx_usb_submit_transaction(usb, pipe_handle, CVMX_USB_TRANSFER_INTERRUPT, 0,	/* flags */
-						      buffer, buffer_length, 0,	/* control_header */
+	submit_handle = __cvmx_usb_submit_transaction(usb, pipe_handle,
+						      CVMX_USB_TRANSFER_INTERRUPT,
+						      0,	/* flags */
+						      buffer, buffer_length,
+						      0,	/* control_header */
 						      0,	/* iso_start_frame */
 						      0,	/* iso_number_packets */
 						      NULL,	/* iso_packets */
@@ -2294,7 +2600,9 @@ EXPORT_SYMBOL(cvmx_usb_submit_interrupt);
  *         cvmx_usb_status_t.
  */
 int cvmx_usb_submit_control(cvmx_usb_state_t * state, int pipe_handle,
-			    uint64_t control_header, uint64_t buffer, int buffer_length, cvmx_usb_callback_func_t callback, void *user_data)
+			    uint64_t control_header, uint64_t buffer,
+			    int buffer_length,
+			    cvmx_usb_callback_func_t callback, void *user_data)
 {
 	int submit_handle;
 	cvmx_usb_internal_state_t *usb = (cvmx_usb_internal_state_t *) state;
@@ -2318,8 +2626,12 @@ int cvmx_usb_submit_control(cvmx_usb_state_t * state, int pipe_handle,
 	if ((header->s.request_type & 0x80) == 0)
 		buffer_length = cvmx_le16_to_cpu(header->s.length);
 
-	submit_handle = __cvmx_usb_submit_transaction(usb, pipe_handle, CVMX_USB_TRANSFER_CONTROL, 0,	/* flags */
-						      buffer, buffer_length, control_header, 0,	/* iso_start_frame */
+	submit_handle = __cvmx_usb_submit_transaction(usb, pipe_handle,
+						      CVMX_USB_TRANSFER_CONTROL,
+						      0,	/* flags */
+						      buffer, buffer_length,
+						      control_header,
+						      0,	/* iso_start_frame */
 						      0,	/* iso_number_packets */
 						      NULL,	/* iso_packets */
 						      callback, user_data);
@@ -2375,7 +2687,11 @@ EXPORT_SYMBOL(cvmx_usb_submit_control);
  */
 int cvmx_usb_submit_isochronous(cvmx_usb_state_t * state, int pipe_handle,
 				int start_frame, int flags,
-				int number_packets, cvmx_usb_iso_packet_t packets[], uint64_t buffer, int buffer_length, cvmx_usb_callback_func_t callback, void *user_data)
+				int number_packets,
+				cvmx_usb_iso_packet_t packets[],
+				uint64_t buffer, int buffer_length,
+				cvmx_usb_callback_func_t callback,
+				void *user_data)
 {
 	int submit_handle;
 	cvmx_usb_internal_state_t *usb = (cvmx_usb_internal_state_t *) state;
@@ -2404,8 +2720,14 @@ int cvmx_usb_submit_isochronous(cvmx_usb_state_t * state, int pipe_handle,
 	if (cvmx_unlikely(buffer_length < 0))
 		CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
 
-	submit_handle = __cvmx_usb_submit_transaction(usb, pipe_handle, CVMX_USB_TRANSFER_ISOCHRONOUS, flags, buffer, buffer_length, 0,	/* control_header */
-						      start_frame, number_packets, packets, callback, user_data);
+	submit_handle = __cvmx_usb_submit_transaction(usb, pipe_handle,
+						      CVMX_USB_TRANSFER_ISOCHRONOUS,
+						      flags,
+						      buffer, buffer_length,
+						      0,	/* control_header */
+						      start_frame,
+						      number_packets, packets,
+						      callback, user_data);
 	CVMX_USB_RETURN(submit_handle);
 }
 
@@ -2423,12 +2745,14 @@ EXPORT_SYMBOL(cvmx_usb_submit_isochronous);
  * @param pipe_handle
  *               Pipe handle to cancel requests in.
  * @param submit_handle
- *               Handle to transaction to cancel, returned by the submit function.
+ *               Handle to transaction to cancel, returned by the submit
+ *               function.
  *
  * @return CVMX_USB_SUCCESS or a negative error code defined in
  *         cvmx_usb_status_t.
  */
-cvmx_usb_status_t cvmx_usb_cancel(cvmx_usb_state_t * state, int pipe_handle, int submit_handle)
+cvmx_usb_status_t cvmx_usb_cancel(cvmx_usb_state_t * state, int pipe_handle,
+				  int submit_handle)
 {
 	cvmx_usb_transaction_t *transaction;
 	cvmx_usb_internal_state_t *usb = (cvmx_usb_internal_state_t *) state;
@@ -2441,7 +2765,8 @@ cvmx_usb_status_t cvmx_usb_cancel(cvmx_usb_state_t * state, int pipe_handle, int
 
 	if (cvmx_unlikely((pipe_handle < 0) || (pipe_handle >= MAX_PIPES)))
 		CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
-	if (cvmx_unlikely((submit_handle < 0) || (submit_handle >= MAX_TRANSACTIONS)))
+	if (cvmx_unlikely((submit_handle < 0) ||
+			  (submit_handle >= MAX_TRANSACTIONS)))
 		CVMX_USB_RETURN(CVMX_USB_INVALID_PARAM);
 
 	/* Fail if the pipe isn't open */
@@ -2456,7 +2781,8 @@ cvmx_usb_status_t cvmx_usb_cancel(cvmx_usb_state_t * state, int pipe_handle, int
 
 	/* If the transaction is the HEAD of the queue and scheduled. We need to
 	   treat it special */
-	if ((pipe->head == transaction) && (pipe->flags & __CVMX_USB_PIPE_FLAGS_SCHEDULED)) {
+	if ((pipe->head == transaction) &&
+	    (pipe->flags & __CVMX_USB_PIPE_FLAGS_SCHEDULED)) {
 		cvmx_usbcx_hccharx_t usbc_hcchar;
 
 		usb->pipe_for_channel[pipe->channel] = NULL;
@@ -2464,14 +2790,22 @@ cvmx_usb_status_t cvmx_usb_cancel(cvmx_usb_state_t * state, int pipe_handle, int
 
 		CVMX_SYNCW;
 
-		usbc_hcchar.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HCCHARX(pipe->channel, usb->index));
-		/* If the channel isn't enabled then the transaction already completed */
+		usbc_hcchar.u32 = __cvmx_usb_read_csr32(usb,
+							CVMX_USBCX_HCCHARX(pipe->channel,
+									   usb->index));
+		/* If the channel isn't enabled then the transaction already
+		 * completed
+		 */
 		if (usbc_hcchar.s.chena) {
 			usbc_hcchar.s.chdis = 1;
-			__cvmx_usb_write_csr32(usb, CVMX_USBCX_HCCHARX(pipe->channel, usb->index), usbc_hcchar.u32);
+			__cvmx_usb_write_csr32(usb,
+					       CVMX_USBCX_HCCHARX(pipe->channel,
+								  usb->index),
+					       usbc_hcchar.u32);
 		}
 	}
-	__cvmx_usb_perform_complete(usb, pipe, transaction, CVMX_USB_COMPLETE_CANCEL);
+	__cvmx_usb_perform_complete(usb, pipe, transaction,
+				    CVMX_USB_COMPLETE_CANCEL);
 	CVMX_USB_RETURN(CVMX_USB_SUCCESS);
 }
 
@@ -2507,7 +2841,8 @@ cvmx_usb_status_t cvmx_usb_cancel_all(cvmx_usb_state_t * state, int pipe_handle)
 	/* Simply loop through and attempt to cancel each transaction */
 	while (pipe->head) {
 		cvmx_usb_status_t result = cvmx_usb_cancel(state, pipe_handle,
-							   __cvmx_usb_get_submit_handle(usb, pipe->head));
+							   __cvmx_usb_get_submit_handle(usb,
+											pipe->head));
 		if (cvmx_unlikely(result != CVMX_USB_SUCCESS))
 			CVMX_USB_RETURN(result);
 	}
@@ -2568,7 +2903,10 @@ EXPORT_SYMBOL(cvmx_usb_close_pipe);
  * @return CVMX_USB_SUCCESS or a negative error code defined in
  *         cvmx_usb_status_t.
  */
-cvmx_usb_status_t cvmx_usb_register_callback(cvmx_usb_state_t * state, cvmx_usb_callback_t reason, cvmx_usb_callback_func_t callback, void *user_data)
+cvmx_usb_status_t cvmx_usb_register_callback(cvmx_usb_state_t * state,
+					     cvmx_usb_callback_t reason,
+					     cvmx_usb_callback_func_t callback,
+					     void *user_data)
 {
 	cvmx_usb_internal_state_t *usb = (cvmx_usb_internal_state_t *) state;
 
@@ -2608,7 +2946,8 @@ int cvmx_usb_get_frame_number(cvmx_usb_state_t * state)
 	CVMX_USB_LOG_CALLED();
 	CVMX_USB_LOG_PARAM("%p", state);
 
-	usbc_hfnum.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HFNUM(usb->index));
+	usbc_hfnum.u32 = __cvmx_usb_read_csr32(usb,
+					       CVMX_USBCX_HFNUM(usb->index));
 	frame_number = usbc_hfnum.s.frnum;
 
 	CVMX_USB_RETURN(frame_number);
@@ -2641,7 +2980,9 @@ static int __cvmx_usb_poll_channel(cvmx_usb_internal_state_t * usb, int channel)
 	CVMX_USB_LOG_PARAM("%d", channel);
 
 	/* Read the interrupt status bits for the channel */
-	usbc_hcint.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HCINTX(channel, usb->index));
+	usbc_hcint.u32 = __cvmx_usb_read_csr32(usb,
+					       CVMX_USBCX_HCINTX(channel,
+								 usb->index));
 
 #if 0
 	cvmx_dprintf("Channel %d%s%s%s%s%s%s%s%s%s%s%s\n", channel,
@@ -2657,44 +2998,63 @@ static int __cvmx_usb_poll_channel(cvmx_usb_internal_state_t * usb, int channel)
 #endif
 
 	if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_NO_DMA) {
-		usbc_hcchar.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HCCHARX(channel, usb->index));
+		usbc_hcchar.u32 = __cvmx_usb_read_csr32(usb,
+							CVMX_USBCX_HCCHARX(channel,
+									   usb->index));
 
 		if (usbc_hcchar.s.chena && usbc_hcchar.s.chdis) {
-			/* There seems to be a bug in CN31XX which can cause interrupt
-			   IN transfers to get stuck until we do a write of HCCHARX
-			   without changing things */
-			__cvmx_usb_write_csr32(usb, CVMX_USBCX_HCCHARX(channel, usb->index), usbc_hcchar.u32);
+			/* There seems to be a bug in CN31XX which can cause
+			 * interrupt IN transfers to get stuck until we do a
+			 * write of HCCHARX without changing things
+			 */
+			__cvmx_usb_write_csr32(usb,
+					       CVMX_USBCX_HCCHARX(channel,
+								  usb->index),
+					       usbc_hcchar.u32);
 			CVMX_USB_RETURN(0);
 		}
 
-		/* In non DMA mode the channels don't halt themselves. We need to
-		   manually disable channels that are left running */
+		/* In non DMA mode the channels don't halt themselves. We need
+		 * to manually disable channels that are left running
+		 */
 		if (!usbc_hcint.s.chhltd) {
 			if (usbc_hcchar.s.chena) {
 				cvmx_usbcx_hcintmskx_t hcintmsk;
 				/* Disable all interrupts except CHHLTD */
 				hcintmsk.u32 = 0;
 				hcintmsk.s.chhltdmsk = 1;
-				__cvmx_usb_write_csr32(usb, CVMX_USBCX_HCINTMSKX(channel, usb->index), hcintmsk.u32);
+				__cvmx_usb_write_csr32(usb,
+						       CVMX_USBCX_HCINTMSKX(channel,
+									    usb->index),
+						       hcintmsk.u32);
 				usbc_hcchar.s.chdis = 1;
-				__cvmx_usb_write_csr32(usb, CVMX_USBCX_HCCHARX(channel, usb->index), usbc_hcchar.u32);
+				__cvmx_usb_write_csr32(usb,
+						       CVMX_USBCX_HCCHARX(channel,
+									  usb->index),
+						       usbc_hcchar.u32);
 				CVMX_USB_RETURN(0);
 			} else if (usbc_hcint.s.xfercompl) {
-				/* Successful IN/OUT with transfer complete. Channel halt isn't needed */
+				/* Successful IN/OUT with transfer complete.
+				 * Channel halt isn't needed
+				 */
 			} else {
-				cvmx_dprintf("USB%d: Channel %d interrupt without halt\n", usb->index, channel);
+				cvmx_dprintf("USB%d: Channel %d interrupt without halt\n",
+					     usb->index, channel);
 				CVMX_USB_RETURN(0);
 			}
 		}
 	} else {
-		/* There is are no interrupts that we need to process when the channel is
-		   still running */
+		/* There is are no interrupts that we need to process when the
+		 * channel is still running
+		 */
 		if (!usbc_hcint.s.chhltd)
 			CVMX_USB_RETURN(0);
 	}
 
 	/* Disable the channel interrupts now that it is done */
-	__cvmx_usb_write_csr32(usb, CVMX_USBCX_HCINTMSKX(channel, usb->index), 0);
+	__cvmx_usb_write_csr32(usb,
+			       CVMX_USBCX_HCINTMSKX(channel, usb->index),
+			       0);
 	usb->idle_hardware_channels |= (1 << channel);
 
 	/* Make sure this channel is tied to a valid pipe */
@@ -2706,33 +3066,43 @@ static int __cvmx_usb_poll_channel(cvmx_usb_internal_state_t * usb, int channel)
 	transaction = pipe->head;
 	CVMX_PREFETCH0(transaction);
 
-	/* Disconnect this pipe from the HW channel. Later the schedule function will
-	   figure out which pipe needs to go */
+	/* Disconnect this pipe from the HW channel. Later the schedule
+	 * function will figure out which pipe needs to go
+	 */
 	usb->pipe_for_channel[channel] = NULL;
 	pipe->flags &= ~__CVMX_USB_PIPE_FLAGS_SCHEDULED;
 
 	/* Read the channel config info so we can figure out how much data
 	   transfered */
-	usbc_hcchar.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HCCHARX(channel, usb->index));
-	usbc_hctsiz.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HCTSIZX(channel, usb->index));
-
-	/* Calculating the number of bytes successfully transferred is dependent on
-	   the transfer direction */
+	usbc_hcchar.u32 = __cvmx_usb_read_csr32(usb,
+						CVMX_USBCX_HCCHARX(channel,
+								   usb->index));
+	usbc_hctsiz.u32 = __cvmx_usb_read_csr32(usb,
+						CVMX_USBCX_HCTSIZX(channel,
+								   usb->index));
+
+	/* Calculating the number of bytes successfully transferred is
+	 * dependent on the transfer direction
+	 */
 	packets_processed = transaction->pktcnt - usbc_hctsiz.s.pktcnt;
 	if (usbc_hcchar.s.epdir) {
-		/* IN transactions are easy. For every byte received the hardware
-		   decrements xfersize. All we need to do is subtract the current
-		   value of xfersize from its starting value and we know how many
-		   bytes were written to the buffer */
-		bytes_this_transfer = transaction->xfersize - usbc_hctsiz.s.xfersize;
+		/* IN transactions are easy. For every byte received the
+		 * hardware decrements xfersize. All we need to do is subtract
+		 * the current value of xfersize from its starting value and we
+		 * know how many bytes were written to the buffer
+		 */
+		bytes_this_transfer =
+			transaction->xfersize - usbc_hctsiz.s.xfersize;
 	} else {
 		/* OUT transaction don't decrement xfersize. Instead pktcnt is
-		   decremented on every successful packet send. The hardware does
-		   this when it receives an ACK, or NYET. If it doesn't
-		   receive one of these responses pktcnt doesn't change */
+		 * decremented on every successful packet send. The hardware
+		 * does this when it receives an ACK, or NYET. If it doesn't
+		 * receive one of these responses pktcnt doesn't change
+		 */
 		bytes_this_transfer = packets_processed * usbc_hcchar.s.mps;
 		/* The last packet may not be a full transfer if we didn't have
-		   enough data */
+		 * enough data
+		 */
 		if (bytes_this_transfer > transaction->xfersize)
 			bytes_this_transfer = transaction->xfersize;
 	}
@@ -2743,102 +3113,136 @@ static int __cvmx_usb_poll_channel(cvmx_usb_internal_state_t * usb, int channel)
 		bytes_in_last_packet = bytes_this_transfer;
 
 	/* As a special case, setup transactions output the setup header, not
-	   the user's data. For this reason we don't count setup data as bytes
-	   transferred */
-	if ((transaction->stage == CVMX_USB_STAGE_SETUP) || (transaction->stage == CVMX_USB_STAGE_SETUP_SPLIT_COMPLETE))
+	 * the user's data. For this reason we don't count setup data as bytes
+	 * transferred
+	 */
+	if ((transaction->stage == CVMX_USB_STAGE_SETUP) ||
+	    (transaction->stage == CVMX_USB_STAGE_SETUP_SPLIT_COMPLETE))
 		bytes_this_transfer = 0;
 
 	/* Optional debug output */
 	if (cvmx_unlikely((usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_TRANSFERS) || (pipe->flags & CVMX_USB_PIPE_FLAGS_DEBUG_TRANSFERS)))
 		cvmx_dprintf("%s: Channel %d halted. Pipe %d transaction %d stage %d bytes=%d\n",
-			     __func__, channel, __cvmx_usb_get_pipe_handle(usb, pipe), __cvmx_usb_get_submit_handle(usb, transaction), transaction->stage, bytes_this_transfer);
+			     __func__, channel,
+			     __cvmx_usb_get_pipe_handle(usb, pipe),
+			     __cvmx_usb_get_submit_handle(usb, transaction),
+			     transaction->stage, bytes_this_transfer);
 
 	/* Add the bytes transferred to the running total. It is important that
-	   bytes_this_transfer doesn't count any data that needs to be
-	   retransmitted */
+	 * bytes_this_transfer doesn't count any data that needs to be
+	 * retransmitted
+	 */
 	transaction->actual_bytes += bytes_this_transfer;
 	if (transaction->type == CVMX_USB_TRANSFER_ISOCHRONOUS)
 		buffer_space_left = transaction->iso_packets[0].length - transaction->actual_bytes;
 	else
 		buffer_space_left = transaction->buffer_length - transaction->actual_bytes;
 
-	/* We need to remember the PID toggle state for the next transaction. The
-	   hardware already updated it for the next transaction */
+	/* We need to remember the PID toggle state for the next transaction.
+	 * The hardware already updated it for the next transaction
+	 */
 	pipe->pid_toggle = !(usbc_hctsiz.s.pid == 0);
 
-	/* For high speed bulk out, assume the next transaction will need to do a
-	   ping before proceeding. If this isn't true the ACK processing below
-	   will clear this flag */
-	if ((pipe->device_speed == CVMX_USB_SPEED_HIGH) && (pipe->transfer_type == CVMX_USB_TRANSFER_BULK) && (pipe->transfer_dir == CVMX_USB_DIRECTION_OUT))
+	/* For high speed bulk out, assume the next transaction will need to do
+	 * a ping before proceeding. If this isn't true the ACK processing
+	 * below will clear this flag
+	 */
+	if ((pipe->device_speed == CVMX_USB_SPEED_HIGH) &&
+	    (pipe->transfer_type == CVMX_USB_TRANSFER_BULK) &&
+	    (pipe->transfer_dir == CVMX_USB_DIRECTION_OUT))
 		pipe->flags |= __CVMX_USB_PIPE_FLAGS_NEED_PING;
 
 	if (usbc_hcint.s.stall) {
-		/* STALL as a response means this transaction cannot be completed
-		   because the device can't process transactions. Tell the user. Any
-		   data that was transferred will be counted on the actual bytes
-		   transferred */
+		/* STALL as a response means this transaction cannot be
+		 * completed because the device can't process transactions.
+		 * Tell the user.  Any data that was transferred will be
+		 * counted on the actual bytes transferred
+		 */
 		pipe->pid_toggle = 0;
-		__cvmx_usb_perform_complete(usb, pipe, transaction, CVMX_USB_COMPLETE_STALL);
+		__cvmx_usb_perform_complete(usb, pipe, transaction,
+					    CVMX_USB_COMPLETE_STALL);
 	} else if (usbc_hcint.s.xacterr) {
-		/* We know at least one packet worked if we get a ACK or NAK. Reset the retry counter */
+		/* We know at least one packet worked if we get a ACK or NAK.
+		 * Reset the retry counter
+		 */
 		if (usbc_hcint.s.nak || usbc_hcint.s.ack)
 			transaction->retries = 0;
 		transaction->retries++;
 		if (transaction->retries > MAX_RETRIES) {
-			/* XactErr as a response means the device signaled something wrong with
-			   the transfer. For example, PID toggle errors cause these */
-			__cvmx_usb_perform_complete(usb, pipe, transaction, CVMX_USB_COMPLETE_XACTERR);
+			/* XactErr as a response means the device signaled
+			 * something wrong with the transfer. For example, PID
+			 * toggle errors cause these
+			 */
+			__cvmx_usb_perform_complete(usb, pipe, transaction,
+						    CVMX_USB_COMPLETE_XACTERR);
 		} else {
-			/* If this was a split then clear our split in progress marker */
+			/* If this was a split then clear our split in progress
+			 * marker
+			 */
 			if (usb->active_split == transaction)
 				usb->active_split = NULL;
-			/* Rewind to the beginning of the transaction by anding off the
-			   split complete bit */
+			/* Rewind to the beginning of the transaction by anding
+			 * off the split complete bit
+			 */
 			transaction->stage &= ~1;
 			pipe->split_sc_frame = -1;
 			pipe->next_tx_frame += pipe->interval;
 			if (pipe->next_tx_frame < usb->frame_number)
-				pipe->next_tx_frame = usb->frame_number + pipe->interval - (usb->frame_number - pipe->next_tx_frame) % pipe->interval;
+				pipe->next_tx_frame =
+					usb->frame_number
+					+ pipe->interval
+					- (usb->frame_number - pipe->next_tx_frame) % pipe->interval;
 		}
 	} else if (usbc_hcint.s.bblerr) {
 		/* Babble Error (BblErr) */
-		__cvmx_usb_perform_complete(usb, pipe, transaction, CVMX_USB_COMPLETE_BABBLEERR);
+		__cvmx_usb_perform_complete(usb, pipe, transaction,
+					    CVMX_USB_COMPLETE_BABBLEERR);
 	} else if (usbc_hcint.s.datatglerr) {
 		/* We'll retry the exact same transaction again */
 		transaction->retries++;
 	} else if (usbc_hcint.s.nyet) {
-		/* NYET as a response is only allowed in three cases: as a response to
-		   a ping, as a response to a split transaction, and as a response to
-		   a bulk out. The ping case is handled by hardware, so we only have
-		   splits and bulk out */
+		/* NYET as a response is only allowed in three cases: as a
+		 * response to a ping, as a response to a split transaction,
+		 * and as a response to a bulk out. The ping case is handled by
+		 * hardware, so we only have splits and bulk out
+		 */
 		if (!__cvmx_usb_pipe_needs_split(usb, pipe)) {
 			transaction->retries = 0;
-			/* If there is more data to go then we need to try again. Otherwise
-			   this transaction is complete */
-			if ((buffer_space_left == 0) || (bytes_in_last_packet < pipe->max_packet))
-				__cvmx_usb_perform_complete(usb, pipe, transaction, CVMX_USB_COMPLETE_SUCCESS);
+			/* If there is more data to go then we need to try
+			 * again.  Otherwise this transaction is complete
+			 */
+			if ((buffer_space_left == 0) ||
+			    (bytes_in_last_packet < pipe->max_packet))
+				__cvmx_usb_perform_complete(usb, pipe,
+							    transaction,
+							    CVMX_USB_COMPLETE_SUCCESS);
 		} else {
-			/* Split transactions retry the split complete 4 times then rewind
-			   to the start split and do the entire transactions again */
+			/* Split transactions retry the split complete 4 times
+			 * then rewind to the start split and do the entire
+			 * transactions again
+			 */
 			transaction->retries++;
 			if ((transaction->retries & 0x3) == 0) {
-				/* Rewind to the beginning of the transaction by anding off the
-				   split complete bit */
+				/* Rewind to the beginning of the transaction
+				 * by anding off the split complete bit
+				 */
 				transaction->stage &= ~1;
 				pipe->split_sc_frame = -1;
 			}
 		}
 	} else if (usbc_hcint.s.ack) {
 		transaction->retries = 0;
-		/* The ACK bit can only be checked after the other error bits. This is
-		   because a multi packet transfer may succeed in a number of packets
-		   and then get a different response on the last packet. In this case
-		   both ACK and the last response bit will be set. If none of the
-		   other response bits is set, then the last packet must have been an
-		   ACK */
-
-		/* Since we got an ACK, we know we don't need to do a ping on this
-		   pipe */
+		/* The ACK bit can only be checked after the other error bits.
+		 * This is because a multi packet transfer may succeed in a
+		 * number of packets and then get a different response on the
+		 * last packet.  In this case both ACK and the last response
+		 * bit will be set.  If none of the other response bits is set,
+		 * then the last packet must have been an ACK
+		 */
+
+		/* Since we got an ACK, we know we don't need to do a ping on
+		 * this pipe
+		 */
 		pipe->flags &= ~__CVMX_USB_PIPE_FLAGS_NEED_PING;
 
 		switch (transaction->type) {
@@ -2847,14 +3251,18 @@ static int __cvmx_usb_poll_channel(cvmx_usb_internal_state_t * usb, int channel)
 			case CVMX_USB_STAGE_NON_CONTROL:
 			case CVMX_USB_STAGE_NON_CONTROL_SPLIT_COMPLETE:
 				/* This should be impossible */
-				__cvmx_usb_perform_complete(usb, pipe, transaction, CVMX_USB_COMPLETE_ERROR);
+				__cvmx_usb_perform_complete(usb, pipe,
+							    transaction,
+							    CVMX_USB_COMPLETE_ERROR);
 				break;
 			case CVMX_USB_STAGE_SETUP:
 				pipe->pid_toggle = 1;
 				if (__cvmx_usb_pipe_needs_split(usb, pipe))
 					transaction->stage = CVMX_USB_STAGE_SETUP_SPLIT_COMPLETE;
 				else {
-					cvmx_usb_control_header_t *header = cvmx_phys_to_ptr(transaction->control_header);
+					cvmx_usb_control_header_t *header =
+						cvmx_phys_to_ptr(transaction->control_header);
+
 					if (header->s.length)
 						transaction->stage = CVMX_USB_STAGE_DATA;
 					else
@@ -2863,7 +3271,9 @@ static int __cvmx_usb_poll_channel(cvmx_usb_internal_state_t * usb, int channel)
 				break;
 			case CVMX_USB_STAGE_SETUP_SPLIT_COMPLETE:
 				{
-					cvmx_usb_control_header_t *header = cvmx_phys_to_ptr(transaction->control_header);
+					cvmx_usb_control_header_t *header =
+						cvmx_phys_to_ptr(transaction->control_header);
+
 					if (header->s.length)
 						transaction->stage = CVMX_USB_STAGE_DATA;
 					else
@@ -2873,22 +3283,26 @@ static int __cvmx_usb_poll_channel(cvmx_usb_internal_state_t * usb, int channel)
 			case CVMX_USB_STAGE_DATA:
 				if (__cvmx_usb_pipe_needs_split(usb, pipe)) {
 					transaction->stage = CVMX_USB_STAGE_DATA_SPLIT_COMPLETE;
-					/* For setup OUT data that are splits, the hardware
-					   doesn't appear to count transferred data. Here
-					   we manually update the data transferred */
+					/* For setup OUT data that are splits,
+					 * the hardware doesn't appear to count
+					 * transferred data.  Here we manually
+					 * update the data transferred
+					 */
 					if (!usbc_hcchar.s.epdir) {
 						if (buffer_space_left < pipe->max_packet)
 							transaction->actual_bytes += buffer_space_left;
 						else
 							transaction->actual_bytes += pipe->max_packet;
 					}
-				} else if ((buffer_space_left == 0) || (bytes_in_last_packet < pipe->max_packet)) {
+				} else if ((buffer_space_left == 0) ||
+					   (bytes_in_last_packet < pipe->max_packet)) {
 					pipe->pid_toggle = 1;
 					transaction->stage = CVMX_USB_STAGE_STATUS;
 				}
 				break;
 			case CVMX_USB_STAGE_DATA_SPLIT_COMPLETE:
-				if ((buffer_space_left == 0) || (bytes_in_last_packet < pipe->max_packet)) {
+				if ((buffer_space_left == 0) ||
+				    (bytes_in_last_packet < pipe->max_packet)) {
 					pipe->pid_toggle = 1;
 					transaction->stage = CVMX_USB_STAGE_STATUS;
 				} else {
@@ -2899,10 +3313,14 @@ static int __cvmx_usb_poll_channel(cvmx_usb_internal_state_t * usb, int channel)
 				if (__cvmx_usb_pipe_needs_split(usb, pipe))
 					transaction->stage = CVMX_USB_STAGE_STATUS_SPLIT_COMPLETE;
 				else
-					__cvmx_usb_perform_complete(usb, pipe, transaction, CVMX_USB_COMPLETE_SUCCESS);
+					__cvmx_usb_perform_complete(usb, pipe,
+								    transaction,
+								    CVMX_USB_COMPLETE_SUCCESS);
 				break;
 			case CVMX_USB_STAGE_STATUS_SPLIT_COMPLETE:
-				__cvmx_usb_perform_complete(usb, pipe, transaction, CVMX_USB_COMPLETE_SUCCESS);
+				__cvmx_usb_perform_complete(usb, pipe,
+							    transaction,
+							    CVMX_USB_COMPLETE_SUCCESS);
 				break;
 			}
 			break;
@@ -2916,54 +3334,80 @@ static int __cvmx_usb_poll_channel(cvmx_usb_internal_state_t * usb, int channel)
 				if (transaction->stage == CVMX_USB_STAGE_NON_CONTROL)
 					transaction->stage = CVMX_USB_STAGE_NON_CONTROL_SPLIT_COMPLETE;
 				else {
-					if (buffer_space_left && (bytes_in_last_packet == pipe->max_packet))
+					if (buffer_space_left &&
+					    (bytes_in_last_packet == pipe->max_packet))
 						transaction->stage = CVMX_USB_STAGE_NON_CONTROL;
 					else {
 						if (transaction->type == CVMX_USB_TRANSFER_INTERRUPT)
 							pipe->next_tx_frame += pipe->interval;
-						__cvmx_usb_perform_complete(usb, pipe, transaction, CVMX_USB_COMPLETE_SUCCESS);
+						__cvmx_usb_perform_complete(usb,
+									    pipe,
+									    transaction,
+									    CVMX_USB_COMPLETE_SUCCESS);
 					}
 				}
 			} else {
 				if ((pipe->device_speed == CVMX_USB_SPEED_HIGH) &&
-				    (pipe->transfer_type == CVMX_USB_TRANSFER_BULK) && (pipe->transfer_dir == CVMX_USB_DIRECTION_OUT) && (usbc_hcint.s.nak))
+				    (pipe->transfer_type == CVMX_USB_TRANSFER_BULK) &&
+				    (pipe->transfer_dir == CVMX_USB_DIRECTION_OUT) &&
+				    (usbc_hcint.s.nak))
 					pipe->flags |= __CVMX_USB_PIPE_FLAGS_NEED_PING;
-				if (!buffer_space_left || (bytes_in_last_packet < pipe->max_packet)) {
+				if (!buffer_space_left ||
+				    (bytes_in_last_packet < pipe->max_packet)) {
 					if (transaction->type == CVMX_USB_TRANSFER_INTERRUPT)
 						pipe->next_tx_frame += pipe->interval;
-					__cvmx_usb_perform_complete(usb, pipe, transaction, CVMX_USB_COMPLETE_SUCCESS);
+					__cvmx_usb_perform_complete(usb, pipe,
+								    transaction,
+								    CVMX_USB_COMPLETE_SUCCESS);
 				}
 			}
 			break;
 		case CVMX_USB_TRANSFER_ISOCHRONOUS:
 			if (__cvmx_usb_pipe_needs_split(usb, pipe)) {
-				/* ISOCHRONOUS OUT splits don't require a complete split stage.
-				   Instead they use a sequence of begin OUT splits to transfer
-				   the data 188 bytes at a time. Once the transfer is complete,
-				   the pipe sleeps until the next schedule interval */
+				/* ISOCHRONOUS OUT splits don't require a
+				 * complete split stage.  Instead they use a
+				 * sequence of begin OUT splits to transfer
+				 * the data 188 bytes at a time.  Once the
+				 * transfer is complete, the pipe sleeps until
+				 * the next schedule interval
+				 */
 				if (pipe->transfer_dir == CVMX_USB_DIRECTION_OUT) {
-					/* If no space left or this wasn't a max size packet then
-					   this transfer is complete. Otherwise start it again
-					   to send the next 188 bytes */
-					if (!buffer_space_left || (bytes_this_transfer < 188)) {
+					/* If no space left or this wasn't a
+					 * max size packet then this transfer
+					 * is complete. Otherwise start it
+					 * again to send the next 188 bytes
+					 */
+					if (!buffer_space_left ||
+					    (bytes_this_transfer < 188)) {
 						pipe->next_tx_frame += pipe->interval;
-						__cvmx_usb_perform_complete(usb, pipe, transaction, CVMX_USB_COMPLETE_SUCCESS);
+						__cvmx_usb_perform_complete(usb,
+									    pipe,
+									    transaction,
+									    CVMX_USB_COMPLETE_SUCCESS);
 					}
 				} else {
 					if (transaction->stage == CVMX_USB_STAGE_NON_CONTROL_SPLIT_COMPLETE) {
-						/* We are in the incoming data phase. Keep getting
-						   data until we run out of space or get a small
-						   packet */
-						if ((buffer_space_left == 0) || (bytes_in_last_packet < pipe->max_packet)) {
+						/* We are in the incoming data
+						 * phase.  Keep getting data
+						 * until we run out of space or
+						 * get a small packet
+						 */
+						if ((buffer_space_left == 0) ||
+						    (bytes_in_last_packet < pipe->max_packet)) {
 							pipe->next_tx_frame += pipe->interval;
-							__cvmx_usb_perform_complete(usb, pipe, transaction, CVMX_USB_COMPLETE_SUCCESS);
+							__cvmx_usb_perform_complete(usb,
+										    pipe,
+										    transaction,
+										    CVMX_USB_COMPLETE_SUCCESS);
 						}
 					} else
 						transaction->stage = CVMX_USB_STAGE_NON_CONTROL_SPLIT_COMPLETE;
 				}
 			} else {
 				pipe->next_tx_frame += pipe->interval;
-				__cvmx_usb_perform_complete(usb, pipe, transaction, CVMX_USB_COMPLETE_SUCCESS);
+				__cvmx_usb_perform_complete(usb, pipe,
+							    transaction,
+							    CVMX_USB_COMPLETE_SUCCESS);
 			}
 			break;
 		}
@@ -2971,15 +3415,17 @@ static int __cvmx_usb_poll_channel(cvmx_usb_internal_state_t * usb, int channel)
 		/* If this was a split then clear our split in progress marker */
 		if (usb->active_split == transaction)
 			usb->active_split = NULL;
-		/* NAK as a response means the device couldn't accept the transaction,
-		   but it should be retried in the future. Rewind to the beginning of
-		   the transaction by anding off the split complete bit. Retry in the
-		   next interval */
+		/* NAK as a response means the device couldn't accept the
+		 * transaction, but it should be retried in the future.
+		 * Rewind to the beginning of the transaction by anding off
+		 * the split complete bit.  Retry in the next interval
+		 */
 		transaction->retries = 0;
 		transaction->stage &= ~1;
 		pipe->next_tx_frame += pipe->interval;
 		if (pipe->next_tx_frame < usb->frame_number)
-			pipe->next_tx_frame = usb->frame_number + pipe->interval - (usb->frame_number - pipe->next_tx_frame) % pipe->interval;
+			pipe->next_tx_frame = usb->frame_number + pipe->interval
+				- (usb->frame_number - pipe->next_tx_frame) % pipe->interval;
 	} else {
 		cvmx_usb_port_status_t port;
 		port = cvmx_usb_get_status((cvmx_usb_state_t *) usb);
@@ -2987,8 +3433,9 @@ static int __cvmx_usb_poll_channel(cvmx_usb_internal_state_t * usb, int channel)
 			/* We'll retry the exact same transaction again */
 			transaction->retries++;
 		} else {
-			/* We get channel halted interrupts with no result bits sets when the
-			   cable is unplugged */
+			/* We get channel halted interrupts with no result bits
+			 * sets when the cable is unplugged
+			 */
 			__cvmx_usb_perform_complete(usb, pipe, transaction, CVMX_USB_COMPLETE_ERROR);
 		}
 	}
@@ -3023,22 +3470,26 @@ cvmx_usb_status_t cvmx_usb_poll(cvmx_usb_state_t * state)
 	CVMX_USB_LOG_PARAM("%p", state);
 
 	/* Update the frame counter */
-	usbc_hfnum.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HFNUM(usb->index));
+	usbc_hfnum.u32 = __cvmx_usb_read_csr32(usb,
+					       CVMX_USBCX_HFNUM(usb->index));
 	if ((usb->frame_number & 0x3fff) > usbc_hfnum.s.frnum)
 		usb->frame_number += 0x4000;
 	usb->frame_number &= ~0x3fffull;
 	usb->frame_number |= usbc_hfnum.s.frnum;
 
 	/* Read the pending interrupts */
-	usbc_gintsts.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_GINTSTS(usb->index));
+	usbc_gintsts.u32 = __cvmx_usb_read_csr32(usb,
+						 CVMX_USBCX_GINTSTS(usb->index));
 
 	/* Clear the interrupts now that we know about them */
-	__cvmx_usb_write_csr32(usb, CVMX_USBCX_GINTSTS(usb->index), usbc_gintsts.u32);
+	__cvmx_usb_write_csr32(usb, CVMX_USBCX_GINTSTS(usb->index),
+			       usbc_gintsts.u32);
 
 	if (usbc_gintsts.s.rxflvl) {
 		/* RxFIFO Non-Empty (RxFLvl)
-		   Indicates that there is at least one packet pending to be read
-		   from the RxFIFO. */
+		 * Indicates that there is at least one packet pending to be
+		 * read from the RxFIFO.
+		 */
 		/* In DMA mode this is handled by hardware */
 		if (usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_NO_DMA)
 			__cvmx_usb_poll_rx_fifo(usb);
@@ -3051,35 +3502,43 @@ cvmx_usb_status_t cvmx_usb_poll(cvmx_usb_state_t * state)
 	if (usbc_gintsts.s.disconnint || usbc_gintsts.s.prtint) {
 		cvmx_usbcx_hprt_t usbc_hprt;
 		/* Disconnect Detected Interrupt (DisconnInt)
-		   Asserted when a device disconnect is detected. */
+		 * Asserted when a device disconnect is detected.
+		 */
 
 		/* Host Port Interrupt (PrtInt)
-		   The core sets this bit to indicate a change in port status of one
-		   of the O2P USB core ports in Host mode. The application must
-		   read the Host Port Control and Status (HPRT) register to
-		   determine the exact event that caused this interrupt. The
-		   application must clear the appropriate status bit in the Host Port
-		   Control and Status register to clear this bit. */
+		 * The core sets this bit to indicate a change in port status of one
+		 * of the O2P USB core ports in Host mode. The application must
+		 * read the Host Port Control and Status (HPRT) register to
+		 * determine the exact event that caused this interrupt. The
+		 * application must clear the appropriate status bit in the Host Port
+		 * Control and Status register to clear this bit.
+		 */
 
 		/* Call the user's port callback */
-		__cvmx_usb_perform_callback(usb, NULL, NULL, CVMX_USB_CALLBACK_PORT_CHANGED, CVMX_USB_COMPLETE_SUCCESS);
+		__cvmx_usb_perform_callback(usb, NULL, NULL,
+					    CVMX_USB_CALLBACK_PORT_CHANGED,
+					    CVMX_USB_COMPLETE_SUCCESS);
 		/* Clear the port change bits */
-		usbc_hprt.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HPRT(usb->index));
+		usbc_hprt.u32 = __cvmx_usb_read_csr32(usb,
+						      CVMX_USBCX_HPRT(usb->index));
 		usbc_hprt.s.prtena = 0;
 		__cvmx_usb_write_csr32(usb, CVMX_USBCX_HPRT(usb->index), usbc_hprt.u32);
 	}
 	if (usbc_gintsts.s.hchint) {
 		/* Host Channels Interrupt (HChInt)
-		   The core sets this bit to indicate that an interrupt is pending on
-		   one of the channels of the core (in Host mode). The application
-		   must read the Host All Channels Interrupt (HAINT) register to
-		   determine the exact number of the channel on which the
-		   interrupt occurred, and then read the corresponding Host
-		   Channel-n Interrupt (HCINTn) register to determine the exact
-		   cause of the interrupt. The application must clear the
-		   appropriate status bit in the HCINTn register to clear this bit. */
+		 * The core sets this bit to indicate that an interrupt is
+		 * pending on one of the channels of the core (in Host mode).
+		 * The application must read the Host All Channels Interrupt
+		 * (HAINT) register to determine the exact number of the
+		 * channel on which the interrupt occurred, and then read
+		 * the corresponding Host Channel-n Interrupt (HCINTn)
+		 * register to determine the exact cause of the interrupt.
+		 * The application must clear the appropriate status bit in
+		 * the HCINTn register to clear this bit.
+		 */
 		cvmx_usbcx_haint_t usbc_haint;
-		usbc_haint.u32 = __cvmx_usb_read_csr32(usb, CVMX_USBCX_HAINT(usb->index));
+		usbc_haint.u32 = __cvmx_usb_read_csr32(usb,
+						       CVMX_USBCX_HAINT(usb->index));
 		while (usbc_haint.u32) {
 			int channel;
 			CVMX_CLZ(channel, usbc_haint.u32);
diff --git a/arch/mips/cavium-octeon/executive/octeon-feature.c b/arch/mips/cavium-octeon/executive/octeon-feature.c
index 890ad59..ce81e4b 100644
--- a/arch/mips/cavium-octeon/executive/octeon-feature.c
+++ b/arch/mips/cavium-octeon/executive/octeon-feature.c
@@ -226,8 +226,8 @@ int octeon_clear_attr(octeon_attr_t attr)
 void __init octeon_attr_init(void)
 {
 #ifndef CVMX_BUILD_FOR_LINUX_KERNEL
-	if (cvmx_coremask_is_first_core(&cvmx_sysinfo_get()->core_mask))
-		octeon_set_attr((int)OCTEON_ATTR_FIRST_CORE);
+	if (cvmx_is_init_core())
+		octeon_set_attr((int)OCTEON_ATTR_INIT_CORE);
 #endif
 }
 
diff --git a/arch/mips/include/asm/octeon/cvmx-app-hotplug.h b/arch/mips/include/asm/octeon/cvmx-app-hotplug.h
index ac75f78..0429cd9 100644
--- a/arch/mips/include/asm/octeon/cvmx-app-hotplug.h
+++ b/arch/mips/include/asm/octeon/cvmx-app-hotplug.h
@@ -125,7 +125,7 @@ struct cvmx_app_hotplug_global {
 typedef struct cvmx_app_hotplug_global cvmx_app_hotplug_global_t;
 
 int is_core_being_hot_plugged(void);
-int is_app_being_booted_or_shutdown(void);
+int is_app_under_boot_or_shutdown(void);
 void set_app_unber_boot(int val);
 void set_app_under_shutdown(int val);
 int cvmx_app_hotplug_shutdown_request(const struct cvmx_coremask *, int);
diff --git a/arch/mips/include/asm/octeon/cvmx-app-init.h b/arch/mips/include/asm/octeon/cvmx-app-init.h
index ba70ee1..20a7435 100644
--- a/arch/mips/include/asm/octeon/cvmx-app-init.h
+++ b/arch/mips/include/asm/octeon/cvmx-app-init.h
@@ -41,7 +41,7 @@
  * @file
  * Header file for simple executive application initialization.  This defines
  * part of the ABI between the bootloader and the application.
- * <hr>$Revision: 84656 $<hr>
+ * <hr>$Revision: 86527 $<hr>
  *
  */
 
diff --git a/arch/mips/include/asm/octeon/cvmx-ase-defs.h b/arch/mips/include/asm/octeon/cvmx-ase-defs.h
index 3e5199a..3faa665 100644
--- a/arch/mips/include/asm/octeon/cvmx-ase-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-ase-defs.h
@@ -405,8 +405,8 @@ typedef union cvmx_ase_backdoor_req_ctl cvmx_ase_backdoor_req_ctl_t;
  * cvmx_ase_backdoor_req_data#
  *
  * The lowest address is first beat (aka control word) and has the SOP. The next address is next
- * beat, etc. The ASE_BACKDOOR_REQ_CTL[CNT]th address has the EOP. See further information in
- * ASE_BACKDOOR_REQ_CTL.
+ * beat, etc. The register offset indicated by ASE_BACKDOOR_REQ_CTL[CNT] has the EOP. See further
+ * information in ASE_BACKDOOR_REQ_CTL.
  */
 union cvmx_ase_backdoor_req_datax {
 	uint64_t u64;
@@ -451,8 +451,8 @@ typedef union cvmx_ase_backdoor_rsp_ctl cvmx_ase_backdoor_rsp_ctl_t;
  * cvmx_ase_backdoor_rsp_data#
  *
  * The lowest address is first beat (aka control word) and has the SOP. The next address is next
- * beat, etc. The ASE_BACKDOOR_RSP_CTL[CNT]th address has the EOP. See further information in
- * ASE_BACKDOOR_RSP_CTL.
+ * beat, etc. The register offset indicated by ASE_BACKDOOR_RSP_CTL[CNT] has the EOP. See further
+ * information in ASE_BACKDOOR_RSP_CTL.
  */
 union cvmx_ase_backdoor_rsp_datax {
 	uint64_t u64;
@@ -617,7 +617,7 @@ typedef union cvmx_ase_config cvmx_ase_config_t;
  * cvmx_ase_ecc_ctl
  *
  * This register can be used to disable ECC checks, insert ECC errors.
- * Fields *ECC_DIS Disable SBE detection/correction and DBE detection. If ECC_DIS is 0x1, then no
+ * Fields *ECC_DIS disable SBE detection/correction and DBE detection. If ECC_DIS is 0x1, then no
  * errors are detected.
  * Fields *ECC_FLIP_SYND flip the syndrome<1:0> bits to generate 1-bit/2-bits error for testing.
  * 0x0 = normal operation
@@ -693,7 +693,7 @@ typedef union cvmx_ase_ecc_ctl cvmx_ase_ecc_ctl_t;
 /**
  * cvmx_ase_ecc_int
  *
- * This register contains the interrupt status for ECC failures. In all cases below EXCEPT
+ * This register contains the interrupt status for ECC failures. In all cases below, except
  * LUE_KDT_*, any request that generates an error has its response marked as errored. The
  * LUE_KDT_DBE error is not indicated in the response packet; the only indication of this error
  * is the interrupt mechanism.
@@ -776,7 +776,7 @@ typedef union cvmx_ase_ecc_int cvmx_ase_ecc_int_t;
  * cvmx_ase_gen_int
  *
  * This register contains the interrupt status for general ASE interrupts. Errors reported in bit
- * positions <38:32>, <7:2>, and <0> are most likely due to software programming errors.
+ * positions <39:32>, <7:2>, and <0> are most likely due to software programming errors.
  * In all LUE* cases below, any request that generates an error has its response marked as
  * errored. These LUE* interrupts are for diagnostic use, not for error handling. For all the
  * LUE* errors below, additional information can be obtained by reading ASE_LUE_ERROR_LOG.
diff --git a/arch/mips/include/asm/octeon/cvmx-bgx.h b/arch/mips/include/asm/octeon/cvmx-bgx.h
new file mode 100644
index 0000000..71ab334
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-bgx.h
@@ -0,0 +1,64 @@
+/***********************license start***************
+ * Copyright (c) 2013  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Functions to configure the BGX MAC.
+ *
+ * <hr>$Revision$<hr>
+ */
+
+#ifndef __CVMX_BGX_H__
+#define __CVMX_BGX_H__
+
+
+/**
+ * @INTERNAL
+ * Configure the bgx mac.
+ *
+ * @param interface Interface to bring up
+ *
+ * @param mode      Mode to configure the bgx mac as
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int bgx_init(int interface, cvmx_helper_interface_mode_t mode);
+
+#endif /* __CVMX_BGX_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-bgxx-defs.h b/arch/mips/include/asm/octeon/cvmx-bgxx-defs.h
index 9dc33d2..c4f17e6 100644
--- a/arch/mips/include/asm/octeon/cvmx-bgxx-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-bgxx-defs.h
@@ -2035,7 +2035,7 @@ union cvmx_bgxx_cmrx_int {
 	uint64_t reserved_3_63                : 61;
 	uint64_t pko_nxc                      : 1;  /**< TX channel out-of-range from PKO interface */
 	uint64_t overflw                      : 1;  /**< RX overflow. */
-	uint64_t pause_drp                    : 1;  /**< RX PAUSE packet was dropped due to full RXB FIFO. */
+	uint64_t pause_drp                    : 1;  /**< RX PAUSE packet was dropped due to full RXB FIFO or during per lmac reset. */
 #else
 	uint64_t pause_drp                    : 1;
 	uint64_t overflw                      : 1;
@@ -2111,11 +2111,11 @@ union cvmx_bgxx_cmrx_rx_bp_drop {
 	struct cvmx_bgxx_cmrx_rx_bp_drop_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_7_63                : 57;
-	uint64_t mark                         : 7;  /**< Number of eight-byte cycles to reserve in the RX FIFO. When the FIFO exceeds this count,
-                                                         packets are dropped and not buffered. MARK should typically be programmed to
-                                                         BGX*_CMR*_RX_LMACS[LMACS] + 1. Failure to program correctly can lead to system
-                                                         instability.
-                                                         MARK should be set considering FIFO partitioning established by BGX_CMR_RX_LMACS[LMACS]. */
+	uint64_t mark                         : 7;  /**< Number of eight-byte cycles to reserve in the RX FIFO. When When the number of free
+                                                         entries in the RX FIFO is less than or equal to MARK, incoming packet data is
+                                                         dropped. Mark additionally indicates the number of entries to reserve in the RX FIFO for
+                                                         closing partially received packets. MARK should typically be programmed to its reset
+                                                         value; failure to program correctly can lead to system instability. */
 #else
 	uint64_t mark                         : 7;
 	uint64_t reserved_7_63                : 57;
@@ -2285,7 +2285,7 @@ union cvmx_bgxx_cmrx_rx_pause_drop_time {
 	struct cvmx_bgxx_cmrx_rx_pause_drop_time_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_16_63               : 48;
-	uint64_t pause_time                   : 16; /**< Time extracted from the dropped PAUSE packet dropped due to RXB FIFO full */
+	uint64_t pause_time                   : 16; /**< Time extracted from the dropped PAUSE packet dropped due to RXB FIFO full or during per lmac reset */
 #else
 	uint64_t pause_time                   : 16;
 	uint64_t reserved_16_63               : 48;
@@ -3100,11 +3100,10 @@ union cvmx_bgxx_cmr_chan_msk_or {
 	uint64_t u64;
 	struct cvmx_bgxx_cmr_chan_msk_or_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t msk_or                       : 64; /**< Assert physical BP when the BP channel vector
-                                                         Assert physical backpressure when the backpressure channel vector combined with MSK_OR
+	uint64_t msk_or                       : 64; /**< Assert physical backpressure when the backpressure channel vector combined with MSK_OR
                                                          indicates backpressure as follows:
-                                                         phys_bp_msk_or = (CHAN_VECTOR<x:y> & MSK_AND<x:y>) != 0
-                                                         phys_bp = phys_bp_msk_or
+                                                         phys_bp_msk_or = (CHAN_VECTOR<x:y> & MSK_OR<x:y>) & MSK_OR<x:y>
+                                                         phys_bp = phys_bp_msk_or || phys_bp_msk_and
                                                          In single LMAC configurations, x = 63, y = 0
                                                          In multi-LMAC configurations, x/y are set as follows:
                                                          LMAC interface 0, x = 15, y = 0
@@ -3281,13 +3280,13 @@ union cvmx_bgxx_cmr_nxc_adr {
 	uint64_t u64;
 	struct cvmx_bgxx_cmr_nxc_adr_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_14_63               : 50;
-	uint64_t lmac_id                      : 2;  /**< Logged LMAC ID associated with NXC exceptions */
+	uint64_t reserved_16_63               : 48;
+	uint64_t lmac_id                      : 4;  /**< Logged LMAC ID associated with NXC exceptions */
 	uint64_t channel                      : 12; /**< Logged channel for NXC exceptions */
 #else
 	uint64_t channel                      : 12;
-	uint64_t lmac_id                      : 2;
-	uint64_t reserved_14_63               : 50;
+	uint64_t lmac_id                      : 4;
+	uint64_t reserved_16_63               : 48;
 #endif
 	} s;
 	struct cvmx_bgxx_cmr_nxc_adr_s        cn78xx;
@@ -3601,8 +3600,8 @@ union cvmx_bgxx_gmp_gmi_rxx_frm_ctl {
                                                          PKI_CL(0..3)_PKIND(0..63)_CFG[HG_EN] should be 0.
                                                          PKI_FRM_LEN_CHK(0..1)[MAXLEN] should be increased by 8.
                                                          PKI_FRM_LEN_CHK(0..1)[MINLEN] should be increased by 8.
-                                                         PIP_TAG_INC(0..63)[EN] should be adjusted.
-                                                         PIP_PRT_CFGB(0..63)[ALT_SKP_EN] should be 0. */
+                                                         PKI_TAG_INC(0..63)_MASK should be adjusted.
+                                                         This supported in uCode in O78 >>> PIP_PRT_CFGB(0..63)[ALT_SKP_EN] should be 0. */
 	uint64_t reserved_11_11               : 1;
 	uint64_t null_dis                     : 1;  /**< When set, do not modify the MOD bits on NULL ticks due to partial packets. */
 	uint64_t pre_align                    : 1;  /**< When set, PREAMBLE parser aligns the SFD byte regardless of the number of previous
@@ -4163,8 +4162,8 @@ union cvmx_bgxx_gmp_gmi_txx_thresh {
 	uint64_t u64;
 	struct cvmx_bgxx_gmp_gmi_txx_thresh_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_9_63                : 55;
-	uint64_t cnt                          : 9;  /**< Number of 128-bit words to accumulate in the TX FIFO before sending on the packet
+	uint64_t reserved_11_63               : 53;
+	uint64_t cnt                          : 11; /**< Number of 128-bit words to accumulate in the TX FIFO before sending on the packet
                                                          interface. This field should be large enough to prevent underflow on the packet interface
                                                          and must never be set to 0x0.
                                                          10G/40G Mode, CNT = 0x100. In all modes, this register cannot exceed the TX FIFO depth as
@@ -4173,8 +4172,8 @@ union cvmx_bgxx_gmp_gmi_txx_thresh {
                                                          BGX*_CMR*_TX_LMACS = 2:     CNT maximum = 0x3FF
                                                          BGX*_CMR*_TX_LMACS = 3,4:  CNT maximum = 0x1FF */
 #else
-	uint64_t cnt                          : 9;
-	uint64_t reserved_9_63                : 55;
+	uint64_t cnt                          : 11;
+	uint64_t reserved_11_63               : 53;
 #endif
 	} s;
 	struct cvmx_bgxx_gmp_gmi_txx_thresh_s cn78xx;
@@ -4923,10 +4922,10 @@ union cvmx_bgxx_smux_cbfc_ctl {
 	uint64_t bck_en                       : 1;  /**< Forward PFC/CBFC PAUSE information to the backpressure block. */
 	uint64_t drp_en                       : 1;  /**< Drop-control enable. When set, drop PFC/CBFC PAUSE frames. */
 	uint64_t tx_en                        : 1;  /**< Transmit enable. When set, allow for PFC/CBFC PAUSE packets. Must be clear in HiGig2 mode
-                                                         i.e. when BGX(0..5)_SMU(0..3)_TX_CTL[HG_EN] = 1 and BGX(0..5)_SMU(0..3)_RX_UDD_SKP[SKIP] =
+                                                         i.e. when BGX(0..5)_SMU(0..3)_TX_CTL[HG_EN] = 1 and BGX(0..5)_SMU(0..3)_RX_UDD_SKP[LEN] =
                                                          16. */
 	uint64_t rx_en                        : 1;  /**< Receive enable. When set, allow for PFC/CBFC PAUSE packets. Must be clear in HiGig2 mode
-                                                         i.e. when BGX(0..5)_SMU(0..3)_TX_CTL[HG_EN] = 1 and BGX(0..5)_SMU(0..3)_RX_UDD_SKP[SKIP] =
+                                                         i.e. when BGX(0..5)_SMU(0..3)_TX_CTL[HG_EN] = 1 and BGX(0..5)_SMU(0..3)_RX_UDD_SKP[LEN] =
                                                          16. */
 #else
 	uint64_t rx_en                        : 1;
@@ -5187,8 +5186,8 @@ union cvmx_bgxx_smux_rx_frm_ctl {
                                                          PKI_CL(0..3)_PKIND(0..63)_CFG[HG_EN] should be 0
                                                          PKI_FRM_LEN_CHK(0..1)[MAXLEN] should be increased by 8
                                                          PKI_FRM_LEN_CHK(0..1)[MINLEN] should be increased by 8
-                                                         PIP_TAG_INC(0..63)[EN] should be adjusted
-                                                         PIP_PRT_CFGB(0..63)[ALT_SKP_EN] should be 0. */
+                                                         PKI_TAG_INC(0..63)_MASK should be adjusted
+                                                         This supported in uCode in O78 >>> PIP_PRT_CFGB(0..63)[ALT_SKP_EN] should be 0. */
 	uint64_t reserved_6_11                : 6;
 	uint64_t ctl_smac                     : 1;  /**< Control PAUSE frames can match station SMAC. */
 	uint64_t ctl_mcst                     : 1;  /**< Control PAUSE frames can match globally assign multicast address. */
@@ -5408,17 +5407,17 @@ union cvmx_bgxx_smux_tx_ctl {
                                                          for normal operation. */
 	uint64_t hg_pause_hgi                 : 2;  /**< HGI field for hardware-generated HiGig PAUSE packets. */
 	uint64_t hg_en                        : 1;  /**< Enable HiGig mode.
-                                                         When this field is set and BGX(0..5)_SMU(0..3)_RX_UDD_SKP[SKIP] = 12, the interface is in
+                                                         When this field is set and BGX(0..5)_SMU(0..3)_RX_UDD_SKP[LEN] = 12, the interface is in
                                                          HiGig/HiGig+ mode and the following must be set:
                                                          BGX(0..5)_SMU(0..3)_RX_FRM_CTL[PRE_CHK] = 0
                                                          BGX(0..5)_SMU(0..3)_RX_UDD_SKP[FCSSEL] = 0
-                                                         BGX(0..5)_SMU(0..3)_RX_UDD_SKP[SKIP] = 12
+                                                         BGX(0..5)_SMU(0..3)_RX_UDD_SKP[LEN] = 12
                                                          BGX(0..5)_SMU(0..3)_TX_APPEND[PREAMBLE] = 0
-                                                         When this field is set and BGX(0..5)_SMU(0..3)_RX_UDD_SKP[SKIP] = 16, the interface is in
+                                                         When this field is set and BGX(0..5)_SMU(0..3)_RX_UDD_SKP[LEN] = 16, the interface is in
                                                          HiGig2 mode and the following must be set:
                                                          BGX(0..5)_SMU(0..3)_RX_FRM_CTL[PRE_CHK] = 0
                                                          BGX(0..5)_SMU(0..3)_RX_UDD_SKP[FCSSEL] = 0
-                                                         BGX(0..5)_SMU(0..3)_RX_UDD_SKP[SKIP] = 16
+                                                         BGX(0..5)_SMU(0..3)_RX_UDD_SKP[LEN] = 16
                                                          BGX(0..5)_SMU(0..3)_TX_APPEND[PREAMBLE] = 0
                                                          BGX(0..5)_SMU(0..3)_SMUX_CBFC_CTL[RX_EN] = 0
                                                          BGX(0..5)_SMU(0..3)_CBFC_CTL[TX_EN] = 0 */
diff --git a/arch/mips/include/asm/octeon/cvmx-ciu3-defs.h b/arch/mips/include/asm/octeon/cvmx-ciu3-defs.h
index 776ddfb..6013fea 100644
--- a/arch/mips/include/asm/octeon/cvmx-ciu3-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-ciu3-defs.h
@@ -676,9 +676,9 @@ union cvmx_ciu3_intr_ram_ecc_st {
 	uint64_t u64;
 	struct cvmx_ciu3_intr_ram_ecc_st_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_59_63               : 5;
-	uint64_t addr                         : 11; /**< Latch the address for latest SBE/DBE that occurred. */
-	uint64_t reserved_6_47                : 42;
+	uint64_t reserved_52_63               : 12;
+	uint64_t addr                         : 20; /**< Latch the address for latest SBE/DBE that occurred. */
+	uint64_t reserved_6_31                : 26;
 	uint64_t sisc_dbe                     : 1;  /**< SISC Double-bit error observed. Throws CIU_INTSN_E::CIU3_ECC_SISC_DBE. */
 	uint64_t sisc_sbe                     : 1;  /**< SISC Single-bit error observed. Throws CIU_INTSN_E::CIU3_ECC_SISC_SBE. */
 	uint64_t idt_dbe                      : 1;  /**< IDT Double-bit error observed. Throws CIU_INTSN_E::CIU3_ECC_IDT_DBE. */
@@ -692,9 +692,9 @@ union cvmx_ciu3_intr_ram_ecc_st {
 	uint64_t idt_dbe                      : 1;
 	uint64_t sisc_sbe                     : 1;
 	uint64_t sisc_dbe                     : 1;
-	uint64_t reserved_6_47                : 42;
-	uint64_t addr                         : 11;
-	uint64_t reserved_59_63               : 5;
+	uint64_t reserved_6_31                : 26;
+	uint64_t addr                         : 20;
+	uint64_t reserved_52_63               : 12;
 #endif
 	} s;
 	struct cvmx_ciu3_intr_ram_ecc_st_s    cn78xx;
diff --git a/arch/mips/include/asm/octeon/cvmx-clock.h b/arch/mips/include/asm/octeon/cvmx-clock.h
index fede873..06e7490 100644
--- a/arch/mips/include/asm/octeon/cvmx-clock.h
+++ b/arch/mips/include/asm/octeon/cvmx-clock.h
@@ -146,6 +146,7 @@ static inline uint64_t cvmx_clock_get_rate(cvmx_clock_t clock)
 }
 #else
 extern uint64_t cvmx_clock_get_rate(cvmx_clock_t clock);
+extern uint64_t cvmx_clock_get_rate_node(int node, cvmx_clock_t clock);
 #endif
 
 #ifdef	__cplusplus
diff --git a/arch/mips/include/asm/octeon/cvmx-coremask.h b/arch/mips/include/asm/octeon/cvmx-coremask.h
index 00b590e..ec6c093 100644
--- a/arch/mips/include/asm/octeon/cvmx-coremask.h
+++ b/arch/mips/include/asm/octeon/cvmx-coremask.h
@@ -60,7 +60,7 @@
  * provide future compatibility if more cores are added to future processors
  * or more nodes are supported.
  *
- * <hr>$Revision: 83933 $<hr>
+ * <hr>$Revision: 87283 $<hr>
  *
  */
 
@@ -371,8 +371,16 @@ static inline uint32_t cvmx_coremask_get32(const cvmx_coremask_t *pcm)
 static inline int cvmx_coremask_cmp(const cvmx_coremask_t *pcm1,
 				    const cvmx_coremask_t *pcm2)
 {
-	return memcmp((void *)pcm1->coremask_bitmap,
-	    (void *)pcm2->coremask_bitmap, CVMX_MAX_USED_CORES_BMP / 8);
+	int i;
+	/* Start from highest node for arithemtically correct result */
+	for ( i = CVMX_COREMASK_USED_BMPSZ-1; i >= 0 ; i-- )
+		if( pcm1->coremask_bitmap[i] != pcm2->coremask_bitmap[i] )
+			return (
+				pcm1->coremask_bitmap[i] -
+				pcm2->coremask_bitmap[i] 
+				);
+
+	return 0;
 }
 
 /*
@@ -607,6 +615,7 @@ cvmx_coremask_is_core_first_core(const cvmx_coremask_t *pcm,
 	return (__builtin_ffs(pcm->coremask_bitmap[n]) == core + 1);
 }
 
+#if	0	//Removed in favor of cvmx_is_init_core()
 /**
  * Test to see if current core is first core in coremask.
  *
@@ -621,7 +630,7 @@ cvmx_coremask_is_first_core(const cvmx_coremask_t *pcm)
 	return cvmx_coremask_is_core_first_core(pcm,
 						cvmx_get_core_num());
 }
-
+#endif
 /**
  * Returns the number of 1 bits set in a coremask
  *
diff --git a/arch/mips/include/asm/octeon/cvmx-dpi-defs.h b/arch/mips/include/asm/octeon/cvmx-dpi-defs.h
index bf9acfe..71320c1 100644
--- a/arch/mips/include/asm/octeon/cvmx-dpi-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-dpi-defs.h
@@ -566,6 +566,17 @@ static inline uint64_t CVMX_DPI_SLI_PRTX_ERR_INFO(unsigned long offset)
 #else
 #define CVMX_DPI_SLI_PRTX_ERR_INFO(offset) (CVMX_ADD_IO_SEG(0x0001DF0000000940ull) + ((offset) & 3) * 8)
 #endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_DPI_SWA_Q_VMID CVMX_DPI_SWA_Q_VMID_FUNC()
+static inline uint64_t CVMX_DPI_SWA_Q_VMID_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_DPI_SWA_Q_VMID not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001DF0000000030ull);
+}
+#else
+#define CVMX_DPI_SWA_Q_VMID (CVMX_ADD_IO_SEG(0x0001DF0000000030ull))
+#endif
 
 /**
  * cvmx_dpi_bist_status
@@ -1583,17 +1594,17 @@ union cvmx_dpi_ecc_int {
 	uint64_t u64;
 	struct cvmx_dpi_ecc_int_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_55_63               : 9;
-	uint64_t ram_dbe                      : 23; /**< Set when a double-bit error is detected in the corresponding ram. Throws
+	uint64_t reserved_63_63               : 1;
+	uint64_t ram_dbe                      : 31; /**< Set when a double-bit error is detected in the corresponding ram. Throws
                                                          DPI_INTSN_E::DPI_ERR_RAM_DBE. */
-	uint64_t reserved_23_31               : 9;
-	uint64_t ram_sbe                      : 23; /**< Set when a single-bit error is detected in the corresponding ram. Throws
+	uint64_t reserved_31_31               : 1;
+	uint64_t ram_sbe                      : 31; /**< Set when a single-bit error is detected in the corresponding ram. Throws
                                                          DPI_INTSN_E::DPI_ERR_RAM_SBE. */
 #else
-	uint64_t ram_sbe                      : 23;
-	uint64_t reserved_23_31               : 9;
-	uint64_t ram_dbe                      : 23;
-	uint64_t reserved_55_63               : 9;
+	uint64_t ram_sbe                      : 31;
+	uint64_t reserved_31_31               : 1;
+	uint64_t ram_dbe                      : 31;
+	uint64_t reserved_63_63               : 1;
 #endif
 	} s;
 	struct cvmx_dpi_ecc_int_s             cn78xx;
@@ -2689,4 +2700,37 @@ union cvmx_dpi_sli_prtx_err_info {
 };
 typedef union cvmx_dpi_sli_prtx_err_info cvmx_dpi_sli_prtx_err_info_t;
 
+/**
+ * cvmx_dpi_swa_q_vmid
+ *
+ * This register defines.
+ *
+ */
+union cvmx_dpi_swa_q_vmid {
+	uint64_t u64;
+	struct cvmx_dpi_swa_q_vmid_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t vmid7                        : 8;  /**< The SWA VMID for Queue 7. */
+	uint64_t vmid6                        : 8;  /**< The SWA VMID for Queue 6. */
+	uint64_t vmid5                        : 8;  /**< The SWA VMID for Queue 5. */
+	uint64_t vmid4                        : 8;  /**< The SWA VMID for Queue 4. */
+	uint64_t vmid3                        : 8;  /**< The SWA VMID for Queue 3. */
+	uint64_t vmid2                        : 8;  /**< The SWA VMID for Queue 2. */
+	uint64_t vmid1                        : 8;  /**< The SWA VMID for Queue 1. */
+	uint64_t vmid0                        : 8;  /**< The SWA VMID for Queue 0. */
+#else
+	uint64_t vmid0                        : 8;
+	uint64_t vmid1                        : 8;
+	uint64_t vmid2                        : 8;
+	uint64_t vmid3                        : 8;
+	uint64_t vmid4                        : 8;
+	uint64_t vmid5                        : 8;
+	uint64_t vmid6                        : 8;
+	uint64_t vmid7                        : 8;
+#endif
+	} s;
+	struct cvmx_dpi_swa_q_vmid_s          cn78xx;
+};
+typedef union cvmx_dpi_swa_q_vmid cvmx_dpi_swa_q_vmid_t;
+
 #endif
diff --git a/arch/mips/include/asm/octeon/cvmx-global-resources.h b/arch/mips/include/asm/octeon/cvmx-global-resources.h
index 5e7375c..e12d06d 100644
--- a/arch/mips/include/asm/octeon/cvmx-global-resources.h
+++ b/arch/mips/include/asm/octeon/cvmx-global-resources.h
@@ -18,6 +18,14 @@
 #define CVMX_GR_TAG_PCAM(x,y,z) \
 	cvmx_get_gr_tag('c','v','m','_','p','c','a','m','_',(x+'0'),(y+'0'),(z+'0'),'.','.','.','.')
 
+#define CVMX_GR_TAG_CIU3_IDT(_n) \
+	cvmx_get_gr_tag('c','v','m','_','c','i','u','3','_', ((_n) + '0'),'_','i','d','t','.','.')
+
+
+/* Allocation of the 512 SW INTSTs (in the  12 bit SW INTSN space) */
+#define CVMX_GR_TAG_CIU3_SWINTSN(_n) \
+	cvmx_get_gr_tag('c','v','m','_','c','i','u','3','_', ((_n) + '0'),'_','s','w','i','s','n')
+
 
 #define TAG_INIT_PART(A,B,C,D,E,F,G,H) ( \
 	(((uint64_t)(A) & 0xff) << 56) | (((uint64_t)(B) & 0xff) << 48) | (((uint64_t)(C) & 0xff) << 40)  | (((uint64_t)(D) & 0xff) << 32) | \
@@ -83,10 +91,10 @@ int cvmx_allocate_global_resource_range(struct global_resource_tag tag,
  * @param allocated_elements returns indexs of the allocated entries.
  * @return returns 0 on success and -1 on failure.
  */
-int cvmx_allocate_global_resource_range_non_contiguos(struct global_resource_tag tag,
-						      uint64_t owner,
-						      int nelements,
-						      int allocated_elements[]);
+int cvmx_resource_alloc_many(struct global_resource_tag tag,
+			     uint64_t owner,
+			     int nelements,
+			     int allocated_elements[]);
 /*
  * @INTERNAL
  * Reserve nelements starting from base in the global resource range with the
@@ -157,10 +165,6 @@ void  cvmx_show_global_resource_range(struct global_resource_tag tag);
  */
 void cvmx_global_resources_show(void);
 
-int cvmx_allocate_global_resource_range_non_contiguous(struct global_resource_tag tag,
-						      uint64_t owner,
-						      int nelements,
-						      int allocated_elements[]);
 
 #endif
 
diff --git a/arch/mips/include/asm/octeon/cvmx-gser.h b/arch/mips/include/asm/octeon/cvmx-gser.h
new file mode 100644
index 0000000..7fa0682
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-gser.h
@@ -0,0 +1,63 @@
+/***********************license start***************
+ * Copyright (c) 2013  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Helper utilities for gser.
+ *
+ * <hr>$Revision$<hr>
+ */
+
+#ifndef __CVMX_GSER_H__
+#define __CVMX_GSER_H__
+
+/**
+ * @INTERNAL
+ * Configure the gser.
+ *
+ * @param interface Interface to bring up
+ *
+ * @param mode      Mode to configure the gser as
+ *
+ * @return Zero on success, negative on failure
+ */
+int gser_init(int interface, cvmx_helper_interface_mode_t mode);
+
+#endif /* __CVMX_GSER_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-gserx-defs.h b/arch/mips/include/asm/octeon/cvmx-gserx-defs.h
index d9024af..82a1427 100644
--- a/arch/mips/include/asm/octeon/cvmx-gserx-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-gserx-defs.h
@@ -537,6 +537,17 @@ static inline uint64_t CVMX_GSERX_DLMX_TX_TERM_OFFSET(unsigned long offset, unsi
 #define CVMX_GSERX_DLMX_TX_TERM_OFFSET(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090003040ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
 #endif
 #if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_IDDQ_MODE(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_IDDQ_MODE(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090000018ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_IDDQ_MODE(block_id) (CVMX_ADD_IO_SEG(0x0001180090000018ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
 static inline uint64_t CVMX_GSERX_LANEX_PX_MODE_0(unsigned long a, unsigned long b, unsigned long c)
 {
 	if (!(
@@ -559,6 +570,39 @@ static inline uint64_t CVMX_GSERX_LANEX_PX_MODE_1(unsigned long a, unsigned long
 #define CVMX_GSERX_LANEX_PX_MODE_1(a, b, c) (CVMX_ADD_IO_SEG(0x00011800904E0048ull) + ((a) << 24) + ((b) << 20) + ((c) << 5))
 #endif
 #if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_RX_VALBBD_CTRL_0(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_RX_VALBBD_CTRL_0(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090440240ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_RX_VALBBD_CTRL_0(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090440240ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_RX_VALBBD_CTRL_1(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_RX_VALBBD_CTRL_1(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090440248ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_RX_VALBBD_CTRL_1(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090440248ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_RX_VALBBD_CTRL_2(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_RX_VALBBD_CTRL_2(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090440250ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_RX_VALBBD_CTRL_2(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090440250ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
 static inline uint64_t CVMX_GSERX_LANEX_VMA_COARSE_CTRL_0(unsigned long offset, unsigned long block_id)
 {
 	if (!(
@@ -1032,15 +1076,26 @@ static inline uint64_t CVMX_GSERX_RX_EIE_DETSTS(unsigned long block_id)
 #define CVMX_GSERX_RX_EIE_DETSTS(block_id) (CVMX_ADD_IO_SEG(0x0001180090000150ull) + ((block_id) & 15) * 0x1000000ull)
 #endif
 #if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_RX_EIE_FILTER(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_RX_EIE_FILTER(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090000158ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_RX_EIE_FILTER(block_id) (CVMX_ADD_IO_SEG(0x0001180090000158ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
 static inline uint64_t CVMX_GSERX_RX_POLARITY(unsigned long block_id)
 {
 	if (!(
 	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
 		cvmx_warn("CVMX_GSERX_RX_POLARITY(%lu) is invalid on this chip\n", block_id);
-	return CVMX_ADD_IO_SEG(0x0001180090000158ull) + ((block_id) & 15) * 0x1000000ull;
+	return CVMX_ADD_IO_SEG(0x0001180090000160ull) + ((block_id) & 15) * 0x1000000ull;
 }
 #else
-#define CVMX_GSERX_RX_POLARITY(block_id) (CVMX_ADD_IO_SEG(0x0001180090000158ull) + ((block_id) & 15) * 0x1000000ull)
+#define CVMX_GSERX_RX_POLARITY(block_id) (CVMX_ADD_IO_SEG(0x0001180090000160ull) + ((block_id) & 15) * 0x1000000ull)
 #endif
 #if CVMX_ENABLE_CSR_ADDRESS_CHECKING
 static inline uint64_t CVMX_GSERX_RX_PSTATE(unsigned long block_id)
@@ -1197,17 +1252,6 @@ static inline uint64_t CVMX_GSERX_SATA_TX_INVERT(unsigned long block_id)
 #define CVMX_GSERX_SATA_TX_INVERT(block_id) (CVMX_ADD_IO_SEG(0x0001180090100220ull))
 #endif
 #if CVMX_ENABLE_CSR_ADDRESS_CHECKING
-static inline uint64_t CVMX_GSERX_SLICEX_PX_MODE(unsigned long a, unsigned long b, unsigned long c)
-{
-	if (!(
-	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((a <= 13)) && ((b <= 1)) && ((c <= 11))))))
-		cvmx_warn("CVMX_GSERX_SLICEX_PX_MODE(%lu,%lu,%lu) is invalid on this chip\n", a, b, c);
-	return CVMX_ADD_IO_SEG(0x0001180090560228ull) + ((a) << 24) + ((b) << 20) + ((c) << 3);
-}
-#else
-#define CVMX_GSERX_SLICEX_PX_MODE(a, b, c) (CVMX_ADD_IO_SEG(0x0001180090560228ull) + ((a) << 24) + ((b) << 20) + ((c) << 3))
-#endif
-#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
 static inline uint64_t CVMX_GSERX_SPD(unsigned long block_id)
 {
 	if (!(
@@ -1265,13 +1309,15 @@ union cvmx_gserx_ana_atest {
                                                          test block, for non-OCI links.
                                                          Note that the OCI0 register is tied to the analog
                                                          test block, for OCI links.
-                                                         The other QLM GSER_ANA_DAC_B registers are unused. */
+                                                         The other QLM GSER_ANA_DAC_B registers are unused.
+                                                         It is not expected that software will need to use this register. */
 	uint64_t ana_dac_a                    : 5;  /**< Used to control A-side DAC input to the analog test block.
                                                          Note that the QLM0 register is tied to the analog
                                                          test block, for non-OCI links.
                                                          Note that the OCI0 register is tied to the analog
                                                          test block, for OCI links.
-                                                         The other QLM GSER_ANA_DAC_A registers are unused. */
+                                                         The other QLM GSER_ANA_DAC_A registers are unused.
+                                                         It is not expected that software will need to use this register. */
 #else
 	uint64_t ana_dac_a                    : 5;
 	uint64_t ana_dac_b                    : 7;
@@ -1295,7 +1341,8 @@ union cvmx_gserx_ana_sel {
                                                          test block, for non-OCI links.
                                                          Note that the QLM8 register is tied to the analog
                                                          test block, for OCI links.
-                                                         The other QLM GSER_ANA_SEL registers are unused. */
+                                                         The other QLM GSER_ANA_SEL registers are unused.
+                                                         It is not expected that software will need to use this register. */
 #else
 	uint64_t ana_sel                      : 9;
 	uint64_t reserved_9_63                : 55;
@@ -1313,20 +1360,27 @@ union cvmx_gserx_br_rxx_ctl {
 	struct cvmx_gserx_br_rxx_ctl_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_3_63                : 61;
-	uint64_t rxt_swm                      : 1;  /**< Set when RX Base-R Link Training is to be performed under software control. */
-	uint64_t rxt_preset                   : 1;  /**< When in SW Base-R Training Mode, this bit is used to determine how to
-                                                         set the preset bit. When preset is set, RX training is disabled.
-                                                         To perform a preset, set this bit prior to setting training enable.
+	uint64_t rxt_swm                      : 1;  /**< Set when RX Base-R Link Training is to be performed under software control.
+                                                         It is not expected that Link Training will need to be performed under
+                                                         software control. */
+	uint64_t rxt_preset                   : 1;  /**< For all link training, this bit determines how to configure the Preset bit
+                                                         in the Coeffient Update Message that is sent to the far end transmitter.
+                                                         When set, a one time request is made that the coefficients be set to a state where
+                                                         equalization is turned off.
+                                                         To perform a preset, set this bit prior to link training. link training needs to
+                                                         be disabled to complete the request and get the rxtrain state machine back to IDLE.
                                                          Note that it is illegal to set both the preset and initialize bits at the same time.
-                                                          1 = Preset is set.  A single CU message is sent to the link partner.
-                                                          0 = Preset is clear. */
-	uint64_t rxt_initialize               : 1;  /**< When in SW Base-R Training Mode, this bit is used to determine how to
-                                                         set the initialize bit the the Coefficient Update Message at the start
+                                                         It is not expected that software will need to set this bit. */
+	uint64_t rxt_initialize               : 1;  /**< For all link training, this bit determines how to configure the Initialize bit
+                                                         in the Coeffient Update Message that is sent to the far end transmitter.
                                                          of RX training.
-                                                         To perform a initialize, set this bit prior to setting training enable.
+                                                         When set, a request is made that the coefficients be set to its INITIALIZE state.
+                                                         To perform a initialize prior to link training, set this bit prior to perfomorming
+                                                         link training.
                                                          Note that it is illegal to set both the preset and initialize bits at the same time.
-                                                          1 = Training starts with initialize set.
-                                                          0 = Training starts with initialize clear. */
+                                                         Since the far end transmitter is required to be initialized prior to starting link
+                                                         training,
+                                                         it is not expected that software will need to set this bit. */
 #else
 	uint64_t rxt_initialize               : 1;
 	uint64_t rxt_preset                   : 1;
@@ -1347,7 +1401,10 @@ union cvmx_gserx_br_rxx_cu {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_9_63                : 55;
 	uint64_t rxt_cu                       : 9;  /**< When RX Base-R Link Training is being performed under software control,
-                                                         This is the Coefficient Update Message to send to the MAC (BGX/OCI). */
+                                                         (GSER_BR_RX(0..3)_CTL.RXT_SWM is set), this is the Coefficient Update Message
+                                                         to send to the MAC (BGX/OCI).
+                                                         It is not expected that software will need to perform link training under
+                                                         software control. */
 #else
 	uint64_t rxt_cu                       : 9;
 	uint64_t reserved_9_63                : 55;
@@ -1361,28 +1418,36 @@ typedef union cvmx_gserx_br_rxx_cu cvmx_gserx_br_rxx_cu_t;
  * cvmx_gser#_br_rx#_eer
  *
  * GSER SW Base-R RX Link Training Equalization Evaluation Request (EER)
- * A write to this register will perform a Equalization Request to the RAW PCS.
+ * A write to RXT_EER will initiate a  Equalization Request to the RAW PCS.
  * A read of this register will return the Equalization Status Message and a valid
  * bit indicating it was updated.
+ * It is not expected that software will need to perform EER requests during link training.
  */
 union cvmx_gserx_br_rxx_eer {
 	uint64_t u64;
 	struct cvmx_gserx_br_rxx_eer_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_15_63               : 49;
-	uint64_t rxt_esv                      : 1;  /**< When RX Base-R Link Training is being performed under software control,
-                                                         This is the bit indicating that the Equalization Status is valid.  Reading
-                                                         this register will clear this bit. */
-	uint64_t rxt_esm                      : 14; /**< When RX Base-R Link Training is being performed under software control,
-                                                         This is the Equalization Status Message from the RAW PCS.
-                                                         bits[13:6]: Figure of merit.
-                                                         bits[5:4]:  RX recommended TXPOST direction change.
-                                                         bits[3:2]:  RX recommended TXMAIN direction change.
-                                                         bits[5:4]:  RX recommended TXPRE direction change. */
+	uint64_t reserved_16_63               : 48;
+	uint64_t rxt_eer                      : 1;  /**< When RX Base-R Link Training is being performed under software control,
+                                                         (GSER_BR_RX(0..3)_CTL.RXT_SWM is set), writing this is the bit will initiate
+                                                         a Equalization Request to the RAW PCS. Reading this bit always returns a
+                                                         zero. */
+	uint64_t rxt_esv                      : 1;  /**< When performing a Equalization Request (RXT_EER), this bit will set
+                                                         indicating that the Equalization Status (RXT_ESM) is valid. When issueing
+                                                         a RXT_EER request, it is expected that RXT_ESV will get written to zero
+                                                         so that a valid RXT_ESM can be determined. */
+	uint64_t rxt_esm                      : 14; /**< When performing a Equalization Request (RXT_EER),
+                                                         this is the Equalization Status Message from the RAW PCS.
+                                                         It is valid with RXT_ESV is set.
+                                                         <13:6>: Figure of merit.
+                                                         <5:4>:  RX recommended TXPOST direction change.
+                                                         <3:2>:  RX recommended TXMAIN direction change.
+                                                         <1:0>:  RX recommended TXPRE direction change. */
 #else
 	uint64_t rxt_esm                      : 14;
 	uint64_t rxt_esv                      : 1;
-	uint64_t reserved_15_63               : 49;
+	uint64_t rxt_eer                      : 1;
+	uint64_t reserved_16_63               : 48;
 #endif
 	} s;
 	struct cvmx_gserx_br_rxx_eer_s        cn78xx;
@@ -1398,7 +1463,10 @@ union cvmx_gserx_br_rxx_sr {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_6_63                : 58;
 	uint64_t rxt_sr                       : 6;  /**< When RX Base-R Link Training is being performed under software control,
-                                                         This is the Status Response Message from the Link Partner. */
+                                                         (GSER_BR_RX(0..3)_CTL.RXT_SWM is set), this is the Status Report Message
+                                                         from the Link Partner.
+                                                         It is not expected that software will need to perform link training under
+                                                         software control. */
 #else
 	uint64_t rxt_sr                       : 6;
 	uint64_t reserved_6_63                : 58;
@@ -1416,7 +1484,9 @@ union cvmx_gserx_br_txx_ctl {
 	struct cvmx_gserx_br_txx_ctl_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_1_63                : 63;
-	uint64_t txt_swm                      : 1;  /**< Set when TX Base-R Link Training is to be performed under software control. */
+	uint64_t txt_swm                      : 1;  /**< Set when TX Base-R Link Training is to be performed under software control.
+                                                         It is not expected that Link Training will need to be performed under
+                                                         software control. */
 #else
 	uint64_t txt_swm                      : 1;
 	uint64_t reserved_1_63                : 63;
@@ -1434,8 +1504,11 @@ union cvmx_gserx_br_txx_cu {
 	struct cvmx_gserx_br_txx_cu_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_9_63                : 55;
-	uint64_t txt_cu                       : 9;  /**< When in SW TX Base-R Link Training, this is the Coefficient Update
-                                                         message from the link partner. */
+	uint64_t txt_cu                       : 9;  /**< When TX Base-R Link Training is being performed under software control,
+                                                         (GSER_BR_TX(0..3)_CTL.TXT_SWM is set), this is the Coefficient Update Message
+                                                         from the Link Partner.
+                                                         It is not expected that software will need to perform link training under
+                                                         software control. */
 #else
 	uint64_t txt_cu                       : 9;
 	uint64_t reserved_9_63                : 55;
@@ -1453,11 +1526,14 @@ union cvmx_gserx_br_txx_cur {
 	struct cvmx_gserx_br_txx_cur_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_14_63               : 50;
-	uint64_t txt_cur                      : 14; /**< When in SW TX Base-R Link Training, this is the Coefficient Update
-                                                         to be written to the RAW PCS.
+	uint64_t txt_cur                      : 14; /**< When TX Base-R Link Training is being performed under software control,
+                                                         (GSER_BR_TX(0..3)_CTL.TXT_SWM is set), this is the Coefficient Update
+                                                         to be written to the PHY.
                                                          bits 13:9: TX_POST[4:0]
                                                          bits 8:4:  TX_SWING[4:0]
-                                                         bits 3:0:  TX_PRE[4:0] */
+                                                         bits 3:0:  TX_PRE[4:0]
+                                                         It is not expected that software will need to perform link training under
+                                                         software control. */
 #else
 	uint64_t txt_cur                      : 14;
 	uint64_t reserved_14_63               : 50;
@@ -1475,10 +1551,13 @@ union cvmx_gserx_br_txx_sr {
 	struct cvmx_gserx_br_txx_sr_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_6_63                : 58;
-	uint64_t txt_sr                       : 6;  /**< When in SW TX Base-R Link Training, this is the Status Response
-                                                         to be sent to the link partner.  Writing this message will cause
+	uint64_t txt_sr                       : 6;  /**< When TX Base-R Link Training is being performed under software control,
+                                                         (GSER_BR_TX(0..3)_CTL.TXT_SWM is set), this is the Status Report (SR)
+                                                         Message to be sent to the link partner. Writing this register will cause
                                                          a new SR message to be sent to the Mac (BGX/OCI) to be forwarded
-                                                         to the link partner. */
+                                                         to the link partner.
+                                                         It is not expected that software will need to perform link training under
+                                                         software control. */
 #else
 	uint64_t txt_sr                       : 6;
 	uint64_t reserved_6_63                : 58;
@@ -1496,21 +1575,23 @@ union cvmx_gserx_cfg {
 	struct cvmx_gserx_cfg_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_5_63                : 59;
-	uint64_t bgx_quad                     : 1;  /**< For non-OCI links, indicates the BGX is
-                                                         in quad aggregation mode when [BGX] is also set.
+	uint64_t bgx_quad                     : 1;  /**< For non-OCI links, indicates the BGX is in quad
+                                                         aggregation mode when GSER_CFG.BGX is also set.
                                                          A single controller is used for all 4 lanes.
                                                          For OCI links this bit has no meaning. */
 	uint64_t bgx_dual                     : 1;  /**< For non-OCI links, indicates the BGX is
-                                                         in dual aggregation mode when [BGX] is also set.
+                                                         in dual aggregation mode when GSER_CFG.BGX is also set.
                                                          A single controller is used for lanes 0 & 1 and
                                                          another controller is used for lanes 2 & 3.
                                                          For OCI links this bit has no meaning. */
-	uint64_t bgx                          : 1;  /**< For non-OCI links, indicates the GSER is configured for BGX mode. For OCI links this bit
-                                                         has no meaning. */
-	uint64_t ila                          : 1;  /**< For non-OCI links, indicates the GSER is configured for ILK/ILA mode. For OCI links, this
-                                                         indicates the GSER is configured for OCI mode. */
-	uint64_t pcie                         : 1;  /**< For non-OCI links, indicates the GSER is configured
-                                                         for PCIE mode.
+	uint64_t bgx                          : 1;  /**< For non-OCI links, indicates the GSER is configured for BGX mode.
+                                                         Only one of the BGX, ILA, or PCIE modes can be set at any one time.
+                                                         For OCI links this bit has no meaning. */
+	uint64_t ila                          : 1;  /**< For non-OCI links, indicates the GSER is configured for ILK/ILA mode.
+                                                         Only one of the BGX, ILA, or PCIE modes can be set at any one time.
+                                                         For OCI links, this bit has no meaning. */
+	uint64_t pcie                         : 1;  /**< For non-OCI links, indicates the GSER is configured for PCIE mode.
+                                                         Only one of the BGX, ILA, or PCIE modes can be set at any one time.
                                                          For OCI links this bit has no meaning. */
 #else
 	uint64_t pcie                         : 1;
@@ -1533,7 +1614,10 @@ union cvmx_gserx_dbg {
 	struct cvmx_gserx_dbg_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_1_63                : 63;
-	uint64_t rxqtm_on                     : 1;  /**< When asserted, the RX FIFOs are turned on. */
+	uint64_t rxqtm_on                     : 1;  /**< For non BGX/ILK configurations, setting this bit will
+                                                         enable the RX FIFOs. This allows recieved data to become
+                                                         visible to the RSL debug port.  It is not expected that
+                                                         SW will need to use this register. */
 #else
 	uint64_t rxqtm_on                     : 1;
 	uint64_t reserved_1_63                : 63;
@@ -2368,6 +2452,24 @@ union cvmx_gserx_dlmx_tx_term_offset {
 typedef union cvmx_gserx_dlmx_tx_term_offset cvmx_gserx_dlmx_tx_term_offset_t;
 
 /**
+ * cvmx_gser#_iddq_mode
+ */
+union cvmx_gserx_iddq_mode {
+	uint64_t u64;
+	struct cvmx_gserx_iddq_mode_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t phy_iddq_mode                : 1;  /**< When set, power downs all circuitry in PHY for IDDQ testing */
+#else
+	uint64_t phy_iddq_mode                : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_gserx_iddq_mode_s         cn78xx;
+};
+typedef union cvmx_gserx_iddq_mode cvmx_gserx_iddq_mode_t;
+
+/**
  * cvmx_gser#_lane#_p#_mode_0
  *
  * RAW PCS Per Lane Global Settings Mode 0 Register
@@ -2380,15 +2482,86 @@ union cvmx_gserx_lanex_px_mode_0 {
 	struct cvmx_gserx_lanex_px_mode_0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_15_63               : 49;
-	uint64_t ctle                         : 2;  /**< CTLE pole value. */
+	uint64_t ctle                         : 2;  /**< Continuous time linear equalizer pole configuration.
+                                                         0x0: ~5dB of peaking at 4 Ghz (Minimum bandwidth).
+                                                         0x1: ~10dB of peaking at 5 Ghz
+                                                         0x2: ~15dB of peaking at 5.5 Ghz
+                                                         0x3: ~20dB of peaking at 6 Ghz (Maximum bandwidth).
+                                                         Recommended Settings:
+                                                           25G_REFCLK100:                0x0
+                                                           5G_REFCLK100:                 0x0
+                                                           8G_REFCLK100:                 0x3
+                                                           125G_REFCLK15625_KX:          0x0
+                                                           3125G_REFCLK15625_XAUI:       0x0
+                                                           103215G_REFCLK15625_KR:       0x3
+                                                           125G_REFCLK15625_SGMII:       0x0
+                                                           5G_REFCLK15625_QSGMII:        0x0
+                                                           625G_REFCLK15625_RXAUI:       0x0
+                                                           25G_REFCLK125:                0x0
+                                                           5G_REFCLK125:                 0x0
+                                                           8G_REFCLK125:                 0x3 */
 	uint64_t pcie                         : 1;  /**< Selects between RX terminations.
-                                                         - 0: pcs_sds_rx_terminate_to_vdda
-                                                         - 1: VSS */
-	uint64_t tx_ldiv                      : 2;  /**< Configues clock divider used to determine the transmit rate. */
-	uint64_t rx_ldiv                      : 2;  /**< Configues clock divider used to determine the receive rate. */
-	uint64_t srate                        : 3;  /**< Used to generate strobe to effectively divide the clock down
-                                                         to a slower rate. */
-	uint64_t op_range                     : 1;  /**< When set DFE is enabled.  Otherwise DFE is disabled. */
+                                                         - 0: Differential termination
+                                                         - 1: Termination between pad and sds_vdds.
+                                                          Recommended Settings:
+                                                            25G_REFCLK100:                0x1
+                                                            5G_REFCLK100:                 0x1
+                                                            8G_REFCLK100:                 0x0
+                                                            125G_REFCLK15625_KX:          0x0
+                                                            3125G_REFCLK15625_XAUI:       0x0
+                                                            103215G_REFCLK15625_KR:       0x0
+                                                            125G_REFCLK15625_SGMII:       0x0
+                                                            5G_REFCLK15625_QSGMII:        0x0
+                                                            625G_REFCLK15625_RXAUI:       0x0
+                                                            25G_REFCLK125:                0x1
+                                                            5G_REFCLK125:                 0x1
+                                                            8G_REFCLK125:                 0x0 */
+	uint64_t tx_ldiv                      : 2;  /**< Configues clock divider used to determine the receive rate. Encoding is:
+                                                         0x0: full data rate
+                                                         0x1: 1/2 data rate
+                                                         0x2: 1/4 data rate
+                                                         0x3: 1/8 data rate
+                                                         Recommended Settings:
+                                                           25G_REFCLK100:                0x1
+                                                           5G_REFCLK100:                 0x0
+                                                           8G_REFCLK100:                 0x0
+                                                           125G_REFCLK15625_KX:          0x2
+                                                           3125G_REFCLK15625_XAUI:       0x1
+                                                           103215G_REFCLK15625_KR:       0x0
+                                                           125G_REFCLK15625_SGMII:       0x2
+                                                           5G_REFCLK15625_QSGMII:        0x0
+                                                           625G_REFCLK15625_RXAUI:       0x0
+                                                           25G_REFCLK125:                0x1
+                                                           5G_REFCLK125:                 0x0
+                                                           8G_REFCLK125:                 0x0 */
+	uint64_t rx_ldiv                      : 2;  /**< Configues clock divider used to determine the receive rate. Encoding is:
+                                                         0x0: full data rate
+                                                         0x1: 1/2 data rate
+                                                         0x2: 1/4 data rate
+                                                         0x3: 1/8 data rate
+                                                         Recommended Settings:
+                                                           25G_REFCLK100:                0x1
+                                                           5G_REFCLK100:                 0x0
+                                                           8G_REFCLK100:                 0x0
+                                                           125G_REFCLK15625_KX:          0x2
+                                                           3125G_REFCLK15625_XAUI:       0x1
+                                                           103215G_REFCLK15625_KR:       0x0
+                                                           125G_REFCLK15625_SGMII:       0x2
+                                                           5G_REFCLK15625_QSGMII:        0x0
+                                                           625G_REFCLK15625_RXAUI:       0x0
+                                                           25G_REFCLK125:                0x1
+                                                           5G_REFCLK125:                 0x0
+                                                           8G_REFCLK125:                 0x0 */
+	uint64_t srate                        : 3;  /**< Sample Rate, Used to generate strobe to effectively divide the clock down
+                                                         to a slower rate.  Encoding is:
+                                                           0x0: full rate
+                                                           0x1: 1/2 data rate
+                                                           0x2: 1/4 data rate
+                                                           0x3: 1/8 data rate
+                                                           0x4: 1/16 data rate
+                                                           else = Reserved.
+                                                         This field should always be set to zero (full rate). */
+	uint64_t reserved_4_4                 : 1;
 	uint64_t tx_mode                      : 2;  /**< TX Data Width:
                                                          0x0 = 8-bit  raw data (not supported).
                                                          0x1 = 10-bit raw data (not supported).
@@ -2402,7 +2575,7 @@ union cvmx_gserx_lanex_px_mode_0 {
 #else
 	uint64_t rx_mode                      : 2;
 	uint64_t tx_mode                      : 2;
-	uint64_t op_range                     : 1;
+	uint64_t reserved_4_4                 : 1;
 	uint64_t srate                        : 3;
 	uint64_t rx_ldiv                      : 2;
 	uint64_t tx_ldiv                      : 2;
@@ -2427,12 +2600,39 @@ union cvmx_gserx_lanex_px_mode_1 {
 	struct cvmx_gserx_lanex_px_mode_1_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_16_63               : 48;
-	uint64_t vma_kr_sel                   : 1;  /**< 0 = Disabled.  Coarse step adaptation selected.
-                                                         1 = Enabled.  Fine step adaptation selected. */
-	uint64_t vma_mm                       : 1;  /**< 0 = Adaptive DFE.
-                                                         1 = Manual DFE. */
-	uint64_t cdr_fgain                    : 4;  /**< CDR frequency gain. */
-	uint64_t ph_acc_adj                   : 10; /**< Phase accumulator adjust. */
+	uint64_t vma_kr_sel                   : 1;  /**< Values at reset:
+                                                         1 = Enabled.  Fine step adaptation selected (10.3125Gbps rate).
+                                                         0 = Disabled.  Coarse step adaptation selected (rates lower than 10.3125Gbps). */
+	uint64_t vma_mm                       : 1;  /**< Manual DFE verses Adaptive DFE mode.
+                                                         Recommended Settings:
+                                                         0 = Adaptive DFE (5Gbps and higher)
+                                                         1 = Manual DFE, fixed tap (3.125Gbps and lower). */
+	uint64_t cdr_fgain                    : 4;  /**< CDR frequency gain. Values at reset:
+                                                         25G_REFCLK100:                0xa
+                                                         5G_REFCLK100:                 0xa
+                                                         8G_REFCLK100:                 0xb
+                                                         125G_REFCLK15625_KX:          0xc
+                                                         3125G_REFCLK15625_XAUI:       0xc
+                                                         103215G_REFCLK15625_KR:       0xa
+                                                         125G_REFCLK15625_SGMII:       0xc
+                                                         5G_REFCLK15625_QSGMII:        0xc
+                                                         625G_REFCLK15625_RXAUI:       0xa
+                                                         25G_REFCLK125:                0xa
+                                                         5G_REFCLK125:                 0xa
+                                                         8G_REFCLK125:                 0xb */
+	uint64_t ph_acc_adj                   : 10; /**< Phase accumulator adjust. Values at reset:
+                                                         25G_REFCLK100:                0x14
+                                                         5G_REFCLK100:                 0x14
+                                                         8G_REFCLK100:                 0x23
+                                                         125G_REFCLK15625_KX:          0x1e
+                                                         3125G_REFCLK15625_XAUI:       0x1e
+                                                         103215G_REFCLK15625_KR:       0xf
+                                                         125G_REFCLK15625_SGMII:       0x1e
+                                                         5G_REFCLK15625_QSGMII:        0x1e
+                                                         625G_REFCLK15625_RXAUI:       0x14
+                                                         25G_REFCLK125:                0x14
+                                                         5G_REFCLK125:                 0x14
+                                                         8G_REFCLK125:                 0x23 */
 #else
 	uint64_t ph_acc_adj                   : 10;
 	uint64_t cdr_fgain                    : 4;
@@ -2446,23 +2646,167 @@ union cvmx_gserx_lanex_px_mode_1 {
 typedef union cvmx_gserx_lanex_px_mode_1 cvmx_gserx_lanex_px_mode_1_t;
 
 /**
+ * cvmx_gser#_lane#_rx_valbbd_ctrl_0
+ *
+ * RAW PCS Per Lane Coarse Adaptive Equalizer Control 0 Register
+ * Per Lane registers are specific to a paticular lane.
+ */
+union cvmx_gserx_lanex_rx_valbbd_ctrl_0 {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_rx_valbbd_ctrl_0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_14_63               : 50;
+	uint64_t agc_gain                     : 2;  /**< AGC Gain. */
+	uint64_t dfe_gain                     : 2;  /**< DFE Gain. */
+	uint64_t dfe_c5_mval                  : 4;  /**< DFE Tap5 Manual Value when GSER_LANE_RX_VALBBD_CTRL_2.DFE_OVRD_EN
+                                                         and GSER_LANE_RX_VALBBD_CTRL_2.DFE_C5_OVRD_VAL are both set. */
+	uint64_t dfe_c5_msgn                  : 1;  /**< DFE Tap5 Manual Sign when GSER_LANE_RX_VALBBD_CTRL_2.DFE_OVRD_EN
+                                                         and GSER_LANE_RX_VALBBD_CTRL_2.DFE_C5_OVRD_VAL are both set. */
+	uint64_t dfe_c4_mval                  : 4;  /**< DFE Tap4 Manual Value when GSER_LANE_RX_VALBBD_CTRL_2.DFE_OVRD_EN
+                                                         and GSER_LANE_RX_VALBBD_CTRL_2.DFE_C5_OVRD_VAL are both set. */
+	uint64_t dfe_c4_msgn                  : 1;  /**< DFE Tap4 Manual Sign when GSER_LANE_RX_VALBBD_CTRL_2.DFE_OVRD_EN
+                                                         and GSER_LANE_RX_VALBBD_CTRL_2.DFE_C5_OVRD_VAL are both set. */
+#else
+	uint64_t dfe_c4_msgn                  : 1;
+	uint64_t dfe_c4_mval                  : 4;
+	uint64_t dfe_c5_msgn                  : 1;
+	uint64_t dfe_c5_mval                  : 4;
+	uint64_t dfe_gain                     : 2;
+	uint64_t agc_gain                     : 2;
+	uint64_t reserved_14_63               : 50;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_rx_valbbd_ctrl_0_s cn78xx;
+};
+typedef union cvmx_gserx_lanex_rx_valbbd_ctrl_0 cvmx_gserx_lanex_rx_valbbd_ctrl_0_t;
+
+/**
+ * cvmx_gser#_lane#_rx_valbbd_ctrl_1
+ *
+ * RAW PCS Per Lane Coarse Adaptive Equalizer Control 1 Register
+ * Per Lane registers are specific to a paticular lane.
+ */
+union cvmx_gserx_lanex_rx_valbbd_ctrl_1 {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_rx_valbbd_ctrl_1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_15_63               : 49;
+	uint64_t dfe_c3_mval                  : 4;  /**< DFE Tap3 Manual Value when GSER_LANE_RX_VALBBD_CTRL_2.DFE_OVRD_EN
+                                                         and GSER_LANE_RX_VALBBD_CTRL_2.DFE_C5_OVRD_VAL are both set. */
+	uint64_t dfe_c3_msgn                  : 1;  /**< DFE Tap3 Manual Sign when GSER_LANE_RX_VALBBD_CTRL_2.DFE_OVRD_EN
+                                                         and GSER_LANE_RX_VALBBD_CTRL_2.DFE_C5_OVRD_VAL are both set. */
+	uint64_t dfe_c2_mval                  : 4;  /**< DFE Tap2 Manual Value when GSER_LANE_RX_VALBBD_CTRL_2.DFE_OVRD_EN
+                                                         and GSER_LANE_RX_VALBBD_CTRL_2.DFE_C5_OVRD_VAL are both set. */
+	uint64_t dfe_c2_msgn                  : 1;  /**< DFE Tap2 Manual Sign when GSER_LANE_RX_VALBBD_CTRL_2.DFE_OVRD_EN
+                                                         and GSER_LANE_RX_VALBBD_CTRL_2.DFE_C5_OVRD_VAL are both set. */
+	uint64_t dfe_c1_mval                  : 4;  /**< DFE Tap1 Manual Value when GSER_LANE_RX_VALBBD_CTRL_2.DFE_OVRD_EN
+                                                         and GSER_LANE_RX_VALBBD_CTRL_2.DFE_C5_OVRD_VAL are both set.
+                                                         Recommended Settings:
+                                                           For the following modes; 5G_REFCLK100, 5G_REFCLK15625_QSGMII, and
+                                                           5G_REFCLK125, it is recommended that DFE_C1_MVAL be set to zero
+                                                           after setting GSER_LANE_MODE_1.VMA_MM and also after updating
+                                                           the GSER_LANE_RX_VALBBD_CTRL_2 register.
+                                                           In all other modes this register can be ignored. */
+	uint64_t dfe_c1_msgn                  : 1;  /**< DFE Tap1 Manual Sign when GSER_LANE_RX_VALBBD_CTRL_2.DFE_OVRD_EN
+                                                         and GSER_LANE_RX_VALBBD_CTRL_2.DFE_C5_OVRD_VAL are both set. */
+#else
+	uint64_t dfe_c1_msgn                  : 1;
+	uint64_t dfe_c1_mval                  : 4;
+	uint64_t dfe_c2_msgn                  : 1;
+	uint64_t dfe_c2_mval                  : 4;
+	uint64_t dfe_c3_msgn                  : 1;
+	uint64_t dfe_c3_mval                  : 4;
+	uint64_t reserved_15_63               : 49;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_rx_valbbd_ctrl_1_s cn78xx;
+};
+typedef union cvmx_gserx_lanex_rx_valbbd_ctrl_1 cvmx_gserx_lanex_rx_valbbd_ctrl_1_t;
+
+/**
+ * cvmx_gser#_lane#_rx_valbbd_ctrl_2
+ *
+ * RAW PCS Per Lane Equalizer Control 2 Register
+ * Per Lane registers are specific to a paticular lane.
+ */
+union cvmx_gserx_lanex_rx_valbbd_ctrl_2 {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_rx_valbbd_ctrl_2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_6_63                : 58;
+	uint64_t dfe_ovrd_en                  : 1;  /**< Overide enable for DFE tap controls.
+                                                         When asserted, the register bits in the GSER_LANE_RX_VALBBD_CTRL_1
+                                                         and GSER_LANE_RX_VALBBD_CTRL_1 registers are used for controlling
+                                                         the DFE tap manual mode, instead the manual mode signal indexed by
+                                                         the GSER_LANE_MODE.LMODE.
+                                                         Recommended Settings:
+                                                           For the following modes; 5G_REFCLK100, 5G_REFCLK15625_QSGMII, and
+                                                           5G_REFCLK125, it is recommended that DFE tap controls be put in
+                                                           manual mode by setting this bit.  In all other modes this register
+                                                           can be ignored. */
+	uint64_t dfe_c5_ovrd_val              : 1;  /**< Overide value for DFE Tap5 Manual Enable.
+                                                         Recommended Settings:
+                                                           For the following modes; 5G_REFCLK100, 5G_REFCLK15625_QSGMII, and
+                                                           5G_REFCLK125, it is recommended that the DFE Tap5 Manual Enable be
+                                                           set after setting GSER_LANE_MODE_1.VMA_MM.
+                                                           In all other modes this register can be ignored. */
+	uint64_t dfe_c4_ovrd_val              : 1;  /**< Overide value for DFE Tap4 Manual Enable.
+                                                         Recommended Settings:
+                                                           For the following modes; 5G_REFCLK100, 5G_REFCLK15625_QSGMII, and
+                                                           5G_REFCLK125, it is recommended that the DFE Tap4 Manual Enable be
+                                                           set after setting GSER_LANE_MODE_1.VMA_MM.
+                                                           In all other modes this register can be ignored. */
+	uint64_t dfe_c3_ovrd_val              : 1;  /**< Overide value for DFE Tap3 Manual Enable.
+                                                         Recommended Settings:
+                                                           For the following modes; 5G_REFCLK100, 5G_REFCLK15625_QSGMII, and
+                                                           5G_REFCLK125, it is recommended that the DFE Tap3 Manual Enable be
+                                                           set after setting GSER_LANE_MODE_1.VMA_MM.
+                                                           In all other modes this register can be ignored. */
+	uint64_t dfe_c2_ovrd_val              : 1;  /**< Overide value for DFE Tap2 Manual Enable.
+                                                         Recommended Settings:
+                                                           For the following modes; 5G_REFCLK100, 5G_REFCLK15625_QSGMII, and
+                                                           5G_REFCLK125, it is recommended that the DFE Tap2 Manual Enable be
+                                                           set after setting GSER_LANE_MODE_1.VMA_MM.
+                                                           In all other modes this register can be ignored. */
+	uint64_t dfe_c1_ovrd_val              : 1;  /**< Overide value for DFE Tap1 Manual Enable.
+                                                         Recommended Settings:
+                                                           For the following modes; 5G_REFCLK100, 5G_REFCLK15625_QSGMII, and
+                                                           5G_REFCLK125, it is recommended that the DFE Tap1 Manual Enable be
+                                                           set after setting GSER_LANE_MODE_1.VMA_MM.
+                                                           In all other modes this register can be ignored. */
+#else
+	uint64_t dfe_c1_ovrd_val              : 1;
+	uint64_t dfe_c2_ovrd_val              : 1;
+	uint64_t dfe_c3_ovrd_val              : 1;
+	uint64_t dfe_c4_ovrd_val              : 1;
+	uint64_t dfe_c5_ovrd_val              : 1;
+	uint64_t dfe_ovrd_en                  : 1;
+	uint64_t reserved_6_63                : 58;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_rx_valbbd_ctrl_2_s cn78xx;
+};
+typedef union cvmx_gserx_lanex_rx_valbbd_ctrl_2 cvmx_gserx_lanex_rx_valbbd_ctrl_2_t;
+
+/**
  * cvmx_gser#_lane#_vma_coarse_ctrl_0
  *
  * RAW PCS Per Lane Coarse VMA Control Configuration 0 Register
  * Per Lane registers are specific to a paticular lane.
+ * It is not expected that software will need to access this register.
  */
 union cvmx_gserx_lanex_vma_coarse_ctrl_0 {
 	uint64_t u64;
 	struct cvmx_gserx_lanex_vma_coarse_ctrl_0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_16_63               : 48;
-	uint64_t iq_max                       : 4;  /**< Slice DLL IQ Maximum Value. */
-	uint64_t iq_min                       : 4;  /**< Slice DLL IQ Minimum Value. */
-	uint64_t iq_step                      : 2;  /**< Slice DLL IQ step size. */
-	uint64_t window_wait                  : 3;  /**< Adaptation window wait setting. */
+	uint64_t iq_max                       : 4;  /**< Slice DLL IQ Maximum Value in VMA Coarse Mode. */
+	uint64_t iq_min                       : 4;  /**< Slice DLL IQ Minimum Value in VMA Coarse Mode. */
+	uint64_t iq_step                      : 2;  /**< Slice DLL IQ step size in VMA Coarse Mode. */
+	uint64_t window_wait                  : 3;  /**< Adaptation window wait setting in VMA Coarse Mode. */
 	uint64_t lms_wait                     : 3;  /**< LMS wait time setting used to control the number
                                                          of samples taken during the collection of
-                                                         statistics. */
+                                                         statistics in VMA Coarse Mode. */
 #else
 	uint64_t lms_wait                     : 3;
 	uint64_t window_wait                  : 3;
@@ -2481,15 +2825,16 @@ typedef union cvmx_gserx_lanex_vma_coarse_ctrl_0 cvmx_gserx_lanex_vma_coarse_ctr
  *
  * RAW PCS Per Lane Coarse VMA Control Configuration 1 Register
  * Per Lane registers are specific to a paticular lane.
+ * It is not expected that software will need to access this register.
  */
 union cvmx_gserx_lanex_vma_coarse_ctrl_1 {
 	uint64_t u64;
 	struct cvmx_gserx_lanex_vma_coarse_ctrl_1_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_10_63               : 54;
-	uint64_t ctle_pmax                    : 4;  /**< RX CTLE peak maximum value. */
-	uint64_t ctle_pmin                    : 4;  /**< RX CTLE peak minimum value. */
-	uint64_t ctle_pstep                   : 2;  /**< CTLE peak step size. */
+	uint64_t ctle_pmax                    : 4;  /**< RX CTLE peak maximum value in VMA Coarse Mode. */
+	uint64_t ctle_pmin                    : 4;  /**< RX CTLE peak minimum value in VMA Coarse Mode */
+	uint64_t ctle_pstep                   : 2;  /**< CTLE peak step size in VMA Coarse Mode. */
 #else
 	uint64_t ctle_pstep                   : 2;
 	uint64_t ctle_pmin                    : 4;
@@ -2505,16 +2850,16 @@ typedef union cvmx_gserx_lanex_vma_coarse_ctrl_1 cvmx_gserx_lanex_vma_coarse_ctr
  * cvmx_gser#_lane#_vma_coarse_ctrl_2
  *
  * Per Lane registers are specific to a paticular lane.
- *
+ * It is not expected that software will need to access this register.
  */
 union cvmx_gserx_lanex_vma_coarse_ctrl_2 {
 	uint64_t u64;
 	struct cvmx_gserx_lanex_vma_coarse_ctrl_2_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_10_63               : 54;
-	uint64_t pctle_gmax                   : 4;  /**< RX PRE-CTLE gain maximum value. */
-	uint64_t pctle_gmin                   : 4;  /**< RX PRE-CTLE gain minimum value. */
-	uint64_t pctle_gstep                  : 2;  /**< CTLE PRE-peak gain step size. */
+	uint64_t pctle_gmax                   : 4;  /**< RX PRE-CTLE gain maximum value in VMA Coarse Mode. */
+	uint64_t pctle_gmin                   : 4;  /**< RX PRE-CTLE gain minimum value in VMA Coarse Mode. */
+	uint64_t pctle_gstep                  : 2;  /**< CTLE PRE-peak gain step size in VMA Coarse Mode */
 #else
 	uint64_t pctle_gstep                  : 2;
 	uint64_t pctle_gmin                   : 4;
@@ -2534,10 +2879,13 @@ union cvmx_gserx_lane_lpbken {
 	struct cvmx_gserx_lane_lpbken_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_4_63                : 60;
-	uint64_t lpbken                       : 4;  /**< For links that are not in PCIE mode (including all OCI links),
-                                                         ties to the mac_pcs_lane_loopnk_en[3:0] pins of the RAW PCS.
-                                                         When asserted in P0 state, Tx-to-Rx serial
-                                                         loopback is activated on lane X. */
+	uint64_t lpbken                       : 4;  /**< For links that are not in PCIE mode (including all OCI links).
+                                                         When asserted in P0 state, allows per lane Tx-to-Rx serial
+                                                         loopback activation.
+                                                         <3>: Lane 3
+                                                         <2>: Lane 2
+                                                         <1>: Lane 1
+                                                         <0>: Lane 0 */
 #else
 	uint64_t lpbken                       : 4;
 	uint64_t reserved_4_63                : 60;
@@ -2556,8 +2904,39 @@ union cvmx_gserx_lane_mode {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_4_63                : 60;
 	uint64_t lmode                        : 4;  /**< For links that are not in PCIE mode (including all OCI links),
-                                                         ties to the mac_pcs_lane_mode[3:0] pins of the RAW PCS.
-                                                         Enumerated by GSER_LMODE_E. */
+                                                         used to index into the PHY table to select electricals and link rate.
+                                                         Note that the PHY table can be modified such that any supported link rate
+                                                         can be derived regardless of the configured LMODE.
+                                                         0x0:   25G_REFCLK100
+                                                         0x1:   5G_REFCLK100
+                                                         0x2:   8G_REFCLK100
+                                                         0x3:   125G_REFCLK15625_KX
+                                                         0x4:   3125G_REFCLK15625_XAUI
+                                                         0x5:   103215G_REFCLK15625_KR
+                                                         0x6:   125G_REFCLK15625_SGMII
+                                                         0x7:   5G_REFCLK15625_QSGMII
+                                                         0x8:   625G_REFCLK15625_RXAUI
+                                                         0x9:   25G_REFCLK125
+                                                         0xa:   5G_REFCLK125
+                                                         0xb:   8G_REFCLK125
+                                                         0xc - 0xf: reserved
+                                                         This register is not used for PCIE configurations.
+                                                         For non-OCI links, this registers defaults to 625G_REFCLK15625_RXAUI.
+                                                         For OCI links, the value is mapped at reset from the GSER_SPD and the
+                                                         appropriate table updates are performed so the rate is obtained for the
+                                                         paticular reference clock.
+                                                         It is recommended that the PHY be in reset when reconfiguring the LMODE
+                                                         (GSER_PHY_CTL.PHY_RESET is set).  If the LMODE is modified when the PHY
+                                                         is out of reset, the GSER_RXTX_STAT.LMC can be used to determine when the
+                                                         PHY has transitioned to the new setting.
+                                                         Once the LMODE has been configured, and the PHY is out of reset, the table
+                                                         entries for the selected LMODE must be updated to reflect the reference
+                                                         clock speed.  Refer to the register description and index into the table
+                                                         using the rate and reference speed to obtain the recommended values.
+                                                          - 1: Write the GSER_PLL_P(0..11)_MODE_0 register
+                                                          - 2: Write the GSER_PLL_P(0..11)_MODE_1 register
+                                                          - 3: Write the GSER_LANE_P(0..11)_MODE_0 register
+                                                          - 4: Write the GSER_LANE_P(0..11)_MODE_1 register */
 #else
 	uint64_t lmode                        : 4;
 	uint64_t reserved_4_63                : 60;
@@ -2576,8 +2955,11 @@ union cvmx_gserx_lane_poff {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_4_63                : 60;
 	uint64_t lpoff                        : 4;  /**< For links that are not in PCIE mode (including all OCI links),
-                                                         ties to the mac_pcs_lane_pwr_off[3:0] pins of the RAW PCS.
-                                                         Control signal to power down lane X. */
+                                                         allows for per lane power down.
+                                                         <3>: Lane 3
+                                                         <2>: Lane 2
+                                                         <1>: Lane 1
+                                                         <0>: Lane 0. */
 #else
 	uint64_t lpoff                        : 4;
 	uint64_t reserved_4_63                : 60;
@@ -2596,8 +2978,16 @@ union cvmx_gserx_lane_srst {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_4_63                : 60;
 	uint64_t lsrst                        : 4;  /**< For links that are not in PCIE mode (including all OCI links),
-                                                         ties to the mac_pcs_lane_soft_reset[3:0] pins of the RAW PCS.
-                                                         Allows reset to Lane X. */
+                                                         resets specific lane (equivalent to the P2 power state) after
+                                                         any pending requests (power state change, rate change) are complete.
+                                                         <3>: Lane 3
+                                                         <2>: Lane 2
+                                                         <1>: Lane 1
+                                                         <0>: Lane 0
+                                                         The lane remains in reset state while this signal is asserted.
+                                                         When the signal deasserts, the lane exits the reset state and
+                                                         the PHY returns to the power state the PHY was in prior.
+                                                         It is not expected that SW will need to use this register. */
 #else
 	uint64_t lsrst                        : 4;
 	uint64_t reserved_4_63                : 60;
@@ -3318,13 +3708,16 @@ union cvmx_gserx_phy_ctl {
 	struct cvmx_gserx_phy_ctl_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_2_63                : 62;
-	uint64_t phy_pd                       : 1;  /**< Tied to the mac_pcs_refclk_pd pin of the RAW PCS.
-                                                         When asserted, the PHY is powered down. */
-	uint64_t phy_reset                    : 1;  /**< Tied to the phy_reset pin of the RAW PCS.
-                                                         When asserted, the PHY is held in reset. */
+	uint64_t phy_reset                    : 1;  /**< When asserted, the PHY is held in reset. This bit is initialized
+                                                         as follows:
+                                                           0 (not reset): bootable PCIe, or OCI when GSER_SPD.SPD
+                                                                          comes up in a bootable mode.
+                                                           1 (reset):     non-bootable PCIe, BGX/ILK, or OCI when GSER_SPD.SPD
+                                                                          comes up in SW_MODE. */
+	uint64_t phy_pd                       : 1;  /**< When asserted, the PHY is powered down. */
 #else
-	uint64_t phy_reset                    : 1;
 	uint64_t phy_pd                       : 1;
+	uint64_t phy_reset                    : 1;
 	uint64_t reserved_2_63                : 62;
 #endif
 	} s;
@@ -3340,8 +3733,9 @@ union cvmx_gserx_pipe_lpbk {
 	struct cvmx_gserx_pipe_lpbk_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_1_63                : 63;
-	uint64_t pcie_lpbk                    : 1;  /**< For non-OCI links, Ties to pipeX_tx2rx_loopbk, analag Serial Loop Back Control
-                                                         input of the PCIE PCS. */
+	uint64_t pcie_lpbk                    : 1;  /**< For links that are in PCIE mode, places the PHY in Serial
+                                                         Loopback mode, where the tx_n/p data are looped back to the
+                                                         rx_n/p. */
 #else
 	uint64_t pcie_lpbk                    : 1;
 	uint64_t reserved_1_63                : 63;
@@ -3364,9 +3758,45 @@ union cvmx_gserx_pll_px_mode_0 {
 	struct cvmx_gserx_pll_px_mode_0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_16_63               : 48;
-	uint64_t pll_icp                      : 4;  /**< PLL Charge pump enable. */
-	uint64_t pll_rloop                    : 3;  /**< Loop resistor tuning. */
-	uint64_t pll_pcs_div                  : 9;  /**< The divider that generates pcs_mac_tx_clk. */
+	uint64_t pll_icp                      : 4;  /**< PLL Charge pump enable.
+                                                         This field must be set appropriately if running a GSER_LANE_MODE.LMODE
+                                                         with a non-default reference clock.  A "NS" indicates that the rate
+                                                         is not supported at the specified reference clock.
+                                                         Recommended Settings:
+                                                                         100Mhz          125Mhz          156.25Mhz
+                                                         1.25G:          0x1             0x1             0x1
+                                                         2.5G:           0x4             0x3             0x3
+                                                         3.125G:         NS              0x1             0x1
+                                                         5.0G:           0x4             0x3             0x3
+                                                         6.25G:          NS              0x1             0x1
+                                                         8.0G:           0x3             0x2             NS
+                                                         10.3215G:       NS              NS              0x1 */
+	uint64_t pll_rloop                    : 3;  /**< Loop resistor tuning.
+                                                         This field must be set appropriately if running a GSER_LANE_MODE.LMODE with
+                                                         a non-default reference clock.  A "NS" indicates that the rate
+                                                         is not supported at the specified reference clock.
+                                                         Recommended Settings:
+                                                         1.25G:          0x3
+                                                         2.5G:           0x3
+                                                         3.125G:         0x3
+                                                         5.0G:           0x3
+                                                         6.25G:          0x3
+                                                         8.0G:           0x5
+                                                         10.3215G:       0x5 */
+	uint64_t pll_pcs_div                  : 9;  /**< The divider that generates pcs_mac_tx_clk. The frequency
+                                                         of the clock is (pll_frequency / PLL_PCS_DIV).
+                                                         This field must be set appropriately if running a GSER_LANE_MODE.LMODE with
+                                                         a non-default reference clock or doesn't default to a 20-bit RX/TX data path.
+                                                         A "NS" indicates that the rate is not supported at the specified reference clock.
+                                                         Recommended Settings:
+                                                         1.25G:          0x28
+                                                         2.5G:           0x5
+                                                         3.125G:         0x24
+                                                         5.0G:           0xa (100 Mhz REFCLK)
+                                                         5.0G:           0x5 (156.25 Mhz REFCLK)
+                                                         6.25G:          0xa
+                                                         8.0G:           0xa
+                                                         10.3215G:       0x24 */
 #else
 	uint64_t pll_pcs_div                  : 9;
 	uint64_t pll_rloop                    : 3;
@@ -3390,12 +3820,55 @@ union cvmx_gserx_pll_px_mode_1 {
 	struct cvmx_gserx_pll_px_mode_1_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_14_63               : 50;
-	uint64_t pll_16p5en                   : 1;  /**< Enable for the div 16.5 clock. */
-	uint64_t pll_cpadj                    : 2;  /**< PLL Charge adjust. */
-	uint64_t pll_pcie3en                  : 1;  /**< Enable PCIE3 Mode. */
-	uint64_t pll_opr                      : 1;  /**< 0: Use Ring Oscillator VCO
-                                                         - 1: Use LC-tank VCO */
-	uint64_t pll_div                      : 9;  /**< PLL divider in feedback path which sets the PLL frequency. */
+	uint64_t pll_16p5en                   : 1;  /**< Enable for the div 16.5 divided down clock.
+                                                         This field must be set appropriately if running a GSER_LANE_MODE.LMODE with
+                                                         a non-default reference clock.  A "NS" indicates that the rate
+                                                         is not supported at the specified reference clock.
+                                                         Recommended Settings:
+                                                                         100Mhz          125Mhz          156.25Mhz
+                                                         1.25G:          0x1             0x1             0x1
+                                                         2.5G:           0x0             0x0             0x0
+                                                         3.125G:         NS              0x1             0x1
+                                                         5.0G:           0x0             0x0             0x0
+                                                         6.25G:          NS              0x0             0x0
+                                                         8.0G:           0x0             0x0             NS
+                                                         10.3215G:       NS              NS              0x1 */
+	uint64_t pll_cpadj                    : 2;  /**< PLL Charge adjust.
+                                                         This field must be set appropriately if running a GSER_LANE_MODE.LMODE with
+                                                         a non-default reference clock.  A "NS" indicates that the rate
+                                                         is not supported at the specified reference clock.
+                                                         Recommended Settings:
+                                                                         100Mhz          125Mhz          156.25Mhz
+                                                         1.25G:          0x2             0x2             0x3
+                                                         2.5G:           0x2             0x1             0x2
+                                                         3.125G:         NS              0x2             0x2
+                                                         5.0G:           0x2             0x1             0x2
+                                                         6.25G:          NS              0x2             0x2
+                                                         8.0G:           0x2             0x1             NS
+                                                         10.3215G:       NS              NS              0x2 */
+	uint64_t pll_pcie3en                  : 1;  /**< Enable PCIE3 Mode.
+                                                         Recommended Settings:
+                                                          - 0: = any rate other than 8Gbps.
+                                                          - 1: = rate is equal to 8Gbps. */
+	uint64_t pll_opr                      : 1;  /**< Pll Op Range
+                                                         - 0: Use Ring Oscillator VCO
+                                                         - 1: Use LC-tank VCO
+                                                          Recommended Settings:
+                                                           - 0: = rates 6.25Gbps and lower.
+                                                           - 1: = rates 8Gbps and higher. */
+	uint64_t pll_div                      : 9;  /**< PLL divider in feedback path which sets the PLL frequency.
+                                                         This field must be set appropriately if running a GSER_LANE_MODE.LMODE with
+                                                         a non-default reference clock.  A "NS" indicates that the rate
+                                                         is not supported at the specified reference clock.
+                                                         Recommended Settings:
+                                                                         100Mhz          125Mhz          156.25Mhz
+                                                         1.25G:          0x19            0x14            0x10
+                                                         2.5G:           0x19            0x14            0x10
+                                                         3.125G:         NS              0x19            0xa
+                                                         5.0G:           0x19            0x14            0x10
+                                                         6.25G:          NS              0x19            0x14
+                                                         8.0G:           0x28            0x20            NS
+                                                         10.3215G:       NS              NS              0x21 */
 #else
 	uint64_t pll_div                      : 9;
 	uint64_t pll_opr                      : 1;
@@ -3417,7 +3890,7 @@ union cvmx_gserx_pll_stat {
 	struct cvmx_gserx_pll_stat_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_1_63                : 63;
-	uint64_t pll_lock                     : 1;  /**< RAW PCS PLL lock indication. */
+	uint64_t pll_lock                     : 1;  /**< When set, inidcates that the PHY PLL is locked. */
 #else
 	uint64_t pll_lock                     : 1;
 	uint64_t reserved_1_63                : 63;
@@ -3437,7 +3910,7 @@ union cvmx_gserx_qlm_stat {
 	uint64_t reserved_2_63                : 62;
 	uint64_t rst_rdy                      : 1;  /**< When asserted, the QLM is configured (CSR_GSER_CAV_CFG)
                                                          and the PLLs are stable. The GSER is ready to accept
-                                                         traffic from the MAC. */
+                                                         tx traffic from the MAC. */
 	uint64_t dcok                         : 1;  /**< When asserted, there is a PLL reference clock indicating there
                                                          is power to the QLM. */
 #else
@@ -3458,10 +3931,13 @@ union cvmx_gserx_refclk_sel {
 	struct cvmx_gserx_refclk_sel_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_3_63                : 61;
-	uint64_t pcie_refclk125               : 1;  /**< For non-OCI links, indicates a 125 Mhz Reference clock. */
-	uint64_t com_clk_sel                  : 1;  /**< For non-OCI links, when set the reference clock is
-                                                         sourced from the external clock mux. */
-	uint64_t use_com1                     : 1;  /**< For non-OCI links, external mux select.
+	uint64_t pcie_refclk125               : 1;  /**< For bootable PCIe links, this is loaded with the pi_pcieN_ref_spd pin
+                                                         at cold reset and indicates a 125 Mhz Reference clock.  It is not
+                                                         used for non-PCIe links. */
+	uint64_t com_clk_sel                  : 1;  /**< When set, the reference clock is sourced from the external clock mux.
+                                                         For bootable PCIe links, this bit is loaded with the
+                                                         pi_pcieN_com0_clk_en pin at cold reset. */
+	uint64_t use_com1                     : 1;  /**< For non-OCI links, this bit controls the external mux select.
                                                          When set, qlmc_refclkn/p_1 are selected as the reference clock
                                                          When clear, qlmc_refclkn/p_0 are selected as the reference clock. */
 #else
@@ -3483,10 +3959,18 @@ union cvmx_gserx_rx_coast {
 	struct cvmx_gserx_rx_coast_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_4_63                : 60;
-	uint64_t coast                        : 4;  /**< For links that are not in PCIE or BGX mode (including all OCI links),
-                                                         ties to the mac_pcs_rx_cdr_coast[3:0] pins of the RAW PCS.  This
-                                                         is a control signal to freeze the frequency of the CDR in the PHY.
-                                                         This signal is only valid in P0 state. */
+	uint64_t coast                        : 4;  /**< For links that are not in PCIE mode (including all OCI links),
+                                                         Control signals to freeze the frequency of the per lane CDR in the PHY.
+                                                         The COAST signals are only valid in P0 state, come up asserted and are
+                                                         deasserted in hardware after detecting the electrical idle exit
+                                                         (GSER_RX_EIE_DETSTS.EIESTS).  Once the COAST signal deasserts, the CDR
+                                                         will be allowed to lock.  In BGX mode, the BGX Mac can also control the COAST
+                                                         inputs to the PHY to allow Auto-Negotiation for backplane Ethernet.
+                                                         It is not expected that SW will need to use this register.
+                                                         <3>: Lane 3
+                                                         <2>: Lane 2
+                                                         <1>: Lane 1
+                                                         <0>: Lane 0 */
 #else
 	uint64_t coast                        : 4;
 	uint64_t reserved_4_63                : 60;
@@ -3505,9 +3989,15 @@ union cvmx_gserx_rx_eie_deten {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_4_63                : 60;
 	uint64_t eiede                        : 4;  /**< For links that are not in PCIE mode (including all OCI links),
-                                                         this bit ties to the mac_pcs_rx_eie_det_en[3:0] pins of the RAW PCS.
-                                                         When set, the RAW PCS looks for an Electrical Idle
-                                                         Exit Condition. */
+                                                         these bits enable per lane Electrical Idle Exit (EIE) detection.
+                                                         When EIE is detected, GSER_RX_EIE_DETSTS.EIELTCH is asserted.
+                                                         EIEDE defaults to the enabled state.  Once EIE has been detected,
+                                                         EIEDE must be disabled, and then enabled again to perform another
+                                                         EIE detection.
+                                                         <3>: Lane 3
+                                                         <2>: Lane 2
+                                                         <1>: Lane 1
+                                                         <0>: Lane 0 */
 #else
 	uint64_t eiede                        : 4;
 	uint64_t reserved_4_63                : 60;
@@ -3525,15 +4015,34 @@ union cvmx_gserx_rx_eie_detsts {
 	struct cvmx_gserx_rx_eie_detsts_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_12_63               : 52;
-	uint64_t cdrlock                      : 4;  /**< Lane X CDR lock time has been met after the detection of the electrical
-                                                         idle exit condition. */
-	uint64_t eiests                       : 4;  /**< Status from lane X's RX indicating the detection of a electical IDLE exit
-                                                         condition.  Note that this is a dynamic indication and valid when the
-                                                         associated mac_pcs_rx_eie_det_en[3:0] bit is set. */
-	uint64_t eieltch                      : 4;  /**< Status from lane X's RX indicating the detection of a electical IDLE exit
-                                                         condition.  Note that this is a latched indication that electrical IDLE
-                                                         exit condition was met at least once during the period of time that the
-                                                         associate mac_pcs_rx_eie_det_en[3:0] bit was set. */
+	uint64_t cdrlock                      : 4;  /**< After an electrical idle exit condition (EIE) has been dectected, the CDR
+                                                         needs 10000 UI to lock.  During this time, there may be RX bit errors.
+                                                         These bits will set when the CDR is guaranteed to be locked.  Note that
+                                                         link training can't start until the lane CDRLOCK is set.  SW can use CDRLOCK
+                                                         to determine when to expect error free Rx data.
+                                                         <11>: Lane 3
+                                                         <10>: Lane 2
+                                                         <9>:  Lane 1
+                                                         <8>:  Lane 0 */
+	uint64_t eiests                       : 4;  /**< When electrical idle exit detection is enabled (GSER_RX_EIE_DETEN.EIEDE is
+                                                         asserted), indicates that an electrical idle exit condition (EIE) was
+                                                         detected. For higher data rates, the received data needs to have sufficient
+                                                         low frequency content (for example, IDLE symbols) for data transitions to be
+                                                         detected and for EIESTS to stay set accordingly. Under most conditions, EIESTS
+                                                         will stay asserted until GSER_RX_EIE_DETEN.EIEDE is deasserted.
+                                                         <7>: Lane 3
+                                                         <6>: Lane 2
+                                                         <5>: Lane 1
+                                                         <4>: Lane 0 */
+	uint64_t eieltch                      : 4;  /**< When electrical idle exit detection is enabled (GSER_RX_EIE_DETEN.EIEDE is
+                                                         asserted), indicates that an electrical idle exit condition (EIE) was
+                                                         detected.  Once a EIE condition has been detected, the per lane EIELTCH will
+                                                         stay set until GSER_RX_EIE_DETEN.EIEDE is deasserted.  Note that there may
+                                                         be RX bit errors until CDRLOCK is set.
+                                                         <3>: Lane 3
+                                                         <2>: Lane 2
+                                                         <1>: Lane 1
+                                                         <0>: Lane 0 */
 #else
 	uint64_t eieltch                      : 4;
 	uint64_t eiests                       : 4;
@@ -3546,6 +4055,40 @@ union cvmx_gserx_rx_eie_detsts {
 typedef union cvmx_gserx_rx_eie_detsts cvmx_gserx_rx_eie_detsts_t;
 
 /**
+ * cvmx_gser#_rx_eie_filter
+ */
+union cvmx_gserx_rx_eie_filter {
+	uint64_t u64;
+	struct cvmx_gserx_rx_eie_filter_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t eii_filt                     : 16; /**< The GSER uses Electrical Idle Inference to determine when a RX lane
+                                                         has reentered Electrical IDLE (EI).  The PHY Electrical IDLE Exit
+                                                         detection supports a minimum pulse width of 400ps, therefore configurations
+                                                         that run faster than 2.5G can indicate EI when the serial lines are still
+                                                         driven.  For rates faster than 2.5G, it takes 16000 UI of consecutive
+                                                         deasserted GSER_RX_EIE_DETSTS.EIESTS for the GSER to infer EI and begin
+                                                         invalidating RX data. In the event of electrical IDLE inference, the
+                                                         following happens:
+                                                           - GSER_RX_EIE_DETSTS.CDRLOCK[LANE] is zeroed
+                                                           - GSER_RX_EIE_DETSTS.EIELTCH[LANE] is zeroed
+                                                           - GSER_RX_EIE_DETSTS.EIESTS[LANE] is zeroed
+                                                           - GSER_RX_COAST.COAST[LANE] is asserted to prevent the CDR from trying to
+                                                             lock on the incoming data stream.
+                                                           - The lane incoming RX data is invalidated.
+                                                         Writing this register to a non-zero value will cause the Electrical
+                                                         Idle Inference to use the EII_FILT count instead of the default settings.
+                                                         Each EII_FILT count represents 20ns of incremental EI Inference Time. */
+#else
+	uint64_t eii_filt                     : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_gserx_rx_eie_filter_s     cn78xx;
+};
+typedef union cvmx_gserx_rx_eie_filter cvmx_gserx_rx_eie_filter_t;
+
+/**
  * cvmx_gser#_rx_polarity
  */
 union cvmx_gserx_rx_polarity {
@@ -3553,8 +4096,13 @@ union cvmx_gserx_rx_polarity {
 	struct cvmx_gserx_rx_polarity_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_4_63                : 60;
-	uint64_t rx_inv                       : 4;  /**< Control signal to invert the polarity of received data.  When
-                                                         asserted, the polarity of the received data is inverted. */
+	uint64_t rx_inv                       : 4;  /**< For links that are not in PCIE mode (including all OCI links),
+                                                         control signal to invert the polarity of received data.  When
+                                                         asserted, the polarity of the received data is inverted.
+                                                         <3>: Lane 3
+                                                         <2>: Lane 2
+                                                         <1>: Lane 1
+                                                         <0>: Lane 0 */
 #else
 	uint64_t rx_inv                       : 4;
 	uint64_t reserved_4_63                : 60;
@@ -3573,11 +4121,17 @@ union cvmx_gserx_rx_pstate {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_3_63                : 61;
 	uint64_t rxpstate                     : 3;  /**< For links that are not in PCIE mode (including all OCI links),
-                                                         ties to the mac_pcs_rx_pstate[3:0] pins of the RAW PCS.
-                                                          0x0 = P0.
-                                                          0x1 = P0s.
-                                                          0x2 = P1.
-                                                          0x3 = P2.
+                                                         allows RX lane power state control. It is not expected that
+                                                         SW will need to use this register.
+                                                          0x0 = P0.  Active State.
+                                                                     All internal clocks in the PHY are operational, the only
+                                                                     state where the PHY transmits and receives link data.
+                                                          0x1 = P0s. Standby State.
+                                                                     The Rx link is disabled.
+                                                          0x2 = P1.  Low Power State:
+                                                                     Selected internal clocks in the PHY are turned off.
+                                                          0x3 = P2.  Power Down State.
+                                                                     All clocks in the PHY are turned off.
                                                           else = Reserved. */
 #else
 	uint64_t rxpstate                     : 3;
@@ -3597,17 +4151,22 @@ union cvmx_gserx_rxtx_stat {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_3_63                : 61;
 	uint64_t lmc                          : 1;  /**< For links that are not in PCIE mode (including all OCI links),
-                                                         this bit is set for a lane mode change (a write
-                                                         to GSER_LANE_MODE) that changes the value of the Lane Mode.
-                                                         This bit is clear when each lane acknowledges the change. */
+                                                         this bit is set when a write is performed to that changes the
+                                                         value of the GSER_LANE_MODE register when the PHY is out of reset.
+                                                         This bit is clear when the PHY acknowledges the change for all
+                                                         4 lanes. */
 	uint64_t tpsc                         : 1;  /**< For links that are not in PCIE mode (including all OCI links),
-                                                         this bit is set for a TX Power state change (a write
-                                                         to GSER_TX_PSTATE that changes the value of the Power State.
-                                                         This bit is clear when each lane acknowledges the change. */
+                                                         this bit is set when a write is performed to that changes the
+                                                         value of the GSER_TX_PSTATE register when the PHY is out of reset
+                                                         This bit is clear when the PHY acknowledges the change for all
+                                                         4 lanes.
+                                                         It is not expected that software will need to change the PSTATE. */
 	uint64_t rpsc                         : 1;  /**< For links that are not in PCIE mode (including all OCI links),
-                                                         this bit is set for a RX Power state change (a write
-                                                         to GSER_RX_PSTATE that changes the value of the Power State.
-                                                         This bit is clear when each lane acknowledges the change. */
+                                                         this bit is set when a write is performed to that changes the
+                                                         value to the GSER_RX_PSTATE register.
+                                                         This bit is clear when the PHY acknowledges the change for all
+                                                         4 lanes.
+                                                         It is not expected that software will need to change the PSTATE. */
 #else
 	uint64_t rpsc                         : 1;
 	uint64_t tpsc                         : 1;
@@ -3936,40 +4495,6 @@ union cvmx_gserx_sata_tx_invert {
 typedef union cvmx_gserx_sata_tx_invert cvmx_gserx_sata_tx_invert_t;
 
 /**
- * cvmx_gser#_slice#_p#_mode
- *
- * Slice Registers are shared across two adjacent lanes. SLICE0 access
- * lane pairs 0 & 1. SLICE1 acceses lane pairs 2 & 3.
- * The Protocol selects the specific protocol register as
- * enumerated by GSER_LMODE_E.
- */
-union cvmx_gserx_slicex_px_mode {
-	uint64_t u64;
-	struct cvmx_gserx_slicex_px_mode_s {
-#ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_15_63               : 49;
-	uint64_t s_spare                      : 2;  /**< <14>: Enable rx1_div33 clock in the SerDes.
-                                                         <13>: Enable rx0_div33 clock in the SerDes. */
-	uint64_t ldll_isel                    : 2;  /**< Lane DLL current select. */
-	uint64_t sdll_isel                    : 2;  /**< Slice DLL current select. */
-	uint64_t pi_bwsel                     : 3;  /**< PI bandwidth select. */
-	uint64_t ldll_bwsel                   : 3;  /**< Lane DLL bandwidth select. */
-	uint64_t sdll_bwsel                   : 3;  /**< Slice DLL bandwidth select. */
-#else
-	uint64_t sdll_bwsel                   : 3;
-	uint64_t ldll_bwsel                   : 3;
-	uint64_t pi_bwsel                     : 3;
-	uint64_t sdll_isel                    : 2;
-	uint64_t ldll_isel                    : 2;
-	uint64_t s_spare                      : 2;
-	uint64_t reserved_15_63               : 49;
-#endif
-	} s;
-	struct cvmx_gserx_slicex_px_mode_s    cn78xx;
-};
-typedef union cvmx_gserx_slicex_px_mode cvmx_gserx_slicex_px_mode_t;
-
-/**
  * cvmx_gser#_spd
  */
 union cvmx_gserx_spd {
@@ -3977,19 +4502,34 @@ union cvmx_gserx_spd {
 	struct cvmx_gserx_spd_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_4_63                : 60;
-	uint64_t spd                          : 4;  /**< For OCI links,
-                                                         spd<3>: When set, indicates 125Mhz reference clock,
-                                                                 When clear, indicates 100Mhz reference clock.
-                                                         spd<2:0>:
-                                                         0x0 = 1.25G
-                                                         0x1 = 2.5G
-                                                         0x2 = 3.125G
-                                                         0x3 = 5G
-                                                         0x4 = 6.25G
-                                                         0x5 = 8G
-                                                         0x6 = 10G
-                                                         0x7 = Software mode, PHY comes up in powerdown state.
-                                                         For non-OCI links these bits are not used. */
+	uint64_t spd                          : 4;  /**< For OCI links, these bits are loaded at cold reset from the
+                                                         pi_oci_spd[3:0] pins and configure the GSER to a rate/reference
+                                                         clock.  This field can be reconfigured and the new
+                                                         GSER_LANE_MODE.LMODE clock will take affect on the next warm reset.
+                                                         For SPD settings that configure a non-default reference clock,
+                                                         hardware will update the PLL settings of the specific lane mode
+                                                         (LMODE) table entry to derive the correct link rate.
+                                                         For non-OCI links, this field is not used.
+                                                         config  refclk      link rate       LMODE
+                                                         0x0:    100Mhz      1.25Gbps        125G_REFCLK15625_KX
+                                                         0x1:    100Mhz      2.5Gbps         25G_REFCLK100
+                                                         0x2:    100Mhz      5Gbps           5G_REFCLK100
+                                                         0x3:    100Mhz      8Gbps           8G_REFCLK100
+                                                         0x4:    125Mhz      1.25Gbps        125G_REFCLK15625_KX
+                                                         0x5:    125Mhz      2.5Gbps         25G_REFCLK125
+                                                         0x6:    125Mhz      3.125Gbps       3125G_REFCLK15625_XAUI
+                                                         0x7:    125Mhz      5Gbps           5G_REFCLK125
+                                                         0x8:    125Mhz      6.25Gbps        625G_REFCLK15625_RXAUI
+                                                         0x9:    125Mhz      8Gbps           8G_REFCLK125
+                                                         0xa:    156.25Mhz   2.5Gbps         25G_REFCLK100
+                                                         0xb:    156.25Mhz   3.125Gbps       3125G_REFCLK15625_XAUI
+                                                         0xc:    156.25Mhz   5Gbps           5G_REFCLK125
+                                                         0xd:    156.25Mhz   6.25Gbps        625G_REFCLK15625_RXAUI
+                                                         0xe:    126.25Mhz   10.3125Gbps     103215G_REFCLK15625_KR
+                                                         0xf:
+                                                         Note that a value of 0xf is called SW_MODE.  The OCI link does not
+                                                         come up configured.  SW can come up and configure the interface at
+                                                         a later time. */
 #else
 	uint64_t spd                          : 4;
 	uint64_t reserved_4_63                : 60;
@@ -4008,7 +4548,8 @@ union cvmx_gserx_srst {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_1_63                : 63;
 	uint64_t srst                         : 1;  /**< When asserted, resets all per lane state in the GSER
-                                                         with the exception of the PHY. */
+                                                         with the exception of the PHY.  It is not expected that
+                                                         SW will need to use this register. */
 #else
 	uint64_t srst                         : 1;
 	uint64_t reserved_1_63                : 63;
@@ -4027,11 +4568,17 @@ union cvmx_gserx_tx_pstate {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_3_63                : 61;
 	uint64_t txpstate                     : 3;  /**< For links that are not in PCIE mode (including all OCI links),
-                                                         ties to the mac_pcs_tx_pstate[3:0] pins of the RAW PCS.
-                                                          0x0 = P0.
-                                                          0x1 = P0s.
-                                                          0x2 = P1.
-                                                          0x3 = P2.
+                                                         allows TX lane power state control. It is not expected that
+                                                         SW will need to use this register.
+                                                          0x0 = P0.  Active State.
+                                                                     All internal clocks in the PHY are operational, the only
+                                                                     state where the PHY transmits and receives link data.
+                                                          0x1 = P0s. Standby State.
+                                                                     The Tx link is disabled.
+                                                          0x2 = P1.  Low Power State:
+                                                                     Selected internal clocks in the PHY are turned off.
+                                                          0x3 = P2.  Power Down.
+                                                                     All clocks in the PHY are turned off.
                                                           else = Reserved. */
 #else
 	uint64_t txpstate                     : 3;
@@ -4051,7 +4598,11 @@ union cvmx_gserx_tx_vboost {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_4_63                : 60;
 	uint64_t vboost                       : 4;  /**< For links that are not in PCIE mode (including all OCI links),
-                                                         ties to the mac_pcs_tx_vboost_en[3:0] pins of the RAW PCS. */
+                                                         boosts the TX Vswing from vdd to 1.0 Vppd.
+                                                         <3>: Lane 3
+                                                         <2>: Lane 2
+                                                         <1>: Lane 1
+                                                         <0>: Lane 0 */
 #else
 	uint64_t vboost                       : 4;
 	uint64_t reserved_4_63                : 60;
diff --git a/arch/mips/include/asm/octeon/cvmx-helper-board.h b/arch/mips/include/asm/octeon/cvmx-helper-board.h
index 545e1f6..c36798e 100644
--- a/arch/mips/include/asm/octeon/cvmx-helper-board.h
+++ b/arch/mips/include/asm/octeon/cvmx-helper-board.h
@@ -43,7 +43,7 @@
  * Helper functions to abstract board specific data about
  * network ports from the rest of the cvmx-helper files.
  *
- * <hr>$Revision: 86434 $<hr>
+ * <hr>$Revision: 86922 $<hr>
  */
 #ifndef __CVMX_HELPER_BOARD_H__
 #define __CVMX_HELPER_BOARD_H__
@@ -314,19 +314,6 @@ cvmx_phy_host_mode_t cvmx_helper_board_get_phy_host_mode(int ipd_port);
 int cvmx_helper_board_get_phy_info(cvmx_phy_info_t *phy_info, int ipd_port);
 #endif
 
-/**
- * @INTERNAL
- * This function outputs the port flags for the specified interface and port.
- *
- * @param interface interface to get the port flags for
- * @param index     port on interface to get the port flags for
- * @param[out] pflags port flags for the specified port.  Not modified if the
- *		      data is unavailable.
- *
- * @return 0 for success, -1 if info no available.
- */
-int __cvmx_helper_board_get_port_flags(int interface, int index);
-
 #ifdef	__cplusplus
 /* *INDENT-OFF* */
 }
diff --git a/arch/mips/include/asm/octeon/cvmx-helper-rgmii.h b/arch/mips/include/asm/octeon/cvmx-helper-rgmii.h
index 93cf850..81c9e08 100644
--- a/arch/mips/include/asm/octeon/cvmx-helper-rgmii.h
+++ b/arch/mips/include/asm/octeon/cvmx-helper-rgmii.h
@@ -43,7 +43,7 @@
  * Functions for RGMII/GMII/MII initialization, configuration,
  * and monitoring.
  *
- * <hr>$Revision: 73842 $<hr>
+ * <hr>$Revision: 86586 $<hr>
  */
 #ifndef __CVMX_HELPER_RGMII_H__
 #define __CVMX_HELPER_RGMII_H__
@@ -93,6 +93,19 @@ extern int __cvmx_helper_rgmii_enable(int interface);
  *
  * @return Link state
  */
+extern cvmx_helper_link_info_t __cvmx_helper_gmii_link_get(int ipd_port);
+
+/**
+ * @INTERNAL
+ * Return the link state of an IPD/PKO port as returned by
+ * auto negotiation. The result of this function may not match
+ * Octeon's link config if auto negotiation has changed since
+ * the last call to cvmx_helper_link_set().
+ *
+ * @param ipd_port IPD/PKO port to query
+ *
+ * @return Link state
+ */
 extern cvmx_helper_link_info_t __cvmx_helper_rgmii_link_get(int ipd_port);
 
 /**
diff --git a/arch/mips/include/asm/octeon/cvmx-helper-sgmii.h b/arch/mips/include/asm/octeon/cvmx-helper-sgmii.h
index 3c9316a..9317f8a 100644
--- a/arch/mips/include/asm/octeon/cvmx-helper-sgmii.h
+++ b/arch/mips/include/asm/octeon/cvmx-helper-sgmii.h
@@ -43,7 +43,7 @@
  * Functions for SGMII initialization, configuration,
  * and monitoring.
  *
- * <hr>$Revision: 73842 $<hr>
+ * <hr>$Revision: 86626 $<hr>
  */
 #ifndef __CVMX_HELPER_SGMII_H__
 #define __CVMX_HELPER_SGMII_H__
@@ -117,4 +117,79 @@ extern int __cvmx_helper_sgmii_link_set(int ipd_port, cvmx_helper_link_info_t li
  */
 extern int __cvmx_helper_sgmii_configure_loopback(int ipd_port, int enable_internal, int enable_external);
 
+/**
+ * @INTERNAL
+ * Probe a SGMII interface and determine the number of ports
+ * connected to it. The SGMII interface should still be down after
+ * this call. This is used by interfaces using the bgx mac.
+ *
+ * @param interface Interface to probe
+ *
+ * @return Number of ports on the interface. Zero to disable.
+ */
+extern int __cvmx_helper_bgx_sgmii_probe(int interface);
+
+/**
+ * @INTERNAL
+ * Bringup and enable a SGMII interface. After this call packet
+ * I/O should be fully functional. This is called with IPD
+ * enabled but PKO disabled. This is used by interfaces using the
+ * bgx mac.
+ *
+ * @param interface Interface to bring up
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int __cvmx_helper_bgx_sgmii_enable(int interface);
+
+/**
+ * @INTERNAL
+ * Return the link state of an IPD/PKO port as returned by
+ * auto negotiation. The result of this function may not match
+ * Octeon's link config if auto negotiation has changed since
+ * the last call to cvmx_helper_link_set(). This is used by
+ * interfaces using the bgx mac.
+ *
+ * @param ipd_port IPD/PKO port to query
+ *
+ * @return Link state
+ */
+extern cvmx_helper_link_info_t __cvmx_helper_bgx_sgmii_link_get(int ipd_port);
+
+/**
+ * @INTERNAL
+ * Configure an IPD/PKO port for the specified link state. This
+ * function does not influence auto negotiation at the PHY level.
+ * The passed link state must always match the link state returned
+ * by cvmx_helper_link_get(). It is normally best to use
+ * cvmx_helper_link_autoconf() instead. This is used by interfaces
+ * using the bgx mac.
+ *
+ * @param ipd_port  IPD/PKO port to configure
+ * @param link_info The new link state
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int __cvmx_helper_bgx_sgmii_link_set(int ipd_port, 
+					    cvmx_helper_link_info_t link_info);
+
+/**
+ * @INTERNAL
+ * Configure a port for internal and/or external loopback. Internal loopback
+ * causes packets sent by the port to be received by Octeon. External loopback
+ * causes packets received from the wire to sent out again. This is used by
+ * interfaces using the bgx mac.
+ *
+ * @param ipd_port IPD/PKO port to loopback.
+ * @param enable_internal
+ *                 Non zero if you want internal loopback
+ * @param enable_external
+ *                 Non zero if you want external loopback
+ *
+ * @return Zero on success, negative on failure.
+ */
+extern int __cvmx_helper_bgx_sgmii_configure_loopback(int ipd_port, 
+						      int enable_internal, 
+						      int enable_external);
+
 #endif
diff --git a/arch/mips/include/asm/octeon/cvmx-helper-xaui.h b/arch/mips/include/asm/octeon/cvmx-helper-xaui.h
index 45b344f..99af3db 100644
--- a/arch/mips/include/asm/octeon/cvmx-helper-xaui.h
+++ b/arch/mips/include/asm/octeon/cvmx-helper-xaui.h
@@ -43,7 +43,7 @@
  * Functions for XAUI initialization, configuration,
  * and monitoring.
  *
- * <hr>$Revision: 73842 $<hr>
+ * <hr>$Revision: 86925 $<hr>
  */
 #ifndef __CVMX_HELPER_XAUI_H__
 #define __CVMX_HELPER_XAUI_H__
@@ -63,6 +63,18 @@ extern int __cvmx_helper_xaui_enumerate(int interface);
 
 /**
  * @INTERNAL
+ * Probe a XAUI interface and determine the number of ports
+ * connected to it. The XAUI interface should still be down
+ * after this call.
+ *
+ * @param interface Interface to probe
+ *
+ * @return Number of ports on the interface. Zero to disable.
+ */
+extern int __cvmx_helper_bgx_xaui_probe(int interface);
+
+/**
+ * @INTERNAL
  * Bringup and enable a XAUI interface. After this call packet
  * I/O should be fully functional. This is called with IPD
  * enabled but PKO disabled.
@@ -75,6 +87,18 @@ extern int __cvmx_helper_xaui_enable(int interface);
 
 /**
  * @INTERNAL
+ * Bringup and enable a XAUI interface. After this call packet
+ * I/O should be fully functional. This is called with IPD
+ * enabled but PKO disabled.
+ *
+ * @param interface Interface to bring up
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int __cvmx_helper_bgx_xaui_enable(int interface);
+
+/**
+ * @INTERNAL
  * Return the link state of an IPD/PKO port as returned by
  * auto negotiation. The result of this function may not match
  * Octeon's link config if auto negotiation has changed since
@@ -88,6 +112,19 @@ extern cvmx_helper_link_info_t __cvmx_helper_xaui_link_get(int ipd_port);
 
 /**
  * @INTERNAL
+ * Return the link state of an IPD/PKO port as returned by
+ * auto negotiation. The result of this function may not match
+ * Octeon's link config if auto negotiation has changed since
+ * the last call to cvmx_helper_link_set().
+ *
+ * @param ipd_port IPD/PKO port to query
+ *
+ * @return Link state
+ */
+extern cvmx_helper_link_info_t __cvmx_helper_bgx_xaui_link_get(int ipd_port);
+
+/**
+ * @INTERNAL
  * Configure an IPD/PKO port for the specified link state. This
  * function does not influence auto negotiation at the PHY level.
  * The passed link state must always match the link state returned
@@ -103,6 +140,22 @@ extern int __cvmx_helper_xaui_link_set(int ipd_port, cvmx_helper_link_info_t lin
 
 /**
  * @INTERNAL
+ * Configure an IPD/PKO port for the specified link state. This
+ * function does not influence auto negotiation at the PHY level.
+ * The passed link state must always match the link state returned
+ * by cvmx_helper_link_get(). It is normally best to use
+ * cvmx_helper_link_autoconf() instead.
+ *
+ * @param ipd_port  IPD/PKO port to configure
+ * @param link_info The new link state
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int __cvmx_helper_bgx_xaui_link_set(int ipd_port,
+					   cvmx_helper_link_info_t link_info);
+
+/**
+ * @INTERNAL
  * Configure a port for internal and/or external loopback. Internal loopback
  * causes packets sent by the port to be received by Octeon. External loopback
  * causes packets received from the wire to sent out again.
@@ -117,4 +170,22 @@ extern int __cvmx_helper_xaui_link_set(int ipd_port, cvmx_helper_link_info_t lin
  */
 extern int __cvmx_helper_xaui_configure_loopback(int ipd_port, int enable_internal, int enable_external);
 
+/**
+ * @INTERNAL
+ * Configure a port for internal and/or external loopback. Internal loopback
+ * causes packets sent by the port to be received by Octeon. External loopback
+ * causes packets received from the wire to sent out again.
+ *
+ * @param ipd_port IPD/PKO port to loopback.
+ * @param enable_internal
+ *                 Non zero if you want internal loopback
+ * @param enable_external
+ *                 Non zero if you want external loopback
+ *
+ * @return Zero on success, negative on failure.
+ */
+extern int __cvmx_helper_bgx_xaui_configure_loopback(int ipd_port,
+						     int enable_internal,
+						     int enable_external);
+
 #endif
diff --git a/arch/mips/include/asm/octeon/cvmx-ila-defs.h b/arch/mips/include/asm/octeon/cvmx-ila-defs.h
index cae55b7..c6030c4 100644
--- a/arch/mips/include/asm/octeon/cvmx-ila-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-ila-defs.h
@@ -1145,7 +1145,8 @@ union cvmx_ila_rx_lnex_cfg {
                                                          destripped.
                                                          If the lane is in internal loopback mode, this field is ignored and skip words are always
                                                          discarded in the lane logic. */
-	uint64_t reserved_6_7                 : 2;
+	uint64_t reserved_7_7                 : 1;
+	uint64_t rx_dis_disp_chk              : 1;  /**< Disable the RX disparity check, see ILA_RX_LNE(0..7)_INT[DISP_ERR]. */
 	uint64_t rx_scrm_sync                 : 1;  /**< RX scrambler-synchronization status. A 1 means synchronization has been achieved. */
 	uint64_t rx_bdry_sync                 : 1;  /**< RX word-boundary-synchronization status. A 1 means synchronization has been achieved */
 	uint64_t rx_dis_ukwn                  : 1;  /**< Disable normal response to unknown words. Unknown words are still logged but do not cause
@@ -1161,7 +1162,8 @@ union cvmx_ila_rx_lnex_cfg {
 	uint64_t rx_dis_ukwn                  : 1;
 	uint64_t rx_bdry_sync                 : 1;
 	uint64_t rx_scrm_sync                 : 1;
-	uint64_t reserved_6_7                 : 2;
+	uint64_t rx_dis_disp_chk              : 1;
+	uint64_t reserved_7_7                 : 1;
 	uint64_t rx_dis_psh_skip              : 1;
 	uint64_t reserved_9_63                : 55;
 #endif
@@ -1177,7 +1179,8 @@ union cvmx_ila_rx_lnex_int {
 	uint64_t u64;
 	struct cvmx_ila_rx_lnex_int_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_9_63                : 55;
+	uint64_t reserved_10_63               : 54;
+	uint64_t disp_err                     : 1;  /**< RX disparity error encountered. Throws ILA_INTSN_E::ILA_RXLNE(0..7)_DISP_ERR. */
 	uint64_t bad_64b67b                   : 1;  /**< Bad 64B/67B code word encountered. Once the bad word reaches the burst-control unit (as
                                                          indicated by ILA_RX(0)_INT[LANE_BAD_WORD]) it is discarded and all open packets receive an
                                                          error. Throws ILA_INTSN_E::ILA_RXLNE(0..7)_BAD_64B67B. */
@@ -1203,7 +1206,8 @@ union cvmx_ila_rx_lnex_int {
 	uint64_t stat_msg                     : 1;
 	uint64_t stat_cnt_ovfl                : 1;
 	uint64_t bad_64b67b                   : 1;
-	uint64_t reserved_9_63                : 55;
+	uint64_t disp_err                     : 1;
+	uint64_t reserved_10_63               : 54;
 #endif
 	} s;
 	struct cvmx_ila_rx_lnex_int_s         cn78xx;
diff --git a/arch/mips/include/asm/octeon/cvmx-ilk-defs.h b/arch/mips/include/asm/octeon/cvmx-ilk-defs.h
index 85c4fa7..0f83922 100644
--- a/arch/mips/include/asm/octeon/cvmx-ilk-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-ilk-defs.h
@@ -3335,7 +3335,8 @@ union cvmx_ilk_rx_lnex_cfg {
                                                          destripped.
                                                          If the lane is in internal loopback mode, this field is ignored and skip words are always
                                                          discarded in the lane logic. */
-	uint64_t reserved_6_7                 : 2;
+	uint64_t reserved_7_7                 : 1;
+	uint64_t rx_dis_disp_chk              : 1;  /**< Disable the RX disparity check, see ILK_RX_LNE(0..15)_INT[DISP_ERR]. */
 	uint64_t rx_scrm_sync                 : 1;  /**< RX scrambler-synchronization status. A 1 means synchronization has been achieved. */
 	uint64_t rx_bdry_sync                 : 1;  /**< RX word-boundary-synchronization status. A 1 means synchronization has been achieved */
 	uint64_t rx_dis_ukwn                  : 1;  /**< Disable normal response to unknown words. Unknown words are still logged but do not cause
@@ -3351,12 +3352,49 @@ union cvmx_ilk_rx_lnex_cfg {
 	uint64_t rx_dis_ukwn                  : 1;
 	uint64_t rx_bdry_sync                 : 1;
 	uint64_t rx_scrm_sync                 : 1;
-	uint64_t reserved_6_7                 : 2;
+	uint64_t rx_dis_disp_chk              : 1;
+	uint64_t reserved_7_7                 : 1;
 	uint64_t rx_dis_psh_skip              : 1;
 	uint64_t reserved_9_63                : 55;
 #endif
 	} s;
-	struct cvmx_ilk_rx_lnex_cfg_s         cn68xx;
+	struct cvmx_ilk_rx_lnex_cfg_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t rx_dis_psh_skip              : 1;  /**< When RX_DIS_PSH_SKIP=0, skip words are de-stripped.
+                                                         When RX_DIS_PSH_SKIP=1, skip words are discarded in the lane
+                                                         logic.
+
+                                                         If the lane is in internal loopback mode, RX_DIS_PSH_SKIP
+                                                         is ignored and skip words are always discarded in the lane
+                                                         logic.
+
+                                                         ***NOTE: Added in pass 2.0 */
+	uint64_t reserved_6_7                 : 2;
+	uint64_t rx_scrm_sync                 : 1;  /**< Rx scrambler synchronization status
+                                                         '1' means synchronization achieved
+
+                                                         ***NOTE: Added in pass 2.0 */
+	uint64_t rx_bdry_sync                 : 1;  /**< Rx word boundary sync status
+                                                         '1' means synchronization achieved */
+	uint64_t rx_dis_ukwn                  : 1;  /**< Disable normal response to unknown words.  They are still
+                                                         logged but do not cause an error to all open channels. */
+	uint64_t rx_dis_scram                 : 1;  /**< Disable lane scrambler (debug) */
+	uint64_t stat_rdclr                   : 1;  /**< CSR read to ILK_RX_LNEx_STAT* clears the selected counter after
+                                                         returning its current value. */
+	uint64_t stat_ena                     : 1;  /**< Enable RX lane statistics counters */
+#else
+	uint64_t stat_ena                     : 1;
+	uint64_t stat_rdclr                   : 1;
+	uint64_t rx_dis_scram                 : 1;
+	uint64_t rx_dis_ukwn                  : 1;
+	uint64_t rx_bdry_sync                 : 1;
+	uint64_t rx_scrm_sync                 : 1;
+	uint64_t reserved_6_7                 : 2;
+	uint64_t rx_dis_psh_skip              : 1;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} cn68xx;
 	struct cvmx_ilk_rx_lnex_cfg_cn68xxp1 {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_5_63                : 59;
@@ -3388,7 +3426,8 @@ union cvmx_ilk_rx_lnex_int {
 	uint64_t u64;
 	struct cvmx_ilk_rx_lnex_int_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_9_63                : 55;
+	uint64_t reserved_10_63               : 54;
+	uint64_t disp_err                     : 1;  /**< RX disparity error encountered. Throws ILK_INTSN_E::ILK_RXLNE(0..15)_DISP_ERR. */
 	uint64_t bad_64b67b                   : 1;  /**< Bad 64B/67B code word encountered. Once the bad word reaches the burst control unit (as
                                                          denoted by ILK_RX(0..1)_INT[LANE_BAD_WORD]) it is discarded and all open packets receive
                                                          an error. Throws ILK_INTSN_E::ILK_RXLNE(0..15)_BAD_64B67B. */
@@ -3414,11 +3453,43 @@ union cvmx_ilk_rx_lnex_int {
 	uint64_t stat_msg                     : 1;
 	uint64_t stat_cnt_ovfl                : 1;
 	uint64_t bad_64b67b                   : 1;
-	uint64_t reserved_9_63                : 55;
+	uint64_t disp_err                     : 1;
+	uint64_t reserved_10_63               : 54;
 #endif
 	} s;
-	struct cvmx_ilk_rx_lnex_int_s         cn68xx;
-	struct cvmx_ilk_rx_lnex_int_s         cn68xxp1;
+	struct cvmx_ilk_rx_lnex_int_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t bad_64b67b                   : 1;  /**< Bad 64B/67B codeword encountered.  Once the bad word reaches
+                                                         the burst control unit (as deonted by
+                                                         ILK_RXx_INT[LANE_BAD_WORD]) it will be tossed and all open
+                                                         packets will receive an error. */
+	uint64_t stat_cnt_ovfl                : 1;  /**< Rx lane statistic counter overflow */
+	uint64_t stat_msg                     : 1;  /**< Status bits for the link or a lane transitioned from a '1'
+                                                         (healthy) to a '0' (problem) */
+	uint64_t dskew_fifo_ovfl              : 1;  /**< Rx deskew fifo overflow occurred. */
+	uint64_t scrm_sync_loss               : 1;  /**< 4 consecutive bad sync words or 3 consecutive scramble state
+                                                         mismatches */
+	uint64_t ukwn_cntl_word               : 1;  /**< Unknown framing control word. Block type does not match any of
+                                                         (SYNC,SCRAM,SKIP,DIAG) */
+	uint64_t crc32_err                    : 1;  /**< Diagnostic CRC32 errors */
+	uint64_t bdry_sync_loss               : 1;  /**< Rx logic loses word boundary sync (16 tries).  Hardware will
+                                                         automatically attempt to regain word boundary sync */
+	uint64_t serdes_lock_loss             : 1;  /**< Rx SERDES loses lock */
+#else
+	uint64_t serdes_lock_loss             : 1;
+	uint64_t bdry_sync_loss               : 1;
+	uint64_t crc32_err                    : 1;
+	uint64_t ukwn_cntl_word               : 1;
+	uint64_t scrm_sync_loss               : 1;
+	uint64_t dskew_fifo_ovfl              : 1;
+	uint64_t stat_msg                     : 1;
+	uint64_t stat_cnt_ovfl                : 1;
+	uint64_t bad_64b67b                   : 1;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} cn68xx;
+	struct cvmx_ilk_rx_lnex_int_cn68xx    cn68xxp1;
 	struct cvmx_ilk_rx_lnex_int_s         cn78xx;
 };
 typedef union cvmx_ilk_rx_lnex_int cvmx_ilk_rx_lnex_int_t;
diff --git a/arch/mips/include/asm/octeon/cvmx-l2c-defs.h b/arch/mips/include/asm/octeon/cvmx-l2c-defs.h
index c7718aa..7c4e84a 100644
--- a/arch/mips/include/asm/octeon/cvmx-l2c-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-l2c-defs.h
@@ -1412,14 +1412,15 @@ static inline uint64_t CVMX_L2C_XMDX_PFC(unsigned long offset)
 /**
  * cvmx_l2c_big_ctl
  *
- * (1) BIGRD interrupts can occur during normal operation as the PP's are allowed to prefetch to
- * non-existent memory locations.  Therefore, BIGRD is for informational purposes only.
+ * L2C_BIG_CTL = L2C Big memory control register
+ *
  *
- * (2) When a HOLERD/BIGRD occurs or HOLEWR/BIGWR blocks a store L2C_TAD(0..7)_ERR will be
- * loaded.  L2C_TAD(0..7)_ERR will be not be locked for a BIGRD, however.
+ * Notes:
+ * (1) BIGRD interrupts can occur during normal operation as the PP's are allowed to prefetch to
+ *     non-existent memory locations.  Therefore, BIGRD is for informational purposes only.
  *
- * (3) The BIG logic only applies to local addresses.  A command for a remote address will not
- * cause a BIGRD/BIGWR on the requesting node.
+ * (2) When HOLEWR/BIGWR blocks a store L2C_VER_ID, L2C_VER_PP, L2C_VER_IOB, and L2C_VER_MSC will be
+ *     loaded just like a store which is blocked by VRTWR.  Additionally, L2C_ERR_XMC will be loaded.
  */
 union cvmx_l2c_big_ctl {
 	uint64_t u64;
@@ -1483,24 +1484,30 @@ union cvmx_l2c_big_ctl {
 	struct cvmx_l2c_big_ctl_cn70xx {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_8_63                : 56;
-	uint64_t maxdram                      : 4;  /**< Amount of configured DRAM
-                                                         0 = reserved
-                                                         1 = 512MB
-                                                         2 = 1GB
-                                                         3 = 2GB
-                                                         4 = 4GB
-                                                         5 = 8GB
-                                                         6 = 16GB
-                                                         7 = 32GB
-                                                         8 = 64GB
-                                                         9 = 128GB
-                                                         10 = 256GB
-                                                         11 = 512GB
-                                                         12-15 reserved
-                                                         Violations of this limit causes L2C to set L2C_TAD(0..0)_INT[BIGRD/BIGWR]. */
+	uint64_t maxdram                      : 4;  /**< Amount of configured DRAM.
+                                                         0x0 = reserved.
+                                                         0x1 = 512 MB.
+                                                         0x2 = 1 GB.
+                                                         0x3 = 2 GB.
+                                                         0x4 = 4 GB.
+                                                         0x5 = 8 GB.
+                                                         0x6 = 16 GB.
+                                                         0x7 = 32 GB.
+                                                         0x8 = 64 GB.
+                                                         0x9 = 128 GB.
+                                                         0xA = 256 GB.
+                                                         0xB = 512 GB.
+                                                         0xC-0xF= reserved.
+                                                         Violations of this limit causes L2C to set L2C_INT_REG[BIGRD/BIGWR].
+                                                         BIGRD interrupts can occur during normal operation as the cores are allowed to prefetch to
+                                                         nonexistent memory locations. Therefore, BIGRD is for informational purposes only.
+                                                         When a HOLERD/BIGRD occurs or HOLEWR/BIGWR blocks a store operation, L2C_TAD(0..0)_ERR is
+                                                         loaded. L2C_TAD(0..0)_ERR is not locked for a BIGRD, however.
+                                                         The BIG logic only applies to local addresses. A command for a remote address does not
+                                                         cause a BIGRD/BIGWR on the requesting node. */
 	uint64_t reserved_1_3                 : 3;
-	uint64_t disbig                       : 1;  /**< When set, disables the BIG/HOLE logic completely. When clear, BIGWR and HOLEWR block
-                                                         stores and BIGRD/HOLERD is reported. */
+	uint64_t disbig                       : 1;  /**< Disable the BIG/HOLE logic. When set, the BIG/HOLE is logic disabled completely. When
+                                                         clear, BIGWR and HOLEWR block stores and BIGRD/HOLERD is reported. */
 #else
 	uint64_t disbig                       : 1;
 	uint64_t reserved_1_3                 : 3;
@@ -2254,10 +2261,10 @@ union cvmx_l2c_cbcx_int {
 	struct cvmx_l2c_cbcx_int_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_4_63                : 60;
-	uint64_t ioccmddbe                    : 1;  /**< IOCCMD double-bit error occurred. See L2C_CBC_IOCERR for logged information. */
-	uint64_t ioccmdsbe                    : 1;  /**< IOCCMD single-bit error occurred. See L2C_CBC_IOCERR for logged information. */
-	uint64_t rsddbe                       : 1;  /**< RSD double-bit error occurred. See L2C_CBC_RSDERR for logged information. */
-	uint64_t rsdsbe                       : 1;  /**< RSD single-bit error occurred. See L2C_CBC_RSDERR for logged information. */
+	uint64_t ioccmddbe                    : 1;  /**< IOCCMD double-bit error occurred. See L2C_CBC(0..3)_IOCERR for logged information. */
+	uint64_t ioccmdsbe                    : 1;  /**< IOCCMD single-bit error occurred. See L2C_CBC(0..3)_IOCERR for logged information. */
+	uint64_t rsddbe                       : 1;  /**< RSD double-bit error occurred. See L2C_CBC(0..3)_RSDERR for logged information. */
+	uint64_t rsdsbe                       : 1;  /**< RSD single-bit error occurred. See L2C_CBC(0..3)_RSDERR for logged information. */
 #else
 	uint64_t rsdsbe                       : 1;
 	uint64_t rsddbe                       : 1;
@@ -2290,7 +2297,7 @@ union cvmx_l2c_cbcx_iocerr {
 	uint64_t reserved_40_61               : 22;
 	uint64_t syn                          : 8;  /**< Error syndrome. */
 	uint64_t reserved_3_31                : 29;
-	uint64_t xmcnum                       : 3;  /**< XMC which had the error. */
+	uint64_t xmcnum                       : 3;  /**< Indicates the XMC that had the error. */
 #else
 	uint64_t xmcnum                       : 3;
 	uint64_t reserved_3_31                : 29;
@@ -2324,9 +2331,9 @@ union cvmx_l2c_cbcx_rsderr {
 	uint64_t reserved_40_61               : 22;
 	uint64_t syn                          : 8;  /**< Error syndrome. */
 	uint64_t reserved_9_31                : 23;
-	uint64_t tadnum                       : 3;  /**< TAD fifo containing the error. */
-	uint64_t qwnum                        : 2;  /**< QW containing the error. */
-	uint64_t rsdnum                       : 4;  /**< RSD which had the error. */
+	uint64_t tadnum                       : 3;  /**< Indicates the TAD FIFO containing the error. */
+	uint64_t qwnum                        : 2;  /**< Indicates the QW containing the error. */
+	uint64_t rsdnum                       : 4;  /**< Indicates the RSD that had the error. */
 #else
 	uint64_t rsdnum                       : 4;
 	uint64_t qwnum                        : 2;
@@ -2906,77 +2913,126 @@ typedef union cvmx_l2c_cfg cvmx_l2c_cfg_t;
  * (3) if the PPID is outside the range of 0-47,255 or if the PP in question is in reset
  * a write will be ignored and reads will timeout the RSL bus.
  *
- * (4) Referring to note (1) above, the following rd/sel values are supported:
+ * (4) Referring to note (1) above, the following root/rd/sel values are supported:
  * NOTE: Put only the "Customer type" in HRM. do not put the "Real type" in HRM.
- * Customer                                                    Real
- * rd     sel     type         Description                                 type
- * ======+=======+==========+==============================================+=========
- * 4      2       RO          COP0 UserLocal                                RW
- * 7      0       RO          COP0 HWREna                                   RW
- * 9      0       RO          COP0 Count                                    RW
- * 9      6       RO          COP0 CvmCount                                 RW
- * 9      7       RO          COP0 CvmCtl                                   RW
- * 11      0       RO          COP0 Compare                                  RW
- * 11      6       RW          COP0 PowThrottle                              RW
- * 12      0       RO          COP0 Status                                   RW
- * 12      1       RO          COP0 IntCtl                                   RO
- * 12      2       RO          COP0 SRSCtl                                   RO
- * 13      0       RO          COP0 Cause                                    RW
- * 14      0       RO          COP0 EPC                                      RW
- * 15      0       RO          COP0 PrID                                     RO
- * 15      1       RO          COP0 EBase                                    RW
- * 16      0       RO          PC Issue Debug Info (see details below)       RO
- * 16      1       RO          PC Fetch Debug Info (see details below)       RO
- * 16      2       RO          PC Fill Debug Info (see details below)        RO
- * 16      3       RO          PC Misc Debug Info (see details below)        RO
- * 18      0       RO          COP0 WatchLo0                                 RW
- * 19      0       RO          COP0 WatchHi0                                 RW
- * 22      0       RO          COP0 MultiCoreDebug                           RW
- * 22      1                   COP0 VoltageMonitor                           RW
- * 23      0       RO          COP0 Debug                                    RW
- * 23      6       RO          COP0 Debug2                                   RO
- * 24      0       RO          COP0 DEPC                                     RW
- * 25      0       RO          COP0 PerfCnt Control0                         RW
- * 25      1       RO          COP0 PerfCnt Counter0                         RW
- * 25      2       RO          COP0 PerfCnt Control1                         RW
- * 25      3       RO          COP0 PerfCnt Counter1                         RW
- * 25      4       RO          COP0 PerfCnt Control2                         RW
- * 25      5       RO          COP0 PerfCnt Counter2                         RW
- * 25      6       RO          COP0 PerfCnt Control3                         RW
- * 25      7       RO          COP0 PerfCnt Counter3                         RW
- * 27      0       RO          COP0 CacheErr (icache)                        RW
- * 27      2              RO          COP0 IcacheDebug                                RW
- * 28      0       RO          COP0 TagLo (icache)                           RW
- * 28      1       RO          COP0 DataLo (icache)                          RW
- * 29      1       RO          COP0 DataHi (icache)                          RW
- * 30      0       RO          COP0 ErrorEPC                                 RW
- * 31      0       RO          COP0 DESAVE                                   RW
- * 31      2       RO          COP0 Scratch                                  RW
- * 31      3       RO          COP0 Scratch1                                 RW
- * 31      4       RO          COP0 Scratch2                                 RW
+ *
+ *              Customer                                           Real
+ * root rd  sel   type         Description                         type
+ * ====+===+===+========+=========================================+====
+ *  1    4   2     RO     CP0 Root.UserLocal                        RW
+ *  0    4   2     RO     CP0 Guest.UserLocal                       RW
+ *  1    7   0     RO     CP0 Root.HWREna                           RW
+ *  0    7   0     RO     CP0 Guest.HWREna                          RW
+ *  1    8   1     RO     CP0 Root.BadInstr                         RW
+ *  0    8   1     RO     CP0 Guest.BadInstr                        RW
+ *  1    8   2     RO     CP0 Root.BadInstrP                        RW
+ *  0    8   2     RO     CP0 Guest.BadInstrP                       RW
+ *  1    9   0     RO     CP0 Root.Count                            RW
+ *  1    9   6     RO     CP0 Root.CvmCount                         RW
+ *  1    9   7     RO     CP0 Root.CvmCtl                           RW
+ *  0    9   7     RO     CP0 Guest.CvmCtl                          RW
+ *  1   10   5     RO     CP0 Root.GuestCtl0                        RW
+ *  1   11   0     RO     CP0 Root.Compare                          RW
+ *  0   11   0     RO     CP0 Guest.Compare                         RW
+ *  1   11   4     RO     CP0 Root.GuestCtl0Ext                     RW
+ *  1   11   6     RW     CP0 Root.PowThrottle                      RW
+ *  1   12   0     RO     CP0 Root.Status                           RW
+ *  0   12   0     RO     CP0 Guest.Status                          RW
+ *  1   12   1     RO     CP0 Root.IntCtl                           RO
+ *  0   12   1     RO     CP0 Guest.IntCtl                          RO
+ *  1   12   2     RO     CP0 Root.SRSCtl                           RO
+ *  0   12   2     RO     CP0 Guest.SRSCtl                          RO
+ *  1   12   6     RO     CP0 Root.GuestCtl0                        RW
+ *  1   12   7     RO     CP0 Root.GTOffset                         RW
+ *  1   13   0     RO     CP0 Root.Cause                            RW
+ *  0   13   0     RO     CP0 Guest.Cause                           RW
+ *  1   14   0     RO     CP0 Root.EPC                              RW
+ *  0   14   0     RO     CP0 Guest.EPC                             RW
+ *  1   15   0     RO     CP0 Root.PrID                             RO
+ *  0   15   0     RO     CP0 Guest.PrID                            RO
+ *  1   15   1     RO     CP0 Root.EBase                            RW
+ *  0   15   1     RO     CP0 Guest.EBase                           RW
+ *  1   16   0     RO     PC Issue Debug Info (see details below)   RO
+ *  1   16   1     RO     PC Fetch Debug Info (see details below)   RO
+ *  1   16   2     RO     PC Fill Debug Info (see details below)    RO
+ *  1   16   3     RO     PC Misc Debug Info (see details below)    RO
+ *  1   16   5     RO     PC Committed Info (see details below)     RO
+ *  1   18   0     RO     CP0 Root.WatchLo0                         RW
+ *  1   19   0     RO     CP0 Root.WatchHi0                         RW
+ *  1   22   0     RO     CP0 Root.MultiCoreDebug                   RW
+ *  1   22   1            CP0 Root.VoltageMonitor                   RW
+ *  1   22   2     RO     CP0 Root.CvmCountOffset                   RW
+ *  1   23   0     RO     CP0 Root.Debug                            RW
+ *  1   23   6     RO     CP0 Root.Debug2                           RO
+ *  1   24   0     RO     CP0 Root.DEPC                             RW
+ *  1   25   0     RO     CP0 Root.PerfCnt Control0                 RW
+ *  1   25   1     RO     CP0 Root.PerfCnt Counter0                 RW
+ *  1   25   2     RO     CP0 Root.PerfCnt Control1                 RW
+ *  1   25   3     RO     CP0 Root.PerfCnt Counter1                 RW
+ *  1   25   4     RO     CP0 Root.PerfCnt Control2                 RW
+ *  1   25   5     RO     CP0 Root.PerfCnt Counter2                 RW
+ *  1   25   6     RO     CP0 Root.PerfCnt Control3                 RW
+ *  1   25   7     RO     CP0 Root.PerfCnt Counter3                 RW
+ *  1   27   0     RO     CP0 Root.CacheErr (icache)                RW
+ *  1   27   2     RO     CP0 Root.IcacheDebug                      RO
+ *  1   28   0     RO     CP0 Root.TagLo (icache)                   RW
+ *  1   28   1     RO     CP0 Root.DataLo (icache)                  RW
+ *  1   29   1     RO     CP0 Root.DataHi (icache)                  RW
+ *  1   30   0     RO     CP0 Root.ErrorEPC                         RW
+ *  0   30   0     RO     CP0 Guest.ErrorEPC                        RW
+ *  1   31   0     RO     CP0 Root.DESAVE                           RW
+ *  1   31   2     RO     CP0 Root.Scratch                          RW
+ *  0   31   2     RO     CP0 Guest.Scratch                         RW
+ *  1   31   3     RO     CP0 Root.Scratch1                         RW
+ *  0   31   3     RO     CP0 Guest.Scratch1                        RW
+ *  1   31   4     RO     CP0 Root.Scratch2                         RW
+ *  0   31   4     RO     CP0 Guest.Scratch2                        RW
+ *  1   31   5     RO     CP0 Root.Scratch3                         RW
+ *
  * PC Issue Debug Info
- * - 63:2 pc0_5a<63:2> // often VA<63:2> of the next instruction to issue
- * //    but can also be the VA of an instruction executing/replaying on pipe 0
- * //    or can also be a VA being filled into the instruction cache
- * //    or can also be unpredictable
- * // <61:49> RAZ
- * 1    illegal      // set when illegal VA
- * 0    delayslot    // set when VA is delayslot (prior branch may be either taken or not taken)
+ *  - 63:2  issue_address<63:2>   // often VA<63:2> (PC) of the next instruction to issue (5a in
+ * pipeline)
+ *                              //    but can also be the PC of an instruction
+ * executing/replaying
+ *                              //    or can also be a PC being filled into the instruction cache
+ *                              //    or can also be unpredictable
+ *                              // <58:50> is a copy of <49>
+ *  1     issue_illegal         // set when issue_address is an illegal PC
+ *  0     issue_delayslot       // set when issue_address is in a delayslot (prior instruction
+ * may be either taken or not taken)
+ *
  * PC Fetch Debug Info
- * - 63:0 fetch_address_3a // VA being fetched from the instruction cache
- * // <61:49>, <1:0> RAZ
+ *  - 63:1  fetch_address<63:1>   // VA <63:0> (PC) being fetched from the instruction cache (3a in
+ * pipeline)
+ *                              // <58:50> is a copy of <49>
+ *                              // <1> RAZ
+ *  0     fetch_guest           // set when fetch_address is for the guest
+ *
  * PC Fill Debug Info
- * - 63:0 fill_address_4a<63:2> // VA<63:2> being filled into instruction cache
- * // valid when waiting_for_ifill_4a is set (see PC Misc Debug Info below)
- * // <61:49> RAZ
- * 1 illegal               // set when illegal VA
- * 0 RAZ
+ *  - 63:2  fill_address<63:2>    // VA<63:2> being filled into instruction cache (4a in pipeline)
+ *                              // valid when waiting_for_ifill is set (see PC Misc Debug Info
+ * below)
+ *                              // <58:50> is a copy of <49>
+ *  1     fill_illegal          // set when fill_address is an illegal PC
+ *  0     fill_guest            // set when fill_address is for the guest
+ *
  * PC Misc Debug Info
- * - 63:3 RAZ
- * 2 mem_stall_3a         // stall term from L1 memory system
- * 1 waiting_for_pfill_4a // when waiting_for_ifill_4a is set, indicates whether instruction
- * cache fill is due to a prefetch
- * 0 waiting_for_ifill_4a // set when there is an outstanding instruction cache fill
+ *  - 63:5  RAZ
+ *  4     kernel_mode           // set if the CPU is in kernel mode (6a in pipeline)
+ *  3     guest_mode            // set if the CPU is in guest mode (6a in pipeline)
+ *  2     mem_stall             // stall term from L1 memory system (3a in pipeline)
+ *  1     waiting_for_pfill     // when waiting_for_ifill is set, indicates whether instruction
+ *                              // cache fill is due to a prefetch (4a in pipeline)
+ *  0     waiting_for_ifill     // set when there is an outstanding instruction cache fill (4a in
+ * pipeline)
+ *
+ * PC Committed Debug Info
+ *  63    commit_guest          // Set if commit_address was for the guest
+ *  - 62:55 commit_ASID           // ASID of commit_address
+ *  - 54:49 commit_address<63:59> // VA<63:59> (PC) of the last committed instruction (11a in
+ * pipeline)
+ *  - 48:0  commit_address<48:0>  // VA<48:0> (PC) of last committed instruction (11a in pipeline)
+ *                              // <1:0> RAZ
  */
 union cvmx_l2c_cop0_adr {
 	uint64_t u64;
@@ -3007,7 +3063,7 @@ typedef union cvmx_l2c_cop0_adr cvmx_l2c_cop0_adr_t;
 /**
  * cvmx_l2c_cop0_dat
  *
- * Provides data access for the COP0 register specified by the L2C_COP_ADR register.
+ * Provides data access for the COP0 register specified by the L2C_COP0_ADR register.
  *
  */
 union cvmx_l2c_cop0_dat {
@@ -3495,9 +3551,10 @@ union cvmx_l2c_ctl {
                                                          3. STDN/SCDN/SCFL */
 	uint64_t xmc_arb_mode                 : 1;  /**< Arbitration mode for ADD bus QOS queues. 0 = fully determined through QOS, 1 = QOS0
                                                          highest priority; QOS 1-7 use normal mode. */
-	uint64_t rdf_cnt                      : 8;  /**< Defines the sample point of the LMC response data in the DCLK/RCLK crossing. For optimal
-                                                         performance set to 10 * (DCLK period/RCLK period) - 1. To disable set to 0. All other
-                                                         values are reserved. */
+	uint64_t rdf_cnt                      : 8;  /**< Defines the sample point of the LMC response data in the DDR-clock/core-clock crossing.
+                                                         For optimal performance set to
+                                                         10 * (DDR-clock period/core-clock period) - 1.
+                                                         To disable set to 0. All other values are reserved. */
 	uint64_t reserved_2_5                 : 4;
 	uint64_t disecc                       : 1;  /**< Tag and data ECC disable. */
 	uint64_t disidxalias                  : 1;  /**< Index alias disable. */
@@ -4174,7 +4231,7 @@ union cvmx_l2c_ecc_ctl {
 	struct cvmx_l2c_ecc_ctl_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_10_63               : 54;
-	uint64_t l2dflip                      : 2;  /**< Generate an ECC error in the L2D (see Note 1). */
+	uint64_t l2dflip                      : 2;  /**< Generate an ECC error in the L2D. See note above. */
 	uint64_t l2tflip                      : 2;  /**< Generate an ECC error in the L2T. */
 	uint64_t rdfflip                      : 2;  /**< Generate an ECC error in RDF memory. */
 	uint64_t xmdflip                      : 2;  /**< Generate an ECC error in all corresponding CBC XMD memories. */
@@ -5620,7 +5677,7 @@ typedef union cvmx_l2c_lfb3 cvmx_l2c_lfb3_t;
  * cvmx_l2c_mci#_bist_status
  *
  * If clear BIST is desired, CLEAR_BIST must be written to 1 before START_BIST is written to 1
- * using a separate CSR write.
+ * using a separate CSR write operation.
  * CLEAR_BIST must not be changed after writing START_BIST to 1 until the BIST operation
  * completes (indicated by START_BIST returning to 0) or operation is undefined.
  */
@@ -5628,7 +5685,7 @@ union cvmx_l2c_mcix_bist_status {
 	uint64_t u64;
 	struct cvmx_l2c_mcix_bist_status_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t start_bist                   : 1;  /**< When written to 1, starts BIST. Will read 1 until BIST is complete. */
+	uint64_t start_bist                   : 1;  /**< When written to 1, starts BIST. Remains 1 until BIST is complete. */
 	uint64_t clear_bist                   : 1;  /**< When BIST is triggered, run clear BIST. */
 	uint64_t reserved_2_61                : 60;
 	uint64_t vbffl                        : 2;  /**< BIST failure status for VBF0-1. */
@@ -5699,8 +5756,8 @@ union cvmx_l2c_mcix_int {
 	struct cvmx_l2c_mcix_int_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_2_63                : 62;
-	uint64_t vbfdbe                       : 1;  /**< VBF double-bit error occurred. See L2C_MCI_ERR for logged information. */
-	uint64_t vbfsbe                       : 1;  /**< VBF single-bit error occurred. See L2C_MCI_ERR for logged information. */
+	uint64_t vbfdbe                       : 1;  /**< VBF double-bit error occurred. See L2C_MCI(0..3)_ERR for logged information. */
+	uint64_t vbfsbe                       : 1;  /**< VBF single-bit error occurred. See L2C_MCI(0..3)_ERR for logged information. */
 #else
 	uint64_t vbfsbe                       : 1;
 	uint64_t vbfdbe                       : 1;
@@ -5720,12 +5777,14 @@ union cvmx_l2c_oci_ctl {
 	struct cvmx_l2c_oci_ctl_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_4_63                : 60;
-	uint64_t iofrcl                       : 1;  /**< When set, L2C services all I/O reads and writes on the local node, regardless of the value
-                                                         of the node ID bits in the physical address. During normal operation this bit is expected
-                                                         to be 0. */
-	uint64_t gksegnode                    : 2;  /**< Initialized to OCI node on reset; writable by software. */
-	uint64_t enaoci                       : 1;  /**< When set, do OCI processing. When clear, OCI references will cause RDDISOCI/WRDISOCI
-                                                         interrupts (NYI). */
+	uint64_t iofrcl                       : 1;  /**< When set, L2C services all I/O read and write operations on the local node, regardless of
+                                                         the value of the node ID bits in the physical address. During normal operation this bit is
+                                                         expected to be 0. */
+	uint64_t gksegnode                    : 2;  /**< Initialized to the OCX_COM_NODE[ID] value on reset, which will equal the OCI_NODE_ID pins
+                                                         on a cold reset, but could be something else on a chip warm or soft reset; writable by
+                                                         software. */
+	uint64_t enaoci                       : 1;  /**< Enable OCI processing. When set, perform OCI processing. When clear, OCI references cause
+                                                         RDDISOCI/WRDISOCI interrupts (NYI). */
 #else
 	uint64_t enaoci                       : 1;
 	uint64_t gksegnode                    : 2;
@@ -6750,8 +6809,8 @@ union cvmx_l2c_tadx_err {
 	uint64_t holewr                       : 1;  /**< Logged information is for a HOLEWR error. */
 	uint64_t reserved_58_59               : 2;
 	uint64_t cmd                          : 7;  /**< XMC command of request causing error. */
-	uint64_t source                       : 7;  /**< XMC 'source' of request causing error. If SOURCE<6>==0, SOURCE<5:0>=PPID else SOURCE<3:0>
-                                                         is BUSID of IOB which made the request. */
+	uint64_t source                       : 7;  /**< XMC 'source' of request causing error. If SOURCE<6>==0, SOURCE<5:0> = PPID else
+                                                         SOURCE<3:0> is BUSID of IOB which made the request. */
 	uint64_t node                         : 4;  /**< OCI Node of XMC request causing error. For BIG* errors NODE will always be the node
                                                          logging the error (BIG* errors are logged at the home node). For HOLE* errors, NODE could
                                                          be any OCI node in the system (HOLE* errors are logged at the requester node). */
@@ -6881,8 +6940,8 @@ union cvmx_l2c_tadx_int {
 	struct cvmx_l2c_tadx_int_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_36_63               : 28;
-	uint64_t wrdisoci                     : 1;  /**< (NYI) Illegal write to remote node with L2C_TAD_CTL[ENAOCI] clear. */
-	uint64_t rddisoci                     : 1;  /**< (NYI) Illegal read to remote node with L2C_TAD_CTL[ENAOCI] clear. */
+	uint64_t wrdisoci                     : 1;  /**< (NYI) Illegal write operation to a remote node with L2C_TAD_CTL[ENAOCI] clear. */
+	uint64_t rddisoci                     : 1;  /**< (NYI) Illegal read operation to a remote node with L2C_TAD_CTL[ENAOCI] clear. */
 	uint64_t rtgdbe                       : 1;  /**< RTG double-bit error */
 	uint64_t rtgsbe                       : 1;  /**< RTG single-bit error */
 	uint64_t reserved_15_31               : 17;
@@ -6971,22 +7030,22 @@ union cvmx_l2c_tadx_int {
 	uint64_t holerd                       : 1;  /**< Read reference to 256MB hole occurred. */
 	uint64_t holewr                       : 1;  /**< Write reference to 256MB hole occurred. */
 	uint64_t noway                        : 1;  /**< No way was available for allocation. L2C sets NOWAY during its processing of a transaction
-                                                         whenever it needed/wanted to allocate a WAY in the L2 cache, but was unable to. NOWAY==1
-                                                         is (generally) not an indication that L2C failed to complete transactions. Rather, it is a
-                                                         hint of possible performance degradation. (For example, L2C must read-modify-write DRAM
-                                                         for every transaction that updates some, but not all, of the bytes in a cache block,
-                                                         misses in the L2 cache, and cannot allocate a WAY.) There is one 'failure' case where L2C
-                                                         will set NOWAY: when it cannot leave a block locked in the L2 cache as part of a LCKL2
-                                                         transaction. See L2C_TTG_ERR for logged information. */
-	uint64_t tagdbe                       : 1;  /**< TAG double-bit error occurred. See L2C_TTG_ERR for logged information. */
-	uint64_t tagsbe                       : 1;  /**< TAG single-bit error occurred. See L2C_TTG_ERR for logged information. */
+                                                         whenever it needed/wanted to allocate a WAY in the L2 cache, but was unable to. When this
+                                                         bit = 1, it is (generally) not an indication that L2C failed to complete transactions.
+                                                         Rather, it is a hint of possible performance degradation. (For example, L2C must read-
+                                                         modify-write DRAM for every transaction that updates some, but not all, of the bytes in a
+                                                         cache block, misses in the L2 cache, and cannot allocate a WAY.) There is one 'failure'
+                                                         case where L2C sets NOWAY: when it cannot leave a block locked in the L2 cache as part of
+                                                         a LCKL2 transaction. See L2C_TTG(0..7)_ERR for logged information. */
+	uint64_t tagdbe                       : 1;  /**< TAG double-bit error occurred. See L2C_TTG(0..7)_ERR for logged information. */
+	uint64_t tagsbe                       : 1;  /**< TAG single-bit error occurred. See L2C_TTG(0..7)_ERR for logged information. */
 	uint64_t reserved_6_7                 : 2;
-	uint64_t fbfdbe                       : 1;  /**< FBF double-bit error occurred. See L2C_TQD_ERR for logged information. */
-	uint64_t fbfsbe                       : 1;  /**< FBF single-bit error occurred. See L2C_TQD_ERR for logged information. */
-	uint64_t sbfdbe                       : 1;  /**< SBF double-bit error occurred. See L2C_TQD_ERR for logged information. */
-	uint64_t sbfsbe                       : 1;  /**< SBF single-bit error occurred. See L2C_TQD_ERR for logged information. */
-	uint64_t l2ddbe                       : 1;  /**< L2D double-bit error occurred. See L2C_TQD_ERR for logged information. */
-	uint64_t l2dsbe                       : 1;  /**< L2D single-bit error occurred. See L2C_TQD_ERR for logged information. */
+	uint64_t fbfdbe                       : 1;  /**< FBF double-bit error occurred. See L2C_TQD(0..7)_ERR for logged information. */
+	uint64_t fbfsbe                       : 1;  /**< FBF single-bit error occurred. See L2C_TQD(0..7)_ERR for logged information. */
+	uint64_t sbfdbe                       : 1;  /**< SBF double-bit error occurred. See L2C_TQD(0..7)_ERR for logged information. */
+	uint64_t sbfsbe                       : 1;  /**< SBF single-bit error occurred. See L2C_TQD(0..7)_ERR for logged information. */
+	uint64_t l2ddbe                       : 1;  /**< L2D double-bit error occurred. See L2C_TQD(0..7)_ERR for logged information. */
+	uint64_t l2dsbe                       : 1;  /**< L2D single-bit error occurred. See L2C_TQD(0..7)_ERR for logged information. */
 #else
 	uint64_t l2dsbe                       : 1;
 	uint64_t l2ddbe                       : 1;
@@ -7013,8 +7072,8 @@ union cvmx_l2c_tadx_int {
 	struct cvmx_l2c_tadx_int_cn78xx {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_36_63               : 28;
-	uint64_t wrdisoci                     : 1;  /**< (NYI) Illegal write to remote node with L2C_TAD_CTL[ENAOCI] clear. */
-	uint64_t rddisoci                     : 1;  /**< (NYI) Illegal read to remote node with L2C_TAD_CTL[ENAOCI] clear. */
+	uint64_t wrdisoci                     : 1;  /**< (NYI) Illegal write operation to a remote node with L2C_TAD_CTL[ENAOCI] clear. */
+	uint64_t rddisoci                     : 1;  /**< (NYI) Illegal read operation to a remote node with L2C_TAD_CTL[ENAOCI] clear. */
 	uint64_t rtgdbe                       : 1;  /**< RTG double-bit error */
 	uint64_t rtgsbe                       : 1;  /**< RTG single-bit error */
 	uint64_t reserved_17_31               : 15;
@@ -7025,22 +7084,22 @@ union cvmx_l2c_tadx_int {
 	uint64_t holerd                       : 1;  /**< Read reference to 256MB hole occurred. */
 	uint64_t holewr                       : 1;  /**< Write reference to 256MB hole occurred. */
 	uint64_t noway                        : 1;  /**< No way was available for allocation. L2C sets NOWAY during its processing of a transaction
-                                                         whenever it needed/wanted to allocate a WAY in the L2 cache, but was unable to. NOWAY==1
-                                                         is (generally) not an indication that L2C failed to complete transactions. Rather, it is a
-                                                         hint of possible performance degradation. (For example, L2C must read-modify-write DRAM
-                                                         for every transaction that updates some, but not all, of the bytes in a cache block,
-                                                         misses in the L2 cache, and cannot allocate a WAY.) There is one 'failure' case where L2C
-                                                         will set NOWAY: when it cannot leave a block locked in the L2 cache as part of a LCKL2
-                                                         transaction. See L2C_TTG_ERR for logged information. */
-	uint64_t tagdbe                       : 1;  /**< TAG double-bit error occurred. See L2C_TTG_ERR for logged information. */
-	uint64_t tagsbe                       : 1;  /**< TAG single-bit error occurred. See L2C_TTG_ERR for logged information. */
+                                                         whenever it needed/wanted to allocate a WAY in the L2 cache, but was unable to. When this
+                                                         bit = 1, it is (generally) not an indication that L2C failed to complete transactions.
+                                                         Rather, it is a hint of possible performance degradation. (For example, L2C must read-
+                                                         modify-write DRAM for every transaction that updates some, but not all, of the bytes in a
+                                                         cache block, misses in the L2 cache, and cannot allocate a WAY.) There is one 'failure'
+                                                         case where L2C sets NOWAY: when it cannot leave a block locked in the L2 cache as part of
+                                                         a LCKL2 transaction. See L2C_TTG(0..7)_ERR for logged information. */
+	uint64_t tagdbe                       : 1;  /**< TAG double-bit error occurred. See L2C_TTG(0..7)_ERR for logged information. */
+	uint64_t tagsbe                       : 1;  /**< TAG single-bit error occurred. See L2C_TTG(0..7)_ERR for logged information. */
 	uint64_t reserved_6_7                 : 2;
-	uint64_t fbfdbe                       : 1;  /**< FBF double-bit error occurred. See L2C_TQD_ERR for logged information. */
-	uint64_t fbfsbe                       : 1;  /**< FBF single-bit error occurred. See L2C_TQD_ERR for logged information. */
-	uint64_t sbfdbe                       : 1;  /**< SBF double-bit error occurred. See L2C_TQD_ERR for logged information. */
-	uint64_t sbfsbe                       : 1;  /**< SBF single-bit error occurred. See L2C_TQD_ERR for logged information. */
-	uint64_t l2ddbe                       : 1;  /**< L2D double-bit error occurred. See L2C_TQD_ERR for logged information. */
-	uint64_t l2dsbe                       : 1;  /**< L2D single-bit error occurred. See L2C_TQD_ERR for logged information. */
+	uint64_t fbfdbe                       : 1;  /**< FBF double-bit error occurred. See L2C_TQD(0..7)_ERR for logged information. */
+	uint64_t fbfsbe                       : 1;  /**< FBF single-bit error occurred. See L2C_TQD(0..7)_ERR for logged information. */
+	uint64_t sbfdbe                       : 1;  /**< SBF double-bit error occurred. See L2C_TQD(0..7)_ERR for logged information. */
+	uint64_t sbfsbe                       : 1;  /**< SBF single-bit error occurred. See L2C_TQD(0..7)_ERR for logged information. */
+	uint64_t l2ddbe                       : 1;  /**< L2D double-bit error occurred. See L2C_TQD(0..7)_ERR for logged information. */
+	uint64_t l2dsbe                       : 1;  /**< L2D single-bit error occurred. See L2C_TQD(0..7)_ERR for logged information. */
 #else
 	uint64_t l2dsbe                       : 1;
 	uint64_t l2ddbe                       : 1;
@@ -7274,11 +7333,11 @@ union cvmx_l2c_tadx_tag {
 	uint64_t u64;
 	struct cvmx_l2c_tadx_tag_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t sblkdty                      : 4;  /**< Sub-block dirty bits. 70xx does not implement true sub-block dirty bits, therefore when
-                                                         L2C_TAD_TAG is written, it is set to all ones if DIRTY is 1, or all zeroes if DIRTY is
-                                                         zero. LTGL2I will always result in similar legal values being loaded. */
+	uint64_t sblkdty                      : 4;  /**< Sub-block dirty bits. INTERNAL: 70xx does not implement true sub-block dirty bits,
+                                                         therefore when L2C_TAD_TAG is written, it is set to all ones if DIRTY is 1, or all zeroes
+                                                         if DIRTY is zero. LTGL2I will always result in similar legal values being loaded. */
 	uint64_t reserved_57_59               : 3;
-	uint64_t businfo                      : 9;  /**< The bus info bits */
+	uint64_t businfo                      : 9;  /**< The bus information bits */
 	uint64_t reserved_47_47               : 1;
 	uint64_t ecc                          : 7;  /**< The tag ECC */
 	uint64_t reserved_3_39                : 37;
@@ -7327,9 +7386,9 @@ union cvmx_l2c_tadx_tag {
 	struct cvmx_l2c_tadx_tag_cn61xx       cn68xxp1;
 	struct cvmx_l2c_tadx_tag_cn70xx {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t sblkdty                      : 4;  /**< Sub-block dirty bits. 70xx does not implement true sub-block dirty bits, therefore when
-                                                         L2C_TAD_TAG is written, it is set to all ones if DIRTY is 1, or all zeroes if DIRTY is
-                                                         zero. LTGL2I will always result in similar legal values being loaded. */
+	uint64_t sblkdty                      : 4;  /**< Sub-block dirty bits. INTERNAL: 70xx does not implement true sub-block dirty bits,
+                                                         therefore when L2C_TAD_TAG is written, it is set to all ones if DIRTY is 1, or all zeroes
+                                                         if DIRTY is zero. LTGL2I will always result in similar legal values being loaded. */
 	uint64_t reserved_56_59               : 4;
 	uint64_t businfo                      : 8;  /**< The businfo bits. Legal values: when [55]==1, we are in idmode and [54:50] must be 0,
                                                          [49:48] are the PPVID of the PP which could be holding the block; when [55]==0, we are in
@@ -7362,11 +7421,11 @@ union cvmx_l2c_tadx_tag {
 	} cn70xx;
 	struct cvmx_l2c_tadx_tag_cn78xx {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t sblkdty                      : 4;  /**< Sub-block dirty bits. 70xx does not implement true sub-block dirty bits, therefore when
-                                                         L2C_TAD_TAG is written, it is set to all ones if DIRTY is 1, or all zeroes if DIRTY is
-                                                         zero. LTGL2I will always result in similar legal values being loaded. */
+	uint64_t sblkdty                      : 4;  /**< Sub-block dirty bits. INTERNAL: 70xx does not implement true sub-block dirty bits,
+                                                         therefore when L2C_TAD_TAG is written, it is set to all ones if DIRTY is 1, or all zeroes
+                                                         if DIRTY is zero. LTGL2I will always result in similar legal values being loaded. */
 	uint64_t reserved_57_59               : 3;
-	uint64_t businfo                      : 9;  /**< The bus info bits */
+	uint64_t businfo                      : 9;  /**< The bus information bits */
 	uint64_t reserved_47_47               : 1;
 	uint64_t ecc                          : 7;  /**< The tag ECC. This field is undefined if L2C_CTL[DISECC] is not 1 when the LTGL2I reads the tags. */
 	uint64_t tag                          : 20; /**< The tag. The tag is the corresponding bits from the L2C+LMC internal L2/DRAM byte address. */
@@ -7402,8 +7461,8 @@ typedef union cvmx_l2c_tadx_tag cvmx_l2c_tadx_tag_t;
  * though there are 32 LFBs/VABs in a full TAD, the number applies to both halves.
  * If MAXLFB is != 0, VBF_THRESH should be less than MAXLFB.
  * If MAXVBF is != 0, VBF_THRESH should be less than MAXVBF.
- * If MAXLFB == 0, EXLRQ + EXRRQ + EXFWD + EXVIC must be less than or equal to 13.
- * If MAXLFB != 0, EXLRQ + EXRRQ + EXFWD + EXVIC must be less than or equal to MAXLFB-3.
+ * If MAXLFB = 0, EXLRQ + EXRRQ + EXFWD + EXVIC must be less than or equal to 13.
+ * If MAXLFB != 0, EXLRQ + EXRRQ + EXFWD + EXVIC must be less than or equal to MAXLFB - 3.
  */
 union cvmx_l2c_tad_ctl {
 	uint64_t u64;
@@ -7411,15 +7470,15 @@ union cvmx_l2c_tad_ctl {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_28_63               : 36;
 	uint64_t exlrq                        : 4;  /**< Extra LFBs to reserve for locally generated XMC commands. None are reserved for functional
-                                                         correctness.  Ignored if L2C_OCI_CTL[ENAOCI] is 0. */
+                                                         correctness. Ignored if L2C_OCI_CTL[ENAOCI] is 0. */
 	uint64_t exrrq                        : 4;  /**< Extra LFBs to reserve for Rxxx OCI commands beyond the 1 required for OCI protocol
-                                                         functional correctness.  Ignored if L2C_OCI_CTL[ENAOCI] is 0. */
+                                                         functional correctness. Ignored if L2C_OCI_CTL[ENAOCI] is 0. */
 	uint64_t exfwd                        : 4;  /**< Extra LFBs to reserve for Fxxx/SINV OCI commands beyond the 1 required for OCI protocol
-                                                         functional correctness.  Ignored if L2C_OCI_CTL[ENAOCI] is 0. */
+                                                         functional correctness. Ignored if L2C_OCI_CTL[ENAOCI] is 0. */
 	uint64_t exvic                        : 4;  /**< Extra LFBs to reserve for VICx OCI commands beyond the 1 required for OCI protocol
-                                                         functional correctness.  Ignored if L2C_OCI_CTL[ENAOCI] is 0. */
+                                                         functional correctness. Ignored if L2C_OCI_CTL[ENAOCI] is 0. */
 	uint64_t vbf_thresh                   : 4;  /**< VBF threshold. When the number of in-use VBFs exceeds this number the L2C TAD increases
-                                                         the priority of all its writes in the LMC. */
+                                                         the priority of all its write operations in the LMC. */
 	uint64_t maxvbf                       : 4;  /**< Maximum VBFs in use at once (0 means 16, 1-15 as expected). */
 	uint64_t maxlfb                       : 4;  /**< Maximum VABs/LFBs in use at once (0 means 16, 1-15 as expected). */
 #else
@@ -7437,7 +7496,7 @@ union cvmx_l2c_tad_ctl {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_11_63               : 53;
 	uint64_t vbf_thresh                   : 3;  /**< VBF threshold. When the number of in-use VBFs exceeds this number the L2C TAD increases
-                                                         the priority of all its writes in the LMC. */
+                                                         the priority of all its write operations in the LMC. */
 	uint64_t reserved_7_7                 : 1;
 	uint64_t maxvbf                       : 3;  /**< Maximum VABs/LFBs in use at once (0 means 16, 1-15 as expected). */
 	uint64_t reserved_3_3                 : 1;
@@ -7551,11 +7610,11 @@ union cvmx_l2c_ttgx_bist_status {
 	struct cvmx_l2c_ttgx_bist_status_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_48_63               : 16;
-	uint64_t rtgfl                        : 16; /**< BIST failure status for RTG ways 0-15 */
+	uint64_t rtgfl                        : 16; /**< BIST failure status for RTG ways. */
 	uint64_t reserved_18_31               : 14;
-	uint64_t lrulfbfl                     : 1;  /**< Always zero for CN78XX. */
+	uint64_t lrulfbfl                     : 1;  /**< Reserved, always zero. */
 	uint64_t lrufl                        : 1;  /**< BIST failure status for tag LRU */
-	uint64_t tagfl                        : 16; /**< BIST failure status for TAG ways 0-15 */
+	uint64_t tagfl                        : 16; /**< BIST failure status for TAG ways. */
 #else
 	uint64_t tagfl                        : 16;
 	uint64_t lrufl                        : 1;
@@ -8015,7 +8074,7 @@ union cvmx_l2c_wpar_iobx {
 	struct cvmx_l2c_wpar_iobx_cn70xx {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_4_63                : 60;
-	uint64_t mask                         : 4;  /**< Way partitioning mask (1 means do not use). The read value of MASK will include bits set
+	uint64_t mask                         : 4;  /**< Way partitioning mask (1 means do not use). The read value of MASK includes bits set
                                                          because of the L2C cripple fuses. */
 #else
 	uint64_t mask                         : 4;
@@ -8057,7 +8116,7 @@ union cvmx_l2c_wpar_ppx {
 	struct cvmx_l2c_wpar_ppx_cn70xx {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_4_63                : 60;
-	uint64_t mask                         : 4;  /**< Way partitioning mask (1 means do not use). The read value of MASK will include bits set
+	uint64_t mask                         : 4;  /**< Way partitioning mask (1 means do not use). The read value of MASK includes bits set
                                                          because of the L2C cripple fuses. */
 #else
 	uint64_t mask                         : 4;
@@ -8102,18 +8161,18 @@ typedef union cvmx_l2c_xmcx_pfc cvmx_l2c_xmcx_pfc_t;
  * Note the following:
  * The ADD bus command chosen must not be a IOB-destined command or operation is UNDEFINED.
  * The ADD bus command will have SID forced to IOB, DID forced to L2C, no virtualization checks
- * performed (always pass), and xmdmsk forced to 0. Note that this implies that commands which
+ * performed (always pass), and xmdmsk forced to 0. Note that this implies that commands that
  * REQUIRE a STORE cycle (STP, STC, SAA, FAA, FAS) should not be used or the results are
- * unpredictable. The sid=IOB means that the way partitioning used for the command is
- * L2C_WPAR_IOB0/1. None of L2C_QOS_IOB0/1, L2C_QOS_PP(0..31), L2C_VIRTID_IOB0/1,
- * L2C_VIRTID_PP(0..31) are used for these commands.
+ * unpredictable. The sid = IOB means that the way partitioning used for the command is
+ * L2C_WPAR_IOB(0..1). Neither L2C_QOS_IOB(0..1) or L2C_QOS_PP(0..47) are used for these
+ * commands.
  * Any FILL responses generated by the ADD bus command are ignored. Generated STINs, however,
  * will correctly invalidate the required cores.
  * Any L2D read generated by the ADD bus command records the syndrome information in
  * L2C_TAD(0..3)_ECC0/1. If ECC is disabled prior to the CSR write, this provides the ability to
  * read the ECC bits directly. If ECC is not disabled, this should log zeros (assuming no ECC
  * errors were found in the block).
- * A write which arrives while the INUSE bit is set will block until the INUSE bit clears. This
+ * A write that arrives while the INUSE bit is set will block until the INUSE bit clears. This
  * gives software two options when needing to issue a stream of write operations to L2C_XMC_CMD:
  * polling on the INUSE bit, or allowing hardware to handle the interlock -- at the expense of
  * locking up the RSL bus for potentially tens of cycles at a time while waiting for an available
@@ -8133,7 +8192,7 @@ union cvmx_l2c_xmc_cmd {
                                                          ordered relative to other traffic) and HW can accept
                                                          another command. */
 	uint64_t reserved_47_62               : 16;
-	uint64_t qos                          : 3;  /**< QOS level to use for simulated XMC request. */
+	uint64_t qos                          : 3;  /**< QOS level to use for simulated ADD bus request. */
 	uint64_t node                         : 4;  /**< OCI node to use for simulated ADD bus request. */
 	uint64_t addr                         : 40; /**< Address to use for simulated XMC request (see Note 6) */
 #else
@@ -8173,12 +8232,12 @@ union cvmx_l2c_xmc_cmd {
                                                          another command. */
 	uint64_t cmd                          : 7;  /**< Command to use for simulated ADD bus request. A new request can be accepted. */
 	uint64_t reserved_47_55               : 9;
-	uint64_t qos                          : 3;  /**< QOS level to use for simulated XMC request. */
+	uint64_t qos                          : 3;  /**< QOS level to use for simulated ADD bus request. */
 	uint64_t node                         : 4;  /**< OCI node to use for simulated ADD bus request. */
-	uint64_t addr                         : 40; /**< Address to use for simulated ADD bus request. (See note The address written to L2C_XMC_CMD
-                                                         is a physical address. L2C performs hole removal and index aliasing (if enabled) on the
-                                                         written address and uses that for the command. This hole removed/index aliased address is
-                                                         what is returned on a read of the L2C_XMC_CMD register..) */
+	uint64_t addr                         : 40; /**< Address to use for simulated ADD bus request. (The address written to L2C_XMC_CMD is a
+                                                         physical address. L2C performs hole removal and index aliasing (if enabled) on the written
+                                                         address and uses that for the command. This hole-removed/index-aliased address is what is
+                                                         returned on a read of L2C_XMC_CMD.) */
 #else
 	uint64_t addr                         : 40;
 	uint64_t node                         : 4;
diff --git a/arch/mips/include/asm/octeon/cvmx-lapx-defs.h b/arch/mips/include/asm/octeon/cvmx-lapx-defs.h
index 84487a1..7c066bdd 100644
--- a/arch/mips/include/asm/octeon/cvmx-lapx-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-lapx-defs.h
@@ -337,11 +337,10 @@ union cvmx_lapx_cfg {
                                                          When ENA transitions from 0 to 1, LAP will build the free list and empty all queue lists.
                                                          Results are unpredictable if ENA is toggled with traffic outstanding. */
 	uint64_t lab_size                     : 3;  /**< Number of LABs versus size of each LAB. This register may only be changed when [ENA]=0.
-                                                         0x0 = 96 LABs, 16 words/LAB (1024 bits)
-                                                         0x1 = 128 LABs, 12 words/LAB (768 bits)
-                                                         0x2 = 192 LABs, 8 words/LAB (512 bits)
-                                                         0x3 = 256 LABs, 6 words/LAB (384 bits)
-                                                         0x4-0x7 Reserved */
+                                                         0x0 = 128 LABs, 16 words/LAB (1024 bits)
+                                                         0x1 = 170 LABs, 12 words/LAB (768 bits)
+                                                         0x2 = 256 LABs, 8 words/LAB (512 bits)
+                                                         0x3-0x7 Reserved */
 #else
 	uint64_t lab_size                     : 3;
 	uint64_t ena                          : 1;
diff --git a/arch/mips/include/asm/octeon/cvmx-lmcx-defs.h b/arch/mips/include/asm/octeon/cvmx-lmcx-defs.h
index fb62e8b..4d0fd65 100644
--- a/arch/mips/include/asm/octeon/cvmx-lmcx-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-lmcx-defs.h
@@ -1608,8 +1608,8 @@ union cvmx_lmcx_bist_ctl {
 	struct cvmx_lmcx_bist_ctl_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_4_63                : 60;
-	uint64_t dlcram_bist_status           : 1;  /**< DLC RAM BIST status, 1 means fail. */
-	uint64_t dlcram_bist_done             : 1;  /**< DLC RAM BIST complete indication, 1 means complete. */
+	uint64_t dlcram_bist_status           : 1;  /**< DLC RAM BIST status; 1 means fail. */
+	uint64_t dlcram_bist_done             : 1;  /**< DLC RAM BIST complete indication; 1 means complete. */
 	uint64_t start_bist                   : 1;  /**< Start BIST on DLC memory. */
 	uint64_t reserved_0_0                 : 1;
 #else
@@ -1709,22 +1709,21 @@ typedef union cvmx_lmcx_bist_result cvmx_lmcx_bist_result_t;
 /**
  * cvmx_lmc#_char_ctl
  *
- * INTERNAL: This register is an assortment of various control fields needed to charecterize the
- * DDR3 interface.
+ * This register provides an assortment of various control fields needed to characterize the DDR3
+ * interface.
  */
 union cvmx_lmcx_char_ctl {
 	uint64_t u64;
 	struct cvmx_lmcx_char_ctl_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_53_63               : 11;
-	uint64_t dq_char_check_lock           : 1;  /**< INTERNAL: Indicates if a lock has been achieved, will only go to 1 if a lock is
-                                                         achieved during the LFSR priming period after LMC(0..3)_CHAR_CTL[DQ_CHAR_CHECK_ENABLE]
-                                                         is set to 1, and will be forced back to 0 when LMC(0..3)_CHAR_CTL[DQ_CHAR_CHECK_ENABLE]
-                                                         is set to 0. */
-	uint64_t dq_char_check_enable         : 1;  /**< INTERNAL: Enable DQ pattern check, on transition from disabled to enable will
-                                                         clear the LMC*CHAR_DQ_ERR_COUNT CSR. */
-	uint64_t dq_char_bit_sel              : 3;  /**< INTERNAL: Select a bit within the byte for DQ characterization pattern check. */
-	uint64_t dq_char_byte_sel             : 4;  /**< INTERNAL: Select a byte of data for DQ characterization pattern check. */
+	uint64_t dq_char_check_lock           : 1;  /**< Indicates if a lock has been achieved. Is set to 1 only if a lock is achieved during the
+                                                         LFSR priming period after DQ_CHAR_CHECK_ENABLE is set to 1, and is forced back to 0 when
+                                                         DQ_CHAR_CHECK_ENABLE is set to 0. */
+	uint64_t dq_char_check_enable         : 1;  /**< Enable DQ pattern check. The transition from disabled to enabled clears
+                                                         LMC(0..3)_CHAR_DQ_ERR_COUNT. */
+	uint64_t dq_char_bit_sel              : 3;  /**< Select a bit within the byte for DQ characterization pattern check. */
+	uint64_t dq_char_byte_sel             : 4;  /**< Select a byte of data for DQ characterization pattern check. */
 	uint64_t dr                           : 1;  /**< Pattern at Data Rate (not Clock Rate) */
 	uint64_t skew_on                      : 1;  /**< Skew adjacent bits */
 	uint64_t en                           : 1;  /**< Enable characterization */
@@ -1798,7 +1797,7 @@ typedef union cvmx_lmcx_char_ctl cvmx_lmcx_char_ctl_t;
 /**
  * cvmx_lmc#_char_dq_err_count
  *
- * INTERNAL: This register counts error in the DQ characterization mode.
+ * This register is used to initiate the various control sequences in the LMC.
  *
  */
 union cvmx_lmcx_char_dq_err_count {
@@ -1806,7 +1805,7 @@ union cvmx_lmcx_char_dq_err_count {
 	struct cvmx_lmcx_char_dq_err_count_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_40_63               : 24;
-	uint64_t dq_err_count                 : 40; /**< INTERNAL: DQ error count. */
+	uint64_t dq_err_count                 : 40; /**< DQ error count. */
 #else
 	uint64_t dq_err_count                 : 40;
 	uint64_t reserved_40_63               : 24;
@@ -1820,8 +1819,8 @@ typedef union cvmx_lmcx_char_dq_err_count cvmx_lmcx_char_dq_err_count_t;
 /**
  * cvmx_lmc#_char_mask0
  *
- * INTERNAL: This register is an assortment of various control fields needed to charecterize the
- * DDR3 interface.
+ * This register provides an assortment of various control fields needed to characterize the DDR3
+ * interface.
  */
 union cvmx_lmcx_char_mask0 {
 	uint64_t u64;
@@ -1847,8 +1846,8 @@ typedef union cvmx_lmcx_char_mask0 cvmx_lmcx_char_mask0_t;
 /**
  * cvmx_lmc#_char_mask1
  *
- * INTERNAL: This register is an assortment of various control fields needed to charecterize the
- * DDR3 interface.
+ * This register provides an assortment of various control fields needed to characterize the DDR3
+ * interface.
  */
 union cvmx_lmcx_char_mask1 {
 	uint64_t u64;
@@ -1876,8 +1875,8 @@ typedef union cvmx_lmcx_char_mask1 cvmx_lmcx_char_mask1_t;
 /**
  * cvmx_lmc#_char_mask2
  *
- * INTERNAL: This register is an assortment of various control fields needed to charecterize the
- * DDR3 interface.
+ * This register provides an assortment of various control fields needed to characterize the DDR3
+ * interface.
  */
 union cvmx_lmcx_char_mask2 {
 	uint64_t u64;
@@ -1903,8 +1902,8 @@ typedef union cvmx_lmcx_char_mask2 cvmx_lmcx_char_mask2_t;
 /**
  * cvmx_lmc#_char_mask3
  *
- * INTERNAL: This register is an assortment of various control fields needed to charecterize the
- * DDR3 interface.
+ * This register provides an assortment of various control fields needed to characterize the DDR3
+ * interface.
  */
 union cvmx_lmcx_char_mask3 {
 	uint64_t u64;
@@ -1932,8 +1931,8 @@ typedef union cvmx_lmcx_char_mask3 cvmx_lmcx_char_mask3_t;
 /**
  * cvmx_lmc#_char_mask4
  *
- * INTERNAL: This register is an assortment of various control fields needed to charecterize the
- * DDR3 interface.
+ * This register is an assortment of various control fields needed to characterize the DDR3 interface.
+ *
  */
 union cvmx_lmcx_char_mask4 {
 	uint64_t u64;
@@ -2126,9 +2125,9 @@ union cvmx_lmcx_comp_ctl2 {
 	struct cvmx_lmcx_comp_ctl2_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_51_63               : 13;
-	uint64_t rclk_char_mode               : 1;  /**< INTERNAL: Select RCLK characterization mode. */
+	uint64_t rclk_char_mode               : 1;  /**< Reserved. INTERNAL: Select RCLK characterization mode. */
 	uint64_t reserved_40_49               : 10;
-	uint64_t ptune_offset                 : 4;  /**< Ptune Offset value. */
+	uint64_t ptune_offset                 : 4;  /**< Ptune offset value. */
 	uint64_t reserved_12_35               : 24;
 	uint64_t cmd_ctl                      : 4;  /**< Drive strength control for CMD/A/RESET_L drivers
                                                          0001 = 24 ohm
@@ -2242,71 +2241,71 @@ union cvmx_lmcx_comp_ctl2 {
 	struct cvmx_lmcx_comp_ctl2_cn70xx {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_51_63               : 13;
-	uint64_t rclk_char_mode               : 1;  /**< INTERNAL: Select RCLK characterization mode. */
+	uint64_t rclk_char_mode               : 1;  /**< Reserved. INTERNAL: Select RCLK characterization mode. */
 	uint64_t ddr__ptune                   : 5;  /**< DDR PCTL from compensation circuit. The encoded value provides debug information for the
                                                          compensation impedance on P-pullup. */
 	uint64_t ddr__ntune                   : 5;  /**< DDR NCTL from compensation circuit. The encoded value provides debug information for the
                                                          compensation impedance on N-pulldown. */
-	uint64_t ptune_offset                 : 4;  /**< Ptune Offset value. */
+	uint64_t ptune_offset                 : 4;  /**< Ptune offset value. */
 	uint64_t ntune_offset                 : 4;  /**< Ntune offset value. */
-	uint64_t m180                         : 1;  /**< Reserved; must be zero. */
+	uint64_t m180                         : 1;  /**< Reserved; must be zero. INTERNAL: Cap impedance at 180 ohm, instead of 240 ohm. */
 	uint64_t byp                          : 1;  /**< Bypass mode. When set, PTUNE,NTUNE are the compensation setting. When clear,
                                                          DDR_PTUNE,DDR_NTUNE are the compensation setting. */
 	uint64_t ptune                        : 5;  /**< PCTL impedance control in bypass mode. */
 	uint64_t ntune                        : 5;  /**< NCTL impedance control in bypass mode. */
 	uint64_t rodt_ctl                     : 4;  /**< RODT NCTL impedance control bits. This field controls ODT values during a memory read.
-                                                         In DDR3 mode:
-                                                         0000 = No ODT. 0011 = 40 ohm.
-                                                         0001 = 20 ohm. 0100 = 60 ohm.
-                                                         0010 = 30 ohm. 0101 = 120 ohm.
-                                                         0110-1111 = Reserved.
+                                                         0x0 = No ODT. 0x3 = 40 ohm.
+                                                         0x1 = 20 ohm. 0x4 = 60 ohm.
+                                                         0x2 = 30 ohm. 0x5 = 120 ohm.
+                                                         0x6-0xF = Reserved
                                                          In DDR4 mode:
-                                                         0000 = No ODT. 0100 = 120 ohm.
-                                                         0001 = 40 ohm. 0101 = 240 ohm.
-                                                         0010 = 60 ohm. 0110 =  34 ohm.
-                                                         0011 = 80 ohm. 0111 =  48 ohm. */
+                                                         0x0 = No ODT. 0x4 = 120 ohm.
+                                                         0x1 = 40 ohm. 0x5 = 240 ohm.
+                                                         0x2 = 60 ohm. 0x6 = 34 ohm.
+                                                         0x3 = 80 ohm. 0x7 = 48 ohm.
+                                                         0x8-0xF = Reserved */
 	uint64_t control_ctl                  : 4;  /**< Drive strength control for ODT, etc. drivers.
                                                          In DDR3 mode:
-                                                         0001 = 24 ohm.    0101 = 40 ohm.
-                                                         0010 = 26.67 ohm. 0110 = 48 ohm.
-                                                         0011 = 30 ohm.    0111 = 60 ohm.
-                                                         0100 = 34.3 ohm.  0000,1000-1111 = Reserved.
+                                                         0x1 = 24 ohm. 0x5 = 40 ohm.
+                                                         0x2 = 26.67 ohm. 0x6 = 48 ohm.
+                                                         0x3 = 30 ohm. 0x7 = 60 ohm.
+                                                         0x4 = 34.3 ohm. 0x0, 0x8-0xF = Reserved.
                                                          In DDR4 mode:
-                                                         0000 = Reserved.  0001 = Reserved.
-                                                         0010 = 26 ohm.    0011 = 30 ohm.
-                                                         0100 = 34 ohm.    0101 = 40 ohm.
-                                                         0110 = 48 ohm.    0111 = 68 ohm.
-                                                         1000-1111 = Reserved. */
+                                                         0x0 = Reserved. 0x4 = 34 ohm.
+                                                         0x1 = Reserved. 0x5 = 40 ohm.
+                                                         0x2 = 26 ohm. 0x6 = 48 ohm.
+                                                         0x3 = 30 ohm. 0x7 = 68 ohm.
+                                                         0x8-0xF = Reserved. */
 	uint64_t cmd_ctl                      : 4;  /**< Drive strength control for CMD/A/RESET_L drivers.
                                                          In DDR3 mode:
-                                                         0001 = 24 ohm. 0101 = 40 ohm.
-                                                         0010 = 26.67 ohm. 0110 = 48 ohm.
-                                                         0011 = 30 ohm. 0111 = 60 ohm.
-                                                         0100 = 34.3 ohm. 0000,1000-1111 = Reserved.
+                                                         0x1 = 24 ohm. 0x5 = 40 ohm.
+                                                         0x2 = 26.67 ohm. 0x6 = 48 ohm.
+                                                         0x3 = 30 ohm. 0x7 = 60 ohm.
+                                                         0x4 = 34.3 ohm. 0x0,0x8-0xF = Reserved.
                                                          In DDR4 mode:
-                                                         0000 = Reserved.  0001 = Reserved.
-                                                         0010 = 26 ohm.    0011 = 30 ohm.
-                                                         0100 = 34 ohm.    0101 = 40 ohm.
-                                                         0110 = 48 ohm.    0111 = 68 ohm.
-                                                         1000-1111 = Reserved. */
-	uint64_t ck_ctl                       : 4;  /**< ""Drive strength control for DDR#_CK_*_P/DDR#_DIMM*_CS*_L/DDR#_DIMM*_ODT_* /DDR#_DIMM*_CKE*
+                                                         0x0 = Reserved. 0x4 = 34 ohm.
+                                                         0x1 = Reserved. 0x5 = 40 ohm.
+                                                         0x2 = 26 ohm. 0x6 = 48 ohm.
+                                                         0x3 = 30 ohm. 0x7 = 68 ohm.
+                                                         0x8-0xF = Reserved. */
+	uint64_t ck_ctl                       : 4;  /**< "Drive strength control for DDR_CK_*_P/DDR_DIMM*_CS*_L/DDR_DIMM*_ODT_* /DDR#_DIMM*_CKE*
                                                          drivers.
                                                          In DDR3 mode:
-                                                         0001 = 24 ohm. 0101 = 40 ohm.
-                                                         0010 = 26.67 ohm. 0110 = 48 ohm.
-                                                         0011 = 30 ohm. 0111 = 60 ohm.
-                                                         0100 = 34.3 ohm. 0000,1000-1111 = Reserved."
+                                                         0x1 = 24 ohm. 0x5 = 40 ohm.
+                                                         0x2 = 26.67 ohm. 0x6 = 48 ohm.
+                                                         0x3 = 30 ohm. 0x7 = 60 ohm.
+                                                         0x4 = 34.3 ohm. 0x0,0x8-0xF = Reserved.
                                                          In DDR4 mode:
-                                                         0000 = Reserved.  0001 = Reserved.
-                                                         0010 = 26 ohm.    0011 = 30 ohm.
-                                                         0100 = 34 ohm.    0101 = 40 ohm.
-                                                         0110 = 48 ohm.    0111 = 68 ohm.
-                                                         1000-1111 = Reserved." */
-	uint64_t dqx_ctl                      : 4;  /**< "Drive strength control for DDR#_DQ* /DDR#_DQS_*_P/N drivers.
-                                                         0001 = 24 ohm. 0101 = 40 ohm.
-                                                         0010 = 26.67 ohm. 0110 = 48 ohm.
-                                                         0011 = 30 ohm. 0111 = 60 ohm.
-                                                         0100 = 34.3 ohm. 0000,1000-1111 = Reserved." */
+                                                         0x0 = Reserved. 0x4 = 34 ohm.
+                                                         0x1 = Reserved. 0x5 = 40 ohm.
+                                                         0x2 = 26 ohm. 0x6 = 48 ohm.
+                                                         0x3 = 30 ohm. 0x7 = 68 ohm.
+                                                         0x8-0xF = Reserved." */
+	uint64_t dqx_ctl                      : 4;  /**< Drive strength control for DDR_DQ* /DDR_DQS_*_P/N drivers.
+                                                         0x1 = 24 ohm. 0x5 = 40 ohm.
+                                                         0x2 = 26.67 ohm. 0x6 = 48 ohm.
+                                                         0x3 = 30 ohm. 0x7 = 60 ohm.
+                                                         0x4 = 34.3 ohm. 0x0,0x8-0xF = Reserved. */
 #else
 	uint64_t dqx_ctl                      : 4;
 	uint64_t ck_ctl                       : 4;
@@ -2348,10 +2347,9 @@ union cvmx_lmcx_config {
 	struct cvmx_lmcx_config_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_63_63               : 1;
-	uint64_t bg2_enable                   : 1;  /**< BG2 pin is active for DDR4 mode.  Only has an effect when LMC*_CONFIG[MODEDDR4] = 1.
-                                                         Typically only cleared for DDR4 x16 devices, where there is no BG2 pin on the device. */
-	uint64_t mode_x4dev                   : 1;  /**< DDR x4 device mode.  Set when using DIMMs with x4 devices or if using
-                                                         embedded x4 devices. */
+	uint64_t bg2_enable                   : 1;  /**< BG2 pin is active for DDR4 mode. Only has an effect when LMC(0..3)_CONFIG[MODEDDR4] = 1.
+                                                         Typically only cleared for DDR4 *16 devices, where there is no BG2 pin on the device. */
+	uint64_t mode_x4dev                   : 1;  /**< DDR *4 device mode. */
 	uint64_t mode32b                      : 1;  /**< 32b Datapath Mode                                          NS
                                                          Set to 1 if we use only 32 DQ pins
                                                          0 for 64b DQ mode. */
@@ -3510,8 +3508,8 @@ union cvmx_lmcx_config {
 	struct cvmx_lmcx_config_cn70xx {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_63_63               : 1;
-	uint64_t bg2_enable                   : 1;  /**< BG2 pin is active for DDR4 mode.  Only has an effect when LMC*_CONFIG[MODEDDR4] = 1.
-                                                         Typically only cleared for DDR4 x16 devices, where there is no BG2 pin on the device. */
+	uint64_t bg2_enable                   : 1;  /**< BG2 pin is active for DDR4 mode. Only has an effect when LMC(0..0)_CONFIG[MODEDDR4] = 1.
+                                                         Typically only cleared for DDR4 *16 devices, where there is no BG2 pin on the device. */
 	uint64_t mode_x4dev                   : 1;  /**< Always reads as 0 for 70xx devices, there is no x4 device support. */
 	uint64_t mode32b                      : 1;  /**< Always reads as 1 for 70xx devices, only 32b mode is supported. */
 	uint64_t scrz                         : 1;  /**< Hide LMC(0..0)_SCRAMBLE_CFG0 and LMC(0..0)_SCRAMBLE_CFG1 when set. */
@@ -3544,12 +3542,12 @@ union cvmx_lmcx_config {
                                                          low two bits of this largest setting is not 3 (i.e. EARLY_UNLOAD_D0_R0 = (maxset<1:0>
                                                          !=3)). */
 	uint64_t init_status                  : 4;  /**< Indicates status of initialization. INIT_STATUS[n] = 1 implies rank n has been
-                                                         initialized.  Software must set necessary RANKMASK bits before executing the
-                                                         initialization sequence using the LMC*_SEQ_CTL register.  If the rank has been
-                                                         selected for init with the RANKMASK bits, the INIT_STATUS bits will be set after
-                                                         successful initialization and after self-refresh exit.  INIT_STATUS determines
-                                                         the chip-selects that assert during refresh, ZQCS, precharge power-down entry/exit,
-                                                         and self-refresh entry SEQ_SEL's. */
+                                                         initialized.
+                                                         Software must set necessary RANKMASK bits before executing the initialization sequence
+                                                         using LMC(0..0)_SEQ_CTL. If the rank has been selected for init with the RANKMASK bits,
+                                                         the INIT_STATUS bits will be set after successful initialization and after self-refresh
+                                                         exit. INIT_STATUS determines the chip-selects that assert during refresh, ZQCS, precharge
+                                                         power-down entry/exit, and self-refresh entry SEQ_SEL's. */
 	uint64_t mirrmask                     : 4;  /**< "Mask determining which ranks are address-mirrored.
                                                          MIRRMASK<n> = 1 means Rank n addresses are mirrored for
                                                          0 <= n <= 3.
@@ -3584,26 +3582,25 @@ union cvmx_lmcx_config {
                                                          Write 0 for SINGLE ranked DIMMs." */
 	uint64_t sref_with_dll                : 1;  /**< Self-refresh entry/exit write mode registers. When set, self-refresh entry sequence writes
                                                          MR2 and MR1 (in this order, in all ranks), and self-refresh exit sequence writes MR1, MR0,
-                                                         MR2, and MR3 (in this order, for all ranks).  The write operations occur before
-                                                         self-refresh entry, and after self-refresh exit.  When clear, self-refresh entry and exit
+                                                         MR2, and MR3 (in this order, for all ranks). The write operations occur before self-
+                                                         refresh entry, and after self-refresh exit. When clear, self-refresh entry and exit
                                                          instruction sequences do not write any mode registers in the DDR3/4 parts. */
 	uint64_t early_dqx                    : 1;  /**< Set this bit to send DQx signals one CK cycle earlier for the case when the shortest DQx
                                                          lines have a larger delay than the CK line. */
-	uint64_t ref_zqcs_int                 : 22; /**< Refresh interval is represented in number of 512 CK cycle increments. ZQCS interval
-                                                         is represented in a number of refresh intervals.  A refresh sequence is triggered when
-                                                         bits <24:18> are equal to 0x0, and a ZQCS sequence is triggered when <39:18> are equal
-                                                         to 0x0.  The ZQCS timer only decrements when the refresh timer is 0.
+	uint64_t ref_zqcs_int                 : 22; /**< Refresh interval is represented in number of 512 CK cycle increments. ZQCS interval is
+                                                         represented in a number of refresh intervals. A refresh sequence is triggered when bits
+                                                         <24:18> are equal to 0x0, and a ZQCS sequence is triggered when <39:18> are equal to 0x0.
+                                                         The ZQCS timer only decrements when the refresh timer is 0.
                                                          Program <24:18> to RND-DN(TREFI/clkPeriod/512).
                                                          A value of 0 in bits <24:18> will effectively turn off refresh.
                                                          Program <36:25> to (RND-DN(ZQCS_Period / Refresh_Period) - 1), where Refresh_Period is the
-                                                         effective period programmed in bis <24:18>. Note that this value should always be greater
+                                                         effective period programmed in bits <24:18>. Note that this value should always be greater
                                                          than 32, to account for resistor calibration delays.
-                                                         000_00000000_00000000: Reserved
-                                                         Max Refresh interval = 127*512= 65024 CK cycles
+                                                         000_00000000_0000000: Reserved
+                                                         Max Refresh interval = 127 * 512= 65024 CK cycles
                                                          Max ZQCS interval = 32768 * 127 * 512 = 2130706432 CK cycles
                                                          If refresh interval is programmed to ~8us, max ZQCS interval is ~262ms, or ~4 ZQCS
-                                                         operations
-                                                         per second.
+                                                         operations per second.
                                                          LMC(0..0)_CONFIG[INIT_STATUS] determines which ranks receive the REF / ZQCS. LMC does not
                                                          send any refreshes / ZQCS's when LMC(0..0)_CONFIG[INIT_STATUS]=0. */
 	uint64_t reset                        : 1;  /**< Reset one-shot pulse for LMC(0..0)_OPS_CNT, LMC(0..0)_IFB_CNT, and LMC(0..0)_DCLK_CNT
@@ -3621,17 +3618,17 @@ union cvmx_lmcx_config {
 	uint64_t pbank_lsb                    : 4;  /**< "DIMM address bit select. Reverting to the explanation for ROW_LSB, PBANK_LSB would be:
                                                          ROW_LSB bit + \#rowbits + \#rankbits
                                                          Decoding for PBANK_LSB:
-                                                         - 0000:DIMM = mem_adr<28> / rank = mem_adr[27] (if RANK_ENA)
-                                                         - 0001:DIMM = mem_adr<29> / rank = mem_adr<28>      &quot;
-                                                         - 0010:DIMM = mem_adr<30> / rank = mem_adr<29>      &quot;
-                                                         - 0011:DIMM = mem_adr<31> / rank = mem_adr<30>      &quot;
-                                                         - 0100:DIMM = mem_adr<32> / rank = mem_adr<31>      &quot;
-                                                         - 0101:DIMM = mem_adr<33> / rank = mem_adr<32>      &quot;
-                                                         - 0110:DIMM = mem_adr<34> / rank = mem_adr<33>      &quot;
-                                                         - 0111:DIMM = mem_adr<35> / rank = mem_adr<34>      &quot;
-                                                         - 1000:DIMM = mem_adr<36> / rank = mem_adr<35>      &quot;
-                                                         - 1001:DIMM = 0 / rank = mem_adr<36>      &quot;
-                                                         - 1010-1111: Reserved
+                                                         0x0: DIMM = mem_adr<28>; if RANK_ENA=1, rank = mem_adr<27>
+                                                         0x1: DIMM = mem_adr<29>; if RANK_ENA=1, rank = mem_adr<28>
+                                                         0x2: DIMM = mem_adr<30>; if RANK_ENA=1, rank = mem_adr<29>
+                                                         0x3: DIMM = mem_adr<31>; if RANK_ENA=1, rank = mem_adr<30>
+                                                         0x4: DIMM = mem_adr<32>; if RANK_ENA=1, rank = mem_adr<31>
+                                                         0x5: DIMM = mem_adr<33>; if RANK_ENA=1, rank = mem_adr<32>
+                                                         0x6: DIMM = mem_adr<34>; if RANK_ENA=1, rank = mem_adr<33>
+                                                         0x7: DIMM = mem_adr<35>; if RANK_ENA=1, rank = mem_adr<34>
+                                                         0x8: DIMM = mem_adr<36>; if RANK_ENA=1, rank = mem_adr<35>
+                                                         0x9: DIMM = 0; if RANK_ENA=1, rank = mem_adr<36>
+                                                         0xA-0xF: reserved
                                                          For example, for a DIMM made of Samsung's K4B1G0846C-F7 1Gb (16M * 8 bit * 8 bank) DDR3
                                                          parts, the column address width = 10, so with 10b of col, 3b of bus, 3b of bank, ROW_LSB =
                                                          16. So, row = mem_adr<29:16>.
@@ -3659,7 +3656,7 @@ union cvmx_lmcx_config {
                                                          For example, for a DIMM made of Samsung's K4B1G0846C-F7 1GB (16M * 8 bit * 8 bank) DDR3
                                                          parts, the column address width = 10, so with 10b of col, 3b of bus, 3b of bank, ROW_LSB =
                                                          16. So, row = mem_adr<29:16>.
-                                                         Refer to ." */
+                                                         Refer to Cache-block Read Transaction Example." */
 	uint64_t ecc_ena                      : 1;  /**< ECC enable. When set, enables the 8b ECC check/correct logic. Should be 1 when used with
                                                          DIMMs with ECC; 0, otherwise.
                                                          When this mode is turned on, DQ<71:64> on write operations contains the ECC code generated
@@ -5338,9 +5335,9 @@ typedef union cvmx_lmcx_ddr2_ctl cvmx_lmcx_ddr2_ctl_t;
 /**
  * cvmx_lmc#_ddr4_dimm_ctl
  *
- * Note that this CSR is only used when LMC(0..3)_CONTROL[RDIMM_ENA] = 1. During an RCW init
- * sequence, this CSR controls LMC's write operations to the extended DDR4 control words in the
- * JEDEC standard registering clock driver on an RDIMM.
+ * This register is used only when LMC(0..3)_CONTROL[RDIMM_ENA] = 1. During an RCW initialization
+ * sequence, this register controls LMC's write operations to the extended DDR4 control words in
+ * the JEDEC standard registering clock driver on an RDIMM.
  */
 union cvmx_lmcx_ddr4_dimm_ctl {
 	uint64_t u64;
@@ -5366,17 +5363,16 @@ typedef union cvmx_lmcx_ddr4_dimm_ctl cvmx_lmcx_ddr4_dimm_ctl_t;
  * This register controls the DDR_CK frequency. For details, refer to CK Speed Programming. See
  * LMC Initialization Sequence for the initialization sequence.
  * DDR PLL Bringup sequence:
- * 1.  Write CLKF, DDR_PS_EN, DFM_PS_EN, DIFFAMP, CPS, CPB.
- * If test mode is going to be activated, then also write jtg__ddr_pll_tm_en1,
- * jtg__ddr_pll_tm_en2, jtg__ddr_pll_tm_en3,
+ * 1. Write CLKF, DDR_PS_EN, DFM_PS_EN, DIFFAMP, CPS, CPB. If test mode is going to be activated,
+ * then also write jtg__ddr_pll_tm_en1, jtg__ddr_pll_tm_en2, jtg__ddr_pll_tm_en3,
  * jtg__ddr_pll_tm_en4, jtg__dfa_pll_tm_en1, jtg__dfa_pll_tm_en2, jtg__dfa_pll_tm_en3,
  * jtg__dfa_pll_tm_en4, JTAG_TEST_MODE
- * 2.  Wait 128 ref clock cycles (7680 rclk cycles)
- * 3.  Write 1 to RESET_N
- * 4.  Wait 1152 ref clocks (1152*16 rclk cycles)
- * 5.  Write 0 to  DDR_DIV_RESET and DFM_DIV_RESET
- * 6.  Wait 10 ref clock cycles (160 rclk cycles) before bringing up the DDR interface
- * If test mode is going to be activated, wait an additional 8191 ref clocks (8191*16 rclk
+ * 2. Wait 128 ref clock cycles (7680 rclk cycles)
+ * 3. Write 1 to RESET_N
+ * 4. Wait 1152 ref clocks (1152*16 rclk cycles)
+ * 5. Write 0 to DDR_DIV_RESET and DFM_DIV_RESET
+ * 6. Wait 10 ref clock cycles (160 rclk cycles) before bringing up the DDR interface
+ * If test mode is going to be activated, wait an additional 8191 ref clocks (8191*16 rclk+
  * cycles) to allow PLL clock alignment.
  */
 union cvmx_lmcx_ddr_pll_ctl {
@@ -5384,8 +5380,8 @@ union cvmx_lmcx_ddr_pll_ctl {
 	struct cvmx_lmcx_ddr_pll_ctl_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_31_63               : 33;
-	uint64_t phy_dcok                     : 1;  /**< Set to power up PHY logic after setting LMC#_DDR_PLL_CTL[DDR4_MODE]. */
-	uint64_t ddr4_mode                    : 1;  /**< DDR4 mode select (0 for DDR3). */
+	uint64_t phy_dcok                     : 1;  /**< Set to power up PHY logic after setting LMC(0..3)_DDR_PLL_CTL[DDR4_MODE]. */
+	uint64_t ddr4_mode                    : 1;  /**< DDR4 mode select: 1 = DDR4, 0 = DDR3. */
 	uint64_t pll_fbslip                   : 1;  /**< PLL FBSLIP indication. */
 	uint64_t pll_lock                     : 1;  /**< PLL LOCK indication. */
 	uint64_t reserved_18_26               : 9;
@@ -5484,15 +5480,14 @@ union cvmx_lmcx_ddr_pll_ctl {
 	uint64_t pll_fbslip                   : 1;  /**< PLL FBSLIP indication. */
 	uint64_t pll_lock                     : 1;  /**< PLL LOCK indication. */
 	uint64_t pll_rfslip                   : 1;  /**< PLL RFSLIP indication. */
-	uint64_t clkr                         : 2;  /**< PLL post divider control. */
-	uint64_t jtg_test_mode                : 1;  /**< "Reserved; must be zero. INTERNAL: JTAG Test Mode. Clock alignment between DCLK & REFCLK
-                                                         as well as FCLK & REFCLK can only be performed after the ddr_pll_divider_reset is
-                                                         deasserted. SW need to wait atleast 10 reference clock cycles after deasserting
-                                                         pll_divider_reset before asserting LMC#_DDR_PLL_CTL[JTG_TEST_MODE]. During alignment
-                                                         (which can take upto 160 microseconds) DCLK and FCLK can exhibit some high frequency
-                                                         pulses. Therefore, all bring up activities in that clock domain need to be delayed (when
-                                                         the chip operates in jtg_test_mode) by about 160 microseconds to ensure that lock is
-                                                         achieved." */
+	uint64_t clkr                         : 2;  /**< PLL post-divider control. */
+	uint64_t jtg_test_mode                : 1;  /**< Reserved; must be zero. INTERNAL: JTAG test mode. Clock alignment between DCLK & REFCLK as
+                                                         well as FCLK & REFCLK can only be performed after the ddr_pll_divider_reset is deasserted.
+                                                         SW need to wait at least 10 reference clock cycles after deasserting pll_divider_reset
+                                                         before asserting LMC(0..0)_DDR_PLL_CTL[JTG_TEST_MODE]. During alignment (which can take up
+                                                         to 160 microseconds) DCLK and FCLK can exhibit some high-frequency pulses. Therefore, all
+                                                         bring up activities in that clock domain need to be delayed (when the chip operates in
+                                                         jtg_test_mode) by about 160 microseconds to ensure that lock is achieved. */
 	uint64_t ddr_div_reset                : 1;  /**< DDR postscalar divider reset. */
 	uint64_t ddr_ps_en                    : 4;  /**< DDR postscalar divide ratio. Determines the LMC CK speed.
                                                          0x0 = divide LMC PLL by 1.
@@ -5611,7 +5606,7 @@ typedef union cvmx_lmcx_delay_cfg cvmx_lmcx_delay_cfg_t;
  * cvmx_lmc#_dimm#_ddr4_params0
  *
  * This register contains values to be programmed into the extra DDR4 control words in the
- * corresponding (registered) DIMM.  These are control words RC1x through RC8x.
+ * corresponding (registered) DIMM. These are control words RC1x through RC8x.
  */
 union cvmx_lmcx_dimmx_ddr4_params0 {
 	uint64_t u64;
@@ -5645,7 +5640,7 @@ typedef union cvmx_lmcx_dimmx_ddr4_params0 cvmx_lmcx_dimmx_ddr4_params0_t;
  * cvmx_lmc#_dimm#_ddr4_params1
  *
  * This register contains values to be programmed into the extra DDR4 control words in the
- * corresponding (registered) DIMM.  These are control words RCBx through RC9x.
+ * corresponding (registered) DIMM. These are control words RC9x through RCBx.
  */
 union cvmx_lmcx_dimmx_ddr4_params1 {
 	uint64_t u64;
@@ -5910,7 +5905,8 @@ union cvmx_lmcx_dll_ctl2 {
 	uint64_t intf_en                      : 1;  /**< Interface enable. */
 	uint64_t dll_bringup                  : 1;  /**< DLL bring up. */
 	uint64_t dreset                       : 1;  /**< System-memory-clock domain reset. The reset signal that is used by the system-memory-clock
-                                                         domain is (DRESET -OR- core-clock reset). */
+                                                         domain is
+                                                         (DRESET -OR- core-clock reset). */
 	uint64_t quad_dll_ena                 : 1;  /**< DLL enable. */
 	uint64_t byp_sel                      : 4;  /**< Reserved; must be zero. INTERNAL: Bypass select.
                                                          0000 = no byte.
@@ -5920,9 +5916,9 @@ union cvmx_lmcx_dll_ctl2 {
                                                          1010 = all bytes.
                                                          1011-1111 = Reserved. */
 	uint64_t byp_setting                  : 9;  /**< Reserved; must be zero. INTERNAL: Bypass setting.
-                                                         DDR3-1600 : 00100010.
-                                                         DDR3-1333 : 00110010.
-                                                         DDR3-1066 : 01001011.
+                                                         DDR3-1600: 00100010.
+                                                         DDR3-1333: 00110010.
+                                                         DDR3-1066: 01001011.
                                                          DDR3-800  : 01110101.
                                                          DDR3-667  : 10010110.
                                                          DDR3-600  : 10101100. */
@@ -6143,7 +6139,7 @@ typedef union cvmx_lmcx_dll_ctl3 cvmx_lmcx_dll_ctl3_t;
  * cvmx_lmc#_dual_memcfg
  *
  * This register controls certain parameters of dual-memory configuration.
- * This register enables the design to have two, separate memory configurations, selected
+ * This register enables the design to have two separate memory configurations, selected
  * dynamically by the reference address. Note however, that both configurations share
  * LMC(0..3)_CONTROL[XOR_BANK], LMC(0..3)_CONFIG [PBANK_LSB], LMC(0..3)_CONFIG[RANK_ENA], and all
  * timing parameters.
@@ -6215,12 +6211,12 @@ union cvmx_lmcx_dual_memcfg {
 	uint64_t row_lsb                      : 3;  /**< Encoding used to determine which memory address bit position represents the low order DDR
                                                          ROW address. Refer to
                                                          LMC(0..0)_CONFIG[ROW_LSB].
-                                                         Refer to . */
+                                                         Refer to Cache-block Read Transaction Example. */
 	uint64_t reserved_4_15                : 12;
 	uint64_t cs_mask                      : 4;  /**< Chip-select mask. This mask corresponds to the four chip-select signals for a memory
                                                          configuration. Each reference address asserts one of the chip-select signals. If that
                                                          chip-select signal has its corresponding CS_MASK bit set, then the config1 parameters are
-                                                         used, otherwise the config0 parameters are used.  In 70xx, CS_MASK[3:2] must be cleared. */
+                                                         used, otherwise the config0 parameters are used. */
 #else
 	uint64_t cs_mask                      : 4;
 	uint64_t reserved_4_15                : 12;
@@ -6298,7 +6294,7 @@ typedef union cvmx_lmcx_ecc_synd cvmx_lmcx_ecc_synd_t;
 /**
  * cvmx_lmc#_ext_config
  *
- * This register has additional configuration and control bits for the LMC
+ * This register has additional configuration and control bits for the LMC.
  *
  */
 union cvmx_lmcx_ext_config {
@@ -6308,23 +6304,23 @@ union cvmx_lmcx_ext_config {
 	uint64_t reserved_21_63               : 43;
 	uint64_t vrefint_seq_deskew           : 1;  /**< Personality bit to change the operation of what is normally the internal
                                                          vref training sequence into the deskew training sequence. */
-	uint64_t read_ena_bprch               : 1;  /**< Drive DQx for one cycle longer than normal during write operations. */
-	uint64_t read_ena_fprch               : 1;  /**< Drive DQX starting one cycle earlier than normal during write operations. */
-	uint64_t slot_ctl_reset_force         : 1;  /**< Write 1 to reset the slot control override for all slot control registers.
-                                                         After writing a 1 to this bit, slot control registers will update with changes made to
-                                                         other timing control registers.  One shot operation, will automatically return to 0
-                                                         after a write to 1. */
-	uint64_t ref_int_lsbs                 : 9;  /**< These are the 9 LSBs for the refresh interval value, default to 0, but can be set to
-                                                         a non-zero value to get a more precise refresh interval. */
+	uint64_t read_ena_bprch               : 1;  /**< Enable pad receiver one cycle longer than normal during read operations. */
+	uint64_t read_ena_fprch               : 1;  /**< Enable pad receiver starting one cycle earlier than normal during read operations. */
+	uint64_t slot_ctl_reset_force         : 1;  /**< Write 1 to reset the slot-control override for all slot-control registers. After writing a
+                                                         1 to this bit, slot-control registers will update with changes made to other timing-
+                                                         control registers. This is a one-shot operation; it automatically returns to 0 after a
+                                                         write to 1. */
+	uint64_t ref_int_lsbs                 : 9;  /**< Refresh-interval value least-significant bits. The default is 0x0; but it can be set to a
+                                                         non-zero value to get a more precise refresh interval. */
 	uint64_t drive_ena_bprch              : 1;  /**< Drive DQx for one cycle longer than normal during write operations. */
 	uint64_t drive_ena_fprch              : 1;  /**< Drive DQX starting one cycle earlier than normal during write operations. */
-	uint64_t dlcram_flip_synd             : 2;  /**< INTERNAL: DLC RAM flip syndrome control bits. */
-	uint64_t dlcram_cor_dis               : 1;  /**< INTERNAL: DLC RAM correction disable control. */
-	uint64_t dlc_nxm_rd                   : 1;  /**< When set, enable NXM events for DLC reads.  Default is disabled, but
+	uint64_t dlcram_flip_synd             : 2;  /**< Reserved. INTERNAL: DLC RAM flip syndrome control bits. */
+	uint64_t dlcram_cor_dis               : 1;  /**< Reserved. INTERNAL: DLC RAM correction disable control. */
+	uint64_t dlc_nxm_rd                   : 1;  /**< When set, enable NXM events for HFA read operations. INTERNAL: Default is disabled, but
                                                          could be useful for debug of DLC/DFA accesses. */
-	uint64_t l2c_nxm_rd                   : 1;  /**< When set, enable NXM events for L2C reads.  Default is disabled
-                                                         as L2C NXM reads are possible and expected during normal operation. */
-	uint64_t l2c_nxm_wr                   : 1;  /**< When set, enable NXM events for L2C writes. */
+	uint64_t l2c_nxm_rd                   : 1;  /**< When set, enable NXM events for L2C read operations. INTERNAL: Default is disabled as L2C
+                                                         NXM read operations are possible and expected during normal operation. */
+	uint64_t l2c_nxm_wr                   : 1;  /**< When set, enable NXM events for L2C write operations. */
 #else
 	uint64_t l2c_nxm_wr                   : 1;
 	uint64_t l2c_nxm_rd                   : 1;
@@ -6342,51 +6338,25 @@ union cvmx_lmcx_ext_config {
 #endif
 	} s;
 	struct cvmx_lmcx_ext_config_s         cn70xx;
-	struct cvmx_lmcx_ext_config_cn78xx {
-#ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_20_63               : 44;
-	uint64_t read_ena_bprch               : 1;  /**< Drive DQx for one cycle longer than normal during write operations. */
-	uint64_t read_ena_fprch               : 1;  /**< Drive DQX starting one cycle earlier than normal during write operations. */
-	uint64_t slot_ctl_reset_force         : 1;  /**< Write 1 to reset the slot control override for all slot control registers.
-                                                         After writing a 1 to this bit, slot control registers will update with changes made to
-                                                         other timing control registers.  One shot operation, will automatically return to 0
-                                                         after a write to 1. */
-	uint64_t ref_int_lsbs                 : 9;  /**< These are the 9 LSBs for the refresh interval value, default to 0, but can be set to
-                                                         a non-zero value to get a more precise refresh interval. */
-	uint64_t drive_ena_bprch              : 1;  /**< Drive DQx for one cycle longer than normal during write operations. */
-	uint64_t drive_ena_fprch              : 1;  /**< Drive DQX starting one cycle earlier than normal during write operations. */
-	uint64_t dlcram_flip_synd             : 2;  /**< INTERNAL: DLC RAM flip syndrome control bits. */
-	uint64_t dlcram_cor_dis               : 1;  /**< INTERNAL: DLC RAM correction disable control. */
-	uint64_t dlc_nxm_rd                   : 1;  /**< When set, enable NXM events for DLC reads.  Default is disabled, but
-                                                         could be useful for debug of DLC/DFA accesses. */
-	uint64_t l2c_nxm_rd                   : 1;  /**< When set, enable NXM events for L2C reads.  Default is disabled
-                                                         as L2C NXM reads are possible and expected during normal operation. */
-	uint64_t l2c_nxm_wr                   : 1;  /**< When set, enable NXM events for L2C writes. */
-#else
-	uint64_t l2c_nxm_wr                   : 1;
-	uint64_t l2c_nxm_rd                   : 1;
-	uint64_t dlc_nxm_rd                   : 1;
-	uint64_t dlcram_cor_dis               : 1;
-	uint64_t dlcram_flip_synd             : 2;
-	uint64_t drive_ena_fprch              : 1;
-	uint64_t drive_ena_bprch              : 1;
-	uint64_t ref_int_lsbs                 : 9;
-	uint64_t slot_ctl_reset_force         : 1;
-	uint64_t read_ena_fprch               : 1;
-	uint64_t read_ena_bprch               : 1;
-	uint64_t reserved_20_63               : 44;
-#endif
-	} cn78xx;
+	struct cvmx_lmcx_ext_config_s         cn78xx;
 };
 typedef union cvmx_lmcx_ext_config cvmx_lmcx_ext_config_t;
 
 /**
  * cvmx_lmc#_fadr
  *
- * This register only captures the first transaction with ECC errors. A DED error can
- * over-write this register with its failing addresses if the first error was a SEC. If you write
- * LMC*_INT -> SEC_ERR/DED_ERR, it clears the error bits and captures the next failing
+ * This register only captures the first transaction with ECC errors. A DED error can over-write
+ * this register with its failing addresses if the first error was a SEC. If you write
+ * LMC(0..3)_INT -> SEC_ERR/DED_ERR, it clears the error bits and captures the next failing
  * address. If FDIMM is 1, that means the error is in the high DIMM.
+ * LMC(0..3)_FADR captures the failing pre-scrambled address location (split into DIMM, bunk,
+ * bank, etc). If scrambling is off, then LMC(0..3)_FADR will also capture the failing physical
+ * location in the DRAM parts. LMC(0..3)_SCRAMBLED_FADR captures the actual failing address
+ * location in the physical DRAM parts, i.e.,
+ * If scrambling is on, LMC(0..3)_SCRAMBLED_FADR contains the failing physical location in the
+ * DRAM parts (split into DIMM, bunk, bank, etc.)
+ * If scrambling is off, the pre-scramble and post-scramble addresses are the same; and so the
+ * contents of LMC(0..3)_SCRAMBLED_FADR match the contents of LMC(0..3)_FADR.
  */
 union cvmx_lmcx_fadr {
 	uint64_t u64;
@@ -6589,7 +6559,7 @@ union cvmx_lmcx_int {
 	struct cvmx_lmcx_int_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_12_63               : 52;
-	uint64_t ddr_err                      : 1;  /**< DDR RAM Error alert interrupt. */
+	uint64_t ddr_err                      : 1;  /**< DDR RAM error alert interrupt. */
 	uint64_t dlcram_ded_err               : 1;  /**< DLC RAM ECC double error detect (DED). */
 	uint64_t dlcram_sec_err               : 1;  /**< DLC RAM ECC single error correct (SEC). */
 	uint64_t ded_err                      : 4;  /**< Double Error detected (DED) of Rd Data
@@ -6745,7 +6715,7 @@ typedef union cvmx_lmcx_int_en cvmx_lmcx_int_en_t;
 /**
  * cvmx_lmc#_lane#_crc_swiz
  *
- * CRC bit swizzle for even and odd ranks.
+ * This register contains the CRC bit swizzle for even and odd ranks.
  *
  */
 union cvmx_lmcx_lanex_crc_swiz {
@@ -7657,27 +7627,27 @@ union cvmx_lmcx_modereg_params3 {
 	uint64_t wr_cmd_lat                   : 2;  /**< Write command latency when CRC and DM are both enabled. */
 	uint64_t fgrm                         : 3;  /**< Fine granularity refresh mode. */
 	uint64_t temp_sense                   : 1;  /**< Temperature sensor readout enable. */
-	uint64_t pda                          : 1;  /**< Per DRAM Addressability. */
+	uint64_t pda                          : 1;  /**< Per DRAM addressability. */
 	uint64_t gd                           : 1;  /**< Gear-down mode. */
 	uint64_t crc                          : 1;  /**< CRC mode. */
-	uint64_t lpasr                        : 2;  /**< LP Auto Self Refresh. */
-	uint64_t tccd_l                       : 3;  /**< TCCD_L timing parameter
-                                                         - 000: 4. 011: 7.
-                                                         - 001: 5. 100: 8.
-                                                         - 010: 6. 101-111: Reserved. */
+	uint64_t lpasr                        : 2;  /**< LP auto self refresh. */
+	uint64_t tccd_l                       : 3;  /**< TCCD_L timing parameter:
+                                                         0x0 = 4. 0x3 = 7.
+                                                         0x1 = 5. 0x4 = 8.
+                                                         0x2 = 6. 0x5-0x7 = reserved. */
 	uint64_t rd_dbi                       : 1;  /**< Read DBI. */
 	uint64_t wr_dbi                       : 1;  /**< Write DBI. */
 	uint64_t dm                           : 1;  /**< Data mask enable. */
 	uint64_t ca_par_pers                  : 1;  /**< Command/address persistent parity error mode. */
 	uint64_t odt_pd                       : 1;  /**< ODT in PD mode. */
 	uint64_t par_lat_mode                 : 3;  /**< Parity latency mode. */
-	uint64_t wr_preamble                  : 1;  /**< Write preamble, 0 = 1 nCK, 1 = 2 nCK. */
-	uint64_t rd_preamble                  : 1;  /**< Write preamble, 0 = 1 nCK, 1 = 2 nCK. */
+	uint64_t wr_preamble                  : 1;  /**< Write preamble, 0 = one nCK, 1 = two nCK. */
+	uint64_t rd_preamble                  : 1;  /**< Write preamble, 0 = one nCK, 1 = two nCK. */
 	uint64_t sre_abort                    : 1;  /**< Self refresh abort. */
-	uint64_t cal                          : 3;  /**< CS to CMD/ADDR latency mode (cycles). */
-	uint64_t vref_mon                     : 1;  /**< Internal VREF monitor, 0 = Disable, 1 = Enable. */
-	uint64_t tc_ref                       : 1;  /**< Temperature Controlled Refresh Range, 0 = Normal, 1 = Extended. */
-	uint64_t max_pd                       : 1;  /**< Maximum power down mode, 0 = Disable, 1 = Enable. */
+	uint64_t cal                          : 3;  /**< CS-to-CMD/ADDR latency mode (cycles). */
+	uint64_t vref_mon                     : 1;  /**< Internal VREF monitor: 0 = disable, 1 = enable. */
+	uint64_t tc_ref                       : 1;  /**< Temperature controlled refresh range: 0 = normal, 1 = extended. */
+	uint64_t max_pd                       : 1;  /**< Maximum power-down mode: 0 = disable, 1 = enable. */
 #else
 	uint64_t max_pd                       : 1;
 	uint64_t tc_ref                       : 1;
@@ -7712,16 +7682,15 @@ typedef union cvmx_lmcx_modereg_params3 cvmx_lmcx_modereg_params3_t;
 /**
  * cvmx_lmc#_mpr_data0
  *
- * Bits <63:0> of MPR data register.
+ * This register provides bits <63:0> of MPR data register.
  *
  */
 union cvmx_lmcx_mpr_data0 {
 	uint64_t u64;
 	struct cvmx_lmcx_mpr_data0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t mpr_data                     : 64; /**< MPR data bits <63:0>.  Bits <7:0> represent the MPR data for the lowest order
-                                                         x4 device (x4 device number 0), bits <15:8> represent x4 device number 1, ...,
-                                                         bits <63:56> are for x4 device number 7. */
+	uint64_t mpr_data                     : 64; /**< MPR data bits<63:0>. Bits<7:0> represent the MPR data for the lowest-order *4 device (*4
+                                                         device 0); bits<15:8> represent *4 device 1; ..., bits<63:56> are for *4 device 7. */
 #else
 	uint64_t mpr_data                     : 64;
 #endif
@@ -7734,16 +7703,15 @@ typedef union cvmx_lmcx_mpr_data0 cvmx_lmcx_mpr_data0_t;
 /**
  * cvmx_lmc#_mpr_data1
  *
- * Bits <127:64> of MPR data register.
+ * This register provides bits <127:64> of MPR data register.
  *
  */
 union cvmx_lmcx_mpr_data1 {
 	uint64_t u64;
 	struct cvmx_lmcx_mpr_data1_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t mpr_data                     : 64; /**< MPR data bits <127:64>.  Bits <7:0> of the field represent the MPR data for
-                                                         x4 device number 8, bits <15:8> represent x4 device numnber 9, ..., and bits
-                                                         <63:56> represent x4 device number 15. */
+	uint64_t mpr_data                     : 64; /**< MPR data bits<127:64>. Bits<7:0> represent the MPR data for *4 device 8; bits<15:8>
+                                                         represent *4 device 9; ...; bits<63:56> are for *4 device 15. */
 #else
 	uint64_t mpr_data                     : 64;
 #endif
@@ -7756,7 +7724,7 @@ typedef union cvmx_lmcx_mpr_data1 cvmx_lmcx_mpr_data1_t;
 /**
  * cvmx_lmc#_mpr_data2
  *
- * Bits <143:128> of MPR data register.
+ * This register provides bits <143:128> of MPR data register.
  *
  */
 union cvmx_lmcx_mpr_data2 {
@@ -7764,8 +7732,8 @@ union cvmx_lmcx_mpr_data2 {
 	struct cvmx_lmcx_mpr_data2_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_16_63               : 48;
-	uint64_t mpr_data                     : 16; /**< MPR data bits <143:128>.  Bits <7:0> of the field represent the MPR data for
-                                                         x4 device number 16, and bits <15:8> represent x4 device number 17. */
+	uint64_t mpr_data                     : 16; /**< MPR data bits<143:128>. Bits<7:0> represent the MPR data for *4 device 16; bits<15:8>
+                                                         represent *4 device 17. */
 #else
 	uint64_t mpr_data                     : 16;
 	uint64_t reserved_16_63               : 48;
@@ -7786,30 +7754,73 @@ union cvmx_lmcx_mr_mpr_ctl {
 	uint64_t u64;
 	struct cvmx_lmcx_mr_mpr_ctl_s {
 #ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_53_63               : 11;
+	uint64_t mr_wr_use_default_value      : 1;  /**< When set, write the value to the MR that is computed from the value set in various CSR
+                                                         fields that would be used during initialization, rather that using the value in the
+                                                         LMC(0..3)_MR_MPR_CTL[MR_WR_ADDR] CSR field.  Useful to re-write the same value or
+                                                         to change single bits without having to compute a whole new value for the MR. */
+	uint64_t mpr_whole_byte_enable        : 1;  /**< Reserved. INTERNAL: Select a whole byte of DRAM data to read when whole-byte mode enabled. */
+	uint64_t mpr_byte_select              : 4;  /**< Reserved. INTERNAL: Select a whole byte of DRAM data to read when whole-byte mode enabled. */
+	uint64_t mpr_bit_select               : 2;  /**< Select which of four bits to read for each nibble of DRAM data. Typically all four bits
+                                                         from a *4 device, or all eight bits from a *8 device, or all 16 bits from a *16 device
+                                                         carry the same data, but this field allows selection of which device bit will be used to
+                                                         read the MPR data. */
+	uint64_t mpr_wr                       : 1;  /**< MPR sequence will perform a write operation when set. */
+	uint64_t mpr_loc                      : 2;  /**< MPR location select for MPR sequence. Only makes a difference for DDR4. */
+	uint64_t mr_wr_pda_enable             : 1;  /**< PDA write enable. When set, MRW operations use PDA, enabled by MR_WR_PDA_MASK per device.
+                                                         Only available for DDR4 devices. */
+	uint64_t mr_wr_pda_mask               : 18; /**< PDA mask. If MR_WR_PDA_ENABLE = 1 and there is a 1 in the bit for this mask value, then
+                                                         the corresponding DRAM device is enabled for the PDA MR write operation.
+                                                         Bit<23> corresponds to the lowest order, *4 device, and bit<40> corresponds to the highest
+                                                         order *4 device, for a total of up to 18 devices. */
+	uint64_t mr_wr_rank                   : 2;  /**< Selects the DRAM rank for either MRW or MPR sequences. */
+	uint64_t mr_wr_sel                    : 3;  /**< Selects which MR to write with the MR write sequence.
+                                                         Which pins to drive and how to drive them is automatically controlled through the DDR3/4
+                                                         mode setting. Bits<19:18> are also used to select the MPR page for an MPR sequence.
+                                                         A value of 0x7 selects an RCW write for both DDR4 and DDR3 MRW operations. */
+	uint64_t mr_wr_addr                   : 18; /**< Sets a value for A<17:0> for MR write operations. Note that many of these bits must be 0
+                                                         for various MRs. Bits<7:0> are also used for write data on an MPR sequence write
+                                                         operation. */
+#else
+	uint64_t mr_wr_addr                   : 18;
+	uint64_t mr_wr_sel                    : 3;
+	uint64_t mr_wr_rank                   : 2;
+	uint64_t mr_wr_pda_mask               : 18;
+	uint64_t mr_wr_pda_enable             : 1;
+	uint64_t mpr_loc                      : 2;
+	uint64_t mpr_wr                       : 1;
+	uint64_t mpr_bit_select               : 2;
+	uint64_t mpr_byte_select              : 4;
+	uint64_t mpr_whole_byte_enable        : 1;
+	uint64_t mr_wr_use_default_value      : 1;
+	uint64_t reserved_53_63               : 11;
+#endif
+	} s;
+	struct cvmx_lmcx_mr_mpr_ctl_cn70xx {
+#ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_52_63               : 12;
-	uint64_t mpr_whole_byte_enable        : 1;  /**< Select a whole byte of DRAM data to read when whole byte mode enabled. */
-	uint64_t mpr_byte_select              : 4;  /**< Select a whole byte of DRAM data to read when whole byte mode enabled. */
-	uint64_t mpr_bit_select               : 2;  /**< Select which of 4 bits to read for each nibble of DRAM data.  Typically all 4 bits
-                                                         from a x4 device, or all 8 bits from a x8 device, or all 16 bits from a x16 device
-                                                         will carry the same data, but this fields allows selection of which device bit will
-                                                         be used to read the MPR data. */
+	uint64_t mpr_whole_byte_enable        : 1;  /**< Reserved. INTERNAL: Select a whole byte of DRAM data to read when whole-byte mode enabled. */
+	uint64_t mpr_byte_select              : 4;  /**< Reserved. INTERNAL: Select a whole byte of DRAM data to read when whole-byte mode enabled. */
+	uint64_t mpr_bit_select               : 2;  /**< Select which of four bits to read for each nibble of DRAM data. Typically all four bits
+                                                         from a *4 device, or all eight bits from a *8 device, or all 16 bits from a *16 device
+                                                         carry the same data, but this field allows selection of which device bit will be used to
+                                                         read the MPR data. */
 	uint64_t mpr_wr                       : 1;  /**< MPR sequence will perform a write operation when set. */
-	uint64_t mpr_loc                      : 2;  /**< MPR location select for MPR sequence, only makes a difference for DDR4. */
-	uint64_t mr_wr_pda_enable             : 1;  /**< PDA write enable, if set MRW operations will use PDA, enabled by the
-                                                         LMC(0..3)_MR_MPR_CTL[MR_WR_PDA_MASK] per device. */
-	uint64_t mr_wr_pda_mask               : 18; /**< PDA mask, if LMC(0..3)_MR_MPR_CTL[MR_WR_PDA_ENABLE] is set and there is a 1 in
-                                                         the bit for this make value, then the correpsonding DRAM device will be enabled
-                                                         for the PDA MR write.  Bit <23> corresponds to the lowest order, x4 device, and
-                                                         bit <40> corresponds to the highest order x4 device, for a total of up to 18
-                                                         devices. */
-	uint64_t mr_wr_rank                   : 2;  /**< This field selects the DRAM rank for either MRW or MPR sequences. */
-	uint64_t mr_wr_sel                    : 3;  /**< Use this field to select which MR to write with the MR write seqeunce.  Which pins
-                                                         to drive and how to drive them is automatically controlled through the DDR3/4 mode
-                                                         setting.  Bits <19:18> are also used to select the MPR page for an MPR sequence.
-                                                         MR_WR_SEL==7 selects an RCW write for both DDR4 and DDR3 MRW operations. */
-	uint64_t mr_wr_addr                   : 18; /**< Use this field to set a value for A<17:0> for MR writes.  Note that many
-                                                         of these bits must be 0 for various MRs.  The lower 8 bits, <7:0> of this
-                                                         field are also used for write data on an MPR sequence write operation. */
+	uint64_t mpr_loc                      : 2;  /**< MPR location select for MPR sequence. Only makes a difference for DDR4. */
+	uint64_t mr_wr_pda_enable             : 1;  /**< PDA write enable. When set, MRW operations use PDA, enabled by MR_WR_PDA_MASK per device.
+                                                         Only available for DDR4 devices. */
+	uint64_t mr_wr_pda_mask               : 18; /**< PDA mask. If MR_WR_PDA_ENABLE = 1 and there is a 1 in the bit for this mask value, then
+                                                         the corresponding DRAM device is enabled for the PDA MR write operation.
+                                                         Bit<23> corresponds to the lowest order, *4 device, and bit<40> corresponds to the highest
+                                                         order *4 device, for a total of up to 18 devices. */
+	uint64_t mr_wr_rank                   : 2;  /**< Selects the DRAM rank for either MRW or MPR sequences. */
+	uint64_t mr_wr_sel                    : 3;  /**< Selects which MR to write with the MR write sequence.
+                                                         Which pins to drive and how to drive them is automatically controlled through the DDR3/4
+                                                         mode setting. Bits<19:18> are also used to select the MPR page for an MPR sequence.
+                                                         A value of 0x7 selects an RCW write for both DDR4 and DDR3 MRW operations. */
+	uint64_t mr_wr_addr                   : 18; /**< Sets a value for A<17:0> for MR write operations. Note that many of these bits must be 0
+                                                         for various MRs. Bits<7:0> are also used for write data on an MPR sequence write
+                                                         operation. */
 #else
 	uint64_t mr_wr_addr                   : 18;
 	uint64_t mr_wr_sel                    : 3;
@@ -7823,8 +7834,7 @@ union cvmx_lmcx_mr_mpr_ctl {
 	uint64_t mpr_whole_byte_enable        : 1;
 	uint64_t reserved_52_63               : 12;
 #endif
-	} s;
-	struct cvmx_lmcx_mr_mpr_ctl_s         cn70xx;
+	} cn70xx;
 	struct cvmx_lmcx_mr_mpr_ctl_s         cn78xx;
 };
 typedef union cvmx_lmcx_mr_mpr_ctl cvmx_lmcx_mr_mpr_ctl_t;
@@ -7833,24 +7843,7 @@ typedef union cvmx_lmcx_mr_mpr_ctl cvmx_lmcx_mr_mpr_ctl_t;
  * cvmx_lmc#_nxm
  *
  * Following is the decoding for mem_msb/rank:
- * - 0000: mem_msb = mem_adr[25]
- * - 0001: mem_msb = mem_adr[26]
- * - 0010: mem_msb = mem_adr[27]
- * - 0011: mem_msb = mem_adr[28]
- * - 0100: mem_msb = mem_adr[29]
- * - 0101: mem_msb = mem_adr[30]
- * - 0110: mem_msb = mem_adr[31]
- * - 0111: mem_msb = mem_adr[32]
- * - 1000: mem_msb = mem_adr[33]
- * - 1001: mem_msb = mem_adr[34]
- * - 1010: mem_msb = mem_adr[35]
- * - 1011: mem_msb = mem_adr[36]
- * 1010-1111 = Reserved
- * For example, for a DIMM made of Samsung's K4B1G0846C-ZCF7 1Gb (16M * 8 bit * 8 bank) DDR3
- * parts, the column address width = 10; so with 10b of col, 3b of bus, 3b of bank, row_lsb = 16.
- * Therefore, row = mem_adr[29:16] and mem_msb = 4.
- * Note also that addresses greater than the max defined space (pbank_msb) are also treated as
- * NXM accesses.
+ *
  */
 union cvmx_lmcx_nxm {
 	uint64_t u64;
@@ -7870,16 +7863,14 @@ union cvmx_lmcx_nxm {
 	uint64_t mem_msb_d0_r1                : 4;  /**< Max Row MSB for DIMM0, RANK1/DIMM0 in Single Ranked */
 	uint64_t mem_msb_d0_r0                : 4;  /**< Max Row MSB for DIMM0, RANK0 */
 	uint64_t cs_mask                      : 8;  /**< Chip select mask.
-                                                         This mask corresponds to the chip selects for a memory
-                                                         configuration.  If LMC*_CONFIG[RANK_ENA]==0 then this
-                                                         mask must be set in pairs because each reference address
-                                                         will assert a pair of chip selects.  If the chip
-                                                         select(s) have a corresponding CS_MASK bit set, then the
-                                                         reference is to non-existent memory (NXM).  LMC will alias a
-                                                         NXM read reference to use the lowest, legal chip select(s)
-                                                         and return 0's. LMC normally discards NXM writes, but will
-                                                         also alias them when LMC*_CONTROL[NXM_WRITE_EN]=1.
-                                                         CS_MASK<7:4> must all be set in 6xxx */
+                                                         CS_MASK[3:0] corresponds to the 4 chip selects for a memory
+                                                         configuration.  If the memory configuration does not populate
+                                                         a rank of memory for a chip select, the corresponding bit in
+                                                         the CS_MASK field must be set, and for 6xxx devices bits
+                                                         CS_MASK[7:4] must always all be set.  LMC will alias a NXM
+                                                         read reference to use the lowest, legal chip select and
+                                                         return 0s for data.  LMC normally discards NXM writes, but
+                                                         will also alias them when LMC*_CONTROL[NXM_WRITE_EN]=1. */
 #else
 	uint64_t cs_mask                      : 8;
 	uint64_t mem_msb_d0_r0                : 4;
@@ -7921,17 +7912,18 @@ union cvmx_lmcx_nxm {
 	struct cvmx_lmcx_nxm_cn70xx {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_24_63               : 40;
-	uint64_t mem_msb_d1_r1                : 4;  /**< Reserved.  INTERNAL: Max row MSB for DIMM1, RANK1/DIMM1 in single ranked. */
-	uint64_t mem_msb_d1_r0                : 4;  /**< Reserved.  INTERNAL: Max row MSB for DIMM1, RANK0. */
+	uint64_t mem_msb_d1_r1                : 4;  /**< Max row MSB for DIMM1, RANK1/DIMM1 in single ranked. */
+	uint64_t mem_msb_d1_r0                : 4;  /**< Max row MSB for DIMM1, RANK0. */
 	uint64_t mem_msb_d0_r1                : 4;  /**< Max row MSB for DIMM0, RANK1/DIMM0 in single ranked. */
 	uint64_t mem_msb_d0_r0                : 4;  /**< Max row MSB for DIMM0, RANK0. */
 	uint64_t reserved_4_7                 : 4;
-	uint64_t cs_mask                      : 4;  /**< CS_MASK[1:0] corresponds to the 2 chip selects for a memory configuration.  If the
-                                                         memory configuration does not populate a rank of memory for a chip select, the
-                                                         corresponding bit in the CS_MASK field must be set, and for 70xx devices bits
-                                                         CS_MASK[3:2] must always both be set.  LMC will alias a NXM read reference to use
-                                                         the lowest, legal chip select and return 0s for data.  LMC normally discards NXM
-                                                         writes, but will also alias them when LMC*_CONTROL[NXM_WRITE_EN]=1. */
+	uint64_t cs_mask                      : 4;  /**< Chip select mask. This mask corresponds to the four chip selects for a memory
+                                                         configuration. If LMC(0..0)_CONFIG[RANK_ENA]=0 then this mask must be set in pairs because
+                                                         each reference address will assert a pair of chip selects. If the chip select(s) have a
+                                                         corresponding CS_MASK bit set, then the reference is to nonexistent memory (NXM). LMC will
+                                                         alias a NXM read reference to use the lowest, legal chip select(s) and return zeros. LMC
+                                                         normally discards NXM write operations, but will also alias them when LMC(0..0)_CONTROL
+                                                         [NXM_WRITE_EN]=1. */
 #else
 	uint64_t cs_mask                      : 4;
 	uint64_t reserved_4_7                 : 4;
@@ -7950,26 +7942,24 @@ typedef union cvmx_lmcx_nxm cvmx_lmcx_nxm_t;
 /**
  * cvmx_lmc#_nxm_fadr
  *
- * This register only captures the first transaction with a NXM error while an interrupt
- * is pending, and will only capture a subsequent event once the interrupt is cleared by
- * writing a 1 to LMC*_INT[NXM_ERR].  It captures the actual L2C-LMC address provided to
- * the LMC that caused the NXM error.  A read or write NXM error will only be captured if
- * enabled using the NXM event enables.
+ * This register captures only the first transaction with a NXM error while an interrupt is
+ * pending, and only captures a subsequent event once the interrupt is cleared by writing a 1 to
+ * LMC(0..3)_INT[NXM_ERR]. It captures the actual L2C-LMC address provided to the LMC that caused
+ * the NXM error. A read or write NXM error is captured only if enabled using the NXM event
+ * enables.
  */
 union cvmx_lmcx_nxm_fadr {
 	uint64_t u64;
 	struct cvmx_lmcx_nxm_fadr_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_39_63               : 25;
-	uint64_t nxm_src                      : 1;  /**< Indicates the source of the operation that caused a NXM error.
-                                                         - 0: source = L2C
-                                                         - 1: source = DLC */
-	uint64_t nxm_type                     : 1;  /**< Indicates the type of operation that caused NXM error.
-                                                         - 0: type = read
-                                                         - 1: type = write */
-	uint64_t nxm_faddr                    : 37; /**< Failing L2C-LMC address.  Bits [3:0] will always be zero for a DLC access, and
-                                                         bits [4:0] will be zero for an L2C access.  Bits [5:4] represent the fill order
-                                                         for an L2C read, and the start point within a cache line for a write. */
+	uint64_t nxm_src                      : 1;  /**< Indicates the source of the operation that caused a NXM error:
+                                                         0 = L2C, 1 = HFA */
+	uint64_t nxm_type                     : 1;  /**< Indicates the type of operation that caused NXM error:
+                                                         0 = Read, 1 = Write */
+	uint64_t nxm_faddr                    : 37; /**< Failing L2C-LMC address. Bits<3:0> are always 0s for an HFA access, and bits<4:0> are
+                                                         always 0s for an L2C access. Bits<5:4> represent the fill order for an L2C read operation,
+                                                         and the start point within a cache line for a write operation. */
 #else
 	uint64_t nxm_faddr                    : 37;
 	uint64_t nxm_type                     : 1;
@@ -8089,35 +8079,36 @@ union cvmx_lmcx_phy_ctl {
 	struct cvmx_lmcx_phy_ctl_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_51_63               : 13;
-	uint64_t phy_reset                    : 1;  /**< INTERNAL: Write to 1 to reset the PHY, one shot operation, will automatically
+	uint64_t phy_reset                    : 1;  /**< Reserved. INTERNAL: Write to 1 to reset the PHY, one-shot operation, will automatically
                                                          clear to value of 0. */
-	uint64_t dsk_dbg_rd_complete          : 1;  /**< INTERNAL: Indicates completion of a read operation, will clear to 0 when
-                                                         a read operation is started, then set to 1 when operation is complete. */
-	uint64_t dsk_dbg_rd_data              : 10; /**< INTERNAL: Data from a deskew read operation.  Only valid when the
+	uint64_t dsk_dbg_rd_complete          : 1;  /**< Reserved. INTERNAL: Indicates completion of a read operation, will clear to 0 when a read
+                                                         operation is started, then set to 1 when operation is complete. */
+	uint64_t dsk_dbg_rd_data              : 10; /**< Reserved. INTERNAL: Data from a deskew read operation. Only valid when the
                                                          LMCX_PHY_CTL[DSK_DBG_RD_COMPLETE] bit is set. */
-	uint64_t dsk_dbg_rd_start             : 1;  /**< INTERNAL: Write 1 to start deskew data read operation, will automatically
-                                                         clear to 0.  Write to 1 will also clear the complete bit. */
-	uint64_t dsk_dbg_clk_scaler           : 2;  /**< INTERNAL: Adjust clock toggle rate for reading deskew debug information:
-                                                         - 0: Deskew read clock toggles every 1 DCLK
-                                                         - 1: Deskew read clock toggles every 2 DCLKs
-                                                         - 2: Deskew read clock toggles every 3 DCLKs
-                                                         - 3: Deskew read clock toggles every 4 DCLKs */
-	uint64_t dsk_dbg_offset               : 2;  /**< INTERNAL: Offset to change delay of deskew debug data return time
-                                                         to LMC from DDR PHY. */
-	uint64_t dsk_dbg_num_bits_sel         : 1;  /**< INTERNAL: Deskew debug, select number of bits per byte lane.
-                                                         - 0: 8 bits per byte lane, no DBI
-                                                         - 1: 9 bits ber byte lane, including DBI */
-	uint64_t dsk_dbg_byte_sel             : 4;  /**< INTERNAL: Deskew debug byte select for read operation.  Values 0-3 correspond
-                                                         to byte lanes 0-3, 4 is for ECC, 5-8 are byte lanes 4-7. */
-	uint64_t dsk_dbg_bit_sel              : 4;  /**< INTERNAL: Deskew debug bit select for dsk read operation. */
+	uint64_t dsk_dbg_rd_start             : 1;  /**< Reserved. INTERNAL: Write 1 to start deskew data read operation, will automatically clear
+                                                         to 0. Write to 1 will also clear the complete bit. */
+	uint64_t dsk_dbg_clk_scaler           : 2;  /**< Reserved. INTERNAL: Adjust clock toggle rate for reading deskew debug information:
+                                                         0 = Deskew read clock toggles every 1 DCLK
+                                                         1 = Deskew read clock toggles every 2 DCLKs
+                                                         2 = Deskew read clock toggles every 3 DCLKs
+                                                         3 = Deskew read clock toggles every 4 DCLKs */
+	uint64_t dsk_dbg_offset               : 2;  /**< Reserved. INTERNAL: Offset to change delay of deskew debug data return time to LMC from
+                                                         DDR PHY. */
+	uint64_t dsk_dbg_num_bits_sel         : 1;  /**< Reserved. INTERNAL: Deskew debug, select number of bits per byte lane.
+                                                         0 = 8 bits per byte lane, no DBI
+                                                         1 = 9 bits ber byte lane, including DBI */
+	uint64_t dsk_dbg_byte_sel             : 4;  /**< Reserved. INTERNAL: Deskew debug byte select for read operation.  Values 0-3 correspond to
+                                                         byte lanes 0-3, 4 is for ECC, 5-8 are byte lanes 4-7. */
+	uint64_t dsk_dbg_bit_sel              : 4;  /**< Reserved. INTERNAL: Deskew debug bit select for dsk read operation. */
 	uint64_t dbi_mode_ena                 : 1;  /**< Enable DBI mode for PHY. */
 	uint64_t ddr_error_n_ena              : 1;  /**< Enable error_alert_n signal for PHY. */
-	uint64_t ref_pin_on                   : 1;  /**< INTERNAL: Voltage reference pin enabled. */
-	uint64_t dac_on                       : 1;  /**< INTERNAL: PHY DAC on. */
+	uint64_t ref_pin_on                   : 1;  /**< Reserved. INTERNAL: Voltage reference pin enabled. */
+	uint64_t dac_on                       : 1;  /**< Reserved. INTERNAL: PHY DAC on. */
 	uint64_t int_pad_loopback_ena         : 1;  /**< DDR pad loopback enable. */
 	uint64_t int_phy_loopback_ena         : 1;  /**< Internal PHY loopback enable. */
-	uint64_t phy_dsk_reset                : 1;  /**< Deskew bypass. */
-	uint64_t phy_dsk_byp                  : 1;  /**< Deskew bypass. */
+	uint64_t phy_dsk_reset                : 1;  /**< PHY deskew reset. When set, the deskew reset signal goes active if the vrefint/deskew
+                                                         training sequence is in the idle state. */
+	uint64_t phy_dsk_byp                  : 1;  /**< PHY deskew bypass. */
 	uint64_t phy_pwr_save_disable         : 1;  /**< DDR PHY power save disable. */
 	uint64_t ten                          : 1;  /**< DDR PHY test enable pin. */
 	uint64_t rx_always_on                 : 1;  /**< Disable dynamic DDR3 IO Rx power gating */
@@ -8586,8 +8577,30 @@ typedef union cvmx_lmcx_read_level_rankx cvmx_lmcx_read_level_rankx_t;
 /**
  * cvmx_lmc#_reset_ctl
  *
- * Specify the RSL base addresses for the block
- *
+ * "Specify the RSL base addresses for the block.
+ * &quot;DDR3RST DDR3 DRAM parts have a RESET# pin that wasn't present in DDR2 parts. The DDR3RST
+ * CSR field controls the assertion of the 7xxx pin that attaches to RESET#. When DDR3RST is set,
+ * 6xxx asserts RESET#. When DDR3RST is clear, 6xxx de-asserts RESET#. DDR3RST is set on a cold
+ * reset. Warm and soft chip resets do not affect the DDR3RST value. Outside of cold reset, only
+ * software CSR writes change the DDR3RST value. DDR3PWARM Enables preserve mode during a warm
+ * reset. When set, the DDR3 controller hardware automatically puts the attached DDR3 DRAM parts
+ * into self refresh (see LMC*CONFIG[SEQ_SEL] below) at the beginning of a warm reset sequence,
+ * provided that the DDR3 controller is up. When clear, the DDR3 controller hardware does not put
+ * the attached DDR3 DRAM parts into self-refresh during a warm reset sequence. DDR3PWARM is
+ * cleared on a cold reset. Warm and soft chip resets do not affect the DDR3PWARM value. Outside
+ * of cold reset, only software CSR writes change the DDR3PWARM value. Note that if a warm reset
+ * follows a soft reset, DDR3PWARM has no effect, as the DDR3 controller is no longer up after
+ * any cold/warm/soft reset sequence. DDR3PSOFT Enables preserve mode during a soft reset. When
+ * set, the DDR3 controller hardware automatically puts the attached DDR3 DRAM parts into self
+ * refresh (see LMC*CONFIG[SEQ_SEL] below) at the beginning of a soft reset sequence, provided
+ * that the DDR3 controller is up. When clear, the DDR3 controller hardware does not put the
+ * attached DDR3 DRAM parts into self-refresh during a soft reset sequence. DDR3PSOFT is cleared
+ * on a cold reset. Warm and soft chip resets do not affect the DDR3PSOFT value. Outside of cold
+ * reset, only software CSR writes change the DDR3PSOFT value. DDR3PSV May be useful for system
+ * software to determine when the DDR3 contents have been preserved. Cleared by hardware during a
+ * cold reset. Never cleared by hardware during a warm/soft reset. Set by hardware during a
+ * warm/soft reset if the hardware automatically put the DDR3 DRAM into self-refresh during the
+ * reset sequence. Can also be written by software (to any value).&quot;"
  */
 union cvmx_lmcx_reset_ctl {
 	uint64_t u64;
@@ -8631,20 +8644,20 @@ union cvmx_lmcx_rlevel_ctl {
 	struct cvmx_lmcx_rlevel_ctl_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_32_63               : 32;
-	uint64_t pattern                      : 8;  /**< Sets the data pattern used to match in read leveling operations. */
+	uint64_t pattern                      : 8;  /**< Sets the data pattern used to match in read-leveling operations. */
 	uint64_t reserved_22_23               : 2;
 	uint64_t delay_unload_3               : 1;  /**< When set, unload the PHY silo one cycle later
                                                          during read-leveling if LMC*_RLEVEL_RANKi[BYTE*<1:0>] = 3
-                                                         DELAY_UNLOAD_3 should normally be set, particularly at higher speeds. */
+                                                         DELAY_UNLOAD_3 should normally be set. */
 	uint64_t delay_unload_2               : 1;  /**< When set, unload the PHY silo one cycle later
                                                          during read-leveling if LMC*_RLEVEL_RANKi[BYTE*<1:0>] = 2
-                                                         DELAY_UNLOAD_2 should normally not be set. */
+                                                         DELAY_UNLOAD_2 should normally be set. */
 	uint64_t delay_unload_1               : 1;  /**< When set, unload the PHY silo one cycle later
                                                          during read-leveling if LMC*_RLEVEL_RANKi[BYTE*<1:0>] = 1
-                                                         DELAY_UNLOAD_1 should normally not be set. */
+                                                         DELAY_UNLOAD_1 should normally be set. */
 	uint64_t delay_unload_0               : 1;  /**< When set, unload the PHY silo one cycle later
                                                          during read-leveling if LMC*_RLEVEL_RANKi[BYTE*<1:0>] = 0
-                                                         DELAY_UNLOAD_0 should normally not be set. */
+                                                         DELAY_UNLOAD_0 should normally be set. */
 	uint64_t bitmask                      : 8;  /**< Mask to select bit lanes on which read-leveling
                                                          feedback is returned when OR_DIS is set to 1 */
 	uint64_t or_dis                       : 1;  /**< Disable or'ing of bits in a byte lane when computing
@@ -8682,16 +8695,16 @@ union cvmx_lmcx_rlevel_ctl {
 	uint64_t reserved_22_63               : 42;
 	uint64_t delay_unload_3               : 1;  /**< When set, unload the PHY silo one cycle later
                                                          during read-leveling if LMC*_RLEVEL_RANKi[BYTE*<1:0>] = 3
-                                                         DELAY_UNLOAD_3 should normally be set, particularly at higher speeds. */
+                                                         DELAY_UNLOAD_3 should normally be set. */
 	uint64_t delay_unload_2               : 1;  /**< When set, unload the PHY silo one cycle later
                                                          during read-leveling if LMC*_RLEVEL_RANKi[BYTE*<1:0>] = 2
-                                                         DELAY_UNLOAD_2 should normally not be set. */
+                                                         DELAY_UNLOAD_2 should normally be set. */
 	uint64_t delay_unload_1               : 1;  /**< When set, unload the PHY silo one cycle later
                                                          during read-leveling if LMC*_RLEVEL_RANKi[BYTE*<1:0>] = 1
-                                                         DELAY_UNLOAD_1 should normally not be set. */
+                                                         DELAY_UNLOAD_1 should normally be set. */
 	uint64_t delay_unload_0               : 1;  /**< When set, unload the PHY silo one cycle later
                                                          during read-leveling if LMC*_RLEVEL_RANKi[BYTE*<1:0>] = 0
-                                                         DELAY_UNLOAD_0 should normally not be set. */
+                                                         DELAY_UNLOAD_0 should normally be set. */
 	uint64_t bitmask                      : 8;  /**< Mask to select bit lanes on which read-leveling
                                                          feedback is returned when OR_DIS is set to 1 */
 	uint64_t or_dis                       : 1;  /**< Disable or'ing of bits in a byte lane when computing
@@ -8974,30 +8987,6 @@ typedef union cvmx_lmcx_rodt_ctl cvmx_lmcx_rodt_ctl_t;
  * pin for the rank that is being read should always be 0x0.
  * When a given RANK is selected, the RODT mask for that rank is used. The resulting RODT mask is
  * driven to the DIMMs in the following manner:
- * RANK_ENA=1                    RANK_ENA=0
- * Mask[3] -> DIMM1_ODT_1                    MBZ
- * Mask[2] -> DIMM1_ODT_0                          DIMM1_ODT_0
- * Mask[1] -> DIMM0_ODT_1                    MBZ
- * Mask[0] -> DIMM0_ODT_0                    DIMM0_ODT_0
- * LMC always reads entire cache blocks and always reads them via two consecutive
- * read CAS operations to the same rank+bank+row spaced exactly 4 CK's apart.
- * When a RODT mask bit is set, LMC asserts the OCTEON ODT output
- * pin(s) starting (CL CWL) CK's after the first read CAS operation. Then, OCTEON
- * normally continues to assert the ODT output pin(s) for 9+LMC*_CONTROL[RODT_BPRCH] more CK's
- * for a total of 10+LMC*_CONTROL[RODT_BPRCH] CK's for the entire cache block read -
- * through the second read CAS operation of the cache block,
- * satisfying the 6 CK DDR3 ODTH8 requirements.
- * But it is possible for OCTEON to issue two cache block reads separated by as few as
- * RtR = 8 or 9 (10 if LMC*_CONTROL[RODT_BPRCH]=1) CK's. In that case, OCTEON asserts the ODT
- * output pin(s)
- * for the RODT mask of the first cache block read for RtR CK's, then asserts
- * the ODT output pin(s) for the RODT mask of the second cache block read for
- * 10+LMC*_CONTROL[RODT_BPRCH] CK's
- * (or less if a third cache block read follows within 8 or 9 (or 10) CK's of this second cache
- * block read).
- * Note that it may be necessary to force LMC to space back-to-back cache block reads
- * to different ranks apart by at least 10+LMC*_CONTROL[RODT_BPRCH] CK's to prevent DDR3 ODTH8
- * violations.
  */
 union cvmx_lmcx_rodt_mask {
 	uint64_t u64;
@@ -9137,10 +9126,10 @@ typedef union cvmx_lmcx_scramble_cfg1 cvmx_lmcx_scramble_cfg1_t;
  * DRAM parts (split into DIMM, bunk, bank, etc);
  * If scrambling is off, the pre-scramble and post-scramble addresses are the same, and so the
  * contents of LMC(0..3)_SCRAMBLED_FADR match the contents of LMC(0..3)_FADR.
- * This register only captures the first transaction with ECC errors. A DED error can
- * over-write this register with its failing addresses if the first error was a SEC. If you write
- * LMC(0..3)_CONFIG -> SEC_ERR/DED_ERR, it clears the error bits and captures the next
- * failing address. If FDIMM is 1, that means the error is in the higher DIMM.
+ * This register only captures the first transaction with ECC errors. A DED error can over-write
+ * this register with its failing addresses if the first error was a SEC. If you write
+ * LMC(0..3)_CONFIG -> SEC_ERR/DED_ERR, it clears the error bits and captures the next failing
+ * address. If FDIMM is 1, that means the error is in the higher DIMM.
  */
 union cvmx_lmcx_scrambled_fadr {
 	uint64_t u64;
@@ -9220,63 +9209,41 @@ union cvmx_lmcx_seq_ctl {
 	struct cvmx_lmcx_seq_ctl_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_6_63                : 58;
-	uint64_t seq_complete                 : 1;  /**< This bit will clear when the LMC(0..3)_SEQ_CTL[INIT_START] bit is set to a 1, and will
-                                                         then be set to 1 when the sequence is completed. */
-	uint64_t seq_sel                      : 4;  /**< Selects the sequence that LMC runs after a 0->1 transition on
-                                                         LMC(0..3)_SEQ_CTL[INIT_START].
-                                                         0x0 = Power-up/init:
-                                                         RANKMASK selects participating ranks (should be all ranks with attached DRAM).
-                                                         DDR*_DIMM*_CKE* signals activated (if they weren't already active).
-                                                         RDIMM register control words 0-15 will be written to RANKMASK-selected RDIMMs when
-                                                         LMC(0..3)_CONTROL[RDIMM_ENA]=1 and corresponding LMC(0..3)_DIMM_CTL[DIMM*_WMASK] bits are
-                                                         set. (Refer to LMC(0..3)_DIMM0/1_PARAMS and LMC(0..3)_DIMM_CTL descriptions below for more
+	uint64_t seq_complete                 : 1;  /**< Sequence complete. This bit is cleared when INIT_START is set to a 1 and then is set to 1
+                                                         when the sequence is completed. */
+	uint64_t seq_sel                      : 4;  /**< Selects the sequence that LMC runs after a 0->1 transition on INIT_START.
+                                                         0x0 = Power-up/initialization:
+                                                         LMC(0..3)_CONFIG[RANKMASK] selects participating ranks (should be all ranks with attached
+                                                         DRAM). DDR*_DIMM*_CKE* signals are activated (if not already active). RDIMM register
+                                                         control words 0-15 are written to LMC(0..3)_CONFIG[RANKMASK]-selected RDIMMs when
+                                                         LMC(0..3)_CONTROL[RDIMM_ENA] = 1 and corresponding LMC(0..3)_DIMM_CTL[DIMM*_WMASK] bits
+                                                         are set. (Refer to LMC(0..3)_DIMM0/1_PARAMS and LMC(0..3)_DIMM_CTL descriptions for more
                                                          details.)
-                                                         MR0, MR1, MR2, and MR3 will be written to selected ranks.
+                                                         The DRAM registers MR0, MR1, MR2, and MR3 are written in the selected ranks.
                                                          0x1 = Read-leveling:
-                                                         RANKMASK selects the rank to be read-leveled.
-                                                         MR3 written to selected rank.
+                                                         LMC(0..3)_CONFIG[RANKMASK] selects the rank to be read-leveled. MR3 written in the
+                                                         selected rank.
                                                          0x2 = Self-refresh entry:
-                                                         INIT_STATUS selects participating ranks (should be all ranks with attached DRAM).
-                                                         MR1 and MR2 will be written to selected ranks if SREF_WITH_DLL=1.
-                                                         DDR*_DIMM*_CKE* signals de-activated.
+                                                         LMC(0..3)_CONFIG[INIT_STATUS] selects the participating ranks (should be all ranks with
+                                                         attached DRAM). MR1 and MR2 are written in the selected ranks if
+                                                         LMC(0..3)_CONFIG[SREF_WITH_DLL] = 1. DDR*_DIMM*_CKE* signals de-activated.
                                                          0x3 = Self-refresh exit:
-                                                         RANKMASK must be set to indicate participating ranks (should be all ranks with attached
-                                                         DRAM).
-                                                         DDR*_DIMM*_CKE* signals activated.
-                                                         MR0, MR1, MR2, and MR3 will be written to participating ranks if SREF_WITH_DLL=1.
-                                                         INIT_STATUS will be updated for ranks that are selected.
+                                                         LMC(0..3)_CONFIG[RANKMASK] must be set to indicate participating ranks (should be all
+                                                         ranks with attached DRAM). DDR*_DIMM*_CKE* signals activated. MR0, MR1, MR2, and MR3 are
+                                                         written in the participating ranks if LMC(0..3)_CONFIG[SREF_WITH_DLL] = 1.
+                                                         LMC(0..3)_CONFIG[INIT_STATUS] is updated for ranks that are selected.
                                                          0x6 = Write-leveling:
-                                                         RANKMASK selects the rank to be write-leveled.
-                                                         INIT_STATUS must indicate all ranks with attached DRAM.
-                                                         MR1 and MR2 written to INIT_STATUS-selected ranks.
-                                                         0x7 = Init RCW
-                                                         RANKMASK selects participating ranks (should be all ranks with attached DRAM).
-                                                         In DDR3 mode, RDIMM register control words 0-15 will be written to RANKMASK-selected
-                                                         RDIMMs when LMC(0..3)_CONTROL[RDIMM_ENA]=1 and corresponding
-                                                         LMC(0..3)_DIMM_CTL[DIMM*_WMASK]
-                                                         bits are set. (Refer to LMC(0..3)_DIMM0/1_PARAMS and LMC(0..3)_DIMM_CTL descriptions below
-                                                         for more details.)
-                                                         0x8 = MRW
-                                                         Mode Register Write sequence.
-                                                         0x9 = MPR
-                                                         MPR register read or write sequence.
-                                                         Self-refresh entry SEQ_SEL's may also be automatically
-                                                         generated by hardware upon a chip warm or soft reset
-                                                         sequence when LMC*_RESET_CTL[DDR3PWARM,DDR3PSOFT] are set.
-                                                         LMC writes the LMC*_MODEREG_PARAMS0 and LMC*_MODEREG_PARAMS1 CSR field values
-                                                         to the Mode registers in the DRAM parts (i.e. MR0, MR1, MR2, and MR3) as part of some of
-                                                         these sequences.
-                                                         Refer to the LMC*_MODEREG_PARAMS0 and LMC*_MODEREG_PARAMS1 descriptions for more details.
-                                                         If there are two consecutive power-up/init's without
-                                                         a DRESET assertion between them, LMC asserts DDR_DIMM*_CKE as part of
-                                                         the first power-up/init, and continues to assert DDR_DIMM*_CKE
-                                                         through the remainder of the first and the second power-up/init.
-                                                         If DDR_DIMM*_CKE deactivation and reactivation is needed for
-                                                         a second power-up/init, a DRESET assertion is required
-                                                         between the first and the second." */
-	uint64_t init_start                   : 1;  /**< A 0->1 transition starts the DDR memory sequence that is selected by
-                                                         LMC(0..3)_SEQ_CTL[SEQ_SEL]. This register is a oneshot and clears itself each time it is
-                                                         set. */
+                                                         LMC(0..3)_CONFIG[RANKMASK] selects the rank to be write-leveled.
+                                                         LMC(0..3)_CONFIG[INIT_STATUS] must indicate all ranks with attached DRAM. MR1 and MR2
+                                                         written in the LMC(0..3)_CONFIG[INIT_STATUS]-selected ranks.
+                                                         0x7 = Initialize RCW:
+                                                         LMC(0..3)_CONFIG[RANKMASK] selects participating ranks (should be all ranks with attached
+                                                         DRAM). In DDR3 mode, RDIMM register control words 0-15 are written to
+                                                         LMC(0..3)_CONFIG[RANKMASK]-selected RDIMMs when LMC(0..3)_CONTROL[RDIMM_ENA] = 1 and
+                                                         corresponding LMC(0..3)_DIMM_CTL[DIMM*_WMASK] bits are set. (Refer to
+                                                         LMC(0..3)_DIMM0/1_PARAMS and LMC(0..3)_DIMM_CTL descriptions for more details.) */
+	uint64_t init_start                   : 1;  /**< A 0->1 transition starts the DDR memory sequence that is selected by SEQ_SEL. This
+                                                         register is a one-shot and clears itself each time it is set. */
 #else
 	uint64_t init_start                   : 1;
 	uint64_t seq_sel                      : 4;
@@ -9295,34 +9262,20 @@ typedef union cvmx_lmcx_seq_ctl cvmx_lmcx_seq_ctl_t;
  * This register is an assortment of control fields needed by the memory controller. If software
  * has not previously written to this register (since the last DRESET), hardware updates the
  * fields in this register to the minimum allowed value when any of LMC(0..3)_RLEVEL_RANK(0..3),
- * LMC(0..3)_WLEVEL_RANK(0..3), LMC(0..3)_CONTROL and LMC(0..3)_MODEREG_PARAMS0 CSRs change.
- * Ideally, only read this register after LMC has been initialized and
+ * LMC(0..3)_WLEVEL_RANK(0..3), LMC(0..3)_CONTROL, and LMC(0..3)_MODEREG_PARAMS0 registers
+ * change. Ideally, only read this register after LMC has been initialized and
  * LMC(0..3)_RLEVEL_RANK(0..3), LMC(0..3)_WLEVEL_RANK(0..3) have valid data.
- * The field value is the minimum CK cycles between when the DRAM
- * part registers CAS commands of the first and second types from different cache blocks.
- *
- * "*_S_INIT" fields are DDR3 timing or DDR4 short timing parameters
- * "*_L_INIT" fields are DDR4 long timing parameters
- *
- * The hardware-calculated minimums are:
- * min R2R_S_INIT = 4
- * min R2W_S_INIT = 8 + (RL + MaxRdSkew) (WL + MinWrSkew) + LMC*_CONTROL[BPRCH]
- * min W2R_S_INIT = 5 + LMC*_TIMING_PARAMS1[TWTR] + WL
- * min W2W_S_INIT = 4
- * min R2R_L_INIT = LMC*_MODEREG_PARAMS3[TCCD_L] (decoded)
- * min R2W_L_INIT = 8 + (RL + MaxRdSkew) (WL + MinWrSkew) + LMC*_CONTROL[BPRCH]
- * min W2R_L_INIT = 5 + LMC*_TIMING_PARAMS2[TWTR_L] + WL
- * min W2W_L_INIT = LMC*_MODEREG_PARAMS3[TCCD_L] (decoded)
- * where
- * RL        = CL  + AL (LMC*_MODEREG_PARAMS0[CL] selects CL, LMC*_MODEREG_PARAMS0[AL] selects
- * AL)
- * WL        = CWL + AL (LMC*_MODEREG_PARAMS0[CWL] selects CWL)
- * MaxRdSkew = max(LMC*_RLEVEL_RANKi[BYTEj]/4) + 1
- * (max is across all ranks i (0..3) and bytes j (0..8))
- * MinWrSkew = min(LMC*_WLEVEL_RANKi[BYTEj]/8) LMC*_CONFIG[EARLY_DQX]
- * (min is across all ranks i (0..3) and bytes j (0..8))
- *
- * R2W_INIT has 1 CK cycle built in for OCTEON-internal ODT settling/channel turnaround time.
+ * The interpretation of the fields in this register depends on LMC(0)_CONFIG[DDR2T]:
+ * If LMC(0..3)_CONFIG[DDR2T]=1, (FieldValue + 4) is the minimum CK cycles between when the DRAM
+ * part registers CAS commands of the 1
+ * st and 2
+ * nd types from different cache blocks.
+ * If LMC(0..3)_CONFIG[DDR2T]=0, (FieldValue + 3) is the minimum CK cycles between when the DRAM
+ * part registers CAS commands of the 1
+ * st and 2
+ * nd types from different cache blocks. FieldValue = 0 is always illegal in this case.
+ * The hardware-calculated minimums for these fields are shown in LMC(0)_SLOT_CTL0 Hardware-
+ * Calculated Minimums.
  */
 union cvmx_lmcx_slot_ctl0 {
 	uint64_t u64;
@@ -9404,29 +9357,17 @@ typedef union cvmx_lmcx_slot_ctl0 cvmx_lmcx_slot_ctl0_t;
  * LMC(0..3)_WLEVEL_RANK(0..3), LMC(0..3)_CONTROL and LMC(0..3)_MODEREG_PARAMS0 CSRs change.
  * Ideally, only read this register after LMC has been initialized and
  * LMC(0..3)_RLEVEL_RANK(0..3), LMC(0..3)_WLEVEL_RANK(0..3) have valid data.
- * The field value is the minimum CK cycles between when the DRAM
- * part registers CAS commands of the first and second types from different cache blocks.
- *
- * The hardware-calculated minimums are:
- * min R2R_XRANK_INIT = 5 + MaxRdSkew MinRdSkew + LMC*_CONTROL[RODT_BPRCH]
- * min R2W_XRANK_INIT = 8 + (RL + MaxRdSkew) - (WL + MinWrSkew) + LMC*_CONTROL[BPRCH]
- * min W2R_XRANK_INIT = 6 + MaxWrSkew + LMC*_CONTROL[FPRCH2]
- * min W2W_XRANK_INIT = 7 + MaxWrSkew - MinWrSkew
- * where
- * RL        = CL  + AL (LMC*_MODEREG_PARAMS0[CL] selects CL, LMC*_MODEREG_PARAMS0[AL] selects
- * AL)
- * WL        = CWL + AL (LMC*_MODEREG_PARAMS0[CWL] selects CWL)
- * MinRdSkew = min(LMC*_RLEVEL_RANKi[BYTEj]/4)                              (min is across all
- * ranks i (0..3) and bytes j (0..8))
- * MaxRdSkew = max(LMC*_RLEVEL_RANKi[BYTEj]/4) + 1                          (max is across all
- * ranks i (0..3) and bytes j (0..8))
- * MinWrSkew = min(LMC*_WLEVEL_RANKi[BYTEj]/8) LMC*_CONFIG[EARLY_DQX]     (min is across all
- * ranks i (0..3) and bytes j (0..8))
- * MaxWrSkew = max(LMC*_WLEVEL_RANKi[BYTEj]/8) LMC*_CONFIG[EARLY_DQX] + 1 (max is across all
- * ranks i (0..3) and bytes j (0..8))
- * R2W_XRANK_INIT has 1 extra CK cycle built in for OCTEON-internal ODT settling/channel
- * turnaround time.
- * W2R_XRANK_INIT has 1 extra CK cycle built in for channel turnaround time.
+ * The interpretation of the fields in this CSR depends on LMC(0)_CONFIG[DDR2T]:
+ * If LMC(0..3)_CONFIG[DDR2T]=1, (FieldValue + 4) is the minimum CK cycles between when the DRAM
+ * part registers CAS commands of the 1
+ * st and 2
+ * nd types from different cache blocks.
+ * If LMC(0..3)_CONFIG[DDR2T]=0, (FieldValue + 3) is the minimum CK cycles between when the DRAM
+ * part registers CAS commands of the 1
+ * st and 2
+ * nd types from different cache blocks. FieldValue = 0 is always illegal in this case.
+ * The hardware-calculated minimums for these fields are shown in LMC(0)_SLOT_CTL1 Hardware-
+ * Calculated Minimums.
  */
 union cvmx_lmcx_slot_ctl1 {
 	uint64_t u64;
@@ -9471,34 +9412,19 @@ typedef union cvmx_lmcx_slot_ctl1 cvmx_lmcx_slot_ctl1_t;
  * This register is an assortment of control fields needed by the memory controller. If software
  * has not previously written to this register (since the last DRESET), hardware updates the
  * fields in this register to the minimum allowed value when any of LMC(0..3)_RLEVEL_RANK(0..3),
- * LMC(0..3)_WLEVEL_RANK(0..3)LMC*_WLEVEL_RANKn, LMC*_CONTROL and LMC*_MODEREG_PARAMS0 CSRs
- * change. Ideally, only read this register after LMC has been initialized and LMC*_RLEVEL_RANKn,
- * LMC*_WLEVEL_RANKn have valid data.
- *
- * The field value is the minimum CK cycles between when the DRAM
- * part registers CAS commands of the first and second types from different cache blocks.
- *
- * The hardware-calculated minimums are:
- * min R2R_XDIMM_INIT = 6 + MaxRdSkew MinRdSkew + LMC*_CONTROL[RODT_BPRCH]
- * min R2W_XDIMM_INIT = 9 + (RL + MaxRdSkew) - (WL + MinWrSkew) + LMC*_CONTROL[BPRCH]
- * min W2R_XDIMM_INIT = 6 + MaxWrSkew + LMC*_CONTROL[FPRCH2]
- * min W2W_XDIMM_INIT = 8 + MaxWrSkew - MinWrSkew
- * where
- * RL        = CL  + AL (LMC*_MODEREG_PARAMS0[CL] selects CL, LMC*_MODEREG_PARAMS0[AL] selects
- * AL)
- * WL        = CWL + AL (LMC*_MODEREG_PARAMS0[CWL] selects CWL)
- * MinRdSkew = min(LMC*_RLEVEL_RANKi[BYTEj]/4)                              (min is across all
- * ranks i (0..3) and bytes j (0..8))
- * MaxRdSkew = max(LMC*_RLEVEL_RANKi[BYTEj]/4) + 1                          (max is across all
- * ranks i (0..3) and bytes j (0..8))
- * MinWrSkew = min(LMC*_WLEVEL_RANKi[BYTEj]/8) LMC*_CONFIG[EARLY_DQX]     (min is across all
- * ranks i (0..3) and bytes j (0..8))
- * MaxWrSkew = max(LMC*_WLEVEL_RANKi[BYTEj]/8) LMC*_CONFIG[EARLY_DQX] + 1 (max is across all
- * ranks i (0..3) and bytes j (0..8))
- * R2W_XDIMM_INIT has 2 extra CK cycles built in for OCTEON-internal ODT settling/channel
- * turnaround time.
- * R2R_XDIMM_INIT, W2R_XRANK_INIT, W2W_XDIMM_INIT have 1 extra CK cycle built in for channel
- * turnaround time.
+ * LMC(0..3)_WLEVEL_RANK(0..3), LMC(0..3)_CONTROL and LMC(0..3)_MODEREG_PARAMS0 CSRs change.
+ * Ideally, only read this register after LMC has been initialized and
+ * LMC(0..3)_RLEVEL_RANK(0..3), LMC(0..3)_WLEVEL_RANK(0..3) have valid data.
+ * The interpretation of the fields in this CSR depends on LMC(0)_CONFIG[DDR2T]:
+ * If LMC(0..3)_CONFIG[DDR2T] = 1, (FieldValue + 4) is the minimum CK cycles between when the
+ * DRAM part registers CAS commands of the 1
+ * st and 2
+ * nd types from different cache blocks.
+ * If LMC(0..3)_CONFIG[DDR2T] = 0, (FieldValue + 3) is the minimum CK cycles between when the
+ * DRAM part registers CAS commands of the 1
+ * st and 2
+ * nd types from different cache blocks. FieldValue = 0 is always illegal in this case.
+ * The hardware-calculated minimums for these fields are shown in LMC Registers.
  */
 union cvmx_lmcx_slot_ctl2 {
 	uint64_t u64;
@@ -10136,30 +10062,29 @@ union cvmx_lmcx_timing_params2 {
 	struct cvmx_lmcx_timing_params2_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_15_63               : 49;
-	uint64_t trtp                         : 4;  /**< Set this field as follows:
+	uint64_t trtp                         : 4;  /**< Specifies the TRTP parameter, in cycles. Set this field as follows:
                                                          RNDUP[TRTP(ns) / TCYC(ns)] - 1,
-                                                         Indicates the tRTP parameter, in cycles.  For DDR3, TYP = max(4 nCK, 7.5ns),
-                                                         for DDR4 the rRTP parameter is dictated by the tWR MR bits. */
-	uint64_t t_rw_op_max                  : 4;  /**< Indicates the maximum delay for a read or write operation to complete, used to set the
+                                                         For DDR3, typical = max(4 nCK, 7.5ns).
+                                                         For DDR4 the TRTP parameter is dictated by the TWR MR bits. */
+	uint64_t t_rw_op_max                  : 4;  /**< Specifies the maximum delay for a read or write operation to complete, used to set the
                                                          timing of MRW and MPR operations. Set this field as follows:
-                                                         RNDUP[Max operation delay (cycles) / 8]
-                                                         TYP = 7 */
-	uint64_t twtr_l                       : 4;  /**< Set this field as follows:
-                                                         RNDUP[TWTR_L(ns) / TCYC(ns)] - 1,
-                                                         where TWTR_L is from the JEDEC DDR4 spec, and TCYC(ns) is the DDR clock frequency (not
+                                                         RNDUP[Maximum operation delay (cycles) / 8]
+                                                         Typical = 0x7. */
+	uint64_t twtr_l                       : 4;  /**< Specifies TWTR_L constraints. Set this field as follows:
+                                                         RNDUP[TWTR_L(ns) / TCYC(ns)] - 1
+                                                         where TWTR_L is from the JEDEC DDR4 spec, and TCYC(ns) is the DDR clock frequency (not the
                                                          data rate).
-                                                         TYP = max(4 nCK, 7.5 ns)
-                                                         INTERNAL: Seem the "- 1" is because we add 1 back into slot timing equation */
-	uint64_t trrd_l                       : 3;  /**< Indicates TRRD_L constraints. Set this field as follows:
+                                                         Typical = MAX(4 nCK, 7.5 ns)
+                                                         INTERNAL: Seems the '- 1' is because we add 1 back into slot timing equation */
+	uint64_t trrd_l                       : 3;  /**< Specifies TRRD_L constraints. Set this field as follows:
                                                          RNDUP[TRRD_L(ns) / TCYC(ns)] - 1,
-                                                         where TRRD_L is from the JEDEC DDR4 spec, and TCYC(ns) is the DDR clock frequency (not
+                                                         where TRRD_L is from the JEDEC DDR4 spec, and TCYC(ns) is the DDR clock frequency (not the
                                                          data rate).
-                                                         TYP = max(4 nCK, 7.5 ns)
-                                                         - 000: Reserved.
-                                                         - 001: 2 TCYC
-                                                         - ...
-                                                         - 110: 7 TCYC
-                                                         - 111: 8 TCYC */
+                                                         Typical = MAX(4 nCK, 7.5 ns)
+                                                         0x0 = reserved. 0x4 = five TCYC.
+                                                         0x1 = two TCYC. 0x5 = six TCYC.
+                                                         0x2 = three TCYC. 0x6 = seven TCYC.
+                                                         0x3 = four TCYC. 0x7 = eight TCYC. */
 #else
 	uint64_t trrd_l                       : 3;
 	uint64_t twtr_l                       : 4;
@@ -10567,10 +10492,10 @@ typedef union cvmx_lmcx_wodt_ctl1 cvmx_lmcx_wodt_ctl1_t;
  * Each rank has its own ODT pin that fans out to all of the memory parts in that DIMM. System
  * designers may prefer different combinations of ODT ONs for write operations into different
  * ranks. CN78XX supports full programmability by way of the mask register below. Each rank
- * position has its own 4-bit programmable field. When the controller does a write to that rank,
+ * position has its own 8-bit programmable field. When the controller does a write to that rank,
  * it sets the 4 ODT pins to the mask pins below. For example, when doing a write into Rank0, a
  * system designer may desire to terminate the lines with the resistor on DIMM0/Rank1. The mask
- * WODT_D0_R0 would then be [0010].
+ * WODT_D0_R0 would then be [00000010].
  * CN78XX drives the appropriate mask values on the ODT pins by default. If this feature is not
  * required, write 0x0 in this register. When a given RANK is selected, the WODT mask for that
  * RANK is used. The resulting WODT mask is driven to the DIMMs in the following manner:
diff --git a/arch/mips/include/asm/octeon/cvmx-mio-defs.h b/arch/mips/include/asm/octeon/cvmx-mio-defs.h
index 5a4d32e..7d56627 100644
--- a/arch/mips/include/asm/octeon/cvmx-mio-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-mio-defs.h
@@ -2212,10 +2212,7 @@ typedef union cvmx_mio_boot_bist_stat cvmx_mio_boot_bist_stat_t;
  * cvmx_mio_boot_comp
  *
  * This register sets the termination of boot-bus output pins.
- * Reset value is as follows:
- * no pullups,                               PCTL=6, NCTL=6 (50 ohm termination)
- * pullup on boot_ad[9]                      PCTL=7, NCTL=7 (40 ohm termination)
- * pullup on boot_ad[10].                    PCTL=4, NCTL=4 (75 ohm termination)
+ *
  */
 union cvmx_mio_boot_comp {
 	uint64_t u64;
@@ -2390,7 +2387,7 @@ union cvmx_mio_boot_dma_cfgx {
 	uint64_t size                         : 20; /**< DMA engine 0-1 size. SIZE is specified in number of bus transfers, where one transfer is
                                                          equal to the following number of bytes, dependent on MIO_BOOT_DMA_TIMn[WIDTH] and
                                                          MIO_BOOT_DMA_TIMn[DDR]:
-                                                         WIDTH DDR Transfer Size (bytes)
+                                                         WIDTH DDR  Transfer Size (bytes)
                                                          0 0 2
                                                          0 1 4
                                                          1 0 4
diff --git a/arch/mips/include/asm/octeon/cvmx-ocx-defs.h b/arch/mips/include/asm/octeon/cvmx-ocx-defs.h
index 81cdd02..65a36ea 100644
--- a/arch/mips/include/asm/octeon/cvmx-ocx-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-ocx-defs.h
@@ -814,7 +814,10 @@ union cvmx_ocx_com_linkx_ctl {
 	uint64_t u64;
 	struct cvmx_ocx_com_linkx_ctl_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_8_63                : 56;
+	uint64_t reserved_9_63                : 55;
+	uint64_t loopback                     : 1;  /**< Reserved. INTERNAL: Diagnostic data loopback.Set to force outgoing link to inbound port.
+                                                         All data and link credits are returned and appear to come from link partner. Typically
+                                                         SERDES should be disabled during this operation. */
 	uint64_t reinit                       : 1;  /**< Reinitialize Link. Setting bit forces link back into init state and also sets DROP bit.
                                                          Bit must be cleared for link to operate normally. */
 	uint64_t gate                         : 1;  /**< Enable clock gating on this link to save power. */
@@ -838,7 +841,8 @@ union cvmx_ocx_com_linkx_ctl {
 	uint64_t auto_clr                     : 1;
 	uint64_t gate                         : 1;
 	uint64_t reinit                       : 1;
-	uint64_t reserved_8_63                : 56;
+	uint64_t loopback                     : 1;
+	uint64_t reserved_9_63                : 55;
 #endif
 	} s;
 	struct cvmx_ocx_com_linkx_ctl_s       cn78xx;
@@ -1036,7 +1040,8 @@ union cvmx_ocx_lnex_int {
 	uint64_t u64;
 	struct cvmx_ocx_lnex_int_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_9_63                : 55;
+	uint64_t reserved_10_63               : 54;
+	uint64_t disp_err                     : 1;  /**< RX disparity error encountered. */
 	uint64_t bad_64b67b                   : 1;  /**< Bad 64B/67B codeword encountered. Once the bad word reaches the burst control unit, as
                                                          denoted by OCX_RXx_INT[LANE_BAD_WORD], it is tossed and all open packets will receive an
                                                          error. */
@@ -1059,7 +1064,8 @@ union cvmx_ocx_lnex_int {
 	uint64_t stat_msg                     : 1;
 	uint64_t stat_cnt_ovfl                : 1;
 	uint64_t bad_64b67b                   : 1;
-	uint64_t reserved_9_63                : 55;
+	uint64_t disp_err                     : 1;
+	uint64_t reserved_10_63               : 54;
 #endif
 	} s;
 	struct cvmx_ocx_lnex_int_s            cn78xx;
diff --git a/arch/mips/include/asm/octeon/cvmx-osm-defs.h b/arch/mips/include/asm/octeon/cvmx-osm-defs.h
index cd706b2..0ae576b 100644
--- a/arch/mips/include/asm/octeon/cvmx-osm-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-osm-defs.h
@@ -139,16 +139,16 @@ union cvmx_osm_ase_rate_limit_ctrl {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_3_63                : 61;
 	uint64_t rwc_rate_limit               : 1;  /**< To support ASE running at SCLK/2, OSM can rate-limit responses sent back to ASE. Each
-                                                         request is assigned to either phase 0 or phase 1. When set, OSM will not send back-to-back
-                                                         responses for requests on the RWC ports with the same phase. Instead a idle cycle will be
+                                                         request is assigned to either phase 0 or phase 1. When set, OSM does not send back-to-back
+                                                         responses for requests on the RWC ports with the same phase. Instead, a idle cycle is
                                                          inserted between the responses. This enable applies to RWC ports. */
 	uint64_t bwc_rate_limit               : 1;  /**< To support ASE running at SCLK/2, OSM can rate-limit responses sent back to ASE. Each
-                                                         request is assigned to either phase 0 or phase 1. When set, OSM will not send back-to-back
-                                                         responses for requests on the BWC port with the same phase. Instead a idle cycle will be
+                                                         request is assigned to either phase 0 or phase 1. When set, OSM does not send back-to-back
+                                                         responses for requests on the BWC port with the same phase. Instead a idle cycle is
                                                          inserted between the responses. */
 	uint64_t twc_rate_limit               : 1;  /**< To support ASE running at SCLK/2, OSM can rate-limit responses sent back to ASE. Each
-                                                         request is assigned to either phase 0 or phase 1. When set, OSM will not send back-to-back
-                                                         responses for requests on the TWC port with the same phase. Instead a idle cycle will be
+                                                         request is assigned to either phase 0 or phase 1. When set, OSM does not send back-to-back
+                                                         responses for requests on the TWC port with the same phase. Instead a idle cycle is
                                                          inserted between the responses. */
 #else
 	uint64_t twc_rate_limit               : 1;
@@ -169,24 +169,20 @@ union cvmx_osm_bankx_ctrl {
 	struct cvmx_osm_bankx_ctrl_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_3_63                : 61;
-	uint64_t bank_assign                  : 3;  /**< See OSM_BANK_ASSIGN_E enumeration for encoding.
-                                                         Port assignment for each memory bank. Memory structure is
-                                                         64k words x 246 data bits (plus ECC). This is further
-                                                         divided into 64 banks each containing 1k words x 246 data bits.
-                                                         A bank can only support one access per cycle, this is implemented
-                                                         by assigning each bank to a specific requester. Most requesters can
-                                                         only make one request per cycle so that mostly solves the problem.
-                                                         RWC is the only requester that can make multiple requests per cycle
-                                                         and will need to implement its own bank-aware scheduler to prevent
-                                                         bank conflicts. Bank assignment can be reconfigured dynamically, but
-                                                         memory accesses to a bank must be quiesced before that bank can be
-                                                         reassigned to another requester. A host request can access any bank,
-                                                         arbitration logic will prevent bank conflicts for host requests.
+	uint64_t bank_assign                  : 3;  /**< Port assignment for each memory bank. Memory structure is 64k words * 246 data bits (plus
+                                                         ECC). This is further divided into 64 banks each containing 1k words * 246 data bits. A
+                                                         bank can only support one access per cycle, this is implemented by assigning each bank to
+                                                         a specific requester. Most requesters can only make one request per cycle so that mostly
+                                                         solves the problem. RWC is the only requester that can make multiple requests per cycle
+                                                         and will need to implement its own bank-aware scheduler to prevent bank conflicts. Bank
+                                                         assignment can be reconfigured dynamically, but memory accesses to a bank must be quiesced
+                                                         before that bank can be reassigned to another requester. A host request can access any
+                                                         bank; arbitration logic will prevent bank conflicts for host requests.
                                                          Addresses: bit<15:10> = bank, bit<9:0> = offset.
-                                                         Bank 0 corresponds to memory address 0x0000-0x03ff.
-                                                         Bank 1 corresponds to memory address 0x0400-0x07ff.
-                                                         Bank 63 corresponds to memory address 0xfc00-0xffff.
-                                                         See OSM_BANK_ASSIGN_E enumeration for encoding. */
+                                                         Bank 0 corresponds to memory address 0x0000-0x03FF.
+                                                         Bank 1 corresponds to memory address 0x0400-0x07FF.
+                                                         Bank 63 corresponds to memory address 0xFC00-0xFFFF.
+                                                         See OSM_BANK_ASSIGN_E for encoding. */
 #else
 	uint64_t bank_assign                  : 3;
 	uint64_t reserved_3_63                : 61;
@@ -199,7 +195,7 @@ typedef union cvmx_osm_bankx_ctrl cvmx_osm_bankx_ctrl_t;
 /**
  * cvmx_osm_ecc_ctrl
  *
- * ECC Control register.
+ * ECC control register.
  *
  */
 union cvmx_osm_ecc_ctrl {
@@ -226,18 +222,17 @@ typedef union cvmx_osm_ecc_ctrl cvmx_osm_ecc_ctrl_t;
 /**
  * cvmx_osm_int_info_addr
  *
- * Address error interrupt info.
  * This register can be used to debug address errors (illegal bank). Fields are captured when
- * there are no outstanding address errors indicated in OSM_INT_STAT and a new address
- * error arrives. Prioritization for multiple events occurring at the same time is indicated by
- * the OSM_ADDR_ERR_SOURCE_E enumeration; highest encoded value has highest priority.
+ * there are no outstanding address errors indicated in OSM_INT_STAT and a new address error
+ * arrives. Prioritization for multiple events occurring at the same time is indicated by the
+ * OSM_ADDR_ERR_SOURCE_E enumeration; highest encoded value has highest priority.
  */
 union cvmx_osm_int_info_addr {
 	uint64_t u64;
 	struct cvmx_osm_int_info_addr_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_35_63               : 29;
-	uint64_t addr_err_source              : 3;  /**< Source of address error, see OSM_OSM_ADDR_ERR_SOURCE_E enumeration. */
+	uint64_t addr_err_source              : 3;  /**< Source of address error; see OSM_ADDR_ERR_SOURCE_E */
 	uint64_t reserved_16_31               : 16;
 	uint64_t addr_err_address             : 16; /**< RAM address of the address error. */
 #else
@@ -254,19 +249,18 @@ typedef union cvmx_osm_int_info_addr cvmx_osm_int_info_addr_t;
 /**
  * cvmx_osm_int_info_ecc
  *
- * ECC error interrupt info.
  * This register can be used to debug ECC failures. Fields are captured when there are no
- * outstanding ECC errors indicated in OSM_INT_STAT and a new ECC error arrives.
- * Prioritization for multiple events occurring at the same time is indicated by the
- * OSM_ECC_ERR_SOURCE_E enumeration; highest encoded value has highest priority. For current bank
- * assignment, see OSM_BANK(0..63)_CTRL.
+ * outstanding ECC errors indicated in OSM_INT_STAT and a new ECC error arrives. Prioritization
+ * for multiple events occurring at the same time is indicated by the OSM_ECC_ERR_SOURCE_E
+ * enumeration; highest encoded value has highest priority. For current bank assignment, see
+ * OSM_BANK(0..63)_CTRL.
  */
 union cvmx_osm_int_info_ecc {
 	uint64_t u64;
 	struct cvmx_osm_int_info_ecc_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_37_63               : 27;
-	uint64_t ecc_err_source               : 5;  /**< Source of ECC error, see OSM_OSM_ECC_ERR_SOURCE_E enumeration. */
+	uint64_t ecc_err_source               : 5;  /**< Source of ECC error; see OSM_ECC_ERR_SOURCE_E. */
 	uint64_t reserved_16_31               : 16;
 	uint64_t ecc_err_address              : 16; /**< RAM address of the ECC error. */
 #else
@@ -283,13 +277,10 @@ typedef union cvmx_osm_int_info_ecc cvmx_osm_int_info_ecc_t;
 /**
  * cvmx_osm_int_stat
  *
- * Interrupt Status.
- * DBEs are detected. SBE's are corrected. For debugging output for ECC DBE/SBE's, see
- * OSM_INT_INFO_ECC.
- * Address Errors happen when a requester attempts to access a bank that was not assigned to it.
- * For example, Bank 0 is assigned to DFA and HNA attempts to access it. For debugging output for
- * Address Errors, see OSM_INT_INFO_ADDR. For current bank assignment, see
- * OSM_BANK(0..63)_CTRL.
+ * For debugging output for ECC DBE/SBEs, see OSM_INT_INFO_ECC. Address errors happen when a
+ * requester attempts to access a bank that was not assigned to it. For example, Bank 0 is
+ * assigned to HFA, and HNA attempts to access it. For debugging output for address errors, see
+ * OSM_INT_INFO_ADDR. For current bank assignment, see OSM_BANK(0..63)_CTRL.
  */
 union cvmx_osm_int_stat {
 	uint64_t u64;
@@ -302,7 +293,7 @@ union cvmx_osm_int_stat {
 	uint64_t bwc_addr_err                 : 1;  /**< BWC port illegal bank address error. */
 	uint64_t twc_addr_err                 : 1;  /**< TWC port illegal bank address error. */
 	uint64_t hna_addr_err                 : 1;  /**< HNA port illegal bank address error. */
-	uint64_t dfa_addr_err                 : 1;  /**< DFA port illegal bank address error. */
+	uint64_t dfa_addr_err                 : 1;  /**< HFA port illegal bank address error. */
 	uint64_t host_sbe                     : 1;  /**< Host port single-bit error. */
 	uint64_t host_dbe                     : 1;  /**< Host port double-bit error. */
 	uint64_t rwc3_sbe                     : 1;  /**< ASE RWC3 port single-bit error. */
@@ -317,8 +308,8 @@ union cvmx_osm_int_stat {
 	uint64_t twc_dbe                      : 1;  /**< ASE TWC port double-bit error. */
 	uint64_t hna_sbe                      : 1;  /**< HNA port single-bit error. */
 	uint64_t hna_dbe                      : 1;  /**< HNA port double-bit error. */
-	uint64_t dfa_sbe                      : 1;  /**< DFA port single-bit error. */
-	uint64_t dfa_dbe                      : 1;  /**< DFA port double-bit error. */
+	uint64_t dfa_sbe                      : 1;  /**< HFA port single-bit error. */
+	uint64_t dfa_dbe                      : 1;  /**< HFA port double-bit error. */
 #else
 	uint64_t dfa_dbe                      : 1;
 	uint64_t dfa_sbe                      : 1;
@@ -353,20 +344,18 @@ typedef union cvmx_osm_int_stat cvmx_osm_int_stat_t;
 /**
  * cvmx_osm_mem#_bist_status
  *
- * Built In Self Test Status Register.
- * Results from BIST runs of OSM's memories.
- * OSM_MEM is instantiated 8 times, each instance of OSM_MEM has its own BIST_STATUS.
- * Each OSM_MEM contains 32 BIST memory instances, so there are 32 status bits
- * per register.
+ * Results from BIST runs of OSM's memories. OSM_MEM is instantiated 8 times, each instance of
+ * OSM_MEM has its own BIST_STATUS. Each OSM_MEM contains 32 BIST memory instances, so there are
+ * 32 status bits per register.
  */
 union cvmx_osm_memx_bist_status {
 	uint64_t u64;
 	struct cvmx_osm_memx_bist_status_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_32_63               : 32;
-	uint64_t bist_status                  : 32; /**< BIST Status of BIST Memory Instance 31..0 in bits 31..0 respectively.
+	uint64_t bist_status                  : 32; /**< BIST status of BIST memory instance 31..0 in bits 31..0 respectively.
                                                          INTERNAL: Each BIST Memory Instance (1 BIST engine + multiple physical memories) contains
-                                                         2 physical memories, srf1024x32m8 and srf1024x33m3. */
+                                                         2 physical memories. */
 #else
 	uint64_t bist_status                  : 32;
 	uint64_t reserved_32_63               : 32;
diff --git a/arch/mips/include/asm/octeon/cvmx-pemx-defs.h b/arch/mips/include/asm/octeon/cvmx-pemx-defs.h
index 9f5a96a..7e34306 100644
--- a/arch/mips/include/asm/octeon/cvmx-pemx-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-pemx-defs.h
@@ -179,19 +179,44 @@ static inline uint64_t CVMX_PEMX_BAR_CTL(unsigned long block_id)
 #if CVMX_ENABLE_CSR_ADDRESS_CHECKING
 static inline uint64_t CVMX_PEMX_BIST_STATUS(unsigned long block_id)
 {
-	if (!(
-	      (OCTEON_IS_MODEL(OCTEON_CN61XX) && ((block_id <= 1))) ||
-	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
-	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id <= 1))) ||
-	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((block_id <= 1))) ||
-	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((block_id <= 2))) ||
-	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 3))) ||
-	      (OCTEON_IS_MODEL(OCTEON_CNF71XX) && ((block_id <= 1)))))
-		cvmx_warn("CVMX_PEMX_BIST_STATUS(%lu) is invalid on this chip\n", block_id);
-	return CVMX_ADD_IO_SEG(0x00011800C0000018ull) + ((block_id) & 3) * 0x1000000ull;
+	switch(cvmx_get_octeon_family()) {
+		case OCTEON_CN66XX & OCTEON_FAMILY_MASK:
+		case OCTEON_CNF71XX & OCTEON_FAMILY_MASK:
+		case OCTEON_CN63XX & OCTEON_FAMILY_MASK:
+		case OCTEON_CN61XX & OCTEON_FAMILY_MASK:
+		case OCTEON_CN68XX & OCTEON_FAMILY_MASK:
+			if ((block_id <= 1))
+				return CVMX_ADD_IO_SEG(0x00011800C0000018ull) + ((block_id) & 1) * 0x1000000ull;
+			break;
+		case OCTEON_CN70XX & OCTEON_FAMILY_MASK:
+			if ((block_id <= 2))
+				return CVMX_ADD_IO_SEG(0x00011800C0000018ull) + ((block_id) & 3) * 0x1000000ull;
+			break;
+		case OCTEON_CN78XX & OCTEON_FAMILY_MASK:
+			if ((block_id <= 3))
+				return CVMX_ADD_IO_SEG(0x00011800C0000440ull) + ((block_id) & 3) * 0x1000000ull;
+			break;
+	}
+	cvmx_warn("CVMX_PEMX_BIST_STATUS (block_id = %lu) not supported on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800C0000018ull) + ((block_id) & 1) * 0x1000000ull;
 }
 #else
-#define CVMX_PEMX_BIST_STATUS(block_id) (CVMX_ADD_IO_SEG(0x00011800C0000018ull) + ((block_id) & 3) * 0x1000000ull)
+static inline uint64_t CVMX_PEMX_BIST_STATUS(unsigned long block_id)
+{
+	switch(cvmx_get_octeon_family()) {
+		case OCTEON_CN66XX & OCTEON_FAMILY_MASK:
+		case OCTEON_CNF71XX & OCTEON_FAMILY_MASK:
+		case OCTEON_CN63XX & OCTEON_FAMILY_MASK:
+		case OCTEON_CN61XX & OCTEON_FAMILY_MASK:
+		case OCTEON_CN68XX & OCTEON_FAMILY_MASK:
+			return CVMX_ADD_IO_SEG(0x00011800C0000018ull) + (block_id) * 0x1000000ull;
+		case OCTEON_CN70XX & OCTEON_FAMILY_MASK:
+			return CVMX_ADD_IO_SEG(0x00011800C0000018ull) + (block_id) * 0x1000000ull;
+		case OCTEON_CN78XX & OCTEON_FAMILY_MASK:
+			return CVMX_ADD_IO_SEG(0x00011800C0000440ull) + (block_id) * 0x1000000ull;
+	}
+	return CVMX_ADD_IO_SEG(0x00011800C0000018ull) + (block_id) * 0x1000000ull;
+}
 #endif
 #if CVMX_ENABLE_CSR_ADDRESS_CHECKING
 static inline uint64_t CVMX_PEMX_BIST_STATUS2(unsigned long block_id)
@@ -209,10 +234,6 @@ static inline uint64_t CVMX_PEMX_BIST_STATUS2(unsigned long block_id)
 			if ((block_id <= 2))
 				return CVMX_ADD_IO_SEG(0x00011800C0000440ull) + ((block_id) & 3) * 0x1000000ull;
 			break;
-		case OCTEON_CN78XX & OCTEON_FAMILY_MASK:
-			if ((block_id <= 3))
-				return CVMX_ADD_IO_SEG(0x00011800C0000440ull) + ((block_id) & 3) * 0x1000000ull;
-			break;
 	}
 	cvmx_warn("CVMX_PEMX_BIST_STATUS2 (block_id = %lu) not supported on this chip\n", block_id);
 	return CVMX_ADD_IO_SEG(0x00011800C0000420ull) + ((block_id) & 1) * 0x1000000ull;
@@ -229,8 +250,6 @@ static inline uint64_t CVMX_PEMX_BIST_STATUS2(unsigned long block_id)
 			return CVMX_ADD_IO_SEG(0x00011800C0000420ull) + (block_id) * 0x1000000ull;
 		case OCTEON_CN70XX & OCTEON_FAMILY_MASK:
 			return CVMX_ADD_IO_SEG(0x00011800C0000440ull) + (block_id) * 0x1000000ull;
-		case OCTEON_CN78XX & OCTEON_FAMILY_MASK:
-			return CVMX_ADD_IO_SEG(0x00011800C0000440ull) + (block_id) * 0x1000000ull;
 	}
 	return CVMX_ADD_IO_SEG(0x00011800C0000420ull) + (block_id) * 0x1000000ull;
 }
@@ -406,26 +425,58 @@ static inline uint64_t CVMX_PEMX_DIAG_STATUS(unsigned long block_id)
 #if CVMX_ENABLE_CSR_ADDRESS_CHECKING
 static inline uint64_t CVMX_PEMX_ECC_ENA(unsigned long block_id)
 {
-	if (!(
-	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((block_id <= 2))) ||
-	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 3)))))
-		cvmx_warn("CVMX_PEMX_ECC_ENA(%lu) is invalid on this chip\n", block_id);
-	return CVMX_ADD_IO_SEG(0x00011800C00000C0ull) + ((block_id) & 3) * 0x1000000ull;
+	switch(cvmx_get_octeon_family()) {
+		case OCTEON_CN70XX & OCTEON_FAMILY_MASK:
+			if ((block_id <= 2))
+				return CVMX_ADD_IO_SEG(0x00011800C00000C0ull) + ((block_id) & 3) * 0x1000000ull;
+			break;
+		case OCTEON_CN78XX & OCTEON_FAMILY_MASK:
+			if ((block_id <= 3))
+				return CVMX_ADD_IO_SEG(0x00011800C0000448ull) + ((block_id) & 3) * 0x1000000ull;
+			break;
+	}
+	cvmx_warn("CVMX_PEMX_ECC_ENA (block_id = %lu) not supported on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800C0000448ull) + ((block_id) & 3) * 0x1000000ull;
 }
 #else
-#define CVMX_PEMX_ECC_ENA(block_id) (CVMX_ADD_IO_SEG(0x00011800C00000C0ull) + ((block_id) & 3) * 0x1000000ull)
+static inline uint64_t CVMX_PEMX_ECC_ENA(unsigned long block_id)
+{
+	switch(cvmx_get_octeon_family()) {
+		case OCTEON_CN70XX & OCTEON_FAMILY_MASK:
+			return CVMX_ADD_IO_SEG(0x00011800C00000C0ull) + (block_id) * 0x1000000ull;
+		case OCTEON_CN78XX & OCTEON_FAMILY_MASK:
+			return CVMX_ADD_IO_SEG(0x00011800C0000448ull) + (block_id) * 0x1000000ull;
+	}
+	return CVMX_ADD_IO_SEG(0x00011800C0000448ull) + (block_id) * 0x1000000ull;
+}
 #endif
 #if CVMX_ENABLE_CSR_ADDRESS_CHECKING
 static inline uint64_t CVMX_PEMX_ECC_SYND_CTRL(unsigned long block_id)
 {
-	if (!(
-	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((block_id <= 2))) ||
-	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 3)))))
-		cvmx_warn("CVMX_PEMX_ECC_SYND_CTRL(%lu) is invalid on this chip\n", block_id);
-	return CVMX_ADD_IO_SEG(0x00011800C00000C8ull) + ((block_id) & 3) * 0x1000000ull;
+	switch(cvmx_get_octeon_family()) {
+		case OCTEON_CN70XX & OCTEON_FAMILY_MASK:
+			if ((block_id <= 2))
+				return CVMX_ADD_IO_SEG(0x00011800C00000C8ull) + ((block_id) & 3) * 0x1000000ull;
+			break;
+		case OCTEON_CN78XX & OCTEON_FAMILY_MASK:
+			if ((block_id <= 3))
+				return CVMX_ADD_IO_SEG(0x00011800C0000450ull) + ((block_id) & 3) * 0x1000000ull;
+			break;
+	}
+	cvmx_warn("CVMX_PEMX_ECC_SYND_CTRL (block_id = %lu) not supported on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800C0000450ull) + ((block_id) & 3) * 0x1000000ull;
 }
 #else
-#define CVMX_PEMX_ECC_SYND_CTRL(block_id) (CVMX_ADD_IO_SEG(0x00011800C00000C8ull) + ((block_id) & 3) * 0x1000000ull)
+static inline uint64_t CVMX_PEMX_ECC_SYND_CTRL(unsigned long block_id)
+{
+	switch(cvmx_get_octeon_family()) {
+		case OCTEON_CN70XX & OCTEON_FAMILY_MASK:
+			return CVMX_ADD_IO_SEG(0x00011800C00000C8ull) + (block_id) * 0x1000000ull;
+		case OCTEON_CN78XX & OCTEON_FAMILY_MASK:
+			return CVMX_ADD_IO_SEG(0x00011800C0000450ull) + (block_id) * 0x1000000ull;
+	}
+	return CVMX_ADD_IO_SEG(0x00011800C0000450ull) + (block_id) * 0x1000000ull;
+}
 #endif
 #if CVMX_ENABLE_CSR_ADDRESS_CHECKING
 static inline uint64_t CVMX_PEMX_INB_READ_CREDITS(unsigned long block_id)
@@ -883,52 +934,54 @@ typedef union cvmx_pemx_bar_ctl cvmx_pemx_bar_ctl_t;
 /**
  * cvmx_pem#_bist_status
  *
- * Contains the diffrent interrupt summary bits of the PEM.
- *
+ * "PEM#_BIST_STATUS2 = PEM BIST Status Register
+ * Results from BIST runs of PEM's memories."
  */
 union cvmx_pemx_bist_status {
 	uint64_t u64;
 	struct cvmx_pemx_bist_status_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_25_63               : 39;
-	uint64_t retrya                       : 1;  /**< Retry Buffer Memory A */
-	uint64_t retryb                       : 1;  /**< Retry Buffer Memory B */
+	uint64_t reserved_26_63               : 38;
 	uint64_t retryc                       : 1;  /**< Retry Buffer Memory C */
-	uint64_t reserved_21_21               : 1;
-	uint64_t rqhdra0                      : 1;  /**< Rx Queue Header Memory A0 */
-	uint64_t rqhdra1                      : 1;  /**< Rx Queue Header Memory A1 */
-	uint64_t rqhdra2                      : 1;  /**< Rx Queue Header Memory A2 */
-	uint64_t rqhdrb0                      : 1;  /**< Rx Queue Header Memory B0 */
-	uint64_t rqhdrb1                      : 1;  /**< Rx Queue Header Memory B1 */
-	uint64_t rqhdrb2                      : 1;  /**< Rx Queue Header Memory B2 */
-	uint64_t rqdataa0                     : 1;  /**< Rx Queue Data Memory A0 */
-	uint64_t rqdataa1                     : 1;  /**< Rx Queue Data Memory A1 */
-	uint64_t rqdataa2                     : 1;  /**< Rx Queue Data Memory A2 */
-	uint64_t rqdataa3                     : 1;  /**< Rx Queue Data Memory A3 */
-	uint64_t rqdataa4                     : 1;  /**< Rx Queue Data Memory A4 */
-	uint64_t rqdatab0                     : 1;  /**< Rx Queue Data Memory B0 */
-	uint64_t rqdatab1                     : 1;  /**< Rx Queue Data Memory B1 */
+	uint64_t reserved_24_24               : 1;
+	uint64_t rqhdrb0                      : 1;  /**< Rx Queue Header Memory Buffer 0 */
+	uint64_t rqhdrb1                      : 1;  /**< Rx Queue Header Memory Buffer 1 */
+	uint64_t rqdatab0                     : 1;  /**< Rx Queue Data Buffer 0 */
+	uint64_t rqdatab1                     : 1;  /**< Rx Queue Data Buffer 1 */
+	uint64_t tlpan_d0                     : 1;  /**< BIST Status for the tlp_n_afifo_data0 */
+	uint64_t tlpan_d1                     : 1;  /**< BIST Status for the tlp_n_afifo_data1 */
+	uint64_t tlpan_ctl                    : 1;  /**< BIST Status for the tlp_n_afifo_ctl */
+	uint64_t tlpap_d0                     : 1;  /**< BIST Status for the tlp_p_afifo_data0 */
+	uint64_t tlpap_d1                     : 1;  /**< BIST Status for the tlp_p_afifo_data1 */
+	uint64_t tlpap_ctl                    : 1;  /**< BIST Status for the tlp_p_afifo_ctl */
+	uint64_t tlpac_d0                     : 1;  /**< BIST Status for the tlp_c_afifo_data0 */
+	uint64_t tlpac_d1                     : 1;  /**< BIST Status for the tlp_c_afifo_data1 */
+	uint64_t tlpac_ctl                    : 1;  /**< BIST Status for the tlp_c_afifo_ctl */
+	uint64_t peai_p2e                     : 1;  /**< BIST Status for the peai__pesc_fifo */
+	uint64_t tlpn_d0                      : 1;  /**< BIST Status for the tlp_n_fifo_data0 */
+	uint64_t tlpn_d1                      : 1;  /**< BIST Status for the tlp_n_fifo_data1 */
 	uint64_t reserved_0_7                 : 8;
 #else
 	uint64_t reserved_0_7                 : 8;
+	uint64_t tlpn_d1                      : 1;
+	uint64_t tlpn_d0                      : 1;
+	uint64_t peai_p2e                     : 1;
+	uint64_t tlpac_ctl                    : 1;
+	uint64_t tlpac_d1                     : 1;
+	uint64_t tlpac_d0                     : 1;
+	uint64_t tlpap_ctl                    : 1;
+	uint64_t tlpap_d1                     : 1;
+	uint64_t tlpap_d0                     : 1;
+	uint64_t tlpan_ctl                    : 1;
+	uint64_t tlpan_d1                     : 1;
+	uint64_t tlpan_d0                     : 1;
 	uint64_t rqdatab1                     : 1;
 	uint64_t rqdatab0                     : 1;
-	uint64_t rqdataa4                     : 1;
-	uint64_t rqdataa3                     : 1;
-	uint64_t rqdataa2                     : 1;
-	uint64_t rqdataa1                     : 1;
-	uint64_t rqdataa0                     : 1;
-	uint64_t rqhdrb2                      : 1;
 	uint64_t rqhdrb1                      : 1;
 	uint64_t rqhdrb0                      : 1;
-	uint64_t rqhdra2                      : 1;
-	uint64_t rqhdra1                      : 1;
-	uint64_t rqhdra0                      : 1;
-	uint64_t reserved_21_21               : 1;
+	uint64_t reserved_24_24               : 1;
 	uint64_t retryc                       : 1;
-	uint64_t retryb                       : 1;
-	uint64_t retrya                       : 1;
-	uint64_t reserved_25_63               : 39;
+	uint64_t reserved_26_63               : 38;
 #endif
 	} s;
 	struct cvmx_pemx_bist_status_cn61xx {
@@ -980,59 +1033,61 @@ union cvmx_pemx_bist_status {
 	} cn70xx;
 	struct cvmx_pemx_bist_status_cn78xx {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_25_63               : 39;
-	uint64_t retrya                       : 1;  /**< Retry Buffer Memory A */
-	uint64_t retryb                       : 1;  /**< Retry Buffer Memory B */
+	uint64_t reserved_26_63               : 38;
 	uint64_t retryc                       : 1;  /**< Retry Buffer Memory C */
 	uint64_t sot                          : 1;  /**< Start of Transfer Memory */
-	uint64_t rqhdra0                      : 1;  /**< Rx Queue Header Memory A0 */
-	uint64_t rqhdra1                      : 1;  /**< Rx Queue Header Memory A1 */
-	uint64_t rqhdra2                      : 1;  /**< Rx Queue Header Memory A2 */
-	uint64_t rqhdrb0                      : 1;  /**< Rx Queue Header Memory B0 */
-	uint64_t rqhdrb1                      : 1;  /**< Rx Queue Header Memory B1 */
-	uint64_t rqhdrb2                      : 1;  /**< Rx Queue Header Memory B2 */
-	uint64_t rqdataa0                     : 1;  /**< Rx Queue Data Memory A0 */
-	uint64_t rqdataa1                     : 1;  /**< Rx Queue Data Memory A1 */
-	uint64_t rqdataa2                     : 1;  /**< Rx Queue Data Memory A2 */
-	uint64_t rqdataa3                     : 1;  /**< Rx Queue Data Memory A3 */
-	uint64_t rqdataa4                     : 1;  /**< Rx Queue Data Memory A4 */
-	uint64_t rqdatab0                     : 1;  /**< Rx Queue Data Memory B0 */
-	uint64_t rqdatab1                     : 1;  /**< Rx Queue Data Memory B1 */
-	uint64_t rqdatab2                     : 1;  /**< Rx Queue Data Memory B2 */
-	uint64_t rqdatab3                     : 1;  /**< Rx Queue Data Memory B3 */
-	uint64_t rqdatab4                     : 1;  /**< Rx Queue Data Memory B4 */
-	uint64_t rqdatac0                     : 1;  /**< Rx Queue Data Memory C0 */
-	uint64_t rqdatac1                     : 1;  /**< Rx Queue Data Memory C1 */
-	uint64_t rqdatac2                     : 1;  /**< Rx Queue Data Memory C2 */
-	uint64_t rqdatac3                     : 1;  /**< Rx Queue Data Memory C3 */
-	uint64_t rqdatac4                     : 1;  /**< Rx Queue Data Memory C4 */
-#else
-	uint64_t rqdatac4                     : 1;
-	uint64_t rqdatac3                     : 1;
-	uint64_t rqdatac2                     : 1;
-	uint64_t rqdatac1                     : 1;
-	uint64_t rqdatac0                     : 1;
-	uint64_t rqdatab4                     : 1;
-	uint64_t rqdatab3                     : 1;
-	uint64_t rqdatab2                     : 1;
+	uint64_t rqhdrb0                      : 1;  /**< Rx Queue Header Memory Buffer 0 */
+	uint64_t rqhdrb1                      : 1;  /**< Rx Queue Header Memory Buffer 1 */
+	uint64_t rqdatab0                     : 1;  /**< Rx Queue Data Buffer 0 */
+	uint64_t rqdatab1                     : 1;  /**< Rx Queue Data Buffer 1 */
+	uint64_t tlpan_d0                     : 1;  /**< BIST Status for the tlp_n_afifo_data0 */
+	uint64_t tlpan_d1                     : 1;  /**< BIST Status for the tlp_n_afifo_data1 */
+	uint64_t tlpan_ctl                    : 1;  /**< BIST Status for the tlp_n_afifo_ctl */
+	uint64_t tlpap_d0                     : 1;  /**< BIST Status for the tlp_p_afifo_data0 */
+	uint64_t tlpap_d1                     : 1;  /**< BIST Status for the tlp_p_afifo_data1 */
+	uint64_t tlpap_ctl                    : 1;  /**< BIST Status for the tlp_p_afifo_ctl */
+	uint64_t tlpac_d0                     : 1;  /**< BIST Status for the tlp_c_afifo_data0 */
+	uint64_t tlpac_d1                     : 1;  /**< BIST Status for the tlp_c_afifo_data1 */
+	uint64_t tlpac_ctl                    : 1;  /**< BIST Status for the tlp_c_afifo_ctl */
+	uint64_t peai_p2e                     : 1;  /**< BIST Status for the peai__pesc_fifo */
+	uint64_t tlpn_d0                      : 1;  /**< BIST Status for the tlp_n_fifo_data0 */
+	uint64_t tlpn_d1                      : 1;  /**< BIST Status for the tlp_n_fifo_data1 */
+	uint64_t tlpn_ctl                     : 1;  /**< BIST Status for the tlp_n_fifo_ctl */
+	uint64_t tlpp_d0                      : 1;  /**< BIST Status for the tlp_p_fifo_data0 */
+	uint64_t tlpp_d1                      : 1;  /**< BIST Status for the tlp_p_fifo_data1 */
+	uint64_t tlpp_ctl                     : 1;  /**< BIST Status for the tlp_p_fifo_ctl */
+	uint64_t tlpc_d0                      : 1;  /**< BIST Status for the tlp_c_fifo_data0 */
+	uint64_t tlpc_d1                      : 1;  /**< BIST Status for the tlp_c_fifo_data1 */
+	uint64_t tlpc_ctl                     : 1;  /**< BIST Status for the tlp_c_fifo_ctl */
+	uint64_t m2s                          : 1;  /**< BIST Status for the m2s_fifo */
+#else
+	uint64_t m2s                          : 1;
+	uint64_t tlpc_ctl                     : 1;
+	uint64_t tlpc_d1                      : 1;
+	uint64_t tlpc_d0                      : 1;
+	uint64_t tlpp_ctl                     : 1;
+	uint64_t tlpp_d1                      : 1;
+	uint64_t tlpp_d0                      : 1;
+	uint64_t tlpn_ctl                     : 1;
+	uint64_t tlpn_d1                      : 1;
+	uint64_t tlpn_d0                      : 1;
+	uint64_t peai_p2e                     : 1;
+	uint64_t tlpac_ctl                    : 1;
+	uint64_t tlpac_d1                     : 1;
+	uint64_t tlpac_d0                     : 1;
+	uint64_t tlpap_ctl                    : 1;
+	uint64_t tlpap_d1                     : 1;
+	uint64_t tlpap_d0                     : 1;
+	uint64_t tlpan_ctl                    : 1;
+	uint64_t tlpan_d1                     : 1;
+	uint64_t tlpan_d0                     : 1;
 	uint64_t rqdatab1                     : 1;
 	uint64_t rqdatab0                     : 1;
-	uint64_t rqdataa4                     : 1;
-	uint64_t rqdataa3                     : 1;
-	uint64_t rqdataa2                     : 1;
-	uint64_t rqdataa1                     : 1;
-	uint64_t rqdataa0                     : 1;
-	uint64_t rqhdrb2                      : 1;
 	uint64_t rqhdrb1                      : 1;
 	uint64_t rqhdrb0                      : 1;
-	uint64_t rqhdra2                      : 1;
-	uint64_t rqhdra1                      : 1;
-	uint64_t rqhdra0                      : 1;
 	uint64_t sot                          : 1;
 	uint64_t retryc                       : 1;
-	uint64_t retryb                       : 1;
-	uint64_t retrya                       : 1;
-	uint64_t reserved_25_63               : 39;
+	uint64_t reserved_26_63               : 38;
 #endif
 	} cn78xx;
 	struct cvmx_pemx_bist_status_cn61xx   cnf71xx;
@@ -1049,21 +1104,17 @@ union cvmx_pemx_bist_status2 {
 	uint64_t u64;
 	struct cvmx_pemx_bist_status2_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_19_63               : 45;
-	uint64_t tlpn_d0                      : 1;  /**< BIST Status for the tlp_n_fifo_data0 */
-	uint64_t tlpn_d1                      : 1;  /**< BIST Status for the tlp_n_fifo_data1 */
-	uint64_t reserved_16_16               : 1;
-	uint64_t tlpp_d0                      : 1;  /**< BIST Status for the tlp_p_fifo_data0 */
-	uint64_t tlpp_d1                      : 1;  /**< BIST Status for the tlp_p_fifo_data1 */
-	uint64_t reserved_0_13                : 14;
+	uint64_t reserved_13_63               : 51;
+	uint64_t tlpn_d                       : 1;  /**< BIST Status for the tlp_n_fifo_data */
+	uint64_t tlpn_ctl                     : 1;  /**< BIST Status for the tlp_n_fifo_ctl */
+	uint64_t tlpp_d                       : 1;  /**< BIST Status for the tlp_p_fifo_data */
+	uint64_t reserved_0_9                 : 10;
 #else
-	uint64_t reserved_0_13                : 14;
-	uint64_t tlpp_d1                      : 1;
-	uint64_t tlpp_d0                      : 1;
-	uint64_t reserved_16_16               : 1;
-	uint64_t tlpn_d1                      : 1;
-	uint64_t tlpn_d0                      : 1;
-	uint64_t reserved_19_63               : 45;
+	uint64_t reserved_0_9                 : 10;
+	uint64_t tlpp_d                       : 1;
+	uint64_t tlpn_ctl                     : 1;
+	uint64_t tlpn_d                       : 1;
+	uint64_t reserved_13_63               : 51;
 #endif
 	} s;
 	struct cvmx_pemx_bist_status2_cn61xx {
@@ -1133,53 +1184,6 @@ union cvmx_pemx_bist_status2 {
 	uint64_t reserved_14_63               : 50;
 #endif
 	} cn70xx;
-	struct cvmx_pemx_bist_status2_cn78xx {
-#ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_20_63               : 44;
-	uint64_t peai_p2e                     : 1;  /**< BIST Status for the peai__pesc_fifo */
-	uint64_t tlpn_d0                      : 1;  /**< BIST Status for the tlp_n_fifo_data0 */
-	uint64_t tlpn_d1                      : 1;  /**< BIST Status for the tlp_n_fifo_data1 */
-	uint64_t tlpn_ctl                     : 1;  /**< BIST Status for the tlp_n_fifo_ctl */
-	uint64_t tlpp_d0                      : 1;  /**< BIST Status for the tlp_p_fifo_data0 */
-	uint64_t tlpp_d1                      : 1;  /**< BIST Status for the tlp_p_fifo_data1 */
-	uint64_t tlpp_ctl                     : 1;  /**< BIST Status for the tlp_p_fifo_ctl */
-	uint64_t tlpc_d0                      : 1;  /**< BIST Status for the tlp_c_fifo_data0 */
-	uint64_t tlpc_d1                      : 1;  /**< BIST Status for the tlp_c_fifo_data1 */
-	uint64_t tlpc_ctl                     : 1;  /**< BIST Status for the tlp_c_fifo_ctl */
-	uint64_t tlpan_d0                     : 1;  /**< BIST Status for the tlp_n_afifo_data0 */
-	uint64_t tlpan_d1                     : 1;  /**< BIST Status for the tlp_n_afifo_data1 */
-	uint64_t tlpan_ctl                    : 1;  /**< BIST Status for the tlp_n_afifo_ctl */
-	uint64_t tlpap_d0                     : 1;  /**< BIST Status for the tlp_p_afifo_data0 */
-	uint64_t tlpap_d1                     : 1;  /**< BIST Status for the tlp_p_afifo_data1 */
-	uint64_t tlpap_ctl                    : 1;  /**< BIST Status for the tlp_p_afifo_ctl */
-	uint64_t tlpac_d0                     : 1;  /**< BIST Status for the tlp_c_afifo_data0 */
-	uint64_t tlpac_d1                     : 1;  /**< BIST Status for the tlp_c_afifo_data1 */
-	uint64_t tlpac_ctl                    : 1;  /**< BIST Status for the tlp_c_afifo_ctl */
-	uint64_t m2s                          : 1;  /**< BIST Status for the m2s_fifo */
-#else
-	uint64_t m2s                          : 1;
-	uint64_t tlpac_ctl                    : 1;
-	uint64_t tlpac_d1                     : 1;
-	uint64_t tlpac_d0                     : 1;
-	uint64_t tlpap_ctl                    : 1;
-	uint64_t tlpap_d1                     : 1;
-	uint64_t tlpap_d0                     : 1;
-	uint64_t tlpan_ctl                    : 1;
-	uint64_t tlpan_d1                     : 1;
-	uint64_t tlpan_d0                     : 1;
-	uint64_t tlpc_ctl                     : 1;
-	uint64_t tlpc_d1                      : 1;
-	uint64_t tlpc_d0                      : 1;
-	uint64_t tlpp_ctl                     : 1;
-	uint64_t tlpp_d1                      : 1;
-	uint64_t tlpp_d0                      : 1;
-	uint64_t tlpn_ctl                     : 1;
-	uint64_t tlpn_d1                      : 1;
-	uint64_t tlpn_d0                      : 1;
-	uint64_t peai_p2e                     : 1;
-	uint64_t reserved_20_63               : 44;
-#endif
-	} cn78xx;
 	struct cvmx_pemx_bist_status2_cn61xx  cnf71xx;
 };
 typedef union cvmx_pemx_bist_status2 cvmx_pemx_bist_status2_t;
@@ -1194,19 +1198,39 @@ union cvmx_pemx_cfg {
 	uint64_t u64;
 	struct cvmx_pemx_cfg_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_0_63                : 64;
+	uint64_t reserved_5_63                : 59;
+	uint64_t laneswap                     : 1;  /**< This field will overwrite the pin setting for lane swapping.
+                                                         When set, lane swapping is performed to/from the SerDes.
+                                                         When clear, no lane swapping is performed. */
+	uint64_t reserved_2_3                 : 2;
+	uint64_t md                           : 2;  /**< This field will overwrite the pin settings for speed.
+                                                         00 - EP Mode, Gen1 Speed
+                                                         01 - EP Mode, Gen2 Speed
+                                                         10 - EP Mode, Gen3 Speed
+                                                         11 - Rsvd */
 #else
-	uint64_t reserved_0_63                : 64;
+	uint64_t md                           : 2;
+	uint64_t reserved_2_3                 : 2;
+	uint64_t laneswap                     : 1;
+	uint64_t reserved_5_63                : 59;
 #endif
 	} s;
 	struct cvmx_pemx_cfg_cn70xx {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_5_63                : 59;
-	uint64_t laneswap                     : 1;  /**< When set, lane swapping is performed to/from the SerDes.
+	uint64_t laneswap                     : 1;  /**< This field will overwrite the pin setting for lane swapping.
+                                                         When set, lane swapping is performed to/from the SerDes.
                                                          When clear, no lane swapping is performed. */
-	uint64_t hostmd                       : 1;  /**< When set, the PEM is configured to be a Root Complex.
+	uint64_t hostmd                       : 1;  /**< This field will overwrite the pin settings for host mode.
+                                                         When set, the PEM is configured to be a Root Complex.
                                                          When clear, the PEM is configured to be an End Point. */
-	uint64_t md                           : 3;  /**< This field will overwrite the pin settings for speed and lane.
+	uint64_t md                           : 3;  /**< This field will overwrite the pin settings for speed and lane
+                                                         configuration. This value is used to set the Maximum Link Width
+                                                         field in the core's Link Capabilities Register (CFG031) to
+                                                         indicate the maximum number of lanes supported. Note that less
+                                                         lanes than the specified maximum can be configured for use via
+                                                         the core's Link Control Register (CFG032) Negotiated Link Width
+                                                         field.
                                                          NOTE - The lower two bits of the MD field must
                                                          be the same across all configured PEMs!
                                                            000 - Gen2 Speed, 2-lanes
@@ -1226,22 +1250,32 @@ union cvmx_pemx_cfg {
 	} cn70xx;
 	struct cvmx_pemx_cfg_cn78xx {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_4_63                : 60;
-	uint64_t laneswap                     : 1;  /**< When set, lane swapping is performed to/from the SerDes.
+	uint64_t reserved_5_63                : 59;
+	uint64_t laneswap                     : 1;  /**< This field will overwrite the pin setting for lane swapping.
+                                                         When set, lane swapping is performed to/from the SerDes.
                                                          When clear, no lane swapping is performed. */
-	uint64_t lanes8                       : 1;  /**< When set, the PEM is configured for 8-lanes,
-                                                         When clear, the PEM is configured for 4-lanes */
-	uint64_t md                           : 2;  /**< When both bits are set, the PEM is configured to be a Root Complex.
-                                                         When both bits are not set, the PEM is configured to be a End Point:
-                                                           00 - EP Mode, Gen1 Speed
-                                                           01 - EP Mode, Gen2 Speed
-                                                           10 - EP Mode, Gen3 Speed
-                                                           11 - RC Mode */
+	uint64_t lanes8                       : 1;  /**< This field will overwrite the pin setting for number of lanes.
+                                                         When set, the PEM is configured for a maximum of 8-lanes,
+                                                         When clear, the PEM is configured for a maximum of 4-lanes.
+                                                         This value is used to set the Maximum Link Width field in the
+                                                         core's Link Capabilities Register (CFG031) to indicate the
+                                                         maximum number of lanes supported. Note that less lanes than
+                                                         the specified maximum can be configured for use via the core's
+                                                         Link Control Register (CFG032) Negotiated Link Width field. */
+	uint64_t hostmd                       : 1;  /**< This field will overwrite the pin settings for host mode.
+                                                         When set, the PEM is configured to be a Root Complex.
+                                                         When clear, the PEM is configured to be an End Point. */
+	uint64_t md                           : 2;  /**< This field will overwrite the pin settings for speed.
+                                                         00 - EP Mode, Gen1 Speed
+                                                         01 - EP Mode, Gen2 Speed
+                                                         10 - EP Mode, Gen3 Speed
+                                                         11 - Rsvd */
 #else
 	uint64_t md                           : 2;
+	uint64_t hostmd                       : 1;
 	uint64_t lanes8                       : 1;
 	uint64_t laneswap                     : 1;
-	uint64_t reserved_4_63                : 60;
+	uint64_t reserved_5_63                : 59;
 #endif
 	} cn78xx;
 };
@@ -1381,6 +1415,93 @@ union cvmx_pemx_ctl_status {
 	uint64_t u64;
 	struct cvmx_pemx_ctl_status_s {
 #ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_7_63                : 57;
+	uint64_t nf_ecrc                      : 1;  /**< Do not forward peer-to-peer ECRC TLPs. */
+	uint64_t dly_one                      : 1;  /**< When set the output client state machines will
+                                                         wait one cycle before starting a new TLP out. */
+	uint64_t lnk_enb                      : 1;  /**< When set '1' the link is enabled when '0' the
+                                                         link is disabled. This bit only is active when in
+                                                         RC mode. */
+	uint64_t ro_ctlp                      : 1;  /**< When set '1' C-TLPs that have the RO bit set will
+                                                         not wait for P-TLPs that normaly would be sent
+                                                         first. */
+	uint64_t fast_lm                      : 1;  /**< When '1' forces fast link mode. */
+	uint64_t inv_ecrc                     : 1;  /**< When '1' causes the LSB of the ECRC to be inverted. */
+	uint64_t inv_lcrc                     : 1;  /**< When '1' causes the LSB of the LCRC to be inverted. */
+#else
+	uint64_t inv_lcrc                     : 1;
+	uint64_t inv_ecrc                     : 1;
+	uint64_t fast_lm                      : 1;
+	uint64_t ro_ctlp                      : 1;
+	uint64_t lnk_enb                      : 1;
+	uint64_t dly_one                      : 1;
+	uint64_t nf_ecrc                      : 1;
+	uint64_t reserved_7_63                : 57;
+#endif
+	} s;
+	struct cvmx_pemx_ctl_status_cn61xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t auto_sd                      : 1;  /**< Link Hardware Autonomous Speed Disable. */
+	uint64_t dnum                         : 5;  /**< Primary bus device number. */
+	uint64_t pbus                         : 8;  /**< Primary bus number. */
+	uint64_t reserved_32_33               : 2;
+	uint64_t cfg_rtry                     : 16; /**< The time x 0x10000 in core clocks to wait for a
+                                                         CPL to a CFG RD that does not carry a Retry Status.
+                                                         Until such time that the timeout occurs and Retry
+                                                         Status is received for a CFG RD, the Read CFG Read
+                                                         will be resent. A value of 0 disables retries and
+                                                         treats a CPL Retry as a CPL UR.
+                                                         When enabled only one CFG RD may be issued until
+                                                         either successful completion or CPL UR. */
+	uint64_t reserved_12_15               : 4;
+	uint64_t pm_xtoff                     : 1;  /**< When WRITTEN with a '1' a single cycle pulse is
+                                                         to the PCIe core pm_xmt_turnoff port. RC mode. */
+	uint64_t pm_xpme                      : 1;  /**< When WRITTEN with a '1' a single cycle pulse is
+                                                         to the PCIe core pm_xmt_pme port. EP mode. */
+	uint64_t ob_p_cmd                     : 1;  /**< When WRITTEN with a '1' a single cycle pulse is
+                                                         to the PCIe core outband_pwrup_cmd port. EP mode. */
+	uint64_t reserved_7_8                 : 2;
+	uint64_t nf_ecrc                      : 1;  /**< Do not forward peer-to-peer ECRC TLPs. */
+	uint64_t dly_one                      : 1;  /**< When set the output client state machines will
+                                                         wait one cycle before starting a new TLP out. */
+	uint64_t lnk_enb                      : 1;  /**< When set '1' the link is enabled when '0' the
+                                                         link is disabled. This bit only is active when in
+                                                         RC mode. */
+	uint64_t ro_ctlp                      : 1;  /**< When set '1' C-TLPs that have the RO bit set will
+                                                         not wait for P-TLPs that normaly would be sent
+                                                         first. */
+	uint64_t fast_lm                      : 1;  /**< When '1' forces fast link mode. */
+	uint64_t inv_ecrc                     : 1;  /**< When '1' causes the LSB of the ECRC to be inverted. */
+	uint64_t inv_lcrc                     : 1;  /**< When '1' causes the LSB of the LCRC to be inverted. */
+#else
+	uint64_t inv_lcrc                     : 1;
+	uint64_t inv_ecrc                     : 1;
+	uint64_t fast_lm                      : 1;
+	uint64_t ro_ctlp                      : 1;
+	uint64_t lnk_enb                      : 1;
+	uint64_t dly_one                      : 1;
+	uint64_t nf_ecrc                      : 1;
+	uint64_t reserved_7_8                 : 2;
+	uint64_t ob_p_cmd                     : 1;
+	uint64_t pm_xpme                      : 1;
+	uint64_t pm_xtoff                     : 1;
+	uint64_t reserved_12_15               : 4;
+	uint64_t cfg_rtry                     : 16;
+	uint64_t reserved_32_33               : 2;
+	uint64_t pbus                         : 8;
+	uint64_t dnum                         : 5;
+	uint64_t auto_sd                      : 1;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} cn61xx;
+	struct cvmx_pemx_ctl_status_cn61xx    cn63xx;
+	struct cvmx_pemx_ctl_status_cn61xx    cn63xxp1;
+	struct cvmx_pemx_ctl_status_cn61xx    cn66xx;
+	struct cvmx_pemx_ctl_status_cn61xx    cn68xx;
+	struct cvmx_pemx_ctl_status_cn61xx    cn68xxp1;
+	struct cvmx_pemx_ctl_status_cn70xx {
+#ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_51_63               : 13;
 	uint64_t inv_dpar                     : 1;  /**< Invert the generated parity to be written into the
                                                          the most significant Data Queue Buffer ram block
@@ -1446,14 +1567,29 @@ union cvmx_pemx_ctl_status {
 	uint64_t inv_dpar                     : 1;
 	uint64_t reserved_51_63               : 13;
 #endif
-	} s;
-	struct cvmx_pemx_ctl_status_cn61xx {
+	} cn70xx;
+	struct cvmx_pemx_ctl_status_cn78xx {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_48_63               : 16;
+	uint64_t inv_dpar                     : 1;  /**< Invert the generated parity to be written into the
+                                                         the most significant Data Queue Buffer ram block
+                                                         to force a parity error when it is later read. */
+	uint64_t inv_hpar                     : 1;  /**< Invert the generated parity to be written into the
+                                                         most significant Header Queue Buffer ram block
+                                                         to force a parity error when it is later read. */
+	uint64_t inv_rpar                     : 1;  /**< Invert the generated parity to be written into the
+                                                         most significant Retry Buffer ram block to force
+                                                         a parity error when it is later read. */
 	uint64_t auto_sd                      : 1;  /**< Link Hardware Autonomous Speed Disable. */
 	uint64_t dnum                         : 5;  /**< Primary bus device number. */
 	uint64_t pbus                         : 8;  /**< Primary bus number. */
-	uint64_t reserved_32_33               : 2;
+	uint64_t no_fwd_prg                   : 16; /**< The time x 0x10000 in core clocks to wait for the
+                                                         TLP FIFOs to be able to unload an entry. If there is
+                                                         no forward progress, such that the timeout occurs,
+                                                         credits will be returned to the SLI and an interrupt
+                                                         (if enabled) will be asserted. Any more TLPs received
+                                                         will be dropped on the floor and the credits
+                                                         associated with those TLPs will be returned, as well.
+                                                         This state will hold until a mac reset is received. */
 	uint64_t cfg_rtry                     : 16; /**< The time x 0x10000 in core clocks to wait for a
                                                          CPL to a CFG RD that does not carry a Retry Status.
                                                          Until such time that the timeout occurs and Retry
@@ -1462,14 +1598,14 @@ union cvmx_pemx_ctl_status {
                                                          treats a CPL Retry as a CPL UR.
                                                          When enabled only one CFG RD may be issued until
                                                          either successful completion or CPL UR. */
-	uint64_t reserved_12_15               : 4;
+	uint64_t reserved_11_14               : 4;
 	uint64_t pm_xtoff                     : 1;  /**< When WRITTEN with a '1' a single cycle pulse is
                                                          to the PCIe core pm_xmt_turnoff port. RC mode. */
 	uint64_t pm_xpme                      : 1;  /**< When WRITTEN with a '1' a single cycle pulse is
                                                          to the PCIe core pm_xmt_pme port. EP mode. */
 	uint64_t ob_p_cmd                     : 1;  /**< When WRITTEN with a '1' a single cycle pulse is
                                                          to the PCIe core outband_pwrup_cmd port. EP mode. */
-	uint64_t reserved_7_8                 : 2;
+	uint64_t reserved_7_7                 : 1;
 	uint64_t nf_ecrc                      : 1;  /**< Do not forward peer-to-peer ECRC TLPs. */
 	uint64_t dly_one                      : 1;  /**< When set the output client state machines will
                                                          wait one cycle before starting a new TLP out. */
@@ -1490,26 +1626,21 @@ union cvmx_pemx_ctl_status {
 	uint64_t lnk_enb                      : 1;
 	uint64_t dly_one                      : 1;
 	uint64_t nf_ecrc                      : 1;
-	uint64_t reserved_7_8                 : 2;
+	uint64_t reserved_7_7                 : 1;
 	uint64_t ob_p_cmd                     : 1;
 	uint64_t pm_xpme                      : 1;
 	uint64_t pm_xtoff                     : 1;
-	uint64_t reserved_12_15               : 4;
+	uint64_t reserved_11_14               : 4;
 	uint64_t cfg_rtry                     : 16;
-	uint64_t reserved_32_33               : 2;
+	uint64_t no_fwd_prg                   : 16;
 	uint64_t pbus                         : 8;
 	uint64_t dnum                         : 5;
 	uint64_t auto_sd                      : 1;
-	uint64_t reserved_48_63               : 16;
+	uint64_t inv_rpar                     : 1;
+	uint64_t inv_hpar                     : 1;
+	uint64_t inv_dpar                     : 1;
 #endif
-	} cn61xx;
-	struct cvmx_pemx_ctl_status_cn61xx    cn63xx;
-	struct cvmx_pemx_ctl_status_cn61xx    cn63xxp1;
-	struct cvmx_pemx_ctl_status_cn61xx    cn66xx;
-	struct cvmx_pemx_ctl_status_cn61xx    cn68xx;
-	struct cvmx_pemx_ctl_status_cn61xx    cn68xxp1;
-	struct cvmx_pemx_ctl_status_s         cn70xx;
-	struct cvmx_pemx_ctl_status_s         cn78xx;
+	} cn78xx;
 	struct cvmx_pemx_ctl_status_cn61xx    cnf71xx;
 };
 typedef union cvmx_pemx_ctl_status cvmx_pemx_ctl_status_t;
@@ -1524,7 +1655,9 @@ union cvmx_pemx_dbg_info {
 	uint64_t u64;
 	struct cvmx_pemx_dbg_info_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_50_63               : 14;
+	uint64_t reserved_53_63               : 11;
+	uint64_t lofp                         : 1;  /**< Lack of Forward Progress at TLP FIFOs timeout occured. */
+	uint64_t reserved_50_51               : 2;
 	uint64_t c_d1_dbe                     : 1;  /**< Detected a TLP CPL Fifo data1 double bit error */
 	uint64_t c_d1_sbe                     : 1;  /**< Detected a TLP CPL Fifo data1 single bit error */
 	uint64_t c_d0_dbe                     : 1;  /**< Detected a TLP CPL Fifo data0 double bit error */
@@ -1648,7 +1781,9 @@ union cvmx_pemx_dbg_info {
 	uint64_t c_d0_dbe                     : 1;
 	uint64_t c_d1_sbe                     : 1;
 	uint64_t c_d1_dbe                     : 1;
-	uint64_t reserved_50_63               : 14;
+	uint64_t reserved_50_51               : 2;
+	uint64_t lofp                         : 1;
+	uint64_t reserved_53_63               : 11;
 #endif
 	} s;
 	struct cvmx_pemx_dbg_info_cn61xx {
@@ -1917,7 +2052,8 @@ union cvmx_pemx_dbg_info {
 	} cn70xx;
 	struct cvmx_pemx_dbg_info_cn78xx {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_52_63               : 12;
+	uint64_t reserved_53_63               : 11;
+	uint64_t lofp                         : 1;  /**< Lack of Forward Progress at TLP FIFOs timeout occured. */
 	uint64_t c_c_dbe                      : 1;  /**< Detected a TLP CPL Fifo ctrl double bit error */
 	uint64_t c_c_sbe                      : 1;  /**< Detected a TLP CPL Fifo ctrl single bit error */
 	uint64_t c_d1_dbe                     : 1;  /**< Detected a TLP CPL Fifo data1 double bit error */
@@ -2070,7 +2206,8 @@ union cvmx_pemx_dbg_info {
 	uint64_t c_d1_dbe                     : 1;
 	uint64_t c_c_sbe                      : 1;
 	uint64_t c_c_dbe                      : 1;
-	uint64_t reserved_52_63               : 12;
+	uint64_t lofp                         : 1;
+	uint64_t reserved_53_63               : 11;
 #endif
 	} cn78xx;
 	struct cvmx_pemx_dbg_info_cn61xx      cnf71xx;
@@ -2087,7 +2224,8 @@ union cvmx_pemx_dbg_info_en {
 	uint64_t u64;
 	struct cvmx_pemx_dbg_info_en_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_52_63               : 12;
+	uint64_t reserved_53_63               : 11;
+	uint64_t lofp_en                      : 1;  /**< Allows PEM_DBG_INFO[52] to generate an interrupt. */
 	uint64_t tpcdbe2                      : 1;  /**< Allows PEM_DBG_INFO[51] to generate an interrupt. */
 	uint64_t reserved_49_50               : 2;
 	uint64_t tpcsbe2                      : 1;  /**< Allows PEM_DBG_INFO[48] to generate an interrupt. */
@@ -2167,7 +2305,8 @@ union cvmx_pemx_dbg_info_en {
 	uint64_t tpcsbe2                      : 1;
 	uint64_t reserved_49_50               : 2;
 	uint64_t tpcdbe2                      : 1;
-	uint64_t reserved_52_63               : 12;
+	uint64_t lofp_en                      : 1;
+	uint64_t reserved_53_63               : 11;
 #endif
 	} s;
 	struct cvmx_pemx_dbg_info_en_cn61xx {
@@ -2345,7 +2484,8 @@ union cvmx_pemx_dbg_info_en {
 	} cn70xx;
 	struct cvmx_pemx_dbg_info_en_cn78xx {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_52_63               : 12;
+	uint64_t reserved_53_63               : 11;
+	uint64_t lofp_en                      : 1;  /**< Allows PEM_DBG_INFO[52] to generate an interrupt. */
 	uint64_t tpcdbe2                      : 1;  /**< Allows PEM_DBG_INFO[51] to generate an interrupt. */
 	uint64_t tpcdbe1                      : 1;  /**< Allows PEM_DBG_INFO[50] to generate an interrupt. */
 	uint64_t tpcdbe0                      : 1;  /**< Allows PEM_DBG_INFO[49] to generate an interrupt. */
@@ -2451,7 +2591,8 @@ union cvmx_pemx_dbg_info_en {
 	uint64_t tpcdbe0                      : 1;
 	uint64_t tpcdbe1                      : 1;
 	uint64_t tpcdbe2                      : 1;
-	uint64_t reserved_52_63               : 12;
+	uint64_t lofp_en                      : 1;
+	uint64_t reserved_53_63               : 11;
 #endif
 	} cn78xx;
 	struct cvmx_pemx_dbg_info_en_cn61xx   cnf71xx;
@@ -2510,7 +2651,7 @@ typedef union cvmx_pemx_diag_status cvmx_pemx_diag_status_t;
 /**
  * cvmx_pem#_ecc_ena
  *
- * Contains enables for ECC RAMs
+ * Contains enables for TLP FIFO ECC RAMs
  *
  */
 union cvmx_pemx_ecc_ena {
@@ -2581,7 +2722,7 @@ typedef union cvmx_pemx_ecc_ena cvmx_pemx_ecc_ena_t;
  * cvmx_pem#_ecc_synd_ctrl
  *
  * PEM_ECC_SYND_CTL
- * Contains Syndrome Control for ECC RAMs
+ * Contains Syndrome Control for TLP FIFO ECC RAMs
  */
 union cvmx_pemx_ecc_synd_ctrl {
 	uint64_t u64;
diff --git a/arch/mips/include/asm/octeon/cvmx-pexp-defs.h b/arch/mips/include/asm/octeon/cvmx-pexp-defs.h
index 19979ad..ffcadf9 100644
--- a/arch/mips/include/asm/octeon/cvmx-pexp-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-pexp-defs.h
@@ -1723,13 +1723,12 @@ static inline uint64_t CVMX_PEXP_SLI_PKTX_INSTR_HEADER(unsigned long offset)
 	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((offset <= 31))) ||
 	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 31))) ||
 	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((offset <= 31))) ||
-	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63))) ||
 	      (OCTEON_IS_MODEL(OCTEON_CNF71XX) && ((offset <= 31)))))
 		cvmx_warn("CVMX_PEXP_SLI_PKTX_INSTR_HEADER(%lu) is invalid on this chip\n", offset);
-	return CVMX_ADD_IO_SEG(0x00011F0000013400ull) + ((offset) & 63) * 16;
+	return CVMX_ADD_IO_SEG(0x00011F0000013400ull) + ((offset) & 31) * 16;
 }
 #else
-#define CVMX_PEXP_SLI_PKTX_INSTR_HEADER(offset) (CVMX_ADD_IO_SEG(0x00011F0000013400ull) + ((offset) & 63) * 16)
+#define CVMX_PEXP_SLI_PKTX_INSTR_HEADER(offset) (CVMX_ADD_IO_SEG(0x00011F0000013400ull) + ((offset) & 31) * 16)
 #endif
 #if CVMX_ENABLE_CSR_ADDRESS_CHECKING
 static inline uint64_t CVMX_PEXP_SLI_PKTX_IN_BP(unsigned long offset)
diff --git a/arch/mips/include/asm/octeon/cvmx-pki-defs.h b/arch/mips/include/asm/octeon/cvmx-pki-defs.h
index e9ffbad..516d5f3 100644
--- a/arch/mips/include/asm/octeon/cvmx-pki-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-pki-defs.h
@@ -1132,7 +1132,7 @@ union cvmx_pki_buf_ctl {
 	uint64_t pkt_off                      : 1;  /**< Packet buffer off. When this bit is set to 1, the PKI does not buffer the received packet
                                                          data; when it is clear to 0, the PKI works normally, buffering the received packet data. */
 	uint64_t reserved_3_4                 : 2;
-	uint64_t pbp_en                       : 1;  /**< Bpid enable. When set, enables the sending of bpid level backpressure to the CN78XX input
+	uint64_t pbp_en                       : 1;  /**< Bpid enable. When set, enables the sending of bpid level backpressure to the input
                                                          interface.
                                                          The application should not de-assert this bit after asserting it. The receivers of this
                                                          bit may have been put into backpressure mode and can only be released by PKI informing
@@ -1196,36 +1196,32 @@ union cvmx_pki_clx_ecc_ctl {
 	uint64_t u64;
 	struct cvmx_pki_clx_ecc_ctl_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_26_63               : 38;
+	uint64_t reserved_24_63               : 40;
 	uint64_t pcam1_flip                   : 2;  /**< PCAM1 flip syndrome bits on write. */
 	uint64_t pcam0_flip                   : 2;  /**< PCAM  flip syndrome bits on write. */
-	uint64_t smem_flip                    : 2;  /**< SMEM flip syndrome bits on write. */
-	uint64_t kmem_flip                    : 2;  /**< KMEM flip syndrome bits on write. Flip syndrome bits <1:0> on writes to the KMEM ram to
+	uint64_t smem_flip                    : 2;  /**< SMEM flip syndrome bits on write. Flip syndrome bits <1:0> on writes to the KMEM ram to
                                                          test single-bit or double-bit error handling. */
 	uint64_t dmem_flip                    : 1;  /**< DMEM flip parity. */
 	uint64_t rf_flip                      : 1;  /**< RF flip parity. */
-	uint64_t reserved_6_15                : 10;
+	uint64_t reserved_5_15                : 11;
 	uint64_t pcam1_cdis                   : 1;  /**< PCAM1 ECC correction disable. */
 	uint64_t pcam0_cdis                   : 1;  /**< PCAM0 ECC correction disable. */
 	uint64_t smem_cdis                    : 1;  /**< SMEM ECC correction disable. */
-	uint64_t kmem_cdis                    : 1;  /**< KMEM ECC correction disable. */
 	uint64_t dmem_cdis                    : 1;  /**< DMEM parity poising disable. */
 	uint64_t rf_cdis                      : 1;  /**< RF RAM parity poising disable. */
 #else
 	uint64_t rf_cdis                      : 1;
 	uint64_t dmem_cdis                    : 1;
-	uint64_t kmem_cdis                    : 1;
 	uint64_t smem_cdis                    : 1;
 	uint64_t pcam0_cdis                   : 1;
 	uint64_t pcam1_cdis                   : 1;
-	uint64_t reserved_6_15                : 10;
+	uint64_t reserved_5_15                : 11;
 	uint64_t rf_flip                      : 1;
 	uint64_t dmem_flip                    : 1;
-	uint64_t kmem_flip                    : 2;
 	uint64_t smem_flip                    : 2;
 	uint64_t pcam0_flip                   : 2;
 	uint64_t pcam1_flip                   : 2;
-	uint64_t reserved_26_63               : 38;
+	uint64_t reserved_24_63               : 40;
 #endif
 	} s;
 	struct cvmx_pki_clx_ecc_ctl_s         cn78xx;
@@ -1239,29 +1235,25 @@ union cvmx_pki_clx_ecc_int {
 	uint64_t u64;
 	struct cvmx_pki_clx_ecc_int_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_10_63               : 54;
+	uint64_t reserved_8_63                : 56;
 	uint64_t pcam1_dbe                    : 1;  /**< PCAM1 ECC double bit error. Throws PKI_INTSN_E::PKI_CL(0..3)_ECC_PCAM1_DBE. */
 	uint64_t pcam1_sbe                    : 1;  /**< PCAM1 ECC single bit error. Throws PKI_INTSN_E::PKI_CL(0..3)_ECC_PCAM1_SBE. */
 	uint64_t pcam0_dbe                    : 1;  /**< PCAM0 ECC double bit error. Throws PKI_INTSN_E::PKI_CL(0..3)_ECC_PCAM0_DBE. */
 	uint64_t pcam0_sbe                    : 1;  /**< PCAM0 ECC single bit error. Throws PKI_INTSN_E::PKI_CL(0..3)_ECC_PCAM0_SBE. */
 	uint64_t smem_dbe                     : 1;  /**< SMEM ECC double bit error. Throws PKI_INTSN_E::PKI_CL(0..3)_ECC_SMEM_DBE. */
 	uint64_t smem_sbe                     : 1;  /**< SMEM ECC single bit error. Throws PKI_INTSN_E::PKI_CL(0..3)_ECC_SMEM_SBE. */
-	uint64_t kmem_dbe                     : 1;  /**< KMEM ECC double bit error. Throws PKI_INTSN_E::PKI_CL(0..3)_ECC_KMEM_DBE. */
-	uint64_t kmem_sbe                     : 1;  /**< KMEM ECC single bit error. Throws PKI_INTSN_E::PKI_CL(0..3)_ECC_KMEM_SBE. */
 	uint64_t dmem_perr                    : 1;  /**< DMEM parity error. Throws PKI_INTSN_E::PKI_CL(0..3)_ECC_DMEM_PERR. */
 	uint64_t rf_perr                      : 1;  /**< RF RAM parity error. Throws PKI_INTSN_E::PKI_CL(0..3)_ECC_RF_PERR. */
 #else
 	uint64_t rf_perr                      : 1;
 	uint64_t dmem_perr                    : 1;
-	uint64_t kmem_sbe                     : 1;
-	uint64_t kmem_dbe                     : 1;
 	uint64_t smem_sbe                     : 1;
 	uint64_t smem_dbe                     : 1;
 	uint64_t pcam0_sbe                    : 1;
 	uint64_t pcam0_dbe                    : 1;
 	uint64_t pcam1_sbe                    : 1;
 	uint64_t pcam1_dbe                    : 1;
-	uint64_t reserved_10_63               : 54;
+	uint64_t reserved_8_63                : 56;
 #endif
 	} s;
 	struct cvmx_pki_clx_ecc_int_s         cn78xx;
@@ -1724,8 +1716,9 @@ union cvmx_pki_clx_stylex_cfg {
                                                          patterns. IPv6 outlaws this and the spec says to always check UDP checksum.
                                                          0 = Spec compliant, do not allow optional code.
                                                          1 = Treat IPv6 as IPv4; the all-0s pattern will cause a UDP checksum pass. */
-	uint64_t lenerr_en                    : 1;  /**< L2 length error check enable. Check if frame was received with L2 length error, see LENERR
-                                                         in Table 10-2. This check is typically not enabled for incoming packets on the DPI ports. */
+	uint64_t lenerr_en                    : 1;  /**< L2 length error check enable. Check if frame was received with L2 length error. This check
+                                                         is typically not enabled for incoming packets on the DPI ports. INTERNAL: Sequencer clears
+                                                         this bit for PKI_BE when SNAP length checks are not appropriate. */
 	uint64_t lenerr_eqpad                 : 1;  /**< L2 length checks exact pad size.
                                                          0 = Length check uses greater then or equal comparison. Packets must have at least minimum
                                                          padding, but may have more. This mode must be used when there may be extra Etherypes
@@ -1736,10 +1729,10 @@ union cvmx_pki_clx_stylex_cfg {
                                                          checks.
                                                          0 = use PKI_FRM_LEN_CHK0
                                                          1 = use PKI_FRM_LEN_CHK1 */
-	uint64_t maxerr_en                    : 1;  /**< Max frame error check enable. See MAXLEN in Table 10-2. */
-	uint64_t minerr_en                    : 1;  /**< Min frame error check enable. See MINLEN in Table 10-2. This check is typically not
-                                                         enabled for incoming packets on the DPI ports. */
-	uint64_t reserved_24_24               : 1;
+	uint64_t maxerr_en                    : 1;  /**< Max frame error check enable. */
+	uint64_t minerr_en                    : 1;  /**< Min frame error check enable. This check is typically not enabled for incoming packets on
+                                                         the DPI ports. */
+	uint64_t qpg_dis_grptag               : 1;  /**< Disable computing group using WQE[TAG]. */
 	uint64_t fcs_strip                    : 1;  /**< Strip L2 FCS bytes from packet, decrease WQE[LEN] by 4 bytes. */
 	uint64_t fcs_chk                      : 1;  /**< FCS Checking enabled. */
 	uint64_t rawdrp                       : 1;  /**< Allow RAW packet drop.
@@ -1768,7 +1761,7 @@ union cvmx_pki_clx_stylex_cfg {
 	uint64_t rawdrp                       : 1;
 	uint64_t fcs_chk                      : 1;
 	uint64_t fcs_strip                    : 1;
-	uint64_t reserved_24_24               : 1;
+	uint64_t qpg_dis_grptag               : 1;
 	uint64_t minerr_en                    : 1;
 	uint64_t maxerr_en                    : 1;
 	uint64_t minmax_sel                   : 1;
@@ -1791,22 +1784,25 @@ union cvmx_pki_clx_stylex_cfg2 {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_32_63               : 32;
 	uint64_t tag_inc                      : 4;  /**< Include masked tags using PKI_TAG_INC(0..31)_MASK. Each bit indicates to include the
-                                                         corresponding PKI_TAG_INC_MASK range, see WQE[TAG]. Multiple TAG_INCs may be selected to
-                                                         allow a tag calculation to include data from floating Layer B through Layer G positions. */
+                                                         corresponding PKI_TAG_INC_MASK range, see PKI_INST_HDR_S. */
 	uint64_t reserved_25_27               : 3;
-	uint64_t tag_masken                   : 1;  /**< Apply PKI_STYLE(0..63)_TAG_MASK to computed tag. */
+	uint64_t tag_masken                   : 1;  /**< Apply PKI_STYLE(0..63)_TAG_MASK to computed tag. INTERNAL: Sequencer must clear for PKI BE
+                                                         when the tag comes from the PKI_INST_HDR_S. */
 	uint64_t tag_src_lg                   : 1;  /**< Include Layer G source address in tuple tag generation. */
 	uint64_t tag_src_lf                   : 1;  /**< Include Layer F source address in tuple tag generation. */
 	uint64_t tag_src_le                   : 1;  /**< Include Layer E source address in tuple tag generation. */
 	uint64_t tag_src_ld                   : 1;  /**< Include Layer D source address in tuple tag generation. */
 	uint64_t tag_src_lc                   : 1;  /**< Include Layer C source address in tuple tag generation. */
-	uint64_t tag_src_lb                   : 1;  /**< Include Layer B source address in tuple tag generation. */
+	uint64_t tag_src_lb                   : 1;  /**< Include Layer B source address in tuple tag generation. INTERNAL: Sequencer must clear
+                                                         TAG_SRC_L* for PKI BE when TCP SYNs are not tagged, or when the tag comes from the
+                                                         PKI_INST_HDR_S. */
 	uint64_t tag_dst_lg                   : 1;  /**< Include Layer G destination address in tuple tag generation. */
 	uint64_t tag_dst_lf                   : 1;  /**< Include Layer F destination address in tuple tag generation. */
 	uint64_t tag_dst_le                   : 1;  /**< Include Layer E destination address in tuple tag generation. */
 	uint64_t tag_dst_ld                   : 1;  /**< Include Layer D destination address in tuple tag generation. */
 	uint64_t tag_dst_lc                   : 1;  /**< Include Layer C destination address in tuple tag generation. */
-	uint64_t tag_dst_lb                   : 1;  /**< Include Layer B destination address in tuple tag generation. */
+	uint64_t tag_dst_lb                   : 1;  /**< Include Layer B destination address in tuple tag generation. INTERNAL: Sequencer must
+                                                         clear TAG_SRC_L* for PKI BE when the tag comes from the PKI_INST_HDR_S. */
 	uint64_t len_lg                       : 1;  /**< Check length of Layer G. */
 	uint64_t len_lf                       : 1;  /**< Check length of Layer F. */
 	uint64_t len_le                       : 1;  /**< Check length of Layer E. */
@@ -2128,8 +2124,8 @@ union cvmx_pki_frm_len_chkx {
 	struct cvmx_pki_frm_len_chkx_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_32_63               : 32;
-	uint64_t maxlen                       : 16; /**< Byte count for max-sized frame check. See MAXLEN in Table 10-2 for adjustments to this value. */
-	uint64_t minlen                       : 16; /**< Byte count for min-sized frame check.See MINLEN in Table 10-2. */
+	uint64_t maxlen                       : 16; /**< Byte count for max-sized frame check. */
+	uint64_t minlen                       : 16; /**< Byte count for min-sized frame check. */
 #else
 	uint64_t minlen                       : 16;
 	uint64_t maxlen                       : 16;
@@ -2270,7 +2266,10 @@ union cvmx_pki_icgx_cfg {
 	uint64_t clusters                     : 4;  /**< Bit-mask of clusters in this cluster group. A given cluster can only be enabled in a
                                                          single cluster group. A value of 0 disables the cluster group, all packets to this group
                                                          will be dropped. IGC(0)'s entry resets to 0xF, all other entries to 0x0. */
-	uint64_t reserved_26_31               : 6;
+	uint64_t reserved_27_31               : 5;
+	uint64_t release_rqd                  : 1;  /**< Release required. For diagnostic use only. INTERNAL:
+                                                         0 = Release of r64 to r95 will occur immediately, no release microop is needed.
+                                                         1 = Release will wait until release microop executes. */
 	uint64_t mlo                          : 1;  /**< Memory low bypass enable. For diagnostic use only. INTERNAL:
                                                          0 = KMEM specifies contents of r48 to r63. The sequencer code expects this setting.
                                                          1 = KMEM specifies contents of r32 to r47. This may be desirable when PKIENA=0 to allow
@@ -2294,7 +2293,8 @@ union cvmx_pki_icgx_cfg {
 	uint64_t timer                        : 12;
 	uint64_t pena                         : 1;
 	uint64_t mlo                          : 1;
-	uint64_t reserved_26_31               : 6;
+	uint64_t release_rqd                  : 1;
+	uint64_t reserved_27_31               : 5;
 	uint64_t clusters                     : 4;
 	uint64_t reserved_36_47               : 12;
 	uint64_t maxipe_use                   : 5;
@@ -2514,10 +2514,12 @@ union cvmx_pki_qpg_tblx {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_60_63               : 4;
 	uint64_t padd                         : 12; /**< Port to channel adder for calculating WQE[CHAN]. */
-	uint64_t reserved_42_47               : 6;
-	uint64_t grp_ok                       : 10; /**< SSO Group to schedule packet to and to load WQE[GRP] with if no error is detected. */
-	uint64_t reserved_26_31               : 6;
-	uint64_t grp_bad                      : 10; /**< SSO Group to schedule packet to and to load WQE[GRP] with if an error is detected. */
+	uint64_t grptag_ok                    : 3;  /**< Number of WQE[TAG] bits to add into WQE[GRP] if no error is detected. */
+	uint64_t reserved_42_44               : 3;
+	uint64_t grp_ok                       : 10; /**< SSO group to schedule packet to and to load WQE[GRP] with if no error is detected. */
+	uint64_t grptag_bad                   : 3;  /**< Number of WQE[TAG] bits to add into WQE[GRP] if an error is detected. */
+	uint64_t reserved_26_28               : 3;
+	uint64_t grp_bad                      : 10; /**< SSO group to schedule packet to and to load WQE[GRP] with if an error is detected. */
 	uint64_t reserved_12_15               : 4;
 	uint64_t aura_node                    : 2;  /**< Aura node number. The node number is part of the upper aura bits, however PKI can only
                                                          allocate from auras on the local node, therefore these bits are hardcoded to the node
@@ -2528,9 +2530,11 @@ union cvmx_pki_qpg_tblx {
 	uint64_t aura_node                    : 2;
 	uint64_t reserved_12_15               : 4;
 	uint64_t grp_bad                      : 10;
-	uint64_t reserved_26_31               : 6;
+	uint64_t reserved_26_28               : 3;
+	uint64_t grptag_bad                   : 3;
 	uint64_t grp_ok                       : 10;
-	uint64_t reserved_42_47               : 6;
+	uint64_t reserved_42_44               : 3;
+	uint64_t grptag_ok                    : 3;
 	uint64_t padd                         : 12;
 	uint64_t reserved_60_63               : 4;
 #endif
@@ -2922,9 +2926,8 @@ union cvmx_pki_statx_stat17 {
 	struct cvmx_pki_statx_stat17_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_48_63               : 16;
-	uint64_t drp_mcast                    : 48; /**< Number of packets with IPv4 or IPv6 L3 multicast destination address, or IPv6 L3 multicast
-                                                         destination address that were dropped due to RED or buffer exhaustion. See WQE[L3M] for
-                                                         the definition of L2 multicast. */
+	uint64_t drp_mcast                    : 48; /**< Number of packets with IPv4 or IPv6 L3 multicast destination address that were dropped due
+                                                         to RED or buffer exhaustion. See WQE[L3M] for the definition of L3 multicast. */
 #else
 	uint64_t drp_mcast                    : 48;
 	uint64_t reserved_48_63               : 16;
@@ -3336,8 +3339,8 @@ typedef union cvmx_pki_tag_incx_mask cvmx_pki_tag_incx_mask_t;
 /**
  * cvmx_pki_tag_secret
  *
- * The source and destination initial values (IVs) in tag generation provide a mechanism for each
- * CN78XX to be unique.
+ * The source and destination initial values (IVs) in tag generation provide a mechanism for
+ * seeding with a random initialization value to reduce cache collision attacks.
  */
 union cvmx_pki_tag_secret {
 	uint64_t u64;
diff --git a/arch/mips/include/asm/octeon/cvmx-pki.h b/arch/mips/include/asm/octeon/cvmx-pki.h
index f034ba9..3ca9528 100644
--- a/arch/mips/include/asm/octeon/cvmx-pki.h
+++ b/arch/mips/include/asm/octeon/cvmx-pki.h
@@ -84,7 +84,7 @@ extern "C" {
 #define CVMX_PKI_MAX_NAME		(16)
 #define CVMX_PKI_MAX_POOL_PROFILES	(64) //modify it later
 #define CVMX_PKI_MAX_AURA_PROFILES	(256) //modify it later
-#define CVMX_PKI_MAX_GROUP_PROFILES	(256)
+#define CVMX_PKI_MAX_SSO_GROUP_PROFILES	(256)
 
 #ifdef CVMX_SUPPORT_SEPARATE_CLUSTER_CONFIG
 #define CVMX_PKI_TOTAL_PCAM_ENTRY	((CVMX_PKI_NUM_CLUSTERS) * (CVMX_PKI_NUM_PCAM_BANK) *\
@@ -148,16 +148,20 @@ struct cvmx_pki_aura_list
 	struct cvmx_pki_aura_profile aura_profile[CVMX_PKI_MAX_AURA_PROFILES];
 };
 
-struct cvmx_pki_group_profile
+struct cvmx_pki_sso_grp_profile
 {
-	char group_name[CVMX_PKI_MAX_NAME];
-	int group_num;
-};
-
-struct cvmx_pki_group_list
+	char grp_name[CVMX_PKI_MAX_NAME];
+	int grp_num;
+	int priority;
+	int weight;
+	int affinity;
+	uint64_t core_affinity_mask;
+	uint64_t core_affinity_mask_set;
+};
+struct cvmx_pki_sso_grp_list
 {
 	int index;
-	struct cvmx_pki_group_profile group_profile[CVMX_PKI_MAX_GROUP_PROFILES];
+	struct cvmx_pki_sso_grp_profile grp_profile[CVMX_PKI_MAX_SSO_GROUP_PROFILES];
 };
 
 struct cvmx_pki_qpg_profile
@@ -475,7 +479,7 @@ struct cvmx_pki_profiles
 	struct cvmx_pki_style_list		style_profile_list;
 	struct cvmx_pki_pool_list		pool_profile_list;
 	struct cvmx_pki_aura_list		aura_profile_list;
-	struct cvmx_pki_group_list	        group_profile_list;
+	struct cvmx_pki_sso_grp_list	        sso_grp_profile_list;
 	struct cvmx_pki_qpg_list	        qpg_profile_list;
 };
 
@@ -874,12 +878,11 @@ int cvmx_pki_set_aura_config(int node, char* aura_name, int aura_num, int pool,
  * This function stores the group configuration in data structure
  * which is then used to program the hardware.
  * @param node  	node number
- * @param aura_name  	name associated with this config
- * @param group		SSO group number (-1 if needs to be allocated)
+ * @param grp_profile	struct to SSO group profile to configure
  * @return 		0 on SUCCESS
                         -1 on failure
  */
-int cvmx_pki_set_group_config(int node, char *name, int group);
+int cvmx_pki_set_sso_group_config(int node, struct cvmx_pki_sso_grp_profile grp_profile);
 
 /**
  * This function stores the qpg configuration in data structure
diff --git a/arch/mips/include/asm/octeon/cvmx-pko-defs.h b/arch/mips/include/asm/octeon/cvmx-pko-defs.h
index 5813ed1..59a4425 100644
--- a/arch/mips/include/asm/octeon/cvmx-pko-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-pko-defs.h
@@ -108,6 +108,17 @@ static inline uint64_t CVMX_PKO_DPFI_STATUS_FUNC(void)
 #define CVMX_PKO_DPFI_STATUS (CVMX_ADD_IO_SEG(0x0001540000C00000ull))
 #endif
 #if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_DQX_BYTES(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_DQX_BYTES(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00015400000000C8ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_DQX_BYTES(offset) (CVMX_ADD_IO_SEG(0x00015400000000C8ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
 static inline uint64_t CVMX_PKO_DQX_CIR(unsigned long offset)
 {
 	if (!(
@@ -141,6 +152,28 @@ static inline uint64_t CVMX_PKO_DQX_DROPPED_PACKETS(unsigned long offset)
 #define CVMX_PKO_DQX_DROPPED_PACKETS(offset) (CVMX_ADD_IO_SEG(0x00015400000000D0ull) + ((offset) & 1023) * 512)
 #endif
 #if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_DQX_FIFO(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_DQX_FIFO(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000300078ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_DQX_FIFO(offset) (CVMX_ADD_IO_SEG(0x0001540000300078ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_DQX_PACKETS(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_DQX_PACKETS(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00015400000000C0ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_DQX_PACKETS(offset) (CVMX_ADD_IO_SEG(0x00015400000000C0ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
 static inline uint64_t CVMX_PKO_DQX_PICK(unsigned long offset)
 {
 	if (!(
@@ -196,28 +229,6 @@ static inline uint64_t CVMX_PKO_DQX_SCHED_STATE(unsigned long offset)
 #define CVMX_PKO_DQX_SCHED_STATE(offset) (CVMX_ADD_IO_SEG(0x0001540000280028ull) + ((offset) & 1023) * 512)
 #endif
 #if CVMX_ENABLE_CSR_ADDRESS_CHECKING
-static inline uint64_t CVMX_PKO_DQX_SENT_BYTES(unsigned long offset)
-{
-	if (!(
-	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
-		cvmx_warn("CVMX_PKO_DQX_SENT_BYTES(%lu) is invalid on this chip\n", offset);
-	return CVMX_ADD_IO_SEG(0x00015400000000C8ull) + ((offset) & 1023) * 512;
-}
-#else
-#define CVMX_PKO_DQX_SENT_BYTES(offset) (CVMX_ADD_IO_SEG(0x00015400000000C8ull) + ((offset) & 1023) * 512)
-#endif
-#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
-static inline uint64_t CVMX_PKO_DQX_SENT_PACKETS(unsigned long offset)
-{
-	if (!(
-	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
-		cvmx_warn("CVMX_PKO_DQX_SENT_PACKETS(%lu) is invalid on this chip\n", offset);
-	return CVMX_ADD_IO_SEG(0x00015400000000C0ull) + ((offset) & 1023) * 512;
-}
-#else
-#define CVMX_PKO_DQX_SENT_PACKETS(offset) (CVMX_ADD_IO_SEG(0x00015400000000C0ull) + ((offset) & 1023) * 512)
-#endif
-#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
 static inline uint64_t CVMX_PKO_DQX_SHAPE(unsigned long offset)
 {
 	if (!(
@@ -328,6 +339,28 @@ static inline uint64_t CVMX_PKO_L1_SQX_CIR(unsigned long offset)
 #define CVMX_PKO_L1_SQX_CIR(offset) (CVMX_ADD_IO_SEG(0x0001540000000018ull) + ((offset) & 31) * 512)
 #endif
 #if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L1_SQX_DROPPED_BYTES(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
+		cvmx_warn("CVMX_PKO_L1_SQX_DROPPED_BYTES(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00015400000000B8ull) + ((offset) & 31) * 512;
+}
+#else
+#define CVMX_PKO_L1_SQX_DROPPED_BYTES(offset) (CVMX_ADD_IO_SEG(0x00015400000000B8ull) + ((offset) & 31) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L1_SQX_DROPPED_PACKETS(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
+		cvmx_warn("CVMX_PKO_L1_SQX_DROPPED_PACKETS(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00015400000000B0ull) + ((offset) & 31) * 512;
+}
+#else
+#define CVMX_PKO_L1_SQX_DROPPED_PACKETS(offset) (CVMX_ADD_IO_SEG(0x00015400000000B0ull) + ((offset) & 31) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
 static inline uint64_t CVMX_PKO_L1_SQX_GREEN(unsigned long offset)
 {
 	if (!(
@@ -339,6 +372,28 @@ static inline uint64_t CVMX_PKO_L1_SQX_GREEN(unsigned long offset)
 #define CVMX_PKO_L1_SQX_GREEN(offset) (CVMX_ADD_IO_SEG(0x0001540000080058ull) + ((offset) & 31) * 512)
 #endif
 #if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L1_SQX_GREEN_BYTES(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
+		cvmx_warn("CVMX_PKO_L1_SQX_GREEN_BYTES(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000000088ull) + ((offset) & 31) * 512;
+}
+#else
+#define CVMX_PKO_L1_SQX_GREEN_BYTES(offset) (CVMX_ADD_IO_SEG(0x0001540000000088ull) + ((offset) & 31) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L1_SQX_GREEN_PACKETS(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
+		cvmx_warn("CVMX_PKO_L1_SQX_GREEN_PACKETS(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000000080ull) + ((offset) & 31) * 512;
+}
+#else
+#define CVMX_PKO_L1_SQX_GREEN_PACKETS(offset) (CVMX_ADD_IO_SEG(0x0001540000000080ull) + ((offset) & 31) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
 static inline uint64_t CVMX_PKO_L1_SQX_LINK(unsigned long offset)
 {
 	if (!(
@@ -383,6 +438,28 @@ static inline uint64_t CVMX_PKO_L1_SQX_RED(unsigned long offset)
 #define CVMX_PKO_L1_SQX_RED(offset) (CVMX_ADD_IO_SEG(0x0001540000080068ull) + ((offset) & 31) * 512)
 #endif
 #if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L1_SQX_RED_BYTES(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
+		cvmx_warn("CVMX_PKO_L1_SQX_RED_BYTES(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00015400000000A8ull) + ((offset) & 31) * 512;
+}
+#else
+#define CVMX_PKO_L1_SQX_RED_BYTES(offset) (CVMX_ADD_IO_SEG(0x00015400000000A8ull) + ((offset) & 31) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L1_SQX_RED_PACKETS(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
+		cvmx_warn("CVMX_PKO_L1_SQX_RED_PACKETS(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00015400000000A0ull) + ((offset) & 31) * 512;
+}
+#else
+#define CVMX_PKO_L1_SQX_RED_PACKETS(offset) (CVMX_ADD_IO_SEG(0x00015400000000A0ull) + ((offset) & 31) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
 static inline uint64_t CVMX_PKO_L1_SQX_SHAPE(unsigned long offset)
 {
 	if (!(
@@ -438,6 +515,28 @@ static inline uint64_t CVMX_PKO_L1_SQX_YELLOW(unsigned long offset)
 #define CVMX_PKO_L1_SQX_YELLOW(offset) (CVMX_ADD_IO_SEG(0x0001540000080060ull) + ((offset) & 31) * 512)
 #endif
 #if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L1_SQX_YELLOW_BYTES(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
+		cvmx_warn("CVMX_PKO_L1_SQX_YELLOW_BYTES(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000000098ull) + ((offset) & 31) * 512;
+}
+#else
+#define CVMX_PKO_L1_SQX_YELLOW_BYTES(offset) (CVMX_ADD_IO_SEG(0x0001540000000098ull) + ((offset) & 31) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L1_SQX_YELLOW_PACKETS(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
+		cvmx_warn("CVMX_PKO_L1_SQX_YELLOW_PACKETS(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000000090ull) + ((offset) & 31) * 512;
+}
+#else
+#define CVMX_PKO_L1_SQX_YELLOW_PACKETS(offset) (CVMX_ADD_IO_SEG(0x0001540000000090ull) + ((offset) & 31) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
 static inline uint64_t CVMX_PKO_L2_SQX_CIR(unsigned long offset)
 {
 	if (!(
@@ -1032,6 +1131,17 @@ static inline uint64_t CVMX_PKO_LUTX(unsigned long offset)
 #define CVMX_PKO_LUTX(offset) (CVMX_ADD_IO_SEG(0x0001540000B00000ull) + ((offset) & 1023) * 8)
 #endif
 #if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_LUT_BIST_DONE CVMX_PKO_LUT_BIST_DONE_FUNC()
+static inline uint64_t CVMX_PKO_LUT_BIST_DONE_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_LUT_BIST_DONE not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000B02020ull);
+}
+#else
+#define CVMX_PKO_LUT_BIST_DONE (CVMX_ADD_IO_SEG(0x0001540000B02020ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
 #define CVMX_PKO_LUT_BIST_STATUS CVMX_PKO_LUT_BIST_STATUS_FUNC()
 static inline uint64_t CVMX_PKO_LUT_BIST_STATUS_FUNC(void)
 {
@@ -1312,6 +1422,17 @@ static inline uint64_t CVMX_PKO_MEM_THROTTLE_PIPE_FUNC(void)
 #define CVMX_PKO_MEM_THROTTLE_PIPE (CVMX_ADD_IO_SEG(0x0001180050001050ull))
 #endif
 #if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_NCB_BIST_STATUS CVMX_PKO_NCB_BIST_STATUS_FUNC()
+static inline uint64_t CVMX_PKO_NCB_BIST_STATUS_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_NCB_BIST_STATUS not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000EFFF00ull);
+}
+#else
+#define CVMX_PKO_NCB_BIST_STATUS (CVMX_ADD_IO_SEG(0x0001540000EFFF00ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
 #define CVMX_PKO_NCB_ECC_CTL0 CVMX_PKO_NCB_ECC_CTL0_FUNC()
 static inline uint64_t CVMX_PKO_NCB_ECC_CTL0_FUNC(void)
 {
@@ -1400,6 +1521,17 @@ static inline uint64_t CVMX_PKO_NCB_TX_ERR_WORD_FUNC(void)
 #define CVMX_PKO_NCB_TX_ERR_WORD (CVMX_ADD_IO_SEG(0x0001540000E00000ull))
 #endif
 #if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_BIST_STATUS CVMX_PKO_PDM_BIST_STATUS_FUNC()
+static inline uint64_t CVMX_PKO_PDM_BIST_STATUS_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_BIST_STATUS not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400008FFF00ull);
+}
+#else
+#define CVMX_PKO_PDM_BIST_STATUS (CVMX_ADD_IO_SEG(0x00015400008FFF00ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
 #define CVMX_PKO_PDM_CFG CVMX_PKO_PDM_CFG_FUNC()
 static inline uint64_t CVMX_PKO_PDM_CFG_FUNC(void)
 {
@@ -1422,6 +1554,28 @@ static inline uint64_t CVMX_PKO_PDM_DQX_MINPAD(unsigned long offset)
 #define CVMX_PKO_PDM_DQX_MINPAD(offset) (CVMX_ADD_IO_SEG(0x00015400008F0000ull) + ((offset) & 1023) * 8)
 #endif
 #if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_DRPBUF_DBG CVMX_PKO_PDM_DRPBUF_DBG_FUNC()
+static inline uint64_t CVMX_PKO_PDM_DRPBUF_DBG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_DRPBUF_DBG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400008000B0ull);
+}
+#else
+#define CVMX_PKO_PDM_DRPBUF_DBG (CVMX_ADD_IO_SEG(0x00015400008000B0ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_DWPBUF_DBG CVMX_PKO_PDM_DWPBUF_DBG_FUNC()
+static inline uint64_t CVMX_PKO_PDM_DWPBUF_DBG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_DWPBUF_DBG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400008000A8ull);
+}
+#else
+#define CVMX_PKO_PDM_DWPBUF_DBG (CVMX_ADD_IO_SEG(0x00015400008000A8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
 #define CVMX_PKO_PDM_ECC_CTL0 CVMX_PKO_PDM_ECC_CTL0_FUNC()
 static inline uint64_t CVMX_PKO_PDM_ECC_CTL0_FUNC(void)
 {
@@ -1477,6 +1631,28 @@ static inline uint64_t CVMX_PKO_PDM_ECC_SBE_STS_CMB0_FUNC(void)
 #define CVMX_PKO_PDM_ECC_SBE_STS_CMB0 (CVMX_ADD_IO_SEG(0x00015400008FFFE8ull))
 #endif
 #if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_ISRD_DBG CVMX_PKO_PDM_ISRD_DBG_FUNC()
+static inline uint64_t CVMX_PKO_PDM_ISRD_DBG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_ISRD_DBG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000800090ull);
+}
+#else
+#define CVMX_PKO_PDM_ISRD_DBG (CVMX_ADD_IO_SEG(0x0001540000800090ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_ISRM_DBG CVMX_PKO_PDM_ISRM_DBG_FUNC()
+static inline uint64_t CVMX_PKO_PDM_ISRM_DBG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_ISRM_DBG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000800098ull);
+}
+#else
+#define CVMX_PKO_PDM_ISRM_DBG (CVMX_ADD_IO_SEG(0x0001540000800098ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
 #define CVMX_PKO_PDM_MEM_ADDR CVMX_PKO_PDM_MEM_ADDR_FUNC()
 static inline uint64_t CVMX_PKO_PDM_MEM_ADDR_FUNC(void)
 {
@@ -1521,6 +1697,17 @@ static inline uint64_t CVMX_PKO_PDM_MEM_RW_STS_FUNC(void)
 #define CVMX_PKO_PDM_MEM_RW_STS (CVMX_ADD_IO_SEG(0x0001540000800028ull))
 #endif
 #if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_MWPBUF_DBG CVMX_PKO_PDM_MWPBUF_DBG_FUNC()
+static inline uint64_t CVMX_PKO_PDM_MWPBUF_DBG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_MWPBUF_DBG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400008000A0ull);
+}
+#else
+#define CVMX_PKO_PDM_MWPBUF_DBG (CVMX_ADD_IO_SEG(0x00015400008000A0ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
 #define CVMX_PKO_PDM_SENDPKT_LMTXX_ERR CVMX_PKO_PDM_SENDPKT_LMTXX_ERR_FUNC()
 static inline uint64_t CVMX_PKO_PDM_SENDPKT_LMTXX_ERR_FUNC(void)
 {
@@ -1543,6 +1730,28 @@ static inline uint64_t CVMX_PKO_PDM_STS_FUNC(void)
 #define CVMX_PKO_PDM_STS (CVMX_ADD_IO_SEG(0x0001540000800008ull))
 #endif
 #if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PEB_BIST_DONE CVMX_PKO_PEB_BIST_DONE_FUNC()
+static inline uint64_t CVMX_PKO_PEB_BIST_DONE_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PEB_BIST_DONE not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000900D08ull);
+}
+#else
+#define CVMX_PKO_PEB_BIST_DONE (CVMX_ADD_IO_SEG(0x0001540000900D08ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PEB_BIST_STATUS CVMX_PKO_PEB_BIST_STATUS_FUNC()
+static inline uint64_t CVMX_PKO_PEB_BIST_STATUS_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PEB_BIST_STATUS not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000900D00ull);
+}
+#else
+#define CVMX_PKO_PEB_BIST_STATUS (CVMX_ADD_IO_SEG(0x0001540000900D00ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
 #define CVMX_PKO_PEB_ECC_CTL0 CVMX_PKO_PEB_ECC_CTL0_FUNC()
 static inline uint64_t CVMX_PKO_PEB_ECC_CTL0_FUNC(void)
 {
@@ -1730,105 +1939,6 @@ static inline uint64_t CVMX_PKO_PEB_TRUNC_ERR_INFO_FUNC(void)
 #define CVMX_PKO_PEB_TRUNC_ERR_INFO (CVMX_ADD_IO_SEG(0x0001540000900C30ull))
 #endif
 #if CVMX_ENABLE_CSR_ADDRESS_CHECKING
-static inline uint64_t CVMX_PKO_PQX_DROPPED_BYTES(unsigned long offset)
-{
-	if (!(
-	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
-		cvmx_warn("CVMX_PKO_PQX_DROPPED_BYTES(%lu) is invalid on this chip\n", offset);
-	return CVMX_ADD_IO_SEG(0x00015400000000B8ull) + ((offset) & 31) * 512;
-}
-#else
-#define CVMX_PKO_PQX_DROPPED_BYTES(offset) (CVMX_ADD_IO_SEG(0x00015400000000B8ull) + ((offset) & 31) * 512)
-#endif
-#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
-static inline uint64_t CVMX_PKO_PQX_DROPPED_PACKETS(unsigned long offset)
-{
-	if (!(
-	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
-		cvmx_warn("CVMX_PKO_PQX_DROPPED_PACKETS(%lu) is invalid on this chip\n", offset);
-	return CVMX_ADD_IO_SEG(0x00015400000000B0ull) + ((offset) & 31) * 512;
-}
-#else
-#define CVMX_PKO_PQX_DROPPED_PACKETS(offset) (CVMX_ADD_IO_SEG(0x00015400000000B0ull) + ((offset) & 31) * 512)
-#endif
-#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
-static inline uint64_t CVMX_PKO_PQX_GREEN_SENT_BYTES(unsigned long offset)
-{
-	if (!(
-	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
-		cvmx_warn("CVMX_PKO_PQX_GREEN_SENT_BYTES(%lu) is invalid on this chip\n", offset);
-	return CVMX_ADD_IO_SEG(0x0001540000000088ull) + ((offset) & 31) * 512;
-}
-#else
-#define CVMX_PKO_PQX_GREEN_SENT_BYTES(offset) (CVMX_ADD_IO_SEG(0x0001540000000088ull) + ((offset) & 31) * 512)
-#endif
-#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
-static inline uint64_t CVMX_PKO_PQX_GREEN_SENT_PACKETS(unsigned long offset)
-{
-	if (!(
-	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
-		cvmx_warn("CVMX_PKO_PQX_GREEN_SENT_PACKETS(%lu) is invalid on this chip\n", offset);
-	return CVMX_ADD_IO_SEG(0x0001540000000080ull) + ((offset) & 31) * 512;
-}
-#else
-#define CVMX_PKO_PQX_GREEN_SENT_PACKETS(offset) (CVMX_ADD_IO_SEG(0x0001540000000080ull) + ((offset) & 31) * 512)
-#endif
-#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
-static inline uint64_t CVMX_PKO_PQX_RED_SENT_BYTES(unsigned long offset)
-{
-	if (!(
-	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
-		cvmx_warn("CVMX_PKO_PQX_RED_SENT_BYTES(%lu) is invalid on this chip\n", offset);
-	return CVMX_ADD_IO_SEG(0x00015400000000A8ull) + ((offset) & 31) * 512;
-}
-#else
-#define CVMX_PKO_PQX_RED_SENT_BYTES(offset) (CVMX_ADD_IO_SEG(0x00015400000000A8ull) + ((offset) & 31) * 512)
-#endif
-#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
-static inline uint64_t CVMX_PKO_PQX_RED_SENT_PACKETS(unsigned long offset)
-{
-	if (!(
-	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
-		cvmx_warn("CVMX_PKO_PQX_RED_SENT_PACKETS(%lu) is invalid on this chip\n", offset);
-	return CVMX_ADD_IO_SEG(0x00015400000000A0ull) + ((offset) & 31) * 512;
-}
-#else
-#define CVMX_PKO_PQX_RED_SENT_PACKETS(offset) (CVMX_ADD_IO_SEG(0x00015400000000A0ull) + ((offset) & 31) * 512)
-#endif
-#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
-static inline uint64_t CVMX_PKO_PQX_TOPOLOGY(unsigned long offset)
-{
-	if (!(
-	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
-		cvmx_warn("CVMX_PKO_PQX_TOPOLOGY(%lu) is invalid on this chip\n", offset);
-	return CVMX_ADD_IO_SEG(0x0001540000000000ull) + ((offset) & 31) * 512;
-}
-#else
-#define CVMX_PKO_PQX_TOPOLOGY(offset) (CVMX_ADD_IO_SEG(0x0001540000000000ull) + ((offset) & 31) * 512)
-#endif
-#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
-static inline uint64_t CVMX_PKO_PQX_YELLOW_SENT_BYTES(unsigned long offset)
-{
-	if (!(
-	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
-		cvmx_warn("CVMX_PKO_PQX_YELLOW_SENT_BYTES(%lu) is invalid on this chip\n", offset);
-	return CVMX_ADD_IO_SEG(0x0001540000000098ull) + ((offset) & 31) * 512;
-}
-#else
-#define CVMX_PKO_PQX_YELLOW_SENT_BYTES(offset) (CVMX_ADD_IO_SEG(0x0001540000000098ull) + ((offset) & 31) * 512)
-#endif
-#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
-static inline uint64_t CVMX_PKO_PQX_YELLOW_SENT_PACKETS(unsigned long offset)
-{
-	if (!(
-	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
-		cvmx_warn("CVMX_PKO_PQX_YELLOW_SENT_PACKETS(%lu) is invalid on this chip\n", offset);
-	return CVMX_ADD_IO_SEG(0x0001540000000090ull) + ((offset) & 31) * 512;
-}
-#else
-#define CVMX_PKO_PQX_YELLOW_SENT_PACKETS(offset) (CVMX_ADD_IO_SEG(0x0001540000000090ull) + ((offset) & 31) * 512)
-#endif
-#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
 #define CVMX_PKO_PSE_DQ_ECC_CTL0 CVMX_PKO_PSE_DQ_ECC_CTL0_FUNC()
 static inline uint64_t CVMX_PKO_PSE_DQ_ECC_CTL0_FUNC(void)
 {
@@ -2540,7 +2650,7 @@ union cvmx_pko_dpfi_flush {
                                                          valid pointers from the pointer cache and return them to the FPA. The flush operation is
                                                          complete when the CACHE_FLUSHED flag in the PKO_DFPI_STATUS register is set. Clearing the
                                                          FLUSH_EN flag results in the DPFI reloading its pointer cache. This flush mechanism should
-                                                         only be enabled when the PKO is quiescent and is intended as test/debug feature. */
+                                                         only be enabled when the PKO is quiescent and all DQs have been closed. */
 #else
 	uint64_t flush_en                     : 1;
 	uint64_t reserved_1_63                : 63;
@@ -2577,7 +2687,9 @@ union cvmx_pko_dpfi_status {
 	uint64_t u64;
 	struct cvmx_pko_dpfi_status_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_13_63               : 51;
+	uint64_t ptr_cnt                      : 32; /**< The number of pointers currently in use for storing descriptors
+                                                         and meta-packets plus those available in the DPFI pointer cache. */
+	uint64_t reserved_13_31               : 19;
 	uint64_t isrd_ptr1_rtn_full           : 1;  /**< 1 = ISRD pointer return register 1 contains a valid pointer
                                                          0 = ISRD pointer return register 1 is empty */
 	uint64_t isrd_ptr0_rtn_full           : 1;  /**< 1 = ISRD pointer return register 0 contains a valid pointer
@@ -2602,7 +2714,8 @@ union cvmx_pko_dpfi_status {
                                                          0 = FPA is providing pointers when requested. */
 	uint64_t dpfi_empty                   : 1;  /**< 1 = DPFI pointer cache is empty.
                                                          0 = DPFI pointer cache is not empty. */
-	uint64_t cache_flushed                : 1;  /**< 1 = Cache flush has completed.
+	uint64_t cache_flushed                : 1;  /**< 1 = Cache flush has completed. PKO_DPFI_STATUS[PTR_CNT] will read zero if all
+                                                             outstanding pointers have been returned to the FPA.
                                                          0 = Cache flush not enabled or in-progress. */
 #else
 	uint64_t cache_flushed                : 1;
@@ -2618,7 +2731,8 @@ union cvmx_pko_dpfi_status {
 	uint64_t isrm_ptr1_rtn_full           : 1;
 	uint64_t isrd_ptr0_rtn_full           : 1;
 	uint64_t isrd_ptr1_rtn_full           : 1;
-	uint64_t reserved_13_63               : 51;
+	uint64_t reserved_13_31               : 19;
+	uint64_t ptr_cnt                      : 32;
 #endif
 	} s;
 	struct cvmx_pko_dpfi_status_s         cn78xx;
@@ -2626,6 +2740,24 @@ union cvmx_pko_dpfi_status {
 typedef union cvmx_pko_dpfi_status cvmx_pko_dpfi_status_t;
 
 /**
+ * cvmx_pko_dq#_bytes
+ */
+union cvmx_pko_dqx_bytes {
+	uint64_t u64;
+	struct cvmx_pko_dqx_bytes_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t count                        : 48; /**< Count. The running count of bytes. Note that this count wraps. */
+#else
+	uint64_t count                        : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pko_dqx_bytes_s           cn78xx;
+};
+typedef union cvmx_pko_dqx_bytes cvmx_pko_dqx_bytes_t;
+
+/**
  * cvmx_pko_dq#_cir
  */
 union cvmx_pko_dqx_cir {
@@ -2695,6 +2827,46 @@ union cvmx_pko_dqx_dropped_packets {
 typedef union cvmx_pko_dqx_dropped_packets cvmx_pko_dqx_dropped_packets_t;
 
 /**
+ * cvmx_pko_dq#_fifo
+ */
+union cvmx_pko_dqx_fifo {
+	uint64_t u64;
+	struct cvmx_pko_dqx_fifo_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_15_63               : 49;
+	uint64_t p_con                        : 1;  /**< Reserved. */
+	uint64_t head                         : 7;  /**< See PKO_L2_SQ(0..511)_POINTERS[PREV]. */
+	uint64_t tail                         : 7;  /**< See PKO_L2_SQ(0..511)_POINTERS[NEXT]. */
+#else
+	uint64_t tail                         : 7;
+	uint64_t head                         : 7;
+	uint64_t p_con                        : 1;
+	uint64_t reserved_15_63               : 49;
+#endif
+	} s;
+	struct cvmx_pko_dqx_fifo_s            cn78xx;
+};
+typedef union cvmx_pko_dqx_fifo cvmx_pko_dqx_fifo_t;
+
+/**
+ * cvmx_pko_dq#_packets
+ */
+union cvmx_pko_dqx_packets {
+	uint64_t u64;
+	struct cvmx_pko_dqx_packets_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_40_63               : 24;
+	uint64_t count                        : 40; /**< Count. The running count of packets. Note that this count wraps. */
+#else
+	uint64_t count                        : 40;
+	uint64_t reserved_40_63               : 24;
+#endif
+	} s;
+	struct cvmx_pko_dqx_packets_s         cn78xx;
+};
+typedef union cvmx_pko_dqx_packets cvmx_pko_dqx_packets_t;
+
+/**
  * cvmx_pko_dq#_pick
  */
 union cvmx_pko_dqx_pick {
@@ -2840,42 +3012,6 @@ union cvmx_pko_dqx_schedule {
 typedef union cvmx_pko_dqx_schedule cvmx_pko_dqx_schedule_t;
 
 /**
- * cvmx_pko_dq#_sent_bytes
- */
-union cvmx_pko_dqx_sent_bytes {
-	uint64_t u64;
-	struct cvmx_pko_dqx_sent_bytes_s {
-#ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_48_63               : 16;
-	uint64_t count                        : 48; /**< Count. The running count of bytes. Note that this count wraps. */
-#else
-	uint64_t count                        : 48;
-	uint64_t reserved_48_63               : 16;
-#endif
-	} s;
-	struct cvmx_pko_dqx_sent_bytes_s      cn78xx;
-};
-typedef union cvmx_pko_dqx_sent_bytes cvmx_pko_dqx_sent_bytes_t;
-
-/**
- * cvmx_pko_dq#_sent_packets
- */
-union cvmx_pko_dqx_sent_packets {
-	uint64_t u64;
-	struct cvmx_pko_dqx_sent_packets_s {
-#ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_40_63               : 24;
-	uint64_t count                        : 40; /**< Count. The running count of packets. Note that this count wraps. */
-#else
-	uint64_t count                        : 40;
-	uint64_t reserved_40_63               : 24;
-#endif
-	} s;
-	struct cvmx_pko_dqx_sent_packets_s    cn78xx;
-};
-typedef union cvmx_pko_dqx_sent_packets cvmx_pko_dqx_sent_packets_t;
-
-/**
  * cvmx_pko_dq#_shape
  */
 union cvmx_pko_dqx_shape {
@@ -2940,7 +3076,15 @@ union cvmx_pko_dqx_sw_xoff {
 	uint64_t u64;
 	struct cvmx_pko_dqx_sw_xoff_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_1_63                : 63;
+	uint64_t reserved_4_63                : 60;
+	uint64_t drain_irq                    : 1;  /**< DRAIN_IRQ. Enables an interrupt that will fire when the drain operation has completed. */
+	uint64_t drain_null_link              : 1;  /**< "DRAIN_NULL_LINK. Conditions the drain path to drain through the null link (i.e. link \#
+                                                         28).
+                                                         As such, channel credits, HW_XOFF and shaping will be disabled on the draining path until
+                                                         the path has drained." */
+	uint64_t drain                        : 1;  /**< DRAIN. This control activates a drain path through the PSE that starts at this node and
+                                                         ends at the SQ1 level. The drain path is prioritized over other paths through PSE and can
+                                                         be used in combination with DRAIN_NULL_LINK and DRAIN_IRQ describe above. */
 	uint64_t xoff                         : 1;  /**< XOFF. The PQ is disabled when XOFF is asserted. PQ is enabled when XOFF is de-asserted.
                                                          NOTE: The associated PKO_L1_SQ(0..31)_TOPOLOGY[LINK] must be configured before using this
                                                          register field. Writing to this register field before the associated
@@ -2948,7 +3092,10 @@ union cvmx_pko_dqx_sw_xoff {
                                                          XOFF state of the wrong SQ. */
 #else
 	uint64_t xoff                         : 1;
-	uint64_t reserved_1_63                : 63;
+	uint64_t drain                        : 1;
+	uint64_t drain_null_link              : 1;
+	uint64_t drain_irq                    : 1;
+	uint64_t reserved_4_63                : 60;
 #endif
 	} s;
 	struct cvmx_pko_dqx_sw_xoff_s         cn78xx;
@@ -3128,6 +3275,42 @@ union cvmx_pko_l1_sqx_cir {
 typedef union cvmx_pko_l1_sqx_cir cvmx_pko_l1_sqx_cir_t;
 
 /**
+ * cvmx_pko_l1_sq#_dropped_bytes
+ */
+union cvmx_pko_l1_sqx_dropped_bytes {
+	uint64_t u64;
+	struct cvmx_pko_l1_sqx_dropped_bytes_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t count                        : 48; /**< Count. The running count of bytes. Note that this count wraps. */
+#else
+	uint64_t count                        : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pko_l1_sqx_dropped_bytes_s cn78xx;
+};
+typedef union cvmx_pko_l1_sqx_dropped_bytes cvmx_pko_l1_sqx_dropped_bytes_t;
+
+/**
+ * cvmx_pko_l1_sq#_dropped_packets
+ */
+union cvmx_pko_l1_sqx_dropped_packets {
+	uint64_t u64;
+	struct cvmx_pko_l1_sqx_dropped_packets_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_40_63               : 24;
+	uint64_t count                        : 40; /**< Count. The running count of packets. Note that this count wraps. */
+#else
+	uint64_t count                        : 40;
+	uint64_t reserved_40_63               : 24;
+#endif
+	} s;
+	struct cvmx_pko_l1_sqx_dropped_packets_s cn78xx;
+};
+typedef union cvmx_pko_l1_sqx_dropped_packets cvmx_pko_l1_sqx_dropped_packets_t;
+
+/**
  * cvmx_pko_l1_sq#_green
  */
 union cvmx_pko_l1_sqx_green {
@@ -3135,12 +3318,12 @@ union cvmx_pko_l1_sqx_green {
 	struct cvmx_pko_l1_sqx_green_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_41_63               : 23;
-	uint64_t head                         : 9;  /**< Head pointer. The index of round-robin linked-list head. */
+	uint64_t head                         : 9;  /**< Head pointer. The index of round-robin linked-list head. For internal use only. */
 	uint64_t reserved_25_31               : 7;
-	uint64_t tail                         : 9;  /**< Tail pointer. The index of round-robin linked-list tail. */
+	uint64_t tail                         : 9;  /**< Tail pointer. The index of round-robin linked-list tail. For internal use only. */
 	uint64_t reserved_10_15               : 6;
 	uint64_t active_vec                   : 10; /**< Active vector. A 10-bit vector, ordered by priority, that indicate which inputs to *this*
-                                                         scheduling queue are active. */
+                                                         scheduling queue are active. For internal use only. */
 #else
 	uint64_t active_vec                   : 10;
 	uint64_t reserved_10_15               : 6;
@@ -3155,6 +3338,42 @@ union cvmx_pko_l1_sqx_green {
 typedef union cvmx_pko_l1_sqx_green cvmx_pko_l1_sqx_green_t;
 
 /**
+ * cvmx_pko_l1_sq#_green_bytes
+ */
+union cvmx_pko_l1_sqx_green_bytes {
+	uint64_t u64;
+	struct cvmx_pko_l1_sqx_green_bytes_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t count                        : 48; /**< Count. The running count of bytes. Note that this count wraps. */
+#else
+	uint64_t count                        : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pko_l1_sqx_green_bytes_s  cn78xx;
+};
+typedef union cvmx_pko_l1_sqx_green_bytes cvmx_pko_l1_sqx_green_bytes_t;
+
+/**
+ * cvmx_pko_l1_sq#_green_packets
+ */
+union cvmx_pko_l1_sqx_green_packets {
+	uint64_t u64;
+	struct cvmx_pko_l1_sqx_green_packets_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_40_63               : 24;
+	uint64_t count                        : 40; /**< Count. The running count of packets. Note that this count wraps. */
+#else
+	uint64_t count                        : 40;
+	uint64_t reserved_40_63               : 24;
+#endif
+	} s;
+	struct cvmx_pko_l1_sqx_green_packets_s cn78xx;
+};
+typedef union cvmx_pko_l1_sqx_green_packets cvmx_pko_l1_sqx_green_packets_t;
+
+/**
  * cvmx_pko_l1_sq#_link
  */
 union cvmx_pko_l1_sqx_link {
@@ -3280,12 +3499,12 @@ union cvmx_pko_l1_sqx_red {
 	struct cvmx_pko_l1_sqx_red_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_41_63               : 23;
-	uint64_t head                         : 9;  /**< Head pointer. The index of round-robin linked-list head. */
+	uint64_t head                         : 9;  /**< Head pointer. The index of round-robin linked-list head. For internal use only. */
 	uint64_t reserved_25_31               : 7;
-	uint64_t tail                         : 9;  /**< Tail pointer. The index of round-robin linked-list tail. */
+	uint64_t tail                         : 9;  /**< Tail pointer. The index of round-robin linked-list tail. For internal use only. */
 	uint64_t reserved_10_15               : 6;
 	uint64_t active_vec                   : 10; /**< Active vector. A 10-bit vector, ordered by priority, that indicate which inputs to *this*
-                                                         scheduling queue are active. */
+                                                         scheduling queue are active. For internal use only. */
 #else
 	uint64_t active_vec                   : 10;
 	uint64_t reserved_10_15               : 6;
@@ -3295,9 +3514,45 @@ union cvmx_pko_l1_sqx_red {
 	uint64_t reserved_41_63               : 23;
 #endif
 	} s;
-	struct cvmx_pko_l1_sqx_red_s          cn78xx;
+	struct cvmx_pko_l1_sqx_red_s          cn78xx;
+};
+typedef union cvmx_pko_l1_sqx_red cvmx_pko_l1_sqx_red_t;
+
+/**
+ * cvmx_pko_l1_sq#_red_bytes
+ */
+union cvmx_pko_l1_sqx_red_bytes {
+	uint64_t u64;
+	struct cvmx_pko_l1_sqx_red_bytes_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t count                        : 48; /**< Count. The running count of bytes. Note that this count wraps. */
+#else
+	uint64_t count                        : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pko_l1_sqx_red_bytes_s    cn78xx;
+};
+typedef union cvmx_pko_l1_sqx_red_bytes cvmx_pko_l1_sqx_red_bytes_t;
+
+/**
+ * cvmx_pko_l1_sq#_red_packets
+ */
+union cvmx_pko_l1_sqx_red_packets {
+	uint64_t u64;
+	struct cvmx_pko_l1_sqx_red_packets_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_40_63               : 24;
+	uint64_t count                        : 40; /**< Count. The running count of packets. Note that this count wraps. */
+#else
+	uint64_t count                        : 40;
+	uint64_t reserved_40_63               : 24;
+#endif
+	} s;
+	struct cvmx_pko_l1_sqx_red_packets_s  cn78xx;
 };
-typedef union cvmx_pko_l1_sqx_red cvmx_pko_l1_sqx_red_t;
+typedef union cvmx_pko_l1_sqx_red_packets cvmx_pko_l1_sqx_red_packets_t;
 
 /**
  * cvmx_pko_l1_sq#_shape
@@ -3354,7 +3609,15 @@ union cvmx_pko_l1_sqx_sw_xoff {
 	uint64_t u64;
 	struct cvmx_pko_l1_sqx_sw_xoff_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_1_63                : 63;
+	uint64_t reserved_4_63                : 60;
+	uint64_t drain_irq                    : 1;  /**< DRAIN_IRQ. Enables an interrupt that will fire when the drain operation has completed. */
+	uint64_t drain_null_link              : 1;  /**< "DRAIN_NULL_LINK. Conditions the drain path to drain through the null link (i.e. link \#
+                                                         28).
+                                                         As such, channel credits, HW_XOFF and shaping will be disabled on the draining path until
+                                                         the path has drained." */
+	uint64_t drain                        : 1;  /**< DRAIN. This control activates a drain path through the PSE that starts at this node and
+                                                         ends at the SQ1 level. The drain path is prioritized over other paths through PSE and can
+                                                         be used in combination with DRAIN_NULL_LINK and DRAIN_IRQ describe above. */
 	uint64_t xoff                         : 1;  /**< XOFF. The PQ is disabled when XOFF is asserted. PQ is enabled when XOFF is de-asserted.
                                                          NOTE: The associated PKO_L1_SQ(0..31)_TOPOLOGY[LINK] must be configured before using this
                                                          register field. Writing to this register field before the associated
@@ -3362,7 +3625,10 @@ union cvmx_pko_l1_sqx_sw_xoff {
                                                          XOFF state of the wrong SQ. */
 #else
 	uint64_t xoff                         : 1;
-	uint64_t reserved_1_63                : 63;
+	uint64_t drain                        : 1;
+	uint64_t drain_null_link              : 1;
+	uint64_t drain_irq                    : 1;
+	uint64_t reserved_4_63                : 60;
 #endif
 	} s;
 	struct cvmx_pko_l1_sqx_sw_xoff_s      cn78xx;
@@ -3427,12 +3693,12 @@ union cvmx_pko_l1_sqx_yellow {
 	struct cvmx_pko_l1_sqx_yellow_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_41_63               : 23;
-	uint64_t head                         : 9;  /**< Head pointer. The index of round-robin linked-list head. */
+	uint64_t head                         : 9;  /**< Head pointer. The index of round-robin linked-list head. For internal use only. */
 	uint64_t reserved_25_31               : 7;
-	uint64_t tail                         : 9;  /**< Tail pointer. The index of round-robin linked-list tail. */
+	uint64_t tail                         : 9;  /**< Tail pointer. The index of round-robin linked-list tail. For internal use only. */
 	uint64_t reserved_10_15               : 6;
 	uint64_t active_vec                   : 10; /**< Active vector. A 10-bit vector, ordered by priority, that indicate which inputs to *this*
-                                                         scheduling queue are active. */
+                                                         scheduling queue are active. For internal use only. */
 #else
 	uint64_t active_vec                   : 10;
 	uint64_t reserved_10_15               : 6;
@@ -3447,6 +3713,42 @@ union cvmx_pko_l1_sqx_yellow {
 typedef union cvmx_pko_l1_sqx_yellow cvmx_pko_l1_sqx_yellow_t;
 
 /**
+ * cvmx_pko_l1_sq#_yellow_bytes
+ */
+union cvmx_pko_l1_sqx_yellow_bytes {
+	uint64_t u64;
+	struct cvmx_pko_l1_sqx_yellow_bytes_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t count                        : 48; /**< Count. The running count of bytes. Note that this count wraps. */
+#else
+	uint64_t count                        : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pko_l1_sqx_yellow_bytes_s cn78xx;
+};
+typedef union cvmx_pko_l1_sqx_yellow_bytes cvmx_pko_l1_sqx_yellow_bytes_t;
+
+/**
+ * cvmx_pko_l1_sq#_yellow_packets
+ */
+union cvmx_pko_l1_sqx_yellow_packets {
+	uint64_t u64;
+	struct cvmx_pko_l1_sqx_yellow_packets_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_40_63               : 24;
+	uint64_t count                        : 40; /**< Count. The running count of packets. Note that this count wraps. */
+#else
+	uint64_t count                        : 40;
+	uint64_t reserved_40_63               : 24;
+#endif
+	} s;
+	struct cvmx_pko_l1_sqx_yellow_packets_s cn78xx;
+};
+typedef union cvmx_pko_l1_sqx_yellow_packets cvmx_pko_l1_sqx_yellow_packets_t;
+
+/**
  * cvmx_pko_l2_sq#_cir
  */
 union cvmx_pko_l2_sqx_cir {
@@ -3487,12 +3789,12 @@ union cvmx_pko_l2_sqx_green {
 	struct cvmx_pko_l2_sqx_green_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_41_63               : 23;
-	uint64_t head                         : 9;  /**< Head pointer. The index of round-robin linked-list head. */
+	uint64_t head                         : 9;  /**< Head pointer. The index of round-robin linked-list head. For internal use only. */
 	uint64_t reserved_25_31               : 7;
-	uint64_t tail                         : 9;  /**< Tail pointer. The index of round-robin linked-list tail. */
+	uint64_t tail                         : 9;  /**< Tail pointer. The index of round-robin linked-list tail. For internal use only. */
 	uint64_t reserved_10_15               : 6;
 	uint64_t active_vec                   : 10; /**< Active vector. A 10-bit vector, ordered by priority, that indicate which inputs to *this*
-                                                         scheduling queue are active. */
+                                                         scheduling queue are active. For internal use only. */
 #else
 	uint64_t active_vec                   : 10;
 	uint64_t reserved_10_15               : 6;
@@ -3615,12 +3917,12 @@ union cvmx_pko_l2_sqx_red {
 	struct cvmx_pko_l2_sqx_red_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_41_63               : 23;
-	uint64_t head                         : 9;  /**< Head pointer. The index of round-robin linked-list head. */
+	uint64_t head                         : 9;  /**< Head pointer. The index of round-robin linked-list head. For internal use only. */
 	uint64_t reserved_25_31               : 7;
-	uint64_t tail                         : 9;  /**< Tail pointer. The index of round-robin linked-list tail. */
+	uint64_t tail                         : 9;  /**< Tail pointer. The index of round-robin linked-list tail. For internal use only. */
 	uint64_t reserved_10_15               : 6;
 	uint64_t active_vec                   : 10; /**< Active vector. A 10-bit vector, ordered by priority, that indicate which inputs to *this*
-                                                         scheduling queue are active. */
+                                                         scheduling queue are active. For internal use only. */
 #else
 	uint64_t active_vec                   : 10;
 	uint64_t reserved_10_15               : 6;
@@ -3749,7 +4051,15 @@ union cvmx_pko_l2_sqx_sw_xoff {
 	uint64_t u64;
 	struct cvmx_pko_l2_sqx_sw_xoff_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_1_63                : 63;
+	uint64_t reserved_4_63                : 60;
+	uint64_t drain_irq                    : 1;  /**< DRAIN_IRQ. Enables an interrupt that will fire when the drain operation has completed. */
+	uint64_t drain_null_link              : 1;  /**< "DRAIN_NULL_LINK. Conditions the drain path to drain through the null link (i.e. link \#
+                                                         28).
+                                                         As such, channel credits, HW_XOFF and shaping will be disabled on the draining path until
+                                                         the path has drained." */
+	uint64_t drain                        : 1;  /**< DRAIN. This control activates a drain path through the PSE that starts at this node and
+                                                         ends at the SQ1 level. The drain path is prioritized over other paths through PSE and can
+                                                         be used in combination with DRAIN_NULL_LINK and DRAIN_IRQ describe above. */
 	uint64_t xoff                         : 1;  /**< XOFF. The PQ is disabled when XOFF is asserted. PQ is enabled when XOFF is de-asserted.
                                                          NOTE: The associated PKO_L1_SQ(0..31)_TOPOLOGY[LINK] must be configured before using this
                                                          register field. Writing to this register field before the associated
@@ -3757,7 +4067,10 @@ union cvmx_pko_l2_sqx_sw_xoff {
                                                          XOFF state of the wrong SQ. */
 #else
 	uint64_t xoff                         : 1;
-	uint64_t reserved_1_63                : 63;
+	uint64_t drain                        : 1;
+	uint64_t drain_null_link              : 1;
+	uint64_t drain_irq                    : 1;
+	uint64_t reserved_4_63                : 60;
 #endif
 	} s;
 	struct cvmx_pko_l2_sqx_sw_xoff_s      cn78xx;
@@ -3805,12 +4118,12 @@ union cvmx_pko_l2_sqx_yellow {
 	struct cvmx_pko_l2_sqx_yellow_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_41_63               : 23;
-	uint64_t head                         : 9;  /**< Head pointer. The index of round-robin linked-list head. */
+	uint64_t head                         : 9;  /**< Head pointer. The index of round-robin linked-list head. For internal use only. */
 	uint64_t reserved_25_31               : 7;
-	uint64_t tail                         : 9;  /**< Tail pointer. The index of round-robin linked-list tail. */
+	uint64_t tail                         : 9;  /**< Tail pointer. The index of round-robin linked-list tail. For internal use only. */
 	uint64_t reserved_10_15               : 6;
 	uint64_t active_vec                   : 10; /**< Active vector. A 10-bit vector, ordered by priority, that indicate which inputs to *this*
-                                                         scheduling queue are active. */
+                                                         scheduling queue are active. For internal use only. */
 #else
 	uint64_t active_vec                   : 10;
 	uint64_t reserved_10_15               : 6;
@@ -4157,7 +4470,15 @@ union cvmx_pko_l3_sqx_sw_xoff {
 	uint64_t u64;
 	struct cvmx_pko_l3_sqx_sw_xoff_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_1_63                : 63;
+	uint64_t reserved_4_63                : 60;
+	uint64_t drain_irq                    : 1;  /**< DRAIN_IRQ. Enables an interrupt that will fire when the drain operation has completed. */
+	uint64_t drain_null_link              : 1;  /**< "DRAIN_NULL_LINK. Conditions the drain path to drain through the null link (i.e. link \#
+                                                         28).
+                                                         As such, channel credits, HW_XOFF and shaping will be disabled on the draining path until
+                                                         the path has drained." */
+	uint64_t drain                        : 1;  /**< DRAIN. This control activates a drain path through the PSE that starts at this node and
+                                                         ends at the SQ1 level. The drain path is prioritized over other paths through PSE and can
+                                                         be used in combination with DRAIN_NULL_LINK and DRAIN_IRQ describe above. */
 	uint64_t xoff                         : 1;  /**< XOFF. The PQ is disabled when XOFF is asserted. PQ is enabled when XOFF is de-asserted.
                                                          NOTE: The associated PKO_L1_SQ(0..31)_TOPOLOGY[LINK] must be configured before using this
                                                          register field. Writing to this register field before the associated
@@ -4165,7 +4486,10 @@ union cvmx_pko_l3_sqx_sw_xoff {
                                                          XOFF state of the wrong SQ. */
 #else
 	uint64_t xoff                         : 1;
-	uint64_t reserved_1_63                : 63;
+	uint64_t drain                        : 1;
+	uint64_t drain_null_link              : 1;
+	uint64_t drain_irq                    : 1;
+	uint64_t reserved_4_63                : 60;
 #endif
 	} s;
 	struct cvmx_pko_l3_sqx_sw_xoff_s      cn78xx;
@@ -4521,7 +4845,15 @@ union cvmx_pko_l4_sqx_sw_xoff {
 	uint64_t u64;
 	struct cvmx_pko_l4_sqx_sw_xoff_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_1_63                : 63;
+	uint64_t reserved_4_63                : 60;
+	uint64_t drain_irq                    : 1;  /**< DRAIN_IRQ. Enables an interrupt that will fire when the drain operation has completed. */
+	uint64_t drain_null_link              : 1;  /**< "DRAIN_NULL_LINK. Conditions the drain path to drain through the null link (i.e. link \#
+                                                         28).
+                                                         As such, channel credits, HW_XOFF and shaping will be disabled on the draining path until
+                                                         the path has drained." */
+	uint64_t drain                        : 1;  /**< DRAIN. This control activates a drain path through the PSE that starts at this node and
+                                                         ends at the SQ1 level. The drain path is prioritized over other paths through PSE and can
+                                                         be used in combination with DRAIN_NULL_LINK and DRAIN_IRQ describe above. */
 	uint64_t xoff                         : 1;  /**< XOFF. The PQ is disabled when XOFF is asserted. PQ is enabled when XOFF is de-asserted.
                                                          NOTE: The associated PKO_L1_SQ(0..31)_TOPOLOGY[LINK] must be configured before using this
                                                          register field. Writing to this register field before the associated
@@ -4529,7 +4861,10 @@ union cvmx_pko_l4_sqx_sw_xoff {
                                                          XOFF state of the wrong SQ. */
 #else
 	uint64_t xoff                         : 1;
-	uint64_t reserved_1_63                : 63;
+	uint64_t drain                        : 1;
+	uint64_t drain_null_link              : 1;
+	uint64_t drain_irq                    : 1;
+	uint64_t reserved_4_63                : 60;
 #endif
 	} s;
 	struct cvmx_pko_l4_sqx_sw_xoff_s      cn78xx;
@@ -4885,7 +5220,15 @@ union cvmx_pko_l5_sqx_sw_xoff {
 	uint64_t u64;
 	struct cvmx_pko_l5_sqx_sw_xoff_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_1_63                : 63;
+	uint64_t reserved_4_63                : 60;
+	uint64_t drain_irq                    : 1;  /**< DRAIN_IRQ. Enables an interrupt that will fire when the drain operation has completed. */
+	uint64_t drain_null_link              : 1;  /**< "DRAIN_NULL_LINK. Conditions the drain path to drain through the null link (i.e. link \#
+                                                         28).
+                                                         As such, channel credits, HW_XOFF and shaping will be disabled on the draining path until
+                                                         the path has drained." */
+	uint64_t drain                        : 1;  /**< DRAIN. This control activates a drain path through the PSE that starts at this node and
+                                                         ends at the SQ1 level. The drain path is prioritized over other paths through PSE and can
+                                                         be used in combination with DRAIN_NULL_LINK and DRAIN_IRQ describe above. */
 	uint64_t xoff                         : 1;  /**< XOFF. The PQ is disabled when XOFF is asserted. PQ is enabled when XOFF is de-asserted.
                                                          NOTE: The associated PKO_L1_SQ(0..31)_TOPOLOGY[LINK] must be configured before using this
                                                          register field. Writing to this register field before the associated
@@ -4893,7 +5236,10 @@ union cvmx_pko_l5_sqx_sw_xoff {
                                                          XOFF state of the wrong SQ. */
 #else
 	uint64_t xoff                         : 1;
-	uint64_t reserved_1_63                : 63;
+	uint64_t drain                        : 1;
+	uint64_t drain_null_link              : 1;
+	uint64_t drain_irq                    : 1;
+	uint64_t reserved_4_63                : 60;
 #endif
 	} s;
 	struct cvmx_pko_l5_sqx_sw_xoff_s      cn78xx;
@@ -4979,19 +5325,35 @@ union cvmx_pko_lutx {
 typedef union cvmx_pko_lutx cvmx_pko_lutx_t;
 
 /**
+ * cvmx_pko_lut_bist_done
+ */
+union cvmx_pko_lut_bist_done {
+	uint64_t u64;
+	struct cvmx_pko_lut_bist_done_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t bist_done                    : 1;  /**< C2Q LUT BIST done. */
+#else
+	uint64_t bist_done                    : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_pko_lut_bist_done_s       cn78xx;
+};
+typedef union cvmx_pko_lut_bist_done cvmx_pko_lut_bist_done_t;
+
+/**
  * cvmx_pko_lut_bist_status
  */
 union cvmx_pko_lut_bist_status {
 	uint64_t u64;
 	struct cvmx_pko_lut_bist_status_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_2_63                : 62;
-	uint64_t bist_done                    : 1;  /**< FIXME needs work - must indicate done status of each RAM. */
-	uint64_t bist_status                  : 1;  /**< FIXME needs work - must indicate status of each RAM. */
+	uint64_t reserved_1_63                : 63;
+	uint64_t bist_status                  : 1;  /**< C2Q LUT BIST status. */
 #else
 	uint64_t bist_status                  : 1;
-	uint64_t bist_done                    : 1;
-	uint64_t reserved_2_63                : 62;
+	uint64_t reserved_1_63                : 63;
 #endif
 	} s;
 	struct cvmx_pko_lut_bist_status_s     cn78xx;
@@ -7195,6 +7557,32 @@ union cvmx_pko_mem_throttle_pipe {
 typedef union cvmx_pko_mem_throttle_pipe cvmx_pko_mem_throttle_pipe_t;
 
 /**
+ * cvmx_pko_ncb_bist_status
+ */
+union cvmx_pko_ncb_bist_status {
+	uint64_t u64;
+	struct cvmx_pko_ncb_bist_status_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t ncbi_l2_out_ram_bist_status  : 1;  /**< BIST status for NCBI_L2_OUT_RAM. */
+	uint64_t ncbi_pp_out_ram_bist_status  : 1;  /**< BIST status for NCBI_PP_OUT_RAM. */
+	uint64_t ncbo_pdm_cmd_dat_ram_bist_status : 1;/**< BIST status for NCBO_PDM_CMD_DAT_RAM. */
+	uint64_t ncbi_l2_pdm_pref_ram_bist_status : 1;/**< BIST status for NCBI_L2_PDM_PREF_RAM. */
+	uint64_t ncbo_pp_fif_ram_bist_status  : 1;  /**< BIST status for NCBO_PP_FIF_RAM. */
+	uint64_t reserved_0_58                : 59;
+#else
+	uint64_t reserved_0_58                : 59;
+	uint64_t ncbo_pp_fif_ram_bist_status  : 1;
+	uint64_t ncbi_l2_pdm_pref_ram_bist_status : 1;
+	uint64_t ncbo_pdm_cmd_dat_ram_bist_status : 1;
+	uint64_t ncbi_pp_out_ram_bist_status  : 1;
+	uint64_t ncbi_l2_out_ram_bist_status  : 1;
+#endif
+	} s;
+	struct cvmx_pko_ncb_bist_status_s     cn78xx;
+};
+typedef union cvmx_pko_ncb_bist_status cvmx_pko_ncb_bist_status_t;
+
+/**
  * cvmx_pko_ncb_ecc_ctl0
  */
 union cvmx_pko_ncb_ecc_ctl0 {
@@ -7386,6 +7774,62 @@ union cvmx_pko_ncb_tx_err_word {
 typedef union cvmx_pko_ncb_tx_err_word cvmx_pko_ncb_tx_err_word_t;
 
 /**
+ * cvmx_pko_pdm_bist_status
+ */
+union cvmx_pko_pdm_bist_status {
+	uint64_t u64;
+	struct cvmx_pko_pdm_bist_status_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t flshb_cache_lo_ram_bist_status : 1;/**< BIST status for FLSHB_CACHE_LO_RAM. */
+	uint64_t flshb_cache_hi_ram_bist_status : 1;/**< BIST status for FLSHB_CACHE_HI_RAM. */
+	uint64_t isrm_ca_iinst_ram_bist_status : 1; /**< BIST status for ISRM_CA_IINST_RAM. */
+	uint64_t isrm_ca_cm_ram_bist_status   : 1;  /**< BIST status for ISRM_CA_CM_RAM. */
+	uint64_t isrm_st_ram2_bist_status     : 1;  /**< BIST status for ISRM_ST_RAM2. */
+	uint64_t isrm_st_ram1_bist_status     : 1;  /**< BIST status for ISRM_ST_RAM1. */
+	uint64_t isrm_st_ram0_bist_status     : 1;  /**< BIST status for ISRM_ST_RAM0. */
+	uint64_t isrd_st_ram3_bist_status     : 1;  /**< BIST status for ISRD_ST_RAM3. */
+	uint64_t isrd_st_ram2_bist_status     : 1;  /**< BIST status for ISRD_ST_RAM2. */
+	uint64_t isrd_st_ram1_bist_status     : 1;  /**< BIST status for ISRD_ST_RAM1. */
+	uint64_t isrd_st_ram0_bist_status     : 1;  /**< BIST status for ISRD_ST_RAM0. */
+	uint64_t drp_hi_ram_bist_status       : 1;  /**< BIST status for DRP_HI_RAM. */
+	uint64_t drp_lo_ram_bist_status       : 1;  /**< BIST status for DRP_LO_RAM. */
+	uint64_t dwp_hi_ram_bist_status       : 1;  /**< BIST status for DWP_HI_RAM. */
+	uint64_t dwp_lo_ram_bist_status       : 1;  /**< BIST status for DWP_LO_RAM. */
+	uint64_t mwp_hi_ram_bist_status       : 1;  /**< BIST status for MWP_HI_RAM. */
+	uint64_t mwp_lo_ram_bist_status       : 1;  /**< BIST status for MWP_LO_RAM. */
+	uint64_t fillb_m_dat_ram_bist_status  : 1;  /**< BIST status for FILLB_M_DAT_RAM. */
+	uint64_t fillb_d_dat_ram_bist_status  : 1;  /**< BIST status for FILLB_D_DAT_RAM. */
+	uint64_t minpad_ram_bist_status       : 1;  /**< BIST status for MINPAD_RAM. */
+	uint64_t reserved_0_43                : 44;
+#else
+	uint64_t reserved_0_43                : 44;
+	uint64_t minpad_ram_bist_status       : 1;
+	uint64_t fillb_d_dat_ram_bist_status  : 1;
+	uint64_t fillb_m_dat_ram_bist_status  : 1;
+	uint64_t mwp_lo_ram_bist_status       : 1;
+	uint64_t mwp_hi_ram_bist_status       : 1;
+	uint64_t dwp_lo_ram_bist_status       : 1;
+	uint64_t dwp_hi_ram_bist_status       : 1;
+	uint64_t drp_lo_ram_bist_status       : 1;
+	uint64_t drp_hi_ram_bist_status       : 1;
+	uint64_t isrd_st_ram0_bist_status     : 1;
+	uint64_t isrd_st_ram1_bist_status     : 1;
+	uint64_t isrd_st_ram2_bist_status     : 1;
+	uint64_t isrd_st_ram3_bist_status     : 1;
+	uint64_t isrm_st_ram0_bist_status     : 1;
+	uint64_t isrm_st_ram1_bist_status     : 1;
+	uint64_t isrm_st_ram2_bist_status     : 1;
+	uint64_t isrm_ca_cm_ram_bist_status   : 1;
+	uint64_t isrm_ca_iinst_ram_bist_status : 1;
+	uint64_t flshb_cache_hi_ram_bist_status : 1;
+	uint64_t flshb_cache_lo_ram_bist_status : 1;
+#endif
+	} s;
+	struct cvmx_pko_pdm_bist_status_s     cn78xx;
+};
+typedef union cvmx_pko_pdm_bist_status cvmx_pko_pdm_bist_status_t;
+
+/**
  * cvmx_pko_pdm_cfg
  */
 union cvmx_pko_pdm_cfg {
@@ -7442,6 +7886,62 @@ union cvmx_pko_pdm_dqx_minpad {
 typedef union cvmx_pko_pdm_dqx_minpad cvmx_pko_pdm_dqx_minpad_t;
 
 /**
+ * cvmx_pko_pdm_drpbuf_dbg
+ */
+union cvmx_pko_pdm_drpbuf_dbg {
+	uint64_t u64;
+	struct cvmx_pko_pdm_drpbuf_dbg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_19_63               : 45;
+	uint64_t cur_state                    : 3;  /**< This is current state from the pbuf controller. */
+	uint64_t track_rd_cnt                 : 6;  /**< This is the track read count value. */
+	uint64_t track_wr_cnt                 : 6;  /**< This is the track write count value. */
+	uint64_t mem_en                       : 4;  /**< These are the memory write/chip enable signals. The order of the bits is:
+                                                         - 3: low wen
+                                                         - 2: low cen
+                                                         - 1: high wen
+                                                         - 0: high cen */
+#else
+	uint64_t mem_en                       : 4;
+	uint64_t track_wr_cnt                 : 6;
+	uint64_t track_rd_cnt                 : 6;
+	uint64_t cur_state                    : 3;
+	uint64_t reserved_19_63               : 45;
+#endif
+	} s;
+	struct cvmx_pko_pdm_drpbuf_dbg_s      cn78xx;
+};
+typedef union cvmx_pko_pdm_drpbuf_dbg cvmx_pko_pdm_drpbuf_dbg_t;
+
+/**
+ * cvmx_pko_pdm_dwpbuf_dbg
+ */
+union cvmx_pko_pdm_dwpbuf_dbg {
+	uint64_t u64;
+	struct cvmx_pko_pdm_dwpbuf_dbg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_19_63               : 45;
+	uint64_t cur_state                    : 3;  /**< This is current state from the pbuf controller. */
+	uint64_t track_rd_cnt                 : 6;  /**< This is the track read count value. */
+	uint64_t track_wr_cnt                 : 6;  /**< This is the track write count value. */
+	uint64_t mem_en                       : 4;  /**< These are the memory write/chip enable signals. The order of the bits is:
+                                                         - 3: low wen
+                                                         - 2: low cen
+                                                         - 1: high wen
+                                                         - 0: high cen */
+#else
+	uint64_t mem_en                       : 4;
+	uint64_t track_wr_cnt                 : 6;
+	uint64_t track_rd_cnt                 : 6;
+	uint64_t cur_state                    : 3;
+	uint64_t reserved_19_63               : 45;
+#endif
+	} s;
+	struct cvmx_pko_pdm_dwpbuf_dbg_s      cn78xx;
+};
+typedef union cvmx_pko_pdm_dwpbuf_dbg cvmx_pko_pdm_dwpbuf_dbg_t;
+
+/**
  * cvmx_pko_pdm_ecc_ctl0
  */
 union cvmx_pko_pdm_ecc_ctl0 {
@@ -7686,6 +8186,150 @@ union cvmx_pko_pdm_ecc_sbe_sts_cmb0 {
 typedef union cvmx_pko_pdm_ecc_sbe_sts_cmb0 cvmx_pko_pdm_ecc_sbe_sts_cmb0_t;
 
 /**
+ * cvmx_pko_pdm_isrd_dbg
+ */
+union cvmx_pko_pdm_isrd_dbg {
+	uint64_t u64;
+	struct cvmx_pko_pdm_isrd_dbg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_44_63               : 20;
+	uint64_t in_arb_reqs                  : 8;  /**< Input arbitration request signals. The order of the bits is:
+                                                         - 43: Fill Response - normal path request
+                                                         - 42: Fill Response - flushb path request
+                                                         - 41: CP queue-open request
+                                                         - 40: CP queue-closed request
+                                                         - 39: CP queue-query request
+                                                         - 38: CP send-packet request
+                                                         - 37: PEB fill request
+                                                         - 36: PEB read request */
+	uint64_t in_arb_gnts                  : 7;  /**< Input arbitration grant signals. The order of the bits is:
+                                                         - 35: Fill Response grant
+                                                         - 34: CP - queue-open grant
+                                                         - 33: CP - queue-close grant
+                                                         - 32: CP - queue-query grant
+                                                         - 31: CP - send-packet grant
+                                                         - 30: PEB Fill grant
+                                                         - 29: PEB Read grant */
+	uint64_t cmt_arb_reqs                 : 7;  /**< Commit arbitration request signals. The order of the bits is:
+                                                         - 28: Fill Response grant
+                                                         - 27: CP - queue-open grant
+                                                         - 26: CP - queue-close grant
+                                                         - 25: CP - queue-query grant
+                                                         - 24: CP - send-packet grant
+                                                         - 23: PEB Fill grant
+                                                         - 22: PEB Read grant */
+	uint64_t cmt_arb_gnts                 : 7;  /**< Commit arbitration grant signals. The order of the bits is:
+                                                         - 21: Fill Response grant
+                                                         - 20: CP - queue-open grant
+                                                         - 19: CP - queue-close grant
+                                                         - 18: CP - queue-query grant
+                                                         - 17: CP - send-packet grant
+                                                         - 16: PEB Fill grant
+                                                         - 15: PEB Read grant */
+	uint64_t in_use                       : 4;  /**< In use signals indicate the execution units are in use. The order of the bits is:
+                                                         - 14: PEB fill unit
+                                                         - 13: PEB read unit
+                                                         - 12: CP unit
+                                                         - 11: Fill Response unit */
+	uint64_t has_cred                     : 4;  /**< Has credit signals indicate there is sufficient credit to commit. The order of the bits
+                                                         is:
+                                                           - 10: Flush Buffer has credit
+                                                          - 9: Fill Buffer has credit
+                                                          - 8: DW command output fifo has credit
+                                                          - 7: DR command output fifo has credit */
+	uint64_t val_exec                     : 7;  /**< Valid bits for the execution units; means the unit can commit if it gets the grant of the
+                                                         commit arb and other conditions are met.
+                                                         The order of the bits is :
+                                                          - 6: fill response unit
+                                                          - 5: CP unit - queue-open
+                                                          - 4: CP unit - queue-close
+                                                          - 3: CP unit - queue-probe
+                                                          - 2: CP unit - send-packet
+                                                          - 1: PEB Fill unit
+                                                          - 0: PEB Read unit */
+#else
+	uint64_t val_exec                     : 7;
+	uint64_t has_cred                     : 4;
+	uint64_t in_use                       : 4;
+	uint64_t cmt_arb_gnts                 : 7;
+	uint64_t cmt_arb_reqs                 : 7;
+	uint64_t in_arb_gnts                  : 7;
+	uint64_t in_arb_reqs                  : 8;
+	uint64_t reserved_44_63               : 20;
+#endif
+	} s;
+	struct cvmx_pko_pdm_isrd_dbg_s        cn78xx;
+};
+typedef union cvmx_pko_pdm_isrd_dbg cvmx_pko_pdm_isrd_dbg_t;
+
+/**
+ * cvmx_pko_pdm_isrm_dbg
+ */
+union cvmx_pko_pdm_isrm_dbg {
+	uint64_t u64;
+	struct cvmx_pko_pdm_isrm_dbg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_34_63               : 30;
+	uint64_t in_arb_reqs                  : 7;  /**< Input arbitration request signals. The order of the bits is:
+                                                         - 33: PSE ack
+                                                         - 32: Fill Response - normal path request
+                                                         - 31: Fill Response - flushb path request
+                                                         - 30: CP queue-open
+                                                         - 29: CP queue-closed
+                                                         - 28: CP queue-query
+                                                         - 27: CP send-packet */
+	uint64_t in_arb_gnts                  : 6;  /**< Input arbitration grant signals. The order of the bits is:
+                                                         - 26: PSE ack
+                                                         - 25: Fill Response
+                                                         - 24: CP - queue-open
+                                                         - 23: CP - queue-close
+                                                         - 22: CP - queue-query
+                                                         - 21: CP - send-packet */
+	uint64_t cmt_arb_reqs                 : 6;  /**< Commit arbitration request signals. The order of the bits is:
+                                                         - 20: PSE ack
+                                                         - 19: Fill Response
+                                                         - 18: CP - queue-open
+                                                         - 17: CP - queue-close
+                                                         - 16: CP - queue-query
+                                                         - 15: CP - send-packet */
+	uint64_t cmt_arb_gnts                 : 6;  /**< Commit arbitration grant signals. The order of the bits is:
+                                                          - 14: PSE ack
+                                                          - 13: Fill Response
+                                                          - 12: CP - queue-open
+                                                          - 11: CP - queue-close
+                                                          - 10: CP - queue-query
+                                                         - 9: CP - send-packet */
+	uint64_t in_use                       : 3;  /**< In use signals indicate the execution units are in use. The order of the bits is:
+                                                         - 8: (PSE) ack unit
+                                                         - 7: Fill Response unit
+                                                         - 6: CP unit */
+	uint64_t has_cred                     : 3;  /**< Has credit signals indicate there is sufficient credit to commit. The order of the bits
+                                                         is:
+                                                          - 5: Flush Buffer has credit
+                                                          - 4: Fill Buffer has credit
+                                                          - 3: MWP command output fifo has credit */
+	uint64_t val_exec                     : 3;  /**< Valid bits for the execution units; means the unit can commit if it gets the grant of the
+                                                         commit arb and other conditions are met.
+                                                         The order of the bits is :
+                                                          - 2: (PSE) ack unit
+                                                          - 1: Fill response unit
+                                                          - 0: CP unit - ALL */
+#else
+	uint64_t val_exec                     : 3;
+	uint64_t has_cred                     : 3;
+	uint64_t in_use                       : 3;
+	uint64_t cmt_arb_gnts                 : 6;
+	uint64_t cmt_arb_reqs                 : 6;
+	uint64_t in_arb_gnts                  : 6;
+	uint64_t in_arb_reqs                  : 7;
+	uint64_t reserved_34_63               : 30;
+#endif
+	} s;
+	struct cvmx_pko_pdm_isrm_dbg_s        cn78xx;
+};
+typedef union cvmx_pko_pdm_isrm_dbg cvmx_pko_pdm_isrm_dbg_t;
+
+/**
  * cvmx_pko_pdm_mem_addr
  */
 union cvmx_pko_pdm_mem_addr {
@@ -7766,6 +8410,34 @@ union cvmx_pko_pdm_mem_rw_sts {
 typedef union cvmx_pko_pdm_mem_rw_sts cvmx_pko_pdm_mem_rw_sts_t;
 
 /**
+ * cvmx_pko_pdm_mwpbuf_dbg
+ */
+union cvmx_pko_pdm_mwpbuf_dbg {
+	uint64_t u64;
+	struct cvmx_pko_pdm_mwpbuf_dbg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_19_63               : 45;
+	uint64_t cur_state                    : 3;  /**< This is current state from the pbuf controller. */
+	uint64_t track_rd_cnt                 : 6;  /**< This is the track read count value. */
+	uint64_t track_wr_cnt                 : 6;  /**< This is the track write count value. */
+	uint64_t mem_en                       : 4;  /**< These are the memory write/chip enable signals. The order of the bits is:
+                                                         - 3: low wen
+                                                         - 2: low cen
+                                                         - 1: high wen
+                                                         - 0: high cen */
+#else
+	uint64_t mem_en                       : 4;
+	uint64_t track_wr_cnt                 : 6;
+	uint64_t track_rd_cnt                 : 6;
+	uint64_t cur_state                    : 3;
+	uint64_t reserved_19_63               : 45;
+#endif
+	} s;
+	struct cvmx_pko_pdm_mwpbuf_dbg_s      cn78xx;
+};
+typedef union cvmx_pko_pdm_mwpbuf_dbg cvmx_pko_pdm_mwpbuf_dbg_t;
+
+/**
  * cvmx_pko_pdm_sendpkt_lmtxx_err
  */
 union cvmx_pko_pdm_sendpkt_lmtxx_err {
@@ -7796,7 +8468,13 @@ union cvmx_pko_pdm_sts {
 	uint64_t u64;
 	struct cvmx_pko_pdm_sts_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_6_63                : 58;
+	uint64_t reserved_9_63                : 55;
+	uint64_t mwpbuf_data_val_err          : 1;  /**< recieved signal that MWPBUF had data valid error. Throws
+                                                         PKO_INTSN_E::PKO_MWPBUF_DATA_VAL_ERR. */
+	uint64_t drpbuf_data_val_err          : 1;  /**< recieved signal that DRPBUF had data valid error. Throws
+                                                         PKO_INTSN_E::PKO_DRPBUF_DATA_VAL_ERR. */
+	uint64_t dwpbuf_data_val_err          : 1;  /**< recieved signal that DWPBUF had data valid error. Throws
+                                                         PKO_INTSN_E::PKO_DWPBUF_DATA_VAL_ERR. */
 	uint64_t sendpkt_lmtdma_err           : 1;  /**< recieved signal that FPA cannot allocate pointer. Throws
                                                          PKO_INTSN_E::PKO_SENDPKT_LMTDMA_ERR. */
 	uint64_t sendpkt_lmtst_err            : 1;  /**< recieved signal that FPA cannot allocate pointer. Throws
@@ -7814,7 +8492,10 @@ union cvmx_pko_pdm_sts {
 	uint64_t fpa_no_ptrs                  : 1;
 	uint64_t sendpkt_lmtst_err            : 1;
 	uint64_t sendpkt_lmtdma_err           : 1;
-	uint64_t reserved_6_63                : 58;
+	uint64_t dwpbuf_data_val_err          : 1;
+	uint64_t drpbuf_data_val_err          : 1;
+	uint64_t mwpbuf_data_val_err          : 1;
+	uint64_t reserved_9_63                : 55;
 #endif
 	} s;
 	struct cvmx_pko_pdm_sts_s             cn78xx;
@@ -7822,6 +8503,144 @@ union cvmx_pko_pdm_sts {
 typedef union cvmx_pko_pdm_sts cvmx_pko_pdm_sts_t;
 
 /**
+ * cvmx_pko_peb_bist_done
+ */
+union cvmx_pko_peb_bist_done {
+	uint64_t u64;
+	struct cvmx_pko_peb_bist_done_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_26_63               : 38;
+	uint64_t add_work_fifo                : 1;  /**< ADD_WORK_FIFO RAM BIST done. */
+	uint64_t pdm_pse_buf_ram              : 1;  /**< PDM_PSE_BUF RAM BIST done. */
+	uint64_t iobp0_fifo_ram               : 1;  /**< IOBP0_FIFO RAM BIST done. */
+	uint64_t iobp1_fifo_ram               : 1;  /**< IOBP1_FIFO RAM BIST done. */
+	uint64_t state_mem0                   : 1;  /**< STATE_MEM0 RAM BIST done. */
+	uint64_t state_mem1                   : 1;  /**< STATE_MEM1 RAM BIST done. */
+	uint64_t state_mem2                   : 1;  /**< STATE_MEM2 RAM BIST done. */
+	uint64_t state_mem3                   : 1;  /**< STATE_MEM3 RAM BIST done. */
+	uint64_t iobp1_uid_fifo_ram           : 1;  /**< IOBP1_UID_FIFO RAM BIST done. */
+	uint64_t nxt_link_ptr_ram             : 1;  /**< NXT_LINK_PTR RAM BIST done. */
+	uint64_t pd_bank0_ram                 : 1;  /**< PD_BANK0 RAM BIST done. */
+	uint64_t pd_bank1_ram                 : 1;  /**< PD_BANK1 RAM BIST done. */
+	uint64_t pd_bank2_ram                 : 1;  /**< PD_BANK2 RAM BIST done. */
+	uint64_t pd_bank3_ram                 : 1;  /**< PD_BANK3 RAM BIST done. */
+	uint64_t pd_var_bank_ram              : 1;  /**< PD_VAR_BANK RAM BIST done. */
+	uint64_t pdm_resp_buf_ram             : 1;  /**< PDM_RESP_BUF RAM BIST done. */
+	uint64_t tx_fifo_pkt_ram              : 1;  /**< TX_FIFO_PKT RAM BIST done. */
+	uint64_t tx_fifo_hdr_ram              : 1;  /**< TX_FIFO_HDR RAM BIST done. */
+	uint64_t tx_fifo_crc_ram              : 1;  /**< TX_FIFO_CRC RAM BIST done. */
+	uint64_t ts_addwork_ram               : 1;  /**< TS_ADDWORK RAM BIST done. */
+	uint64_t send_mem_ts_fifo             : 1;  /**< SEND_MEM_TS_FIFO RAM BIST done. */
+	uint64_t send_mem_stdn_fifo           : 1;  /**< SEND_MEM_STDN_FIFO RAM BIST done. */
+	uint64_t send_mem_fifo                : 1;  /**< SEND_MEM_FIFO RAM BIST done. */
+	uint64_t pkt_mrk_ram                  : 1;  /**< PKT_MRK RAM BIST done. */
+	uint64_t peb_st_inf_ram               : 1;  /**< PEB_ST_INF RAM BIST done. */
+	uint64_t peb_sm_jmp_ram               : 1;  /**< PEB_SM_JMP RAM BIST done.
+                                                         1 = BIST complete.
+                                                         0 = BIST in progress. */
+#else
+	uint64_t peb_sm_jmp_ram               : 1;
+	uint64_t peb_st_inf_ram               : 1;
+	uint64_t pkt_mrk_ram                  : 1;
+	uint64_t send_mem_fifo                : 1;
+	uint64_t send_mem_stdn_fifo           : 1;
+	uint64_t send_mem_ts_fifo             : 1;
+	uint64_t ts_addwork_ram               : 1;
+	uint64_t tx_fifo_crc_ram              : 1;
+	uint64_t tx_fifo_hdr_ram              : 1;
+	uint64_t tx_fifo_pkt_ram              : 1;
+	uint64_t pdm_resp_buf_ram             : 1;
+	uint64_t pd_var_bank_ram              : 1;
+	uint64_t pd_bank3_ram                 : 1;
+	uint64_t pd_bank2_ram                 : 1;
+	uint64_t pd_bank1_ram                 : 1;
+	uint64_t pd_bank0_ram                 : 1;
+	uint64_t nxt_link_ptr_ram             : 1;
+	uint64_t iobp1_uid_fifo_ram           : 1;
+	uint64_t state_mem3                   : 1;
+	uint64_t state_mem2                   : 1;
+	uint64_t state_mem1                   : 1;
+	uint64_t state_mem0                   : 1;
+	uint64_t iobp1_fifo_ram               : 1;
+	uint64_t iobp0_fifo_ram               : 1;
+	uint64_t pdm_pse_buf_ram              : 1;
+	uint64_t add_work_fifo                : 1;
+	uint64_t reserved_26_63               : 38;
+#endif
+	} s;
+	struct cvmx_pko_peb_bist_done_s       cn78xx;
+};
+typedef union cvmx_pko_peb_bist_done cvmx_pko_peb_bist_done_t;
+
+/**
+ * cvmx_pko_peb_bist_status
+ */
+union cvmx_pko_peb_bist_status {
+	uint64_t u64;
+	struct cvmx_pko_peb_bist_status_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_26_63               : 38;
+	uint64_t add_work_fifo                : 1;  /**< ADD_WORK_FIFO RAM BIST status. */
+	uint64_t pdm_pse_buf_ram              : 1;  /**< PDM_PSE_BUF RAM BIST status. */
+	uint64_t iobp0_fifo_ram               : 1;  /**< IOBP0_FIFO RAM BIST status. */
+	uint64_t iobp1_fifo_ram               : 1;  /**< IOBP1_FIFO RAM BIST status. */
+	uint64_t state_mem0                   : 1;  /**< STATE_MEM0 RAM BIST status. */
+	uint64_t state_mem1                   : 1;  /**< STATE_MEM1 RAM BIST status. */
+	uint64_t state_mem2                   : 1;  /**< STATE_MEM2 RAM BIST status. */
+	uint64_t state_mem3                   : 1;  /**< STATE_MEM3 RAM BIST status. */
+	uint64_t iobp1_uid_fifo_ram           : 1;  /**< IOBP1_UID_FIFO RAM BIST status. */
+	uint64_t nxt_link_ptr_ram             : 1;  /**< NXT_LINK_PTR RAM BIST status. */
+	uint64_t pd_bank0_ram                 : 1;  /**< PD_BANK0 RAM BIST status. */
+	uint64_t pd_bank1_ram                 : 1;  /**< PD_BANK1 RAM BIST status. */
+	uint64_t pd_bank2_ram                 : 1;  /**< PD_BANK2 RAM BIST status. */
+	uint64_t pd_bank3_ram                 : 1;  /**< PD_BANK3 RAM BIST status. */
+	uint64_t pd_var_bank_ram              : 1;  /**< PD_VAR_BANK RAM BIST status. */
+	uint64_t pdm_resp_buf_ram             : 1;  /**< PDM_RESP_BUF RAM BIST status. */
+	uint64_t tx_fifo_pkt_ram              : 1;  /**< TX_FIFO_PKT RAM BIST status. */
+	uint64_t tx_fifo_hdr_ram              : 1;  /**< TX_FIFO_HDR RAM BIST status. */
+	uint64_t tx_fifo_crc_ram              : 1;  /**< TX_FIFO_CRC RAM BIST status. */
+	uint64_t ts_addwork_ram               : 1;  /**< TS_ADDWORK RAM BIST status. */
+	uint64_t send_mem_ts_fifo             : 1;  /**< SEND_MEM_TS_FIFO RAM BIST status. */
+	uint64_t send_mem_stdn_fifo           : 1;  /**< SEND_MEM_STDN_FIFO RAM BIST status. */
+	uint64_t send_mem_fifo                : 1;  /**< SEND_MEM_FIFO RAM BIST status. */
+	uint64_t pkt_mrk_ram                  : 1;  /**< PKT_MRK RAM BIST status. */
+	uint64_t peb_st_inf_ram               : 1;  /**< PEB_ST_INF RAM BIST status. */
+	uint64_t peb_sm_jmp_ram               : 1;  /**< PEB_SM_JMP RAM BIST status. 0 = BIST passed; 1 = BIST failed. */
+#else
+	uint64_t peb_sm_jmp_ram               : 1;
+	uint64_t peb_st_inf_ram               : 1;
+	uint64_t pkt_mrk_ram                  : 1;
+	uint64_t send_mem_fifo                : 1;
+	uint64_t send_mem_stdn_fifo           : 1;
+	uint64_t send_mem_ts_fifo             : 1;
+	uint64_t ts_addwork_ram               : 1;
+	uint64_t tx_fifo_crc_ram              : 1;
+	uint64_t tx_fifo_hdr_ram              : 1;
+	uint64_t tx_fifo_pkt_ram              : 1;
+	uint64_t pdm_resp_buf_ram             : 1;
+	uint64_t pd_var_bank_ram              : 1;
+	uint64_t pd_bank3_ram                 : 1;
+	uint64_t pd_bank2_ram                 : 1;
+	uint64_t pd_bank1_ram                 : 1;
+	uint64_t pd_bank0_ram                 : 1;
+	uint64_t nxt_link_ptr_ram             : 1;
+	uint64_t iobp1_uid_fifo_ram           : 1;
+	uint64_t state_mem3                   : 1;
+	uint64_t state_mem2                   : 1;
+	uint64_t state_mem1                   : 1;
+	uint64_t state_mem0                   : 1;
+	uint64_t iobp1_fifo_ram               : 1;
+	uint64_t iobp0_fifo_ram               : 1;
+	uint64_t pdm_pse_buf_ram              : 1;
+	uint64_t add_work_fifo                : 1;
+	uint64_t reserved_26_63               : 38;
+#endif
+	} s;
+	struct cvmx_pko_peb_bist_status_s     cn78xx;
+};
+typedef union cvmx_pko_peb_bist_status cvmx_pko_peb_bist_status_t;
+
+/**
  * cvmx_pko_peb_ecc_ctl0
  */
 union cvmx_pko_peb_ecc_ctl0 {
@@ -8361,170 +9180,6 @@ union cvmx_pko_peb_trunc_err_info {
 typedef union cvmx_pko_peb_trunc_err_info cvmx_pko_peb_trunc_err_info_t;
 
 /**
- * cvmx_pko_pq#_dropped_bytes
- */
-union cvmx_pko_pqx_dropped_bytes {
-	uint64_t u64;
-	struct cvmx_pko_pqx_dropped_bytes_s {
-#ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_48_63               : 16;
-	uint64_t count                        : 48; /**< Count. The running count of bytes. Note that this count wraps. */
-#else
-	uint64_t count                        : 48;
-	uint64_t reserved_48_63               : 16;
-#endif
-	} s;
-	struct cvmx_pko_pqx_dropped_bytes_s   cn78xx;
-};
-typedef union cvmx_pko_pqx_dropped_bytes cvmx_pko_pqx_dropped_bytes_t;
-
-/**
- * cvmx_pko_pq#_dropped_packets
- */
-union cvmx_pko_pqx_dropped_packets {
-	uint64_t u64;
-	struct cvmx_pko_pqx_dropped_packets_s {
-#ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_40_63               : 24;
-	uint64_t count                        : 40; /**< Count. The running count of packets. Note that this count wraps. */
-#else
-	uint64_t count                        : 40;
-	uint64_t reserved_40_63               : 24;
-#endif
-	} s;
-	struct cvmx_pko_pqx_dropped_packets_s cn78xx;
-};
-typedef union cvmx_pko_pqx_dropped_packets cvmx_pko_pqx_dropped_packets_t;
-
-/**
- * cvmx_pko_pq#_green_sent_bytes
- */
-union cvmx_pko_pqx_green_sent_bytes {
-	uint64_t u64;
-	struct cvmx_pko_pqx_green_sent_bytes_s {
-#ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_48_63               : 16;
-	uint64_t count                        : 48; /**< Count. The running count of bytes. Note that this count wraps. */
-#else
-	uint64_t count                        : 48;
-	uint64_t reserved_48_63               : 16;
-#endif
-	} s;
-	struct cvmx_pko_pqx_green_sent_bytes_s cn78xx;
-};
-typedef union cvmx_pko_pqx_green_sent_bytes cvmx_pko_pqx_green_sent_bytes_t;
-
-/**
- * cvmx_pko_pq#_green_sent_packets
- */
-union cvmx_pko_pqx_green_sent_packets {
-	uint64_t u64;
-	struct cvmx_pko_pqx_green_sent_packets_s {
-#ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_40_63               : 24;
-	uint64_t count                        : 40; /**< Count. The running count of packets. Note that this count wraps. */
-#else
-	uint64_t count                        : 40;
-	uint64_t reserved_40_63               : 24;
-#endif
-	} s;
-	struct cvmx_pko_pqx_green_sent_packets_s cn78xx;
-};
-typedef union cvmx_pko_pqx_green_sent_packets cvmx_pko_pqx_green_sent_packets_t;
-
-/**
- * cvmx_pko_pq#_red_sent_bytes
- */
-union cvmx_pko_pqx_red_sent_bytes {
-	uint64_t u64;
-	struct cvmx_pko_pqx_red_sent_bytes_s {
-#ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_48_63               : 16;
-	uint64_t count                        : 48; /**< Count. The running count of bytes. Note that this count wraps. */
-#else
-	uint64_t count                        : 48;
-	uint64_t reserved_48_63               : 16;
-#endif
-	} s;
-	struct cvmx_pko_pqx_red_sent_bytes_s  cn78xx;
-};
-typedef union cvmx_pko_pqx_red_sent_bytes cvmx_pko_pqx_red_sent_bytes_t;
-
-/**
- * cvmx_pko_pq#_red_sent_packets
- */
-union cvmx_pko_pqx_red_sent_packets {
-	uint64_t u64;
-	struct cvmx_pko_pqx_red_sent_packets_s {
-#ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_40_63               : 24;
-	uint64_t count                        : 40; /**< Count. The running count of packets. Note that this count wraps. */
-#else
-	uint64_t count                        : 40;
-	uint64_t reserved_40_63               : 24;
-#endif
-	} s;
-	struct cvmx_pko_pqx_red_sent_packets_s cn78xx;
-};
-typedef union cvmx_pko_pqx_red_sent_packets cvmx_pko_pqx_red_sent_packets_t;
-
-/**
- * cvmx_pko_pq#_topology
- */
-union cvmx_pko_pqx_topology {
-	uint64_t u64;
-	struct cvmx_pko_pqx_topology_s {
-#ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_14_63               : 50;
-	uint64_t peb_fifo                     : 5;  /**< PEB FIFO. The PEB transmit FIFO number. A value of 0x1F means unassigned. */
-	uint64_t reserved_0_8                 : 9;
-#else
-	uint64_t reserved_0_8                 : 9;
-	uint64_t peb_fifo                     : 5;
-	uint64_t reserved_14_63               : 50;
-#endif
-	} s;
-	struct cvmx_pko_pqx_topology_s        cn78xx;
-};
-typedef union cvmx_pko_pqx_topology cvmx_pko_pqx_topology_t;
-
-/**
- * cvmx_pko_pq#_yellow_sent_bytes
- */
-union cvmx_pko_pqx_yellow_sent_bytes {
-	uint64_t u64;
-	struct cvmx_pko_pqx_yellow_sent_bytes_s {
-#ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_48_63               : 16;
-	uint64_t count                        : 48; /**< Count. The running count of bytes. Note that this count wraps. */
-#else
-	uint64_t count                        : 48;
-	uint64_t reserved_48_63               : 16;
-#endif
-	} s;
-	struct cvmx_pko_pqx_yellow_sent_bytes_s cn78xx;
-};
-typedef union cvmx_pko_pqx_yellow_sent_bytes cvmx_pko_pqx_yellow_sent_bytes_t;
-
-/**
- * cvmx_pko_pq#_yellow_sent_packets
- */
-union cvmx_pko_pqx_yellow_sent_packets {
-	uint64_t u64;
-	struct cvmx_pko_pqx_yellow_sent_packets_s {
-#ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_40_63               : 24;
-	uint64_t count                        : 40; /**< Count. The running count of packets. Note that this count wraps. */
-#else
-	uint64_t count                        : 40;
-	uint64_t reserved_40_63               : 24;
-#endif
-	} s;
-	struct cvmx_pko_pqx_yellow_sent_packets_s cn78xx;
-};
-typedef union cvmx_pko_pqx_yellow_sent_packets cvmx_pko_pqx_yellow_sent_packets_t;
-
-/**
  * cvmx_pko_pse_dq_ecc_ctl0
  */
 union cvmx_pko_pse_dq_ecc_ctl0 {
diff --git a/arch/mips/include/asm/octeon/cvmx-pow.h b/arch/mips/include/asm/octeon/cvmx-pow.h
index c4191ce..c04a601 100644
--- a/arch/mips/include/asm/octeon/cvmx-pow.h
+++ b/arch/mips/include/asm/octeon/cvmx-pow.h
@@ -87,6 +87,9 @@ extern "C" {
 #define CVMX_ENABLE_POW_CHECKS 1
 #endif
 
+#define CVMX_SSO_NUM_GROUPS_78XX	(256)
+#define CVMX_SSO_NUM_GROUPS_SET		(CVMX_SSO_NUM_GROUPS_78XX/64)
+
 /**
  * Wait flag values for pow functions.
  */
@@ -709,7 +712,7 @@ typedef union {
 		uint64_t pend_get_work:1;
 					    /**< Set when there is a pending GET_WORK */
 		uint64_t pend_get_work_wait:1;
-					    /**< when pend_get_work is set, this biit indicates that the 
+					    /**< when pend_get_work is set, this biit indicates that the
                                                  wait bit was set. */
 		uint64_t pend_nosched:1;
 					    /**< Set when nosched is desired and pend_desched is set. */
@@ -806,7 +809,7 @@ typedef union {
 		uint64_t pend_get_work:1;
 					    /**< Set when there is a pending GET_WORK */
 		uint64_t pend_get_work_wait:1;
-					    /**< when pend_get_work is set, this biit indicates that the 
+					    /**< when pend_get_work is set, this biit indicates that the
                                                  wait bit was set. */
 		uint64_t pend_nosched:1;
 					    /**< Set when nosched is desired and pend_desched is set. */
@@ -1536,6 +1539,13 @@ typedef union {
 
 /* CSR typedefs have been moved to cvmx-pow-defs.h */
 
+/*enum for group priority parameters which needs modification*/
+enum cvmx_sso_group_modify_mask{
+	CVMX_SSO_MODIFY_GROUP_PRIORITY = 0x01,
+	CVMX_SSO_MODIFY_GROUP_WEIGHT = 0x02,
+	CVMX_SSO_MODIFY_GROUP_AFFINITY = 0x04
+};
+
 /**
  * Get the POW tag for this core. This returns the current
  * tag type, tag, group, and POW entry index associated with
@@ -2219,6 +2229,116 @@ static inline void cvmx_pow_set_group_mask(uint64_t core_num, uint64_t mask)
 }
 
 /**
+ * This function sets the group mask for a core.  The group mask
+ * indicates which groups each core will accept work from. There are
+ * 256 groups in 78xx.
+ *
+ * @param node 		node number
+ * @param core_num   	core to apply mask to
+ * @param mask_set	78XX has 2 set of masks per core each with 256 groups.
+ *                      Cores can choose which mask set to get work from when
+                        getting the work.
+ * @param mask   	Group mask. There are 256 groups, divided in 4 of 64 bit mask sets.
+ * 	        	Each 1 bit in the mask enables the core to accept work from
+ *      	        the corresponding group.
+ */
+static inline void cvmx_pow_set_group_mask_78xx(int node, uint64_t core_num,
+		uint64_t mask_set, const uint64_t mask[])
+{
+	int grp;
+
+	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
+		cvmx_sso_ppx_sx_grpmskx_t grp_msk;
+		for (grp = 0; grp < CVMX_SSO_NUM_GROUPS_SET; grp++) {
+			if(mask_set & 1) {
+				grp_msk.u64 = cvmx_read_csr_node(node, CVMX_SSO_PPX_SX_GRPMSKX(core_num, 0, grp));
+				grp_msk.s.grp_msk |= mask[grp];
+				cvmx_write_csr_node(node, CVMX_SSO_PPX_SX_GRPMSKX(core_num, 0, grp), grp_msk.u64);
+			}
+			if(mask_set & 2) {
+				grp_msk.u64 = cvmx_read_csr_node(node, CVMX_SSO_PPX_SX_GRPMSKX(core_num, 1, grp));
+				grp_msk.s.grp_msk |= mask[grp];
+				cvmx_write_csr_node(node, CVMX_SSO_PPX_SX_GRPMSKX(core_num, 1, grp), grp_msk.u64);
+			}
+		}
+	}
+}
+
+/**
+ * This function sets the the affinity of group to the cores in 78xx.
+ * It sets up all the cores in core_mask to accept work from the specified group.
+ *
+ * @param node 		node number
+ * @param group  	group to accept work from.
+ * @param core_mask	mask of all the cores which will accept work from this group
+ * @param mask_set	every core has set of 2 masks which can be set to accept work
+ *                      from 256 groups. At the time of get_work, cores can choose which
+ *			mask_set to get work from.
+ */
+static inline void cvmx_sso_set_group_core_affinity(int node, int group,
+		uint64_t core_mask, int mask_set)
+{
+	cvmx_sso_ppx_sx_grpmskx_t grp_msk;
+	int core;
+	int grp_index  = group >> 6;
+	int bit_pos = group % 64;
+
+	//cvmx_dprintf("Vinita group=%d grp_index=%d bit_pos=%d core_mask=0x%lx\n",group,grp_index,bit_pos,core_mask);
+	while((core = __builtin_ffsll(core_mask))) {
+		core--;
+		if(mask_set & 1) {
+			grp_msk.u64 = cvmx_read_csr_node(node, CVMX_SSO_PPX_SX_GRPMSKX(core, 0, grp_index));
+			grp_msk.s.grp_msk |= (uint64_t)(1ull << bit_pos);
+			cvmx_write_csr_node(node, CVMX_SSO_PPX_SX_GRPMSKX(core, 0, grp_index), grp_msk.u64);
+		}
+		if(mask_set & 2) {
+			grp_msk.u64 = cvmx_read_csr_node(node, CVMX_SSO_PPX_SX_GRPMSKX(core, 1, grp_index));
+			grp_msk.s.grp_msk |= (uint64_t)(1ull << bit_pos);
+			cvmx_write_csr_node(node, CVMX_SSO_PPX_SX_GRPMSKX(core, 1, grp_index), grp_msk.u64);
+		}
+		core_mask &= core_mask - 1;
+	}
+
+}
+
+/**
+ * This function sets the priority and group affinity arbitration for each group.
+ *
+ * @param node 		node number
+ * @param group  	group to apply mask parameters to
+ * @param priority	priority of the group relative to other groups
+ *			0x0 - highest priority
+ *			0x7 - lowest priority
+ * @param weight	Cross-group arbitration weight to apply to this group.
+ *			valid values are 1-63
+ *			h/w default is 0x3f
+ * @param affinity	Processor affinity arbitration weight to apply to this group.
+ *			If zero, affinity is disabled.
+ *			valid values are 0-15
+ *			h/w default which is 0xf.
+ * @param modify_mask   mask of the parameters which needs to be modified.
+ *			enum cvmx_sso_group_modify_mask
+ *                      to modify only priority -- set bit0
+ *                      to modify only weight   -- set bit1
+ *			to modify only affinity -- set bit2
+ */
+static inline void cvmx_sso_set_group_priority(int node , int group, int priority,
+					       int weight, int affinity,
+					       enum cvmx_sso_group_modify_mask modify_mask)
+{
+	cvmx_sso_grpx_pri_t grp_pri;
+
+	grp_pri.u64 = cvmx_read_csr_node(node, CVMX_SSO_GRPX_PRI(group));
+	if(modify_mask & CVMX_SSO_MODIFY_GROUP_PRIORITY)
+		grp_pri.s.pri = priority;
+	if(modify_mask & CVMX_SSO_MODIFY_GROUP_WEIGHT)
+		grp_pri.s.weight = weight;
+	if(modify_mask & CVMX_SSO_MODIFY_GROUP_AFFINITY)
+		grp_pri.s.affinity = affinity;
+	cvmx_write_csr_node(node,CVMX_SSO_GRPX_PRI(group),grp_pri.u64);
+}
+
+/**
  * This function sets POW static priorities for a core. Each input queue has
  * an associated priority value.
  *
diff --git a/arch/mips/include/asm/octeon/cvmx-qlm.h b/arch/mips/include/asm/octeon/cvmx-qlm.h
index 928ff2c..7ea006d 100644
--- a/arch/mips/include/asm/octeon/cvmx-qlm.h
+++ b/arch/mips/include/asm/octeon/cvmx-qlm.h
@@ -1,5 +1,5 @@
 /***********************license start***************
- * Copyright (c) 2011  Cavium Inc. (support@cavium.com). All rights
+ * Copyright (c) 2011-2013  Cavium Inc. (support@cavium.com). All rights
  * reserved.
  *
  *
@@ -42,7 +42,7 @@
  *
  * Helper utilities for qlm.
  *
- * <hr>$Revision: 74158 $<hr>
+ * <hr>$Revision: 87408 $<hr>
  */
 
 #ifndef __CVMX_QLM_H__
@@ -50,6 +50,20 @@
 
 #include "cvmx.h"
 
+/*
+ * Interface 0 on the 78xx can be connected to qlm 0 or qlm 2. When interface
+ * 0 is connected to qlm 0, this macro must be set to 0. When interface 0 is
+ * connected to qlm 2, this macro must be set to 1.
+ */
+#define MUX_78XX_IFACE0		0
+
+/*
+ * Interface 1 on the 78xx can be connected to qlm 1 or qlm 3. When interface
+ * 1 is connected to qlm 1, this macro must be set to 0. When interface 1 is
+ * connected to qlm 3, this macro must be set to 1.
+ */
+#define MUX_78XX_IFACE1		0
+
 /* Uncomment this line to print QLM JTAG state */
 /* #define CVMX_QLM_DUMP_STATE 1 */
 
@@ -159,16 +173,33 @@ enum cvmx_qlm_mode {
 	CVMX_QLM_MODE_SRIO_2X2,	/* 2x2 short / long */
 	CVMX_QLM_MODE_SRIO_4X1,	/* 4x1 short / long */
 	CVMX_QLM_MODE_ILK,
+	CVMX_QLM_MODE_QSGMII,
+	CVMX_QLM_MODE_SGMII_SGMII,
+	CVMX_QLM_MODE_SGMII_DISABLED,
+	CVMX_QLM_MODE_DISABLED_SGMII,
+	CVMX_QLM_MODE_SGMII_QSGMII,
+	CVMX_QLM_MODE_QSGMII_QSGMII,
+	CVMX_QLM_MODE_QSGMII_DISABLED,
+	CVMX_QLM_MODE_DISABLED_QSGMII,
+	CVMX_QLM_MODE_QSGMII_SGMII,
+	CVMX_QLM_MODE_RXAUI_1X2,
+};
+
+enum cvmx_gmx_inf_mode {
+	CVMX_GMX_INF_MODE_DISABLED = 0,
+	CVMX_GMX_INF_MODE_SGMII = 1,     /* Other interface can be SGMII or QSGMII */
+	CVMX_GMX_INF_MODE_QSGMII = 2,    /* Other interface can be SGMII or QSGMII */
+	CVMX_GMX_INF_MODE_RXAUI = 3,     /* Only interface 0, interface 1 must be DISABLED */
 };
 
 /*
  * Read QLM and return mode.
  */
 extern enum cvmx_qlm_mode cvmx_qlm_get_mode(int qlm);
+extern enum cvmx_qlm_mode cvmx_qlm_get_dlm_mode(int qlm, int interface);
 
 extern void cvmx_qlm_display_registers(int qlm);
 
-extern int cvmx_qlm_configure_qlm(int qlm, int speed, int mode,
-				  int rc, int pcie2x1);
+extern int cvmx_qlm_measure_clock(int qlm);
 
 #endif /* __CVMX_QLM_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-sli-defs.h b/arch/mips/include/asm/octeon/cvmx-sli-defs.h
index 5dee079..8e7dcc0 100644
--- a/arch/mips/include/asm/octeon/cvmx-sli-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-sli-defs.h
@@ -742,13 +742,12 @@ static inline uint64_t CVMX_SLI_PKTX_INSTR_HEADER(unsigned long offset)
 	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((offset <= 31))) ||
 	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 31))) ||
 	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((offset <= 31))) ||
-	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63))) ||
 	      (OCTEON_IS_MODEL(OCTEON_CNF71XX) && ((offset <= 31)))))
 		cvmx_warn("CVMX_SLI_PKTX_INSTR_HEADER(%lu) is invalid on this chip\n", offset);
-	return 0x0000000000003400ull + ((offset) & 63) * 16;
+	return 0x0000000000003400ull + ((offset) & 31) * 16;
 }
 #else
-#define CVMX_SLI_PKTX_INSTR_HEADER(offset) (0x0000000000003400ull + ((offset) & 63) * 16)
+#define CVMX_SLI_PKTX_INSTR_HEADER(offset) (0x0000000000003400ull + ((offset) & 31) * 16)
 #endif
 #if CVMX_ENABLE_CSR_ADDRESS_CHECKING
 static inline uint64_t CVMX_SLI_PKTX_INT_LEVELS(unsigned long offset)
@@ -4157,9 +4156,11 @@ union cvmx_sli_int_sum {
 	uint64_t pcnt                         : 1;  /**< Packet counter has an interrupt. The specific rings can be found in SLI_PKT_CNT_INT.
                                                          Throws SLI_INTSN_E::SLI_INT_PCNT. */
 	uint64_t reserved_1_3                 : 3;
-	uint64_t rml_to                       : 1;  /**< A read or write transfer to a RSL, or the assertion from the OCX that read data is
-                                                         available after sending a read command to the OCX, that did not complete within
-                                                         SLI_WINDOW_CTL[TIME] coprocessor-clock cycles. Throws a SLI_INTSN_E::SLI_INT_RML_TO. */
+	uint64_t rml_to                       : 1;  /**< A read or write transfer to a RSL that did not complete within SLI_WINDOW_CTL[TIME] sclk
+                                                         cycles, or
+                                                         a notification from the OCI that is has sent a previously written command and can take
+                                                         another within
+                                                         SLI_WINDOW_CTL[OCX_TIME]. Throws a SLI_INTSN_E::SLI_INT_RML_TO. */
 #else
 	uint64_t rml_to                       : 1;
 	uint64_t reserved_1_3                 : 3;
@@ -5871,7 +5872,7 @@ typedef union cvmx_sli_pktx_instr_fifo_rsize cvmx_sli_pktx_instr_fifo_rsize_t;
 /**
  * cvmx_sli_pkt#_instr_header
  *
- * "SLI_PKT[0..63]_INSTR_HEADER = SLI Packet ring# Instruction Header.
+ * "SLI_PKT[0..31]_INSTR_HEADER = SLI Packet ring# Instruction Header.
  * VAlues used to build input packet header."
  */
 union cvmx_sli_pktx_instr_header {
@@ -6016,7 +6017,6 @@ union cvmx_sli_pktx_instr_header {
 	struct cvmx_sli_pktx_instr_header_s   cn68xx;
 	struct cvmx_sli_pktx_instr_header_cn61xx cn68xxp1;
 	struct cvmx_sli_pktx_instr_header_cn61xx cn70xx;
-	struct cvmx_sli_pktx_instr_header_s   cn78xx;
 	struct cvmx_sli_pktx_instr_header_cn61xx cnf71xx;
 };
 typedef union cvmx_sli_pktx_instr_header cvmx_sli_pktx_instr_header_t;
@@ -8274,6 +8274,22 @@ union cvmx_sli_window_ctl {
 	uint64_t u64;
 	struct cvmx_sli_window_ctl_s {
 #ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t ocx_time                     : 32; /**< When a command acknowledge or a request to fetch read-data is expected from the OCI, The
+                                                         SLI will
+                                                         wait this many sclks before determining the OCI is not going to respond and timeout the
+                                                         request. */
+	uint64_t time                         : 32; /**< Time to wait in core clocks for a
+                                                         BAR0 access to completeon the NCB
+                                                         before timing out. A value of 0 will cause no
+                                                         timeouts. A minimum value of 0x200000 should be
+                                                         used when this register is not set to 0x0. */
+#else
+	uint64_t time                         : 32;
+	uint64_t ocx_time                     : 32;
+#endif
+	} s;
+	struct cvmx_sli_window_ctl_cn61xx {
+#ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_32_63               : 32;
 	uint64_t time                         : 32; /**< Time to wait in core clocks for a
                                                          BAR0 access to completeon the NCB
@@ -8284,16 +8300,15 @@ union cvmx_sli_window_ctl {
 	uint64_t time                         : 32;
 	uint64_t reserved_32_63               : 32;
 #endif
-	} s;
-	struct cvmx_sli_window_ctl_s          cn61xx;
-	struct cvmx_sli_window_ctl_s          cn63xx;
-	struct cvmx_sli_window_ctl_s          cn63xxp1;
-	struct cvmx_sli_window_ctl_s          cn66xx;
-	struct cvmx_sli_window_ctl_s          cn68xx;
-	struct cvmx_sli_window_ctl_s          cn68xxp1;
-	struct cvmx_sli_window_ctl_s          cn70xx;
+	} cn61xx;
+	struct cvmx_sli_window_ctl_cn61xx     cn63xx;
+	struct cvmx_sli_window_ctl_cn61xx     cn63xxp1;
+	struct cvmx_sli_window_ctl_cn61xx     cn66xx;
+	struct cvmx_sli_window_ctl_cn61xx     cn68xx;
+	struct cvmx_sli_window_ctl_cn61xx     cn68xxp1;
+	struct cvmx_sli_window_ctl_cn61xx     cn70xx;
 	struct cvmx_sli_window_ctl_s          cn78xx;
-	struct cvmx_sli_window_ctl_s          cnf71xx;
+	struct cvmx_sli_window_ctl_cn61xx     cnf71xx;
 };
 typedef union cvmx_sli_window_ctl cvmx_sli_window_ctl_t;
 
diff --git a/arch/mips/include/asm/octeon/cvmx-sso-defs.h b/arch/mips/include/asm/octeon/cvmx-sso-defs.h
index 10eca8e..a6db03f 100644
--- a/arch/mips/include/asm/octeon/cvmx-sso-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-sso-defs.h
@@ -3650,8 +3650,8 @@ union cvmx_sso_ppx_sx_grpmskx {
                                                          GRPMSK(3) for groups <255:192>
                                                          A value of 0x0 in each GRPMSK(0..3) for a given core prevents the core from receiving new
                                                          work. Cores that will never receive work should use GRPMSK(0..3)=0x0; while this setting
-                                                         is not special in CN78XX, for backward and forward compatibility this may enable
-                                                         reallocation of internal resources to the remaining (non-zero-mask) cores. */
+                                                         is not special in SSO, for backward and forward compatibility this may enable reallocation
+                                                         of internal resources to the remaining (non-zero-mask) cores. */
 #else
 	uint64_t grp_msk                      : 64;
 #endif
@@ -4032,7 +4032,7 @@ union cvmx_sso_sl_ppx_pendtag {
 	uint64_t u64;
 	struct cvmx_sso_sl_ppx_pendtag_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t pend_switch                  : 1;  /**< Send when there is a pending SWTAG, SWTAG_DESCHED, or SWTAG_FULL to ORDERED or ATOMIC. If
+	uint64_t pend_switch                  : 1;  /**< Set when there is a pending SWTAG, SWTAG_DESCHED, or SWTAG_FULL to ORDERED or ATOMIC. If
                                                          the register read was issued after an indexed GET_WORK, the DESCHED portion of a
                                                          SWTAG_DESCHED cannot still be pending. */
 	uint64_t pend_get_work                : 1;  /**< Set when there is a pending GET_WORK. */
@@ -4079,7 +4079,7 @@ union cvmx_sso_sl_ppx_pendwqp {
 	uint64_t u64;
 	struct cvmx_sso_sl_ppx_pendwqp_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t pend_switch                  : 1;  /**< Send when there is a pending SWTAG, SWTAG_DESCHED, or SWTAG_FULL to ORDERED or ATOMIC. If
+	uint64_t pend_switch                  : 1;  /**< Set when there is a pending SWTAG, SWTAG_DESCHED, or SWTAG_FULL to ORDERED or ATOMIC. If
                                                          the status load was issued after an indexed GET_WORK, the DESCHED portion of a
                                                          SWTAG_DESCHED cannot still be pending. */
 	uint64_t pend_get_work                : 1;  /**< Set when there is a pending GET_WORK. */
diff --git a/arch/mips/include/asm/octeon/cvmx-sysinfo.h b/arch/mips/include/asm/octeon/cvmx-sysinfo.h
index 13c4c15..45bbb83 100644
--- a/arch/mips/include/asm/octeon/cvmx-sysinfo.h
+++ b/arch/mips/include/asm/octeon/cvmx-sysinfo.h
@@ -42,7 +42,7 @@
  *
  * This module provides system/board information obtained by the bootloader.
  *
- * <hr>$Revision: 83018 $<hr>
+ * <hr>$Revision: 87314 $<hr>
  *
  */
 
@@ -92,7 +92,7 @@ struct cvmx_sysinfo {
 	cvmx_coremask_t core_mask;
 			     /**< coremask defining cores running application */
 	uint32_t init_core;
-			     /**< Deprecated, use cvmx_coremask_first_core() to select init core */
+			     /**< The initial boot core for this application */
 	uint64_t exception_base_addr;
 				       /**< exception base address, as set by bootloader */
 	uint32_t cpu_clock_hz;
@@ -147,6 +147,34 @@ typedef struct cvmx_sysinfo cvmx_sysinfo_t;
 
 extern struct cvmx_sysinfo *cvmx_sysinfo_get(void);
 
+/*
+ * This function determines if the current core is the initial boot core
+ * for the application, which may not necesarily be the numerically lowest
+ * core number in the core mask.
+ * RETURNS 1 if the current core is the initial core for the application,
+ * -1 on error, and 0 otherwise.
+ *
+ * NOTE: Use this function instead of OCTEON_IS_FIRST_CORE() or 
+ * cvmx_coremask_is_first_core() to protect code sections that
+ * need to be executted only on one core.
+ * Also, note that when hotplugging is enabled, the initial core
+ * will not be allowed to be unplugged, unless the entire application is
+ * being shut down.
+ */
+static inline int cvmx_is_init_core(void)
+{
+#ifdef CVMX_BUILD_FOR_TOOLCHAIN
+  extern int __octeon_init_core_p;
+  return __octeon_init_core_p;
+#else
+	struct cvmx_sysinfo * si = cvmx_sysinfo_get();
+	if( si != NULL )
+		return si->init_core == cvmx_get_core_num();
+	else
+		return -1;
+#endif
+}
+
 /**
  * This function adds the current cpu to sysinfo coremask
  *
diff --git a/arch/mips/include/asm/octeon/cvmx-uctlx-defs.h b/arch/mips/include/asm/octeon/cvmx-uctlx-defs.h
index 30a7e0a..a5c5c42 100644
--- a/arch/mips/include/asm/octeon/cvmx-uctlx-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-uctlx-defs.h
@@ -659,13 +659,14 @@ union cvmx_uctlx_ctl {
 	uint64_t ref_clk_sel                  : 2;  /**< Reference clock select. Choose reference clock source for the SuperSpeed and HighSpeed PLL
                                                          blocks.
                                                          0x0 = Reference clock source for both PLLs come from the USB pads.
-                                                         0x1 = Reference clock source for SuperSpeed PLL is from the USB pads, reference clock
-                                                         source for HighSpeed PLL is PLL_REF_CLK.
+                                                         0x1 = Reserved
                                                          0x2 = Reserved.
                                                          0x3 = Reserved.
-                                                         The PLL_REF_CLK is a 50MHz reference clock from an on-chip PLL. This value can be changed
-                                                         only during UPHY_RST.
-                                                         Note: If REF_CLK_SEL = 0x0, then the reference clock input cannot be spread-spectrum. */
+                                                         This value can be changed only during UPHY_RST.
+                                                         Note: If REF_CLK_SEL = 0x0, then the reference clock input cannot be spread-spectrum.
+                                                         INTERNAL: For the 0x1 selection, reference clock source for SuperSpeed PLL is from the USB
+                                                         pads, reference clock source for HighSpeed PLL is PLL_REF_CLK. But in 78xx, PLL_REF_CLK
+                                                         cannot be routed to USB without violating jitter requirements */
 	uint64_t ssc_en                       : 1;  /**< Enables spread-spectrum clock production in the SuperSpeed function.
                                                          If the input reference clock for the SuperSpeed PLL is already spread-spectrum,
                                                          then do not enable this function. The clocks sourced to the SuperSpeed function
@@ -688,73 +689,29 @@ union cvmx_uctlx_ctl {
                                                          corresponds to the frequency-synthesis coefficient.
                                                          [55:53]: modulus - 1,
                                                          [52:47]: 2's complement push amount
-                                                         A value of 0x0 means this feature is disabled. The legal values are as follows:
-                                                         If REF_CLK_SEL = 0x0, then 0x0 is the only legal value.
-                                                         If REF_CLK_SEL = 0x1:
-                                                         0x108 = If the external reference clock is 19.2MHz, 24MHz, 26MHz, 38.4MHz, 48MHz, 52MHz,
-                                                         76.8MHz, 96MHz, 104MHz.
-                                                         0x0 = If the external reference clock is another supported frequency (see list in
-                                                         MPLL_MULTIPLIER description).
-                                                         All other values are reserved. This value may only be changed during UPHY_RST.
-                                                         Note: If REF_CLK_SEL = 0x1, then MPLL_MULTPLIER, REF_CLK_DIV2, and SSC_REF_CLK_SEL must
-                                                         all be programmed to the same frequency setting. */
-	uint64_t mpll_multiplier              : 7;  /**< Multiplies the reference clock to a frequency suitable for intended operating speed. The
-                                                         legal values are:
-                                                         If REF_CLK_SEL = 0x0, then 0x0 is the only legal value.
-                                                         If REF_CLK_SEL = 0x1 then:
-                                                         0x02 = 19.2 MHz on the external reference clock
-                                                         0x7D = 20 MHz on the external reference clock
-                                                         0x68 = 24 MHz on the external reference clock
-                                                         0x64 = 25 MHz on the external reference clock
-                                                         0x60 = 26 MHz on the external reference clock
-                                                         0x41 = 38.4 MHz on the external reference clock
-                                                         0x7D = 40 MHz on the external reference clock
-                                                         0x34 = 48 MHz on the external reference clock
-                                                         0x32 = 50 MHz on the external reference clock
-                                                         0x30 = 52 MHz on the external reference clock
-                                                         0x41 = 76.8 MHz on the external reference clock
-                                                         0x1A = 96 MHz on the external reference clock
-                                                         0x19 = 100 MHz on the external reference clock
-                                                         0x18 = 104 MHz on the external reference clock if REF_CLK_DIV2 is 0x0
-                                                         0x30 = 104 MHz on the external reference clock if REF_CLK_DIV2 is 0x1
-                                                         0x28 = 125 MHz on the external reference clock
-                                                         0x19 = 200 MHz on the external reference clock
-                                                         All other values are reserved.
+                                                         Must leave at reset value of 0x0.
+                                                         This value may only be changed during UPHY_RST. */
+	uint64_t mpll_multiplier              : 7;  /**< Multiplies the reference clock to a frequency suitable for intended operating speed.
+                                                         Must leave at reset value of 0x0.
                                                          This value may only be changed during UPHY_RST.
-                                                         Note: If REF_CLK_SEL = 0x1, then MPLL_MULTPLIER, REF_CLK_DIV2, and SSC_REF_CLK_SEL must
-                                                         all be programmed to the same frequency setting. When REF_CLK_SEL = 0x0, this value is
-                                                         superceded by the REF_CLK_FSEL<5:3> selection. */
+                                                         This value is superceded by the REF_CLK_FSEL<5:3> selection. */
 	uint64_t ref_ssp_en                   : 1;  /**< Enables reference clock to the prescaler for SuperSpeed function. This should always be
                                                          enabled since this output clock is used to drive the UAHC suspend-mode clock during low-
                                                          power states.
                                                          This value can be changed only during UPHY_RST or during low-power states.
                                                          The reference clock must be running and stable before UPHY_RST is deasserted and before
                                                          REF_SSP_EN is asserted. */
-	uint64_t ref_clk_div2                 : 1;  /**< Divides the reference clock by 2 before feeding it into the REF_CLK_FSEL divider. The
-                                                         legal values are:
-                                                         If REF_CLK_SEL = 0x0, then for all reference clock frequencies, 0x0 is the only legal
-                                                         value.
-                                                         If REF_CLK_SEL = 0x1:
-                                                             0x1: if external reference clock is 125MHz, 40MHz, 76.8MHz, or 200MHz.
-                                                             0x0 or 0x1: if external reference clock is 104MHz
-                                                                         (depending on MPLL_MULTIPLIER setting)
-                                                             0x0: if external reference clock is another supported frequency,
-                                                                  (see list in MPLL_MULTIPLIER description).
-                                                         This value can be changed only during UPHY_RST.
-                                                         Note: If REF_CLK_SEL = 0x1, then MPLL_MULTPLIER, REF_CLK_DIV2, and SSC_REF_CLK_SEL must
-                                                         all be programmed to the same frequency setting. */
+	uint64_t ref_clk_div2                 : 1;  /**< Divides the reference clock by 2 before feeding it into the REF_CLK_FSEL divider.
+                                                         Must leave at reset value of 0x0.
+                                                         This value can be changed only during UPHY_RST. */
 	uint64_t ref_clk_fsel                 : 6;  /**< Selects the reference clock frequency for the SuperSpeed and HighSpeed PLL blocks. The
                                                          legal values are as follows:
-                                                         If REF_CLK_SEL = 0x0:
                                                          0x27 = External reference clock 100 MHz
                                                          0x2A = External reference clock 24 MHz
                                                          0x31 = External reference clock 20 MHz
                                                          0x38 = External reference clock 19.2 MHz
-                                                         If REF_CLK_SEL = 0x1, then 0x7 is the only legal value.
                                                          All other values are reserved.
-                                                         This value may only be changed during UPHY_RST.
-                                                         Note: When REF_CLK_SEL = 0x1, then MPLL_MULTPLIER, REF_CLK_DIV2, and
-                                                         SSC_REF_CLK_SEL must all be programmed to the same frequency setting. */
+                                                         This value may only be changed during UPHY_RST. */
 	uint64_t reserved_31_31               : 1;
 	uint64_t h_clk_en                     : 1;  /**< Host-controller-clock enable. When set to 1, the host-controller clock is generated. This
                                                          also enables access to UCTL registers 0x30-0xF8. */
diff --git a/arch/mips/include/asm/octeon/octeon-boot-info.h b/arch/mips/include/asm/octeon/octeon-boot-info.h
index db7bc29..9676b81 100644
--- a/arch/mips/include/asm/octeon/octeon-boot-info.h
+++ b/arch/mips/include/asm/octeon/octeon-boot-info.h
@@ -51,8 +51,10 @@
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <linux/types.h>
 #include <asm/octeon/cvmx-asm.h>
+#elif defined(__U_BOOT__)
+# include <asm/arch/cvmx-asm.h>
 #else
-#include "cvmx-asm.h"
+# include "cvmx-asm.h"
 #endif
 
 #define OCTEON_BOOT_MOVEABLE_MAGIC	0xDB00110ad358eace
diff --git a/arch/mips/include/asm/octeon/octeon-feature.h b/arch/mips/include/asm/octeon/octeon-feature.h
index 7044384..59d093e 100644
--- a/arch/mips/include/asm/octeon/octeon-feature.h
+++ b/arch/mips/include/asm/octeon/octeon-feature.h
@@ -67,12 +67,12 @@ typedef enum {
  * runtime. It provides a unified way to answer yes-or-no quetions.
  */
 typedef enum {
-	OCTEON_ATTR_FIRST_CORE,	/* first core in the boot coremask */
+	OCTEON_ATTR_INIT_CORE,	/* initial core to run the app when booted */
 	OCTEON_ATTR_TRACED,	/* the core is traced */
 	OCTEON_ATTR_NO_IOCFG,	/* skip IO config in cvmx_user_app_init() */
 	OCTEON_ATTR_MAX
 } octeon_attr_t;
-#define OCTEON_IS_FIRST_CORE()	octeon_has_attr(OCTEON_ATTR_FIRST_CORE)
+#define OCTEON_IS_INIT_CORE()	octeon_has_attr(OCTEON_ATTR_INIT_CORE)
 #define OCTEON_IS_TRACED()	octeon_has_attr(OCTEON_ATTR_TRACED)
 #define OCTEON_IS_NO_IOCFG()	octeon_has_attr(OCTEON_ATTR_NO_IOCFG)
 
-- 
2.6.2

