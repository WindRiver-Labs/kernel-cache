From b61758fd6acfe0df6de42b5db2f110f609e92e69 Mon Sep 17 00:00:00 2001
From: Leonid Rosenboim <lrosenboim@caviumnetworks.com>
Date: Thu, 3 Jul 2014 18:05:50 -0700
Subject: [PATCH 662/974] MIPS: Octeon: Sync SE files, update ethernet driver
 to match

SE FPA module major cleanup, seapration of generic vs. native
API layers, removal of unused functions, change of confusing
function names.
Update of Octeon and Octeon3 ethernet drivers to use native FPA
API layer for the appropriate chip family.

Signed-off-by: Leonid Rosenboim <lrosenboim@caviumnetworks.com>
[Original patch taken from Cavium SDK 3.1.2-568]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
---
 .../executive/cvmx-appcfg-transport.c              |    4 +-
 arch/mips/cavium-octeon/executive/cvmx-bch.c       |   59 +-
 arch/mips/cavium-octeon/executive/cvmx-bootmem.c   |    2 +-
 arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c |   16 +-
 .../mips/cavium-octeon/executive/cvmx-dma-engine.c |    4 +-
 .../cavium-octeon/executive/cvmx-fpa-resource.c    |  330 +--
 .../executive/cvmx-global-resources.c              |   58 +-
 .../mips/cavium-octeon/executive/cvmx-helper-bgx.c |   72 +-
 .../mips/cavium-octeon/executive/cvmx-helper-cfg.c |  156 +-
 .../mips/cavium-octeon/executive/cvmx-helper-ilk.c |   24 +-
 .../mips/cavium-octeon/executive/cvmx-helper-ipd.c |    5 +-
 .../mips/cavium-octeon/executive/cvmx-helper-pki.c | 2404 ++++++++++++--------
 .../mips/cavium-octeon/executive/cvmx-helper-pko.c |   16 +-
 .../cavium-octeon/executive/cvmx-helper-pko3.c     |   92 +-
 .../cavium-octeon/executive/cvmx-helper-sgmii.c    |   23 +-
 .../cavium-octeon/executive/cvmx-helper-util.c     |  145 +-
 arch/mips/cavium-octeon/executive/cvmx-helper.c    |   24 +-
 arch/mips/cavium-octeon/executive/cvmx-pcie.c      |   74 +-
 .../cavium-octeon/executive/cvmx-pki-cluster.h     |  636 ------
 arch/mips/cavium-octeon/executive/cvmx-pki.c       |  746 +++---
 arch/mips/cavium-octeon/executive/cvmx-pko.c       |    2 -
 .../mips/cavium-octeon/executive/cvmx-pko3-queue.c |   36 +-
 arch/mips/cavium-octeon/executive/cvmx-pko3.c      |   16 +-
 arch/mips/cavium-octeon/executive/cvmx-qlm.c       |  263 ++-
 arch/mips/cavium-octeon/executive/cvmx-range.c     |   98 +-
 arch/mips/cavium-octeon/executive/octeon-feature.c |    4 +-
 arch/mips/cavium-octeon/executive/octeon-model.c   |    7 +-
 arch/mips/include/asm/octeon/cvmx-app-init.h       |    4 +-
 arch/mips/include/asm/octeon/cvmx-bch.h            |    2 +-
 arch/mips/include/asm/octeon/cvmx-clock.h          |    2 +-
 arch/mips/include/asm/octeon/cvmx-cmd-queue.h      |   17 +-
 arch/mips/include/asm/octeon/cvmx-coremask.h       |    6 +-
 arch/mips/include/asm/octeon/cvmx-dma-engine.h     |    6 +-
 arch/mips/include/asm/octeon/cvmx-fpa.h            |  301 +++
 arch/mips/include/asm/octeon/cvmx-fpa1.h           |  185 +-
 arch/mips/include/asm/octeon/cvmx-fpa3.h           |  608 +++--
 .../include/asm/octeon/cvmx-global-resources.h     |    5 +
 arch/mips/include/asm/octeon/cvmx-helper-bgx.h     |   20 +
 arch/mips/include/asm/octeon/cvmx-helper-cfg.h     |   28 +-
 arch/mips/include/asm/octeon/cvmx-helper-fpa.h     |   68 +-
 arch/mips/include/asm/octeon/cvmx-helper-pki.h     |  234 +-
 arch/mips/include/asm/octeon/cvmx-helper-util.h    |   87 +-
 arch/mips/include/asm/octeon/cvmx-helper.h         |    5 +-
 arch/mips/include/asm/octeon/cvmx-hwpko.h          |    4 +-
 arch/mips/include/asm/octeon/cvmx-ilk.h            |    2 +-
 arch/mips/include/asm/octeon/cvmx-pcie.h           |    4 +-
 arch/mips/include/asm/octeon/cvmx-pip.h            |  380 +++-
 arch/mips/include/asm/octeon/cvmx-pki-cluster.h    |   32 +-
 arch/mips/include/asm/octeon/cvmx-pki-resources.h  |   15 +-
 arch/mips/include/asm/octeon/cvmx-pki.h            |  431 ++--
 arch/mips/include/asm/octeon/cvmx-pko3.h           |   36 +-
 arch/mips/include/asm/octeon/cvmx-pow.h            |  219 +-
 arch/mips/include/asm/octeon/cvmx-qlm.h            |    4 +-
 arch/mips/include/asm/octeon/cvmx-range.h          |    3 +
 arch/mips/include/asm/octeon/cvmx-wqe.h            |  108 +-
 arch/mips/include/asm/octeon/cvmx.h                |    1 +
 arch/mips/include/asm/octeon/octeon-boot-info.h    |    4 +-
 arch/mips/include/asm/octeon/octeon-feature.h      |   53 +-
 arch/mips/include/asm/octeon/octeon-model.h        |   15 +-
 drivers/net/ethernet/octeon/ethernet-mem.c         |   10 +-
 drivers/net/ethernet/octeon/ethernet.c             |    8 +
 drivers/net/ethernet/octeon/octeon3-ethernet.c     |  105 +-
 62 files changed, 5049 insertions(+), 3279 deletions(-)
 delete mode 100644 arch/mips/cavium-octeon/executive/cvmx-pki-cluster.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-fpa.h

diff --git a/arch/mips/cavium-octeon/executive/cvmx-appcfg-transport.c b/arch/mips/cavium-octeon/executive/cvmx-appcfg-transport.c
index cb8c875..3b14fc6 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-appcfg-transport.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-appcfg-transport.c
@@ -41,7 +41,7 @@
 #include <linux/export.h>
 #include <asm/octeon/cvmx.h>
 #include <asm/octeon/cvmx-bootmem.h>
-#include <asm/octeon/cvmx-fpa1.h>
+#include <asm/octeon/cvmx-fpa.h>
 #include <asm/octeon/cvmx-ipd.h>
 #include <asm/octeon/cvmx-helper.h>
 #include <asm/octeon/cvmx-helper-cfg.h>
@@ -50,7 +50,7 @@
 #else
 #include "cvmx.h"
 #include "cvmx-bootmem.h"
-#include "cvmx-fpa1.h"
+#include "cvmx-fpa.h"
 #include "cvmx-ipd.h"
 #include "cvmx-helper.h"
 #include "cvmx-helper-cfg.h"
diff --git a/arch/mips/cavium-octeon/executive/cvmx-bch.c b/arch/mips/cavium-octeon/executive/cvmx-bch.c
index c90a78d..06598d6 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-bch.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-bch.c
@@ -20,7 +20,7 @@
  *     derived from this software without specific prior written
  *     permission.
  *
- * This Software, including technical data, may be subject to U.S. export  control
+ * This Software, including technical data, may be subject to U.S. export control
  * laws, including the U.S. Export Administration Act and its  associated
  * regulations, and may be subject to export or import  regulations in other
  * countries.
@@ -28,8 +28,8 @@
  * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
  * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
  * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
- * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
- * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION
+ * OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
  * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
  * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
  * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
@@ -45,15 +45,20 @@
  * <hr>$Revision: 79788 $<hr>
  */
 
+
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 # include <asm/octeon/cvmx.h>
 # include <asm/octeon/cvmx-config.h>
 # include <asm/octeon/cvmx-bch-defs.h>
 # include <asm/octeon/cvmx-bch.h>
-# include <asm/octeon/cvmx-fpa1.h>
+# include <asm/octeon/cvmx-fpa.h>
 # include <asm/octeon/cvmx-helper-fpa.h>
 # include <asm/octeon/cvmx-cmd-queue.h>
+#include <linux/kernel.h>
+#include <linux/slab.h>
+
 #elif defined(CVMX_BUILD_FOR_UBOOT)
+
 # include <common.h>
 # include <asm/arch/cvmx.h>
 # include <asm/arch/cvmx-bch-defs.h>
@@ -84,7 +89,6 @@ CVMX_SHARED cvmx_bch_app_config_t bch_config = {
 };
 
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
-extern int cvm_oct_mem_fill_fpa(int pool, int elements);
 extern int cvm_oct_alloc_fpa_pool(int pool, int size);
 #endif
 
@@ -103,6 +107,7 @@ int cvmx_bch_initialize(void)
 
 	/* Initialize FPA pool for BCH pool buffers */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+	int i;
 	bch_pool = CVMX_FPA_OUTPUT_BUFFER_POOL;
 	bch_pool_size = CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE;
 
@@ -112,10 +117,13 @@ int cvmx_bch_initialize(void)
 
 	bch_pool = cvm_oct_alloc_fpa_pool(bch_pool, bch_pool_size);
 	if (bch_pool < 0) {
-		pr_err("cvm_oct_alloc_fpa_pool(%d, %lld)\n", bch_pool, bch_pool_size);
+		pr_err("cvm_oct_alloc_fpa_pool(%d, %lld)\n",
+		       bch_pool, bch_pool_size);
 		return -ENOMEM;
 	}
-	cvm_oct_mem_fill_fpa(bch_pool, 128);
+
+	for (i = 0; i < 16; i++)
+		cvmx_fpa1_free(kmalloc(bch_pool_size, GFP_KERNEL), bch_pool, 0);
 #else
 	bch_pool = (int)cvmx_fpa_get_bch_pool();
 	bch_pool_size = cvmx_fpa_get_bch_pool_block_size();
@@ -147,8 +155,8 @@ int cvmx_bch_initialize(void)
 	bch_cmd_buf.s.dwb = bch_pool_size / 128;
 	bch_cmd_buf.s.pool = bch_pool;
 	bch_cmd_buf.s.size = bch_pool_size / 8;
-	bch_cmd_buf.s.ptr =
-		cvmx_ptr_to_phys(cvmx_cmd_queue_buffer(CVMX_CMD_QUEUE_BCH)) >> 7;
+	bch_cmd_buf.s.ptr = cvmx_ptr_to_phys(
+		cvmx_cmd_queue_buffer(CVMX_CMD_QUEUE_BCH)) >> 7;
 	cvmx_write_csr(CVMX_BCH_CMD_BUF, bch_cmd_buf.u64);
 	cvmx_write_csr(CVMX_BCH_GEN_INT, 7);
 	cvmx_write_csr(CVMX_BCH_GEN_INT_EN, 0);
@@ -170,6 +178,7 @@ EXPORT_SYMBOL(cvmx_bch_initialize);
 int cvmx_bch_shutdown(void)
 {
 	cvmx_bch_ctl_t bch_ctl;
+	int bch_pool;
 
 	debug("%s: ENTER\n", __func__);
 	bch_ctl.u64 = cvmx_read_csr(CVMX_BCH_CTL);
@@ -177,8 +186,24 @@ int cvmx_bch_shutdown(void)
 	cvmx_write_csr(CVMX_BCH_CTL, bch_ctl.u64);
 	cvmx_wait(4);
 
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+	bch_pool = CVMX_FPA_OUTPUT_BUFFER_POOL;
+#else
+	bch_pool = (int)cvmx_fpa_get_bch_pool();
+#endif
 	cvmx_cmd_queue_shutdown(CVMX_CMD_QUEUE_BCH);
 
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+	/* FIXME: BCH cleanup in SE : AJ */
+	{
+		int i;
+		for (i = 0; i < 16; i++)
+			kfree(cvmx_fpa1_alloc(bch_pool));
+	}
+#else
+	cvmx_fpa_shutdown_pool(bch_pool);
+#endif
+	/* AJ: Fix for FPA3 */
 	return 0;
 }
 EXPORT_SYMBOL(cvmx_bch_shutdown);
@@ -189,7 +214,7 @@ EXPORT_SYMBOL(cvmx_bch_shutdown);
  * @param buffer_size	buffer size of pool
  * @param buffer_count	number of buffers to allocate to pool
  */
-void cvmx_bch_set_cmd_que_pool_config (int64_t pool, uint64_t buffer_size,
+void cvmx_bch_set_cmd_que_pool_config(int64_t pool, uint64_t buffer_size,
 				       uint64_t buffer_count)
 {
 	bch_config.command_queue_pool.pool_num = pool;
@@ -212,8 +237,8 @@ void cvmx_bch_get_cmd_que_pool_config(cvmx_fpa_pool_config_t *bch_pool)
  * @param[in] block	8-byte aligned pointer to data block to calculate ECC
  * @param block_size	Size of block in bytes, must be a multiple of two.
  * @param ecc_level	Number of errors that must be corrected.  The number of
- * 			parity bytes is equal to ((15 * ecc_level) + 7) / 8.
- * 			Must be 4, 8, 16, 24, 32, 40, 48, 56, 60 or 64.
+ *			parity bytes is equal to ((15 * ecc_level) + 7) / 8.
+ *			Must be 4, 8, 16, 24, 32, 40, 48, 56, 60 or 64.
  * @param[out] ecc	8-byte aligned pointer to where ecc data should go
  * @param[in] response	pointer to where responses will be written.
  *
@@ -237,7 +262,7 @@ int cvmx_bch_encode(const void *block, uint16_t block_size,
 	command.s.oword.ptr = cvmx_ptr_to_phys(ecc);
 	command.s.iword.ptr = cvmx_ptr_to_phys((void *)block);
 	command.s.resp.ptr = cvmx_ptr_to_phys((void *)response);
-	debug("Command: cword: 0x%llx, oword: 0x%llx, iword: 0x%llx, resp: 0x%llx\n",
+	debug("Cmd: cword:0x%llx, oword:0x%llx, iword:0x%llx, resp:0x%llx\n",
 	      command.u64[0], command.u64[1], command.u64[2], command.u64[3]);
 	result = cvmx_cmd_queue_write(CVMX_CMD_QUEUE_BCH, 1,
 				      sizeof(command) / sizeof(uint64_t),
@@ -263,9 +288,9 @@ EXPORT_SYMBOL(cvmx_bch_encode);
  * @param ecc_level		Number of errors that must be corrected.  The
  *				number of parity bytes is equal to
  *				((15 * ecc_level) + 7) / 8.
- * 				Must be 4, 8, 16, 24, 32, 40, 48, 56, 60 or 64.
+ *				Must be 4, 8, 16, 24, 32, 40, 48, 56, 60 or 64.
  * @param[out] block_out	8-byte aligned pointer to corrected data buffer.
- * 				This should not be the same as block_ecc_in.
+ *				This should not be the same as block_ecc_in.
  * @param[in] response		pointer to where responses will be written.
  *
  * @return Zero on success, negative on failure.
@@ -286,9 +311,9 @@ int cvmx_bch_decode(const void *block_ecc_in, uint16_t block_size,
 	command.s.cword.size = block_size;
 
 	command.s.oword.ptr = cvmx_ptr_to_phys((void *)block_out);
- 	command.s.iword.ptr = cvmx_ptr_to_phys((void *)block_ecc_in);
+	command.s.iword.ptr = cvmx_ptr_to_phys((void *)block_ecc_in);
 	command.s.resp.ptr = cvmx_ptr_to_phys((void *)response);
-	debug("Command: cword: 0x%llx, oword: 0x%llx, iword: 0x%llx, resp: 0x%llx\n",
+	debug("Cmd: cword:0x%llx, oword:0x%llx, iword:0x%llx, resp:0x%llx\n",
 	      command.u64[0], command.u64[1], command.u64[2], command.u64[3]);
 	result = cvmx_cmd_queue_write(CVMX_CMD_QUEUE_BCH, 1,
 				      sizeof(command) / sizeof(uint64_t),
diff --git a/arch/mips/cavium-octeon/executive/cvmx-bootmem.c b/arch/mips/cavium-octeon/executive/cvmx-bootmem.c
index 3a5de94..5d2c94a 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-bootmem.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-bootmem.c
@@ -43,7 +43,7 @@
  * Simple allocate only memory allocator.  Used to allocate memory at
  * application start time.
  *
- * <hr>$Revision: 102155 $<hr>
+ * <hr>$Revision: 103461 $<hr>
  *
  */
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c b/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c
index 3c09cbe..c213b22 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c
@@ -43,7 +43,7 @@
  * Support functions for managing command queues used for
  * various hardware blocks.
  *
- * <hr>$Revision: 96722 $<hr>
+ * <hr>$Revision: 103822 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <linux/export.h>
@@ -53,12 +53,12 @@
 #include <asm/octeon/cvmx-pexp-defs.h>
 #include <asm/octeon/cvmx-dpi-defs.h>
 #include <asm/octeon/cvmx-pko-defs.h>
-#include <asm/octeon/cvmx-fpa1.h>
+#include <asm/octeon/cvmx-fpa.h>
 #include <asm/octeon/cvmx-cmd-queue.h>
 #else
 #include "cvmx.h"
 #include "cvmx-bootmem.h"
-#include "cvmx-fpa1.h"
+#include "cvmx-fpa.h"
 #include "cvmx-cmd-queue.h"
 #endif
 
@@ -147,7 +147,7 @@ cvmx_cmd_queue_result_t cvmx_cmd_queue_initialize(cvmx_cmd_queue_id_t queue_id,
 	} else if (max_depth != 0)
 		return CVMX_CMD_QUEUE_INVALID_PARAM;
 
-	if ((fpa_pool < 0) || (fpa_pool > 7))
+	if ((fpa_pool < 0) || (fpa_pool >= CVMX_FPA_NUM_POOLS))
 		return CVMX_CMD_QUEUE_INVALID_PARAM;
 	if ((pool_size < 128) || (pool_size > 65536))
 		return CVMX_CMD_QUEUE_INVALID_PARAM;
@@ -229,11 +229,8 @@ cvmx_cmd_queue_result_t cvmx_cmd_queue_shutdown(cvmx_cmd_queue_id_t queue_id)
 
 	__cvmx_cmd_queue_lock(queue_id, qptr);
 	if (qptr->base_ptr_div128) {
-		if (octeon_has_feature(OCTEON_FEATURE_FPA3))
-			cvmx_fpa3_free_aura(cvmx_phys_to_ptr((uint64_t) qptr->base_ptr_div128 << 7),
-					   0, qptr->fpa_pool, 0);
-		else
-			cvmx_fpa1_free(cvmx_phys_to_ptr((uint64_t) qptr->base_ptr_div128 << 7),
+		cvmx_fpa_free(cvmx_phys_to_ptr(
+				       (uint64_t) qptr->base_ptr_div128 << 7),
 				       qptr->fpa_pool, 0);
 		qptr->base_ptr_div128 = 0;
 	}
@@ -285,6 +282,7 @@ int cvmx_cmd_queue_length(cvmx_cmd_queue_id_t queue_id)
 		}
 	case CVMX_CMD_QUEUE_ZIP:
 	case CVMX_CMD_QUEUE_DFA:
+	case CVMX_CMD_QUEUE_HNA:
 	case CVMX_CMD_QUEUE_RAID:
 		/* FIXME: Implement other lengths */
 		return 0;
diff --git a/arch/mips/cavium-octeon/executive/cvmx-dma-engine.c b/arch/mips/cavium-octeon/executive/cvmx-dma-engine.c
index e65035c..dfcc12a 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-dma-engine.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-dma-engine.c
@@ -43,11 +43,12 @@
  * Interface to the PCI / PCIe DMA engines. These are only avialable
  * on chips with PCI / PCIe.
  *
- * <hr>$Revision: 100051 $<hr>
+ * <hr>$Revision: 103836 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <linux/export.h>
 #include <asm/octeon/cvmx.h>
+#include <asm/octeon/cvmx-fpa.h>
 #include <asm/octeon/octeon-model.h>
 #include <asm/octeon/cvmx-cmd-queue.h>
 #include <asm/octeon/cvmx-dma-engine.h>
@@ -57,6 +58,7 @@
 #include <asm/octeon/cvmx-dpi-defs.h>
 #include <asm/octeon/cvmx-pexp-defs.h>
 #include <asm/octeon/cvmx-helper-cfg.h>
+#include <asm/octeon/cvmx-helper-fpa.h>
 #else
 #include "cvmx.h"
 #include "cvmx-bootmem.h"
diff --git a/arch/mips/cavium-octeon/executive/cvmx-fpa-resource.c b/arch/mips/cavium-octeon/executive/cvmx-fpa-resource.c
index 4e9c92f..5fc6787 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-fpa-resource.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-fpa-resource.c
@@ -40,204 +40,266 @@
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include "linux/export.h"
 #include "asm/octeon/cvmx.h"
-#include "asm/octeon/cvmx-fpa3.h"
-#include "asm/octeon/cvmx-fpa1.h"
+#include "asm/octeon/cvmx-fpa.h"
 #include "asm/octeon/cvmx-global-resources.h"
 #else
 #include "cvmx.h"
-#include "cvmx-fpa3.h"
+#include "cvmx-fpa.h"
 #include "cvmx-global-resources.h"
 #include "cvmx-sysinfo.h"
 #endif
 
-
-static struct global_resource_tag get_fpa_resource_tag(int node)
+static struct global_resource_tag
+get_fpa1_resource_tag(void)
 {
-	switch(node) {
-	case 0:
 		return CVMX_GR_TAG_FPA;
-	case 1:
-		return cvmx_get_gr_tag('c','v','m','_','f','p','a','_','0','1','.','.','.','.','.','.');
-	case 2:
-		return cvmx_get_gr_tag('c','v','m','_','f','p','a','_','0','2','.','.','.','.','.','.');
-	case 3:
-		return cvmx_get_gr_tag('c','v','m','_','f','p','a','_','0','3','.','.','.','.','.','.');
-	default:
-		/* Add a panic?? */
-		return cvmx_get_gr_tag('i','n','v','a','l','i','d','.','.','.','.','.','.','.','.','.');
-	}
+}
+
+static struct global_resource_tag
+get_fpa3_aura_resource_tag(int node)
+{
+	return cvmx_get_gr_tag('c', 'v', 'm', '_', 'a', 'u', 'r', 'a', '_',
+		node+'0', '.', '.', '.', '.', '.', '.');
 }
 
 
-static struct global_resource_tag get_aura_resource_tag(int node)
+static struct global_resource_tag
+get_fpa3_pool_resource_tag(int node)
 {
-	switch(node) {
-	case 0:
-		return cvmx_get_gr_tag('c','v','m','_','a','u','r','a','_','0','_','0','.','.','.','.');
-	case 1:
-		return cvmx_get_gr_tag('c','v','m','_','a','u','r','a','_','0','_','1','.','.','.','.');
-	case 2:
-		return cvmx_get_gr_tag('c','v','m','_','a','u','r','a','_','0','_','2','.','.','.','.');
-	case 3:
-		return cvmx_get_gr_tag('c','v','m','_','a','u','r','a','_','0','_','3','.','.','.','.');
-	default:
-		/* Add a panic?? */
-		return cvmx_get_gr_tag('i','n','v','a','l','i','d','.','.','.','.','.','.','.','.','.');
-	}
+	return cvmx_get_gr_tag('c', 'v', 'm', '_', 'p', 'o', 'o', 'l', '_',
+		node+'0', '.', '.', '.', '.', '.', '.');
+}
+
+int cvmx_fpa_get_max_pools(void)
+{
+	if (octeon_has_feature(OCTEON_FEATURE_FPA3))
+		return CVMX_FPA3_NUM_AURAS;
+	else if (OCTEON_IS_MODEL(OCTEON_CN68XX))
+		return  CVMX_FPA1_NUM_POOLS;
+	else
+		return  CVMX_FPA1_NUM_POOLS-1;
+}
+
+uint64_t cvmx_fpa3_get_aura_owner(cvmx_fpa3_gaura_t aura)
+{
+	return cvmx_get_global_resource_owner(
+		get_fpa3_aura_resource_tag(aura.node),
+		aura.laura);
+}
+
+uint64_t cvmx_fpa1_get_pool_owner(cvmx_fpa1_pool_t pool)
+{
+	return cvmx_get_global_resource_owner(
+		get_fpa1_resource_tag(), pool);
+}
+
+uint64_t cvmx_fpa_get_pool_owner(int pool_num)
+{
+	if (octeon_has_feature(OCTEON_FEATURE_FPA3))
+		return cvmx_fpa3_get_aura_owner(
+			cvmx_fpa1_pool_to_fpa3_aura(pool_num));
+	else
+		return cvmx_fpa1_get_pool_owner(pool_num);
 }
 
 /**
- * This will allocate/reserve count number of FPA pools on the specified node to the
- * calling application. These pools will be for exclusive use of the application
- * until they are freed.
- * @param pools_allocated is an array of length count allocated by the application
- * before invoking the cvmx_allocate_fpa_pool call. On return it will contain the
- * index numbers of the pools allocated.
- * If -1 it finds the empty pool to allocate otherwise it reserves the specified pool.
- * @return 0 on success and -1 on failure.
  */
-int cvmx_fpa_allocate_fpa_pools(int node, int pools_allocated[], int count)
+cvmx_fpa3_gaura_t
+cvmx_fpa3_reserve_aura(int node, int desired_aura_num)
 {
-	int num_pools = CVMX_FPA_NUM_POOLS;
-	uint64_t owner = 0;
+	uint64_t owner = cvmx_get_app_id();
 	int rv = 0;
 	struct global_resource_tag tag;
+	cvmx_fpa3_gaura_t aura;
 
-	if (node == -1) node = cvmx_get_node_num();
+	if (node == -1)
+		node = cvmx_get_node_num();
 
-	tag = get_fpa_resource_tag(node);
+	tag = get_fpa3_aura_resource_tag(node);
 
-	if (octeon_has_feature(OCTEON_FEATURE_FPA3))
-		num_pools = CVMX_FPA3_NUM_POOLS;
-
-	if (cvmx_create_global_resource_range(tag, num_pools) != 0) {
-		cvmx_dprintf("ERROR: failed to create FPA global resource for"
-			     " node=%d\n", node);
-		return -1;
+	if (cvmx_create_global_resource_range(tag, CVMX_FPA3_NUM_AURAS) != 0) {
+		cvmx_printf("ERROR: %s: global resource create node=%u\n",
+			__func__, node);
+		return CVMX_FPA3_INVALID_GAURA;
 	}
 
-	if (pools_allocated[0] >= 0) {
-		while (count--) {
-			rv = cvmx_reserve_global_resource_range(tag, owner, pools_allocated[count], 1);
-			if (rv < 0)
-				return CVMX_RESOURCE_ALREADY_RESERVED;
-		}
+	if (desired_aura_num >= 0)
+		rv = cvmx_reserve_global_resource_range(
+			tag, owner, desired_aura_num, 1);
+	else
+		rv = cvmx_resource_alloc_reverse(tag, owner);
 
-	} else {
-		rv = cvmx_resource_alloc_many(tag, owner,
-					      count,
-					      pools_allocated);
+	if (rv < 0) {
+		cvmx_printf("ERROR: %s: node=%u desired aura=%d\n",
+			__func__, node, desired_aura_num);
+		return CVMX_FPA3_INVALID_GAURA;
 	}
-	return rv;
+
+	aura = __cvmx_fpa3_gaura(node, rv);
+
+	return aura;
 }
 
-/** Allocates the pool from global resources and reserves them
-  * @param pool	    FPA pool to allocate/reserve. If -1 it
-  *                 finds the empty pool to allocate.
-  * @return         Allocated pool number OR -1 if fails to allocate
-                    the pool
-  */
-int cvmx_fpa_alloc_pool(int pool)
+int cvmx_fpa3_release_aura(cvmx_fpa3_gaura_t aura)
 {
-	int pool_num = pool;
+	struct global_resource_tag tag = get_fpa3_aura_resource_tag(aura.node);
+	int laura = aura.laura;
 
-	if (cvmx_fpa_allocate_fpa_pools(-1, &pool_num, 1) < 0) {
+	if (!__cvmx_fpa3_aura_valid(aura))
 		return -1;
-	}
 
-	return pool_num;
+	return
+		cvmx_free_global_resource_range_multiple(tag, &laura, 1);
 }
-EXPORT_SYMBOL(cvmx_fpa_alloc_pool);
 
-int cvmx_fpa_reserve_error_pool(int node)
+/**
+ */
+cvmx_fpa3_pool_t
+cvmx_fpa3_reserve_pool(int node, int desired_pool_num)
 {
-	int pool_num = 0;
-
-	if (!octeon_has_feature(OCTEON_FEATURE_FPA3))
-		return 0;
+	uint64_t owner = cvmx_get_app_id();
+	int rv = 0;
+	struct global_resource_tag tag;
+	cvmx_fpa3_pool_t pool;
 
 	if (node == -1) node = cvmx_get_node_num();
 
-	if (cvmx_sysinfo_get()->init_core != cvmx_get_core_num())
-		return 0;
-	cvmx_fpa_allocate_fpa_pools(node, &pool_num, 1);
+	tag = get_fpa3_pool_resource_tag(node);
+
+	if (cvmx_create_global_resource_range(tag, CVMX_FPA3_NUM_POOLX) != 0) {
+		cvmx_printf("ERROR: %s: global resource create node=%u\n",
+			__func__, node);
+		return CVMX_FPA3_INVALID_POOL;
+	}
 
-	return 0;
+	if (desired_pool_num >= 0)
+		rv = cvmx_reserve_global_resource_range(
+			tag, owner, desired_pool_num, 1);
+	else
+		rv = cvmx_resource_alloc_reverse(tag, owner);
+
+	if (rv < 0) {
+		cvmx_printf("ERROR: %s: node=%u desired_pool=%d\n",
+			__func__, node, desired_pool_num);
+		return CVMX_FPA3_INVALID_POOL;
+	}
+
+	pool = __cvmx_fpa3_pool(node, rv);
+
+	return pool;
 }
 
-/** Release/Frees the specified pool
-  * @param pool	    Pool to free
-  * @return         0 for success -1 failure
-  */
-int cvmx_fpa_release_pool(int pool) /* AJ: Gotta fix this */
+int cvmx_fpa3_release_pool(cvmx_fpa3_pool_t pool)
 {
-	if (cvmx_free_global_resource_range_with_base(CVMX_GR_TAG_FPA, pool, 1) == -1) {
-		cvmx_dprintf("\nERROR Failed to release FPA pool %d", (int)pool);
+	struct global_resource_tag tag = get_fpa3_pool_resource_tag(pool.node);
+	int lpool = pool.lpool;
+
+	if (!__cvmx_fpa3_pool_valid(pool))
 		return -1;
-	}
-	return 0;
+
+	return
+		cvmx_free_global_resource_range_multiple(tag, &lpool, 1);
 }
-EXPORT_SYMBOL(cvmx_fpa_release_pool);
 
-int cvmx_fpa3_allocate_auras(int node, int auras_allocated[], int count)
+cvmx_fpa1_pool_t
+cvmx_fpa1_reserve_pool(int desired_pool_num)
 {
-	int num_aura = CVMX_FPA3_AURA_NUM;
-	uint64_t owner = 0;
-	int rv = 0;
-	struct global_resource_tag tag = get_aura_resource_tag(node);
+	uint64_t owner = cvmx_get_app_id();
+	struct global_resource_tag tag;
+	int rv;
+
+	tag = get_fpa1_resource_tag();
 
-	if (!OCTEON_IS_MODEL(OCTEON_CN78XX)) {
-		cvmx_dprintf("ERROR:  Aura allocation not supported on this model\n");
+	if (cvmx_create_global_resource_range(tag, CVMX_FPA1_NUM_POOLS) != 0) {
+		cvmx_printf("ERROR: %s: global resource not created\n",
+			__func__);
 		return -1;
 	}
 
-	if (cvmx_create_global_resource_range(tag, num_aura) != 0) {
-		cvmx_dprintf("ERROR: failed to create aura global resource for node=%d\n", node);
-		return -1;
+	if (desired_pool_num >= 0) {
+		rv = cvmx_reserve_global_resource_range(
+			tag, owner, desired_pool_num, 1);
+	} else {
+		rv = cvmx_resource_alloc_reverse(tag, owner);
 	}
-	if (auras_allocated[0] >= 0) {
-		while (count--) {
-			rv = cvmx_reserve_global_resource_range(tag, owner, auras_allocated[count], 1);
-			if (rv < 0)
-				return CVMX_RESOURCE_ALREADY_RESERVED;
-		}
-	} else
-		rv = cvmx_resource_alloc_many(tag, owner, count,
-					auras_allocated);
-	return rv;
 
+	if (rv <  0) {
+		cvmx_printf("ERROR: %s: FPA_POOL %d unavailable\n",
+			__func__, desired_pool_num);
+		return CVMX_RESOURCE_ALREADY_RESERVED;
+	}
+	return (cvmx_fpa1_pool_t) rv;
 }
-EXPORT_SYMBOL(cvmx_fpa3_allocate_auras);
 
-int cvmx_fpa3_allocate_aura(int node)
+int cvmx_fpa1_release_pool(cvmx_fpa1_pool_t pool)
 {
-	int r;
-	int aura = -1;
+	struct global_resource_tag tag;
 
-	r = cvmx_fpa3_allocate_auras(node, &aura, 1);
+	tag = get_fpa1_resource_tag();
 
-	return r == 0 ? aura : -1;
+	return
+		cvmx_free_global_resource_range_multiple(tag, &pool, 1);
 }
-EXPORT_SYMBOL(cvmx_fpa3_allocate_aura);
 
-int cvmx_fpa3_free_auras(int node, int *auras_allocated, int count)
+/* 
+ * FIXME:
+ * An easier way to acheive the same would be to
+ * query the block size of a "pool"
+ */
+int cvmx_fpa1_is_pool_available(cvmx_fpa1_pool_t pool)
 {
-	int rv;
-	struct global_resource_tag tag = get_aura_resource_tag(node);
+	if (cvmx_fpa1_reserve_pool(pool) == -1)
+		return 0;
+	cvmx_fpa1_release_pool(pool);
+	return 1;
+}
+
+int cvmx_fpa3_is_pool_available(int node, int lpool)
+{
+	cvmx_fpa3_pool_t pool;
+	if (lpool < 0)
+		return 1;
 
-	rv = cvmx_free_global_resource_range_multiple(tag, auras_allocated,
-						      count);
-	return rv;
+	pool = cvmx_fpa3_reserve_pool(node, lpool);
+
+	if (!__cvmx_fpa3_pool_valid(pool))
+		return 0;
+
+	cvmx_fpa3_release_pool(pool);
+	return 1;
 }
-EXPORT_SYMBOL(cvmx_fpa3_free_auras);
 
-int cvmx_fpa3_free_pools(int node, int *pools_allocated, int count)
+int cvmx_fpa3_is_aura_available(int node, int laura)
 {
-	int rv;
-	struct global_resource_tag tag = get_fpa_resource_tag(node);
+	cvmx_fpa3_gaura_t aura;
+
+	if (laura < 0)
+		return 1;
+
+	aura = cvmx_fpa3_reserve_aura(node, laura);
 
-	rv = cvmx_free_global_resource_range_multiple(tag, pools_allocated,
-						      count);
-	return rv;
+	if (!__cvmx_fpa3_aura_valid(aura))
+		return 0;
+
+	cvmx_fpa3_release_aura(aura);
+	return 1;
 }
-EXPORT_SYMBOL(cvmx_fpa3_free_pools);
+
+/**
+ * Return if aura/pool is already reserved
+ * @param node - node of fpa to check, -1 for current node
+ * @param pool_num - pool to check (aura for o78+)
+ * @return 0 if reserved, 1 if available
+ */
+int cvmx_fpa_is_pool_available(int pool_num)
+{
+	if (octeon_has_feature(OCTEON_FEATURE_FPA3))
+		return cvmx_fpa3_is_aura_available(0, pool_num);
+	else
+		return cvmx_fpa1_is_pool_available(pool_num);
+}
+
+EXPORT_SYMBOL(cvmx_fpa3_reserve_aura);
+EXPORT_SYMBOL(cvmx_fpa3_release_aura);
+EXPORT_SYMBOL(cvmx_fpa3_reserve_pool);
+EXPORT_SYMBOL(cvmx_fpa3_release_pool);
diff --git a/arch/mips/cavium-octeon/executive/cvmx-global-resources.c b/arch/mips/cavium-octeon/executive/cvmx-global-resources.c
index 355dffe..4ff3232 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-global-resources.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-global-resources.c
@@ -109,11 +109,15 @@ typedef struct cvmx_global_resources
 } cvmx_global_resources_t;
 
 /* Not the right place, putting it here for now */
-CVMX_SHARED int cvmx_enable_helper_flag = 0;
+CVMX_SHARED int cvmx_enable_helper_flag;
+CVMX_SHARED uint64_t cvmx_app_id;
 
 static int dbg = 0;
 extern int __cvmx_bootmem_phy_free(uint64_t phy_addr, uint64_t size, uint32_t flags);
 
+/*
+ * Global named memory can be accessed anywhere even in 32-bit mode
+ */
 static CVMX_SHARED uint64_t __cvmx_global_resources_addr = 0;
 
 /**
@@ -430,6 +434,24 @@ int cvmx_resource_alloc_many(struct global_resource_tag tag,
 	return rv;
 }
 
+int cvmx_resource_alloc_reverse(struct global_resource_tag tag,
+				uint64_t owner)
+{
+	uint64_t addr = cvmx_get_global_resource(tag, 1);
+	int rv;
+
+	if (addr == 0) {
+		char tagname[256];
+		__cvmx_get_tagname(&tag, tagname);
+		cvmx_dprintf("ERROR: cannot find resource %s\n", tagname);
+		return -1;
+	}
+	__cvmx_global_resource_lock();
+	rv = cvmx_range_alloc_ordered(addr, owner, 1, 1, 1);
+	__cvmx_global_resource_unlock();
+	return rv;
+}
+
 int cvmx_reserve_global_resource_range(struct global_resource_tag tag,
 				       uint64_t owner, int base,
 				       int nelements)
@@ -523,6 +545,12 @@ int free_global_resources(void)
 	return 0;
 }
 
+uint64_t cvmx_get_global_resource_owner(struct global_resource_tag tag, int base)
+{
+	uint64_t addr = cvmx_get_global_resource(tag, 1);
+
+	return cvmx_range_get_owner(addr, base);
+}
 
 void cvmx_global_resources_show(void)
 {
@@ -556,3 +584,31 @@ void cvmx_global_resources_show(void)
 
 }
 EXPORT_SYMBOL(free_global_resources);
+
+void cvmx_app_id_init(void *bootmem)
+{
+	uint64_t *p = (uint64_t *) bootmem;
+
+	*p = 0;
+}
+
+uint64_t cvmx_allocate_app_id(void)
+{
+	uint64_t *vptr;
+
+	vptr = (uint64_t *)cvmx_bootmem_alloc_named_range_once(
+		sizeof(cvmx_app_id), 0, 1<<31, 128,
+		"cvmx_app_id", cvmx_app_id_init);
+
+	cvmx_app_id = __atomic_add_fetch(vptr, 1, __ATOMIC_SEQ_CST);
+
+	cvmx_dprintf("CVMX_APP_ID = %lx.\n", (unsigned long)cvmx_app_id);
+	return cvmx_app_id;
+}
+
+uint64_t cvmx_get_app_id(void)
+{
+	if (cvmx_app_id == 0)
+		cvmx_allocate_app_id();
+	return cvmx_app_id;
+}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-bgx.c b/arch/mips/cavium-octeon/executive/cvmx-helper-bgx.c
index 1e96046..3814bb2 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-bgx.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-bgx.c
@@ -97,6 +97,29 @@ int __cvmx_helper_bgx_enumerate(int xiface)
 
 /**
  * @INTERNAL
+ * Disable the BGX port
+ *
+ * @param  IPD port of the BGX interface to disable
+ */
+void cvmx_helper_bgx_disable(int xipd_port)
+{
+	int xiface = cvmx_helper_get_interface_num(xipd_port);
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(xipd_port);
+	int interface = xi.interface;
+	int node = xi.node;
+	int index = cvmx_helper_get_interface_index_num(xp.port);
+	cvmx_bgxx_cmrx_config_t cmr_config;
+
+	cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface));
+	cmr_config.s.enable = 0;
+	cmr_config.s.data_pkt_tx_en = 0;
+	cmr_config.s.data_pkt_rx_en = 0;
+	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
+}
+
+/**
+ * @INTERNAL
  * Configure the bgx mac.
  *
  * @param interface Interface to bring up
@@ -1114,6 +1137,7 @@ int __cvmx_helper_bgx_xaui_enable(int xiface)
 
 	__cvmx_bgx_common_init(xiface);
 	__cvmx_bgx_common_init_pknd(xiface);
+
 	for (index = 0; index < num_ports; index++) {
 		int res = __cvmx_helper_bgx_xaui_init(index, xiface);
 		if (res == -1) {
@@ -1128,7 +1152,6 @@ int __cvmx_helper_bgx_xaui_enable(int xiface)
 		smu_tx_thresh.s.cnt = 0x100;
 		cvmx_write_csr_node(node, CVMX_BGXX_SMUX_TX_THRESH(index, interface),
 					smu_tx_thresh.u64);
-
 		cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface));
 		cmr_config.s.enable = 1;
 		cmr_config.s.data_pkt_tx_en = 1;
@@ -1391,3 +1414,50 @@ void cvmx_helper_bgx_tx_options(unsigned node,
 	}
 }
 
+/**
+ * Set mac for the ipd_port
+ *
+ * @param xipd_port ipd_port to set the mac
+ * @param bcst      If set, accept all broadcast packets
+ * @param mcst      Multicast mode
+ * 		    0 = Force reject all multicast packets
+ * 		    1 = Force accept all multicast packets
+ * 		    2 = use the address filter CAM.
+ * @param mac       mac address for the ipd_port		    
+ */ 		    
+void cvmx_helper_bgx_set_mac(int xipd_port, int bcst, int mcst, uint64_t mac)
+{
+	int xiface = cvmx_helper_get_interface_num(xipd_port);
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	int interface = xi.interface;
+	int node = xi.node;
+	int index = (xipd_port >> 4) & 0xf;
+	cvmx_bgxx_cmr_rx_adrx_cam_t adr_cam;
+	cvmx_bgxx_cmrx_rx_adr_ctl_t adr_ctl;
+	cvmx_bgxx_cmrx_config_t cmr_config;
+	int saved_state;
+
+	cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface));
+	saved_state = cmr_config.s.enable;
+	cmr_config.s.enable = 0;
+	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
+
+	/* Set the mac */
+	adr_cam.u64 = 0;
+	adr_cam.s.id = index;
+	adr_cam.s.en = 1;
+	adr_cam.s.adr = mac;
+	cvmx_write_csr_node(node, CVMX_BGXX_CMR_RX_ADRX_CAM(index * 8, interface), adr_cam.u64);
+
+	adr_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_RX_ADR_CTL(index, interface));
+	adr_ctl.s.cam_accept = 1;  /* Accept the packet on DMAC CAM address */
+	adr_ctl.s.mcst_mode = mcst;   /* Use the address filter CAM */
+	adr_ctl.s.bcst_accept = bcst; /* Accept all broadcast packets */
+	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_RX_ADR_CTL(index, interface), adr_ctl.u64);
+	/* Set SMAC for PAUSE frames */
+	cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_SMACX(index, interface), mac);
+
+	/* Restore back the interface state */
+	cmr_config.s.enable = saved_state;
+	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
+}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c b/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c
index 7e33198..fde6197 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c
@@ -81,21 +81,22 @@ CVMX_SHARED struct cvmx_cfg_port_param cvmx_cfg_port[CVMX_MAX_NODES][CVMX_HELPER
 		{[0 ... CVMX_HELPER_CFG_MAX_PORT_PER_IFACE - 1] =
 			{
 #ifndef CVMX_BUILD_FOR_LINUX_KERNEL
-				/* port_fdt_node = */ CVMX_HELPER_CFG_INVALID_VALUE,
-				/* phy_fdt_node = */ CVMX_HELPER_CFG_INVALID_VALUE,
+				.port_fdt_node = CVMX_HELPER_CFG_INVALID_VALUE,
+				.phy_fdt_node = CVMX_HELPER_CFG_INVALID_VALUE,
 #endif
-				/* ccpp_pknd = */ CVMX_HELPER_CFG_INVALID_VALUE,
-				/* ccpp_bpid =  */ CVMX_HELPER_CFG_INVALID_VALUE,
-				/* ccpp_pko_port_base = */ CVMX_HELPER_CFG_INVALID_VALUE,
-				/* ccpp_pko_num_ports = */ CVMX_HELPER_CFG_INVALID_VALUE,
-				/* agl_rx_clk_skew = */ 0,
-				/* valid = */ true,
-				/* sgmii_phy_mode = */ false,
-				/* sgmii_1000x_mode = */ false,
-				/* agl_rx_clk_delay_bypass = */ false,
-				/* force_link_up = */ false,
-				/* disable_an = */ false,
-				/* link_down_pwr_dn = */ false
+				.phy_info = NULL,
+				.ccpp_pknd = CVMX_HELPER_CFG_INVALID_VALUE,
+				.ccpp_bpid =  CVMX_HELPER_CFG_INVALID_VALUE,
+				.ccpp_pko_port_base = CVMX_HELPER_CFG_INVALID_VALUE,
+				.ccpp_pko_num_ports = CVMX_HELPER_CFG_INVALID_VALUE,
+				.agl_rx_clk_skew = 0,
+				.valid = true,
+				.sgmii_phy_mode = false,
+				.sgmii_1000x_mode = false,
+				.agl_rx_clk_delay_bypass = false,
+				.force_link_up = false,
+				.disable_an = false,
+				.link_down_pwr_dn = false
 			}
 		}
 	};
@@ -149,6 +150,9 @@ static CVMX_SHARED uint64_t cvmx_cfg_opts[CVMX_HELPER_CFG_OPT_MAX] = {[0 ... CVM
  */
 static CVMX_SHARED int cvmx_cfg_max_pko_engines;	/* # of PKO DMA engines
 							   allocated */
+static int cvmx_pko_queue_alloc(uint64_t port, int count);
+static const int dbg = 0;
+
 int __cvmx_helper_cfg_pknd(int xiface, int index)
 {
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
@@ -227,8 +231,9 @@ int cvmx_pko_queue_grp_alloc(uint64_t start, uint64_t end, uint64_t count)
 		ret_val = cvmx_pko_queue_alloc(port, count);
 		if (ret_val == -1)
 		{
-			cvmx_dprintf("ERROR: Failed to allocate queue for port=%d count=%d \n", (int) port,
-				     (int) count);
+			cvmx_printf("ERROR: %sL Failed to allocate queue "
+				"for port=%d count=%d\n",
+				__func__, (int) port, (int) count);
 			return ret_val;
 		}
 	}
@@ -310,17 +315,31 @@ int init_cvmx_pko_que_range(void)
  * @return  0 on success
  *         -1 on failure
  */
-int cvmx_pko_queue_alloc(uint64_t port, uint64_t count)
+static int cvmx_pko_queue_alloc(uint64_t port, int count)
 {
     int ret_val = -1;
     int highest_queue;
 
     init_cvmx_pko_que_range();
+
+    if (cvmx_pko_queue_table[port].ccppp_num_queues == count)
+	    return cvmx_pko_queue_table[port].ccppp_queue_base;
+
+    if (cvmx_pko_queue_table[port].ccppp_num_queues > 0) {
+	    cvmx_printf("WARNING: %s port=%d already %d queues\n",
+		__func__, (int)port, 
+		(int)cvmx_pko_queue_table[port].ccppp_num_queues);
+	return -1;
+    }
+
     if (port >= CVMX_HELPER_CFG_MAX_PKO_QUEUES) {
-	    cvmx_dprintf("ERROR: %s port=%d > %d", __FUNCTION__, (int) port, CVMX_HELPER_CFG_MAX_PKO_QUEUES );
+	    cvmx_printf("ERROR: %s port=%d > %d\n", __func__,
+		(int) port, CVMX_HELPER_CFG_MAX_PKO_QUEUES );
 	    return -1;
     }
-    ret_val = cvmx_allocate_global_resource_range(CVMX_GR_TAG_PKO_QUEUES, port, count, 1);
+
+    ret_val = cvmx_allocate_global_resource_range(CVMX_GR_TAG_PKO_QUEUES,
+	port, count, 1);
     //cvmx_dprintf("allocated pko que : port=%02d base=%02d count=%02d \n", (int) port, ret_val, (int) count);
     if (ret_val == -1)
         return ret_val;
@@ -483,7 +502,6 @@ void cvmx_helper_cfg_set_jabber_and_frame_max()
 	int interface, port;
 
 	if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
-		int node = cvmx_get_node_num();
 
 		/*Set the frame max size and jabber size to 65535. */
 		for (interface = 0; interface < cvmx_helper_get_number_of_interfaces(); interface++) {
@@ -494,11 +512,6 @@ void cvmx_helper_cfg_set_jabber_and_frame_max()
 
 			switch (imode) {
 			case CVMX_HELPER_INTERFACE_MODE_SGMII:
-				for (port = 0; port < num_ports; port++) {
-					cvmx_pki_set_max_frm_len(node, port, -1);
-					cvmx_write_csr(CVMX_BGXX_GMP_GMI_RXX_JABBER(port, interface), 65535);
-				}
-				break;
 			case CVMX_HELPER_INTERFACE_MODE_XAUI:
 			case CVMX_HELPER_INTERFACE_MODE_RXAUI:
 			case CVMX_HELPER_INTERFACE_MODE_XLAUI:
@@ -506,8 +519,10 @@ void cvmx_helper_cfg_set_jabber_and_frame_max()
 			case CVMX_HELPER_INTERFACE_MODE_10G_KR:
 			case CVMX_HELPER_INTERFACE_MODE_40G_KR4:
 				for (port = 0; port < num_ports; port++) {
-					cvmx_pki_set_max_frm_len(node, port, -1);
-					cvmx_write_csr(CVMX_BGXX_SMUX_RX_JABBER(port, interface), 65535);
+					int ipd_port;
+					ipd_port = cvmx_helper_get_ipd_port(interface, port);
+					cvmx_pki_set_max_frm_len(ipd_port, -1);
+					cvmx_write_csr(CVMX_BGXX_GMP_GMI_RXX_JABBER(port, interface), 65535);
 				}
 				break;
 			default:
@@ -695,6 +710,9 @@ static int cvmx_helper_cfg_dft_nqueues(int pko_port)
 		/* Should never be called */
 		ret = 1;
 	}
+	/* Override for sanity in case of empty static config table */
+	if (ret == 0)
+		ret = 1;
 	return ret;
 }
 
@@ -729,7 +747,9 @@ static int cvmx_helper_cfg_init_pko_iports_and_queues_using_static_config(void)
 		n = cvmx_helper_cfg_dft_nqueues(i);
 		base = cvmx_pko_queue_alloc(i, n);
 		if (base == -1)  {
-			cvmx_dprintf("ERROR: failed to alloc queues=%d for pko port=%d\n", n, i);
+			cvmx_printf("ERROR: %s: failed to alloc %d queues "
+				"for pko port=%d\n",
+				__func__, n, i);
 			rv = -1;
 		}
 	}
@@ -893,17 +913,25 @@ int __cvmx_helper_init_port_config_data_local(void)
 	return rv;
 }
 
-extern int is_app_config_string_set(void);
-
+/*
+ * This call is made from Linux octeon_ethernet driver
+ * to setup the PKO with a specific queue count and
+ * internal port count configuration.
+ */
 int cvmx_pko_alloc_iport_and_queues(int interface, int port, int port_cnt, int queue_cnt)
 {
-	int rv,p, port_start, cnt;
+	int rv, p, port_start, cnt;
+
+	if (dbg)
+		cvmx_dprintf("%s: intf %d/%d pcnt %d qcnt %d\n",
+			__func__, interface, port, port_cnt, queue_cnt);
 
 	if (octeon_has_feature(OCTEON_FEATURE_PKND)) {
 		rv = cvmx_pko_internal_ports_alloc(interface, port, port_cnt);
 		if (rv < 0)  {
-			cvmx_dprintf("ERROR: failed to allocate internal ports for"
-				     "interface=%d port=%d cnt=%d\n", interface, port,
+			cvmx_printf("ERROR: %s: failed to allocate internal ports for"
+				     "interface=%d port=%d cnt=%d\n",
+				     __func__, interface, port,
 				     port_cnt);
 			return -1;
 		}
@@ -917,8 +945,9 @@ int cvmx_pko_alloc_iport_and_queues(int interface, int port, int port_cnt, int q
 	for (p = port_start; p < port_start + cnt; p++) {
 		rv = cvmx_pko_queue_alloc(p, queue_cnt);
 		if (rv < 0)  {
-			cvmx_dprintf("ERROR: failed to allocate queues for port=%d"
-				     "cnt=%d\n", p, queue_cnt);
+			cvmx_printf("ERROR: %s: failed to allocate "
+				"queues for port=%d cnt=%d\n",
+				__func__, p, queue_cnt);
 			return -1;
 		}
 	}
@@ -928,27 +957,25 @@ EXPORT_SYMBOL(cvmx_pko_alloc_iport_and_queues);
 
 int __cvmx_helper_init_port_config_data(void)
 {
-
 	int rv = 0;
 	int i, j, n;
-	int dbg = 0;
-	int static_config_set = 0;
 	int num_interfaces, interface;
-
-#if  ( !defined(CVMX_BUILD_FOR_LINUX_KERNEL)  && !defined(__U_BOOT__))
-	if (!(is_app_config_string_set()))
-		static_config_set = 1;
-#endif
-#ifdef __U_BOOT__
-	static_config_set = 1;
+#if !defined(CVMX_BUILD_FOR_LINUX_KERNEL)
+	const int use_static_config = 1;
+#else
+	const int use_static_config = 0;
 #endif
 
+	if (dbg)
+		cvmx_printf("%s:\n",__func__);
+
 	if (octeon_has_feature(OCTEON_FEATURE_PKND))
 	{
 		int pknd = 0, bpid = 0;
-		if (static_config_set) {
+
+		if (use_static_config)
 			cvmx_helper_cfg_init_pko_iports_and_queues_using_static_config();
-		}
+
 		/* Initialize pknd and bpid */
 		for (i = 0; i < cvmx_helper_get_number_of_interfaces(); i++) {
                         n = cvmx_helper_interface_enumerate(i);
@@ -971,9 +998,8 @@ int __cvmx_helper_init_port_config_data(void)
 		cvmx_helper_cfg_assert(bpid <= CVMX_HELPER_CFG_MAX_PIP_BPID);
 
 	} else {
-		if (static_config_set) {
+		if (use_static_config)
 			cvmx_pko_queue_init_from_cvmx_config_non_pknd();
-		}
 	}
 
 	/* init ports, queues which are not initialized */
@@ -1095,7 +1121,7 @@ void cvmx_helper_set_port_autonegotiation(int xiface, int index, bool enable)
 #endif /* !CVMX_BUILD_FOR_LINUX_KERNEL */
 
 /**
- * INTERNAL
+ * @INTERNAL
  * Returns if autonegotiation is enabled or not.
  *
  * @param xiface	node and interface
@@ -1109,3 +1135,33 @@ bool cvmx_helper_get_port_autonegotiation(int xiface, int index)
 	return !cvmx_cfg_port[xi.node][xi.interface][index].disable_an;
 }
 
+/**
+ * @INTERNAL
+ * Sets the PHY info data structure
+ *
+ * @param xiface	node and interface
+ * @param index		port index
+ * @param[in] phy_info	phy information data structure pointer
+ */
+void cvmx_helper_set_port_phy_info(int xiface, int index,
+				   struct cvmx_phy_info *phy_info)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	cvmx_cfg_port[xi.node][xi.interface][index].phy_info = phy_info;
+}
+
+/**
+ * @INTERNAL
+ * Returns the PHY information data structure for a port
+ *
+ * @param xiface	node and interface
+ * @param index		port index
+ *
+ * @return pointer to PHY information data structure or NULL if not set
+ */
+struct cvmx_phy_info *
+cvmx_helper_get_port_phy_info(int xiface, int index)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	return cvmx_cfg_port[xi.node][xi.interface][index].phy_info;
+}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-ilk.c b/arch/mips/cavium-octeon/executive/cvmx-helper-ilk.c
index 5dea91b..edce8bd 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-ilk.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-ilk.c
@@ -103,7 +103,7 @@ void __cvmx_ilk_init_cal_cn78xx(int interface)
 
 	/* Initialize all rx calendar entries to on */
 	rx_entry.u64 = 0;
-	rx_entry.s.ctl = XON;
+	rx_entry.s.ctl = XOFF;
 	for (i = 0; i < CVMX_ILK_MAX_CAL_IDX; i++) {
 		cvmx_write_csr(CVMX_ILK_RXX_CAL_ENTRYX(i, interface),
 			       rx_entry.u64);
@@ -300,13 +300,11 @@ void __cvmx_ilk_write_tx_cal_entry_cn78xx(int interface,
 		tx_cal.s.ctl = 1;
 		cvmx_write_csr(CVMX_ILK_TXX_CAL_ENTRYX(index - 1, interface),
 				tx_cal.u64);
-	} else {
-		tx_cal.u64 = 0;
-		tx_cal.s.ctl = 0;
-		tx_cal.s.channel = channel;
-		cvmx_write_csr(CVMX_ILK_TXX_CAL_ENTRYX(index - 1, interface),
-				tx_cal.u64);
 	}
+	tx_cal.u64 = 0;
+	tx_cal.s.ctl = 0;
+	tx_cal.s.channel = channel;
+	cvmx_write_csr(CVMX_ILK_TXX_CAL_ENTRYX(index, interface), tx_cal.u64);
 }
 	
 /**
@@ -342,13 +340,11 @@ void __cvmx_ilk_write_rx_cal_entry_cn78xx(int interface,
 		rx_cal.s.ctl = 1;
 		cvmx_write_csr(CVMX_ILK_RXX_CAL_ENTRYX(index - 1, interface),
 				rx_cal.u64);
-	} else {
-		rx_cal.u64 = 0;
-		rx_cal.s.ctl = 0;
-		rx_cal.s.channel = channel;
-		cvmx_write_csr(CVMX_ILK_RXX_CAL_ENTRYX(index - 1, interface),
-				rx_cal.u64);
 	}
+	rx_cal.u64 = 0;
+	rx_cal.s.ctl = 0;
+	rx_cal.s.channel = channel;
+	cvmx_write_csr(CVMX_ILK_RXX_CAL_ENTRYX(index, interface), rx_cal.u64);
 }
 
 void __cvmx_ilk_write_rx_cal_entry_cn68xx(int interface,
@@ -804,7 +800,7 @@ retry:
 	result.s.link_up = 1;
 	result.s.full_duplex = 1;
  	if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
-		int qlm = cvmx_qlm_interface(interface);
+		int qlm = cvmx_qlm_interface(xiface);
 		result.s.speed = cvmx_qlm_get_gbaud_mhz(qlm) * 64 / 67;
 	} else
 		result.s.speed = cvmx_qlm_get_gbaud_mhz(1 + interface) * 64 / 67;
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-ipd.c b/arch/mips/cavium-octeon/executive/cvmx-helper-ipd.c
index c2eda2a..b4455df 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-ipd.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-ipd.c
@@ -47,7 +47,7 @@
 #include <asm/octeon/cvmx.h>
 #include <asm/octeon/cvmx-ipd-defs.h>
 #include <asm/octeon/cvmx-ipd.h>
-#include <asm/octeon/cvmx-fpa1.h>
+#include <asm/octeon/cvmx-fpa.h>
 #include <asm/octeon/cvmx-ipd.h>
 #include <asm/octeon/cvmx-pip.h>
 #include <asm/octeon/cvmx-helper-pki.h>
@@ -293,5 +293,8 @@ void cvmx_helper_ipd_pkt_wqe_le_mode(bool mode)
 		ipd_ctl_status.s.pkt_lend = mode;
 		ipd_ctl_status.s.wqe_lend = mode;
 		cvmx_write_csr(CVMX_IPD_CTL_STATUS, ipd_ctl_status.u64);
+	} else {
+		int node = cvmx_get_node_num();
+		cvmx_helper_pki_set_little_endian(node);
 	}
 }
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-pki.c b/arch/mips/cavium-octeon/executive/cvmx-helper-pki.c
index 776b40b..553b042 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-pki.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-pki.c
@@ -58,7 +58,7 @@
 #include "cvmx-error.h"
 #include "cvmx-pki-defs.h"
 #include "cvmx-pki.h"
-#include "cvmx-fpa.h"
+#include "cvmx-fpa3.h"
 #include "cvmx-helper-pki.h"
 #include "cvmx-pki-resources.h"
 #include "cvmx-pow.h"
@@ -66,27 +66,14 @@
 #include "cvmx-global-resources.h"
 #endif
 
-struct cvmx_pki_cluster_grp_config {
-	int grp_num;
-	uint64_t cluster_mask; /* bit mask of cluster assigned to this cluster group */
-};
-
-struct cvmx_pki_sso_grp_config {
-	int sso_grp_num;
-	int priority;
-	int weight;
-	int affinity;
-	uint64_t core_mask;
-	uint8_t core_mask_set;
-};
-
-static CVMX_SHARED int pki_helper_debug;
+static CVMX_SHARED int pki_helper_debug = 0;
 
 CVMX_SHARED bool cvmx_pki_dflt_init[CVMX_MAX_NODES] = {[0 ... CVMX_MAX_NODES-1] = 1};
 
 static CVMX_SHARED struct cvmx_pki_cluster_grp_config pki_dflt_clgrp[CVMX_MAX_NODES] = {
 	{0, 0xf},
-	{0, 0xf} };
+	{0, 0xf}
+};
 
 CVMX_SHARED struct cvmx_pki_pool_config pki_dflt_pool[CVMX_MAX_NODES] = { [0 ... CVMX_MAX_NODES-1] = {
 	.pool_num = -1,
@@ -102,16 +89,174 @@ CVMX_SHARED struct cvmx_pki_style_config pki_dflt_style[CVMX_MAX_NODES] = { [0 .
 	.parm_cfg = {.lenerr_en = 1, .maxerr_en = 1, .minerr_en = 1,
 	.fcs_strip = 1, .fcs_chk = 1, .first_skip = 40, .mbuff_size = 2048} } };
 
-static CVMX_SHARED struct cvmx_pki_sso_grp_config pki_dflt_sso_grp[CVMX_MAX_NODES];
-static CVMX_SHARED struct cvmx_pki_qpg_config pki_dflt_qpg[CVMX_MAX_NODES];
+CVMX_SHARED struct cvmx_pki_sso_grp_config pki_dflt_sso_grp[CVMX_MAX_NODES];
+CVMX_SHARED struct cvmx_pki_qpg_config pki_dflt_qpg[CVMX_MAX_NODES];
 CVMX_SHARED struct cvmx_pki_pkind_config pki_dflt_pkind[CVMX_MAX_NODES];
 CVMX_SHARED uint64_t pkind_style_map[CVMX_MAX_NODES][CVMX_PKI_NUM_PKIND] = { [0 ... CVMX_MAX_NODES-1] = {
 	0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,
-        16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,
+	16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,
 	32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47,
 	48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63} };
 
-int __cvmx_helper_setup_pki_cluster_groups(int node)
+/* To store the qos watcher values before they are written to pcam when watcher is enabled
+There is no cvmx-pip.c file exist so it ended up here */
+CVMX_SHARED struct cvmx_pki_legacy_qos_watcher qos_watcher[8];
+
+
+/** @INTERNAL
+ * This function setsup default ltype map
+ * @param node    node number
+ */
+void __cvmx_helper_pki_set_dflt_ltype_map(int node)
+{
+	cvmx_pki_write_ltype_map(node,
+		CVMX_PKI_LTYPE_E_NONE, CVMX_PKI_BELTYPE_NONE);
+	cvmx_pki_write_ltype_map(node,
+		CVMX_PKI_LTYPE_E_ENET, CVMX_PKI_BELTYPE_MISC);
+	cvmx_pki_write_ltype_map(node,
+		CVMX_PKI_LTYPE_E_VLAN, CVMX_PKI_BELTYPE_MISC);
+	cvmx_pki_write_ltype_map(node,
+		CVMX_PKI_LTYPE_E_SNAP_PAYLD, CVMX_PKI_BELTYPE_MISC);
+	cvmx_pki_write_ltype_map(node,
+		 CVMX_PKI_LTYPE_E_ARP, CVMX_PKI_BELTYPE_MISC);
+	cvmx_pki_write_ltype_map(node,
+		CVMX_PKI_LTYPE_E_RARP, CVMX_PKI_BELTYPE_MISC);
+	cvmx_pki_write_ltype_map(node,
+		CVMX_PKI_LTYPE_E_IP4, CVMX_PKI_BELTYPE_IP4);
+	cvmx_pki_write_ltype_map(node,
+		CVMX_PKI_LTYPE_E_IP4_OPT, CVMX_PKI_BELTYPE_IP4);
+	cvmx_pki_write_ltype_map(node,
+		CVMX_PKI_LTYPE_E_IP6, CVMX_PKI_BELTYPE_IP6);
+	cvmx_pki_write_ltype_map(node,
+		CVMX_PKI_LTYPE_E_IP6_OPT, CVMX_PKI_BELTYPE_IP6);
+	cvmx_pki_write_ltype_map(node,
+		CVMX_PKI_LTYPE_E_IPSEC_ESP, CVMX_PKI_BELTYPE_MISC);
+	cvmx_pki_write_ltype_map(node,
+		CVMX_PKI_LTYPE_E_IPFRAG, CVMX_PKI_BELTYPE_MISC);
+	cvmx_pki_write_ltype_map(node,
+		CVMX_PKI_LTYPE_E_IPCOMP, CVMX_PKI_BELTYPE_MISC);
+	cvmx_pki_write_ltype_map(node,
+		CVMX_PKI_LTYPE_E_TCP, CVMX_PKI_BELTYPE_TCP);
+	cvmx_pki_write_ltype_map(node,
+		CVMX_PKI_LTYPE_E_UDP, CVMX_PKI_BELTYPE_UDP);
+	cvmx_pki_write_ltype_map(node,
+		CVMX_PKI_LTYPE_E_SCTP, CVMX_PKI_BELTYPE_SCTP);
+	cvmx_pki_write_ltype_map(node,
+		CVMX_PKI_LTYPE_E_UDP_VXLAN, CVMX_PKI_BELTYPE_UDP);
+	cvmx_pki_write_ltype_map(node,
+		CVMX_PKI_LTYPE_E_GRE, CVMX_PKI_BELTYPE_MISC);
+	cvmx_pki_write_ltype_map(node,
+		CVMX_PKI_LTYPE_E_NVGRE, CVMX_PKI_BELTYPE_MISC);
+	cvmx_pki_write_ltype_map(node,
+		CVMX_PKI_LTYPE_E_GTP, CVMX_PKI_BELTYPE_MISC);
+	cvmx_pki_write_ltype_map(node,
+		CVMX_PKI_LTYPE_E_SW28, CVMX_PKI_BELTYPE_MISC);
+	cvmx_pki_write_ltype_map(node,
+		CVMX_PKI_LTYPE_E_SW29, CVMX_PKI_BELTYPE_MISC);
+	cvmx_pki_write_ltype_map(node,
+		CVMX_PKI_LTYPE_E_SW30, CVMX_PKI_BELTYPE_MISC);
+	cvmx_pki_write_ltype_map(node,
+		CVMX_PKI_LTYPE_E_SW31, CVMX_PKI_BELTYPE_MISC);
+}
+EXPORT_SYMBOL(__cvmx_helper_pki_set_dflt_ltype_map);
+
+/** @INTERNAL
+ * This function installs the default VLAN entries to identify
+ * the VLAN and set WQE[vv], WQE[vs] if VLAN is found. In 78XX
+ * hardware (PKI) is not hardwired to recognize any 802.1Q VLAN
+ * Ethertypes
+ *
+ * @param node    node number
+ */
+int __cvmx_helper_pki_install_dflt_vlan(int node)
+{
+	struct cvmx_pki_pcam_input pcam_input;
+	struct cvmx_pki_pcam_action pcam_action;
+	enum cvmx_pki_term field;
+	int index;
+	int bank;
+	uint64_t cl_mask = CVMX_PKI_CLUSTER_ALL;
+
+	memset(&pcam_input, 0, sizeof(pcam_input));
+	memset(&pcam_action, 0, sizeof(pcam_action));
+
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X)) {
+		/* PKI-20858 */
+		int i;
+		for (i = 0; i < 4; i++) {
+			union cvmx_pki_clx_ecc_ctl ecc_ctl;
+			ecc_ctl.u64 = cvmx_read_csr_node(node,
+				CVMX_PKI_CLX_ECC_CTL(i));
+			ecc_ctl.s.pcam_en = 0;
+			ecc_ctl.s.pcam0_cdis = 1;
+			ecc_ctl.s.pcam1_cdis = 1;
+			cvmx_write_csr_node(node,
+				CVMX_PKI_CLX_ECC_CTL(i), ecc_ctl.u64);
+		}
+	}
+
+	for (field = CVMX_PKI_PCAM_TERM_ETHTYPE0;
+		field < CVMX_PKI_PCAM_TERM_ETHTYPE2; field++) {
+		bank = field & 0x01;
+
+		index = cvmx_pki_pcam_entry_alloc(node,
+			CVMX_PKI_FIND_AVAL_ENTRY, bank, cl_mask);
+		if (index < 0) {
+			cvmx_dprintf("ERROR: Allocating pcam entry node=%d bank=%d\n",
+				node, bank);
+			return -1;
+		}
+		pcam_input.style = 0;
+		pcam_input.style_mask = 0;
+		pcam_input.field = field;
+		pcam_input.field_mask = 0xfd;
+		pcam_input.data = 0x81000000;
+		pcam_input.data_mask = 0xffff0000;
+		pcam_action.parse_mode_chg = CVMX_PKI_PARSE_NO_CHG;
+		pcam_action.layer_type_set = CVMX_PKI_LTYPE_E_VLAN;
+		pcam_action.style_add = 0;
+		pcam_action.pointer_advance = 4;
+		cvmx_pki_pcam_write_entry(node, index, cl_mask,
+			pcam_input, pcam_action);/*vinita_to_do, cluster_mask*/
+
+		index = cvmx_pki_pcam_entry_alloc(node,
+			CVMX_PKI_FIND_AVAL_ENTRY, bank, cl_mask);
+		if (index < 0) {
+			cvmx_dprintf("ERROR: Allocating pcam entry node=%d bank=%d\n",
+				node, bank);
+			return -1;
+		}
+		pcam_input.data = 0x88a80000;
+		cvmx_pki_pcam_write_entry(node, index, cl_mask, pcam_input,
+			pcam_action);/*vinita_to_do, cluster_mask*/
+
+		index = cvmx_pki_pcam_entry_alloc(node,
+			CVMX_PKI_FIND_AVAL_ENTRY, bank, cl_mask);
+		if (index < 0) {
+			cvmx_dprintf("ERROR: Allocating pcam entry node=%d bank=%d\n",
+				node, bank);
+			return -1;
+		}
+		pcam_input.data = 0x92000000;
+		cvmx_pki_pcam_write_entry(node, index, cl_mask, pcam_input,
+			pcam_action);/*vinita_to_do, cluster_mask*/
+
+		index = cvmx_pki_pcam_entry_alloc(node,
+			CVMX_PKI_FIND_AVAL_ENTRY, bank, cl_mask);
+		if (index < 0) {
+			cvmx_dprintf("ERROR: Allocating pcam entry node=%d bank=%d\n",
+				node, bank);
+			return -1;
+		}
+		pcam_input.data = 0x91000000;
+		cvmx_pki_pcam_write_entry(node, index, cl_mask, pcam_input,
+			pcam_action);/*vinita_to_do, cluster_mask*/
+	}
+	return 0;
+}
+EXPORT_SYMBOL(__cvmx_helper_pki_install_dflt_vlan);
+
+static int __cvmx_helper_setup_pki_cluster_groups(int node)
 {
 	uint64_t cl_mask;
 	int cl_group;
@@ -147,17 +292,12 @@ int __cvmx_helper_pki_setup_sso_groups(int node)
 	uint8_t core_mask_set;
 
 	/* try to reserve sso groups and configure them if they are not configured */
-	/* vinita_to_do uncomment below when sso resource alloc is ready */
-#if 1
-	grp = pki_dflt_sso_grp[node].sso_grp_num;
-#else
-	grp = cvmx_sso_grp_alloc(node, pki_dflt_sso_grp[node].sso_grp_num);
+	grp = cvmx_sso_allocate_group_range(node, &pki_dflt_sso_grp[node].group, 1);
 	if (grp == CVMX_RESOURCE_ALLOC_FAILED)
 		return -1;
 	else if (grp == CVMX_RESOURCE_ALREADY_RESERVED)
 		return 0; /* sso group already configured, share it */
 
-#endif
 	xgrp.xgrp = grp;
 	priority = pki_dflt_sso_grp[node].priority;
 	weight = pki_dflt_sso_grp[node].weight;
@@ -181,72 +321,59 @@ int __cvmx_helper_pki_setup_sso_groups(int node)
  * This function sets up pools/auras to be used by PKI
  * @param node    node number
  */
-int __cvmx_helper_pki_setup_fpa_pools(int node)
+static int __cvmx_helper_pki_setup_fpa_pools(int node)
 {
-	int rs;
 	uint64_t buffer_count;
 	uint64_t buffer_size;
 
+	if (__cvmx_fpa3_aura_valid(pki_dflt_aura[node].aura))
+		return 0; /* aura already configured, share it */
+
 	buffer_count = pki_dflt_pool[node].buffer_count;
+	buffer_size = pki_dflt_pool[node].buffer_size;
+
 	if (buffer_count != 0) {
-		rs = cvmx_fpa_allocate_fpa_pools(node, &pki_dflt_pool[node].pool_num, 1);
-		if (rs == -1) {
-			if (pki_dflt_pool[node].pool_num == -1) {
-				cvmx_dprintf("ERROR: Failed to allocate pool %d\n", pki_dflt_pool[node].pool_num);
-				return -1;
-			}
-		} else {
-			buffer_size = pki_dflt_pool[node].buffer_size;
-			if (pki_helper_debug)
-				cvmx_dprintf("pki-helper: set fpa pool %d with buffer size %d buffer cnt %d\n",
-			pki_dflt_pool[node].pool_num, (int)buffer_size, (int)buffer_count);
-			cvmx_fpa3_pool_stack_init(node, pki_dflt_pool[node].pool_num, "PKI Pool0", 0,
-						 buffer_count, FPA_NATURAL_ALIGNMENT,
-						 buffer_size);
-			pki_dflt_aura[node].pool_num =  pki_dflt_pool[node].pool_num;
+		pki_dflt_pool[node].pool =
+			cvmx_fpa3_setup_fill_pool(node,
+				pki_dflt_pool[node].pool_num, "PKI POOL DFLT",
+				buffer_size, buffer_count, NULL);
+		if (!__cvmx_fpa3_pool_valid(pki_dflt_pool[node].pool)) {
+			cvmx_printf("ERROR: %s: Failed to allocate pool %d\n",
+				__func__, pki_dflt_pool[node].pool_num);
+			return -1;
 		}
+		pki_dflt_pool[node].pool_num = pki_dflt_pool[node].pool.lpool;
+
+		if (pki_helper_debug)
+			cvmx_dprintf("%s pool %d with buffer size %d cnt %d\n",
+				__func__, pki_dflt_pool[node].pool_num,
+				(int)buffer_size, (int)buffer_count);
+
+		pki_dflt_aura[node].pool_num =  pki_dflt_pool[node].pool_num;
+		pki_dflt_aura[node].pool =  pki_dflt_pool[node].pool;
 	}
+
 	buffer_count = pki_dflt_aura[node].buffer_count;
+
 	if (buffer_count != 0) {
-		rs = cvmx_helper_fpa3_add_aura_to_pool(node, pki_dflt_pool[node].pool_num,
-						       &pki_dflt_aura[node].aura_num,
-						       buffer_count, NULL, "PKI Aura0");
-		if (rs == -1) {
-			if (pki_dflt_aura[node].aura_num == -1) {
-				cvmx_dprintf("ERROR: Failed to allocate aura %d\n", pki_dflt_aura[node].aura_num);
+		pki_dflt_aura[node].aura =
+			cvmx_fpa3_set_aura_for_pool(
+				pki_dflt_aura[node].pool,
+				pki_dflt_aura[node].aura_num,
+				"PKI DFLT AURA",
+				buffer_size, buffer_count);
+
+		if (!__cvmx_fpa3_aura_valid(pki_dflt_aura[node].aura)) {
+			cvmx_dprintf("ERROR: %sL Failed to allocate aura %d\n",
+				__func__, pki_dflt_aura[node].aura_num);
 				return -1;
-			} else
-				return 0; /* aura already configured, share it */
 		}
 	}
 	return 0;
 }
-#endif
-
-/**
- * This function writes qpg entry at specified offset in hardware
- *
- * @param node		node number
- * @param index		offset in qpg entry to write to.
- * @param padd		port address for channel calculation
- * @param aura		aura number to send packet to
- * @param group_ok	group number to send packet to if there is no error
- * @param group_bad	group number to send packet  to if there is error
- */
-static void cvmx_pki_write_qpg_entry(int node, int index, int padd, int aura,
-					      int group_ok, int group_bad)
-{
-	cvmx_pki_qpg_tblx_t qpg_tbl;
-	qpg_tbl.u64 = cvmx_read_csr_node(node, CVMX_PKI_QPG_TBLX(index));
-	qpg_tbl.s.padd = padd;
-	qpg_tbl.s.laura = aura;
-	qpg_tbl.s.grp_ok = group_ok;
-	qpg_tbl.s.grp_bad = group_bad;
-	cvmx_write_csr_node(node, CVMX_PKI_QPG_TBLX(index), qpg_tbl.u64);
-}
-
+#endif /* ! CVMX_BUILD_FOR_LINUX_KERNEL */
 
-int __cvmx_helper_setup_pki_qpg_table(int node)
+static int __cvmx_helper_setup_pki_qpg_table(int node)
 {
 	int offset;
 
@@ -256,188 +383,14 @@ int __cvmx_helper_setup_pki_qpg_table(int node)
 	else if (offset == CVMX_RESOURCE_ALREADY_RESERVED)
 		return 0; /* share the qpg table entry */
 	if (pki_helper_debug)
-		cvmx_dprintf("pki-helper: set qpg entry at offset %d with port add %d aura %d \
-				grp_ok %d grp_bad %d\n", offset, pki_dflt_qpg[node].port_add,
-				pki_dflt_qpg[node].aura, pki_dflt_qpg[node].grp_ok, pki_dflt_qpg[node].grp_bad);
-	cvmx_pki_write_qpg_entry(node, offset, pki_dflt_qpg[node].port_add, pki_dflt_qpg[node].aura,
-				 pki_dflt_qpg[node].grp_ok, pki_dflt_qpg[node].grp_bad);
-	return 0;
-}
-
-void __cvmx_helper_pki_set_ltype_map(int node)
-{
-        cvmx_pki_write_ltype_map(node, CVMX_PKI_LTYPE_E_NONE, CVMX_PKI_BELTYPE_NONE);
-        cvmx_pki_write_ltype_map(node, CVMX_PKI_LTYPE_E_ENET, CVMX_PKI_BELTYPE_MISC);
-        cvmx_pki_write_ltype_map(node, CVMX_PKI_LTYPE_E_VLAN, CVMX_PKI_BELTYPE_MISC);
-        cvmx_pki_write_ltype_map(node, CVMX_PKI_LTYPE_E_SNAP_PAYLD, CVMX_PKI_BELTYPE_MISC);
-        cvmx_pki_write_ltype_map(node, CVMX_PKI_LTYPE_E_ARP, CVMX_PKI_BELTYPE_MISC);
-        cvmx_pki_write_ltype_map(node, CVMX_PKI_LTYPE_E_RARP, CVMX_PKI_BELTYPE_MISC);
-        cvmx_pki_write_ltype_map(node, CVMX_PKI_LTYPE_E_IP4, CVMX_PKI_BELTYPE_IP4);
-        cvmx_pki_write_ltype_map(node, CVMX_PKI_LTYPE_E_IP4_OPT, CVMX_PKI_BELTYPE_IP4);
-        cvmx_pki_write_ltype_map(node, CVMX_PKI_LTYPE_E_IP6, CVMX_PKI_BELTYPE_IP6);
-        cvmx_pki_write_ltype_map(node, CVMX_PKI_LTYPE_E_IP6_OPT, CVMX_PKI_BELTYPE_IP6);
-        cvmx_pki_write_ltype_map(node, CVMX_PKI_LTYPE_E_IPSEC_ESP, CVMX_PKI_BELTYPE_MISC);
-        cvmx_pki_write_ltype_map(node, CVMX_PKI_LTYPE_E_IPFRAG, CVMX_PKI_BELTYPE_MISC);
-        cvmx_pki_write_ltype_map(node, CVMX_PKI_LTYPE_E_IPCOMP, CVMX_PKI_BELTYPE_MISC);
-        cvmx_pki_write_ltype_map(node, CVMX_PKI_LTYPE_E_TCP, CVMX_PKI_BELTYPE_TCP);
-        cvmx_pki_write_ltype_map(node, CVMX_PKI_LTYPE_E_UDP, CVMX_PKI_BELTYPE_UDP);
-        cvmx_pki_write_ltype_map(node, CVMX_PKI_LTYPE_E_SCTP, CVMX_PKI_BELTYPE_SCTP);
-        cvmx_pki_write_ltype_map(node, CVMX_PKI_LTYPE_E_UDP_VXLAN, CVMX_PKI_BELTYPE_UDP);
-        cvmx_pki_write_ltype_map(node, CVMX_PKI_LTYPE_E_GRE, CVMX_PKI_BELTYPE_MISC);
-        cvmx_pki_write_ltype_map(node, CVMX_PKI_LTYPE_E_NVGRE, CVMX_PKI_BELTYPE_MISC);
-        cvmx_pki_write_ltype_map(node, CVMX_PKI_LTYPE_E_GTP, CVMX_PKI_BELTYPE_MISC);
-        cvmx_pki_write_ltype_map(node, CVMX_PKI_LTYPE_E_SW28, CVMX_PKI_BELTYPE_MISC);
-        cvmx_pki_write_ltype_map(node, CVMX_PKI_LTYPE_E_SW29, CVMX_PKI_BELTYPE_MISC);
-        cvmx_pki_write_ltype_map(node, CVMX_PKI_LTYPE_E_SW30, CVMX_PKI_BELTYPE_MISC);
-        cvmx_pki_write_ltype_map(node, CVMX_PKI_LTYPE_E_SW31, CVMX_PKI_BELTYPE_MISC);
-}
-EXPORT_SYMBOL(__cvmx_helper_pki_set_ltype_map);
-/**
- * This function installs the default VLAN entries to identify
- * the VLAN and set WQE[vv], WQE[vs] if VLAN is found. In 78XX
- * hardware (PKI) is not hardwired to recognize any 802.1Q VLAN
- * Ethertypes
- *
- * @param node    node number
- */
-int __cvmx_helper_pki_install_default_vlan(int node)
-{
-	struct cvmx_pki_pcam_input pcam_input;
-	struct cvmx_pki_pcam_action pcam_action;
-	enum cvmx_pki_term field;
-	int index;
-	int bank;
-	uint64_t cl_mask = CVMX_PKI_CLUSTER_ALL;
-
-	if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X)) {
-		/* PKI-20858 */
-		int i;
-		for (i = 0; i < 4; i++) {
-			union cvmx_pki_clx_ecc_ctl ecc_ctl;
-			ecc_ctl.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_ECC_CTL(i));
-			ecc_ctl.s.pcam_en = 0;
-			ecc_ctl.s.pcam0_cdis = 1;
-			ecc_ctl.s.pcam1_cdis = 1;
-			cvmx_write_csr_node(node, CVMX_PKI_CLX_ECC_CTL(i), ecc_ctl.u64);
-		}
-	}
-
-	for (field = CVMX_PKI_PCAM_TERM_E_ETHTYPE0; field < CVMX_PKI_PCAM_TERM_E_ETHTYPE2; field++) {
-		bank = field & 0x01;
-
-		index = cvmx_pki_pcam_entry_alloc(node, CVMX_PKI_FIND_AVAL_ENTRY, bank, cl_mask);
-		if (index < 0) {
-			cvmx_dprintf("ERROR: Allocating pcam entry node=%d bank=%d\n", node, bank);
-			return -1;
-		}
-		pcam_input.style = 0;
-		pcam_input.style_mask = 0;
-		pcam_input.field = field;
-		pcam_input.field_mask = 0xfd;
-		pcam_input.data = 0x81000000;
-		pcam_input.data_mask = 0xffff0000;
-		pcam_action.parse_mode_chg = CVMX_PKI_PARSE_NO_CHG;
-		pcam_action.layer_type_set = CVMX_PKI_LTYPE_E_VLAN;
-		pcam_action.style_add = 0;
-		pcam_action.pointer_advance = 4;
-		cvmx_pki_pcam_write_entry(node, index, cl_mask, pcam_input, pcam_action);/*vinita_to_do, cluster_mask*/
-
-		index = cvmx_pki_pcam_entry_alloc(node, CVMX_PKI_FIND_AVAL_ENTRY, bank, cl_mask);
-		if (index < 0) {
-			cvmx_dprintf("ERROR: Allocating pcam entry node=%d bank=%d\n", node, bank);
-			return -1;
-		}
-		pcam_input.data = 0x88a80000;
-		cvmx_pki_pcam_write_entry(node, index, cl_mask, pcam_input, pcam_action);/*vinita_to_do, cluster_mask*/
-
-		index = cvmx_pki_pcam_entry_alloc(node, CVMX_PKI_FIND_AVAL_ENTRY, bank, cl_mask);
-		if (index < 0) {
-			cvmx_dprintf("ERROR: Allocating pcam entry node=%d bank=%d\n", node, bank);
-			return -1;
-		}
-		pcam_input.data = 0x92000000;
-		cvmx_pki_pcam_write_entry(node, index, cl_mask, pcam_input, pcam_action);/*vinita_to_do, cluster_mask*/
-
-		index = cvmx_pki_pcam_entry_alloc(node, CVMX_PKI_FIND_AVAL_ENTRY, bank, cl_mask);
-		if (index < 0) {
-			cvmx_dprintf("ERROR: Allocating pcam entry node=%d bank=%d\n", node, bank);
-			return -1;
-		}
-		pcam_input.data = 0x91000000;
-		cvmx_pki_pcam_write_entry(node, index, cl_mask, pcam_input, pcam_action);/*vinita_to_do, cluster_mask*/
-	}
-	return 0;
-}
-EXPORT_SYMBOL(__cvmx_helper_pki_install_default_vlan);
-
-void cvmx_helper_pki_enable(int node)
-{
-	if (pki_helper_debug)
-		cvmx_dprintf("enable PKI on node %d\n", node);
-	__cvmx_helper_pki_install_default_vlan(node);
-	cvmx_pki_setup_clusters(node);
-	cvmx_pki_enable_backpressure(node);
-	cvmx_pki_parse_enable(node, 0);
-	cvmx_pki_enable(node);
-}
-
-int __cvmx_helper_pki_global_setup(int node)
-{
-        __cvmx_helper_pki_set_ltype_map(node);
-	if (!cvmx_pki_dflt_init[node])
-		return 0;
-#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
-	/* Setup the packet pools*/
-	__cvmx_helper_pki_setup_fpa_pools(node);
-#endif
-	/*set up default cluster*/
-	__cvmx_helper_setup_pki_cluster_groups(node);
-	//__cvmx_helper_pki_setup_sso_groups(node);
-	__cvmx_helper_setup_pki_qpg_table(node);
-	return 0;
-}
-
-/* Frees up PKI resources consumed by that port. This function should only be called
-  if port resources (fpa pools aura, style qpg entry pcam entry etc.) are not shared */
-int cvmx_helper_pki_port_shutdown(int node)
-{
-	/* remove pcam entries */
-	/* vinita_to_do implemet later */
-	/* __cvmx_pki_port_rsrc_free(node); */
+		cvmx_dprintf("pki-helper: set qpg entry at offset %d with port add %d aura %d grp_ok %d grp_bad %d\n",
+			offset, pki_dflt_qpg[node].port_add,
+			pki_dflt_qpg[node].aura_num, pki_dflt_qpg[node].grp_ok,
+			pki_dflt_qpg[node].grp_bad);
+	cvmx_pki_write_qpg_entry(node, offset, &pki_dflt_qpg[node]);
 	return 0;
 }
 
-/* Shutdown complete PKI hardware and software resources */
-void cvmx_helper_pki_shutdown(int node)
-{
-	/* remove pcam entries */
-	/* Disable PKI */
-	cvmx_pki_disable(node);
-	/* Free all prefetched buffers */
-	__cvmx_pki_free_ptr(node);
-	/* Reset PKI */
-	cvmx_pki_reset(node);
-	/* Free all the allocated PKI resources
-	except fpa pools & aura which will be done in fpa block */
-	__cvmx_pki_global_rsrc_free(node);
-}
-
-int cvmx_helper_pki_get_num_qpg_entry(enum cvmx_pki_qpg_qos qpg_qos)
-{
-	if (qpg_qos == CVMX_PKI_QPG_QOS_NONE)
-		return 1;
-	else if (qpg_qos == CVMX_PKI_QPG_QOS_VLAN || qpg_qos == CVMX_PKI_QPG_QOS_MPLS)
-		return 8;
-	else if (qpg_qos == CVMX_PKI_QPG_QOS_DSA_SRC) /*vinita_to_do for higig2*/
-		return 32;
-	else if (qpg_qos == CVMX_PKI_QPG_QOS_DIFFSERV || qpg_qos == CVMX_PKI_QPG_QOS_HIGIG)
-		return 64;
-	else {
-		cvmx_dprintf("ERROR: unrecognized qpg_qos = %d", qpg_qos);
-		return 0;
-	}
-}
-
 int __cvmx_helper_pki_port_setup(int node, int ipd_port)
 {
 	int interface, index;
@@ -464,104 +417,45 @@ int __cvmx_helper_pki_port_setup(int node, int ipd_port)
 			cvmx_dprintf("pki-helper: set style %d with default parameters\n", style_num);
 		pkind_style_map[node][pknd] = style_num;
 		/* configure style with default parameters */
-		cvmx_pki_set_style_config(node, style_num, CVMX_PKI_CLUSTER_ALL,
-				     &pki_dflt_style[node]);
+		cvmx_pki_write_style_config(node, style_num, CVMX_PKI_CLUSTER_ALL,
+					  &pki_dflt_style[node]);
 	}
 	if (pki_helper_debug)
 		cvmx_dprintf("pki-helper: set pkind %d with initial style %d\n", pknd, style_num);
 	/* write pkind configuration */
 	pkind_cfg = pki_dflt_pkind[node];
 	pkind_cfg.initial_style = style_num;
-	cvmx_pki_set_pkind_config(node, pknd, &pkind_cfg);
+	cvmx_pki_write_pkind_config(node, pknd, &pkind_cfg);
 	return 0;
 }
 
-int cvmx_helper_pki_setup_qpg_table(int node, int num_entries,
-				    struct cvmx_pki_qpg_config *qpg_cfg)
+int __cvmx_helper_pki_global_setup(int node)
 {
-	int entry;
-	int offset;
-
-	if (pki_helper_debug)
-		cvmx_dprintf("allocated %d qpg entries", num_entries);
-	offset = cvmx_pki_qpg_entry_alloc(node, qpg_cfg->qpg_base, num_entries);
-	if (pki_helper_debug)
-		cvmx_dprintf("at offset %d \n", offset);
-	if (offset == CVMX_RESOURCE_ALREADY_RESERVED) {
-		cvmx_dprintf("INFO:setup_qpg_table: offset %d already reserved\n", qpg_cfg->qpg_base);
-		return CVMX_RESOURCE_ALREADY_RESERVED;
-	} else if (offset == CVMX_RESOURCE_ALLOC_FAILED) {
-		cvmx_dprintf("ERROR:setup_qpg_table: no more entries available\n");
-		return CVMX_RESOURCE_ALLOC_FAILED;
-	}
-	qpg_cfg->qpg_base = offset;
-	for (entry = 0; entry < num_entries; entry++, offset++, qpg_cfg++) {
-		cvmx_pki_write_qpg_entry(node, offset,
-					 qpg_cfg->port_add, qpg_cfg->aura,
-					 qpg_cfg->grp_ok, qpg_cfg->grp_bad);
+	__cvmx_helper_pki_set_dflt_ltype_map(node);
+	if (!cvmx_pki_dflt_init[node])
+		return 0;
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+	/* Setup the packet pools*/
+	__cvmx_helper_pki_setup_fpa_pools(node);
+#endif
+	/*set up default cluster*/
+	__cvmx_helper_setup_pki_cluster_groups(node);
+	//__cvmx_helper_pki_setup_sso_groups(node);
+	__cvmx_helper_setup_pki_qpg_table(node);
+	/*
+	 * errata PKI-19103 backward compat has only 1 aura
+	 * no head line blocking
+	 */
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X)) {
+		cvmx_pki_buf_ctl_t buf_ctl;
+		buf_ctl.u64 = cvmx_read_csr_node(node, CVMX_PKI_BUF_CTL);
+		buf_ctl.s.fpa_wait = 1;
+		cvmx_write_csr_node(node, CVMX_PKI_BUF_CTL, buf_ctl.u64);
 	}
-	return offset - num_entries;
+	return 0;
 }
 
-void cvmx_helper_pki_set_fcs_op(int node, int interface, int nports, int has_fcs)
-{
-	int index;
-	int pknd;
-	int cluster = 0;
-	cvmx_pki_clx_pkindx_cfg_t pkind_cfg;
-
-	for (index = 0; index < nports; index++) {
-		pknd = cvmx_helper_get_pknd(interface, index);
-                while (cluster < CVMX_PKI_NUM_CLUSTER) {
-                    /*vinita_to_do; find the cluster in use*/
-                    pkind_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_CFG(pknd, cluster));
-                    pkind_cfg.s.fcs_pres = has_fcs;
-                    cvmx_write_csr_node(node, CVMX_PKI_CLX_PKINDX_CFG(pknd, cluster), pkind_cfg.u64);
-                    cluster++;
-                }
-                /* make sure fcs_strip and fcs_check is also enable/disable for the style used by that port*/
-                cvmx_pki_endis_fcs_check(node, pknd, has_fcs, has_fcs);
-                cluster = 0;
-	}
-}
-
-/**
- * This function sets the wqe buffer mode of all ports. First packet data buffer can reside
- * either in same buffer as wqe OR it can go in separate buffer. If used the later mode,
- * make sure software allocate enough buffers to now have wqe separate from packet data.
- * @param node	                node number.
- * @param pkt_outside_wqe.	0 = The packet link pointer will be at word [FIRST_SKIP]
- *				    immediately followed by packet data, in the same buffer
- *				    as the work queue entry.
- *				1 = The packet link pointer will be at word [FIRST_SKIP] in a new
- *				    buffer separate from the work queue entry. Words following the
- *				    WQE in the same cache line will be zeroed, other lines in the
- *				    buffer will not be modified and will retain stale data (from the
- *				    buffers previous use). This setting may decrease the peak PKI
- *				    performance by up to half on small packets.
- */
-void cvmx_helper_pki_set_wqe_mode(int node, bool pkt_outside_wqe)
-{
-        int interface, port, pknd;
-        int num_intf, num_ports;
-        uint64_t style;
-
-        /* get the pkind used by this ipd port */
-        num_intf = cvmx_helper_get_number_of_interfaces();
-        for (interface = 0; interface < num_intf; interface++) {
-                num_ports = cvmx_helper_ports_on_interface(interface);
-                /*Skip invalid/disabled interfaces */
-                if (num_ports <= 0)
-                        continue;
-                for (port = 0; port < num_ports; port++) {
-                        pknd = cvmx_helper_get_pknd(interface, port);
-                        style = cvmx_pki_get_pkind_style(node, pknd);
-                        cvmx_pki_set_wqe_mode(node, style, pkt_outside_wqe);
-                }
-        }
-}
-
-void cvmx_helper_pki_set_dflt_pool(int node, int pool, int buffer_size, int buffer_count)
+void cvmx_helper_pki_set_dflt_pool(int node, int pool, int buffer_size, int buffer_count)
 {
 	if (pool == 0)
 		pool = -1;
@@ -581,6 +475,7 @@ void cvmx_helper_pki_set_dflt_pool_buffer(int node, int buffer_count)
 {
 	pki_dflt_pool[node].buffer_count = buffer_count;
 }
+
 void cvmx_helper_pki_set_dflt_aura_buffer(int node, int buffer_count)
 {
 	pki_dflt_aura[node].buffer_count = buffer_count;
@@ -617,6 +512,103 @@ void cvmx_helper_pki_no_dflt_init(int node)
 }
 
 /**
+ * This function Enabled the PKI hardware to
+ * start accepting/processing packets.
+ *
+ * @param node    node number
+ */
+void cvmx_helper_pki_enable(int node)
+{
+	if (pki_helper_debug)
+		cvmx_dprintf("enable PKI on node %d\n", node);
+	__cvmx_helper_pki_install_dflt_vlan(node);
+	cvmx_pki_setup_clusters(node);
+	cvmx_pki_enable_backpressure(node);
+	cvmx_pki_parse_enable(node, 0);
+	cvmx_pki_enable(node);
+}
+
+/**
+ * This function frees up PKI resources consumed by that port.
+ * This function should only be called if port resources
+ * (fpa pools aura, style qpg entry pcam entry etc.) are not shared
+ * @param ipd_port      ipd port number for which resources need to
+ *		      be freed.
+ */
+int cvmx_helper_pki_port_shutdown(int ipd_port)
+{
+	/* remove pcam entries */
+	/* vinita_to_do implemet later */
+	/* __cvmx_pki_port_rsrc_free(node); */
+	return 0;
+}
+
+/**
+ * This function shuts down complete PKI hardware
+ * and software resources.
+ * @param node	  node number where PKI needs to shutdown.
+ */
+void cvmx_helper_pki_shutdown(int node)
+{
+	/* remove pcam entries */
+	/* Disable PKI */
+	cvmx_pki_disable(node);
+	/* Free all prefetched buffers */
+	__cvmx_pki_free_ptr(node);
+	/* Reset PKI */
+	cvmx_pki_reset(node);
+	/* Free all the allocated PKI resources
+	except fpa pools & aura which will be done in fpa block */
+	__cvmx_pki_global_rsrc_free(node);
+}
+
+/**
+ * This function calculates how mant qpf entries will be needed for
+ * a particular QOS.
+ * @param qpg_qos       qos value for which entries need to be calculated.
+ */
+int cvmx_helper_pki_get_num_qpg_entry(enum cvmx_pki_qpg_qos qpg_qos)
+{
+	if (qpg_qos == CVMX_PKI_QPG_QOS_NONE)
+		return 1;
+	else if (qpg_qos == CVMX_PKI_QPG_QOS_VLAN || qpg_qos == CVMX_PKI_QPG_QOS_MPLS)
+		return 8;
+	else if (qpg_qos == CVMX_PKI_QPG_QOS_DSA_SRC) /*vinita_to_do for higig2*/
+		return 32;
+	else if (qpg_qos == CVMX_PKI_QPG_QOS_DIFFSERV || qpg_qos == CVMX_PKI_QPG_QOS_HIGIG)
+		return 64;
+	else {
+		cvmx_dprintf("ERROR: unrecognized qpg_qos = %d", qpg_qos);
+		return 0;
+	}
+}
+
+/**
+ * This function setups the qos table by allocating qpg entry and writing
+ * the provided parameters to that entry (offset).
+ * @param node	  node number.
+ * @param qpg_cfg       pointer to struct containing qpg configuration
+ */
+int cvmx_helper_pki_set_qpg_entry(int node, struct cvmx_pki_qpg_config *qpg_cfg)
+{
+	int offset;
+
+	offset = cvmx_pki_qpg_entry_alloc(node, qpg_cfg->qpg_base, 1);
+	if (pki_helper_debug)
+		cvmx_dprintf("at offset %d \n", offset);
+	if (offset == CVMX_RESOURCE_ALREADY_RESERVED) {
+		cvmx_dprintf("INFO:setup_qpg_table: offset %d already reserved\n", qpg_cfg->qpg_base);
+		return CVMX_RESOURCE_ALREADY_RESERVED;
+	} else if (offset == CVMX_RESOURCE_ALLOC_FAILED) {
+		cvmx_dprintf("ERROR:setup_qpg_table: no more entries available\n");
+		return CVMX_RESOURCE_ALLOC_FAILED;
+	}
+	qpg_cfg->qpg_base = offset;
+	cvmx_pki_write_qpg_entry(node, offset, qpg_cfg);
+	return offset;
+}
+
+/**
  * This function sets up aura QOS for RED, backpressure and tail-drop.
  *
  * @param node       node number.
@@ -625,7 +617,7 @@ void cvmx_helper_pki_no_dflt_init(int node)
  *			1: enable 0:disable
  * @param pass_thresh   pass threshold for RED.
  * @param drop_thresh   drop threshold for RED
- * @param ena_bp        enable backpressure based on [BP] level.
+ * @param ena_bp	enable backpressure based on [BP] level.
  *			1:enable 0:disable
  * @param bp_thresh     backpressure threshold.
  * @param ena_drop      enable tail drop.
@@ -636,8 +628,13 @@ int cvmx_helper_setup_aura_qos(int node, int aura, bool ena_red, bool ena_drop,
 			       uint64_t pass_thresh, uint64_t drop_thresh,
 			       bool ena_bp, uint64_t bp_thresh)
 {
+	cvmx_fpa3_gaura_t gaura;
+
+	/* FIXME: change upper-layer arguments to new handle types */
+	gaura = __cvmx_fpa3_gaura(node, aura);
+
 	ena_red = ena_red | ena_drop;
-	cvmx_fpa_setup_aura_qos(node, aura, ena_red, pass_thresh, drop_thresh,
+	cvmx_fpa3_setup_aura_qos(gaura, ena_red, pass_thresh, drop_thresh,
 				ena_bp, bp_thresh);
 	cvmx_pki_enable_aura_qos(node, aura, ena_red, ena_drop, ena_bp);
 	return 0;
@@ -647,726 +644,1113 @@ int cvmx_helper_setup_aura_qos(int node, int aura, bool ena_red, bool ena_drop,
  * This function maps specified bpid to all the auras from which it can receive bp and
  * then maps that bpid to all the channels, that bpid can asserrt bp on.
  *
- * @param node          node number.
- * @param aura          aura number which will back pressure specified bpid.
- * @param bpid          bpid to map.
+ * @param node	  node number.
+ * @param aura	  aura number which will back pressure specified bpid.
+ * @param bpid	  bpid to map.
  * @param chl_map       array of channels to map to that bpid.
  * @param chl_cnt	number of channel/ports to map to that bpid.
  * @return Zero on success. Negative on failure
  */
 int cvmx_helper_pki_map_aura_chl_bpid(int node, uint16_t aura, uint16_t bpid,
-                                      uint16_t chl_map[], uint16_t chl_cnt)
+				      uint16_t chl_map[], uint16_t chl_cnt)
 {
-        uint16_t channel;
-
-        if (aura >= CVMX_PKI_NUM_AURA) {
-                cvmx_dprintf("ERROR: aura %d is > supported in hw\n", aura);
-                return -1;
-        }
-        if (bpid >= CVMX_PKI_NUM_BPID) {
-                cvmx_dprintf("ERROR: bpid %d is > supported in hw\n", bpid);
-                return -1;
-        }
-        cvmx_pki_write_aura_bpid(node, aura, bpid);
-        while (chl_cnt--) {
-                channel = chl_map[chl_cnt];
-                if ( channel >= CVMX_PKI_NUM_CHANNEL) {
-                        cvmx_dprintf("ERROR: channel %d is > supported in hw\n", channel);
-                        return -1;
-                }
-                cvmx_pki_write_channel_bpid(node, channel, bpid);
-        }
-        return 0;
-}
+	uint16_t channel;
 
-int cvmx_helper_pki_port_shift(int xiface, enum cvmx_pki_qpg_qos qpg_qos)
-{
-        uint8_t num_qos;
-        cvmx_helper_interface_mode_t mode = cvmx_helper_interface_get_mode(xiface);
-
-        num_qos = cvmx_helper_pki_get_num_qpg_entry(qpg_qos);
-        if ((mode != CVMX_HELPER_INTERFACE_MODE_SGMII) &&
-             (mode != CVMX_HELPER_INTERFACE_MODE_NPI) &&
-             (mode != CVMX_HELPER_INTERFACE_MODE_LOOP)) {
-                return ffs(num_qos) - 1;
-             }
-             else if (num_qos <= 16)
-                     return 0;
-             else if (num_qos <= 32)
-                     return 1;
-             else
-                     return 2;
+	if (aura >= CVMX_PKI_NUM_AURA) {
+		cvmx_dprintf("ERROR: aura %d is > supported in hw\n", aura);
+		return -1;
+	}
+	if (bpid >= CVMX_PKI_NUM_BPID) {
+		cvmx_dprintf("ERROR: bpid %d is > supported in hw\n", bpid);
+		return -1;
+	}
+	cvmx_pki_write_aura_bpid(node, aura, bpid);
+	while (chl_cnt--) {
+		channel = chl_map[chl_cnt];
+		if (channel >= CVMX_PKI_NUM_CHANNEL) {
+			cvmx_dprintf("ERROR: channel %d is > supported in hw\n", channel);
+			return -1;
+		}
+		cvmx_pki_write_channel_bpid(node, channel, bpid);
+	}
+	return 0;
 }
 
-#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
-
-int cvmx_helper_pki_set_gbl_schd(int node, struct cvmx_pki_global_schd *gblsch)
+/** @INTERNAL
+ * This function returns the value of port shift required
+ * if all the ports on that interface are using same style and
+ * configuring qpg_qos != NONE
+ */
+int __cvmx_helper_pki_port_shift(int xiface, enum cvmx_pki_qpg_qos qpg_qos)
 {
-        int rs;
-
-        if (gblsch->setup_pool) {
-                if (pki_helper_debug)
-                        cvmx_dprintf("pki-helper:gbl setup global pool %d buff_size %d blocks %d\n",
-                                     gblsch->pool, (int)gblsch->pool_buff_size, (int)gblsch->pool_max_buff);
-                cvmx_helper_fpa3_init_pool(node, node, &gblsch->pool, gblsch->pool_buff_size,
-                                           gblsch->pool_max_buff, gblsch->pool_name);
-                if (pki_helper_debug)
-                        cvmx_dprintf("pool alloced is %d\n", gblsch->pool);
-        }
-        if (gblsch->setup_aura) {
-                if (pki_helper_debug)
-                        cvmx_dprintf("pki-helper:gbl setup global aura %d pool %d blocks %d\n",
-                                     gblsch->aura, gblsch->pool, (int)gblsch->aura_buff_cnt);
-                cvmx_helper_fpa3_add_aura_to_pool(node, gblsch->pool, &gblsch->aura,
-                                gblsch->aura_buff_cnt, NULL, gblsch->aura_name);
-                if (pki_helper_debug)
-                        cvmx_dprintf("aura alloced is %d\n", gblsch->aura);
-
-        }
-        if (gblsch->setup_sso_grp) {
-                rs = cvmx_sso_allocate_group(node);
-                if (rs < 0) {
-                        cvmx_dprintf("pki-helper:gbl ERROR: sso grp not available\n");
-                        return rs;
-                }
-                gblsch->sso_grp = rs;
-                if (pki_helper_debug)
-                        cvmx_dprintf("pki-helper:gbl: sso grp alloced is %d\n", gblsch->sso_grp);
-        }
-        return 0;
+	uint8_t num_qos;
+	cvmx_helper_interface_mode_t mode = cvmx_helper_interface_get_mode(xiface);
+
+	num_qos = cvmx_helper_pki_get_num_qpg_entry(qpg_qos);
+	if ((mode != CVMX_HELPER_INTERFACE_MODE_SGMII) &&
+	     (mode != CVMX_HELPER_INTERFACE_MODE_NPI) &&
+	     (mode != CVMX_HELPER_INTERFACE_MODE_LOOP)) {
+		return ffs(num_qos) - 1;
+	     } else if (num_qos <= 16)
+		     return 0;
+	     else if (num_qos <= 32)
+		     return 1;
+	     else
+		     return 2;
 }
-#endif /* CVMX_BUILD_FOR_LINUX_KERNEL */
+
 
 int __cvmx_helper_pki_qos_rsrcs(int node, struct cvmx_pki_qos_schd *qossch)
 {
 #ifndef CVMX_BUILD_FOR_LINUX_KERNEL
-        int rs;
-
-        /* Reserve pool resources */
-        if (qossch->pool_per_qos && qossch->pool < 0) {
-                if (pki_helper_debug)
-                        cvmx_dprintf("pki-helper:qos: setup pool %d buff_size %d blocks %d\n",
-                                     qossch->pool, (int)qossch->pool_buff_size, (int)qossch->pool_max_buff);
-                rs = cvmx_helper_fpa3_init_pool(node, node, &qossch->pool,
-                                qossch->pool_buff_size, qossch->pool_max_buff, qossch->pool_name);
-                if (rs < 0) {
-                        cvmx_dprintf("pki-helper:qos:ERROR: pool init failed\n");
-                        return rs;
-                }
-                if (pki_helper_debug)
-                        cvmx_dprintf("pool alloced is %d\n", qossch->pool);
-        }
-        /* Reserve aura resources */
-        if (qossch->aura_per_qos && qossch->aura < 0) {
-                if (pki_helper_debug)
-                        cvmx_dprintf("pki-helper:qos setup aura %d pool %d blocks %d\n",
-                                     qossch->aura, qossch->pool, (int)qossch->aura_buff_cnt);
-                rs = cvmx_helper_fpa3_add_aura_to_pool(node, qossch->pool,
-                                &qossch->aura, qossch->aura_buff_cnt, NULL, qossch->aura_name);
-                if (rs < 0) {
-                        cvmx_dprintf("pki-helper:qos:ERROR: aura init failed\n");
-                        return rs;
-                }
-                if (pki_helper_debug)
-                        cvmx_dprintf("aura alloced is %d\n", qossch->aura);
-        }
-        /* Reserve sso group resources */
-        if (qossch->sso_grp_per_qos && qossch->sso_grp < 0) {
-                rs = cvmx_sso_allocate_group(node);
-                if (rs < 0) {
-                        cvmx_dprintf("pki-helper:qos ERROR: sso grp not available\n");
-                        return rs;
-                }
-                qossch->sso_grp = rs;
-                if (pki_helper_debug)
-                        cvmx_dprintf("pki-helper:qos: sso grp alloced is %d\n", qossch->sso_grp);
-        }
+	int rs;
+
+	/* Reserve pool resources */
+	if (qossch->pool_per_qos && qossch->pool_num < 0) {
+		if (pki_helper_debug)
+			cvmx_dprintf("pki-helper:qos: setup pool %d buff_size %d blocks %d\n",
+				     qossch->pool_num, (int)qossch->pool_buff_size, (int)qossch->pool_max_buff);
+
+		qossch->_pool = cvmx_fpa3_setup_fill_pool(node,
+			qossch->pool_num, qossch->pool_name,
+			qossch->pool_buff_size,
+			qossch->pool_max_buff, NULL);
+
+		if (!__cvmx_fpa3_pool_valid(qossch->_pool)) {
+			cvmx_printf("ERROR: %s POOL %d init failed\n",
+				__func__, qossch->pool_num);
+			return -1;
+		}
+
+		qossch->pool_num = qossch->_pool.lpool;
+		if (pki_helper_debug)
+			cvmx_dprintf("pool alloced is %d\n", qossch->pool_num);
+	}
+	/* Reserve aura resources */
+	if (qossch->aura_per_qos && qossch->aura_num < 0) {
+		if (pki_helper_debug)
+			cvmx_dprintf("pki-helper:qos setup aura %d pool %d blocks %d\n",
+				     qossch->aura_num, qossch->pool_num,
+				     (int)qossch->aura_buff_cnt);
+
+		qossch->_aura = cvmx_fpa3_set_aura_for_pool(qossch->_pool,
+				qossch->aura_num,
+				qossch->aura_name,
+				qossch->pool_buff_size,
+				qossch->aura_buff_cnt);
+
+		if (!__cvmx_fpa3_aura_valid(qossch->_aura)) {
+			cvmx_printf("ERROR: %s AURA %d init failed\n",
+				__func__, qossch->aura_num);
+			return -1;
+		}
+
+		qossch->aura_num = qossch->_aura.laura;
+		if (pki_helper_debug)
+			cvmx_dprintf("aura alloced is %d\n", qossch->aura_num);
+	}
+	/* Reserve sso group resources */
+	if (qossch->sso_grp_per_qos && qossch->sso_grp < 0) {
+		rs = cvmx_sso_allocate_group(node);
+		if (rs < 0) {
+			cvmx_dprintf("pki-helper:qos ERROR: sso grp not available\n");
+			return rs;
+		}
+		qossch->sso_grp = rs;
+		if (pki_helper_debug)
+			cvmx_dprintf("pki-helper:qos: sso grp alloced is %d\n", qossch->sso_grp);
+	}
 #endif /* CVMX_BUILD_FOR_LINUX_KERNEL */
-        return 0;
+	return 0;
 }
 
 int __cvmx_helper_pki_port_rsrcs(int node, struct cvmx_pki_prt_schd *prtsch)
 {
 #ifndef CVMX_BUILD_FOR_LINUX_KERNEL
-        int rs;
-
-        /* Reserve pool resources */
-        if (prtsch->pool_per_prt && prtsch->pool < 0) {
-                if (pki_helper_debug)
-                        cvmx_dprintf("pki-helper:port setup pool %d buff_size %d blocks %d\n",
-                                     prtsch->pool, (int)prtsch->pool_buff_size, (int)prtsch->pool_max_buff);
-                rs = cvmx_helper_fpa3_init_pool(node, node, &prtsch->pool, prtsch->pool_buff_size,
-                                prtsch->pool_max_buff, prtsch->pool_name);
-                if (rs < 0) {
-                        cvmx_dprintf("pki-helper:port:ERROR: pool init failed\n");
-                        return rs;
-                }
-                if (pki_helper_debug)
-                        cvmx_dprintf("pool alloced is %d\n", prtsch->pool);
-        }
-        /* Reserve aura resources */
-        if (prtsch->aura_per_prt && prtsch->aura < 0) {
-                if (pki_helper_debug)
-                        cvmx_dprintf("pki-helper:port setup aura %d pool %d blocks %d\n",
-                                     prtsch->aura, prtsch->pool, (int)prtsch->aura_buff_cnt);
-                rs = cvmx_helper_fpa3_add_aura_to_pool(node, prtsch->pool,
-                                &prtsch->aura, prtsch->aura_buff_cnt, NULL, prtsch->aura_name);
-                if (rs < 0) {
-                        cvmx_dprintf("pki-helper:port:ERROR: aura init failed\n");
-                        return rs;
-                }
-                if (pki_helper_debug)
-                        cvmx_dprintf("aura alloced is %d\n", prtsch->aura);
-        }
-        /* Reserve sso group resources */
-        if (prtsch->sso_grp_per_prt && prtsch->sso_grp < 0) {
-                rs = cvmx_sso_allocate_group(node);
-                if (rs < 0) {
-                        cvmx_dprintf("pki-helper:port:ERROR: sso grp not available\n");
-                        return rs;
-                }
-                prtsch->sso_grp = rs;
-                if (pki_helper_debug)
-                        cvmx_dprintf("pki-helper:port: sso grp alloced is %d\n", prtsch->sso_grp);
-        }
+	int rs;
+
+	/* Reserve pool resources */
+	if (prtsch->pool_per_prt && prtsch->pool_num < 0) {
+		if (pki_helper_debug)
+			cvmx_dprintf("pki-helper:port setup pool %d buff_size %d blocks %d\n",
+				     prtsch->pool_num, (int)prtsch->pool_buff_size, (int)prtsch->pool_max_buff);
+
+		prtsch->_pool = cvmx_fpa3_setup_fill_pool(node,
+			prtsch->pool_num, prtsch->pool_name,
+			prtsch->pool_buff_size, prtsch->pool_max_buff, NULL);
+
+		if (!__cvmx_fpa3_pool_valid(prtsch->_pool)) {
+			cvmx_printf("ERROR: %s: POOL %d init failed\n",
+				__func__, prtsch->pool_num);
+			return -1;
+		}
+		prtsch->pool_num = prtsch->_pool.lpool;
+		if (pki_helper_debug)
+			cvmx_dprintf("pool alloced is %d\n", prtsch->pool_num);
+	}
+	/* Reserve aura resources */
+	if (prtsch->aura_per_prt && prtsch->aura_num < 0) {
+		if (pki_helper_debug)
+			cvmx_dprintf("pki-helper:port setup aura %d pool %d blocks %d\n",
+				     prtsch->aura_num, prtsch->pool_num, (int)prtsch->aura_buff_cnt);
+		prtsch->_aura = cvmx_fpa3_set_aura_for_pool(prtsch->_pool,
+			prtsch->aura_num, prtsch->aura_name,
+			prtsch->pool_buff_size,
+			prtsch->aura_buff_cnt);
+
+		if (!__cvmx_fpa3_aura_valid(prtsch->_aura)) {
+			cvmx_printf("ERROR: %s: AURA %d init failed\n",
+				__func__, prtsch->aura_num);
+			return -1;
+		}
+
+		prtsch->aura_num = prtsch->_aura.laura;
+
+		if (pki_helper_debug)
+			cvmx_dprintf("aura alloced is %d\n", prtsch->aura_num);
+	}
+	/* Reserve sso group resources */
+	if (prtsch->sso_grp_per_prt && prtsch->sso_grp < 0) {
+		rs = cvmx_sso_allocate_group(node);
+		if (rs < 0) {
+			cvmx_dprintf("pki-helper:port:ERROR: sso grp not available\n");
+			return rs;
+		}
+		prtsch->sso_grp = rs;
+		if (pki_helper_debug)
+			cvmx_dprintf("pki-helper:port: sso grp alloced is %d\n", prtsch->sso_grp);
+	}
 #endif /* CVMX_BUILD_FOR_LINUX_KERNEL */
-        return 0;
+	return 0;
 }
+
 int __cvmx_helper_pki_intf_rsrcs(int node, struct cvmx_pki_intf_schd *intf)
 {
 #ifndef CVMX_BUILD_FOR_LINUX_KERNEL
-        int rs;
-
-        if (intf->pool_per_intf && intf->pool < 0) {
-                if (pki_helper_debug)
-                        cvmx_dprintf("pki-helper:intf: setup pool %d buff_size %d blocks %d\n",
-                                     intf->pool, (int)intf->pool_buff_size, (int)intf->pool_max_buff);
-                rs = cvmx_helper_fpa3_init_pool(node, node, &intf->pool, intf->pool_buff_size,
-                                intf->pool_max_buff, intf->pool_name);
-                if (rs == CVMX_RESOURCE_ALLOC_FAILED)
-                        return -1;
-                if (pki_helper_debug)
-                        cvmx_dprintf("pool alloced is %d\n", intf->pool);
-
-        }
-        if (intf->aura_per_intf && intf->aura < 0) {
-                if (pki_helper_debug)
-                        cvmx_dprintf("pki-helper:intf: setup aura %d pool %d blocks %d\n",
-                                     intf->aura, intf->pool, (int)intf->aura_buff_cnt);
-                rs = cvmx_helper_fpa3_add_aura_to_pool(node, intf->pool, &intf->aura,
-                                intf->aura_buff_cnt, NULL, intf->aura_name);
-                if (rs == CVMX_RESOURCE_ALLOC_FAILED)
-                        return -1;
-                if (pki_helper_debug)
-                        cvmx_dprintf("aura alloced is %d\n", intf->aura);
-        }
-        if (intf->sso_grp_per_intf && intf->sso_grp < 0) {
-                rs = cvmx_sso_allocate_group(node);
-                if (rs < 0) {
-                        cvmx_dprintf("pki-helper:intf:ERROR: sso grp not available\n");
-                        return rs;
-                }
-                intf->sso_grp = rs;
-        }
+	int rs;
+
+	if (intf->pool_per_intf && intf->pool_num < 0) {
+		if (pki_helper_debug)
+			cvmx_dprintf("pki-helper:intf: setup pool %d buff_size %d blocks %d\n",
+				     intf->pool_num, (int)intf->pool_buff_size, (int)intf->pool_max_buff);
+		intf->_pool = cvmx_fpa3_setup_fill_pool(node, intf->pool_num,
+			intf->pool_name, intf->pool_buff_size,
+			intf->pool_max_buff, NULL);
+
+		if (!__cvmx_fpa3_pool_valid(intf->_pool)) {
+			cvmx_printf("ERROR: %s: POOL %d init failed\n",
+				__func__, intf->pool_num);
+			return -1;
+		}
+		intf->pool_num = intf->_pool.lpool;
+
+		if (pki_helper_debug)
+			cvmx_dprintf("pool alloced is %d\n", intf->pool_num);
+
+	}
+	if (intf->aura_per_intf && intf->aura_num < 0) {
+		if (pki_helper_debug)
+			cvmx_dprintf("pki-helper:intf: setup aura %d pool %d blocks %d\n",
+			     intf->aura_num, intf->pool_num, (int)intf->aura_buff_cnt);
+		intf->_aura = cvmx_fpa3_set_aura_for_pool(intf->_pool,
+			intf->aura_num, intf->aura_name,
+			intf->pool_buff_size,
+			intf->aura_buff_cnt);
+
+		if (!__cvmx_fpa3_aura_valid(intf->_aura)) {
+			cvmx_printf("ERROR: %s: AURA %d init failed\n",
+				__func__, intf->aura_num);
+
+			return -1;
+		}
+
+		intf->aura_num = intf->_aura.laura;
+
+		if (pki_helper_debug)
+			cvmx_dprintf("aura alloced is %d\n", intf->aura_num);
+	}
+	if (intf->sso_grp_per_intf && intf->sso_grp < 0) {
+		rs = cvmx_sso_allocate_group(node);
+		if (rs < 0) {
+			cvmx_dprintf("pki-helper:intf:ERROR: sso grp not available\n");
+			return rs;
+		}
+		intf->sso_grp = rs;
+	}
 #endif /* CVMX_BUILD_FOR_LINUX_KERNEL */
-        return 0;
+	return 0;
 }
 
 int __cvmx_helper_pki_set_intf_qpg(int node, int port, int qpg_base, int num_entry,
-                                   struct cvmx_pki_intf_schd *intfsch)
+			   struct cvmx_pki_intf_schd *intfsch)
 {
-        int offset;
-        int entry;
-        int port_add, aura, grp_ok, grp_bad;
-
-        if (pki_helper_debug)
-                cvmx_dprintf("pki-helper:intf_qpg port %d qpg_base %d num_entry %d",
-                             port, qpg_base, num_entry);
-        offset = cvmx_pki_qpg_entry_alloc(node, qpg_base, num_entry);
-        if (offset == CVMX_RESOURCE_ALREADY_RESERVED) {
-                cvmx_dprintf("pki-helper: INFO: qpg entries will be shared\n");
-                return offset;
-        } else if (offset == CVMX_RESOURCE_ALLOC_FAILED) {
-                cvmx_dprintf("pki-helper: ERROR: qpg entries not available\n");
-                return offset;
-        } else if (intfsch->qpg_base < 0)
-                intfsch->qpg_base = offset;
-                if (pki_helper_debug)
-                        cvmx_dprintf("qpg_base allocated is %d\n",offset);
-                for (entry = 0; entry < num_entry; entry++) {
-                        port_add = intfsch->prt_s[port].qos_s[entry].port_add;
-                        aura = intfsch->prt_s[port].qos_s[entry].aura;
-                        grp_ok = intfsch->prt_s[port].qos_s[entry].sso_grp;
-                        grp_bad = intfsch->prt_s[port].qos_s[entry].sso_grp;
-                        cvmx_pki_write_qpg_entry(node, (offset + entry),
-                                        port_add, aura, grp_ok, grp_bad);
-                }
-                return offset;
+	int offset;
+	int entry;
+	struct cvmx_pki_qpg_config qpg_cfg;
+
+	memset(&qpg_cfg, 0, sizeof(qpg_cfg));
+	if (pki_helper_debug)
+		cvmx_dprintf("pki-helper:intf_qpg port %d qpg_base %d num_entry %d",
+			     port, qpg_base, num_entry);
+	offset = cvmx_pki_qpg_entry_alloc(node, qpg_base, num_entry);
+	if (offset == CVMX_RESOURCE_ALREADY_RESERVED) {
+		cvmx_dprintf("pki-helper: INFO: qpg entries will be shared\n");
+		return offset;
+	} else if (offset == CVMX_RESOURCE_ALLOC_FAILED) {
+		cvmx_dprintf("pki-helper: ERROR: qpg entries not available\n");
+		return offset;
+	} else if (intfsch->qpg_base < 0)
+		intfsch->qpg_base = offset;
+	if (pki_helper_debug)
+		cvmx_dprintf("qpg_base allocated is %d\n", offset);
+	for (entry = 0; entry < num_entry; entry++) {
+		qpg_cfg.port_add = intfsch->prt_s[port].qos_s[entry].port_add;
+		qpg_cfg.aura_num = intfsch->prt_s[port].qos_s[entry].aura_num;
+		qpg_cfg.grp_ok = intfsch->prt_s[port].qos_s[entry].sso_grp;
+		qpg_cfg.grp_bad = intfsch->prt_s[port].qos_s[entry].sso_grp;
+		cvmx_pki_write_qpg_entry(node, (offset + entry), &qpg_cfg);
+	}
+	return offset;
 }
 
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+/**
+ * This function sets up the global pool, aura and sso group
+ * resources which application can use between any interfaces
+ * and ports.
+ * @param node          node number
+ * @param gblsch        pointer to struct containing global
+ *                      scheduling parameters.
+ */
+int cvmx_helper_pki_set_gbl_schd(int node, struct cvmx_pki_global_schd *gblsch)
+{
+	int rs;
+
+	if (gblsch->setup_pool) {
+		if (pki_helper_debug)
+			cvmx_dprintf("%s: gbl setup global pool %d buff_size %d blocks %d\n",
+				__func__, gblsch->pool_num,
+				(int)gblsch->pool_buff_size,
+				(int)gblsch->pool_max_buff);
+
+		gblsch->_pool =
+		    cvmx_fpa3_setup_fill_pool(node, gblsch->pool_num,
+			gblsch->pool_name, gblsch->pool_buff_size,
+			gblsch->pool_max_buff, NULL);
+
+		if (!__cvmx_fpa3_pool_valid(gblsch->_pool)) {
+			cvmx_printf("ERROR: %s: POOL %u:%d unavailable\n",
+				__func__, node, gblsch->pool_num);
+			return -1;
+		}
+
+		gblsch->pool_num = gblsch->_pool.lpool;
 
+		if (pki_helper_debug)
+			cvmx_dprintf("pool alloced is %d\n", gblsch->pool_num);
+	}
+	if (gblsch->setup_aura) {
+		if (pki_helper_debug)
+			cvmx_dprintf("%s: gbl setup global aura %d pool %d blocks %d\n",
+				__func__, gblsch->aura_num, gblsch->pool_num,
+				(int)gblsch->aura_buff_cnt);
+
+		gblsch->_aura = cvmx_fpa3_set_aura_for_pool(gblsch->_pool,
+			gblsch->aura_num,
+			gblsch->aura_name,
+			gblsch->pool_buff_size,
+			gblsch->aura_buff_cnt);
+
+		if (!__cvmx_fpa3_aura_valid(gblsch->_aura)) {
+			cvmx_printf("ERROR: %s: AURA %u:%d unavailable\n",
+				__func__, node, gblsch->aura_num);
+			return -1;
+		}
+
+		gblsch->aura_num = gblsch->_aura.laura;
+
+		if (pki_helper_debug)
+			cvmx_dprintf("aura alloced is %d\n", gblsch->aura_num);
+
+	}
+	if (gblsch->setup_sso_grp) {
+		rs = cvmx_sso_allocate_group(node);
+		if (rs < 0) {
+			cvmx_dprintf("pki-helper:gbl ERROR: sso grp not available\n");
+			return rs;
+		}
+		gblsch->sso_grp = rs;
+		if (pki_helper_debug)
+			cvmx_dprintf("pki-helper:gbl: sso grp alloced is %d\n", gblsch->sso_grp);
+	}
+	return 0;
+}
+#endif /* ! CVMX_BUILD_FOR_LINUX_KERNEL */
+
+/**
+ * This function sets up scheduling parameters (pool, aura, sso group etc)
+ * of an ipd port.
+ * @param ipd_port      ipd port number
+ * @param prtsch        pointer to struct containing port's
+ *                      scheduling parameters.
+ */
 int cvmx_helper_pki_init_port(int ipd_port, struct cvmx_pki_prt_schd *prtsch)
 {
-        int num_qos;
-        int qos;
-        struct cvmx_pki_qos_schd *qossch;
-        struct cvmx_pki_style_config style_cfg;
-        struct cvmx_pki_pkind_config pknd_cfg;
-        int xiface = cvmx_helper_get_interface_num(ipd_port);
-        int pknd;
-        uint16_t mbuff_size;
-        int rs;
-
-        struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(ipd_port);
-        num_qos = cvmx_helper_pki_get_num_qpg_entry(prtsch->qpg_qos);
-        mbuff_size = prtsch->pool_buff_size;
-
-        /* Reserve port resources */
-        rs = __cvmx_helper_pki_port_rsrcs(xp.node, prtsch);
-        if (rs)
-                return rs;
-        /* Reserve qpg resources */
-        if (prtsch->qpg_base < 0) {
-                rs = cvmx_pki_qpg_entry_alloc(xp.node, prtsch->qpg_base, num_qos);
-                if (rs < 0) {
-                        cvmx_dprintf("pki-helper:port:ERROR: qpg entries not available\n");
-                        return CVMX_RESOURCE_ALLOC_FAILED;
-                }
-                prtsch->qpg_base = rs;
-                if (pki_helper_debug)
-                        cvmx_dprintf("port %d qpg_base %d allocated\n",ipd_port, prtsch->qpg_base);
-        }
-
-        if (prtsch->qpg_qos) {
-                for (qos = 0; qos < num_qos; qos++) {
-                        qossch = &prtsch->qos_s[qos];
-                        if (!qossch->pool_per_qos)
-                                qossch->pool = prtsch->pool;
-                        else if (qossch->pool_buff_size < mbuff_size)
-                                mbuff_size = qossch->pool_buff_size;
-                        if (!qossch->aura_per_qos)
-                                qossch->aura = prtsch->aura;
-                        if (!qossch->sso_grp_per_qos)
-                                qossch->sso_grp = prtsch->sso_grp;
-
-                        /* Reserve qos resources */
-                        rs = __cvmx_helper_pki_qos_rsrcs(xp.node, qossch);
-                        if (rs)
-                                return rs;
-                        cvmx_pki_write_qpg_entry(xp.node, prtsch->qpg_base + qos, qossch->port_add, qossch->aura,
-                                        qossch->sso_grp, qossch->sso_grp);
-                        if (pki_helper_debug)
-                                cvmx_dprintf("port %d qos %d has port_add %d aura %d grp %d\n",
-                                             ipd_port, qos, qossch->port_add, qossch->aura, qossch->sso_grp);
-                }
-        } else
-                cvmx_pki_write_qpg_entry(xp.node, prtsch->qpg_base, 0,
-                                         prtsch->aura, prtsch->sso_grp, prtsch->sso_grp);
-
-                /* Allocate style here and map it to the port */
-                rs = cvmx_pki_style_alloc(xp.node, prtsch->style);
-                if (rs == CVMX_RESOURCE_ALREADY_RESERVED) {
-                        cvmx_dprintf("pki-helper: INFO: style will be shared\n");
-                } else if (rs == CVMX_RESOURCE_ALLOC_FAILED) {
-                        cvmx_dprintf("pki-helper: ERROR: style not available\n");
-                        return CVMX_RESOURCE_ALLOC_FAILED;
-                } else {
-                        prtsch->style = rs;
-                        if (pki_helper_debug)
-                                cvmx_dprintf("port %d has style %d\n", ipd_port,prtsch->style);
-                        style_cfg = pki_dflt_style[xp.node];
-                        style_cfg.parm_cfg.qpg_qos = prtsch->qpg_qos;
-                        style_cfg.parm_cfg.qpg_base = prtsch->qpg_base;
-                        style_cfg.parm_cfg.qpg_port_msb = 0;
-                        style_cfg.parm_cfg.qpg_port_sh = 0;
-                        style_cfg.parm_cfg.mbuff_size = mbuff_size;
-                        cvmx_pki_set_style_config(xp.node, prtsch->style,
-                                        CVMX_PKI_CLUSTER_ALL, &style_cfg);
-                }
-                pknd = cvmx_helper_get_pknd(xiface, cvmx_helper_get_interface_index_num(ipd_port));
-                cvmx_pki_get_pkind_config(xp.node, pknd, &pknd_cfg);
-                pknd_cfg.initial_style = prtsch->style;
-                pknd_cfg.fcs_pres = __cvmx_helper_get_has_fcs(xiface);
-                cvmx_pki_set_pkind_config(xp.node, pknd, &pknd_cfg);
-        //cvmx_pki_show_port(xp.node, xiface, cvmx_helper_get_interface_index_num(ipd_port));
-
-                return 0;
+	int num_qos;
+	int qos;
+	struct cvmx_pki_qpg_config qpg_cfg;
+	struct cvmx_pki_qos_schd *qossch;
+	struct cvmx_pki_style_config style_cfg;
+	struct cvmx_pki_pkind_config pknd_cfg;
+	int xiface = cvmx_helper_get_interface_num(ipd_port);
+	int pknd;
+	uint16_t mbuff_size;
+	int rs;
+
+	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(ipd_port);
+	num_qos = cvmx_helper_pki_get_num_qpg_entry(prtsch->qpg_qos);
+	mbuff_size = prtsch->pool_buff_size;
+	memset(&qpg_cfg, 0, sizeof(qpg_cfg));
+
+	/* Reserve port resources */
+	rs = __cvmx_helper_pki_port_rsrcs(xp.node, prtsch);
+	if (rs)
+		return rs;
+	/* Reserve qpg resources */
+	if (prtsch->qpg_base < 0) {
+		rs = cvmx_pki_qpg_entry_alloc(xp.node, prtsch->qpg_base, num_qos);
+		if (rs < 0) {
+			cvmx_dprintf("pki-helper:port:ERROR: qpg entries not available\n");
+			return CVMX_RESOURCE_ALLOC_FAILED;
+		}
+		prtsch->qpg_base = rs;
+		if (pki_helper_debug)
+			cvmx_dprintf("port %d qpg_base %d allocated\n",
+				ipd_port, prtsch->qpg_base);
+	}
+
+	if (prtsch->qpg_qos) {
+		for (qos = 0; qos < num_qos; qos++) {
+			qossch = &prtsch->qos_s[qos];
+			if (!qossch->pool_per_qos)
+				qossch->pool_num = prtsch->pool_num;
+			else if (qossch->pool_buff_size < mbuff_size)
+				mbuff_size = qossch->pool_buff_size;
+			if (!qossch->aura_per_qos)
+				qossch->aura_num = prtsch->aura_num;
+			if (!qossch->sso_grp_per_qos)
+				qossch->sso_grp = prtsch->sso_grp;
+
+			/* Reserve qos resources */
+			rs = __cvmx_helper_pki_qos_rsrcs(xp.node, qossch);
+			if (rs)
+				return rs;
+			qpg_cfg.port_add = qossch->port_add;
+			qpg_cfg.aura_num = qossch->aura_num;
+			qpg_cfg.grp_ok = qossch->sso_grp;
+			qpg_cfg.grp_bad = qossch->sso_grp;
+			cvmx_pki_write_qpg_entry(xp.node, prtsch->qpg_base + qos, &qpg_cfg);
+			if (pki_helper_debug)
+				cvmx_dprintf("port %d qos %d has port_add %d aura %d grp %d\n",
+				ipd_port, qos, qossch->port_add,
+				qossch->aura_num, qossch->sso_grp);
+		}
+	} else
+		qpg_cfg.port_add = 0;
+		qpg_cfg.aura_num = prtsch->aura_num;
+		qpg_cfg.grp_ok = prtsch->sso_grp;
+		qpg_cfg.grp_bad = prtsch->sso_grp;
+		cvmx_pki_write_qpg_entry(xp.node, prtsch->qpg_base, &qpg_cfg);
+		/* Allocate style here and map it to the port */
+		rs = cvmx_pki_style_alloc(xp.node, prtsch->style);
+		if (rs == CVMX_RESOURCE_ALREADY_RESERVED) {
+			cvmx_dprintf("pki-helper: INFO: style will be shared\n");
+		} else if (rs == CVMX_RESOURCE_ALLOC_FAILED) {
+			cvmx_dprintf("pki-helper: ERROR: style not available\n");
+			return CVMX_RESOURCE_ALLOC_FAILED;
+		} else {
+			prtsch->style = rs;
+			if (pki_helper_debug)
+				cvmx_dprintf("port %d has style %d\n",
+					ipd_port, prtsch->style);
+			style_cfg = pki_dflt_style[xp.node];
+			style_cfg.parm_cfg.qpg_qos = prtsch->qpg_qos;
+			style_cfg.parm_cfg.qpg_base = prtsch->qpg_base;
+			style_cfg.parm_cfg.qpg_port_msb = 0;
+			style_cfg.parm_cfg.qpg_port_sh = 0;
+			style_cfg.parm_cfg.mbuff_size = mbuff_size;
+			cvmx_pki_write_style_config(xp.node, prtsch->style,
+				CVMX_PKI_CLUSTER_ALL, &style_cfg);
+		}
+		pknd = cvmx_helper_get_pknd(xiface, cvmx_helper_get_interface_index_num(ipd_port));
+		cvmx_pki_read_pkind_config(xp.node, pknd, &pknd_cfg);
+		pknd_cfg.initial_style = prtsch->style;
+		pknd_cfg.fcs_pres = __cvmx_helper_get_has_fcs(xiface);
+		cvmx_pki_write_pkind_config(xp.node, pknd, &pknd_cfg);
+		return 0;
 }
 EXPORT_SYMBOL(cvmx_helper_pki_init_port);
 
-int cvmx_helper_pki_init_interface(const int xiface,
-                                   struct cvmx_pki_intf_schd *intf, struct cvmx_pki_global_schd *gbl_schd)
+/**
+ * This function sets up scheduling parameters (pool, aura, sso group etc)
+ * of an interface (all ports/channels on that interface).
+ * @param xiface        interface number with node.
+ * @param intf_sch      pointer to struct containing interface
+ *                      scheduling parameters.
+ * @param gbl_sch       pointer to struct containing global scheduling parameters
+ *                      (can be NULL if not used)
+ */
+int cvmx_helper_pki_init_interface(const int xiface, struct cvmx_pki_intf_schd *intfsch,
+			    struct cvmx_pki_global_schd *gblsch)
 {
-        const uint16_t num_ports = cvmx_helper_ports_on_interface(xiface);
-        uint8_t qos;
-        uint16_t port = num_ports;
-        uint8_t port_msb = 0;
-        uint8_t port_shift = 0;
-        uint16_t num_entry = 0;
-        uint8_t num_qos;
-        int pknd;
-        int rs;
-        int has_fcs;
-        int ipd_port;
-        int qpg_base;
-        uint64_t mbuff_size = 0;
-        struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
-        enum cvmx_pki_qpg_qos qpg_qos = CVMX_PKI_QPG_QOS_NONE;
-        struct cvmx_pki_prt_schd *prtsch;
-        struct cvmx_pki_qos_schd *qossch;
-        struct cvmx_pki_style_config style_cfg;
-        struct cvmx_pki_pkind_config pknd_cfg;
-
-        has_fcs = __cvmx_helper_get_has_fcs(xiface);
-
-        if (!intf->pool_per_intf) {
-                intf->pool = gbl_schd->pool;
-        } else
-                mbuff_size = intf->pool_buff_size;
-                if (!intf->aura_per_intf)
-                        intf->aura = gbl_schd->aura;
-                if (!intf->sso_grp_per_intf)
-                        intf->sso_grp = gbl_schd->sso_grp;
-
-                /* Allocate interface resources */
-                rs = __cvmx_helper_pki_intf_rsrcs(xi.node, intf);
-                if (rs)
-                        return rs;
-
-                for (port = 0; port < num_ports; port++) {
-                        prtsch = &intf->prt_s[port];
-
-                        /* Skip invalid/disabled ports */
-                        if (!cvmx_helper_is_port_valid(xiface, port) || prtsch->cfg_port)
-                                continue;
-
-                        if (!prtsch->pool_per_prt)
-                                prtsch->pool = intf->pool;
-                        else if (prtsch->pool_buff_size < mbuff_size || !mbuff_size)
-                                mbuff_size = prtsch->pool_buff_size;
-                        if (!prtsch->aura_per_prt)
-                                prtsch->aura = intf->aura;
-                        if (!prtsch->sso_grp_per_prt)
-                                prtsch->sso_grp = intf->sso_grp;
-
-                        rs = __cvmx_helper_pki_port_rsrcs(xi.node, prtsch);
-                        if (rs)
-                                return rs;
-
-                        /* Port is using qpg qos to schedule packets to differnet aura or sso group */
-                        num_qos = cvmx_helper_pki_get_num_qpg_entry(prtsch->qpg_qos);
-
-                        /* All ports will share the aura from port 0 for the respective qos */
-                        /* Port 0 should never have this set to TRUE **/
-                        if (intf->qos_share_aura && (port != 0)) {
-                                if (pki_helper_debug)
-                                        cvmx_dprintf("All ports will share same aura for all qos\n");
-                                for (qos = 0; qos < num_qos; qos++) {
-                                        qossch = &prtsch->qos_s[qos];
-                                        prtsch->qpg_qos = intf->prt_s[0].qpg_qos;
-                                        qossch->pool_per_qos = intf->prt_s[0].qos_s[qos].pool_per_qos;
-                                        qossch->aura_per_qos = intf->prt_s[0].qos_s[qos].aura_per_qos;
-                                        qossch->pool = intf->prt_s[0].qos_s[qos].pool;
-                                        qossch->aura = intf->prt_s[0].qos_s[qos].aura;
-                                }
-
-                        }
-                        if (intf->qos_share_grp && (port != 0)) {
-                                if (pki_helper_debug)
-                                        cvmx_dprintf("All ports will share same sso group for all qos\n");
-                                for (qos = 0; qos < num_qos; qos++) {
-                                        qossch = &prtsch->qos_s[qos];
-                                        qossch->sso_grp_per_qos = intf->prt_s[0].qos_s[qos].sso_grp_per_qos;
-                                        qossch->sso_grp = intf->prt_s[0].qos_s[qos].sso_grp;
-                                }
-                        }
-                        for (qos = 0; qos < num_qos; qos++) {
-                                qossch = &prtsch->qos_s[qos];
-                                if (!qossch->pool_per_qos) {
-                                        qossch->pool = prtsch->pool;
-                                //cvmx_dprintf("qos %d pool %d\n", qos, prtsch->pool);
-                                }
-                                else if (qossch->pool_buff_size < mbuff_size || !mbuff_size)
-                                        mbuff_size = qossch->pool_buff_size;
-                                if (!qossch->aura_per_qos)
-                                        qossch->aura = prtsch->aura;
-                                if (!qossch->sso_grp_per_qos)
-                                        qossch->sso_grp = prtsch->sso_grp;
-                                rs = __cvmx_helper_pki_qos_rsrcs(xi.node, qossch);
-                                if (rs)
-                                        return rs;
-                        }
-                }
+	const uint16_t num_ports = cvmx_helper_ports_on_interface(xiface);
+	uint8_t qos;
+	uint16_t port = num_ports;
+	uint8_t port_msb = 0;
+	uint8_t port_shift = 0;
+	uint16_t num_entry = 0;
+	uint8_t num_qos;
+	int pknd;
+	int rs;
+	int has_fcs;
+	int ipd_port;
+	int qpg_base;
+	uint64_t mbuff_size = 0;
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	enum cvmx_pki_qpg_qos qpg_qos = CVMX_PKI_QPG_QOS_NONE;
+	struct cvmx_pki_qpg_config qpg_cfg;
+	struct cvmx_pki_prt_schd *prtsch;
+	struct cvmx_pki_qos_schd *qossch;
+	struct cvmx_pki_style_config style_cfg;
+	struct cvmx_pki_pkind_config pknd_cfg;
+
+	has_fcs = __cvmx_helper_get_has_fcs(xiface);
+	memset(&qpg_cfg, 0, sizeof(qpg_cfg));
+
+	if (!intfsch->pool_per_intf) {
+		if (gblsch != NULL) {
+			intfsch->_pool = gblsch->_pool;
+			intfsch->pool_num = gblsch->pool_num;
+		} else {
+			cvmx_dprintf("ERROR: global scheduling is in use but is NULL\n");
+			return -1;
+		}
+	} else {
+		if (intfsch == NULL) {
+			cvmx_dprintf("ERROR: interface scheduling pointer is NULL\n");
+			return -1;
+		}
+		mbuff_size = intfsch->pool_buff_size;
+	}
+	if (!intfsch->aura_per_intf) {
+		intfsch->_aura = gblsch->_aura;
+		intfsch->aura_num = gblsch->aura_num;
+	}
+	if (!intfsch->sso_grp_per_intf)
+		intfsch->sso_grp = gblsch->sso_grp;
+
+	/* Allocate interface resources */
+	rs = __cvmx_helper_pki_intf_rsrcs(xi.node, intfsch);
+	if (rs)
+		return rs;
+
+	for (port = 0; port < num_ports; port++) {
+		prtsch = &intfsch->prt_s[port];
+
+		/* Skip invalid/disabled ports */
+		if (!cvmx_helper_is_port_valid(xiface, port) || prtsch->cfg_port)
+			continue;
+
+		if (!prtsch->pool_per_prt) {
+			prtsch->pool_num = intfsch->pool_num;
+			prtsch->_pool = intfsch->_pool;
+		} else if (prtsch->pool_buff_size < mbuff_size || !mbuff_size)
+			mbuff_size = prtsch->pool_buff_size;
+		if (!prtsch->aura_per_prt) {
+			prtsch->aura_num = intfsch->aura_num;
+			prtsch->_aura = intfsch->_aura;
+		}
+		if (!prtsch->sso_grp_per_prt)
+			prtsch->sso_grp = intfsch->sso_grp;
+
+		rs = __cvmx_helper_pki_port_rsrcs(xi.node, prtsch);
+		if (rs)
+			return rs;
+
+		/* Port is using qpg qos to schedule packets to differnet aura or sso group */
+		num_qos = cvmx_helper_pki_get_num_qpg_entry(prtsch->qpg_qos);
+
+		/* All ports will share the aura from port 0 for the respective qos */
+		/* Port 0 should never have this set to TRUE **/
+		if (intfsch->qos_share_aura && (port != 0)) {
+			if (pki_helper_debug)
+				cvmx_dprintf("All ports will share same aura for all qos\n");
+			for (qos = 0; qos < num_qos; qos++) {
+				qossch = &prtsch->qos_s[qos];
+				prtsch->qpg_qos = intfsch->prt_s[0].qpg_qos;
+				qossch->pool_per_qos = intfsch->prt_s[0].qos_s[qos].pool_per_qos;
+				qossch->aura_per_qos = intfsch->prt_s[0].qos_s[qos].aura_per_qos;
+				qossch->pool_num = intfsch->prt_s[0].qos_s[qos].pool_num;
+				qossch->_pool = intfsch->prt_s[0].qos_s[qos]._pool;
+				qossch->aura_num = intfsch->prt_s[0].qos_s[qos].aura_num;
+				qossch->_aura = intfsch->prt_s[0].qos_s[qos]._aura;
+			}
+
+		}
+		if (intfsch->qos_share_grp && (port != 0)) {
+			if (pki_helper_debug)
+				cvmx_dprintf("All ports will share same sso group for all qos\n");
+			for (qos = 0; qos < num_qos; qos++) {
+				qossch = &prtsch->qos_s[qos];
+				qossch->sso_grp_per_qos =
+					intfsch->prt_s[0].qos_s[qos].sso_grp_per_qos;
+				qossch->sso_grp =
+					intfsch->prt_s[0].qos_s[qos].sso_grp;
+			}
+		}
+		for (qos = 0; qos < num_qos; qos++) {
+			qossch = &prtsch->qos_s[qos];
+			if (!qossch->pool_per_qos) {
+				qossch->pool_num = prtsch->pool_num;
+				qossch->_pool = prtsch->_pool;
+				if (pki_helper_debug)
+					cvmx_dprintf("qos %d pool %d\n",
+						qos, prtsch->pool_num);
+			} else if (qossch->pool_buff_size < mbuff_size ||
+				    !mbuff_size)
+				mbuff_size = qossch->pool_buff_size;
+			if (!qossch->aura_per_qos) {
+				qossch->aura_num = prtsch->aura_num;
+				qossch->_aura = prtsch->_aura;
+			}
+			if (!qossch->sso_grp_per_qos)
+				qossch->sso_grp = prtsch->sso_grp;
+			rs = __cvmx_helper_pki_qos_rsrcs(xi.node, qossch);
+			if (rs)
+				return rs;
+		}
+	}
 	/* Using port shift and port msb to schedule packets from differnt port to differnt
-                auras and different sso group */
-                /* Using QPG_QOS to schedule packets to different aura and sso group */
+	auras and different sso group */
+	/* Using QPG_QOS to schedule packets to different aura and sso group */
 	/* If ports needs to send packets to different aura and sso group
-                depending on packet qos */
-                /* We will need to set up aura and sso group for each port and each qos */
+	depending on packet qos */
+	/* We will need to set up aura and sso group for each port and each qos */
 	/* If all ports are using same style, they will be using same qpg_qos so
-                check only for port 0*/
-                if (intf->style_per_intf) {
-                        if (intf->prt_s[0].qpg_qos) { /* all ports using same style will use same qos defined in port 0 config */
-                                qpg_qos = intf->prt_s[0].qpg_qos;
-                                num_qos = cvmx_helper_pki_get_num_qpg_entry(intf->prt_s[0].qpg_qos);
-                                if (intf->qos_share_aura && intf->qos_share_grp) {
+	check only for port 0*/
+	if (intfsch->style_per_intf) {
+		if (intfsch->prt_s[0].qpg_qos) { /* all ports using same style will use same qos defined in port 0 config */
+			qpg_qos = intfsch->prt_s[0].qpg_qos;
+			num_qos = cvmx_helper_pki_get_num_qpg_entry(
+				intfsch->prt_s[0].qpg_qos);
+			if (intfsch->qos_share_aura && intfsch->qos_share_grp) {
 				/* All ports will use same qpg offset so no need for
-                                        port_msb or port shift */
-                                        port_msb = 0;
-                                        port_shift = 0;
-                                        num_entry = num_qos;
-                                        qpg_base = intf->qpg_base;
-                                        rs = __cvmx_helper_pki_set_intf_qpg(xi.node, 0, qpg_base, num_entry, intf);
-                                        if (rs == -1)
-                                                return rs;
-                                        intf->qpg_base = rs;
-                                } else {
-                                        port_msb = 8;
-                                        port_shift = cvmx_helper_pki_port_shift(xiface, intf->prt_s[0].qpg_qos);
-                                //cvmx_dprintf("pki-helper: num qpg entry needed %d\n", (int)num_entry);
-                                //cvmx_dprintf("pki-helper:port_msb= %d port_shift=%d\n", port_msb, port_shift);
-                                        num_entry = num_qos;
-                                        for (port = 0; port < num_ports; port++) {
-                                                /* Skip invalid/disabled ports */
-                                                prtsch =  &intf->prt_s[port];
-                                                if (!cvmx_helper_is_port_valid(xiface, port) || prtsch->cfg_port)
-                                                        continue;
-                                                ipd_port = cvmx_helper_get_ipd_port(xiface, port);
-                                                qpg_base = intf->qpg_base + ((ipd_port & 0xff) << port_shift);
-                                                rs = __cvmx_helper_pki_set_intf_qpg(xi.node, port, qpg_base, num_entry, intf);
-                                                if (rs == -1)
-                                                        return rs;
-                                                prtsch->qpg_base = rs;
-                                        }
-                                        intf->qpg_base = intf->prt_s[0].qpg_base;
-                                }
-                        } else if (intf->prt_s[0].aura_per_prt || intf->prt_s[0].sso_grp_per_prt) {
-                                /* Every port is using their own aura or group but no qos */
-                                port_msb = 8;
-                                port_shift = 0;
-                                num_entry = 1;
-                        //cvmx_dprintf("pki-helper: aura/grp_per_prt: num qpg entry needed %d\n", (int)num_entry);
-                                for (port = 0; port < num_ports; port++) {
-                                        prtsch =  &intf->prt_s[port];
-                                        /* Skip invalid/disabled ports */
-                                        if (!cvmx_helper_is_port_valid(xiface, port) || prtsch->cfg_port)
-                                                continue;
-                                        ipd_port = cvmx_helper_get_ipd_port(xiface, port);
-                                        qpg_base = intf->qpg_base + ((ipd_port & 0xff) << port_shift);
-                                //cvmx_dprintf("port %d intf_q_base=%d q_base= %d\n", port, intf->qpg_base, qpg_base);
-                                        qpg_base = cvmx_pki_qpg_entry_alloc(xi.node, qpg_base, num_entry);
-                                        if (qpg_base == CVMX_RESOURCE_ALREADY_RESERVED) {
-                                                cvmx_dprintf("pki-helper: INFO: qpg entries will be shared\n");
-                                        } else if (qpg_base == CVMX_RESOURCE_ALLOC_FAILED) {
-                                                cvmx_dprintf("pki-helper: ERROR: qpg entries not available\n");
-                                                return qpg_base;
-                                        } else {
-                                                if (intf->qpg_base < 0)
-                                                        intf->qpg_base = qpg_base;
-                                                prtsch->qpg_base = qpg_base;
-                                        }
-                                        cvmx_pki_write_qpg_entry(xi.node, qpg_base, 0, prtsch->aura,                                                prtsch->sso_grp, prtsch->sso_grp);
-                                }
-                                intf->qpg_base = intf->prt_s[0].qpg_base;
-                        } else { /* All ports on that intf use same port_add, aura & sso grps */
+				port_msb or port shift */
+				port_msb = 0;
+				port_shift = 0;
+				num_entry = num_qos;
+				qpg_base = intfsch->qpg_base;
+				rs = __cvmx_helper_pki_set_intf_qpg(
+					xi.node, 0, qpg_base, num_entry,
+					intfsch);
+				if (rs == -1)
+					return rs;
+				intfsch->qpg_base = rs;
+			} else {
+				port_msb = 8;
+				port_shift = __cvmx_helper_pki_port_shift(xiface, intfsch->prt_s[0].qpg_qos);
+				if (pki_helper_debug) {
+					cvmx_dprintf("pki-helper: num qpg entry needed %d\n", (int)num_entry);
+					cvmx_dprintf("pki-helper:port_msb= %d port_shift=%d\n", port_msb, port_shift);
+				}
+				num_entry = num_qos;
+				for (port = 0; port < num_ports; port++) {
+					/* Skip invalid/disabled ports */
+					prtsch =  &intfsch->prt_s[port];
+					if (!cvmx_helper_is_port_valid(xiface, port) || prtsch->cfg_port)
+						continue;
+					ipd_port = cvmx_helper_get_ipd_port(xiface, port);
+					qpg_base = intfsch->qpg_base + ((ipd_port & 0xff) << port_shift);
+					rs = __cvmx_helper_pki_set_intf_qpg(
+						xi.node, port, qpg_base,
+						num_entry, intfsch);
+					if (rs == -1)
+						return rs;
+					prtsch->qpg_base = rs;
+				}
+				intfsch->qpg_base = intfsch->prt_s[0].qpg_base;
+			}
+		} else if (intfsch->prt_s[0].aura_per_prt || intfsch->prt_s[0].sso_grp_per_prt) {
+			/* Every port is using their own aura or group but no qos */
+			port_msb = 8;
+			port_shift = 0;
+			num_entry = 1;
+			if (pki_helper_debug)
+				cvmx_dprintf("pki-helper: aura/grp_per_prt: num qpg entry needed %d\n", (int)num_entry);
+			for (port = 0; port < num_ports; port++) {
+				prtsch =  &intfsch->prt_s[port];
+				/* Skip invalid/disabled ports */
+				if (!cvmx_helper_is_port_valid(xiface, port) || prtsch->cfg_port)
+					continue;
+				ipd_port = cvmx_helper_get_ipd_port(xiface, port);
+				qpg_base = intfsch->qpg_base + ((ipd_port & 0xff) << port_shift);
+				if (pki_helper_debug)
+					cvmx_dprintf("port %d intf_q_base=%d q_base= %d\n",
+						port, intfsch->qpg_base, qpg_base);
+				qpg_base = cvmx_pki_qpg_entry_alloc(xi.node, qpg_base, num_entry);
+				if (qpg_base == CVMX_RESOURCE_ALREADY_RESERVED) {
+					cvmx_dprintf("pki-helper: INFO: qpg entries will be shared\n");
+				} else if (qpg_base == CVMX_RESOURCE_ALLOC_FAILED) {
+					cvmx_dprintf("pki-helper: ERROR: qpg entries not available\n");
+					return qpg_base;
+				} else {
+					if (intfsch->qpg_base < 0)
+						intfsch->qpg_base = qpg_base;
+					prtsch->qpg_base = qpg_base;
+				}
+				qpg_cfg.port_add = 0;
+				qpg_cfg.aura_num = prtsch->aura_num;
+				qpg_cfg.grp_ok = prtsch->sso_grp;
+				qpg_cfg.grp_bad = prtsch->sso_grp;
+				cvmx_pki_write_qpg_entry(xi.node, qpg_base, &qpg_cfg);
+			}
+			intfsch->qpg_base = intfsch->prt_s[0].qpg_base;
+		} else { /* All ports on that intf use same port_add, aura & sso grps */
 			/* All ports will use same qpg offset so no need for
-                                port_msb or port shift */
-                                port_msb = 0;
-                                port_shift = 0;
-                                num_entry = 1;
-                                qpg_base = intf->qpg_base;
-                                qpg_base = cvmx_pki_qpg_entry_alloc(xi.node, qpg_base, num_entry);
-                                if (qpg_base == CVMX_RESOURCE_ALREADY_RESERVED) {
-                                        cvmx_dprintf("pki-helper: INFO: qpg entries will be shared\n");
-                                } else if (qpg_base == CVMX_RESOURCE_ALLOC_FAILED) {
-                                        cvmx_dprintf("pki-helper: ERROR: qpg entries not available\n");
-                                        return qpg_base;
-                                } else
-                                        intf->qpg_base = qpg_base;
-                                        cvmx_pki_write_qpg_entry(xi.node, qpg_base, 0, intf->aura,
-                                                        intf->sso_grp, intf->sso_grp);
-                        }
-                        if (!mbuff_size) {
-                                if (!gbl_schd->setup_pool) {
-                                        cvmx_dprintf("No pool has setup for intf %d\n", xiface);
-                                        return -1;
-                                }
-                                mbuff_size = gbl_schd->pool_buff_size;
-                                cvmx_dprintf("interface %d is using global pool\n", xiface);
-                        }
-                        /* Allocate style here and map it to all ports on interface */
-                        rs = cvmx_pki_style_alloc(xi.node, intf->style);
-                        if (rs == CVMX_RESOURCE_ALREADY_RESERVED) {
-                                cvmx_dprintf("passthrough: INFO: style will be shared\n");
-                        } else if (rs == CVMX_RESOURCE_ALLOC_FAILED) {
-                                cvmx_dprintf("passthrough: ERROR: style not available\n");
-                                return CVMX_RESOURCE_ALLOC_FAILED;
-                        } else {
-                                intf->style = rs;
-                                if (pki_helper_debug)
-                                        cvmx_dprintf("style %d allocated intf %d qpg_base %d\n", intf->style, xiface, intf->qpg_base);
-                                style_cfg = pki_dflt_style[xi.node];
-                                style_cfg.parm_cfg.qpg_qos = qpg_qos;
-                                style_cfg.parm_cfg.qpg_base = intf->qpg_base;
-                                style_cfg.parm_cfg.qpg_port_msb = port_msb;
-                                style_cfg.parm_cfg.qpg_port_sh = port_shift;
-                                style_cfg.parm_cfg.mbuff_size = mbuff_size;
-                                cvmx_pki_set_style_config(xi.node, intf->style,
-                                                CVMX_PKI_CLUSTER_ALL, &style_cfg);
-                        }
-                        for (port = 0; port < num_ports; port++) {
-                                prtsch = &intf->prt_s[port];
-                                /* Skip invalid/disabled ports */
-                                if (!cvmx_helper_is_port_valid(xiface, port) || prtsch->cfg_port)
-                                        continue;
-                                prtsch->style = intf->style;
-                                pknd = cvmx_helper_get_pknd(xiface, port);
-                                cvmx_pki_get_pkind_config(xi.node, pknd, &pknd_cfg);
-                                pknd_cfg.initial_style = intf->style;
-                                pknd_cfg.fcs_pres = has_fcs;
-                                cvmx_pki_set_pkind_config(xi.node, pknd, &pknd_cfg);
-                        //cvmx_pki_show_port(xi.node, xiface, port);
-                        }
-                } else if (intf->style_per_prt) {
-                        port_msb = 0;
-                        port_shift = 0;
-                        for (port = 0; port < num_ports; port++) {
-                                prtsch = &intf->prt_s[port];
-                                /* Skip invalid/disabled ports */
-                                if (!cvmx_helper_is_port_valid(xiface, port) || prtsch->cfg_port)
-                                        continue;
-                                if (prtsch->qpg_qos && intf->qos_share_aura &&
-                                    intf->qos_share_grp && port !=0) {
-                                        if(pki_helper_debug)
-                                                cvmx_dprintf("intf %d has all ports share qos aura n grps\n",xiface);
-                                /* Ports have differnet styles but want to share same qpg entries.
-                                        this might never be the case */
-                                        prtsch->qpg_base = intf->prt_s[0].qpg_base;
-                                    }
-                                    ipd_port = cvmx_helper_get_ipd_port(xiface, port);
-                                    cvmx_helper_pki_init_port(ipd_port, prtsch);
-                        }
-                }
-                return 0;
+			port_msb or port shift */
+			port_msb = 0;
+			port_shift = 0;
+			num_entry = 1;
+			qpg_base = intfsch->qpg_base;
+			qpg_base = cvmx_pki_qpg_entry_alloc(xi.node, qpg_base, num_entry);
+			if (qpg_base == CVMX_RESOURCE_ALREADY_RESERVED) {
+				cvmx_dprintf("pki-helper: INFO: qpg entries will be shared\n");
+			} else if (qpg_base == CVMX_RESOURCE_ALLOC_FAILED) {
+				cvmx_dprintf("pki-helper: ERROR: qpg entries not available\n");
+				return qpg_base;
+			} else
+				intfsch->qpg_base = qpg_base;
+
+			qpg_cfg.port_add = 0;
+			qpg_cfg.aura_num = intfsch->aura_num;
+			qpg_cfg.grp_ok = intfsch->sso_grp;
+			qpg_cfg.grp_bad = intfsch->sso_grp;
+			cvmx_pki_write_qpg_entry(xi.node, qpg_base, &qpg_cfg);
+		}
+		if (!mbuff_size) {
+			if (!gblsch->setup_pool) {
+				cvmx_dprintf("No pool has setup for intf %d\n",
+					xiface);
+				return -1;
+			}
+			mbuff_size = gblsch->pool_buff_size;
+			cvmx_dprintf("interface %d is using global pool\n", xiface);
+		}
+		/* Allocate style here and map it to all ports on interface */
+		rs = cvmx_pki_style_alloc(xi.node, intfsch->style);
+		if (rs == CVMX_RESOURCE_ALREADY_RESERVED) {
+			cvmx_dprintf("passthrough: INFO: style will be shared\n");
+		} else if (rs == CVMX_RESOURCE_ALLOC_FAILED) {
+			cvmx_dprintf("passthrough: ERROR: style not available\n");
+			return CVMX_RESOURCE_ALLOC_FAILED;
+		} else {
+			intfsch->style = rs;
+			if (pki_helper_debug)
+				cvmx_dprintf("style %d allocated intf %d qpg_base %d\n",
+					intfsch->style, xiface,
+					intfsch->qpg_base);
+			style_cfg = pki_dflt_style[xi.node];
+			style_cfg.parm_cfg.qpg_qos = qpg_qos;
+			style_cfg.parm_cfg.qpg_base = intfsch->qpg_base;
+			style_cfg.parm_cfg.qpg_port_msb = port_msb;
+			style_cfg.parm_cfg.qpg_port_sh = port_shift;
+			style_cfg.parm_cfg.mbuff_size = mbuff_size;
+			cvmx_pki_write_style_config(xi.node, intfsch->style,
+				CVMX_PKI_CLUSTER_ALL, &style_cfg);
+		}
+		for (port = 0; port < num_ports; port++) {
+			prtsch = &intfsch->prt_s[port];
+			/* Skip invalid/disabled ports */
+			if (!cvmx_helper_is_port_valid(xiface, port) || prtsch->cfg_port)
+				continue;
+			prtsch->style = intfsch->style;
+			pknd = cvmx_helper_get_pknd(xiface, port);
+			cvmx_pki_read_pkind_config(xi.node, pknd, &pknd_cfg);
+			pknd_cfg.initial_style = intfsch->style;
+			pknd_cfg.fcs_pres = has_fcs;
+			cvmx_pki_write_pkind_config(xi.node, pknd, &pknd_cfg);
+		}
+	} else if (intfsch->style_per_prt) {
+		port_msb = 0;
+		port_shift = 0;
+		for (port = 0; port < num_ports; port++) {
+			prtsch = &intfsch->prt_s[port];
+			/* Skip invalid/disabled ports */
+			if (!cvmx_helper_is_port_valid(xiface, port) || prtsch->cfg_port)
+				continue;
+			if (prtsch->qpg_qos && intfsch->qos_share_aura &&
+				intfsch->qos_share_grp && port != 0) {
+				if (pki_helper_debug)
+					cvmx_dprintf("intf %d has all ports share qos aura n grps\n",
+						xiface);
+				/* Ports have differnet styles but want
+				 * to share same qpg entries.
+				this might never be the case */
+				prtsch->qpg_base = intfsch->prt_s[0].qpg_base;
+			}
+			ipd_port = cvmx_helper_get_ipd_port(xiface, port);
+			cvmx_helper_pki_init_port(ipd_port, prtsch);
+		}
+	}
+	return 0;
 }
 
-#if 0
-static const char* pki_ltype_sprint(int ltype) {
-        switch (ltype) {
-                case CVMX_PKI_LTYPE_E_ENET:	return "(ENET)";
-                case CVMX_PKI_LTYPE_E_VLAN:	return "(VLAN)";
-                case CVMX_PKI_LTYPE_E_SNAP_PAYLD:return "(SNAP_PAYLD)";
-                case CVMX_PKI_LTYPE_E_ARP:	return "(ARP)";
-                case CVMX_PKI_LTYPE_E_RARP:	return "(RARP)";
-                case CVMX_PKI_LTYPE_E_IP4:	return "(IP4)";
-                case CVMX_PKI_LTYPE_E_IP4_OPT:	return "(IP4_OPT)";
-                case CVMX_PKI_LTYPE_E_IP6:	return "(IP6)";
-                case CVMX_PKI_LTYPE_E_IP6_OPT:	return "(IP6_OPT)";
-                case CVMX_PKI_LTYPE_E_IPSEC_ESP:	return "(IPSEC_ESP)";
-                case CVMX_PKI_LTYPE_E_IPFRAG:	return "(IPFRAG)";
-                case CVMX_PKI_LTYPE_E_IPCOMP:	return "(IPCOMP)";
-                case CVMX_PKI_LTYPE_E_TCP:	return "(TCP)";
-                case CVMX_PKI_LTYPE_E_UDP:	return "(UDP)";
-                case CVMX_PKI_LTYPE_E_SCTP:	return "(SCTP)";
-                case CVMX_PKI_LTYPE_E_UDP_VXLAN:	return "(UDP_VXLAN)";
-                case CVMX_PKI_LTYPE_E_GRE:	return "(GRE)";
-                case CVMX_PKI_LTYPE_E_NVGRE:	return "(NVGRE)";
-                case CVMX_PKI_LTYPE_E_GTP:	return "(GTP)";
-                default: 			return "";
-        }
+/**
+ * This function gets all the PKI parameters related to that
+ * particular port from hardware.
+ * @param ipd_port	ipd port number to get parameter of
+ * @param port_cfg	pointer to structure where to store read parameters
+ */
+void cvmx_pki_get_port_config(int ipd_port, struct cvmx_pki_port_config *port_cfg)
+{
+	int interface, index, pknd;
+	int style, cl_mask;
+	cvmx_pki_icgx_cfg_t pki_cl_msk;
+	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(ipd_port);
+
+	/* get the pkind used by this ipd port */
+	interface = cvmx_helper_get_interface_num(ipd_port);
+	index = cvmx_helper_get_interface_index_num(ipd_port);
+	pknd = cvmx_helper_get_pknd(interface, index);
+
+	cvmx_pki_read_pkind_config(xp.node, pknd, &port_cfg->pkind_cfg);
+	style = port_cfg->pkind_cfg.initial_style;
+	pki_cl_msk.u64 = cvmx_read_csr_node(xp.node, CVMX_PKI_ICGX_CFG(port_cfg->pkind_cfg.cluster_grp));
+	cl_mask = pki_cl_msk.s.clusters;
+	cvmx_pki_read_style_config(xp.node, style, cl_mask, &port_cfg->style_cfg);
 }
+EXPORT_SYMBOL(cvmx_pki_get_port_config);
 
-void pki_wqe_dump(const cvmx_wqe_78xx_t* wqp)
+/**
+ * This function sets all the PKI parameters related to that
+ * particular port in hardware.
+ * @param ipd_port	ipd port number to get parameter of
+ * @param port_cfg	pointer to structure containing port parameters
+ */
+void cvmx_pki_set_port_config(int ipd_port, struct cvmx_pki_port_config *port_cfg)
 {
-        int i;
-        /* it is not cvmx_shared so per core only */
-        static uint64_t count;
-
-        cvmx_dprintf("Wqe entry for packet %lld\n", (unsigned long long)count++);
-        cvmx_dprintf("    WORD%02d: %016llx", (int)0, (unsigned long long)wqp->word0.u64);
-        cvmx_dprintf(" aura=0x%x", wqp->word0.aura);
-        cvmx_dprintf(" apad=%d", wqp->word0.apad);
-        cvmx_dprintf(" chan=0x%x", wqp->word0.channel);
-        cvmx_dprintf(" bufs=%d" , wqp->word0.bufs);
-        cvmx_dprintf(" style=0x%x" , wqp->word0.style);
-        cvmx_dprintf(" pknd=0x%x" , wqp->word0.pknd);
-        cvmx_dprintf("\n");
-        cvmx_dprintf("    WORD%02d: %016llx", (int)1, (unsigned long long)wqp->word1.u64);
-        cvmx_dprintf(" len=%d" , wqp->word1.len);
-        cvmx_dprintf(" grp=0x%x" , wqp->word1.grp);
-        cvmx_dprintf(" tt=%s", OCT_TAG_TYPE_STRING(wqp->word1.tag_type));
-        cvmx_dprintf(" tag=0x%08x" , wqp->word1.tag);
-        cvmx_dprintf("\n");
-        if (wqp->word2.u64) {
-                cvmx_dprintf("    WORD%02d: %016llx"  , (int)2, (unsigned long long)wqp->word2.u64);
-                if (wqp->word2.le_hdr_type)
-                        cvmx_dprintf(" [LAE]");
-                if (wqp->word2.lb_hdr_type)
-                        cvmx_dprintf(" lbty=%d"  "%s",
-                               wqp->word2.lb_hdr_type, pki_ltype_sprint(wqp->word2.lb_hdr_type));
-                if (wqp->word2.lc_hdr_type)
-                        cvmx_dprintf(" lcty=%d"  "%s",
-                               wqp->word2.lc_hdr_type, pki_ltype_sprint(wqp->word2.lc_hdr_type));
-                if (wqp->word2.ld_hdr_type)
-                        cvmx_dprintf(" ldty=%d"  "%s",
-                               wqp->word2.ld_hdr_type, pki_ltype_sprint(wqp->word2.ld_hdr_type));
-                if (wqp->word2.le_hdr_type)
-                        cvmx_dprintf(" lety=%d"  "%s",
-                               wqp->word2.le_hdr_type, pki_ltype_sprint(wqp->word2.le_hdr_type));
-                if (wqp->word2.lf_hdr_type)
-                        cvmx_dprintf(" lfty=%d"  "%s",
-                               wqp->word2.lf_hdr_type, pki_ltype_sprint(wqp->word2.lf_hdr_type));
-                if (wqp->word2.lg_hdr_type)
-                        cvmx_dprintf(" lgty=%d"  "%s",
-                               wqp->word2.lg_hdr_type, pki_ltype_sprint(wqp->word2.lg_hdr_type));
-                if (wqp->word2.pcam_flag1)
-                        cvmx_dprintf(" PF1");
-                if (wqp->word2.pcam_flag2)
-                        cvmx_dprintf(" PF2");
-                if (wqp->word2.pcam_flag3)
-                        cvmx_dprintf(" PF3");
-                if (wqp->word2.pcam_flag4)
-                        cvmx_dprintf(" PF4");
-                if (wqp->word2.vlan_valid || wqp->word2.vlan_stacked) {
-                        if (wqp->word2.vlan_valid)
-                                cvmx_dprintf(" vlan valid");
-                        if (wqp->word2.vlan_stacked)
-                                cvmx_dprintf(" vlan stacked");
-                        cvmx_dprintf(" ");
+	int interface, index, pknd;
+	int style, cl_mask;
+	cvmx_pki_icgx_cfg_t pki_cl_msk;
+	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(ipd_port);
+
+	/* get the pkind used by this ipd port */
+	interface = cvmx_helper_get_interface_num(ipd_port);
+	index = cvmx_helper_get_interface_index_num(ipd_port);
+	pknd = cvmx_helper_get_pknd(interface, index);
+
+	if (cvmx_pki_write_pkind_config(xp.node, pknd, &port_cfg->pkind_cfg))
+		return;
+	style = port_cfg->pkind_cfg.initial_style;
+	pki_cl_msk.u64 = cvmx_read_csr_node(xp.node, CVMX_PKI_ICGX_CFG(port_cfg->pkind_cfg.cluster_grp));
+	cl_mask = pki_cl_msk.s.clusters;
+	cvmx_pki_write_style_config(xp.node, style, cl_mask, &port_cfg->style_cfg);
 }
-                if (wqp->word2.stat_inc) cvmx_dprintf(" stat_inc");
-                if (wqp->word2.is_frag) cvmx_dprintf(" L3 Fragment");
-                if (wqp->word2.is_l3_bcast) cvmx_dprintf(" L3 Broadcast");
-                if (wqp->word2.is_l3_mcast) cvmx_dprintf(" L3 Multicast");
-                if (wqp->word2.is_l2_bcast) cvmx_dprintf(" L2 Broadcast");
-                if (wqp->word2.is_l2_mcast) cvmx_dprintf(" L2 Multicast");
-                if (wqp->word2.is_raw) cvmx_dprintf(" RAW");
-                if (wqp->word2.err_level || wqp->word2.err_code) {
-                        cvmx_dprintf(" errlev=%d" , wqp->word2.err_level);
-                        cvmx_dprintf(" opcode=0x%x" , wqp->word2.err_code);
+EXPORT_SYMBOL(cvmx_pki_set_port_config);
+
+/**
+ * This function displays all the PKI parameters related to that
+ * particular port.
+ * @param ipd_port	ipd port number to display parameter of
+ */
+void cvmx_pki_show_port_config(int ipd_port)
+{
+	int interface, index, pknd;
+	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(ipd_port);
+
+	interface = cvmx_helper_get_interface_num(ipd_port);
+	index = cvmx_helper_get_interface_index_num(ipd_port);
+	pknd = cvmx_helper_get_pknd(interface, index);
+	cvmx_dprintf("Showing stats for intf 0x%x port %d------------------\n", interface, index);
+	cvmx_pki_show_pkind_attributes(xp.node, pknd);
+	cvmx_dprintf("END STAUS------------------------\n\n");
 }
-                cvmx_dprintf("\n");
+
+void cvmx_helper_pki_errata(int node)
+{
+	struct cvmx_pki_global_config gbl_cfg;
+
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X)) {
+		cvmx_pki_read_global_config(node, &gbl_cfg);
+		gbl_cfg.fpa_wait = CVMX_PKI_WAIT_PKT;
+		cvmx_pki_write_global_config(node, &gbl_cfg);
+	}
 }
-        cvmx_dprintf("    WORD%02d: %016llx", (int)3, (unsigned long long)wqp->packet_ptr.u64);
-
-        cvmx_dprintf(" size=%d" , wqp->packet_ptr.size);
-        cvmx_dprintf(" addr=0x%llx" , (unsigned long long)wqp->packet_ptr.addr);
-
-        cvmx_dprintf("\n");
-        if (wqp->word4.u64) {
-                cvmx_dprintf("    WORD%02d: %016llx", (int)4, (unsigned long long)wqp->word4.u64);
-                if (wqp->word4.ptr_layer_a) cvmx_dprintf(" laptr=%d" , wqp->word4.ptr_layer_a);
-                if (wqp->word4.ptr_layer_b) cvmx_dprintf(" lbptr=%d" , wqp->word4.ptr_layer_b);
-                if (wqp->word4.ptr_layer_c) cvmx_dprintf(" lcptr=%d" , wqp->word4.ptr_layer_c);
-                if (wqp->word4.ptr_layer_d) cvmx_dprintf(" ldptr=%d" , wqp->word4.ptr_layer_d);
-                if (wqp->word4.ptr_layer_e) cvmx_dprintf(" leptr=%d" , wqp->word4.ptr_layer_e);
-                if (wqp->word4.ptr_layer_f) cvmx_dprintf(" lfptr=%d" , wqp->word4.ptr_layer_f);
-                if (wqp->word4.ptr_layer_g) cvmx_dprintf(" lgptr=%d" , wqp->word4.ptr_layer_g);
-                if (wqp->word4.ptr_vlan) cvmx_dprintf(" vlptr=%d" , wqp->word4.ptr_vlan);
-                cvmx_dprintf("\n");
+
+static const char *pki_ltype_sprint(int ltype)
+{
+	switch (ltype) {
+	case CVMX_PKI_LTYPE_E_ENET:	return "(ENET)";
+	case CVMX_PKI_LTYPE_E_VLAN:	return "(VLAN)";
+	case CVMX_PKI_LTYPE_E_SNAP_PAYLD:return "(SNAP_PAYLD)";
+	case CVMX_PKI_LTYPE_E_ARP:	return "(ARP)";
+	case CVMX_PKI_LTYPE_E_RARP:	return "(RARP)";
+	case CVMX_PKI_LTYPE_E_IP4:	return "(IP4)";
+	case CVMX_PKI_LTYPE_E_IP4_OPT:	return "(IP4_OPT)";
+	case CVMX_PKI_LTYPE_E_IP6:	return "(IP6)";
+	case CVMX_PKI_LTYPE_E_IP6_OPT:	return "(IP6_OPT)";
+	case CVMX_PKI_LTYPE_E_IPSEC_ESP:return "(IPSEC_ESP)";
+	case CVMX_PKI_LTYPE_E_IPFRAG:	return "(IPFRAG)";
+	case CVMX_PKI_LTYPE_E_IPCOMP:	return "(IPCOMP)";
+	case CVMX_PKI_LTYPE_E_TCP:	return "(TCP)";
+	case CVMX_PKI_LTYPE_E_UDP:	return "(UDP)";
+	case CVMX_PKI_LTYPE_E_SCTP:	return "(SCTP)";
+	case CVMX_PKI_LTYPE_E_UDP_VXLAN:return "(UDP_VXLAN)";
+	case CVMX_PKI_LTYPE_E_GRE:	return "(GRE)";
+	case CVMX_PKI_LTYPE_E_NVGRE:	return "(NVGRE)";
+	case CVMX_PKI_LTYPE_E_GTP:	return "(GTP)";
+	default:			return "";
+	}
 }
-        for (i=0; i < 10; ++i) {
-                if (wqp->wqe_data[i]) cvmx_dprintf("    WORD%02d: %016llx"  "\n", i+5, (unsigned long long)wqp->wqe_data[i]);
+
+void cvmx_pki_dump_wqe(const cvmx_wqe_78xx_t *wqp)
+{
+	int i;
+	/* it is not cvmx_shared so per core only */
+	static uint64_t count;
+
+	cvmx_dprintf("Wqe entry for packet %lld\n", (unsigned long long)count++);
+	cvmx_dprintf("    WORD%02d: %016llx", (int)0, (unsigned long long)wqp->word0.u64);
+	cvmx_dprintf(" aura=0x%x", wqp->word0.aura);
+	cvmx_dprintf(" apad=%d", wqp->word0.apad);
+	cvmx_dprintf(" chan=0x%x", wqp->word0.channel);
+	cvmx_dprintf(" bufs=%d" , wqp->word0.bufs);
+	cvmx_dprintf(" style=0x%x" , wqp->word0.style);
+	cvmx_dprintf(" pknd=0x%x" , wqp->word0.pknd);
+	cvmx_dprintf("\n");
+	cvmx_dprintf("    WORD%02d: %016llx", (int)1, (unsigned long long)wqp->word1.u64);
+	cvmx_dprintf(" len=%d" , wqp->word1.len);
+	cvmx_dprintf(" grp=0x%x" , wqp->word1.grp);
+	cvmx_dprintf(" tt=%s", OCT_TAG_TYPE_STRING(wqp->word1.tag_type));
+	cvmx_dprintf(" tag=0x%08x" , wqp->word1.tag);
+	cvmx_dprintf("\n");
+	if (wqp->word2.u64) {
+		cvmx_dprintf("    WORD%02d: %016llx"  , (int)2, (unsigned long long)wqp->word2.u64);
+		if (wqp->word2.le_hdr_type)
+			cvmx_dprintf(" [LAE]");
+		if (wqp->word2.lb_hdr_type)
+			cvmx_dprintf(" lbty=%d"  "%s",
+				wqp->word2.lb_hdr_type, pki_ltype_sprint(wqp->word2.lb_hdr_type));
+		if (wqp->word2.lc_hdr_type)
+			cvmx_dprintf(" lcty=%d"  "%s",
+				wqp->word2.lc_hdr_type, pki_ltype_sprint(wqp->word2.lc_hdr_type));
+		if (wqp->word2.ld_hdr_type)
+			cvmx_dprintf(" ldty=%d"  "%s",
+				wqp->word2.ld_hdr_type, pki_ltype_sprint(wqp->word2.ld_hdr_type));
+		if (wqp->word2.le_hdr_type)
+			cvmx_dprintf(" lety=%d"  "%s",
+				wqp->word2.le_hdr_type, pki_ltype_sprint(wqp->word2.le_hdr_type));
+		if (wqp->word2.lf_hdr_type)
+			cvmx_dprintf(" lfty=%d"  "%s",
+				wqp->word2.lf_hdr_type, pki_ltype_sprint(wqp->word2.lf_hdr_type));
+		if (wqp->word2.lg_hdr_type)
+			cvmx_dprintf(" lgty=%d"  "%s",
+				wqp->word2.lg_hdr_type, pki_ltype_sprint(wqp->word2.lg_hdr_type));
+		if (wqp->word2.pcam_flag1)
+			cvmx_dprintf(" PF1");
+		if (wqp->word2.pcam_flag2)
+			cvmx_dprintf(" PF2");
+		if (wqp->word2.pcam_flag3)
+			cvmx_dprintf(" PF3");
+		if (wqp->word2.pcam_flag4)
+			cvmx_dprintf(" PF4");
+		if (wqp->word2.vlan_valid || wqp->word2.vlan_stacked) {
+			if (wqp->word2.vlan_valid)
+				cvmx_dprintf(" vlan valid");
+			if (wqp->word2.vlan_stacked)
+				cvmx_dprintf(" vlan stacked");
+			cvmx_dprintf(" ");
+		}
+		if (wqp->word2.stat_inc)
+			cvmx_dprintf(" stat_inc");
+		if (wqp->word2.is_frag)
+			cvmx_dprintf(" L3 Fragment");
+		if (wqp->word2.is_l3_bcast)
+			cvmx_dprintf(" L3 Broadcast");
+		if (wqp->word2.is_l3_mcast)
+			cvmx_dprintf(" L3 Multicast");
+		if (wqp->word2.is_l2_bcast)
+			cvmx_dprintf(" L2 Broadcast");
+		if (wqp->word2.is_l2_mcast)
+			cvmx_dprintf(" L2 Multicast");
+		if (wqp->word2.is_raw)
+			cvmx_dprintf(" RAW");
+		if (wqp->word2.err_level || wqp->word2.err_code) {
+			cvmx_dprintf(" errlev=%d" , wqp->word2.err_level);
+			cvmx_dprintf(" opcode=0x%x" , wqp->word2.err_code);
+		}
+		cvmx_dprintf("\n");
+	}
+	cvmx_dprintf("    WORD%02d: %016llx", (int)3, (unsigned long long)wqp->packet_ptr.u64);
+
+	cvmx_dprintf(" size=%d" , wqp->packet_ptr.size);
+	cvmx_dprintf(" addr=0x%llx" , (unsigned long long)wqp->packet_ptr.addr);
+
+	cvmx_dprintf("\n");
+	if (wqp->word4.u64) {
+		cvmx_dprintf("    WORD%02d: %016llx", (int)4, (unsigned long long)wqp->word4.u64);
+		if (wqp->word4.ptr_layer_a)
+			cvmx_dprintf(" laptr=%d" , wqp->word4.ptr_layer_a);
+		if (wqp->word4.ptr_layer_b)
+			cvmx_dprintf(" lbptr=%d" , wqp->word4.ptr_layer_b);
+		if (wqp->word4.ptr_layer_c)
+			cvmx_dprintf(" lcptr=%d" , wqp->word4.ptr_layer_c);
+		if (wqp->word4.ptr_layer_d)
+			cvmx_dprintf(" ldptr=%d" , wqp->word4.ptr_layer_d);
+		if (wqp->word4.ptr_layer_e)
+			cvmx_dprintf(" leptr=%d" , wqp->word4.ptr_layer_e);
+		if (wqp->word4.ptr_layer_f)
+			cvmx_dprintf(" lfptr=%d" , wqp->word4.ptr_layer_f);
+		if (wqp->word4.ptr_layer_g)
+			cvmx_dprintf(" lgptr=%d" , wqp->word4.ptr_layer_g);
+		if (wqp->word4.ptr_vlan)
+			cvmx_dprintf(" vlptr=%d" , wqp->word4.ptr_vlan);
+		cvmx_dprintf("\n");
+	}
+	for (i = 0; i < 10; ++i) {
+		if (wqp->wqe_data[i])
+			cvmx_dprintf("    WORD%02d: %016llx"  "\n",
+				i+5, (unsigned long long)wqp->wqe_data[i]);
+	}
 }
+
+
+
+/**
+ * Modifies maximum frame length to check.
+ * It modifies the global frame length set used by this port, any other
+ * port using the same set will get affected too.
+ * @param node		node number
+ * @param ipd_port	ipd port for which to modify max len.
+ * @param max_size	maximum frame length
+ */
+void cvmx_pki_set_max_frm_len(int ipd_port, uint32_t max_size)
+{
+	/* On CN78XX frame check is enabled for a style n and
+	PKI_CLX_STYLE_CFG[minmax_sel] selects which set of
+	MAXLEN/MINLEN to use. */
+	int interface, index, pknd;
+	cvmx_pki_clx_stylex_cfg_t style_cfg;
+	cvmx_pki_frm_len_chkx_t frame_len;
+	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(ipd_port);
+	int cluster = 0;
+	int style;
+	int sel;
+
+	/* get the pkind used by this ipd port */
+	interface = cvmx_helper_get_interface_num(ipd_port);
+	index = cvmx_helper_get_interface_index_num(ipd_port);
+	pknd = cvmx_helper_get_pknd(interface, index);
+
+	style = cvmx_pki_get_pkind_style(xp.node, pknd);
+	style_cfg.u64 = cvmx_read_csr_node(xp.node, CVMX_PKI_CLX_STYLEX_CFG(style, cluster));
+	sel = style_cfg.s.minmax_sel;
+	frame_len.u64 = cvmx_read_csr(CVMX_PKI_FRM_LEN_CHKX(sel));
+	frame_len.s.maxlen = max_size;
+	cvmx_write_csr_node(xp.node, CVMX_PKI_FRM_LEN_CHKX(sel), frame_len.u64);
+}
+
+/**
+ * This function sets up all th eports of particular interface
+ * for chosen fcs mode. (only use for backward compatibility).
+ * New application can control it via init_interfcae calls.
+ * @param node		node number.
+ * @param interfcae	interfcae number.
+ * @param has_fcs	1 -- enable fcs check and fcs strip.
+ *			0 -- disable fcs check.
+ */
+void cvmx_helper_pki_set_fcs_op(int node, int interface, int nports, int has_fcs)
+{
+	int index;
+	int pknd;
+	int cluster = 0;
+	cvmx_pki_clx_pkindx_cfg_t pkind_cfg;
+
+	for (index = 0; index < nports; index++) {
+		pknd = cvmx_helper_get_pknd(interface, index);
+		while (cluster < CVMX_PKI_NUM_CLUSTER) {
+			/*vinita_to_do; find the cluster in use*/
+			pkind_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_CFG(pknd, cluster));
+			pkind_cfg.s.fcs_pres = has_fcs;
+			cvmx_write_csr_node(node, CVMX_PKI_CLX_PKINDX_CFG(pknd, cluster), pkind_cfg.u64);
+			cluster++;
+		}
+		/* make sure fcs_strip and fcs_check is also enable/disable for the style used by that port*/
+		cvmx_pki_endis_fcs_check(node, pknd, has_fcs, has_fcs);
+		cluster = 0;
+	}
+}
+
+/**
+ * This function sets the wqe buffer mode of all ports. First packet data buffer can reside
+ * either in same buffer as wqe OR it can go in separate buffer. If used the later mode,
+ * make sure software allocate enough buffers to now have wqe separate from packet data.
+ * @param node			node number.
+ * @param pkt_outside_wqe.	0 = The packet link pointer will be at word [FIRST_SKIP]
+ *				immediately followed by packet data, in the same buffer
+ *				as the work queue entry.
+ *				1 = The packet link pointer will be at word [FIRST_SKIP] in a new
+ *				buffer separate from the work queue entry. Words following the
+ *				WQE in the same cache line will be zeroed, other lines in the
+ *				buffer will not be modified and will retain stale data (from the
+ *				buffers previous use). This setting may decrease the peak PKI
+ *				performance by up to half on small packets.
+ */
+void cvmx_helper_pki_set_wqe_mode(int node, bool pkt_outside_wqe)
+{
+	int interface, port, pknd;
+	int num_intf, num_ports;
+	uint64_t style;
+
+	/* get the pkind used by this ipd port */
+	num_intf = cvmx_helper_get_number_of_interfaces();
+	for (interface = 0; interface < num_intf; interface++) {
+		num_ports = cvmx_helper_ports_on_interface(interface);
+		/*Skip invalid/disabled interfaces */
+		if (num_ports <= 0)
+			continue;
+		for (port = 0; port < num_ports; port++) {
+			pknd = cvmx_helper_get_pknd(interface, port);
+			style = cvmx_pki_get_pkind_style(node, pknd);
+			cvmx_pki_set_wqe_mode(node, style, pkt_outside_wqe);
+		}
+	}
+}
+
+/**
+ * This function sets the Packet mode of all ports and styles to little-endian.
+ * It Changes write operations of packet data to L2C to
+ * be in little-endian. Does not change the WQE header format, which is
+ * properly endian neutral.
+ * @param node		node number.
+ */
+void cvmx_helper_pki_set_little_endian(int node)
+{
+	int interface, port, pknd;
+	int num_intf, num_ports;
+	uint64_t style;
+
+	/* get the pkind used by this ipd port */
+	num_intf = cvmx_helper_get_number_of_interfaces();
+	for (interface = 0; interface < num_intf; interface++) {
+		num_ports = cvmx_helper_ports_on_interface(interface);
+		/*Skip invalid/disabled interfaces */
+		if (num_ports <= 0)
+			continue;
+		for (port = 0; port < num_ports; port++) {
+			pknd = cvmx_helper_get_pknd(interface, port);
+			style = cvmx_pki_get_pkind_style(node, pknd);
+			cvmx_pki_set_little_endian(node, style);
+		}
+	}
 }
-#endif
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-pko.c b/arch/mips/cavium-octeon/executive/cvmx-helper-pko.c
index e7231f4..5643c4f 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-pko.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-pko.c
@@ -42,7 +42,7 @@
  *
  * Helper Functions for the PKO
  *
- * $Id: cvmx-helper-pko.c 96989 2014-04-23 19:40:33Z ajasty $
+ * $Id: cvmx-helper-pko.c 103836 2014-09-03 02:00:52Z lrosenboim $
  */
 
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
@@ -57,13 +57,13 @@
 #include "cvmx-helper.h"
 #include "cvmx-helper-ilk.h"
 #include "cvmx-ipd.h"
-#include "cvmx-fpa.h"
+#include "cvmx-fpa1.h"
 #include "cvmx-hwpko.h"
 #include "cvmx-global-resources.h"
 #endif
 
 //XXX- these config data structures will go away soon!
-static CVMX_SHARED int64_t pko_fpa_config_pool = 2;
+static CVMX_SHARED int64_t pko_fpa_config_pool = -1;
 static CVMX_SHARED uint64_t pko_fpa_config_size = 1024;
 static CVMX_SHARED uint64_t pko_fpa_config_count = 0;
 
@@ -138,9 +138,12 @@ static int cvmx_helper_pko_pool_init(void)
 
 	/* Avoid redundant pool creation */
 	if (cvmx_fpa_get_block_size(pool) > 0) {
+#ifdef DEBUG
 		cvmx_dprintf("WARNING: %s: "
 			"pool %d already initialized\n",
 			__func__, pool);
+#endif
+		/* FIXME: Should check available buffer count */
 		return pool;
 	}
 
@@ -149,11 +152,6 @@ static int cvmx_helper_pko_pool_init(void)
 	buf_count = CVMX_PKO_MAX_OUTPUT_QUEUES + (pkt_buf_count * 3) / 8;
 
 	/* Allocate pools for pko command queues */
-	if (!cvmx_fpa_is_pool_available(-1, pool)) {
-		cvmx_helper_fpa_fill_pool(pool, buf_count, NULL);
-		return pool;
-	}
-	/* Allocate pools for pko command queues */
 	rc = __cvmx_helper_initialize_fpa_pool(pool,
 				cvmx_fpa_get_pko_pool_block_size(),
 				buf_count, "PKO Cmd-bufs");
@@ -161,6 +159,7 @@ static int cvmx_helper_pko_pool_init(void)
 	if (rc < 0)
 		cvmx_dprintf("%s: ERROR: in PKO buffer pool\n", __func__);
 
+	pool = rc;
 	return pool;
 }
 #endif
@@ -184,7 +183,6 @@ int cvmx_helper_pko_init(void)
 
 	__cvmx_helper_init_port_config_data();
 
-
 	cvmx_pko_hw_init(
 		cvmx_fpa_get_pko_pool(),
 		cvmx_fpa_get_pko_pool_block_size()
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-pko3.c b/arch/mips/cavium-octeon/executive/cvmx-helper-pko3.c
index 8964341..54a5266 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-pko3.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-pko3.c
@@ -49,6 +49,7 @@
 #include <asm/octeon/cvmx.h>
 #include <asm/octeon/cvmx-clock.h>
 #include <asm/octeon/cvmx-ilk.h>
+#include <asm/octeon/cvmx-fpa3.h>
 #include <asm/octeon/cvmx-pko3.h>
 #include <asm/octeon/cvmx-pko3-resources.h>
 #include <asm/octeon/cvmx-helper.h>
@@ -59,7 +60,7 @@
 #else
 #include "cvmx.h"
 #include "cvmx-ilk.h"
-#include "cvmx-fpa.h"
+#include "cvmx-fpa3.h"
 #include "cvmx-pko3.h"
 #include "cvmx-pko3-resources.h"
 #include "cvmx-helper.h"
@@ -97,8 +98,7 @@ static const int cvmx_pko_default_channel_level = 0;
 static const int debug = 0;
 
 /* These global variables are relevant for boot CPU only */
-int16_t __cvmx_pko3_aura_num = 1020;
-int16_t __cvmx_pko3_pool_num = -1;
+static cvmx_fpa3_gaura_t __cvmx_pko3_aura;
 
 /* This constant can not be modified, defined here for clarity only */
 #define CVMX_PKO3_POOL_BUFFER_SIZE 4096 /* 78XX PKO requires 4KB */
@@ -144,31 +144,9 @@ static int __cvmx_helper_pko3_res_owner(int ipd_port)
  */
 static int __cvmx_pko3_config_memory(unsigned node)
 {
-	int pool_num , aura_num;
-	int pool, block_size;
+	cvmx_fpa3_gaura_t aura;
+	int aura_num;
 	unsigned buf_count;
-	int res;
-
-	aura_num = __cvmx_pko3_aura_num;
-	pool_num = __cvmx_pko3_pool_num;
-
-        /* Check for legacy PKO buffer pool */
-        pool = cvmx_fpa_get_pko_pool();
-	block_size = cvmx_fpa_get_block_size(pool);
-
-        /* Avoid redundant pool creation */
-        if (block_size > 0 && block_size == CVMX_PKO3_POOL_BUFFER_SIZE) {
-                cvmx_dprintf("WARNING: %s: "
-                        "Legacy PKO pool %d re-used, "
-			"buffer count may not suffice.\n",
-                        __func__, pool);
-                aura_num = pool_num = pool;
-        } else if (block_size > 0) {
-                cvmx_dprintf("WARNING: %s: "
-                        "Legacy PKO pool %d created with wrong buffer size %u, "
-			"ignored.\n",
-                        __func__, pool, block_size);
-	}
 
 	/* Simulator has limited memory */
 	if (cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_SIM)
@@ -176,26 +154,22 @@ static int __cvmx_pko3_config_memory(unsigned node)
 	else
 		buf_count = CVMX_PKO3_POOL_BUFFERS;
 
-	if (aura_num != pool) {
+	cvmx_dprintf("%s: creating AURA with %u buffers\n",
+		__func__, buf_count);
 
-		cvmx_dprintf("%s: creating aura %u with %u buffers\n",
-			__func__, aura_num, buf_count);
+	aura = cvmx_fpa3_setup_aura_and_pool(node, -1,
+		"PKO3 AURA", NULL,
+		CVMX_PKO3_POOL_BUFFER_SIZE, buf_count);
 
-		res = cvmx_helper_fpa_init(node, &pool_num, &aura_num,
-				CVMX_PKO3_POOL_BUFFER_SIZE,
-				buf_count,
-				"PKO Aura",
-				NULL);
-
-		if (res < 0) return res;
+	if (!__cvmx_fpa3_aura_valid(aura)) {
+		cvmx_printf("ERROR: %s AURA create failed\n", __func__);
+		return -1;
 	}
 
-	/* Store numbers e.g. for destruction */
-	__cvmx_pko3_pool_num = pool_num;
-	__cvmx_pko3_aura_num = aura_num;
+	aura_num = aura.node << 10 | aura.laura;
 
-	/* Combine LAURA with NODE */
-	aura_num |= node << 10;
+	/* Store handle for destruction */
+	__cvmx_pko3_aura = aura;
 
 	return aura_num;
 }
@@ -968,8 +942,9 @@ int cvmx_helper_pko3_init_interface(int xiface)
 		pad_enable_pko = pad_enable;
 
 	if(debug)
-		cvmx_dprintf("%s: FCS=%d pad=%d\n",
-			__func__, fcs_enable, pad_enable);
+		cvmx_dprintf("%s: iface %u:%u FCS=%d pad=%d pko=%d\n",
+			__func__, xi.node, xi.interface,
+			fcs_enable, pad_enable, pad_enable_pko);
 
 	/* Setup interface options */
 	for (subif = 0; subif < num_ports; subif++) {
@@ -998,13 +973,13 @@ int cvmx_helper_pko3_init_interface(int xiface)
 			res = cvmx_pko3_interface_options(xiface, subif,
 				fcs_enable, pad_enable_pko, fcs_sof_off);
 		} else if (pad_enable == pad_enable_pko) {
-			/* BGX interface with FCS/PAD done by PKO */
+			/* BGX interface: FCS/PAD done by PKO */
 			res = cvmx_pko3_interface_options(xiface, subif,
 				  fcs_enable, pad_enable, fcs_sof_off);
 			cvmx_helper_bgx_tx_options(xi.node, xi.interface, subif,
 				false, false);
 		} else {
-			/* BGX interface with FCS/PAd done by BGX */
+			/* BGX interface: FCS/PAD done by BGX */
 			res = cvmx_pko3_interface_options(xiface, subif,
 				  false, false, fcs_sof_off);
 			cvmx_helper_bgx_tx_options(xi.node, xi.interface, subif,
@@ -1015,6 +990,10 @@ int cvmx_helper_pko3_init_interface(int xiface)
 			cvmx_dprintf("WARNING: %s: "
 				"option set failed on iface %u:%u/%u\n",
 				__FUNCTION__, xi.node, xi.interface, subif);
+		if (debug)
+			cvmx_dprintf("%s: face %u:%u/%u fifo size %d\n",
+				__func__, xi.node, xi.interface, subif,
+				cvmx_pko3_port_fifo_size(xiface, subif));
 	}
 	return 0;
 
@@ -1058,6 +1037,7 @@ int __cvmx_helper_pko3_init_global(unsigned int node, uint16_t gaura)
 }
 EXPORT_SYMBOL(__cvmx_helper_pko3_init_global);
 
+/*#define	__PKO_HW_DEBUG*/
 #ifdef	__PKO_HW_DEBUG
 #define	CVMX_DUMP_REGX(reg) cvmx_dprintf("%s=%#llx\n",#reg,(long long)cvmx_read_csr(reg))
 #define	CVMX_DUMP_REGD(reg) cvmx_dprintf("%s=%lld.\n",#reg,(long long)cvmx_read_csr(reg))
@@ -1109,7 +1089,8 @@ int cvmx_helper_pko3_init_global(unsigned int node)
 {
 	void *ptr;
 	int res = -1;
-	int16_t gaura, aura = -1, anode = node;
+	unsigned aura_num = ~0;
+	cvmx_fpa3_gaura_t aura;
 
 #ifndef CVMX_BUILD_FOR_LINUX_KERNEL
 	/* Allocate memory required by PKO3 */
@@ -1121,27 +1102,26 @@ int cvmx_helper_pko3_init_global(unsigned int node)
 		return res;
 	}
 
-	gaura = res;
-	aura = res & ((1 << 10)-1);
-	anode = res >> 10;
+	aura_num = res;
+	aura = __cvmx_pko3_aura;
 
 	/* Exercise the FPA to make sure the AURA is functional */
-	ptr = cvmx_fpa3_alloc_aura(anode, aura);
+	ptr = cvmx_fpa3_alloc(aura);
 
 	if (ptr == NULL )
 		res = -1;
 	else {
-		cvmx_fpa3_free_aura(ptr, anode, aura, 0);
+		cvmx_fpa3_free_nosync(ptr, aura, 0);
 		res = 0;
 	}
 
 	if (res < 0) {
-		cvmx_dprintf("ERROR: %s: FPA failure AURA=%u:%#x\n",
-			__func__, anode, aura);
+		cvmx_dprintf("ERROR: %s: FPA failure AURA=%u:%d\n",
+			__func__, aura.node, aura.laura);
 		return -1;
 	}
 
-	res = __cvmx_helper_pko3_init_global(node, gaura);
+	res = __cvmx_helper_pko3_init_global(node, aura_num);
 
 	if (res < 0)
 		cvmx_dprintf("ERROR: %s: failed to start PPKO\n",__func__);
@@ -1308,7 +1288,7 @@ int cvmx_helper_pko3_shutdown(unsigned int node)
 
 #ifndef CVMX_BUILD_FOR_LINUX_KERNEL
 	/* shut down AURA/POOL we created, and free its resources */
-	cvmx_helper_fpa3_shutdown_aura_and_pool(node, __cvmx_pko3_aura_num);
+	cvmx_fpa3_shutdown_aura_and_pool(__cvmx_pko3_aura);
 #endif /* CVMX_BUILD_FOR_LINUX_KERNEL */
 	return res;
 }
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.c b/arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.c
index f3a0fb4..68238f2 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.c
@@ -43,7 +43,7 @@
  * Functions for SGMII initialization, configuration,
  * and monitoring.
  *
- * <hr>$Revision: 100490 $<hr>
+ * <hr>$Revision: 102466 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
@@ -220,7 +220,26 @@ static int __cvmx_helper_sgmii_hardware_init_link(int interface, int index)
 
 	control_reg.s.an_en = !an_disable;
 
-	/* Force a PCS reset by powering down the PCS interface */
+	/* Force a PCS reset by powering down the PCS interface
+	 * This is needed to deal with broken Qualcomm/Atheros PHYs and switches
+	 * which never recover if PCS is not power cycled.  The alternative
+	 * is to power cycle or hardware reset the Qualcomm devices whenever
+	 * SGMII is initialized.
+	 *
+	 * This is needed for the QCA8033 PHYs as well as the QCA833X switches
+	 * to work.  The QCA8337 switch has additional SGMII problems and is
+	 * best avoided if at all possible.  Failure to power cycle PCS prevents
+	 * any traffic from flowing between Octeon and Qualcomm devices if there
+	 * is a warm reset.  Even a software reset to the Qualcomm device will
+	 * not work.
+	 *
+	 * Note that this problem has been reported between Qualcomm and other
+	 * vendor's processors as well so this problem is not unique to
+	 * Qualcomm and Octeon.
+	 *
+	 * Power cycling PCS doesn't hurt anything with non-Qualcomm devices
+	 * other than adding a 25ms delay during initialization.
+	 */
 	control_reg.s.pwr_dn = 1;
 	cvmx_write_csr(CVMX_PCSX_MRX_CONTROL_REG(index, interface),
 		       control_reg.u64);
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-util.c b/arch/mips/cavium-octeon/executive/cvmx-helper-util.c
index 9395432..f06198a 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-util.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-util.c
@@ -243,25 +243,27 @@ int cvmx_helper_dump_packet(cvmx_wqe_t *work)
 	uint8_t *data_address;
 	uint8_t *end_of_data;
 
-	cvmx_dprintf("WORD0 = %lx\n", (unsigned long)work->word0.u64);
-	cvmx_dprintf("WORD1 = %lx\n", (unsigned long)work->word1.u64);
-	cvmx_dprintf("WORD2 = %lx\n", (unsigned long)work->word2.u64);
-	cvmx_dprintf("Packet Length:   %u\n", cvmx_wqe_get_len(work));
-	cvmx_dprintf("    Input Port:  %u\n", cvmx_wqe_get_port(work));
 	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
-		cvmx_dprintf("    pkind:       %u\n", work->word0.pki.pknd);
-		cvmx_dprintf("    Style:       %u\n", cvmx_wqe_get_style(work));
-		cvmx_dprintf("    Aura:        %u\n", cvmx_wqe_get_aura(work));
-		cvmx_dprintf("    Group:       %u\n", cvmx_wqe_get_grp(work));
-	} else
+		cvmx_pki_dump_wqe(wqe);
+		cvmx_wqe_pki_errata_20776(work);
+	} else {
+		cvmx_dprintf("WORD0 = %lx\n", (unsigned long)work->word0.u64);
+		cvmx_dprintf("WORD1 = %lx\n", (unsigned long)work->word1.u64);
+		cvmx_dprintf("WORD2 = %lx\n", (unsigned long)work->word2.u64);
+		cvmx_dprintf("Packet Length:   %u\n", cvmx_wqe_get_len(work));
+		cvmx_dprintf("    Input Port:  %u\n", cvmx_wqe_get_port(work));
 		cvmx_dprintf("    QoS:         %u\n", cvmx_wqe_get_qos(work));
-	cvmx_dprintf("    Buffers:     %u\n", cvmx_wqe_get_bufs(work));
+		cvmx_dprintf("    Buffers:     %u\n", cvmx_wqe_get_bufs(work));
+	}
+
 	if (cvmx_wqe_get_bufs(work) == 0) {
+		int wqe_pool;
 		if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
 			cvmx_dprintf("%s: ERROR: Unexpected bufs==0 in WQE\n",
 				__func__);
-		} else {
-		int wqe_pool = (int)cvmx_fpa_get_wqe_pool();
+			return -1;
+		}
+		wqe_pool = (int)cvmx_fpa_get_wqe_pool();
 		buffer_ptr.u64 = 0;
 		buffer_ptr.s.pool = wqe_pool;
 
@@ -283,10 +285,9 @@ int cvmx_helper_dump_packet(cvmx_wqe_t *work)
 			pip_gbl_cfg.u64 = cvmx_read_csr(CVMX_PIP_GBL_CFG);
 			buffer_ptr.s.addr += pip_gbl_cfg.s.nip_shf;
 		}
-		}
-	} else {
+	} else
 		buffer_ptr = work->packet_ptr;
-	}
+
 	remaining_bytes = cvmx_wqe_get_len(work);
 
 	while (remaining_bytes) {
@@ -335,7 +336,7 @@ int cvmx_helper_dump_packet(cvmx_wqe_t *work)
 
 		if (remaining_bytes) {
 			if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE) &&
-			    !wqe->word2.software)
+				!wqe->pki_wqe_translated)
 				buffer_ptr.u64 = *(uint64_t *)
 					cvmx_phys_to_ptr(bptr.addr-8);
 			else
@@ -346,6 +347,22 @@ int cvmx_helper_dump_packet(cvmx_wqe_t *work)
 	return 0;
 }
 
+/**
+ * @INTERNAL
+ *
+ * Extract NO_WPTR mode from PIP/IPD register
+ */
+static int __cvmx_ipd_mode_no_wptr(void)
+{
+	if (octeon_has_feature(OCTEON_FEATURE_NO_WPTR)) {
+		cvmx_ipd_ctl_status_t ipd_ctl_status;
+
+		ipd_ctl_status.u64 = cvmx_read_csr(CVMX_IPD_CTL_STATUS);
+		return  (ipd_ctl_status.s.no_wptr);
+	}
+	return 0;
+}
+
 
 /**
  * Extract packet data buffer pointer from work queue entry.
@@ -366,6 +383,7 @@ cvmx_buf_ptr_t cvmx_wqe_get_packet_ptr(cvmx_wqe_t *work)
 		cvmx_buf_ptr_t optr, lptr;
 		cvmx_buf_ptr_pki_t nptr;
 		unsigned pool, bufs;
+		int node = cvmx_get_node_num();
 
 		/* In case of repeated calls of this function */
 		if (wqe->pki_wqe_translated || wqe->word2.software ) {
@@ -380,13 +398,18 @@ cvmx_buf_ptr_t cvmx_wqe_get_packet_ptr(cvmx_wqe_t *work)
 		optr.u64=0;
 		optr.s.pool = pool;
 		optr.s.addr = nptr.addr;
-		optr.s.size = nptr.size;
+		if (bufs == 1)
+			optr.s.size = pki_dflt_pool[node].buffer_size -
+				pki_dflt_style[node].parm_cfg.first_skip -
+				8 - wqe->word0.apad;
+		else
+			optr.s.size = nptr.size;
 
 		/* Calculate the "back" offset */
 		if (!nptr.packet_outside_wqe)
 			optr.s.back = (nptr.addr-cvmx_ptr_to_phys(wqe)) >> 7;
 		else
-                        optr.s.back = (pki_dflt_style[0].parm_cfg.first_skip + 8 + wqe->word0.apad) >> 7;
+			optr.s.back = (pki_dflt_style[node].parm_cfg.first_skip + 8 + wqe->word0.apad) >> 7;
 		lptr = optr;
 
 		/* Follow pointer and convert all linked pointers */
@@ -395,18 +418,18 @@ cvmx_buf_ptr_t cvmx_wqe_get_packet_ptr(cvmx_wqe_t *work)
 
 			vptr = cvmx_phys_to_ptr(lptr.s.addr);
 
-                        memcpy(&nptr, vptr - 8, 8);
+			memcpy(&nptr, vptr - 8, 8);
 			/* Errata (PKI-20776) PKI_BUFLINK_S's are endian-swapped
-                        CN78XX pass 1.x has a bug where the packet pointer in each segment is
-                        written in the opposite endianness of the configured mode. Fix these
-                        here */
-                        if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X))
-                                nptr.u64 = __builtin_bswap64(nptr.u64);
+			CN78XX pass 1.x has a bug where the packet pointer in each segment is
+			written in the opposite endianness of the configured mode. Fix these
+			here */
+			if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X))
+				nptr.u64 = __builtin_bswap64(nptr.u64);
 			lptr.u64=0;
 			lptr.s.pool = pool;
 			lptr.s.addr = nptr.addr;
 			lptr.s.size = nptr.size;
-                        lptr.s.back = (pki_dflt_style[0].parm_cfg.later_skip + 8) >> 7;	//XXX- not guarangeed !!
+			lptr.s.back = (pki_dflt_style[0].parm_cfg.later_skip + 8) >> 7;	/* TBD: not guaranteed !! */
 
 			memcpy(vptr-8, &lptr, 8);
 			bufs --;
@@ -419,19 +442,23 @@ cvmx_buf_ptr_t cvmx_wqe_get_packet_ptr(cvmx_wqe_t *work)
 	} else {
 		cvmx_buf_ptr_t bptr;
 
-		if (work->word2.s.bufs > 0 || work->word2.s.software)
+		if (cvmx_likely(work->word2.s.bufs > 0))
+			return work->packet_ptr;
+
+		if (work->word2.s.software)
 			return work->packet_ptr;
 
 		/* data is only in WQE, convert it into a buf_ptr */
 		bptr.u64 = 0;
 		bptr.s.size = cvmx_wqe_get_len(work);
-		bptr.s.pool = cvmx_read_csr(CVMX_IPD_WQE_FPA_QUEUE) & 7;
 		bptr.s.addr = cvmx_ptr_to_phys(work) + 32;
 
 		/* For CN68XX it could be NO_WPTR or Dynamic-Short cause */
 		if (__cvmx_ipd_mode_no_wptr()) {
 			/* Packet pool is hardwired to 0 in relevant SoCs */
 			bptr.s.pool = 0;
+		} else {
+			bptr.s.pool = cvmx_read_csr(CVMX_IPD_WQE_FPA_QUEUE) & 7;
 		}
 
 		/* FIXME- RAWFULL case not handled yet */
@@ -439,16 +466,16 @@ cvmx_buf_ptr_t cvmx_wqe_get_packet_ptr(cvmx_wqe_t *work)
 		if (work->word2.s_cn38xx.not_IP ||
 		    work->word2.s_cn38xx.rcv_error) {
 			/* Adjust data offset for non-IP packets */
-                        union cvmx_pip_gbl_cfg pip_gbl_cfg;
-                        pip_gbl_cfg.u64 = cvmx_read_csr(CVMX_PIP_GBL_CFG);
-                        bptr.s.addr += pip_gbl_cfg.s.nip_shf;
+			union cvmx_pip_gbl_cfg pip_gbl_cfg;
+			pip_gbl_cfg.u64 = cvmx_read_csr(CVMX_PIP_GBL_CFG);
+			bptr.s.addr += pip_gbl_cfg.s.nip_shf;
 		} else {
 			/* Adjust data start address for IP protocols */
-                        union cvmx_pip_ip_offset pip_ip_offset;
-                        pip_ip_offset.u64 = cvmx_read_csr(CVMX_PIP_IP_OFFSET);
-                        bptr.s.addr += (pip_ip_offset.s.offset << 3) -
+			union cvmx_pip_ip_offset pip_ip_offset;
+			pip_ip_offset.u64 = cvmx_read_csr(CVMX_PIP_IP_OFFSET);
+			bptr.s.addr += (pip_ip_offset.s.offset << 3) -
 				work->word2.s.ip_offset;
-                        bptr.s.addr += (work->word2.s.is_v6 ^ 1) << 2;
+			bptr.s.addr += (work->word2.s.is_v6 ^ 1) << 2;
 		}
 
 		/* Calculate the "back" offset in 64-bit words */
@@ -469,9 +496,8 @@ cvmx_buf_ptr_t cvmx_wqe_get_packet_ptr(cvmx_wqe_t *work)
 
 void cvmx_wqe_free(cvmx_wqe_t *work)
 {
-	unsigned ncl;
+	unsigned ncl = 0;
 	unsigned pool;
-	uint64_t paddr;
 	cvmx_wqe_78xx_t * wqe = (void *) work;
 
 	/* Free native untranslated 78xx WQE */
@@ -486,24 +512,30 @@ void cvmx_wqe_free(cvmx_wqe_t *work)
 			return;
 	} else {
 		/* determine if packet is inside WQE the old way */
-		if (cvmx_wqe_get_bufs(work) > 0) {
+		if (cvmx_likely(work->word2.s_cn38xx.bufs > 0)) {
+			uint64_t paddr, paddr1;
+
 			/* Check if the first data buffer is inside WQE */
-			paddr = (work->packet_ptr.s.addr >> 7) -
-				work->packet_ptr.s.back;
-			paddr = paddr << 7;
+			paddr = (work->packet_ptr.s.addr & (~0x7full)) -
+				(work->packet_ptr.s.back << 7);
+			paddr1 = cvmx_ptr_to_phys(work);
 
 			/* do not free WQE if contains first data buffer */
-			if (paddr == cvmx_ptr_to_phys(work))
+			if (paddr == paddr1)
 				return;
 		}
 	}
 
 	/* At this point it is clear the WQE needs to be freed */
 	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
+		cvmx_fpa3_gaura_t aura =
+			__cvmx_fpa3_gaura(
+				wqe->word0.aura >> 10,
+				wqe->word0.aura * 0x3ff);
 		/* First buffer outside WQE, but WQE comes from the same AURA */
 		/* Only a few words have been touched, not entire buf */
 		ncl = 1;
-		cvmx_fpa3_free_gaura(work, wqe->word0.aura, ncl);
+		cvmx_fpa3_free(work, aura, ncl);
 	} else {
 		/* Determine FPA pool the WQE buffer belongs to */
 		if (__cvmx_ipd_mode_no_wptr()) {
@@ -550,6 +582,7 @@ void cvmx_helper_free_packet_data(cvmx_wqe_t *work)
 
 	if (number_buffers == 0)
 		return;
+	cvmx_wqe_pki_errata_20776(work);
 
 	/* Interpret PKI-style bufptr unless it has been translated */
 	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE) &&
@@ -575,7 +608,11 @@ void cvmx_helper_free_packet_data(cvmx_wqe_t *work)
 	while (number_buffers--) {
 		if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE) &&
 		    !wqe->pki_wqe_translated) {
-			unsigned aura = cvmx_wqe_get_aura(work);
+			cvmx_fpa3_gaura_t aura =
+				__cvmx_fpa3_gaura(
+					wqe->word0.aura >> 10,
+					wqe->word0.aura * 0x3ff);
+
 			bptr.u64 = buffer_ptr.u64;
 
 			ncl = (bptr.size + CVMX_CACHE_LINE_SIZE-1)/
@@ -588,7 +625,7 @@ void cvmx_helper_free_packet_data(cvmx_wqe_t *work)
 			next_buffer_ptr = *(uint64_t *)
 				cvmx_phys_to_ptr(bptr.addr - 8);
 			/* FPA AURA comes from WQE, includes node */
-			cvmx_fpa3_free_gaura(cvmx_phys_to_ptr(start_of_buffer),	aura, ncl);
+			cvmx_fpa3_free(cvmx_phys_to_ptr(start_of_buffer), aura, ncl);
 		} else {
 			ncl = (buffer_ptr.s.size + CVMX_CACHE_LINE_SIZE-1)/
 				CVMX_CACHE_LINE_SIZE + buffer_ptr.s.back;
@@ -603,12 +640,14 @@ void cvmx_helper_free_packet_data(cvmx_wqe_t *work)
 			next_buffer_ptr = *(uint64_t *)
 				cvmx_phys_to_ptr(buffer_ptr.s.addr - 8);
 			/* FPA pool comes from buf_ptr itself */
-			if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE))
-				/* FIXME:  Which node is it? */
-				cvmx_fpa3_free_aura(cvmx_phys_to_ptr(start_of_buffer), 0, (int)buffer_ptr.s.pool, ncl);
-			else
-				cvmx_fpa1_free(cvmx_phys_to_ptr(start_of_buffer),
-					      buffer_ptr.s.pool, ncl);
+			if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
+				cvmx_fpa3_gaura_t aura =
+					cvmx_fpa1_pool_to_fpa3_aura(buffer_ptr.s.pool);
+				cvmx_fpa3_free(cvmx_phys_to_ptr(start_of_buffer), aura, ncl);
+			} else
+				cvmx_fpa1_free(
+					cvmx_phys_to_ptr(start_of_buffer),
+					buffer_ptr.s.pool, ncl);
 		}
 		buffer_ptr.u64 = next_buffer_ptr;
 	}
@@ -845,7 +884,9 @@ int cvmx_helper_get_ipd_port(int xiface, int index)
 		if (port_map[xi.interface].type == GMII) {
 			cvmx_helper_interface_mode_t mode;
 			mode = cvmx_helper_interface_get_mode(xiface);
-			if (mode == CVMX_HELPER_INTERFACE_MODE_XAUI) {
+			if (mode == CVMX_HELPER_INTERFACE_MODE_XAUI
+			    || (mode == CVMX_HELPER_INTERFACE_MODE_RXAUI
+				&& OCTEON_IS_MODEL(OCTEON_CN68XX))) {
 				ipd_port += port_map[xi.interface].ipd_port_adj;
 				return ipd_port;
 			} else
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper.c b/arch/mips/cavium-octeon/executive/cvmx-helper.c
index 0b253d1..b219e16 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper.c
@@ -63,7 +63,7 @@
 
 #include <asm/octeon/cvmx-agl.h>
 #include <asm/octeon/cvmx-gmx.h>
-#include <asm/octeon/cvmx-fpa1.h>
+#include <asm/octeon/cvmx-fpa.h>
 #include <asm/octeon/cvmx-pip.h>
 #include <asm/octeon/cvmx-hwpko.h>
 #include <asm/octeon/cvmx-pko3.h>
@@ -505,7 +505,8 @@ int __cvmx_helper_init_interface(int xiface, int num_ipd_ports,
 		panic("Cannot allocate memory in __cvmx_helper_init_interface.");
 #else
 	snprintf(name, sizeof(name), "__int_%d_link_info", xi.interface);
-	addr = CAST64(cvmx_bootmem_alloc_named_range_once(sz, 0, 0, 128, name, NULL));
+	addr = CAST64(cvmx_bootmem_alloc_named_range_once(sz, 0, 0,
+		__alignof(cvmx_helper_link_info_t), name, NULL));
 	piface->cvif_ipd_port_link_info = (cvmx_helper_link_info_t *) __cvmx_phys_addr_to_ptr(addr, sz);
 #endif
 	if (!piface->cvif_ipd_port_link_info) {
@@ -2499,4 +2500,23 @@ void cvmx_helper_setup_simulator_io_buffer_counts(int node, int num_packet_buffe
 	}
 }
 
+void *cvmx_helper_mem_alloc(int node, uint64_t alloc_size, uint64_t align)
+{
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+	return kmalloc(alloc_size, GFP_NOIO | GFP_DMA); //FIXME alignment
+#else
+	return cvmx_phys_to_ptr(cvmx_bootmem_phy_alloc_range(alloc_size, align,
+							     cvmx_addr_on_node(node, 0ull),
+							     cvmx_addr_on_node(node, 0xffffffffff)));
+#endif
+}
+
+void cvmx_helper_mem_free(void *buffer, uint64_t size)
+{
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+	kfree(buffer);
+#else
+	__cvmx_bootmem_phy_free(cvmx_ptr_to_phys(buffer), size, 0);
+#endif
+}
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pcie.c b/arch/mips/cavium-octeon/executive/cvmx-pcie.c
index 08820d7..f8e2fdc 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pcie.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pcie.c
@@ -42,7 +42,7 @@
  *
  * Interface to PCIe as a host(RC) or target(EP)
  *
- * <hr>$Revision: 100548 $<hr>
+ * <hr>$Revision: 103672 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
@@ -433,7 +433,7 @@ static void __cvmx_pcie_rc_initialize_config_space(int pcie_port)
 		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG452(pcie_port), cfg452.u32);
 	}
 
-	if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X)) {
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_0)) {
 		cvmx_pciercx_cfg089_t cfg089;
 		cvmx_pciercx_cfg090_t cfg090;
 		cvmx_pciercx_cfg091_t cfg091;
@@ -1016,7 +1016,7 @@ static int __cvmx_pcie_rc_initialize_link_gen2(int pcie_port)
 	try_gen3 = (pciercx_cfg031.s.mls == 3);
 
 	/* Errata (GSER-21178) PCIe gen3 doesn't work */
-	if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X) && try_gen3) {
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_0) && try_gen3) {
 		/* Force Gen1 for initial link bringup. We'll fix it later */
 		pciercx_cfg031.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG031(pcie_port));
 		pciercx_cfg031.s.mls = 1;
@@ -1042,7 +1042,7 @@ static int __cvmx_pcie_rc_initialize_link_gen2(int pcie_port)
 	} while ((pciercx_cfg032.s.dlla == 0) || (pciercx_cfg032.s.lt == 1));
 
 	/* Errata (GSER-21178) PCIe gen3 doesn't work, continued */
-	if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X) && try_gen3) {
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_0) && try_gen3) {
 		cvmx_pciercx_cfg031_t cfg031;
 		cvmx_pciercx_cfg032_t cfg032;
 		cvmx_pciercx_cfg040_t cfg040;
@@ -1190,51 +1190,6 @@ static int __cvmx_pcie_get_qlm(int pcie_port)
 }
 
 /**
- * @INTERNAL
- * Set the MPLL Multiplier for PCIe port
- */
-static void __cvmx_pcie_set_mult(int pcie_port)
-{
-	cvmx_gserx_dlmx_ref_clkdiv2_t ref_clkdiv2;
-	cvmx_gserx_dlmx_mpll_multiplier_t mpll_multiplier;
-	uint64_t meas_refclock, mult;
-	int baud_mhz = 2500;
-	int qlm = __cvmx_pcie_get_qlm(pcie_port);
-	int old_multiplier = 56; /* Value as per Errata 20669 */
-
-	if (!OCTEON_IS_MODEL(OCTEON_CN70XX))
-		return;
-
-	if (qlm == -1)
-		return;
-
-	ref_clkdiv2.u64 = cvmx_read_csr(CVMX_GSERX_DLMX_REF_CLKDIV2(qlm, 0));
-	if (ref_clkdiv2.s.ref_clkdiv2 == 0) {
-		ref_clkdiv2.s.ref_clkdiv2 = 1;
-		cvmx_write_csr(CVMX_GSERX_DLMX_REF_CLKDIV2(qlm, 0),
-				ref_clkdiv2.u64);
-		cvmx_wait(10000);
-	}
-
-	meas_refclock = cvmx_qlm_measure_clock(qlm);
-	if (meas_refclock == 0) {
-		cvmx_warn("DLM%d: Reference clock not running\n", qlm);
-		return;
-	}
-
-	mult = (uint64_t)baud_mhz * 1000000 + (meas_refclock/2);
-	mult /= meas_refclock;
-
-	do {
-		mpll_multiplier.u64 = cvmx_read_csr(CVMX_GSERX_DLMX_MPLL_MULTIPLIER(qlm, 0));
-		mpll_multiplier.s.mpll_multiplier = --old_multiplier;
-		cvmx_write_csr(CVMX_GSERX_DLMX_MPLL_MULTIPLIER(qlm, 0), mpll_multiplier.u64);
-		/* Wait for 1 ms */
-		cvmx_wait_usec(1000);
-	} while (old_multiplier > (int)mult);
-}
-
-/**
  * Initialize a PCIe gen 2 port for use in host(RC) mode. It doesn't enumerate
  * the bus.
  *
@@ -1529,7 +1484,10 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 	cvmx_wait_usec(1000);
 
 	/* Set MPLL multiplier as per Errata 20669. */
-	__cvmx_pcie_set_mult(pcie_port);
+	if (OCTEON_IS_MODEL(OCTEON_CN70XX)) {
+		int qlm = __cvmx_pcie_get_qlm(pcie_port);
+		__cvmx_qlm_set_mult(qlm, 2500, 56);
+	}
 
 	/* Check and make sure PCIe came out of reset. If it doesn't the board
 	   probably hasn't wired the clocks up and the interface should be
@@ -1928,6 +1886,11 @@ void cvmx_pcie_config_write32(int pcie_port, int bus, int dev, int fn,
  */
 uint32_t cvmx_pcie_cfgx_read(int pcie_port, uint32_t cfg_offset)
 {
+	return cvmx_pcie_cfgx_read_node(0, pcie_port, cfg_offset);
+}
+
+uint32_t cvmx_pcie_cfgx_read_node(int node, int pcie_port, uint32_t cfg_offset)
+{
 	if (octeon_has_feature(OCTEON_FEATURE_NPEI)) {
 		cvmx_pescx_cfg_rd_t pescx_cfg_rd;
 		pescx_cfg_rd.u64 = 0;
@@ -1939,8 +1902,8 @@ uint32_t cvmx_pcie_cfgx_read(int pcie_port, uint32_t cfg_offset)
 		cvmx_pemx_cfg_rd_t pemx_cfg_rd;
 		pemx_cfg_rd.u64 = 0;
 		pemx_cfg_rd.s.addr = cfg_offset;
-		cvmx_write_csr(CVMX_PEMX_CFG_RD(pcie_port), pemx_cfg_rd.u64);
-		pemx_cfg_rd.u64 = cvmx_read_csr(CVMX_PEMX_CFG_RD(pcie_port));
+		cvmx_write_csr_node(node, CVMX_PEMX_CFG_RD(pcie_port), pemx_cfg_rd.u64);
+		pemx_cfg_rd.u64 = cvmx_read_csr_node(node, CVMX_PEMX_CFG_RD(pcie_port));
 		return pemx_cfg_rd.s.data;
 	}
 }
@@ -1955,6 +1918,11 @@ uint32_t cvmx_pcie_cfgx_read(int pcie_port, uint32_t cfg_offset)
  */
 void cvmx_pcie_cfgx_write(int pcie_port, uint32_t cfg_offset, uint32_t val)
 {
+	cvmx_pcie_cfgx_write_node(0, pcie_port, cfg_offset, val);
+}
+
+void cvmx_pcie_cfgx_write_node(int node, int pcie_port, uint32_t cfg_offset, uint32_t val)
+{
 	if (octeon_has_feature(OCTEON_FEATURE_NPEI)) {
 		cvmx_pescx_cfg_wr_t pescx_cfg_wr;
 		pescx_cfg_wr.u64 = 0;
@@ -1966,7 +1934,7 @@ void cvmx_pcie_cfgx_write(int pcie_port, uint32_t cfg_offset, uint32_t val)
 		pemx_cfg_wr.u64 = 0;
 		pemx_cfg_wr.s.addr = cfg_offset;
 		pemx_cfg_wr.s.data = val;
-		cvmx_write_csr(CVMX_PEMX_CFG_WR(pcie_port), pemx_cfg_wr.u64);
+		cvmx_write_csr_node(node, CVMX_PEMX_CFG_WR(pcie_port), pemx_cfg_wr.u64);
 	}
 }
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pki-cluster.h b/arch/mips/cavium-octeon/executive/cvmx-pki-cluster.h
deleted file mode 100644
index 66cd876..0000000
--- a/arch/mips/cavium-octeon/executive/cvmx-pki-cluster.h
+++ /dev/null
@@ -1,636 +0,0 @@
-/* This file is autgenerated from obj/ipemainc.elf */
-const int cvmx_pki_cluster_code_length = 632;
-const uint64_t cvmx_pki_cluster_code_default[] = {
-    0x000000000a000000ull,
-    0x0000413a68024070ull,
-    0x0000813800200020ull,
-    0x900081b800200020ull,
-    0x0004dd80ffff0001ull,
-    0x000455ab68010b0eull,
-    0x00045fba46010000ull,
-    0x9046898120002000ull,
-    0x0004418068010028ull,
-    0x90665326680100f0ull,
-    0x0004413f68004070ull,
-    0x000653a7680100f0ull,
-    0x00045dbb6803a0f0ull,
-    0x000401bb48000001ull,
-    0x00045cb968030870ull,
-    0x0007debd00100010ull,
-    0x0000813b80008000ull,
-    0x0004413b68004070ull,
-    0x9001c00000000000ull,
-    0x9021c00000000000ull,
-    0x00044180680100f0ull,
-    0x0004c639ff000200ull,
-    0x0004400172030000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x000041ba68034078ull,
-    0x0000512268030870ull,
-    0x000041bc68034070ull,
-    0x00005d3a68030870ull,
-    0x000483891f000000ull,
-    0x000f542868090a48ull,
-    0x000f583068020070ull,
-    0x00045cb942080000ull,
-    0x0004552a4e09312dull,
-    0x00045cb968082868ull,
-    0x0004410246090000ull,
-    0x0000813901000000ull,
-    0x000481b800010001ull,
-    0x000685b800020002ull,
-    0xa006823800010001ull,
-    0x0006c639ff000500ull,
-    0xa0685f3e68010405ull,
-    0x0008418368010800ull,
-    0xa0485f3e68010305ull,
-    0xa4085f3e68010028ull,
-    0xa441c00000000000ull,
-    0x0009418368010030ull,
-    0xa466400172030001ull,
-    0x00095f3e68030030ull,
-    0x00095f3e68010416ull,
-    0x0006debd00010001ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x00065cb942080000ull,
-    0x0006552a4e09312dull,
-    0x00065cb968082868ull,
-    0x0006410246090000ull,
-    0x9000813901000000ull,
-    0x0004c639ff000a00ull,
-    0x0004400072010000ull,
-    0x00048181ffffffffull,
-    0x0007820101000100ull,
-    0x00048301ffff0180ull,
-    0x0008d5ab10001000ull,
-    0x0004d4a900010001ull,
-    0x0001c00000000000ull,
-    0x00045cb942080000ull,
-    0x9024552a4e09312dull,
-    0x0004c639ff000b00ull,
-    0x90445f80680100f0ull,
-    0x000459b368020070ull,
-    0x000401024000000cull,
-    0x0006823fffffffffull,
-    0x00088281ffffffffull,
-    0x000ad5ab20002000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0004403f72010001ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x000c8b3fffffc200ull,
-    0x000c8b01ffff0001ull,
-    0x000ddebd00020002ull,
-    0x00045cb942080000ull,
-    0x0004552a4e09312dull,
-    0x00045cb968082868ull,
-    0x0004410246090000ull,
-    0x0000813901000000ull,
-    0x000481b800080008ull,
-    0x9846c639ff001200ull,
-    0x9861c00000000000ull,
-    0x00064180680100f0ull,
-    0x0006400372010000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x000683891f000200ull,
-    0x000ed52a00800080ull,
-    0x000e5e3c68020070ull,
-    0x00065cb942080000ull,
-    0x0006552a4e09312dull,
-    0x00065cb968082868ull,
-    0x0006410246090000ull,
-    0x0000813d00020002ull,
-    0x0004893901000000ull,
-    0x9004893800040004ull,
-    0x9024c639ff001300ull,
-    0x00044180680100f0ull,
-    0x9044400372010001ull,
-    0x0001c00000000000ull,
-    0x00045f3e68010044ull,
-    0x0004debd00040004ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x000483891f000200ull,
-    0x000ed52a00800080ull,
-    0x000e5e3c68020070ull,
-    0x00045cb942080000ull,
-    0x0004552a4e09312dull,
-    0x00045cb968082868ull,
-    0x0004410246090000ull,
-    0x000581b902000000ull,
-    0x9826c639ff001800ull,
-    0x9801c00000000000ull,
-    0x00064180680100f0ull,
-    0x0006400172030000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x000682091f000200ull,
-    0x000883aa00800080ull,
-    0x000ed52a00400040ull,
-    0x000e5e3c68020870ull,
-    0x000fd52a00800080ull,
-    0x000f5e3c68020070ull,
-    0x000983891f000000ull,
-    0x000f54a968090148ull,
-    0x000f59b368020870ull,
-    0x00065cb942080000ull,
-    0x0006552a4e09312dull,
-    0x00065cb968082868ull,
-    0x0006410246090000ull,
-    0x000081b902000000ull,
-    0x9826c639ff001900ull,
-    0x9801c00000000000ull,
-    0x00064180680100f0ull,
-    0x0006400172030001ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x000682091f000200ull,
-    0x000883aa00800080ull,
-    0x000ed52a00400040ull,
-    0x000e5e3c68020870ull,
-    0x000fd52a00800080ull,
-    0x000f5e3c68020070ull,
-    0x000983891f000000ull,
-    0x000f54a968090148ull,
-    0x000f59b368020870ull,
-    0x00065cb942080000ull,
-    0x0006552a4e09312dull,
-    0x00065cb968082868ull,
-    0x0006410246090000ull,
-    0x000081b902000000ull,
-    0x9826c639ff001a00ull,
-    0x9801c00000000000ull,
-    0x00064180680100f0ull,
-    0x0006400172030000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x000682091f000200ull,
-    0x000883aa00800080ull,
-    0x000ed52a00400040ull,
-    0x000e5e3c68020870ull,
-    0x000fd52a00800080ull,
-    0x000f5e3c68020070ull,
-    0x000983891f000000ull,
-    0x000f54a968090148ull,
-    0x000f59b368020870ull,
-    0x00065cb942080000ull,
-    0x0006552a4e09312dull,
-    0x00065cb968082868ull,
-    0x0006410246090000ull,
-    0x000081b902000000ull,
-    0x9826c639ff001b00ull,
-    0x9801c00000000000ull,
-    0x00064180680100f0ull,
-    0x0006400172030001ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x000682091f000200ull,
-    0x000883aa00800080ull,
-    0x000ed52a00400040ull,
-    0x000e5e3c68020870ull,
-    0x000fd52a00800080ull,
-    0x000f5e3c68020070ull,
-    0x000983891f000000ull,
-    0x000f54a968090148ull,
-    0x000f59b368020870ull,
-    0x00065cb942080000ull,
-    0x0006552a4e09312dull,
-    0x00065cb968082868ull,
-    0x0006410246090000ull,
-    0x9000813902000000ull,
-    0x000481b800400040ull,
-    0x00004180680100f0ull,
-    0x00068981ffff8847ull,
-    0x00068581ffff8848ull,
-    0x0006debd00080008ull,
-    0x9806c639ff001e00ull,
-    0x9821c00000000000ull,
-    0x00065f80680100f0ull,
-    0x0006403f72010000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x00065cb942080000ull,
-    0x0006552a4e09312dull,
-    0x00065cb968082868ull,
-    0x0006823902000000ull,
-    0x0008010240000002ull,
-    0xac28828101000100ull,
-    0x000b010240000002ull,
-    0xa42b820101000100ull,
-    0x0009010240000002ull,
-    0xac29828101000100ull,
-    0x000b010240000002ull,
-    0xa42b820101000100ull,
-    0x0009010240000002ull,
-    0xac29828101000100ull,
-    0x000b010240000002ull,
-    0x00065f3e68010629ull,
-    0x00040183840005ffull,
-    0x0006010240000008ull,
-    0x9801c00000000000ull,
-    0x0006debd00200020ull,
-    0x00048181ffff0806ull,
-    0x0006d4a907c00180ull,
-    0x00048201ffff8035ull,
-    0x00068581ffff8035ull,
-    0x0008d4a907c001c0ull,
-    0x0006dcb97c007c00ull,
-    0x00048201ffff0800ull,
-    0x00088601ffff86ddull,
-    0x00068581ffff0800ull,
-    0x00068581ffff86ddull,
-    0x0008d4a907c00200ull,
-    0x0007823d00200020ull,
-    0x000685bd00200020ull,
-    0x0008d4a907c00140ull,
-    0x0006010240000002ull,
-    0x8001c00000000000ull,
-    0x0006593268020070ull,
-    0x000315ab74000227ull,
-    0x9000813904000000ull,
-    0x0001c00000000000ull,
-    0x00048181f0004000ull,
-    0x9886593268020070ull,
-    0x0006d4a907c00200ull,
-    0x00068201ff000000ull,
-    0xa40815ab74000345ull,
-    0x0009debd01000100ull,
-    0x0009418068010038ull,
-    0x0009028386000005ull,
-    0xac8a15ab74000343ull,
-    0x000b5a3468010870ull,
-    0x000b5a3468010070ull,
-    0xac6b8203000f0005ull,
-    0x0009d4a907c00240ull,
-    0x000b820120000000ull,
-    0x000886011fff0000ull,
-    0x0009552a6801000dull,
-    0x0009d4a9f8006800ull,
-    0x0009593268020870ull,
-    0x0006418068030230ull,
-    0x0006410242030000ull,
-    0x9c01c00000000000ull,
-    0x0001c00000000000ull,
-    0x00078201f0006000ull,
-    0x0008593268020070ull,
-    0xa068d4a907c00280ull,
-    0x00085a3468010874ull,
-    0x0008818100ff0000ull,
-    0x000615ab74000345ull,
-    0x00075a3468010078ull,
-    0x0007010240000028ull,
-    0xa80782b400ff0000ull,
-    0x000ad4a907c002c0ull,
-    0x000a5a3468010078ull,
-    0x000a410244010000ull,
-    0xa80782b400ff003cull,
-    0x000ad4a907c002c0ull,
-    0x000a5a3468010078ull,
-    0x000a410244010000ull,
-    0xa80782b400ff002bull,
-    0x000ad4a907c002c0ull,
-    0x000a5a3468010078ull,
-    0x000a410244010000ull,
-    0xa80782b400ff002cull,
-    0x000ad4a9ffc06ac0ull,
-    0x000a593268020870ull,
-    0x000ad52a00010001ull,
-    0x000a5a3468010078ull,
-    0x0007debd01000100ull,
-    0x000481bd01000100ull,
-    0x0006c639ff002300ull,
-    0x000641aa68034000ull,
-    0x000641a968034846ull,
-    0x0006403472030001ull,
-    0x0004822902000200ull,
-    0x000915ab74000341ull,
-    0x000082aa00010001ull,
-    0x000a86ab00ff0045ull,
-    0x000adcb978007800ull,
-    0x00008229fa006a00ull,
-    0x00065cb942080000ull,
-    0x0006552a4e09312dull,
-    0x00065cb968082868ull,
-    0x0006410246090000ull,
-    0x8001c00000000000ull,
-    0x00088a3908000000ull,
-    0xa02315ab74000343ull,
-    0x000881b400ff0011ull,
-    0x00068981ffff2118ull,
-    0x0006593268020870ull,
-    0x0006d4a9f8009800ull,
-    0x9c26debd02000200ull,
-    0x0007813400ff002full,
-    0x00048901ffff6558ull,
-    0x0004593268020870ull,
-    0x0004d4a9f800a800ull,
-    0x0004debd02000200ull,
-    0x000882bd02000200ull,
-    0xa86ac639ff002800ull,
-    0xa841c00000000000ull,
-    0x000a418368010878ull,
-    0x000a400172030000ull,
-    0x000a5bb768030078ull,
-    0x000a5b36680100f0ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x000a5cb942080000ull,
-    0x000a552a4e09312dull,
-    0x000a5cb968082868ull,
-    0x000a410246090000ull,
-    0x0000812907c004c0ull,
-    0x0004852907c00540ull,
-    0x9004893910000000ull,
-    0x0001c00000000000ull,
-    0x00048181f0004000ull,
-    0x988658b168020070ull,
-    0x0006d428001f0008ull,
-    0x00068201ff000000ull,
-    0xa40815ab74000545ull,
-    0x0009debd04000400ull,
-    0x0009418068010038ull,
-    0x0009028384000004ull,
-    0xac8a15ab74000543ull,
-    0x000b5a3468010870ull,
-    0x000b5a3468010070ull,
-    0xac6b8303000f0005ull,
-    0x000dd428001f0009ull,
-    0x000b830120000000ull,
-    0x000c87011fff0000ull,
-    0x000dd42803e001a0ull,
-    0x000d58b168020870ull,
-    0x000ddcb960006000ull,
-    0x0006418068030230ull,
-    0x0006410242030000ull,
-    0x9c01c00000000000ull,
-    0x0001c00000000000ull,
-    0x00078201f0006000ull,
-    0x000858b168020070ull,
-    0xa068d428001f000aull,
-    0x00085a3468010874ull,
-    0x0008818100ff0000ull,
-    0x000615ab74000545ull,
-    0x00075a3468010078ull,
-    0x0007010240000028ull,
-    0xa80782b400ff0000ull,
-    0x000ad428001f000bull,
-    0x000a5a3468010078ull,
-    0x000a410244010000ull,
-    0xa80782b400ff003cull,
-    0x000ad428001f000bull,
-    0x000a5a3468010078ull,
-    0x000a410244010000ull,
-    0xa80782b400ff002bull,
-    0x000ad428001f000bull,
-    0x000a5a3468010078ull,
-    0x000a410244010000ull,
-    0xa80782b400ff002cull,
-    0x000ad42803ff01abull,
-    0x000adcb960006000ull,
-    0x000a58b168020870ull,
-    0x000a5a3468010078ull,
-    0x0007debd04000400ull,
-    0x000481bd04000400ull,
-    0x0006c639ff002b00ull,
-    0x000641aa68034000ull,
-    0x000641a868034840ull,
-    0x0006403472030001ull,
-    0x0004822902000200ull,
-    0x000815ab74000541ull,
-    0x000082ab00ff0045ull,
-    0x000adcb960006000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x00065cb942080000ull,
-    0x0006552a4e09312dull,
-    0x00065cb968082868ull,
-    0x0006410246090000ull,
-    0x8001c00000000000ull,
-    0x0001c00000000000ull,
-    0x000315ab74000561ull,
-    0x0000813920000000ull,
-    0x000481b400ff006cull,
-    0x0006d42803e001c0ull,
-    0x000658b168020870ull,
-    0x0007823400ff0032ull,
-    0xa048863400ff0033ull,
-    0x0008d42803e00180ull,
-    0xa0685ab5680100f0ull,
-    0x000858b168020870ull,
-    0x00085dbb680100f0ull,
-    0x986981b400ff002full,
-    0x0006d42803e00280ull,
-    0x00065ab5680100f0ull,
-    0x000658b168020870ull,
-    0x0004823400ff0011ull,
-    0x0008d42803e00220ull,
-    0x000481b400ff0084ull,
-    0x0008863400ff0084ull,
-    0x0006d42803e00240ull,
-    0x000481b400ff0006ull,
-    0x98c8863400ff0006ull,
-    0x0006d42803e00200ull,
-    0x90265ebd68010b31ull,
-    0x0004c639ff003000ull,
-    0x0004403472010000ull,
-    0x000858b168020870ull,
-    0x00088181ffff0000ull,
-    0x000615ab74000664ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x000081b800100010ull,
-    0x00045cb942080000ull,
-    0x0004552a4e09312dull,
-    0x00045cb968082868ull,
-    0x0004410246090000ull,
-    0x000483891f000000ull,
-    0x000f542868090a48ull,
-    0x000f583068020070ull,
-    0x000689b940004000ull,
-    0x000689a803e00000ull,
-    0x000641b168004078ull,
-    0x0006413842030000ull,
-    0x9801c00000000000ull,
-    0x9821c00000000000ull,
-    0x00064180680100f0ull,
-    0x0006c639ff003900ull,
-    0x0006400172030001ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x000683891f000000ull,
-    0x000f542868090a48ull,
-    0x000f583068020070ull,
-    0x00065cb942080000ull,
-    0x0006552a4e09312dull,
-    0x00065cb968082868ull,
-    0x0006410246090000ull,
-    0x00005fb968004250ull,
-    0x0000003f70000000ull,
-    0x000041b968034070ull,
-    0x0000512268030070ull,
-    0x0000813800200020ull,
-    0x0004413a68024070ull,
-    0x9001c00000000000ull,
-    0x000081b800200020ull,
-    0x9026898180008000ull,
-    0x0004890110001000ull,
-    0x000456ad680100a0ull,
-    0x0006898180008000ull,
-    0x000652a56801001dull,
-    0x000456ad68090b5bull,
-    0x000556ad680900f0ull,
-    0x0000562c680800f0ull,
-    0x0000833d00200020ull,
-    0x000c872907c00000ull,
-    0x000dd62c20000000ull,
-    0x0000822902800280ull,
-    0x000841b268034070ull,
-    0x000982a802800280ull,
-    0x000a41b168034070ull,
-    0x000b822907c00000ull,
-    0x0000003f70000800ull,
-    0x000941b268034070ull,
-    0x0000418048030000ull,
-    0x0000018340000008ull,
-    0x0009018348000004ull,
-    0x000050a168030c20ull,
-    0x000082aa00800080ull,
-    0x000850a168080c2bull,
-    0x000752a56808001eull,
-    0x000a822a00400040ull,
-    0x00088a0900010001ull,
-    0x000841bc68034078ull,
-    0x000941bc68034070ull,
-    0x000a583068030870ull,
-    0x0005c180ffff0000ull,
-    0x00058288001e0000ull,
-    0x000b8208001e0008ull,
-    0x00085d2168004030ull,
-    0x00098308001e0010ull,
-    0x00088608001e0010ull,
-    0x000c5d2168004070ull,
-    0x0008418068080025ull,
-    0x000841ba6803a0f0ull,
-    0x000856ad40030000ull,
-    0x0008c180ffff0000ull,
-    0x0005820807000500ull,
-    0x00088a3d00010001ull,
-    0x000841be68004050ull,
-    0x0005828807000300ull,
-    0x000a8abd00040004ull,
-    0x000a41be68004040ull,
-    0x0005820807000100ull,
-    0x00088a2a00800080ull,
-    0x0008413068004078ull,
-    0xa021c00000000000ull,
-    0x0005828807000200ull,
-    0x000841806801002dull,
-    0x000a8abd00080008ull,
-    0x000a41be68004026ull,
-    0x0005820807000400ull,
-    0x00088a2902000200ull,
-    0x000841b46800405aull,
-    0x000556ad40030000ull,
-    0x000081bd00100010ull,
-    0x0006c180ffff0000ull,
-    0x0006822a00800080ull,
-    0x00088a0900100010ull,
-    0x0008413c68024070ull,
-    0xa021c00000000000ull,
-    0x0006832902000200ull,
-    0x0008c181f0008000ull,
-    0x000841834c00ffffull,
-    0x0006822a00400040ull,
-    0x00088a0900200020ull,
-    0x0008413c68024078ull,
-    0xa021c00000000000ull,
-    0x000c828901000100ull,
-    0x0008dc01f0008000ull,
-    0x000841b84c03ffffull,
-    0x000a823400ff0033ull,
-    0x000841bb4c03ffffull,
-    0x0008863400ff0014ull,
-    0x000841b54c03ffffull,
-    0x000c828900400040ull,
-    0x000a41b44c0300ffull,
-    0x000682a9f800a800ull,
-    0x000a86a9f8009800ull,
-    0x000a8a8904000400ull,
-    0x000a41b74c0300ffull,
-    0x000a41b64c03ffffull,
-    0x000682287c005800ull,
-    0x00088a0902000200ull,
-    0x0008413068024070ull,
-    0xa001c00000000000ull,
-    0x0006830900020002ull,
-    0x00088281e0002000ull,
-    0xa84a868108000800ull,
-    0xa861c00000000000ull,
-    0x000a41814c03ffffull,
-    0x000a41814c03ffffull,
-    0x000653a7680300f0ull,
-    0x000c5321680040b0ull,
-    0x000dd3260fff0fffull,
-    0x0000003f70000400ull,
-    0x0001c00000000000ull,
-    0x0001c00000000000ull,
-    0x000082a902000200ull,
-    0x000a413268024070ull,
-    0xa50a822902800280ull,
-    0x0006828900800080ull,
-    0x00098301ffffffffull,
-    0xb12d8381f000e000ull,
-    0x000ed5ab40004000ull,
-    0xa18c8381ffffffffull,
-    0x000a8abd08000800ull,
-    0x000e8781ff00ff00ull,
-    0x000ed5ab80008000ull,
-    0x000a8abd40000000ull,
-    0x0000572e680800f0ull,
-    0x000057af680900f0ull,
-    0x0007d72ef0ff0000ull,
-    0x0007d7aff0000000ull,
-    0x000ad72e00fc0000ull,
-    0x0000000008000000ull
-};
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pki.c b/arch/mips/cavium-octeon/executive/cvmx-pki.c
index 4fb65ca..a2bb7a3 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pki.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pki.c
@@ -49,7 +49,6 @@
 #include <asm/octeon/cvmx-pki.h>
 #include <asm/octeon/cvmx-fpa3.h>
 #include <asm/octeon/cvmx-pki-cluster.h>
-#include <asm/octeon/cvmx-pki-resources.h>
 #else
 #include "cvmx.h"
 #include "cvmx-version.h"
@@ -57,7 +56,6 @@
 #include "cvmx-pki-defs.h"
 #include "cvmx-pki.h"
 #include "cvmx-fpa3.h"
-#include "cvmx-pki-resources.h"
 #include "cvmx-pki-cluster.h"
 #endif
 
@@ -156,23 +154,30 @@ void __cvmx_pki_free_ptr(int node)
 	cvmx_write_csr_node(node, CVMX_PKI_BUF_CTL, buf_ctl.u64);
 }
 
-void cvmx_pki_read_global_cfg(int node, struct cvmx_pki_global_config *gbl_cfg)
+/**
+ * This function reads global configuration of PKI block.
+ * @param node		      node number.
+ * @param gbl_cfg	      pointer to struct to read global configuration
+ */
+void cvmx_pki_read_global_config(int node, struct cvmx_pki_global_config *gbl_cfg)
 {
-        cvmx_pki_stat_ctl_t stat_ctl;
-        cvmx_pki_icgx_cfg_t pki_cl_grp;
-        cvmx_pki_gbl_pen_t gbl_pen_reg;
-        cvmx_pki_tag_secret_t tag_secret_reg;
-        cvmx_pki_frm_len_chkx_t frm_len_chk;
-        int cl_grp;
-        int id;
+	cvmx_pki_stat_ctl_t stat_ctl;
+	cvmx_pki_icgx_cfg_t pki_cl_grp;
+	cvmx_pki_gbl_pen_t gbl_pen_reg;
+	cvmx_pki_tag_secret_t tag_secret_reg;
+	cvmx_pki_frm_len_chkx_t frm_len_chk;
+	cvmx_pki_buf_ctl_t buf_ctl;
+	int cl_grp;
+	int id;
 
 	stat_ctl.u64 = cvmx_read_csr_node(node, CVMX_PKI_STAT_CTL);
-        gbl_cfg->stat_mode = stat_ctl.s.mode;
+	gbl_cfg->stat_mode = stat_ctl.s.mode;
 
-        for (cl_grp = 0; cl_grp < CVMX_PKI_NUM_CLUSTER_GROUP; cl_grp++) {
-                pki_cl_grp.u64 = cvmx_read_csr_node(node, CVMX_PKI_ICGX_CFG(cl_grp));
-                gbl_cfg->cluster_mask[cl_grp] = pki_cl_grp.s.clusters;
-        }
+	for (cl_grp = 0; cl_grp < CVMX_PKI_NUM_CLUSTER_GROUP; cl_grp++) {
+		pki_cl_grp.u64 = cvmx_read_csr_node(node,
+			CVMX_PKI_ICGX_CFG(cl_grp));
+		gbl_cfg->cluster_mask[cl_grp] = pki_cl_grp.s.clusters;
+	}
 	gbl_pen_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_GBL_PEN);
 	gbl_cfg->gbl_pen.virt_pen = gbl_pen_reg.s.virt_pen;
 	gbl_cfg->gbl_pen.clg_pen = gbl_pen_reg.s.clg_pen;
@@ -191,11 +196,15 @@ void cvmx_pki_read_global_cfg(int node, struct cvmx_pki_global_config *gbl_cfg)
 	gbl_cfg->tag_secret.dst = tag_secret_reg.s.dst;
 	gbl_cfg->tag_secret.src = tag_secret_reg.s.src;
 
-        for (id = 0; id < CVMX_PKI_NUM_FRAME_CHECK; id++) {
-                frm_len_chk.u64 = cvmx_read_csr_node(node, CVMX_PKI_FRM_LEN_CHKX(id));
-                gbl_cfg->frm_len[id].maxlen = frm_len_chk.s.maxlen;
-                gbl_cfg->frm_len[id].minlen = frm_len_chk.s.minlen;
-        }
+	for (id = 0; id < CVMX_PKI_NUM_FRAME_CHECK; id++) {
+		frm_len_chk.u64 = cvmx_read_csr_node(node,
+			CVMX_PKI_FRM_LEN_CHKX(id));
+		gbl_cfg->frm_len[id].maxlen = frm_len_chk.s.maxlen;
+		gbl_cfg->frm_len[id].minlen = frm_len_chk.s.minlen;
+	}
+
+	buf_ctl.u64 = cvmx_read_csr_node(node, CVMX_PKI_BUF_CTL);
+	gbl_cfg->fpa_wait = buf_ctl.s.fpa_wait;
 }
 
 /**
@@ -208,7 +217,7 @@ void cvmx_pki_read_global_cfg(int node, struct cvmx_pki_global_config *gbl_cfg)
  *
  */
 static void cvmx_pki_write_frame_len(int node, int id,
-				     struct cvmx_pki_frame_len len_chk)
+	struct cvmx_pki_frame_len len_chk)
 {
 	cvmx_pki_frm_len_chkx_t frm_len_chk;
 	frm_len_chk.u64 = cvmx_read_csr_node(node, CVMX_PKI_FRM_LEN_CHKX(id));
@@ -217,10 +226,15 @@ static void cvmx_pki_write_frame_len(int node, int id,
 	cvmx_write_csr_node(node, CVMX_PKI_FRM_LEN_CHKX(id), frm_len_chk.u64);
 }
 
-
-void cvmx_pki_write_global_cfg(int node, struct cvmx_pki_global_config *gbl_cfg)
+/**
+ * This function writes global configuration of PKI into hw.
+ * @param node		      node number.
+ * @param gbl_cfg	      pointer to struct to global configuration
+ */
+void cvmx_pki_write_global_config(int node, struct cvmx_pki_global_config *gbl_cfg)
 {
 	cvmx_pki_stat_ctl_t stat_ctl;
+	cvmx_pki_buf_ctl_t buf_ctl;
 	int cl_grp;
 
 	for (cl_grp = 0; cl_grp < CVMX_PKI_NUM_CLUSTER_GROUP; cl_grp++)
@@ -230,6 +244,10 @@ void cvmx_pki_write_global_cfg(int node, struct cvmx_pki_global_config *gbl_cfg)
 	stat_ctl.s.mode = gbl_cfg->stat_mode;
 	cvmx_write_csr_node(node, CVMX_PKI_STAT_CTL, stat_ctl.u64);
 
+	buf_ctl.u64 = cvmx_read_csr_node(node, CVMX_PKI_BUF_CTL);
+	buf_ctl.s.fpa_wait = gbl_cfg->fpa_wait;
+	cvmx_write_csr_node(node, CVMX_PKI_BUF_CTL, buf_ctl.u64);
+
 	cvmx_pki_write_global_parse(node, gbl_cfg->gbl_pen);
 	cvmx_pki_write_tag_secret(node, gbl_cfg->tag_secret);
 	cvmx_pki_write_frame_len(node, 0, gbl_cfg->frm_len[0]);
@@ -237,16 +255,69 @@ void cvmx_pki_write_global_cfg(int node, struct cvmx_pki_global_config *gbl_cfg)
 }
 
 /**
- * This function writes per pkind parameters in hardware which defines how
+ * This function reads per pkind parameters in hardware which defines how
   the incoming packet is processed.
  * @param node		      node number.
  * @param pkind               PKI supports a large number of incoming interfaces
  *                            and packets arriving on different interfaces or channels
  *                            may want to be processed differently. PKI uses the pkind to
  *                            determine how the incoming packet is processed.
- * @param pkind_cfg	      struct conatining pkind configuration need to be written to hw
+ * @param pkind_cfg	      pointer to struct conatining pkind configuration read from hw
+ */
+int cvmx_pki_read_pkind_config(int node, int pkind, struct cvmx_pki_pkind_config *pkind_cfg)
+{
+	int cluster = 0;
+	uint64_t cl_mask;
+	cvmx_pki_pkindx_icgsel_t pkind_clsel;
+	cvmx_pki_clx_pkindx_style_t pkind_cfg_style;
+	cvmx_pki_icgx_cfg_t pki_cl_grp;
+	cvmx_pki_clx_pkindx_cfg_t pknd_cfg_reg;
+	cvmx_pki_clx_pkindx_skip_t pknd_skip_reg;
+
+	pkind_clsel.u64 = cvmx_read_csr_node(node,
+		CVMX_PKI_PKINDX_ICGSEL(pkind));
+	pki_cl_grp.u64 = cvmx_read_csr_node(node,
+		CVMX_PKI_ICGX_CFG(pkind_clsel.s.icg));
+	pkind_cfg->cluster_grp = (uint8_t)pkind_clsel.s.icg;
+	cl_mask = (uint64_t)pki_cl_grp.s.clusters;
+	cluster = __builtin_ffsll(cl_mask) - 1;
+
+	pkind_cfg_style.u64 = cvmx_read_csr_node(node,
+		CVMX_PKI_CLX_PKINDX_STYLE(pkind, cluster));
+	pkind_cfg->initial_parse_mode = pkind_cfg_style.s.pm;
+	pkind_cfg->initial_style = pkind_cfg_style.s.style;
+
+	pknd_cfg_reg.u64 = cvmx_read_csr_node(node,
+		CVMX_PKI_CLX_PKINDX_CFG(pkind, cluster));
+	pkind_cfg->fcs_pres = pknd_cfg_reg.s.fcs_pres;
+	pkind_cfg->parse_en.inst_hdr = pknd_cfg_reg.s.inst_hdr;
+	pkind_cfg->parse_en.mpls_en = pknd_cfg_reg.s.mpls_en;
+	pkind_cfg->parse_en.lg_custom = pknd_cfg_reg.s.lg_custom;
+	pkind_cfg->parse_en.fulc_en = pknd_cfg_reg.s.fulc_en;
+	pkind_cfg->parse_en.dsa_en = pknd_cfg_reg.s.dsa_en;
+	pkind_cfg->parse_en.hg2_en = pknd_cfg_reg.s.hg2_en;
+	pkind_cfg->parse_en.hg_en = pknd_cfg_reg.s.hg_en;
+
+	pknd_skip_reg.u64 = cvmx_read_csr_node(node,
+		CVMX_PKI_CLX_PKINDX_SKIP(pkind, cluster));
+	pkind_cfg->fcs_skip = pknd_skip_reg.s.fcs_skip;
+	pkind_cfg->inst_skip = pknd_skip_reg.s.inst_skip;
+
+	return 0;
+}
+
+/**
+ * This function writes per pkind parameters in hardware which defines how
+  the incoming packet is processed.
+ * @param node		node number.
+ * @param pkind		PKI supports a large number of incoming interfaces
+ *			and packets arriving on different interfaces or channels
+ *			may want to be processed differently. PKI uses the pkind to
+ *			determine how the incoming packet is processed.
+ * @param pkind_cfg	pointer to struct conatining pkind configuration
+ * 			need to be written in hw
  */
-int cvmx_pki_set_pkind_config(int node, int pkind, struct cvmx_pki_pkind_config *pkind_cfg)
+int cvmx_pki_write_pkind_config(int node, int pkind, struct cvmx_pki_pkind_config *pkind_cfg)
 {
 	int cluster = 0;
 	uint64_t cluster_mask;
@@ -255,6 +326,7 @@ int cvmx_pki_set_pkind_config(int node, int pkind, struct cvmx_pki_pkind_config
 	cvmx_pki_clx_pkindx_style_t pkind_cfg_style;
 	cvmx_pki_icgx_cfg_t pki_cl_grp;
 	cvmx_pki_clx_pkindx_cfg_t pknd_cfg_reg;
+	cvmx_pki_clx_pkindx_skip_t pknd_skip_reg;
 
 
 	if (pkind >= CVMX_PKI_NUM_PKIND || pkind_cfg->cluster_grp >= CVMX_PKI_NUM_CLUSTER_GROUP
@@ -274,8 +346,11 @@ int cvmx_pki_set_pkind_config(int node, int pkind, struct cvmx_pki_pkind_config
 			pkind_cfg_style.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_STYLE(pkind, cluster));
 			pkind_cfg_style.s.pm = pkind_cfg->initial_parse_mode;
 			pkind_cfg_style.s.style = pkind_cfg->initial_style;
-			cvmx_write_csr_node(node, CVMX_PKI_CLX_PKINDX_STYLE(pkind, cluster), pkind_cfg_style.u64);
-			pknd_cfg_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_CFG(pkind, cluster));
+			cvmx_write_csr_node(node,
+				CVMX_PKI_CLX_PKINDX_STYLE(pkind, cluster),
+				pkind_cfg_style.u64);
+			pknd_cfg_reg.u64 = cvmx_read_csr_node(node,
+				CVMX_PKI_CLX_PKINDX_CFG(pkind, cluster));
 			pknd_cfg_reg.s.fcs_pres = pkind_cfg->fcs_pres;
 			pknd_cfg_reg.s.inst_hdr = pkind_cfg->parse_en.inst_hdr;
 			pknd_cfg_reg.s.mpls_en = pkind_cfg->parse_en.mpls_en;
@@ -284,15 +359,66 @@ int cvmx_pki_set_pkind_config(int node, int pkind, struct cvmx_pki_pkind_config
 			pknd_cfg_reg.s.dsa_en = pkind_cfg->parse_en.dsa_en;
 			pknd_cfg_reg.s.hg2_en = pkind_cfg->parse_en.hg2_en;
 			pknd_cfg_reg.s.hg_en = pkind_cfg->parse_en.hg_en;
-			cvmx_write_csr_node(node, CVMX_PKI_CLX_PKINDX_CFG(pkind, cluster), pknd_cfg_reg.u64);
+			cvmx_write_csr_node(node,
+				CVMX_PKI_CLX_PKINDX_CFG(pkind, cluster),
+				pknd_cfg_reg.u64);
+			pknd_skip_reg.u64 = cvmx_read_csr_node(node,
+				CVMX_PKI_CLX_PKINDX_SKIP(pkind, cluster));
+			pknd_skip_reg.s.fcs_skip = pkind_cfg->fcs_skip;
+			pknd_skip_reg.s.inst_skip = pkind_cfg->inst_skip;
+			cvmx_write_csr_node(node,
+				CVMX_PKI_CLX_PKINDX_SKIP(pkind, cluster),
+				pknd_skip_reg.u64);
 		}
 		cluster++;
 	}
 	return 0;
 }
 
-/**
- * This function writes/configures parameters associated with tag configuration in hardware.
+ /** This function reads parameters associated with tag configuration in hardware.
+ * @param node		node number.
+ * @param style		style to configure tag for
+ * @param cluster_mask	Mask of clusters to configure the style for.
+ * @param tag_cfg	pointer to tag configuration struct.
+ */
+void cvmx_pki_read_tag_config(int node, int style, uint64_t cluster_mask,
+	struct cvmx_pki_style_tag_cfg *tag_cfg)
+{
+	cvmx_pki_clx_stylex_cfg2_t style_cfg2_reg;
+	cvmx_pki_clx_stylex_alg_t style_alg_reg;
+	int cluster = __builtin_ffsll(cluster_mask) - 1;
+
+	style_cfg2_reg.u64 = cvmx_read_csr_node(node,
+		CVMX_PKI_CLX_STYLEX_CFG2(style, cluster));
+	style_alg_reg.u64 = cvmx_read_csr_node(node,
+		CVMX_PKI_CLX_STYLEX_ALG(style, cluster));
+
+	tag_cfg->tag_fields.layer_g_src = style_cfg2_reg.s.tag_src_lg;
+	tag_cfg->tag_fields.layer_f_src = style_cfg2_reg.s.tag_src_lf;
+	tag_cfg->tag_fields.layer_e_src = style_cfg2_reg.s.tag_src_le;
+	tag_cfg->tag_fields.layer_d_src = style_cfg2_reg.s.tag_src_ld;
+	tag_cfg->tag_fields.layer_c_src = style_cfg2_reg.s.tag_src_lc;
+	tag_cfg->tag_fields.layer_b_src = style_cfg2_reg.s.tag_src_lb;
+	tag_cfg->tag_fields.layer_g_dst = style_cfg2_reg.s.tag_dst_lg;
+	tag_cfg->tag_fields.layer_f_dst = style_cfg2_reg.s.tag_dst_lf;
+	tag_cfg->tag_fields.layer_e_dst = style_cfg2_reg.s.tag_dst_le;
+	tag_cfg->tag_fields.layer_d_dst = style_cfg2_reg.s.tag_dst_ld;
+	tag_cfg->tag_fields.layer_c_dst = style_cfg2_reg.s.tag_dst_lc;
+	tag_cfg->tag_fields.layer_b_dst = style_cfg2_reg.s.tag_dst_lb;
+	tag_cfg->tag_fields.tag_vni = style_alg_reg.s.tag_vni;
+	tag_cfg->tag_fields.tag_gtp = style_alg_reg.s.tag_gtp;
+	tag_cfg->tag_fields.tag_spi = style_alg_reg.s.tag_spi;
+	tag_cfg->tag_fields.tag_sync = style_alg_reg.s.tag_syn;
+	tag_cfg->tag_fields.ip_prot_nexthdr = style_alg_reg.s.tag_pctl;
+	tag_cfg->tag_fields.second_vlan = style_alg_reg.s.tag_vs1;
+	tag_cfg->tag_fields.first_vlan = style_alg_reg.s.tag_vs0;
+	tag_cfg->tag_fields.mpls_label = style_alg_reg.s.tag_mpls0;
+	tag_cfg->tag_fields.input_port = style_alg_reg.s.tag_prt;
+
+	/** vinita_to_do get mask tag*/
+}
+
+ /** This function writes/configures parameters associated with tag configuration in hardware.
  * @param node	              node number.
  * @param style		      style to configure tag for
  * @param cluster_mask	      Mask of clusters to configure the style for.
@@ -320,8 +446,11 @@ void cvmx_pki_write_tag_config(int node, int style, uint64_t cluster_mask,
 			style_cfg2_reg.s.tag_dst_ld = tag_cfg->tag_fields.layer_d_dst;
 			style_cfg2_reg.s.tag_dst_lc = tag_cfg->tag_fields.layer_c_dst;
 			style_cfg2_reg.s.tag_dst_lb = tag_cfg->tag_fields.layer_b_dst;
-			cvmx_write_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG2(style, cluster), style_cfg2_reg.u64);
-			style_alg_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_ALG(style, cluster));
+			cvmx_write_csr_node(node,
+				CVMX_PKI_CLX_STYLEX_CFG2(style, cluster),
+				style_cfg2_reg.u64);
+			style_alg_reg.u64 = cvmx_read_csr_node(node,
+				CVMX_PKI_CLX_STYLEX_ALG(style, cluster));
 			style_alg_reg.s.tag_vni = tag_cfg->tag_fields.tag_vni;
 			style_alg_reg.s.tag_gtp = tag_cfg->tag_fields.tag_gtp;
 			style_alg_reg.s.tag_spi = tag_cfg->tag_fields.tag_spi;
@@ -339,6 +468,80 @@ void cvmx_pki_write_tag_config(int node, int style, uint64_t cluster_mask,
 	}
 }
 
+/**
+ * This function reads parameters associated with style in hardware.
+ * @param node		node number.
+ * @param style		style to read from.
+ * @param cluster_mask	Mask of clusters style belongs to.
+ * @param style_cfg	pointer to style config struct.
+ */
+void cvmx_pki_read_style_config(int node, int style, uint64_t cluster_mask,
+	struct cvmx_pki_style_config *style_cfg)
+{
+	cvmx_pki_clx_stylex_cfg_t style_cfg_reg;
+	cvmx_pki_clx_stylex_cfg2_t style_cfg2_reg;
+	cvmx_pki_clx_stylex_alg_t style_alg_reg;
+	cvmx_pki_stylex_buf_t style_buf_reg;
+	int cluster = __builtin_ffsll(cluster_mask) - 1;
+
+	style_cfg_reg.u64 = cvmx_read_csr_node(node,
+		CVMX_PKI_CLX_STYLEX_CFG(style, cluster));
+	style_cfg2_reg.u64 = cvmx_read_csr_node(node,
+		CVMX_PKI_CLX_STYLEX_CFG2(style, cluster));
+	style_alg_reg.u64 = cvmx_read_csr_node(node,
+		CVMX_PKI_CLX_STYLEX_ALG(style, cluster));
+	style_buf_reg.u64 = cvmx_read_csr_node(node,
+		CVMX_PKI_STYLEX_BUF(style));
+
+	style_cfg->parm_cfg.ip6_udp_opt = style_cfg_reg.s.ip6_udp_opt;
+	style_cfg->parm_cfg.lenerr_en = style_cfg_reg.s.lenerr_en;
+	style_cfg->parm_cfg.lenerr_eqpad = style_cfg_reg.s.lenerr_eqpad;
+	style_cfg->parm_cfg.maxerr_en = style_cfg_reg.s.maxerr_en;
+	style_cfg->parm_cfg.minerr_en = style_cfg_reg.s.minerr_en;
+	style_cfg->parm_cfg.fcs_chk = style_cfg_reg.s.fcs_chk;
+	style_cfg->parm_cfg.fcs_strip = style_cfg_reg.s.fcs_strip;
+	style_cfg->parm_cfg.minmax_sel = style_cfg_reg.s.minmax_sel;
+	style_cfg->parm_cfg.qpg_base = style_cfg_reg.s.qpg_base;
+	style_cfg->parm_cfg.qpg_dis_padd = style_cfg_reg.s.qpg_dis_padd;
+	style_cfg->parm_cfg.qpg_dis_aura = style_cfg_reg.s.qpg_dis_aura;
+	style_cfg->parm_cfg.qpg_dis_grp = style_cfg_reg.s.qpg_dis_grp;
+	style_cfg->parm_cfg.qpg_dis_grptag = style_cfg_reg.s.qpg_dis_grptag;
+	style_cfg->parm_cfg.rawdrp = style_cfg_reg.s.rawdrp;
+	style_cfg->parm_cfg.force_drop = style_cfg_reg.s.drop;
+	style_cfg->parm_cfg.nodrop = style_cfg_reg.s.nodrop;
+
+	style_cfg->parm_cfg.len_lg = style_cfg2_reg.s.len_lg;
+	style_cfg->parm_cfg.len_lf = style_cfg2_reg.s.len_lf;
+	style_cfg->parm_cfg.len_le = style_cfg2_reg.s.len_le;
+	style_cfg->parm_cfg.len_ld = style_cfg2_reg.s.len_ld;
+	style_cfg->parm_cfg.len_lc = style_cfg2_reg.s.len_lc;
+	style_cfg->parm_cfg.len_lb = style_cfg2_reg.s.len_lb;
+	style_cfg->parm_cfg.csum_lg = style_cfg2_reg.s.csum_lg;
+	style_cfg->parm_cfg.csum_lf = style_cfg2_reg.s.csum_lf;
+	style_cfg->parm_cfg.csum_le = style_cfg2_reg.s.csum_le;
+	style_cfg->parm_cfg.csum_ld = style_cfg2_reg.s.csum_ld;
+	style_cfg->parm_cfg.csum_lc = style_cfg2_reg.s.csum_lc;
+	style_cfg->parm_cfg.csum_lb = style_cfg2_reg.s.csum_lb;
+
+	style_cfg->parm_cfg.qpg_qos = style_alg_reg.s.qpg_qos;
+	style_cfg->parm_cfg.tag_type = style_alg_reg.s.tt;
+	style_cfg->parm_cfg.apad_nip = style_alg_reg.s.apad_nip;
+	style_cfg->parm_cfg.qpg_port_sh = style_alg_reg.s.qpg_port_sh;
+	style_cfg->parm_cfg.qpg_port_msb = style_alg_reg.s.qpg_port_msb;
+	style_cfg->parm_cfg.wqe_vs = style_alg_reg.s.wqe_vs;
+
+	style_cfg->parm_cfg.pkt_lend = style_buf_reg.s.pkt_lend;
+	style_cfg->parm_cfg.wqe_hsz = style_buf_reg.s.wqe_hsz;
+	style_cfg->parm_cfg.wqe_skip = style_buf_reg.s.wqe_skip * 128;
+	style_cfg->parm_cfg.first_skip = style_buf_reg.s.first_skip * 8;
+	style_cfg->parm_cfg.later_skip = style_buf_reg.s.later_skip * 8;
+	style_cfg->parm_cfg.cache_mode = style_buf_reg.s.opc_mode;
+	style_cfg->parm_cfg.mbuff_size = style_buf_reg.s.mb_size * 8;
+	style_cfg->parm_cfg.dis_wq_dat = style_buf_reg.s.dis_wq_dat;
+
+	cvmx_pki_read_tag_config(node, style, cluster_mask, &style_cfg->tag_cfg);
+}
+
 
 /**
  * This function writes/configures parameters associated with style in hardware.
@@ -347,7 +550,7 @@ void cvmx_pki_write_tag_config(int node, int style, uint64_t cluster_mask,
  * @param cluster_mask	      Mask of clusters to configure the style for.
  * @param style_cfg	      pointer to style config struct.
  */
-void cvmx_pki_set_style_config(int node, uint64_t style, uint64_t cluster_mask,
+void cvmx_pki_write_style_config(int node, uint64_t style, uint64_t cluster_mask,
 			    struct cvmx_pki_style_config *style_cfg)
 {
 	cvmx_pki_clx_stylex_cfg_t style_cfg_reg;
@@ -414,230 +617,66 @@ void cvmx_pki_set_style_config(int node, uint64_t style, uint64_t cluster_mask,
 	style_buf_reg.s.dis_wq_dat = style_cfg->parm_cfg.dis_wq_dat;
 	cvmx_write_csr_node(node, CVMX_PKI_STYLEX_BUF(style), style_buf_reg.u64);
 
-        cvmx_pki_write_tag_config(node, style, cluster_mask, &style_cfg->tag_cfg);
-}
-
-void cvmx_pki_get_tag_config(int node, int style, uint64_t cluster_mask,
-			       struct cvmx_pki_style_tag_cfg *tag_cfg)
-{
-	cvmx_pki_clx_stylex_cfg2_t style_cfg2_reg;
-	cvmx_pki_clx_stylex_alg_t style_alg_reg;
-	int cluster = __builtin_ffsll(cluster_mask) - 1;
-
-	style_cfg2_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG2(style, cluster));
-	style_alg_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_ALG(style, cluster));
-
-	tag_cfg->tag_fields.layer_g_src = style_cfg2_reg.s.tag_src_lg;
-	tag_cfg->tag_fields.layer_f_src = style_cfg2_reg.s.tag_src_lf;
-	tag_cfg->tag_fields.layer_e_src = style_cfg2_reg.s.tag_src_le;
-	tag_cfg->tag_fields.layer_d_src = style_cfg2_reg.s.tag_src_ld;
-	tag_cfg->tag_fields.layer_c_src = style_cfg2_reg.s.tag_src_lc;
-	tag_cfg->tag_fields.layer_b_src = style_cfg2_reg.s.tag_src_lb;
-	tag_cfg->tag_fields.layer_g_dst = style_cfg2_reg.s.tag_dst_lg;
-	tag_cfg->tag_fields.layer_f_dst = style_cfg2_reg.s.tag_dst_lf;
-	tag_cfg->tag_fields.layer_e_dst = style_cfg2_reg.s.tag_dst_le;
-	tag_cfg->tag_fields.layer_d_dst = style_cfg2_reg.s.tag_dst_ld;
-	tag_cfg->tag_fields.layer_c_dst = style_cfg2_reg.s.tag_dst_lc;
-	tag_cfg->tag_fields.layer_b_dst = style_cfg2_reg.s.tag_dst_lb;
-	tag_cfg->tag_fields.tag_vni = style_alg_reg.s.tag_vni;
-	tag_cfg->tag_fields.tag_gtp = style_alg_reg.s.tag_gtp;
-	tag_cfg->tag_fields.tag_spi = style_alg_reg.s.tag_spi;
-	tag_cfg->tag_fields.tag_sync = style_alg_reg.s.tag_syn;
-	tag_cfg->tag_fields.ip_prot_nexthdr = style_alg_reg.s.tag_pctl;
-	tag_cfg->tag_fields.second_vlan = style_alg_reg.s.tag_vs1;
-	tag_cfg->tag_fields.first_vlan = style_alg_reg.s.tag_vs0;
-	tag_cfg->tag_fields.mpls_label = style_alg_reg.s.tag_mpls0;
-	tag_cfg->tag_fields.input_port = style_alg_reg.s.tag_prt;
-
-	/** vinita_to_do get mask tag*/
-}
-
-
-void cvmx_pki_get_style_config(int node, int style, uint64_t cluster_mask,
-			       struct cvmx_pki_style_config *style_cfg)
-{
-	cvmx_pki_clx_stylex_cfg_t style_cfg_reg;
-	cvmx_pki_clx_stylex_cfg2_t style_cfg2_reg;
-	cvmx_pki_clx_stylex_alg_t style_alg_reg;
-	cvmx_pki_stylex_buf_t     style_buf_reg;
-	int cluster = __builtin_ffsll(cluster_mask) - 1;
-
-	style_cfg_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(style, cluster));
-	style_cfg2_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG2(style, cluster));
-	style_alg_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_ALG(style, cluster));
-        style_buf_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_STYLEX_BUF(style));
-
-	style_cfg->parm_cfg.ip6_udp_opt = style_cfg_reg.s.ip6_udp_opt;
-	style_cfg->parm_cfg.lenerr_en = style_cfg_reg.s.lenerr_en;
-	style_cfg->parm_cfg.lenerr_eqpad = style_cfg_reg.s.lenerr_eqpad;
-	style_cfg->parm_cfg.maxerr_en = style_cfg_reg.s.maxerr_en;
-	style_cfg->parm_cfg.minerr_en = style_cfg_reg.s.minerr_en;
-	style_cfg->parm_cfg.fcs_chk = style_cfg_reg.s.fcs_chk;
-	style_cfg->parm_cfg.fcs_strip = style_cfg_reg.s.fcs_strip;
-	style_cfg->parm_cfg.minmax_sel = style_cfg_reg.s.minmax_sel;
-	style_cfg->parm_cfg.qpg_base = style_cfg_reg.s.qpg_base;
-	style_cfg->parm_cfg.qpg_dis_padd = style_cfg_reg.s.qpg_dis_padd;
-	style_cfg->parm_cfg.qpg_dis_aura = style_cfg_reg.s.qpg_dis_aura;
-	style_cfg->parm_cfg.qpg_dis_grp = style_cfg_reg.s.qpg_dis_grp;
-	style_cfg->parm_cfg.qpg_dis_grptag = style_cfg_reg.s.qpg_dis_grptag;
-	style_cfg->parm_cfg.rawdrp = style_cfg_reg.s.rawdrp;
-	style_cfg->parm_cfg.force_drop = style_cfg_reg.s.drop;
-	style_cfg->parm_cfg.nodrop = style_cfg_reg.s.nodrop;
-
-	style_cfg->parm_cfg.len_lg = style_cfg2_reg.s.len_lg;
-	style_cfg->parm_cfg.len_lf = style_cfg2_reg.s.len_lf;
-	style_cfg->parm_cfg.len_le = style_cfg2_reg.s.len_le;
-	style_cfg->parm_cfg.len_ld = style_cfg2_reg.s.len_ld;
-	style_cfg->parm_cfg.len_lc = style_cfg2_reg.s.len_lc;
-	style_cfg->parm_cfg.len_lb = style_cfg2_reg.s.len_lb;
-	style_cfg->parm_cfg.csum_lg = style_cfg2_reg.s.csum_lg;
-	style_cfg->parm_cfg.csum_lf = style_cfg2_reg.s.csum_lf;
-	style_cfg->parm_cfg.csum_le = style_cfg2_reg.s.csum_le;
-	style_cfg->parm_cfg.csum_ld = style_cfg2_reg.s.csum_ld;
-	style_cfg->parm_cfg.csum_lc = style_cfg2_reg.s.csum_lc;
-	style_cfg->parm_cfg.csum_lb = style_cfg2_reg.s.csum_lb;
-
-	style_cfg->parm_cfg.qpg_qos = style_alg_reg.s.qpg_qos;
-	style_cfg->parm_cfg.tag_type = style_alg_reg.s.tt;
-	style_cfg->parm_cfg.apad_nip = style_alg_reg.s.apad_nip;
-	style_cfg->parm_cfg.qpg_port_sh = style_alg_reg.s.qpg_port_sh;
-	style_cfg->parm_cfg.qpg_port_msb = style_alg_reg.s.qpg_port_msb;
-	style_cfg->parm_cfg.wqe_vs = style_alg_reg.s.wqe_vs;
-
-	style_cfg->parm_cfg.pkt_lend = style_buf_reg.s.pkt_lend;
-	style_cfg->parm_cfg.wqe_hsz = style_buf_reg.s.wqe_hsz;
-	style_cfg->parm_cfg.wqe_skip = style_buf_reg.s.wqe_skip * 128;
-	style_cfg->parm_cfg.first_skip = style_buf_reg.s.first_skip * 8;
-	style_cfg->parm_cfg.later_skip = style_buf_reg.s.later_skip * 8;
-	style_cfg->parm_cfg.cache_mode = style_buf_reg.s.opc_mode;
-	style_cfg->parm_cfg.mbuff_size = style_buf_reg.s.mb_size * 8;
-	style_cfg->parm_cfg.dis_wq_dat = style_buf_reg.s.dis_wq_dat;
-
-	cvmx_pki_get_tag_config(node, style, cluster_mask, &style_cfg->tag_cfg);
+	cvmx_pki_write_tag_config(node, style, cluster_mask, &style_cfg->tag_cfg);
 }
 
-int cvmx_pki_get_pkind_config(int node, int pkind, struct cvmx_pki_pkind_config *pkind_cfg)
+/**
+ * This function reads qpg entry at specified offset from qpg table
+ *
+ * @param node		node number
+ * @param offset	offset in qpg table to read from.
+ * @param qpg_cfg	pointer to structure containing qpg values
+ */
+int cvmx_pki_read_qpg_entry(int node, int offset, struct cvmx_pki_qpg_config *qpg_cfg)
 {
-	int cluster = 0;
-	uint64_t cl_mask;
-	cvmx_pki_pkindx_icgsel_t pkind_clsel;
-	cvmx_pki_clx_pkindx_style_t pkind_cfg_style;
-	cvmx_pki_icgx_cfg_t pki_cl_grp;
-	cvmx_pki_clx_pkindx_cfg_t pknd_cfg_reg;
-
-	pkind_clsel.u64 = cvmx_read_csr_node(node, CVMX_PKI_PKINDX_ICGSEL(pkind));
-	pki_cl_grp.u64 = cvmx_read_csr_node(node, CVMX_PKI_ICGX_CFG(pkind_clsel.s.icg));
-	pkind_cfg->cluster_grp = (uint8_t)pkind_clsel.s.icg;
-	cl_mask = (uint64_t)pki_cl_grp.s.clusters;
-	cluster = __builtin_ffsll(cl_mask) - 1;
-
-	pkind_cfg_style.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_STYLE(pkind, cluster));
-	pkind_cfg->initial_parse_mode = pkind_cfg_style.s.pm;
-	pkind_cfg->initial_style = pkind_cfg_style.s.style;
-
-	pknd_cfg_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_CFG(pkind, cluster));
-	pkind_cfg->fcs_pres = pknd_cfg_reg.s.fcs_pres;
-	pkind_cfg->parse_en.inst_hdr = pknd_cfg_reg.s.inst_hdr;
-	pkind_cfg->parse_en.mpls_en = pknd_cfg_reg.s.mpls_en;
-	pkind_cfg->parse_en.lg_custom = pknd_cfg_reg.s.lg_custom;
-	pkind_cfg->parse_en.fulc_en = pknd_cfg_reg.s.fulc_en;
-	pkind_cfg->parse_en.dsa_en = pknd_cfg_reg.s.dsa_en;
-	pkind_cfg->parse_en.hg2_en = pknd_cfg_reg.s.hg2_en;
-	pkind_cfg->parse_en.hg_en = pknd_cfg_reg.s.hg_en;
+	cvmx_pki_qpg_tblx_t qpg_tbl;
+	if (offset >= CVMX_PKI_NUM_QPG_ENTRY) {
+		cvmx_dprintf("ERROR: qpg offset %d is >= 2048\n", offset);
+		return -1;
+	}
+	qpg_tbl.u64 = cvmx_read_csr_node(node, CVMX_PKI_QPG_TBLX(offset));
+	qpg_cfg->aura_num = qpg_tbl.s.laura;
+	qpg_cfg->port_add = qpg_tbl.s.padd;
+	qpg_cfg->grp_ok = qpg_tbl.s.grp_ok;
+	qpg_cfg->grp_bad = qpg_tbl.s.grp_bad;
+	qpg_cfg->grptag_ok = qpg_tbl.s.grptag_ok;
+	qpg_cfg->grptag_bad = qpg_tbl.s.grptag_bad;
 	return 0;
 }
 
-int cvmx_pki_get_pkind_style(int node, int pkind)
-{
-	int cluster = 0;
-
-	cvmx_pki_clx_pkindx_style_t pkind_style;
-
-	pkind_style.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_STYLE(pkind, cluster));
-
-	return pkind_style.s.style;
-}
-
-void cvmx_pki_config_port(int ipd_port, struct cvmx_pki_port_config *port_cfg)
-{
-	int interface, index, pknd;
-	int style, cl_mask;
-	cvmx_pki_icgx_cfg_t pki_cl_msk;
-	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(ipd_port);
-
-	/* get the pkind used by this ipd port */
-	interface = cvmx_helper_get_interface_num(ipd_port);
-	index = cvmx_helper_get_interface_index_num(ipd_port);
-	pknd = cvmx_helper_get_pknd(interface, index);
-
-        if (cvmx_pki_set_pkind_config(xp.node, pknd, &port_cfg->pkind_cfg))
-                return;
-	style = port_cfg->pkind_cfg.initial_style;
-	pki_cl_msk.u64 = cvmx_read_csr_node(xp.node, CVMX_PKI_ICGX_CFG(port_cfg->pkind_cfg.cluster_grp));
-	cl_mask = pki_cl_msk.s.clusters;
-	cvmx_pki_set_style_config(xp.node, style, cl_mask, &port_cfg->style_cfg);
-}
-EXPORT_SYMBOL(cvmx_pki_config_port);
-
-void cvmx_pki_get_port_config(int ipd_port, struct cvmx_pki_port_config *port_cfg)
-{
-	int interface, index, pknd;
-	int style, cl_mask;
-	cvmx_pki_icgx_cfg_t pki_cl_msk;
-	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(ipd_port);
-
-	/* get the pkind used by this ipd port */
-	interface = cvmx_helper_get_interface_num(ipd_port);
-	index = cvmx_helper_get_interface_index_num(ipd_port);
-	pknd = cvmx_helper_get_pknd(interface, index);
-
-	cvmx_pki_get_pkind_config(xp.node, pknd, &port_cfg->pkind_cfg);
-	style = port_cfg->pkind_cfg.initial_style;
-	pki_cl_msk.u64 = cvmx_read_csr_node(xp.node, CVMX_PKI_ICGX_CFG(port_cfg->pkind_cfg.cluster_grp));
-	cl_mask = pki_cl_msk.s.clusters;
-	cvmx_pki_get_style_config(xp.node, style, cl_mask, &port_cfg->style_cfg);
-}
-EXPORT_SYMBOL(cvmx_pki_get_port_config);
-
 /**
- * This function sets the wqe buffer mode. First packet data buffer can reside
- * either in same buffer as wqe OR it can go in separate buffer. If used the later mode,
- * make sure software allocate enough buffers to now have wqe separate from packet data.
- * @param node	              node number.
- * @param style		      style to configure.
- * @param pkt_outside_wqe.	0 = The packet link pointer will be at word [FIRST_SKIP]
- *				    immediately followed by packet data, in the same buffer
- *				    as the work queue entry.
- *				1 = The packet link pointer will be at word [FIRST_SKIP] in a new
- *				    buffer separate from the work queue entry. Words following the
- *				    WQE in the same cache line will be zeroed, other lines in the
- *				    buffer will not be modified and will retain stale data (from the
- * 				    buffers previous use). This setting may decrease the peak PKI
- *				    performance by up to half on small packets.
+ * This function writes qpg entry at specified offset in qpg table
+ *
+ * @param node		node number
+ * @param offset	offset in qpg table to write to.
+ * @param qpg_cfg	pointer to stricture containing qpg values
  */
-void cvmx_pki_set_wqe_mode(int node, uint64_t style, bool pkt_outside_wqe)
+void cvmx_pki_write_qpg_entry(int node, int offset, struct cvmx_pki_qpg_config *qpg_cfg)
 {
-	cvmx_pki_stylex_buf_t     style_buf_reg;
-
-	style_buf_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_STYLEX_BUF(style));
-	style_buf_reg.s.dis_wq_dat = pkt_outside_wqe;
-	cvmx_write_csr_node(node, CVMX_PKI_STYLEX_BUF(style), style_buf_reg.u64);
+	cvmx_pki_qpg_tblx_t qpg_tbl;
+	qpg_tbl.u64 = cvmx_read_csr_node(node, CVMX_PKI_QPG_TBLX(offset));
+	qpg_tbl.s.padd = qpg_cfg->port_add;
+	qpg_tbl.s.laura = qpg_cfg->aura_num;
+	qpg_tbl.s.grp_ok = qpg_cfg->grp_ok;
+	qpg_tbl.s.grp_bad = qpg_cfg->grp_bad;
+	qpg_tbl.s.grptag_ok = qpg_cfg->grptag_ok;
+	qpg_tbl.s.grptag_bad = qpg_cfg->grptag_bad;
+	cvmx_write_csr_node(node, CVMX_PKI_QPG_TBLX(offset), qpg_tbl.u64);
 }
 
 
 /**
  * This function writes pcam entry at given offset in pcam table in hardware
  *
- * @param node	              node number.
- * @param index		      offset in pcam table.
- * @param cluster_mask	      Mask of clusters in which to write pcam entry.
- * @param input 	      input keys to pcam match passed as struct.
- * @param action	      pcam match action passed as struct
+ * @param node		node number.
+ * @param index		offset in pcam table.
+ * @param cluster_mask	Mask of clusters in which to write pcam entry.
+ * @param input 	input keys to pcam match passed as struct.
+ * @param action	pcam match action passed as struct
  *
  */
 int cvmx_pki_pcam_write_entry(int node, int index, uint64_t cluster_mask,
-				struct cvmx_pki_pcam_input input,
-				struct cvmx_pki_pcam_action action)
+	struct cvmx_pki_pcam_input input, struct cvmx_pki_pcam_action action)
 {
 	int bank;
 	int cluster = 0;
@@ -652,6 +691,9 @@ int cvmx_pki_pcam_write_entry(int node, int index, uint64_t cluster_mask,
 	bank = (int)(input.field & 0x01);
 	while (cluster < CVMX_PKI_NUM_CLUSTER) {
 		if (cluster_mask & (0x01L << cluster)) {
+			pcam_term.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PCAMX_TERMX(cluster, bank, index));
+			pcam_term.s.valid = 0;
+			cvmx_write_csr_node(node, CVMX_PKI_CLX_PCAMX_TERMX(cluster, bank, index), pcam_term.u64);
 			pcam_match.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PCAMX_MATCHX(cluster, bank, index));
 			pcam_match.s.data1 = input.data & input.data_mask;
 			pcam_match.s.data0 = (~input.data) & input.data_mask;
@@ -674,24 +716,23 @@ int cvmx_pki_pcam_write_entry(int node, int index, uint64_t cluster_mask,
 		cluster++;
 	}
 	return 0;
-
 }
 
 /**
  * Enables/Disabled QoS (RED Drop, Tail Drop & backpressure) for the
  * PKI aura.
- * @param node      node number
- * @param aura      to enable/disable QoS on.
- * @param ena_red   Enable/Disable RED drop between pass and drop level
- *                  1-enable 0-disable
- * @param ena_drop  Enable/disable tail drop when max drop level exceeds
- *                  1-enable 0-disable
- * @param ena_red   Enable/Disable asserting backpressure on bpid when
- *                  max DROP level exceeds.
- *                  1-enable 0-disable
+ * @param node		node number
+ * @param aura		to enable/disable QoS on.
+ * @param ena_red	Enable/Disable RED drop between pass and drop level
+ *			1-enable 0-disable
+ * @param ena_drop	Enable/disable tail drop when max drop level exceeds
+ *			1-enable 0-disable
+ * @param ena_red	Enable/Disable asserting backpressure on bpid when
+ *			max DROP level exceeds.
+ *			1-enable 0-disable
  */
 int cvmx_pki_enable_aura_qos(int node, int aura, bool ena_red,
-			     bool ena_drop, bool ena_bp)
+	bool ena_drop, bool ena_bp)
 {
 	cvmx_pki_aurax_cfg_t pki_aura_cfg;
 
@@ -721,7 +762,8 @@ int cvmx_pki_write_aura_bpid(int node, int aura, int bpid)
 	cvmx_pki_aurax_cfg_t pki_aura_cfg;
 
 	if (aura >= CVMX_PKI_NUM_AURA || bpid >= CVMX_PKI_NUM_BPID) {
-		cvmx_dprintf("ERROR: PKI config aura_bp aura = %d bpid = %d", aura, bpid);
+		cvmx_dprintf("ERROR: PKI config aura_bp aura = %d bpid = %d",
+			aura, bpid);
 		return -1;
 	}
 	pki_aura_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKI_AURAX_CFG(aura));
@@ -735,8 +777,8 @@ int cvmx_pki_write_aura_bpid(int node, int aura, int bpid)
  * from the specified bpid.
  * Each channel listens for backpressure on a specific bpid.
  * Each bpid can backpressure multiple channels.
- * @param node    node number
- * @param bpid    bpid from which, channel will receive backpressure.
+ * @param node	node number
+ * @param bpid	bpid from which, channel will receive backpressure.
  * @param channel channel numner to receive backpressue.
  */
 int cvmx_pki_write_channel_bpid(int node, int channel, int bpid)
@@ -744,7 +786,8 @@ int cvmx_pki_write_channel_bpid(int node, int channel, int bpid)
 	cvmx_pki_chanx_cfg_t pki_chan_cfg;
 
 	if (channel >= CVMX_PKI_NUM_CHANNEL || bpid >= CVMX_PKI_NUM_BPID) {
-		cvmx_dprintf("ERROR: PKI config channel_bp channel = %d bpid = %d", channel, bpid);
+		cvmx_dprintf("ERROR: PKI config channel_bp channel = %d bpid = %d",
+			channel, bpid);
 		return -1;
 	}
 
@@ -755,6 +798,64 @@ int cvmx_pki_write_channel_bpid(int node, int channel, int bpid)
 }
 
 /**
+ * This function gives the initial style used by that pkind.
+ * @param node	node number.
+ * @param pkind	pkind number.
+ */
+int cvmx_pki_get_pkind_style(int node, int pkind)
+{
+	int cluster = 0;
+
+	cvmx_pki_clx_pkindx_style_t pkind_style;
+
+	pkind_style.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_STYLE(pkind, cluster));
+
+	return pkind_style.s.style;
+}
+
+/**
+ * This function sets the wqe buffer mode. First packet data buffer can reside
+ * either in same buffer as wqe OR it can go in separate buffer. If used the later mode,
+ * make sure software allocate enough buffers to now have wqe separate from packet data.
+ * @param node		node number.
+ * @param style		style to configure.
+ * @param pkt_outside_wqe.	0 = The packet link pointer will be at word [FIRST_SKIP]
+ *				immediately followed by packet data, in the same buffer
+ *				as the work queue entry.
+ *				1 = The packet link pointer will be at word [FIRST_SKIP] in a new
+ *				buffer separate from the work queue entry. Words following the
+ *				WQE in the same cache line will be zeroed, other lines in the
+ *				buffer will not be modified and will retain stale data (from the
+ * 				buffers previous use). This setting may decrease the peak PKI
+ *				performance by up to half on small packets.
+ */
+void cvmx_pki_set_wqe_mode(int node, uint64_t style, bool pkt_outside_wqe)
+{
+	cvmx_pki_stylex_buf_t style_buf_reg;
+
+	style_buf_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_STYLEX_BUF(style));
+	style_buf_reg.s.dis_wq_dat = pkt_outside_wqe;
+	cvmx_write_csr_node(node, CVMX_PKI_STYLEX_BUF(style), style_buf_reg.u64);
+}
+
+/**
+ * This function sets the Packet mode of all ports and styles to little-endian.
+ * It Changes write operations of packet data to L2C to
+ * be in little-endian. Does not change the WQE header format, which is
+ * properly endian neutral.
+ * @param node		node number.
+ * @param style 	style to configure.
+ */
+void cvmx_pki_set_little_endian(int node, uint64_t style)
+{
+	cvmx_pki_stylex_buf_t style_buf_reg;
+
+	style_buf_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_STYLEX_BUF(style));
+	style_buf_reg.s.pkt_lend = 1;
+	cvmx_write_csr_node(node, CVMX_PKI_STYLEX_BUF(style), style_buf_reg.u64);
+}
+
+/**
  * Enables/Disables fcs check and fcs stripping on the pkind.
  * @param node		node number
  * @param pknd		pkind to apply settings on.
@@ -772,15 +873,18 @@ void cvmx_pki_endis_fcs_check(int node, int pknd, bool fcs_chk, bool fcs_strip)
 	cvmx_pki_clx_pkindx_style_t pkind_style;
 	cvmx_pki_clx_stylex_cfg_t style_cfg;
 
-        while (cluster < CVMX_PKI_NUM_CLUSTER) {
-            pkind_style.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_STYLE(pknd, cluster));
-            style = pkind_style.s.style;
-            style_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(style, cluster));
-            style_cfg.s.fcs_chk = fcs_chk;
-            style_cfg.s.fcs_strip = fcs_strip;
-            cvmx_write_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(style, cluster), style_cfg.u64);
-            cluster++;
-        }
+	while (cluster < CVMX_PKI_NUM_CLUSTER) {
+		pkind_style.u64 = cvmx_read_csr_node(node,
+			CVMX_PKI_CLX_PKINDX_STYLE(pknd, cluster));
+		style = pkind_style.s.style;
+		style_cfg.u64 = cvmx_read_csr_node(node,
+			CVMX_PKI_CLX_STYLEX_CFG(style, cluster));
+		style_cfg.s.fcs_chk = fcs_chk;
+		style_cfg.s.fcs_strip = fcs_strip;
+		cvmx_write_csr_node(node,
+			CVMX_PKI_CLX_STYLEX_CFG(style, cluster), style_cfg.u64);
+		cluster++;
+	}
 }
 
 /**
@@ -802,15 +906,18 @@ void cvmx_pki_endis_l2_errs(int node, int pknd, bool l2len_err,
 	cvmx_pki_clx_stylex_cfg_t style_cfg;
 
 	while (cluster < CVMX_PKI_NUM_CLUSTER) {
-            pkind_style.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_STYLE(pknd, cluster));
-            style = pkind_style.s.style;
-            style_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(style, cluster));
-            style_cfg.s.lenerr_en = l2len_err;
-            style_cfg.s.maxerr_en = maxframe_err;
-            style_cfg.s.minerr_en = minframe_err;
-            cvmx_write_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(style, cluster), style_cfg.u64);
-            cluster++;
-        }
+		pkind_style.u64 = cvmx_read_csr_node(node,
+			CVMX_PKI_CLX_PKINDX_STYLE(pknd, cluster));
+		style = pkind_style.s.style;
+		style_cfg.u64 = cvmx_read_csr_node(node,
+			CVMX_PKI_CLX_STYLEX_CFG(style, cluster));
+		style_cfg.s.lenerr_en = l2len_err;
+		style_cfg.s.maxerr_en = maxframe_err;
+		style_cfg.s.minerr_en = minframe_err;
+		cvmx_write_csr_node(node,
+			CVMX_PKI_CLX_STYLEX_CFG(style, cluster), style_cfg.u64);
+		cluster++;
+	}
 }
 
 /**
@@ -826,53 +933,24 @@ void cvmx_pki_dis_frame_len_chk(int node, int pknd)
 	cvmx_pki_clx_stylex_cfg_t style_cfg;
 
 	while (cluster < CVMX_PKI_NUM_CLUSTER) {
-            pkind_style.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_STYLE(pknd, cluster));
-            style = pkind_style.s.style;
-            style_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(style, cluster));
-            style_cfg.s.maxerr_en = 0;
-            style_cfg.s.minerr_en = 0;
-            cvmx_write_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(style, cluster), style_cfg.u64);
-            cluster++;
-        }
+		pkind_style.u64 = cvmx_read_csr_node(node,
+			CVMX_PKI_CLX_PKINDX_STYLE(pknd, cluster));
+		style = pkind_style.s.style;
+		style_cfg.u64 = cvmx_read_csr_node(node,
+			CVMX_PKI_CLX_STYLEX_CFG(style, cluster));
+		style_cfg.s.maxerr_en = 0;
+		style_cfg.s.minerr_en = 0;
+		cvmx_write_csr_node(node,
+			CVMX_PKI_CLX_STYLEX_CFG(style, cluster), style_cfg.u64);
+		cluster++;
+	}
 }
 
-/**
- * Modifies maximum frame length to check.
- * It modifies the global frame length set used by this port, any other
- * port using the same set will get affected too.
- * @param node		node number
- * @param ipd_port	ipd port for which to modify max len.
- * @param max_size	maximum frame length
- */
-void cvmx_pki_set_max_frm_len(int node, int ipd_port, uint32_t max_size)
-{
-	/* On CN78XX frame check is enabled for a style n and
-	PKI_CLX_STYLE_CFG[minmax_sel] selects which set of
-	MAXLEN/MINLEN to use. */
-	int interface, index, pknd;
-	cvmx_pki_clx_stylex_cfg_t style_cfg;
-	cvmx_pki_frm_len_chkx_t frame_len;
-	int cluster = 0;
-	int style;
-	int sel;
-
-	/* get the pkind used by this ipd port */
-	interface = cvmx_helper_get_interface_num(ipd_port);
-	index = cvmx_helper_get_interface_index_num(ipd_port);
-	pknd = cvmx_helper_get_pknd(interface, index);
-
-	style = cvmx_pki_get_pkind_style(node, pknd);
-	style_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(style, cluster));
-	sel = style_cfg.s.minmax_sel;
-	frame_len.u64 = cvmx_read_csr(CVMX_PKI_FRM_LEN_CHKX(sel));
-	frame_len.s.maxlen = max_size;
-	cvmx_write_csr_node(node, CVMX_PKI_FRM_LEN_CHKX(sel), frame_len.u64);
-}
 
 /**
  * This function shows the qpg table entries,
  * read directly from hardware.
- * @param node    node number
+ * @param node	node number
  */
 void cvmx_pki_show_qpg_entries(int node, uint16_t num_entry)
 {
@@ -885,13 +963,13 @@ void cvmx_pki_show_qpg_entries(int node, uint16_t num_entry)
 		qpg_tbl.u64 = cvmx_read_csr_node(node, CVMX_PKI_QPG_TBLX(index));
 		cvmx_dprintf("\n%d	", index);
 		cvmx_dprintf("PADD %-16lu",
-			     (unsigned long)qpg_tbl.s.padd);
+			(unsigned long)qpg_tbl.s.padd);
 		cvmx_dprintf("GRP_OK %-16lu",
-			     (unsigned long)qpg_tbl.s.grp_ok);
+			(unsigned long)qpg_tbl.s.grp_ok);
 		cvmx_dprintf("GRP_BAD %-16lu",
-			     (unsigned long)qpg_tbl.s.grp_bad);
+			(unsigned long)qpg_tbl.s.grp_bad);
 		cvmx_dprintf("LAURA %-16lu",
-			     (unsigned long)qpg_tbl.s.laura);
+			(unsigned long)qpg_tbl.s.laura);
 	}
 }
 
@@ -913,11 +991,12 @@ void cvmx_pki_show_pcam_entries(int node)
 			for (index = 0; index < CVMX_PKI_NUM_PCAM_ENTRY; index++) {
 				cvmx_dprintf("\n%d", index);
 				cvmx_dprintf("             %-16lx",
-				     (unsigned long)cvmx_read_csr_node(node, CVMX_PKI_CLX_PCAMX_TERMX(cluster, bank, index)));
+				    (unsigned long)cvmx_read_csr_node(node,
+					CVMX_PKI_CLX_PCAMX_TERMX(cluster, bank, index)));
 				cvmx_dprintf("     %-16lx",
-					     (unsigned long)cvmx_read_csr_node(node, CVMX_PKI_CLX_PCAMX_MATCHX(cluster, bank, index)));
+				     (unsigned long)cvmx_read_csr_node(node, CVMX_PKI_CLX_PCAMX_MATCHX(cluster, bank, index)));
 				cvmx_dprintf("     %-16lx",
-					     (unsigned long)cvmx_read_csr_node(node, CVMX_PKI_CLX_PCAMX_ACTIONX(cluster, bank, index)));
+				     (unsigned long)cvmx_read_csr_node(node, CVMX_PKI_CLX_PCAMX_ACTIONX(cluster, bank, index)));
 			}
 		}
 	}
@@ -983,7 +1062,7 @@ void cvmx_pki_show_pkind_attributes(int node, int pkind)
 		cvmx_dprintf("ERROR: PKIND %d is beyond range\n", pkind);
 		return;
 	}
-        cvmx_dprintf("Showing stats for pkind %d------------------\n", pkind);
+	cvmx_dprintf("Showing stats for pkind %d------------------\n", pkind);
 	pkind_clsel.u64 = cvmx_read_csr_node(node, CVMX_PKI_PKINDX_ICGSEL(pkind));
 	cvmx_dprintf("cluster group:	%d\n", pkind_clsel.s.icg);
 	pki_cl_grp.u64 = cvmx_read_csr_node(node, CVMX_PKI_ICGX_CFG(pkind_clsel.s.icg));
@@ -991,9 +1070,12 @@ void cvmx_pki_show_pkind_attributes(int node, int pkind)
 
 	while (cluster < CVMX_PKI_NUM_CLUSTER) {
 		if (pki_cl_grp.s.clusters & (0x01L << cluster)) {
-                        cvmx_dprintf("pkind %d config 0x%llx\n", pkind, (unsigned long long)cvmx_read_csr_node(node,
-                                     CVMX_PKI_CLX_PKINDX_CFG(pkind, cluster)));
-			pkind_cfg_style.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_STYLE(pkind, cluster));
+			cvmx_dprintf("pkind %d config 0x%llx\n",
+				pkind,
+				(unsigned long long)cvmx_read_csr_node(node,
+				CVMX_PKI_CLX_PKINDX_CFG(pkind, cluster)));
+			pkind_cfg_style.u64 = cvmx_read_csr_node(node,
+				CVMX_PKI_CLX_PKINDX_STYLE(pkind, cluster));
 			cvmx_dprintf("initial parse Mode: %d\n", pkind_cfg_style.s.pm);
 			cvmx_dprintf("initial_style: %d\n", pkind_cfg_style.s.style);
 			style_alg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_ALG(pkind_cfg_style.s.style, cluster));
@@ -1001,9 +1083,9 @@ void cvmx_pki_show_pkind_attributes(int node, int pkind)
 			style_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(pkind_cfg_style.s.style, cluster));
 			cvmx_dprintf("style_cfg: 0x%llx\n", (unsigned long long)style_cfg.u64);
 			cvmx_dprintf("style_cfg2: 0x%llx\n",
-				     (unsigned long long)cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG2(pkind_cfg_style.s.style, cluster)));
+				(unsigned long long)cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG2(pkind_cfg_style.s.style, cluster)));
 			cvmx_dprintf("style_buf: 0x%llx\n",
-				     (unsigned long long)cvmx_read_csr_node(node, CVMX_PKI_STYLEX_BUF(pkind_cfg_style.s.style)));
+				(unsigned long long)cvmx_read_csr_node(node, CVMX_PKI_STYLEX_BUF(pkind_cfg_style.s.style)));
 			break;
 		}
 	}
@@ -1011,15 +1093,9 @@ void cvmx_pki_show_pkind_attributes(int node, int pkind)
 	cvmx_dprintf("qpg qos: %d\n", style_alg.s.qpg_qos);
 	for (index = 0; index < 8; index++) {
 		cvmx_dprintf("qpg index %d: 0x%llx\n", (index+style_cfg.s.qpg_base),
-			     (unsigned long long)cvmx_read_csr_node(node, CVMX_PKI_QPG_TBLX(style_cfg.s.qpg_base+index)));
+			(unsigned long long)cvmx_read_csr_node(node, CVMX_PKI_QPG_TBLX(style_cfg.s.qpg_base+index)));
 	}
 }
 
-void cvmx_pki_show_port(int node, int interface, int index)
-{
-        int pknd = cvmx_helper_get_pknd(interface, index);
-        cvmx_dprintf("Showing stats for intf 0x%x port %d------------------\n", interface, index);
-        cvmx_pki_show_pkind_attributes(node, pknd);
-        cvmx_dprintf("END STAUS------------------------\n\n");
-}
+
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pko.c b/arch/mips/cavium-octeon/executive/cvmx-pko.c
index 8860608..2e5ac5a 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pko.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pko.c
@@ -53,7 +53,6 @@
 #include <asm/octeon/cvmx-helper-cfg.h>
 #include <asm/octeon/cvmx-helper-util.h>
 #include <asm/octeon/cvmx-fpa1.h>
-#include <asm/octeon/cvmx-fpa3.h>
 #include <asm/octeon/cvmx-clock.h>
 #else
 #include "cvmx.h"
@@ -63,7 +62,6 @@
 #include "cvmx-helper-cfg.h"
 #include "cvmx-helper-util.h"
 #include "cvmx-fpa1.h"
-#include "cvmx-fpa3.h"
 #ifndef __U_BOOT__
 #include "cvmx-error.h"
 #endif
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pko3-queue.c b/arch/mips/cavium-octeon/executive/cvmx-pko3-queue.c
index 4a182fa..52c2167 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pko3-queue.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pko3-queue.c
@@ -89,7 +89,7 @@ struct cvmx_pko3_dq {
  * of a global named block which has 2^12 entries per each
  * possible node.
  */
-struct cvmx_pko3_dq *__cvmx_pko3_dq_table;
+CVMX_TLS struct cvmx_pko3_dq *__cvmx_pko3_dq_table;
 
 int cvmx_pko3_get_queue_base(int ipd_port)
 {
@@ -226,12 +226,12 @@ int __cvmx_pko3_ipd_dq_register(int xiface, int index,
 
 	if(debug)
 		cvmx_dprintf("%s: ipd=%#x ix=%#x dq %u cnt %u\n",
-			__FUNCTION__, ipd_port, i, dq_base, dq_count);
+			__func__, ipd_port, i, dq_base, dq_count);
 
 	/* Check the IPD port has not already been configured */
 	if(dq_table[i].dq_count > 0 ) {
 		cvmx_dprintf("%s: ERROR: IPD %#x already registered\n",
-			__FUNCTION__, ipd_port);
+			__func__, ipd_port);
 		return -1;
 	}
 
@@ -271,14 +271,14 @@ int __cvmx_pko3_ipd_dq_unregister(int xiface, int index)
 
 	if (dq_table[i].dq_count == 0) {
 		cvmx_dprintf("%s:ipd=%#x already released\n",
-			__FUNCTION__, ipd_port);
+			__func__, ipd_port);
 		return -1;
 	}
 
 	if(debug)
 		cvmx_dprintf("%s:ipd=%#x release dq %u cnt %u\n",
-			     __FUNCTION__, ipd_port, 
-			     dq_table[i].dq_base, 
+			     __func__, ipd_port,
+			     dq_table[i].dq_base,
 			     dq_table[i].dq_count);
 
 	dq_table[i].dq_count = 0;
@@ -334,7 +334,7 @@ static uint16_t cvmx_pko3_chan_2_xchan(uint16_t ipd_port)
 }
 
 /*
- * Map channel number in PKO 
+ * Map channel number in PKO
  *
  * @param node is to specify the node to which this configuration is applied.
  * @param pq_num specifies the Port Queue (i.e. L1) queue number.
@@ -366,7 +366,7 @@ void cvmx_pko3_map_channel(unsigned node,
 
 	if(xchan & 0xf000) {
 		cvmx_dprintf("%s: ERROR: channel %#x not recognized\n",
-			__FUNCTION__, channel);
+			__func__, channel);
 		return;
 	}
 
@@ -380,7 +380,7 @@ void cvmx_pko3_map_channel(unsigned node,
 	if (debug)
 		cvmx_dprintf("%s: channel %#x (compressed=%#x) mapped "
 				"L2/L3 SQ=%u, PQ=%u\n",
-			__FUNCTION__, channel, xchan, l2_l3_q_num, pq_num);
+			__func__, channel, xchan, l2_l3_q_num, pq_num);
 }
 
 /*
@@ -757,7 +757,7 @@ int cvmx_pko3_pq_config_children(unsigned node, unsigned mac_num,
 
 	if(debug)
 		cvmx_dprintf("%s: L1/PQ%u MAC%u child_base %u rr_pri %u\n",
-		__FUNCTION__, pq_num, mac_num, child_base, rr_prio);
+		__func__, pq_num, mac_num, child_base, rr_prio);
 
 	cvmx_pko_configure_port_queue(node,
 		pq_num, child_base, rr_prio, mac_num);
@@ -767,7 +767,7 @@ int cvmx_pko3_pq_config_children(unsigned node, unsigned mac_num,
 		if (debug)
 			cvmx_dprintf("%s: "
 				"L2/SQ%u->PQ%u prio %u rr_quantum %#x\n",
-				__FUNCTION__,
+				__func__,
 				child, pq_num, prio, rr_quantum);
 
 		cvmx_pko_configure_l2_queue(node,
@@ -859,7 +859,7 @@ int cvmx_pko3_sq_config_children(unsigned int node, unsigned parent_level,
 
 	if(debug)
 		cvmx_dprintf("%s: Parent L%u/SQ%u child_base %u rr_pri %u\n",
-		__FUNCTION__, parent_level, parent_queue, child_base, rr_prio);
+		__func__, parent_level, parent_queue, child_base, rr_prio);
 
 	/* Parent is configured with child */
 
@@ -867,7 +867,7 @@ int cvmx_pko3_sq_config_children(unsigned int node, unsigned parent_level,
 		if (debug)
 			cvmx_dprintf("%s: "
 				"L%u/SQ%u->L%u/SQ%u prio %u rr_quantum %#x\n",
-				__FUNCTION__,
+				__func__,
 				child_level, child,
 				parent_level, parent_queue,
 				prio, rr_quantum);
@@ -887,7 +887,7 @@ int cvmx_pko3_sq_config_children(unsigned int node, unsigned parent_level,
 
 /**
  * Convert bitrate and burst size to SQx_xIR register values
- * 
+ *
  * @INTERNAL
  *
  * Common function to convert bit-rate (ie kilo-bits-per-sec)
@@ -909,7 +909,7 @@ int cvmx_pko3_sq_config_children(unsigned int node, unsigned parent_level,
  *
  * Note that the bust error could be as a result of this function
  * enforcing the minimum MTU as the minimum burst size allowed.
- * 
+ *
  */
 static int cvmx_pko3_shaper_rate_compute(unsigned long tclk,
 		cvmx_pko_l1_sqx_cir_t *reg,
@@ -967,7 +967,7 @@ static int cvmx_pko3_shaper_rate_compute(unsigned long tclk,
 
 	/* Find the minimum burst size needed for rate */
 	min_burst = (rate_tocks << div_exp) / tclk;
-	
+
 	/* Override with minimum MTU (could become per-port cfg) */
 	if (min_burst < __pko3_min_mtu)
 		min_burst = __pko3_min_mtu;
@@ -1009,11 +1009,11 @@ static int cvmx_pko3_shaper_rate_compute(unsigned long tclk,
 				reg->s.burst_exponent);
 	/* Convert in additional bytes as in argument */
 	burst_v = burst_v << (tock_bytes_exp);
-	
+
 	if (debug)
 		cvmx_dprintf("%s: result rate=%'llu kbips burst=%llu bytes\n",
 			__func__,rate_v, burst_v);
-	
+
 	/* Compute error in parts-per-million */
 	rate_v = abs(rate_v - rate_kbips);
 	burst_v = abs(burst_v - burst_bytes);
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pko3.c b/arch/mips/cavium-octeon/executive/cvmx-pko3.c
index 196e9be..487471e 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pko3.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pko3.c
@@ -50,7 +50,7 @@
 #include "cvmx.h"
 #include "cvmx-hwpko.h"	/* For legacy support */
 #include "cvmx-pko3.h"
-#include "cvmx-fpa.h"
+#include "cvmx-fpa3.h"
 #include "cvmx-helper-pko3.h"
 #include <errno.h>
 #endif
@@ -67,7 +67,7 @@ static const bool __native_le = 1;
 	if(debug)		\
 	cvmx_dprintf("%s=%#llx\n",#reg,(long long)cvmx_read_csr_node(node,reg))
 
-int32_t __cvmx_pko3_dq_depth[1024];
+CVMX_TLS int32_t __cvmx_pko3_dq_depth[1024];
 
 static int cvmx_pko_setup_macs(int node);
 
@@ -149,7 +149,7 @@ int cvmx_pko3_hw_init_global(int node, uint16_t aura)
 	/* set the aura number in pko, use aura node from parameter */
 	pko_aura.u64 = 0;
 	pko_aura.s.node = aura >> 10;
-	pko_aura.s.laura = aura & (CVMX_FPA3_AURA_NUM-1);
+	pko_aura.s.laura = aura;
 	cvmx_write_csr_node(node, CVMX_PKO_DPFI_FPA_AURA, pko_aura.u64);
 
 	CVMX_DUMP_REGX(CVMX_PKO_DPFI_FPA_AURA);
@@ -1266,13 +1266,15 @@ static int cvmx_pko3_pdesc_subdc_add(cvmx_pko3_pdesc_t *pdesc,
 
 	/* Allocate jump buffer */
 	if (cvmx_unlikely(pdesc->jump_buf == NULL)) {
-		unsigned pko_gaura;
+		uint16_t pko_gaura;
+		cvmx_fpa3_gaura_t aura;
 		unsigned fpa_node = cvmx_get_node_num();
 
 		/* Allocate jump buffer from PKO internal FPA AURA, size=4KiB */
 		pko_gaura = __cvmx_pko3_aura_get(fpa_node);
+		aura = __cvmx_fpa3_gaura(pko_gaura >> 10, pko_gaura & 0x3ff);
 
-		pdesc->jump_buf = cvmx_fpa3_alloc_gaura(pko_gaura);
+		pdesc->jump_buf = cvmx_fpa3_alloc(aura);
                 if(pdesc->jump_buf == NULL)
                         return -EINVAL;
 
@@ -1424,7 +1426,9 @@ int cvmx_pko3_pdesc_buf_append(cvmx_pko3_pdesc_t *pdesc, void *p_data,
 
 		/* First mbuf, calculate headroom */
 #ifndef CVMX_BUILD_FOR_LINUX_KERNEL
-                buf_sz = cvmx_fpa3_get_aura_buf_size(gaura);
+		cvmx_fpa3_gaura_t aura;
+		aura = __cvmx_fpa3_gaura(gaura >> 10, gaura & 0x3ff);
+                buf_sz = cvmx_fpa3_get_aura_buf_size(aura);
 #endif
 		pdesc->headroom = (unsigned long)p_data & (buf_sz-1);
 		pdesc->last_aura = hdr_s->s.aura = gaura;
diff --git a/arch/mips/cavium-octeon/executive/cvmx-qlm.c b/arch/mips/cavium-octeon/executive/cvmx-qlm.c
index e8d9851..5074562 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-qlm.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-qlm.c
@@ -42,7 +42,7 @@
  *
  * Helper utilities for qlm.
  *
- * <hr>$Revision: 100230 $<hr>
+ * <hr>$Revision: 102508 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
@@ -119,9 +119,9 @@ extern const __cvmx_qlm_jtag_field_t __cvmx_qlm_jtag_field_cn68xx[];
 #ifdef CVMX_BUILD_FOR_LINUX_HOST
 extern void octeon_remote_read_mem(void *buffer, uint64_t physical_address, int length);
 extern void octeon_remote_write_mem(uint64_t physical_address, const void *buffer, int length);
-uint32_t __cvmx_qlm_jtag_xor_ref[5][CVMX_QLM_JTAG_UINT32];
+uint32_t __cvmx_qlm_jtag_xor_ref[5][CVMX_QLM_JTAG_UINT32*8];
 #else
-typedef uint32_t qlm_jtag_uint32_t[CVMX_QLM_JTAG_UINT32];
+typedef uint32_t qlm_jtag_uint32_t[CVMX_QLM_JTAG_UINT32*8];
 CVMX_SHARED qlm_jtag_uint32_t *__cvmx_qlm_jtag_xor_ref;
 #endif
 
@@ -196,7 +196,7 @@ int cvmx_qlm_interface(int xiface)
 				return qlm;
 			else
 				return -1;
-		} else if (xi.interface >= 7) { /* ILK */
+		} else if (xi.interface <= 7) { /* ILK */
 			int qlm;
 			for (qlm = 4; qlm < 8; qlm++) {
 				/* Make sure the QLM is powered and out of reset */
@@ -293,7 +293,7 @@ void cvmx_qlm_init(void)
 	int qlm;
 	int qlm_jtag_length;
 	char *qlm_jtag_name = "cvmx_qlm_jtag";
-	int qlm_jtag_size = CVMX_QLM_JTAG_UINT32 * 8 * 4;
+	int qlm_jtag_size = CVMX_QLM_JTAG_UINT32 * 8 * sizeof(uint32_t);
 	static uint64_t qlm_base = 0;
 	const cvmx_bootmem_named_block_desc_t *desc;
 
@@ -308,7 +308,7 @@ void cvmx_qlm_init(void)
 
 	qlm_jtag_length = cvmx_qlm_jtag_get_length();
 
-	if (4 * qlm_jtag_length > (int)sizeof(__cvmx_qlm_jtag_xor_ref[0]) * 8) {
+	if (sizeof(uint32_t) * qlm_jtag_length > (int)sizeof(__cvmx_qlm_jtag_xor_ref[0]) * 8) {
 		cvmx_dprintf("ERROR: cvmx_qlm_init: JTAG chain larger than XOR ref size\n");
 		return;
 	}
@@ -330,7 +330,10 @@ void cvmx_qlm_init(void)
 	}
 
 	/* Create named block to store the initial JTAG state. */
-	qlm_base = cvmx_bootmem_phy_named_block_alloc(qlm_jtag_size, 0, 0, 128, qlm_jtag_name, CVMX_BOOTMEM_FLAG_END_ALLOC);
+	qlm_base = cvmx_bootmem_phy_named_block_alloc(qlm_jtag_size,
+		0, 1ull<<29, 128,	/* KSEG0 addresable */
+		qlm_jtag_name,
+		CVMX_BOOTMEM_FLAG_END_ALLOC);
 
 	if (qlm_base == -1ull) {
 		cvmx_dprintf("ERROR: cvmx_qlm_init: Error in creating %s named block\n", qlm_jtag_name);
@@ -631,6 +634,109 @@ void __cvmx_qlm_pcie_cfg_rxd_set_tweak(int qlm, int lane)
 }
 
 /**
+ * Get the speed (Gbaud) of the QLM in Mhz for a given node.
+ *
+ * @param node   node of the QLM
+ * @param qlm    QLM to examine
+ *
+ * @return Speed in Mhz
+ */
+int cvmx_qlm_get_gbaud_mhz_node(int node, int qlm)
+{
+	cvmx_gserx_lane_mode_t lane_mode;
+	cvmx_gserx_cfg_t cfg;
+
+	if (!octeon_has_feature(OCTEON_FEATURE_MULTINODE))
+		return 0;
+
+	if (qlm >= 8)
+		return -1;	/* FIXME for OCI */
+	/* Check if QLM is configured */
+	cfg.u64 = cvmx_read_csr_node(node, CVMX_GSERX_CFG(qlm));
+	if (cfg.u64 == 0)
+		return -1;
+	if (cfg.s.pcie) {
+		int pem = 0;
+		cvmx_pemx_cfg_t pemx_cfg;
+		switch(qlm) {
+		case 0: /* Either PEM0 x4 of PEM0 x8 */
+			pem = 0;
+			break;
+		case 1: /* Either PEM0 x4 of PEM1 x4 */
+			pemx_cfg.u64 = cvmx_read_csr_node(node, CVMX_PEMX_CFG(0));
+			if (pemx_cfg.cn78xx.lanes8)
+				pem = 0;
+			else
+				pem = 1;
+			break;
+		case 2: /* Either PEM2 x4 of PEM2 x8 */
+			pem = 2;
+			break;
+		case 3: /* Either PEM2 x8 of PEM3 x4 or x8 */
+			/* Can be last 4 lanes of PEM2 */
+			pemx_cfg.u64 = cvmx_read_csr_node(node, CVMX_PEMX_CFG(2));
+			if (pemx_cfg.cn78xx.lanes8)
+				pem = 2;
+			else {
+				pemx_cfg.u64 = cvmx_read_csr_node(node, CVMX_PEMX_CFG(3));
+				if (pemx_cfg.cn78xx.lanes8)
+					pem = 3;
+				else
+					pem = 2;
+			}
+			break;
+		case 4: /* Either PEM3 x8 of PEM3 x4 */
+			pem = 3;
+			break;
+		default:
+			cvmx_dprintf("QLM%d: Should be in PCIe mode\n", qlm);
+			break;
+		}
+		pemx_cfg.u64 = cvmx_read_csr_node(node, CVMX_PEMX_CFG(pem));
+		switch(pemx_cfg.s.md) {
+			case 0: /* Gen1 */
+				return 2500;
+			case 1: /* Gen2 */
+				return 5000;
+			case 2: /* Gen3 */
+				return 8000;
+			default:
+				return 0;
+		}
+	} else {
+		lane_mode.u64 = cvmx_read_csr_node(node, CVMX_GSERX_LANE_MODE(qlm));
+		switch (lane_mode.s.lmode) {
+		case R_25G_REFCLK100:
+			return 2500;
+		case R_5G_REFCLK100:
+			return 5000;
+		case R_8G_REFCLK100:
+			return 8000;
+		case R_125G_REFCLK15625_KX:
+			return 1250;
+		case R_3125G_REFCLK15625_XAUI:
+			return 3125;
+		case R_103125G_REFCLK15625_KR:
+			return 10312;
+		case R_125G_REFCLK15625_SGMII:
+			return 1250;
+		case R_5G_REFCLK15625_QSGMII:
+			return 5000;
+		case R_625G_REFCLK15625_RXAUI:
+			return 6250;
+		case R_25G_REFCLK125:
+			return 2500;
+		case R_5G_REFCLK125:
+			return 5000;
+		case R_8G_REFCLK125:
+			return 8000;
+		default:
+			return 0;
+		}
+	}
+}
+
+/**
  * Get the speed (Gbaud) of the QLM in Mhz.
  *
  * @param qlm    QLM to examine
@@ -771,93 +877,7 @@ int cvmx_qlm_get_gbaud_mhz(int qlm)
 		freq = (freq + 500000) / 1000000;
 		return freq;
 	} else if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
-		cvmx_gserx_lane_mode_t lane_mode;
-		cvmx_gserx_cfg_t cfg;
-		if (qlm >= 8)
-			return -1;	/* FIXME for OCI */
-		/* Check if QLM is configured */
-		cfg.u64 = cvmx_read_csr(CVMX_GSERX_CFG(qlm));
-		if (cfg.u64 == 0)
-			return -1;
-		if (cfg.s.pcie) {
-			int pem = 0;
-			cvmx_pemx_cfg_t pemx_cfg;
-			switch(qlm) {
-			case 0: /* Either PEM0 x4 of PEM0 x8 */
-				pem = 0;
-				break;
-			case 1: /* Either PEM0 x4 of PEM1 x4 */
-				pemx_cfg.u64 = cvmx_read_csr(CVMX_PEMX_CFG(0));
-				if (pemx_cfg.cn78xx.lanes8)
-					pem = 0;
-				else
-					pem = 1;
-				break;
-			case 2: /* Either PEM2 x4 of PEM2 x8 */
-				pem = 2;
-				break;
-			case 3: /* Either PEM2 x8 of PEM3 x4 or x8 */
-				/* Can be last 4 lanes of PEM2 */
-				pemx_cfg.u64 = cvmx_read_csr(CVMX_PEMX_CFG(2));
-				if (pemx_cfg.cn78xx.lanes8)
-					pem = 2;
-				else {
-					pemx_cfg.u64 = cvmx_read_csr(CVMX_PEMX_CFG(3));
-					if (pemx_cfg.cn78xx.lanes8)
-						pem = 3;
-					else
-						pem = 2;
-				}
-				break;
-			case 4: /* Either PEM3 x8 of PEM3 x4 */
-				pem = 3;
-				break;
-			default:
-				cvmx_dprintf("QLM%d: Should be in PCIe mode\n", qlm);
-				break;
-			}
-			pemx_cfg.u64 = cvmx_read_csr(CVMX_PEMX_CFG(pem));
-			switch(pemx_cfg.s.md) {
-				case 0: /* Gen1 */
-					return 2500;
-				case 1: /* Gen2 */
-					return 5000;
-				case 2: /* Gen3 */
-					return 8000;
-				default:
-					return 0;
-			}
-		} else {
-			lane_mode.u64 = cvmx_read_csr(CVMX_GSERX_LANE_MODE(qlm));
-			switch (lane_mode.s.lmode) {
-			case R_25G_REFCLK100:
-				return 2500;
-			case R_5G_REFCLK100:
-				return 5000;
-			case R_8G_REFCLK100:
-				return 8000;
-			case R_125G_REFCLK15625_KX:
-				return 1250;
-			case R_3125G_REFCLK15625_XAUI:
-				return 3125;
-			case R_103125G_REFCLK15625_KR:
-				return 10312;
-			case R_125G_REFCLK15625_SGMII:
-				return 1250;
-			case R_5G_REFCLK15625_QSGMII:
-				return 5000;
-			case R_625G_REFCLK15625_RXAUI:
-				return 6250;
-			case R_25G_REFCLK125:
-				return 2500;
-			case R_5G_REFCLK125:
-				return 5000;
-			case R_8G_REFCLK125:
-				return 8000;
-			default:
-				return 0;
-			}
-		}
+		return cvmx_qlm_get_gbaud_mhz_node(cvmx_get_node_num(), qlm);
 	}
 	return 0;
 }
@@ -1119,6 +1139,7 @@ static enum cvmx_qlm_mode __cvmx_qlm_get_mode_cn6xxx(int qlm)
 		case 0x7:	/* SRIO 4x1 long */
 			if (!OCTEON_IS_MODEL(OCTEON_CN66XX_PASS1_0))
 				return CVMX_QLM_MODE_SRIO_4X1;
+		/* fallthrough */
 		default:
 			return CVMX_QLM_MODE_DISABLED;
 		}
@@ -1206,6 +1227,58 @@ static enum cvmx_qlm_mode __cvmx_qlm_get_mode_cn6xxx(int qlm)
 	return CVMX_QLM_MODE_DISABLED;
 }
 
+/**
+ * @INTERNAL
+ * Decrement the MPLL Multiplier for the DLM as per Errata G-20669
+ *
+ * @param qlm            DLM to configure
+ * @param baud_mhz       Speed of the DLM configured at
+ * @param old_multiplier MPLL_MULTIPLIER value to decrement
+ */
+void __cvmx_qlm_set_mult(int qlm, int baud_mhz, int old_multiplier)
+{
+	cvmx_gserx_dlmx_mpll_multiplier_t mpll_multiplier;
+	uint64_t meas_refclock, mult;
+
+	if (!OCTEON_IS_MODEL(OCTEON_CN70XX))
+		return;
+
+	if (qlm == -1)
+		return;
+
+	meas_refclock = cvmx_qlm_measure_clock(qlm);
+	if (meas_refclock == 0) {
+		cvmx_warn("DLM%d: Reference clock not running\n", qlm);
+		return;
+	}
+
+	mult = (uint64_t)baud_mhz * 1000000 + (meas_refclock/2);
+	mult /= meas_refclock;
+
+#ifdef CVMX_BUILD_FOR_UBOOT
+	/* For simulator just write the multiplier directly, to make it
+	   faster to boot. */
+	if (gd->arch.board_desc.board_type == CVMX_BOARD_TYPE_SIM) {
+		cvmx_write_csr(CVMX_GSERX_DLMX_MPLL_MULTIPLIER(qlm, 0), mult);
+		return;
+	}
+#endif
+
+	/* 6. Decrease MPLL_MULTIPLIER by one continually until it reaches
+	     the desired long-term setting, ensuring that each MPLL_MULTIPLIER
+	     value is constant for at least 1 msec before changing to the next
+	     value. The desired long-term setting is as indicated in HRM tables
+	     21-1, 21-2, and 21-3. This is not required with the HRM
+	     sequence. */
+	do {
+		mpll_multiplier.u64 = cvmx_read_csr(CVMX_GSERX_DLMX_MPLL_MULTIPLIER(qlm, 0));
+		mpll_multiplier.s.mpll_multiplier = --old_multiplier;
+		cvmx_write_csr(CVMX_GSERX_DLMX_MPLL_MULTIPLIER(qlm, 0), mpll_multiplier.u64);
+		/* Wait for 1 ms */
+		cvmx_wait_usec(1000);
+	} while (old_multiplier > (int)mult);
+}
+
 enum cvmx_qlm_mode cvmx_qlm_get_mode_cn78xx(int node, int qlm)
 {
 	cvmx_gserx_cfg_t gserx_cfg;
@@ -1389,7 +1462,7 @@ int cvmx_qlm_measure_clock(int qlm)
 	/* Force the reference to 156.25Mhz when running in simulation.
 	   This supports the most speeds */
 #ifdef CVMX_BUILD_FOR_UBOOT
-	if (gd->board_type == CVMX_BOARD_TYPE_SIM)
+	if (gd->arch.board_desc.board_type == CVMX_BOARD_TYPE_SIM)
 		return 156250000;
 #elif !defined(CVMX_BUILD_FOR_LINUX_HOST)
 	if (cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_SIM)
diff --git a/arch/mips/cavium-octeon/executive/cvmx-range.c b/arch/mips/cavium-octeon/executive/cvmx-range.c
index 6557f51..6669b7e 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-range.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-range.c
@@ -47,6 +47,8 @@
 #define addr_of_element(base, index) (1ull << 63 | (base + sizeof(uint64_t) + (index) * sizeof(uint64_t)))
 #define addr_of_size(base) (1ull << 63 | base)
 
+static const int debug = 0;
+
 int cvmx_range_memory_size(int nelements)
 {
 	return sizeof(uint64_t) * (nelements + 1);
@@ -75,29 +77,62 @@ static int64_t cvmx_range_find_next_available(uint64_t range_addr, uint64_t inde
 
 	for (i = index; i < size; i += align) {
 		uint64_t r_owner = cvmx_read64_uint64(addr_of_element(range_addr,i));
-		//cvmx_dprintf("index=%d owner=%llx\n", (int) i, (unsigned long long) r_owner);
+		if (debug)
+			cvmx_dprintf("index=%d owner=%llx\n", (int) i,
+				     (unsigned long long) r_owner);
 		if (r_owner == CVMX_RANGE_AVAILABLE)
 			return i;
 	}
 	return -1;
 }
 
-int cvmx_range_alloc(uint64_t range_addr, uint64_t owner, uint64_t cnt, int align)
+static int64_t cvmx_range_find_last_available(uint64_t range_addr, uint64_t index, uint64_t align)
+{
+	uint64_t i;
+	uint64_t size = cvmx_read64_uint64(addr_of_size(range_addr));
+
+	if (index == 0)
+		index = size - 1;
+
+	while ((index % align) != 0)
+		index++;
+
+	for (i = index; i > align; i -= align) {
+		uint64_t r_owner = cvmx_read64_uint64(
+			addr_of_element(range_addr, i));
+		if (debug)
+			cvmx_dprintf("index=%d owner=%llx\n", (int) i,
+				(unsigned long long) r_owner);
+		if (r_owner == CVMX_RANGE_AVAILABLE)
+			return i;
+	}
+	return -1;
+}
+
+int cvmx_range_alloc_ordered(uint64_t range_addr, uint64_t owner, uint64_t cnt, int align, int reverse)
 {
 	uint64_t i=0, size;
 	int64_t first_available;
 
-	//cvmx_dprintf("%s: range_addr=%llx  owner=%llx cnt=%d \n", __FUNCTION__,
-	//	     (unsigned long long) range_addr, (unsigned long long) owner, (int)cnt);
+	if (debug)
+		cvmx_dprintf("%s: range_addr=%llx  owner=%llx cnt=%d \n",
+			__func__,
+			(unsigned long long) range_addr,
+			(unsigned long long) owner,
+			(int)cnt);
+
 	size = cvmx_read64_uint64(addr_of_size(range_addr));
-	//cvmx_dprintf("%s: size=%d\n", __FUNCTION__, size);
 	while (i < size) {
 		uint64_t available_cnt=0;
-		first_available = cvmx_range_find_next_available(range_addr, i, align);
+		if (reverse) first_available = cvmx_range_find_last_available(range_addr, i, align);
+		else first_available = cvmx_range_find_next_available(range_addr, i, align);
 		if (first_available == -1)
 			return -1;
 		i = first_available;
-		//cvmx_dprintf("%s: first_available=%d \n", __FUNCTION__, (int) first_available);
+
+		if (debug)
+			cvmx_dprintf("%s: first_available=%d \n", __func__,
+				     (int) first_available);
 		while ((available_cnt != cnt) && (i < size)) {
 			uint64_t r_owner = cvmx_read64_uint64(addr_of_element(range_addr,i));
 			if (r_owner == CVMX_RANGE_AVAILABLE)
@@ -105,12 +140,15 @@ int cvmx_range_alloc(uint64_t range_addr, uint64_t owner, uint64_t cnt, int alig
 			i++;
 		}
 		if (available_cnt == cnt) {
-			//cvmx_dprintf("%s: first_available=%d available=%d \n", __FUNCTION__,
-			//	     (int) first_available, (int) available_cnt);
 			uint64_t j;
+
+			if (debug)
+				cvmx_dprintf("%s: first_available=%d available=%d\n",
+					     __func__, (int) first_available,
+					     (int) available_cnt);
+
 			for (j = first_available; j < first_available + cnt; j++) {
 				uint64_t a = addr_of_element(range_addr,j);
-				//cvmx_dprintf("%s: j=%d a=%llx \n", __FUNCTION__, (int) j, (unsigned long long) a);
 				cvmx_write64_uint64(a, owner);
 			}
 			return first_available;
@@ -121,6 +159,11 @@ int cvmx_range_alloc(uint64_t range_addr, uint64_t owner, uint64_t cnt, int alig
 	return -1;
 }
 
+int cvmx_range_alloc(uint64_t range_addr, uint64_t owner, uint64_t cnt, int align)
+{
+	return cvmx_range_alloc_ordered(range_addr, owner, cnt, align, 0);
+}
+
 int  cvmx_range_alloc_non_contiguos(uint64_t range_addr, uint64_t owner, uint64_t cnt,
 				    int elements[])
 {
@@ -130,7 +173,9 @@ int  cvmx_range_alloc_non_contiguos(uint64_t range_addr, uint64_t owner, uint64_
 	size = cvmx_read64_uint64(addr_of_size(range_addr));
 	for (i = 0; i < size; i++) {
 		uint64_t r_owner = cvmx_read64_uint64(addr_of_element(range_addr,i));
-		//cvmx_dprintf("index=%d owner=%llx\n", (int) i, (unsigned long long) r_owner);
+		if (debug)
+			cvmx_dprintf("index=%d owner=%llx\n", (int) i,
+				     (unsigned long long) r_owner);
 		if (r_owner == CVMX_RANGE_AVAILABLE)
 			elements[element_index++] = (int) i;
 
@@ -138,8 +183,9 @@ int  cvmx_range_alloc_non_contiguos(uint64_t range_addr, uint64_t owner, uint64_
 			break;
 	}
 	if (element_index != cnt) {
-		cvmx_dprintf("ERROR: failed to allocate non contiguos cnt=%d"
-			     " available=%d\n", (int)cnt, (int) element_index);
+		if (debug)
+			cvmx_dprintf("ERROR: failed to allocate non contiguous cnt=%d"
+				     " available=%d\n", (int)cnt, (int) element_index);
 		return -1;
 	}
 	for (i = 0; i < cnt; i++) {
@@ -157,14 +203,23 @@ int cvmx_range_reserve(uint64_t range_addr, uint64_t owner, uint64_t base, uint6
 
 	size = cvmx_read64_uint64(addr_of_size(range_addr));
 	if (up > size) {
-		cvmx_dprintf("ERROR: invalid base or cnt size=%d base+cnt=%d \n", (int) size, (int)up);
+		if (debug)
+			cvmx_dprintf("ERROR: invalid base or cnt. "
+			    "range_addr=0x%llx, owner=0x%llx, size=%d base+cnt=%d\n",
+			     (unsigned long long)range_addr,
+			     (unsigned long long)owner,
+			     (int)size, (int)up);
 		return -1;
 	}
 	for (i = base; i < up; i++) {
 		r_owner = cvmx_read64_uint64(addr_of_element(range_addr,i));
-		//cvmx_dprintf("%d: %llx\n", (int) i,(unsigned long long) r_owner);
+		if (debug)
+			cvmx_dprintf("%d: %llx\n", (int) i, (unsigned long long) r_owner);
 		if (r_owner != CVMX_RANGE_AVAILABLE) {
-			cvmx_dprintf("INFO: resource already reserved base+cnt=%d %llu %llu %llx %llx %llx\n", (int)i, (unsigned long long)cnt, (unsigned long long)base, (unsigned long long)r_owner, (unsigned long long)range_addr, (unsigned long long)owner);
+			cvmx_dprintf("INFO: resource already reserved base+cnt=%d %llu %llu %llx %llx %llx\n",
+				     (int)i, (unsigned long long)cnt, (unsigned long long)base,
+				     (unsigned long long)r_owner, (unsigned long long)range_addr,
+				     (unsigned long long)owner);
 			return -1;
 		}
 	}
@@ -246,6 +301,17 @@ int cvmx_range_free_with_base(uint64_t range_addr, int base, int cnt)
 	return 0;
 }
 
+uint64_t cvmx_range_get_owner(uint64_t range_addr, uint64_t base)
+{
+	uint64_t size = cvmx_read64_uint64(addr_of_size(range_addr));
+	if (base >= size) {
+		cvmx_dprintf("ERROR: invalid base or cnt size=%d base=%d\n",
+			(int) size, (int)base);
+		return 0;
+	}
+	return cvmx_read64_uint64(addr_of_element(range_addr, base));
+}
+
 void cvmx_range_show(uint64_t range_addr)
 {
 	uint64_t pval, val, size, pindex, i;
diff --git a/arch/mips/cavium-octeon/executive/octeon-feature.c b/arch/mips/cavium-octeon/executive/octeon-feature.c
index 6b3036d..4752804 100644
--- a/arch/mips/cavium-octeon/executive/octeon-feature.c
+++ b/arch/mips/cavium-octeon/executive/octeon-feature.c
@@ -1,5 +1,5 @@
 /***********************license start***************
- * Copyright (c) 2003-2010  Cavium Inc. (support@cavium.com). All rights
+ * Copyright (c) 2003-2014  Cavium Inc. (support@cavium.com). All rights
  * reserved.
  *
  *
@@ -140,6 +140,8 @@ void __init octeon_feature_init(void)
 	OCTEON_FEATURE_SET(OCTEON_FEATURE_OCLA);
 	OCTEON_FEATURE_SET(OCTEON_FEATURE_FAU);
 	OCTEON_FEATURE_SET(OCTEON_FEATURE_PKO3);
+	OCTEON_FEATURE_SET(OCTEON_FEATURE_HNA);
+	OCTEON_FEATURE_SET(OCTEON_FEATURE_BGX_MIX);
 
 	val = OCTEON_FEATURE_SUCCESS;
 
diff --git a/arch/mips/cavium-octeon/executive/octeon-model.c b/arch/mips/cavium-octeon/executive/octeon-model.c
index 2a644cc..af95856 100644
--- a/arch/mips/cavium-octeon/executive/octeon-model.c
+++ b/arch/mips/cavium-octeon/executive/octeon-model.c
@@ -43,7 +43,7 @@
  * File defining functions for working with different Octeon
  * models.
  *
- * <hr>$Revision: 100627 $<hr>
+ * <hr>$Revision: 102057 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/octeon.h>
@@ -434,7 +434,10 @@ const char *octeon_model_get_string_buffer(uint32_t chip_id, char *buffer)
 			suffix = "AAP";
 		break;
 	case 0x95:		/* CN78XX */
-		family = "78";
+		if (OCTEON_IS_MODEL(OCTEON_CN76XX))
+			family = "76";
+		else
+			family = "78";
 		if (fus_dat3.cn78xx.l2c_crip == 2)
 			family = "77";
 		if (fus_dat3.cn78xx.nozip
diff --git a/arch/mips/include/asm/octeon/cvmx-app-init.h b/arch/mips/include/asm/octeon/cvmx-app-init.h
index 41fbf9f..f3eb084 100644
--- a/arch/mips/include/asm/octeon/cvmx-app-init.h
+++ b/arch/mips/include/asm/octeon/cvmx-app-init.h
@@ -41,7 +41,7 @@
  * @file
  * Header file for simple executive application initialization.  This defines
  * part of the ABI between the bootloader and the application.
- * <hr>$Revision: 100175 $<hr>
+ * <hr>$Revision: 101905 $<hr>
  *
  */
 
@@ -281,6 +281,7 @@ enum cvmx_board_types_enum {
 	CVMX_BOARD_TYPE_NIC210NVG = 63,
 	CVMX_BOARD_TYPE_SFF7000 = 64,
 	CVMX_BOARD_TYPE_EBB7800_CFG1 = 65,
+	CVMX_BOARD_TYPE_TB7600 = 66,
 	CVMX_BOARD_TYPE_MAX,
 	/* NOTE:  256-257 are being used by a customer. */
 
@@ -414,6 +415,7 @@ static inline const char *cvmx_board_type_to_string(enum cvmx_board_types_enum t
 		ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_NIC210NVG)
 		ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_SFF7000)
 		ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_EBB7800_CFG1)
+		ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_TB7600)
 		ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_MAX)
 
 		/* Customer boards listed here */
diff --git a/arch/mips/include/asm/octeon/cvmx-bch.h b/arch/mips/include/asm/octeon/cvmx-bch.h
index 481d2af..b0cbd6a 100644
--- a/arch/mips/include/asm/octeon/cvmx-bch.h
+++ b/arch/mips/include/asm/octeon/cvmx-bch.h
@@ -51,7 +51,7 @@
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 # include <asm/octeon/octeon.h>
 # include <asm/octeon/cvmx-bch-defs.h>
-# include <asm/octeon/cvmx-fpa1.h>
+# include <asm/octeon/cvmx-fpa.h>
 #elif defined(CVMX_BUILD_FOR_UBOOT)
 # include <common.h>
 # include <asm/arch/cvmx.h>
diff --git a/arch/mips/include/asm/octeon/cvmx-clock.h b/arch/mips/include/asm/octeon/cvmx-clock.h
index dd4e398..5f27e49 100644
--- a/arch/mips/include/asm/octeon/cvmx-clock.h
+++ b/arch/mips/include/asm/octeon/cvmx-clock.h
@@ -1,5 +1,5 @@
 /***********************license start***************
- * Copyright (c) 2003-2010  Cavium Inc. (support@cavium.com). All rights
+ * Copyright (c) 2003-2014  Cavium Inc. (support@cavium.com). All rights
  * reserved.
  *
  *
diff --git a/arch/mips/include/asm/octeon/cvmx-cmd-queue.h b/arch/mips/include/asm/octeon/cvmx-cmd-queue.h
index d6314d7..927fd26 100644
--- a/arch/mips/include/asm/octeon/cvmx-cmd-queue.h
+++ b/arch/mips/include/asm/octeon/cvmx-cmd-queue.h
@@ -1,5 +1,5 @@
 /***********************license start***************
- * Copyright (c) 2003-2010  Cavium Inc. (support@cavium.com). All rights
+ * Copyright (c) 2003-2014  Cavium Inc. (support@cavium.com). All rights
  * reserved.
  *
  *
@@ -46,7 +46,7 @@
  * The common command queue infrastructure abstracts out the
  * software necessary for adding to Octeon's chained queue
  * structures. These structures are used for commands to the
- * PKO, ZIP, DFA, RAID, and DMA engine blocks. Although each
+ * PKO, ZIP, DFA, RAID, HNA, and DMA engine blocks. Although each
  * hardware unit takes commands and CSRs of different types,
  * they all use basic linked command buffers to store the
  * pending request. In general, users of the CVMX API don't
@@ -82,15 +82,12 @@
  * internal cycle counter to completely eliminate any causes of
  * bus traffic.
  *
- * <hr> $Revision: 99270 $ <hr>
+ * <hr> $Revision: 103822 $ <hr>
  */
 
 #ifndef __CVMX_CMD_QUEUE_H__
 #define __CVMX_CMD_QUEUE_H__
 
-#include "cvmx-fpa1.h"
-#include "cvmx-fpa3.h"
-
 #ifdef	__cplusplus
 /* *INDENT-OFF* */
 extern "C" {
@@ -123,7 +120,8 @@ typedef enum {
 #define CVMX_CMD_QUEUE_DMA(queue) ((cvmx_cmd_queue_id_t)(CVMX_CMD_QUEUE_DMA_BASE + (0xffff&(queue))))
 	CVMX_CMD_QUEUE_BCH = 0x50000,
 #define CVMX_CMD_QUEUE_BCH(queue) ((cvmx_cmd_queue_id_t)(CVMX_CMD_QUEUE_BCH + (0xffff&(queue))))
-	CVMX_CMD_QUEUE_END = 0x60000,
+	CVMX_CMD_QUEUE_HNA = 0x60000,
+	CVMX_CMD_QUEUE_END = 0x70000,
 } cvmx_cmd_queue_id_t;
 
 /**
@@ -337,10 +335,7 @@ __cvmx_cmd_queue_get_state(cvmx_cmd_queue_id_t queue_id)
 static inline uint64_t *__cvmx_cmd_queue_alloc_buffer(int pool)
 {
 	uint64_t *new_buffer;
-	if (octeon_has_feature(OCTEON_FEATURE_FPA3))
-		new_buffer = cvmx_fpa3_alloc_aura(0, pool);
-	else
-		new_buffer = cvmx_fpa1_alloc(pool);
+	new_buffer = cvmx_fpa_alloc(pool);
 	return new_buffer;
 }
 
diff --git a/arch/mips/include/asm/octeon/cvmx-coremask.h b/arch/mips/include/asm/octeon/cvmx-coremask.h
index aff91ea..57d5c84 100644
--- a/arch/mips/include/asm/octeon/cvmx-coremask.h
+++ b/arch/mips/include/asm/octeon/cvmx-coremask.h
@@ -60,7 +60,7 @@
  * provide future compatibility if more cores are added to future processors
  * or more nodes are supported.
  *
- * <hr>$Revision: 97423 $<hr>
+ * <hr>$Revision: 101121 $<hr>
  *
  */
 
@@ -641,9 +641,9 @@ cvmx_coremask_is_core_first_core(const cvmx_coremask_t *pcm,
 	for (i = 0; i < n; i++)
 		if (pcm->coremask_bitmap[i] != 0)
 			return 0;
-	if (__builtin_ffs(pcm->coremask_bitmap[n]) < core + 1)
+	if (__builtin_ffsll(pcm->coremask_bitmap[n]) < core + 1)
 		return 0;
-	return (__builtin_ffs(pcm->coremask_bitmap[n]) == core + 1);
+	return (__builtin_ffsll(pcm->coremask_bitmap[n]) == core + 1);
 }
 
 /*
diff --git a/arch/mips/include/asm/octeon/cvmx-dma-engine.h b/arch/mips/include/asm/octeon/cvmx-dma-engine.h
index 210d796..6d303c7 100644
--- a/arch/mips/include/asm/octeon/cvmx-dma-engine.h
+++ b/arch/mips/include/asm/octeon/cvmx-dma-engine.h
@@ -43,7 +43,7 @@
  * Interface to the PCI / PCIe DMA engines. These are only avialable
  * on chips with PCI / PCIe.
  *
- * <hr>$Revision: 95258 $<hr>
+ * <hr>$Revision: 103822 $<hr>
  */
 
 #ifndef __CVMX_DMA_ENGINES_H__
@@ -51,10 +51,10 @@
 
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx-dpi-defs.h>
-#include <asm/octeon/cvmx-fpa1.h>
+#include <asm/octeon/cvmx-fpa.h>
 #else
 #include "cvmx-dpi-defs.h"
-#include "cvmx-fpa1.h"
+#include "cvmx-fpa.h"
 #endif
 
 #ifdef	__cplusplus
diff --git a/arch/mips/include/asm/octeon/cvmx-fpa.h b/arch/mips/include/asm/octeon/cvmx-fpa.h
new file mode 100644
index 0000000..a60cb44
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-fpa.h
@@ -0,0 +1,301 @@
+/***********************license start***************
+ * Copyright (c) 2003-2010  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Interface to the hardware Free Pool Allocator.
+ *
+ * <hr>$Revision: 103836 $<hr>
+ *
+ */
+
+#ifndef __CVMX_FPA_H__
+#define __CVMX_FPA_H__
+
+#include "cvmx-scratch.h"
+#include "cvmx.h"
+
+#include "cvmx-fpa-defs.h"
+#include "cvmx-fpa1.h"
+#include "cvmx-fpa3.h"
+
+#define CVMX_FPA_MIN_BLOCK_SIZE 128
+#define CVMX_FPA_ALIGNMENT      128
+#define CVMX_FPA_POOL_NAME_LEN  16
+
+/* On CN78XX in backward-compatible mode, pool is mapped to AURA */
+#define CVMX_FPA_NUM_POOLS (octeon_has_feature(OCTEON_FEATURE_FPA3) ? \
+			CVMX_FPA3_NUM_POOLX : CVMX_FPA1_NUM_POOLS)
+
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+extern "C" {
+/* *INDENT-ON* */
+#endif
+
+/**
+ * Structure to store FPA pool configuration parameters.
+ */
+struct cvmx_fpa_pool_config {
+	int64_t pool_num;
+	uint64_t buffer_size;
+	uint64_t buffer_count;
+};
+typedef struct cvmx_fpa_pool_config cvmx_fpa_pool_config_t;
+
+/**
+ * Return the name of the pool
+ *
+ * @param pool   Pool to get the name of
+ * @return The name
+ */
+const char *cvmx_fpa_get_name(int pool);
+
+/**
+ * Return the base of the pool
+ *
+ * @param pool   Pool to get the base of
+ * @return The base
+ */
+void *cvmx_fpa_get_base(int pool);
+
+/**
+ * Check if a pointer belongs to an FPA pool. Return non-zero
+ * if the supplied pointer is inside the memory controlled by
+ * an FPA pool.
+ *
+ * @param pool   Pool to check
+ * @param ptr    Pointer to check
+ * @return Non-zero if pointer is in the pool. Zero if not
+ */
+int cvmx_fpa_is_member(int pool, void *ptr);
+
+
+/**
+ * Initialize FPA per node
+ */
+int cvmx_fpa_global_init_node(int node);
+
+/**
+ * Enable the FPA
+ */
+static inline void cvmx_fpa_enable(void)
+{
+	if (!octeon_has_feature(OCTEON_FEATURE_FPA3))
+		cvmx_fpa1_enable();
+	else
+		cvmx_fpa_global_init_node(0);
+}
+
+/**
+ * Disable the FPA
+ */
+static inline void cvmx_fpa_disable(void)
+{
+	if (!octeon_has_feature(OCTEON_FEATURE_FPA3))
+		cvmx_fpa1_disable();
+	/* FPA3 does not have a disable funcion */
+}
+
+/**
+ * @INTERNAL
+ * @OBSOLETE
+ *
+ * Kept for transition assistance only
+ */
+static inline void cvmx_fpa_global_initialize(void)
+{
+	cvmx_fpa_global_init_node(0);
+}
+
+
+/**
+ * @INTERNAL
+ *
+ * Convert FPA1 style POOL into FPA3 AURA in
+ * backward compatibility mode.
+ */
+static inline cvmx_fpa3_gaura_t
+cvmx_fpa1_pool_to_fpa3_aura(cvmx_fpa1_pool_t pool)
+{
+	if ((octeon_has_feature(OCTEON_FEATURE_FPA3))) {
+		cvmx_fpa3_gaura_t aura = __cvmx_fpa3_gaura(0, pool);
+		return aura;
+	}
+	return CVMX_FPA3_INVALID_GAURA;
+}
+
+/**
+ * Get a new block from the FPA
+ *
+ * @param pool   Pool to get the block from
+ * @return Pointer to the block or NULL on failure
+ */
+static inline void *cvmx_fpa_alloc(uint64_t pool)
+{
+	/* FPA3 is handled differently */
+	if ((octeon_has_feature(OCTEON_FEATURE_FPA3))) {
+		return cvmx_fpa3_alloc(
+			cvmx_fpa1_pool_to_fpa3_aura(pool));
+	} else
+		return cvmx_fpa1_alloc(pool);
+}
+
+/**
+ * Asynchronously get a new block from the FPA
+ *
+ * The result of cvmx_fpa_async_alloc() may be retrieved using
+ * cvmx_fpa_async_alloc_finish().
+ *
+ * @param scr_addr Local scratch address to put response in.  This is a byte
+ *		   address but must be 8 byte aligned.
+ * @param pool      Pool to get the block from
+ */
+static inline void cvmx_fpa_async_alloc(uint64_t scr_addr, uint64_t pool)
+{
+	if ((octeon_has_feature(OCTEON_FEATURE_FPA3))) {
+		return cvmx_fpa3_async_alloc(scr_addr,
+			cvmx_fpa1_pool_to_fpa3_aura(pool));
+	} else
+		return cvmx_fpa1_async_alloc(scr_addr, pool);
+}
+
+/**
+ * Retrieve the result of cvmx_fpa_async_alloc
+ *
+ * @param scr_addr The Local scratch address.  Must be the same value
+ * passed to cvmx_fpa_async_alloc().
+ *
+ * @param pool Pool the block came from.  Must be the same value
+ * passed to cvmx_fpa_async_alloc.
+ *
+ * @return Pointer to the block or NULL on failure
+ */
+static inline void *cvmx_fpa_async_alloc_finish(uint64_t scr_addr, uint64_t pool)
+{
+	if ((octeon_has_feature(OCTEON_FEATURE_FPA3)))
+		return cvmx_fpa3_async_alloc_finish(
+			scr_addr, cvmx_fpa1_pool_to_fpa3_aura(pool));
+	else
+		return cvmx_fpa1_async_alloc_finish(
+			scr_addr, pool);
+}
+
+/**
+ * Free a block allocated with a FPA pool.
+ * Does NOT provide memory ordering in cases where the memory block was
+ * modified by the core.
+ *
+ * @param ptr    Block to free
+ * @param pool   Pool to put it in
+ * @param num_cache_lines
+ *               Cache lines to invalidate
+ */
+static inline void cvmx_fpa_free_nosync(void *ptr, uint64_t pool,
+					uint64_t num_cache_lines)
+{
+	/* FPA3 is handled differently */
+	if ((octeon_has_feature(OCTEON_FEATURE_FPA3)))
+		cvmx_fpa3_free_nosync(ptr, cvmx_fpa1_pool_to_fpa3_aura(pool),
+			num_cache_lines);
+	else
+		cvmx_fpa1_free_nosync(ptr, pool, num_cache_lines);
+}
+
+/**
+ * Free a block allocated with a FPA pool.  Provides required memory
+ * ordering in cases where memory block was modified by core.
+ *
+ * @param ptr    Block to free
+ * @param pool   Pool to put it in
+ * @param num_cache_lines
+ *               Cache lines to invalidate
+ */
+static inline void cvmx_fpa_free(void *ptr, uint64_t pool,
+				 uint64_t num_cache_lines)
+{
+	if ((octeon_has_feature(OCTEON_FEATURE_FPA3)))
+		cvmx_fpa3_free(ptr, cvmx_fpa1_pool_to_fpa3_aura(pool),
+			num_cache_lines);
+	else
+		cvmx_fpa1_free(ptr, pool, num_cache_lines);
+}
+
+
+/**
+ * Setup a FPA pool to control a new block of memory.
+ * This can only be called once per pool. Make sure proper
+ * locking enforces this.
+ *
+ * @param pool       Pool to initialize
+ * @param name       Constant character string to name this pool.
+ *                   String is not copied.
+ * @param buffer     Pointer to the block of memory to use. This must be
+ *                   accessable by all processors and external hardware.
+ * @param block_size Size for each block controlled by the FPA
+ * @param num_blocks Number of blocks
+ *
+ * @return the pool number on Success,
+ *         -1 on failure
+ */
+extern int cvmx_fpa_setup_pool(int pool, const char *name, void *buffer,
+			       uint64_t block_size, uint64_t num_blocks);
+
+extern int cvmx_fpa_shutdown_pool(int pool);
+
+/**
+ * Gets the block size of buffer in specified pool
+ * @param pool	 Pool to get the block size from
+ * @return       Size of buffer in specified pool
+ */
+extern unsigned cvmx_fpa_get_block_size(int pool);
+
+extern int cvmx_fpa_is_pool_available(int pool_num);
+extern uint64_t cvmx_fpa_get_pool_owner(int pool_num);
+extern int cvmx_fpa_get_max_pools(void);
+extern int cvmx_fpa_get_current_count(int pool_num);
+
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+}
+/* *INDENT-ON* */
+#endif
+
+#endif /*  __CVM_FPA_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-fpa1.h b/arch/mips/include/asm/octeon/cvmx-fpa1.h
index 700a576..a2cfc7b 100644
--- a/arch/mips/include/asm/octeon/cvmx-fpa1.h
+++ b/arch/mips/include/asm/octeon/cvmx-fpa1.h
@@ -1,5 +1,5 @@
 /***********************license start***************
- * Copyright (c) 2003-2010  Cavium Inc. (support@cavium.com). All rights
+ * Copyright (c) 2014  Cavium Inc. (support@cavium.com). All rights
  * reserved.
  *
  *
@@ -40,52 +40,90 @@
 /**
  * @file
  *
- * Interface to the hardware Free Pool Allocator on the 78xx.
+ * Interface to the hardware Free Pool Allocator on Octeon chips.
+ * These are the legacy models, i.e. prior to CN78XX/CN76XX.
  *
- * <hr>$Revision: 94697 $<hr>
+ * <hr>$Revision: 103822 $<hr>
  *
  */
 
-#ifndef __CVMX_FPA1_H__
-#define __CVMX_FPA1_H__
 
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include <asm/octeon/cvmx-scratch.h>
 #include <asm/octeon/cvmx-fpa-defs.h>
+#else
+#include "cvmx-scratch.h"
+#include "cvmx-fpa-defs.h"
 #endif
 
-#define CVMX_FPA_NUM_POOLS      8
+#ifndef __CVMX_FPA1_HW_H__
+#define __CVMX_FPA1_HW_H__
 
-/**
- * Structure to store FPA pool configuration parameters.
- */
-typedef struct cvmx_fpa_pool_config
-{
-	int64_t pool_num;
-	uint64_t buffer_size;
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+extern "C" {
+/* *INDENT-ON* */
+#endif
+
+/* Legacy pool range is 0..7 and 8 on CN68XX */
+typedef	int cvmx_fpa1_pool_t;
+#define CVMX_FPA1_NUM_POOLS      9
+#define CVMX_FPA1_INVALID_POOL ((cvmx_fpa1_pool_t)-1)
+#define	CVMX_FPA1_NAME_SIZE	16
+
+/* FIXME: To be reworked for named blocks */
+typedef struct {
+	char name[CVMX_FPA1_NAME_SIZE];
+//	uint64_t buffer_size;
+	uint64_t size; /* Block size of pool buffers */
 	uint64_t buffer_count;
-}cvmx_fpa_pool_config_t;
+	uint64_t base_paddr; /* Base of buffer is allocated at initialization */
+} cvmx_fpa1_pool_info_t;
 
 /**
- * Allocate or reserve  the specified fpa pool.
+ * Structure describing the data format used for stores to the FPA.
+ */
+typedef union {
+	uint64_t u64;
+	struct {
+		CVMX_BITFIELD_FIELD(uint64_t scraddr:8,	/**
+					 * the (64-bit word) location in
+					 * scratchpad to write to (if len != 0)
+					 */
+		CVMX_BITFIELD_FIELD(uint64_t len:8,		/**
+					 * the number of words in the response
+					 * (0 => no response)
+					 */
+		CVMX_BITFIELD_FIELD(uint64_t did:8,		/**
+					 * the ID of the device on the
+					 * non-coherent bus
+					 */
+		CVMX_BITFIELD_FIELD(uint64_t addr:40,	/**
+					 * the address that will appear in the
+					 * first tick on the NCB bus
+					 */
+		))));
+	} s;
+} cvmx_fpa1_iobdma_data_t;
+
+/*
+ * Allocate or reserve the specified fpa pool.
  *
  * @param pool	  FPA pool to allocate/reserve. If -1 it
  *                finds an empty pool to allocate.
- * @return        Alloctaed pool number or -1 if fails to allocate
-                  the pool
+ * @return        Alloctaed pool number or CVMX_FPA1_POOL_INVALID
+ *                if fails to allocate the pool
  */
-int cvmx_fpa_alloc_pool(int pool);
+cvmx_fpa1_pool_t cvmx_fpa1_reserve_pool(cvmx_fpa1_pool_t pool);
 
 /**
  * Free the specified fpa pool.
  * @param pool	   Pool to free
  * @return         0 for success -1 failure
  */
-int cvmx_fpa_release_pool(int pool);
-
-int cvmx_fpa1_pool_init(int pool_id, int num_blocks, int block_size,
-		       void *buffer, const char *name);
+int cvmx_fpa1_release_pool(cvmx_fpa1_pool_t pool);
 
-static inline void cvmx_fpa1_free(void *ptr, uint64_t pool,
+static inline void cvmx_fpa1_free(void *ptr, cvmx_fpa1_pool_t pool,
 	uint64_t num_cache_lines)
 {
 	cvmx_addr_t newptr;
@@ -102,15 +140,15 @@ static inline void cvmx_fpa1_free(void *ptr, uint64_t pool,
 	cvmx_write_io(newptr.u64, num_cache_lines);
 }
 
-static inline void cvmx_fpa1_free_nosync(void *ptr, uint64_t pool,
-					uint64_t num_cache_lines)
+static inline void cvmx_fpa1_free_nosync(void *ptr, cvmx_fpa1_pool_t pool,
+					unsigned num_cache_lines)
 {
 	cvmx_addr_t newptr;
 	newptr.u64 = cvmx_ptr_to_phys(ptr);
 	newptr.sfilldidspace.didspace =
 		CVMX_ADDR_DIDSPACE(CVMX_FULL_DID(CVMX_OCT_DID_FPA, pool));
 	/* Prevent GCC from reordering around free */
-	asm volatile ("":::"memory");
+	asm volatile ("" : : : "memory");
 	/* value written is number of cache lines not written back */
 	cvmx_write_io(newptr.u64, num_cache_lines);
 }
@@ -126,10 +164,10 @@ static inline void cvmx_fpa1_enable(void)
 	status.u64 = cvmx_read_csr(CVMX_FPA_CTL_STATUS);
 	if (status.s.enb) {
 		/*
-	 	* CN68XXP1 should not reset the FPA (doing so may break
-		* the SSO, so we may end up enabling it more than once.
-		* Just return and don't spew messages.
-	 	*/
+		 * CN68XXP1 should not reset the FPA (doing so may break
+		 * the SSO, so we may end up enabling it more than once.
+		 * Just return and don't spew messages.
+		 */
 		return;
 	}
 
@@ -138,7 +176,24 @@ static inline void cvmx_fpa1_enable(void)
 	cvmx_write_csr(CVMX_FPA_CTL_STATUS, status.u64);
 }
 
-static inline void *cvmx_fpa1_alloc(uint64_t pool)
+/**
+ * Reset FPA to disable. Make sure buffers from all FPA pools are freed
+ * before disabling FPA.
+ */
+static inline void cvmx_fpa1_disable(void)
+{
+	cvmx_fpa_ctl_status_t status;
+
+	if (OCTEON_IS_MODEL(OCTEON_CN68XX_PASS1))
+		return;
+
+	status.u64 = cvmx_read_csr(CVMX_FPA_CTL_STATUS);
+	status.s.reset = 1;
+	cvmx_write_csr(CVMX_FPA_CTL_STATUS, status.u64);
+}
+
+
+static inline void *cvmx_fpa1_alloc(cvmx_fpa1_pool_t pool)
 {
 	uint64_t address;
 
@@ -156,4 +211,72 @@ static inline void *cvmx_fpa1_alloc(uint64_t pool)
 	}
 }
 
+/**
+ * Asynchronously get a new block from the FPA
+ * @INTERNAL
+ *
+ * The result of cvmx_fpa_async_alloc() may be retrieved using
+ * cvmx_fpa_async_alloc_finish().
+ *
+ * @param scr_addr Local scratch address to put response in.  This is a byte
+ *		   address but must be 8 byte aligned.
+ * @param pool      Pool to get the block from
+ */
+static inline void
+cvmx_fpa1_async_alloc(uint64_t scr_addr, cvmx_fpa1_pool_t pool)
+{
+	cvmx_fpa1_iobdma_data_t data;
+
+	/* Hardware only uses 64 bit aligned locations, so convert from byte
+	 * address to 64-bit index
+	 */
+	data.u64 = 0ull;
+	data.s.scraddr = scr_addr >> 3;
+	data.s.len = 1;
+	data.s.did = CVMX_FULL_DID(CVMX_OCT_DID_FPA, pool);
+	data.s.addr = 0;
+
+	cvmx_scratch_write64(scr_addr, 0ull);
+	CVMX_SYNCW;
+	cvmx_send_single(data.u64);
+}
+
+/**
+ * Retrieve the result of cvmx_fpa_async_alloc
+ * @INTERNAL
+ *
+ * @param scr_addr The Local scratch address.  Must be the same value
+ * passed to cvmx_fpa_async_alloc().
+ *
+ * @param pool Pool the block came from.  Must be the same value
+ * passed to cvmx_fpa_async_alloc.
+ *
+ * @return Pointer to the block or NULL on failure
+ */
+static inline void *
+cvmx_fpa1_async_alloc_finish(uint64_t scr_addr, cvmx_fpa1_pool_t pool)
+{
+	uint64_t address;
+
+	CVMX_SYNCIOBDMA;
+
+	address = cvmx_scratch_read64(scr_addr);
+	if (cvmx_likely(address))
+		return cvmx_phys_to_ptr(address);
+	else
+		return cvmx_fpa1_alloc(pool);
+}
+
+
+static inline uint64_t cvmx_fpa1_get_available(cvmx_fpa1_pool_t pool)
+{
+	return cvmx_read_csr(CVMX_FPA_QUEX_AVAILABLE(pool));
+}
+
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+}
+/* *INDENT-ON* */
 #endif
+
+#endif /* __CVMX_FPA1_HW_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-fpa3.h b/arch/mips/include/asm/octeon/cvmx-fpa3.h
index 0f1410e..c98ef0a 100644
--- a/arch/mips/include/asm/octeon/cvmx-fpa3.h
+++ b/arch/mips/include/asm/octeon/cvmx-fpa3.h
@@ -40,26 +40,22 @@
 /**
  * @file
  *
- * Interface to the hardware Free Pool Allocator on the 78xx.
+ * Interface to the CN78XX Free Pool Allocator, a.k.a. FPA3
  *
- * <hr>$Revision: 94697 $<hr>
+ * <hr>$Revision: 103836 $<hr>
  *
  */
 
-#ifndef __CVMX_FPA3_H__
-#define __CVMX_FPA3_H__
-
-#include "cvmx.h"
-
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include <asm/octeon/cvmx-fpa-defs.h>
+#include <asm/octeon/cvmx-scratch.h>
+#else
 #include "cvmx-fpa-defs.h"
+#include "cvmx-scratch.h"
 #endif
 
-enum fpa_pool_alignment {
-	FPA_NATURAL_ALIGNMENT,
-	FPA_OFFSET_ALIGNMENT,
-	FPA_OPAQUE_ALIGNMENT
-};
+#ifndef __CVMX_FPA3_H__
+#define __CVMX_FPA3_H__
 
 #ifdef	__cplusplus
 /* *INDENT-OFF* */
@@ -67,19 +63,129 @@ extern "C" {
 /* *INDENT-ON* */
 #endif
 
-#define CVMX_FPA3_AURA_NUM       	1024
-#define CVMX_FPA3_NUM_POOLS			64
-#define CVMX_FPA3_AURA_NAME_LEN  	32
+typedef struct {
+	unsigned res0:6;
+	unsigned node:2;
+	unsigned res1:2;
+	unsigned lpool:6;
+	unsigned valid_magic:16;
+} cvmx_fpa3_pool_t;
+
+typedef struct {
+	unsigned res0:6;
+	unsigned node:2;
+	unsigned res1:6;
+	unsigned laura:10;
+	unsigned valid_magic:16;
+} cvmx_fpa3_gaura_t;
+
+#define CVMX_FPA3_VALID_MAGIC 0xf9a3
+#define CVMX_FPA3_INVALID_GAURA ((cvmx_fpa3_gaura_t){0, 0, 0, 0, 0})
+#define CVMX_FPA3_INVALID_POOL ((cvmx_fpa3_pool_t){0, 0, 0, 0, 0})
+
+static inline bool __cvmx_fpa3_aura_valid(cvmx_fpa3_gaura_t aura)
+{
+	if (aura.valid_magic != CVMX_FPA3_VALID_MAGIC)
+		return false;
+	return true;
+}
 
-#define CVMX_FPA3_NULL_POOL 		0
+static inline bool __cvmx_fpa3_pool_valid(cvmx_fpa3_pool_t pool)
+{
+	if (pool.valid_magic != CVMX_FPA3_VALID_MAGIC)
+			return false;
+	return true;
+}
+
+static inline cvmx_fpa3_gaura_t __cvmx_fpa3_gaura(int node, int laura)
+{
+	cvmx_fpa3_gaura_t aura;
+
+	if (node < 0)
+		node = cvmx_get_node_num();
+	if (laura < 0)
+		return CVMX_FPA3_INVALID_GAURA;
+
+	aura.node = node;
+	aura.laura = laura;
+	aura.valid_magic = CVMX_FPA3_VALID_MAGIC;
+	return aura;
+}
+
+static inline cvmx_fpa3_pool_t __cvmx_fpa3_pool(int node, int lpool)
+{
+	cvmx_fpa3_pool_t pool;
+
+	if (node < 0)
+		node = cvmx_get_node_num();
+	if (lpool < 0)
+		return CVMX_FPA3_INVALID_POOL;
+
+	pool.node = node;
+	pool.lpool = lpool;
+	pool.valid_magic = CVMX_FPA3_VALID_MAGIC;
+	return pool;
+}
+
+#undef CVMX_FPA3_VALID_MAGIC
 
-//#define CVMX_DEBUG_FPA3 			1
 /**
- * Struct describing load allocate operation addresses for FPA pool.
+ * Structure describing the data format used for stores to the FPA.
  */
 typedef union {
 	uint64_t u64;
 	struct {
+		CVMX_BITFIELD_FIELD(uint64_t scraddr:8,	/**
+					 * the (64-bit word) location in
+					 * scratchpad to write to (if len != 0)
+					 */
+		CVMX_BITFIELD_FIELD(uint64_t len:8,		/**
+					 * the number of words in the response
+					 * (0 => no response)
+					 */
+		CVMX_BITFIELD_FIELD(uint64_t did:8,		/**
+					 * the ID of the device on the
+					 * non-coherent bus
+					 */
+		CVMX_BITFIELD_FIELD(uint64_t addr:40,	/**
+					 * the address that will appear in the
+					 * first tick on the NCB bus
+					 */
+		))));
+	} s;
+	struct {
+		CVMX_BITFIELD_FIELD(uint64_t scraddr:8,     /**
+					 * the (64-bit word) location in
+					 * scratchpad to write to (if len != 0)
+					 */
+		CVMX_BITFIELD_FIELD(uint64_t len:8,		/**
+					 * length of return in workds, must be
+					 * one.  If the pool has no availale pts
+					 * in the selected pool then the ptr
+					 * returned for the IOBDMA operation are
+					 * 0s, indicating that the pool does not
+					 * have an adequate number of ptrs to
+					 * satisfy the IOBDMA.
+					 */
+		CVMX_BITFIELD_FIELD(uint64_t did:8,		/**
+					 * the ID of the device on the
+					 * non-coherent bus
+					 */
+		CVMX_BITFIELD_FIELD(uint64_t node:4,	/** OCI node number */
+		CVMX_BITFIELD_FIELD(uint64_t red:1,		/** Perform RED on allocation */
+		CVMX_BITFIELD_FIELD(uint64_t reserved2:9,   /** Reserved */
+		CVMX_BITFIELD_FIELD(uint64_t aura:10,	/** Aura number */
+		CVMX_BITFIELD_FIELD(uint64_t reserved3:16,	/** Reserved */
+		))))))));
+	} cn78xx;
+} cvmx_fpa3_iobdma_data_t;
+
+/**
+ * Struct describing load allocate operation addresses for FPA pool.
+ */
+union cvmx_fpa3_load_data {
+	uint64_t u64;
+	struct {
 		CVMX_BITFIELD_FIELD(uint64_t seg:2,
 		CVMX_BITFIELD_FIELD(uint64_t reserved1:13,
 		CVMX_BITFIELD_FIELD(uint64_t io:1,		/** Indicates I/O space */
@@ -94,12 +200,13 @@ typedef union {
 		CVMX_BITFIELD_FIELD(uint64_t reserved3:16,	/** Reserved */
 		)))))))));
 	};
-} cvmx_fpa3_load_data_t;
+};
+typedef union cvmx_fpa3_load_data cvmx_fpa3_load_data_t;
 
 /**
  * Struct describing store free operation addresses from FPA pool.
  */
-typedef union {
+union cvmx_fpa3_store_addr {
 	uint64_t u64;
 	struct {
 		CVMX_BITFIELD_FIELD(uint64_t seg:2,
@@ -122,50 +229,47 @@ typedef union {
 		CVMX_BITFIELD_FIELD(uint64_t reserved4:3,	/** Reserved */
 		)))))))))));
 	};
-} cvmx_fpa3_store_addr_t;
-
-int cvmx_fpa3_allocate_auras(int node, int auras_allocated[], int count);
-int cvmx_fpa3_allocate_aura(int node);
-int cvmx_fpa3_free_auras(int node, int *auras_allocated, int count);
-int cvmx_fpa3_allocate_pools(int node, int pools_allocated[], int count);
-int cvmx_fpa3_free_pools(int node, int *pools_allocated, int count);
-
-void cvmx_fpa3_pool_set_stack(int node, int pool_id, uint64_t base, uint64_t len);
+};
+typedef union cvmx_fpa3_store_addr cvmx_fpa3_store_addr_t;
 
-static inline int cvmx_fpa3_arua_to_guara(int node, int laura)
-{
-	return node << 10 | laura;
-}
+enum cvmx_fpa3_pool_alignment_e {
+	FPA_NATURAL_ALIGNMENT,
+	FPA_OFFSET_ALIGNMENT,
+	FPA_OPAQUE_ALIGNMENT
+};
 
-int cvmx_fpa3_aura_to_pool(int node, int aura_id);
-void* cvmx_fpa3_aura_get_base(int node, int aura_id);
+#define CVMX_FPA3_NUM_POOLX	                64
+#define CVMX_FPA3_NUM_AURAS                     1024
+#define	CVMX_FPA3_AURAX_LIMIT_MAX               ((1ull<<40)-1)
 
 /**
- * Get a new block from the FPA Aura
+ * Get a new block from the FPA pool
+ *
+ * @INTERNAL
  *
  * @param node  - node number
- * @param aura  - aura to get the block from
+ * @param pool  - pool to get the block from
  * @return pointer to the block or NULL on failure
  */
-static inline void *cvmx_fpa3_alloc_aura(unsigned int node, unsigned int aura)
+static inline void *cvmx_fpa3_alloc(cvmx_fpa3_gaura_t aura)
 {
 	uint64_t address;
 	cvmx_fpa3_load_data_t load_addr;
 
+	if (CVMX_ENABLE_PARAMETER_CHECKING) {
+		if (!__cvmx_fpa3_aura_valid(aura))
+			return NULL;
+	}
+
 	load_addr.u64 = 0;
 	load_addr.seg = CVMX_MIPS_SPACE_XKPHYS;
 	load_addr.io = 1;
 	load_addr.did = 0x29;    /* Device ID. Indicates FPA. */
-	load_addr.node = node;   /* OCI node number */
+	load_addr.node = aura.node;
 	load_addr.red = 0;       /* Perform RED on allocation.
-					 * FIXME to use config option
-					 */
-	load_addr.aura = aura;   /* Aura number */
-
-#ifdef CVMX_DEBUG_FPA3
-	if (cvmx_fpa3_aura_to_pool(node, aura) == CVMX_FPA3_NULL_POOL)
-		cvmx_dprintf("Error: FPA aura %u assigned to null pool.\n", aura);
-#endif
+				  * FIXME to use config option
+				  */
+	load_addr.aura = aura.laura;
 
 	address = cvmx_read64_uint64(load_addr.u64);
 	if (!address)
@@ -173,238 +277,356 @@ static inline void *cvmx_fpa3_alloc_aura(unsigned int node, unsigned int aura)
 	return cvmx_phys_to_ptr(address);
 }
 
-static inline void *cvmx_fpa3_alloc_gaura(unsigned int gaura)
+/**
+ * Asynchronously get a new block from the FPA
+ * @INTERNAL
+ *
+ * The result of cvmx_fpa_async_alloc() may be retrieved using
+ * cvmx_fpa_async_alloc_finish().
+ *
+ * @param scr_addr Local scratch address to put response in.  This is a byte
+ *		   address but must be 8 byte aligned.
+ * @param aura     Global aura to get the block from
+ */
+static inline void
+cvmx_fpa3_async_alloc(uint64_t scr_addr, cvmx_fpa3_gaura_t aura)
 {
-	return cvmx_fpa3_alloc_aura(gaura >> 10, gaura & 0x3ff);
+	cvmx_fpa3_iobdma_data_t data;
+
+	/* Hardware only uses 64 bit aligned locations, so convert from byte
+	 * address to 64-bit index
+	 */
+	data.u64 = 0ull;
+	data.cn78xx.scraddr = scr_addr >> 3;
+	data.cn78xx.len = 1;
+	data.cn78xx.did = 0x29;
+	data.cn78xx.node = aura.node;
+	data.cn78xx.aura = aura.laura;
+	cvmx_scratch_write64(scr_addr, 0ull);
+
+	CVMX_SYNCW;
+	cvmx_send_single(data.u64);
 }
 
 /**
- * Free a pointer back to the aura.
+ * Retrieve the result of cvmx_fpa3_async_alloc
+ * @INTERNAL
+ *
+ * @param scr_addr The Local scratch address.  Must be the same value
+ * passed to cvmx_fpa_async_alloc().
  *
- * @param node   node number
- * @param aura   aura number
+ * @param aura Global aura the block came from.  Must be the same value
+ * passed to cvmx_fpa_async_alloc.
+ *
+ * @return Pointer to the block or NULL on failure
+ */
+static inline void *
+cvmx_fpa3_async_alloc_finish(uint64_t scr_addr, cvmx_fpa3_gaura_t aura)
+{
+	uint64_t address;
+
+	CVMX_SYNCIOBDMA;
+
+	address = cvmx_scratch_read64(scr_addr);
+	if (cvmx_likely(address))
+		return cvmx_phys_to_ptr(address);
+	else
+		/* Try regular alloc if async failed */
+		return cvmx_fpa3_alloc(aura);
+}
+
+/**
+ * Free a pointer back to the pool.
+ * @INTERNAL
+ *
+ * @param aura   global aura number
  * @param ptr    physical address of block to free.
  * @param num_cache_lines Cache lines to invalidate
  */
-static inline void cvmx_fpa3_free_aura(void *ptr, uint64_t node, int aura,
-				       unsigned int num_cache_lines)
+static inline void cvmx_fpa3_free(void *ptr, cvmx_fpa3_gaura_t aura,
+				  unsigned int num_cache_lines)
 {
 	cvmx_fpa3_store_addr_t newptr;
 	cvmx_addr_t newdata;
 
+	if (CVMX_ENABLE_PARAMETER_CHECKING) {
+		if (!__cvmx_fpa3_aura_valid(aura))
+			return;
+	}
+
 	newdata.u64 = cvmx_ptr_to_phys(ptr);
 
+	/* Make sure that any previous writes to memory go out before we free
+	   this buffer. This also serves as a barrier to prevent GCC from
+	   reordering operations to after the free. */
+	CVMX_SYNCWS;
+
 	newptr.u64 = 0;
 	newptr.seg = CVMX_MIPS_SPACE_XKPHYS;
 	newptr.io = 1;
 	newptr.did = 0x29;    /* Device id, indicates FPA */
-	newptr.node = node;   /* OCI node number. */
-	newptr.aura = aura;   /* Aura number */
+	newptr.node = aura.node;
+	newptr.aura = aura.laura;
 	newptr.fabs = 0;	/* Free absolute. FIXME to use config option */
 	newptr.dwb_count = num_cache_lines;
 
-	/*cvmx_dprintf("aura = %d ptr_to_phys(ptr) = 0x%llx newptr.u64 = 0x%llx"
-		     " ptr = %p \n", ptr, aura, (ULL) newptr.u64
-		     (ULL) cvmx_ptr_to_phys(ptr)); */
-	/* Make sure that any previous writes to memory go out before we free
-	   this buffer. This also serves as a barrier to prevent GCC from
-	   reordering operations to after the free. */
-	CVMX_SYNCWS;
-        cvmx_write_io(newptr.u64, newdata.u64);
-}
-
-static inline void cvmx_fpa3_free_gaura(void *ptr, unsigned int gaura,
-				       unsigned int num_cache_lines)
-{
-	cvmx_fpa3_free_aura(ptr, gaura >> 10, gaura & 0x3ff, num_cache_lines);
+	cvmx_write_io(newptr.u64, newdata.u64);
 }
 
 /**
- * Free a pointer back to the aura, without flushing the write buffer
+ * Free a pointer back to the pool without flushing the write buffer.
+ * @INTERNAL
  *
- * @param node   node number
- * @param aura   aura number
+ * @param aura   global aura number
  * @param ptr    physical address of block to free.
  * @param num_cache_lines Cache lines to invalidate
  */
-static inline void cvmx_fpa3_free_aura_nosync(void *ptr, uint64_t node, int aura,
-					      uint64_t num_cache_lines)
+static inline void cvmx_fpa3_free_nosync(void *ptr, cvmx_fpa3_gaura_t aura,
+				  unsigned int num_cache_lines)
 {
 	cvmx_fpa3_store_addr_t newptr;
 	cvmx_addr_t newdata;
 
+	if (CVMX_ENABLE_PARAMETER_CHECKING) {
+		if (!__cvmx_fpa3_aura_valid(aura))
+			return;
+	}
+
 	newdata.u64 = cvmx_ptr_to_phys(ptr);
 
+	/* Prevent GCC from reordering writes to (*ptr) */
+	asm volatile ("" : : : "memory");
+
 	newptr.u64 = 0;
 	newptr.seg = CVMX_MIPS_SPACE_XKPHYS;
 	newptr.io = 1;
 	newptr.did = 0x29;    /* Device id, indicates FPA */
-	newptr.node = node;   /* OCI node number. */
-	newptr.aura = aura;   /* Aura number */
+	newptr.node = aura.node;
+	newptr.aura = aura.laura;
 	newptr.fabs = 0;	/* Free absolute. FIXME to use config option */
 	newptr.dwb_count = num_cache_lines;
-	/* Prevent GCC from reordering around free */
-	asm volatile ("":::"memory");
+
 	cvmx_write_io(newptr.u64, newdata.u64);
 }
 
-static inline void __cvmx_fpa_aura_cfg(int node, int aura, int pool,
-				       int buffers_cnt, int ptr_dis)
+static inline int cvmx_fpa3_pool_is_enabled(cvmx_fpa3_pool_t pool)
+{
+	cvmx_fpa_poolx_cfg_t pool_cfg;
+	if (!__cvmx_fpa3_pool_valid(pool))
+		return -1;
+
+	pool_cfg.u64 = cvmx_read_csr_node(
+		pool.node, CVMX_FPA_POOLX_CFG(pool.lpool));
+	return pool_cfg.cn78xx.ena;
+}
+
+static inline int cvmx_fpa3_config_red_params(unsigned node, int qos_avg_en, 
+					      int red_lvl_dly, int avg_dly)
 {
-       cvmx_fpa_aurax_cfg_t aura_cfg;
-       uint64_t pool64 = pool;
+	cvmx_fpa_gen_cfg_t fpa_cfg;
+	cvmx_fpa_red_delay_t red_delay;
 
-       aura_cfg.u64 = cvmx_read_csr_node(node, CVMX_FPA_AURAX_CFG(aura));
-       aura_cfg.s.ptr_dis = ptr_dis;
-        /* Configure CVMX_FPA_AURAX_CNT_LEVELS, CVMX_FPA_AURAX_POOL_LEVELS  */
-       cvmx_write_csr_node(node, CVMX_FPA_AURAX_CFG(aura), aura_cfg.u64);
-       cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT_ADD(aura), buffers_cnt);
-       cvmx_write_csr_node(node, CVMX_FPA_AURAX_POOL(aura), pool64);
+	fpa_cfg.u64 = cvmx_read_csr_node(node, CVMX_FPA_GEN_CFG);
+	fpa_cfg.s.avg_en = qos_avg_en;
+	fpa_cfg.s.lvl_dly = red_lvl_dly;
+	cvmx_write_csr_node(node, CVMX_FPA_GEN_CFG, fpa_cfg.u64);
 
-       /* TODO : config back pressure, RED */
+	red_delay.u64 = cvmx_read_csr_node(node, CVMX_FPA_RED_DELAY);
+	red_delay.s.avg_dly = avg_dly;
+	cvmx_write_csr_node(node, CVMX_FPA_RED_DELAY, red_delay.u64);
+	return 0;
 }
 
-/**
- * This function sets up QOS related parameter for specified Aura.
- *
- * @param node       node number.
- * @param aura       aura to configure.
- * @param ena_red       enable RED based on [DROP] and [PASS] levels
-                        1: enable 0:disable
- * @param pass_thresh   pass threshold for RED.
- * @param drop_thresh   drop threshold for RED
- * @param ena_bp        enable backpressure based on [BP] level.
-                        1:enable 0:disable
- * @param bp_thresh     backpressure threshold.
- *
- */
-static inline void cvmx_fpa_setup_aura_qos(int node, int aura, bool ena_red,
-					   uint64_t pass_thresh,
-					   uint64_t drop_thresh,
-					   bool ena_bp,uint64_t bp_thresh)
+static inline void cvmx_fpa3_setup_aura_qos(cvmx_fpa3_gaura_t aura,
+			bool ena_red,
+			uint64_t pass_thresh,
+			uint64_t drop_thresh,
+			bool ena_bp, uint64_t bp_thresh)
 {
-	uint64_t shift=0;
+	uint64_t shift = 0;
 	uint64_t shift_thresh;
 	cvmx_fpa_aurax_cnt_levels_t aura_level;
 
-	shift_thresh = bp_thresh > drop_thresh ? bp_thresh:drop_thresh;
+	if (!__cvmx_fpa3_aura_valid(aura))
+		return;
 
-	while ( (shift_thresh & (uint64_t)(~(0xff)))) {
+	shift_thresh = (bp_thresh > drop_thresh) ? bp_thresh : drop_thresh;
+
+	while ((shift_thresh & (uint64_t)(~(0xff)))) {
 		shift_thresh = shift_thresh >> 1;
 		shift++;
 	};
 
-	aura_level.u64 = cvmx_read_csr_node(node,CVMX_FPA_AURAX_CNT_LEVELS(aura));
+	aura_level.u64 = cvmx_read_csr_node(aura.node,
+				    CVMX_FPA_AURAX_CNT_LEVELS(aura.laura));
 	aura_level.s.pass = pass_thresh >> shift;
 	aura_level.s.drop = drop_thresh >> shift;
 	aura_level.s.bp = bp_thresh >> shift;
 	aura_level.s.shift = shift;
 	aura_level.s.red_ena = ena_red;
 	aura_level.s.bp_ena = ena_bp;
-	cvmx_write_csr_node(node,CVMX_FPA_AURAX_CNT_LEVELS(aura),aura_level.u64);
+	cvmx_write_csr_node(aura.node, CVMX_FPA_AURAX_CNT_LEVELS(aura.laura),
+			    aura_level.u64);
 }
 
 /**
- * This call will initialise the stack of the specified pool. Only the stack
- * memory which is the memory that holds the buffer pointers is allocated.
- * For now assume that natural alignment is used. When using natural alignment
- * the hardware needs to be initialised with the buffer size and hence it is
- * specified at the time of pool initialisation. The value will be buffered and
- * used later during cvmx_fpa_aura_init() to allocate buffers.
- * Before invoking this call the application should already have ownership of
- * the pool, the ownership is obtained when pool is one of the values in the
- * pools_allocated array after invocation of cvmx_allocate_fpa_pool()
- * The linux device driver chooses to not use this call as it would initialise
- * the FPA pool with kernel memory as opposed to using bootmem.
- * @param node     - specifies the node of FPA pool.
- * @parma pool     - Specifies the FPA pool number.
- * @param name     - Specifies the FPA pool name.
- * @param mem_node - specifies the node from which the memory for the stack
- *                   is allocated.
- * @param max_buffer_cnt - specifies the maximum buffers that FPA pool can hold.
- * @parm  align          - specifies the alignment type.
- * @param buffer_sz      - Only when the alignment is natural this field is used
- *                         to specify the size of each buffer in the FPA .
+ * Get the FPA3 POOL underneath FPA3 AURA, containing all its buffers
  *
+ * @INTERNAL
  */
-int cvmx_fpa3_pool_stack_init(int node, int pool, const char *name, int mem_node,
-			     int max_buffer_cnt, enum fpa_pool_alignment align,
-			     int buffer_sz);
-
-/**
- * This call will allocate buffers_cnt number of buffers from  the bootmemory
- * of bootmem_node and populate the aura specified with the allocated buffers.
- * The size of the buffers is obtained from the buffer_sz used to initialise the
- * stack of the FPA pool associated with the aura. This also means that the aura
- * has already been mapped to an FPA pool. More parameters is possible to
- * specify RED and buffer level parameters. This is another that would not be
- * used by the Linux device driver, the driver would populate the buffers
- * in the pool using it's own allocation mechanism.
- * @param node     - specifies the node of aura to be initialized
- * @parma aura     - specifies the aura to be initalized.
- * @param name     - specifies the name of aura to be initalized.
- * @param mem_node - specifies the node from which the memory for the buffers
- *                   is allocated.
- * @param buffers  - Block of memory to use for the aura buffers. If NULL,
- *                   aura memory is allocated.
- * @param ptr_dis - Need to look into this more but is on the lines of of
- *		    whether the hardware checks double frees.
- */
-int cvmx_fpa3_aura_init(int node, int aura, const char *name, int mem_node,
-		       void *buffers, int buffers_cnt, int ptr_dis);
-
-static inline int cvmx_fpa3_config_red_params(int node, int qos_avg_en, int red_lvl_dly,
-			       int avg_dly)
+static inline cvmx_fpa3_pool_t
+cvmx_fpa3_aura_to_pool(cvmx_fpa3_gaura_t aura)
 {
-	cvmx_fpa_gen_cfg_t fpa_cfg;
-	cvmx_fpa_red_delay_t red_delay;
+	cvmx_fpa3_pool_t pool;
+	cvmx_fpa_aurax_pool_t aurax_pool;
 
-	fpa_cfg.u64 = cvmx_read_csr_node(node,CVMX_FPA_GEN_CFG);
-	fpa_cfg.s.avg_en = qos_avg_en;
-	fpa_cfg.s.lvl_dly = red_lvl_dly;
-	cvmx_write_csr_node(node,CVMX_FPA_GEN_CFG,fpa_cfg.u64);
+	if (CVMX_ENABLE_PARAMETER_CHECKING) {
+		if (!__cvmx_fpa3_aura_valid(aura))
+			return CVMX_FPA3_INVALID_POOL;
+	}
 
-	red_delay.u64 = cvmx_read_csr_node(node,CVMX_FPA_RED_DELAY);
-	red_delay.s.avg_dly = avg_dly;
-	cvmx_write_csr_node(node,CVMX_FPA_RED_DELAY,red_delay.u64);
-	return 0;
+	aurax_pool.u64 = cvmx_read_csr_node(aura.node,
+		CVMX_FPA_AURAX_POOL(aura.laura));
+
+	pool = __cvmx_fpa3_pool(aura.node, aurax_pool.s.pool);
+	return pool;
 }
 
+
 /**
- * Gets the buffer size of the specified AURA,
- * which is a 12-bit quantity, with the upper 2 bits
- * representing the OCI node number, a.k.a. GAURA.
+ * Gets the buffer size of the specified pool,
  *
- * @param gaura Global aura number
- * @return Returns size of the buffers in the specified aura.
+ * @param pool Global aura number
+ * @return Returns size of the buffers in the specified pool.
  */
-unsigned cvmx_fpa3_get_aura_buf_size(uint16_t gaura);
+static inline int cvmx_fpa3_get_aura_buf_size(cvmx_fpa3_gaura_t aura)
+{
+	cvmx_fpa3_pool_t pool;
+	cvmx_fpa_poolx_cfg_t pool_cfg;
+	int block_size;
+
+	pool = cvmx_fpa3_aura_to_pool(aura);
+
+	if (CVMX_ENABLE_PARAMETER_CHECKING) {
+		if (!__cvmx_fpa3_pool_valid(pool))
+			return -1;
+	}
+	pool_cfg.u64 = cvmx_read_csr_node(pool.node,
+		CVMX_FPA_POOLX_CFG(pool.lpool));
+	block_size = pool_cfg.cn78xx.buf_size << 7;
+	return block_size;
+}
 
 /**
- * This will map auras specified in the auras_index[] array to specified
- * FPA pool_index.
- * The array is assumed to contain count number of entries.
- * @param count is the number of entries in the auras_index array.
- * @pool_index is the index of the fpa pool.
- * @return 0 on success
+ * Return the number of available buffers in an AURA
+ *
+ * FIXME: Add <limit>-<count> limitation.
  */
-int cvmx_fpa3_assign_auras(int node, int auras_index[], int count,
-			  int pool_index);
-
-int cvmx_fpa3_assign_aura(int node, int aura, int pool_index);
+static inline long long cvmx_fpa3_get_available(cvmx_fpa3_gaura_t aura)
+{
+	cvmx_fpa3_pool_t pool;
+	cvmx_fpa_poolx_available_t avail_reg;
+	cvmx_fpa_aurax_cnt_t cnt_reg;
+	cvmx_fpa_aurax_cnt_limit_t limit_reg;
+	long long ret;
+
+	pool = cvmx_fpa3_aura_to_pool(aura);
+
+	if (CVMX_ENABLE_PARAMETER_CHECKING) {
+		if (!__cvmx_fpa3_pool_valid(pool))
+			return -1LL;
+	}
+
+	/* Get POOL available buffer count */
+	avail_reg.u64 = cvmx_read_csr_node(pool.node,
+		CVMX_FPA_POOLX_AVAILABLE(pool.lpool));
+
+	/* Get AURA current available count */
+	cnt_reg.u64 = cvmx_read_csr_node(aura.node,
+		CVMX_FPA_AURAX_CNT(aura.laura));
+	limit_reg.u64 = cvmx_read_csr_node(aura.node,
+		CVMX_FPA_AURAX_CNT_LIMIT(aura.laura));
+
+	/* Calculate AURA-based buffer allowance */
+	ret = limit_reg.cn78xx.limit -
+		cnt_reg.cn78xx.cnt;
+
+	/* Use POOL real buffer availability when less then allowance */
+	if (ret > (long long) avail_reg.cn78xx.count)
+		ret = avail_reg.cn78xx.count;
+
+	return ret;
+}
 
-static inline void cvmx_fpa3_disable_pool(int node, int pool_num)
+extern cvmx_fpa3_gaura_t cvmx_fpa3_reserve_aura(int node, int desired_aura_num);
+extern int cvmx_fpa3_release_aura(cvmx_fpa3_gaura_t aura);
+extern cvmx_fpa3_pool_t cvmx_fpa3_reserve_pool(int node, int desired_pool_num);
+extern int cvmx_fpa3_release_pool(cvmx_fpa3_pool_t pool);
+extern int cvmx_fpa3_is_aura_available(int node, int aura_num);
+extern int cvmx_fpa3_is_pool_available(int node, int pool_num);
+
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+extern cvmx_fpa3_pool_t cvmx_fpa3_setup_fill_pool(
+	int node, int desired_pool, const char *name,
+	unsigned block_size, unsigned num_blocks, void *buffer);
+
+extern cvmx_fpa3_gaura_t cvmx_fpa3_set_aura_for_pool
+	(cvmx_fpa3_pool_t pool, int desired_aura,
+	const char *name, unsigned block_size, unsigned num_blocks);
+
+extern cvmx_fpa3_gaura_t cvmx_fpa3_setup_aura_and_pool(
+		int node, int desired_aura,
+		const char *name, void *buffer,
+		unsigned block_size, unsigned num_blocks);
+
+extern int cvmx_fpa3_shutdown_aura_and_pool(cvmx_fpa3_gaura_t aura);
+extern int cvmx_fpa3_shutdown_aura(cvmx_fpa3_gaura_t aura);
+extern int cvmx_fpa3_shutdown_pool(cvmx_fpa3_pool_t pool);
+extern const char *cvmx_fpa3_get_pool_name(cvmx_fpa3_pool_t pool);
+extern const char *cvmx_fpa3_get_aura_name(cvmx_fpa3_gaura_t aura);
+
+#ifdef CVMX_BUILD_FOR_UBOOT
+/* FIXME: Need a different macro for stage2 of u-boot */
+
+static inline void
+cvmx_fpa3_stage2_init(int aura, int pool, uint64_t stack_paddr, int stacklen,
+			int buffer_sz, int buf_cnt)
 {
 	cvmx_fpa_poolx_cfg_t pool_cfg;
 
+	/* Configure pool stack */
+	cvmx_write_csr_node(0, CVMX_FPA_POOLX_STACK_BASE(pool), stack_paddr);
+	cvmx_write_csr_node(0, CVMX_FPA_POOLX_STACK_ADDR(pool), stack_paddr);
+	cvmx_write_csr_node(0, CVMX_FPA_POOLX_STACK_END(pool),
+		stack_paddr + stacklen);
+
+	/* Configure pool with buffer size */
 	pool_cfg.u64 = 0;
+	pool_cfg.cn78xx.nat_align = 1;
+	pool_cfg.cn78xx.buf_size = buffer_sz >> 7;
+	pool_cfg.cn78xx.l_type = 0x2;
 	pool_cfg.cn78xx.ena = 0;
+	cvmx_write_csr_node(0, CVMX_FPA_POOLX_CFG(pool), pool_cfg.u64);
+	/* Reset pool before starting */
+	pool_cfg.cn78xx.ena = 1;
+	cvmx_write_csr_node(0, CVMX_FPA_POOLX_CFG(pool), pool_cfg.u64);
+
+	cvmx_write_csr_node(0, CVMX_FPA_AURAX_CFG(aura), 0);
+	cvmx_write_csr_node(0, CVMX_FPA_AURAX_CNT_ADD(aura), buf_cnt);
+	cvmx_write_csr_node(0, CVMX_FPA_AURAX_POOL(aura), (uint64_t) pool);
+}
 
-	cvmx_write_csr_node(node, CVMX_FPA_POOLX_CFG(pool_num), pool_cfg.u64);
-	cvmx_write_csr_node(node, CVMX_FPA_POOLX_STACK_BASE(pool_num), 0);
-	cvmx_write_csr_node(node, CVMX_FPA_POOLX_STACK_ADDR(pool_num), 0);
-	cvmx_write_csr_node(node, CVMX_FPA_POOLX_STACK_END(pool_num), 0);
+static inline void
+cvmx_fpa3_stage2_disable(int aura, int pool)
+{
+	cvmx_write_csr_node(0, CVMX_FPA_AURAX_POOL(aura), 0);
+	cvmx_write_csr_node(0, CVMX_FPA_POOLX_CFG(pool), 0);
+	cvmx_write_csr_node(0, CVMX_FPA_POOLX_STACK_BASE(pool), 0);
+	cvmx_write_csr_node(0, CVMX_FPA_POOLX_STACK_ADDR(pool), 0);
+	cvmx_write_csr_node(0, CVMX_FPA_POOLX_STACK_END(pool), 0);
 }
+#endif /* CVMX_BUILD_FOR_UBOOT */
+#endif	/* !CVMX_BUILD_FOR_LINUX_KERNEL */
 
 #ifdef	__cplusplus
 /* *INDENT-OFF* */
@@ -412,4 +634,4 @@ static inline void cvmx_fpa3_disable_pool(int node, int pool_num)
 /* *INDENT-ON* */
 #endif
 
-#endif /*  __CVM_FPA3_H__ */
+#endif /* __CVMX_FPA3_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-global-resources.h b/arch/mips/include/asm/octeon/cvmx-global-resources.h
index c28e8dd..b730b1f 100644
--- a/arch/mips/include/asm/octeon/cvmx-global-resources.h
+++ b/arch/mips/include/asm/octeon/cvmx-global-resources.h
@@ -101,6 +101,7 @@ int cvmx_resource_alloc_many(struct global_resource_tag tag,
 			     uint64_t owner,
 			     int nelements,
 			     int allocated_elements[]);
+int cvmx_resource_alloc_reverse(struct global_resource_tag, uint64_t owner);
 /*
  * @INTERNAL
  * Reserve nelements starting from base in the global resource range with the
@@ -159,6 +160,8 @@ int cvmx_free_global_resource_range_with_owner(struct global_resource_tag tag,
  */
 int free_global_resources(void);
 
+uint64_t cvmx_get_global_resource_owner(struct global_resource_tag tag,
+					int base);
 /*
  * @INTERNAL
  * Shows the global resource range with the specified tag. Use mainly for debug.
@@ -171,6 +174,8 @@ void  cvmx_show_global_resource_range(struct global_resource_tag tag);
  */
 void cvmx_global_resources_show(void);
 
+uint64_t cvmx_allocate_app_id(void);
+uint64_t cvmx_get_app_id(void);
 
 #endif
 
diff --git a/arch/mips/include/asm/octeon/cvmx-helper-bgx.h b/arch/mips/include/asm/octeon/cvmx-helper-bgx.h
index e14cccc..85793fd 100644
--- a/arch/mips/include/asm/octeon/cvmx-helper-bgx.h
+++ b/arch/mips/include/asm/octeon/cvmx-helper-bgx.h
@@ -54,6 +54,14 @@ extern int __cvmx_helper_bgx_enumerate(int interface);
 
 /**
  * @INTERNAL
+ * Disable the BGX port
+ *
+ * @param  IPD port of the BGX interface to disable
+ */
+extern void cvmx_helper_bgx_disable(int xipd_port);
+
+/**
+ * @INTERNAL
  * Probe a SGMII interface and determine the number of ports
  * connected to it. The SGMII/XAUI interface should still be down after
  * this call. This is used by interfaces using the bgx mac.
@@ -236,4 +244,16 @@ extern void cvmx_helper_bgx_tx_options(unsigned node,
 	unsigned interface, unsigned index,
 	bool fcs_enable, bool pad_enable);
 
+/**
+ * Set mac for the ipd_port
+ *
+ * @param xipd_port ipd_port to set the mac
+ * @param bcst      If set, accept all broadcast packets
+ * @param mcst      Multicast mode
+ * 		    0 = Force reject all multicast packets
+ * 		    1 = Force accept all multicast packets
+ * 		    2 = use the address filter CAM.
+ * @param mac       mac address for the ipd_port		    
+ */ 		    
+extern void cvmx_helper_bgx_set_mac(int xipd_port, int bcst, int mcst, uint64_t mac);
 #endif
diff --git a/arch/mips/include/asm/octeon/cvmx-helper-cfg.h b/arch/mips/include/asm/octeon/cvmx-helper-cfg.h
index 39db403..0b636c9 100644
--- a/arch/mips/include/asm/octeon/cvmx-helper-cfg.h
+++ b/arch/mips/include/asm/octeon/cvmx-helper-cfg.h
@@ -119,6 +119,8 @@ enum cvmx_helper_cfg_option {
 };
 typedef enum cvmx_helper_cfg_option cvmx_helper_cfg_option_t;
 
+struct cvmx_phy_info;
+
 /*
  * Per physical port
  */
@@ -127,6 +129,7 @@ struct cvmx_cfg_port_param {
 	int port_fdt_node;		/** Node offset in FDT of node */
 	int phy_fdt_node;		/** Node offset in FDT of PHY */
 #endif
+	struct cvmx_phy_info *phy_info;	/** Data structure with PHY information */
 	int8_t ccpp_pknd;
 	int8_t ccpp_bpid;
 	int8_t ccpp_pko_port_base;
@@ -408,6 +411,7 @@ extern void cvmx_helper_cfg_store_short_packets_in_wqe(void);
  int cvmx_pko_alloc_iport_and_queues(int interface, int port, int port_cnt,
 				     int queue_cnt);
 
+#if 0
 /*
  * Allocated a block of queues for the specified port.
  *
@@ -418,6 +422,7 @@ extern void cvmx_helper_cfg_store_short_packets_in_wqe(void);
  *         -1 on failure
  */
 int cvmx_pko_queue_alloc(uint64_t port, uint64_t count);
+#endif
 
 /*
  * Free the queues that are associated with the specified port
@@ -575,7 +580,7 @@ extern void cvmx_helper_set_port_autonegotiation(int xiface, int index,
 						 bool enable);
 
 /**
- * INTERNAL
+ * @INTERNAL
  * Returns if autonegotiation is enabled or not.
  *
  * @param xiface	node and interface
@@ -585,6 +590,27 @@ extern void cvmx_helper_set_port_autonegotiation(int xiface, int index,
  */
 extern bool cvmx_helper_get_port_autonegotiation(int xiface, int index);
 
+/**
+ * @INTERNAL
+ * Sets the PHY info data structure
+ *
+ * @param xiface	node and interface
+ * @param index		port index
+ * @param[in] phy_info	phy information data structure pointer
+ */
+extern void cvmx_helper_set_port_phy_info(int xiface, int index,
+					  struct cvmx_phy_info *phy_info);
+/**
+ * @INTERNAL
+ * Returns the PHY information data structure for a port
+ *
+ * @param xiface	node and interface
+ * @param index		port index
+ *
+ * @return pointer to PHY information data structure or NULL if not set
+ */
+extern struct cvmx_phy_info *cvmx_helper_get_port_phy_info(int xiface, int index);
+
 /*
  * Initializes cvmx with user specified config info.
  */
diff --git a/arch/mips/include/asm/octeon/cvmx-helper-fpa.h b/arch/mips/include/asm/octeon/cvmx-helper-fpa.h
index 98cbda8..b29757e 100644
--- a/arch/mips/include/asm/octeon/cvmx-helper-fpa.h
+++ b/arch/mips/include/asm/octeon/cvmx-helper-fpa.h
@@ -1,5 +1,5 @@
 /***********************license start***************
- * Copyright (c) 2003-2010  Cavium Inc. (support@cavium.com). All rights
+ * Copyright (c) 2003-2014  Cavium Inc. (support@cavium.com). All rights
  * reserved.
  *
  *
@@ -80,72 +80,8 @@ extern int __cvmx_helper_initialize_fpa_pool(int pool, uint64_t buffer_size,
 					     uint64_t buffers,
 					     const char *name);
 
-/**
- * Function to create a simple 1:1 pool/aura configuration.
- * Meant to make pool/aura initialization easy for SE apps.
- *
- * @param node - configure fpa on this node
- * @param pool_num - pointer to pool to use, set to -1 to allocate
- * @param aura_id - pointer to aura to use, set to -1 to allocate
- * @param block_size - size of aura buffers to use
- * @param num_blocks - number of buffers to allocate
- * @param name - name to assign pool
- * @param buffers - pointer to buffer memory to use, NULL to allocate
- *
- * @return -1 on error, 0 on success with buffers containing allocated memory if passed NULL
- */
-
-extern int cvmx_helper_fpa_init(int node, int *pool_num, int *aura_id, int block_size,
-				int num_blocks, const char *name, void *buffers);
-
-/**
- * Function to setup and initialize a pool.
- *
- * @param node - configure fpa on this node
- * @param mem_node - if memory should be allocated from a different node
- * @param pool_num - pointer to pool to use, or -1 to allocate
- * @param block_size - size of buffers in pool
- * @param num_blocks - max number of buffers allowed
- * @param name - name to register
- */
-
-extern int cvmx_helper_fpa3_init_pool(int node, int mem_node, int *pool_num, int block_size, int num_blocks, const char *name);
-
-/**
- * Function to add an aura to an existing pool
- *
- * @param node - configure fpa on this node
- * @param pool_num - pool to attach aura to
- * @param aura_id - pointer to aura to use, set to -1 to allocate
- * @param block_size - size of buffers to use
- * @param num_blocks - number of blocks to allocate
- * @param buffer - pointer to buffer memory to use, NULL to allocate
- *
- * @return -1 on error, 0 on success with buffers containing allocated memory if passed NULL
- */
-
-extern int cvmx_helper_fpa3_add_aura_to_pool(int node, int pool_num, int *aura_id, int num_blocks, void *buffer, const char *name);
-
-/**
- * Function to fill a pre-78xx fpa pool with memory
- *
- * @param node - configure fpa on this node
- * @param mem_node - if memory should be allocated from a different node
- * @param pool_num - pool to fill
- * @param num_blocks - number of blocks to add (of pool's block_size)
- * @param buffer - buffer to add blocks from
- */
-
-extern int cvmx_helper_fpa_fill_pool(int pool_num, int num_blocks, void *buffer);
-
-extern int cvmx_helper_fpa3_shutdown_pool(int node, int pool_num);
-
-extern int cvmx_helper_fpa3_shutdown_aura(int node, int aura_id);
-
-extern int cvmx_helper_fpa3_shutdown_aura_and_pool(int node, int aura_id);
-
 extern int cvmx_helper_shutdown_fpa_pools(int node);
 
-extern int cvmx_helper_fpa_pool_init(int node, int pool_num, int block_size, int max_blocks, const char *name);
+extern void cvmx_helper_fpa_dump(int node);
 
 #endif /* __CVMX_HELPER_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-helper-pki.h b/arch/mips/include/asm/octeon/cvmx-helper-pki.h
index e33d01a..e1278da 100644
--- a/arch/mips/include/asm/octeon/cvmx-helper-pki.h
+++ b/arch/mips/include/asm/octeon/cvmx-helper-pki.h
@@ -65,101 +65,141 @@ extern "C" {
 #define CVMX_PKI_FIND_AVAILABLE_RSRC    (-1)
 
 struct cvmx_pki_qos_schd {
+	cvmx_fpa3_gaura_t _aura;	/* FPA3 AURA handle */
+	cvmx_fpa3_pool_t _pool;	/* FPA3 POOL handle */
 	bool pool_per_qos;	/* This qos priority will use its own pool, if FALSE use port pool */
-        int pool;		/* pool number to use, if -1 allocated by sdk otherwise software should alloc it */
+	int pool_num;		/* pool number to use, if -1 allocated by sdk otherwise software should alloc it */
 	char *pool_name;
 	uint64_t pool_buff_size;/* size of buffer in pool , if this priority is using its own pool*/
-                                /* it's good to have same buffer size if qos are using separate pools */
+				/* it's good to have same buffer size if qos are using separate pools */
 	uint64_t pool_max_buff;	/* number of max buffers allowed in the pool, if this priority is using its own pool*/
 	bool aura_per_qos;	/* This qos priority will use its own aura, if FALSE use port aura */
-        int aura;		/* aura number to use, if -1 allocated by sdk otherwise software should alloc it */
+	int aura_num;		/* aura number to use, if -1 allocated by sdk otherwise software should alloc it */
 	char *aura_name;
 	uint64_t aura_buff_cnt;	/* number of buffers in aura, if this priority is using its own aura*/
 	bool sso_grp_per_qos;	/* This qos priority will use its own group, if FALSE use port group */
-        int sso_grp;		/* group number to use, if -1 allocated by sdk otherwise software should alloc it */
+	int sso_grp;		/* group number to use, if -1 allocated by sdk otherwise software should alloc it */
 	uint16_t port_add;      /* for BGX super MAC ports which wants to have PFC enabled */
-        int qpg_base;           /* offset in qpg table to use, if -1 allocated by sdk otherwise software should alloc it*/
+	int qpg_base;           /* offset in qpg table to use, if -1 allocated by sdk otherwise software should alloc it*/
 };
 
 struct cvmx_pki_prt_schd {
-        bool cfg_port;          /* Set to 1 if this port on the interface is not used */
+	cvmx_fpa3_pool_t _pool;	/* FPA3 POOL handle */
+	cvmx_fpa3_gaura_t _aura;/* FPA3 AURA handle */
+	bool cfg_port;          /* Set to 1 if this port on the interface is not used */
 	int style;              /* If style_per_prt is TRUE in interface schd */
 	bool pool_per_prt; 	/* Port will use its own pool, if FALSE use interface pool */
-	int pool;		/* pool number to use, if -1 allocated by sdk otherwise software should alloc it*/
+	int pool_num;		/* pool number to use, if -1 allocated by sdk otherwise software should alloc it*/
 	char *pool_name;
 	uint64_t pool_buff_size;/*size of buffer in pool , if this port is using its own pool*/
-                                /* it's good to have same buffer size if ports are using same style but different pools*/
+			/* it's good to have same buffer size if ports are using same style but different pools*/
 	uint64_t pool_max_buff;	/* number of max buffers allowed in the pool, if this port is using its own pool*/
 	bool aura_per_prt;	/* port will use its own aura, if FALSE use interface aura */
-        int aura;		/* aura number to use, if -1 allocated by sdk otherwise software should alloc it */
+	int aura_num;		/* aura number to use, if -1 allocated by sdk otherwise software should alloc it */
 	char *aura_name;
 	uint64_t aura_buff_cnt;	/* number of buffers in aura, if this pool is using its own aura*/
 	bool sso_grp_per_prt; 	/* port will use its own sso group, if FALSE use interface group*/
-        int sso_grp;		/* sso group number to use, if -1 allocated by sdk otherwise software should alloc it */
+	int sso_grp;		/* sso group number to use, if -1 allocated by sdk otherwise software should alloc it */
 	enum cvmx_pki_qpg_qos qpg_qos;
-        int qpg_base;           /* offset in qpg table to use, if -1 allocated by sdk otherwise software should alloc it*/
+	int qpg_base;           /* offset in qpg table to use, if -1 allocated by sdk otherwise software should alloc it*/
 	struct cvmx_pki_qos_schd qos_s[CVMX_MAX_QOS_PRIORITY];
 };
 
 struct cvmx_pki_intf_schd {
+	cvmx_fpa3_pool_t _pool;	/* FPA3 POOL handle */
+	cvmx_fpa3_gaura_t _aura;/* FPA3 AURA handle */
 	bool style_per_prt;	/* Every port will use different style/profile */
 	bool style_per_intf;	/* otherwise all ports on this interface will use same style/profile */
-        int style;              /* style number to use, if -1 allocated by sdk otherwise software should alloc it*/
+	int style;              /* style number to use, if -1 allocated by sdk otherwise software should alloc it*/
 	bool pool_per_intf; 	/* Ports will use either this shared pool or their own pool*/
-        int pool;		/* pool number to use, if -1 allocated by sdk otherwise software should alloc it*/
+	int pool_num;		/* pool number to use, if -1 allocated by sdk otherwise software should alloc it*/
 	char *pool_name;
 	uint64_t pool_buff_size;
 	uint64_t pool_max_buff;
 	bool aura_per_intf; 	/* Ports will use either this shared aura or their own aura */
-        int aura;		/* aura number to use, if -1 allocated by sdk otherwise software should alloc it*/
+	int aura_num;		/* aura number to use, if -1 allocated by sdk otherwise software should alloc it*/
 	char *aura_name;
 	uint64_t aura_buff_cnt;
 	bool sso_grp_per_intf;	/* Ports will use either this shared group or their own aura */
-        int sso_grp;		/* sso group number to use, if -1 allocated by sdk otherwise software should alloc it */
+	int sso_grp;		/* sso group number to use, if -1 allocated by sdk otherwise software should alloc it */
 	bool qos_share_aura;	/* All ports share the same aura for respective qos if qpg_qos used*/
 	bool qos_share_grp; 	/* All ports share the same sso group for respective qos if qps qos used*/
-        int qpg_base;           /* offset in qpg table to use, if -1 allocated by sdk otherwise software should alloc it*/
+	int qpg_base;           /* offset in qpg table to use, if -1 allocated by sdk otherwise software should alloc it*/
 	struct cvmx_pki_prt_schd prt_s[CVMX_MAX_PORT_PER_INTERFACE];
 };
 
 struct cvmx_pki_global_schd {
 	bool setup_pool;
-        int pool;              /* pool number to use, if -1 allocated by sdk otherwise software should alloc it*/
+	int pool_num;              /* pool number to use, if -1 allocated by sdk otherwise software should alloc it*/
 	char *pool_name;
 	uint64_t pool_buff_size;
 	uint64_t pool_max_buff;
 	bool setup_aura;
-        int aura;              /* aura number to use, if -1 allocated by sdk otherwise software should alloc it*/
+	int aura_num;              /* aura number to use, if -1 allocated by sdk otherwise software should alloc it*/
 	char *aura_name;
 	uint64_t aura_buff_cnt;
 	bool setup_sso_grp;
-        int sso_grp;            /* sso group number to use, if -1 allocated by sdk otherwise software should alloc it */
+	int sso_grp;            /* sso group number to use, if -1 allocated by sdk otherwise software should alloc it */
+	cvmx_fpa3_pool_t _pool;
+	cvmx_fpa3_gaura_t _aura;
+};
+
+struct cvmx_pki_legacy_qos_watcher {
+	bool configured;
+	enum cvmx_pki_term field;
+	uint32_t data;
+	uint32_t data_mask;
+	uint8_t advance;
+	uint8_t sso_grp;
 };
 
 extern CVMX_SHARED bool cvmx_pki_dflt_init[CVMX_MAX_NODES];
 
 extern CVMX_SHARED struct cvmx_pki_pool_config pki_dflt_pool[CVMX_MAX_NODES];
 extern CVMX_SHARED struct cvmx_pki_aura_config pki_dflt_aura[CVMX_MAX_NODES];
-extern uint64_t style_qpg_base_map[CVMX_MAX_NODES];
 extern CVMX_SHARED struct cvmx_pki_style_config pki_dflt_style[CVMX_MAX_NODES];
 extern CVMX_SHARED struct cvmx_pki_pkind_config pki_dflt_pkind[CVMX_MAX_NODES];
 extern CVMX_SHARED uint64_t pkind_style_map[CVMX_MAX_NODES][CVMX_PKI_NUM_PKIND];
+extern CVMX_SHARED struct cvmx_pki_sso_grp_config pki_dflt_sso_grp[CVMX_MAX_NODES];
+extern CVMX_SHARED struct cvmx_pki_legacy_qos_watcher qos_watcher[8];
 
+/**
+ * This function Enabled the PKI hardware to
+ * start accepting/processing packets.
+ * @param node    node number
+ */
 void cvmx_helper_pki_enable(int node);
-int cvmx_helper_setup_pki_port(int node, int pknd);
-int cvmx_helper_pki_setup_qpg_table(int node, int num_entries,
-				    struct cvmx_pki_qpg_config *qpg_cfg);
-void cvmx_helper_pki_set_fcs_op(int node, int interface, int nports, int has_fcs);
+
+/**
+ * This function frees up PKI resources consumed by that port.
+ * This function should only be called if port resources
+ * (fpa pools aura, style qpg entry pcam entry etc.) are not shared
+ * @param ipd_port      ipd port number for which resources need to
+ *                      be freed.
+ */
+int cvmx_helper_pki_port_shutdown(int ipd_port);
+
+/**
+ * This function shuts down complete PKI hardware
+ * and software resources.
+ * @param node          node number where PKI needs to shutdown.
+ */
+void cvmx_helper_pki_shutdown(int node);
+
+/**
+ * This function calculates how mant qpf entries will be needed for
+ * a particular QOS.
+ * @param qpg_qos       qos value for which entries need to be calculated.
+ */
 int cvmx_helper_pki_get_num_qpg_entry(enum cvmx_pki_qpg_qos qpg_qos);
-int __cvmx_helper_pki_port_setup(int node, int ipd_port);
-int __cvmx_helper_pki_global_setup(int node);
-int __cvmx_helper_pki_install_default_vlan(int node);
-void cvmx_helper_pki_set_dflt_pool(int node, int pool,
-				   int buffer_size, int buffer_count);
-void cvmx_helper_pki_set_dflt_aura(int node, int aura,
-				   int pool, int buffer_count);
-void cvmx_helper_pki_set_dflt_pool_buffer(int node, int buffer_count);
-void cvmx_helper_pki_set_dflt_aura_buffer(int node, int buffer_count);
+
+/**
+ * This function setups the qos table by allocating qpg entry and writing
+ * the provided parameters to that entry (offset).
+ * @param node          node number.
+ * @param qpg_cfg       pointer to struct containing qpg configuration
+ */
+int cvmx_helper_pki_set_qpg_entry(int node, struct cvmx_pki_qpg_config *qpg_cfg);
 
 /**
  * This function sets up aura QOS for RED, backpressure and tail-drop.
@@ -174,12 +214,13 @@ void cvmx_helper_pki_set_dflt_aura_buffer(int node, int buffer_count);
  *			1:enable 0:disable
  * @param bp_thresh     backpressure threshold.
  * @param ena_drop      enable tail drop.
- *                      1:enable 0:disable
+ *			1:enable 0:disable
  * @return Zero on success. Negative on failure
  */
 int cvmx_helper_setup_aura_qos(int node, int aura, bool ena_red, bool ena_drop,
-			       uint64_t pass_thresh, uint64_t drop_thresh,
-			       bool ena_bp, uint64_t bp_thresh);
+	uint64_t pass_thresh, uint64_t drop_thresh,
+	bool ena_bp, uint64_t bp_thresh);
+
 /**
  * This function maps specified bpid to all the auras from which it can receive bp and
  * then maps that bpid to all the channels, that bpid can asserrt bp on.
@@ -192,26 +233,83 @@ int cvmx_helper_setup_aura_qos(int node, int aura, bool ena_red, bool ena_drop,
  * @return Zero on success. Negative on failure
  */
 int cvmx_helper_pki_map_aura_chl_bpid(int node, uint16_t aura, uint16_t bpid,
-                                      uint16_t chl_map[], uint16_t chl_cnt);
+		uint16_t chl_map[], uint16_t chl_cnt);
 
-void cvmx_helper_pki_set_dflt_pkind_map(int node, int pkind, int style);
-void cvmx_helper_pki_get_dflt_style(int node, struct cvmx_pki_style_config *style_cfg);
-void cvmx_helper_pki_set_dflt_style(int node, struct cvmx_pki_style_config *style_cfg);
+/**
+ * This function sets up the global pool, aura and sso group
+ * resources which application can use between any interfaces
+ * and ports.
+ * @param node          node number
+ * @param gblsch        pointer to struct containing global
+ *                      scheduling parameters.
+ */
+int cvmx_helper_pki_set_gbl_schd(int node, struct cvmx_pki_global_schd *gblsch);
 
+/**
+ * This function sets up scheduling parameters (pool, aura, sso group etc)
+ * of an ipd port.
+ * @param ipd_port      ipd port number
+ * @param prtsch        pointer to struct containing port's
+ *                      scheduling parameters.
+ */
+int cvmx_helper_pki_init_port(int ipd_port, struct cvmx_pki_prt_schd *prtsch);
 
-/* Shutdown complete PKI hardware and software resources */
-void cvmx_helper_pki_shutdown(int node);
+/**
+ * This function sets up scheduling parameters (pool, aura, sso group etc)
+ * of an interface (all ports/channels on that interface).
+ * @param xiface        interface number with node.
+ * @param intf_sch      pointer to struct containing interface
+ *                      scheduling parameters.
+ * @param gbl_sch       pointer to struct containing global scheduling parameters
+ *                      (can be NULL if not used)
+ */
+int cvmx_helper_pki_init_interface(const int xiface,
+	struct cvmx_pki_intf_schd *intfsch,
+	struct cvmx_pki_global_schd *gblsch);
+/**
+ * This function gets all the PKI parameters related to that
+ * particular port from hardware.
+ * @param ipd_port      ipd port number to get parameter of
+ * @param port_cfg      pointer to structure where to store read parameters
+ */
+void cvmx_pki_get_port_config(int ipd_port, struct cvmx_pki_port_config *port_cfg);
 
-int cvmx_helper_pki_set_gbl_schd(int node, struct cvmx_pki_global_schd *gbl_schd);
+/**
+ * This function sets all the PKI parameters related to that
+ * particular port in hardware.
+ * @param ipd_port      ipd port number to get parameter of
+ * @param port_cfg      pointer to structure containing port parameters
+ */
+void cvmx_pki_set_port_config(int ipd_port, struct cvmx_pki_port_config *port_cfg);
 
-int cvmx_helper_pki_init_interface(int , struct cvmx_pki_intf_schd *intf, struct cvmx_pki_global_schd *gbl_schd);
+/**
+ * This function displays all the PKI parameters related to that
+ * particular port.
+ * @param ipd_port      ipd port number to display parameter of
+ */
+void cvmx_pki_show_port_config(int ipd_port);
+
+/**
+ * Modifies maximum frame length to check.
+ * It modifies the global frame length set used by this port, any other
+ * port using the same set will get affected too.
+ * @param node		node number
+ * @param ipd_port	ipd port for which to modify max len.
+ * @param max_size	maximum frame length
+ */
+void cvmx_pki_set_max_frm_len(int ipd_port, uint32_t max_size);
+
+/**
+ * This function sets up all th eports of particular interface
+ * for chosen fcs mode. (only use for backward compatibility).
+ * New application can control it via init_interfcae calls.
+ * @param node          node number.
+ * @param interfcae     interfcae number.
+ * @param has_fcs       1 -- enable fcs check and fcs strip.
+ *                      0 -- disable fcs check.
+ */
+void cvmx_helper_pki_set_fcs_op(int node, int interface, int nports, int has_fcs);
 
-int cvmx_helper_pki_init_interface(int xiface,
-				   struct cvmx_pki_intf_schd *intf, struct cvmx_pki_global_schd *gbl_schd);
-int cvmx_helper_pki_init_port(int ipd_port, struct cvmx_pki_prt_schd *prtsch);
-void cvmx_helper_pki_no_dflt_init(int node);
-void cvmx_helper_pki_get_dflt_style(int node, struct cvmx_pki_style_config *style_cfg);
-void cvmx_helper_pki_set_dflt_style(int node, struct cvmx_pki_style_config *style_cfg);
 /**
  * This function sets the wqe buffer mode of all ports. First packet data buffer can reside
  * either in same buffer as wqe OR it can go in separate buffer. If used the later mode,
@@ -228,8 +326,40 @@ void cvmx_helper_pki_set_dflt_style(int node, struct cvmx_pki_style_config *styl
  *				    performance by up to half on small packets.
  */
 void cvmx_helper_pki_set_wqe_mode(int node, bool pkt_outside_wqe);
-void pki_wqe_dump(const cvmx_wqe_78xx_t* wqp);
-void __cvmx_helper_pki_set_ltype_map(int node);
+
+/**
+ * This function sets the Packet mode of all ports and styles to little-endian.
+ * It Changes write operations of packet data to L2C to
+ * be in little-endian. Does not change the WQE header format, which is
+ * properly endian neutral.
+ * @param node	                node number.
+ */
+void cvmx_helper_pki_set_little_endian(int node);
+
+void cvmx_helper_pki_set_dflt_pool(int node, int pool,
+				   int buffer_size, int buffer_count);
+void cvmx_helper_pki_set_dflt_aura(int node, int aura,
+				   int pool, int buffer_count);
+void cvmx_helper_pki_set_dflt_pool_buffer(int node, int buffer_count);
+
+void cvmx_helper_pki_set_dflt_aura_buffer(int node, int buffer_count);
+
+void cvmx_helper_pki_set_dflt_pkind_map(int node, int pkind, int style);
+
+void cvmx_helper_pki_get_dflt_style(int node, struct cvmx_pki_style_config *style_cfg);
+
+void cvmx_helper_pki_set_dflt_style(int node, struct cvmx_pki_style_config *style_cfg);
+
+void cvmx_helper_pki_no_dflt_init(int node);
+
+void cvmx_pki_dump_wqe(const cvmx_wqe_78xx_t *wqp);
+
+int __cvmx_helper_pki_port_setup(int node, int ipd_port);
+
+int __cvmx_helper_pki_global_setup(int node);
+
+int __cvmx_helper_pki_install_dflt_vlan(int node);
+void __cvmx_helper_pki_set_dflt_ltype_map(int node);
 
 #ifdef __cplusplus
 /* *INDENT-OFF* */
diff --git a/arch/mips/include/asm/octeon/cvmx-helper-util.h b/arch/mips/include/asm/octeon/cvmx-helper-util.h
index a857974..0b0e379 100644
--- a/arch/mips/include/asm/octeon/cvmx-helper-util.h
+++ b/arch/mips/include/asm/octeon/cvmx-helper-util.h
@@ -42,7 +42,7 @@
  *
  * Small helper utilities.
  *
- * <hr>$Revision: 100097 $<hr>
+ * <hr>$Revision: 103836 $<hr>
  */
 
 #ifndef __CVMX_HELPER_UTIL_H__
@@ -51,9 +51,7 @@
 #include "cvmx.h"
 #include "cvmx-mio-defs.h"
 #include "cvmx-helper.h"
-#include "cvmx-fpa1.h"
-#include "cvmx-fpa3.h"
-
+#include "cvmx-fpa.h"
 
 typedef char cvmx_pknd_t;
 typedef char cvmx_bpid_t;
@@ -154,32 +152,34 @@ static inline int cvmx_helper_node_interface_to_xiface(int node, int interface)
  */
 static inline void cvmx_helper_free_pki_pkt_data(cvmx_wqe_t *work)
 {
-        uint64_t        number_buffers;
-        uint64_t        start_of_buffer;
-        cvmx_buf_ptr_pki_t  next_buffer_ptr;
-        cvmx_buf_ptr_pki_t  buffer_ptr;
-        cvmx_wqe_78xx_t *wqe = (cvmx_wqe_78xx_t*)work;
-
-        if (!octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
-                cvmx_dprintf("ERROR: free_pki_pkt_data: Supported only on 78xx\n");
-                return;
-        }
-        /* Make sure errata pki-20776 has been applied*/
-        cvmx_wqe_pki_errata_20776(work);
-        buffer_ptr = wqe->packet_ptr;
-        number_buffers = cvmx_wqe_get_bufs(work);
-
-        while (number_buffers--) {
-                unsigned aura = cvmx_wqe_get_aura(work);
-                /* XXX- assumes the buffer is cache-line aligned and naturally aligned mode*/
-                start_of_buffer = (buffer_ptr.addr >> 7) << 7;
-                /* Read pointer to next buffer before we free the current buffer. */
-                next_buffer_ptr = *(cvmx_buf_ptr_pki_t *)
-                                cvmx_phys_to_ptr(buffer_ptr.addr - 8);
-                /* FPA AURA comes from WQE, includes node */
-                cvmx_fpa3_free_gaura(cvmx_phys_to_ptr(start_of_buffer),	aura, 0);
-                buffer_ptr = next_buffer_ptr;
-        }
+	uint64_t        number_buffers;
+	uint64_t        start_of_buffer;
+	cvmx_buf_ptr_pki_t  next_buffer_ptr;
+	cvmx_buf_ptr_pki_t  buffer_ptr;
+	cvmx_wqe_78xx_t *wqe = (cvmx_wqe_78xx_t*)work;
+
+	if (!octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
+		cvmx_dprintf("ERROR: free_pki_pkt_data: Supported only on 78xx\n");
+		return;
+	}
+	/* Make sure errata pki-20776 has been applied*/
+	cvmx_wqe_pki_errata_20776(work);
+	buffer_ptr = wqe->packet_ptr;
+	number_buffers = cvmx_wqe_get_bufs(work);
+
+	while (number_buffers--) {
+		/* FIXME: change WQE function prototype */
+		unsigned x = cvmx_wqe_get_aura(work);
+		cvmx_fpa3_gaura_t aura = __cvmx_fpa3_gaura(x >> 10, x & 0x3ff);
+		/* XXX- assumes the buffer is cache-line aligned and naturally aligned mode*/
+		start_of_buffer = (buffer_ptr.addr >> 7) << 7;
+		/* Read pointer to next buffer before we free the current buffer. */
+		next_buffer_ptr = *(cvmx_buf_ptr_pki_t *)
+			cvmx_phys_to_ptr(buffer_ptr.addr - 8);
+		/* FPA AURA comes from WQE, includes node */
+		cvmx_fpa3_free(cvmx_phys_to_ptr(start_of_buffer), aura, 0);
+		buffer_ptr = next_buffer_ptr;
+	}
 }
 
 /**
@@ -192,16 +192,23 @@ static inline void cvmx_helper_free_pki_pkt_data(cvmx_wqe_t *work)
  */
 static inline void cvmx_wqe_pki_free(cvmx_wqe_t *work)
 {
-        cvmx_wqe_78xx_t *wqe = (cvmx_wqe_78xx_t *)work;
-
-        if (!octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
-                cvmx_dprintf("ERROR: cvmx_wqe_free: Supported only on 78xx\n");
-                return;
-        }
-        /* Do nothing if the first packet buffer shares WQE buffer */
-        if (!wqe->packet_ptr.packet_outside_wqe)
-                return;
-        cvmx_fpa3_free_gaura(work, cvmx_wqe_get_aura(work), 0);
+	cvmx_wqe_78xx_t *wqe = (cvmx_wqe_78xx_t *)work;
+	unsigned x;
+	cvmx_fpa3_gaura_t aura;
+
+	if (!octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
+		cvmx_dprintf("ERROR: cvmx_wqe_free: Supported only on 78xx\n");
+		return;
+	}
+	/* Do nothing if the first packet buffer shares WQE buffer */
+	if (!wqe->packet_ptr.packet_outside_wqe)
+		return;
+
+	/* FIXME change WQE function prototype */
+	x = cvmx_wqe_get_aura(work);
+	aura = __cvmx_fpa3_gaura(x >> 10, x & 0x3ff);
+
+	cvmx_fpa3_free(work, aura, 0);
 }
 
 /**
diff --git a/arch/mips/include/asm/octeon/cvmx-helper.h b/arch/mips/include/asm/octeon/cvmx-helper.h
index e5c8b33..9ec1372 100644
--- a/arch/mips/include/asm/octeon/cvmx-helper.h
+++ b/arch/mips/include/asm/octeon/cvmx-helper.h
@@ -42,7 +42,7 @@
  *
  * Helper functions for common, but complicated tasks.
  *
- * <hr>$Revision: 98310 $<hr>
+ * <hr>$Revision: 101694 $<hr>
  */
 
 #ifndef __CVMX_HELPER_H__
@@ -522,6 +522,9 @@ enum cvmx_pko_padding {
  */
 int __cvmx_helper_get_has_fcs(int interface);
 
+void *cvmx_helper_mem_alloc(int node, uint64_t alloc_size, uint64_t align);
+void cvmx_helper_mem_free(void *buffer, uint64_t size);
+
 #ifdef  __cplusplus
 /* *INDENT-OFF* */
 }
diff --git a/arch/mips/include/asm/octeon/cvmx-hwpko.h b/arch/mips/include/asm/octeon/cvmx-hwpko.h
index 0f5eba7..8a1f56d 100644
--- a/arch/mips/include/asm/octeon/cvmx-hwpko.h
+++ b/arch/mips/include/asm/octeon/cvmx-hwpko.h
@@ -74,7 +74,7 @@
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include "cvmx-pko-defs.h"
 #include <asm/octeon/cvmx-hwfau.h>
-#include <asm/octeon/cvmx-fpa1.h>
+#include <asm/octeon/cvmx-fpa.h>
 #include <asm/octeon/cvmx-pow.h>
 #include <asm/octeon/cvmx-cmd-queue.h>
 #include <asm/octeon/cvmx-helper.h>
@@ -82,7 +82,7 @@
 #include <asm/octeon/cvmx-helper-pko.h>
 #else
 #include "cvmx-hwfau.h"
-#include "cvmx-fpa1.h"
+#include "cvmx-fpa.h"
 #include "cvmx-pow.h"
 #include "cvmx-pko3.h"	/* for back-comp */
 #include "cvmx-cmd-queue.h"
diff --git a/arch/mips/include/asm/octeon/cvmx-ilk.h b/arch/mips/include/asm/octeon/cvmx-ilk.h
index d4a8afe..1c3243b 100644
--- a/arch/mips/include/asm/octeon/cvmx-ilk.h
+++ b/arch/mips/include/asm/octeon/cvmx-ilk.h
@@ -105,7 +105,7 @@ typedef struct {
 
 #define CVMX_ILK_MAX_PIPES 45
 /* Max number of channels allowed */
-#define CVMX_ILK_MAX_CHANS 8	//FIXME: increase for CN78XX !
+#define CVMX_ILK_MAX_CHANS 8
 
 extern CVMX_SHARED unsigned char cvmx_ilk_chans[CVMX_NUM_ILK_INTF];
 extern unsigned char cvmx_ilk_chan_map[CVMX_NUM_ILK_INTF][CVMX_ILK_MAX_CHANS];
diff --git a/arch/mips/include/asm/octeon/cvmx-pcie.h b/arch/mips/include/asm/octeon/cvmx-pcie.h
index 4a85ae5..69440b0 100644
--- a/arch/mips/include/asm/octeon/cvmx-pcie.h
+++ b/arch/mips/include/asm/octeon/cvmx-pcie.h
@@ -42,7 +42,7 @@
  *
  * Interface to PCIe as a host(RC) or target(EP)
  *
- * <hr>$Revision: 93533 $<hr>
+ * <hr>$Revision: 102522 $<hr>
  */
 
 #ifndef __CVMX_PCIE_H__
@@ -303,6 +303,7 @@ void cvmx_pcie_config_write32(int pcie_port, int bus, int dev, int fn, int reg,
  * @return Value read
  */
 uint32_t cvmx_pcie_cfgx_read(int pcie_port, uint32_t cfg_offset);
+uint32_t cvmx_pcie_cfgx_read_node(int node, int pcie_port, uint32_t cfg_offset);
 
 /**
  * Write a PCIe config space register indirectly. This is used for
@@ -313,6 +314,7 @@ uint32_t cvmx_pcie_cfgx_read(int pcie_port, uint32_t cfg_offset);
  * @param val        Value to write
  */
 void cvmx_pcie_cfgx_write(int pcie_port, uint32_t cfg_offset, uint32_t val);
+void cvmx_pcie_cfgx_write_node(int node, int pcie_port, uint32_t cfg_offset, uint32_t val);
 
 /**
  * Write a 32bit value to the Octeon NPEI register space
diff --git a/arch/mips/include/asm/octeon/cvmx-pip.h b/arch/mips/include/asm/octeon/cvmx-pip.h
index 1a306d4..6797430 100644
--- a/arch/mips/include/asm/octeon/cvmx-pip.h
+++ b/arch/mips/include/asm/octeon/cvmx-pip.h
@@ -42,7 +42,7 @@
  *
  * Interface to the hardware Packet Input Processing unit.
  *
- * <hr>$Revision: 97097 $<hr>
+ * <hr>$Revision: 103836 $<hr>
  */
 
 #ifndef __CVMX_PIP_H__
@@ -50,12 +50,14 @@
 
 #include "cvmx-wqe.h"
 #include "cvmx-pki.h"
+#include "cvmx-helper-pki.h"
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include "cvmx-pip-defs.h"
 #endif
 
 #include "cvmx-helper.h"
 #include "cvmx-helper-util.h"
+#include "cvmx-pki-resources.h"
 
 #ifdef	__cplusplus
 /* *INDENT-OFF* */
@@ -64,6 +66,7 @@ extern "C" {
 #endif
 
 #define CVMX_PIP_NUM_INPUT_PORTS                46
+#define CVMX_PIP_NUM_WATCHERS                   8
 
 /*
  * Encodes the different error and exception codes
@@ -285,31 +288,282 @@ typedef union {
 	} s;
 } cvmx_pip_pkt_inst_hdr_t;
 
+enum cvmx_pki_pcam_match {
+	CVMX_PKI_PCAM_MATCH_IP,
+	CVMX_PKI_PCAM_MATCH_IPV4,
+	CVMX_PKI_PCAM_MATCH_IPV6,
+	CVMX_PKI_PCAM_MATCH_TCP
+};
+
 /* CSR typedefs have been moved to cvmx-pip-defs.h */
+static inline int cvmx_pip_config_watcher(int index, int type, uint16_t match,
+		    uint16_t mask, int grp, int qos)
+{
+	if (index >= CVMX_PIP_NUM_WATCHERS) {
+		cvmx_dprintf("ERROR: pip watcher %d is > than supported\n", index);
+		return -1;
+	}
+	if (octeon_has_feature(OCTEON_FEATURE_PKI)) {
+		/* store in software for now, only when the watcher is enabled program the entry*/
+		if (type == CVMX_PIP_QOS_WATCH_PROTNH) {
+			qos_watcher[index].field = CVMX_PKI_PCAM_TERM_L3_FLAGS;
+			qos_watcher[index].data = (uint32_t)(match << 16);
+			qos_watcher[index].data_mask = (uint32_t)(mask << 16);
+			qos_watcher[index].advance = 0;
+		} else if (type == CVMX_PIP_QOS_WATCH_TCP) {
+			qos_watcher[index].field = CVMX_PKI_PCAM_TERM_L4_PORT;
+			qos_watcher[index].data = 0x060000;
+			qos_watcher[index].data |= (uint32_t)match;
+			qos_watcher[index].data_mask = (uint32_t)(mask);
+			qos_watcher[index].advance = 0;
+		} else if (type == CVMX_PIP_QOS_WATCH_UDP) {
+			qos_watcher[index].field = CVMX_PKI_PCAM_TERM_L4_PORT;
+			qos_watcher[index].data = 0x110000;
+			qos_watcher[index].data |= (uint32_t)match;
+			qos_watcher[index].data_mask = (uint32_t)(mask);
+			qos_watcher[index].advance = 0;
+		} else if (type == 0x4 /*CVMX_PIP_QOS_WATCH_ETHERTYPE*/) {
+			qos_watcher[index].field = CVMX_PKI_PCAM_TERM_ETHTYPE0;
+			if (match == 0x8100) {
+				cvmx_dprintf("ERROR: default vlan entry already exist, cant set watcher\n");
+				return -1;
+			}
+			qos_watcher[index].data = (uint32_t)(match << 16);
+			qos_watcher[index].data_mask = (uint32_t)(mask << 16);
+			qos_watcher[index].advance = 4;
+		} else {
+			cvmx_dprintf("ERROR: Unsupported watcher type %d\n", type);
+			return -1;
+		}
+		if (grp >= 32) {
+			cvmx_dprintf("ERROR: grp %d out of range for backward compat 78xx\n", grp);
+			return -1;
+		}
+		qos_watcher[index].sso_grp = (uint8_t)(grp << 3 | qos);
+		qos_watcher[index].configured = 1;
+	} else {
+		/* Implement it later */
+	}
+	return 0;
+}
+
+static inline int __cvmx_pip_set_tag_type(int node, int style,
+	int tag_type, int field)
+{
+	struct cvmx_pki_style_config style_cfg;
+	int style_num;
+	int pcam_offset;
+	int bank;
+	struct cvmx_pki_pcam_input pcam_input;
+	struct cvmx_pki_pcam_action pcam_action;
+
+	/* All other style parameters remain same except tag type */
+	cvmx_pki_read_style_config(node, style, CVMX_PKI_CLUSTER_ALL, &style_cfg);
+	style_cfg.parm_cfg.tag_type = tag_type;
+	style_num = cvmx_pki_style_alloc(node, -1);
+	if (style_num < 0) {
+		cvmx_dprintf("ERROR: style not available to set tag type\n");
+		return -1;
+	}
+	cvmx_pki_write_style_config(node, style_num, CVMX_PKI_CLUSTER_ALL, &style_cfg);
+	memset(&pcam_input, 0, sizeof(pcam_input));
+	memset(&pcam_action, 0, sizeof(pcam_action));
+	pcam_input.style = style;
+	pcam_input.style_mask = 0xff;
+	if (field == CVMX_PKI_PCAM_MATCH_IP) {
+		pcam_input.field = CVMX_PKI_PCAM_TERM_ETHTYPE0;
+		pcam_input.field_mask = 0xff;
+		pcam_input.data = 0x08000000;
+		pcam_input.data_mask = 0xffff0000;
+		pcam_action.pointer_advance = 4;
+		/* legacy will write to all clusters*/
+		bank = 0;
+		pcam_offset = cvmx_pki_pcam_entry_alloc(node, CVMX_PKI_FIND_AVAL_ENTRY, bank, CVMX_PKI_CLUSTER_ALL);
+		if (pcam_offset < 0) {
+			cvmx_dprintf("ERROR: pcam entry not available to enable qos watcher\n");
+			cvmx_pki_style_free(node, style_num);
+			return -1;
+		}
+		pcam_action.parse_mode_chg = CVMX_PKI_PARSE_NO_CHG;
+		pcam_action.layer_type_set = CVMX_PKI_LTYPE_E_NONE;
+		pcam_action.style_add = (uint8_t)(style_num - style);
+		cvmx_pki_pcam_write_entry(node, pcam_offset, CVMX_PKI_CLUSTER_ALL, pcam_input, pcam_action);
+		field = CVMX_PKI_PCAM_MATCH_IPV6;
+	}
+	if (field == CVMX_PKI_PCAM_MATCH_IPV4) {
+		pcam_input.field = CVMX_PKI_PCAM_TERM_ETHTYPE0;
+		pcam_input.field_mask = 0xff;
+		pcam_input.data = 0x08000000;
+		pcam_input.data_mask = 0xffff0000;
+		pcam_action.pointer_advance = 4;
+	} else if (field == CVMX_PKI_PCAM_MATCH_IPV6) {
+		pcam_input.field = CVMX_PKI_PCAM_TERM_ETHTYPE0;
+		pcam_input.field_mask = 0xff;
+		pcam_input.data = 0x86dd00000;
+		pcam_input.data_mask = 0xffff0000;
+		pcam_action.pointer_advance = 4;
+	} else if (field == CVMX_PKI_PCAM_MATCH_TCP) {
+		pcam_input.field = CVMX_PKI_PCAM_TERM_L4_PORT;
+		pcam_input.field_mask = 0xff;
+		pcam_input.data = 0x60000;
+		pcam_input.data_mask = 0xff0000;
+		pcam_action.pointer_advance = 0;
+	}
+	pcam_action.parse_mode_chg = CVMX_PKI_PARSE_NO_CHG;
+	pcam_action.layer_type_set = CVMX_PKI_LTYPE_E_NONE;
+	pcam_action.style_add = (uint8_t)(style_num - style);
+	bank = pcam_input.field & 0x01;
+	pcam_offset = cvmx_pki_pcam_entry_alloc(node, CVMX_PKI_FIND_AVAL_ENTRY, bank, CVMX_PKI_CLUSTER_ALL);
+	if (pcam_offset < 0) {
+		cvmx_dprintf("ERROR: pcam entry not available to enable qos watcher\n");
+		cvmx_pki_style_free(node, style_num);
+		return -1;
+	}
+	cvmx_pki_pcam_write_entry(node, pcam_offset, CVMX_PKI_CLUSTER_ALL, pcam_input, pcam_action);
+	return style_num;
+	}
+
+/* Only for legacy internal use */
+static inline int __cvmx_pip_enable_watcher_78xx(int node, int index, int style)
+	{
+	struct cvmx_pki_style_config style_cfg;
+	struct cvmx_pki_qpg_config qpg_cfg;
+	struct cvmx_pki_pcam_input pcam_input;
+	struct cvmx_pki_pcam_action pcam_action;
+	int style_num;
+	int qpg_offset;
+	int pcam_offset;
+	int bank;
+
+	if (!qos_watcher[index].configured) {
+		cvmx_dprintf("ERROR: qos watcher %d should be configured before enable\n", index);
+		return -1;
+	}
+	/* All other style parameters remain same except grp and qos and qps base */
+	cvmx_pki_read_style_config(node, style, CVMX_PKI_CLUSTER_ALL, &style_cfg);
+	cvmx_pki_read_qpg_entry(node, style_cfg.parm_cfg.qpg_base, &qpg_cfg);
+	qpg_cfg.grp_ok = qos_watcher[index].sso_grp;
+	qpg_cfg.grp_bad = qos_watcher[index].sso_grp;
+	qpg_offset = cvmx_helper_pki_set_qpg_entry(node, &qpg_cfg);
+	if (qpg_offset == -1) {
+		cvmx_dprintf("Warning: no new qpg entry available to enable watcher\n");
+		return -1;
+	}
+	/* try to reserve the style, if it is not configured already, reserve
+	and configure it */
+	style_cfg.parm_cfg.qpg_base = qpg_offset;
+	style_num = cvmx_pki_style_alloc(node, -1);
+	if (style_num < 0) {
+		cvmx_dprintf("ERROR: style not available to enable qos watcher\n");
+		cvmx_pki_qpg_entry_free(node, qpg_offset, 1);
+		return -1;
+	}
+	cvmx_pki_write_style_config(node, style_num, CVMX_PKI_CLUSTER_ALL, &style_cfg);
+	/* legacy will write to all clusters*/
+	bank = qos_watcher[index].field & 0x01;
+	pcam_offset = cvmx_pki_pcam_entry_alloc(node, CVMX_PKI_FIND_AVAL_ENTRY, bank, CVMX_PKI_CLUSTER_ALL);
+	if (pcam_offset < 0) {
+		cvmx_dprintf("ERROR: pcam entry not available to enable qos watcher\n");
+		cvmx_pki_style_free(node, style_num);
+		cvmx_pki_qpg_entry_free(node, qpg_offset, 1);
+		return -1;
+	}
+	memset(&pcam_input, 0, sizeof(pcam_input));
+	memset(&pcam_action, 0, sizeof(pcam_action));
+	pcam_input.style = style;
+	pcam_input.style_mask = 0xff;
+	pcam_input.field = qos_watcher[index].field;
+	pcam_input.field_mask = 0xff;
+	pcam_input.data = qos_watcher[index].data;
+	pcam_input.data_mask = qos_watcher[index].data_mask;
+	pcam_action.parse_mode_chg = CVMX_PKI_PARSE_NO_CHG;
+	pcam_action.layer_type_set = CVMX_PKI_LTYPE_E_NONE;
+	pcam_action.style_add = (uint8_t)(style_num - style);
+	pcam_action.pointer_advance = qos_watcher[index].advance;
+	cvmx_pki_pcam_write_entry(node, pcam_offset, CVMX_PKI_CLUSTER_ALL, pcam_input, pcam_action);
+	return 0;
+	}
 
 /**
  * Configure an ethernet input port
  *
  * @param ipd_port Port number to configure
  * @param port_cfg Port hardware configuration
- * @param port_tag_cfg
- *                 Port POW tagging configuration
+ * @param port_tag_cfg Port POW tagging configuration
  */
 static inline void cvmx_pip_config_port(uint64_t ipd_port, cvmx_pip_prt_cfgx_t port_cfg, cvmx_pip_prt_tagx_t port_tag_cfg)
-{
+	{
+	struct cvmx_pki_qpg_config qpg_cfg;
+	int qpg_offset;
+	uint8_t tcp_tag = 0xff;
+	uint8_t ip_tag = 0xaa;
+	int style, nstyle, n4style, n6style;
+
 	if (octeon_has_feature(OCTEON_FEATURE_PKI)) {
 		struct cvmx_pki_port_config pki_prt_cfg;
+		struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(ipd_port);
 
 		cvmx_pki_get_port_config(ipd_port, &pki_prt_cfg);
-		if (port_cfg.s.ih_pri || port_cfg.s.pad_len || port_cfg.s.vlan_len)
+		style = pki_prt_cfg.pkind_cfg.initial_style;
+		if (port_cfg.s.ih_pri || port_cfg.s.vlan_len || port_cfg.s.pad_len)
 			cvmx_dprintf("Warning: 78xx: use different config for this option\n");
 		pki_prt_cfg.style_cfg.parm_cfg.minmax_sel = port_cfg.s.len_chk_sel;
 		pki_prt_cfg.style_cfg.parm_cfg.lenerr_en = port_cfg.s.lenerr_en;
 		pki_prt_cfg.style_cfg.parm_cfg.maxerr_en = port_cfg.s.maxerr_en;
 		pki_prt_cfg.style_cfg.parm_cfg.minerr_en = port_cfg.s.minerr_en;
-		if (port_cfg.s.grp_wat_47 || port_cfg.s.qos_wat_47)
-			cvmx_dprintf("Warning: 78xx use different method for grp and qos watchers\n");
+		pki_prt_cfg.style_cfg.parm_cfg.fcs_chk = port_cfg.s.crc_en;
+		if (port_cfg.s.grp_wat || port_cfg.s.qos_wat ||
+		    port_cfg.s.grp_wat_47 || port_cfg.s.qos_wat_47) {
+			uint8_t group_mask = (uint8_t)(port_cfg.s.grp_wat | (uint8_t)(port_cfg.s.grp_wat_47 << 4));
+			uint8_t qos_mask = (uint8_t)(port_cfg.s.qos_wat | (uint8_t)(port_cfg.s.qos_wat_47 << 4));
+			int i;
+			for (i=0; i<CVMX_PIP_NUM_WATCHERS; i++) {
+				if ((group_mask & (1 << i)) || (qos_mask & (1 << i)))
+					__cvmx_pip_enable_watcher_78xx(xp.node, i, style);
+			}
+		}
+		if (port_tag_cfg.s.tag_mode) {
+			if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X))
+				cvmx_printf("Warning: mask tag is not supported in 78xx pass1\n");
+			else {}
+				/* need to implement for 78xx*/
+		}
+		if (port_cfg.s.tag_inc)
+			cvmx_dprintf("Warning: 78xx uses differnet method for tag generation\n");
 		pki_prt_cfg.style_cfg.parm_cfg.rawdrp = port_cfg.s.rawdrp;
+		pki_prt_cfg.pkind_cfg.parse_en.inst_hdr = port_cfg.s.inst_hdr;
+		if (port_cfg.s.hg_qos)
+			pki_prt_cfg.style_cfg.parm_cfg.qpg_qos = CVMX_PKI_QPG_QOS_HIGIG;
+		else if (port_cfg.s.qos_vlan)
+			pki_prt_cfg.style_cfg.parm_cfg.qpg_qos = CVMX_PKI_QPG_QOS_VLAN;
+		else if (port_cfg.s.qos_diff)
+			pki_prt_cfg.style_cfg.parm_cfg.qpg_qos = CVMX_PKI_QPG_QOS_DIFFSERV;
+		if (port_cfg.s.qos_vod)
+			cvmx_dprintf("Warning: 78xx needs pcam entries installed to achieve qos_vod\n");
+		if (port_cfg.s.qos) {
+			cvmx_pki_read_qpg_entry(xp.node,
+				pki_prt_cfg.style_cfg.parm_cfg.qpg_base, &qpg_cfg);
+			qpg_cfg.grp_ok |= port_cfg.s.qos;
+			qpg_cfg.grp_bad |= port_cfg.s.qos;
+			qpg_offset = cvmx_helper_pki_set_qpg_entry(xp.node, &qpg_cfg);
+			if (qpg_offset == -1)
+				cvmx_dprintf("Warning: no new qpg entry available, will not modify qos\n");
+			else
+				pki_prt_cfg.style_cfg.parm_cfg.qpg_base = qpg_offset;
+		}
+		if (port_tag_cfg.s.grp != pki_dflt_sso_grp[xp.node].group) {
+			cvmx_pki_read_qpg_entry(xp.node,
+					pki_prt_cfg.style_cfg.parm_cfg.qpg_base, &qpg_cfg);
+			qpg_cfg.grp_ok |= (uint8_t)(port_tag_cfg.s.grp << 3);
+			qpg_cfg.grp_bad |= (uint8_t)(port_tag_cfg.s.grp << 3);
+			qpg_offset = cvmx_helper_pki_set_qpg_entry(xp.node, &qpg_cfg);
+			if (qpg_offset == -1)
+				cvmx_dprintf("Warning: no new qpg entry available, will not modify group\n");
+			else
+				pki_prt_cfg.style_cfg.parm_cfg.qpg_base = qpg_offset;
+		}
+		pki_prt_cfg.pkind_cfg.parse_en.dsa_en = port_cfg.s.dsa_en;
+		pki_prt_cfg.pkind_cfg.parse_en.hg_en = port_cfg.s.higig_en;
 		pki_prt_cfg.style_cfg.tag_cfg.tag_fields.layer_c_src = port_tag_cfg.s.ip6_src_flag |
 				port_tag_cfg.s.ip4_src_flag;
 		pki_prt_cfg.style_cfg.tag_cfg.tag_fields.layer_c_dst = port_tag_cfg.s.ip6_dst_flag |
@@ -323,38 +577,84 @@ static inline void cvmx_pip_config_port(uint64_t ipd_port, cvmx_pip_prt_cfgx_t p
 		pki_prt_cfg.style_cfg.tag_cfg.tag_fields.input_port = port_tag_cfg.s.inc_prt_flag;
 		pki_prt_cfg.style_cfg.tag_cfg.tag_fields.first_vlan = port_tag_cfg.s.inc_vlan;
 		pki_prt_cfg.style_cfg.tag_cfg.tag_fields.second_vlan = port_tag_cfg.s.inc_vs;
-                if ((port_tag_cfg.s.tcp6_tag_type == port_tag_cfg.s.tcp4_tag_type) &&
-                     (port_tag_cfg.s.ip6_tag_type == port_tag_cfg.s.ip4_tag_type) &&
-                     (port_tag_cfg.s.tcp6_tag_type == port_tag_cfg.s.ip6_tag_type))
-                        pki_prt_cfg.style_cfg.parm_cfg.tag_type = port_tag_cfg.s.tcp6_tag_type;
-                else
-                    cvmx_dprintf("Warning: 78xx use different method for tcp/ip tag type\n");
-		if (port_tag_cfg.s.grp)
-			cvmx_dprintf("Warning: 78xx use different method for sso group scheduling\n");
-		if (port_tag_cfg.s.portadd_en)
-			cvmx_dprintf("Warning: 78xx use different method for portadd_en\n");
+
+		if (port_tag_cfg.s.tcp6_tag_type == port_tag_cfg.s.tcp4_tag_type)
+			tcp_tag = port_tag_cfg.s.tcp6_tag_type;
+		if (port_tag_cfg.s.ip6_tag_type == port_tag_cfg.s.ip4_tag_type)
+			ip_tag = port_tag_cfg.s.ip6_tag_type;
+		pki_prt_cfg.style_cfg.parm_cfg.tag_type = port_tag_cfg.s.non_tag_type;
+		if (tcp_tag == ip_tag && tcp_tag == port_tag_cfg.s.non_tag_type)
+			pki_prt_cfg.style_cfg.parm_cfg.tag_type = tcp_tag;
+		else if (tcp_tag == ip_tag) {
+			/* allocate and copy style */
+			/* modify tag type */
+			/*pcam entry for ip6 && ip4 match*/
+			/* default is non tag type */
+			__cvmx_pip_set_tag_type(xp.node, style, ip_tag, CVMX_PKI_PCAM_MATCH_IP);
+		}
+		else if (ip_tag == port_tag_cfg.s.non_tag_type)
+		{
+			/* allocate and copy style */
+			/* modify tag type */
+			/*pcam entry for tcp6 & tcp4 match*/
+			/* default is non tag type */
+			__cvmx_pip_set_tag_type(xp.node, style, tcp_tag, CVMX_PKI_PCAM_MATCH_TCP);
+		}
+		else {
+			if (ip_tag != 0xaa) {
+				nstyle = __cvmx_pip_set_tag_type(xp.node, style, ip_tag, CVMX_PKI_PCAM_MATCH_IP);
+				if (tcp_tag != 0xff)
+					__cvmx_pip_set_tag_type(xp.node, nstyle, tcp_tag, CVMX_PKI_PCAM_MATCH_TCP);
+				else {
+					n4style = __cvmx_pip_set_tag_type(xp.node, nstyle, ip_tag,
+							CVMX_PKI_PCAM_MATCH_IPV4);
+					__cvmx_pip_set_tag_type(xp.node, n4style, port_tag_cfg.s.tcp4_tag_type,
+							CVMX_PKI_PCAM_MATCH_TCP);
+					n6style = __cvmx_pip_set_tag_type(xp.node, nstyle, ip_tag,
+							CVMX_PKI_PCAM_MATCH_IPV6);
+					__cvmx_pip_set_tag_type(xp.node, n6style, port_tag_cfg.s.tcp6_tag_type,
+							CVMX_PKI_PCAM_MATCH_TCP);
+				}
+			}
+			else {
+				n4style = __cvmx_pip_set_tag_type(xp.node, style, port_tag_cfg.s.ip4_tag_type,
+						CVMX_PKI_PCAM_MATCH_IPV4);
+				n6style = __cvmx_pip_set_tag_type(xp.node, style, port_tag_cfg.s.ip6_tag_type,
+						CVMX_PKI_PCAM_MATCH_IPV6);
+				if (tcp_tag != 0xff) {
+					__cvmx_pip_set_tag_type(xp.node, n4style, tcp_tag, CVMX_PKI_PCAM_MATCH_TCP);
+					__cvmx_pip_set_tag_type(xp.node, n6style, tcp_tag, CVMX_PKI_PCAM_MATCH_TCP);
+				} else {
+					__cvmx_pip_set_tag_type(xp.node, n4style, port_tag_cfg.s.tcp4_tag_type,
+							CVMX_PKI_PCAM_MATCH_TCP);
+					__cvmx_pip_set_tag_type(xp.node, n6style, port_tag_cfg.s.tcp6_tag_type,
+							CVMX_PKI_PCAM_MATCH_TCP);
+				}
+			}
+		}
+		pki_prt_cfg.style_cfg.parm_cfg.qpg_dis_padd = !port_tag_cfg.s.portadd_en;
+
 		if (port_cfg.s.mode == 0x1)
 			pki_prt_cfg.pkind_cfg.initial_parse_mode = CVMX_PKI_PARSE_LA_TO_LG;
 		else if (port_cfg.s.mode == 0x2)
 			pki_prt_cfg.pkind_cfg.initial_parse_mode = CVMX_PKI_PARSE_LC_TO_LG;
 		else
 			pki_prt_cfg.pkind_cfg.initial_parse_mode = CVMX_PKI_PARSE_NOTHING;
-
 		/* This is only for backward compatibility, not all the parameters are supported in 78xx */
-		cvmx_pki_config_port(ipd_port, &pki_prt_cfg);
+		cvmx_pki_set_port_config(ipd_port, &pki_prt_cfg);
 	} else {
-	   if (octeon_has_feature(OCTEON_FEATURE_PKND)) {
-		int interface, index, pknd;
-
-		interface = cvmx_helper_get_interface_num(ipd_port);
-		index = cvmx_helper_get_interface_index_num(ipd_port);
-		pknd = cvmx_helper_get_pknd(interface, index);
-
-		ipd_port = pknd;	/* overload port_num with pknd */
-	   }
-	   cvmx_write_csr(CVMX_PIP_PRT_CFGX(ipd_port), port_cfg.u64);
-	   cvmx_write_csr(CVMX_PIP_PRT_TAGX(ipd_port), port_tag_cfg.u64);
-        }
+		if (octeon_has_feature(OCTEON_FEATURE_PKND)) {
+			int interface, index, pknd;
+
+			interface = cvmx_helper_get_interface_num(ipd_port);
+			index = cvmx_helper_get_interface_index_num(ipd_port);
+			pknd = cvmx_helper_get_pknd(interface, index);
+
+			ipd_port = pknd;	/* overload port_num with pknd */
+		}
+		cvmx_write_csr(CVMX_PIP_PRT_CFGX(ipd_port), port_cfg.u64);
+		cvmx_write_csr(CVMX_PIP_PRT_TAGX(ipd_port), port_tag_cfg.u64);
+	}
 }
 
 /**
@@ -631,14 +931,18 @@ static inline void cvmx_pip_set_frame_check(int interface, uint32_t max_size)
 	   PIP_PRT_CFG[len_chk_sel] selects which set of
 	   MAXLEN/MINLEN to use. */
 	if (octeon_has_feature(OCTEON_FEATURE_PKND)) {
-		if (!OCTEON_IS_MODEL(OCTEON_CN78XX)) {
-			cvmx_pip_prt_cfgx_t config;
-			int port;
-			int num_ports = cvmx_helper_ports_on_interface(interface);
-			for (port = 0; port < num_ports; port++) {
-				int pknd = cvmx_helper_get_pknd(interface, port);
+		int port;
+		int num_ports = cvmx_helper_ports_on_interface(interface);
+		for (port = 0; port < num_ports; port++) {
+			if (octeon_has_feature(OCTEON_FEATURE_PKI)) {
+				int ipd_port;
+				ipd_port = cvmx_helper_get_ipd_port(interface, port);
+				cvmx_pki_set_max_frm_len(ipd_port, max_size);
+			} else {
+				int pknd;
 				int sel;
-
+				cvmx_pip_prt_cfgx_t config;
+				pknd = cvmx_helper_get_pknd(interface, port);
 				config.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(pknd));
 				sel = config.s.len_chk_sel;
 				frm_len.u64 = cvmx_read_csr(CVMX_PIP_FRM_LEN_CHKX(sel));
diff --git a/arch/mips/include/asm/octeon/cvmx-pki-cluster.h b/arch/mips/include/asm/octeon/cvmx-pki-cluster.h
index a1ee9c0..15541a1 100644
--- a/arch/mips/include/asm/octeon/cvmx-pki-cluster.h
+++ b/arch/mips/include/asm/octeon/cvmx-pki-cluster.h
@@ -1,5 +1,5 @@
 /* This file is autgenerated from obj/ipemainc.elf */
-const int cvmx_pki_cluster_code_length = 655;
+const int cvmx_pki_cluster_code_length = 673;
 const uint64_t cvmx_pki_cluster_code_default[] = {
     0x000000000a000000ull,
     0x0000413a68024070ull,
@@ -392,9 +392,9 @@ const uint64_t cvmx_pki_cluster_code_default[] = {
     0xa4068201ff000000ull,
     0x000815ab74000545ull,
     0x0009418068010038ull,
-    0xa429028384000005ull,
+    0xa429028386000005ull,
     0x0009debd04000400ull,
-    0xac8a068184000014ull,
+    0xac8a068186000014ull,
     0x000a15ab74000543ull,
     0x000b5a3468010070ull,
     0xac6b8303000f0005ull,
@@ -612,8 +612,8 @@ const uint64_t cvmx_pki_cluster_code_default[] = {
     0x000682a9f800a800ull,
     0x000a86a9f8009800ull,
     0x000a8a8904000400ull,
-    0x000a41b74c0300ffull,
     0x000a41b64c03ffffull,
+    0x000a41b74c0300ffull,
     0x0000828901000100ull,
     0x000a822803e00180ull,
     0x00088a3400ff0033ull,
@@ -639,9 +639,14 @@ const uint64_t cvmx_pki_cluster_code_default[] = {
     0x000a413268024070ull,
     0xa50a822902800280ull,
     0x0004893d08000800ull,
-    0xb1298301ffffffffull,
-    0x00098381f000e000ull,
-    0xa18c8b01ffffffffull,
+    0x00098301ffffffffull,
+    0xa4c98381f000e000ull,
+    0x00095f00680100f0ull,
+    0xa5295f3e64010000ull,
+    0x0001c00000000000ull,
+    0xa4ec8b01ffffffffull,
+    0x00095d00680100f0ull,
+    0xa1895d3a64010000ull,
     0x000cd5ab80008000ull,
     0x00088a01ff00ff00ull,
     0x0008d5ab40004000ull,
@@ -652,6 +657,19 @@ const uint64_t cvmx_pki_cluster_code_default[] = {
     0x0007d72ef1ff0000ull,
     0x0007d7aff0000000ull,
     0x0004d72e00fc0000ull,
+    0x0000812c00020002ull,
+    0x0004892907c00200ull,
+    0x000441a7680040f0ull,
+    0x000441be4c03ffffull,
+    0x000441ba4c03ffffull,
+    0x000481a803c00200ull,
+    0x0006413168024078ull,
+    0x9801c00000000000ull,
+    0x9821c00000000000ull,
+    0x00065f80680100f0ull,
+    0x00065fbf64010000ull,
+    0x000641bf4c03ffffull,
+    0x000452a568030250ull,
     0x0000000008000000ull,
     0x0001c00000000000ull,
     0x0001c00000000000ull,
diff --git a/arch/mips/include/asm/octeon/cvmx-pki-resources.h b/arch/mips/include/asm/octeon/cvmx-pki-resources.h
index e77792c..edb3dd0 100644
--- a/arch/mips/include/asm/octeon/cvmx-pki-resources.h
+++ b/arch/mips/include/asm/octeon/cvmx-pki-resources.h
@@ -126,7 +126,20 @@ int cvmx_pki_style_free(int node, int style);
    @param cl_grp	cluster group to free
  * @return 	 	0 on success or -1 on failure
  */
-int cvmx_pki_cluster_grp(int node, int cl_grp);
+int cvmx_pki_cluster_grp_free(int node, int cl_grp);
+
+
+/**
+ * This function frees QPG table entries per node.
+ * @param node	 	node number.
+ * @param base_offset	base_offset in qpg table. If -1, first available
+ *			qpg base_offset will be allocated. If base_offset is positive
+ *		 	number and in range, it will try to allocate specified base_offset.
+ * @param count		number of consecutive qpg entries to allocate. They will be consecutive
+ *			from base offset.
+ * @return 	 	qpg table base offset number on success, -1 on failure.
+ */
+int cvmx_pki_qpg_entry_free(int node, int base_offset, int count);
 
 /**
  * This function frees  clusters  from per node
diff --git a/arch/mips/include/asm/octeon/cvmx-pki.h b/arch/mips/include/asm/octeon/cvmx-pki.h
index b5d3187..5d0f908 100644
--- a/arch/mips/include/asm/octeon/cvmx-pki.h
+++ b/arch/mips/include/asm/octeon/cvmx-pki.h
@@ -161,6 +161,11 @@ enum cvmx_pki_stats_mode {
 	CVMX_PKI_STAT_MODE_STYLE
 };
 
+enum cvmx_pki_fpa_wait {
+	CVMX_PKI_DROP_PKT,
+	CVMX_PKI_WAIT_PKT
+};
+
 #define PKI_BELTYPE_E__NONE_M                              (0x0)
 #define PKI_BELTYPE_E__MISC_M                              (0x1)
 #define PKI_BELTYPE_E__IP4_M                               (0x2)
@@ -179,7 +184,7 @@ enum cvmx_pki_beltype { /* PKI_BELTYPE_E_t */
 	CVMX_PKI_BELTYPE_UDP    = PKI_BELTYPE_E__UDP_M,
 	CVMX_PKI_BELTYPE_SCTP   = PKI_BELTYPE_E__SCTP_M,
 	CVMX_PKI_BELTYPE_SNAP   = PKI_BELTYPE_E__SNAP_M,
-        CVMX_PKI_BELTYPE_MAX   = CVMX_PKI_BELTYPE_SNAP
+	CVMX_PKI_BELTYPE_MAX   = CVMX_PKI_BELTYPE_SNAP
 };
 
 struct cvmx_pki_frame_len {
@@ -237,28 +242,49 @@ struct cvmx_pki_pkind_parse {
 
 struct cvmx_pki_pool_config {
 	int pool_num;
+	cvmx_fpa3_pool_t pool;
 	uint64_t buffer_size;
 	uint64_t buffer_count;
 };
 
 struct cvmx_pki_qpg_config {
-	int  qpg_base;
-	int  port_add;
-	int  aura;
-	int  grp_ok;
-	int  grp_bad;
+	int qpg_base;
+	int port_add;
+	int aura_num;
+	int grp_ok;
+	int grp_bad;
+	int grptag_ok;
+	int grptag_bad;
 };
 
 struct cvmx_pki_aura_config {
 	int aura_num;
 	int pool_num;
+	cvmx_fpa3_pool_t pool;
+	cvmx_fpa3_gaura_t aura;
 	int buffer_count;
 };
 
-/* This is per style structure for configuring port parameters, it is kind of of profile
-   which can be assigned to any port. If multiple ports are assigned same style be aware
-   that modiying that style will modify the respective parameters for all the ports which
-   are using this style */
+struct cvmx_pki_cluster_grp_config {
+	int grp_num;
+	uint64_t cluster_mask; /* bit mask of cluster assigned to this cluster group */
+};
+
+struct cvmx_pki_sso_grp_config {
+	int group;
+	int priority;
+	int weight;
+	int affinity;
+	uint64_t core_mask;
+	uint8_t core_mask_set;
+};
+
+/* This is per style structure for configuring port parameters,
+ * it is kind of of profile which can be assigned to any port.
+ * If multiple ports are assigned same style be aware that modifying
+ * that style will modify the respective parameters for all the ports
+ * which are using this style
+ */
 struct cvmx_pki_style_parm {
 
 	bool ip6_udp_opt;	/**< IPv6/UDP checksum is optional. IPv4 allows an optional UDP checksum by sending the all-0s
@@ -377,7 +403,16 @@ struct cvmx_pki_pkind_config {
 					considered part of a IP, TCP or other header for length error checks.*/
 	struct cvmx_pki_pkind_parse parse_en;
 	enum cvmx_pki_pkind_parse_mode	initial_parse_mode;
-	int initial_style;
+	uint8_t fcs_skip;
+	uint8_t inst_skip;              /**< Skip amount from front of packet to first byte covered by FCS start. The
+	                                skip must be even. If PTP_MODE, the 8-byte timestamp is prepended to
+	                                the packet, and FCS_SKIP must be at least 8.*/
+	int initial_style;             /**< Skip amount from front of packet to begin parsing at. If
+	                                PKI_CL(0..3)_PKIND(0..63)_CFG[INST_HDR] is set, points at the first
+	                                byte of the instruction header. If INST_HDR is clear, points at the first
+	                                byte to begin parsing at. The skip must be even. If PTP_MODE, the 8-
+	                                byte timestamp is prepended to the packet, and INST_SKIP must be at
+	                                least 8. */
 	bool custom_l2_hdr;		/**< Valid.custom L2 hesder extraction
 					0 = Disable custom L2 header extraction.
 					1 = Enable custom L2 header extraction.
@@ -429,6 +464,12 @@ struct cvmx_pki_global_config {
 								(Does not apply to the PKI_STAT_INB* registers.)
 								0 = X represents the packet's pkind
 								1 = X represents the low 6-bits of packet's final style */
+	enum cvmx_pki_fpa_wait fpa_wait;			/**< Policy when FPA runs out of buffers:
+								0 = Drop the remainder of the packet requesting the buffer, and
+								set WQE[OPCODE] to RE_MEMOUT.
+								1 = Wait until buffers become available, only dropping packets if
+								buffering ahead of PKI fills. This may lead to head-of-line
+								blocking of packets on other Auras */
 	struct cvmx_pki_global_parse gbl_pen;			/**< Controls Global parsing options for chip */
 	struct cvmx_pki_tag_sec tag_secret;			/**< Secret value for src/dst tag tuple to reduce cache collision attacks */
 	struct cvmx_pki_frame_len frm_len[CVMX_PKI_NUM_FRAME_CHECK]; /**< values for max and min frame length to check against,2 combination */
@@ -457,32 +498,32 @@ struct cvmx_pki_global_config {
 #define CVMX_PKI_PCAM_TERM_E_LG_CUSTOM_M                       (0x39)
 
 enum cvmx_pki_term {
-	CVMX_PKI_PCAM_TERM_E_NONE                    = CVMX_PKI_PCAM_TERM_E_NONE_M,
-	CVMX_PKI_PCAM_TERM_E_L2_CUSTOM               = CVMX_PKI_PCAM_TERM_E_L2_CUSTOM_M,
-	CVMX_PKI_PCAM_TERM_E_HIGIG                   = CVMX_PKI_PCAM_TERM_E_HIGIG_M,
-	CVMX_PKI_PCAM_TERM_E_DMACH                   = CVMX_PKI_PCAM_TERM_E_DMACH_M,
-	CVMX_PKI_PCAM_TERM_E_DMACL                   = CVMX_PKI_PCAM_TERM_E_DMACL_M,
-	CVMX_PKI_PCAM_TERM_E_GLORT                   = CVMX_PKI_PCAM_TERM_E_GLORT_M,
-	CVMX_PKI_PCAM_TERM_E_DSA                     = CVMX_PKI_PCAM_TERM_E_DSA_M,
-	CVMX_PKI_PCAM_TERM_E_ETHTYPE0                = CVMX_PKI_PCAM_TERM_E_ETHTYPE0_M,
-	CVMX_PKI_PCAM_TERM_E_ETHTYPE1                = CVMX_PKI_PCAM_TERM_E_ETHTYPE1_M,
-	CVMX_PKI_PCAM_TERM_E_ETHTYPE2                = CVMX_PKI_PCAM_TERM_E_ETHTYPE2_M,
-	CVMX_PKI_PCAM_TERM_E_ETHTYPE3                = CVMX_PKI_PCAM_TERM_E_ETHTYPE3_M,
-	CVMX_PKI_PCAM_TERM_E_MPLS0                   = CVMX_PKI_PCAM_TERM_E_MPLS0_M,
-	CVMX_PKI_PCAM_TERM_E_L3_FLAGS                = CVMX_PKI_PCAM_TERM_E_L3_FLAGS_M,
-	CVMX_PKI_PCAM_TERM_E_LD_VNI                  = CVMX_PKI_PCAM_TERM_E_LD_VNI_M,
-	CVMX_PKI_PCAM_TERM_E_IL3_FLAGS               = CVMX_PKI_PCAM_TERM_E_IL3_FLAGS_M,
-	CVMX_PKI_PCAM_TERM_E_L4_PORT                 = CVMX_PKI_PCAM_TERM_E_L4_PORT_M,
-	CVMX_PKI_PCAM_TERM_E_LG_CUSTOM               = CVMX_PKI_PCAM_TERM_E_LG_CUSTOM_M
+	CVMX_PKI_PCAM_TERM_NONE                    = CVMX_PKI_PCAM_TERM_E_NONE_M,
+	CVMX_PKI_PCAM_TERM_L2_CUSTOM               = CVMX_PKI_PCAM_TERM_E_L2_CUSTOM_M,
+	CVMX_PKI_PCAM_TERM_HIGIG                   = CVMX_PKI_PCAM_TERM_E_HIGIG_M,
+	CVMX_PKI_PCAM_TERM_DMACH                   = CVMX_PKI_PCAM_TERM_E_DMACH_M,
+	CVMX_PKI_PCAM_TERM_DMACL                   = CVMX_PKI_PCAM_TERM_E_DMACL_M,
+	CVMX_PKI_PCAM_TERM_GLORT                   = CVMX_PKI_PCAM_TERM_E_GLORT_M,
+	CVMX_PKI_PCAM_TERM_DSA                     = CVMX_PKI_PCAM_TERM_E_DSA_M,
+	CVMX_PKI_PCAM_TERM_ETHTYPE0                = CVMX_PKI_PCAM_TERM_E_ETHTYPE0_M,
+	CVMX_PKI_PCAM_TERM_ETHTYPE1                = CVMX_PKI_PCAM_TERM_E_ETHTYPE1_M,
+	CVMX_PKI_PCAM_TERM_ETHTYPE2                = CVMX_PKI_PCAM_TERM_E_ETHTYPE2_M,
+	CVMX_PKI_PCAM_TERM_ETHTYPE3                = CVMX_PKI_PCAM_TERM_E_ETHTYPE3_M,
+	CVMX_PKI_PCAM_TERM_MPLS0                   = CVMX_PKI_PCAM_TERM_E_MPLS0_M,
+	CVMX_PKI_PCAM_TERM_L3_FLAGS                = CVMX_PKI_PCAM_TERM_E_L3_FLAGS_M,
+	CVMX_PKI_PCAM_TERM_LD_VNI                  = CVMX_PKI_PCAM_TERM_E_LD_VNI_M,
+	CVMX_PKI_PCAM_TERM_IL3_FLAGS               = CVMX_PKI_PCAM_TERM_E_IL3_FLAGS_M,
+	CVMX_PKI_PCAM_TERM_L4_PORT                 = CVMX_PKI_PCAM_TERM_E_L4_PORT_M,
+	CVMX_PKI_PCAM_TERM_LG_CUSTOM               = CVMX_PKI_PCAM_TERM_E_LG_CUSTOM_M
 };
 
 struct cvmx_pki_pcam_input {
 	uint64_t		style;
-	uint64_t		style_mask;
+	uint64_t		style_mask; /* bits: 1-match, 0-dont care */
 	enum cvmx_pki_term	field;
-	uint32_t		field_mask;
+	uint32_t		field_mask; /* bits: 1-match, 0-dont care */
 	uint64_t		data;
-	uint64_t		data_mask;
+	uint64_t		data_mask; /* bits: 1-match, 0-dont care */
 };
 
 struct cvmx_pki_pcam_action {
@@ -624,14 +665,15 @@ static inline void cvmx_pki_write_tag_secret(int node, struct cvmx_pki_tag_sec t
 	cvmx_write_csr_node(node, CVMX_PKI_TAG_SECRET, tag_secret_reg.u64);
 }
 
-static inline void cvmx_pki_write_ltype_map(int node, enum cvmx_pki_layer_type layer,
-					    enum cvmx_pki_beltype backend)
+static inline void cvmx_pki_write_ltype_map(int node,
+		enum cvmx_pki_layer_type layer,
+		enum cvmx_pki_beltype backend)
 {
 	cvmx_pki_ltypex_map_t ltype_map;
-        if (layer > CVMX_PKI_LTYPE_E_MAX || backend > CVMX_PKI_BELTYPE_MAX) {
-                cvmx_dprintf("ERROR: invalid ltype beltype mapping\n");
-                return;
-        }
+	if (layer > CVMX_PKI_LTYPE_E_MAX || backend > CVMX_PKI_BELTYPE_MAX) {
+		cvmx_dprintf("ERROR: invalid ltype beltype mapping\n");
+		return;
+	}
 	ltype_map.u64 = cvmx_read_csr_node(node, CVMX_PKI_LTYPEX_MAP(layer));
 	ltype_map.s.beltype = backend;
 	cvmx_write_csr_node(node, CVMX_PKI_LTYPEX_MAP(layer), ltype_map.u64);
@@ -672,12 +714,12 @@ static inline void cvmx_pki_enable_backpressure(int node)
 	cvmx_write_csr_node(node, CVMX_PKI_BUF_CTL, pki_buf_ctl.u64);
 }
 
-#define READCORRECT(cnt, node, value, addr)          \
-{       cnt = 0;    \
-        while(value >= (1ull << 48) && cnt++ < 20) \
-                value = cvmx_read_csr_node(node, addr); \
-        if (cnt >= 20)  \
-                cvmx_dprintf("count stuck for 0x%llx\n", addr);    }
+#define READCORRECT(cnt, node, value, addr)	\
+	{cnt = 0;	\
+	while (value >= (1ull << 48) && cnt++ < 20) \
+		value = cvmx_read_csr_node(node, addr); \
+	if (cnt >= 20)  \
+		cvmx_dprintf("count stuck for 0x%llx\n", addr); }
 
 
 /**
@@ -776,32 +818,32 @@ static inline void cvmx_pki_clear_port_stats(int node, uint64_t port)
 	pki_pknd_inb_stat1.s.octs = 0;
 	pki_pknd_inb_stat2.s.errs = 0;
 
-        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT0(pknd), stat0.u64);
-        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT1(pknd), stat1.u64);
-        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT2(pknd), stat2.u64);
-        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT3(pknd), stat3.u64);
-        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT4(pknd), stat4.u64);
-        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT5(pknd), stat5.u64);
-        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT6(pknd), stat6.u64);
-        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT7(pknd), stat7.u64);
-        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT8(pknd), stat8.u64);
-        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT9(pknd), stat9.u64);
-        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT10(pknd), stat10.u64);
-        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT11(pknd), stat11.u64);
-        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT14(pknd), stat14.u64);
-        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT15(pknd), stat15.u64);
-        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT16(pknd), stat16.u64);
-        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT17(pknd), stat17.u64);
-        cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST0(pknd), hist0.u64);
-        cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST1(pknd), hist1.u64);
-        cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST2(pknd), hist2.u64);
-        cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST3(pknd), hist3.u64);
-        cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST4(pknd), hist4.u64);
-        cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST5(pknd), hist5.u64);
-        cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST6(pknd), hist6.u64);
-        cvmx_write_csr_node(node, CVMX_PKI_PKNDX_INB_STAT0(pknd), pki_pknd_inb_stat0.u64);
-        cvmx_write_csr_node(node, CVMX_PKI_PKNDX_INB_STAT1(pknd), pki_pknd_inb_stat1.u64);
-        cvmx_write_csr_node(node, CVMX_PKI_PKNDX_INB_STAT2(pknd), pki_pknd_inb_stat2.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT0(pknd), stat0.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT1(pknd), stat1.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT2(pknd), stat2.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT3(pknd), stat3.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT4(pknd), stat4.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT5(pknd), stat5.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT6(pknd), stat6.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT7(pknd), stat7.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT8(pknd), stat8.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT9(pknd), stat9.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT10(pknd), stat10.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT11(pknd), stat11.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT14(pknd), stat14.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT15(pknd), stat15.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT16(pknd), stat16.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT17(pknd), stat17.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST0(pknd), hist0.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST1(pknd), hist1.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST2(pknd), hist2.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST3(pknd), hist3.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST4(pknd), hist4.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST5(pknd), hist5.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST6(pknd), hist6.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_PKNDX_INB_STAT0(pknd), pki_pknd_inb_stat0.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_PKNDX_INB_STAT1(pknd), pki_pknd_inb_stat1.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_PKNDX_INB_STAT2(pknd), pki_pknd_inb_stat2.u64);
 
 }
 
@@ -842,76 +884,76 @@ static inline void cvmx_pki_get_stats(int node, int index, struct cvmx_pki_port_
 	cvmx_pki_pkndx_inb_stat0_t pki_pknd_inb_stat0;
 	cvmx_pki_pkndx_inb_stat1_t pki_pknd_inb_stat1;
 	cvmx_pki_pkndx_inb_stat2_t pki_pknd_inb_stat2;
-        int cnt;
+	int cnt;
 
 	stat0.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT0(index));
-        READCORRECT(cnt, node, stat0.u64, CVMX_PKI_STATX_STAT0(index));
+	READCORRECT(cnt, node, stat0.u64, CVMX_PKI_STATX_STAT0(index));
 
 	stat1.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT1(index));
-        READCORRECT(cnt, node, stat1.u64, CVMX_PKI_STATX_STAT1(index));
+	READCORRECT(cnt, node, stat1.u64, CVMX_PKI_STATX_STAT1(index));
 
 	stat2.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT2(index));
-        READCORRECT(cnt, node, stat2.u64, CVMX_PKI_STATX_STAT2(index));
+	READCORRECT(cnt, node, stat2.u64, CVMX_PKI_STATX_STAT2(index));
 
 	stat3.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT3(index));
-        READCORRECT(cnt, node, stat3.u64, CVMX_PKI_STATX_STAT3(index));
+	READCORRECT(cnt, node, stat3.u64, CVMX_PKI_STATX_STAT3(index));
 
 	stat4.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT4(index));
-        READCORRECT(cnt, node, stat4.u64, CVMX_PKI_STATX_STAT4(index));
+	READCORRECT(cnt, node, stat4.u64, CVMX_PKI_STATX_STAT4(index));
 
 	stat5.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT5(index));
-        READCORRECT(cnt, node, stat5.u64, CVMX_PKI_STATX_STAT5(index));
+	READCORRECT(cnt, node, stat5.u64, CVMX_PKI_STATX_STAT5(index));
 
 	stat6.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT6(index));
-        READCORRECT(cnt, node, stat6.u64, CVMX_PKI_STATX_STAT6(index));
+	READCORRECT(cnt, node, stat6.u64, CVMX_PKI_STATX_STAT6(index));
 
 	stat7.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT7(index));
-        READCORRECT(cnt, node, stat7.u64, CVMX_PKI_STATX_STAT7(index));
+	READCORRECT(cnt, node, stat7.u64, CVMX_PKI_STATX_STAT7(index));
 
 	stat8.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT8(index));
-        READCORRECT(cnt, node, stat8.u64, CVMX_PKI_STATX_STAT8(index));
+	READCORRECT(cnt, node, stat8.u64, CVMX_PKI_STATX_STAT8(index));
 
 	stat9.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT9(index));
-        READCORRECT(cnt, node, stat9.u64, CVMX_PKI_STATX_STAT9(index));
+	READCORRECT(cnt, node, stat9.u64, CVMX_PKI_STATX_STAT9(index));
 
 	stat10.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT10(index));
-        READCORRECT(cnt, node, stat10.u64, CVMX_PKI_STATX_STAT10(index));
+	READCORRECT(cnt, node, stat10.u64, CVMX_PKI_STATX_STAT10(index));
 
 	stat11.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT11(index));
-        READCORRECT(cnt, node, stat11.u64, CVMX_PKI_STATX_STAT11(index));
+	READCORRECT(cnt, node, stat11.u64, CVMX_PKI_STATX_STAT11(index));
 
 	stat14.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT14(index));
-        READCORRECT(cnt, node, stat14.u64, CVMX_PKI_STATX_STAT14(index));
+	READCORRECT(cnt, node, stat14.u64, CVMX_PKI_STATX_STAT14(index));
 
 	stat15.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT15(index));
-        READCORRECT(cnt, node, stat15.u64, CVMX_PKI_STATX_STAT15(index));
+	READCORRECT(cnt, node, stat15.u64, CVMX_PKI_STATX_STAT15(index));
 
 	stat16.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT16(index));
-        READCORRECT(cnt, node, stat16.u64, CVMX_PKI_STATX_STAT16(index));
+	READCORRECT(cnt, node, stat16.u64, CVMX_PKI_STATX_STAT16(index));
 
 	stat17.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT17(index));
-        READCORRECT(cnt, node, stat17.u64, CVMX_PKI_STATX_STAT17(index));
+	READCORRECT(cnt, node, stat17.u64, CVMX_PKI_STATX_STAT17(index));
 
 	hist0.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST0(index));
-        READCORRECT(cnt, node, hist0.u64, CVMX_PKI_STATX_HIST0(index));
+	READCORRECT(cnt, node, hist0.u64, CVMX_PKI_STATX_HIST0(index));
 
 	hist1.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST1(index));
-        READCORRECT(cnt, node, hist1.u64, CVMX_PKI_STATX_HIST1(index));
+	READCORRECT(cnt, node, hist1.u64, CVMX_PKI_STATX_HIST1(index));
 
 	hist2.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST2(index));
-        READCORRECT(cnt, node, hist2.u64, CVMX_PKI_STATX_HIST2(index));
+	READCORRECT(cnt, node, hist2.u64, CVMX_PKI_STATX_HIST2(index));
 
 	hist3.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST3(index));
-        READCORRECT(cnt, node, hist3.u64, CVMX_PKI_STATX_HIST3(index));
+	READCORRECT(cnt, node, hist3.u64, CVMX_PKI_STATX_HIST3(index));
 
 	hist4.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST4(index));
-        READCORRECT(cnt, node, hist4.u64, CVMX_PKI_STATX_HIST4(index));
+	READCORRECT(cnt, node, hist4.u64, CVMX_PKI_STATX_HIST4(index));
 
 	hist5.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST5(index));
-        READCORRECT(cnt, node, hist5.u64, CVMX_PKI_STATX_HIST5(index));
+	READCORRECT(cnt, node, hist5.u64, CVMX_PKI_STATX_HIST5(index));
 
 	hist6.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST6(index));
-        READCORRECT(cnt, node, hist6.u64, CVMX_PKI_STATX_HIST6(index));
+	READCORRECT(cnt, node, hist6.u64, CVMX_PKI_STATX_HIST6(index));
 
 	pki_pknd_inb_stat0.u64 = cvmx_read_csr_node(node, CVMX_PKI_PKNDX_INB_STAT0(index));
 	pki_pknd_inb_stat1.u64 = cvmx_read_csr_node(node, CVMX_PKI_PKNDX_INB_STAT1(index));
@@ -990,6 +1032,45 @@ void cvmx_pki_enable(int node);
 void cvmx_pki_disable(int node);
 
 /**
+ * This function soft resets pki
+ * @param node	node to enable pki in.
+ */
+void cvmx_pki_reset(int node);
+
+/**
+ * This function sets the clusters in PKI
+ * @param node	node to set clusters in.
+ */
+int cvmx_pki_setup_clusters(int node);
+
+/**
+ * This function reads global configuration of PKI block.
+ * @param node		      node number.
+ * @param gbl_cfg	      pointer to struct to read global configuration
+ */
+void cvmx_pki_read_global_config(int node, struct cvmx_pki_global_config *gbl_cfg);
+
+/**
+ * This function writes global configuration of PKI into hw.
+ * @param node		      node number.
+ * @param gbl_cfg	      pointer to struct to global configuration
+ */
+void cvmx_pki_write_global_config(int node, struct cvmx_pki_global_config *gbl_cfg);
+
+
+/**
+ * This function reads per pkind parameters in hardware which defines how
+  the incoming packet is processed.
+ * @param node		      node number.
+ * @param pkind               PKI supports a large number of incoming interfaces
+ *                            and packets arriving on different interfaces or channels
+ *                            may want to be processed differently. PKI uses the pkind to
+ *                            determine how the incoming packet is processed.
+ * @param pkind_cfg	      pointer to struct conatining pkind configuration read from hw
+ */
+int cvmx_pki_read_pkind_config(int node, int pkind, struct cvmx_pki_pkind_config *pkind_cfg);
+
+/**
  * This function writes per pkind parameters in hardware which defines how
   the incoming packet is processed.
  * @param node		      node number.
@@ -997,47 +1078,81 @@ void cvmx_pki_disable(int node);
  *                            and packets arriving on different interfaces or channels
  *                            may want to be processed differently. PKI uses the pkind to
  *                            determine how the incoming packet is processed.
- * @param pkind_cfg	      struct conatining pkind configuration need to be written to hw
+ * @param pkind_cfg	      pointer to struct conatining pkind configuration need to be written
+ *                            in hw
  */
-int cvmx_pki_set_pkind_config(int node, int pkind, struct cvmx_pki_pkind_config *pkind_cfg);
+int cvmx_pki_write_pkind_config(int node, int pkind, struct cvmx_pki_pkind_config *pkind_cfg);
 
-int cvmx_pki_get_pkind_config(int node, int pkind, struct cvmx_pki_pkind_config *pkind_cfg);
+ /** This function reads parameters associated with tag configuration in hardware.
+ * @param node	              node number.
+ * @param style		      style to configure tag for
+ * @param cluster_mask	      Mask of clusters to configure the style for.
+ * @param tag_cfg	      pointer to tag configuration struct.
+  */
+void cvmx_pki_read_tag_config(int node, int style, uint64_t cluster_mask,
+		struct cvmx_pki_style_tag_cfg *tag_cfg);
 
-/**
- * This function writes/configures parameters associated with tag configuration in hardware.
+ /** This function writes/configures parameters associated with tag configuration in hardware.
  * @param node	              node number.
  * @param style		      style to configure tag for
  * @param cluster_mask	      Mask of clusters to configure the style for.
  * @param tag_cfg	      pointer to taf configuration struct.
- */
+  */
 void cvmx_pki_write_tag_config(int node, int style, uint64_t cluster_mask,
-			       struct cvmx_pki_style_tag_cfg *tag_cfg);
+		struct cvmx_pki_style_tag_cfg *tag_cfg);
+
+/**
+ * This function reads parameters associated with style in hardware.
+ * @param node	              node number.
+ * @param style		      style to read from.
+ * @param cluster_mask	      Mask of clusters style belongs to.
+ * @param style_cfg	      pointer to style config struct.
+ */
+void cvmx_pki_read_style_config(int node, int style, uint64_t cluster_mask,
+		struct cvmx_pki_style_config *style_cfg);
 
 /**
  * This function writes/configures parameters associated with style in hardware.
- * @param node	              node to which style belong.
+ * @param node	              node number.
  * @param style		      style to configure.
  * @param cluster_mask	      Mask of clusters to configure the style for.
- * @param style_cfg	      parameters to configure for style passed in struct.
+ * @param style_cfg	      pointer to style config struct.
+ */
+void cvmx_pki_write_style_config(int node, uint64_t style,
+		uint64_t cluster_mask,
+		struct cvmx_pki_style_config *style_cfg);
+/**
+ * This function reads qpg entry at specified offset from qpg table
+ *
+ * @param node		node number
+ * @param offset	offset in qpg table to read from.
+ * @param qpg_cfg       pointer to structure containing qpg values
+ */
+int cvmx_pki_read_qpg_entry(int node, int offset, struct cvmx_pki_qpg_config *qpg_cfg);
+
+
+/**
+ * This function writes qpg entry at specified offset in qpg table
+ *
+ * @param node		node number
+ * @param offset	offset in qpg table to write to.
+ * @param qpg_cfg       pointer to stricture containing qpg values
  */
-void cvmx_pki_set_style_config(int node, uint64_t style, uint64_t cluster_mask,
-			    struct cvmx_pki_style_config *style_cfg);
+void cvmx_pki_write_qpg_entry(int node, int offset, struct cvmx_pki_qpg_config *qpg_cfg);
 
 /**
  * This function writes pcam entry at given offset in pcam table in hardware
  *
- * @param node			node number.
- * @param index			offset in pcam table.
- * @param cluster_mask		Mask of clusters in which to write pcam entry.
- * @param input			input keys to pcam match passed as struct.
- * @param action		pcam match action passed as struct
+ * @param node	              node number.
+ * @param index		      offset in pcam table.
+ * @param cluster_mask	      Mask of clusters in which to write pcam entry.
+ * @param input 	      input keys to pcam match passed as struct.
+ * @param action	      pcam match action passed as struct
  *
  */
 int cvmx_pki_pcam_write_entry(int node, int index, uint64_t cluster_mask,
-				struct cvmx_pki_pcam_input input, struct cvmx_pki_pcam_action action);
-
-int cvmx_pki_setup_clusters(int node);
-
+		struct cvmx_pki_pcam_input input,
+		struct cvmx_pki_pcam_action action);
 /**
  * Configures the channel which will receive backpressure
  * from the specified bpid.
@@ -1074,28 +1189,14 @@ int cvmx_pki_write_aura_bpid(int node, int aura, int bpid);
  *                  1-enable 0-disable
  */
 int cvmx_pki_enable_aura_qos(int node, int aura, bool ena_red,
-			      bool ena_drop, bool ena_bp);
-
-/**
- * This function get the buffer size of the given pool number
- * @param node  node number
- * @param pool  fpa pool number
- * @return	buffer size SUCCESS
- *		-1 if pool number is not found in pool list
- */
-int cvmx_pki_get_pool_buffer_size(int node, int pool);
+		bool ena_drop, bool ena_bp);
 
 /**
- * This function get the buffer size of the given aura number
- * @param node  node number
- * @param pool  fpa aura number
- * @return	buffer size SUCCESS
- *		-1 if aura number is not found in aura list
+ * This function gives the initial style used by that pkind.
+ * @param node          node number.
+ * @param pkind         pkind number.
  */
-int cvmx_pki_get_aura_buffer_size(int node, int aura);
-
-int cvmx_pki_get_mbuff_size(int node, int base_offset);
-
+int cvmx_pki_get_pkind_style(int node, int pkind);
 
 /**
  * This function sets the wqe buffer mode. First packet data buffer can reside
@@ -1116,6 +1217,16 @@ int cvmx_pki_get_mbuff_size(int node, int base_offset);
 void cvmx_pki_set_wqe_mode(int node, uint64_t style, bool pkt_outside_wqe);
 
 /**
+ * This function sets the Packet mode of all ports and styles to little-endian.
+ * It Changes write operations of packet data to L2C to
+ * be in little-endian. Does not change the WQE header format, which is
+ * properly endian neutral.
+ * @param node	              node number.
+ * @param style 	      style to configure.
+ */
+void cvmx_pki_set_little_endian(int node, uint64_t style);
+
+/**
  * Enables/Disables l2 length error check and max & min frame length checks
  * @param node		node number
  * @param pknd		pkind to disable error for.
@@ -1126,7 +1237,7 @@ void cvmx_pki_set_wqe_mode(int node, uint64_t style, bool pkt_outside_wqe);
  *			0 -- Disable error checks
  */
 void cvmx_pki_endis_l2_errs(int node, int pknd, bool l2len_err,
-			    bool maxframe_err, bool minframe_err);
+			bool maxframe_err, bool minframe_err);
 
 /**
  * Enables/Disables fcs check and fcs stripping on the pkind.
@@ -1140,26 +1251,46 @@ void cvmx_pki_endis_l2_errs(int node, int pknd, bool l2len_err,
  *			0 -- Do not strip L2 FCS.
  */
 void cvmx_pki_endis_fcs_check(int node, int pknd, bool fcs_chk, bool fcs_strip);
+
 /**
- * Modifies maximum frame length to check.
- * It modifies the global frame length set used by this port, any other
- * port using the same set will get affected too.
- * @param node   node number
- * @param port	 ipd port for which to modify max len.
- * @param max_size	maximum frame length
+ * This function shows the qpg table entries,
+ * read directly from hardware.
+ * @param node    node number
  */
-void cvmx_pki_set_max_frm_len(int node, int port, uint32_t max_size);
-void cvmx_pki_get_style_config(int node, int style, uint64_t cl_mask,
-			       struct cvmx_pki_style_config *style_cfg);
-void cvmx_pki_config_port(int ipd_port, struct cvmx_pki_port_config *port_cfg);
-void cvmx_pki_get_port_config(int ipd_port, struct cvmx_pki_port_config *port_cfg);
-void cvmx_pki_reset(int node);
-int cvmx_pki_get_pkind_style(int node, int pkind);
-void __cvmx_pki_free_ptr(int node);
 void cvmx_pki_show_qpg_entries(int node, uint16_t num_entry);
-void cvmx_pki_show_port(int node, int interface, int index);
-void cvmx_pki_write_global_cfg(int node, struct cvmx_pki_global_config *gbl_cfg);
-void cvmx_pki_read_global_cfg(int node, struct cvmx_pki_global_config *gbl_cfg);
+
+/**
+ * This function shows the pcam table in raw format,
+ * read directly from hardware.
+ * @param node    node number
+ */
+void cvmx_pki_show_pcam_entries(int node);
+
+/**
+ * This function shows the valid entries in readable format,
+ * read directly from hardware.
+ * @param node    node number
+ */
+void cvmx_pki_show_valid_pcam_entries(int node);
+
+/**
+ * This function shows the pkind attributes in readable format,
+ * read directly from hardware.
+ * @param node    node number
+ */
+void cvmx_pki_show_pkind_attributes(int node, int pkind);
+
+/**
+ * @INTERNAL
+ * This function is called by cvmx_helper_shutdown() to extract
+ * all FPA buffers out of the PKI. After this function
+ * completes, all FPA buffers that were prefetched by PKI
+ * wil be in the apropriate FPA pool. This functions does not reset
+ * PKI.
+ * WARNING: It is very important that PKI be
+ * reset soon after a call to this function.
+ */
+void __cvmx_pki_free_ptr(int node);
 
 #ifdef	__cplusplus
 /* *INDENT-OFF* */
diff --git a/arch/mips/include/asm/octeon/cvmx-pko3.h b/arch/mips/include/asm/octeon/cvmx-pko3.h
index d1362051e..84ebac6 100644
--- a/arch/mips/include/asm/octeon/cvmx-pko3.h
+++ b/arch/mips/include/asm/octeon/cvmx-pko3.h
@@ -313,14 +313,14 @@ union cvmx_pko_lmtdma_data {
 typedef union cvmx_pko_lmtdma_data cvmx_pko_lmtdma_data_t;
 
 /* per-core DQ depth cached value */
-extern int32_t __cvmx_pko3_dq_depth[1024];
+extern CVMX_TLS int32_t __cvmx_pko3_dq_depth[1024];
 
 extern int cvmx_pko3_internal_buffer_count(unsigned node);
 
 /*
  * PKO descriptor queue operation error string
  *
- * @param dqstatus is the enumeration returned from hardware, 
+ * @param dqstatus is the enumeration returned from hardware,
  * 	  PKO_QUERY_RTN_S[DQSTATUS].
  *
  * @return static constant string error description
@@ -409,7 +409,7 @@ cvmx_pko3_cvmseg_addr(void)
 }
 
 /*
- * @INTERNAL  
+ * @INTERNAL
  * Deliver PKO SEND commands via CVMSEG LM and LMTDMA/LMTTST.
  * The command should be already stored in the CVMSEG address.
  *
@@ -422,7 +422,7 @@ cvmx_pko3_cvmseg_addr(void)
  *
  * NOTE: Internal use only.
  */
-static inline cvmx_pko_query_rtn_t 
+static inline cvmx_pko_query_rtn_t
 __cvmx_pko3_lmtdma(uint8_t node, uint16_t dq, unsigned numwords)
 {
 	const enum cvmx_pko_dqop dqop = CVMX_PKO_DQ_SEND;
@@ -435,7 +435,7 @@ __cvmx_pko3_lmtdma(uint8_t node, uint16_t dq, unsigned numwords)
 	pko_status.u64 = 0;
 
 	/* LMTDMA address offset is (nWords-1) */
-	dma_addr = 0xffffffffffffa400ull; 
+	dma_addr = CVMX_LMTDMA_ORDERED_IO_ADDR;
 	dma_addr += (numwords - 1) << 3;
 
 #ifdef	CVMX_PKO3_DQ_MAX_DEPTH
@@ -481,14 +481,14 @@ __cvmx_pko3_lmtdma(uint8_t node, uint16_t dq, unsigned numwords)
 	pko_send_dma_data.s.dqop = dqop;
 	pko_send_dma_data.s.dq = dq;
 
-	/* Push all data into CVMSEG LM */
+	/* Barrier: make sure all prior writes complete before the following */
 	CVMX_SYNCWS;
 
 	/* issue PKO DMA */
 	cvmx_write64_uint64(dma_addr, pko_send_dma_data.u64);
 
 	if (cvmx_unlikely(pko_send_dma_data.s.rtnlen)) {
-		/* Wait for completion */
+		/* Wait for LMTDMA completion */
 		CVMX_SYNCIOBDMA;
 
 		/* Retreive real result */
@@ -510,13 +510,13 @@ __cvmx_pko3_lmtdma(uint8_t node, uint16_t dq, unsigned numwords)
 
 
 /*
- * @INTERNAL  
+ * @INTERNAL
  * Sends PKO descriptor commands via CVMSEG LM and LMTDMA.
  * @param node is the destination node
  * @param dq is the destonation descriptor queue.
  * @param cmds[] is an array of 64-bit PKO3 headers/subheaders
  * @param numworkds is the number of outgoing words
- * @param dqop is the operation code 
+ * @param dqop is the operation code
  * @return the PKO3 native query result structure.
  *
  * <numwords> must be between 1 and 15 for CVMX_PKO_DQ_SEND command
@@ -524,7 +524,7 @@ __cvmx_pko3_lmtdma(uint8_t node, uint16_t dq, unsigned numwords)
  *
  * NOTE: Internal use only.
  */
-static inline cvmx_pko_query_rtn_t 
+static inline cvmx_pko_query_rtn_t
 __cvmx_pko3_do_dma(uint8_t node, uint16_t dq, uint64_t cmds[],
 	unsigned numwords, enum cvmx_pko_dqop dqop)
 {
@@ -538,10 +538,10 @@ __cvmx_pko3_do_dma(uint8_t node, uint16_t dq, uint64_t cmds[],
 
 	/* With 0 data to send, this is an IOBDMA, else LMTDMA operation */
 	if(numwords == 0) {
-		dma_addr = 0xffffffffffffa200ull;
+		dma_addr = CVMX_IOBDMA_ORDERED_IO_ADDR;
 	} else {
 		/* LMTDMA address offset is (nWords-1) */
-		dma_addr = 0xffffffffffffa400ull; 
+		dma_addr = CVMX_LMTDMA_ORDERED_IO_ADDR;
 		dma_addr += (numwords - 1) << 3;
 	}
 
@@ -592,14 +592,14 @@ __cvmx_pko3_do_dma(uint8_t node, uint16_t dq, uint64_t cmds[],
 	pko_send_dma_data.s.dqop = dqop;
 	pko_send_dma_data.s.dq = dq;
 
-	/* Push all data into CVMSEG LM */
+	/* Barrier: make sure all prior writes complete before the following */
 	CVMX_SYNCWS;
 
 	/* issue PKO DMA */
 	cvmx_write64_uint64(dma_addr, pko_send_dma_data.u64);
 
 	if (pko_send_dma_data.s.rtnlen) {
-		/* Wait for completion */
+		/* Wait LMTDMA for completion */
 		CVMX_SYNCIOBDMA;
 
 		/* Retreive real result */
@@ -625,7 +625,7 @@ __cvmx_pko3_do_dma(uint8_t node, uint16_t dq, uint64_t cmds[],
  *
  * @INTERNAL
  *
- * @param dq is a global destination queue number 
+ * @param dq is a global destination queue number
  * @param pki_ptr specifies packet first linked pointer as returned from
  * 'cvmx_wqe_get_pki_pkt_ptr()'.
  * @param len is the total number of bytes in the packet.
@@ -658,6 +658,10 @@ cvmx_pko3_xmit_link_buf(int dq,cvmx_buf_ptr_pki_t pki_ptr,
 	hdr_s.s.df = (gaura < 0);
 	hdr_s.s.ii = 1;
 	hdr_s.s.aura = (gaura >= 0)? gaura: 0;
+#ifdef __LITTLE_ENDIAN_BITFIELD
+	hdr_s.s.le = 1;
+	cvmx_wqe_pko_errata_22235(pki_ptr, len);
+#endif
 
 	/* Fill in gather */
 	gtr_s.u64 = 0;
@@ -707,7 +711,7 @@ static inline unsigned __cvmx_pko3_aura_get(unsigned node)
 
 	pko_aura.u64 = cvmx_read_csr_node(node, CVMX_PKO_DPFI_FPA_AURA);
 
-	aura =  pko_aura.s.node << 10 | pko_aura.s.laura;
+	aura =  (pko_aura.s.node << 10) | pko_aura.s.laura;
 	return aura;
 }
 
diff --git a/arch/mips/include/asm/octeon/cvmx-pow.h b/arch/mips/include/asm/octeon/cvmx-pow.h
index 68e589c..f4b32c8 100644
--- a/arch/mips/include/asm/octeon/cvmx-pow.h
+++ b/arch/mips/include/asm/octeon/cvmx-pow.h
@@ -764,6 +764,44 @@ typedef union {
 #endif
 	} s_sstatus1_cn68xx;
 
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t pend_switch:1;
+					    /**< Set when there is a pending non-UNSCHEDULED SWTAG or
+					SWTAG_FULL, and the SSO entry has not left the list for the original tag. */
+		uint64_t pend_get_work:1;
+					    /**< Set when there is a pending GET_WORK */
+		uint64_t pend_get_work_wait:1;
+					    /**< when pend_get_work is set, this biit indicates that the
+					 wait bit was set. */
+		uint64_t pend_nosched:1;
+					    /**< Set when nosched is desired and pend_desched is set. */
+		uint64_t pend_nosched_clr:1;
+					    /**< Set when there is a pending CLR_NSCHED. */
+		uint64_t pend_desched:1;
+					    /**< Set when there is a pending DESCHED or SWTAG_DESCHED. */
+		uint64_t pend_alloc_we:1;
+					    /**< Set when there is a pending ALLOC_WE. */
+		uint64_t reserved_56:1;
+		uint64_t prep_index:12;
+		uint64_t reserved_42_43:2;
+		uint64_t pend_tag:42;
+					    /**< This is the wqp when pend_nosched_clr is set. */
+#else
+		uint64_t pend_tag:42;
+		uint64_t reserved_42_43:2;
+		uint64_t prep_index:12;
+		uint64_t reserved_56:1;
+		uint64_t pend_prep:1;
+		uint64_t pend_alloc_we:1;
+		uint64_t pend_desched:1;
+		uint64_t pend_nosched_clr:1;
+		uint64_t pend_nosched:1;
+		uint64_t pend_get_work_wait:1;
+		uint64_t pend_get_work:1;
+		uint64_t pend_switch:1;
+#endif
+	} s_sso_ppx_pendwqp_cn78xx;
     /**
      * Result for a POW Status Load (when get_cur==1, get_wqp==0, and get_rev==0)
      */
@@ -827,6 +865,30 @@ typedef union {
 #endif
 	} s_sstatus2_cn68xx;
 
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t tailc:1;
+		uint64_t reserved_60_62:3;
+		uint64_t index:12;
+		uint64_t reserved_46_47:2;
+		uint64_t grp:10;
+		uint64_t head:1;
+		uint64_t tail:1;
+		uint64_t tt:2;
+		uint64_t tag:32;
+#else
+		uint64_t tag:32;
+		uint64_t tt:2;
+		uint64_t tail:1;
+		uint64_t head:1;
+		uint64_t grp:10;
+		uint64_t reserved_46_47:2;
+		uint64_t index:12;
+		uint64_t reserved_60_62:3;
+		uint64_t tailc:1;
+#endif
+	} s_sso_ppx_tag_cn78xx;
+
     /**
      * Result for a POW Status Load (when get_cur==1, get_wqp==0, and get_rev==1)
      */
@@ -881,6 +943,20 @@ typedef union {
 #endif
 	} s_sstatus3_cn68xx;
 
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t reserved_58_63:6;
+		uint64_t grp:10;
+		uint64_t reserved_42_47:6;
+		uint64_t tag:42;
+#else
+		uint64_t tag:42;
+		uint64_t reserved_42_47:6;
+		uint64_t grp:10;
+		uint64_t reserved_58_63:6;
+#endif
+	} s_sso_ppx_wqp_cn78xx;
+
     /**
      * Result for a POW Status Load (when get_cur==1, get_wqp==1, and get_rev==0)
      */
@@ -936,6 +1012,33 @@ typedef union {
 #endif
 	} s_sstatus4_cn68xx;
 
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t tailc:1;
+		uint64_t reserved_60_62:3;
+		uint64_t index:12;
+		uint64_t reserved_38_47:10;
+		uint64_t grp:10;
+		uint64_t head:1;
+		uint64_t tail:1;
+		uint64_t reserved_25:1;
+		uint64_t revlink_index:12;
+		uint64_t link_index_vld:1;
+		uint64_t link_index:12;
+#else
+		uint64_t link_index:12;
+		uint64_t link_index_vld:1;
+		uint64_t revlink_index:12;
+		uint64_t reserved_25:1;
+		uint64_t tail:1;
+		uint64_t head:1;
+		uint64_t grp:10;
+		uint64_t reserved_38_47:10;
+		uint64_t index:12;
+		uint64_t reserved_60_62:3;
+		uint64_t tailc:1;
+#endif
+	} s_sso_ppx_links_cn78xx;
     /**
      * Result for a POW Status Load (when get_cur==1, get_wqp==1, and get_rev==1)
      */
@@ -1004,6 +1107,23 @@ typedef union {
 #endif
 	} s_smemload0_cn68xx;
 
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t reserved_39_63:25;
+		uint64_t tail:1;	/**< Set when this SSO entry is at
+					the tail of its tag list (also set
+					when in the NULL or NULL_NULL state). */
+		uint64_t reserved_34_36:3;
+		uint64_t tag_type:2;	    /**< The tag type of the SSO entry. */
+		uint64_t tag:32;	    /**< The tag of the SSO entry. */
+#else
+		uint64_t tag:32;
+		uint64_t tag_type:2;
+		uint64_t reserved_34_36:3;
+		uint64_t tail:1;
+		uint64_t reserved_38_63:26;
+#endif
+	} s_sso_iaq_ppx_tag_cn78xx;
     /**
      * Result For POW Memory Load (get_des == 0 and get_wqp == 1)
      */
@@ -1044,6 +1164,26 @@ typedef union {
 #endif
 	} s_smemload1_cn68xx;
 
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t reserved_48_63:2;
+		uint64_t head:1;
+		uint64_t nosched:1;	    /**< The nosched bit for the SSO entry. */
+		uint64_t reserved_58_59:2;
+		uint64_t grp:10;		    /**< The group of the SSO entry. */
+		uint64_t reserved_42_47:6;
+		uint64_t wqp:42;	    /**< The WQP held in the SSO entry. */
+#else
+		uint64_t wqp:42;	    /**< The WQP held in the SSO entry. */
+		uint64_t reserved_42_47:6;
+		uint64_t grp:10;		    /**< The group of the SSO entry. */
+		uint64_t reserved_58_59:2;
+		uint64_t nosched:1;	    /**< The nosched bit for the SSO entry. */
+		uint64_t head:1;
+		uint64_t reserved_48_63:2;
+#endif
+	} s_sso_ientx_wqpgrp_cn78xx;
+
     /**
      * Result For POW Memory Load (get_des == 1)
      */
@@ -1094,6 +1234,34 @@ typedef union {
 #endif
 	} s_smemload2_cn68xx;
 
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t pend_switch		:1;  /**< Set when there is a pending SWTAG, SWTAG_DESCHED, or SWTAG_FULL to ORDERED or ATOMIC. If
+					the register read was issued after an indexed GET_WORK, the DESCHED portion of a
+					SWTAG_DESCHED cannot still be pending. */
+	uint64_t pend_get_work		:1;  /**< Set when there is a pending GET_WORK. */
+	uint64_t pend_get_work_wait	:1;  /**< When PEND_GET_WORK is set, indicates that the WAITW bit was set. */
+	uint64_t pend_nosched		:1;  /**< Set when nosched is desired and PEND_DESCHED is set. */
+	uint64_t pend_nosched_clr	:1;  /**< Set when there is a pending CLR_NSCHED. */
+	uint64_t pend_desched		:1;  /**< Set when there is a pending DESCHED or SWTAG_DESCHED. */
+	uint64_t pend_alloc_we		:1;  /**< Set when there is a pending ALLOC_WE. */
+	uint64_t reserved_34_56		:23;
+	uint64_t pend_tt		:2;  /**< The tag type when PEND_SWITCH is set. */
+	uint64_t pend_tag		:32; /**< The tag when PEND_SWITCH is set. */
+#else
+	uint64_t pend_tag		:32;
+	uint64_t pend_tt		:2;
+	uint64_t reserved_34_56		:23;
+	uint64_t pend_alloc_we		:1;
+	uint64_t pend_desched		:1;
+	uint64_t pend_nosched_clr	:1;
+	uint64_t pend_nosched		:1;
+	uint64_t pend_get_work_wait	:1;
+	uint64_t pend_get_work		:1;
+	uint64_t pend_switch		:1;
+#endif
+	} s_sso_ppx_pendtag_cn78xx;
+
     /**
      * Result For SSO Memory Load (opcode is ML_LINKS)
      */
@@ -1101,11 +1269,11 @@ typedef union {
 #ifdef __BIG_ENDIAN_BITFIELD
 		uint64_t reserved_24_63:40;
 		uint64_t fwd_index:11;
-					    /**< The next entry in the tag list connected to the descheduled head. */
+		/**< The next entry in the tag list connected to the descheduled head. */
 		uint64_t reserved_11_12:2;
 		uint64_t next_index:11;
-					    /**< The next entry in the input, free, descheduled_head list
-                                                 (unpredicatble if entry is the tail of the list). */
+		/**< The next entry in the input, free, descheduled_head list
+		(unpredicatble if entry is the tail of the list). */
 #else
 		uint64_t next_index:11;
 		uint64_t reserved_11_12:2;
@@ -1114,6 +1282,28 @@ typedef union {
 #endif
 	} s_smemload3_cn68xx;
 
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_28_63		:36;
+	uint64_t prev_index		:12;
+	/**< The previous entry in the tag chain. Unpredictable if the entry
+	 * is at the head of the list or the head of a conflicted tag chain. */
+	uint64_t reserved_13_15		:3;
+	uint64_t next_index_vld		:1;
+	/**< The NEXT_INDEX is valid. Unpredictable unless the entry is the
+	 * tail entry of an atomic tag chain. */
+	uint64_t next_index		:12;
+	/**< The next entry in the tag chain or conflicted tag chain.
+	 * Unpredictable if the entry is at the tail of the list. */
+#else
+	uint64_t next_index		:12;
+	uint64_t next_index_vld		:1;
+	uint64_t reserved_13_15		:3;
+	uint64_t prev_index		:12;
+	uint64_t reserved_28_63		:36;
+#endif
+	} s_sso_ientx_links_cn78xx;
+
     /**
      * Result For POW Index/Pointer Load (get_rmt == 0/get_des_get_tail == 0)
      */
@@ -1154,7 +1344,8 @@ typedef union {
 	} sindexload0;
 
     /**
-     * Result for SSO Index/Pointer Load(opcode == IPL_IQ/IPL_DESCHED/IPL_NOSCHED)
+     * Result for SSO Index/Pointer Load(opcode ==
+     * IPL_IQ/IPL_DESCHED/IPL_NOSCHED)
      */
 	struct {
 #ifdef __BIG_ENDIAN_BITFIELD
@@ -1569,14 +1760,23 @@ static inline void cvmx_pow_tag_sw_wait(void)
 {
 	const uint64_t MAX_CYCLES = 1ull << 31;
 	uint64_t switch_complete;
-	uint64_t start_cycle = cvmx_get_cycle();
+	uint64_t start_cycle;
+
+	if (CVMX_ENABLE_POW_CHECKS)
+		start_cycle = cvmx_get_cycle();
+
 	while (1) {
 		CVMX_MF_CHORD(switch_complete);
-		if (cvmx_unlikely(switch_complete))
+		if (cvmx_likely(switch_complete))
 			break;
-		if (cvmx_unlikely(cvmx_get_cycle() > start_cycle + MAX_CYCLES)) {
-			cvmx_dprintf("WARNING: Tag switch is taking a long time, possible deadlock\n");
-			start_cycle = -MAX_CYCLES - 1;
+		if (CVMX_ENABLE_POW_CHECKS) {
+			if (cvmx_unlikely(cvmx_get_cycle() >
+				(start_cycle + MAX_CYCLES))) {
+				cvmx_dprintf("WARNING: %s:"
+					"Tag switch is taking a long time, "
+					"possible deadlock\n", __func__);
+			start_cycle += MAX_CYCLES - 1;
+			}
 		}
 	}
 }
@@ -3014,6 +3214,7 @@ extern void cvmx_pow_display(void *buffer, int buffer_size);
  * @return Number of POW entries
  */
 extern int cvmx_pow_get_num_entries(void);
+extern int cvmx_pow_get_dump_size(void);
 
 /**
  * This will allocate count number of SSO groups on the specified node to the
diff --git a/arch/mips/include/asm/octeon/cvmx-qlm.h b/arch/mips/include/asm/octeon/cvmx-qlm.h
index e8b9633..59d28f8 100644
--- a/arch/mips/include/asm/octeon/cvmx-qlm.h
+++ b/arch/mips/include/asm/octeon/cvmx-qlm.h
@@ -42,7 +42,7 @@
  *
  * Helper utilities for qlm.
  *
- * <hr>$Revision: 98310 $<hr>
+ * <hr>$Revision: 102508 $<hr>
  */
 
 #ifndef __CVMX_QLM_H__
@@ -160,6 +160,7 @@ extern void __cvmx_qlm_pcie_cfg_rxd_set_tweak(int qlm, int lane);
  * @return Speed in Mhz
  */
 extern int cvmx_qlm_get_gbaud_mhz(int qlm);
+extern int cvmx_qlm_get_gbaud_mhz_node(int node, int qlm);
 
 enum cvmx_qlm_mode {
 	CVMX_QLM_MODE_DISABLED = -1,
@@ -227,6 +228,7 @@ enum cvmx_pemx_cfg_mode {
 extern enum cvmx_qlm_mode cvmx_qlm_get_mode(int qlm);
 enum cvmx_qlm_mode cvmx_qlm_get_mode_cn78xx(int node, int qlm);
 extern enum cvmx_qlm_mode cvmx_qlm_get_dlm_mode(int dlm_mode, int interface);
+extern void __cvmx_qlm_set_mult(int qlm, int baud_mhz, int old_multiplier);
 
 extern void cvmx_qlm_display_registers(int qlm);
 
diff --git a/arch/mips/include/asm/octeon/cvmx-range.h b/arch/mips/include/asm/octeon/cvmx-range.h
index 71b7f08..351985e 100644
--- a/arch/mips/include/asm/octeon/cvmx-range.h
+++ b/arch/mips/include/asm/octeon/cvmx-range.h
@@ -43,11 +43,14 @@
 
 extern int  cvmx_range_init(uint64_t range_addr, int size);
 extern int  cvmx_range_alloc(uint64_t range_addr, uint64_t owner, uint64_t cnt, int align);
+extern int  cvmx_range_alloc_ordered(uint64_t range_addr, uint64_t owner,
+				     uint64_t cnt, int align, int reverse);
 extern int  cvmx_range_alloc_non_contiguos(uint64_t range_addr, uint64_t owner, uint64_t cnt,
 					   int elements[]);
 extern int  cvmx_range_reserve(uint64_t range_addr, uint64_t owner, uint64_t base, uint64_t cnt);
 extern int  cvmx_range_free_with_base(uint64_t range_addr, int base, int cnt);
 extern int  cvmx_range_free_with_owner(uint64_t range_addr, uint64_t owner);
+extern uint64_t cvmx_range_get_owner(uint64_t range_addr, uint64_t base);
 extern void cvmx_range_show(uint64_t range_addr);
 extern int  cvmx_range_memory_size(int nelements);
 extern int cvmx_range_free_mutiple(uint64_t range_addr, int bases[], int count);
diff --git a/arch/mips/include/asm/octeon/cvmx-wqe.h b/arch/mips/include/asm/octeon/cvmx-wqe.h
index 2ddabe0..aba88f3 100644
--- a/arch/mips/include/asm/octeon/cvmx-wqe.h
+++ b/arch/mips/include/asm/octeon/cvmx-wqe.h
@@ -859,6 +859,8 @@ typedef union {
 	};
 } cvmx_pki_wqe_word1_t;
 
+#define pki_errata20776 word1.rsvd_0
+
 typedef union {
 	uint64_t u64;
 	struct {
@@ -1465,22 +1467,6 @@ static inline int cvmx_wqe_is_vlan_stacked(cvmx_wqe_t *work)
 }
 
 /**
- * @INTERNAL
- *
- * Extract NO_WPTR mode from PIP/IPD register
- */
-static inline int __cvmx_ipd_mode_no_wptr(void)
-{
-	cvmx_ipd_ctl_status_t ipd_ctl_status;
-
-	if (!octeon_has_feature(OCTEON_FEATURE_NO_WPTR))
-		return 0;
-
-	ipd_ctl_status.u64 = cvmx_read_csr(CVMX_IPD_CTL_STATUS);
-	return (ipd_ctl_status.s.no_wptr);
-}
-
-/**
  * Extract packet data buffer pointer from work queue entry.
  *
  * Returns the legacy (Octeon1/Octeon2) buffer pointer structure
@@ -1503,7 +1489,8 @@ static inline int cvmx_wqe_get_bufs(cvmx_wqe_t *work)
 	else {
 #ifndef CVMX_BUILD_FOR_LINUX_KERNEL
 		/* Adjust for packet-in-WQE cases */
-		if (work->word2.s_cn38xx.bufs == 0 && !work->word2.s.software)
+		if (cvmx_unlikely(work->word2.s_cn38xx.bufs == 0 &&
+		    !work->word2.s.software))
 			(void) cvmx_wqe_get_packet_ptr(work);
 #endif
 		bufs = work->word2.s_cn38xx.bufs;
@@ -1552,28 +1539,57 @@ extern cvmx_wqe_t * cvmx_wqe_soft_create(void *data_p, unsigned data_sz);
    here */
 static inline void cvmx_wqe_pki_errata_20776(cvmx_wqe_t *work)
 {
-        cvmx_wqe_78xx_t *wqe = (cvmx_wqe_78xx_t*) work;
-
-        if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X) && !wqe->pki_wqe_translated)
-        {
-                uint64_t bufs;
-                cvmx_buf_ptr_pki_t buffer_next;
-
-                bufs = wqe->word0.bufs;
-                buffer_next = wqe->packet_ptr;
-                while (bufs > 1) {
-                        cvmx_buf_ptr_pki_t next;
-                        void *nextaddr = cvmx_phys_to_ptr(buffer_next.addr - 8);
-                        memcpy (&next, nextaddr, sizeof(next));
-                        next.u64 = __builtin_bswap64(next.u64);
-                        memcpy (nextaddr, &next, sizeof(next));
-                        buffer_next = next;
-                        bufs--;
-                }
-        }
-        wqe->pki_wqe_translated = 1;
+#ifndef __LITTLE_ENDIAN_BITFIELD
+	cvmx_wqe_78xx_t *wqe = (cvmx_wqe_78xx_t*) work;
+
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X) && !wqe->pki_errata20776)
+	{
+		uint64_t bufs;
+		cvmx_buf_ptr_pki_t buffer_next;
+
+		bufs = wqe->word0.bufs;
+		buffer_next = wqe->packet_ptr;
+		while (bufs > 1) {
+			cvmx_buf_ptr_pki_t next;
+			void *nextaddr = cvmx_phys_to_ptr(buffer_next.addr - 8);
+			memcpy (&next, nextaddr, sizeof(next));
+			next.u64 = __builtin_bswap64(next.u64);
+			memcpy (nextaddr, &next, sizeof(next));
+			buffer_next = next;
+			bufs--;
+		}
+		wqe->pki_errata20776 = 1;
+	}
+#endif
+}
+
+/* Errata (PKO-22235) PKI_BUFLINK_S's are endian-swapped
+   CN78XX pass 1.x has a bug where the packet pointer in each segment is
+   written in the opposite endianness of the configured mode. Fix these
+   here */
+static inline void cvmx_wqe_pko_errata_22235(cvmx_buf_ptr_pki_t packet_ptr, unsigned len)
+{
+#ifdef __LITTLE_ENDIAN_BITFIELD
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X))
+	{
+		unsigned total_len = 0;
+		cvmx_buf_ptr_pki_t buffer_next;
+		buffer_next = packet_ptr;
+		total_len += packet_ptr.size;
+		while (total_len < len) {
+			cvmx_buf_ptr_pki_t next;
+			void *nextaddr = cvmx_phys_to_ptr(buffer_next.addr - 8);
+			buffer_next.u64 = *(unsigned long long*)(nextaddr);
+			total_len += buffer_next.size;
+			memcpy(&next, nextaddr, sizeof(next));
+			next.u64 = __builtin_bswap64(next.u64);
+			memcpy(nextaddr, &next, sizeof(next));
+		}
+	}
+#endif
 }
 
+
 /**
  * @INTERNAL
  *
@@ -1583,13 +1599,13 @@ static inline void cvmx_wqe_pki_errata_20776(cvmx_wqe_t *work)
  */
 static inline cvmx_buf_ptr_pki_t cvmx_wqe_get_pki_pkt_ptr(cvmx_wqe_t *work)
 {
-        cvmx_wqe_78xx_t * wqe = (cvmx_wqe_78xx_t *) work;
-        if (!octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
-                cvmx_buf_ptr_pki_t x = {0};
-                return x;
-        }
-        cvmx_wqe_pki_errata_20776(work);
-        return wqe->packet_ptr;
+	cvmx_wqe_78xx_t * wqe = (cvmx_wqe_78xx_t *) work;
+	if (!octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
+		cvmx_buf_ptr_pki_t x = {0};
+		return x;
+	}
+	cvmx_wqe_pki_errata_20776(work);
+	return wqe->packet_ptr;
 }
 /**
  * Set the buffer segment count for a packet.
@@ -1621,7 +1637,7 @@ static inline unsigned cvmx_wqe_get_l3_offset(cvmx_wqe_t *work)
 	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
 		cvmx_wqe_78xx_t *wqe = (void *)work;
 		/* Match 4 values: IPv4/v6 w/wo options */
-	        if ((wqe->word2.lc_hdr_type & 0x1c) == CVMX_PKI_LTYPE_E_IP4)
+		if ((wqe->word2.lc_hdr_type & 0x1c) == CVMX_PKI_LTYPE_E_IP4)
 			return wqe->word4.ptr_layer_c;
 	} else {
 		return work->word2.s.ip_offset;
@@ -1644,7 +1660,7 @@ static inline unsigned cvmx_wqe_set_l3_offset(cvmx_wqe_t *work, unsigned ip_off)
 	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
 		cvmx_wqe_78xx_t *wqe = (void *)work;
 		/* Match 4 values: IPv4/v6 w/wo options */
-	        if ((wqe->word2.lc_hdr_type & 0x1c) == CVMX_PKI_LTYPE_E_IP4)
+		if ((wqe->word2.lc_hdr_type & 0x1c) == CVMX_PKI_LTYPE_E_IP4)
 			wqe->word4.ptr_layer_c = ip_off;
 	} else {
 		work->word2.s.ip_offset = ip_off;
diff --git a/arch/mips/include/asm/octeon/cvmx.h b/arch/mips/include/asm/octeon/cvmx.h
index dc8763d..6982c3b 100644
--- a/arch/mips/include/asm/octeon/cvmx.h
+++ b/arch/mips/include/asm/octeon/cvmx.h
@@ -34,6 +34,7 @@
 #include <linux/io.h>
 
 #define CVMX_SHARED
+#define CVMX_TLS
 
 /* These macros for use when using 32 bit pointers. */
 #define CVMX_MIPS32_SPACE_KSEG0 1l
diff --git a/arch/mips/include/asm/octeon/octeon-boot-info.h b/arch/mips/include/asm/octeon/octeon-boot-info.h
index 0c1d8d9..c0c6df5 100644
--- a/arch/mips/include/asm/octeon/octeon-boot-info.h
+++ b/arch/mips/include/asm/octeon/octeon-boot-info.h
@@ -57,7 +57,9 @@
 # include "cvmx-asm.h"
 #endif
 
-#define OCTEON_BOOT_MOVEABLE_MAGIC	0xDB00110ad358eace
+/* Currently only used by u-boot.  Applications use the value in
+ * cvmx-boot-vector.h */
+#define OCTEON_BOOT_MOVEABLE_MAGIC 0xdb00110ad358eace
 
 /** Offset of magic number in the boot bus moveable region */
 #define OCTEON_BOOT_MOVEABLE_MAGIC_OFFSET	0x70
diff --git a/arch/mips/include/asm/octeon/octeon-feature.h b/arch/mips/include/asm/octeon/octeon-feature.h
index 28b6436..b1615b1 100644
--- a/arch/mips/include/asm/octeon/octeon-feature.h
+++ b/arch/mips/include/asm/octeon/octeon-feature.h
@@ -1,5 +1,5 @@
 /***********************license start***************
- * Copyright (c) 2003-2013  Cavium Inc. (support@cavium.com). All rights
+ * Copyright (c) 2003-2014  Cavium Inc. (support@cavium.com). All rights
  * reserved.
  *
  *
@@ -170,6 +170,10 @@ typedef enum {
 				/**<  Octeon has OCLA */
 	OCTEON_FEATURE_FAU,
 				/**<  Octeon has FAU */
+	OCTEON_FEATURE_BGX_MIX,
+				/**<  Octeon has FAU */
+	OCTEON_FEATURE_HNA,
+				/**<  Octeon has HNA */
 	OCTEON_MAX_FEATURE
 } octeon_feature_t;
 
@@ -180,9 +184,9 @@ static inline int octeon_has_feature_OCTEON_FEATURE_SAAD(void)
 
 static inline int octeon_has_feature_OCTEON_FEATURE_ZIP(void)
 {
-	if ((OCTEON_IS_MODEL(OCTEON_CN30XX) || OCTEON_IS_MODEL(OCTEON_CN50XX)
+	if (OCTEON_IS_MODEL(OCTEON_CN30XX) || OCTEON_IS_MODEL(OCTEON_CN50XX)
 	     || OCTEON_IS_MODEL(OCTEON_CN52XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)
-	     || OCTEON_IS_MODEL(OCTEON_CN70XX)) && !OCTEON_IS_MODEL(OCTEON_CN78XX))
+	     || OCTEON_IS_MODEL(OCTEON_CN70XX))
 		return 0;
 	else
 		return !cvmx_fuse_read(121);
@@ -262,21 +266,24 @@ static inline int octeon_has_feature_OCTEON_FEATURE_LED_CONTROLLER(void)
 
 static inline int octeon_has_feature_OCTEON_FEATURE_TRA(void)
 {
-	return !(OCTEON_IS_MODEL(OCTEON_CN30XX) || OCTEON_IS_MODEL(OCTEON_CN50XX)
+	return !(OCTEON_IS_MODEL(OCTEON_CN30XX)
+		 || OCTEON_IS_MODEL(OCTEON_CN50XX)
 		 || OCTEON_IS_OCTEON3());
 }
 
 static inline int octeon_has_feature_OCTEON_FEATURE_MGMT_PORT(void)
 {
 	return (OCTEON_IS_MODEL(OCTEON_CN56XX)
-		|| OCTEON_IS_MODEL(OCTEON_CN52XX) || OCTEON_IS_MODEL(OCTEON_CN78XX)
+		|| OCTEON_IS_MODEL(OCTEON_CN52XX)
 		|| OCTEON_IS_OCTEON2());	/* OCTEON II or later */
 }
 
 static inline int octeon_has_feature_OCTEON_FEATURE_RAID(void)
 {
-	return OCTEON_IS_MODEL(OCTEON_CN56XX) || OCTEON_IS_MODEL(OCTEON_CN52XX)
-	       || !OCTEON_IS_OCTEON1PLUS() || OCTEON_IS_MODEL(OCTEON_CN78XX);	/* OCTEON II or later */
+	return OCTEON_IS_MODEL(OCTEON_CN56XX)
+		|| OCTEON_IS_MODEL(OCTEON_CN52XX)
+		|| OCTEON_IS_OCTEON2()
+		|| OCTEON_IS_OCTEON3();
 }
 
 static inline int octeon_has_feature_OCTEON_FEATURE_USB(void)
@@ -312,6 +319,14 @@ static inline int octeon_has_feature_OCTEON_FEATURE_HFA(void)
 		return !cvmx_fuse_read(90);
 }
 
+static inline int octeon_has_feature_OCTEON_FEATURE_HNA(void)
+{
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+		return !cvmx_fuse_read(134);
+	else
+		return 0;
+}
+
 static inline int octeon_has_feature_OCTEON_FEATURE_DFM(void)
 {
 	if (!(OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX)))
@@ -333,8 +348,8 @@ static inline int octeon_has_feature_OCTEON_FEATURE_NPEI(void)
 
 static inline int octeon_has_feature_OCTEON_FEATURE_PKND(void)
 {
-	return (OCTEON_IS_MODEL(OCTEON_CN68XX) ||
-		OCTEON_IS_MODEL(OCTEON_CN78XX));
+	return OCTEON_IS_MODEL(OCTEON_CN68XX)
+		|| OCTEON_IS_MODEL(OCTEON_CN78XX);
 }
 
 static inline int octeon_has_feature_OCTEON_FEATURE_CN68XX_WQE(void)
@@ -386,8 +401,7 @@ static inline int octeon_has_feature_OCTEON_FEATURE_MMC(void)
 {
 	return (OCTEON_IS_MODEL(OCTEON_CN61XX)
 		|| OCTEON_IS_MODEL(OCTEON_CNF71XX)
-		|| OCTEON_IS_MODEL(OCTEON_CN78XX)
-		|| OCTEON_IS_MODEL(OCTEON_CN70XX));
+		|| OCTEON_IS_OCTEON3());
 }
 
 static inline int octeon_has_feature_OCTEON_FEATURE_ROM(void)
@@ -436,29 +450,32 @@ static inline int octeon_has_feature_OCTEON_FEATURE_SPI(void)
 		|| OCTEON_IS_MODEL(OCTEON_CN66XX)
 		|| OCTEON_IS_MODEL(OCTEON_CN61XX)
 		|| OCTEON_IS_MODEL(OCTEON_CNF71XX)
-		|| OCTEON_IS_MODEL(OCTEON_CN70XX)
-		|| OCTEON_IS_MODEL(OCTEON_CN78XX));
+		|| OCTEON_IS_OCTEON3());
 }
 
 static inline int octeon_has_feature_OCTEON_FEATURE_PKI(void)
 {
-	return (OCTEON_IS_MODEL(OCTEON_CN78XX));
+	return OCTEON_IS_MODEL(OCTEON_CN78XX);
 }
 
 static inline int octeon_has_feature_OCTEON_FEATURE_PKO3(void)
 {
-	return (OCTEON_IS_MODEL(OCTEON_CN78XX));
+	return OCTEON_IS_MODEL(OCTEON_CN78XX);
 }
 
 static inline int octeon_has_feature_OCTEON_FEATURE_OCLA(void)
 {
-	return (OCTEON_IS_MODEL(OCTEON_CN70XX)
-		|| OCTEON_IS_MODEL(OCTEON_CN78XX));
+	return OCTEON_IS_OCTEON3();
 }
 
 static inline int octeon_has_feature_OCTEON_FEATURE_FAU(void)
 {
-	return !(OCTEON_IS_MODEL(OCTEON_CN78XX));
+	return !OCTEON_IS_MODEL(OCTEON_CN78XX);
+}
+
+static inline int octeon_has_feature_OCTEON_FEATURE_BGX_MIX(void)
+{
+	return OCTEON_IS_MODEL(OCTEON_CN78XX);
 }
 
 /*
diff --git a/arch/mips/include/asm/octeon/octeon-model.h b/arch/mips/include/asm/octeon/octeon-model.h
index f0c18f6..56ac429 100644
--- a/arch/mips/include/asm/octeon/octeon-model.h
+++ b/arch/mips/include/asm/octeon/octeon-model.h
@@ -43,7 +43,7 @@
  * File defining different Octeon model IDs and macros to
  * compare them.
  *
- * <hr>$Revision: 95942 $<hr>
+ * <hr>$Revision: 102289 $<hr>
  */
 
 #ifndef __OCTEON_MODEL_H__
@@ -101,6 +101,7 @@ extern "C" {
 
 #define OCTEON_CN70XX_PASS1_0   0x000d9600
 #define OCTEON_CN70XX_PASS1_1   0x000d9601
+#define OCTEON_CN70XX_PASS1_2   0x000d9602
 
 #define OCTEON_CN70XX_PASS2_0   0x000d9608
 
@@ -117,6 +118,8 @@ extern "C" {
 #define OCTEON_CN78XX_PASS1_X   (OCTEON_CN78XX_PASS1_0 | OM_IGNORE_MINOR_REVISION)
 #define OCTEON_CN78XX_PASS2_X   (OCTEON_CN78XX_PASS2_0 | OM_IGNORE_MINOR_REVISION)
 
+#define OCTEON_CN76XX		  (0x000d9540 | OM_CHECK_SUBMODEL)
+
 /*
  * CNF7XXX models with new revision encoding
  */
@@ -283,7 +286,7 @@ extern "C" {
 **     <4>:   alternate package
 **     <3:0>: revision
 **
-** CN5XXX:
+** CN5XXX and older models:
 **
 **     bits
 **     <7>:   reserved (0)
@@ -302,9 +305,9 @@ extern "C" {
 /* CN5XXX and later use different layout of bits in the revision ID field */
 #define OCTEON_58XX_FAMILY_MASK      OCTEON_38XX_FAMILY_MASK
 #define OCTEON_58XX_FAMILY_REV_MASK  0x00ffff3f
-#define OCTEON_58XX_MODEL_MASK       0x00ffffc0
+#define OCTEON_58XX_MODEL_MASK       0x00ffff40
 #define OCTEON_58XX_MODEL_REV_MASK   (OCTEON_58XX_FAMILY_REV_MASK | OCTEON_58XX_MODEL_MASK)
-#define OCTEON_58XX_MODEL_MINOR_REV_MASK (OCTEON_58XX_MODEL_REV_MASK & 0x00fffff8)
+#define OCTEON_58XX_MODEL_MINOR_REV_MASK (OCTEON_58XX_MODEL_REV_MASK & 0x00ffff38)
 #define OCTEON_5XXX_MODEL_MASK       0x00ff0fc0
 
 #define __OCTEON_MATCH_MASK__(x,y,z) (((x) & (z)) == ((y) & (z)))
@@ -336,10 +339,10 @@ extern "C" {
        && __OCTEON_MATCH_MASK__((chip_model), (arg_model), OCTEON_58XX_FAMILY_REV_MASK)) || \
      ((((arg_model) & (OM_FLAG_MASK)) == OM_IGNORE_MINOR_REVISION) \
        && __OCTEON_MATCH_MASK__((chip_model), (arg_model), OCTEON_58XX_MODEL_MINOR_REV_MASK)) || \
+     ((((arg_model) & (OM_FLAG_MASK)) == OM_CHECK_SUBMODEL) \
+       && __OCTEON_MATCH_MASK__((chip_model), (arg_model), OCTEON_58XX_MODEL_MASK)) || \
      ((((arg_model) & (OM_FLAG_MASK)) == OM_IGNORE_REVISION) \
        && __OCTEON_MATCH_MASK__((chip_model), (arg_model), OCTEON_58XX_FAMILY_MASK)) || \
-     ((((arg_model) & (OM_FLAG_MASK)) == OM_CHECK_SUBMODEL) \
-       && __OCTEON_MATCH_MASK__((chip_model), (arg_model), OCTEON_58XX_MODEL_REV_MASK)) || \
      ((((arg_model) & (OM_MATCH_5XXX_FAMILY_MODELS)) == OM_MATCH_5XXX_FAMILY_MODELS) \
        && ((chip_model & OCTEON_PRID_MASK) >= OCTEON_CN58XX_PASS1_0) && ((chip_model & OCTEON_PRID_MASK) < OCTEON_CN63XX_PASS1_0)) || \
      ((((arg_model) & (OM_MATCH_6XXX_FAMILY_MODELS)) == OM_MATCH_6XXX_FAMILY_MODELS) \
diff --git a/drivers/net/ethernet/octeon/ethernet-mem.c b/drivers/net/ethernet/octeon/ethernet-mem.c
index 78dc46d..b7a1772 100644
--- a/drivers/net/ethernet/octeon/ethernet-mem.c
+++ b/drivers/net/ethernet/octeon/ethernet-mem.c
@@ -217,6 +217,8 @@ int cvm_oct_alloc_fpa_pool(int pool, int size)
 	if (pool >= (int)ARRAY_SIZE(cvm_oct_pools) || size < 128)
 		return -EINVAL;
 
+	BUG_ON(octeon_has_feature(OCTEON_FEATURE_FPA3));
+
 	mutex_lock(&cvm_oct_pools_lock);
 
 	if (pool >= 0) {
@@ -233,7 +235,7 @@ int cvm_oct_alloc_fpa_pool(int pool, int size)
 			}
 		}
 		/* reserve/alloc fpa pool */
-		pool = cvmx_fpa_alloc_pool(pool);
+		pool = cvmx_fpa1_reserve_pool(pool);
 		if (pool < 0) {
 			ret = -EINVAL;
 			goto out;
@@ -249,7 +251,7 @@ int cvm_oct_alloc_fpa_pool(int pool, int size)
 			}
 
 		/* Alloc fpa pool */
-		pool = cvmx_fpa_alloc_pool(pool);
+		pool = cvmx_fpa1_reserve_pool(pool);
 		if (pool < 0) {
 			/* No empties. */
 			ret = -EINVAL;
@@ -277,7 +279,7 @@ int cvm_oct_alloc_fpa_pool(int pool, int size)
 		if (!cvm_oct_pools[pool].kmem) {
 			ret = -ENOMEM;
 			cvm_oct_pools[pool].pool = -1;
-			cvmx_fpa_release_pool(pool);
+			cvmx_fpa1_release_pool(pool);
 			goto out;
 		}
 	}
@@ -312,7 +314,7 @@ int cvm_oct_release_fpa_pool(int pool)
 	cvm_oct_pools[pool].users--;
 
 	if (cvm_oct_pools[pool].users == 0)
-		cvmx_fpa_release_pool(pool);
+		cvmx_fpa1_release_pool(pool);
 
 	ret = 0;
 out:
diff --git a/drivers/net/ethernet/octeon/ethernet.c b/drivers/net/ethernet/octeon/ethernet.c
index dda5101..cd94908 100644
--- a/drivers/net/ethernet/octeon/ethernet.c
+++ b/drivers/net/ethernet/octeon/ethernet.c
@@ -940,6 +940,14 @@ static int cvm_oct_probe(struct platform_device *pdev)
 				priv->num_tx_queues = cvmx_pko_get_num_queues(priv->ipd_port);
 			}
 
+			if (priv->num_tx_queues == 0) {
+				dev_err(&pdev->dev,
+					"tx_queue count not configured for port %d:%d\n",
+					interface, interface_port);
+				free_netdev(dev);
+				continue;
+			}
+
 			BUG_ON(priv->num_tx_queues < 1);
 			BUG_ON(priv->num_tx_queues > 32);
 
diff --git a/drivers/net/ethernet/octeon/octeon3-ethernet.c b/drivers/net/ethernet/octeon/octeon3-ethernet.c
index 589c7a6..b2539f6 100644
--- a/drivers/net/ethernet/octeon/octeon3-ethernet.c
+++ b/drivers/net/ethernet/octeon/octeon3-ethernet.c
@@ -111,11 +111,11 @@ struct octeon3_ethernet_node {
 	bool init_done;
 	int next_cpu_irq_affinity;
 	int numa_node;
-	int pki_packet_pool;
-	int sso_pool;
-	int pko_pool;
-	int sso_aura;
-	int pko_aura;
+	cvmx_fpa3_pool_t  pki_packet_pool;
+	cvmx_fpa3_pool_t sso_pool;
+	cvmx_fpa3_pool_t pko_pool;
+	cvmx_fpa3_gaura_t sso_aura;
+	cvmx_fpa3_gaura_t pko_aura;
 	int tx_complete_grp;
 	int tx_irq;
 	cpumask_t tx_affinity_hint;
@@ -157,7 +157,7 @@ static void octeon3_eth_gen_affinity(int node, cpumask_t *mask)
 	cpumask_set_cpu(cpu, mask);
 }
 
-static int octeon3_eth_fpa_pool_init(unsigned int node, unsigned int pool, int num_ptrs)
+static int octeon3_eth_fpa_pool_init(cvmx_fpa3_pool_t pool, int num_ptrs)
 {
 	void *pool_stack;
 	u64 pool_stack_start, pool_stack_end;
@@ -165,13 +165,13 @@ static int octeon3_eth_fpa_pool_init(unsigned int node, unsigned int pool, int n
 	union cvmx_fpa_poolx_cfg cfg;
 	int stack_size = (DIV_ROUND_UP(num_ptrs, 29) + 1) * 128;
 
-	cvmx_write_csr_node(node, CVMX_FPA_POOLX_CFG(pool), 0);
-	cvmx_write_csr_node(node, CVMX_FPA_POOLX_START_ADDR(pool), 128);
+	cvmx_write_csr_node(pool.node, CVMX_FPA_POOLX_CFG(pool.lpool), 0);
+	cvmx_write_csr_node(pool.node, CVMX_FPA_POOLX_START_ADDR(pool.lpool), 128);
 	limit_addr.u64 = 0;
 	limit_addr.cn78xx.addr = ~0ll;
-	cvmx_write_csr_node(node, CVMX_FPA_POOLX_END_ADDR(pool), limit_addr.u64);
+	cvmx_write_csr_node(pool.node, CVMX_FPA_POOLX_END_ADDR(pool.lpool), limit_addr.u64);
 
-	pool_stack = kmalloc_node(stack_size, GFP_KERNEL, node);
+	pool_stack = kmalloc_node(stack_size, GFP_KERNEL, pool.node);
 	if (!pool_stack)
 		return -ENOMEM;
 
@@ -179,30 +179,32 @@ static int octeon3_eth_fpa_pool_init(unsigned int node, unsigned int pool, int n
 	pool_stack_end = round_down(pool_stack_start + stack_size, 128);
 	pool_stack_start = round_up(pool_stack_start, 128);
 
-	cvmx_write_csr_node(node, CVMX_FPA_POOLX_STACK_BASE(pool), pool_stack_start);
-	cvmx_write_csr_node(node, CVMX_FPA_POOLX_STACK_ADDR(pool), pool_stack_start);
-	cvmx_write_csr_node(node, CVMX_FPA_POOLX_STACK_END(pool), pool_stack_end);
+	cvmx_write_csr_node(pool.node, CVMX_FPA_POOLX_STACK_BASE(pool.lpool), pool_stack_start);
+	cvmx_write_csr_node(pool.node, CVMX_FPA_POOLX_STACK_ADDR(pool.lpool), pool_stack_start);
+	cvmx_write_csr_node(pool.node, CVMX_FPA_POOLX_STACK_END(pool.lpool), pool_stack_end);
 
 	cfg.u64 = 0;
 	cfg.s.l_type = 2; /* Load with DWB */
 	cfg.s.ena = 1;
-	cvmx_write_csr_node(node, CVMX_FPA_POOLX_CFG(pool), cfg.u64);
+	cvmx_write_csr_node(pool.node, CVMX_FPA_POOLX_CFG(pool.lpool), cfg.u64);
 	return 0;
 }
 
-static int octeon3_eth_fpa_aura_init(unsigned int node, unsigned int pool, int aura, unsigned int limit)
+static int octeon3_eth_fpa_aura_init(cvmx_fpa3_pool_t pool, cvmx_fpa3_gaura_t aura, unsigned int limit)
 {
 	int shift;
 	union cvmx_fpa_aurax_cnt_levels cnt_levels;
 	unsigned int drop, pass;
 
+	BUG_ON(aura.node != pool.node);
+
 	limit *= 2; /* allow twice the limit before saturation at zero. */
 
-	cvmx_write_csr_node(node, CVMX_FPA_AURAX_CFG(aura), 0);
-	cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT_LIMIT(aura), limit);
-	cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT(aura), limit);
-	cvmx_write_csr_node(node, CVMX_FPA_AURAX_POOL(aura), pool);
-	cvmx_write_csr_node(node, CVMX_FPA_AURAX_POOL_LEVELS(aura), 0); /* No per-pool RED/Drop */
+	cvmx_write_csr_node(aura.node, CVMX_FPA_AURAX_CFG(aura.laura), 0);
+	cvmx_write_csr_node(aura.node, CVMX_FPA_AURAX_CNT_LIMIT(aura.laura), limit);
+	cvmx_write_csr_node(aura.node, CVMX_FPA_AURAX_CNT(aura.laura), limit);
+	cvmx_write_csr_node(aura.node, CVMX_FPA_AURAX_POOL(aura.laura), pool.lpool);
+	cvmx_write_csr_node(aura.node, CVMX_FPA_AURAX_POOL_LEVELS(aura.laura), 0); /* No per-pool RED/Drop */
 
 	shift = 0;
 	while ((limit >> shift) > 255)
@@ -216,7 +218,7 @@ static int octeon3_eth_fpa_aura_init(unsigned int node, unsigned int pool, int a
 	cnt_levels.s.red_ena = 1;
 	cnt_levels.s.drop = drop;
 	cnt_levels.s.pass = pass;
-	cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT_LEVELS(aura), cnt_levels.u64);
+	cvmx_write_csr_node(aura.node, CVMX_FPA_AURAX_CNT_LEVELS(aura.laura), cnt_levels.u64);
 
 	return 0;
 }
@@ -246,7 +248,7 @@ static int octeon3_eth_sso_init(unsigned int node, int aura)
 
 	for (i = 0; i < 256; i++) {
 		u64 phys;
-		void *mem = cvmx_fpa3_alloc_aura(node, aura);
+		void *mem = cvmx_fpa3_alloc(__cvmx_fpa3_gaura(node, aura));
 		if (!mem) {
 			rv = -ENOMEM;
 			goto err;
@@ -329,7 +331,9 @@ static void octeon3_eth_replentish_rx(struct octeon3_ethernet *priv, int count)
 		}
 		buf = (void **)PTR_ALIGN(skb->head, 128);
 		buf[0] = skb;
-		cvmx_fpa3_free_aura(buf, priv->numa_node, priv->pki_laura, 0);
+		cvmx_fpa3_free(buf,
+			__cvmx_fpa3_gaura(priv->numa_node, priv->pki_laura),
+			0);
 	}
 }
 
@@ -459,32 +463,33 @@ static int octeon3_eth_global_init(unsigned int node)
 	fpa_cfg.s.lvl_dly = 3;
 	cvmx_write_csr_node(node, CVMX_FPA_GEN_CFG, fpa_cfg.u64);
 
-	oen->sso_pool = cvmx_fpa_alloc_pool(-1);
-	if (oen->sso_pool < 0) {
+	oen->sso_pool = cvmx_fpa3_reserve_pool(node, -1);
+	if (!__cvmx_fpa3_pool_valid(oen->sso_pool)) {
 		rv = -ENODEV;
 		goto done;
 	}
-	oen->pko_pool = cvmx_fpa_alloc_pool(-1);
-	if (oen->pko_pool < 0) {
+	oen->pko_pool = cvmx_fpa3_reserve_pool(node, -1);
+	if (!__cvmx_fpa3_pool_valid(oen->pko_pool)) {
 		rv = -ENODEV;
 		goto done;
 	}
-	oen->pki_packet_pool = cvmx_fpa_alloc_pool(-1);
-	if (oen->pki_packet_pool < 0) {
+	oen->pki_packet_pool = cvmx_fpa3_reserve_pool(node, -1);
+	if (!__cvmx_fpa3_pool_valid(oen->pki_packet_pool)) {
 		rv = -ENODEV;
 		goto done;
 	}
-	oen->sso_aura = cvmx_fpa3_allocate_aura(node);
-	oen->pko_aura = cvmx_fpa3_allocate_aura(node);
+	oen->sso_aura = cvmx_fpa3_reserve_aura(node, -1);
+	oen->pko_aura = cvmx_fpa3_reserve_aura(node, -1);
 
 	pr_err("octeon3_eth_global_init  SSO:%d:%d, PKO:%d:%d\n",
-	       oen->sso_pool, oen->sso_aura,  oen->pko_pool, oen->pko_aura);
+	       oen->sso_pool.lpool, oen->sso_aura.laura,
+	       oen->pko_pool.lpool, oen->pko_aura.laura);
 
-	octeon3_eth_fpa_pool_init(node, oen->sso_pool, 40960);
-	octeon3_eth_fpa_pool_init(node, oen->pko_pool, 40960);
-	octeon3_eth_fpa_pool_init(node, oen->pki_packet_pool, 64 * num_packet_buffers);
-	octeon3_eth_fpa_aura_init(node, oen->sso_pool, oen->sso_aura, 20480);
-	octeon3_eth_fpa_aura_init(node, oen->pko_pool, oen->pko_aura, 20480);
+	octeon3_eth_fpa_pool_init(oen->sso_pool, 40960);
+	octeon3_eth_fpa_pool_init(oen->pko_pool, 40960);
+	octeon3_eth_fpa_pool_init(oen->pki_packet_pool, 64 * num_packet_buffers);
+	octeon3_eth_fpa_aura_init(oen->sso_pool, oen->sso_aura, 20480);
+	octeon3_eth_fpa_aura_init(oen->pko_pool, oen->pko_aura, 20480);
 
 	if (!octeon3_eth_sso_pko_cache) {
 		octeon3_eth_sso_pko_cache = kmem_cache_create("sso_pko", 4096, 128, 0, NULL);
@@ -500,16 +505,17 @@ static int octeon3_eth_global_init(unsigned int node)
 			rv = -ENOMEM;
 			goto done;
 		}
-		cvmx_fpa3_free_aura(mem, node, oen->sso_aura, 0);
+		cvmx_fpa3_free(mem, oen->sso_aura, 0);
 		mem = kmem_cache_alloc_node(octeon3_eth_sso_pko_cache, GFP_KERNEL, node);
 		if (!mem) {
 			rv = -ENOMEM;
 			goto done;
 		}
-		cvmx_fpa3_free_aura(mem, node, oen->pko_aura, 0);
+		cvmx_fpa3_free(mem, oen->pko_aura, 0);
 	}
 
-	rv = octeon3_eth_sso_init(node, oen->sso_aura);
+	BUG_ON(node != oen->sso_aura.node);
+	rv = octeon3_eth_sso_init(node, oen->sso_aura.laura);
 	if (rv)
 		goto done;
 
@@ -523,15 +529,16 @@ static int octeon3_eth_global_init(unsigned int node)
 	}
 
 	__cvmx_helper_init_port_config_data();
-	rv = __cvmx_helper_pko3_init_global(node, cvmx_fpa3_arua_to_guara(node, oen->pko_aura));
+	rv = __cvmx_helper_pko3_init_global(node, oen->pko_aura.laura | (node << 10));
 	if (rv) {
 		pr_err("cvmx_helper_pko3_init_global failed\n");
 		rv = -ENODEV;
 		goto done;
 	}
-	__cvmx_helper_pki_install_default_vlan(node);
+
+	__cvmx_helper_pki_install_dflt_vlan(node);
 	cvmx_pki_setup_clusters(node);
-	__cvmx_helper_pki_set_ltype_map(node);
+	__cvmx_helper_pki_set_dflt_ltype_map(node);
 	cvmx_pki_enable_backpressure(node);
 	cvmx_pki_parse_enable(node, 0);
 	cvmx_pki_enable(node);
@@ -757,6 +764,7 @@ static int octeon3_eth_ndo_init(struct net_device *netdev)
 	struct cvmx_xport xdq;
 	int r, i;
 	const u8 *mac;
+	cvmx_fpa3_gaura_t aura;
 
 	netif_carrier_off(netdev);
 
@@ -795,8 +803,9 @@ static int octeon3_eth_ndo_init(struct net_device *netdev)
 	xdq = cvmx_helper_ipd_port_to_xport(node_dq);
 
 	priv->pko_queue = xdq.port;
-	priv->pki_laura = cvmx_fpa3_allocate_aura(priv->numa_node);
-	octeon3_eth_fpa_aura_init(priv->numa_node, oen->pki_packet_pool, priv->pki_laura,
+	aura = cvmx_fpa3_reserve_aura(priv->numa_node, -1);
+	priv->pki_laura = aura.laura;
+	octeon3_eth_fpa_aura_init(oen->pki_packet_pool, aura,
 				  num_packet_buffers * 2);
 
 	base_rx_grp = -1;
@@ -825,7 +834,7 @@ static int octeon3_eth_ndo_init(struct net_device *netdev)
 	prt_schd->style = -1; /* Allocate net style per port */
 	prt_schd->qpg_base = -1;
 	prt_schd->aura_per_prt = true;
-	prt_schd->aura = priv->pki_laura;
+	prt_schd->aura_num = priv->pki_laura;
 	prt_schd->sso_grp_per_prt = true;
 	prt_schd->sso_grp = base_rx_grp;
 	prt_schd->qpg_qos = CVMX_PKI_QPG_QOS_NONE;
@@ -891,7 +900,7 @@ static int octeon3_eth_ndo_init(struct net_device *netdev)
 	pki_prt_cfg.style_cfg.tag_cfg.tag_fields.layer_f_src = 1;
 	pki_prt_cfg.style_cfg.tag_cfg.tag_fields.layer_g_src = 1;
 
-	cvmx_pki_config_port(ipd_port, &pki_prt_cfg);
+	cvmx_pki_set_port_config(ipd_port, &pki_prt_cfg);
 
 	i = 0;
 	while ((priv->num_rx_cxt & (1 << i)) == 0)
@@ -1024,7 +1033,7 @@ static int octeon3_eth_ndo_stop(struct net_device *netdev)
 
 	/* Free the packet buffers */
 	for (;;) {
-		w = cvmx_fpa3_alloc_aura(priv->numa_node, priv->pki_laura);
+		w = cvmx_fpa3_alloc(__cvmx_fpa3_gaura(priv->numa_node, priv->pki_laura));
 		if (!w)
 			break;
 		skb = w[0];
-- 
2.6.2

