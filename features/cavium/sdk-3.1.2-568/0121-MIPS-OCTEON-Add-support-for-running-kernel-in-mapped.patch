From 2f97effcf709bbd670678267e1fa152b1ecde073 Mon Sep 17 00:00:00 2001
From: David Daney <david.daney@cavium.com>
Date: Thu, 20 Sep 2012 11:19:46 -0700
Subject: [PATCH 121/974] MIPS: OCTEON: Add support for running kernel in
 mapped address space.

Allow the kernel to run in mapped address space via a wired TLB entry.
This allows the same kernel binary to run as more than a single
execution instance in the same system.  Also module function calls can
be done with a single instruction for higher performance.

Make __phys_addr work for addresses in mapped kernel images.
Make virt_to_phys() work for all unmapped addresses.

Signed-off-by: David Daney <david.daney@cavium.com>
Signed-off-by: Leonid Rosenboim <lrosenboim@caviumnetworks.com>
[Original patch taken from Cavium SDK 3.1.2-568]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
---
 arch/mips/Kconfig                                  |  18 ++++
 arch/mips/Makefile                                 |   3 +
 .../asm/mach-cavium-octeon/kernel-entry-init.h     | 111 +++++++++++++++++++++
 arch/mips/include/asm/page.h                       |  20 ++--
 arch/mips/include/asm/pgtable-64.h                 |   7 +-
 arch/mips/kernel/Makefile                          |   6 +-
 arch/mips/kernel/vmlinux.lds.S                     |  15 ++-
 arch/mips/mm/page.c                                |  23 +++++
 arch/mips/mm/tlb-r4k.c                             |   2 +
 arch/mips/mm/tlbex.c                               |  13 +++
 10 files changed, 206 insertions(+), 12 deletions(-)

diff --git a/arch/mips/Kconfig b/arch/mips/Kconfig
index c22942b..9ac2116 100644
--- a/arch/mips/Kconfig
+++ b/arch/mips/Kconfig
@@ -1820,6 +1820,24 @@ config CEVT_GIC
 	  detecting a GIC, it will fall back to the R4K timer for the
 	  generation of clock events.
 
+config MAPPED_KERNEL
+	bool "Mapped kernel"
+	depends on CPU_CAVIUM_OCTEON
+	help
+	  Select this option if you want the kernel's code and data to
+	  be in mapped memory.  The kernel will be mapped using a
+	  single wired TLB entry, thus reducing the number of
+	  available TLB entries by one.  Kernel modules will be able
+	  to use a more efficient calling convention.
+
+config PHYS_LOAD_ADDRESS
+	hex "Physical load address"
+	depends on MAPPED_KERNEL
+	default 0xffffffff81000000
+	help
+	  The physical load address reflected as the program header
+	  physical address in the kernel ELF image.
+
 config BOARD_SCACHE
 	bool
 
diff --git a/arch/mips/Makefile b/arch/mips/Makefile
index 5065bbb..da23eca 100644
--- a/arch/mips/Makefile
+++ b/arch/mips/Makefile
@@ -90,8 +90,11 @@ all-$(CONFIG_SYS_SUPPORTS_ZBOOT)+= vmlinuz
 cflags-y			+= -G 0 -mno-abicalls -fno-pic -pipe
 cflags-y			+= -msoft-float
 LDFLAGS_vmlinux			+= -G 0 -static -n -nostdlib
+
+ifndef CONFIG_MAPPED_KERNEL
 KBUILD_AFLAGS_MODULE		+= -mlong-calls
 KBUILD_CFLAGS_MODULE		+= -mlong-calls
+endif
 
 cflags-y += -ffreestanding
 
diff --git a/arch/mips/include/asm/mach-cavium-octeon/kernel-entry-init.h b/arch/mips/include/asm/mach-cavium-octeon/kernel-entry-init.h
index e08978f..e949799 100644
--- a/arch/mips/include/asm/mach-cavium-octeon/kernel-entry-init.h
+++ b/arch/mips/include/asm/mach-cavium-octeon/kernel-entry-init.h
@@ -36,6 +36,117 @@ FEXPORT(octeon_hotplug_entry)
 	move	a3, zero
 7:
 #endif
+	mfc0	v0, CP0_STATUS
+	/* Force 64-bit addressing enabled */
+	ori	v0, v0, (ST0_UX | ST0_SX | ST0_KX)
+	mtc0	v0, CP0_STATUS
+
+	# Clear the TLB.
+	mfc0	v0, $16, 1	# Config1
+	dsrl	v0, v0, 25
+	andi	v0, v0, 0x3f
+	mfc0	v1, $16, 3	# Config3
+	bgez	v1, 1f
+	mfc0	v1, $16, 4	# Config4
+	andi	v1, 0x7f
+	dsll	v1, 6
+	or	v0, v0, v1
+1:				# Number of TLBs in v0
+
+	dmtc0	zero, $2, 0	# EntryLo0
+	dmtc0	zero, $3, 0	# EntryLo1
+	dmtc0	zero, $5, 0	# PageMask
+	dla	t0, 0xffffffff90000000
+10:
+	dmtc0	t0, $10, 0	# EntryHi
+	tlbp
+	mfc0	t1, $0, 0	# Index
+	bltz	t1, 1f
+	tlbr
+	dmtc0	zero, $2, 0	# EntryLo0
+	dmtc0	zero, $3, 0	# EntryLo1
+	dmtc0	zero, $5, 0	# PageMask
+	tlbwi			# Make it a 'normal' sized page
+	daddiu	t0, t0, 8192
+	b	10b
+1:
+	mtc0	v0, $0, 0	# Index
+	tlbwi
+	.set	noreorder
+	bne	v0, zero, 10b
+	 addiu	v0, v0, -1
+	.set	reorder
+
+	mtc0	zero, $0, 0	# Index
+	dmtc0	zero, $10, 0	# EntryHi
+
+#ifdef CONFIG_MAPPED_KERNEL
+	# Set up the TLB index 0 for wired access to kernel.
+	# Assume we were loaded with sufficient alignment so that we
+	# can cover the image with two pages.
+	dla	v0, _end
+	dla	s0, _text
+	dsubu	v0, v0, s0	# size of image
+	move	v1, zero
+	li	t1, -1		# shift count.
+1:	dsrl	v0, v0, 1	# mask into v1
+	dsll	v1, v1, 1
+	daddiu	t1, t1, 1
+	ori	v1, v1, 1
+	bne	v0, zero, 1b
+	daddiu	t2, t1, -6
+	mtc0	v1, $5, 0	# PageMask
+	dla	t3, 0xffffffffc0000000 # kernel address
+	dmtc0	t3, $10, 0	# EntryHi
+	.set push
+	.set noreorder
+	.set nomacro
+	bal	1f
+	nop
+1:
+	.set pop
+
+	dsra	v0, ra, 31
+	daddiu	v0, v0, 1	# if it were a ckseg0 address v0 will be zero.
+	beqz	v0, 3f
+	dli	v0, 0x07ffffffffffffff	# Otherwise assume xkphys.
+	b	2f
+3:
+	dli	v0, 0x7fffffff
+
+2:	and	ra, ra, v0	# physical address of pc in ra
+	dla	v0, 1b
+	dsubu	v0, v0, s0	# distance from _text to 1: in v0
+	dsubu	ra, ra, v0	# ra is physical address of _text
+	dsrl	v1, v1, 1
+	nor	v1, v1, zero
+	and	ra, ra, v1	# mask it with the page mask
+	dsubu	v1, t3, ra	# virtual to physical offset into v1
+	dsrlv	v0, ra, t1
+	dsllv	v0, v0, t2
+	ori	v0, v0, 0x1f
+	dmtc0	v0, $2, 0  # EntryLo1
+	dsrlv	v0, ra, t1
+	daddiu	v0, v0, 1
+	dsllv	v0, v0, t2
+	ori	v0, v0, 0x1f
+	dmtc0	v0, $3, 0  # EntryLo2
+	mtc0	$0, $0, 0  # Set index to zero
+	tlbwi
+	li	v0, 1
+	mtc0	v0, $6, 0  # Wired
+	dla	v0, phys_to_kernel_offset
+	sd	v1, 0(v0)
+	dla	v0, kernel_image_end
+	li	v1, 2
+	dsllv	v1, v1, t1
+	daddu	v1, v1, t3
+	sd	v1, 0(v0)
+	dla	v0, continue_in_mapped_space
+	jr	v0
+
+continue_in_mapped_space:
+#endif
 	# Read the cavium mem control register
 	dmfc0	v0, CP0_CVMMEMCTL_REG
 	# Clear the lower 6 bits, the CVMSEG size
diff --git a/arch/mips/include/asm/page.h b/arch/mips/include/asm/page.h
index f59552f..623ba94 100644
--- a/arch/mips/include/asm/page.h
+++ b/arch/mips/include/asm/page.h
@@ -45,6 +45,8 @@
 #define HUGETLB_PAGE_ORDER	({BUILD_BUG(); 0; })
 #endif /* CONFIG_MIPS_HUGE_TLB_SUPPORT */
 
+#ifndef __ASSEMBLY__
+
 #include <linux/pfn.h>
 
 extern void build_clear_page(void);
@@ -140,11 +142,8 @@ typedef struct { unsigned long pgprot; } pgprot_t;
  * __pa()/__va() should be used only during mem init.
  */
 #ifdef CONFIG_64BIT
-#define __pa(x)								\
-({									\
-    unsigned long __x = (unsigned long)(x);				\
-    __x < CKSEG0 ? XPHYSADDR(__x) : CPHYSADDR(__x);			\
-})
+unsigned long __phys_addr(unsigned long x);
+#define __pa(x)	__phys_addr((unsigned long)x)
 #else
 #define __pa(x)								\
     ((unsigned long)(x) - PAGE_OFFSET + PHYS_OFFSET)
@@ -164,8 +163,13 @@ typedef struct { unsigned long pgprot; } pgprot_t;
  * until GCC 3.x has been retired before we can apply
  * https://patchwork.linux-mips.org/patch/1541/
  */
-
-#define __pa_symbol(x)	__pa(RELOC_HIDE((unsigned long)(x), 0))
+# ifdef CONFIG_MAPPED_KERNEL
+extern unsigned long phys_to_kernel_offset;
+#  define __pa_symbol(x)	(RELOC_HIDE((unsigned long)(x), 0) - phys_to_kernel_offset)
+# else
+#  define __pa_symbol(x)	__pa(RELOC_HIDE((unsigned long)(x), 0))
+# endif
+#endif /*__ASSEMBLY__*/
 
 #define pfn_to_kaddr(pfn)	__va((pfn) << PAGE_SHIFT)
 
@@ -194,7 +198,7 @@ static inline int pfn_valid(unsigned long pfn)
 		    : 0);						\
 })
 
-#endif
+#endif /*defined(CONFIG_NEED_MULTIPLE_NODES)*/
 
 #define virt_to_page(kaddr)	pfn_to_page(PFN_DOWN(virt_to_phys(kaddr)))
 
diff --git a/arch/mips/include/asm/pgtable-64.h b/arch/mips/include/asm/pgtable-64.h
index e1c49a9..ae3011a 100644
--- a/arch/mips/include/asm/pgtable-64.h
+++ b/arch/mips/include/asm/pgtable-64.h
@@ -135,7 +135,12 @@
 #if defined(CONFIG_MODULES) && defined(KBUILD_64BIT_SYM32) && \
 	VMALLOC_START != CKSSEG
 /* Load modules into 32bit-compatible segment. */
-#define MODULE_START	CKSSEG
+#ifdef CONFIG_MAPPED_KERNEL
+extern unsigned long kernel_image_end;
+#define MODULE_START	kernel_image_end
+#else
+#define MODULE_START   CKSSEG
+#endif
 #define MODULE_END	(FIXADDR_START-2*PAGE_SIZE)
 #endif
 
diff --git a/arch/mips/kernel/Makefile b/arch/mips/kernel/Makefile
index 0121233..9aa9102 100644
--- a/arch/mips/kernel/Makefile
+++ b/arch/mips/kernel/Makefile
@@ -114,4 +114,8 @@ CFLAGS_branch.o			= $(CFLAGS_DSP)
 CFLAGS_ptrace.o			= $(CFLAGS_DSP)
 endif
 
-CPPFLAGS_vmlinux.lds		:= $(KBUILD_CFLAGS)
+ifdef CONFIG_MAPPED_KERNEL
+  PHYS_LOAD_ADDRESS = -D"PHYSADDR=$(CONFIG_PHYS_LOAD_ADDRESS)"
+endif
+
+CPPFLAGS_vmlinux.lds		:= $(KBUILD_CFLAGS) $(PHYS_LOAD_ADDRESS)
diff --git a/arch/mips/kernel/vmlinux.lds.S b/arch/mips/kernel/vmlinux.lds.S
index 457a785..94aaa2e9 100644
--- a/arch/mips/kernel/vmlinux.lds.S
+++ b/arch/mips/kernel/vmlinux.lds.S
@@ -3,6 +3,11 @@
 
 #define PAGE_SIZE _PAGE_SIZE
 
+#ifdef PHYSADDR
+phys_entry = kernel_entry - VMLINUX_LOAD_ADDRESS + PHYSADDR;
+#define LOAD_OFFSET (VMLINUX_LOAD_ADDRESS - PHYSADDR)
+#endif
+
 /*
  * Put .bss..swapper_pg_dir as the first thing in .bss. This will
  * ensure that it has .bss alignment (64K).
@@ -14,9 +19,15 @@
 #undef mips
 #define mips mips
 OUTPUT_ARCH(mips)
+#ifdef PHYSADDR
+ENTRY(phys_entry)
+#define AT_LOCATION AT(PHYSADDR)
+#else
 ENTRY(kernel_entry)
+#define AT_LOCATION
+#endif
 PHDRS {
-	text PT_LOAD FLAGS(7);	/* RWX */
+	text PT_LOAD AT_LOCATION FLAGS(7);	/* RWX */
 #ifndef CONFIG_DISABLE_ELF_NOTE_HEADER
 	note PT_NOTE FLAGS(4);	/* R__ */
 #endif
@@ -52,7 +63,7 @@ SECTIONS
 	. = VMLINUX_LOAD_ADDRESS;
 	/* read-only */
 	_text = .;	/* Text and read-only data */
-	.text : {
+	.text : AT_LOCATION {
 		TEXT_TEXT
 		SCHED_TEXT
 		LOCK_TEXT
diff --git a/arch/mips/mm/page.c b/arch/mips/mm/page.c
index 36a6758..b3afd48 100644
--- a/arch/mips/mm/page.c
+++ b/arch/mips/mm/page.c
@@ -25,6 +25,7 @@
 #include <asm/prefetch.h>
 #include <asm/bootinfo.h>
 #include <asm/mipsregs.h>
+#include <asm/sections.h>
 #include <asm/mmu_context.h>
 #include <asm/cpu.h>
 #include <asm/war.h>
@@ -37,6 +38,28 @@
 
 #include <asm/uasm.h>
 
+#ifdef CONFIG_MAPPED_KERNEL
+/* Initialized so it is not clobbered when .bss is zeroed.  */
+unsigned long phys_to_kernel_offset = 1;
+unsigned long kernel_image_end = 1;
+#endif
+
+#ifdef CONFIG_64BIT
+unsigned long __phys_addr(unsigned long x)
+{
+#ifdef CONFIG_MAPPED_KERNEL
+	if ((char *)x >= _text && (char *)x < _end)
+		return x - phys_to_kernel_offset;
+#endif
+	if (x < CKSEG0)
+		return XPHYSADDR(x);
+	if (x < CKSSEG)
+		return CPHYSADDR(x);
+	BUG();
+}
+EXPORT_SYMBOL(__phys_addr);
+#endif /* CONFIG_64BIT */
+
 /* Registers used in the assembled routines. */
 #define ZERO 0
 #define AT 2
diff --git a/arch/mips/mm/tlb-r4k.c b/arch/mips/mm/tlb-r4k.c
index c643de4..355bc81 100644
--- a/arch/mips/mm/tlb-r4k.c
+++ b/arch/mips/mm/tlb-r4k.c
@@ -408,7 +408,9 @@ void __cpuinit tlb_init(void)
 	 *     be set to fixed-size pages.
 	 */
 	write_c0_pagemask(PM_DEFAULT_MASK);
+#ifndef CONFIG_MAPPED_KERNEL
 	write_c0_wired(0);
+#endif
 	if (current_cpu_type() == CPU_R10000 ||
 	    current_cpu_type() == CPU_R12000 ||
 	    current_cpu_type() == CPU_R14000)
diff --git a/arch/mips/mm/tlbex.c b/arch/mips/mm/tlbex.c
index 6418724..054c48d 100644
--- a/arch/mips/mm/tlbex.c
+++ b/arch/mips/mm/tlbex.c
@@ -1495,6 +1495,18 @@ static void __cpuinit build_r4000_setup_pgd(void)
 
 	if (pgd_reg == -1) {
 		/* PGD << 11 in c0_Context */
+#ifdef CONFIG_MAPPED_KERNEL
+		/*
+		 * if (&swapper_pg_dir == pgd)
+		 *     pgd = pgd - phys_to_kernel_offset;
+		 */
+		UASM_i_LA(&p, a1, (long)&swapper_pg_dir);
+		uasm_il_bne(&p, &r, a0, a1, label_tlbl_goaround1);
+		UASM_i_LA_mostly(&p, a1, (long)&phys_to_kernel_offset);
+		UASM_i_LW(&p, a1, uasm_rel_lo((long)&phys_to_kernel_offset), a1);
+		UASM_i_SUBU(&p, a0, a0, a1);
+		/* Fall through to tlbl_goaround1. */
+#else
 		/*
 		 * If it is a ckseg0 address, convert to a physical
 		 * address.  Shifting right by 29 and adding 4 will
@@ -1506,6 +1518,7 @@ static void __cpuinit build_r4000_setup_pgd(void)
 		uasm_il_bnez(&p, &r, a1, label_tlbl_goaround1);
 		uasm_i_nop(&p);
 		uasm_i_dinsm(&p, a0, 0, 29, 64 - 29);
+#endif
 		uasm_l_tlbl_goaround1(&l, p);
 		UASM_i_SLL(&p, a0, a0, 11);
 		uasm_i_jr(&p, 31);
-- 
2.6.2

