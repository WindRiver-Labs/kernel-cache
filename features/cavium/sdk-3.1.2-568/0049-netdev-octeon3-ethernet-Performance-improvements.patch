From 73b3f57a8bb863bb08170d87f9ddad20cfb52849 Mon Sep 17 00:00:00 2001
From: Carlos Munoz <cmunoz@caviumnetworks.com>
Date: Tue, 6 Jan 2015 15:37:52 -0800
Subject: [PATCH 049/184] netdev: octeon3-ethernet: Performance improvements.

Source: Cavium Networks, Inc.
MR: 00000
Type: Integration
Disposition: Merged from Octeon Tree
ChangeID: 898f5ca16902ef4376873327da37a2fc24f4e975
Description:

Signed-off-by: Carlos Munoz <cmunoz@caviumnetworks.com>
Signed-off-by: Corey Minyard <cminyard@mvista.com>
[Original patch taken from octeon-linux-kernel-patches-SDK-3.1.2-release]
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 drivers/net/ethernet/octeon/octeon3-ethernet.c | 54 +++++++++++++++++---------
 1 file changed, 35 insertions(+), 19 deletions(-)

diff --git a/drivers/net/ethernet/octeon/octeon3-ethernet.c b/drivers/net/ethernet/octeon/octeon3-ethernet.c
index 39c59cf..5d1b611 100644
--- a/drivers/net/ethernet/octeon/octeon3-ethernet.c
+++ b/drivers/net/ethernet/octeon/octeon3-ethernet.c
@@ -253,7 +253,7 @@ struct octeon3_ethernet_node {
 };
 
 /* This array keeps track of the number of napis running on each cpu */
-static u8 octeon3_cpu_napi_cnt[NR_CPUS];
+static u8 octeon3_cpu_napi_cnt[NR_CPUS] __cacheline_aligned_in_smp;
 
 static int recycle_skbs = 1;
 module_param(recycle_skbs, int, 0644);
@@ -844,22 +844,29 @@ static struct sk_buff *octeon3_eth_work_to_skb(void *w)
  *				called with the napi_alloc_lock lock held.
  *
  *  node:			Node to allocate cpu from.
+ *  cpu:			Cpu to bind the napi to:
+ *					<  0: use any cpu.
+ *					>= 0: use requested cpu.
  *
  *  returns:			cpu number or a negative error code.
  */
-static int octeon3_napi_alloc_cpu(int	node)
+static int octeon3_napi_alloc_cpu(int	node,
+				  int	cpu)
 {
-	int				cpu;
 	int				min_cnt = MAX_NAPIS_PER_NODE;
 	int				min_cpu = -EBUSY;
 
-	for_each_cpu(cpu, cpumask_of_node(node)) {
-		if (octeon3_cpu_napi_cnt[cpu] == 0) {
-			min_cpu = cpu;
-			break;
-		} else if (octeon3_cpu_napi_cnt[cpu] < min_cnt) {
-			min_cnt = octeon3_cpu_napi_cnt[cpu];
-			min_cpu = cpu;
+	if (cpu >= 0)
+		min_cpu = cpu;
+	else {
+		for_each_cpu(cpu, cpumask_of_node(node)) {
+			if (octeon3_cpu_napi_cnt[cpu] == 0) {
+				min_cpu = cpu;
+				break;
+			} else if (octeon3_cpu_napi_cnt[cpu] < min_cnt) {
+				min_cnt = octeon3_cpu_napi_cnt[cpu];
+				min_cpu = cpu;
+			}
 		}
 	}
 
@@ -875,17 +882,20 @@ static int octeon3_napi_alloc_cpu(int	node)
  *
  *  cxt:			Receive context the napi will be added to.
  *  idx:			Napi index within the receive context.
+ *  cpu:			Cpu to bind the napi to:
+ *					<  0: use any cpu.
+ *					>= 0: use requested cpu.
  *
  *  Returns:			Pointer to napi wrapper or NULL on error.
  */
 static struct octeon3_napi_wrapper *octeon3_napi_alloc(struct octeon3_rx *cxt,
-						       int		  idx)
+						       int		  idx,
+						       int		  cpu)
 {
 	struct octeon3_ethernet_node	*oen;
 	struct octeon3_ethernet		*priv = cxt->parent;
 	int				node = priv->numa_node;
 	unsigned long			flags;
-	int				cpu;
 	int				i;
 
 	oen = octeon3_eth_node + node;
@@ -895,7 +905,7 @@ static struct octeon3_napi_wrapper *octeon3_napi_alloc(struct octeon3_rx *cxt,
 	for (i = 0; i < MAX_NAPIS_PER_NODE; i++) {
 		if (napi_wrapper[node][i].available) {
 			/* Allocate a cpu to use */
-			cpu = octeon3_napi_alloc_cpu(node);
+			cpu = octeon3_napi_alloc_cpu(node, cpu);
 			if (cpu < 0)
 				break;
 
@@ -985,7 +995,7 @@ static int octeon3_add_napi_to_cxt(struct octeon3_rx *cxt)
 	spin_unlock_irqrestore(&cxt->napi_idx_lock, flags);
 
 	/* Get a free napi block */
-	napiw = octeon3_napi_alloc(cxt, idx);
+	napiw = octeon3_napi_alloc(cxt, idx, -1);
 	if (unlikely(napiw == NULL)) {
 		spin_lock_irqsave(&cxt->napi_idx_lock, flags);
 		bitmap_clear(cxt->napi_idx_bitmap, idx, 1);
@@ -1533,7 +1543,8 @@ static int octeon3_eth_ndo_open(struct net_device *netdev)
 	int r;
 
 	for (i = 0; i < priv->num_rx_cxt; i++) {
-		unsigned int sso_intsn;
+		unsigned int	sso_intsn;
+		int		cpu;
 
 		rx = priv->rx_cxt + i;
 		sso_intsn = SSO_INTSN_EXE << 12 | rx->rx_grp;
@@ -1569,9 +1580,10 @@ static int octeon3_eth_ndo_open(struct net_device *netdev)
 			goto err3;
 		}
 		bitmap_set(priv->rx_cxt[i].napi_idx_bitmap, idx, 1);
+		cpu = cpumask_first(&rx->rx_affinity_hint);
 
 		priv->rx_cxt[i].napiw = octeon3_napi_alloc(&priv->rx_cxt[i],
-							   idx);
+							   idx, cpu);
 		if (priv->rx_cxt[i].napiw == NULL) {
 			r = -ENOMEM;
 			goto err4;
@@ -1742,11 +1754,15 @@ static int octeon3_eth_ndo_start_xmit(struct sk_buff *skb, struct net_device *ne
 		}
 	}
 
+	/* SSO can only fall behind when the skb is not recycled */
+	if (can_recycle_skb == false) {
+		oen = octeon3_eth_node + priv->numa_node;
+		aq_cnt.u64 = cvmx_read_csr_node(oen->numa_node,
+				CVMX_SSO_GRPX_AQ_CNT(oen->tx_complete_grp));
+	}
+
 	/* Drop the packet if pko or sso are not keeping up */
-	oen = octeon3_eth_node + priv->numa_node;
 	backlog = atomic64_inc_return(&priv->tx_backlog);
-	aq_cnt.u64 = cvmx_read_csr_node(oen->numa_node,
-				CVMX_SSO_GRPX_AQ_CNT(oen->tx_complete_grp));
 	if (unlikely(backlog > MAX_TX_QUEUE_DEPTH) ||
 	    (can_recycle_skb == false && aq_cnt.s.aq_cnt > 100000)) {
 		if (use_tx_queues) {
-- 
1.9.1

