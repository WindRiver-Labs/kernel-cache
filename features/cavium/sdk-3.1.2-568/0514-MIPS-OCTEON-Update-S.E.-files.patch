From 3f815b883361561448a6daa5ac6097ce9fec52bb Mon Sep 17 00:00:00 2001
From: David Daney <david.daney@cavium.com>
Date: Sun, 2 Feb 2014 13:46:59 -0800
Subject: [PATCH 514/974] MIPS: OCTEON: Update S.E. files.

Signed-off-by: David Daney <david.daney@cavium.com>
[Original patch taken from Cavium SDK 3.1.2-568]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
---
 arch/mips/cavium-octeon/executive/cvmx-clock.c     |    3 +-
 .../cavium-octeon/executive/cvmx-fpa-resource.c    |    2 +-
 .../cavium-octeon/executive/cvmx-helper-board.c    |  137 +-
 .../mips/cavium-octeon/executive/cvmx-helper-cfg.c |   97 +-
 .../mips/cavium-octeon/executive/cvmx-helper-ipd.c |   12 +-
 .../cavium-octeon/executive/cvmx-helper-loop.c     |    6 +-
 .../mips/cavium-octeon/executive/cvmx-helper-npi.c |    6 +-
 .../mips/cavium-octeon/executive/cvmx-helper-pki.c | 1096 ++++++++----
 .../mips/cavium-octeon/executive/cvmx-helper-pko.c |    7 +-
 .../cavium-octeon/executive/cvmx-helper-sgmii.c    |   11 +-
 .../cavium-octeon/executive/cvmx-helper-util.c     |    5 +
 arch/mips/cavium-octeon/executive/cvmx-helper.c    |   18 +
 arch/mips/cavium-octeon/executive/cvmx-pcie.c      |    4 +-
 arch/mips/cavium-octeon/executive/cvmx-pki.c       |   91 +-
 arch/mips/cavium-octeon/executive/cvmx-pko3.c      |    2 +-
 arch/mips/cavium-octeon/executive/cvmx-qlm.c       |  185 +-
 arch/mips/cavium-octeon/executive/octeon-model.c   |    7 +-
 arch/mips/include/asm/octeon/cvmx-app-init.h       |    4 +-
 arch/mips/include/asm/octeon/cvmx-clock.h          |    6 +-
 arch/mips/include/asm/octeon/cvmx-cmd-queue.h      |    6 +-
 arch/mips/include/asm/octeon/cvmx-coremask.h       |    5 +-
 arch/mips/include/asm/octeon/cvmx-fpa-defs.h       |   96 +-
 arch/mips/include/asm/octeon/cvmx-fpa3.h           |    2 +-
 arch/mips/include/asm/octeon/cvmx-gserx-defs.h     |  205 ++-
 arch/mips/include/asm/octeon/cvmx-helper-board.h   |   12 +-
 arch/mips/include/asm/octeon/cvmx-helper-cfg.h     |   48 +
 arch/mips/include/asm/octeon/cvmx-helper-pki.h     |   75 +-
 arch/mips/include/asm/octeon/cvmx-l2c-defs.h       |  167 +-
 arch/mips/include/asm/octeon/cvmx-mio-defs.h       |   19 +-
 arch/mips/include/asm/octeon/cvmx-pip.h            |   23 +-
 arch/mips/include/asm/octeon/cvmx-pki-cluster.h    |   68 +-
 arch/mips/include/asm/octeon/cvmx-pki-defs.h       |    4 +-
 arch/mips/include/asm/octeon/cvmx-pki.h            |   29 +-
 arch/mips/include/asm/octeon/cvmx-pko-defs.h       |  647 +++----
 arch/mips/include/asm/octeon/cvmx-tim-defs.h       | 1816 ++++++++++++++++++++
 arch/mips/include/asm/octeon/cvmx-wqe.h            |    5 +-
 arch/mips/include/asm/octeon/octeon-feature.h      |    1 +
 37 files changed, 3919 insertions(+), 1008 deletions(-)
 create mode 100644 arch/mips/include/asm/octeon/cvmx-tim-defs.h

diff --git a/arch/mips/cavium-octeon/executive/cvmx-clock.c b/arch/mips/cavium-octeon/executive/cvmx-clock.c
index f0ea278..e4c2027 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-clock.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-clock.c
@@ -113,9 +113,10 @@ uint64_t cvmx_clock_get_rate_node(int node, cvmx_clock_t clock)
 
 	switch (clock) {
 	case CVMX_CLOCK_SCLK:
-	case CVMX_CLOCK_TIM:
 	case CVMX_CLOCK_IPD:
 		return rate_sclk;
+	case CVMX_CLOCK_TIM:
+		return rate_sclk;
 
 	case CVMX_CLOCK_RCLK:
 	case CVMX_CLOCK_CORE:
diff --git a/arch/mips/cavium-octeon/executive/cvmx-fpa-resource.c b/arch/mips/cavium-octeon/executive/cvmx-fpa-resource.c
index abac157..c2766a7 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-fpa-resource.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-fpa-resource.c
@@ -141,7 +141,7 @@ int cvmx_fpa_alloc_pool(int pool)
 {
 	int pool_num = pool;
 
-	if (cvmx_fpa_allocate_fpa_pools(-1, &pool_num, 1) != 0) {
+	if (cvmx_fpa_allocate_fpa_pools(-1, &pool_num, 1) < 0) {
 		return -1;
 	}
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-board.c b/arch/mips/cavium-octeon/executive/cvmx-helper-board.c
index cb9c0eb..4876a40 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-board.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-board.c
@@ -1,5 +1,5 @@
 /***********************license start***************
- * Copyright (c) 2003-2013  Cavium Inc. (support@cavium.com). All rights
+ * Copyright (c) 2003-2014  Cavium Inc. (support@cavium.com). All rights
  * reserved.
  *
  *
@@ -71,10 +71,11 @@
 #include "cvmx-gpio.h"
 #include "octeon_mem_map.h"
 #include "cvmx-bootmem.h"
+
 #ifdef __U_BOOT__
-# include <libfdt.h>
+# include "cvmx-helper-fdt.h"
 #else
-# include "libfdt/libfdt.h"
+# include "libfdt/cvmx-helper-fdt.h"
 #endif
 #endif
 
@@ -358,6 +359,110 @@ static int __get_muxed_mdio_info_from_dt(cvmx_phy_info_t *phy_info,
 }
 
 /**
+ * @INTERNAL
+ * Converts a BGX address to the node, interface and port number
+ *
+ * @param bgx_addr	Address of CSR register
+ *
+ * @return node, interface and port number, will be -1 for invalid address.
+ */
+static struct cvmx_xiface
+__cvmx_78xx_bgx_reg_addr_to_xiface(uint64_t bgx_addr)
+{
+	struct cvmx_xiface xi = {-1, -1};
+
+	xi.node = cvmx_csr_addr_to_node(bgx_addr);
+	bgx_addr = cvmx_csr_addr_strip_node(bgx_addr);
+	if ((bgx_addr & 0xFFFFFFFFF0000000) != 0x00011800E0000000) {
+		cvmx_dprintf("%s: Invalid BGX address 0x%llx\n",
+			     __func__, (unsigned long long)bgx_addr);
+		xi.node = -1;
+		return xi;
+	}
+	xi.interface = (bgx_addr >> 28) & 0x0F;
+
+	return xi;
+}
+
+/**
+ * @INTERNAL
+ * Parse the device tree and set whether a port is valid or not.
+ *
+ * @param fdt_addr	Pointer to device tree
+ *
+ * @return 0 for success, -1 on error.
+ */
+int __cvmx_helper_parse_78xx_bgx_dt(void *fdt_addr)
+{
+	int port_index;
+	int dbg = device_tree_dbg;
+	struct cvmx_xiface xi;
+	int fdt_port_node;
+	int fdt_interface_node;
+	int fdt_phy_node;
+	uint64_t reg_addr;
+	int index = -1;
+	int xiface;
+
+	while ((fdt_port_node = fdt_node_offset_by_compatible(fdt_addr, index,
+					"cavium,octeon-7890-bgx-port")) >= 0) {
+		/* Get the port number */
+		port_index = cvmx_fdt_get_int(fdt_addr, fdt_port_node,
+					      "reg", -1);
+		if (port_index < 0) {
+			cvmx_dprintf("Error: missing reg field for bgx port in device tree\n");
+			return -1;
+		}
+		/* Get the interface number */
+		fdt_interface_node = fdt_parent_offset(fdt_addr, fdt_port_node);
+		if (fdt_interface_node < 0) {
+			cvmx_dprintf("Error: device tree corrupt!\n");
+			return -1;
+		}
+		if (fdt_node_check_compatible(fdt_addr, fdt_interface_node,
+					      "cavium,octeon-7890-bgx")) {
+			cvmx_dprintf("Error: incompatible Ethernet MAC Nexus in device tree!\n");
+			return -1;
+		}
+		reg_addr = cvmx_fdt_get_addr(fdt_addr, fdt_interface_node,
+					     "reg");
+		if (dbg)
+			cvmx_dprintf("%s: BGX interface address: 0x%llx\n",
+				     __func__, (unsigned long long)reg_addr);
+		if (reg_addr == FDT_ADDR_T_NONE) {
+			cvmx_dprintf("Device tree BGX node has invalid address 0x%llx\n",
+				     (unsigned long long)reg_addr);
+			return -1;
+		}
+		reg_addr = cvmx_fdt_translate_address(fdt_addr,
+						      fdt_interface_node,
+						      (uint32_t *)&reg_addr);
+		xi = __cvmx_78xx_bgx_reg_addr_to_xiface(reg_addr);
+		if (xi.node < 0) {
+			cvmx_dprintf("Device tree BGX node has invalid address 0x%llx\n",
+				     (unsigned long long)reg_addr);
+			return -1;
+		}
+		xiface = cvmx_helper_node_interface_to_xiface(xi.node,
+							      xi.interface);
+		cvmx_helper_set_port_fdt_node_offset(xiface, port_index,
+						     fdt_port_node);
+		cvmx_helper_set_port_valid(xiface, port_index, true);
+
+		fdt_phy_node = cvmx_fdt_lookup_phandle(fdt_addr, fdt_port_node,
+						       "phy");
+		if (fdt_phy_node >= 0) {
+			cvmx_helper_set_phy_fdt_node_offset(xiface, port_index,
+							    fdt_phy_node);
+		} else {
+			cvmx_helper_set_phy_fdt_node_offset(xiface, port_index,
+							    -1);
+		}
+	}
+	return 0;
+}
+
+/**
  * Returns if a port is present on an interface
  *
  * @param fdt_addr - address fo flat device tree
@@ -377,9 +482,25 @@ int __cvmx_helper_board_get_port_from_dt(void *fdt_addr, int ipd_port)
 	int xiface = cvmx_helper_get_interface_num(ipd_port);
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
 	uint32_t *val;
+	int phy_node_offset;
 
-	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
-		return 1;
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+		static int fdt_ports_initialized = 0;
+
+		port_index = cvmx_helper_get_interface_index_num(ipd_port);
+
+		if (!fdt_ports_initialized) {
+			if (!__cvmx_helper_parse_78xx_bgx_dt(fdt_addr))
+				fdt_ports_initialized = 1;
+			else {
+				cvmx_dprintf("%s: Error parsing FDT\n",
+					     __func__);
+				return -1;
+			}
+		}
+
+		return cvmx_helper_is_port_valid(xiface, port_index);
+	}
 
 	mode = cvmx_helper_interface_get_mode(xiface);
 
@@ -443,6 +564,12 @@ int __cvmx_helper_board_get_port_from_dt(void *fdt_addr, int ipd_port)
 	if (eth < 0)
 		return -1;
 
+	cvmx_helper_set_port_fdt_node_offset(xiface, port_index, eth);
+
+	phy_node_offset = cvmx_fdt_get_int(fdt_addr, eth, "phy", -1);
+	cvmx_helper_set_phy_fdt_node_offset(xiface, port_index,
+					    phy_node_offset);
+
 	if (fdt_getprop(fdt_addr, eth, "cavium,sgmii-mac-phy-mode", NULL))
 		cvmx_helper_set_mac_phy_mode(xiface, port_index, true);
 	else
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c b/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c
index 381da61..d0275a5 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c
@@ -72,17 +72,27 @@
 #include "cvmx-pko-internal-ports-range.h"
 #endif
 
-#if defined(min)
-#else
-#define min( a, b ) ( ( a ) < ( b ) ) ? ( a ) : ( b )
+#if !defined(min)
+# define min( a, b ) ( ( a ) < ( b ) ) ? ( a ) : ( b )
 #endif
 
 CVMX_SHARED struct cvmx_cfg_port_param cvmx_cfg_port[CVMX_MAX_NODES][CVMX_HELPER_MAX_IFACE][CVMX_HELPER_CFG_MAX_PORT_PER_IFACE] =
-	{[0 ... CVMX_MAX_NODES - 1][0 ... CVMX_HELPER_MAX_IFACE - 1] = {[0 ... CVMX_HELPER_CFG_MAX_PORT_PER_IFACE - 1] =
-				      	      { CVMX_HELPER_CFG_INVALID_VALUE, CVMX_HELPER_CFG_INVALID_VALUE,
-				                CVMX_HELPER_CFG_INVALID_VALUE, CVMX_HELPER_CFG_INVALID_VALUE,
-	                                        CVMX_HELPER_CFG_INVALID_VALUE, 0,
-	                                        0, 0, 0, false}}};
+	{[0 ... CVMX_MAX_NODES - 1][0 ... CVMX_HELPER_MAX_IFACE - 1] =
+		{[0 ... CVMX_HELPER_CFG_MAX_PORT_PER_IFACE - 1] =
+			{
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+				CVMX_HELPER_CFG_INVALID_VALUE,
+				CVMX_HELPER_CFG_INVALID_VALUE,
+#endif
+				CVMX_HELPER_CFG_INVALID_VALUE,
+				CVMX_HELPER_CFG_INVALID_VALUE,
+				CVMX_HELPER_CFG_INVALID_VALUE,
+				CVMX_HELPER_CFG_INVALID_VALUE,
+				CVMX_HELPER_CFG_INVALID_VALUE,
+				0, 0, 0, 0, false
+			}
+		}
+	};
 
 /*
  * Indexed by the pko_port number
@@ -816,13 +826,17 @@ int __cvmx_helper_init_port_valid(void)
 #ifndef CVMX_BUILD_FOR_LINUX_KERNEL
 	int i, j, n;
 	bool valid;
+	static void *fdt_addr = 0;
 
+	if (fdt_addr == 0)
+		fdt_addr = __cvmx_phys_addr_to_ptr(cvmx_sysinfo_get()->fdt_addr,
+						   (128*1024));
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+		return __cvmx_helper_parse_78xx_bgx_dt(fdt_addr);
+
+	/* TODO: Update this to behave more like 78XX */
 	for (i = 0; i < cvmx_helper_get_number_of_interfaces(); i++) {
-		static void *fdt_addr = 0;
 
-		if (fdt_addr == 0)
-			fdt_addr = __cvmx_phys_addr_to_ptr(cvmx_sysinfo_get()->fdt_addr,
-							   (128*1024));
 		n = cvmx_helper_interface_enumerate(i);
 		for (j = 0; j < n; j++) {
 			int ipd_port = cvmx_helper_get_ipd_port(i, j);
@@ -988,3 +1002,62 @@ int __cvmx_helper_init_port_config_data(void)
 	return rv;
 }
 EXPORT_SYMBOL(__cvmx_helper_init_port_config_data);
+
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+/**
+ * @INTERNAL
+ * Store the FDT node offset in the device tree of a port
+ *
+ * @param xiface	node and interface
+ * @param index		port index
+ * @param node_offset	node offset to store
+ */
+void cvmx_helper_set_port_fdt_node_offset(int xiface, int index,
+					  int node_offset)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	cvmx_cfg_port[xi.node][xi.interface][index].port_fdt_node = node_offset;
+}
+
+/**
+ * @INTERNAL
+ * Return the FDT node offset in the device tree of a port
+ *
+ * @param xiface	node and interface
+ * @param index		port index
+ * @return		node offset of port or -1 if invalid
+ */
+int cvmx_helper_get_port_fdt_node_offset(int xiface, int index)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	return cvmx_cfg_port[xi.node][xi.interface][index].port_fdt_node;
+}
+
+/**
+ * @INTERNAL
+ * Store the FDT node offset in the device tree of a phy
+ *
+ * @param xiface	node and interface
+ * @param index		port index
+ * @param node_offset	node offset to store
+ */
+void cvmx_helper_set_phy_fdt_node_offset(int xiface, int index, int node_offset)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	cvmx_cfg_port[xi.node][xi.interface][index].phy_fdt_node = node_offset;
+}
+
+/**
+ * @INTERNAL
+ * Return the FDT node offset in the device tree of a phy
+ *
+ * @param xiface	node and interface
+ * @param index		port index
+ * @return		node offset of phy or -1 if invalid
+ */
+int cvmx_helper_get_phy_fdt_node_offset(int xiface, int index)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	return cvmx_cfg_port[xi.node][xi.interface][index].phy_fdt_node;
+}
+#endif /* !CVMX_BUILD_FOR_LINUX_KERNEL */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-ipd.c b/arch/mips/cavium-octeon/executive/cvmx-helper-ipd.c
index b5d66c0..c2eda2a 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-ipd.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-ipd.c
@@ -247,13 +247,6 @@ int __cvmx_helper_ipd_setup_interface(int interface)
 	if (num_ports == CVMX_HELPER_CFG_INVALID_VALUE)
 		return 0;
 
-	mode = cvmx_helper_interface_get_mode(interface);
-
-	if (!octeon_has_feature(OCTEON_FEATURE_PKI)) {
-		if (mode == CVMX_HELPER_INTERFACE_MODE_LOOP)
-			__cvmx_helper_loop_enable(interface);
-	}
-
 	delta = 1;
 	if (octeon_has_feature(OCTEON_FEATURE_PKND)) {
 		if (interface < CVMX_HELPER_MAX_GMX)
@@ -274,6 +267,11 @@ int __cvmx_helper_ipd_setup_interface(int interface)
 			   cvmx_helper_ports_on_interface(interface),
 			   __cvmx_helper_get_has_fcs(interface));
 
+	mode = cvmx_helper_interface_get_mode(interface);
+	
+	if (mode == CVMX_HELPER_INTERFACE_MODE_LOOP)
+			__cvmx_helper_loop_enable(interface);
+
 	return 0;
 }
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-loop.c b/arch/mips/cavium-octeon/executive/cvmx-helper-loop.c
index 869180f..7dcae9d6 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-loop.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-loop.c
@@ -43,7 +43,7 @@
  * Functions for LOOP initialization, configuration,
  * and monitoring.
  *
- * <hr>$Revision: 94747 $<hr>
+ * <hr>$Revision: 97657 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
@@ -106,6 +106,7 @@ int __cvmx_helper_loop_enable(int interface)
 		if (octeon_has_feature(OCTEON_FEATURE_PKI)) {
 			int node = cvmx_get_node_num();
 			cvmx_pki_endis_l2_errs(node, offset, 1, 0, 0);
+                        cvmx_pki_endis_fcs_check(node, offset, 0, 0);
 		} else {
 			port_cfg.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(offset));
 			port_cfg.s.maxerr_en = 0;
@@ -122,8 +123,7 @@ int __cvmx_helper_loop_enable(int interface)
 		ipd_sub_port_fcs.u64 = cvmx_read_csr(CVMX_IPD_SUB_PORT_FCS);
 		ipd_sub_port_fcs.s.port_bit2 = 0;
 		cvmx_write_csr(CVMX_IPD_SUB_PORT_FCS, ipd_sub_port_fcs.u64);
-	}
-
+	} 
 	/*
  	 * Set PKND and BPID for loopback ports.
  	 */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-npi.c b/arch/mips/cavium-octeon/executive/cvmx-helper-npi.c
index 70687d4..a8d6ca7 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-npi.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-npi.c
@@ -43,7 +43,7 @@
  * Functions for NPI initialization, configuration,
  * and monitoring.
  *
- * <hr>$Revision: 95258 $<hr>
+ * <hr>$Revision: 97463 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
@@ -106,8 +106,10 @@ int __cvmx_helper_npi_probe(int interface)
  *
  * @return Zero on success, negative on failure
  */
-int __cvmx_helper_npi_enable(int interface)
+int __cvmx_helper_npi_enable(int xiface)
 {
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	int interface = xi.interface;
 	int port;
 	int num_ports = cvmx_helper_ports_on_interface(interface);
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-pki.c b/arch/mips/cavium-octeon/executive/cvmx-helper-pki.c
index 0952e09..167b088 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-pki.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-pki.c
@@ -98,12 +98,12 @@ static CVMX_SHARED struct cvmx_pki_cluster_grp_config pki_dflt_clgrp[CVMX_MAX_NO
 CVMX_SHARED struct cvmx_pki_pool_config pki_dflt_pool[CVMX_MAX_NODES] = { [0 ... CVMX_MAX_NODES-1] = {
 	.pool_num = -1,
 	.buffer_size = 2048,
-	.buffer_count = 1000} };
+	.buffer_count = 0} };
 
 static CVMX_SHARED struct cvmx_pki_aura_config pki_dflt_aura[CVMX_MAX_NODES] = { [0 ... CVMX_MAX_NODES-1] = {
 	.aura_num = 0,
 	.pool_num = -1,
-	.buffer_count = 1000} };
+	.buffer_count = 0} };
 
 CVMX_SHARED struct cvmx_pki_style_config pki_dflt_style[CVMX_MAX_NODES] = { [0 ... CVMX_MAX_NODES-1] = {
 	.parm_cfg = {.lenerr_en = 1, .maxerr_en = 1, .minerr_en = 1,
@@ -226,7 +226,7 @@ int __cvmx_helper_pki_setup_fpa_pools(int node)
 			} else
 				return 0; /* aura already configured, share it */
 		}
-	}	
+	}
 	return 0;
 }
 #endif
@@ -490,6 +490,42 @@ void cvmx_helper_pki_set_fcs_op(int node, int interface, int nports, int has_fcs
 	}
 }
 
+/**
+ * This function sets the wqe buffer mode of all ports. First packet data buffer can reside
+ * either in same buffer as wqe OR it can go in separate buffer. If used the later mode,
+ * make sure software allocate enough buffers to now have wqe separate from packet data.
+ * @param node	                node number.
+ * @param pkt_outside_wqe.	0 = The packet link pointer will be at word [FIRST_SKIP]
+ *				    immediately followed by packet data, in the same buffer
+ *				    as the work queue entry.
+ *				1 = The packet link pointer will be at word [FIRST_SKIP] in a new
+ *				    buffer separate from the work queue entry. Words following the
+ *				    WQE in the same cache line will be zeroed, other lines in the
+ *				    buffer will not be modified and will retain stale data (from the
+ *				    bufferâ€™s previous use). This setting may decrease the peak PKI
+ *				    performance by up to half on small packets.
+ */
+void cvmx_helper_pki_set_wqe_mode(int node, bool pkt_outside_wqe)
+{
+        int interface, port, pknd;
+        int num_intf, num_ports;
+        uint64_t style;
+
+        /* get the pkind used by this ipd port */
+        num_intf = cvmx_helper_get_number_of_interfaces();
+        for (interface = 0; interface < num_intf; interface++) {
+                num_ports = cvmx_helper_ports_on_interface(interface);
+                /*Skip invalid/disabled interfaces */
+                if (num_ports <= 0)
+                        continue;
+                for (port = 0; port < num_ports; port++) {
+                        pknd = cvmx_helper_get_pknd(interface, port);
+                        style = cvmx_pki_get_pkind_style(node, pknd);
+                        cvmx_pki_set_wqe_mode(node, style, pkt_outside_wqe);
+                }
+        }
+}
+
 void cvmx_helper_pki_set_dflt_pool(int node, int pool, int buffer_size, int buffer_count)
 {
 	if (pool == 0)
@@ -577,397 +613,725 @@ int cvmx_helper_setup_aura_qos(int node, int aura, bool ena_red, bool ena_drop,
  * then maps that bpid to all the channels, that bpid can asserrt bp on.
  *
  * @param node          node number.
- * @param aura_map      array of auras to map to that bpid.
- * @param aura_cnt      number of auras to map to the bpid
+ * @param aura          aura number which will back pressure specified bpid.
+ * @param bpid          bpid to map.
  * @param chl_map       array of channels to map to that bpid.
- * @param chl_cnt       number of channels to map to the bpid
- * @param bpid          bpid to map
+ * @param chl_cnt	number of channel/ports to map to that bpid.
  * @return Zero on success. Negative on failure
  */
-int cvmx_helper_pki_map_aura_chl_bpid(int node, int aura_map[], int aura_cnt,
-				      int chl_map[], int chl_cnt, int bpid)
+int cvmx_helper_pki_map_aura_chl_bpid(int node, uint16_t aura, uint16_t bpid,
+                                      uint16_t chl_map[], uint16_t chl_cnt)
 {
-	while (aura_cnt--)
-		cvmx_pki_write_aura_bpid(node, aura_map[aura_cnt], bpid);
-	while (chl_cnt--)
-		cvmx_pki_write_channel_bpid(node, chl_map[chl_cnt], bpid);
-	return 0;
+        uint16_t channel;
+
+        if (aura >= CVMX_PKI_NUM_AURA) {
+                cvmx_dprintf("ERROR: aura %d is > supported in hw\n", aura);
+                return -1;
+        }
+        if (bpid >= CVMX_PKI_NUM_BPID) {
+                cvmx_dprintf("ERROR: bpid %d is > supported in hw\n", bpid);
+                return -1;
+        }
+        cvmx_pki_write_aura_bpid(node, aura, bpid);
+        while (chl_cnt--) {
+                channel = chl_map[chl_cnt];
+                if ( channel >= CVMX_PKI_NUM_CHANNEL) {
+                        cvmx_dprintf("ERROR: channel %d is > supported in hw\n", channel);
+                        return -1;
+                }
+                cvmx_pki_write_channel_bpid(node, channel, bpid);
+        }
+        return 0;
 }
 
-int cvmx_helper_pki_port_msb(uint16_t num_ports)
+int cvmx_helper_pki_port_shift(int xiface, enum cvmx_pki_qpg_qos qpg_qos)
 {
-	if (num_ports == 0)
-		return 0;
-	else if (num_ports <= 16)
-		return 4;
-	else if (num_ports <= 256)
-		return 8;
-	else
-		cvmx_dprintf("num_ports %d not supported\n", num_ports);
-	return 0;
+        uint8_t num_qos;
+        cvmx_helper_interface_mode_t mode = cvmx_helper_interface_get_mode(xiface);
+
+        num_qos = cvmx_helper_pki_get_num_qpg_entry(qpg_qos);
+        if ((mode != CVMX_HELPER_INTERFACE_MODE_SGMII) &&
+             (mode != CVMX_HELPER_INTERFACE_MODE_NPI) &&
+             (mode != CVMX_HELPER_INTERFACE_MODE_LOOP)) {
+                return ffs(num_qos) - 1;
+             }
+             else if (num_qos <= 16)
+                     return 0;
+             else if (num_qos <= 32)
+                     return 1;
+             else
+                     return 2;
 }
 
-int cvmx_helper_pki_port_shift(enum cvmx_pki_qpg_qos qpg_qos)
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+
+int cvmx_helper_pki_set_gbl_schd(int node, struct cvmx_pki_global_schd *gblsch)
 {
-	uint8_t num_qos;
-	num_qos = cvmx_helper_pki_get_num_qpg_entry(qpg_qos);
-	return ffs(num_qos) - 1;
+        int rs;
+
+        if (gblsch->setup_pool) {
+                if (pki_helper_debug)
+                        cvmx_dprintf("pki-helper:gbl setup global pool %d buff_size %d blocks %d\n",
+                                     gblsch->pool, (int)gblsch->pool_buff_size, (int)gblsch->pool_max_buff);
+                cvmx_helper_fpa3_init_pool(node, node, &gblsch->pool, gblsch->pool_buff_size,
+                                           gblsch->pool_max_buff, gblsch->pool_name);
+                if (pki_helper_debug)
+                        cvmx_dprintf("pool alloced is %d\n", gblsch->pool);
+        }
+        if (gblsch->setup_aura) {
+                if (pki_helper_debug)
+                        cvmx_dprintf("pki-helper:gbl setup global aura %d pool %d blocks %d\n",
+                                     gblsch->aura, gblsch->pool, (int)gblsch->aura_buff_cnt);
+                cvmx_helper_fpa3_add_aura_to_pool(node, gblsch->pool, &gblsch->aura,
+                                gblsch->aura_buff_cnt, NULL, gblsch->aura_name);
+                if (pki_helper_debug)
+                        cvmx_dprintf("aura alloced is %d\n", gblsch->pool);
+
+        }
+        if (gblsch->setup_sso_grp) {
+                rs = cvmx_sso_allocate_group(node);
+                if (rs < 0) {
+                        cvmx_dprintf("pki-helper:gbl ERROR: sso grp not available\n");
+                        return rs;
+                }
+                gblsch->sso_grp = rs;
+                if (pki_helper_debug)
+                        cvmx_dprintf("pki-helper:qos: sso grp alloced is %d\n", gblsch->sso_grp);
+        }
+        return 0;
 }
+#endif /* CVMX_BUILD_FOR_LINUX_KERNEL */
 
-int cvmx_helper_pki_init_port(int ipd_port, struct cvmx_pki_prt_schd *prtsch)
+int __cvmx_helper_pki_qos_rsrcs(int node, struct cvmx_pki_qos_schd *qossch)
 {
-	int num_qos;
-	int qos;
-	int qpg_base;
-	struct cvmx_pki_qos_schd *qossch;
-	struct cvmx_pki_style_config style_cfg;
-	struct cvmx_pki_pkind_config pknd_cfg;
-	int xiface = cvmx_helper_get_interface_num(ipd_port);
-	int pknd;
-	int rs;
-	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(ipd_port);
-
-	if (prtsch->qpg_qos) {
-		num_qos = cvmx_helper_pki_get_num_qpg_entry(prtsch->qpg_qos);
-		qpg_base = cvmx_pki_qpg_entry_alloc(xp.node, -1, num_qos);
-		for (qos = 0; qos < num_qos; qos++) {
-			qossch = &prtsch->qos_s[qos];
-			cvmx_pki_write_qpg_entry(xp.node, qpg_base + qos, qossch->port_add, qossch->aura,
-						 qossch->sso_grp, qossch->sso_grp);
-		}
-	} else {
-		num_qos = 1;
-		qpg_base = cvmx_pki_qpg_entry_alloc(xp.node, -1, num_qos);
-		cvmx_pki_write_qpg_entry(xp.node, qpg_base, 0, prtsch->aura, prtsch->sso_grp, prtsch->sso_grp);
-		prtsch->qpg_base = qpg_base;
-	}
-
-	/* Allocate style here and map it to the port */
-	rs = cvmx_pki_style_alloc(xp.node, prtsch->style);
-	if (rs == CVMX_RESOURCE_ALREADY_RESERVED) {
-		cvmx_dprintf("passthrough: INFO: style will be shared\n");
-	} else if (rs == CVMX_RESOURCE_ALLOC_FAILED) {
-		cvmx_dprintf("passthrough: ERROR: style not available\n");
-		return CVMX_RESOURCE_ALLOC_FAILED;
-	} else {
-		prtsch->style = rs;
-		cvmx_pki_get_style_config(xp.node, prtsch->style,
-					  CVMX_PKI_CLUSTER_ALL, &style_cfg);
-		style_cfg.parm_cfg.qpg_qos = prtsch->qpg_qos;
-		style_cfg.parm_cfg.qpg_base = prtsch->qpg_base;
-		style_cfg.parm_cfg.qpg_port_msb = 0;
-		style_cfg.parm_cfg.qpg_port_sh = 0;
-		cvmx_pki_set_style_config(xp.node, prtsch->style,
-					  CVMX_PKI_CLUSTER_ALL, &style_cfg);
-	}
-	pknd = cvmx_helper_get_pknd(xiface, cvmx_helper_get_interface_index_num(ipd_port));
-	cvmx_pki_get_pkind_config(xp.node, pknd, &pknd_cfg);
-	pknd_cfg.initial_style = prtsch->style;
-	pknd_cfg.fcs_pres = __cvmx_helper_get_has_fcs(xiface);
-	cvmx_pki_set_pkind_config(xp.node, pknd, &pknd_cfg);
-
-	return 0;
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+        int rs;
+
+        /* Reserve pool resources */
+        if (qossch->pool_per_qos && qossch->pool < 0) {
+                if (pki_helper_debug)
+                        cvmx_dprintf("pki-helper:qos: setup pool %d buff_size %d blocks %d\n",
+                                     qossch->pool, (int)qossch->pool_buff_size, (int)qossch->pool_max_buff);
+                rs = cvmx_helper_fpa3_init_pool(node, node, &qossch->pool,
+                                qossch->pool_buff_size, qossch->pool_max_buff, qossch->pool_name);
+                if (rs < 0) {
+                        cvmx_dprintf("pki-helper:qos:ERROR: pool init failed\n");
+                        return rs;
+                }
+                if (pki_helper_debug)
+                        cvmx_dprintf("pool alloced is %d\n", qossch->pool);
+        }
+        /* Reserve aura resources */
+        if (qossch->aura_per_qos && qossch->aura < 0) {
+                if (pki_helper_debug)
+                        cvmx_dprintf("pki-helper:qos setup aura %d pool %d blocks %d\n",
+                                     qossch->aura, qossch->pool, (int)qossch->aura_buff_cnt);
+                rs = cvmx_helper_fpa3_add_aura_to_pool(node, qossch->pool,
+                                &qossch->aura, qossch->aura_buff_cnt, NULL, qossch->aura_name);
+                if (rs < 0) {
+                        cvmx_dprintf("pki-helper:qos:ERROR: aura init failed\n");
+                        return rs;
+                }
+                if (pki_helper_debug)
+                        cvmx_dprintf("aura alloced is %d\n", qossch->aura);
+        }
+        /* Reserve sso group resources */
+        if (qossch->sso_grp_per_qos && qossch->sso_grp < 0) {
+                rs = cvmx_sso_allocate_group(node);
+                if (rs < 0) {
+                        cvmx_dprintf("pki-helper:qos ERROR: sso grp not available\n");
+                        return rs;
+                }
+                qossch->sso_grp = rs;
+                if (pki_helper_debug)
+                        cvmx_dprintf("pki-helper:qos: sso grp alloced is %d\n", qossch->sso_grp);
+        }
+#endif /* CVMX_BUILD_FOR_LINUX_KERNEL */
+        return 0;
 }
-EXPORT_SYMBOL(cvmx_helper_pki_init_port);
-
-#if 0
 
-int cvmx_helper_pki_set_gbl_schd(int node, struct cvmx_pki_global_schd *gblsch)
+int __cvmx_helper_pki_port_rsrcs(int node, struct cvmx_pki_prt_schd *prtsch)
 {
-	if (gblsch->setup_pool) {
-		if (pki_helper_debug)
-			cvmx_dprintf("pki-helper: setup global pool %d buff_size %d blocks %d",
-				     gblsch->pool, (int)gblsch->pool_buff_size, (int)gblsch->pool_max_buff);
-		cvmx_helper_fpa3_init_pool(node, node, &gblsch->pool, gblsch->pool_buff_size,
-					  gblsch->pool_max_buff, gblsch->pool_name);
-		if (pki_helper_debug)
-			cvmx_dprintf("pool alloced is %d\n", gblsch->pool);
-	}
-	if (gblsch->setup_aura) {
-		if (pki_helper_debug)
-			cvmx_dprintf("pki-helper: setup global aura %d pool %d blocks %d",
-				     gblsch->aura, gblsch->pool, (int)gblsch->aura_buff_cnt);
-		cvmx_helper_fpa3_add_aura_to_pool(node, gblsch->pool, &gblsch->aura,
-				gblsch->aura_buff_cnt, NULL, gblsch->aura_name);
-		if (pki_helper_debug)
-			cvmx_dprintf("aura alloced is %d\n", gblsch->pool);
-
-	}
-	if (gblsch->setup_sso_grp) {
-		/* FIXME: add support in sso to alloc sso group */
-	}
-	return 0;
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+        int rs;
+
+        /* Reserve pool resources */
+        if (prtsch->pool_per_prt && prtsch->pool < 0) {
+                if (pki_helper_debug)
+                        cvmx_dprintf("pki-helper:port setup pool %d buff_size %d blocks %d\n",
+                                     prtsch->pool, (int)prtsch->pool_buff_size, (int)prtsch->pool_max_buff);
+                rs = cvmx_helper_fpa3_init_pool(node, node, &prtsch->pool, prtsch->pool_buff_size,
+                                prtsch->pool_max_buff, prtsch->pool_name);
+                if (rs < 0) {
+                        cvmx_dprintf("pki-helper:port:ERROR: pool init failed\n");
+                        return rs;
+                }
+                if (pki_helper_debug)
+                        cvmx_dprintf("pool alloced is %d\n", prtsch->pool);
+        }
+        /* Reserve aura resources */
+        if (prtsch->aura_per_prt && prtsch->aura < 0) {
+                if (pki_helper_debug)
+                        cvmx_dprintf("pki-helper:port setup aura %d pool %d blocks %d\n",
+                                     prtsch->aura, prtsch->pool, (int)prtsch->aura_buff_cnt);
+                rs = cvmx_helper_fpa3_add_aura_to_pool(node, prtsch->pool,
+                                &prtsch->aura, prtsch->aura_buff_cnt, NULL, prtsch->aura_name);
+                if (rs < 0) {
+                        cvmx_dprintf("pki-helper:port:ERROR: aura init failed\n");
+                        return rs;
+                }
+                if (pki_helper_debug)
+                        cvmx_dprintf("aura alloced is %d\n", prtsch->aura);
+        }
+        /* Reserve sso group resources */
+        if (prtsch->sso_grp_per_prt && prtsch->sso_grp < 0) {
+                rs = cvmx_sso_allocate_group(node);
+                if (rs < 0) {
+                        cvmx_dprintf("pki-helper:port:ERROR: sso grp not available\n");
+                        return rs;
+                }
+                prtsch->sso_grp = rs;
+                if (pki_helper_debug)
+                        cvmx_dprintf("pki-helper:port: sso grp alloced is %d\n", prtsch->sso_grp);
+        }
+#endif /* CVMX_BUILD_FOR_LINUX_KERNEL */
+        return 0;
 }
-
-int cvmx_helper_pki_init_interface(int xiface,
-				   struct cvmx_pki_intf_schd *intf, struct cvmx_pki_global_schd *gbl_schd)
+int __cvmx_helper_pki_intf_rsrcs(int node, struct cvmx_pki_intf_schd *intf)
 {
-	const int num_ports = cvmx_helper_ports_on_interface(xiface);
-	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
-	uint8_t qos;
-	int  port;
-	uint8_t port_msb = 0;
-	uint8_t port_shift = 0;
-	uint16_t num_entry = 0;
-	uint8_t num_qos;
-	int pknd;
-	int rs;
-	int has_fcs;
-	enum cvmx_pki_qpg_qos qpg_qos = CVMX_PKI_QPG_QOS_NONE;
-	struct cvmx_pki_qpg_config *qpg_cfg;
-	struct cvmx_pki_qpg_config *qpg_start;
-	struct cvmx_pki_prt_schd *prtsch;
-	struct cvmx_pki_qos_schd *qossch;
-	struct cvmx_pki_style_config style_cfg;
-	struct cvmx_pki_pkind_config pknd_cfg;
-
-
-	has_fcs = __cvmx_helper_get_has_fcs(xiface);
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+        int rs;
+
+        if (intf->pool_per_intf && intf->pool < 0) {
+                if (pki_helper_debug)
+                        cvmx_dprintf("pki-helper:intf: setup pool %d buff_size %d blocks %d\n",
+                                     intf->pool, (int)intf->pool_buff_size, (int)intf->pool_max_buff);
+                rs = cvmx_helper_fpa3_init_pool(node, node, &intf->pool, intf->pool_buff_size,
+                                intf->pool_max_buff, intf->pool_name);
+                if (rs == CVMX_RESOURCE_ALLOC_FAILED)
+                        return -1;
+                if (pki_helper_debug)
+                        cvmx_dprintf("pool alloced is %d\n", intf->pool);
+
+        }
+        if (intf->aura_per_intf && intf->aura < 0) {
+                if (pki_helper_debug)
+                        cvmx_dprintf("pki-helper:intf: setup aura %d pool %d blocks %d\n",
+                                     intf->aura, intf->pool, (int)intf->aura_buff_cnt);
+                rs = cvmx_helper_fpa3_add_aura_to_pool(node, intf->pool, &intf->aura,
+                                intf->aura_buff_cnt, NULL, intf->aura_name);
+                if (rs == CVMX_RESOURCE_ALLOC_FAILED)
+                        return -1;
+                if (pki_helper_debug)
+                        cvmx_dprintf("aura alloced is %d\n", intf->aura);
+        }
+        if (intf->sso_grp_per_intf && intf->sso_grp < 0) {
+                rs = cvmx_sso_allocate_group(node);
+                if (rs < 0) {
+                        cvmx_dprintf("pki-helper:intf:ERROR: sso grp not available\n");
+                        return rs;
+                }
+                intf->sso_grp = rs;
+        }
+#endif /* CVMX_BUILD_FOR_LINUX_KERNEL */
+        return 0;
+}
 
-	if (intf->pool_per_intf) {
-		if (pki_helper_debug)
-			cvmx_dprintf("pki-helper: setup intf %x pool %d buff_size %d blocks %d\n",
-				     xiface, intf->pool, (int)intf->pool_buff_size, (int)intf->pool_max_buff);
-		cvmx_helper_fpa3_init_pool(xi.node, xi.node, &intf->pool, intf->pool_buff_size,
-					  intf->pool_max_buff, intf->pool_name);
-		if (pki_helper_debug)
-			cvmx_dprintf("pool alloced is %d\n", intf->pool);
+int __cvmx_helper_pki_set_intf_qpg(int node, int port, int qpg_base, int num_entry,
+                                   struct cvmx_pki_intf_schd *intfsch)
+{
+        int offset;
+        int entry;
+        int port_add, aura, grp_ok, grp_bad;
+
+        if (pki_helper_debug)
+                cvmx_dprintf("pki-helper:intf_qpg port %d qpg_base %d num_entry %d",
+                             port, qpg_base, num_entry);
+        offset = cvmx_pki_qpg_entry_alloc(node, qpg_base, num_entry);
+        if (offset == CVMX_RESOURCE_ALREADY_RESERVED) {
+                cvmx_dprintf("pki-helper: INFO: qpg entries will be shared\n");
+                return offset;
+        } else if (offset == CVMX_RESOURCE_ALLOC_FAILED) {
+                cvmx_dprintf("pki-helper: ERROR: qpg entries not available\n");
+                return offset;
+        } else if (intfsch->qpg_base < 0)
+                intfsch->qpg_base = offset;
+                if (pki_helper_debug)
+                        cvmx_dprintf("qpg_base allocated is %d\n",offset);
+                for (entry = 0; entry < num_entry; entry++) {
+                        port_add = intfsch->prt_s[port].qos_s[entry].port_add;
+                        aura = intfsch->prt_s[port].qos_s[entry].aura;
+                        grp_ok = intfsch->prt_s[port].qos_s[entry].sso_grp;
+                        grp_bad = intfsch->prt_s[port].qos_s[entry].sso_grp;
+                        cvmx_pki_write_qpg_entry(node, (offset + entry),
+                                        port_add, aura, grp_ok, grp_bad);
+                }
+                return offset;
+}
 
-	} else
-		intf->pool = gbl_schd->pool;
-	if (intf->aura_per_intf) {
-		if (pki_helper_debug)
-			cvmx_dprintf("pki-helper: setup intf %x aura %d pool %d blocks %d\n",
-				     xiface, intf->aura, intf->pool,
-				     (int)intf->aura_buff_cnt);
-		cvmx_helper_fpa3_add_aura_to_pool(xi.node, intf->pool, &intf->aura,
-				intf->aura_buff_cnt, NULL, intf->aura_name);
-		if (pki_helper_debug)
-			cvmx_dprintf("aura alloced is %d\n", intf->aura);
-	} else
-		intf->aura = gbl_schd->aura;
-	if (intf->sso_grp_per_intf) {
-		/* vinita_to_do for sso groups in here and for ports & qos */
-	} else
-		intf->sso_grp = gbl_schd->sso_grp;
-
-	for (port = 0; port < num_ports; port++) {
-		prtsch = &intf->prt_s[port];
-
-		/* Skip invalid/disabled ports */
-		if (!cvmx_helper_is_port_valid(xiface, port))
-			continue;
-
-		/* If you want to setup pool per port and assign port aura to that individual pool
-		setup pool/port call cvmx_helper_fpa_setup_pool here*/
-		if (prtsch->pool_per_prt) {
-			if (pki_helper_debug)
-				cvmx_dprintf("pki-helper: setup intf %x port %d pool %d buff_size %d blocks %d\n",
-					     xiface, port, prtsch->pool, (int)prtsch->pool_buff_size,
-					     (int)prtsch->pool_max_buff);
-			cvmx_helper_fpa3_init_pool(xi.node, xi.node, &prtsch->pool, prtsch->pool_buff_size,
-					prtsch->pool_max_buff, prtsch->pool_name);
-			if (pki_helper_debug)
-				cvmx_dprintf("pool alloced is %d\n", prtsch->pool);
-		} else
-			prtsch->pool = intf->pool;
-		if (prtsch->aura_per_prt) {
-			if (pki_helper_debug)
-				cvmx_dprintf("pki-helper: setup intf %x port %d aura %d pool %d blocks %d\n",
-					     xiface, port, prtsch->aura, prtsch->pool,
-					     (int)prtsch->aura_buff_cnt);
-			cvmx_helper_fpa3_add_aura_to_pool(xi.node, prtsch->pool,
-				&prtsch->aura, prtsch->aura_buff_cnt, NULL, prtsch->aura_name);
-			if (pki_helper_debug)
-				cvmx_dprintf("aura alloced is %d\n", prtsch->aura);
-		} else
-			prtsch->aura = intf->aura;
-		if (prtsch->sso_grp_per_prt) {
-			/* vinita_to_do for sso groups in here */
-		} else
-			prtsch->sso_grp = intf->sso_grp;
-
-		/* Port is using qpg qos to schedule packets to differnet aura or sso group */
-		num_qos = cvmx_helper_pki_get_num_qpg_entry(prtsch->qpg_qos);
-		/* All ports will share the aura from port 0 for the respective qos */
-		/* Port 0 should never have this set to TRUE **/
-		if (intf->qos_share_aura && (port != 0)) {
-			for (qos = 0; qos < num_qos; qos++) {
-				prtsch->qos_s[qos].pool = intf->prt_s[0].qos_s[qos].pool;
-				prtsch->qos_s[qos].aura = intf->prt_s[0].qos_s[qos].aura;
-			}
 
-		} else {
-			for (qos = 0; qos < num_qos; qos++) {
-				qossch = &prtsch->qos_s[qos];
-
-				/* If you want to setup pool per port and assign port aura to that individual pool
-				setup pool/port call cvmx_helper_fpa_setup_pool here*/
-				if (qossch->pool_per_qos) {
-					if (pki_helper_debug)
-						cvmx_dprintf("pki-helper: setup intf %x port %d qos %d pool %d buff_size %d blocks %d\n",
-								xiface, port, qos, qossch->pool, (int)qossch->pool_buff_size,
-								(int)qossch->pool_max_buff);
-					cvmx_helper_fpa3_init_pool(xi.node, xi.node, &qossch->pool, qossch->pool_buff_size, qossch->pool_max_buff, qossch->pool_name);
-					if (pki_helper_debug)
-						cvmx_dprintf("pool alloced is %d\n", qossch->pool);
-				} else
-					qossch->pool = prtsch->pool;
-				if (qossch->aura_per_qos) {
-					if (pki_helper_debug)
-						cvmx_dprintf("pki-helper: setup intf %x port %d qos %d aura %d pool %d blocks %d\n",
-							xiface, port, qos, qossch->aura, qossch->pool,
-							(int)qossch->aura_buff_cnt);
-					cvmx_helper_fpa3_add_aura_to_pool(xi.node, qossch->pool,
-						&qossch->aura, qossch->aura_buff_cnt, NULL,
-						qossch->aura_name);
-					if (pki_helper_debug)
-						cvmx_dprintf("aura alloced is %d\n", qossch->aura);
-				} else
-					qossch->aura = prtsch->aura;
-			}
-		}
+int cvmx_helper_pki_init_port(int ipd_port, struct cvmx_pki_prt_schd *prtsch)
+{
+        int num_qos;
+        int qos;
+        struct cvmx_pki_qos_schd *qossch;
+        struct cvmx_pki_style_config style_cfg;
+        struct cvmx_pki_pkind_config pknd_cfg;
+        int xiface = cvmx_helper_get_interface_num(ipd_port);
+        int pknd;
+        uint16_t mbuff_size;
+        int rs;
+
+        struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(ipd_port);
+        num_qos = cvmx_helper_pki_get_num_qpg_entry(prtsch->qpg_qos);
+        mbuff_size = prtsch->pool_buff_size;
+
+        /* Reserve port resources */
+        rs = __cvmx_helper_pki_port_rsrcs(xp.node, prtsch);
+        if (rs)
+                return rs;
+        /* Reserve qpg resources */
+        if (prtsch->qpg_base < 0) {
+                rs = cvmx_pki_qpg_entry_alloc(xp.node, prtsch->qpg_base, num_qos);
+                if (rs < 0) {
+                        cvmx_dprintf("pki-helper:port:ERROR: qpg entries not available\n");
+                        return CVMX_RESOURCE_ALLOC_FAILED;
+                }
+                prtsch->qpg_base = rs;
+                if (pki_helper_debug)
+                        cvmx_dprintf("port %d qpg_base %d allocated\n",ipd_port, prtsch->qpg_base);
+        }
+
+        if (prtsch->qpg_qos) {
+                for (qos = 0; qos < num_qos; qos++) {
+                        qossch = &prtsch->qos_s[qos];
+                        if (!qossch->pool_per_qos)
+                                qossch->pool = prtsch->pool;
+                        else if (qossch->pool_buff_size < mbuff_size)
+                                mbuff_size = qossch->pool_buff_size;
+                        if (!qossch->aura_per_qos)
+                                qossch->aura = prtsch->aura;
+                        if (!qossch->sso_grp_per_qos)
+                                qossch->sso_grp = prtsch->sso_grp;
+
+                        /* Reserve qos resources */
+                        rs = __cvmx_helper_pki_qos_rsrcs(xp.node, qossch);
+                        if (rs)
+                                return rs;
+                        cvmx_pki_write_qpg_entry(xp.node, prtsch->qpg_base + qos, qossch->port_add, qossch->aura,
+                                        qossch->sso_grp, qossch->sso_grp);
+                        if (pki_helper_debug)
+                                cvmx_dprintf("port %d qos %d has port_add %d aura %d grp %d\n",
+                                             ipd_port, qos, qossch->port_add, qossch->aura, qossch->sso_grp);
+                }
+        } else
+                cvmx_pki_write_qpg_entry(xp.node, prtsch->qpg_base, 0,
+                                         prtsch->aura, prtsch->sso_grp, prtsch->sso_grp);
+
+                /* Allocate style here and map it to the port */
+                rs = cvmx_pki_style_alloc(xp.node, prtsch->style);
+                if (rs == CVMX_RESOURCE_ALREADY_RESERVED) {
+                        cvmx_dprintf("pki-helper: INFO: style will be shared\n");
+                } else if (rs == CVMX_RESOURCE_ALLOC_FAILED) {
+                        cvmx_dprintf("pki-helper: ERROR: style not available\n");
+                        return CVMX_RESOURCE_ALLOC_FAILED;
+                } else {
+                        prtsch->style = rs;
+                        if (pki_helper_debug)
+                                cvmx_dprintf("port %d has style %d\n", ipd_port,prtsch->style);
+                        style_cfg = pki_dflt_style[xp.node];
+                        style_cfg.parm_cfg.qpg_qos = prtsch->qpg_qos;
+                        style_cfg.parm_cfg.qpg_base = prtsch->qpg_base;
+                        style_cfg.parm_cfg.qpg_port_msb = 0;
+                        style_cfg.parm_cfg.qpg_port_sh = 0;
+                        style_cfg.parm_cfg.mbuff_size = mbuff_size;
+                        cvmx_pki_set_style_config(xp.node, prtsch->style,
+                                        CVMX_PKI_CLUSTER_ALL, &style_cfg);
+                }
+                pknd = cvmx_helper_get_pknd(xiface, cvmx_helper_get_interface_index_num(ipd_port));
+                cvmx_pki_get_pkind_config(xp.node, pknd, &pknd_cfg);
+                pknd_cfg.initial_style = prtsch->style;
+                pknd_cfg.fcs_pres = __cvmx_helper_get_has_fcs(xiface);
+                cvmx_pki_set_pkind_config(xp.node, pknd, &pknd_cfg);
+        //cvmx_pki_show_port(xp.node, xiface, cvmx_helper_get_interface_index_num(ipd_port));
+
+                return 0;
+}
+EXPORT_SYMBOL(cvmx_helper_pki_init_port);
 
-		/* All ports will share the sso grp from port 0 for the respective qos */
-		/* Port 0 should never have this set to TRUE **/
-		num_qos = cvmx_helper_pki_get_num_qpg_entry(prtsch->qpg_qos);
-		if (intf->qos_share_grp && (port != 0)) {
-			for (qos = 0; qos < num_qos; qos++) {
-				prtsch->qos_s[qos].sso_grp = intf->prt_s[0].qos_s[qos].sso_grp;
-			}
-		} else {
-			for (qos = 0; qos < num_qos; qos++) {
-				qossch = &prtsch->qos_s[qos];
-				if (qossch->sso_grp_per_qos) {
-					/* vinita_to_do for sso groups in here */
-				} else
-					qossch->sso_grp = prtsch->sso_grp;
-			}
-		}
-	}
+int cvmx_helper_pki_init_interface(const int xiface,
+                                   struct cvmx_pki_intf_schd *intf, struct cvmx_pki_global_schd *gbl_schd)
+{
+        const uint16_t num_ports = cvmx_helper_ports_on_interface(xiface);
+        uint8_t qos;
+        uint16_t port = num_ports;
+        uint8_t port_msb = 0;
+        uint8_t port_shift = 0;
+        uint16_t num_entry = 0;
+        uint8_t num_qos;
+        int pknd;
+        int rs;
+        int has_fcs;
+        int ipd_port;
+        int qpg_base;
+        uint64_t mbuff_size = 0;
+        struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+        enum cvmx_pki_qpg_qos qpg_qos = CVMX_PKI_QPG_QOS_NONE;
+        struct cvmx_pki_prt_schd *prtsch;
+        struct cvmx_pki_qos_schd *qossch;
+        struct cvmx_pki_style_config style_cfg;
+        struct cvmx_pki_pkind_config pknd_cfg;
+
+        has_fcs = __cvmx_helper_get_has_fcs(xiface);
+
+        if (!intf->pool_per_intf) {
+                intf->pool = gbl_schd->pool;
+        } else
+                mbuff_size = intf->pool_buff_size;
+                if (!intf->aura_per_intf)
+                        intf->aura = gbl_schd->aura;
+                if (!intf->sso_grp_per_intf)
+                        intf->sso_grp = gbl_schd->sso_grp;
+
+                /* Allocate interface resources */
+                rs = __cvmx_helper_pki_intf_rsrcs(xi.node, intf);
+                if (rs)
+                        return rs;
+
+                for (port = 0; port < num_ports; port++) {
+                        prtsch = &intf->prt_s[port];
+
+                        /* Skip invalid/disabled ports */
+                        if (!cvmx_helper_is_port_valid(xiface, port) || prtsch->cfg_port)
+                                continue;
+
+                        if (!prtsch->pool_per_prt)
+                                prtsch->pool = intf->pool;
+                        else if (prtsch->pool_buff_size < mbuff_size || !mbuff_size)
+                                mbuff_size = prtsch->pool_buff_size;
+                        if (!prtsch->aura_per_prt)
+                                prtsch->aura = intf->aura;
+                        if (!prtsch->sso_grp_per_prt)
+                                prtsch->sso_grp = intf->sso_grp;
+
+                        rs = __cvmx_helper_pki_port_rsrcs(xi.node, prtsch);
+                        if (rs)
+                                return rs;
+
+                        /* Port is using qpg qos to schedule packets to differnet aura or sso group */
+                        num_qos = cvmx_helper_pki_get_num_qpg_entry(prtsch->qpg_qos);
+
+                        /* All ports will share the aura from port 0 for the respective qos */
+                        /* Port 0 should never have this set to TRUE **/
+                        if (intf->qos_share_aura && (port != 0)) {
+                                if (pki_helper_debug)
+                                        cvmx_dprintf("All ports will share same aura for all qos\n");
+                                for (qos = 0; qos < num_qos; qos++) {
+                                        qossch = &prtsch->qos_s[qos];
+                                        prtsch->qpg_qos = intf->prt_s[0].qpg_qos;
+                                        qossch->pool_per_qos = intf->prt_s[0].qos_s[qos].pool_per_qos;
+                                        qossch->aura_per_qos = intf->prt_s[0].qos_s[qos].aura_per_qos;
+                                        qossch->pool = intf->prt_s[0].qos_s[qos].pool;
+                                        qossch->aura = intf->prt_s[0].qos_s[qos].aura;
+                                }
+
+                        }
+                        if (intf->qos_share_grp && (port != 0)) {
+                                if (pki_helper_debug)
+                                        cvmx_dprintf("All ports will share same sso group for all qos\n");
+                                for (qos = 0; qos < num_qos; qos++) {
+                                        qossch = &prtsch->qos_s[qos];
+                                        qossch->sso_grp_per_qos = intf->prt_s[0].qos_s[qos].sso_grp_per_qos;
+                                        qossch->sso_grp = intf->prt_s[0].qos_s[qos].sso_grp;
+                                }
+                        }
+                        for (qos = 0; qos < num_qos; qos++) {
+                                qossch = &prtsch->qos_s[qos];
+                                if (!qossch->pool_per_qos) {
+                                        qossch->pool = prtsch->pool;
+                                //cvmx_dprintf("qos %d pool %d\n", qos, prtsch->pool);
+                                }
+                                else if (qossch->pool_buff_size < mbuff_size || !mbuff_size)
+                                        mbuff_size = qossch->pool_buff_size;
+                                if (!qossch->aura_per_qos)
+                                        qossch->aura = prtsch->aura;
+                                if (!qossch->sso_grp_per_qos)
+                                        qossch->sso_grp = prtsch->sso_grp;
+                                rs = __cvmx_helper_pki_qos_rsrcs(xi.node, qossch);
+                                if (rs)
+                                        return rs;
+                        }
+                }
 	/* Using port shift and port msb to schedule packets from differnt port to differnt
-	auras and different sso group */
-	/* Using QPG_QOS to schedule packets to different aura and sso group */
+                auras and different sso group */
+                /* Using QPG_QOS to schedule packets to different aura and sso group */
 	/* If ports needs to send packets to different aura and sso group
-	depending on packet qos */
-	/* We will need to set up aura and sso group for each port and each qos */
+                depending on packet qos */
+                /* We will need to set up aura and sso group for each port and each qos */
 	/* If all ports are using same style, they will be using same qpg_qos so
-	check only for port 0*/
-	if (intf->style_per_intf) {
-		if (intf->prt_s[0].qpg_qos) { /* all ports using same style will use same qos defined in port 0 config */
-			qpg_qos = intf->prt_s[0].qpg_qos;
-			num_qos = cvmx_helper_pki_get_num_qpg_entry(intf->prt_s[0].qpg_qos);
-			if (intf->qos_share_aura && intf->qos_share_grp) {
+                check only for port 0*/
+                if (intf->style_per_intf) {
+                        if (intf->prt_s[0].qpg_qos) { /* all ports using same style will use same qos defined in port 0 config */
+                                qpg_qos = intf->prt_s[0].qpg_qos;
+                                num_qos = cvmx_helper_pki_get_num_qpg_entry(intf->prt_s[0].qpg_qos);
+                                if (intf->qos_share_aura && intf->qos_share_grp) {
 				/* All ports will use same qpg offset so no need for
-				port_msb or port shift */
-				port_msb = 0;
-				port_shift = 0;
-				num_entry = num_qos;
-				qpg_start = malloc(sizeof(struct cvmx_pki_qpg_config) * num_qos);
-				qpg_start->qpg_base = intf->qpg_base;
-				qpg_cfg = qpg_start;
-				for (qos = 0; qos < num_qos; qos++, qpg_cfg++) {
-					qpg_cfg->port_add = intf->prt_s[0].qos_s[qos].port_add;
-					qpg_cfg->aura = intf->prt_s[0].qos_s[qos].aura;
-					qpg_cfg->grp_ok = intf->prt_s[0].qos_s[qos].sso_grp;
-					qpg_cfg->grp_bad = intf->prt_s[0].qos_s[qos].sso_grp;
-				}
-			} else {
-				port_msb = cvmx_helper_pki_port_msb(num_ports);
-				port_shift = cvmx_helper_pki_port_shift(intf->prt_s[0].qpg_qos);
-				qpg_start = malloc(num_ports * num_qos *
-						(sizeof(struct cvmx_pki_qpg_config)));
-				qpg_start->qpg_base = intf->qpg_base;
-				qpg_cfg = qpg_start;
-				num_entry = num_ports * num_qos;
-				for (port = 0; port < num_ports; port++) {
-					prtsch = &intf->prt_s[port];
-					for (qos = 0; qos < num_qos; qos++, qpg_cfg++) {
-						qossch = &prtsch->qos_s[qos];
-						qpg_cfg->port_add = qossch->port_add;
-						qpg_cfg->aura = qossch->aura;
-						qpg_cfg->grp_ok = qossch->sso_grp;
-						qpg_cfg->grp_bad = qossch->sso_grp;
-					}
-				}
-			}
-		} else if (intf->prt_s[0].aura_per_prt || intf->prt_s[0].sso_grp_per_prt) {
-			/* Every port is using their own aura or group */
-			port_msb = cvmx_helper_pki_port_msb(num_ports);
-			port_shift = 0;
-			num_entry = num_ports;
-			qpg_start = malloc(num_ports * (sizeof(struct cvmx_pki_qpg_config)));
-			qpg_start->qpg_base = intf->qpg_base;
-			qpg_cfg = qpg_start;
-			for (port = 0; port < num_ports; port++, qpg_cfg++) {
-				prtsch = &intf->prt_s[port];
-				qpg_cfg->port_add = 0;
-				qpg_cfg->aura = prtsch->aura;
-				qpg_cfg->grp_ok = prtsch->sso_grp;
-				qpg_cfg->grp_bad = prtsch->sso_grp;
-			}
-		} else { /* All ports on that intf use same port_add, aura & sso grps */
+                                        port_msb or port shift */
+                                        port_msb = 0;
+                                        port_shift = 0;
+                                        num_entry = num_qos;
+                                        qpg_base = intf->qpg_base;
+                                        rs = __cvmx_helper_pki_set_intf_qpg(xi.node, 0, qpg_base, num_entry, intf);
+                                        if (rs == -1)
+                                                return rs;
+                                        intf->qpg_base = rs;
+                                } else {
+                                        port_msb = 8;
+                                        port_shift = cvmx_helper_pki_port_shift(xiface, intf->prt_s[0].qpg_qos);
+                                //cvmx_dprintf("pki-helper: num qpg entry needed %d\n", (int)num_entry);
+                                //cvmx_dprintf("pki-helper:port_msb= %d port_shift=%d\n", port_msb, port_shift);
+                                        num_entry = num_qos;
+                                        for (port = 0; port < num_ports; port++) {
+                                                /* Skip invalid/disabled ports */
+                                                prtsch =  &intf->prt_s[port];
+                                                if (!cvmx_helper_is_port_valid(xiface, port) || prtsch->cfg_port)
+                                                        continue;
+                                                ipd_port = cvmx_helper_get_ipd_port(xiface, port);
+                                                qpg_base = intf->qpg_base + ((ipd_port & 0xff) << port_shift);
+                                                rs = __cvmx_helper_pki_set_intf_qpg(xi.node, port, qpg_base, num_entry, intf);
+                                                if (rs == -1)
+                                                        return rs;
+                                                prtsch->qpg_base = rs;
+                                        }
+                                        intf->qpg_base = intf->prt_s[0].qpg_base;
+                                }
+                        } else if (intf->prt_s[0].aura_per_prt || intf->prt_s[0].sso_grp_per_prt) {
+                                /* Every port is using their own aura or group but no qos */
+                                port_msb = 8;
+                                port_shift = 0;
+                                num_entry = 1;
+                        //cvmx_dprintf("pki-helper: aura/grp_per_prt: num qpg entry needed %d\n", (int)num_entry);
+                                for (port = 0; port < num_ports; port++) {
+                                        prtsch =  &intf->prt_s[port];
+                                        /* Skip invalid/disabled ports */
+                                        if (!cvmx_helper_is_port_valid(xiface, port) || prtsch->cfg_port)
+                                                continue;
+                                        ipd_port = cvmx_helper_get_ipd_port(xiface, port);
+                                        qpg_base = intf->qpg_base + ((ipd_port & 0xff) << port_shift);
+                                //cvmx_dprintf("port %d intf_q_base=%d q_base= %d\n", port, intf->qpg_base, qpg_base);
+                                        qpg_base = cvmx_pki_qpg_entry_alloc(xi.node, qpg_base, num_entry);
+                                        if (qpg_base == CVMX_RESOURCE_ALREADY_RESERVED) {
+                                                cvmx_dprintf("pki-helper: INFO: qpg entries will be shared\n");
+                                        } else if (qpg_base == CVMX_RESOURCE_ALLOC_FAILED) {
+                                                cvmx_dprintf("pki-helper: ERROR: qpg entries not available\n");
+                                                return qpg_base;
+                                        } else {
+                                                if (intf->qpg_base < 0)
+                                                        intf->qpg_base = qpg_base;
+                                                prtsch->qpg_base = qpg_base;
+                                        }
+                                        cvmx_pki_write_qpg_entry(xi.node, qpg_base, 0, prtsch->aura,                                                prtsch->sso_grp, prtsch->sso_grp);
+                                }
+                                intf->qpg_base = intf->prt_s[0].qpg_base;
+                        } else { /* All ports on that intf use same port_add, aura & sso grps */
 			/* All ports will use same qpg offset so no need for
-			port_msb or port shift */
-			port_msb = 0;
-			port_shift = 0;
-			num_entry = 1;
-			qpg_start = malloc(sizeof(struct cvmx_pki_qpg_config));
-			qpg_start->qpg_base = intf->qpg_base;
-			qpg_start->port_add = 0;
-			qpg_start->aura = intf->aura;
-			qpg_start->grp_ok = intf->sso_grp;
-			qpg_start->grp_bad = intf->sso_grp;
-		}
+                                port_msb or port shift */
+                                port_msb = 0;
+                                port_shift = 0;
+                                num_entry = 1;
+                                qpg_base = intf->qpg_base;
+                                qpg_base = cvmx_pki_qpg_entry_alloc(xi.node, qpg_base, num_entry);
+                                if (qpg_base == CVMX_RESOURCE_ALREADY_RESERVED) {
+                                        cvmx_dprintf("pki-helper: INFO: qpg entries will be shared\n");
+                                } else if (qpg_base == CVMX_RESOURCE_ALLOC_FAILED) {
+                                        cvmx_dprintf("pki-helper: ERROR: qpg entries not available\n");
+                                        return qpg_base;
+                                } else
+                                        intf->qpg_base = qpg_base;
+                                        cvmx_pki_write_qpg_entry(xi.node, qpg_base, 0, intf->aura,
+                                                        intf->sso_grp, intf->sso_grp);
+                        }
+                        if (!mbuff_size) {
+                                if (!gbl_schd->setup_pool) {
+                                        cvmx_dprintf("No pool has setup for intf %d\n", xiface);
+                                        return -1;
+                                }
+                                mbuff_size = gbl_schd->pool_buff_size;
+                                cvmx_dprintf("interface %d is using global pool\n", xiface);
+                        }
+                        /* Allocate style here and map it to all ports on interface */
+                        rs = cvmx_pki_style_alloc(xi.node, intf->style);
+                        if (rs == CVMX_RESOURCE_ALREADY_RESERVED) {
+                                cvmx_dprintf("passthrough: INFO: style will be shared\n");
+                        } else if (rs == CVMX_RESOURCE_ALLOC_FAILED) {
+                                cvmx_dprintf("passthrough: ERROR: style not available\n");
+                                return CVMX_RESOURCE_ALLOC_FAILED;
+                        } else {
+                                intf->style = rs;
+                                if (pki_helper_debug)
+                                        cvmx_dprintf("style %d allocated intf %d qpg_base %d\n", intf->style, xiface, intf->qpg_base);
+                                style_cfg = pki_dflt_style[xi.node];
+                                style_cfg.parm_cfg.qpg_qos = qpg_qos;
+                                style_cfg.parm_cfg.qpg_base = intf->qpg_base;
+                                style_cfg.parm_cfg.qpg_port_msb = port_msb;
+                                style_cfg.parm_cfg.qpg_port_sh = port_shift;
+                                style_cfg.parm_cfg.mbuff_size = mbuff_size;
+                                cvmx_pki_set_style_config(xi.node, intf->style,
+                                                CVMX_PKI_CLUSTER_ALL, &style_cfg);
+                        }
+                        for (port = 0; port < num_ports; port++) {
+                                prtsch = &intf->prt_s[port];
+                                /* Skip invalid/disabled ports */
+                                if (!cvmx_helper_is_port_valid(xiface, port) || prtsch->cfg_port)
+                                        continue;
+                                prtsch->style = intf->style;
+                                pknd = cvmx_helper_get_pknd(xiface, port);
+                                cvmx_pki_get_pkind_config(xi.node, pknd, &pknd_cfg);
+                                pknd_cfg.initial_style = intf->style;
+                                pknd_cfg.fcs_pres = has_fcs;
+                                cvmx_pki_set_pkind_config(xi.node, pknd, &pknd_cfg);
+                        //cvmx_pki_show_port(xi.node, xiface, port);
+                        }
+                } else if (intf->style_per_prt) {
+                        port_msb = 0;
+                        port_shift = 0;
+                        for (port = 0; port < num_ports; port++) {
+                                prtsch = &intf->prt_s[port];
+                                /* Skip invalid/disabled ports */
+                                if (!cvmx_helper_is_port_valid(xiface, port) || prtsch->cfg_port)
+                                        continue;
+                                if (prtsch->qpg_qos && intf->qos_share_aura &&
+                                    intf->qos_share_grp && port !=0) {
+                                        if(pki_helper_debug)
+                                                cvmx_dprintf("intf %d has all ports share qos aura n grps\n",xiface);
+                                /* Ports have differnet styles but want to share same qpg entries.
+                                        this might never be the case */
+                                        prtsch->qpg_base = intf->prt_s[0].qpg_base;
+                                    }
+                                    ipd_port = cvmx_helper_get_ipd_port(xiface, port);
+                                    cvmx_helper_pki_init_port(ipd_port, prtsch);
+                        }
+                }
+                return 0;
+}
 
-		rs = cvmx_helper_pki_setup_qpg_table(xi.node, num_entry, qpg_start);
-		if (rs == CVMX_RESOURCE_ALREADY_RESERVED) {
-			cvmx_dprintf("passthrough: INFO: qpg entries will be shared\n");
-		} else if (rs == CVMX_RESOURCE_ALLOC_FAILED) {
-			cvmx_dprintf("passthrough: ERROR: qpg entries not available\n");
-			free(qpg_start);
-			return CVMX_RESOURCE_ALLOC_FAILED;
-		} else
-			intf->qpg_base = qpg_start->qpg_base;
-
-		/* Allocate style here and map it to all ports on interface */
-		rs = cvmx_pki_style_alloc(xi.node, intf->style);
-		if (rs == CVMX_RESOURCE_ALREADY_RESERVED) {
-			cvmx_dprintf("passthrough: INFO: style will be shared\n");
-		} else if (rs == CVMX_RESOURCE_ALLOC_FAILED) {
-			cvmx_dprintf("passthrough: ERROR: style not available\n");
-			return CVMX_RESOURCE_ALLOC_FAILED;
-		} else {
-			intf->style = rs;
-			if (pki_helper_debug)
-				cvmx_dprintf("style %d allocated intf %x qpg_base %d\n", intf->style, xiface, intf->qpg_base);
-			cvmx_pki_get_style_config(xi.node, intf->style,
-					CVMX_PKI_CLUSTER_ALL, &style_cfg);
-			style_cfg.parm_cfg.qpg_qos = qpg_qos;
-			style_cfg.parm_cfg.qpg_base = intf->qpg_base;
-			style_cfg.parm_cfg.qpg_port_msb = port_msb;
-			style_cfg.parm_cfg.qpg_port_sh = port_shift;
-			cvmx_pki_set_style_config(xi.node, intf->style,
-					CVMX_PKI_CLUSTER_ALL, &style_cfg);
-		}
-		for (port = 0; port < num_ports; port++) {
-			pknd = cvmx_helper_get_pknd(xiface, port);
-			cvmx_pki_get_pkind_config(xi.node, pknd, &pknd_cfg);
-			pknd_cfg.initial_style = intf->style;
-			pknd_cfg.fcs_pres = has_fcs;
-			cvmx_pki_set_pkind_config(xi.node, pknd, &pknd_cfg);
-		}
-	} else if (intf->style_per_prt) {
-		for (port = 0; port < num_ports; port++) {
-			int r;
-			int ipd_port = cvmx_helper_get_ipd_port(xiface, port);
-			prtsch = &intf->prt_s[port];
-			r = cvmx_helper_pki_init_port(ipd_port, prtsch);
-			if (r)
-				return r;
-		}
-	}
-	return 0;
+#if 0
+static const char* pki_ltype_sprint(int ltype) {
+        switch (ltype) {
+                case CVMX_PKI_LTYPE_E_ENET:	return "(ENET)";
+                case CVMX_PKI_LTYPE_E_VLAN:	return "(VLAN)";
+                case CVMX_PKI_LTYPE_E_SNAP_PAYLD:return "(SNAP_PAYLD)";
+                case CVMX_PKI_LTYPE_E_ARP:	return "(ARP)";
+                case CVMX_PKI_LTYPE_E_RARP:	return "(RARP)";
+                case CVMX_PKI_LTYPE_E_IP4:	return "(IP4)";
+                case CVMX_PKI_LTYPE_E_IP4_OPT:	return "(IP4_OPT)";
+                case CVMX_PKI_LTYPE_E_IP6:	return "(IP6)";
+                case CVMX_PKI_LTYPE_E_IP6_OPT:	return "(IP6_OPT)";
+                case CVMX_PKI_LTYPE_E_IPSEC_ESP:	return "(IPSEC_ESP)";
+                case CVMX_PKI_LTYPE_E_IPFRAG:	return "(IPFRAG)";
+                case CVMX_PKI_LTYPE_E_IPCOMP:	return "(IPCOMP)";
+                case CVMX_PKI_LTYPE_E_TCP:	return "(TCP)";
+                case CVMX_PKI_LTYPE_E_UDP:	return "(UDP)";
+                case CVMX_PKI_LTYPE_E_SCTP:	return "(SCTP)";
+                case CVMX_PKI_LTYPE_E_UDP_VXLAN:	return "(UDP_VXLAN)";
+                case CVMX_PKI_LTYPE_E_GRE:	return "(GRE)";
+                case CVMX_PKI_LTYPE_E_NVGRE:	return "(NVGRE)";
+                case CVMX_PKI_LTYPE_E_GTP:	return "(GTP)";
+                default: 			return "";
+}
 }
 
+void pki_wqe_dump(const cvmx_wqe_78xx_t* wqp)
+{
+        int i;
+        /* it is not cvmx_shared so per core only */
+        static uint64_t count;
+
+        printf("Wqe entry for packet %ld\n", count++);
+        printf("    WORD%02d: %016lx", (int)0, wqp->word0.u64);
+        printf(" aura=0x%x", wqp->word0.aura);
+        printf(" apad=%d", wqp->word0.apad);
+        printf(" chan=0x%x", wqp->word0.channel);
+        printf(" bufs=%d" , wqp->word0.bufs);
+        printf(" style=0x%x" , wqp->word0.style);
+        printf(" pknd=0x%x" , wqp->word0.pknd);
+        printf("\n");
+        printf("    WORD%02d: %016lx", (int)1, wqp->word1.u64);
+        printf(" len=%d" , wqp->word1.len);
+        printf(" grp=0x%x" , wqp->word1.grp);
+        printf(" tt=%s", OCT_TAG_TYPE_STRING(wqp->word1.tag_type));
+        printf(" tag=0x%08x" , wqp->word1.tag);
+        printf("\n");
+        if (wqp->word2.u64) {
+                printf("    WORD%02d: %016lx"  , (int)2, wqp->word2.u64);
+                if (wqp->word2.le_hdr_type)
+                        printf(" [LAE]");
+                if (wqp->word2.lb_hdr_type)
+                        printf(" lbty=%d"  "%s",
+                               wqp->word2.lb_hdr_type, pki_ltype_sprint(wqp->word2.lb_hdr_type));
+                if (wqp->word2.lc_hdr_type)
+                        printf(" lcty=%d"  "%s",
+                               wqp->word2.lc_hdr_type, pki_ltype_sprint(wqp->word2.lc_hdr_type));
+                if (wqp->word2.ld_hdr_type)
+                        printf(" ldty=%d"  "%s",
+                               wqp->word2.ld_hdr_type, pki_ltype_sprint(wqp->word2.ld_hdr_type));
+                if (wqp->word2.le_hdr_type)
+                        printf(" lety=%d"  "%s",
+                               wqp->word2.le_hdr_type, pki_ltype_sprint(wqp->word2.le_hdr_type));
+                if (wqp->word2.lf_hdr_type)
+                        printf(" lfty=%d"  "%s",
+                               wqp->word2.lf_hdr_type, pki_ltype_sprint(wqp->word2.lf_hdr_type));
+                if (wqp->word2.lg_hdr_type)
+                        printf(" lgty=%d"  "%s",
+                               wqp->word2.lg_hdr_type, pki_ltype_sprint(wqp->word2.lg_hdr_type));
+                if (wqp->word2.pcam_flag1)
+                        printf(" PF1");
+                if (wqp->word2.pcam_flag2)
+                        printf(" PF2");
+                if (wqp->word2.pcam_flag3)
+                        printf(" PF3");
+                if (wqp->word2.pcam_flag4)
+                        printf(" PF4");
+                if (wqp->word2.vlan_valid || wqp->word2.vlan_stacked) {
+                        if (wqp->word2.vlan_valid)
+                                printf(" vlan valid");
+                        if (wqp->word2.vlan_stacked)
+                                printf(" vlan stacked");
+                        printf(" ");
+}
+                if (wqp->word2.stat_inc) printf(" stat_inc");
+                if (wqp->word2.is_frag) printf(" L3 Fragment");
+                if (wqp->word2.is_l3_bcast) printf(" L3 Broadcast");
+                if (wqp->word2.is_l3_mcast) printf(" L3 Multicast");
+                if (wqp->word2.is_l2_bcast) printf(" L2 Broadcast");
+                if (wqp->word2.is_l2_mcast) printf(" L2 Multicast");
+                if (wqp->word2.is_raw) printf(" RAW");
+                if (wqp->word2.err_level || wqp->word2.err_code) {
+                        printf(" errlev=%d" , wqp->word2.err_level);
+                        printf(" opcode=0x%x" , wqp->word2.err_code);
+}
+                printf("\n");
+}
+        printf("    WORD%02d: %016lx", (int)3, wqp->packet_ptr.u64);
+
+        printf(" size=%d" , wqp->packet_ptr.size);
+        printf(" addr=0x%llx" , (unsigned long long)wqp->packet_ptr.addr);
+
+        printf("\n");
+        if (wqp->word4.u64) {
+                printf("    WORD%02d: %016lx", (int)4, wqp->word4.u64);
+                if (wqp->word4.ptr_layer_a) printf(" laptr=%d" , wqp->word4.ptr_layer_a);
+                if (wqp->word4.ptr_layer_b) printf(" lbptr=%d" , wqp->word4.ptr_layer_b);
+                if (wqp->word4.ptr_layer_c) printf(" lcptr=%d" , wqp->word4.ptr_layer_c);
+                if (wqp->word4.ptr_layer_d) printf(" ldptr=%d" , wqp->word4.ptr_layer_d);
+                if (wqp->word4.ptr_layer_e) printf(" leptr=%d" , wqp->word4.ptr_layer_e);
+                if (wqp->word4.ptr_layer_f) printf(" lfptr=%d" , wqp->word4.ptr_layer_f);
+                if (wqp->word4.ptr_layer_g) printf(" lgptr=%d" , wqp->word4.ptr_layer_g);
+                if (wqp->word4.ptr_vlan) printf(" vlptr=%d" , wqp->word4.ptr_vlan);
+                printf("\n");
+}
+        for (i=0; i < 10; ++i) {
+                if (wqp->wqe_data[i]) printf("    WORD%02d: %016lx"  "\n", i+5, wqp->wqe_data[i]);
+}
+}
 #endif
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-pko.c b/arch/mips/cavium-octeon/executive/cvmx-helper-pko.c
index fcdc009..e7231f4 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-pko.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-pko.c
@@ -42,7 +42,7 @@
  *
  * Helper Functions for the PKO
  *
- * $Id: cvmx-helper-pko.c 95258 2014-03-18 23:43:53Z ddaney $
+ * $Id: cvmx-helper-pko.c 96989 2014-04-23 19:40:33Z ajasty $
  */
 
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
@@ -149,6 +149,11 @@ static int cvmx_helper_pko_pool_init(void)
 	buf_count = CVMX_PKO_MAX_OUTPUT_QUEUES + (pkt_buf_count * 3) / 8;
 
 	/* Allocate pools for pko command queues */
+	if (!cvmx_fpa_is_pool_available(-1, pool)) {
+		cvmx_helper_fpa_fill_pool(pool, buf_count, NULL);
+		return pool;
+	}
+	/* Allocate pools for pko command queues */
 	rc = __cvmx_helper_initialize_fpa_pool(pool,
 				cvmx_fpa_get_pko_pool_block_size(),
 				buf_count, "PKO Cmd-bufs");
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.c b/arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.c
index cfa9cc4..29c2522 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.c
@@ -43,7 +43,7 @@
  * Functions for SGMII initialization, configuration,
  * and monitoring.
  *
- * <hr>$Revision: 96598 $<hr>
+ * <hr>$Revision: 97202 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
@@ -214,6 +214,15 @@ static int __cvmx_helper_sgmii_hardware_init_link(int interface, int index)
 	 * sgmii negotiation starts.
 	 */
 	control_reg.s.an_en = 1;
+	/* Force a PCS reset by powering down the PCS interface */
+	control_reg.s.pwr_dn = 1;
+	cvmx_write_csr(CVMX_PCSX_MRX_CONTROL_REG(index, interface),
+		       control_reg.u64);
+
+	if (cvmx_sysinfo_get()->board_type != CVMX_BOARD_TYPE_SIM)
+		/* 25ms should be enough, 10ms is too short */
+		cvmx_wait_usec(25000);
+
 	control_reg.s.pwr_dn = 0;
 	cvmx_write_csr(CVMX_PCSX_MRX_CONTROL_REG(index, interface),
 		       control_reg.u64);
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-util.c b/arch/mips/cavium-octeon/executive/cvmx-helper-util.c
index e74cadf..3b376e0 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-util.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-util.c
@@ -615,6 +615,7 @@ void cvmx_helper_setup_legacy_red(int pass_thresh, int drop_thresh)
 {
 	unsigned int node = cvmx_get_node_num();
 	int aura, bpid;
+	int buf_cnt;
 	bool ena_red = 0, ena_drop = 0, ena_bp = 0;
 
 #define FPA_RED_AVG_DLY	1
@@ -630,6 +631,10 @@ void cvmx_helper_setup_legacy_red(int pass_thresh, int drop_thresh)
 	/* Assumption is that all packets from all interface and ports goes in same poolx/aurax
 	for backward compatibility*/
 	aura = cvmx_fpa_get_packet_pool();
+	buf_cnt = cvmx_fpa_get_packet_pool_buffer_count();
+	pass_thresh = buf_cnt - pass_thresh;
+	drop_thresh = buf_cnt - drop_thresh;
+
 	/* Map aura to bpid 0*/
 	bpid = 0;
 	cvmx_pki_write_aura_bpid(node, aura, bpid);
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper.c b/arch/mips/cavium-octeon/executive/cvmx-helper.c
index 2f973b9..38dda54 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper.c
@@ -52,6 +52,7 @@
 #include <asm/octeon/cvmx-sriox-defs.h>
 #include <asm/octeon/cvmx-npi-defs.h>
 #include <asm/octeon/cvmx-mio-defs.h>
+#include <asm/octeon/cvmx-pcsx-defs.h>
 #include <asm/octeon/cvmx-pexp-defs.h>
 #include <asm/octeon/cvmx-pip-defs.h>
 #include <asm/octeon/cvmx-asxx-defs.h>
@@ -1908,6 +1909,7 @@ int cvmx_helper_shutdown_packet_io_global(void)
 	uint64_t packet_pool_size = cvmx_fpa_get_packet_pool_block_size();
 	int wqe_pool = (int)cvmx_fpa_get_wqe_pool();
 	int node = cvmx_get_node_num();
+	cvmx_pcsx_mrx_control_reg_t control_reg;
 
 	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
 		return __cvmx_helper_shutdown_packet_io_global_cn78xx(node);
@@ -2018,6 +2020,22 @@ step2:
 					cvmx_dprintf("GMX TX path timeout waiting for idle\n");
 					result = -1;
 				}
+				/* For SGMII some PHYs require that the PCS
+				 * interface be powered down and reset (i.e.
+				 * Atheros/Qualcomm PHYs).
+				 */
+				if (cvmx_helper_interface_get_mode(interface) ==
+				    CVMX_HELPER_INTERFACE_MODE_SGMII) {
+					uint64_t reg;
+
+					reg = CVMX_PCSX_MRX_CONTROL_REG(index,
+									interface);
+					/* Power down the interface */
+					control_reg.u64 = cvmx_read_csr(reg);
+					control_reg.s.pwr_dn = 1;
+					cvmx_write_csr(reg, control_reg.u64);
+					cvmx_read_csr(reg);
+				}
 			}
 			break;
 		case CVMX_HELPER_INTERFACE_MODE_AGL:
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pcie.c b/arch/mips/cavium-octeon/executive/cvmx-pcie.c
index b8a2712..a13536b 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pcie.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pcie.c
@@ -42,7 +42,7 @@
  *
  * Interface to PCIe as a host(RC) or target(EP)
  *
- * <hr>$Revision: 96178 $<hr>
+ * <hr>$Revision: 97385 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
@@ -1686,7 +1686,7 @@ extern int cvmx_pcie_is_host_mode(int pcie_port);
  */
 int cvmx_pcie_ep_initialize(int pcie_port)
 {
-	if (!cvmx_pcie_is_host_mode(pcie_port))
+	if (cvmx_pcie_is_host_mode(pcie_port))
 		return -1;
 
 	/* CN63XX Pass 1.0 errata G-14395 requires the QLM De-emphasis be
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pki.c b/arch/mips/cavium-octeon/executive/cvmx-pki.c
index c77e3d3..14e8371 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pki.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pki.c
@@ -156,13 +156,61 @@ void __cvmx_pki_free_ptr(int node)
 	cvmx_write_csr_node(node, CVMX_PKI_BUF_CTL, buf_ctl.u64);
 }
 
+void cvmx_pki_read_global_cfg(int node, struct cvmx_pki_global_config *gbl_cfg)
+{
+        cvmx_pki_stat_ctl_t stat_ctl;
+        cvmx_pki_icgx_cfg_t pki_cl_grp;
+        cvmx_pki_gbl_pen_t gbl_pen_reg;
+        cvmx_pki_tag_secret_t tag_secret_reg;
+        cvmx_pki_frm_len_chkx_t frm_len_chk;
+        int cl_grp;
+        int id;
+
+	stat_ctl.u64 = cvmx_read_csr_node(node, CVMX_PKI_STAT_CTL);
+        gbl_cfg->stat_mode = stat_ctl.s.mode;
+
+        for (cl_grp = 0; cl_grp < CVMX_PKI_NUM_CLUSTER_GROUP; cl_grp++) {
+                pki_cl_grp.u64 = cvmx_read_csr_node(node, CVMX_PKI_ICGX_CFG(cl_grp));
+                gbl_cfg->cluster_mask[cl_grp] = pki_cl_grp.s.clusters;
+        }
+	gbl_pen_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_GBL_PEN);
+	gbl_cfg->gbl_pen.virt_pen = gbl_pen_reg.s.virt_pen;
+	gbl_cfg->gbl_pen.clg_pen = gbl_pen_reg.s.clg_pen;
+	gbl_cfg->gbl_pen.cl2_pen = gbl_pen_reg.s.cl2_pen;
+	gbl_cfg->gbl_pen.l4_pen = gbl_pen_reg.s.l4_pen;
+	gbl_cfg->gbl_pen.il3_pen = gbl_pen_reg.s.il3_pen;
+	gbl_cfg->gbl_pen.l3_pen = gbl_pen_reg.s.l3_pen;
+	gbl_cfg->gbl_pen.mpls_pen = gbl_pen_reg.s.mpls_pen;
+	gbl_cfg->gbl_pen.fulc_pen = gbl_pen_reg.s.fulc_pen;
+	gbl_cfg->gbl_pen.dsa_pen = gbl_pen_reg.s.dsa_pen;
+	gbl_cfg->gbl_pen.hg_pen = gbl_pen_reg.s.hg_pen;
+
+	tag_secret_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_TAG_SECRET);
+	gbl_cfg->tag_secret.dst6 = tag_secret_reg.s.dst6;
+	gbl_cfg->tag_secret.src6 = tag_secret_reg.s.src6;
+	gbl_cfg->tag_secret.dst = tag_secret_reg.s.dst;
+	gbl_cfg->tag_secret.src = tag_secret_reg.s.src;
+
+        for (id = 0; id < CVMX_PKI_NUM_FRAME_CHECK; id++) {
+                frm_len_chk.u64 = cvmx_read_csr_node(node, CVMX_PKI_FRM_LEN_CHKX(id));
+                gbl_cfg->frm_len[id].maxlen = frm_len_chk.s.maxlen;
+                gbl_cfg->frm_len[id].minlen = frm_len_chk.s.minlen;
+        }
+	/* vinita_to_do remaining parameters */
+}
+
 void cvmx_pki_write_global_cfg(int node, struct cvmx_pki_global_config *gbl_cfg)
 {
+	cvmx_pki_stat_ctl_t stat_ctl;
 	int cl_grp;
 
 	for (cl_grp = 0; cl_grp < CVMX_PKI_NUM_CLUSTER_GROUP; cl_grp++)
 		cvmx_pki_attach_cluster_to_group(node, cl_grp, gbl_cfg->cluster_mask[cl_grp]);
-	cvmx_pki_write_stats_mode(node, gbl_cfg->stat_mode);
+
+	stat_ctl.u64 = 0;
+	stat_ctl.s.mode = gbl_cfg->stat_mode;
+	cvmx_write_csr_node(node, CVMX_PKI_STAT_CTL, stat_ctl.u64);
+
 	cvmx_pki_write_global_parse(node, gbl_cfg->gbl_pen);
 	cvmx_pki_write_tag_secret(node, gbl_cfg->tag_secret);
 	cvmx_pki_write_frame_len(node, 0, gbl_cfg->frm_len[0]);
@@ -209,22 +257,19 @@ int cvmx_pki_set_pkind_config(int node, int pkind, struct cvmx_pki_pkind_config
 			pkind_cfg_style.s.pm = pkind_cfg->initial_parse_mode;
 			pkind_cfg_style.s.style = pkind_cfg->initial_style;
 			cvmx_write_csr_node(node, CVMX_PKI_CLX_PKINDX_STYLE(pkind, cluster), pkind_cfg_style.u64);
+			pknd_cfg_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_CFG(pkind, cluster));
+			pknd_cfg_reg.s.fcs_pres = pkind_cfg->fcs_pres;
+			pknd_cfg_reg.s.inst_hdr = pkind_cfg->parse_en.inst_hdr;
+			pknd_cfg_reg.s.mpls_en = pkind_cfg->parse_en.mpls_en;
+			pknd_cfg_reg.s.lg_custom = pkind_cfg->parse_en.lg_custom;
+			pknd_cfg_reg.s.fulc_en = pkind_cfg->parse_en.fulc_en;
+			pknd_cfg_reg.s.dsa_en = pkind_cfg->parse_en.dsa_en;
+			pknd_cfg_reg.s.hg2_en = pkind_cfg->parse_en.hg2_en;
+			pknd_cfg_reg.s.hg_en = pkind_cfg->parse_en.hg_en;
+			cvmx_write_csr_node(node, CVMX_PKI_CLX_PKINDX_CFG(pkind, cluster), pknd_cfg_reg.u64);
 		}
 		cluster++;
 	}
-
-	cluster = 0;	/* XXX: LEO: prior loop sets it out of range ! */
-	pknd_cfg_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_CFG(pkind, cluster));
-	pknd_cfg_reg.s.fcs_pres = pkind_cfg->fcs_pres;
-	pknd_cfg_reg.s.inst_hdr = pkind_cfg->parse_en.inst_hdr;
-	pknd_cfg_reg.s.mpls_en = pkind_cfg->parse_en.mpls_en;
-	pknd_cfg_reg.s.lg_custom = pkind_cfg->parse_en.lg_custom;
-	pknd_cfg_reg.s.fulc_en = pkind_cfg->parse_en.fulc_en;
-	pknd_cfg_reg.s.dsa_en = pkind_cfg->parse_en.dsa_en;
-	pknd_cfg_reg.s.hg2_en = pkind_cfg->parse_en.hg2_en;
-	pknd_cfg_reg.s.hg_en = pkind_cfg->parse_en.hg_en;
-	cvmx_write_csr_node(node, CVMX_PKI_CLX_PKINDX_CFG(pkind, cluster), pknd_cfg_reg.u64);
-
 	return 0;
 }
 
@@ -402,7 +447,7 @@ void cvmx_pki_get_style_config(int node, int style, uint64_t cluster_mask,
 	style_cfg_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(style, cluster));
 	style_cfg2_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG2(style, cluster));
 	style_alg_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_ALG(style, cluster));
-	style_buf_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(style, cluster));
+        style_buf_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_STYLEX_BUF(style));
 
 	style_cfg->parm_cfg.ip6_udp_opt = style_cfg_reg.s.ip6_udp_opt;
 	style_cfg->parm_cfg.lenerr_en = style_cfg_reg.s.lenerr_en;
@@ -709,7 +754,7 @@ void cvmx_pki_endis_fcs_check(int node, int pknd, bool fcs_chk, bool fcs_strip)
 	cvmx_pki_clx_stylex_cfg_t style_cfg;
 
 	/*vinita_to_do; find the cluster in use*/
-	pkind_style.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_CFG(pknd, cluster));
+	pkind_style.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_STYLE(pknd, cluster));
 	style = pkind_style.s.style;
 	style_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(style, cluster));
 	style_cfg.s.fcs_chk = fcs_chk;
@@ -736,7 +781,7 @@ void cvmx_pki_endis_l2_errs(int node, int pknd, bool l2len_err,
 	cvmx_pki_clx_stylex_cfg_t style_cfg;
 
 	/*vinita_to_do; find the cluster in use*/
-	pkind_style.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_CFG(pknd, cluster));
+	pkind_style.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_STYLE(pknd, cluster));
 	style = pkind_style.s.style;
 	style_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(style, cluster));
 	style_cfg.s.lenerr_en = l2len_err;
@@ -758,7 +803,7 @@ void cvmx_pki_dis_frame_len_chk(int node, int pknd)
 	cvmx_pki_clx_stylex_cfg_t style_cfg;
 
 	/*vinita_to_do; find the cluster in use*/
-	pkind_style.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_CFG(pknd, cluster));
+	pkind_style.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_STYLE(pknd, cluster));
 	style = pkind_style.s.style;
 	style_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(style, cluster));
 	style_cfg.s.maxerr_en = 0;
@@ -914,6 +959,7 @@ void cvmx_pki_show_pkind_attributes(int node, int pkind)
 		cvmx_dprintf("ERROR: PKIND %d is beyond range\n", pkind);
 		return;
 	}
+        cvmx_dprintf("Showing stats for pkind %d------------------\n", pkind);
 	pkind_clsel.u64 = cvmx_read_csr_node(node, CVMX_PKI_PKINDX_ICGSEL(pkind));
 	cvmx_dprintf("cluster group:	%d\n", pkind_clsel.s.icg);
 	pki_cl_grp.u64 = cvmx_read_csr_node(node, CVMX_PKI_ICGX_CFG(pkind_clsel.s.icg));
@@ -921,6 +967,7 @@ void cvmx_pki_show_pkind_attributes(int node, int pkind)
 
 	while (cluster < CVMX_PKI_NUM_CLUSTER) {
 		if (pki_cl_grp.s.clusters & (0x01L << cluster)) {
+                         cvmx_dprintf("pkind %d config 0x%llx\n", pkind, (unsigned long long)cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_CFG(pkind, cluster)));
 			/*vinita_to_do later modify in human readble format or now just print register value*/
 			pkind_cfg_style.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_STYLE(pkind, cluster));
 			cvmx_dprintf("initial parse Mode: %d\n", pkind_cfg_style.s.pm);
@@ -944,5 +991,11 @@ void cvmx_pki_show_pkind_attributes(int node, int pkind)
 	}
 }
 
-
+void cvmx_pki_show_port(int node, int interface, int index)
+{
+        int pknd = cvmx_helper_get_pknd(interface, index);
+        cvmx_dprintf("Showing stats for intf 0x%x port %d------------------\n", interface, index);
+        cvmx_pki_show_pkind_attributes(node, pknd);
+        cvmx_dprintf("END STAUS------------------------\n\n");
+}
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pko3.c b/arch/mips/cavium-octeon/executive/cvmx-pko3.c
index a10b863..e883729 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pko3.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pko3.c
@@ -1153,7 +1153,7 @@ int cvmx_pko3_pdesc_buf_append(cvmx_pko3_pdesc_t *pdesc, void *p_data,
 	
 	hdr_s = (void *) &pdesc->word[0];
 
-	if(pdesc->last_aura == -1) {
+	if(pdesc->last_aura == -1 && gaura != ((unsigned)-1)) {
 		unsigned buf_sz = 128;
 
 		/* First mbuf, calculate headroom */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-qlm.c b/arch/mips/cavium-octeon/executive/cvmx-qlm.c
index bfd2a66..e1a8ad4 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-qlm.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-qlm.c
@@ -42,7 +42,7 @@
  *
  * Helper utilities for qlm.
  *
- * <hr>$Revision: 96753 $<hr>
+ * <hr>$Revision: 97444 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
@@ -77,6 +77,25 @@
 DECLARE_GLOBAL_DATA_PTR;
 #endif
 
+/* Their is a copy of this in bootloader qlm configuration, make sure
+   to update both the places till i figure out */
+#define R_25G_REFCLK100             0x0
+#define R_5G_REFCLK100              0x1
+#define R_8G_REFCLK100              0x2
+#define R_125G_REFCLK15625_KX       0x3
+#define R_3125G_REFCLK15625_XAUI    0x4
+#define R_103215G_REFCLK15625_KR    0x5
+#define R_125G_REFCLK15625_SGMII    0x6
+#define R_5G_REFCLK15625_QSGMII     0x7
+#define R_625G_REFCLK15625_RXAUI    0x8
+#define R_25G_REFCLK125             0x9
+#define R_5G_REFCLK125              0xa
+#define R_8G_REFCLK125              0xb
+
+static const int REF_100MHZ = 100000000;
+static const int REF_125MHZ = 125000000;
+static const int REF_156MHZ = 156250000;
+
 /**
  * The JTAG chain for CN52XX and CN56XX is 4 * 268 bits long, or 1072.
  * CN5XXX full chain shift is:
@@ -756,37 +775,91 @@ int cvmx_qlm_get_gbaud_mhz(int qlm)
 		return freq;
 	} else if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
 		cvmx_gserx_lane_mode_t lane_mode;
+		cvmx_gserx_cfg_t cfg;
 		if (qlm < 8)
 			return -1;	/* FIXME for OCI */
-		lane_mode.u64 = cvmx_read_csr(CVMX_GSERX_LANE_MODE(qlm));
-		switch (lane_mode.s.lmode)
-		{
-		case 0x0: /* R_25G_REFCLK100 */
-			return 2500;
-		case 0x1: /* R_5G_REFCLK100 */
-			return 5000;
-		case 0x2: /* R_8G_REFCLK100 */
-			return 8000;
-		case 0x3: /* R_125G_REFCLK15625_KX */
-			return 1250;
-		case 0x4: /* R_3125G_REFCLK15625_XAUI */
-			return 3125;
-		case 0x5: /* R_103215G_REFCLK15625_KR */
-			return 10321;
-		case 0x6: /* R_125G_REFCLK15625_SGMII */
-			return 1250;
-		case 0x7: /* R_5G_REFCLK15625_QSGMII */
-			return 5000;
-		case 0x8: /* R_625G_REFCLK15625_RXAUI */
-			return 6250;
-		case 0x9: /* R_25G_REFCLK125 */
-			return 2500;
-		case 0xa: /* R_5G_REFCLK125 */
-			return 5000;
-		case 0xb: /* R_8G_REFCLK125 */
-			return 8000;
-		default:
-			return 0;
+		/* Check if QLM is configured */
+		cfg.u64 = cvmx_read_csr(CVMX_GSERX_CFG(qlm));
+		if (cfg.u64 == 0)
+			return -1;
+		if (cfg.s.pcie) {
+			int pem = 0;
+			cvmx_pemx_cfg_t pemx_cfg;
+			switch(qlm) {
+			case 0: /* Either PEM0 x4 of PEM0 x8 */
+				pem = 0;
+				break;
+			case 1: /* Either PEM0 x4 of PEM1 x4 */
+				pemx_cfg.u64 = cvmx_read_csr(CVMX_PEMX_CFG(0));
+				if (pemx_cfg.cn78xx.lanes8)
+					pem = 0;
+				else
+					pem = 1;
+				break;
+			case 2: /* Either PEM2 x4 of PEM2 x8 */
+				pem = 2;
+				break;
+			case 3: /* Either PEM2 x8 of PEM3 x4 or x8 */
+				/* Can be last 4 lanes of PEM2 */
+				pemx_cfg.u64 = cvmx_read_csr(CVMX_PEMX_CFG(2));
+				if (pemx_cfg.cn78xx.lanes8)
+					pem = 2;
+				else {
+					pemx_cfg.u64 = cvmx_read_csr(CVMX_PEMX_CFG(3));
+					if (pemx_cfg.cn78xx.lanes8)
+						pem = 3;
+					else
+						pem = 2;
+				}
+				break;
+			case 4: /* Either PEM3 x8 of PEM3 x4 */
+				pem = 3;
+				break;
+			default:
+				cvmx_dprintf("QLM%d: Should be in PCIe mode\n", qlm);
+				break;
+			}
+			pemx_cfg.u64 = cvmx_read_csr(CVMX_PEMX_CFG(pem));
+			switch(pemx_cfg.s.md) {
+				case 0: /* Gen1 */
+					return 2500;
+				case 1: /* Gen2 */
+					return 5000;
+				case 2: /* Gen3 */
+					return 8000;
+				default:
+					return 0;
+			}
+		} else {
+			lane_mode.u64 = cvmx_read_csr(CVMX_GSERX_LANE_MODE(qlm));
+			switch (lane_mode.s.lmode) {
+			case R_25G_REFCLK100:
+				return 2500;
+			case R_5G_REFCLK100:
+				return 5000;
+			case R_8G_REFCLK100:
+				return 8000;
+			case R_125G_REFCLK15625_KX:
+				return 1250;
+			case R_3125G_REFCLK15625_XAUI:
+				return 3125;
+			case R_103215G_REFCLK15625_KR:
+				return 10321;
+			case R_125G_REFCLK15625_SGMII:
+				return 1250;
+			case R_5G_REFCLK15625_QSGMII:
+				return 5000;
+			case R_625G_REFCLK15625_RXAUI:
+				return 6250;
+			case R_25G_REFCLK125:
+				return 2500;
+			case R_5G_REFCLK125:
+				return 5000;
+			case R_8G_REFCLK125:
+				return 8000;
+			default:
+				return 0;
+			}
 		}
 	}
 	return 0;
@@ -1246,6 +1319,56 @@ enum cvmx_qlm_mode cvmx_qlm_get_mode(int qlm)
 	return CVMX_QLM_MODE_DISABLED;
 }
 
+int cvmx_qlm_measure_clock_cn78xx(int qlm)
+{
+	cvmx_gserx_cfg_t cfg;
+	cvmx_gserx_refclk_sel_t refclk_sel;
+	cvmx_gserx_lane_mode_t lane_mode;
+
+	if (qlm < 8)
+		return -1; /* FIXME for OCI */
+
+	cfg.u64 = cvmx_read_csr(CVMX_GSERX_CFG(qlm));
+
+	if (cfg.s.pcie) {
+		refclk_sel.u64 = cvmx_read_csr(CVMX_GSERX_REFCLK_SEL(qlm));
+		if (refclk_sel.s.pcie_refclk125)
+			return REF_125MHZ; /* Ref 125 Mhz */
+		else
+			return REF_100MHZ; /* Ref 100Mhz */
+	}
+
+	lane_mode.u64 = cvmx_read_csr(CVMX_GSERX_LANE_MODE(qlm));
+	switch(lane_mode.s.lmode) {
+	case R_25G_REFCLK100:
+		return REF_100MHZ;
+	case R_5G_REFCLK100:
+		return REF_100MHZ;
+	case R_8G_REFCLK100:
+		return REF_100MHZ;
+	case R_125G_REFCLK15625_KX:
+		return REF_156MHZ;
+	case R_3125G_REFCLK15625_XAUI:
+		return REF_156MHZ;
+	case R_103215G_REFCLK15625_KR:
+		return REF_156MHZ;
+	case R_125G_REFCLK15625_SGMII:
+		return REF_156MHZ;
+	case R_5G_REFCLK15625_QSGMII:
+		return REF_156MHZ;
+	case R_625G_REFCLK15625_RXAUI:
+		return REF_156MHZ;
+	case R_25G_REFCLK125:
+		return REF_125MHZ;
+	case R_5G_REFCLK125:
+		return REF_125MHZ;
+	case R_8G_REFCLK125:
+		return REF_125MHZ;
+	default:
+		return 0;
+	}
+}
+
 /**
  * Measure the reference clock of a QLM
  *
@@ -1272,7 +1395,7 @@ int cvmx_qlm_measure_clock(int qlm)
 		return -1;
 
 	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
-		evcnt_offset = 0x20;
+		return cvmx_qlm_measure_clock_cn78xx(qlm);
 
 	/* Force the reference to 156.25Mhz when running in simulation.
 	   This supports the most speeds */
diff --git a/arch/mips/cavium-octeon/executive/octeon-model.c b/arch/mips/cavium-octeon/executive/octeon-model.c
index d5f7f94..3d9a6cb 100644
--- a/arch/mips/cavium-octeon/executive/octeon-model.c
+++ b/arch/mips/cavium-octeon/executive/octeon-model.c
@@ -43,7 +43,7 @@
  * File defining functions for working with different Octeon
  * models.
  *
- * <hr>$Revision: 95860 $<hr>
+ * <hr>$Revision: 97490 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/octeon.h>
@@ -147,7 +147,7 @@ const char *octeon_model_get_string_buffer(uint32_t chip_id, char *buffer)
 		fus3.u64 = cvmx_read_csr(CVMX_L2D_FUS3);
 	fus_dat2.u64 = cvmx_read_csr(CVMX_MIO_FUS_DAT2);
 	fus_dat3.u64 = cvmx_read_csr(CVMX_MIO_FUS_DAT3);
-	num_cores = cvmx_pop(cvmx_read_csr(CVMX_CIU_FUSE));
+	num_cores = cvmx_octeon_num_cores();
 
 	/* Make sure the non existent devices look disabled */
 	switch ((chip_id >> 8) & 0xff) {
@@ -185,6 +185,9 @@ const char *octeon_model_get_string_buffer(uint32_t chip_id, char *buffer)
 	/* Use the number of cores to determine the last 2 digits of the model
 	   number. There are some exceptions that are fixed later */
 	switch (num_cores) {
+	case 48:
+		core_model = "90";
+		break;
 	case 32:
 		core_model = "80";
 		break;
diff --git a/arch/mips/include/asm/octeon/cvmx-app-init.h b/arch/mips/include/asm/octeon/cvmx-app-init.h
index b4fe073..f108d74 100644
--- a/arch/mips/include/asm/octeon/cvmx-app-init.h
+++ b/arch/mips/include/asm/octeon/cvmx-app-init.h
@@ -41,7 +41,7 @@
  * @file
  * Header file for simple executive application initialization.  This defines
  * part of the ABI between the bootloader and the application.
- * <hr>$Revision: 96616 $<hr>
+ * <hr>$Revision: 97057 $<hr>
  *
  */
 
@@ -277,6 +277,7 @@ enum cvmx_board_types_enum {
 	CVMX_BOARD_TYPE_NAS7000_REF = 58,
 	CVMX_BOARD_TYPE_EAP7000_REF = 59,
 	CVMX_BOARD_TYPE_ROUTER7000_REF = 60,
+	CVMX_BOARD_TYPE_EBB7800 = 61,
 	CVMX_BOARD_TYPE_MAX,
 	/* NOTE:  256-257 are being used by a customer. */
 
@@ -406,6 +407,7 @@ static inline const char *cvmx_board_type_to_string(enum cvmx_board_types_enum t
 		ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_NAS7000_REF)
 		ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_EAP7000_REF)
 		ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_ROUTER7000_REF)
+		ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_EBB7800)
 		ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_MAX)
 
 		/* Customer boards listed here */
diff --git a/arch/mips/include/asm/octeon/cvmx-clock.h b/arch/mips/include/asm/octeon/cvmx-clock.h
index e3033dd..dd4e398 100644
--- a/arch/mips/include/asm/octeon/cvmx-clock.h
+++ b/arch/mips/include/asm/octeon/cvmx-clock.h
@@ -51,6 +51,7 @@
 #include <asm/octeon/octeon.h>
 #include <asm/octeon/cvmx-lmcx-defs.h>
 #include <asm/octeon/cvmx-fpa-defs.h>
+#include <asm/octeon/cvmx-tim-defs.h>
 #else
 #include "cvmx.h"
 #endif
@@ -112,11 +113,14 @@ static inline uint64_t cvmx_clock_get_count(cvmx_clock_t clock)
 #endif
 		}
 	case CVMX_CLOCK_SCLK:
-	case CVMX_CLOCK_TIM:
 	case CVMX_CLOCK_IPD:
 		if (OCTEON_IS_MODEL(OCTEON_CN78XX))
 			return cvmx_read_csr(CVMX_FPA_CLK_COUNT);
 		return cvmx_read_csr(CVMX_IPD_CLK_COUNT);
+	case CVMX_CLOCK_TIM:
+		if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+			return cvmx_read_csr(CVMX_TIM_FR_RN_CYCLES);
+		return cvmx_read_csr(CVMX_IPD_CLK_COUNT);
 
 	case CVMX_CLOCK_DDR:
 		if (OCTEON_IS_OCTEON2() || OCTEON_IS_OCTEON3())
diff --git a/arch/mips/include/asm/octeon/cvmx-cmd-queue.h b/arch/mips/include/asm/octeon/cvmx-cmd-queue.h
index 8e8083f0..1b14997 100644
--- a/arch/mips/include/asm/octeon/cvmx-cmd-queue.h
+++ b/arch/mips/include/asm/octeon/cvmx-cmd-queue.h
@@ -82,7 +82,7 @@
  * internal cycle counter to completely eliminate any causes of
  * bus traffic.
  *
- * <hr> $Revision: 95726 $ <hr>
+ * <hr> $Revision: 97196 $ <hr>
  */
 
 #ifndef __CVMX_CMD_QUEUE_H__
@@ -259,6 +259,7 @@ static inline int __cvmx_cmd_queue_get_index(cvmx_cmd_queue_id_t queue_id)
 static inline void __cvmx_cmd_queue_lock(cvmx_cmd_queue_id_t queue_id,
 					 __cvmx_cmd_queue_state_t * qptr)
 {
+#ifndef __U_BOOT__
 	extern CVMX_SHARED __cvmx_cmd_queue_all_state_t *__cvmx_cmd_queue_state_ptr;
 	int tmp;
 	int my_ticket;
@@ -291,6 +292,7 @@ static inline void __cvmx_cmd_queue_lock(cvmx_cmd_queue_id_t queue_id,
 		      : [ticket_ptr] "=m"(__cvmx_cmd_queue_state_ptr->ticket[__cvmx_cmd_queue_get_index(queue_id)]),
 		      [now_serving] "=m"(qptr->now_serving),[ticket] "=&r"(tmp),[my_ticket] "=&r"(my_ticket)
 	    );
+#endif
 }
 
 /**
@@ -301,12 +303,14 @@ static inline void __cvmx_cmd_queue_lock(cvmx_cmd_queue_id_t queue_id,
  */
 static inline void __cvmx_cmd_queue_unlock(__cvmx_cmd_queue_state_t * qptr)
 {
+#ifndef __U_BOOT__
 	uint32_t ns;
 
 	ns = qptr->now_serving + 1;
 	CVMX_SYNCWS;		/* Order queue manipulation with respect to the unlock.  */
 	qptr->now_serving = ns;
 	CVMX_SYNCWS;		/* nudge out the unlock. */
+#endif
 }
 
 /**
diff --git a/arch/mips/include/asm/octeon/cvmx-coremask.h b/arch/mips/include/asm/octeon/cvmx-coremask.h
index 578f7ae..aff91ea 100644
--- a/arch/mips/include/asm/octeon/cvmx-coremask.h
+++ b/arch/mips/include/asm/octeon/cvmx-coremask.h
@@ -60,7 +60,7 @@
  * provide future compatibility if more cores are added to future processors
  * or more nodes are supported.
  *
- * <hr>$Revision: 92640 $<hr>
+ * <hr>$Revision: 97423 $<hr>
  *
  */
 
@@ -439,7 +439,8 @@ CVMX_COREMASK_UNARY_DEFUN(dup, +)   /* cvmx_coremask_dup(pcm1, pcm2): pcm1 = pcm
  * - set all (valid) bits in *pcm to 1
  */
 #define cvmx_coremask_complement(pcm)	cvmx_coremask_not(pcm, pcm)
-#define cvmx_coremask_clear_all(pcm)	cvmx_coremask_clear(pcm, NULL)
+/* On clear, even clear the unused bits */
+#define cvmx_coremask_clear_all(pcm)	do {*(pcm) = (cvmx_coremask_t)CVMX_COREMASK_EMPTY;} while (0)
 #define cvmx_coremask_set_all(pcm)	cvmx_coremask_fill(pcm, NULL)
 
 /*
diff --git a/arch/mips/include/asm/octeon/cvmx-fpa-defs.h b/arch/mips/include/asm/octeon/cvmx-fpa-defs.h
index 2f0e683..7f77e36 100644
--- a/arch/mips/include/asm/octeon/cvmx-fpa-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-fpa-defs.h
@@ -755,7 +755,7 @@ static inline uint64_t CVMX_FPA_WQE_THRESHOLD_FUNC(void)
 /**
  * cvmx_fpa_addr_range_error
  *
- * When any FPA_POOL(0..63)_INT[RANGE] error occurs, this register is latched with additional
+ * When any FPA_POOL()_INT[RANGE] error occurs, this register is latched with additional
  * error information.
  */
 union cvmx_fpa_addr_range_error {
@@ -803,7 +803,7 @@ typedef union cvmx_fpa_addr_range_error cvmx_fpa_addr_range_error_t;
 /**
  * cvmx_fpa_aura#_cfg
  *
- * This register configures aura backpressure, etc; see Aura Limits.
+ * This register configures aura backpressure, etc.
  *
  */
 union cvmx_fpa_aurax_cfg {
@@ -818,14 +818,14 @@ union cvmx_fpa_aurax_cfg {
                                                          later returns, this may result in the count intermittently being higher than the number of
                                                          buffers actually in use by packets visible to software.
                                                          1 = Pointer allocations/returns will not automatically change the count.
-                                                         Note specific requests to change the count, including FPA_AURA(0..1023)_CNT_ADD,
-                                                         PKO_SEND_AURA_S, or PKI_AURA(0..1023)_CFG[PKT_ADD] will be applied regardless of the
+                                                         Note specific requests to change the count, including FPA_AURA()_CNT_ADD,
+                                                         PKO_SEND_AURA_S, or PKI_AURA()_CFG[PKT_ADD] will be applied regardless of the
                                                          setting of this bit. */
 	uint64_t avg_con                      : 9;  /**< This value controls how much of each present average resource level is used to calculate
                                                          the new resource level. The value is a number from 0 to 256, which represents AVG_CON/256
                                                          of the average resource level that will be used in the calculation:
-                                                         next_LEVEL = (AVG_CON/256) * prev_LEVEL
-                                                         + (1-(AVG_CON/256)) * count
+                                                         _  next_LEVEL = (AVG_CON/256) * prev_LEVEL
+                                                         _  + (1-(AVG_CON/256)) * count
                                                          Note setting this value to zero will disable averaging, and always use the most immediate
                                                          levels. FPA_GEN_CFG[AVG_EN] must be set and FPA_GEN_CFG[LVL_DLY] must be nonzero to
                                                          globally enable averaging. FPA_RED_DELAY[AVG_DLY] controls the periodicity of the level
@@ -848,7 +848,7 @@ union cvmx_fpa_aurax_cnt {
 	struct cvmx_fpa_aurax_cnt_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_40_63               : 24;
-	uint64_t cnt                          : 40; /**< The current aura count. See Aura Counts. */
+	uint64_t cnt                          : 40; /**< The current aura count. */
 #else
 	uint64_t cnt                          : 40;
 	uint64_t reserved_40_63               : 24;
@@ -866,10 +866,13 @@ union cvmx_fpa_aurax_cnt_add {
 	struct cvmx_fpa_aurax_cnt_add_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_40_63               : 24;
-	uint64_t cnt                          : 40; /**< The value to be added to FPA_AURA(0..1023)_CNT. The value may alternatively be a 2's
+	uint64_t cnt                          : 40; /**< The value to be added to FPA_AURA()_CNT. The value may alternatively be a 2's
                                                          complement of a value to be subtracted. Subtraction or addition that results in overflow
                                                          will zero the count, not roll-around, and set either FPA_ERR_INT[CNT_ADD] or
-                                                         FPA_ERR_INT[CNT_SUB]. */
+                                                         FPA_ERR_INT[CNT_SUB].
+                                                         This register is intended for use when FPA_AURA()_CFG[PTR_DIS] is set.  If
+                                                         FPA_AURA()_CFG[PTR_DIS] is clear, this register would typically only be used if buffers
+                                                         are being re-provisioned. */
 #else
 	uint64_t cnt                          : 40;
 	uint64_t reserved_40_63               : 24;
@@ -888,16 +891,16 @@ union cvmx_fpa_aurax_cnt_levels {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_40_63               : 24;
 	uint64_t bp_ena                       : 1;  /**< Enable backpressure based on [BP] level. If set FPA_GEN_CFG[LVL_DLY] must be nonzero.
-                                                         PKI_AURA(0..1023)_CFG[ENA_BP] must also be set for backpressure to propagate through PKI. */
+                                                         PKI_AURA()_CFG[ENA_BP] must also be set for backpressure to propagate through PKI. */
 	uint64_t red_ena                      : 1;  /**< Enable RED based on [DROP] and [PASS] levels. If set FPA_GEN_CFG[LVL_DLY] must be nonzero.
                                                          If set, RED is performed on core requests with FPA_ALLOC_LD_S[RED] set, and/or PKI
-                                                         requests if PKI_AURA(0..1023)_CFG[ENA_RED] is set. */
-	uint64_t shift                        : 6;  /**< Right shift to apply to FPA_AURA(0..1023)_CNT to result in a 8-bit relative depth to be
-                                                         used for [DROP/PASS/LEVEL]. See Aura Counts. */
+                                                         requests if PKI_AURA()_CFG[ENA_RED] is set. */
+	uint64_t shift                        : 6;  /**< Right shift to apply to FPA_AURA()_CNT to result in a 8-bit relative depth to be
+                                                         used for [DROP/PASS/LEVEL]. */
 	uint64_t bp                           : 8;  /**< Backpressure will be applied if the immediate shifted level is equal to or greater than this value. */
 	uint64_t drop                         : 8;  /**< Packet will be dropped if the average shifted level is equal to or greater than this value. */
 	uint64_t pass                         : 8;  /**< Packet will be passed if the average shifted level is less than this value. */
-	uint64_t level                        : 8;  /**< Current shifted level, averaged with FPA_AURA(0..1023)_CNT.
+	uint64_t level                        : 8;  /**< Current shifted level, averaged with FPA_AURA()_CNT.
                                                          CNT levels track usage; the lower the level the more free resources. */
 #else
 	uint64_t level                        : 8;
@@ -922,7 +925,7 @@ union cvmx_fpa_aurax_cnt_limit {
 	struct cvmx_fpa_aurax_cnt_limit_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_40_63               : 24;
-	uint64_t limit                        : 40; /**< When FPA_AURA(0..1023)_CNT is equal to or greater than this value, any allocations using
+	uint64_t limit                        : 40; /**< When FPA_AURA()_CNT is equal to or greater than this value, any allocations using
                                                          this aura will fail. This allows a hard resource division between multiple auras sharing a
                                                          common pool. */
 #else
@@ -942,10 +945,9 @@ union cvmx_fpa_aurax_cnt_threshold {
 	struct cvmx_fpa_aurax_cnt_threshold_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_40_63               : 24;
-	uint64_t thresh                       : 40; /**< When FPA_AURA(0..1023)_CNT, after being modified, is equal to or crosses this value (i.e.
+	uint64_t thresh                       : 40; /**< When FPA_AURA()_CNT, after being modified, is equal to or crosses this value (i.e.
                                                          value was greater than, then becomes less than, or the value was less than and becomes
-                                                         greater than) the corresponding bit in FPA_AURA(0..1023)_INT is set. See Aura Count
-                                                         Threshold Interrupts. */
+                                                         greater than) the corresponding bit in FPA_AURA()_INT is set. */
 #else
 	uint64_t thresh                       : 40;
 	uint64_t reserved_40_63               : 24;
@@ -963,10 +965,10 @@ union cvmx_fpa_aurax_int {
 	struct cvmx_fpa_aurax_int_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_1_63                : 63;
-	uint64_t thresh                       : 1;  /**< Watermark interrupt pending. Set and throws FPA_INTSN_E::FPA_AURA(0..1024)_THRESH when
-                                                         FPA_AURA(0..1023)_INT, after being modified, is equal to or crosses
-                                                         FPA_AURA(0..1023)_CNT_THRESHOLD (i.e. value was greater than, then becomes less then, or
-                                                         value was less than, and becomes greater than). See Aura Count Threshold Interrupts. */
+	uint64_t thresh                       : 1;  /**< Watermark interrupt pending. Set and throws FPA_INTSN_E::FPA_AURA()_THRESH when
+                                                         FPA_AURA()_INT, after being modified, is equal to or crosses
+                                                         FPA_AURA()_CNT_THRESHOLD (i.e. value was greater than, then becomes less then, or
+                                                         value was less than, and becomes greater than). */
 #else
 	uint64_t thresh                       : 1;
 	uint64_t reserved_1_63                : 63;
@@ -1007,12 +1009,12 @@ union cvmx_fpa_aurax_pool_levels {
 	uint64_t reserved_40_63               : 24;
 	uint64_t bp_ena                       : 1;  /**< Enable backpressure based on [BP] level. If set FPA_GEN_CFG[LVL_DLY] must be nonzero. */
 	uint64_t red_ena                      : 1;  /**< Enable RED based on [DROP] and [PASS] levels. If set FPA_GEN_CFG[LVL_DLY] must be nonzero. */
-	uint64_t shift                        : 6;  /**< Right shift to apply to FPA_POOL(0..63)_AVAILABLE to result in a 8-bit relative depth to
-                                                         be used for [DROP/PASS/LEVEL]. See Aura Counts. */
+	uint64_t shift                        : 6;  /**< Right shift to apply to FPA_POOL()_AVAILABLE to result in a 8-bit relative depth to
+                                                         be used for [DROP/PASS/LEVEL]. */
 	uint64_t bp                           : 8;  /**< Backpressure will be indicated if the immediate shifted level is equal to or less than this value. */
 	uint64_t drop                         : 8;  /**< Packet will be dropped if the average shifted level is equal to or less than this value. */
 	uint64_t pass                         : 8;  /**< Packet will be passed if the average shifted level is larger than this value. */
-	uint64_t level                        : 8;  /**< Current shifted level, averaged with FPA_POOL(0..63)_AVAILABLE[AVG_CON].
+	uint64_t level                        : 8;  /**< Current shifted level, averaged with FPA_POOL()_AVAILABLE[AVG_CON].
                                                          FPA levels track availability; the higher the level the more free resources. */
 #else
 	uint64_t level                        : 8;
@@ -1250,10 +1252,10 @@ union cvmx_fpa_ecc_int {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_52_63               : 12;
 	uint64_t ram_dbe                      : 20; /**< Set when a double-bit error is detected in the corresponding RAM. Throws
-                                                         FPA_INTSN_E::FPA_ECC_RAM_DBE. */
+                                                         FPA_INTSN_E::FPA_ERR_RAM_DBE. */
 	uint64_t reserved_20_31               : 12;
 	uint64_t ram_sbe                      : 20; /**< Set when a single-bit error is detected in the corresponding RAM. Throws
-                                                         FPA_INTSN_E::FPA_ECC_RAM_SBE. */
+                                                         FPA_INTSN_E::FPA_ERR_RAM_SBE. */
 #else
 	uint64_t ram_sbe                      : 20;
 	uint64_t reserved_20_31               : 12;
@@ -1280,9 +1282,9 @@ union cvmx_fpa_err_int {
                                                          FPA_INTSN_E::FPA_ERR_HW_SUB. */
 	uint64_t hw_add                       : 1;  /**< Set when hardware does an add to the count that caused the counter to wrap. Throws
                                                          FPA_INTSN_E::FPA_ERR_HW_ADD. */
-	uint64_t cnt_sub                      : 1;  /**< Set when a write to FPA_AURA(0..1023)_CNT_ADD does a subtract to the count that would have
+	uint64_t cnt_sub                      : 1;  /**< Set when a write to FPA_AURA()_CNT_ADD does a subtract to the count that would have
                                                          caused the counter to wrap, so the count was zeroed. Throws FPA_INTSN_E::FPA_ERR_CNT_SUB. */
-	uint64_t cnt_add                      : 1;  /**< Set when a write to FPA_AURA(0..1023)_CNT_ADD does an add to the count that would have
+	uint64_t cnt_add                      : 1;  /**< Set when a write to FPA_AURA()_CNT_ADD does an add to the count that would have
                                                          caused the counter to wrap, so the count was zeroed. Throws FPA_INTSN_E::FPA_ERR_CNT_ADD. */
 #else
 	uint64_t cnt_add                      : 1;
@@ -1577,18 +1579,18 @@ union cvmx_fpa_gen_cfg {
 	uint64_t halfrate                     : 1;  /**< Half rate. Limit peak alloc/free rate to half of peak to insure all alloc/frees are
                                                          visible to OCLA. */
 	uint64_t ocla_bp                      : 1;  /**< OCLA backpressure enable. When OCLA FIFOs are near full, allow OCLA to backpressure
-                                                         alloc/frees. See also HALFRATE. */
+                                                         alloc/frees. See also [HALFRATE]. */
 	uint64_t lvl_dly                      : 6;  /**< Levelizer delay. Number of cycles between level computations for backpressure and RED.
                                                          Increasing values decrease power and leave additional bandwidth for allocate/deallocates.
                                                          Zero disables, one indicates a level computation every other cycle, etc. Once set to
                                                          nonzero must not be later set to zero without resetting FPA. */
 	uint64_t pools                        : 2;  /**< Number of pools. Each halving of the number of pools doubles the buffering available to
                                                          the remaining pools, leading to some improvement in memory bandwidth. Value must not be
-                                                         changed if FPA_POOL(0..63)_CFG[ENA] is set for any pool.
+                                                         changed if FPA_POOL()_CFG[ENA] is set for any pool.
                                                          0x0 = 64 pools, 320 FPF entries per pool.
                                                          0x1 = 32 pools, 640 FPF entries per pool.
                                                          0x2 = 16 pools, 1280 FPF entries per pool.
-                                                         0x3 = Reserved */
+                                                         0x3 = Reserved. */
 	uint64_t avg_en                       : 1;  /**< QoS averaging enable. When set, compute average buffer levels, and [LVL_DLY] must be non-
                                                          zero. When clear, do not compute averages and save a few mW of power. */
 	uint64_t clk_override                 : 1;  /**< Conditional clock override. */
@@ -3138,7 +3140,7 @@ union cvmx_fpa_poolx_available {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_36_63               : 28;
 	uint64_t count                        : 36; /**< The number of free pages available in this pool. INTERNAL: Sized for 41 bit address - 7
-                                                         bit cache line */
+                                                         bit cache line. */
 #else
 	uint64_t count                        : 36;
 	uint64_t reserved_36_63               : 28;
@@ -3177,7 +3179,7 @@ union cvmx_fpa_poolx_cfg {
                                                          0 = use STF.
                                                          1 = use STT. */
 	uint64_t nat_align                    : 1;  /**< Returning buffers should be rounded to the nearest natural alignment specified with
-                                                         [BUF_SIZE]. See Buffer Alignment. */
+                                                         [BUF_SIZE]. */
 	uint64_t ena                          : 1;  /**< Enable. Must be set after writing pool configuration, if clear any allocations will fail
                                                          and returns will be dropped. If any pool configuration is changed after writing this bit,
                                                          the FPA may operate incorrectly. */
@@ -3255,8 +3257,8 @@ union cvmx_fpa_poolx_fpf_marks {
 	uint64_t fpf_rd                       : 11; /**< When the number of free-page pointers in a pool drops below this value and there are free-
                                                          page pointers in DRAM, the FPA reads one page of pointers from DRAM. The recommended value
                                                          for this field is:
-                                                         fpf_sz * 0.75
-                                                         where, fpf_sz = 320 * 2^FPA_GEN_CFG[POOLS].
+                                                         _  fpf_sz * 0.75
+                                                         _  where, fpf_sz = 320 * 2^FPA_GEN_CFG[POOLS].
                                                          The maximum value is fpf_sz - 48.
                                                          It is recommended that software APIs represent this value as a percentage of fpf_sz, as
                                                          fpf_sz may vary between products.
@@ -3286,16 +3288,16 @@ union cvmx_fpa_poolx_int {
 	struct cvmx_fpa_poolx_int_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_4_63                : 60;
-	uint64_t thresh                       : 1;  /**< Set and throws FPA_INTSN_E::FPA_POOL(0..63)_THRESH when FPA_POOL(0..63)_AVAILABLE is equal
-                                                         to FPA_POOL(0..63)_THRESHOLD and a pointer is allocated or deallocated. */
-	uint64_t range                        : 1;  /**< Set and throws FPA_INTSN_E::FPA_POOL(0..63)_RANGE when a pointer address does not fall in
-                                                         the address range for that pool specified by FPA_POOL(0..63)_START_ADDR and
-                                                         FPA_POOL(0..63)_END_ADDR. */
-	uint64_t crcerr                       : 1;  /**< Set and throws FPA_INTSN_E::FPA_POOL(0..63)_CRCERR when a page read from the DRAM contains
+	uint64_t thresh                       : 1;  /**< Set and throws FPA_INTSN_E::FPA_POOL()_THRESH when FPA_POOL()_AVAILABLE is equal
+                                                         to FPA_POOL()_THRESHOLD and a pointer is allocated or deallocated. */
+	uint64_t range                        : 1;  /**< Set and throws FPA_INTSN_E::FPA_POOL()_RANGE when a pointer address does not fall in
+                                                         the address range for that pool specified by FPA_POOL()_START_ADDR and
+                                                         FPA_POOL()_END_ADDR. */
+	uint64_t crcerr                       : 1;  /**< Set and throws FPA_INTSN_E::FPA_POOL()_CRCERR when a page read from the DRAM contains
                                                          inconsistent data (FPA ownership CRC does not match what FPA wrote). Most likely indicates
                                                          the stack has been fatally corrupted. */
-	uint64_t ovfls                        : 1;  /**< Set and throws FPA_INTSN_E::FPA_POOL(0..63)_OVFLS on stack overflow; when
-                                                         FPA_POOL(0..63)_STACK_ADDR would exceed FPA_POOL(0..63)_STACK_END. */
+	uint64_t ovfls                        : 1;  /**< Set and throws FPA_INTSN_E::FPA_POOL()_OVFLS on stack overflow; when
+                                                         FPA_POOL()_STACK_ADDR would exceed FPA_POOL()_STACK_END. */
 #else
 	uint64_t ovfls                        : 1;
 	uint64_t crcerr                       : 1;
@@ -3333,7 +3335,7 @@ union cvmx_fpa_poolx_stack_addr {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_42_63               : 22;
 	uint64_t addr                         : 35; /**< Next address. The address of the next stack write. Must be initialized to
-                                                         FPA_POOL(0..63)_STACK_BASE[ADDR] when stack is createdFPA_POOL(0..63)_STACK_BASE. */
+                                                         FPA_POOL()_STACK_BASE[ADDR] when stack is created. */
 	uint64_t reserved_0_6                 : 7;
 #else
 	uint64_t reserved_0_6                 : 7;
@@ -3374,7 +3376,7 @@ union cvmx_fpa_poolx_stack_end {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_42_63               : 22;
 	uint64_t addr                         : 35; /**< Stack ending address plus one line; hardware will never write this address. If
-                                                         FPA_POOL(0..63)_STACK_ADDR is equal to this value, the stack is full. */
+                                                         FPA_POOL()_STACK_ADDR is equal to this value, the stack is full. */
 	uint64_t reserved_0_6                 : 7;
 #else
 	uint64_t reserved_0_6                 : 7;
@@ -3727,7 +3729,7 @@ union cvmx_fpa_red_delay {
                                                          of all aura levels to track changes in the actual free space more slowly. Larger AVG_DLY
                                                          also causes the moving averages of all aura levels to track changes in the actual free
                                                          space more slowly, but does not affect backpressure. Larger
-                                                         FPA_AURA(0..1023)_CFG[AVG_CON]) values causes a specific aura to track more slowly, but
+                                                         FPA_AURA()_CFG[AVG_CON]) values causes a specific aura to track more slowly, but
                                                          only affects an individual aura level, rather than all. */
 #else
 	uint64_t avg_dly                      : 14;
diff --git a/arch/mips/include/asm/octeon/cvmx-fpa3.h b/arch/mips/include/asm/octeon/cvmx-fpa3.h
index c0fe6a0..c674cb8 100644
--- a/arch/mips/include/asm/octeon/cvmx-fpa3.h
+++ b/arch/mips/include/asm/octeon/cvmx-fpa3.h
@@ -250,7 +250,7 @@ static inline void __cvmx_fpa_aura_cfg(int node, int aura, int pool,
        aura_cfg.s.ptr_dis = ptr_dis;
         /* Configure CVMX_FPA_AURAX_CNT_LEVELS, CVMX_FPA_AURAX_POOL_LEVELS  */
        cvmx_write_csr_node(node, CVMX_FPA_AURAX_CFG(aura), aura_cfg.u64);
-       cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT(aura), buffers_cnt);
+       cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT_ADD(aura), buffers_cnt);
        cvmx_write_csr_node(node, CVMX_FPA_AURAX_POOL(aura), pool64);
 
        /* TODO : config back pressure, RED */
diff --git a/arch/mips/include/asm/octeon/cvmx-gserx-defs.h b/arch/mips/include/asm/octeon/cvmx-gserx-defs.h
index be60cd7..5b0a7c2 100644
--- a/arch/mips/include/asm/octeon/cvmx-gserx-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-gserx-defs.h
@@ -493,6 +493,28 @@ static inline uint64_t CVMX_GSERX_DLMX_TX_TERM_OFFSET(unsigned long offset, unsi
 #define CVMX_GSERX_DLMX_TX_TERM_OFFSET(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090003040ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
 #endif
 #if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_GLBL_TAD(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_GLBL_TAD(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090460400ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_GLBL_TAD(block_id) (CVMX_ADD_IO_SEG(0x0001180090460400ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_GLBL_TM_ADMON(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_GLBL_TM_ADMON(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090460408ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_GLBL_TM_ADMON(block_id) (CVMX_ADD_IO_SEG(0x0001180090460408ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
 static inline uint64_t CVMX_GSERX_IDDQ_MODE(unsigned long block_id)
 {
 	if (!(
@@ -1230,6 +1252,17 @@ static inline uint64_t CVMX_GSERX_QLM_STAT(unsigned long block_id)
 #define CVMX_GSERX_QLM_STAT(block_id) (CVMX_ADD_IO_SEG(0x00011800900000A0ull) + ((block_id) & 15) * 0x1000000ull)
 #endif
 #if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_RDET_TIME(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_RDET_TIME(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904E0008ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_RDET_TIME(block_id) (CVMX_ADD_IO_SEG(0x00011800904E0008ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
 static inline uint64_t CVMX_GSERX_REFCLK_SEL(unsigned long block_id)
 {
 	if (!(
@@ -1450,6 +1483,17 @@ static inline uint64_t CVMX_GSERX_SCRATCH(unsigned long block_id)
 #define CVMX_GSERX_SCRATCH(block_id) (CVMX_ADD_IO_SEG(0x0001180090000020ull) + ((block_id) & 15) * 0x1000000ull)
 #endif
 #if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_SLICE_CFG(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_SLICE_CFG(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090460060ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_SLICE_CFG(block_id) (CVMX_ADD_IO_SEG(0x0001180090460060ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
 static inline uint64_t CVMX_GSERX_SPD(unsigned long block_id)
 {
 	if (!(
@@ -2563,6 +2607,107 @@ union cvmx_gserx_dlmx_tx_term_offset {
 typedef union cvmx_gserx_dlmx_tx_term_offset cvmx_gserx_dlmx_tx_term_offset_t;
 
 /**
+ * cvmx_gser#_glbl_tad
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_glbl_tad {
+	uint64_t u64;
+	struct cvmx_gserx_glbl_tad_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t pcs_sds_tad_8_5              : 4;  /**< AMON Specific mode selection.
+                                                         Set GSER(0..13)_GLBL_TM_ADMON[AMON_ON].
+                                                         Decodes 0x0 - 0x4 require GSER(0..13)_GLBL_TM_ADMON[LSEL] set.
+                                                         Decodes 0x5 - 0x5 do not require GSER(0..13)_GLBL_TM_ADMON[LSEL] set.
+                                                         In both cases, the resulting signals can be observed on the AMON pin.
+                                                         0x0 = TX txdrv DAC 100ua sink current monitor.
+                                                         0x1 = TX vcnt precision dcc.
+                                                         0x2 = RX sdll topregout.
+                                                         0x3 = RX ldll vctrl_i.
+                                                         0x4 = RX RX term VCM voltage.
+                                                         0x5 = Global bandgap voltage.
+                                                         0x6 = Global CTAT voltage.
+                                                         0x7 = Global internal 100ua reference current.
+                                                         0x8 = Global external 100ua reference current.
+                                                         0x9 = Global Rterm calibration reference voltage.
+                                                         0xA = Global Rterm calibration comparator voltage.
+                                                         0xB = Global Force VCNT thru DAC.
+                                                         0xC = Global VDD voltage.
+                                                         0xD = Global VDDCLK voltage.
+                                                         0xE = Global PLL regulate VCO supply.
+                                                         0xF = Global VCTRL for VCO varactor control. */
+	uint64_t pcs_sds_tad_4_0              : 5;  /**< DMON Specific mode selection.
+                                                         Set GSER(0..13)_GLBL_TM_ADMON[DMON_ON].
+                                                         Decodes 0x0 - 0xe require GSER(0..13)_GLBL_TM_ADMON[LSEL] set.
+                                                         Decodes 0xf - 0x1f do not require GSER(0..13)_GLBL_TM_ADMON[LSEL] set.
+                                                         In both cases, the resulting signals can be observed on the DMON pin.
+                                                         0x00 = DFE Data Q.
+                                                         0x01 = DFE Edge I.
+                                                         0x02 = DFE CK Q.
+                                                         0x03 = DFE CK I.
+                                                         0x04 = TBD.
+                                                         0x05-0x7 = Reserved.
+                                                         0x08 = RX ld_rx[0].
+                                                         0x09 = RX rx_clk.
+                                                         0x0A = RX q_error_stg.
+                                                         0x0B = RX q_data_stg.
+                                                         0x0C-0x0E = Reserved.
+                                                         0x0F = Special case to observe supply in global. Sds_vdda and a internal regulated supply
+                                                         can be observed on DMON and DMONB
+                                                         respectively.  sds_vss can be observed on AMON. GSER(0..13)_GLBL_TM_ADMON[AMON_ON]
+                                                         must not be set.
+                                                         0x10: PLL_CLK 0 degree.
+                                                         0x11: Sds_tst_fb_clk.
+                                                         0x12: Buffered refclk.
+                                                         0x13: Div 8 of core clock (core_clk_out).
+                                                         0x14-0x1F: Reserved. */
+#else
+	uint64_t pcs_sds_tad_4_0              : 5;
+	uint64_t pcs_sds_tad_8_5              : 4;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_gserx_glbl_tad_s          cn78xx;
+};
+typedef union cvmx_gserx_glbl_tad cvmx_gserx_glbl_tad_t;
+
+/**
+ * cvmx_gser#_glbl_tm_admon
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_glbl_tm_admon {
+	uint64_t u64;
+	struct cvmx_gserx_glbl_tm_admon_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_8_63                : 56;
+	uint64_t amon_on                      : 1;  /**< When set, AMON test mode is enabled; see GSER()_GLBL_TAD. */
+	uint64_t dmon_on                      : 1;  /**< When set, DMON test mode is enabled; see GSER()_GLBL_TAD. */
+	uint64_t reserved_3_5                 : 3;
+	uint64_t lsel                         : 3;  /**< Three bits to select 1 out of 4 lanes for AMON/DMON test.
+                                                         0x0 = Selects lane 0.
+                                                         0x1 = Selects lane 1.
+                                                         0x2 = Selects lane 2.
+                                                         0x3 = Selects lane 3.
+                                                         0x4-0x7 = Reserved. */
+#else
+	uint64_t lsel                         : 3;
+	uint64_t reserved_3_5                 : 3;
+	uint64_t dmon_on                      : 1;
+	uint64_t amon_on                      : 1;
+	uint64_t reserved_8_63                : 56;
+#endif
+	} s;
+	struct cvmx_gserx_glbl_tm_admon_s     cn78xx;
+};
+typedef union cvmx_gserx_glbl_tm_admon cvmx_gserx_glbl_tm_admon_t;
+
+/**
  * cvmx_gser#_iddq_mode
  *
  * These registers are only reset by hardware during chip cold reset. The values of the CSR
@@ -3583,9 +3728,7 @@ union cvmx_gserx_lane_mode {
                                                          GSER_SPD and the appropriate table updates are performed so the rate is obtained for the
                                                          particular reference clock.
                                                          It is recommended that the PHY be in reset when reconfiguring the LMODE
-                                                         (GSER(0..13)_PHY_CTL[PHY_RESET] is set). If the LMODE is modified when the PHY is out of
-                                                         reset, GSER(0..13)_RXTX_STAT[LMC] can be used to determine when the PHY has
-                                                         transitioned to the new setting.
+                                                         (GSER(0..13)_PHY_CTL[PHY_RESET] is set).
                                                          Once the LMODE has been configured, and the PHY is out of reset, the table entries for the
                                                          selected LMODE must be updated to reflect the reference clock speed. Refer to the register
                                                          description and index into the table using the rate and reference speed to obtain the
@@ -4982,6 +5125,34 @@ union cvmx_gserx_qlm_stat {
 typedef union cvmx_gserx_qlm_stat cvmx_gserx_qlm_stat_t;
 
 /**
+ * cvmx_gser#_rdet_time
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_rdet_time {
+	uint64_t u64;
+	struct cvmx_gserx_rdet_time_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t rdet_time_3                  : 4;  /**< Determines the time allocated for disabling the RX detect
+                                                         circuit, and returning to common mode. */
+	uint64_t rdet_time_2                  : 4;  /**< Determines the time allocated for the RX detect circuit to
+                                                         detect a receiver. */
+	uint64_t rdet_time_1                  : 8;  /**< Determines the time allocated for enabling the RX detect circuit. */
+#else
+	uint64_t rdet_time_1                  : 8;
+	uint64_t rdet_time_2                  : 4;
+	uint64_t rdet_time_3                  : 4;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_gserx_rdet_time_s         cn78xx;
+};
+typedef union cvmx_gserx_rdet_time cvmx_gserx_rdet_time_t;
+
+/**
  * cvmx_gser#_refclk_sel
  *
  * This register selects the reference clock.
@@ -5530,6 +5701,34 @@ union cvmx_gserx_scratch {
 typedef union cvmx_gserx_scratch cvmx_gserx_scratch_t;
 
 /**
+ * cvmx_gser#_slice_cfg
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_slice_cfg {
+	uint64_t u64;
+	struct cvmx_gserx_slice_cfg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_12_63               : 52;
+	uint64_t tx_rx_detect_lvl_enc         : 4;  /**< Determines the RX Detect level, pcs_sds_tx_rx_detect_lvl[9:0],
+                                                         (which is a 1-hot signal), where the level is equal to to
+                                                         2^TX_RX_DETECT_LVL_ENC. */
+	uint64_t reserved_2_7                 : 6;
+	uint64_t pcs_sds_tx_stress_eye        : 2;  /**< Controls TX stress eye. */
+#else
+	uint64_t pcs_sds_tx_stress_eye        : 2;
+	uint64_t reserved_2_7                 : 6;
+	uint64_t tx_rx_detect_lvl_enc         : 4;
+	uint64_t reserved_12_63               : 52;
+#endif
+	} s;
+	struct cvmx_gserx_slice_cfg_s         cn78xx;
+};
+typedef union cvmx_gserx_slice_cfg cvmx_gserx_slice_cfg_t;
+
+/**
  * cvmx_gser#_spd
  *
  * These registers are only reset by hardware during chip cold reset. The values of the CSR
diff --git a/arch/mips/include/asm/octeon/cvmx-helper-board.h b/arch/mips/include/asm/octeon/cvmx-helper-board.h
index af37ec0..fedc548 100644
--- a/arch/mips/include/asm/octeon/cvmx-helper-board.h
+++ b/arch/mips/include/asm/octeon/cvmx-helper-board.h
@@ -43,7 +43,7 @@
  * Helper functions to abstract board specific data about
  * network ports from the rest of the cvmx-helper files.
  *
- * <hr>$Revision: 96046 $<hr>
+ * <hr>$Revision: 97664 $<hr>
  */
 #ifndef __CVMX_HELPER_BOARD_H__
 #define __CVMX_HELPER_BOARD_H__
@@ -313,6 +313,16 @@ cvmx_phy_host_mode_t cvmx_helper_board_get_phy_host_mode(int ipd_port);
  * NOTE: The phy_info data structure is subject to change.
  */
 int cvmx_helper_board_get_phy_info(cvmx_phy_info_t *phy_info, int ipd_port);
+
+/**
+ * @INTERNAL
+ * Parse the device tree and set whether a port is valid or not.
+ *
+ * @param fdt_addr	Pointer to device tree
+ *
+ * @return 0 for success, -1 on error.
+ */
+int __cvmx_helper_parse_78xx_bgx_dt(void *fdt_addr);
 #endif
 
 #ifdef	__cplusplus
diff --git a/arch/mips/include/asm/octeon/cvmx-helper-cfg.h b/arch/mips/include/asm/octeon/cvmx-helper-cfg.h
index c05e19b..62808d5 100644
--- a/arch/mips/include/asm/octeon/cvmx-helper-cfg.h
+++ b/arch/mips/include/asm/octeon/cvmx-helper-cfg.h
@@ -123,6 +123,10 @@ typedef enum cvmx_helper_cfg_option cvmx_helper_cfg_option_t;
  * Per physical port
  */
 struct cvmx_cfg_port_param {
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+	int port_fdt_node;		/** Node offset in FDT of node */
+	int phy_fdt_node;		/** Node offset in FDT of PHY */
+#endif
 	int8_t ccpp_pknd;
 	int8_t ccpp_bpid;
 	int8_t ccpp_pko_port_base;
@@ -512,6 +516,50 @@ extern uint8_t cvmx_helper_get_agl_rx_clock_skew(int interface, int index);
 extern void cvmx_helper_set_agl_rx_clock_skew(int interface, int index,
 					      uint8_t value);
 
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+/**
+ * @INTERNAL
+ * Store the FDT node offset in the device tree of a port
+ *
+ * @param xiface	node and interface
+ * @param index		port index
+ * @param node_offset	node offset to store
+ */
+extern void cvmx_helper_set_port_fdt_node_offset(int xiface, int index,
+						 int node_offset);
+
+/**
+ * @INTERNAL
+ * Return the FDT node offset in the device tree of a port
+ *
+ * @param xiface	node and interface
+ * @param index		port index
+ * @return		node offset of port or -1 if invalid
+ */
+extern int cvmx_helper_get_port_fdt_node_offset(int xiface, int index);
+
+/**
+ * @INTERNAL
+ * Store the FDT node offset in the device tree of a phy
+ *
+ * @param xiface	node and interface
+ * @param index		port index
+ * @param node_offset	node offset to store
+ */
+extern void cvmx_helper_set_phy_fdt_node_offset(int xiface, int index,
+						int node_offset);
+
+/**
+ * @INTERNAL
+ * Return the FDT node offset in the device tree of a phy
+ *
+ * @param xiface	node and interface
+ * @param index		port index
+ * @return		node offset of phy or -1 if invalid
+ */
+extern int cvmx_helper_get_phy_fdt_node_offset(int xiface, int index);
+#endif /* !CVMX_BUILD_FOR_LINUX_KERNEL */
+
 /*
  * Initializes cvmx with user specified config info.
  */
diff --git a/arch/mips/include/asm/octeon/cvmx-helper-pki.h b/arch/mips/include/asm/octeon/cvmx-helper-pki.h
index c039bfc..049328a 100644
--- a/arch/mips/include/asm/octeon/cvmx-helper-pki.h
+++ b/arch/mips/include/asm/octeon/cvmx-helper-pki.h
@@ -62,74 +62,78 @@ extern "C" {
 /* Modify this if more than 8 ilk channels need to be supported */
 #define CVMX_MAX_PORT_PER_INTERFACE	8
 #define CVMX_MAX_QOS_PRIORITY		64
+#define CVMX_PKI_FIND_AVAILABLE_RSRC    (-1)
 
 struct cvmx_pki_qos_schd {
 	bool pool_per_qos;	/* This qos priority will use its own pool, if FALSE use port pool */
-	int pool;		/* pool number to use, if -1 allocated by software */
+        int pool;		/* pool number to use, if -1 allocated by sdk otherwise software should alloc it */
 	char *pool_name;
 	uint64_t pool_buff_size;/* size of buffer in pool , if this priority is using its own pool*/
+                                /* it's good to have same buffer size if qos are using separate pools */
 	uint64_t pool_max_buff;	/* number of max buffers allowed in the pool, if this priority is using its own pool*/
 	bool aura_per_qos;	/* This qos priority will use its own aura, if FALSE use port aura */
-	int aura;		/* aura number to use, if -1 allocated by software */
+        int aura;		/* aura number to use, if -1 allocated by sdk otherwise software should alloc it */
 	char *aura_name;
 	uint64_t aura_buff_cnt;	/* number of buffers in aura, if this priority is using its own aura*/
 	bool sso_grp_per_qos;	/* This qos priority will use its own group, if FALSE use port group */
-	int sso_grp;		/* group number to use, if -1 allocated by software */
-	uint16_t port_add;       /* for BGX super MAC ports which wants to have PFC enabled */
-	unsigned qpg_base;
+        int sso_grp;		/* group number to use, if -1 allocated by sdk otherwise software should alloc it */
+	uint16_t port_add;      /* for BGX super MAC ports which wants to have PFC enabled */
+        int qpg_base;           /* offset in qpg table to use, if -1 allocated by sdk otherwise software should alloc it*/
 };
 
 struct cvmx_pki_prt_schd {
+        bool cfg_port;          /* Set to 1 if this port on the interface is not used */
 	int style;              /* If style_per_prt is TRUE in interface schd */
 	bool pool_per_prt; 	/* Port will use its own pool, if FALSE use interface pool */
-	int pool;		/* pool number to use, if -1 allocated by software */
+	int pool;		/* pool number to use, if -1 allocated by sdk otherwise software should alloc it*/
 	char *pool_name;
 	uint64_t pool_buff_size;/*size of buffer in pool , if this port is using its own pool*/
+                                /* it's good to have same buffer size if ports are using same style but different pools*/
 	uint64_t pool_max_buff;	/* number of max buffers allowed in the pool, if this port is using its own pool*/
 	bool aura_per_prt;	/* port will use its own aura, if FALSE use interface aura */
-	int aura;		/* aura number to use, if -1 allocated by software */
+        int aura;		/* aura number to use, if -1 allocated by sdk otherwise software should alloc it */
 	char *aura_name;
 	uint64_t aura_buff_cnt;	/* number of buffers in aura, if this pool is using its own aura*/
 	bool sso_grp_per_prt; 	/* port will use its own sso group, if FALSE use interface group*/
-	int sso_grp;		/* sso group number to use, if -1 allocated by software */
+        int sso_grp;		/* sso group number to use, if -1 allocated by sdk otherwise software should alloc it */
 	enum cvmx_pki_qpg_qos qpg_qos;
-	unsigned qpg_base;
+        int qpg_base;           /* offset in qpg table to use, if -1 allocated by sdk otherwise software should alloc it*/
 	struct cvmx_pki_qos_schd qos_s[CVMX_MAX_QOS_PRIORITY];
 };
 
 struct cvmx_pki_intf_schd {
 	bool style_per_prt;	/* Every port will use different style/profile */
 	bool style_per_intf;	/* otherwise all ports on this interface will use same style/profile */
-	int style;
+        int style;              /* style number to use, if -1 allocated by sdk otherwise software should alloc it*/
 	bool pool_per_intf; 	/* Ports will use either this shared pool or their own pool*/
-	int pool;		/* pool number to use, if -1 allocated by software*/
+        int pool;		/* pool number to use, if -1 allocated by sdk otherwise software should alloc it*/
 	char *pool_name;
 	uint64_t pool_buff_size;
 	uint64_t pool_max_buff;
 	bool aura_per_intf; 	/* Ports will use either this shared aura or their own aura */
-	int aura;		/* aura number to use, if -1 allocated by software*/
+        int aura;		/* aura number to use, if -1 allocated by sdk otherwise software should alloc it*/
 	char *aura_name;
 	uint64_t aura_buff_cnt;
 	bool sso_grp_per_intf;	/* Ports will use either this shared group or their own aura */
-	int sso_grp;		/* sso group number to use, if -1 allocated by software */
+        int sso_grp;		/* sso group number to use, if -1 allocated by sdk otherwise software should alloc it */
 	bool qos_share_aura;	/* All ports share the same aura for respective qos if qpg_qos used*/
 	bool qos_share_grp; 	/* All ports share the same sso group for respective qos if qps qos used*/
-	unsigned qpg_base;
+        int qpg_base;           /* offset in qpg table to use, if -1 allocated by sdk otherwise software should alloc it*/
 	struct cvmx_pki_prt_schd prt_s[CVMX_MAX_PORT_PER_INTERFACE];
 };
 
 struct cvmx_pki_global_schd {
 	bool setup_pool;
-	int pool;
+        int pool;              /* pool number to use, if -1 allocated by sdk otherwise software should alloc it*/
 	char *pool_name;
 	uint64_t pool_buff_size;
 	uint64_t pool_max_buff;
 	bool setup_aura;
-	int aura;
+        int aura;              /* aura number to use, if -1 allocated by sdk otherwise software should alloc it*/
 	char *aura_name;
 	uint64_t aura_buff_cnt;
 	bool setup_sso_grp;
-	int sso_grp;
+        int sso_grp;            /* sso group number to use, if -1 allocated by sdk otherwise software should alloc it */
 };
 
 extern CVMX_SHARED bool cvmx_pki_dflt_init[CVMX_MAX_NODES];
@@ -175,8 +179,20 @@ void cvmx_helper_pki_set_dflt_aura_buffer(int node, int buffer_count);
 int cvmx_helper_setup_aura_qos(int node, int aura, bool ena_red, bool ena_drop,
 			       uint64_t pass_thresh, uint64_t drop_thresh,
 			       bool ena_bp, uint64_t bp_thresh);
-int cvmx_helper_pki_map_aura_chl_bpid(int node, int aura_map[], int aura_cnt,
-				      int chl_map[], int chl_cnt, int bpid);
+/**
+ * This function maps specified bpid to all the auras from which it can receive bp and
+ * then maps that bpid to all the channels, that bpid can asserrt bp on.
+ *
+ * @param node          node number.
+ * @param aura          aura number which will back pressure specified bpid.
+ * @param bpid          bpid to map.
+ * @param chl_map       array of channels to map to that bpid.
+ * @param chl_cnt	number of channel/ports to map to that bpid.
+ * @return Zero on success. Negative on failure
+ */
+int cvmx_helper_pki_map_aura_chl_bpid(int node, uint16_t aura, uint16_t bpid,
+                                      uint16_t chl_map[], uint16_t chl_cnt);
+
 void cvmx_helper_pki_set_dflt_pkind_map(int node, int pkind, int style);
 void cvmx_helper_pki_get_dflt_style(int node, struct cvmx_pki_style_config *style_cfg);
 void cvmx_helper_pki_set_dflt_style(int node, struct cvmx_pki_style_config *style_cfg);
@@ -187,10 +203,31 @@ void cvmx_helper_pki_shutdown(int node);
 
 int cvmx_helper_pki_set_gbl_schd(int node, struct cvmx_pki_global_schd *gbl_schd);
 
+int cvmx_helper_pki_init_interface(int , struct cvmx_pki_intf_schd *intf, struct cvmx_pki_global_schd *gbl_schd);
+
 int cvmx_helper_pki_init_interface(int xiface,
 				   struct cvmx_pki_intf_schd *intf, struct cvmx_pki_global_schd *gbl_schd);
 int cvmx_helper_pki_init_port(int ipd_port, struct cvmx_pki_prt_schd *prtsch);
 void cvmx_helper_pki_no_dflt_init(int node);
+void cvmx_helper_pki_get_dflt_style(int node, struct cvmx_pki_style_config *style_cfg);
+void cvmx_helper_pki_set_dflt_style(int node, struct cvmx_pki_style_config *style_cfg);
+/**
+ * This function sets the wqe buffer mode of all ports. First packet data buffer can reside
+ * either in same buffer as wqe OR it can go in separate buffer. If used the later mode,
+ * make sure software allocate enough buffers to now have wqe separate from packet data.
+ * @param node	                node number.
+ * @param pkt_outside_wqe.	0 = The packet link pointer will be at word [FIRST_SKIP]
+ *				    immediately followed by packet data, in the same buffer
+ *				    as the work queue entry.
+ *				1 = The packet link pointer will be at word [FIRST_SKIP] in a new
+ *				    buffer separate from the work queue entry. Words following the
+ *				    WQE in the same cache line will be zeroed, other lines in the
+ *				    buffer will not be modified and will retain stale data (from the
+ *				    bufferâ€™s previous use). This setting may decrease the peak PKI
+ *				    performance by up to half on small packets.
+ */
+void cvmx_helper_pki_set_wqe_mode(int node, bool pkt_outside_wqe);
+void pki_wqe_dump(const cvmx_wqe_78xx_t* wqp);
 
 #ifdef __cplusplus
 /* *INDENT-OFF* */
diff --git a/arch/mips/include/asm/octeon/cvmx-l2c-defs.h b/arch/mips/include/asm/octeon/cvmx-l2c-defs.h
index 2540ece..17c175b 100644
--- a/arch/mips/include/asm/octeon/cvmx-l2c-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-l2c-defs.h
@@ -2338,7 +2338,7 @@ typedef union cvmx_l2c_cbcx_bist_status cvmx_l2c_cbcx_bist_status_t;
 /**
  * cvmx_l2c_cbc#_dll
  *
- * Register for DLL observability
+ * Register for DLL observability.
  *
  */
 union cvmx_l2c_cbcx_dll {
@@ -2421,17 +2421,17 @@ union cvmx_l2c_cbcx_int {
 	uint64_t holerd                       : 1;  /**< Read reference to 256MB hole occurred. */
 	uint64_t holewr                       : 1;  /**< Write reference to 256MB hole occurred. */
 	uint64_t iowrdisoci                   : 1;  /**< Illegal I/O write operation to a remote node with L2C_OCI_CTL[ENAOCI][node] clear. See
-                                                         L2C_CBC(0..3)_IODISOCIERR for logged information. This interrupt applies to IOBST8,
+                                                         L2C_CBC()_IODISOCIERR for logged information. This interrupt applies to IOBST8,
                                                          IOBST16, IOBST32, IOBST64, IOBADDR, LMTST, and LMTDMA XMC commands. */
 	uint64_t iorddisoci                   : 1;  /**< Illegal I/O read operation to a remote node with L2C_OCI_CTL[ENAOCI][node] clear. See
-                                                         L2C_CBC(0..3)_IODISOCIERR for logged information. This interrupt applies to IOBLD8,
+                                                         L2C_CBC()_IODISOCIERR for logged information. This interrupt applies to IOBLD8,
                                                          IOBLD16, IOBLD32, IOBLD64, IOBDMA, and LMTDMA XMC commands. */
-	uint64_t mibdbe                       : 1;  /**< MIB double-bit error occurred. See L2C_CBC(2..3)_MIBERR for logged information. */
-	uint64_t mibsbe                       : 1;  /**< MIB double-bit error occurred. See L2C_CBC(2..3)_MIBERR for logged information. */
+	uint64_t mibdbe                       : 1;  /**< MIB double-bit error occurred. See L2C_CBC()_MIBERR for logged information. */
+	uint64_t mibsbe                       : 1;  /**< MIB double-bit error occurred. See L2C_CBC()_MIBERR for logged information. */
 	uint64_t ioccmddbe                    : 1;  /**< IOCCMD double-bit error occurred. See L2C_CBC(0..3)_IOCERR for logged information. */
 	uint64_t ioccmdsbe                    : 1;  /**< IOCCMD single-bit error occurred. See L2C_CBC(0..3)_IOCERR for logged information. */
-	uint64_t rsddbe                       : 1;  /**< RSD double-bit error occurred. See L2C_CBC(0..3)_RSDERR for logged information. */
-	uint64_t rsdsbe                       : 1;  /**< RSD single-bit error occurred. See L2C_CBC(0..3)_RSDERR for logged information. */
+	uint64_t rsddbe                       : 1;  /**< RSD double-bit error occurred. See L2C_CBC()_RSDERR for logged information. */
+	uint64_t rsdsbe                       : 1;  /**< RSD single-bit error occurred. See L2C_CBC()_RSDERR for logged information. */
 #else
 	uint64_t rsdsbe                       : 1;
 	uint64_t rsddbe                       : 1;
@@ -2451,8 +2451,8 @@ union cvmx_l2c_cbcx_int {
 	uint64_t reserved_4_63                : 60;
 	uint64_t ioccmddbe                    : 1;  /**< IOCCMD double-bit error occurred. See L2C_CBC(0..3)_IOCERR for logged information. */
 	uint64_t ioccmdsbe                    : 1;  /**< IOCCMD single-bit error occurred. See L2C_CBC(0..3)_IOCERR for logged information. */
-	uint64_t rsddbe                       : 1;  /**< RSD double-bit error occurred. See L2C_CBC(0..3)_RSDERR for logged information. */
-	uint64_t rsdsbe                       : 1;  /**< RSD single-bit error occurred. See L2C_CBC(0..3)_RSDERR for logged information. */
+	uint64_t rsddbe                       : 1;  /**< RSD double-bit error occurred. See L2C_CBC()_RSDERR for logged information. */
+	uint64_t rsdsbe                       : 1;  /**< RSD single-bit error occurred. See L2C_CBC()_RSDERR for logged information. */
 #else
 	uint64_t rsdsbe                       : 1;
 	uint64_t rsddbe                       : 1;
@@ -2468,16 +2468,16 @@ union cvmx_l2c_cbcx_int {
 	uint64_t holerd                       : 1;  /**< Read reference to 256MB hole occurred. */
 	uint64_t holewr                       : 1;  /**< Write reference to 256MB hole occurred. */
 	uint64_t iowrdisoci                   : 1;  /**< Illegal I/O write operation to a remote node with L2C_OCI_CTL[ENAOCI][node] clear. See
-                                                         L2C_CBC(0..3)_IODISOCIERR for logged information. This interrupt applies to IOBST8,
+                                                         L2C_CBC()_IODISOCIERR for logged information. This interrupt applies to IOBST8,
                                                          IOBST16, IOBST32, IOBST64, IOBADDR, LMTST, and LMTDMA XMC commands. */
 	uint64_t iorddisoci                   : 1;  /**< Illegal I/O read operation to a remote node with L2C_OCI_CTL[ENAOCI][node] clear. See
-                                                         L2C_CBC(0..3)_IODISOCIERR for logged information. This interrupt applies to IOBLD8,
+                                                         L2C_CBC()_IODISOCIERR for logged information. This interrupt applies to IOBLD8,
                                                          IOBLD16, IOBLD32, IOBLD64, IOBDMA, and LMTDMA XMC commands. */
-	uint64_t mibdbe                       : 1;  /**< MIB double-bit error occurred. See L2C_CBC(2..3)_MIBERR for logged information. */
-	uint64_t mibsbe                       : 1;  /**< MIB double-bit error occurred. See L2C_CBC(2..3)_MIBERR for logged information. */
+	uint64_t mibdbe                       : 1;  /**< MIB double-bit error occurred. See L2C_CBC()_MIBERR for logged information. */
+	uint64_t mibsbe                       : 1;  /**< MIB double-bit error occurred. See L2C_CBC()_MIBERR for logged information. */
 	uint64_t reserved_2_3                 : 2;
-	uint64_t rsddbe                       : 1;  /**< RSD double-bit error occurred. See L2C_CBC(0..3)_RSDERR for logged information. */
-	uint64_t rsdsbe                       : 1;  /**< RSD single-bit error occurred. See L2C_CBC(0..3)_RSDERR for logged information. */
+	uint64_t rsddbe                       : 1;  /**< RSD double-bit error occurred. See L2C_CBC()_RSDERR for logged information. */
+	uint64_t rsdsbe                       : 1;  /**< RSD single-bit error occurred. See L2C_CBC()_RSDERR for logged information. */
 #else
 	uint64_t rsdsbe                       : 1;
 	uint64_t rsddbe                       : 1;
@@ -2537,7 +2537,7 @@ typedef union cvmx_l2c_cbcx_iocerr cvmx_l2c_cbcx_iocerr_t;
  * This register records error information associated with IORDDISOCI/IOWRDISOCI interrupts.
  * IOWRDISOCI events take priority over previously captured IORDDISOCI events. Of the available
  * I/O transactions, some commands will either set IORDDISOCI, set IOWRDISOCI, or set both
- * IORDDISOCI and IOWRDISOCI. See L2C_CBC(0..3)_INT for information about which I/O transactions
+ * IORDDISOCI and IOWRDISOCI. See L2C_CBC()_INT for information about which I/O transactions
  * may result in IORDDISOCI/IOWRDISOCI interrupts.
  */
 union cvmx_l2c_cbcx_iodisocierr {
@@ -3194,16 +3194,17 @@ typedef union cvmx_l2c_cfg cvmx_l2c_cfg_t;
  * NOTE: for 78xx, if the PPID is outside the range of 0-47,255 the write will be ignored and
  * reads will return 0x2bad2bad2bad2bad
  *
- * (1) RD and SEL are as defined in the HRM description of Core Coprocessor 0 registers
- * and note 4 below.
+ * 1. RD and SEL are as defined in the description of Core Coprocessor 0 registers.
  *
- * (2) if a COP0 register cannot be accessed by this mechanism the write be silently ignored and
+ * 2. If a COP0 register cannot be accessed by this mechanism the write be silently ignored and
  * the read data will be 0x2bad2bad2bad2bad.
  *
- * (3) if the PPID is outside the range of 0-47,255 or if the PP in question is in reset
+ * 3. If the PPID is outside the range of 0-47,255 or if the core in question is in reset
  * a write will be ignored and reads will timeout the RSL bus.
  *
- * (4) Referring to note (1) above, the following root/rd/sel values are supported:
+ * INTERNAL: In HRM, see table in L2C chapter body text.
+ * Referring to note 1 above, the following root/rd/sel values are supported:
+ *
  * NOTE: Put only the "Customer type" in HRM. do not put the "Real type" in HRM.
  *
  *              Customer                                           Real
@@ -3250,7 +3251,7 @@ typedef union cvmx_l2c_cfg cvmx_l2c_cfg_t;
  *  1   18   0     RO     CP0 Root.WatchLo0                         RW
  *  1   19   0     RO     CP0 Root.WatchHi0                         RW
  *  1   22   0     RO     CP0 Root.MultiCoreDebug                   RW
- *  1   22   1            CP0 Root.ImplDebug                        R0
+ *  1   22   1     --     CP0 Root.ImplDebug                        R0
  *  1   22   2     RO     CP0 Root.CvmCountOffset                   RW
  *  1   23   0     RO     CP0 Root.Debug                            RW
  *  1   23   6     RO     CP0 Root.Debug2                           RO
@@ -3283,17 +3284,17 @@ typedef union cvmx_l2c_cfg cvmx_l2c_cfg_t;
  *  - 63:2  issue_address<63:2>   // often VA<63:2> (PC) of the next instruction to issue (5a in
  * pipeline)
  *                              //    but can also be the PC of an instruction
- * executing/replaying
+ *                              //    executing/replaying
  *                              //    or can also be a PC being filled into the instruction cache
  *                              //    or can also be unpredictable
  *                              // <58:50> is a copy of <49>
  *  1     issue_illegal         // set when issue_address is an illegal PC
  *  0     issue_delayslot       // set when issue_address is in a delayslot (prior instruction
- * may be either taken or not taken)
+ *                              // may be either taken or not taken)
  *
  * PC Fetch Debug Info
  *  - 63:1  fetch_address<63:1>   // VA <63:0> (PC) being fetched from the instruction cache (3a in
- * pipeline)
+ *                              // pipeline)
  *                              // <58:50> is a copy of <49>
  *                              // <1> RAZ
  *  0     fetch_guest           // set when fetch_address is for the guest
@@ -3301,7 +3302,7 @@ typedef union cvmx_l2c_cfg cvmx_l2c_cfg_t;
  * PC Fill Debug Info
  *  - 63:2  fill_address<63:2>    // VA<63:2> being filled into instruction cache (4a in pipeline)
  *                              // valid when waiting_for_ifill is set (see PC Misc Debug Info
- * below)
+ *                              // below)
  *                              // <58:50> is a copy of <49>
  *  1     fill_illegal          // set when fill_address is an illegal PC
  *  0     fill_guest            // set when fill_address is for the guest
@@ -3314,13 +3315,13 @@ typedef union cvmx_l2c_cfg cvmx_l2c_cfg_t;
  *  1     waiting_for_pfill     // when waiting_for_ifill is set, indicates whether instruction
  *                              // cache fill is due to a prefetch (4a in pipeline)
  *  0     waiting_for_ifill     // set when there is an outstanding instruction cache fill (4a in
- * pipeline)
+ *                              // pipeline)
  *
  * PC Committed Debug Info
  *  63    commit_guest          // Set if commit_address was for the guest
  *  - 62:55 commit_ASID           // ASID of commit_address
  *  - 54:49 commit_address<63:59> // VA<63:59> (PC) of the last committed instruction (11a in
- * pipeline)
+ *                              // pipeline)
  *  - 48:0  commit_address<48:0>  // VA<48:0> (PC) of last committed instruction (11a in pipeline)
  *                              // <1:0> RAZ
  */
@@ -3333,8 +3334,8 @@ union cvmx_l2c_cop0_adr {
 	uint64_t reserved_15_15               : 1;
 	uint64_t mbz                          : 6;  /**< Must be written to zero. */
 	uint64_t root                         : 1;  /**< If 1, root register is accessed, if 0 guest register is accessed. */
-	uint64_t rd                           : 5;  /**< COP0 register number */
-	uint64_t sel                          : 3;  /**< COP0 sel */
+	uint64_t rd                           : 5;  /**< COP0 register number. */
+	uint64_t sel                          : 3;  /**< COP0 select. */
 #else
 	uint64_t sel                          : 3;
 	uint64_t rd                           : 5;
@@ -3834,9 +3835,9 @@ union cvmx_l2c_ctl {
 	uint64_t discclk                      : 1;  /**< Disable conditional clocking in L2C PNR blocks. */
 	uint64_t reserved_16_23               : 8;
 	uint64_t rsp_arb_mode                 : 1;  /**< Arbitration mode for RSC/RSD bus. 0 = round-robin; 1 = static priority.
-                                                         1. IOR data
-                                                         2. STIN/FILLs
-                                                         3. STDN/SCDN/SCFL */
+                                                         1. IOR data.
+                                                         2. STIN/FILLs.
+                                                         3. STDN/SCDN/SCFL. */
 	uint64_t xmc_arb_mode                 : 1;  /**< Arbitration mode for ADD bus QOS queues. 0 = fully determined through QOS, 1 = QOS0
                                                          highest priority; QOS 1-7 use normal mode. */
 	uint64_t rdf_cnt                      : 8;  /**< Defines the sample point of the LMC response data in the DDR-clock/core-clock crossing.
@@ -3873,9 +3874,9 @@ union cvmx_l2c_ctl {
 	uint64_t discclk                      : 1;  /**< Disable conditional clocking in L2C PNR blocks. */
 	uint64_t reserved_16_23               : 8;
 	uint64_t rsp_arb_mode                 : 1;  /**< Arbitration mode for RSC/RSD bus. 0 = round-robin; 1 = static priority.
-                                                         1. IOR data
-                                                         2. STIN/FILLs
-                                                         3. STDN/SCDN/SCFL */
+                                                         1. IOR data.
+                                                         2. STIN/FILLs.
+                                                         3. STDN/SCDN/SCFL. */
 	uint64_t xmc_arb_mode                 : 1;  /**< Arbitration mode for ADD bus QOS queues. 0 = fully determined through QOS, 1 = QOS0
                                                          highest priority; QOS 1-7 use normal mode. */
 	uint64_t rdf_cnt                      : 8;  /**< Defines the sample point of the LMC response data in the DDR-clock/core-clock crossing.
@@ -4547,6 +4548,7 @@ typedef union cvmx_l2c_dut_mapx cvmx_l2c_dut_mapx_t;
  * 0x1 = Single-bit error on ECC[0].
  * 0x2 = Single-bit error on ECC[1].
  * 0x3 = Double-bit error on ECC[1:0].
+ *
  * L2DFLIP allows software to generate L2DSBE, L2DDBE, VBFSBE, and VBFDBE errors for the purposes
  * of testing error handling code. When one (or both) of these bits are set, a PL2 that misses in
  * the L2 will fill with the appropriate error in the first two OWs of the fill. Software can
@@ -4554,6 +4556,7 @@ typedef union cvmx_l2c_dut_mapx cvmx_l2c_dut_mapx_t;
  * PL2 that hits in the L2 will not inject any errors. Therefore sending a WBIL2 prior to the PL2
  * is recommended to make a miss likely. (If multiple processors are involved, software must be
  * sure that no other processor or I/O device can bring the block into the L2).
+ *
  * To generate a VBFSBE or VBFDBE, software must first get the cache block into the cache with an
  * error using a PL2 that misses the L2. Then a store partial to a portion of the cache block
  * without the error must change the block to dirty. Then, a subsequent WBL2/WBIL2/victim will
@@ -6089,10 +6092,10 @@ union cvmx_l2c_mcix_err {
 	uint64_t u64;
 	struct cvmx_l2c_mcix_err_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t vbfdbe1                      : 1;  /**< INDEX/SYN1 corresponds to a double-bit VBF ECC error */
-	uint64_t vbfdbe0                      : 1;  /**< INDEX/SYN0 corresponds to a double-bit VBF ECC error */
-	uint64_t vbfsbe1                      : 1;  /**< INDEX/SYN1 corresponds to a single-bit VBF ECC error */
-	uint64_t vbfsbe0                      : 1;  /**< INDEX/SYN0 corresponds to a single-bit VBF ECC error */
+	uint64_t vbfdbe1                      : 1;  /**< INDEX/SYN1 corresponds to a double-bit VBF ECC error. */
+	uint64_t vbfdbe0                      : 1;  /**< INDEX/SYN0 corresponds to a double-bit VBF ECC error. */
+	uint64_t vbfsbe1                      : 1;  /**< INDEX/SYN1 corresponds to a single-bit VBF ECC error. */
+	uint64_t vbfsbe0                      : 1;  /**< INDEX/SYN0 corresponds to a single-bit VBF ECC error. */
 	uint64_t reserved_48_59               : 12;
 	uint64_t syn1                         : 8;  /**< Error syndrome for QW1 ([127:64]). */
 	uint64_t syn0                         : 8;  /**< Error syndrome for QW0 ([63:0]). */
@@ -6131,8 +6134,8 @@ union cvmx_l2c_mcix_int {
 	struct cvmx_l2c_mcix_int_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_2_63                : 62;
-	uint64_t vbfdbe                       : 1;  /**< VBF double-bit error occurred. See L2C_MCI(0..3)_ERR for logged information. */
-	uint64_t vbfsbe                       : 1;  /**< VBF single-bit error occurred. See L2C_MCI(0..3)_ERR for logged information. */
+	uint64_t vbfdbe                       : 1;  /**< VBF double-bit error occurred. See L2C_MCI()_ERR for logged information. */
+	uint64_t vbfsbe                       : 1;  /**< VBF single-bit error occurred. See L2C_MCI()_ERR for logged information. */
 #else
 	uint64_t vbfsbe                       : 1;
 	uint64_t vbfdbe                       : 1;
@@ -6159,7 +6162,7 @@ union cvmx_l2c_oci_ctl {
 	uint64_t lock_local_stc               : 1;  /**< When set, L2 STC operations to remote addresses which miss at the requester will be
                                                          performed locally (if possible) on the requesting node. Default operation will instead
                                                          send the STC request to be performed on the home node. For CAS ops LOCK_LOCAL_CAS. */
-	uint64_t lock_local_pp                : 1;  /**< When clear, L2 atomic operations (excluding CAS/STC) pp initiated requests to remote
+	uint64_t lock_local_pp                : 1;  /**< When clear, L2 atomic operations (excluding CAS/STC) core initiated requests to remote
                                                          addresses which miss at the requester will send the atomic request to be performed on the
                                                          home node. Default operation will instead  be performed locally on the requesting node.
                                                          For request initiated by IOB & for STC & CAS ops, see
@@ -6174,7 +6177,7 @@ union cvmx_l2c_oci_ctl {
                                                          Actual timeout is between 1x and 2x this interval. For example if SHTOLEN = 14 (the reset
                                                          value), the timeout is between 16K and 32K core clocks. Note: a value of 0 disables this
                                                          timer. */
-	uint64_t shtoioen                     : 1;  /**< When set, any PP issues any of an IO load, acking store, IOBDMA, LMTDMA, acking IOBADDR,
+	uint64_t shtoioen                     : 1;  /**< When set, any core issues any of an IO load, acking store, IOBDMA, LMTDMA, acking IOBADDR,
                                                          or acking LMTST to a node that doesn't exist (existence defined by the ENAOCI bits), then
                                                          the hardware sets [SHTO]. */
 	uint64_t shtoen                       : 3;  /**< When set, if the corresponding OCI link is down, the hardware sets [SHTO].
@@ -6199,7 +6202,7 @@ union cvmx_l2c_oci_ctl {
 	uint64_t lock_local_iob               : 1;  /**< When set, L2 atomic operations (excluding CAS/STC) initiated by IOB to remote addresses
                                                          which miss at the requester are performed locally on the requesting node. When clear the
                                                          operation instead sends the atomic request to be performed on the home node. For request
-                                                         initiated by PP & for STC & CAS ops see LOCK_LOCAL_PP/LOCK_LOCAL_STC/LOCK_LOCAL_CAS.
+                                                         initiated by core for STC and CAS ops; see LOCK_LOCAL_PP/LOCK_LOCAL_STC/LOCK_LOCAL_CAS.
                                                          Default is set to 1 (local locks). */
 	uint64_t iofrcl                       : 1;  /**< When set, L2C services all I/O read and write operations on the local node, regardless of
                                                          the value of the node ID bits in the physical address. During normal operation this bit is
@@ -7473,24 +7476,24 @@ union cvmx_l2c_tadx_int {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_36_63               : 28;
 	uint64_t wrdisoci                     : 1;  /**< Illegal write operation to a remote node with L2C_OCI_CTL[ENAOCI][node] clear. See
-                                                         L2C_TAD(0..7)_ERR for for logged information. */
+                                                         L2C_TAD()_ERR for for logged information. */
 	uint64_t rddisoci                     : 1;  /**< Illegal read operation to a remote node with L2C_OCI_CTL[ENAOCI][node] clear. Note
                                                          RDDISOCI interrupts can occur during normal operation as the cores are allowed to prefetch
                                                          to nonexistent memory locations. Therefore, RDDISOCI is for informational purposes only.
-                                                         See L2C_TAD(0..7)_ERR for logged information. */
+                                                         See L2C_TAD()_ERR for logged information. */
 	uint64_t rtgdbe                       : 1;  /**< RTG double-bit error. */
 	uint64_t rtgsbe                       : 1;  /**< RTG single-bit error. */
 	uint64_t reserved_18_31               : 14;
 	uint64_t lfbto                        : 1;  /**< An LFB entry (or more) has encountered a timeout condition When LFBTO timeout condition
-                                                         occurs L2C_TAD(0..7)_TIMEOUT is loaded. L2C_TAD(0..7)_TIMEOUT is loaded with info from the
+                                                         occurs L2C_TAD()_TIMEOUT is loaded. L2C_TAD()_TIMEOUT is loaded with info from the
                                                          first LFB that timed out. if multiple LFB timed out simultaneously, then the it will
                                                          capture info from the lowest LFB number that timed out. */
 	uint64_t reserved_15_16               : 2;
 	uint64_t bigrd                        : 1;  /**< Read reference past L2C_BIG_CTL[MAXDRAM] occurred. BIGRD interrupts can occur during
                                                          normal operation as the cores are allowed to prefetch to nonexistent memory locations.
-                                                         Therefore, BIGRD is for informational purposes only. See L2C_TAD(0..7)_ERR for logged
+                                                         Therefore, BIGRD is for informational purposes only. See L2C_TAD()_ERR for logged
                                                          information. */
-	uint64_t bigwr                        : 1;  /**< Write reference past L2C_BIG_CTL[MAXDRAM] occurred. See L2C_TAD(0..7)_ERR for logged information. */
+	uint64_t bigwr                        : 1;  /**< Write reference past L2C_BIG_CTL[MAXDRAM] occurred. See L2C_TAD()_ERR for logged information. */
 	uint64_t holerd                       : 1;  /**< Read reference to 256MB hole occurred. */
 	uint64_t holewr                       : 1;  /**< Write reference to 256MB hole occurred. */
 	uint64_t reserved_2_10                : 9;
@@ -7582,16 +7585,16 @@ union cvmx_l2c_tadx_int {
                                                          modify-write DRAM for every transaction that updates some, but not all, of the bytes in a
                                                          cache block, misses in the L2 cache, and cannot allocate a WAY.) There is one 'failure'
                                                          case where L2C sets NOWAY: when it cannot leave a block locked in the L2 cache as part of
-                                                         a LCKL2 transaction. See L2C_TTG(0..7)_ERR for logged information. */
-	uint64_t tagdbe                       : 1;  /**< TAG double-bit error occurred. See L2C_TTG(0..7)_ERR for logged information. */
-	uint64_t tagsbe                       : 1;  /**< TAG single-bit error occurred. See L2C_TTG(0..7)_ERR for logged information. */
+                                                         a LCKL2 transaction. See L2C_TTG()_ERR for logged information. */
+	uint64_t tagdbe                       : 1;  /**< TAG double-bit error occurred. See L2C_TTG()_ERR for logged information. */
+	uint64_t tagsbe                       : 1;  /**< TAG single-bit error occurred. See L2C_TTG()_ERR for logged information. */
 	uint64_t reserved_6_7                 : 2;
-	uint64_t fbfdbe                       : 1;  /**< FBF double-bit error occurred. See L2C_TQD(0..7)_ERR for logged information. */
-	uint64_t fbfsbe                       : 1;  /**< FBF single-bit error occurred. See L2C_TQD(0..7)_ERR for logged information. */
-	uint64_t sbfdbe                       : 1;  /**< SBF double-bit error occurred. See L2C_TQD(0..7)_ERR for logged information. */
-	uint64_t sbfsbe                       : 1;  /**< SBF single-bit error occurred. See L2C_TQD(0..7)_ERR for logged information. */
-	uint64_t l2ddbe                       : 1;  /**< L2D double-bit error occurred. See L2C_TQD(0..7)_ERR for logged information. */
-	uint64_t l2dsbe                       : 1;  /**< L2D single-bit error occurred. See L2C_TQD(0..7)_ERR for logged information. */
+	uint64_t fbfdbe                       : 1;  /**< FBF double-bit error occurred. See L2C_TQD()_ERR for logged information. */
+	uint64_t fbfsbe                       : 1;  /**< FBF single-bit error occurred. See L2C_TQD()_ERR for logged information. */
+	uint64_t sbfdbe                       : 1;  /**< SBF double-bit error occurred. See L2C_TQD()_ERR for logged information. */
+	uint64_t sbfsbe                       : 1;  /**< SBF single-bit error occurred. See L2C_TQD()_ERR for logged information. */
+	uint64_t l2ddbe                       : 1;  /**< L2D double-bit error occurred. See L2C_TQD()_ERR for logged information. */
+	uint64_t l2dsbe                       : 1;  /**< L2D single-bit error occurred. See L2C_TQD()_ERR for logged information. */
 #else
 	uint64_t l2dsbe                       : 1;
 	uint64_t l2ddbe                       : 1;
@@ -7620,25 +7623,25 @@ union cvmx_l2c_tadx_int {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_36_63               : 28;
 	uint64_t wrdisoci                     : 1;  /**< Illegal write operation to a remote node with L2C_OCI_CTL[ENAOCI][node] clear. See
-                                                         L2C_TAD(0..7)_ERR for for logged information. */
+                                                         L2C_TAD()_ERR for for logged information. */
 	uint64_t rddisoci                     : 1;  /**< Illegal read operation to a remote node with L2C_OCI_CTL[ENAOCI][node] clear. Note
                                                          RDDISOCI interrupts can occur during normal operation as the cores are allowed to prefetch
                                                          to nonexistent memory locations. Therefore, RDDISOCI is for informational purposes only.
-                                                         See L2C_TAD(0..7)_ERR for logged information. */
+                                                         See L2C_TAD()_ERR for logged information. */
 	uint64_t rtgdbe                       : 1;  /**< RTG double-bit error. */
 	uint64_t rtgsbe                       : 1;  /**< RTG single-bit error. */
 	uint64_t reserved_18_31               : 14;
 	uint64_t lfbto                        : 1;  /**< An LFB entry (or more) has encountered a timeout condition When LFBTO timeout condition
-                                                         occurs L2C_TAD(0..7)_TIMEOUT is loaded. L2C_TAD(0..7)_TIMEOUT is loaded with info from the
+                                                         occurs L2C_TAD()_TIMEOUT is loaded. L2C_TAD()_TIMEOUT is loaded with info from the
                                                          first LFB that timed out. if multiple LFB timed out simultaneously, then the it will
                                                          capture info from the lowest LFB number that timed out. */
 	uint64_t wrdislmc                     : 1;  /**< Illegal write to disabled LMC error. A DRAM write arrived before LMC was enabled. */
 	uint64_t rddislmc                     : 1;  /**< Illegal read to disabled LMC error. A DRAM read arrived before LMC was enabled. */
 	uint64_t bigrd                        : 1;  /**< Read reference past L2C_BIG_CTL[MAXDRAM] occurred. BIGRD interrupts can occur during
                                                          normal operation as the cores are allowed to prefetch to nonexistent memory locations.
-                                                         Therefore, BIGRD is for informational purposes only. See L2C_TAD(0..7)_ERR for logged
+                                                         Therefore, BIGRD is for informational purposes only. See L2C_TAD()_ERR for logged
                                                          information. */
-	uint64_t bigwr                        : 1;  /**< Write reference past L2C_BIG_CTL[MAXDRAM] occurred. See L2C_TAD(0..7)_ERR for logged information. */
+	uint64_t bigwr                        : 1;  /**< Write reference past L2C_BIG_CTL[MAXDRAM] occurred. See L2C_TAD()_ERR for logged information. */
 	uint64_t reserved_11_12               : 2;
 	uint64_t noway                        : 1;  /**< No way was available for allocation. L2C sets NOWAY during its processing of a transaction
                                                          whenever it needed/wanted to allocate a WAY in the L2 cache, but was unable to. When this
@@ -7647,16 +7650,16 @@ union cvmx_l2c_tadx_int {
                                                          modify-write DRAM for every transaction that updates some, but not all, of the bytes in a
                                                          cache block, misses in the L2 cache, and cannot allocate a WAY.) There is one 'failure'
                                                          case where L2C sets NOWAY: when it cannot leave a block locked in the L2 cache as part of
-                                                         a LCKL2 transaction. See L2C_TTG(0..7)_ERR for logged information. */
-	uint64_t tagdbe                       : 1;  /**< TAG double-bit error occurred. See L2C_TTG(0..7)_ERR for logged information. */
-	uint64_t tagsbe                       : 1;  /**< TAG single-bit error occurred. See L2C_TTG(0..7)_ERR for logged information. */
+                                                         a LCKL2 transaction. See L2C_TTG()_ERR for logged information. */
+	uint64_t tagdbe                       : 1;  /**< TAG double-bit error occurred. See L2C_TTG()_ERR for logged information. */
+	uint64_t tagsbe                       : 1;  /**< TAG single-bit error occurred. See L2C_TTG()_ERR for logged information. */
 	uint64_t reserved_6_7                 : 2;
-	uint64_t fbfdbe                       : 1;  /**< FBF double-bit error occurred. See L2C_TQD(0..7)_ERR for logged information. */
-	uint64_t fbfsbe                       : 1;  /**< FBF single-bit error occurred. See L2C_TQD(0..7)_ERR for logged information. */
-	uint64_t sbfdbe                       : 1;  /**< SBF double-bit error occurred. See L2C_TQD(0..7)_ERR for logged information. */
-	uint64_t sbfsbe                       : 1;  /**< SBF single-bit error occurred. See L2C_TQD(0..7)_ERR for logged information. */
-	uint64_t l2ddbe                       : 1;  /**< L2D double-bit error occurred. See L2C_TQD(0..7)_ERR for logged information. */
-	uint64_t l2dsbe                       : 1;  /**< L2D single-bit error occurred. See L2C_TQD(0..7)_ERR for logged information. */
+	uint64_t fbfdbe                       : 1;  /**< FBF double-bit error occurred. See L2C_TQD()_ERR for logged information. */
+	uint64_t fbfsbe                       : 1;  /**< FBF single-bit error occurred. See L2C_TQD()_ERR for logged information. */
+	uint64_t sbfdbe                       : 1;  /**< SBF double-bit error occurred. See L2C_TQD()_ERR for logged information. */
+	uint64_t sbfsbe                       : 1;  /**< SBF single-bit error occurred. See L2C_TQD()_ERR for logged information. */
+	uint64_t l2ddbe                       : 1;  /**< L2D double-bit error occurred. See L2C_TQD()_ERR for logged information. */
+	uint64_t l2dsbe                       : 1;  /**< L2D single-bit error occurred. See L2C_TQD()_ERR for logged information. */
 #else
 	uint64_t l2dsbe                       : 1;
 	uint64_t l2ddbe                       : 1;
@@ -7935,9 +7938,9 @@ union cvmx_l2c_tadx_tag {
 	uint64_t reserved_58_59               : 2;
 	uint64_t businfo                      : 9;  /**< The bus information bits. Ignored/loaded with 0 for RTG accesses. */
 	uint64_t ecc                          : 7;  /**< The tag ECC. This field is undefined if L2C_CTL[DISECC] is not 1 when the LTGL2I reads the tags. */
-	uint64_t tag                          : 22; /**< The tag. TAG[39:20] is the corresponding bits from the L2C+LMC internal L2/DRAM byte
-                                                         address. TAG[41:40] is the OCI node of the address. The RTG must always have the
-                                                         TAG[41:40] == to the current node or operation is undefined. */
+	uint64_t tag                          : 22; /**< The tag. TAG<39:20> is the corresponding bits from the L2C+LMC internal L2/DRAM byte
+                                                         address. TAG<41:40> is the OCI node of the address. The RTG must always have the
+                                                         TAG<41:40> equal to the current node or operation is undefined. */
 	uint64_t reserved_6_19                : 14;
 	uint64_t node                         : 2;  /**< The node ID for the remote node which holds this block. Ignored/loaded with 0 for TAG accesses. */
 	uint64_t ts                           : 2;  /**< The tag state.
@@ -8026,7 +8029,7 @@ union cvmx_l2c_tadx_timetwo {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_33_63               : 31;
 	uint64_t sid                          : 4;  /**< Source id of the original request, that is 'source' of request. This is only valid if the
-                                                         request is a local request (valid if L2C_TAD(0..7)_TIMEOUT[CMD] is an XMC request and not
+                                                         request is a local request (valid if L2C_TAD()_TIMEOUT[CMD] is an XMC request and not
                                                          relevant if it is an OCI request). */
 	uint64_t busid                        : 4;  /**< Busid of the original request, that is 'source' of request. */
 	uint64_t vabst                        : 3;  /**< This is the LFB internal state if INFOLFB is set, else will contain VAB internal state if
@@ -8221,7 +8224,7 @@ union cvmx_l2c_ttgx_bist_status {
 	uint64_t rtgfl                        : 16; /**< BIST failure status for RTG ways. */
 	uint64_t reserved_18_31               : 14;
 	uint64_t lrulfbfl                     : 1;  /**< Reserved, always zero. */
-	uint64_t lrufl                        : 1;  /**< BIST failure status for tag LRU */
+	uint64_t lrufl                        : 1;  /**< BIST failure status for tag LRU. */
 	uint64_t tagfl                        : 16; /**< BIST failure status for TAG ways. */
 #else
 	uint64_t tagfl                        : 16;
@@ -8239,7 +8242,7 @@ union cvmx_l2c_ttgx_bist_status {
 	uint64_t rtgfl                        : 16; /**< Always zero for 70xx. */
 	uint64_t reserved_18_31               : 14;
 	uint64_t lrulfbfl                     : 1;  /**< BIST failure status for LRULFB memory */
-	uint64_t lrufl                        : 1;  /**< BIST failure status for tag LRU */
+	uint64_t lrufl                        : 1;  /**< BIST failure status for tag LRU. */
 	uint64_t tagfl                        : 16; /**< BIST failure status for TAG ways. */
 #else
 	uint64_t tagfl                        : 16;
@@ -8790,18 +8793,21 @@ typedef union cvmx_l2c_xmcx_pfc cvmx_l2c_xmcx_pfc_t;
  *
  * Note the following:
  * The ADD bus command chosen must not be a IOB-destined command or operation is UNDEFINED.
+ *
  * The ADD bus command will have SID forced to IOB, DID forced to L2C, no virtualization checks
  * performed (always pass), and xmdmsk forced to 0. Note that this implies that commands that
  * REQUIRE a STORE cycle (STP, STC, SAA, FAA, FAS) should not be used or the results are
  * unpredictable. The sid = IOB means that the way partitioning used for the command is
- * L2C_WPAR_IOB(0..1). Neither L2C_QOS_IOB(0..1) or L2C_QOS_PP(0..47) are used for these
+ * L2C_WPAR_IOB(). Neither L2C_QOS_IOB() nor L2C_QOS_PP() are used for these
  * commands.
+ *
  * Any FILL responses generated by the ADD bus command are ignored. Generated STINs, however,
  * will correctly invalidate the required cores.
  * Any L2D read generated by the ADD bus command records the syndrome information in
  * L2C_TAD(0..3)_ECC0/1. If ECC is disabled prior to the CSR write, this provides the ability to
  * read the ECC bits directly. If ECC is not disabled, this should log zeros (assuming no ECC
  * errors were found in the block).
+ *
  * A write that arrives while the INUSE bit is set will block until the INUSE bit clears. This
  * gives software two options when needing to issue a stream of write operations to L2C_XMC_CMD:
  * polling on the INUSE bit, or allowing hardware to handle the interlock -- at the expense of
@@ -8809,6 +8815,7 @@ typedef union cvmx_l2c_xmcx_pfc cvmx_l2c_xmcx_pfc_t;
  * LFB/VAB entry. Note that when the INUSE bit clears, the only ordering it implies is that
  * software can send another ADD bus command. Subsequent commands may complete out of order
  * relative to earlier commands.
+ *
  * The address written to L2C_XMC_CMD is a physical address. L2C performs hole removal and index
  * aliasing (if enabled) on the written address and uses that for the command. This hole
  * removed/index aliased address is what is returned on a read of the L2C_XMC_CMD register.
diff --git a/arch/mips/include/asm/octeon/cvmx-mio-defs.h b/arch/mips/include/asm/octeon/cvmx-mio-defs.h
index f5831f7..5ff8a7f 100644
--- a/arch/mips/include/asm/octeon/cvmx-mio-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-mio-defs.h
@@ -2386,11 +2386,11 @@ typedef union cvmx_mio_boot_dma_adrx cvmx_mio_boot_dma_adrx_t;
  * core clocks or the RML timeout specified in SLI_WINDOW_CTL[TIME] coprocessor clocks if
  * accesses to the bootbus occur while DMA operations are in progress.
  * The DMA operation duration in coprocessor clocks as:
- * MIO_BOOT_DMA_CFGn[SIZE] * MIO_BOOT_DMA_TIMn[TIM_MULT] * CYCLE_TIME.
+ * MIO_BOOT_DMA_CFG()[SIZE] * MIO_BOOT_DMA_TIM()[TIM_MULT] * CYCLE_TIME.
  * Where:
  * CYCLE_TIME = MIO_BOOT_DMA_TIMn[RD_DLY+PAUSE+DMACK_H+WE_N+WE_A+OE_N+OE_A+DMACK_S].
  * Coprocessor clocks can be converted to core clocks by multiplying the value by the clock ratio
- * MIO_RST_BOOT[C_MUL] / MIO_RST_BOOT[PNR_MUL]
+ * RST_BOOT[C_MUL] / RST_BOOT[PNR_MUL].
  */
 union cvmx_mio_boot_dma_cfgx {
 	uint64_t u64;
@@ -3292,9 +3292,9 @@ union cvmx_mio_emm_access_wdog {
 	struct cvmx_mio_emm_access_wdog_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_32_63               : 32;
-	uint64_t clk_cnt                      : 32; /**< Number of SCLKs to allow for a DMA store operation to complete before hardware will halt
-                                                         the operation.
-                                                         Hardware will inject and error on the next 512-byte block boundary.   The pending DMA
+	uint64_t clk_cnt                      : 32; /**< Number of coprocessor-clocks to allow for a store operation to the device to complete
+                                                         before hardware will halt the operation.
+                                                         Hardware will inject an error on the next 512-byte block boundary.   The pending DMA
                                                          operation can be resumed or terminated. A value of zero disables timer. */
 #else
 	uint64_t clk_cnt                      : 32;
@@ -3742,7 +3742,8 @@ typedef union cvmx_mio_emm_dma_fifo_cfg cvmx_mio_emm_dma_fifo_cfg_t;
  * This register specifies a command that is loaded into the eMMC internal DMA FIFO.  The FIFO is
  * used to queue up operations for the MIO_EMM_DMA_CFG/MIO_EMM_DMA_ADR when the DMA completes
  * successfully. Writes to this register store both the MIO_EMM_DMA_FIFO_CMD and the
- * MIO_EMM_DMA_FIFO_ADR contents into the FIFO and increment the MIO_DMA_FIFO_CFG[COUNT] field.
+ * MIO_EMM_DMA_FIFO_ADR contents into the FIFO and increment the MIO_EMM_DMA_FIFO_CFG[COUNT]
+ * field.
  * Note: This register has a similar format to the MIO_EMM_DMA_CFG register with the exception
  * that the EN and CLR fields are absent. These are supported in the MIO_EMM_DMA_FIFO_CFG.
  */
@@ -3787,7 +3788,8 @@ union cvmx_mio_emm_dma_int {
 	struct cvmx_mio_emm_dma_int_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_2_63                : 62;
-	uint64_t fifo                         : 1;  /**< Internal DMA FIFO has dropped to level specified by MIO_DMA_FIFO_CFG[INT_LVL]. Throws
+	uint64_t fifo                         : 1;  /**< INIERNAL Set as reserved in HRM
+                                                         Internal DMA FIFO has dropped to level specified by MIO_EMM_DMA_FIFO_CFG[INT_LVL]. Throws
                                                          MIO_EMM_INTSN_E::MIO_EMM_DMA_FIFO. */
 	uint64_t done                         : 1;  /**< Internal DMA engine request completion interrupt. Throws
                                                          MIO_EMM_INTSN_E::MIO_EMM_DMA_DONE. */
@@ -4068,7 +4070,8 @@ union cvmx_mio_emm_rsp_sts {
                                                          SW can terminate the transfer by writing MIO_EMM_DMA[DMA_VAL]=1
                                                          and MIO_EMM_DMA[DAT_NULL]=1.   HW will clear DMA_PEND and
                                                          perform the DMA operation. */
-	uint64_t acc_timeout                  : 1;  /**< The DMA store operation took longer than MIO_EMM_ACCESS_WDOG[CLK_CNT] SCLKs to complete.
+	uint64_t acc_timeout                  : 1;  /**< The store operation to the device took longer than MIO_EMM_ACCESS_WDOG[CLK_CNT]
+                                                         coprocessor-clocks to complete.
                                                          Valid when DMA_PEND=1. */
 	uint64_t reserved_29_54               : 26;
 	uint64_t dbuf_err                     : 1;  /**< For CMD_TYPE=1, indicates a DMA read data arrived from card
diff --git a/arch/mips/include/asm/octeon/cvmx-pip.h b/arch/mips/include/asm/octeon/cvmx-pip.h
index 6805d28..1a306d4 100644
--- a/arch/mips/include/asm/octeon/cvmx-pip.h
+++ b/arch/mips/include/asm/octeon/cvmx-pip.h
@@ -42,7 +42,7 @@
  *
  * Interface to the hardware Packet Input Processing unit.
  *
- * <hr>$Revision: 96703 $<hr>
+ * <hr>$Revision: 97097 $<hr>
  */
 
 #ifndef __CVMX_PIP_H__
@@ -323,9 +323,12 @@ static inline void cvmx_pip_config_port(uint64_t ipd_port, cvmx_pip_prt_cfgx_t p
 		pki_prt_cfg.style_cfg.tag_cfg.tag_fields.input_port = port_tag_cfg.s.inc_prt_flag;
 		pki_prt_cfg.style_cfg.tag_cfg.tag_fields.first_vlan = port_tag_cfg.s.inc_vlan;
 		pki_prt_cfg.style_cfg.tag_cfg.tag_fields.second_vlan = port_tag_cfg.s.inc_vs;
-		if (port_tag_cfg.s.tcp6_tag_type || port_tag_cfg.s.tcp4_tag_type ||
-				 port_tag_cfg.s.ip6_tag_type || port_tag_cfg.s.ip4_tag_type)
-			cvmx_dprintf("Warning: 78xx use different method for tcp/ip tag type\n");
+                if ((port_tag_cfg.s.tcp6_tag_type == port_tag_cfg.s.tcp4_tag_type) &&
+                     (port_tag_cfg.s.ip6_tag_type == port_tag_cfg.s.ip4_tag_type) &&
+                     (port_tag_cfg.s.tcp6_tag_type == port_tag_cfg.s.ip6_tag_type))
+                        pki_prt_cfg.style_cfg.parm_cfg.tag_type = port_tag_cfg.s.tcp6_tag_type;
+                else
+                    cvmx_dprintf("Warning: 78xx use different method for tcp/ip tag type\n");
 		if (port_tag_cfg.s.grp)
 			cvmx_dprintf("Warning: 78xx use different method for sso group scheduling\n");
 		if (port_tag_cfg.s.portadd_en)
@@ -339,9 +342,8 @@ static inline void cvmx_pip_config_port(uint64_t ipd_port, cvmx_pip_prt_cfgx_t p
 
 		/* This is only for backward compatibility, not all the parameters are supported in 78xx */
 		cvmx_pki_config_port(ipd_port, &pki_prt_cfg);
-	}
-
-	if (octeon_has_feature(OCTEON_FEATURE_PKND)) {
+	} else {
+	   if (octeon_has_feature(OCTEON_FEATURE_PKND)) {
 		int interface, index, pknd;
 
 		interface = cvmx_helper_get_interface_num(ipd_port);
@@ -349,9 +351,10 @@ static inline void cvmx_pip_config_port(uint64_t ipd_port, cvmx_pip_prt_cfgx_t p
 		pknd = cvmx_helper_get_pknd(interface, index);
 
 		ipd_port = pknd;	/* overload port_num with pknd */
-	}
-	cvmx_write_csr(CVMX_PIP_PRT_CFGX(ipd_port), port_cfg.u64);
-	cvmx_write_csr(CVMX_PIP_PRT_TAGX(ipd_port), port_tag_cfg.u64);
+	   }
+	   cvmx_write_csr(CVMX_PIP_PRT_CFGX(ipd_port), port_cfg.u64);
+	   cvmx_write_csr(CVMX_PIP_PRT_TAGX(ipd_port), port_tag_cfg.u64);
+        }
 }
 
 /**
diff --git a/arch/mips/include/asm/octeon/cvmx-pki-cluster.h b/arch/mips/include/asm/octeon/cvmx-pki-cluster.h
index d18b38a..a1ee9c0 100644
--- a/arch/mips/include/asm/octeon/cvmx-pki-cluster.h
+++ b/arch/mips/include/asm/octeon/cvmx-pki-cluster.h
@@ -1,11 +1,11 @@
 /* This file is autgenerated from obj/ipemainc.elf */
-const int cvmx_pki_cluster_code_length = 653;
+const int cvmx_pki_cluster_code_length = 655;
 const uint64_t cvmx_pki_cluster_code_default[] = {
     0x000000000a000000ull,
     0x0000413a68024070ull,
     0x0000813800200020ull,
     0x900081b800200020ull,
-    0x0004dd80ffff0001ull,
+    0x0004da00ffff0001ull,
     0x000455ab68010b0eull,
     0x00045fba46010000ull,
     0x9046898120002000ull,
@@ -13,8 +13,8 @@ const uint64_t cvmx_pki_cluster_code_default[] = {
     0x90665300680100f0ull,
     0x0004413f68004070ull,
     0x00065380680100f0ull,
-    0x00045dbb6803a0f0ull,
-    0x000401bb48000001ull,
+    0x00045a346803a0f0ull,
+    0x000401b448000001ull,
     0x00045cb968030870ull,
     0x0007debd00100010ull,
     0x0000813b80008000ull,
@@ -23,16 +23,13 @@ const uint64_t cvmx_pki_cluster_code_default[] = {
     0x9021c00000000000ull,
     0x00044180680100f0ull,
     0x0004c639ff000200ull,
-    0x0004400172030000ull,
+    0x0004400372010000ull,
     0x0001c00000000000ull,
     0x0001c00000000000ull,
     0x000041ba68034078ull,
     0x0000512268030870ull,
     0x000041bc68034070ull,
     0x00005d3a68030870ull,
-    0x000483891f000000ull,
-    0x000f542868090a48ull,
-    0x000f583068020070ull,
     0x00045cb942080000ull,
     0x0004552a4e09312dull,
     0x00045cb968082868ull,
@@ -235,11 +232,12 @@ const uint64_t cvmx_pki_cluster_code_default[] = {
     0x0006410246090000ull,
     0x9000813902000000ull,
     0x000481b800400040ull,
-    0x00004180680100f0ull,
     0x00068981ffff8847ull,
     0x00068581ffff8848ull,
     0x0006debd00080008ull,
-    0x9806c639ff001e00ull,
+    0x0006c639ff001e00ull,
+    0x0006010240000002ull,
+    0x9801c00000000000ull,
     0x9821c00000000000ull,
     0x00065f80680100f0ull,
     0x0006403f72010000ull,
@@ -252,20 +250,22 @@ const uint64_t cvmx_pki_cluster_code_default[] = {
     0x00065cb942080000ull,
     0x0006552a4e09312dull,
     0x00065cb968082868ull,
+    0x0006010240000004ull,
     0x0006823902000000ull,
-    0x0008010240000002ull,
+    0x00065f3e68010629ull,
     0xac28828101000100ull,
-    0x000b010240000002ull,
+    0x000b010240000004ull,
     0xa42b820101000100ull,
-    0x0009010240000002ull,
+    0x0009010240000004ull,
     0xac29828101000100ull,
-    0x000b010240000002ull,
+    0x000b010240000004ull,
     0xa42b820101000100ull,
-    0x0009010240000002ull,
+    0x0009010240000004ull,
     0xac29828101000100ull,
-    0x000b010240000002ull,
-    0x00065f3e68010629ull,
-    0x00040183840005ffull,
+    0x000b010240000004ull,
+    0x9000813902000000ull,
+    0x0001c00000000000ull,
+    0x00040181840005ffull,
     0x0006010240000008ull,
     0x9801c00000000000ull,
     0x0006debd00200020ull,
@@ -488,7 +488,7 @@ const uint64_t cvmx_pki_cluster_code_default[] = {
     0x00088281ffff0000ull,
     0x000615ab74000664ull,
     0x000a15ab74000664ull,
-    0x000081b800100010ull,
+    0x000081b940004000ull,
     0x00045cb942080000ull,
     0x0004552a4e09312dull,
     0x00045cb968082868ull,
@@ -498,28 +498,30 @@ const uint64_t cvmx_pki_cluster_code_default[] = {
     0x000f583068020070ull,
     0x000042a486020000ull,
     0x000a15ab74000661ull,
-    0x000689b940004000ull,
-    0x000689a803e00000ull,
-    0x000641b168004078ull,
-    0x0006413840030000ull,
-    0x9801c00000000000ull,
-    0x9821c00000000000ull,
-    0x00064180680100f0ull,
-    0x0006c639ff003900ull,
-    0x0006400172030001ull,
+    0x000685a803e00000ull,
+    0x000782b800100010ull,
+    0x000a41b168004078ull,
+    0x000a410040030000ull,
+    0x000a41ba68004078ull,
+    0x000a410240030000ull,
+    0xa801c00000000000ull,
+    0xa821c00000000000ull,
+    0x000a4180680100f0ull,
+    0x000ac639ff003900ull,
+    0x000a400372010001ull,
     0x0001c00000000000ull,
     0x0001c00000000000ull,
     0x0001c00000000000ull,
     0x0001c00000000000ull,
     0x0001c00000000000ull,
     0x0001c00000000000ull,
-    0x000683891f000000ull,
+    0x000a83891f000000ull,
     0x000f542868090a48ull,
     0x000f583068020070ull,
-    0x00065cb942080000ull,
-    0x0006552a4e09312dull,
-    0x00065cb968082868ull,
-    0x0006410246090000ull,
+    0x000a5cb942080000ull,
+    0x000a552a4e09312dull,
+    0x000a5cb968082868ull,
+    0x000a410246090000ull,
     0x00005fb968004250ull,
     0x0000003f70000000ull,
     0x000041b968034070ull,
diff --git a/arch/mips/include/asm/octeon/cvmx-pki-defs.h b/arch/mips/include/asm/octeon/cvmx-pki-defs.h
index e56c157..2103dce 100644
--- a/arch/mips/include/asm/octeon/cvmx-pki-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-pki-defs.h
@@ -2502,8 +2502,8 @@ union cvmx_pki_icgx_cfg {
                                                          1 = Normal sequencer operation. */
 	uint64_t timer                        : 12; /**< Current hold-off timer. Enables even spreading of cluster utilization over time; while
                                                          TIMER is non-zero, a cluster in this group will not start parsing. When a cluster in this
-                                                         group starts parsing, TIMER is set to DELAY, and decrements every sclk cycle. TIMER is
-                                                         zeroed if all clusters in this group are idle. */
+                                                         group starts parsing, TIMER is set to DELAY, and decrements every coprocessor-clock. TIMER
+                                                         is zeroed if all clusters in this group are idle. */
 	uint64_t delay                        : 12; /**< Delay between cluster starts, as described under TIMER. If zero, a cluster can start at
                                                          any time relative to other clusters. DELAY should be typically selected to minimize the
                                                          average observed parser latency by loading with the parsing delay divided by the number of
diff --git a/arch/mips/include/asm/octeon/cvmx-pki.h b/arch/mips/include/asm/octeon/cvmx-pki.h
index b8cd740..dddb27a 100644
--- a/arch/mips/include/asm/octeon/cvmx-pki.h
+++ b/arch/mips/include/asm/octeon/cvmx-pki.h
@@ -613,14 +613,6 @@ static inline int cvmx_pki_attach_cluster_to_group(int node, uint64_t cluster_gr
 	return 0;
 }
 
-static inline void cvmx_pki_write_stats_mode(int node, enum cvmx_pki_stats_mode mode)
-{
-	cvmx_pki_stat_ctl_t stat_ctl;
-	stat_ctl.u64 = cvmx_read_csr_node(node, CVMX_PKI_STAT_CTL);
-	stat_ctl.s.mode = mode;
-	cvmx_write_csr_node(node, CVMX_PKI_STAT_CTL, stat_ctl.u64);
-}
-
 static inline void cvmx_pki_write_global_parse(int node, struct cvmx_pki_global_parse gbl_pen)
 {
 	cvmx_pki_gbl_pen_t gbl_pen_reg;
@@ -838,24 +830,6 @@ static inline void cvmx_pki_get_flow_stats(int node, uint64_t style_num, struct
 }
 
 /**
- * Controls how the PKI statistics counters are handled
- * The PKI_STAT*_X registers can be indexed either by port kind (pkind), or
- * final style. (Does not apply to the PKI_STAT_INB* registers.)
- * @param node		node number
- * @param mode          mode to index counter with
- *			0 =  collect counter per packetâ€™s pkind
- *			1 =  collect counter per packetâ€™s final style
- */
-static inline void cvmx_pki_set_stats_mode(int node, enum cvmx_pki_stats_mode mode)
-{
-	cvmx_pki_stat_ctl_t stat_ctl;
-
-	stat_ctl.u64 = cvmx_read_csr_node(node, CVMX_PKI_STAT_CTL);
-	stat_ctl.s.mode = mode;
-	cvmx_write_csr_node(node, CVMX_PKI_STAT_CTL, stat_ctl.u64);
-}
-
-/**
  * This function enables pki
  * @param node	 node to enable pki in.
  */
@@ -1035,6 +1009,9 @@ void cvmx_pki_reset(int node);
 int cvmx_pki_get_pkind_style(int node, int pkind);
 void __cvmx_pki_free_ptr(int node);
 void cvmx_pki_show_qpg_entries(int node, uint16_t num_entry);
+void cvmx_pki_show_port(int node, int interface, int index);
+void cvmx_pki_write_global_cfg(int node, struct cvmx_pki_global_config *gbl_cfg);
+void cvmx_pki_read_global_cfg(int node, struct cvmx_pki_global_config *gbl_cfg);
 
 #ifdef	__cplusplus
 /* *INDENT-OFF* */
diff --git a/arch/mips/include/asm/octeon/cvmx-pko-defs.h b/arch/mips/include/asm/octeon/cvmx-pko-defs.h
index ca1927e..48be7dcd 100644
--- a/arch/mips/include/asm/octeon/cvmx-pko-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-pko-defs.h
@@ -3100,8 +3100,8 @@ union cvmx_pko_dpfi_fpa_aura {
 	uint64_t node                         : 2;  /**< Node number of current chip, to ensure that the aura is on the local node. */
 	uint64_t laura                        : 10; /**< Local aura to use for PKO command buffering. Must be on local OCI node.
                                                          The FPA aura selected by LAURA must select an FPA pool whose
-                                                         FPA_POOL(0..63)_CFG[NAT_ALIGN]=1, and
-                                                         (FPA_POOL(0..63)_CFG[BUF_SIZE] - FPA_POOL(0..63)_CFG[BUF_OFFSET]) >= 4 KB/128. */
+                                                         FPA_POOL()_CFG[NAT_ALIGN]=1, and
+                                                         (FPA_POOL()_CFG[BUF_SIZE] - FPA_POOL()_CFG[BUF_OFFSET]) >= 4 KB/128. */
 #else
 	uint64_t laura                        : 10;
 	uint64_t node                         : 2;
@@ -3163,7 +3163,7 @@ typedef union cvmx_pko_dpfi_status cvmx_pko_dpfi_status_t;
 /**
  * cvmx_pko_dq#_bytes
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_GREEN_BYTES.
+ * This register has the same bit fields as PKO_L1_SQ()_GREEN_BYTES.
  *
  */
 union cvmx_pko_dqx_bytes {
@@ -3184,7 +3184,7 @@ typedef union cvmx_pko_dqx_bytes cvmx_pko_dqx_bytes_t;
 /**
  * cvmx_pko_dq#_cir
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_CIR.
+ * This register has the same bit fields as PKO_L1_SQ()_CIR.
  *
  */
 union cvmx_pko_dqx_cir {
@@ -3226,7 +3226,7 @@ typedef union cvmx_pko_dqx_cir cvmx_pko_dqx_cir_t;
 /**
  * cvmx_pko_dq#_dropped_bytes
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_GREEN_BYTES.
+ * This register has the same bit fields as PKO_L1_SQ()_GREEN_BYTES.
  *
  */
 union cvmx_pko_dqx_dropped_bytes {
@@ -3247,7 +3247,7 @@ typedef union cvmx_pko_dqx_dropped_bytes cvmx_pko_dqx_dropped_bytes_t;
 /**
  * cvmx_pko_dq#_dropped_packets
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_GREEN_PACKETS.
+ * This register has the same bit fields as PKO_L1_SQ()_GREEN_PACKETS.
  *
  */
 union cvmx_pko_dqx_dropped_packets {
@@ -3274,8 +3274,8 @@ union cvmx_pko_dqx_fifo {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_15_63               : 49;
 	uint64_t p_con                        : 1;  /**< Reserved. */
-	uint64_t head                         : 7;  /**< See PKO_L2_SQ(0..511)_POINTERS[PREV]. */
-	uint64_t tail                         : 7;  /**< See PKO_L2_SQ(0..511)_POINTERS[NEXT]. */
+	uint64_t head                         : 7;  /**< See PKO_L2_SQ()_POINTERS[PREV]. */
+	uint64_t tail                         : 7;  /**< See PKO_L2_SQ()_POINTERS[NEXT]. */
 #else
 	uint64_t tail                         : 7;
 	uint64_t head                         : 7;
@@ -3290,7 +3290,7 @@ typedef union cvmx_pko_dqx_fifo cvmx_pko_dqx_fifo_t;
 /**
  * cvmx_pko_dq#_packets
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_GREEN_PACKETS.
+ * This register has the same bit fields as PKO_L1_SQ()_GREEN_PACKETS.
  *
  */
 union cvmx_pko_dqx_packets {
@@ -3311,7 +3311,7 @@ typedef union cvmx_pko_dqx_packets cvmx_pko_dqx_packets_t;
 /**
  * cvmx_pko_dq#_pick
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_PICK.
+ * This register has the same bit fields as PKO_L1_SQ()_PICK.
  *
  */
 union cvmx_pko_dqx_pick {
@@ -3319,7 +3319,7 @@ union cvmx_pko_dqx_pick {
 	struct cvmx_pko_dqx_pick_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t dq                           : 10; /**< Descriptor queue. Index of originating descriptor queue. */
-	uint64_t color                        : 2;  /**< See PKO_L2_SQ(0..511)_SHAPE[COLOR]. */
+	uint64_t color                        : 2;  /**< See PKO_L2_SQ()_SHAPE[COLOR]. */
 	uint64_t child                        : 10; /**< Child index. When the C_CON bit of this result is set, indicating that this result is
                                                          connected in a flow that extends through the child result, this is the index of that child
                                                          result. */
@@ -3330,10 +3330,10 @@ union cvmx_pko_dqx_pick {
 	uint64_t jump                         : 1;  /**< Jump. Set when metapacket originated from a jump descriptor. */
 	uint64_t fpd                          : 1;  /**< First packet descriptor. Set when metapacket was the first in a cacheline. */
 	uint64_t ds                           : 1;  /**< Don't send. Set when metapacket is not to be sent. */
-	uint64_t adjust                       : 9;  /**< See PKO_L2_SQ(0..511)_SHAPE[ADJUST]. */
+	uint64_t adjust                       : 9;  /**< See PKO_L2_SQ()_SHAPE[ADJUST]. */
 	uint64_t pir_dis                      : 1;  /**< PIR disable. Peak shaper disabled. */
 	uint64_t cir_dis                      : 1;  /**< CIR disable. Committed shaper disabled. */
-	uint64_t red_algo_override            : 2;  /**< See PKO_L2_SQ(0..511)_SHAPE[RED_ALGO]. */
+	uint64_t red_algo_override            : 2;  /**< See PKO_L2_SQ()_SHAPE[RED_ALGO]. */
 	uint64_t length                       : 16; /**< Packet length. The packet length in bytes. */
 #else
 	uint64_t length                       : 16;
@@ -3360,7 +3360,7 @@ typedef union cvmx_pko_dqx_pick cvmx_pko_dqx_pick_t;
 /**
  * cvmx_pko_dq#_pir
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_CIR.
+ * This register has the same bit fields as PKO_L1_SQ()_CIR.
  *
  */
 union cvmx_pko_dqx_pir {
@@ -3402,7 +3402,7 @@ typedef union cvmx_pko_dqx_pir cvmx_pko_dqx_pir_t;
 /**
  * cvmx_pko_dq#_pointers
  *
- * This register has the same bit fields as PKO_L4_SQ(0..1023)_POINTERS.
+ * This register has the same bit fields as PKO_L4_SQ()_POINTERS.
  *
  */
 union cvmx_pko_dqx_pointers {
@@ -3410,9 +3410,9 @@ union cvmx_pko_dqx_pointers {
 	struct cvmx_pko_dqx_pointers_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_26_63               : 38;
-	uint64_t prev                         : 10; /**< See PKO_L2_SQ(0..511)_POINTERS[PREV]. */
+	uint64_t prev                         : 10; /**< See PKO_L2_SQ()_POINTERS[PREV]. */
 	uint64_t reserved_10_15               : 6;
-	uint64_t next                         : 10; /**< See PKO_L2_SQ(0..511)_POINTERS[NEXT]. */
+	uint64_t next                         : 10; /**< See PKO_L2_SQ()_POINTERS[NEXT]. */
 #else
 	uint64_t next                         : 10;
 	uint64_t reserved_10_15               : 6;
@@ -3427,7 +3427,7 @@ typedef union cvmx_pko_dqx_pointers cvmx_pko_dqx_pointers_t;
 /**
  * cvmx_pko_dq#_sched_state
  *
- * This register has the same bit fields as PKO_L2_SQ(0..511)_SCHED_STATE.
+ * This register has the same bit fields as PKO_L2_SQ()_SCHED_STATE.
  *
  */
 union cvmx_pko_dqx_sched_state {
@@ -3448,7 +3448,7 @@ typedef union cvmx_pko_dqx_sched_state cvmx_pko_dqx_sched_state_t;
 /**
  * cvmx_pko_dq#_schedule
  *
- * This register has the same bit fields as PKO_L2_SQ(0..511)_SCHEDULE.
+ * This register has the same bit fields as PKO_L2_SQ()_SCHEDULE.
  *
  */
 union cvmx_pko_dqx_schedule {
@@ -3465,7 +3465,7 @@ union cvmx_pko_dqx_schedule {
 	uint64_t rr_quantum                   : 24; /**< Round-robin (DWRR) quantum. The deficit-weighted round-robin quantum (24-bit unsigned
                                                          integer).
                                                          The packet size used in all DWRR calculations is:
-                                                            (PKO_Ln_SQm_SHAPE[LENGTH_DISABLE] ? 0 : (PKO_SEND_HDR_S[TOTAL] + CALCPAD)) +
+                                                         _  (PKO_Ln_SQm_SHAPE[LENGTH_DISABLE] ? 0 : (PKO_SEND_HDR_S[TOTAL] + CALCPAD)) +
                                                             PKO_SEND_EXT_S[SHAPECHG] +
                                                             PKO_Ln_SQm_SHAPE[ADJUST]
                                                          where n and m correspond to this PKO_Ln_SQm_SCHEDULE CSR. CALCPAD is zero when
@@ -3486,7 +3486,7 @@ typedef union cvmx_pko_dqx_schedule cvmx_pko_dqx_schedule_t;
 /**
  * cvmx_pko_dq#_shape
  *
- * This register has the same bit fields as PKO_L5_SQ(0..1023)_SHAPE.
+ * This register has the same bit fields as PKO_L5_SQ()_SHAPE.
  *
  */
 union cvmx_pko_dqx_shape {
@@ -3495,13 +3495,13 @@ union cvmx_pko_dqx_shape {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_25_63               : 39;
 	uint64_t length_disable               : 1;  /**< Length disable. Disables the use of packet lengths in shaping calculations such that only
-                                                         the value of PKO_L5_SQ(0..1023)_SHAPE[ADJUST]. */
+                                                         the value of PKO_L5_SQ()_SHAPE[ADJUST]. */
 	uint64_t reserved_13_23               : 11;
 	uint64_t yellow_disable               : 1;  /**< Disable yellow transitions. Disables green-to-yellow packet color marking transitions when set. */
 	uint64_t red_disable                  : 1;  /**< Disable red transitions. Disables green-to-red and yellow-to-red packet color marking
                                                          transitions when set. */
-	uint64_t red_algo                     : 2;  /**< See PKO_L2_SQ(0..511)_SHAPE[RED_ALGO]. */
-	uint64_t adjust                       : 9;  /**< See PKO_L2_SQ(0..511)_SHAPE[ADJUST]. */
+	uint64_t red_algo                     : 2;  /**< See PKO_L2_SQ()_SHAPE[RED_ALGO]. */
+	uint64_t adjust                       : 9;  /**< See PKO_L2_SQ()_SHAPE[ADJUST]. */
 #else
 	uint64_t adjust                       : 9;
 	uint64_t red_algo                     : 2;
@@ -3519,7 +3519,7 @@ typedef union cvmx_pko_dqx_shape cvmx_pko_dqx_shape_t;
 /**
  * cvmx_pko_dq#_shape_state
  *
- * This register has the same bit fields as PKO_L2_SQ(0..511)_SHAPE_STATE.
+ * This register has the same bit fields as PKO_L2_SQ()_SHAPE_STATE.
  *
  */
 union cvmx_pko_dqx_shape_state {
@@ -3529,10 +3529,10 @@ union cvmx_pko_dqx_shape_state {
 	uint64_t reserved_60_63               : 4;
 	uint64_t tw_timestamp                 : 6;  /**< Time-wheel timestamp. Debug access to the live time-wheel timestamp. */
 	uint64_t color                        : 2;  /**< Shaper color status. Debug access to the live shaper state.
-                                                         0x0 = Green - operating in 'committed' range
-                                                         0x1 = Yellow - operating in 'excess/peak' range
-                                                         0x2 = Red - operating in 'oversubscribed' range
-                                                         0x3 = Reserved */
+                                                         0x0 = Green - operating in 'committed' range.
+                                                         0x1 = Yellow - operating in 'excess/peak' range.
+                                                         0x2 = Red - operating in 'oversubscribed' range.
+                                                         0x3 = Reserved. */
 	uint64_t pir_accum                    : 26; /**< Peak information rate accumulator. Debug access to the live PIR accumulator. */
 	uint64_t cir_accum                    : 26; /**< Committed information rate accumulator. Debug access to the live CIR accumulator. */
 #else
@@ -3550,7 +3550,7 @@ typedef union cvmx_pko_dqx_shape_state cvmx_pko_dqx_shape_state_t;
 /**
  * cvmx_pko_dq#_sw_xoff
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_SW_XOFF.
+ * This register has the same bit fields as PKO_L1_SQ()_SW_XOFF.
  *
  */
 union cvmx_pko_dqx_sw_xoff {
@@ -3566,9 +3566,9 @@ union cvmx_pko_dqx_sw_xoff {
                                                          ends at the SQ1 level. The drain path is prioritized over other paths through PSE and can
                                                          be used in combination with DRAIN_NULL_LINK and DRAIN_IRQ. */
 	uint64_t xoff                         : 1;  /**< XOFF. The PQ is disabled when XOFF is asserted. PQ is enabled when XOFF is deasserted.
-                                                         NOTE: The associated PKO_L1_SQ(0..31)_TOPOLOGY[LINK] must be configured before using this
+                                                         NOTE: The associated PKO_L1_SQ()_TOPOLOGY[LINK] must be configured before using this
                                                          register field. Writing to this register field before the associated
-                                                         PKO_L1_SQ(0..31)_TOPOLOGY[LINK] value is configured can result in modifying the software
+                                                         PKO_L1_SQ()_TOPOLOGY[LINK] value is configured can result in modifying the software
                                                          XOFF state of the wrong SQ. */
 #else
 	uint64_t xoff                         : 1;
@@ -3590,7 +3590,7 @@ union cvmx_pko_dqx_topology {
 	struct cvmx_pko_dqx_topology_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_26_63               : 38;
-	uint64_t parent                       : 10; /**< See PKO_L2_SQ(0..511)_TOPOLOGY[PARENT]. */
+	uint64_t parent                       : 10; /**< See PKO_L2_SQ()_TOPOLOGY[PARENT]. */
 	uint64_t reserved_0_15                : 16;
 #else
 	uint64_t reserved_0_15                : 16;
@@ -3611,7 +3611,7 @@ union cvmx_pko_dqx_wm_cnt {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_48_63               : 16;
 	uint64_t count                        : 48; /**< Watermark count. The running value of the watermark counter. This value is a count of
-                                                         bytes or packets as specified in PKO_DQ(0..1023)_WM_CTL[KIND]. */
+                                                         bytes or packets as specified in PKO_DQ()_WM_CTL[KIND]. */
 #else
 	uint64_t count                        : 48;
 	uint64_t reserved_48_63               : 16;
@@ -3636,7 +3636,7 @@ union cvmx_pko_dqx_wm_ctl {
                                                          generated when the specified threshold is reached or crossed. Subsequent interrupt
                                                          messages are only generated after this bit has been cleared. */
 	uint64_t threshold                    : 48; /**< Watermark Threshold. This threshold is compared to the watermark count of
-                                                         PKO_DQ(0..1023)_WM_CNT[COUNT] and an interrupt is generated when the count reaches or
+                                                         PKO_DQ()_WM_CNT[COUNT] and an interrupt is generated when the count reaches or
                                                          crosses the threshold. */
 #else
 	uint64_t threshold                    : 48;
@@ -3660,7 +3660,7 @@ union cvmx_pko_dqx_wm_ctl_w1c {
 	uint64_t reserved_49_63               : 15;
 	uint64_t intr                         : 1;  /**< Interrupt. The interrupt bit is asserted and an interrupt message to the CIU is generated
                                                          when the specified threshold is crossed. Subsequent interrupt messages are only generated
-                                                         after this bit has been cleared by writing 1. Throws PKO_INTSN_E::PKO_DQ(0..1023)_WM. */
+                                                         after this bit has been cleared by writing 1. Throws PKO_INTSN_E::PKO_DQ()_WM. */
 	uint64_t reserved_0_47                : 48;
 #else
 	uint64_t reserved_0_47                : 48;
@@ -3752,7 +3752,7 @@ union cvmx_pko_formatx_ctl {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_28_63               : 36;
 	uint64_t ip4_ck                       : 1;  /**< IPv4 header checksum recalculate */
-	uint64_t offset                       : 11; /**< Bits to add to PKO_SEND_EXT[MARKPTR]*8 to determine where to start marking. */
+	uint64_t offset                       : 11; /**< Bits to add to PKO_SEND_EXT_S[MARKPTR]*8 to determine where to start marking. */
 	uint64_t y_mask                       : 4;  /**< Yellow mark mask. Corresponding bits in packet's data are cleared when marking packet yellow. */
 	uint64_t y_val                        : 4;  /**< Yellow mark value. Corresponding bits in packet's data are set when marking packet yellow. */
 	uint64_t r_mask                       : 4;  /**< Red mark mask. Corresponding bits in packet's data are cleared when marking packet red. */
@@ -3813,7 +3813,7 @@ typedef union cvmx_pko_l1_sqx_cir cvmx_pko_l1_sqx_cir_t;
 /**
  * cvmx_pko_l1_sq#_dropped_bytes
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_GREEN_BYTES.
+ * This register has the same bit fields as PKO_L1_SQ()_GREEN_BYTES.
  *
  */
 union cvmx_pko_l1_sqx_dropped_bytes {
@@ -3834,7 +3834,7 @@ typedef union cvmx_pko_l1_sqx_dropped_bytes cvmx_pko_l1_sqx_dropped_bytes_t;
 /**
  * cvmx_pko_l1_sq#_dropped_packets
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_GREEN_PACKETS.
+ * This register has the same bit fields as PKO_L1_SQ()_GREEN_PACKETS.
  *
  */
 union cvmx_pko_l1_sqx_dropped_packets {
@@ -3925,7 +3925,7 @@ union cvmx_pko_l1_sqx_link {
 	struct cvmx_pko_l1_sqx_link_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_49_63               : 15;
-	uint64_t link                         : 5;  /**< Link index. Must match PKO_L1_SQ(0..31)_TOPOLOGY[LINK]. */
+	uint64_t link                         : 5;  /**< Link index. Must match PKO_L1_SQ()_TOPOLOGY[LINK]. */
 	uint64_t reserved_32_43               : 12;
 	uint64_t cc_word_cnt                  : 20; /**< Channel credit word count. This value, plus 1 MTU, represents the maximum outstanding
                                                          aggregate word count (words are 16 bytes) for all channels feeding into this PQ. Note that
@@ -3963,7 +3963,7 @@ union cvmx_pko_l1_sqx_pick {
 	struct cvmx_pko_l1_sqx_pick_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t dq                           : 10; /**< Descriptor queue. Index of originating descriptor queue. */
-	uint64_t color                        : 2;  /**< See PKO_L2_SQ(0..511)_SHAPE[COLOR]. */
+	uint64_t color                        : 2;  /**< See PKO_L2_SQ()_SHAPE[COLOR]. */
 	uint64_t child                        : 10; /**< Child index. When the C_CON bit of this result is set, indicating that this result is
                                                          connected in a flow that extends through the child result, this is the index of that child
                                                          result. */
@@ -3974,10 +3974,10 @@ union cvmx_pko_l1_sqx_pick {
 	uint64_t jump                         : 1;  /**< Jump. Set when metapacket originated from a jump descriptor. */
 	uint64_t fpd                          : 1;  /**< First packet descriptor. Set when metapacket was the first in a cacheline. */
 	uint64_t ds                           : 1;  /**< Don't send. Set when metapacket is not to be sent. */
-	uint64_t adjust                       : 9;  /**< See PKO_L2_SQ(0..511)_SHAPE[ADJUST]. */
+	uint64_t adjust                       : 9;  /**< See PKO_L2_SQ()_SHAPE[ADJUST]. */
 	uint64_t pir_dis                      : 1;  /**< PIR disable. Peak shaper disabled. */
 	uint64_t cir_dis                      : 1;  /**< CIR disable. Committed shaper disabled. */
-	uint64_t red_algo_override            : 2;  /**< See PKO_L2_SQ(0..511)_SHAPE[RED_ALGO]. */
+	uint64_t red_algo_override            : 2;  /**< See PKO_L2_SQ()_SHAPE[RED_ALGO]. */
 	uint64_t length                       : 16; /**< Packet length. The packet length in bytes. */
 #else
 	uint64_t length                       : 16;
@@ -4004,7 +4004,7 @@ typedef union cvmx_pko_l1_sqx_pick cvmx_pko_l1_sqx_pick_t;
 /**
  * cvmx_pko_l1_sq#_red
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_YELLOW.
+ * This register has the same bit fields as PKO_L1_SQ()_YELLOW.
  *
  */
 union cvmx_pko_l1_sqx_red {
@@ -4029,7 +4029,7 @@ typedef union cvmx_pko_l1_sqx_red cvmx_pko_l1_sqx_red_t;
 /**
  * cvmx_pko_l1_sq#_red_bytes
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_GREEN_BYTES.
+ * This register has the same bit fields as PKO_L1_SQ()_GREEN_BYTES.
  *
  */
 union cvmx_pko_l1_sqx_red_bytes {
@@ -4050,7 +4050,7 @@ typedef union cvmx_pko_l1_sqx_red_bytes cvmx_pko_l1_sqx_red_bytes_t;
 /**
  * cvmx_pko_l1_sq#_red_packets
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_GREEN_PACKETS.
+ * This register has the same bit fields as PKO_L1_SQ()_GREEN_PACKETS.
  *
  */
 union cvmx_pko_l1_sqx_red_packets {
@@ -4094,7 +4094,7 @@ union cvmx_pko_l1_sqx_shape {
 	struct cvmx_pko_l1_sqx_shape_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_18_63               : 46;
-	uint64_t link                         : 5;  /**< Link index. Must match PKO_L1_SQ(0..31)_TOPOLOGY[LINK]. */
+	uint64_t link                         : 5;  /**< Link index. Must match PKO_L1_SQ()_TOPOLOGY[LINK]. */
 	uint64_t reserved_0_12                : 13;
 #else
 	uint64_t reserved_0_12                : 13;
@@ -4117,8 +4117,8 @@ union cvmx_pko_l1_sqx_shape_state {
 	uint64_t tw_timestamp                 : 6;  /**< Time-wheel timestamp. Debug access to the live time-wheel timestamp. */
 	uint64_t reserved_53_53               : 1;
 	uint64_t color                        : 1;  /**< Shaper color status. Debug access to the live shaper state.
-                                                         0x0 = Green - operating in 'committed' range
-                                                         0x1 = Red - operating in 'oversubscribed' range or inactive */
+                                                         0 = Green - operating in 'committed' range.
+                                                         1 = Red - operating in 'oversubscribed' range or inactive. */
 	uint64_t reserved_26_51               : 26;
 	uint64_t cir_accum                    : 26; /**< Committed information rate accumulator. Debug access to the live CIR accumulator. */
 #else
@@ -4150,9 +4150,9 @@ union cvmx_pko_l1_sqx_sw_xoff {
                                                          ends at the SQ1 level. The drain path is prioritized over other paths through PSE and can
                                                          be used in combination with DRAIN_NULL_LINK and DRAIN_IRQ. */
 	uint64_t xoff                         : 1;  /**< XOFF. The PQ is disabled when XOFF is asserted. PQ is enabled when XOFF is deasserted.
-                                                         NOTE: The associated PKO_L1_SQ(0..31)_TOPOLOGY[LINK] must be configured before using this
+                                                         NOTE: The associated PKO_L1_SQ()_TOPOLOGY[LINK] must be configured before using this
                                                          register field. Writing to this register field before the associated
-                                                         PKO_L1_SQ(0..31)_TOPOLOGY[LINK] value is configured can result in modifying the software
+                                                         PKO_L1_SQ()_TOPOLOGY[LINK] value is configured can result in modifying the software
                                                          XOFF state of the wrong SQ. */
 #else
 	uint64_t xoff                         : 1;
@@ -4236,7 +4236,7 @@ typedef union cvmx_pko_l1_sqx_yellow cvmx_pko_l1_sqx_yellow_t;
 /**
  * cvmx_pko_l1_sq#_yellow_bytes
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_GREEN_BYTES.
+ * This register has the same bit fields as PKO_L1_SQ()_GREEN_BYTES.
  *
  */
 union cvmx_pko_l1_sqx_yellow_bytes {
@@ -4257,7 +4257,7 @@ typedef union cvmx_pko_l1_sqx_yellow_bytes cvmx_pko_l1_sqx_yellow_bytes_t;
 /**
  * cvmx_pko_l1_sq#_yellow_packets
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_GREEN_PACKETS.
+ * This register has the same bit fields as PKO_L1_SQ()_GREEN_PACKETS.
  *
  */
 union cvmx_pko_l1_sqx_yellow_packets {
@@ -4332,7 +4332,7 @@ typedef union cvmx_pko_l1_sqb_debug cvmx_pko_l1_sqb_debug_t;
 /**
  * cvmx_pko_l2_sq#_cir
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_CIR.
+ * This register has the same bit fields as PKO_L1_SQ()_CIR.
  *
  */
 union cvmx_pko_l2_sqx_cir {
@@ -4374,7 +4374,7 @@ typedef union cvmx_pko_l2_sqx_cir cvmx_pko_l2_sqx_cir_t;
 /**
  * cvmx_pko_l2_sq#_green
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_GREEN.
+ * This register has the same bit fields as PKO_L1_SQ()_GREEN.
  *
  */
 union cvmx_pko_l2_sqx_green {
@@ -4406,7 +4406,7 @@ typedef union cvmx_pko_l2_sqx_green cvmx_pko_l2_sqx_green_t;
 /**
  * cvmx_pko_l2_sq#_pick
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_PICK.
+ * This register has the same bit fields as PKO_L1_SQ()_PICK.
  *
  */
 union cvmx_pko_l2_sqx_pick {
@@ -4414,7 +4414,7 @@ union cvmx_pko_l2_sqx_pick {
 	struct cvmx_pko_l2_sqx_pick_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t dq                           : 10; /**< Descriptor queue. Index of originating descriptor queue. */
-	uint64_t color                        : 2;  /**< See PKO_L2_SQ(0..511)_SHAPE[COLOR]. */
+	uint64_t color                        : 2;  /**< See PKO_L2_SQ()_SHAPE[COLOR]. */
 	uint64_t child                        : 10; /**< Child index. When the C_CON bit of this result is set, indicating that this result is
                                                          connected in a flow that extends through the child result, this is the index of that child
                                                          result. */
@@ -4425,10 +4425,10 @@ union cvmx_pko_l2_sqx_pick {
 	uint64_t jump                         : 1;  /**< Jump. Set when metapacket originated from a jump descriptor. */
 	uint64_t fpd                          : 1;  /**< First packet descriptor. Set when metapacket was the first in a cacheline. */
 	uint64_t ds                           : 1;  /**< Don't send. Set when metapacket is not to be sent. */
-	uint64_t adjust                       : 9;  /**< See PKO_L2_SQ(0..511)_SHAPE[ADJUST]. */
+	uint64_t adjust                       : 9;  /**< See PKO_L2_SQ()_SHAPE[ADJUST]. */
 	uint64_t pir_dis                      : 1;  /**< PIR disable. Peak shaper disabled. */
 	uint64_t cir_dis                      : 1;  /**< CIR disable. Committed shaper disabled. */
-	uint64_t red_algo_override            : 2;  /**< See PKO_L2_SQ(0..511)_SHAPE[RED_ALGO]. */
+	uint64_t red_algo_override            : 2;  /**< See PKO_L2_SQ()_SHAPE[RED_ALGO]. */
 	uint64_t length                       : 16; /**< Packet length. The packet length in bytes. */
 #else
 	uint64_t length                       : 16;
@@ -4455,7 +4455,7 @@ typedef union cvmx_pko_l2_sqx_pick cvmx_pko_l2_sqx_pick_t;
 /**
  * cvmx_pko_l2_sq#_pir
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_CIR.
+ * This register has the same bit fields as PKO_L1_SQ()_CIR.
  *
  */
 union cvmx_pko_l2_sqx_pir {
@@ -4519,7 +4519,7 @@ typedef union cvmx_pko_l2_sqx_pointers cvmx_pko_l2_sqx_pointers_t;
 /**
  * cvmx_pko_l2_sq#_red
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_RED.
+ * This register has the same bit fields as PKO_L1_SQ()_RED.
  *
  */
 union cvmx_pko_l2_sqx_red {
@@ -4576,7 +4576,7 @@ union cvmx_pko_l2_sqx_schedule {
 	uint64_t rr_quantum                   : 24; /**< Round-robin (DWRR) quantum. The deficit-weighted round-robin quantum (24-bit unsigned
                                                          integer).
                                                          The packet size used in all DWRR calculations is:
-                                                            (PKO_Ln_SQm_SHAPE[LENGTH_DISABLE] ? 0 : (PKO_SEND_HDR_S[TOTAL] + CALCPAD)) +
+                                                         _  (PKO_Ln_SQm_SHAPE[LENGTH_DISABLE] ? 0 : (PKO_SEND_HDR_S[TOTAL] + CALCPAD)) +
                                                             PKO_SEND_EXT_S[SHAPECHG] +
                                                             PKO_Ln_SQm_SHAPE[ADJUST]
                                                          where n and m correspond to this PKO_Ln_SQm_SCHEDULE CSR. CALCPAD is zero when
@@ -4603,7 +4603,7 @@ union cvmx_pko_l2_sqx_shape {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_25_63               : 39;
 	uint64_t length_disable               : 1;  /**< Length disable. Disables the use of packet lengths in shaping calculations such that only
-                                                         the values of PKO_L2_SQ(0..511)_SHAPE[ADJUST] and PKO_SEND_EXT_S[SHAPECHG] are used. */
+                                                         the values of PKO_L2_SQ()_SHAPE[ADJUST] and PKO_SEND_EXT_S[SHAPECHG] are used. */
 	uint64_t reserved_13_23               : 11;
 	uint64_t yellow_disable               : 1;  /**< Disable yellow transitions. Disables green-to-yellow packet color marking transitions when set. */
 	uint64_t red_disable                  : 1;  /**< Disable red transitions. Disables green-to-red and yellow-to-red packet color marking
@@ -4640,10 +4640,10 @@ union cvmx_pko_l2_sqx_shape_state {
 	uint64_t reserved_60_63               : 4;
 	uint64_t tw_timestamp                 : 6;  /**< Time-wheel timestamp. Debug access to the live time-wheel timestamp. */
 	uint64_t color                        : 2;  /**< Shaper color status. Debug access to the live shaper state.
-                                                         0x0 = Green - operating in 'committed' range
-                                                         0x1 = Yellow - operating in 'excess/peak' range
-                                                         0x2 = Red - operating in 'oversubscribed' range
-                                                         0x3 = Reserved */
+                                                         0x0 = Green - operating in 'committed' range.
+                                                         0x1 = Yellow - operating in 'excess/peak' range.
+                                                         0x2 = Red - operating in 'oversubscribed' range.
+                                                         0x3 = Reserved. */
 	uint64_t pir_accum                    : 26; /**< Peak information rate accumulator. Debug access to the live PIR accumulator. */
 	uint64_t cir_accum                    : 26; /**< Committed information rate accumulator. Debug access to the live CIR accumulator. */
 #else
@@ -4661,7 +4661,7 @@ typedef union cvmx_pko_l2_sqx_shape_state cvmx_pko_l2_sqx_shape_state_t;
 /**
  * cvmx_pko_l2_sq#_sw_xoff
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_SW_XOFF.
+ * This register has the same bit fields as PKO_L1_SQ()_SW_XOFF.
  *
  */
 union cvmx_pko_l2_sqx_sw_xoff {
@@ -4677,9 +4677,9 @@ union cvmx_pko_l2_sqx_sw_xoff {
                                                          ends at the SQ1 level. The drain path is prioritized over other paths through PSE and can
                                                          be used in combination with DRAIN_NULL_LINK and DRAIN_IRQ. */
 	uint64_t xoff                         : 1;  /**< XOFF. The PQ is disabled when XOFF is asserted. PQ is enabled when XOFF is deasserted.
-                                                         NOTE: The associated PKO_L1_SQ(0..31)_TOPOLOGY[LINK] must be configured before using this
+                                                         NOTE: The associated PKO_L1_SQ()_TOPOLOGY[LINK] must be configured before using this
                                                          register field. Writing to this register field before the associated
-                                                         PKO_L1_SQ(0..31)_TOPOLOGY[LINK] value is configured can result in modifying the software
+                                                         PKO_L1_SQ()_TOPOLOGY[LINK] value is configured can result in modifying the software
                                                          XOFF state of the wrong SQ. */
 #else
 	uint64_t xoff                         : 1;
@@ -4701,7 +4701,7 @@ union cvmx_pko_l2_sqx_topology {
 	struct cvmx_pko_l2_sqx_topology_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_41_63               : 23;
-	uint64_t prio_anchor                  : 9;  /**< See PKO_L1_SQ(0..31)_TOPOLOGY[PRIO_ANCHOR]. */
+	uint64_t prio_anchor                  : 9;  /**< See PKO_L1_SQ()_TOPOLOGY[PRIO_ANCHOR]. */
 	uint64_t reserved_21_31               : 11;
 	uint64_t parent                       : 5;  /**< Parent queue index. The index of the shaping element at the next lower hierarchical level
                                                          that accepts this shaping element's outputs. Refer to the PKO_*_SQn_TOPOLOGY
@@ -4709,7 +4709,7 @@ union cvmx_pko_l2_sqx_topology {
                                                          which shapers at the next lower level. When this shaper is unused, we recommend that
                                                          PARENT be zero. */
 	uint64_t reserved_5_15                : 11;
-	uint64_t rr_prio                      : 4;  /**< See PKO_L1_SQ(0..31)_TOPOLOGY[RR_PRIO]. */
+	uint64_t rr_prio                      : 4;  /**< See PKO_L1_SQ()_TOPOLOGY[RR_PRIO]. */
 	uint64_t reserved_0_0                 : 1;
 #else
 	uint64_t reserved_0_0                 : 1;
@@ -4728,7 +4728,7 @@ typedef union cvmx_pko_l2_sqx_topology cvmx_pko_l2_sqx_topology_t;
 /**
  * cvmx_pko_l2_sq#_yellow
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_YELLOW.
+ * This register has the same bit fields as PKO_L1_SQ()_YELLOW.
  *
  */
 union cvmx_pko_l2_sqx_yellow {
@@ -4843,7 +4843,7 @@ typedef union cvmx_pko_l3_l2_sqx_channel cvmx_pko_l3_l2_sqx_channel_t;
 /**
  * cvmx_pko_l3_sq#_cir
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_CIR.
+ * This register has the same bit fields as PKO_L1_SQ()_CIR.
  *
  */
 union cvmx_pko_l3_sqx_cir {
@@ -4910,7 +4910,7 @@ typedef union cvmx_pko_l3_sqx_green cvmx_pko_l3_sqx_green_t;
 /**
  * cvmx_pko_l3_sq#_pick
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_PICK.
+ * This register has the same bit fields as PKO_L1_SQ()_PICK.
  *
  */
 union cvmx_pko_l3_sqx_pick {
@@ -4918,7 +4918,7 @@ union cvmx_pko_l3_sqx_pick {
 	struct cvmx_pko_l3_sqx_pick_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t dq                           : 10; /**< Descriptor queue. Index of originating descriptor queue. */
-	uint64_t color                        : 2;  /**< See PKO_L2_SQ(0..511)_SHAPE[COLOR]. */
+	uint64_t color                        : 2;  /**< See PKO_L2_SQ()_SHAPE[COLOR]. */
 	uint64_t child                        : 10; /**< Child index. When the C_CON bit of this result is set, indicating that this result is
                                                          connected in a flow that extends through the child result, this is the index of that child
                                                          result. */
@@ -4929,10 +4929,10 @@ union cvmx_pko_l3_sqx_pick {
 	uint64_t jump                         : 1;  /**< Jump. Set when metapacket originated from a jump descriptor. */
 	uint64_t fpd                          : 1;  /**< First packet descriptor. Set when metapacket was the first in a cacheline. */
 	uint64_t ds                           : 1;  /**< Don't send. Set when metapacket is not to be sent. */
-	uint64_t adjust                       : 9;  /**< See PKO_L2_SQ(0..511)_SHAPE[ADJUST]. */
+	uint64_t adjust                       : 9;  /**< See PKO_L2_SQ()_SHAPE[ADJUST]. */
 	uint64_t pir_dis                      : 1;  /**< PIR disable. Peak shaper disabled. */
 	uint64_t cir_dis                      : 1;  /**< CIR disable. Committed shaper disabled. */
-	uint64_t red_algo_override            : 2;  /**< See PKO_L2_SQ(0..511)_SHAPE[RED_ALGO]. */
+	uint64_t red_algo_override            : 2;  /**< See PKO_L2_SQ()_SHAPE[RED_ALGO]. */
 	uint64_t length                       : 16; /**< Packet length. The packet length in bytes. */
 #else
 	uint64_t length                       : 16;
@@ -4959,7 +4959,7 @@ typedef union cvmx_pko_l3_sqx_pick cvmx_pko_l3_sqx_pick_t;
 /**
  * cvmx_pko_l3_sq#_pir
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_CIR.
+ * This register has the same bit fields as PKO_L1_SQ()_CIR.
  *
  */
 union cvmx_pko_l3_sqx_pir {
@@ -5001,7 +5001,7 @@ typedef union cvmx_pko_l3_sqx_pir cvmx_pko_l3_sqx_pir_t;
 /**
  * cvmx_pko_l3_sq#_pointers
  *
- * This register has the same bit fields as PKO_L2_SQ(0..511)_POINTERS.
+ * This register has the same bit fields as PKO_L2_SQ()_POINTERS.
  *
  */
 union cvmx_pko_l3_sqx_pointers {
@@ -5026,7 +5026,7 @@ typedef union cvmx_pko_l3_sqx_pointers cvmx_pko_l3_sqx_pointers_t;
 /**
  * cvmx_pko_l3_sq#_red
  *
- * This register has the same bit fields as PKO_L3_SQ(0..511)_YELLOW.
+ * This register has the same bit fields as PKO_L3_SQ()_YELLOW.
  *
  */
 union cvmx_pko_l3_sqx_red {
@@ -5049,7 +5049,7 @@ typedef union cvmx_pko_l3_sqx_red cvmx_pko_l3_sqx_red_t;
 /**
  * cvmx_pko_l3_sq#_sched_state
  *
- * This register has the same bit fields as PKO_L2_SQ(0..511)_SCHED_STATE.
+ * This register has the same bit fields as PKO_L2_SQ()_SCHED_STATE.
  *
  */
 union cvmx_pko_l3_sqx_sched_state {
@@ -5070,7 +5070,7 @@ typedef union cvmx_pko_l3_sqx_sched_state cvmx_pko_l3_sqx_sched_state_t;
 /**
  * cvmx_pko_l3_sq#_schedule
  *
- * This register has the same bit fields as PKO_L2_SQ(0..511)_SCHEDULE.
+ * This register has the same bit fields as PKO_L2_SQ()_SCHEDULE.
  *
  */
 union cvmx_pko_l3_sqx_schedule {
@@ -5087,7 +5087,7 @@ union cvmx_pko_l3_sqx_schedule {
 	uint64_t rr_quantum                   : 24; /**< Round-robin (DWRR) quantum. The deficit-weighted round-robin quantum (24-bit unsigned
                                                          integer).
                                                          The packet size used in all DWRR calculations is:
-                                                            (PKO_Ln_SQm_SHAPE[LENGTH_DISABLE] ? 0 : (PKO_SEND_HDR_S[TOTAL] + CALCPAD)) +
+                                                         _  (PKO_Ln_SQm_SHAPE[LENGTH_DISABLE] ? 0 : (PKO_SEND_HDR_S[TOTAL] + CALCPAD)) +
                                                             PKO_SEND_EXT_S[SHAPECHG] +
                                                             PKO_Ln_SQm_SHAPE[ADJUST]
                                                          where n and m correspond to this PKO_Ln_SQm_SCHEDULE CSR. CALCPAD is zero when
@@ -5119,8 +5119,8 @@ union cvmx_pko_l3_sqx_shape {
 	uint64_t yellow_disable               : 1;  /**< Disable yellow transitions. Disables green-to-yellow packet color marking transitions when set. */
 	uint64_t red_disable                  : 1;  /**< Disable red transitions. Disables green-to-red and yellow-to-red packet color marking
                                                          transitions when set. */
-	uint64_t red_algo                     : 2;  /**< See PKO_L2_SQ(0..511)_SHAPE[RED_ALGO]. */
-	uint64_t adjust                       : 9;  /**< See PKO_L2_SQ(0..511)_SHAPE[ADJUST]. */
+	uint64_t red_algo                     : 2;  /**< See PKO_L2_SQ()_SHAPE[RED_ALGO]. */
+	uint64_t adjust                       : 9;  /**< See PKO_L2_SQ()_SHAPE[ADJUST]. */
 #else
 	uint64_t adjust                       : 9;
 	uint64_t red_algo                     : 2;
@@ -5138,7 +5138,7 @@ typedef union cvmx_pko_l3_sqx_shape cvmx_pko_l3_sqx_shape_t;
 /**
  * cvmx_pko_l3_sq#_shape_state
  *
- * This register has the same bit fields as PKO_L2_SQ(0..511)_SHAPE_STATE.
+ * This register has the same bit fields as PKO_L2_SQ()_SHAPE_STATE.
  *
  */
 union cvmx_pko_l3_sqx_shape_state {
@@ -5148,10 +5148,10 @@ union cvmx_pko_l3_sqx_shape_state {
 	uint64_t reserved_60_63               : 4;
 	uint64_t tw_timestamp                 : 6;  /**< Time-wheel timestamp. Debug access to the live time-wheel timestamp. */
 	uint64_t color                        : 2;  /**< Shaper color status. Debug access to the live shaper state.
-                                                         0x0 = Green - operating in 'committed' range
-                                                         0x1 = Yellow - operating in 'excess/peak' range
-                                                         0x2 = Red - operating in 'oversubscribed' range
-                                                         0x3 = Reserved */
+                                                         0x0 = Green - operating in 'committed' range.
+                                                         0x1 = Yellow - operating in 'excess/peak' range.
+                                                         0x2 = Red - operating in 'oversubscribed' range.
+                                                         0x3 = Reserved. */
 	uint64_t pir_accum                    : 26; /**< Peak information rate accumulator. Debug access to the live PIR accumulator. */
 	uint64_t cir_accum                    : 26; /**< Committed information rate accumulator. Debug access to the live CIR accumulator. */
 #else
@@ -5169,7 +5169,7 @@ typedef union cvmx_pko_l3_sqx_shape_state cvmx_pko_l3_sqx_shape_state_t;
 /**
  * cvmx_pko_l3_sq#_sw_xoff
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_SW_XOFF
+ * This register has the same bit fields as PKO_L1_SQ()_SW_XOFF
  *
  */
 union cvmx_pko_l3_sqx_sw_xoff {
@@ -5185,9 +5185,9 @@ union cvmx_pko_l3_sqx_sw_xoff {
                                                          ends at the SQ1 level. The drain path is prioritized over other paths through PSE and can
                                                          be used in combination with DRAIN_NULL_LINK and DRAIN_IRQ. */
 	uint64_t xoff                         : 1;  /**< XOFF. The PQ is disabled when XOFF is asserted. PQ is enabled when XOFF is deasserted.
-                                                         NOTE: The associated PKO_L1_SQ(0..31)_TOPOLOGY[LINK] must be configured before using this
+                                                         NOTE: The associated PKO_L1_SQ()_TOPOLOGY[LINK] must be configured before using this
                                                          register field. Writing to this register field before the associated
-                                                         PKO_L1_SQ(0..31)_TOPOLOGY[LINK] value is configured can result in modifying the software
+                                                         PKO_L1_SQ()_TOPOLOGY[LINK] value is configured can result in modifying the software
                                                          XOFF state of the wrong SQ. */
 #else
 	uint64_t xoff                         : 1;
@@ -5209,11 +5209,11 @@ union cvmx_pko_l3_sqx_topology {
 	struct cvmx_pko_l3_sqx_topology_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_42_63               : 22;
-	uint64_t prio_anchor                  : 10; /**< See PKO_L1_SQ(0..31)_TOPOLOGY[PRIO_ANCHOR]. */
+	uint64_t prio_anchor                  : 10; /**< See PKO_L1_SQ()_TOPOLOGY[PRIO_ANCHOR]. */
 	uint64_t reserved_25_31               : 7;
-	uint64_t parent                       : 9;  /**< See PKO_L2_SQ(0..511)_TOPOLOGY[PARENT]. */
+	uint64_t parent                       : 9;  /**< See PKO_L2_SQ()_TOPOLOGY[PARENT]. */
 	uint64_t reserved_5_15                : 11;
-	uint64_t rr_prio                      : 4;  /**< See PKO_L1_SQ(0..31)_TOPOLOGY[RR_PRIO]. */
+	uint64_t rr_prio                      : 4;  /**< See PKO_L1_SQ()_TOPOLOGY[RR_PRIO]. */
 	uint64_t reserved_0_0                 : 1;
 #else
 	uint64_t reserved_0_0                 : 1;
@@ -5306,7 +5306,7 @@ typedef union cvmx_pko_l3_sqb_debug cvmx_pko_l3_sqb_debug_t;
 /**
  * cvmx_pko_l4_sq#_cir
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_CIR.
+ * This register has the same bit fields as PKO_L1_SQ()_CIR.
  *
  */
 union cvmx_pko_l4_sqx_cir {
@@ -5348,7 +5348,7 @@ typedef union cvmx_pko_l4_sqx_cir cvmx_pko_l4_sqx_cir_t;
 /**
  * cvmx_pko_l4_sq#_green
  *
- * This register has the same bit fields as PKO_L3_SQ(0..511)_GREEN.
+ * This register has the same bit fields as PKO_L3_SQ()_GREEN.
  *
  */
 union cvmx_pko_l4_sqx_green {
@@ -5376,7 +5376,7 @@ typedef union cvmx_pko_l4_sqx_green cvmx_pko_l4_sqx_green_t;
 /**
  * cvmx_pko_l4_sq#_pick
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_PICK.
+ * This register has the same bit fields as PKO_L1_SQ()_PICK.
  *
  */
 union cvmx_pko_l4_sqx_pick {
@@ -5384,7 +5384,7 @@ union cvmx_pko_l4_sqx_pick {
 	struct cvmx_pko_l4_sqx_pick_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t dq                           : 10; /**< Descriptor queue. Index of originating descriptor queue. */
-	uint64_t color                        : 2;  /**< See PKO_L2_SQ(0..511)_SHAPE[COLOR]. */
+	uint64_t color                        : 2;  /**< See PKO_L2_SQ()_SHAPE[COLOR]. */
 	uint64_t child                        : 10; /**< Child index. When the C_CON bit of this result is set, indicating that this result is
                                                          connected in a flow that extends through the child result, this is the index of that child
                                                          result. */
@@ -5395,10 +5395,10 @@ union cvmx_pko_l4_sqx_pick {
 	uint64_t jump                         : 1;  /**< Jump. Set when metapacket originated from a jump descriptor. */
 	uint64_t fpd                          : 1;  /**< First packet descriptor. Set when metapacket was the first in a cacheline. */
 	uint64_t ds                           : 1;  /**< Don't send. Set when metapacket is not to be sent. */
-	uint64_t adjust                       : 9;  /**< See PKO_L2_SQ(0..511)_SHAPE[ADJUST]. */
+	uint64_t adjust                       : 9;  /**< See PKO_L2_SQ()_SHAPE[ADJUST]. */
 	uint64_t pir_dis                      : 1;  /**< PIR disable. Peak shaper disabled. */
 	uint64_t cir_dis                      : 1;  /**< CIR disable. Committed shaper disabled. */
-	uint64_t red_algo_override            : 2;  /**< See PKO_L2_SQ(0..511)_SHAPE[RED_ALGO]. */
+	uint64_t red_algo_override            : 2;  /**< See PKO_L2_SQ()_SHAPE[RED_ALGO]. */
 	uint64_t length                       : 16; /**< Packet length. The packet length in bytes. */
 #else
 	uint64_t length                       : 16;
@@ -5425,7 +5425,7 @@ typedef union cvmx_pko_l4_sqx_pick cvmx_pko_l4_sqx_pick_t;
 /**
  * cvmx_pko_l4_sq#_pir
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_CIR.
+ * This register has the same bit fields as PKO_L1_SQ()_CIR.
  *
  */
 union cvmx_pko_l4_sqx_pir {
@@ -5472,9 +5472,9 @@ union cvmx_pko_l4_sqx_pointers {
 	struct cvmx_pko_l4_sqx_pointers_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_26_63               : 38;
-	uint64_t prev                         : 10; /**< See PKO_L2_SQ(0..511)_POINTERS[PREV]. */
+	uint64_t prev                         : 10; /**< See PKO_L2_SQ()_POINTERS[PREV]. */
 	uint64_t reserved_10_15               : 6;
-	uint64_t next                         : 10; /**< See PKO_L2_SQ(0..511)_POINTERS[NEXT]. */
+	uint64_t next                         : 10; /**< See PKO_L2_SQ()_POINTERS[NEXT]. */
 #else
 	uint64_t next                         : 10;
 	uint64_t reserved_10_15               : 6;
@@ -5489,7 +5489,7 @@ typedef union cvmx_pko_l4_sqx_pointers cvmx_pko_l4_sqx_pointers_t;
 /**
  * cvmx_pko_l4_sq#_red
  *
- * This register has the same bit fields as PKO_L3_SQ(0..511)_YELLOW.
+ * This register has the same bit fields as PKO_L3_SQ()_YELLOW.
  *
  */
 union cvmx_pko_l4_sqx_red {
@@ -5512,7 +5512,7 @@ typedef union cvmx_pko_l4_sqx_red cvmx_pko_l4_sqx_red_t;
 /**
  * cvmx_pko_l4_sq#_sched_state
  *
- * This register has the same bit fields as PKO_L2_SQ(0..511)_SCHED_STATE.
+ * This register has the same bit fields as PKO_L2_SQ()_SCHED_STATE.
  *
  */
 union cvmx_pko_l4_sqx_sched_state {
@@ -5533,7 +5533,7 @@ typedef union cvmx_pko_l4_sqx_sched_state cvmx_pko_l4_sqx_sched_state_t;
 /**
  * cvmx_pko_l4_sq#_schedule
  *
- * This register has the same bit fields as PKO_L2_SQ(0..511)_SCHEDULE.
+ * This register has the same bit fields as PKO_L2_SQ()_SCHEDULE.
  *
  */
 union cvmx_pko_l4_sqx_schedule {
@@ -5550,7 +5550,7 @@ union cvmx_pko_l4_sqx_schedule {
 	uint64_t rr_quantum                   : 24; /**< Round-robin (DWRR) quantum. The deficit-weighted round-robin quantum (24-bit unsigned
                                                          integer).
                                                          The packet size used in all DWRR calculations is:
-                                                            (PKO_Ln_SQm_SHAPE[LENGTH_DISABLE] ? 0 : (PKO_SEND_HDR_S[TOTAL] + CALCPAD)) +
+                                                         _  (PKO_Ln_SQm_SHAPE[LENGTH_DISABLE] ? 0 : (PKO_SEND_HDR_S[TOTAL] + CALCPAD)) +
                                                             PKO_SEND_EXT_S[SHAPECHG] +
                                                             PKO_Ln_SQm_SHAPE[ADJUST]
                                                          where n and m correspond to this PKO_Ln_SQm_SCHEDULE CSR. CALCPAD is zero when
@@ -5571,7 +5571,7 @@ typedef union cvmx_pko_l4_sqx_schedule cvmx_pko_l4_sqx_schedule_t;
 /**
  * cvmx_pko_l4_sq#_shape
  *
- * This register has the same bit fields as PKO_L3_SQ(0..511)_SHAPE.
+ * This register has the same bit fields as PKO_L3_SQ()_SHAPE.
  *
  */
 union cvmx_pko_l4_sqx_shape {
@@ -5585,8 +5585,8 @@ union cvmx_pko_l4_sqx_shape {
 	uint64_t yellow_disable               : 1;  /**< Disable yellow transitions. Disables green-to-yellow packet color marking transitions when set. */
 	uint64_t red_disable                  : 1;  /**< Disable red transitions. Disables green-to-red and yellow-to-red packet color marking
                                                          transitions when set. */
-	uint64_t red_algo                     : 2;  /**< See PKO_L2_SQ(0..511)_SHAPE[RED_ALGO]. */
-	uint64_t adjust                       : 9;  /**< See PKO_L2_SQ(0..511)_SHAPE[ADJUST]. */
+	uint64_t red_algo                     : 2;  /**< See PKO_L2_SQ()_SHAPE[RED_ALGO]. */
+	uint64_t adjust                       : 9;  /**< See PKO_L2_SQ()_SHAPE[ADJUST]. */
 #else
 	uint64_t adjust                       : 9;
 	uint64_t red_algo                     : 2;
@@ -5604,7 +5604,7 @@ typedef union cvmx_pko_l4_sqx_shape cvmx_pko_l4_sqx_shape_t;
 /**
  * cvmx_pko_l4_sq#_shape_state
  *
- * This register has the same bit fields as PKO_L2_SQ(0..511)_SHAPE_STATE.
+ * This register has the same bit fields as PKO_L2_SQ()_SHAPE_STATE.
  *
  */
 union cvmx_pko_l4_sqx_shape_state {
@@ -5614,10 +5614,10 @@ union cvmx_pko_l4_sqx_shape_state {
 	uint64_t reserved_60_63               : 4;
 	uint64_t tw_timestamp                 : 6;  /**< Time-wheel timestamp. Debug access to the live time-wheel timestamp. */
 	uint64_t color                        : 2;  /**< Shaper color status. Debug access to the live shaper state.
-                                                         0x0 = Green - operating in 'committed' range
-                                                         0x1 = Yellow - operating in 'excess/peak' range
-                                                         0x2 = Red - operating in 'oversubscribed' range
-                                                         0x3 = Reserved */
+                                                         0x0 = Green - operating in 'committed' range.
+                                                         0x1 = Yellow - operating in 'excess/peak' range.
+                                                         0x2 = Red - operating in 'oversubscribed' range.
+                                                         0x3 = Reserved. */
 	uint64_t pir_accum                    : 26; /**< Peak information rate accumulator. Debug access to the live PIR accumulator. */
 	uint64_t cir_accum                    : 26; /**< Committed information rate accumulator. Debug access to the live CIR accumulator. */
 #else
@@ -5635,7 +5635,7 @@ typedef union cvmx_pko_l4_sqx_shape_state cvmx_pko_l4_sqx_shape_state_t;
 /**
  * cvmx_pko_l4_sq#_sw_xoff
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_SW_XOFF.
+ * This register has the same bit fields as PKO_L1_SQ()_SW_XOFF.
  *
  */
 union cvmx_pko_l4_sqx_sw_xoff {
@@ -5651,9 +5651,9 @@ union cvmx_pko_l4_sqx_sw_xoff {
                                                          ends at the SQ1 level. The drain path is prioritized over other paths through PSE and can
                                                          be used in combination with DRAIN_NULL_LINK and DRAIN_IRQ. */
 	uint64_t xoff                         : 1;  /**< XOFF. The PQ is disabled when XOFF is asserted. PQ is enabled when XOFF is deasserted.
-                                                         NOTE: The associated PKO_L1_SQ(0..31)_TOPOLOGY[LINK] must be configured before using this
+                                                         NOTE: The associated PKO_L1_SQ()_TOPOLOGY[LINK] must be configured before using this
                                                          register field. Writing to this register field before the associated
-                                                         PKO_L1_SQ(0..31)_TOPOLOGY[LINK] value is configured can result in modifying the software
+                                                         PKO_L1_SQ()_TOPOLOGY[LINK] value is configured can result in modifying the software
                                                          XOFF state of the wrong SQ. */
 #else
 	uint64_t xoff                         : 1;
@@ -5675,11 +5675,11 @@ union cvmx_pko_l4_sqx_topology {
 	struct cvmx_pko_l4_sqx_topology_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_42_63               : 22;
-	uint64_t prio_anchor                  : 10; /**< See PKO_L1_SQ(0..31)_TOPOLOGY[PRIO_ANCHOR]. */
+	uint64_t prio_anchor                  : 10; /**< See PKO_L1_SQ()_TOPOLOGY[PRIO_ANCHOR]. */
 	uint64_t reserved_25_31               : 7;
-	uint64_t parent                       : 9;  /**< See PKO_L2_SQ(0..511)_TOPOLOGY[PARENT]. */
+	uint64_t parent                       : 9;  /**< See PKO_L2_SQ()_TOPOLOGY[PARENT]. */
 	uint64_t reserved_5_15                : 11;
-	uint64_t rr_prio                      : 4;  /**< See PKO_L1_SQ(0..31)_TOPOLOGY[RR_PRIO]. */
+	uint64_t rr_prio                      : 4;  /**< See PKO_L1_SQ()_TOPOLOGY[RR_PRIO]. */
 	uint64_t reserved_0_0                 : 1;
 #else
 	uint64_t reserved_0_0                 : 1;
@@ -5698,7 +5698,7 @@ typedef union cvmx_pko_l4_sqx_topology cvmx_pko_l4_sqx_topology_t;
 /**
  * cvmx_pko_l4_sq#_yellow
  *
- * This register has the same bit fields as PKO_L3_SQ(0..511)_YELLOW.
+ * This register has the same bit fields as PKO_L3_SQ()_YELLOW.
  *
  */
 union cvmx_pko_l4_sqx_yellow {
@@ -5775,7 +5775,7 @@ typedef union cvmx_pko_l4_sqb_debug cvmx_pko_l4_sqb_debug_t;
 /**
  * cvmx_pko_l5_sq#_cir
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_CIR.
+ * This register has the same bit fields as PKO_L1_SQ()_CIR.
  *
  */
 union cvmx_pko_l5_sqx_cir {
@@ -5817,7 +5817,7 @@ typedef union cvmx_pko_l5_sqx_cir cvmx_pko_l5_sqx_cir_t;
 /**
  * cvmx_pko_l5_sq#_green
  *
- * This register has the same bit fields as PKO_L3_SQ(0..511)_GREEN.
+ * This register has the same bit fields as PKO_L3_SQ()_GREEN.
  *
  */
 union cvmx_pko_l5_sqx_green {
@@ -5845,7 +5845,7 @@ typedef union cvmx_pko_l5_sqx_green cvmx_pko_l5_sqx_green_t;
 /**
  * cvmx_pko_l5_sq#_pick
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_PICK.
+ * This register has the same bit fields as PKO_L1_SQ()_PICK.
  *
  */
 union cvmx_pko_l5_sqx_pick {
@@ -5853,7 +5853,7 @@ union cvmx_pko_l5_sqx_pick {
 	struct cvmx_pko_l5_sqx_pick_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t dq                           : 10; /**< Descriptor queue. Index of originating descriptor queue. */
-	uint64_t color                        : 2;  /**< See PKO_L2_SQ(0..511)_SHAPE[COLOR]. */
+	uint64_t color                        : 2;  /**< See PKO_L2_SQ()_SHAPE[COLOR]. */
 	uint64_t child                        : 10; /**< Child index. When the C_CON bit of this result is set, indicating that this result is
                                                          connected in a flow that extends through the child result, this is the index of that child
                                                          result. */
@@ -5864,10 +5864,10 @@ union cvmx_pko_l5_sqx_pick {
 	uint64_t jump                         : 1;  /**< Jump. Set when metapacket originated from a jump descriptor. */
 	uint64_t fpd                          : 1;  /**< First packet descriptor. Set when metapacket was the first in a cacheline. */
 	uint64_t ds                           : 1;  /**< Don't send. Set when metapacket is not to be sent. */
-	uint64_t adjust                       : 9;  /**< See PKO_L2_SQ(0..511)_SHAPE[ADJUST]. */
+	uint64_t adjust                       : 9;  /**< See PKO_L2_SQ()_SHAPE[ADJUST]. */
 	uint64_t pir_dis                      : 1;  /**< PIR disable. Peak shaper disabled. */
 	uint64_t cir_dis                      : 1;  /**< CIR disable. Committed shaper disabled. */
-	uint64_t red_algo_override            : 2;  /**< See PKO_L2_SQ(0..511)_SHAPE[RED_ALGO]. */
+	uint64_t red_algo_override            : 2;  /**< See PKO_L2_SQ()_SHAPE[RED_ALGO]. */
 	uint64_t length                       : 16; /**< Packet length. The packet length in bytes. */
 #else
 	uint64_t length                       : 16;
@@ -5894,7 +5894,7 @@ typedef union cvmx_pko_l5_sqx_pick cvmx_pko_l5_sqx_pick_t;
 /**
  * cvmx_pko_l5_sq#_pir
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_CIR.
+ * This register has the same bit fields as PKO_L1_SQ()_CIR.
  *
  */
 union cvmx_pko_l5_sqx_pir {
@@ -5936,7 +5936,7 @@ typedef union cvmx_pko_l5_sqx_pir cvmx_pko_l5_sqx_pir_t;
 /**
  * cvmx_pko_l5_sq#_pointers
  *
- * This register has the same bit fields as PKO_L4_SQ(0..1023)_POINTERS.
+ * This register has the same bit fields as PKO_L4_SQ()_POINTERS.
  *
  */
 union cvmx_pko_l5_sqx_pointers {
@@ -5944,9 +5944,9 @@ union cvmx_pko_l5_sqx_pointers {
 	struct cvmx_pko_l5_sqx_pointers_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_26_63               : 38;
-	uint64_t prev                         : 10; /**< See PKO_L2_SQ(0..511)_POINTERS[PREV]. */
+	uint64_t prev                         : 10; /**< See PKO_L2_SQ()_POINTERS[PREV]. */
 	uint64_t reserved_10_15               : 6;
-	uint64_t next                         : 10; /**< See PKO_L2_SQ(0..511)_POINTERS[NEXT]. */
+	uint64_t next                         : 10; /**< See PKO_L2_SQ()_POINTERS[NEXT]. */
 #else
 	uint64_t next                         : 10;
 	uint64_t reserved_10_15               : 6;
@@ -5961,7 +5961,7 @@ typedef union cvmx_pko_l5_sqx_pointers cvmx_pko_l5_sqx_pointers_t;
 /**
  * cvmx_pko_l5_sq#_red
  *
- * This register has the same bit fields as PKO_L3_SQ(0..511)_YELLOW.
+ * This register has the same bit fields as PKO_L3_SQ()_YELLOW.
  *
  */
 union cvmx_pko_l5_sqx_red {
@@ -5984,7 +5984,7 @@ typedef union cvmx_pko_l5_sqx_red cvmx_pko_l5_sqx_red_t;
 /**
  * cvmx_pko_l5_sq#_sched_state
  *
- * This register has the same bit fields as PKO_L2_SQ(0..511)_SCHED_STATE.
+ * This register has the same bit fields as PKO_L2_SQ()_SCHED_STATE.
  *
  */
 union cvmx_pko_l5_sqx_sched_state {
@@ -6005,7 +6005,7 @@ typedef union cvmx_pko_l5_sqx_sched_state cvmx_pko_l5_sqx_sched_state_t;
 /**
  * cvmx_pko_l5_sq#_schedule
  *
- * This register has the same bit fields as PKO_L2_SQ(0..511)_SCHEDULE.
+ * This register has the same bit fields as PKO_L2_SQ()_SCHEDULE.
  *
  */
 union cvmx_pko_l5_sqx_schedule {
@@ -6022,7 +6022,7 @@ union cvmx_pko_l5_sqx_schedule {
 	uint64_t rr_quantum                   : 24; /**< Round-robin (DWRR) quantum. The deficit-weighted round-robin quantum (24-bit unsigned
                                                          integer).
                                                          The packet size used in all DWRR calculations is:
-                                                            (PKO_Ln_SQm_SHAPE[LENGTH_DISABLE] ? 0 : (PKO_SEND_HDR_S[TOTAL] + CALCPAD)) +
+                                                         _  (PKO_Ln_SQm_SHAPE[LENGTH_DISABLE] ? 0 : (PKO_SEND_HDR_S[TOTAL] + CALCPAD)) +
                                                             PKO_SEND_EXT_S[SHAPECHG] +
                                                             PKO_Ln_SQm_SHAPE[ADJUST]
                                                          where n and m correspond to this PKO_Ln_SQm_SCHEDULE CSR. CALCPAD is zero when
@@ -6049,13 +6049,13 @@ union cvmx_pko_l5_sqx_shape {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_25_63               : 39;
 	uint64_t length_disable               : 1;  /**< Length disable. Disables the use of packet lengths in shaping calculations such that only
-                                                         the value of PKO_L5_SQ(0..1023)_SHAPE[ADJUST]. */
+                                                         the value of PKO_L5_SQ()_SHAPE[ADJUST]. */
 	uint64_t reserved_13_23               : 11;
 	uint64_t yellow_disable               : 1;  /**< Disable yellow transitions. Disables green-to-yellow packet color marking transitions when set. */
 	uint64_t red_disable                  : 1;  /**< Disable red transitions. Disables green-to-red and yellow-to-red packet color marking
                                                          transitions when set. */
-	uint64_t red_algo                     : 2;  /**< See PKO_L2_SQ(0..511)_SHAPE[RED_ALGO]. */
-	uint64_t adjust                       : 9;  /**< See PKO_L2_SQ(0..511)_SHAPE[ADJUST]. */
+	uint64_t red_algo                     : 2;  /**< See PKO_L2_SQ()_SHAPE[RED_ALGO]. */
+	uint64_t adjust                       : 9;  /**< See PKO_L2_SQ()_SHAPE[ADJUST]. */
 #else
 	uint64_t adjust                       : 9;
 	uint64_t red_algo                     : 2;
@@ -6073,7 +6073,7 @@ typedef union cvmx_pko_l5_sqx_shape cvmx_pko_l5_sqx_shape_t;
 /**
  * cvmx_pko_l5_sq#_shape_state
  *
- * This register has the same bit fields as PKO_L2_SQ(0..511)_SHAPE_STATE.
+ * This register has the same bit fields as PKO_L2_SQ()_SHAPE_STATE.
  *
  */
 union cvmx_pko_l5_sqx_shape_state {
@@ -6083,10 +6083,10 @@ union cvmx_pko_l5_sqx_shape_state {
 	uint64_t reserved_60_63               : 4;
 	uint64_t tw_timestamp                 : 6;  /**< Time-wheel timestamp. Debug access to the live time-wheel timestamp. */
 	uint64_t color                        : 2;  /**< Shaper color status. Debug access to the live shaper state.
-                                                         0x0 = Green - operating in 'committed' range
-                                                         0x1 = Yellow - operating in 'excess/peak' range
-                                                         0x2 = Red - operating in 'oversubscribed' range
-                                                         0x3 = Reserved */
+                                                         0x0 = Green - operating in 'committed' range.
+                                                         0x1 = Yellow - operating in 'excess/peak' range.
+                                                         0x2 = Red - operating in 'oversubscribed' range.
+                                                         0x3 = Reserved. */
 	uint64_t pir_accum                    : 26; /**< Peak information rate accumulator. Debug access to the live PIR accumulator. */
 	uint64_t cir_accum                    : 26; /**< Committed information rate accumulator. Debug access to the live CIR accumulator. */
 #else
@@ -6104,7 +6104,7 @@ typedef union cvmx_pko_l5_sqx_shape_state cvmx_pko_l5_sqx_shape_state_t;
 /**
  * cvmx_pko_l5_sq#_sw_xoff
  *
- * This register has the same bit fields as PKO_L1_SQ(0..31)_SW_XOFF.
+ * This register has the same bit fields as PKO_L1_SQ()_SW_XOFF.
  *
  */
 union cvmx_pko_l5_sqx_sw_xoff {
@@ -6120,9 +6120,9 @@ union cvmx_pko_l5_sqx_sw_xoff {
                                                          ends at the SQ1 level. The drain path is prioritized over other paths through PSE and can
                                                          be used in combination with DRAIN_NULL_LINK and DRAIN_IRQ. */
 	uint64_t xoff                         : 1;  /**< XOFF. The PQ is disabled when XOFF is asserted. PQ is enabled when XOFF is deasserted.
-                                                         NOTE: The associated PKO_L1_SQ(0..31)_TOPOLOGY[LINK] must be configured before using this
+                                                         NOTE: The associated PKO_L1_SQ()_TOPOLOGY[LINK] must be configured before using this
                                                          register field. Writing to this register field before the associated
-                                                         PKO_L1_SQ(0..31)_TOPOLOGY[LINK] value is configured can result in modifying the software
+                                                         PKO_L1_SQ()_TOPOLOGY[LINK] value is configured can result in modifying the software
                                                          XOFF state of the wrong SQ. */
 #else
 	uint64_t xoff                         : 1;
@@ -6144,11 +6144,11 @@ union cvmx_pko_l5_sqx_topology {
 	struct cvmx_pko_l5_sqx_topology_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_42_63               : 22;
-	uint64_t prio_anchor                  : 10; /**< See PKO_L1_SQ(0..31)_TOPOLOGY[PRIO_ANCHOR]. */
+	uint64_t prio_anchor                  : 10; /**< See PKO_L1_SQ()_TOPOLOGY[PRIO_ANCHOR]. */
 	uint64_t reserved_26_31               : 6;
-	uint64_t parent                       : 10; /**< See PKO_L2_SQ(0..511)_TOPOLOGY[PARENT]. */
+	uint64_t parent                       : 10; /**< See PKO_L2_SQ()_TOPOLOGY[PARENT]. */
 	uint64_t reserved_5_15                : 11;
-	uint64_t rr_prio                      : 4;  /**< See PKO_L1_SQ(0..31)_TOPOLOGY[RR_PRIO]. */
+	uint64_t rr_prio                      : 4;  /**< See PKO_L1_SQ()_TOPOLOGY[RR_PRIO]. */
 	uint64_t reserved_0_0                 : 1;
 #else
 	uint64_t reserved_0_0                 : 1;
@@ -6167,7 +6167,7 @@ typedef union cvmx_pko_l5_sqx_topology cvmx_pko_l5_sqx_topology_t;
 /**
  * cvmx_pko_l5_sq#_yellow
  *
- * This register has the same bit fields as PKO_L3_SQ(0..511)_YELLOW.
+ * This register has the same bit fields as PKO_L3_SQ()_YELLOW.
  *
  */
 union cvmx_pko_l5_sqx_yellow {
@@ -6329,8 +6329,9 @@ union cvmx_pko_lut_ecc_dbe_sts_cmb0 {
 	uint64_t u64;
 	struct cvmx_pko_lut_ecc_dbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t lut_dbe_cmb0                 : 1;  /**< This bit is the logical OR of all bits in PKO_LUT_ECC_DBE_STS. To clear this bit, software
-                                                         must clear bits in PKO_LUT_ECC_DBE_STS. When this bit is set, the corresponding interrupt
+	uint64_t lut_dbe_cmb0                 : 1;  /**< This bit is the logical OR of all bits in PKO_LUT_ECC_DBE_STS0. To clear this bit,
+                                                         software
+                                                         must clear bits in PKO_LUT_ECC_DBE_STS0. When this bit is set, the corresponding interrupt
                                                          is set. Throws PKO_INTSN_E::PKO_LUT_DBE_CMB0. INTERNAL: Instances:
                                                          pko_pnr2.nonpse.pko_c2q_lut.pko_c2q_lut_ram_i */
 	uint64_t reserved_0_62                : 63;
@@ -6369,8 +6370,9 @@ union cvmx_pko_lut_ecc_sbe_sts_cmb0 {
 	uint64_t u64;
 	struct cvmx_pko_lut_ecc_sbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t lut_sbe_cmb0                 : 1;  /**< This bit is the logical OR of all bits in PKO_LUT_ECC_SBE_STS. To clear this bit, software
-                                                         must clear bits in PKO_LUT_ECC_SBE_STS. When this bit is set, the corresponding interrupt
+	uint64_t lut_sbe_cmb0                 : 1;  /**< This bit is the logical OR of all bits in PKO_LUT_ECC_SBE_STS0. To clear this bit,
+                                                         software
+                                                         must clear bits in PKO_LUT_ECC_SBE_STS0. When this bit is set, the corresponding interrupt
                                                          is set. Throws PKO_INTSN_E::PKO_LUT_SBE_CMB0. INTERNAL: Instances:
                                                          pko_pnr2.nonpse.pko_c2q_lut.pko_c2q_lut_ram_i */
 	uint64_t reserved_0_62                : 63;
@@ -6391,14 +6393,14 @@ union cvmx_pko_macx_cfg {
 	struct cvmx_pko_macx_cfg_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_17_63               : 47;
-	uint64_t min_pad_ena                  : 1;  /**< Minimum padding is enabled for this MAC/FIFO */
-	uint64_t fcs_ena                      : 1;  /**< Enable outside FCS for this MAC/FIFO */
+	uint64_t min_pad_ena                  : 1;  /**< Minimum padding is enabled for this MAC/FIFO. */
+	uint64_t fcs_ena                      : 1;  /**< Enable outside FCS for this MAC/FIFO. */
 	uint64_t fcs_sop_off                  : 8;  /**< FCS start of packet offset. For this MAC, the number of bytes in the front of each packet
                                                          to exclude from FCS. */
 	uint64_t skid_max_cnt                 : 2;  /**< Maximum number of SKID credits. 0x0 = 16; 0x1 = 32; 0x2 = 64. */
 	uint64_t fifo_num                     : 5;  /**< The PEB TX FIFO number assigned to the given MAC. A value of 0x1F means unassigned. Unused
                                                          MACs must be assigned a FIFO_NUM = 0x1F. For each active MAC, a unique FIFO_NUM must be
-                                                         assigned. Legal values depend on the values in PKO_PTGF(0..7)_CFG[SIZE]. Assigning the
+                                                         assigned. Legal values depend on the values in PKO_PTGF()_CFG[SIZE]. Assigning the
                                                          same FIFO_NUM to more than a single active MAC will have unpredictable results. FIFOs 0x1E
                                                          and 0x1D are invalid and will cause unpredictable results if used. */
 #else
@@ -8645,8 +8647,9 @@ union cvmx_pko_ncb_ecc_dbe_sts_cmb0 {
 	uint64_t u64;
 	struct cvmx_pko_ncb_ecc_dbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t ncb_dbe_cmb0                 : 1;  /**< This bit is the logical OR of all bits in PKO_NCB_ECC_DBE_STS. To clear this bit, software
-                                                         must clear bits in PKO_NCB_ECC_DBE_STS. When this bit is set, the corresponding interrupt
+	uint64_t ncb_dbe_cmb0                 : 1;  /**< This bit is the logical OR of all bits in PKO_NCB_ECC_DBE_STS0. To clear this bit,
+                                                         software
+                                                         must clear bits in PKO_NCB_ECC_DBE_STS0. When this bit is set, the corresponding interrupt
                                                          is set. Throws PKO_INTSN_E::PKO_NCB_DBE_CMB0. INTERNAL: Instances:
                                                          pko_pnr2.nonpse.ncb.pko_ncbi_outb.ncbi_txr.l2_out_fifo
                                                          pko_pnr2.nonpse.ncb.pko_ncbi_outb.ncbi_txr.pp_out_fifo
@@ -8701,8 +8704,9 @@ union cvmx_pko_ncb_ecc_sbe_sts_cmb0 {
 	uint64_t u64;
 	struct cvmx_pko_ncb_ecc_sbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t ncb_sbe_cmb0                 : 1;  /**< This bit is the logical OR of all bits in PKO_NCB_ECC_SBE_STS. To clear this bit, software
-                                                         must clear bits in PKO_NCB_ECC_SBE_STS. When this bit is set, the corresponding interrupt
+	uint64_t ncb_sbe_cmb0                 : 1;  /**< This bit is the logical OR of all bits in PKO_NCB_ECC_SBE_STS0. To clear this bit,
+                                                         software
+                                                         must clear bits in PKO_NCB_ECC_SBE_STS0. When this bit is set, the corresponding interrupt
                                                          is set. Throws PKO_INTSN_E::PKO_NCB_SBE_CMB0. INTERNAL: Instances:
                                                          pko_pnr2.nonpse.ncb.pko_ncbi_outb.ncbi_txr.l2_out_fifo
                                                          pko_pnr2.nonpse.ncb.pko_ncbi_outb.ncbi_txr.pp_out_fifo
@@ -8925,11 +8929,11 @@ union cvmx_pko_pdm_cp_dbg {
 	uint64_t stateless_fif_cnt            : 6;  /**< Stateless fifo count. */
 	uint64_t reserved_5_9                 : 5;
 	uint64_t op_fif_not_full              : 5;  /**< Output fifo not full signals. The order of the bits is:
-                                                         0x4 = ISR CMD FIFO not full
-                                                         0x3 = DESC DAT FIFO HIGH not full
-                                                         0x2 = DESC DAT FIFO LOW not full
-                                                         0x1 = MP DAT FIFO not full
-                                                         0x0 = PSE CMD RESP FIFO has credit */
+                                                         0x4 = ISR CMD FIFO not full.
+                                                         0x3 = DESC DAT FIFO HIGH not full.
+                                                         0x2 = DESC DAT FIFO LOW not full.
+                                                         0x1 = MP DAT FIFO not full.
+                                                         0x0 = PSE CMD RESP FIFO has credit. */
 #else
 	uint64_t op_fif_not_full              : 5;
 	uint64_t reserved_5_9                 : 5;
@@ -8981,10 +8985,10 @@ union cvmx_pko_pdm_drpbuf_dbg {
 	uint64_t reserved_17_20               : 4;
 	uint64_t mem_addr                     : 13; /**< Memory address for pbuf ram. */
 	uint64_t mem_en                       : 4;  /**< Memory write/chip enable signals. The order of the bits is:
-                                                         0x3 = low wen
-                                                         0x2 = low cen
-                                                         0x1 = high wen
-                                                         0x0 = high cen. */
+                                                         0x3 = Low wen.
+                                                         0x2 = Low cen.
+                                                         0x1 = High wen.
+                                                         0x0 = High cen. */
 #else
 	uint64_t mem_en                       : 4;
 	uint64_t mem_addr                     : 13;
@@ -9021,10 +9025,10 @@ union cvmx_pko_pdm_dwpbuf_dbg {
 	uint64_t reserved_17_20               : 4;
 	uint64_t mem_addr                     : 13; /**< Memory address for pbuf ram. */
 	uint64_t mem_en                       : 4;  /**< Memory write/chip enable signals. The order of the bits is:
-                                                         0x3 = low wen
-                                                         0x2 = low cen
-                                                         0x1 = high wen
-                                                         0x0 = high cen. */
+                                                         0x3 = Low wen.
+                                                         0x2 = Low cen.
+                                                         0x1 = High wen.
+                                                         0x0 = High cen. */
 #else
 	uint64_t mem_en                       : 4;
 	uint64_t mem_addr                     : 13;
@@ -9251,8 +9255,9 @@ union cvmx_pko_pdm_ecc_dbe_sts_cmb0 {
 	uint64_t u64;
 	struct cvmx_pko_pdm_ecc_dbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t pdm_dbe_cmb0                 : 1;  /**< This bit is the logical OR of all bits in PKO_PDM_ECC_DBE_STS. To clear this bit, software
-                                                         must clear bits in PKO_PDM_ECC_DBE_STS. When this bit is set, the corresponding interrupt
+	uint64_t pdm_dbe_cmb0                 : 1;  /**< This bit is the logical OR of all bits in PKO_PDM_ECC_DBE_STS0. To clear this bit,
+                                                         software
+                                                         must clear bits in PKO_PDM_ECC_DBE_STS0. When this bit is set, the corresponding interrupt
                                                          is set. Throws PKO_INTSN_E::PKO_PDM_DBE_CMB0. INTERNAL: Instances:
                                                          fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.flshb.flshb_cache_hi
                                                          fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.flshb.flshb_cache_lo
@@ -9373,8 +9378,9 @@ union cvmx_pko_pdm_ecc_sbe_sts_cmb0 {
 	uint64_t u64;
 	struct cvmx_pko_pdm_ecc_sbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t pdm_sbe_cmb0                 : 1;  /**< This bit is the logical OR of all bits in PKO_PDM_ECC_SBE_STS. To clear this bit, software
-                                                         must clear bits in PKO_PDM_ECC_SBE_STS. When this bit is set, the corresponding interrupt
+	uint64_t pdm_sbe_cmb0                 : 1;  /**< This bit is the logical OR of all bits in PKO_PDM_ECC_SBE_STS0. To clear this bit,
+                                                         software
+                                                         must clear bits in PKO_PDM_ECC_SBE_STS0. When this bit is set, the corresponding interrupt
                                                          is set. Throws PKO_INTSN_E::PKO_PDM_SBE_CMB0. INTERNAL: Instances:
                                                          fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.flshb.flshb_cache_hi
                                                          fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.flshb.flshb_cache_lo
@@ -9568,43 +9574,43 @@ union cvmx_pko_pdm_isrd_dbg {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_44_63               : 20;
 	uint64_t in_arb_reqs                  : 8;  /**< Input arbitration request signals. The order of the bits is:
-                                                         0x2B = Fill response - normal path request
-                                                         0x2A = Fill response - flushb path request
-                                                         0x29 = CP queue-open request
-                                                         0x28 = CP queue-closed request
-                                                         0x27 = CP queue-query request
-                                                         0x26 = CP send-packet request
-                                                         0x25 = PEB fill request
-                                                         0x24 = PEB read request */
+                                                         0x2B = Fill response - normal path request.
+                                                         0x2A = Fill response - flushb path request.
+                                                         0x29 = CP queue-open request.
+                                                         0x28 = CP queue-closed request.
+                                                         0x27 = CP queue-query request.
+                                                         0x26 = CP send-packet request.
+                                                         0x25 = PEB fill request.
+                                                         0x24 = PEB read request. */
 	uint64_t in_arb_gnts                  : 7;  /**< Input arbitration grant signals. The order of the bits is:
-                                                         0x23 = Fill response grant
-                                                         0x22 = CP - queue-open grant
-                                                         0x21 = CP - queue-close grant
-                                                         0x20 = CP - queue-query grant
-                                                         0x1F = CP - send-packet grant
-                                                         0x1E = PEB fill grant
-                                                         0x1D = PEB read grant */
+                                                         0x23 = Fill response grant.
+                                                         0x22 = CP - queue-open grant.
+                                                         0x21 = CP - queue-close grant.
+                                                         0x20 = CP - queue-query grant.
+                                                         0x1F = CP - send-packet grant.
+                                                         0x1E = PEB fill grant.
+                                                         0x1D = PEB read grant. */
 	uint64_t cmt_arb_reqs                 : 7;  /**< Commit arbitration request signals. The order of the bits is:
-                                                         0x1C = Fill response grant
-                                                         0x1B = CP - queue-open grant
-                                                         0x1A = CP - queue-close grant
-                                                         0x19 = CP - queue-query grant
-                                                         0x18 = CP - send-packet grant
-                                                         0x17 = PEB fill grant
-                                                         0x16 = PEB read grant */
+                                                         0x1C = Fill response grant.
+                                                         0x1B = CP - queue-open grant.
+                                                         0x1A = CP - queue-close grant.
+                                                         0x19 = CP - queue-query grant.
+                                                         0x18 = CP - send-packet grant.
+                                                         0x17 = PEB fill grant.
+                                                         0x16 = PEB read grant. */
 	uint64_t cmt_arb_gnts                 : 7;  /**< Commit arbitration grant signals. The order of the bits is:
-                                                         0x15 = Fill response grant
-                                                         0x14 = CP - queue-open grant
-                                                         0x13 = CP - queue-close grant
-                                                         0x12 = CP - queue-query grant
-                                                         0x11 = CP - send-packet grant
-                                                         0x10 = PEB fill grant
-                                                         0xF = PEB read grant */
+                                                         0x15 = Fill response grant.
+                                                         0x14 = CP - queue-open grant.
+                                                         0x13 = CP - queue-close grant.
+                                                         0x12 = CP - queue-query grant.
+                                                         0x11 = CP - send-packet grant.
+                                                         0x10 = PEB fill grant.
+                                                         0xF = PEB read grant. */
 	uint64_t in_use                       : 4;  /**< In use signals indicate the execution units are in use. The order of the bits is:
-                                                         0xE = PEB fill unit
-                                                         0xD = PEB read unit
-                                                         0xC = CP unit
-                                                         0xB = Fill response unit */
+                                                         0xE = PEB fill unit.
+                                                         0xD = PEB read unit.
+                                                         0xC = CP unit.
+                                                         0xB = Fill response unit. */
 	uint64_t has_cred                     : 4;  /**< Has credit signals indicate there is sufficient credit to commit. The order of the bits
                                                          is:
                                                          0xA = Flush buffer has credit
@@ -9613,13 +9619,13 @@ union cvmx_pko_pdm_isrd_dbg {
                                                          0x7 = DR command output FIFO has credit */
 	uint64_t val_exec                     : 7;  /**< Valid bits for the execution units; means the unit can commit if it gets the grant of the
                                                          commit arb and other conditions are met. The order of the bits is:
-                                                         0x6 = Fill response unit
-                                                         0x5 = CP unit - queue-open
-                                                         0x4 = CP unit - queue-close
-                                                         0x3 = CP unit - queue-probe
-                                                         0x2 = CP unit - send-packet
-                                                         0x1 = PEB fill unit
-                                                         0x0 = PEB read unit */
+                                                         0x6 = Fill response unit.
+                                                         0x5 = CP unit - queue-open.
+                                                         0x4 = CP unit - queue-close.
+                                                         0x3 = CP unit - queue-probe.
+                                                         0x2 = CP unit - send-packet.
+                                                         0x1 = PEB fill unit.
+                                                         0x0 = PEB read unit. */
 #else
 	uint64_t val_exec                     : 7;
 	uint64_t has_cred                     : 4;
@@ -9674,48 +9680,48 @@ union cvmx_pko_pdm_isrm_dbg {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_34_63               : 30;
 	uint64_t in_arb_reqs                  : 7;  /**< Input arbitration request signals. The order of the bits is:
-                                                         0x21 = PSE ACK
-                                                         0x20 = Fill Response - normal path request
-                                                         0x1F = Fill Response - flushb path request
-                                                         0x1E = CP queue-open
-                                                         0x1D = CP queue-closed
-                                                         0x1C = CP queue-query
-                                                         0x1B = CP send-packet */
+                                                         0x21 = PSE ACK.
+                                                         0x20 = Fill Response - normal path request.
+                                                         0x1F = Fill Response - flushb path request.
+                                                         0x1E = CP queue-open.
+                                                         0x1D = CP queue-closed.
+                                                         0x1C = CP queue-query.
+                                                         0x1B = CP send-packet. */
 	uint64_t in_arb_gnts                  : 6;  /**< Input arbitration grant signals. The order of the bits is:
-                                                         0x1A: PSE ACK
-                                                         0x19 = Fill Response
-                                                         0x18 = CP - queue-open
-                                                         0x17 = CP - queue-close
-                                                         0x16 = CP - queue-query
-                                                         0x15 = CP - send-packet */
+                                                         0x1A = PSE ACK.
+                                                         0x19 = Fill Response.
+                                                         0x18 = CP - queue-open.
+                                                         0x17 = CP - queue-close.
+                                                         0x16 = CP - queue-query.
+                                                         0x15 = CP - send-packet. */
 	uint64_t cmt_arb_reqs                 : 6;  /**< Commit arbitration request signals. The order of the bits is:
-                                                         0x14 = PSE ACK
-                                                         0x13 = Fill Response
-                                                         0x12 = CP - queue-open
-                                                         0x11 = CP - queue-close
-                                                         0x10 = CP - queue-query
-                                                         0xF CP - send-packet */
+                                                         0x14 = PSE ACK.
+                                                         0x13 = Fill Response.
+                                                         0x12 = CP - queue-open.
+                                                         0x11 = CP - queue-close.
+                                                         0x10 = CP - queue-query.
+                                                         0xF CP - send-packet. */
 	uint64_t cmt_arb_gnts                 : 6;  /**< Commit arbitration grant signals. The order of the bits is:
-                                                         0xE = PSE ACK
-                                                         0xD = Fill Response
-                                                         0xC = CP - queue-open
-                                                         0xB = CP - queue-close
-                                                         0xA = CP - queue-query
-                                                         0x9 = CP - send-packet */
+                                                         0xE = PSE ACK.
+                                                         0xD = Fill Response.
+                                                         0xC = CP - queue-open.
+                                                         0xB = CP - queue-close.
+                                                         0xA = CP - queue-query.
+                                                         0x9 = CP - send-packet. */
 	uint64_t in_use                       : 3;  /**< In use signals indicate the execution units are in use. The order of the bits is:
-                                                         0x8 = (PSE) ACK unit
-                                                         0x7 = Fill response unit
-                                                         0x6 = CP unit */
+                                                         0x8 = (PSE) ACK unit.
+                                                         0x7 = Fill response unit.
+                                                         0x6 = CP unit. */
 	uint64_t has_cred                     : 3;  /**< Has credit signals indicate there is sufficient credit to commit. The order of the bits
                                                          is:
-                                                         0x5 = Flush buffer has credit
-                                                         0x4 = Fill buffer has credit
-                                                         0x3 = MWP command output FIFO has credit */
+                                                         0x5 = Flush buffer has credit.
+                                                         0x4 = Fill buffer has credit.
+                                                         0x3 = MWP command output FIFO has credit. */
 	uint64_t val_exec                     : 3;  /**< Valid bits for the execution units; means the unit can commit if it gets the grant of the
                                                          commit arb and other conditions are met. The order of the bits is:
-                                                         0x2 = (PSE) ACK unit
-                                                         0x1 = Fill response unit
-                                                         0x0 = CP unit - ALL */
+                                                         0x2 = (PSE) ACK unit.
+                                                         0x1 = Fill response unit.
+                                                         0x0 = CP unit - ALL. */
 #else
 	uint64_t val_exec                     : 3;
 	uint64_t has_cred                     : 3;
@@ -9765,7 +9771,12 @@ union cvmx_pko_pdm_mem_addr {
 	struct cvmx_pko_pdm_mem_addr_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t memsel                       : 3;  /**< Memory select. Selects the RAM to read or write to.
-                                                         0 = Invalid, 1 = ISRM states, 2 = ISRD states, 3 = DWPBUF, 4 = DRPBUF, 5 = MWPBUF */
+                                                         0 = Invalid.
+                                                         1 = ISRM states.
+                                                         2 = ISRD states.
+                                                         3 = DWPBUF.
+                                                         4 = DRPBUF.
+                                                         5 = MWPBUF. */
 	uint64_t reserved_17_60               : 44;
 	uint64_t memaddr                      : 14; /**< Memory address for the RAM. */
 	uint64_t reserved_2_2                 : 1;
@@ -9858,10 +9869,10 @@ union cvmx_pko_pdm_mwpbuf_dbg {
 	uint64_t reserved_17_20               : 4;
 	uint64_t mem_addr                     : 13; /**< Memory address for pbuf ram. */
 	uint64_t mem_en                       : 4;  /**< Memory write/chip enable signals. The order of the bits is:
-                                                         0x3 = low wen
-                                                         0x2 = low cen
-                                                         0x1 = high wen
-                                                         0x0 = high cen. */
+                                                         0x3 = Low wen.
+                                                         0x2 = Low cen.
+                                                         0x1 = High wen.
+                                                         0x0 = High cen. */
 #else
 	uint64_t mem_en                       : 4;
 	uint64_t mem_addr                     : 13;
@@ -9905,7 +9916,7 @@ union cvmx_pko_pdm_sts {
                                                          Enumerated by PKO_DQSTATUS_E. */
 	uint64_t qcmd_iobx_err                : 1;  /**< Queue command IOBDMA/IOBLD error status occurred in PKO/PDM.
                                                          PKO_PDM_STS[QCMD_IOBX_ERR_STS] contains the status code. Note that FPA being out of
-                                                         pointers does not set this bit. (See PKO_FPA_NO_PTRS). Throws
+                                                         pointers does not set this bit. (See PKO_FPA_NO_PTRS.) Throws
                                                          PKO_INTSN_E::PKO_QCMD_IOBX_ERR. */
 	uint64_t sendpkt_lmtdma_err_sts       : 4;  /**< This is the status field of the command response on the LMTDMA failure indicated by
                                                          PKO_PDM_STS[SENDPKT_LMTDMA_ERR] bits being asserted. Note that if multiple errors occur,
@@ -9913,7 +9924,7 @@ union cvmx_pko_pdm_sts {
                                                          cleared. Enumerated by PKO_DQSTATUS_E. */
 	uint64_t sendpkt_lmtdma_err           : 1;  /**< Send-packet of type LMTDMA error status occurred in PKO/PDM.
                                                          PKO_PDM_STS[SENDPKT_LMTDMA_ERR_STS] contains the status code. Note that FPA being out of
-                                                         pointers does not set this bit. (See PKO_FPA_NO_PTRS). Throws
+                                                         pointers does not set this bit. (See PKO_FPA_NO_PTRS.) Throws
                                                          PKO_INTSN_E::PKO_SENDPKT_LMTDMA_ERR. */
 	uint64_t sendpkt_lmtst_err_sts        : 4;  /**< This is the status field of the command response on the LMTST failure indicated by
                                                          PKO_PDM_STS[SENDPKT_LMTST_ERR] bits being asserted. Note that if multiple errors occur
@@ -9921,7 +9932,7 @@ union cvmx_pko_pdm_sts {
                                                          cleared. Enumerated by PKO_DQSTATUS_E. */
 	uint64_t sendpkt_lmtst_err            : 1;  /**< Send-packet of type LMTST error status occurred in PKO/PDM.
                                                          PKO_PDM_STS[SENDPKT_LMTST_ERR_STS] contains the status code. Note that FPA being out of
-                                                         pointers does not set this bit. (See PKO_FPA_NO_PTRS). Throws
+                                                         pointers does not set this bit. (See PKO_FPA_NO_PTRS.) Throws
                                                          PKO_INTSN_E::PKO_SENDPKT_LMTST_ERR. */
 	uint64_t fpa_no_ptrs                  : 1;  /**< FPA signaled PKO that FPA can not allocate pointers. This is a fatal error. Throws
                                                          PKO_INTSN_E::PKO_FPA_NO_PTRS. */
@@ -10249,8 +10260,9 @@ union cvmx_pko_peb_ecc_dbe_sts_cmb0 {
 	uint64_t u64;
 	struct cvmx_pko_peb_ecc_dbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t peb_dbe_cmb0                 : 1;  /**< This bit is the logical OR of all bits in PKO_PEB_ECC_DBE_STS. To clear this bit, software
-                                                         must clear bits in PKO_PEB_ECC_DBE_STS. When this bit is set, the corresponding interrupt
+	uint64_t peb_dbe_cmb0                 : 1;  /**< This bit is the logical OR of all bits in PKO_PEB_ECC_DBE_STS0. To clear this bit,
+                                                         software
+                                                         must clear bits in PKO_PEB_ECC_DBE_STS0. When this bit is set, the corresponding interrupt
                                                          is set. Throws PKO_INTSN_E::PKO_PEB_DBE_CMB0. INTERNAL: Instances:
                                                          pko_pnr3.pko_pnr3_peb.pko_peb_proc_i.pko_peb_iobp1_uid_fifo_i
                                                          pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_iobp0_fifo_i
@@ -10371,8 +10383,9 @@ union cvmx_pko_peb_ecc_sbe_sts_cmb0 {
 	uint64_t u64;
 	struct cvmx_pko_peb_ecc_sbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t peb_sbe_cmb0                 : 1;  /**< This bit is the logical OR of all bits in PKO_PEB_ECC_SBE_STS. To clear this bit, software
-                                                         must clear bits in PKO_PEB_ECC_SBE_STS. When this bit is set, the corresponding interrupt
+	uint64_t peb_sbe_cmb0                 : 1;  /**< This bit is the logical OR of all bits in PKO_PEB_ECC_SBE_STS0. To clear this bit,
+                                                         software
+                                                         must clear bits in PKO_PEB_ECC_SBE_STS0. When this bit is set, the corresponding interrupt
                                                          is set. Throws PKO_INTSN_E::PKO_PEB_SBE_CMB0. INTERNAL: Instances:
                                                          pko_pnr3.pko_pnr3_peb.pko_peb_proc_i.pko_peb_iobp1_uid_fifo_i
                                                          pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_iobp0_fifo_i
@@ -10413,7 +10426,7 @@ union cvmx_pko_peb_err_int {
 	struct cvmx_pko_peb_err_int_s {
 #ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_10_63               : 54;
-	uint64_t peb_macx_cfg_wr_err          : 1;  /**< Asserted when software writes a FIFO number to PKO_MAC(0..27)_CFG when that FIFO is
+	uint64_t peb_macx_cfg_wr_err          : 1;  /**< Asserted when software writes a FIFO number to PKO_MAC()_CFG when that FIFO is
                                                          already assigned. Throws PKO_INTSN_E::PEB_MACX_CFG_WR_ERR. */
 	uint64_t peb_max_link_err             : 1;  /**< Asserted when 200 LINK segments have been followed. Indicates likelihood of infinite loop.
                                                          Throws PKO_INTSN_E::PEB_MAX_LINK_ERR. */
@@ -10923,8 +10936,8 @@ union cvmx_pko_pse_dq_ecc_dbe_sts_cmb0 {
 	uint64_t u64;
 	struct cvmx_pko_pse_dq_ecc_dbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t pse_dq_dbe_cmb0              : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_DQ_ECC_DBE_STS.
-                                                         To clear this bit, software must clear bits in PKO_PSE_DQ_ECC_DBE_STS.
+	uint64_t pse_dq_dbe_cmb0              : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_DQ_ECC_DBE_STS0.
+                                                         To clear this bit, software must clear bits in PKO_PSE_DQ_ECC_DBE_STS0.
                                                          When this bit is set, the corresponding interrupt is set.
                                                          Throws PKO_INTSN_E::PKO_PSE_DQ_DBE_CMB0.
                                                          INTERNAL: Instances:
@@ -10988,8 +11001,8 @@ union cvmx_pko_pse_dq_ecc_sbe_sts_cmb0 {
 	uint64_t u64;
 	struct cvmx_pko_pse_dq_ecc_sbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t pse_dq_sbe_cmb0              : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_DQ_ECC_SBE_STS.
-                                                         To clear this bit, software must clear bits in PKO_PSE_DQ_ECC_SBE_STS.
+	uint64_t pse_dq_sbe_cmb0              : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_DQ_ECC_SBE_STS0.
+                                                         To clear this bit, software must clear bits in PKO_PSE_DQ_ECC_SBE_STS0.
                                                          When this bit is set, the corresponding interrupt is set.
                                                          Throws PKO_INTSN_E::PKO_PSE_DQ_SBE_CMB0.
                                                          INTERNAL: Instances:
@@ -11155,8 +11168,8 @@ union cvmx_pko_pse_pq_ecc_dbe_sts_cmb0 {
 	uint64_t u64;
 	struct cvmx_pko_pse_pq_ecc_dbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t pse_pq_dbe_cmb0              : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_PQ_ECC_DBE_STS.
-                                                         To clear this bit, software must clear bits in PKO_PSE_PQ_ECC_DBE_STS.
+	uint64_t pse_pq_dbe_cmb0              : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_PQ_ECC_DBE_STS0.
+                                                         To clear this bit, software must clear bits in PKO_PSE_PQ_ECC_DBE_STS0.
                                                          When this bit is set, the corresponding interrupt is set.
                                                          Throws PKO_INTSN_E::PKO_PSE_PQ_DBE_CMB0.
                                                          INTERNAL: Instances:
@@ -11224,8 +11237,8 @@ union cvmx_pko_pse_pq_ecc_sbe_sts_cmb0 {
 	uint64_t u64;
 	struct cvmx_pko_pse_pq_ecc_sbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t pse_pq_sbe_cmb0              : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_PQ_ECC_SBE_STS.
-                                                         To clear this bit, software must clear bits in PKO_PSE_PQ_ECC_SBE_STS.
+	uint64_t pse_pq_sbe_cmb0              : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_PQ_ECC_SBE_STS0.
+                                                         To clear this bit, software must clear bits in PKO_PSE_PQ_ECC_SBE_STS0.
                                                          When this bit is set, the corresponding interrupt is set.
                                                          Throws PKO_INTSN_E::PKO_PSE_PQ_SBE_CMB0.
                                                          INTERNAL: Instances:
@@ -11469,8 +11482,8 @@ union cvmx_pko_pse_sq1_ecc_dbe_sts_cmb0 {
 	uint64_t u64;
 	struct cvmx_pko_pse_sq1_ecc_dbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t pse_sq1_dbe_cmb0             : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_SQ1_ECC_DBE_STS.
-                                                         To clear this bit, software must clear bits in PKO_PSE_SQ1_ECC_DBE_STS.
+	uint64_t pse_sq1_dbe_cmb0             : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_SQ1_ECC_DBE_STS0.
+                                                         To clear this bit, software must clear bits in PKO_PSE_SQ1_ECC_DBE_STS0.
                                                          When this bit is set, the corresponding interrupt is set.
                                                          Throws PKO_INTSN_E::PKO_PSE_SQ1_DBE_CMB0.
                                                          INTERNAL: Instances:
@@ -11575,8 +11588,8 @@ union cvmx_pko_pse_sq1_ecc_sbe_sts_cmb0 {
 	uint64_t u64;
 	struct cvmx_pko_pse_sq1_ecc_sbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t pse_sq1_sbe_cmb0             : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_SQ1_ECC_SBE_STS.
-                                                         To clear this bit, software must clear bits in PKO_PSE_SQ1_ECC_SBE_STS.
+	uint64_t pse_sq1_sbe_cmb0             : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_SQ1_ECC_SBE_STS0.
+                                                         To clear this bit, software must clear bits in PKO_PSE_SQ1_ECC_SBE_STS0.
                                                          When this bit is set, the corresponding interrupt is set.
                                                          Throws PKO_INTSN_E::PKO_PSE_SQ1_SBE_CMB0.
                                                          INTERNAL: Instances:
@@ -11784,8 +11797,8 @@ union cvmx_pko_pse_sq2_ecc_dbe_sts_cmb0 {
 	uint64_t u64;
 	struct cvmx_pko_pse_sq2_ecc_dbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t pse_sq2_dbe_cmb0             : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_SQ2_ECC_DBE_STS.
-                                                         To clear this bit, software must clear bits in PKO_PSE_SQ2_ECC_DBE_STS.
+	uint64_t pse_sq2_dbe_cmb0             : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_SQ2_ECC_DBE_STS0.
+                                                         To clear this bit, software must clear bits in PKO_PSE_SQ2_ECC_DBE_STS0.
                                                          When this bit is set, the corresponding interrupt is set.
                                                          Throws PKO_INTSN_E::PKO_PSE_SQ2_DBE_CMB0.
                                                          INTERNAL: Instances:
@@ -11871,8 +11884,8 @@ union cvmx_pko_pse_sq2_ecc_sbe_sts_cmb0 {
 	uint64_t u64;
 	struct cvmx_pko_pse_sq2_ecc_sbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t pse_sq2_sbe_cmb0             : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_SQ2_ECC_SBE_STS.
-                                                         To clear this bit, software must clear bits in PKO_PSE_SQ2_ECC_SBE_STS.
+	uint64_t pse_sq2_sbe_cmb0             : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_SQ2_ECC_SBE_STS0.
+                                                         To clear this bit, software must clear bits in PKO_PSE_SQ2_ECC_SBE_STS0.
                                                          When this bit is set, the corresponding interrupt is set.
                                                          Throws PKO_INTSN_E::PKO_PSE_SQ2_SBE_CMB0.
                                                          INTERNAL: Instances:
@@ -12143,8 +12156,8 @@ union cvmx_pko_pse_sq3_ecc_dbe_sts_cmb0 {
 	uint64_t u64;
 	struct cvmx_pko_pse_sq3_ecc_dbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t pse_sq3_dbe_cmb0             : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_SQ3_ECC_DBE_STS.
-                                                         To clear this bit, software must clear bits in PKO_PSE_SQ3_ECC_DBE_STS.
+	uint64_t pse_sq3_dbe_cmb0             : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_SQ3_ECC_DBE_STS0.
+                                                         To clear this bit, software must clear bits in PKO_PSE_SQ3_ECC_DBE_STS0.
                                                          When this bit is set, the corresponding interrupt is set.
                                                          Throws PKO_INTSN_E::PKO_PSE_SQ3_DBE_CMB0.
                                                          INTERNAL: Instances:
@@ -12262,8 +12275,8 @@ union cvmx_pko_pse_sq3_ecc_sbe_sts_cmb0 {
 	uint64_t u64;
 	struct cvmx_pko_pse_sq3_ecc_sbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t pse_sq3_sbe_cmb0             : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_SQ3_ECC_SBE_STS.
-                                                         To clear this bit, software must clear bits in PKO_PSE_SQ3_ECC_SBE_STS.
+	uint64_t pse_sq3_sbe_cmb0             : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_SQ3_ECC_SBE_STS0.
+                                                         To clear this bit, software must clear bits in PKO_PSE_SQ3_ECC_SBE_STS0.
                                                          When this bit is set, the corresponding interrupt is set.
                                                          Throws PKO_INTSN_E::PKO_PSE_SQ3_SBE_CMB0.
                                                          INTERNAL: Instances:
@@ -12542,8 +12555,8 @@ union cvmx_pko_pse_sq4_ecc_dbe_sts_cmb0 {
 	uint64_t u64;
 	struct cvmx_pko_pse_sq4_ecc_dbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t pse_sq4_dbe_cmb0             : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_SQ4_ECC_DBE_STS.
-                                                         To clear this bit, software must clear bits in PKO_PSE_SQ4_ECC_DBE_STS.
+	uint64_t pse_sq4_dbe_cmb0             : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_SQ4_ECC_DBE_STS0.
+                                                         To clear this bit, software must clear bits in PKO_PSE_SQ4_ECC_DBE_STS0.
                                                          When this bit is set, the corresponding interrupt is set.
                                                          Throws PKO_INTSN_E::PKO_PSE_SQ4_DBE_CMB0.
                                                          INTERNAL: Instances:
@@ -12661,8 +12674,8 @@ union cvmx_pko_pse_sq4_ecc_sbe_sts_cmb0 {
 	uint64_t u64;
 	struct cvmx_pko_pse_sq4_ecc_sbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t pse_sq4_sbe_cmb0             : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_SQ4_ECC_SBE_STS.
-                                                         To clear this bit, software must clear bits in PKO_PSE_SQ4_ECC_SBE_STS.
+	uint64_t pse_sq4_sbe_cmb0             : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_SQ4_ECC_SBE_STS0.
+                                                         To clear this bit, software must clear bits in PKO_PSE_SQ4_ECC_SBE_STS0.
                                                          When this bit is set, the corresponding interrupt is set.
                                                          Throws PKO_INTSN_E::PKO_PSE_SQ4_SBE_CMB0.
                                                          INTERNAL: Instances:
@@ -12941,8 +12954,8 @@ union cvmx_pko_pse_sq5_ecc_dbe_sts_cmb0 {
 	uint64_t u64;
 	struct cvmx_pko_pse_sq5_ecc_dbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t pse_sq5_dbe_cmb0             : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_SQ5_ECC_DBE_STS.
-                                                         To clear this bit, software must clear bits in PKO_PSE_SQ5_ECC_DBE_STS.
+	uint64_t pse_sq5_dbe_cmb0             : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_SQ5_ECC_DBE_STS0.
+                                                         To clear this bit, software must clear bits in PKO_PSE_SQ5_ECC_DBE_STS0.
                                                          When this bit is set, the corresponding interrupt is set.
                                                          Throws PKO_INTSN_E::PKO_PSE_SQ5_DBE_CMB0.
                                                          INTERNAL: Instances:
@@ -13060,8 +13073,8 @@ union cvmx_pko_pse_sq5_ecc_sbe_sts_cmb0 {
 	uint64_t u64;
 	struct cvmx_pko_pse_sq5_ecc_sbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t pse_sq5_sbe_cmb0             : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_SQ5_ECC_SBE_STS.
-                                                         To clear this bit, software must clear bits in PKO_PSE_SQ5_ECC_SBE_STS.
+	uint64_t pse_sq5_sbe_cmb0             : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_SQ5_ECC_SBE_STS0.
+                                                         To clear this bit, software must clear bits in PKO_PSE_SQ5_ECC_SBE_STS0.
                                                          When this bit is set, the corresponding interrupt is set.
                                                          Throws PKO_INTSN_E::PKO_PSE_SQ5_SBE_CMB0.
                                                          INTERNAL: Instances:
@@ -13109,7 +13122,7 @@ union cvmx_pko_ptfx_status {
 	uint64_t in_flight_cnt                : 7;  /**< Number of packets currently in-flight within PEB for this link. Useful both for
                                                          reconfiguration (able to disable a FIFO when it is empty) and debugging. */
 	uint64_t mac_num                      : 5;  /**< MAC assigned to the given PKO TX FIFO. A value of 0x1F means unassigned. These register
-                                                         values are derived automatically by the hardware from the PKO_MAC(0..27)_CFG[FIFO_NUM]
+                                                         values are derived automatically by the hardware from the PKO_MAC()_CFG[FIFO_NUM]
                                                          settings. */
 #else
 	uint64_t mac_num                      : 5;
@@ -13161,7 +13174,7 @@ union cvmx_pko_ptgfx_cfg {
                                                          the NULL) should never exceed 250 Gb/s. This field represents the rate for each active
                                                          FIFO
                                                          in PEB; thus the calculation for throughput is a function of the SIZE field and whether or
-                                                         not the FIFO is assigned to a MAC in PKO_MAC(0..27)_CFG.
+                                                         not the FIFO is assigned to a MAC in PKO_MAC()_CFG.
                                                          0x0 = 6.25 Gb/s.
                                                          0x1 = 12.5 Gb/s.
                                                          0x2 = 25 Gb/s.
@@ -13171,35 +13184,35 @@ union cvmx_pko_ptgfx_cfg {
 	uint64_t size                         : 3;  /**< "PKO supports up to 29 independent TX FIFOs where 0-27 are physical and 28 is virtual. The
                                                          FIFOs are grouped into 8 sets of four contiguously numbered queues where each FIFO has a
                                                          base storage amount of 2.5K bytes of buffering.
-                                                         PKO_PTGF(0)_CFG -> FIFO# 0-3
-                                                         PKO_PTGF(1)_CFG -> FIFO# 4-7
-                                                         PKO_PTGF(2)_CFG -> FIFO# 8-11
-                                                         PKO_PTGF(3)_CFG -> FIFO# 12-15
-                                                         PKO_PTGF(4)_CFG -> FIFO# 16-19
-                                                         PKO_PTGF(5)_CFG -> FIFO# 20-23
-                                                         PKO_PTGF(6)_CFG -> FIFO# 24-27
-                                                         PKO_PTGF(7)_CFG -> FIFO# 28 (Virtual/NULL)
+                                                         _ PKO_PTGF(0)_CFG -> FIFO 0-3
+                                                         _ PKO_PTGF(1)_CFG -> FIFO 4-7
+                                                         _ PKO_PTGF(2)_CFG -> FIFO 8-11
+                                                         _ PKO_PTGF(3)_CFG -> FIFO 12-15
+                                                         _ PKO_PTGF(4)_CFG -> FIFO 16-19
+                                                         _ PKO_PTGF(5)_CFG -> FIFO 20-23
+                                                         _ PKO_PTGF(6)_CFG -> FIFO 24-27
+                                                         _ PKO_PTGF(7)_CFG -> FIFO 28 (Virtual/NULL)
                                                          Within each set, 2 or 4 FIFOs can be combined to produce a larger FIFO if desired. The
                                                          SIZE field is used to configure the number and depth of the FIFOs in a set. The supported
                                                          options for a FIFO set are as follows:
-                                                         SIZE: Set of 4 Contiguously Numbered FIFOs
-                                                         ------------------------------------------
-                                                         xxx      Queue0  Queue1  Queue2  Queue3
-                                                         000 :     2.5k    2.5k    2.5k    2.5k
-                                                         001 :     5.0k    0.0k    2.5k    2.5k
-                                                         010 :     2.5k    2.5k    5.0k    0.0k
-                                                         011 :     5.0k    0.0k    5.0k    0.0k
-                                                         100 :    10.0k    0.0k    0.0k    0.0k
+                                                         _ SIZE: Set of 4 Contiguously Numbered FIFOs
+                                                         _ ------------------------------------------
+                                                         _ xxx      Queue0  Queue1  Queue2  Queue3
+                                                         _ 000 :     2.5k    2.5k    2.5k    2.5k
+                                                         _ 001 :     5.0k    0.0k    2.5k    2.5k
+                                                         _ 010 :     2.5k    2.5k    5.0k    0.0k
+                                                         _ 011 :     5.0k    0.0k    5.0k    0.0k
+                                                         _ 100 :    10.0k    0.0k    0.0k    0.0k
                                                          Note: 101-111 are illegal SIZE values and should not be used.
                                                          Note that when a FIFO is set to a size of 0K bytes, FIFO_NUM is no longer legal and cannot
                                                          be assigned to an active MAC. For example, for the set of FIFOs 8-11, if the
                                                          PKO_PTGF(2)_CFG[SIZE] = 3'b100, then FIFO_NUMs 9, 10 and 11 are no longer valid. Only
                                                          FIFO_NUM = 8 is available from this set for assignment to a MAC because all of the 10
-                                                         Kbytes of buffering was configured to FIFO#8.
+                                                         Kbytes of buffering was configured to FIFO 8.
                                                          FIFO_NUM = 28 is a virtual FIFO and is used exclusively to indicate the NULL FIFO. Packets
                                                          targeting the NULL FIFO are dropped by PKO and their buffers returned to the FPA. The SIZE
                                                          field for PKO_PTGF(7) should always be set to zero. Modifications to this field require
-                                                         two writes. The first write must assert PKO_PTGF(0..7)_CFG[RESET] to reset the address
+                                                         two writes. The first write must assert PKO_PTGF()_CFG[RESET] to reset the address
                                                          pointers for the FIFOS in this group. The second write clears the RESET bit as well as
                                                          configures the new SIZE values." */
 #else
diff --git a/arch/mips/include/asm/octeon/cvmx-tim-defs.h b/arch/mips/include/asm/octeon/cvmx-tim-defs.h
new file mode 100644
index 0000000..6edc64a
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-tim-defs.h
@@ -0,0 +1,1816 @@
+/***********************license start***************
+ * Copyright (c) 2003-2014  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+
+/**
+ * cvmx-tim-defs.h
+ *
+ * Configuration and status register (CSR) type definitions for
+ * Octeon tim.
+ *
+ * This file is auto generated. Do not edit.
+ *
+ * <hr>$Revision$<hr>
+ *
+ */
+#ifndef __CVMX_TIM_DEFS_H__
+#define __CVMX_TIM_DEFS_H__
+
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_BIST_RESULT CVMX_TIM_BIST_RESULT_FUNC()
+static inline uint64_t CVMX_TIM_BIST_RESULT_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_TIM_BIST_RESULT not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058000020ull);
+}
+#else
+#define CVMX_TIM_BIST_RESULT (CVMX_ADD_IO_SEG(0x0001180058000020ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_DBG2 CVMX_TIM_DBG2_FUNC()
+static inline uint64_t CVMX_TIM_DBG2_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_TIM_DBG2 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00011800580000A0ull);
+}
+#else
+#define CVMX_TIM_DBG2 (CVMX_ADD_IO_SEG(0x00011800580000A0ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_DBG3 CVMX_TIM_DBG3_FUNC()
+static inline uint64_t CVMX_TIM_DBG3_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_TIM_DBG3 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00011800580000A8ull);
+}
+#else
+#define CVMX_TIM_DBG3 (CVMX_ADD_IO_SEG(0x00011800580000A8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_ECC_CFG CVMX_TIM_ECC_CFG_FUNC()
+static inline uint64_t CVMX_TIM_ECC_CFG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_TIM_ECC_CFG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058000018ull);
+}
+#else
+#define CVMX_TIM_ECC_CFG (CVMX_ADD_IO_SEG(0x0001180058000018ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_TIM_ENGX_ACTIVE(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 3)))))
+		cvmx_warn("CVMX_TIM_ENGX_ACTIVE(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180058001000ull) + ((offset) & 3) * 8;
+}
+#else
+#define CVMX_TIM_ENGX_ACTIVE(offset) (CVMX_ADD_IO_SEG(0x0001180058001000ull) + ((offset) & 3) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_FR_RN_CYCLES CVMX_TIM_FR_RN_CYCLES_FUNC()
+static inline uint64_t CVMX_TIM_FR_RN_CYCLES_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_TIM_FR_RN_CYCLES not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00011800580000C0ull);
+}
+#else
+#define CVMX_TIM_FR_RN_CYCLES (CVMX_ADD_IO_SEG(0x00011800580000C0ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_FR_RN_GPIOS CVMX_TIM_FR_RN_GPIOS_FUNC()
+static inline uint64_t CVMX_TIM_FR_RN_GPIOS_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_TIM_FR_RN_GPIOS not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00011800580000C8ull);
+}
+#else
+#define CVMX_TIM_FR_RN_GPIOS (CVMX_ADD_IO_SEG(0x00011800580000C8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_FR_RN_TT CVMX_TIM_FR_RN_TT_FUNC()
+static inline uint64_t CVMX_TIM_FR_RN_TT_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX)))
+		cvmx_warn("CVMX_TIM_FR_RN_TT not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058000010ull);
+}
+#else
+#define CVMX_TIM_FR_RN_TT (CVMX_ADD_IO_SEG(0x0001180058000010ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_GPIO_EN CVMX_TIM_GPIO_EN_FUNC()
+static inline uint64_t CVMX_TIM_GPIO_EN_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_TIM_GPIO_EN not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058000080ull);
+}
+#else
+#define CVMX_TIM_GPIO_EN (CVMX_ADD_IO_SEG(0x0001180058000080ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_INT0 CVMX_TIM_INT0_FUNC()
+static inline uint64_t CVMX_TIM_INT0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_TIM_INT0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058000030ull);
+}
+#else
+#define CVMX_TIM_INT0 (CVMX_ADD_IO_SEG(0x0001180058000030ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_INT0_EN CVMX_TIM_INT0_EN_FUNC()
+static inline uint64_t CVMX_TIM_INT0_EN_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX)))
+		cvmx_warn("CVMX_TIM_INT0_EN not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058000038ull);
+}
+#else
+#define CVMX_TIM_INT0_EN (CVMX_ADD_IO_SEG(0x0001180058000038ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_INT0_EVENT CVMX_TIM_INT0_EVENT_FUNC()
+static inline uint64_t CVMX_TIM_INT0_EVENT_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX)))
+		cvmx_warn("CVMX_TIM_INT0_EVENT not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058000040ull);
+}
+#else
+#define CVMX_TIM_INT0_EVENT (CVMX_ADD_IO_SEG(0x0001180058000040ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_INT_ECCERR CVMX_TIM_INT_ECCERR_FUNC()
+static inline uint64_t CVMX_TIM_INT_ECCERR_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_TIM_INT_ECCERR not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058000060ull);
+}
+#else
+#define CVMX_TIM_INT_ECCERR (CVMX_ADD_IO_SEG(0x0001180058000060ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_INT_ECCERR_EN CVMX_TIM_INT_ECCERR_EN_FUNC()
+static inline uint64_t CVMX_TIM_INT_ECCERR_EN_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX)))
+		cvmx_warn("CVMX_TIM_INT_ECCERR_EN not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058000068ull);
+}
+#else
+#define CVMX_TIM_INT_ECCERR_EN (CVMX_ADD_IO_SEG(0x0001180058000068ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_INT_ECCERR_EVENT0 CVMX_TIM_INT_ECCERR_EVENT0_FUNC()
+static inline uint64_t CVMX_TIM_INT_ECCERR_EVENT0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_TIM_INT_ECCERR_EVENT0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058000070ull);
+}
+#else
+#define CVMX_TIM_INT_ECCERR_EVENT0 (CVMX_ADD_IO_SEG(0x0001180058000070ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_INT_ECCERR_EVENT1 CVMX_TIM_INT_ECCERR_EVENT1_FUNC()
+static inline uint64_t CVMX_TIM_INT_ECCERR_EVENT1_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_TIM_INT_ECCERR_EVENT1 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058000078ull);
+}
+#else
+#define CVMX_TIM_INT_ECCERR_EVENT1 (CVMX_ADD_IO_SEG(0x0001180058000078ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_MEM_DEBUG0 CVMX_TIM_MEM_DEBUG0_FUNC()
+static inline uint64_t CVMX_TIM_MEM_DEBUG0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_TIM_MEM_DEBUG0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058001100ull);
+}
+#else
+#define CVMX_TIM_MEM_DEBUG0 (CVMX_ADD_IO_SEG(0x0001180058001100ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_MEM_DEBUG1 CVMX_TIM_MEM_DEBUG1_FUNC()
+static inline uint64_t CVMX_TIM_MEM_DEBUG1_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_TIM_MEM_DEBUG1 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058001108ull);
+}
+#else
+#define CVMX_TIM_MEM_DEBUG1 (CVMX_ADD_IO_SEG(0x0001180058001108ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_MEM_DEBUG2 CVMX_TIM_MEM_DEBUG2_FUNC()
+static inline uint64_t CVMX_TIM_MEM_DEBUG2_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_TIM_MEM_DEBUG2 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058001110ull);
+}
+#else
+#define CVMX_TIM_MEM_DEBUG2 (CVMX_ADD_IO_SEG(0x0001180058001110ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_MEM_RING0 CVMX_TIM_MEM_RING0_FUNC()
+static inline uint64_t CVMX_TIM_MEM_RING0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_TIM_MEM_RING0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058001000ull);
+}
+#else
+#define CVMX_TIM_MEM_RING0 (CVMX_ADD_IO_SEG(0x0001180058001000ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_MEM_RING1 CVMX_TIM_MEM_RING1_FUNC()
+static inline uint64_t CVMX_TIM_MEM_RING1_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_TIM_MEM_RING1 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058001008ull);
+}
+#else
+#define CVMX_TIM_MEM_RING1 (CVMX_ADD_IO_SEG(0x0001180058001008ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_REG_BIST_RESULT CVMX_TIM_REG_BIST_RESULT_FUNC()
+static inline uint64_t CVMX_TIM_REG_BIST_RESULT_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_TIM_REG_BIST_RESULT not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058000080ull);
+}
+#else
+#define CVMX_TIM_REG_BIST_RESULT (CVMX_ADD_IO_SEG(0x0001180058000080ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_REG_ERROR CVMX_TIM_REG_ERROR_FUNC()
+static inline uint64_t CVMX_TIM_REG_ERROR_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_TIM_REG_ERROR not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058000088ull);
+}
+#else
+#define CVMX_TIM_REG_ERROR (CVMX_ADD_IO_SEG(0x0001180058000088ull))
+#endif
+#define CVMX_TIM_REG_FLAGS (CVMX_ADD_IO_SEG(0x0001180058000000ull))
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_REG_INT_MASK CVMX_TIM_REG_INT_MASK_FUNC()
+static inline uint64_t CVMX_TIM_REG_INT_MASK_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_TIM_REG_INT_MASK not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058000090ull);
+}
+#else
+#define CVMX_TIM_REG_INT_MASK (CVMX_ADD_IO_SEG(0x0001180058000090ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_REG_READ_IDX CVMX_TIM_REG_READ_IDX_FUNC()
+static inline uint64_t CVMX_TIM_REG_READ_IDX_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_TIM_REG_READ_IDX not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058000008ull);
+}
+#else
+#define CVMX_TIM_REG_READ_IDX (CVMX_ADD_IO_SEG(0x0001180058000008ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_TIM_RINGX_AURA(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_TIM_RINGX_AURA(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180058002C00ull) + ((offset) & 63) * 8;
+}
+#else
+#define CVMX_TIM_RINGX_AURA(offset) (CVMX_ADD_IO_SEG(0x0001180058002C00ull) + ((offset) & 63) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_TIM_RINGX_CTL0(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 63))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_TIM_RINGX_CTL0(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180058002000ull) + ((offset) & 63) * 8;
+}
+#else
+#define CVMX_TIM_RINGX_CTL0(offset) (CVMX_ADD_IO_SEG(0x0001180058002000ull) + ((offset) & 63) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_TIM_RINGX_CTL1(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 63))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_TIM_RINGX_CTL1(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180058002400ull) + ((offset) & 63) * 8;
+}
+#else
+#define CVMX_TIM_RINGX_CTL1(offset) (CVMX_ADD_IO_SEG(0x0001180058002400ull) + ((offset) & 63) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_TIM_RINGX_CTL2(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 63))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_TIM_RINGX_CTL2(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180058002800ull) + ((offset) & 63) * 8;
+}
+#else
+#define CVMX_TIM_RINGX_CTL2(offset) (CVMX_ADD_IO_SEG(0x0001180058002800ull) + ((offset) & 63) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_TIM_RINGX_DBG0(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_TIM_RINGX_DBG0(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180058003000ull) + ((offset) & 63) * 8;
+}
+#else
+#define CVMX_TIM_RINGX_DBG0(offset) (CVMX_ADD_IO_SEG(0x0001180058003000ull) + ((offset) & 63) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_TIM_RINGX_DBG1(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_TIM_RINGX_DBG1(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180058001200ull) + ((offset) & 63) * 8;
+}
+#else
+#define CVMX_TIM_RINGX_DBG1(offset) (CVMX_ADD_IO_SEG(0x0001180058001200ull) + ((offset) & 63) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_TIM_RINGX_REL(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_TIM_RINGX_REL(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180058003000ull) + ((offset) & 63) * 8;
+}
+#else
+#define CVMX_TIM_RINGX_REL(offset) (CVMX_ADD_IO_SEG(0x0001180058003000ull) + ((offset) & 63) * 8)
+#endif
+
+/**
+ * cvmx_tim_bist_result
+ *
+ * This register provides access to the internal timer BIST results. Each bit is the BIST result
+ * of an individual memory (per bit, 0 = pass and 1 = fail).
+ */
+union cvmx_tim_bist_result {
+	uint64_t u64;
+	struct cvmx_tim_bist_result_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t tstmp_mem                    : 1;  /**< BIST result of the Time Stamp memory. */
+	uint64_t wqe_fifo                     : 1;  /**< BIST result of the NCB_WQE FIFO. */
+	uint64_t lslr_fifo                    : 1;  /**< BIST result of the NCB_LSLR FIFO. */
+	uint64_t rds_mem                      : 1;  /**< BIST result of the RDS memory. */
+#else
+	uint64_t rds_mem                      : 1;
+	uint64_t lslr_fifo                    : 1;
+	uint64_t wqe_fifo                     : 1;
+	uint64_t tstmp_mem                    : 1;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_tim_bist_result_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_3_63                : 61;
+	uint64_t wqe_fifo                     : 1;  /**< BIST result of the NCB_WQE FIFO (0=pass, !0=fail) */
+	uint64_t lslr_fifo                    : 1;  /**< BIST result of the NCB_LSLR FIFO (0=pass, !0=fail) */
+	uint64_t rds_mem                      : 1;  /**< BIST result of the RDS memory (0=pass, !0=fail) */
+#else
+	uint64_t rds_mem                      : 1;
+	uint64_t lslr_fifo                    : 1;
+	uint64_t wqe_fifo                     : 1;
+	uint64_t reserved_3_63                : 61;
+#endif
+	} cn68xx;
+	struct cvmx_tim_bist_result_cn68xx    cn68xxp1;
+	struct cvmx_tim_bist_result_s         cn78xx;
+};
+typedef union cvmx_tim_bist_result cvmx_tim_bist_result_t;
+
+/**
+ * cvmx_tim_dbg2
+ */
+union cvmx_tim_dbg2 {
+	uint64_t u64;
+	struct cvmx_tim_dbg2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t mem_alloc_reg                : 8;  /**< IOI load memory allocation status. */
+	uint64_t reserved_46_55               : 10;
+	uint64_t rwf_fifo_level               : 6;  /**< IOI requests FIFO level. */
+	uint64_t wqe_fifo_level               : 8;  /**< IOI WQE LD FIFO level. */
+	uint64_t reserved_16_31               : 16;
+	uint64_t fsm3_state                   : 4;  /**< FSM 3 current state. */
+	uint64_t fsm2_state                   : 4;  /**< FSM 2 current state. */
+	uint64_t fsm1_state                   : 4;  /**< FSM 1 current state. */
+	uint64_t fsm0_state                   : 4;  /**< FSM 0 current state. */
+#else
+	uint64_t fsm0_state                   : 4;
+	uint64_t fsm1_state                   : 4;
+	uint64_t fsm2_state                   : 4;
+	uint64_t fsm3_state                   : 4;
+	uint64_t reserved_16_31               : 16;
+	uint64_t wqe_fifo_level               : 8;
+	uint64_t rwf_fifo_level               : 6;
+	uint64_t reserved_46_55               : 10;
+	uint64_t mem_alloc_reg                : 8;
+#endif
+	} s;
+	struct cvmx_tim_dbg2_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t mem_alloc_reg                : 8;  /**< NCB Load Memory Allocation status */
+	uint64_t reserved_51_55               : 5;
+	uint64_t gnt_fifo_level               : 3;  /**< NCB GRANT FIFO level */
+	uint64_t reserved_45_47               : 3;
+	uint64_t rwf_fifo_level               : 5;  /**< NCB requests FIFO level */
+	uint64_t wqe_fifo_level               : 8;  /**< NCB WQE LD FIFO level */
+	uint64_t reserved_16_31               : 16;
+	uint64_t fsm3_state                   : 4;  /**< FSM 3 current state */
+	uint64_t fsm2_state                   : 4;  /**< FSM 2 current state */
+	uint64_t fsm1_state                   : 4;  /**< FSM 1 current state */
+	uint64_t fsm0_state                   : 4;  /**< FSM 0 current state */
+#else
+	uint64_t fsm0_state                   : 4;
+	uint64_t fsm1_state                   : 4;
+	uint64_t fsm2_state                   : 4;
+	uint64_t fsm3_state                   : 4;
+	uint64_t reserved_16_31               : 16;
+	uint64_t wqe_fifo_level               : 8;
+	uint64_t rwf_fifo_level               : 5;
+	uint64_t reserved_45_47               : 3;
+	uint64_t gnt_fifo_level               : 3;
+	uint64_t reserved_51_55               : 5;
+	uint64_t mem_alloc_reg                : 8;
+#endif
+	} cn68xx;
+	struct cvmx_tim_dbg2_cn68xx           cn68xxp1;
+	struct cvmx_tim_dbg2_cn78xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t mem_alloc_reg                : 8;  /**< IOI load memory allocation status. */
+	uint64_t reserved_53_55               : 3;
+	uint64_t gnt_fifo_level               : 4;  /**< IOI grant FIFO level. */
+	uint64_t fpa_fifo_level               : 3;  /**< FPA FIFO level. */
+	uint64_t rwf_fifo_level               : 6;  /**< IOI requests FIFO level. */
+	uint64_t wqe_fifo_level               : 8;  /**< IOI WQE LD FIFO level. */
+	uint64_t reserved_16_31               : 16;
+	uint64_t fsm3_state                   : 4;  /**< FSM 3 current state. */
+	uint64_t fsm2_state                   : 4;  /**< FSM 2 current state. */
+	uint64_t fsm1_state                   : 4;  /**< FSM 1 current state. */
+	uint64_t fsm0_state                   : 4;  /**< FSM 0 current state. */
+#else
+	uint64_t fsm0_state                   : 4;
+	uint64_t fsm1_state                   : 4;
+	uint64_t fsm2_state                   : 4;
+	uint64_t fsm3_state                   : 4;
+	uint64_t reserved_16_31               : 16;
+	uint64_t wqe_fifo_level               : 8;
+	uint64_t rwf_fifo_level               : 6;
+	uint64_t fpa_fifo_level               : 3;
+	uint64_t gnt_fifo_level               : 4;
+	uint64_t reserved_53_55               : 3;
+	uint64_t mem_alloc_reg                : 8;
+#endif
+	} cn78xx;
+};
+typedef union cvmx_tim_dbg2 cvmx_tim_dbg2_t;
+
+/**
+ * cvmx_tim_dbg3
+ */
+union cvmx_tim_dbg3 {
+	uint64_t u64;
+	struct cvmx_tim_dbg3_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t rings_pending_vec            : 64; /**< Pending rings vector. Indicates which ring in TIM are pending traversal. Bit 0 represents
+                                                         ring 0 while bit 63 represents ring 63. */
+#else
+	uint64_t rings_pending_vec            : 64;
+#endif
+	} s;
+	struct cvmx_tim_dbg3_s                cn68xx;
+	struct cvmx_tim_dbg3_s                cn68xxp1;
+	struct cvmx_tim_dbg3_s                cn78xx;
+};
+typedef union cvmx_tim_dbg3 cvmx_tim_dbg3_t;
+
+/**
+ * cvmx_tim_ecc_cfg
+ */
+union cvmx_tim_ecc_cfg {
+	uint64_t u64;
+	struct cvmx_tim_ecc_cfg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_3_63                : 61;
+	uint64_t ecc_flp_syn                  : 2;  /**< ECC flip syndrome. Flip the ECC's syndrome for testing purposes, to test SBE and DBE ECC
+                                                         interrupts. */
+	uint64_t ecc_en                       : 1;  /**< Enable ECC correction of the ring data structure memory. */
+#else
+	uint64_t ecc_en                       : 1;
+	uint64_t ecc_flp_syn                  : 2;
+	uint64_t reserved_3_63                : 61;
+#endif
+	} s;
+	struct cvmx_tim_ecc_cfg_s             cn68xx;
+	struct cvmx_tim_ecc_cfg_s             cn68xxp1;
+	struct cvmx_tim_ecc_cfg_s             cn78xx;
+};
+typedef union cvmx_tim_ecc_cfg cvmx_tim_ecc_cfg_t;
+
+/**
+ * cvmx_tim_eng#_active
+ */
+union cvmx_tim_engx_active {
+	uint64_t u64;
+	struct cvmx_tim_engx_active_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t act                          : 1;  /**< Engine active. For diagnostic use. */
+	uint64_t reserved_6_7                 : 2;
+	uint64_t ring_id                      : 6;  /**< Current ring ID. For diagnostic use. */
+#else
+	uint64_t ring_id                      : 6;
+	uint64_t reserved_6_7                 : 2;
+	uint64_t act                          : 1;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_tim_engx_active_s         cn78xx;
+};
+typedef union cvmx_tim_engx_active cvmx_tim_engx_active_t;
+
+/**
+ * cvmx_tim_fr_rn_cycles
+ */
+union cvmx_tim_fr_rn_cycles {
+	uint64_t u64;
+	struct cvmx_tim_fr_rn_cycles_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t count                        : 64; /**< Count of system coprocessor-clock cycles. This register is only writable when
+                                                         TIM_REG_FLAGS[ENA_TIM] = 0. */
+#else
+	uint64_t count                        : 64;
+#endif
+	} s;
+	struct cvmx_tim_fr_rn_cycles_s        cn78xx;
+};
+typedef union cvmx_tim_fr_rn_cycles cvmx_tim_fr_rn_cycles_t;
+
+/**
+ * cvmx_tim_fr_rn_gpios
+ */
+union cvmx_tim_fr_rn_gpios {
+	uint64_t u64;
+	struct cvmx_tim_fr_rn_gpios_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t count                        : 64; /**< Count of GPIO cycles. This register is only writable when TIM_REG_FLAGS[ENA_TIM] = 0. */
+#else
+	uint64_t count                        : 64;
+#endif
+	} s;
+	struct cvmx_tim_fr_rn_gpios_s         cn78xx;
+};
+typedef union cvmx_tim_fr_rn_gpios cvmx_tim_fr_rn_gpios_t;
+
+/**
+ * cvmx_tim_fr_rn_tt
+ *
+ * Notes:
+ * For every 64 entries in a bucket interval should be at
+ * least 1us.
+ * Minimal recommended value for Threshold register is 1us
+ */
+union cvmx_tim_fr_rn_tt {
+	uint64_t u64;
+	struct cvmx_tim_fr_rn_tt_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_54_63               : 10;
+	uint64_t thld_gp                      : 22; /**< Free Running Timer Threshold. Defines the reset value
+                                                         for the free running timer when it reaches zero during
+                                                         it's count down. This threshold only applies to the
+                                                         timer that is driven by GPIO edge as defined at
+                                                         TIM_REG_FLAGS.GPIO_EDGE
+                                                         ***NOTE: Added in pass 2.0 */
+	uint64_t reserved_22_31               : 10;
+	uint64_t fr_rn_tt                     : 22; /**< Free Running Timer Threshold. Defines the reset value
+                                                         for the free running timer when it reaches zero during
+                                                         it's count down.
+                                                         FR_RN_TT will be used in both cases where free running
+                                                         clock is driven externally or internally.
+                                                         Interval programming guidelines:
+                                                         For every 64 entries in a bucket interval should be at
+                                                         least 1us.
+                                                         Minimal recommended value for FR_RN_TT is 1us. */
+#else
+	uint64_t fr_rn_tt                     : 22;
+	uint64_t reserved_22_31               : 10;
+	uint64_t thld_gp                      : 22;
+	uint64_t reserved_54_63               : 10;
+#endif
+	} s;
+	struct cvmx_tim_fr_rn_tt_s            cn68xx;
+	struct cvmx_tim_fr_rn_tt_cn68xxp1 {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_22_63               : 42;
+	uint64_t fr_rn_tt                     : 22; /**< Free Running Timer Threshold. Defines the reset value
+                                                         for the free running timer when it reaches zero during
+                                                         it's count down.
+                                                         FR_RN_TT will be used in both cases where free running
+                                                         clock is driven externally or internally.
+                                                         Interval programming guidelines:
+                                                         For every 64 entries in a bucket interval should be at
+                                                         least 1us.
+                                                         Minimal recommended value for FR_RN_TT is 1us. */
+#else
+	uint64_t fr_rn_tt                     : 22;
+	uint64_t reserved_22_63               : 42;
+#endif
+	} cn68xxp1;
+};
+typedef union cvmx_tim_fr_rn_tt cvmx_tim_fr_rn_tt_t;
+
+/**
+ * cvmx_tim_gpio_en
+ */
+union cvmx_tim_gpio_en {
+	uint64_t u64;
+	struct cvmx_tim_gpio_en_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t gpio_en                      : 64; /**< Each bit corresponds to rings [63:0] respectively. This register reflects the values
+                                                         written to TIM_RING(0..63)_CTL1 [ENA_GPIO]. For debug only; Reserved. */
+#else
+	uint64_t gpio_en                      : 64;
+#endif
+	} s;
+	struct cvmx_tim_gpio_en_s             cn68xx;
+	struct cvmx_tim_gpio_en_s             cn78xx;
+};
+typedef union cvmx_tim_gpio_en cvmx_tim_gpio_en_t;
+
+/**
+ * cvmx_tim_int0
+ *
+ * A ring is in error if its interval has elapsed more than once without having been serviced,
+ * either due to too many events in this ring's previous interval, or another ring having too
+ * many events to process within this ring's interval. This is usually a programming error where
+ * the number of entries in the bucket is too large for the interval specified across the rings.
+ * When in error, TIM may process events in an interval later than requested. Any bit in the INT
+ * field should be cleared by writing one to it.
+ */
+union cvmx_tim_int0 {
+	uint64_t u64;
+	struct cvmx_tim_int0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t int0                         : 64; /**< Interrupt bit per ring. Each bit indicates the ring number in error. Throws INTSN
+                                                         TIM_INTSN_E::TIM_RING(0..63)_SLOW */
+#else
+	uint64_t int0                         : 64;
+#endif
+	} s;
+	struct cvmx_tim_int0_s                cn68xx;
+	struct cvmx_tim_int0_s                cn68xxp1;
+	struct cvmx_tim_int0_s                cn78xx;
+};
+typedef union cvmx_tim_int0 cvmx_tim_int0_t;
+
+/**
+ * cvmx_tim_int0_en
+ *
+ * Notes:
+ * When bit at TIM_INT0_EN is set it enables the corresponding TIM_INTO's bit for interrupt generation
+ * If enable bit is cleared the corresponding bit at TIM_INT0 will still be set.
+ * Interrupt to the cores is generated by : |(TIM_INT0 & TIM_INT0_EN0)
+ */
+union cvmx_tim_int0_en {
+	uint64_t u64;
+	struct cvmx_tim_int0_en_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t int0_en                      : 64; /**< Bit enable corresponding to TIM_INT0. */
+#else
+	uint64_t int0_en                      : 64;
+#endif
+	} s;
+	struct cvmx_tim_int0_en_s             cn68xx;
+	struct cvmx_tim_int0_en_s             cn68xxp1;
+};
+typedef union cvmx_tim_int0_en cvmx_tim_int0_en_t;
+
+/**
+ * cvmx_tim_int0_event
+ */
+union cvmx_tim_int0_event {
+	uint64_t u64;
+	struct cvmx_tim_int0_event_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_6_63                : 58;
+	uint64_t ring_id                      : 6;  /**< The first Ring ID where an interrupt occurred. */
+#else
+	uint64_t ring_id                      : 6;
+	uint64_t reserved_6_63                : 58;
+#endif
+	} s;
+	struct cvmx_tim_int0_event_s          cn68xx;
+	struct cvmx_tim_int0_event_s          cn68xxp1;
+};
+typedef union cvmx_tim_int0_event cvmx_tim_int0_event_t;
+
+/**
+ * cvmx_tim_int_eccerr
+ *
+ * Notes:
+ * Each bit in this reg is set regardless of TIM_INT_ECCERR_EN value.
+ *
+ */
+union cvmx_tim_int_eccerr {
+	uint64_t u64;
+	struct cvmx_tim_int_eccerr_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t ctl_dbe                      : 1;  /**< TIM CTL memory had a double-bit error. Throws INTSN TIM_INTSN_E::TIM_ECCERR_CTL_DBE. */
+	uint64_t ctl_sbe                      : 1;  /**< TIM CTL memory had a single-bit error. Throws INTSN TIM_INTSN_E::TIM_ECCERR_CTL_SBE. */
+	uint64_t dbe                          : 1;  /**< TIM RDS memory had a double-bit error. Throws INTSN TIM_INTSN_E::TIM_ECCERR_DBE. */
+	uint64_t sbe                          : 1;  /**< TIM RDS memory had a single-bit error. Throws INTSN TIM_INTSN_E::TIM_ECCERR_SBE. */
+#else
+	uint64_t sbe                          : 1;
+	uint64_t dbe                          : 1;
+	uint64_t ctl_sbe                      : 1;
+	uint64_t ctl_dbe                      : 1;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_tim_int_eccerr_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_2_63                : 62;
+	uint64_t dbe                          : 1;  /**< TIM RDS memory had a Double Bit Error */
+	uint64_t sbe                          : 1;  /**< TIM RDS memory had a Single Bit Error */
+#else
+	uint64_t sbe                          : 1;
+	uint64_t dbe                          : 1;
+	uint64_t reserved_2_63                : 62;
+#endif
+	} cn68xx;
+	struct cvmx_tim_int_eccerr_cn68xx     cn68xxp1;
+	struct cvmx_tim_int_eccerr_s          cn78xx;
+};
+typedef union cvmx_tim_int_eccerr cvmx_tim_int_eccerr_t;
+
+/**
+ * cvmx_tim_int_eccerr_en
+ *
+ * Notes:
+ * When mask bit is set, the corresponding bit in TIM_INT_ECCERR is enabled. If mask bit is cleared the
+ * corresponding bit in TIM_INT_ECCERR will still be set but interrupt will not be reported.
+ */
+union cvmx_tim_int_eccerr_en {
+	uint64_t u64;
+	struct cvmx_tim_int_eccerr_en_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_2_63                : 62;
+	uint64_t dbe_en                       : 1;  /**< Bit mask corresponding to TIM_REG_ECCERR.DBE */
+	uint64_t sbe_en                       : 1;  /**< Bit mask corresponding to TIM_REG_ECCERR.SBE */
+#else
+	uint64_t sbe_en                       : 1;
+	uint64_t dbe_en                       : 1;
+	uint64_t reserved_2_63                : 62;
+#endif
+	} s;
+	struct cvmx_tim_int_eccerr_en_s       cn68xx;
+	struct cvmx_tim_int_eccerr_en_s       cn68xxp1;
+};
+typedef union cvmx_tim_int_eccerr_en cvmx_tim_int_eccerr_en_t;
+
+/**
+ * cvmx_tim_int_eccerr_event0
+ */
+union cvmx_tim_int_eccerr_event0 {
+	uint64_t u64;
+	struct cvmx_tim_int_eccerr_event0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_7_63                : 57;
+	uint64_t add                          : 7;  /**< Memory address where the error occurred. */
+#else
+	uint64_t add                          : 7;
+	uint64_t reserved_7_63                : 57;
+#endif
+	} s;
+	struct cvmx_tim_int_eccerr_event0_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_15_63               : 49;
+	uint64_t synd                         : 7;  /**< ECC Syndrome */
+	uint64_t add                          : 8;  /**< Memory address where the Error occurred. */
+#else
+	uint64_t add                          : 8;
+	uint64_t synd                         : 7;
+	uint64_t reserved_15_63               : 49;
+#endif
+	} cn68xx;
+	struct cvmx_tim_int_eccerr_event0_cn68xx cn68xxp1;
+	struct cvmx_tim_int_eccerr_event0_cn78xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_14_63               : 50;
+	uint64_t synd                         : 7;  /**< ECC syndrome bits. */
+	uint64_t add                          : 7;  /**< Memory address where the error occurred. */
+#else
+	uint64_t add                          : 7;
+	uint64_t synd                         : 7;
+	uint64_t reserved_14_63               : 50;
+#endif
+	} cn78xx;
+};
+typedef union cvmx_tim_int_eccerr_event0 cvmx_tim_int_eccerr_event0_t;
+
+/**
+ * cvmx_tim_int_eccerr_event1
+ */
+union cvmx_tim_int_eccerr_event1 {
+	uint64_t u64;
+	struct cvmx_tim_int_eccerr_event1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_55_63               : 9;
+	uint64_t org_ecc                      : 7;  /**< Original ECC bits where the error occurred. */
+	uint64_t org_rds_dat                  : 48; /**< Memory original data where the error occurred. */
+#else
+	uint64_t org_rds_dat                  : 48;
+	uint64_t org_ecc                      : 7;
+	uint64_t reserved_55_63               : 9;
+#endif
+	} s;
+	struct cvmx_tim_int_eccerr_event1_s   cn68xx;
+	struct cvmx_tim_int_eccerr_event1_s   cn68xxp1;
+	struct cvmx_tim_int_eccerr_event1_s   cn78xx;
+};
+typedef union cvmx_tim_int_eccerr_event1 cvmx_tim_int_eccerr_event1_t;
+
+/**
+ * cvmx_tim_mem_debug0
+ *
+ * Notes:
+ * Internal per-ring state intended for debug use only - tim.ctl[47:0]
+ * This CSR is a memory of 16 entries, and thus, the TIM_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.
+ */
+union cvmx_tim_mem_debug0 {
+	uint64_t u64;
+	struct cvmx_tim_mem_debug0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t ena                          : 1;  /**< Ring timer enable */
+	uint64_t reserved_46_46               : 1;
+	uint64_t count                        : 22; /**< Time offset for the ring
+                                                         Set to INTERVAL and counts down by 1 every 1024
+                                                         cycles when ENA==1. The HW forces a bucket
+                                                         traversal (and resets COUNT to INTERVAL) whenever
+                                                         the decrement would cause COUNT to go negative.
+                                                         COUNT is unpredictable whenever ENA==0.
+                                                         COUNT is reset to INTERVAL whenever TIM_MEM_RING1
+                                                         is written for the ring. */
+	uint64_t reserved_22_23               : 2;
+	uint64_t interval                     : 22; /**< Timer interval - 1 */
+#else
+	uint64_t interval                     : 22;
+	uint64_t reserved_22_23               : 2;
+	uint64_t count                        : 22;
+	uint64_t reserved_46_46               : 1;
+	uint64_t ena                          : 1;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_tim_mem_debug0_s          cn30xx;
+	struct cvmx_tim_mem_debug0_s          cn31xx;
+	struct cvmx_tim_mem_debug0_s          cn38xx;
+	struct cvmx_tim_mem_debug0_s          cn38xxp2;
+	struct cvmx_tim_mem_debug0_s          cn50xx;
+	struct cvmx_tim_mem_debug0_s          cn52xx;
+	struct cvmx_tim_mem_debug0_s          cn52xxp1;
+	struct cvmx_tim_mem_debug0_s          cn56xx;
+	struct cvmx_tim_mem_debug0_s          cn56xxp1;
+	struct cvmx_tim_mem_debug0_s          cn58xx;
+	struct cvmx_tim_mem_debug0_s          cn58xxp1;
+	struct cvmx_tim_mem_debug0_s          cn61xx;
+	struct cvmx_tim_mem_debug0_s          cn63xx;
+	struct cvmx_tim_mem_debug0_s          cn63xxp1;
+	struct cvmx_tim_mem_debug0_s          cn66xx;
+	struct cvmx_tim_mem_debug0_s          cn70xx;
+	struct cvmx_tim_mem_debug0_s          cn70xxp1;
+	struct cvmx_tim_mem_debug0_s          cnf71xx;
+};
+typedef union cvmx_tim_mem_debug0 cvmx_tim_mem_debug0_t;
+
+/**
+ * cvmx_tim_mem_debug1
+ *
+ * Notes:
+ * Internal per-ring state intended for debug use only - tim.sta[63:0]
+ * This CSR is a memory of 16 entries, and thus, the TIM_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.
+ */
+union cvmx_tim_mem_debug1 {
+	uint64_t u64;
+	struct cvmx_tim_mem_debug1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t bucket                       : 13; /**< Current bucket[12:0]
+                                                         Reset to 0 whenever TIM_MEM_RING0 is written for
+                                                         the ring. Incremented (modulo BSIZE) once per
+                                                         bucket traversal.
+                                                         See TIM_MEM_DEBUG2[BUCKET]. */
+	uint64_t base                         : 31; /**< Pointer[35:5] to bucket[0] */
+	uint64_t bsize                        : 20; /**< Number of buckets - 1 */
+#else
+	uint64_t bsize                        : 20;
+	uint64_t base                         : 31;
+	uint64_t bucket                       : 13;
+#endif
+	} s;
+	struct cvmx_tim_mem_debug1_s          cn30xx;
+	struct cvmx_tim_mem_debug1_s          cn31xx;
+	struct cvmx_tim_mem_debug1_s          cn38xx;
+	struct cvmx_tim_mem_debug1_s          cn38xxp2;
+	struct cvmx_tim_mem_debug1_s          cn50xx;
+	struct cvmx_tim_mem_debug1_s          cn52xx;
+	struct cvmx_tim_mem_debug1_s          cn52xxp1;
+	struct cvmx_tim_mem_debug1_s          cn56xx;
+	struct cvmx_tim_mem_debug1_s          cn56xxp1;
+	struct cvmx_tim_mem_debug1_s          cn58xx;
+	struct cvmx_tim_mem_debug1_s          cn58xxp1;
+	struct cvmx_tim_mem_debug1_s          cn61xx;
+	struct cvmx_tim_mem_debug1_s          cn63xx;
+	struct cvmx_tim_mem_debug1_s          cn63xxp1;
+	struct cvmx_tim_mem_debug1_s          cn66xx;
+	struct cvmx_tim_mem_debug1_s          cn70xx;
+	struct cvmx_tim_mem_debug1_s          cn70xxp1;
+	struct cvmx_tim_mem_debug1_s          cnf71xx;
+};
+typedef union cvmx_tim_mem_debug1 cvmx_tim_mem_debug1_t;
+
+/**
+ * cvmx_tim_mem_debug2
+ *
+ * Notes:
+ * Internal per-ring state intended for debug use only - tim.sta[95:64]
+ * This CSR is a memory of 16 entries, and thus, the TIM_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.
+ */
+union cvmx_tim_mem_debug2 {
+	uint64_t u64;
+	struct cvmx_tim_mem_debug2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_24_63               : 40;
+	uint64_t cpool                        : 3;  /**< Free list used to free chunks */
+	uint64_t csize                        : 13; /**< Number of words per chunk */
+	uint64_t reserved_7_7                 : 1;
+	uint64_t bucket                       : 7;  /**< Current bucket[19:13]
+                                                         See TIM_MEM_DEBUG1[BUCKET]. */
+#else
+	uint64_t bucket                       : 7;
+	uint64_t reserved_7_7                 : 1;
+	uint64_t csize                        : 13;
+	uint64_t cpool                        : 3;
+	uint64_t reserved_24_63               : 40;
+#endif
+	} s;
+	struct cvmx_tim_mem_debug2_s          cn30xx;
+	struct cvmx_tim_mem_debug2_s          cn31xx;
+	struct cvmx_tim_mem_debug2_s          cn38xx;
+	struct cvmx_tim_mem_debug2_s          cn38xxp2;
+	struct cvmx_tim_mem_debug2_s          cn50xx;
+	struct cvmx_tim_mem_debug2_s          cn52xx;
+	struct cvmx_tim_mem_debug2_s          cn52xxp1;
+	struct cvmx_tim_mem_debug2_s          cn56xx;
+	struct cvmx_tim_mem_debug2_s          cn56xxp1;
+	struct cvmx_tim_mem_debug2_s          cn58xx;
+	struct cvmx_tim_mem_debug2_s          cn58xxp1;
+	struct cvmx_tim_mem_debug2_s          cn61xx;
+	struct cvmx_tim_mem_debug2_s          cn63xx;
+	struct cvmx_tim_mem_debug2_s          cn63xxp1;
+	struct cvmx_tim_mem_debug2_s          cn66xx;
+	struct cvmx_tim_mem_debug2_s          cn70xx;
+	struct cvmx_tim_mem_debug2_s          cn70xxp1;
+	struct cvmx_tim_mem_debug2_s          cnf71xx;
+};
+typedef union cvmx_tim_mem_debug2 cvmx_tim_mem_debug2_t;
+
+/**
+ * cvmx_tim_mem_ring0
+ *
+ * Notes:
+ * TIM_MEM_RING0 must not be written for a ring when TIM_MEM_RING1[ENA] is set for the ring.
+ * Every write to TIM_MEM_RING0 clears the current bucket for the ring. (The current bucket is
+ * readable via TIM_MEM_DEBUG2[BUCKET],TIM_MEM_DEBUG1[BUCKET].)
+ * BASE is a 32-byte aligned pointer[35:0].  Only pointer[35:5] are stored because pointer[4:0] = 0.
+ * This CSR is a memory of 16 entries, and thus, the TIM_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.
+ */
+union cvmx_tim_mem_ring0 {
+	uint64_t u64;
+	struct cvmx_tim_mem_ring0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_55_63               : 9;
+	uint64_t first_bucket                 : 31; /**< Pointer[35:5] to bucket[0] */
+	uint64_t num_buckets                  : 20; /**< Number of buckets - 1 */
+	uint64_t ring                         : 4;  /**< Ring ID */
+#else
+	uint64_t ring                         : 4;
+	uint64_t num_buckets                  : 20;
+	uint64_t first_bucket                 : 31;
+	uint64_t reserved_55_63               : 9;
+#endif
+	} s;
+	struct cvmx_tim_mem_ring0_s           cn30xx;
+	struct cvmx_tim_mem_ring0_s           cn31xx;
+	struct cvmx_tim_mem_ring0_s           cn38xx;
+	struct cvmx_tim_mem_ring0_s           cn38xxp2;
+	struct cvmx_tim_mem_ring0_s           cn50xx;
+	struct cvmx_tim_mem_ring0_s           cn52xx;
+	struct cvmx_tim_mem_ring0_s           cn52xxp1;
+	struct cvmx_tim_mem_ring0_s           cn56xx;
+	struct cvmx_tim_mem_ring0_s           cn56xxp1;
+	struct cvmx_tim_mem_ring0_s           cn58xx;
+	struct cvmx_tim_mem_ring0_s           cn58xxp1;
+	struct cvmx_tim_mem_ring0_s           cn61xx;
+	struct cvmx_tim_mem_ring0_s           cn63xx;
+	struct cvmx_tim_mem_ring0_s           cn63xxp1;
+	struct cvmx_tim_mem_ring0_s           cn66xx;
+	struct cvmx_tim_mem_ring0_s           cn70xx;
+	struct cvmx_tim_mem_ring0_s           cn70xxp1;
+	struct cvmx_tim_mem_ring0_s           cnf71xx;
+};
+typedef union cvmx_tim_mem_ring0 cvmx_tim_mem_ring0_t;
+
+/**
+ * cvmx_tim_mem_ring1
+ *
+ * Notes:
+ * After a 1->0 transition on ENA, the HW will still complete a bucket traversal for the ring
+ * if it was pending or active prior to the transition. (SW must delay to ensure the completion
+ * of the traversal before reprogramming the ring.)
+ * Every write to TIM_MEM_RING1 resets the current time offset for the ring to the INTERVAL value.
+ * (The current time offset for the ring is readable via TIM_MEM_DEBUG0[COUNT].)
+ * CSIZE must be at least 16.  It is illegal to program CSIZE to a value that is less than 16.
+ * This CSR is a memory of 16 entries, and thus, the TIM_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.
+ */
+union cvmx_tim_mem_ring1 {
+	uint64_t u64;
+	struct cvmx_tim_mem_ring1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_43_63               : 21;
+	uint64_t enable                       : 1;  /**< Ring timer enable
+                                                         When clear, the ring is disabled and TIM
+                                                         will not traverse any new buckets for the ring. */
+	uint64_t pool                         : 3;  /**< Free list used to free chunks */
+	uint64_t words_per_chunk              : 13; /**< Number of words per chunk */
+	uint64_t interval                     : 22; /**< Timer interval - 1, measured in 1024 cycle ticks */
+	uint64_t ring                         : 4;  /**< Ring ID */
+#else
+	uint64_t ring                         : 4;
+	uint64_t interval                     : 22;
+	uint64_t words_per_chunk              : 13;
+	uint64_t pool                         : 3;
+	uint64_t enable                       : 1;
+	uint64_t reserved_43_63               : 21;
+#endif
+	} s;
+	struct cvmx_tim_mem_ring1_s           cn30xx;
+	struct cvmx_tim_mem_ring1_s           cn31xx;
+	struct cvmx_tim_mem_ring1_s           cn38xx;
+	struct cvmx_tim_mem_ring1_s           cn38xxp2;
+	struct cvmx_tim_mem_ring1_s           cn50xx;
+	struct cvmx_tim_mem_ring1_s           cn52xx;
+	struct cvmx_tim_mem_ring1_s           cn52xxp1;
+	struct cvmx_tim_mem_ring1_s           cn56xx;
+	struct cvmx_tim_mem_ring1_s           cn56xxp1;
+	struct cvmx_tim_mem_ring1_s           cn58xx;
+	struct cvmx_tim_mem_ring1_s           cn58xxp1;
+	struct cvmx_tim_mem_ring1_s           cn61xx;
+	struct cvmx_tim_mem_ring1_s           cn63xx;
+	struct cvmx_tim_mem_ring1_s           cn63xxp1;
+	struct cvmx_tim_mem_ring1_s           cn66xx;
+	struct cvmx_tim_mem_ring1_s           cn70xx;
+	struct cvmx_tim_mem_ring1_s           cn70xxp1;
+	struct cvmx_tim_mem_ring1_s           cnf71xx;
+};
+typedef union cvmx_tim_mem_ring1 cvmx_tim_mem_ring1_t;
+
+/**
+ * cvmx_tim_reg_bist_result
+ *
+ * Notes:
+ * Access to the internal BiST results
+ * Each bit is the BiST result of an individual memory (per bit, 0=pass and 1=fail).
+ */
+union cvmx_tim_reg_bist_result {
+	uint64_t u64;
+	struct cvmx_tim_reg_bist_result_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t sta                          : 2;  /**< BiST result of the STA   memories (0=pass, !0=fail) */
+	uint64_t ncb                          : 1;  /**< BiST result of the NCB   memories (0=pass, !0=fail) */
+	uint64_t ctl                          : 1;  /**< BiST result of the CTL   memories (0=pass, !0=fail) */
+#else
+	uint64_t ctl                          : 1;
+	uint64_t ncb                          : 1;
+	uint64_t sta                          : 2;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_tim_reg_bist_result_s     cn30xx;
+	struct cvmx_tim_reg_bist_result_s     cn31xx;
+	struct cvmx_tim_reg_bist_result_s     cn38xx;
+	struct cvmx_tim_reg_bist_result_s     cn38xxp2;
+	struct cvmx_tim_reg_bist_result_s     cn50xx;
+	struct cvmx_tim_reg_bist_result_s     cn52xx;
+	struct cvmx_tim_reg_bist_result_s     cn52xxp1;
+	struct cvmx_tim_reg_bist_result_s     cn56xx;
+	struct cvmx_tim_reg_bist_result_s     cn56xxp1;
+	struct cvmx_tim_reg_bist_result_s     cn58xx;
+	struct cvmx_tim_reg_bist_result_s     cn58xxp1;
+	struct cvmx_tim_reg_bist_result_s     cn61xx;
+	struct cvmx_tim_reg_bist_result_s     cn63xx;
+	struct cvmx_tim_reg_bist_result_s     cn63xxp1;
+	struct cvmx_tim_reg_bist_result_s     cn66xx;
+	struct cvmx_tim_reg_bist_result_s     cn70xx;
+	struct cvmx_tim_reg_bist_result_s     cn70xxp1;
+	struct cvmx_tim_reg_bist_result_s     cnf71xx;
+};
+typedef union cvmx_tim_reg_bist_result cvmx_tim_reg_bist_result_t;
+
+/**
+ * cvmx_tim_reg_error
+ *
+ * A ring is in error if its interval has elapsed more than once without having been serviced,
+ * either due to too many events in this ring's previous interval, or another ring having too
+ * many events to process within this ring's interval. This is usually a programming error where
+ * the number of entries in the bucket is too large for the interval specified across the rings.
+ * When in error, TIM may process events in an interval later than requested. Any bit in the INT
+ * field should be cleared by writing one to it.
+ */
+union cvmx_tim_reg_error {
+	uint64_t u64;
+	struct cvmx_tim_reg_error_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t mask                         : 16; /**< Bit mask indicating the rings in error */
+#else
+	uint64_t mask                         : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_tim_reg_error_s           cn30xx;
+	struct cvmx_tim_reg_error_s           cn31xx;
+	struct cvmx_tim_reg_error_s           cn38xx;
+	struct cvmx_tim_reg_error_s           cn38xxp2;
+	struct cvmx_tim_reg_error_s           cn50xx;
+	struct cvmx_tim_reg_error_s           cn52xx;
+	struct cvmx_tim_reg_error_s           cn52xxp1;
+	struct cvmx_tim_reg_error_s           cn56xx;
+	struct cvmx_tim_reg_error_s           cn56xxp1;
+	struct cvmx_tim_reg_error_s           cn58xx;
+	struct cvmx_tim_reg_error_s           cn58xxp1;
+	struct cvmx_tim_reg_error_s           cn61xx;
+	struct cvmx_tim_reg_error_s           cn63xx;
+	struct cvmx_tim_reg_error_s           cn63xxp1;
+	struct cvmx_tim_reg_error_s           cn66xx;
+	struct cvmx_tim_reg_error_s           cn70xx;
+	struct cvmx_tim_reg_error_s           cn70xxp1;
+	struct cvmx_tim_reg_error_s           cnf71xx;
+};
+typedef union cvmx_tim_reg_error cvmx_tim_reg_error_t;
+
+/**
+ * cvmx_tim_reg_flags
+ *
+ * This register provides flags for TIM.
+ *
+ */
+union cvmx_tim_reg_flags {
+	uint64_t u64;
+	struct cvmx_tim_reg_flags_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_7_63                : 57;
+	uint64_t gpio_edge                    : 2;  /**< Edge used for GPIO timing.
+                                                         0x0 = no edges and the timer tick is not generated.
+                                                         0x1 = TIM counts low to high transitions.
+                                                         0x2 = TIM counts high to low transitions.
+                                                         0x3 = TIM counts both low to high and high to low transitions. */
+	uint64_t ena_gpio                     : 1;  /**< Enable the external control of GPIO over the free
+                                                         running timer.
+                                                         When set, free running timer will be driven by GPIO.
+                                                         Free running timer will count posedge or negedge of the
+                                                         GPIO pin based on GPIO_EDGE register. */
+	uint64_t ena_dfb                      : 1;  /**< Enable Don't Free Buffer. When set chunk buffer
+                                                         would not be released by the TIM back to FPA. */
+	uint64_t reset                        : 1;  /**< Reset oneshot pulse for free-running structures */
+	uint64_t enable_dwb                   : 1;  /**< Enables non-zero DonwWriteBacks when set
+                                                         When set, enables the use of
+                                                         DontWriteBacks during the buffer freeing
+                                                         operations. */
+	uint64_t enable_timers                : 1;  /**< Enables the TIM section when set
+                                                         When set, TIM is in normal operation.
+                                                         When clear, time is effectively stopped for all
+                                                         rings in TIM. */
+#else
+	uint64_t enable_timers                : 1;
+	uint64_t enable_dwb                   : 1;
+	uint64_t reset                        : 1;
+	uint64_t ena_dfb                      : 1;
+	uint64_t ena_gpio                     : 1;
+	uint64_t gpio_edge                    : 2;
+	uint64_t reserved_7_63                : 57;
+#endif
+	} s;
+	struct cvmx_tim_reg_flags_cn30xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_3_63                : 61;
+	uint64_t reset                        : 1;  /**< Reset oneshot pulse for free-running structures */
+	uint64_t enable_dwb                   : 1;  /**< Enables non-zero DonwWriteBacks when set
+                                                         When set, enables the use of
+                                                         DontWriteBacks during the buffer freeing
+                                                         operations. */
+	uint64_t enable_timers                : 1;  /**< Enables the TIM section when set
+                                                         When set, TIM is in normal operation.
+                                                         When clear, time is effectively stopped for all
+                                                         rings in TIM. */
+#else
+	uint64_t enable_timers                : 1;
+	uint64_t enable_dwb                   : 1;
+	uint64_t reset                        : 1;
+	uint64_t reserved_3_63                : 61;
+#endif
+	} cn30xx;
+	struct cvmx_tim_reg_flags_cn30xx      cn31xx;
+	struct cvmx_tim_reg_flags_cn30xx      cn38xx;
+	struct cvmx_tim_reg_flags_cn30xx      cn38xxp2;
+	struct cvmx_tim_reg_flags_cn30xx      cn50xx;
+	struct cvmx_tim_reg_flags_cn30xx      cn52xx;
+	struct cvmx_tim_reg_flags_cn30xx      cn52xxp1;
+	struct cvmx_tim_reg_flags_cn30xx      cn56xx;
+	struct cvmx_tim_reg_flags_cn30xx      cn56xxp1;
+	struct cvmx_tim_reg_flags_cn30xx      cn58xx;
+	struct cvmx_tim_reg_flags_cn30xx      cn58xxp1;
+	struct cvmx_tim_reg_flags_cn30xx      cn61xx;
+	struct cvmx_tim_reg_flags_cn30xx      cn63xx;
+	struct cvmx_tim_reg_flags_cn30xx      cn63xxp1;
+	struct cvmx_tim_reg_flags_cn30xx      cn66xx;
+	struct cvmx_tim_reg_flags_s           cn68xx;
+	struct cvmx_tim_reg_flags_s           cn68xxp1;
+	struct cvmx_tim_reg_flags_cn30xx      cn70xx;
+	struct cvmx_tim_reg_flags_cn30xx      cn70xxp1;
+	struct cvmx_tim_reg_flags_cn78xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_7_63                : 57;
+	uint64_t gpio_edge                    : 2;  /**< Edge used for GPIO timing.
+                                                         0x0 = no edges and the timer tick is not generated.
+                                                         0x1 = TIM counts low to high transitions.
+                                                         0x2 = TIM counts high to low transitions.
+                                                         0x3 = TIM counts both low to high and high to low transitions. */
+	uint64_t reserved_3_4                 : 2;
+	uint64_t reset                        : 1;  /**< Reset. One-shot pulse for free-running timer FR_RN_HT. */
+	uint64_t reserved_1_1                 : 1;
+	uint64_t enable_timers                : 1;  /**< When set, TIM is in normal operation. When clear, time is effectively stopped for all
+                                                         rings in TIM.
+                                                         TIM has a counter (see FR_RN_HT) that causes a periodic tick. This counter is shared by
+                                                         all rings. Each Timer tick causes the hardware to decrement the time count for all enabled
+                                                         rings.
+                                                         When ENA_TIM = 0, the hardware stops the shared periodic counter, FR_RN_HT, so there are
+                                                         no more ticks, and there are no more new bucket traversals.
+                                                         If ENA_TIM transitions 1->0, TIM longer creates new bucket traversals, but does traverse
+                                                         any rings that previously expired and are pending hardware traversal. */
+#else
+	uint64_t enable_timers                : 1;
+	uint64_t reserved_1_1                 : 1;
+	uint64_t reset                        : 1;
+	uint64_t reserved_3_4                 : 2;
+	uint64_t gpio_edge                    : 2;
+	uint64_t reserved_7_63                : 57;
+#endif
+	} cn78xx;
+	struct cvmx_tim_reg_flags_cn30xx      cnf71xx;
+};
+typedef union cvmx_tim_reg_flags cvmx_tim_reg_flags_t;
+
+/**
+ * cvmx_tim_reg_int_mask
+ *
+ * Notes:
+ * Note that this CSR is present only in chip revisions beginning with pass2.
+ * When mask bit is set, the interrupt is enabled.
+ */
+union cvmx_tim_reg_int_mask {
+	uint64_t u64;
+	struct cvmx_tim_reg_int_mask_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t mask                         : 16; /**< Bit mask corresponding to TIM_REG_ERROR.MASK above */
+#else
+	uint64_t mask                         : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_tim_reg_int_mask_s        cn30xx;
+	struct cvmx_tim_reg_int_mask_s        cn31xx;
+	struct cvmx_tim_reg_int_mask_s        cn38xx;
+	struct cvmx_tim_reg_int_mask_s        cn38xxp2;
+	struct cvmx_tim_reg_int_mask_s        cn50xx;
+	struct cvmx_tim_reg_int_mask_s        cn52xx;
+	struct cvmx_tim_reg_int_mask_s        cn52xxp1;
+	struct cvmx_tim_reg_int_mask_s        cn56xx;
+	struct cvmx_tim_reg_int_mask_s        cn56xxp1;
+	struct cvmx_tim_reg_int_mask_s        cn58xx;
+	struct cvmx_tim_reg_int_mask_s        cn58xxp1;
+	struct cvmx_tim_reg_int_mask_s        cn61xx;
+	struct cvmx_tim_reg_int_mask_s        cn63xx;
+	struct cvmx_tim_reg_int_mask_s        cn63xxp1;
+	struct cvmx_tim_reg_int_mask_s        cn66xx;
+	struct cvmx_tim_reg_int_mask_s        cn70xx;
+	struct cvmx_tim_reg_int_mask_s        cn70xxp1;
+	struct cvmx_tim_reg_int_mask_s        cnf71xx;
+};
+typedef union cvmx_tim_reg_int_mask cvmx_tim_reg_int_mask_t;
+
+/**
+ * cvmx_tim_reg_read_idx
+ *
+ * Notes:
+ * Provides the read index during a CSR read operation to any of the CSRs that are physically stored
+ * as memories.  The names of these CSRs begin with the prefix "TIM_MEM_".
+ * IDX[7:0] is the read index.  INC[7:0] is an increment that is added to IDX[7:0] after any CSR read.
+ * The intended use is to initially write this CSR such that IDX=0 and INC=1.  Then, the entire
+ * contents of a CSR memory can be read with consecutive CSR read commands.
+ */
+union cvmx_tim_reg_read_idx {
+	uint64_t u64;
+	struct cvmx_tim_reg_read_idx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t inc                          : 8;  /**< Increment to add to current index for next index */
+	uint64_t index                        : 8;  /**< Index to use for next memory CSR read */
+#else
+	uint64_t index                        : 8;
+	uint64_t inc                          : 8;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_tim_reg_read_idx_s        cn30xx;
+	struct cvmx_tim_reg_read_idx_s        cn31xx;
+	struct cvmx_tim_reg_read_idx_s        cn38xx;
+	struct cvmx_tim_reg_read_idx_s        cn38xxp2;
+	struct cvmx_tim_reg_read_idx_s        cn50xx;
+	struct cvmx_tim_reg_read_idx_s        cn52xx;
+	struct cvmx_tim_reg_read_idx_s        cn52xxp1;
+	struct cvmx_tim_reg_read_idx_s        cn56xx;
+	struct cvmx_tim_reg_read_idx_s        cn56xxp1;
+	struct cvmx_tim_reg_read_idx_s        cn58xx;
+	struct cvmx_tim_reg_read_idx_s        cn58xxp1;
+	struct cvmx_tim_reg_read_idx_s        cn61xx;
+	struct cvmx_tim_reg_read_idx_s        cn63xx;
+	struct cvmx_tim_reg_read_idx_s        cn63xxp1;
+	struct cvmx_tim_reg_read_idx_s        cn66xx;
+	struct cvmx_tim_reg_read_idx_s        cn70xx;
+	struct cvmx_tim_reg_read_idx_s        cn70xxp1;
+	struct cvmx_tim_reg_read_idx_s        cnf71xx;
+};
+typedef union cvmx_tim_reg_read_idx cvmx_tim_reg_read_idx_t;
+
+/**
+ * cvmx_tim_ring#_aura
+ */
+union cvmx_tim_ringx_aura {
+	uint64_t u64;
+	struct cvmx_tim_ringx_aura_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t aura                         : 16; /**< Aura number used to free and return chucks to. Bits <15:12> must be zero. */
+#else
+	uint64_t aura                         : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_tim_ringx_aura_s          cn78xx;
+};
+typedef union cvmx_tim_ringx_aura cvmx_tim_ringx_aura_t;
+
+/**
+ * cvmx_tim_ring#_ctl0
+ *
+ * Notes:
+ * This CSR is a memory of 64 entries
+ * After a 1 to 0 transition on ENA, the HW will still complete a bucket traversal for the ring
+ * if it was pending or active prior to the transition. (SW must delay to ensure the completion
+ * of the traversal before reprogramming the ring.)
+ */
+union cvmx_tim_ringx_ctl0 {
+	uint64_t u64;
+	struct cvmx_tim_ringx_ctl0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_0_63                : 64;
+#else
+	uint64_t reserved_0_63                : 64;
+#endif
+	} s;
+	struct cvmx_tim_ringx_ctl0_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_47_63               : 17;
+	uint64_t ena                          : 1;  /**< Ring timer enable */
+	uint64_t intc                         : 2;  /**< Interval count for Error. Defines how many intervals
+                                                         could elapse from bucket expiration till actual
+                                                         bucket traversal before HW asserts an error.
+                                                         Typical value is 0,1,2. */
+	uint64_t timercount                   : 22; /**< Timer Count represents the ring offset; how many timer
+                                                         ticks have left till the interval expiration.
+                                                         Typical initialization value should be Interval/Constant,
+                                                         it is recommended that constant should be unique per ring
+                                                         This will create an offset between the rings.
+                                                         Once ENA is set,
+                                                         TIMERCOUNT counts down timer ticks. When TIMERCOUNT
+                                                         reaches zero, ring's interval expired and the HW forces
+                                                         a bucket traversal (and resets TIMERCOUNT to INTERVAL)
+                                                         TIMERCOUNT is unpredictable whenever ENA==0.
+                                                         It is SW responsibility to set TIMERCOUNT before
+                                                         TIM_RINGX_CTL0.ENA transitions from 0 to 1.
+                                                         When the field is set to X it would take X+1 timer tick
+                                                         for the interval to expire. */
+	uint64_t interval                     : 22; /**< Timer interval. Measured in Timer Ticks, where timer
+                                                         ticks are defined by TIM_FR_RN_TT.FR_RN_TT. */
+#else
+	uint64_t interval                     : 22;
+	uint64_t timercount                   : 22;
+	uint64_t intc                         : 2;
+	uint64_t ena                          : 1;
+	uint64_t reserved_47_63               : 17;
+#endif
+	} cn68xx;
+	struct cvmx_tim_ringx_ctl0_cn68xx     cn68xxp1;
+	struct cvmx_tim_ringx_ctl0_cn78xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t expire_offset                : 32; /**< Time at which the next bucket will be serviced, or offset. See also TIM_RING(0..63)_REL
+                                                         for the position relative to current time.
+                                                         If TIM_RING(0..63)_CTL1[ENA] = 0, then contains an offset. When ENA transitions from a
+                                                         zero to a one this offset will be added to the current time and loaded back into
+                                                         EXPIRE_OFFSET. Thus the offset sets the delta time between ENA transitioning to one and
+                                                         the very first time the ring will be serviced. Software should program different offsets
+                                                         on each ring to reduce congestion to prevent many rings from otherwise expiring
+                                                         concurrently.
+                                                         If TIM_RING(0..63)_CTL1[ENA] = 1, then contains the time the next bucket will be serviced.
+                                                         When EXPIRE_OFFSET reaches the current time (TIM_FR_RN_CYCLES or TIM_FR_RN_GPIOS),
+                                                         EXPIRE_OFFSET is set to the next expiration time (current time plus
+                                                         TIM_RING(0..63)_CTL0[INTERVAL]).
+                                                         EXPIRE_OFFSET is unpredictable after ENA_GPIO changes or TIM_RING(0..63)_CTL1[ENA]
+                                                         transitions from 1 to 0, and must be reprogrammed before (re-) setting
+                                                         TIM_RING(0..63)_CTL1[ENA]. */
+	uint64_t interval                     : 32; /**< Timer interval, measured in cycles or GPIO transitions.
+                                                         For every 64 entries in a bucket, the interval should be at least 1u. Minimal recommended
+                                                         value is 1u. */
+#else
+	uint64_t interval                     : 32;
+	uint64_t expire_offset                : 32;
+#endif
+	} cn78xx;
+};
+typedef union cvmx_tim_ringx_ctl0 cvmx_tim_ringx_ctl0_t;
+
+/**
+ * cvmx_tim_ring#_ctl1
+ *
+ * Notes:
+ * This CSR is a memory of 64 entries
+ * ***NOTE: Added fields in pass 2.0
+ */
+union cvmx_tim_ringx_ctl1 {
+	uint64_t u64;
+	struct cvmx_tim_ringx_ctl1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_51_63               : 13;
+	uint64_t rcf_busy                     : 1;  /**< Ring reconfiguration busy. When ENA is cleared, this bit will remain set until hardware
+                                                         completes the idling of the ring. ENA must not be re-enabled until clear. */
+	uint64_t intc                         : 2;  /**< Interval count for error. Defines how many intervals could elapse from bucket expiration
+                                                         until actual bucket traversal before hardware asserts an error. Typical value is 0x0, 0x1,
+                                                         0x2. */
+	uint64_t ena                          : 1;  /**< Ring timer enable. After a 1 to 0 transition on ENA, the hardware still completes a bucket
+                                                         traversal for the ring if it were pending or active prior to the transition. When
+                                                         clearing, software must delay until TIM_RING(0..63)_REL[RING_ESR] = 0 to ensure the
+                                                         completion of the traversal before reprogramming the ring. When setting, RCF_BUSY must be
+                                                         clear. */
+	uint64_t ena_gpio                     : 1;  /**< When set, ring's timer tick is generated by the GPIO timer. The GPIO edge is defined by
+                                                         TIM_REG_FLAGS[GPIO_EDGE]. The default value (zero) means that timer ticks are generated
+                                                         from the internal timer. To change ENA_GPIO:
+                                                         1. TIM_RING(0..63)_CTL1[ENA] is cleared.
+                                                         2. [ENA_GPIO] is changed.
+                                                         3. TIM_RING(0..63)_CTL0[EXPIRE_OFFSET] is reprogrammed appropriately.
+                                                         4. TIM_RING(0..63)_CTL1[ENA] is set. */
+	uint64_t ena_prd                      : 1;  /**< Enable periodic mode, which disables the memory write of zeros to NUM_ENTRIES and
+                                                         CHUNK_REMAINDER when a bucket is traversed. In periodic mode ENA_DFB and ENA_LDWB must
+                                                         also be clear. */
+	uint64_t reserved_44_44               : 1;
+	uint64_t ena_dfb                      : 1;  /**< Enable don't free buffer. When set, chunk buffer is not released by the TIM back to FPA. */
+	uint64_t cpool                        : 3;  /**< FPA Free list to free chunks to. */
+	uint64_t bucket                       : 20; /**< Current bucket. Should be set to 0x0 by software at enable time. Incremented once per
+                                                         bucket traversal. */
+	uint64_t bsize                        : 20; /**< Number of buckets minus one. If BSIZE = 0, there is only one bucket in the ring. */
+#else
+	uint64_t bsize                        : 20;
+	uint64_t bucket                       : 20;
+	uint64_t cpool                        : 3;
+	uint64_t ena_dfb                      : 1;
+	uint64_t reserved_44_44               : 1;
+	uint64_t ena_prd                      : 1;
+	uint64_t ena_gpio                     : 1;
+	uint64_t ena                          : 1;
+	uint64_t intc                         : 2;
+	uint64_t rcf_busy                     : 1;
+	uint64_t reserved_51_63               : 13;
+#endif
+	} s;
+	struct cvmx_tim_ringx_ctl1_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_47_63               : 17;
+	uint64_t ena_gpio                     : 1;  /**< When set, ring's timer tick will be generated by the
+                                                         GPIO Timer. GPIO edge is defined by
+                                                         TIM_REG_FLAGS.GPIO_EDGE
+                                                         Default value zero means that timer ticks will
+                                                         be genearated from the Internal Timer */
+	uint64_t ena_prd                      : 1;  /**< Enable Periodic Mode which would disable the memory
+                                                         write of zeros to num_entries and chunk_remainder
+                                                         when a bucket is traveresed. */
+	uint64_t ena_dwb                      : 1;  /**< When set, enables the use of Dont Write Back during
+                                                         FPA buffer freeing operations */
+	uint64_t ena_dfb                      : 1;  /**< Enable Don't Free Buffer. When set chunk buffer
+                                                         would not be released by the TIM back to FPA. */
+	uint64_t cpool                        : 3;  /**< FPA Free list to free chunks to. */
+	uint64_t bucket                       : 20; /**< Current bucket. Should be set to zero by SW at
+                                                         enable time.
+                                                         Incremented once per bucket traversal. */
+	uint64_t bsize                        : 20; /**< Number of buckets minus one. If BSIZE==0 there is
+                                                         only one bucket in the ring. */
+#else
+	uint64_t bsize                        : 20;
+	uint64_t bucket                       : 20;
+	uint64_t cpool                        : 3;
+	uint64_t ena_dfb                      : 1;
+	uint64_t ena_dwb                      : 1;
+	uint64_t ena_prd                      : 1;
+	uint64_t ena_gpio                     : 1;
+	uint64_t reserved_47_63               : 17;
+#endif
+	} cn68xx;
+	struct cvmx_tim_ringx_ctl1_cn68xxp1 {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_43_63               : 21;
+	uint64_t cpool                        : 3;  /**< FPA Free list to free chunks to. */
+	uint64_t bucket                       : 20; /**< Current bucket. Should be set to zero by SW at
+                                                         enable time.
+                                                         Incremented once per bucket traversal. */
+	uint64_t bsize                        : 20; /**< Number of buckets minus one. If BSIZE==0 there is
+                                                         only one bucket in the ring. */
+#else
+	uint64_t bsize                        : 20;
+	uint64_t bucket                       : 20;
+	uint64_t cpool                        : 3;
+	uint64_t reserved_43_63               : 21;
+#endif
+	} cn68xxp1;
+	struct cvmx_tim_ringx_ctl1_cn78xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_51_63               : 13;
+	uint64_t rcf_busy                     : 1;  /**< Ring reconfiguration busy. When ENA is cleared, this bit will remain set until hardware
+                                                         completes the idling of the ring. ENA must not be re-enabled until clear. */
+	uint64_t intc                         : 2;  /**< Interval count for error. Defines how many intervals could elapse from bucket expiration
+                                                         until actual bucket traversal before hardware asserts an error. Typical value is 0x0, 0x1,
+                                                         0x2. */
+	uint64_t ena                          : 1;  /**< Ring timer enable. After a 1 to 0 transition on ENA, the hardware still completes a bucket
+                                                         traversal for the ring if it were pending or active prior to the transition. When
+                                                         clearing, software must delay until TIM_RING(0..63)_REL[RING_ESR] = 0 to ensure the
+                                                         completion of the traversal before reprogramming the ring. When setting, RCF_BUSY must be
+                                                         clear. */
+	uint64_t ena_gpio                     : 1;  /**< When set, ring's timer tick is generated by the GPIO timer. The GPIO edge is defined by
+                                                         TIM_REG_FLAGS[GPIO_EDGE]. The default value (zero) means that timer ticks are generated
+                                                         from the internal timer. To change ENA_GPIO:
+                                                         1. TIM_RING(0..63)_CTL1[ENA] is cleared.
+                                                         2. [ENA_GPIO] is changed.
+                                                         3. TIM_RING(0..63)_CTL0[EXPIRE_OFFSET] is reprogrammed appropriately.
+                                                         4. TIM_RING(0..63)_CTL1[ENA] is set. */
+	uint64_t ena_prd                      : 1;  /**< Enable periodic mode, which disables the memory write of zeros to NUM_ENTRIES and
+                                                         CHUNK_REMAINDER when a bucket is traversed. In periodic mode ENA_DFB and ENA_LDWB must
+                                                         also be clear. */
+	uint64_t ena_ldwb                     : 1;  /**< When set, enables the use of Load and Don't-Write-Back when reading timer entry cache lines. */
+	uint64_t ena_dfb                      : 1;  /**< Enable don't free buffer. When set, chunk buffer is not released by the TIM back to FPA. */
+	uint64_t reserved_40_42               : 3;
+	uint64_t bucket                       : 20; /**< Current bucket. Should be set to 0x0 by software at enable time. Incremented once per
+                                                         bucket traversal. */
+	uint64_t bsize                        : 20; /**< Number of buckets minus one. If BSIZE = 0, there is only one bucket in the ring. */
+#else
+	uint64_t bsize                        : 20;
+	uint64_t bucket                       : 20;
+	uint64_t reserved_40_42               : 3;
+	uint64_t ena_dfb                      : 1;
+	uint64_t ena_ldwb                     : 1;
+	uint64_t ena_prd                      : 1;
+	uint64_t ena_gpio                     : 1;
+	uint64_t ena                          : 1;
+	uint64_t intc                         : 2;
+	uint64_t rcf_busy                     : 1;
+	uint64_t reserved_51_63               : 13;
+#endif
+	} cn78xx;
+};
+typedef union cvmx_tim_ringx_ctl1 cvmx_tim_ringx_ctl1_t;
+
+/**
+ * cvmx_tim_ring#_ctl2
+ *
+ * Notes:
+ * BASE is a 32-byte aligned pointer[35:0].  Only pointer[35:5] are stored because pointer[4:0] = 0.
+ * This CSR is a memory of 64 entries
+ */
+union cvmx_tim_ringx_ctl2 {
+	uint64_t u64;
+	struct cvmx_tim_ringx_ctl2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_0_63                : 64;
+#else
+	uint64_t reserved_0_63                : 64;
+#endif
+	} s;
+	struct cvmx_tim_ringx_ctl2_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_47_63               : 17;
+	uint64_t csize                        : 13; /**< Number of words per chunk. CSIZE mod(16) should be
+                                                         zero. */
+	uint64_t reserved_31_33               : 3;
+	uint64_t base                         : 31; /**< Pointer[35:5] to bucket[0] */
+#else
+	uint64_t base                         : 31;
+	uint64_t reserved_31_33               : 3;
+	uint64_t csize                        : 13;
+	uint64_t reserved_47_63               : 17;
+#endif
+	} cn68xx;
+	struct cvmx_tim_ringx_ctl2_cn68xx     cn68xxp1;
+	struct cvmx_tim_ringx_ctl2_cn78xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_53_63               : 11;
+	uint64_t csize                        : 13; /**< Number of eight-byte words per chunk. CSIZE mod(16) should be zero. */
+	uint64_t reserved_37_39               : 3;
+	uint64_t base                         : 37; /**< Pointer<41:5> to bucket<0>. */
+#else
+	uint64_t base                         : 37;
+	uint64_t reserved_37_39               : 3;
+	uint64_t csize                        : 13;
+	uint64_t reserved_53_63               : 11;
+#endif
+	} cn78xx;
+};
+typedef union cvmx_tim_ringx_ctl2 cvmx_tim_ringx_ctl2_t;
+
+/**
+ * cvmx_tim_ring#_dbg0
+ */
+union cvmx_tim_ringx_dbg0 {
+	uint64_t u64;
+	struct cvmx_tim_ringx_dbg0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t fr_rn_ht                     : 22; /**< Free Running Hardware Timer. Shared by all rings and is
+                                                         used to generate the Timer Tick based on
+                                                         FR_RN_TT. */
+	uint64_t timercount                   : 22; /**< Timer Count represents the ring's offset.
+                                                         Refer to TIM_RINGX_CTL0. */
+	uint64_t cur_bucket                   : 20; /**< Current bucket. Indicates the ring's current bucket.
+                                                         Refer to TIM_RINGX_CTL1.BUCKET. */
+#else
+	uint64_t cur_bucket                   : 20;
+	uint64_t timercount                   : 22;
+	uint64_t fr_rn_ht                     : 22;
+#endif
+	} s;
+	struct cvmx_tim_ringx_dbg0_s          cn68xx;
+	struct cvmx_tim_ringx_dbg0_s          cn68xxp1;
+};
+typedef union cvmx_tim_ringx_dbg0 cvmx_tim_ringx_dbg0_t;
+
+/**
+ * cvmx_tim_ring#_dbg1
+ */
+union cvmx_tim_ringx_dbg1 {
+	uint64_t u64;
+	struct cvmx_tim_ringx_dbg1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_2_63                : 62;
+	uint64_t ring_esr                     : 2;  /**< Ring Expiration Status Register.
+                                                         This register hold the expiration status of the ring.
+                                                         2'b00 - Ring was recently traversed.
+                                                         2'b01 - Interval expired. Ring is queued to be traversed.
+                                                         2'b10 - 1st interval expiration while ring is queued to be
+                                                         traversed.
+                                                         2'b11 - 2nd interval expiration while ring is queued to be
+                                                         traversed. */
+#else
+	uint64_t ring_esr                     : 2;
+	uint64_t reserved_2_63                : 62;
+#endif
+	} s;
+	struct cvmx_tim_ringx_dbg1_s          cn68xx;
+	struct cvmx_tim_ringx_dbg1_s          cn68xxp1;
+};
+typedef union cvmx_tim_ringx_dbg1 cvmx_tim_ringx_dbg1_t;
+
+/**
+ * cvmx_tim_ring#_rel
+ *
+ * Current positions of the TIM walker in both time and ring position, for easy synchronization
+ * with software.
+ */
+union cvmx_tim_ringx_rel {
+	uint64_t u64;
+	struct cvmx_tim_ringx_rel_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t cur_bucket                   : 20; /**< Current bucket. Indicates the ring's current bucket. See TIM_RING(0..63)_CTL1[BUCKET]. */
+	uint64_t reserved_34_43               : 10;
+	uint64_t ring_esr                     : 2;  /**< Ring expiration status register. These registers hold the expiration status of the ring.
+                                                         0x0 = Ring has not expired.
+                                                         0x1 = Interval expired. Ring is queued to be traversed.
+                                                         0x2 = First interval expiration while ring is queued to be traversed.
+                                                         0x3 = Second interval expiration while ring is queued to be traversed.
+                                                         This field is zeroed when TIM_RING(0..63)_CTL1[ENA] transitions from 0 -> 1. */
+	uint64_t timercount                   : 32; /**< Timer count indicates how many timer ticks are left until the interval expiration,
+                                                         calculated as TIM_RING(0..63)_CTL0[EXPIRE_OFFSET] minus current time (TIM_FR_RN_CYCLES or
+                                                         TIM_FR_RN_GPIOS).
+                                                         Once ENA = 1, TIMERCOUNT will be observed to count down timer ticks. When TIMERCOUNT
+                                                         reaches 0x0, the ring's interval expired and the hardware forces a bucket traversal (and
+                                                         increments RING_ESR).
+                                                         Typical initialization value should be interval/constant; Cavium recommends that the
+                                                         constant be unique per ring. This creates an offset between the rings.
+                                                         TIMERCOUNT becomes and remains unpredictable whenever ENA = 0 or ENA_GPIO changes. It is
+                                                         software's responsibility to set TIMERCOUNT before TIM_RING(0..63)_CTL1[ENA] transitions
+                                                         from 0 -> 1. */
+#else
+	uint64_t timercount                   : 32;
+	uint64_t ring_esr                     : 2;
+	uint64_t reserved_34_43               : 10;
+	uint64_t cur_bucket                   : 20;
+#endif
+	} s;
+	struct cvmx_tim_ringx_rel_s           cn78xx;
+};
+typedef union cvmx_tim_ringx_rel cvmx_tim_ringx_rel_t;
+
+#endif
diff --git a/arch/mips/include/asm/octeon/cvmx-wqe.h b/arch/mips/include/asm/octeon/cvmx-wqe.h
index f527dc5..ac5538a 100644
--- a/arch/mips/include/asm/octeon/cvmx-wqe.h
+++ b/arch/mips/include/asm/octeon/cvmx-wqe.h
@@ -1015,6 +1015,7 @@ typedef struct {
 	/**
          * WORD 5/6/7 may be extended here if WQE_HSZ is set to !0
 	 */
+        uint64_t        wqe_data[10];
 
 } CVMX_CACHE_LINE_ALIGNED cvmx_wqe_78xx_t;
 
@@ -1570,7 +1571,7 @@ static inline cvmx_buf_ptr_pki_t cvmx_wqe_get_pki_pkt_ptr(cvmx_wqe_t *work)
  *
  * @return Returns the actual resulting value in the WQE fielda
  *
- */ 
+ */
 static inline unsigned cvmx_wqe_set_bufs(cvmx_wqe_t *work, unsigned bufs)
 {
 	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
@@ -1752,7 +1753,7 @@ static inline void cvmx_wqe_set_l3_frag(cvmx_wqe_t *work, bool set)
 /**
  * Return true if the packet Layer-3 protocol is ARP.
  */
-static inline bool cvmx_wqe_is_l3_arp(cvmx_wqe_t *work) 
+static inline bool cvmx_wqe_is_l3_arp(cvmx_wqe_t *work)
 {
 	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
 		cvmx_wqe_78xx_t *wqe = (void *)work;
diff --git a/arch/mips/include/asm/octeon/octeon-feature.h b/arch/mips/include/asm/octeon/octeon-feature.h
index 718622d..9f0cdbf 100644
--- a/arch/mips/include/asm/octeon/octeon-feature.h
+++ b/arch/mips/include/asm/octeon/octeon-feature.h
@@ -384,6 +384,7 @@ static inline int octeon_has_feature_OCTEON_FEATURE_MMC(void)
 {
 	return (OCTEON_IS_MODEL(OCTEON_CN61XX)
 		|| OCTEON_IS_MODEL(OCTEON_CNF71XX)
+		|| OCTEON_IS_MODEL(OCTEON_CN78XX)
 		|| OCTEON_IS_MODEL(OCTEON_CN70XX));
 }
 
-- 
2.6.2

