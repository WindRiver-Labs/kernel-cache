From 33f7e33d47c6891ffadb4b139ed619da083eea6c Mon Sep 17 00:00:00 2001
From: Chandrakala Chavva <cchavva@caviumnetworks.com>
Date: Thu, 24 Jul 2014 18:05:50 -0700
Subject: [PATCH 942/974] MIPS:OCTEON: Sync-up SE files as of -r125784

Signed-off-by: Chandrakala Chavva <cchavva@caviumnetworks.com>
[Original patch taken from Cavium SDK 3.1.2-568]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
---
 .../mips/cavium-octeon/executive/cvmx-helper-bgx.c | 210 +++-----
 .../mips/cavium-octeon/executive/cvmx-helper-ilk.c |  11 +
 .../mips/cavium-octeon/executive/cvmx-helper-pki.c | 255 +++++++++-
 .../cavium-octeon/executive/cvmx-helper-pko3.c     | 111 +++--
 arch/mips/cavium-octeon/executive/cvmx-helper.c    |   3 +-
 arch/mips/cavium-octeon/executive/cvmx-ila.c       |  45 +-
 .../cavium-octeon/executive/cvmx-pki-resources.c   |   2 +-
 arch/mips/cavium-octeon/executive/cvmx-pki.c       | 554 ++++++++++++++++++---
 arch/mips/cavium-octeon/executive/cvmx-pko3.c      |  25 +-
 arch/mips/include/asm/octeon/cvmx-ase-defs.h       |  14 +-
 arch/mips/include/asm/octeon/cvmx-bootmem.h        |   2 +-
 arch/mips/include/asm/octeon/cvmx-dpi-defs.h       |   2 +-
 arch/mips/include/asm/octeon/cvmx-gserx-defs.h     |   9 +-
 arch/mips/include/asm/octeon/cvmx-helper-bgx.h     |   1 +
 arch/mips/include/asm/octeon/cvmx-helper-pki.h     |   9 +
 arch/mips/include/asm/octeon/cvmx-lmcx-defs.h      | 373 +++++++++++---
 arch/mips/include/asm/octeon/cvmx-mio-defs.h       |  30 +-
 arch/mips/include/asm/octeon/cvmx-pip.h            |   5 +-
 arch/mips/include/asm/octeon/cvmx-pki.h            | 280 +----------
 arch/mips/include/asm/octeon/cvmx-pow.h            |  63 ++-
 arch/mips/include/asm/octeon/cvmx-sli-defs.h       |  63 ++-
 arch/mips/include/asm/octeon/cvmx-sriox-defs.h     |  59 ++-
 arch/mips/include/asm/octeon/cvmx-wqe.h            |  52 +-
 arch/mips/include/asm/octeon/octeon-model.h        |   4 +-
 24 files changed, 1428 insertions(+), 754 deletions(-)

diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-bgx.c b/arch/mips/cavium-octeon/executive/cvmx-helper-bgx.c
index ea7f815..c40325e 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-bgx.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-bgx.c
@@ -67,37 +67,6 @@
 static const int debug = 0;
 
 /**
- * Delay after enabling an interface based on the mode.  Different modes take
- * different amounts of time.
- */
-static void
-__cvmx_helper_bgx_interface_enable_delay(cvmx_helper_interface_mode_t mode)
-{
-	/* Don't delay if we running under the simulator. */
-	if (cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_SIM)
-		return;
-
-	switch (mode) {
-	case CVMX_HELPER_INTERFACE_MODE_10G_KR:
-	case CVMX_HELPER_INTERFACE_MODE_40G_KR4:
-	case CVMX_HELPER_INTERFACE_MODE_XLAUI:
-	case CVMX_HELPER_INTERFACE_MODE_XFI:
-		cvmx_wait_usec(250000);
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_RXAUI:
-	case CVMX_HELPER_INTERFACE_MODE_XAUI:
-		cvmx_wait_usec(100000);
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_SGMII:
-		cvmx_wait_usec(50000);
-		break;
-	default:
-		cvmx_wait_usec(50000);
-		break;
-	}
-}
-
-/**
  * @INTERNAL
  *
  * Returns number of ports based on interface
@@ -188,8 +157,6 @@ void cvmx_helper_bgx_disable(int xipd_port)
 	cvmx_bgxx_cmrx_config_t cmr_config;
 
 	cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface));
-	if (!OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_0) || index)
-		cmr_config.s.enable = 0;
 	if (debug)
 		cvmx_dprintf("%s: Disabling tx and rx packets on xipd port 0x%x\n",
 			     __func__, xipd_port);
@@ -625,7 +592,6 @@ static int __cvmx_helper_bgx_sgmii_hardware_init_link_speed(int xiface,
 							    int index,
 							    cvmx_helper_link_info_t link_info)
 {
-	int is_enabled = 1;
 	cvmx_bgxx_cmrx_config_t cmr_config;
 	cvmx_bgxx_gmp_pcs_miscx_ctl_t gmp_miscx_ctl;
 	cvmx_bgxx_gmp_gmi_prtx_cfg_t gmp_prtx_cfg;
@@ -639,14 +605,11 @@ static int __cvmx_helper_bgx_sgmii_hardware_init_link_speed(int xiface,
 		cvmx_dprintf("%s: interface %u:%d/%d\n",
 		__func__, xi.node, xi.interface, index);
 
-	/* Errata bgx-22429*/
-	if (!OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X) || index) {
-		/* Disable GMX before we make any changes. Remember the enable state */
-		cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface));
-		is_enabled = cmr_config.s.enable;
-		cmr_config.s.enable = 0;
-		cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface), cmr_config.u64);
-	}
+	/* Disable GMX before we make any changes. */
+	cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface));
+	cmr_config.s.data_pkt_tx_en = 0;
+	cmr_config.s.data_pkt_rx_en = 0;
+	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface), cmr_config.u64);
 
 	/* Wait for GMX to be idle */
 	if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_GMP_GMI_PRTX_CFG(index, xi.interface),
@@ -724,13 +687,10 @@ static int __cvmx_helper_bgx_sgmii_hardware_init_link_speed(int xiface,
 	/* Read GMX CFG again to make sure the config completed */
 	cvmx_read_csr_node(node, CVMX_BGXX_GMP_GMI_PRTX_CFG(index, xi.interface));
 
-	/* Restore the enabled/disabled state */
-	/* bgx-22429 */
+	/* Enable back BGX. */
 	cmr_config.u64 = cvmx_read_csr_node(node,
 					    CVMX_BGXX_CMRX_CONFIG(index,
 								  xi.interface));
-	if (!OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X) || index)
-		cmr_config.s.enable = is_enabled;
 	if (debug)
 		cvmx_dprintf("%s: Enabling tx and rx packets on %d:%d\n",
 			     __func__, xi.interface, index);
@@ -825,51 +785,6 @@ cvmx_helper_link_info_t __cvmx_helper_bgx_sgmii_link_get(int xipd_port)
 }
 
 /**
- * This works around an errata where disabling LMAC 0 resets the RX_FIFO read
- * pointers for all LMACs.  This only affects CN78XX pass 1.0 and 1.1.
- *
- * @param xipd_port	ipd port number
- * @param link_up	1 if link isup, 0 if link down
- *
- * @return	0 for success, -1 on error
- */
-int cvmx_helper_bgx_errata_22429(int xipd_port, int link_up)
-{
-	cvmx_bgxx_cmrx_config_t cmr_config;
-	int xiface = cvmx_helper_get_interface_num(xipd_port);
-	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
-	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(xipd_port);
-	int interface = xi.interface;
-	int node = xi.node;
-	int index = cvmx_helper_get_interface_index_num(xp.port);
-	int num_ports = cvmx_helper_ports_on_interface(xiface);
-
-	/* Errata does not apply for only 1 LMAC */
-	if (num_ports == 1 || !OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X))
-		return 0;
-
-	/* Errata BGX-22429 */
-	if (link_up) {
-		if (!index) /*does not apply for port 0*/
-			return 0;
-		cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(0, interface));
-		if (!cmr_config.s.enable) {
-			cmr_config.s.enable = 1;
-			cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(0, interface), cmr_config.u64);
-		}
-	} else {
-		if (index) /*does not apply to port 1-3 */
-			return 0;
-		while (--num_ports) {
-			cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(num_ports, interface));
-			if (cmr_config.s.enable)
-				return -1;
-		}
-	}
-	return 0;
-}
-
-/**
  * This sequence brings down the link for the XCV RGMII interface
  *
  * @param interface	Interface (BGX) number.  Port index is always 0
@@ -911,10 +826,6 @@ static void __cvmx_helper_bgx_rgmii_link_set_down(int interface)
 	mr_control.s.pwr_dn = 1;
 	cvmx_write_csr(CVMX_BGXX_GMP_PCS_MRX_CONTROL(0, interface),
 		       mr_control.u64);
-
-	cmr_config.u64 = cvmx_read_csr(CVMX_BGXX_CMRX_CONFIG(0, interface));
-	cmr_config.s.enable = 0;
-	cvmx_write_csr(CVMX_BGXX_CMRX_CONFIG(0, interface), cmr_config.u64);
 }
 
 /**
@@ -994,16 +905,19 @@ int __cvmx_helper_bgx_sgmii_link_set(int xipd_port,
 		cvmx_dprintf("%s: interface %u:%d/%d\n",
 		__func__, xi.node, xi.interface, index);
 
-	if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X)) {
-		cvmx_helper_bgx_errata_22429(xipd_port, link_info.s.link_up);
-	}
-
 	cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index,
 									iface));
 	if (link_info.s.link_up) {
 		cmr_config.s.enable = 1;
 		cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, iface),
 				    cmr_config.u64);
+		/* Apply workaround for errata BGX-22429 */
+		if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X) && index) {
+			cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(0, iface));
+			cmr_config.s.enable = 1;
+			cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(0, iface),
+					    cmr_config.u64);
+		}
 		__cvmx_helper_bgx_sgmii_hardware_init_link(xiface, index);
 	} else if (cvmx_helper_bgx_is_rgmii(xi.interface, index)) {
 		if (debug)
@@ -1050,7 +964,6 @@ static int __cvmx_helper_bgx_xaui_init(int index, int xiface)
 	int interface = xi.interface;
 	int node = xi.node;
 	int use_auto_neg = 0;
-	int xipd_port = cvmx_helper_get_ipd_port(xiface, index);
 
 	if (debug)
 		cvmx_dprintf("%s: interface %u:%d/%d\n",
@@ -1085,11 +998,6 @@ static int __cvmx_helper_bgx_xaui_init(int index, int xiface)
 		/* 2. Write BGX(0..5)_CMR(0..3)_CONFIG[ENABLE] to 0,
 		      BGX(0..5)_SPU(0..3)_CONTROL1[LO_PWR] = 1 and
 		      BGX(0..5)_SPU(0..3)_MISC_CONTROL[RX_PACKET_DIS] = 1. */
-		if (!cvmx_helper_bgx_errata_22429(xipd_port, 0)) {
-			cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface));
-			cmr_config.s.enable = 0;
-			cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface), cmr_config.u64);
-		}
 		spu_control1.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, xi.interface));
 		spu_control1.s.lo_pwr = 1;
 		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, xi.interface), spu_control1.u64);
@@ -1188,7 +1096,6 @@ static int __cvmx_helper_bgx_xaui_init(int index, int xiface)
 
 		/* 3h. Set BGX(0..5)_CMR(0..3)_CONFIG[ENABLE] = 1 and
 		    BGX(0..5)_SPU(0..3)_CONTROL1[LO_PWR] = 0 to enable the LMAC. */
-		cvmx_helper_bgx_errata_22429(xipd_port, 1);
 		cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface));
 		cmr_config.s.enable = 1;
 		cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface), cmr_config.u64);
@@ -1222,8 +1129,6 @@ static int __cvmx_helper_bgx_xaui_init(int index, int xiface)
 		dbg_control.s.us_clk_period = clock_mhz - 1;
 		cvmx_write_csr_node(node, CVMX_BGXX_SPU_DBG_CONTROL(xi.interface), dbg_control.u64);
 	}
-	/* The PHY often takes at least 100ms to stabilize */
-	__cvmx_helper_bgx_interface_enable_delay(mode);
 	return 0;
 }
 
@@ -1343,7 +1248,7 @@ int __cvmx_helper_bgx_port_init(int xipd_port, int phy_pres)
 			cvmx_write_csr(CVMX_XCV_RESET, xcv_reset.u64);
 		}
 	} else {
-		int res;
+		int res, lmacs, cred;
 		cvmx_bgxx_smux_tx_thresh_t smu_tx_thresh;
 
 		res = __cvmx_helper_bgx_xaui_init(index, xiface);
@@ -1351,14 +1256,18 @@ int __cvmx_helper_bgx_port_init(int xipd_port, int phy_pres)
 			/*cvmx_dprintf("Failed to enable XAUI for %d:BGX(%d,%d)\n", xi.node, xi.interface, index);*/
 			return res;
 		}
-
+		/* Threshold value is calculated based on the PKO_MCI1_MAX_CRED(x)
+		 * description in HRM.*/
+		lmacs = __cvmx_helper_bgx_enumerate(xi.interface);
+		cred = CVMX_BGX_TX_FIFO_SIZE / lmacs / 16;
 		smu_tx_thresh.u64 = 0;
-		/* Hopefully big enough to avoid underrun, but not too
-		* big to adversly effect shaping.
-		*/
-		smu_tx_thresh.s.cnt = 0x100;
+		smu_tx_thresh.s.cnt = cred - 10;
 		cvmx_write_csr_node(xi.node, CVMX_BGXX_SMUX_TX_THRESH(index, xi.interface),
 				    smu_tx_thresh.u64);
+		if (debug)
+			cvmx_dprintf("%s: BGX%d:%d TX-thresh=%d\n",
+				__func__, xi.interface, index, smu_tx_thresh.s.cnt);
+
 		/* Set disparity for RXAUI interface as described in the
 		Marvell RXAUI Interface specification. */
 		if (mode == CVMX_HELPER_INTERFACE_MODE_RXAUI && phy_pres) {
@@ -1440,7 +1349,6 @@ int __cvmx_helper_bgx_sgmii_configure_loopback(int xipd_port, int enable_interna
 static int __cvmx_helper_bgx_xaui_link_init(int index, int xiface)
 {
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
-	int interface = xi.interface;
 	int node = xi.node;
 	cvmx_bgxx_spux_status1_t spu_status1;
 	cvmx_bgxx_spux_status2_t spu_status2;
@@ -1455,7 +1363,7 @@ static int __cvmx_helper_bgx_xaui_link_init(int index, int xiface)
 		cvmx_dprintf("%s: interface %u:%d/%d\n",
 		__func__, xi.node, xi.interface, index);
 
-	rgmii_first = cvmx_helper_bgx_is_rgmii(interface, index);
+	rgmii_first = cvmx_helper_bgx_is_rgmii(xi.interface, index);
 
 	mode = cvmx_helper_bgx_get_mode(xiface, index);
 	if (mode == CVMX_HELPER_INTERFACE_MODE_10G_KR
@@ -1475,33 +1383,33 @@ static int __cvmx_helper_bgx_xaui_link_init(int index, int xiface)
 		cvmx_bgxx_spux_br_pmd_control_t pmd_control;
 
 		spu_an_control.u64 = cvmx_read_csr_node(node,
-					CVMX_BGXX_SPUX_AN_CONTROL(index, interface));
+					CVMX_BGXX_SPUX_AN_CONTROL(index, xi.interface));
 		if (spu_an_control.s.an_en) {
 			if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X)) {
 				cvmx_bgxx_spux_int_t spu_int;
 				spu_int.u64 = cvmx_read_csr_node(node,
-						CVMX_BGXX_SPUX_INT(index, interface));
+						CVMX_BGXX_SPUX_INT(index, xi.interface));
 				if (!spu_int.s.an_link_good) {
 					/* Clear the auto negotiation (W1C) */
 					spu_int.u64 = 0;
 					spu_int.s.an_complete = 1;
 					spu_int.s.an_link_good = 1;
 					spu_int.s.an_page_rx = 1;
-					cvmx_write_csr_node(node, CVMX_BGXX_SPUX_INT(index, interface), spu_int.u64);
+					cvmx_write_csr_node(node, CVMX_BGXX_SPUX_INT(index, xi.interface), spu_int.u64);
 					/* Restart auto negotiation */
-					spu_an_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_AN_CONTROL(index, interface));
+					spu_an_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_AN_CONTROL(index, xi.interface));
 					spu_an_control.s.an_restart = 1;
-					cvmx_write_csr_node(node, CVMX_BGXX_SPUX_AN_CONTROL(index, interface), spu_an_control.u64);
+					cvmx_write_csr_node(node, CVMX_BGXX_SPUX_AN_CONTROL(index, xi.interface), spu_an_control.u64);
 					return -1;
 				}
 			} else {
 				spu_an_status.u64 = cvmx_read_csr_node(node,
-						CVMX_BGXX_SPUX_AN_STATUS(index, interface));
+						CVMX_BGXX_SPUX_AN_STATUS(index, xi.interface));
 				if (!spu_an_status.s.an_complete) {
 					/* Restart auto negotiation */
-					spu_an_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_AN_CONTROL(index, interface));
+					spu_an_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_AN_CONTROL(index, xi.interface));
 					spu_an_control.s.an_restart = 1;
-					cvmx_write_csr_node(node, CVMX_BGXX_SPUX_AN_CONTROL(index, interface), spu_an_control.u64);
+					cvmx_write_csr_node(node, CVMX_BGXX_SPUX_AN_CONTROL(index, xi.interface), spu_an_control.u64);
 					return -1;
 				}
 			}
@@ -1509,15 +1417,15 @@ static int __cvmx_helper_bgx_xaui_link_init(int index, int xiface)
 
 		if (use_training) {
 			spu_int.u64 = cvmx_read_csr_node(node,
-						  CVMX_BGXX_SPUX_INT(index, interface));
+						  CVMX_BGXX_SPUX_INT(index, xi.interface));
 			pmd_control.u64 = cvmx_read_csr_node(node,
-						CVMX_BGXX_SPUX_BR_PMD_CONTROL(index, interface));
+						CVMX_BGXX_SPUX_BR_PMD_CONTROL(index, xi.interface));
 			if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X)
 			    && pmd_control.s.train_en == 0) {
-				__cvmx_bgx_start_training(node, interface, index);
+				__cvmx_bgx_start_training(node, xi.interface, index);
 				return -1;
 			} else if (spu_int.s.training_failure) {
-				__cvmx_bgx_restart_training(node, interface, index);
+				__cvmx_bgx_restart_training(node, xi.interface, index);
 				return -1;
 			}
 			if (!spu_int.s.training_done) {
@@ -1541,7 +1449,8 @@ static int __cvmx_helper_bgx_xaui_link_init(int index, int xiface)
 				/* If BGX2 uses both dlms, then configure other dlm also. */
 				if (OCTEON_IS_MODEL(OCTEON_CN73XX) && xi.interface == 2)
 					__cvmx_qlm_rx_equalization(node, 6, lane);
-			} else if (CVMX_HELPER_INTERFACE_MODE_RXAUI) { // RXAUI
+			/* RXAUI */
+			} else if (mode == CVMX_HELPER_INTERFACE_MODE_RXAUI) {
 				lane = index * 2;
 				if (OCTEON_IS_MODEL(OCTEON_CN73XX)
 				    && index >= 2
@@ -1552,13 +1461,14 @@ static int __cvmx_helper_bgx_xaui_link_init(int index, int xiface)
 					lane--;
 				__cvmx_qlm_rx_equalization(node, qlm, lane);
 				__cvmx_qlm_rx_equalization(node, qlm, lane + 1);
-			} else if (cmr_config.s.lmac_type != 5) { // !RGMII
+			/* XFI */
+			} else if (cmr_config.s.lmac_type != 5) {
 				if (rgmii_first)
 					lane--;
 				if (OCTEON_IS_MODEL(OCTEON_CN73XX)
 				    && index >= 2
 				    && xi.interface == 2) {
-					lane = 0;
+					lane = index - 2;
 				}
 				__cvmx_qlm_rx_equalization(node, qlm, lane);
 			}
@@ -1566,7 +1476,7 @@ static int __cvmx_helper_bgx_xaui_link_init(int index, int xiface)
 
 		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SPUX_CONTROL1(index, xi.interface),
 					  cvmx_bgxx_spux_control1_t, reset, ==, 0, 10000)) {
-			cvmx_dprintf("ERROR: %d:BGX%d:%d: PCS in reset", node, interface, index);
+			cvmx_dprintf("ERROR: %d:BGX%d:%d: PCS in reset", node, xi.interface, index);
 			return -1;
 		}
 
@@ -1576,7 +1486,7 @@ static int __cvmx_helper_bgx_xaui_link_init(int index, int xiface)
 		    || mode == CVMX_HELPER_INTERFACE_MODE_40G_KR4) {
 			if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SPUX_BR_STATUS1(index, xi.interface),
 					  cvmx_bgxx_spux_br_status1_t, blk_lock, ==, 1, 10000)) {
-				//cvmx_dprintf("ERROR: %d:BGX%d:%d: BASE-R PCS block not locked\n", node, interface, index);
+				/* cvmx_dprintf("ERROR: %d:BGX%d:%d: BASE-R PCS block not locked\n", node, xi.interface, index); */
                 		return -1;
 			}
 		} else {
@@ -1584,7 +1494,7 @@ static int __cvmx_helper_bgx_xaui_link_init(int index, int xiface)
 			/* Wait for PCS to be aligned */
 			if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SPUX_BX_STATUS(index, xi.interface),
 				  cvmx_bgxx_spux_bx_status_t, alignd, ==, 1, 10000)) {
-				cvmx_dprintf("ERROR: %d:BGX%d:%d: PCS not aligned\n", node, interface, index);
+				cvmx_dprintf("ERROR: %d:BGX%d:%d: PCS not aligned\n", node, xi.interface, index);
 				return -1;
 			}
 		}
@@ -1597,9 +1507,9 @@ static int __cvmx_helper_bgx_xaui_link_init(int index, int xiface)
 		spu_status2.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_STATUS2(index, xi.interface));
 		if (spu_status2.s.rcvflt) {
 			cvmx_dprintf("ERROR: %d:BGX%d:%d: Receive fault, need to retry\n",
-					node, interface, index);
+					node, xi.interface, index);
 			if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X) && use_training)
-				__cvmx_bgx_restart_training(node, interface, index);
+				__cvmx_bgx_restart_training(node, xi.interface, index);
 			/* cvmx_dprintf("training restarting\n"); */
 			return -1;
 		}
@@ -1607,28 +1517,28 @@ static int __cvmx_helper_bgx_xaui_link_init(int index, int xiface)
 		/* Wait for MAC RX to be ready */
 		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SMUX_RX_CTL(index, xi.interface),
 					  cvmx_bgxx_smux_rx_ctl_t, status, ==, 0, 10000)) {
-			cvmx_dprintf("ERROR: %d:BGX%d:%d: RX not ready\n", node, interface, index);
+			cvmx_dprintf("ERROR: %d:BGX%d:%d: RX not ready\n", node, xi.interface, index);
 			return -1;
 		}
 
 		/* Wait for BGX RX to be idle */
 		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SMUX_CTRL(index, xi.interface),
 				  cvmx_bgxx_smux_ctrl_t, rx_idle, ==, 1, 10000)) {
-			cvmx_dprintf("ERROR: %d:BGX%d:%d: RX not idle\n", node, interface, index);
+			cvmx_dprintf("ERROR: %d:BGX%d:%d: RX not idle\n", node, xi.interface, index);
 			return -1;
 		}
 
 		/* Wait for GMX TX to be idle */
 		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SMUX_CTRL(index, xi.interface),
 				  cvmx_bgxx_smux_ctrl_t, tx_idle, ==, 1, 10000)) {
-			cvmx_dprintf("ERROR: %d:BGX%d:%d: TX not idle\n", node, interface, index);
+			cvmx_dprintf("ERROR: %d:BGX%d:%d: TX not idle\n", node, xi.interface, index);
 			return -1;
 		}
 
 		/* rcvflt should still be 0 */
 		spu_status2.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_STATUS2(index, xi.interface));
 		if (spu_status2.s.rcvflt) {
-			cvmx_dprintf("ERROR: %d:BGX%d:%d: Receive fault, need to retry\n", node, interface, index);
+			cvmx_dprintf("ERROR: %d:BGX%d:%d: Receive fault, need to retry\n", node, xi.interface, index);
 			return -1;
 		}
 
@@ -1639,7 +1549,7 @@ static int __cvmx_helper_bgx_xaui_link_init(int index, int xiface)
 
 		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SPUX_STATUS1(index, xi.interface),
 				cvmx_bgxx_spux_status1_t, rcv_lnk, ==, 1, 10000)) {
-			cvmx_dprintf("ERROR: %d:BGX%d:%d: Receive link down\n", node, interface, index);
+			cvmx_dprintf("ERROR: %d:BGX%d:%d: Receive link down\n", node, xi.interface, index);
 			return -1;
 		}
 	}
@@ -1656,7 +1566,6 @@ static int __cvmx_helper_bgx_xaui_link_init(int index, int xiface)
 	cmr_config.s.data_pkt_rx_en = 1;
 	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface),
 			    cmr_config.u64);
-
 	return 0;
 }
 
@@ -2109,7 +2018,6 @@ void cvmx_helper_bgx_set_mac(int xipd_port, int bcst, int mcst, uint64_t mac)
 	cvmx_bgxx_cmrx_rx_adr_ctl_t adr_ctl;
 	cvmx_bgxx_cmrx_config_t cmr_config;
 	int saved_state;
-	cvmx_helper_interface_mode_t mode;
 
 	index = cvmx_helper_get_interface_index_num(xipd_port);
 
@@ -2121,8 +2029,9 @@ void cvmx_helper_bgx_set_mac(int xipd_port, int bcst, int mcst, uint64_t mac)
 		__func__, xi.node, xi.interface, index);
 
 	cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface));
-	saved_state = cmr_config.s.enable;
-	cmr_config.s.enable = 0;
+	saved_state = cmr_config.s.data_pkt_tx_en;
+	cmr_config.s.data_pkt_tx_en = 0;
+	cmr_config.s.data_pkt_rx_en = 0;
 	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface), cmr_config.u64);
 
 	/* Set the mac */
@@ -2148,13 +2057,9 @@ void cvmx_helper_bgx_set_mac(int xipd_port, int bcst, int mcst, uint64_t mac)
 	cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_SMACX(index, xi.interface), mac);
 
 	/* Restore back the interface state */
-	cmr_config.s.enable = saved_state;
+	cmr_config.s.data_pkt_tx_en = saved_state;
+	cmr_config.s.data_pkt_rx_en = 1;
 	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface), cmr_config.u64);
-	/* Wait 100ms after bringing up the link to give the PHY some time */
-	if (cmr_config.s.enable) {
-		mode = cvmx_helper_bgx_get_mode(xiface, index);
-		__cvmx_helper_bgx_interface_enable_delay(mode);
-	}
 }
 
 /**
@@ -2253,7 +2158,8 @@ int cvmx_helper_bgx_shutdown_port(int xiface, int index)
 	cmr_config.u64 = cvmx_read_csr_node(node,
 		CVMX_BGXX_CMRX_CONFIG(index, xi.interface));
 
-	cmr_config.s.enable = 0;
+	cmr_config.s.data_pkt_tx_en = 0;
+	cmr_config.s.data_pkt_rx_en = 0;
 	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface),
 		cmr_config.u64);
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-ilk.c b/arch/mips/cavium-octeon/executive/cvmx-helper-ilk.c
index fcba07d..019b917 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-ilk.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-ilk.c
@@ -480,6 +480,8 @@ int __cvmx_helper_ilk_probe(int xiface)
 	if (interface >= CVMX_NUM_ILK_INTF)
 		return 0;
 
+
+
 	/* the configuration should be done only once */
 	if (cvmx_ilk_get_intf_ena(xiface))
 		return cvmx_ilk_chans[xi.node][interface];
@@ -818,6 +820,11 @@ retry:
 		//cvmx_dprintf("ILK%d: Lane alignment complete\n", interface);
 	}
 
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+	/* Enable error interrupts, now link is up */
+	cvmx_error_enable_group(CVMX_ERROR_GROUP_ILK, 0);
+#endif
+
 	result.s.link_up = 1;
 	result.s.full_duplex = 1;
  	if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
@@ -845,6 +852,10 @@ fail:
 					cvmx_write_csr(CVMX_ILK_RX_LNEX_INT_EN(i), ~0x1ff);
 			}
 		}
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+		/* Disable error interrupts */
+		cvmx_error_enable_group(CVMX_ERROR_GROUP_ILK, 0);
+#endif
 	}
 
 	return result;
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-pki.c b/arch/mips/cavium-octeon/executive/cvmx-helper-pki.c
index 90d4b07..1670967 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-pki.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-pki.c
@@ -71,6 +71,7 @@ static CVMX_SHARED int pki_helper_debug;
 
 CVMX_SHARED bool cvmx_pki_dflt_init[CVMX_MAX_NODES] = {[0 ... CVMX_MAX_NODES-1] = 1};
 
+static CVMX_SHARED bool cvmx_pki_dflt_bp_en[CVMX_MAX_NODES] = {[0 ... CVMX_MAX_NODES - 1] = true};
 static CVMX_SHARED struct cvmx_pki_cluster_grp_config pki_dflt_clgrp[CVMX_MAX_NODES] = {
 	{0, 0xf},
 	{0, 0xf}
@@ -102,6 +103,8 @@ CVMX_SHARED uint64_t pkind_style_map[CVMX_MAX_NODES][CVMX_PKI_NUM_PKIND] = { [0
 /* To store the qos watcher values before they are written to pcam when watcher is enabled
 There is no cvmx-pip.c file exist so it ended up here */
 CVMX_SHARED struct cvmx_pki_legacy_qos_watcher qos_watcher[8];
+CVMX_SHARED uint64_t pcam_dmach[CVMX_PKI_NUM_PCAM_ENTRY] = {-1};
+CVMX_SHARED uint64_t pcam_dmacl[CVMX_PKI_NUM_PCAM_ENTRY] = {-1};
 
 
 /** @INTERNAL
@@ -512,6 +515,11 @@ void cvmx_helper_pki_no_dflt_init(int node)
 	cvmx_pki_dflt_init[node] = 0;
 }
 
+void cvmx_helper_pki_set_dflt_bp_en(int node, bool bp_en)
+{
+	cvmx_pki_dflt_bp_en[node] = bp_en;
+}
+
 /**
  * This function Enabled the PKI hardware to
  * start accepting/processing packets.
@@ -524,7 +532,8 @@ void cvmx_helper_pki_enable(int node)
 		cvmx_dprintf("enable PKI on node %d\n", node);
 	__cvmx_helper_pki_install_dflt_vlan(node);
 	cvmx_pki_setup_clusters(node);
-	cvmx_pki_enable_backpressure(node);
+	if (cvmx_pki_dflt_bp_en[node])
+		cvmx_pki_enable_backpressure(node);
 	cvmx_pki_parse_enable(node, 0);
 	cvmx_pki_enable(node);
 }
@@ -917,6 +926,8 @@ int __cvmx_helper_pki_intf_rsrcs(int node, struct cvmx_pki_intf_schd *intf)
 			return rs;
 		}
 		intf->sso_grp = rs | (node << 8);
+		if (pki_helper_debug)
+			cvmx_dprintf("pki-helper:intf-rsrc: sso grp alloced is %d\n", intf->sso_grp);
 	}
 #endif /* CVMX_BUILD_FOR_LINUX_KERNEL */
 	return 0;
@@ -1171,19 +1182,19 @@ int cvmx_helper_pki_init_interface(const int xiface, struct cvmx_pki_intf_schd *
 	memset(&qpg_cfg, 0, sizeof(qpg_cfg));
 
 	if (pki_helper_debug)
-		cvmx_dprintf("pki-helper:intf-init:intf%d initialize\n", xiface);
+		cvmx_dprintf("pki-helper:intf-init:intf0x%x initialize--------------------------------\n", xiface);
 
 	if (!intfsch->pool_per_intf) {
 		if (gblsch != NULL) {
 			intfsch->_pool = gblsch->_pool;
 			intfsch->pool_num = gblsch->pool_num;
 		} else {
-			cvmx_dprintf("ERROR:pki-helper:intf-init:intf%d: global scheduling is in use but is NULL\n", xiface);
+			cvmx_dprintf("ERROR:pki-helper:intf-init:intf0x%x: global scheduling is in use but is NULL\n", xiface);
 			return -1;
 		}
 	} else {
 		if (intfsch == NULL) {
-			cvmx_dprintf("ERROR:pki-helper:intf-init:intf%d: interface scheduling pointer is NULL\n", xiface);
+			cvmx_dprintf("ERROR:pki-helper:intf-init:intf0x%x: interface scheduling pointer is NULL\n", xiface);
 			return -1;
 		}
 		mbuff_size = intfsch->pool_buff_size;
@@ -1210,6 +1221,7 @@ int cvmx_helper_pki_init_interface(const int xiface, struct cvmx_pki_intf_schd *
 		if (!prtsch->pool_per_prt) {
 			prtsch->pool_num = intfsch->pool_num;
 			prtsch->_pool = intfsch->_pool;
+			prtsch->pool_buff_size = intfsch->pool_buff_size;
 		} else if (prtsch->pool_buff_size < mbuff_size || !mbuff_size)
 			mbuff_size = prtsch->pool_buff_size;
 		if (!prtsch->aura_per_prt) {
@@ -1233,7 +1245,7 @@ int cvmx_helper_pki_init_interface(const int xiface, struct cvmx_pki_intf_schd *
 		/* Port 0 should never have this set to TRUE **/
 		if (intfsch->qos_share_aura && (port != 0)) {
 			if (pki_helper_debug)
-				cvmx_dprintf("pki-helper:intf-init:intf%d All ports will share same aura for all qos\n", xiface);
+				cvmx_dprintf("pki-helper:intf-init:intf0x%x All ports will share same aura for all qos\n", xiface);
 			for (qos = 0; qos < num_qos; qos++) {
 				qossch = &prtsch->qos_s[qos];
 				prtsch->qpg_qos = intfsch->prt_s[0].qpg_qos;
@@ -1248,7 +1260,7 @@ int cvmx_helper_pki_init_interface(const int xiface, struct cvmx_pki_intf_schd *
 		}
 		if (intfsch->qos_share_grp && (port != 0)) {
 			if (pki_helper_debug)
-				cvmx_dprintf("pki-helper:intf-init:intf%d: All ports will share same sso group for all qos\n",xiface);
+				cvmx_dprintf("pki-helper:intf-init:intf0x%x: All ports will share same sso group for all qos\n", xiface);
 			for (qos = 0; qos < num_qos; qos++) {
 				qossch = &prtsch->qos_s[qos];
 				qossch->sso_grp_per_qos =
@@ -1263,10 +1275,9 @@ int cvmx_helper_pki_init_interface(const int xiface, struct cvmx_pki_intf_schd *
 				qossch->pool_num = prtsch->pool_num;
 				qossch->_pool = prtsch->_pool;
 				if (pki_helper_debug)
-					cvmx_dprintf("pki-helper:intf-init:intf%d: qos %d has pool %d\n",
+					cvmx_dprintf("pki-helper:intf-init:intf0x%x: qos %d has pool %d\n",
 						xiface, qos, prtsch->pool_num);
-			} else if (qossch->pool_buff_size < mbuff_size ||
-				    !mbuff_size)
+			} else if (qossch->pool_buff_size < mbuff_size || !mbuff_size)
 				mbuff_size = qossch->pool_buff_size;
 			if (!qossch->aura_per_qos) {
 				qossch->aura_num = prtsch->aura_num;
@@ -1388,7 +1399,7 @@ int cvmx_helper_pki_init_interface(const int xiface, struct cvmx_pki_intf_schd *
 		}
 		if (!mbuff_size) {
 			if (!gblsch->setup_pool) {
-				cvmx_dprintf("No pool has setup for intf %d\n",
+				cvmx_dprintf("No pool has setup for intf 0x%x\n",
 					xiface);
 				return -1;
 			}
@@ -1405,7 +1416,7 @@ int cvmx_helper_pki_init_interface(const int xiface, struct cvmx_pki_intf_schd *
 		} else {
 			intfsch->style = rs;
 			if (pki_helper_debug)
-				cvmx_dprintf("style %d allocated intf %d qpg_base %d\n",
+				cvmx_dprintf("style %d allocated intf 0x%x qpg_base %d\n",
 					intfsch->style, xiface,
 					intfsch->qpg_base);
 			style_cfg = pki_dflt_style[xi.node];
@@ -1440,7 +1451,7 @@ int cvmx_helper_pki_init_interface(const int xiface, struct cvmx_pki_intf_schd *
 			if (prtsch->qpg_qos && intfsch->qos_share_aura &&
 				intfsch->qos_share_grp && port != 0) {
 				if (pki_helper_debug)
-					cvmx_dprintf("intf %d has all ports share qos aura n grps\n",
+					cvmx_dprintf("intf 0x%x has all ports share qos aura n grps\n",
 						xiface);
 				/* Ports have differnet styles but want
 				 * to share same qpg entries.
@@ -1512,7 +1523,7 @@ EXPORT_SYMBOL(cvmx_pki_set_port_config);
  * particular port.
  * @param ipd_port	ipd port number to display parameter of
  */
-void cvmx_pki_show_port_config(int ipd_port)
+void cvmx_helper_pki_show_port_config(int ipd_port)
 {
 	int interface, index, pknd;
 	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(ipd_port);
@@ -1803,3 +1814,221 @@ void cvmx_helper_pki_set_little_endian(int node)
 		}
 	}
 }
+
+/**
+ * This function modifies the sso group where packets from specified port needs to be routed
+ * @param ipd_port			pki port number.
+ * @param grp_ok			sso group where good packets are routed
+ * @param grp_bad			sso group where errored packets are routed
+ * NOTE: This function assumes that each port has its own style/profile and is not using qpg qos
+ */
+void cvmx_helper_pki_modify_prtgrp(int ipd_port, int grp_ok, int grp_bad)
+{
+	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(ipd_port);
+	struct cvmx_pki_port_config port_cfg;
+	struct cvmx_pki_qpg_config qpg_cfg;
+	int index;
+
+	cvmx_pki_get_port_config(ipd_port, &port_cfg);
+	/* TODO: expand it to calculate index in other cases hrm:10.5.3*/
+	index = port_cfg.style_cfg.parm_cfg.qpg_base;
+	cvmx_pki_read_qpg_entry(xp.node, index, &qpg_cfg);
+	qpg_cfg.grp_ok = grp_ok;
+	qpg_cfg.grp_bad = grp_bad;
+	cvmx_pki_write_qpg_entry(xp.node, index, &qpg_cfg);
+}
+
+int cvmx_pki_clone_style(int node, int style, uint64_t cluster_mask)
+{
+	int new_style;
+	struct cvmx_pki_style_config style_cfg;
+
+	cvmx_pki_read_style_config(node, style, cluster_mask, &style_cfg);
+	new_style = cvmx_pki_style_alloc(node, CVMX_PKI_FIND_AVAL_ENTRY);
+	if (new_style < 0)
+		return -1;
+	cvmx_pki_write_style_config(node, new_style, cluster_mask, &style_cfg);
+	return new_style;
+}
+
+/* Optimize if use at runtime */
+int cvmx_pki_add_entry(uint64_t *array, uint64_t match, int index, int num_entries)
+{
+	if (index >= num_entries)
+		return -1;
+	array[index] = match;
+	return 0;
+}
+
+/* Optimize if use at runtime */
+int cvmx_pki_find_entry(uint64_t *array, uint64_t match, int num_entries)
+{
+	int i;
+
+	for (i = 0; i < num_entries; i++) {
+		if ((array[i] & 0xffffffffff) == match)
+			return i;
+	}
+	return -1;
+}
+
+
+/**
+ * This function send the packets to specified style/profile if
+ * specified mac address and specified input style/profile matches.
+ * @param node			node number.
+ * @param style			style/profile to match against
+ * @param mac_addr		mac address to match
+ * @param mac_addr_mask		mask of mac address bits
+ *                              1: exact match
+ *				0: don't care
+ *				ex: to exactly match mac address 0x0a0203040506 mask = 0xffffffffffff
+ *                                  to match only first 2 bytes  0x0a02xxxxxxxx mask = 0xffff00000000
+ * @param final_style		final style (contains aura/sso_grp etc) to route matched packet to.
+ */
+int cvmx_helper_pki_route_dmac(int node, int style, uint64_t mac_addr, uint64_t mac_addr_mask, int final_style)
+{
+	struct cvmx_pki_pcam_input pcam_input;
+	struct cvmx_pki_pcam_action pcam_action;
+	int bank;
+	int index;
+	int interim_style = style;
+	uint64_t cl_mask = CVMX_PKI_CLUSTER_ALL;
+	uint32_t data_to_match;
+	uint32_t data_to_mask;
+	uint64_t match_h;
+	uint64_t match_l;
+
+	memset(&pcam_input, 0, sizeof(pcam_input));
+	memset(&pcam_action, 0, sizeof(pcam_action));
+	data_to_match = (mac_addr >> CVMX_PKI_DMACH_SHIFT) & CVMX_PKI_DMACH_MASK;
+	data_to_mask = (mac_addr_mask >> CVMX_PKI_DMACH_SHIFT) & CVMX_PKI_DMACH_MASK;
+	match_h = (uint64_t)(data_to_match & data_to_mask) | (uint64_t)(style << 16);
+	if (!data_to_mask)
+		goto pcam_dmacl;
+	index = cvmx_pki_find_entry(pcam_dmach, match_h, CVMX_PKI_NUM_PCAM_ENTRY);
+
+	if (index >= 0) {
+		interim_style = (pcam_dmach[index] >> 40) & 0xffffffffff;
+		goto pcam_dmacl;
+	}
+	bank = 0;
+	index = cvmx_pki_pcam_entry_alloc(node, CVMX_PKI_FIND_AVAL_ENTRY, bank, cl_mask);
+	if (index < 0) {
+		cvmx_dprintf("ERROR: Allocating pcam entry node=%d bank=%d\n",
+			node, bank);
+		return -1;
+	}
+	pcam_input.style = style;
+	pcam_input.style_mask = 0xffffffffffffffff;
+	pcam_input.field = CVMX_PKI_PCAM_TERM_DMACH;
+	pcam_input.field_mask = 0xff;
+	pcam_input.data = data_to_match;
+	pcam_input.data_mask =  data_to_mask;
+	pcam_action.parse_mode_chg = CVMX_PKI_PARSE_NO_CHG;
+	pcam_action.parse_flag_set = 0;
+	pcam_action.layer_type_set = CVMX_PKI_LTYPE_E_NONE;
+	interim_style = cvmx_pki_clone_style(node, style, cl_mask);
+	if (interim_style < 0) {
+		cvmx_dprintf("ERROR: Failed to allocate interim style\n");
+		return -1;
+	}
+	pcam_action.style_add = interim_style - style;
+	pcam_action.pointer_advance = 0;
+	cvmx_pki_pcam_write_entry(node, index, cl_mask,
+			pcam_input, pcam_action);/*cluster_mask in pass2*/
+	match_h |= (uint64_t)(((uint64_t)interim_style << 40) & 0xff0000000000);
+	cvmx_pki_add_entry(pcam_dmach, match_h, index, CVMX_PKI_NUM_PCAM_ENTRY);
+pcam_dmacl:
+	bank = 1;
+	data_to_match = (mac_addr & CVMX_PKI_DMACL_MASK);
+	data_to_mask = (mac_addr_mask & CVMX_PKI_DMACL_MASK);
+	if (!data_to_mask)
+		return 0;
+	match_l = (uint64_t)(data_to_match & data_to_mask) | ((uint64_t)interim_style << 32);
+	if (cvmx_pki_find_entry(pcam_dmacl, match_l, CVMX_PKI_NUM_PCAM_ENTRY) >= 0)
+		return 0;
+	index = cvmx_pki_pcam_entry_alloc(node,
+			CVMX_PKI_FIND_AVAL_ENTRY, bank, cl_mask);
+	if (index < 0) {
+		cvmx_dprintf("ERROR: Allocating pcam entry node=%d bank=%d\n",
+			node, bank);
+		return -1;
+	}
+	cvmx_pki_add_entry(pcam_dmacl, match_l, index, CVMX_PKI_NUM_PCAM_ENTRY);
+	pcam_input.style = interim_style;
+	pcam_input.style_mask = 0xffffffffffffffff;
+	pcam_input.field = CVMX_PKI_PCAM_TERM_DMACL;
+	pcam_input.field_mask = 0xff;
+	pcam_input.data = data_to_match;
+	pcam_input.data_mask = data_to_mask;
+	/* customer need to decide if they want to resume parsing or terminate it,
+	if further match found in pcam it will take precedence */
+	pcam_action.parse_mode_chg = CVMX_PKI_PARSE_NO_CHG;
+	pcam_action.parse_flag_set = 0;
+	pcam_action.layer_type_set = CVMX_PKI_LTYPE_E_NONE;
+	pcam_action.style_add = final_style - interim_style;
+	pcam_action.pointer_advance = 0;
+	cvmx_pki_pcam_write_entry(node, index, cl_mask,
+			pcam_input, pcam_action);/*cluster_mask in pass2*/
+
+	return 0;
+}
+
+/**
+ * This function send the packets to specified sso group if
+ * specified mac address and specified input port matches.
+ * NOTE: This function will always create a new style/profile for the specified
+ * sso group even if style/profile already exist and if the style used by this ipd port is
+ * shared all the ports using that style will get affected.
+ * similar function to use: cvmx_helper_pki_route_dmac()
+ * @param node			node number.
+ * @param ipd_port		ipd port on which mac address match needs to be performed.
+ * @param mac_addr		mac address to match
+ * @param mac_addr_mask		mask of mac address bits
+ *                              1: exact match
+ *				0: don't care
+ *				ex: to exactly match mac address 0x0a0203040506 mask = 0xffffffffffff
+ *                                  to match only first 2 bytes  0x0a02xxxxxxxx mask = 0xffff00000000
+ * @param grp			sso group to route matched packet to.
+ * @return 			success: final style containing routed sso group
+ *				fail: -1
+ */
+int cvmx_helper_pki_route_prt_dmac(int ipd_port, uint64_t mac_addr, uint64_t mac_addr_mask, int grp)
+{
+	int style;
+	int new_style;
+	int offset, index;
+	struct cvmx_pki_style_config st_cfg;
+	struct cvmx_pki_port_config port_cfg;
+	struct cvmx_pki_qpg_config qpg_cfg;
+	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(ipd_port);
+	int node = xp.node;
+
+	/* 1. Get the current/initial style config used by this port */
+	cvmx_pki_get_port_config(ipd_port, &port_cfg);
+	style = port_cfg.pkind_cfg.initial_style;
+	st_cfg = port_cfg.style_cfg;
+
+	/* 2. Create new style/profile from current and modify it to steer traffic to specified grp */
+	new_style = cvmx_pki_style_alloc(node, CVMX_PKI_FIND_AVAL_ENTRY);
+	if (new_style < 0) {
+		cvmx_printf("ERROR: %s: new style not available\n", __func__);
+		return -1;
+	}
+	offset = st_cfg.parm_cfg.qpg_base;
+	cvmx_pki_read_qpg_entry(node, offset, &qpg_cfg);
+	qpg_cfg.qpg_base = CVMX_PKI_FIND_AVAL_ENTRY;
+	qpg_cfg.grp_ok = grp;
+	qpg_cfg.grp_bad = grp;
+	index = cvmx_helper_pki_set_qpg_entry(node, &qpg_cfg);
+	if (index < 0) {
+		cvmx_printf("ERROR: %s: new qpg entry not available\n", __func__);
+		return -1;
+	}
+	st_cfg.parm_cfg.qpg_base = index;
+	cvmx_pki_write_style_config(node, new_style, CVMX_PKI_CLUSTER_ALL, &st_cfg);
+	cvmx_helper_pki_route_dmac(node, style, 0xffffffffffff, 0xffffffffffff, new_style);
+	cvmx_helper_pki_route_dmac(node, style, 0x0a0203040506, 0xffffffffffff, new_style);
+	return new_style;
+}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-pko3.c b/arch/mips/cavium-octeon/executive/cvmx-helper-pko3.c
index 54a4d62..23bb11d 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-pko3.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-pko3.c
@@ -175,9 +175,10 @@ static int __cvmx_pko3_config_memory(unsigned node)
 	}
 #endif /* !U_BOOT */
 
-	cvmx_dprintf("%s: creating AURA with %u buffers for up to %d packets, "
-		"%d per interface\n",
-		__func__, buf_count, __pko_pkt_budget, __pko_pkt_quota);
+	if (debug)
+		cvmx_dprintf("%s: Creating AURA with %u buffers for up to %d total packets,"
+			" %d packets per interface\n",
+			__func__, buf_count, __pko_pkt_budget, __pko_pkt_quota);
 
 	aura = cvmx_fpa3_setup_aura_and_pool(node, -1,
 		"PKO3 AURA", NULL,
@@ -1884,48 +1885,65 @@ int cvmx_helper_pko3_config_dump(unsigned int node)
 
 	/* Display FIFO Groups: */
 	DLMPRINT("FIFO Groups:");
-	cvmx_printf("Group: (FIFOs)\n");
 	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
-		ngroups = 7;
+		ngroups = 8;
 	else
-		ngroups = 4;
-	for (group = 0, pcrc32 = 0, base = 0; group < ngroups; group++) {
+		ngroups = 5;
+	for (group = 0; group < ngroups; group++) {
 		cvmx_pko_ptgfx_cfg_t fgcfg;
-		CVMX_MT_CRC_POLYNOMIAL(0x1edc6f41);
-		CVMX_MT_CRC_IV(0xffffffff);
+		char *pfg_sizes[8] = {
+			[0] = " 2.5  2.5  2.5  2.5",
+			[1] = " 5.0  n/a  2.5  2.5",
+			[2] = " 2.5  2.5  5.0  n/a",
+			[3] = " 5.0  n/a  5.0  n/a",
+			[4] = "10.0  n/a  n/a  n/a",
+			[5] = "???", [6] = "???", [7] = "???"};
+		char *pfg_rates[8] = {
+			[0] = "6.25", [1] = "12.5", [2] = "25.0", [3] = "50.0",
+			[4] = "100.0", [5] = "???", [6] = "???", [7] = "???"
+		};
 		fgcfg.u64 = cvmx_read_csr_node(node, CVMX_PKO_PTGFX_CFG(group));
-		CVMX_MT_CRC_DWORD(fgcfg.u64);
-		CVMX_MF_CRC_IV(crc32);
-		if (crc32 == pcrc32)
-			continue;
-		if (group > 0 && (group - 1) != base)
-			cvmx_printf("\nGROUP(s) %02d-%02d -- same as GROUP %02d\n",
-				group - 1, base + 1, base);
-		pcrc32 = crc32;
-		base = group;
-		cvmx_printf("Group %d: (%d, %d, %d, %d)\n",
-			group, group * 4, group * 4 + 1, group * 4 + 2, group * 4 + 3);
-		PARPRINT(2, "Size", "%*d\n", PKO_PRN_DPLEN(1), fgcfg.s.size);
-		PARPRINT(2, "Rate", "%*d\n", PKO_PRN_DPLEN(1), fgcfg.s.rate);
+		sprintf(lines[0], "Group %d: (FIFOs)", group);
+		if (group == (ngroups - 1)) {
+			sprintf(lines[1], "Virtual/NULL");
+			sprintf(lines[2], "10.0");
+		} else {
+			sprintf(lines[1], "%4d %4d %4d %4d",
+				group * 4, group * 4 + 1, group * 4 + 2, group * 4 + 3);
+			sprintf(lines[2], "%s", pfg_sizes[fgcfg.s.size]);
+		}
+		PARPRINT(0, lines[0], "%*s\n", PKO_PRN_DPLEN(1), lines[1]);
+		PARPRINT(2, "Size (KB)", "%*s\n", PKO_PRN_DPLEN(1), lines[2]);
+		PARPRINT(2, "Rate (Gbps)", "%*s\n", PKO_PRN_DPLEN(1), pfg_rates[fgcfg.s.rate]);
 	}
-	if ((group - 1) != base)
-		cvmx_printf("\nGROUP(s) %02d-%02d -- same as GROUP %02d\n",
-			group - 1, base + 1, base);
 	return 0;
 }
 
+static int pko3_find_l1q_from_dq(int node, int dq)
+{
+	int queue, nqueues;
+	cvmx_pko_l1_sqx_pick_t l1pick;
+
+	nqueues = cvmx_pko3_num_level_queues(CVMX_PKO_PORT_QUEUES);
+	for (queue = 0; queue < nqueues; queue++) {
+		l1pick.u64 = cvmx_read_csr_node(node, CVMX_PKO_L1_SQX_PICK(queue));
+		if (l1pick.s.dq == dq)
+			return queue;
+	}
+	return -1;
+}
+
 #undef PKO_PRN_HEADLEN
 #define PKO_PRN_HEADLEN  36 
 #undef PKO_PRN_DATALEN
 #define PKO_PRN_DATALEN  44
 int cvmx_helper_pko3_stats_dump(unsigned int node)
 {
-	int queue, nqueues, n;
+	int queue, nqueues, n, nmacs;
 	cvmx_pko_dqx_packets_t dq_pkts;
 	cvmx_pko_dqx_bytes_t dq_bytes;
 	cvmx_pko_dqx_dropped_packets_t dq_drppkts;
 	cvmx_pko_dqx_dropped_bytes_t dq_drpbytes;
-	cvmx_pko_dqx_shape_state_t dq_shape_stat;
 	cvmx_pko_l1_sqx_dropped_packets_t l1_drppkts;
 	cvmx_pko_l1_sqx_dropped_bytes_t l1_drpbytes;
 	cvmx_pko_l1_sqx_red_packets_t l1_redpkts;
@@ -1935,7 +1953,6 @@ int cvmx_helper_pko3_stats_dump(unsigned int node)
 	cvmx_pko_l1_sqx_green_packets_t l1_grnpkts;
 	cvmx_pko_l1_sqx_green_bytes_t l1_grnbytes;
 	cvmx_pko_l1_sqx_topology_t l1top;
-	cvmx_pko_l1_sqx_shape_state_t l1_shape_stat;
 	cvmx_pko_macx_cfg_t maccfg;
 	char lines[4][256];
 
@@ -1953,17 +1970,7 @@ int cvmx_helper_pko3_stats_dump(unsigned int node)
 		n = dq_pkts.s.count + dq_bytes.s.count + dq_drppkts.s.count + dq_drpbytes.s.count;
 		if (n == 0)
 			continue;
-		cvmx_printf("DQ%d:\n", queue);
-		dq_shape_stat.u64 = cvmx_read_csr_node(node, CVMX_PKO_DQX_SHAPE_STATE(queue));
-		switch (dq_shape_stat.s.color) {
-		case 0: sprintf(lines[0], "Green"); break;
-		case 1: sprintf(lines[0], "Yellow"); break;
-		case 2: sprintf(lines[0], "Red"); break;
-		default: sprintf(lines[0], "Undef"); break;
-		}
-		PARPRINT(4, "Conn.Status", "%*s\n", PKO_PRN_DPLEN(1), lines[0]);
-		PARPRINT(4, "PIR Accum", "%*d\n", PKO_PRN_DPLEN(1), dq_shape_stat.s.pir_accum);
-		PARPRINT(4, "CIR Accum", "%*d\n", PKO_PRN_DPLEN(1), dq_shape_stat.s.cir_accum);
+		cvmx_printf("DQ%d => L1-SQ%d\n", queue, pko3_find_l1q_from_dq(node, queue));
 		if (dq_pkts.s.count)
 			PARPRINT(4, "Packets", "%*lld\n", PKO_PRN_DPLEN(1), (long long)dq_pkts.s.count);
 		if (dq_bytes.s.count)
@@ -1975,6 +1982,7 @@ int cvmx_helper_pko3_stats_dump(unsigned int node)
 	}
 	DLMPRINT("Port(L1) Queues:");
 	nqueues = cvmx_pko3_num_level_queues(CVMX_PKO_PORT_QUEUES);
+	nmacs = __cvmx_pko3_num_macs();
 	for (queue = 0; queue < nqueues; queue++) {
 		l1_grnpkts.u64 = cvmx_read_csr_node(node, CVMX_PKO_L1_SQX_GREEN_PACKETS(queue));
 		l1_grnbytes.u64 = cvmx_read_csr_node(node, CVMX_PKO_L1_SQX_GREEN_BYTES(queue));
@@ -1989,18 +1997,23 @@ int cvmx_helper_pko3_stats_dump(unsigned int node)
 		if (n == 0)
 			continue;
 
-		l1_shape_stat.u64 = cvmx_read_csr_node(node, CVMX_PKO_L1_SQX_SHAPE_STATE(queue));
 		l1top.u64 = cvmx_read_csr_node(node, CVMX_PKO_L1_SQX_TOPOLOGY(queue));
-		if (OCTEON_IS_MODEL(OCTEON_CN78XX))
-			sprintf(lines[0], "%s", pko_macmap[l1top.s.link][0]);
-		else if (OCTEON_IS_MODEL(OCTEON_CN73XX))
-			sprintf(lines[0], "%s", pko_macmap[l1top.s.link][1]);
-		else if (OCTEON_IS_MODEL(OCTEON_CNF75XX))
-			sprintf(lines[0], "%s", pko_macmap[l1top.s.link][2]);
-		maccfg.u64 = cvmx_read_csr_node(node, CVMX_PKO_MACX_CFG(l1top.s.link));
+		if (l1top.s.link > nmacs) {
+			maccfg.u64 = 0;
+			sprintf(lines[0], "Undef");
+		} else if (l1top.s.link == nmacs) {
+			maccfg.u64 = 0;
+			sprintf(lines[0], "NULL");
+		} else {
+			if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+				sprintf(lines[0], "%s", pko_macmap[l1top.s.link][0]);
+			else if (OCTEON_IS_MODEL(OCTEON_CN73XX))
+				sprintf(lines[0], "%s", pko_macmap[l1top.s.link][1]);
+			else if (OCTEON_IS_MODEL(OCTEON_CNF75XX))
+				sprintf(lines[0], "%s", pko_macmap[l1top.s.link][2]);
+			maccfg.u64 = cvmx_read_csr_node(node, CVMX_PKO_MACX_CFG(l1top.s.link));
+		}
 		cvmx_printf("L1-SQ%d => MAC%d (%s) => FIFO%d:\n", queue, l1top.s.link, lines[0], maccfg.s.fifo_num);
-		PARPRINT(4, "Conn.Status", "%*s\n", PKO_PRN_DPLEN(1), (l1_shape_stat.s.color == 0) ? "Green" : "Red");
-		PARPRINT(4, "CIR Accum", "%*d\n", PKO_PRN_DPLEN(1), l1_shape_stat.s.cir_accum);
 		if (l1_grnpkts.s.count)
 			PARPRINT(4, "Green Packets", "%*lld\n", PKO_PRN_DPLEN(1), (long long)l1_grnpkts.s.count);
 		if (l1_grnbytes.s.count)
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper.c b/arch/mips/cavium-octeon/executive/cvmx-helper.c
index 35eabf8..12b220b 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper.c
@@ -1563,6 +1563,8 @@ static int __cvmx_helper_global_setup_backpressure(int node)
 		case CVMX_HELPER_INTERFACE_MODE_SRIO:
 		case CVMX_HELPER_INTERFACE_MODE_ILK:
 		case CVMX_HELPER_INTERFACE_MODE_NPI:
+		case CVMX_HELPER_INTERFACE_MODE_PICMG:
+			break;
 		case CVMX_HELPER_INTERFACE_MODE_LOOP:
 		case CVMX_HELPER_INTERFACE_MODE_XAUI:
 		case CVMX_HELPER_INTERFACE_MODE_RXAUI:
@@ -1579,7 +1581,6 @@ static int __cvmx_helper_global_setup_backpressure(int node)
 		case CVMX_HELPER_INTERFACE_MODE_SPI:
 		case CVMX_HELPER_INTERFACE_MODE_SGMII:
 		case CVMX_HELPER_INTERFACE_MODE_QSGMII:
-		case CVMX_HELPER_INTERFACE_MODE_PICMG:
 		case CVMX_HELPER_INTERFACE_MODE_MIXED:
 			bpmask = (cvmx_rgmii_backpressure_dis) ? 0xF : 0;
 			if (octeon_has_feature(OCTEON_FEATURE_BGX))
diff --git a/arch/mips/cavium-octeon/executive/cvmx-ila.c b/arch/mips/cavium-octeon/executive/cvmx-ila.c
index d1212c8..e45c50b 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-ila.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-ila.c
@@ -1,5 +1,5 @@
 /***********************license start***************
- * Copyright (c) 2014  Cavium Inc. (support@cavium.com). All rights
+ * Copyright (c) 2014-2015  Cavium Inc. (support@cavium.com). All rights
  * reserved.
  *
  *
@@ -40,11 +40,13 @@
 /**
  * @file
  *
- * Functions to configure the ILK-LA interface.
+ * Functions to configure the ILA-LA interface.
  *
  * <hr>$Id$<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include <linux/slab.h>
+#include <linux/export.h>
 #include <asm/octeon/cvmx.h>
 #include <asm/octeon/cvmx-helper.h>
 #include <asm/octeon/cvmx-ila.h>
@@ -52,6 +54,7 @@
 #include <asm/octeon/cvmx-qlm.h>
 #else
 #include "cvmx.h"
+#include "cvmx-error.h"
 #include "cvmx-helper.h"
 #include "cvmx-ila.h"
 #include "cvmx-qlm.h"
@@ -61,7 +64,7 @@ CVMX_SHARED int cvmx_ila_chans = 2;
 
 /**
  * @INTERNAL
- * Return the link state of an IPD/PKO port as returned by ILK-LA link status.
+ * Return the link state of an IPD/PKO port as returned by ILA-LA link status.
  *
  * @param lane_mask lane_mask
  *
@@ -108,7 +111,7 @@ retry:
 		rx_cfg1.s.rx_bdry_lock_ena = lane_mask;
 		rx_cfg1.s.rx_align_ena = 0;
 		cvmx_write_csr_node(node, CVMX_ILA_RXX_CFG1(0), rx_cfg1.u64);
-		//cvmx_dprintf("ILK-LA: Looking for word boundary lock\n");
+		/* cvmx_dprintf("ILA-LA: Looking for word boundary lock\n"); */
 
 		goto retry;
 	}
@@ -123,7 +126,7 @@ retry:
 
 			rx_cfg1.s.rx_align_ena = 1;
 			cvmx_write_csr_node(node, CVMX_ILA_RXX_CFG1(0), rx_cfg1.u64);
-			//cvmx_printf("ILK-LA: Looking for lane alignment\n");
+			/* cvmx_printf("ILA-LA: Looking for lane alignment\n"); */
 			goto retry;
 		}
 		goto fail;
@@ -133,7 +136,7 @@ retry:
 		rx_cfg1.s.rx_bdry_lock_ena = 0;
 		rx_cfg1.s.rx_align_ena = 0;
 		cvmx_write_csr_node(node, CVMX_ILA_RXX_CFG1(0), rx_cfg1.u64);
-		//cvmx_dprintf("ILK-LA: Lane alignment failed\n");
+		/* cvmx_dprintf("ILA-LA: Lane alignment failed\n"); */
 		goto fail;
 	}
 
@@ -157,13 +160,18 @@ retry:
 			}
 		}
 
-		//cvmx_dprintf("ILK-LA: Lane alignment complete\n");
+		/* cvmx_dprintf("ILA-LA: Lane alignment complete\n"); */
 	}
 
 	if (!rx_int.s.lane_align_done) {
 		goto retry;
 	}
 
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+	/* Enable error interrupts, now link is up */
+	cvmx_error_enable_group(CVMX_ERROR_GROUP_ILA, 0);
+#endif
+
 	result.u64 = 0;
 	result.s.link_up = 1;
 	result.s.full_duplex = 1;
@@ -178,16 +186,20 @@ fail:
 		rx_cfg1.s.pkt_ena = 0;
 		cvmx_write_csr_node(node, CVMX_ILA_RXX_CFG1(0), rx_cfg1.u64);
 
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
 		/* Disable error interrupts */
+		cvmx_error_disable_group(CVMX_ERROR_GROUP_ILA, 0);
+#endif
 	}
 
 	return result;
 }
+EXPORT_SYMBOL(__cvmx_ila_link_get);
 
 /**
- * Initialize ILK-LA interface
+ * Initialize ILA-LA interface
  *
- * @param lane_mask  Lanes to initialize ILK-LA interface.
+ * @param lane_mask  Lanes to initialize ILA-LA interface.
  * @return  0 on success and -1 on failure.
  */
 int cvmx_ila_initialize(int lane_mask)
@@ -208,6 +220,11 @@ int cvmx_ila_initialize(int lane_mask)
 		ser_cfg.s.ser_reset_n = lane_mask;
 		cvmx_write_csr_node(node, CVMX_ILA_SER_CFG, ser_cfg.u64);
 	}
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+	/* Disable all error interrupts, enable after the interface is up. */
+	if (lane_mask)
+		cvmx_error_disable_group(CVMX_ERROR_GROUP_ILA, 0);
+#endif
 
 	/* Enable RX lanes */
 	rx_cfg0.u64 = cvmx_read_csr_node(node, CVMX_ILA_RXX_CFG0(0));
@@ -253,6 +270,7 @@ out:
 
 	return -1;
 }
+EXPORT_SYMBOL(cvmx_ila_initialize);
 
 int cvmx_ila_disable(void)
 {
@@ -272,14 +290,15 @@ int cvmx_ila_disable(void)
 
 	return 0;
 }
+EXPORT_SYMBOL(cvmx_ila_disable);
 
 /**
- * Enable or disable LA mode in ILK header.
+ * Enable or disable LA mode in ILA header.
  *
  * @param channel channel
- * @param mode   If set, enable LA mode in ILK header, else disable
+ * @param mode   If set, enable LA mode in ILA header, else disable
  *
- * @return ILK header
+ * @return ILA header
  */
 cvmx_ila_header_t cvmx_ila_configure_header(int channel, int mode)
 {
@@ -294,4 +313,4 @@ cvmx_ila_header_t cvmx_ila_configure_header(int channel, int mode)
 
 	return ila_header;
 }
-			
+EXPORT_SYMBOL(cvmx_ila_configure_header);
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pki-resources.c b/arch/mips/cavium-octeon/executive/cvmx-pki-resources.c
index 78d64d0..721a00e 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pki-resources.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pki-resources.c
@@ -256,8 +256,8 @@ int cvmx_pki_cluster_free(int node, uint64_t cluster_mask)
 					return -1;
 				}
 			}
-			cluster++;
 		}
+		cluster++;
 	}
 	return 0;
 }
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pki.c b/arch/mips/cavium-octeon/executive/cvmx-pki.c
index fe6e1da..fc936d4c 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pki.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pki.c
@@ -1187,13 +1187,290 @@ void cvmx_pki_show_pkind_attributes(int node, int pkind)
 	}
 }
 
-#ifdef CVMX_DUMP_PKI
-/*
- * Show PKI integrated configuration.
- * See function prototype in cvmx-pki.h
+#define READCORRECT(cnt, node, value, addr)	\
+	{cnt = 0;	\
+	while (value >= (1ull << 48) && cnt++ < 20) \
+		value = cvmx_read_csr_node(node, addr); \
+	if (cnt >= 20)  \
+		cvmx_dprintf("count stuck for 0x%llx\n", (long long unsigned int)addr); }
+
+/**
+ * Get the status counters for index from PKI.
+ *
+ * @param node	   node number
+ * @param index    pkind number (if PKI_STATS_CTL:mode=0) or
+ *		   style(flow) number (if PKI_STATS_CTL:mode=1)
+ * @param status   Where to put the results.
  */
-int cvmx_pki_config_dump(unsigned node)
+void cvmx_pki_get_stats(int node, int index, struct cvmx_pki_port_stats *status)
 {
+	cvmx_pki_statx_stat0_t stat0;
+	cvmx_pki_statx_stat1_t stat1;
+	cvmx_pki_statx_stat2_t stat2;
+	cvmx_pki_statx_stat3_t stat3;
+	cvmx_pki_statx_stat4_t stat4;
+	cvmx_pki_statx_stat5_t stat5;
+	cvmx_pki_statx_stat6_t stat6;
+	cvmx_pki_statx_stat7_t stat7;
+	cvmx_pki_statx_stat8_t stat8;
+	cvmx_pki_statx_stat9_t stat9;
+	cvmx_pki_statx_stat10_t stat10;
+	cvmx_pki_statx_stat11_t stat11;
+	cvmx_pki_statx_stat14_t stat14;
+	cvmx_pki_statx_stat15_t stat15;
+	cvmx_pki_statx_stat16_t stat16;
+	cvmx_pki_statx_stat17_t stat17;
+	cvmx_pki_statx_hist0_t hist0;
+	cvmx_pki_statx_hist1_t hist1;
+	cvmx_pki_statx_hist2_t hist2;
+	cvmx_pki_statx_hist3_t hist3;
+	cvmx_pki_statx_hist4_t hist4;
+	cvmx_pki_statx_hist5_t hist5;
+	cvmx_pki_statx_hist6_t hist6;
+	cvmx_pki_pkndx_inb_stat0_t pki_pknd_inb_stat0;
+	cvmx_pki_pkndx_inb_stat1_t pki_pknd_inb_stat1;
+	cvmx_pki_pkndx_inb_stat2_t pki_pknd_inb_stat2;
+	int cnt;
+
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+	/* Accessing PKI stat registers can timeout based on the Errata
+	   PKI-20775, disable SLI_INT_SUM[RML_TO] before reading the stats
+	   enable back after clearing the interrupt. */
+	cvmx_error_intsn_disable_v3(0x1f000);
+#endif
+	stat0.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT0(index));
+	READCORRECT(cnt, node, stat0.u64, CVMX_PKI_STATX_STAT0(index));
+
+	stat1.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT1(index));
+	READCORRECT(cnt, node, stat1.u64, CVMX_PKI_STATX_STAT1(index));
+
+	stat2.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT2(index));
+	READCORRECT(cnt, node, stat2.u64, CVMX_PKI_STATX_STAT2(index));
+
+	stat3.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT3(index));
+	READCORRECT(cnt, node, stat3.u64, CVMX_PKI_STATX_STAT3(index));
+
+	stat4.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT4(index));
+	READCORRECT(cnt, node, stat4.u64, CVMX_PKI_STATX_STAT4(index));
+
+	stat5.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT5(index));
+	READCORRECT(cnt, node, stat5.u64, CVMX_PKI_STATX_STAT5(index));
+
+	stat6.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT6(index));
+	READCORRECT(cnt, node, stat6.u64, CVMX_PKI_STATX_STAT6(index));
+
+	stat7.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT7(index));
+	READCORRECT(cnt, node, stat7.u64, CVMX_PKI_STATX_STAT7(index));
+
+	stat8.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT8(index));
+	READCORRECT(cnt, node, stat8.u64, CVMX_PKI_STATX_STAT8(index));
+
+	stat9.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT9(index));
+	READCORRECT(cnt, node, stat9.u64, CVMX_PKI_STATX_STAT9(index));
+
+	stat10.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT10(index));
+	READCORRECT(cnt, node, stat10.u64, CVMX_PKI_STATX_STAT10(index));
+
+	stat11.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT11(index));
+	READCORRECT(cnt, node, stat11.u64, CVMX_PKI_STATX_STAT11(index));
+
+	stat14.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT14(index));
+	READCORRECT(cnt, node, stat14.u64, CVMX_PKI_STATX_STAT14(index));
+
+	stat15.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT15(index));
+	READCORRECT(cnt, node, stat15.u64, CVMX_PKI_STATX_STAT15(index));
+
+	stat16.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT16(index));
+	READCORRECT(cnt, node, stat16.u64, CVMX_PKI_STATX_STAT16(index));
+
+	stat17.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT17(index));
+	READCORRECT(cnt, node, stat17.u64, CVMX_PKI_STATX_STAT17(index));
+
+	hist0.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST0(index));
+	READCORRECT(cnt, node, hist0.u64, CVMX_PKI_STATX_HIST0(index));
+
+	hist1.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST1(index));
+	READCORRECT(cnt, node, hist1.u64, CVMX_PKI_STATX_HIST1(index));
+
+	hist2.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST2(index));
+	READCORRECT(cnt, node, hist2.u64, CVMX_PKI_STATX_HIST2(index));
+
+	hist3.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST3(index));
+	READCORRECT(cnt, node, hist3.u64, CVMX_PKI_STATX_HIST3(index));
+
+	hist4.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST4(index));
+	READCORRECT(cnt, node, hist4.u64, CVMX_PKI_STATX_HIST4(index));
+
+	hist5.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST5(index));
+	READCORRECT(cnt, node, hist5.u64, CVMX_PKI_STATX_HIST5(index));
+
+	hist6.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST6(index));
+	READCORRECT(cnt, node, hist6.u64, CVMX_PKI_STATX_HIST6(index));
+
+	pki_pknd_inb_stat0.u64 = cvmx_read_csr_node(node, CVMX_PKI_PKNDX_INB_STAT0(index));
+	pki_pknd_inb_stat1.u64 = cvmx_read_csr_node(node, CVMX_PKI_PKNDX_INB_STAT1(index));
+	pki_pknd_inb_stat2.u64 = cvmx_read_csr_node(node, CVMX_PKI_PKNDX_INB_STAT2(index));
+
+	status->dropped_octets = stat4.s.drp_octs;
+	status->dropped_packets = stat3.s.drp_pkts;
+	status->octets = stat1.s.octs;
+	status->pci_raw_packets = stat2.s.raw;
+	status->packets = stat0.s.pkts;
+	status->multicast_packets = stat6.s.mcast;
+	status->broadcast_packets = stat5.s.bcast;
+	status->len_64_packets = hist0.s.h1to63;
+	status->len_65_127_packets = hist1.s.h64to127;
+	status->len_128_255_packets = hist2.s.h128to255;
+	status->len_256_511_packets = hist3.s.h256to511;
+	status->len_512_1023_packets = hist4.s.h512to1023;
+	status->len_1024_1518_packets = hist5.s.h1024to1518;
+	status->len_1519_max_packets = hist6.s.h1519;
+	status->fcs_align_err_packets = stat7.s.fcs;
+	status->runt_packets = stat9.s.undersz;
+	status->runt_crc_packets = stat8.s.frag;
+	status->oversize_packets = stat11.s.oversz;
+	status->oversize_crc_packets = stat10.s.jabber;
+	status->mcast_l2_red_packets = stat15.s.drp_mcast;
+	status->bcast_l2_red_packets = stat14.s.drp_bcast;
+	status->mcast_l3_red_packets = stat17.s.drp_mcast;
+	status->bcast_l3_red_packets = stat16.s.drp_bcast;
+	status->inb_packets = pki_pknd_inb_stat0.s.pkts;
+	status->inb_octets = pki_pknd_inb_stat1.s.octs;
+	status->inb_errors = pki_pknd_inb_stat2.s.errs;
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+	/* Enable SLI_INT_SUM[RML_TO] interrupt after clear the pending interrupt. */
+	cvmx_write_csr_node(node, CVMX_CIU3_ISCX_W1C(0x1f000), 1);
+	cvmx_error_intsn_enable_v3(0x1f000);
+#endif
+}
+EXPORT_SYMBOL(cvmx_pki_get_stats);
+
+/**
+ * Clear the statistics counters for a port.
+ *
+ * @param node	   node number
+ * @param port Port number (ipd_port) to get statistics for.
+ *		   Make sure PKI_STATS_CTL:mode is set to 0 for
+ *		   collecting per port/pkind stats.
+ */
+void cvmx_pki_clear_port_stats(int node, uint64_t port)
+{
+	int interface = cvmx_helper_get_interface_num(port);
+	int index = cvmx_helper_get_interface_index_num(port);
+	int pknd = cvmx_helper_get_pknd(interface, index);
+
+	cvmx_pki_statx_stat0_t stat0;
+	cvmx_pki_statx_stat1_t stat1;
+	cvmx_pki_statx_stat2_t stat2;
+	cvmx_pki_statx_stat3_t stat3;
+	cvmx_pki_statx_stat4_t stat4;
+	cvmx_pki_statx_stat5_t stat5;
+	cvmx_pki_statx_stat6_t stat6;
+	cvmx_pki_statx_stat7_t stat7;
+	cvmx_pki_statx_stat8_t stat8;
+	cvmx_pki_statx_stat9_t stat9;
+	cvmx_pki_statx_stat10_t stat10;
+	cvmx_pki_statx_stat11_t stat11;
+	cvmx_pki_statx_stat14_t stat14;
+	cvmx_pki_statx_stat15_t stat15;
+	cvmx_pki_statx_stat16_t stat16;
+	cvmx_pki_statx_stat17_t stat17;
+	cvmx_pki_statx_hist0_t hist0;
+	cvmx_pki_statx_hist1_t hist1;
+	cvmx_pki_statx_hist2_t hist2;
+	cvmx_pki_statx_hist3_t hist3;
+	cvmx_pki_statx_hist4_t hist4;
+	cvmx_pki_statx_hist5_t hist5;
+	cvmx_pki_statx_hist6_t hist6;
+	cvmx_pki_pkndx_inb_stat0_t pki_pknd_inb_stat0;
+	cvmx_pki_pkndx_inb_stat1_t pki_pknd_inb_stat1;
+	cvmx_pki_pkndx_inb_stat2_t pki_pknd_inb_stat2;
+
+	stat0.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT0(pknd));
+	stat1.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT1(pknd));
+	stat2.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT2(pknd));
+	stat3.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT3(pknd));
+	stat4.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT4(pknd));
+	stat5.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT5(pknd));
+	stat6.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT6(pknd));
+	stat7.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT7(pknd));
+	stat8.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT8(pknd));
+	stat9.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT9(pknd));
+	stat10.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT10(pknd));
+	stat11.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT11(pknd));
+	stat14.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT14(pknd));
+	stat15.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT15(pknd));
+	stat16.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT16(pknd));
+	stat17.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT17(pknd));
+	hist0.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST0(pknd));
+	hist1.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST1(pknd));
+	hist2.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST2(pknd));
+	hist3.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST3(pknd));
+	hist4.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST4(pknd));
+	hist5.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST5(pknd));
+	hist6.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST6(pknd));
+	pki_pknd_inb_stat0.u64 = cvmx_read_csr_node(node, CVMX_PKI_PKNDX_INB_STAT0(pknd));
+	pki_pknd_inb_stat1.u64 = cvmx_read_csr_node(node, CVMX_PKI_PKNDX_INB_STAT1(pknd));
+	pki_pknd_inb_stat2.u64 = cvmx_read_csr_node(node, CVMX_PKI_PKNDX_INB_STAT2(pknd));
+
+	stat4.s.drp_octs = 0;
+	stat3.s.drp_pkts = 0;
+	stat1.s.octs = 0;
+	stat2.s.raw = 0;
+	stat0.s.pkts = 0;
+	stat6.s.mcast = 0;
+	stat5.s.bcast = 0;
+	hist0.s.h1to63 = 0;
+	hist1.s.h64to127 = 0;
+	hist2.s.h128to255 = 0;
+	hist3.s.h256to511 = 0;
+	hist4.s.h512to1023 = 0;
+	hist5.s.h1024to1518 = 0;
+	hist6.s.h1519 = 0;
+	stat7.s.fcs = 0;
+	stat9.s.undersz = 0;
+	stat8.s.frag = 0;
+	stat11.s.oversz = 0;
+	stat10.s.jabber = 0;
+	stat15.s.drp_mcast = 0;
+	stat14.s.drp_bcast = 0;
+	stat17.s.drp_mcast = 0;
+	stat16.s.drp_bcast = 0;
+	pki_pknd_inb_stat0.s.pkts = 0;
+	pki_pknd_inb_stat1.s.octs = 0;
+	pki_pknd_inb_stat2.s.errs = 0;
+
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT0(pknd), stat0.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT1(pknd), stat1.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT2(pknd), stat2.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT3(pknd), stat3.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT4(pknd), stat4.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT5(pknd), stat5.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT6(pknd), stat6.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT7(pknd), stat7.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT8(pknd), stat8.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT9(pknd), stat9.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT10(pknd), stat10.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT11(pknd), stat11.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT14(pknd), stat14.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT15(pknd), stat15.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT16(pknd), stat16.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT17(pknd), stat17.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST0(pknd), hist0.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST1(pknd), hist1.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST2(pknd), hist2.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST3(pknd), hist3.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST4(pknd), hist4.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST5(pknd), hist5.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST6(pknd), hist6.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_PKNDX_INB_STAT0(pknd), pki_pknd_inb_stat0.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_PKNDX_INB_STAT1(pknd), pki_pknd_inb_stat1.u64);
+	cvmx_write_csr_node(node, CVMX_PKI_PKNDX_INB_STAT2(pknd), pki_pknd_inb_stat2.u64);
+}
+EXPORT_SYMBOL(cvmx_pki_clear_port_stats);
+
+#ifdef CVMX_DUMP_PKI
+
 #define PKI_PRN_HEADLEN   28
 #define PKI_PRN_DATALEN   52
 #define PKI_PRN_LINELEN   (PKI_PRN_HEADLEN + PKI_PRN_DATALEN)
@@ -1203,6 +1480,36 @@ int cvmx_pki_config_dump(unsigned node)
 #define MAX(__a, __b) (((__a) > (__b)) ? (__a) : (__b))
 #endif
 
+static int pki_find_pkind_chans(int pkind, char *buf, int blen);
+
+static int pki_get_num_bgxs(void)
+{
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+		return 6;
+	else if (OCTEON_IS_MODEL(OCTEON_CN73XX))
+		return 3;
+	else if (OCTEON_IS_MODEL(OCTEON_CNF75XX))
+		return 1;
+	return 0;
+}
+
+static void pki_flprint(int level, char *name, const char *format, ...)
+{
+	char dbuf[PKI_PRN_DATALEN + 1];
+	int offs;
+	va_list args;
+	va_start(args, format);
+	offs = level * 2;
+	vsnprintf(dbuf, PKI_PRN_DATALEN + 1, format, args);
+	cvmx_printf("%*s%-*s%*s\n", offs, "", PKI_PRN_HEADLEN - offs, name, PKI_PRN_DATALEN, dbuf);
+	va_end(args);
+}
+/*
+ * Show PKI integrated configuration.
+ * See function prototype in cvmx-pki.h
+ */
+int cvmx_pki_config_dump(unsigned node)
+{
 #define DLMPRINT(__format, ...) \
 	do { \
 		int __n; \
@@ -1244,16 +1551,6 @@ int cvmx_pki_config_dump(unsigned node)
 	__rc; \
 })
 
-	void printfl(int level, char *name, const char *format, ...) {
-		char dbuf[PKI_PRN_DATALEN + 1];
-		int offs;
-		va_list args;
-		va_start(args, format);
-		offs = level * 2;
-		vsnprintf(dbuf, PKI_PRN_DATALEN + 1, format, args);
-		cvmx_printf("%*s%-*s%*s\n", offs, "", PKI_PRN_HEADLEN - offs, name, PKI_PRN_DATALEN, dbuf);
-		va_end(args);
-	}
 	void int2cstr(char *buf, int data, int nbits) {
 		char *lbits[8] = {"G","F", "E", "D", "C", "B", "?", "?"};
 		buf[0] = '\0';
@@ -1303,6 +1600,7 @@ int cvmx_pki_config_dump(unsigned node)
 	int mask, ibase, n, k, i, __i;
 	int pkind, cluster, style, group;
 	uint32_t crc32, pcrc32;
+	char chans[128];
 	char lines[4][128];
 
 	/* Show Global Configuration. */
@@ -1314,13 +1612,13 @@ int cvmx_pki_config_dump(unsigned node)
 	cvmx_printf("\n%s\n", lines[0]);
 	cvmx_printf("   PKI Configuration (Node %d)\n", node);
 	cvmx_printf("%s\n", lines[0]);
-	printfl(0, "PKI Enabled/Active", "%d/%d", ctl.s.pki_en, rst.s.active);
-	printfl(0, "Packet buffering", "%*s", PKI_PRN_DATALEN, ctl.s.pkt_off ? "Disabled" : "Enabled");
-	printfl(0, "FPA buffer policy", "%*s", PKI_PRN_DATALEN, ctl.s.fpa_wait ? "Wait" : "Drop");
-	printfl(0, "BPID backpressure", "%*s", PKI_PRN_DATALEN, ctl.s.pbp_en ? "Enabled" : "Disabled");
-	printfl(0, "", "%*s%*s%*s%*s", PKI_PRN_DPLEN(4), "DST6", PKI_PRN_DPLEN(4), "SRC6",
+	pki_flprint(0, "PKI Enabled/Active", "%d/%d", ctl.s.pki_en, rst.s.active);
+	pki_flprint(0, "Packet buffering", "%*s", PKI_PRN_DATALEN, ctl.s.pkt_off ? "Disabled" : "Enabled");
+	pki_flprint(0, "FPA buffer policy", "%*s", PKI_PRN_DATALEN, ctl.s.fpa_wait ? "Wait" : "Drop");
+	pki_flprint(0, "BPID backpressure", "%*s", PKI_PRN_DATALEN, ctl.s.pbp_en ? "Enabled" : "Disabled");
+	pki_flprint(0, "", "%*s%*s%*s%*s", PKI_PRN_DPLEN(4), "DST6", PKI_PRN_DPLEN(4), "SRC6",
 		PKI_PRN_DPLEN(4), "DST", PKI_PRN_DPLEN(4), "SRC");
-	printfl(0, "Tag secret words (hex)", "%*x%*x%*x%*x", PKI_PRN_DPLEN(4), secret.s.dst6,
+	pki_flprint(0, "Tag secret words (hex)", "%*x%*x%*x%*x", PKI_PRN_DPLEN(4), secret.s.dst6,
 		PKI_PRN_DPLEN(4), secret.s.src6, PKI_PRN_DPLEN(4), secret.s.dst,
 		PKI_PRN_DPLEN(4), secret.s.src);
 	cvmx_printf("%-30s %4s %4s %4s %4s %4s %4s %4s %4s %4s %4s\n", "",
@@ -1360,6 +1658,8 @@ int cvmx_pki_config_dump(unsigned node)
 			CVMX_MT_CRC_DWORD(pkl2cust[cluster].u64 & ((1ull << 16) - 1));
 			CVMX_MT_CRC_DWORD(pklgcust[cluster].u64 & ((1ull << 8) - 1));
 		}
+		if (pki_find_pkind_chans(pkind, chans, PKI_PRN_DATALEN))
+			CVMX_MT_CRC_DWORD(pkind);
 		CVMX_MF_CRC_IV(crc32);
 		if (crc32 == pcrc32)
 			continue;
@@ -1369,10 +1669,11 @@ int cvmx_pki_config_dump(unsigned node)
 		ibase = pkind;
         
 		DLMPRINT("PKIND %02d:", pkind);
+		pki_flprint(0, "Channel", "%s", (chans[0] != 0) ? chans : "--");
 		NSPRINT(nclusters, __i, sprintf(lines[__i], "Cluster%d", __i));
 		NMPRINT(nclusters, mask, __i, 0, "", "%*s", lines[__i]);
 		cvmx_printf("Mapping:\n");
-		printfl(1, "Cluster Group", "%*d", PKI_PRN_DATALEN, cgsel.s.icg);
+		pki_flprint(1, "Cluster Group", "%*d", PKI_PRN_DATALEN, cgsel.s.icg);
 		cvmx_printf("Parsing:\n");
 		NMPRINT(nclusters, mask, __i, 1, "Initial Style", "%*d", pkstyle[__i].s.style);
 		NSPRINT(nclusters, __i, sprintf(lines[__i], "%c%c%c%c%c%c%c",
@@ -1416,6 +1717,7 @@ int cvmx_pki_config_dump(unsigned node)
 		cvmx_pki_clx_stylex_cfg_t stcfg[CVMX_PKI_NUM_CLUSTER];
 		cvmx_pki_clx_stylex_cfg2_t stcfg2[CVMX_PKI_NUM_CLUSTER];
 		cvmx_pki_clx_stylex_alg_t stalg[CVMX_PKI_NUM_CLUSTER];
+		cvmx_pki_qpg_tblx_t qpgtx[CVMX_PKI_NUM_CLUSTER];
 
 		CVMX_MT_CRC_POLYNOMIAL(0x1edc6f41);
 		CVMX_MT_CRC_IV(0xffffffff);
@@ -1430,6 +1732,7 @@ int cvmx_pki_config_dump(unsigned node)
 			stcfg[cluster].u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(style, cluster));
 			stcfg2[cluster].u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG2(style, cluster));
 			stalg[cluster].u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_ALG(style, cluster));
+			qpgtx[cluster].u64 = cvmx_read_csr_node(node, CVMX_PKI_QPG_TBLX(stcfg[cluster].s.qpg_base));
 			CVMX_MT_CRC_DWORD(stcfg[cluster].u64 & 0x7FFF07FF);
 			CVMX_MT_CRC_DWORD(stcfg2[cluster].u64 & ((1ull << 32) - 1));
 			CVMX_MT_CRC_DWORD(stalg[cluster].u64 & ((1ull << 32) - 1));
@@ -1448,12 +1751,13 @@ int cvmx_pki_config_dump(unsigned node)
 		NMPRINT(nclusters, mask, __i, 0, "", "%*s", lines[__i]);
 		cvmx_printf("Buffering:\n");
 		stbuf.u64 = cvmx_read_csr_node(node, CVMX_PKI_STYLEX_BUF(style));
-		printfl(1, "WQE header", "%s", stbuf.s.wqe_hsz ? "WORD0..5" : "WORD0..4");
-		printfl(1, "WQE and Data buffers", "%s", stbuf.s.dis_wq_dat ? "Separate" : "Same");
-		printfl(1, "WQE Skip", "%d", stbuf.s.wqe_skip);
-		printfl(1, "First Skip", "%d", stbuf.s.first_skip);
-		printfl(1, "Later Skip", "%d", stbuf.s.later_skip);
-		printfl(1, "OPC Mode", "%d", stbuf.s.opc_mode);
+		pki_flprint(1, "WQE header", "%s", stbuf.s.wqe_hsz ? "WORD0..5" : "WORD0..4");
+		pki_flprint(1, "WQE and Data buffers", "%s", stbuf.s.dis_wq_dat ? "Separate" : "Same");
+		pki_flprint(1, "WQE Skip (bytes)", "%d", stbuf.s.wqe_skip * 128);
+		pki_flprint(1, "First Skip (bytes)", "%d", stbuf.s.first_skip * 8);
+		pki_flprint(1, "Later Skip (bytes)", "%d", stbuf.s.later_skip * 8);
+		pki_flprint(1, "MB Size (bytes)", "%d", stbuf.s.mb_size * 8);
+		pki_flprint(1, "OPC Mode", "%d", stbuf.s.opc_mode);
 		NMPRINT(nclusters, mask, __i, 0, "Strip FCS", "%*s", stcfg[__i].s.fcs_strip ? "Yes":"No");
 		NMPRINT(nclusters, mask, __i, 0, "Drop", "%*d", stcfg[__i].s.drop);
 		NMPRINT(nclusters, mask, __i, 0, "No Drop", "%*d", stcfg[__i].s.nodrop);
@@ -1493,11 +1797,11 @@ int cvmx_pki_config_dump(unsigned node)
 				tagctl.u64 = cvmx_read_csr_node(node, CVMX_PKI_TAG_INCX_CTL(tagx));
 				incmask.u64 = cvmx_read_csr_node(node, CVMX_PKI_TAG_INCX_MASK(tagx));
 				sprintf(lines[0], "M-Tag%d", i);
-				printfl(2, lines[0], "%s:%d:%016llx", mtagptr_map[tagctl.s.ptr_sel], tagctl.s.offset, incmask.s.en);
+				pki_flprint(2, lines[0], "%s:%d:%016llx", mtagptr_map[tagctl.s.ptr_sel], tagctl.s.offset, incmask.s.en);
 			}
 		}
 		if (NMCMPEQ(0, stcfg2[__i].s.tag_masken, 4, mask, __i) != 0) {
-			printfl(1, "Tag Mask (hex)", "%*x", PKI_PRN_DATALEN, tagmask.s.mask);
+			pki_flprint(1, "Tag Mask (hex)", "%*x", PKI_PRN_DATALEN, tagmask.s.mask);
 		}
 		cvmx_printf("QPG:\n");
 		NMPRINT(nclusters, mask, __i, 1, "QOS Algo", "%*s", qpgqos_map[stalg[__i].s.qpg_qos]);
@@ -1506,10 +1810,24 @@ int cvmx_pki_config_dump(unsigned node)
 			NMPRINT(nclusters, mask, __i, 1, "Port MSB (hex)", "%*x", stalg[__i].s.qpg_port_msb);
 			NMPRINT(nclusters, mask, __i, 1, "Port Shift (dec)", "%*d", stalg[__i].s.qpg_port_sh);
 		}
-		NMPRINT(nclusters, mask, __i, 1, "QPG => PortAdder", "%*s", stcfg[__i].s.qpg_dis_padd ? "Off":"On");
-		NMPRINT(nclusters, mask, __i, 1, "QPG => Aura", "%*s", stcfg[__i].s.qpg_dis_aura ? "Off":"On");
-		NMPRINT(nclusters, mask, __i, 1, "QPG => Group", "%*s", stcfg[__i].s.qpg_dis_grp ? "Off":"On");
-		NMPRINT(nclusters, mask, __i, 1, "WQE[TAG] => Group", "%*s", stcfg[__i].s.qpg_dis_grptag ? "Off":"On");
+		NSPRINT(nclusters, __i, stcfg[__i].s.qpg_dis_padd ? \
+				sprintf(lines[__i], "Dis") : sprintf(lines[__i], "%d",qpgtx[__i].s.padd));
+		NMPRINT(nclusters, mask, __i, 1, "QPG PortAdder", "%*s", lines[__i]);
+		NSPRINT(nclusters, __i, stcfg[__i].s.qpg_dis_aura ? \
+				sprintf(lines[__i], "Dis") : sprintf(lines[__i], "%d",qpgtx[__i].s.laura));
+		NMPRINT(nclusters, mask, __i, 1, "QPG Aura", "%*s", lines[__i]);
+		NSPRINT(nclusters, __i, stcfg[__i].s.qpg_dis_grp ? \
+				sprintf(lines[__i], "Dis") : sprintf(lines[__i], "%d",qpgtx[__i].s.grp_ok));
+		NMPRINT(nclusters, mask, __i, 1, "QPG Group OK", "%*s", lines[__i]);
+		NSPRINT(nclusters, __i, stcfg[__i].s.qpg_dis_grp ? \
+				sprintf(lines[__i], "Dis") : sprintf(lines[__i], "%d",qpgtx[__i].s.grp_bad));
+		NMPRINT(nclusters, mask, __i, 1, "QPG Group Bad", "%*s", lines[__i]);
+		NSPRINT(nclusters, __i, stcfg[__i].s.qpg_dis_grptag ? \
+				sprintf(lines[__i], "Dis") : sprintf(lines[__i], "%d",qpgtx[__i].s.grptag_ok));
+		NMPRINT(nclusters, mask, __i, 1, "QPG GrpTag OK", "%*s", lines[__i]);
+		NSPRINT(nclusters, __i, stcfg[__i].s.qpg_dis_grptag ? \
+				sprintf(lines[__i], "Dis") : sprintf(lines[__i], "%d",qpgtx[__i].s.grptag_bad));
+		NMPRINT(nclusters, mask, __i, 1, "QPG GrpTag Bad", "%*s", lines[__i]);
 	}
 	if (style > 0 && (style - 1) != ibase)
 		cvmx_printf("\nSTYLE(s) %02d-%02d -- same as STYLE %02d\n", style - 1, ibase + 1, ibase);
@@ -1523,8 +1841,8 @@ int cvmx_pki_config_dump(unsigned node)
 
 		DLMPRINT("CLUSTER GROUP %d:", group);
 		cvmx_printf("Action = {PMC(hex) : +STYLE(dec) : PF(hex) : SETTY(dec) : ADVANCE(dec)}\n");
-		printfl(0, "Parsing", "%s", cgcfg.s.pena ? "Enabled" : "Disabled");
-		printfl(0, "Entry", "%*s%*s", PKI_PRN_DPLEN(2), "PCAM0", PKI_PRN_DPLEN(2), "PCAM1");
+		pki_flprint(0, "Parsing", "%s", cgcfg.s.pena ? "Enabled" : "Disabled");
+		pki_flprint(0, "Entry", "%*s%*s", PKI_PRN_DPLEN(2), "PCAM0", PKI_PRN_DPLEN(2), "PCAM1");
 		for (cluster = 0; cluster < nclusters; cluster++) {
 			cvmx_pki_clx_pcamx_termx_t term[CVMX_PKI_NUM_PCAM_BANK];
 			cvmx_pki_clx_pcamx_matchx_t	match[CVMX_PKI_NUM_PCAM_BANK];
@@ -1544,7 +1862,7 @@ int cvmx_pki_config_dump(unsigned node)
 				if ((term[0].s.valid + term[1].s.valid) == 0)
 					continue;
 				sprintf(lines[0], "[%d]", entry);
-				printfl(0, lines[0], "");
+				pki_flprint(0, lines[0], "");
 				NSPRINT(2, __i, sprintf(lines[__i], "%x:%x", term[__i].s.term1,
 					term[__i].s.term1 ^ term[__i].s.term0));
 				NMPRINT(2, 0x3, __i, 1, "Term:Mask (hex)", "%*s", lines[__i]);
@@ -1597,6 +1915,8 @@ int cvmx_pki_config_dump(unsigned node)
 	cvmx_printf("%-*s%*s%*s\n", n * 6, "Channel", n, "", n, "BPID");
 	for (i = 0; i < CVMX_PKI_NUM_CHANNEL; i++) {
 		cvmx_pki_chanx_cfg_t chan;
+		int nbgxs = pki_get_num_bgxs();
+
 		chan.u64 = cvmx_read_csr_node(node, CVMX_PKI_CHANX_CFG(i));
 		if (chan.s.imp == 1 && chan.s.bpid != 0) {
 
@@ -1616,7 +1936,7 @@ int cvmx_pki_config_dump(unsigned node)
 				k = i - 0x400;
 				sprintf(lines[1], "ILK%d:%d", k / 256, k % 256);
 			}
-			else if (i >= 0x800 && i < (0x800 + 6 * 4 * 16)) { /* BGX */
+			else if (i >= 0x800 && i < (0x800 + nbgxs * 4 * 16)) { /* BGX */
 				k = i - 0x800;
 				sprintf(lines[1], "BGX%d:Port%d:%d", k / (4 * 16),
 					(k % (4 * 16)) / 16, (k % (4 * 16)) % 16);
@@ -1636,67 +1956,139 @@ int cvmx_pki_config_dump(unsigned node)
  */
 int cvmx_pki_stats_dump(unsigned node)
 {
-	int i;
-	int64_t n;
+	int pkind;
+	int64_t count;
 	cvmx_pki_stat_ctl_t ctl;
 	struct cvmx_pki_port_stats stats;
+	char chans[128];
+	char line[256];
 
+	memset(line, '*', PKI_PRN_LINELEN); line[PKI_PRN_LINELEN] = '\0';
+	cvmx_dprintf("\n%s\n", line);
 	cvmx_dprintf("PKI Statistics on Node %d:\n", node);
+	cvmx_dprintf("%s\n", line);
 	ctl.u64 = cvmx_read_csr_node(node, CVMX_PKI_STAT_CTL);
 
-	for (i = 0; i < CVMX_PKI_NUM_PKIND; i++) {
-		cvmx_pki_get_stats(node, i, &stats);
-		if (stats.inb_octets > 0) {
-			cvmx_dprintf("PKIND %d\n", i);
-			cvmx_dprintf("%26s %14lld\n", "Packets:",
-				(long long)stats.inb_packets);
-			if (stats.inb_errors > 0)
-				cvmx_dprintf("%26s %14lld\n", "Err.packets:",
-					(long long)stats.inb_errors);
-			if (ctl.s.mode == 0x1) {
-				if (stats.octets > 0) {
-					cvmx_dprintf("STYLE %d\n", i);
-					cvmx_dprintf("%26s %14lld\n", "Packets:",
-						(long long)stats.packets);
-				}
-			}
+	for (pkind = 0; pkind < CVMX_PKI_NUM_PKIND; pkind++) {
+		cvmx_pki_get_stats(node, pkind, &stats);
+
+		if ((stats.inb_octets + stats.inb_errors) > 0) {
+			pki_find_pkind_chans(pkind, chans, PKI_PRN_DATALEN);
+			cvmx_dprintf("PKIND %d  <= %s\n", pkind, (chans[0] != 0) ? chans : "??");
+			if (stats.inb_octets)
+				pki_flprint(2, "Packets:", "%lld", stats.inb_packets);
+			if (stats.inb_errors)
+				pki_flprint(2, "Err.packets:", "%lld", stats.inb_errors);
+		}
+		count = stats.packets;
+		count += stats.len_64_packets;
+		count += stats.len_65_127_packets;
+		count += stats.len_128_255_packets;
+		count += stats.len_256_511_packets;
+		count += stats.len_512_1023_packets;
+		count += stats.len_1024_1518_packets;
+		count += stats.len_1519_max_packets;
+		count += stats.pci_raw_packets;
+		count += stats.dropped_packets;
+		count += stats.fcs_align_err_packets;
+		count += stats.runt_crc_packets;
+		count += stats.runt_packets;
+		count += stats.oversize_crc_packets;
+		count += stats.oversize_packets;
+
+		if (count > 0) {
+			if (ctl.s.mode == 0x1)
+				cvmx_dprintf("STYLE %d\n", pkind);
+			if (stats.packets > 0)
+				pki_flprint(2, "Non-dropped packets:", "%lld", stats.packets);
 			if (stats.len_64_packets > 0)
-				cvmx_dprintf("%26s %14lld\n", "1..63 packets:",
-					(long long)stats.len_64_packets);
+				pki_flprint(2, "64-byte packets:", "%lld", stats.len_64_packets);
 			if (stats.len_65_127_packets > 0)
-				cvmx_dprintf("%26s %14lld\n", "64..127 packets:",
-					(long long)stats.len_65_127_packets);
+				pki_flprint(2, "65-128-byte packets:", "%lld", stats.len_65_127_packets);
 			if (stats.len_128_255_packets > 0)
-				cvmx_dprintf("%26s %14lld\n", "128..255 packets:",
-					(long long)stats.len_128_255_packets);
+				pki_flprint(2, "128-255-byte packets:", "%lld", stats.len_128_255_packets);
 			if (stats.len_256_511_packets > 0)
-				cvmx_dprintf("%26s %14lld\n", "256..511 packets:",
-					(long long)stats.len_256_511_packets);
+				pki_flprint(2, "256-511-byte packets:", "%lld", stats.len_256_511_packets);
 			if (stats.len_512_1023_packets > 0)
-				cvmx_dprintf("%26s %14lld\n", "512..1023 packets:",
-					(long long)stats.len_512_1023_packets);
+				pki_flprint(2, "512-1023-byte packets:", "%lld", stats.len_512_1023_packets);
 			if (stats.len_1024_1518_packets > 0)
-				cvmx_dprintf("%26s %14lld\n", "1024..1518 packets:",
-					(long long)stats.len_1024_1518_packets);
+				pki_flprint(2, "1024-1518-byte packets:", "%lld", stats.len_1024_1518_packets);
 			if (stats.len_1519_max_packets > 0)
-				cvmx_dprintf("%26s %14lld\n", ">1518 packets:",
-					(long long)stats.len_1519_max_packets);
+				pki_flprint(2, ">1519-byte packets:", "%lld", stats.len_1519_max_packets);
 			if (stats.pci_raw_packets > 0)
-				cvmx_dprintf("%26s %14lld\n", "Raw packets:",
-					(long long)stats.pci_raw_packets);
+				pki_flprint(2, "Raw packets:", "%lld", stats.pci_raw_packets);
 			if (stats.dropped_packets > 0)
-				cvmx_dprintf("%26s %14lld\n", "Dropped packets:",
-					(long long)stats.dropped_packets);
-			n = stats.fcs_align_err_packets;
-			n += stats.runt_crc_packets;
-			n += stats.runt_packets;
-			n += stats.oversize_crc_packets;
-			n += stats.oversize_packets;
-			if (n > 0)
-				cvmx_dprintf("%26s %14lld\n", "Err.packets:", (long long)n);
+				pki_flprint(2, "Dropped packets:", "%lld", stats.dropped_packets);
+			if (stats.fcs_align_err_packets > 0)
+				pki_flprint(2, "FCS errors:", "%lld", stats.fcs_align_err_packets);
+			if (stats.runt_crc_packets > 0)
+				pki_flprint(2, "Runt&FCS errors:", "%lld", stats.runt_crc_packets);
+			if (stats.runt_packets > 0)
+				pki_flprint(2, "Runt errors:", "%lld", stats.runt_packets);
+			if (stats.oversize_crc_packets > 0)
+				pki_flprint(2, "Oversize&FCS errors:", "%lld", stats.oversize_crc_packets);
+			if (stats.oversize_packets > 0)
+				pki_flprint(2, "Oversize packets:", "%lld", stats.oversize_packets);
 		}
 	}
 	return 0;
 }
+
+int pki_find_pkind_chans(int pkind, char *buf, int blen)
+{
+	int node, bi, block, port, nbgxs, nchans;
+	cvmx_bgxx_cmrx_rx_id_map_t bgxmap;
+	cvmx_lbk_chx_pkind_t lbkmap;
+	int bcheck_term(int bi, int blen) {
+		if ((bi + 8/*name max lendth*/) >= blen) {
+			sprintf(&buf[bi], " ..."); 
+			return 1;
+		}
+		return 0;
+	}
+
+	memset(buf, 0, blen);
+	if ((nbgxs = pki_get_num_bgxs()) == 0)
+		return 0;
+	node = cvmx_get_node_num();
+	/* BGX */
+	bi = nchans = 0;
+	for (block = 0; block < nbgxs; block++) {
+		for (port = 0; port < 4; port++) {
+			bgxmap.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_RX_ID_MAP(port, block));
+			if (bgxmap.s.pknd == pkind) {
+				nchans++;
+				bi += sprintf(&buf[bi], " BGX%d:%d", block, port); 
+				if (bcheck_term(bi, blen) != 0)
+					return nchans;
+			}
+		}
+	}
+#if 0
+//TODO: Somehow, the register read causes the CPU crash.
+	// cvmx_sli_pkt_gbl_control_t dpimap;
+	/* DPI */
+	dpimap.u64 = cvmx_read_csr_node(node, CVMX_SLI_PKT_GBL_CONTROL);
+	(void)dpimap;
+	if (dpimap.s.bpkind == pkind) {
+		nchans++;
+		bi += sprintf(&buf[bi], " DPI"); 
+		if (bcheck_term(bi, blen) != 0)
+			return nchans;
+	}
+#endif
+	/* LBK */
+	for (block = 0; block < 64; block++) {
+		lbkmap.u64 = cvmx_read_csr_node(node, CVMX_LBK_CHX_PKIND(block));
+		if (lbkmap.s.pkind == pkind) {
+			nchans++;
+			bi += sprintf(&buf[bi], " LBK:%d", block); 
+			if (bcheck_term(bi, blen) != 0)
+				return nchans;
+		}
+	}
+	return nchans;
+}
+
 #endif /* CVMX_DUMP_PKI */
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pko3.c b/arch/mips/cavium-octeon/executive/cvmx-pko3.c
index 83772a9..769b76e 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pko3.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pko3.c
@@ -186,9 +186,10 @@ int cvmx_pko3_hw_init_global(int node, uint16_t aura)
 		return -1;
 	}
 
-	/* set max outstanding requests in IOBP for any FIFO */
-	ptf_iobp_cfg.u64 = 0;
-	ptf_iobp_cfg.s.max_read_size = 72;	/* HRM: typical=0x48 */
+	/* Set max outstanding requests in IOBP for any FIFO.
+	 * Use 3 to reduce chances of getting "underflow" in BGX TX FIFO. */
+	ptf_iobp_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKO_PTF_IOBP_CFG);
+	ptf_iobp_cfg.s.max_read_size = 3;
 	cvmx_write_csr_node(node, CVMX_PKO_PTF_IOBP_CFG, ptf_iobp_cfg.u64);
 
 	/* Set minimum packet size per Ethernet standard */
@@ -541,7 +542,6 @@ static int cvmx_pko_setup_macs(int node)
 	unsigned fifo_count = 0;
 	unsigned max_fifos = 0, fifo_groups = 0;
 	struct {
-		cvmx_helper_interface_mode_t mac_mode;
 		uint8_t fifo_cnt;
 		uint8_t fifo_id;
 		uint8_t pri;
@@ -565,8 +565,6 @@ static int cvmx_pko_setup_macs(int node)
 
 	/* Initialize all MACs as disabled */
 	for(mac_num = 0; mac_num < __cvmx_pko3_num_macs(); mac_num++) {
-		cvmx_pko3_mac_table[mac_num].mac_mode =
-			CVMX_HELPER_INTERFACE_MODE_DISABLED;
 		cvmx_pko3_mac_table[mac_num].pri = 0;
 		cvmx_pko3_mac_table[mac_num].fifo_cnt = 0;
 		cvmx_pko3_mac_table[mac_num].fifo_id = 0x1f;
@@ -600,7 +598,6 @@ static int cvmx_pko_setup_macs(int node)
 				continue;
 			}
 
-			cvmx_pko3_mac_table[i].mac_mode = mode;
 			if(mode == CVMX_HELPER_INTERFACE_MODE_RXAUI) {
 				cvmx_pko3_mac_table[i].fifo_cnt = 2;
 				cvmx_pko3_mac_table[i].pri = 2;
@@ -614,7 +611,7 @@ static int cvmx_pko_setup_macs(int node)
 				cvmx_pko3_mac_table[i].mac_fifo_cnt = 4;
 			} else if (mode == CVMX_HELPER_INTERFACE_MODE_XFI) {
 				cvmx_pko3_mac_table[i].fifo_cnt = 4;
-				cvmx_pko3_mac_table[i].pri = 1;
+				cvmx_pko3_mac_table[i].pri = 3;
 				cvmx_pko3_mac_table[i].spd = 10;
 				cvmx_pko3_mac_table[i].mac_fifo_cnt = 1;
 			} else if (mode == CVMX_HELPER_INTERFACE_MODE_XLAUI) {
@@ -629,6 +626,13 @@ static int cvmx_pko_setup_macs(int node)
 				/* ILK/SRIO: speed depends on lane count */
 				cvmx_pko3_mac_table[i].spd = 40;
 				cvmx_pko3_mac_table[i].mac_fifo_cnt = 4;
+			} else if (mode == CVMX_HELPER_INTERFACE_MODE_NPI) {
+				cvmx_pko3_mac_table[i].fifo_cnt = 4;
+				cvmx_pko3_mac_table[i].pri = 2;
+				/* Actual speed depends on PCIe lanes/mode */
+				cvmx_pko3_mac_table[i].spd = 50;
+				/* SLI Tx FIFO size to be revisitted */
+				cvmx_pko3_mac_table[i].mac_fifo_cnt = 1;
 			} else {
 				cvmx_pko3_mac_table[i].fifo_cnt = 1;
 				cvmx_pko3_mac_table[i].pri = 1;
@@ -740,7 +744,10 @@ static int cvmx_pko_setup_macs(int node)
 			pko_ptgfx_cfg.s.reset = 1;
 		pko_ptgfx_cfg.s.size = fifo_group_cfg[fifo] ;
 		if( fifo_group_spd[fifo] >= 40 )
-			pko_ptgfx_cfg.s.rate = 3;	/* 50 Gbps */
+			if( pko_ptgfx_cfg.s.size >= 3)
+				pko_ptgfx_cfg.s.rate = 3;	/* 50 Gbps */
+			else
+				pko_ptgfx_cfg.s.rate = 2;	/* 25 Gbps */
 		else if( fifo_group_spd[fifo] >= 20 )
 			pko_ptgfx_cfg.s.rate = 2;	/* 25 Gbps */
 		else if( fifo_group_spd[fifo] >= 10 )
diff --git a/arch/mips/include/asm/octeon/cvmx-ase-defs.h b/arch/mips/include/asm/octeon/cvmx-ase-defs.h
index c1ad227..9170c0a 100644
--- a/arch/mips/include/asm/octeon/cvmx-ase-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-ase-defs.h
@@ -731,7 +731,9 @@ union cvmx_ase_ecc_int {
                                                          INTERNAL: RFT replicated 5 times for timing purposes, this indicates error for any of the
                                                          RFT. */
 	uint64_t lue_tat_dbe                  : 1;  /**< Detected double-bit error on LUE HST ruleDB access table. This bit is not set for software
-                                                         accesses to the TAT; it only gets set for lookup accesses. */
+                                                         accesses to the TAT; it only gets set for lookup accesses. It is expected that error
+                                                         recovery will
+                                                         require resetting the ASE and loading corrected software into the TAT. */
 	uint64_t lue_tat_sbe                  : 1;  /**< Detected and corrected single-bit error on LUE HST ruleDB access table. This bit is not
                                                          set for software accesses to the TAT; it only gets set for lookup accesses. */
 	uint64_t lue_kdb_dbe                  : 1;  /**< Detected double-bit error on LUE KRQ key data buffer. */
@@ -807,10 +809,12 @@ union cvmx_ase_gen_int {
                                                          INTERNAL: The cause could also be a remote request or migration request. */
 	uint64_t lue_hr_err_log               : 1;  /**< An error occurred for a host request and generated a host response with error. */
 	uint64_t reserved_35_36               : 2;
-	uint64_t lue_tic_bad_write            : 1;  /**< Data was loaded in to the TIC that results in a wrap condition. Either the TAT row pointed
-                                                         to by the TIC entry is invalid, or the starting TAT row and the increment value points
-                                                         beyond the send of the TAT. It is expected that error recovery will require loading
-                                                         corrected software into the TIC. */
+	uint64_t lue_tic_bad_write            : 1;  /**< A data load to the TIC was prevented that would have caused a wrap condition. Either the
+                                                         TAT row pointed
+                                                         to by the TIC entry was invalid, or the starting TAT row and the increment value pointed
+                                                         beyond the end of the TAT. It is expected that error recovery will require loading
+                                                         correct software into the TIC.
+                                                         INTERNAL: Hardware will never set this bit in Pass 1.x. */
 	uint64_t lue_tic_multi_hit            : 1;  /**< A TIC lookup request resulted in multiple entries reporting a hit. It is expected that
                                                          error recovery will require resetting the ASE and loading corrected software into the TIC. */
 	uint64_t lue_tic_miss                 : 1;  /**< A TIC lookup request did not match a valid entry. It is expected that error recovery will
diff --git a/arch/mips/include/asm/octeon/cvmx-bootmem.h b/arch/mips/include/asm/octeon/cvmx-bootmem.h
index 0490dff..4a54038 100644
--- a/arch/mips/include/asm/octeon/cvmx-bootmem.h
+++ b/arch/mips/include/asm/octeon/cvmx-bootmem.h
@@ -42,7 +42,7 @@
  * Simple allocate only memory allocator.  Used to allocate memory at application
  * start time.
  *
- * <hr>$Revision: 109474 $<hr>
+ * <hr>$Revision: 125700 $<hr>
  *
  */
 
diff --git a/arch/mips/include/asm/octeon/cvmx-dpi-defs.h b/arch/mips/include/asm/octeon/cvmx-dpi-defs.h
index c09e1a7..b179f4c 100644
--- a/arch/mips/include/asm/octeon/cvmx-dpi-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-dpi-defs.h
@@ -1929,7 +1929,7 @@ union cvmx_dpi_engx_buf {
 	uint64_t base                         : 6;  /**< The base address in 512-byte blocks of the DMA engine FIFO. */
 	uint64_t blks                         : 4;  /**< The size of the DMA engine FIFO. The sum of the allocated FIFOs across all six
                                                          DPI_ENG()_BUF[BLKS] registers must not exceed the overall RDB memory size of
-                                                         16 KB.
+                                                         32 KB.
                                                          0x0 = Engine disabled.
                                                          0x1 = 0.5 KB FIFO.
                                                          0x2 = 1.0 KB FIFO.
diff --git a/arch/mips/include/asm/octeon/cvmx-gserx-defs.h b/arch/mips/include/asm/octeon/cvmx-gserx-defs.h
index 9305b8d..d3a2e0f 100644
--- a/arch/mips/include/asm/octeon/cvmx-gserx-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-gserx-defs.h
@@ -9497,19 +9497,20 @@ union cvmx_gserx_spd {
                                                          pins during chip cold reset. For non-CCPI links, this field is not used.
                                                          For SPD settings that configure a non-default reference clock, hardware updates the PLL
                                                          settings of the specific lane mode (LMODE) table entry to derive the correct link rate.
+                                                         (changed for pass 2)
                                                          <pre>
                                                          SPD   REFCLK      Link rate   LMODE
-                                                         0x0:  100 MHz     1.25 Gb     R_125G_REFCLK15625_KX
+                                                         0x0:  100 MHz     5 Gb        R_5G_REFCLK100
                                                          0x1:  100 MHz     2.5 Gb      R_25G_REFCLK100
                                                          0x2:  100 MHz     5 Gb        R_5G_REFCLK100
                                                          0x3:  100 MHz     8 Gb        R_8G_REFCLK100
-                                                         0x4:  125 MHz     1.25 Gb     R_125G_REFCLK15625_KX
-                                                         0x5:  125 MHz     2.5 Gb      R_25G_REFCLK125
+                                                         0x4:  100 MHz     8 Gb        R_8G_REFCLK100
+                                                         0x5:  100 MHz     8 Gb        R_8G_REFCLK100
                                                          0x6:  125 MHz     3.125 Gb    R_3125G_REFCLK15625_XAUI
                                                          0x7:  125 MHz     5 Gb        R_5G_REFCLK125
                                                          0x8:  125 MHz     6.25 Gb     R_625G_REFCLK15625_RXAUI
                                                          0x9:  125 MHz     8 Gb        R_8G_REFCLK125
-                                                         0xA:  156.25 MHz  2.5 Gb      R_25G_REFCLK100
+                                                         0xA:  156.25 MHz  10.3125 Gb  R_103125G_REFCLK15625_KR
                                                          0xB:  156.25 MHz  3.125 Gb    R_3125G_REFCLK15625_XAUI
                                                          0xC:  156.25 MHz  5 Gb        R_5G_REFCLK125
                                                          0xD:  156.25 MHz  6.25 Gb     R_625G_REFCLK15625_RXAUI
diff --git a/arch/mips/include/asm/octeon/cvmx-helper-bgx.h b/arch/mips/include/asm/octeon/cvmx-helper-bgx.h
index 49a69cc..c1b7b8b 100644
--- a/arch/mips/include/asm/octeon/cvmx-helper-bgx.h
+++ b/arch/mips/include/asm/octeon/cvmx-helper-bgx.h
@@ -59,6 +59,7 @@ extern "C" {
 #endif
 
 #define CVMX_BGX_RX_FIFO_SIZE	(64 * 1024)
+#define CVMX_BGX_TX_FIFO_SIZE	(32 * 1024)
 
 extern int __cvmx_helper_bgx_enumerate(int xiface);
 
diff --git a/arch/mips/include/asm/octeon/cvmx-helper-pki.h b/arch/mips/include/asm/octeon/cvmx-helper-pki.h
index 2944031..6bf02ab 100644
--- a/arch/mips/include/asm/octeon/cvmx-helper-pki.h
+++ b/arch/mips/include/asm/octeon/cvmx-helper-pki.h
@@ -351,14 +351,23 @@ void cvmx_helper_pki_set_dflt_style(int node, struct cvmx_pki_style_config *styl
 
 void cvmx_helper_pki_no_dflt_init(int node);
 
+void cvmx_helper_pki_set_dflt_bp_en(int node, bool bp_en);
+
 void cvmx_pki_dump_wqe(const cvmx_wqe_78xx_t *wqp);
 
 int __cvmx_helper_pki_port_setup(int node, int ipd_port);
 
 int __cvmx_helper_pki_global_setup(int node);
+void cvmx_helper_pki_show_port_config(int ipd_port);
 
 int __cvmx_helper_pki_install_dflt_vlan(int node);
 void __cvmx_helper_pki_set_dflt_ltype_map(int node);
+int cvmx_helper_pki_route_dmac(int node, int style, uint64_t mac_addr, uint64_t mac_addr_mask, int final_style);
+int cvmx_pki_clone_style(int node, int style, uint64_t cluster_mask);
+void cvmx_helper_pki_modify_prtgrp(int ipd_port, int grp_ok, int grp_bad);
+int cvmx_helper_pki_route_prt_dmac(int ipd_port, uint64_t mac_addr, uint64_t mac_addr_mask, int grp);
+
+void cvmx_helper_pki_errata(int node);
 
 #ifdef __cplusplus
 /* *INDENT-OFF* */
diff --git a/arch/mips/include/asm/octeon/cvmx-lmcx-defs.h b/arch/mips/include/asm/octeon/cvmx-lmcx-defs.h
index 90359ac..327bc4e 100644
--- a/arch/mips/include/asm/octeon/cvmx-lmcx-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-lmcx-defs.h
@@ -5730,7 +5730,11 @@ union cvmx_lmcx_dbtrain_ctl {
 	uint64_t u64;
 	struct cvmx_lmcx_dbtrain_ctl_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_62_63               : 2;
+	uint64_t reserved_63_63               : 1;
+	uint64_t lfsr_pattern_sel             : 1;  /**< If set high, the sequence uses 32-bit LFSR pattern when generating data sequence
+                                                         during the General R/W training (LMC()_DBTRAIN_CTL[RW_TRAIN] == 1).
+                                                         INTERNAL:
+                                                         The LFSR polynomials are programmed by the CSR LMC()_CHAR_CTL[PRBS]. */
 	uint64_t cmd_count_ext                : 2;  /**< Extension bits to the field DBTRAIN_CTL[READ_CMD_COUNT]. This enables
                                                          up to 128 read and write commmands. */
 	uint64_t db_output_impedance          : 3;  /**< Reserved.
@@ -5803,7 +5807,8 @@ union cvmx_lmcx_dbtrain_ctl {
 	uint64_t db_sel                       : 1;
 	uint64_t db_output_impedance          : 3;
 	uint64_t cmd_count_ext                : 2;
-	uint64_t reserved_62_63               : 2;
+	uint64_t lfsr_pattern_sel             : 1;
+	uint64_t reserved_63_63               : 1;
 #endif
 	} s;
 	struct cvmx_lmcx_dbtrain_ctl_cn73xx {
@@ -5880,7 +5885,84 @@ union cvmx_lmcx_dbtrain_ctl {
 #endif
 	} cn73xx;
 	struct cvmx_lmcx_dbtrain_ctl_s        cn78xxp2;
-	struct cvmx_lmcx_dbtrain_ctl_s        cnf75xx;
+	struct cvmx_lmcx_dbtrain_ctl_cnf75xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_62_63               : 2;
+	uint64_t cmd_count_ext                : 2;  /**< Extension bits to the field DBTRAIN_CTL[READ_CMD_COUNT]. This enables
+                                                         up to 128 read and write commmands. */
+	uint64_t db_output_impedance          : 3;  /**< Reserved.
+                                                         INTERNAL:
+                                                         Host Interface DQ/DQS Output Driver Impedance control.
+                                                         This is the default value used during Host Interface Write Leveling in LRDIMM
+                                                         environment, i.e., CONFIG[LRDIMM_ENA] = 1, SEQ_CTL[SEQ_SEL] = 0x6.
+                                                         0x0 = RZQ/6 (40 ohm).
+                                                         0x1 = RZQ/7 (34 ohm).
+                                                         0x2 = RZQ/5 (48 ohm).
+                                                         0x3-0x7 = Reserved. */
+	uint64_t db_sel                       : 1;  /**< Reserved.
+                                                         INTERNAL:
+                                                         Used when running Host Interface Write Leveling.
+                                                         0 = selects DIMM0's Data Buffer.
+                                                         1 = selects DIMM1's Data Buffer. */
+	uint64_t tccd_sel                     : 1;  /**< When set, the sequence uses MODEREG_PARAMS3[TCCD_L] to space out
+                                                         back-to-back read commands. Otherwise it will space out back-to-back
+                                                         reads with a default value of 4 cycles.
+                                                         While in DRAM MPR mode, reads from Page 0 may use tCCD_S or tCCD_L.
+                                                         Reads from Pages 1, 2 or 3 however must use tCCD_L, thereby requring
+                                                         this bit to be set. */
+	uint64_t rw_train                     : 1;  /**< When set, the sequence will perform a Write to the DRAM
+                                                         memory array using burst patern that are set in the CSRs
+                                                         LMC()_GENERAL_PURPOSE0[DATA]<61:0>, LMC()_GENERAL_PURPOSE1[DATA]<61:0> and
+                                                         LMC()_GENERAL_PURPOSE2[DATA]<15:0>.
+                                                         This burst pattern gets shifted by one byte at every cycle.
+                                                         The sequence will then do the reads to the same location and compare
+                                                         the data coming back with this pattern.
+                                                         The bit-wise comparison result gets stored in
+                                                         LMC()_MPR_DATA0[MPR_DATA]<63:0> and LMC()_MPR_DATA1[MPR_DATA]<7:0>. */
+	uint64_t read_dq_count                : 7;  /**< Reserved.
+                                                         INTERNAL:
+                                                         The amount of cycles until a pulse is issued to sample the DQ into the
+                                                         MPR register. This bits control the timing of when to sample the data
+                                                         buffer training result. */
+	uint64_t read_cmd_count               : 5;  /**< The amount of Read and Write Commands to be sent during the R/W training.
+                                                         INTERNAL:
+                                                         This can be set to zero in which case the sequence does not send any
+                                                         Read commands to accommodate for the DWL training mode. */
+	uint64_t write_ena                    : 1;  /**< Reserved.
+                                                         INTERNAL:
+                                                         Enables the write operation. This is mainly used to accomplish the MWD
+                                                         training sequence of the data buffer.
+                                                         LMC()_DBTRAIN_CTL[ACTIVATE] must be set to 1 for this to take effect. */
+	uint64_t activate                     : 1;  /**< Reserved.
+                                                         INTERNAL: Enables the activate command during the data buffer training sequence. */
+	uint64_t prank                        : 2;  /**< Physical Rank bits for Read/Write/Activate operation. */
+	uint64_t lrank                        : 3;  /**< Reserved.
+                                                         INTERNAL:
+                                                         Logical Rank bits for Read/Write/Activate operation during the data buffer
+                                                         training. */
+	uint64_t row_a                        : 18; /**< The row address for the Activate command. */
+	uint64_t bg                           : 2;  /**< The bank group that the R/W commands are directed to. */
+	uint64_t ba                           : 2;  /**< The bank address for the R/W commands are directed to. */
+	uint64_t column_a                     : 13; /**< Column address for the R/W operation. */
+#else
+	uint64_t column_a                     : 13;
+	uint64_t ba                           : 2;
+	uint64_t bg                           : 2;
+	uint64_t row_a                        : 18;
+	uint64_t lrank                        : 3;
+	uint64_t prank                        : 2;
+	uint64_t activate                     : 1;
+	uint64_t write_ena                    : 1;
+	uint64_t read_cmd_count               : 5;
+	uint64_t read_dq_count                : 7;
+	uint64_t rw_train                     : 1;
+	uint64_t tccd_sel                     : 1;
+	uint64_t db_sel                       : 1;
+	uint64_t db_output_impedance          : 3;
+	uint64_t cmd_count_ext                : 2;
+	uint64_t reserved_62_63               : 2;
+#endif
+	} cnf75xx;
 };
 typedef union cvmx_lmcx_dbtrain_ctl cvmx_lmcx_dbtrain_ctl_t;
 
@@ -7495,7 +7577,15 @@ union cvmx_lmcx_ext_config {
 	uint64_t u64;
 	struct cvmx_lmcx_ext_config_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t reserved_60_63               : 4;
+	uint64_t reserved_61_63               : 3;
+	uint64_t bc4_dqs_ena                  : 1;  /**< Reserved.
+                                                         INTERNAL:
+                                                           Added in Pass 2.0.
+                                                           CYA bit.
+                                                           When set to 1, LMC produces only 3 cycles of DQS transitions
+                                                           everytime it sends out BC4 Write operation.
+                                                           When set to 0, LMC produces the full bursts of DQS transitions,
+                                                           even for BC4 Write ops. */
 	uint64_t ref_block                    : 1;  /**< When set, LMC is blocked to initiate any refresh sequence. LMC then will only
                                                          allow refresh sequence to start when LMC()_REF_STATUS[REF_COUNT] has
                                                          reached the maximum value of 0x7. */
@@ -7655,7 +7745,8 @@ union cvmx_lmcx_ext_config {
 	uint64_t mrs_one_side                 : 1;
 	uint64_t mrs_side                     : 1;
 	uint64_t ref_block                    : 1;
-	uint64_t reserved_60_63               : 4;
+	uint64_t bc4_dqs_ena                  : 1;
+	uint64_t reserved_61_63               : 3;
 #endif
 	} s;
 	struct cvmx_lmcx_ext_config_cn70xx {
@@ -7697,10 +7788,173 @@ union cvmx_lmcx_ext_config {
 #endif
 	} cn70xx;
 	struct cvmx_lmcx_ext_config_cn70xx    cn70xxp1;
-	struct cvmx_lmcx_ext_config_s         cn73xx;
+	struct cvmx_lmcx_ext_config_cn73xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_60_63               : 4;
+	uint64_t ref_block                    : 1;  /**< When set, LMC is blocked to initiate any refresh sequence. LMC then will only
+                                                         allow refresh sequence to start when LMC()_REF_STATUS[REF_COUNT] has
+                                                         reached the maximum value of 0x7. */
+	uint64_t mrs_side                     : 1;  /**< Only applies when EXT_CONFIG[MRS_ONE_SIDE] is set.
+                                                         0 = MRS command is sent to the A side of an RDIMM.
+                                                         1 = MRS command is sent to the B side of an RDIMM. */
+	uint64_t mrs_one_side                 : 1;  /**< Only applies to DDR4 RDIMM.
+                                                         When set, MRS commands are directed to either the A or B
+                                                         side of the RCD.
+                                                         PDA operation is NOT allowed when this bit is set. In
+                                                         other words, MR_MPR_CTL[MR_WR_PDA_ENABLE]
+                                                         must be cleared before running MRW sequence with this
+                                                         bit turned on. */
+	uint64_t mrs_bside_invert_disable     : 1;  /**< When set, the command decoder cancels the auto inversion of
+                                                         A3-A9, A11, A13, A17, BA0, BA1 and BG0 during MRS/MRS_PDA
+                                                         command to the B side of the RDIMM.
+                                                         When set, make sure that the RCD's control word
+                                                         RC00 DA[0] = 1 so that the output inversion is disabled in
+                                                         the DDR4 RCD. */
+	uint64_t dimm_sel_invert_off          : 1;  /**< During coalesce_address_mode, the default logic would be to invert
+                                                         the pbank bit whenever NXM[MEM_MSB_D1_R0] > NXM[MEM_MSB_D0_R0].
+                                                         When this bit is set to 1, it disables this default behaviour.
+                                                         This configuration has lower priority compared to
+                                                         DIMM_SEL_FORCE_INVERT. */
+	uint64_t dimm_sel_force_invert        : 1;  /**< When set to 1, this bit forces the pbank bit to be inverted
+                                                         when in coalesce_address_mode. That is, pbank value of 0 selects
+                                                         DIMM1 instead of DIMM0.
+                                                         Intended to be use for the case of DIMM1 having bigger rank/s
+                                                         than DIMM0. This bit has priority over DIMM_SEL_INVERT_OFF. */
+	uint64_t coalesce_address_mode        : 1;  /**< When set to 1, LMC coalesces the L2C+LMC internal address mapping
+                                                         to create a uniform memory space that are free from holes in
+                                                         between ranks. When different size DIMMs are used, the DIMM with
+                                                         the higher capacity is mapped to the lower address space. */
+	uint64_t dimm1_cid                    : 2;  /**< Reserved.
+                                                         INTERNAL:
+                                                         DIMM1 configuration bits that represent the number of Chip
+                                                         ID of the DRAM. This value is use for decoding address
+                                                         as well as routing Chip IDs to the appropriate output
+                                                         pins.
+                                                         0x0 = 0 Chip ID  (Mono-Die stack).
+                                                         0x1 = 1 Chip ID  (2H 3DS).
+                                                         0x2 = 2 Chip IDs (4H 3DS).
+                                                         0x3 = 3 Chip IDs (8H 3DS). */
+	uint64_t dimm0_cid                    : 2;  /**< Reserved.
+                                                         INTERNAL:
+                                                         DIMM0 configuration bits that represent the number of Chip
+                                                         ID of the DRAM. This value is use for decoding address
+                                                         as well as routing Chip IDs to the appropriate output
+                                                         pins.
+                                                         0x0 = 0 Chip ID  (Mono-Die stack).
+                                                         0x1 = 1 Chip ID  (2H 3DS).
+                                                         0x2 = 2 Chip IDs (4H 3DS).
+                                                         0x3 = 3 Chip IDs (8H 3DS). */
+	uint64_t rcd_parity_check             : 1;  /**< Enables the one cycle delay of the CA parity output. This MUST be set to 1 when using DDR4
+                                                         RDIMM AND parity checking in RCD is enabled (RC0E DA0 = 1). Set this to 0 otherwise.
+                                                         To enable the parity checking in RCD, set this bit first BEFORE issuing the RCW write RC0E
+                                                         DA0 = 1. */
+	uint64_t reserved_46_47               : 2;
+	uint64_t error_alert_n_sample         : 1;  /**< Read to get a sample of the DDR*_ERROR_ALERT_L signal. */
+	uint64_t ea_int_polarity              : 1;  /**< Set to invert DDR*_ERROR_ALERT_L interrupt polarity. When clear, interrupt is signaled on
+                                                         the rising edge of DDR*_ERROR_ALERT_L. When set, interrupt is signalled on the falling
+                                                         edge of DDR*_ERROR_ALERT_L. */
+	uint64_t reserved_43_43               : 1;
+	uint64_t par_addr_mask                : 3;  /**< Mask applied to parity for address bits 14, 13, and 12. Clear to exclude these address
+                                                         bits from the parity calculation, necessary if the DRAM device does not have these pins. */
+	uint64_t reserved_38_39               : 2;
+	uint64_t mrs_cmd_override             : 1;  /**< Set to override the behavior of MRS and RCW operations.
+                                                         If this bit is set, the override behavior is governed by the control field
+                                                         MRS_CMD_SELECT. See LMC()_EXT_CONFIG[MRS_CMD_SELECT] for detail.
+                                                         If this bit is cleared, select operation where signals other than CS are active before
+                                                         and after the CS_N active cycle (except for the case when interfacing with DDR3 RDIMM). */
+	uint64_t mrs_cmd_select               : 1;  /**< When MRS_CMD_OVERRIDE is set, use this bit to select which style of operation for MRS and
+                                                         RCW commands.
+                                                         If this bit is clear, select operation where signals other than CS are active before and
+                                                         after the CS_N active cycle.
+                                                         When this bit is set, select the operation where the other command signals (DDR*_RAS_L,
+                                                         DDR*_CAS_L, DDR*_WE_L, DDR*_A<15:0>, etc) all are active only during the cycle where the
+                                                         CS_N is also active. */
+	uint64_t reserved_33_35               : 3;
+	uint64_t invert_data                  : 1;  /**< Set this bit to cause all data to be inverted before writing or reading to/from DRAM. This
+                                                         effectively uses the scramble logic to instead invert all the data, so this bit must not
+                                                         be set if data scrambling is enabled. May be useful if data inversion will result in lower
+                                                         power. */
+	uint64_t reserved_30_31               : 2;
+	uint64_t cmd_rti                      : 1;  /**< Set this bit to change the behavior of the LMC to return to a completely idle command (no
+                                                         CS active, no command pins active, and address/bank address/bank group all low) on the
+                                                         interface after an active command, rather than only forcing the CS inactive between
+                                                         commands. */
+	uint64_t cal_ena                      : 1;  /**< Set to cause LMC to operate in CAL mode. First set LMC()_MODEREG_PARAMS3[CAL], then
+                                                         set CAL_ENA. */
+	uint64_t reserved_27_27               : 1;
+	uint64_t par_include_a17              : 1;  /**< If set, include A17 in parity calculations in DDR4 mode. */
+	uint64_t par_include_bg1              : 1;  /**< If set, include BG1 in parity calculations in DDR4 mode. */
+	uint64_t gen_par                      : 1;  /**< Enable parity generation in the DRAM commands; must be set prior to enabling parity in
+                                                         register or DRAM devices. */
+	uint64_t reserved_21_23               : 3;
+	uint64_t vrefint_seq_deskew           : 1;  /**< Personality bit to change the operation of what is normally the internal Vref training
+                                                         sequence into the deskew training sequence. */
+	uint64_t read_ena_bprch               : 1;  /**< Enable pad receiver one cycle longer than normal during read operations. */
+	uint64_t read_ena_fprch               : 1;  /**< Enable pad receiver starting one cycle earlier than normal during read operations. */
+	uint64_t slot_ctl_reset_force         : 1;  /**< Write 1 to reset the slot-control override for all slot-control registers. After writing a
+                                                         1 to this bit, slot-control registers will update with changes made to other timing-
+                                                         control registers. This is a one-shot operation; it automatically returns to 0 after a
+                                                         write to 1. */
+	uint64_t ref_int_lsbs                 : 9;  /**< Refresh-interval value least-significant bits. The default is 0x0.
+                                                         Refresh interval is represented in number of 512 CK cycle increments and is controlled by
+                                                         the CSR LMC()_CONFIG[REF_ZQCS_INT]. More precise refresh interval however (in number of
+                                                         1 CK cycle) can be achieved by setting this field to a non-zero value. */
+	uint64_t drive_ena_bprch              : 1;  /**< Drive DQx for one cycle longer than normal during write operations. */
+	uint64_t drive_ena_fprch              : 1;  /**< Drive DQX starting one cycle earlier than normal during write operations. */
+	uint64_t dlcram_flip_synd             : 2;  /**< Reserved. INTERNAL: DLC RAM flip syndrome control bits. */
+	uint64_t dlcram_cor_dis               : 1;  /**< Reserved. INTERNAL: DLC RAM correction disable control. */
+	uint64_t dlc_nxm_rd                   : 1;  /**< When set, enable NXM events for HFA read operations. INTERNAL: Default is disabled, but
+                                                         could be useful for debug of DLC/DFA accesses. */
+	uint64_t l2c_nxm_rd                   : 1;  /**< When set, enable NXM events for L2C read operations. INTERNAL: Default is disabled as L2C
+                                                         NXM read operations are possible and expected during normal operation. */
+	uint64_t l2c_nxm_wr                   : 1;  /**< When set, enable NXM events for L2C write operations. */
+#else
+	uint64_t l2c_nxm_wr                   : 1;
+	uint64_t l2c_nxm_rd                   : 1;
+	uint64_t dlc_nxm_rd                   : 1;
+	uint64_t dlcram_cor_dis               : 1;
+	uint64_t dlcram_flip_synd             : 2;
+	uint64_t drive_ena_fprch              : 1;
+	uint64_t drive_ena_bprch              : 1;
+	uint64_t ref_int_lsbs                 : 9;
+	uint64_t slot_ctl_reset_force         : 1;
+	uint64_t read_ena_fprch               : 1;
+	uint64_t read_ena_bprch               : 1;
+	uint64_t vrefint_seq_deskew           : 1;
+	uint64_t reserved_21_23               : 3;
+	uint64_t gen_par                      : 1;
+	uint64_t par_include_bg1              : 1;
+	uint64_t par_include_a17              : 1;
+	uint64_t reserved_27_27               : 1;
+	uint64_t cal_ena                      : 1;
+	uint64_t cmd_rti                      : 1;
+	uint64_t reserved_30_31               : 2;
+	uint64_t invert_data                  : 1;
+	uint64_t reserved_33_35               : 3;
+	uint64_t mrs_cmd_select               : 1;
+	uint64_t mrs_cmd_override             : 1;
+	uint64_t reserved_38_39               : 2;
+	uint64_t par_addr_mask                : 3;
+	uint64_t reserved_43_43               : 1;
+	uint64_t ea_int_polarity              : 1;
+	uint64_t error_alert_n_sample         : 1;
+	uint64_t reserved_46_47               : 2;
+	uint64_t rcd_parity_check             : 1;
+	uint64_t dimm0_cid                    : 2;
+	uint64_t dimm1_cid                    : 2;
+	uint64_t coalesce_address_mode        : 1;
+	uint64_t dimm_sel_force_invert        : 1;
+	uint64_t dimm_sel_invert_off          : 1;
+	uint64_t mrs_bside_invert_disable     : 1;
+	uint64_t mrs_one_side                 : 1;
+	uint64_t mrs_side                     : 1;
+	uint64_t ref_block                    : 1;
+	uint64_t reserved_60_63               : 4;
+#endif
+	} cn73xx;
 	struct cvmx_lmcx_ext_config_s         cn78xx;
 	struct cvmx_lmcx_ext_config_s         cn78xxp2;
-	struct cvmx_lmcx_ext_config_s         cnf75xx;
+	struct cvmx_lmcx_ext_config_cn73xx    cnf75xx;
 };
 typedef union cvmx_lmcx_ext_config cvmx_lmcx_ext_config_t;
 
@@ -11085,6 +11339,42 @@ union cvmx_lmcx_ppr_ctl {
 	uint64_t u64;
 	struct cvmx_lmcx_ppr_ctl_s {
 #ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_27_63               : 37;
+	uint64_t lrank_sel                    : 3;  /**< Selects which logical rank to perform the Post Package Repair sequence.
+                                                         Package Ranks are selected by the CSR LMC()_MR_MPR_CTL[MR_WR_RANK]. */
+	uint64_t skip_issue_security          : 1;  /**< Personality bit for the PPR sequence. When set, this field forces the sequence to skip
+                                                         issuing four consecutive MR0 commands that suppliy the Security Key. */
+	uint64_t sppr                         : 1;  /**< Personality bit for the PPR sequence. When set, this field forces the sequence to run
+                                                         the Soft PPR mode. */
+	uint64_t tpgm                         : 10; /**< Indicates the programming time (tPGM) constraint used when running PPR sequence.
+                                                         For hard PPR (PPR_CTL[SPPR] = 0), set this field as follows:
+                                                         RNDUP[TPGM(ns) / (1048576 * TCYC(ns))].
+                                                         For soft PPR (PPR_CTL[SPPR] = 1), set this field as follows:
+                                                         RNDUP[TPGM(ns) / TCYC(ns))].
+                                                         TPGM is from the JEDEC DDR4 spec, and TCYC(ns) is the DDR clock frequency (not data
+                                                         rate). */
+	uint64_t tpgm_exit                    : 5;  /**< Indicates PPR Exit time (tPGM_Exit) contrainst used when running PPR sequence.
+                                                         Set this field as follows:
+                                                         _ RNDUP[TPGM_EXIT(ns) / TCYC(ns)]
+                                                         where TPGM_EXIT is from the JEDEC DDR4 spec, and TCYC(ns) is the DDR clock frequency (not
+                                                         data rate). */
+	uint64_t tpgmpst                      : 7;  /**< Indicates New Address Setting time (tPGMPST) constraint used when running PPR sequence.
+                                                         Set this field as follows:
+                                                         _ RNDUP[TPGMPST(ns) / (1024 * TCYC(ns))]
+                                                         where TPGMPST is from the JEDEC DDR4 spec, and TCYC(ns) is the DDR clock frequency (not
+                                                         data rate). */
+#else
+	uint64_t tpgmpst                      : 7;
+	uint64_t tpgm_exit                    : 5;
+	uint64_t tpgm                         : 10;
+	uint64_t sppr                         : 1;
+	uint64_t skip_issue_security          : 1;
+	uint64_t lrank_sel                    : 3;
+	uint64_t reserved_27_63               : 37;
+#endif
+	} s;
+	struct cvmx_lmcx_ppr_ctl_cn73xx {
+#ifdef __BIG_ENDIAN_BITFIELD
 	uint64_t reserved_24_63               : 40;
 	uint64_t skip_issue_security          : 1;  /**< Personality bit for the PPR sequence. When set, this field forces the sequence to skip
                                                          issuing four consecutive MR0 commands that suppliy the Security Key. */
@@ -11115,10 +11405,9 @@ union cvmx_lmcx_ppr_ctl {
 	uint64_t skip_issue_security          : 1;
 	uint64_t reserved_24_63               : 40;
 #endif
-	} s;
-	struct cvmx_lmcx_ppr_ctl_s            cn73xx;
+	} cn73xx;
 	struct cvmx_lmcx_ppr_ctl_s            cn78xxp2;
-	struct cvmx_lmcx_ppr_ctl_s            cnf75xx;
+	struct cvmx_lmcx_ppr_ctl_cn73xx       cnf75xx;
 };
 typedef union cvmx_lmcx_ppr_ctl cvmx_lmcx_ppr_ctl_t;
 
@@ -12134,65 +12423,13 @@ union cvmx_lmcx_seq_ctl {
 	uint64_t reserved_6_63                : 58;
 	uint64_t seq_complete                 : 1;  /**< Sequence complete. This bit is cleared when INIT_START is set to a 1 and then is set to 1
                                                          when the sequence is completed. */
-	uint64_t seq_sel                      : 4;  /**< Selects the sequence that LMC runs after a 0->1 transition on INIT_START.
-                                                         0x0 = Power-up/initialization:
-                                                         LMC()_CONFIG[RANKMASK] selects participating ranks (should be all ranks with attached
-                                                         DRAM). DDR*_DIMM*_CKE* signals are activated (if not already active). RDIMM register
-                                                         control words 0-15 are written to LMC()_CONFIG[RANKMASK]-selected RDIMMs when
-                                                         LMC()_CONTROL[RDIMM_ENA] = 1 and corresponding LMC()_DIMM_CTL[DIMM*_WMASK] bits
-                                                         are set. (Refer to LMC()_DIMM(0..1)_PARAMS and LMC()_DIMM_CTL descriptions for
-                                                         more details.)
-                                                         The DRAM registers MR0-MR6 are written in the selected ranks.
-                                                         0x1 = Read-leveling:
-                                                         LMC()_CONFIG[RANKMASK] selects the rank to be read-leveled. MR3 written in the
-                                                         selected rank.
-                                                         0x2 = Self-refresh entry:
-                                                         LMC()_CONFIG[INIT_STATUS] selects the participating ranks (should be all ranks with
-                                                         attached DRAM). MR1 and MR2 are written in the selected ranks if
-                                                         LMC()_CONFIG[SREF_WITH_DLL] = 1. DDR*_DIMM*_CKE* signals de-activated.
-                                                         0x3 = Self-refresh exit:
-                                                         LMC()_CONFIG[RANKMASK] must be set to indicate participating ranks (should be all
-                                                         ranks with attached DRAM). DDR*_DIMM*_CKE* signals activated. MR0, MR1, MR2, and MR3 are
-                                                         written in the participating ranks if LMC()_CONFIG[SREF_WITH_DLL] = 1.
-                                                         LMC()_CONFIG[INIT_STATUS] is updated for ranks that are selected.
-                                                         0x6 = Write-leveling:
-                                                         LMC()_CONFIG[RANKMASK] selects the rank to be write-leveled.
-                                                         LMC()_CONFIG[INIT_STATUS] must indicate all ranks with attached DRAM. MR1 and MR2
-                                                         written in the LMC()_CONFIG[INIT_STATUS]-selected ranks.
-                                                         0x7 = Initialize RCW:
-                                                         LMC()_CONFIG[RANKMASK] selects participating ranks (should be all ranks with attached
-                                                         DRAM). In DDR3 mode, RDIMM register control words 0-15 are written to
-                                                         LMC()_CONFIG[RANKMASK]-selected RDIMMs when LMC()_CONTROL[RDIMM_ENA] = 1 and
-                                                         corresponding LMC()_DIMM_CTL[DIMM*_WMASK] bits are set. (Refer to
-                                                         LMC()_DIMM(0..1)_PARAMS and LMC()_DIMM_CTL descriptions for more details.)
-                                                         0x8 = MRW
-                                                         Mode Register Write sequence.
-                                                         0x9 = MPR
-                                                         MPR register read or write sequence.
-                                                         0xa = VREFINT
-                                                         Vref internal training sequence, also used as deskew training sequence when
-                                                         LMC(0..0)_EXT_CONFIG[VREFINT_SEQ_DESKEW] is set.
-                                                         0xb = Offset Training
-                                                         Offset training sequence.
-                                                         0xe = General R/W Training.
-                                                         0xf = DDR4 Post Package Repair sequence. See LMC()_PPR_CTL for more detail.
-                                                         Self-refresh entry SEQ_SEL's may also be automatically
-                                                         generated by hardware upon a chip warm or soft reset
-                                                         sequence when LMC*_RESET_CTL[DDR3PWARM,DDR3PSOFT] are set.
-                                                         LMC writes the LMC*_MODEREG_PARAMS0 and LMC*_MODEREG_PARAMS1 CSR field values
-                                                         to the Mode registers in the DRAM parts (i.e. MR0, MR1, MR2, and MR3) as part of some of
+	uint64_t seq_sel                      : 4;  /**< Selects the sequence that LMC runs after a 0->1 transition on INIT_START, as
+                                                         enumerated by LMC_SEQ_SEL_E.
+                                                         LMC writes the LMC()_MODEREG_PARAMS0 and LMC()_MODEREG_PARAMS1 CSR field values
+                                                         to the Mode registers in the DRAM parts (i.e. MR0-MR6) as part of some of
                                                          these sequences.
-                                                         Refer to the LMC*_MODEREG_PARAMS0 and LMC*_MODEREG_PARAMS1 descriptions for more details.
-                                                         If there are two consecutive power-up/init's without
-                                                         a DRESET assertion between them, LMC asserts DDR_CKE* as part of
-                                                         the first power-up/init, and continues to assert DDR_CKE*
-                                                         through the remainder of the first and the second power-up/init.
-                                                         If DDR_CKE* deactivation and reactivation is needed for
-                                                         a second power-up/init, a DRESET assertion is required
-                                                         between the first and the second."
-                                                         INTERNAL:
-                                                         0xe = Data Buffer Training. Configurable to run different modes of Data Buffer
-                                                         training on DDR4 LRDIMM. See LMC()_DBTRAIN_CTL for more detail. */
+                                                         Refer to the LMC()_MODEREG_PARAMS0 and LMC()_MODEREG_PARAMS1 descriptions for more
+                                                         details. */
 	uint64_t init_start                   : 1;  /**< A 0->1 transition starts the DDR memory sequence that is selected by
                                                          LMC()_SEQ_CTL[SEQ_SEL].
                                                          This register is a one-shot and clears itself each time it is set. */
diff --git a/arch/mips/include/asm/octeon/cvmx-mio-defs.h b/arch/mips/include/asm/octeon/cvmx-mio-defs.h
index d6e2a78..7aa363f 100644
--- a/arch/mips/include/asm/octeon/cvmx-mio-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-mio-defs.h
@@ -5775,7 +5775,7 @@ union cvmx_mio_fus_dat3 {
 	uint64_t u64;
 	struct cvmx_mio_fus_dat3_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t ema0                         : 6;  /**< Fuse information - EMA0. INTERNAL: dflt value is 0x02. Soft or hard blow of these fuses
+	uint64_t ema0                         : 6;  /**< Fuse information - EMA0. INTERNAL: dflt value is 0x11. Soft or hard blow of these fuses
                                                          will XOR with this value. */
 	uint64_t pll_ctl                      : 10; /**< Fuse information - PLL control. */
 	uint64_t dfa_info_dte                 : 3;  /**< Reserved. INTERNAL: Fuse information - HFA information (HTE). */
@@ -6086,7 +6086,7 @@ union cvmx_mio_fus_dat3 {
 	} cn70xxp1;
 	struct cvmx_mio_fus_dat3_cn73xx {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t ema0                         : 6;  /**< Fuse information - EMA0. INTERNAL: dflt value is 0x02. Soft or hard blow of these fuses
+	uint64_t ema0                         : 6;  /**< Fuse information - EMA0. INTERNAL: dflt value is 0x11. Soft or hard blow of these fuses
                                                          will XOR with this value. */
 	uint64_t pll_ctl                      : 10; /**< Fuse information - PLL control. */
 	uint64_t dfa_info_dte                 : 3;  /**< Fuse information - HFA information (HTE). */
@@ -6113,14 +6113,18 @@ union cvmx_mio_fus_dat3 {
 	uint64_t efus_ign                     : 1;  /**< Fuse information - efuse ignore. */
 	uint64_t nozip                        : 1;  /**< Fuse information - ZIP disable. */
 	uint64_t nodfa_dte                    : 1;  /**< Fuse information - HFA disable (HTE). */
-	uint64_t ema1                         : 6;  /**< Fuse information - EMA1. INTERNAL: Default value is 0x11. Soft or hard blow of these fuses
+	uint64_t ema1                         : 6;  /**< Fuse information - EMA1. INTERNAL: Default value is 0x02. Soft or hard blow of these fuses
                                                          will XOR with this value. */
 	uint64_t nohna_dte                    : 1;  /**< Fuse information - HNA disable (DTE). */
 	uint64_t hna_info_dte                 : 3;  /**< Fuse information - HNA information (DTE). */
 	uint64_t hna_info_clm                 : 4;  /**< Fuse information - HNA information (cluster mask). */
 	uint64_t reserved_9_9                 : 1;
-	uint64_t core_pll_mul                 : 5;  /**< Core-clock PLL multiplier. */
-	uint64_t pnr_pll_mul                  : 4;  /**< Coprocessor-clock PLL multiplier. */
+	uint64_t core_pll_mul                 : 5;  /**< Core-clock PLL multiplier hardware limit. Indicates maximum
+                                                         value for PLL_MUL[5:1] straps.  Any strap setting above this
+                                                         value will be ignored.  A value of 0 indicates no hardware limit. */
+	uint64_t pnr_pll_mul                  : 4;  /**< Coprocessor-clock PLL multiplier hardware limit.  Indicates maximum
+                                                         value for PNR_MUL[5:1] straps.  Any strap setting above this
+                                                         value will be ignored.  A value of 0 indicates no hardware limit. */
 #else
 	uint64_t pnr_pll_mul                  : 4;
 	uint64_t core_pll_mul                 : 5;
@@ -6150,7 +6154,7 @@ union cvmx_mio_fus_dat3 {
 	} cn73xx;
 	struct cvmx_mio_fus_dat3_cn78xx {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t ema0                         : 6;  /**< Fuse information - EMA0. INTERNAL: dflt value is 0x02. Soft or hard blow of these fuses
+	uint64_t ema0                         : 6;  /**< Fuse information - EMA0. INTERNAL: dflt value is 0x11. Soft or hard blow of these fuses
                                                          will XOR with this value. */
 	uint64_t pll_ctl                      : 10; /**< Fuse information - PLL control. */
 	uint64_t dfa_info_dte                 : 3;  /**< Fuse information - HFA information (HTE). */
@@ -6172,7 +6176,7 @@ union cvmx_mio_fus_dat3 {
 	uint64_t efus_ign                     : 1;  /**< Fuse information - efuse ignore. */
 	uint64_t nozip                        : 1;  /**< Fuse information - ZIP disable. */
 	uint64_t nodfa_dte                    : 1;  /**< Fuse information - HFA disable (HTE). */
-	uint64_t ema1                         : 6;  /**< Fuse information - EMA1. INTERNAL: Default value is 0x11. Soft or hard blow of these fuses
+	uint64_t ema1                         : 6;  /**< Fuse information - EMA1. INTERNAL: Default value is 0x02. Soft or hard blow of these fuses
                                                          will XOR with this value. */
 	uint64_t nohna_dte                    : 1;  /**< Fuse information - HNA disable (DTE). */
 	uint64_t hna_info_dte                 : 3;  /**< Fuse information - HNA information (DTE). */
@@ -6206,7 +6210,7 @@ union cvmx_mio_fus_dat3 {
 	struct cvmx_mio_fus_dat3_cn61xx       cnf71xx;
 	struct cvmx_mio_fus_dat3_cnf75xx {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint64_t ema0                         : 6;  /**< Fuse information - EMA0. INTERNAL: dflt value is 0x02. Soft or hard blow of these fuses
+	uint64_t ema0                         : 6;  /**< Fuse information - EMA0. INTERNAL: dflt value is 0x11. Soft or hard blow of these fuses
                                                          will XOR with this value. */
 	uint64_t pll_ctl                      : 10; /**< Fuse information - PLL control. */
 	uint64_t dfa_info_dte                 : 3;  /**< Reserved. INTERNAL: Fuse information - HFA information (HTE). */
@@ -6233,11 +6237,15 @@ union cvmx_mio_fus_dat3 {
 	uint64_t efus_ign                     : 1;  /**< Fuse information - efuse ignore. */
 	uint64_t nozip                        : 1;  /**< Reserved. INTERNAL: Fuse information - ZIP disable. */
 	uint64_t nodfa_dte                    : 1;  /**< Reserved. INTERNAL: Fuse information - HFA disable (HTE). */
-	uint64_t ema1                         : 6;  /**< Fuse information - EMA1. INTERNAL: Default value is 0x11. Soft or hard blow of these fuses
+	uint64_t ema1                         : 6;  /**< Fuse information - EMA1. INTERNAL: Default value is 0x02. Soft or hard blow of these fuses
                                                          will XOR with this value. */
 	uint64_t reserved_9_17                : 9;
-	uint64_t core_pll_mul                 : 5;  /**< Core-clock PLL multiplier. */
-	uint64_t pnr_pll_mul                  : 4;  /**< Coprocessor-clock PLL multiplier. */
+	uint64_t core_pll_mul                 : 5;  /**< Core-clock PLL multiplier hardware limit. Indicates maximum
+                                                         value for PLL_MUL[5:1] straps.  Any strap setting above this
+                                                         value will be ignored.  A value of 0 indicates no hardware limit. */
+	uint64_t pnr_pll_mul                  : 4;  /**< Coprocessor-clock PLL multiplier hardware limit.  Indicates maximum
+                                                         value for PNR_MUL[5:1] straps.  Any strap setting above this
+                                                         value will be ignored.  A value of 0 indicates no hardware limit. */
 #else
 	uint64_t pnr_pll_mul                  : 4;
 	uint64_t core_pll_mul                 : 5;
diff --git a/arch/mips/include/asm/octeon/cvmx-pip.h b/arch/mips/include/asm/octeon/cvmx-pip.h
index ad33e11..bbf6a61 100644
--- a/arch/mips/include/asm/octeon/cvmx-pip.h
+++ b/arch/mips/include/asm/octeon/cvmx-pip.h
@@ -42,7 +42,7 @@
  *
  * Interface to the hardware Packet Input Processing unit.
  *
- * <hr>$Revision: 115965 $<hr>
+ * <hr>$Revision: 124170 $<hr>
  */
 
 #ifndef __CVMX_PIP_H__
@@ -442,6 +442,7 @@ static inline int __cvmx_pip_enable_watcher_78xx(int node, int index, int style)
 	/* All other style parameters remain same except grp and qos and qps base */
 	cvmx_pki_read_style_config(node, style, CVMX_PKI_CLUSTER_ALL, &style_cfg);
 	cvmx_pki_read_qpg_entry(node, style_cfg.parm_cfg.qpg_base, &qpg_cfg);
+	qpg_cfg.qpg_base = CVMX_PKI_FIND_AVAL_ENTRY;
 	qpg_cfg.grp_ok = qos_watcher[index].sso_grp;
 	qpg_cfg.grp_bad = qos_watcher[index].sso_grp;
 	qpg_offset = cvmx_helper_pki_set_qpg_entry(node, &qpg_cfg);
@@ -543,6 +544,7 @@ static inline void cvmx_pip_config_port(uint64_t ipd_port, cvmx_pip_prt_cfgx_t p
 		if (port_cfg.s.qos) {
 			cvmx_pki_read_qpg_entry(xp.node,
 				pki_prt_cfg.style_cfg.parm_cfg.qpg_base, &qpg_cfg);
+			qpg_cfg.qpg_base = CVMX_PKI_FIND_AVAL_ENTRY;
 			qpg_cfg.grp_ok |= port_cfg.s.qos;
 			qpg_cfg.grp_bad |= port_cfg.s.qos;
 			qpg_offset = cvmx_helper_pki_set_qpg_entry(xp.node, &qpg_cfg);
@@ -554,6 +556,7 @@ static inline void cvmx_pip_config_port(uint64_t ipd_port, cvmx_pip_prt_cfgx_t p
 		if (port_tag_cfg.s.grp != pki_dflt_sso_grp[xp.node].group) {
 			cvmx_pki_read_qpg_entry(xp.node,
 					pki_prt_cfg.style_cfg.parm_cfg.qpg_base, &qpg_cfg);
+			qpg_cfg.qpg_base = CVMX_PKI_FIND_AVAL_ENTRY;
 			qpg_cfg.grp_ok |= (uint8_t)(port_tag_cfg.s.grp << 3);
 			qpg_cfg.grp_bad |= (uint8_t)(port_tag_cfg.s.grp << 3);
 			qpg_offset = cvmx_helper_pki_set_qpg_entry(xp.node, &qpg_cfg);
diff --git a/arch/mips/include/asm/octeon/cvmx-pki.h b/arch/mips/include/asm/octeon/cvmx-pki.h
index 3942da5..fd9bed96 100644
--- a/arch/mips/include/asm/octeon/cvmx-pki.h
+++ b/arch/mips/include/asm/octeon/cvmx-pki.h
@@ -545,6 +545,13 @@ enum cvmx_pki_term {
 	CVMX_PKI_PCAM_TERM_LG_CUSTOM               = CVMX_PKI_PCAM_TERM_E_LG_CUSTOM_M
 };
 
+#define CVMX_PKI_DMACH_SHIFT	(32)
+#define CVMX_PKI_DMACH_MASK	cvmx_build_mask(16)
+#define CVMX_PKI_DMACL_MASK	CVMX_PKI_DATA_MASK_32
+#define CVMX_PKI_DATA_MASK_32	cvmx_build_mask(32)
+#define CVMX_PKI_DATA_MASK_16	cvmx_build_mask(16)
+#define CVMX_PKI_DMAC_MATCH_EXACT   cvmx_build_mask(48)
+
 struct cvmx_pki_pcam_input {
 	uint64_t		style;
 	uint64_t		style_mask; /* bits: 1-match, 0-dont care */
@@ -743,139 +750,15 @@ static inline void cvmx_pki_enable_backpressure(int node)
 	cvmx_write_csr_node(node, CVMX_PKI_BUF_CTL, pki_buf_ctl.u64);
 }
 
-#define READCORRECT(cnt, node, value, addr)	\
-	{cnt = 0;	\
-	while (value >= (1ull << 48) && cnt++ < 20) \
-		value = cvmx_read_csr_node(node, addr); \
-	if (cnt >= 20)  \
-		cvmx_dprintf("count stuck for 0x%llx\n", (long long unsigned int)addr); }
-
-
 /**
- * Get the statistics counters for a port.
+ * Clear the statistics counters for a port.
  *
  * @param node	   node number
  * @param port Port number (ipd_port) to get statistics for.
  *		   Make sure PKI_STATS_CTL:mode is set to 0 for
  *		   collecting per port/pkind stats.
- *
  */
-static inline void cvmx_pki_clear_port_stats(int node, uint64_t port)
-{
-	int interface = cvmx_helper_get_interface_num(port);
-	int index = cvmx_helper_get_interface_index_num(port);
-	int pknd = cvmx_helper_get_pknd(interface, index);
-
-	cvmx_pki_statx_stat0_t stat0;
-	cvmx_pki_statx_stat1_t stat1;
-	cvmx_pki_statx_stat2_t stat2;
-	cvmx_pki_statx_stat3_t stat3;
-	cvmx_pki_statx_stat4_t stat4;
-	cvmx_pki_statx_stat5_t stat5;
-	cvmx_pki_statx_stat6_t stat6;
-	cvmx_pki_statx_stat7_t stat7;
-	cvmx_pki_statx_stat8_t stat8;
-	cvmx_pki_statx_stat9_t stat9;
-	cvmx_pki_statx_stat10_t stat10;
-	cvmx_pki_statx_stat11_t stat11;
-	cvmx_pki_statx_stat14_t stat14;
-	cvmx_pki_statx_stat15_t stat15;
-	cvmx_pki_statx_stat16_t stat16;
-	cvmx_pki_statx_stat17_t stat17;
-	cvmx_pki_statx_hist0_t hist0;
-	cvmx_pki_statx_hist1_t hist1;
-	cvmx_pki_statx_hist2_t hist2;
-	cvmx_pki_statx_hist3_t hist3;
-	cvmx_pki_statx_hist4_t hist4;
-	cvmx_pki_statx_hist5_t hist5;
-	cvmx_pki_statx_hist6_t hist6;
-	cvmx_pki_pkndx_inb_stat0_t pki_pknd_inb_stat0;
-	cvmx_pki_pkndx_inb_stat1_t pki_pknd_inb_stat1;
-	cvmx_pki_pkndx_inb_stat2_t pki_pknd_inb_stat2;
-
-	stat0.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT0(pknd));
-	stat1.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT1(pknd));
-	stat2.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT2(pknd));
-	stat3.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT3(pknd));
-	stat4.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT4(pknd));
-	stat5.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT5(pknd));
-	stat6.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT6(pknd));
-	stat7.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT7(pknd));
-	stat8.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT8(pknd));
-	stat9.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT9(pknd));
-	stat10.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT10(pknd));
-	stat11.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT11(pknd));
-	stat14.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT14(pknd));
-	stat15.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT15(pknd));
-	stat16.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT16(pknd));
-	stat17.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT17(pknd));
-	hist0.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST0(pknd));
-	hist1.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST1(pknd));
-	hist2.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST2(pknd));
-	hist3.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST3(pknd));
-	hist4.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST4(pknd));
-	hist5.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST5(pknd));
-	hist6.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST6(pknd));
-	pki_pknd_inb_stat0.u64 = cvmx_read_csr_node(node, CVMX_PKI_PKNDX_INB_STAT0(pknd));
-	pki_pknd_inb_stat1.u64 = cvmx_read_csr_node(node, CVMX_PKI_PKNDX_INB_STAT1(pknd));
-	pki_pknd_inb_stat2.u64 = cvmx_read_csr_node(node, CVMX_PKI_PKNDX_INB_STAT2(pknd));
-
-	stat4.s.drp_octs = 0;
-	stat3.s.drp_pkts = 0;
-	stat1.s.octs = 0;
-	stat2.s.raw = 0;
-	stat0.s.pkts = 0;
-	stat6.s.mcast = 0;
-	stat5.s.bcast = 0;
-	hist0.s.h1to63 = 0;
-	hist1.s.h64to127 = 0;
-	hist2.s.h128to255 = 0;
-	hist3.s.h256to511 = 0;
-	hist4.s.h512to1023 = 0;
-	hist5.s.h1024to1518 = 0;
-	hist6.s.h1519 = 0;
-	stat7.s.fcs = 0;
-	stat9.s.undersz = 0;
-	stat8.s.frag = 0;
-	stat11.s.oversz = 0;
-	stat10.s.jabber = 0;
-	stat15.s.drp_mcast = 0;
-	stat14.s.drp_bcast = 0;
-	stat17.s.drp_mcast = 0;
-	stat16.s.drp_bcast = 0;
-	pki_pknd_inb_stat0.s.pkts = 0;
-	pki_pknd_inb_stat1.s.octs = 0;
-	pki_pknd_inb_stat2.s.errs = 0;
-
-	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT0(pknd), stat0.u64);
-	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT1(pknd), stat1.u64);
-	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT2(pknd), stat2.u64);
-	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT3(pknd), stat3.u64);
-	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT4(pknd), stat4.u64);
-	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT5(pknd), stat5.u64);
-	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT6(pknd), stat6.u64);
-	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT7(pknd), stat7.u64);
-	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT8(pknd), stat8.u64);
-	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT9(pknd), stat9.u64);
-	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT10(pknd), stat10.u64);
-	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT11(pknd), stat11.u64);
-	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT14(pknd), stat14.u64);
-	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT15(pknd), stat15.u64);
-	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT16(pknd), stat16.u64);
-	cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT17(pknd), stat17.u64);
-	cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST0(pknd), hist0.u64);
-	cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST1(pknd), hist1.u64);
-	cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST2(pknd), hist2.u64);
-	cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST3(pknd), hist3.u64);
-	cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST4(pknd), hist4.u64);
-	cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST5(pknd), hist5.u64);
-	cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST6(pknd), hist6.u64);
-	cvmx_write_csr_node(node, CVMX_PKI_PKNDX_INB_STAT0(pknd), pki_pknd_inb_stat0.u64);
-	cvmx_write_csr_node(node, CVMX_PKI_PKNDX_INB_STAT1(pknd), pki_pknd_inb_stat1.u64);
-	cvmx_write_csr_node(node, CVMX_PKI_PKNDX_INB_STAT2(pknd), pki_pknd_inb_stat2.u64);
-
-}
-
+void cvmx_pki_clear_port_stats(int node, uint64_t port);
 
 /**
  * Get the status counters for index from PKI.
@@ -885,149 +768,7 @@ static inline void cvmx_pki_clear_port_stats(int node, uint64_t port)
  *		   style(flow) number (if PKI_STATS_CTL:mode=1)
  * @param status   Where to put the results.
  */
-static inline void cvmx_pki_get_stats(int node, int index, struct cvmx_pki_port_stats *status)
-{
-	cvmx_pki_statx_stat0_t stat0;
-	cvmx_pki_statx_stat1_t stat1;
-	cvmx_pki_statx_stat2_t stat2;
-	cvmx_pki_statx_stat3_t stat3;
-	cvmx_pki_statx_stat4_t stat4;
-	cvmx_pki_statx_stat5_t stat5;
-	cvmx_pki_statx_stat6_t stat6;
-	cvmx_pki_statx_stat7_t stat7;
-	cvmx_pki_statx_stat8_t stat8;
-	cvmx_pki_statx_stat9_t stat9;
-	cvmx_pki_statx_stat10_t stat10;
-	cvmx_pki_statx_stat11_t stat11;
-	cvmx_pki_statx_stat14_t stat14;
-	cvmx_pki_statx_stat15_t stat15;
-	cvmx_pki_statx_stat16_t stat16;
-	cvmx_pki_statx_stat17_t stat17;
-	cvmx_pki_statx_hist0_t hist0;
-	cvmx_pki_statx_hist1_t hist1;
-	cvmx_pki_statx_hist2_t hist2;
-	cvmx_pki_statx_hist3_t hist3;
-	cvmx_pki_statx_hist4_t hist4;
-	cvmx_pki_statx_hist5_t hist5;
-	cvmx_pki_statx_hist6_t hist6;
-	cvmx_pki_pkndx_inb_stat0_t pki_pknd_inb_stat0;
-	cvmx_pki_pkndx_inb_stat1_t pki_pknd_inb_stat1;
-	cvmx_pki_pkndx_inb_stat2_t pki_pknd_inb_stat2;
-	int cnt;
-
-#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
-	/* Accessing PKI stat registers can timeout based on the Errata
-	   PKI-20775, disable SLI_INT_SUM[RML_TO] before reading the stats
-	   enable back after clearing the interrupt. */
-	cvmx_error_intsn_disable_v3(0x1f000);
-#endif
-
-	stat0.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT0(index));
-	READCORRECT(cnt, node, stat0.u64, CVMX_PKI_STATX_STAT0(index));
-
-	stat1.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT1(index));
-	READCORRECT(cnt, node, stat1.u64, CVMX_PKI_STATX_STAT1(index));
-
-	stat2.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT2(index));
-	READCORRECT(cnt, node, stat2.u64, CVMX_PKI_STATX_STAT2(index));
-
-	stat3.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT3(index));
-	READCORRECT(cnt, node, stat3.u64, CVMX_PKI_STATX_STAT3(index));
-
-	stat4.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT4(index));
-	READCORRECT(cnt, node, stat4.u64, CVMX_PKI_STATX_STAT4(index));
-
-	stat5.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT5(index));
-	READCORRECT(cnt, node, stat5.u64, CVMX_PKI_STATX_STAT5(index));
-
-	stat6.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT6(index));
-	READCORRECT(cnt, node, stat6.u64, CVMX_PKI_STATX_STAT6(index));
-
-	stat7.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT7(index));
-	READCORRECT(cnt, node, stat7.u64, CVMX_PKI_STATX_STAT7(index));
-
-	stat8.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT8(index));
-	READCORRECT(cnt, node, stat8.u64, CVMX_PKI_STATX_STAT8(index));
-
-	stat9.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT9(index));
-	READCORRECT(cnt, node, stat9.u64, CVMX_PKI_STATX_STAT9(index));
-
-	stat10.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT10(index));
-	READCORRECT(cnt, node, stat10.u64, CVMX_PKI_STATX_STAT10(index));
-
-	stat11.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT11(index));
-	READCORRECT(cnt, node, stat11.u64, CVMX_PKI_STATX_STAT11(index));
-
-	stat14.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT14(index));
-	READCORRECT(cnt, node, stat14.u64, CVMX_PKI_STATX_STAT14(index));
-
-	stat15.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT15(index));
-	READCORRECT(cnt, node, stat15.u64, CVMX_PKI_STATX_STAT15(index));
-
-	stat16.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT16(index));
-	READCORRECT(cnt, node, stat16.u64, CVMX_PKI_STATX_STAT16(index));
-
-	stat17.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT17(index));
-	READCORRECT(cnt, node, stat17.u64, CVMX_PKI_STATX_STAT17(index));
-
-	hist0.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST0(index));
-	READCORRECT(cnt, node, hist0.u64, CVMX_PKI_STATX_HIST0(index));
-
-	hist1.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST1(index));
-	READCORRECT(cnt, node, hist1.u64, CVMX_PKI_STATX_HIST1(index));
-
-	hist2.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST2(index));
-	READCORRECT(cnt, node, hist2.u64, CVMX_PKI_STATX_HIST2(index));
-
-	hist3.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST3(index));
-	READCORRECT(cnt, node, hist3.u64, CVMX_PKI_STATX_HIST3(index));
-
-	hist4.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST4(index));
-	READCORRECT(cnt, node, hist4.u64, CVMX_PKI_STATX_HIST4(index));
-
-	hist5.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST5(index));
-	READCORRECT(cnt, node, hist5.u64, CVMX_PKI_STATX_HIST5(index));
-
-	hist6.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST6(index));
-	READCORRECT(cnt, node, hist6.u64, CVMX_PKI_STATX_HIST6(index));
-
-	pki_pknd_inb_stat0.u64 = cvmx_read_csr_node(node, CVMX_PKI_PKNDX_INB_STAT0(index));
-	pki_pknd_inb_stat1.u64 = cvmx_read_csr_node(node, CVMX_PKI_PKNDX_INB_STAT1(index));
-	pki_pknd_inb_stat2.u64 = cvmx_read_csr_node(node, CVMX_PKI_PKNDX_INB_STAT2(index));
-
-	status->dropped_octets = stat4.s.drp_octs;
-	status->dropped_packets = stat3.s.drp_pkts;
-	status->octets = stat1.s.octs;
-	status->pci_raw_packets = stat2.s.raw;
-	status->packets = stat0.s.pkts;
-	status->multicast_packets = stat6.s.mcast;
-	status->broadcast_packets = stat5.s.bcast;
-	status->len_64_packets = hist0.s.h1to63;
-	status->len_65_127_packets = hist1.s.h64to127;
-	status->len_128_255_packets = hist2.s.h128to255;
-	status->len_256_511_packets = hist3.s.h256to511;
-	status->len_512_1023_packets = hist4.s.h512to1023;
-	status->len_1024_1518_packets = hist5.s.h1024to1518;
-	status->len_1519_max_packets = hist6.s.h1519;
-	status->fcs_align_err_packets = stat7.s.fcs;
-	status->runt_packets = stat9.s.undersz;
-	status->runt_crc_packets = stat8.s.frag;
-	status->oversize_packets = stat11.s.oversz;
-	status->oversize_crc_packets = stat10.s.jabber;
-	status->mcast_l2_red_packets = stat15.s.drp_mcast;
-	status->bcast_l2_red_packets = stat14.s.drp_bcast;
-	status->mcast_l3_red_packets = stat17.s.drp_mcast;
-	status->bcast_l3_red_packets = stat16.s.drp_bcast;
-	status->inb_packets = pki_pknd_inb_stat0.s.pkts;
-	status->inb_octets = pki_pknd_inb_stat1.s.octs;
-	status->inb_errors = pki_pknd_inb_stat2.s.errs;
-
-#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
-	/* Enable SLI_INT_SUM[RML_TO] interrupt after clear the pending interrupt. */
-	cvmx_write_csr_node(node, CVMX_CIU3_ISCX_W1C(0x1f000), 1);
-	cvmx_error_intsn_enable_v3(0x1f000);
-#endif
-}
+void cvmx_pki_get_stats(int node, int index, struct cvmx_pki_port_stats *status);
 
 /**
  * Get the statistics counters for a port.
@@ -1075,7 +816,6 @@ int cvmx_pki_config_dump(unsigned node);
  */
 int cvmx_pki_stats_dump(unsigned node);
 
-
 /**
  * This function enables pki
  * @param node	 node to enable pki in.
diff --git a/arch/mips/include/asm/octeon/cvmx-pow.h b/arch/mips/include/asm/octeon/cvmx-pow.h
index fe018c4..ac1a9a3 100644
--- a/arch/mips/include/asm/octeon/cvmx-pow.h
+++ b/arch/mips/include/asm/octeon/cvmx-pow.h
@@ -1650,6 +1650,34 @@ static inline unsigned cvmx_sso_num_xgrp(void)
 	return 0;
 }
 
+/**
+ * @INTERNAL
+ * Return the number of POW groups on current model.
+ * In case of CN78XX/CN73XX this is the number of equivalent
+ * "legacy groups" on the chip when it is used in backward
+ * compatible mode.
+ */
+static inline unsigned cvmx_pow_num_groups(void)
+{
+	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE))
+		return cvmx_sso_num_xgrp() >> 3;
+	else if (octeon_has_feature(OCTEON_FEATURE_CN68XX_WQE))
+		return 64;
+	else
+		return 16;
+}
+
+/**
+ * @INTERNAL
+ * Return the number of mask-set registers.
+ */
+static inline unsigned cvmx_sso_num_maskset(void)
+{
+	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE))
+		return 2;
+	else
+		return 1;
+}
 
 /**
  * Get the POW tag for this core. This returns the current
@@ -2668,8 +2696,27 @@ static inline void cvmx_pow_work_submit_node(cvmx_wqe_t * wqp, uint32_t tag, cvm
  */
 static inline void cvmx_pow_set_group_mask(uint64_t core_num, uint64_t mask)
 {
+	uint64_t valid_mask;
+	int num_groups = cvmx_pow_num_groups();
+
+	if (num_groups >= 64)
+		valid_mask = ~0ull;
+	else
+		valid_mask = (1ull << num_groups)-1;
+
+	if ((mask & valid_mask) == 0) {
+	    cvmx_printf("ERROR: "
+		"%s empty group mask disables work on core# %llu, ignored.\n",
+		__func__, (unsigned long long) core_num);
+	    return;
+	}
+
+	cvmx_warn_if(mask & (~valid_mask),
+		"%s group number range exceeded: %#llx\n",
+		__func__, (unsigned long long) mask);
+
 	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
-		const unsigned mask_set = 0;
+		unsigned mask_set;
 		cvmx_sso_ppx_sx_grpmskx_t grp_msk;
 		unsigned core, node;
 		unsigned rix;	/* Register index */
@@ -2677,10 +2724,6 @@ static inline void cvmx_pow_set_group_mask(uint64_t core_num, uint64_t mask)
 		unsigned bit;	/* bit index */
 		unsigned xgrp;	/* native group # */
 
-		cvmx_warn_if(mask & (~0xffffffffull),
-			"%s group number range exceeded: %#llx\n",
-			__FUNCTION__, (unsigned long long) mask);
-
 		node = cvmx_coremask_core_to_node(core_num);
 		core = cvmx_coremask_core_on_node(core_num);
 
@@ -2698,8 +2741,10 @@ static inline void cvmx_pow_set_group_mask(uint64_t core_num, uint64_t mask)
 					grp_msk.s.grp_msk |= 1ull << bit;
 				/* Pre-set to all 0's */
 			}
-
-			cvmx_write_csr_node(node,
+			for (mask_set = 0;
+			    mask_set < cvmx_sso_num_maskset();
+			    mask_set ++)
+			    cvmx_write_csr_node(node,
 				CVMX_SSO_PPX_SX_GRPMSKX(core, mask_set, rix),
 				grp_msk.u64);
 		}
@@ -2710,10 +2755,6 @@ static inline void cvmx_pow_set_group_mask(uint64_t core_num, uint64_t mask)
 	} else {
 		cvmx_pow_pp_grp_mskx_t grp_msk;
 
-		cvmx_warn_if(mask & ~0xffffull,
-			"%s group number range exceeded: %#llx\n",
-			__FUNCTION__, (unsigned long long) mask);
-
 		grp_msk.u64 = cvmx_read_csr(CVMX_POW_PP_GRP_MSKX(core_num));
 		grp_msk.s.grp_msk = mask & 0xffff;
 		cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(core_num), grp_msk.u64);
diff --git a/arch/mips/include/asm/octeon/cvmx-sli-defs.h b/arch/mips/include/asm/octeon/cvmx-sli-defs.h
index a553cf4..07b905a 100644
--- a/arch/mips/include/asm/octeon/cvmx-sli-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-sli-defs.h
@@ -3883,8 +3883,8 @@ union cvmx_sli_ciu_int_sum {
                                                          this bit is set.
                                                          Throws SLI_INTSN_E::SLI_INT_M2P0_PPPF_ERR. */
 	uint64_t m2p0_ppvf_err                : 1;  /**< On Mac2 PF0, when an error response is received for a VF PP transaction read, this bit is
-                                                         set.
-                                                         A subsequent read to SLI_MAC()_PF()_PP_VF_INT is required to discover which VF.
+                                                         set. This bit should be cleared and followed by a read
+                                                         to SLI_MAC()_PF()_PP_VF_INT to discover which VF.
                                                          Throws SLI_INTSN_E::SLI_INT_M2P0_PPVF_ERR. */
 	uint64_t m2p0_pktpf_err               : 1;  /**< On Mac2 PF0, When an error response is received for a PF packet transaction read or a
                                                          doorbell
@@ -3893,16 +3893,15 @@ union cvmx_sli_ciu_int_sum {
                                                          Throws SLI_INTSN_E::SLI_INT_M2P0_PKTPF_ERR. */
 	uint64_t m2p0_pktvf_err               : 1;  /**< On Mac2 PF0, when an error response is received for a VF PP transaction read, a doorbell
                                                          overflow for a ring associated with a VF occurs or an illegal memory access from a VF
-                                                         occurs,
-                                                         this bit is set.
-                                                         A subsequent read to SLI_MAC()_PF()_PKT_VF_INT is required to discover which VF.
+                                                         occurs, this bit is set. This bit should be cleared and followed by a read
+                                                         to SLI_MAC()_PF()_PKT_VF_INT to discover which VF.
                                                          Throws SLI_INTSN_E::SLI_INT_M2P0_PKTVF_ERR. */
 	uint64_t m2p0_dmapf_err               : 1;  /**< On Mac2 PF0, when an error response is received for a PF DMA transcation read, this bit is
                                                          set.
                                                          Throws SLI_INTSN_E::SLI_INT_M2P0_DMAPF_ERR. */
 	uint64_t m2p0_dmavf_err               : 1;  /**< When an error response is received for a VF DMA transaction read on MAC2 PF0, this bit is
-                                                         set.
-                                                         A subsequent read to SLI_MAC()_PF()_DMA_VF_INT is required to discover which VF.
+                                                         set. This bit should be cleared and followed by a read to SLI_MAC()_PF()_DMA_VF_INT to
+                                                         discover which VF.
                                                          Throws SLI_INTSN_E::SLI_INT_M2P0_DMAVF_ERR. */
 	uint64_t m1p0_pppf_err                : 1;  /**< On Mac1 PF0, when an error response is received for a PF PP transaction read,
                                                          this bit is set.
@@ -3919,8 +3918,8 @@ union cvmx_sli_ciu_int_sum {
                                                          this bit is set.
                                                          Throws SLI_INTSN_E::SLI_INT_M0P1_PPPF_ERR. */
 	uint64_t m0p1_ppvf_err                : 1;  /**< On Mac0 PF1, when an error response is received for a VF PP transaction read, this bit is
-                                                         set.
-                                                         A subsequent read to SLI_MAC()_PF()_PP_VF_INT is required to discover which VF.
+                                                         set. This bit should be cleared and followed by a read to SLI_MAC()_PF()_PP_VF_INT to
+                                                         discover which VF.
                                                          Throws SLI_INTSN_E::SLI_INT_M0P1_PPVF_ERR. */
 	uint64_t m0p1_pktpf_err               : 1;  /**< On Mac0 PF1, When an error response is received for a PF packet transaction read or a
                                                          doorbell
@@ -3929,23 +3928,22 @@ union cvmx_sli_ciu_int_sum {
                                                          Throws SLI_INTSN_E::SLI_INT_M0P1_PKTPF_ERR. */
 	uint64_t m0p1_pktvf_err               : 1;  /**< On Mac0 PF1, when an error response is received for a VF PP transaction read, a doorbell
                                                          overflow for a ring associated with a VF occurs or an illegal memory access from a VF
-                                                         occurs,
-                                                         this bit is set.
-                                                         A subsequent read to SLI_MAC()_PF()_PKT_VF_INT is required to discover which VF.
+                                                         occurs, this bit is set. This bit should be cleared and followed by a read to
+                                                         SLI_MAC()_PF()_PKT_VF_INT to discover which VF.
                                                          Throws SLI_INTSN_E::SLI_INT_M0P1_PKTVF_ERR. */
 	uint64_t m0p1_dmapf_err               : 1;  /**< On Mac0 PF1, when an error response is received for a PF DMA transcation read, this bit is
                                                          set.
                                                          Throws SLI_INTSN_E::SLI_INT_M0P1_DMAPF_ERR. */
 	uint64_t m0p1_dmavf_err               : 1;  /**< When an error response is received for a VF DMA transaction read on MAC0 PF1, this bit is
-                                                         set.
-                                                         A subsequent read to SLI_MAC()_PF()_DMA_VF_INT is required to discover which VF.
+                                                         set. This bit should be cleared and followed by a read to SLI_MAC()_PF()_DMA_VF_INT to
+                                                         discover which VF.
                                                          Throws SLI_INTSN_E::SLI_INT_M0P1_DMAVF_ERR. */
 	uint64_t m0p0_pppf_err                : 1;  /**< On Mac0 PF0, when an error response is received for a PF PP transaction read,
                                                          this bit is set.
                                                          Throws SLI_INTSN_E::SLI_INT_M0P0_PPPF_ERR. */
 	uint64_t m0p0_ppvf_err                : 1;  /**< On Mac0 PF0, when an error response is received for a VF PP transaction read, this bit is
-                                                         set.
-                                                         A subsequent read to SLI_MAC()_PF()_PP_VF_INT is required to discover which VF.
+                                                         set. This bit should be cleared and followed by a read to SLI_MAC()_PF()_PP_VF_INT to
+                                                         discover which VF is set.
                                                          Note: this will only be set for SRIOV PF's PEM0 PF0 PF1 PEM2 PF0
                                                          Throws SLI_INTSN_E::SLI_INT_M0P0_PPVF_ERR. */
 	uint64_t m0p0_pktpf_err               : 1;  /**< On Mac0 PF0, When an error response is received for a PF packet transaction read or a
@@ -3955,36 +3953,37 @@ union cvmx_sli_ciu_int_sum {
                                                          Throws SLI_INTSN_E::SLI_INT_M0P0_PKTPF_ERR. */
 	uint64_t m0p0_pktvf_err               : 1;  /**< On Mac0 PF0, when an error response is received for a VF PP transaction read, a doorbell
                                                          overflow for a ring associated with a VF occurs or an illegal memory access from a VF
-                                                         occurs,
-                                                         this bit is set.
-                                                         A subsequent read to SLI_MAC()_PF()_PKT_VF_INT is required to discover which VF.
+                                                         occurs, this bit is set. This bit should be cleared and followed by a read to
+                                                         SLI_MAC()_PF()_PKT_VF_INT to discover which VF is set.
                                                          Throws SLI_INTSN_E::SLI_INT_M0P0_PKTVF_ERR. */
 	uint64_t m0p0_dmapf_err               : 1;  /**< On Mac0 PF0, when an error response is received for a PF DMA transcation read, this bit is
                                                          set.
                                                          Throws SLI_INTSN_E::SLI_INT_M0P0_DMAPF_ERR. */
 	uint64_t m0p0_dmavf_err               : 1;  /**< When an error response is received for a VF DMA transaction read on MAC0 PF0, this bit is
-                                                         set.
-                                                         A subsequent read to SLI_MAC()_PF()_DMA_VF_INT is required to discover which VF.
+                                                         set. This bit should be cleared and followed by a read to SLI_MAC()_PF()_DMA_VF_INT is
+                                                         required to discover which VF is set.
                                                          Note: this will only be set for SRIOV PF's PEM0 PF0 PF1 PEM2 PF0
                                                          Throws SLI_INTSN_E::SLI_INT_M0P0_DMAVF_ERR. */
 	uint64_t m2v0_flr                     : 1;  /**< A FLR occurred for a VF on PEM2 PF0
-                                                         A subsequent read to SLI_MAC()_PF()_FLR_VF_INT is required to discover which VF
-                                                         had a FLR
+                                                         This bit should be cleared and followed by a read to SLI_MAC()_PF()_FLR_VF_INT to discover
+                                                         which VF experienced the FLR.
                                                          Throws SLI_INTSN_E::SLI_INT_M2V0_FLR. */
-	uint64_t m2p0_flr                     : 1;  /**< A FLR occurred for PEM0 PF2
+	uint64_t m2p0_flr                     : 1;  /**< A FLR occurred for PEM0 PF2. This bit should be cleared and followed by a read to
+                                                         SLI_MAC()_PF()_FLR_VF_INT
+                                                         to discover which VF experienced the FLR.
                                                          Throws SLI_INTSN_E::SLI_INT_M2P0_FLR. */
 	uint64_t reserved_5_8                 : 4;
 	uint64_t m0v1_flr                     : 1;  /**< A FLR occurred for a VF on PEM0 PF1
-                                                         A subsequent read to SLI_MAC()_PF()_FLR_VF_INT is required to discover which VF
-                                                         had a FLR
+                                                         This bit should be cleared and followed by a read to SLI_MAC()_PF()_FLR_VF_INT
+                                                         to discover which VF experienced the FLR.
                                                          Throws SLI_INTSN_E::SLI_INT_M0V1_FLR. */
-	uint64_t m0p1_flr                     : 1;  /**< A FLR occurred for PEM0 PF1
+	uint64_t m0p1_flr                     : 1;  /**< A FLR occurred for PEM0 PF1.
                                                          Throws SLI_INTSN_E::SLI_INT_M0P1_FLR. */
-	uint64_t m0v0_flr                     : 1;  /**< A FLR occurred for a VF on PEM0 PF0
-                                                         A subsequent read to SLI_MAC()_PF()_FLR_VF_INT is required to discover which VF
-                                                         had a FLR
+	uint64_t m0v0_flr                     : 1;  /**< A FLR occurred for a VF on PEM0 PF0. This bit should be cleared and followed by a read to
+                                                         SLI_MAC()_PF()_FLR_VF_INT
+                                                         to discover which VF experienced the FLR.
                                                          Throws SLI_INTSN_E::SLI_INT_M0V0_FLR. */
-	uint64_t m0p0_flr                     : 1;  /**< A FLR occurred for PEM0 PF0
+	uint64_t m0p0_flr                     : 1;  /**< A FLR occurred for PEM0 PF0.
                                                          Throws SLI_INTSN_E::SLI_INT_M0P0_FLR. */
 	uint64_t rml_to                       : 1;  /**< A read or write transfer to a RSL that did not complete within
                                                          SLI_WINDOW_CTL[TIME] coprocessor-clock cycles
@@ -10248,7 +10247,7 @@ union cvmx_sli_pkt_gbl_control {
 	uint64_t pkpfval                      : 1;  /**< when zero, only VF's are subject to SLI_PKT_PKIND_VALID constraints, and PF instructions
                                                          can select any PKI PKIND.
                                                          When one, both PF's and VF's are subject to SLI_PKT_PKIND_VALID constraints. */
-	uint64_t bpflr_d                      : 1;  /**< Disables clearing SLI_PKT_OUT_BP_EN bit on an FLR. */
+	uint64_t bpflr_d                      : 1;  /**< Disables clearing SLI_PKT_OUT_BP_EN bit on a FLR. */
 	uint64_t noptr_d                      : 1;  /**< Disables putting a ring into reset when a packet is received from PKO and
                                                          the associated ring has no doorbells to send the packet out.
                                                          SLI_PKT_IN_DONE()_CNTS[CNT] when written. */
diff --git a/arch/mips/include/asm/octeon/cvmx-sriox-defs.h b/arch/mips/include/asm/octeon/cvmx-sriox-defs.h
index e0a7d65..aac32f1 100644
--- a/arch/mips/include/asm/octeon/cvmx-sriox-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-sriox-defs.h
@@ -1786,10 +1786,35 @@ typedef union cvmx_sriox_imsg_statusx cvmx_sriox_imsg_statusx_t;
 /**
  * cvmx_srio#_imsg_vport_thr
  *
- * SRIO0_IMSG_VPORT_THR[MAX_TOT] must be >= SRIO0_IMSG_VPORT_THR[BUF_THR]
+ * This register allocates the virtual ports (vports) between the two
+ * inbound message ports used by each SRIO MAC.  These channels are also
+ * know as Reassembly IDs (RIDs) on some of the other devices.  Care must
+ * be taken to avoid using the same vport on more than one device.  The
+ * 75xx default settings do create a conflict that force reprograming this
+ * register before message traffic can be enabled.  The typical 75xx vport
+ * IDs allocations allow the SRIO MACS to use vports 6 thru 95.  The default
+ * csr values allocate the following:
+ *
+ * SRIO0  BASE =  8, MAX_TOT = 44, SP_EN = 1
+ *     This uses vports  8 thru 50 for MP Messages and vport 52 for SP Messages
+ * SRIO1  BASE = 52, MAX_TOT = 44, SP_EN = 1
+ *     This uses vports 52 thru 94 for MP Messages and vport 96 for SP Messages
+ *
+ * Unfortunately vport 52 conflicts between SRIOs and vport 96 conflicts
+ * with another device.  Multiple configurations can be used to resolve the
+ * issue depending on SRIO usage.  Many more options are possible.
+ *
+ *   1.  Changing the value of MAX_TOT to 43 on both devices.
+ *   2.  Switching the SRIO0 BASE to 6 and the SRIO1 BASE to 51 allows
+ *       vports 6 thru 95 to be allocated.
+ *   3.  If a single SRIO handles inbound message traffic then setting
+ *       SRIOn BASE = 8, MAX_TOT = 47, SP_EN = 1
+ *   4.  If Single Packet message are uncommon then setting SP_EN = 0
+ *       on both SRIOs.
+ *
  * This register can be accessed regardless of the value in
  * SRIO()_STATUS_REG[ACCESS] and is not effected by MAC reset.  The
- * maximum number of VPORTs allocated to a MAC is limited to 48.
+ * maximum number of VPORTs allocated to a MAC is limited to 47.
  *
  * This register is reset by the coprocessor-clock reset.
  */
@@ -1800,13 +1825,14 @@ union cvmx_sriox_imsg_vport_thr {
 	uint64_t reserved_63_63               : 1;
 	uint64_t base                         : 7;  /**< Vport starting offset.  The Vports used between SRIO0 and SRIO1 must not overlap
                                                          with each other or other devices.  The first 8 vports are initially for BGX, Loopback,
-                                                         etc. See PKI_REASM_E.
-                                                         Default is 8 (to 51) for SRIO0 and 52 (to 95) for SRIO1. */
+                                                         etc. See PKI_REASM_E. */
 	uint64_t reserved_54_55               : 2;
 	uint64_t max_tot                      : 6;  /**< Sets max number of vports available to this SRIO MAC.  Maximum value supported by
                                                          hardware is 47 with SP_VPORT set or 46 with SP_VPORT clear.  The total number of
                                                          vports available to SRIO MACs by the 75xx is 94 but the number available to SRIO
-                                                         depends on the configuration of other blocks.  Default is 44 vports each. */
+                                                         depends on the configuration of other blocks.  The default is 44 vports each.
+                                                         MAX_TOT value must be greater than or equal to the value in BUF_THR to
+                                                         effectively limit the number of vports used. */
 	uint64_t reserved_46_47               : 2;
 	uint64_t max_s1                       : 6;  /**< Diagnostic Use only.  Must be written to 0x30 for normal operation. */
 	uint64_t reserved_38_39               : 2;
@@ -1828,9 +1854,19 @@ union cvmx_sriox_imsg_vport_thr {
                                                          small, larger BUF_THR values may improve
                                                          performance. */
 	uint64_t reserved_14_15               : 2;
-	uint64_t max_p1                       : 6;  /**< Maximum number of open vports in port 1. */
+	uint64_t max_p1                       : 6;  /**< Maximum number of open vports in port 1.  Setting the value
+                                                         less than MAX_TOT (MAX_TOT-1 if SP_VPORT is set) effectively
+                                                         further restricts the number of vports (partial messages)
+                                                         received by the port at a time.
+                                                         The values used in the MAX_P1 and MAX_P0 fields can be used
+                                                         to "reserve" vports for each message port. */
 	uint64_t reserved_6_7                 : 2;
-	uint64_t max_p0                       : 6;  /**< Maximum number of open vports in port 0. */
+	uint64_t max_p0                       : 6;  /**< Maximum number of open vports in port 0.  Setting the value
+                                                         less than MAX_TOT (MAX_TOT-1 if SP_VPORT is set) effectively
+                                                         further restricts the number of vports (partial messages)
+                                                         received by the port at a time.
+                                                         The values used in the MAX_P1 and MAX_P0 fields can be used
+                                                         to "reserve" vports for each message port. */
 #else
 	uint64_t max_p0                       : 6;
 	uint64_t reserved_6_7                 : 2;
@@ -4190,10 +4226,11 @@ union cvmx_sriox_status_reg {
 	uint64_t reserved_9_63                : 55;
 	uint64_t host                         : 1;  /**< SRIO Host Setting.  This field is initialized on a cold reset based on the
                                                          value of the corresponding SRIOx_SPD pins. If the pins are set to 15 then
-                                                         the port is disabled and set to host otherwise it is initialized as an end
-                                                         point.  The values in this field are used to determine the setting in the
-                                                         SRIOMAINT()_PORT_GEN_CTL[HOST] field.  The value is not modified during
-                                                         a warm or soft reset and should be set before SRIO bit is enabled.
+                                                         the port is disabled and HOST is set otherwise it is initialized as an endpoint
+                                                         (HOST is cleared).  The values in this field are used to determine
+                                                         the setting in the SRIOMAINT()_PORT_GEN_CTL[HOST] field.  The value is not
+                                                         modified during a warm or soft reset and should be set before SRIO bit is
+                                                         enabled.
                                                           0 = SRIO port is endpoint (EP).
                                                           1 = SRIO port is host. */
 	uint64_t spd                          : 4;  /**< SRIO Speed Setting.  This field is initialized on a cold reset based on the
diff --git a/arch/mips/include/asm/octeon/cvmx-wqe.h b/arch/mips/include/asm/octeon/cvmx-wqe.h
index b2a9d6f..6d46e27 100644
--- a/arch/mips/include/asm/octeon/cvmx-wqe.h
+++ b/arch/mips/include/asm/octeon/cvmx-wqe.h
@@ -70,7 +70,7 @@ extern "C" {
                                 (((x) == CVMX_POW_TAG_TYPE_NULL) ?  "NULL" : \
                                 "NULL_NULL")))
 
-/* Error levels in WQE entry word2 */
+/* Error levels in WQE WORD2 (ERRLEV).*/
 #define PKI_ERRLEV_E__RE_M                                 (0x0)
 #define PKI_ERRLEV_E__LA_M                                 (0x1)
 #define PKI_ERRLEV_E__LB_M                                 (0x2)
@@ -90,6 +90,23 @@ enum cvmx_pki_errlevel {
 	CVMX_PKI_ERRLEV_E_LF                         = PKI_ERRLEV_E__LF_M,
 	CVMX_PKI_ERRLEV_E_LG                         = PKI_ERRLEV_E__LG_M
 };
+#define CVMX_PKI_ERRLEV_MAX  (1 << 3) /* The size of WORD2:ERRLEV field.*/
+
+/* Error code in WQE WORD2 (OPCODE).*/
+#define CVMX_PKI_OPCODE_RE_NONE		0x0
+#define CVMX_PKI_OPCODE_RE_PARTIAL	0x1
+#define CVMX_PKI_OPCODE_RE_JABBER	0x2
+#define CVMX_PKI_OPCODE_RE_FCS		0x7
+#define CVMX_PKI_OPCODE_RE_FCS_RCV	0x8
+#define CVMX_PKI_OPCODE_RE_TERMINATE	0x9
+#define CVMX_PKI_OPCODE_RE_RX_CTL	0xb
+#define CVMX_PKI_OPCODE_RE_SKIP		0xc
+#define CVMX_PKI_OPCODE_RE_DMAPKT	0xf
+#define CVMX_PKI_OPCODE_RE_PKIPAR	0x13
+#define CVMX_PKI_OPCODE_RE_PKIPCAM	0x14
+#define CVMX_PKI_OPCODE_RE_MEMOUT	0x15
+
+#define CVMX_PKI_OPCODE_MAX  (1 << 8) /* The size of WORD2:OPCODE field.*/
 
 /* Layer types in pki */
 #define CVMX_PKI_LTYPE_E_NONE_M                                (0x0)
@@ -726,20 +743,6 @@ typedef union {
 #endif
 } cvmx_pki_wqe_word2_t;
 
-#define CVMX_PKI_OPCODE_RE_NONE		0x0
-#define CVMX_PKI_OPCODE_RE_PARTIAL	0x1
-#define CVMX_PKI_OPCODE_RE_JABBER	0x2
-#define CVMX_PKI_OPCODE_RE_FCS		0x7
-#define CVMX_PKI_OPCODE_RE_FCS_RCV	0x8
-#define CVMX_PKI_OPCODE_RE_TERMINATE	0x9
-#define CVMX_PKI_OPCODE_RE_RX_CTL	0xb
-#define CVMX_PKI_OPCODE_RE_SKIP		0xc
-#define CVMX_PKI_OPCODE_RE_DMAPKT	0xf
-#define CVMX_PKI_OPCODE_RE_PKIPAR	0x13
-#define CVMX_PKI_OPCODE_RE_PKIPCAM	0x14
-#define CVMX_PKI_OPCODE_RE_MEMOUT	0x15
-
-
 typedef union {
 	uint64_t u64;
 	cvmx_pki_wqe_word2_t pki;
@@ -1271,16 +1274,27 @@ static inline void cvmx_wqe_set_len(cvmx_wqe_t *work, int len)
 /**
  * This function returns if there was L2/L1 errors detected in packet.
  * @param work	pointer to work queue entry
- * @return	error code -- If L2/L1 error was found in packet
- *		0 -- If no L2/L1 error was found in packet.
+ * @return	0 if packet had no error, non-zero to indicate error code.
+ *
+ * Please refer to HRM for the specific model for full enumaration of error
+ * codes.
+ *
+ * With Octeon1/Octeon2 models, the returned code indicates L1/L2 errors.
+ * On CN73XX/CN78XX, the return code is the value of PKI_OPCODE_E,
+ * if it is non-zero, otherwise the returned code will be derived from
+ * PKI_ERRLEV_E such that an error indicated in LayerA will return 0x20,
+ * LayerB - 0x30, LayerC - 0x40 and so forth.
  */
 static inline int cvmx_wqe_get_rcv_err(cvmx_wqe_t *work)
 {
 	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
 		cvmx_wqe_78xx_t *wqe = (void *)work;
 		if (wqe->word2.err_level == CVMX_PKI_ERRLEV_E_RE ||
-		    wqe->word2.err_level == CVMX_PKI_ERRLEV_E_LA)
-			return(wqe->word2.err_code);
+		    wqe->word2.err_code != 0)
+		    return wqe->word2.err_code;
+		else
+		    return (wqe->word2.err_level << 4) + 0x10;
+
 	}
 	else if (work->word2.snoip.rcv_error)
 		return (work->word2.snoip.err_code);
diff --git a/arch/mips/include/asm/octeon/octeon-model.h b/arch/mips/include/asm/octeon/octeon-model.h
index 29b24b0..004ac53 100644
--- a/arch/mips/include/asm/octeon/octeon-model.h
+++ b/arch/mips/include/asm/octeon/octeon-model.h
@@ -43,7 +43,7 @@
  * File defining different Octeon model IDs and macros to
  * compare them.
  *
- * <hr>$Revision: 114196 $<hr>
+ * <hr>$Revision: 124439 $<hr>
  */
 
 #ifndef __OCTEON_MODEL_H__
@@ -107,6 +107,8 @@ extern "C" {
 #define OCTEON_CN73XX           (OCTEON_CN73XX_PASS1_0 | OM_IGNORE_REVISION)
 #define OCTEON_CN73XX_PASS1_X   (OCTEON_CN73XX_PASS1_0 | OM_IGNORE_MINOR_REVISION)
 
+#define OCTEON_CN72XX		OCTEON_CN73XX
+
 #define OCTEON_CN70XX_PASS1_0   0x000d9600
 #define OCTEON_CN70XX_PASS1_1   0x000d9601
 #define OCTEON_CN70XX_PASS1_2   0x000d9602
-- 
2.6.2

