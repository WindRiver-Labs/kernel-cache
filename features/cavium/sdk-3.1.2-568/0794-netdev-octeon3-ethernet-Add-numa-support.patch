From 2b81220555b326d8bb2fa306abd1021f5ad75db5 Mon Sep 17 00:00:00 2001
From: Carlos Munoz <cmunoz@caviumnetworks.com>
Date: Thu, 10 Jul 2014 18:05:50 -0700
Subject: [PATCH 794/974] netdev: octeon3-ethernet: Add numa support.

Signed-off-by: Carlos Munoz <cmunoz@caviumnetworks.com>
[Original patch taken from Cavium SDK 3.1.2-568]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
---
 drivers/net/ethernet/octeon/octeon3-ethernet.c | 197 +++++++++++++++----------
 1 file changed, 119 insertions(+), 78 deletions(-)

diff --git a/drivers/net/ethernet/octeon/octeon3-ethernet.c b/drivers/net/ethernet/octeon/octeon3-ethernet.c
index 51340b7..45aff8a 100644
--- a/drivers/net/ethernet/octeon/octeon3-ethernet.c
+++ b/drivers/net/ethernet/octeon/octeon3-ethernet.c
@@ -161,8 +161,12 @@ struct octeon3_napi_wrapper {
 	struct octeon3_rx	*cxt;
 } ____cacheline_aligned_in_smp;
 
+/* Up to 2 napis per core are supported */
+#define MAX_NAPI_PER_CPU	2
+#define MAX_NAPIS_PER_NODE	(CVMX_MAX_CORES * MAX_NAPI_PER_CPU)
+
 static struct octeon3_napi_wrapper
-napi_wrapper[OCTEON3_ETH_MAX_NUMA_NODES][CVMX_MAX_CORES]
+napi_wrapper[OCTEON3_ETH_MAX_NUMA_NODES][MAX_NAPIS_PER_NODE]
 __cacheline_aligned_in_smp;
 
 struct octeon3_ethernet;
@@ -244,11 +248,12 @@ struct octeon3_ethernet_node {
 	struct octeon3_ethernet_worker workers[8];
 	struct mutex device_list_lock;
 	struct list_head device_list;
-	DECLARE_BITMAP(napi_cpu_bitmap, CVMX_MAX_CORES);
-	int napi_max_cpus;
 	spinlock_t napi_alloc_lock;
 };
 
+/* This array keeps track of the number of napis running on each cpu */
+static u8 octeon3_cpu_napi_cnt[NR_CPUS];
+
 static int recycle_skbs = 1;
 module_param(recycle_skbs, int, 0644);
 MODULE_PARM_DESC(recycle_skbs, "Allow recycling skbs back to fpa auras.");
@@ -322,7 +327,7 @@ static void octeon3_eth_sso_pass1_limit(int node, int	grp)
  * Map auras to the field priv->buffers_needed. Used to speed up packet
  * transmission.
  */
-static void *aura2buffers_needed[1024];
+static void *aura2bufs_needed[OCTEON3_ETH_MAX_NUMA_NODES][CVMX_FPA3_NUM_AURAS];
 
 static int octeon3_eth_lgrp_to_ggrp(int node, int grp)
 {
@@ -601,13 +606,11 @@ static int octeon3_eth_tx_complete_worker(void *data)
 	struct octeon3_ethernet_worker *worker = data;
 	struct octeon3_ethernet_node *oen = worker->oen;
 	int backlog;
-	int loops;
-	const int tx_complete_batch = 100;
-	int backlog_stop_thresh = worker->order == 0 ? 0 : (worker->order - 1) * 80;
+	int order = worker->order;
+	int tx_complete_stop_thresh = order * 100;
+	int backlog_stop_thresh = order == 0 ? 31 : order * 80;
 	int i;
-#ifndef CONFIG_PREEMPT
-	unsigned long last_jiffies = jiffies;
-#endif
+
 	for (;;) {
 		/*
 		 * replaced by wait_event to avoid warnings like
@@ -615,29 +618,16 @@ static int octeon3_eth_tx_complete_worker(void *data)
 		 */
 		wait_event_interruptible(worker->queue, octeon3_eth_tx_complete_runnable(worker));
 		atomic_dec_if_positive(&worker->kick); /* clear the flag */
-		loops = 0;
+
 		do {
-		re_enter:
-			loops++;
 			backlog = octeon3_eth_replenish_all(oen);
-			if (loops > 3 && backlog > 100 * worker->order &&
-			    worker->order < ARRAY_SIZE(oen->workers) - 1) {
-				atomic_set(&oen->workers[worker->order + 1].kick, 1);
-				wake_up(&oen->workers[worker->order + 1].queue);
-			}
-			for (i = 0; i < tx_complete_batch; i++) {
+			for (i = 0; i < 100; i++) {
 				void **work;
 				struct net_device *tx_netdev;
 				struct octeon3_ethernet *tx_priv;
 				struct sk_buff *skb;
 				struct wr_ret r;
 
-#ifndef CONFIG_PREEMPT
-				if (jiffies != last_jiffies) {
-					schedule();
-					last_jiffies = jiffies;
-				}
-#endif
 				r = octeon3_eth_work_request_grp_sync(oen->tx_complete_grp, CVMX_POW_NO_WAIT);
 				work = r.work;
 				if (work == NULL)
@@ -650,14 +640,25 @@ static int octeon3_eth_tx_complete_worker(void *data)
 				skb = container_of((void *)work, struct sk_buff, cb);
 				dev_kfree_skb(skb);
 			}
-			aq_cnt.u64 = cvmx_read_csr_node(oen->numa_node, CVMX_SSO_GRPX_AQ_CNT(oen->tx_complete_grp));
-		} while (backlog > backlog_stop_thresh   && aq_cnt.s.aq_cnt > worker->order * tx_complete_batch);
+
+			aq_cnt.u64 = cvmx_read_csr_node(oen->numa_node,
+				    CVMX_SSO_GRPX_AQ_CNT(oen->tx_complete_grp));
+			if ((backlog > backlog_stop_thresh ||
+			     aq_cnt.s.aq_cnt > tx_complete_stop_thresh) &&
+			    order < ARRAY_SIZE(oen->workers) - 1) {
+				atomic_set(&oen->workers[order + 1].kick, 1);
+				wake_up(&oen->workers[order + 1].queue);
+			}
+		} while (!need_resched() &&
+			 (backlog > backlog_stop_thresh ||
+			  aq_cnt.s.aq_cnt > tx_complete_stop_thresh));
+
+		cond_resched();
+
 		if (!octeon3_eth_tx_complete_runnable(worker))
 			octeon3_eth_sso_irq_set_armed(oen->numa_node, oen->tx_complete_grp, true);
-		aq_cnt.u64 = cvmx_read_csr_node(oen->numa_node, CVMX_SSO_GRPX_AQ_CNT(oen->tx_complete_grp));
-		if (aq_cnt.s.aq_cnt > worker->order * tx_complete_batch)
-			goto re_enter;
 	}
+
 	return 0;
 }
 
@@ -831,46 +832,66 @@ static struct sk_buff *octeon3_eth_work_to_skb(void *w)
 	return skb;
 }
 
+/* octeon3_napi_alloc_cpu:	Find an available cpu. This function must be
+ *				called with the napi_alloc_lock lock held.
+ *
+ *  node:			Node to allocate cpu from.
+ *
+ *  returns:			cpu number or a negative error code.
+ */
+static int octeon3_napi_alloc_cpu(int	node)
+{
+	int				cpu;
+	int				min_cnt = MAX_NAPI_PER_CPU;
+	int				min_cpu = -EBUSY;
+
+	for_each_cpu(cpu, cpumask_of_node(node)) {
+		if (octeon3_cpu_napi_cnt[cpu] == 0) {
+			min_cpu = cpu;
+			break;
+		} else if (octeon3_cpu_napi_cnt[cpu] < min_cnt) {
+			min_cnt = octeon3_cpu_napi_cnt[cpu];
+			min_cpu = cpu;
+		}
+	}
+
+	if (min_cpu < 0) {
+		pr_err("WARNING: Failed to allocate a cpu\n");
+		return min_cpu;
+	}
+
+	octeon3_cpu_napi_cnt[min_cpu]++;
+
+	return min_cpu;
+}
+
 /* octeon3_napi_alloc:		Allocate a napi.
  *
  *  cxt:			Receive context the napi will be added to.
  *  idx:			Napi index within the receive context.
- *  cpu:			Cpu to bind the napi to:
- *					<  0: use any cpu.
- *					>= 0: use requested cpu.
  *
  *  Returns:			Pointer to napi wrapper or NULL on error.
  */
 static struct octeon3_napi_wrapper *octeon3_napi_alloc(struct octeon3_rx *cxt,
-						       int		  idx,
-						       int		  cpu)
+						       int		  idx)
 {
 	struct octeon3_ethernet_node	*oen;
 	struct octeon3_ethernet		*priv = cxt->parent;
 	int				node = priv->numa_node;
 	unsigned long			flags;
+	int				cpu;
 	int				i;
 
 	oen = octeon3_eth_node + node;
 	spin_lock_irqsave(&oen->napi_alloc_lock, flags);
 
 	/* Find a free napi wrapper */
-	for (i = 0; i < CVMX_MAX_CORES; i++) {
+	for (i = 0; i < MAX_NAPIS_PER_NODE; i++) {
 		if (napi_wrapper[node][i].available) {
-			/* Assign a cpu to use (a free cpu if possible) */
-			if (cpu < 0) {
-				cpu = find_first_zero_bit(oen->napi_cpu_bitmap,
-							  oen->napi_max_cpus);
-				if (cpu < oen->napi_max_cpus) {
-					bitmap_set(oen->napi_cpu_bitmap,
-						   cpu, 1);
-				} else {
-					/* Choose a random cpu */
-					get_random_bytes(&cpu, sizeof(int));
-					cpu = cpu % oen->napi_max_cpus;
-				}
-			} else
-				bitmap_set(oen->napi_cpu_bitmap, cpu, 1);
+			/* Allocate a cpu to use */
+			cpu = octeon3_napi_alloc_cpu(node);
+			if (cpu < 0)
+				break;
 
 			napi_wrapper[node][i].available = 0;
 			napi_wrapper[node][i].idx = idx;
@@ -918,7 +939,7 @@ static int octeon3_rm_napi_from_cxt(int				node,
 
 	/* Free the napi block */
 	spin_lock_irqsave(&oen->napi_alloc_lock, flags);
-	bitmap_clear(oen->napi_cpu_bitmap, napiw->cpu, 1);
+	octeon3_cpu_napi_cnt[napiw->cpu]--;
 	napiw->available = 1;
 	napiw->idx = -1;
 	napiw->cpu = -1;
@@ -958,7 +979,7 @@ static int octeon3_add_napi_to_cxt(struct octeon3_rx *cxt)
 	spin_unlock_irqrestore(&cxt->napi_idx_lock, flags);
 
 	/* Get a free napi block */
-	napiw = octeon3_napi_alloc(cxt, idx, -1);
+	napiw = octeon3_napi_alloc(cxt, idx);
 	if (unlikely(napiw == NULL)) {
 		spin_lock_irqsave(&cxt->napi_idx_lock, flags);
 		bitmap_clear(cxt->napi_idx_bitmap, idx, 1);
@@ -991,6 +1012,7 @@ static int octeon3_eth_rx_one(struct octeon3_rx *rx, bool is_async,
 	struct wr_ret r;
 	struct octeon3_ethernet *priv = rx->parent;
 	void **buf;
+	u64 gaura;
 
 	if (is_async == true)
 		r = octeon3_eth_get_work_response_async();
@@ -1007,11 +1029,18 @@ static int octeon3_eth_rx_one(struct octeon3_rx *rx, bool is_async,
 
 	skb = octeon3_eth_work_to_skb(work);
 
-	/* Save the aura this skb came from to allow the pko to free the skb
-	 * back to the correct aura.
+	/* Save the aura and node this skb came from to allow the pko to free
+	 * the skb back to the correct aura. A magic number is also added to
+	 * later verify the skb came from the fpa.
+	 *
+	 *  63                                    12 11  10 9                  0
+	 * ---------------------------------------------------------------------
+	 * |                  magic                 | node |        aura       |
+	 * ---------------------------------------------------------------------
 	 */
 	buf = (void **)PTR_ALIGN(skb->head, 128);
-	buf[SKB_AURA_OFFSET] = (void *)(SKB_AURA_MAGIC | priv->pki_laura);
+	gaura = SKB_AURA_MAGIC | work->word0.aura;
+	buf[SKB_AURA_OFFSET] = (void *)gaura;
 
 	segments = work->word0.bufs;
 	ret = segments;
@@ -1230,15 +1259,12 @@ static int octeon3_napi_init_node(int node, struct net_device *netdev)
 	if (oen->napi_init_done)
 		goto done;
 
-	bitmap_zero(oen->napi_cpu_bitmap, CVMX_MAX_CORES);
-	oen->napi_max_cpus = nr_cpus_node(node);
-
-	for (i = 0; i < CVMX_MAX_CORES; i++) {
+	for (i = 0; i < MAX_NAPIS_PER_NODE; i++) {
 		netif_napi_add(netdev, &napi_wrapper[node][i].napi,
 			       octeon3_eth_napi, 32);
 		napi_enable(&napi_wrapper[node][i].napi);
 		napi_wrapper[node][i].available = 1;
-		napi_wrapper[node][i].idx = 0;
+		napi_wrapper[node][i].idx = -1;
 		napi_wrapper[node][i].cpu = -1;
 		napi_wrapper[node][i].cxt = NULL;
 	}
@@ -1332,7 +1358,8 @@ static int octeon3_eth_ndo_init(struct net_device *netdev)
 	priv->pki_laura = aura.laura;
 	octeon3_eth_fpa_aura_init(oen->pki_packet_pool, aura,
 				  num_packet_buffers * 2);
-	aura2buffers_needed[priv->pki_laura] = &priv->buffers_needed;
+	aura2bufs_needed[priv->numa_node][priv->pki_laura] =
+		&priv->buffers_needed;
 
 	base_rx_grp = -1;
 	r = cvmx_sso_allocate_group_range(priv->numa_node, &base_rx_grp, rx_contexts);
@@ -1501,7 +1528,6 @@ static int octeon3_eth_ndo_open(struct net_device *netdev)
 	struct irq_domain *d = octeon_irq_get_block_domain(priv->numa_node, SSO_INTSN_EXE);
 	struct octeon3_rx *rx;
 	int idx;
-	int cpu;
 	int i;
 	int r;
 
@@ -1543,9 +1569,8 @@ static int octeon3_eth_ndo_open(struct net_device *netdev)
 		}
 		bitmap_set(priv->rx_cxt[i].napi_idx_bitmap, idx, 1);
 
-		cpu = cpumask_first(&rx->rx_affinity_hint);
 		priv->rx_cxt[i].napiw = octeon3_napi_alloc(&priv->rx_cxt[i],
-							   idx, cpu);
+							   idx);
 		if (priv->rx_cxt[i].napiw == NULL) {
 			r = -ENOMEM;
 			goto err4;
@@ -1670,6 +1695,8 @@ static int octeon3_eth_ndo_start_xmit(struct sk_buff *skb, struct net_device *ne
 	union cvmx_pko_send_mem send_mem;
 	union cvmx_pko_lmtdma_data lmtdma_data;
 	union cvmx_pko_query_rtn query_rtn;
+	union cvmx_sso_grpx_aq_cnt aq_cnt;
+	struct octeon3_ethernet_node *oen;
 	u8 l4_hdr = 0;
 	unsigned int checksum_alg;
 	long backlog;
@@ -1678,7 +1705,7 @@ static int octeon3_eth_ndo_start_xmit(struct sk_buff *skb, struct net_device *ne
 	u64 dma_addr;
 	void **work;
 	bool can_recycle_skb = false;
-	int aura = 0;
+	int gaura = 0;
 	void *buffers_needed = NULL;
 	void **buf;
 
@@ -1702,9 +1729,29 @@ static int octeon3_eth_ndo_start_xmit(struct sk_buff *skb, struct net_device *ne
 		magic = (uint64_t)buf[SKB_AURA_OFFSET];
 		if (likely(buf[SKB_PTR_OFFSET] == skb) &&
 		    likely((magic & 0xfffffffffffff000) == SKB_AURA_MAGIC)) {
+			int		node;
+			int		aura;
+
 			can_recycle_skb = true;
-			aura = magic & 0xfff;
-			buffers_needed = aura2buffers_needed[aura];
+			gaura = magic & 0xfff;
+			node = gaura >> 10;
+			aura = gaura & 0x3ff;
+			buffers_needed = aura2bufs_needed[node][aura];
+		}
+	}
+
+	/* Drop the packet if pko or sso are not keeping up */
+	oen = octeon3_eth_node + priv->numa_node;
+	backlog = atomic64_inc_return(&priv->tx_backlog);
+	aq_cnt.u64 = cvmx_read_csr_node(oen->numa_node,
+				CVMX_SSO_GRPX_AQ_CNT(oen->tx_complete_grp));
+	if (unlikely(backlog > MAX_TX_QUEUE_DEPTH) ||
+	    (can_recycle_skb == false && aq_cnt.s.aq_cnt > 100000)) {
+		if (use_tx_queues) {
+			netif_stop_queue(netdev);
+		} else {
+			atomic64_dec(&priv->tx_backlog);
+			goto skip_xmit;
 		}
 	}
 
@@ -1722,16 +1769,6 @@ static int octeon3_eth_ndo_start_xmit(struct sk_buff *skb, struct net_device *ne
 	work[0] = netdev;
 	work[1] = NULL;
 
-	backlog = atomic64_inc_return(&priv->tx_backlog);
-	if (unlikely(backlog > MAX_TX_QUEUE_DEPTH)) {
-		if (use_tx_queues) {
-			netif_stop_queue(netdev);
-		} else {
-			atomic64_dec(&priv->tx_backlog);
-			goto skip_xmit;
-		}
-	}
-
 	/* Adjust the port statistics. */
 	atomic64_inc(&priv->tx_packets);
 	atomic64_add(skb->len, &priv->tx_octets);
@@ -1740,6 +1777,8 @@ static int octeon3_eth_ndo_start_xmit(struct sk_buff *skb, struct net_device *ne
 	 * submitting the command below
 	 */
 	wmb();
+
+	/* Build the pko command */
 	send_hdr.u64 = 0;
 #ifdef __LITTLE_ENDIAN
 	send_hdr.s.le = 1;
@@ -1748,7 +1787,7 @@ static int octeon3_eth_ndo_start_xmit(struct sk_buff *skb, struct net_device *ne
 		send_hdr.s.n2 = 1; /* Don't allocate to L2 */
 	send_hdr.s.df = 1; /* Don't automatically free to FPA */
 	send_hdr.s.total = skb->len;
-	send_hdr.s.aura = aura;
+	send_hdr.s.aura = gaura;
 
 	if (skb->ip_summed != CHECKSUM_NONE) {
 #ifndef BROKEN_SIMULATOR_CSUM
@@ -1789,6 +1828,7 @@ static int octeon3_eth_ndo_start_xmit(struct sk_buff *skb, struct net_device *ne
 			break;
 		}
 	}
+	preempt_disable();
 	cvmx_scratch_write64(scr_off, send_hdr.u64);
 	scr_off += sizeof(send_hdr);
 
@@ -1869,6 +1909,7 @@ static int octeon3_eth_ndo_start_xmit(struct sk_buff *skb, struct net_device *ne
 	lmtdma_data.s.dq = priv->pko_queue;
 	dma_addr = 0xffffffffffffa400ull | ((scr_off & 0x78) - 8);
 	cvmx_write64_uint64(dma_addr, lmtdma_data.u64);
+	preempt_enable();
 
 	if (wait_pko_response) {
 		CVMX_SYNCIOBDMA;
-- 
2.6.2

