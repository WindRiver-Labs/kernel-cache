From 5015deb95caab54cdccefa40261a67d43967503c Mon Sep 17 00:00:00 2001
From: David Daney <david.daney@cavium.com>
Date: Fri, 29 Jun 2012 13:49:52 -0700
Subject: [PATCH 103/337] MIPS: OCTEON: Add support for cn68XX SSO/work queue.

Based On SDK 3.0.0-482

This is needed for follow-on patches to enable octeon-ethernet to work
on cn68XX.

Signed-off-by: David Daney <david.daney@cavium.com>
Signed-off-by: Corey Minyard <cminyard@mvista.com>
Signed-off-by: Jack Tan <jack.tan@windriver.com>
---
 arch/mips/include/asm/octeon/cvmx-address.h |    1 +
 arch/mips/include/asm/octeon/cvmx-pko.h     |   26 -
 arch/mips/include/asm/octeon/cvmx-pow.h     | 2249 +++++++++++++--------------
 arch/mips/include/asm/octeon/cvmx-wqe.h     |  847 ++++++++---
 drivers/staging/octeon/ethernet-rx.c        |   43 +-
 5 files changed, 1728 insertions(+), 1438 deletions(-)

diff --git a/arch/mips/include/asm/octeon/cvmx-address.h b/arch/mips/include/asm/octeon/cvmx-address.h
index 3c74d82..5616f59 100644
--- a/arch/mips/include/asm/octeon/cvmx-address.h
+++ b/arch/mips/include/asm/octeon/cvmx-address.h
@@ -259,6 +259,7 @@ typedef union {
 #define CVMX_OCT_DID_TAG_TAG2       CVMX_FULL_DID(CVMX_OCT_DID_TAG, 2ULL)
 #define CVMX_OCT_DID_TAG_TAG3       CVMX_FULL_DID(CVMX_OCT_DID_TAG, 3ULL)
 #define CVMX_OCT_DID_TAG_NULL_RD    CVMX_FULL_DID(CVMX_OCT_DID_TAG, 4ULL)
+#define CVMX_OCT_DID_TAG_TAG5       CVMX_FULL_DID(CVMX_OCT_DID_TAG, 5ULL)
 #define CVMX_OCT_DID_TAG_CSR        CVMX_FULL_DID(CVMX_OCT_DID_TAG, 7ULL)
 #define CVMX_OCT_DID_FAU_FAI        CVMX_FULL_DID(CVMX_OCT_DID_IOB, 0ULL)
 #define CVMX_OCT_DID_TIM_CSR        CVMX_FULL_DID(CVMX_OCT_DID_TIM, 0ULL)
diff --git a/arch/mips/include/asm/octeon/cvmx-pko.h b/arch/mips/include/asm/octeon/cvmx-pko.h
index 57095b0..beaa8f4 100644
--- a/arch/mips/include/asm/octeon/cvmx-pko.h
+++ b/arch/mips/include/asm/octeon/cvmx-pko.h
@@ -368,24 +368,6 @@ static inline void cvmx_pko_doorbell(uint64_t ipd_port, uint64_t queue,
 static inline void cvmx_pko_send_packet_prepare(uint64_t port, uint64_t queue,
 						cvmx_pko_lock_t use_locking)
 {
-	if (use_locking == CVMX_PKO_LOCK_ATOMIC_TAG) {
-		/*
-		 * Must do a full switch here to handle all cases.  We
-		 * use a fake WQE pointer, as the POW does not access
-		 * this memory.  The WQE pointer and group are only
-		 * used if this work is descheduled, which is not
-		 * supported by the
-		 * cvmx_pko_send_packet_prepare/cvmx_pko_send_packet_finish
-		 * combination.  Note that this is a special case in
-		 * which these fake values can be used - this is not a
-		 * general technique.
-		 */
-		uint32_t tag = CVMX_TAG_SW_BITS_INTERNAL << CVMX_TAG_SW_SHIFT |
-			CVMX_TAG_SUBGROUP_PKO << CVMX_TAG_SUBGROUP_SHIFT |
-			(CVMX_TAG_SUBGROUP_MASK & queue);
-		cvmx_pow_tag_sw_full((cvmx_wqe_t *) cvmx_phys_to_ptr(0x80),
-				     tag, CVMX_POW_TAG_TYPE_ATOMIC, 0);
-	}
 }
 
 #define cvmx_pko_send_packet_prepare_pkoid	cvmx_pko_send_packet_prepare
@@ -419,8 +401,6 @@ cvmx_pko_send_packet_finish(uint64_t ipd_port, uint64_t queue,
 			    cvmx_pko_lock_t use_locking)
 {
 	cvmx_cmd_queue_result_t result;
-	if (use_locking == CVMX_PKO_LOCK_ATOMIC_TAG)
-		cvmx_pow_tag_sw_wait();
 	result = cvmx_cmd_queue_write2(CVMX_CMD_QUEUE_PKO(queue),
 				       (use_locking == CVMX_PKO_LOCK_CMD_QUEUE),
 				       pko_command.u64, packet.u64);
@@ -464,8 +444,6 @@ cvmx_pko_send_packet_finish3(uint64_t ipd_port, uint64_t queue,
 			     uint64_t addr, cvmx_pko_lock_t use_locking)
 {
 	cvmx_cmd_queue_result_t result;
-	if (use_locking == CVMX_PKO_LOCK_ATOMIC_TAG)
-		cvmx_pow_tag_sw_wait();
 	result = cvmx_cmd_queue_write3(CVMX_CMD_QUEUE_PKO(queue),
 				       (use_locking == CVMX_PKO_LOCK_CMD_QUEUE),
 				       pko_command.u64, packet.u64, addr);
@@ -782,8 +760,6 @@ cvmx_pko_send_packet_finish_pkoid(int pko_port, uint64_t queue,
 				  cvmx_pko_lock_t use_locking)
 {
 	cvmx_cmd_queue_result_t result;
-	if (use_locking == CVMX_PKO_LOCK_ATOMIC_TAG)
-		cvmx_pow_tag_sw_wait();
 	result = cvmx_cmd_queue_write2(CVMX_CMD_QUEUE_PKO(queue),
 				       (use_locking == CVMX_PKO_LOCK_CMD_QUEUE),
 				       pko_command.u64, packet.u64);
@@ -823,8 +799,6 @@ cvmx_pko_send_packet_finish3_pkoid(uint64_t pko_port, uint64_t queue,
 				   cvmx_pko_lock_t use_locking)
 {
 	cvmx_cmd_queue_result_t result;
-	if (use_locking == CVMX_PKO_LOCK_ATOMIC_TAG)
-		cvmx_pow_tag_sw_wait();
 	result = cvmx_cmd_queue_write3(CVMX_CMD_QUEUE_PKO(queue),
 				       (use_locking == CVMX_PKO_LOCK_CMD_QUEUE),
 				       pko_command.u64, packet.u64, addr);
diff --git a/arch/mips/include/asm/octeon/cvmx-pow.h b/arch/mips/include/asm/octeon/cvmx-pow.h
index 999aefe..e2134d8 100644
--- a/arch/mips/include/asm/octeon/cvmx-pow.h
+++ b/arch/mips/include/asm/octeon/cvmx-pow.h
@@ -1,10 +1,10 @@
 /***********************license start***************
- * Author: Cavium Networks
+ * Author: Cavium Inc.
  *
- * Contact: support@caviumnetworks.com
+ * Contact: support@cavium.com
  * This file is part of the OCTEON SDK
  *
- * Copyright (c) 2003-2008 Cavium Networks
+ * Copyright (c) 2003-2011 Cavium Inc.
  *
  * This file is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License, Version 2, as
@@ -22,77 +22,48 @@
  * or visit http://www.gnu.org/licenses/.
  *
  * This file may also be available under a different license from Cavium.
- * Contact Cavium Networks for more information
+ * Contact Cavium Inc. for more information
  ***********************license end**************************************/
 
-/**
- * Interface to the hardware Packet Order / Work unit.
- *
- * New, starting with SDK 1.7.0, cvmx-pow supports a number of
- * extended consistency checks. The define
- * CVMX_ENABLE_POW_CHECKS controls the runtime insertion of POW
- * internal state checks to find common programming errors. If
- * CVMX_ENABLE_POW_CHECKS is not defined, checks are by default
- * enabled. For example, cvmx-pow will check for the following
- * program errors or POW state inconsistency.
- * - Requesting a POW operation with an active tag switch in
- *   progress.
- * - Waiting for a tag switch to complete for an excessively
- *   long period. This is normally a sign of an error in locking
- *   causing deadlock.
- * - Illegal tag switches from NULL_NULL.
- * - Illegal tag switches from NULL.
- * - Illegal deschedule request.
- * - WQE pointer not matching the one attached to the core by
- *   the POW.
- *
- */
-
 #ifndef __CVMX_POW_H__
 #define __CVMX_POW_H__
 
-#include <asm/octeon/cvmx-pow-defs.h>
-
 #include "cvmx-scratch.h"
 #include "cvmx-wqe.h"
 
-/* Default to having all POW constancy checks turned on */
-#ifndef CVMX_ENABLE_POW_CHECKS
-#define CVMX_ENABLE_POW_CHECKS 1
-#endif
+/**
+ * Wait flag values for pow functions.
+ */
+enum cvmx_pow_wait {
+	CVMX_POW_WAIT = 1,
+	CVMX_POW_NO_WAIT = 0,
+};
 
 enum cvmx_pow_tag_type {
 	/* Tag ordering is maintained */
-	CVMX_POW_TAG_TYPE_ORDERED   = 0L,
+	CVMX_POW_TAG_TYPE_ORDERED = 0L,
 	/* Tag ordering is maintained, and at most one PP has the tag */
-	CVMX_POW_TAG_TYPE_ATOMIC    = 1L,
+	CVMX_POW_TAG_TYPE_ATOMIC = 1L,
 	/*
-	 * The work queue entry from the order - NEVER tag switch from
-	 * NULL to NULL
+	 * The work queue entry from the order
+	 * - NEVER tag switch from NULL to NULL
 	 */
-	CVMX_POW_TAG_TYPE_NULL      = 2L,
-	/* A tag switch to NULL, and there is no space reserved in POW
+	CVMX_POW_TAG_TYPE_NULL = 2L,
+	/*
+	 * A tag switch to NULL, and there is no space reserved in POW
 	 * - NEVER tag switch to NULL_NULL
 	 * - NEVER tag switch from NULL_NULL
 	 * - NULL_NULL is entered at the beginning of time and on a deschedule.
-	 * - NULL_NULL can be exited by a new work request. A NULL_SWITCH
-	 * load can also switch the state to NULL
+	 * - NULL_NULL can be exited by a new work request.
+	 *    A NULL_SWITCH load can also switch the state to NULL
 	 */
 	CVMX_POW_TAG_TYPE_NULL_NULL = 3L
 };
 
 /**
- * Wait flag values for pow functions.
- */
-typedef enum {
-	CVMX_POW_WAIT = 1,
-	CVMX_POW_NO_WAIT = 0,
-} cvmx_pow_wait_t;
-
-/**
  *  POW tag operations.  These are used in the data stored to the POW.
  */
-typedef enum {
+enum cvmx_pow_tag_op {
 	/*
 	 * switch the tag (only) for this PP
 	 * - the previous tag should be non-NULL in this case
@@ -100,6 +71,7 @@ typedef enum {
 	 * - fields used: op, type, tag
 	 */
 	CVMX_POW_TAG_OP_SWTAG = 0L,
+
 	/*
 	 * switch the tag for this PP, with full information
 	 * - this should be used when the previous tag is NULL
@@ -107,27 +79,32 @@ typedef enum {
 	 * - fields used: address, op, grp, type, tag
 	 */
 	CVMX_POW_TAG_OP_SWTAG_FULL = 1L,
+
 	/*
 	 * switch the tag (and/or group) for this PP and de-schedule
 	 * - OK to keep the tag the same and only change the group
 	 * - fields used: op, no_sched, grp, type, tag
 	 */
 	CVMX_POW_TAG_OP_SWTAG_DESCH = 2L,
+
 	/*
 	 * just de-schedule
 	 * - fields used: op, no_sched
 	 */
 	CVMX_POW_TAG_OP_DESCH = 3L,
+
 	/*
 	 * create an entirely new work queue entry
 	 * - fields used: address, op, qos, grp, type, tag
 	 */
 	CVMX_POW_TAG_OP_ADDWQ = 4L,
+
 	/*
 	 * just update the work queue pointer and grp for this PP
 	 * - fields used: address, op, grp
 	 */
 	CVMX_POW_TAG_OP_UPDATE_WQP_GRP = 5L,
+
 	/*
 	 * set the no_sched bit on the de-schedule list
 	 *
@@ -139,15 +116,16 @@ typedef enum {
 	 *
 	 * - fields used: address, index, op
 	 *
-	 *  Before issuing a *_NSCHED operation, SW must guarantee
-	 *  that all prior deschedules and set/clr NSCHED operations
-	 *  are complete and all prior switches are complete. The
-	 *  hardware provides the opsdone bit and swdone bit for SW
-	 *  polling. After issuing a *_NSCHED operation, SW must
-	 *  guarantee that the set/clr NSCHED is complete before any
-	 *  subsequent operations.
+	 * Before issuing a *_NSCHED operation, SW must guarantee that
+	 * all prior deschedules and set/clr NSCHED operations are
+	 * complete and all prior switches are complete. The hardware
+	 * provides the opsdone bit and swdone bit for SW
+	 * polling. After issuing a *_NSCHED operation, SW must
+	 * guarantee that the set/clr NSCHED is complete before any
+	 * subsequent operations.
 	 */
 	CVMX_POW_TAG_OP_SET_NSCHED = 6L,
+
 	/*
 	 * clears the no_sched bit on the de-schedule list
 	 *
@@ -168,16 +146,18 @@ typedef enum {
 	 * subsequent operations.
 	 */
 	CVMX_POW_TAG_OP_CLR_NSCHED = 7L,
+
 	/* do nothing */
 	CVMX_POW_TAG_OP_NOP = 15L
-} cvmx_pow_tag_op_t;
+};
 
 /**
  * This structure defines the store data on a store to POW
  */
-typedef union {
+union cvmx_pow_tag_req {
 	uint64_t u64;
 	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
 		/*
 		 * Don't reschedule this entry. no_sched is used for
 		 * CVMX_POW_TAG_OP_SWTAG_DESCH and
@@ -185,121 +165,209 @@ typedef union {
 		 */
 		uint64_t no_sched:1;
 		uint64_t unused:2;
-		/* Tontains index of entry for a CVMX_POW_TAG_OP_*_NSCHED */
+		/* Dontains index of entry for a CVMX_POW_TAG_OP_*_NSCHED */
 		uint64_t index:13;
 		/* The operation to perform */
-		cvmx_pow_tag_op_t op:4;
+		uint64_t op:4;
 		uint64_t unused2:2;
 		/*
 		 * The QOS level for the packet. qos is only used for
 		 * CVMX_POW_TAG_OP_ADDWQ
 		 */
 		uint64_t qos:3;
-		/*
-		 * The group that the work queue entry will be
-		 * scheduled to grp is used for CVMX_POW_TAG_OP_ADDWQ,
-		 * CVMX_POW_TAG_OP_SWTAG_FULL,
-		 * CVMX_POW_TAG_OP_SWTAG_DESCH, and
-		 * CVMX_POW_TAG_OP_UPDATE_WQP_GRP
-		 */
+		/* The group that the work queue entry will be scheduled to */
 		uint64_t grp:4;
-		/*
-		 * The type of the tag. type is used for everything
-		 * except CVMX_POW_TAG_OP_DESCH,
-		 * CVMX_POW_TAG_OP_UPDATE_WQP_GRP, and
-		 * CVMX_POW_TAG_OP_*_NSCHED
-		 */
 		uint64_t type:3;
-		/*
-		 * The actual tag. tag is used for everything except
-		 * CVMX_POW_TAG_OP_DESCH,
-		 * CVMX_POW_TAG_OP_UPDATE_WQP_GRP, and
-		 * CVMX_POW_TAG_OP_*_NSCHED
-		 */
 		uint64_t tag:32;
-	} s;
-} cvmx_pow_tag_req_t;
+#else
+		uint64_t tag:32;
+		uint64_t type:3;
+		uint64_t grp:4;
+		uint64_t qos:3;
+		uint64_t unused2:2;
+		uint64_t op:4;
+		uint64_t index:13;
+		uint64_t unused:2;
+		uint64_t no_sched:1;
+#endif
+	} s_cn38xx;
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t no_sched:1;
+		uint64_t op:4;
+		uint64_t unused1:4;
+		uint64_t index:11;
+		uint64_t unused2:1;
+		uint64_t grp:6;
+		uint64_t unused3:3;
+		uint64_t type:2;
+		uint64_t tag:32;
+#else
+		uint64_t tag:32;
+		uint64_t type:2;
+		uint64_t unused3:3;
+		uint64_t grp:6;
+		uint64_t unused2:1;
+		uint64_t index:11;
+		uint64_t unused1:4;
+		uint64_t op:4;
+		uint64_t no_sched:1;
+#endif
+	} s_cn68xx_clr;
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t no_sched:1;
+		uint64_t op:4;
+		uint64_t unused1:12;
+		uint64_t qos:3;
+		uint64_t unused2:1;
+		uint64_t grp:6;
+		uint64_t unused3:3;
+		uint64_t type:2;
+		uint64_t tag:32;
+#else
+		uint64_t tag:32;
+		uint64_t type:2;
+		uint64_t unused3:3;
+		uint64_t grp:6;
+		uint64_t unused2:1;
+		uint64_t qos:3;
+		uint64_t unused1:12;
+		uint64_t op:4;
+		uint64_t no_sched:1;
+#endif
+	} s_cn68xx_add;
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t no_sched:1;
+		uint64_t op:4;
+		uint64_t unused1:16;
+		uint64_t grp:6;
+		uint64_t unused3:3;
+		uint64_t type:2;
+		uint64_t tag:32;
+#else
+		uint64_t tag:32;
+		uint64_t type:2;
+		uint64_t unused3:3;
+		uint64_t grp:6;
+		uint64_t unused1:16;
+		uint64_t op:4;
+		uint64_t no_sched:1;
+#endif
+	} s_cn68xx_other;
+
+};
+
+struct cvmx_pow_tag_info {
+	uint32_t tag;
+	uint16_t index;
+	uint8_t grp;
+	uint8_t tag_type;
+};
 
 /**
  * This structure describes the address to load stuff from POW
  */
-typedef union {
+union cvmx_pow_load_addr {
 	uint64_t u64;
 
     /**
      * Address for new work request loads (did<2:0> == 0)
      */
 	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
 		/* Mips64 address region. Should be CVMX_IO_SEG */
 		uint64_t mem_region:2;
-		/* Must be zero */
-		uint64_t reserved_49_61:13;
-		/* Must be one */
-		uint64_t is_io:1;
-		/* the ID of POW -- did<2:0> == 0 in this case */
-		uint64_t did:8;
-		/* Must be zero */
-		uint64_t reserved_4_39:36;
-		/*
-		 * If set, don't return load response until work is
-		 * available.
-		 */
-		uint64_t wait:1;
-		/* Must be zero */
+		uint64_t reserved_49_61:13; /* Must be zero */
+		uint64_t is_io:1;	    /* Must be one */
+		uint64_t did:8;		    /* the ID of POW -- did<2:0> == 0 in this case */
+		uint64_t reserved_4_39:36;  /* Must be zero */
+		uint64_t wait:1;	    /* If set, don't return load response until work is available */
+		uint64_t reserved_0_2:3;    /* Must be zero */
+#else
 		uint64_t reserved_0_2:3;
+		uint64_t wait:1;
+		uint64_t reserved_4_39:36;
+		uint64_t did:8;
+		uint64_t is_io:1;
+		uint64_t reserved_49_61:13;
+		uint64_t mem_region:2;
+#endif
 	} swork;
 
     /**
      * Address for loads to get POW internal status
      */
 	struct {
-		/* Mips64 address region. Should be CVMX_IO_SEG */
+#ifdef __BIG_ENDIAN_BITFIELD
 		uint64_t mem_region:2;
-		/* Must be zero */
 		uint64_t reserved_49_61:13;
-		/* Must be one */
 		uint64_t is_io:1;
-		/* the ID of POW -- did<2:0> == 1 in this case */
-		uint64_t did:8;
-		/* Must be zero */
+		uint64_t did:8;	    /* the ID of POW -- did<2:0> == 1 in this case */
 		uint64_t reserved_10_39:30;
-		/* The core id to get status for */
-		uint64_t coreid:4;
+		uint64_t coreid:4;  /* The core id to get status for */
 		/*
 		 * If set and get_cur is set, return reverse tag-list
-		 * pointer rather than forward tag-list pointer.
+		 * pointer rather than forward tag-list pointer
 		 */
 		uint64_t get_rev:1;
-		/*
-		 * If set, return current status rather than pending
-		 * status.
-		 */
-		uint64_t get_cur:1;
-		/*
-		 * If set, get the work-queue pointer rather than
-		 * tag/type.
-		 */
-		uint64_t get_wqp:1;
-		/* Must be zero */
+		uint64_t get_cur:1; /* If set, return current status rather than pending status */
+		uint64_t get_wqp:1; /* If set, get the work-queue pointer rather than tag/type */
+		uint64_t reserved_0_2:3; /* Must be zero */
+#else
 		uint64_t reserved_0_2:3;
+		uint64_t get_wqp:1;
+		uint64_t get_cur:1;
+		uint64_t get_rev:1;
+		uint64_t coreid:4;
+		uint64_t reserved_10_39:30;
+		uint64_t did:8;
+		uint64_t is_io:1;
+		uint64_t reserved_49_61:13;
+		uint64_t mem_region:2;
+#endif
 	} sstatus;
 
     /**
-     * Address for memory loads to get POW internal state
+     * Address for loads to get 68XX SS0 internal status
      */
 	struct {
-		/* Mips64 address region. Should be CVMX_IO_SEG */
+#ifdef __BIG_ENDIAN_BITFIELD
 		uint64_t mem_region:2;
-		/* Must be zero */
 		uint64_t reserved_49_61:13;
-		/* Must be one */
 		uint64_t is_io:1;
-		/* the ID of POW -- did<2:0> == 2 in this case */
+		uint64_t did:8; /* the ID of POW -- did<2:0> == 1 in this case */
+		uint64_t reserved_14_39:26;
+		uint64_t coreid:5; /* The core id to get status for */
+		uint64_t reserved_6_8:3;
+		uint64_t opcode:3; /* Status operation */
+		uint64_t reserved_0_2:3;
+#else
+		uint64_t reserved_0_2:3;
+		uint64_t opcode:3;
+		uint64_t reserved_6_8:3;
+		uint64_t coreid:5;
+		uint64_t reserved_14_39:26;
 		uint64_t did:8;
-		/* Must be zero */
+		uint64_t is_io:1;
+		uint64_t reserved_49_61:13;
+		uint64_t mem_region:2;
+#endif
+	} sstatus_cn68xx;
+
+    /**
+     * Address for memory loads to get POW internal state
+     */
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t mem_region:2;
+		uint64_t reserved_49_61:13;
+		uint64_t is_io:1;
+		uint64_t did:8;	/* the ID of POW -- did<2:0> == 2 in this case */
 		uint64_t reserved_16_39:24;
-		/* POW memory index */
-		uint64_t index:11;
+		uint64_t index:11;  /* POW memory index */
+
 		/*
 		 * If set, return deschedule information rather than
 		 * the standard response for work-queue index (invalid
@@ -312,23 +380,56 @@ typedef union {
 		 * tag/type (no effect when get_des set).
 		 */
 		uint64_t get_wqp:1;
-		/* Must be zero */
 		uint64_t reserved_0_2:3;
+#else
+		uint64_t reserved_0_2:3;
+		uint64_t get_wqp:1;
+		uint64_t get_des:1;
+		uint64_t index:11;
+		uint64_t reserved_16_39:24;
+		uint64_t did:8;
+		uint64_t is_io:1;
+		uint64_t reserved_49_61:13;
+		uint64_t mem_region:2;
+#endif
 	} smemload;
 
     /**
-     * Address for index/pointer loads
+     * Address for memory loads to get SSO internal state
      */
 	struct {
-		/* Mips64 address region. Should be CVMX_IO_SEG */
+#ifdef __BIG_ENDIAN_BITFIELD
 		uint64_t mem_region:2;
-		/* Must be zero */
 		uint64_t reserved_49_61:13;
-		/* Must be one */
 		uint64_t is_io:1;
-		/* the ID of POW -- did<2:0> == 3 in this case */
+		uint64_t did:8;	/* the ID of SSO - did<2:0> == 2 in this case */
+		uint64_t reserved_20_39:20;
+		uint64_t index:11; /* SSO memory index */
+		uint64_t reserved_6_8:3;
+		uint64_t opcode:3; /* Read TAG/WQ pointer/pending tag/next potr */
+		uint64_t reserved_0_2:3;
+#else
+		uint64_t reserved_0_2:3;
+		uint64_t opcode:3;
+		uint64_t reserved_3_5:3;
+		uint64_t index:11;
+		uint64_t reserved_20_39:20;
 		uint64_t did:8;
-		/* Must be zero */
+		uint64_t is_io:1;
+		uint64_t reserved_49_61:13;
+		uint64_t mem_region:2;
+#endif
+	} smemload_cn68xx;
+
+    /**
+     * Address for index/pointer loads
+     */
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t mem_region:2;
+		uint64_t reserved_49_61:13;
+		uint64_t is_io:1;
+		uint64_t did:8;  /* the ID of POW -- did<2:0> == 3 in this case */
 		uint64_t reserved_9_39:31;
 		/*
 		 * when {get_rmt ==0 AND get_des_get_tail == 0}, this
@@ -341,14 +442,14 @@ typedef union {
 		 * lists.  The two memory-input queue lists associated
 		 * with each QOS level are:
 		 *
-		 * - qosgrp = 0, qosgrp = 8:      QOS0
-		 * - qosgrp = 1, qosgrp = 9:      QOS1
-		 * - qosgrp = 2, qosgrp = 10:     QOS2
-		 * - qosgrp = 3, qosgrp = 11:     QOS3
-		 * - qosgrp = 4, qosgrp = 12:     QOS4
-		 * - qosgrp = 5, qosgrp = 13:     QOS5
-		 * - qosgrp = 6, qosgrp = 14:     QOS6
-		 * - qosgrp = 7, qosgrp = 15:     QOS7
+		 *  - qosgrp = 0, qosgrp = 8:      QOS0
+		 *  - qosgrp = 1, qosgrp = 9:      QOS1
+		 *  - qosgrp = 2, qosgrp = 10:     QOS2
+		 *  - qosgrp = 3, qosgrp = 11:     QOS3
+		 *  - qosgrp = 4, qosgrp = 12:     QOS4
+		 *  - qosgrp = 5, qosgrp = 13:     QOS5
+		 *  - qosgrp = 6, qosgrp = 14:     QOS6
+		 *  - qosgrp = 7, qosgrp = 15:     QOS7
 		 */
 		uint64_t qosgrp:4;
 		/*
@@ -364,124 +465,213 @@ typedef union {
 		 * local indexes for the specified qos level.
 		 */
 		uint64_t get_rmt:1;
-		/* Must be zero */
 		uint64_t reserved_0_2:3;
+#else
+		uint64_t reserved_0_2:3;
+		uint64_t get_rmt:1;
+		uint64_t get_des_get_tail:1;
+		uint64_t qosgrp:4;
+		uint64_t reserved_9_39:31;
+		uint64_t did:8;
+		uint64_t is_io:1;
+		uint64_t reserved_49_61:13;
+		uint64_t mem_region:2;
+#endif
 	} sindexload;
 
     /**
-     * address for NULL_RD request (did<2:0> == 4) when this is read,
-     * HW attempts to change the state to NULL if it is NULL_NULL (the
-     * hardware cannot switch from NULL_NULL to NULL if a POW entry is
-     * not available - software may need to recover by finishing
-     * another piece of work before a POW entry can ever become
-     * available.)
+     * Address for a Index/Pointer loads to get SSO internal state
      */
 	struct {
-		/* Mips64 address region. Should be CVMX_IO_SEG */
+#ifdef __BIG_ENDIAN_BITFIELD
 		uint64_t mem_region:2;
-		/* Must be zero */
 		uint64_t reserved_49_61:13;
-		/* Must be one */
 		uint64_t is_io:1;
-		/* the ID of POW -- did<2:0> == 4 in this case */
+		uint64_t did:8;	/* the ID of SSO - did<2:0> == 2 in this case */
+		uint64_t reserved_15_39:25;
+		/*
+		 * When opcode = IPL_IQ, this field specifies IQ (or
+		 * QOS).  When opcode = IPL_DESCHED, this field
+		 * specifies the group.  This field is reserved for
+		 * all other opcodes.
+		 */
+		uint64_t qos_grp:6;
+		uint64_t reserved_6_8:3;
+		uint64_t opcode:3;  /* Read TAG/WQ pointer/pending tag/next potr */
+		uint64_t reserved_0_2:3;
+#else
+		uint64_t reserved_0_2:3;
+		uint64_t opcode:3;
+		uint64_t reserved_3_5:3;
+		uint64_t qos_grp:6;
+		uint64_t reserved_15_39:25;
 		uint64_t did:8;
-		/* Must be zero */
+		uint64_t is_io:1;
+		uint64_t reserved_49_61:13;
+		uint64_t mem_region:2;
+#endif
+	} sindexload_cn68xx;
+
+    /**
+     * address for NULL_RD request (did<2:0> == 4)
+     * when this is read, HW attempts to change the state to NULL if it is NULL_NULL
+     * (the hardware cannot switch from NULL_NULL to NULL if a POW entry is not available -
+     * software may need to recover by finishing another piece of work before a POW
+     * entry can ever become available.)
+     */
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t mem_region:2;
+		uint64_t reserved_49_61:13;
+		uint64_t is_io:1;
+		uint64_t did:8;	/* the ID of POW -- did<2:0> == 4 in this case */
+		uint64_t reserved_0_39:40;
+#else
 		uint64_t reserved_0_39:40;
+		uint64_t did:8;
+		uint64_t is_io:1;
+		uint64_t reserved_49_61:13;
+		uint64_t mem_region:2;
+#endif
 	} snull_rd;
-} cvmx_pow_load_addr_t;
+};
 
 /**
- * This structure defines the response to a load/SENDSINGLE to POW
- * (except CSR reads)
+ * This structure defines the response to a load/SENDSINGLE to POW (except CSR reads)
  */
-typedef union {
+union cvmx_pow_tag_load_resp {
 	uint64_t u64;
 
     /**
      * Response to new work request loads
      */
 	struct {
-		/*
-		 * Set when no new work queue entry was returned.  *
-		 * If there was de-scheduled work, the HW will
-		 * definitely return it. When this bit is set, it
-		 * could mean either mean:
-		 *
-		 * - There was no work, or
-		 *
-		 * - There was no work that the HW could find. This
-		 *   case can happen, regardless of the wait bit value
-		 *   in the original request, when there is work in
-		 *   the IQ's that is too deep down the list.
-		 */
+#ifdef __BIG_ENDIAN_BITFIELD
 		uint64_t no_work:1;
-		/* Must be zero */
 		uint64_t reserved_40_62:23;
-		/* 36 in O1 -- the work queue pointer */
 		uint64_t addr:40;
+#else
+		uint64_t addr:40;
+		uint64_t reserved_40_62:23;
+		uint64_t no_work:1;
+#endif
 	} s_work;
 
     /**
      * Result for a POW Status Load (when get_cur==0 and get_wqp==0)
      */
 	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
 		uint64_t reserved_62_63:2;
-		/* Set when there is a pending non-NULL SWTAG or
-		 * SWTAG_FULL, and the POW entry has not left the list
-		 * for the original tag. */
+		/*
+		 * Set when there is a pending non-NULL SWTAG or
+		 * SWTAG_FULL, and the POW entry has not left the list for the original tag.
+		 */
 		uint64_t pend_switch:1;
 		/* Set when SWTAG_FULL and pend_switch is set. */
 		uint64_t pend_switch_full:1;
-		/*
-		 * Set when there is a pending NULL SWTAG, or an
-		 * implicit switch to NULL.
-		 */
+		/* Set when there is a pending NULL SWTAG, or an implicit switch to NULL. */
 		uint64_t pend_switch_null:1;
 		/* Set when there is a pending DESCHED or SWTAG_DESCHED. */
 		uint64_t pend_desched:1;
-		/*
-		 * Set when there is a pending SWTAG_DESCHED and
-		 * pend_desched is set.
-		 */
+		/* Set when there is a pending SWTAG_DESCHED and pend_desched is set. */
 		uint64_t pend_desched_switch:1;
 		/* Set when nosched is desired and pend_desched is set. */
 		uint64_t pend_nosched:1;
 		/* Set when there is a pending GET_WORK. */
 		uint64_t pend_new_work:1;
-		/*
-		 * When pend_new_work is set, this bit indicates that
-		 * the wait bit was set.
-		 */
+		/* When pend_new_work is set, this bit indicates that the wait bit was set. */
 		uint64_t pend_new_work_wait:1;
 		/* Set when there is a pending NULL_RD. */
 		uint64_t pend_null_rd:1;
 		/* Set when there is a pending CLR_NSCHED. */
 		uint64_t pend_nosched_clr:1;
 		uint64_t reserved_51:1;
-		/* This is the index when pend_nosched_clr is set. */
+		/* The index when pend_nosched_clr is set. */
 		uint64_t pend_index:11;
-		/*
-		 * This is the new_grp when (pend_desched AND
-		 * pend_desched_switch) is set.
-		 */
+		/* The new_grp when (pend_desched AND pend_desched_switch) is set. */
 		uint64_t pend_grp:4;
 		uint64_t reserved_34_35:2;
+		/* The tag type when pend_switch or (pend_desched AND pend_desched_switch) are set. */
+		uint64_t pend_type:2;
+		/* The tag when pend_switch or (pend_desched AND pend_desched_switch) are set. */
+		uint64_t pend_tag:32;
+#else
+		uint64_t pend_tag:32;
+		uint64_t pend_type:2;
+		uint64_t reserved_34_35:2;
+		uint64_t pend_grp:4;
+		uint64_t pend_index:11;
+		uint64_t reserved_51:1;
+		uint64_t pend_nosched_clr:1;
+		uint64_t pend_null_rd:1;
+		uint64_t pend_new_work_wait:1;
+		uint64_t pend_new_work:1;
+		uint64_t pend_nosched:1;
+		uint64_t pend_desched_switch:1;
+		uint64_t pend_desched:1;
+		uint64_t pend_switch_null:1;
+		uint64_t pend_switch_full:1;
+		uint64_t pend_switch:1;
+		uint64_t reserved_62_63:2;
+#endif
+	} s_sstatus0;
+
+    /**
+     * Result for a SSO Status Load (when opcode is SL_PENDTAG)
+     */
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
 		/*
-		 * This is the tag type when pend_switch or
-		 * (pend_desched AND pend_desched_switch) are set.
+		 * Set when there is a pending non-UNSCHEDULED SWTAG
+		 * or SWTAG_FULL, and the SSO entry has not left the
+		 * list for the original tag.
 		 */
-		uint64_t pend_type:2;
+		uint64_t pend_switch:1;
+		/* Set when there is a pending GET_WORK */
+		uint64_t pend_get_work:1;
 		/*
-		 * - this is the tag when pend_switch or (pend_desched
-		 *    AND pend_desched_switch) are set.
+		 * when pend_get_work is set, this biit indicates that the
+		 * wait bit was set.
 		 */
+		uint64_t pend_get_work_wait:1;
+		/* Set when nosched is desired and pend_desched is set. */
+		uint64_t pend_nosched:1;
+		/* Set when there is a pending CLR_NSCHED. */
+		uint64_t pend_nosched_clr:1;
+		/* Set when there is a pending DESCHED or SWTAG_DESCHED. */
+		uint64_t pend_desched:1;
+		/* Set when there is a pending ALLOC_WE. */
+		uint64_t pend_alloc_we:1;
+		uint64_t reserved_48_56:9;
+		/* The index when pend_nosched_clr is set. */
+		uint64_t pend_index:11;
+		uint64_t reserved_34_36:3;
+		/* The tag type when pend_switch is set. */
+		uint64_t pend_type:2;
+		/* The tag when pend_switch is set. */
 		uint64_t pend_tag:32;
-	} s_sstatus0;
+#else
+		uint64_t pend_tag:32;
+		uint64_t pend_type:2;
+		uint64_t reserved_34_36:3;
+		uint64_t pend_index:11;
+		uint64_t reserved_48_56:9;
+		uint64_t pend_alloc_we:1;
+		uint64_t pend_desched:1;
+		uint64_t pend_nosched_clr:1;
+		uint64_t pend_nosched:1;
+		uint64_t pend_get_work_wait:1;
+		uint64_t pend_get_work:1;
+		uint64_t pend_switch:1;
+#endif
+	} s_sstatus0_cn68xx;
 
     /**
      * Result for a POW Status Load (when get_cur==0 and get_wqp==1)
      */
 	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
 		uint64_t reserved_62_63:2;
 		/*
 		 * Set when there is a pending non-NULL SWTAG or
@@ -491,51 +681,100 @@ typedef union {
 		uint64_t pend_switch:1;
 		/* Set when SWTAG_FULL and pend_switch is set. */
 		uint64_t pend_switch_full:1;
-		/*
-		 * Set when there is a pending NULL SWTAG, or an
-		 * implicit switch to NULL.
-		 */
+		/**< Set when there is a pending NULL SWTAG, or an implicit switch to NUL */
 		uint64_t pend_switch_null:1;
-		/*
-		 * Set when there is a pending DESCHED or
-		 * SWTAG_DESCHED.
-		 */
+		/* Set when there is a pending DESCHED or SWTAG_DESCHED. */
 		uint64_t pend_desched:1;
-		/*
-		 * Set when there is a pending SWTAG_DESCHED and
-		 * pend_desched is set.
-		 */
+		/* Set when there is a pending SWTAG_DESCHED and pend_desched is set. */
 		uint64_t pend_desched_switch:1;
 		/* Set when nosched is desired and pend_desched is set. */
 		uint64_t pend_nosched:1;
 		/* Set when there is a pending GET_WORK. */
 		uint64_t pend_new_work:1;
-		/*
-		 * When pend_new_work is set, this bit indicates that
-		 * the wait bit was set.
-		 */
+		/**< When pend_new_work is set, this bit indicates that the wait bit was t. */
 		uint64_t pend_new_work_wait:1;
 		/* Set when there is a pending NULL_RD. */
 		uint64_t pend_null_rd:1;
 		/* Set when there is a pending CLR_NSCHED. */
 		uint64_t pend_nosched_clr:1;
 		uint64_t reserved_51:1;
-		/* This is the index when pend_nosched_clr is set. */
+		/* The index when pend_nosched_clr is set. */
 		uint64_t pend_index:11;
-		/*
-		 * This is the new_grp when (pend_desched AND
-		 * pend_desched_switch) is set.
-		 */
+		/* The new_grp when (pend_desched AND pend_desched_switch) is set. */
 		uint64_t pend_grp:4;
-		/* This is the wqp when pend_nosched_clr is set. */
+		/* The wqp when pend_nosched_clr is set. */
 		uint64_t pend_wqp:36;
+#else
+		uint64_t pend_wqp:36;
+		uint64_t pend_grp:4;
+		uint64_t pend_index:11;
+		uint64_t reserved_51:1;
+		uint64_t pend_nosched_clr:1;
+		uint64_t pend_null_rd:1;
+		uint64_t pend_new_work_wait:1;
+		uint64_t pend_new_work:1;
+		uint64_t pend_nosched:1;
+		uint64_t pend_desched_switch:1;
+		uint64_t pend_desched:1;
+		uint64_t pend_switch_null:1;
+		uint64_t pend_switch_full:1;
+		uint64_t pend_switch:1;
+		uint64_t reserved_62_63:2;
+#endif
 	} s_sstatus1;
 
     /**
-     * Result for a POW Status Load (when get_cur==1, get_wqp==0, and
-     * get_rev==0)
+     * Result for a SSO Status Load (when opcode is SL_PENDWQP)
+     */
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		/*
+		 * Set when there is a pending non-UNSCHEDULED SWTAG
+		 * or SWTAG_FULL, and the SSO entry has not left the
+		 * list for the original tag.
+		 */
+		uint64_t pend_switch:1;
+		/* Set when there is a pending GET_WORK */
+		uint64_t pend_get_work:1;
+		/*
+		 * when pend_get_work is set, this biit indicates that
+		 * the wait bit was set.
+		 */
+		uint64_t pend_get_work_wait:1;
+		/* Set when nosched is desired and pend_desched is set. */
+		uint64_t pend_nosched:1;
+		/* Set when there is a pending CLR_NSCHED. */
+		uint64_t pend_nosched_clr:1;
+		/* Set when there is a pending DESCHED or SWTAG_DESCHED. */
+		uint64_t pend_desched:1;
+		/* Set when there is a pending ALLOC_WE. */
+		uint64_t pend_alloc_we:1;
+		uint64_t reserved_51_56:6;
+		/* The index when pend_nosched_clr is set. */
+		uint64_t pend_index:11;
+		uint64_t reserved_38_39:2;
+		/* The wqp when pend_nosched_clr is set. */
+		uint64_t pend_wqp:38;
+#else
+		uint64_t pend_wqp:38;
+		uint64_t reserved_38_39:2;
+		uint64_t pend_index:11;
+		uint64_t reserved_51_56:6;
+		uint64_t pend_alloc_we:1;
+		uint64_t pend_desched:1;
+		uint64_t pend_nosched_clr:1;
+		uint64_t pend_nosched:1;
+		uint64_t pend_get_work_wait:1;
+		uint64_t pend_get_work:1;
+		uint64_t pend_switch:1;
+#endif
+	} s_sstatus1_cn68xx;
+
+    /**
+     * Result for a POW Status Load (when get_cur==1, get_wqp==0, and get_rev==0)
      */
 	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
 		uint64_t reserved_62_63:2;
 		/*
 		 * Points to the next POW entry in the tag list when
@@ -544,10 +783,7 @@ typedef union {
 		uint64_t link_index:11;
 		/* The POW entry attached to the core. */
 		uint64_t index:11;
-		/*
-		 * The group attached to the core (updated when new
-		 * tag list entered on SWTAG_FULL).
-		 */
+		/* The group attached to the core (updated when new tag list entered on SWTAG_FULL). */
 		uint64_t grp:4;
 		/*
 		 * Set when this POW entry is at the head of its tag
@@ -573,12 +809,73 @@ typedef union {
 		 * SWTAG_DESCHED).
 		 */
 		uint64_t tag:32;
+#else
+		uint64_t tag:32;
+		uint64_t tag_type:2;
+		uint64_t tail:1;
+		uint64_t head:1;
+		uint64_t grp:4;
+		uint64_t index:11;
+		uint64_t link_index:11;
+		uint64_t reserved_62_63:2;
+#endif
 	} s_sstatus2;
 
     /**
+     * Result for a SSO Status Load (when opcode is SL_TAG)
+     */
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t reserved_57_63:7;
+		/* The SSO entry attached to the core. */
+		uint64_t index:11;
+		uint64_t reserved_45:1;
+		/*
+		 * The group attached to the core (updated when new
+		 * tag list entered on SWTAG_FULL).
+		 */
+		uint64_t grp:6;
+		/*
+		 * Set when this SSO entry is at the head of its tag
+		 * list (also set when in the UNSCHEDULED or EMPTY
+		 * state).
+		 */
+		uint64_t head:1;
+		/*
+		 * Set when this SSO entry is at the tail of its tag list (also set when in the
+		 * UNSCHEDULED or EMPTY state).
+		 */
+		uint64_t tail:1;
+		uint64_t reserved_34_36:3;
+		/*
+		 * The tag type attached to the core (updated when new tag list entered
+		 * on SWTAG, SWTAG_FULL, or SWTAG_DESCHED).
+		 */
+		uint64_t tag_type:2;
+		/*
+		 * The tag attached to the core (updated when new tag
+		 * list entered on SWTAG, SWTAG_FULL, or
+		 * SWTAG_DESCHED).
+		 */
+		uint64_t tag:32;
+#else
+		uint64_t tag:32;
+		uint64_t tag_type:2;
+		uint64_t reserved_34_36:3;
+		uint64_t tail:1;
+		uint64_t head:1;
+		uint64_t grp:6;
+		uint64_t reserved_45:1;
+		uint64_t index:11;
+		uint64_t reserved_57_63:7;
+#endif
+	} s_sstatus2_cn68xx;
+
+    /**
      * Result for a POW Status Load (when get_cur==1, get_wqp==0, and get_rev==1)
      */
 	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
 		uint64_t reserved_62_63:2;
 		/*
 		 * Points to the prior POW entry in the tag list when
@@ -589,12 +886,10 @@ typedef union {
 		uint64_t revlink_index:11;
 		/* The POW entry attached to the core. */
 		uint64_t index:11;
-		/*
-		 * The group attached to the core (updated when new
-		 * tag list entered on SWTAG_FULL).
-		 */
+		/* The group attached to the core (updated when new tag list entered on SWTAG_FULL). */
 		uint64_t grp:4;
-		/* Set when this POW entry is at the head of its tag
+		/*
+		 * Set when this POW entry is at the head of its tag
 		 * list (also set when in the NULL or NULL_NULL
 		 * state).
 		 */
@@ -617,13 +912,50 @@ typedef union {
 		 * SWTAG_DESCHED).
 		 */
 		uint64_t tag:32;
+#else
+		uint64_t tag:32;
+		uint64_t tag_type:2;
+		uint64_t tail:1;
+		uint64_t head:1;
+		uint64_t grp:4;
+		uint64_t index:11;
+		uint64_t revlink_index:11;
+		uint64_t reserved_62_63:2;
+#endif
 	} s_sstatus3;
 
     /**
-     * Result for a POW Status Load (when get_cur==1, get_wqp==1, and
-     * get_rev==0)
+     * Result for a SSO Status Load (when opcode is SL_WQP)
      */
 	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t reserved_58_63:6;
+		/* The SSO entry attached to the core. */
+		uint64_t index:11;
+		uint64_t reserved_46:1;
+		/*
+		 * The group attached to the core (updated when new
+		 * tag list entered on SWTAG_FULL).
+		 */
+		uint64_t grp:6;
+		uint64_t reserved_38_39:2;
+		/* The wqp attached to the core (updated when new tag list entered on SWTAG_FULL). */
+		uint64_t wqp:38;
+#else
+		uint64_t wqp:38;
+		uint64_t reserved_38_39:2;
+		uint64_t grp:6;
+		uint64_t reserved_46:1;
+		uint64_t index:11;
+		uint64_t reserved_58_63:6;
+#endif
+	} s_sstatus3_cn68xx;
+
+    /**
+     * Result for a POW Status Load (when get_cur==1, get_wqp==1, and get_rev==0)
+     */
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
 		uint64_t reserved_62_63:2;
 		/*
 		 * Points to the next POW entry in the tag list when
@@ -632,23 +964,78 @@ typedef union {
 		uint64_t link_index:11;
 		/* The POW entry attached to the core. */
 		uint64_t index:11;
+		/* The group attached to the core (updated when new tag list entered on SWTAG_FULL). */
+		uint64_t grp:4;
+		/* The wqp attached to the core (updated when new tag list entered on SWTAG_FULL). */
+		uint64_t wqp:36;
+#else
+		uint64_t wqp:36;
+		uint64_t grp:4;
+		uint64_t index:11;
+		uint64_t link_index:11;
+		uint64_t reserved_62_63:2;
+#endif
+	} s_sstatus4;
+
+    /**
+     * Result for a SSO Status Load (when opcode is SL_LINKS)
+     */
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t reserved_46_63:18;
+		/* The SSO entry attached to the core. */
+		uint64_t index:11;
+		uint64_t reserved_34:1;
 		/*
-		 * The group attached to the core (updated when new
-		 * tag list entered on SWTAG_FULL).
+		 * The group attached to the core (updated when new tag list entered on
+		 * SWTAG_FULL).
 		 */
-		uint64_t grp:4;
+		uint64_t grp:6;
 		/*
-		 * The wqp attached to the core (updated when new tag
-		 * list entered on SWTAG_FULL).
+		 * Set when this SSO entry is at the head of its tag
+		 * list (also set when in the UNSCHEDULED or EMPTY
+		 * state).
 		 */
-		uint64_t wqp:36;
-	} s_sstatus4;
+		uint64_t head:1;
+		/*
+		 * Set when this SSO entry is at the tail of its tag
+		 * list (also set when in the UNSCHEDULED or EMPTY
+		 * state).
+		 */
+		uint64_t tail:1;
+		uint64_t reserved_24_25:2;
+		/*
+		 * Points to the prior SSO entry in the tag list when
+		 * head==0 (and tag_type is not UNSCHEDULED or
+		 * EMPTY).
+		 */
+		uint64_t revlink_index:11;
+		uint64_t reserved_11_12:2;
+		/*
+		 * Points to the next SSO entry in the tag list when
+		 * tail==0 (and tag_type is not UNSCHEDULDED or
+		 * EMPTY).
+		 */
+		uint64_t link_index:11;
+#else
+		uint64_t link_index:11;
+		uint64_t reserved_11_12:2;
+		uint64_t revlink_index:11;
+		uint64_t reserved_24_25:2;
+		uint64_t tail:1;
+		uint64_t head:1;
+		uint64_t grp:6;
+		uint64_t reserved_34:1;
+		uint64_t index:11;
+		uint64_t reserved_46_63:18;
+#endif
+	} s_sstatus4_cn68xx;
 
     /**
-     * Result for a POW Status Load (when get_cur==1, get_wqp==1, and
-     * get_rev==1)
+     * Result for a POW Status Load (when get_cur==1, get_wqp==1, and get_rev==1)
      */
 	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
 		uint64_t reserved_62_63:2;
 		/*
 		 * Points to the prior POW entry in the tag list when
@@ -659,22 +1046,24 @@ typedef union {
 		uint64_t revlink_index:11;
 		/* The POW entry attached to the core. */
 		uint64_t index:11;
-		/*
-		 * The group attached to the core (updated when new
-		 * tag list entered on SWTAG_FULL).
-		 */
+		/* The group attached to the core (updated when new tag list entered on SWTAG_FULL). */
 		uint64_t grp:4;
-		/*
-		 * The wqp attached to the core (updated when new tag
-		 * list entered on SWTAG_FULL).
-		 */
+		/* The wqp attached to the core (updated when new tag list entered on SWTAG_FULL). */
+		uint64_t wqp:36;
+#else
 		uint64_t wqp:36;
+		uint64_t grp:4;
+		uint64_t index:11;
+		uint64_t revlink_index:11;
+		uint64_t reserved_62_63:2;
+#endif
 	} s_sstatus5;
 
     /**
      * Result For POW Memory Load (get_des == 0 and get_wqp == 0)
      */
 	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
 		uint64_t reserved_51_63:13;
 		/*
 		 * The next entry in the input, free, descheduled_head
@@ -695,130 +1084,243 @@ typedef union {
 		uint64_t tag_type:2;
 		/* The tag of the POW entry. */
 		uint64_t tag:32;
+#else
+		uint64_t tag:32;
+		uint64_t tag_type:2;
+		uint64_t tail:1;
+		uint64_t reserved_35:1;
+		uint64_t grp:4;
+		uint64_t next_index:11;
+		uint64_t reserved_51_63:13;
+#endif
 	} s_smemload0;
 
     /**
+     * Result For SSO Memory Load (opcode is ML_TAG)
+     */
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t reserved_38_63:26;
+		/*
+		 * Set when this SSO entry is at the tail of its tag
+		 * list (also set when in the NULL or NULL_NULL
+		 * state). */
+		uint64_t tail:1;
+		uint64_t reserved_34_36:3;
+		uint64_t tag_type:2; /* The tag type of the SSO entry. */
+		uint64_t tag:32;     /* The tag of the SSO entry. */
+#else
+		uint64_t tag:32;
+		uint64_t tag_type:2;
+		uint64_t reserved_34_36:3;
+		uint64_t tail:1;
+		uint64_t reserved_38_63:26;
+#endif
+	} s_smemload0_cn68xx;
+
+    /**
      * Result For POW Memory Load (get_des == 0 and get_wqp == 1)
      */
 	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
 		uint64_t reserved_51_63:13;
 		/*
-		 * The next entry in the input, free, descheduled_head
-		 * list (unpredictable if entry is the tail of the
-		 * list).
+		 * The next entry in the input, free, descheduled_head list
+		 * (unpredictable if entry is the tail of the list).
 		 */
 		uint64_t next_index:11;
-		/* The group of the POW entry. */
-		uint64_t grp:4;
-		/* The WQP held in the POW entry. */
+		uint64_t grp:4;  /* The group of the POW entry. */
+		uint64_t wqp:36; /* The WQP held in the POW entry. */
+#else
 		uint64_t wqp:36;
+		uint64_t grp:4;
+		uint64_t next_index:11;
+		uint64_t reserved_51_63:13;
+#endif
 	} s_smemload1;
 
     /**
+     * Result For SSO Memory Load (opcode is ML_WQPGRP)
+     */
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t reserved_48_63:16;
+		uint64_t nosched:1;
+		uint64_t reserved_46:1;
+		uint64_t grp:6;
+		uint64_t reserved_38_39:2;
+		uint64_t wqp:38;
+#else
+		uint64_t wqp:38;
+		uint64_t reserved_38_39:2;
+		uint64_t grp:6;
+		uint64_t reserved_46:1;
+		uint64_t nosched:1;
+		uint64_t reserved_51_63:16;
+#endif
+	} s_smemload1_cn68xx;
+
+    /**
      * Result For POW Memory Load (get_des == 1)
      */
 	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
 		uint64_t reserved_51_63:13;
-		/*
-		 * The next entry in the tag list connected to the
-		 * descheduled head.
-		 */
+		/* The next entry in the tag list connected to the descheduled head. */
 		uint64_t fwd_index:11;
-		/* The group of the POW entry. */
 		uint64_t grp:4;
-		/* The nosched bit for the POW entry. */
 		uint64_t nosched:1;
 		/* There is a pending tag switch */
 		uint64_t pend_switch:1;
+		/* The next tag type for the new tag list when pend_switch is set. */
+		uint64_t pend_type:2;
+		/* The next tag for the new tag list when pend_switch is set. */
+		uint64_t pend_tag:32;
+#else
+		uint64_t pend_tag:32;
+		uint64_t pend_type:2;
+		uint64_t pend_switch:1;
+		uint64_t nosched:1;
+		uint64_t grp:4;
+		uint64_t fwd_index:11;
+		uint64_t reserved_51_63:13;
+#endif
+	} s_smemload2;
+
+    /**
+     * Result For SSO Memory Load (opcode is ML_PENTAG)
+     */
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t reserved_38_63:26;
 		/*
-		 * The next tag type for the new tag list when
-		 * pend_switch is set.
+		 * Set when there is a pending non-UNSCHEDULED SWTAG
+		 * or SWTAG_FULL, and the SSO entry has not left the
+		 * list for the original tag.
 		 */
+		uint64_t pend_switch:1;
+		uint64_t reserved_34_36:3;
+		/* The next tag type for the new tag list when pend_switch is set. */
+		uint64_t pend_type:2;
+		/* The next tag for the new tag list when pend_switch is set. */
+		uint64_t pend_tag:32;
+#else
+		uint64_t pend_tag:32;
 		uint64_t pend_type:2;
+		uint64_t reserved_34_36:3;
+		uint64_t pend_switch:1;
+		uint64_t reserved_38_63:26;
+#endif
+	} s_smemload2_cn68xx;
+
+    /**
+     * Result For SSO Memory Load (opcode is ML_LINKS)
+     */
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t reserved_24_63:40;
+		/* The next entry in the tag list connected to the descheduled head. */
+		uint64_t fwd_index:11;
+		uint64_t reserved_11_12:2;
 		/*
-		 * The next tag for the new tag list when pend_switch
-		 * is set.
+		 * The next entry in the input, free, descheduled_head
+		 * list (unpredicatble if entry is the tail of the
+		 * list).
 		 */
-		uint64_t pend_tag:32;
-	} s_smemload2;
+		uint64_t next_index:11;
+#else
+		uint64_t next_index:11;
+		uint64_t reserved_11_12:2;
+		uint64_t fwd_index:11;
+		uint64_t reserved_24_63:40;
+#endif
+	} s_smemload3_cn68xx;
 
     /**
      * Result For POW Index/Pointer Load (get_rmt == 0/get_des_get_tail == 0)
      */
 	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
 		uint64_t reserved_52_63:12;
-		/*
-		 * set when there is one or more POW entries on the
-		 * free list.
-		 */
+		/* set when there is one or more POW entries on the free list. */
 		uint64_t free_val:1;
-		/*
-		 * set when there is exactly one POW entry on the free
-		 * list.
-		 */
+		/* set when there is exactly one POW entry on the free list. */
 		uint64_t free_one:1;
 		uint64_t reserved_49:1;
-		/*
-		 * when free_val is set, indicates the first entry on
-		 * the free list.
-		 */
+		/* when free_val is set, indicates the first entry on the free list. */
 		uint64_t free_head:11;
 		uint64_t reserved_37:1;
-		/*
-		 * when free_val is set, indicates the last entry on
-		 * the free list.
-		 */
+		/* when free_val is set, indicates the last entry on the free list. */
 		uint64_t free_tail:11;
-		/*
-		 * set when there is one or more POW entries on the
-		 * input Q list selected by qosgrp.
-		 */
+		/* set when there is one or more POW entries on the input Q list selected by qosgrp. */
 		uint64_t loc_val:1;
-		/*
-		 * set when there is exactly one POW entry on the
-		 * input Q list selected by qosgrp.
-		 */
+		/* set when there is exactly one POW entry on the input Q list selected by qosgrp. */
 		uint64_t loc_one:1;
 		uint64_t reserved_23:1;
-		/*
-		 * when loc_val is set, indicates the first entry on
-		 * the input Q list selected by qosgrp.
-		 */
+		/* when loc_val is set, indicates the first entry on the input Q list selected by qosgrp. */
 		uint64_t loc_head:11;
 		uint64_t reserved_11:1;
-		/*
-		 * when loc_val is set, indicates the last entry on
-		 * the input Q list selected by qosgrp.
-		 */
+		/* when loc_val is set, indicates the last entry on the input Q list selected by qosgrp. */
+		uint64_t loc_tail:11;
+#else
 		uint64_t loc_tail:11;
+		uint64_t reserved_11:1;
+		uint64_t loc_head:11;
+		uint64_t reserved_23:1;
+		uint64_t loc_one:1;
+		uint64_t loc_val:1;
+		uint64_t free_tail:11;
+		uint64_t reserved_37:1;
+		uint64_t free_head:11;
+		uint64_t reserved_49:1;
+		uint64_t free_one:1;
+		uint64_t free_val:1;
+		uint64_t reserved_52_63:12;
+#endif
 	} sindexload0;
 
     /**
+     * Result for SSO Index/Pointer Load(opcode == IPL_IQ/IPL_DESCHED/IPL_NOSCHED)
+     */
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t reserved_28_63:36;
+		/* If set, one or more valid entries are in the queue. */
+		uint64_t queue_val:1;
+		/* If set, exactly one valid entry is in the queue. */
+		uint64_t queue_one:1;
+		uint64_t reserved_24_25:2;
+		/* Index of entry at the head of the queue. */
+		uint64_t queue_head:11;
+		uint64_t reserved_11_12:2;
+		/* Index of entry at the tail of the queue. */
+		uint64_t queue_tail:11;
+#else
+		uint64_t queue_tail:11;
+		uint64_t reserved_11_12:2;
+		uint64_t queue_head:11;
+		uint64_t reserved_24_25:2;
+		uint64_t queue_one:1;
+		uint64_t queue_val:1;
+		uint64_t reserved_28_63:36;
+#endif
+	} sindexload0_cn68xx;
+
+    /**
      * Result For POW Index/Pointer Load (get_rmt == 0/get_des_get_tail == 1)
      */
 	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
 		uint64_t reserved_52_63:12;
-		/*
-		 * set when there is one or more POW entries on the
-		 * nosched list.
-		 */
+		/* set when there is one or more POW entries on the nosched list. */
 		uint64_t nosched_val:1;
-		/*
-		 * set when there is exactly one POW entry on the
-		 * nosched list.
-		 */
+		/* set when there is exactly one POW entry on the nosched list. */
 		uint64_t nosched_one:1;
 		uint64_t reserved_49:1;
-		/*
-		 * when nosched_val is set, indicates the first entry
-		 * on the nosched list.
-		 */
+		/* when nosched_val is set, indicates the first entry on the nosched list. */
 		uint64_t nosched_head:11;
 		uint64_t reserved_37:1;
-		/*
-		 * when nosched_val is set, indicates the last entry
-		 * on the nosched list.
-		 */
+		/* when nosched_val is set, indicates the last entry on the nosched list. */
 		uint64_t nosched_tail:11;
 		/*
 		 * set when there is one or more descheduled heads on
@@ -826,7 +1328,7 @@ typedef union {
 		 */
 		uint64_t des_val:1;
 		/*
-		 * set when there is exactly one descheduled head on
+		 *set when there is exactly one descheduled head on
 		 * the descheduled list selected by qosgrp.
 		 */
 		uint64_t des_one:1;
@@ -843,12 +1345,63 @@ typedef union {
 		 * head on the descheduled list selected by qosgrp.
 		 */
 		uint64_t des_tail:11;
+#else
+		uint64_t des_tail:11;
+		uint64_t reserved_11:1;
+		uint64_t des_head:11;
+		uint64_t reserved_23:1;
+		uint64_t des_one:1;
+		uint64_t des_val:1;
+		uint64_t nosched_tail:11;
+		uint64_t reserved_37:1;
+		uint64_t nosched_head:11;
+		uint64_t reserved_49:1;
+		uint64_t nosched_one:1;
+		uint64_t nosched_val:1;
+		uint64_t reserved_52_63:12;
+#endif
 	} sindexload1;
 
     /**
+     * Result for SSO Index/Pointer Load(opcode == IPL_FREE0/IPL_FREE1/IPL_FREE2)
+     */
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t reserved_60_63:4;
+		/* - Subqueue with current head */
+		uint64_t qnum_head:2;
+		/* - Subqueue with current tail */
+		uint64_t qnum_tail:2;
+		uint64_t reserved_28_55:28;
+		/* - If set, one or more valid entries are in the queue. */
+		uint64_t queue_val:1;
+		/* - If set, exactly one valid entry is in the queue. */
+		uint64_t queue_one:1;
+		uint64_t reserved_24_25:2;
+		/* - Index of entry at the head of the queue. */
+		uint64_t queue_head:11;
+		uint64_t reserved_11_12:2;
+		/* - Index of entry at the tail of the queue. */
+		uint64_t queue_tail:11;
+#else
+		uint64_t queue_tail:11;
+		uint64_t reserved_11_12:2;
+		uint64_t queue_head:11;
+		uint64_t reserved_24_25:2;
+		uint64_t queue_one:1;
+		uint64_t queue_val:1;
+		uint64_t reserved_28_55:28;
+		uint64_t qnum_tail:2;
+		uint64_t qnum_head:2;
+		uint64_t reserved_60_63:4;
+#endif
+	} sindexload1_cn68xx;
+
+    /**
      * Result For POW Index/Pointer Load (get_rmt == 1/get_des_get_tail == 0)
      */
 	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
 		uint64_t reserved_39_63:25;
 		/*
 		 * Set when this DRAM list is the current head
@@ -877,13 +1430,20 @@ typedef union {
 		 * qosgrp.
 		 */
 		uint64_t rmt_head:36;
+#else
+		uint64_t rmt_head:36;
+		uint64_t rmt_one:1;
+		uint64_t rmt_val:1;
+		uint64_t rmt_is_head:1;
+		uint64_t reserved_39_63:25;
+#endif
 	} sindexload2;
 
     /**
-     * Result For POW Index/Pointer Load (get_rmt ==
-     * 1/get_des_get_tail == 1)
+     * Result For POW Index/Pointer Load (get_rmt == 1/get_des_get_tail == 1)
      */
 	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
 		uint64_t reserved_39_63:25;
 		/*
 		 * set when this DRAM list is the current head
@@ -908,38 +1468,74 @@ typedef union {
 		uint64_t rmt_one:1;
 		/*
 		 * when rmt_val is set, indicates the last piece of
-		 * work on the DRAM input Q list selected by
-		 * qosgrp.
+		 * work on the DRAM input Q list selected by qosgrp.
 		 */
 		uint64_t rmt_tail:36;
+#else
+		uint64_t rmt_tail:36;
+		uint64_t rmt_one:1;
+		uint64_t rmt_val:1;
+		uint64_t rmt_is_head:1;
+		uint64_t reserved_39_63:25;
+#endif
 	} sindexload3;
 
     /**
      * Response to NULL_RD request loads
      */
 	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
 		uint64_t unused:62;
-		/* of type cvmx_pow_tag_type_t. state is one of the
-		 * following:
-		 *
-		 * - CVMX_POW_TAG_TYPE_ORDERED
-		 * - CVMX_POW_TAG_TYPE_ATOMIC
-		 * - CVMX_POW_TAG_TYPE_NULL
-		 * - CVMX_POW_TAG_TYPE_NULL_NULL
+		/*
+		 * of type cvmx_pow_tag_type_t. state is one of the following:
+		 *  - CVMX_POW_TAG_TYPE_ORDERED
+		 *  - CVMX_POW_TAG_TYPE_ATOMIC
+		 *  - CVMX_POW_TAG_TYPE_NULL
+		 *  - CVMX_POW_TAG_TYPE_NULL_NULL
 		 */
 		uint64_t state:2;
+#else
+		uint64_t state:2;
+		uint64_t unused:62;
+#endif
 	} s_null_rd;
 
-} cvmx_pow_tag_load_resp_t;
+};
+
+union cvmx_pow_sl_tag_resp {
+	uint64_t u64;
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t reserved_57_63:7;
+		uint64_t index:11;
+		uint64_t reserved_45:1;
+		uint64_t grp:6;
+		uint64_t head:1;
+		uint64_t tail:1;
+		uint64_t reserved_34_36:3;
+		uint64_t tag_type:2;
+		uint64_t tag:32;
+#else
+		uint64_t tag:32;
+		uint64_t tag_type:2;
+		uint64_t reserved_34_36:3;
+		uint64_t tail:1;
+		uint64_t head:1;
+		uint64_t grp:6;
+		uint64_t reserved_45:1;
+		uint64_t index:11;
+		uint64_t reserved_57_63:7;
+#endif
+	} s;
+};
 
 /**
  * This structure describes the address used for stores to the POW.
- *  The store address is meaningful on stores to the POW.  The
- *  hardware assumes that an aligned 64-bit store was used for all
- *  these stores.  Note the assumption that the work queue entry is
- *  aligned on an 8-byte boundary (since the low-order 3 address bits
- *  must be zero).  Note that not all fields are used by all
- *  operations.
+ *  The store address is meaningful on stores to the POW.  The hardware assumes that an aligned
+ *  64-bit store was used for all these stores.
+ *  Note the assumption that the work queue entry is aligned on an 8-byte
+ *  boundary (since the low-order 3 address bits must be zero).
+ *  Note that not all fields are used by all operations.
  *
  *  NOTE: The following is the behavior of the pending switch bit at the PP
  *       for POW stores (i.e. when did<7:3> == 0xc)
@@ -951,40 +1547,44 @@ typedef union {
  *     - No other loads/stores have an affect on the pending switch bit
  *     - The switch bus from POW can clear the pending switch bit
  *
- *  NOTE: did<2:0> == 2 is used by the HW for a special single-cycle
- *  ADDWQ command that only contains the pointer). SW must never use
- *  did<2:0> == 2.
+ *  NOTE: did<2:0> == 2 is used by the HW for a special single-cycle ADDWQ command
+ *  that only contains the pointer). SW must never use did<2:0> == 2.
  */
-typedef union {
+union cvmx_pow_tag_store_addr {
     /**
      * Unsigned 64 bit integer representation of store address
      */
 	uint64_t u64;
 
 	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
 		/* Memory region.  Should be CVMX_IO_SEG in most cases */
 		uint64_t mem_reg:2;
-		uint64_t reserved_49_61:13;	/* Must be zero */
-		uint64_t is_io:1;	/* Must be one */
-		/* Device ID of POW.  Note that different sub-dids are used. */
-		uint64_t did:8;
-		uint64_t reserved_36_39:4;	/* Must be zero */
-		/* Address field. addr<2:0> must be zero */
-		uint64_t addr:36;
+		uint64_t reserved_49_61:13; /* Must be zero */
+		uint64_t is_io:1;	    /* Must be one */
+		uint64_t did:8;		    /* Device ID of POW.  Note that different sub-dids are used. */
+		uint64_t reserved_36_39:4;  /* Must be zero */
+		uint64_t addr:36;	    /* Address field. addr<2:0> must be zero */
+#else
+		uint64_t addr:36;
+		uint64_t reserved_36_39:4;
+		uint64_t did:8;
+		uint64_t is_io:1;
+		uint64_t reserved_49_61:13;
+		uint64_t mem_reg:2;
+#endif
 	} stag;
-} cvmx_pow_tag_store_addr_t;
+};
 
 /**
  * decode of the store data when an IOBDMA SENDSINGLE is sent to POW
  */
-typedef union {
+union cvmx_pow_iobdma_store {
 	uint64_t u64;
 
 	struct {
-		/*
-		 * the (64-bit word) location in scratchpad to write
-		 * to (if len != 0)
-		 */
+#ifdef __BIG_ENDIAN_BITFIELD
+		/* the (64-bit word) location in scratchpad to write to (if len != 0) */
 		uint64_t scraddr:8;
 		/* the number of words in the response (0 => no response) */
 		uint64_t len:8;
@@ -994,11 +1594,17 @@ typedef union {
 		/* if set, don't return load response until work is available */
 		uint64_t wait:1;
 		uint64_t unused2:3;
+#else
+		uint64_t unused2:3;
+		uint64_t wait:1;
+		uint64_t unused:36;
+		uint64_t did:8;
+		uint64_t len:8;
+		uint64_t scraddr:8;
+#endif
 	} s;
 
-} cvmx_pow_iobdma_store_t;
-
-/* CSR typedefs have been moved to cvmx-csr-*.h */
+};
 
 /**
  * Get the POW tag for this core. This returns the current
@@ -1009,24 +1615,38 @@ typedef union {
  *
  * Returns Current tag
  */
-static inline cvmx_pow_tag_req_t cvmx_pow_get_current_tag(void)
+static inline struct cvmx_pow_tag_info cvmx_pow_get_current_tag(void)
 {
-	cvmx_pow_load_addr_t load_addr;
-	cvmx_pow_tag_load_resp_t load_resp;
-	cvmx_pow_tag_req_t result;
-
-	load_addr.u64 = 0;
-	load_addr.sstatus.mem_region = CVMX_IO_SEG;
-	load_addr.sstatus.is_io = 1;
-	load_addr.sstatus.did = CVMX_OCT_DID_TAG_TAG1;
-	load_addr.sstatus.coreid = cvmx_get_core_num();
-	load_addr.sstatus.get_cur = 1;
-	load_resp.u64 = cvmx_read_csr(load_addr.u64);
-	result.u64 = 0;
-	result.s.grp = load_resp.s_sstatus2.grp;
-	result.s.index = load_resp.s_sstatus2.index;
-	result.s.type = load_resp.s_sstatus2.tag_type;
-	result.s.tag = load_resp.s_sstatus2.tag;
+	union cvmx_pow_load_addr load_addr;
+	struct cvmx_pow_tag_info result;
+
+	if (octeon_has_feature(OCTEON_FEATURE_CN68XX_WQE)) {
+		union cvmx_pow_sl_tag_resp load_resp;
+		load_addr.u64 = 0;
+		load_addr.sstatus_cn68xx.mem_region = CVMX_IO_SEG;
+		load_addr.sstatus_cn68xx.is_io = 1;
+		load_addr.sstatus_cn68xx.did = CVMX_OCT_DID_TAG_TAG5;
+		load_addr.sstatus_cn68xx.coreid = cvmx_get_core_num();
+		load_addr.sstatus_cn68xx.opcode = 3;
+		load_resp.u64 = cvmx_read_csr(load_addr.u64);
+		result.grp = load_resp.s.grp;
+		result.index = load_resp.s.index;
+		result.tag_type = load_resp.s.tag_type;
+		result.tag = load_resp.s.tag;
+	} else {
+		union cvmx_pow_tag_load_resp load_resp;
+		load_addr.u64 = 0;
+		load_addr.sstatus.mem_region = CVMX_IO_SEG;
+		load_addr.sstatus.is_io = 1;
+		load_addr.sstatus.did = CVMX_OCT_DID_TAG_TAG1;
+		load_addr.sstatus.coreid = cvmx_get_core_num();
+		load_addr.sstatus.get_cur = 1;
+		load_resp.u64 = cvmx_read_csr(load_addr.u64);
+		result.grp = load_resp.s_sstatus2.grp;
+		result.index = load_resp.s_sstatus2.index;
+		result.tag_type = load_resp.s_sstatus2.tag_type;
+		result.tag = load_resp.s_sstatus2.tag;
+	}
 	return result;
 }
 
@@ -1038,78 +1658,48 @@ static inline cvmx_pow_tag_req_t cvmx_pow_get_current_tag(void)
  */
 static inline cvmx_wqe_t *cvmx_pow_get_current_wqp(void)
 {
-	cvmx_pow_load_addr_t load_addr;
-	cvmx_pow_tag_load_resp_t load_resp;
-
-	load_addr.u64 = 0;
-	load_addr.sstatus.mem_region = CVMX_IO_SEG;
-	load_addr.sstatus.is_io = 1;
-	load_addr.sstatus.did = CVMX_OCT_DID_TAG_TAG1;
-	load_addr.sstatus.coreid = cvmx_get_core_num();
-	load_addr.sstatus.get_cur = 1;
-	load_addr.sstatus.get_wqp = 1;
-	load_resp.u64 = cvmx_read_csr(load_addr.u64);
-	return (cvmx_wqe_t *) cvmx_phys_to_ptr(load_resp.s_sstatus4.wqp);
-}
-
-#ifndef CVMX_MF_CHORD
-#define CVMX_MF_CHORD(dest)         CVMX_RDHWR(dest, 30)
-#endif
-
-/**
- * Print a warning if a tag switch is pending for this core
- *
- * @function: Function name checking for a pending tag switch
- */
-static inline void __cvmx_pow_warn_if_pending_switch(const char *function)
-{
-	uint64_t switch_complete;
-	CVMX_MF_CHORD(switch_complete);
-	if (!switch_complete)
-		pr_warning("%s called with tag switch in progress\n", function);
-}
-
-/**
- * Waits for a tag switch to complete by polling the completion bit.
- * Note that switches to NULL complete immediately and do not need
- * to be waited for.
- */
-static inline void cvmx_pow_tag_sw_wait(void)
-{
-	const uint64_t MAX_CYCLES = 1ull << 31;
-	uint64_t switch_complete;
-	uint64_t start_cycle = cvmx_get_cycle();
-	while (1) {
-		CVMX_MF_CHORD(switch_complete);
-		if (unlikely(switch_complete))
-			break;
-		if (unlikely(cvmx_get_cycle() > start_cycle + MAX_CYCLES)) {
-			pr_warning("Tag switch is taking a long time, "
-				   "possible deadlock\n");
-			start_cycle = -MAX_CYCLES - 1;
-		}
+	union cvmx_pow_load_addr load_addr;
+	union cvmx_pow_tag_load_resp load_resp;
+
+	if (octeon_has_feature(OCTEON_FEATURE_CN68XX_WQE)) {
+		load_addr.u64 = 0;
+		load_addr.sstatus_cn68xx.mem_region = CVMX_IO_SEG;
+		load_addr.sstatus_cn68xx.is_io = 1;
+		load_addr.sstatus_cn68xx.did = CVMX_OCT_DID_TAG_TAG5;
+		load_addr.sstatus_cn68xx.coreid = cvmx_get_core_num();
+		load_addr.sstatus_cn68xx.opcode = 3;
+		load_resp.u64 = cvmx_read_csr(load_addr.u64);
+		if (load_resp.s_sstatus3_cn68xx.wqp)
+			return (cvmx_wqe_t *) cvmx_phys_to_ptr(load_resp.s_sstatus3_cn68xx.wqp);
+		else
+			return (cvmx_wqe_t *) 0;
+	} else {
+		load_addr.u64 = 0;
+		load_addr.sstatus.mem_region = CVMX_IO_SEG;
+		load_addr.sstatus.is_io = 1;
+		load_addr.sstatus.did = CVMX_OCT_DID_TAG_TAG1;
+		load_addr.sstatus.coreid = cvmx_get_core_num();
+		load_addr.sstatus.get_cur = 1;
+		load_addr.sstatus.get_wqp = 1;
+		load_resp.u64 = cvmx_read_csr(load_addr.u64);
+		return (cvmx_wqe_t *) cvmx_phys_to_ptr(load_resp.s_sstatus4.wqp);
 	}
 }
 
 /**
- * Synchronous work request.  Requests work from the POW.
- * This function does NOT wait for previous tag switches to complete,
- * so the caller must ensure that there is not a pending tag switch.
+ * Synchronous work request.  Requests work from the POW.  This
+ * function does NOT wait for previous tag switches to complete, so
+ * the caller must ensure that there is not a pending tag switch.
  *
- * @wait:   When set, call stalls until work becomes avaiable, or times out.
- *               If not set, returns immediately.
+ * @wait: When set, call stalls until work becomes avaiable, or times
+ *        out.  If not set, returns immediately.
  *
- * Returns Returns the WQE pointer from POW. Returns NULL if no work
- * was available.
+ * Returns Returns the WQE pointer from POW. Returns NULL if no work was available.
  */
-static inline cvmx_wqe_t *cvmx_pow_work_request_sync_nocheck(cvmx_pow_wait_t
-							     wait)
+static inline cvmx_wqe_t *cvmx_pow_work_request_sync(enum cvmx_pow_wait wait)
 {
-	cvmx_pow_load_addr_t ptr;
-	cvmx_pow_tag_load_resp_t result;
-
-	if (CVMX_ENABLE_POW_CHECKS)
-		__cvmx_pow_warn_if_pending_switch(__func__);
+	union cvmx_pow_load_addr ptr;
+	union cvmx_pow_tag_load_resp result;
 
 	ptr.u64 = 0;
 	ptr.swork.mem_region = CVMX_IO_SEG;
@@ -1126,107 +1716,32 @@ static inline cvmx_wqe_t *cvmx_pow_work_request_sync_nocheck(cvmx_pow_wait_t
 }
 
 /**
- * Synchronous work request.  Requests work from the POW.
- * This function waits for any previous tag switch to complete before
- * requesting the new work.
- *
- * @wait:   When set, call stalls until work becomes avaiable, or times out.
- *               If not set, returns immediately.
- *
- * Returns Returns the WQE pointer from POW. Returns NULL if no work
- * was available.
- */
-static inline cvmx_wqe_t *cvmx_pow_work_request_sync(cvmx_pow_wait_t wait)
-{
-	if (CVMX_ENABLE_POW_CHECKS)
-		__cvmx_pow_warn_if_pending_switch(__func__);
-
-	/* Must not have a switch pending when requesting work */
-	cvmx_pow_tag_sw_wait();
-	return cvmx_pow_work_request_sync_nocheck(wait);
-
-}
-
-/**
- * Synchronous null_rd request.  Requests a switch out of NULL_NULL POW state.
- * This function waits for any previous tag switch to complete before
- * requesting the null_rd.
- *
- * Returns Returns the POW state of type cvmx_pow_tag_type_t.
- */
-static inline enum cvmx_pow_tag_type cvmx_pow_work_request_null_rd(void)
-{
-	cvmx_pow_load_addr_t ptr;
-	cvmx_pow_tag_load_resp_t result;
-
-	if (CVMX_ENABLE_POW_CHECKS)
-		__cvmx_pow_warn_if_pending_switch(__func__);
-
-	/* Must not have a switch pending when requesting work */
-	cvmx_pow_tag_sw_wait();
-
-	ptr.u64 = 0;
-	ptr.snull_rd.mem_region = CVMX_IO_SEG;
-	ptr.snull_rd.is_io = 1;
-	ptr.snull_rd.did = CVMX_OCT_DID_TAG_NULL_RD;
-
-	result.u64 = cvmx_read_csr(ptr.u64);
-
-	return (enum cvmx_pow_tag_type) result.s_null_rd.state;
-}
-
-/**
- * Asynchronous work request.  Work is requested from the POW unit,
- * and should later be checked with function
- * cvmx_pow_work_response_async.  This function does NOT wait for
- * previous tag switches to complete, so the caller must ensure that
- * there is not a pending tag switch.
+ * Asynchronous work request.  Work is requested from the POW unit, and should later
+ * be checked with function cvmx_pow_work_response_async.
+ * This function does NOT wait for previous tag switches to complete,
+ * so the caller must ensure that there is not a pending tag switch.
  *
  * @scr_addr: Scratch memory address that response will be returned
  *            to, which is either a valid WQE, or a response with the
  *            invalid bit set.  Byte address, must be 8 byte aligned.
- *
- * @wait: 1 to cause response to wait for work to become available (or
- *        timeout), 0 to cause response to return immediately
+ * @wait:     1 to cause response to wait for work to become available (or timeout)
+ *            0 to cause response to return immediately
  */
-static inline void cvmx_pow_work_request_async_nocheck(int scr_addr,
-						       cvmx_pow_wait_t wait)
+static inline void cvmx_pow_work_request_async(int scr_addr, enum cvmx_pow_wait wait)
 {
-	cvmx_pow_iobdma_store_t data;
-
-	if (CVMX_ENABLE_POW_CHECKS)
-		__cvmx_pow_warn_if_pending_switch(__func__);
+	union cvmx_pow_iobdma_store data;
 
 	/* scr_addr must be 8 byte aligned */
+	data.u64 = 0;
 	data.s.scraddr = scr_addr >> 3;
 	data.s.len = 1;
 	data.s.did = CVMX_OCT_DID_TAG_SWTAG;
 	data.s.wait = wait;
 	cvmx_send_single(data.u64);
 }
-
-/**
- * Asynchronous work request.  Work is requested from the POW unit,
- * and should later be checked with function
- * cvmx_pow_work_response_async.  This function waits for any previous
- * tag switch to complete before requesting the new work.
- *
- * @scr_addr: Scratch memory address that response will be returned
- *            to, which is either a valid WQE, or a response with the
- *            invalid bit set.  Byte address, must be 8 byte aligned.
- *
- * @wait: 1 to cause response to wait for work to become available (or
- *                  timeout), 0 to cause response to return immediately
- */
-static inline void cvmx_pow_work_request_async(int scr_addr,
-					       cvmx_pow_wait_t wait)
+static inline void cvmx_pow_work_request_async_nocheck(int scr_addr, enum cvmx_pow_wait wait)
 {
-	if (CVMX_ENABLE_POW_CHECKS)
-		__cvmx_pow_warn_if_pending_switch(__func__);
-
-	/* Must not have a switch pending when requesting work */
-	cvmx_pow_tag_sw_wait();
-	cvmx_pow_work_request_async_nocheck(scr_addr, wait);
+	cvmx_pow_work_request_async(scr_addr, wait);
 }
 
 /**
@@ -1235,13 +1750,11 @@ static inline void cvmx_pow_work_request_async(int scr_addr,
  *
  * @scr_addr: Scratch memory address to get result from Byte address,
  *            must be 8 byte aligned.
- *
- * Returns Returns the WQE from the scratch register, or NULL if no
- * work was available.
+ * Returns the WQE from the scratch register, or NULL if no work was available.
  */
 static inline cvmx_wqe_t *cvmx_pow_work_response_async(int scr_addr)
 {
-	cvmx_pow_tag_load_resp_t result;
+	union cvmx_pow_tag_load_resp result;
 
 	CVMX_SYNCIOBDMA;
 	result.u64 = cvmx_scratch_read64(scr_addr);
@@ -1268,322 +1781,54 @@ static inline uint64_t cvmx_pow_work_invalid(cvmx_wqe_t *wqe_ptr)
 }
 
 /**
- * Starts a tag switch to the provided tag value and tag type.
- * Completion for the tag switch must be checked for separately.  This
- * function does NOT update the work queue entry in dram to match tag
- * value and type, so the application must keep track of these if they
- * are important to the application.  This tag switch command must not
- * be used for switches to NULL, as the tag switch pending bit will be
- * set by the switch request, but never cleared by the hardware.
- *
- * NOTE: This should not be used when switching from a NULL tag.  Use
- * cvmx_pow_tag_sw_full() instead.
- *
- * This function does no checks, so the caller must ensure that any
- * previous tag switch has completed.
- *
- * @tag:      new tag value
- * @tag_type: new tag type (ordered or atomic)
- */
-static inline void cvmx_pow_tag_sw_nocheck(uint32_t tag,
-					   enum cvmx_pow_tag_type tag_type)
-{
-	cvmx_addr_t ptr;
-	cvmx_pow_tag_req_t tag_req;
-
-	if (CVMX_ENABLE_POW_CHECKS) {
-		cvmx_pow_tag_req_t current_tag;
-		__cvmx_pow_warn_if_pending_switch(__func__);
-		current_tag = cvmx_pow_get_current_tag();
-		if (current_tag.s.type == CVMX_POW_TAG_TYPE_NULL_NULL)
-			pr_warning("%s called with NULL_NULL tag\n",
-				   __func__);
-		if (current_tag.s.type == CVMX_POW_TAG_TYPE_NULL)
-			pr_warning("%s called with NULL tag\n", __func__);
-		if ((current_tag.s.type == tag_type)
-		   && (current_tag.s.tag == tag))
-			pr_warning("%s called to perform a tag switch to the "
-				   "same tag\n",
-			     __func__);
-		if (tag_type == CVMX_POW_TAG_TYPE_NULL)
-			pr_warning("%s called to perform a tag switch to "
-				   "NULL. Use cvmx_pow_tag_sw_null() instead\n",
-			     __func__);
-	}
-
-	/*
-	 * Note that WQE in DRAM is not updated here, as the POW does
-	 * not read from DRAM once the WQE is in flight.  See hardware
-	 * manual for complete details.  It is the application's
-	 * responsibility to keep track of the current tag value if
-	 * that is important.
-	 */
-
-	tag_req.u64 = 0;
-	tag_req.s.op = CVMX_POW_TAG_OP_SWTAG;
-	tag_req.s.tag = tag;
-	tag_req.s.type = tag_type;
-
-	ptr.u64 = 0;
-	ptr.sio.mem_region = CVMX_IO_SEG;
-	ptr.sio.is_io = 1;
-	ptr.sio.did = CVMX_OCT_DID_TAG_SWTAG;
-
-	/* once this store arrives at POW, it will attempt the switch
-	   software must wait for the switch to complete separately */
-	cvmx_write_io(ptr.u64, tag_req.u64);
-}
-
-/**
- * Starts a tag switch to the provided tag value and tag type.
- * Completion for the tag switch must be checked for separately.  This
- * function does NOT update the work queue entry in dram to match tag
- * value and type, so the application must keep track of these if they
- * are important to the application.  This tag switch command must not
- * be used for switches to NULL, as the tag switch pending bit will be
- * set by the switch request, but never cleared by the hardware.
- *
- * NOTE: This should not be used when switching from a NULL tag.  Use
- * cvmx_pow_tag_sw_full() instead.
- *
- * This function waits for any previous tag switch to complete, and also
- * displays an error on tag switches to NULL.
- *
- * @tag:      new tag value
- * @tag_type: new tag type (ordered or atomic)
- */
-static inline void cvmx_pow_tag_sw(uint32_t tag,
-				   enum cvmx_pow_tag_type tag_type)
-{
-	if (CVMX_ENABLE_POW_CHECKS)
-		__cvmx_pow_warn_if_pending_switch(__func__);
-
-	/*
-	 * Note that WQE in DRAM is not updated here, as the POW does
-	 * not read from DRAM once the WQE is in flight.  See hardware
-	 * manual for complete details.  It is the application's
-	 * responsibility to keep track of the current tag value if
-	 * that is important.
-	 */
-
-	/*
-	 * Ensure that there is not a pending tag switch, as a tag
-	 * switch cannot be started if a previous switch is still
-	 * pending.
-	 */
-	cvmx_pow_tag_sw_wait();
-	cvmx_pow_tag_sw_nocheck(tag, tag_type);
-}
-
-/**
- * Starts a tag switch to the provided tag value and tag type.
- * Completion for the tag switch must be checked for separately.  This
- * function does NOT update the work queue entry in dram to match tag
- * value and type, so the application must keep track of these if they
- * are important to the application.  This tag switch command must not
- * be used for switches to NULL, as the tag switch pending bit will be
- * set by the switch request, but never cleared by the hardware.
- *
- * This function must be used for tag switches from NULL.
- *
- * This function does no checks, so the caller must ensure that any
- * previous tag switch has completed.
- *
- * @wqp:      pointer to work queue entry to submit.  This entry is
- *            updated to match the other parameters
- * @tag:      tag value to be assigned to work queue entry
- * @tag_type: type of tag
- * @group:    group value for the work queue entry.
- */
-static inline void cvmx_pow_tag_sw_full_nocheck(cvmx_wqe_t *wqp, uint32_t tag,
-						enum cvmx_pow_tag_type tag_type,
-						uint64_t group)
-{
-	cvmx_addr_t ptr;
-	cvmx_pow_tag_req_t tag_req;
-
-	if (CVMX_ENABLE_POW_CHECKS) {
-		cvmx_pow_tag_req_t current_tag;
-		__cvmx_pow_warn_if_pending_switch(__func__);
-		current_tag = cvmx_pow_get_current_tag();
-		if (current_tag.s.type == CVMX_POW_TAG_TYPE_NULL_NULL)
-			pr_warning("%s called with NULL_NULL tag\n",
-				   __func__);
-		if ((current_tag.s.type == tag_type)
-		   && (current_tag.s.tag == tag))
-			pr_warning("%s called to perform a tag switch to "
-				   "the same tag\n",
-			     __func__);
-		if (tag_type == CVMX_POW_TAG_TYPE_NULL)
-			pr_warning("%s called to perform a tag switch to "
-				   "NULL. Use cvmx_pow_tag_sw_null() instead\n",
-			     __func__);
-		if (wqp != cvmx_phys_to_ptr(0x80))
-			if (wqp != cvmx_pow_get_current_wqp())
-				pr_warning("%s passed WQE(%p) doesn't match "
-					   "the address in the POW(%p)\n",
-				     __func__, wqp,
-				     cvmx_pow_get_current_wqp());
-	}
-
-	/*
-	 * Note that WQE in DRAM is not updated here, as the POW does
-	 * not read from DRAM once the WQE is in flight.  See hardware
-	 * manual for complete details.  It is the application's
-	 * responsibility to keep track of the current tag value if
-	 * that is important.
-	 */
-
-	tag_req.u64 = 0;
-	tag_req.s.op = CVMX_POW_TAG_OP_SWTAG_FULL;
-	tag_req.s.tag = tag;
-	tag_req.s.type = tag_type;
-	tag_req.s.grp = group;
-
-	ptr.u64 = 0;
-	ptr.sio.mem_region = CVMX_IO_SEG;
-	ptr.sio.is_io = 1;
-	ptr.sio.did = CVMX_OCT_DID_TAG_SWTAG;
-	ptr.sio.offset = CAST64(wqp);
-
-	/*
-	 * once this store arrives at POW, it will attempt the switch
-	 * software must wait for the switch to complete separately.
-	 */
-	cvmx_write_io(ptr.u64, tag_req.u64);
-}
-
-/**
- * Starts a tag switch to the provided tag value and tag type.
- * Completion for the tag switch must be checked for separately.  This
- * function does NOT update the work queue entry in dram to match tag
- * value and type, so the application must keep track of these if they
- * are important to the application.  This tag switch command must not
- * be used for switches to NULL, as the tag switch pending bit will be
- * set by the switch request, but never cleared by the hardware.
- *
- * This function must be used for tag switches from NULL.
- *
- * This function waits for any pending tag switches to complete
- * before requesting the tag switch.
- *
- * @wqp:      pointer to work queue entry to submit.  This entry is updated
- *            to match the other parameters
- * @tag:      tag value to be assigned to work queue entry
- * @tag_type: type of tag
- * @group:      group value for the work queue entry.
- */
-static inline void cvmx_pow_tag_sw_full(cvmx_wqe_t *wqp, uint32_t tag,
-					enum cvmx_pow_tag_type tag_type,
-					uint64_t group)
-{
-	if (CVMX_ENABLE_POW_CHECKS)
-		__cvmx_pow_warn_if_pending_switch(__func__);
-
-	/*
-	 * Ensure that there is not a pending tag switch, as a tag
-	 * switch cannot be started if a previous switch is still
-	 * pending.
-	 */
-	cvmx_pow_tag_sw_wait();
-	cvmx_pow_tag_sw_full_nocheck(wqp, tag, tag_type, group);
-}
-
-/**
- * Switch to a NULL tag, which ends any ordering or
- * synchronization provided by the POW for the current
- * work queue entry.  This operation completes immediately,
- * so completion should not be waited for.
- * This function does NOT wait for previous tag switches to complete,
- * so the caller must ensure that any previous tag switches have completed.
- */
-static inline void cvmx_pow_tag_sw_null_nocheck(void)
-{
-	cvmx_addr_t ptr;
-	cvmx_pow_tag_req_t tag_req;
-
-	if (CVMX_ENABLE_POW_CHECKS) {
-		cvmx_pow_tag_req_t current_tag;
-		__cvmx_pow_warn_if_pending_switch(__func__);
-		current_tag = cvmx_pow_get_current_tag();
-		if (current_tag.s.type == CVMX_POW_TAG_TYPE_NULL_NULL)
-			pr_warning("%s called with NULL_NULL tag\n",
-				   __func__);
-		if (current_tag.s.type == CVMX_POW_TAG_TYPE_NULL)
-			pr_warning("%s called when we already have a "
-				   "NULL tag\n",
-			     __func__);
-	}
-
-	tag_req.u64 = 0;
-	tag_req.s.op = CVMX_POW_TAG_OP_SWTAG;
-	tag_req.s.type = CVMX_POW_TAG_TYPE_NULL;
-
-	ptr.u64 = 0;
-	ptr.sio.mem_region = CVMX_IO_SEG;
-	ptr.sio.is_io = 1;
-	ptr.sio.did = CVMX_OCT_DID_TAG_TAG1;
-
-	cvmx_write_io(ptr.u64, tag_req.u64);
-
-	/* switch to NULL completes immediately */
-}
-
-/**
- * Switch to a NULL tag, which ends any ordering or
- * synchronization provided by the POW for the current
- * work queue entry.  This operation completes immediately,
- * so completion should not be waited for.
- * This function waits for any pending tag switches to complete
- * before requesting the switch to NULL.
- */
-static inline void cvmx_pow_tag_sw_null(void)
-{
-	if (CVMX_ENABLE_POW_CHECKS)
-		__cvmx_pow_warn_if_pending_switch(__func__);
-
-	/*
-	 * Ensure that there is not a pending tag switch, as a tag
-	 * switch cannot be started if a previous switch is still
-	 * pending.
-	 */
-	cvmx_pow_tag_sw_wait();
-	cvmx_pow_tag_sw_null_nocheck();
-
-	/* switch to NULL completes immediately */
-}
-
-/**
- * Submits work to an input queue.  This function updates the work
- * queue entry in DRAM to match the arguments given.  Note that the
- * tag provided is for the work queue entry submitted, and is
- * unrelated to the tag that the core currently holds.
+ * Submits work to an input queue.  This function updates the work queue entry in DRAM to match
+ * the arguments given.
+ * Note that the tag provided is for the work queue entry submitted, and is unrelated to the tag that
+ * the core currently holds.
  *
- * @wqp:      pointer to work queue entry to submit.  This entry is
- *            updated to match the other parameters
+ * @wqp:      pointer to work queue entry to submit.  This entry is updated to match the other parameters
  * @tag:      tag value to be assigned to work queue entry
  * @tag_type: type of tag
  * @qos:      Input queue to add to.
  * @grp:      group value for the work queue entry.
  */
 static inline void cvmx_pow_work_submit(cvmx_wqe_t *wqp, uint32_t tag,
-					enum cvmx_pow_tag_type tag_type,
+					uint64_t tag_type,
 					uint64_t qos, uint64_t grp)
 {
 	cvmx_addr_t ptr;
-	cvmx_pow_tag_req_t tag_req;
-
-	wqp->qos = qos;
-	wqp->tag = tag;
-	wqp->tag_type = tag_type;
-	wqp->grp = grp;
+	union cvmx_pow_tag_req tag_req;
 
 	tag_req.u64 = 0;
-	tag_req.s.op = CVMX_POW_TAG_OP_ADDWQ;
-	tag_req.s.type = tag_type;
-	tag_req.s.tag = tag;
-	tag_req.s.qos = qos;
-	tag_req.s.grp = grp;
+
+	wqp->word1.tag = tag;
+	wqp->word1.tag_type = tag_type;
+
+	if (octeon_has_feature(OCTEON_FEATURE_CN68XX_WQE)) {
+		/* Reset all reserved bits */
+		wqp->word1.cn68xx.zero_0 = 0;
+		wqp->word1.cn68xx.zero_1 = 0;
+		wqp->word1.cn68xx.zero_2 = 0;
+		wqp->word1.cn68xx.qos = qos;
+		wqp->word1.cn68xx.grp = grp;
+
+		tag_req.s_cn68xx_add.op = CVMX_POW_TAG_OP_ADDWQ;
+		tag_req.s_cn68xx_add.type = tag_type;
+		tag_req.s_cn68xx_add.tag = tag;
+		tag_req.s_cn68xx_add.qos = qos;
+		tag_req.s_cn68xx_add.grp = grp;
+	} else {
+		/* Reset all reserved bits */
+		wqp->word1.cn38xx.zero_2 = 0;
+		wqp->word1.cn38xx.qos = qos;
+		wqp->word1.cn38xx.grp = grp;
+
+		tag_req.s_cn38xx.op = CVMX_POW_TAG_OP_ADDWQ;
+		tag_req.s_cn38xx.type = tag_type;
+		tag_req.s_cn38xx.tag = tag;
+		tag_req.s_cn38xx.qos = qos;
+		tag_req.s_cn38xx.grp = grp;
+	}
 
 	ptr.u64 = 0;
 	ptr.sio.mem_region = CVMX_IO_SEG;
@@ -1591,315 +1836,44 @@ static inline void cvmx_pow_work_submit(cvmx_wqe_t *wqp, uint32_t tag,
 	ptr.sio.did = CVMX_OCT_DID_TAG_TAG1;
 	ptr.sio.offset = cvmx_ptr_to_phys(wqp);
 
-	/*
-	 * SYNC write to memory before the work submit.  This is
-	 * necessary as POW may read values from DRAM at this time.
-	 */
+	/* SYNC write to memory before the work submit.  This is necessary
+	 ** as POW may read values from DRAM at this time */
 	CVMX_SYNCWS;
 	cvmx_write_io(ptr.u64, tag_req.u64);
 }
 
-/**
- * This function sets the group mask for a core.  The group mask
- * indicates which groups each core will accept work from. There are
- * 16 groups.
- *
- * @core_num:   core to apply mask to
- * @mask:   Group mask. There are 16 groups, so only bits 0-15 are valid,
- *               representing groups 0-15.
- *               Each 1 bit in the mask enables the core to accept work from
- *               the corresponding group.
- */
-static inline void cvmx_pow_set_group_mask(uint64_t core_num, uint64_t mask)
-{
-	union cvmx_pow_pp_grp_mskx grp_msk;
-
-	grp_msk.u64 = cvmx_read_csr(CVMX_POW_PP_GRP_MSKX(core_num));
-	grp_msk.s.grp_msk = mask;
-	cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(core_num), grp_msk.u64);
-}
-
-/**
- * This function sets POW static priorities for a core. Each input queue has
- * an associated priority value.
- *
- * @core_num:   core to apply priorities to
- * @priority:   Vector of 8 priorities, one per POW Input Queue (0-7).
- *                   Highest priority is 0 and lowest is 7. A priority value
- *                   of 0xF instructs POW to skip the Input Queue when
- *                   scheduling to this specific core.
- *                   NOTE: priorities should not have gaps in values, meaning
- *                         {0,1,1,1,1,1,1,1} is a valid configuration while
- *                         {0,2,2,2,2,2,2,2} is not.
- */
-static inline void cvmx_pow_set_priority(uint64_t core_num,
-					 const uint8_t priority[])
-{
-	/* POW priorities are supported on CN5xxx and later */
-	if (!OCTEON_IS_MODEL(OCTEON_CN3XXX)) {
-		union cvmx_pow_pp_grp_mskx grp_msk;
-
-		grp_msk.u64 = cvmx_read_csr(CVMX_POW_PP_GRP_MSKX(core_num));
-		grp_msk.s.qos0_pri = priority[0];
-		grp_msk.s.qos1_pri = priority[1];
-		grp_msk.s.qos2_pri = priority[2];
-		grp_msk.s.qos3_pri = priority[3];
-		grp_msk.s.qos4_pri = priority[4];
-		grp_msk.s.qos5_pri = priority[5];
-		grp_msk.s.qos6_pri = priority[6];
-		grp_msk.s.qos7_pri = priority[7];
-
-		/* Detect gaps between priorities and flag error */
-		{
-			int i;
-			uint32_t prio_mask = 0;
-
-			for (i = 0; i < 8; i++)
-				if (priority[i] != 0xF)
-					prio_mask |= 1 << priority[i];
-
-			if (prio_mask ^ ((1 << cvmx_pop(prio_mask)) - 1)) {
-				pr_err("POW static priorities should be "
-				       "contiguous (0x%llx)\n",
-				     (unsigned long long)prio_mask);
-				return;
-			}
-		}
-
-		cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(core_num), grp_msk.u64);
-	}
-}
-
-/**
- * Performs a tag switch and then an immediate deschedule. This completes
- * immediately, so completion must not be waited for.  This function does NOT
- * update the wqe in DRAM to match arguments.
- *
- * This function does NOT wait for any prior tag switches to complete, so the
- * calling code must do this.
- *
- * Note the following CAVEAT of the Octeon HW behavior when
- * re-scheduling DE-SCHEDULEd items whose (next) state is
- * ORDERED:
- *   - If there are no switches pending at the time that the
- *     HW executes the de-schedule, the HW will only re-schedule
- *     the head of the FIFO associated with the given tag. This
- *     means that in many respects, the HW treats this ORDERED
- *     tag as an ATOMIC tag. Note that in the SWTAG_DESCH
- *     case (to an ORDERED tag), the HW will do the switch
- *     before the deschedule whenever it is possible to do
- *     the switch immediately, so it may often look like
- *     this case.
- *   - If there is a pending switch to ORDERED at the time
- *     the HW executes the de-schedule, the HW will perform
- *     the switch at the time it re-schedules, and will be
- *     able to reschedule any/all of the entries with the
- *     same tag.
- * Due to this behavior, the RECOMMENDATION to software is
- * that they have a (next) state of ATOMIC when they
- * DE-SCHEDULE. If an ORDERED tag is what was really desired,
- * SW can choose to immediately switch to an ORDERED tag
- * after the work (that has an ATOMIC tag) is re-scheduled.
- * Note that since there are never any tag switches pending
- * when the HW re-schedules, this switch can be IMMEDIATE upon
- * the reception of the pointer during the re-schedule.
- *
- * @tag:      New tag value
- * @tag_type: New tag type
- * @group:    New group value
- * @no_sched: Control whether this work queue entry will be rescheduled.
- *                 - 1 : don't schedule this work
- *                 - 0 : allow this work to be scheduled.
- */
-static inline void cvmx_pow_tag_sw_desched_nocheck(
-	uint32_t tag,
-	enum cvmx_pow_tag_type tag_type,
-	uint64_t group,
-	uint64_t no_sched)
-{
-	cvmx_addr_t ptr;
-	cvmx_pow_tag_req_t tag_req;
-
-	if (CVMX_ENABLE_POW_CHECKS) {
-		cvmx_pow_tag_req_t current_tag;
-		__cvmx_pow_warn_if_pending_switch(__func__);
-		current_tag = cvmx_pow_get_current_tag();
-		if (current_tag.s.type == CVMX_POW_TAG_TYPE_NULL_NULL)
-			pr_warning("%s called with NULL_NULL tag\n",
-				   __func__);
-		if (current_tag.s.type == CVMX_POW_TAG_TYPE_NULL)
-			pr_warning("%s called with NULL tag. Deschedule not "
-				   "allowed from NULL state\n",
-			     __func__);
-		if ((current_tag.s.type != CVMX_POW_TAG_TYPE_ATOMIC)
-			&& (tag_type != CVMX_POW_TAG_TYPE_ATOMIC))
-			pr_warning("%s called where neither the before or "
-				   "after tag is ATOMIC\n",
-			     __func__);
-	}
-
-	tag_req.u64 = 0;
-	tag_req.s.op = CVMX_POW_TAG_OP_SWTAG_DESCH;
-	tag_req.s.tag = tag;
-	tag_req.s.type = tag_type;
-	tag_req.s.grp = group;
-	tag_req.s.no_sched = no_sched;
-
-	ptr.u64 = 0;
-	ptr.sio.mem_region = CVMX_IO_SEG;
-	ptr.sio.is_io = 1;
-	ptr.sio.did = CVMX_OCT_DID_TAG_TAG3;
-	/*
-	 * since TAG3 is used, this store will clear the local pending
-	 * switch bit.
-	 */
-	cvmx_write_io(ptr.u64, tag_req.u64);
-}
-
-/**
- * Performs a tag switch and then an immediate deschedule. This completes
- * immediately, so completion must not be waited for.  This function does NOT
- * update the wqe in DRAM to match arguments.
- *
- * This function waits for any prior tag switches to complete, so the
- * calling code may call this function with a pending tag switch.
- *
- * Note the following CAVEAT of the Octeon HW behavior when
- * re-scheduling DE-SCHEDULEd items whose (next) state is
- * ORDERED:
- *   - If there are no switches pending at the time that the
- *     HW executes the de-schedule, the HW will only re-schedule
- *     the head of the FIFO associated with the given tag. This
- *     means that in many respects, the HW treats this ORDERED
- *     tag as an ATOMIC tag. Note that in the SWTAG_DESCH
- *     case (to an ORDERED tag), the HW will do the switch
- *     before the deschedule whenever it is possible to do
- *     the switch immediately, so it may often look like
- *     this case.
- *   - If there is a pending switch to ORDERED at the time
- *     the HW executes the de-schedule, the HW will perform
- *     the switch at the time it re-schedules, and will be
- *     able to reschedule any/all of the entries with the
- *     same tag.
- * Due to this behavior, the RECOMMENDATION to software is
- * that they have a (next) state of ATOMIC when they
- * DE-SCHEDULE. If an ORDERED tag is what was really desired,
- * SW can choose to immediately switch to an ORDERED tag
- * after the work (that has an ATOMIC tag) is re-scheduled.
- * Note that since there are never any tag switches pending
- * when the HW re-schedules, this switch can be IMMEDIATE upon
- * the reception of the pointer during the re-schedule.
- *
- * @tag:      New tag value
- * @tag_type: New tag type
- * @group:    New group value
- * @no_sched: Control whether this work queue entry will be rescheduled.
- *                 - 1 : don't schedule this work
- *                 - 0 : allow this work to be scheduled.
- */
-static inline void cvmx_pow_tag_sw_desched(uint32_t tag,
-					   enum cvmx_pow_tag_type tag_type,
-					   uint64_t group, uint64_t no_sched)
-{
-	if (CVMX_ENABLE_POW_CHECKS)
-		__cvmx_pow_warn_if_pending_switch(__func__);
-
-	/* Need to make sure any writes to the work queue entry are complete */
-	CVMX_SYNCWS;
-	/*
-	 * Ensure that there is not a pending tag switch, as a tag
-	 * switch cannot be started if a previous switch is still
-	 * pending.
-	 */
-	cvmx_pow_tag_sw_wait();
-	cvmx_pow_tag_sw_desched_nocheck(tag, tag_type, group, no_sched);
-}
-
-/**
- * Descchedules the current work queue entry.
- *
- * @no_sched: no schedule flag value to be set on the work queue
- *            entry.  If this is set the entry will not be
- *            rescheduled.
+/*
+ * Define usage of bits within the 32 bit tag values.
  */
-static inline void cvmx_pow_desched(uint64_t no_sched)
-{
-	cvmx_addr_t ptr;
-	cvmx_pow_tag_req_t tag_req;
-
-	if (CVMX_ENABLE_POW_CHECKS) {
-		cvmx_pow_tag_req_t current_tag;
-		__cvmx_pow_warn_if_pending_switch(__func__);
-		current_tag = cvmx_pow_get_current_tag();
-		if (current_tag.s.type == CVMX_POW_TAG_TYPE_NULL_NULL)
-			pr_warning("%s called with NULL_NULL tag\n",
-				   __func__);
-		if (current_tag.s.type == CVMX_POW_TAG_TYPE_NULL)
-			pr_warning("%s called with NULL tag. Deschedule not "
-				   "expected from NULL state\n",
-			     __func__);
-	}
-
-	/* Need to make sure any writes to the work queue entry are complete */
-	CVMX_SYNCWS;
-
-	tag_req.u64 = 0;
-	tag_req.s.op = CVMX_POW_TAG_OP_DESCH;
-	tag_req.s.no_sched = no_sched;
-
-	ptr.u64 = 0;
-	ptr.sio.mem_region = CVMX_IO_SEG;
-	ptr.sio.is_io = 1;
-	ptr.sio.did = CVMX_OCT_DID_TAG_TAG3;
-	/*
-	 * since TAG3 is used, this store will clear the local pending
-	 * switch bit.
-	 */
-	cvmx_write_io(ptr.u64, tag_req.u64);
-}
-
-/****************************************************
-* Define usage of bits within the 32 bit tag values.
-*****************************************************/
 
 /*
- * Number of bits of the tag used by software.  The SW bits are always
- * a contiguous block of the high starting at bit 31.  The hardware
- * bits are always the low bits.  By default, the top 8 bits of the
- * tag are reserved for software, and the low 24 are set by the IPD
- * unit.
+ * Number of bits of the tag used by software.  The SW bits
+ * are always a contiguous block of the high starting at bit 31.
+ * The hardware bits are always the low bits.  By default, the top 8 bits
+ * of the tag are reserved for software, and the low 24 are set by the IPD unit.
  */
 #define CVMX_TAG_SW_BITS    (8)
 #define CVMX_TAG_SW_SHIFT   (32 - CVMX_TAG_SW_BITS)
 
 /* Below is the list of values for the top 8 bits of the tag. */
-/*
- * Tag values with top byte of this value are reserved for internal
- * executive uses.
- */
+/* Tag values with top byte of this value are reserved for internal executive uses */
 #define CVMX_TAG_SW_BITS_INTERNAL  0x1
-/* The executive divides the remaining 24 bits as follows:
- *  - the upper 8 bits (bits 23 - 16 of the tag) define a subgroup
- *
- *  - the lower 16 bits (bits 15 - 0 of the tag) define are the value
- *    with the subgroup
- *
- * Note that this section describes the format of tags generated by
- * software - refer to the hardware documentation for a description of
- * the tags values generated by the packet input hardware.  Subgroups
- * are defined here.
+/*
+ * The executive divides the remaining 24 bits as follows:
+ *  * the upper 8 bits (bits 23 - 16 of the tag) define a subgroup
+ *  * the lower 16 bits (bits 15 - 0 of the tag) define are the value with the subgroup
+ * Note that this section describes the format of tags generated by software - refer to the
+ * hardware documentation for a description of the tags values generated by the packet input
+ * hardware.
+ * Subgroups are defined here
  */
-/* Mask for the value portion of the tag */
-#define CVMX_TAG_SUBGROUP_MASK  0xFFFF
+#define CVMX_TAG_SUBGROUP_MASK  0xFFFF	/* Mask for the value portion of the tag */
 #define CVMX_TAG_SUBGROUP_SHIFT 16
 #define CVMX_TAG_SUBGROUP_PKO  0x1
 
 /* End of executive tag subgroup definitions */
 
-/*
- * The remaining values software bit values 0x2 - 0xff are available
- * for application use.
- */
+/* The remaining values software bit values 0x2 - 0xff are available for application use */
 
 /**
  * This function creates a 32 bit tag value from the two values provided.
@@ -1907,7 +1881,6 @@ static inline void cvmx_pow_desched(uint64_t no_sched)
  * @sw_bits: The upper bits (number depends on configuration) are set
  *           to this value.  The remainder of bits are set by the
  *           hw_bits parameter.
- *
  * @hw_bits: The lower bits (number depends on configuration) are set
  *           to this value.  The remainder of bits are set by the
  *           sw_bits parameter.
@@ -1916,8 +1889,7 @@ static inline void cvmx_pow_desched(uint64_t no_sched)
  */
 static inline uint32_t cvmx_pow_tag_compose(uint64_t sw_bits, uint64_t hw_bits)
 {
-	return ((sw_bits & cvmx_build_mask(CVMX_TAG_SW_BITS)) <<
-			CVMX_TAG_SW_SHIFT) |
+	return ((sw_bits & cvmx_build_mask(CVMX_TAG_SW_BITS)) << CVMX_TAG_SW_SHIFT) |
 		(hw_bits & cvmx_build_mask(32 - CVMX_TAG_SW_BITS));
 }
 
@@ -1926,13 +1898,11 @@ static inline uint32_t cvmx_pow_tag_compose(uint64_t sw_bits, uint64_t hw_bits)
  *
  * @tag:    32 bit tag value
  *
- * Returns N bit software tag value, where N is configurable with the
- * CVMX_TAG_SW_BITS define
+ * Returns N bit software tag value, where N is configurable with the CVMX_TAG_SW_BITS define
  */
 static inline uint32_t cvmx_pow_tag_get_sw_bits(uint64_t tag)
 {
-	return (tag >> (32 - CVMX_TAG_SW_BITS)) &
-		cvmx_build_mask(CVMX_TAG_SW_BITS);
+	return (tag >> (32 - CVMX_TAG_SW_BITS)) & cvmx_build_mask(CVMX_TAG_SW_BITS);
 }
 
 /**
@@ -1941,8 +1911,7 @@ static inline uint32_t cvmx_pow_tag_get_sw_bits(uint64_t tag)
  *
  * @tag:    32 bit tag value
  *
- * Returns (32 - N) bit software tag value, where N is configurable
- * with the CVMX_TAG_SW_BITS define
+ * Returns (32 - N) bit software tag value, where N is configurable with the CVMX_TAG_SW_BITS define
  */
 static inline uint32_t cvmx_pow_tag_get_hw_bits(uint64_t tag)
 {
@@ -1950,33 +1919,11 @@ static inline uint32_t cvmx_pow_tag_get_hw_bits(uint64_t tag)
 }
 
 /**
- * Store the current POW internal state into the supplied
- * buffer. It is recommended that you pass a buffer of at least
- * 128KB. The format of the capture may change based on SDK
- * version and Octeon chip.
- *
- * @buffer: Buffer to store capture into
- * @buffer_size:
- *               The size of the supplied buffer
- *
- * Returns Zero on success, negative on failure
- */
-extern int cvmx_pow_capture(void *buffer, int buffer_size);
-
-/**
- * Dump a POW capture to the console in a human readable format.
- *
- * @buffer: POW capture from cvmx_pow_capture()
- * @buffer_size:
- *               Size of the buffer
- */
-extern void cvmx_pow_display(void *buffer, int buffer_size);
-
-/**
  * Return the number of POW entries supported by this chip
  *
  * Returns Number of POW entries
  */
 extern int cvmx_pow_get_num_entries(void);
 
+
 #endif /* __CVMX_POW_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-wqe.h b/arch/mips/include/asm/octeon/cvmx-wqe.h
index 6536109..1062a50 100644
--- a/arch/mips/include/asm/octeon/cvmx-wqe.h
+++ b/arch/mips/include/asm/octeon/cvmx-wqe.h
@@ -1,10 +1,10 @@
 /***********************license start***************
- * Author: Cavium Networks
+ * Author: Cavium Inc.
  *
- * Contact: support@caviumnetworks.com
+ * Contact: support@cavium.com
  * This file is part of the OCTEON SDK
  *
- * Copyright (c) 2003-2008 Cavium Networks
+ * Copyright (c) 2003-2012 Cavium Inc.
  *
  * This file is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License, Version 2, as
@@ -22,11 +22,10 @@
  * or visit http://www.gnu.org/licenses/.
  *
  * This file may also be available under a different license from Cavium.
- * Contact Cavium Networks for more information
+ * Contact Cavium Inc. for more information
  ***********************license end**************************************/
 
-/**
- *
+/*
  * This header file defines the work queue entry (wqe) data structure.
  * Since this is a commonly used structure that depends on structures
  * from several hardware blocks, those definitions have been placed
@@ -40,14 +39,11 @@
 #ifndef __CVMX_WQE_H__
 #define __CVMX_WQE_H__
 
-#include "cvmx-packet.h"
-
 
-#define OCT_TAG_TYPE_STRING(x)						\
-	(((x) == CVMX_POW_TAG_TYPE_ORDERED) ?  "ORDERED" :		\
-		(((x) == CVMX_POW_TAG_TYPE_ATOMIC) ?  "ATOMIC" :	\
-			(((x) == CVMX_POW_TAG_TYPE_NULL) ?  "NULL" :	\
-				"NULL_NULL")))
+#define OCT_TAG_TYPE_STRING(x) (((x) == CVMX_POW_TAG_TYPE_ORDERED) ?  "ORDERED" : \
+				(((x) == CVMX_POW_TAG_TYPE_ATOMIC) ?  "ATOMIC" : \
+				 (((x) == CVMX_POW_TAG_TYPE_NULL) ?  "NULL" : \
+				  "NULL_NULL")))
 
 /**
  * HW decode / err_code in work queue entry
@@ -55,24 +51,24 @@
 typedef union {
 	uint64_t u64;
 
-	/* Use this struct if the hardware determines that the packet is IP */
+#ifdef __BIG_ENDIAN_BITFIELD
+	/* hardware determined that the packet is IP */
 	struct {
-		/* HW sets this to the number of buffers used by this packet */
+		/* The number of buffers used by this packet */
 		uint64_t bufs:8;
-		/* HW sets to the number of L2 bytes prior to the IP */
+		/* The number of L2 bytes prior to the IP */
 		uint64_t ip_offset:8;
 		/* set to 1 if we found DSA/VLAN in the L2 */
 		uint64_t vlan_valid:1;
 		/* Set to 1 if the DSA/VLAN tag is stacked */
 		uint64_t vlan_stacked:1;
 		uint64_t unassigned:1;
-		/* HW sets to the DSA/VLAN CFI flag (valid when vlan_valid) */
+		/* DSA/VLAN CFI flag (valid when vlan_valid) */
 		uint64_t vlan_cfi:1;
-		/* HW sets to the DSA/VLAN_ID field (valid when vlan_valid) */
+		/*  DSA/VLAN_ID field (valid when vlan_valid) */
 		uint64_t vlan_id:12;
-		/* Ring Identifier (if PCIe). Requires PIP_GBL_CTL[RING_EN]=1 */
-		uint64_t pr:4;
-		uint64_t unassigned2:8;
+		/* 38xx and 68xx have different definitions.  */
+		uint64_t varies:12;
 		/* the packet needs to be decompressed */
 		uint64_t dec_ipcomp:1;
 		/* the packet is either TCP or UDP */
@@ -82,91 +78,113 @@ typedef union {
 		/* the packet is IPv6 */
 		uint64_t is_v6:1;
 
-		/*
-		 * (rcv_error, not_IP, IP_exc, is_frag, L4_error,
-		 * software, etc.).
-		 */
+		/* (rcv_error, not_IP, IP_exc, is_frag, L4_error, software, etc.) */
 
-		/*
-		 * reserved for software use, hardware will clear on
-		 * packet creation.
-		 */
+		/* reserved for software use, hardware will clear on packet creation */
 		uint64_t software:1;
+
 		/* exceptional conditions below */
-		/* the receive interface hardware detected an L4 error
+
+		/*
+		 * the receive interface hardware detected an L4 error
 		 * (only applies if !is_frag) (only applies if
 		 * !rcv_error && !not_IP && !IP_exc && !is_frag)
 		 * failure indicated in err_code below, decode:
 		 *
 		 * - 1 = Malformed L4
 		 * - 2 = L4 Checksum Error: the L4 checksum value is
-		 * - 3 = UDP Length Error: The UDP length field would
-		 *       make the UDP data longer than what remains in
-		 *       the IP packet (as defined by the IP header
-		 *       length field).
-		 * - 4 = Bad L4 Port: either the source or destination
-		 *       TCP/UDP port is 0.
-		 * - 8 = TCP FIN Only: the packet is TCP and only the
-		 *       FIN flag set.
-		 * - 9 = TCP No Flags: the packet is TCP and no flags
-		 *       are set.
-		 * - 10 = TCP FIN RST: the packet is TCP and both FIN
-		 *        and RST are set.
-		 * - 11 = TCP SYN URG: the packet is TCP and both SYN
-		 *        and URG are set.
-		 * - 12 = TCP SYN RST: the packet is TCP and both SYN
-		 *        and RST are set.
-		 * - 13 = TCP SYN FIN: the packet is TCP and both SYN
-		 *        and FIN are set.
+		 * - 3 = UDP Length Error: The UDP length field would make the UDP data longer than what
+		 *     remains in the IP packet (as defined by the IP header length field).
+		 * - 4 = Bad L4 Port: either the source or destination TCP/UDP port is 0.
+		 * - 8 = TCP FIN Only: the packet is TCP and only the FIN flag set.
+		 * - 9 = TCP No Flags: the packet is TCP and no flags are set.
+		 * - 10 = TCP FIN RST: the packet is TCP and both FIN and RST are set.
+		 * - 11 = TCP SYN URG: the packet is TCP and both SYN and URG are set.
+		 * - 12 = TCP SYN RST: the packet is TCP and both SYN and RST are set.
+		 * - 13 = TCP SYN FIN: the packet is TCP and both SYN and FIN are set.
 		 */
 		uint64_t L4_error:1;
 		/* set if the packet is a fragment */
 		uint64_t is_frag:1;
-		/* the receive interface hardware detected an IP error
+		/*
+		 * the receive interface hardware detected an IP error
 		 * / exception (only applies if !rcv_error && !not_IP)
 		 * failure indicated in err_code below, decode:
 		 *
-		 * - 1 = Not IP: the IP version field is neither 4 nor
-		 *       6.
-		 * - 2 = IPv4 Header Checksum Error: the IPv4 header
-		 *       has a checksum violation.
-		 * - 3 = IP Malformed Header: the packet is not long
-		 *       enough to contain the IP header.
-		 * - 4 = IP Malformed: the packet is not long enough
-		 *	 to contain the bytes indicated by the IP
-		 *	 header. Pad is allowed.
-		 * - 5 = IP TTL Hop: the IPv4 TTL field or the IPv6
-		 *       Hop Count field are zero.
+		 * - 1 = Not IP: the IP version field is neither 4 nor 6.
+		 * - 2 = IPv4 Header Checksum Error: the IPv4 header has a checksum violation.
+		 * - 3 = IP Malformed Header: the packet is not long enough to contain the IP header.
+		 * - 4 = IP Malformed: the packet is not long enough to contain the bytes indicated by the IP
+		 *     header. Pad is allowed.
+		 * - 5 = IP TTL Hop: the IPv4 TTL field or the IPv6 Hop Count field are zero.
 		 * - 6 = IP Options
 		 */
 		uint64_t IP_exc:1;
-		/*
-		 * Set if the hardware determined that the packet is a
-		 * broadcast.
-		 */
+		/* set if the hardware determined that the packet is a broadcast */
 		uint64_t is_bcast:1;
-		/*
-		 * St if the hardware determined that the packet is a
-		 * multi-cast.
-		 */
+		/* set if the hardware determined that the packet is a multi-cast */
 		uint64_t is_mcast:1;
-		/*
-		 * Set if the packet may not be IP (must be zero in
-		 * this case).
-		 */
+		/* set if the packet may not be IP (must be zero in this case) */
 		uint64_t not_IP:1;
-		/*
-		 * The receive interface hardware detected a receive
-		 * error (must be zero in this case).
-		 */
+		/* the receive interface hardware detected a receive error (must be zero in this case) */
 		uint64_t rcv_error:1;
-		/* lower err_code = first-level descriptor of the
-		 * work */
-		/* zero for packet submitted by hardware that isn't on
-		 * the slow path */
-		/* type is cvmx_pip_err_t */
 		uint64_t err_code:8;
 	} s;
+	struct {
+		/* as above */
+		uint64_t bufs:8;
+		uint64_t ip_offset:8;
+		uint64_t vlan_valid:1;
+		uint64_t vlan_stacked:1;
+		uint64_t unassigned:1;
+		uint64_t vlan_cfi:1;
+		uint64_t vlan_id:12;
+
+		/* MAC/PIP port number.  */
+		uint64_t port:12;
+
+		uint64_t dec_ipcomp:1;
+		uint64_t tcp_or_udp:1;
+		uint64_t dec_ipsec:1;
+		uint64_t is_v6:1;
+		uint64_t software:1;
+		uint64_t L4_error:1;
+		uint64_t is_frag:1;
+		uint64_t IP_exc:1;
+		uint64_t is_bcast:1;
+		uint64_t is_mcast:1;
+		uint64_t not_IP:1;
+		uint64_t rcv_error:1;
+		uint64_t err_code:8;
+	} s_cn68xx;
+	struct {
+		/* as above */
+		uint64_t bufs:8;
+		uint64_t ip_offset:8;
+		uint64_t vlan_valid:1;
+		uint64_t vlan_stacked:1;
+		uint64_t unassigned:1;
+		uint64_t vlan_cfi:1;
+		uint64_t vlan_id:12;
+		/* Ring Identifier (if PCIe). Requires PIP_GBL_CTL[RING_EN]=1 */
+		uint64_t pr:4;
+		uint64_t unassigned2a:4;
+		uint64_t unassigned2:4;
+
+		uint64_t dec_ipcomp:1;
+		uint64_t tcp_or_udp:1;
+		uint64_t dec_ipsec:1;
+		uint64_t is_v6:1;
+		uint64_t software:1;
+		uint64_t L4_error:1;
+		uint64_t is_frag:1;
+		uint64_t IP_exc:1;
+		uint64_t is_bcast:1;
+		uint64_t is_mcast:1;
+		uint64_t not_IP:1;
+		uint64_t rcv_error:1;
+		uint64_t err_code:8;
+	} s_cn38xx;
 
 	/* use this to get at the 16 vlan bits */
 	struct {
@@ -175,130 +193,403 @@ typedef union {
 		uint64_t unused2:32;
 	} svlan;
 
-	/*
-	 * use this struct if the hardware could not determine that
-	 * the packet is ip.
-	 */
+	/* use this struct if the hardware could not determine that the packet is ip */
 	struct {
-		/*
-		 * HW sets this to the number of buffers used by this
-		 * packet.
-		 */
+		/* The number of buffers used by this packet */
 		uint64_t bufs:8;
 		uint64_t unused:8;
-		/* set to 1 if we found DSA/VLAN in the L2 */
+		/* 1 if we found DSA/VLAN in the L2 */
 		uint64_t vlan_valid:1;
-		/* Set to 1 if the DSA/VLAN tag is stacked */
+		/* 1 if the DSA/VLAN tag is stacked */
 		uint64_t vlan_stacked:1;
 		uint64_t unassigned:1;
-		/*
-		 * HW sets to the DSA/VLAN CFI flag (valid when
-		 * vlan_valid)
-		 */
+		/* DSA/VLAN CFI flag (valid when vlan_valid) */
 		uint64_t vlan_cfi:1;
-		/*
-		 * HW sets to the DSA/VLAN_ID field (valid when
-		 * vlan_valid).
-		 */
+		/* DSA/VLAN_ID field (valid when vlan_valid) */
 		uint64_t vlan_id:12;
+
+		/* 38xx and 68xx have different definitions.  */
+		uint64_t varies:12;
+		uint64_t unassigned2:4;
+
+		/* reserved for software use, hardware will clear on packet creation */
+		uint64_t software:1;
+		uint64_t unassigned3:1;
+		uint64_t is_rarp:1;
+		uint64_t is_arp:1;
+		uint64_t is_bcast:1;
+		uint64_t is_mcast:1;
+		/* set if the packet may not be IP (must be one in this case) */
+		uint64_t not_IP:1;
 		/*
-		 * Ring Identifier (if PCIe). Requires
-		 * PIP_GBL_CTL[RING_EN]=1
+		 * the receive interface hardware detected a receive
+		 * error.  Failure indicated in err_code below,
+		 * decode:
+		 *
+		 * - 1 = partial error: a packet was partially received, but internal
+		 *     buffering / bandwidth was not adequate to receive the entire packet.
+		 * - 2 = jabber error: the RGMII packet was too large and is truncated.
+		 * - 3 = overrun error: the RGMII packet is longer than allowed and had
+		 *     an FCS error.
+		 * - 4 = oversize error: the RGMII packet is longer than allowed.
+		 * - 5 = alignment error: the RGMII packet is not an integer number of bytes
+		 *     and had an FCS error (100M and 10M only).
+		 * - 6 = fragment error: the RGMII packet is shorter than allowed and had an
+		 *     FCS error.
+		 * - 7 = GMX FCS error: the RGMII packet had an FCS error.
+		 * - 8 = undersize error: the RGMII packet is shorter than allowed.
+		 * - 9 = extend error: the RGMII packet had an extend error.
+		 * - 10 = length mismatch error: the RGMII packet had a length that did not
+		 *     match the length field in the L2 HDR.
+		 * - 11 = RGMII RX error/SPI4 DIP4 Error: the RGMII packet had one or more
+		 *     data reception errors (RXERR) or the SPI4 packet had one or more DIP4
+		 *     errors.
+		 * - 12 = RGMII skip error/SPI4 Abort Error: the RGMII packet was not large
+		 *     enough to cover the skipped bytes or the SPI4 packet was terminated
+		 *     with an About EOPS.
+		 * - 13 = RGMII nibble error/SPI4 Port NXA Error: the RGMII packet had a
+		 *     studder error (data not repeated - 10/100M only) or the SPI4 packet
+		 *     was sent to an NXA.
+		 * - 16 = FCS error: a SPI4.2 packet had an FCS error.
+		 * - 17 = Skip error: a packet was not large enough to cover the skipped bytes.
+		 * - 18 = L2 header malformed: the packet is not long enough to contain the L2
 		 */
+		uint64_t rcv_error:1;
+
+		uint64_t err_code:8;
+	} snoip;
+	struct {
+		/* as above */
+		uint64_t bufs:8;
+		uint64_t unused:8;
+		uint64_t vlan_valid:1;
+		uint64_t vlan_stacked:1;
+		uint64_t unassigned:1;
+		uint64_t vlan_cfi:1;
+		uint64_t vlan_id:12;
+
+		/* MAC/PIP port number.  */
+		uint64_t port:12;
+
+		uint64_t unassigned2:4;
+		uint64_t software:1;
+		uint64_t unassigned3:1;
+		uint64_t is_rarp:1;
+		uint64_t is_arp:1;
+		uint64_t is_bcast:1;
+		uint64_t is_mcast:1;
+		uint64_t not_IP:1;
+		uint64_t rcv_error:1;
+		uint64_t err_code:8;
+	} snoip_cn68xx;
+	struct {
+		/* as above */
+		uint64_t bufs:8;
+		uint64_t unused:8;
+		uint64_t vlan_valid:1;
+		uint64_t vlan_stacked:1;
+		uint64_t unassigned:1;
+		uint64_t vlan_cfi:1;
+		uint64_t vlan_id:12;
+		/* Ring Identifier (if PCIe). Requires PIP_GBL_CTL[RING_EN]=1 */
 		uint64_t pr:4;
-		uint64_t unassigned2:12;
-		/*
-		 * reserved for software use, hardware will clear on
-		 * packet creation.
-		 */
+		uint64_t unassigned2a:8;
+
+		uint64_t unassigned2:4;
 		uint64_t software:1;
 		uint64_t unassigned3:1;
-		/*
-		 * set if the hardware determined that the packet is
-		 * rarp.
-		 */
 		uint64_t is_rarp:1;
+		uint64_t is_arp:1;
+		uint64_t is_bcast:1;
+		uint64_t is_mcast:1;
+		uint64_t not_IP:1;
+		uint64_t rcv_error:1;
+		uint64_t err_code:8;
+	} snoip_cn38xx;
+#else				/* __LITTLE_ENDIAN_BITFIELD */
+	struct {
+		uint64_t err_code:8;
+		uint64_t rcv_error:1;
+		uint64_t not_IP:1;
+		uint64_t is_mcast:1;
+		uint64_t is_bcast:1;
+		uint64_t IP_exc:1;
+		uint64_t is_frag:1;
+		uint64_t L4_error:1;
+		uint64_t software:1;
+		uint64_t is_v6:1;
+		uint64_t dec_ipsec:1;
+		uint64_t tcp_or_udp:1;
+		uint64_t dec_ipcomp:1;
+		uint64_t varies:12;
+		uint64_t vlan_id:12;
+		uint64_t vlan_cfi:1;
+		uint64_t unassigned:1;
+		uint64_t vlan_stacked:1;
+		uint64_t vlan_valid:1;
+		uint64_t ip_offset:8;
+		uint64_t bufs:8;
+	} s;
+	struct {
+		uint64_t err_code:8;
+		uint64_t rcv_error:1;
+		uint64_t not_IP:1;
+		uint64_t is_mcast:1;
+		uint64_t is_bcast:1;
+		uint64_t IP_exc:1;
+		uint64_t is_frag:1;
+		uint64_t L4_error:1;
+		uint64_t software:1;
+		uint64_t is_v6:1;
+		uint64_t dec_ipsec:1;
+		uint64_t tcp_or_udp:1;
+		uint64_t dec_ipcomp:1;
+		uint64_t port:12;
+		uint64_t vlan_id:12;
+		uint64_t vlan_cfi:1;
+		uint64_t unassigned:1;
+		uint64_t vlan_stacked:1;
+		uint64_t vlan_valid:1;
+		uint64_t ip_offset:8;
+		uint64_t bufs:8;
+	} s_cn68xx;
+	struct {
+		uint64_t err_code:8;
+		uint64_t rcv_error:1;
+		uint64_t not_IP:1;
+		uint64_t is_mcast:1;
+		uint64_t is_bcast:1;
+		uint64_t IP_exc:1;
+		uint64_t is_frag:1;
+		uint64_t L4_error:1;
+		uint64_t software:1;
+		uint64_t is_v6:1;
+		uint64_t dec_ipsec:1;
+		uint64_t tcp_or_udp:1;
+		uint64_t dec_ipcomp:1;
+		uint64_t unassigned2:4;
+		uint64_t unassigned2a:4;
+		uint64_t pr:4;
+		uint64_t vlan_id:12;
+		uint64_t vlan_cfi:1;
+		uint64_t unassigned:1;
+		uint64_t vlan_stacked:1;
+		uint64_t vlan_valid:1;
+		uint64_t ip_offset:8;
+		uint64_t bufs:8;
+	} s_cn38xx;
+
+	struct {
+		uint64_t unused2:32;
+		uint64_t vlan:16;
+		uint64_t unused1:16;
+	} svlan;
+
+	struct {
+		uint64_t err_code:8;
+		uint64_t rcv_error:1;
+		uint64_t not_IP:1;
+		uint64_t is_mcast:1;
+		uint64_t is_bcast:1;
+		uint64_t is_arp:1;
+		uint64_t is_rarp:1;
+		uint64_t unassigned3:1;
+		uint64_t software:1;
+		uint64_t unassigned2:4;
+		uint64_t varies:12;
+		uint64_t vlan_id:12;
+		uint64_t vlan_cfi:1;
+		uint64_t unassigned:1;
+		uint64_t vlan_stacked:1;
+		uint64_t vlan_valid:1;
+		uint64_t unused:8;
+		uint64_t bufs:8;
+	} snoip;
+	struct {
+		uint64_t err_code:8;
+		uint64_t rcv_error:1;
+		uint64_t not_IP:1;
+		uint64_t is_mcast:1;
+		uint64_t is_bcast:1;
+		uint64_t is_arp:1;
+		uint64_t is_rarp:1;
+		uint64_t unassigned3:1;
+		uint64_t software:1;
+		uint64_t unassigned2:4;
+		uint64_t port:12;
+		uint64_t vlan_id:12;
+		uint64_t vlan_cfi:1;
+		uint64_t unassigned:1;
+		uint64_t vlan_stacked:1;
+		uint64_t vlan_valid:1;
+		uint64_t unused:8;
+		uint64_t bufs:8;
+	} snoip_cn68xx;
+	struct {
+		uint64_t err_code:8;
+		uint64_t rcv_error:1;
+		uint64_t not_IP:1;
+		uint64_t is_mcast:1;
+		uint64_t is_bcast:1;
+		uint64_t is_arp:1;
+		uint64_t is_rarp:1;
+		uint64_t unassigned3:1;
+		uint64_t software:1;
+		uint64_t unassigned2:4;
+		uint64_t unassigned2a:8;
+		uint64_t pr:4;
+		uint64_t vlan_id:12;
+		uint64_t vlan_cfi:1;
+		uint64_t unassigned:1;
+		uint64_t vlan_stacked:1;
+		uint64_t vlan_valid:1;
+		uint64_t unused:8;
+		uint64_t bufs:8;
+	} snoip_cn38xx;
+#endif				/* __LITTLE_ENDIAN_BITFIELD */
+} cvmx_pip_wqe_word2_t;
+
+typedef union {
+	uint64_t u64;
+#ifdef __BIG_ENDIAN_BITFIELD
+	struct {
 		/*
-		 * set if the hardware determined that the packet is
-		 * arp
+		 * raw chksum result generated by the HW
 		 */
-		uint64_t is_arp:1;
+		uint16_t hw_chksum;
 		/*
-		 * set if the hardware determined that the packet is a
-		 * broadcast.
+		 * Field unused by hardware - available for software
 		 */
-		uint64_t is_bcast:1;
+		uint8_t unused;
 		/*
-		 * set if the hardware determined that the packet is a
-		 * multi-cast
+		 * Next pointer used by hardware for list maintenance.
+		 * May be written/read by HW before the work queue
+		 * entry is scheduled to a PP (Only 36 bits used in
+		 * Octeon 1)
 		 */
-		uint64_t is_mcast:1;
+		uint64_t next_ptr:40;
+
+	} cn38xx;
+	struct {
+		uint64_t l4ptr:8;	/* 56..63 */
+		uint64_t unused0:8;	/* 48..55 */
+		uint64_t l3ptr:8;	/* 40..47 */
+		uint64_t l2ptr:8;	/* 32..39 */
+		uint64_t unused1:18;	/* 14..31 */
+		uint64_t bpid:6;	/* 8..13 */
+		uint64_t unused2:2;	/* 6..7 */
+		uint64_t pknd:6;	/* 0..5 */
+	} cn68xx;
+#else
+	struct {
+		uint64_t next_ptr:40;
+		uint8_t unused;
+		uint16_t hw_chksum;
+	} cn38xx;
+	struct {
+		uint64_t pknd:6;
+		uint64_t unused2:2;
+		uint64_t bpid:6;
+		uint64_t unused1:18;
+		uint64_t l2ptr:8;
+		uint64_t l3ptr:8;
+		uint64_t unused0:8;
+		uint64_t l4ptr:8;
+	} cn68xx;
+#endif
+} cvmx_pip_wqe_word0_t;
+
+typedef union {
+	uint64_t u64;
+	cvmx_pip_wqe_word0_t pip;
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t unused:24;
+		uint64_t next_ptr:40;	/* on cn68xx this is unused as well */
+#else
+		uint64_t next_ptr:40;
+		uint64_t unused:24;
+#endif
+	} raw;
+} cvmx_wqe_word0_t;
+
+typedef union {
+	uint64_t u64;
+#ifdef __BIG_ENDIAN_BITFIELD
+	struct {
+		uint64_t len:16;
+		uint64_t varies:14;
 		/*
-		 * set if the packet may not be IP (must be one in
-		 * this case)
+		 * the type of the tag (ORDERED, ATOMIC, NULL)
 		 */
-		uint64_t not_IP:1;
-		/* The receive interface hardware detected a receive
-		 * error.  Failure indicated in err_code below,
-		 * decode:
-		 *
-		 * - 1 = partial error: a packet was partially
-		 *       received, but internal buffering / bandwidth
-		 *       was not adequate to receive the entire
-		 *       packet.
-		 * - 2 = jabber error: the RGMII packet was too large
-		 *       and is truncated.
-		 * - 3 = overrun error: the RGMII packet is longer
-		 *       than allowed and had an FCS error.
-		 * - 4 = oversize error: the RGMII packet is longer
-		 *       than allowed.
-		 * - 5 = alignment error: the RGMII packet is not an
-		 *       integer number of bytes
-		 *       and had an FCS error (100M and 10M only).
-		 * - 6 = fragment error: the RGMII packet is shorter
-		 *       than allowed and had an FCS error.
-		 * - 7 = GMX FCS error: the RGMII packet had an FCS
-		 *       error.
-		 * - 8 = undersize error: the RGMII packet is shorter
-		 *       than allowed.
-		 * - 9 = extend error: the RGMII packet had an extend
-		 *       error.
-		 * - 10 = length mismatch error: the RGMII packet had
-		 *        a length that did not match the length field
-		 *        in the L2 HDR.
-		 * - 11 = RGMII RX error/SPI4 DIP4 Error: the RGMII
-		 * 	  packet had one or more data reception errors
-		 * 	  (RXERR) or the SPI4 packet had one or more
-		 * 	  DIP4 errors.
-		 * - 12 = RGMII skip error/SPI4 Abort Error: the RGMII
-		 *        packet was not large enough to cover the
-		 *        skipped bytes or the SPI4 packet was
-		 *        terminated with an About EOPS.
-		 * - 13 = RGMII nibble error/SPI4 Port NXA Error: the
-		 *        RGMII packet had a studder error (data not
-		 *        repeated - 10/100M only) or the SPI4 packet
-		 *        was sent to an NXA.
-		 * - 16 = FCS error: a SPI4.2 packet had an FCS error.
-		 * - 17 = Skip error: a packet was not large enough to
-		 *        cover the skipped bytes.
-		 * - 18 = L2 header malformed: the packet is not long
-		 *        enough to contain the L2.
+		uint64_t tag_type:2;
+		uint64_t tag:32;
+	};
+	struct {
+		uint64_t len:16;
+		uint64_t zero_0:1;
+		/*
+		 * HW sets this to what it thought the priority of the input packet was
 		 */
+		uint64_t qos:3;
 
-		uint64_t rcv_error:1;
+		uint64_t zero_1:1;
 		/*
-		 * lower err_code = first-level descriptor of the
-		 * work
+		 * the group that the work queue entry will be scheduled to
 		 */
+		uint64_t grp:6;
+		uint64_t zero_2:3;
+		uint64_t tag_type:2;
+		uint64_t tag:32;
+	} cn68xx;
+	struct {
+		uint64_t len:16;
 		/*
-		 * zero for packet submitted by hardware that isn't on
-		 * the slow path
+		 * HW sets this to input physical port
 		 */
-		/* type is cvmx_pip_err_t (union, so can't use directly */
-		uint64_t err_code:8;
-	} snoip;
+		uint64_t ipprt:6;
 
-} cvmx_pip_wqe_word2;
+		/*
+		 * HW sets this to what it thought the priority of the input packet was
+		 */
+		uint64_t qos:3;
+
+		/*
+		 * the group that the work queue entry will be scheduled to
+		 */
+		uint64_t grp:4;
+		uint64_t zero_2:1;
+		uint64_t tag_type:2;
+		uint64_t tag:32;
+	} cn38xx;
+#else
+	struct {
+		uint64_t tag:32;
+		uint64_t tag_type:2;
+		uint64_t varies:14;
+		uint64_t len:16;
+	};
+	struct {
+		uint64_t tag:32;
+		uint64_t tag_type:2;
+		uint64_t zero_2:3;
+		uint64_t grp:6;
+		uint64_t zero_1:1;
+		uint64_t qos:3;
+		uint64_t zero_0:1;
+		uint64_t len:16;
+	} cn68xx;
+	struct {
+		uint64_t tag:32;
+		uint64_t tag_type:2;
+		uint64_t zero_2:1;
+		uint64_t grp:4;
+		uint64_t qos:3;
+		uint64_t ipprt:6;
+		uint64_t len:16;
+	} cn38xx;
+#endif
+} cvmx_wqe_word1_t;
 
 /**
  * Work queue entry format
@@ -307,91 +598,169 @@ typedef union {
  */
 typedef struct {
 
-    /*****************************************************************
+    /*
      * WORD 0
      *  HW WRITE: the following 64 bits are filled by HW when a packet arrives
      */
 
-    /**
-     * raw chksum result generated by the HW
-     */
-	uint16_t hw_chksum;
-    /**
-     * Field unused by hardware - available for software
-     */
-	uint8_t unused;
-    /**
-     * Next pointer used by hardware for list maintenance.
-     * May be written/read by HW before the work queue
-     *           entry is scheduled to a PP
-     * (Only 36 bits used in Octeon 1)
-     */
-	uint64_t next_ptr:40;
+	cvmx_wqe_word0_t word0;
 
     /*****************************************************************
      * WORD 1
      *  HW WRITE: the following 64 bits are filled by HW when a packet arrives
      */
 
-    /**
-     * HW sets to the total number of bytes in the packet
+	cvmx_wqe_word1_t word1;
+    /*
+     * WORD 2
+     *   HW WRITE: the following 64-bits are filled in by hardware when a packet arrives
+     *   This indicates a variety of status and error conditions.
      */
-	uint64_t len:16;
-    /**
-     * HW sets this to input physical port
-     */
-	uint64_t ipprt:6;
-
-    /**
-     * HW sets this to what it thought the priority of the input packet was
-     */
-	uint64_t qos:3;
+	cvmx_pip_wqe_word2_t word2;
 
-    /**
-     * the group that the work queue entry will be scheduled to
-     */
-	uint64_t grp:4;
-    /**
-     * the type of the tag (ORDERED, ATOMIC, NULL)
-     */
-	uint64_t tag_type:3;
-    /**
-     * the synchronization/ordering tag
-     */
-	uint64_t tag:32;
-
-    /**
-     * WORD 2 HW WRITE: the following 64-bits are filled in by
-     *   hardware when a packet arrives This indicates a variety of
-     *   status and error conditions.
-     */
-	cvmx_pip_wqe_word2 word2;
-
-    /**
+    /*
      * Pointer to the first segment of the packet.
      */
 	union cvmx_buf_ptr packet_ptr;
 
-    /**
+    /*
      *   HW WRITE: octeon will fill in a programmable amount from the
      *             packet, up to (at most, but perhaps less) the amount
      *             needed to fill the work queue entry to 128 bytes
-     *
-     *   If the packet is recognized to be IP, the hardware starts
-     *   (except that the IPv4 header is padded for appropriate
-     *   alignment) writing here where the IP header starts.  If the
-     *   packet is not recognized to be IP, the hardware starts
-     *   writing the beginning of the packet here.
+     *   If the packet is recognized to be IP, the hardware starts (except that
+     *   the IPv4 header is padded for appropriate alignment) writing here where
+     *   the IP header starts.
+     *   If the packet is not recognized to be IP, the hardware starts writing
+     *   the beginning of the packet here.
      */
 	uint8_t packet_data[96];
+} CVMX_CACHE_LINE_ALIGNED cvmx_wqe_t;
 
-    /**
-     * If desired, SW can make the work Q entry any length. For the
-     * purposes of discussion here, Assume 128B always, as this is all that
-     * the hardware deals with.
-     *
-     */
+static inline int cvmx_wqe_get_port(cvmx_wqe_t *work)
+{
+	int port;
+
+	if (octeon_has_feature(OCTEON_FEATURE_CN68XX_WQE))
+		port = work->word2.s_cn68xx.port;
+	else
+		port = work->word1.cn38xx.ipprt;
+
+	return port;
+}
+
+static inline void cvmx_wqe_set_port(cvmx_wqe_t *work, int port)
+{
+	if (octeon_has_feature(OCTEON_FEATURE_CN68XX_WQE))
+		port = work->word2.s_cn68xx.port = port;
+	else
+		port = work->word1.cn38xx.ipprt = port;
+}
+
+static inline int cvmx_wqe_get_grp(cvmx_wqe_t *work)
+{
+	int grp;
+
+	if (octeon_has_feature(OCTEON_FEATURE_CN68XX_WQE))
+		grp = work->word1.cn68xx.grp;
+	else
+		grp = work->word1.cn38xx.grp;
+
+	return grp;
+}
+
+static inline void cvmx_wqe_set_grp(cvmx_wqe_t *work, int grp)
+{
+	if (octeon_has_feature(OCTEON_FEATURE_CN68XX_WQE))
+		work->word1.cn68xx.grp = grp;
+	else
+		work->word1.cn38xx.grp = grp;
+}
+
+static inline int cvmx_wqe_get_qos(cvmx_wqe_t *work)
+{
+	int qos;
+
+	if (octeon_has_feature(OCTEON_FEATURE_CN68XX_WQE))
+		qos = work->word1.cn68xx.qos;
+	else
+		qos = work->word1.cn38xx.qos;
+
+	return qos;
+}
+
+static inline void cvmx_wqe_set_qos(cvmx_wqe_t *work, int qos)
+{
+	if (octeon_has_feature(OCTEON_FEATURE_CN68XX_WQE))
+		work->word1.cn68xx.qos = qos;
+	else
+		work->word1.cn38xx.qos = qos;
+}
+
+static inline int cvmx_wqe_get_len(cvmx_wqe_t *work)
+{
+	int len;
+
+	if (octeon_has_feature(OCTEON_FEATURE_CN68XX_WQE))
+		len = work->word1.cn68xx.len;
+	else
+		len = work->word1.cn38xx.len;
+
+	return len;
+}
+
+static inline void cvmx_wqe_set_len(cvmx_wqe_t *work, int len)
+{
+	if (octeon_has_feature(OCTEON_FEATURE_CN68XX_WQE))
+		work->word1.cn68xx.len = len;
+	else
+		work->word1.cn38xx.len = len;
+}
+
+static inline uint32_t cvmx_wqe_get_tag(cvmx_wqe_t *work)
+{
+	return work->word1.tag;
+}
+
+static inline void cvmx_wqe_set_tag(cvmx_wqe_t *work, uint32_t tag)
+{
+	work->word1.tag = tag;
+}
+
+static inline int cvmx_wqe_get_tt(cvmx_wqe_t *work)
+{
+	return work->word1.tag_type;
+}
+
+static inline void cvmx_wqe_set_tt(cvmx_wqe_t *work, int tt)
+{
+	if (octeon_has_feature(OCTEON_FEATURE_CN68XX_WQE)) {
+		work->word1.cn68xx.tag_type = tt;
+		work->word1.cn68xx.zero_2 = 0;
+	} else {
+		work->word1.cn38xx.tag_type = tt;
+		work->word1.cn38xx.zero_2 = 0;
+	}
+}
+
+static inline int cvmx_wqe_get_unused8(cvmx_wqe_t *work)
+{
+	int len;
+
+	if (octeon_has_feature(OCTEON_FEATURE_CN68XX_WQE))
+		len = work->word0.pip.cn68xx.unused1;
+	else
+		len = work->word0.pip.cn38xx.unused;
+
+	return len;
+}
+
+static inline void cvmx_wqe_set_unused8(cvmx_wqe_t *work, int v)
+{
+	if (octeon_has_feature(OCTEON_FEATURE_CN68XX_WQE))
+		work->word0.pip.cn68xx.unused1 = v;
+	else
+		work->word0.pip.cn38xx.unused = v;
+}
 
-} CVMX_CACHE_LINE_ALIGNED cvmx_wqe_t;
 
 #endif /* __CVMX_WQE_H__ */
diff --git a/drivers/staging/octeon/ethernet-rx.c b/drivers/staging/octeon/ethernet-rx.c
index 176ff9d..0fa9c45 100644
--- a/drivers/staging/octeon/ethernet-rx.c
+++ b/drivers/staging/octeon/ethernet-rx.c
@@ -146,7 +146,7 @@ static irqreturn_t cvm_oct_do_interrupt(int cpl, void *dev_id)
  */
 static inline int cvm_oct_check_rcv_error(cvmx_wqe_t *work)
 {
-	if ((work->word2.snoip.err_code == 10) && (work->len <= 64)) {
+	if ((work->word2.snoip.err_code == 10) && (work->word1.len <= 64)) {
 		/*
 		 * Ignore length errors on min size packets. Some
 		 * equipment incorrectly pads packets to 64+4FCS
@@ -166,8 +166,8 @@ static inline int cvm_oct_check_rcv_error(cvmx_wqe_t *work)
 		 * packet to determine if we can remove a non spec
 		 * preamble and generate a correct packet.
 		 */
-		int interface = cvmx_helper_get_interface_num(work->ipprt);
-		int index = cvmx_helper_get_interface_index_num(work->ipprt);
+		int interface = cvmx_helper_get_interface_num(work->word1.cn38xx.ipprt);
+		int index = cvmx_helper_get_interface_index_num(work->word1.cn38xx.ipprt);
 		union cvmx_gmxx_rxx_frm_ctl gmxx_rxx_frm_ctl;
 		gmxx_rxx_frm_ctl.u64 =
 		    cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface));
@@ -177,7 +177,7 @@ static inline int cvm_oct_check_rcv_error(cvmx_wqe_t *work)
 			    cvmx_phys_to_ptr(work->packet_ptr.s.addr);
 			int i = 0;
 
-			while (i < work->len - 1) {
+			while (i < work->word1.len - 1) {
 				if (*ptr != 0x55)
 					break;
 				ptr++;
@@ -189,14 +189,14 @@ static inline int cvm_oct_check_rcv_error(cvmx_wqe_t *work)
 				  printk_ratelimited("Port %d received 0xd5 preamble\n", work->ipprt);
 				 */
 				work->packet_ptr.s.addr += i + 1;
-				work->len -= i + 5;
+				work->word1.len -= i + 5;
 			} else if ((*ptr & 0xf) == 0xd) {
 				/*
 				  printk_ratelimited("Port %d received 0x?d preamble\n", work->ipprt);
 				 */
 				work->packet_ptr.s.addr += i;
-				work->len -= i + 4;
-				for (i = 0; i < work->len; i++) {
+				work->word1.len -= i + 4;
+				for (i = 0; i < work->word1.len; i++) {
 					*ptr =
 					    ((*ptr & 0xf0) >> 4) |
 					    ((*(ptr + 1) & 0xf) << 4);
@@ -205,7 +205,7 @@ static inline int cvm_oct_check_rcv_error(cvmx_wqe_t *work)
 			} else {
 				printk_ratelimited("Port %d unknown preamble, packet "
 						   "dropped\n",
-						   work->ipprt);
+						   work->word1.cn38xx.ipprt);
 				/*
 				   cvmx_helper_dump_packet(work);
 				 */
@@ -215,7 +215,7 @@ static inline int cvm_oct_check_rcv_error(cvmx_wqe_t *work)
 		}
 	} else {
 		printk_ratelimited("Port %d receive error code %d, packet dropped\n",
-				   work->ipprt, work->word2.snoip.err_code);
+				   work->word1.cn38xx.ipprt, work->word2.snoip.err_code);
 		cvm_oct_free_work(work);
 		return 1;
 	}
@@ -308,7 +308,7 @@ static int cvm_oct_napi_poll(struct napi_struct *napi, int budget)
 			prefetch(&skb->head);
 			prefetch(&skb->len);
 		}
-		prefetch(cvm_oct_device[work->ipprt]);
+		prefetch(cvm_oct_device[work->word1.cn38xx.ipprt]);
 
 		/* Immediately throw away all packets with receive errors */
 		if (unlikely(work->word2.snoip.rcv_error)) {
@@ -324,7 +324,7 @@ static int cvm_oct_napi_poll(struct napi_struct *napi, int budget)
 		if (likely(skb_in_hw)) {
 			skb->data = skb->head + work->packet_ptr.s.addr - cvmx_ptr_to_phys(skb->head);
 			prefetch(skb->data);
-			skb->len = work->len;
+			skb->len = work->word1.len;
 			skb_set_tail_pointer(skb, skb->len);
 			packet_not_copied = 1;
 		} else {
@@ -332,11 +332,10 @@ static int cvm_oct_napi_poll(struct napi_struct *napi, int budget)
 			 * We have to copy the packet. First allocate
 			 * an skbuff for it.
 			 */
-			skb = dev_alloc_skb(work->len);
+			skb = dev_alloc_skb(work->word1.len);
 			if (!skb) {
-				printk_ratelimited("Port %d failed to allocate "
-						   "skbuff, packet dropped\n",
-						   work->ipprt);
+				printk_ratelimited("Port %d failed to allocate skbuff, packet dropped\n",
+						   work->word1.cn38xx.ipprt);
 				cvm_oct_free_work(work);
 				continue;
 			}
@@ -358,12 +357,12 @@ static int cvm_oct_napi_poll(struct napi_struct *napi, int budget)
 					else
 						ptr += 6;
 				}
-				memcpy(skb_put(skb, work->len), ptr, work->len);
+				memcpy(skb_put(skb, work->word1.len), ptr, work->word1.len);
 				/* No packet buffers to free */
 			} else {
 				int segments = work->word2.s.bufs;
 				union cvmx_buf_ptr segment_ptr = work->packet_ptr;
-				int len = work->len;
+				int len = work->word1.len;
 
 				while (segments--) {
 					union cvmx_buf_ptr next_ptr =
@@ -397,9 +396,9 @@ static int cvm_oct_napi_poll(struct napi_struct *napi, int budget)
 			packet_not_copied = 0;
 		}
 
-		if (likely((work->ipprt < TOTAL_NUMBER_OF_PORTS) &&
-			   cvm_oct_device[work->ipprt])) {
-			struct net_device *dev = cvm_oct_device[work->ipprt];
+		if (likely((work->word1.cn38xx.ipprt < TOTAL_NUMBER_OF_PORTS) &&
+			   cvm_oct_device[work->word1.cn38xx.ipprt])) {
+			struct net_device *dev = cvm_oct_device[work->word1.cn38xx.ipprt];
 			struct octeon_ethernet *priv = netdev_priv(dev);
 
 			/*
@@ -417,7 +416,7 @@ static int cvm_oct_napi_poll(struct napi_struct *napi, int budget)
 					skb->ip_summed = CHECKSUM_UNNECESSARY;
 
 				/* Increment RX stats for virtual ports */
-				if (work->ipprt >= CVMX_PIP_NUM_INPUT_PORTS) {
+				if (work->word1.cn38xx.ipprt >= CVMX_PIP_NUM_INPUT_PORTS) {
 #ifdef CONFIG_64BIT
 					atomic64_add(1, (atomic64_t *)&priv->stats.rx_packets);
 					atomic64_add(skb->len, (atomic64_t *)&priv->stats.rx_bytes);
@@ -447,7 +446,7 @@ static int cvm_oct_napi_poll(struct napi_struct *napi, int budget)
 			 * doesn't exist.
 			 */
 			printk_ratelimited("Port %d not controlled by Linux, packet dropped\n",
-				   work->ipprt);
+					   work->word1.cn38xx.ipprt);
 			dev_kfree_skb_irq(skb);
 		}
 		/*
-- 
1.7.5.4

