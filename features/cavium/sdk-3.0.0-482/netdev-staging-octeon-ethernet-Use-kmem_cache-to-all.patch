From c2619e1d0205614613a7d6106fc63c9ea4057631 Mon Sep 17 00:00:00 2001
From: David Daney <david.daney@cavium.com>
Date: Mon, 2 Jul 2012 16:13:31 -0700
Subject: [PATCH 106/337] netdev/staging: octeon-ethernet: Use kmem_cache to
 allocate working memory.

Based On SDK 3.0.0-482

... and also create a mechanism to manage FPA allocation and sharing.

Signed-off-by: David Daney <david.daney@cavium.com>
Signed-off-by: Corey Minyard <cminyard@mvista.com>
Signed-off-by: Jack Tan <jack.tan@windriver.com>
---
 drivers/staging/octeon/ethernet-mem.c    |  281 ++++++++++++++++++++++--------
 drivers/staging/octeon/ethernet-rx.c     |    2 +-
 drivers/staging/octeon/ethernet-tx.c     |    2 +-
 drivers/staging/octeon/ethernet.c        |   53 +++++--
 drivers/staging/octeon/octeon-ethernet.h |   27 +++-
 5 files changed, 275 insertions(+), 90 deletions(-)

diff --git a/drivers/staging/octeon/ethernet-mem.c b/drivers/staging/octeon/ethernet-mem.c
index 78b6cb7..c27ddd8 100644
--- a/drivers/staging/octeon/ethernet-mem.c
+++ b/drivers/staging/octeon/ethernet-mem.c
@@ -1,10 +1,10 @@
 /**********************************************************************
- * Author: Cavium Networks
+ * Author: Cavium, Inc.
  *
- * Contact: support@caviumnetworks.com
+ * Contact: support@cavium.com
  * This file is part of the OCTEON SDK
  *
- * Copyright (c) 2003-2010 Cavium Networks
+ * Copyright (c) 2003-2012 Cavium, Inc.
  *
  * This file is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License, Version 2, as
@@ -22,17 +22,34 @@
  * or visit http://www.gnu.org/licenses/.
  *
  * This file may also be available under a different license from Cavium.
- * Contact Cavium Networks for more information
+ * Contact Cavium, Inc. for more information
 **********************************************************************/
-#include <linux/kernel.h>
 #include <linux/netdevice.h>
+#include <linux/export.h>
+#include <linux/kernel.h>
 #include <linux/slab.h>
 
 #include <asm/octeon/octeon.h>
+#include <asm/octeon/cvmx-fpa.h>
 
 #include "ethernet-defines.h"
+#include "octeon-ethernet.h"
 
-#include <asm/octeon/cvmx-fpa.h>
+struct fpa_pool {
+	int pool;
+	int users;
+	int size;
+	char kmem_name[20];
+	struct kmem_cache *kmem;
+	int (*fill)(struct fpa_pool *p, int num);
+	int (*empty)(struct fpa_pool *p, int num);
+};
+
+static DEFINE_SPINLOCK(cvm_oct_pools_lock);
+/* Eight pools. */
+static struct fpa_pool cvm_oct_pools[] = {
+	{-1}, {-1}, {-1}, {-1}, {-1}, {-1}, {-1}, {-1}
+};
 
 /**
  * cvm_oct_fill_hw_skbuff - fill the supplied hardware pool with skbuffs
@@ -42,22 +59,26 @@
  *
  * Returns the actual number of buffers allocated.
  */
-static int cvm_oct_fill_hw_skbuff(int pool, int size, int elements)
+static int cvm_oct_fill_hw_skbuff(struct fpa_pool *pool, int elements)
 {
 	int freed = elements;
+	int size = pool->size;
+	int pool_num = pool->pool;
 	while (freed) {
-
-		struct sk_buff *skb = dev_alloc_skb(size + 256);
-		if (unlikely(skb == NULL)) {
-			pr_warning
-			    ("Failed to allocate skb for hardware pool %d\n",
-			     pool);
+		int extra_reserve;
+		u8 *desired_data;
+		struct sk_buff *skb = alloc_skb(size + CVM_OCT_SKB_TO_FPA_PADDING,
+						GFP_KERNEL);
+		if (skb == NULL) {
+			pr_err("Failed to allocate skb for hardware pool %d\n",
+			       pool_num);
 			break;
 		}
-
-		skb_reserve(skb, 256 - (((unsigned long)skb->data) & 0x7f));
+		desired_data = cvm_oct_get_fpa_head(skb);
+		extra_reserve = desired_data - skb->data;
+		skb_reserve(skb, extra_reserve);
 		*(struct sk_buff **)(skb->data - sizeof(void *)) = skb;
-		cvmx_fpa_free(skb->data, pool, DONT_WRITEBACK(size / 128));
+		cvmx_fpa_free(skb->data, pool_num, DONT_WRITEBACK(size / 128));
 		freed--;
 	}
 	return elements - freed;
@@ -69,26 +90,28 @@ static int cvm_oct_fill_hw_skbuff(int pool, int size, int elements)
  * @size:     Size of the buffer needed for the pool
  * @elements: Number of buffers to allocate
  */
-static void cvm_oct_free_hw_skbuff(int pool, int size, int elements)
+static int cvm_oct_free_hw_skbuff(struct fpa_pool *pool, int elements)
 {
 	char *memory;
+	int pool_num = pool->pool;
 
 	do {
-		memory = cvmx_fpa_alloc(pool);
+		memory = cvmx_fpa_alloc(pool_num);
 		if (memory) {
-			struct sk_buff *skb =
-			    *(struct sk_buff **)(memory - sizeof(void *));
+			struct sk_buff *skb = *cvm_oct_packet_to_skb(memory);
 			elements--;
 			dev_kfree_skb(skb);
 		}
 	} while (memory);
 
 	if (elements < 0)
-		pr_warning("Freeing of pool %u had too many skbuffs (%d)\n",
-		     pool, elements);
+		pr_err("Freeing of pool %u had too many skbuffs (%d)\n",
+		       pool_num, elements);
 	else if (elements > 0)
-		pr_warning("Freeing of pool %u is missing %d skbuffs\n",
-		       pool, elements);
+		pr_err("Freeing of pool %u is missing %d skbuffs\n",
+		       pool_num, elements);
+
+	return 0;
 }
 
 /**
@@ -99,32 +122,19 @@ static void cvm_oct_free_hw_skbuff(int pool, int size, int elements)
  *
  * Returns the actual number of buffers allocated.
  */
-static int cvm_oct_fill_hw_memory(int pool, int size, int elements)
+static int cvm_oct_fill_hw_kmem(struct fpa_pool *pool, int elements)
 {
 	char *memory;
-	char *fpa;
 	int freed = elements;
 
 	while (freed) {
-		/*
-		 * FPA memory must be 128 byte aligned.  Since we are
-		 * aligning we need to save the original pointer so we
-		 * can feed it to kfree when the memory is returned to
-		 * the kernel.
-		 *
-		 * We allocate an extra 256 bytes to allow for
-		 * alignment and space for the original pointer saved
-		 * just before the block.
-		 */
-		memory = kmalloc(size + 256, GFP_ATOMIC);
+		memory = kmem_cache_alloc(pool->kmem, GFP_KERNEL);
 		if (unlikely(memory == NULL)) {
-			pr_warning("Unable to allocate %u bytes for FPA pool %d\n",
-				   elements * size, pool);
+			pr_err("Unable to allocate %u bytes for FPA pool %d\n",
+			       elements * pool->size, pool->pool);
 			break;
 		}
-		fpa = (char *)(((unsigned long)memory + 256) & ~0x7fUL);
-		*((char **)fpa - 1) = memory;
-		cvmx_fpa_free(fpa, pool, 0);
+		cvmx_fpa_free(memory, pool->pool, 0);
 		freed--;
 	}
 	return elements - freed;
@@ -136,42 +146,175 @@ static int cvm_oct_fill_hw_memory(int pool, int size, int elements)
  * @size:     Size of each buffer in the pool
  * @elements: Number of buffers that should be in the pool
  */
-static void cvm_oct_free_hw_memory(int pool, int size, int elements)
+static int cvm_oct_free_hw_kmem(struct fpa_pool *pool, int elements)
 {
-	char *memory;
 	char *fpa;
-	do {
-		fpa = cvmx_fpa_alloc(pool);
-		if (fpa) {
-			elements--;
-			fpa = (char *)phys_to_virt(cvmx_ptr_to_phys(fpa));
-			memory = *((char **)fpa - 1);
-			kfree(memory);
-		}
-	} while (fpa);
+	while (elements) {
+		fpa = cvmx_fpa_alloc(pool->pool);
+		if (!fpa)
+			break;
+		elements--;
+		kmem_cache_free(pool->kmem, fpa);
+	}
 
 	if (elements < 0)
-		pr_warning("Freeing of pool %u had too many buffers (%d)\n",
-			pool, elements);
+		pr_err("Freeing of pool %u had too many buffers (%d)\n",
+		       pool->pool, elements);
 	else if (elements > 0)
-		pr_warning("Warning: Freeing of pool %u is missing %d buffers\n",
-			pool, elements);
+		pr_err("Warning: Freeing of pool %u is missing %d buffers\n",
+		       pool->pool, elements);
+	return elements;
 }
 
-int cvm_oct_mem_fill_fpa(int pool, int size, int elements)
+int cvm_oct_mem_fill_fpa(int pool, int elements)
 {
-	int freed;
-	if (USE_SKBUFFS_IN_HW && pool == CVMX_FPA_PACKET_POOL)
-		freed = cvm_oct_fill_hw_skbuff(pool, size, elements);
-	else
-		freed = cvm_oct_fill_hw_memory(pool, size, elements);
-	return freed;
+	struct fpa_pool *p;
+
+	if (pool < 0 || pool >= ARRAY_SIZE(cvm_oct_pools))
+		return -EINVAL;
+
+	p = cvm_oct_pools + pool;
+
+	return p->fill(p, elements);
 }
+EXPORT_SYMBOL(cvm_oct_mem_fill_fpa);
 
-void cvm_oct_mem_empty_fpa(int pool, int size, int elements)
+int cvm_oct_mem_empty_fpa(int pool, int elements)
 {
-	if (USE_SKBUFFS_IN_HW && pool == CVMX_FPA_PACKET_POOL)
-		cvm_oct_free_hw_skbuff(pool, size, elements);
-	else
-		cvm_oct_free_hw_memory(pool, size, elements);
+	struct fpa_pool *p;
+
+	if (pool < 0 || pool >= ARRAY_SIZE(cvm_oct_pools))
+		return -EINVAL;
+
+	p = cvm_oct_pools + pool;
+	if (p->empty)
+		return p->empty(p, elements);
+
+	return 0;
+}
+EXPORT_SYMBOL(cvm_oct_mem_empty_fpa);
+
+void cvm_oct_mem_cleanup(void)
+{
+	int i;
+
+	spin_lock(&cvm_oct_pools_lock);
+
+	for (i = 0; i < ARRAY_SIZE(cvm_oct_pools); i++)
+		if (cvm_oct_pools[i].kmem)
+			kmem_cache_shrink(cvm_oct_pools[i].kmem);
+	spin_unlock(&cvm_oct_pools_lock);
+}
+EXPORT_SYMBOL(cvm_oct_mem_cleanup);
+
+/**
+ * cvm_oct_alloc_fpa_pool() - Allocate an FPA pool of the given size
+ * @pool:  Requested pool number (-1 for don't care).
+ * @size:  The size of the pool elements.
+ *
+ * Returns the pool number or a negative number on error.
+ */
+int cvm_oct_alloc_fpa_pool(int pool, int size)
+{
+	int i;
+	int ret = 0;
+
+	if (pool >= (int)ARRAY_SIZE(cvm_oct_pools) || size < 128)
+		return -EINVAL;
+
+	spin_lock(&cvm_oct_pools_lock);
+
+	if (pool >= 0) {
+		if (cvm_oct_pools[pool].pool != -1) {
+			if (cvm_oct_pools[pool].size == size) {
+				/* Already allocated */
+				cvm_oct_pools[pool].users++;
+				ret = pool;
+				goto out;
+			} else {
+				/* conflict */
+				ret = -EINVAL;
+				goto out;
+			}
+		}
+	} else {
+		/* Find an established pool */
+		for (i = 0; i < ARRAY_SIZE(cvm_oct_pools); i++)
+			if (cvm_oct_pools[i].pool >= 0 &&
+			    cvm_oct_pools[i].size == size) {
+				cvm_oct_pools[i].users++;
+				ret = i;
+				goto out;
+			}
+
+		/* Find an empty pool */
+		for (i = 0; i < ARRAY_SIZE(cvm_oct_pools); i++)
+			if (cvm_oct_pools[i].pool == -1) {
+				pool = i;
+				break;
+			}
+		if (pool < 0) {
+			/* No empties. */
+			ret = -EINVAL;
+			goto out;
+		}
+	}
+
+	/* Setup the pool */
+	cvm_oct_pools[pool].pool = pool;
+	cvm_oct_pools[pool].users++;
+	cvm_oct_pools[pool].size = size;
+	if (USE_SKBUFFS_IN_HW && pool == 0) {
+		/* Special packet pool */
+		cvm_oct_pools[pool].fill = cvm_oct_fill_hw_skbuff;
+		cvm_oct_pools[pool].empty = cvm_oct_free_hw_skbuff;
+	} else {
+		snprintf(cvm_oct_pools[pool].kmem_name,
+			 sizeof(cvm_oct_pools[pool].kmem_name),
+			 "oct-fpa-%d", size);
+		cvm_oct_pools[pool].fill = cvm_oct_fill_hw_kmem;
+		cvm_oct_pools[pool].empty = cvm_oct_free_hw_kmem;
+		cvm_oct_pools[pool].kmem =
+			kmem_cache_create(cvm_oct_pools[pool].kmem_name,
+					  size, 128, 0, NULL);
+		if (!cvm_oct_pools[pool].kmem) {
+			ret = -ENOMEM;
+			cvm_oct_pools[pool].pool = -1;
+			goto out;
+		}
+	}
+	ret = pool;
+out:
+	spin_unlock(&cvm_oct_pools_lock);
+	return ret;
+}
+EXPORT_SYMBOL(cvm_oct_alloc_fpa_pool);
+
+/**
+ * cvm_oct_release_fpa_pool() - Releases an FPA pool
+ * @pool:  Pool number.
+ *
+ * This undoes the action of cvm_oct_alloc_fpa_pool().
+ *
+ * Returns zero on success.
+ */
+int cvm_oct_release_fpa_pool(int pool)
+{
+	int ret = -EINVAL;
+
+	if (pool < 0 || pool >= (int)ARRAY_SIZE(cvm_oct_pools))
+		return ret;
+
+	spin_lock(&cvm_oct_pools_lock);
+
+	if (cvm_oct_pools[pool].users <= 0) {
+		pr_err("Error: Unbalanced FPA pool allocation\n");
+		goto out;
+	}
+	cvm_oct_pools[pool].users--;
+	ret = 0;
+out:
+	spin_unlock(&cvm_oct_pools_lock);
+	return ret;
 }
+EXPORT_SYMBOL(cvm_oct_release_fpa_pool);
diff --git a/drivers/staging/octeon/ethernet-rx.c b/drivers/staging/octeon/ethernet-rx.c
index bf63c64..c5557da 100644
--- a/drivers/staging/octeon/ethernet-rx.c
+++ b/drivers/staging/octeon/ethernet-rx.c
@@ -280,7 +280,7 @@ static int cvm_oct_napi_poll(struct napi_struct *napi, int budget)
 			break;
 		}
 		packet_ptr = work->packet_ptr;
-		pskb = (struct sk_buff **)(cvm_oct_get_buffer_ptr(work->packet_ptr) - sizeof(void *));
+		pskb = cvm_oct_packet_to_skb(cvm_oct_get_buffer_ptr(packet_ptr));
 		prefetch(pskb);
 
 		if (USE_ASYNC_IOBDMA && rx_count < (budget - 1)) {
diff --git a/drivers/staging/octeon/ethernet-tx.c b/drivers/staging/octeon/ethernet-tx.c
index fa868c3..5da4122 100644
--- a/drivers/staging/octeon/ethernet-tx.c
+++ b/drivers/staging/octeon/ethernet-tx.c
@@ -230,7 +230,7 @@ int cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev)
 	 * shown a 25% increase in performance under some loads.
 	 */
 #if REUSE_SKBUFFS_WITHOUT_FREE
-	fpa_head = skb->head + 256 - ((unsigned long)skb->head & 0x7f);
+	fpa_head = cvm_oct_get_fpa_head(skb);
 	if (unlikely(skb->data < fpa_head)) {
 		/*
 		 * printk("TX buffer beginning can't meet FPA
diff --git a/drivers/staging/octeon/ethernet.c b/drivers/staging/octeon/ethernet.c
index e1c32ca..4cb300f 100644
--- a/drivers/staging/octeon/ethernet.c
+++ b/drivers/staging/octeon/ethernet.c
@@ -138,19 +138,39 @@ static void cvm_oct_periodic_worker(struct work_struct *work)
 
 	if (!atomic_read(&cvm_oct_poll_queue_stopping))
 		queue_delayed_work(cvm_oct_poll_queue, &priv->port_periodic_work, HZ);
- }
+}
+
+static int cvm_oct_num_output_buffers;
 
 static __devinit void cvm_oct_configure_common_hw(void)
 {
 	/* Setup the FPA */
 	cvmx_fpa_enable();
-	cvm_oct_mem_fill_fpa(CVMX_FPA_PACKET_POOL, CVMX_FPA_PACKET_POOL_SIZE,
-			     num_packet_buffers);
-	cvm_oct_mem_fill_fpa(CVMX_FPA_WQE_POOL, CVMX_FPA_WQE_POOL_SIZE,
-			     num_packet_buffers);
-	if (CVMX_FPA_OUTPUT_BUFFER_POOL != CVMX_FPA_PACKET_POOL)
+
+	if (cvm_oct_alloc_fpa_pool(CVMX_FPA_PACKET_POOL,
+				   CVMX_FPA_PACKET_POOL_SIZE) < 0) {
+		pr_err("cvm_oct_alloc_fpa_pool(CVMX_FPA_PACKET_POOL, CVMX_FPA_PACKET_POOL_SIZE) failed.\n");
+		return;
+	}
+	cvm_oct_mem_fill_fpa(CVMX_FPA_PACKET_POOL, num_packet_buffers);
+
+	if (cvm_oct_alloc_fpa_pool(CVMX_FPA_WQE_POOL,
+				   CVMX_FPA_WQE_POOL_SIZE) < 0) {
+		pr_err("cvm_oct_alloc_fpa_pool(CVMX_FPA_WQE_POOL, CVMX_FPA_WQE_POOL_SIZE) failed.\n");
+		return;
+	}
+	cvm_oct_mem_fill_fpa(CVMX_FPA_WQE_POOL, num_packet_buffers);
+
+	if (CVMX_FPA_OUTPUT_BUFFER_POOL != CVMX_FPA_PACKET_POOL) {
+		cvm_oct_num_output_buffers = 128;
+		if (cvm_oct_alloc_fpa_pool(CVMX_FPA_OUTPUT_BUFFER_POOL,
+					   CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE) < 0) {
+			pr_err("cvm_oct_alloc_fpa_pool(CVMX_FPA_OUTPUT_BUFFER_POOL, CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE) failed.\n");
+			return;
+		}
 		cvm_oct_mem_fill_fpa(CVMX_FPA_OUTPUT_BUFFER_POOL,
-				     CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE, 128);
+				     cvm_oct_num_output_buffers);
+	}
 
 	if (USE_RED)
 		cvmx_helper_setup_red(num_packet_buffers / 4,
@@ -698,7 +718,7 @@ static int __devinit cvm_oct_probe(struct platform_device *pdev)
 				 * own MAX_OUT_QUEUE_DEPTH worth of
 				 * WQE to track the transmit skbs.
 				 */
-				cvm_oct_mem_fill_fpa(CVMX_FPA_WQE_POOL, CVMX_FPA_WQE_POOL_SIZE,
+				cvm_oct_mem_fill_fpa(CVMX_FPA_WQE_POOL,
 						     PER_DEVICE_EXTRA_WQE);
 				num_devices_extra_wqe++;
 				queue_delayed_work(cvm_oct_poll_queue,
@@ -759,13 +779,20 @@ static int __devexit cvm_oct_remove(struct platform_device *pdev)
 	destroy_workqueue(cvm_oct_poll_queue);
 
 	/* Free the HW pools */
-	cvm_oct_mem_empty_fpa(CVMX_FPA_PACKET_POOL, CVMX_FPA_PACKET_POOL_SIZE,
-			      num_packet_buffers);
-	cvm_oct_mem_empty_fpa(CVMX_FPA_WQE_POOL, CVMX_FPA_WQE_POOL_SIZE,
+	cvm_oct_mem_empty_fpa(CVMX_FPA_PACKET_POOL, num_packet_buffers);
+	cvm_oct_release_fpa_pool(CVMX_FPA_PACKET_POOL);
+
+	cvm_oct_mem_empty_fpa(CVMX_FPA_WQE_POOL,
 			      num_packet_buffers + num_devices_extra_wqe * PER_DEVICE_EXTRA_WQE);
-	if (CVMX_FPA_OUTPUT_BUFFER_POOL != CVMX_FPA_PACKET_POOL)
+	cvm_oct_release_fpa_pool(CVMX_FPA_WQE_POOL);
+
+	if (CVMX_FPA_OUTPUT_BUFFER_POOL != CVMX_FPA_PACKET_POOL) {
 		cvm_oct_mem_empty_fpa(CVMX_FPA_OUTPUT_BUFFER_POOL,
-				      CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE, 128);
+				      cvm_oct_num_output_buffers);
+		cvm_oct_release_fpa_pool(CVMX_FPA_OUTPUT_BUFFER_POOL);
+	}
+	cvm_oct_mem_cleanup();
+
 	return 0;
 }
 
diff --git a/drivers/staging/octeon/octeon-ethernet.h b/drivers/staging/octeon/octeon-ethernet.h
index 0881916..8b34e82 100644
--- a/drivers/staging/octeon/octeon-ethernet.h
+++ b/drivers/staging/octeon/octeon-ethernet.h
@@ -118,8 +118,11 @@ void cvm_oct_poll_controller(struct net_device *dev);
 void cvm_oct_rx_initialize(void);
 void cvm_oct_rx_shutdown(void);
 
-int cvm_oct_mem_fill_fpa(int pool, int size, int elements);
-void cvm_oct_mem_empty_fpa(int pool, int size, int elements);
+int cvm_oct_mem_fill_fpa(int pool, int elements);
+int cvm_oct_mem_empty_fpa(int pool, int elements);
+int cvm_oct_alloc_fpa_pool(int pool, int size);
+int cvm_oct_release_fpa_pool(int pool);
+void cvm_oct_mem_cleanup(void);
 
 extern const struct ethtool_ops cvm_oct_ethtool_ops;
 
@@ -147,7 +150,6 @@ static inline void cvm_oct_rx_refill_pool(int fill_threshold)
 		cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE,
 				      -number_to_free);
 		num_freed = cvm_oct_mem_fill_fpa(CVMX_FPA_PACKET_POOL,
-						 CVMX_FPA_PACKET_POOL_SIZE,
 						 number_to_free);
 		if (num_freed != number_to_free) {
 			cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE,
@@ -158,13 +160,26 @@ static inline void cvm_oct_rx_refill_pool(int fill_threshold)
 
 /**
  * cvm_oct_get_buffer_ptr - convert packet data address to pointer
- * @packet_ptr: Packet data hardware address
+ * @pd: Packet data hardware address
  *
  * Returns Packet buffer pointer
  */
-static inline void *cvm_oct_get_buffer_ptr(union cvmx_buf_ptr packet_ptr)
+static inline void *cvm_oct_get_buffer_ptr(union cvmx_buf_ptr pd)
 {
-	return cvmx_phys_to_ptr(((packet_ptr.s.addr >> 7) - packet_ptr.s.back) << 7);
+	return phys_to_virt(((pd.s.addr >> 7) - pd.s.back) << 7);
+}
+
+static inline struct sk_buff **cvm_oct_packet_to_skb(void *packet)
+{
+	char *p = packet;
+	return (struct sk_buff **)(p - sizeof(void *));
+}
+
+#define CVM_OCT_SKB_TO_FPA_PADDING (128 + sizeof(void *) - 1)
+
+static inline u8 *cvm_oct_get_fpa_head(struct sk_buff *skb)
+{
+	return (u8 *)((unsigned long)(skb->head + CVM_OCT_SKB_TO_FPA_PADDING) & ~0x7ful);
 }
 
 #endif
-- 
1.7.5.4

