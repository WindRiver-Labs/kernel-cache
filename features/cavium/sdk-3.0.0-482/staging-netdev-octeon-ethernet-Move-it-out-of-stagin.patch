From c657fbbdc3242162bd8289dc1e087fb4bd23f015 Mon Sep 17 00:00:00 2001
From: David Daney <david.daney@cavium.com>
Date: Wed, 8 Aug 2012 17:46:44 -0700
Subject: [PATCH 135/337] staging/netdev: octeon-ethernet: Move it out of
 staging.

Based On SDK 3.0.0-482

Signed-off-by: David Daney <david.daney@cavium.com>
Signed-off-by: Corey Minyard <cminyard@mvista.com>
Signed-off-by: Jack Tan <jack.tan@windriver.com>
---
 drivers/net/ethernet/Makefile                  |    2 +-
 drivers/net/ethernet/octeon/Kconfig            |   17 +
 drivers/net/ethernet/octeon/Makefile           |   10 +
 drivers/net/ethernet/octeon/ethernet-defines.h |  105 +++
 drivers/net/ethernet/octeon/ethernet-mdio.c    |  319 +++++++++
 drivers/net/ethernet/octeon/ethernet-mem.c     |  320 +++++++++
 drivers/net/ethernet/octeon/ethernet-rgmii.c   |  354 ++++++++++
 drivers/net/ethernet/octeon/ethernet-rx.c      |  684 ++++++++++++++++++++
 drivers/net/ethernet/octeon/ethernet-sgmii.c   |  122 ++++
 drivers/net/ethernet/octeon/ethernet-spi.c     |  283 ++++++++
 drivers/net/ethernet/octeon/ethernet-tx.c      |  487 ++++++++++++++
 drivers/net/ethernet/octeon/ethernet.c         |  825 ++++++++++++++++++++++++
 drivers/net/ethernet/octeon/octeon-ethernet.h  |  189 ++++++
 drivers/staging/Kconfig                        |    2 -
 drivers/staging/Makefile                       |    1 -
 drivers/staging/octeon/Kconfig                 |   13 -
 drivers/staging/octeon/Makefile                |   22 -
 drivers/staging/octeon/ethernet-defines.h      |  105 ---
 drivers/staging/octeon/ethernet-mdio.c         |  319 ---------
 drivers/staging/octeon/ethernet-mem.c          |  320 ---------
 drivers/staging/octeon/ethernet-rgmii.c        |  354 ----------
 drivers/staging/octeon/ethernet-rx.c           |  684 --------------------
 drivers/staging/octeon/ethernet-sgmii.c        |  122 ----
 drivers/staging/octeon/ethernet-spi.c          |  283 --------
 drivers/staging/octeon/ethernet-tx.c           |  487 --------------
 drivers/staging/octeon/ethernet.c              |  825 ------------------------
 drivers/staging/octeon/octeon-ethernet.h       |  189 ------
 27 files changed, 3716 insertions(+), 3727 deletions(-)
 create mode 100644 drivers/net/ethernet/octeon/ethernet-defines.h
 create mode 100644 drivers/net/ethernet/octeon/ethernet-mdio.c
 create mode 100644 drivers/net/ethernet/octeon/ethernet-mem.c
 create mode 100644 drivers/net/ethernet/octeon/ethernet-rgmii.c
 create mode 100644 drivers/net/ethernet/octeon/ethernet-rx.c
 create mode 100644 drivers/net/ethernet/octeon/ethernet-sgmii.c
 create mode 100644 drivers/net/ethernet/octeon/ethernet-spi.c
 create mode 100644 drivers/net/ethernet/octeon/ethernet-tx.c
 create mode 100644 drivers/net/ethernet/octeon/ethernet.c
 create mode 100644 drivers/net/ethernet/octeon/octeon-ethernet.h
 delete mode 100644 drivers/staging/octeon/Kconfig
 delete mode 100644 drivers/staging/octeon/Makefile
 delete mode 100644 drivers/staging/octeon/ethernet-defines.h
 delete mode 100644 drivers/staging/octeon/ethernet-mdio.c
 delete mode 100644 drivers/staging/octeon/ethernet-mem.c
 delete mode 100644 drivers/staging/octeon/ethernet-rgmii.c
 delete mode 100644 drivers/staging/octeon/ethernet-rx.c
 delete mode 100644 drivers/staging/octeon/ethernet-sgmii.c
 delete mode 100644 drivers/staging/octeon/ethernet-spi.c
 delete mode 100644 drivers/staging/octeon/ethernet-tx.c
 delete mode 100644 drivers/staging/octeon/ethernet.c
 delete mode 100644 drivers/staging/octeon/octeon-ethernet.h

diff --git a/drivers/net/ethernet/Makefile b/drivers/net/ethernet/Makefile
index ae0e21c..30d69e9 100644
--- a/drivers/net/ethernet/Makefile
+++ b/drivers/net/ethernet/Makefile
@@ -48,7 +48,7 @@ obj-$(CONFIG_NET_NETX) += netx-eth.o
 obj-$(CONFIG_NET_VENDOR_NUVOTON) += nuvoton/
 obj-$(CONFIG_NET_VENDOR_NVIDIA) += nvidia/
 obj-$(CONFIG_LPC_ENET) += nxp/
-obj-$(CONFIG_OCTEON_MGMT_ETHERNET) += octeon/
+obj-$(CONFIG_NET_VENDOR_OCTEON) += octeon/
 obj-$(CONFIG_NET_VENDOR_OKI) += oki-semi/
 obj-$(CONFIG_KGDBOE) += kgdboe.o
 obj-$(CONFIG_ETHOC) += ethoc.o
diff --git a/drivers/net/ethernet/octeon/Kconfig b/drivers/net/ethernet/octeon/Kconfig
index 3de52ff..02967a6 100644
--- a/drivers/net/ethernet/octeon/Kconfig
+++ b/drivers/net/ethernet/octeon/Kconfig
@@ -1,14 +1,31 @@
 #
 # Cavium network device configuration
 #
+config OCTEON_ETHERNET
+	tristate "Cavium Inc. OCTEON Ethernet support"
+	depends on CPU_CAVIUM_OCTEON
+	select PHYLIB
+	select MDIO_OCTEON
+	select NET_VENDOR_OCTEON
+	help
+	  This driver supports the builtin ethernet ports on Cavium
+	  Inc.' products in the Octeon family. This driver supports the
+	  CN3XXX, CN5XXX, CN6XXX and CNF7XXX OCTEON processors.
+
+	  To compile this driver as a module, choose M here.  The module
+	  will be called octeon-ethernet.
 
 config OCTEON_MGMT_ETHERNET
 	tristate "Octeon Management port ethernet driver (CN5XXX, CN6XXX)"
 	depends on  CPU_CAVIUM_OCTEON
 	select PHYLIB
 	select MDIO_OCTEON
+	select NET_VENDOR_OCTEON
 	default y
 	---help---
 	  This option enables the ethernet driver for the management
 	  port on Cavium Networks' Octeon CN57XX, CN56XX, CN55XX,
 	  CN54XX, CN52XX, and CN6XXX chips.
+
+config NET_VENDOR_OCTEON
+	bool
diff --git a/drivers/net/ethernet/octeon/Makefile b/drivers/net/ethernet/octeon/Makefile
index efa41c1..6dddc6c 100644
--- a/drivers/net/ethernet/octeon/Makefile
+++ b/drivers/net/ethernet/octeon/Makefile
@@ -3,3 +3,13 @@
 #
 
 obj-$(CONFIG_OCTEON_MGMT_ETHERNET)	+= octeon_mgmt.o
+obj-${CONFIG_OCTEON_ETHERNET} +=  octeon-ethernet.o
+
+octeon-ethernet-objs := ethernet.o
+octeon-ethernet-objs += ethernet-mdio.o
+octeon-ethernet-objs += ethernet-mem.o
+octeon-ethernet-objs += ethernet-rgmii.o
+octeon-ethernet-objs += ethernet-rx.o
+octeon-ethernet-objs += ethernet-sgmii.o
+octeon-ethernet-objs += ethernet-spi.o
+octeon-ethernet-objs += ethernet-tx.o
diff --git a/drivers/net/ethernet/octeon/ethernet-defines.h b/drivers/net/ethernet/octeon/ethernet-defines.h
new file mode 100644
index 0000000..96109a1
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-defines.h
@@ -0,0 +1,105 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+
+/*
+ * A few defines are used to control the operation of this driver:
+ *  CONFIG_CAVIUM_RESERVE32
+ *      This kernel config options controls the amount of memory configured
+ *      in a wired TLB entry for all processes to share. If this is set, the
+ *      driver will use this memory instead of kernel memory for pools. This
+ *      allows 32bit userspace application to access the buffers, but also
+ *      requires all received packets to be copied.
+ *  CONFIG_CAVIUM_OCTEON_NUM_PACKET_BUFFERS
+ *      This kernel config option allows the user to control the number of
+ *      packet and work queue buffers allocated by the driver. If this is zero,
+ *      the driver uses the default from below.
+ *  USE_SKBUFFS_IN_HW
+ *      Tells the driver to populate the packet buffers with kernel skbuffs.
+ *      This allows the driver to receive packets without copying them. It also
+ *      means that 32bit userspace can't access the packet buffers.
+ *  USE_HW_TCPUDP_CHECKSUM
+ *      Controls if the Octeon TCP/UDP checksum engine is used for packet
+ *      output. If this is zero, the kernel will perform the checksum in
+ *      software.
+ *  USE_ASYNC_IOBDMA
+ *      Use asynchronous IO access to hardware. This uses Octeon's asynchronous
+ *      IOBDMAs to issue IO accesses without stalling. Set this to zero
+ *      to disable this. Note that IOBDMAs require CVMSEG.
+ *  REUSE_SKBUFFS_WITHOUT_FREE
+ *      Allows the TX path to free an skbuff into the FPA hardware pool. This
+ *      can significantly improve performance for forwarding and bridging, but
+ *      may be somewhat dangerous. Checks are made, but if any buffer is reused
+ *      without the proper Linux cleanup, the networking stack may have very
+ *      bizarre bugs.
+ */
+#ifndef __ETHERNET_DEFINES_H__
+#define __ETHERNET_DEFINES_H__
+
+#include <asm/octeon/cvmx-config.h>
+
+
+#define OCTEON_ETHERNET_VERSION "2.0"
+
+#ifndef CONFIG_CAVIUM_RESERVE32
+#define CONFIG_CAVIUM_RESERVE32 0
+#endif
+
+#define USE_SKBUFFS_IN_HW           1
+#ifdef CONFIG_NETFILTER
+#define REUSE_SKBUFFS_WITHOUT_FREE  0
+#else
+#define REUSE_SKBUFFS_WITHOUT_FREE  1
+#endif
+
+#define USE_HW_TCPUDP_CHECKSUM      1
+
+/* Enable Random Early Dropping under load */
+#define USE_RED                     1
+#define USE_ASYNC_IOBDMA            (CONFIG_CAVIUM_OCTEON_CVMSEG_SIZE > 0)
+
+/*
+ * Allow SW based preamble removal at 10Mbps to workaround PHYs giving
+ * us bad preambles.
+ */
+#define USE_10MBPS_PREAMBLE_WORKAROUND 1
+/*
+ * Use this to have all FPA frees also tell the L2 not to write data
+ * to memory.
+ */
+#define DONT_WRITEBACK(x)           (x)
+/* Use this to not have FPA frees control L2 */
+/*#define DONT_WRITEBACK(x)         0   */
+
+/* Maximum number of SKBs to try to free per xmit packet. */
+#define MAX_OUT_QUEUE_DEPTH 1000
+
+#define FAU_NUM_PACKET_BUFFERS_TO_FREE (CVMX_FAU_REG_END - sizeof(u32))
+
+#define TOTAL_NUMBER_OF_PORTS       (CVMX_PIP_NUM_INPUT_PORTS+1)
+
+
+#endif /* __ETHERNET_DEFINES_H__ */
diff --git a/drivers/net/ethernet/octeon/ethernet-mdio.c b/drivers/net/ethernet/octeon/ethernet-mdio.c
new file mode 100644
index 0000000..4d00fc8
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-mdio.c
@@ -0,0 +1,319 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+#include <linux/kernel.h>
+#include <linux/ethtool.h>
+#include <linux/phy.h>
+#include <linux/ratelimit.h>
+#include <linux/of_mdio.h>
+#include <linux/net_tstamp.h>
+
+#include <net/dst.h>
+
+#include <asm/octeon/octeon.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+
+#include <asm/octeon/cvmx-helper-board.h>
+
+#include <asm/octeon/cvmx-smix-defs.h>
+#include <asm/octeon/cvmx-gmxx-defs.h>
+#include <asm/octeon/cvmx-pip-defs.h>
+#include <asm/octeon/cvmx-pko-defs.h>
+
+static void cvm_oct_get_drvinfo(struct net_device *dev,
+				struct ethtool_drvinfo *info)
+{
+	strcpy(info->driver, "octeon-ethernet");
+	strcpy(info->version, OCTEON_ETHERNET_VERSION);
+	strcpy(info->bus_info, "Builtin");
+}
+
+static int cvm_oct_get_settings(struct net_device *dev, struct ethtool_cmd *cmd)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	if (priv->phydev)
+		return phy_ethtool_gset(priv->phydev, cmd);
+
+	return -EINVAL;
+}
+
+static int cvm_oct_set_settings(struct net_device *dev, struct ethtool_cmd *cmd)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	if (priv->phydev)
+		return phy_ethtool_sset(priv->phydev, cmd);
+
+	return -EINVAL;
+}
+
+static int cvm_oct_nway_reset(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	if (priv->phydev)
+		return phy_start_aneg(priv->phydev);
+
+	return -EINVAL;
+}
+
+const struct ethtool_ops cvm_oct_ethtool_ops = {
+	.get_drvinfo = cvm_oct_get_drvinfo,
+	.get_settings = cvm_oct_get_settings,
+	.set_settings = cvm_oct_set_settings,
+	.nway_reset = cvm_oct_nway_reset,
+	.get_link = ethtool_op_get_link,
+};
+
+/**
+ * cvm_oct_ioctl_hwtstamp - IOCTL support for timestamping
+ * @dev:    Device to change
+ * @rq:     the request
+ * @cmd:    the command
+ *
+ * Returns Zero on success
+ */
+static int cvm_oct_ioctl_hwtstamp(struct net_device *dev,
+				  struct ifreq *rq, int cmd)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	struct hwtstamp_config config;
+	union cvmx_mio_ptp_clock_cfg ptp;
+	union cvmx_gmxx_rxx_frm_ctl frm_ctl;
+	union cvmx_pip_prt_cfgx prt_cfg;
+
+	if (copy_from_user(&config, rq->ifr_data, sizeof(config)))
+		return -EFAULT;
+
+	if (config.flags) /* reserved for future extensions */
+		return -EINVAL;
+
+	/* Check the status of hardware for tiemstamps */
+	if (OCTEON_IS_MODEL(OCTEON_CN6XXX) || OCTEON_IS_MODEL(OCTEON_CNF7XXX)) {
+		/* Write TX timestamp into word 4 */
+		cvmx_write_csr(CVMX_PKO_REG_TIMESTAMP, 4);
+
+		switch (priv->imode) {
+		case CVMX_HELPER_INTERFACE_MODE_XAUI:
+		case CVMX_HELPER_INTERFACE_MODE_RXAUI:
+		case CVMX_HELPER_INTERFACE_MODE_SGMII:
+			break;
+		default:
+			/* No timestamp support*/
+			return -EOPNOTSUPP;
+		}
+
+		ptp.u64 = octeon_read_ptp_csr(CVMX_MIO_PTP_CLOCK_CFG);
+		if (!ptp.s.ptp_en) {
+			/* It should have been enabled by csrc-octeon-ptp */
+			netdev_err(dev, "Error: PTP clock not enabled\n");
+			/* No timestamp support*/
+			return -EOPNOTSUPP;
+		}
+	} else {
+			/* No timestamp support*/
+			return -EOPNOTSUPP;
+	}
+
+	switch (config.tx_type) {
+	case HWTSTAMP_TX_OFF:
+		priv->tx_timestamp_hw = 0;
+		break;
+	case HWTSTAMP_TX_ON:
+		priv->tx_timestamp_hw = 1;
+		break;
+	default:
+		return -ERANGE;
+	}
+
+	switch (config.rx_filter) {
+	case HWTSTAMP_FILTER_NONE:
+		priv->rx_timestamp_hw = 0;
+
+		frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface));
+		frm_ctl.s.ptp_mode = 0;
+		cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface), frm_ctl.u64);
+
+		prt_cfg.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(priv->ipd_port));
+		prt_cfg.s.skip = 0;
+		cvmx_write_csr(CVMX_PIP_PRT_CFGX(priv->ipd_port), prt_cfg.u64);
+		break;
+	case HWTSTAMP_FILTER_ALL:
+	case HWTSTAMP_FILTER_SOME:
+	case HWTSTAMP_FILTER_PTP_V1_L4_EVENT:
+	case HWTSTAMP_FILTER_PTP_V1_L4_SYNC:
+	case HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ:
+	case HWTSTAMP_FILTER_PTP_V2_L4_EVENT:
+	case HWTSTAMP_FILTER_PTP_V2_L4_SYNC:
+	case HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ:
+	case HWTSTAMP_FILTER_PTP_V2_L2_EVENT:
+	case HWTSTAMP_FILTER_PTP_V2_L2_SYNC:
+	case HWTSTAMP_FILTER_PTP_V2_L2_DELAY_REQ:
+	case HWTSTAMP_FILTER_PTP_V2_EVENT:
+	case HWTSTAMP_FILTER_PTP_V2_SYNC:
+	case HWTSTAMP_FILTER_PTP_V2_DELAY_REQ:
+		priv->rx_timestamp_hw = 1;
+		config.rx_filter = HWTSTAMP_FILTER_ALL;
+		frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface));
+		frm_ctl.s.ptp_mode = 1;
+		cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface), frm_ctl.u64);
+
+		prt_cfg.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(priv->ipd_port));
+		prt_cfg.s.skip = 8;
+		cvmx_write_csr(CVMX_PIP_PRT_CFGX(priv->ipd_port), prt_cfg.u64);
+		break;
+	default:
+		return -ERANGE;
+	}
+
+	if (copy_to_user(rq->ifr_data, &config, sizeof(config)))
+		return -EFAULT;
+
+	return 0;
+}
+
+/**
+ * cvm_oct_ioctl - IOCTL support for PHY control
+ * @dev:    Device to change
+ * @rq:     the request
+ * @cmd:    the command
+ *
+ * Returns Zero on success
+ */
+int cvm_oct_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	if (!netif_running(dev))
+		return -EINVAL;
+
+	switch (cmd) {
+	case SIOCSHWTSTAMP:
+		return cvm_oct_ioctl_hwtstamp(dev, rq, cmd);
+
+	default:
+		if (priv->phydev)
+			return phy_mii_ioctl(priv->phydev, rq, cmd);
+	}
+	return -EOPNOTSUPP;
+}
+
+static void cvm_oct_note_carrier(struct octeon_ethernet *priv,
+				 cvmx_helper_link_info_t li)
+{
+	if (li.s.link_up) {
+		pr_notice_ratelimited("%s: %u Mbps %s duplex, port %d\n",
+				      netdev_name(priv->netdev), li.s.speed,
+				      (li.s.full_duplex) ? "Full" : "Half",
+				      priv->ipd_port);
+	} else {
+		pr_notice_ratelimited("%s: Link down\n",
+				      netdev_name(priv->netdev));
+	}
+}
+
+/**
+ * cvm_oct_set_carrier - common wrapper of netif_carrier_{on,off}
+ *
+ * @priv: Device struct.
+ * @link_info: Current state.
+ */
+void cvm_oct_set_carrier(struct octeon_ethernet *priv,
+			 cvmx_helper_link_info_t link_info)
+{
+	cvm_oct_note_carrier(priv, link_info);
+	if (link_info.s.link_up) {
+		if (!netif_carrier_ok(priv->netdev))
+			netif_carrier_on(priv->netdev);
+	} else {
+		if (netif_carrier_ok(priv->netdev))
+			netif_carrier_off(priv->netdev);
+	}
+}
+
+void cvm_oct_adjust_link(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	cvmx_helper_link_info_t link_info;
+
+	if (priv->last_link != priv->phydev->link) {
+		priv->last_link = priv->phydev->link;
+		link_info.u64 = 0;
+		link_info.s.link_up = priv->last_link ? 1 : 0;
+		link_info.s.full_duplex = priv->phydev->duplex ? 1 : 0;
+		link_info.s.speed = priv->phydev->speed;
+
+		cvmx_helper_link_set(priv->ipd_port, link_info);
+
+		cvm_oct_note_carrier(priv, link_info);
+	}
+}
+
+/**
+ * cvm_oct_phy_setup_device - setup the PHY
+ *
+ * @dev:    Device to setup
+ *
+ * Returns Zero on success, negative on failure
+ */
+int cvm_oct_phy_setup_device(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	struct device_node *phy_node;
+
+	if (!priv->of_node)
+		goto no_phy;
+
+	phy_node = of_parse_phandle(priv->of_node, "phy-handle", 0);
+	if (!phy_node)
+		goto no_phy;
+
+	priv->phydev = of_phy_connect(dev, phy_node, cvm_oct_adjust_link, 0,
+				      PHY_INTERFACE_MODE_GMII);
+
+	if (priv->phydev == NULL)
+		return -ENODEV;
+
+	priv->last_link = 0;
+	phy_start_aneg(priv->phydev);
+
+	return 0;
+no_phy:
+	/* If there is no phy, assume a direct MAC connection and that
+	 * the link is up.
+	 */
+	netif_carrier_on(dev);
+	return 0;
+}
diff --git a/drivers/net/ethernet/octeon/ethernet-mem.c b/drivers/net/ethernet/octeon/ethernet-mem.c
new file mode 100644
index 0000000..19f6034
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-mem.c
@@ -0,0 +1,320 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+#include <linux/netdevice.h>
+#include <linux/export.h>
+#include <linux/kernel.h>
+#include <linux/slab.h>
+
+#include <asm/octeon/octeon.h>
+#include <asm/octeon/cvmx-fpa.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+
+struct fpa_pool {
+	int pool;
+	int users;
+	int size;
+	char kmem_name[20];
+	struct kmem_cache *kmem;
+	int (*fill)(struct fpa_pool *p, int num);
+	int (*empty)(struct fpa_pool *p, int num);
+};
+
+static DEFINE_SPINLOCK(cvm_oct_pools_lock);
+/* Eight pools. */
+static struct fpa_pool cvm_oct_pools[] = {
+	{-1}, {-1}, {-1}, {-1}, {-1}, {-1}, {-1}, {-1}
+};
+
+/**
+ * cvm_oct_fill_hw_skbuff - fill the supplied hardware pool with skbuffs
+ * @pool:     Pool to allocate an skbuff for
+ * @size:     Size of the buffer needed for the pool
+ * @elements: Number of buffers to allocate
+ *
+ * Returns the actual number of buffers allocated.
+ */
+static int cvm_oct_fill_hw_skbuff(struct fpa_pool *pool, int elements)
+{
+	int freed = elements;
+	int size = pool->size;
+	int pool_num = pool->pool;
+	while (freed) {
+		int extra_reserve;
+		u8 *desired_data;
+		struct sk_buff *skb = alloc_skb(size + CVM_OCT_SKB_TO_FPA_PADDING,
+						GFP_ATOMIC);
+		if (skb == NULL) {
+			pr_err("Failed to allocate skb for hardware pool %d\n",
+			       pool_num);
+			break;
+		}
+		desired_data = cvm_oct_get_fpa_head(skb);
+		extra_reserve = desired_data - skb->data;
+		skb_reserve(skb, extra_reserve);
+		*(struct sk_buff **)(skb->data - sizeof(void *)) = skb;
+		cvmx_fpa_free(skb->data, pool_num, DONT_WRITEBACK(size / 128));
+		freed--;
+	}
+	return elements - freed;
+}
+
+/**
+ * cvm_oct_free_hw_skbuff- free hardware pool skbuffs
+ * @pool:     Pool to allocate an skbuff for
+ * @size:     Size of the buffer needed for the pool
+ * @elements: Number of buffers to allocate
+ */
+static int cvm_oct_free_hw_skbuff(struct fpa_pool *pool, int elements)
+{
+	char *memory;
+	int pool_num = pool->pool;
+
+	do {
+		memory = cvmx_fpa_alloc(pool_num);
+		if (memory) {
+			struct sk_buff *skb = *cvm_oct_packet_to_skb(memory);
+			elements--;
+			dev_kfree_skb(skb);
+		}
+	} while (memory);
+
+	if (elements < 0)
+		pr_err("Freeing of pool %u had too many skbuffs (%d)\n",
+		       pool_num, elements);
+	else if (elements > 0)
+		pr_err("Freeing of pool %u is missing %d skbuffs\n",
+		       pool_num, elements);
+
+	return 0;
+}
+
+/**
+ * cvm_oct_fill_hw_memory - fill a hardware pool with memory.
+ * @pool:     Pool to populate
+ * @size:     Size of each buffer in the pool
+ * @elements: Number of buffers to allocate
+ *
+ * Returns the actual number of buffers allocated.
+ */
+static int cvm_oct_fill_hw_kmem(struct fpa_pool *pool, int elements)
+{
+	char *memory;
+	int freed = elements;
+
+	while (freed) {
+		memory = kmem_cache_alloc(pool->kmem, GFP_KERNEL);
+		if (unlikely(memory == NULL)) {
+			pr_err("Unable to allocate %u bytes for FPA pool %d\n",
+			       elements * pool->size, pool->pool);
+			break;
+		}
+		cvmx_fpa_free(memory, pool->pool, 0);
+		freed--;
+	}
+	return elements - freed;
+}
+
+/**
+ * cvm_oct_free_hw_memory - Free memory allocated by cvm_oct_fill_hw_memory
+ * @pool:     FPA pool to free
+ * @size:     Size of each buffer in the pool
+ * @elements: Number of buffers that should be in the pool
+ */
+static int cvm_oct_free_hw_kmem(struct fpa_pool *pool, int elements)
+{
+	char *fpa;
+	while (elements) {
+		fpa = cvmx_fpa_alloc(pool->pool);
+		if (!fpa)
+			break;
+		elements--;
+		kmem_cache_free(pool->kmem, fpa);
+	}
+
+	if (elements < 0)
+		pr_err("Freeing of pool %u had too many buffers (%d)\n",
+		       pool->pool, elements);
+	else if (elements > 0)
+		pr_err("Warning: Freeing of pool %u is missing %d buffers\n",
+		       pool->pool, elements);
+	return elements;
+}
+
+int cvm_oct_mem_fill_fpa(int pool, int elements)
+{
+	struct fpa_pool *p;
+
+	if (pool < 0 || pool >= ARRAY_SIZE(cvm_oct_pools))
+		return -EINVAL;
+
+	p = cvm_oct_pools + pool;
+
+	return p->fill(p, elements);
+}
+EXPORT_SYMBOL(cvm_oct_mem_fill_fpa);
+
+int cvm_oct_mem_empty_fpa(int pool, int elements)
+{
+	struct fpa_pool *p;
+
+	if (pool < 0 || pool >= ARRAY_SIZE(cvm_oct_pools))
+		return -EINVAL;
+
+	p = cvm_oct_pools + pool;
+	if (p->empty)
+		return p->empty(p, elements);
+
+	return 0;
+}
+EXPORT_SYMBOL(cvm_oct_mem_empty_fpa);
+
+void cvm_oct_mem_cleanup(void)
+{
+	int i;
+
+	spin_lock(&cvm_oct_pools_lock);
+
+	for (i = 0; i < ARRAY_SIZE(cvm_oct_pools); i++)
+		if (cvm_oct_pools[i].kmem)
+			kmem_cache_shrink(cvm_oct_pools[i].kmem);
+	spin_unlock(&cvm_oct_pools_lock);
+}
+EXPORT_SYMBOL(cvm_oct_mem_cleanup);
+
+/**
+ * cvm_oct_alloc_fpa_pool() - Allocate an FPA pool of the given size
+ * @pool:  Requested pool number (-1 for don't care).
+ * @size:  The size of the pool elements.
+ *
+ * Returns the pool number or a negative number on error.
+ */
+int cvm_oct_alloc_fpa_pool(int pool, int size)
+{
+	int i;
+	int ret = 0;
+
+	if (pool >= (int)ARRAY_SIZE(cvm_oct_pools) || size < 128)
+		return -EINVAL;
+
+	spin_lock(&cvm_oct_pools_lock);
+
+	if (pool >= 0) {
+		if (cvm_oct_pools[pool].pool != -1) {
+			if (cvm_oct_pools[pool].size == size) {
+				/* Already allocated */
+				cvm_oct_pools[pool].users++;
+				ret = pool;
+				goto out;
+			} else {
+				/* conflict */
+				ret = -EINVAL;
+				goto out;
+			}
+		}
+	} else {
+		/* Find an established pool */
+		for (i = 0; i < ARRAY_SIZE(cvm_oct_pools); i++)
+			if (cvm_oct_pools[i].pool >= 0 &&
+			    cvm_oct_pools[i].size == size) {
+				cvm_oct_pools[i].users++;
+				ret = i;
+				goto out;
+			}
+
+		/* Find an empty pool */
+		for (i = 0; i < ARRAY_SIZE(cvm_oct_pools); i++)
+			if (cvm_oct_pools[i].pool == -1) {
+				pool = i;
+				break;
+			}
+		if (pool < 0) {
+			/* No empties. */
+			ret = -EINVAL;
+			goto out;
+		}
+	}
+
+	/* Setup the pool */
+	cvm_oct_pools[pool].pool = pool;
+	cvm_oct_pools[pool].users++;
+	cvm_oct_pools[pool].size = size;
+	if (USE_SKBUFFS_IN_HW && pool == 0) {
+		/* Special packet pool */
+		cvm_oct_pools[pool].fill = cvm_oct_fill_hw_skbuff;
+		cvm_oct_pools[pool].empty = cvm_oct_free_hw_skbuff;
+	} else {
+		snprintf(cvm_oct_pools[pool].kmem_name,
+			 sizeof(cvm_oct_pools[pool].kmem_name),
+			 "oct-fpa-%d", size);
+		cvm_oct_pools[pool].fill = cvm_oct_fill_hw_kmem;
+		cvm_oct_pools[pool].empty = cvm_oct_free_hw_kmem;
+		cvm_oct_pools[pool].kmem =
+			kmem_cache_create(cvm_oct_pools[pool].kmem_name,
+					  size, 128, 0, NULL);
+		if (!cvm_oct_pools[pool].kmem) {
+			ret = -ENOMEM;
+			cvm_oct_pools[pool].pool = -1;
+			goto out;
+		}
+	}
+	ret = pool;
+out:
+	spin_unlock(&cvm_oct_pools_lock);
+	return ret;
+}
+EXPORT_SYMBOL(cvm_oct_alloc_fpa_pool);
+
+/**
+ * cvm_oct_release_fpa_pool() - Releases an FPA pool
+ * @pool:  Pool number.
+ *
+ * This undoes the action of cvm_oct_alloc_fpa_pool().
+ *
+ * Returns zero on success.
+ */
+int cvm_oct_release_fpa_pool(int pool)
+{
+	int ret = -EINVAL;
+
+	if (pool < 0 || pool >= (int)ARRAY_SIZE(cvm_oct_pools))
+		return ret;
+
+	spin_lock(&cvm_oct_pools_lock);
+
+	if (cvm_oct_pools[pool].users <= 0) {
+		pr_err("Error: Unbalanced FPA pool allocation\n");
+		goto out;
+	}
+	cvm_oct_pools[pool].users--;
+	ret = 0;
+out:
+	spin_unlock(&cvm_oct_pools_lock);
+	return ret;
+}
+EXPORT_SYMBOL(cvm_oct_release_fpa_pool);
diff --git a/drivers/net/ethernet/octeon/ethernet-rgmii.c b/drivers/net/ethernet/octeon/ethernet-rgmii.c
new file mode 100644
index 0000000..2db614f
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-rgmii.c
@@ -0,0 +1,354 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/interrupt.h>
+#include <linux/phy.h>
+#include <linux/ratelimit.h>
+#include <net/dst.h>
+
+#include <asm/octeon/octeon.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+
+#include <asm/octeon/cvmx-helper.h>
+
+#include <asm/octeon/cvmx-ipd-defs.h>
+#include <asm/octeon/cvmx-npi-defs.h>
+#include <asm/octeon/cvmx-gmxx-defs.h>
+
+DEFINE_SPINLOCK(global_register_lock);
+
+static int number_rgmii_ports;
+
+static void cvm_oct_rgmii_poll(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	unsigned long flags = 0;
+	cvmx_helper_link_info_t link_info;
+	int use_global_register_lock = (priv->phydev == NULL);
+
+	BUG_ON(in_interrupt());
+	if (use_global_register_lock) {
+		/* Take the global register lock since we are going to
+		 * touch registers that affect more than one port.
+		 */
+		spin_lock_irqsave(&global_register_lock, flags);
+	} else {
+		mutex_lock(&priv->phydev->bus->mdio_lock);
+	}
+
+	link_info = cvmx_helper_link_get(priv->ipd_port);
+	if (link_info.u64 == priv->link_info) {
+		/* If the 10Mbps preamble workaround is supported and we're
+		 * at 10Mbps we may need to do some special checking.
+		 */
+		if (USE_10MBPS_PREAMBLE_WORKAROUND && (link_info.s.speed == 10)) {
+			/* Read the GMXX_RXX_INT_REG[PCTERR] bit and
+			 * see if we are getting preamble errors.
+			 */
+			union cvmx_gmxx_rxx_int_reg gmxx_rxx_int_reg;
+			gmxx_rxx_int_reg.u64 =
+				cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(priv->interface_port, priv->interface));
+			if (gmxx_rxx_int_reg.s.pcterr) {
+				/* We are getting preamble errors at
+				 * 10Mbps.  Most likely the PHY is
+				 * giving us packets with mis aligned
+				 * preambles. In order to get these
+				 * packets we need to disable preamble
+				 * checking and do it in software.
+				 */
+				union cvmx_gmxx_rxx_frm_ctl gmxx_rxx_frm_ctl;
+				union cvmx_ipd_sub_port_fcs ipd_sub_port_fcs;
+
+				/* Disable preamble checking */
+				gmxx_rxx_frm_ctl.u64 =
+					cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface));
+				gmxx_rxx_frm_ctl.s.pre_chk = 0;
+				cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface),
+					       gmxx_rxx_frm_ctl.u64);
+
+				/* Disable FCS stripping */
+				ipd_sub_port_fcs.u64 = cvmx_read_csr(CVMX_IPD_SUB_PORT_FCS);
+				ipd_sub_port_fcs.s.port_bit &= 0xffffffffull ^ (1ull << priv->ipd_port);
+				cvmx_write_csr(CVMX_IPD_SUB_PORT_FCS,
+					       ipd_sub_port_fcs.u64);
+
+				/* Clear any error bits */
+				cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(priv->interface_port, priv->interface),
+					       gmxx_rxx_int_reg.u64);
+				printk_ratelimited("%s: Using 10Mbps with software preamble removal\n",
+						   dev->name);
+			}
+		}
+
+		if (use_global_register_lock)
+			spin_unlock_irqrestore(&global_register_lock, flags);
+		else
+			mutex_unlock(&priv->phydev->bus->mdio_lock);
+		return;
+	}
+
+	/* If the 10Mbps preamble workaround is allowed we need to on
+	 * preamble checking, FCS stripping, and clear error bits on
+	 * every speed change. If errors occur during 10Mbps operation
+	 * the above code will change this stuff.
+	 */
+	if (USE_10MBPS_PREAMBLE_WORKAROUND) {
+		union cvmx_gmxx_rxx_frm_ctl gmxx_rxx_frm_ctl;
+		union cvmx_ipd_sub_port_fcs ipd_sub_port_fcs;
+		union cvmx_gmxx_rxx_int_reg gmxx_rxx_int_reg;
+
+		/* Enable preamble checking */
+		gmxx_rxx_frm_ctl.u64 =
+		    cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface));
+		gmxx_rxx_frm_ctl.s.pre_chk = 1;
+		cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface),
+			       gmxx_rxx_frm_ctl.u64);
+		/* Enable FCS stripping */
+		ipd_sub_port_fcs.u64 = cvmx_read_csr(CVMX_IPD_SUB_PORT_FCS);
+		ipd_sub_port_fcs.s.port_bit |= 1ull << priv->ipd_port;
+		cvmx_write_csr(CVMX_IPD_SUB_PORT_FCS, ipd_sub_port_fcs.u64);
+		/* Clear any error bits */
+		gmxx_rxx_int_reg.u64 =
+			cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(priv->interface_port, priv->interface));
+		cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(priv->interface_port, priv->interface),
+			       gmxx_rxx_int_reg.u64);
+	}
+	if (priv->phydev == NULL) {
+		link_info = cvmx_helper_link_autoconf(priv->ipd_port);
+		priv->link_info = link_info.u64;
+	}
+
+	if (use_global_register_lock)
+		spin_unlock_irqrestore(&global_register_lock, flags);
+	else
+		mutex_unlock(&priv->phydev->bus->mdio_lock);
+
+	if (priv->phydev == NULL)
+		cvm_oct_set_carrier(priv, link_info);
+}
+
+static irqreturn_t cvm_oct_rgmii_rml_interrupt(int cpl, void *dev_id)
+{
+	union cvmx_npi_rsl_int_blocks rsl_int_blocks;
+	int index;
+	irqreturn_t return_status = IRQ_NONE;
+
+	rsl_int_blocks.u64 = cvmx_read_csr(CVMX_NPI_RSL_INT_BLOCKS);
+
+	/* Check and see if this interrupt was caused by the GMX0 block */
+	if (rsl_int_blocks.s.gmx0) {
+		int interface = 0;
+		/* Loop through every port of this interface */
+		for (index = 0;
+		     index < cvmx_helper_ports_on_interface(interface);
+		     index++) {
+			/* Read the GMX interrupt status bits */
+			union cvmx_gmxx_rxx_int_reg gmx_rx_int_reg;
+			gmx_rx_int_reg.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(index, interface));
+			gmx_rx_int_reg.u64 &= cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(index, interface));
+			/* Poll the port if inband status changed */
+			if (gmx_rx_int_reg.s.phy_dupx
+			    || gmx_rx_int_reg.s.phy_link
+			    || gmx_rx_int_reg.s.phy_spd) {
+				struct octeon_ethernet *priv = cvm_oct_by_port[cvmx_helper_get_ipd_port(interface, index)];
+
+				if (priv && !atomic_read(&cvm_oct_poll_queue_stopping))
+					queue_work(cvm_oct_poll_queue, &priv->port_work);
+
+				gmx_rx_int_reg.u64 = 0;
+				gmx_rx_int_reg.s.phy_dupx = 1;
+				gmx_rx_int_reg.s.phy_link = 1;
+				gmx_rx_int_reg.s.phy_spd = 1;
+				cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(index, interface),
+					       gmx_rx_int_reg.u64);
+				return_status = IRQ_HANDLED;
+			}
+		}
+	}
+
+	/* Check and see if this interrupt was caused by the GMX1 block */
+	if (rsl_int_blocks.s.gmx1) {
+		int interface = 1;
+		/* Loop through every port of this interface */
+		for (index = 0;
+		     index < cvmx_helper_ports_on_interface(interface);
+		     index++) {
+			/* Read the GMX interrupt status bits */
+			union cvmx_gmxx_rxx_int_reg gmx_rx_int_reg;
+			gmx_rx_int_reg.u64 =
+				cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(index, interface));
+			gmx_rx_int_reg.u64 &=
+				cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(index, interface));
+			/* Poll the port if inband status changed */
+			if (gmx_rx_int_reg.s.phy_dupx
+			    || gmx_rx_int_reg.s.phy_link
+			    || gmx_rx_int_reg.s.phy_spd) {
+				struct octeon_ethernet *priv = cvm_oct_by_port[cvmx_helper_get_ipd_port(interface, index)];
+
+				if (priv && !atomic_read(&cvm_oct_poll_queue_stopping))
+					queue_work(cvm_oct_poll_queue, &priv->port_work);
+
+				gmx_rx_int_reg.u64 = 0;
+				gmx_rx_int_reg.s.phy_dupx = 1;
+				gmx_rx_int_reg.s.phy_link = 1;
+				gmx_rx_int_reg.s.phy_spd = 1;
+				cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(index, interface),
+					       gmx_rx_int_reg.u64);
+				return_status = IRQ_HANDLED;
+			}
+		}
+	}
+	return return_status;
+}
+
+static void cvm_oct_rgmii_immediate_poll(struct work_struct *work)
+{
+	struct octeon_ethernet *priv = container_of(work, struct octeon_ethernet, port_work);
+	cvm_oct_rgmii_poll(priv->netdev);
+}
+
+int cvm_oct_rgmii_open(struct net_device *dev)
+{
+	union cvmx_gmxx_prtx_cfg gmx_cfg;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	cvmx_helper_link_info_t link_info;
+	int rv;
+
+	rv = cvm_oct_phy_setup_device(dev);
+	if (rv)
+		return rv;
+
+
+	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
+	gmx_cfg.s.en = 1;
+	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface), gmx_cfg.u64);
+
+	if (!octeon_is_simulation()) {
+		if (priv->phydev) {
+			int r = phy_read_status(priv->phydev);
+			if (r == 0 && priv->phydev->link == 0)
+				netif_carrier_off(dev);
+			cvm_oct_adjust_link(dev);
+		} else {
+			link_info = cvmx_helper_link_get(priv->ipd_port);
+			if (!link_info.s.link_up)
+				netif_carrier_off(dev);
+			spin_lock(&priv->poll_lock);
+			priv->poll = cvm_oct_rgmii_poll;
+			spin_unlock(&priv->poll_lock);
+		}
+	}
+
+	INIT_WORK(&priv->port_work, cvm_oct_rgmii_immediate_poll);
+	/* Due to GMX errata in CN3XXX series chips, it is necessary
+	 * to take the link down immediately when the PHY changes
+	 * state. In order to do this we call the poll function every
+	 * time the RGMII inband status changes.  This may cause
+	 * problems if the PHY doesn't implement inband status
+	 * properly.
+	 */
+	if (number_rgmii_ports == 0) {
+		rv = request_irq(OCTEON_IRQ_RML, cvm_oct_rgmii_rml_interrupt,
+				 IRQF_SHARED, "RGMII", &number_rgmii_ports);
+		if (rv != 0)
+			return rv;
+	}
+	number_rgmii_ports++;
+
+	/* Only true RGMII ports need to be polled. In GMII mode, port
+	 * 0 is really a RGMII port.
+	 */
+	if (((priv->imode == CVMX_HELPER_INTERFACE_MODE_GMII)
+	     && (priv->ipd_port == 0))
+	    || (priv->imode == CVMX_HELPER_INTERFACE_MODE_RGMII)) {
+		if (!octeon_is_simulation()) {
+			union cvmx_gmxx_rxx_int_en gmx_rx_int_en;
+			/* Enable interrupts on inband status changes
+			 * for this port.
+			 */
+			gmx_rx_int_en.u64 =
+				cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(priv->interface_port, priv->interface));
+			gmx_rx_int_en.s.phy_dupx = 1;
+			gmx_rx_int_en.s.phy_link = 1;
+			gmx_rx_int_en.s.phy_spd = 1;
+			cvmx_write_csr(CVMX_GMXX_RXX_INT_EN(priv->interface_port, priv->interface),
+				       gmx_rx_int_en.u64);
+		}
+	}
+
+	return 0;
+}
+
+int cvm_oct_rgmii_stop(struct net_device *dev)
+{
+	union cvmx_gmxx_prtx_cfg gmx_cfg;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
+	gmx_cfg.s.en = 0;
+	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface), gmx_cfg.u64);
+
+	/* Only true RGMII ports need to be polled. In GMII mode, port
+	 * 0 is really a RGMII port.
+	 */
+	if (((priv->imode == CVMX_HELPER_INTERFACE_MODE_GMII)
+	     && (priv->ipd_port == 0))
+	    || (priv->imode == CVMX_HELPER_INTERFACE_MODE_RGMII)) {
+		if (!octeon_is_simulation()) {
+			union cvmx_gmxx_rxx_int_en gmx_rx_int_en;
+			/* Disable interrupts on inband status changes
+			 * for this port.
+			 */
+			gmx_rx_int_en.u64 =
+				cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(priv->interface_port, priv->interface));
+			gmx_rx_int_en.s.phy_dupx = 0;
+			gmx_rx_int_en.s.phy_link = 0;
+			gmx_rx_int_en.s.phy_spd = 0;
+			cvmx_write_csr(CVMX_GMXX_RXX_INT_EN(priv->interface_port, priv->interface),
+				       gmx_rx_int_en.u64);
+		}
+	}
+
+	/* Remove the interrupt handler when the last port is removed. */
+	number_rgmii_ports--;
+	if (number_rgmii_ports == 0)
+		free_irq(OCTEON_IRQ_RML, &number_rgmii_ports);
+	cancel_work_sync(&priv->port_work);
+
+	return 0;
+}
+
+int cvm_oct_rgmii_init(struct net_device *dev)
+{
+	cvm_oct_common_init(dev);
+	dev->netdev_ops->ndo_stop(dev);
+
+	return 0;
+}
diff --git a/drivers/net/ethernet/octeon/ethernet-rx.c b/drivers/net/ethernet/octeon/ethernet-rx.c
new file mode 100644
index 0000000..950cac0
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-rx.c
@@ -0,0 +1,684 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/cache.h>
+#include <linux/cpumask.h>
+#include <linux/netdevice.h>
+#include <linux/init.h>
+#include <linux/etherdevice.h>
+#include <linux/ip.h>
+#include <linux/string.h>
+#include <linux/prefetch.h>
+#include <linux/ratelimit.h>
+#include <linux/smp.h>
+#include <linux/interrupt.h>
+#include <net/dst.h>
+#ifdef CONFIG_XFRM
+#include <linux/xfrm.h>
+#include <net/xfrm.h>
+#endif /* CONFIG_XFRM */
+
+#include <asm/octeon/octeon.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+
+#include <asm/octeon/cvmx-helper.h>
+#include <asm/octeon/cvmx-wqe.h>
+#include <asm/octeon/cvmx-fau.h>
+#include <asm/octeon/cvmx-pow.h>
+#include <asm/octeon/cvmx-pip.h>
+#include <asm/octeon/cvmx-scratch.h>
+
+#include <asm/octeon/cvmx-gmxx-defs.h>
+
+struct cvm_napi_wrapper {
+	struct napi_struct napi;
+	int available;
+} ____cacheline_aligned_in_smp;
+
+static struct cvm_napi_wrapper cvm_oct_napi[NR_CPUS] __cacheline_aligned_in_smp;
+
+struct cvm_oct_core_state {
+	int baseline_cores;
+	/* We want to read this without having to acquire the lock,
+	 * make it volatile so we are likely to get a fairly current
+	 * value.
+	 */
+	volatile int active_cores;
+	/* cvm_napi_wrapper.available and active_cores must be kept
+	 * consistent with this lock.
+	 */
+	spinlock_t lock;
+} ____cacheline_aligned_in_smp;
+
+static struct cvm_oct_core_state core_state __cacheline_aligned_in_smp;
+
+static void cvm_oct_enable_napi(void *_)
+{
+	int cpu = smp_processor_id();
+	napi_schedule(&cvm_oct_napi[cpu].napi);
+}
+
+static void cvm_oct_enable_one_cpu(void)
+{
+	int v;
+	int cpu;
+	unsigned long flags;
+	spin_lock_irqsave(&core_state.lock, flags);
+	/* ... if a CPU is available, Turn on NAPI polling for that CPU.  */
+	for_each_online_cpu(cpu) {
+		if (cvm_oct_napi[cpu].available > 0) {
+			cvm_oct_napi[cpu].available--;
+			core_state.active_cores++;
+			spin_unlock_irqrestore(&core_state.lock, flags);
+			if (cpu == smp_processor_id()) {
+				cvm_oct_enable_napi(NULL);
+			} else {
+#ifdef CONFIG_SMP
+				v = smp_call_function_single(cpu, cvm_oct_enable_napi,
+							     NULL, 0);
+				if (v)
+					panic("Can't enable NAPI.");
+#else
+				BUG();
+#endif
+			}
+			goto out;
+		}
+	}
+	spin_unlock_irqrestore(&core_state.lock, flags);
+out:
+	return;
+}
+
+static void cvm_oct_no_more_work(struct napi_struct *napi)
+{
+	struct cvm_napi_wrapper *nr = container_of(napi, struct cvm_napi_wrapper, napi);
+	int current_active;
+	unsigned long flags;
+
+	spin_lock_irqsave(&core_state.lock, flags);
+
+	core_state.active_cores--;
+	current_active = core_state.active_cores;
+	nr->available++;
+	BUG_ON(nr->available != 1);
+
+	spin_unlock_irqrestore(&core_state.lock, flags);
+
+	if (current_active == 0) {
+		/* No more CPUs doing processing, enable interrupts so
+		 * we can start processing again when there is
+		 * something to do.
+		 */
+		union cvmx_pow_wq_int_thrx int_thr;
+		int_thr.u64 = 0;
+		int_thr.s.iq_thr = 1;
+		int_thr.s.ds_thr = 1;
+		/* Enable POW interrupt when our port has at
+		 * least one packet.
+		 */
+		cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group),
+			       int_thr.u64);
+	}
+}
+
+/**
+ * cvm_oct_do_interrupt - interrupt handler.
+ *
+ * The interrupt occurs whenever the POW has packets in our group.
+ *
+ */
+static irqreturn_t cvm_oct_do_interrupt(int cpl, void *dev_id)
+{
+	int cpu = smp_processor_id();
+	unsigned long flags;
+	union cvmx_pow_wq_int wq_int;
+
+	/* Disable the IRQ and start napi_poll. */
+
+	cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group), 0);
+
+	wq_int.u64 = 0;
+	wq_int.s.wq_int = 1 << pow_receive_group;
+	cvmx_write_csr(CVMX_POW_WQ_INT, wq_int.u64);
+
+	spin_lock_irqsave(&core_state.lock, flags);
+
+	/* ... and NAPI better not be running on this CPU.  */
+	BUG_ON(cvm_oct_napi[cpu].available != 1);
+	cvm_oct_napi[cpu].available--;
+
+	/* There better be cores available...  */
+	core_state.active_cores++;
+	BUG_ON(core_state.active_cores > core_state.baseline_cores);
+
+	spin_unlock_irqrestore(&core_state.lock, flags);
+
+	cvm_oct_enable_napi(NULL);
+
+	return IRQ_HANDLED;
+}
+
+/**
+ * cvm_oct_check_rcv_error - process receive errors
+ * @work: Work queue entry pointing to the packet.
+ *
+ * Returns Non-zero if the packet can be dropped, zero otherwise.
+ */
+static inline int cvm_oct_check_rcv_error(cvmx_wqe_t *work)
+{
+	if ((work->word2.snoip.err_code == 10) && (work->word1.len <= 64)) {
+		/* Ignore length errors on min size packets. Some
+		 * equipment incorrectly pads packets to 64+4FCS
+		 * instead of 60+4FCS.  Note these packets still get
+		 * counted as frame errors.
+		 */
+	} else
+	    if (USE_10MBPS_PREAMBLE_WORKAROUND &&
+		((work->word2.snoip.err_code == 5) ||
+		 (work->word2.snoip.err_code == 7))) {
+		/* We received a packet with either an alignment error
+		 * or a FCS error. This may be signalling that we are
+		 * running 10Mbps with GMXX_RXX_FRM_CTL[PRE_CHK}
+		 * off. If this is the case we need to parse the
+		 * packet to determine if we can remove a non spec
+		 * preamble and generate a correct packet.
+		 */
+		int interface = cvmx_helper_get_interface_num(work->word1.cn38xx.ipprt);
+		int index = cvmx_helper_get_interface_index_num(work->word1.cn38xx.ipprt);
+		union cvmx_gmxx_rxx_frm_ctl gmxx_rxx_frm_ctl;
+		gmxx_rxx_frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface));
+		if (gmxx_rxx_frm_ctl.s.pre_chk == 0) {
+
+			u8 *ptr = phys_to_virt(work->packet_ptr.s.addr);
+			int i = 0;
+
+			while (i < work->word1.len - 1) {
+				if (*ptr != 0x55)
+					break;
+				ptr++;
+				i++;
+			}
+
+			if (*ptr == 0xd5) {
+				work->packet_ptr.s.addr += i + 1;
+				work->word1.len -= i + 5;
+			} else if ((*ptr & 0xf) == 0xd) {
+				work->packet_ptr.s.addr += i;
+				work->word1.len -= i + 4;
+				for (i = 0; i < work->word1.len; i++) {
+					*ptr =
+					    ((*ptr & 0xf0) >> 4) |
+					    ((*(ptr + 1) & 0xf) << 4);
+					ptr++;
+				}
+			} else {
+				printk_ratelimited("Port %d unknown preamble, packet dropped\n",
+						   work->word1.cn38xx.ipprt);
+				cvm_oct_free_work(work);
+				return 1;
+			}
+		}
+	} else {
+		printk_ratelimited("Port %d receive error code %d, packet dropped\n",
+				   work->word1.cn38xx.ipprt, work->word2.snoip.err_code);
+		cvm_oct_free_work(work);
+		return 1;
+	}
+
+	return 0;
+}
+
+/**
+ * cvm_oct_ptp_to_ktime - Convert a hardware PTP timestamp into a
+ * kernel timestamp.
+ *
+ * @ptptime: 64 bit PTP timestamp, normally in nanoseconds
+ *
+ * Return ktime_t
+ */
+static ktime_t cvm_oct_ptp_to_ktime(u64 ptptime)
+{
+	ktime_t ktimebase;
+	u64 ptpbase;
+	unsigned long flags;
+
+	local_irq_save(flags);
+	/* Fill the icache with the code */
+	ktime_get_real();
+	/* Flush all pending operations */
+	mb();
+	/* Read the time and PTP clock as close together as
+	 * possible. It is important that this sequence take the same
+	 * amount of time to reduce jitter
+	 */
+	ktimebase = ktime_get_real();
+	ptpbase = octeon_read_ptp_csr(CVMX_MIO_PTP_CLOCK_HI);
+	local_irq_restore(flags);
+
+	return ktime_sub_ns(ktimebase, ptpbase - ptptime);
+}
+
+/**
+ * cvm_oct_napi_poll - the NAPI poll function.
+ * @napi: The NAPI instance, or null if called from cvm_oct_poll_controller
+ * @budget: Maximum number of packets to receive.
+ *
+ * Returns the number of packets processed.
+ */
+static int cvm_oct_napi_poll(struct napi_struct *napi, int budget)
+{
+	const int	coreid = cvmx_get_core_num();
+	u64		old_group_mask;
+	u64		old_scratch;
+	int		rx_count = 0;
+	bool		did_work_request = false;
+	bool		packet_copied;
+
+	char		*p = (char *)cvm_oct_by_port;
+	/* Prefetch cvm_oct_device since we know we need it soon */
+	prefetch(&p[0]);
+	prefetch(&p[SMP_CACHE_BYTES]);
+	prefetch(&p[2 * SMP_CACHE_BYTES]);
+
+	if (USE_ASYNC_IOBDMA) {
+		/* Save scratch in case userspace is using it */
+		CVMX_SYNCIOBDMA;
+		old_scratch = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
+	}
+
+	/* Only allow work for our group (and preserve priorities) */
+	old_group_mask = cvmx_read_csr(CVMX_POW_PP_GRP_MSKX(coreid));
+	cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(coreid),
+		       (old_group_mask & ~0xFFFFull) | 1 << pow_receive_group);
+
+	if (USE_ASYNC_IOBDMA) {
+		cvmx_pow_work_request_async(CVMX_SCR_SCRATCH, CVMX_POW_NO_WAIT);
+		did_work_request = true;
+	}
+
+	while (rx_count < budget) {
+		struct sk_buff *skb = NULL;
+		struct sk_buff **pskb = NULL;
+		bool skb_in_hw;
+		cvmx_wqe_t *work;
+		unsigned int segments;
+		int packets_to_replace = 0;
+		unsigned int packet_len;
+
+		union cvmx_buf_ptr  packet_ptr;
+
+		if (USE_ASYNC_IOBDMA && did_work_request)
+			work = cvmx_pow_work_response_async(CVMX_SCR_SCRATCH);
+		else
+			work = cvmx_pow_work_request_sync(CVMX_POW_NO_WAIT);
+
+		prefetch(work);
+		did_work_request = false;
+		if (work == NULL) {
+			union cvmx_pow_wq_int wq_int;
+			wq_int.u64 = 0;
+			wq_int.s.wq_int = 1 << pow_receive_group;
+			cvmx_write_csr(CVMX_POW_WQ_INT, wq_int.u64);
+			break;
+		}
+		packet_ptr = work->packet_ptr;
+		pskb = cvm_oct_packet_to_skb(cvm_oct_get_buffer_ptr(packet_ptr));
+		prefetch(pskb);
+
+		if (USE_ASYNC_IOBDMA && rx_count < (budget - 1)) {
+			cvmx_pow_work_request_async_nocheck(CVMX_SCR_SCRATCH, CVMX_POW_NO_WAIT);
+			did_work_request = true;
+		}
+
+		if (rx_count == 0) {
+			/* First time through, see if there is enough
+			 * work waiting to merit waking another
+			 * CPU.
+			 */
+			union cvmx_pow_wq_int_cntx counts;
+			int backlog;
+			int cores_in_use = core_state.active_cores;
+			counts.u64 = cvmx_read_csr(CVMX_POW_WQ_INT_CNTX(pow_receive_group));
+			backlog = counts.s.iq_cnt + counts.s.ds_cnt;
+			if (backlog > budget * cores_in_use && napi != NULL)
+				cvm_oct_enable_one_cpu();
+		}
+
+		/* If WORD2[SOFTWARE] then this WQE is a complete for
+		 * a TX packet.
+		 */
+		if (work->word2.s.software) {
+			struct octeon_ethernet *priv;
+			int packet_qos = work->word0.raw.unused;
+
+			skb = (struct sk_buff *)packet_ptr.u64;
+			priv = netdev_priv(skb->dev);
+			if (!netif_running(skb->dev))
+				netif_wake_queue(skb->dev);
+			if (unlikely((skb_shinfo(skb)->tx_flags | SKBTX_IN_PROGRESS) != 0 &&
+				     priv->tx_timestamp_hw)) {
+					u64 ns = *(u64 *)work->packet_data;
+					struct skb_shared_hwtstamps ts;
+					ts.syststamp = cvm_oct_ptp_to_ktime(ns);
+					ts.hwtstamp = ns_to_ktime(ns);
+					skb_tstamp_tx(skb, &ts);
+			}
+			dev_kfree_skb_any(skb);
+
+			cvmx_fpa_free(work, CVMX_FPA_WQE_POOL, DONT_WRITEBACK(1));
+
+			/* We are done with this one, adjust the queue
+			 * depth.
+			 */
+			cvmx_fau_atomic_add32(priv->tx_queue[packet_qos].fau, -1);
+			continue;
+		}
+		segments = work->word2.s.bufs;
+		skb_in_hw = USE_SKBUFFS_IN_HW && segments > 0;
+		if (likely(skb_in_hw)) {
+			skb = *pskb;
+			prefetch(&skb->head);
+			prefetch(&skb->len);
+		}
+		prefetch(cvm_oct_by_port[work->word1.cn38xx.ipprt]);
+
+		/* Immediately throw away all packets with receive errors */
+		if (unlikely(work->word2.snoip.rcv_error)) {
+			if (cvm_oct_check_rcv_error(work))
+				continue;
+		}
+
+		packet_len = work->word1.len;
+		/* We can only use the zero copy path if skbuffs are
+		 * in the FPA pool and the packet fits in a single
+		 * buffer.
+		 */
+		if (likely(skb_in_hw)) {
+			skb->data = phys_to_virt(packet_ptr.s.addr);
+			prefetch(skb->data);
+			skb->len = packet_len;
+			packets_to_replace = segments;
+			if (likely(segments == 1)) {
+				skb_set_tail_pointer(skb, skb->len);
+			} else {
+				struct sk_buff *current_skb = skb;
+				struct sk_buff *next_skb = NULL;
+				unsigned int segment_size;
+				bool first_frag = true;
+
+				skb_frag_list_init(skb);
+				/* Multi-segment packet. */
+				for (;;) {
+					/* Octeon Errata PKI-100: The segment size is
+					 * wrong. Until it is fixed, calculate the
+					 * segment size based on the packet pool
+					 * buffer size. When it is fixed, the
+					 * following line should be replaced with this
+					 * one: int segment_size =
+					 * segment_ptr.s.size;
+					 */
+					segment_size = CVMX_FPA_PACKET_POOL_SIZE -
+						(packet_ptr.s.addr - (((packet_ptr.s.addr >> 7) - packet_ptr.s.back) << 7));
+					if (segment_size > packet_len)
+						segment_size = packet_len;
+					if (!first_frag) {
+						current_skb->len = segment_size;
+						skb->data_len += segment_size;
+						skb->truesize += current_skb->truesize;
+					}
+					skb_set_tail_pointer(current_skb, segment_size);
+					packet_len -= segment_size;
+					segments--;
+					if (segments == 0)
+						break;
+					packet_ptr = *(union cvmx_buf_ptr *)phys_to_virt(packet_ptr.s.addr - 8);
+					next_skb = *cvm_oct_packet_to_skb(cvm_oct_get_buffer_ptr(packet_ptr));
+					if (first_frag) {
+						skb_frag_add_head(current_skb, next_skb);
+					} else {
+						current_skb->next = next_skb;
+						next_skb->next = NULL;
+					}
+					current_skb = next_skb;
+					first_frag = false;
+					current_skb->data = phys_to_virt(packet_ptr.s.addr);
+				}
+			}
+			packet_copied = false;
+		} else {
+			/* We have to copy the packet. First allocate
+			 * an skbuff for it.
+			 */
+			skb = dev_alloc_skb(packet_len);
+			if (!skb) {
+				printk_ratelimited("Port %d failed to allocate skbuff, packet dropped\n",
+						   work->word1.cn38xx.ipprt);
+				cvm_oct_free_work(work);
+				continue;
+			}
+
+			/* Check if we've received a packet that was
+			 * entirely stored in the work entry.
+			 */
+			if (unlikely(work->word2.s.bufs == 0)) {
+				u8 *ptr = work->packet_data;
+
+				if (likely(!work->word2.s.not_IP)) {
+					/* The beginning of the packet
+					 * moves for IP packets.
+					 */
+					if (work->word2.s.is_v6)
+						ptr += 2;
+					else
+						ptr += 6;
+				}
+				memcpy(skb_put(skb, packet_len), ptr, packet_len);
+				/* No packet buffers to free */
+			} else {
+				int segments = work->word2.s.bufs;
+				union cvmx_buf_ptr segment_ptr = work->packet_ptr;
+
+				while (segments--) {
+					union cvmx_buf_ptr next_ptr =
+					    *(union cvmx_buf_ptr *)phys_to_virt(segment_ptr.s.addr - 8);
+
+			/* Octeon Errata PKI-100: The segment size is
+			 * wrong. Until it is fixed, calculate the
+			 * segment size based on the packet pool
+			 * buffer size. When it is fixed, the
+			 * following line should be replaced with this
+			 * one: int segment_size =
+			 * segment_ptr.s.size;
+			 */
+					int segment_size = CVMX_FPA_PACKET_POOL_SIZE -
+						(segment_ptr.s.addr - (((segment_ptr.s.addr >> 7) - segment_ptr.s.back) << 7));
+					/* Don't copy more than what
+					 * is left in the packet.
+					 */
+					if (segment_size > packet_len)
+						segment_size = packet_len;
+					/* Copy the data into the packet */
+					memcpy(skb_put(skb, segment_size),
+					       phys_to_virt(segment_ptr.s.addr),
+					       segment_size);
+					packet_len -= segment_size;
+					segment_ptr = next_ptr;
+				}
+			}
+			packet_copied = true;
+		}
+
+		if (likely((work->word1.cn38xx.ipprt < TOTAL_NUMBER_OF_PORTS) &&
+			   cvm_oct_by_port[work->word1.cn38xx.ipprt])) {
+			struct octeon_ethernet *priv = cvm_oct_by_port[work->word1.cn38xx.ipprt];
+
+			/* Only accept packets for devices that are
+			 * currently up.
+			 */
+			if (likely(priv->netdev->flags & IFF_UP)) {
+				if (priv->rx_timestamp_hw) {
+					/* The first 8 bytes are the timestamp */
+					u64 ns = *(u64 *)skb->data;
+					struct skb_shared_hwtstamps *ts;
+					ts = skb_hwtstamps(skb);
+					ts->hwtstamp = ns_to_ktime(ns);
+					ts->syststamp = cvm_oct_ptp_to_ktime(ns);
+					__skb_pull(skb, 8);
+				}
+				skb->protocol = eth_type_trans(skb, priv->netdev);
+				skb->dev = priv->netdev;
+
+				if (unlikely(work->word2.s.not_IP || work->word2.s.IP_exc ||
+					work->word2.s.L4_error || !work->word2.s.tcp_or_udp))
+					skb->ip_summed = CHECKSUM_NONE;
+				else
+					skb->ip_summed = CHECKSUM_UNNECESSARY;
+
+				/* Increment RX stats for virtual ports */
+				if (work->word1.cn38xx.ipprt >= CVMX_PIP_NUM_INPUT_PORTS) {
+					atomic64_add(1, (atomic64_t *)&priv->netdev->stats.rx_packets);
+					atomic64_add(skb->len, (atomic64_t *)&priv->netdev->stats.rx_bytes);
+				}
+				netif_receive_skb(skb);
+				rx_count++;
+			} else {
+				/* Drop any packet received for a device that isn't up */
+				atomic64_add(1, (atomic64_t *)&priv->netdev->stats.rx_dropped);
+				dev_kfree_skb_any(skb);
+			}
+		} else {
+			/* Drop any packet received for a device that
+			 * doesn't exist.
+			 */
+			printk_ratelimited("Port %d not controlled by Linux, packet dropped\n",
+					   work->word1.cn38xx.ipprt);
+			dev_kfree_skb_any(skb);
+		}
+		/* Check to see if the skbuff and work share the same
+		 * packet buffer.
+		 */
+		if (USE_SKBUFFS_IN_HW && likely(!packet_copied)) {
+			/* This buffer needs to be replaced, increment
+			 * the number of buffers we need to free by
+			 * one.
+			 */
+			cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE,
+					      packets_to_replace);
+
+			cvmx_fpa_free(work, CVMX_FPA_WQE_POOL,
+				      DONT_WRITEBACK(1));
+		} else {
+			cvm_oct_free_work(work);
+		}
+	}
+	/* Restore the original POW group mask */
+	cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(coreid), old_group_mask);
+	if (USE_ASYNC_IOBDMA) {
+		/* Restore the scratch area */
+		cvmx_scratch_write64(CVMX_SCR_SCRATCH, old_scratch);
+	}
+	cvm_oct_rx_refill_pool(0);
+
+	if (rx_count < budget && napi != NULL) {
+		/* No more work */
+		napi_complete(napi);
+		cvm_oct_no_more_work(napi);
+	}
+	return rx_count;
+}
+
+#ifdef CONFIG_NET_POLL_CONTROLLER
+/**
+ * cvm_oct_poll_controller - poll for receive packets
+ * device.
+ *
+ * @dev:    Device to poll. Unused
+ */
+void cvm_oct_poll_controller(struct net_device *dev)
+{
+	cvm_oct_napi_poll(NULL, 16);
+}
+#endif
+
+void cvm_oct_rx_initialize(void)
+{
+	int i;
+	struct net_device *dev_for_napi = NULL;
+
+	if (list_empty(&cvm_oct_list))
+		panic("No net_devices were allocated.");
+
+	dev_for_napi = list_first_entry(&cvm_oct_list,
+					struct octeon_ethernet,
+					list)->netdev;
+
+	if (max_rx_cpus >= 1  && max_rx_cpus < num_online_cpus())
+		core_state.baseline_cores = max_rx_cpus;
+	else
+		core_state.baseline_cores = num_online_cpus();
+
+	for_each_possible_cpu(i) {
+		cvm_oct_napi[i].available = 1;
+		netif_napi_add(dev_for_napi, &cvm_oct_napi[i].napi,
+			       cvm_oct_napi_poll, rx_napi_weight);
+		napi_enable(&cvm_oct_napi[i].napi);
+	}
+	/* Before interrupts are enabled, no RX processing will occur,
+	 * so we can initialize all those things out side of the
+	 * lock.
+	 */
+	spin_lock_init(&core_state.lock);
+
+	/* Register an IRQ hander for to receive POW interrupts */
+	i = request_irq(OCTEON_IRQ_WORKQ0 + pow_receive_group,
+			cvm_oct_do_interrupt, 0, dev_for_napi->name, &cvm_oct_list);
+
+	if (i)
+		panic("Could not acquire Ethernet IRQ %d\n",
+		      OCTEON_IRQ_WORKQ0 + pow_receive_group);
+
+	/* Scheduld NAPI now.  This will indirectly enable interrupts. */
+	preempt_disable();
+	cvm_oct_enable_one_cpu();
+	preempt_enable();
+}
+
+void cvm_oct_rx_shutdown(void)
+{
+	int i;
+	/* Shutdown all of the NAPIs */
+	for_each_possible_cpu(i)
+		netif_napi_del(&cvm_oct_napi[i].napi);
+
+	/* Free the interrupt handler */
+	free_irq(OCTEON_IRQ_WORKQ0 + pow_receive_group, &cvm_oct_list);
+
+}
diff --git a/drivers/net/ethernet/octeon/ethernet-sgmii.c b/drivers/net/ethernet/octeon/ethernet-sgmii.c
new file mode 100644
index 0000000..c6768af
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-sgmii.c
@@ -0,0 +1,122 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+**********************************************************************/
+#include <linux/phy.h>
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/ratelimit.h>
+#include <net/dst.h>
+
+#include <asm/octeon/octeon.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+
+#include <asm/octeon/cvmx-helper.h>
+
+#include <asm/octeon/cvmx-gmxx-defs.h>
+
+/* Although these functions are called cvm_oct_sgmii_*, they also
+ * happen to be used for the XAUI ports as well.
+ */
+
+static void cvm_oct_sgmii_poll(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	cvmx_helper_link_info_t link_info;
+
+	link_info = cvmx_helper_link_get(priv->ipd_port);
+	if (link_info.u64 == priv->link_info)
+		return;
+
+	link_info = cvmx_helper_link_autoconf(priv->ipd_port);
+	priv->link_info = link_info.u64;
+
+	/* Tell the core */
+	cvm_oct_set_carrier(priv, link_info);
+}
+
+int cvm_oct_sgmii_open(struct net_device *dev)
+{
+	union cvmx_gmxx_prtx_cfg gmx_cfg;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	cvmx_helper_link_info_t link_info;
+	int rv;
+
+	rv = cvm_oct_phy_setup_device(dev);
+	if (rv)
+		return rv;
+
+	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
+	gmx_cfg.s.en = 1;
+	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface), gmx_cfg.u64);
+
+	if (!octeon_is_simulation()) {
+		if (priv->phydev) {
+			int r = phy_read_status(priv->phydev);
+			if (r == 0 && priv->phydev->link == 0)
+				netif_carrier_off(dev);
+			cvm_oct_adjust_link(dev);
+		} else {
+			link_info = cvmx_helper_link_get(priv->ipd_port);
+			if (!link_info.s.link_up)
+				netif_carrier_off(dev);
+			spin_lock(&priv->poll_lock);
+			priv->poll = cvm_oct_sgmii_poll;
+			spin_unlock(&priv->poll_lock);
+			cvm_oct_sgmii_poll(dev);
+		}
+	}
+	return 0;
+}
+
+int cvm_oct_sgmii_stop(struct net_device *dev)
+{
+	union cvmx_gmxx_prtx_cfg gmx_cfg;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
+	gmx_cfg.s.en = 0;
+	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface), gmx_cfg.u64);
+
+	spin_lock(&priv->poll_lock);
+	priv->poll = NULL;
+	spin_unlock(&priv->poll_lock);
+
+	if (priv->phydev)
+		phy_disconnect(priv->phydev);
+	priv->phydev = NULL;
+
+	return 0;
+}
+
+int cvm_oct_sgmii_init(struct net_device *dev)
+{
+	cvm_oct_common_init(dev);
+	dev->netdev_ops->ndo_stop(dev);
+
+	return 0;
+}
diff --git a/drivers/net/ethernet/octeon/ethernet-spi.c b/drivers/net/ethernet/octeon/ethernet-spi.c
new file mode 100644
index 0000000..d37e53f
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-spi.c
@@ -0,0 +1,283 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/interrupt.h>
+#include <net/dst.h>
+
+#include <asm/octeon/octeon.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+
+#include <asm/octeon/cvmx-spi.h>
+
+#include <asm/octeon/cvmx-npi-defs.h>
+#include <asm/octeon/cvmx-spxx-defs.h>
+#include <asm/octeon/cvmx-stxx-defs.h>
+
+static int number_spi_ports;
+static int need_retrain[2] = { 0, 0 };
+
+static irqreturn_t cvm_oct_spi_rml_interrupt(int cpl, void *dev_id)
+{
+	irqreturn_t return_status = IRQ_NONE;
+	union cvmx_npi_rsl_int_blocks rsl_int_blocks;
+
+	/* Check and see if this interrupt was caused by the GMX block */
+	rsl_int_blocks.u64 = cvmx_read_csr(CVMX_NPI_RSL_INT_BLOCKS);
+	if (rsl_int_blocks.s.spx1) {	/* 19 - SPX1_INT_REG & STX1_INT_REG */
+		union cvmx_spxx_int_reg spx_int_reg;
+		union cvmx_stxx_int_reg stx_int_reg;
+
+		spx_int_reg.u64 = cvmx_read_csr(CVMX_SPXX_INT_REG(1));
+		cvmx_write_csr(CVMX_SPXX_INT_REG(1), spx_int_reg.u64);
+		if (!need_retrain[1]) {
+			spx_int_reg.u64 &= cvmx_read_csr(CVMX_SPXX_INT_MSK(1));
+			if (spx_int_reg.s.spf)
+				pr_err("SPI1: SRX Spi4 interface down\n");
+			if (spx_int_reg.s.calerr)
+				pr_err("SPI1: SRX Spi4 Calendar table parity error\n");
+			if (spx_int_reg.s.syncerr)
+				pr_err("SPI1: SRX Consecutive Spi4 DIP4 errors have exceeded SPX_ERR_CTL[ERRCNT]\n");
+			if (spx_int_reg.s.diperr)
+				pr_err("SPI1: SRX Spi4 DIP4 error\n");
+			if (spx_int_reg.s.tpaovr)
+				pr_err("SPI1: SRX Selected port has hit TPA overflow\n");
+			if (spx_int_reg.s.rsverr)
+				pr_err("SPI1: SRX Spi4 reserved control word detected\n");
+			if (spx_int_reg.s.drwnng)
+				pr_err("SPI1: SRX Spi4 receive FIFO drowning/overflow\n");
+			if (spx_int_reg.s.clserr)
+				pr_err("SPI1: SRX Spi4 packet closed on non-16B alignment without EOP\n");
+			if (spx_int_reg.s.spiovr)
+				pr_err("SPI1: SRX Spi4 async FIFO overflow\n");
+			if (spx_int_reg.s.abnorm)
+				pr_err("SPI1: SRX Abnormal packet termination (ERR bit)\n");
+			if (spx_int_reg.s.prtnxa)
+				pr_err("SPI1: SRX Port out of range\n");
+		}
+
+		stx_int_reg.u64 = cvmx_read_csr(CVMX_STXX_INT_REG(1));
+		cvmx_write_csr(CVMX_STXX_INT_REG(1), stx_int_reg.u64);
+		if (!need_retrain[1]) {
+			stx_int_reg.u64 &= cvmx_read_csr(CVMX_STXX_INT_MSK(1));
+			if (stx_int_reg.s.syncerr)
+				pr_err("SPI1: STX Interface encountered a fatal error\n");
+			if (stx_int_reg.s.frmerr)
+				pr_err("SPI1: STX FRMCNT has exceeded STX_DIP_CNT[MAXFRM]\n");
+			if (stx_int_reg.s.unxfrm)
+				pr_err("SPI1: STX Unexpected framing sequence\n");
+			if (stx_int_reg.s.nosync)
+				pr_err("SPI1: STX ERRCNT has exceeded STX_DIP_CNT[MAXDIP]\n");
+			if (stx_int_reg.s.diperr)
+				pr_err("SPI1: STX DIP2 error on the Spi4 Status channel\n");
+			if (stx_int_reg.s.datovr)
+				pr_err("SPI1: STX Spi4 FIFO overflow error\n");
+			if (stx_int_reg.s.ovrbst)
+				pr_err("SPI1: STX Transmit packet burst too big\n");
+			if (stx_int_reg.s.calpar1)
+				pr_err("SPI1: STX Calendar Table Parity Error Bank1\n");
+			if (stx_int_reg.s.calpar0)
+				pr_err("SPI1: STX Calendar Table Parity Error Bank0\n");
+		}
+
+		cvmx_write_csr(CVMX_SPXX_INT_MSK(1), 0);
+		cvmx_write_csr(CVMX_STXX_INT_MSK(1), 0);
+		need_retrain[1] = 1;
+		return_status = IRQ_HANDLED;
+	}
+
+	if (rsl_int_blocks.s.spx0) {	/* 18 - SPX0_INT_REG & STX0_INT_REG */
+		union cvmx_spxx_int_reg spx_int_reg;
+		union cvmx_stxx_int_reg stx_int_reg;
+
+		spx_int_reg.u64 = cvmx_read_csr(CVMX_SPXX_INT_REG(0));
+		cvmx_write_csr(CVMX_SPXX_INT_REG(0), spx_int_reg.u64);
+		if (!need_retrain[0]) {
+			spx_int_reg.u64 &= cvmx_read_csr(CVMX_SPXX_INT_MSK(0));
+			if (spx_int_reg.s.spf)
+				pr_err("SPI0: SRX Spi4 interface down\n");
+			if (spx_int_reg.s.calerr)
+				pr_err("SPI0: SRX Spi4 Calendar table parity error\n");
+			if (spx_int_reg.s.syncerr)
+				pr_err("SPI0: SRX Consecutive Spi4 DIP4 errors have exceeded SPX_ERR_CTL[ERRCNT]\n");
+			if (spx_int_reg.s.diperr)
+				pr_err("SPI0: SRX Spi4 DIP4 error\n");
+			if (spx_int_reg.s.tpaovr)
+				pr_err("SPI0: SRX Selected port has hit TPA overflow\n");
+			if (spx_int_reg.s.rsverr)
+				pr_err("SPI0: SRX Spi4 reserved control word detected\n");
+			if (spx_int_reg.s.drwnng)
+				pr_err("SPI0: SRX Spi4 receive FIFO drowning/overflow\n");
+			if (spx_int_reg.s.clserr)
+				pr_err("SPI0: SRX Spi4 packet closed on non-16B alignment without EOP\n");
+			if (spx_int_reg.s.spiovr)
+				pr_err("SPI0: SRX Spi4 async FIFO overflow\n");
+			if (spx_int_reg.s.abnorm)
+				pr_err("SPI0: SRX Abnormal packet termination (ERR bit)\n");
+			if (spx_int_reg.s.prtnxa)
+				pr_err("SPI0: SRX Port out of range\n");
+		}
+
+		stx_int_reg.u64 = cvmx_read_csr(CVMX_STXX_INT_REG(0));
+		cvmx_write_csr(CVMX_STXX_INT_REG(0), stx_int_reg.u64);
+		if (!need_retrain[0]) {
+			stx_int_reg.u64 &= cvmx_read_csr(CVMX_STXX_INT_MSK(0));
+			if (stx_int_reg.s.syncerr)
+				pr_err("SPI0: STX Interface encountered a fatal error\n");
+			if (stx_int_reg.s.frmerr)
+				pr_err("SPI0: STX FRMCNT has exceeded STX_DIP_CNT[MAXFRM]\n");
+			if (stx_int_reg.s.unxfrm)
+				pr_err("SPI0: STX Unexpected framing sequence\n");
+			if (stx_int_reg.s.nosync)
+				pr_err("SPI0: STX ERRCNT has exceeded STX_DIP_CNT[MAXDIP]\n");
+			if (stx_int_reg.s.diperr)
+				pr_err("SPI0: STX DIP2 error on the Spi4 Status channel\n");
+			if (stx_int_reg.s.datovr)
+				pr_err("SPI0: STX Spi4 FIFO overflow error\n");
+			if (stx_int_reg.s.ovrbst)
+				pr_err("SPI0: STX Transmit packet burst too big\n");
+			if (stx_int_reg.s.calpar1)
+				pr_err("SPI0: STX Calendar Table Parity Error Bank1\n");
+			if (stx_int_reg.s.calpar0)
+				pr_err("SPI0: STX Calendar Table Parity Error Bank0\n");
+		}
+
+		cvmx_write_csr(CVMX_SPXX_INT_MSK(0), 0);
+		cvmx_write_csr(CVMX_STXX_INT_MSK(0), 0);
+		need_retrain[0] = 1;
+		return_status = IRQ_HANDLED;
+	}
+
+	return return_status;
+}
+
+static void cvm_oct_spi_enable_error_reporting(int interface)
+{
+	union cvmx_spxx_int_msk spxx_int_msk;
+	union cvmx_stxx_int_msk stxx_int_msk;
+
+	spxx_int_msk.u64 = cvmx_read_csr(CVMX_SPXX_INT_MSK(interface));
+	spxx_int_msk.s.calerr = 1;
+	spxx_int_msk.s.syncerr = 1;
+	spxx_int_msk.s.diperr = 1;
+	spxx_int_msk.s.tpaovr = 1;
+	spxx_int_msk.s.rsverr = 1;
+	spxx_int_msk.s.drwnng = 1;
+	spxx_int_msk.s.clserr = 1;
+	spxx_int_msk.s.spiovr = 1;
+	spxx_int_msk.s.abnorm = 1;
+	spxx_int_msk.s.prtnxa = 1;
+	cvmx_write_csr(CVMX_SPXX_INT_MSK(interface), spxx_int_msk.u64);
+
+	stxx_int_msk.u64 = cvmx_read_csr(CVMX_STXX_INT_MSK(interface));
+	stxx_int_msk.s.frmerr = 1;
+	stxx_int_msk.s.unxfrm = 1;
+	stxx_int_msk.s.nosync = 1;
+	stxx_int_msk.s.diperr = 1;
+	stxx_int_msk.s.datovr = 1;
+	stxx_int_msk.s.ovrbst = 1;
+	stxx_int_msk.s.calpar1 = 1;
+	stxx_int_msk.s.calpar0 = 1;
+	cvmx_write_csr(CVMX_STXX_INT_MSK(interface), stxx_int_msk.u64);
+}
+
+static void cvm_oct_spi_poll(struct net_device *dev)
+{
+	static int spi4000_port;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	int interface;
+
+	for (interface = 0; interface < 2; interface++) {
+		if ((priv->ipd_port == interface * 16) && need_retrain[interface]) {
+			if (cvmx_spi_restart_interface
+			    (interface, CVMX_SPI_MODE_DUPLEX, 10) == 0) {
+				need_retrain[interface] = 0;
+				cvm_oct_spi_enable_error_reporting(interface);
+			}
+		}
+
+		/* The SPI4000 TWSI interface is very slow. In order
+		 * not to bring the system to a crawl, we only poll a
+		 * single port every second. This means negotiation
+		 * speed changes take up to 10 seconds, but at least
+		 * we don't waste absurd amounts of time waiting for
+		 * TWSI.
+		 */
+		if (priv->ipd_port == spi4000_port) {
+			/* This function does nothing if it is called on an
+			 * interface without a SPI4000.
+			 */
+			cvmx_spi4000_check_speed(interface, priv->ipd_port);
+			/* Normal ordering increments. By decrementing
+			 * we only match once per iteration.
+			 */
+			spi4000_port--;
+			if (spi4000_port < 0)
+				spi4000_port = 10;
+		}
+	}
+}
+
+int cvm_oct_spi_init(struct net_device *dev)
+{
+	int r;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	if (number_spi_ports == 0) {
+		r = request_irq(OCTEON_IRQ_RML, cvm_oct_spi_rml_interrupt,
+				IRQF_SHARED, "SPI", &number_spi_ports);
+		if (r) {
+			netdev_err(dev, "request_irq(%d) failed\n",
+				   OCTEON_IRQ_RML);
+			return r;
+		}
+	}
+	number_spi_ports++;
+
+	if ((priv->ipd_port == 0) || (priv->ipd_port == 16)) {
+		cvm_oct_spi_enable_error_reporting(priv->interface);
+		priv->poll = cvm_oct_spi_poll;
+	}
+	cvm_oct_common_init(dev);
+	return 0;
+}
+
+void cvm_oct_spi_uninit(struct net_device *dev)
+{
+	int interface;
+
+	number_spi_ports--;
+	if (number_spi_ports == 0) {
+		for (interface = 0; interface < 2; interface++) {
+			cvmx_write_csr(CVMX_SPXX_INT_MSK(interface), 0);
+			cvmx_write_csr(CVMX_STXX_INT_MSK(interface), 0);
+		}
+		free_irq(OCTEON_IRQ_RML, &number_spi_ports);
+	}
+}
diff --git a/drivers/net/ethernet/octeon/ethernet-tx.c b/drivers/net/ethernet/octeon/ethernet-tx.c
new file mode 100644
index 0000000..95572f1
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-tx.c
@@ -0,0 +1,487 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/init.h>
+#include <linux/etherdevice.h>
+#include <linux/ip.h>
+#include <linux/ratelimit.h>
+#include <linux/string.h>
+#include <linux/interrupt.h>
+#include <net/dst.h>
+
+#include <asm/octeon/octeon.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+
+#include <asm/octeon/cvmx-wqe.h>
+#include <asm/octeon/cvmx-fau.h>
+#include <asm/octeon/cvmx-pip.h>
+#include <asm/octeon/cvmx-pko.h>
+#include <asm/octeon/cvmx-helper.h>
+
+#include <asm/octeon/cvmx-gmxx-defs.h>
+
+/*
+ * You can define GET_SKBUFF_QOS() to override how the skbuff output
+ * function determines which output queue is used. The default
+ * implementation always uses the base queue for the port. If, for
+ * example, you wanted to use the skb->priority fieid, define
+ * GET_SKBUFF_QOS as: #define GET_SKBUFF_QOS(skb) ((skb)->priority)
+ */
+#ifndef GET_SKBUFF_QOS
+#define GET_SKBUFF_QOS(skb) 0
+#endif
+
+#if REUSE_SKBUFFS_WITHOUT_FREE
+static bool cvm_oct_skb_ok_for_reuse(struct sk_buff *skb)
+{
+	unsigned char *fpa_head = cvm_oct_get_fpa_head(skb);
+
+	if (unlikely(skb->data < fpa_head))
+		return false;
+
+	if (unlikely(fpa_head - skb->head < sizeof(void *)))
+		return false;
+
+	if (unlikely((skb_end_pointer(skb) - fpa_head) < CVMX_FPA_PACKET_POOL_SIZE))
+		return false;
+
+	if (unlikely(skb_shared(skb)) ||
+	    unlikely(skb_cloned(skb)) ||
+	    unlikely(skb->fclone != SKB_FCLONE_UNAVAILABLE))
+		return false;
+
+	return true;
+}
+
+static void cvm_oct_skb_prepare_for_reuse(struct sk_buff *skb)
+{
+	int r;
+	unsigned char *fpa_head = cvm_oct_get_fpa_head(skb);
+
+	skb->data_len = 0;
+	skb_frag_list_init(skb);
+
+	/* The check also resets all the fields. */
+	r = skb_recycle_check(skb, CVMX_FPA_PACKET_POOL_SIZE);
+	WARN(!r, "SKB recycle logic fail\n");
+
+	*(struct sk_buff **)(fpa_head - sizeof(void *)) = skb;
+	skb->truesize = sizeof(*skb) + skb_end_pointer(skb) - skb->head;
+}
+
+static inline void cvm_oct_set_back(struct sk_buff *skb,
+				    union cvmx_buf_ptr *hw_buffer)
+{
+	unsigned char *fpa_head = cvm_oct_get_fpa_head(skb);
+
+	hw_buffer->s.back = ((unsigned long)skb->data >> 7) - ((unsigned long)fpa_head >> 7);
+}
+#else
+static bool cvm_oct_skb_ok_for_reuse(struct sk_buff *skb)
+{
+	return false;
+}
+static void cvm_oct_skb_prepare_for_reuse(struct sk_buff *skb)
+{
+	/* Do nothing */
+}
+
+static inline void cvm_oct_set_back(struct sk_buff *skb,
+				    union cvmx_buf_ptr *hw_buffer)
+{
+	/* Do nothing. */
+}
+
+#endif
+
+/**
+ * cvm_oct_xmit - transmit a packet
+ * @skb:    Packet to send
+ * @dev:    Device info structure
+ *
+ * Returns Always returns NETDEV_TX_OK
+ */
+int cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+	struct sk_buff *skb_tmp;
+	cvmx_pko_command_word0_t pko_command;
+	union cvmx_buf_ptr hw_buffer;
+	u64 old_scratch;
+	u64 old_scratch2;
+	int qos;
+	int i;
+	int frag_count;
+	enum {QUEUE_HW, QUEUE_WQE, QUEUE_DROP} queue_type;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	s32 queue_depth;
+	s32 buffers_to_free;
+	s32 buffers_being_recycled;
+	unsigned long flags;
+	cvmx_wqe_t *work = NULL;
+	bool timestamp_this_skb = false;
+
+	/* Prefetch the private data structure.  It is larger than one
+	 * cache line.
+	 */
+	prefetch(priv);
+
+	if (USE_ASYNC_IOBDMA) {
+		/* Save scratch in case userspace is using it */
+		CVMX_SYNCIOBDMA;
+		old_scratch = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
+		old_scratch2 = cvmx_scratch_read64(CVMX_SCR_SCRATCH + 8);
+
+		/* Fetch and increment the number of packets to be
+		 * freed.
+		 */
+		cvmx_fau_async_fetch_and_add32(CVMX_SCR_SCRATCH + 8,
+					       FAU_NUM_PACKET_BUFFERS_TO_FREE,
+					       0);
+	}
+
+#ifdef CVM_OCT_LOCKLESS
+	qos = cvmx_get_core_num();
+#else
+	/* The check on CVMX_PKO_QUEUES_PER_PORT_* is designed to
+	 * completely remove "qos" in the event neither interface
+	 * supports multiple queues per port.
+	 */
+	if (priv->tx_multiple_queues) {
+		qos = GET_SKBUFF_QOS(skb);
+		if (qos <= 0)
+			qos = 0;
+		else if (qos >= priv->num_tx_queues)
+			qos = 0;
+	} else
+		qos = 0;
+#endif
+	if (USE_ASYNC_IOBDMA) {
+		cvmx_fau_async_fetch_and_add32(CVMX_SCR_SCRATCH,
+					       priv->tx_queue[qos].fau, 1);
+	}
+
+	frag_count = 0;
+	if (skb_has_frag_list(skb))
+		skb_walk_frags(skb, skb_tmp)
+			frag_count++;
+	/* We have space for 12 segment pointers, If there will be
+	 * more than that, we must linearize.  The count is: 1 (base
+	 * SKB) + frag_count + nr_frags.
+	 */
+	if (unlikely(skb_shinfo(skb)->nr_frags + frag_count > 11)) {
+		if (unlikely(__skb_linearize(skb))) {
+			queue_type = QUEUE_DROP;
+			goto skip_xmit;
+		}
+		frag_count = 0;
+	}
+
+	/* The CN3XXX series of parts has an errata (GMX-401) which
+	 * causes the GMX block to hang if a collision occurs towards
+	 * the end of a <68 byte packet. As a workaround for this, we
+	 * pad packets to be 68 bytes whenever we are in half duplex
+	 * mode. We don't handle the case of having a small packet but
+	 * no room to add the padding.  The kernel should always give
+	 * us at least a cache line
+	 */
+	if ((skb->len < 64) && OCTEON_IS_MODEL(OCTEON_CN3XXX)) {
+		union cvmx_gmxx_prtx_cfg gmx_prt_cfg;
+
+		if (priv->interface < 2) {
+			/* We only need to pad packet in half duplex mode */
+			gmx_prt_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
+			if (gmx_prt_cfg.s.duplex == 0) {
+				int add_bytes = 64 - skb->len;
+				if ((skb_tail_pointer(skb) + add_bytes) <= skb_end_pointer(skb))
+					memset(__skb_put(skb, add_bytes), 0, add_bytes);
+			}
+		}
+	}
+
+	/* Build the PKO command */
+	pko_command.u64 = 0;
+	/* Don't pollute L2 with the outgoing packet */
+	pko_command.s.n2 = 1;
+	pko_command.s.segs = 1;
+	pko_command.s.total_bytes = skb->len;
+	pko_command.s.size0 = CVMX_FAU_OP_SIZE_32;
+	pko_command.s.subone0 = 1;
+	pko_command.s.reg0 = priv->tx_queue[qos].fau;
+	pko_command.s.dontfree = 1;
+
+	/* Build the PKO buffer pointer */
+	hw_buffer.u64 = 0; /* Implies pool == 0, i == 0 */
+	if (skb_shinfo(skb)->nr_frags == 0 && frag_count == 0) {
+		hw_buffer.s.addr = virt_to_phys(skb->data);
+		hw_buffer.s.size = skb->len;
+		cvm_oct_set_back(skb, &hw_buffer);
+		buffers_being_recycled = 1;
+	} else {
+		u64 *hw_buffer_list;
+		bool can_do_reuse = true;
+
+		work = cvmx_fpa_alloc(CVMX_FPA_WQE_POOL);
+		if (unlikely(!work)) {
+			netdev_err(dev, "Failed WQE allocate\n");
+			queue_type = QUEUE_DROP;
+			goto skip_xmit;
+		}
+		hw_buffer_list = (u64 *)work->packet_data;
+		hw_buffer.s.addr = virt_to_phys(skb->data);
+		hw_buffer.s.size = skb_headlen(skb);
+		if (skb_shinfo(skb)->nr_frags == 0 && cvm_oct_skb_ok_for_reuse(skb)) {
+			cvm_oct_set_back(skb, &hw_buffer);
+		} else {
+			hw_buffer.s.back = 0;
+			can_do_reuse = false;
+		}
+		hw_buffer_list[0] = hw_buffer.u64;
+		for (i = 1; i <= skb_shinfo(skb)->nr_frags; i++) {
+			struct skb_frag_struct *fs = skb_shinfo(skb)->frags + i - 1;
+			hw_buffer.s.addr = virt_to_phys((u8 *)page_address(fs->page.p) + fs->page_offset);
+			hw_buffer.s.size = fs->size;
+			hw_buffer_list[i] = hw_buffer.u64;
+			can_do_reuse = false;
+		}
+		skb_walk_frags(skb, skb_tmp) {
+			hw_buffer.s.addr = virt_to_phys(skb_tmp->data);
+			hw_buffer.s.size = skb_tmp->len;
+			if (cvm_oct_skb_ok_for_reuse(skb_tmp)) {
+				cvm_oct_set_back(skb_tmp, &hw_buffer);
+			} else {
+				hw_buffer.s.back = 0;
+				can_do_reuse = false;
+			}
+			hw_buffer_list[i] = hw_buffer.u64;
+			i++;
+		}
+		hw_buffer.s.addr = virt_to_phys(hw_buffer_list);
+		hw_buffer.s.size = i;
+		hw_buffer.s.back = 0;
+		hw_buffer.s.pool = CVMX_FPA_WQE_POOL;
+		buffers_being_recycled = i;
+		pko_command.s.segs = hw_buffer.s.size;
+		pko_command.s.gather = 1;
+		if (!can_do_reuse)
+			goto dont_put_skbuff_in_hw;
+	}
+
+	if (unlikely(priv->tx_timestamp_hw &&
+		     (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP))) {
+		timestamp_this_skb = true;
+		skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+		goto dont_put_skbuff_in_hw;
+	}
+	/* See if we can put this skb in the FPA pool. Any strange
+	 * behavior from the Linux networking stack will most likely
+	 * be caused by a bug in the following code. If some field is
+	 * in use by the network stack and get carried over when a
+	 * buffer is reused, bad thing may happen.  If in doubt and
+	 * you dont need the absolute best performance, disable the
+	 * define REUSE_SKBUFFS_WITHOUT_FREE. The reuse of buffers has
+	 * shown a 25% increase in performance under some loads.
+	 */
+#if REUSE_SKBUFFS_WITHOUT_FREE
+	if (!cvm_oct_skb_ok_for_reuse(skb))
+		goto dont_put_skbuff_in_hw;
+	if (unlikely(skb_header_cloned(skb)))
+		goto dont_put_skbuff_in_hw;
+	if (unlikely(skb->destructor))
+		goto dont_put_skbuff_in_hw;
+
+
+	/* We can use this buffer in the FPA.  We don't need the FAU
+	 * update anymore
+	 */
+	pko_command.s.dontfree = 0;
+
+#endif /* REUSE_SKBUFFS_WITHOUT_FREE */
+
+dont_put_skbuff_in_hw:
+
+	/* Check if we can use the hardware checksumming */
+	if (USE_HW_TCPUDP_CHECKSUM && (skb->protocol == htons(ETH_P_IP)) &&
+	    (ip_hdr(skb)->version == 4) && (ip_hdr(skb)->ihl == 5) &&
+	    ((ip_hdr(skb)->frag_off == 0) || (ip_hdr(skb)->frag_off == htons(1 << 14)))
+	    && ((ip_hdr(skb)->protocol == IPPROTO_TCP) || (ip_hdr(skb)->protocol == IPPROTO_UDP))) {
+		/* Use hardware checksum calc */
+		pko_command.s.ipoffp1 = sizeof(struct ethhdr) + 1;
+		if (unlikely(priv->imode == CVMX_HELPER_INTERFACE_MODE_SRIO))
+			pko_command.s.ipoffp1 += 8;
+	}
+
+	if (USE_ASYNC_IOBDMA) {
+		/* Get the number of skbuffs in use by the hardware */
+		CVMX_SYNCIOBDMA;
+		queue_depth = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
+		buffers_to_free = cvmx_scratch_read64(CVMX_SCR_SCRATCH + 8);
+	} else {
+		/* Get the number of skbuffs in use by the hardware */
+		queue_depth = cvmx_fau_fetch_and_add32(priv->tx_queue[qos].fau, 1);
+		buffers_to_free = cvmx_fau_fetch_and_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);
+	}
+
+	/* If we're sending faster than the receive can free them then
+	 * don't do the HW free.
+	 */
+	if (unlikely(buffers_to_free < -100))
+		pko_command.s.dontfree = 1;
+
+	/* Drop this packet if we have too many already queued to the HW */
+	if (unlikely(queue_depth >= MAX_OUT_QUEUE_DEPTH)) {
+		if (dev->tx_queue_len != 0) {
+			netif_stop_queue(dev);
+		} else {
+			/* If not using normal queueing.  */
+			queue_type = QUEUE_DROP;
+			goto skip_xmit;
+		}
+	}
+
+	if (pko_command.s.dontfree) {
+		queue_type = QUEUE_WQE;
+	} else {
+		queue_type = QUEUE_HW;
+		if (buffers_being_recycled > 1) {
+			struct sk_buff *tskb, *nskb;
+			/* We are committed to use hardware free, restore the
+			 * frag list to empty on the first SKB
+			 */
+			tskb = skb_shinfo(skb)->frag_list;
+			while (tskb) {
+				nskb = tskb->next;
+				cvm_oct_skb_prepare_for_reuse(tskb);
+				tskb = nskb;
+			}
+		}
+		cvm_oct_skb_prepare_for_reuse(skb);
+	}
+
+	if (queue_type == QUEUE_WQE) {
+		if (!work) {
+			work = cvmx_fpa_alloc(CVMX_FPA_WQE_POOL);
+			if (unlikely(!work)) {
+				netdev_err(dev, "Failed WQE allocate\n");
+				queue_type = QUEUE_DROP;
+				goto skip_xmit;
+			}
+		}
+
+		pko_command.s.rsp = 1;
+		pko_command.s.wqp = 1;
+		/* work->unused will carry the qos for this packet,
+		 * this allows us to find the proper FAU when freeing
+		 * the packet.  We decrement the FAU when the WQE is
+		 * replaced in the pool.
+		 */
+		pko_command.s.reg0 = 0;
+		work->word0.u64 = 0;
+		work->word0.raw.unused = (u8)qos;
+
+		work->word1.u64 = 0;
+		work->word1.tag_type = CVMX_POW_TAG_TYPE_NULL;
+		work->word1.tag = 0;
+		work->word2.u64 = 0;
+		work->word2.s.software = 1;
+		cvmx_wqe_set_grp(work, pow_receive_group);
+		work->packet_ptr.u64 = (unsigned long)skb;
+	}
+
+	local_irq_save(flags);
+
+	cvmx_pko_send_packet_prepare_pkoid(priv->pko_port,
+					   priv->tx_queue[qos].queue,
+					   CVMX_PKO_LOCK_CMD_QUEUE);
+
+	/* Send the packet to the output queue */
+	if (queue_type == QUEUE_WQE) {
+		u64 word2 = virt_to_phys(work);
+		if (timestamp_this_skb)
+			word2 |= 1ull << 40; /* Bit 40 controls timestamps */
+
+		if (unlikely(cvmx_pko_send_packet_finish3_pkoid(priv->pko_port,
+							  priv->tx_queue[qos].queue, pko_command, hw_buffer,
+							  word2, CVMX_PKO_LOCK_CMD_QUEUE))) {
+				queue_type = QUEUE_DROP;
+				cvmx_fpa_free(work, CVMX_FPA_WQE_POOL, DONT_WRITEBACK(1));
+				netdev_err(dev, "Failed to send the packet with wqe\n");
+		}
+	} else {
+		if (unlikely(cvmx_pko_send_packet_finish_pkoid(priv->pko_port,
+							 priv->tx_queue[qos].queue,
+							 pko_command, hw_buffer,
+							 CVMX_PKO_LOCK_CMD_QUEUE))) {
+			netdev_err(dev, "Failed to send the packet\n");
+			queue_type = QUEUE_DROP;
+		}
+	}
+	local_irq_restore(flags);
+
+skip_xmit:
+	switch (queue_type) {
+	case QUEUE_DROP:
+		cvmx_fau_atomic_add32(priv->tx_queue[qos].fau, -1);
+		dev_kfree_skb_any(skb);
+		dev->stats.tx_dropped++;
+		break;
+	case QUEUE_HW:
+		cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, -buffers_being_recycled);
+		break;
+	case QUEUE_WQE:
+		/* Cleanup is done on the RX path when the WQE returns */
+		break;
+	default:
+		BUG();
+	}
+
+	if (USE_ASYNC_IOBDMA) {
+		CVMX_SYNCIOBDMA;
+		/* Restore the scratch area */
+		cvmx_scratch_write64(CVMX_SCR_SCRATCH, old_scratch);
+		cvmx_scratch_write64(CVMX_SCR_SCRATCH + 8, old_scratch2);
+	}
+
+	return NETDEV_TX_OK;
+}
+
+void cvm_oct_tx_shutdown_dev(struct net_device *dev)
+{
+}
+
+void cvm_oct_tx_initialize(void)
+{
+
+}
+
+void cvm_oct_tx_shutdown(void)
+{
+
+}
diff --git a/drivers/net/ethernet/octeon/ethernet.c b/drivers/net/ethernet/octeon/ethernet.c
new file mode 100644
index 0000000..51ad641
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet.c
@@ -0,0 +1,825 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+#include <linux/platform_device.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/phy.h>
+#include <linux/slab.h>
+#include <linux/of_net.h>
+#include <linux/interrupt.h>
+
+#include <net/dst.h>
+
+#include <asm/octeon/octeon.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+
+#include <asm/octeon/cvmx-pip.h>
+#include <asm/octeon/cvmx-pko.h>
+#include <asm/octeon/cvmx-fau.h>
+#include <asm/octeon/cvmx-ipd.h>
+#include <asm/octeon/cvmx-helper.h>
+
+#include <asm/octeon/cvmx-gmxx-defs.h>
+#include <asm/octeon/cvmx-smix-defs.h>
+
+#if defined(CONFIG_CAVIUM_OCTEON_NUM_PACKET_BUFFERS) \
+	&& CONFIG_CAVIUM_OCTEON_NUM_PACKET_BUFFERS
+int num_packet_buffers = CONFIG_CAVIUM_OCTEON_NUM_PACKET_BUFFERS;
+#else
+int num_packet_buffers = 1024;
+#endif
+module_param(num_packet_buffers, int, 0444);
+MODULE_PARM_DESC(num_packet_buffers, "\n"
+	"\tNumber of packet buffers to allocate and store in the\n"
+	"\tFPA. By default, 1024 packet buffers are used unless\n"
+	"\tCONFIG_CAVIUM_OCTEON_NUM_PACKET_BUFFERS is defined.");
+
+int pow_receive_group = 15;
+module_param(pow_receive_group, int, 0444);
+MODULE_PARM_DESC(pow_receive_group, "\n"
+	"\tPOW group to receive packets from. All ethernet hardware\n"
+	"\twill be configured to send incomming packets to this POW\n"
+	"\tgroup. Also any other software can submit packets to this\n"
+	"\tgroup for the kernel to process.");
+
+int max_rx_cpus = -1;
+module_param(max_rx_cpus, int, 0444);
+MODULE_PARM_DESC(max_rx_cpus, "\n"
+	"\t\tThe maximum number of CPUs to use for packet reception.\n"
+	"\t\tUse -1 to use all available CPUs.");
+
+int rx_napi_weight = 32;
+module_param(rx_napi_weight, int, 0444);
+MODULE_PARM_DESC(rx_napi_weight, "The NAPI WEIGHT parameter.");
+
+/**
+ * cvm_oct_poll_queue - Workqueue for polling operations.
+ */
+struct workqueue_struct *cvm_oct_poll_queue;
+
+/**
+ * cvm_oct_poll_queue_stopping - flag to indicate polling should stop.
+ *
+ * Set to one right before cvm_oct_poll_queue is destroyed.
+ */
+atomic_t cvm_oct_poll_queue_stopping = ATOMIC_INIT(0);
+
+/* cvm_oct_by_port is an array of every ethernet device owned by this
+ * driver indexed by the ipd input port number.
+ */
+struct octeon_ethernet *cvm_oct_by_port[TOTAL_NUMBER_OF_PORTS] __cacheline_aligned;
+
+/* cvm_oct_list is a list of all cvm_oct_private_t created by this driver. */
+LIST_HEAD(cvm_oct_list);
+
+static void cvm_oct_rx_refill_worker(struct work_struct *work);
+static DECLARE_DELAYED_WORK(cvm_oct_rx_refill_work, cvm_oct_rx_refill_worker);
+
+static void cvm_oct_rx_refill_worker(struct work_struct *work)
+{
+	/* FPA 0 may have been drained, try to refill it if we need
+	 * more than num_packet_buffers / 2, otherwise normal receive
+	 * processing will refill it.  If it were drained, no packets
+	 * could be received so cvm_oct_napi_poll would never be
+	 * invoked to do the refill.
+	 */
+	cvm_oct_rx_refill_pool(num_packet_buffers / 2);
+
+	if (!atomic_read(&cvm_oct_poll_queue_stopping))
+		queue_delayed_work(cvm_oct_poll_queue,
+				   &cvm_oct_rx_refill_work, HZ);
+}
+
+static void cvm_oct_periodic_worker(struct work_struct *work)
+{
+	struct octeon_ethernet *priv = container_of(work,
+						    struct octeon_ethernet,
+						    port_periodic_work.work);
+	void (*poll_fn) (struct net_device *);
+
+	spin_lock(&priv->poll_lock);
+	poll_fn = priv->poll;
+	spin_unlock(&priv->poll_lock);
+
+	if (poll_fn)
+		poll_fn(priv->netdev);
+
+	priv->netdev->netdev_ops->ndo_get_stats(priv->netdev);
+
+	if (!atomic_read(&cvm_oct_poll_queue_stopping))
+		queue_delayed_work(cvm_oct_poll_queue, &priv->port_periodic_work, HZ);
+}
+
+static int cvm_oct_num_output_buffers;
+
+static __devinit int cvm_oct_configure_common_hw(void)
+{
+	/* Setup the FPA */
+	cvmx_fpa_enable();
+
+	if (cvm_oct_alloc_fpa_pool(CVMX_FPA_PACKET_POOL,
+				   CVMX_FPA_PACKET_POOL_SIZE) < 0) {
+		pr_err("cvm_oct_alloc_fpa_pool(CVMX_FPA_PACKET_POOL, CVMX_FPA_PACKET_POOL_SIZE) failed.\n");
+		return -ENOMEM;
+	}
+	cvm_oct_mem_fill_fpa(CVMX_FPA_PACKET_POOL, num_packet_buffers);
+
+	if (cvm_oct_alloc_fpa_pool(CVMX_FPA_WQE_POOL,
+				   CVMX_FPA_WQE_POOL_SIZE) < 0) {
+		pr_err("cvm_oct_alloc_fpa_pool(CVMX_FPA_WQE_POOL, CVMX_FPA_WQE_POOL_SIZE) failed.\n");
+		return -ENOMEM;;
+	}
+	cvm_oct_mem_fill_fpa(CVMX_FPA_WQE_POOL, num_packet_buffers);
+
+	if (CVMX_FPA_OUTPUT_BUFFER_POOL != CVMX_FPA_PACKET_POOL) {
+		cvm_oct_num_output_buffers = 128;
+		if (cvm_oct_alloc_fpa_pool(CVMX_FPA_OUTPUT_BUFFER_POOL,
+					   CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE) < 0) {
+			pr_err("cvm_oct_alloc_fpa_pool(CVMX_FPA_OUTPUT_BUFFER_POOL, CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE) failed.\n");
+			return -ENOMEM;;
+		}
+		cvm_oct_mem_fill_fpa(CVMX_FPA_OUTPUT_BUFFER_POOL,
+				     cvm_oct_num_output_buffers);
+	}
+
+	if (USE_RED)
+		cvmx_helper_setup_red(num_packet_buffers / 4,
+				      num_packet_buffers / 8);
+
+	return 0;
+}
+
+/**
+ * cvm_oct_free_work- Free a work queue entry
+ *
+ * @work_queue_entry: Work queue entry to free
+ *
+ * Returns Zero on success, Negative on failure.
+ */
+int cvm_oct_free_work(void *work_queue_entry)
+{
+	cvmx_wqe_t *work = work_queue_entry;
+
+	int segments = work->word2.s.bufs;
+	union cvmx_buf_ptr segment_ptr = work->packet_ptr;
+
+	while (segments--) {
+		union cvmx_buf_ptr next_ptr = *(union cvmx_buf_ptr *)phys_to_virt(segment_ptr.s.addr - 8);
+		if (!segment_ptr.s.i)
+			cvmx_fpa_free(cvm_oct_get_buffer_ptr(segment_ptr),
+				      segment_ptr.s.pool,
+				      DONT_WRITEBACK(CVMX_FPA_PACKET_POOL_SIZE / 128));
+		segment_ptr = next_ptr;
+	}
+	cvmx_fpa_free(work, CVMX_FPA_WQE_POOL, DONT_WRITEBACK(1));
+
+	return 0;
+}
+EXPORT_SYMBOL(cvm_oct_free_work);
+
+/* Lock to protect racy cvmx_pko_get_port_status() */
+static DEFINE_SPINLOCK(cvm_oct_tx_stat_lock);
+
+/**
+ * cvm_oct_common_get_stats - get the low level ethernet statistics
+ * @dev:    Device to get the statistics from
+ *
+ * Returns Pointer to the statistics
+ */
+static struct net_device_stats *cvm_oct_common_get_stats(struct net_device *dev)
+{
+	unsigned long flags;
+	cvmx_pip_port_status_t rx_status;
+	cvmx_pko_port_status_t tx_status;
+	u64 current_tx_octets;
+	u32 current_tx_packets;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	if (octeon_is_simulation()) {
+		/* The simulator doesn't support statistics */
+		memset(&rx_status, 0, sizeof(rx_status));
+		memset(&tx_status, 0, sizeof(tx_status));
+	} else {
+		cvmx_pip_get_port_status(priv->ipd_port, 1, &rx_status);
+
+		spin_lock_irqsave(&cvm_oct_tx_stat_lock, flags);
+		cvmx_pko_get_port_status(priv->ipd_port, 0, &tx_status);
+		current_tx_packets = tx_status.packets;
+		current_tx_octets = tx_status.octets;
+		/* The tx_packets counter is 32-bits as are all these
+		 * variables.  No truncation necessary.
+		 */
+		tx_status.packets = current_tx_packets - priv->last_tx_packets;
+		/* The tx_octets counter is only 48-bits, so we need
+		 * to truncate in case there was a wrap-around
+		 */
+		tx_status.octets = (current_tx_octets - priv->last_tx_octets) & 0xffffffffffffull;
+		priv->last_tx_packets = current_tx_packets;
+		priv->last_tx_octets = current_tx_octets;
+		spin_unlock_irqrestore(&cvm_oct_tx_stat_lock, flags);
+	}
+
+	dev->stats.rx_packets += rx_status.inb_packets;
+	dev->stats.tx_packets += tx_status.packets;
+	dev->stats.rx_bytes += rx_status.inb_octets;
+	dev->stats.tx_bytes += tx_status.octets;
+	dev->stats.multicast += rx_status.multicast_packets;
+	dev->stats.rx_crc_errors += rx_status.inb_errors;
+	dev->stats.rx_frame_errors += rx_status.fcs_align_err_packets;
+
+	/* The drop counter must be incremented atomically since the
+	 * RX tasklet also increments it.
+	 */
+	atomic64_add(rx_status.dropped_packets,
+		     (atomic64_t *)&dev->stats.rx_dropped);
+
+	return &dev->stats;
+}
+
+/**
+ * cvm_oct_common_change_mtu - change the link MTU
+ * @dev:     Device to change
+ * @new_mtu: The new MTU
+ *
+ * Returns Zero on success
+ */
+static int cvm_oct_common_change_mtu(struct net_device *dev, int new_mtu)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+#if IS_ENABLED(CONFIG_VLAN_8021Q)
+	int vlan_bytes = 4;
+#else
+	int vlan_bytes = 0;
+#endif
+
+	/* Limit the MTU to make sure the ethernet packets are between
+	 * 64 bytes and 65535 bytes.
+	 */
+	if ((new_mtu + 14 + 4 + vlan_bytes < 64)
+	    || (new_mtu + 14 + 4 + vlan_bytes > 65392)) {
+		netdev_err(dev, "MTU must be between %d and %d.\n",
+			   64 - 14 - 4 - vlan_bytes,
+			   65392 - 14 - 4 - vlan_bytes);
+		return -EINVAL;
+	}
+	dev->mtu = new_mtu;
+
+	if ((priv->interface < 2)
+	    && (cvmx_helper_interface_get_mode(priv->interface) !=
+		CVMX_HELPER_INTERFACE_MODE_SPI)) {
+		/* Add ethernet header and FCS, and VLAN if configured. */
+		int max_packet = new_mtu + 14 + 4 + vlan_bytes;
+
+		if (OCTEON_IS_MODEL(OCTEON_CN3XXX)
+		    || OCTEON_IS_MODEL(OCTEON_CN58XX)) {
+			/* Signal errors on packets larger than the MTU */
+			cvmx_write_csr(CVMX_GMXX_RXX_FRM_MAX(priv->interface_port, priv->interface),
+				       max_packet);
+		} else {
+			union cvmx_pip_prt_cfgx port_cfg;
+			int offset = cvmx_helper_get_ipd_port(priv->interface, priv->interface_port);
+
+			port_cfg.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(offset));
+			if (port_cfg.s.maxerr_en) {
+				/* Disable the PIP check as it can
+				 * only be controlled over a group of
+				 * ports, let the check be done in the
+				 * GMX instead.
+				 */
+				port_cfg.s.maxerr_en = 0;
+				cvmx_write_csr(CVMX_PIP_PRT_CFGX(offset), port_cfg.u64);
+			}
+		}
+		/* Set the hardware to truncate packets larger than
+		 * the MTU. The jabber register must be set to a
+		 * multiple of 8 bytes, so round up.
+		 */
+		cvmx_write_csr(CVMX_GMXX_RXX_JABBER(priv->interface_port, priv->interface),
+			       (max_packet + 7) & ~7u);
+	}
+	return 0;
+}
+
+/**
+ * cvm_oct_common_set_multicast_list - set the multicast list
+ * @dev:    Device to work on
+ */
+static void cvm_oct_common_set_multicast_list(struct net_device *dev)
+{
+	union cvmx_gmxx_prtx_cfg gmx_cfg;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	if ((priv->interface < 2)
+	    && (cvmx_helper_interface_get_mode(priv->interface) !=
+		CVMX_HELPER_INTERFACE_MODE_SPI)) {
+		union cvmx_gmxx_rxx_adr_ctl control;
+		control.u64 = 0;
+		control.s.bcst = 1;	/* Allow broadcast MAC addresses */
+
+		if (!netdev_mc_empty(dev) || (dev->flags & IFF_ALLMULTI) ||
+		    (dev->flags & IFF_PROMISC))
+			/* Force accept multicast packets */
+			control.s.mcst = 2;
+		else
+			/* Force reject multicat packets */
+			control.s.mcst = 1;
+
+		if (dev->flags & IFF_PROMISC)
+			/* Reject matches if promisc. Since CAM is
+			 * shut off, should accept everything.
+			 */
+			control.s.cam_mode = 0;
+		else
+			/* Filter packets based on the CAM */
+			control.s.cam_mode = 1;
+
+		gmx_cfg.u64 =
+		    cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
+		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface),
+			       gmx_cfg.u64 & ~1ull);
+
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CTL(priv->interface_port, priv->interface),
+			       control.u64);
+		if (dev->flags & IFF_PROMISC)
+			cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM_EN(priv->interface_port, priv->interface), 0);
+		else
+			cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM_EN(priv->interface_port, priv->interface), 1);
+
+		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface),
+			       gmx_cfg.u64);
+	}
+}
+
+/**
+ * cvm_oct_common_set_mac_address - set the hardware MAC address for a device
+ * @dev:    The device in question.
+ * @addr:   Address structure to change it too.
+ *
+ * Returns Zero on success
+ */
+static int cvm_oct_set_mac_filter(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	union cvmx_gmxx_prtx_cfg gmx_cfg;
+
+	if ((priv->interface < 2)
+	    && (cvmx_helper_interface_get_mode(priv->interface) !=
+		CVMX_HELPER_INTERFACE_MODE_SPI)) {
+		int i;
+		u8 *ptr = dev->dev_addr;
+		u64 mac = 0;
+		for (i = 0; i < 6; i++)
+			mac = (mac << 8) | (u64)ptr[i];
+
+		gmx_cfg.u64 =
+		    cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
+		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface),
+			       gmx_cfg.u64 & ~1ull);
+
+		cvmx_write_csr(CVMX_GMXX_SMACX(priv->interface_port, priv->interface), mac);
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM0(priv->interface_port, priv->interface),
+			       ptr[0]);
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM1(priv->interface_port, priv->interface),
+			       ptr[1]);
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM2(priv->interface_port, priv->interface),
+			       ptr[2]);
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM3(priv->interface_port, priv->interface),
+			       ptr[3]);
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM4(priv->interface_port, priv->interface),
+			       ptr[4]);
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM5(priv->interface_port, priv->interface),
+			       ptr[5]);
+		cvm_oct_common_set_multicast_list(dev);
+		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface),
+			       gmx_cfg.u64);
+	}
+	return 0;
+}
+
+static int cvm_oct_common_set_mac_address(struct net_device *dev, void *addr)
+{
+	int r = eth_mac_addr(dev, addr);
+
+	if (r)
+		return r;
+	return cvm_oct_set_mac_filter(dev);
+}
+
+/**
+ * cvm_oct_common_init - per network device initialization
+ * @dev:    Device to initialize
+ *
+ * Returns Zero on success
+ */
+int cvm_oct_common_init(struct net_device *dev)
+{
+	unsigned long flags;
+	cvmx_pko_port_status_t tx_status;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	const u8 *mac = NULL;
+
+	if (priv->of_node)
+		mac = of_get_mac_address(priv->of_node);
+
+	if (mac && is_valid_ether_addr(mac)) {
+		memcpy(dev->dev_addr, mac, ETH_ALEN);
+		dev->addr_assign_type &= ~NET_ADDR_RANDOM;
+	} else {
+		eth_hw_addr_random(dev);
+	}
+
+	if (priv->num_tx_queues != -1) {
+		dev->features |= NETIF_F_SG | NETIF_F_FRAGLIST;
+		if (USE_HW_TCPUDP_CHECKSUM)
+			dev->features |= NETIF_F_IP_CSUM;
+	}
+
+	/* We do our own locking, Linux doesn't need to */
+	dev->features |= NETIF_F_LLTX;
+	SET_ETHTOOL_OPS(dev, &cvm_oct_ethtool_ops);
+
+	cvm_oct_set_mac_filter(dev);
+	dev->netdev_ops->ndo_change_mtu(dev, dev->mtu);
+
+	spin_lock_irqsave(&cvm_oct_tx_stat_lock, flags);
+	cvmx_pko_get_port_status(priv->ipd_port, 0, &tx_status);
+	priv->last_tx_packets = tx_status.packets;
+	priv->last_tx_octets = tx_status.octets;
+	/* Zero out stats for port so we won't mistakenly show
+	 * counters from the bootloader.
+	 */
+	memset(&dev->stats, 0, sizeof(struct net_device_stats));
+	spin_unlock_irqrestore(&cvm_oct_tx_stat_lock, flags);
+
+	return 0;
+}
+
+static const struct net_device_ops cvm_oct_npi_netdev_ops = {
+	.ndo_init		= cvm_oct_common_init,
+	.ndo_start_xmit		= cvm_oct_xmit,
+	.ndo_set_rx_mode	= cvm_oct_common_set_multicast_list,
+	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_common_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+
+/* SGMII and XAUI handled the same so they both use this. */
+static const struct net_device_ops cvm_oct_sgmii_netdev_ops = {
+	.ndo_init		= cvm_oct_sgmii_init,
+	.ndo_open		= cvm_oct_sgmii_open,
+	.ndo_stop		= cvm_oct_sgmii_stop,
+	.ndo_start_xmit		= cvm_oct_xmit,
+	.ndo_set_rx_mode	= cvm_oct_common_set_multicast_list,
+	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_common_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+static const struct net_device_ops cvm_oct_spi_netdev_ops = {
+	.ndo_init		= cvm_oct_spi_init,
+	.ndo_uninit		= cvm_oct_spi_uninit,
+	.ndo_start_xmit		= cvm_oct_xmit,
+	.ndo_set_rx_mode	= cvm_oct_common_set_multicast_list,
+	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_common_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+static const struct net_device_ops cvm_oct_rgmii_netdev_ops = {
+	.ndo_init		= cvm_oct_rgmii_init,
+	.ndo_open		= cvm_oct_rgmii_open,
+	.ndo_stop		= cvm_oct_rgmii_stop,
+	.ndo_start_xmit		= cvm_oct_xmit,
+	.ndo_set_rx_mode	= cvm_oct_common_set_multicast_list,
+	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_common_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+
+extern void octeon_mdiobus_force_mod_depencency(void);
+
+static int num_devices_extra_wqe;
+#define PER_DEVICE_EXTRA_WQE (MAX_OUT_QUEUE_DEPTH)
+
+static struct device_node * __devinit cvm_oct_of_get_child(const struct device_node *parent,
+							   int reg_val)
+{
+	struct device_node *node = NULL;
+	int size;
+	const __be32 *addr;
+
+	for (;;) {
+		node = of_get_next_child(parent, node);
+		if (!node)
+			break;
+		addr = of_get_property(node, "reg", &size);
+		if (addr && (be32_to_cpu(*addr) == reg_val))
+			break;
+	}
+	return node;
+}
+
+static struct device_node * __devinit cvm_oct_node_for_port(struct device_node *pip,
+							    int interface, int port)
+{
+	struct device_node *ni, *np;
+
+	ni = cvm_oct_of_get_child(pip, interface);
+	if (!ni)
+		return NULL;
+
+	np = cvm_oct_of_get_child(ni, port);
+	of_node_put(ni);
+
+	return np;
+}
+
+static int __devinit cvm_oct_probe(struct platform_device *pdev)
+{
+	int num_interfaces;
+	int interface;
+	int fau = FAU_NUM_PACKET_BUFFERS_TO_FREE;
+	int qos, r;
+	struct device_node *pip;
+
+	octeon_mdiobus_force_mod_depencency();
+	pr_notice("octeon-ethernet %s\n", OCTEON_ETHERNET_VERSION);
+
+	pip = pdev->dev.of_node;
+	if (!pip) {
+		dev_err(&pdev->dev, "No of_node.\n");
+		return -EINVAL;
+	}
+
+	cvm_oct_poll_queue = create_singlethread_workqueue("octeon-ethernet");
+	if (cvm_oct_poll_queue == NULL) {
+		dev_err(&pdev->dev, "Cannot create workqueue");
+		return -ENOMEM;
+	}
+
+	r = cvm_oct_configure_common_hw();
+	if (r)
+		return r;
+
+	cvmx_helper_initialize_packet_io_global();
+
+	/* Change the input group for all ports before input is enabled */
+	num_interfaces = cvmx_helper_get_number_of_interfaces();
+	for (interface = 0; interface < num_interfaces; interface++) {
+		int num_ports = cvmx_helper_ports_on_interface(interface);
+		int port;
+
+		for (port = cvmx_helper_get_ipd_port(interface, 0);
+		     port < cvmx_helper_get_ipd_port(interface, num_ports);
+		     port++) {
+			union cvmx_pip_prt_tagx pip_prt_tagx;
+			pip_prt_tagx.u64 = cvmx_read_csr(CVMX_PIP_PRT_TAGX(port));
+			pip_prt_tagx.s.grp = pow_receive_group;
+			cvmx_write_csr(CVMX_PIP_PRT_TAGX(port), pip_prt_tagx.u64);
+		}
+	}
+
+	cvmx_helper_ipd_and_packet_input_enable();
+
+	/* Initialize the FAU used for counting packet buffers that
+	 * need to be freed.
+	 */
+	cvmx_fau_atomic_write32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);
+
+	num_interfaces = cvmx_helper_get_number_of_interfaces();
+	for (interface = 0; interface < num_interfaces; interface++) {
+		cvmx_helper_interface_mode_t imode = cvmx_helper_interface_get_mode(interface);
+		int num_ports = cvmx_helper_ports_on_interface(interface);
+		int interface_port;
+
+		for (interface_port = 0; interface_port < num_ports;
+		     interface_port++) {
+			struct octeon_ethernet *priv;
+			int base_queue;
+			struct net_device *dev =
+			    alloc_etherdev(sizeof(struct octeon_ethernet));
+			if (!dev) {
+				dev_err(&pdev->dev,
+					"Failed to allocate ethernet device for port %d:%d\n",
+					interface, interface_port);
+				continue;
+			}
+
+			/* Initialize the device private structure. */
+			priv = netdev_priv(dev);
+			priv->of_node = cvm_oct_node_for_port(pip, interface, interface_port);
+			priv->netdev = dev;
+			priv->interface = interface;
+			priv->interface_port = interface_port;
+			spin_lock_init(&priv->poll_lock);
+			INIT_DELAYED_WORK(&priv->port_periodic_work,
+					  cvm_oct_periodic_worker);
+			priv->imode = imode;
+			priv->ipd_port = cvmx_helper_get_ipd_port(interface, interface_port);
+			priv->pko_port = cvmx_helper_get_pko_port(interface, interface_port);
+			base_queue = cvmx_pko_get_base_queue(priv->ipd_port);
+			priv->num_tx_queues = cvmx_pko_get_num_queues(priv->ipd_port);
+
+			BUG_ON(priv->num_tx_queues < 1);
+			BUG_ON(priv->num_tx_queues > 32);
+
+			for (qos = 0; qos < priv->num_tx_queues; qos++) {
+				priv->tx_queue[qos].queue = base_queue + qos;
+				fau = fau - sizeof(u32);
+				priv->tx_queue[qos].fau = fau;
+				cvmx_fau_atomic_write32(priv->tx_queue[qos].fau, 0);
+			}
+
+			/* Cache the fact that there may be multiple queues */
+			priv->tx_multiple_queues =
+				(CVMX_PKO_QUEUES_PER_PORT_INTERFACE0 > 1) ||
+				(CVMX_PKO_QUEUES_PER_PORT_INTERFACE1 > 1);
+
+			switch (priv->imode) {
+
+			/* These types don't support ports to IPD/PKO */
+			case CVMX_HELPER_INTERFACE_MODE_DISABLED:
+			case CVMX_HELPER_INTERFACE_MODE_PCIE:
+			case CVMX_HELPER_INTERFACE_MODE_PICMG:
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_NPI:
+				dev->netdev_ops = &cvm_oct_npi_netdev_ops;
+				strcpy(dev->name, "npi%d");
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_XAUI:
+				dev->netdev_ops = &cvm_oct_sgmii_netdev_ops;
+				strcpy(dev->name, "xaui%d");
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_LOOP:
+				dev->netdev_ops = &cvm_oct_npi_netdev_ops;
+				strcpy(dev->name, "loop%d");
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_SGMII:
+				dev->netdev_ops = &cvm_oct_sgmii_netdev_ops;
+				strcpy(dev->name, "eth%d");
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_SPI:
+				dev->netdev_ops = &cvm_oct_spi_netdev_ops;
+				strcpy(dev->name, "spi%d");
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_RGMII:
+			case CVMX_HELPER_INTERFACE_MODE_GMII:
+				dev->netdev_ops = &cvm_oct_rgmii_netdev_ops;
+				strcpy(dev->name, "eth%d");
+				break;
+			}
+
+			netif_carrier_off(dev);
+			if (!dev->netdev_ops) {
+				free_netdev(dev);
+			} else if (register_netdev(dev) < 0) {
+				dev_err(&pdev->dev,
+					"Failed to register ethernet device for interface %d, port %d\n",
+					interface, priv->ipd_port);
+				free_netdev(dev);
+			} else {
+				list_add_tail(&priv->list, &cvm_oct_list);
+				cvm_oct_by_port[priv->ipd_port] = priv;
+				/* Each transmit queue will need its
+				 * own MAX_OUT_QUEUE_DEPTH worth of
+				 * WQE to track the transmit skbs.
+				 */
+				cvm_oct_mem_fill_fpa(CVMX_FPA_WQE_POOL,
+						     PER_DEVICE_EXTRA_WQE);
+				num_devices_extra_wqe++;
+				queue_delayed_work(cvm_oct_poll_queue,
+						   &priv->port_periodic_work, HZ);
+			}
+		}
+	}
+
+	cvm_oct_tx_initialize();
+	cvm_oct_rx_initialize();
+
+	queue_delayed_work(cvm_oct_poll_queue, &cvm_oct_rx_refill_work, HZ);
+
+	return 0;
+}
+
+static int __devexit cvm_oct_remove(struct platform_device *pdev)
+{
+	struct octeon_ethernet *priv;
+	struct octeon_ethernet *tmp;
+
+	/* Disable POW interrupt */
+	cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group), 0);
+
+	cvmx_ipd_disable();
+
+	atomic_inc_return(&cvm_oct_poll_queue_stopping);
+	cancel_delayed_work_sync(&cvm_oct_rx_refill_work);
+
+	cvm_oct_rx_shutdown();
+	cvm_oct_tx_shutdown();
+
+	cvmx_pko_disable();
+
+	/* Free the ethernet devices */
+	list_for_each_entry_safe_reverse(priv, tmp, &cvm_oct_list, list) {
+		list_del(&priv->list);
+		cancel_delayed_work_sync(&priv->port_periodic_work);
+		cvm_oct_tx_shutdown_dev(priv->netdev);
+		unregister_netdev(priv->netdev);
+		cvm_oct_by_port[priv->ipd_port] = NULL;
+		free_netdev(priv->netdev);
+	}
+
+	cvmx_helper_shutdown_packet_io_global();
+
+	destroy_workqueue(cvm_oct_poll_queue);
+
+	/* Free the HW pools */
+	cvm_oct_mem_empty_fpa(CVMX_FPA_PACKET_POOL, num_packet_buffers);
+	cvm_oct_release_fpa_pool(CVMX_FPA_PACKET_POOL);
+
+	cvm_oct_mem_empty_fpa(CVMX_FPA_WQE_POOL,
+			      num_packet_buffers + num_devices_extra_wqe * PER_DEVICE_EXTRA_WQE);
+	cvm_oct_release_fpa_pool(CVMX_FPA_WQE_POOL);
+
+	if (CVMX_FPA_OUTPUT_BUFFER_POOL != CVMX_FPA_PACKET_POOL) {
+		cvm_oct_mem_empty_fpa(CVMX_FPA_OUTPUT_BUFFER_POOL,
+				      cvm_oct_num_output_buffers);
+		cvm_oct_release_fpa_pool(CVMX_FPA_OUTPUT_BUFFER_POOL);
+	}
+	cvm_oct_mem_cleanup();
+
+	return 0;
+}
+
+static struct of_device_id cvm_oct_match[] = {
+	{
+		.compatible = "cavium,octeon-3860-pip",
+	},
+	{},
+};
+MODULE_DEVICE_TABLE(of, cvm_oct_match);
+
+static struct platform_driver cvm_oct_driver = {
+	.probe		= cvm_oct_probe,
+	.remove		= __devexit_p(cvm_oct_remove),
+	.driver		= {
+		.owner	= THIS_MODULE,
+		.name	= KBUILD_MODNAME,
+		.of_match_table = cvm_oct_match,
+	},
+};
+
+module_platform_driver(cvm_oct_driver);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Cavium Networks <support@caviumnetworks.com>");
+MODULE_DESCRIPTION("Cavium Networks Octeon ethernet driver.");
diff --git a/drivers/net/ethernet/octeon/octeon-ethernet.h b/drivers/net/ethernet/octeon/octeon-ethernet.h
new file mode 100644
index 0000000..8358e91
--- /dev/null
+++ b/drivers/net/ethernet/octeon/octeon-ethernet.h
@@ -0,0 +1,189 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+
+/*
+ * External interface for the Cavium Octeon ethernet driver.
+ */
+#ifndef OCTEON_ETHERNET_H
+#define OCTEON_ETHERNET_H
+
+#include <linux/of.h>
+
+#include <asm/octeon/cvmx-helper.h>
+#include <asm/octeon/cvmx-fau.h>
+
+/**
+ * This is the definition of the Ethernet driver's private
+ * driver state stored in netdev_priv(dev).
+ */
+struct octeon_ethernet {
+	int ipd_port;
+	int pko_port;
+	int interface;
+	int interface_port;
+
+	/* My netdev. */
+	struct net_device *netdev;
+	/* My location in the cvm_oct_list */
+	struct list_head list;
+
+	/* Type of port. This is one of the enums in
+	 * cvmx_helper_interface_mode_t
+	 */
+	int imode;
+
+	unsigned int tx_timestamp_hw:1;
+	unsigned int rx_timestamp_hw:1;
+	unsigned int tx_multiple_queues:1;
+
+	/* Number of elements in tx_queue below */
+	int                     num_tx_queues;
+
+	struct {
+		/* PKO hardware queue for the port */
+		int	queue;
+		/* Hardware fetch and add to count outstanding tx buffers */
+		int	fau;
+	} tx_queue[32];
+
+	struct phy_device *phydev;
+	unsigned int last_link;
+	/* Last negotiated link state */
+	u64 link_info;
+	/* Called periodically to check link status */
+	spinlock_t poll_lock;
+	void (*poll) (struct net_device *dev);
+	struct delayed_work	port_periodic_work;
+	struct work_struct	port_work;	/* may be unused. */
+	struct device_node	*of_node;
+	u64 last_tx_octets;
+	u32 last_tx_packets;
+};
+
+int cvm_oct_free_work(void *work_queue_entry);
+
+int cvm_oct_rgmii_init(struct net_device *dev);
+int cvm_oct_rgmii_open(struct net_device *dev);
+int cvm_oct_rgmii_stop(struct net_device *dev);
+
+int cvm_oct_sgmii_init(struct net_device *dev);
+int cvm_oct_sgmii_open(struct net_device *dev);
+int cvm_oct_sgmii_stop(struct net_device *dev);
+
+int cvm_oct_spi_init(struct net_device *dev);
+void cvm_oct_spi_uninit(struct net_device *dev);
+
+int cvm_oct_xaui_init(struct net_device *dev);
+int cvm_oct_xaui_open(struct net_device *dev);
+int cvm_oct_xaui_stop(struct net_device *dev);
+
+int cvm_oct_common_init(struct net_device *dev);
+
+void cvm_oct_set_carrier(struct octeon_ethernet *priv,
+				cvmx_helper_link_info_t link_info);
+void cvm_oct_adjust_link(struct net_device *dev);
+
+int cvm_oct_ioctl(struct net_device *dev, struct ifreq *rq, int cmd);
+int cvm_oct_phy_setup_device(struct net_device *dev);
+
+int cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev);
+int cvm_oct_transmit_qos(struct net_device *dev, void *work_queue_entry,
+			 int do_free, int qos);
+void cvm_oct_tx_initialize(void);
+void cvm_oct_tx_shutdown(void);
+void cvm_oct_tx_shutdown_dev(struct net_device *dev);
+
+void cvm_oct_poll_controller(struct net_device *dev);
+void cvm_oct_rx_initialize(void);
+void cvm_oct_rx_shutdown(void);
+
+int cvm_oct_mem_fill_fpa(int pool, int elements);
+int cvm_oct_mem_empty_fpa(int pool, int elements);
+int cvm_oct_alloc_fpa_pool(int pool, int size);
+int cvm_oct_release_fpa_pool(int pool);
+void cvm_oct_mem_cleanup(void);
+
+extern const struct ethtool_ops cvm_oct_ethtool_ops;
+
+extern int always_use_pow;
+extern int pow_send_group;
+extern int pow_receive_group;
+extern char pow_send_list[];
+extern struct list_head cvm_oct_list;
+extern struct octeon_ethernet *cvm_oct_by_port[];
+
+extern struct workqueue_struct *cvm_oct_poll_queue;
+extern atomic_t cvm_oct_poll_queue_stopping;
+
+extern int max_rx_cpus;
+extern int rx_napi_weight;
+
+static inline void cvm_oct_rx_refill_pool(int fill_threshold)
+{
+	int number_to_free;
+	int num_freed;
+	/* Refill the packet buffer pool */
+	number_to_free =
+		cvmx_fau_fetch_and_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);
+
+	if (number_to_free > fill_threshold) {
+		cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE,
+				      -number_to_free);
+		num_freed = cvm_oct_mem_fill_fpa(CVMX_FPA_PACKET_POOL,
+						 number_to_free);
+		if (num_freed != number_to_free) {
+			cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE,
+					number_to_free - num_freed);
+		}
+	}
+}
+
+/**
+ * cvm_oct_get_buffer_ptr - convert packet data address to pointer
+ * @pd: Packet data hardware address
+ *
+ * Returns Packet buffer pointer
+ */
+static inline void *cvm_oct_get_buffer_ptr(union cvmx_buf_ptr pd)
+{
+	return phys_to_virt(((pd.s.addr >> 7) - pd.s.back) << 7);
+}
+
+static inline struct sk_buff **cvm_oct_packet_to_skb(void *packet)
+{
+	char *p = packet;
+	return (struct sk_buff **)(p - sizeof(void *));
+}
+
+#define CVM_OCT_SKB_TO_FPA_PADDING (128 + sizeof(void *) - 1)
+
+static inline u8 *cvm_oct_get_fpa_head(struct sk_buff *skb)
+{
+	return (u8 *)((unsigned long)(skb->head + CVM_OCT_SKB_TO_FPA_PADDING) & ~0x7ful);
+}
+
+#endif
diff --git a/drivers/staging/Kconfig b/drivers/staging/Kconfig
index 257b86f..3541336 100644
--- a/drivers/staging/Kconfig
+++ b/drivers/staging/Kconfig
@@ -66,8 +66,6 @@ source "drivers/staging/line6/Kconfig"
 
 source "drivers/staging/lttng/Kconfig"
 
-source "drivers/staging/octeon/Kconfig"
-
 source "drivers/staging/serqt_usb2/Kconfig"
 
 source "drivers/staging/quatech_usb2/Kconfig"
diff --git a/drivers/staging/Makefile b/drivers/staging/Makefile
index fecd665..107f5177 100644
--- a/drivers/staging/Makefile
+++ b/drivers/staging/Makefile
@@ -27,7 +27,6 @@ obj-$(CONFIG_LINE6_USB)		+= line6/
 obj-$(CONFIG_LTTNG)		+= lttng/
 obj-$(CONFIG_USB_SERIAL_QUATECH2)	+= serqt_usb2/
 obj-$(CONFIG_USB_SERIAL_QUATECH_USB2)	+= quatech_usb2/
-obj-$(CONFIG_OCTEON_ETHERNET)	+= octeon/
 obj-$(CONFIG_VT6655)		+= vt6655/
 obj-$(CONFIG_VT6656)		+= vt6656/
 obj-$(CONFIG_VME_BUS)		+= vme/
diff --git a/drivers/staging/octeon/Kconfig b/drivers/staging/octeon/Kconfig
deleted file mode 100644
index 9493128..0000000
--- a/drivers/staging/octeon/Kconfig
+++ /dev/null
@@ -1,13 +0,0 @@
-config OCTEON_ETHERNET
-	tristate "Cavium Networks Octeon Ethernet support"
-	depends on CPU_CAVIUM_OCTEON && NETDEVICES
-	select PHYLIB
-	select MDIO_OCTEON
-	help
-	  This driver supports the builtin ethernet ports on Cavium
-	  Networks' products in the Octeon family. This driver supports the
-	  CN3XXX and CN5XXX Octeon processors.
-
-	  To compile this driver as a module, choose M here.  The module
-	  will be called octeon-ethernet.
-
diff --git a/drivers/staging/octeon/Makefile b/drivers/staging/octeon/Makefile
deleted file mode 100644
index 23f37e5..0000000
--- a/drivers/staging/octeon/Makefile
+++ /dev/null
@@ -1,22 +0,0 @@
-# This file is subject to the terms and conditions of the GNU General Public
-# License.  See the file "COPYING" in the main directory of this archive
-# for more details.
-#
-# Copyright (C) 2005-2009 Cavium Networks
-#
-
-#
-# Makefile for Cavium OCTEON on-board ethernet driver
-#
-
-obj-${CONFIG_OCTEON_ETHERNET} :=  octeon-ethernet.o
-
-octeon-ethernet-y := ethernet.o
-octeon-ethernet-y += ethernet-mdio.o
-octeon-ethernet-y += ethernet-mem.o
-octeon-ethernet-y += ethernet-rgmii.o
-octeon-ethernet-y += ethernet-rx.o
-octeon-ethernet-y += ethernet-sgmii.o
-octeon-ethernet-y += ethernet-spi.o
-octeon-ethernet-y += ethernet-tx.o
-
diff --git a/drivers/staging/octeon/ethernet-defines.h b/drivers/staging/octeon/ethernet-defines.h
deleted file mode 100644
index 96109a1..0000000
--- a/drivers/staging/octeon/ethernet-defines.h
+++ /dev/null
@@ -1,105 +0,0 @@
-/**********************************************************************
- * Author: Cavium, Inc.
- *
- * Contact: support@cavium.com
- * This file is part of the OCTEON SDK
- *
- * Copyright (c) 2003-2012 Cavium, Inc.
- *
- * This file is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, Version 2, as
- * published by the Free Software Foundation.
- *
- * This file is distributed in the hope that it will be useful, but
- * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
- * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
- * NONINFRINGEMENT.  See the GNU General Public License for more
- * details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this file; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- * or visit http://www.gnu.org/licenses/.
- *
- * This file may also be available under a different license from Cavium.
- * Contact Cavium, Inc. for more information
- **********************************************************************/
-
-/*
- * A few defines are used to control the operation of this driver:
- *  CONFIG_CAVIUM_RESERVE32
- *      This kernel config options controls the amount of memory configured
- *      in a wired TLB entry for all processes to share. If this is set, the
- *      driver will use this memory instead of kernel memory for pools. This
- *      allows 32bit userspace application to access the buffers, but also
- *      requires all received packets to be copied.
- *  CONFIG_CAVIUM_OCTEON_NUM_PACKET_BUFFERS
- *      This kernel config option allows the user to control the number of
- *      packet and work queue buffers allocated by the driver. If this is zero,
- *      the driver uses the default from below.
- *  USE_SKBUFFS_IN_HW
- *      Tells the driver to populate the packet buffers with kernel skbuffs.
- *      This allows the driver to receive packets without copying them. It also
- *      means that 32bit userspace can't access the packet buffers.
- *  USE_HW_TCPUDP_CHECKSUM
- *      Controls if the Octeon TCP/UDP checksum engine is used for packet
- *      output. If this is zero, the kernel will perform the checksum in
- *      software.
- *  USE_ASYNC_IOBDMA
- *      Use asynchronous IO access to hardware. This uses Octeon's asynchronous
- *      IOBDMAs to issue IO accesses without stalling. Set this to zero
- *      to disable this. Note that IOBDMAs require CVMSEG.
- *  REUSE_SKBUFFS_WITHOUT_FREE
- *      Allows the TX path to free an skbuff into the FPA hardware pool. This
- *      can significantly improve performance for forwarding and bridging, but
- *      may be somewhat dangerous. Checks are made, but if any buffer is reused
- *      without the proper Linux cleanup, the networking stack may have very
- *      bizarre bugs.
- */
-#ifndef __ETHERNET_DEFINES_H__
-#define __ETHERNET_DEFINES_H__
-
-#include <asm/octeon/cvmx-config.h>
-
-
-#define OCTEON_ETHERNET_VERSION "2.0"
-
-#ifndef CONFIG_CAVIUM_RESERVE32
-#define CONFIG_CAVIUM_RESERVE32 0
-#endif
-
-#define USE_SKBUFFS_IN_HW           1
-#ifdef CONFIG_NETFILTER
-#define REUSE_SKBUFFS_WITHOUT_FREE  0
-#else
-#define REUSE_SKBUFFS_WITHOUT_FREE  1
-#endif
-
-#define USE_HW_TCPUDP_CHECKSUM      1
-
-/* Enable Random Early Dropping under load */
-#define USE_RED                     1
-#define USE_ASYNC_IOBDMA            (CONFIG_CAVIUM_OCTEON_CVMSEG_SIZE > 0)
-
-/*
- * Allow SW based preamble removal at 10Mbps to workaround PHYs giving
- * us bad preambles.
- */
-#define USE_10MBPS_PREAMBLE_WORKAROUND 1
-/*
- * Use this to have all FPA frees also tell the L2 not to write data
- * to memory.
- */
-#define DONT_WRITEBACK(x)           (x)
-/* Use this to not have FPA frees control L2 */
-/*#define DONT_WRITEBACK(x)         0   */
-
-/* Maximum number of SKBs to try to free per xmit packet. */
-#define MAX_OUT_QUEUE_DEPTH 1000
-
-#define FAU_NUM_PACKET_BUFFERS_TO_FREE (CVMX_FAU_REG_END - sizeof(u32))
-
-#define TOTAL_NUMBER_OF_PORTS       (CVMX_PIP_NUM_INPUT_PORTS+1)
-
-
-#endif /* __ETHERNET_DEFINES_H__ */
diff --git a/drivers/staging/octeon/ethernet-mdio.c b/drivers/staging/octeon/ethernet-mdio.c
deleted file mode 100644
index 4d00fc8..0000000
--- a/drivers/staging/octeon/ethernet-mdio.c
+++ /dev/null
@@ -1,319 +0,0 @@
-/**********************************************************************
- * Author: Cavium, Inc.
- *
- * Contact: support@cavium.com
- * This file is part of the OCTEON SDK
- *
- * Copyright (c) 2003-2012 Cavium, Inc.
- *
- * This file is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, Version 2, as
- * published by the Free Software Foundation.
- *
- * This file is distributed in the hope that it will be useful, but
- * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
- * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
- * NONINFRINGEMENT.  See the GNU General Public License for more
- * details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this file; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- * or visit http://www.gnu.org/licenses/.
- *
- * This file may also be available under a different license from Cavium.
- * Contact Cavium, Inc. for more information
- **********************************************************************/
-#include <linux/kernel.h>
-#include <linux/ethtool.h>
-#include <linux/phy.h>
-#include <linux/ratelimit.h>
-#include <linux/of_mdio.h>
-#include <linux/net_tstamp.h>
-
-#include <net/dst.h>
-
-#include <asm/octeon/octeon.h>
-
-#include "ethernet-defines.h"
-#include "octeon-ethernet.h"
-
-#include <asm/octeon/cvmx-helper-board.h>
-
-#include <asm/octeon/cvmx-smix-defs.h>
-#include <asm/octeon/cvmx-gmxx-defs.h>
-#include <asm/octeon/cvmx-pip-defs.h>
-#include <asm/octeon/cvmx-pko-defs.h>
-
-static void cvm_oct_get_drvinfo(struct net_device *dev,
-				struct ethtool_drvinfo *info)
-{
-	strcpy(info->driver, "octeon-ethernet");
-	strcpy(info->version, OCTEON_ETHERNET_VERSION);
-	strcpy(info->bus_info, "Builtin");
-}
-
-static int cvm_oct_get_settings(struct net_device *dev, struct ethtool_cmd *cmd)
-{
-	struct octeon_ethernet *priv = netdev_priv(dev);
-
-	if (priv->phydev)
-		return phy_ethtool_gset(priv->phydev, cmd);
-
-	return -EINVAL;
-}
-
-static int cvm_oct_set_settings(struct net_device *dev, struct ethtool_cmd *cmd)
-{
-	struct octeon_ethernet *priv = netdev_priv(dev);
-
-	if (!capable(CAP_NET_ADMIN))
-		return -EPERM;
-
-	if (priv->phydev)
-		return phy_ethtool_sset(priv->phydev, cmd);
-
-	return -EINVAL;
-}
-
-static int cvm_oct_nway_reset(struct net_device *dev)
-{
-	struct octeon_ethernet *priv = netdev_priv(dev);
-
-	if (!capable(CAP_NET_ADMIN))
-		return -EPERM;
-
-	if (priv->phydev)
-		return phy_start_aneg(priv->phydev);
-
-	return -EINVAL;
-}
-
-const struct ethtool_ops cvm_oct_ethtool_ops = {
-	.get_drvinfo = cvm_oct_get_drvinfo,
-	.get_settings = cvm_oct_get_settings,
-	.set_settings = cvm_oct_set_settings,
-	.nway_reset = cvm_oct_nway_reset,
-	.get_link = ethtool_op_get_link,
-};
-
-/**
- * cvm_oct_ioctl_hwtstamp - IOCTL support for timestamping
- * @dev:    Device to change
- * @rq:     the request
- * @cmd:    the command
- *
- * Returns Zero on success
- */
-static int cvm_oct_ioctl_hwtstamp(struct net_device *dev,
-				  struct ifreq *rq, int cmd)
-{
-	struct octeon_ethernet *priv = netdev_priv(dev);
-	struct hwtstamp_config config;
-	union cvmx_mio_ptp_clock_cfg ptp;
-	union cvmx_gmxx_rxx_frm_ctl frm_ctl;
-	union cvmx_pip_prt_cfgx prt_cfg;
-
-	if (copy_from_user(&config, rq->ifr_data, sizeof(config)))
-		return -EFAULT;
-
-	if (config.flags) /* reserved for future extensions */
-		return -EINVAL;
-
-	/* Check the status of hardware for tiemstamps */
-	if (OCTEON_IS_MODEL(OCTEON_CN6XXX) || OCTEON_IS_MODEL(OCTEON_CNF7XXX)) {
-		/* Write TX timestamp into word 4 */
-		cvmx_write_csr(CVMX_PKO_REG_TIMESTAMP, 4);
-
-		switch (priv->imode) {
-		case CVMX_HELPER_INTERFACE_MODE_XAUI:
-		case CVMX_HELPER_INTERFACE_MODE_RXAUI:
-		case CVMX_HELPER_INTERFACE_MODE_SGMII:
-			break;
-		default:
-			/* No timestamp support*/
-			return -EOPNOTSUPP;
-		}
-
-		ptp.u64 = octeon_read_ptp_csr(CVMX_MIO_PTP_CLOCK_CFG);
-		if (!ptp.s.ptp_en) {
-			/* It should have been enabled by csrc-octeon-ptp */
-			netdev_err(dev, "Error: PTP clock not enabled\n");
-			/* No timestamp support*/
-			return -EOPNOTSUPP;
-		}
-	} else {
-			/* No timestamp support*/
-			return -EOPNOTSUPP;
-	}
-
-	switch (config.tx_type) {
-	case HWTSTAMP_TX_OFF:
-		priv->tx_timestamp_hw = 0;
-		break;
-	case HWTSTAMP_TX_ON:
-		priv->tx_timestamp_hw = 1;
-		break;
-	default:
-		return -ERANGE;
-	}
-
-	switch (config.rx_filter) {
-	case HWTSTAMP_FILTER_NONE:
-		priv->rx_timestamp_hw = 0;
-
-		frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface));
-		frm_ctl.s.ptp_mode = 0;
-		cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface), frm_ctl.u64);
-
-		prt_cfg.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(priv->ipd_port));
-		prt_cfg.s.skip = 0;
-		cvmx_write_csr(CVMX_PIP_PRT_CFGX(priv->ipd_port), prt_cfg.u64);
-		break;
-	case HWTSTAMP_FILTER_ALL:
-	case HWTSTAMP_FILTER_SOME:
-	case HWTSTAMP_FILTER_PTP_V1_L4_EVENT:
-	case HWTSTAMP_FILTER_PTP_V1_L4_SYNC:
-	case HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ:
-	case HWTSTAMP_FILTER_PTP_V2_L4_EVENT:
-	case HWTSTAMP_FILTER_PTP_V2_L4_SYNC:
-	case HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ:
-	case HWTSTAMP_FILTER_PTP_V2_L2_EVENT:
-	case HWTSTAMP_FILTER_PTP_V2_L2_SYNC:
-	case HWTSTAMP_FILTER_PTP_V2_L2_DELAY_REQ:
-	case HWTSTAMP_FILTER_PTP_V2_EVENT:
-	case HWTSTAMP_FILTER_PTP_V2_SYNC:
-	case HWTSTAMP_FILTER_PTP_V2_DELAY_REQ:
-		priv->rx_timestamp_hw = 1;
-		config.rx_filter = HWTSTAMP_FILTER_ALL;
-		frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface));
-		frm_ctl.s.ptp_mode = 1;
-		cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface), frm_ctl.u64);
-
-		prt_cfg.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(priv->ipd_port));
-		prt_cfg.s.skip = 8;
-		cvmx_write_csr(CVMX_PIP_PRT_CFGX(priv->ipd_port), prt_cfg.u64);
-		break;
-	default:
-		return -ERANGE;
-	}
-
-	if (copy_to_user(rq->ifr_data, &config, sizeof(config)))
-		return -EFAULT;
-
-	return 0;
-}
-
-/**
- * cvm_oct_ioctl - IOCTL support for PHY control
- * @dev:    Device to change
- * @rq:     the request
- * @cmd:    the command
- *
- * Returns Zero on success
- */
-int cvm_oct_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)
-{
-	struct octeon_ethernet *priv = netdev_priv(dev);
-
-	if (!netif_running(dev))
-		return -EINVAL;
-
-	switch (cmd) {
-	case SIOCSHWTSTAMP:
-		return cvm_oct_ioctl_hwtstamp(dev, rq, cmd);
-
-	default:
-		if (priv->phydev)
-			return phy_mii_ioctl(priv->phydev, rq, cmd);
-	}
-	return -EOPNOTSUPP;
-}
-
-static void cvm_oct_note_carrier(struct octeon_ethernet *priv,
-				 cvmx_helper_link_info_t li)
-{
-	if (li.s.link_up) {
-		pr_notice_ratelimited("%s: %u Mbps %s duplex, port %d\n",
-				      netdev_name(priv->netdev), li.s.speed,
-				      (li.s.full_duplex) ? "Full" : "Half",
-				      priv->ipd_port);
-	} else {
-		pr_notice_ratelimited("%s: Link down\n",
-				      netdev_name(priv->netdev));
-	}
-}
-
-/**
- * cvm_oct_set_carrier - common wrapper of netif_carrier_{on,off}
- *
- * @priv: Device struct.
- * @link_info: Current state.
- */
-void cvm_oct_set_carrier(struct octeon_ethernet *priv,
-			 cvmx_helper_link_info_t link_info)
-{
-	cvm_oct_note_carrier(priv, link_info);
-	if (link_info.s.link_up) {
-		if (!netif_carrier_ok(priv->netdev))
-			netif_carrier_on(priv->netdev);
-	} else {
-		if (netif_carrier_ok(priv->netdev))
-			netif_carrier_off(priv->netdev);
-	}
-}
-
-void cvm_oct_adjust_link(struct net_device *dev)
-{
-	struct octeon_ethernet *priv = netdev_priv(dev);
-	cvmx_helper_link_info_t link_info;
-
-	if (priv->last_link != priv->phydev->link) {
-		priv->last_link = priv->phydev->link;
-		link_info.u64 = 0;
-		link_info.s.link_up = priv->last_link ? 1 : 0;
-		link_info.s.full_duplex = priv->phydev->duplex ? 1 : 0;
-		link_info.s.speed = priv->phydev->speed;
-
-		cvmx_helper_link_set(priv->ipd_port, link_info);
-
-		cvm_oct_note_carrier(priv, link_info);
-	}
-}
-
-/**
- * cvm_oct_phy_setup_device - setup the PHY
- *
- * @dev:    Device to setup
- *
- * Returns Zero on success, negative on failure
- */
-int cvm_oct_phy_setup_device(struct net_device *dev)
-{
-	struct octeon_ethernet *priv = netdev_priv(dev);
-	struct device_node *phy_node;
-
-	if (!priv->of_node)
-		goto no_phy;
-
-	phy_node = of_parse_phandle(priv->of_node, "phy-handle", 0);
-	if (!phy_node)
-		goto no_phy;
-
-	priv->phydev = of_phy_connect(dev, phy_node, cvm_oct_adjust_link, 0,
-				      PHY_INTERFACE_MODE_GMII);
-
-	if (priv->phydev == NULL)
-		return -ENODEV;
-
-	priv->last_link = 0;
-	phy_start_aneg(priv->phydev);
-
-	return 0;
-no_phy:
-	/* If there is no phy, assume a direct MAC connection and that
-	 * the link is up.
-	 */
-	netif_carrier_on(dev);
-	return 0;
-}
diff --git a/drivers/staging/octeon/ethernet-mem.c b/drivers/staging/octeon/ethernet-mem.c
deleted file mode 100644
index 19f6034..0000000
--- a/drivers/staging/octeon/ethernet-mem.c
+++ /dev/null
@@ -1,320 +0,0 @@
-/**********************************************************************
- * Author: Cavium, Inc.
- *
- * Contact: support@cavium.com
- * This file is part of the OCTEON SDK
- *
- * Copyright (c) 2003-2012 Cavium, Inc.
- *
- * This file is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, Version 2, as
- * published by the Free Software Foundation.
- *
- * This file is distributed in the hope that it will be useful, but
- * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
- * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
- * NONINFRINGEMENT.  See the GNU General Public License for more
- * details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this file; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- * or visit http://www.gnu.org/licenses/.
- *
- * This file may also be available under a different license from Cavium.
- * Contact Cavium, Inc. for more information
- **********************************************************************/
-#include <linux/netdevice.h>
-#include <linux/export.h>
-#include <linux/kernel.h>
-#include <linux/slab.h>
-
-#include <asm/octeon/octeon.h>
-#include <asm/octeon/cvmx-fpa.h>
-
-#include "ethernet-defines.h"
-#include "octeon-ethernet.h"
-
-struct fpa_pool {
-	int pool;
-	int users;
-	int size;
-	char kmem_name[20];
-	struct kmem_cache *kmem;
-	int (*fill)(struct fpa_pool *p, int num);
-	int (*empty)(struct fpa_pool *p, int num);
-};
-
-static DEFINE_SPINLOCK(cvm_oct_pools_lock);
-/* Eight pools. */
-static struct fpa_pool cvm_oct_pools[] = {
-	{-1}, {-1}, {-1}, {-1}, {-1}, {-1}, {-1}, {-1}
-};
-
-/**
- * cvm_oct_fill_hw_skbuff - fill the supplied hardware pool with skbuffs
- * @pool:     Pool to allocate an skbuff for
- * @size:     Size of the buffer needed for the pool
- * @elements: Number of buffers to allocate
- *
- * Returns the actual number of buffers allocated.
- */
-static int cvm_oct_fill_hw_skbuff(struct fpa_pool *pool, int elements)
-{
-	int freed = elements;
-	int size = pool->size;
-	int pool_num = pool->pool;
-	while (freed) {
-		int extra_reserve;
-		u8 *desired_data;
-		struct sk_buff *skb = alloc_skb(size + CVM_OCT_SKB_TO_FPA_PADDING,
-						GFP_ATOMIC);
-		if (skb == NULL) {
-			pr_err("Failed to allocate skb for hardware pool %d\n",
-			       pool_num);
-			break;
-		}
-		desired_data = cvm_oct_get_fpa_head(skb);
-		extra_reserve = desired_data - skb->data;
-		skb_reserve(skb, extra_reserve);
-		*(struct sk_buff **)(skb->data - sizeof(void *)) = skb;
-		cvmx_fpa_free(skb->data, pool_num, DONT_WRITEBACK(size / 128));
-		freed--;
-	}
-	return elements - freed;
-}
-
-/**
- * cvm_oct_free_hw_skbuff- free hardware pool skbuffs
- * @pool:     Pool to allocate an skbuff for
- * @size:     Size of the buffer needed for the pool
- * @elements: Number of buffers to allocate
- */
-static int cvm_oct_free_hw_skbuff(struct fpa_pool *pool, int elements)
-{
-	char *memory;
-	int pool_num = pool->pool;
-
-	do {
-		memory = cvmx_fpa_alloc(pool_num);
-		if (memory) {
-			struct sk_buff *skb = *cvm_oct_packet_to_skb(memory);
-			elements--;
-			dev_kfree_skb(skb);
-		}
-	} while (memory);
-
-	if (elements < 0)
-		pr_err("Freeing of pool %u had too many skbuffs (%d)\n",
-		       pool_num, elements);
-	else if (elements > 0)
-		pr_err("Freeing of pool %u is missing %d skbuffs\n",
-		       pool_num, elements);
-
-	return 0;
-}
-
-/**
- * cvm_oct_fill_hw_memory - fill a hardware pool with memory.
- * @pool:     Pool to populate
- * @size:     Size of each buffer in the pool
- * @elements: Number of buffers to allocate
- *
- * Returns the actual number of buffers allocated.
- */
-static int cvm_oct_fill_hw_kmem(struct fpa_pool *pool, int elements)
-{
-	char *memory;
-	int freed = elements;
-
-	while (freed) {
-		memory = kmem_cache_alloc(pool->kmem, GFP_KERNEL);
-		if (unlikely(memory == NULL)) {
-			pr_err("Unable to allocate %u bytes for FPA pool %d\n",
-			       elements * pool->size, pool->pool);
-			break;
-		}
-		cvmx_fpa_free(memory, pool->pool, 0);
-		freed--;
-	}
-	return elements - freed;
-}
-
-/**
- * cvm_oct_free_hw_memory - Free memory allocated by cvm_oct_fill_hw_memory
- * @pool:     FPA pool to free
- * @size:     Size of each buffer in the pool
- * @elements: Number of buffers that should be in the pool
- */
-static int cvm_oct_free_hw_kmem(struct fpa_pool *pool, int elements)
-{
-	char *fpa;
-	while (elements) {
-		fpa = cvmx_fpa_alloc(pool->pool);
-		if (!fpa)
-			break;
-		elements--;
-		kmem_cache_free(pool->kmem, fpa);
-	}
-
-	if (elements < 0)
-		pr_err("Freeing of pool %u had too many buffers (%d)\n",
-		       pool->pool, elements);
-	else if (elements > 0)
-		pr_err("Warning: Freeing of pool %u is missing %d buffers\n",
-		       pool->pool, elements);
-	return elements;
-}
-
-int cvm_oct_mem_fill_fpa(int pool, int elements)
-{
-	struct fpa_pool *p;
-
-	if (pool < 0 || pool >= ARRAY_SIZE(cvm_oct_pools))
-		return -EINVAL;
-
-	p = cvm_oct_pools + pool;
-
-	return p->fill(p, elements);
-}
-EXPORT_SYMBOL(cvm_oct_mem_fill_fpa);
-
-int cvm_oct_mem_empty_fpa(int pool, int elements)
-{
-	struct fpa_pool *p;
-
-	if (pool < 0 || pool >= ARRAY_SIZE(cvm_oct_pools))
-		return -EINVAL;
-
-	p = cvm_oct_pools + pool;
-	if (p->empty)
-		return p->empty(p, elements);
-
-	return 0;
-}
-EXPORT_SYMBOL(cvm_oct_mem_empty_fpa);
-
-void cvm_oct_mem_cleanup(void)
-{
-	int i;
-
-	spin_lock(&cvm_oct_pools_lock);
-
-	for (i = 0; i < ARRAY_SIZE(cvm_oct_pools); i++)
-		if (cvm_oct_pools[i].kmem)
-			kmem_cache_shrink(cvm_oct_pools[i].kmem);
-	spin_unlock(&cvm_oct_pools_lock);
-}
-EXPORT_SYMBOL(cvm_oct_mem_cleanup);
-
-/**
- * cvm_oct_alloc_fpa_pool() - Allocate an FPA pool of the given size
- * @pool:  Requested pool number (-1 for don't care).
- * @size:  The size of the pool elements.
- *
- * Returns the pool number or a negative number on error.
- */
-int cvm_oct_alloc_fpa_pool(int pool, int size)
-{
-	int i;
-	int ret = 0;
-
-	if (pool >= (int)ARRAY_SIZE(cvm_oct_pools) || size < 128)
-		return -EINVAL;
-
-	spin_lock(&cvm_oct_pools_lock);
-
-	if (pool >= 0) {
-		if (cvm_oct_pools[pool].pool != -1) {
-			if (cvm_oct_pools[pool].size == size) {
-				/* Already allocated */
-				cvm_oct_pools[pool].users++;
-				ret = pool;
-				goto out;
-			} else {
-				/* conflict */
-				ret = -EINVAL;
-				goto out;
-			}
-		}
-	} else {
-		/* Find an established pool */
-		for (i = 0; i < ARRAY_SIZE(cvm_oct_pools); i++)
-			if (cvm_oct_pools[i].pool >= 0 &&
-			    cvm_oct_pools[i].size == size) {
-				cvm_oct_pools[i].users++;
-				ret = i;
-				goto out;
-			}
-
-		/* Find an empty pool */
-		for (i = 0; i < ARRAY_SIZE(cvm_oct_pools); i++)
-			if (cvm_oct_pools[i].pool == -1) {
-				pool = i;
-				break;
-			}
-		if (pool < 0) {
-			/* No empties. */
-			ret = -EINVAL;
-			goto out;
-		}
-	}
-
-	/* Setup the pool */
-	cvm_oct_pools[pool].pool = pool;
-	cvm_oct_pools[pool].users++;
-	cvm_oct_pools[pool].size = size;
-	if (USE_SKBUFFS_IN_HW && pool == 0) {
-		/* Special packet pool */
-		cvm_oct_pools[pool].fill = cvm_oct_fill_hw_skbuff;
-		cvm_oct_pools[pool].empty = cvm_oct_free_hw_skbuff;
-	} else {
-		snprintf(cvm_oct_pools[pool].kmem_name,
-			 sizeof(cvm_oct_pools[pool].kmem_name),
-			 "oct-fpa-%d", size);
-		cvm_oct_pools[pool].fill = cvm_oct_fill_hw_kmem;
-		cvm_oct_pools[pool].empty = cvm_oct_free_hw_kmem;
-		cvm_oct_pools[pool].kmem =
-			kmem_cache_create(cvm_oct_pools[pool].kmem_name,
-					  size, 128, 0, NULL);
-		if (!cvm_oct_pools[pool].kmem) {
-			ret = -ENOMEM;
-			cvm_oct_pools[pool].pool = -1;
-			goto out;
-		}
-	}
-	ret = pool;
-out:
-	spin_unlock(&cvm_oct_pools_lock);
-	return ret;
-}
-EXPORT_SYMBOL(cvm_oct_alloc_fpa_pool);
-
-/**
- * cvm_oct_release_fpa_pool() - Releases an FPA pool
- * @pool:  Pool number.
- *
- * This undoes the action of cvm_oct_alloc_fpa_pool().
- *
- * Returns zero on success.
- */
-int cvm_oct_release_fpa_pool(int pool)
-{
-	int ret = -EINVAL;
-
-	if (pool < 0 || pool >= (int)ARRAY_SIZE(cvm_oct_pools))
-		return ret;
-
-	spin_lock(&cvm_oct_pools_lock);
-
-	if (cvm_oct_pools[pool].users <= 0) {
-		pr_err("Error: Unbalanced FPA pool allocation\n");
-		goto out;
-	}
-	cvm_oct_pools[pool].users--;
-	ret = 0;
-out:
-	spin_unlock(&cvm_oct_pools_lock);
-	return ret;
-}
-EXPORT_SYMBOL(cvm_oct_release_fpa_pool);
diff --git a/drivers/staging/octeon/ethernet-rgmii.c b/drivers/staging/octeon/ethernet-rgmii.c
deleted file mode 100644
index 2db614f..0000000
--- a/drivers/staging/octeon/ethernet-rgmii.c
+++ /dev/null
@@ -1,354 +0,0 @@
-/**********************************************************************
- * Author: Cavium, Inc.
- *
- * Contact: support@cavium.com
- * This file is part of the OCTEON SDK
- *
- * Copyright (c) 2003-2012 Cavium, Inc.
- *
- * This file is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, Version 2, as
- * published by the Free Software Foundation.
- *
- * This file is distributed in the hope that it will be useful, but
- * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
- * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
- * NONINFRINGEMENT.  See the GNU General Public License for more
- * details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this file; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- * or visit http://www.gnu.org/licenses/.
- *
- * This file may also be available under a different license from Cavium.
- * Contact Cavium, Inc. for more information
- **********************************************************************/
-#include <linux/kernel.h>
-#include <linux/netdevice.h>
-#include <linux/interrupt.h>
-#include <linux/phy.h>
-#include <linux/ratelimit.h>
-#include <net/dst.h>
-
-#include <asm/octeon/octeon.h>
-
-#include "ethernet-defines.h"
-#include "octeon-ethernet.h"
-
-#include <asm/octeon/cvmx-helper.h>
-
-#include <asm/octeon/cvmx-ipd-defs.h>
-#include <asm/octeon/cvmx-npi-defs.h>
-#include <asm/octeon/cvmx-gmxx-defs.h>
-
-DEFINE_SPINLOCK(global_register_lock);
-
-static int number_rgmii_ports;
-
-static void cvm_oct_rgmii_poll(struct net_device *dev)
-{
-	struct octeon_ethernet *priv = netdev_priv(dev);
-	unsigned long flags = 0;
-	cvmx_helper_link_info_t link_info;
-	int use_global_register_lock = (priv->phydev == NULL);
-
-	BUG_ON(in_interrupt());
-	if (use_global_register_lock) {
-		/* Take the global register lock since we are going to
-		 * touch registers that affect more than one port.
-		 */
-		spin_lock_irqsave(&global_register_lock, flags);
-	} else {
-		mutex_lock(&priv->phydev->bus->mdio_lock);
-	}
-
-	link_info = cvmx_helper_link_get(priv->ipd_port);
-	if (link_info.u64 == priv->link_info) {
-		/* If the 10Mbps preamble workaround is supported and we're
-		 * at 10Mbps we may need to do some special checking.
-		 */
-		if (USE_10MBPS_PREAMBLE_WORKAROUND && (link_info.s.speed == 10)) {
-			/* Read the GMXX_RXX_INT_REG[PCTERR] bit and
-			 * see if we are getting preamble errors.
-			 */
-			union cvmx_gmxx_rxx_int_reg gmxx_rxx_int_reg;
-			gmxx_rxx_int_reg.u64 =
-				cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(priv->interface_port, priv->interface));
-			if (gmxx_rxx_int_reg.s.pcterr) {
-				/* We are getting preamble errors at
-				 * 10Mbps.  Most likely the PHY is
-				 * giving us packets with mis aligned
-				 * preambles. In order to get these
-				 * packets we need to disable preamble
-				 * checking and do it in software.
-				 */
-				union cvmx_gmxx_rxx_frm_ctl gmxx_rxx_frm_ctl;
-				union cvmx_ipd_sub_port_fcs ipd_sub_port_fcs;
-
-				/* Disable preamble checking */
-				gmxx_rxx_frm_ctl.u64 =
-					cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface));
-				gmxx_rxx_frm_ctl.s.pre_chk = 0;
-				cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface),
-					       gmxx_rxx_frm_ctl.u64);
-
-				/* Disable FCS stripping */
-				ipd_sub_port_fcs.u64 = cvmx_read_csr(CVMX_IPD_SUB_PORT_FCS);
-				ipd_sub_port_fcs.s.port_bit &= 0xffffffffull ^ (1ull << priv->ipd_port);
-				cvmx_write_csr(CVMX_IPD_SUB_PORT_FCS,
-					       ipd_sub_port_fcs.u64);
-
-				/* Clear any error bits */
-				cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(priv->interface_port, priv->interface),
-					       gmxx_rxx_int_reg.u64);
-				printk_ratelimited("%s: Using 10Mbps with software preamble removal\n",
-						   dev->name);
-			}
-		}
-
-		if (use_global_register_lock)
-			spin_unlock_irqrestore(&global_register_lock, flags);
-		else
-			mutex_unlock(&priv->phydev->bus->mdio_lock);
-		return;
-	}
-
-	/* If the 10Mbps preamble workaround is allowed we need to on
-	 * preamble checking, FCS stripping, and clear error bits on
-	 * every speed change. If errors occur during 10Mbps operation
-	 * the above code will change this stuff.
-	 */
-	if (USE_10MBPS_PREAMBLE_WORKAROUND) {
-		union cvmx_gmxx_rxx_frm_ctl gmxx_rxx_frm_ctl;
-		union cvmx_ipd_sub_port_fcs ipd_sub_port_fcs;
-		union cvmx_gmxx_rxx_int_reg gmxx_rxx_int_reg;
-
-		/* Enable preamble checking */
-		gmxx_rxx_frm_ctl.u64 =
-		    cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface));
-		gmxx_rxx_frm_ctl.s.pre_chk = 1;
-		cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface),
-			       gmxx_rxx_frm_ctl.u64);
-		/* Enable FCS stripping */
-		ipd_sub_port_fcs.u64 = cvmx_read_csr(CVMX_IPD_SUB_PORT_FCS);
-		ipd_sub_port_fcs.s.port_bit |= 1ull << priv->ipd_port;
-		cvmx_write_csr(CVMX_IPD_SUB_PORT_FCS, ipd_sub_port_fcs.u64);
-		/* Clear any error bits */
-		gmxx_rxx_int_reg.u64 =
-			cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(priv->interface_port, priv->interface));
-		cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(priv->interface_port, priv->interface),
-			       gmxx_rxx_int_reg.u64);
-	}
-	if (priv->phydev == NULL) {
-		link_info = cvmx_helper_link_autoconf(priv->ipd_port);
-		priv->link_info = link_info.u64;
-	}
-
-	if (use_global_register_lock)
-		spin_unlock_irqrestore(&global_register_lock, flags);
-	else
-		mutex_unlock(&priv->phydev->bus->mdio_lock);
-
-	if (priv->phydev == NULL)
-		cvm_oct_set_carrier(priv, link_info);
-}
-
-static irqreturn_t cvm_oct_rgmii_rml_interrupt(int cpl, void *dev_id)
-{
-	union cvmx_npi_rsl_int_blocks rsl_int_blocks;
-	int index;
-	irqreturn_t return_status = IRQ_NONE;
-
-	rsl_int_blocks.u64 = cvmx_read_csr(CVMX_NPI_RSL_INT_BLOCKS);
-
-	/* Check and see if this interrupt was caused by the GMX0 block */
-	if (rsl_int_blocks.s.gmx0) {
-		int interface = 0;
-		/* Loop through every port of this interface */
-		for (index = 0;
-		     index < cvmx_helper_ports_on_interface(interface);
-		     index++) {
-			/* Read the GMX interrupt status bits */
-			union cvmx_gmxx_rxx_int_reg gmx_rx_int_reg;
-			gmx_rx_int_reg.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(index, interface));
-			gmx_rx_int_reg.u64 &= cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(index, interface));
-			/* Poll the port if inband status changed */
-			if (gmx_rx_int_reg.s.phy_dupx
-			    || gmx_rx_int_reg.s.phy_link
-			    || gmx_rx_int_reg.s.phy_spd) {
-				struct octeon_ethernet *priv = cvm_oct_by_port[cvmx_helper_get_ipd_port(interface, index)];
-
-				if (priv && !atomic_read(&cvm_oct_poll_queue_stopping))
-					queue_work(cvm_oct_poll_queue, &priv->port_work);
-
-				gmx_rx_int_reg.u64 = 0;
-				gmx_rx_int_reg.s.phy_dupx = 1;
-				gmx_rx_int_reg.s.phy_link = 1;
-				gmx_rx_int_reg.s.phy_spd = 1;
-				cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(index, interface),
-					       gmx_rx_int_reg.u64);
-				return_status = IRQ_HANDLED;
-			}
-		}
-	}
-
-	/* Check and see if this interrupt was caused by the GMX1 block */
-	if (rsl_int_blocks.s.gmx1) {
-		int interface = 1;
-		/* Loop through every port of this interface */
-		for (index = 0;
-		     index < cvmx_helper_ports_on_interface(interface);
-		     index++) {
-			/* Read the GMX interrupt status bits */
-			union cvmx_gmxx_rxx_int_reg gmx_rx_int_reg;
-			gmx_rx_int_reg.u64 =
-				cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(index, interface));
-			gmx_rx_int_reg.u64 &=
-				cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(index, interface));
-			/* Poll the port if inband status changed */
-			if (gmx_rx_int_reg.s.phy_dupx
-			    || gmx_rx_int_reg.s.phy_link
-			    || gmx_rx_int_reg.s.phy_spd) {
-				struct octeon_ethernet *priv = cvm_oct_by_port[cvmx_helper_get_ipd_port(interface, index)];
-
-				if (priv && !atomic_read(&cvm_oct_poll_queue_stopping))
-					queue_work(cvm_oct_poll_queue, &priv->port_work);
-
-				gmx_rx_int_reg.u64 = 0;
-				gmx_rx_int_reg.s.phy_dupx = 1;
-				gmx_rx_int_reg.s.phy_link = 1;
-				gmx_rx_int_reg.s.phy_spd = 1;
-				cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(index, interface),
-					       gmx_rx_int_reg.u64);
-				return_status = IRQ_HANDLED;
-			}
-		}
-	}
-	return return_status;
-}
-
-static void cvm_oct_rgmii_immediate_poll(struct work_struct *work)
-{
-	struct octeon_ethernet *priv = container_of(work, struct octeon_ethernet, port_work);
-	cvm_oct_rgmii_poll(priv->netdev);
-}
-
-int cvm_oct_rgmii_open(struct net_device *dev)
-{
-	union cvmx_gmxx_prtx_cfg gmx_cfg;
-	struct octeon_ethernet *priv = netdev_priv(dev);
-	cvmx_helper_link_info_t link_info;
-	int rv;
-
-	rv = cvm_oct_phy_setup_device(dev);
-	if (rv)
-		return rv;
-
-
-	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
-	gmx_cfg.s.en = 1;
-	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface), gmx_cfg.u64);
-
-	if (!octeon_is_simulation()) {
-		if (priv->phydev) {
-			int r = phy_read_status(priv->phydev);
-			if (r == 0 && priv->phydev->link == 0)
-				netif_carrier_off(dev);
-			cvm_oct_adjust_link(dev);
-		} else {
-			link_info = cvmx_helper_link_get(priv->ipd_port);
-			if (!link_info.s.link_up)
-				netif_carrier_off(dev);
-			spin_lock(&priv->poll_lock);
-			priv->poll = cvm_oct_rgmii_poll;
-			spin_unlock(&priv->poll_lock);
-		}
-	}
-
-	INIT_WORK(&priv->port_work, cvm_oct_rgmii_immediate_poll);
-	/* Due to GMX errata in CN3XXX series chips, it is necessary
-	 * to take the link down immediately when the PHY changes
-	 * state. In order to do this we call the poll function every
-	 * time the RGMII inband status changes.  This may cause
-	 * problems if the PHY doesn't implement inband status
-	 * properly.
-	 */
-	if (number_rgmii_ports == 0) {
-		rv = request_irq(OCTEON_IRQ_RML, cvm_oct_rgmii_rml_interrupt,
-				 IRQF_SHARED, "RGMII", &number_rgmii_ports);
-		if (rv != 0)
-			return rv;
-	}
-	number_rgmii_ports++;
-
-	/* Only true RGMII ports need to be polled. In GMII mode, port
-	 * 0 is really a RGMII port.
-	 */
-	if (((priv->imode == CVMX_HELPER_INTERFACE_MODE_GMII)
-	     && (priv->ipd_port == 0))
-	    || (priv->imode == CVMX_HELPER_INTERFACE_MODE_RGMII)) {
-		if (!octeon_is_simulation()) {
-			union cvmx_gmxx_rxx_int_en gmx_rx_int_en;
-			/* Enable interrupts on inband status changes
-			 * for this port.
-			 */
-			gmx_rx_int_en.u64 =
-				cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(priv->interface_port, priv->interface));
-			gmx_rx_int_en.s.phy_dupx = 1;
-			gmx_rx_int_en.s.phy_link = 1;
-			gmx_rx_int_en.s.phy_spd = 1;
-			cvmx_write_csr(CVMX_GMXX_RXX_INT_EN(priv->interface_port, priv->interface),
-				       gmx_rx_int_en.u64);
-		}
-	}
-
-	return 0;
-}
-
-int cvm_oct_rgmii_stop(struct net_device *dev)
-{
-	union cvmx_gmxx_prtx_cfg gmx_cfg;
-	struct octeon_ethernet *priv = netdev_priv(dev);
-
-	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
-	gmx_cfg.s.en = 0;
-	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface), gmx_cfg.u64);
-
-	/* Only true RGMII ports need to be polled. In GMII mode, port
-	 * 0 is really a RGMII port.
-	 */
-	if (((priv->imode == CVMX_HELPER_INTERFACE_MODE_GMII)
-	     && (priv->ipd_port == 0))
-	    || (priv->imode == CVMX_HELPER_INTERFACE_MODE_RGMII)) {
-		if (!octeon_is_simulation()) {
-			union cvmx_gmxx_rxx_int_en gmx_rx_int_en;
-			/* Disable interrupts on inband status changes
-			 * for this port.
-			 */
-			gmx_rx_int_en.u64 =
-				cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(priv->interface_port, priv->interface));
-			gmx_rx_int_en.s.phy_dupx = 0;
-			gmx_rx_int_en.s.phy_link = 0;
-			gmx_rx_int_en.s.phy_spd = 0;
-			cvmx_write_csr(CVMX_GMXX_RXX_INT_EN(priv->interface_port, priv->interface),
-				       gmx_rx_int_en.u64);
-		}
-	}
-
-	/* Remove the interrupt handler when the last port is removed. */
-	number_rgmii_ports--;
-	if (number_rgmii_ports == 0)
-		free_irq(OCTEON_IRQ_RML, &number_rgmii_ports);
-	cancel_work_sync(&priv->port_work);
-
-	return 0;
-}
-
-int cvm_oct_rgmii_init(struct net_device *dev)
-{
-	cvm_oct_common_init(dev);
-	dev->netdev_ops->ndo_stop(dev);
-
-	return 0;
-}
diff --git a/drivers/staging/octeon/ethernet-rx.c b/drivers/staging/octeon/ethernet-rx.c
deleted file mode 100644
index 950cac0..0000000
--- a/drivers/staging/octeon/ethernet-rx.c
+++ /dev/null
@@ -1,684 +0,0 @@
-/**********************************************************************
- * Author: Cavium, Inc.
- *
- * Contact: support@cavium.com
- * This file is part of the OCTEON SDK
- *
- * Copyright (c) 2003-2012 Cavium, Inc.
- *
- * This file is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, Version 2, as
- * published by the Free Software Foundation.
- *
- * This file is distributed in the hope that it will be useful, but
- * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
- * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
- * NONINFRINGEMENT.  See the GNU General Public License for more
- * details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this file; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- * or visit http://www.gnu.org/licenses/.
- *
- * This file may also be available under a different license from Cavium.
- * Contact Cavium, Inc. for more information
- **********************************************************************/
-#include <linux/module.h>
-#include <linux/kernel.h>
-#include <linux/cache.h>
-#include <linux/cpumask.h>
-#include <linux/netdevice.h>
-#include <linux/init.h>
-#include <linux/etherdevice.h>
-#include <linux/ip.h>
-#include <linux/string.h>
-#include <linux/prefetch.h>
-#include <linux/ratelimit.h>
-#include <linux/smp.h>
-#include <linux/interrupt.h>
-#include <net/dst.h>
-#ifdef CONFIG_XFRM
-#include <linux/xfrm.h>
-#include <net/xfrm.h>
-#endif /* CONFIG_XFRM */
-
-#include <asm/octeon/octeon.h>
-
-#include "ethernet-defines.h"
-#include "octeon-ethernet.h"
-
-#include <asm/octeon/cvmx-helper.h>
-#include <asm/octeon/cvmx-wqe.h>
-#include <asm/octeon/cvmx-fau.h>
-#include <asm/octeon/cvmx-pow.h>
-#include <asm/octeon/cvmx-pip.h>
-#include <asm/octeon/cvmx-scratch.h>
-
-#include <asm/octeon/cvmx-gmxx-defs.h>
-
-struct cvm_napi_wrapper {
-	struct napi_struct napi;
-	int available;
-} ____cacheline_aligned_in_smp;
-
-static struct cvm_napi_wrapper cvm_oct_napi[NR_CPUS] __cacheline_aligned_in_smp;
-
-struct cvm_oct_core_state {
-	int baseline_cores;
-	/* We want to read this without having to acquire the lock,
-	 * make it volatile so we are likely to get a fairly current
-	 * value.
-	 */
-	volatile int active_cores;
-	/* cvm_napi_wrapper.available and active_cores must be kept
-	 * consistent with this lock.
-	 */
-	spinlock_t lock;
-} ____cacheline_aligned_in_smp;
-
-static struct cvm_oct_core_state core_state __cacheline_aligned_in_smp;
-
-static void cvm_oct_enable_napi(void *_)
-{
-	int cpu = smp_processor_id();
-	napi_schedule(&cvm_oct_napi[cpu].napi);
-}
-
-static void cvm_oct_enable_one_cpu(void)
-{
-	int v;
-	int cpu;
-	unsigned long flags;
-	spin_lock_irqsave(&core_state.lock, flags);
-	/* ... if a CPU is available, Turn on NAPI polling for that CPU.  */
-	for_each_online_cpu(cpu) {
-		if (cvm_oct_napi[cpu].available > 0) {
-			cvm_oct_napi[cpu].available--;
-			core_state.active_cores++;
-			spin_unlock_irqrestore(&core_state.lock, flags);
-			if (cpu == smp_processor_id()) {
-				cvm_oct_enable_napi(NULL);
-			} else {
-#ifdef CONFIG_SMP
-				v = smp_call_function_single(cpu, cvm_oct_enable_napi,
-							     NULL, 0);
-				if (v)
-					panic("Can't enable NAPI.");
-#else
-				BUG();
-#endif
-			}
-			goto out;
-		}
-	}
-	spin_unlock_irqrestore(&core_state.lock, flags);
-out:
-	return;
-}
-
-static void cvm_oct_no_more_work(struct napi_struct *napi)
-{
-	struct cvm_napi_wrapper *nr = container_of(napi, struct cvm_napi_wrapper, napi);
-	int current_active;
-	unsigned long flags;
-
-	spin_lock_irqsave(&core_state.lock, flags);
-
-	core_state.active_cores--;
-	current_active = core_state.active_cores;
-	nr->available++;
-	BUG_ON(nr->available != 1);
-
-	spin_unlock_irqrestore(&core_state.lock, flags);
-
-	if (current_active == 0) {
-		/* No more CPUs doing processing, enable interrupts so
-		 * we can start processing again when there is
-		 * something to do.
-		 */
-		union cvmx_pow_wq_int_thrx int_thr;
-		int_thr.u64 = 0;
-		int_thr.s.iq_thr = 1;
-		int_thr.s.ds_thr = 1;
-		/* Enable POW interrupt when our port has at
-		 * least one packet.
-		 */
-		cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group),
-			       int_thr.u64);
-	}
-}
-
-/**
- * cvm_oct_do_interrupt - interrupt handler.
- *
- * The interrupt occurs whenever the POW has packets in our group.
- *
- */
-static irqreturn_t cvm_oct_do_interrupt(int cpl, void *dev_id)
-{
-	int cpu = smp_processor_id();
-	unsigned long flags;
-	union cvmx_pow_wq_int wq_int;
-
-	/* Disable the IRQ and start napi_poll. */
-
-	cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group), 0);
-
-	wq_int.u64 = 0;
-	wq_int.s.wq_int = 1 << pow_receive_group;
-	cvmx_write_csr(CVMX_POW_WQ_INT, wq_int.u64);
-
-	spin_lock_irqsave(&core_state.lock, flags);
-
-	/* ... and NAPI better not be running on this CPU.  */
-	BUG_ON(cvm_oct_napi[cpu].available != 1);
-	cvm_oct_napi[cpu].available--;
-
-	/* There better be cores available...  */
-	core_state.active_cores++;
-	BUG_ON(core_state.active_cores > core_state.baseline_cores);
-
-	spin_unlock_irqrestore(&core_state.lock, flags);
-
-	cvm_oct_enable_napi(NULL);
-
-	return IRQ_HANDLED;
-}
-
-/**
- * cvm_oct_check_rcv_error - process receive errors
- * @work: Work queue entry pointing to the packet.
- *
- * Returns Non-zero if the packet can be dropped, zero otherwise.
- */
-static inline int cvm_oct_check_rcv_error(cvmx_wqe_t *work)
-{
-	if ((work->word2.snoip.err_code == 10) && (work->word1.len <= 64)) {
-		/* Ignore length errors on min size packets. Some
-		 * equipment incorrectly pads packets to 64+4FCS
-		 * instead of 60+4FCS.  Note these packets still get
-		 * counted as frame errors.
-		 */
-	} else
-	    if (USE_10MBPS_PREAMBLE_WORKAROUND &&
-		((work->word2.snoip.err_code == 5) ||
-		 (work->word2.snoip.err_code == 7))) {
-		/* We received a packet with either an alignment error
-		 * or a FCS error. This may be signalling that we are
-		 * running 10Mbps with GMXX_RXX_FRM_CTL[PRE_CHK}
-		 * off. If this is the case we need to parse the
-		 * packet to determine if we can remove a non spec
-		 * preamble and generate a correct packet.
-		 */
-		int interface = cvmx_helper_get_interface_num(work->word1.cn38xx.ipprt);
-		int index = cvmx_helper_get_interface_index_num(work->word1.cn38xx.ipprt);
-		union cvmx_gmxx_rxx_frm_ctl gmxx_rxx_frm_ctl;
-		gmxx_rxx_frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface));
-		if (gmxx_rxx_frm_ctl.s.pre_chk == 0) {
-
-			u8 *ptr = phys_to_virt(work->packet_ptr.s.addr);
-			int i = 0;
-
-			while (i < work->word1.len - 1) {
-				if (*ptr != 0x55)
-					break;
-				ptr++;
-				i++;
-			}
-
-			if (*ptr == 0xd5) {
-				work->packet_ptr.s.addr += i + 1;
-				work->word1.len -= i + 5;
-			} else if ((*ptr & 0xf) == 0xd) {
-				work->packet_ptr.s.addr += i;
-				work->word1.len -= i + 4;
-				for (i = 0; i < work->word1.len; i++) {
-					*ptr =
-					    ((*ptr & 0xf0) >> 4) |
-					    ((*(ptr + 1) & 0xf) << 4);
-					ptr++;
-				}
-			} else {
-				printk_ratelimited("Port %d unknown preamble, packet dropped\n",
-						   work->word1.cn38xx.ipprt);
-				cvm_oct_free_work(work);
-				return 1;
-			}
-		}
-	} else {
-		printk_ratelimited("Port %d receive error code %d, packet dropped\n",
-				   work->word1.cn38xx.ipprt, work->word2.snoip.err_code);
-		cvm_oct_free_work(work);
-		return 1;
-	}
-
-	return 0;
-}
-
-/**
- * cvm_oct_ptp_to_ktime - Convert a hardware PTP timestamp into a
- * kernel timestamp.
- *
- * @ptptime: 64 bit PTP timestamp, normally in nanoseconds
- *
- * Return ktime_t
- */
-static ktime_t cvm_oct_ptp_to_ktime(u64 ptptime)
-{
-	ktime_t ktimebase;
-	u64 ptpbase;
-	unsigned long flags;
-
-	local_irq_save(flags);
-	/* Fill the icache with the code */
-	ktime_get_real();
-	/* Flush all pending operations */
-	mb();
-	/* Read the time and PTP clock as close together as
-	 * possible. It is important that this sequence take the same
-	 * amount of time to reduce jitter
-	 */
-	ktimebase = ktime_get_real();
-	ptpbase = octeon_read_ptp_csr(CVMX_MIO_PTP_CLOCK_HI);
-	local_irq_restore(flags);
-
-	return ktime_sub_ns(ktimebase, ptpbase - ptptime);
-}
-
-/**
- * cvm_oct_napi_poll - the NAPI poll function.
- * @napi: The NAPI instance, or null if called from cvm_oct_poll_controller
- * @budget: Maximum number of packets to receive.
- *
- * Returns the number of packets processed.
- */
-static int cvm_oct_napi_poll(struct napi_struct *napi, int budget)
-{
-	const int	coreid = cvmx_get_core_num();
-	u64		old_group_mask;
-	u64		old_scratch;
-	int		rx_count = 0;
-	bool		did_work_request = false;
-	bool		packet_copied;
-
-	char		*p = (char *)cvm_oct_by_port;
-	/* Prefetch cvm_oct_device since we know we need it soon */
-	prefetch(&p[0]);
-	prefetch(&p[SMP_CACHE_BYTES]);
-	prefetch(&p[2 * SMP_CACHE_BYTES]);
-
-	if (USE_ASYNC_IOBDMA) {
-		/* Save scratch in case userspace is using it */
-		CVMX_SYNCIOBDMA;
-		old_scratch = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
-	}
-
-	/* Only allow work for our group (and preserve priorities) */
-	old_group_mask = cvmx_read_csr(CVMX_POW_PP_GRP_MSKX(coreid));
-	cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(coreid),
-		       (old_group_mask & ~0xFFFFull) | 1 << pow_receive_group);
-
-	if (USE_ASYNC_IOBDMA) {
-		cvmx_pow_work_request_async(CVMX_SCR_SCRATCH, CVMX_POW_NO_WAIT);
-		did_work_request = true;
-	}
-
-	while (rx_count < budget) {
-		struct sk_buff *skb = NULL;
-		struct sk_buff **pskb = NULL;
-		bool skb_in_hw;
-		cvmx_wqe_t *work;
-		unsigned int segments;
-		int packets_to_replace = 0;
-		unsigned int packet_len;
-
-		union cvmx_buf_ptr  packet_ptr;
-
-		if (USE_ASYNC_IOBDMA && did_work_request)
-			work = cvmx_pow_work_response_async(CVMX_SCR_SCRATCH);
-		else
-			work = cvmx_pow_work_request_sync(CVMX_POW_NO_WAIT);
-
-		prefetch(work);
-		did_work_request = false;
-		if (work == NULL) {
-			union cvmx_pow_wq_int wq_int;
-			wq_int.u64 = 0;
-			wq_int.s.wq_int = 1 << pow_receive_group;
-			cvmx_write_csr(CVMX_POW_WQ_INT, wq_int.u64);
-			break;
-		}
-		packet_ptr = work->packet_ptr;
-		pskb = cvm_oct_packet_to_skb(cvm_oct_get_buffer_ptr(packet_ptr));
-		prefetch(pskb);
-
-		if (USE_ASYNC_IOBDMA && rx_count < (budget - 1)) {
-			cvmx_pow_work_request_async_nocheck(CVMX_SCR_SCRATCH, CVMX_POW_NO_WAIT);
-			did_work_request = true;
-		}
-
-		if (rx_count == 0) {
-			/* First time through, see if there is enough
-			 * work waiting to merit waking another
-			 * CPU.
-			 */
-			union cvmx_pow_wq_int_cntx counts;
-			int backlog;
-			int cores_in_use = core_state.active_cores;
-			counts.u64 = cvmx_read_csr(CVMX_POW_WQ_INT_CNTX(pow_receive_group));
-			backlog = counts.s.iq_cnt + counts.s.ds_cnt;
-			if (backlog > budget * cores_in_use && napi != NULL)
-				cvm_oct_enable_one_cpu();
-		}
-
-		/* If WORD2[SOFTWARE] then this WQE is a complete for
-		 * a TX packet.
-		 */
-		if (work->word2.s.software) {
-			struct octeon_ethernet *priv;
-			int packet_qos = work->word0.raw.unused;
-
-			skb = (struct sk_buff *)packet_ptr.u64;
-			priv = netdev_priv(skb->dev);
-			if (!netif_running(skb->dev))
-				netif_wake_queue(skb->dev);
-			if (unlikely((skb_shinfo(skb)->tx_flags | SKBTX_IN_PROGRESS) != 0 &&
-				     priv->tx_timestamp_hw)) {
-					u64 ns = *(u64 *)work->packet_data;
-					struct skb_shared_hwtstamps ts;
-					ts.syststamp = cvm_oct_ptp_to_ktime(ns);
-					ts.hwtstamp = ns_to_ktime(ns);
-					skb_tstamp_tx(skb, &ts);
-			}
-			dev_kfree_skb_any(skb);
-
-			cvmx_fpa_free(work, CVMX_FPA_WQE_POOL, DONT_WRITEBACK(1));
-
-			/* We are done with this one, adjust the queue
-			 * depth.
-			 */
-			cvmx_fau_atomic_add32(priv->tx_queue[packet_qos].fau, -1);
-			continue;
-		}
-		segments = work->word2.s.bufs;
-		skb_in_hw = USE_SKBUFFS_IN_HW && segments > 0;
-		if (likely(skb_in_hw)) {
-			skb = *pskb;
-			prefetch(&skb->head);
-			prefetch(&skb->len);
-		}
-		prefetch(cvm_oct_by_port[work->word1.cn38xx.ipprt]);
-
-		/* Immediately throw away all packets with receive errors */
-		if (unlikely(work->word2.snoip.rcv_error)) {
-			if (cvm_oct_check_rcv_error(work))
-				continue;
-		}
-
-		packet_len = work->word1.len;
-		/* We can only use the zero copy path if skbuffs are
-		 * in the FPA pool and the packet fits in a single
-		 * buffer.
-		 */
-		if (likely(skb_in_hw)) {
-			skb->data = phys_to_virt(packet_ptr.s.addr);
-			prefetch(skb->data);
-			skb->len = packet_len;
-			packets_to_replace = segments;
-			if (likely(segments == 1)) {
-				skb_set_tail_pointer(skb, skb->len);
-			} else {
-				struct sk_buff *current_skb = skb;
-				struct sk_buff *next_skb = NULL;
-				unsigned int segment_size;
-				bool first_frag = true;
-
-				skb_frag_list_init(skb);
-				/* Multi-segment packet. */
-				for (;;) {
-					/* Octeon Errata PKI-100: The segment size is
-					 * wrong. Until it is fixed, calculate the
-					 * segment size based on the packet pool
-					 * buffer size. When it is fixed, the
-					 * following line should be replaced with this
-					 * one: int segment_size =
-					 * segment_ptr.s.size;
-					 */
-					segment_size = CVMX_FPA_PACKET_POOL_SIZE -
-						(packet_ptr.s.addr - (((packet_ptr.s.addr >> 7) - packet_ptr.s.back) << 7));
-					if (segment_size > packet_len)
-						segment_size = packet_len;
-					if (!first_frag) {
-						current_skb->len = segment_size;
-						skb->data_len += segment_size;
-						skb->truesize += current_skb->truesize;
-					}
-					skb_set_tail_pointer(current_skb, segment_size);
-					packet_len -= segment_size;
-					segments--;
-					if (segments == 0)
-						break;
-					packet_ptr = *(union cvmx_buf_ptr *)phys_to_virt(packet_ptr.s.addr - 8);
-					next_skb = *cvm_oct_packet_to_skb(cvm_oct_get_buffer_ptr(packet_ptr));
-					if (first_frag) {
-						skb_frag_add_head(current_skb, next_skb);
-					} else {
-						current_skb->next = next_skb;
-						next_skb->next = NULL;
-					}
-					current_skb = next_skb;
-					first_frag = false;
-					current_skb->data = phys_to_virt(packet_ptr.s.addr);
-				}
-			}
-			packet_copied = false;
-		} else {
-			/* We have to copy the packet. First allocate
-			 * an skbuff for it.
-			 */
-			skb = dev_alloc_skb(packet_len);
-			if (!skb) {
-				printk_ratelimited("Port %d failed to allocate skbuff, packet dropped\n",
-						   work->word1.cn38xx.ipprt);
-				cvm_oct_free_work(work);
-				continue;
-			}
-
-			/* Check if we've received a packet that was
-			 * entirely stored in the work entry.
-			 */
-			if (unlikely(work->word2.s.bufs == 0)) {
-				u8 *ptr = work->packet_data;
-
-				if (likely(!work->word2.s.not_IP)) {
-					/* The beginning of the packet
-					 * moves for IP packets.
-					 */
-					if (work->word2.s.is_v6)
-						ptr += 2;
-					else
-						ptr += 6;
-				}
-				memcpy(skb_put(skb, packet_len), ptr, packet_len);
-				/* No packet buffers to free */
-			} else {
-				int segments = work->word2.s.bufs;
-				union cvmx_buf_ptr segment_ptr = work->packet_ptr;
-
-				while (segments--) {
-					union cvmx_buf_ptr next_ptr =
-					    *(union cvmx_buf_ptr *)phys_to_virt(segment_ptr.s.addr - 8);
-
-			/* Octeon Errata PKI-100: The segment size is
-			 * wrong. Until it is fixed, calculate the
-			 * segment size based on the packet pool
-			 * buffer size. When it is fixed, the
-			 * following line should be replaced with this
-			 * one: int segment_size =
-			 * segment_ptr.s.size;
-			 */
-					int segment_size = CVMX_FPA_PACKET_POOL_SIZE -
-						(segment_ptr.s.addr - (((segment_ptr.s.addr >> 7) - segment_ptr.s.back) << 7));
-					/* Don't copy more than what
-					 * is left in the packet.
-					 */
-					if (segment_size > packet_len)
-						segment_size = packet_len;
-					/* Copy the data into the packet */
-					memcpy(skb_put(skb, segment_size),
-					       phys_to_virt(segment_ptr.s.addr),
-					       segment_size);
-					packet_len -= segment_size;
-					segment_ptr = next_ptr;
-				}
-			}
-			packet_copied = true;
-		}
-
-		if (likely((work->word1.cn38xx.ipprt < TOTAL_NUMBER_OF_PORTS) &&
-			   cvm_oct_by_port[work->word1.cn38xx.ipprt])) {
-			struct octeon_ethernet *priv = cvm_oct_by_port[work->word1.cn38xx.ipprt];
-
-			/* Only accept packets for devices that are
-			 * currently up.
-			 */
-			if (likely(priv->netdev->flags & IFF_UP)) {
-				if (priv->rx_timestamp_hw) {
-					/* The first 8 bytes are the timestamp */
-					u64 ns = *(u64 *)skb->data;
-					struct skb_shared_hwtstamps *ts;
-					ts = skb_hwtstamps(skb);
-					ts->hwtstamp = ns_to_ktime(ns);
-					ts->syststamp = cvm_oct_ptp_to_ktime(ns);
-					__skb_pull(skb, 8);
-				}
-				skb->protocol = eth_type_trans(skb, priv->netdev);
-				skb->dev = priv->netdev;
-
-				if (unlikely(work->word2.s.not_IP || work->word2.s.IP_exc ||
-					work->word2.s.L4_error || !work->word2.s.tcp_or_udp))
-					skb->ip_summed = CHECKSUM_NONE;
-				else
-					skb->ip_summed = CHECKSUM_UNNECESSARY;
-
-				/* Increment RX stats for virtual ports */
-				if (work->word1.cn38xx.ipprt >= CVMX_PIP_NUM_INPUT_PORTS) {
-					atomic64_add(1, (atomic64_t *)&priv->netdev->stats.rx_packets);
-					atomic64_add(skb->len, (atomic64_t *)&priv->netdev->stats.rx_bytes);
-				}
-				netif_receive_skb(skb);
-				rx_count++;
-			} else {
-				/* Drop any packet received for a device that isn't up */
-				atomic64_add(1, (atomic64_t *)&priv->netdev->stats.rx_dropped);
-				dev_kfree_skb_any(skb);
-			}
-		} else {
-			/* Drop any packet received for a device that
-			 * doesn't exist.
-			 */
-			printk_ratelimited("Port %d not controlled by Linux, packet dropped\n",
-					   work->word1.cn38xx.ipprt);
-			dev_kfree_skb_any(skb);
-		}
-		/* Check to see if the skbuff and work share the same
-		 * packet buffer.
-		 */
-		if (USE_SKBUFFS_IN_HW && likely(!packet_copied)) {
-			/* This buffer needs to be replaced, increment
-			 * the number of buffers we need to free by
-			 * one.
-			 */
-			cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE,
-					      packets_to_replace);
-
-			cvmx_fpa_free(work, CVMX_FPA_WQE_POOL,
-				      DONT_WRITEBACK(1));
-		} else {
-			cvm_oct_free_work(work);
-		}
-	}
-	/* Restore the original POW group mask */
-	cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(coreid), old_group_mask);
-	if (USE_ASYNC_IOBDMA) {
-		/* Restore the scratch area */
-		cvmx_scratch_write64(CVMX_SCR_SCRATCH, old_scratch);
-	}
-	cvm_oct_rx_refill_pool(0);
-
-	if (rx_count < budget && napi != NULL) {
-		/* No more work */
-		napi_complete(napi);
-		cvm_oct_no_more_work(napi);
-	}
-	return rx_count;
-}
-
-#ifdef CONFIG_NET_POLL_CONTROLLER
-/**
- * cvm_oct_poll_controller - poll for receive packets
- * device.
- *
- * @dev:    Device to poll. Unused
- */
-void cvm_oct_poll_controller(struct net_device *dev)
-{
-	cvm_oct_napi_poll(NULL, 16);
-}
-#endif
-
-void cvm_oct_rx_initialize(void)
-{
-	int i;
-	struct net_device *dev_for_napi = NULL;
-
-	if (list_empty(&cvm_oct_list))
-		panic("No net_devices were allocated.");
-
-	dev_for_napi = list_first_entry(&cvm_oct_list,
-					struct octeon_ethernet,
-					list)->netdev;
-
-	if (max_rx_cpus >= 1  && max_rx_cpus < num_online_cpus())
-		core_state.baseline_cores = max_rx_cpus;
-	else
-		core_state.baseline_cores = num_online_cpus();
-
-	for_each_possible_cpu(i) {
-		cvm_oct_napi[i].available = 1;
-		netif_napi_add(dev_for_napi, &cvm_oct_napi[i].napi,
-			       cvm_oct_napi_poll, rx_napi_weight);
-		napi_enable(&cvm_oct_napi[i].napi);
-	}
-	/* Before interrupts are enabled, no RX processing will occur,
-	 * so we can initialize all those things out side of the
-	 * lock.
-	 */
-	spin_lock_init(&core_state.lock);
-
-	/* Register an IRQ hander for to receive POW interrupts */
-	i = request_irq(OCTEON_IRQ_WORKQ0 + pow_receive_group,
-			cvm_oct_do_interrupt, 0, dev_for_napi->name, &cvm_oct_list);
-
-	if (i)
-		panic("Could not acquire Ethernet IRQ %d\n",
-		      OCTEON_IRQ_WORKQ0 + pow_receive_group);
-
-	/* Scheduld NAPI now.  This will indirectly enable interrupts. */
-	preempt_disable();
-	cvm_oct_enable_one_cpu();
-	preempt_enable();
-}
-
-void cvm_oct_rx_shutdown(void)
-{
-	int i;
-	/* Shutdown all of the NAPIs */
-	for_each_possible_cpu(i)
-		netif_napi_del(&cvm_oct_napi[i].napi);
-
-	/* Free the interrupt handler */
-	free_irq(OCTEON_IRQ_WORKQ0 + pow_receive_group, &cvm_oct_list);
-
-}
diff --git a/drivers/staging/octeon/ethernet-sgmii.c b/drivers/staging/octeon/ethernet-sgmii.c
deleted file mode 100644
index c6768af..0000000
--- a/drivers/staging/octeon/ethernet-sgmii.c
+++ /dev/null
@@ -1,122 +0,0 @@
-/**********************************************************************
- * Author: Cavium, Inc.
- *
- * Contact: support@cavium.com
- * This file is part of the OCTEON SDK
- *
- * Copyright (c) 2003-2012 Cavium, Inc.
- *
- * This file is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, Version 2, as
- * published by the Free Software Foundation.
- *
- * This file is distributed in the hope that it will be useful, but
- * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
- * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
- * NONINFRINGEMENT.  See the GNU General Public License for more
- * details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this file; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- * or visit http://www.gnu.org/licenses/.
- *
- * This file may also be available under a different license from Cavium.
- * Contact Cavium, Inc. for more information
-**********************************************************************/
-#include <linux/phy.h>
-#include <linux/kernel.h>
-#include <linux/netdevice.h>
-#include <linux/ratelimit.h>
-#include <net/dst.h>
-
-#include <asm/octeon/octeon.h>
-
-#include "ethernet-defines.h"
-#include "octeon-ethernet.h"
-
-#include <asm/octeon/cvmx-helper.h>
-
-#include <asm/octeon/cvmx-gmxx-defs.h>
-
-/* Although these functions are called cvm_oct_sgmii_*, they also
- * happen to be used for the XAUI ports as well.
- */
-
-static void cvm_oct_sgmii_poll(struct net_device *dev)
-{
-	struct octeon_ethernet *priv = netdev_priv(dev);
-	cvmx_helper_link_info_t link_info;
-
-	link_info = cvmx_helper_link_get(priv->ipd_port);
-	if (link_info.u64 == priv->link_info)
-		return;
-
-	link_info = cvmx_helper_link_autoconf(priv->ipd_port);
-	priv->link_info = link_info.u64;
-
-	/* Tell the core */
-	cvm_oct_set_carrier(priv, link_info);
-}
-
-int cvm_oct_sgmii_open(struct net_device *dev)
-{
-	union cvmx_gmxx_prtx_cfg gmx_cfg;
-	struct octeon_ethernet *priv = netdev_priv(dev);
-	cvmx_helper_link_info_t link_info;
-	int rv;
-
-	rv = cvm_oct_phy_setup_device(dev);
-	if (rv)
-		return rv;
-
-	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
-	gmx_cfg.s.en = 1;
-	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface), gmx_cfg.u64);
-
-	if (!octeon_is_simulation()) {
-		if (priv->phydev) {
-			int r = phy_read_status(priv->phydev);
-			if (r == 0 && priv->phydev->link == 0)
-				netif_carrier_off(dev);
-			cvm_oct_adjust_link(dev);
-		} else {
-			link_info = cvmx_helper_link_get(priv->ipd_port);
-			if (!link_info.s.link_up)
-				netif_carrier_off(dev);
-			spin_lock(&priv->poll_lock);
-			priv->poll = cvm_oct_sgmii_poll;
-			spin_unlock(&priv->poll_lock);
-			cvm_oct_sgmii_poll(dev);
-		}
-	}
-	return 0;
-}
-
-int cvm_oct_sgmii_stop(struct net_device *dev)
-{
-	union cvmx_gmxx_prtx_cfg gmx_cfg;
-	struct octeon_ethernet *priv = netdev_priv(dev);
-
-	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
-	gmx_cfg.s.en = 0;
-	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface), gmx_cfg.u64);
-
-	spin_lock(&priv->poll_lock);
-	priv->poll = NULL;
-	spin_unlock(&priv->poll_lock);
-
-	if (priv->phydev)
-		phy_disconnect(priv->phydev);
-	priv->phydev = NULL;
-
-	return 0;
-}
-
-int cvm_oct_sgmii_init(struct net_device *dev)
-{
-	cvm_oct_common_init(dev);
-	dev->netdev_ops->ndo_stop(dev);
-
-	return 0;
-}
diff --git a/drivers/staging/octeon/ethernet-spi.c b/drivers/staging/octeon/ethernet-spi.c
deleted file mode 100644
index d37e53f..0000000
--- a/drivers/staging/octeon/ethernet-spi.c
+++ /dev/null
@@ -1,283 +0,0 @@
-/**********************************************************************
- * Author: Cavium, Inc.
- *
- * Contact: support@cavium.com
- * This file is part of the OCTEON SDK
- *
- * Copyright (c) 2003-2012 Cavium, Inc.
- *
- * This file is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, Version 2, as
- * published by the Free Software Foundation.
- *
- * This file is distributed in the hope that it will be useful, but
- * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
- * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
- * NONINFRINGEMENT.  See the GNU General Public License for more
- * details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this file; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- * or visit http://www.gnu.org/licenses/.
- *
- * This file may also be available under a different license from Cavium.
- * Contact Cavium, Inc. for more information
- **********************************************************************/
-#include <linux/kernel.h>
-#include <linux/netdevice.h>
-#include <linux/interrupt.h>
-#include <net/dst.h>
-
-#include <asm/octeon/octeon.h>
-
-#include "ethernet-defines.h"
-#include "octeon-ethernet.h"
-
-#include <asm/octeon/cvmx-spi.h>
-
-#include <asm/octeon/cvmx-npi-defs.h>
-#include <asm/octeon/cvmx-spxx-defs.h>
-#include <asm/octeon/cvmx-stxx-defs.h>
-
-static int number_spi_ports;
-static int need_retrain[2] = { 0, 0 };
-
-static irqreturn_t cvm_oct_spi_rml_interrupt(int cpl, void *dev_id)
-{
-	irqreturn_t return_status = IRQ_NONE;
-	union cvmx_npi_rsl_int_blocks rsl_int_blocks;
-
-	/* Check and see if this interrupt was caused by the GMX block */
-	rsl_int_blocks.u64 = cvmx_read_csr(CVMX_NPI_RSL_INT_BLOCKS);
-	if (rsl_int_blocks.s.spx1) {	/* 19 - SPX1_INT_REG & STX1_INT_REG */
-		union cvmx_spxx_int_reg spx_int_reg;
-		union cvmx_stxx_int_reg stx_int_reg;
-
-		spx_int_reg.u64 = cvmx_read_csr(CVMX_SPXX_INT_REG(1));
-		cvmx_write_csr(CVMX_SPXX_INT_REG(1), spx_int_reg.u64);
-		if (!need_retrain[1]) {
-			spx_int_reg.u64 &= cvmx_read_csr(CVMX_SPXX_INT_MSK(1));
-			if (spx_int_reg.s.spf)
-				pr_err("SPI1: SRX Spi4 interface down\n");
-			if (spx_int_reg.s.calerr)
-				pr_err("SPI1: SRX Spi4 Calendar table parity error\n");
-			if (spx_int_reg.s.syncerr)
-				pr_err("SPI1: SRX Consecutive Spi4 DIP4 errors have exceeded SPX_ERR_CTL[ERRCNT]\n");
-			if (spx_int_reg.s.diperr)
-				pr_err("SPI1: SRX Spi4 DIP4 error\n");
-			if (spx_int_reg.s.tpaovr)
-				pr_err("SPI1: SRX Selected port has hit TPA overflow\n");
-			if (spx_int_reg.s.rsverr)
-				pr_err("SPI1: SRX Spi4 reserved control word detected\n");
-			if (spx_int_reg.s.drwnng)
-				pr_err("SPI1: SRX Spi4 receive FIFO drowning/overflow\n");
-			if (spx_int_reg.s.clserr)
-				pr_err("SPI1: SRX Spi4 packet closed on non-16B alignment without EOP\n");
-			if (spx_int_reg.s.spiovr)
-				pr_err("SPI1: SRX Spi4 async FIFO overflow\n");
-			if (spx_int_reg.s.abnorm)
-				pr_err("SPI1: SRX Abnormal packet termination (ERR bit)\n");
-			if (spx_int_reg.s.prtnxa)
-				pr_err("SPI1: SRX Port out of range\n");
-		}
-
-		stx_int_reg.u64 = cvmx_read_csr(CVMX_STXX_INT_REG(1));
-		cvmx_write_csr(CVMX_STXX_INT_REG(1), stx_int_reg.u64);
-		if (!need_retrain[1]) {
-			stx_int_reg.u64 &= cvmx_read_csr(CVMX_STXX_INT_MSK(1));
-			if (stx_int_reg.s.syncerr)
-				pr_err("SPI1: STX Interface encountered a fatal error\n");
-			if (stx_int_reg.s.frmerr)
-				pr_err("SPI1: STX FRMCNT has exceeded STX_DIP_CNT[MAXFRM]\n");
-			if (stx_int_reg.s.unxfrm)
-				pr_err("SPI1: STX Unexpected framing sequence\n");
-			if (stx_int_reg.s.nosync)
-				pr_err("SPI1: STX ERRCNT has exceeded STX_DIP_CNT[MAXDIP]\n");
-			if (stx_int_reg.s.diperr)
-				pr_err("SPI1: STX DIP2 error on the Spi4 Status channel\n");
-			if (stx_int_reg.s.datovr)
-				pr_err("SPI1: STX Spi4 FIFO overflow error\n");
-			if (stx_int_reg.s.ovrbst)
-				pr_err("SPI1: STX Transmit packet burst too big\n");
-			if (stx_int_reg.s.calpar1)
-				pr_err("SPI1: STX Calendar Table Parity Error Bank1\n");
-			if (stx_int_reg.s.calpar0)
-				pr_err("SPI1: STX Calendar Table Parity Error Bank0\n");
-		}
-
-		cvmx_write_csr(CVMX_SPXX_INT_MSK(1), 0);
-		cvmx_write_csr(CVMX_STXX_INT_MSK(1), 0);
-		need_retrain[1] = 1;
-		return_status = IRQ_HANDLED;
-	}
-
-	if (rsl_int_blocks.s.spx0) {	/* 18 - SPX0_INT_REG & STX0_INT_REG */
-		union cvmx_spxx_int_reg spx_int_reg;
-		union cvmx_stxx_int_reg stx_int_reg;
-
-		spx_int_reg.u64 = cvmx_read_csr(CVMX_SPXX_INT_REG(0));
-		cvmx_write_csr(CVMX_SPXX_INT_REG(0), spx_int_reg.u64);
-		if (!need_retrain[0]) {
-			spx_int_reg.u64 &= cvmx_read_csr(CVMX_SPXX_INT_MSK(0));
-			if (spx_int_reg.s.spf)
-				pr_err("SPI0: SRX Spi4 interface down\n");
-			if (spx_int_reg.s.calerr)
-				pr_err("SPI0: SRX Spi4 Calendar table parity error\n");
-			if (spx_int_reg.s.syncerr)
-				pr_err("SPI0: SRX Consecutive Spi4 DIP4 errors have exceeded SPX_ERR_CTL[ERRCNT]\n");
-			if (spx_int_reg.s.diperr)
-				pr_err("SPI0: SRX Spi4 DIP4 error\n");
-			if (spx_int_reg.s.tpaovr)
-				pr_err("SPI0: SRX Selected port has hit TPA overflow\n");
-			if (spx_int_reg.s.rsverr)
-				pr_err("SPI0: SRX Spi4 reserved control word detected\n");
-			if (spx_int_reg.s.drwnng)
-				pr_err("SPI0: SRX Spi4 receive FIFO drowning/overflow\n");
-			if (spx_int_reg.s.clserr)
-				pr_err("SPI0: SRX Spi4 packet closed on non-16B alignment without EOP\n");
-			if (spx_int_reg.s.spiovr)
-				pr_err("SPI0: SRX Spi4 async FIFO overflow\n");
-			if (spx_int_reg.s.abnorm)
-				pr_err("SPI0: SRX Abnormal packet termination (ERR bit)\n");
-			if (spx_int_reg.s.prtnxa)
-				pr_err("SPI0: SRX Port out of range\n");
-		}
-
-		stx_int_reg.u64 = cvmx_read_csr(CVMX_STXX_INT_REG(0));
-		cvmx_write_csr(CVMX_STXX_INT_REG(0), stx_int_reg.u64);
-		if (!need_retrain[0]) {
-			stx_int_reg.u64 &= cvmx_read_csr(CVMX_STXX_INT_MSK(0));
-			if (stx_int_reg.s.syncerr)
-				pr_err("SPI0: STX Interface encountered a fatal error\n");
-			if (stx_int_reg.s.frmerr)
-				pr_err("SPI0: STX FRMCNT has exceeded STX_DIP_CNT[MAXFRM]\n");
-			if (stx_int_reg.s.unxfrm)
-				pr_err("SPI0: STX Unexpected framing sequence\n");
-			if (stx_int_reg.s.nosync)
-				pr_err("SPI0: STX ERRCNT has exceeded STX_DIP_CNT[MAXDIP]\n");
-			if (stx_int_reg.s.diperr)
-				pr_err("SPI0: STX DIP2 error on the Spi4 Status channel\n");
-			if (stx_int_reg.s.datovr)
-				pr_err("SPI0: STX Spi4 FIFO overflow error\n");
-			if (stx_int_reg.s.ovrbst)
-				pr_err("SPI0: STX Transmit packet burst too big\n");
-			if (stx_int_reg.s.calpar1)
-				pr_err("SPI0: STX Calendar Table Parity Error Bank1\n");
-			if (stx_int_reg.s.calpar0)
-				pr_err("SPI0: STX Calendar Table Parity Error Bank0\n");
-		}
-
-		cvmx_write_csr(CVMX_SPXX_INT_MSK(0), 0);
-		cvmx_write_csr(CVMX_STXX_INT_MSK(0), 0);
-		need_retrain[0] = 1;
-		return_status = IRQ_HANDLED;
-	}
-
-	return return_status;
-}
-
-static void cvm_oct_spi_enable_error_reporting(int interface)
-{
-	union cvmx_spxx_int_msk spxx_int_msk;
-	union cvmx_stxx_int_msk stxx_int_msk;
-
-	spxx_int_msk.u64 = cvmx_read_csr(CVMX_SPXX_INT_MSK(interface));
-	spxx_int_msk.s.calerr = 1;
-	spxx_int_msk.s.syncerr = 1;
-	spxx_int_msk.s.diperr = 1;
-	spxx_int_msk.s.tpaovr = 1;
-	spxx_int_msk.s.rsverr = 1;
-	spxx_int_msk.s.drwnng = 1;
-	spxx_int_msk.s.clserr = 1;
-	spxx_int_msk.s.spiovr = 1;
-	spxx_int_msk.s.abnorm = 1;
-	spxx_int_msk.s.prtnxa = 1;
-	cvmx_write_csr(CVMX_SPXX_INT_MSK(interface), spxx_int_msk.u64);
-
-	stxx_int_msk.u64 = cvmx_read_csr(CVMX_STXX_INT_MSK(interface));
-	stxx_int_msk.s.frmerr = 1;
-	stxx_int_msk.s.unxfrm = 1;
-	stxx_int_msk.s.nosync = 1;
-	stxx_int_msk.s.diperr = 1;
-	stxx_int_msk.s.datovr = 1;
-	stxx_int_msk.s.ovrbst = 1;
-	stxx_int_msk.s.calpar1 = 1;
-	stxx_int_msk.s.calpar0 = 1;
-	cvmx_write_csr(CVMX_STXX_INT_MSK(interface), stxx_int_msk.u64);
-}
-
-static void cvm_oct_spi_poll(struct net_device *dev)
-{
-	static int spi4000_port;
-	struct octeon_ethernet *priv = netdev_priv(dev);
-	int interface;
-
-	for (interface = 0; interface < 2; interface++) {
-		if ((priv->ipd_port == interface * 16) && need_retrain[interface]) {
-			if (cvmx_spi_restart_interface
-			    (interface, CVMX_SPI_MODE_DUPLEX, 10) == 0) {
-				need_retrain[interface] = 0;
-				cvm_oct_spi_enable_error_reporting(interface);
-			}
-		}
-
-		/* The SPI4000 TWSI interface is very slow. In order
-		 * not to bring the system to a crawl, we only poll a
-		 * single port every second. This means negotiation
-		 * speed changes take up to 10 seconds, but at least
-		 * we don't waste absurd amounts of time waiting for
-		 * TWSI.
-		 */
-		if (priv->ipd_port == spi4000_port) {
-			/* This function does nothing if it is called on an
-			 * interface without a SPI4000.
-			 */
-			cvmx_spi4000_check_speed(interface, priv->ipd_port);
-			/* Normal ordering increments. By decrementing
-			 * we only match once per iteration.
-			 */
-			spi4000_port--;
-			if (spi4000_port < 0)
-				spi4000_port = 10;
-		}
-	}
-}
-
-int cvm_oct_spi_init(struct net_device *dev)
-{
-	int r;
-	struct octeon_ethernet *priv = netdev_priv(dev);
-
-	if (number_spi_ports == 0) {
-		r = request_irq(OCTEON_IRQ_RML, cvm_oct_spi_rml_interrupt,
-				IRQF_SHARED, "SPI", &number_spi_ports);
-		if (r) {
-			netdev_err(dev, "request_irq(%d) failed\n",
-				   OCTEON_IRQ_RML);
-			return r;
-		}
-	}
-	number_spi_ports++;
-
-	if ((priv->ipd_port == 0) || (priv->ipd_port == 16)) {
-		cvm_oct_spi_enable_error_reporting(priv->interface);
-		priv->poll = cvm_oct_spi_poll;
-	}
-	cvm_oct_common_init(dev);
-	return 0;
-}
-
-void cvm_oct_spi_uninit(struct net_device *dev)
-{
-	int interface;
-
-	number_spi_ports--;
-	if (number_spi_ports == 0) {
-		for (interface = 0; interface < 2; interface++) {
-			cvmx_write_csr(CVMX_SPXX_INT_MSK(interface), 0);
-			cvmx_write_csr(CVMX_STXX_INT_MSK(interface), 0);
-		}
-		free_irq(OCTEON_IRQ_RML, &number_spi_ports);
-	}
-}
diff --git a/drivers/staging/octeon/ethernet-tx.c b/drivers/staging/octeon/ethernet-tx.c
deleted file mode 100644
index 95572f1..0000000
--- a/drivers/staging/octeon/ethernet-tx.c
+++ /dev/null
@@ -1,487 +0,0 @@
-/**********************************************************************
- * Author: Cavium, Inc.
- *
- * Contact: support@cavium.com
- * This file is part of the OCTEON SDK
- *
- * Copyright (c) 2003-2012 Cavium, Inc.
- *
- * This file is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, Version 2, as
- * published by the Free Software Foundation.
- *
- * This file is distributed in the hope that it will be useful, but
- * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
- * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
- * NONINFRINGEMENT.  See the GNU General Public License for more
- * details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this file; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- * or visit http://www.gnu.org/licenses/.
- *
- * This file may also be available under a different license from Cavium.
- * Contact Cavium, Inc. for more information
- **********************************************************************/
-#include <linux/module.h>
-#include <linux/kernel.h>
-#include <linux/netdevice.h>
-#include <linux/init.h>
-#include <linux/etherdevice.h>
-#include <linux/ip.h>
-#include <linux/ratelimit.h>
-#include <linux/string.h>
-#include <linux/interrupt.h>
-#include <net/dst.h>
-
-#include <asm/octeon/octeon.h>
-
-#include "ethernet-defines.h"
-#include "octeon-ethernet.h"
-
-#include <asm/octeon/cvmx-wqe.h>
-#include <asm/octeon/cvmx-fau.h>
-#include <asm/octeon/cvmx-pip.h>
-#include <asm/octeon/cvmx-pko.h>
-#include <asm/octeon/cvmx-helper.h>
-
-#include <asm/octeon/cvmx-gmxx-defs.h>
-
-/*
- * You can define GET_SKBUFF_QOS() to override how the skbuff output
- * function determines which output queue is used. The default
- * implementation always uses the base queue for the port. If, for
- * example, you wanted to use the skb->priority fieid, define
- * GET_SKBUFF_QOS as: #define GET_SKBUFF_QOS(skb) ((skb)->priority)
- */
-#ifndef GET_SKBUFF_QOS
-#define GET_SKBUFF_QOS(skb) 0
-#endif
-
-#if REUSE_SKBUFFS_WITHOUT_FREE
-static bool cvm_oct_skb_ok_for_reuse(struct sk_buff *skb)
-{
-	unsigned char *fpa_head = cvm_oct_get_fpa_head(skb);
-
-	if (unlikely(skb->data < fpa_head))
-		return false;
-
-	if (unlikely(fpa_head - skb->head < sizeof(void *)))
-		return false;
-
-	if (unlikely((skb_end_pointer(skb) - fpa_head) < CVMX_FPA_PACKET_POOL_SIZE))
-		return false;
-
-	if (unlikely(skb_shared(skb)) ||
-	    unlikely(skb_cloned(skb)) ||
-	    unlikely(skb->fclone != SKB_FCLONE_UNAVAILABLE))
-		return false;
-
-	return true;
-}
-
-static void cvm_oct_skb_prepare_for_reuse(struct sk_buff *skb)
-{
-	int r;
-	unsigned char *fpa_head = cvm_oct_get_fpa_head(skb);
-
-	skb->data_len = 0;
-	skb_frag_list_init(skb);
-
-	/* The check also resets all the fields. */
-	r = skb_recycle_check(skb, CVMX_FPA_PACKET_POOL_SIZE);
-	WARN(!r, "SKB recycle logic fail\n");
-
-	*(struct sk_buff **)(fpa_head - sizeof(void *)) = skb;
-	skb->truesize = sizeof(*skb) + skb_end_pointer(skb) - skb->head;
-}
-
-static inline void cvm_oct_set_back(struct sk_buff *skb,
-				    union cvmx_buf_ptr *hw_buffer)
-{
-	unsigned char *fpa_head = cvm_oct_get_fpa_head(skb);
-
-	hw_buffer->s.back = ((unsigned long)skb->data >> 7) - ((unsigned long)fpa_head >> 7);
-}
-#else
-static bool cvm_oct_skb_ok_for_reuse(struct sk_buff *skb)
-{
-	return false;
-}
-static void cvm_oct_skb_prepare_for_reuse(struct sk_buff *skb)
-{
-	/* Do nothing */
-}
-
-static inline void cvm_oct_set_back(struct sk_buff *skb,
-				    union cvmx_buf_ptr *hw_buffer)
-{
-	/* Do nothing. */
-}
-
-#endif
-
-/**
- * cvm_oct_xmit - transmit a packet
- * @skb:    Packet to send
- * @dev:    Device info structure
- *
- * Returns Always returns NETDEV_TX_OK
- */
-int cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev)
-{
-	struct sk_buff *skb_tmp;
-	cvmx_pko_command_word0_t pko_command;
-	union cvmx_buf_ptr hw_buffer;
-	u64 old_scratch;
-	u64 old_scratch2;
-	int qos;
-	int i;
-	int frag_count;
-	enum {QUEUE_HW, QUEUE_WQE, QUEUE_DROP} queue_type;
-	struct octeon_ethernet *priv = netdev_priv(dev);
-	s32 queue_depth;
-	s32 buffers_to_free;
-	s32 buffers_being_recycled;
-	unsigned long flags;
-	cvmx_wqe_t *work = NULL;
-	bool timestamp_this_skb = false;
-
-	/* Prefetch the private data structure.  It is larger than one
-	 * cache line.
-	 */
-	prefetch(priv);
-
-	if (USE_ASYNC_IOBDMA) {
-		/* Save scratch in case userspace is using it */
-		CVMX_SYNCIOBDMA;
-		old_scratch = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
-		old_scratch2 = cvmx_scratch_read64(CVMX_SCR_SCRATCH + 8);
-
-		/* Fetch and increment the number of packets to be
-		 * freed.
-		 */
-		cvmx_fau_async_fetch_and_add32(CVMX_SCR_SCRATCH + 8,
-					       FAU_NUM_PACKET_BUFFERS_TO_FREE,
-					       0);
-	}
-
-#ifdef CVM_OCT_LOCKLESS
-	qos = cvmx_get_core_num();
-#else
-	/* The check on CVMX_PKO_QUEUES_PER_PORT_* is designed to
-	 * completely remove "qos" in the event neither interface
-	 * supports multiple queues per port.
-	 */
-	if (priv->tx_multiple_queues) {
-		qos = GET_SKBUFF_QOS(skb);
-		if (qos <= 0)
-			qos = 0;
-		else if (qos >= priv->num_tx_queues)
-			qos = 0;
-	} else
-		qos = 0;
-#endif
-	if (USE_ASYNC_IOBDMA) {
-		cvmx_fau_async_fetch_and_add32(CVMX_SCR_SCRATCH,
-					       priv->tx_queue[qos].fau, 1);
-	}
-
-	frag_count = 0;
-	if (skb_has_frag_list(skb))
-		skb_walk_frags(skb, skb_tmp)
-			frag_count++;
-	/* We have space for 12 segment pointers, If there will be
-	 * more than that, we must linearize.  The count is: 1 (base
-	 * SKB) + frag_count + nr_frags.
-	 */
-	if (unlikely(skb_shinfo(skb)->nr_frags + frag_count > 11)) {
-		if (unlikely(__skb_linearize(skb))) {
-			queue_type = QUEUE_DROP;
-			goto skip_xmit;
-		}
-		frag_count = 0;
-	}
-
-	/* The CN3XXX series of parts has an errata (GMX-401) which
-	 * causes the GMX block to hang if a collision occurs towards
-	 * the end of a <68 byte packet. As a workaround for this, we
-	 * pad packets to be 68 bytes whenever we are in half duplex
-	 * mode. We don't handle the case of having a small packet but
-	 * no room to add the padding.  The kernel should always give
-	 * us at least a cache line
-	 */
-	if ((skb->len < 64) && OCTEON_IS_MODEL(OCTEON_CN3XXX)) {
-		union cvmx_gmxx_prtx_cfg gmx_prt_cfg;
-
-		if (priv->interface < 2) {
-			/* We only need to pad packet in half duplex mode */
-			gmx_prt_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
-			if (gmx_prt_cfg.s.duplex == 0) {
-				int add_bytes = 64 - skb->len;
-				if ((skb_tail_pointer(skb) + add_bytes) <= skb_end_pointer(skb))
-					memset(__skb_put(skb, add_bytes), 0, add_bytes);
-			}
-		}
-	}
-
-	/* Build the PKO command */
-	pko_command.u64 = 0;
-	/* Don't pollute L2 with the outgoing packet */
-	pko_command.s.n2 = 1;
-	pko_command.s.segs = 1;
-	pko_command.s.total_bytes = skb->len;
-	pko_command.s.size0 = CVMX_FAU_OP_SIZE_32;
-	pko_command.s.subone0 = 1;
-	pko_command.s.reg0 = priv->tx_queue[qos].fau;
-	pko_command.s.dontfree = 1;
-
-	/* Build the PKO buffer pointer */
-	hw_buffer.u64 = 0; /* Implies pool == 0, i == 0 */
-	if (skb_shinfo(skb)->nr_frags == 0 && frag_count == 0) {
-		hw_buffer.s.addr = virt_to_phys(skb->data);
-		hw_buffer.s.size = skb->len;
-		cvm_oct_set_back(skb, &hw_buffer);
-		buffers_being_recycled = 1;
-	} else {
-		u64 *hw_buffer_list;
-		bool can_do_reuse = true;
-
-		work = cvmx_fpa_alloc(CVMX_FPA_WQE_POOL);
-		if (unlikely(!work)) {
-			netdev_err(dev, "Failed WQE allocate\n");
-			queue_type = QUEUE_DROP;
-			goto skip_xmit;
-		}
-		hw_buffer_list = (u64 *)work->packet_data;
-		hw_buffer.s.addr = virt_to_phys(skb->data);
-		hw_buffer.s.size = skb_headlen(skb);
-		if (skb_shinfo(skb)->nr_frags == 0 && cvm_oct_skb_ok_for_reuse(skb)) {
-			cvm_oct_set_back(skb, &hw_buffer);
-		} else {
-			hw_buffer.s.back = 0;
-			can_do_reuse = false;
-		}
-		hw_buffer_list[0] = hw_buffer.u64;
-		for (i = 1; i <= skb_shinfo(skb)->nr_frags; i++) {
-			struct skb_frag_struct *fs = skb_shinfo(skb)->frags + i - 1;
-			hw_buffer.s.addr = virt_to_phys((u8 *)page_address(fs->page.p) + fs->page_offset);
-			hw_buffer.s.size = fs->size;
-			hw_buffer_list[i] = hw_buffer.u64;
-			can_do_reuse = false;
-		}
-		skb_walk_frags(skb, skb_tmp) {
-			hw_buffer.s.addr = virt_to_phys(skb_tmp->data);
-			hw_buffer.s.size = skb_tmp->len;
-			if (cvm_oct_skb_ok_for_reuse(skb_tmp)) {
-				cvm_oct_set_back(skb_tmp, &hw_buffer);
-			} else {
-				hw_buffer.s.back = 0;
-				can_do_reuse = false;
-			}
-			hw_buffer_list[i] = hw_buffer.u64;
-			i++;
-		}
-		hw_buffer.s.addr = virt_to_phys(hw_buffer_list);
-		hw_buffer.s.size = i;
-		hw_buffer.s.back = 0;
-		hw_buffer.s.pool = CVMX_FPA_WQE_POOL;
-		buffers_being_recycled = i;
-		pko_command.s.segs = hw_buffer.s.size;
-		pko_command.s.gather = 1;
-		if (!can_do_reuse)
-			goto dont_put_skbuff_in_hw;
-	}
-
-	if (unlikely(priv->tx_timestamp_hw &&
-		     (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP))) {
-		timestamp_this_skb = true;
-		skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
-		goto dont_put_skbuff_in_hw;
-	}
-	/* See if we can put this skb in the FPA pool. Any strange
-	 * behavior from the Linux networking stack will most likely
-	 * be caused by a bug in the following code. If some field is
-	 * in use by the network stack and get carried over when a
-	 * buffer is reused, bad thing may happen.  If in doubt and
-	 * you dont need the absolute best performance, disable the
-	 * define REUSE_SKBUFFS_WITHOUT_FREE. The reuse of buffers has
-	 * shown a 25% increase in performance under some loads.
-	 */
-#if REUSE_SKBUFFS_WITHOUT_FREE
-	if (!cvm_oct_skb_ok_for_reuse(skb))
-		goto dont_put_skbuff_in_hw;
-	if (unlikely(skb_header_cloned(skb)))
-		goto dont_put_skbuff_in_hw;
-	if (unlikely(skb->destructor))
-		goto dont_put_skbuff_in_hw;
-
-
-	/* We can use this buffer in the FPA.  We don't need the FAU
-	 * update anymore
-	 */
-	pko_command.s.dontfree = 0;
-
-#endif /* REUSE_SKBUFFS_WITHOUT_FREE */
-
-dont_put_skbuff_in_hw:
-
-	/* Check if we can use the hardware checksumming */
-	if (USE_HW_TCPUDP_CHECKSUM && (skb->protocol == htons(ETH_P_IP)) &&
-	    (ip_hdr(skb)->version == 4) && (ip_hdr(skb)->ihl == 5) &&
-	    ((ip_hdr(skb)->frag_off == 0) || (ip_hdr(skb)->frag_off == htons(1 << 14)))
-	    && ((ip_hdr(skb)->protocol == IPPROTO_TCP) || (ip_hdr(skb)->protocol == IPPROTO_UDP))) {
-		/* Use hardware checksum calc */
-		pko_command.s.ipoffp1 = sizeof(struct ethhdr) + 1;
-		if (unlikely(priv->imode == CVMX_HELPER_INTERFACE_MODE_SRIO))
-			pko_command.s.ipoffp1 += 8;
-	}
-
-	if (USE_ASYNC_IOBDMA) {
-		/* Get the number of skbuffs in use by the hardware */
-		CVMX_SYNCIOBDMA;
-		queue_depth = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
-		buffers_to_free = cvmx_scratch_read64(CVMX_SCR_SCRATCH + 8);
-	} else {
-		/* Get the number of skbuffs in use by the hardware */
-		queue_depth = cvmx_fau_fetch_and_add32(priv->tx_queue[qos].fau, 1);
-		buffers_to_free = cvmx_fau_fetch_and_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);
-	}
-
-	/* If we're sending faster than the receive can free them then
-	 * don't do the HW free.
-	 */
-	if (unlikely(buffers_to_free < -100))
-		pko_command.s.dontfree = 1;
-
-	/* Drop this packet if we have too many already queued to the HW */
-	if (unlikely(queue_depth >= MAX_OUT_QUEUE_DEPTH)) {
-		if (dev->tx_queue_len != 0) {
-			netif_stop_queue(dev);
-		} else {
-			/* If not using normal queueing.  */
-			queue_type = QUEUE_DROP;
-			goto skip_xmit;
-		}
-	}
-
-	if (pko_command.s.dontfree) {
-		queue_type = QUEUE_WQE;
-	} else {
-		queue_type = QUEUE_HW;
-		if (buffers_being_recycled > 1) {
-			struct sk_buff *tskb, *nskb;
-			/* We are committed to use hardware free, restore the
-			 * frag list to empty on the first SKB
-			 */
-			tskb = skb_shinfo(skb)->frag_list;
-			while (tskb) {
-				nskb = tskb->next;
-				cvm_oct_skb_prepare_for_reuse(tskb);
-				tskb = nskb;
-			}
-		}
-		cvm_oct_skb_prepare_for_reuse(skb);
-	}
-
-	if (queue_type == QUEUE_WQE) {
-		if (!work) {
-			work = cvmx_fpa_alloc(CVMX_FPA_WQE_POOL);
-			if (unlikely(!work)) {
-				netdev_err(dev, "Failed WQE allocate\n");
-				queue_type = QUEUE_DROP;
-				goto skip_xmit;
-			}
-		}
-
-		pko_command.s.rsp = 1;
-		pko_command.s.wqp = 1;
-		/* work->unused will carry the qos for this packet,
-		 * this allows us to find the proper FAU when freeing
-		 * the packet.  We decrement the FAU when the WQE is
-		 * replaced in the pool.
-		 */
-		pko_command.s.reg0 = 0;
-		work->word0.u64 = 0;
-		work->word0.raw.unused = (u8)qos;
-
-		work->word1.u64 = 0;
-		work->word1.tag_type = CVMX_POW_TAG_TYPE_NULL;
-		work->word1.tag = 0;
-		work->word2.u64 = 0;
-		work->word2.s.software = 1;
-		cvmx_wqe_set_grp(work, pow_receive_group);
-		work->packet_ptr.u64 = (unsigned long)skb;
-	}
-
-	local_irq_save(flags);
-
-	cvmx_pko_send_packet_prepare_pkoid(priv->pko_port,
-					   priv->tx_queue[qos].queue,
-					   CVMX_PKO_LOCK_CMD_QUEUE);
-
-	/* Send the packet to the output queue */
-	if (queue_type == QUEUE_WQE) {
-		u64 word2 = virt_to_phys(work);
-		if (timestamp_this_skb)
-			word2 |= 1ull << 40; /* Bit 40 controls timestamps */
-
-		if (unlikely(cvmx_pko_send_packet_finish3_pkoid(priv->pko_port,
-							  priv->tx_queue[qos].queue, pko_command, hw_buffer,
-							  word2, CVMX_PKO_LOCK_CMD_QUEUE))) {
-				queue_type = QUEUE_DROP;
-				cvmx_fpa_free(work, CVMX_FPA_WQE_POOL, DONT_WRITEBACK(1));
-				netdev_err(dev, "Failed to send the packet with wqe\n");
-		}
-	} else {
-		if (unlikely(cvmx_pko_send_packet_finish_pkoid(priv->pko_port,
-							 priv->tx_queue[qos].queue,
-							 pko_command, hw_buffer,
-							 CVMX_PKO_LOCK_CMD_QUEUE))) {
-			netdev_err(dev, "Failed to send the packet\n");
-			queue_type = QUEUE_DROP;
-		}
-	}
-	local_irq_restore(flags);
-
-skip_xmit:
-	switch (queue_type) {
-	case QUEUE_DROP:
-		cvmx_fau_atomic_add32(priv->tx_queue[qos].fau, -1);
-		dev_kfree_skb_any(skb);
-		dev->stats.tx_dropped++;
-		break;
-	case QUEUE_HW:
-		cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, -buffers_being_recycled);
-		break;
-	case QUEUE_WQE:
-		/* Cleanup is done on the RX path when the WQE returns */
-		break;
-	default:
-		BUG();
-	}
-
-	if (USE_ASYNC_IOBDMA) {
-		CVMX_SYNCIOBDMA;
-		/* Restore the scratch area */
-		cvmx_scratch_write64(CVMX_SCR_SCRATCH, old_scratch);
-		cvmx_scratch_write64(CVMX_SCR_SCRATCH + 8, old_scratch2);
-	}
-
-	return NETDEV_TX_OK;
-}
-
-void cvm_oct_tx_shutdown_dev(struct net_device *dev)
-{
-}
-
-void cvm_oct_tx_initialize(void)
-{
-
-}
-
-void cvm_oct_tx_shutdown(void)
-{
-
-}
diff --git a/drivers/staging/octeon/ethernet.c b/drivers/staging/octeon/ethernet.c
deleted file mode 100644
index 51ad641..0000000
--- a/drivers/staging/octeon/ethernet.c
+++ /dev/null
@@ -1,825 +0,0 @@
-/**********************************************************************
- * Author: Cavium, Inc.
- *
- * Contact: support@cavium.com
- * This file is part of the OCTEON SDK
- *
- * Copyright (c) 2003-2012 Cavium, Inc.
- *
- * This file is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, Version 2, as
- * published by the Free Software Foundation.
- *
- * This file is distributed in the hope that it will be useful, but
- * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
- * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
- * NONINFRINGEMENT.  See the GNU General Public License for more
- * details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this file; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- * or visit http://www.gnu.org/licenses/.
- *
- * This file may also be available under a different license from Cavium.
- * Contact Cavium, Inc. for more information
- **********************************************************************/
-#include <linux/platform_device.h>
-#include <linux/kernel.h>
-#include <linux/init.h>
-#include <linux/module.h>
-#include <linux/netdevice.h>
-#include <linux/etherdevice.h>
-#include <linux/phy.h>
-#include <linux/slab.h>
-#include <linux/of_net.h>
-#include <linux/interrupt.h>
-
-#include <net/dst.h>
-
-#include <asm/octeon/octeon.h>
-
-#include "ethernet-defines.h"
-#include "octeon-ethernet.h"
-
-#include <asm/octeon/cvmx-pip.h>
-#include <asm/octeon/cvmx-pko.h>
-#include <asm/octeon/cvmx-fau.h>
-#include <asm/octeon/cvmx-ipd.h>
-#include <asm/octeon/cvmx-helper.h>
-
-#include <asm/octeon/cvmx-gmxx-defs.h>
-#include <asm/octeon/cvmx-smix-defs.h>
-
-#if defined(CONFIG_CAVIUM_OCTEON_NUM_PACKET_BUFFERS) \
-	&& CONFIG_CAVIUM_OCTEON_NUM_PACKET_BUFFERS
-int num_packet_buffers = CONFIG_CAVIUM_OCTEON_NUM_PACKET_BUFFERS;
-#else
-int num_packet_buffers = 1024;
-#endif
-module_param(num_packet_buffers, int, 0444);
-MODULE_PARM_DESC(num_packet_buffers, "\n"
-	"\tNumber of packet buffers to allocate and store in the\n"
-	"\tFPA. By default, 1024 packet buffers are used unless\n"
-	"\tCONFIG_CAVIUM_OCTEON_NUM_PACKET_BUFFERS is defined.");
-
-int pow_receive_group = 15;
-module_param(pow_receive_group, int, 0444);
-MODULE_PARM_DESC(pow_receive_group, "\n"
-	"\tPOW group to receive packets from. All ethernet hardware\n"
-	"\twill be configured to send incomming packets to this POW\n"
-	"\tgroup. Also any other software can submit packets to this\n"
-	"\tgroup for the kernel to process.");
-
-int max_rx_cpus = -1;
-module_param(max_rx_cpus, int, 0444);
-MODULE_PARM_DESC(max_rx_cpus, "\n"
-	"\t\tThe maximum number of CPUs to use for packet reception.\n"
-	"\t\tUse -1 to use all available CPUs.");
-
-int rx_napi_weight = 32;
-module_param(rx_napi_weight, int, 0444);
-MODULE_PARM_DESC(rx_napi_weight, "The NAPI WEIGHT parameter.");
-
-/**
- * cvm_oct_poll_queue - Workqueue for polling operations.
- */
-struct workqueue_struct *cvm_oct_poll_queue;
-
-/**
- * cvm_oct_poll_queue_stopping - flag to indicate polling should stop.
- *
- * Set to one right before cvm_oct_poll_queue is destroyed.
- */
-atomic_t cvm_oct_poll_queue_stopping = ATOMIC_INIT(0);
-
-/* cvm_oct_by_port is an array of every ethernet device owned by this
- * driver indexed by the ipd input port number.
- */
-struct octeon_ethernet *cvm_oct_by_port[TOTAL_NUMBER_OF_PORTS] __cacheline_aligned;
-
-/* cvm_oct_list is a list of all cvm_oct_private_t created by this driver. */
-LIST_HEAD(cvm_oct_list);
-
-static void cvm_oct_rx_refill_worker(struct work_struct *work);
-static DECLARE_DELAYED_WORK(cvm_oct_rx_refill_work, cvm_oct_rx_refill_worker);
-
-static void cvm_oct_rx_refill_worker(struct work_struct *work)
-{
-	/* FPA 0 may have been drained, try to refill it if we need
-	 * more than num_packet_buffers / 2, otherwise normal receive
-	 * processing will refill it.  If it were drained, no packets
-	 * could be received so cvm_oct_napi_poll would never be
-	 * invoked to do the refill.
-	 */
-	cvm_oct_rx_refill_pool(num_packet_buffers / 2);
-
-	if (!atomic_read(&cvm_oct_poll_queue_stopping))
-		queue_delayed_work(cvm_oct_poll_queue,
-				   &cvm_oct_rx_refill_work, HZ);
-}
-
-static void cvm_oct_periodic_worker(struct work_struct *work)
-{
-	struct octeon_ethernet *priv = container_of(work,
-						    struct octeon_ethernet,
-						    port_periodic_work.work);
-	void (*poll_fn) (struct net_device *);
-
-	spin_lock(&priv->poll_lock);
-	poll_fn = priv->poll;
-	spin_unlock(&priv->poll_lock);
-
-	if (poll_fn)
-		poll_fn(priv->netdev);
-
-	priv->netdev->netdev_ops->ndo_get_stats(priv->netdev);
-
-	if (!atomic_read(&cvm_oct_poll_queue_stopping))
-		queue_delayed_work(cvm_oct_poll_queue, &priv->port_periodic_work, HZ);
-}
-
-static int cvm_oct_num_output_buffers;
-
-static __devinit int cvm_oct_configure_common_hw(void)
-{
-	/* Setup the FPA */
-	cvmx_fpa_enable();
-
-	if (cvm_oct_alloc_fpa_pool(CVMX_FPA_PACKET_POOL,
-				   CVMX_FPA_PACKET_POOL_SIZE) < 0) {
-		pr_err("cvm_oct_alloc_fpa_pool(CVMX_FPA_PACKET_POOL, CVMX_FPA_PACKET_POOL_SIZE) failed.\n");
-		return -ENOMEM;
-	}
-	cvm_oct_mem_fill_fpa(CVMX_FPA_PACKET_POOL, num_packet_buffers);
-
-	if (cvm_oct_alloc_fpa_pool(CVMX_FPA_WQE_POOL,
-				   CVMX_FPA_WQE_POOL_SIZE) < 0) {
-		pr_err("cvm_oct_alloc_fpa_pool(CVMX_FPA_WQE_POOL, CVMX_FPA_WQE_POOL_SIZE) failed.\n");
-		return -ENOMEM;;
-	}
-	cvm_oct_mem_fill_fpa(CVMX_FPA_WQE_POOL, num_packet_buffers);
-
-	if (CVMX_FPA_OUTPUT_BUFFER_POOL != CVMX_FPA_PACKET_POOL) {
-		cvm_oct_num_output_buffers = 128;
-		if (cvm_oct_alloc_fpa_pool(CVMX_FPA_OUTPUT_BUFFER_POOL,
-					   CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE) < 0) {
-			pr_err("cvm_oct_alloc_fpa_pool(CVMX_FPA_OUTPUT_BUFFER_POOL, CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE) failed.\n");
-			return -ENOMEM;;
-		}
-		cvm_oct_mem_fill_fpa(CVMX_FPA_OUTPUT_BUFFER_POOL,
-				     cvm_oct_num_output_buffers);
-	}
-
-	if (USE_RED)
-		cvmx_helper_setup_red(num_packet_buffers / 4,
-				      num_packet_buffers / 8);
-
-	return 0;
-}
-
-/**
- * cvm_oct_free_work- Free a work queue entry
- *
- * @work_queue_entry: Work queue entry to free
- *
- * Returns Zero on success, Negative on failure.
- */
-int cvm_oct_free_work(void *work_queue_entry)
-{
-	cvmx_wqe_t *work = work_queue_entry;
-
-	int segments = work->word2.s.bufs;
-	union cvmx_buf_ptr segment_ptr = work->packet_ptr;
-
-	while (segments--) {
-		union cvmx_buf_ptr next_ptr = *(union cvmx_buf_ptr *)phys_to_virt(segment_ptr.s.addr - 8);
-		if (!segment_ptr.s.i)
-			cvmx_fpa_free(cvm_oct_get_buffer_ptr(segment_ptr),
-				      segment_ptr.s.pool,
-				      DONT_WRITEBACK(CVMX_FPA_PACKET_POOL_SIZE / 128));
-		segment_ptr = next_ptr;
-	}
-	cvmx_fpa_free(work, CVMX_FPA_WQE_POOL, DONT_WRITEBACK(1));
-
-	return 0;
-}
-EXPORT_SYMBOL(cvm_oct_free_work);
-
-/* Lock to protect racy cvmx_pko_get_port_status() */
-static DEFINE_SPINLOCK(cvm_oct_tx_stat_lock);
-
-/**
- * cvm_oct_common_get_stats - get the low level ethernet statistics
- * @dev:    Device to get the statistics from
- *
- * Returns Pointer to the statistics
- */
-static struct net_device_stats *cvm_oct_common_get_stats(struct net_device *dev)
-{
-	unsigned long flags;
-	cvmx_pip_port_status_t rx_status;
-	cvmx_pko_port_status_t tx_status;
-	u64 current_tx_octets;
-	u32 current_tx_packets;
-	struct octeon_ethernet *priv = netdev_priv(dev);
-
-	if (octeon_is_simulation()) {
-		/* The simulator doesn't support statistics */
-		memset(&rx_status, 0, sizeof(rx_status));
-		memset(&tx_status, 0, sizeof(tx_status));
-	} else {
-		cvmx_pip_get_port_status(priv->ipd_port, 1, &rx_status);
-
-		spin_lock_irqsave(&cvm_oct_tx_stat_lock, flags);
-		cvmx_pko_get_port_status(priv->ipd_port, 0, &tx_status);
-		current_tx_packets = tx_status.packets;
-		current_tx_octets = tx_status.octets;
-		/* The tx_packets counter is 32-bits as are all these
-		 * variables.  No truncation necessary.
-		 */
-		tx_status.packets = current_tx_packets - priv->last_tx_packets;
-		/* The tx_octets counter is only 48-bits, so we need
-		 * to truncate in case there was a wrap-around
-		 */
-		tx_status.octets = (current_tx_octets - priv->last_tx_octets) & 0xffffffffffffull;
-		priv->last_tx_packets = current_tx_packets;
-		priv->last_tx_octets = current_tx_octets;
-		spin_unlock_irqrestore(&cvm_oct_tx_stat_lock, flags);
-	}
-
-	dev->stats.rx_packets += rx_status.inb_packets;
-	dev->stats.tx_packets += tx_status.packets;
-	dev->stats.rx_bytes += rx_status.inb_octets;
-	dev->stats.tx_bytes += tx_status.octets;
-	dev->stats.multicast += rx_status.multicast_packets;
-	dev->stats.rx_crc_errors += rx_status.inb_errors;
-	dev->stats.rx_frame_errors += rx_status.fcs_align_err_packets;
-
-	/* The drop counter must be incremented atomically since the
-	 * RX tasklet also increments it.
-	 */
-	atomic64_add(rx_status.dropped_packets,
-		     (atomic64_t *)&dev->stats.rx_dropped);
-
-	return &dev->stats;
-}
-
-/**
- * cvm_oct_common_change_mtu - change the link MTU
- * @dev:     Device to change
- * @new_mtu: The new MTU
- *
- * Returns Zero on success
- */
-static int cvm_oct_common_change_mtu(struct net_device *dev, int new_mtu)
-{
-	struct octeon_ethernet *priv = netdev_priv(dev);
-#if IS_ENABLED(CONFIG_VLAN_8021Q)
-	int vlan_bytes = 4;
-#else
-	int vlan_bytes = 0;
-#endif
-
-	/* Limit the MTU to make sure the ethernet packets are between
-	 * 64 bytes and 65535 bytes.
-	 */
-	if ((new_mtu + 14 + 4 + vlan_bytes < 64)
-	    || (new_mtu + 14 + 4 + vlan_bytes > 65392)) {
-		netdev_err(dev, "MTU must be between %d and %d.\n",
-			   64 - 14 - 4 - vlan_bytes,
-			   65392 - 14 - 4 - vlan_bytes);
-		return -EINVAL;
-	}
-	dev->mtu = new_mtu;
-
-	if ((priv->interface < 2)
-	    && (cvmx_helper_interface_get_mode(priv->interface) !=
-		CVMX_HELPER_INTERFACE_MODE_SPI)) {
-		/* Add ethernet header and FCS, and VLAN if configured. */
-		int max_packet = new_mtu + 14 + 4 + vlan_bytes;
-
-		if (OCTEON_IS_MODEL(OCTEON_CN3XXX)
-		    || OCTEON_IS_MODEL(OCTEON_CN58XX)) {
-			/* Signal errors on packets larger than the MTU */
-			cvmx_write_csr(CVMX_GMXX_RXX_FRM_MAX(priv->interface_port, priv->interface),
-				       max_packet);
-		} else {
-			union cvmx_pip_prt_cfgx port_cfg;
-			int offset = cvmx_helper_get_ipd_port(priv->interface, priv->interface_port);
-
-			port_cfg.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(offset));
-			if (port_cfg.s.maxerr_en) {
-				/* Disable the PIP check as it can
-				 * only be controlled over a group of
-				 * ports, let the check be done in the
-				 * GMX instead.
-				 */
-				port_cfg.s.maxerr_en = 0;
-				cvmx_write_csr(CVMX_PIP_PRT_CFGX(offset), port_cfg.u64);
-			}
-		}
-		/* Set the hardware to truncate packets larger than
-		 * the MTU. The jabber register must be set to a
-		 * multiple of 8 bytes, so round up.
-		 */
-		cvmx_write_csr(CVMX_GMXX_RXX_JABBER(priv->interface_port, priv->interface),
-			       (max_packet + 7) & ~7u);
-	}
-	return 0;
-}
-
-/**
- * cvm_oct_common_set_multicast_list - set the multicast list
- * @dev:    Device to work on
- */
-static void cvm_oct_common_set_multicast_list(struct net_device *dev)
-{
-	union cvmx_gmxx_prtx_cfg gmx_cfg;
-	struct octeon_ethernet *priv = netdev_priv(dev);
-
-	if ((priv->interface < 2)
-	    && (cvmx_helper_interface_get_mode(priv->interface) !=
-		CVMX_HELPER_INTERFACE_MODE_SPI)) {
-		union cvmx_gmxx_rxx_adr_ctl control;
-		control.u64 = 0;
-		control.s.bcst = 1;	/* Allow broadcast MAC addresses */
-
-		if (!netdev_mc_empty(dev) || (dev->flags & IFF_ALLMULTI) ||
-		    (dev->flags & IFF_PROMISC))
-			/* Force accept multicast packets */
-			control.s.mcst = 2;
-		else
-			/* Force reject multicat packets */
-			control.s.mcst = 1;
-
-		if (dev->flags & IFF_PROMISC)
-			/* Reject matches if promisc. Since CAM is
-			 * shut off, should accept everything.
-			 */
-			control.s.cam_mode = 0;
-		else
-			/* Filter packets based on the CAM */
-			control.s.cam_mode = 1;
-
-		gmx_cfg.u64 =
-		    cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
-		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface),
-			       gmx_cfg.u64 & ~1ull);
-
-		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CTL(priv->interface_port, priv->interface),
-			       control.u64);
-		if (dev->flags & IFF_PROMISC)
-			cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM_EN(priv->interface_port, priv->interface), 0);
-		else
-			cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM_EN(priv->interface_port, priv->interface), 1);
-
-		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface),
-			       gmx_cfg.u64);
-	}
-}
-
-/**
- * cvm_oct_common_set_mac_address - set the hardware MAC address for a device
- * @dev:    The device in question.
- * @addr:   Address structure to change it too.
- *
- * Returns Zero on success
- */
-static int cvm_oct_set_mac_filter(struct net_device *dev)
-{
-	struct octeon_ethernet *priv = netdev_priv(dev);
-	union cvmx_gmxx_prtx_cfg gmx_cfg;
-
-	if ((priv->interface < 2)
-	    && (cvmx_helper_interface_get_mode(priv->interface) !=
-		CVMX_HELPER_INTERFACE_MODE_SPI)) {
-		int i;
-		u8 *ptr = dev->dev_addr;
-		u64 mac = 0;
-		for (i = 0; i < 6; i++)
-			mac = (mac << 8) | (u64)ptr[i];
-
-		gmx_cfg.u64 =
-		    cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
-		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface),
-			       gmx_cfg.u64 & ~1ull);
-
-		cvmx_write_csr(CVMX_GMXX_SMACX(priv->interface_port, priv->interface), mac);
-		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM0(priv->interface_port, priv->interface),
-			       ptr[0]);
-		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM1(priv->interface_port, priv->interface),
-			       ptr[1]);
-		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM2(priv->interface_port, priv->interface),
-			       ptr[2]);
-		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM3(priv->interface_port, priv->interface),
-			       ptr[3]);
-		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM4(priv->interface_port, priv->interface),
-			       ptr[4]);
-		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM5(priv->interface_port, priv->interface),
-			       ptr[5]);
-		cvm_oct_common_set_multicast_list(dev);
-		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface),
-			       gmx_cfg.u64);
-	}
-	return 0;
-}
-
-static int cvm_oct_common_set_mac_address(struct net_device *dev, void *addr)
-{
-	int r = eth_mac_addr(dev, addr);
-
-	if (r)
-		return r;
-	return cvm_oct_set_mac_filter(dev);
-}
-
-/**
- * cvm_oct_common_init - per network device initialization
- * @dev:    Device to initialize
- *
- * Returns Zero on success
- */
-int cvm_oct_common_init(struct net_device *dev)
-{
-	unsigned long flags;
-	cvmx_pko_port_status_t tx_status;
-	struct octeon_ethernet *priv = netdev_priv(dev);
-	const u8 *mac = NULL;
-
-	if (priv->of_node)
-		mac = of_get_mac_address(priv->of_node);
-
-	if (mac && is_valid_ether_addr(mac)) {
-		memcpy(dev->dev_addr, mac, ETH_ALEN);
-		dev->addr_assign_type &= ~NET_ADDR_RANDOM;
-	} else {
-		eth_hw_addr_random(dev);
-	}
-
-	if (priv->num_tx_queues != -1) {
-		dev->features |= NETIF_F_SG | NETIF_F_FRAGLIST;
-		if (USE_HW_TCPUDP_CHECKSUM)
-			dev->features |= NETIF_F_IP_CSUM;
-	}
-
-	/* We do our own locking, Linux doesn't need to */
-	dev->features |= NETIF_F_LLTX;
-	SET_ETHTOOL_OPS(dev, &cvm_oct_ethtool_ops);
-
-	cvm_oct_set_mac_filter(dev);
-	dev->netdev_ops->ndo_change_mtu(dev, dev->mtu);
-
-	spin_lock_irqsave(&cvm_oct_tx_stat_lock, flags);
-	cvmx_pko_get_port_status(priv->ipd_port, 0, &tx_status);
-	priv->last_tx_packets = tx_status.packets;
-	priv->last_tx_octets = tx_status.octets;
-	/* Zero out stats for port so we won't mistakenly show
-	 * counters from the bootloader.
-	 */
-	memset(&dev->stats, 0, sizeof(struct net_device_stats));
-	spin_unlock_irqrestore(&cvm_oct_tx_stat_lock, flags);
-
-	return 0;
-}
-
-static const struct net_device_ops cvm_oct_npi_netdev_ops = {
-	.ndo_init		= cvm_oct_common_init,
-	.ndo_start_xmit		= cvm_oct_xmit,
-	.ndo_set_rx_mode	= cvm_oct_common_set_multicast_list,
-	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
-	.ndo_do_ioctl		= cvm_oct_ioctl,
-	.ndo_change_mtu		= cvm_oct_common_change_mtu,
-	.ndo_get_stats		= cvm_oct_common_get_stats,
-#ifdef CONFIG_NET_POLL_CONTROLLER
-	.ndo_poll_controller	= cvm_oct_poll_controller,
-#endif
-};
-
-/* SGMII and XAUI handled the same so they both use this. */
-static const struct net_device_ops cvm_oct_sgmii_netdev_ops = {
-	.ndo_init		= cvm_oct_sgmii_init,
-	.ndo_open		= cvm_oct_sgmii_open,
-	.ndo_stop		= cvm_oct_sgmii_stop,
-	.ndo_start_xmit		= cvm_oct_xmit,
-	.ndo_set_rx_mode	= cvm_oct_common_set_multicast_list,
-	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
-	.ndo_do_ioctl		= cvm_oct_ioctl,
-	.ndo_change_mtu		= cvm_oct_common_change_mtu,
-	.ndo_get_stats		= cvm_oct_common_get_stats,
-#ifdef CONFIG_NET_POLL_CONTROLLER
-	.ndo_poll_controller	= cvm_oct_poll_controller,
-#endif
-};
-static const struct net_device_ops cvm_oct_spi_netdev_ops = {
-	.ndo_init		= cvm_oct_spi_init,
-	.ndo_uninit		= cvm_oct_spi_uninit,
-	.ndo_start_xmit		= cvm_oct_xmit,
-	.ndo_set_rx_mode	= cvm_oct_common_set_multicast_list,
-	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
-	.ndo_do_ioctl		= cvm_oct_ioctl,
-	.ndo_change_mtu		= cvm_oct_common_change_mtu,
-	.ndo_get_stats		= cvm_oct_common_get_stats,
-#ifdef CONFIG_NET_POLL_CONTROLLER
-	.ndo_poll_controller	= cvm_oct_poll_controller,
-#endif
-};
-static const struct net_device_ops cvm_oct_rgmii_netdev_ops = {
-	.ndo_init		= cvm_oct_rgmii_init,
-	.ndo_open		= cvm_oct_rgmii_open,
-	.ndo_stop		= cvm_oct_rgmii_stop,
-	.ndo_start_xmit		= cvm_oct_xmit,
-	.ndo_set_rx_mode	= cvm_oct_common_set_multicast_list,
-	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
-	.ndo_do_ioctl		= cvm_oct_ioctl,
-	.ndo_change_mtu		= cvm_oct_common_change_mtu,
-	.ndo_get_stats		= cvm_oct_common_get_stats,
-#ifdef CONFIG_NET_POLL_CONTROLLER
-	.ndo_poll_controller	= cvm_oct_poll_controller,
-#endif
-};
-
-extern void octeon_mdiobus_force_mod_depencency(void);
-
-static int num_devices_extra_wqe;
-#define PER_DEVICE_EXTRA_WQE (MAX_OUT_QUEUE_DEPTH)
-
-static struct device_node * __devinit cvm_oct_of_get_child(const struct device_node *parent,
-							   int reg_val)
-{
-	struct device_node *node = NULL;
-	int size;
-	const __be32 *addr;
-
-	for (;;) {
-		node = of_get_next_child(parent, node);
-		if (!node)
-			break;
-		addr = of_get_property(node, "reg", &size);
-		if (addr && (be32_to_cpu(*addr) == reg_val))
-			break;
-	}
-	return node;
-}
-
-static struct device_node * __devinit cvm_oct_node_for_port(struct device_node *pip,
-							    int interface, int port)
-{
-	struct device_node *ni, *np;
-
-	ni = cvm_oct_of_get_child(pip, interface);
-	if (!ni)
-		return NULL;
-
-	np = cvm_oct_of_get_child(ni, port);
-	of_node_put(ni);
-
-	return np;
-}
-
-static int __devinit cvm_oct_probe(struct platform_device *pdev)
-{
-	int num_interfaces;
-	int interface;
-	int fau = FAU_NUM_PACKET_BUFFERS_TO_FREE;
-	int qos, r;
-	struct device_node *pip;
-
-	octeon_mdiobus_force_mod_depencency();
-	pr_notice("octeon-ethernet %s\n", OCTEON_ETHERNET_VERSION);
-
-	pip = pdev->dev.of_node;
-	if (!pip) {
-		dev_err(&pdev->dev, "No of_node.\n");
-		return -EINVAL;
-	}
-
-	cvm_oct_poll_queue = create_singlethread_workqueue("octeon-ethernet");
-	if (cvm_oct_poll_queue == NULL) {
-		dev_err(&pdev->dev, "Cannot create workqueue");
-		return -ENOMEM;
-	}
-
-	r = cvm_oct_configure_common_hw();
-	if (r)
-		return r;
-
-	cvmx_helper_initialize_packet_io_global();
-
-	/* Change the input group for all ports before input is enabled */
-	num_interfaces = cvmx_helper_get_number_of_interfaces();
-	for (interface = 0; interface < num_interfaces; interface++) {
-		int num_ports = cvmx_helper_ports_on_interface(interface);
-		int port;
-
-		for (port = cvmx_helper_get_ipd_port(interface, 0);
-		     port < cvmx_helper_get_ipd_port(interface, num_ports);
-		     port++) {
-			union cvmx_pip_prt_tagx pip_prt_tagx;
-			pip_prt_tagx.u64 = cvmx_read_csr(CVMX_PIP_PRT_TAGX(port));
-			pip_prt_tagx.s.grp = pow_receive_group;
-			cvmx_write_csr(CVMX_PIP_PRT_TAGX(port), pip_prt_tagx.u64);
-		}
-	}
-
-	cvmx_helper_ipd_and_packet_input_enable();
-
-	/* Initialize the FAU used for counting packet buffers that
-	 * need to be freed.
-	 */
-	cvmx_fau_atomic_write32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);
-
-	num_interfaces = cvmx_helper_get_number_of_interfaces();
-	for (interface = 0; interface < num_interfaces; interface++) {
-		cvmx_helper_interface_mode_t imode = cvmx_helper_interface_get_mode(interface);
-		int num_ports = cvmx_helper_ports_on_interface(interface);
-		int interface_port;
-
-		for (interface_port = 0; interface_port < num_ports;
-		     interface_port++) {
-			struct octeon_ethernet *priv;
-			int base_queue;
-			struct net_device *dev =
-			    alloc_etherdev(sizeof(struct octeon_ethernet));
-			if (!dev) {
-				dev_err(&pdev->dev,
-					"Failed to allocate ethernet device for port %d:%d\n",
-					interface, interface_port);
-				continue;
-			}
-
-			/* Initialize the device private structure. */
-			priv = netdev_priv(dev);
-			priv->of_node = cvm_oct_node_for_port(pip, interface, interface_port);
-			priv->netdev = dev;
-			priv->interface = interface;
-			priv->interface_port = interface_port;
-			spin_lock_init(&priv->poll_lock);
-			INIT_DELAYED_WORK(&priv->port_periodic_work,
-					  cvm_oct_periodic_worker);
-			priv->imode = imode;
-			priv->ipd_port = cvmx_helper_get_ipd_port(interface, interface_port);
-			priv->pko_port = cvmx_helper_get_pko_port(interface, interface_port);
-			base_queue = cvmx_pko_get_base_queue(priv->ipd_port);
-			priv->num_tx_queues = cvmx_pko_get_num_queues(priv->ipd_port);
-
-			BUG_ON(priv->num_tx_queues < 1);
-			BUG_ON(priv->num_tx_queues > 32);
-
-			for (qos = 0; qos < priv->num_tx_queues; qos++) {
-				priv->tx_queue[qos].queue = base_queue + qos;
-				fau = fau - sizeof(u32);
-				priv->tx_queue[qos].fau = fau;
-				cvmx_fau_atomic_write32(priv->tx_queue[qos].fau, 0);
-			}
-
-			/* Cache the fact that there may be multiple queues */
-			priv->tx_multiple_queues =
-				(CVMX_PKO_QUEUES_PER_PORT_INTERFACE0 > 1) ||
-				(CVMX_PKO_QUEUES_PER_PORT_INTERFACE1 > 1);
-
-			switch (priv->imode) {
-
-			/* These types don't support ports to IPD/PKO */
-			case CVMX_HELPER_INTERFACE_MODE_DISABLED:
-			case CVMX_HELPER_INTERFACE_MODE_PCIE:
-			case CVMX_HELPER_INTERFACE_MODE_PICMG:
-				break;
-
-			case CVMX_HELPER_INTERFACE_MODE_NPI:
-				dev->netdev_ops = &cvm_oct_npi_netdev_ops;
-				strcpy(dev->name, "npi%d");
-				break;
-
-			case CVMX_HELPER_INTERFACE_MODE_XAUI:
-				dev->netdev_ops = &cvm_oct_sgmii_netdev_ops;
-				strcpy(dev->name, "xaui%d");
-				break;
-
-			case CVMX_HELPER_INTERFACE_MODE_LOOP:
-				dev->netdev_ops = &cvm_oct_npi_netdev_ops;
-				strcpy(dev->name, "loop%d");
-				break;
-
-			case CVMX_HELPER_INTERFACE_MODE_SGMII:
-				dev->netdev_ops = &cvm_oct_sgmii_netdev_ops;
-				strcpy(dev->name, "eth%d");
-				break;
-
-			case CVMX_HELPER_INTERFACE_MODE_SPI:
-				dev->netdev_ops = &cvm_oct_spi_netdev_ops;
-				strcpy(dev->name, "spi%d");
-				break;
-
-			case CVMX_HELPER_INTERFACE_MODE_RGMII:
-			case CVMX_HELPER_INTERFACE_MODE_GMII:
-				dev->netdev_ops = &cvm_oct_rgmii_netdev_ops;
-				strcpy(dev->name, "eth%d");
-				break;
-			}
-
-			netif_carrier_off(dev);
-			if (!dev->netdev_ops) {
-				free_netdev(dev);
-			} else if (register_netdev(dev) < 0) {
-				dev_err(&pdev->dev,
-					"Failed to register ethernet device for interface %d, port %d\n",
-					interface, priv->ipd_port);
-				free_netdev(dev);
-			} else {
-				list_add_tail(&priv->list, &cvm_oct_list);
-				cvm_oct_by_port[priv->ipd_port] = priv;
-				/* Each transmit queue will need its
-				 * own MAX_OUT_QUEUE_DEPTH worth of
-				 * WQE to track the transmit skbs.
-				 */
-				cvm_oct_mem_fill_fpa(CVMX_FPA_WQE_POOL,
-						     PER_DEVICE_EXTRA_WQE);
-				num_devices_extra_wqe++;
-				queue_delayed_work(cvm_oct_poll_queue,
-						   &priv->port_periodic_work, HZ);
-			}
-		}
-	}
-
-	cvm_oct_tx_initialize();
-	cvm_oct_rx_initialize();
-
-	queue_delayed_work(cvm_oct_poll_queue, &cvm_oct_rx_refill_work, HZ);
-
-	return 0;
-}
-
-static int __devexit cvm_oct_remove(struct platform_device *pdev)
-{
-	struct octeon_ethernet *priv;
-	struct octeon_ethernet *tmp;
-
-	/* Disable POW interrupt */
-	cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group), 0);
-
-	cvmx_ipd_disable();
-
-	atomic_inc_return(&cvm_oct_poll_queue_stopping);
-	cancel_delayed_work_sync(&cvm_oct_rx_refill_work);
-
-	cvm_oct_rx_shutdown();
-	cvm_oct_tx_shutdown();
-
-	cvmx_pko_disable();
-
-	/* Free the ethernet devices */
-	list_for_each_entry_safe_reverse(priv, tmp, &cvm_oct_list, list) {
-		list_del(&priv->list);
-		cancel_delayed_work_sync(&priv->port_periodic_work);
-		cvm_oct_tx_shutdown_dev(priv->netdev);
-		unregister_netdev(priv->netdev);
-		cvm_oct_by_port[priv->ipd_port] = NULL;
-		free_netdev(priv->netdev);
-	}
-
-	cvmx_helper_shutdown_packet_io_global();
-
-	destroy_workqueue(cvm_oct_poll_queue);
-
-	/* Free the HW pools */
-	cvm_oct_mem_empty_fpa(CVMX_FPA_PACKET_POOL, num_packet_buffers);
-	cvm_oct_release_fpa_pool(CVMX_FPA_PACKET_POOL);
-
-	cvm_oct_mem_empty_fpa(CVMX_FPA_WQE_POOL,
-			      num_packet_buffers + num_devices_extra_wqe * PER_DEVICE_EXTRA_WQE);
-	cvm_oct_release_fpa_pool(CVMX_FPA_WQE_POOL);
-
-	if (CVMX_FPA_OUTPUT_BUFFER_POOL != CVMX_FPA_PACKET_POOL) {
-		cvm_oct_mem_empty_fpa(CVMX_FPA_OUTPUT_BUFFER_POOL,
-				      cvm_oct_num_output_buffers);
-		cvm_oct_release_fpa_pool(CVMX_FPA_OUTPUT_BUFFER_POOL);
-	}
-	cvm_oct_mem_cleanup();
-
-	return 0;
-}
-
-static struct of_device_id cvm_oct_match[] = {
-	{
-		.compatible = "cavium,octeon-3860-pip",
-	},
-	{},
-};
-MODULE_DEVICE_TABLE(of, cvm_oct_match);
-
-static struct platform_driver cvm_oct_driver = {
-	.probe		= cvm_oct_probe,
-	.remove		= __devexit_p(cvm_oct_remove),
-	.driver		= {
-		.owner	= THIS_MODULE,
-		.name	= KBUILD_MODNAME,
-		.of_match_table = cvm_oct_match,
-	},
-};
-
-module_platform_driver(cvm_oct_driver);
-
-MODULE_LICENSE("GPL");
-MODULE_AUTHOR("Cavium Networks <support@caviumnetworks.com>");
-MODULE_DESCRIPTION("Cavium Networks Octeon ethernet driver.");
diff --git a/drivers/staging/octeon/octeon-ethernet.h b/drivers/staging/octeon/octeon-ethernet.h
deleted file mode 100644
index 8358e91..0000000
--- a/drivers/staging/octeon/octeon-ethernet.h
+++ /dev/null
@@ -1,189 +0,0 @@
-/**********************************************************************
- * Author: Cavium, Inc.
- *
- * Contact: support@cavium.com
- * This file is part of the OCTEON SDK
- *
- * Copyright (c) 2003-2012 Cavium, Inc.
- *
- * This file is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, Version 2, as
- * published by the Free Software Foundation.
- *
- * This file is distributed in the hope that it will be useful, but
- * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
- * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
- * NONINFRINGEMENT.  See the GNU General Public License for more
- * details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this file; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- * or visit http://www.gnu.org/licenses/.
- *
- * This file may also be available under a different license from Cavium.
- * Contact Cavium, Inc. for more information
- **********************************************************************/
-
-/*
- * External interface for the Cavium Octeon ethernet driver.
- */
-#ifndef OCTEON_ETHERNET_H
-#define OCTEON_ETHERNET_H
-
-#include <linux/of.h>
-
-#include <asm/octeon/cvmx-helper.h>
-#include <asm/octeon/cvmx-fau.h>
-
-/**
- * This is the definition of the Ethernet driver's private
- * driver state stored in netdev_priv(dev).
- */
-struct octeon_ethernet {
-	int ipd_port;
-	int pko_port;
-	int interface;
-	int interface_port;
-
-	/* My netdev. */
-	struct net_device *netdev;
-	/* My location in the cvm_oct_list */
-	struct list_head list;
-
-	/* Type of port. This is one of the enums in
-	 * cvmx_helper_interface_mode_t
-	 */
-	int imode;
-
-	unsigned int tx_timestamp_hw:1;
-	unsigned int rx_timestamp_hw:1;
-	unsigned int tx_multiple_queues:1;
-
-	/* Number of elements in tx_queue below */
-	int                     num_tx_queues;
-
-	struct {
-		/* PKO hardware queue for the port */
-		int	queue;
-		/* Hardware fetch and add to count outstanding tx buffers */
-		int	fau;
-	} tx_queue[32];
-
-	struct phy_device *phydev;
-	unsigned int last_link;
-	/* Last negotiated link state */
-	u64 link_info;
-	/* Called periodically to check link status */
-	spinlock_t poll_lock;
-	void (*poll) (struct net_device *dev);
-	struct delayed_work	port_periodic_work;
-	struct work_struct	port_work;	/* may be unused. */
-	struct device_node	*of_node;
-	u64 last_tx_octets;
-	u32 last_tx_packets;
-};
-
-int cvm_oct_free_work(void *work_queue_entry);
-
-int cvm_oct_rgmii_init(struct net_device *dev);
-int cvm_oct_rgmii_open(struct net_device *dev);
-int cvm_oct_rgmii_stop(struct net_device *dev);
-
-int cvm_oct_sgmii_init(struct net_device *dev);
-int cvm_oct_sgmii_open(struct net_device *dev);
-int cvm_oct_sgmii_stop(struct net_device *dev);
-
-int cvm_oct_spi_init(struct net_device *dev);
-void cvm_oct_spi_uninit(struct net_device *dev);
-
-int cvm_oct_xaui_init(struct net_device *dev);
-int cvm_oct_xaui_open(struct net_device *dev);
-int cvm_oct_xaui_stop(struct net_device *dev);
-
-int cvm_oct_common_init(struct net_device *dev);
-
-void cvm_oct_set_carrier(struct octeon_ethernet *priv,
-				cvmx_helper_link_info_t link_info);
-void cvm_oct_adjust_link(struct net_device *dev);
-
-int cvm_oct_ioctl(struct net_device *dev, struct ifreq *rq, int cmd);
-int cvm_oct_phy_setup_device(struct net_device *dev);
-
-int cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev);
-int cvm_oct_transmit_qos(struct net_device *dev, void *work_queue_entry,
-			 int do_free, int qos);
-void cvm_oct_tx_initialize(void);
-void cvm_oct_tx_shutdown(void);
-void cvm_oct_tx_shutdown_dev(struct net_device *dev);
-
-void cvm_oct_poll_controller(struct net_device *dev);
-void cvm_oct_rx_initialize(void);
-void cvm_oct_rx_shutdown(void);
-
-int cvm_oct_mem_fill_fpa(int pool, int elements);
-int cvm_oct_mem_empty_fpa(int pool, int elements);
-int cvm_oct_alloc_fpa_pool(int pool, int size);
-int cvm_oct_release_fpa_pool(int pool);
-void cvm_oct_mem_cleanup(void);
-
-extern const struct ethtool_ops cvm_oct_ethtool_ops;
-
-extern int always_use_pow;
-extern int pow_send_group;
-extern int pow_receive_group;
-extern char pow_send_list[];
-extern struct list_head cvm_oct_list;
-extern struct octeon_ethernet *cvm_oct_by_port[];
-
-extern struct workqueue_struct *cvm_oct_poll_queue;
-extern atomic_t cvm_oct_poll_queue_stopping;
-
-extern int max_rx_cpus;
-extern int rx_napi_weight;
-
-static inline void cvm_oct_rx_refill_pool(int fill_threshold)
-{
-	int number_to_free;
-	int num_freed;
-	/* Refill the packet buffer pool */
-	number_to_free =
-		cvmx_fau_fetch_and_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);
-
-	if (number_to_free > fill_threshold) {
-		cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE,
-				      -number_to_free);
-		num_freed = cvm_oct_mem_fill_fpa(CVMX_FPA_PACKET_POOL,
-						 number_to_free);
-		if (num_freed != number_to_free) {
-			cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE,
-					number_to_free - num_freed);
-		}
-	}
-}
-
-/**
- * cvm_oct_get_buffer_ptr - convert packet data address to pointer
- * @pd: Packet data hardware address
- *
- * Returns Packet buffer pointer
- */
-static inline void *cvm_oct_get_buffer_ptr(union cvmx_buf_ptr pd)
-{
-	return phys_to_virt(((pd.s.addr >> 7) - pd.s.back) << 7);
-}
-
-static inline struct sk_buff **cvm_oct_packet_to_skb(void *packet)
-{
-	char *p = packet;
-	return (struct sk_buff **)(p - sizeof(void *));
-}
-
-#define CVM_OCT_SKB_TO_FPA_PADDING (128 + sizeof(void *) - 1)
-
-static inline u8 *cvm_oct_get_fpa_head(struct sk_buff *skb)
-{
-	return (u8 *)((unsigned long)(skb->head + CVM_OCT_SKB_TO_FPA_PADDING) & ~0x7ful);
-}
-
-#endif
-- 
1.7.5.4

