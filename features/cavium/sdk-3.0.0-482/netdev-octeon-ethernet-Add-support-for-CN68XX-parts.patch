From 01b5054a59c4fb7917534c9d0b80344e89791388 Mon Sep 17 00:00:00 2001
From: David Daney <david.daney@cavium.com>
Date: Fri, 24 Aug 2012 14:37:13 -0700
Subject: [PATCH 145/337] netdev: octeon-ethernet: Add support for CN68XX
 parts.

Based On SDK 3.0.0-482

Input port numbering is changed as is the initialization of the
Synchronization Scheduling and Ordering (SSO) unit.  To optimize the
NAPI polling function, we factor it in to a separate file that is
included twice with different behavior selected via preprocessor
symbols.

Signed-off-by: David Daney <david.daney@cavium.com>
Signed-off-by: Corey Minyard <cminyard@mvista.com>
Signed-off-by: Jack Tan <jack.tan@windriver.com>
---
 drivers/net/ethernet/octeon/ethernet-napi.c   |  409 ++++++++++++++++++
 drivers/net/ethernet/octeon/ethernet-rgmii.c  |    4 +-
 drivers/net/ethernet/octeon/ethernet-rx.c     |  567 ++++++++++---------------
 drivers/net/ethernet/octeon/ethernet-tx.c     |   14 -
 drivers/net/ethernet/octeon/ethernet.c        |  103 ++++-
 drivers/net/ethernet/octeon/octeon-ethernet.h |   16 +-
 6 files changed, 730 insertions(+), 383 deletions(-)
 create mode 100644 drivers/net/ethernet/octeon/ethernet-napi.c

diff --git a/drivers/net/ethernet/octeon/ethernet-napi.c b/drivers/net/ethernet/octeon/ethernet-napi.c
new file mode 100644
index 0000000..62ec472
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-napi.c
@@ -0,0 +1,409 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+
+/* This file is included in ethernet-rx.c *twice* */
+
+#undef CVM_OCT_NAPI_POLL
+#undef CVM_OCT_NAPI_HAS_CN68XX_SSO
+
+#ifdef CVM_OCT_NAPI_68
+#define CVM_OCT_NAPI_POLL cvm_oct_napi_poll_68
+#define CVM_OCT_NAPI_HAS_CN68XX_SSO 1
+#else
+#define CVM_OCT_NAPI_POLL cvm_oct_napi_poll_38
+#define CVM_OCT_NAPI_HAS_CN68XX_SSO 0
+#endif
+
+/**
+ * cvm_oct_napi_poll - the NAPI poll function.
+ * @napi: The NAPI instance, or null if called from cvm_oct_poll_controller
+ * @budget: Maximum number of packets to receive.
+ *
+ * Returns the number of packets processed.
+ */
+static int CVM_OCT_NAPI_POLL(struct napi_struct *napi, int budget)
+{
+	const int	coreid = cvmx_get_core_num();
+	int		no_work_count = 0;
+	u64		old_group_mask;
+	u64		old_scratch;
+	int		rx_count = 0;
+	bool		did_work_request = false;
+	bool		packet_copied;
+
+	char		*p = (char *)cvm_oct_by_pkind;
+	/* Prefetch cvm_oct_device since we know we need it soon */
+	prefetch(&p[0]);
+	prefetch(&p[SMP_CACHE_BYTES]);
+	prefetch(&p[2 * SMP_CACHE_BYTES]);
+
+	if (USE_ASYNC_IOBDMA) {
+		/* Save scratch in case userspace is using it */
+		CVMX_SYNCIOBDMA;
+		old_scratch = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
+	}
+
+	/* Only allow work for our group (and preserve priorities) */
+	if (CVM_OCT_NAPI_HAS_CN68XX_SSO) {
+		old_group_mask = cvmx_read_csr(CVMX_SSO_PPX_GRP_MSK(coreid));
+		cvmx_write_csr(CVMX_SSO_PPX_GRP_MSK(coreid),
+			       1ull << pow_receive_group);
+		/* Read it back so it takes effect before we request work */
+		cvmx_read_csr(CVMX_SSO_PPX_GRP_MSK(coreid));
+	} else {
+		old_group_mask = cvmx_read_csr(CVMX_POW_PP_GRP_MSKX(coreid));
+		cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(coreid),
+			       (old_group_mask & ~0xFFFFull) | 1 << pow_receive_group);
+	}
+
+	if (USE_ASYNC_IOBDMA) {
+		cvmx_pow_work_request_async(CVMX_SCR_SCRATCH, CVMX_POW_NO_WAIT);
+		did_work_request = true;
+	}
+
+	while (rx_count < budget) {
+		struct sk_buff *skb = NULL;
+		struct sk_buff **pskb = NULL;
+		struct octeon_ethernet *priv;
+		bool skb_in_hw;
+		cvmx_wqe_t *work;
+		int port;
+		unsigned int segments;
+		int packets_to_replace = 0;
+		unsigned int packet_len;
+
+		union cvmx_buf_ptr  packet_ptr;
+
+		if (USE_ASYNC_IOBDMA && did_work_request)
+			work = cvmx_pow_work_response_async(CVMX_SCR_SCRATCH);
+		else
+			work = cvmx_pow_work_request_sync(CVMX_POW_NO_WAIT);
+
+		prefetch(work);
+		did_work_request = false;
+		if (unlikely(work == NULL)) {
+			/* It takes so long to get here, so lets wait
+			 * around a little to see if another packet
+			 * comes in.
+			 */
+			if (no_work_count < 2)
+				break;
+			no_work_count++;
+			ndelay(500);
+			continue;
+		}
+		packet_ptr = work->packet_ptr;
+		pskb = cvm_oct_packet_to_skb(cvm_oct_get_buffer_ptr(packet_ptr));
+		prefetch(pskb);
+
+		if (USE_ASYNC_IOBDMA && rx_count < (budget - 1)) {
+			cvmx_pow_work_request_async_nocheck(CVMX_SCR_SCRATCH, CVMX_POW_NO_WAIT);
+			did_work_request = true;
+		}
+
+		if (rx_count == 0) {
+			/* First time through, see if there is enough
+			 * work waiting to merit waking another
+			 * CPU.
+			 */
+			int backlog;
+			int cores_in_use = core_state.active_cores;
+			if (CVM_OCT_NAPI_HAS_CN68XX_SSO) {
+				union cvmx_sso_wq_int_cntx counts;
+				counts.u64 = cvmx_read_csr(CVMX_SSO_WQ_INT_CNTX(pow_receive_group));
+				backlog = counts.s.iq_cnt + counts.s.ds_cnt;
+			} else {
+				union cvmx_pow_wq_int_cntx counts;
+				counts.u64 = cvmx_read_csr(CVMX_POW_WQ_INT_CNTX(pow_receive_group));
+				backlog = counts.s.iq_cnt + counts.s.ds_cnt;
+			}
+			if (backlog > 16 * cores_in_use &&
+			    napi != NULL &&
+			    cores_in_use < core_state.baseline_cores)
+				cvm_oct_enable_one_cpu();
+		}
+
+		/* If WORD2[SOFTWARE] then this WQE is a complete for
+		 * a TX packet.
+		 */
+		if (work->word2.s.software) {
+			struct octeon_ethernet *priv;
+			int packet_qos = work->word0.raw.unused;
+
+			skb = (struct sk_buff *)packet_ptr.u64;
+			priv = netdev_priv(skb->dev);
+			if (!netif_running(skb->dev))
+				netif_wake_queue(skb->dev);
+			if (unlikely((skb_shinfo(skb)->tx_flags | SKBTX_IN_PROGRESS) != 0 &&
+				     priv->tx_timestamp_hw)) {
+					u64 ns = *(u64 *)work->packet_data;
+					struct skb_shared_hwtstamps ts;
+					ts.syststamp = cvm_oct_ptp_to_ktime(ns);
+					ts.hwtstamp = ns_to_ktime(ns);
+					skb_tstamp_tx(skb, &ts);
+			}
+			dev_kfree_skb_any(skb);
+
+			cvmx_fpa_free(work, CVMX_FPA_WQE_POOL, DONT_WRITEBACK(1));
+
+			/* We are done with this one, adjust the queue
+			 * depth.
+			 */
+			cvmx_fau_atomic_add32(priv->tx_queue[packet_qos].fau, -1);
+			continue;
+		}
+		segments = work->word2.s.bufs;
+		skb_in_hw = USE_SKBUFFS_IN_HW && segments > 0;
+		if (likely(skb_in_hw)) {
+			skb = *pskb;
+			prefetch(&skb->head);
+			prefetch(&skb->len);
+		}
+
+		if (CVM_OCT_NAPI_HAS_CN68XX_SSO)
+			port = work->word0.pip.cn68xx.pknd;
+		else
+			port = work->word1.cn38xx.ipprt;
+
+		prefetch(cvm_oct_by_pkind[port]);
+
+		/* Immediately throw away all packets with receive errors */
+		if (unlikely(work->word2.snoip.rcv_error)) {
+			if (cvm_oct_check_rcv_error(work))
+				continue;
+		}
+
+		if (CVM_OCT_NAPI_HAS_CN68XX_SSO) {
+			if (unlikely(cvm_oct_by_pkind[port] == NULL))
+				priv = cvm_oct_dev_for_port(work->word2.s_cn68xx.port);
+			else
+				priv = cvm_oct_by_pkind[port];
+		} else {
+			priv = cvm_oct_by_pkind[port];
+		}
+
+		packet_len = work->word1.len;
+		/* We can only use the zero copy path if skbuffs are
+		 * in the FPA pool and the packet fits in a single
+		 * buffer.
+		 */
+		if (likely(skb_in_hw)) {
+			skb->data = phys_to_virt(packet_ptr.s.addr);
+			prefetch(skb->data);
+			skb->len = packet_len;
+			packets_to_replace = segments;
+			if (likely(segments == 1)) {
+				skb_set_tail_pointer(skb, skb->len);
+			} else {
+				struct sk_buff *current_skb = skb;
+				struct sk_buff *next_skb = NULL;
+				unsigned int segment_size;
+				bool first_frag = true;
+
+				skb_frag_list_init(skb);
+				/* Multi-segment packet. */
+				for (;;) {
+					/* Octeon Errata PKI-100: The segment size is
+					 * wrong. Until it is fixed, calculate the
+					 * segment size based on the packet pool
+					 * buffer size. When it is fixed, the
+					 * following line should be replaced with this
+					 * one: int segment_size =
+					 * segment_ptr.s.size;
+					 */
+					segment_size = CVMX_FPA_PACKET_POOL_SIZE -
+						(packet_ptr.s.addr - (((packet_ptr.s.addr >> 7) - packet_ptr.s.back) << 7));
+					if (segment_size > packet_len)
+						segment_size = packet_len;
+					if (!first_frag) {
+						current_skb->len = segment_size;
+						skb->data_len += segment_size;
+						skb->truesize += current_skb->truesize;
+					}
+					skb_set_tail_pointer(current_skb, segment_size);
+					packet_len -= segment_size;
+					segments--;
+					if (segments == 0)
+						break;
+					packet_ptr = *(union cvmx_buf_ptr *)phys_to_virt(packet_ptr.s.addr - 8);
+					next_skb = *cvm_oct_packet_to_skb(cvm_oct_get_buffer_ptr(packet_ptr));
+					if (first_frag) {
+						skb_frag_add_head(current_skb, next_skb);
+					} else {
+						current_skb->next = next_skb;
+						next_skb->next = NULL;
+					}
+					current_skb = next_skb;
+					first_frag = false;
+					current_skb->data = phys_to_virt(packet_ptr.s.addr);
+				}
+			}
+			packet_copied = false;
+		} else {
+			/* We have to copy the packet. First allocate
+			 * an skbuff for it.
+			 */
+			skb = dev_alloc_skb(packet_len);
+			if (!skb) {
+				printk_ratelimited("Port %d failed to allocate skbuff, packet dropped\n",
+						   port);
+				cvm_oct_free_work(work);
+				continue;
+			}
+
+			/* Check if we've received a packet that was
+			 * entirely stored in the work entry.
+			 */
+			if (unlikely(work->word2.s.bufs == 0)) {
+				u8 *ptr = work->packet_data;
+
+				if (likely(!work->word2.s.not_IP)) {
+					/* The beginning of the packet
+					 * moves for IP packets.
+					 */
+					if (work->word2.s.is_v6)
+						ptr += 2;
+					else
+						ptr += 6;
+				}
+				memcpy(skb_put(skb, packet_len), ptr, packet_len);
+				/* No packet buffers to free */
+			} else {
+				int segments = work->word2.s.bufs;
+				union cvmx_buf_ptr segment_ptr = work->packet_ptr;
+
+				while (segments--) {
+					union cvmx_buf_ptr next_ptr =
+					    *(union cvmx_buf_ptr *)phys_to_virt(segment_ptr.s.addr - 8);
+
+			/* Octeon Errata PKI-100: The segment size is
+			 * wrong. Until it is fixed, calculate the
+			 * segment size based on the packet pool
+			 * buffer size. When it is fixed, the
+			 * following line should be replaced with this
+			 * one: int segment_size =
+			 * segment_ptr.s.size;
+			 */
+					int segment_size = CVMX_FPA_PACKET_POOL_SIZE -
+						(segment_ptr.s.addr - (((segment_ptr.s.addr >> 7) - segment_ptr.s.back) << 7));
+					/* Don't copy more than what
+					 * is left in the packet.
+					 */
+					if (segment_size > packet_len)
+						segment_size = packet_len;
+					/* Copy the data into the packet */
+					memcpy(skb_put(skb, segment_size),
+					       phys_to_virt(segment_ptr.s.addr),
+					       segment_size);
+					packet_len -= segment_size;
+					segment_ptr = next_ptr;
+				}
+			}
+			packet_copied = true;
+		}
+
+		if (likely(priv)) {
+			/* Only accept packets for devices that are
+			 * currently up.
+			 */
+			if (likely(priv->netdev->flags & IFF_UP)) {
+				if (priv->rx_timestamp_hw) {
+					/* The first 8 bytes are the timestamp */
+					u64 ns = *(u64 *)skb->data;
+					struct skb_shared_hwtstamps *ts;
+					ts = skb_hwtstamps(skb);
+					ts->hwtstamp = ns_to_ktime(ns);
+					ts->syststamp = cvm_oct_ptp_to_ktime(ns);
+					__skb_pull(skb, 8);
+				}
+				skb->protocol = eth_type_trans(skb, priv->netdev);
+				skb->dev = priv->netdev;
+
+				if (unlikely(work->word2.s.not_IP || work->word2.s.IP_exc ||
+					work->word2.s.L4_error || !work->word2.s.tcp_or_udp))
+					skb->ip_summed = CHECKSUM_NONE;
+				else
+					skb->ip_summed = CHECKSUM_UNNECESSARY;
+
+				/* Increment RX stats for virtual ports */
+				if (port >= CVMX_PIP_NUM_INPUT_PORTS) {
+					atomic64_add(1, (atomic64_t *)&priv->netdev->stats.rx_packets);
+					atomic64_add(skb->len, (atomic64_t *)&priv->netdev->stats.rx_bytes);
+				}
+				netif_receive_skb(skb);
+				rx_count++;
+			} else {
+				/* Drop any packet received for a device that isn't up */
+				atomic64_add(1, (atomic64_t *)&priv->netdev->stats.rx_dropped);
+				dev_kfree_skb_any(skb);
+			}
+		} else {
+			/* Drop any packet received for a device that
+			 * doesn't exist.
+			 */
+			printk_ratelimited("Port %d not controlled by Linux, packet dropped\n",
+					   port);
+			dev_kfree_skb_any(skb);
+		}
+		/* Check to see if the skbuff and work share the same
+		 * packet buffer.
+		 */
+		if (USE_SKBUFFS_IN_HW && likely(!packet_copied)) {
+			/* This buffer needs to be replaced, increment
+			 * the number of buffers we need to free by
+			 * one.
+			 */
+			cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE,
+					      packets_to_replace);
+
+			cvmx_fpa_free(work, CVMX_FPA_WQE_POOL,
+				      DONT_WRITEBACK(1));
+		} else {
+			cvm_oct_free_work(work);
+		}
+	}
+	/* Restore the original POW group mask */
+	if (CVM_OCT_NAPI_HAS_CN68XX_SSO) {
+		cvmx_write_csr(CVMX_SSO_PPX_GRP_MSK(coreid), old_group_mask);
+		/* Read it back so it takes effect before ?? */
+		cvmx_read_csr(CVMX_SSO_PPX_GRP_MSK(coreid));
+	} else {
+		cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(coreid), old_group_mask);
+	}
+	if (USE_ASYNC_IOBDMA) {
+		/* Restore the scratch area */
+		cvmx_scratch_write64(CVMX_SCR_SCRATCH, old_scratch);
+	}
+	cvm_oct_rx_refill_pool(0);
+
+	if (rx_count < budget && napi != NULL) {
+		/* No more work */
+		napi_complete(napi);
+		cvm_oct_no_more_work(napi);
+	}
+	return rx_count;
+}
diff --git a/drivers/net/ethernet/octeon/ethernet-rgmii.c b/drivers/net/ethernet/octeon/ethernet-rgmii.c
index 2db614f..ccce09b 100644
--- a/drivers/net/ethernet/octeon/ethernet-rgmii.c
+++ b/drivers/net/ethernet/octeon/ethernet-rgmii.c
@@ -177,7 +177,7 @@ static irqreturn_t cvm_oct_rgmii_rml_interrupt(int cpl, void *dev_id)
 			if (gmx_rx_int_reg.s.phy_dupx
 			    || gmx_rx_int_reg.s.phy_link
 			    || gmx_rx_int_reg.s.phy_spd) {
-				struct octeon_ethernet *priv = cvm_oct_by_port[cvmx_helper_get_ipd_port(interface, index)];
+				struct octeon_ethernet *priv = cvm_oct_dev_for_port(cvmx_helper_get_ipd_port(interface, index));
 
 				if (priv && !atomic_read(&cvm_oct_poll_queue_stopping))
 					queue_work(cvm_oct_poll_queue, &priv->port_work);
@@ -210,7 +210,7 @@ static irqreturn_t cvm_oct_rgmii_rml_interrupt(int cpl, void *dev_id)
 			if (gmx_rx_int_reg.s.phy_dupx
 			    || gmx_rx_int_reg.s.phy_link
 			    || gmx_rx_int_reg.s.phy_spd) {
-				struct octeon_ethernet *priv = cvm_oct_by_port[cvmx_helper_get_ipd_port(interface, index)];
+				struct octeon_ethernet *priv = cvm_oct_dev_for_port(cvmx_helper_get_ipd_port(interface, index));
 
 				if (priv && !atomic_read(&cvm_oct_poll_queue_stopping))
 					queue_work(cvm_oct_poll_queue, &priv->port_work);
diff --git a/drivers/net/ethernet/octeon/ethernet-rx.c b/drivers/net/ethernet/octeon/ethernet-rx.c
index 950cac0..a6dd083 100644
--- a/drivers/net/ethernet/octeon/ethernet-rx.c
+++ b/drivers/net/ethernet/octeon/ethernet-rx.c
@@ -56,6 +56,7 @@
 #include <asm/octeon/cvmx-scratch.h>
 
 #include <asm/octeon/cvmx-gmxx-defs.h>
+#include <asm/octeon/cvmx-sso-defs.h>
 
 struct cvm_napi_wrapper {
 	struct napi_struct napi;
@@ -137,15 +138,28 @@ static void cvm_oct_no_more_work(struct napi_struct *napi)
 		 * we can start processing again when there is
 		 * something to do.
 		 */
-		union cvmx_pow_wq_int_thrx int_thr;
-		int_thr.u64 = 0;
-		int_thr.s.iq_thr = 1;
-		int_thr.s.ds_thr = 1;
-		/* Enable POW interrupt when our port has at
-		 * least one packet.
-		 */
-		cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group),
-			       int_thr.u64);
+		if (OCTEON_IS_MODEL(OCTEON_CN68XX)) {
+			union cvmx_sso_wq_int_thrx int_thr;
+			int_thr.u64 = 0;
+			int_thr.s.iq_thr = 1;
+			int_thr.s.ds_thr = 1;
+			/*
+			 * Enable SSO interrupt when our port has at
+			 * least one packet.
+			 */
+			cvmx_write_csr(CVMX_SSO_WQ_INT_THRX(pow_receive_group),
+				       int_thr.u64);
+		} else {
+			union cvmx_pow_wq_int_thrx int_thr;
+			int_thr.u64 = 0;
+			int_thr.s.iq_thr = 1;
+			int_thr.s.ds_thr = 1;
+			/* Enable POW interrupt when our port has at
+			 * least one packet.
+			 */
+			cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group),
+				       int_thr.u64);
+		}
 	}
 }
 
@@ -159,15 +173,20 @@ static irqreturn_t cvm_oct_do_interrupt(int cpl, void *dev_id)
 {
 	int cpu = smp_processor_id();
 	unsigned long flags;
-	union cvmx_pow_wq_int wq_int;
 
 	/* Disable the IRQ and start napi_poll. */
+	if (OCTEON_IS_MODEL(OCTEON_CN68XX)) {
+		cvmx_write_csr(CVMX_SSO_WQ_INT_THRX(pow_receive_group), 0);
+		cvmx_write_csr(CVMX_SSO_WQ_INT, 1ULL << pow_receive_group);
+	} else {
+		union cvmx_pow_wq_int wq_int;
 
-	cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group), 0);
+		cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group), 0);
 
-	wq_int.u64 = 0;
-	wq_int.s.wq_int = 1 << pow_receive_group;
-	cvmx_write_csr(CVMX_POW_WQ_INT, wq_int.u64);
+		wq_int.u64 = 0;
+		wq_int.s.wq_int = 1 << pow_receive_group;
+		cvmx_write_csr(CVMX_POW_WQ_INT, wq_int.u64);
+	}
 
 	spin_lock_irqsave(&core_state.lock, flags);
 
@@ -286,349 +305,111 @@ static ktime_t cvm_oct_ptp_to_ktime(u64 ptptime)
 	return ktime_sub_ns(ktimebase, ptpbase - ptptime);
 }
 
-/**
- * cvm_oct_napi_poll - the NAPI poll function.
- * @napi: The NAPI instance, or null if called from cvm_oct_poll_controller
- * @budget: Maximum number of packets to receive.
- *
- * Returns the number of packets processed.
- */
-static int cvm_oct_napi_poll(struct napi_struct *napi, int budget)
-{
-	const int	coreid = cvmx_get_core_num();
-	u64		old_group_mask;
-	u64		old_scratch;
-	int		rx_count = 0;
-	bool		did_work_request = false;
-	bool		packet_copied;
-
-	char		*p = (char *)cvm_oct_by_port;
-	/* Prefetch cvm_oct_device since we know we need it soon */
-	prefetch(&p[0]);
-	prefetch(&p[SMP_CACHE_BYTES]);
-	prefetch(&p[2 * SMP_CACHE_BYTES]);
-
-	if (USE_ASYNC_IOBDMA) {
-		/* Save scratch in case userspace is using it */
-		CVMX_SYNCIOBDMA;
-		old_scratch = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
-	}
+#undef CVM_OCT_NAPI_68
+#include "ethernet-napi.c"
 
-	/* Only allow work for our group (and preserve priorities) */
-	old_group_mask = cvmx_read_csr(CVMX_POW_PP_GRP_MSKX(coreid));
-	cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(coreid),
-		       (old_group_mask & ~0xFFFFull) | 1 << pow_receive_group);
+#define CVM_OCT_NAPI_68
+#include "ethernet-napi.c"
 
-	if (USE_ASYNC_IOBDMA) {
-		cvmx_pow_work_request_async(CVMX_SCR_SCRATCH, CVMX_POW_NO_WAIT);
-		did_work_request = true;
-	}
-
-	while (rx_count < budget) {
-		struct sk_buff *skb = NULL;
-		struct sk_buff **pskb = NULL;
-		bool skb_in_hw;
-		cvmx_wqe_t *work;
-		unsigned int segments;
-		int packets_to_replace = 0;
-		unsigned int packet_len;
-
-		union cvmx_buf_ptr  packet_ptr;
-
-		if (USE_ASYNC_IOBDMA && did_work_request)
-			work = cvmx_pow_work_response_async(CVMX_SCR_SCRATCH);
-		else
-			work = cvmx_pow_work_request_sync(CVMX_POW_NO_WAIT);
-
-		prefetch(work);
-		did_work_request = false;
-		if (work == NULL) {
-			union cvmx_pow_wq_int wq_int;
-			wq_int.u64 = 0;
-			wq_int.s.wq_int = 1 << pow_receive_group;
-			cvmx_write_csr(CVMX_POW_WQ_INT, wq_int.u64);
-			break;
-		}
-		packet_ptr = work->packet_ptr;
-		pskb = cvm_oct_packet_to_skb(cvm_oct_get_buffer_ptr(packet_ptr));
-		prefetch(pskb);
+static int (*cvm_oct_napi_poll)(struct napi_struct *, int);
 
-		if (USE_ASYNC_IOBDMA && rx_count < (budget - 1)) {
-			cvmx_pow_work_request_async_nocheck(CVMX_SCR_SCRATCH, CVMX_POW_NO_WAIT);
-			did_work_request = true;
-		}
-
-		if (rx_count == 0) {
-			/* First time through, see if there is enough
-			 * work waiting to merit waking another
-			 * CPU.
-			 */
-			union cvmx_pow_wq_int_cntx counts;
-			int backlog;
-			int cores_in_use = core_state.active_cores;
-			counts.u64 = cvmx_read_csr(CVMX_POW_WQ_INT_CNTX(pow_receive_group));
-			backlog = counts.s.iq_cnt + counts.s.ds_cnt;
-			if (backlog > budget * cores_in_use && napi != NULL)
-				cvm_oct_enable_one_cpu();
-		}
-
-		/* If WORD2[SOFTWARE] then this WQE is a complete for
-		 * a TX packet.
-		 */
-		if (work->word2.s.software) {
-			struct octeon_ethernet *priv;
-			int packet_qos = work->word0.raw.unused;
-
-			skb = (struct sk_buff *)packet_ptr.u64;
-			priv = netdev_priv(skb->dev);
-			if (!netif_running(skb->dev))
-				netif_wake_queue(skb->dev);
-			if (unlikely((skb_shinfo(skb)->tx_flags | SKBTX_IN_PROGRESS) != 0 &&
-				     priv->tx_timestamp_hw)) {
-					u64 ns = *(u64 *)work->packet_data;
-					struct skb_shared_hwtstamps ts;
-					ts.syststamp = cvm_oct_ptp_to_ktime(ns);
-					ts.hwtstamp = ns_to_ktime(ns);
-					skb_tstamp_tx(skb, &ts);
-			}
-			dev_kfree_skb_any(skb);
-
-			cvmx_fpa_free(work, CVMX_FPA_WQE_POOL, DONT_WRITEBACK(1));
+#ifdef CONFIG_NET_POLL_CONTROLLER
 
-			/* We are done with this one, adjust the queue
-			 * depth.
-			 */
-			cvmx_fau_atomic_add32(priv->tx_queue[packet_qos].fau, -1);
-			continue;
-		}
-		segments = work->word2.s.bufs;
-		skb_in_hw = USE_SKBUFFS_IN_HW && segments > 0;
-		if (likely(skb_in_hw)) {
-			skb = *pskb;
-			prefetch(&skb->head);
-			prefetch(&skb->len);
-		}
-		prefetch(cvm_oct_by_port[work->word1.cn38xx.ipprt]);
+/**
+ * cvm_oct_poll_controller - poll for receive packets
+ * device.
+ *
+ * @dev:    Device to poll. Unused
+ */
+void cvm_oct_poll_controller(struct net_device *dev)
+{
+	cvm_oct_napi_poll(NULL, 16);
+}
+#endif
 
-		/* Immediately throw away all packets with receive errors */
-		if (unlikely(work->word2.snoip.rcv_error)) {
-			if (cvm_oct_check_rcv_error(work))
-				continue;
-		}
+static struct kmem_cache *cvm_oct_kmem_sso;
+static int cvm_oct_sso_fptr_count;
 
-		packet_len = work->word1.len;
-		/* We can only use the zero copy path if skbuffs are
-		 * in the FPA pool and the packet fits in a single
-		 * buffer.
-		 */
-		if (likely(skb_in_hw)) {
-			skb->data = phys_to_virt(packet_ptr.s.addr);
-			prefetch(skb->data);
-			skb->len = packet_len;
-			packets_to_replace = segments;
-			if (likely(segments == 1)) {
-				skb_set_tail_pointer(skb, skb->len);
-			} else {
-				struct sk_buff *current_skb = skb;
-				struct sk_buff *next_skb = NULL;
-				unsigned int segment_size;
-				bool first_frag = true;
-
-				skb_frag_list_init(skb);
-				/* Multi-segment packet. */
-				for (;;) {
-					/* Octeon Errata PKI-100: The segment size is
-					 * wrong. Until it is fixed, calculate the
-					 * segment size based on the packet pool
-					 * buffer size. When it is fixed, the
-					 * following line should be replaced with this
-					 * one: int segment_size =
-					 * segment_ptr.s.size;
-					 */
-					segment_size = CVMX_FPA_PACKET_POOL_SIZE -
-						(packet_ptr.s.addr - (((packet_ptr.s.addr >> 7) - packet_ptr.s.back) << 7));
-					if (segment_size > packet_len)
-						segment_size = packet_len;
-					if (!first_frag) {
-						current_skb->len = segment_size;
-						skb->data_len += segment_size;
-						skb->truesize += current_skb->truesize;
-					}
-					skb_set_tail_pointer(current_skb, segment_size);
-					packet_len -= segment_size;
-					segments--;
-					if (segments == 0)
-						break;
-					packet_ptr = *(union cvmx_buf_ptr *)phys_to_virt(packet_ptr.s.addr - 8);
-					next_skb = *cvm_oct_packet_to_skb(cvm_oct_get_buffer_ptr(packet_ptr));
-					if (first_frag) {
-						skb_frag_add_head(current_skb, next_skb);
-					} else {
-						current_skb->next = next_skb;
-						next_skb->next = NULL;
-					}
-					current_skb = next_skb;
-					first_frag = false;
-					current_skb->data = phys_to_virt(packet_ptr.s.addr);
-				}
-			}
-			packet_copied = false;
-		} else {
-			/* We have to copy the packet. First allocate
-			 * an skbuff for it.
-			 */
-			skb = dev_alloc_skb(packet_len);
-			if (!skb) {
-				printk_ratelimited("Port %d failed to allocate skbuff, packet dropped\n",
-						   work->word1.cn38xx.ipprt);
-				cvm_oct_free_work(work);
-				continue;
-			}
+static int cvm_oct_sso_initialize(int num_wqe)
+{
+	union cvmx_sso_cfg sso_cfg;
+	union cvmx_fpa_fpfx_marks fpa_marks;
+	int i;
+	int rwq_bufs;
 
-			/* Check if we've received a packet that was
-			 * entirely stored in the work entry.
-			 */
-			if (unlikely(work->word2.s.bufs == 0)) {
-				u8 *ptr = work->packet_data;
-
-				if (likely(!work->word2.s.not_IP)) {
-					/* The beginning of the packet
-					 * moves for IP packets.
-					 */
-					if (work->word2.s.is_v6)
-						ptr += 2;
-					else
-						ptr += 6;
-				}
-				memcpy(skb_put(skb, packet_len), ptr, packet_len);
-				/* No packet buffers to free */
-			} else {
-				int segments = work->word2.s.bufs;
-				union cvmx_buf_ptr segment_ptr = work->packet_ptr;
-
-				while (segments--) {
-					union cvmx_buf_ptr next_ptr =
-					    *(union cvmx_buf_ptr *)phys_to_virt(segment_ptr.s.addr - 8);
-
-			/* Octeon Errata PKI-100: The segment size is
-			 * wrong. Until it is fixed, calculate the
-			 * segment size based on the packet pool
-			 * buffer size. When it is fixed, the
-			 * following line should be replaced with this
-			 * one: int segment_size =
-			 * segment_ptr.s.size;
-			 */
-					int segment_size = CVMX_FPA_PACKET_POOL_SIZE -
-						(segment_ptr.s.addr - (((segment_ptr.s.addr >> 7) - segment_ptr.s.back) << 7));
-					/* Don't copy more than what
-					 * is left in the packet.
-					 */
-					if (segment_size > packet_len)
-						segment_size = packet_len;
-					/* Copy the data into the packet */
-					memcpy(skb_put(skb, segment_size),
-					       phys_to_virt(segment_ptr.s.addr),
-					       segment_size);
-					packet_len -= segment_size;
-					segment_ptr = next_ptr;
-				}
-			}
-			packet_copied = true;
-		}
+	if (!OCTEON_IS_MODEL(OCTEON_CN68XX))
+		return 0;
 
-		if (likely((work->word1.cn38xx.ipprt < TOTAL_NUMBER_OF_PORTS) &&
-			   cvm_oct_by_port[work->word1.cn38xx.ipprt])) {
-			struct octeon_ethernet *priv = cvm_oct_by_port[work->word1.cn38xx.ipprt];
+	rwq_bufs = 48 + DIV_ROUND_UP(num_wqe, 26);
+	cvm_oct_sso_fptr_count = rwq_bufs;
+	cvm_oct_kmem_sso = kmem_cache_create("octeon_ethernet_sso", 256, 128, 0, NULL);
+	if (cvm_oct_kmem_sso == NULL) {
+		pr_err("cannot create kmem_cache for octeon_ethernet_sso\n");
+		return -ENOMEM;
+	}
 
-			/* Only accept packets for devices that are
-			 * currently up.
-			 */
-			if (likely(priv->netdev->flags & IFF_UP)) {
-				if (priv->rx_timestamp_hw) {
-					/* The first 8 bytes are the timestamp */
-					u64 ns = *(u64 *)skb->data;
-					struct skb_shared_hwtstamps *ts;
-					ts = skb_hwtstamps(skb);
-					ts->hwtstamp = ns_to_ktime(ns);
-					ts->syststamp = cvm_oct_ptp_to_ktime(ns);
-					__skb_pull(skb, 8);
-				}
-				skb->protocol = eth_type_trans(skb, priv->netdev);
-				skb->dev = priv->netdev;
-
-				if (unlikely(work->word2.s.not_IP || work->word2.s.IP_exc ||
-					work->word2.s.L4_error || !work->word2.s.tcp_or_udp))
-					skb->ip_summed = CHECKSUM_NONE;
-				else
-					skb->ip_summed = CHECKSUM_UNNECESSARY;
-
-				/* Increment RX stats for virtual ports */
-				if (work->word1.cn38xx.ipprt >= CVMX_PIP_NUM_INPUT_PORTS) {
-					atomic64_add(1, (atomic64_t *)&priv->netdev->stats.rx_packets);
-					atomic64_add(skb->len, (atomic64_t *)&priv->netdev->stats.rx_bytes);
-				}
-				netif_receive_skb(skb);
-				rx_count++;
-			} else {
-				/* Drop any packet received for a device that isn't up */
-				atomic64_add(1, (atomic64_t *)&priv->netdev->stats.rx_dropped);
-				dev_kfree_skb_any(skb);
-			}
-		} else {
-			/* Drop any packet received for a device that
-			 * doesn't exist.
-			 */
-			printk_ratelimited("Port %d not controlled by Linux, packet dropped\n",
-					   work->word1.cn38xx.ipprt);
-			dev_kfree_skb_any(skb);
+	/*
+	 * CN68XX-P1 may reset with the wrong values, put in
+	 * the correct values.
+	 */
+	fpa_marks.u64 = 0;
+	fpa_marks.s.fpf_wr = 0xa4;
+	fpa_marks.s.fpf_rd = 0x40;
+	cvmx_write_csr(CVMX_FPA_FPF8_MARKS, fpa_marks.u64);
+
+	/* Make sure RWI/RWO is disabled. */
+	sso_cfg.u64 = cvmx_read_csr(CVMX_SSO_CFG);
+	sso_cfg.s.rwen = 0;
+	cvmx_write_csr(CVMX_SSO_CFG, sso_cfg.u64);
+
+	while (rwq_bufs) {
+		union cvmx_sso_rwq_psh_fptr fptr;
+		void *mem;
+
+		mem = kmem_cache_alloc(cvm_oct_kmem_sso, GFP_KERNEL);
+		if (mem == NULL) {
+			pr_err("cannot allocate memory from octeon_ethernet_sso\n");
+			return -ENOMEM;
 		}
-		/* Check to see if the skbuff and work share the same
-		 * packet buffer.
-		 */
-		if (USE_SKBUFFS_IN_HW && likely(!packet_copied)) {
-			/* This buffer needs to be replaced, increment
-			 * the number of buffers we need to free by
-			 * one.
-			 */
-			cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE,
-					      packets_to_replace);
-
-			cvmx_fpa_free(work, CVMX_FPA_WQE_POOL,
-				      DONT_WRITEBACK(1));
-		} else {
-			cvm_oct_free_work(work);
+		for (;;) {
+			fptr.u64 = cvmx_read_csr(CVMX_SSO_RWQ_PSH_FPTR);
+			if (!fptr.s.full)
+				break;
+			__delay(1000);
 		}
+		fptr.s.fptr = virt_to_phys(mem) >> 7;
+		cvmx_write_csr(CVMX_SSO_RWQ_PSH_FPTR, fptr.u64);
+		rwq_bufs--;
 	}
-	/* Restore the original POW group mask */
-	cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(coreid), old_group_mask);
-	if (USE_ASYNC_IOBDMA) {
-		/* Restore the scratch area */
-		cvmx_scratch_write64(CVMX_SCR_SCRATCH, old_scratch);
-	}
-	cvm_oct_rx_refill_pool(0);
+	for (i = 0; i < 8; i++) {
+		union cvmx_sso_rwq_head_ptrx head_ptr;
+		union cvmx_sso_rwq_tail_ptrx tail_ptr;
+		void *mem;
+
+		mem = kmem_cache_alloc(cvm_oct_kmem_sso, GFP_KERNEL);
+		if (mem == NULL) {
+			pr_err("cannot allocate memory from octeon_ethernet_sso\n");
+			return -ENOMEM;
+		}
 
-	if (rx_count < budget && napi != NULL) {
-		/* No more work */
-		napi_complete(napi);
-		cvm_oct_no_more_work(napi);
+		head_ptr.u64 = 0;
+		tail_ptr.u64 = 0;
+		head_ptr.s.ptr = virt_to_phys(mem) >> 7;
+		tail_ptr.s.ptr = head_ptr.s.ptr;
+		cvmx_write_csr(CVMX_SSO_RWQ_HEAD_PTRX(i), head_ptr.u64);
+		cvmx_write_csr(CVMX_SSO_RWQ_TAIL_PTRX(i), tail_ptr.u64);
 	}
-	return rx_count;
-}
+	/* Now enable the SS0  RWI/RWO */
+	sso_cfg.u64 = cvmx_read_csr(CVMX_SSO_CFG);
+	sso_cfg.s.rwen = 1;
+	sso_cfg.s.rwq_byp_dis = 0;
+	sso_cfg.s.rwio_byp_dis = 0;
+	cvmx_write_csr(CVMX_SSO_CFG, sso_cfg.u64);
 
-#ifdef CONFIG_NET_POLL_CONTROLLER
-/**
- * cvm_oct_poll_controller - poll for receive packets
- * device.
- *
- * @dev:    Device to poll. Unused
- */
-void cvm_oct_poll_controller(struct net_device *dev)
-{
-	cvm_oct_napi_poll(NULL, 16);
+	return 0;
 }
-#endif
 
-void cvm_oct_rx_initialize(void)
+void cvm_oct_rx_initialize(int num_wqe)
 {
 	int i;
 	struct net_device *dev_for_napi = NULL;
@@ -636,6 +417,11 @@ void cvm_oct_rx_initialize(void)
 	if (list_empty(&cvm_oct_list))
 		panic("No net_devices were allocated.");
 
+	if (OCTEON_IS_MODEL(OCTEON_CN68XX))
+		cvm_oct_napi_poll = cvm_oct_napi_poll_68;
+	else
+		cvm_oct_napi_poll = cvm_oct_napi_poll_38;
+
 	dev_for_napi = list_first_entry(&cvm_oct_list,
 					struct octeon_ethernet,
 					list)->netdev;
@@ -665,20 +451,115 @@ void cvm_oct_rx_initialize(void)
 		panic("Could not acquire Ethernet IRQ %d\n",
 		      OCTEON_IRQ_WORKQ0 + pow_receive_group);
 
+	if (cvm_oct_sso_initialize(num_wqe))
+		goto err;
+
 	/* Scheduld NAPI now.  This will indirectly enable interrupts. */
 	preempt_disable();
 	cvm_oct_enable_one_cpu();
 	preempt_enable();
+	return;
+err:
+	free_irq(OCTEON_IRQ_WORKQ0 + pow_receive_group, &cvm_oct_list);
+	return;
 }
 
-void cvm_oct_rx_shutdown(void)
+void cvm_oct_rx_shutdown0(void)
 {
 	int i;
+
+	/* Disable POW/SSO interrupt */
+	if (OCTEON_IS_MODEL(OCTEON_CN68XX))
+		cvmx_write_csr(CVMX_SSO_WQ_INT_THRX(pow_receive_group), 0);
+	else
+		cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group), 0);
+
+	/* Free the interrupt handler */
+	free_irq(OCTEON_IRQ_WORKQ0 + pow_receive_group, &cvm_oct_list);
+
 	/* Shutdown all of the NAPIs */
 	for_each_possible_cpu(i)
 		netif_napi_del(&cvm_oct_napi[i].napi);
+}
 
-	/* Free the interrupt handler */
-	free_irq(OCTEON_IRQ_WORKQ0 + pow_receive_group, &cvm_oct_list);
+void cvm_oct_rx_shutdown1(void)
+{
+	union cvmx_fpa_quex_available queue_available;
+	union cvmx_sso_cfg sso_cfg;
+	union cvmx_sso_rwq_pop_fptr pop_fptr;
+	union cvmx_sso_rwq_psh_fptr fptr;
+	union cvmx_sso_fpage_cnt fpage_cnt;
+	int num_to_transfer, count, i;
+	void *mem;
+
+
+
+	if (!OCTEON_IS_MODEL(OCTEON_CN68XX))
+		return;
+
+	sso_cfg.u64 = cvmx_read_csr(CVMX_SSO_CFG);
+	sso_cfg.s.rwen = 0;
+	sso_cfg.s.rwq_byp_dis = 1;
+	cvmx_write_csr(CVMX_SSO_CFG, sso_cfg.u64);
+	cvmx_read_csr(CVMX_SSO_CFG);
+	queue_available.u64 = cvmx_read_csr(CVMX_FPA_QUEX_AVAILABLE(8));
+
+	/* Make CVMX_FPA_QUEX_AVAILABLE(8) % 16 == 0*/
+	for (num_to_transfer = (16 - queue_available.s.que_siz) % 16;
+	     num_to_transfer > 0; num_to_transfer--) {
+		do {
+			pop_fptr.u64 = cvmx_read_csr(CVMX_SSO_RWQ_POP_FPTR);
+		} while (!pop_fptr.s.val);
+		for (;;) {
+			fptr.u64 = cvmx_read_csr(CVMX_SSO_RWQ_PSH_FPTR);
+			if (!fptr.s.full)
+				break;
+			__delay(1000);
+		}
+		fptr.s.fptr = pop_fptr.s.fptr;
+		cvmx_write_csr(CVMX_SSO_RWQ_PSH_FPTR, fptr.u64);
+	}
+	cvmx_read_csr(CVMX_SSO_CFG);
+
+	do {
+		queue_available.u64 = cvmx_read_csr(CVMX_FPA_QUEX_AVAILABLE(8));
+	} while (queue_available.s.que_siz % 16);
+
+	sso_cfg.s.rwen = 1;
+	sso_cfg.s.rwq_byp_dis = 0;
+	cvmx_write_csr(CVMX_SSO_CFG, sso_cfg.u64);
+
+	for (i = 0; i < 8; i++) {
+		union cvmx_sso_rwq_head_ptrx head_ptr;
+		union cvmx_sso_rwq_tail_ptrx tail_ptr;
+
+		head_ptr.u64 = cvmx_read_csr(CVMX_SSO_RWQ_HEAD_PTRX(i));
+		tail_ptr.u64 = cvmx_read_csr(CVMX_SSO_RWQ_TAIL_PTRX(i));
+		WARN_ON(head_ptr.s.ptr != tail_ptr.s.ptr);
+
+		mem = phys_to_virt(((u64)head_ptr.s.ptr) << 7);
+		kmem_cache_free(cvm_oct_kmem_sso, mem);
+	}
+
+	count = 0;
+
+	do {
+		do {
+			pop_fptr.u64 = cvmx_read_csr(CVMX_SSO_RWQ_POP_FPTR);
+			if (pop_fptr.s.val) {
+				mem = phys_to_virt(((u64)pop_fptr.s.fptr) << 7);
+				kmem_cache_free(cvm_oct_kmem_sso, mem);
+				count++;
+			}
+		} while (pop_fptr.s.val);
+		fpage_cnt.u64 = cvmx_read_csr(CVMX_SSO_FPAGE_CNT);
+	} while (fpage_cnt.s.fpage_cnt);
+
+	WARN_ON(count != cvm_oct_sso_fptr_count);
 
+	sso_cfg.s.rwen = 0;
+	sso_cfg.s.rwq_byp_dis = 0;
+	cvmx_write_csr(CVMX_SSO_CFG, sso_cfg.u64);
+	kmem_cache_destroy(cvm_oct_kmem_sso);
+	cvm_oct_kmem_sso = NULL;
 }
diff --git a/drivers/net/ethernet/octeon/ethernet-tx.c b/drivers/net/ethernet/octeon/ethernet-tx.c
index 95572f1..99e48b8 100644
--- a/drivers/net/ethernet/octeon/ethernet-tx.c
+++ b/drivers/net/ethernet/octeon/ethernet-tx.c
@@ -471,17 +471,3 @@ skip_xmit:
 
 	return NETDEV_TX_OK;
 }
-
-void cvm_oct_tx_shutdown_dev(struct net_device *dev)
-{
-}
-
-void cvm_oct_tx_initialize(void)
-{
-
-}
-
-void cvm_oct_tx_shutdown(void)
-{
-
-}
diff --git a/drivers/net/ethernet/octeon/ethernet.c b/drivers/net/ethernet/octeon/ethernet.c
index 5fb5d3f..167c0d4 100644
--- a/drivers/net/ethernet/octeon/ethernet.c
+++ b/drivers/net/ethernet/octeon/ethernet.c
@@ -93,10 +93,13 @@ struct workqueue_struct *cvm_oct_poll_queue;
  */
 atomic_t cvm_oct_poll_queue_stopping = ATOMIC_INIT(0);
 
-/* cvm_oct_by_port is an array of every ethernet device owned by this
- * driver indexed by the ipd input port number.
+/* cvm_oct_by_pkind is an array of every ethernet device owned by this
+ * driver indexed by the IPD pkind/port_number.  If an entry is empty
+ * (NULL) it either doesn't exist, or there was a collision.  The two
+ * cases can be distinguished by trying to look up via
+ * cvm_oct_dev_for_port();
  */
-struct octeon_ethernet *cvm_oct_by_port[TOTAL_NUMBER_OF_PORTS] __cacheline_aligned;
+struct octeon_ethernet *cvm_oct_by_pkind[64] __cacheline_aligned;
 
 /* cvm_oct_list is a list of all cvm_oct_private_t created by this driver. */
 LIST_HEAD(cvm_oct_list);
@@ -547,6 +550,46 @@ extern void octeon_mdiobus_force_mod_depencency(void);
 static int num_devices_extra_wqe;
 #define PER_DEVICE_EXTRA_WQE (MAX_OUT_QUEUE_DEPTH)
 
+static struct rb_root cvm_oct_ipd_tree = RB_ROOT;
+
+void cvm_oct_add_ipd_port(struct octeon_ethernet *port)
+{
+	struct rb_node **link = &cvm_oct_ipd_tree.rb_node;
+	struct rb_node *parent = NULL;
+	struct octeon_ethernet *n;
+	int value = port->key;
+
+	while (*link) {
+		parent = *link;
+		n = rb_entry(parent, struct octeon_ethernet, ipd_tree);
+
+		if (value < n->key)
+			link = &(*link)->rb_left;
+		else if (value > n->key)
+			link = &(*link)->rb_right;
+		else
+			BUG();
+	}
+	rb_link_node(&port->ipd_tree, parent, link);
+	rb_insert_color(&port->ipd_tree, &cvm_oct_ipd_tree);
+}
+
+struct octeon_ethernet *cvm_oct_dev_for_port(int port_number)
+{
+	struct rb_node *n = cvm_oct_ipd_tree.rb_node;
+	while (n) {
+		struct octeon_ethernet *s = rb_entry(n, struct octeon_ethernet, ipd_tree);
+
+		if (s->key > port_number)
+			n = n->rb_left;
+		else if (s->key < port_number)
+			n = n->rb_left;
+		else
+			return s;
+	}
+	return NULL;
+}
+
 static struct device_node * __devinit cvm_oct_of_get_child(const struct device_node *parent,
 							   int reg_val)
 {
@@ -660,6 +703,7 @@ static int __devinit cvm_oct_probe(struct platform_device *pdev)
 					  cvm_oct_periodic_worker);
 			priv->imode = imode;
 			priv->ipd_port = cvmx_helper_get_ipd_port(interface, interface_port);
+			priv->key = priv->ipd_port;
 			priv->pko_port = cvmx_helper_get_pko_port(interface, interface_port);
 			base_queue = cvmx_pko_get_base_queue(priv->ipd_port);
 			priv->num_tx_queues = cvmx_pko_get_num_queues(priv->ipd_port);
@@ -667,6 +711,11 @@ static int __devinit cvm_oct_probe(struct platform_device *pdev)
 			BUG_ON(priv->num_tx_queues < 1);
 			BUG_ON(priv->num_tx_queues > 32);
 
+			if (octeon_has_feature(OCTEON_FEATURE_PKND))
+				priv->ipd_pkind = cvmx_helper_get_pknd(interface, interface_port);
+			else
+				priv->ipd_pkind = priv->ipd_port;
+
 			for (qos = 0; qos < priv->num_tx_queues; qos++) {
 				priv->tx_queue[qos].queue = base_queue + qos;
 				fau = fau - sizeof(u32);
@@ -680,7 +729,6 @@ static int __devinit cvm_oct_probe(struct platform_device *pdev)
 				(CVMX_PKO_QUEUES_PER_PORT_INTERFACE1 > 1);
 
 			switch (priv->imode) {
-
 			/* These types don't support ports to IPD/PKO */
 			case CVMX_HELPER_INTERFACE_MODE_DISABLED:
 			case CVMX_HELPER_INTERFACE_MODE_PCIE:
@@ -729,7 +777,12 @@ static int __devinit cvm_oct_probe(struct platform_device *pdev)
 				free_netdev(dev);
 			} else {
 				list_add_tail(&priv->list, &cvm_oct_list);
-				cvm_oct_by_port[priv->ipd_port] = priv;
+				if (cvm_oct_by_pkind[priv->ipd_pkind] == NULL)
+					cvm_oct_by_pkind[priv->ipd_pkind] = priv;
+				else
+					cvm_oct_by_pkind[priv->ipd_pkind] = (void *)-1L;
+
+				cvm_oct_add_ipd_port(priv);
 				/* Each transmit queue will need its
 				 * own MAX_OUT_QUEUE_DEPTH worth of
 				 * WQE to track the transmit skbs.
@@ -743,8 +796,7 @@ static int __devinit cvm_oct_probe(struct platform_device *pdev)
 		}
 	}
 
-	cvm_oct_tx_initialize();
-	cvm_oct_rx_initialize();
+	cvm_oct_rx_initialize(num_packet_buffers + num_devices_extra_wqe * PER_DEVICE_EXTRA_WQE);
 
 	queue_delayed_work(cvm_oct_poll_queue, &cvm_oct_rx_refill_work, HZ);
 
@@ -756,31 +808,50 @@ static int __devexit cvm_oct_remove(struct platform_device *pdev)
 	struct octeon_ethernet *priv;
 	struct octeon_ethernet *tmp;
 
-	/* Disable POW interrupt */
-	cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group), 0);
+	/* Put both taking the interface down and unregistering it
+	 * under the lock.  That way the devices cannot be taken back
+	 * up in the middle of everything.
+	 */
+	rtnl_lock();
+
+	/* Take down all the interfaces, this disables the GMX and
+	 * prevents it from getting into a Bad State when IPD is
+	 * disabled.
+	 */
+	list_for_each_entry(priv, &cvm_oct_list, list) {
+		unsigned int f = dev_get_flags(priv->netdev);
+		dev_change_flags(priv->netdev, f & ~IFF_UP);
+	}
+
+	mdelay(10);
 
 	cvmx_ipd_disable();
 
+	mdelay(10);
+
 	atomic_inc_return(&cvm_oct_poll_queue_stopping);
 	cancel_delayed_work_sync(&cvm_oct_rx_refill_work);
 
-	cvm_oct_rx_shutdown();
-	cvm_oct_tx_shutdown();
+	cvm_oct_rx_shutdown0();
+
+	/* unregister the ethernet devices */
+	list_for_each_entry(priv, &cvm_oct_list, list) {
+		cancel_delayed_work_sync(&priv->port_periodic_work);
+		unregister_netdevice(priv->netdev);
+	}
 
-	cvmx_pko_disable();
+	rtnl_unlock();
 
 	/* Free the ethernet devices */
 	list_for_each_entry_safe_reverse(priv, tmp, &cvm_oct_list, list) {
 		list_del(&priv->list);
-		cancel_delayed_work_sync(&priv->port_periodic_work);
-		cvm_oct_tx_shutdown_dev(priv->netdev);
-		unregister_netdev(priv->netdev);
-		cvm_oct_by_port[priv->ipd_port] = NULL;
 		free_netdev(priv->netdev);
 	}
 
 	cvmx_helper_shutdown_packet_io_global();
 
+	cvm_oct_rx_shutdown1();
+
 	destroy_workqueue(cvm_oct_poll_queue);
 
 	/* Free the HW pools */
diff --git a/drivers/net/ethernet/octeon/octeon-ethernet.h b/drivers/net/ethernet/octeon/octeon-ethernet.h
index 8358e91..a02f79c 100644
--- a/drivers/net/ethernet/octeon/octeon-ethernet.h
+++ b/drivers/net/ethernet/octeon/octeon-ethernet.h
@@ -41,8 +41,11 @@
  * driver state stored in netdev_priv(dev).
  */
 struct octeon_ethernet {
+	struct rb_node ipd_tree;
+	int key;
 	int ipd_port;
 	int pko_port;
+	int ipd_pkind;
 	int interface;
 	int interface_port;
 
@@ -83,6 +86,7 @@ struct octeon_ethernet {
 	u64 last_tx_octets;
 	u32 last_tx_packets;
 };
+struct octeon_ethernet *cvm_oct_dev_for_port(int);
 
 int cvm_oct_free_work(void *work_queue_entry);
 
@@ -111,15 +115,11 @@ int cvm_oct_ioctl(struct net_device *dev, struct ifreq *rq, int cmd);
 int cvm_oct_phy_setup_device(struct net_device *dev);
 
 int cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev);
-int cvm_oct_transmit_qos(struct net_device *dev, void *work_queue_entry,
-			 int do_free, int qos);
-void cvm_oct_tx_initialize(void);
-void cvm_oct_tx_shutdown(void);
-void cvm_oct_tx_shutdown_dev(struct net_device *dev);
 
 void cvm_oct_poll_controller(struct net_device *dev);
-void cvm_oct_rx_initialize(void);
-void cvm_oct_rx_shutdown(void);
+void cvm_oct_rx_initialize(int);
+void cvm_oct_rx_shutdown0(void);
+void cvm_oct_rx_shutdown1(void);
 
 int cvm_oct_mem_fill_fpa(int pool, int elements);
 int cvm_oct_mem_empty_fpa(int pool, int elements);
@@ -134,7 +134,7 @@ extern int pow_send_group;
 extern int pow_receive_group;
 extern char pow_send_list[];
 extern struct list_head cvm_oct_list;
-extern struct octeon_ethernet *cvm_oct_by_port[];
+extern struct octeon_ethernet *cvm_oct_by_pkind[];
 
 extern struct workqueue_struct *cvm_oct_poll_queue;
 extern atomic_t cvm_oct_poll_queue_stopping;
-- 
1.7.5.4

