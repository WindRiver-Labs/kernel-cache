From 559d7913faa66f6b61c17cfac8e8435312b7849f Mon Sep 17 00:00:00 2001
From: David Daney <david.daney@cavium.com>
Date: Mon, 8 Oct 2012 17:03:20 -0700
Subject: [PATCH 193/337] netdev: octeon-ethernet: Use
 octeon_send_ipi_single() instead of
 smp_call_function_single()

Based On SDK 3.0.0-482

Since octeon_send_ipi_single() is lockless, deadlocks are impossible.

This should really fix the issue of:

cvm_oct_enable_one_cpu() uses smp_call_function_single() and this
cannot be invoked from softirq. Invoking it from softirq could cause
deadlocks.  The deadlock was seen it tests with tasklist_lock but
could have been any other rw lock.

       CPU X                                   CPU Y

1. Acquires read_lock(&tasklist_lock)

2. Gets interrupted and                  2. spins trying to do
   calls smp_call_function_single           write_lock_irq(&tasklist_lock)
   on CPU Y

3. Calls smp_call_function_single on CPU Z

smp_call_function_single use a per CPU data structure, the second
smp_call_function_single will wait until the remote call from the
first smp_call_function_single is completed. In the deadlock scenario
above the first remote call will never get a chance to execute as CPU
Y has interrupts disabled and waiting for CPU X to release the read
lock.  CPU X is spinning waiting for the first remote call from
smp_call_function_single to complete.

Signed-off-by: David Daney <david.daney@cavium.com>
Signed-off-by: Corey Minyard <cminyard@mvista.com>
Signed-off-by: Jack Tan <jack.tan@windriver.com>
---
 drivers/net/ethernet/octeon/ethernet-rx.c |   20 ++++++++++++--------
 1 files changed, 12 insertions(+), 8 deletions(-)

diff --git a/drivers/net/ethernet/octeon/ethernet-rx.c b/drivers/net/ethernet/octeon/ethernet-rx.c
index a6dd083..11cafe0 100644
--- a/drivers/net/ethernet/octeon/ethernet-rx.c
+++ b/drivers/net/ethernet/octeon/ethernet-rx.c
@@ -80,7 +80,9 @@ struct cvm_oct_core_state {
 
 static struct cvm_oct_core_state core_state __cacheline_aligned_in_smp;
 
-static void cvm_oct_enable_napi(void *_)
+static int cvm_oct_enable_one_message;
+
+static void cvm_oct_enable_napi(void)
 {
 	int cpu = smp_processor_id();
 	napi_schedule(&cvm_oct_napi[cpu].napi);
@@ -88,7 +90,6 @@ static void cvm_oct_enable_napi(void *_)
 
 static void cvm_oct_enable_one_cpu(void)
 {
-	int v;
 	int cpu;
 	unsigned long flags;
 	spin_lock_irqsave(&core_state.lock, flags);
@@ -99,13 +100,10 @@ static void cvm_oct_enable_one_cpu(void)
 			core_state.active_cores++;
 			spin_unlock_irqrestore(&core_state.lock, flags);
 			if (cpu == smp_processor_id()) {
-				cvm_oct_enable_napi(NULL);
+				cvm_oct_enable_napi();
 			} else {
 #ifdef CONFIG_SMP
-				v = smp_call_function_single(cpu, cvm_oct_enable_napi,
-							     NULL, 0);
-				if (v)
-					panic("Can't enable NAPI.");
+				octeon_send_ipi_single(cpu, cvm_oct_enable_one_message);
 #else
 				BUG();
 #endif
@@ -200,7 +198,7 @@ static irqreturn_t cvm_oct_do_interrupt(int cpl, void *dev_id)
 
 	spin_unlock_irqrestore(&core_state.lock, flags);
 
-	cvm_oct_enable_napi(NULL);
+	cvm_oct_enable_napi();
 
 	return IRQ_HANDLED;
 }
@@ -417,6 +415,10 @@ void cvm_oct_rx_initialize(int num_wqe)
 	if (list_empty(&cvm_oct_list))
 		panic("No net_devices were allocated.");
 
+	cvm_oct_enable_one_message = octeon_request_ipi_handler(cvm_oct_enable_napi);
+	if (cvm_oct_enable_one_message < 0)
+		panic("cvm_oct_rx_initialize: No IPI handler handles available\n");
+
 	if (OCTEON_IS_MODEL(OCTEON_CN68XX))
 		cvm_oct_napi_poll = cvm_oct_napi_poll_68;
 	else
@@ -477,6 +479,8 @@ void cvm_oct_rx_shutdown0(void)
 	/* Free the interrupt handler */
 	free_irq(OCTEON_IRQ_WORKQ0 + pow_receive_group, &cvm_oct_list);
 
+	octeon_release_ipi_handler(cvm_oct_enable_one_message);
+
 	/* Shutdown all of the NAPIs */
 	for_each_possible_cpu(i)
 		netif_napi_del(&cvm_oct_napi[i].napi);
-- 
1.7.5.4

