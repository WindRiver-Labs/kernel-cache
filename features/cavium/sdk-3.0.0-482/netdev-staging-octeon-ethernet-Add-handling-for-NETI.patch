From 058e2f73acf911e9da59978f660c82443efd5988 Mon Sep 17 00:00:00 2001
From: David Daney <david.daney@cavium.com>
Date: Mon, 2 Jul 2012 17:55:36 -0700
Subject: [PATCH 107/337] netdev/staging: octeon-ethernet: Add handling for
 NETIF_F_FRAGLIST

Based On SDK 3.0.0-482

Fragment lists are no more difficult to handle than sg arrays, so
handle them too.  Clean-up of SKB recycling checks needed to handle
recycling of fragmented SKBs.

Signed-off-by: David Daney <david.daney@cavium.com>
Signed-off-by: Corey Minyard <cminyard@mvista.com>
Signed-off-by: Jack Tan <jack.tan@windriver.com>
---
 drivers/staging/octeon/ethernet-tx.c |  254 ++++++++++++++++++++--------------
 drivers/staging/octeon/ethernet.c    |    2 +-
 2 files changed, 151 insertions(+), 105 deletions(-)

diff --git a/drivers/staging/octeon/ethernet-tx.c b/drivers/staging/octeon/ethernet-tx.c
index 5da4122..f36be70 100644
--- a/drivers/staging/octeon/ethernet-tx.c
+++ b/drivers/staging/octeon/ethernet-tx.c
@@ -54,8 +54,6 @@
 
 #include <asm/octeon/cvmx-gmxx-defs.h>
 
-#define CVM_OCT_SKB_CB(skb)	((u64 *)((skb)->cb))
-
 /*
  * You can define GET_SKBUFF_QOS() to override how the skbuff output
  * function determines which output queue is used. The default
@@ -67,19 +65,69 @@
 #define GET_SKBUFF_QOS(skb) 0
 #endif
 
-/* Maximum number of SKBs to try to free per xmit packet. */
-#define MAX_SKB_TO_FREE (MAX_OUT_QUEUE_DEPTH * 2)
+#if REUSE_SKBUFFS_WITHOUT_FREE
+static bool cvm_oct_skb_ok_for_reuse(struct sk_buff *skb)
+{
+	unsigned char *fpa_head = cvm_oct_get_fpa_head(skb);
+
+	if (unlikely(skb->data < fpa_head))
+		return false;
+
+	if (unlikely(fpa_head - skb->head < sizeof(void *)))
+		return false;
+
+	if (unlikely((skb_end_pointer(skb) - fpa_head) < CVMX_FPA_PACKET_POOL_SIZE))
+		return false;
+
+	if (unlikely(skb_shared(skb)) ||
+	    unlikely(skb_cloned(skb)) ||
+	    unlikely(skb->fclone != SKB_FCLONE_UNAVAILABLE))
+		return false;
+
+	return true;
+}
+
+static void cvm_oct_skb_prepare_for_reuse(struct sk_buff *skb)
+{
+	int r;
+	unsigned char *fpa_head = cvm_oct_get_fpa_head(skb);
+
+	skb->data_len = 0;
+	skb_frag_list_init(skb);
+
+	/* The check also resets all the fields. */
+	r = skb_recycle_check(skb, CVMX_FPA_PACKET_POOL_SIZE);
+	WARN(!r, "SKB recycle logic fail\n");
+
+	*(struct sk_buff **)(fpa_head - sizeof(void *)) = skb;
+	skb->truesize = sizeof(*skb) + skb_end_pointer(skb) - skb->head;
+}
+
+static inline void cvm_oct_set_back(struct sk_buff *skb,
+				    union cvmx_buf_ptr *hw_buffer)
+{
+	unsigned char *fpa_head = cvm_oct_get_fpa_head(skb);
+
+	hw_buffer->s.back = ((unsigned long)skb->data >> 7) - ((unsigned long)fpa_head >> 7);
+}
+#else
+static bool cvm_oct_skb_ok_for_reuse(struct sk_buff *skb)
+{
+	return false;
+}
+static void cvm_oct_skb_prepare_for_reuse(struct sk_buff *skb)
+{
+	/* Do nothing */
+}
 
-static inline int32_t cvm_oct_adjust_skb_to_free(int32_t skb_to_free, int fau)
+static inline void cvm_oct_set_back(struct sk_buff *skb,
+				    union cvmx_buf_ptr *hw_buffer)
 {
-	int32_t undo;
-	undo = skb_to_free > 0 ? MAX_SKB_TO_FREE : skb_to_free + MAX_SKB_TO_FREE;
-	if (undo > 0)
-		cvmx_fau_atomic_add32(fau, -undo);
-	skb_to_free = -skb_to_free > MAX_SKB_TO_FREE ? MAX_SKB_TO_FREE : -skb_to_free;
-	return skb_to_free;
+	/* Do nothing. */
 }
 
+#endif
+
 /**
  * cvm_oct_xmit - transmit a packet
  * @skb:    Packet to send
@@ -89,21 +137,21 @@ static inline int32_t cvm_oct_adjust_skb_to_free(int32_t skb_to_free, int fau)
  */
 int cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev)
 {
+	struct sk_buff *skb_tmp;
 	cvmx_pko_command_word0_t pko_command;
 	union cvmx_buf_ptr hw_buffer;
 	uint64_t old_scratch;
 	uint64_t old_scratch2;
 	int qos;
 	int i;
+	int frag_count;
 	enum {QUEUE_HW, QUEUE_WQE, QUEUE_DROP} queue_type;
 	struct octeon_ethernet *priv = netdev_priv(dev);
 	s32 queue_depth;
 	s32 buffers_to_free;
+	s32 buffers_being_recycled;
 	unsigned long flags;
 	cvmx_wqe_t *work = NULL;
-#if REUSE_SKBUFFS_WITHOUT_FREE
-	unsigned char *fpa_head;
-#endif
 
 	/*
 	 * Prefetch the private data structure.  It is larger that one
@@ -148,15 +196,21 @@ int cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev)
 					       priv->tx_queue[qos].fau, 1);
 	}
 
+	frag_count = 0;
+	if (skb_has_frag_list(skb))
+		skb_walk_frags(skb, skb_tmp)
+			frag_count++;
 	/*
-	 * We have space for 6 segment pointers, If there will be more
-	 * than that, we must linearize.
+	 * We have space for 12 segment pointers, If there will be
+	 * more than that, we must linearize.  The count is: 1 (base
+	 * SKB) + frag_count + nr_frags.
 	 */
-	if (unlikely(skb_shinfo(skb)->nr_frags > 5)) {
+	if (unlikely(skb_shinfo(skb)->nr_frags + frag_count > 11)) {
 		if (unlikely(__skb_linearize(skb))) {
 			queue_type = QUEUE_DROP;
 			goto skip_xmit;
 		}
+		frag_count = 0;
 	}
 
 	/*
@@ -192,31 +246,64 @@ int cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev)
 	pko_command.s.total_bytes = skb->len;
 	pko_command.s.size0 = CVMX_FAU_OP_SIZE_32;
 	pko_command.s.subone0 = 1;
-
+	pko_command.s.reg0 = priv->tx_queue[qos].fau;
 	pko_command.s.dontfree = 1;
 
 	/* Build the PKO buffer pointer */
-	hw_buffer.u64 = 0;
-	if (skb_shinfo(skb)->nr_frags == 0) {
-		hw_buffer.s.addr = XKPHYS_TO_PHYS((u64)skb->data);
-		hw_buffer.s.pool = 0;
+	hw_buffer.u64 = 0; /* Implies pool == 0, i == 0 */
+	if (skb_shinfo(skb)->nr_frags == 0 && frag_count == 0) {
+		hw_buffer.s.addr = virt_to_phys(skb->data);
 		hw_buffer.s.size = skb->len;
+		cvm_oct_set_back(skb, &hw_buffer);
+		buffers_being_recycled = 1;
 	} else {
-		hw_buffer.s.addr = XKPHYS_TO_PHYS((u64)skb->data);
-		hw_buffer.s.pool = 0;
+		u64 *hw_buffer_list;
+		bool can_do_reuse = true;
+
+		work = cvmx_fpa_alloc(CVMX_FPA_WQE_POOL);
+		if (unlikely(!work)) {
+			netdev_err(dev, "Failed WQE allocate\n");
+			queue_type = QUEUE_DROP;
+			goto skip_xmit;
+		}
+		hw_buffer_list = (u64 *)work->packet_data;
+		hw_buffer.s.addr = virt_to_phys(skb->data);
 		hw_buffer.s.size = skb_headlen(skb);
-		CVM_OCT_SKB_CB(skb)[0] = hw_buffer.u64;
-		for (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {
-			struct skb_frag_struct *fs = skb_shinfo(skb)->frags + i;
-			hw_buffer.s.addr = XKPHYS_TO_PHYS((u64)(page_address(fs->page.p) + fs->page_offset));
+		if (skb_shinfo(skb)->nr_frags == 0 && cvm_oct_skb_ok_for_reuse(skb)) {
+			cvm_oct_set_back(skb, &hw_buffer);
+		} else {
+			hw_buffer.s.back = 0;
+			can_do_reuse = false;
+		}
+		hw_buffer_list[0] = hw_buffer.u64;
+		for (i = 1; i <= skb_shinfo(skb)->nr_frags; i++) {
+			struct skb_frag_struct *fs = skb_shinfo(skb)->frags + i - 1;
+			hw_buffer.s.addr = virt_to_phys((u8 *)page_address(fs->page.p) + fs->page_offset);
 			hw_buffer.s.size = fs->size;
-			CVM_OCT_SKB_CB(skb)[i + 1] = hw_buffer.u64;
+			hw_buffer_list[i] = hw_buffer.u64;
+			can_do_reuse = false;
 		}
-		hw_buffer.s.addr = XKPHYS_TO_PHYS((u64)CVM_OCT_SKB_CB(skb));
-		hw_buffer.s.size = skb_shinfo(skb)->nr_frags + 1;
-		pko_command.s.segs = skb_shinfo(skb)->nr_frags + 1;
+		skb_walk_frags(skb, skb_tmp) {
+			hw_buffer.s.addr = virt_to_phys(skb_tmp->data);
+			hw_buffer.s.size = skb_tmp->len;
+			if (cvm_oct_skb_ok_for_reuse(skb_tmp)) {
+				cvm_oct_set_back(skb_tmp, &hw_buffer);
+			} else {
+				hw_buffer.s.back = 0;
+				can_do_reuse = false;
+			}
+			hw_buffer_list[i] = hw_buffer.u64;
+			i++;
+		}
+		hw_buffer.s.addr = virt_to_phys(hw_buffer_list);
+		hw_buffer.s.size = i;
+		hw_buffer.s.back = 0;
+		hw_buffer.s.pool = CVMX_FPA_WQE_POOL;
+		buffers_being_recycled = i;
+		pko_command.s.segs = hw_buffer.s.size;
 		pko_command.s.gather = 1;
-		goto dont_put_skbuff_in_hw;
+		if (!can_do_reuse)
+			goto dont_put_skbuff_in_hw;
 	}
 
 	/*
@@ -230,33 +317,9 @@ int cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev)
 	 * shown a 25% increase in performance under some loads.
 	 */
 #if REUSE_SKBUFFS_WITHOUT_FREE
-	fpa_head = cvm_oct_get_fpa_head(skb);
-	if (unlikely(skb->data < fpa_head)) {
-		/*
-		 * printk("TX buffer beginning can't meet FPA
-		 * alignment constraints\n");
-		 */
-		goto dont_put_skbuff_in_hw;
-	}
-	if (unlikely
-	    ((skb_end_pointer(skb) - fpa_head) < CVMX_FPA_PACKET_POOL_SIZE)) {
-		/*
-		   printk("TX buffer isn't large enough for the FPA\n");
-		 */
+	if (!cvm_oct_skb_ok_for_reuse(skb))
 		goto dont_put_skbuff_in_hw;
-	}
-	if (unlikely(skb_shared(skb))) {
-		/*
-		   printk("TX buffer sharing data with someone else\n");
-		 */
-		goto dont_put_skbuff_in_hw;
-	}
-	if (unlikely(skb_cloned(skb))) {
-		/*
-		   printk("TX buffer has been cloned\n");
-		 */
-		goto dont_put_skbuff_in_hw;
-	}
+
 	if (unlikely(skb_header_cloned(skb))) {
 		/*
 		   printk("TX buffer header has been cloned\n");
@@ -269,20 +332,6 @@ int cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev)
 		 */
 		goto dont_put_skbuff_in_hw;
 	}
-	if (unlikely(skb_shinfo(skb)->nr_frags)) {
-		/*
-		   printk("TX buffer has fragments\n");
-		 */
-		goto dont_put_skbuff_in_hw;
-	}
-	if (unlikely
-	    (skb->truesize !=
-	     sizeof(*skb) + skb_end_pointer(skb) - skb->head)) {
-		/*
-		   printk("TX buffer truesize has been changed\n");
-		 */
-		goto dont_put_skbuff_in_hw;
-	}
 
 	/*
 	 * We can use this buffer in the FPA.  We don't need the FAU
@@ -290,27 +339,6 @@ int cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev)
 	 */
 	pko_command.s.dontfree = 0;
 
-	hw_buffer.s.back = ((unsigned long)skb->data >> 7) - ((unsigned long)fpa_head >> 7);
-	*(struct sk_buff **)(fpa_head - sizeof(void *)) = skb;
-
-	/*
-	 * The skbuff will be reused without ever being freed. We must
-	 * cleanup a bunch of core things.
-	 */
-	dst_release(skb_dst(skb));
-	skb_dst_set(skb, NULL);
-#ifdef CONFIG_XFRM
-	secpath_put(skb->sp);
-	skb->sp = NULL;
-#endif
-	nf_reset(skb);
-
-#ifdef CONFIG_NET_SCHED
-	skb->tc_index = 0;
-#ifdef CONFIG_NET_CLS_ACT
-	skb->tc_verd = 0;
-#endif /* CONFIG_NET_CLS_ACT */
-#endif /* CONFIG_NET_SCHED */
 #endif /* REUSE_SKBUFFS_WITHOUT_FREE */
 
 dont_put_skbuff_in_hw:
@@ -318,11 +346,12 @@ dont_put_skbuff_in_hw:
 	/* Check if we can use the hardware checksumming */
 	if (USE_HW_TCPUDP_CHECKSUM && (skb->protocol == htons(ETH_P_IP)) &&
 	    (ip_hdr(skb)->version == 4) && (ip_hdr(skb)->ihl == 5) &&
-	    ((ip_hdr(skb)->frag_off == 0) || (ip_hdr(skb)->frag_off == 1 << 14))
-	    && ((ip_hdr(skb)->protocol == IPPROTO_TCP)
-		|| (ip_hdr(skb)->protocol == IPPROTO_UDP))) {
+	    ((ip_hdr(skb)->frag_off == 0) || (ip_hdr(skb)->frag_off == htons(1 << 14)))
+	    && ((ip_hdr(skb)->protocol == IPPROTO_TCP) || (ip_hdr(skb)->protocol == IPPROTO_UDP))) {
 		/* Use hardware checksum calc */
 		pko_command.s.ipoffp1 = sizeof(struct ethhdr) + 1;
+		if (unlikely(priv->imode == CVMX_HELPER_INTERFACE_MODE_SRIO))
+			pko_command.s.ipoffp1 += 8;
 	}
 
 	if (USE_ASYNC_IOBDMA) {
@@ -354,17 +383,34 @@ dont_put_skbuff_in_hw:
 		}
 	}
 
-	if (pko_command.s.dontfree)
+	if (pko_command.s.dontfree) {
 		queue_type = QUEUE_WQE;
-	else
+	} else {
 		queue_type = QUEUE_HW;
+		if (buffers_being_recycled > 1) {
+			struct sk_buff *tskb, *nskb;
+			/*
+			 * We are committed to use hardware free, restore the
+			 * frag list to empty on the first SKB
+			 */
+			tskb = skb_shinfo(skb)->frag_list;
+			while (tskb) {
+				nskb = tskb->next;
+				cvm_oct_skb_prepare_for_reuse(tskb);
+				tskb = nskb;
+			}
+		}
+		cvm_oct_skb_prepare_for_reuse(skb);
+	}
 
 	if (queue_type == QUEUE_WQE) {
-		work = cvmx_fpa_alloc(CVMX_FPA_WQE_POOL);
-		if (unlikely(!work)) {
-			netdev_err(dev, "Failed WQE allocate\n");
-			queue_type = QUEUE_DROP;
-			goto skip_xmit;
+		if (!work) {
+			work = cvmx_fpa_alloc(CVMX_FPA_WQE_POOL);
+			if (unlikely(!work)) {
+				netdev_err(dev, "Failed WQE allocate\n");
+				queue_type = QUEUE_DROP;
+				goto skip_xmit;
+			}
 		}
 
 		pko_command.s.rsp = 1;
diff --git a/drivers/staging/octeon/ethernet.c b/drivers/staging/octeon/ethernet.c
index 4cb300f..a955533 100644
--- a/drivers/staging/octeon/ethernet.c
+++ b/drivers/staging/octeon/ethernet.c
@@ -444,7 +444,7 @@ int cvm_oct_common_init(struct net_device *dev)
 	}
 
 	if (priv->num_tx_queues != -1) {
-		dev->features |= NETIF_F_SG;
+		dev->features |= NETIF_F_SG | NETIF_F_FRAGLIST;
 		if (USE_HW_TCPUDP_CHECKSUM)
 			dev->features |= NETIF_F_IP_CSUM;
 	}
-- 
1.7.5.4

