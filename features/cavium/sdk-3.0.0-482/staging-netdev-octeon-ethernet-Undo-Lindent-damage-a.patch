From 997df0f13d2605fb778e584dda928b531b2ec194 Mon Sep 17 00:00:00 2001
From: David Daney <david.daney@cavium.com>
Date: Wed, 4 Jul 2012 17:03:54 -0700
Subject: [PATCH 116/337] staging/netdev: octeon-ethernet: Undo Lindent damage
 and more.

Based On SDK 3.0.0-482

... also fix some spelling and use:

/* multiline comment...

instead of:

/*
 * multiline comment...

Signed-off-by: David Daney <david.daney@cavium.com>
Signed-off-by: Corey Minyard <cminyard@mvista.com>
Signed-off-by: Jack Tan <jack.tan@windriver.com>
---
 drivers/staging/octeon/ethernet-rgmii.c |   88 ++++++++-----------------
 drivers/staging/octeon/ethernet-rx.c    |   89 ++++++++------------------
 drivers/staging/octeon/ethernet-spi.c   |  108 +++++++++---------------------
 drivers/staging/octeon/ethernet-tx.c    |   63 ++++++-------------
 drivers/staging/octeon/ethernet.c       |   11 +--
 5 files changed, 109 insertions(+), 250 deletions(-)

diff --git a/drivers/staging/octeon/ethernet-rgmii.c b/drivers/staging/octeon/ethernet-rgmii.c
index 7adea77..bf74113 100644
--- a/drivers/staging/octeon/ethernet-rgmii.c
+++ b/drivers/staging/octeon/ethernet-rgmii.c
@@ -55,8 +55,7 @@ static void cvm_oct_rgmii_poll(struct net_device *dev)
 
 	BUG_ON(in_interrupt());
 	if (use_global_register_lock) {
-		/*
-		 * Take the global register lock since we are going to
+		/* Take the global register lock since we are going to
 		 * touch registers that affect more than one port.
 		 */
 		spin_lock_irqsave(&global_register_lock, flags);
@@ -66,24 +65,18 @@ static void cvm_oct_rgmii_poll(struct net_device *dev)
 
 	link_info = cvmx_helper_link_get(priv->ipd_port);
 	if (link_info.u64 == priv->link_info) {
-
-		/*
-		 * If the 10Mbps preamble workaround is supported and we're
+		/* If the 10Mbps preamble workaround is supported and we're
 		 * at 10Mbps we may need to do some special checking.
 		 */
 		if (USE_10MBPS_PREAMBLE_WORKAROUND && (link_info.s.speed == 10)) {
-
-			/*
-			 * Read the GMXX_RXX_INT_REG[PCTERR] bit and
+			/* Read the GMXX_RXX_INT_REG[PCTERR] bit and
 			 * see if we are getting preamble errors.
 			 */
 			union cvmx_gmxx_rxx_int_reg gmxx_rxx_int_reg;
 			gmxx_rxx_int_reg.u64 =
 				cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(priv->interface_port, priv->interface));
 			if (gmxx_rxx_int_reg.s.pcterr) {
-
-				/*
-				 * We are getting preamble errors at
+				/* We are getting preamble errors at
 				 * 10Mbps.  Most likely the PHY is
 				 * giving us packets with mis aligned
 				 * preambles. In order to get these
@@ -95,16 +88,14 @@ static void cvm_oct_rgmii_poll(struct net_device *dev)
 
 				/* Disable preamble checking */
 				gmxx_rxx_frm_ctl.u64 =
-				    cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface));
+					cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface));
 				gmxx_rxx_frm_ctl.s.pre_chk = 0;
 				cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface),
 					       gmxx_rxx_frm_ctl.u64);
 
 				/* Disable FCS stripping */
-				ipd_sub_port_fcs.u64 =
-				    cvmx_read_csr(CVMX_IPD_SUB_PORT_FCS);
-				ipd_sub_port_fcs.s.port_bit &=
-				    0xffffffffull ^ (1ull << priv->ipd_port);
+				ipd_sub_port_fcs.u64 = cvmx_read_csr(CVMX_IPD_SUB_PORT_FCS);
+				ipd_sub_port_fcs.s.port_bit &= 0xffffffffull ^ (1ull << priv->ipd_port);
 				cvmx_write_csr(CVMX_IPD_SUB_PORT_FCS,
 					       ipd_sub_port_fcs.u64);
 
@@ -124,11 +115,11 @@ static void cvm_oct_rgmii_poll(struct net_device *dev)
 	}
 
 	/* If the 10Mbps preamble workaround is allowed we need to on
-	   preamble checking, FCS stripping, and clear error bits on
-	   every speed change. If errors occur during 10Mbps operation
-	   the above code will change this stuff */
+	 * preamble checking, FCS stripping, and clear error bits on
+	 * every speed change. If errors occur during 10Mbps operation
+	 * the above code will change this stuff.
+	*/
 	if (USE_10MBPS_PREAMBLE_WORKAROUND) {
-
 		union cvmx_gmxx_rxx_frm_ctl gmxx_rxx_frm_ctl;
 		union cvmx_ipd_sub_port_fcs ipd_sub_port_fcs;
 		union cvmx_gmxx_rxx_int_reg gmxx_rxx_int_reg;
@@ -145,7 +136,7 @@ static void cvm_oct_rgmii_poll(struct net_device *dev)
 		cvmx_write_csr(CVMX_IPD_SUB_PORT_FCS, ipd_sub_port_fcs.u64);
 		/* Clear any error bits */
 		gmxx_rxx_int_reg.u64 =
-		    cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(priv->interface_port, priv->interface));
+			cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(priv->interface_port, priv->interface));
 		cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(priv->interface_port, priv->interface),
 			       gmxx_rxx_int_reg.u64);
 	}
@@ -174,29 +165,21 @@ static irqreturn_t cvm_oct_rgmii_rml_interrupt(int cpl, void *dev_id)
 
 	/* Check and see if this interrupt was caused by the GMX0 block */
 	if (rsl_int_blocks.s.gmx0) {
-
 		int interface = 0;
 		/* Loop through every port of this interface */
 		for (index = 0;
 		     index < cvmx_helper_ports_on_interface(interface);
 		     index++) {
-
 			/* Read the GMX interrupt status bits */
 			union cvmx_gmxx_rxx_int_reg gmx_rx_int_reg;
-			gmx_rx_int_reg.u64 =
-			    cvmx_read_csr(CVMX_GMXX_RXX_INT_REG
-					  (index, interface));
-			gmx_rx_int_reg.u64 &=
-			    cvmx_read_csr(CVMX_GMXX_RXX_INT_EN
-					  (index, interface));
+			gmx_rx_int_reg.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(index, interface));
+			gmx_rx_int_reg.u64 &= cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(index, interface));
 			/* Poll the port if inband status changed */
 			if (gmx_rx_int_reg.s.phy_dupx
 			    || gmx_rx_int_reg.s.phy_link
 			    || gmx_rx_int_reg.s.phy_spd) {
-
 				struct net_device *dev =
-				    cvm_oct_device[cvmx_helper_get_ipd_port
-						   (interface, index)];
+					cvm_oct_device[cvmx_helper_get_ipd_port(interface, index)];
 				struct octeon_ethernet *priv = netdev_priv(dev);
 
 				if (dev && !atomic_read(&cvm_oct_poll_queue_stopping))
@@ -206,8 +189,7 @@ static irqreturn_t cvm_oct_rgmii_rml_interrupt(int cpl, void *dev_id)
 				gmx_rx_int_reg.s.phy_dupx = 1;
 				gmx_rx_int_reg.s.phy_link = 1;
 				gmx_rx_int_reg.s.phy_spd = 1;
-				cvmx_write_csr(CVMX_GMXX_RXX_INT_REG
-					       (index, interface),
+				cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(index, interface),
 					       gmx_rx_int_reg.u64);
 				return_status = IRQ_HANDLED;
 			}
@@ -216,29 +198,23 @@ static irqreturn_t cvm_oct_rgmii_rml_interrupt(int cpl, void *dev_id)
 
 	/* Check and see if this interrupt was caused by the GMX1 block */
 	if (rsl_int_blocks.s.gmx1) {
-
 		int interface = 1;
 		/* Loop through every port of this interface */
 		for (index = 0;
 		     index < cvmx_helper_ports_on_interface(interface);
 		     index++) {
-
 			/* Read the GMX interrupt status bits */
 			union cvmx_gmxx_rxx_int_reg gmx_rx_int_reg;
 			gmx_rx_int_reg.u64 =
-			    cvmx_read_csr(CVMX_GMXX_RXX_INT_REG
-					  (index, interface));
+				cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(index, interface));
 			gmx_rx_int_reg.u64 &=
-			    cvmx_read_csr(CVMX_GMXX_RXX_INT_EN
-					  (index, interface));
+				cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(index, interface));
 			/* Poll the port if inband status changed */
 			if (gmx_rx_int_reg.s.phy_dupx
 			    || gmx_rx_int_reg.s.phy_link
 			    || gmx_rx_int_reg.s.phy_spd) {
-
 				struct net_device *dev =
-				    cvm_oct_device[cvmx_helper_get_ipd_port
-						   (interface, index)];
+					cvm_oct_device[cvmx_helper_get_ipd_port(interface, index)];
 				struct octeon_ethernet *priv = netdev_priv(dev);
 
 				if (dev && !atomic_read(&cvm_oct_poll_queue_stopping))
@@ -248,8 +224,7 @@ static irqreturn_t cvm_oct_rgmii_rml_interrupt(int cpl, void *dev_id)
 				gmx_rx_int_reg.s.phy_dupx = 1;
 				gmx_rx_int_reg.s.phy_link = 1;
 				gmx_rx_int_reg.s.phy_spd = 1;
-				cvmx_write_csr(CVMX_GMXX_RXX_INT_REG
-					       (index, interface),
+				cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(index, interface),
 					       gmx_rx_int_reg.u64);
 				return_status = IRQ_HANDLED;
 			}
@@ -297,8 +272,7 @@ int cvm_oct_rgmii_open(struct net_device *dev)
 	}
 
 	INIT_WORK(&priv->port_work, cvm_oct_rgmii_immediate_poll);
-	/*
-	 * Due to GMX errata in CN3XXX series chips, it is necessary
+	/* Due to GMX errata in CN3XXX series chips, it is necessary
 	 * to take the link down immediately when the PHY changes
 	 * state. In order to do this we call the poll function every
 	 * time the RGMII inband status changes.  This may cause
@@ -313,23 +287,19 @@ int cvm_oct_rgmii_open(struct net_device *dev)
 	}
 	number_rgmii_ports++;
 
-	/*
-	 * Only true RGMII ports need to be polled. In GMII mode, port
+	/* Only true RGMII ports need to be polled. In GMII mode, port
 	 * 0 is really a RGMII port.
 	 */
 	if (((priv->imode == CVMX_HELPER_INTERFACE_MODE_GMII)
 	     && (priv->ipd_port == 0))
 	    || (priv->imode == CVMX_HELPER_INTERFACE_MODE_RGMII)) {
-
 		if (!octeon_is_simulation()) {
-
 			union cvmx_gmxx_rxx_int_en gmx_rx_int_en;
-			/*
-			 * Enable interrupts on inband status changes
+			/* Enable interrupts on inband status changes
 			 * for this port.
 			 */
 			gmx_rx_int_en.u64 =
-			    cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(priv->interface_port, priv->interface));
+				cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(priv->interface_port, priv->interface));
 			gmx_rx_int_en.s.phy_dupx = 1;
 			gmx_rx_int_en.s.phy_link = 1;
 			gmx_rx_int_en.s.phy_spd = 1;
@@ -350,23 +320,19 @@ int cvm_oct_rgmii_stop(struct net_device *dev)
 	gmx_cfg.s.en = 0;
 	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface), gmx_cfg.u64);
 
-	/*
-	 * Only true RGMII ports need to be polled. In GMII mode, port
+	/* Only true RGMII ports need to be polled. In GMII mode, port
 	 * 0 is really a RGMII port.
 	 */
 	if (((priv->imode == CVMX_HELPER_INTERFACE_MODE_GMII)
 	     && (priv->ipd_port == 0))
 	    || (priv->imode == CVMX_HELPER_INTERFACE_MODE_RGMII)) {
-
 		if (!octeon_is_simulation()) {
-
 			union cvmx_gmxx_rxx_int_en gmx_rx_int_en;
-			/*
-			 * Disable interrupts on inband status changes
+			/* Disable interrupts on inband status changes
 			 * for this port.
 			 */
 			gmx_rx_int_en.u64 =
-			    cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(priv->interface_port, priv->interface));
+				cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(priv->interface_port, priv->interface));
 			gmx_rx_int_en.s.phy_dupx = 0;
 			gmx_rx_int_en.s.phy_link = 0;
 			gmx_rx_int_en.s.phy_spd = 0;
diff --git a/drivers/staging/octeon/ethernet-rx.c b/drivers/staging/octeon/ethernet-rx.c
index f0bce44d..649161b 100644
--- a/drivers/staging/octeon/ethernet-rx.c
+++ b/drivers/staging/octeon/ethernet-rx.c
@@ -66,14 +66,12 @@ static struct cvm_napi_wrapper cvm_oct_napi[NR_CPUS] __cacheline_aligned_in_smp;
 
 struct cvm_oct_core_state {
 	int baseline_cores;
-	/*
-	 * We want to read this without having to acquire the lock,
+	/* We want to read this without having to acquire the lock,
 	 * make it volatile so we are likely to get a fairly current
 	 * value.
 	 */
 	volatile int active_cores;
-	/*
-	 * cvm_napi_wrapper.available and active_cores must be kept
+	/* cvm_napi_wrapper.available and active_cores must be kept
 	 * consistent with this lock.
 	 */
 	spinlock_t lock;
@@ -135,8 +133,7 @@ static void cvm_oct_no_more_work(struct napi_struct *napi)
 	spin_unlock_irqrestore(&core_state.lock, flags);
 
 	if (current_active == 0) {
-		/*
-		 * No more CPUs doing processing, enable interrupts so
+		/* No more CPUs doing processing, enable interrupts so
 		 * we can start processing again when there is
 		 * something to do.
 		 */
@@ -144,8 +141,7 @@ static void cvm_oct_no_more_work(struct napi_struct *napi)
 		int_thr.u64 = 0;
 		int_thr.s.iq_thr = 1;
 		int_thr.s.ds_thr = 1;
-		/*
-		 * Enable POW interrupt when our port has at
+		/* Enable POW interrupt when our port has at
 		 * least one packet.
 		 */
 		cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group),
@@ -199,19 +195,16 @@ static irqreturn_t cvm_oct_do_interrupt(int cpl, void *dev_id)
 static inline int cvm_oct_check_rcv_error(cvmx_wqe_t *work)
 {
 	if ((work->word2.snoip.err_code == 10) && (work->word1.len <= 64)) {
-		/*
-		 * Ignore length errors on min size packets. Some
+		/* Ignore length errors on min size packets. Some
 		 * equipment incorrectly pads packets to 64+4FCS
 		 * instead of 60+4FCS.  Note these packets still get
 		 * counted as frame errors.
 		 */
 	} else
-	    if (USE_10MBPS_PREAMBLE_WORKAROUND
-		&& ((work->word2.snoip.err_code == 5)
-		    || (work->word2.snoip.err_code == 7))) {
-
-		/*
-		 * We received a packet with either an alignment error
+	    if (USE_10MBPS_PREAMBLE_WORKAROUND &&
+		((work->word2.snoip.err_code == 5) ||
+		 (work->word2.snoip.err_code == 7))) {
+		/* We received a packet with either an alignment error
 		 * or a FCS error. This may be signalling that we are
 		 * running 10Mbps with GMXX_RXX_FRM_CTL[PRE_CHK}
 		 * off. If this is the case we need to parse the
@@ -221,8 +214,7 @@ static inline int cvm_oct_check_rcv_error(cvmx_wqe_t *work)
 		int interface = cvmx_helper_get_interface_num(work->word1.cn38xx.ipprt);
 		int index = cvmx_helper_get_interface_index_num(work->word1.cn38xx.ipprt);
 		union cvmx_gmxx_rxx_frm_ctl gmxx_rxx_frm_ctl;
-		gmxx_rxx_frm_ctl.u64 =
-		    cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface));
+		gmxx_rxx_frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface));
 		if (gmxx_rxx_frm_ctl.s.pre_chk == 0) {
 
 			uint8_t *ptr = phys_to_virt(work->packet_ptr.s.addr);
@@ -236,15 +228,9 @@ static inline int cvm_oct_check_rcv_error(cvmx_wqe_t *work)
 			}
 
 			if (*ptr == 0xd5) {
-				/*
-				  printk_ratelimited("Port %d received 0xd5 preamble\n", work->ipprt);
-				 */
 				work->packet_ptr.s.addr += i + 1;
 				work->word1.len -= i + 5;
 			} else if ((*ptr & 0xf) == 0xd) {
-				/*
-				  printk_ratelimited("Port %d received 0x?d preamble\n", work->ipprt);
-				 */
 				work->packet_ptr.s.addr += i;
 				work->word1.len -= i + 4;
 				for (i = 0; i < work->word1.len; i++) {
@@ -254,12 +240,8 @@ static inline int cvm_oct_check_rcv_error(cvmx_wqe_t *work)
 					ptr++;
 				}
 			} else {
-				printk_ratelimited("Port %d unknown preamble, packet "
-						   "dropped\n",
+				printk_ratelimited("Port %d unknown preamble, packet dropped\n",
 						   work->word1.cn38xx.ipprt);
-				/*
-				   cvmx_helper_dump_packet(work);
-				 */
 				cvm_oct_free_work(work);
 				return 1;
 			}
@@ -344,8 +326,7 @@ static int cvm_oct_napi_poll(struct napi_struct *napi, int budget)
 		}
 
 		if (rx_count == 0) {
-			/*
-			 * First time through, see if there is enough
+			/* First time through, see if there is enough
 			 * work waiting to merit waking another
 			 * CPU.
 			 */
@@ -358,8 +339,7 @@ static int cvm_oct_napi_poll(struct napi_struct *napi, int budget)
 				cvm_oct_enable_one_cpu();
 		}
 
-		/*
-		 * If WORD2[SOFTWARE] then this WQE is a complete for
+		/* If WORD2[SOFTWARE] then this WQE is a complete for
 		 * a TX packet.
 		 */
 		if (work->word2.s.software) {
@@ -374,8 +354,7 @@ static int cvm_oct_napi_poll(struct napi_struct *napi, int budget)
 
 			cvmx_fpa_free(work, CVMX_FPA_WQE_POOL, DONT_WRITEBACK(1));
 
-			/*
-			 * We are done with this one, adjust the queue
+			/* We are done with this one, adjust the queue
 			 * depth.
 			 */
 			cvmx_fau_atomic_add32(priv->tx_queue[packet_qos].fau, -1);
@@ -397,8 +376,7 @@ static int cvm_oct_napi_poll(struct napi_struct *napi, int budget)
 		}
 
 		packet_len = work->word1.len;
-		/*
-		 * We can only use the zero copy path if skbuffs are
+		/* We can only use the zero copy path if skbuffs are
 		 * in the FPA pool and the packet fits in a single
 		 * buffer.
 		 */
@@ -418,8 +396,7 @@ static int cvm_oct_napi_poll(struct napi_struct *napi, int budget)
 				skb_frag_list_init(skb);
 				/* Multi-segment packet. */
 				for (;;) {
-					/*
-					 * Octeon Errata PKI-100: The segment size is
+					/* Octeon Errata PKI-100: The segment size is
 					 * wrong. Until it is fixed, calculate the
 					 * segment size based on the packet pool
 					 * buffer size. When it is fixed, the
@@ -456,8 +433,7 @@ static int cvm_oct_napi_poll(struct napi_struct *napi, int budget)
 			}
 			packet_copied = false;
 		} else {
-			/*
-			 * We have to copy the packet. First allocate
+			/* We have to copy the packet. First allocate
 			 * an skbuff for it.
 			 */
 			skb = dev_alloc_skb(packet_len);
@@ -468,16 +444,14 @@ static int cvm_oct_napi_poll(struct napi_struct *napi, int budget)
 				continue;
 			}
 
-			/*
-			 * Check if we've received a packet that was
+			/* Check if we've received a packet that was
 			 * entirely stored in the work entry.
 			 */
 			if (unlikely(work->word2.s.bufs == 0)) {
 				uint8_t *ptr = work->packet_data;
 
 				if (likely(!work->word2.s.not_IP)) {
-					/*
-					 * The beginning of the packet
+					/* The beginning of the packet
 					 * moves for IP packets.
 					 */
 					if (work->word2.s.is_v6)
@@ -495,8 +469,7 @@ static int cvm_oct_napi_poll(struct napi_struct *napi, int budget)
 					union cvmx_buf_ptr next_ptr =
 					    *(union cvmx_buf_ptr *)phys_to_virt(segment_ptr.s.addr - 8);
 
-			/*
-			 * Octeon Errata PKI-100: The segment size is
+			/* Octeon Errata PKI-100: The segment size is
 			 * wrong. Until it is fixed, calculate the
 			 * segment size based on the packet pool
 			 * buffer size. When it is fixed, the
@@ -506,8 +479,7 @@ static int cvm_oct_napi_poll(struct napi_struct *napi, int budget)
 			 */
 					int segment_size = CVMX_FPA_PACKET_POOL_SIZE -
 						(segment_ptr.s.addr - (((segment_ptr.s.addr >> 7) - segment_ptr.s.back) << 7));
-					/*
-					 * Don't copy more than what
+					/* Don't copy more than what
 					 * is left in the packet.
 					 */
 					if (segment_size > packet_len)
@@ -528,8 +500,7 @@ static int cvm_oct_napi_poll(struct napi_struct *napi, int budget)
 			struct net_device *dev = cvm_oct_device[work->word1.cn38xx.ipprt];
 			struct octeon_ethernet *priv = netdev_priv(dev);
 
-			/*
-			 * Only accept packets for devices that are
+			/* Only accept packets for devices that are
 			 * currently up.
 			 */
 			if (likely(dev->flags & IFF_UP)) {
@@ -556,10 +527,6 @@ static int cvm_oct_napi_poll(struct napi_struct *napi, int budget)
 				rx_count++;
 			} else {
 				/* Drop any packet received for a device that isn't up */
-				/*
-				  printk_ratelimited("%s: Device not up, packet dropped\n",
-					   dev->name);
-				*/
 #ifdef CONFIG_64BIT
 				atomic64_add(1, (atomic64_t *)&priv->stats.rx_dropped);
 #else
@@ -568,21 +535,18 @@ static int cvm_oct_napi_poll(struct napi_struct *napi, int budget)
 				dev_kfree_skb_irq(skb);
 			}
 		} else {
-			/*
-			 * Drop any packet received for a device that
+			/* Drop any packet received for a device that
 			 * doesn't exist.
 			 */
 			printk_ratelimited("Port %d not controlled by Linux, packet dropped\n",
 					   work->word1.cn38xx.ipprt);
 			dev_kfree_skb_irq(skb);
 		}
-		/*
-		 * Check to see if the skbuff and work share the same
+		/* Check to see if the skbuff and work share the same
 		 * packet buffer.
 		 */
 		if (USE_SKBUFFS_IN_HW && likely(!packet_copied)) {
-			/*
-			 * This buffer needs to be replaced, increment
+			/* This buffer needs to be replaced, increment
 			 * the number of buffers we need to free by
 			 * one.
 			 */
@@ -650,8 +614,7 @@ void cvm_oct_rx_initialize(void)
 			       cvm_oct_napi_poll, rx_napi_weight);
 		napi_enable(&cvm_oct_napi[i].napi);
 	}
-	/*
-	 * Before interrupts are enabled, no RX processing will occur,
+	/* Before interrupts are enabled, no RX processing will occur,
 	 * so we can initialize all those things out side of the
 	 * lock.
 	 */
diff --git a/drivers/staging/octeon/ethernet-spi.c b/drivers/staging/octeon/ethernet-spi.c
index 193685d..a1c05a6 100644
--- a/drivers/staging/octeon/ethernet-spi.c
+++ b/drivers/staging/octeon/ethernet-spi.c
@@ -51,43 +51,33 @@ static irqreturn_t cvm_oct_spi_rml_interrupt(int cpl, void *dev_id)
 	/* Check and see if this interrupt was caused by the GMX block */
 	rsl_int_blocks.u64 = cvmx_read_csr(CVMX_NPI_RSL_INT_BLOCKS);
 	if (rsl_int_blocks.s.spx1) {	/* 19 - SPX1_INT_REG & STX1_INT_REG */
-
 		union cvmx_spxx_int_reg spx_int_reg;
 		union cvmx_stxx_int_reg stx_int_reg;
 
 		spx_int_reg.u64 = cvmx_read_csr(CVMX_SPXX_INT_REG(1));
 		cvmx_write_csr(CVMX_SPXX_INT_REG(1), spx_int_reg.u64);
 		if (!need_retrain[1]) {
-
 			spx_int_reg.u64 &= cvmx_read_csr(CVMX_SPXX_INT_MSK(1));
 			if (spx_int_reg.s.spf)
 				pr_err("SPI1: SRX Spi4 interface down\n");
 			if (spx_int_reg.s.calerr)
-				pr_err("SPI1: SRX Spi4 Calendar table "
-				       "parity error\n");
+				pr_err("SPI1: SRX Spi4 Calendar table parity error\n");
 			if (spx_int_reg.s.syncerr)
-				pr_err("SPI1: SRX Consecutive Spi4 DIP4 "
-				       "errors have exceeded "
-				       "SPX_ERR_CTL[ERRCNT]\n");
+				pr_err("SPI1: SRX Consecutive Spi4 DIP4 errors have exceeded SPX_ERR_CTL[ERRCNT]\n");
 			if (spx_int_reg.s.diperr)
 				pr_err("SPI1: SRX Spi4 DIP4 error\n");
 			if (spx_int_reg.s.tpaovr)
-				pr_err("SPI1: SRX Selected port has hit "
-				       "TPA overflow\n");
+				pr_err("SPI1: SRX Selected port has hit TPA overflow\n");
 			if (spx_int_reg.s.rsverr)
-				pr_err("SPI1: SRX Spi4 reserved control "
-				       "word detected\n");
+				pr_err("SPI1: SRX Spi4 reserved control word detected\n");
 			if (spx_int_reg.s.drwnng)
-				pr_err("SPI1: SRX Spi4 receive FIFO "
-				       "drowning/overflow\n");
+				pr_err("SPI1: SRX Spi4 receive FIFO drowning/overflow\n");
 			if (spx_int_reg.s.clserr)
-				pr_err("SPI1: SRX Spi4 packet closed on "
-				       "non-16B alignment without EOP\n");
+				pr_err("SPI1: SRX Spi4 packet closed on non-16B alignment without EOP\n");
 			if (spx_int_reg.s.spiovr)
 				pr_err("SPI1: SRX Spi4 async FIFO overflow\n");
 			if (spx_int_reg.s.abnorm)
-				pr_err("SPI1: SRX Abnormal packet "
-				       "termination (ERR bit)\n");
+				pr_err("SPI1: SRX Abnormal packet termination (ERR bit)\n");
 			if (spx_int_reg.s.prtnxa)
 				pr_err("SPI1: SRX Port out of range\n");
 		}
@@ -95,34 +85,25 @@ static irqreturn_t cvm_oct_spi_rml_interrupt(int cpl, void *dev_id)
 		stx_int_reg.u64 = cvmx_read_csr(CVMX_STXX_INT_REG(1));
 		cvmx_write_csr(CVMX_STXX_INT_REG(1), stx_int_reg.u64);
 		if (!need_retrain[1]) {
-
 			stx_int_reg.u64 &= cvmx_read_csr(CVMX_STXX_INT_MSK(1));
 			if (stx_int_reg.s.syncerr)
-				pr_err("SPI1: STX Interface encountered a "
-				       "fatal error\n");
+				pr_err("SPI1: STX Interface encountered a fatal error\n");
 			if (stx_int_reg.s.frmerr)
-				pr_err("SPI1: STX FRMCNT has exceeded "
-				       "STX_DIP_CNT[MAXFRM]\n");
+				pr_err("SPI1: STX FRMCNT has exceeded STX_DIP_CNT[MAXFRM]\n");
 			if (stx_int_reg.s.unxfrm)
-				pr_err("SPI1: STX Unexpected framing "
-				       "sequence\n");
+				pr_err("SPI1: STX Unexpected framing sequence\n");
 			if (stx_int_reg.s.nosync)
-				pr_err("SPI1: STX ERRCNT has exceeded "
-				       "STX_DIP_CNT[MAXDIP]\n");
+				pr_err("SPI1: STX ERRCNT has exceeded STX_DIP_CNT[MAXDIP]\n");
 			if (stx_int_reg.s.diperr)
-				pr_err("SPI1: STX DIP2 error on the Spi4 "
-				       "Status channel\n");
+				pr_err("SPI1: STX DIP2 error on the Spi4 Status channel\n");
 			if (stx_int_reg.s.datovr)
 				pr_err("SPI1: STX Spi4 FIFO overflow error\n");
 			if (stx_int_reg.s.ovrbst)
-				pr_err("SPI1: STX Transmit packet burst "
-				       "too big\n");
+				pr_err("SPI1: STX Transmit packet burst too big\n");
 			if (stx_int_reg.s.calpar1)
-				pr_err("SPI1: STX Calendar Table Parity "
-				       "Error Bank1\n");
+				pr_err("SPI1: STX Calendar Table Parity Error Bank1\n");
 			if (stx_int_reg.s.calpar0)
-				pr_err("SPI1: STX Calendar Table Parity "
-				       "Error Bank0\n");
+				pr_err("SPI1: STX Calendar Table Parity Error Bank0\n");
 		}
 
 		cvmx_write_csr(CVMX_SPXX_INT_MSK(1), 0);
@@ -138,36 +119,27 @@ static irqreturn_t cvm_oct_spi_rml_interrupt(int cpl, void *dev_id)
 		spx_int_reg.u64 = cvmx_read_csr(CVMX_SPXX_INT_REG(0));
 		cvmx_write_csr(CVMX_SPXX_INT_REG(0), spx_int_reg.u64);
 		if (!need_retrain[0]) {
-
 			spx_int_reg.u64 &= cvmx_read_csr(CVMX_SPXX_INT_MSK(0));
 			if (spx_int_reg.s.spf)
 				pr_err("SPI0: SRX Spi4 interface down\n");
 			if (spx_int_reg.s.calerr)
-				pr_err("SPI0: SRX Spi4 Calendar table "
-				       "parity error\n");
+				pr_err("SPI0: SRX Spi4 Calendar table parity error\n");
 			if (spx_int_reg.s.syncerr)
-				pr_err("SPI0: SRX Consecutive Spi4 DIP4 "
-				       "errors have exceeded "
-				       "SPX_ERR_CTL[ERRCNT]\n");
+				pr_err("SPI0: SRX Consecutive Spi4 DIP4 errors have exceeded SPX_ERR_CTL[ERRCNT]\n");
 			if (spx_int_reg.s.diperr)
 				pr_err("SPI0: SRX Spi4 DIP4 error\n");
 			if (spx_int_reg.s.tpaovr)
-				pr_err("SPI0: SRX Selected port has hit "
-				       "TPA overflow\n");
+				pr_err("SPI0: SRX Selected port has hit TPA overflow\n");
 			if (spx_int_reg.s.rsverr)
-				pr_err("SPI0: SRX Spi4 reserved control "
-				       "word detected\n");
+				pr_err("SPI0: SRX Spi4 reserved control word detected\n");
 			if (spx_int_reg.s.drwnng)
-				pr_err("SPI0: SRX Spi4 receive FIFO "
-				       "drowning/overflow\n");
+				pr_err("SPI0: SRX Spi4 receive FIFO drowning/overflow\n");
 			if (spx_int_reg.s.clserr)
-				pr_err("SPI0: SRX Spi4 packet closed on "
-				       "non-16B alignment without EOP\n");
+				pr_err("SPI0: SRX Spi4 packet closed on non-16B alignment without EOP\n");
 			if (spx_int_reg.s.spiovr)
 				pr_err("SPI0: SRX Spi4 async FIFO overflow\n");
 			if (spx_int_reg.s.abnorm)
-				pr_err("SPI0: SRX Abnormal packet "
-				       "termination (ERR bit)\n");
+				pr_err("SPI0: SRX Abnormal packet termination (ERR bit)\n");
 			if (spx_int_reg.s.prtnxa)
 				pr_err("SPI0: SRX Port out of range\n");
 		}
@@ -175,34 +147,25 @@ static irqreturn_t cvm_oct_spi_rml_interrupt(int cpl, void *dev_id)
 		stx_int_reg.u64 = cvmx_read_csr(CVMX_STXX_INT_REG(0));
 		cvmx_write_csr(CVMX_STXX_INT_REG(0), stx_int_reg.u64);
 		if (!need_retrain[0]) {
-
 			stx_int_reg.u64 &= cvmx_read_csr(CVMX_STXX_INT_MSK(0));
 			if (stx_int_reg.s.syncerr)
-				pr_err("SPI0: STX Interface encountered a "
-				       "fatal error\n");
+				pr_err("SPI0: STX Interface encountered a fatal error\n");
 			if (stx_int_reg.s.frmerr)
-				pr_err("SPI0: STX FRMCNT has exceeded "
-				       "STX_DIP_CNT[MAXFRM]\n");
+				pr_err("SPI0: STX FRMCNT has exceeded STX_DIP_CNT[MAXFRM]\n");
 			if (stx_int_reg.s.unxfrm)
-				pr_err("SPI0: STX Unexpected framing "
-				       "sequence\n");
+				pr_err("SPI0: STX Unexpected framing sequence\n");
 			if (stx_int_reg.s.nosync)
-				pr_err("SPI0: STX ERRCNT has exceeded "
-				       "STX_DIP_CNT[MAXDIP]\n");
+				pr_err("SPI0: STX ERRCNT has exceeded STX_DIP_CNT[MAXDIP]\n");
 			if (stx_int_reg.s.diperr)
-				pr_err("SPI0: STX DIP2 error on the Spi4 "
-				       "Status channel\n");
+				pr_err("SPI0: STX DIP2 error on the Spi4 Status channel\n");
 			if (stx_int_reg.s.datovr)
 				pr_err("SPI0: STX Spi4 FIFO overflow error\n");
 			if (stx_int_reg.s.ovrbst)
-				pr_err("SPI0: STX Transmit packet burst "
-				       "too big\n");
+				pr_err("SPI0: STX Transmit packet burst too big\n");
 			if (stx_int_reg.s.calpar1)
-				pr_err("SPI0: STX Calendar Table Parity "
-				       "Error Bank1\n");
+				pr_err("SPI0: STX Calendar Table Parity Error Bank1\n");
 			if (stx_int_reg.s.calpar0)
-				pr_err("SPI0: STX Calendar Table Parity "
-				       "Error Bank0\n");
+				pr_err("SPI0: STX Calendar Table Parity Error Bank0\n");
 		}
 
 		cvmx_write_csr(CVMX_SPXX_INT_MSK(0), 0);
@@ -251,9 +214,7 @@ static void cvm_oct_spi_poll(struct net_device *dev)
 	int interface;
 
 	for (interface = 0; interface < 2; interface++) {
-
 		if ((priv->ipd_port == interface * 16) && need_retrain[interface]) {
-
 			if (cvmx_spi_restart_interface
 			    (interface, CVMX_SPI_MODE_DUPLEX, 10) == 0) {
 				need_retrain[interface] = 0;
@@ -261,8 +222,7 @@ static void cvm_oct_spi_poll(struct net_device *dev)
 			}
 		}
 
-		/*
-		 * The SPI4000 TWSI interface is very slow. In order
+		/* The SPI4000 TWSI interface is very slow. In order
 		 * not to bring the system to a crawl, we only poll a
 		 * single port every second. This means negotiation
 		 * speed changes take up to 10 seconds, but at least
@@ -270,13 +230,11 @@ static void cvm_oct_spi_poll(struct net_device *dev)
 		 * TWSI.
 		 */
 		if (priv->ipd_port == spi4000_port) {
-			/*
-			 * This function does nothing if it is called on an
+			/* This function does nothing if it is called on an
 			 * interface without a SPI4000.
 			 */
 			cvmx_spi4000_check_speed(interface, priv->ipd_port);
-			/*
-			 * Normal ordering increments. By decrementing
+			/* Normal ordering increments. By decrementing
 			 * we only match once per iteration.
 			 */
 			spi4000_port--;
diff --git a/drivers/staging/octeon/ethernet-tx.c b/drivers/staging/octeon/ethernet-tx.c
index 27c56e4..0a84cf8 100644
--- a/drivers/staging/octeon/ethernet-tx.c
+++ b/drivers/staging/octeon/ethernet-tx.c
@@ -153,8 +153,7 @@ int cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev)
 	unsigned long flags;
 	cvmx_wqe_t *work = NULL;
 
-	/*
-	 * Prefetch the private data structure.  It is larger that one
+	/* Prefetch the private data structure.  It is larger than one
 	 * cache line.
 	 */
 	prefetch(priv);
@@ -165,8 +164,7 @@ int cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev)
 		old_scratch = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
 		old_scratch2 = cvmx_scratch_read64(CVMX_SCR_SCRATCH + 8);
 
-		/*
-		 * Fetch and increment the number of packets to be
+		/* Fetch and increment the number of packets to be
 		 * freed.
 		 */
 		cvmx_fau_async_fetch_and_add32(CVMX_SCR_SCRATCH + 8,
@@ -177,8 +175,7 @@ int cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev)
 #ifdef CVM_OCT_LOCKLESS
 	qos = cvmx_get_core_num();
 #else
-	/*
-	 * The check on CVMX_PKO_QUEUES_PER_PORT_* is designed to
+	/* The check on CVMX_PKO_QUEUES_PER_PORT_* is designed to
 	 * completely remove "qos" in the event neither interface
 	 * supports multiple queues per port.
 	 */
@@ -200,8 +197,7 @@ int cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev)
 	if (skb_has_frag_list(skb))
 		skb_walk_frags(skb, skb_tmp)
 			frag_count++;
-	/*
-	 * We have space for 12 segment pointers, If there will be
+	/* We have space for 12 segment pointers, If there will be
 	 * more than that, we must linearize.  The count is: 1 (base
 	 * SKB) + frag_count + nr_frags.
 	 */
@@ -213,8 +209,7 @@ int cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev)
 		frag_count = 0;
 	}
 
-	/*
-	 * The CN3XXX series of parts has an errata (GMX-401) which
+	/* The CN3XXX series of parts has an errata (GMX-401) which
 	 * causes the GMX block to hang if a collision occurs towards
 	 * the end of a <68 byte packet. As a workaround for this, we
 	 * pad packets to be 68 bytes whenever we are in half duplex
@@ -227,21 +222,19 @@ int cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev)
 
 		if (priv->interface < 2) {
 			/* We only need to pad packet in half duplex mode */
-			gmx_prt_cfg.u64 =
-			    cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
+			gmx_prt_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
 			if (gmx_prt_cfg.s.duplex == 0) {
 				int add_bytes = 64 - skb->len;
-				if ((skb_tail_pointer(skb) + add_bytes) <=
-				    skb_end_pointer(skb))
-					memset(__skb_put(skb, add_bytes), 0,
-					       add_bytes);
+				if ((skb_tail_pointer(skb) + add_bytes) <= skb_end_pointer(skb))
+					memset(__skb_put(skb, add_bytes), 0, add_bytes);
 			}
 		}
 	}
 
 	/* Build the PKO command */
 	pko_command.u64 = 0;
-	pko_command.s.n2 = 1;	/* Don't pollute L2 with the outgoing packet */
+	/* Don't pollute L2 with the outgoing packet */
+	pko_command.s.n2 = 1;
 	pko_command.s.segs = 1;
 	pko_command.s.total_bytes = skb->len;
 	pko_command.s.size0 = CVMX_FAU_OP_SIZE_32;
@@ -306,8 +299,7 @@ int cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev)
 			goto dont_put_skbuff_in_hw;
 	}
 
-	/*
-	 * See if we can put this skb in the FPA pool. Any strange
+	/* See if we can put this skb in the FPA pool. Any strange
 	 * behavior from the Linux networking stack will most likely
 	 * be caused by a bug in the following code. If some field is
 	 * in use by the network stack and get carried over when a
@@ -319,22 +311,13 @@ int cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev)
 #if REUSE_SKBUFFS_WITHOUT_FREE
 	if (!cvm_oct_skb_ok_for_reuse(skb))
 		goto dont_put_skbuff_in_hw;
-
-	if (unlikely(skb_header_cloned(skb))) {
-		/*
-		   printk("TX buffer header has been cloned\n");
-		 */
-		goto dont_put_skbuff_in_hw;
-	}
-	if (unlikely(skb->destructor)) {
-		/*
-		   printk("TX buffer has a destructor\n");
-		 */
+	if (unlikely(skb_header_cloned(skb)))
+ 		goto dont_put_skbuff_in_hw;
+	if (unlikely(skb->destructor))
 		goto dont_put_skbuff_in_hw;
-	}
 
-	/*
-	 * We can use this buffer in the FPA.  We don't need the FAU
+
+	/* We can use this buffer in the FPA.  We don't need the FAU
 	 * update anymore
 	 */
 	pko_command.s.dontfree = 0;
@@ -365,8 +348,7 @@ dont_put_skbuff_in_hw:
 		buffers_to_free = cvmx_fau_fetch_and_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);
 	}
 
-	/*
-	 * If we're sending faster than the receive can free them then
+	/* If we're sending faster than the receive can free them then
 	 * don't do the HW free.
 	 */
 	if (unlikely(buffers_to_free < -100))
@@ -389,8 +371,7 @@ dont_put_skbuff_in_hw:
 		queue_type = QUEUE_HW;
 		if (buffers_being_recycled > 1) {
 			struct sk_buff *tskb, *nskb;
-			/*
-			 * We are committed to use hardware free, restore the
+			/* We are committed to use hardware free, restore the
 			 * frag list to empty on the first SKB
 			 */
 			tskb = skb_shinfo(skb)->frag_list;
@@ -415,8 +396,7 @@ dont_put_skbuff_in_hw:
 
 		pko_command.s.rsp = 1;
 		pko_command.s.wqp = 1;
-		/*
-		 * work->unused will carry the qos for this packet,
+		/* work->unused will carry the qos for this packet,
 		 * this allows us to find the proper FAU when freeing
 		 * the packet.  We decrement the FAU when the WQE is
 		 * replaced in the pool.
@@ -488,11 +468,6 @@ skip_xmit:
 	return NETDEV_TX_OK;
 }
 
-/**
- * cvm_oct_tx_shutdown_dev - free all skb that are currently queued for TX.
- * @dev:    Device being shutdown
- *
- */
 void cvm_oct_tx_shutdown_dev(struct net_device *dev)
 {
 }
diff --git a/drivers/staging/octeon/ethernet.c b/drivers/staging/octeon/ethernet.c
index 8b196fd..c941932 100644
--- a/drivers/staging/octeon/ethernet.c
+++ b/drivers/staging/octeon/ethernet.c
@@ -388,7 +388,7 @@ static int cvm_oct_set_mac_filter(struct net_device *dev)
 		uint8_t *ptr = dev->dev_addr;
 		uint64_t mac = 0;
 		for (i = 0; i < 6; i++)
-			mac = (mac << 8) | (uint64_t)ptr[i];
+		mac = (mac << 8) | (uint64_t)ptr[i];
 
 		gmx_cfg.u64 =
 		    cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
@@ -599,11 +599,9 @@ static int __devinit cvm_oct_probe(struct platform_device *pdev)
 		     port < cvmx_helper_get_ipd_port(interface, num_ports);
 		     port++) {
 			union cvmx_pip_prt_tagx pip_prt_tagx;
-			pip_prt_tagx.u64 =
-			    cvmx_read_csr(CVMX_PIP_PRT_TAGX(port));
+			pip_prt_tagx.u64 = cvmx_read_csr(CVMX_PIP_PRT_TAGX(port));
 			pip_prt_tagx.s.grp = pow_receive_group;
-			cvmx_write_csr(CVMX_PIP_PRT_TAGX(port),
-				       pip_prt_tagx.u64);
+			cvmx_write_csr(CVMX_PIP_PRT_TAGX(port), pip_prt_tagx.u64);
 		}
 	}
 
@@ -617,8 +615,7 @@ static int __devinit cvm_oct_probe(struct platform_device *pdev)
 
 	num_interfaces = cvmx_helper_get_number_of_interfaces();
 	for (interface = 0; interface < num_interfaces; interface++) {
-		cvmx_helper_interface_mode_t imode =
-		    cvmx_helper_interface_get_mode(interface);
+		cvmx_helper_interface_mode_t imode = cvmx_helper_interface_get_mode(interface);
 		int num_ports = cvmx_helper_ports_on_interface(interface);
 		int interface_port;
 
-- 
1.7.5.4

