From d85b8091ad35484b826753d0d80bb18b1854d0d2 Mon Sep 17 00:00:00 2001
From: David Daney <ddaney@caviumnetworks.com>
Date: Fri, 14 May 2010 14:26:28 -0700
Subject: [PATCH 047/337] MIPS: Use Octeon2 atomic instructions when
 cpu_has_octeon2_isa.

Based On SDK 3.0.0-482

Signed-off-by: David Daney <ddaney@caviumnetworks.com>
Signed-off-by: Corey Minyard <cminyard@mvista.com>
Signed-off-by: Jack Tan <jack.tan@windriver.com>
---
 arch/mips/include/asm/atomic.h  |  120 ++++++++++++++++++++++++++++++++++++---
 arch/mips/include/asm/cmpxchg.h |   36 +++++++++++-
 2 files changed, 146 insertions(+), 10 deletions(-)

diff --git a/arch/mips/include/asm/atomic.h b/arch/mips/include/asm/atomic.h
index 3f4c5cb..a409e90 100644
--- a/arch/mips/include/asm/atomic.h
+++ b/arch/mips/include/asm/atomic.h
@@ -49,7 +49,15 @@
  */
 static __inline__ void atomic_add(int i, atomic_t * v)
 {
-	if (kernel_uses_llsc && R10000_LLSC_WAR) {
+	if (cpu_has_saa) {
+		__asm__ __volatile__(
+		".set	push\n\t"
+		".set	arch=octeon\n\t"
+		"saa    %1, (%2)\t# atomic_add (%0)\n\t"
+		".set	pop"
+		: "+m" (v->counter)
+		: "r" (i), "r" (&v->counter));
+	} else if (kernel_uses_llsc && R10000_LLSC_WAR) {
 		int temp;
 
 		__asm__ __volatile__(
@@ -92,7 +100,15 @@ static __inline__ void atomic_add(int i, atomic_t * v)
  */
 static __inline__ void atomic_sub(int i, atomic_t * v)
 {
-	if (kernel_uses_llsc && R10000_LLSC_WAR) {
+	if (cpu_has_saa) {
+		__asm__ __volatile__(
+		".set	push\n\t"
+		".set	arch=octeon\n\t"
+		"saa    %1, (%2)\t# atomic_sub(%0)\n\t"
+		".set	pop"
+		: "+m" (v->counter)
+		: "r" (-i), "r" (&v->counter));
+	} else if (kernel_uses_llsc && R10000_LLSC_WAR) {
 		int temp;
 
 		__asm__ __volatile__(
@@ -135,7 +151,25 @@ static __inline__ int atomic_add_return(int i, atomic_t * v)
 
 	smp_mb__before_llsc();
 
-	if (kernel_uses_llsc && R10000_LLSC_WAR) {
+	if (cpu_has_octeon2_isa && kernel_uses_llsc) {
+		/*
+		 * For proper barrier semantics, the preceding
+		 * smp_mb__before_llsc() must expand to syncw.
+		 */
+		if (__builtin_constant_p(i) && i == 1)
+			__asm__ __volatile__("lai\t%0,(%2)\t# atomic_add_return (%1)"
+					: "=r" (result), "+m" (v->counter)
+					: "r" (&v->counter));
+		else if (__builtin_constant_p(i) && i == -1)
+			__asm__ __volatile__("lad\t%0,(%2)\t# atomic_add_return (%1)"
+					: "=r" (result), "+m" (v->counter)
+					: "r" (&v->counter));
+		else
+			__asm__ __volatile__("laa\t%0,(%2),%3\t# atomic_add_return (%1)"
+					: "=r" (result), "+m" (v->counter)
+					: "r" (&v->counter), "r" (i));
+		result += i;
+	} else if (kernel_uses_llsc && R10000_LLSC_WAR) {
 		int temp;
 
 		__asm__ __volatile__(
@@ -186,7 +220,25 @@ static __inline__ int atomic_sub_return(int i, atomic_t * v)
 
 	smp_mb__before_llsc();
 
-	if (kernel_uses_llsc && R10000_LLSC_WAR) {
+	if (cpu_has_octeon2_isa && kernel_uses_llsc) {
+		/*
+		 * For proper barrier semantics, the preceding
+		 * smp_mb__before_llsc() must expand to syncw.
+		 */
+		if (__builtin_constant_p(i) && i == -1)
+			__asm__ __volatile__("lai\t%0,(%2)\t# atomic_sub_return (%1)"
+					: "=r" (result), "+m" (v->counter)
+					: "r" (&v->counter));
+		else if (__builtin_constant_p(i) && i == 1)
+			__asm__ __volatile__("lad\t%0,(%2)\t# atomic_sub_return (%1)"
+					: "=r" (result), "+m" (v->counter)
+					: "r" (&v->counter));
+		else
+			__asm__ __volatile__("laa\t%0,(%2),%3\t# atomic_sub_return (%1)"
+					: "=r" (result), "+m" (v->counter)
+					: "r" (&v->counter), "r" (-i));
+		result -= i;
+	} else if (kernel_uses_llsc && R10000_LLSC_WAR) {
 		int temp;
 
 		__asm__ __volatile__(
@@ -420,7 +472,15 @@ static __inline__ int __atomic_add_unless(atomic_t *v, int a, int u)
  */
 static __inline__ void atomic64_add(long i, atomic64_t * v)
 {
-	if (kernel_uses_llsc && R10000_LLSC_WAR) {
+	if (cpu_has_saa) {
+		__asm__ __volatile__(
+		".set	push\n\t"
+		".set	arch=octeon\n\t"
+		"saad   %1, (%2)\t# atomic64_add (%0)\n\t"
+		".set	pop"
+		: "+m" (v->counter)
+		: "r" (i), "r" (v));
+	} else if (kernel_uses_llsc && R10000_LLSC_WAR) {
 		long temp;
 
 		__asm__ __volatile__(
@@ -463,7 +523,15 @@ static __inline__ void atomic64_add(long i, atomic64_t * v)
  */
 static __inline__ void atomic64_sub(long i, atomic64_t * v)
 {
-	if (kernel_uses_llsc && R10000_LLSC_WAR) {
+	if (cpu_has_saa) {
+		__asm__ __volatile__(
+		".set	push\n\t"
+		".set	arch=octeon\n\t"
+		"saad    %1, (%2)\t# atomic64_sub (%0)\n\t"
+		".set	pop"
+		: "+m" (v->counter)
+		: "r" (-i), "r" (v));
+	} else if (kernel_uses_llsc && R10000_LLSC_WAR) {
 		long temp;
 
 		__asm__ __volatile__(
@@ -506,7 +574,25 @@ static __inline__ long atomic64_add_return(long i, atomic64_t * v)
 
 	smp_mb__before_llsc();
 
-	if (kernel_uses_llsc && R10000_LLSC_WAR) {
+	if (cpu_has_octeon2_isa && kernel_uses_llsc) {
+		/*
+		 * For proper barrier semantics, the preceding
+		 * smp_mb__before_llsc() must expand to syncw.
+		 */
+		if (__builtin_constant_p(i) && i == 1)
+			__asm__ __volatile__("laid\t%0,(%2)\t# atomic64_add_return (%1)"
+					: "=r" (result), "+m" (v->counter)
+					: "r" (&v->counter));
+		else if (__builtin_constant_p(i) && i == -1)
+			__asm__ __volatile__("ladd\t%0,(%2)\t# atomic64_add_return (%1)"
+					: "=r" (result), "+m" (v->counter)
+					: "r" (&v->counter));
+		else
+			__asm__ __volatile__("laad\t%0,(%2),%3\t# atomic64_add_return (%1)"
+					: "=r" (result), "+m" (v->counter)
+					: "r" (&v->counter), "r" (i));
+		result += i;
+	} else if (kernel_uses_llsc && R10000_LLSC_WAR) {
 		long temp;
 
 		__asm__ __volatile__(
@@ -557,7 +643,25 @@ static __inline__ long atomic64_sub_return(long i, atomic64_t * v)
 
 	smp_mb__before_llsc();
 
-	if (kernel_uses_llsc && R10000_LLSC_WAR) {
+	if (cpu_has_octeon2_isa && kernel_uses_llsc) {
+		/*
+		 * For proper barrier semantics, the preceding
+		 * smp_mb__before_llsc() must expand to syncw.
+		 */
+		if (__builtin_constant_p(i) && i == -1)
+			__asm__ __volatile__("laid\t%0,(%2)\t# atomic64_sub_return (%1)"
+					: "=r" (result), "+m" (v->counter)
+					: "r" (&v->counter));
+		else if (__builtin_constant_p(i) && i == 1)
+			__asm__ __volatile__("ladd\t%0,(%2)\t# atomic64_sub_return (%1)"
+					: "=r" (result), "+m" (v->counter)
+					: "r" (&v->counter));
+		else
+			__asm__ __volatile__("laad\t%0,(%2),%3\t# atomic64_sub_return (%1)"
+					: "=r" (result), "+m" (v->counter)
+					: "r" (&v->counter), "r" (-i));
+		result -= i;
+	} else if (kernel_uses_llsc && R10000_LLSC_WAR) {
 		long temp;
 
 		__asm__ __volatile__(
diff --git a/arch/mips/include/asm/cmpxchg.h b/arch/mips/include/asm/cmpxchg.h
index 285a41f..16a16f9 100644
--- a/arch/mips/include/asm/cmpxchg.h
+++ b/arch/mips/include/asm/cmpxchg.h
@@ -17,7 +17,23 @@ static inline unsigned long __xchg_u32(volatile int * m, unsigned int val)
 
 	smp_mb__before_llsc();
 
-	if (kernel_uses_llsc && R10000_LLSC_WAR) {
+	if (cpu_has_octeon2_isa && kernel_uses_llsc) {
+		if (__builtin_constant_p(val) && val == 0)
+			__asm__ __volatile__("lac\t%0,(%1)\t# xchg_u32"
+					: "=r" (retval)
+					: "r" (m)
+					: "memory");
+		else if (__builtin_constant_p(val) && val == 0xffffffffu)
+			__asm__ __volatile__("las\t%0,(%1)\t# xchg_u32"
+					: "=r" (retval)
+					: "r" (m)
+					: "memory");
+		else
+			__asm__ __volatile__("law\t%0,(%1),%2\t# xchg_u32"
+					: "=r" (retval)
+					: "r" (m), "r" (val)
+					: "memory");
+	} else if (kernel_uses_llsc && R10000_LLSC_WAR) {
 		unsigned long dummy;
 
 		__asm__ __volatile__(
@@ -69,7 +85,23 @@ static inline __u64 __xchg_u64(volatile __u64 * m, __u64 val)
 
 	smp_mb__before_llsc();
 
-	if (kernel_uses_llsc && R10000_LLSC_WAR) {
+	if (cpu_has_octeon2_isa && kernel_uses_llsc) {
+		if (__builtin_constant_p(val) && val == 0)
+			__asm__ __volatile__("lacd\t%0,(%1)\t# xchg_u64"
+					: "=r" (retval)
+					: "r" (m)
+					: "memory");
+		else if (__builtin_constant_p(val) && val == 0xffffffffffffffffull)
+			__asm__ __volatile__("lasd\t%0,(%1)\t# xchg_u64"
+					: "=r" (retval)
+					: "r" (m)
+					: "memory");
+		else
+			__asm__ __volatile__("lawd\t%0,(%1),%2\t# xchg_u64"
+					: "=r" (retval)
+					: "r" (m), "r" (val)
+					: "memory");
+	} else if (kernel_uses_llsc && R10000_LLSC_WAR) {
 		unsigned long dummy;
 
 		__asm__ __volatile__(
-- 
1.7.5.4

