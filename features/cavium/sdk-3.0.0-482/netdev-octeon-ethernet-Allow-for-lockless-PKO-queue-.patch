From 52abf257d5c1c4bbff9c8d24e5d61ee64c54750d Mon Sep 17 00:00:00 2001
From: David Daney <david.daney@cavium.com>
Date: Tue, 11 Dec 2012 11:01:34 -0800
Subject: [PATCH 262/337] netdev/octeon-ethernet: Allow for lockless PKO queue
 access and add disable_core_queueing parameter.

Based On SDK 3.0.0-482

Eliminating these locks almost doubles IPv4 forwarding rates.

Signed-off-by: David Daney <david.daney@cavium.com>
Signed-off-by: Corey Minyard <cminyard@mvista.com>
Signed-off-by: Jack Tan <jack.tan@windriver.com>
---
 .../mips/cavium-octeon/executive/cvmx-helper-cfg.c |   21 +-
 .../cavium-octeon/executive/cvmx-helper-util.c     |   12 +-
 arch/mips/cavium-octeon/executive/cvmx-helper.c    |    1 +
 arch/mips/include/asm/octeon/cvmx-cmd-queue.h      |   42 ++-
 arch/mips/include/asm/octeon/cvmx-helper-util.h    |   16 +-
 drivers/net/ethernet/octeon/ethernet-tx.c          |  355 +-----------------
 drivers/net/ethernet/octeon/ethernet-xmit.c        |  395 ++++++++++++++++++++
 drivers/net/ethernet/octeon/ethernet.c             |  165 ++++++++-
 drivers/net/ethernet/octeon/octeon-ethernet.h      |    1 +
 9 files changed, 616 insertions(+), 392 deletions(-)
 create mode 100644 drivers/net/ethernet/octeon/ethernet-xmit.c

diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c b/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c
index bee6b11..2f40e74 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c
@@ -666,31 +666,25 @@ static int cvmx_helper_cfg_init_pko_iports_and_queues_using_static_config(void)
 
 int __cvmx_helper_init_port_valid(void)
 {
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
 	int i, j, n;
 
 	for (i = 0; i < cvmx_helper_get_number_of_interfaces(); i++) {
-#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
 		static void *fdt_addr = 0;
 
 		if (fdt_addr == 0)
-			fdt_addr = __cvmx_phys_addr_to_ptr(
-						cvmx_sysinfo_get()->fdt_addr,
-						(128*1024));
-#endif
+			fdt_addr = __cvmx_phys_addr_to_ptr(cvmx_sysinfo_get()->fdt_addr,
+							   (128*1024));
 		n = cvmx_helper_interface_enumerate(i);
 		for (j = 0; j < n; j++) {
-#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
 			int ipd_port = cvmx_helper_get_ipd_port(i, j);
-			if (__cvmx_helper_board_get_port_from_dt(
-						fdt_addr, ipd_port) == 1)
-				cvmx_helper_port_valid[i][j] = 1;
+			if (__cvmx_helper_board_get_port_from_dt(fdt_addr, ipd_port) == 1)
+				cvmx_helper_set_port_valid(i, j, true);
 			else
-				cvmx_helper_port_valid[i][j] = 0;
-#else
-		cvmx_helper_port_valid[i][j] = 1;
-#endif
+				cvmx_helper_set_port_valid(i, j, false);
 		}
 	}
+#endif
 	return 0;
 }
 
@@ -761,6 +755,7 @@ int cvmx_pko_alloc_iport_and_queues(int interface, int port, int port_cnt, int q
 	}
 	return 0;
 }
+EXPORT_SYMBOL(cvmx_pko_alloc_iport_and_queues);
 
 int __cvmx_helper_init_port_config_data(void)
 {
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-util.c b/arch/mips/cavium-octeon/executive/cvmx-helper-util.c
index 7af108f..7d3b334 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-util.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-util.c
@@ -90,7 +90,7 @@ struct cvmx_iface {
  */
 static CVMX_SHARED struct cvmx_iface cvmx_interfaces[CVMX_HELPER_MAX_IFACE];
 
-CVMX_SHARED int cvmx_helper_port_valid[CVMX_HELPER_MAX_IFACE][CVMX_HELPER_MAX_PORTS];
+static CVMX_SHARED bool __cvmx_helper_port_invalid[CVMX_HELPER_MAX_IFACE][CVMX_HELPER_MAX_PORTS];
 
 #ifndef CVMX_BUILD_FOR_LINUX_KERNEL
 /**
@@ -843,6 +843,7 @@ int cvmx_helper_get_interface_index_num(int ipd_port)
 
 	return -1;
 }
+EXPORT_SYMBOL(cvmx_helper_get_interface_index_num);
 
 /**
  * Returns if port is valid for a given interface
@@ -854,7 +855,12 @@ int cvmx_helper_get_interface_index_num(int ipd_port)
  */
 int cvmx_helper_is_port_valid(int interface, int index)
 {
-	return  cvmx_helper_port_valid[interface][index];
+	return  !__cvmx_helper_port_invalid[interface][index];
 }
+EXPORT_SYMBOL(cvmx_helper_is_port_valid);
 
-EXPORT_SYMBOL(cvmx_helper_get_interface_index_num);
+void cvmx_helper_set_port_valid(int interface, int index, bool valid)
+{
+	__cvmx_helper_port_invalid[interface][index] = !valid;
+}
+EXPORT_SYMBOL(cvmx_helper_set_port_valid);
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper.c b/arch/mips/cavium-octeon/executive/cvmx-helper.c
index 5326041..f83f206 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper.c
@@ -643,6 +643,7 @@ int cvmx_helper_interface_enumerate(int interface)
 		return 0;
 	}
 }
+EXPORT_SYMBOL(cvmx_helper_interface_enumerate);
 
 /**
  * This function probes an interface to determine the actual number of
diff --git a/arch/mips/include/asm/octeon/cvmx-cmd-queue.h b/arch/mips/include/asm/octeon/cvmx-cmd-queue.h
index 5c23b56..330ec56 100644
--- a/arch/mips/include/asm/octeon/cvmx-cmd-queue.h
+++ b/arch/mips/include/asm/octeon/cvmx-cmd-queue.h
@@ -82,15 +82,12 @@
  * internal cycle counter to completely eliminate any causes of
  * bus traffic.
  *
- * <hr> $Revision$ <hr>
+ * <hr> $Revision: 79133 $ <hr>
  */
 
 #ifndef __CVMX_CMD_QUEUE_H__
 #define __CVMX_CMD_QUEUE_H__
 
-#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
-#endif
-
 #include "cvmx-fpa.h"
 
 #ifdef	__cplusplus
@@ -141,9 +138,10 @@ typedef enum {
 } cvmx_cmd_queue_result_t;
 
 typedef struct {
-	uint8_t now_serving;	    /**< You have lock when this is your ticket */
-	uint64_t unused1:24;
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t now_serving;	    /**< You have lock when this is your ticket */
 	uint32_t max_depth;	    /**< Maximum outstanding command words */
+
 	uint64_t fpa_pool:3;	    /**< FPA pool buffers come from */
 	uint64_t base_ptr_div128:29;
 				    /**< Top of command buffer pointer shifted 7 */
@@ -151,6 +149,16 @@ typedef struct {
 	uint64_t pool_size_m1:13;
 				    /**< FPA buffer size in 64bit words minus 1 */
 	uint64_t index:13;	    /**< Number of commands already used in buffer */
+#else
+	uint32_t max_depth;
+	uint32_t now_serving;
+
+	uint64_t index:13;
+	uint64_t pool_size_m1:13;
+	uint64_t unused2:6;
+	uint64_t base_ptr_div128:29;
+	uint64_t fpa_pool:3;
+#endif
 } __cvmx_cmd_queue_state_t;
 
 /**
@@ -249,25 +257,27 @@ static inline int __cvmx_cmd_queue_get_index(cvmx_cmd_queue_id_t queue_id)
 	asm volatile (".set push\n"
 		      ".set noreorder\n"
 		      "1:\n"
-		      "ll     %[my_ticket], %[ticket_ptr]\n"	/* Atomic add one to ticket_ptr */
-		      "li     %[ticket], 1\n"	/*    and store the original value */
-		      "baddu  %[ticket], %[my_ticket]\n"	/*    in my_ticket */
-		      "sc     %[ticket], %[ticket_ptr]\n"
+		      "lld     %[my_ticket], %[ticket_ptr]\n"
+		      /* Atomic add one to ticket_ptr 64-bit operation for endian nutral access. */
+		      "daddiu  %[ticket], %[my_ticket], 1\n"
+		      /*    and store the original value  in my_ticket */
+		      "scd     %[ticket], %[ticket_ptr]\n"
 		      "beqz   %[ticket], 1b\n"
-		      " nop\n"
-		      "lbu    %[ticket], %[now_serving]\n"	/* Load the current now_serving ticket */
+		      " sll	%[my_ticket],%[my_ticket],0\n"        /* truncate to 32 bits */
+		      "lw    %[ticket], %[now_serving]\n"	/* Load the current now_serving ticket */
 		      "2:\n"
 		      "beq    %[ticket], %[my_ticket], 4f\n"	/* Jump out if now_serving == my_ticket */
 		      " subu   %[ticket], %[my_ticket], %[ticket]\n"	/* Find out how many tickets are in front of me */
 		      "subu  %[ticket], 1\n"	/* Use tickets in front of me minus one to delay */
-		      "cins   %[ticket], %[ticket], 5, 7\n"	/* Delay will be ((tickets in front)-1)*32 loops */
+		      "sll   %[ticket], %[ticket], 5\n"	/* Delay will be ((tickets in front)-1)*32 loops */
 		      "3:\n"
 		      "bnez   %[ticket], 3b\n"	/* Loop here until our ticket might be up */
 		      " subu  %[ticket], 1\n"
 		      "b      2b\n"	/* Jump back up to check out ticket again */
-		      " lbu   %[ticket], %[now_serving]\n"	/* Load the current now_serving ticket */
+		      " lw    %[ticket], %[now_serving]\n"	/* Load the current now_serving ticket */
 		      "4:\n"
-		      ".set pop\n":[ticket_ptr] "=m"(__cvmx_cmd_queue_state_ptr->ticket[__cvmx_cmd_queue_get_index(queue_id)]),
+		      ".set pop\n"
+		      : [ticket_ptr] "=m"(__cvmx_cmd_queue_state_ptr->ticket[__cvmx_cmd_queue_get_index(queue_id)]),
 		      [now_serving] "=m"(qptr->now_serving),[ticket] "=&r"(tmp),[my_ticket] "=&r"(my_ticket)
 	    );
 }
@@ -280,7 +290,7 @@ static inline int __cvmx_cmd_queue_get_index(cvmx_cmd_queue_id_t queue_id)
  */
 static inline void __cvmx_cmd_queue_unlock(__cvmx_cmd_queue_state_t * qptr)
 {
-	uint8_t ns;
+	uint32_t ns;
 
 	ns = qptr->now_serving + 1;
 	CVMX_SYNCWS;		/* Order queue manipulation with respect to the unlock.  */
diff --git a/arch/mips/include/asm/octeon/cvmx-helper-util.h b/arch/mips/include/asm/octeon/cvmx-helper-util.h
index eb8d328..dc43537 100644
--- a/arch/mips/include/asm/octeon/cvmx-helper-util.h
+++ b/arch/mips/include/asm/octeon/cvmx-helper-util.h
@@ -63,8 +63,6 @@ typedef char cvmx_bpid_t;
 #define CVMX_HELPER_MAX_IFACE		9
 #define CVMX_HELPER_MAX_PORTS		16
 
-extern CVMX_SHARED int cvmx_helper_port_valid[CVMX_HELPER_MAX_IFACE][CVMX_HELPER_MAX_PORTS];
-
 /**
  * Convert a interface mode into a human readable string
  *
@@ -311,15 +309,25 @@ extern int cvmx_helper_get_interface_index_num(int ipd_port);
 
 /**
  * Returns if port is valid for a given interface
- * 
+ *
  * @param interface  interface to check
  * @param index      port index in the interface
- * 
+ *
  * @return status of the port present or not.
  */
 extern int cvmx_helper_is_port_valid(int interface, int index);
 
 /**
+ * Set the value returned by cvmx_helper_is_port_valid()
+ *
+ * @param interface  interface to check
+ * @param index      port index in the interface
+ * @param valid      true or false.
+ *
+ */
+void cvmx_helper_set_port_valid(int interface, int index, bool valid);
+
+/**
  * Get port kind for a given port in an interface.
  *
  * @param interface  Interface
diff --git a/drivers/net/ethernet/octeon/ethernet-tx.c b/drivers/net/ethernet/octeon/ethernet-tx.c
index 698a628..2dd4c0f 100644
--- a/drivers/net/ethernet/octeon/ethernet-tx.c
+++ b/drivers/net/ethernet/octeon/ethernet-tx.c
@@ -123,355 +123,8 @@ static inline void cvm_oct_set_back(struct sk_buff *skb,
 
 #endif
 
-/**
- * cvm_oct_xmit - transmit a packet
- * @skb:    Packet to send
- * @dev:    Device info structure
- *
- * Returns Always returns NETDEV_TX_OK
- */
-int cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev)
-{
-	struct sk_buff *skb_tmp;
-	cvmx_pko_command_word0_t pko_command;
-	union cvmx_buf_ptr hw_buffer;
-	u64 old_scratch;
-	u64 old_scratch2;
-	int qos;
-	int i;
-	int frag_count;
-	enum {QUEUE_HW, QUEUE_WQE, QUEUE_DROP} queue_type;
-	struct octeon_ethernet *priv = netdev_priv(dev);
-	s32 queue_depth;
-	s32 buffers_to_free;
-	s32 buffers_being_recycled;
-	unsigned long flags;
-	cvmx_wqe_t *work = NULL;
-	bool timestamp_this_skb = false;
-
-	/* Prefetch the private data structure.  It is larger than one
-	 * cache line.
-	 */
-	prefetch(priv);
-
-	if (USE_ASYNC_IOBDMA) {
-		/* Save scratch in case userspace is using it */
-		CVMX_SYNCIOBDMA;
-		old_scratch = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
-		old_scratch2 = cvmx_scratch_read64(CVMX_SCR_SCRATCH + 8);
-
-		/* Fetch and increment the number of packets to be
-		 * freed.
-		 */
-		cvmx_fau_async_fetch_and_add32(CVMX_SCR_SCRATCH + 8,
-					       FAU_NUM_PACKET_BUFFERS_TO_FREE,
-					       0);
-	}
-
-#ifdef CVM_OCT_LOCKLESS
-	qos = cvmx_get_core_num();
-#else
-	/* The check on CVMX_PKO_QUEUES_PER_PORT_* is designed to
-	 * completely remove "qos" in the event neither interface
-	 * supports multiple queues per port.
-	 */
-	if (priv->tx_multiple_queues) {
-		qos = GET_SKBUFF_QOS(skb);
-		if (qos <= 0)
-			qos = 0;
-		else if (qos >= priv->num_tx_queues)
-			qos = 0;
-	} else
-		qos = 0;
-#endif
-	if (USE_ASYNC_IOBDMA) {
-		cvmx_fau_async_fetch_and_add32(CVMX_SCR_SCRATCH,
-					       priv->tx_queue[qos].fau, 1);
-	}
-
-	frag_count = 0;
-	if (skb_has_frag_list(skb))
-		skb_walk_frags(skb, skb_tmp)
-			frag_count++;
-	/* We have space for 12 segment pointers, If there will be
-	 * more than that, we must linearize.  The count is: 1 (base
-	 * SKB) + frag_count + nr_frags.
-	 */
-	if (unlikely(skb_shinfo(skb)->nr_frags + frag_count > 11)) {
-		if (unlikely(__skb_linearize(skb))) {
-			queue_type = QUEUE_DROP;
-			goto skip_xmit;
-		}
-		frag_count = 0;
-	}
-
-	/* The CN3XXX series of parts has an errata (GMX-401) which
-	 * causes the GMX block to hang if a collision occurs towards
-	 * the end of a <68 byte packet. As a workaround for this, we
-	 * pad packets to be 68 bytes whenever we are in half duplex
-	 * mode. We don't handle the case of having a small packet but
-	 * no room to add the padding.  The kernel should always give
-	 * us at least a cache line
-	 */
-	if ((skb->len < 64) && OCTEON_IS_MODEL(OCTEON_CN3XXX)) {
-		union cvmx_gmxx_prtx_cfg gmx_prt_cfg;
-
-		if (priv->interface < 2) {
-			/* We only need to pad packet in half duplex mode */
-			gmx_prt_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
-			if (gmx_prt_cfg.s.duplex == 0) {
-				int add_bytes = 64 - skb->len;
-				if ((skb_tail_pointer(skb) + add_bytes) <= skb_end_pointer(skb))
-					memset(__skb_put(skb, add_bytes), 0, add_bytes);
-			}
-		}
-	}
-
-	/* Build the PKO command */
-	pko_command.u64 = 0;
-#ifdef __LITTLE_ENDIAN
-	pko_command.s.le = 1;
-#endif
-	/* Don't pollute L2 with the outgoing packet */
-	pko_command.s.n2 = 1;
-	pko_command.s.segs = 1;
-	pko_command.s.total_bytes = skb->len;
-	pko_command.s.size0 = CVMX_FAU_OP_SIZE_32;
-	pko_command.s.subone0 = 1;
-	pko_command.s.reg0 = priv->tx_queue[qos].fau;
-	pko_command.s.dontfree = 1;
-
-	/* Build the PKO buffer pointer */
-	hw_buffer.u64 = 0; /* Implies pool == 0, i == 0 */
-	if (skb_shinfo(skb)->nr_frags == 0 && frag_count == 0) {
-		hw_buffer.s.addr = virt_to_phys(skb->data);
-		hw_buffer.s.size = skb->len;
-		cvm_oct_set_back(skb, &hw_buffer);
-		buffers_being_recycled = 1;
-	} else {
-		u64 *hw_buffer_list;
-		bool can_do_reuse = true;
-
-		work = cvmx_fpa_alloc(wqe_pool);
-		if (unlikely(!work)) {
-			netdev_err(dev, "Failed WQE allocate\n");
-			queue_type = QUEUE_DROP;
-			goto skip_xmit;
-		}
-		hw_buffer_list = (u64 *)work->packet_data;
-		hw_buffer.s.addr = virt_to_phys(skb->data);
-		hw_buffer.s.size = skb_headlen(skb);
-		if (skb_shinfo(skb)->nr_frags == 0 && cvm_oct_skb_ok_for_reuse(skb)) {
-			cvm_oct_set_back(skb, &hw_buffer);
-		} else {
-			hw_buffer.s.back = 0;
-			can_do_reuse = false;
-		}
-		hw_buffer_list[0] = hw_buffer.u64;
-		for (i = 1; i <= skb_shinfo(skb)->nr_frags; i++) {
-			struct skb_frag_struct *fs = skb_shinfo(skb)->frags + i - 1;
-			hw_buffer.s.addr = virt_to_phys((u8 *)page_address(fs->page.p) + fs->page_offset);
-			hw_buffer.s.size = fs->size;
-			hw_buffer_list[i] = hw_buffer.u64;
-			can_do_reuse = false;
-		}
-		skb_walk_frags(skb, skb_tmp) {
-			hw_buffer.s.addr = virt_to_phys(skb_tmp->data);
-			hw_buffer.s.size = skb_tmp->len;
-			if (cvm_oct_skb_ok_for_reuse(skb_tmp)) {
-				cvm_oct_set_back(skb_tmp, &hw_buffer);
-			} else {
-				hw_buffer.s.back = 0;
-				can_do_reuse = false;
-			}
-			hw_buffer_list[i] = hw_buffer.u64;
-			i++;
-		}
-		hw_buffer.s.addr = virt_to_phys(hw_buffer_list);
-		hw_buffer.s.size = i;
-		hw_buffer.s.back = 0;
-		hw_buffer.s.pool = wqe_pool;
-		buffers_being_recycled = i;
-		pko_command.s.segs = hw_buffer.s.size;
-		pko_command.s.gather = 1;
-		if (!can_do_reuse)
-			goto dont_put_skbuff_in_hw;
-	}
-
-	if (unlikely(priv->tx_timestamp_hw &&
-		     (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP))) {
-		timestamp_this_skb = true;
-		skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
-		goto dont_put_skbuff_in_hw;
-	}
-	/* See if we can put this skb in the FPA pool. Any strange
-	 * behavior from the Linux networking stack will most likely
-	 * be caused by a bug in the following code. If some field is
-	 * in use by the network stack and get carried over when a
-	 * buffer is reused, bad thing may happen.  If in doubt and
-	 * you dont need the absolute best performance, disable the
-	 * define REUSE_SKBUFFS_WITHOUT_FREE. The reuse of buffers has
-	 * shown a 25% increase in performance under some loads.
-	 */
-#if REUSE_SKBUFFS_WITHOUT_FREE
-	if (!cvm_oct_skb_ok_for_reuse(skb))
-		goto dont_put_skbuff_in_hw;
-	if (unlikely(skb_header_cloned(skb)))
-		goto dont_put_skbuff_in_hw;
-	if (unlikely(skb->destructor))
-		goto dont_put_skbuff_in_hw;
-
+#define CVM_OCT_LOCKLESS 1
+#include "ethernet-xmit.c"
 
-	/* We can use this buffer in the FPA.  We don't need the FAU
-	 * update anymore
-	 */
-	pko_command.s.dontfree = 0;
-
-#endif /* REUSE_SKBUFFS_WITHOUT_FREE */
-
-dont_put_skbuff_in_hw:
-
-	/* Check if we can use the hardware checksumming */
-	if (USE_HW_TCPUDP_CHECKSUM && (skb->protocol == htons(ETH_P_IP)) &&
-	    (ip_hdr(skb)->version == 4) && (ip_hdr(skb)->ihl == 5) &&
-	    ((ip_hdr(skb)->frag_off == 0) || (ip_hdr(skb)->frag_off == htons(1 << 14)))
-	    && ((ip_hdr(skb)->protocol == IPPROTO_TCP) || (ip_hdr(skb)->protocol == IPPROTO_UDP))) {
-		/* Use hardware checksum calc */
-		pko_command.s.ipoffp1 = sizeof(struct ethhdr) + 1;
-		if (unlikely(priv->imode == CVMX_HELPER_INTERFACE_MODE_SRIO))
-			pko_command.s.ipoffp1 += 8;
-	}
-
-	if (USE_ASYNC_IOBDMA) {
-		/* Get the number of skbuffs in use by the hardware */
-		CVMX_SYNCIOBDMA;
-		queue_depth = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
-		buffers_to_free = cvmx_scratch_read64(CVMX_SCR_SCRATCH + 8);
-	} else {
-		/* Get the number of skbuffs in use by the hardware */
-		queue_depth = cvmx_fau_fetch_and_add32(priv->tx_queue[qos].fau, 1);
-		buffers_to_free = cvmx_fau_fetch_and_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);
-	}
-
-	/* If we're sending faster than the receive can free them then
-	 * don't do the HW free.
-	 */
-	if (unlikely(buffers_to_free < -100))
-		pko_command.s.dontfree = 1;
-
-	/* Drop this packet if we have too many already queued to the HW */
-	if (unlikely(queue_depth >= MAX_OUT_QUEUE_DEPTH)) {
-		if (dev->tx_queue_len != 0) {
-			netif_stop_queue(dev);
-		} else {
-			/* If not using normal queueing.  */
-			queue_type = QUEUE_DROP;
-			goto skip_xmit;
-		}
-	}
-
-	if (pko_command.s.dontfree) {
-		queue_type = QUEUE_WQE;
-	} else {
-		queue_type = QUEUE_HW;
-		if (buffers_being_recycled > 1) {
-			struct sk_buff *tskb, *nskb;
-			/* We are committed to use hardware free, restore the
-			 * frag list to empty on the first SKB
-			 */
-			tskb = skb_shinfo(skb)->frag_list;
-			while (tskb) {
-				nskb = tskb->next;
-				cvm_oct_skb_prepare_for_reuse(tskb);
-				tskb = nskb;
-			}
-		}
-		cvm_oct_skb_prepare_for_reuse(skb);
-	}
-
-	if (queue_type == QUEUE_WQE) {
-		if (!work) {
-			work = cvmx_fpa_alloc(wqe_pool);
-			if (unlikely(!work)) {
-				netdev_err(dev, "Failed WQE allocate\n");
-				queue_type = QUEUE_DROP;
-				goto skip_xmit;
-			}
-		}
-
-		pko_command.s.rsp = 1;
-		pko_command.s.wqp = 1;
-		/* work->unused will carry the qos for this packet,
-		 * this allows us to find the proper FAU when freeing
-		 * the packet.  We decrement the FAU when the WQE is
-		 * replaced in the pool.
-		 */
-		pko_command.s.reg0 = 0;
-		work->word0.u64 = 0;
-		work->word0.raw.unused = (u8)qos;
-
-		work->word1.u64 = 0;
-		work->word1.tag_type = CVMX_POW_TAG_TYPE_NULL;
-		work->word1.tag = 0;
-		work->word2.u64 = 0;
-		work->word2.s.software = 1;
-		cvmx_wqe_set_grp(work, pow_receive_group);
-		work->packet_ptr.u64 = (unsigned long)skb;
-	}
-
-	local_irq_save(flags);
-
-	cvmx_pko_send_packet_prepare_pkoid(priv->pko_port,
-					   priv->tx_queue[qos].queue,
-					   CVMX_PKO_LOCK_CMD_QUEUE);
-
-	/* Send the packet to the output queue */
-	if (queue_type == QUEUE_WQE) {
-		u64 word2 = virt_to_phys(work);
-		if (timestamp_this_skb)
-			word2 |= 1ull << 40; /* Bit 40 controls timestamps */
-
-		if (unlikely(cvmx_pko_send_packet_finish3_pkoid(priv->pko_port,
-							  priv->tx_queue[qos].queue, pko_command, hw_buffer,
-							  word2, CVMX_PKO_LOCK_CMD_QUEUE))) {
-				queue_type = QUEUE_DROP;
-				cvmx_fpa_free(work, wqe_pool, DONT_WRITEBACK(1));
-				netdev_err(dev, "Failed to send the packet with wqe\n");
-		}
-	} else {
-		if (unlikely(cvmx_pko_send_packet_finish_pkoid(priv->pko_port,
-							 priv->tx_queue[qos].queue,
-							 pko_command, hw_buffer,
-							 CVMX_PKO_LOCK_CMD_QUEUE))) {
-			netdev_err(dev, "Failed to send the packet\n");
-			queue_type = QUEUE_DROP;
-		}
-	}
-	local_irq_restore(flags);
-
-skip_xmit:
-	switch (queue_type) {
-	case QUEUE_DROP:
-		cvmx_fau_atomic_add32(priv->tx_queue[qos].fau, -1);
-		dev_kfree_skb_any(skb);
-		dev->stats.tx_dropped++;
-		break;
-	case QUEUE_HW:
-		cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, -buffers_being_recycled);
-		break;
-	case QUEUE_WQE:
-		/* Cleanup is done on the RX path when the WQE returns */
-		break;
-	default:
-		BUG();
-	}
-
-	if (USE_ASYNC_IOBDMA) {
-		CVMX_SYNCIOBDMA;
-		/* Restore the scratch area */
-		cvmx_scratch_write64(CVMX_SCR_SCRATCH, old_scratch);
-		cvmx_scratch_write64(CVMX_SCR_SCRATCH + 8, old_scratch2);
-	}
-
-	return NETDEV_TX_OK;
-}
+#undef CVM_OCT_LOCKLESS
+#include "ethernet-xmit.c"
diff --git a/drivers/net/ethernet/octeon/ethernet-xmit.c b/drivers/net/ethernet/octeon/ethernet-xmit.c
new file mode 100644
index 0000000..864c793
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-xmit.c
@@ -0,0 +1,395 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+
+#undef CVM_OCT_XMIT
+#undef CVM_OCT_PKO_LOCK_TYPE
+
+#ifdef CVM_OCT_LOCKLESS
+#define CVM_OCT_XMIT cvm_oct_xmit_lockless
+#define CVM_OCT_PKO_LOCK_TYPE CVMX_PKO_LOCK_NONE
+#else
+#define CVM_OCT_XMIT cvm_oct_xmit
+#define CVM_OCT_PKO_LOCK_TYPE CVMX_PKO_LOCK_CMD_QUEUE
+#endif
+
+/**
+ * cvm_oct_xmit - transmit a packet
+ * @skb:    Packet to send
+ * @dev:    Device info structure
+ *
+ * Returns Always returns NETDEV_TX_OK
+ */
+int
+CVM_OCT_XMIT
+(struct sk_buff *skb, struct net_device *dev)
+{
+	struct sk_buff *skb_tmp;
+	cvmx_pko_command_word0_t pko_command;
+	union cvmx_buf_ptr hw_buffer;
+	u64 old_scratch;
+	u64 old_scratch2;
+	int qos;
+	int i;
+	int frag_count;
+	enum {QUEUE_HW, QUEUE_WQE, QUEUE_DROP} queue_type;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	s32 queue_depth;
+	s32 buffers_to_free;
+	s32 buffers_being_recycled;
+	unsigned long flags;
+	cvmx_wqe_t *work = NULL;
+	bool timestamp_this_skb = false;
+
+	/* Prefetch the private data structure.  It is larger than one
+	 * cache line.
+	 */
+	prefetch(priv);
+
+	if (USE_ASYNC_IOBDMA) {
+		/* Save scratch in case userspace is using it */
+		CVMX_SYNCIOBDMA;
+		old_scratch = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
+		old_scratch2 = cvmx_scratch_read64(CVMX_SCR_SCRATCH + 8);
+
+		/* Fetch and increment the number of packets to be
+		 * freed.
+		 */
+		cvmx_fau_async_fetch_and_add32(CVMX_SCR_SCRATCH + 8,
+					       FAU_NUM_PACKET_BUFFERS_TO_FREE,
+					       0);
+	}
+
+#ifdef CVM_OCT_LOCKLESS
+	qos = cvmx_get_core_num();
+#else
+	/* The check on CVMX_PKO_QUEUES_PER_PORT_* is designed to
+	 * completely remove "qos" in the event neither interface
+	 * supports multiple queues per port.
+	 */
+	if (priv->tx_multiple_queues) {
+		qos = GET_SKBUFF_QOS(skb);
+		if (qos <= 0)
+			qos = 0;
+		else if (qos >= priv->num_tx_queues)
+			qos = 0;
+	} else
+		qos = 0;
+#endif
+	if (USE_ASYNC_IOBDMA) {
+		cvmx_fau_async_fetch_and_add32(CVMX_SCR_SCRATCH,
+					       priv->tx_queue[qos].fau, 1);
+	}
+
+	frag_count = 0;
+	if (skb_has_frag_list(skb))
+		skb_walk_frags(skb, skb_tmp)
+			frag_count++;
+	/* We have space for 12 segment pointers, If there will be
+	 * more than that, we must linearize.  The count is: 1 (base
+	 * SKB) + frag_count + nr_frags.
+	 */
+	if (unlikely(skb_shinfo(skb)->nr_frags + frag_count > 11)) {
+		if (unlikely(__skb_linearize(skb))) {
+			queue_type = QUEUE_DROP;
+			goto skip_xmit;
+		}
+		frag_count = 0;
+	}
+
+#ifndef CVM_OCT_LOCKLESS
+	/* The CN3XXX series of parts has an errata (GMX-401) which
+	 * causes the GMX block to hang if a collision occurs towards
+	 * the end of a <68 byte packet. As a workaround for this, we
+	 * pad packets to be 68 bytes whenever we are in half duplex
+	 * mode. We don't handle the case of having a small packet but
+	 * no room to add the padding.  The kernel should always give
+	 * us at least a cache line
+	 */
+	if ((skb->len < 64) && OCTEON_IS_MODEL(OCTEON_CN3XXX)) {
+		union cvmx_gmxx_prtx_cfg gmx_prt_cfg;
+
+		if (priv->interface < 2) {
+			/* We only need to pad packet in half duplex mode */
+			gmx_prt_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
+			if (gmx_prt_cfg.s.duplex == 0) {
+				int add_bytes = 64 - skb->len;
+				if ((skb_tail_pointer(skb) + add_bytes) <= skb_end_pointer(skb))
+					memset(__skb_put(skb, add_bytes), 0, add_bytes);
+			}
+		}
+	}
+#endif
+	/* Build the PKO command */
+	pko_command.u64 = 0;
+#ifdef __LITTLE_ENDIAN
+	pko_command.s.le = 1;
+#endif
+	/* Don't pollute L2 with the outgoing packet */
+	pko_command.s.n2 = 1;
+	pko_command.s.segs = 1;
+	pko_command.s.total_bytes = skb->len;
+	/* Use fau0 to decrement the number of packets queued */
+	pko_command.s.size0 = CVMX_FAU_OP_SIZE_32;
+	pko_command.s.subone0 = 1;
+	pko_command.s.reg0 = priv->tx_queue[qos].fau;
+	pko_command.s.dontfree = 1;
+
+	/* Build the PKO buffer pointer */
+	hw_buffer.u64 = 0; /* Implies pool == 0, i == 0 */
+	if (skb_shinfo(skb)->nr_frags == 0 && frag_count == 0) {
+		hw_buffer.s.addr = virt_to_phys(skb->data);
+		hw_buffer.s.size = skb->len;
+		cvm_oct_set_back(skb, &hw_buffer);
+		buffers_being_recycled = 1;
+	} else {
+		u64 *hw_buffer_list;
+		bool can_do_reuse = true;
+
+		work = cvmx_fpa_alloc(wqe_pool);
+		if (unlikely(!work)) {
+			netdev_err(dev, "Failed WQE allocate\n");
+			queue_type = QUEUE_DROP;
+			goto skip_xmit;
+		}
+		hw_buffer_list = (u64 *)work->packet_data;
+		hw_buffer.s.addr = virt_to_phys(skb->data);
+		hw_buffer.s.size = skb_headlen(skb);
+		if (skb_shinfo(skb)->nr_frags == 0 && cvm_oct_skb_ok_for_reuse(skb)) {
+			cvm_oct_set_back(skb, &hw_buffer);
+		} else {
+			hw_buffer.s.back = 0;
+			can_do_reuse = false;
+		}
+		hw_buffer_list[0] = hw_buffer.u64;
+		for (i = 1; i <= skb_shinfo(skb)->nr_frags; i++) {
+			struct skb_frag_struct *fs = skb_shinfo(skb)->frags + i - 1;
+			hw_buffer.s.addr = virt_to_phys((u8 *)page_address(fs->page.p) + fs->page_offset);
+			hw_buffer.s.size = fs->size;
+			hw_buffer_list[i] = hw_buffer.u64;
+			can_do_reuse = false;
+		}
+		skb_walk_frags(skb, skb_tmp) {
+			hw_buffer.s.addr = virt_to_phys(skb_tmp->data);
+			hw_buffer.s.size = skb_tmp->len;
+			if (cvm_oct_skb_ok_for_reuse(skb_tmp)) {
+				cvm_oct_set_back(skb_tmp, &hw_buffer);
+			} else {
+				hw_buffer.s.back = 0;
+				can_do_reuse = false;
+			}
+			hw_buffer_list[i] = hw_buffer.u64;
+			i++;
+		}
+		hw_buffer.s.addr = virt_to_phys(hw_buffer_list);
+		hw_buffer.s.size = i;
+		hw_buffer.s.back = 0;
+		hw_buffer.s.pool = wqe_pool;
+		buffers_being_recycled = i;
+		pko_command.s.segs = hw_buffer.s.size;
+		pko_command.s.gather = 1;
+		if (!can_do_reuse)
+			goto dont_put_skbuff_in_hw;
+	}
+
+	if (unlikely(priv->tx_timestamp_hw &&
+		     (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP))) {
+		timestamp_this_skb = true;
+		skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+		goto dont_put_skbuff_in_hw;
+	}
+	/* See if we can put this skb in the FPA pool. Any strange
+	 * behavior from the Linux networking stack will most likely
+	 * be caused by a bug in the following code. If some field is
+	 * in use by the network stack and get carried over when a
+	 * buffer is reused, bad thing may happen.  If in doubt and
+	 * you dont need the absolute best performance, disable the
+	 * define REUSE_SKBUFFS_WITHOUT_FREE. The reuse of buffers has
+	 * shown a 25% increase in performance under some loads.
+	 */
+#if REUSE_SKBUFFS_WITHOUT_FREE
+	if (!cvm_oct_skb_ok_for_reuse(skb))
+		goto dont_put_skbuff_in_hw;
+	if (unlikely(skb_header_cloned(skb)))
+		goto dont_put_skbuff_in_hw;
+	if (unlikely(skb->destructor))
+		goto dont_put_skbuff_in_hw;
+
+
+	/* We can use this buffer in the FPA.  We don't need the FAU
+	 * update anymore
+	 */
+	pko_command.s.dontfree = 0;
+
+#endif /* REUSE_SKBUFFS_WITHOUT_FREE */
+
+dont_put_skbuff_in_hw:
+
+	/* Check if we can use the hardware checksumming */
+	if (USE_HW_TCPUDP_CHECKSUM && (skb->protocol == htons(ETH_P_IP)) &&
+	    (ip_hdr(skb)->version == 4) && (ip_hdr(skb)->ihl == 5) &&
+	    ((ip_hdr(skb)->frag_off == 0) || (ip_hdr(skb)->frag_off == htons(1 << 14)))
+	    && ((ip_hdr(skb)->protocol == IPPROTO_TCP) || (ip_hdr(skb)->protocol == IPPROTO_UDP))) {
+		/* Use hardware checksum calc */
+		pko_command.s.ipoffp1 = sizeof(struct ethhdr) + 1;
+		if (unlikely(priv->imode == CVMX_HELPER_INTERFACE_MODE_SRIO))
+			pko_command.s.ipoffp1 += 8;
+	}
+
+	if (USE_ASYNC_IOBDMA) {
+		/* Get the number of skbuffs in use by the hardware */
+		CVMX_SYNCIOBDMA;
+		queue_depth = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
+		buffers_to_free = cvmx_scratch_read64(CVMX_SCR_SCRATCH + 8);
+	} else {
+		/* Get the number of skbuffs in use by the hardware */
+		queue_depth = cvmx_fau_fetch_and_add32(priv->tx_queue[qos].fau, 1);
+		buffers_to_free = cvmx_fau_fetch_and_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);
+	}
+
+	/* If we're sending faster than the receive can free them then
+	 * don't do the HW free.
+	 */
+	if (unlikely(buffers_to_free < -100))
+		pko_command.s.dontfree = 1;
+
+	/* Drop this packet if we have too many already queued to the HW */
+	if (unlikely(queue_depth >= MAX_OUT_QUEUE_DEPTH)) {
+		if (dev->tx_queue_len != 0) {
+			netif_stop_queue(dev);
+		} else {
+			/* If not using normal queueing.  */
+			queue_type = QUEUE_DROP;
+			goto skip_xmit;
+		}
+	}
+
+	if (pko_command.s.dontfree) {
+		queue_type = QUEUE_WQE;
+	} else {
+		queue_type = QUEUE_HW;
+		if (buffers_being_recycled > 1) {
+			struct sk_buff *tskb, *nskb;
+			/* We are committed to use hardware free, restore the
+			 * frag list to empty on the first SKB
+			 */
+			tskb = skb_shinfo(skb)->frag_list;
+			while (tskb) {
+				nskb = tskb->next;
+				cvm_oct_skb_prepare_for_reuse(tskb);
+				tskb = nskb;
+			}
+		}
+		cvm_oct_skb_prepare_for_reuse(skb);
+	}
+
+	if (queue_type == QUEUE_WQE) {
+		if (!work) {
+			work = cvmx_fpa_alloc(wqe_pool);
+			if (unlikely(!work)) {
+				netdev_err(dev, "Failed WQE allocate\n");
+				queue_type = QUEUE_DROP;
+				goto skip_xmit;
+			}
+		}
+
+		pko_command.s.rsp = 1;
+		pko_command.s.wqp = 1;
+		/* work->unused will carry the qos for this packet,
+		 * this allows us to find the proper FAU when freeing
+		 * the packet.  We decrement the FAU when the WQE is
+		 * replaced in the pool.
+		 */
+		pko_command.s.reg0 = 0;
+		work->word0.u64 = 0;
+		work->word0.raw.unused = (u8)qos;
+
+		work->word1.u64 = 0;
+		work->word1.tag_type = CVMX_POW_TAG_TYPE_NULL;
+		work->word1.tag = 0;
+		work->word2.u64 = 0;
+		work->word2.s.software = 1;
+		cvmx_wqe_set_grp(work, pow_receive_group);
+		work->packet_ptr.u64 = (unsigned long)skb;
+	}
+
+	local_irq_save(flags);
+
+	cvmx_pko_send_packet_prepare_pkoid(priv->pko_port,
+					   priv->tx_queue[qos].queue,
+					   CVM_OCT_PKO_LOCK_TYPE);
+
+	/* Send the packet to the output queue */
+	if (queue_type == QUEUE_WQE) {
+		u64 word2 = virt_to_phys(work);
+		if (timestamp_this_skb)
+			word2 |= 1ull << 40; /* Bit 40 controls timestamps */
+
+		if (unlikely(cvmx_pko_send_packet_finish3_pkoid(priv->pko_port,
+							  priv->tx_queue[qos].queue, pko_command, hw_buffer,
+							  word2, CVM_OCT_PKO_LOCK_TYPE))) {
+				queue_type = QUEUE_DROP;
+				netdev_err(dev, "Failed to send the packet with wqe\n");
+		}
+	} else {
+		if (unlikely(cvmx_pko_send_packet_finish_pkoid(priv->pko_port,
+							 priv->tx_queue[qos].queue,
+							 pko_command, hw_buffer,
+							 CVM_OCT_PKO_LOCK_TYPE))) {
+			netdev_err(dev, "Failed to send the packet\n");
+			queue_type = QUEUE_DROP;
+		}
+	}
+	local_irq_restore(flags);
+
+skip_xmit:
+	switch (queue_type) {
+	case QUEUE_DROP:
+		cvmx_fau_atomic_add32(priv->tx_queue[qos].fau, -1);
+		dev_kfree_skb_any(skb);
+		dev->stats.tx_dropped++;
+		if (work)
+			cvmx_fpa_free(work, wqe_pool, DONT_WRITEBACK(1));
+		break;
+	case QUEUE_HW:
+		cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, -buffers_being_recycled);
+		break;
+	case QUEUE_WQE:
+		/* Cleanup is done on the RX path when the WQE returns */
+		break;
+	default:
+		BUG();
+	}
+
+	if (USE_ASYNC_IOBDMA) {
+		CVMX_SYNCIOBDMA;
+		/* Restore the scratch area */
+		cvmx_scratch_write64(CVMX_SCR_SCRATCH, old_scratch);
+		cvmx_scratch_write64(CVMX_SCR_SCRATCH + 8, old_scratch2);
+	}
+
+	return NETDEV_TX_OK;
+}
diff --git a/drivers/net/ethernet/octeon/ethernet.c b/drivers/net/ethernet/octeon/ethernet.c
index 0f9f74f..7085bba 100644
--- a/drivers/net/ethernet/octeon/ethernet.c
+++ b/drivers/net/ethernet/octeon/ethernet.c
@@ -72,6 +72,13 @@ MODULE_PARM_DESC(pow_receive_group, "\n"
 	"\tgroup. Also any other software can submit packets to this\n"
 	"\tgroup for the kernel to process.");
 
+static int disable_core_queueing = 1;
+module_param(disable_core_queueing, int, S_IRUGO);
+MODULE_PARM_DESC(disable_core_queueing, "\n"
+		"\t\tWhen set the networking core's tx_queue_len is set to zero.  This\n"
+		"\t\tallows packets to be sent without lock contention in the packet scheduler\n"
+		"\t\tresulting in some cases in improved throughput.");
+
 int max_rx_cpus = -1;
 module_param(max_rx_cpus, int, 0444);
 MODULE_PARM_DESC(max_rx_cpus, "\n"
@@ -82,6 +89,10 @@ int rx_napi_weight = 32;
 module_param(rx_napi_weight, int, 0444);
 MODULE_PARM_DESC(rx_napi_weight, "The NAPI WEIGHT parameter.");
 
+static int disable_lockless_pko;
+module_param(disable_lockless_pko, int, S_IRUGO);
+MODULE_PARM_DESC(disable_lockless_pko, "Disable lockless PKO access (use locking for queues instead).");
+
 /* internal ports count for each port in a interface */
 int iport_count = 1;
 /* pko queue count for each port in a interface */
@@ -161,9 +172,101 @@ static void cvm_oct_periodic_worker(struct work_struct *work)
 
 static int cvm_oct_num_output_buffers;
 
-static __devinit int cvm_oct_configure_common_hw(void)
+static __devinit int cvm_oct_get_total_pko_queues(void)
+{
+	if (OCTEON_IS_MODEL(OCTEON_CN38XX))
+		return 128;
+	else if (OCTEON_IS_MODEL(OCTEON_CN3XXX))
+		return 32;
+	else if (OCTEON_IS_MODEL(OCTEON_CN50XX))
+		return 32;
+	else
+		return 256;
+}
+
+static __devinit bool cvm_oct_pko_lockless(void)
 {
+	int interface, num_interfaces;
+	int queues = 0;
+
+	if (disable_lockless_pko)
+		return false;
+
+	/* CN3XXX require workarounds in xmit.  Disable lockless for
+	 * CN3XXX to optimize the lockless case with out the
+	 * workarounds.
+	 */
+	if (OCTEON_IS_MODEL(OCTEON_CN3XXX))
+		return false;
+
+	num_interfaces = cvmx_helper_get_number_of_interfaces();
+	for (interface = 0; interface < num_interfaces; interface++) {
+		int num_ports, port;
+		cvmx_helper_interface_mode_t imode = cvmx_helper_interface_get_mode(interface);
 
+		num_ports = cvmx_helper_interface_enumerate(interface);
+		for (port = 0; port < num_ports; port++) {
+			if (!cvmx_helper_is_port_valid(interface, port))
+				continue;
+			switch (imode) {
+			case CVMX_HELPER_INTERFACE_MODE_XAUI:
+			case CVMX_HELPER_INTERFACE_MODE_RXAUI:
+			case CVMX_HELPER_INTERFACE_MODE_SGMII:
+			case CVMX_HELPER_INTERFACE_MODE_RGMII:
+			case CVMX_HELPER_INTERFACE_MODE_GMII:
+			case CVMX_HELPER_INTERFACE_MODE_SPI:
+				queues += num_possible_cpus();
+				break;
+			case CVMX_HELPER_INTERFACE_MODE_NPI:
+			case CVMX_HELPER_INTERFACE_MODE_LOOP:
+#ifdef CONFIG_RAPIDIO
+			case CVMX_HELPER_INTERFACE_MODE_SRIO:
+#endif
+				queues += 1;
+				break;
+			default:
+				break;
+			}
+		}
+	}
+	return queues <= cvm_oct_get_total_pko_queues();
+}
+
+static __devinit void cvm_oct_set_pko_multiqueue(void)
+{
+	int interface, num_interfaces, rv;
+
+	num_interfaces = cvmx_helper_get_number_of_interfaces();
+	for (interface = 0; interface < num_interfaces; interface++) {
+		int num_ports, port;
+		cvmx_helper_interface_mode_t imode = cvmx_helper_interface_get_mode(interface);
+
+		num_ports = cvmx_helper_interface_enumerate(interface);
+		for (port = 0; port < num_ports; port++) {
+			if (!cvmx_helper_is_port_valid(interface, port))
+				continue;
+			switch (imode) {
+			case CVMX_HELPER_INTERFACE_MODE_XAUI:
+			case CVMX_HELPER_INTERFACE_MODE_RXAUI:
+			case CVMX_HELPER_INTERFACE_MODE_SGMII:
+			case CVMX_HELPER_INTERFACE_MODE_RGMII:
+			case CVMX_HELPER_INTERFACE_MODE_GMII:
+			case CVMX_HELPER_INTERFACE_MODE_SPI:
+				rv = cvmx_pko_alloc_iport_and_queues(interface, port, 1,
+								     num_possible_cpus());
+				WARN(rv, "cvmx_pko_alloc_iport_and_queues failed");
+				if (rv)
+					return;
+				break;
+			default:
+				break;
+			}
+		}
+	}
+}
+
+static __devinit int cvm_oct_configure_common_hw(void)
+{
 	/* Setup the FPA */
 	cvmx_fpa_enable();
 
@@ -217,6 +320,9 @@ static __devinit int cvm_oct_configure_common_hw(void)
 	/* more configuration needs to be done, so enable ipd seperately */
 	cvmx_ipd_cfg.ipd_enable = 0;
 
+	if (cvm_oct_pko_lockless())
+		cvm_oct_set_pko_multiqueue();
+
 	cvmx_helper_initialize_packet_io_global();
 
 #ifdef __LITTLE_ENDIAN
@@ -569,6 +675,20 @@ static const struct net_device_ops cvm_oct_sgmii_netdev_ops = {
 	.ndo_poll_controller	= cvm_oct_poll_controller,
 #endif
 };
+static const struct net_device_ops cvm_oct_sgmii_lockless_netdev_ops = {
+	.ndo_init		= cvm_oct_sgmii_init,
+	.ndo_open		= cvm_oct_sgmii_open,
+	.ndo_stop		= cvm_oct_sgmii_stop,
+	.ndo_start_xmit		= cvm_oct_xmit_lockless,
+	.ndo_set_rx_mode	= cvm_oct_common_set_multicast_list,
+	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_common_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
 static const struct net_device_ops cvm_oct_spi_netdev_ops = {
 	.ndo_init		= cvm_oct_spi_init,
 	.ndo_uninit		= cvm_oct_spi_uninit,
@@ -583,6 +703,20 @@ static const struct net_device_ops cvm_oct_spi_netdev_ops = {
 	.ndo_poll_controller	= cvm_oct_poll_controller,
 #endif
 };
+static const struct net_device_ops cvm_oct_spi_lockless_netdev_ops = {
+	.ndo_init		= cvm_oct_spi_init,
+	.ndo_uninit		= cvm_oct_spi_uninit,
+	.ndo_open		= cvm_oct_phy_setup_device,
+	.ndo_start_xmit		= cvm_oct_xmit_lockless,
+	.ndo_set_rx_mode	= cvm_oct_common_set_multicast_list,
+	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_common_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
 static const struct net_device_ops cvm_oct_rgmii_netdev_ops = {
 	.ndo_init		= cvm_oct_rgmii_init,
 	.ndo_open		= cvm_oct_rgmii_open,
@@ -597,6 +731,20 @@ static const struct net_device_ops cvm_oct_rgmii_netdev_ops = {
 	.ndo_poll_controller	= cvm_oct_poll_controller,
 #endif
 };
+static const struct net_device_ops cvm_oct_rgmii_lockless_netdev_ops = {
+	.ndo_init		= cvm_oct_rgmii_init,
+	.ndo_open		= cvm_oct_rgmii_open,
+	.ndo_stop		= cvm_oct_rgmii_stop,
+	.ndo_start_xmit		= cvm_oct_xmit_lockless,
+	.ndo_set_rx_mode	= cvm_oct_common_set_multicast_list,
+	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_common_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
 #ifdef CONFIG_RAPIDIO
 static const struct net_device_ops cvm_oct_srio_netdev_ops = {
 	.ndo_init		= cvm_oct_srio_init,
@@ -762,6 +910,9 @@ static int __devinit cvm_oct_probe(struct platform_device *pdev)
 				continue;
 			}
 
+			if (disable_core_queueing)
+				dev->tx_queue_len = 0;
+
 			/* Initialize the device private structure. */
 			priv = netdev_priv(dev);
 			INIT_LIST_HEAD(&priv->srio_bcast);
@@ -830,7 +981,8 @@ static int __devinit cvm_oct_probe(struct platform_device *pdev)
 
 			case CVMX_HELPER_INTERFACE_MODE_XAUI:
 			case CVMX_HELPER_INTERFACE_MODE_RXAUI:
-				dev->netdev_ops = &cvm_oct_sgmii_netdev_ops;
+				dev->netdev_ops = priv->tx_multiple_queues ?
+					&cvm_oct_sgmii_lockless_netdev_ops : &cvm_oct_sgmii_netdev_ops;
 				strcpy(dev->name, "xaui%d");
 				break;
 
@@ -840,18 +992,21 @@ static int __devinit cvm_oct_probe(struct platform_device *pdev)
 				break;
 
 			case CVMX_HELPER_INTERFACE_MODE_SGMII:
-				dev->netdev_ops = &cvm_oct_sgmii_netdev_ops;
+				dev->netdev_ops = priv->tx_multiple_queues ?
+					&cvm_oct_sgmii_lockless_netdev_ops : &cvm_oct_sgmii_netdev_ops;
 				strcpy(dev->name, "eth%d");
 				break;
 
 			case CVMX_HELPER_INTERFACE_MODE_SPI:
-				dev->netdev_ops = &cvm_oct_spi_netdev_ops;
+				dev->netdev_ops = priv->tx_multiple_queues ?
+					&cvm_oct_spi_lockless_netdev_ops : &cvm_oct_spi_netdev_ops;
 				strcpy(dev->name, "spi%d");
 				break;
 
 			case CVMX_HELPER_INTERFACE_MODE_RGMII:
 			case CVMX_HELPER_INTERFACE_MODE_GMII:
-				dev->netdev_ops = &cvm_oct_rgmii_netdev_ops;
+				dev->netdev_ops = priv->tx_multiple_queues ?
+					&cvm_oct_rgmii_lockless_netdev_ops : &cvm_oct_rgmii_netdev_ops;
 				strcpy(dev->name, "eth%d");
 				break;
 #ifdef CONFIG_RAPIDIO
diff --git a/drivers/net/ethernet/octeon/octeon-ethernet.h b/drivers/net/ethernet/octeon/octeon-ethernet.h
index 7f964d2..7c7ebf3 100644
--- a/drivers/net/ethernet/octeon/octeon-ethernet.h
+++ b/drivers/net/ethernet/octeon/octeon-ethernet.h
@@ -132,6 +132,7 @@ int cvm_oct_ioctl(struct net_device *dev, struct ifreq *rq, int cmd);
 int cvm_oct_phy_setup_device(struct net_device *dev);
 
 int cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev);
+int cvm_oct_xmit_lockless(struct sk_buff *skb, struct net_device *dev);
 
 void cvm_oct_poll_controller(struct net_device *dev);
 void cvm_oct_rx_initialize(int);
-- 
1.7.5.4

