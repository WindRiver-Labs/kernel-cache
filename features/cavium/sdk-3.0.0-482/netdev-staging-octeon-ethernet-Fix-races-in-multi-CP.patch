From ac43ba64fb18f5e095423c8f77d32e2b5e5278fe Mon Sep 17 00:00:00 2001
From: David Daney <david.daney@cavium.com>
Date: Wed, 4 Jul 2012 12:26:17 -0700
Subject: [PATCH 111/337] netdev/staging: octeon-ethernet: Fix races in
 multi-CPU receive.

Based On SDK 3.0.0-482

We need to hold a lock when updating to ensure consistency.

Signed-off-by: David Daney <david.daney@cavium.com>
Signed-off-by: Corey Minyard <cminyard@mvista.com>
Signed-off-by: Jack Tan <jack.tan@windriver.com>
---
 drivers/staging/octeon/ethernet-rx.c |  153 ++++++++++++++++++++++------------
 1 files changed, 99 insertions(+), 54 deletions(-)

diff --git a/drivers/staging/octeon/ethernet-rx.c b/drivers/staging/octeon/ethernet-rx.c
index c5557da..7daacf9 100644
--- a/drivers/staging/octeon/ethernet-rx.c
+++ b/drivers/staging/octeon/ethernet-rx.c
@@ -43,8 +43,6 @@
 #include <net/xfrm.h>
 #endif /* CONFIG_XFRM */
 
-#include <linux/atomic.h>
-
 #include <asm/octeon/octeon.h>
 
 #include "ethernet-defines.h"
@@ -61,6 +59,7 @@
 
 struct cvm_napi_wrapper {
 	struct napi_struct napi;
+	int available;
 } ____cacheline_aligned_in_smp;
 
 static struct cvm_napi_wrapper cvm_oct_napi[NR_CPUS] __cacheline_aligned_in_smp;
@@ -68,11 +67,16 @@ static struct cvm_napi_wrapper cvm_oct_napi[NR_CPUS] __cacheline_aligned_in_smp;
 struct cvm_oct_core_state {
 	int baseline_cores;
 	/*
-	 * The number of additional cores that could be processing
-	 * input packtes.
+	 * We want to read this without having to acquire the lock,
+	 * make it volatile so we are likely to get a fairly current
+	 * value.
+	 */
+	volatile int active_cores;
+	/*
+	 * cvm_napi_wrapper.available and active_cores must be kept
+	 * consistent with this lock.
 	 */
-	atomic_t available_cores;
-	cpumask_t cpu_state;
+	spinlock_t lock;
 } ____cacheline_aligned_in_smp;
 
 static struct cvm_oct_core_state core_state __cacheline_aligned_in_smp;
@@ -87,39 +91,66 @@ static void cvm_oct_enable_one_cpu(void)
 {
 	int v;
 	int cpu;
-
-	/* Check to see if more CPUs are available for receive processing... */
-	v = atomic_sub_if_positive(1, &core_state.available_cores);
-	if (v < 0)
-		return;
-
+	unsigned long flags;
+	spin_lock_irqsave(&core_state.lock, flags);
 	/* ... if a CPU is available, Turn on NAPI polling for that CPU.  */
 	for_each_online_cpu(cpu) {
-		if (!cpu_test_and_set(cpu, core_state.cpu_state)) {
-			v = smp_call_function_single(cpu, cvm_oct_enable_napi,
-						     NULL, 0);
-			if (v)
-				panic("Can't enable NAPI.");
-			break;
+		if (cvm_oct_napi[cpu].available > 0) {
+			cvm_oct_napi[cpu].available--;
+			core_state.active_cores++;
+			spin_unlock_irqrestore(&core_state.lock, flags);
+			if (cpu == smp_processor_id()) {
+				cvm_oct_enable_napi(NULL);
+			} else {
+#ifdef CONFIG_SMP
+				v = smp_call_function_single(cpu, cvm_oct_enable_napi,
+                                                    NULL, 0);
+				if (v)
+					panic("Can't enable NAPI.");
+#else
+				BUG();
+#endif
+			}
+			goto out;
 		}
 	}
+	spin_unlock_irqrestore(&core_state.lock, flags);
+out:
+	return;
 }
 
-static void cvm_oct_no_more_work(void)
+static void cvm_oct_no_more_work(struct napi_struct *napi)
 {
-	int cpu = smp_processor_id();
+	struct cvm_napi_wrapper *nr = container_of(napi, struct cvm_napi_wrapper, napi);
+	int current_active;
+	unsigned long flags;
 
-	/*
-	 * CPU zero is special.  It always has the irq enabled when
-	 * waiting for incoming packets.
-	 */
-	if (cpu == 0) {
-		enable_irq(OCTEON_IRQ_WORKQ0 + pow_receive_group);
-		return;
-	}
+	spin_lock_irqsave(&core_state.lock, flags);
+
+	core_state.active_cores--;
+	current_active = core_state.active_cores;
+	nr->available++;
+	BUG_ON(nr->available != 1);
 
-	cpu_clear(cpu, core_state.cpu_state);
-	atomic_add(1, &core_state.available_cores);
+	spin_unlock_irqrestore(&core_state.lock, flags);
+
+	if (current_active == 0) {
+		/*
+		 * No more CPUs doing processing, enable interrupts so
+		 * we can start processing again when there is
+		 * something to do.
+		 */
+		union cvmx_pow_wq_int_thrx int_thr;
+		int_thr.u64 = 0;
+		int_thr.s.iq_thr = 1;
+		int_thr.s.ds_thr = 1;
+		/*
+		 * Enable POW interrupt when our port has at
+		 * least one packet.
+		 */
+		cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group),
+			       int_thr.u64);
+	}
 }
 
 /**
@@ -130,8 +161,30 @@ static void cvm_oct_no_more_work(void)
  */
 static irqreturn_t cvm_oct_do_interrupt(int cpl, void *dev_id)
 {
+	int cpu = smp_processor_id();
+	unsigned long flags;
+	union cvmx_pow_wq_int wq_int;
+
 	/* Disable the IRQ and start napi_poll. */
-	disable_irq_nosync(OCTEON_IRQ_WORKQ0 + pow_receive_group);
+
+	cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group), 0);
+
+	wq_int.u64 = 0;
+	wq_int.s.wq_int = 1 << pow_receive_group;
+	cvmx_write_csr(CVMX_POW_WQ_INT, wq_int.u64);
+
+	spin_lock_irqsave(&core_state.lock, flags);
+
+	/* ... and NAPI better not be running on this CPU.  */
+	BUG_ON(cvm_oct_napi[cpu].available != 1);
+	cvm_oct_napi[cpu].available--;
+
+	/* There better be cores available...  */
+	core_state.active_cores++;
+	BUG_ON(core_state.active_cores > core_state.baseline_cores);
+
+	spin_unlock_irqrestore(&core_state.lock, flags);
+
 	cvm_oct_enable_napi(NULL);
 
 	return IRQ_HANDLED;
@@ -274,7 +327,6 @@ static int cvm_oct_napi_poll(struct napi_struct *napi, int budget)
 		if (work == NULL) {
 			union cvmx_pow_wq_int wq_int;
 			wq_int.u64 = 0;
-			wq_int.s.iq_dis = 1 << pow_receive_group;
 			wq_int.s.wq_int = 1 << pow_receive_group;
 			cvmx_write_csr(CVMX_POW_WQ_INT, wq_int.u64);
 			break;
@@ -296,7 +348,7 @@ static int cvm_oct_napi_poll(struct napi_struct *napi, int budget)
 			 */
 			union cvmx_pow_wq_int_cntx counts;
 			int backlog;
-			int cores_in_use = core_state.baseline_cores - atomic_read(&core_state.available_cores);
+			int cores_in_use = core_state.active_cores;
 			counts.u64 = cvmx_read_csr(CVMX_POW_WQ_INT_CNTX(pow_receive_group));
 			backlog = counts.s.iq_cnt + counts.s.ds_cnt;
 			if (backlog > budget * cores_in_use && napi != NULL)
@@ -503,7 +555,7 @@ static int cvm_oct_napi_poll(struct napi_struct *napi, int budget)
 	if (rx_count < budget && napi != NULL) {
 		/* No more work */
 		napi_complete(napi);
-		cvm_oct_no_more_work();
+		cvm_oct_no_more_work(napi);
 	}
 	return rx_count;
 }
@@ -525,8 +577,6 @@ void cvm_oct_rx_initialize(void)
 {
 	int i;
 	struct net_device *dev_for_napi = NULL;
-	union cvmx_pow_wq_int_thrx int_thr;
-	union cvmx_pow_wq_int_pc int_pc;
 
 	for (i = 0; i < TOTAL_NUMBER_OF_PORTS; i++) {
 		if (cvm_oct_device[i]) {
@@ -538,18 +588,24 @@ void cvm_oct_rx_initialize(void)
 	if (NULL == dev_for_napi)
 		panic("No net_devices were allocated.");
 
-	if (max_rx_cpus > 1  && max_rx_cpus < num_online_cpus())
-		atomic_set(&core_state.available_cores, max_rx_cpus);
+	if (max_rx_cpus >= 1  && max_rx_cpus < num_online_cpus())
+		core_state.baseline_cores = max_rx_cpus;
 	else
-		atomic_set(&core_state.available_cores, num_online_cpus());
-	core_state.baseline_cores = atomic_read(&core_state.available_cores);
+		core_state.baseline_cores = num_online_cpus();
 
-	core_state.cpu_state = CPU_MASK_NONE;
 	for_each_possible_cpu(i) {
+		cvm_oct_napi[i].available = 1;
 		netif_napi_add(dev_for_napi, &cvm_oct_napi[i].napi,
 			       cvm_oct_napi_poll, rx_napi_weight);
 		napi_enable(&cvm_oct_napi[i].napi);
 	}
+	/*
+	 * Before interrupts are enabled, no RX processing will occur,
+	 * so we can initialize all those things out side of the
+	 * lock.
+	 */
+	spin_lock_init(&core_state.lock);
+
 	/* Register an IRQ hander for to receive POW interrupts */
 	i = request_irq(OCTEON_IRQ_WORKQ0 + pow_receive_group,
 			cvm_oct_do_interrupt, 0, "Ethernet", cvm_oct_device);
@@ -558,21 +614,10 @@ void cvm_oct_rx_initialize(void)
 		panic("Could not acquire Ethernet IRQ %d\n",
 		      OCTEON_IRQ_WORKQ0 + pow_receive_group);
 
-	disable_irq_nosync(OCTEON_IRQ_WORKQ0 + pow_receive_group);
-
-	int_thr.u64 = 0;
-	int_thr.s.tc_en = 1;
-	int_thr.s.tc_thr = 1;
-	/* Enable POW interrupt when our port has at least one packet */
-	cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group), int_thr.u64);
-
-	int_pc.u64 = 0;
-	int_pc.s.pc_thr = 5;
-	cvmx_write_csr(CVMX_POW_WQ_INT_PC, int_pc.u64);
-
-
 	/* Scheduld NAPI now.  This will indirectly enable interrupts. */
+	preempt_disable();
 	cvm_oct_enable_one_cpu();
+	preempt_enable();
 }
 
 void cvm_oct_rx_shutdown(void)
-- 
1.7.5.4

