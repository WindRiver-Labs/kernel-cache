From ee273534b109282d8203d6dda3e3836a491c85cc Mon Sep 17 00:00:00 2001
From: Chandrakala Chavva <cchavva@caviumnetworks.com>
Date: Fri, 15 Nov 2013 18:10:56 -0800
Subject: [PATCH 345/518] MIPS: OCTEON: Sync up SE files.

Source: Cavium Networks, Inc.
MR: 00000
Type: Integration
Disposition: Merged from Octeon Tree
ChangeID: 2a113018bd1d9a9ce15472fa06ef2b5f69fbc64c
Description:

New queueid added for BCH block.

Bug #8686

Signed-off-by: Chandrakala Chavva <cchavva@caviumnetworks.com>
Signed-off-by: Corey Minyard <cminyard@mvista.com>
[Original patch taken from Cavium SDK 3.1.1 525]
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c | 36 ++++++++-----
 .../mips/cavium-octeon/executive/cvmx-helper-agl.c |  7 ++-
 arch/mips/cavium-octeon/executive/octeon-feature.c |  3 ++
 arch/mips/include/asm/octeon/cvmx-cmd-queue.h      | 63 ++++++++++++++--------
 arch/mips/include/asm/octeon/octeon-feature.h      | 21 +++++++-
 5 files changed, 92 insertions(+), 38 deletions(-)

diff --git a/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c b/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c
index 0ab102b..4466f29 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c
@@ -1,5 +1,5 @@
 /***********************license start***************
- * Copyright (c) 2003-2010  Cavium Inc. (support@cavium.com). All rights
+ * Copyright (c) 2003-2013  Cavium Inc. (support@cavium.com). All rights
  * reserved.
  *
  *
@@ -43,7 +43,7 @@
  * Support functions for managing command queues used for
  * various hardware blocks.
  *
- * <hr>$Revision: 78972 $<hr>
+ * <hr>$Revision: 90196 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <linux/export.h>
@@ -89,19 +89,25 @@ static cvmx_cmd_queue_result_t __cvmx_cmd_queue_init_state_ptr(void)
 #if defined(CONFIG_CAVIUM_RESERVE32) && CONFIG_CAVIUM_RESERVE32
 	if (octeon_reserve32_memory)
 		__cvmx_cmd_queue_state_ptr = cvmx_bootmem_alloc_named_range(sizeof(*__cvmx_cmd_queue_state_ptr),
-									    octeon_reserve32_memory,
-									    octeon_reserve32_memory + (CONFIG_CAVIUM_RESERVE32 << 20) - 1,
-									    128, alloc_name);
+					       octeon_reserve32_memory,
+					       (octeon_reserve32_memory
+					        + (CONFIG_CAVIUM_RESERVE32 << 20)
+					        - 1),
+					      128, alloc_name);
 	else
 #endif
-		__cvmx_cmd_queue_state_ptr = cvmx_bootmem_alloc_named(sizeof(*__cvmx_cmd_queue_state_ptr), 128, alloc_name);
+		__cvmx_cmd_queue_state_ptr =
+			cvmx_bootmem_alloc_named(sizeof(*__cvmx_cmd_queue_state_ptr),
+						 128, alloc_name);
 #else
-	__cvmx_cmd_queue_state_ptr = cvmx_bootmem_alloc_named(sizeof(*__cvmx_cmd_queue_state_ptr), 128, alloc_name);
+	__cvmx_cmd_queue_state_ptr = cvmx_bootmem_alloc_named(sizeof(*__cvmx_cmd_queue_state_ptr),
+							      128, alloc_name);
 #endif
 	if (__cvmx_cmd_queue_state_ptr)
 		memset(__cvmx_cmd_queue_state_ptr, 0, sizeof(*__cvmx_cmd_queue_state_ptr));
 	else {
-		const cvmx_bootmem_named_block_desc_t *block_desc = cvmx_bootmem_find_named_block(alloc_name);
+		const cvmx_bootmem_named_block_desc_t *block_desc =
+				cvmx_bootmem_find_named_block(alloc_name);
 		if (block_desc)
 			__cvmx_cmd_queue_state_ptr = cvmx_phys_to_ptr(block_desc->base_addr);
 		else {
@@ -175,10 +181,13 @@ cvmx_cmd_queue_result_t cvmx_cmd_queue_initialize(cvmx_cmd_queue_id_t queue_id,
 		union cvmx_fpa_ctl_status status;
 		void *buffer;
 
-		status.u64 = cvmx_read_csr(CVMX_FPA_CTL_STATUS);
-		if (!status.s.enb) {
-			cvmx_dprintf("ERROR: cvmx_cmd_queue_initialize: FPA is not enabled.\n");
-			return CVMX_CMD_QUEUE_NO_MEMORY;
+		if (!(octeon_has_feature(OCTEON_FEATURE_FPA3))) {
+			status.u64 = cvmx_read_csr(CVMX_FPA_CTL_STATUS);
+			if (!status.s.enb) {
+				cvmx_dprintf("ERROR: cvmx_cmd_queue_initialize:"
+					     " FPA is not enabled.\n");
+				return CVMX_CMD_QUEUE_NO_MEMORY;
+			}
 		}
 		buffer = cvmx_fpa_alloc(fpa_pool);
 		if (buffer == NULL) {
@@ -289,6 +298,9 @@ int cvmx_cmd_queue_length(cvmx_cmd_queue_id_t queue_id)
 			dmax_counts.u64 = cvmx_read_csr(CVMX_DPI_DMAX_COUNTS(queue_id & 0x7));
 			return dmax_counts.s.dbell;
 		}
+	case CVMX_CMD_QUEUE_BCH:
+		/* Not available */
+		return 0;
 	case CVMX_CMD_QUEUE_END:
 		return CVMX_CMD_QUEUE_INVALID_PARAM;
 	}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-agl.c b/arch/mips/cavium-octeon/executive/cvmx-helper-agl.c
index 0dc4ddd..cc81ac9 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-agl.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-agl.c
@@ -197,6 +197,7 @@ int __cvmx_helper_agl_probe(int interface)
 int __cvmx_helper_agl_enable(int interface)
 {
 	int port = cvmx_helper_agl_get_port(interface);
+	int ipd_port = cvmx_helper_get_ipd_port(interface, port);
 	union cvmx_pko_mem_port_ptrs pko_mem_port_ptrs;
 	union cvmx_pko_reg_read_idx read_idx;
 	int do_link_set = 1;
@@ -230,7 +231,7 @@ int __cvmx_helper_agl_enable(int interface)
 		do_link_set = 0;
 #endif
 	if (do_link_set)
-		cvmx_agl_link_set(port, cvmx_agl_link_get(port), 1);
+		cvmx_agl_link_set(port, cvmx_agl_link_get(ipd_port), 1);
 
 	return 0;
 }
@@ -248,9 +249,7 @@ int __cvmx_helper_agl_enable(int interface)
  */
 cvmx_helper_link_info_t __cvmx_helper_agl_link_get(int ipd_port)
 {
-	int interface = cvmx_helper_get_interface_num(ipd_port);
-	int port = cvmx_helper_agl_get_port(interface);
-	return cvmx_agl_link_get(port);
+	return cvmx_agl_link_get(ipd_port);
 }
 
 /**
diff --git a/arch/mips/cavium-octeon/executive/octeon-feature.c b/arch/mips/cavium-octeon/executive/octeon-feature.c
index ce81e4b..858bf56 100644
--- a/arch/mips/cavium-octeon/executive/octeon-feature.c
+++ b/arch/mips/cavium-octeon/executive/octeon-feature.c
@@ -102,6 +102,7 @@ void __init octeon_feature_init(void)
 
 	OCTEON_FEATURE_SET(OCTEON_FEATURE_SAAD);
 	OCTEON_FEATURE_SET(OCTEON_FEATURE_ZIP);
+	OCTEON_FEATURE_SET(OCTEON_FEATURE_ZIP3);
 	OCTEON_FEATURE_SET(OCTEON_FEATURE_CRYPTO);
 	OCTEON_FEATURE_SET(OCTEON_FEATURE_DORM_CRYPTO);
 	OCTEON_FEATURE_SET(OCTEON_FEATURE_PCIE);
@@ -131,8 +132,10 @@ void __init octeon_feature_init(void)
 	OCTEON_FEATURE_SET(OCTEON_FEATURE_MULTICAST_TIMER);
 	OCTEON_FEATURE_SET(OCTEON_FEATURE_MULTINODE);
 	OCTEON_FEATURE_SET(OCTEON_FEATURE_CIU3);
+	OCTEON_FEATURE_SET(OCTEON_FEATURE_FPA3);
 	OCTEON_FEATURE_SET(OCTEON_FEATURE_CN78XX_WQE);
 	OCTEON_FEATURE_SET(OCTEON_FEATURE_SPI);
+	OCTEON_FEATURE_SET(OCTEON_FEATURE_BCH);
 
 	val = OCTEON_FEATURE_SUCCESS;
 
diff --git a/arch/mips/include/asm/octeon/cvmx-cmd-queue.h b/arch/mips/include/asm/octeon/cvmx-cmd-queue.h
index c53b241..f7f780b 100644
--- a/arch/mips/include/asm/octeon/cvmx-cmd-queue.h
+++ b/arch/mips/include/asm/octeon/cvmx-cmd-queue.h
@@ -82,7 +82,7 @@
  * internal cycle counter to completely eliminate any causes of
  * bus traffic.
  *
- * <hr> $Revision: 85265 $ <hr>
+ * <hr> $Revision: 90195 $ <hr>
  */
 
 #ifndef __CVMX_CMD_QUEUE_H__
@@ -120,7 +120,9 @@ typedef enum {
 	CVMX_CMD_QUEUE_RAID = 0x30000,
 	CVMX_CMD_QUEUE_DMA_BASE = 0x40000,
 #define CVMX_CMD_QUEUE_DMA(queue) ((cvmx_cmd_queue_id_t)(CVMX_CMD_QUEUE_DMA_BASE + (0xffff&(queue))))
-	CVMX_CMD_QUEUE_END = 0x50000,
+	CVMX_CMD_QUEUE_BCH = 0x50000,
+#define CVMX_CMD_QUEUE_BCH(queue) ((cvmx_cmd_queue_id_t)(CVMX_CMD_QUEUE_BCH + (0xffff&(queue))))
+	CVMX_CMD_QUEUE_END = 0x60000,
 } cvmx_cmd_queue_id_t;
 
 /**
@@ -186,7 +188,9 @@ typedef struct {
  *
  * @return CVMX_CMD_QUEUE_SUCCESS or a failure code
  */
-cvmx_cmd_queue_result_t cvmx_cmd_queue_initialize(cvmx_cmd_queue_id_t queue_id, int max_depth, int fpa_pool, int pool_size);
+cvmx_cmd_queue_result_t cvmx_cmd_queue_initialize(cvmx_cmd_queue_id_t queue_id,
+						  int max_depth, int fpa_pool,
+						  int pool_size);
 
 /**
  * Shutdown a queue a free it's command buffers to the FPA. The
@@ -231,10 +235,12 @@ void *cvmx_cmd_queue_buffer(cvmx_cmd_queue_id_t queue_id);
  */
 static inline int __cvmx_cmd_queue_get_index(cvmx_cmd_queue_id_t queue_id)
 {
-	/* Warning: This code currently only works with devices that have 256 queues
-	   or less. Devices with more than 16 queues are laid out in memory to allow
-	   cores quick access to every 16th queue. This reduces cache thrashing
-	   when you are running 16 queues per port to support lockless operation */
+	/* Warning: This code currently only works with devices that have 256
+	 * queues or less.  Devices with more than 16 queues are laid out in
+	 * memory to allow cores quick access to every 16th queue. This reduces
+	 * cache thrashing when you are running 16 queues per port to support
+	 * lockless operation
+	 */
 	int unit = queue_id >> 16;
 	int q = (queue_id >> 4) & 0xf;
 	int core = queue_id & 0xf;
@@ -248,7 +254,9 @@ static inline int __cvmx_cmd_queue_get_index(cvmx_cmd_queue_id_t queue_id)
  *
  * @param queue_id Queue ID to lock
  * @param qptr     Pointer to the queue's global state
- */ static inline void __cvmx_cmd_queue_lock(cvmx_cmd_queue_id_t queue_id, __cvmx_cmd_queue_state_t * qptr)
+ */
+static inline void __cvmx_cmd_queue_lock(cvmx_cmd_queue_id_t queue_id,
+					 __cvmx_cmd_queue_state_t * qptr)
 {
 	extern CVMX_SHARED __cvmx_cmd_queue_all_state_t *__cvmx_cmd_queue_state_ptr;
 	int tmp;
@@ -258,7 +266,9 @@ static inline int __cvmx_cmd_queue_get_index(cvmx_cmd_queue_id_t queue_id)
 		      ".set noreorder\n"
 		      "1:\n"
 		      "lld     %[my_ticket], %[ticket_ptr]\n"
-		      /* Atomic add one to ticket_ptr 64-bit operation for endian nutral access. */
+		      /* Atomic add one to ticket_ptr 64-bit operation for
+		       * endian nutral access.
+		       */
 		      "daddiu  %[ticket], %[my_ticket], 1\n"
 		      /*    and store the original value  in my_ticket */
 		      "scd     %[ticket], %[ticket_ptr]\n"
@@ -306,7 +316,8 @@ static inline void __cvmx_cmd_queue_unlock(__cvmx_cmd_queue_state_t * qptr)
  *
  * @return Queue structure or NULL on failure
  */
-static inline __cvmx_cmd_queue_state_t *__cvmx_cmd_queue_get_state(cvmx_cmd_queue_id_t queue_id)
+static inline __cvmx_cmd_queue_state_t *
+__cvmx_cmd_queue_get_state(cvmx_cmd_queue_id_t queue_id)
 {
 	extern CVMX_SHARED __cvmx_cmd_queue_all_state_t *__cvmx_cmd_queue_state_ptr;
 	if (CVMX_ENABLE_PARAMETER_CHECKING) {
@@ -333,7 +344,9 @@ static inline __cvmx_cmd_queue_state_t *__cvmx_cmd_queue_get_state(cvmx_cmd_queu
  *
  * @return CVMX_CMD_QUEUE_SUCCESS or a failure code
  */
-static inline cvmx_cmd_queue_result_t cvmx_cmd_queue_write(cvmx_cmd_queue_id_t queue_id, int use_locking, int cmd_count, uint64_t * cmds)
+static inline cvmx_cmd_queue_result_t
+cvmx_cmd_queue_write(cvmx_cmd_queue_id_t queue_id, int use_locking,
+		     int cmd_count, uint64_t * cmds)
 {
 	__cvmx_cmd_queue_state_t *qptr = __cvmx_cmd_queue_get_state(queue_id);
 
@@ -379,16 +392,18 @@ static inline cvmx_cmd_queue_result_t cvmx_cmd_queue_write(cvmx_cmd_queue_id_t q
 			return CVMX_CMD_QUEUE_NO_MEMORY;
 		}
 		ptr = (uint64_t *) cvmx_phys_to_ptr((uint64_t) qptr->base_ptr_div128 << 7);
-		/* Figure out how many command words will fit in this buffer. One
-		   location will be needed for the next buffer pointer */
+		/* Figure out how many command words will fit in this buffer.
+		 * One location will be needed for the next buffer pointer
+		 */
 		count = qptr->pool_size_m1 - qptr->index;
 		ptr += qptr->index;
 		cmd_count -= count;
 		while (count--)
 			*ptr++ = *cmds++;
 		*ptr = cvmx_ptr_to_phys(new_buffer);
-		/* The current buffer is full and has a link to the next buffer. Time
-		   to write the rest of the commands into the new buffer */
+		/* The current buffer is full and has a link to the next buffer.
+		 * Time to write the rest of the commands into the new buffer
+		 */
 		qptr->base_ptr_div128 = *ptr >> 7;
 		qptr->index = cmd_count;
 		ptr = new_buffer;
@@ -416,7 +431,9 @@ static inline cvmx_cmd_queue_result_t cvmx_cmd_queue_write(cvmx_cmd_queue_id_t q
  *
  * @return CVMX_CMD_QUEUE_SUCCESS or a failure code
  */
-static inline cvmx_cmd_queue_result_t cvmx_cmd_queue_write2(cvmx_cmd_queue_id_t queue_id, int use_locking, uint64_t cmd1, uint64_t cmd2)
+static inline cvmx_cmd_queue_result_t
+cvmx_cmd_queue_write2(cvmx_cmd_queue_id_t queue_id, int use_locking,
+		      uint64_t cmd1, uint64_t cmd2)
 {
 	__cvmx_cmd_queue_state_t *qptr = __cvmx_cmd_queue_get_state(queue_id);
 
@@ -497,7 +514,9 @@ static inline cvmx_cmd_queue_result_t cvmx_cmd_queue_write2(cvmx_cmd_queue_id_t
  *
  * @return CVMX_CMD_QUEUE_SUCCESS or a failure code
  */
-static inline cvmx_cmd_queue_result_t cvmx_cmd_queue_write3(cvmx_cmd_queue_id_t queue_id, int use_locking, uint64_t cmd1, uint64_t cmd2, uint64_t cmd3)
+static inline cvmx_cmd_queue_result_t
+cvmx_cmd_queue_write3(cvmx_cmd_queue_id_t queue_id, int use_locking,
+		      uint64_t cmd1, uint64_t cmd2, uint64_t cmd3)
 {
 	__cvmx_cmd_queue_state_t *qptr = __cvmx_cmd_queue_get_state(queue_id);
 
@@ -531,8 +550,9 @@ static inline cvmx_cmd_queue_result_t cvmx_cmd_queue_write3(cvmx_cmd_queue_id_t
 		ptr[2] = cmd3;
 	} else {
 		uint64_t *ptr;
-		/* Figure out how many command words will fit in this buffer. One
-		   location will be needed for the next buffer pointer */
+		/* Figure out how many command words will fit in this buffer.
+		 * One location will be needed for the next buffer pointer
+		 */
 		int count = qptr->pool_size_m1 - qptr->index;
 		/* We need a new command buffer. Fail if there isn't one available */
 		uint64_t *new_buffer = (uint64_t *) cvmx_fpa_alloc(qptr->fpa_pool);
@@ -551,8 +571,9 @@ static inline cvmx_cmd_queue_result_t cvmx_cmd_queue_write3(cvmx_cmd_queue_id_t
 				*ptr++ = cmd3;
 		}
 		*ptr = cvmx_ptr_to_phys(new_buffer);
-		/* The current buffer is full and has a link to the next buffer. Time
-		   to write the rest of the commands into the new buffer */
+		/* The current buffer is full and has a link to the next buffer.
+		 * Time to write the rest of the commands into the new buffer
+		 */
 		qptr->base_ptr_div128 = *ptr >> 7;
 		qptr->index = 0;
 		ptr = new_buffer;
diff --git a/arch/mips/include/asm/octeon/octeon-feature.h b/arch/mips/include/asm/octeon/octeon-feature.h
index 59d093e..b9e45cb 100644
--- a/arch/mips/include/asm/octeon/octeon-feature.h
+++ b/arch/mips/include/asm/octeon/octeon-feature.h
@@ -42,7 +42,6 @@
  *
  * File defining checks for different Octeon features.
  *
- * <hr>$Revision: 30468 $<hr>
  */
 
 #ifndef __OCTEON_FEATURE_H__
@@ -153,10 +152,16 @@ typedef enum {
 				/**<  Octeon has node support */
 	OCTEON_FEATURE_CIU3,
 				/**<  Octeon has CIU3 */
+	OCTEON_FEATURE_FPA3,
+				/**<  Octeon has FPA first seen on 78XX */
 	OCTEON_FEATURE_CN78XX_WQE,
 				/**<  CN78XX has different fields in word0 - word2 */
 	OCTEON_FEATURE_SPI,
 				/**< Octeon supports SPI interfaces */
+	OCTEON_FEATURE_ZIP3,
+				/**<  Octeon has zip first seen on 78XX */
+ 	OCTEON_FEATURE_BCH,
+  				/**< Octeon supports BCH ECC */
 	OCTEON_MAX_FEATURE
 } octeon_feature_t;
 
@@ -175,6 +180,16 @@ static inline int octeon_has_feature_OCTEON_FEATURE_ZIP(void)
 		return !cvmx_fuse_read(121);
 }
 
+static inline int octeon_has_feature_OCTEON_FEATURE_ZIP3(void)
+{
+	return (OCTEON_IS_MODEL(OCTEON_CN78XX));
+}
+
+static inline int octeon_has_feature_OCTEON_FEATURE_BCH(void)
+{
+	return OCTEON_IS_OCTEON3();
+}
+
 static inline int octeon_has_feature_OCTEON_FEATURE_CRYPTO(void)
 {
 	if (!OCTEON_IS_OCTEON1PLUS()) {	/* OCTEON II and later */
@@ -329,6 +344,10 @@ static inline int octeon_has_feature_OCTEON_FEATURE_CIU3(void)
 	return (OCTEON_IS_MODEL(OCTEON_CN78XX));
 }
 
+static inline int octeon_has_feature_OCTEON_FEATURE_FPA3(void)
+{
+	return (OCTEON_IS_MODEL(OCTEON_CN78XX));
+}
 
 static inline int octeon_has_feature_OCTEON_FEATURE_NAND(void)
 {
-- 
1.9.1

