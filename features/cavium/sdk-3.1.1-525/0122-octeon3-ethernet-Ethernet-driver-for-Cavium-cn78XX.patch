From 9f513d6fbfe4a01e4e81666c3bd6c0015c6c256d Mon Sep 17 00:00:00 2001
From: Bin Jiang <bin.jiang@windriver.com>
Date: Thu, 30 Oct 2014 17:21:29 +0800
Subject: [PATCH 122/122] octeon3-ethernet: Ethernet driver for Cavium cn78XX.

Add driver for Cavium cn78xx ethernet.

Signed-off-by: David Daney <david.daney@cavium.com>
[Original patch taken from Cavium SDK 3.1.1 525]
Signed-off-by: Bin Jiang <bin.jiang@windriver.com>
---
 arch/mips/cavium-octeon/executive/Makefile         |     7 +-
 arch/mips/cavium-octeon/executive/cvmx-bootmem.c   |    17 +
 .../cavium-octeon/executive/cvmx-fpa-resource.c    |   243 +
 .../executive/cvmx-global-resources.c              |   579 +
 .../mips/cavium-octeon/executive/cvmx-helper-bgx.c |  1338 ++
 .../mips/cavium-octeon/executive/cvmx-helper-cfg.c |  1049 ++
 .../mips/cavium-octeon/executive/cvmx-helper-pki.c |  1334 ++
 .../cavium-octeon/executive/cvmx-helper-pko3.c     |  1147 ++
 .../cavium-octeon/executive/cvmx-helper-sgmii.c    |     5 +
 .../cavium-octeon/executive/cvmx-helper-util.c     |   214 +-
 arch/mips/cavium-octeon/executive/cvmx-helper.c    |  2076 ++-
 .../cavium-octeon/executive/cvmx-pki-resources.c   |   402 +
 arch/mips/cavium-octeon/executive/cvmx-pki.c       |  1007 ++
 .../executive/cvmx-pko-internal-ports-range.c      |   170 +
 arch/mips/cavium-octeon/executive/cvmx-pko.c       |    19 +
 .../mips/cavium-octeon/executive/cvmx-pko3-queue.c |  1212 ++
 .../cavium-octeon/executive/cvmx-pko3-resources.c  |   173 +
 arch/mips/cavium-octeon/executive/cvmx-pko3.c      |  1613 ++
 arch/mips/cavium-octeon/executive/cvmx-qlm.c       |  1421 ++
 arch/mips/cavium-octeon/executive/cvmx-range.c     |   275 +
 .../cavium-octeon/executive/cvmx-sso-resources.c   |   107 +
 arch/mips/include/asm/octeon/cvmx-address.h        |     9 +
 arch/mips/include/asm/octeon/cvmx-atomic.h         |   676 +
 arch/mips/include/asm/octeon/cvmx-bgx.h            |    64 +
 arch/mips/include/asm/octeon/cvmx-bgxx-defs.h      |  7495 +++++++++
 arch/mips/include/asm/octeon/cvmx-bootmem.h        |    27 +
 arch/mips/include/asm/octeon/cvmx-ciu-defs.h       |     1 +
 arch/mips/include/asm/octeon/cvmx-clock.h          |   162 +
 arch/mips/include/asm/octeon/cvmx-fpa1.h           |   159 +
 arch/mips/include/asm/octeon/cvmx-fpa3.h           |   415 +
 .../include/asm/octeon/cvmx-global-resources.h     |   179 +
 arch/mips/include/asm/octeon/cvmx-gmxx-defs.h      |     1 +
 arch/mips/include/asm/octeon/cvmx-gserx-defs.h     |  6837 ++++++++
 arch/mips/include/asm/octeon/cvmx-helper-bgx.h     |   239 +
 arch/mips/include/asm/octeon/cvmx-helper-cfg.h     |   575 +
 arch/mips/include/asm/octeon/cvmx-helper-ilk.h     |   134 +
 arch/mips/include/asm/octeon/cvmx-helper-pki.h     |   238 +
 arch/mips/include/asm/octeon/cvmx-helper-pko.h     |   100 +
 arch/mips/include/asm/octeon/cvmx-helper-pko3.h    |   107 +
 arch/mips/include/asm/octeon/cvmx-helper-util.h    |   118 +
 arch/mips/include/asm/octeon/cvmx-helper.h         |    90 +-
 arch/mips/include/asm/octeon/cvmx-ilk-defs.h       |  4979 ++++++
 arch/mips/include/asm/octeon/cvmx-ilk.h            |   226 +
 arch/mips/include/asm/octeon/cvmx-mio-defs.h       |     3 +
 arch/mips/include/asm/octeon/cvmx-packet.h         |    19 +-
 arch/mips/include/asm/octeon/cvmx-pciercx-defs.h   |     1 +
 arch/mips/include/asm/octeon/cvmx-pemx-defs.h      |   102 +
 arch/mips/include/asm/octeon/cvmx-pip.h            |     2 +-
 arch/mips/include/asm/octeon/cvmx-pki-cluster.h    |   659 +
 arch/mips/include/asm/octeon/cvmx-pki-defs.h       |  3701 +++++
 arch/mips/include/asm/octeon/cvmx-pki-resources.h  |   162 +
 arch/mips/include/asm/octeon/cvmx-pki.h            |  1206 ++
 arch/mips/include/asm/octeon/cvmx-pko-defs.h       | 16484 ++++++++++++++++---
 .../asm/octeon/cvmx-pko-internal-ports-range.h     |    79 +
 arch/mips/include/asm/octeon/cvmx-pko.h            |    25 +-
 arch/mips/include/asm/octeon/cvmx-pko3-queue.h     |   245 +
 arch/mips/include/asm/octeon/cvmx-pko3-resources.h |    82 +
 arch/mips/include/asm/octeon/cvmx-pko3.h           |   829 +
 arch/mips/include/asm/octeon/cvmx-pow.h            |   148 +-
 arch/mips/include/asm/octeon/cvmx-qlm.h            |   235 +
 arch/mips/include/asm/octeon/cvmx-range.h          |    55 +
 .../mips/include/asm/octeon/cvmx-sriomaintx-defs.h |  4401 +++++
 arch/mips/include/asm/octeon/cvmx-sriox-defs.h     |     1 +
 arch/mips/include/asm/octeon/cvmx-sysinfo.h        |    59 +
 arch/mips/include/asm/octeon/cvmx-tim-defs.h       |  1816 ++
 arch/mips/include/asm/octeon/cvmx-wqe.h            |   275 +-
 arch/mips/include/asm/octeon/cvmx.h                |    15 +-
 arch/mips/include/asm/octeon/octeon-feature.h      |     5 +-
 drivers/staging/Makefile                           |     1 +
 drivers/staging/octeon/Kconfig                     |    16 +
 drivers/staging/octeon/Makefile                    |     3 +
 drivers/staging/octeon/octeon-bgx-nexus.c          |   160 +
 drivers/staging/octeon/octeon-bgx-port.c           |   415 +
 drivers/staging/octeon/octeon-bgx.h                |    53 +
 drivers/staging/octeon/octeon3-ethernet.c          |  1147 ++
 75 files changed, 66534 insertions(+), 3109 deletions(-)
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-fpa-resource.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-global-resources.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-helper-bgx.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-helper-pki.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-helper-pko3.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-pki-resources.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-pki.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-pko-internal-ports-range.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-pko3-queue.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-pko3-resources.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-pko3.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-qlm.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-range.c
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-sso-resources.c
 create mode 100644 arch/mips/include/asm/octeon/cvmx-atomic.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-bgx.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-bgxx-defs.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-clock.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-fpa1.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-fpa3.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-global-resources.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-gserx-defs.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-helper-bgx.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-helper-cfg.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-helper-ilk.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-helper-pki.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-helper-pko.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-helper-pko3.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-ilk-defs.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-ilk.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-pki-cluster.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-pki-defs.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-pki-resources.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-pki.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-pko-internal-ports-range.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-pko3-queue.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-pko3-resources.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-pko3.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-qlm.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-range.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-sriomaintx-defs.h
 create mode 100644 arch/mips/include/asm/octeon/cvmx-tim-defs.h
 create mode 100644 drivers/staging/octeon/octeon-bgx-nexus.c
 create mode 100644 drivers/staging/octeon/octeon-bgx-port.c
 create mode 100644 drivers/staging/octeon/octeon-bgx.h
 create mode 100644 drivers/staging/octeon/octeon3-ethernet.c

diff --git a/arch/mips/cavium-octeon/executive/Makefile b/arch/mips/cavium-octeon/executive/Makefile
index 2b56935..48246bb 100644
--- a/arch/mips/cavium-octeon/executive/Makefile
+++ b/arch/mips/cavium-octeon/executive/Makefile
@@ -14,7 +14,12 @@ obj-y += cvmx-pko.o cvmx-spi.o cvmx-cmd-queue.o \
 	cvmx-helper-board.o cvmx-helper.o cvmx-helper-xaui.o \
 	cvmx-helper-rgmii.o cvmx-helper-sgmii.o cvmx-helper-npi.o \
 	cvmx-helper-loop.o cvmx-helper-spi.o cvmx-helper-util.o \
-	cvmx-interrupt-decodes.o cvmx-interrupt-rsl.o
+	cvmx-interrupt-decodes.o cvmx-interrupt-rsl.o \
+	cvmx-fpa-resource.o cvmx-sso-resources.o cvmx-pko3-resources.o\
+	cvmx-global-resources.o cvmx-range.o cvmx-helper-pko3.o \
+	cvmx-helper-pki.o cvmx-qlm.o cvmx-helper-cfg.o cvmx-pki-resources.o \
+	cvmx-pki.o cvmx-pko3-queue.o cvmx-pko3.o cvmx-helper-bgx.o \
+	cvmx-pko-internal-ports-range.o
 
 obj-y += cvmx-helper-errata.o cvmx-helper-jtag.o
 obj-$(CONFIG_CAVIUM_OCTEON_ERROR_TREE)	+= cvmx-error-trees.o
diff --git a/arch/mips/cavium-octeon/executive/cvmx-bootmem.c b/arch/mips/cavium-octeon/executive/cvmx-bootmem.c
index c04e06b..1cde6d7 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-bootmem.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-bootmem.c
@@ -499,6 +499,23 @@ int cvmx_bootmem_free_named(const char *name)
 }
 
 const cvmx_bootmem_named_block_desc_t *
+__cvmx_bootmem_find_named_block_flags(char *name, uint32_t flags)
+{
+        /* FIXME: Returning a single static object is probably a bad thing */
+        static cvmx_bootmem_named_block_desc_t desc;
+        uint64_t named_addr = cvmx_bootmem_phy_named_block_find(name, flags);
+        if (named_addr) {
+                desc.base_addr = CVMX_BOOTMEM_NAMED_GET_FIELD(named_addr,
+                                                              base_addr);
+                desc.size = CVMX_BOOTMEM_NAMED_GET_FIELD(named_addr, size);
+                strncpy(desc.name, name, sizeof(desc.name));
+                desc.name[sizeof(desc.name) - 1] = 0;
+                return &desc;
+        } else  
+                return NULL;
+}
+
+const cvmx_bootmem_named_block_desc_t *
 cvmx_bootmem_find_named_block(const char *name)
 {
 	/* FIXME: Returning a single static object is probably a bad thing */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-fpa-resource.c b/arch/mips/cavium-octeon/executive/cvmx-fpa-resource.c
new file mode 100644
index 0000000..4e9c92f
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-fpa-resource.c
@@ -0,0 +1,243 @@
+/***********************license start***************
+ * Copyright (c) 2012  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include "linux/export.h"
+#include "asm/octeon/cvmx.h"
+#include "asm/octeon/cvmx-fpa3.h"
+#include "asm/octeon/cvmx-fpa1.h"
+#include "asm/octeon/cvmx-global-resources.h"
+#else
+#include "cvmx.h"
+#include "cvmx-fpa3.h"
+#include "cvmx-global-resources.h"
+#include "cvmx-sysinfo.h"
+#endif
+
+
+static struct global_resource_tag get_fpa_resource_tag(int node)
+{
+	switch(node) {
+	case 0:
+		return CVMX_GR_TAG_FPA;
+	case 1:
+		return cvmx_get_gr_tag('c','v','m','_','f','p','a','_','0','1','.','.','.','.','.','.');
+	case 2:
+		return cvmx_get_gr_tag('c','v','m','_','f','p','a','_','0','2','.','.','.','.','.','.');
+	case 3:
+		return cvmx_get_gr_tag('c','v','m','_','f','p','a','_','0','3','.','.','.','.','.','.');
+	default:
+		/* Add a panic?? */
+		return cvmx_get_gr_tag('i','n','v','a','l','i','d','.','.','.','.','.','.','.','.','.');
+	}
+}
+
+
+static struct global_resource_tag get_aura_resource_tag(int node)
+{
+	switch(node) {
+	case 0:
+		return cvmx_get_gr_tag('c','v','m','_','a','u','r','a','_','0','_','0','.','.','.','.');
+	case 1:
+		return cvmx_get_gr_tag('c','v','m','_','a','u','r','a','_','0','_','1','.','.','.','.');
+	case 2:
+		return cvmx_get_gr_tag('c','v','m','_','a','u','r','a','_','0','_','2','.','.','.','.');
+	case 3:
+		return cvmx_get_gr_tag('c','v','m','_','a','u','r','a','_','0','_','3','.','.','.','.');
+	default:
+		/* Add a panic?? */
+		return cvmx_get_gr_tag('i','n','v','a','l','i','d','.','.','.','.','.','.','.','.','.');
+	}
+}
+
+/**
+ * This will allocate/reserve count number of FPA pools on the specified node to the
+ * calling application. These pools will be for exclusive use of the application
+ * until they are freed.
+ * @param pools_allocated is an array of length count allocated by the application
+ * before invoking the cvmx_allocate_fpa_pool call. On return it will contain the
+ * index numbers of the pools allocated.
+ * If -1 it finds the empty pool to allocate otherwise it reserves the specified pool.
+ * @return 0 on success and -1 on failure.
+ */
+int cvmx_fpa_allocate_fpa_pools(int node, int pools_allocated[], int count)
+{
+	int num_pools = CVMX_FPA_NUM_POOLS;
+	uint64_t owner = 0;
+	int rv = 0;
+	struct global_resource_tag tag;
+
+	if (node == -1) node = cvmx_get_node_num();
+
+	tag = get_fpa_resource_tag(node);
+
+	if (octeon_has_feature(OCTEON_FEATURE_FPA3))
+		num_pools = CVMX_FPA3_NUM_POOLS;
+
+	if (cvmx_create_global_resource_range(tag, num_pools) != 0) {
+		cvmx_dprintf("ERROR: failed to create FPA global resource for"
+			     " node=%d\n", node);
+		return -1;
+	}
+
+	if (pools_allocated[0] >= 0) {
+		while (count--) {
+			rv = cvmx_reserve_global_resource_range(tag, owner, pools_allocated[count], 1);
+			if (rv < 0)
+				return CVMX_RESOURCE_ALREADY_RESERVED;
+		}
+
+	} else {
+		rv = cvmx_resource_alloc_many(tag, owner,
+					      count,
+					      pools_allocated);
+	}
+	return rv;
+}
+
+/** Allocates the pool from global resources and reserves them
+  * @param pool	    FPA pool to allocate/reserve. If -1 it
+  *                 finds the empty pool to allocate.
+  * @return         Allocated pool number OR -1 if fails to allocate
+                    the pool
+  */
+int cvmx_fpa_alloc_pool(int pool)
+{
+	int pool_num = pool;
+
+	if (cvmx_fpa_allocate_fpa_pools(-1, &pool_num, 1) < 0) {
+		return -1;
+	}
+
+	return pool_num;
+}
+EXPORT_SYMBOL(cvmx_fpa_alloc_pool);
+
+int cvmx_fpa_reserve_error_pool(int node)
+{
+	int pool_num = 0;
+
+	if (!octeon_has_feature(OCTEON_FEATURE_FPA3))
+		return 0;
+
+	if (node == -1) node = cvmx_get_node_num();
+
+	if (cvmx_sysinfo_get()->init_core != cvmx_get_core_num())
+		return 0;
+	cvmx_fpa_allocate_fpa_pools(node, &pool_num, 1);
+
+	return 0;
+}
+
+/** Release/Frees the specified pool
+  * @param pool	    Pool to free
+  * @return         0 for success -1 failure
+  */
+int cvmx_fpa_release_pool(int pool) /* AJ: Gotta fix this */
+{
+	if (cvmx_free_global_resource_range_with_base(CVMX_GR_TAG_FPA, pool, 1) == -1) {
+		cvmx_dprintf("\nERROR Failed to release FPA pool %d", (int)pool);
+		return -1;
+	}
+	return 0;
+}
+EXPORT_SYMBOL(cvmx_fpa_release_pool);
+
+int cvmx_fpa3_allocate_auras(int node, int auras_allocated[], int count)
+{
+	int num_aura = CVMX_FPA3_AURA_NUM;
+	uint64_t owner = 0;
+	int rv = 0;
+	struct global_resource_tag tag = get_aura_resource_tag(node);
+
+	if (!OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+		cvmx_dprintf("ERROR:  Aura allocation not supported on this model\n");
+		return -1;
+	}
+
+	if (cvmx_create_global_resource_range(tag, num_aura) != 0) {
+		cvmx_dprintf("ERROR: failed to create aura global resource for node=%d\n", node);
+		return -1;
+	}
+	if (auras_allocated[0] >= 0) {
+		while (count--) {
+			rv = cvmx_reserve_global_resource_range(tag, owner, auras_allocated[count], 1);
+			if (rv < 0)
+				return CVMX_RESOURCE_ALREADY_RESERVED;
+		}
+	} else
+		rv = cvmx_resource_alloc_many(tag, owner, count,
+					auras_allocated);
+	return rv;
+
+}
+EXPORT_SYMBOL(cvmx_fpa3_allocate_auras);
+
+int cvmx_fpa3_allocate_aura(int node)
+{
+	int r;
+	int aura = -1;
+
+	r = cvmx_fpa3_allocate_auras(node, &aura, 1);
+
+	return r == 0 ? aura : -1;
+}
+EXPORT_SYMBOL(cvmx_fpa3_allocate_aura);
+
+int cvmx_fpa3_free_auras(int node, int *auras_allocated, int count)
+{
+	int rv;
+	struct global_resource_tag tag = get_aura_resource_tag(node);
+
+	rv = cvmx_free_global_resource_range_multiple(tag, auras_allocated,
+						      count);
+	return rv;
+}
+EXPORT_SYMBOL(cvmx_fpa3_free_auras);
+
+int cvmx_fpa3_free_pools(int node, int *pools_allocated, int count)
+{
+	int rv;
+	struct global_resource_tag tag = get_fpa_resource_tag(node);
+
+	rv = cvmx_free_global_resource_range_multiple(tag, pools_allocated,
+						      count);
+	return rv;
+}
+EXPORT_SYMBOL(cvmx_fpa3_free_pools);
diff --git a/arch/mips/cavium-octeon/executive/cvmx-global-resources.c b/arch/mips/cavium-octeon/executive/cvmx-global-resources.c
new file mode 100644
index 0000000..3e6a874
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-global-resources.c
@@ -0,0 +1,579 @@
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include <linux/types.h>
+#include <linux/export.h>
+#include "asm/octeon/cvmx-global-resources.h"
+#include "asm/octeon/cvmx-bootmem.h"
+#include "asm/octeon/cvmx.h"
+#include "asm/octeon/cvmx-helper-cfg.h"
+#include "asm/octeon/cvmx-range.h"
+#else
+#include "cvmx.h"
+#include "cvmx-platform.h"
+#include "cvmx-global-resources.h"
+#include "cvmx-bootmem.h"
+#include "cvmx-helper-cfg.h"
+#include "cvmx-range.h"
+#endif
+
+#ifdef CVMX_BUILD_FOR_LINUX_HOST
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <unistd.h>
+#endif
+
+#define CVMX_MAX_GLOBAL_RESOURCES 128
+#define CVMX_RESOURCES_ENTRIES_SIZE (sizeof(cvmx_global_resource_entry_t) * \
+					CVMX_MAX_GLOBAL_RESOURCES)
+
+/**
+ * This macro returns a member of the data
+ * structure. The argument "field" is the member name of the
+ * structure to read. The return type is a uint64_t.
+ */
+#define CVMX_GLOBAL_RESOURCES_GET_FIELD(field)				\
+	__cvmx_struct_get_unsigned_field(__cvmx_global_resources_addr,	\
+		offsetof(cvmx_global_resources_t, field),		\
+		SIZEOF_FIELD(cvmx_global_resources_t, field))
+
+/**
+ * This macro writes a member of the cvmx_global_resources_t
+ * structure. The argument "field" is the member name of the
+ * cvmx_global_resources_t to write.
+ */
+#define CVMX_GLOBAL_RESOURCES_SET_FIELD(field, value)			\
+	__cvmx_struct_set_unsigned_field(__cvmx_global_resources_addr,	\
+		offsetof(cvmx_global_resources_t, field),		\
+		SIZEOF_FIELD(cvmx_global_resources_t, field), value)
+
+/**
+ * This macro returns a member of the cvmx_global_resource_entry_t.
+ * The argument "field" is the member name of this structure.
+ * the return type is a uint64_t. The "addr" parameter is the physical
+ * address of the structure.
+ */
+#define CVMX_RESOURCE_ENTRY_GET_FIELD(addr, field)			\
+	__cvmx_struct_get_unsigned_field(addr,				\
+		offsetof(cvmx_global_resource_entry_t, field),  	\
+		SIZEOF_FIELD(cvmx_global_resource_entry_t, field))
+
+/**
+ * This macro writes a member of the cvmx_global_resource_entry_t
+ * structure. The argument "field" is the member name of the
+ * cvmx_global_resource_entry_t to write. The "addr" parameter
+ * is the physical address of the structure.
+ */
+#define CVMX_RESOURCE_ENTRY_SET_FIELD(addr, field, value)		\
+	__cvmx_struct_set_unsigned_field(addr,				\
+		offsetof(cvmx_global_resource_entry_t, field),  	\
+		SIZEOF_FIELD(cvmx_global_resource_entry_t, field), value)
+
+#define CVMX_GET_RESOURCE_ENTRY(count) 						\
+		(__cvmx_global_resources_addr + 				\
+			offsetof(cvmx_global_resources_t, resource_entry) + 	\
+			(count * sizeof(cvmx_global_resource_entry_t)))
+
+#define CVMX_RESOURCE_TAG_SET_FIELD(addr, field, value) 		\
+	__cvmx_struct_set_unsigned_field(addr,				\
+		offsetof(struct global_resource_tag, field), 		\
+		SIZEOF_FIELD(struct global_resource_tag, field), value)
+
+#define CVMX_RESOURCE_TAG_GET_FIELD(addr, field) 		\
+	__cvmx_struct_get_unsigned_field(addr, 			\
+		offsetof(struct global_resource_tag, field),	\
+		SIZEOF_FIELD(struct global_resource_tag, field))
+
+
+
+
+#define MAX_RESOURCE_TAG_LEN 16
+#define CVMX_GLOBAL_RESOURCE_NO_LOCKING (1)
+
+typedef struct cvmx_global_resource_entry
+{
+	struct global_resource_tag tag;
+	uint64_t phys_addr;
+	uint64_t size;
+} cvmx_global_resource_entry_t;
+
+typedef struct cvmx_global_resources
+{
+#ifdef __LITTLE_ENDIAN_BITFIELD
+	uint32_t rlock;
+	uint32_t pad;
+#else
+	uint32_t pad;
+	uint32_t rlock;
+#endif
+	uint64_t entry_cnt;
+	cvmx_global_resource_entry_t resource_entry[];
+} cvmx_global_resources_t;
+
+/* Not the right place, putting it here for now */
+CVMX_SHARED int cvmx_enable_helper_flag = 0;
+
+static int dbg = 0;
+extern int __cvmx_bootmem_phy_free(uint64_t phy_addr, uint64_t size, uint32_t flags);
+
+static CVMX_SHARED uint64_t __cvmx_global_resources_addr = 0;
+
+/**
+ * This macro returns the size of a member of a structure.
+ */
+#define SIZEOF_FIELD(s, field) sizeof(((s *)NULL)->field)
+
+/**
+ * This function is the implementation of the get macros defined
+ * for individual structure members. The argument are generated
+ * by the macros inorder to read only the needed memory.
+ *
+ * @param base   64bit physical address of the complete structure
+ * @param offset Offset from the beginning of the structure to the member being
+ *               accessed.
+ * @param size   Size of the structure member.
+ *
+ * @return Value of the structure member promoted into a uint64_t.
+ */
+static inline uint64_t __cvmx_struct_get_unsigned_field(uint64_t base, int offset,
+					       		int size)
+{
+	base = (1ull << 63) | (base + offset);
+	switch (size) {
+	case 4:
+		return cvmx_read64_uint32(base);
+	case 8:
+		return cvmx_read64_uint64(base);
+	default:
+		return 0;
+	}
+}
+
+/**
+ * This function is the implementation of the set macros defined
+ * for individual structure members. The argument are generated
+ * by the macros in order to write only the needed memory.
+ *
+ * @param base   64bit physical address of the complete structure
+ * @param offset Offset from the beginning of the structure to the member being
+ *               accessed.
+ * @param size   Size of the structure member.
+ * @param value  Value to write into the structure
+ */
+static inline void __cvmx_struct_set_unsigned_field(uint64_t base, int offset, int size,
+					   		uint64_t value)
+{
+	base = (1ull << 63) | (base + offset);
+	switch (size) {
+	case 4:
+		cvmx_write64_uint32(base, value);
+		break;
+	case 8:
+		cvmx_write64_uint64(base, value);
+		break;
+	default:
+		break;
+	}
+}
+
+/* Get the global resource lock. */
+static inline void __cvmx_global_resource_lock(void)
+{
+	uint64_t lock_addr = (1ull << 63) |
+			(__cvmx_global_resources_addr + offsetof(cvmx_global_resources_t,
+							   rlock));
+	unsigned int tmp;
+
+	__asm__ __volatile__(".set noreorder            \n"
+			     "1: ll   %[tmp], 0(%[addr])\n"
+			     "   bnez %[tmp], 1b        \n"
+			     "   li   %[tmp], 1         \n"
+			     "   sc   %[tmp], 0(%[addr])\n"
+			     "   beqz %[tmp], 1b        \n"
+			     "   nop                    \n"
+			     ".set reorder              \n"
+			     : [tmp] "=&r"(tmp)
+			     : [addr] "r"(lock_addr)
+			     : "memory");
+}
+
+/* Release the global resource lock. */
+static inline void __cvmx_global_resource_unlock(void)
+{
+	uint64_t lock_addr = (1ull << 63) |
+		(__cvmx_global_resources_addr + offsetof(cvmx_global_resources_t, rlock));
+	CVMX_SYNCW;
+	__asm__ __volatile__("sw $0, 0(%[addr])\n"
+			     : : [addr] "r"(lock_addr)
+			     : "memory");
+	CVMX_SYNCW;
+}
+
+static uint64_t __cvmx_alloc_bootmem_for_global_resources(int sz)
+{
+       void *tmp;
+
+       tmp = cvmx_bootmem_alloc_range(sz, CVMX_CACHE_LINE_SIZE, 0, 0);
+       return cvmx_ptr_to_phys(tmp);
+}
+
+static inline void __cvmx_get_tagname(struct global_resource_tag *rtag,
+					char *tagname)
+{
+	int i, j, k;
+
+	j = 0;
+	k = 8;
+	for (i = 7 ; i >= 0; i--, j++, k++) {
+		tagname[j] = (rtag->lo >> (i * 8)) & 0xff;
+		tagname[k] = (rtag->hi >> (i * 8)) & 0xff;
+	}
+}
+
+static uint64_t __cvmx_global_resources_init(void)
+{
+	cvmx_bootmem_named_block_desc_t *block_desc;
+	int sz =  sizeof(cvmx_global_resources_t) + CVMX_RESOURCES_ENTRIES_SIZE;
+	int64_t tmp_phys;
+	int count = 0;
+	uint64_t base = 0;
+
+	cvmx_bootmem_lock();
+
+	block_desc = (cvmx_bootmem_named_block_desc_t *)
+		__cvmx_bootmem_find_named_block_flags(CVMX_GLOBAL_RESOURCES_DATA_NAME,
+						      CVMX_BOOTMEM_FLAG_NO_LOCKING);
+	if (!block_desc) {
+		if (dbg)
+			cvmx_dprintf("%s: allocating global resources\n", __FUNCTION__);
+
+		tmp_phys = cvmx_bootmem_phy_named_block_alloc(sz, 0, 0, CVMX_CACHE_LINE_SIZE,
+							      CVMX_GLOBAL_RESOURCES_DATA_NAME,
+							      CVMX_BOOTMEM_FLAG_NO_LOCKING);
+		if (tmp_phys < 0) {
+			cvmx_dprintf("ERROR : failed to allocate global resource name block. sz=%d \n", sz);
+			goto end;
+		}
+		__cvmx_global_resources_addr = (uint64_t) tmp_phys;
+
+		if (dbg)
+			cvmx_dprintf("%s: memset global resources %llu\n", __FUNCTION__,
+				     CAST_ULL(__cvmx_global_resources_addr));
+
+		base = (1ull << 63) | __cvmx_global_resources_addr;
+		for (count = 0; count < (sz/8); count++) {
+			cvmx_write64_uint64(base, 0);
+			base += 8;
+		}
+	} else {
+		if (dbg)
+			cvmx_dprintf("%s:found global resource\n", __FUNCTION__);
+		__cvmx_global_resources_addr = block_desc->base_addr;
+	}
+ end:
+	cvmx_bootmem_unlock();
+	if (dbg)
+		cvmx_dprintf("__cvmx_global_resources_addr=%llu sz=%d \n",
+			     CAST_ULL(__cvmx_global_resources_addr), sz);
+	return __cvmx_global_resources_addr;
+}
+
+uint64_t cvmx_get_global_resource(struct global_resource_tag tag, int no_lock)
+{
+	uint64_t entry_cnt = 0;
+	uint64_t resource_entry_addr  = 0;
+	int count = 0;
+	uint64_t rphys_addr = 0;
+	uint64_t tag_lo = 0, tag_hi = 0;
+
+	if (__cvmx_global_resources_addr == 0)
+		__cvmx_global_resources_init();
+	if (!no_lock)
+		__cvmx_global_resource_lock();
+
+	entry_cnt = CVMX_GLOBAL_RESOURCES_GET_FIELD(entry_cnt);
+	while (entry_cnt > 0) {
+		resource_entry_addr = CVMX_GET_RESOURCE_ENTRY(count);
+		tag_lo = CVMX_RESOURCE_TAG_GET_FIELD(resource_entry_addr, lo);
+		tag_hi = CVMX_RESOURCE_TAG_GET_FIELD(resource_entry_addr, hi);
+
+		if (tag_lo == tag.lo && tag_hi == tag.hi) {
+			if (dbg)
+				cvmx_dprintf("%s: Found global resource entry\n", __FUNCTION__);
+			break;
+		}
+		entry_cnt--;
+		count++;
+	}
+
+	if (entry_cnt == 0) {
+		if (dbg)
+			cvmx_dprintf("%s: no matching global resource entry found\n", __FUNCTION__);
+		if (!no_lock)
+			__cvmx_global_resource_unlock();
+		return 0;
+	}
+	rphys_addr = CVMX_RESOURCE_ENTRY_GET_FIELD(resource_entry_addr, phys_addr);
+	if (!no_lock)
+		__cvmx_global_resource_unlock();
+
+	return rphys_addr;
+}
+
+uint64_t cvmx_create_global_resource(struct global_resource_tag tag, uint64_t size,
+				     int no_lock, int *new)
+{
+	uint64_t entry_count = 0;
+	uint64_t resource_entry_addr  = 0;
+	uint64_t phys_addr;
+
+	if (__cvmx_global_resources_addr == 0)
+		__cvmx_global_resources_init();
+
+	if (!no_lock)
+		__cvmx_global_resource_lock();
+
+	phys_addr = cvmx_get_global_resource(tag, CVMX_GLOBAL_RESOURCE_NO_LOCKING);
+	if (phys_addr != 0) {
+		/* we already have the resource, return it */
+		*new = 0;
+		goto end;
+	}
+
+	*new = 1;
+	entry_count = CVMX_GLOBAL_RESOURCES_GET_FIELD(entry_cnt);
+	if (entry_count >= CVMX_MAX_GLOBAL_RESOURCES) {
+		cvmx_dprintf("ERROR: reached max global resources limit\n");
+		phys_addr = 0;
+		goto end;
+	}
+
+        /* Allocate bootmem for the resource*/
+	phys_addr = __cvmx_alloc_bootmem_for_global_resources(size);
+	if (!phys_addr) {
+		cvmx_dprintf("ERROR: unable to bootmem size=%d \n", (int) size);
+		goto end;
+	}
+
+	resource_entry_addr = CVMX_GET_RESOURCE_ENTRY(entry_count);
+	CVMX_RESOURCE_ENTRY_SET_FIELD(resource_entry_addr, phys_addr, phys_addr);
+	CVMX_RESOURCE_ENTRY_SET_FIELD(resource_entry_addr, size, size);
+	CVMX_RESOURCE_TAG_SET_FIELD(resource_entry_addr, lo, tag.lo);
+	CVMX_RESOURCE_TAG_SET_FIELD(resource_entry_addr, hi, tag.hi);
+	/* update entry_cnt */
+	entry_count += 1;
+	CVMX_GLOBAL_RESOURCES_SET_FIELD(entry_cnt, entry_count);
+
+ end:
+	if (!no_lock)
+		__cvmx_global_resource_unlock();
+
+	return phys_addr;
+}
+
+int cvmx_create_global_resource_range(struct global_resource_tag tag, int nelements)
+{
+	int sz = cvmx_range_memory_size(nelements);
+	int new;
+	uint64_t addr;
+	int rv=0;
+
+	if (__cvmx_global_resources_addr == 0)
+		__cvmx_global_resources_init();
+
+	__cvmx_global_resource_lock();
+	addr = cvmx_create_global_resource(tag, sz, 1, &new);
+	if (!addr) {
+		__cvmx_global_resource_unlock();
+		return -1;
+	}
+	if (new) {
+		rv = cvmx_range_init(addr, nelements);
+	}
+	__cvmx_global_resource_unlock();
+	return rv;
+}
+
+
+int cvmx_allocate_global_resource_range(struct global_resource_tag tag, uint64_t owner,
+					int nelements, int alignment)
+{
+	uint64_t addr = cvmx_get_global_resource(tag,1);
+	int base;
+
+	if (addr == 0) {
+		char tagname[256];
+		__cvmx_get_tagname(&tag, tagname);
+		cvmx_dprintf("ERROR: cannot find resource %s\n", tagname);
+		return -1;
+	}
+	__cvmx_global_resource_lock();
+	base = cvmx_range_alloc(addr, owner, nelements, alignment);
+	__cvmx_global_resource_unlock();
+	return base;
+}
+
+int cvmx_resource_alloc_many(struct global_resource_tag tag,
+			     uint64_t owner,
+			     int nelements,
+			     int allocated_elements[]) {
+	uint64_t addr = cvmx_get_global_resource(tag,1);
+	int rv;
+
+	if (addr == 0) {
+		char tagname[256];
+		__cvmx_get_tagname(&tag, tagname);
+		cvmx_dprintf("ERROR: cannot find resource %s\n", tagname);
+		return -1;
+	}
+	__cvmx_global_resource_lock();
+	rv = cvmx_range_alloc_non_contiguos(addr, owner, nelements, allocated_elements);
+	__cvmx_global_resource_unlock();
+	return rv;
+}
+
+int cvmx_reserve_global_resource_range(struct global_resource_tag tag,
+				       uint64_t owner, int base,
+				       int nelements)
+{
+	uint64_t addr = cvmx_get_global_resource(tag,1);
+	int start;
+
+	__cvmx_global_resource_lock();
+	start = cvmx_range_reserve(addr, owner, base, nelements);
+	__cvmx_global_resource_unlock();
+	return start;
+}
+
+int cvmx_free_global_resource_range_with_base(struct global_resource_tag tag,
+					      int base, int nelements)
+{
+	uint64_t addr = cvmx_get_global_resource(tag,1);
+	int rv;
+
+	__cvmx_global_resource_lock();
+	rv = cvmx_range_free_with_base(addr, base, nelements);
+	__cvmx_global_resource_unlock();
+	return rv;
+}
+
+int cvmx_free_global_resource_range_multiple(struct global_resource_tag tag,
+					     int bases[], int nelements)
+{
+	uint64_t addr = cvmx_get_global_resource(tag,1);
+	int rv;
+
+	__cvmx_global_resource_lock();
+	rv = cvmx_range_free_mutiple(addr, bases, nelements);
+	__cvmx_global_resource_unlock();
+	return rv;
+}
+
+int cvmx_free_global_resource_range_with_owner(struct global_resource_tag tag,
+					      int owner)
+{
+	uint64_t addr = cvmx_get_global_resource(tag,1);
+	int rv;
+
+	__cvmx_global_resource_lock();
+	rv = cvmx_range_free_with_owner(addr, owner);
+	__cvmx_global_resource_unlock();
+	return rv;
+}
+
+void cvmx_show_global_resource_range(struct global_resource_tag tag)
+{
+	uint64_t addr = cvmx_get_global_resource(tag,1);
+
+	cvmx_range_show(addr);
+}
+
+
+int free_global_resources(void)
+{
+	int rc;
+	int i, entry_cnt;
+	uint64_t resource_entry_addr, phys_addr, size;
+
+	if (__cvmx_global_resources_addr == 0)
+		__cvmx_global_resources_init();
+
+	__cvmx_global_resource_lock();
+
+	entry_cnt = CVMX_GLOBAL_RESOURCES_GET_FIELD(entry_cnt);
+
+	/* get and free all the global resources */
+	for (i = 0; i < entry_cnt; i++) {
+		resource_entry_addr = CVMX_GET_RESOURCE_ENTRY(i);
+		phys_addr = CVMX_RESOURCE_ENTRY_GET_FIELD(resource_entry_addr, phys_addr);
+		size = CVMX_RESOURCE_ENTRY_GET_FIELD(resource_entry_addr, size);
+		/* free the resource */
+		rc = __cvmx_bootmem_phy_free(phys_addr, size, 0);
+		if (!rc) {
+			cvmx_dprintf("ERROR: %s: could not free memory to bootmem\n", __FUNCTION__);
+		}
+	}
+
+	__cvmx_global_resource_unlock();
+
+	rc = cvmx_bootmem_free_named(CVMX_GLOBAL_RESOURCES_DATA_NAME);
+	if (dbg)
+		cvmx_dprintf("freed global resources named block rc=%d \n",rc);
+
+	__cvmx_global_resources_addr = 0;
+
+	return 0;
+}
+
+
+void cvmx_global_resources_show(void)
+{
+	uint64_t entry_cnt;
+	uint64_t p;
+	char tagname[MAX_RESOURCE_TAG_LEN+1];
+	struct global_resource_tag rtag;
+	uint64_t count;
+	uint64_t phys_addr;
+
+	if (__cvmx_global_resources_addr == 0)
+		__cvmx_global_resources_init();
+
+	__cvmx_global_resource_lock();
+
+	entry_cnt = CVMX_GLOBAL_RESOURCES_GET_FIELD(entry_cnt);
+	memset (tagname, 0, MAX_RESOURCE_TAG_LEN + 1);
+
+	cvmx_dprintf("%s: cvmx-global-resources: \n",__FUNCTION__);
+	for (count = 0; count < entry_cnt; count++) {
+		p = CVMX_GET_RESOURCE_ENTRY(count);
+		phys_addr = CVMX_RESOURCE_ENTRY_GET_FIELD(p, phys_addr);
+		rtag.lo = CVMX_RESOURCE_TAG_GET_FIELD(p, lo);
+		rtag.hi = CVMX_RESOURCE_TAG_GET_FIELD(p, hi);
+		__cvmx_get_tagname(&rtag, tagname);
+		cvmx_dprintf("Global Resource tag name: %s Resource Address: %llx\n",
+			     tagname, CAST_ULL(phys_addr));
+	}
+
+	__cvmx_global_resource_unlock();
+
+}
+EXPORT_SYMBOL(free_global_resources);
+
+void cvmx_resource_cleanup(struct global_resource_tag tag)
+{
+	uint64_t range_addr = cvmx_get_global_resource(tag, 1);
+	if (range_addr == 0) {
+		char tagname[256];
+		 __cvmx_get_tagname(&tag, tagname);
+		 cvmx_dprintf("ERROR: cannot find resource %s\n", tagname);
+	}
+	__cvmx_global_resource_lock();
+	cvmx_range_cleanup(range_addr);
+	 __cvmx_global_resource_unlock();
+}
+
+void octeon_ethernet_resource_cleanup(void)
+{
+	cvmx_resource_cleanup(CVMX_GR_TAG_PKO_QUEUES);
+	cvmx_resource_cleanup(CVMX_GR_TAG_PKO_IPORTS);
+	cvmx_resource_cleanup(CVMX_GR_TAG_FPA);
+}
+EXPORT_SYMBOL(octeon_ethernet_resource_cleanup);
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-bgx.c b/arch/mips/cavium-octeon/executive/cvmx-helper-bgx.c
new file mode 100644
index 0000000..de413bf
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-bgx.c
@@ -0,0 +1,1338 @@
+/***********************license start***************
+ * Copyright (c) 2014  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Functions to configure the BGX MAC.
+ *
+ * <hr>$Id$<hr>
+ */
+#include <asm/octeon/cvmx.h>
+#include <asm/octeon/cvmx-helper.h>
+#include <asm/octeon/cvmx-clock.h>
+#include <asm/octeon/cvmx-qlm.h>
+#include <asm/octeon/cvmx-helper-bgx.h>
+#include <asm/octeon/cvmx-helper-board.h>
+#include <asm/octeon/cvmx-helper-cfg.h>
+#include <asm/octeon/cvmx-bgxx-defs.h>
+#include <asm/octeon/cvmx-gserx-defs.h>
+
+int __cvmx_helper_bgx_enumerate(int xiface)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	int node = xi.node;
+	int qlm;
+	enum cvmx_qlm_mode mode;
+
+	/*
+	 * Check the QLM is configured correctly for SGMII, verify the
+	 * speed as well as the mode.
+	 */
+	qlm = cvmx_qlm_interface(xiface);
+	if (qlm == -1)
+		return 0;
+
+	mode = cvmx_qlm_get_mode_cn78xx(node, qlm);
+	if (mode == CVMX_QLM_MODE_SGMII) {
+	/* FIXME: Check here if SGMII is a MIX interface */
+		return 4;
+	} else if (mode == CVMX_QLM_MODE_XAUI
+		   || mode == CVMX_QLM_MODE_XLAUI
+		   || mode == CVMX_QLM_MODE_40G_KR4) {
+		return 1;
+	} else if (mode == CVMX_QLM_MODE_RXAUI) {
+		return 2;
+	} else if (mode == CVMX_QLM_MODE_XFI
+		   || mode == CVMX_QLM_MODE_10G_KR) {
+		return 4;
+	} else
+		return 0;
+}
+
+/**
+ * @INTERNAL
+ * Configure the bgx mac.
+ *
+ * @param interface Interface to bring up
+ *
+ * @param mode      Mode to configure the bgx mac as
+ */
+static void __cvmx_bgx_common_init(int xiface)
+{
+	cvmx_bgxx_cmrx_config_t	cmr_config;
+	cvmx_bgxx_cmr_rx_lmacs_t bgx_cmr_rx_lmacs;
+	cvmx_bgxx_cmr_tx_lmacs_t bgx_cmr_tx_lmacs;
+	cvmx_helper_interface_mode_t mode;
+	int num_ports;
+	int index;
+	int lmac_type = 0;
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	int interface = xi.interface;
+	int node = xi.node;
+	int lane_to_sds = 0;
+
+	num_ports = cvmx_helper_ports_on_interface(xiface);
+	mode = cvmx_helper_interface_get_mode(xiface);
+
+	switch (mode) {
+	case CVMX_HELPER_INTERFACE_MODE_SGMII:
+		lmac_type = 0;
+		lane_to_sds = 1;
+		break;
+	case CVMX_HELPER_INTERFACE_MODE_XAUI:
+		lmac_type = 1;
+		lane_to_sds = 0xe4;
+		break;
+	default:
+		break;
+	}
+
+	/* Set mode and lanes for all interface ports */
+	for (index = 0; index < num_ports; index++) {
+		cmr_config.u64 =
+			cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface));
+		cmr_config.s.enable = 0;
+		cmr_config.s.lmac_type = lmac_type;
+		cmr_config.s.lane_to_sds = ((lane_to_sds == 1) ? index
+					     : ((lane_to_sds == 0)
+						 ? (index ? 0xe : 4) : lane_to_sds));
+		cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
+	}
+
+	bgx_cmr_rx_lmacs.u64 = 0;
+	bgx_cmr_rx_lmacs.s.lmacs = num_ports;
+	cvmx_write_csr_node(node, CVMX_BGXX_CMR_RX_LMACS(interface), bgx_cmr_rx_lmacs.u64);
+
+	bgx_cmr_tx_lmacs.u64 = 0;
+	bgx_cmr_tx_lmacs.s.lmacs = num_ports;
+	cvmx_write_csr_node(node, CVMX_BGXX_CMR_TX_LMACS(interface), bgx_cmr_tx_lmacs.u64);
+
+}
+
+static void __cvmx_bgx_common_init_pknd(int xiface)
+{
+	int num_ports;
+	int index;
+	int num_chl = 16; /*modify it to 64 for xlaui and xaui*/
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	int interface = xi.interface;
+	int node = xi.node;
+	cvmx_bgxx_cmrx_rx_bp_on_t bgx_rx_bp_on;
+
+	num_ports = cvmx_helper_ports_on_interface(xiface);
+
+	/* Modify bp_on mark, depending on number of LMACS on that interface
+	and write it for every port */
+	bgx_rx_bp_on.u64 = 0;
+	bgx_rx_bp_on.s.mark = (CVMX_BGX_RX_FIFO_SIZE / (num_ports * 4 * 16));
+
+	for (index = 0; index < num_ports; index++) {
+		/* Setup pkind */
+		int pknd = cvmx_helper_get_pknd(xiface, index);
+		cvmx_bgxx_cmrx_rx_id_map_t cmr_rx_id_map;
+                cvmx_bgxx_cmr_chan_msk_and_t chan_msk_and;
+	        cvmx_bgxx_cmr_chan_msk_or_t chan_msk_or;
+
+		cmr_rx_id_map.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_RX_ID_MAP(index, interface));
+		cmr_rx_id_map.s.pknd = pknd;
+		cvmx_write_csr_node(node, CVMX_BGXX_CMRX_RX_ID_MAP(index, interface),
+			       cmr_rx_id_map.u64);
+                 /* Set backpressure channel mask AND/OR registers */
+                chan_msk_and.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMR_CHAN_MSK_AND(interface));
+                chan_msk_or.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMR_CHAN_MSK_OR(interface));
+                chan_msk_and.s.msk_and |= ((1 << num_chl) - 1) << (16 * index);
+                chan_msk_or.s.msk_or |= ((1 << num_chl) - 1) << (16 * index);
+                cvmx_write_csr_node(node, CVMX_BGXX_CMR_CHAN_MSK_AND(interface), chan_msk_and.u64);
+                cvmx_write_csr_node(node, CVMX_BGXX_CMR_CHAN_MSK_OR(interface), chan_msk_or.u64);
+		/* set rx back pressure (bp_on) on value */
+		cvmx_write_csr_node(node, CVMX_BGXX_CMRX_RX_BP_ON(index, interface), bgx_rx_bp_on.u64);
+	}
+}
+
+/**
+ * @INTERNAL
+ * Probe a SGMII interface and determine the number of ports
+ * connected to it. The SGMII interface should still be down after
+ * this call. This is used by interfaces using the bgx mac.
+ *
+ * @param interface Interface to probe
+ *
+ * @return Number of ports on the interface. Zero to disable.
+ */
+int __cvmx_helper_bgx_probe(int xiface)
+{
+	__cvmx_bgx_common_init(xiface);	
+	return __cvmx_helper_bgx_enumerate(xiface);
+}
+
+/**
+ * @INTERNAL
+ * Perform initialization required only once for an SGMII port.
+ *
+ * @param interface Interface to init
+ * @param index     Index of prot on the interface
+ *
+ * @return Zero on success, negative on failure
+ */
+static int __cvmx_helper_bgx_sgmii_hardware_init_one_time(int xiface, int index)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	int interface = xi.interface;
+	int node = xi.node;
+	const uint64_t clock_mhz = cvmx_clock_get_rate_node(node, CVMX_CLOCK_SCLK) / 1000000;
+	cvmx_bgxx_gmp_pcs_miscx_ctl_t gmp_misc_ctl;
+	cvmx_bgxx_gmp_pcs_linkx_timer_t gmp_timer;
+
+	if (!cvmx_helper_is_port_valid(interface, index))
+		return 0;
+
+	/*
+	 * Write PCS*_LINK*_TIMER_COUNT_REG[COUNT] with the
+	 * appropriate value. 1000BASE-X specifies a 10ms
+	 * interval. SGMII specifies a 1.6ms interval.
+	 */
+	gmp_misc_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface));
+	/* Adjust the MAC mode if requested by device tree */
+	gmp_misc_ctl.s.mac_phy =
+		cvmx_helper_get_mac_phy_mode(xiface, index);
+	gmp_misc_ctl.s.mode =
+		cvmx_helper_get_1000x_mode(xiface, index);
+	cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface), gmp_misc_ctl.u64);
+
+	gmp_timer.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_LINKX_TIMER(index, interface));
+	if (gmp_misc_ctl.s.mode)
+		/* 1000BASE-X */
+		gmp_timer.s.count = (10000ull * clock_mhz) >> 10;
+	else
+		/* SGMII */
+		gmp_timer.s.count = (1600ull * clock_mhz) >> 10;
+
+	cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_LINKX_TIMER(index, interface), gmp_timer.u64);
+
+	/*
+	 * Write the advertisement register to be used as the
+	 * tx_Config_Reg<D15:D0> of the autonegotiation.  In
+	 * 1000BASE-X mode, tx_Config_Reg<D15:D0> is PCS*_AN*_ADV_REG.
+	 * In SGMII PHY mode, tx_Config_Reg<D15:D0> is
+	 * PCS*_SGM*_AN_ADV_REG.  In SGMII MAC mode,
+	 * tx_Config_Reg<D15:D0> is the fixed value 0x4001, so this
+	 * step can be skipped.
+	 */
+	if (gmp_misc_ctl.s.mode) {
+		/* 1000BASE-X */
+		cvmx_bgxx_gmp_pcs_anx_adv_t gmp_an_adv;
+		gmp_an_adv.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_ANX_ADV(index, interface));
+		gmp_an_adv.s.rem_flt = 0;
+		gmp_an_adv.s.pause = 3;
+		gmp_an_adv.s.hfd = 1;
+		gmp_an_adv.s.fd = 1;
+		cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_ANX_ADV(index, interface), gmp_an_adv.u64);
+	} else {
+		if (gmp_misc_ctl.s.mac_phy) {
+			/* PHY Mode */
+			cvmx_bgxx_gmp_pcs_sgmx_an_adv_t gmp_sgmx_an_adv;
+			gmp_sgmx_an_adv.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_SGMX_AN_ADV(index, interface));
+			gmp_sgmx_an_adv.s.dup = 1;
+			gmp_sgmx_an_adv.s.speed = 2;
+			cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_SGMX_AN_ADV(index, interface),
+				       gmp_sgmx_an_adv.u64);
+		} else {
+			/* MAC Mode - Nothing to do */
+		}
+	}
+	return 0;
+}
+
+/**
+ * @INTERNAL
+ * Bring up the SGMII interface to be ready for packet I/O but
+ * leave I/O disabled using the GMX override. This function
+ * follows the bringup documented in 10.6.3 of the manual.
+ *
+ * @param interface Interface to bringup
+ * @param num_ports Number of ports on the interface
+ *
+ * @return Zero on success, negative on failure
+ */
+static int __cvmx_helper_bgx_sgmii_hardware_init(int xiface, int num_ports)
+{
+	int index;
+	int do_link_set = 1;
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	int interface = xi.interface;
+	int node = xi.node;
+
+	__cvmx_bgx_common_init(xiface);
+	__cvmx_bgx_common_init_pknd(xiface);
+
+	for (index = 0; index < num_ports; index++) {
+		int xipd_port = cvmx_helper_get_ipd_port(xiface, index);
+		cvmx_bgxx_gmp_gmi_txx_thresh_t gmi_tx_thresh;
+
+		if (!cvmx_helper_is_port_valid(xiface, index))
+			continue;
+		__cvmx_helper_bgx_sgmii_hardware_init_one_time(xiface, index);
+
+		/* Set TX Threshold */
+		gmi_tx_thresh.u64 = 0;
+		gmi_tx_thresh.s.cnt = 0x20;
+		cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_THRESH(index, interface),
+					gmi_tx_thresh.u64);
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+		/*
+		 * Linux kernel driver will call ....link_set with the
+		 * proper link state. In the simulator there is no
+		 * link state polling and hence it is set from
+		 * here.
+		 */
+		if (!(cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_SIM))
+			do_link_set = 0;
+#endif
+		if (do_link_set)
+			__cvmx_helper_bgx_sgmii_link_set(xipd_port,
+					__cvmx_helper_bgx_sgmii_link_get(xipd_port));
+	}
+
+	return 0;
+}
+
+/**
+ * @INTERNAL
+ * Bringup and enable a SGMII interface. After this call packet
+ * I/O should be fully functional. This is called with IPD
+ * enabled but PKO disabled. This is used by interfaces using
+ * the bgx mac.
+ *
+ * @param interface Interface to bring up
+ *
+ * @return Zero on success, negative on failure
+ */
+int __cvmx_helper_bgx_sgmii_enable(int xiface)
+{
+	cvmx_bgxx_cmrx_config_t cmr_config;
+	cvmx_bgxx_gmp_gmi_txx_append_t gmp_txx_append;
+	cvmx_bgxx_gmp_gmi_txx_sgmii_ctl_t gmp_sgmii_ctl;
+	int num_ports;
+	int index;
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	int interface = xi.interface;
+	int node = xi.node;
+
+	num_ports = cvmx_helper_ports_on_interface(xiface);
+
+	__cvmx_helper_bgx_sgmii_hardware_init(xiface, num_ports);
+
+	for (index = 0; index < num_ports; index++) {
+		gmp_txx_append.u64 = cvmx_read_csr_node(node,
+					CVMX_BGXX_GMP_GMI_TXX_APPEND(index, interface));
+#if 0
+		/* FCS/PAD options are set in cvmx_helper_bgx_tx_options() */
+		gmp_txx_append.s.fcs = 0;
+		gmp_txx_append.s.pad = 0;
+		cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_APPEND(index, interface), gmp_txx_append.u64);
+#endif
+
+		gmp_sgmii_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_SGMII_CTL(index, interface));
+		gmp_sgmii_ctl.s.align = gmp_txx_append.s.preamble ? 0 : 1;
+		cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_SGMII_CTL(index, interface),
+				gmp_sgmii_ctl.u64);
+
+		cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface));
+		cmr_config.s.enable = 1;
+		cmr_config.s.data_pkt_tx_en = 1;
+		cmr_config.s.data_pkt_rx_en = 1;
+		cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
+	}
+
+	return 0;
+}
+
+/**
+ * @INTERNAL
+ * Initialize the SERTES link for the first time or after a loss
+ * of link.
+ *
+ * @param interface Interface to init
+ * @param index     Index of prot on the interface
+ *
+ * @return Zero on success, negative on failure
+ */
+static int __cvmx_helper_bgx_sgmii_hardware_init_link(int xiface, int index)
+{
+	cvmx_bgxx_gmp_pcs_mrx_control_t gmp_control;
+	cvmx_bgxx_gmp_pcs_miscx_ctl_t gmp_misc_ctl;
+	int phy_mode, mode_1000x;
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	int interface = xi.interface;
+	int node = xi.node;
+
+	if (!cvmx_helper_is_port_valid(xiface, index))
+		return 0;
+
+	gmp_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface));
+	/* Take PCS through a reset sequence */
+	if (cvmx_sysinfo_get()->board_type != CVMX_BOARD_TYPE_SIM) {
+		gmp_control.s.reset = 1;
+		cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface),
+		       					     gmp_control.u64);
+
+		/* Wait until GMP_PCS_MRX_CONTROL[reset] comes out of reset */
+		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface),
+				cvmx_bgxx_gmp_pcs_mrx_control_t, reset, ==, 0, 10000)) {
+			cvmx_dprintf("SGMII%d: Timeout waiting for port %d to finish reset\n", interface, index);
+			return -1;
+		}
+	}
+
+	/* Write GMP_PCS_MR*_CONTROL[RST_AN]=1 to ensure a fresh SGMII
+	   negotiation starts. */
+	gmp_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface));
+	gmp_control.s.rst_an = 1;
+	gmp_control.s.an_en = 1;
+	gmp_control.s.pwr_dn = 0;
+	cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface),
+		       gmp_control.u64);
+
+
+	phy_mode = cvmx_helper_get_mac_phy_mode(xiface, index);
+	mode_1000x = cvmx_helper_get_1000x_mode(xiface, index);
+
+	gmp_misc_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface));
+	gmp_misc_ctl.s.mac_phy = phy_mode;
+	gmp_misc_ctl.s.mode = mode_1000x;
+	cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface), gmp_misc_ctl.u64);
+
+	if (phy_mode)
+		/* In PHY mode we can't query the link status so we just
+		   assume that the link is up */
+		return 0;
+
+	/* Wait for GMP_PCS_MRX_CONTROL[an_cpt] to be set, indicating that
+	   SGMII autonegotiation is complete. In MAC mode this isn't an
+	   ethernet link, but a link between OCTEON and PHY. */
+
+	if ((cvmx_sysinfo_get()->board_type != CVMX_BOARD_TYPE_SIM) &&
+	     CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_GMP_PCS_MRX_STATUS(index, interface),
+				   cvmx_bgxx_gmp_pcs_mrx_status_t, an_cpt,
+				   ==, 1, 10000)) {
+		cvmx_dprintf("SGMII%d: Port %d link timeout\n", interface, index);
+		return -1;
+	}
+
+	return 0;
+}
+
+/**
+ * @INTERNAL
+ * Configure an SGMII link to the specified speed after the SERTES
+ * link is up.
+ *
+ * @param interface Interface to init
+ * @param index     Index of prot on the interface
+ * @param link_info Link state to configure
+ *
+ * @return Zero on success, negative on failure
+ */
+static int __cvmx_helper_bgx_sgmii_hardware_init_link_speed(int xiface,
+							    int index,
+							    cvmx_helper_link_info_t link_info)
+{
+	int is_enabled;
+	cvmx_bgxx_cmrx_config_t cmr_config;
+	cvmx_bgxx_gmp_pcs_miscx_ctl_t gmp_miscx_ctl;
+	cvmx_bgxx_gmp_gmi_prtx_cfg_t gmp_prtx_cfg;
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	int interface = xi.interface;
+	int node = xi.node;
+
+	if (!cvmx_helper_is_port_valid(xiface, index))
+		return 0;
+
+	/* Disable GMX before we make any changes. Remember the enable state */
+	cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface));
+	is_enabled = cmr_config.s.enable;
+	cmr_config.s.enable = 0;
+	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
+
+	/* Wait for GMX to be idle */
+	if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_GMP_GMI_PRTX_CFG(index, interface),
+				  cvmx_bgxx_gmp_gmi_prtx_cfg_t, rx_idle, ==, 1, 10000) ||
+	    CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_GMP_GMI_PRTX_CFG(index, interface),
+				  cvmx_bgxx_gmp_gmi_prtx_cfg_t, tx_idle, ==, 1, 10000)) {
+		cvmx_dprintf("SGMII%d: Timeout waiting for port %d to be idle\n",
+			     interface, index);
+		return -1;
+	}
+
+	/* Read GMX CFG again to make sure the disable completed */
+	gmp_prtx_cfg.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_GMI_PRTX_CFG(index, interface));
+
+	/*
+	 * Get the misc control for PCS. We will need to set the
+	 * duplication amount.
+	 */
+	gmp_miscx_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface));
+
+	/*
+	 * Use GMXENO to force the link down if the status we get says
+	 * it should be down.
+	 */
+	gmp_miscx_ctl.s.gmxeno = !link_info.s.link_up;
+
+	/* Only change the duplex setting if the link is up */
+	if (link_info.s.link_up)
+		gmp_prtx_cfg.s.duplex = link_info.s.full_duplex;
+
+	/* Do speed based setting for GMX */
+	switch (link_info.s.speed) {
+	case 10:
+		gmp_prtx_cfg.s.speed = 0;
+		gmp_prtx_cfg.s.speed_msb = 1;
+		gmp_prtx_cfg.s.slottime = 0;
+		/* Setting from GMX-603 */
+		gmp_miscx_ctl.s.samp_pt = 25;
+		cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_SLOT(index, interface), 64);
+		cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_BURST(index, interface), 0);
+		break;
+	case 100:
+		gmp_prtx_cfg.s.speed = 0;
+		gmp_prtx_cfg.s.speed_msb = 0;
+		gmp_prtx_cfg.s.slottime = 0;
+		gmp_miscx_ctl.s.samp_pt = 0x5;
+		cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_SLOT(index, interface), 64);
+		cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_BURST(index, interface), 0);
+		break;
+	case 1000:
+		gmp_prtx_cfg.s.speed = 1;
+		gmp_prtx_cfg.s.speed_msb = 0;
+		gmp_prtx_cfg.s.slottime = 1;
+		gmp_miscx_ctl.s.samp_pt = 1;
+		cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_SLOT(index, interface), 512);
+		if (gmp_prtx_cfg.s.duplex)
+			/* full duplex */
+			cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_BURST(index, interface), 0);
+		else
+			/* half duplex */
+			cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_BURST(index, interface), 8192);
+		break;
+	default:
+		break;
+	}
+
+	/* Write the new misc control for PCS */
+	cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface),
+		       gmp_miscx_ctl.u64);
+
+	/* Write the new GMX settings with the port still disabled */
+	cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_PRTX_CFG(index, interface), gmp_prtx_cfg.u64);
+
+	/* Read GMX CFG again to make sure the config completed */
+	cvmx_read_csr_node(node, CVMX_BGXX_GMP_GMI_PRTX_CFG(index, interface));
+
+	/* Restore the enabled / disabled state */
+	cmr_config.s.enable = is_enabled;
+	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
+
+	return 0;
+}
+
+/**
+ * @INTERNAL
+ * Return the link state of an IPD/PKO port as returned by
+ * auto negotiation. The result of this function may not match
+ * Octeon's link config if auto negotiation has changed since
+ * the last call to cvmx_helper_link_set(). This is used by
+ * interfaces using the bgx mac.
+ *
+ * @param ipd_port IPD/PKO port to query
+ *
+ * @return Link state
+ */
+cvmx_helper_link_info_t __cvmx_helper_bgx_sgmii_link_get(int xipd_port)
+{
+	cvmx_helper_link_info_t result;
+	cvmx_bgxx_gmp_pcs_mrx_control_t gmp_control;
+	cvmx_bgxx_gmp_pcs_miscx_ctl_t gmp_misc_ctl;
+	int xiface = cvmx_helper_get_interface_num(xipd_port);
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(xipd_port);
+	int interface = xi.interface;
+	int node = xi.node;
+	int index = cvmx_helper_get_interface_index_num(xp.port);
+	int speed = 1000;
+
+	result.u64 = 0;
+
+	if (!cvmx_helper_is_port_valid(xiface, index))
+		return result;
+
+	if (cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_SIM) {
+		/* The simulator gives you a simulated 1Gbps full duplex link */
+		result.s.link_up = 1;
+		result.s.full_duplex = 1;
+		result.s.speed = speed;
+		return result;
+	}
+
+	speed = cvmx_qlm_get_gbaud_mhz(0) * 8 / 10;
+
+	gmp_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface));
+	if (gmp_control.s.loopbck1) {
+		/* Force 1Gbps full duplex link for internal loopback */
+		result.s.link_up = 1;
+		result.s.full_duplex = 1;
+		result.s.speed = speed;
+		return result;
+	}
+
+	gmp_misc_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface));
+	if (gmp_misc_ctl.s.mac_phy) {
+		/* PHY Mode */
+		/* Note that this also works for 1000base-X mode */
+
+		result.s.speed = speed;
+		result.s.full_duplex = 1;
+		result.s.link_up = 1;
+		return result;
+	} else {
+		/* MAC Mode */
+		result = __cvmx_helper_board_link_get(xipd_port);
+	}
+	return result;
+}
+
+/**
+ * @INTERNAL
+ * Configure an IPD/PKO port for the specified link state. This
+ * function does not influence auto negotiation at the PHY level.
+ * The passed link state must always match the link state returned
+ * by cvmx_helper_link_get(). It is normally best to use
+ * cvmx_helper_link_autoconf() instead. This is used by interfaces
+ * using the bgx mac.
+ *
+ * @param ipd_port  IPD/PKO port to configure
+ * @param link_info The new link state
+ *
+ * @return Zero on success, negative on failure
+ */
+int __cvmx_helper_bgx_sgmii_link_set(int xipd_port,
+				 cvmx_helper_link_info_t link_info)
+{
+	cvmx_bgxx_cmrx_config_t cmr_config;
+	int xiface = cvmx_helper_get_interface_num(xipd_port);
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(xipd_port);
+	int interface = xi.interface;
+	int node = xi.node;
+	int index = cvmx_helper_get_interface_index_num(xp.port);
+
+	if (!cvmx_helper_is_port_valid(xiface, index))
+		return 0;
+
+	cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface));
+	if (link_info.s.link_up) {
+		cmr_config.s.enable = 1;
+		cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
+
+		__cvmx_helper_bgx_sgmii_hardware_init_link(xiface, index);
+	} else {
+		cvmx_bgxx_gmp_pcs_miscx_ctl_t gmp_misc_ctl;
+
+		cmr_config.s.enable = 0;
+		cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
+		gmp_misc_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface));
+
+		/* Disable autonegotiation only when MAC mode. */
+		if (gmp_misc_ctl.s.mac_phy == 0) {
+			cvmx_bgxx_gmp_pcs_mrx_control_t gmp_control;
+
+			gmp_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface));
+			gmp_control.s.an_en = 0;
+			cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface), gmp_control.u64);
+			cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface));
+		}
+		/*
+		 * Use GMXENO to force the link down it will get
+		 * reenabled later...
+		 */
+		gmp_misc_ctl.s.gmxeno = 1;
+		cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface),
+			       gmp_misc_ctl.u64);
+		cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface));
+		return 0;
+	}
+	return __cvmx_helper_bgx_sgmii_hardware_init_link_speed(xiface, index, link_info);
+}
+
+/**
+ * @INTERNAL
+ * Configure a port for internal and/or external loopback. Internal loopback
+ * causes packets sent by the port to be received by Octeon. External loopback
+ * causes packets received from the wire to sent out again. This is used by
+ * interfaces using the bgx mac.
+ *
+ * @param ipd_port IPD/PKO port to loopback.
+ * @param enable_internal
+ *                 Non zero if you want internal loopback
+ * @param enable_external
+ *                 Non zero if you want external loopback
+ *
+ * @return Zero on success, negative on failure.
+ */
+int __cvmx_helper_bgx_sgmii_configure_loopback(int xipd_port, int enable_internal,
+					   int enable_external)
+{
+	int xiface = cvmx_helper_get_interface_num(xipd_port);
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(xipd_port);
+	int interface = xi.interface;
+	int node = xi.node;
+	int index = cvmx_helper_get_interface_index_num(xp.port);
+	cvmx_bgxx_gmp_pcs_mrx_control_t gmp_mrx_control;
+	cvmx_bgxx_gmp_pcs_miscx_ctl_t gmp_misc_ctl;
+
+	if (!cvmx_helper_is_port_valid(xiface, index))
+		return 0;
+
+	gmp_mrx_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface));
+	gmp_mrx_control.s.loopbck1 = enable_internal;
+	cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface), gmp_mrx_control.u64);
+
+	gmp_misc_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface));
+	gmp_misc_ctl.s.loopbck2 = enable_external;
+	cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface), gmp_misc_ctl.u64);
+
+	__cvmx_helper_bgx_sgmii_hardware_init_link(xiface, index);
+
+	return 0;
+}
+
+/**
+ * @INTERNAL
+ * Bringup XAUI interface. After this call packet I/O should be
+ * fully functional.
+ *
+ * @param interface Interface to bring up
+ *
+ * @return Zero on success, negative on failure
+ */
+static int __cvmx_helper_bgx_xaui_init(int index, int xiface)
+{
+	cvmx_bgxx_cmrx_config_t cmr_config;
+	cvmx_bgxx_spux_misc_control_t spu_misc_control;
+	cvmx_bgxx_spux_control1_t spu_control1;
+	cvmx_bgxx_spux_an_control_t spu_an_control;
+	cvmx_bgxx_spux_an_adv_t spu_an_adv;
+	cvmx_bgxx_spux_fec_control_t spu_fec_control;
+	cvmx_bgxx_spu_dbg_control_t spu_dbg_control;
+	cvmx_bgxx_smux_tx_append_t  smu_tx_append;
+	cvmx_bgxx_smux_tx_ctl_t smu_tx_ctl;
+	cvmx_helper_interface_mode_t mode;
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	int interface = xi.interface;
+	int node = xi.node;
+	int use_auto_neg = 0;
+	int use_training = 0;
+
+	mode = cvmx_helper_interface_get_mode(xiface);
+
+	/* NOTE: This code was moved first, out of order compared to the HRM
+	   because the RESET causes all SPU registers to loose their value */
+	/* 4. Next, bring up the SMU/SPU and the BGX reconciliation layer logic: */
+	/* 4a. Take SMU/SPU through a reset sequence. Write
+	   BGX(0..5)_SPU(0..3)_CONTROL1[RESET] = 1. Read
+	   BGX(0..5)_SPU(0..3)_CONTROL1[RESET] until it changes value to 0. Keep
+	   BGX(0..5)_SPU(0..3)_MISC_CONTROL[RX_PACKET_DIS] = 1 to disable
+	   reception. */
+	if (cvmx_sysinfo_get()->board_type != CVMX_BOARD_TYPE_SIM) {
+		spu_control1.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, interface));
+		spu_control1.s.reset = 1;
+		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, interface), spu_control1.u64);
+
+		/* Wait for PCS to come out of reset */
+		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SPUX_CONTROL1(index, interface),
+				cvmx_bgxx_spux_control1_t, reset, ==, 0, 10000)) {
+			cvmx_dprintf("BGX%d: SPU stuck in reset\n", interface);
+			return -1;
+		}
+
+		/* 1. Write BGX(0..5)_CMR(0..3)_CONFIG[ENABLE] to 0,
+		      BGX(0..5)_SPU(0..3)_CONTROL1[LO_PWR] = 1 and
+		      BGX(0..5)_SPU(0..3)_MISC_CONTROL[RX_PACKET_DIS] = 1. */
+		cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface));
+		cmr_config.s.enable = 0;
+		cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
+
+		spu_control1.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, interface));
+		spu_control1.s.lo_pwr = 1;
+		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, interface), spu_control1.u64);
+
+		spu_misc_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_MISC_CONTROL(index, interface));
+		spu_misc_control.s.rx_packet_dis = 1;
+		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_MISC_CONTROL(index, interface), spu_misc_control.u64);
+
+		/* 2. At this point, it may be appropriate to disable all BGX and SMU/SPU
+		    interrupts, as a number of them will occur during bring-up of the Link.
+		    - zero BGX(0..5)_SMU(0..3)_RX_INT
+		    - zero BGX(0..5)_SMU(0..3)_TX_INT
+		    - zero BGX(0..5)_SPU(0..3)_INT */
+		cvmx_write_csr_node(node, CVMX_BGXX_SMUX_RX_INT(index, interface),
+			cvmx_read_csr_node(node, CVMX_BGXX_SMUX_RX_INT(index, interface)));
+		cvmx_write_csr_node(node, CVMX_BGXX_SMUX_TX_INT(index, interface),
+			cvmx_read_csr_node(node, CVMX_BGXX_SMUX_TX_INT(index, interface)));
+		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_INT(index, interface),
+			cvmx_read_csr_node(node, CVMX_BGXX_SPUX_INT(index, interface)));
+
+		/* 3. Configure the BGX LMAC. */
+		/* 3a. Configure the LMAC type (40GBASE-R/10GBASE-R/RXAUI/XAUI) and
+		     SerDes selection in the BGX(0..5)_CMR(0..3)_CONFIG register, but keep
+		     the ENABLE, DATA_PKT_TX_EN and DATA_PKT_RX_EN bits clear. */
+		/* Already done in bgx_setup_one_time */
+
+		/* 3b. Write BGX(0..5)_SPU(0..3)_CONTROL1[LO_PWR] = 1 and
+		     BGX(0..5)_SPU(0..3)_MISC_CONTROL[RX_PACKET_DIS] = 1. */
+		spu_control1.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, interface));
+		spu_control1.s.lo_pwr = 1;
+		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, interface), spu_control1.u64);
+
+		spu_misc_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_MISC_CONTROL(index, interface));
+		spu_misc_control.s.rx_packet_dis = 1;
+		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_MISC_CONTROL(index, interface), spu_misc_control.u64);
+
+		/* 3c. Initialize the selected SerDes lane(s) in the QLM. See Section
+		      28.1.2.2 in the GSER chapter. */
+		/* Already done in QLM setup */
+
+		/* 3d. For 10GBASE-KR or 40GBASE-KR, enable link training by writing
+		     BGX(0..5)_SPU(0..3)_BR_PMD_CONTROL[TRAIN_EN] = 1. */
+		if (use_training) {
+			cvmx_bgxx_spux_br_pmd_control_t spu_br_pmd_control;
+			spu_br_pmd_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_CONTROL(index, interface));
+			spu_br_pmd_control.s.train_en = 1;
+			cvmx_write_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_CONTROL(index, interface), spu_br_pmd_control.u64);
+
+		}
+	}
+
+	/* 3e. Program all other relevant BGX configuration while
+	       BGX(0..5)_CMR(0..3)_CONFIG[ENABLE] = 0. This includes all things
+	       described in this chapter. */
+	/* Always add FCS to PAUSE frames */
+	smu_tx_append.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SMUX_TX_APPEND(index, interface));
+	smu_tx_append.s.fcs_d = 1;
+	cvmx_write_csr_node(node, CVMX_BGXX_SMUX_TX_APPEND(index, interface), smu_tx_append.u64);
+
+	/* 3f. If Forward Error Correction is desired for 10GBASE-R or 40GBASE-R,
+	       enable it by writing BGX(0..5)_SPU(0..3)_FEC_CONTROL[FEC_EN] = 1. */
+	if (cvmx_sysinfo_get()->board_type != CVMX_BOARD_TYPE_SIM) {
+		/* FEC is optional for 10GBASE-KR, 40GBASE-KR4, and XLAUI. We're going
+		to disable it by default */
+		spu_fec_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_CONTROL(index, interface));
+		spu_fec_control.s.fec_en = 0;
+		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_CONTROL(index, interface), spu_fec_control.u64);
+
+		/* 3g. If Auto-Negotiation is desired, configure and enable
+		      Auto-Negotiation as described in Section 33.6.2. */
+		spu_an_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_AN_CONTROL(index, interface));
+		spu_an_control.s.an_en = use_auto_neg;
+		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_AN_CONTROL(index, interface), spu_an_control.u64);
+
+		spu_fec_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_FEC_CONTROL(index, interface));
+		spu_an_adv.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_AN_ADV(index, interface));
+		spu_an_adv.s.fec_req = spu_fec_control.s.fec_en;
+		spu_an_adv.s.fec_able = 1;
+		spu_an_adv.s.a100g_cr10 = 0;
+		spu_an_adv.s.a40g_cr4 = 0;
+		spu_an_adv.s.a40g_kr4 = 0;
+		spu_an_adv.s.a10g_kr = 0;
+		spu_an_adv.s.a10g_kx4 = 0;
+		spu_an_adv.s.a1g_kx = 0;
+		spu_an_adv.s.rf = 0;
+		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_AN_ADV(index, interface), spu_an_adv.u64);
+
+		/* 3. Set BGX(0..5)_SPU_DBG_CONTROL[AN_ARB_LINK_CHK_EN] = 1. */
+		spu_dbg_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPU_DBG_CONTROL(interface));
+		spu_dbg_control.s.an_arb_link_chk_en = use_auto_neg;
+		cvmx_write_csr_node(node, CVMX_BGXX_SPU_DBG_CONTROL(interface), spu_dbg_control.u64);
+
+		/* 4. Execute the link bring-up sequence in Section 33.6.3. */
+
+		/* 5. If the auto-negotiation protocol is successful,
+		    BGX(0..5)_SPU(0..3)_AN_ADV[AN_COMPLETE] is set along with
+		    BGX(0..5)_SPU(0..3)_INT[AN_COMPLETE] when the link is up. */
+
+		/* 3h. Set BGX(0..5)_CMR(0..3)_CONFIG[ENABLE] = 1 and
+		    BGX(0..5)_SPU(0..3)_CONTROL1[LO_PWR] = 0 to enable the LMAC. */
+		cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface));
+		cmr_config.s.enable = 1;
+		cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
+
+		spu_control1.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, interface));
+		spu_control1.s.lo_pwr = 0;
+		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, interface), spu_control1.u64);
+	}
+
+	/* 4b. Set the polarity and lane swapping of the QLM SerDes. Refer to
+	   Section 33.4.1, BGX(0..5)_SPU(0..3)_MISC_CONTROL[XOR_TXPLRT,XOR_RXPLRT]
+	   and BGX(0..5)_SPU(0..3)_MISC_CONTROL[TXPLRT,RXPLRT]. */
+
+	/* 4c. Write BGX(0..5)_SPU(0..3)_CONTROL1[LO_PWR] = 0. */
+	spu_control1.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, interface));
+	spu_control1.s.lo_pwr = 0;
+	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, interface), spu_control1.u64);
+
+	/* 4d. Select Deficit Idle Count mode and unidirectional enable/disable
+	   via BGX(0..5)_SMU(0..3)_TX_CTL[DIC_EN,UNI_EN]. */
+	smu_tx_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SMUX_TX_CTL(index, interface));
+	smu_tx_ctl.s.dic_en = 1;
+	smu_tx_ctl.s.uni_en = 0;
+	cvmx_write_csr_node(node, CVMX_BGXX_SMUX_TX_CTL(index, interface), smu_tx_ctl.u64);
+
+	return 0;
+}
+
+static int __cvmx_helper_bgx_xaui_link_init(int index, int xiface)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	int interface = xi.interface;
+	int node = xi.node;
+	cvmx_bgxx_spux_status1_t spu_status1;
+	cvmx_bgxx_spux_status2_t spu_status2;
+	cvmx_bgxx_spux_int_t spu_int;
+	cvmx_bgxx_spux_misc_control_t spu_misc_control;
+	cvmx_helper_interface_mode_t mode;
+	int use_training = 0;
+
+	mode = cvmx_helper_interface_get_mode(xiface);
+
+	/* Disable packet reception */
+	spu_misc_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_MISC_CONTROL(index, interface));
+	spu_misc_control.s.rx_packet_dis = 1;
+	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_MISC_CONTROL(index, interface), spu_misc_control.u64);
+
+	if (cvmx_sysinfo_get()->board_type != CVMX_BOARD_TYPE_SIM) {
+		if (use_training) {
+			spu_int.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_INT(index, interface));
+			if (!spu_int.s.training_done) {
+				cvmx_bgxx_spux_br_pmd_control_t pmd_control;
+				/* Clear the training interrupts (W1C) */
+
+				spu_int.u64 = 0;
+				spu_int.s.training_failure = 1;
+				spu_int.s.training_done = 1;
+				cvmx_write_csr_node(node, CVMX_BGXX_SPUX_INT(index, interface), spu_int.u64);
+
+				/* Restart training */
+				pmd_control.u64 = cvmx_read_csr_node(node,
+							CVMX_BGXX_SPUX_BR_PMD_CONTROL(index, interface));
+				pmd_control.s.train_restart = 1;
+				cvmx_write_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_CONTROL(index, interface), pmd_control.u64);
+
+				/*cvmx_dprintf("Restarting link training\n"); */
+				return -1;
+			}
+		}
+
+		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SPUX_CONTROL1(index, interface),
+					  cvmx_bgxx_spux_control1_t, reset, ==, 0, 10000)) {
+			cvmx_dprintf("ERROR: %d:BGX%d:%d: PCS in reset", node, interface, index);
+			return -1;
+		}
+
+			/* (5) Check to make sure that the link appears up and stable. */
+			/* Wait for PCS to be aligned */
+			if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SPUX_BX_STATUS(index, interface),
+				  cvmx_bgxx_spux_bx_status_t, alignd, ==, 1, 10000)) {
+				cvmx_dprintf("ERROR: %d:BGX%d:%d: PCS not aligned\n", node, interface, index);
+				return -1;
+			}
+
+		/* Clear rcvflt bit (latching high) and read it back */
+		spu_status2.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_STATUS2(index, interface));
+		spu_status2.s.rcvflt = 1;
+		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_STATUS2(index, interface), spu_status2.u64);
+
+		spu_status2.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_STATUS2(index, interface));
+		if (spu_status2.s.rcvflt) {
+			cvmx_dprintf("ERROR: %d:BGX%d:%d: Receive fault, need to retry\n",
+					node, interface, index);
+
+			if (use_training) {
+				cvmx_bgxx_spux_br_pmd_control_t pmd_control;
+				spu_int.u64 = 0;
+				spu_int.s.training_failure = 1;
+				spu_int.s.training_done = 1;
+				cvmx_write_csr_node(node, CVMX_BGXX_SPUX_INT(index, interface), spu_int.u64);
+
+				/* Restart training */
+				pmd_control.u64 = cvmx_read_csr_node(node,
+							CVMX_BGXX_SPUX_BR_PMD_CONTROL(index, interface));
+				pmd_control.s.train_restart = 1;
+				cvmx_write_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_CONTROL(index, interface), pmd_control.u64);
+			}
+			/*cvmx_dprintf("training restarting\n"); */
+			return -1;
+		}
+
+		/* Wait for MAC RX to be ready */
+		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SMUX_RX_CTL(index, interface),
+					  cvmx_bgxx_smux_rx_ctl_t, status, ==, 0, 10000)) {
+			cvmx_dprintf("ERROR: %d:BGX%d:%d: RX not ready\n", node, interface, index);
+			return -1;
+		}
+
+		/* Wait for BGX RX to be idle */
+		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SMUX_CTRL(index, interface),
+				  cvmx_bgxx_smux_ctrl_t, rx_idle, ==, 1, 10000)) {
+			cvmx_dprintf("ERROR: %d:BGX%d:%d: RX not idle\n", node, interface, index);
+			return -1;
+		}
+
+		/* Wait for GMX TX to be idle */
+		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SMUX_CTRL(index, interface),
+				  cvmx_bgxx_smux_ctrl_t, tx_idle, ==, 1, 10000)) {
+			cvmx_dprintf("ERROR: %d:BGX%d:%d: TX not idle\n", node, interface, index);
+			return -1;
+		}
+
+		/* rcvflt should still be 0 */
+		spu_status2.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_STATUS2(index, interface));
+		if (spu_status2.s.rcvflt) {
+			cvmx_dprintf("ERROR: %d:BGX%d:%d: Receive fault, need to retry\n", node, interface, index);
+			return -1;
+		}
+
+		/* Receive link is latching low. Force it high and verify it */
+		spu_status1.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_STATUS1(index, interface));
+		spu_status1.s.rcv_lnk = 1;
+		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_STATUS1(index, interface), spu_status1.u64);
+
+		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SPUX_STATUS1(index, interface),
+				cvmx_bgxx_spux_status1_t, rcv_lnk, ==, 1, 10000)) {
+			cvmx_dprintf("ERROR: %d:BGX%d:%d: Receive link down\n", node, interface, index);
+			return -1;
+		}
+	}
+
+	/* (7) Enable packet transmit and receive */
+	spu_misc_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_MISC_CONTROL(index, interface));
+	spu_misc_control.s.rx_packet_dis = 0;
+	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_MISC_CONTROL(index, interface), spu_misc_control.u64);
+
+	return 0;
+}
+
+int __cvmx_helper_bgx_xaui_enable(int xiface)
+{
+	cvmx_bgxx_smux_tx_thresh_t smu_tx_thresh;
+	cvmx_bgxx_cmrx_config_t cmr_config;
+	int index;
+	int num_ports = cvmx_helper_ports_on_interface(xiface);
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	int interface = xi.interface;
+	int node = xi.node;
+
+	__cvmx_bgx_common_init(xiface);
+	__cvmx_bgx_common_init_pknd(xiface);
+	for (index = 0; index < num_ports; index++) {
+		int res = __cvmx_helper_bgx_xaui_init(index, xiface);
+		if (res == -1) {
+			cvmx_dprintf("Failed to enable XAUI for BGX(%d,%d)\n", interface, index);
+			return res;
+		}
+
+		smu_tx_thresh.u64 = 0;
+		/* Hopefully big enough to avoid underrun, but not too
+		 * big to adversly effect shaping.
+		 */
+		smu_tx_thresh.s.cnt = 0x100;
+		cvmx_write_csr_node(node, CVMX_BGXX_SMUX_TX_THRESH(index, interface),
+					smu_tx_thresh.u64);
+
+		cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface));
+		cmr_config.s.enable = 1;
+		cmr_config.s.data_pkt_tx_en = 1;
+		cmr_config.s.data_pkt_rx_en = 1;
+		cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
+	}
+	return 0;
+}
+
+cvmx_helper_link_info_t __cvmx_helper_bgx_xaui_link_get(int xipd_port)
+{
+	int xiface = cvmx_helper_get_interface_num(xipd_port);
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(xipd_port);
+	int interface = xi.interface;
+	int node = xi.node;
+	int index = cvmx_helper_get_interface_index_num(xp.port);
+	cvmx_bgxx_spux_status1_t spu_status1;
+	cvmx_helper_link_info_t result;
+
+	result.u64 = 0;
+
+	spu_status1.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_STATUS1(index, interface));
+
+	if (spu_status1.s.rcv_lnk) {
+		int lanes;
+		int qlm = cvmx_qlm_interface(xiface);
+		uint64_t speed;
+		cvmx_helper_interface_mode_t mode;
+		result.s.link_up = 1;
+		result.s.full_duplex = 1;
+		speed = cvmx_qlm_get_gbaud_mhz(qlm);
+		mode = cvmx_helper_interface_get_mode(xiface);
+		lanes = 4 / cvmx_helper_ports_on_interface(xiface);
+
+		switch(mode) {
+		default:
+			/* Using 8b10b symbol encoding */
+			speed = (speed * 8 + 5) / 10;
+			break;
+		}
+		speed *= lanes;
+		result.s.speed = speed;
+	} else {
+		int res;
+		res = __cvmx_helper_bgx_xaui_link_init(index, xiface);
+		if (res == -1) {
+			/*cvmx_dprintf("Failed to get BGX(%d,%d) link\n", interface, index); */
+			return result;
+		}
+	}
+
+	return result;
+}
+
+int __cvmx_helper_bgx_xaui_link_set(int xipd_port, cvmx_helper_link_info_t link_info)
+{
+	int xiface = cvmx_helper_get_interface_num(xipd_port);
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(xipd_port);
+	int interface = xi.interface;
+	int node = xi.node;
+	int index = cvmx_helper_get_interface_index_num(xp.port);
+	cvmx_bgxx_smux_tx_ctl_t smu_tx_ctl;
+	cvmx_bgxx_smux_rx_ctl_t smu_rx_ctl;
+	cvmx_bgxx_spux_status1_t spu_status1;
+
+	smu_tx_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SMUX_TX_CTL(index, interface));
+	smu_rx_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SMUX_RX_CTL(index, interface));
+	spu_status1.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_STATUS1(index, interface));
+
+	/* If the link shouldn't be up, then just return */
+	if (!link_info.s.link_up)
+		return 0;
+
+	/* Do nothing if both RX and TX are happy */
+	if ((smu_tx_ctl.s.ls == 0) && (smu_rx_ctl.s.status == 0) && spu_status1.s.rcv_lnk)
+		return 0;
+
+	/* Bring the link up */
+	return __cvmx_helper_bgx_xaui_link_init(index, xiface);
+}
+
+int __cvmx_helper_bgx_xaui_configure_loopback(int xipd_port,
+						     int enable_internal,
+						     int enable_external)
+{
+	int xiface = cvmx_helper_get_interface_num(xipd_port);
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(xipd_port);
+	int interface = xi.interface;
+	int node = xi.node;
+	int index = cvmx_helper_get_interface_index_num(xp.port);
+	cvmx_bgxx_spux_control1_t spu_control1;
+	cvmx_bgxx_smux_ext_loopback_t smu_ext_loopback;
+
+	/* Set the internal loop */
+	spu_control1.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, interface));
+	spu_control1.s.loopbck = enable_internal;
+	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, interface), spu_control1.u64);
+	/* Set the external loop */
+	smu_ext_loopback.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SMUX_EXT_LOOPBACK(index, interface));
+	smu_ext_loopback.s.en = enable_external;
+	cvmx_write_csr_node(node, CVMX_BGXX_SMUX_EXT_LOOPBACK(index, interface), smu_ext_loopback.u64);
+
+	return __cvmx_helper_bgx_xaui_link_init(index, xiface);
+}
+
+/**
+ * @INTERNAL
+ * Configure Priority-Based Flow Control (a.k.a. PFC/CBFC)
+ * on a specific BGX interface/port.
+ */
+void __cvmx_helper_bgx_xaui_config_pfc(unsigned node,
+		unsigned interface, unsigned port, bool pfc_enable)
+{
+	cvmx_bgxx_smux_cbfc_ctl_t cbfc_ctl;
+
+	cbfc_ctl.u64 = cvmx_read_csr_node(node,
+		CVMX_BGXX_SMUX_CBFC_CTL(port, interface)
+		);
+
+	/* Enable all PFC controls if requiested */
+	cbfc_ctl.s.rx_en = pfc_enable;
+	cbfc_ctl.s.tx_en = pfc_enable;
+#if 0
+	cbfc_ctl.s.bck_en = 1;
+	cbfc_ctl.s.phys_en = 0xff;
+	cbfc_ctl.s.logl_en = 0xff;
+	cbfc_ctl.s.drp_en = pfc_enable;
+#endif
+#ifdef DEBUG
+	printf("%s: CVMX_BGXX_SMUX_CBFC_CTL(%d,%d)=%#llx\n",
+		__func__, port, interface, (unsigned long long)cbfc_ctl.u64);
+#endif
+	cvmx_write_csr_node(node,
+		CVMX_BGXX_SMUX_CBFC_CTL(port, interface),
+		cbfc_ctl.u64);
+}
+
+
+/**
+ * This function control how the hardware handles incoming PAUSE
+ * packets. The most common modes of operation:
+ * ctl_bck = 1, ctl_drp = 1: hardware handles everything
+ * ctl_bck = 0, ctl_drp = 0: software sees all PAUSE frames
+ * ctl_bck = 0, ctl_drp = 1: all PAUSE frames are completely ignored
+ * @param node		node number.
+ * @param interface	interface number
+ * @param port		port number
+ * @param ctl_bck	1: Forward PAUSE information to TX block
+ * @param ctl_drp	1: Drop control PAUSE frames.
+ */
+void cvmx_helper_bgx_rx_pause_ctl(unsigned node, unsigned interface,
+			unsigned port, unsigned ctl_bck, unsigned ctl_drp)
+{
+	cvmx_bgxx_smux_rx_frm_ctl_t frm_ctl;
+
+	frm_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SMUX_RX_CTL(port, interface));
+	frm_ctl.s.ctl_bck = ctl_bck;
+	frm_ctl.s.ctl_drp = ctl_drp;
+	cvmx_write_csr_node(node, CVMX_BGXX_SMUX_RX_CTL(port, interface), frm_ctl.u64);
+}
+
+/**
+ * This function configures the receive action taken for multicast, broadcast
+ * and dmac filter match packets.
+ * @param node		node number.
+ * @param interface	interface number
+ * @param port		port number
+ * @param cam_accept	0: reject packets on dmac filter match
+ *                      1: accept packet on dmac filter match
+ * @param mcast_mode	0x0 = Force reject all multicast packets
+ *                      0x1 = Force accept all multicast packets
+ *                      0x2 = Use the address filter CAM
+ * @param bcast_accept  0 = Reject all broadcast packets
+ *                      1 = Accept all broadcast packets
+ */
+void cvmx_helper_bgx_rx_adr_ctl(unsigned node, unsigned interface, unsigned port,
+                                 unsigned cam_accept, unsigned mcast_mode, unsigned bcast_accept)
+{
+        cvmx_bgxx_cmrx_rx_adr_ctl_t adr_ctl;
+
+        adr_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_RX_ADR_CTL(port, interface));
+        adr_ctl.s.cam_accept = cam_accept;
+        adr_ctl.s.mcst_mode = mcast_mode;
+        adr_ctl.s.bcst_accept = bcast_accept;
+
+        cvmx_write_csr_node(node, CVMX_BGXX_CMRX_RX_ADR_CTL(port, interface), adr_ctl.u64);
+}
+
+/**
+ * Function to control the generation of FCS, padding by the BGX
+ *
+ */
+void cvmx_helper_bgx_tx_options(unsigned node,
+	unsigned interface, unsigned index,
+	bool fcs_enable, bool pad_enable)
+{
+	cvmx_bgxx_cmrx_config_t cmr_config;
+	cvmx_bgxx_cmrx_config_t cmr_config0;
+	cvmx_bgxx_gmp_gmi_txx_append_t gmp_txx_append;
+	cvmx_bgxx_gmp_gmi_txx_min_pkt_t gmp_min_pkt;
+	cvmx_bgxx_smux_tx_min_pkt_t smu_min_pkt;
+	cvmx_bgxx_smux_tx_append_t  smu_tx_append;
+
+	cmr_config0.u64 = cvmx_read_csr_node(node,
+		CVMX_BGXX_CMRX_CONFIG(0, interface));
+	cmr_config.u64 = cvmx_read_csr_node(node,
+		CVMX_BGXX_CMRX_CONFIG(index, interface));
+
+	/* Temp: initial mode setting is only applied to LMAC(0), use that */
+	if (cmr_config0.s.lmac_type != cmr_config.s.lmac_type) {
+		cvmx_dprintf("WARNING: %s: "
+			"BGX(%d).CMR(0).LMAC_TYPE != BGX(%d).CMR(%d).LMAC_TYPE\n",
+			__func__, interface, interface, index);
+		cmr_config.s.lmac_type = cmr_config0.s.lmac_type;
+	}
+
+	if (cmr_config.s.lmac_type == 0) {
+		gmp_min_pkt.u64 = 0;
+		/* per HRM Sec 34.3.4.4 */
+		gmp_min_pkt.s.min_size = 59;
+		cvmx_write_csr_node(node,
+                        CVMX_BGXX_GMP_GMI_TXX_MIN_PKT(index, interface),
+			gmp_min_pkt.u64);
+		gmp_txx_append.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_GMP_GMI_TXX_APPEND(index, interface));
+		gmp_txx_append.s.fcs = fcs_enable;
+		gmp_txx_append.s.pad = pad_enable;
+		cvmx_write_csr_node(node,
+			CVMX_BGXX_GMP_GMI_TXX_APPEND(index, interface),
+			gmp_txx_append.u64);
+	} else {
+		smu_min_pkt.u64 = 0;
+		/* HRM Sec 33.3.4.3 should read 64 */
+		 smu_min_pkt.s.min_size = 0x40;
+		cvmx_write_csr_node(node,
+                        CVMX_BGXX_SMUX_TX_MIN_PKT(index, interface),
+			smu_min_pkt.u64);
+		smu_tx_append.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_SMUX_TX_APPEND(index, interface));
+		smu_tx_append.s.fcs_c = fcs_enable;
+		smu_tx_append.s.pad = pad_enable;
+		cvmx_write_csr_node(node,
+			CVMX_BGXX_SMUX_TX_APPEND(index, interface),
+			smu_tx_append.u64);
+	}
+}
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c b/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c
new file mode 100644
index 0000000..7c1b629
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c
@@ -0,0 +1,1049 @@
+/***********************license start***************
+ * Copyright (c) 2003-2013  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Helper Functions for the Configuration Framework
+ *
+ * <hr>$Revision: 0 $<hr>
+ */
+#include <linux/export.h>
+#include <asm/octeon/cvmx.h>
+#include <asm/octeon/cvmx-helper.h>
+#include <asm/octeon/cvmx-helper-board.h>
+#include <asm/octeon/cvmx-helper-util.h>
+#include <asm/octeon/cvmx-helper-cfg.h>
+#include <asm/octeon/cvmx-helper-ilk.h>
+#include <asm/octeon/cvmx-ilk.h>
+#include <asm/octeon/cvmx-range.h>
+#include <asm/octeon/cvmx-global-resources.h>
+#include <asm/octeon/cvmx-pko-internal-ports-range.h>
+
+#if !defined(min)
+# define min( a, b ) ( ( a ) < ( b ) ) ? ( a ) : ( b )
+#endif
+
+CVMX_SHARED struct cvmx_cfg_port_param cvmx_cfg_port[CVMX_MAX_NODES][CVMX_HELPER_MAX_IFACE][CVMX_HELPER_CFG_MAX_PORT_PER_IFACE] =
+	{[0 ... CVMX_MAX_NODES - 1][0 ... CVMX_HELPER_MAX_IFACE - 1] =
+		{[0 ... CVMX_HELPER_CFG_MAX_PORT_PER_IFACE - 1] =
+			{
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+				CVMX_HELPER_CFG_INVALID_VALUE,
+				CVMX_HELPER_CFG_INVALID_VALUE,
+#endif
+				CVMX_HELPER_CFG_INVALID_VALUE,
+				CVMX_HELPER_CFG_INVALID_VALUE,
+				CVMX_HELPER_CFG_INVALID_VALUE,
+				CVMX_HELPER_CFG_INVALID_VALUE,
+				CVMX_HELPER_CFG_INVALID_VALUE,
+				0, 0, 0, 0, false
+			}
+		}
+	};
+
+/*
+ * Indexed by the pko_port number
+ */
+static CVMX_SHARED int __cvmx_cfg_pko_highest_queue;
+CVMX_SHARED struct cvmx_cfg_pko_port_param cvmx_pko_queue_table [CVMX_HELPER_CFG_MAX_PKO_PORT] =
+{[0 ... CVMX_HELPER_CFG_MAX_PKO_PORT - 1] = {CVMX_HELPER_CFG_INVALID_VALUE,CVMX_HELPER_CFG_INVALID_VALUE}};
+
+CVMX_SHARED cvmx_user_static_pko_queue_config_t __cvmx_pko_queue_static_config;
+
+CVMX_SHARED struct cvmx_cfg_pko_port_map cvmx_cfg_pko_port_map[CVMX_HELPER_CFG_MAX_PKO_PORT] =
+	{[0 ... CVMX_HELPER_CFG_MAX_PKO_PORT - 1] = {CVMX_HELPER_CFG_INVALID_VALUE,CVMX_HELPER_CFG_INVALID_VALUE,
+						     CVMX_HELPER_CFG_INVALID_VALUE}};
+
+/*
+ * This array assists translation from ipd_port to pko_port.
+ * The ``16'' is the rounded value for the 3rd 4-bit value of
+ * ipd_port, used to differentiate ``interfaces.''
+ */
+static CVMX_SHARED struct cvmx_cfg_pko_port_pair ipd2pko_port_cache[16]
+    [CVMX_HELPER_CFG_MAX_PORT_PER_IFACE] =
+	{[0 ... 15] = {[0 ... CVMX_HELPER_CFG_MAX_PORT_PER_IFACE - 1] =
+		       { CVMX_HELPER_CFG_INVALID_VALUE,	CVMX_HELPER_CFG_INVALID_VALUE}}};
+
+
+#ifdef CVMX_USER_DEFINED_HELPER_CONFIG_INIT
+
+static CVMX_SHARED int cvmx_cfg_default_pko_nqueues = 1;
+
+/*
+ * A pool for holding the pko_nqueues for the pko_ports assigned to a
+ * physical port.
+ */
+static CVMX_SHARED uint8_t cvmx_cfg_pko_nqueue_pool[CVMX_HELPER_CFG_MAX_PKO_QUEUES] =
+	{[0 ... CVMX_HELPER_CFG_MAX_PKO_QUEUES - 1] = 1 };
+
+#endif
+
+/*
+ * Options
+ *
+ * Each array-elem's intial value is also the option's default value.
+ */
+static CVMX_SHARED uint64_t cvmx_cfg_opts[CVMX_HELPER_CFG_OPT_MAX] = {[0 ... CVMX_HELPER_CFG_OPT_MAX - 1] = 1 };
+
+/*
+ * MISC
+ */
+static CVMX_SHARED int cvmx_cfg_max_pko_engines;	/* # of PKO DMA engines
+							   allocated */
+int __cvmx_helper_cfg_pknd(int xiface, int index)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	return cvmx_cfg_port[xi.node][xi.interface][index].ccpp_pknd;
+}
+
+int __cvmx_helper_cfg_bpid(int xiface, int index)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	return cvmx_cfg_port[xi.node][xi.interface][index].ccpp_bpid;
+}
+
+int __cvmx_helper_cfg_pko_port_base(int xiface, int index)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	return cvmx_cfg_port[xi.node][xi.interface][index].ccpp_pko_port_base;
+}
+
+int __cvmx_helper_cfg_pko_port_num(int xiface, int index)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	return cvmx_cfg_port[xi.node][xi.interface][index].ccpp_pko_num_ports;
+}
+
+int __cvmx_helper_cfg_pko_queue_num(int pko_port)
+{
+	return cvmx_pko_queue_table[pko_port].ccppp_num_queues;
+}
+
+int __cvmx_helper_cfg_pko_queue_base(int pko_port)
+{
+	return cvmx_pko_queue_table[pko_port].ccppp_queue_base;
+}
+
+int __cvmx_helper_cfg_pko_max_queue(void)
+{
+	return __cvmx_cfg_pko_highest_queue;
+}
+
+int __cvmx_helper_cfg_pko_max_engine(void)
+{
+	return cvmx_cfg_max_pko_engines;
+}
+
+int cvmx_helper_cfg_opt_set(cvmx_helper_cfg_option_t opt, uint64_t val)
+{
+	if (opt >= CVMX_HELPER_CFG_OPT_MAX)
+		return -1;
+
+	cvmx_cfg_opts[opt] = val;
+
+	return 0;
+}
+
+uint64_t cvmx_helper_cfg_opt_get(cvmx_helper_cfg_option_t opt)
+{
+	if (opt >= CVMX_HELPER_CFG_OPT_MAX)
+		return (uint64_t) CVMX_HELPER_CFG_INVALID_VALUE;
+
+	return cvmx_cfg_opts[opt];
+}
+
+/*
+ * initialize the queue allocation list. the existing static allocation result
+ * is used as a starting point to ensure backward compatibility.
+ *
+ * @return  0 on success
+ *         -1 on failure
+ */
+int cvmx_pko_queue_grp_alloc(uint64_t start, uint64_t end, uint64_t count)
+{
+	uint64_t port;
+	int ret_val;
+
+	for (port = start; port < end; port++) {
+		ret_val = cvmx_pko_queue_alloc(port, count);
+		if (ret_val == -1)
+		{
+			cvmx_dprintf("ERROR: Failed to allocate queue for port=%d count=%d \n", (int) port,
+				     (int) count);
+			return ret_val;
+		}
+	}
+	return 0;
+}
+
+int cvmx_pko_queue_init_from_cvmx_config_non_pknd(void)
+{
+	int ret_val = -1;
+	uint64_t count, start, end;
+
+	start = 0;
+	end   = __cvmx_pko_queue_static_config.non_pknd.pko_ports_per_interface[0];
+	count = __cvmx_pko_queue_static_config.non_pknd. pko_queues_per_port_interface[0];
+	cvmx_pko_queue_grp_alloc(start,end,count);
+
+	start = 16;
+	end = start + __cvmx_pko_queue_static_config.non_pknd.pko_ports_per_interface[1];
+	count = __cvmx_pko_queue_static_config.non_pknd.pko_queues_per_port_interface[1];
+	ret_val = cvmx_pko_queue_grp_alloc(start,end,count);
+	if (ret_val != 0)
+		return -1;
+
+	start = end; end = 36 ; count = __cvmx_pko_queue_static_config.non_pknd.pko_queues_per_port_pci;
+	cvmx_pko_queue_grp_alloc(start,end,count);
+	if (ret_val != 0)
+		return -1;
+
+	start = end; end = 40; count = __cvmx_pko_queue_static_config.non_pknd.pko_queues_per_port_loop;
+	cvmx_pko_queue_grp_alloc(start,end,count);
+	if (ret_val != 0)
+		return -1;
+
+	start = end; end = 42; count = __cvmx_pko_queue_static_config.non_pknd.pko_queues_per_port_srio[0];
+	cvmx_pko_queue_grp_alloc(start, end, count);
+	if (ret_val != 0)
+		return -1;
+
+	start = end; end = 44; count = __cvmx_pko_queue_static_config.non_pknd.pko_queues_per_port_srio[1];
+	cvmx_pko_queue_grp_alloc(start, end, count);
+	if (ret_val != 0)
+		return -1;
+
+	start = end; end = 46; count = __cvmx_pko_queue_static_config.non_pknd.pko_queues_per_port_srio[2];
+	cvmx_pko_queue_grp_alloc(start,end, count);
+	if (ret_val != 0)
+		return -1;
+
+	start = end; end = 48; count = __cvmx_pko_queue_static_config.non_pknd.pko_queues_per_port_srio[3];
+	cvmx_pko_queue_grp_alloc(start, end, count);
+	if (ret_val != 0)
+		return -1;
+	return 0;
+}
+
+static int CVMX_SHARED queue_range_init = 0;
+
+int init_cvmx_pko_que_range(void)
+{
+	int rv = 0;
+
+	if (queue_range_init)
+		return 0;
+	queue_range_init = 1;
+	rv = cvmx_create_global_resource_range(CVMX_GR_TAG_PKO_QUEUES, CVMX_HELPER_CFG_MAX_PKO_QUEUES);
+	if (rv!=0) {
+		cvmx_dprintf("ERROR : Failed to initalize pko queues range\n");
+	}
+	return rv;
+}
+
+
+/*
+ * get a block of "count" queues for "port"
+ *
+ * @param  port   the port for which the queues are requested
+ * @param  count  the number of queues requested
+ *
+ * @return  0 on success
+ *         -1 on failure
+ */
+int cvmx_pko_queue_alloc(uint64_t port, uint64_t count)
+{
+    int ret_val = -1;
+    int highest_queue;
+
+    init_cvmx_pko_que_range();
+    if (port >= CVMX_HELPER_CFG_MAX_PKO_QUEUES) {
+	    cvmx_dprintf("ERROR: %s port=%d > %d", __FUNCTION__, (int) port, CVMX_HELPER_CFG_MAX_PKO_QUEUES );
+	    return -1;
+    }
+    ret_val = cvmx_allocate_global_resource_range(CVMX_GR_TAG_PKO_QUEUES, port, count, 1);
+    //cvmx_dprintf("allocated pko que : port=%02d base=%02d count=%02d \n", (int) port, ret_val, (int) count);
+    if (ret_val == -1)
+        return ret_val;
+    cvmx_pko_queue_table[port].ccppp_queue_base = ret_val;
+    cvmx_pko_queue_table[port].ccppp_num_queues = count;
+
+    highest_queue = ret_val + count - 1;
+    if (highest_queue > __cvmx_cfg_pko_highest_queue)
+	    __cvmx_cfg_pko_highest_queue = highest_queue;
+    return 0;
+}
+
+/*
+ * return the queues for "port"
+ *
+ * @param  port   the port for which the queues are returned
+ *
+ * @return  0 on success
+ *         -1 on failure
+ */
+int cvmx_pko_queue_free(uint64_t port)
+{
+    int ret_val = -1;
+
+    init_cvmx_pko_que_range();
+    if (port >= CVMX_HELPER_CFG_MAX_PKO_QUEUES) {
+	    cvmx_dprintf("ERROR: %s port=%d > %d", __FUNCTION__,
+			 (int) port, CVMX_HELPER_CFG_MAX_PKO_QUEUES);
+	    return -1;
+    }
+
+    ret_val = cvmx_free_global_resource_range_with_base(CVMX_GR_TAG_PKO_QUEUES,
+							cvmx_pko_queue_table[port].ccppp_queue_base,
+							cvmx_pko_queue_table[port].ccppp_num_queues);
+    if (ret_val != 0)
+        return ret_val;
+
+    cvmx_pko_queue_table[port].ccppp_num_queues = 0;
+    cvmx_pko_queue_table[port].ccppp_queue_base = CVMX_HELPER_CFG_INVALID_VALUE;
+    ret_val = 0;
+    return ret_val;
+}
+
+void cvmx_pko_queue_free_all(void)
+{
+	int i;
+
+	for(i=0; i< CVMX_HELPER_CFG_MAX_PKO_PORT; i++)
+		if (cvmx_pko_queue_table[i].ccppp_queue_base != CVMX_HELPER_CFG_INVALID_VALUE)
+			cvmx_pko_queue_free(i);
+	//cvmx_range_show(pko_queue_range);
+}
+
+void cvmx_pko_queue_show()
+{
+	int i;
+
+	cvmx_show_global_resource_range(CVMX_GR_TAG_PKO_QUEUES);
+	for(i=0; i< CVMX_HELPER_CFG_MAX_PKO_PORT; i++)
+		if (cvmx_pko_queue_table[i].ccppp_queue_base != CVMX_HELPER_CFG_INVALID_VALUE)
+			cvmx_dprintf("port=%d que_base=%d que_num=%d \n", i,
+				     (int) cvmx_pko_queue_table[i].ccppp_queue_base,
+				     (int) cvmx_pko_queue_table[i].ccppp_num_queues);
+}
+
+EXPORT_SYMBOL(__cvmx_helper_cfg_pknd);
+EXPORT_SYMBOL(__cvmx_helper_cfg_bpid);
+EXPORT_SYMBOL(__cvmx_helper_cfg_pko_port_base);
+EXPORT_SYMBOL(__cvmx_helper_cfg_pko_port_num);
+EXPORT_SYMBOL(__cvmx_helper_cfg_pko_queue_base);
+EXPORT_SYMBOL(__cvmx_helper_cfg_pko_queue_num);
+EXPORT_SYMBOL(__cvmx_helper_cfg_pko_max_queue);
+EXPORT_SYMBOL(__cvmx_helper_cfg_pko_port_interface);
+EXPORT_SYMBOL(__cvmx_helper_cfg_pko_port_index);
+EXPORT_SYMBOL(__cvmx_helper_cfg_pko_port_eid);
+EXPORT_SYMBOL(__cvmx_helper_cfg_pko_max_engine);
+EXPORT_SYMBOL(cvmx_helper_cfg_opt_get);
+EXPORT_SYMBOL(cvmx_helper_cfg_opt_set);
+EXPORT_SYMBOL(cvmx_helper_cfg_ipd2pko_port_base);
+EXPORT_SYMBOL(cvmx_helper_cfg_ipd2pko_port_num);
+EXPORT_SYMBOL(cvmx_pko_queue_free_all);
+
+
+void cvmx_helper_cfg_show_cfg(void)
+{
+	int i, j;
+
+	for (i = 0; i < cvmx_helper_get_number_of_interfaces(); i++) {
+		cvmx_dprintf("cvmx_helper_cfg_show_cfg: interface%d mode %10s nports%4d\n", i,
+			     cvmx_helper_interface_mode_to_string(cvmx_helper_interface_get_mode(i)),
+			     cvmx_helper_interface_enumerate(i));
+
+		for (j = 0; j < cvmx_helper_interface_enumerate(i); j++) {
+			cvmx_dprintf("\tpknd[%i][%d]%d", i, j, __cvmx_helper_cfg_pknd(i, j));
+			cvmx_dprintf(" pko_port_base[%i][%d]%d", i, j, __cvmx_helper_cfg_pko_port_base(i, j));
+			cvmx_dprintf(" pko_port_num[%i][%d]%d\n", i, j, __cvmx_helper_cfg_pko_port_num(i, j));
+		}
+	}
+
+	for (i = 0; i < CVMX_HELPER_CFG_MAX_PKO_PORT; i++) {
+		if (__cvmx_helper_cfg_pko_queue_base(i) != CVMX_HELPER_CFG_INVALID_VALUE) {
+			cvmx_dprintf("cvmx_helper_cfg_show_cfg: pko_port%d qbase%d nqueues%d "
+				     "interface%d index%d\n", i,
+				     __cvmx_helper_cfg_pko_queue_base(i),
+				     __cvmx_helper_cfg_pko_queue_num(i), __cvmx_helper_cfg_pko_port_interface(i),
+				     __cvmx_helper_cfg_pko_port_index(i));
+		}
+	}
+}
+
+/*
+ * initialize cvmx_cfg_pko_port_map
+ */
+void cvmx_helper_cfg_init_pko_port_map(void)
+{
+	int i, j, k;
+	int pko_eid;
+	int pko_port_base, pko_port_max;
+	cvmx_helper_interface_mode_t mode;
+
+	/*
+	 * one pko_eid is allocated to each port except for ILK, NPI, and
+	 * LOOP. Each of the three has one eid.
+	 */
+	pko_eid = 0;
+	for (i = 0; i < cvmx_helper_get_number_of_interfaces(); i++) {
+		mode = cvmx_helper_interface_get_mode(i);
+		for (j = 0; j < cvmx_helper_interface_enumerate(i); j++) {
+			pko_port_base = cvmx_cfg_port[0][i][j].ccpp_pko_port_base;
+			pko_port_max = pko_port_base + cvmx_cfg_port[0][i][j].ccpp_pko_num_ports;
+			cvmx_helper_cfg_assert(pko_port_base != CVMX_HELPER_CFG_INVALID_VALUE);
+			cvmx_helper_cfg_assert(pko_port_max >= pko_port_base);
+			for (k = pko_port_base; k < pko_port_max; k++) {
+				cvmx_cfg_pko_port_map[k].ccppl_interface = i;
+				cvmx_cfg_pko_port_map[k].ccppl_index = j;
+				cvmx_cfg_pko_port_map[k].ccppl_eid = pko_eid;
+			}
+
+			if (!(mode == CVMX_HELPER_INTERFACE_MODE_NPI || mode == CVMX_HELPER_INTERFACE_MODE_LOOP
+			      || mode == CVMX_HELPER_INTERFACE_MODE_ILK))
+					pko_eid++;
+		}
+
+		if (mode == CVMX_HELPER_INTERFACE_MODE_NPI || mode == CVMX_HELPER_INTERFACE_MODE_LOOP ||
+		    mode == CVMX_HELPER_INTERFACE_MODE_ILK)
+			pko_eid++;
+	}
+
+	/*
+	 * Legal pko_eids [0, 0x13] should not be exhausted.
+	 */
+	cvmx_helper_cfg_assert(pko_eid <= 0x14);
+
+	cvmx_cfg_max_pko_engines = pko_eid;
+}
+
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+void cvmx_helper_cfg_set_jabber_and_frame_max()
+{
+	int interface, port;
+
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+		int node = cvmx_get_node_num();
+
+		/*Set the frame max size and jabber size to 65535. */
+		for (interface = 0; interface < cvmx_helper_get_number_of_interfaces(); interface++) {
+			/* Set the frame max size and jabber size to 65535, as the defaults
+		   	are too small. */
+			cvmx_helper_interface_mode_t imode = cvmx_helper_interface_get_mode(interface);
+			int num_ports = cvmx_helper_ports_on_interface(interface);
+
+			switch (imode) {
+			case CVMX_HELPER_INTERFACE_MODE_SGMII:
+				for (port = 0; port < num_ports; port++) {
+					cvmx_pki_set_max_frm_len(node, port, -1);
+					cvmx_write_csr(CVMX_BGXX_GMP_GMI_RXX_JABBER(port, interface), 65535);
+				}
+				break;
+			case CVMX_HELPER_INTERFACE_MODE_XAUI:
+			case CVMX_HELPER_INTERFACE_MODE_RXAUI:
+			case CVMX_HELPER_INTERFACE_MODE_XLAUI:
+			case CVMX_HELPER_INTERFACE_MODE_XFI:
+			case CVMX_HELPER_INTERFACE_MODE_10G_KR:
+			case CVMX_HELPER_INTERFACE_MODE_40G_KR4:
+				for (port = 0; port < num_ports; port++) {
+					cvmx_pki_set_max_frm_len(node, port, -1);
+					cvmx_write_csr(CVMX_BGXX_SMUX_RX_JABBER(port, interface), 65535);
+				}
+				break;
+			default:
+				break;
+			}
+		}
+	} else {
+
+		/*Set the frame max size and jabber size to 65535. */
+		for (interface = 0; interface < cvmx_helper_get_number_of_interfaces(); interface++) {
+			/* Set the frame max size and jabber size to 65535, as the defaults
+		   	are too small. */
+			cvmx_helper_interface_mode_t imode = cvmx_helper_interface_get_mode(interface);
+			int num_ports = cvmx_helper_ports_on_interface(interface);
+
+			switch (imode) {
+			case CVMX_HELPER_INTERFACE_MODE_SGMII:
+			case CVMX_HELPER_INTERFACE_MODE_QSGMII:
+			case CVMX_HELPER_INTERFACE_MODE_XAUI:
+			case CVMX_HELPER_INTERFACE_MODE_RXAUI:
+				for (port = 0; port < num_ports; port++)
+					cvmx_write_csr(CVMX_GMXX_RXX_JABBER(port, interface), 65535);
+				/* Set max and min value for frame check */
+				cvmx_pip_set_frame_check(interface, -1);
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_RGMII:
+			case CVMX_HELPER_INTERFACE_MODE_GMII:
+				/* Set max and min value for frame check */
+				cvmx_pip_set_frame_check(interface, -1);
+				for (port = 0; port < num_ports; port++) {
+					if (!OCTEON_IS_MODEL(OCTEON_CN50XX))
+						cvmx_write_csr(CVMX_GMXX_RXX_FRM_MAX(port, interface), 65535);
+					cvmx_write_csr(CVMX_GMXX_RXX_JABBER(port, interface), 65535);
+				}
+				break;
+			case CVMX_HELPER_INTERFACE_MODE_ILK:
+				/* Set max and min value for frame check */
+				cvmx_pip_set_frame_check(interface, -1);
+				for (port = 0; port < num_ports; port++) {
+					int ipd_port = cvmx_helper_get_ipd_port(interface, port);
+					cvmx_ilk_enable_la_header(ipd_port, 0);
+				}
+				break;
+			case CVMX_HELPER_INTERFACE_MODE_SRIO:
+				/* Set max and min value for frame check */
+				cvmx_pip_set_frame_check(interface, -1);
+				break;
+			case CVMX_HELPER_INTERFACE_MODE_AGL:
+				/* Set max and min value for frame check */
+				cvmx_pip_set_frame_check(interface, -1);
+				cvmx_write_csr(CVMX_AGL_GMX_RXX_FRM_MAX(0), 65535);
+				cvmx_write_csr(CVMX_AGL_GMX_RXX_JABBER(0), 65535);
+				break;
+			default:
+				break;
+			}
+		}
+	}
+}
+
+void cvmx_helper_cfg_store_short_packets_in_wqe()
+{
+	int interface, port;
+
+	if (!OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+		for (interface = 0; interface < cvmx_helper_get_number_of_interfaces(); interface++) {
+			int num_ports = cvmx_helper_ports_on_interface(interface);
+			/* Enable storing short packets only in the WQE */
+			for (port = 0; port < num_ports; port++) {
+				cvmx_pip_port_cfg_t port_cfg;
+				int pknd = port;
+				if (octeon_has_feature(OCTEON_FEATURE_PKND))
+					pknd = cvmx_helper_get_pknd(interface, port);
+				else
+					pknd = cvmx_helper_get_ipd_port(interface, port);
+				port_cfg.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(pknd));
+				port_cfg.s.dyn_rs = 1;
+				cvmx_write_csr(CVMX_PIP_PRT_CFGX(pknd), port_cfg.u64);
+			}
+		}
+	}
+}
+#endif /* CVMX_BUILD_FOR_LINUX_KERNEL*/
+
+
+int __cvmx_helper_cfg_pko_port_interface(int pko_port)
+{
+	return cvmx_cfg_pko_port_map[pko_port].ccppl_interface;
+}
+
+int __cvmx_helper_cfg_pko_port_index(int pko_port)
+{
+	return cvmx_cfg_pko_port_map[pko_port].ccppl_index;
+}
+
+int __cvmx_helper_cfg_pko_port_eid(int pko_port)
+{
+	return cvmx_cfg_pko_port_map[pko_port].ccppl_eid;
+}
+
+#define IPD2PKO_CACHE_Y(ipd_port)	(ipd_port) >> 8
+#define IPD2PKO_CACHE_X(ipd_port)	(ipd_port) & 0xff
+
+/*
+ * ipd_port to pko_port translation cache
+ */
+int __cvmx_helper_cfg_init_ipd2pko_cache(void)
+{
+	int i, j, n;
+	int ipd_y, ipd_x, ipd_port;
+
+	for (i = 0; i < cvmx_helper_get_number_of_interfaces(); i++) {
+		n = cvmx_helper_interface_enumerate(i);
+
+		for (j = 0; j < n; j++) {
+			ipd_port = cvmx_helper_get_ipd_port(i, j);
+			ipd_y = IPD2PKO_CACHE_Y(ipd_port);
+			ipd_x = IPD2PKO_CACHE_X(ipd_port);
+			ipd2pko_port_cache[ipd_y]
+			    [(ipd_port & 0x800) ? ((ipd_x >> 4) & 3) : ipd_x] = (struct cvmx_cfg_pko_port_pair) {
+			__cvmx_helper_cfg_pko_port_base(i, j), __cvmx_helper_cfg_pko_port_num(i, j)};
+		}
+	}
+
+	return 0;
+}
+
+int cvmx_helper_cfg_ipd2pko_port_base(int ipd_port)
+{
+	int ipd_y, ipd_x;
+
+	/* Internal PKO ports are not present in PKO3 */
+	if(OCTEON_IS_MODEL(OCTEON_CN78XX))
+		return ipd_port;
+
+	ipd_y = IPD2PKO_CACHE_Y(ipd_port);
+	ipd_x = IPD2PKO_CACHE_X(ipd_port);
+
+	return ipd2pko_port_cache[ipd_y]
+	    [(ipd_port & 0x800) ? ((ipd_x >> 4) & 3) : ipd_x].ccppp_base_port;
+}
+
+int cvmx_helper_cfg_ipd2pko_port_num(int ipd_port)
+{
+	int ipd_y, ipd_x;
+
+	ipd_y = IPD2PKO_CACHE_Y(ipd_port);
+	ipd_x = IPD2PKO_CACHE_X(ipd_port);
+
+	return ipd2pko_port_cache[ipd_y]
+	    [(ipd_port & 0x800) ? ((ipd_x >> 4) & 3) : ipd_x].ccppp_nports;
+}
+
+/**
+ * Return the number of queues to be assigned to this pko_port
+ *
+ * @param pko_port
+ * @return the number of queues for this pko_port
+ *
+ */
+static int cvmx_helper_cfg_dft_nqueues(int pko_port)
+{
+	cvmx_helper_interface_mode_t mode;
+	int interface;
+	int n;
+	int ret;
+
+	interface = __cvmx_helper_cfg_pko_port_interface(pko_port);
+	mode = cvmx_helper_interface_get_mode(interface);
+
+	n = NUM_ELEMENTS(__cvmx_pko_queue_static_config.pknd.pko_cfg_iface);
+
+	if (mode == CVMX_HELPER_INTERFACE_MODE_LOOP) {
+		ret =  __cvmx_pko_queue_static_config.pknd.pko_cfg_loop.queues_per_port;
+	}
+	else if (mode == CVMX_HELPER_INTERFACE_MODE_NPI) {
+		ret =  __cvmx_pko_queue_static_config.pknd.pko_cfg_npi.queues_per_port;
+	}
+
+
+	else if ((interface >= 0) && (interface < n) )  {
+		ret = __cvmx_pko_queue_static_config.pknd.pko_cfg_iface[interface].queues_per_port;
+	} else {
+		/* Should never be called */
+		ret = 1;
+	}
+	return ret;
+}
+
+static int cvmx_helper_cfg_init_pko_iports_and_queues_using_static_config(void)
+{
+	int pko_port_base = 0 ;
+	int cvmx_cfg_default_pko_nports = 1;
+	int i, j, n, k;
+	int rv = 0;
+
+	/* When not using config file, each port is assigned one internal pko port*/
+	for (i = 0; i < cvmx_helper_get_number_of_interfaces(); i++) {
+		n = cvmx_helper_interface_enumerate(i);
+		for (j = 0; j < n; j++) {
+			cvmx_cfg_port[0][i][j].ccpp_pko_port_base = pko_port_base;
+			cvmx_cfg_port[0][i][j].ccpp_pko_num_ports = cvmx_cfg_default_pko_nports;
+			/* Initalize interface early here so that the
+			   cvmx_helper_cfg_dft_nqueues() below
+			   can get the interface number corresponding to the pko port */
+			for (k = pko_port_base; k < pko_port_base + cvmx_cfg_default_pko_nports; k++) {
+				cvmx_cfg_pko_port_map[k].ccppl_interface = i;
+			}
+			pko_port_base += cvmx_cfg_default_pko_nports;
+		}
+	}
+	cvmx_helper_cfg_assert(pko_port_base <= CVMX_HELPER_CFG_MAX_PKO_PORT);
+
+
+	/* Assigning queues per pko */
+	for (i = 0; i < pko_port_base; i++) {
+		int base;
+		n = cvmx_helper_cfg_dft_nqueues(i);
+		base = cvmx_pko_queue_alloc(i, n);
+		if (base == -1)  {
+			cvmx_dprintf("ERROR: failed to alloc queues=%d for pko port=%d\n", n, i);
+			rv = -1;
+		}
+	}
+	return rv;
+}
+
+/**
+ * Returns if port is valid for a given interface
+ *
+ * @param interface  interface to check
+ * @param index      port index in the interface
+ *
+ * @return status of the port present or not.
+ */
+int cvmx_helper_is_port_valid(int xiface, int index)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	return  cvmx_cfg_port[xi.node][xi.interface][index].valid;
+}
+EXPORT_SYMBOL(cvmx_helper_is_port_valid);
+
+void cvmx_helper_set_port_valid(int xiface, int index, bool valid)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	cvmx_cfg_port[xi.node][xi.interface][index].valid = valid;
+}
+EXPORT_SYMBOL(cvmx_helper_set_port_valid);
+
+void cvmx_helper_set_mac_phy_mode(int xiface, int index, bool valid)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	cvmx_cfg_port[xi.node][xi.interface][index].sgmii_phy_mode = valid;
+}
+EXPORT_SYMBOL(cvmx_helper_set_mac_phy_mode);
+
+bool cvmx_helper_get_mac_phy_mode(int xiface, int index)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	return cvmx_cfg_port[xi.node][xi.interface][index].sgmii_phy_mode;
+}
+EXPORT_SYMBOL(cvmx_helper_get_mac_phy_mode);
+
+void cvmx_helper_set_1000x_mode(int xiface, int index, bool valid)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	cvmx_cfg_port[xi.node][xi.interface][index].sgmii_1000x_mode = valid;
+}
+EXPORT_SYMBOL(cvmx_helper_set_1000x_mode);
+
+bool cvmx_helper_get_1000x_mode(int xiface, int index)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	return cvmx_cfg_port[xi.node][xi.interface][index].sgmii_1000x_mode;
+}
+EXPORT_SYMBOL(cvmx_helper_get_1000x_mode);
+
+void cvmx_helper_set_agl_rx_clock_delay_bypass(int xiface, int index,
+					       bool valid)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	cvmx_cfg_port[xi.node][xi.interface][index].agl_rx_clk_delay_bypass = valid;
+}
+EXPORT_SYMBOL(cvmx_helper_set_agl_rx_clock_delay_bypass);
+
+bool cvmx_helper_get_agl_rx_clock_delay_bypass(int xiface, int index)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	return cvmx_cfg_port[xi.node][xi.interface][index].agl_rx_clk_delay_bypass;
+}
+EXPORT_SYMBOL(cvmx_helper_get_agl_rx_clock_delay_bypass);
+
+void cvmx_helper_set_agl_rx_clock_skew(int xiface, int index, uint8_t value)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	cvmx_cfg_port[xi.node][xi.interface][index].agl_rx_clk_skew = value;
+}
+EXPORT_SYMBOL(cvmx_helper_set_agl_rx_clock_skew);
+
+uint8_t cvmx_helper_get_agl_rx_clock_skew(int xiface, int index)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	return cvmx_cfg_port[xi.node][xi.interface][index].agl_rx_clk_skew;
+}
+EXPORT_SYMBOL(cvmx_helper_get_agl_rx_clock_skew);
+
+void cvmx_helper_set_port_force_link_up(int xiface, int index, bool value)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	cvmx_cfg_port[xi.node][xi.interface][index].force_link_up = value;
+}
+EXPORT_SYMBOL(cvmx_helper_set_port_force_link_up);
+
+bool cvmx_helper_get_port_force_link_up(int xiface, int index)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	return cvmx_cfg_port[xi.node][xi.interface][index].force_link_up;
+}
+EXPORT_SYMBOL(cvmx_helper_get_port_force_link_up);
+
+int __cvmx_helper_init_port_valid(void)
+{
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+	int i, j, n;
+	bool valid;
+	static void *fdt_addr = 0;
+
+	if (fdt_addr == 0)
+		fdt_addr = __cvmx_phys_addr_to_ptr(cvmx_sysinfo_get()->fdt_addr,
+						   (128*1024));
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+		return __cvmx_helper_parse_78xx_bgx_dt(fdt_addr);
+
+	/* TODO: Update this to behave more like 78XX */
+	for (i = 0; i < cvmx_helper_get_number_of_interfaces(); i++) {
+
+		n = cvmx_helper_interface_enumerate(i);
+		for (j = 0; j < n; j++) {
+			int ipd_port = cvmx_helper_get_ipd_port(i, j);
+			valid = (__cvmx_helper_board_get_port_from_dt(fdt_addr,
+								      ipd_port) == 1);
+			cvmx_helper_set_port_valid(i, j, valid);
+		}
+	}
+#endif
+	return 0;
+}
+
+typedef int (*cvmx_import_config_t)(void);
+cvmx_import_config_t cvmx_import_app_config = NULL;
+
+int __cvmx_helper_init_port_config_data_local(void)
+{
+	int rv = 0;
+	int dbg = 0;
+
+	if (octeon_has_feature(OCTEON_FEATURE_PKND))
+	{
+		if (cvmx_import_app_config) {
+			rv = (*cvmx_import_app_config)();
+			if (rv != 0) {
+				cvmx_dprintf("failed to import config\n");
+				return -1;
+			}
+		}
+
+		cvmx_helper_cfg_init_pko_port_map();
+		__cvmx_helper_cfg_init_ipd2pko_cache();
+	} else {
+		if (cvmx_import_app_config) {
+			rv = (*cvmx_import_app_config)();
+			if (rv != 0) {
+				cvmx_dprintf("failed to import config\n");
+				return -1;
+			}
+		}
+	}
+	if (dbg) {
+		cvmx_helper_cfg_show_cfg();
+		cvmx_pko_queue_show();
+	}
+	return rv;
+}
+
+extern int is_app_config_string_set(void);
+
+int cvmx_pko_alloc_iport_and_queues(int interface, int port, int port_cnt, int queue_cnt)
+{
+	int rv,p, port_start, cnt;
+
+	if (octeon_has_feature(OCTEON_FEATURE_PKND)) {
+		rv = cvmx_pko_internal_ports_alloc(interface, port, port_cnt);
+		if (rv < 0)  {
+			cvmx_dprintf("ERROR: failed to allocate internal ports for"
+				     "interface=%d port=%d cnt=%d\n", interface, port,
+				     port_cnt);
+			return -1;
+		}
+		port_start =  __cvmx_helper_cfg_pko_port_base(interface, port);
+		cnt  = __cvmx_helper_cfg_pko_port_num(interface, port);
+	} else {
+		port_start = cvmx_helper_get_ipd_port(interface, port);
+		cnt = 1;
+	}
+
+	for (p = port_start; p < port_start + cnt; p++) {
+		rv = cvmx_pko_queue_alloc(p, queue_cnt);
+		if (rv < 0)  {
+			cvmx_dprintf("ERROR: failed to allocate queues for port=%d"
+				     "cnt=%d\n", p, queue_cnt);
+			return -1;
+		}
+	}
+	return 0;
+}
+EXPORT_SYMBOL(cvmx_pko_alloc_iport_and_queues);
+
+int __cvmx_helper_init_port_config_data(void)
+{
+
+	int rv = 0;
+	int i, j, n;
+	int dbg = 0;
+	int static_config_set = 0;
+	int num_interfaces, interface;
+
+#if  ( !defined(CVMX_BUILD_FOR_LINUX_KERNEL)  && !defined(__U_BOOT__))
+	if (!(is_app_config_string_set()))
+		static_config_set = 1;
+#endif
+#ifdef __U_BOOT__
+	static_config_set = 1;
+#endif
+
+	if (octeon_has_feature(OCTEON_FEATURE_PKND))
+	{
+		int pknd = 0, bpid = 0;
+		if (static_config_set) {
+			cvmx_helper_cfg_init_pko_iports_and_queues_using_static_config();
+		}
+		/* Initialize pknd and bpid */
+		for (i = 0; i < cvmx_helper_get_number_of_interfaces(); i++) {
+			n = cvmx_helper_interface_enumerate(i);
+			for (j = 0; j < n; j++) {
+				cvmx_cfg_port[0][i][j].ccpp_pknd = pknd++;
+				cvmx_cfg_port[0][i][j].ccpp_bpid = bpid++;
+			}
+		}
+
+		cvmx_helper_cfg_assert(pknd <= CVMX_HELPER_CFG_MAX_PIP_PKND);
+		cvmx_helper_cfg_assert(bpid <= CVMX_HELPER_CFG_MAX_PIP_BPID);
+
+	} else {
+		if (static_config_set) {
+			cvmx_pko_queue_init_from_cvmx_config_non_pknd();
+		}
+	}
+
+	/* init ports, queues which are not initialized */
+	num_interfaces = cvmx_helper_get_number_of_interfaces();
+	for (interface = 0; interface < num_interfaces; interface++) {
+		int num_ports = __cvmx_helper_early_ports_on_interface(interface);
+		int port, port_base, queue;
+
+		for (port = 0; port < num_ports; port++) {
+			bool init_req = false;
+
+			if (octeon_has_feature(OCTEON_FEATURE_PKND)) {
+				port_base = __cvmx_helper_cfg_pko_port_base(interface, port);
+				if (port_base == CVMX_HELPER_CFG_INVALID_VALUE)
+					init_req = true;
+			} else {
+				port_base = cvmx_helper_get_ipd_port(interface, port);
+				queue = __cvmx_helper_cfg_pko_queue_base(port_base);
+				if (queue == CVMX_HELPER_CFG_INVALID_VALUE)
+					init_req = true;
+			}
+
+			if (init_req) {
+				rv = cvmx_pko_alloc_iport_and_queues(interface,
+								     port, 1, 1);
+				if (rv < 0) {
+					cvmx_dprintf("cvm_pko_alloc_iport_and_queues failed.\n");
+					return rv;
+				}
+			}
+		}
+	}
+
+	if (octeon_has_feature(OCTEON_FEATURE_PKND)) {
+		cvmx_helper_cfg_init_pko_port_map();
+		__cvmx_helper_cfg_init_ipd2pko_cache();
+	}
+
+	if (dbg) {
+		cvmx_helper_cfg_show_cfg();
+		cvmx_pko_queue_show();
+	}
+	return rv;
+}
+EXPORT_SYMBOL(__cvmx_helper_init_port_config_data);
+
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+/**
+ * @INTERNAL
+ * Store the FDT node offset in the device tree of a port
+ *
+ * @param xiface	node and interface
+ * @param index		port index
+ * @param node_offset	node offset to store
+ */
+void cvmx_helper_set_port_fdt_node_offset(int xiface, int index,
+					  int node_offset)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	cvmx_cfg_port[xi.node][xi.interface][index].port_fdt_node = node_offset;
+}
+
+/**
+ * @INTERNAL
+ * Return the FDT node offset in the device tree of a port
+ *
+ * @param xiface	node and interface
+ * @param index		port index
+ * @return		node offset of port or -1 if invalid
+ */
+int cvmx_helper_get_port_fdt_node_offset(int xiface, int index)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	return cvmx_cfg_port[xi.node][xi.interface][index].port_fdt_node;
+}
+
+/**
+ * @INTERNAL
+ * Store the FDT node offset in the device tree of a phy
+ *
+ * @param xiface	node and interface
+ * @param index		port index
+ * @param node_offset	node offset to store
+ */
+void cvmx_helper_set_phy_fdt_node_offset(int xiface, int index, int node_offset)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	cvmx_cfg_port[xi.node][xi.interface][index].phy_fdt_node = node_offset;
+}
+
+/**
+ * @INTERNAL
+ * Return the FDT node offset in the device tree of a phy
+ *
+ * @param xiface	node and interface
+ * @param index		port index
+ * @return		node offset of phy or -1 if invalid
+ */
+int cvmx_helper_get_phy_fdt_node_offset(int xiface, int index)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	return cvmx_cfg_port[xi.node][xi.interface][index].phy_fdt_node;
+}
+#endif /* !CVMX_BUILD_FOR_LINUX_KERNEL */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-pki.c b/arch/mips/cavium-octeon/executive/cvmx-helper-pki.c
new file mode 100644
index 0000000..5fedac3
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-pki.c
@@ -0,0 +1,1334 @@
+/***********************license start***************
+ * Copyright (c) 2003-2010  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * PKI helper functions.
+ */
+#include <linux/module.h>
+#include <asm/octeon/cvmx.h>
+#include <asm/octeon/cvmx-pip-defs.h>
+#include <asm/octeon/cvmx-pki-defs.h>
+#include <asm/octeon/cvmx-pki.h>
+#include <asm/octeon/cvmx-pow.h>
+#include <asm/octeon/cvmx-pki-resources.h>
+#include <asm/octeon/cvmx-helper-util.h>
+#include <asm/octeon/cvmx-ipd.h>
+#include <asm/octeon/cvmx-global-resources.h>
+#include <asm/octeon/cvmx-helper-pki.h>
+
+
+struct cvmx_pki_cluster_grp_config {
+	int grp_num;
+	uint64_t cluster_mask; /* bit mask of cluster assigned to this cluster group */
+};
+
+struct cvmx_pki_sso_grp_config {
+	int sso_grp_num;
+	int priority;
+	int weight;
+	int affinity;
+	uint64_t core_mask;
+	uint8_t core_mask_set;
+};
+
+static CVMX_SHARED int pki_helper_debug;
+
+CVMX_SHARED bool cvmx_pki_dflt_init[CVMX_MAX_NODES] = {[0 ... CVMX_MAX_NODES-1] = 1};
+
+static CVMX_SHARED struct cvmx_pki_cluster_grp_config pki_dflt_clgrp[CVMX_MAX_NODES] = {
+	{0, 0xf},
+	{0, 0xf} };
+
+CVMX_SHARED struct cvmx_pki_pool_config pki_dflt_pool[CVMX_MAX_NODES] = { [0 ... CVMX_MAX_NODES-1] = {
+	.pool_num = -1,
+	.buffer_size = 2048,
+	.buffer_count = 0} };
+
+CVMX_SHARED struct cvmx_pki_aura_config pki_dflt_aura[CVMX_MAX_NODES] = { [0 ... CVMX_MAX_NODES-1] = {
+	.aura_num = 0,
+	.pool_num = -1,
+	.buffer_count = 0} };
+
+CVMX_SHARED struct cvmx_pki_style_config pki_dflt_style[CVMX_MAX_NODES] = { [0 ... CVMX_MAX_NODES-1] = {
+	.parm_cfg = {.lenerr_en = 1, .maxerr_en = 1, .minerr_en = 1,
+	.fcs_strip = 1, .fcs_chk = 1, .first_skip = 40, .mbuff_size = 2048} } };
+
+static CVMX_SHARED struct cvmx_pki_sso_grp_config pki_dflt_sso_grp[CVMX_MAX_NODES];
+static CVMX_SHARED struct cvmx_pki_qpg_config pki_dflt_qpg[CVMX_MAX_NODES];
+CVMX_SHARED struct cvmx_pki_pkind_config pki_dflt_pkind[CVMX_MAX_NODES];
+CVMX_SHARED uint64_t pkind_style_map[CVMX_MAX_NODES][CVMX_PKI_NUM_PKIND] = { [0 ... CVMX_MAX_NODES-1] = {
+	0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,
+        16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,
+	32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47,
+	48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63} };
+
+int __cvmx_helper_setup_pki_cluster_groups(int node)
+{
+	uint64_t cl_mask;
+	int cl_group;
+
+	cl_group = cvmx_pki_cluster_grp_alloc(node, pki_dflt_clgrp[node].grp_num);
+	if (cl_group == -1) {
+		if (pki_dflt_clgrp[node].grp_num == -1)
+			return -1;
+		else
+			return 0; /* cluster already configured, share it */
+	}
+	cl_mask = pki_dflt_clgrp[node].cluster_mask;
+	if (pki_helper_debug)
+		cvmx_dprintf("pki-helper: setup pki cluster grp %d with cl_mask 0x%llx\n",
+			     (int)cl_group, (unsigned long long)cl_mask);
+	cvmx_pki_attach_cluster_to_group(node, cl_group, cl_mask);
+	return 0;
+}
+
+/**
+ * This function sets up pools/auras to be used by PKI
+ * @param node    node number
+ */
+int __cvmx_helper_pki_setup_sso_groups(int node)
+{
+	cvmx_coremask_t core_mask = CVMX_COREMASK_EMPTY;
+	cvmx_xgrp_t xgrp;
+	int grp;
+	int priority;
+	int weight;
+	int affinity;
+	uint64_t modify_mask;
+	uint8_t core_mask_set;
+
+	/* try to reserve sso groups and configure them if they are not configured */
+	/* vinita_to_do uncomment below when sso resource alloc is ready */
+#if 1
+	grp = pki_dflt_sso_grp[node].sso_grp_num;
+#else
+	grp = cvmx_sso_grp_alloc(node, pki_dflt_sso_grp[node].sso_grp_num);
+	if (grp == CVMX_RESOURCE_ALLOC_FAILED)
+		return -1;
+	else if (grp == CVMX_RESOURCE_ALREADY_RESERVED)
+		return 0; /* sso group already configured, share it */
+
+#endif
+	xgrp.xgrp = grp;
+	priority = pki_dflt_sso_grp[node].priority;
+	weight = pki_dflt_sso_grp[node].weight;
+	affinity = pki_dflt_sso_grp[node].affinity;
+	core_mask_set = pki_dflt_sso_grp[node].core_mask_set;
+	cvmx_coremask_set64_node(&core_mask, node, pki_dflt_sso_grp[node].core_mask);
+	modify_mask = CVMX_SSO_MODIFY_GROUP_PRIORITY |
+			CVMX_SSO_MODIFY_GROUP_WEIGHT |
+			CVMX_SSO_MODIFY_GROUP_AFFINITY;
+	if (pki_helper_debug)
+		cvmx_dprintf("pki-helper: set sso grp %d with priority %d weight %d core_mask 0x%llx\n",
+			     grp, priority, weight, (unsigned long long)pki_dflt_sso_grp[node].core_mask);
+	cvmx_sso_set_group_priority(node, xgrp, priority, weight,
+				    affinity, modify_mask);
+	cvmx_sso_set_group_core_affinity(xgrp, &core_mask, core_mask_set);
+	return 0;
+}
+
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+/**
+ * This function sets up pools/auras to be used by PKI
+ * @param node    node number
+ */
+int __cvmx_helper_pki_setup_fpa_pools(int node)
+{
+	int rs;
+	uint64_t buffer_count;
+	uint64_t buffer_size;
+
+	buffer_count = pki_dflt_pool[node].buffer_count;
+	if (buffer_count != 0) {
+		rs = cvmx_fpa_allocate_fpa_pools(node, &pki_dflt_pool[node].pool_num, 1);
+		if (rs == -1) {
+			if (pki_dflt_pool[node].pool_num == -1) {
+				cvmx_dprintf("ERROR: Failed to allocate pool %d\n", pki_dflt_pool[node].pool_num);
+				return -1;
+			}
+		} else {
+			buffer_size = pki_dflt_pool[node].buffer_size;
+			if (pki_helper_debug)
+				cvmx_dprintf("pki-helper: set fpa pool %d with buffer size %d buffer cnt %d\n",
+			pki_dflt_pool[node].pool_num, (int)buffer_size, (int)buffer_count);
+			cvmx_fpa3_pool_stack_init(node, pki_dflt_pool[node].pool_num, "PKI Pool0", 0,
+						 buffer_count, FPA_NATURAL_ALIGNMENT,
+						 buffer_size);
+			pki_dflt_aura[node].pool_num =  pki_dflt_pool[node].pool_num;
+		}
+	}
+	buffer_count = pki_dflt_aura[node].buffer_count;
+	if (buffer_count != 0) {
+		rs = cvmx_helper_fpa3_add_aura_to_pool(node, pki_dflt_pool[node].pool_num,
+						       &pki_dflt_aura[node].aura_num,
+						       buffer_count, NULL, "PKI Aura0");
+		if (rs == -1) {
+			if (pki_dflt_aura[node].aura_num == -1) {
+				cvmx_dprintf("ERROR: Failed to allocate aura %d\n", pki_dflt_aura[node].aura_num);
+				return -1;
+			} else
+				return 0; /* aura already configured, share it */
+		}
+	}
+	return 0;
+}
+#endif
+
+int __cvmx_helper_setup_pki_qpg_table(int node)
+{
+	int offset;
+
+	offset = cvmx_pki_qpg_entry_alloc(node, pki_dflt_qpg[node].qpg_base, 1);
+	if (offset == CVMX_RESOURCE_ALLOC_FAILED)
+		return -1;
+	else if (offset == CVMX_RESOURCE_ALREADY_RESERVED)
+		return 0; /* share the qpg table entry */
+	if (pki_helper_debug)
+		cvmx_dprintf("pki-helper: set qpg entry at offset %d with port add %d aura %d \
+				grp_ok %d grp_bad %d\n", offset, pki_dflt_qpg[node].port_add,
+				pki_dflt_qpg[node].aura, pki_dflt_qpg[node].grp_ok, pki_dflt_qpg[node].grp_bad);
+	cvmx_pki_write_qpg_entry(node, offset, pki_dflt_qpg[node].port_add, pki_dflt_qpg[node].aura,
+				 pki_dflt_qpg[node].grp_ok, pki_dflt_qpg[node].grp_bad);
+	return 0;
+}
+
+#if 0
+int __cvmx_helper_setup_pki_pcam_table(int node)
+{
+	uint64_t index;
+	int bank;
+
+	struct cvmx_pki_pcam_config *pcam_cfg;
+	index = pki_profiles[node].pcam_list.index;
+
+	while (index--) {
+		pcam_cfg = &pki_profiles[node].pcam_list.pcam_cfg[index];
+		bank = pcam_cfg->pcam_input.field % 2;
+		pcam_cfg->entry_num = cvmx_pki_pcam_alloc_entry(node, pcam_cfg->entry_num, bank, pcam_cfg->cluster_mask);
+		if (pcam_cfg->entry_num == -1) {
+			cvmx_dprintf("ERROR: Allocating pcam entry\n");
+			return -1;
+		}
+		cvmx_pki_pcam_write_entry(node, pcam_cfg->entry_num,
+					  pcam_cfg->cluster_mask, pcam_cfg->pcam_input,
+					  pcam_cfg->pcam_action);
+	}
+	return 0;
+}
+#endif
+
+/**
+ * This function installs the default VLAN entries to identify
+ * the VLAN and set WQE[vv], WQE[vs] if VLAN is found. In 78XX
+ * hardware (PKI) is not hardwired to recognize any 802.1Q VLAN
+ * Ethertypes
+ *
+ * @param node    node number
+ */
+int __cvmx_helper_pki_install_default_vlan(int node)
+{
+	struct cvmx_pki_pcam_input pcam_input;
+	struct cvmx_pki_pcam_action pcam_action;
+	enum cvmx_pki_term field;
+	int index;
+	int bank;
+	uint64_t cl_mask = CVMX_PKI_CLUSTER_ALL;
+
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X)) {
+		/* PKI-20858 */
+		int i;
+		for (i = 0; i < 4; i++) {
+			union cvmx_pki_clx_ecc_ctl ecc_ctl;
+			ecc_ctl.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_ECC_CTL(i));
+			ecc_ctl.s.pcam_en = 0;
+			ecc_ctl.s.pcam0_cdis = 1;
+			ecc_ctl.s.pcam1_cdis = 1;
+			cvmx_write_csr_node(node, CVMX_PKI_CLX_ECC_CTL(i), ecc_ctl.u64);
+		}
+	}
+
+	for (field = CVMX_PKI_PCAM_TERM_E_ETHTYPE0; field < CVMX_PKI_PCAM_TERM_E_ETHTYPE2; field++) {
+		bank = field & 0x01;
+
+		index = cvmx_pki_pcam_entry_alloc(node, CVMX_PKI_FIND_AVAL_ENTRY, bank, cl_mask);
+		if (index < 0) {
+			cvmx_dprintf("ERROR: Allocating pcam entry node=%d bank=%d\n", node, bank);
+			return -1;
+		}
+		pcam_input.style = 0;
+		pcam_input.style_mask = 0;
+		pcam_input.field = field;
+		pcam_input.field_mask = 0xfd;
+		pcam_input.data = 0x81000000;
+		pcam_input.data_mask = 0xffff0000;
+		pcam_action.parse_mode_chg = CVMX_PKI_PARSE_NO_CHG;
+		pcam_action.layer_type_set = CVMX_PKI_LTYPE_E_VLAN;
+		pcam_action.style_add = 0;
+		pcam_action.pointer_advance = 4;
+		cvmx_pki_pcam_write_entry(node, index, cl_mask, pcam_input, pcam_action);/*vinita_to_do, cluster_mask*/
+
+		index = cvmx_pki_pcam_entry_alloc(node, CVMX_PKI_FIND_AVAL_ENTRY, bank, cl_mask);
+		if (index < 0) {
+			cvmx_dprintf("ERROR: Allocating pcam entry node=%d bank=%d\n", node, bank);
+			return -1;
+		}
+		pcam_input.data = 0x88a80000;
+		cvmx_pki_pcam_write_entry(node, index, cl_mask, pcam_input, pcam_action);/*vinita_to_do, cluster_mask*/
+
+		index = cvmx_pki_pcam_entry_alloc(node, CVMX_PKI_FIND_AVAL_ENTRY, bank, cl_mask);
+		if (index < 0) {
+			cvmx_dprintf("ERROR: Allocating pcam entry node=%d bank=%d\n", node, bank);
+			return -1;
+		}
+		pcam_input.data = 0x92000000;
+		cvmx_pki_pcam_write_entry(node, index, cl_mask, pcam_input, pcam_action);/*vinita_to_do, cluster_mask*/
+
+		index = cvmx_pki_pcam_entry_alloc(node, CVMX_PKI_FIND_AVAL_ENTRY, bank, cl_mask);
+		if (index < 0) {
+			cvmx_dprintf("ERROR: Allocating pcam entry node=%d bank=%d\n", node, bank);
+			return -1;
+		}
+		pcam_input.data = 0x91000000;
+		cvmx_pki_pcam_write_entry(node, index, cl_mask, pcam_input, pcam_action);/*vinita_to_do, cluster_mask*/
+	}
+	return 0;
+}
+EXPORT_SYMBOL(__cvmx_helper_pki_install_default_vlan);
+
+void cvmx_helper_pki_enable(int node)
+{
+	if (pki_helper_debug)
+		cvmx_dprintf("enable PKI on node %d\n", node);
+	__cvmx_helper_pki_install_default_vlan(node);
+	cvmx_pki_setup_clusters(node);
+	cvmx_pki_enable_backpressure(node);
+	cvmx_pki_parse_enable(node, 0);
+	cvmx_pki_enable(node);
+}
+
+int __cvmx_helper_pki_global_setup(int node)
+{
+	if (!cvmx_pki_dflt_init[node])
+		return 0;
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+	/* Setup the packet pools*/
+	__cvmx_helper_pki_setup_fpa_pools(node);
+#endif
+	/*set up default cluster*/
+	__cvmx_helper_setup_pki_cluster_groups(node);
+	//__cvmx_helper_pki_setup_sso_groups(node);
+	__cvmx_helper_setup_pki_qpg_table(node);
+	return 0;
+}
+
+/* Frees up PKI resources consumed by that port. This function should only be called
+  if port resources (fpa pools aura, style qpg entry pcam entry etc.) are not shared */
+int cvmx_helper_pki_port_shutdown(int node)
+{
+	/* remove pcam entries */
+	/* vinita_to_do implemet later */
+	/* __cvmx_pki_port_rsrc_free(node); */
+	return 0;
+}
+
+/* Shutdown complete PKI hardware and software resources */
+void cvmx_helper_pki_shutdown(int node)
+{
+	/* remove pcam entries */
+	/* Disable PKI */
+	cvmx_pki_disable(node);
+	/* Free all prefetched buffers */
+	__cvmx_pki_free_ptr(node);
+	/* Reset PKI */
+	cvmx_pki_reset(node);
+	/* Free all the allocated PKI resources
+	except fpa pools & aura which will be done in fpa block */
+	__cvmx_pki_global_rsrc_free(node);
+}
+
+int cvmx_helper_pki_get_num_qpg_entry(enum cvmx_pki_qpg_qos qpg_qos)
+{
+	if (qpg_qos == CVMX_PKI_QPG_QOS_NONE)
+		return 1;
+	else if (qpg_qos == CVMX_PKI_QPG_QOS_VLAN || qpg_qos == CVMX_PKI_QPG_QOS_MPLS)
+		return 8;
+	else if (qpg_qos == CVMX_PKI_QPG_QOS_DSA_SRC) /*vinita_to_do for higig2*/
+		return 32;
+	else if (qpg_qos == CVMX_PKI_QPG_QOS_DIFFSERV || qpg_qos == CVMX_PKI_QPG_QOS_HIGIG)
+		return 64;
+	else {
+		cvmx_dprintf("ERROR: unrecognized qpg_qos = %d", qpg_qos);
+		return 0;
+	}
+}
+
+int __cvmx_helper_pki_port_setup(int node, int ipd_port)
+{
+	int interface, index;
+	int pknd, style_num;
+	int rs;
+	struct cvmx_pki_pkind_config pkind_cfg;
+
+	if (!cvmx_pki_dflt_init[node])
+		return 0;
+	interface = cvmx_helper_get_interface_num(ipd_port);
+	index = cvmx_helper_get_interface_index_num(ipd_port);
+
+	pknd = cvmx_helper_get_pknd(interface, index);
+	style_num = pkind_style_map[node][pknd];
+
+	/* try to reserve the style, if it is not configured already, reserve
+	and configure it */
+	rs = cvmx_pki_style_alloc(node, style_num);
+	if (rs < 0) {
+		if (rs == CVMX_RESOURCE_ALLOC_FAILED)
+			return -1;
+	} else {
+		if (pki_helper_debug)
+			cvmx_dprintf("pki-helper: set style %d with default parameters\n", style_num);
+		pkind_style_map[node][pknd] = style_num;
+		/* configure style with default parameters */
+		cvmx_pki_set_style_config(node, style_num, CVMX_PKI_CLUSTER_ALL,
+				     &pki_dflt_style[node]);
+	}
+	if (pki_helper_debug)
+		cvmx_dprintf("pki-helper: set pkind %d with initial style %d\n", pknd, style_num);
+	/* write pkind configuration */
+	pkind_cfg = pki_dflt_pkind[node];
+	pkind_cfg.initial_style = style_num;
+	cvmx_pki_set_pkind_config(node, pknd, &pkind_cfg);
+	return 0;
+}
+
+int cvmx_helper_pki_setup_qpg_table(int node, int num_entries,
+				    struct cvmx_pki_qpg_config *qpg_cfg)
+{
+	int entry;
+	int offset;
+
+	if (pki_helper_debug)
+		cvmx_dprintf("allocated %d qpg entries", num_entries);
+	offset = cvmx_pki_qpg_entry_alloc(node, qpg_cfg->qpg_base, num_entries);
+	if (pki_helper_debug)
+		cvmx_dprintf("at offset %d \n", offset);
+	if (offset == CVMX_RESOURCE_ALREADY_RESERVED) {
+		cvmx_dprintf("INFO:setup_qpg_table: offset %d already reserved\n", qpg_cfg->qpg_base);
+		return CVMX_RESOURCE_ALREADY_RESERVED;
+	} else if (offset == CVMX_RESOURCE_ALLOC_FAILED) {
+		cvmx_dprintf("ERROR:setup_qpg_table: no more entries available\n");
+		return CVMX_RESOURCE_ALLOC_FAILED;
+	}
+	qpg_cfg->qpg_base = offset;
+	for (entry = 0; entry < num_entries; entry++, offset++, qpg_cfg++) {
+		cvmx_pki_write_qpg_entry(node, offset,
+					 qpg_cfg->port_add, qpg_cfg->aura,
+					 qpg_cfg->grp_ok, qpg_cfg->grp_bad);
+	}
+	return offset - num_entries;
+}
+
+void cvmx_helper_pki_set_fcs_op(int node, int interface, int nports, int has_fcs)
+{
+	int index;
+	int pknd;
+	int cluster = 0;
+	cvmx_pki_clx_pkindx_cfg_t pkind_cfg;
+
+	for (index = 0; index < nports; index++) {
+		pknd = cvmx_helper_get_pknd(interface, index);
+                while (cluster < CVMX_PKI_NUM_CLUSTER) {
+                    /*vinita_to_do; find the cluster in use*/
+                    pkind_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_CFG(pknd, cluster));
+                    pkind_cfg.s.fcs_pres = has_fcs;
+                    cvmx_write_csr_node(node, CVMX_PKI_CLX_PKINDX_CFG(pknd, cluster), pkind_cfg.u64);
+                    cluster++;
+                }
+                /* make sure fcs_strip and fcs_check is also enable/disable for the style used by that port*/
+                cvmx_pki_endis_fcs_check(node, pknd, has_fcs, has_fcs);
+                cluster = 0;
+	}
+}
+
+/**
+ * This function sets the wqe buffer mode of all ports. First packet data buffer can reside
+ * either in same buffer as wqe OR it can go in separate buffer. If used the later mode,
+ * make sure software allocate enough buffers to now have wqe separate from packet data.
+ * @param node	                node number.
+ * @param pkt_outside_wqe.	0 = The packet link pointer will be at word [FIRST_SKIP]
+ *				    immediately followed by packet data, in the same buffer
+ *				    as the work queue entry.
+ *				1 = The packet link pointer will be at word [FIRST_SKIP] in a new
+ *				    buffer separate from the work queue entry. Words following the
+ *				    WQE in the same cache line will be zeroed, other lines in the
+ *				    buffer will not be modified and will retain stale data (from the
+ *				    buffers previous use). This setting may decrease the peak PKI
+ *				    performance by up to half on small packets.
+ */
+void cvmx_helper_pki_set_wqe_mode(int node, bool pkt_outside_wqe)
+{
+        int interface, port, pknd;
+        int num_intf, num_ports;
+        uint64_t style;
+
+        /* get the pkind used by this ipd port */
+        num_intf = cvmx_helper_get_number_of_interfaces();
+        for (interface = 0; interface < num_intf; interface++) {
+                num_ports = cvmx_helper_ports_on_interface(interface);
+                /*Skip invalid/disabled interfaces */
+                if (num_ports <= 0)
+                        continue;
+                for (port = 0; port < num_ports; port++) {
+                        pknd = cvmx_helper_get_pknd(interface, port);
+                        style = cvmx_pki_get_pkind_style(node, pknd);
+                        cvmx_pki_set_wqe_mode(node, style, pkt_outside_wqe);
+                }
+        }
+}
+
+void cvmx_helper_pki_set_dflt_pool(int node, int pool, int buffer_size, int buffer_count)
+{
+	if (pool == 0)
+		pool = -1;
+	pki_dflt_pool[node].pool_num = pool;
+	pki_dflt_pool[node].buffer_size = buffer_size;
+	pki_dflt_pool[node].buffer_count = buffer_count;
+}
+
+void cvmx_helper_pki_set_dflt_aura(int node, int aura, int pool, int buffer_count)
+{
+	pki_dflt_aura[node].aura_num = aura;
+	pki_dflt_aura[node].pool_num = pool;
+	pki_dflt_aura[node].buffer_count = buffer_count;
+}
+
+void cvmx_helper_pki_set_dflt_pool_buffer(int node, int buffer_count)
+{
+	pki_dflt_pool[node].buffer_count = buffer_count;
+}
+void cvmx_helper_pki_set_dflt_aura_buffer(int node, int buffer_count)
+{
+	pki_dflt_aura[node].buffer_count = buffer_count;
+}
+
+void cvmx_helper_pki_set_dflt_style(int node, struct cvmx_pki_style_config *style_cfg)
+{
+	pki_dflt_style[node] = *style_cfg;
+}
+
+void cvmx_helper_pki_get_dflt_style(int node, struct cvmx_pki_style_config *style_cfg)
+{
+	*style_cfg = pki_dflt_style[node];
+}
+
+void cvmx_helper_pki_set_dflt_qpg(int node, struct cvmx_pki_qpg_config *qpg_cfg)
+{
+	pki_dflt_qpg[node] = *qpg_cfg;
+}
+
+void cvmx_helper_pki_get_dflt_qpg(int node, struct cvmx_pki_qpg_config *qpg_cfg)
+{
+	*qpg_cfg = pki_dflt_qpg[node];
+}
+
+void cvmx_helper_pki_set_dflt_pkind_map(int node, int pkind, int style)
+{
+	pkind_style_map[node][pkind] = style;
+}
+
+void cvmx_helper_pki_no_dflt_init(int node)
+{
+	cvmx_pki_dflt_init[node] = 0;
+}
+
+/**
+ * This function sets up aura QOS for RED, backpressure and tail-drop.
+ *
+ * @param node       node number.
+ * @param aura       aura to configure.
+ * @param ena_red       enable RED based on [DROP] and [PASS] levels
+ *			1: enable 0:disable
+ * @param pass_thresh   pass threshold for RED.
+ * @param drop_thresh   drop threshold for RED
+ * @param ena_bp        enable backpressure based on [BP] level.
+ *			1:enable 0:disable
+ * @param bp_thresh     backpressure threshold.
+ * @param ena_drop      enable tail drop.
+ *			1:enable 0:disable
+ * @return Zero on success. Negative on failure
+ */
+int cvmx_helper_setup_aura_qos(int node, int aura, bool ena_red, bool ena_drop,
+			       uint64_t pass_thresh, uint64_t drop_thresh,
+			       bool ena_bp, uint64_t bp_thresh)
+{
+	ena_red = ena_red | ena_drop;
+	cvmx_fpa_setup_aura_qos(node, aura, ena_red, pass_thresh, drop_thresh,
+				ena_bp, bp_thresh);
+	cvmx_pki_enable_aura_qos(node, aura, ena_red, ena_drop, ena_bp);
+	return 0;
+}
+
+/**
+ * This function maps specified bpid to all the auras from which it can receive bp and
+ * then maps that bpid to all the channels, that bpid can asserrt bp on.
+ *
+ * @param node          node number.
+ * @param aura          aura number which will back pressure specified bpid.
+ * @param bpid          bpid to map.
+ * @param chl_map       array of channels to map to that bpid.
+ * @param chl_cnt	number of channel/ports to map to that bpid.
+ * @return Zero on success. Negative on failure
+ */
+int cvmx_helper_pki_map_aura_chl_bpid(int node, uint16_t aura, uint16_t bpid,
+                                      uint16_t chl_map[], uint16_t chl_cnt)
+{
+        uint16_t channel;
+
+        if (aura >= CVMX_PKI_NUM_AURA) {
+                cvmx_dprintf("ERROR: aura %d is > supported in hw\n", aura);
+                return -1;
+        }
+        if (bpid >= CVMX_PKI_NUM_BPID) {
+                cvmx_dprintf("ERROR: bpid %d is > supported in hw\n", bpid);
+                return -1;
+        }
+        cvmx_pki_write_aura_bpid(node, aura, bpid);
+        while (chl_cnt--) {
+                channel = chl_map[chl_cnt];
+                if ( channel >= CVMX_PKI_NUM_CHANNEL) {
+                        cvmx_dprintf("ERROR: channel %d is > supported in hw\n", channel);
+                        return -1;
+                }
+                cvmx_pki_write_channel_bpid(node, channel, bpid);
+        }
+        return 0;
+}
+
+int cvmx_helper_pki_port_shift(int xiface, enum cvmx_pki_qpg_qos qpg_qos)
+{
+        uint8_t num_qos;
+        cvmx_helper_interface_mode_t mode = cvmx_helper_interface_get_mode(xiface);
+
+        num_qos = cvmx_helper_pki_get_num_qpg_entry(qpg_qos);
+        if ((mode != CVMX_HELPER_INTERFACE_MODE_SGMII) &&
+             (mode != CVMX_HELPER_INTERFACE_MODE_NPI) &&
+             (mode != CVMX_HELPER_INTERFACE_MODE_LOOP)) {
+                return ffs(num_qos) - 1;
+             }
+             else if (num_qos <= 16)
+                     return 0;
+             else if (num_qos <= 32)
+                     return 1;
+             else
+                     return 2;
+}
+
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+
+int cvmx_helper_pki_set_gbl_schd(int node, struct cvmx_pki_global_schd *gblsch)
+{
+        int rs;
+
+        if (gblsch->setup_pool) {
+                if (pki_helper_debug)
+                        cvmx_dprintf("pki-helper:gbl setup global pool %d buff_size %d blocks %d\n",
+                                     gblsch->pool, (int)gblsch->pool_buff_size, (int)gblsch->pool_max_buff);
+                cvmx_helper_fpa3_init_pool(node, node, &gblsch->pool, gblsch->pool_buff_size,
+                                           gblsch->pool_max_buff, gblsch->pool_name);
+                if (pki_helper_debug)
+                        cvmx_dprintf("pool alloced is %d\n", gblsch->pool);
+        }
+        if (gblsch->setup_aura) {
+                if (pki_helper_debug)
+                        cvmx_dprintf("pki-helper:gbl setup global aura %d pool %d blocks %d\n",
+                                     gblsch->aura, gblsch->pool, (int)gblsch->aura_buff_cnt);
+                cvmx_helper_fpa3_add_aura_to_pool(node, gblsch->pool, &gblsch->aura,
+                                gblsch->aura_buff_cnt, NULL, gblsch->aura_name);
+                if (pki_helper_debug)
+                        cvmx_dprintf("aura alloced is %d\n", gblsch->aura);
+
+        }
+        if (gblsch->setup_sso_grp) {
+                rs = cvmx_sso_allocate_group(node);
+                if (rs < 0) {
+                        cvmx_dprintf("pki-helper:gbl ERROR: sso grp not available\n");
+                        return rs;
+                }
+                gblsch->sso_grp = rs;
+                if (pki_helper_debug)
+                        cvmx_dprintf("pki-helper:gbl: sso grp alloced is %d\n", gblsch->sso_grp);
+        }
+        return 0;
+}
+#endif /* CVMX_BUILD_FOR_LINUX_KERNEL */
+
+int __cvmx_helper_pki_qos_rsrcs(int node, struct cvmx_pki_qos_schd *qossch)
+{
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+        int rs;
+
+        /* Reserve pool resources */
+        if (qossch->pool_per_qos && qossch->pool < 0) {
+                if (pki_helper_debug)
+                        cvmx_dprintf("pki-helper:qos: setup pool %d buff_size %d blocks %d\n",
+                                     qossch->pool, (int)qossch->pool_buff_size, (int)qossch->pool_max_buff);
+                rs = cvmx_helper_fpa3_init_pool(node, node, &qossch->pool,
+                                qossch->pool_buff_size, qossch->pool_max_buff, qossch->pool_name);
+                if (rs < 0) {
+                        cvmx_dprintf("pki-helper:qos:ERROR: pool init failed\n");
+                        return rs;
+                }
+                if (pki_helper_debug)
+                        cvmx_dprintf("pool alloced is %d\n", qossch->pool);
+        }
+        /* Reserve aura resources */
+        if (qossch->aura_per_qos && qossch->aura < 0) {
+                if (pki_helper_debug)
+                        cvmx_dprintf("pki-helper:qos setup aura %d pool %d blocks %d\n",
+                                     qossch->aura, qossch->pool, (int)qossch->aura_buff_cnt);
+                rs = cvmx_helper_fpa3_add_aura_to_pool(node, qossch->pool,
+                                &qossch->aura, qossch->aura_buff_cnt, NULL, qossch->aura_name);
+                if (rs < 0) {
+                        cvmx_dprintf("pki-helper:qos:ERROR: aura init failed\n");
+                        return rs;
+                }
+                if (pki_helper_debug)
+                        cvmx_dprintf("aura alloced is %d\n", qossch->aura);
+        }
+        /* Reserve sso group resources */
+        if (qossch->sso_grp_per_qos && qossch->sso_grp < 0) {
+                rs = cvmx_sso_allocate_group(node);
+                if (rs < 0) {
+                        cvmx_dprintf("pki-helper:qos ERROR: sso grp not available\n");
+                        return rs;
+                }
+                qossch->sso_grp = rs;
+                if (pki_helper_debug)
+                        cvmx_dprintf("pki-helper:qos: sso grp alloced is %d\n", qossch->sso_grp);
+        }
+#endif /* CVMX_BUILD_FOR_LINUX_KERNEL */
+        return 0;
+}
+
+int __cvmx_helper_pki_port_rsrcs(int node, struct cvmx_pki_prt_schd *prtsch)
+{
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+        int rs;
+
+        /* Reserve pool resources */
+        if (prtsch->pool_per_prt && prtsch->pool < 0) {
+                if (pki_helper_debug)
+                        cvmx_dprintf("pki-helper:port setup pool %d buff_size %d blocks %d\n",
+                                     prtsch->pool, (int)prtsch->pool_buff_size, (int)prtsch->pool_max_buff);
+                rs = cvmx_helper_fpa3_init_pool(node, node, &prtsch->pool, prtsch->pool_buff_size,
+                                prtsch->pool_max_buff, prtsch->pool_name);
+                if (rs < 0) {
+                        cvmx_dprintf("pki-helper:port:ERROR: pool init failed\n");
+                        return rs;
+                }
+                if (pki_helper_debug)
+                        cvmx_dprintf("pool alloced is %d\n", prtsch->pool);
+        }
+        /* Reserve aura resources */
+        if (prtsch->aura_per_prt && prtsch->aura < 0) {
+                if (pki_helper_debug)
+                        cvmx_dprintf("pki-helper:port setup aura %d pool %d blocks %d\n",
+                                     prtsch->aura, prtsch->pool, (int)prtsch->aura_buff_cnt);
+                rs = cvmx_helper_fpa3_add_aura_to_pool(node, prtsch->pool,
+                                &prtsch->aura, prtsch->aura_buff_cnt, NULL, prtsch->aura_name);
+                if (rs < 0) {
+                        cvmx_dprintf("pki-helper:port:ERROR: aura init failed\n");
+                        return rs;
+                }
+                if (pki_helper_debug)
+                        cvmx_dprintf("aura alloced is %d\n", prtsch->aura);
+        }
+        /* Reserve sso group resources */
+        if (prtsch->sso_grp_per_prt && prtsch->sso_grp < 0) {
+                rs = cvmx_sso_allocate_group(node);
+                if (rs < 0) {
+                        cvmx_dprintf("pki-helper:port:ERROR: sso grp not available\n");
+                        return rs;
+                }
+                prtsch->sso_grp = rs;
+                if (pki_helper_debug)
+                        cvmx_dprintf("pki-helper:port: sso grp alloced is %d\n", prtsch->sso_grp);
+        }
+#endif /* CVMX_BUILD_FOR_LINUX_KERNEL */
+        return 0;
+}
+int __cvmx_helper_pki_intf_rsrcs(int node, struct cvmx_pki_intf_schd *intf)
+{
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+        int rs;
+
+        if (intf->pool_per_intf && intf->pool < 0) {
+                if (pki_helper_debug)
+                        cvmx_dprintf("pki-helper:intf: setup pool %d buff_size %d blocks %d\n",
+                                     intf->pool, (int)intf->pool_buff_size, (int)intf->pool_max_buff);
+                rs = cvmx_helper_fpa3_init_pool(node, node, &intf->pool, intf->pool_buff_size,
+                                intf->pool_max_buff, intf->pool_name);
+                if (rs == CVMX_RESOURCE_ALLOC_FAILED)
+                        return -1;
+                if (pki_helper_debug)
+                        cvmx_dprintf("pool alloced is %d\n", intf->pool);
+
+        }
+        if (intf->aura_per_intf && intf->aura < 0) {
+                if (pki_helper_debug)
+                        cvmx_dprintf("pki-helper:intf: setup aura %d pool %d blocks %d\n",
+                                     intf->aura, intf->pool, (int)intf->aura_buff_cnt);
+                rs = cvmx_helper_fpa3_add_aura_to_pool(node, intf->pool, &intf->aura,
+                                intf->aura_buff_cnt, NULL, intf->aura_name);
+                if (rs == CVMX_RESOURCE_ALLOC_FAILED)
+                        return -1;
+                if (pki_helper_debug)
+                        cvmx_dprintf("aura alloced is %d\n", intf->aura);
+        }
+        if (intf->sso_grp_per_intf && intf->sso_grp < 0) {
+                rs = cvmx_sso_allocate_group(node);
+                if (rs < 0) {
+                        cvmx_dprintf("pki-helper:intf:ERROR: sso grp not available\n");
+                        return rs;
+                }
+                intf->sso_grp = rs;
+        }
+#endif /* CVMX_BUILD_FOR_LINUX_KERNEL */
+        return 0;
+}
+
+int __cvmx_helper_pki_set_intf_qpg(int node, int port, int qpg_base, int num_entry,
+                                   struct cvmx_pki_intf_schd *intfsch)
+{
+        int offset;
+        int entry;
+        int port_add, aura, grp_ok, grp_bad;
+
+        if (pki_helper_debug)
+                cvmx_dprintf("pki-helper:intf_qpg port %d qpg_base %d num_entry %d",
+                             port, qpg_base, num_entry);
+        offset = cvmx_pki_qpg_entry_alloc(node, qpg_base, num_entry);
+        if (offset == CVMX_RESOURCE_ALREADY_RESERVED) {
+                cvmx_dprintf("pki-helper: INFO: qpg entries will be shared\n");
+                return offset;
+        } else if (offset == CVMX_RESOURCE_ALLOC_FAILED) {
+                cvmx_dprintf("pki-helper: ERROR: qpg entries not available\n");
+                return offset;
+        } else if (intfsch->qpg_base < 0)
+                intfsch->qpg_base = offset;
+                if (pki_helper_debug)
+                        cvmx_dprintf("qpg_base allocated is %d\n",offset);
+                for (entry = 0; entry < num_entry; entry++) {
+                        port_add = intfsch->prt_s[port].qos_s[entry].port_add;
+                        aura = intfsch->prt_s[port].qos_s[entry].aura;
+                        grp_ok = intfsch->prt_s[port].qos_s[entry].sso_grp;
+                        grp_bad = intfsch->prt_s[port].qos_s[entry].sso_grp;
+                        cvmx_pki_write_qpg_entry(node, (offset + entry),
+                                        port_add, aura, grp_ok, grp_bad);
+                }
+                return offset;
+}
+
+
+int cvmx_helper_pki_init_port(int ipd_port, struct cvmx_pki_prt_schd *prtsch)
+{
+        int num_qos;
+        int qos;
+        struct cvmx_pki_qos_schd *qossch;
+        struct cvmx_pki_style_config style_cfg;
+        struct cvmx_pki_pkind_config pknd_cfg;
+        int xiface = cvmx_helper_get_interface_num(ipd_port);
+        int pknd;
+        uint16_t mbuff_size;
+        int rs;
+
+        struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(ipd_port);
+        num_qos = cvmx_helper_pki_get_num_qpg_entry(prtsch->qpg_qos);
+        mbuff_size = prtsch->pool_buff_size;
+
+        /* Reserve port resources */
+        rs = __cvmx_helper_pki_port_rsrcs(xp.node, prtsch);
+        if (rs)
+                return rs;
+        /* Reserve qpg resources */
+        if (prtsch->qpg_base < 0) {
+                rs = cvmx_pki_qpg_entry_alloc(xp.node, prtsch->qpg_base, num_qos);
+                if (rs < 0) {
+                        cvmx_dprintf("pki-helper:port:ERROR: qpg entries not available\n");
+                        return CVMX_RESOURCE_ALLOC_FAILED;
+                }
+                prtsch->qpg_base = rs;
+                if (pki_helper_debug)
+                        cvmx_dprintf("port %d qpg_base %d allocated\n",ipd_port, prtsch->qpg_base);
+        }
+
+        if (prtsch->qpg_qos) {
+                for (qos = 0; qos < num_qos; qos++) {
+                        qossch = &prtsch->qos_s[qos];
+                        if (!qossch->pool_per_qos)
+                                qossch->pool = prtsch->pool;
+                        else if (qossch->pool_buff_size < mbuff_size)
+                                mbuff_size = qossch->pool_buff_size;
+                        if (!qossch->aura_per_qos)
+                                qossch->aura = prtsch->aura;
+                        if (!qossch->sso_grp_per_qos)
+                                qossch->sso_grp = prtsch->sso_grp;
+
+                        /* Reserve qos resources */
+                        rs = __cvmx_helper_pki_qos_rsrcs(xp.node, qossch);
+                        if (rs)
+                                return rs;
+                        cvmx_pki_write_qpg_entry(xp.node, prtsch->qpg_base + qos, qossch->port_add, qossch->aura,
+                                        qossch->sso_grp, qossch->sso_grp);
+                        if (pki_helper_debug)
+                                cvmx_dprintf("port %d qos %d has port_add %d aura %d grp %d\n",
+                                             ipd_port, qos, qossch->port_add, qossch->aura, qossch->sso_grp);
+                }
+        } else
+                cvmx_pki_write_qpg_entry(xp.node, prtsch->qpg_base, 0,
+                                         prtsch->aura, prtsch->sso_grp, prtsch->sso_grp);
+
+                /* Allocate style here and map it to the port */
+                rs = cvmx_pki_style_alloc(xp.node, prtsch->style);
+                if (rs == CVMX_RESOURCE_ALREADY_RESERVED) {
+                        cvmx_dprintf("pki-helper: INFO: style will be shared\n");
+                } else if (rs == CVMX_RESOURCE_ALLOC_FAILED) {
+                        cvmx_dprintf("pki-helper: ERROR: style not available\n");
+                        return CVMX_RESOURCE_ALLOC_FAILED;
+                } else {
+                        prtsch->style = rs;
+                        if (pki_helper_debug)
+                                cvmx_dprintf("port %d has style %d\n", ipd_port,prtsch->style);
+                        style_cfg = pki_dflt_style[xp.node];
+                        style_cfg.parm_cfg.qpg_qos = prtsch->qpg_qos;
+                        style_cfg.parm_cfg.qpg_base = prtsch->qpg_base;
+                        style_cfg.parm_cfg.qpg_port_msb = 0;
+                        style_cfg.parm_cfg.qpg_port_sh = 0;
+                        style_cfg.parm_cfg.mbuff_size = mbuff_size;
+                        cvmx_pki_set_style_config(xp.node, prtsch->style,
+                                        CVMX_PKI_CLUSTER_ALL, &style_cfg);
+                }
+                pknd = cvmx_helper_get_pknd(xiface, cvmx_helper_get_interface_index_num(ipd_port));
+                cvmx_pki_get_pkind_config(xp.node, pknd, &pknd_cfg);
+                pknd_cfg.initial_style = prtsch->style;
+                pknd_cfg.fcs_pres = __cvmx_helper_get_has_fcs(xiface);
+                cvmx_pki_set_pkind_config(xp.node, pknd, &pknd_cfg);
+        //cvmx_pki_show_port(xp.node, xiface, cvmx_helper_get_interface_index_num(ipd_port));
+
+                return 0;
+}
+EXPORT_SYMBOL(cvmx_helper_pki_init_port);
+
+int cvmx_helper_pki_init_interface(const int xiface,
+                                   struct cvmx_pki_intf_schd *intf, struct cvmx_pki_global_schd *gbl_schd)
+{
+        const uint16_t num_ports = cvmx_helper_ports_on_interface(xiface);
+        uint8_t qos;
+        uint16_t port = num_ports;
+        uint8_t port_msb = 0;
+        uint8_t port_shift = 0;
+        uint16_t num_entry = 0;
+        uint8_t num_qos;
+        int pknd;
+        int rs;
+        int has_fcs;
+        int ipd_port;
+        int qpg_base;
+        uint64_t mbuff_size = 0;
+        struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+        enum cvmx_pki_qpg_qos qpg_qos = CVMX_PKI_QPG_QOS_NONE;
+        struct cvmx_pki_prt_schd *prtsch;
+        struct cvmx_pki_qos_schd *qossch;
+        struct cvmx_pki_style_config style_cfg;
+        struct cvmx_pki_pkind_config pknd_cfg;
+
+        has_fcs = __cvmx_helper_get_has_fcs(xiface);
+
+        if (!intf->pool_per_intf) {
+                intf->pool = gbl_schd->pool;
+        } else
+                mbuff_size = intf->pool_buff_size;
+                if (!intf->aura_per_intf)
+                        intf->aura = gbl_schd->aura;
+                if (!intf->sso_grp_per_intf)
+                        intf->sso_grp = gbl_schd->sso_grp;
+
+                /* Allocate interface resources */
+                rs = __cvmx_helper_pki_intf_rsrcs(xi.node, intf);
+                if (rs)
+                        return rs;
+
+                for (port = 0; port < num_ports; port++) {
+                        prtsch = &intf->prt_s[port];
+
+                        /* Skip invalid/disabled ports */
+                        if (!cvmx_helper_is_port_valid(xiface, port) || prtsch->cfg_port)
+                                continue;
+
+                        if (!prtsch->pool_per_prt)
+                                prtsch->pool = intf->pool;
+                        else if (prtsch->pool_buff_size < mbuff_size || !mbuff_size)
+                                mbuff_size = prtsch->pool_buff_size;
+                        if (!prtsch->aura_per_prt)
+                                prtsch->aura = intf->aura;
+                        if (!prtsch->sso_grp_per_prt)
+                                prtsch->sso_grp = intf->sso_grp;
+
+                        rs = __cvmx_helper_pki_port_rsrcs(xi.node, prtsch);
+                        if (rs)
+                                return rs;
+
+                        /* Port is using qpg qos to schedule packets to differnet aura or sso group */
+                        num_qos = cvmx_helper_pki_get_num_qpg_entry(prtsch->qpg_qos);
+
+                        /* All ports will share the aura from port 0 for the respective qos */
+                        /* Port 0 should never have this set to TRUE **/
+                        if (intf->qos_share_aura && (port != 0)) {
+                                if (pki_helper_debug)
+                                        cvmx_dprintf("All ports will share same aura for all qos\n");
+                                for (qos = 0; qos < num_qos; qos++) {
+                                        qossch = &prtsch->qos_s[qos];
+                                        prtsch->qpg_qos = intf->prt_s[0].qpg_qos;
+                                        qossch->pool_per_qos = intf->prt_s[0].qos_s[qos].pool_per_qos;
+                                        qossch->aura_per_qos = intf->prt_s[0].qos_s[qos].aura_per_qos;
+                                        qossch->pool = intf->prt_s[0].qos_s[qos].pool;
+                                        qossch->aura = intf->prt_s[0].qos_s[qos].aura;
+                                }
+
+                        }
+                        if (intf->qos_share_grp && (port != 0)) {
+                                if (pki_helper_debug)
+                                        cvmx_dprintf("All ports will share same sso group for all qos\n");
+                                for (qos = 0; qos < num_qos; qos++) {
+                                        qossch = &prtsch->qos_s[qos];
+                                        qossch->sso_grp_per_qos = intf->prt_s[0].qos_s[qos].sso_grp_per_qos;
+                                        qossch->sso_grp = intf->prt_s[0].qos_s[qos].sso_grp;
+                                }
+                        }
+                        for (qos = 0; qos < num_qos; qos++) {
+                                qossch = &prtsch->qos_s[qos];
+                                if (!qossch->pool_per_qos) {
+                                        qossch->pool = prtsch->pool;
+                                //cvmx_dprintf("qos %d pool %d\n", qos, prtsch->pool);
+                                }
+                                else if (qossch->pool_buff_size < mbuff_size || !mbuff_size)
+                                        mbuff_size = qossch->pool_buff_size;
+                                if (!qossch->aura_per_qos)
+                                        qossch->aura = prtsch->aura;
+                                if (!qossch->sso_grp_per_qos)
+                                        qossch->sso_grp = prtsch->sso_grp;
+                                rs = __cvmx_helper_pki_qos_rsrcs(xi.node, qossch);
+                                if (rs)
+                                        return rs;
+                        }
+                }
+	/* Using port shift and port msb to schedule packets from differnt port to differnt
+                auras and different sso group */
+                /* Using QPG_QOS to schedule packets to different aura and sso group */
+	/* If ports needs to send packets to different aura and sso group
+                depending on packet qos */
+                /* We will need to set up aura and sso group for each port and each qos */
+	/* If all ports are using same style, they will be using same qpg_qos so
+                check only for port 0*/
+                if (intf->style_per_intf) {
+                        if (intf->prt_s[0].qpg_qos) { /* all ports using same style will use same qos defined in port 0 config */
+                                qpg_qos = intf->prt_s[0].qpg_qos;
+                                num_qos = cvmx_helper_pki_get_num_qpg_entry(intf->prt_s[0].qpg_qos);
+                                if (intf->qos_share_aura && intf->qos_share_grp) {
+				/* All ports will use same qpg offset so no need for
+                                        port_msb or port shift */
+                                        port_msb = 0;
+                                        port_shift = 0;
+                                        num_entry = num_qos;
+                                        qpg_base = intf->qpg_base;
+                                        rs = __cvmx_helper_pki_set_intf_qpg(xi.node, 0, qpg_base, num_entry, intf);
+                                        if (rs == -1)
+                                                return rs;
+                                        intf->qpg_base = rs;
+                                } else {
+                                        port_msb = 8;
+                                        port_shift = cvmx_helper_pki_port_shift(xiface, intf->prt_s[0].qpg_qos);
+                                //cvmx_dprintf("pki-helper: num qpg entry needed %d\n", (int)num_entry);
+                                //cvmx_dprintf("pki-helper:port_msb= %d port_shift=%d\n", port_msb, port_shift);
+                                        num_entry = num_qos;
+                                        for (port = 0; port < num_ports; port++) {
+                                                /* Skip invalid/disabled ports */
+                                                prtsch =  &intf->prt_s[port];
+                                                if (!cvmx_helper_is_port_valid(xiface, port) || prtsch->cfg_port)
+                                                        continue;
+                                                ipd_port = cvmx_helper_get_ipd_port(xiface, port);
+                                                qpg_base = intf->qpg_base + ((ipd_port & 0xff) << port_shift);
+                                                rs = __cvmx_helper_pki_set_intf_qpg(xi.node, port, qpg_base, num_entry, intf);
+                                                if (rs == -1)
+                                                        return rs;
+                                                prtsch->qpg_base = rs;
+                                        }
+                                        intf->qpg_base = intf->prt_s[0].qpg_base;
+                                }
+                        } else if (intf->prt_s[0].aura_per_prt || intf->prt_s[0].sso_grp_per_prt) {
+                                /* Every port is using their own aura or group but no qos */
+                                port_msb = 8;
+                                port_shift = 0;
+                                num_entry = 1;
+                        //cvmx_dprintf("pki-helper: aura/grp_per_prt: num qpg entry needed %d\n", (int)num_entry);
+                                for (port = 0; port < num_ports; port++) {
+                                        prtsch =  &intf->prt_s[port];
+                                        /* Skip invalid/disabled ports */
+                                        if (!cvmx_helper_is_port_valid(xiface, port) || prtsch->cfg_port)
+                                                continue;
+                                        ipd_port = cvmx_helper_get_ipd_port(xiface, port);
+                                        qpg_base = intf->qpg_base + ((ipd_port & 0xff) << port_shift);
+                                //cvmx_dprintf("port %d intf_q_base=%d q_base= %d\n", port, intf->qpg_base, qpg_base);
+                                        qpg_base = cvmx_pki_qpg_entry_alloc(xi.node, qpg_base, num_entry);
+                                        if (qpg_base == CVMX_RESOURCE_ALREADY_RESERVED) {
+                                                cvmx_dprintf("pki-helper: INFO: qpg entries will be shared\n");
+                                        } else if (qpg_base == CVMX_RESOURCE_ALLOC_FAILED) {
+                                                cvmx_dprintf("pki-helper: ERROR: qpg entries not available\n");
+                                                return qpg_base;
+                                        } else {
+                                                if (intf->qpg_base < 0)
+                                                        intf->qpg_base = qpg_base;
+                                                prtsch->qpg_base = qpg_base;
+                                        }
+                                        cvmx_pki_write_qpg_entry(xi.node, qpg_base, 0, prtsch->aura,                                                prtsch->sso_grp, prtsch->sso_grp);
+                                }
+                                intf->qpg_base = intf->prt_s[0].qpg_base;
+                        } else { /* All ports on that intf use same port_add, aura & sso grps */
+			/* All ports will use same qpg offset so no need for
+                                port_msb or port shift */
+                                port_msb = 0;
+                                port_shift = 0;
+                                num_entry = 1;
+                                qpg_base = intf->qpg_base;
+                                qpg_base = cvmx_pki_qpg_entry_alloc(xi.node, qpg_base, num_entry);
+                                if (qpg_base == CVMX_RESOURCE_ALREADY_RESERVED) {
+                                        cvmx_dprintf("pki-helper: INFO: qpg entries will be shared\n");
+                                } else if (qpg_base == CVMX_RESOURCE_ALLOC_FAILED) {
+                                        cvmx_dprintf("pki-helper: ERROR: qpg entries not available\n");
+                                        return qpg_base;
+                                } else
+                                        intf->qpg_base = qpg_base;
+                                        cvmx_pki_write_qpg_entry(xi.node, qpg_base, 0, intf->aura,
+                                                        intf->sso_grp, intf->sso_grp);
+                        }
+                        if (!mbuff_size) {
+                                if (!gbl_schd->setup_pool) {
+                                        cvmx_dprintf("No pool has setup for intf %d\n", xiface);
+                                        return -1;
+                                }
+                                mbuff_size = gbl_schd->pool_buff_size;
+                                cvmx_dprintf("interface %d is using global pool\n", xiface);
+                        }
+                        /* Allocate style here and map it to all ports on interface */
+                        rs = cvmx_pki_style_alloc(xi.node, intf->style);
+                        if (rs == CVMX_RESOURCE_ALREADY_RESERVED) {
+                                cvmx_dprintf("passthrough: INFO: style will be shared\n");
+                        } else if (rs == CVMX_RESOURCE_ALLOC_FAILED) {
+                                cvmx_dprintf("passthrough: ERROR: style not available\n");
+                                return CVMX_RESOURCE_ALLOC_FAILED;
+                        } else {
+                                intf->style = rs;
+                                if (pki_helper_debug)
+                                        cvmx_dprintf("style %d allocated intf %d qpg_base %d\n", intf->style, xiface, intf->qpg_base);
+                                style_cfg = pki_dflt_style[xi.node];
+                                style_cfg.parm_cfg.qpg_qos = qpg_qos;
+                                style_cfg.parm_cfg.qpg_base = intf->qpg_base;
+                                style_cfg.parm_cfg.qpg_port_msb = port_msb;
+                                style_cfg.parm_cfg.qpg_port_sh = port_shift;
+                                style_cfg.parm_cfg.mbuff_size = mbuff_size;
+                                cvmx_pki_set_style_config(xi.node, intf->style,
+                                                CVMX_PKI_CLUSTER_ALL, &style_cfg);
+                        }
+                        for (port = 0; port < num_ports; port++) {
+                                prtsch = &intf->prt_s[port];
+                                /* Skip invalid/disabled ports */
+                                if (!cvmx_helper_is_port_valid(xiface, port) || prtsch->cfg_port)
+                                        continue;
+                                prtsch->style = intf->style;
+                                pknd = cvmx_helper_get_pknd(xiface, port);
+                                cvmx_pki_get_pkind_config(xi.node, pknd, &pknd_cfg);
+                                pknd_cfg.initial_style = intf->style;
+                                pknd_cfg.fcs_pres = has_fcs;
+                                cvmx_pki_set_pkind_config(xi.node, pknd, &pknd_cfg);
+                        //cvmx_pki_show_port(xi.node, xiface, port);
+                        }
+                } else if (intf->style_per_prt) {
+                        port_msb = 0;
+                        port_shift = 0;
+                        for (port = 0; port < num_ports; port++) {
+                                prtsch = &intf->prt_s[port];
+                                /* Skip invalid/disabled ports */
+                                if (!cvmx_helper_is_port_valid(xiface, port) || prtsch->cfg_port)
+                                        continue;
+                                if (prtsch->qpg_qos && intf->qos_share_aura &&
+                                    intf->qos_share_grp && port !=0) {
+                                        if(pki_helper_debug)
+                                                cvmx_dprintf("intf %d has all ports share qos aura n grps\n",xiface);
+                                /* Ports have differnet styles but want to share same qpg entries.
+                                        this might never be the case */
+                                        prtsch->qpg_base = intf->prt_s[0].qpg_base;
+                                    }
+                                    ipd_port = cvmx_helper_get_ipd_port(xiface, port);
+                                    cvmx_helper_pki_init_port(ipd_port, prtsch);
+                        }
+                }
+                return 0;
+}
+
+#if 0
+static const char* pki_ltype_sprint(int ltype) {
+        switch (ltype) {
+                case CVMX_PKI_LTYPE_E_ENET:	return "(ENET)";
+                case CVMX_PKI_LTYPE_E_VLAN:	return "(VLAN)";
+                case CVMX_PKI_LTYPE_E_SNAP_PAYLD:return "(SNAP_PAYLD)";
+                case CVMX_PKI_LTYPE_E_ARP:	return "(ARP)";
+                case CVMX_PKI_LTYPE_E_RARP:	return "(RARP)";
+                case CVMX_PKI_LTYPE_E_IP4:	return "(IP4)";
+                case CVMX_PKI_LTYPE_E_IP4_OPT:	return "(IP4_OPT)";
+                case CVMX_PKI_LTYPE_E_IP6:	return "(IP6)";
+                case CVMX_PKI_LTYPE_E_IP6_OPT:	return "(IP6_OPT)";
+                case CVMX_PKI_LTYPE_E_IPSEC_ESP:	return "(IPSEC_ESP)";
+                case CVMX_PKI_LTYPE_E_IPFRAG:	return "(IPFRAG)";
+                case CVMX_PKI_LTYPE_E_IPCOMP:	return "(IPCOMP)";
+                case CVMX_PKI_LTYPE_E_TCP:	return "(TCP)";
+                case CVMX_PKI_LTYPE_E_UDP:	return "(UDP)";
+                case CVMX_PKI_LTYPE_E_SCTP:	return "(SCTP)";
+                case CVMX_PKI_LTYPE_E_UDP_VXLAN:	return "(UDP_VXLAN)";
+                case CVMX_PKI_LTYPE_E_GRE:	return "(GRE)";
+                case CVMX_PKI_LTYPE_E_NVGRE:	return "(NVGRE)";
+                case CVMX_PKI_LTYPE_E_GTP:	return "(GTP)";
+                default: 			return "";
+        }
+}
+
+void pki_wqe_dump(const cvmx_wqe_78xx_t* wqp)
+{
+        int i;
+        /* it is not cvmx_shared so per core only */
+        static uint64_t count;
+
+        cvmx_dprintf("Wqe entry for packet %lld\n", (unsigned long long)count++);
+        cvmx_dprintf("    WORD%02d: %016llx", (int)0, (unsigned long long)wqp->word0.u64);
+        cvmx_dprintf(" aura=0x%x", wqp->word0.aura);
+        cvmx_dprintf(" apad=%d", wqp->word0.apad);
+        cvmx_dprintf(" chan=0x%x", wqp->word0.channel);
+        cvmx_dprintf(" bufs=%d" , wqp->word0.bufs);
+        cvmx_dprintf(" style=0x%x" , wqp->word0.style);
+        cvmx_dprintf(" pknd=0x%x" , wqp->word0.pknd);
+        cvmx_dprintf("\n");
+        cvmx_dprintf("    WORD%02d: %016llx", (int)1, (unsigned long long)wqp->word1.u64);
+        cvmx_dprintf(" len=%d" , wqp->word1.len);
+        cvmx_dprintf(" grp=0x%x" , wqp->word1.grp);
+        cvmx_dprintf(" tt=%s", OCT_TAG_TYPE_STRING(wqp->word1.tag_type));
+        cvmx_dprintf(" tag=0x%08x" , wqp->word1.tag);
+        cvmx_dprintf("\n");
+        if (wqp->word2.u64) {
+                cvmx_dprintf("    WORD%02d: %016llx"  , (int)2, (unsigned long long)wqp->word2.u64);
+                if (wqp->word2.le_hdr_type)
+                        cvmx_dprintf(" [LAE]");
+                if (wqp->word2.lb_hdr_type)
+                        cvmx_dprintf(" lbty=%d"  "%s",
+                               wqp->word2.lb_hdr_type, pki_ltype_sprint(wqp->word2.lb_hdr_type));
+                if (wqp->word2.lc_hdr_type)
+                        cvmx_dprintf(" lcty=%d"  "%s",
+                               wqp->word2.lc_hdr_type, pki_ltype_sprint(wqp->word2.lc_hdr_type));
+                if (wqp->word2.ld_hdr_type)
+                        cvmx_dprintf(" ldty=%d"  "%s",
+                               wqp->word2.ld_hdr_type, pki_ltype_sprint(wqp->word2.ld_hdr_type));
+                if (wqp->word2.le_hdr_type)
+                        cvmx_dprintf(" lety=%d"  "%s",
+                               wqp->word2.le_hdr_type, pki_ltype_sprint(wqp->word2.le_hdr_type));
+                if (wqp->word2.lf_hdr_type)
+                        cvmx_dprintf(" lfty=%d"  "%s",
+                               wqp->word2.lf_hdr_type, pki_ltype_sprint(wqp->word2.lf_hdr_type));
+                if (wqp->word2.lg_hdr_type)
+                        cvmx_dprintf(" lgty=%d"  "%s",
+                               wqp->word2.lg_hdr_type, pki_ltype_sprint(wqp->word2.lg_hdr_type));
+                if (wqp->word2.pcam_flag1)
+                        cvmx_dprintf(" PF1");
+                if (wqp->word2.pcam_flag2)
+                        cvmx_dprintf(" PF2");
+                if (wqp->word2.pcam_flag3)
+                        cvmx_dprintf(" PF3");
+                if (wqp->word2.pcam_flag4)
+                        cvmx_dprintf(" PF4");
+                if (wqp->word2.vlan_valid || wqp->word2.vlan_stacked) {
+                        if (wqp->word2.vlan_valid)
+                                cvmx_dprintf(" vlan valid");
+                        if (wqp->word2.vlan_stacked)
+                                cvmx_dprintf(" vlan stacked");
+                        cvmx_dprintf(" ");
+}
+                if (wqp->word2.stat_inc) cvmx_dprintf(" stat_inc");
+                if (wqp->word2.is_frag) cvmx_dprintf(" L3 Fragment");
+                if (wqp->word2.is_l3_bcast) cvmx_dprintf(" L3 Broadcast");
+                if (wqp->word2.is_l3_mcast) cvmx_dprintf(" L3 Multicast");
+                if (wqp->word2.is_l2_bcast) cvmx_dprintf(" L2 Broadcast");
+                if (wqp->word2.is_l2_mcast) cvmx_dprintf(" L2 Multicast");
+                if (wqp->word2.is_raw) cvmx_dprintf(" RAW");
+                if (wqp->word2.err_level || wqp->word2.err_code) {
+                        cvmx_dprintf(" errlev=%d" , wqp->word2.err_level);
+                        cvmx_dprintf(" opcode=0x%x" , wqp->word2.err_code);
+}
+                cvmx_dprintf("\n");
+}
+        cvmx_dprintf("    WORD%02d: %016llx", (int)3, (unsigned long long)wqp->packet_ptr.u64);
+
+        cvmx_dprintf(" size=%d" , wqp->packet_ptr.size);
+        cvmx_dprintf(" addr=0x%llx" , (unsigned long long)wqp->packet_ptr.addr);
+
+        cvmx_dprintf("\n");
+        if (wqp->word4.u64) {
+                cvmx_dprintf("    WORD%02d: %016llx", (int)4, (unsigned long long)wqp->word4.u64);
+                if (wqp->word4.ptr_layer_a) cvmx_dprintf(" laptr=%d" , wqp->word4.ptr_layer_a);
+                if (wqp->word4.ptr_layer_b) cvmx_dprintf(" lbptr=%d" , wqp->word4.ptr_layer_b);
+                if (wqp->word4.ptr_layer_c) cvmx_dprintf(" lcptr=%d" , wqp->word4.ptr_layer_c);
+                if (wqp->word4.ptr_layer_d) cvmx_dprintf(" ldptr=%d" , wqp->word4.ptr_layer_d);
+                if (wqp->word4.ptr_layer_e) cvmx_dprintf(" leptr=%d" , wqp->word4.ptr_layer_e);
+                if (wqp->word4.ptr_layer_f) cvmx_dprintf(" lfptr=%d" , wqp->word4.ptr_layer_f);
+                if (wqp->word4.ptr_layer_g) cvmx_dprintf(" lgptr=%d" , wqp->word4.ptr_layer_g);
+                if (wqp->word4.ptr_vlan) cvmx_dprintf(" vlptr=%d" , wqp->word4.ptr_vlan);
+                cvmx_dprintf("\n");
+}
+        for (i=0; i < 10; ++i) {
+                if (wqp->wqe_data[i]) cvmx_dprintf("    WORD%02d: %016llx"  "\n", i+5, (unsigned long long)wqp->wqe_data[i]);
+}
+}
+#endif
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-pko3.c b/arch/mips/cavium-octeon/executive/cvmx-helper-pko3.c
new file mode 100644
index 0000000..ba2b05f
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-pko3.c
@@ -0,0 +1,1147 @@
+/***********************license start***************
+ * Copyright (c) 2003-2013  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/*
+ * File version info: $Rev$
+ *
+ * PKOv3 helper file
+ */
+//#define	__SUPPORT_PFC_ON_XAUI
+
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include <linux/module.h>
+#include <asm/octeon/cvmx.h>
+#include <asm/octeon/cvmx-clock.h>
+#include <asm/octeon/cvmx-ilk.h>
+#include <asm/octeon/cvmx-pko3.h>
+#include <asm/octeon/cvmx-pko3-resources.h>
+#include <asm/octeon/cvmx-helper.h>
+#include <asm/octeon/cvmx-helper-pko.h>
+#include <asm/octeon/cvmx-helper-pko3.h>
+#include <asm/octeon/cvmx-helper-bgx.h>
+#include <asm/octeon/cvmx-helper-cfg.h>
+#else
+#include "cvmx.h"
+#include "cvmx-ilk.h"
+#include "cvmx-fpa.h"
+#include "cvmx-pko3.h"
+#include "cvmx-pko3-resources.h"
+#include "cvmx-helper.h"
+#include "cvmx-helper-pko.h"
+#include "cvmx-helper-pko3.h"
+#include "cvmx-helper-bgx.h"
+#include "cvmx-helper-cfg.h"
+#endif
+
+/*
+ * PKO3 requires 4 buffers for each active Descriptor Queue,
+ * and because it is not known how many DQs will in fact be used
+ * when the PKO pool is populated, it is allocated the maximum
+ * number it may required.
+ * The additional 1K buffers are provisioned to acomodate
+ * jump buffers for long descriptor commands, that should
+ * be rarely used.
+ */
+#ifndef CVMX_PKO3_POOL_BUFFERS
+#ifdef CVMX_PKO3_DQ_MAX_DEPTH
+/* Assume worst case of 16 words per command per DQ */
+#define CVMX_PKO3_POOL_BUFFERS (CVMX_PKO3_DQ_MAX_DEPTH*16/511*1024);
+#else
+#define CVMX_PKO3_POOL_BUFFERS (1024*4+1024)
+#endif
+#endif
+/* Simulator has limited memory, use fewer buffers */
+#ifndef	CVMX_PKO3_POOL_BUFS_SIM
+#define CVMX_PKO3_POOL_BUFS_SIM (1024*4+1024)
+#endif
+
+/* channels are present at L2 queue level by default */
+static const int cvmx_pko_default_channel_level = 0;
+
+static const int debug = 0;
+
+/* These global variables are relevant for boot CPU only */
+int16_t __cvmx_pko3_aura_num = 1020;
+int16_t __cvmx_pko3_pool_num = -1;
+
+/* This constant can not be modified, defined here for clarity only */
+#define CVMX_PKO3_POOL_BUFFER_SIZE 4096 /* 78XX PKO requires 4KB */
+
+/**
+ * @INTERNAL
+ *
+ * Build an owner tag based on interface/port
+ */
+static int __cvmx_helper_pko3_res_owner(int ipd_port)
+{
+	int res_owner;
+	const int res_owner_pfix = 0x19d0 << 14;
+
+	ipd_port &= 0x3fff;	/* 12-bit for local CHAN_E value + node */
+
+	res_owner = res_owner_pfix | ipd_port;
+
+	return res_owner;
+}
+
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+/**
+ * Configure an AURA/POOL designated for PKO internal use.
+ *
+ * This pool is used for (a) memory buffers that store PKO descriptor queues,
+ * (b) buffers for use with PKO_SEND_JUMP_S sub-header.
+ *
+ * The buffers of type (a) are never accessed by software, and their number
+ * should be at least equal to 4 times the number of descriptor queues
+ * in use.
+ *
+ * Type (b) buffers are consumed by PKO3 command-composition code,
+ * and are released by the hardware upon completion of transmission.
+ *
+ * @returns -1 if the pool could not be established or 12-bit AURA
+ * that includes the node number for use in PKO3 intialization call.
+ *
+ * NOTE: Linux kernel should pass its own aura to PKO3 initialization
+ * function so that the buffers can be mapped into kernel space
+ * for when software needs to adccess their contents.
+ *
+ */
+static int __cvmx_pko3_config_memory(unsigned node)
+{
+	int pool_num , aura_num;
+	int pool, block_size;
+	unsigned buf_count;
+	int res;
+
+	aura_num = __cvmx_pko3_aura_num;
+	pool_num = __cvmx_pko3_pool_num;
+
+        /* Check for legacy PKO buffer pool */
+        pool = cvmx_fpa_get_pko_pool();
+	block_size = cvmx_fpa_get_block_size(pool);
+
+        /* Avoid redundant pool creation */
+        if (block_size > 0 && block_size == CVMX_PKO3_POOL_BUFFER_SIZE) {
+                cvmx_dprintf("WARNING: %s: "
+                        "Legacy PKO pool %d re-used, "
+			"buffer count may not suffice.\n",
+                        __func__, pool);
+                aura_num = pool_num = pool;
+        } else if (block_size > 0) {
+                cvmx_dprintf("WARNING: %s: "
+                        "Legacy PKO pool %d created with wrong buffer size %u, "
+			"ignored.\n",
+                        __func__, pool, block_size);
+	}
+
+	/* Simulator has limited memory */
+	if (cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_SIM)
+		buf_count = CVMX_PKO3_POOL_BUFS_SIM;
+	else
+		buf_count = CVMX_PKO3_POOL_BUFFERS;
+
+	if (aura_num != pool) {
+
+		cvmx_dprintf("%s: creating aura %u with %u buffers\n",
+			__func__, aura_num, buf_count);
+
+		res = cvmx_helper_fpa_init(node, &pool_num, &aura_num,
+				CVMX_PKO3_POOL_BUFFER_SIZE,
+				buf_count,
+				"PKO Aura",
+				NULL);
+
+		if (res < 0) return res;
+	}
+
+	/* Store numbers e.g. for destruction */
+	__cvmx_pko3_pool_num = pool_num;
+	__cvmx_pko3_aura_num = aura_num;
+
+	/* Combine LAURA with NODE */
+	aura_num |= node << 10;
+
+	return aura_num;
+}
+
+
+#endif
+
+/** Initialize a channelized port
+ * This is intended for LOOP and NPI interfaces which have one MAC
+ * per interface and need a channel per subinterface (e.g. ring).
+ *
+ * FIXME: Consider merging this function with the ILK configuration code
+ */
+static int __cvmx_pko3_config_chan_interface( int xiface, unsigned num_chans,
+	uint8_t num_queues, bool prioritized)
+{
+	int l1_q_num;
+	int l2_q_num;
+	int res;
+	int pko_mac_num;
+	uint16_t ipd_port;
+	int res_owner, prio;
+	unsigned i;
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+
+	if (prioritized && num_queues > 1)
+		prio = num_queues;
+	else
+		prio = -1;
+
+	if(debug)
+		cvmx_dprintf("%s: configuring xiface %u:%u with "
+				"%u chans %u queues each\n",
+				__FUNCTION__, xi.node, xi.interface,
+				num_chans, num_queues);
+
+	/* all channels all go to the same mac */
+	pko_mac_num = __cvmx_pko3_get_mac_num(xiface, 0);
+	if (pko_mac_num < 0) {
+                cvmx_dprintf ("%s: ERROR Invalid interface\n", __FUNCTION__);
+		return -1;
+	}
+
+	/* Resources of all channels on this port have common owner */
+	ipd_port = cvmx_helper_get_ipd_port(xiface, 0);
+
+	/* Build an identifiable owner */
+	res_owner = __cvmx_helper_pko3_res_owner(ipd_port);
+
+	/* Reserve port queue to make sure the MAC is not already configured */
+	l1_q_num = pko_mac_num;
+        l1_q_num = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_PORT_QUEUES,
+				res_owner, l1_q_num, 1);
+
+	if (l1_q_num != pko_mac_num) {
+                cvmx_dprintf ("%s: ERROR Reserving L1 PQ\n", __FUNCTION__);
+		return -1;
+	}
+
+
+        /* allocate level 2 queues, one per channel */
+        l2_q_num = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_L2_QUEUES, res_owner,
+					 -1, num_chans);
+        if (l2_q_num < 0) {
+                cvmx_dprintf ("%s: ERROR allocation L2 SQ\n", __FUNCTION__);
+                return -1;
+        }
+
+
+	/* Configre <num_chans> children for MAC, non-prioritized */
+	res = cvmx_pko3_pq_config_children( xi.node,
+			pko_mac_num, l2_q_num, num_chans, -1);
+
+	if (res < 0) {
+		cvmx_dprintf("%s: ERROR: Failed channel queues\n",
+			__FUNCTION__);
+		return -1;
+	}
+
+	/* Configure children with one DQ per channel */
+	for (i = 0; i < num_chans; i++) {
+		int l3_q, l4_q, l5_q, dq, res;
+		unsigned chan = i;
+
+		l3_q = l4_q = l5_q = dq = -1;
+
+		ipd_port = cvmx_helper_get_ipd_port(xiface, chan);
+
+		/* map channels to l2 queues */
+		cvmx_pko3_map_channel(xi.node, l1_q_num, l2_q_num+chan,
+			ipd_port);
+
+		l3_q = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_L3_QUEUES,
+			res_owner, -1, 1);
+		if(l3_q < 0) goto _fail;
+
+		res = cvmx_pko3_sq_config_children(xi.node, 2, l2_q_num+chan,
+			l3_q, 1, 1);
+		if(res < 0) goto _fail;
+
+		l4_q = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_L4_QUEUES,
+			res_owner, -1, 1);
+		if(l4_q < 0) goto _fail;
+
+		res = cvmx_pko3_sq_config_children(xi.node, 3,
+			l3_q, l4_q, 1, 1);
+		if(res < 0) goto _fail;
+
+		l5_q = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_L5_QUEUES,
+			res_owner, -1, 1);
+		if(l5_q < 0) goto _fail;
+		res = cvmx_pko3_sq_config_children(xi.node, 4,
+			l4_q, l5_q, 1, 1);
+		if(res < 0) goto _fail;
+
+		dq = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_DESCR_QUEUES,
+			res_owner, -1, num_queues);
+		if(dq < 0) goto _fail;
+
+		res = cvmx_pko3_sq_config_children(xi.node, 5, l5_q,
+			dq, num_queues, prio);
+		if(res < 0) goto _fail;
+
+		/* register DQ range with the translation table */
+		res = __cvmx_pko3_ipd_dq_register(xiface, chan, dq, num_queues);
+		if(res < 0) goto _fail;
+	}
+
+	return 0;
+  _fail:
+	cvmx_dprintf("%s: ERROR:configuring queues for xiface %u chan %u\n",
+		     __FILE__, (unsigned int)xiface, i);
+	return -1;
+}
+
+/** Initialize a single Ethernet port with PFC-style channels
+ *
+ * One interface can contain multiple ports, this function is per-port
+ * Here, a physical port is allocated 8 logical channel, one per VLAN
+ * tag priority, one DQ is assigned to each channel, and all 8 DQs
+ * are registered for that IPD port.
+ * Note that the DQs are arrange such that the Ethernet QoS/PCP field
+ * can be used as an offset to the value returned by cvmx_pko_base_queue_get().
+ *
+ * For HighGig2 mode, 16 channels may be desired, instead of 8,
+ * but this function does not support that.
+ */
+static int __cvmx_pko3_config_pfc_interface(int xiface, unsigned port)
+{
+	int l1_q_num;
+	int l2_q_num;
+	int res;
+	int pko_mac_num;
+	int l3_q, l4_q, l5_q, dq;
+	const unsigned num_chans = 8;
+	cvmx_xiface_t xi = cvmx_helper_xiface_to_node_interface(xiface);
+	unsigned node = xi.node;
+	uint16_t ipd_port;
+	int res_owner;
+	unsigned i;
+
+	if(debug)
+		cvmx_dprintf("%s: configuring xiface %u:%u port %u with %u PFC channels\n",
+			__FUNCTION__, node, xi.interface, port, num_chans);
+
+	/* Get MAC number for the iface/port */
+	pko_mac_num = __cvmx_pko3_get_mac_num(xiface, port);
+	if (pko_mac_num < 0) {
+		cvmx_dprintf ("%s: ERROR Invalid interface\n", __FUNCTION__);
+		return -1;
+	}
+
+	ipd_port = cvmx_helper_get_ipd_port(xiface, port);
+
+	/* Build an identifiable owner identifier */
+	res_owner = __cvmx_helper_pko3_res_owner(ipd_port);
+
+	/* Allocate port queue to make sure the MAC is not already configured */
+	l1_q_num = cvmx_pko_alloc_queues(node, CVMX_PKO_PORT_QUEUES,
+				res_owner, pko_mac_num, 1);
+
+	if (l1_q_num != pko_mac_num) {
+		cvmx_dprintf ("%s: ERROR allocation L1 SQ\n", __FUNCTION__);
+		return -1;
+	}
+
+
+	/* allocate or reserve level 2 queues */
+	l2_q_num = cvmx_pko_alloc_queues(node, CVMX_PKO_L2_QUEUES, res_owner,
+					 -1, num_chans);
+	if (l2_q_num < 0) {
+		cvmx_dprintf ("%s: ERROR allocation L2 SQ\n", __FUNCTION__);
+		return -1;
+	}
+
+
+	/* Configre <num_chans> children for MAC, with static priority */
+	res = cvmx_pko3_pq_config_children( node,
+			pko_mac_num, l2_q_num, num_chans, num_chans);
+
+	if (res < 0) {
+		cvmx_dprintf("Error: Could not setup PFC Channel queues\n");
+		return -1;
+	}
+
+	/* Allocate all SQ levels at once to assure contigous range */
+	l3_q = cvmx_pko_alloc_queues(node, CVMX_PKO_L3_QUEUES,
+			res_owner, -1, num_chans);
+	l4_q = cvmx_pko_alloc_queues(node, CVMX_PKO_L4_QUEUES,
+			res_owner, -1, num_chans);
+	l5_q = cvmx_pko_alloc_queues(node, CVMX_PKO_L5_QUEUES,
+			res_owner, -1, num_chans);
+	dq = cvmx_pko_alloc_queues(node, CVMX_PKO_DESCR_QUEUES,
+			res_owner, -1, num_chans);
+	if (l3_q < 0 || l4_q < 0 || l5_q < 0 ||dq < 0) {
+		cvmx_dprintf("%s: ERROR:could not allocate queues, "
+			"xiface %u:%u port %u\n",
+			__FUNCTION__, xi.node, xi.interface, port);
+		return -1;
+	}
+
+	/* Configure children with one DQ per channel */
+	for (i = 0; i < num_chans; i++) {
+		uint16_t chan, dq_num;
+		/* <i> moves in priority order, 0=highest, 7=lowest */
+
+		/* Get CHAN_E value for this PFC channel, PCP in low 3 bits */
+		chan = ipd_port | cvmx_helper_prio2qos(i);
+
+		/* map channels to L2 queues */
+		cvmx_pko3_map_channel(node, l1_q_num, l2_q_num+i, chan);
+
+		cvmx_pko3_sq_config_children(node, 2, l2_q_num+i, l3_q+i, 1, 1);
+
+		cvmx_pko3_sq_config_children(node, 3, l3_q+i, l4_q+i, 1, 1);
+
+		cvmx_pko3_sq_config_children(node, 4, l4_q+i, l5_q+i, 1, 1);
+
+		/* Configure DQs in QoS order, so that QoS/PCP can be index */
+		dq_num = dq + cvmx_helper_prio2qos(i);
+		cvmx_pko3_sq_config_children(node, 5, l5_q+i, dq_num, 1, 1);
+	}
+
+	/* register entire DQ range with the IPD translation table */
+	__cvmx_pko3_ipd_dq_register(xiface, port, dq, num_chans);
+
+	return 0;
+}
+
+/** 
+ * Initialize a simple interface with a a given number of
+ * fair or prioritized queues.
+ * This function will assign one channel per sub-interface.
+ */
+int __cvmx_pko3_config_gen_interface(int xiface, uint8_t subif,
+	uint8_t num_queues, bool prioritized)
+{
+	int l1_q_num;
+	int l2_q_num;
+	int res, res_owner;
+	int pko_mac_num;
+	int l3_q, l4_q, l5_q, dq;
+	uint16_t ipd_port;
+	cvmx_xiface_t xi = cvmx_helper_xiface_to_node_interface(xiface);
+	int static_pri;
+
+	if (num_queues == 0) {
+		num_queues = 1;
+		cvmx_dprintf("%s: WARNING xiface %#x misconfigured\n",
+			__func__, xiface);
+	}
+
+	if(debug)
+		cvmx_dprintf("%s: configuring xiface %u:%u/%u nq=%u %s\n",
+			     __FUNCTION__, xi.node, xi.interface, subif,
+			    num_queues, (prioritized)?"qos":"fair");
+
+	/* Get MAC number for the iface/port */
+	pko_mac_num = __cvmx_pko3_get_mac_num(xiface, subif);
+	if (pko_mac_num < 0) {
+		cvmx_dprintf ("%s: ERROR Invalid interface %u:%u\n",
+			__FUNCTION__, xi.node, xi.interface);
+		return -1;
+	}
+
+	ipd_port = cvmx_helper_get_ipd_port(xiface, subif);
+
+	if(debug)
+		cvmx_dprintf("%s: xiface %u:%u/%u ipd_port=%#03x\n",
+			     __FUNCTION__, xi.node, xi.interface, subif,
+				ipd_port);
+
+	/* Build an identifiable owner identifier */
+	res_owner = __cvmx_helper_pko3_res_owner(ipd_port);
+
+	/* Reserve port queue to make sure the MAC is not already configured */
+	l1_q_num = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_PORT_QUEUES,
+					 res_owner, pko_mac_num, 1);
+
+	if (l1_q_num != pko_mac_num) {
+		cvmx_dprintf("%s: ERROR xiface %u:%u/%u"
+			" failed allocation L1 SQ\n",
+			__FUNCTION__, xi.node, xi.interface, subif);
+		return -1;
+	}
+
+	/* allocate or reserve level 2 queues */
+	l2_q_num = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_L2_QUEUES, res_owner,
+				-1, 1);
+	if (l2_q_num < 0) {
+		cvmx_dprintf("%s: ERROR xiface %u:%u/%u"
+			"  failed allocation L2 SQ\n",
+			__FUNCTION__, xi.node, xi.interface, subif);
+		return -1;
+	}
+
+
+	/* Configre L2 SQ */
+	res = cvmx_pko3_pq_config_children(xi.node, pko_mac_num,
+				l2_q_num, 1, 1);
+
+	if (res < 0) {
+		cvmx_dprintf("%s: ERROR xiface %u:%u/%u"
+			" failed configuring PQ\n",
+			__FUNCTION__, xi.node, xi.interface, subif);
+		return -1;
+	}
+
+	l3_q = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_L3_QUEUES,
+				res_owner, -1, 1);
+	l4_q = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_L4_QUEUES,
+				res_owner, -1, 1);
+	l5_q = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_L5_QUEUES,
+				res_owner, -1, 1);
+	dq = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_DESCR_QUEUES,
+				res_owner, -1, num_queues);
+	if (dq < 0) {
+		cvmx_dprintf("%s: ERROR xiface %u:%u/%u"
+			" failed configuring DQs\n",
+			__FUNCTION__, xi.node, xi.interface, subif);
+		return -1;
+	}
+
+	/* Configure hierarchy */
+	cvmx_pko3_sq_config_children(xi.node, 2, l2_q_num, l3_q, 1, 1);
+	cvmx_pko3_sq_config_children(xi.node, 3, l3_q, l4_q, 1, 1);
+	cvmx_pko3_sq_config_children(xi.node, 4, l4_q, l5_q, 1, 1);
+
+	/* Configure DQs relative priority (a.k.a. scheduling) */
+	if (prioritized) {
+		/* With 8 queues or fewer, use static priority, else WRR */
+		static_pri = (num_queues < 9)? num_queues: 0;
+	} else {
+		/* Set equal-RR scheduling among queues */
+		static_pri = -1;
+	}
+
+	cvmx_pko3_sq_config_children(xi.node, 5, l5_q, dq,
+		num_queues, static_pri);
+
+	/* map IPD/channel to L2 queues */
+	cvmx_pko3_map_channel(xi.node, l1_q_num, l2_q_num, ipd_port);
+
+	/* register DQ/IPD translation */
+	__cvmx_pko3_ipd_dq_register(xiface, subif, dq, num_queues);
+
+	if(debug)
+		cvmx_dprintf("%s: xiface %u:%u/%u qs %u-%u\n",
+			     __FUNCTION__, xi.node, xi.interface, subif,
+				dq, dq+num_queues-1);
+	return 0;
+}
+EXPORT_SYMBOL(__cvmx_pko3_config_gen_interface);
+
+/** Initialize the NULL interface
+ *
+ * A NULL interface is a special case in that it is not
+ * one of the enumerated interfaces in the system, and does
+ * not apply to input either. Still, it can be very handy
+ * for dealing with packets that should be discarded in
+ * a generic, streamlined way.
+ *
+ * The Descriptor Queue 0 will be reserved for the NULL interface
+ * and the normalized (i.e. IPD) port number has the all-ones value.
+ */
+static int __cvmx_pko3_config_null_interface(unsigned int node)
+{
+	int l1_q_num;
+	int l2_q_num;
+	int l3_q, l4_q, l5_q;
+	int res, res_owner;
+	int xiface;
+
+	const int dq = 0;	/* Reserve DQ#0 for NULL */
+	const int pko_mac_num = 0x1C; /* MAC# 28 virtual MAC for NULL */
+
+	if(debug)
+		cvmx_dprintf("%s: configuring null interface\n", __FUNCTION__);
+
+	/* Build an identifiable owner identifier by MAC# for easy release */
+	res_owner = __cvmx_helper_pko3_res_owner(CVMX_PKO3_IPD_PORT_NULL);
+	if (res_owner < 0) {
+		cvmx_dprintf ("%s: ERROR Invalid interface\n", __FUNCTION__);
+		return -1;
+	}
+
+	/* Reserve port queue to make sure the MAC is not already configured */
+	l1_q_num = cvmx_pko_alloc_queues(node, CVMX_PKO_PORT_QUEUES,
+				res_owner, pko_mac_num, 1);
+
+	if (l1_q_num != pko_mac_num) {
+		cvmx_dprintf ("%s: ERROR reserving L1 SQ\n", __FUNCTION__);
+		return -1;
+	}
+
+	/* allocate or reserve level 2 queues */
+	l2_q_num = cvmx_pko_alloc_queues(node, CVMX_PKO_L2_QUEUES, res_owner,
+				-1, 1);
+	if (l2_q_num < 0) {
+		cvmx_dprintf ("%s: ERROR allocating L2 SQ\n", __FUNCTION__);
+		return -1;
+	}
+
+
+	/* Configre L2 SQ */
+	res = cvmx_pko3_pq_config_children(node, pko_mac_num, l2_q_num, 1, 1);
+
+	if (res < 0) {
+		cvmx_dprintf("%s: ERROR: L2 queue\n", __FUNCTION__);
+		return -1;
+	}
+
+	l3_q = cvmx_pko_alloc_queues(node, CVMX_PKO_L3_QUEUES, res_owner,-1, 1);
+	l4_q = cvmx_pko_alloc_queues(node, CVMX_PKO_L4_QUEUES, res_owner,-1, 1);
+	l5_q = cvmx_pko_alloc_queues(node, CVMX_PKO_L5_QUEUES, res_owner,-1, 1);
+
+	/* Reserve DQ at 0 by convention */
+	res = cvmx_pko_alloc_queues(node, CVMX_PKO_DESCR_QUEUES, res_owner,
+		dq, 1);
+	if (dq != res) {
+		cvmx_dprintf("%s: ERROR: could not reserve DQs\n",
+			__FUNCTION__);
+		return -1;
+	}
+
+	/* Configure hierarchy */
+	cvmx_pko3_sq_config_children(node, 2, l2_q_num, l3_q, 1, 1);
+	cvmx_pko3_sq_config_children(node, 3, l3_q, l4_q, 1, 1);
+	cvmx_pko3_sq_config_children(node, 4, l4_q, l5_q, 1, 1);
+	cvmx_pko3_sq_config_children(node, 5, l5_q, dq, 1, 1);
+
+	/* NULL interface does not need to map to a CHAN_E */
+
+	/* register DQ/IPD translation */
+	xiface = cvmx_helper_node_interface_to_xiface(node, __CVMX_XIFACE_NULL);
+	__cvmx_pko3_ipd_dq_register(xiface, 0, dq, 1);
+
+	/* open the null DQ here */
+	res = cvmx_pko_dq_open(node, dq);
+
+	return 0;
+}
+
+/** Open all descriptor queues belonging to an interface/port
+ * @INTERNAL
+ */
+int __cvmx_pko3_helper_dqs_activate(int xiface, int index, bool min_pad)
+{
+	int  ipd_port,dq_base, dq_count, i;
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+
+	/* Get local IPD port for the interface */
+	ipd_port = cvmx_helper_get_ipd_port(xiface, index);
+	if(ipd_port < 0) {
+		cvmx_dprintf("%s: ERROR: No IPD port for interface %d port %d\n",
+			     __FUNCTION__, xiface, index);
+		return -1;
+	}
+
+	/* Get DQ# range for the IPD port */
+	dq_base = cvmx_pko3_get_queue_base(ipd_port);
+	dq_count = cvmx_pko3_get_queue_num(ipd_port);
+	if( dq_base < 0 || dq_count <= 0) {
+		cvmx_dprintf("%s: ERROR: No descriptor queues for interface %d port %d\n",
+			     __FUNCTION__, xiface, index);
+		return -1;
+	}
+
+	/* Mask out node from global DQ# */
+	dq_base &= (1<<10)-1;
+
+	for(i = 0; i < dq_count; i++) {
+		cvmx_pko_dq_open(xi.node, dq_base + i);
+		cvmx_pko3_dq_options(xi.node, dq_base + i, min_pad);
+	}
+
+
+	return i;
+}
+EXPORT_SYMBOL(__cvmx_pko3_helper_dqs_activate);
+
+/** Configure and initialize PKO3 for an interface
+ *
+ * @param node
+ * @param interface is the interface number to configure
+ * @return 0 on success.
+ *
+ */
+int cvmx_helper_pko3_init_interface(int xiface)
+{
+	cvmx_helper_interface_mode_t mode;
+	int subif, num_ports;
+	bool fcs_enable, pad_enable;
+	uint8_t fcs_sof_off = 0;
+	uint8_t num_queues = 1;
+	bool qos = false, pfc = false;
+	int res = -1;
+	cvmx_xiface_t xi = cvmx_helper_xiface_to_node_interface(xiface);
+
+	mode = cvmx_helper_interface_get_mode(xiface);
+	num_ports = cvmx_helper_interface_enumerate(xiface);
+	subif = 0;
+
+	if ((unsigned) xi.interface <
+		NUM_ELEMENTS(__cvmx_pko_queue_static_config
+			.pknd.pko_cfg_iface)) {
+		pfc = __cvmx_pko_queue_static_config
+			.pknd.pko_cfg_iface[xi.interface]
+			.pfc_enable;
+		num_queues =
+			__cvmx_pko_queue_static_config.
+			pknd.pko_cfg_iface[xi.interface]
+			.queues_per_port;
+		qos =
+			__cvmx_pko_queue_static_config.
+			pknd.pko_cfg_iface[xi.interface]
+			.qos_enable;
+	}
+
+	/* Skip non-existent interfaces */
+	if(num_ports < 1) {
+		cvmx_dprintf("%s: ERROR invalid interface %d\n",
+			     __FUNCTION__, xiface );
+		return -1;
+	}
+
+	if (mode == CVMX_HELPER_INTERFACE_MODE_LOOP) {
+		num_queues =
+			__cvmx_pko_queue_static_config.
+				pknd.pko_cfg_loop.queues_per_port;
+		qos =
+			__cvmx_pko_queue_static_config.
+				pknd.pko_cfg_loop.qos_enable;
+		res = __cvmx_pko3_config_chan_interface(xiface, num_ports,
+				num_queues, qos);
+		if (res < 0) {
+			goto __cfg_error;
+		}
+	}
+
+	else if (mode == CVMX_HELPER_INTERFACE_MODE_NPI) {
+		num_queues =
+			__cvmx_pko_queue_static_config.
+				pknd.pko_cfg_npi.queues_per_port;
+		qos =
+			__cvmx_pko_queue_static_config.
+				pknd.pko_cfg_npi.qos_enable;
+		res = __cvmx_pko3_config_chan_interface(xiface, num_ports,
+				num_queues, qos);
+		if (res < 0) {
+			goto __cfg_error;
+		}
+	}
+
+	/* Setup all ethernet configured for PFC */
+	else if (pfc) {
+		/* PFC interfaces have 8 prioritized queues */
+		for (subif = 0; subif < num_ports; subif++) {
+			res = __cvmx_pko3_config_pfc_interface(
+				xiface, subif);
+			if (res < 0)
+				goto __cfg_error;
+
+			/* Enable PFC/CBFC on BGX */
+			__cvmx_helper_bgx_xaui_config_pfc(xi.node,
+				xi.interface, subif, true);
+		}
+	}
+
+	/* All other interfaces follow static configuration */
+	else {
+
+		for (subif = 0; subif < num_ports; subif++) {
+			res = __cvmx_pko3_config_gen_interface(xiface, subif,
+				num_queues, qos);
+			if (res < 0) {
+				goto __cfg_error;
+			}
+		}
+	}
+
+	fcs_enable = __cvmx_helper_get_has_fcs(xiface);
+	pad_enable = __cvmx_helper_get_pko_padding(xiface);
+
+	if(debug)
+		cvmx_dprintf("%s: FCS=%d pad=%d\n",
+			__func__, fcs_enable, pad_enable);
+
+	/* Setup interface options */
+	for (subif = 0; subif < num_ports; subif++) {
+		/* Open interface/port DQs to allow transmission to begin */
+#ifdef	__PKO_MINPAD_FIXED
+		res = __cvmx_pko3_helper_dqs_activate(xiface,
+			subif, pad_enable);
+#else
+		/* PAD=false on all interfaces, it is broken */
+		res = __cvmx_pko3_helper_dqs_activate(xiface, subif, false);
+#endif
+		if (res < 0)
+			goto __cfg_error;
+
+		/* ILK has only one MAC, subif == logical-channel */
+		if (mode == CVMX_HELPER_INTERFACE_MODE_ILK && subif > 0)
+			continue;
+
+		/* LOOP has only one MAC, subif == logical-channel */
+		if (mode == CVMX_HELPER_INTERFACE_MODE_LOOP && subif > 0)
+			continue;
+
+		/* NPI has only one MAC, subif == 'ring' */
+		if (mode == CVMX_HELPER_INTERFACE_MODE_NPI && subif > 0)
+			continue;
+#ifdef	__PKO_MINPAD_FIXED
+		res = cvmx_pko3_interface_options(xiface, subif,
+				  fcs_enable, pad_enable, fcs_sof_off);
+#else
+		if (xi.interface > 5) {
+			/* Not BGX, use PKO FCS but not PAD as it is broken */
+			res = cvmx_pko3_interface_options(xiface, subif,
+				  fcs_enable,
+				/* pad_enable */false,
+				fcs_sof_off);
+		} else {
+			/* Use BGX feature because PKO NONPAD i sbroken */
+			res = cvmx_pko3_interface_options(xiface, subif,
+				  false, false, fcs_sof_off);
+			cvmx_helper_bgx_tx_options(xi.node, xi.interface, subif,
+				fcs_enable, pad_enable);
+		}
+#endif
+		if(res < 0)
+			cvmx_dprintf("%s: WARNING: failed to set options for interface %d subif %d\n",
+				     __func__, xiface, subif);
+	}
+	return 0;
+
+  __cfg_error:
+	cvmx_dprintf("%s: ERROR configuring interface %u subif %u\n",
+		     __FUNCTION__, xiface, subif);
+	return -1;
+}
+
+/**
+ * Global initialization for PKO3
+ *
+ * Should only be called once on each node
+ *
+ * TBD: Resolve the kernel case.
+ * When Linux eats up the entire memory, bootmem will be unable to
+ * satisfy our request, and the memory needs to come from Linux free pages.
+ */
+int __cvmx_helper_pko3_init_global(unsigned int node, uint16_t gaura)
+{
+	int res;
+
+	res = cvmx_pko3_hw_init_global(node, gaura);
+	if(res < 0) {
+		cvmx_dprintf("%s: ERROR: failed block initialization\n",
+			__FUNCTION__);
+		return res;
+	}
+
+	/* configure channel level */
+	cvmx_pko_setup_channel_credit_level(node,
+		cvmx_pko_default_channel_level);
+
+	/* add NULL MAC/DQ setup */
+	res = __cvmx_pko3_config_null_interface(node);
+	if (res < 0)
+		cvmx_dprintf("%s: ERROR creating NULL interface\n",
+			__FUNCTION__);
+
+	return res;
+}
+EXPORT_SYMBOL(__cvmx_helper_pko3_init_global);
+
+#ifdef	__PKO_HW_DEBUG
+#define	CVMX_DUMP_REGX(reg) cvmx_dprintf("%s=%#llx\n",#reg,(long long)cvmx_read_csr(reg))
+#define	CVMX_DUMP_REGD(reg) cvmx_dprintf("%s=%lld.\n",#reg,(long long)cvmx_read_csr(reg))
+/*
+ * function for debugging PKO reconfiguration
+ */
+void cvmx_fpa3_aura_dump_regs(unsigned node, uint16_t aura)
+	{
+	int pool_num =
+		cvmx_read_csr_node(node,CVMX_FPA_AURAX_POOL(aura));
+
+	CVMX_DUMP_REGX(CVMX_FPA_AURAX_POOL(aura));
+	CVMX_DUMP_REGX(CVMX_FPA_POOLX_CFG(pool_num));
+	CVMX_DUMP_REGX(CVMX_FPA_POOLX_OP_PC(pool_num));
+	CVMX_DUMP_REGX(CVMX_FPA_POOLX_INT(pool_num));
+	CVMX_DUMP_REGD(CVMX_FPA_POOLX_AVAILABLE(pool_num));
+	CVMX_DUMP_REGD(CVMX_FPA_POOLX_THRESHOLD(pool_num));
+	CVMX_DUMP_REGX(CVMX_FPA_AURAX_CFG(aura));
+	CVMX_DUMP_REGX(CVMX_FPA_AURAX_INT(aura));
+	CVMX_DUMP_REGD(CVMX_FPA_AURAX_CNT(aura));
+	CVMX_DUMP_REGD(CVMX_FPA_AURAX_CNT_LIMIT(aura));
+	CVMX_DUMP_REGX(CVMX_FPA_AURAX_CNT_THRESHOLD(aura));
+	CVMX_DUMP_REGX(CVMX_FPA_AURAX_CNT_LEVELS(aura));
+	CVMX_DUMP_REGX(CVMX_FPA_AURAX_POOL_LEVELS(aura));
+
+	}
+
+void cvmx_pko3_dump_regs(unsigned node)
+{
+	(void) node;
+	CVMX_DUMP_REGX( CVMX_PKO_NCB_INT );
+	CVMX_DUMP_REGX( CVMX_PKO_PEB_ERR_INT );
+	CVMX_DUMP_REGX( CVMX_PKO_PDM_ECC_DBE_STS_CMB0 );
+	CVMX_DUMP_REGX( CVMX_PKO_PDM_ECC_SBE_STS_CMB0 );
+	CVMX_DUMP_REGX( CVMX_PKO_PEB_ECC_DBE_STS0 );
+	CVMX_DUMP_REGX( CVMX_PKO_PEB_ECC_DBE_STS_CMB0 );
+}
+#endif	/* __PKO_HW_DEBUG */
+
+/**
+ * Global initialization for PKO3
+ *
+ * Should only be called once on each node
+ *
+ * When Linux eats up the entire memory, bootmem will be unable to
+ * satisfy our request, and the memory needs to come from Linux free pages.
+ */
+int cvmx_helper_pko3_init_global(unsigned int node)
+{
+	void *ptr;
+	int res = -1;
+	int16_t gaura, aura = -1, anode = node;
+
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+	/* Allocate memory required by PKO3 */
+	res = __cvmx_pko3_config_memory(node);
+#endif
+	if(res < 0) {
+		cvmx_dprintf("%s: ERROR: PKO3 memory allocation error\n",
+			__FUNCTION__);
+		return res;
+	}
+
+	gaura = res;
+	aura = res & ((1 << 10)-1);
+	anode = res >> 10;
+
+	/* Exercise the FPA to make sure the AURA is functional */
+	ptr = cvmx_fpa3_alloc_aura(anode, aura);
+
+	if (ptr == NULL )
+		res = -1;
+	else {
+		cvmx_fpa3_free_aura(ptr, anode, aura, 0);
+		res = 0;
+	}
+
+	if (res < 0) {
+		cvmx_dprintf("ERROR: %s: FPA failure AURA=%u:%#x\n",
+			__func__, anode, aura);
+		return -1;
+	}
+
+	res = __cvmx_helper_pko3_init_global(node, gaura);
+
+	if (res < 0)
+		cvmx_dprintf("ERROR: %s: failed to start PPKO\n",__func__);
+
+	return res;
+}
+
+/**
+ * Uninitialize PKO3 interface
+ *
+ * Release all resources held by PKO for an interface.
+ * The shutdown code is the same for all supported interfaces.
+ *
+ * NOTE: The NULL virtual interface is identified by interface
+ * number -1, which translates into IPD port 0xfff, MAC#28. [Kludge]
+ */
+int cvmx_helper_pko3_shut_interface(int xiface)
+{
+	int index, num_ports;
+	int dq_base, dq_count;
+	uint16_t ipd_port;
+	int i, res_owner, res;
+	uint64_t cycles;
+	const unsigned timeout = 10;	/* milliseconds */
+	cvmx_xiface_t xi = cvmx_helper_xiface_to_node_interface(xiface);
+
+	if(__cvmx_helper_xiface_is_null(xiface)) {
+		/* Special case for NULL interface */
+		num_ports = 1;
+	} else {
+		cvmx_helper_interface_mode_t mode;
+		mode = cvmx_helper_interface_get_mode(xiface);
+		num_ports = cvmx_helper_interface_enumerate(xiface);
+		(void) mode;
+	}
+
+	/* Skip non-existent interfaces silently */
+	if(num_ports < 1) {
+		return -1;
+	}
+
+	if (debug)
+		cvmx_dprintf("%s: xiface %u:%d ports %d\n",
+			__func__, xi.node, xi.interface , num_ports);
+
+	for (index = 0; index < num_ports; index ++) {
+
+		if (__cvmx_helper_xiface_is_null(xiface))
+			ipd_port = CVMX_PKO3_IPD_PORT_NULL;
+		else
+			ipd_port = cvmx_helper_get_ipd_port(xiface, index);
+
+		/* Retreive DQ range for the index */
+                dq_base = cvmx_pko3_get_queue_base(ipd_port);
+                dq_count = cvmx_pko3_get_queue_num(ipd_port);
+
+                if( dq_base < 0 || dq_count < 0) {
+                        cvmx_dprintf("%s: ERROR: No descriptor queues for interface %d index %d\n",
+                                __FUNCTION__, xiface, index);
+			continue;
+		}
+
+		/* Get rid of node-number in DQ# */
+		dq_base &= (1 << 10)-1;
+
+		if (debug)
+			cvmx_dprintf("%s: xiface %u:%d port %d dq %u-%u\n",
+			__func__, xi.node, xi.interface, index,
+			dq_base, dq_base + dq_count -1);
+
+		/* Unregister the DQs for the port, should stop traffic */
+		res = __cvmx_pko3_ipd_dq_unregister(xiface, index);
+		if(res < 0) {
+                        cvmx_dprintf("%s: ERROR: can not unregister queues for interface %d index %d\n",
+                                __FUNCTION__, xiface, index);
+			continue;
+		}
+
+		/* Begin draining all queues */
+		for(i = 0; i < dq_count; i++) {
+			cvmx_pko3_dq_drain(xi.node, dq_base + i);
+		}
+
+		/* Wait for all queues to drain, and close them */
+		for(i = 0; i < dq_count; i++) {
+			/* Prepare timeout */
+			cycles = cvmx_get_cycle();
+			cycles += cvmx_clock_get_rate(CVMX_CLOCK_CORE)/1000 * timeout;
+
+			/* Wait for queue to drain */
+			do {
+				res = cvmx_pko3_dq_query(xi.node, dq_base + i);
+				if (cycles < cvmx_get_cycle())
+					break;
+			} while(res > 0);
+
+			if (res != 0)
+				cvmx_dprintf("%s: ERROR: querying queue %u\n",
+					__FUNCTION__, dq_base + i);
+
+			/* Close the queue, free internal buffers */
+			res = cvmx_pko3_dq_close(xi.node, dq_base + i);
+
+			if (res < 0)
+				cvmx_dprintf("%s: ERROR: closing queue %u\n",
+					__FUNCTION__, dq_base + i);
+
+		}
+
+		/* Release all global resources owned by this interface/port */
+
+		res_owner = __cvmx_helper_pko3_res_owner(ipd_port);
+		if (res_owner < 0) {
+			cvmx_dprintf ("%s: ERROR no resource owner ticket\n",
+				__FUNCTION__);
+			continue;
+		}
+
+		cvmx_pko_free_queues(xi.node, CVMX_PKO_DESCR_QUEUES, res_owner);
+		cvmx_pko_free_queues(xi.node, CVMX_PKO_L5_QUEUES, res_owner);
+		cvmx_pko_free_queues(xi.node, CVMX_PKO_L4_QUEUES, res_owner);
+		cvmx_pko_free_queues(xi.node, CVMX_PKO_L3_QUEUES, res_owner);
+		cvmx_pko_free_queues(xi.node, CVMX_PKO_L2_QUEUES, res_owner);
+		cvmx_pko_free_queues(xi.node, CVMX_PKO_PORT_QUEUES, res_owner);
+
+	} /* for port */
+
+	return 0;
+}
+
+/**
+ * Shutdown PKO3
+ *
+ * Should be called after all interfaces have been shut down on the PKO3.
+ *
+ * Disables the PKO, frees all its buffers.
+ */
+int cvmx_helper_pko3_shutdown(unsigned int node)
+{
+	unsigned dq;
+	int res;
+
+	 /* destroy NULL interface here, only PKO knows about it */
+	cvmx_helper_pko3_shut_interface(cvmx_helper_node_interface_to_xiface(node, __CVMX_XIFACE_NULL));
+
+#ifdef	__PKO_DQ_CLOSE_ERRATA_FIXED
+	/* Check that all DQs are closed */
+	/* this seems to cause issue on HW:
+	 * the error code differs from expected
+	 */
+	for(dq =0; dq < (1<<10); dq++) {
+		res = cvmx_pko3_dq_close(node, dq);
+		if (res != 0) {
+			cvmx_dprintf("%s: ERROR: PKO3 descriptor queue %u "
+				"could not be closed\n",
+				__FUNCTION__, dq);
+			return -1;
+		}
+	}
+#endif
+	(void) dq;
+	res = cvmx_pko3_hw_disable(node);
+
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+	/* shut down AURA/POOL we created, and free its resources */
+	cvmx_helper_fpa3_shutdown_aura_and_pool(node, __cvmx_pko3_aura_num);
+#endif /* CVMX_BUILD_FOR_LINUX_KERNEL */
+	return res;
+}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.c b/arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.c
index 45f18cc..5b6867f 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.c
@@ -379,6 +379,11 @@ int __cvmx_helper_sgmii_enable(int interface)
 			       gmxx_prtx_cfg.u64);
 		__cvmx_interrupt_pcsx_intx_en_reg_enable(index, interface);
 	}
+      /* The common BGX settings are already done in the appropriate
+          enable functions, nothing to do here. */
+       if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+                return 0;
+
 	__cvmx_interrupt_pcsxx_int_en_reg_enable(interface);
 	__cvmx_interrupt_gmxx_enable(interface);
 	return 0;
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-util.c b/arch/mips/cavium-octeon/executive/cvmx-helper-util.c
index 453d7f6..8d50788 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-util.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-util.c
@@ -76,6 +76,8 @@ const char *cvmx_helper_interface_mode_to_string(cvmx_helper_interface_mode_t
 		return "NPI";
 	case CVMX_HELPER_INTERFACE_MODE_LOOP:
 		return "LOOP";
+	case CVMX_HELPER_INTERFACE_MODE_ILK:
+		return "ILK";
 	}
 	return "UNKNOWN";
 }
@@ -271,6 +273,11 @@ int __cvmx_helper_setup_gmx(int interface, int num_ports)
 	union cvmx_gmxx_txx_thresh gmx_tx_thresh;
 	int index;
 
+	/* The common BGX settings are already done in the appropriate
+           enable functions, nothing to do here. */
+        if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+                return 0;
+
 	/* Tell GMX the number of TX ports on this interface */
 	gmx_tx_prts.u64 = cvmx_read_csr(CVMX_GMXX_TX_PRTS(interface));
 	gmx_tx_prts.s.prts = num_ports;
@@ -363,6 +370,65 @@ int __cvmx_helper_setup_gmx(int interface, int num_ports)
 }
 
 /**
+ * @INTERNAL
+ * These are the interface types needed to convert interface numbers to ipd
+ * ports.
+ *
+ * @param GMII
+ *	This type is used for sgmii, rgmii, xaui and rxaui interfaces.
+ * @param ILK
+ *	This type is used for ilk interfaces.
+ * @param NPI
+ *	This type is used for npi interfaces.
+ * @param LB
+ *	This type is used for loopback interfaces.
+ * @param INVALID_IF_TYPE
+ *	This type indicates the interface hasn't been configured.
+ */
+enum port_map_if_type {
+	INVALID_IF_TYPE = 0,
+	GMII,
+};
+
+/**
+ * @INTERNAL
+ * This structure is used to map interface numbers to ipd ports.
+ *
+ * @param type
+ *	Interface type
+ * @param first_ipd_port
+ *	First IPD port number assigned to this interface.
+ * @param last_ipd_port
+ *	Last IPD port number assigned to this interface.
+ * @param ipd_port_adj
+ *	Different octeon chips require different ipd ports for the
+ *	same interface port/mode configuration. This value is used
+ *	to account for that difference.
+ */
+struct ipd_port_map {
+	enum port_map_if_type	type;
+	int			first_ipd_port;
+	int			last_ipd_port;
+	int			ipd_port_adj;
+};
+/**
+ * @INTERNAL
+ * Interface number to ipd port map for the octeon 78xx.
+ *
+ * This mapping corresponds to WQE(CHAN) enumeration in
+ * HRM Sections 11.15, MKI_CHAN_E, Section 11.6
+ *
+ */
+static const struct ipd_port_map ipd_port_map_78xx[CVMX_HELPER_MAX_IFACE] = {
+	{GMII,	0x800,	0x83f,	0x00},		/* Interface 0 - BGX0 */
+	{GMII,	0x900,	0x93f,	0x00},		/* Interface 1  -BGX1 */
+	{GMII,	0xa00,	0xa3f,	0x00},		/* Interface 2  -BGX2 */
+	{GMII,	0xb00,	0xb3f,	0x00},		/* Interface 3 - BGX3 */
+	{GMII,	0xc00,	0xc3f,	0x00},		/* Interface 4 - BGX4 */
+	{GMII,	0xd00,	0xd3f,	0x00},		/* Interface 5 - BGX5 */
+};
+
+/**
  * Returns the IPD/PKO port number for a port on the given
  * interface.
  *
@@ -371,22 +437,66 @@ int __cvmx_helper_setup_gmx(int interface, int num_ports)
  *
  * Returns IPD/PKO port number
  */
-int cvmx_helper_get_ipd_port(int interface, int port)
+int cvmx_helper_get_ipd_port(int interface, int index)
 {
-	switch (interface) {
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(interface);
+	if (octeon_has_feature(OCTEON_FEATURE_PKND)) {
+		const struct ipd_port_map	*port_map;
+		int				ipd_port;
+
+		if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+			port_map = ipd_port_map_78xx;
+			ipd_port = cvmx_helper_node_to_ipd_port(xi.node, 0);
+		}
+		else
+			return -1;
+
+		ipd_port += port_map[xi.interface].first_ipd_port;
+		if (port_map[xi.interface].type == GMII) {
+			cvmx_helper_interface_mode_t mode;
+			mode = cvmx_helper_interface_get_mode(interface);
+			if (mode == CVMX_HELPER_INTERFACE_MODE_XAUI) {
+				ipd_port += port_map[xi.interface].ipd_port_adj;
+				return ipd_port;
+			} else
+				return ipd_port + (index * 16);
+		} else
+			return -1;
+	}
+
+	switch (xi.interface) {
 	case 0:
-		return port;
+		return index;
 	case 1:
-		return port + 16;
+		return index + 16;
 	case 2:
-		return port + 32;
+		return index + 32;
 	case 3:
-		return port + 36;
+		return index + 36;
+	case 4:
+		return index + 40;
+	case 5:
+		return index + 42;
+	case 6:
+		return index + 44;
+	case 7:
+		return index + 46;
 	}
 	return -1;
 }
 EXPORT_SYMBOL_GPL(cvmx_helper_get_ipd_port);
 
+extern int __cvmx_helper_cfg_pknd(int interface, int index);
+
+int cvmx_helper_get_pknd(int xiface, int index)
+{
+        if (octeon_has_feature(OCTEON_FEATURE_PKND))
+                return __cvmx_helper_cfg_pknd(xiface, index);
+
+        return CVMX_INVALID_PKND;
+}
+EXPORT_SYMBOL(cvmx_helper_get_pknd);
+
 /**
  * Returns the interface number for an IPD/PKO port number.
  *
@@ -396,18 +506,37 @@ EXPORT_SYMBOL_GPL(cvmx_helper_get_ipd_port);
  */
 int cvmx_helper_get_interface_num(int ipd_port)
 {
-	if (ipd_port < 16)
-		return 0;
-	else if (ipd_port < 32)
-		return 1;
-	else if (ipd_port < 36)
-		return 2;
-	else if (ipd_port < 40)
-		return 3;
-	else
-		cvmx_dprintf("cvmx_helper_get_interface_num: Illegal IPD "
-			     "port number\n");
-
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+		const struct ipd_port_map	*port_map;
+		int				i;
+		struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(ipd_port);
+		port_map = ipd_port_map_78xx;
+		for (i = 0; i < CVMX_HELPER_MAX_IFACE; i++) {
+			if (xp.port >= port_map[i].first_ipd_port &&
+			    xp.port <= port_map[i].last_ipd_port)
+				return cvmx_helper_node_interface_to_xiface(xp.node, i);
+		}
+		return -1;
+	} else {
+		if (ipd_port < 16)
+			return 0;
+		else if (ipd_port < 32)
+			return 1;
+		else if (ipd_port < 36)
+			return 2;
+		else if (ipd_port < 40)
+			return 3;
+		else if (ipd_port < 42)
+			return 4;
+		else if (ipd_port < 44)
+			return 5;
+		else if (ipd_port < 46)
+			return 6;
+		else if (ipd_port < 48)
+			return 7;
+	}
+	cvmx_dprintf("cvmx_helper_get_interface_num: Illegal IPD port number %d\n",
+		     ipd_port);
 	return -1;
 }
 EXPORT_SYMBOL_GPL(cvmx_helper_get_interface_num);
@@ -422,16 +551,57 @@ EXPORT_SYMBOL_GPL(cvmx_helper_get_interface_num);
  */
 int cvmx_helper_get_interface_index_num(int ipd_port)
 {
+	if (octeon_has_feature(OCTEON_FEATURE_PKND)) {
+		const struct ipd_port_map	*port_map;
+		int				port;
+		enum port_map_if_type		type = INVALID_IF_TYPE;
+		int				i;
+		int				num_interfaces;
+
+		if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+			port_map = ipd_port_map_78xx;
+		else
+			return -1;
+
+		num_interfaces = cvmx_helper_get_number_of_interfaces();
+
+		/* Get the interface type of the ipd port */
+		for (i = 0; i < num_interfaces; i++) {
+			if (ipd_port >= port_map[i].first_ipd_port &&
+			    ipd_port <= port_map[i].last_ipd_port) {
+				type = port_map[i].type;
+				break;
+			}
+		}
+
+		/* Convert the ipd port to the interface port */
+		switch (type) {
+		case GMII:
+			port = ((ipd_port & 0xff) >> 6);
+			return port ? (port - 1) : ((ipd_port & 0xff) >> 4);
+			break;
+
+		default:
+			cvmx_dprintf("cvmx_helper_get_interface_index_num: "
+				     "Illegal IPD port number %d\n", ipd_port);
+			return -1;
+		}
+	}
 	if (ipd_port < 32)
 		return ipd_port & 15;
-	else if (ipd_port < 36)
-		return ipd_port & 3;
 	else if (ipd_port < 40)
 		return ipd_port & 3;
+	else if (ipd_port < 48)
+		return ipd_port & 1;
 	else
-		cvmx_dprintf("cvmx_helper_get_interface_index_num: "
-			     "Illegal IPD port number\n");
+		cvmx_dprintf("cvmx_helper_get_interface_index_num: Illegal IPD port number\n");
 
 	return -1;
 }
 EXPORT_SYMBOL_GPL(cvmx_helper_get_interface_index_num);
+
+int cvmx_helper_get_pko_port(int interface, int port)
+{
+        return cvmx_pko_get_base_pko_port(interface, port);
+}
+EXPORT_SYMBOL(cvmx_helper_get_pko_port);
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper.c b/arch/mips/cavium-octeon/executive/cvmx-helper.c
index 8553ad5..7052b31 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper.c
@@ -1,77 +1,548 @@
 /***********************license start***************
- * Author: Cavium Networks
+ * Copyright (c) 2003-2014  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
  *
- * Contact: support@caviumnetworks.com
- * This file is part of the OCTEON SDK
  *
- * Copyright (c) 2003-2008 Cavium Networks
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
  *
- * This file is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, Version 2, as
- * published by the Free Software Foundation.
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
  *
- * This file is distributed in the hope that it will be useful, but
- * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
- * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
- * NONINFRINGEMENT.  See the GNU General Public License for more
- * details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this file; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- * or visit http://www.gnu.org/licenses/.
- *
- * This file may also be available under a different license from Cavium.
- * Contact Cavium Networks for more information
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
  ***********************license end**************************************/
 
-/*
+/**
+ * @file
  *
  * Helper functions for common, but complicated tasks.
  *
  */
-#include <asm/octeon/octeon.h>
-
-#include <asm/octeon/cvmx-config.h>
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include <linux/slab.h>
+#include <linux/export.h>
+#include <asm/octeon/cvmx.h>
+#include <asm/octeon/cvmx-bootmem.h>
+#include <asm/octeon/cvmx-bgxx-defs.h>
+#include <asm/octeon/cvmx-sriox-defs.h>
+#include <asm/octeon/cvmx-npi-defs.h>
+#include <asm/octeon/cvmx-mio-defs.h>
+#include <asm/octeon/cvmx-pcsx-defs.h>
+#include <asm/octeon/cvmx-pexp-defs.h>
+#include <asm/octeon/cvmx-pip-defs.h>
+#include <asm/octeon/cvmx-asxx-defs.h>
+#include <asm/octeon/cvmx-gmxx-defs.h>
+#include <asm/octeon/cvmx-smix-defs.h>
+#include <asm/octeon/cvmx-dbg-defs.h>
+#include <asm/octeon/cvmx-sso-defs.h>
 
-#include <asm/octeon/cvmx-fpa.h>
+#include <asm/octeon/cvmx-fpa1.h>
 #include <asm/octeon/cvmx-pip.h>
-#include <asm/octeon/cvmx-pko.h>
+#include <asm/octeon/cvmx-pko3.h>
 #include <asm/octeon/cvmx-ipd.h>
+#include <asm/octeon/cvmx-qlm.h>
 #include <asm/octeon/cvmx-spi.h>
+#include <asm/octeon/cvmx-clock.h>
 #include <asm/octeon/cvmx-helper.h>
+#include <asm/octeon/cvmx-helper-bgx.h>
 #include <asm/octeon/cvmx-helper-board.h>
+#include <asm/octeon/cvmx-helper-errata.h>
+#include <asm/octeon/cvmx-helper-cfg.h>
+#include <asm/octeon/cvmx-helper-pki.h>
+#include <asm/octeon/cvmx-pki.h>
+#include <asm/octeon/cvmx-helper-pko.h>
+#include <asm/octeon/cvmx-helper-pko3.h>
+#else
+#include "cvmx.h"
+#include "cvmx-sysinfo.h"
+#include "cvmx-bootmem.h"
+#include "cvmx-version.h"
+#include "cvmx-gmx.h"
+#include "cvmx-error.h"
+
+#include "cvmx-fpa.h"
+#include "cvmx-pip.h"
+#include "cvmx-pko.h"
+#include "cvmx-pko3.h"
+#include "cvmx-ipd.h"
+#include "cvmx-qlm.h"
+#include "cvmx-spi.h"
+#include "cvmx-helper.h"
+#include "cvmx-helper-bgx.h"
+#include "cvmx-helper-board.h"
+#include "cvmx-helper-errata.h"
+#include "cvmx-helper-cfg.h"
+#include "cvmx-helper-pki.h"
+#include "cvmx-pki.h"
+#include "cvmx-helper-pko.h"
+#include "cvmx-helper-pko3.h"
+#include "cvmx-helper-ipd.h"
+#endif
 
-#include <asm/octeon/cvmx-pip-defs.h>
-#include <asm/octeon/cvmx-smix-defs.h>
-#include <asm/octeon/cvmx-asxx-defs.h>
 
 /**
- * cvmx_override_pko_queue_priority(int ipd_port, uint64_t
- * priorities[16]) is a function pointer. It is meant to allow
- * customization of the PKO queue priorities based on the port
- * number. Users should set this pointer to a function before
- * calling any cvmx-helper operations.
+ * @INTERNAL
+ * This structure specifies the interface methods used by an interface.
+ *
+ * @param mode		Interface mode.
+ *
+ * @param enumerate	Method the get number of interface ports.
+ *
+ * @param probe		Method to probe an interface to get the number of
+ *			connected ports.
+ *
+ * @param enable	Method to enable an interface
+ *
+ * @param link_get	Method to get the state of an interface link.
+ *
+ * @param link_set	Method to configure an interface link to the specified
+ *			state.
+ *
+ * @param loopback	Method to configure a port in loopback.
  */
-void (*cvmx_override_pko_queue_priority) (int pko_port,
-					  uint64_t priorities[16]);
+struct iface_ops {
+	cvmx_helper_interface_mode_t	mode;
+	int				(*enumerate)(int xiface);
+	int				(*probe)(int xiface);
+	int				(*enable)(int xiface);
+	cvmx_helper_link_info_t		(*link_get)(int ipd_port);
+	int				(*link_set)(int ipd_port,
+					     cvmx_helper_link_info_t link_info);
+	int				(*loopback)(int ipd_port,
+						    int en_in, int en_ex);
+};
 
 /**
- * cvmx_override_ipd_port_setup(int ipd_port) is a function
- * pointer. It is meant to allow customization of the IPD port
- * setup before packet input/output comes online. It is called
- * after cvmx-helper does the default IPD configuration, but
- * before IPD is enabled. Users should set this pointer to a
- * function before calling any cvmx-helper operations.
+ * @INTERNAL
+ * This structure is used by disabled interfaces.
+ */
+static const struct iface_ops iface_ops_dis = {
+	.mode		= CVMX_HELPER_INTERFACE_MODE_DISABLED,
+};
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as gmii.
+ */
+static const struct iface_ops iface_ops_gmii = {
+	.mode		= CVMX_HELPER_INTERFACE_MODE_GMII,
+	.enumerate	= __cvmx_helper_rgmii_probe,
+	.probe		= __cvmx_helper_rgmii_probe,
+	.enable		= __cvmx_helper_rgmii_enable,
+	.link_set	= __cvmx_helper_rgmii_link_set,
+	.loopback	= __cvmx_helper_rgmii_configure_loopback,
+};
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as rgmii.
+ */
+static const struct iface_ops iface_ops_rgmii = {
+	.mode		= CVMX_HELPER_INTERFACE_MODE_RGMII,
+	.enumerate	= __cvmx_helper_rgmii_probe,
+	.probe		= __cvmx_helper_rgmii_probe,
+	.enable		= __cvmx_helper_rgmii_enable,
+	.link_get	= __cvmx_helper_rgmii_link_get,
+	.link_set	= __cvmx_helper_rgmii_link_set,
+	.loopback	= __cvmx_helper_rgmii_configure_loopback,
+};
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as sgmii that use the gmx mac.
+ */
+static const struct iface_ops iface_ops_sgmii = {
+	.mode		= CVMX_HELPER_INTERFACE_MODE_SGMII,
+	.enumerate	= __cvmx_helper_sgmii_enumerate,
+	.probe		= __cvmx_helper_sgmii_probe,
+	.enable		= __cvmx_helper_sgmii_enable,
+	.link_get	= __cvmx_helper_sgmii_link_get,
+	.link_set	= __cvmx_helper_sgmii_link_set,
+	.loopback	= __cvmx_helper_sgmii_configure_loopback,
+};
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as sgmii that use the bgx mac.
+ */
+static const struct iface_ops iface_ops_bgx_sgmii = {
+	.mode		= CVMX_HELPER_INTERFACE_MODE_SGMII,
+	.enumerate	= __cvmx_helper_bgx_enumerate,
+	.probe		= __cvmx_helper_bgx_probe,
+	.enable		= __cvmx_helper_bgx_sgmii_enable,
+	.link_get	= __cvmx_helper_bgx_sgmii_link_get,
+	.link_set	= __cvmx_helper_bgx_sgmii_link_set,
+	.loopback	= __cvmx_helper_bgx_sgmii_configure_loopback,
+};
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as qsgmii.
+ */
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as xaui using the gmx mac.
+ */
+static const struct iface_ops iface_ops_xaui = {
+	.mode		= CVMX_HELPER_INTERFACE_MODE_XAUI,
+	.enumerate	= __cvmx_helper_xaui_enumerate,
+	.probe		= __cvmx_helper_xaui_probe,
+	.enable		= __cvmx_helper_xaui_enable,
+	.link_get	= __cvmx_helper_xaui_link_get,
+	.link_set	= __cvmx_helper_xaui_link_set,
+	.loopback	= __cvmx_helper_xaui_configure_loopback,
+};
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as xaui using the gmx mac.
+ */
+static const struct iface_ops iface_ops_bgx_xaui = {
+	.mode		= CVMX_HELPER_INTERFACE_MODE_XAUI,
+	.enumerate	= __cvmx_helper_bgx_enumerate,
+	.probe		= __cvmx_helper_bgx_probe,
+	.enable		= __cvmx_helper_bgx_xaui_enable,
+	.link_get	= __cvmx_helper_bgx_xaui_link_get,
+	.link_set	= __cvmx_helper_bgx_xaui_link_set,
+	.loopback	= __cvmx_helper_bgx_xaui_configure_loopback,
+};
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as rxaui.
+ */
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as xaui using the gmx mac.
+ */
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as xfi.
+ */
+
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as ilk.
+ */
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as npi.
+ */
+static const struct iface_ops iface_ops_npi = {
+	.mode		= CVMX_HELPER_INTERFACE_MODE_NPI,
+	.enumerate	= __cvmx_helper_npi_probe,
+	.probe		= __cvmx_helper_npi_probe,
+	.enable		= __cvmx_helper_npi_enable,
+};
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as srio.
+ */
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as agl.
+ */
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as picmg.
+ */
+static const struct iface_ops iface_ops_picmg = {
+	.mode		= CVMX_HELPER_INTERFACE_MODE_PICMG,
+	.enumerate	= __cvmx_helper_sgmii_enumerate,
+	.probe		= __cvmx_helper_sgmii_probe,
+	.enable		= __cvmx_helper_sgmii_enable,
+	.link_get	= __cvmx_helper_sgmii_link_get,
+	.link_set	= __cvmx_helper_sgmii_link_set,
+	.loopback	= __cvmx_helper_sgmii_configure_loopback,
+};
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as spi.
  */
-void (*cvmx_override_ipd_port_setup) (int ipd_port);
 
-/* Port count per interface */
-static int interface_port_count[5];
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
+ * configured as loop.
+ */
+static const struct iface_ops iface_ops_loop = {
+	.mode		= CVMX_HELPER_INTERFACE_MODE_LOOP,
+	.probe		= __cvmx_helper_loop_probe,
+};
+
+CVMX_SHARED const struct iface_ops *iface_node_ops[CVMX_MAX_NODES][CVMX_HELPER_MAX_IFACE];
+#define iface_ops iface_node_ops[0]
+
+struct cvmx_iface {
+	int cvif_ipd_nports;
+	int cvif_has_fcs;	/* PKO fcs for this interface. */
+	enum cvmx_pko_padding cvif_padding;
+	cvmx_helper_link_info_t *cvif_ipd_port_link_info;
+};
+
+/*
+ * This has to be static as u-boot expects to probe an interface and
+ * gets the number of its ports.
+ */
+static CVMX_SHARED struct cvmx_iface cvmx_interfaces[CVMX_MAX_NODES][CVMX_HELPER_MAX_IFACE];
+
+
+
+int __cvmx_helper_get_num_ipd_ports(int xiface)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	struct cvmx_iface *piface;
+
+	if (xi.interface >= cvmx_helper_get_number_of_interfaces())
+		return -1;
+
+	piface = &cvmx_interfaces[xi.node][xi.interface];
+	return piface->cvif_ipd_nports;
+}
+
+enum cvmx_pko_padding __cvmx_helper_get_pko_padding(int xiface)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	struct cvmx_iface *piface;
+
+	if (xi.interface >= cvmx_helper_get_number_of_interfaces())
+		return CVMX_PKO_PADDING_NONE;
+
+	piface = &cvmx_interfaces[xi.node][xi.interface];
+	return piface->cvif_padding;
+}
+EXPORT_SYMBOL(__cvmx_helper_get_pko_padding);
+
+int __cvmx_helper_init_interface(int xiface, int num_ipd_ports,
+				 int has_fcs, enum cvmx_pko_padding pad)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	struct cvmx_iface *piface;
+	cvmx_helper_link_info_t *p;
+	int i;
+	int sz;
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+	uint64_t addr;
+	char name[32];
+#endif
+
+	if (xi.interface >= cvmx_helper_get_number_of_interfaces())
+		return -1;
+
+	piface = &cvmx_interfaces[xi.node][xi.interface];
+	piface->cvif_ipd_nports = num_ipd_ports;
+	piface->cvif_padding = pad;
+
+	piface->cvif_has_fcs = has_fcs;
+
+	/*
+	 * allocate the per-ipd_port link_info structure
+	 */
+	sz = piface->cvif_ipd_nports * sizeof(cvmx_helper_link_info_t);
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+	if (sz == 0)
+		sz = sizeof(cvmx_helper_link_info_t);
+	piface->cvif_ipd_port_link_info = (cvmx_helper_link_info_t *) kmalloc(sz, GFP_KERNEL);
+	if (ZERO_OR_NULL_PTR(piface->cvif_ipd_port_link_info))
+		panic("Cannot allocate memory in __cvmx_helper_init_interface.");
+#else
+	snprintf(name, sizeof(name), "__int_%d_link_info", xi.interface);
+	addr = CAST64(cvmx_bootmem_alloc_named_range_once(sz, 0, 0, 128, name, NULL));
+	piface->cvif_ipd_port_link_info = (cvmx_helper_link_info_t *) __cvmx_phys_addr_to_ptr(addr, sz);
+#endif
+	if (!piface->cvif_ipd_port_link_info) {
+		if (sz != 0)
+			cvmx_dprintf("iface %d failed to alloc link info\n",
+				     xi.interface);
+		return -1;
+	}
+
+	/* Initialize them */
+	p = piface->cvif_ipd_port_link_info;
+
+	for (i = 0; i < piface->cvif_ipd_nports; i++) {
+		(*p).u64 = 0;
+		p++;
+	}
+	return 0;
+}
+
+
+/*
+ * Shut down the interfaces; free the resources.
+ * @INTERNAL
+ */
+void __cvmx_helper_shutdown_interfaces_node(unsigned int node)
+{
+	int i;
+	int nifaces;		/* number of interfaces */
+	struct cvmx_iface *piface;
+
+	nifaces = cvmx_helper_get_number_of_interfaces();
+	for (i = 0; i < nifaces; i++) {
+		piface = &cvmx_interfaces[node][i];
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+		if (piface->cvif_ipd_port_link_info)
+			kfree(piface->cvif_ipd_port_link_info);
+#elif defined(__U_BOOT__) && 0
+		if (piface->cvif_ipd_port_link_info) {
+			char name[32];
+			snprintf(name, sizeof(name),
+				 "__int_%d_link_info", i);
+			cvmx_bootmem_phy_named_block_free(name, 0);
+		}
+#else
+		/*
+		 * For SE apps, bootmem was meant to be allocated and never
+		 * freed.
+		 */
+#endif
+		piface->cvif_ipd_port_link_info = 0;
+	}
+}
+
+void __cvmx_helper_shutdown_interfaces(void)
+{
+	unsigned int node = cvmx_get_node_num();
+	__cvmx_helper_shutdown_interfaces_node(node);
+}
+
+int __cvmx_helper_set_link_info(int xiface, int index, cvmx_helper_link_info_t link_info)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	struct cvmx_iface *piface;
+
+	if (xi.interface >= cvmx_helper_get_number_of_interfaces())
+		return -1;
+
+	piface = &cvmx_interfaces[xi.node][xi.interface];
+
+	if (piface->cvif_ipd_port_link_info) {
+		piface->cvif_ipd_port_link_info[index] = link_info;
+		return 0;
+	}
+
+	return -1;
+}
+
+cvmx_helper_link_info_t __cvmx_helper_get_link_info(int xiface, int port)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	struct cvmx_iface *piface;
+	cvmx_helper_link_info_t err;
+
+	err.u64 = 0;
+
+	if (xi.interface >= cvmx_helper_get_number_of_interfaces())
+		return err;
+	piface = &cvmx_interfaces[xi.node][xi.interface];
+
+	if (piface->cvif_ipd_port_link_info)
+		return piface->cvif_ipd_port_link_info[port];
+
+	return err;
+}
+
+/**
+ * Returns if FCS is enabled for the specified interface and port
+ *
+ * @param interface - interface to check
+ *
+ * @return zero if FCS is not used, otherwise FCS is used.
+ */
+int __cvmx_helper_get_has_fcs(int xiface)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	return cvmx_interfaces[xi.node][xi.interface].cvif_has_fcs;
+}
+EXPORT_SYMBOL(__cvmx_helper_get_has_fcs);
+
+CVMX_SHARED uint64_t  cvmx_rgmii_backpressure_dis = 1;
+
+typedef int (*cvmx_export_config_t)(void);
+cvmx_export_config_t cvmx_export_app_config;
+
+void cvmx_rgmii_set_back_pressure(uint64_t backpressure_dis)
+{
+	cvmx_rgmii_backpressure_dis = backpressure_dis;
+}
+
+/*
+ * internal functions that are not exported in the .h file but must be
+ * declared to make gcc happy.
+ */
+extern cvmx_helper_link_info_t __cvmx_helper_get_link_info(int interface, int port);
+
+/**
+ * cvmx_override_iface_phy_mode(int interface, int index) is a function pointer.
+ * It is meant to allow customization of interfaces which do not have a PHY.
+ *
+ * @returns 0 if MAC decides TX_CONFIG_REG or 1 if PHY decides  TX_CONFIG_REG.
+ *
+ * If this function pointer is NULL then it defaults to the MAC.
+ */
+CVMX_SHARED int (*cvmx_override_iface_phy_mode) (int interface, int index);
+EXPORT_SYMBOL(cvmx_override_iface_phy_mode);
 
-/* Port last configured link info index by IPD/PKO port */
-static cvmx_helper_link_info_t
-    port_link_info[CVMX_PIP_NUM_INPUT_PORTS];
+/**
+ * cvmx_override_ipd_port_setup(int ipd_port) is a function
+ * pointer. It is meant to allow customization of the IPD
+ * port/port kind setup before packet input/output comes online.
+ * It is called after cvmx-helper does the default IPD configuration,
+ * but before IPD is enabled. Users should set this pointer to a
+ * function before calling any cvmx-helper operations.
+ */
+CVMX_SHARED void (*cvmx_override_ipd_port_setup) (int ipd_port) = NULL;
 
 /**
  * Return the number of interfaces the chip has. Each interface
@@ -79,59 +550,396 @@ static cvmx_helper_link_info_t
  * but the CNX0XX and CNX1XX are exceptions. These only support
  * one interface.
  *
- * Returns Number of interfaces on chip
+ * @return Number of interfaces on chip
  */
 int cvmx_helper_get_number_of_interfaces(void)
 {
-	if (OCTEON_IS_MODEL(OCTEON_CN56XX) || OCTEON_IS_MODEL(OCTEON_CN52XX))
+
+	if (OCTEON_IS_MODEL(OCTEON_CN68XX))
+		return 9;
+	else if (OCTEON_IS_MODEL(OCTEON_CN66XX))
+		if (OCTEON_IS_MODEL(OCTEON_CN66XX_PASS1_0))
+			return 7;
+		else
+			return 8;
+	else if (OCTEON_IS_MODEL(OCTEON_CN63XX))
+		return 6;
+	else if (OCTEON_IS_MODEL(OCTEON_CN56XX) ||
+		 OCTEON_IS_MODEL(OCTEON_CN52XX) ||
+		 OCTEON_IS_MODEL(OCTEON_CN61XX) ||
+		 OCTEON_IS_MODEL(OCTEON_CNF71XX))
 		return 4;
+	else if (OCTEON_IS_MODEL(OCTEON_CN70XX))
+		return 5;
+	else if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+		return 10;
 	else
 		return 3;
 }
-EXPORT_SYMBOL_GPL(cvmx_helper_get_number_of_interfaces);
+EXPORT_SYMBOL(cvmx_helper_get_number_of_interfaces);
+
+int __cvmx_helper_early_ports_on_interface(int interface)
+{
+	if (octeon_has_feature(OCTEON_FEATURE_PKND))
+		return cvmx_helper_interface_enumerate(interface);
+	else {
+		int ports = cvmx_helper_interface_enumerate(interface);
+		ports = __cvmx_helper_board_interface_probe(interface, ports);
+		return ports;
+	}
+}
+EXPORT_SYMBOL(__cvmx_helper_early_ports_on_interface);
 
 /**
  * Return the number of ports on an interface. Depending on the
  * chip and configuration, this can be 1-16. A value of 0
  * specifies that the interface doesn't exist or isn't usable.
  *
- * @interface: Interface to get the port count for
+ * @param interface Interface to get the port count for
  *
- * Returns Number of ports on interface. Can be Zero.
+ * @return Number of ports on interface. Can be Zero.
  */
 int cvmx_helper_ports_on_interface(int interface)
 {
-	return interface_port_count[interface];
+	if (octeon_has_feature(OCTEON_FEATURE_PKND))
+		return cvmx_helper_interface_enumerate(interface);
+	else
+		return __cvmx_helper_get_num_ipd_ports(interface);
+}
+EXPORT_SYMBOL(cvmx_helper_ports_on_interface);
+
+/**
+ * @INTERNAL
+ * Return interface mode for CN70XX.
+ */
+static cvmx_helper_interface_mode_t __cvmx_get_mode_cn70xx(int interface)
+{
+	/* SGMII/RXAUI/QSGMII */
+	if (interface < 2) {
+		enum cvmx_qlm_mode qlm_mode = cvmx_qlm_get_dlm_mode(0, interface);
+
+		if (qlm_mode == CVMX_QLM_MODE_SGMII)
+			iface_ops[interface] = &iface_ops_sgmii;
+		else
+			iface_ops[interface] = &iface_ops_dis;
+	}
+	else if (interface == 2) /* DPI */
+		iface_ops[interface] = &iface_ops_npi;
+	else if (interface == 3) /* LOOP */
+		iface_ops[interface] = &iface_ops_loop;
+	else
+		iface_ops[interface] = &iface_ops_dis;
+
+	return iface_ops[interface]->mode;
+}
+
+/**
+ * @INTERNAL
+ * Return interface mode for CN78XX.
+ */
+static cvmx_helper_interface_mode_t __cvmx_get_mode_cn78xx(int xiface)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	/* SGMII/RXAUI/XAUI */
+	if (xi.interface < 6) {
+		int qlm = cvmx_qlm_interface(xiface);
+		enum cvmx_qlm_mode qlm_mode;
+
+		if (qlm == -1) {
+			iface_node_ops[xi.node][xi.interface] = &iface_ops_dis;
+			return iface_node_ops[xi.node][xi.interface]->mode;
+		}
+		qlm_mode = cvmx_qlm_get_mode_cn78xx(xi.node, qlm);
+
+		if (qlm_mode == CVMX_QLM_MODE_SGMII)
+			iface_node_ops[xi.node][xi.interface] = &iface_ops_bgx_sgmii;
+		else if (qlm_mode == CVMX_QLM_MODE_XAUI)
+			iface_node_ops[xi.node][xi.interface] = &iface_ops_bgx_xaui;
+		else
+			iface_node_ops[xi.node][xi.interface] = &iface_ops_dis;
+	} else if (xi.interface < 8) {
+		enum cvmx_qlm_mode qlm_mode;
+		if (xi.interface == 6) {
+			qlm_mode = cvmx_qlm_get_mode_cn78xx(xi.node, 4);
+				iface_node_ops[xi.node][xi.interface] = &iface_ops_dis;
+		} else if (xi.interface == 7) {
+			qlm_mode = cvmx_qlm_get_mode_cn78xx(xi.node, 5);
+				iface_node_ops[xi.node][xi.interface] = &iface_ops_dis;
+		}
+	} else if (xi.interface == 8) { /* DPI */
+		iface_node_ops[xi.node][xi.interface] = &iface_ops_npi;
+	} else if (xi.interface == 9) { /* LOOP */
+		iface_node_ops[xi.node][xi.interface] = &iface_ops_loop;
+	} else
+		iface_node_ops[xi.node][xi.interface] = &iface_ops_dis;
+
+	return iface_node_ops[xi.node][xi.interface]->mode;
+}
+
+/**
+ * @INTERNAL
+ * Return interface mode for CN68xx.
+ */
+static cvmx_helper_interface_mode_t __cvmx_get_mode_cn68xx(int interface)
+{
+	union cvmx_mio_qlmx_cfg qlm_cfg;
+
+	switch (interface) {
+	case 0:
+		qlm_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(0));
+		/* QLM is disabled when QLM SPD is 15. */
+		if (qlm_cfg.s.qlm_spd == 15)
+			iface_ops[interface] = &iface_ops_dis;
+		else if (qlm_cfg.s.qlm_cfg == 2)
+			iface_ops[interface] = &iface_ops_sgmii;
+		else if (qlm_cfg.s.qlm_cfg == 3)
+			iface_ops[interface] = &iface_ops_xaui;
+		else
+			iface_ops[interface] = &iface_ops_dis;
+		break;
+
+	case 1:
+		qlm_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(0));
+		/* QLM is disabled when QLM SPD is 15. */
+		if (qlm_cfg.s.qlm_spd == 15)
+			iface_ops[interface] = &iface_ops_dis;
+		else
+			iface_ops[interface] = &iface_ops_dis;
+		break;
+
+	case 2:
+	case 3:
+	case 4:
+		qlm_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(interface));
+		/* QLM is disabled when QLM SPD is 15. */
+		if (qlm_cfg.s.qlm_spd == 15)
+			iface_ops[interface] = &iface_ops_dis;
+		else if (qlm_cfg.s.qlm_cfg == 2)
+			iface_ops[interface] = &iface_ops_sgmii;
+		else if (qlm_cfg.s.qlm_cfg == 3)
+			iface_ops[interface] = &iface_ops_xaui;
+		else
+			iface_ops[interface] = &iface_ops_dis;
+		break;
+
+	case 5:
+	case 6:
+		qlm_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(interface - 4));
+		/* QLM is disabled when QLM SPD is 15. */
+		if (qlm_cfg.s.qlm_spd == 15)
+			iface_ops[interface] = &iface_ops_dis;
+		else
+			iface_ops[interface] = &iface_ops_dis;
+		break;
+
+	case 7:
+		qlm_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(3));
+		/* QLM is disabled when QLM SPD is 15. */
+		if (qlm_cfg.s.qlm_spd == 15) {
+			iface_ops[interface] = &iface_ops_dis;
+		} else if (qlm_cfg.s.qlm_cfg != 0) {
+			qlm_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(1));
+			if (qlm_cfg.s.qlm_cfg != 0)
+				iface_ops[interface] = &iface_ops_dis;
+			else
+				iface_ops[interface] = &iface_ops_npi;
+		}
+		else
+			iface_ops[interface] = &iface_ops_npi;
+		break;
+
+	case 8:
+		iface_ops[interface] = &iface_ops_loop;
+		break;
+
+	default:
+		iface_ops[interface] = &iface_ops_dis;
+		break;
+	}
+
+	return iface_ops[interface]->mode;
+}
+
+/**
+ * @INTERNAL
+ * Return interface mode for an Octeon II
+ */
+static cvmx_helper_interface_mode_t __cvmx_get_mode_octeon2(int interface)
+{
+	union cvmx_gmxx_inf_mode mode;
+
+	if (OCTEON_IS_MODEL(OCTEON_CN68XX))
+		return __cvmx_get_mode_cn68xx(interface);
+
+	if (interface == 2)
+		iface_ops[interface] = &iface_ops_npi;
+	else if (interface == 3)
+		iface_ops[interface] = &iface_ops_loop;
+	else if ((OCTEON_IS_MODEL(OCTEON_CN63XX) &&
+		  (interface == 4 || interface == 5)) ||
+		 (OCTEON_IS_MODEL(OCTEON_CN66XX) &&
+		  interface >= 4 && interface <= 7)) {
+		/* Only present in CN63XX & CN66XX Octeon model */
+		union cvmx_sriox_status_reg sriox_status_reg;
+
+		/* cn66xx pass1.0 has only 2 SRIO interfaces. */
+		if ((interface == 5 || interface == 7) &&
+		    OCTEON_IS_MODEL(OCTEON_CN66XX_PASS1_0))
+			iface_ops[interface] = &iface_ops_dis;
+		else if (interface == 5 && OCTEON_IS_MODEL(OCTEON_CN66XX))
+			/*
+			 * Later passes of cn66xx support SRIO0 - x4/x2/x1,
+			 * SRIO2 - x2/x1, SRIO3 - x1
+			 */
+			iface_ops[interface] = &iface_ops_dis;
+		else {
+			sriox_status_reg.u64 =
+				cvmx_read_csr(CVMX_SRIOX_STATUS_REG(interface - 4));
+				iface_ops[interface] = &iface_ops_dis;
+		}
+	}
+	else if (OCTEON_IS_MODEL(OCTEON_CN66XX)) {
+		union cvmx_mio_qlmx_cfg mio_qlm_cfg;
+
+		/* QLM2 is SGMII0 and QLM1 is SGMII1 */
+		if (interface == 0)
+			mio_qlm_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(2));
+		else if (interface == 1)
+			mio_qlm_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(1));
+		else {
+			iface_ops[interface] = &iface_ops_dis;
+			return iface_ops[interface]->mode;
+		}
+
+		if (mio_qlm_cfg.s.qlm_spd == 15)
+			iface_ops[interface] = &iface_ops_dis;
+		else if (mio_qlm_cfg.s.qlm_cfg == 9)
+			iface_ops[interface] = &iface_ops_sgmii;
+		else if (mio_qlm_cfg.s.qlm_cfg == 11)
+			iface_ops[interface] = &iface_ops_xaui;
+		else
+			iface_ops[interface] = &iface_ops_dis;
+	} else if (OCTEON_IS_MODEL(OCTEON_CN61XX)) {
+		union cvmx_mio_qlmx_cfg qlm_cfg;
+
+		if (interface == 0)
+			qlm_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(2));
+		else if (interface == 1)
+			qlm_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(0));
+		else {
+			iface_ops[interface] = &iface_ops_dis;
+			return iface_ops[interface]->mode;
+		}
+
+		if (qlm_cfg.s.qlm_spd == 15)
+			iface_ops[interface] = &iface_ops_dis;
+		else if (qlm_cfg.s.qlm_cfg == 2)
+			iface_ops[interface] = &iface_ops_sgmii;
+		else if (qlm_cfg.s.qlm_cfg == 3)
+			iface_ops[interface] = &iface_ops_xaui;
+		else
+			iface_ops[interface] = &iface_ops_dis;
+	} else if (OCTEON_IS_MODEL(OCTEON_CNF71XX)) {
+		if (interface == 0) {
+			union cvmx_mio_qlmx_cfg qlm_cfg;
+			qlm_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(0));
+			if (qlm_cfg.s.qlm_cfg == 2)
+				iface_ops[interface] = &iface_ops_sgmii;
+			else
+				iface_ops[interface] = &iface_ops_dis;
+		}
+		else
+			iface_ops[interface] = &iface_ops_dis;
+	}
+	else if (interface == 1 && OCTEON_IS_MODEL(OCTEON_CN63XX))
+		iface_ops[interface] = &iface_ops_dis;
+	else {
+		mode.u64 = cvmx_read_csr(CVMX_GMXX_INF_MODE(interface));
+
+		if (OCTEON_IS_MODEL(OCTEON_CN63XX)) {
+			switch (mode.cn63xx.mode) {
+			case 0:
+				iface_ops[interface] = &iface_ops_sgmii;
+				break;
+
+			case 1:
+				iface_ops[interface] = &iface_ops_xaui;
+				break;
+
+			default:
+				iface_ops[interface] = &iface_ops_dis;
+				break;
+			}
+		} else {
+			if (!mode.s.en)
+				iface_ops[interface] = &iface_ops_dis;
+			else if (mode.s.type)
+				iface_ops[interface] = &iface_ops_gmii;
+			else
+				iface_ops[interface] = &iface_ops_rgmii;
+		}
+	}
+
+	return iface_ops[interface]->mode;
 }
-EXPORT_SYMBOL_GPL(cvmx_helper_ports_on_interface);
 
 /**
  * Get the operating mode of an interface. Depending on the Octeon
  * chip and configuration, this function returns an enumeration
  * of the type of packet I/O supported by an interface.
  *
- * @interface: Interface to probe
+ * @param xiface Interface to probe
  *
- * Returns Mode of the interface. Unknown or unsupported interfaces return
- *	   DISABLED.
+ * @return Mode of the interface. Unknown or unsupported interfaces return
+ *         DISABLED.
  */
-cvmx_helper_interface_mode_t cvmx_helper_interface_get_mode(int interface)
+cvmx_helper_interface_mode_t cvmx_helper_interface_get_mode(int xiface)
 {
 	union cvmx_gmxx_inf_mode mode;
-	if (interface == 2)
-		return CVMX_HELPER_INTERFACE_MODE_NPI;
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+
+	if (xi.interface < 0 ||
+	    xi.interface >= cvmx_helper_get_number_of_interfaces())
+		return CVMX_HELPER_INTERFACE_MODE_DISABLED;
+
+	/*
+	 * Check if the interface mode has been already cached. If it has,
+	 * simply return it. Otherwise, fall through the rest of the code to
+	 * determine the interface mode and cache it in iface_ops.
+	 */
+	if (iface_node_ops[xi.node][xi.interface] != NULL)
+		return iface_node_ops[xi.node][xi.interface]->mode;
+
+	/*
+	 * OCTEON III models
+	 */
+	if (OCTEON_IS_MODEL(OCTEON_CN70XX))
+		return __cvmx_get_mode_cn70xx(xi.interface);
 
-	if (interface == 3) {
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+		return __cvmx_get_mode_cn78xx(xiface);
+
+	/*
+	 * Octeon II models
+	 */
+	if (OCTEON_IS_OCTEON2())
+		return __cvmx_get_mode_octeon2(xi.interface);
+
+	/*
+	 * Octeon and Octeon Plus models
+	 */
+	if (xi.interface == 2)
+		iface_ops[xi.interface] = &iface_ops_npi;
+	else if (xi.interface == 3) {
 		if (OCTEON_IS_MODEL(OCTEON_CN56XX)
 		    || OCTEON_IS_MODEL(OCTEON_CN52XX))
-			return CVMX_HELPER_INTERFACE_MODE_LOOP;
+			iface_ops[xi.interface] = &iface_ops_loop;
 		else
-			return CVMX_HELPER_INTERFACE_MODE_DISABLED;
+			iface_ops[xi.interface] = &iface_ops_dis;
 	}
-
-	if (interface == 0
-	    && cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_CN3005_EVB_HS5
-	    && cvmx_sysinfo_get()->board_rev_major == 1) {
+	else if (xi.interface == 0 &&
+	    cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_CN3005_EVB_HS5 &&
+	    cvmx_sysinfo_get()->board_rev_major == 1) {
 		/*
 		 * Lie about interface type of CN3005 board.  This
 		 * board has a switch on port 1 like the other
@@ -142,199 +950,117 @@ cvmx_helper_interface_mode_t cvmx_helper_interface_get_mode(int interface)
 		 * output of this function) there is no difference in
 		 * setup between GMII and RGMII modes.
 		 */
-		return CVMX_HELPER_INTERFACE_MODE_GMII;
+		iface_ops[xi.interface] = &iface_ops_gmii;
 	}
-
-	/* Interface 1 is always disabled on CN31XX and CN30XX */
-	if ((interface == 1)
-	    && (OCTEON_IS_MODEL(OCTEON_CN31XX) || OCTEON_IS_MODEL(OCTEON_CN30XX)
+	else if ((xi.interface == 1)
+	    && (OCTEON_IS_MODEL(OCTEON_CN31XX)
+		|| OCTEON_IS_MODEL(OCTEON_CN30XX)
 		|| OCTEON_IS_MODEL(OCTEON_CN50XX)
 		|| OCTEON_IS_MODEL(OCTEON_CN52XX)))
-		return CVMX_HELPER_INTERFACE_MODE_DISABLED;
-
-	mode.u64 = cvmx_read_csr(CVMX_GMXX_INF_MODE(interface));
-
-	if (OCTEON_IS_MODEL(OCTEON_CN56XX) || OCTEON_IS_MODEL(OCTEON_CN52XX)) {
-		switch (mode.cn56xx.mode) {
-		case 0:
-			return CVMX_HELPER_INTERFACE_MODE_DISABLED;
-		case 1:
-			return CVMX_HELPER_INTERFACE_MODE_XAUI;
-		case 2:
-			return CVMX_HELPER_INTERFACE_MODE_SGMII;
-		case 3:
-			return CVMX_HELPER_INTERFACE_MODE_PICMG;
-		default:
-			return CVMX_HELPER_INTERFACE_MODE_DISABLED;
+		/* Interface 1 is always disabled on CN31XX and CN30XX */
+		iface_ops[xi.interface] = &iface_ops_dis;
+	else {
+		mode.u64 = cvmx_read_csr(CVMX_GMXX_INF_MODE(xi.interface));
+
+		if (OCTEON_IS_MODEL(OCTEON_CN56XX) ||
+		    OCTEON_IS_MODEL(OCTEON_CN52XX)) {
+			switch (mode.cn56xx.mode) {
+			case 0:
+				iface_ops[xi.interface] = &iface_ops_dis;
+				break;
+
+			case 1:
+				iface_ops[xi.interface] = &iface_ops_xaui;
+				break;
+
+			case 2:
+				iface_ops[xi.interface] = &iface_ops_sgmii;
+				break;
+
+			case 3:
+				iface_ops[xi.interface] = &iface_ops_picmg;
+				break;
+
+			default:
+				iface_ops[xi.interface] = &iface_ops_dis;
+				break;
+			}
+		} else {
+			if (!mode.s.en)
+				iface_ops[xi.interface] = &iface_ops_dis;
+			else if (mode.s.type) {
+					iface_ops[xi.interface] = &iface_ops_gmii;
+			} else
+				iface_ops[xi.interface] = &iface_ops_rgmii;
 		}
-	} else {
-		if (!mode.s.en)
-			return CVMX_HELPER_INTERFACE_MODE_DISABLED;
-
-		if (mode.s.type) {
-			if (OCTEON_IS_MODEL(OCTEON_CN38XX)
-			    || OCTEON_IS_MODEL(OCTEON_CN58XX))
-				return CVMX_HELPER_INTERFACE_MODE_SPI;
-			else
-				return CVMX_HELPER_INTERFACE_MODE_GMII;
-		} else
-			return CVMX_HELPER_INTERFACE_MODE_RGMII;
 	}
-}
-EXPORT_SYMBOL_GPL(cvmx_helper_interface_get_mode);
-
-/**
- * Configure the IPD/PIP tagging and QoS options for a specific
- * port. This function determines the POW work queue entry
- * contents for a port. The setup performed here is controlled by
- * the defines in executive-config.h.
- *
- * @ipd_port: Port to configure. This follows the IPD numbering, not the
- *		   per interface numbering
- *
- * Returns Zero on success, negative on failure
- */
-static int __cvmx_helper_port_setup_ipd(int ipd_port)
-{
-	union cvmx_pip_prt_cfgx port_config;
-	union cvmx_pip_prt_tagx tag_config;
-
-	port_config.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(ipd_port));
-	tag_config.u64 = cvmx_read_csr(CVMX_PIP_PRT_TAGX(ipd_port));
-
-	/* Have each port go to a different POW queue */
-	port_config.s.qos = ipd_port & 0x7;
-
-	/* Process the headers and place the IP header in the work queue */
-	port_config.s.mode = CVMX_HELPER_INPUT_PORT_SKIP_MODE;
-
-	tag_config.s.ip6_src_flag = CVMX_HELPER_INPUT_TAG_IPV6_SRC_IP;
-	tag_config.s.ip6_dst_flag = CVMX_HELPER_INPUT_TAG_IPV6_DST_IP;
-	tag_config.s.ip6_sprt_flag = CVMX_HELPER_INPUT_TAG_IPV6_SRC_PORT;
-	tag_config.s.ip6_dprt_flag = CVMX_HELPER_INPUT_TAG_IPV6_DST_PORT;
-	tag_config.s.ip6_nxth_flag = CVMX_HELPER_INPUT_TAG_IPV6_NEXT_HEADER;
-	tag_config.s.ip4_src_flag = CVMX_HELPER_INPUT_TAG_IPV4_SRC_IP;
-	tag_config.s.ip4_dst_flag = CVMX_HELPER_INPUT_TAG_IPV4_DST_IP;
-	tag_config.s.ip4_sprt_flag = CVMX_HELPER_INPUT_TAG_IPV4_SRC_PORT;
-	tag_config.s.ip4_dprt_flag = CVMX_HELPER_INPUT_TAG_IPV4_DST_PORT;
-	tag_config.s.ip4_pctl_flag = CVMX_HELPER_INPUT_TAG_IPV4_PROTOCOL;
-	tag_config.s.inc_prt_flag = CVMX_HELPER_INPUT_TAG_INPUT_PORT;
-	tag_config.s.tcp6_tag_type = CVMX_HELPER_INPUT_TAG_TYPE;
-	tag_config.s.tcp4_tag_type = CVMX_HELPER_INPUT_TAG_TYPE;
-	tag_config.s.ip6_tag_type = CVMX_HELPER_INPUT_TAG_TYPE;
-	tag_config.s.ip4_tag_type = CVMX_HELPER_INPUT_TAG_TYPE;
-	tag_config.s.non_tag_type = CVMX_HELPER_INPUT_TAG_TYPE;
-	/* Put all packets in group 0. Other groups can be used by the app */
-	tag_config.s.grp = 0;
-
-	cvmx_pip_config_port(ipd_port, port_config, tag_config);
-
-	/* Give the user a chance to override our setting for each port */
-	if (cvmx_override_ipd_port_setup)
-		cvmx_override_ipd_port_setup(ipd_port);
-
-	return 0;
-}
 
-/**
- * This function sets the interface_port_count[interface] correctly,
- * without modifying any hardware configuration.  Hardware setup of
- * the ports will be performed later.
- *
- * @interface: Interface to probe
- *
- * Returns Zero on success, negative on failure
- */
-int cvmx_helper_interface_enumerate(int interface)
-{
-	switch (cvmx_helper_interface_get_mode(interface)) {
-		/* These types don't support ports to IPD/PKO */
-	case CVMX_HELPER_INTERFACE_MODE_DISABLED:
-	case CVMX_HELPER_INTERFACE_MODE_PCIE:
-		interface_port_count[interface] = 0;
-		break;
-		/* XAUI is a single high speed port */
-	case CVMX_HELPER_INTERFACE_MODE_XAUI:
-		interface_port_count[interface] =
-		    __cvmx_helper_xaui_enumerate(interface);
-		break;
-		/*
-		 * RGMII/GMII/MII are all treated about the same. Most
-		 * functions refer to these ports as RGMII.
-		 */
-	case CVMX_HELPER_INTERFACE_MODE_RGMII:
-	case CVMX_HELPER_INTERFACE_MODE_GMII:
-		interface_port_count[interface] =
-		    __cvmx_helper_rgmii_enumerate(interface);
-		break;
-		/*
-		 * SPI4 can have 1-16 ports depending on the device at
-		 * the other end.
-		 */
-	case CVMX_HELPER_INTERFACE_MODE_SPI:
-		interface_port_count[interface] =
-		    __cvmx_helper_spi_enumerate(interface);
-		break;
-		/*
-		 * SGMII can have 1-4 ports depending on how many are
-		 * hooked up.
-		 */
-	case CVMX_HELPER_INTERFACE_MODE_SGMII:
-	case CVMX_HELPER_INTERFACE_MODE_PICMG:
-		interface_port_count[interface] =
-		    __cvmx_helper_sgmii_enumerate(interface);
-		break;
-		/* PCI target Network Packet Interface */
-	case CVMX_HELPER_INTERFACE_MODE_NPI:
-		interface_port_count[interface] =
-		    __cvmx_helper_npi_enumerate(interface);
-		break;
-		/*
-		 * Special loopback only ports. These are not the same
-		 * as other ports in loopback mode.
-		 */
-	case CVMX_HELPER_INTERFACE_MODE_LOOP:
-		interface_port_count[interface] =
-		    __cvmx_helper_loop_enumerate(interface);
-		break;
-	}
+	return iface_ops[xi.interface]->mode;
+}
+EXPORT_SYMBOL(cvmx_helper_interface_get_mode);
 
-	interface_port_count[interface] =
-	    __cvmx_helper_board_interface_probe(interface,
-						interface_port_count
-						[interface]);
+/**
+ * Determine the actual number of hardware ports connected to an
+ * interface. It doesn't setup the ports or enable them.
+ *
+ * @param interface Interface to enumerate
+ *
+ * @return The number of ports on the interface, negative on failure
+ */
+int cvmx_helper_interface_enumerate(int xiface)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	int	result = 0;
 
-	/* Make sure all global variables propagate to other cores */
-	CVMX_SYNCWS;
+	cvmx_helper_interface_get_mode(xiface);
+	if (iface_node_ops[xi.node][xi.interface]->enumerate)
+		result = iface_node_ops[xi.node][xi.interface]->enumerate(xiface);
 
-	return 0;
+	return result;
 }
+EXPORT_SYMBOL(cvmx_helper_interface_enumerate);
 
 /**
- * This function probes an interface to determine the actual
- * number of hardware ports connected to it. It doesn't setup the
- * ports or enable them. The main goal here is to set the global
- * interface_port_count[interface] correctly. Hardware setup of the
- * ports will be performed later.
+ * This function probes an interface to determine the actual number of
+ * hardware ports connected to it. It does some setup the ports but
+ * doesn't enable them. The main goal here is to set the global
+ * interface_port_count[interface] correctly. Final hardware setup of
+ * the ports will be performed later.
  *
- * @interface: Interface to probe
+ * @param interface Interface to probe
  *
- * Returns Zero on success, negative on failure
+ * @return Zero on success, negative on failure
  */
-int cvmx_helper_interface_probe(int interface)
+int cvmx_helper_interface_probe(int xiface)
 {
-	cvmx_helper_interface_enumerate(interface);
-	/* At this stage in the game we don't want packets to be moving yet.
-	   The following probe calls should perform hardware setup
-	   needed to determine port counts. Receive must still be disabled */
-	switch (cvmx_helper_interface_get_mode(interface)) {
+	/*
+	 * At this stage in the game we don't want packets to be
+	 * moving yet.  The following probe calls should perform
+	 * hardware setup needed to determine port counts. Receive
+	 * must still be disabled.
+	 */
+	int nports;
+	int has_fcs;
+	enum cvmx_pko_padding padding = CVMX_PKO_PADDING_NONE;
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+
+	nports = -1;
+	has_fcs = 0;
+
+	cvmx_helper_interface_get_mode(xiface);
+	if (iface_node_ops[xi.node][xi.interface]->probe)
+		nports = iface_node_ops[xi.node][xi.interface]->probe(xiface);
+
+	switch (iface_node_ops[xi.node][xi.interface]->mode) {
 		/* These types don't support ports to IPD/PKO */
 	case CVMX_HELPER_INTERFACE_MODE_DISABLED:
 	case CVMX_HELPER_INTERFACE_MODE_PCIE:
+		nports = 0;
 		break;
 		/* XAUI is a single high speed port */
 	case CVMX_HELPER_INTERFACE_MODE_XAUI:
-		__cvmx_helper_xaui_probe(interface);
+		has_fcs = 1;
+		padding = CVMX_PKO_PADDING_60;
 		break;
 		/*
 		 * RGMII/GMII/MII are all treated about the same. Most
@@ -342,14 +1068,14 @@ int cvmx_helper_interface_probe(int interface)
 		 */
 	case CVMX_HELPER_INTERFACE_MODE_RGMII:
 	case CVMX_HELPER_INTERFACE_MODE_GMII:
-		__cvmx_helper_rgmii_probe(interface);
+		padding = CVMX_PKO_PADDING_60;
 		break;
 		/*
 		 * SPI4 can have 1-16 ports depending on the device at
 		 * the other end.
 		 */
 	case CVMX_HELPER_INTERFACE_MODE_SPI:
-		__cvmx_helper_spi_probe(interface);
+		padding = CVMX_PKO_PADDING_60;
 		break;
 		/*
 		 * SGMII can have 1-4 ports depending on how many are
@@ -357,566 +1083,272 @@ int cvmx_helper_interface_probe(int interface)
 		 */
 	case CVMX_HELPER_INTERFACE_MODE_SGMII:
 	case CVMX_HELPER_INTERFACE_MODE_PICMG:
-		__cvmx_helper_sgmii_probe(interface);
+		has_fcs = 1;
 		break;
 		/* PCI target Network Packet Interface */
 	case CVMX_HELPER_INTERFACE_MODE_NPI:
-		__cvmx_helper_npi_probe(interface);
 		break;
 		/*
 		 * Special loopback only ports. These are not the same
 		 * as other ports in loopback mode.
 		 */
 	case CVMX_HELPER_INTERFACE_MODE_LOOP:
-		__cvmx_helper_loop_probe(interface);
+		break;
+		/* SRIO has 2^N ports, where N is number of interfaces */
+	case CVMX_HELPER_INTERFACE_MODE_ILK:
 		break;
 	}
 
+	if (nports == -1)
+		return -1;
+
+	if (!octeon_has_feature(OCTEON_FEATURE_PKND))
+		has_fcs = 0;
+
+	nports = __cvmx_helper_board_interface_probe(xiface, nports);
+	__cvmx_helper_init_interface(xiface, nports, has_fcs, padding);
 	/* Make sure all global variables propagate to other cores */
 	CVMX_SYNCWS;
 
 	return 0;
 }
+EXPORT_SYMBOL(cvmx_helper_interface_probe);
 
 /**
- * Setup the IPD/PIP for the ports on an interface. Packet
- * classification and tagging are set for every port on the
- * interface. The number of ports on the interface must already
- * have been probed.
- *
- * @interface: Interface to setup IPD/PIP for
- *
- * Returns Zero on success, negative on failure
+ * @INTERNAL
+ * Verify the per port IPD backpressure is aligned properly.
+ * @return Zero if working, non zero if misaligned
  */
-static int __cvmx_helper_interface_setup_ipd(int interface)
+int __cvmx_helper_backpressure_is_misaligned(void)
 {
-	int ipd_port = cvmx_helper_get_ipd_port(interface, 0);
-	int num_ports = interface_port_count[interface];
+	uint64_t ipd_int_enb;
+	union cvmx_ipd_ctl_status ipd_reg;
+	uint64_t bp_status0;
+	uint64_t bp_status1;
+	const int port0 = 0;
+	const int port1 = 16;
+	cvmx_helper_interface_mode_t mode0, mode1;
+
+	mode0 = cvmx_helper_interface_get_mode(0);
+	mode1 = cvmx_helper_interface_get_mode(1);
+
+	if (!((cvmx_sysinfo_get()->board_type != CVMX_BOARD_TYPE_SIM) &&
+	     (OCTEON_IS_MODEL(OCTEON_CN3XXX) ||
+	      OCTEON_IS_MODEL(OCTEON_CN5XXX))))
+		return 0;
+
+	/* Disable error interrupts while we check backpressure */
+	ipd_int_enb = cvmx_read_csr(CVMX_IPD_INT_ENB);
+	cvmx_write_csr(CVMX_IPD_INT_ENB, 0);
+
+	/* Enable per port backpressure */
+	ipd_reg.u64 = cvmx_read_csr(CVMX_IPD_CTL_STATUS);
+	ipd_reg.s.pbp_en = 1;
+	cvmx_write_csr(CVMX_IPD_CTL_STATUS, ipd_reg.u64);
+
+	if (mode0 != CVMX_HELPER_INTERFACE_MODE_DISABLED) {
+		/* Enable backpressure for port with a zero threshold */
+		cvmx_write_csr(CVMX_IPD_PORTX_BP_PAGE_CNT(port0), 1 << 17);
+		/* Add 1000 to the page count to simulate packets coming in */
+		cvmx_write_csr(CVMX_IPD_SUB_PORT_BP_PAGE_CNT,
+			       (port0 << 25) | 1000);
+	}
 
-	while (num_ports--) {
-		__cvmx_helper_port_setup_ipd(ipd_port);
-		ipd_port++;
+	if (mode1 != CVMX_HELPER_INTERFACE_MODE_DISABLED) {
+		/* Enable backpressure for port with a zero threshold */
+		cvmx_write_csr(CVMX_IPD_PORTX_BP_PAGE_CNT(port1), 1 << 17);
+		/* Add 1000 to the page count to simulate packets coming in */
+		cvmx_write_csr(CVMX_IPD_SUB_PORT_BP_PAGE_CNT,
+			       (port1 << 25) | 1000);
 	}
-	return 0;
-}
 
-/**
- * Setup global setting for IPD/PIP not related to a specific
- * interface or port. This must be called before IPD is enabled.
- *
- * Returns Zero on success, negative on failure.
- */
-static int __cvmx_helper_global_setup_ipd(void)
-{
-	/* Setup the global packet input options */
-	cvmx_ipd_config(CVMX_FPA_PACKET_POOL_SIZE / 8,
-			CVMX_HELPER_FIRST_MBUFF_SKIP / 8,
-			CVMX_HELPER_NOT_FIRST_MBUFF_SKIP / 8,
-			/* The +8 is to account for the next ptr */
-			(CVMX_HELPER_FIRST_MBUFF_SKIP + 8) / 128,
-			/* The +8 is to account for the next ptr */
-			(CVMX_HELPER_NOT_FIRST_MBUFF_SKIP + 8) / 128,
-			CVMX_FPA_WQE_POOL,
-			CVMX_IPD_OPC_MODE_STT,
-			CVMX_HELPER_ENABLE_BACK_PRESSURE);
-	return 0;
-}
+	/* Wait 500 cycles for the BP to update */
+	cvmx_wait(500);
 
-/**
- * Setup the PKO for the ports on an interface. The number of
- * queues per port and the priority of each PKO output queue
- * is set here. PKO must be disabled when this function is called.
- *
- * @interface: Interface to setup PKO for
- *
- * Returns Zero on success, negative on failure
- */
-static int __cvmx_helper_interface_setup_pko(int interface)
-{
-	/*
-	 * Each packet output queue has an associated priority. The
-	 * higher the priority, the more often it can send a packet. A
-	 * priority of 8 means it can send in all 8 rounds of
-	 * contention. We're going to make each queue one less than
-	 * the last.  The vector of priorities has been extended to
-	 * support CN5xxx CPUs, where up to 16 queues can be
-	 * associated to a port.  To keep backward compatibility we
-	 * don't change the initial 8 priorities and replicate them in
-	 * the second half.  With per-core PKO queues (PKO lockless
-	 * operation) all queues have the same priority.
-	 */
-	uint64_t priorities[16] =
-	    { 8, 7, 6, 5, 4, 3, 2, 1, 8, 7, 6, 5, 4, 3, 2, 1 };
+	/* Read the BP state from the debug select register */
+	switch (mode0) {
+	case CVMX_HELPER_INTERFACE_MODE_SPI:
+		cvmx_write_csr(CVMX_NPI_DBG_SELECT, 0x9004);
+		bp_status0 = cvmx_read_csr(CVMX_DBG_DATA);
+		bp_status0 = 0xffff & ~bp_status0;
+		break;
+	case CVMX_HELPER_INTERFACE_MODE_RGMII:
+	case CVMX_HELPER_INTERFACE_MODE_GMII:
+		cvmx_write_csr(CVMX_NPI_DBG_SELECT, 0x0e00);
+		bp_status0 = 0xffff & cvmx_read_csr(CVMX_DBG_DATA);
+		break;
+	case CVMX_HELPER_INTERFACE_MODE_XAUI:
+	case CVMX_HELPER_INTERFACE_MODE_SGMII:
+	case CVMX_HELPER_INTERFACE_MODE_PICMG:
+		cvmx_write_csr(CVMX_PEXP_NPEI_DBG_SELECT, 0x0e00);
+		bp_status0 = 0xffff & cvmx_read_csr(CVMX_PEXP_NPEI_DBG_DATA);
+		break;
+	default:
+		bp_status0 = 1 << port0;
+		break;
+	}
 
-	/*
-	 * Setup the IPD/PIP and PKO for the ports discovered
-	 * above. Here packet classification, tagging and output
-	 * priorities are set.
-	 */
-	int ipd_port = cvmx_helper_get_ipd_port(interface, 0);
-	int num_ports = interface_port_count[interface];
-	while (num_ports--) {
-		/*
-		 * Give the user a chance to override the per queue
-		 * priorities.
-		 */
-		if (cvmx_override_pko_queue_priority)
-			cvmx_override_pko_queue_priority(ipd_port, priorities);
-
-		cvmx_pko_config_port(ipd_port,
-				     cvmx_pko_get_base_queue_per_core(ipd_port,
-								      0),
-				     cvmx_pko_get_num_queues(ipd_port),
-				     priorities);
-		ipd_port++;
+	/* Read the BP state from the debug select register */
+	switch (mode1) {
+	case CVMX_HELPER_INTERFACE_MODE_SPI:
+		cvmx_write_csr(CVMX_NPI_DBG_SELECT, 0x9804);
+		bp_status1 = cvmx_read_csr(CVMX_DBG_DATA);
+		bp_status1 = 0xffff & ~bp_status1;
+		break;
+	case CVMX_HELPER_INTERFACE_MODE_RGMII:
+	case CVMX_HELPER_INTERFACE_MODE_GMII:
+		cvmx_write_csr(CVMX_NPI_DBG_SELECT, 0x1600);
+		bp_status1 = 0xffff & cvmx_read_csr(CVMX_DBG_DATA);
+		break;
+	case CVMX_HELPER_INTERFACE_MODE_XAUI:
+	case CVMX_HELPER_INTERFACE_MODE_SGMII:
+	case CVMX_HELPER_INTERFACE_MODE_PICMG:
+		cvmx_write_csr(CVMX_PEXP_NPEI_DBG_SELECT, 0x1600);
+		bp_status1 = 0xffff & cvmx_read_csr(CVMX_PEXP_NPEI_DBG_DATA);
+		break;
+	default:
+		bp_status1 = 1 << (port1 - 16);
+		break;
 	}
-	return 0;
-}
 
-/**
- * Setup global setting for PKO not related to a specific
- * interface or port. This must be called before PKO is enabled.
- *
- * Returns Zero on success, negative on failure.
- */
-static int __cvmx_helper_global_setup_pko(void)
-{
-	/*
-	 * Disable tagwait FAU timeout. This needs to be done before
-	 * anyone might start packet output using tags.
-	 */
-	union cvmx_iob_fau_timeout fau_to;
-	fau_to.u64 = 0;
-	fau_to.s.tout_val = 0xfff;
-	fau_to.s.tout_enb = 0;
-	cvmx_write_csr(CVMX_IOB_FAU_TIMEOUT, fau_to.u64);
-	return 0;
-}
+	if (mode0 != CVMX_HELPER_INTERFACE_MODE_DISABLED) {
+		/* Shutdown BP */
+		cvmx_write_csr(CVMX_IPD_SUB_PORT_BP_PAGE_CNT, (port0 << 25) | (0x1ffffff & -1000));
+		cvmx_write_csr(CVMX_IPD_PORTX_BP_PAGE_CNT(port0), 0);
+	}
 
-/**
- * Setup global backpressure setting.
- *
- * Returns Zero on success, negative on failure
- */
-static int __cvmx_helper_global_setup_backpressure(void)
-{
-#if CVMX_HELPER_DISABLE_RGMII_BACKPRESSURE
-	/* Disable backpressure if configured to do so */
-	/* Disable backpressure (pause frame) generation */
-	int num_interfaces = cvmx_helper_get_number_of_interfaces();
-	int interface;
-	for (interface = 0; interface < num_interfaces; interface++) {
-		switch (cvmx_helper_interface_get_mode(interface)) {
-		case CVMX_HELPER_INTERFACE_MODE_DISABLED:
-		case CVMX_HELPER_INTERFACE_MODE_PCIE:
-		case CVMX_HELPER_INTERFACE_MODE_NPI:
-		case CVMX_HELPER_INTERFACE_MODE_LOOP:
-		case CVMX_HELPER_INTERFACE_MODE_XAUI:
-			break;
-		case CVMX_HELPER_INTERFACE_MODE_RGMII:
-		case CVMX_HELPER_INTERFACE_MODE_GMII:
-		case CVMX_HELPER_INTERFACE_MODE_SPI:
-		case CVMX_HELPER_INTERFACE_MODE_SGMII:
-		case CVMX_HELPER_INTERFACE_MODE_PICMG:
-			cvmx_gmx_set_backpressure_override(interface, 0xf);
-			break;
-		}
+	if (mode1 != CVMX_HELPER_INTERFACE_MODE_DISABLED) {
+		/* Shutdown BP */
+		cvmx_write_csr(CVMX_IPD_SUB_PORT_BP_PAGE_CNT, (port1 << 25) | (0x1ffffff & -1000));
+		cvmx_write_csr(CVMX_IPD_PORTX_BP_PAGE_CNT(port1), 0);
 	}
-#endif
 
-	return 0;
+	/* Clear any error interrupts that might have been set */
+	cvmx_write_csr(CVMX_IPD_INT_SUM, 0x1f);
+	cvmx_write_csr(CVMX_IPD_INT_ENB, ipd_int_enb);
+
+	return (bp_status0 != (1ull << port0)) || (bp_status1 != (1ull << (port1 - 16)));
 }
 
 /**
+ * @INTERNAL
  * Enable packet input/output from the hardware. This function is
  * called after all internal setup is complete and IPD is enabled.
  * After this function completes, packets will be accepted from the
  * hardware ports. PKO should still be disabled to make sure packets
  * aren't sent out partially setup hardware.
  *
- * @interface: Interface to enable
+ * @param interface Interface to enable
+ * @param iflags Interface flags
+ * @param pflags Array of flags, one per port on the interface
  *
- * Returns Zero on success, negative on failure
+ * @return Zero on success, negative on failure
  */
-static int __cvmx_helper_packet_hardware_enable(int interface)
+int __cvmx_helper_packet_hardware_enable(int xiface)
 {
 	int result = 0;
-	switch (cvmx_helper_interface_get_mode(interface)) {
-		/* These types don't support ports to IPD/PKO */
-	case CVMX_HELPER_INTERFACE_MODE_DISABLED:
-	case CVMX_HELPER_INTERFACE_MODE_PCIE:
-		/* Nothing to do */
-		break;
-		/* XAUI is a single high speed port */
-	case CVMX_HELPER_INTERFACE_MODE_XAUI:
-		result = __cvmx_helper_xaui_enable(interface);
-		break;
-		/*
-		 * RGMII/GMII/MII are all treated about the same. Most
-		 * functions refer to these ports as RGMII
-		 */
-	case CVMX_HELPER_INTERFACE_MODE_RGMII:
-	case CVMX_HELPER_INTERFACE_MODE_GMII:
-		result = __cvmx_helper_rgmii_enable(interface);
-		break;
-		/*
-		 * SPI4 can have 1-16 ports depending on the device at
-		 * the other end
-		 */
-	case CVMX_HELPER_INTERFACE_MODE_SPI:
-		result = __cvmx_helper_spi_enable(interface);
-		break;
-		/*
-		 * SGMII can have 1-4 ports depending on how many are
-		 * hooked up
-		 */
-	case CVMX_HELPER_INTERFACE_MODE_SGMII:
-	case CVMX_HELPER_INTERFACE_MODE_PICMG:
-		result = __cvmx_helper_sgmii_enable(interface);
-		break;
-		/* PCI target Network Packet Interface */
-	case CVMX_HELPER_INTERFACE_MODE_NPI:
-		result = __cvmx_helper_npi_enable(interface);
-		break;
-		/*
-		 * Special loopback only ports. These are not the same
-		 * as other ports in loopback mode
-		 */
-	case CVMX_HELPER_INTERFACE_MODE_LOOP:
-		result = __cvmx_helper_loop_enable(interface);
-		break;
-	}
-	result |= __cvmx_helper_board_hardware_enable(interface);
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+
+	cvmx_helper_interface_get_mode(xiface);
+	if (iface_node_ops[xi.node][xi.interface]->enable)
+		result = iface_node_ops[xi.node][xi.interface]->enable(xiface);
+	result |= __cvmx_helper_board_hardware_enable(xiface);
 	return result;
 }
+EXPORT_SYMBOL(__cvmx_helper_packet_hardware_enable);
 
 /**
- * Function to adjust internal IPD pointer alignments
+ * Does core local initialization for packet io
  *
- * Returns 0 on success
- *	   !0 on failure
+ * @return Zero on success, non-zero on failure
  */
-int __cvmx_helper_errata_fix_ipd_ptr_alignment(void)
+int cvmx_helper_initialize_packet_io_local(void)
 {
-#define FIX_IPD_FIRST_BUFF_PAYLOAD_BYTES \
-     (CVMX_FPA_PACKET_POOL_SIZE-8-CVMX_HELPER_FIRST_MBUFF_SKIP)
-#define FIX_IPD_NON_FIRST_BUFF_PAYLOAD_BYTES \
-	(CVMX_FPA_PACKET_POOL_SIZE-8-CVMX_HELPER_NOT_FIRST_MBUFF_SKIP)
-#define FIX_IPD_OUTPORT 0
-	/* Ports 0-15 are interface 0, 16-31 are interface 1 */
-#define INTERFACE(port) (port >> 4)
-#define INDEX(port) (port & 0xf)
-	uint64_t *p64;
-	cvmx_pko_command_word0_t pko_command;
-	union cvmx_buf_ptr g_buffer, pkt_buffer;
-	cvmx_wqe_t *work;
-	int size, num_segs = 0, wqe_pcnt, pkt_pcnt;
-	union cvmx_gmxx_prtx_cfg gmx_cfg;
-	int retry_cnt;
-	int retry_loop_cnt;
-	int i;
-	cvmx_helper_link_info_t link_info;
-
-	/* Save values for restore at end */
-	uint64_t prtx_cfg =
-	    cvmx_read_csr(CVMX_GMXX_PRTX_CFG
-			  (INDEX(FIX_IPD_OUTPORT), INTERFACE(FIX_IPD_OUTPORT)));
-	uint64_t tx_ptr_en =
-	    cvmx_read_csr(CVMX_ASXX_TX_PRT_EN(INTERFACE(FIX_IPD_OUTPORT)));
-	uint64_t rx_ptr_en =
-	    cvmx_read_csr(CVMX_ASXX_RX_PRT_EN(INTERFACE(FIX_IPD_OUTPORT)));
-	uint64_t rxx_jabber =
-	    cvmx_read_csr(CVMX_GMXX_RXX_JABBER
-			  (INDEX(FIX_IPD_OUTPORT), INTERFACE(FIX_IPD_OUTPORT)));
-	uint64_t frame_max =
-	    cvmx_read_csr(CVMX_GMXX_RXX_FRM_MAX
-			  (INDEX(FIX_IPD_OUTPORT), INTERFACE(FIX_IPD_OUTPORT)));
-
-	/* Configure port to gig FDX as required for loopback mode */
-	cvmx_helper_rgmii_internal_loopback(FIX_IPD_OUTPORT);
-
-	/*
-	 * Disable reception on all ports so if traffic is present it
-	 * will not interfere.
-	 */
-	cvmx_write_csr(CVMX_ASXX_RX_PRT_EN(INTERFACE(FIX_IPD_OUTPORT)), 0);
-
-	cvmx_wait(100000000ull);
-
-	for (retry_loop_cnt = 0; retry_loop_cnt < 10; retry_loop_cnt++) {
-		retry_cnt = 100000;
-		wqe_pcnt = cvmx_read_csr(CVMX_IPD_PTR_COUNT);
-		pkt_pcnt = (wqe_pcnt >> 7) & 0x7f;
-		wqe_pcnt &= 0x7f;
-
-		num_segs = (2 + pkt_pcnt - wqe_pcnt) & 3;
-
-		if (num_segs == 0)
-			goto fix_ipd_exit;
-
-		num_segs += 1;
-
-		size =
-		    FIX_IPD_FIRST_BUFF_PAYLOAD_BYTES +
-		    ((num_segs - 1) * FIX_IPD_NON_FIRST_BUFF_PAYLOAD_BYTES) -
-		    (FIX_IPD_NON_FIRST_BUFF_PAYLOAD_BYTES / 2);
-
-		cvmx_write_csr(CVMX_ASXX_PRT_LOOP(INTERFACE(FIX_IPD_OUTPORT)),
-			       1 << INDEX(FIX_IPD_OUTPORT));
-		CVMX_SYNC;
-
-		g_buffer.u64 = 0;
-		g_buffer.s.addr =
-		    cvmx_ptr_to_phys(cvmx_fpa_alloc(CVMX_FPA_WQE_POOL));
-		if (g_buffer.s.addr == 0) {
-			cvmx_dprintf("WARNING: FIX_IPD_PTR_ALIGNMENT "
-				     "buffer allocation failure.\n");
-			goto fix_ipd_exit;
-		}
-
-		g_buffer.s.pool = CVMX_FPA_WQE_POOL;
-		g_buffer.s.size = num_segs;
-
-		pkt_buffer.u64 = 0;
-		pkt_buffer.s.addr =
-		    cvmx_ptr_to_phys(cvmx_fpa_alloc(CVMX_FPA_PACKET_POOL));
-		if (pkt_buffer.s.addr == 0) {
-			cvmx_dprintf("WARNING: FIX_IPD_PTR_ALIGNMENT "
-				     "buffer allocation failure.\n");
-			goto fix_ipd_exit;
-		}
-		pkt_buffer.s.i = 1;
-		pkt_buffer.s.pool = CVMX_FPA_PACKET_POOL;
-		pkt_buffer.s.size = FIX_IPD_FIRST_BUFF_PAYLOAD_BYTES;
-
-		p64 = (uint64_t *) cvmx_phys_to_ptr(pkt_buffer.s.addr);
-		p64[0] = 0xffffffffffff0000ull;
-		p64[1] = 0x08004510ull;
-		p64[2] = ((uint64_t) (size - 14) << 48) | 0x5ae740004000ull;
-		p64[3] = 0x3a5fc0a81073c0a8ull;
-
-		for (i = 0; i < num_segs; i++) {
-			if (i > 0)
-				pkt_buffer.s.size =
-				    FIX_IPD_NON_FIRST_BUFF_PAYLOAD_BYTES;
-
-			if (i == (num_segs - 1))
-				pkt_buffer.s.i = 0;
-
-			*(uint64_t *) cvmx_phys_to_ptr(g_buffer.s.addr +
-						       8 * i) = pkt_buffer.u64;
-		}
-
-		/* Build the PKO command */
-		pko_command.u64 = 0;
-		pko_command.s.segs = num_segs;
-		pko_command.s.total_bytes = size;
-		pko_command.s.dontfree = 0;
-		pko_command.s.gather = 1;
-
-		gmx_cfg.u64 =
-		    cvmx_read_csr(CVMX_GMXX_PRTX_CFG
-				  (INDEX(FIX_IPD_OUTPORT),
-				   INTERFACE(FIX_IPD_OUTPORT)));
-		gmx_cfg.s.en = 1;
-		cvmx_write_csr(CVMX_GMXX_PRTX_CFG
-			       (INDEX(FIX_IPD_OUTPORT),
-				INTERFACE(FIX_IPD_OUTPORT)), gmx_cfg.u64);
-		cvmx_write_csr(CVMX_ASXX_TX_PRT_EN(INTERFACE(FIX_IPD_OUTPORT)),
-			       1 << INDEX(FIX_IPD_OUTPORT));
-		cvmx_write_csr(CVMX_ASXX_RX_PRT_EN(INTERFACE(FIX_IPD_OUTPORT)),
-			       1 << INDEX(FIX_IPD_OUTPORT));
-
-		cvmx_write_csr(CVMX_GMXX_RXX_JABBER
-			       (INDEX(FIX_IPD_OUTPORT),
-				INTERFACE(FIX_IPD_OUTPORT)), 65392 - 14 - 4);
-		cvmx_write_csr(CVMX_GMXX_RXX_FRM_MAX
-			       (INDEX(FIX_IPD_OUTPORT),
-				INTERFACE(FIX_IPD_OUTPORT)), 65392 - 14 - 4);
-
-		cvmx_pko_send_packet_prepare(FIX_IPD_OUTPORT,
-					     cvmx_pko_get_base_queue
-					     (FIX_IPD_OUTPORT),
-					     CVMX_PKO_LOCK_CMD_QUEUE);
-		cvmx_pko_send_packet_finish(FIX_IPD_OUTPORT,
-					    cvmx_pko_get_base_queue
-					    (FIX_IPD_OUTPORT), pko_command,
-					    g_buffer, CVMX_PKO_LOCK_CMD_QUEUE);
-
-		CVMX_SYNC;
-
-		do {
-			work = cvmx_pow_work_request_sync(CVMX_POW_WAIT);
-			retry_cnt--;
-		} while ((work == NULL) && (retry_cnt > 0));
-
-		if (!retry_cnt)
-			cvmx_dprintf("WARNING: FIX_IPD_PTR_ALIGNMENT "
-				     "get_work() timeout occurred.\n");
-
-		/* Free packet */
-		if (work)
-			cvmx_helper_free_packet_data(work);
-	}
-
-fix_ipd_exit:
-
-	/* Return CSR configs to saved values */
-	cvmx_write_csr(CVMX_GMXX_PRTX_CFG
-		       (INDEX(FIX_IPD_OUTPORT), INTERFACE(FIX_IPD_OUTPORT)),
-		       prtx_cfg);
-	cvmx_write_csr(CVMX_ASXX_TX_PRT_EN(INTERFACE(FIX_IPD_OUTPORT)),
-		       tx_ptr_en);
-	cvmx_write_csr(CVMX_ASXX_RX_PRT_EN(INTERFACE(FIX_IPD_OUTPORT)),
-		       rx_ptr_en);
-	cvmx_write_csr(CVMX_GMXX_RXX_JABBER
-		       (INDEX(FIX_IPD_OUTPORT), INTERFACE(FIX_IPD_OUTPORT)),
-		       rxx_jabber);
-	cvmx_write_csr(CVMX_GMXX_RXX_FRM_MAX
-		       (INDEX(FIX_IPD_OUTPORT), INTERFACE(FIX_IPD_OUTPORT)),
-		       frame_max);
-	cvmx_write_csr(CVMX_ASXX_PRT_LOOP(INTERFACE(FIX_IPD_OUTPORT)), 0);
-	/* Set link to down so autonegotiation will set it up again */
-	link_info.u64 = 0;
-	cvmx_helper_link_set(FIX_IPD_OUTPORT, link_info);
-
-	/*
-	 * Bring the link back up as autonegotiation is not done in
-	 * user applications.
-	 */
-	cvmx_helper_link_autoconf(FIX_IPD_OUTPORT);
-
-	CVMX_SYNC;
-	if (num_segs)
-		cvmx_dprintf("WARNING: FIX_IPD_PTR_ALIGNMENT failed.\n");
-
-	return !!num_segs;
-
+	return 0;
 }
+struct cvmx_buffer_list {
+	struct cvmx_buffer_list *next;
+};
 
 /**
- * Called after all internal packet IO paths are setup. This
- * function enables IPD/PIP and begins packet input and output.
+ * Disables the sending of flow control (pause) frames on the specified
+ * GMX port(s).
+ *
+ * @param interface Which interface (0 or 1)
+ * @param port_mask Mask (4bits) of which ports on the interface to disable
+ *                  backpressure on.
+ *                  1 => disable backpressure
+ *                  0 => enable backpressure
  *
- * Returns Zero on success, negative on failure
+ * @return 0 on success
+ *         -1 on error
  */
-int cvmx_helper_ipd_and_packet_input_enable(void)
+int cvmx_gmx_set_backpressure_override(uint32_t interface, uint32_t port_mask)
 {
-	int num_interfaces;
-	int interface;
-
-	/* Enable IPD */
-	cvmx_ipd_enable();
-
-	/*
-	 * Time to enable hardware ports packet input and output. Note
-	 * that at this point IPD/PIP must be fully functional and PKO
-	 * must be disabled
-	 */
-	num_interfaces = cvmx_helper_get_number_of_interfaces();
-	for (interface = 0; interface < num_interfaces; interface++) {
-		if (cvmx_helper_ports_on_interface(interface) > 0)
-			__cvmx_helper_packet_hardware_enable(interface);
-	}
-
-	/* Finally enable PKO now that the entire path is up and running */
-	cvmx_pko_enable();
+	union cvmx_gmxx_tx_ovr_bp gmxx_tx_ovr_bp;
+	/* Check for valid arguments */
+	if (port_mask & ~0xf || interface & ~0x1)
+		return -1;
+	return -1;
 
-	if ((OCTEON_IS_MODEL(OCTEON_CN31XX_PASS1)
-	     || OCTEON_IS_MODEL(OCTEON_CN30XX_PASS1))
-	    && (cvmx_sysinfo_get()->board_type != CVMX_BOARD_TYPE_SIM))
-		__cvmx_helper_errata_fix_ipd_ptr_alignment();
+	gmxx_tx_ovr_bp.u64 = 0;
+	gmxx_tx_ovr_bp.s.en = port_mask;	/* Per port Enable back pressure override */
+	gmxx_tx_ovr_bp.s.ign_full = port_mask;	/* Ignore the RX FIFO full when computing BP */
+	cvmx_write_csr(CVMX_GMXX_TX_OVR_BP(interface), gmxx_tx_ovr_bp.u64);
 	return 0;
 }
-EXPORT_SYMBOL_GPL(cvmx_helper_ipd_and_packet_input_enable);
 
 /**
- * Initialize the PIP, IPD, and PKO hardware to support
- * simple priority based queues for the ethernet ports. Each
- * port is configured with a number of priority queues based
- * on CVMX_PKO_QUEUES_PER_PORT_* where each queue is lower
- * priority than the previous.
+ * Disables the sending of flow control (pause) frames on the specified
+ * AGL (RGMII) port(s).
+ *
+ * @param interface Which interface (0 or 1)
+ * @param port_mask Mask (4bits) of which ports on the interface to disable
+ *                  backpressure on.
+ *                  1 => disable backpressure
+ *                  0 => enable backpressure
  *
- * Returns Zero on success, non-zero on failure
+ * @return 0 on success
+ *         -1 on error
  */
-int cvmx_helper_initialize_packet_io_global(void)
-{
-	int result = 0;
-	int interface;
-	union cvmx_l2c_cfg l2c_cfg;
-	union cvmx_smix_en smix_en;
-	const int num_interfaces = cvmx_helper_get_number_of_interfaces();
-
-	/*
-	 * CN52XX pass 1: Due to a bug in 2nd order CDR, it needs to
-	 * be disabled.
-	 */
-	if (OCTEON_IS_MODEL(OCTEON_CN52XX_PASS1_0))
-		__cvmx_helper_errata_qlm_disable_2nd_order_cdr(1);
-
-	/*
-	 * Tell L2 to give the IOB statically higher priority compared
-	 * to the cores. This avoids conditions where IO blocks might
-	 * be starved under very high L2 loads.
-	 */
-	l2c_cfg.u64 = cvmx_read_csr(CVMX_L2C_CFG);
-	l2c_cfg.s.lrf_arb_mode = 0;
-	l2c_cfg.s.rfb_arb_mode = 0;
-	cvmx_write_csr(CVMX_L2C_CFG, l2c_cfg.u64);
-
-	/* Make sure SMI/MDIO is enabled so we can query PHYs */
-	smix_en.u64 = cvmx_read_csr(CVMX_SMIX_EN(0));
-	if (!smix_en.s.en) {
-		smix_en.s.en = 1;
-		cvmx_write_csr(CVMX_SMIX_EN(0), smix_en.u64);
-	}
-
-	/* Newer chips actually have two SMI/MDIO interfaces */
-	if (!OCTEON_IS_MODEL(OCTEON_CN3XXX) &&
-	    !OCTEON_IS_MODEL(OCTEON_CN58XX) &&
-	    !OCTEON_IS_MODEL(OCTEON_CN50XX)) {
-		smix_en.u64 = cvmx_read_csr(CVMX_SMIX_EN(1));
-		if (!smix_en.s.en) {
-			smix_en.s.en = 1;
-			cvmx_write_csr(CVMX_SMIX_EN(1), smix_en.u64);
-		}
-	}
 
-	cvmx_pko_initialize_global();
-	for (interface = 0; interface < num_interfaces; interface++) {
-		result |= cvmx_helper_interface_probe(interface);
-		if (cvmx_helper_ports_on_interface(interface) > 0)
-			cvmx_dprintf("Interface %d has %d ports (%s)\n",
-				     interface,
-				     cvmx_helper_ports_on_interface(interface),
-				     cvmx_helper_interface_mode_to_string
-				     (cvmx_helper_interface_get_mode
-				      (interface)));
-		result |= __cvmx_helper_interface_setup_ipd(interface);
-		result |= __cvmx_helper_interface_setup_pko(interface);
-	}
-
-	result |= __cvmx_helper_global_setup_ipd();
-	result |= __cvmx_helper_global_setup_pko();
-
-	/* Enable any flow control and backpressure */
-	result |= __cvmx_helper_global_setup_backpressure();
+/**
+ * Disables the sending of flow control (pause) frames on the specified
+ * BGX port(s).
+ *
+ * @param interface Which interface (0 or 1)
+ * @param port_mask Mask (4bits) of which ports on the interface to disable
+ *                  backpressure on.
+ *                  1 => disable backpressure
+ *                  0 => enable backpressure
+ *
+ * @return 0 on success
+ *         -1 on error
+ */
+int cvmx_bgx_set_backpressure_override(uint32_t interface, uint32_t port_mask)
+{
+	cvmx_bgxx_cmr_rx_ovr_bp_t rx_ovr_bp;
 
-#if CVMX_HELPER_ENABLE_IPD
-	result |= cvmx_helper_ipd_and_packet_input_enable();
-#endif
-	return result;
+	/* Check for valid arguments */
+	rx_ovr_bp.u64 = 0;
+	rx_ovr_bp.s.en = port_mask;	/* Per port Enable back pressure override */
+	rx_ovr_bp.s.ign_fifo_bp = port_mask;	/* Ignore the RX FIFO full when computing BP */
+	cvmx_write_csr(CVMX_BGXX_CMR_RX_OVR_BP(interface), rx_ovr_bp.u64);
+	return 0;
 }
-EXPORT_SYMBOL_GPL(cvmx_helper_initialize_packet_io_global);
 
 /**
- * Does core local initialization for packet io
+ * Does core local shutdown of packet io
  *
- * Returns Zero on success, non-zero on failure
+ * @return Zero on success, non-zero on failure
  */
-int cvmx_helper_initialize_packet_io_local(void)
+int cvmx_helper_shutdown_packet_io_local(void)
 {
-	return cvmx_pko_initialize_local();
+	/*
+	 * Currently there is nothing to do per core. This may change
+	 * in the future.
+	 */
+	return 0;
 }
 
 /**
@@ -924,9 +1356,9 @@ int cvmx_helper_initialize_packet_io_local(void)
  * function basically does the equivalent of:
  * cvmx_helper_link_set(ipd_port, cvmx_helper_link_get(ipd_port));
  *
- * @ipd_port: IPD/PKO port to auto configure
+ * @param ipd_port IPD/PKO port to auto configure
  *
- * Returns Link state after configure
+ * @return Link state after configure
  */
 cvmx_helper_link_info_t cvmx_helper_link_autoconf(int ipd_port)
 {
@@ -934,25 +1366,32 @@ cvmx_helper_link_info_t cvmx_helper_link_autoconf(int ipd_port)
 	int interface = cvmx_helper_get_interface_num(ipd_port);
 	int index = cvmx_helper_get_interface_index_num(ipd_port);
 
-	if (index >= cvmx_helper_ports_on_interface(interface)) {
+	if (interface == -1 || index == -1 ||
+	    index >= cvmx_helper_ports_on_interface(interface)) {
 		link_info.u64 = 0;
 		return link_info;
 	}
 
 	link_info = cvmx_helper_link_get(ipd_port);
-	if (link_info.u64 == port_link_info[ipd_port].u64)
+	if (link_info.u64 == (__cvmx_helper_get_link_info(interface, index)).u64)
 		return link_info;
 
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+	if (!link_info.s.link_up)
+		cvmx_error_disable_group(CVMX_ERROR_GROUP_ETHERNET, ipd_port);
+#endif
+
 	/* If we fail to set the link speed, port_link_info will not change */
 	cvmx_helper_link_set(ipd_port, link_info);
 
-	/*
-	 * port_link_info should be the current value, which will be
-	 * different than expect if cvmx_helper_link_set() failed.
-	 */
-	return port_link_info[ipd_port];
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+	if (link_info.s.link_up)
+		cvmx_error_enable_group(CVMX_ERROR_GROUP_ETHERNET, ipd_port);
+#endif
+
+	return link_info;
 }
-EXPORT_SYMBOL_GPL(cvmx_helper_link_autoconf);
+EXPORT_SYMBOL(cvmx_helper_link_autoconf);
 
 /**
  * Return the link state of an IPD/PKO port as returned by
@@ -960,58 +1399,34 @@ EXPORT_SYMBOL_GPL(cvmx_helper_link_autoconf);
  * Octeon's link config if auto negotiation has changed since
  * the last call to cvmx_helper_link_set().
  *
- * @ipd_port: IPD/PKO port to query
+ * @param ipd_port IPD/PKO port to query
  *
- * Returns Link state
+ * @return Link state
  */
 cvmx_helper_link_info_t cvmx_helper_link_get(int ipd_port)
 {
 	cvmx_helper_link_info_t result;
-	int interface = cvmx_helper_get_interface_num(ipd_port);
+	int xiface = cvmx_helper_get_interface_num(ipd_port);
 	int index = cvmx_helper_get_interface_index_num(ipd_port);
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
 
-	/* The default result will be a down link unless the code below
-	   changes it */
+	/*
+	 * The default result will be a down link unless the code
+	 * below changes it.
+	 */
 	result.u64 = 0;
 
-	if (index >= cvmx_helper_ports_on_interface(interface))
+	if (__cvmx_helper_xiface_is_null(xiface) || index == -1 ||
+	    index >= cvmx_helper_ports_on_interface(xiface))
 		return result;
 
-	switch (cvmx_helper_interface_get_mode(interface)) {
-	case CVMX_HELPER_INTERFACE_MODE_DISABLED:
-	case CVMX_HELPER_INTERFACE_MODE_PCIE:
-		/* Network links are not supported */
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_XAUI:
-		result = __cvmx_helper_xaui_link_get(ipd_port);
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_GMII:
-		if (index == 0)
-			result = __cvmx_helper_rgmii_link_get(ipd_port);
-		else {
-			result.s.full_duplex = 1;
-			result.s.link_up = 1;
-			result.s.speed = 1000;
-		}
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_RGMII:
-		result = __cvmx_helper_rgmii_link_get(ipd_port);
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_SPI:
-		result = __cvmx_helper_spi_link_get(ipd_port);
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_SGMII:
-	case CVMX_HELPER_INTERFACE_MODE_PICMG:
-		result = __cvmx_helper_sgmii_link_get(ipd_port);
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_NPI:
-	case CVMX_HELPER_INTERFACE_MODE_LOOP:
-		/* Network links are not supported */
-		break;
-	}
+	cvmx_helper_interface_get_mode(xiface);
+	if (iface_node_ops[xi.node][xi.interface]->link_get)
+		result = iface_node_ops[xi.node][xi.interface]->link_get(ipd_port);
+
 	return result;
 }
-EXPORT_SYMBOL_GPL(cvmx_helper_link_get);
+EXPORT_SYMBOL(cvmx_helper_link_get);
 
 /**
  * Configure an IPD/PKO port for the specified link state. This
@@ -1020,105 +1435,66 @@ EXPORT_SYMBOL_GPL(cvmx_helper_link_get);
  * by cvmx_helper_link_get(). It is normally best to use
  * cvmx_helper_link_autoconf() instead.
  *
- * @ipd_port:  IPD/PKO port to configure
- * @link_info: The new link state
+ * @param ipd_port  IPD/PKO port to configure
+ * @param link_info The new link state
  *
- * Returns Zero on success, negative on failure
+ * @return Zero on success, negative on failure
  */
 int cvmx_helper_link_set(int ipd_port, cvmx_helper_link_info_t link_info)
 {
 	int result = -1;
-	int interface = cvmx_helper_get_interface_num(ipd_port);
+	int xiface = cvmx_helper_get_interface_num(ipd_port);
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
 	int index = cvmx_helper_get_interface_index_num(ipd_port);
 
-	if (index >= cvmx_helper_ports_on_interface(interface))
+	if (__cvmx_helper_xiface_is_null(xiface) || index == -1 ||
+	    index >= cvmx_helper_ports_on_interface(xiface))
 		return -1;
 
-	switch (cvmx_helper_interface_get_mode(interface)) {
-	case CVMX_HELPER_INTERFACE_MODE_DISABLED:
-	case CVMX_HELPER_INTERFACE_MODE_PCIE:
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_XAUI:
-		result = __cvmx_helper_xaui_link_set(ipd_port, link_info);
-		break;
-		/*
-		 * RGMII/GMII/MII are all treated about the same. Most
-		 * functions refer to these ports as RGMII.
-		 */
-	case CVMX_HELPER_INTERFACE_MODE_RGMII:
-	case CVMX_HELPER_INTERFACE_MODE_GMII:
-		result = __cvmx_helper_rgmii_link_set(ipd_port, link_info);
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_SPI:
-		result = __cvmx_helper_spi_link_set(ipd_port, link_info);
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_SGMII:
-	case CVMX_HELPER_INTERFACE_MODE_PICMG:
-		result = __cvmx_helper_sgmii_link_set(ipd_port, link_info);
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_NPI:
-	case CVMX_HELPER_INTERFACE_MODE_LOOP:
-		break;
-	}
-	/* Set the port_link_info here so that the link status is updated
-	   no matter how cvmx_helper_link_set is called. We don't change
-	   the value if link_set failed */
+	cvmx_helper_interface_get_mode(xiface);
+	if (iface_node_ops[xi.node][xi.interface]->link_set)
+		result = iface_node_ops[xi.node][xi.interface]->link_set(ipd_port, link_info);
+
+	/*
+	 * Set the port_link_info here so that the link status is
+	 * updated no matter how cvmx_helper_link_set is called. We
+	 * don't change the value if link_set failed.
+	 */
 	if (result == 0)
-		port_link_info[ipd_port].u64 = link_info.u64;
+		__cvmx_helper_set_link_info(xiface, index, link_info);
 	return result;
 }
-EXPORT_SYMBOL_GPL(cvmx_helper_link_set);
+EXPORT_SYMBOL(cvmx_helper_link_set);
 
 /**
  * Configure a port for internal and/or external loopback. Internal loopback
  * causes packets sent by the port to be received by Octeon. External loopback
  * causes packets received from the wire to sent out again.
  *
- * @ipd_port: IPD/PKO port to loopback.
- * @enable_internal:
- *		   Non zero if you want internal loopback
- * @enable_external:
- *		   Non zero if you want external loopback
+ * @param ipd_port IPD/PKO port to loopback.
+ * @param enable_internal
+ *                 Non zero if you want internal loopback
+ * @param enable_external
+ *                 Non zero if you want external loopback
  *
- * Returns Zero on success, negative on failure.
+ * @return Zero on success, negative on failure.
  */
 int cvmx_helper_configure_loopback(int ipd_port, int enable_internal,
 				   int enable_external)
 {
 	int result = -1;
-	int interface = cvmx_helper_get_interface_num(ipd_port);
+	int xiface = cvmx_helper_get_interface_num(ipd_port);
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
 	int index = cvmx_helper_get_interface_index_num(ipd_port);
 
-	if (index >= cvmx_helper_ports_on_interface(interface))
+	if (index >= cvmx_helper_ports_on_interface(xiface))
 		return -1;
 
-	switch (cvmx_helper_interface_get_mode(interface)) {
-	case CVMX_HELPER_INTERFACE_MODE_DISABLED:
-	case CVMX_HELPER_INTERFACE_MODE_PCIE:
-	case CVMX_HELPER_INTERFACE_MODE_SPI:
-	case CVMX_HELPER_INTERFACE_MODE_NPI:
-	case CVMX_HELPER_INTERFACE_MODE_LOOP:
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_XAUI:
-		result =
-		    __cvmx_helper_xaui_configure_loopback(ipd_port,
-							  enable_internal,
-							  enable_external);
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_RGMII:
-	case CVMX_HELPER_INTERFACE_MODE_GMII:
-		result =
-		    __cvmx_helper_rgmii_configure_loopback(ipd_port,
-							   enable_internal,
-							   enable_external);
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_SGMII:
-	case CVMX_HELPER_INTERFACE_MODE_PICMG:
-		result =
-		    __cvmx_helper_sgmii_configure_loopback(ipd_port,
-							   enable_internal,
-							   enable_external);
-		break;
-	}
+	cvmx_helper_interface_get_mode(xiface);
+	if (iface_node_ops[xi.node][xi.interface]->loopback)
+		result = iface_node_ops[xi.node][xi.interface]->loopback(ipd_port,
+									 enable_internal,
+									 enable_external);
+
 	return result;
 }
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pki-resources.c b/arch/mips/cavium-octeon/executive/cvmx-pki-resources.c
new file mode 100644
index 0000000..5ab9abd
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-pki-resources.c
@@ -0,0 +1,402 @@
+/***********************license start***************
+ * Copyright (c) 2003-2010  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * PKI Support.
+ */
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include <linux/module.h>
+#include <asm/octeon/cvmx.h>
+#include <asm/octeon/cvmx-pki-defs.h>
+#include <asm/octeon/cvmx-pki.h>
+#include "asm/octeon/cvmx-global-resources.h"
+#include "asm/octeon/cvmx-range.h"
+#else
+#include "cvmx.h"
+#include "cvmx-version.h"
+#include "cvmx-error.h"
+#include "cvmx-pki.h"
+#include "cvmx-global-resources.h"
+#include "cvmx-range.h"
+#endif
+
+/**
+ * This function allocates/reserves a style from pool of global styles per node.
+ * @param node	node to allocate style from.
+ * @param style	style to allocate, if -1 it will be allocated
+ *		first available style from style resource. If index is positive
+ *		number and in range, it will try to allocate specified style.
+ * @return 	style number on success,
+ *		-1 on alloc failure.
+ *		-2 on resource already reserved.
+ */
+int cvmx_pki_style_alloc(int node, int style)
+{
+	int rs;
+
+	if (cvmx_create_global_resource_range(CVMX_GR_TAG_STYLE(node), CVMX_PKI_NUM_INTERNAL_STYLE)) {
+		cvmx_dprintf("ERROR: Failed to create styles global resource\n");
+		return -1;
+	}
+	if (style >= 0) {
+		rs = cvmx_reserve_global_resource_range(CVMX_GR_TAG_STYLE(node), style, style, 1);
+		if (rs == -1) {
+			cvmx_dprintf("INFO: style %d is already reserved\n", (int)style);
+			return CVMX_RESOURCE_ALREADY_RESERVED;
+		}
+	} else {
+		rs = cvmx_allocate_global_resource_range(CVMX_GR_TAG_STYLE(node), style, 1, 1);
+		if (rs == -1) {
+			cvmx_dprintf("ERROR: Failed to allocate style\n");
+			return CVMX_RESOURCE_ALLOC_FAILED;
+		}
+	}
+	style = rs;
+	return style;
+}
+
+/**
+ * This function frees a style from pool of global styles per node.
+ * @param node	 node to free style from.
+ * @param style	 style to free
+ * @return 	 0 on success, -1 on failure.
+ */
+int cvmx_pki_style_free(int node, int style)
+{
+	if (cvmx_free_global_resource_range_with_base(CVMX_GR_TAG_STYLE(node), style, 1) == -1) {
+		cvmx_dprintf("ERROR Failed to release style %d\n", (int)style);
+		return -1;
+	}
+	return 0;
+}
+
+
+/**
+ * This function allocates/reserves a cluster group from per node
+   cluster group resources.
+ * @param node	 	node to allocate cluster group from.
+   @param cl_grp	cluster group to allocate/reserve, if -1 ,
+ *			allocate any available cluster group.
+ * @return 	 	cluster group number
+ *			-1 on alloc failure.
+ *			-2 on resource already reserved.
+ */
+int cvmx_pki_cluster_grp_alloc(int node, int cl_grp)
+{
+	int rs;
+
+	if (node >= CVMX_MAX_NODES) {
+		cvmx_dprintf("Invalid node number %d\n", node);
+		return -1;
+	}
+	if (cvmx_create_global_resource_range(CVMX_GR_TAG_CLUSTER_GRP(node), CVMX_PKI_NUM_CLUSTER_GROUP)) {
+		cvmx_dprintf("Failed to create Cluster group global resource\n");
+		return -1;
+	}
+	if (cl_grp >= 0) {
+		rs = cvmx_reserve_global_resource_range(CVMX_GR_TAG_CLUSTER_GRP(node), 0, cl_grp, 1);
+		if (rs == -1) {
+			cvmx_dprintf("INFO: cl_grp %d is already reserved\n", (int)cl_grp);
+			return CVMX_RESOURCE_ALREADY_RESERVED;
+		}
+	} else {
+		rs = cvmx_allocate_global_resource_range(CVMX_GR_TAG_CLUSTER_GRP(node), 0, 1, 1);
+		if (rs == -1) {
+			cvmx_dprintf("Warning: Failed to alloc cluster grp\n");
+			return CVMX_RESOURCE_ALLOC_FAILED;
+		}
+	}
+	cl_grp = rs;
+	return cl_grp;
+}
+
+/**
+ * This function frees a cluster group from per node
+   cluster group resources.
+ * @param node	 	node to free cluster group from.
+   @param cl_grp	cluster group to free
+ * @return 	 	0 on success
+ *			-1 on alloc failure.
+ *			-2 on resource already reserved.
+ */
+int cvmx_pki_cluster_grp_free(int node, int cl_grp)
+{
+	if (cvmx_free_global_resource_range_with_base(CVMX_GR_TAG_CLUSTER_GRP(node), cl_grp, 1) == -1) {
+		cvmx_dprintf("ERROR Failed to release cluster group %d\n", (int)cl_grp);
+		return -1;
+	}
+	return 0;
+}
+
+/**
+ * This function allocates/reserves a cluster from per node
+ * cluster resources.
+ * @param node	 	node to allocate cluster group from.
+ * @param cluster_mask	mask of clusters  to allocate/reserve, if -1 ,
+ *			allocate any available clusters.
+ * @param num_clusters	number of clusters that will be allocated
+ */
+int cvmx_pki_cluster_alloc(int node, int num_clusters, uint64_t *cluster_mask)
+{
+	int cluster = 0;
+	int clusters[CVMX_PKI_NUM_CLUSTER];
+
+	if (node >= CVMX_MAX_NODES) {
+		cvmx_dprintf("Invalid node number %d\n", node);
+		return -1;
+	}
+	if (cvmx_create_global_resource_range(CVMX_GR_TAG_CLUSTERS(node), CVMX_PKI_NUM_CLUSTER)) {
+		cvmx_dprintf("Failed to create Clusters global resource\n");
+		return -1;
+	}
+	if (*cluster_mask > 0) {
+		while (cluster < CVMX_PKI_NUM_CLUSTER) {
+			if (*cluster_mask & (0x01L << cluster)) {
+				if (cvmx_reserve_global_resource_range(CVMX_GR_TAG_CLUSTERS(node), 0, cluster, 1) == -1) {
+					cvmx_dprintf("ERROR: allocating cluster %d\n", cluster);
+					return -1;
+				}
+			}
+			cluster++;
+		}
+	} else {
+		if (cvmx_resource_alloc_many(CVMX_GR_TAG_CLUSTERS(node), 0, num_clusters, clusters) == -1) {
+			   cvmx_dprintf("ERROR: allocating clusters\n");
+			   return -1;
+		}
+		*cluster_mask = 0;
+		while (num_clusters--)
+			*cluster_mask |= (0x1ul << clusters[num_clusters]);
+	}
+	return 0;
+}
+
+/**
+ * This function frees  clusters  from per node
+   clusters resources.
+ * @param node	 	node to free clusters from.
+ * @param cluster_mask  mask of clusters need freeing
+ * @return 	 	0 on success or -1 on failure
+ */
+int cvmx_pki_cluster_free(int node, uint64_t cluster_mask)
+{
+	int cluster = 0;
+	if (cluster_mask > 0) {
+		while (cluster < CVMX_PKI_NUM_CLUSTER) {
+			if (cluster_mask & (0x01L << cluster)) {
+				if (cvmx_free_global_resource_range_with_base(
+						CVMX_GR_TAG_CLUSTERS(node), cluster, 1) == -1) {
+					cvmx_dprintf("ERROR: freeing cluster %d\n", cluster);
+					return -1;
+				}
+			}
+			cluster++;
+		}
+	}
+	return 0;
+}
+
+/**
+ * This function allocates/reserves a pcam entry from node
+ * @param node	 	node to allocate pcam entry from.
+ * @param index  	index of pacm entry (0-191), if -1 ,
+ *			allocate any available pcam entry.
+ * @param bank		pcam bank where to allocate/reserve pcan entry from
+ * @param cluster_mask  mask of clusters from which pcam entry is needed.
+ * @return 	 	pcam entry of -1 on failure
+ */
+int cvmx_pki_pcam_entry_alloc(int node, int index, int bank, uint64_t cluster_mask)
+{
+	int rs = 0;
+	uint64_t cluster = 0;
+
+	while (cluster < CVMX_PKI_NUM_CLUSTER) {
+		if (cluster_mask & (0x01L << cluster)) {
+			if (cvmx_create_global_resource_range(CVMX_GR_TAG_PCAM(node, cluster, bank),
+				CVMX_PKI_TOTAL_PCAM_ENTRY)) {
+				cvmx_dprintf("Failed to create pki pcam global resource\n");
+				return -1;
+			}
+			if (index >= 0)
+				rs = cvmx_reserve_global_resource_range(CVMX_GR_TAG_PCAM(node, cluster, bank),
+						cluster, index, 1);
+			else
+				rs = cvmx_allocate_global_resource_range(CVMX_GR_TAG_PCAM(node, cluster, bank),
+						cluster, 1, 1);
+			if (rs == -1) {
+				cvmx_dprintf("Error:index %d not available in cluster %d bank %d",
+						(int)index, (int)cluster, bank);
+				return -1;
+			}
+			cluster++;
+		}
+	}
+	index = rs;
+	/*vinita to_do , implement cluster handle, for now assume
+	all clusters will have same base index*/
+	return index;
+}
+
+/**
+ * This function frees a pcam entry from node
+ * @param node	 	node to allocate pcam entry from.
+   @param index  	index of pacm entry (0-191) needs to be freed.
+ * @param bank		pcam bank where to free pcam entry from
+ * @param cluster_mask  mask of clusters from which pcam entry is freed.
+ * @return 	 	0 on success OR -1 on failure
+ */
+int cvmx_pki_pcam_entry_free(int node, int index, int bank, uint64_t cluster_mask)
+{
+	uint64_t cluster = 0;
+
+	while (cluster < CVMX_PKI_NUM_CLUSTER) {
+		if (cluster_mask & (0x01L << cluster)) {
+			if (cvmx_free_global_resource_range_with_base (
+						CVMX_GR_TAG_PCAM(node, cluster, bank), index, 1) == -1) {
+				cvmx_dprintf("ERROR: freeing cluster %d\n", (int)cluster);
+				return -1;
+			}
+			cluster++;
+		}
+	}
+	return 0;
+}
+
+
+/**
+ * This function allocates/reserves QPG table entries per node.
+ * @param node	 	node number.
+ * @param base_offset	base_offset in qpg table. If -1, first available
+ *			qpg base_offset will be allocated. If base_offset is positive
+ *		 	number and in range, it will try to allocate specified base_offset.
+ * @param count		number of consecutive qpg entries to allocate. They will be consecutive
+ *                       from base offset.
+ * @return 	 	qpg table base offset number on success
+ *			-1 on alloc failure.
+ *			-2 on resource already reserved.
+ */
+int cvmx_pki_qpg_entry_alloc(int node, int base_offset, int count)
+{
+	int rs;
+
+	if (cvmx_create_global_resource_range(CVMX_GR_TAG_QPG_ENTRY(node), CVMX_PKI_NUM_QPG_ENTRY)) {
+		cvmx_dprintf("\nERROR: Failed to create qpg_entry global resource\n");
+		return -1;
+	}
+	if (base_offset >= 0) {
+		rs = cvmx_reserve_global_resource_range(CVMX_GR_TAG_QPG_ENTRY(node),
+				base_offset, base_offset, count);
+		if (rs == -1) {
+			cvmx_dprintf("\nINFO: qpg entry %d is already reserved\n", (int)base_offset);
+			return CVMX_RESOURCE_ALREADY_RESERVED;
+		}
+	} else {
+		rs = cvmx_allocate_global_resource_range(CVMX_GR_TAG_QPG_ENTRY(node), base_offset, count, 1);
+		if (rs == -1) {
+			cvmx_dprintf("ERROR: Failed to allocate qpg entry\n");
+			return CVMX_RESOURCE_ALLOC_FAILED;
+		}
+	}
+	base_offset = rs;
+	return base_offset;
+}
+
+/**
+ * This function frees QPG table entries per node.
+ * @param node	 	node number.
+ * @param base_offset	base_offset in qpg table. If -1, first available
+ *			qpg base_offset will be allocated. If base_offset is positive
+ *		 	number and in range, it will try to allocate specified base_offset.
+ * @param count		number of consecutive qpg entries to allocate. They will be consecutive
+ *			from base offset.
+ * @return 	 	qpg table base offset number on success, -1 on failure.
+ */
+int cvmx_pki_qpg_entry_free(int node, int base_offset, int count)
+{
+	if (cvmx_free_global_resource_range_with_base(CVMX_GR_TAG_QPG_ENTRY(node), base_offset, count) == -1) {
+		cvmx_dprintf("\nERROR Failed to release qpg offset %d", (int)base_offset);
+		return -1;
+	}
+	return 0;
+}
+
+/**
+ * This function frees all the PKI software resources
+ * (clusters, styles, qpg_entry, pcam_entry etc) for the specified node
+ */
+void __cvmx_pki_global_rsrc_free(int node)
+{
+	int cnt;
+	int cluster, bank;
+
+	cnt = CVMX_PKI_NUM_CLUSTER_GROUP;
+	if (cvmx_free_global_resource_range_with_base(CVMX_GR_TAG_CLUSTER_GRP(node), 0, cnt) == -1) {
+		cvmx_dprintf("pki-rsrc:ERROR Failed to release all styles\n");
+	}
+
+#if 0
+	cnt = CVMX_PKI_NUM_CLUSTER;
+	if (cvmx_free_global_resource_range_with_base(CVMX_GR_TAG_CLUSTERS(node), 0, cnt) == -1) {
+		cvmx_dprintf("pki-rsrc:ERROR Failed to release all clusters\n");
+	}
+#endif
+
+	cnt = CVMX_PKI_NUM_FINAL_STYLE;
+	if (cvmx_free_global_resource_range_with_base(CVMX_GR_TAG_STYLE(node), 0, cnt) == -1) {
+		cvmx_dprintf("pki-rsrc:ERROR Failed to release all styles\n");
+	}
+
+	cnt = CVMX_PKI_NUM_QPG_ENTRY;
+	if (cvmx_free_global_resource_range_with_base(CVMX_GR_TAG_QPG_ENTRY(node), 0, cnt) == -1) {
+		cvmx_dprintf("pki-rsrc:ERROR Failed to release all qpg entries\n");
+	}
+
+	cnt = CVMX_PKI_NUM_PCAM_ENTRY;
+	for (cluster = 0; cluster < CVMX_PKI_NUM_CLUSTER; cluster++) {
+		for (bank = 0; bank < CVMX_PKI_NUM_PCAM_BANK; bank++) {
+			if (cvmx_free_global_resource_range_with_base (
+				CVMX_GR_TAG_PCAM(node, cluster, bank), 0, cnt) == -1) {
+				cvmx_dprintf("pki-rsrc:ERROR Failed to release all pcan entries\n");
+			}
+		}
+	}
+
+}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pki.c b/arch/mips/cavium-octeon/executive/cvmx-pki.c
new file mode 100644
index 0000000..d6bea5f
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-pki.c
@@ -0,0 +1,1007 @@
+/***********************license start***************
+ * Copyright (c) 2003-2010  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * PKI Support.
+ */
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include <linux/module.h>
+#include <asm/octeon/cvmx.h>
+#include <asm/octeon/cvmx-pki-defs.h>
+#include <asm/octeon/cvmx-pki.h>
+#include <asm/octeon/cvmx-fpa3.h>
+#include <asm/octeon/cvmx-pki-cluster.h>
+#include <asm/octeon/cvmx-pki-resources.h>
+#else
+#include "cvmx.h"
+#include "cvmx-version.h"
+#include "cvmx-error.h"
+#include "cvmx-pki-defs.h"
+#include "cvmx-pki.h"
+#include "cvmx-fpa3.h"
+#include "cvmx-pki-resources.h"
+#include "cvmx-pki-cluster.h"
+#endif
+
+
+/**
+ * This function enables pki
+ * @param node	node to enable pki in.
+ */
+void cvmx_pki_enable(int node)
+{
+
+	cvmx_pki_sft_rst_t pki_sft_rst;
+	cvmx_pki_buf_ctl_t pki_en;
+
+	pki_sft_rst.u64 = cvmx_read_csr_node(node, CVMX_PKI_SFT_RST);
+
+	while (pki_sft_rst.s.busy != 0)
+		pki_sft_rst.u64 = cvmx_read_csr_node(node, CVMX_PKI_SFT_RST);
+
+	pki_en.u64 = cvmx_read_csr_node(node, CVMX_PKI_BUF_CTL);
+	if (pki_en.s.pki_en)
+		cvmx_dprintf("Warning: Enabling PKI when PKI already enabled.\n");
+
+	pki_en.s.pki_en = 1;
+
+	cvmx_write_csr_node(node, CVMX_PKI_BUF_CTL, pki_en.u64);
+
+}
+EXPORT_SYMBOL(cvmx_pki_enable);
+
+/**
+ * This function disables pki
+ * @param node	node to disable pki in.
+ */
+void cvmx_pki_disable(int node)
+{
+	cvmx_pki_buf_ctl_t pki_en;
+	pki_en.u64 = cvmx_read_csr_node(node, CVMX_PKI_BUF_CTL);
+	pki_en.s.pki_en = 0;
+	cvmx_write_csr_node(node, CVMX_PKI_BUF_CTL, pki_en.u64);
+
+}
+EXPORT_SYMBOL(cvmx_pki_disable);
+
+/**
+ * This function soft resets pki
+ * @param node	node to enable pki in.
+ */
+void cvmx_pki_reset(int node)
+{
+	cvmx_pki_sft_rst_t pki_sft_rst;
+
+	pki_sft_rst.u64 = cvmx_read_csr_node(node, CVMX_PKI_SFT_RST);
+
+	while (pki_sft_rst.s.active != 0)
+		pki_sft_rst.u64 = cvmx_read_csr_node(node, CVMX_PKI_SFT_RST);
+	pki_sft_rst.s.rst = 1;
+	cvmx_write_csr_node(node, CVMX_PKI_SFT_RST, pki_sft_rst.u64);
+	while (pki_sft_rst.s.busy != 0)
+		pki_sft_rst.u64 = cvmx_read_csr_node(node, CVMX_PKI_SFT_RST);
+}
+
+/**
+ * This function sets the clusters in PKI
+ * @param node	node to set clusters in.
+ */
+int cvmx_pki_setup_clusters(int node)
+{
+	int i;
+
+	for (i = 0; i < cvmx_pki_cluster_code_length; i++)
+		cvmx_write_csr_node(node, CVMX_PKI_IMEMX(i), cvmx_pki_cluster_code_default[i]);
+
+	return 0;
+}
+EXPORT_SYMBOL(cvmx_pki_setup_clusters);
+
+/**
+ * @INTERNAL
+ * This function is called by cvmx_helper_shutdown() to extract
+ * all FPA buffers out of the PKI. After this function
+ * completes, all FPA buffers that were prefetched by PKI
+ * wil be in the apropriate FPA pool. This functions does not reset
+ * PKI.
+ * WARNING: It is very important that PKI be
+ * reset soon after a call to this function.
+ */
+void __cvmx_pki_free_ptr(int node)
+{
+	cvmx_pki_buf_ctl_t buf_ctl;
+	buf_ctl.u64 = cvmx_read_csr_node(node, CVMX_PKI_BUF_CTL);
+	/*Disable buffering any data*/
+	buf_ctl.s.pkt_off = 1;
+	/*diable caching of any data and return all the prefetched buffers to fpa*/
+	buf_ctl.s.fpa_cac_dis = 1;
+	cvmx_write_csr_node(node, CVMX_PKI_BUF_CTL, buf_ctl.u64);
+}
+
+void cvmx_pki_read_global_cfg(int node, struct cvmx_pki_global_config *gbl_cfg)
+{
+        cvmx_pki_stat_ctl_t stat_ctl;
+        cvmx_pki_icgx_cfg_t pki_cl_grp;
+        cvmx_pki_gbl_pen_t gbl_pen_reg;
+        cvmx_pki_tag_secret_t tag_secret_reg;
+        cvmx_pki_frm_len_chkx_t frm_len_chk;
+        int cl_grp;
+        int id;
+
+	stat_ctl.u64 = cvmx_read_csr_node(node, CVMX_PKI_STAT_CTL);
+        gbl_cfg->stat_mode = stat_ctl.s.mode;
+
+        for (cl_grp = 0; cl_grp < CVMX_PKI_NUM_CLUSTER_GROUP; cl_grp++) {
+                pki_cl_grp.u64 = cvmx_read_csr_node(node, CVMX_PKI_ICGX_CFG(cl_grp));
+                gbl_cfg->cluster_mask[cl_grp] = pki_cl_grp.s.clusters;
+        }
+	gbl_pen_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_GBL_PEN);
+	gbl_cfg->gbl_pen.virt_pen = gbl_pen_reg.s.virt_pen;
+	gbl_cfg->gbl_pen.clg_pen = gbl_pen_reg.s.clg_pen;
+	gbl_cfg->gbl_pen.cl2_pen = gbl_pen_reg.s.cl2_pen;
+	gbl_cfg->gbl_pen.l4_pen = gbl_pen_reg.s.l4_pen;
+	gbl_cfg->gbl_pen.il3_pen = gbl_pen_reg.s.il3_pen;
+	gbl_cfg->gbl_pen.l3_pen = gbl_pen_reg.s.l3_pen;
+	gbl_cfg->gbl_pen.mpls_pen = gbl_pen_reg.s.mpls_pen;
+	gbl_cfg->gbl_pen.fulc_pen = gbl_pen_reg.s.fulc_pen;
+	gbl_cfg->gbl_pen.dsa_pen = gbl_pen_reg.s.dsa_pen;
+	gbl_cfg->gbl_pen.hg_pen = gbl_pen_reg.s.hg_pen;
+
+	tag_secret_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_TAG_SECRET);
+	gbl_cfg->tag_secret.dst6 = tag_secret_reg.s.dst6;
+	gbl_cfg->tag_secret.src6 = tag_secret_reg.s.src6;
+	gbl_cfg->tag_secret.dst = tag_secret_reg.s.dst;
+	gbl_cfg->tag_secret.src = tag_secret_reg.s.src;
+
+        for (id = 0; id < CVMX_PKI_NUM_FRAME_CHECK; id++) {
+                frm_len_chk.u64 = cvmx_read_csr_node(node, CVMX_PKI_FRM_LEN_CHKX(id));
+                gbl_cfg->frm_len[id].maxlen = frm_len_chk.s.maxlen;
+                gbl_cfg->frm_len[id].minlen = frm_len_chk.s.minlen;
+        }
+	/* vinita_to_do remaining parameters */
+}
+
+void cvmx_pki_write_global_cfg(int node, struct cvmx_pki_global_config *gbl_cfg)
+{
+	cvmx_pki_stat_ctl_t stat_ctl;
+	int cl_grp;
+
+	for (cl_grp = 0; cl_grp < CVMX_PKI_NUM_CLUSTER_GROUP; cl_grp++)
+		cvmx_pki_attach_cluster_to_group(node, cl_grp, gbl_cfg->cluster_mask[cl_grp]);
+
+	stat_ctl.u64 = 0;
+	stat_ctl.s.mode = gbl_cfg->stat_mode;
+	cvmx_write_csr_node(node, CVMX_PKI_STAT_CTL, stat_ctl.u64);
+
+	cvmx_pki_write_global_parse(node, gbl_cfg->gbl_pen);
+	cvmx_pki_write_tag_secret(node, gbl_cfg->tag_secret);
+	cvmx_pki_write_frame_len(node, 0, gbl_cfg->frm_len[0]);
+	cvmx_pki_write_frame_len(node, 1, gbl_cfg->frm_len[1]);
+	/* vinita_to_do remaining parameters */
+}
+
+/**
+ * This function writes per pkind parameters in hardware which defines how
+  the incoming packet is processed.
+ * @param node		      node number.
+ * @param pkind               PKI supports a large number of incoming interfaces
+ *                            and packets arriving on different interfaces or channels
+ *                            may want to be processed differently. PKI uses the pkind to
+ *                            determine how the incoming packet is processed.
+ * @param pkind_cfg	      struct conatining pkind configuration need to be written to hw
+ */
+int cvmx_pki_set_pkind_config(int node, int pkind, struct cvmx_pki_pkind_config *pkind_cfg)
+{
+	int cluster = 0;
+	uint64_t cluster_mask;
+
+	cvmx_pki_pkindx_icgsel_t pkind_clsel;
+	cvmx_pki_clx_pkindx_style_t pkind_cfg_style;
+	cvmx_pki_icgx_cfg_t pki_cl_grp;
+	cvmx_pki_clx_pkindx_cfg_t pknd_cfg_reg;
+
+
+	if (pkind >= CVMX_PKI_NUM_PKIND || pkind_cfg->cluster_grp >= CVMX_PKI_NUM_CLUSTER_GROUP
+		  || pkind_cfg->initial_style >= CVMX_PKI_NUM_FINAL_STYLE) {
+		cvmx_dprintf("ERROR: Configuring PKIND pkind = %d cluster_group = %d style = %d\n",
+			     pkind, pkind_cfg->cluster_grp, pkind_cfg->initial_style);
+		return -1;
+	}
+	pkind_clsel.u64 = cvmx_read_csr_node(node, CVMX_PKI_PKINDX_ICGSEL(pkind));
+	pkind_clsel.s.icg = pkind_cfg->cluster_grp;
+	cvmx_write_csr_node(node, CVMX_PKI_PKINDX_ICGSEL(pkind), pkind_clsel.u64);
+
+	pki_cl_grp.u64 = cvmx_read_csr_node(node, CVMX_PKI_ICGX_CFG(pkind_cfg->cluster_grp));
+	cluster_mask = (uint64_t)pki_cl_grp.s.clusters;
+	while (cluster < CVMX_PKI_NUM_CLUSTER) {
+		if (cluster_mask & (0x01L << cluster)) {
+			pkind_cfg_style.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_STYLE(pkind, cluster));
+			pkind_cfg_style.s.pm = pkind_cfg->initial_parse_mode;
+			pkind_cfg_style.s.style = pkind_cfg->initial_style;
+			cvmx_write_csr_node(node, CVMX_PKI_CLX_PKINDX_STYLE(pkind, cluster), pkind_cfg_style.u64);
+			pknd_cfg_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_CFG(pkind, cluster));
+			pknd_cfg_reg.s.fcs_pres = pkind_cfg->fcs_pres;
+			pknd_cfg_reg.s.inst_hdr = pkind_cfg->parse_en.inst_hdr;
+			pknd_cfg_reg.s.mpls_en = pkind_cfg->parse_en.mpls_en;
+			pknd_cfg_reg.s.lg_custom = pkind_cfg->parse_en.lg_custom;
+			pknd_cfg_reg.s.fulc_en = pkind_cfg->parse_en.fulc_en;
+			pknd_cfg_reg.s.dsa_en = pkind_cfg->parse_en.dsa_en;
+			pknd_cfg_reg.s.hg2_en = pkind_cfg->parse_en.hg2_en;
+			pknd_cfg_reg.s.hg_en = pkind_cfg->parse_en.hg_en;
+			cvmx_write_csr_node(node, CVMX_PKI_CLX_PKINDX_CFG(pkind, cluster), pknd_cfg_reg.u64);
+		}
+		cluster++;
+	}
+	return 0;
+}
+
+/**
+ * This function writes/configures parameters associated with tag configuration in hardware.
+ * @param node	              node number.
+ * @param style		      style to configure tag for
+ * @param cluster_mask	      Mask of clusters to configure the style for.
+ * @param tag_cfg	      pointer to taf configuration struct.
+ */
+void cvmx_pki_write_tag_config(int node, int style, uint64_t cluster_mask,
+			       struct cvmx_pki_style_tag_cfg *tag_cfg)
+{
+	cvmx_pki_clx_stylex_cfg2_t style_cfg2_reg;
+	cvmx_pki_clx_stylex_alg_t style_alg_reg;
+	int cluster = 0;
+
+	while (cluster < CVMX_PKI_NUM_CLUSTER) {
+		if (cluster_mask & (0x01L << cluster)) {
+			style_cfg2_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG2(style, cluster));
+			style_cfg2_reg.s.tag_src_lg = tag_cfg->tag_fields.layer_g_src;
+			style_cfg2_reg.s.tag_src_lf = tag_cfg->tag_fields.layer_f_src;
+			style_cfg2_reg.s.tag_src_le = tag_cfg->tag_fields.layer_e_src;
+			style_cfg2_reg.s.tag_src_ld = tag_cfg->tag_fields.layer_d_src;
+			style_cfg2_reg.s.tag_src_lc = tag_cfg->tag_fields.layer_c_src;
+			style_cfg2_reg.s.tag_src_lb = tag_cfg->tag_fields.layer_b_src;
+			style_cfg2_reg.s.tag_dst_lg = tag_cfg->tag_fields.layer_g_dst;
+			style_cfg2_reg.s.tag_dst_lf = tag_cfg->tag_fields.layer_f_dst;
+			style_cfg2_reg.s.tag_dst_le = tag_cfg->tag_fields.layer_e_dst;
+			style_cfg2_reg.s.tag_dst_ld = tag_cfg->tag_fields.layer_d_dst;
+			style_cfg2_reg.s.tag_dst_lc = tag_cfg->tag_fields.layer_c_dst;
+			style_cfg2_reg.s.tag_dst_lb = tag_cfg->tag_fields.layer_b_dst;
+			cvmx_write_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG2(style, cluster), style_cfg2_reg.u64);
+			style_alg_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_ALG(style, cluster));
+			style_alg_reg.s.tag_vni = tag_cfg->tag_fields.tag_vni;
+			style_alg_reg.s.tag_gtp = tag_cfg->tag_fields.tag_gtp;
+			style_alg_reg.s.tag_spi = tag_cfg->tag_fields.tag_spi;
+			style_alg_reg.s.tag_syn = tag_cfg->tag_fields.tag_sync;
+			style_alg_reg.s.tag_pctl = tag_cfg->tag_fields.ip_prot_nexthdr;
+			style_alg_reg.s.tag_vs1 = tag_cfg->tag_fields.second_vlan;
+			style_alg_reg.s.tag_vs0 = tag_cfg->tag_fields.first_vlan;
+			style_alg_reg.s.tag_mpls0 = tag_cfg->tag_fields.mpls_label;
+			style_alg_reg.s.tag_prt = tag_cfg->tag_fields.input_port;
+			cvmx_write_csr_node(node, CVMX_PKI_CLX_STYLEX_ALG(style, cluster), style_alg_reg.u64);
+
+			/* vinita_to_do add mask tag */
+		}
+		cluster++;
+	}
+}
+
+
+/**
+ * This function writes/configures parameters associated with style in hardware.
+ * @param node	              node number.
+ * @param style		      style to configure.
+ * @param cluster_mask	      Mask of clusters to configure the style for.
+ * @param style_cfg	      pointer to style config struct.
+ */
+void cvmx_pki_set_style_config(int node, uint64_t style, uint64_t cluster_mask,
+			    struct cvmx_pki_style_config *style_cfg)
+{
+	cvmx_pki_clx_stylex_cfg_t style_cfg_reg;
+	cvmx_pki_clx_stylex_cfg2_t style_cfg2_reg;
+	cvmx_pki_clx_stylex_alg_t style_alg_reg;
+	cvmx_pki_stylex_buf_t     style_buf_reg;
+	int cluster = 0;
+
+	while (cluster < CVMX_PKI_NUM_CLUSTER) {
+		if (cluster_mask & (0x01L << cluster)) {
+			style_cfg_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(style, cluster));
+			style_cfg_reg.s.ip6_udp_opt = style_cfg->parm_cfg.ip6_udp_opt;
+			style_cfg_reg.s.lenerr_en = style_cfg->parm_cfg.lenerr_en;
+			style_cfg_reg.s.lenerr_eqpad = style_cfg->parm_cfg.lenerr_eqpad;
+			style_cfg_reg.s.maxerr_en = style_cfg->parm_cfg.maxerr_en;
+			style_cfg_reg.s.minerr_en = style_cfg->parm_cfg.minerr_en;
+			style_cfg_reg.s.fcs_chk = style_cfg->parm_cfg.fcs_chk;
+			style_cfg_reg.s.fcs_strip = style_cfg->parm_cfg.fcs_strip;
+			style_cfg_reg.s.minmax_sel = style_cfg->parm_cfg.minmax_sel;
+			style_cfg_reg.s.qpg_base = style_cfg->parm_cfg.qpg_base;
+			style_cfg_reg.s.qpg_dis_padd = style_cfg->parm_cfg.qpg_dis_padd;
+			style_cfg_reg.s.qpg_dis_aura = style_cfg->parm_cfg.qpg_dis_aura;
+			style_cfg_reg.s.qpg_dis_grp = style_cfg->parm_cfg.qpg_dis_grp;
+			style_cfg_reg.s.qpg_dis_grptag = style_cfg->parm_cfg.qpg_dis_grptag;
+			style_cfg_reg.s.rawdrp = style_cfg->parm_cfg.rawdrp;
+			style_cfg_reg.s.drop = style_cfg->parm_cfg.force_drop;
+			style_cfg_reg.s.nodrop = style_cfg->parm_cfg.nodrop;
+			cvmx_write_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(style, cluster), style_cfg_reg.u64);
+
+			style_cfg2_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG2(style, cluster));
+			style_cfg2_reg.s.len_lg = style_cfg->parm_cfg.len_lg;
+			style_cfg2_reg.s.len_lf = style_cfg->parm_cfg.len_lf;
+			style_cfg2_reg.s.len_le = style_cfg->parm_cfg.len_le;
+			style_cfg2_reg.s.len_ld = style_cfg->parm_cfg.len_ld;
+			style_cfg2_reg.s.len_lc = style_cfg->parm_cfg.len_lc;
+			style_cfg2_reg.s.len_lb = style_cfg->parm_cfg.len_lb;
+			style_cfg2_reg.s.csum_lg = style_cfg->parm_cfg.csum_lg;
+			style_cfg2_reg.s.csum_lf = style_cfg->parm_cfg.csum_lf;
+			style_cfg2_reg.s.csum_le = style_cfg->parm_cfg.csum_le;
+			style_cfg2_reg.s.csum_ld = style_cfg->parm_cfg.csum_ld;
+			style_cfg2_reg.s.csum_lc = style_cfg->parm_cfg.csum_lc;
+			style_cfg2_reg.s.csum_lb = style_cfg->parm_cfg.csum_lb;
+			cvmx_write_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG2(style, cluster), style_cfg2_reg.u64);
+
+			style_alg_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_ALG(style, cluster));
+			style_alg_reg.s.qpg_qos = style_cfg->parm_cfg.qpg_qos;
+			style_alg_reg.s.tt = style_cfg->parm_cfg.tag_type;
+			style_alg_reg.s.apad_nip = style_cfg->parm_cfg.apad_nip;
+			style_alg_reg.s.qpg_port_sh = style_cfg->parm_cfg.qpg_port_sh;
+			style_alg_reg.s.qpg_port_msb = style_cfg->parm_cfg.qpg_port_msb;
+			style_alg_reg.s.wqe_vs = style_cfg->parm_cfg.wqe_vs;
+			cvmx_write_csr_node(node, CVMX_PKI_CLX_STYLEX_ALG(style, cluster), style_alg_reg.u64);
+
+			cvmx_pki_write_tag_config(node, style, cluster_mask, &style_cfg->tag_cfg);
+		}
+		cluster++;
+	}
+	style_buf_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_STYLEX_BUF(style));
+	style_buf_reg.s.pkt_lend = style_cfg->parm_cfg.pkt_lend;
+	style_buf_reg.s.wqe_hsz = style_cfg->parm_cfg.wqe_hsz;
+	style_buf_reg.s.wqe_skip = (style_cfg->parm_cfg.wqe_skip)/128;
+	style_buf_reg.s.first_skip = (style_cfg->parm_cfg.first_skip)/8;
+	style_buf_reg.s.later_skip = style_cfg->parm_cfg.later_skip/8;
+	style_buf_reg.s.opc_mode = style_cfg->parm_cfg.cache_mode;
+	style_buf_reg.s.mb_size = (style_cfg->parm_cfg.mbuff_size)/8;
+	style_buf_reg.s.dis_wq_dat = style_cfg->parm_cfg.dis_wq_dat;
+	cvmx_write_csr_node(node, CVMX_PKI_STYLEX_BUF(style), style_buf_reg.u64);
+}
+
+void cvmx_pki_get_tag_config(int node, int style, uint64_t cluster_mask,
+			       struct cvmx_pki_style_tag_cfg *tag_cfg)
+{
+	cvmx_pki_clx_stylex_cfg2_t style_cfg2_reg;
+	cvmx_pki_clx_stylex_alg_t style_alg_reg;
+	int cluster = __builtin_ffsll(cluster_mask) - 1;
+
+	style_cfg2_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG2(style, cluster));
+	style_alg_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_ALG(style, cluster));
+
+	tag_cfg->tag_fields.layer_g_src = style_cfg2_reg.s.tag_src_lg;
+	tag_cfg->tag_fields.layer_f_src = style_cfg2_reg.s.tag_src_lf;
+	tag_cfg->tag_fields.layer_e_src = style_cfg2_reg.s.tag_src_le;
+	tag_cfg->tag_fields.layer_d_src = style_cfg2_reg.s.tag_src_ld;
+	tag_cfg->tag_fields.layer_c_src = style_cfg2_reg.s.tag_src_lc;
+	tag_cfg->tag_fields.layer_b_src = style_cfg2_reg.s.tag_src_lb;
+	tag_cfg->tag_fields.layer_g_dst = style_cfg2_reg.s.tag_dst_lg;
+	tag_cfg->tag_fields.layer_f_dst = style_cfg2_reg.s.tag_dst_lf;
+	tag_cfg->tag_fields.layer_e_dst = style_cfg2_reg.s.tag_dst_le;
+	tag_cfg->tag_fields.layer_d_dst = style_cfg2_reg.s.tag_dst_ld;
+	tag_cfg->tag_fields.layer_c_dst = style_cfg2_reg.s.tag_dst_lc;
+	tag_cfg->tag_fields.layer_b_dst = style_cfg2_reg.s.tag_dst_lb;
+	tag_cfg->tag_fields.tag_vni = style_alg_reg.s.tag_vni;
+	tag_cfg->tag_fields.tag_gtp = style_alg_reg.s.tag_gtp;
+	tag_cfg->tag_fields.tag_spi = style_alg_reg.s.tag_spi;
+	tag_cfg->tag_fields.tag_sync = style_alg_reg.s.tag_syn;
+	tag_cfg->tag_fields.ip_prot_nexthdr = style_alg_reg.s.tag_pctl;
+	tag_cfg->tag_fields.second_vlan = style_alg_reg.s.tag_vs1;
+	tag_cfg->tag_fields.first_vlan = style_alg_reg.s.tag_vs0;
+	tag_cfg->tag_fields.mpls_label = style_alg_reg.s.tag_mpls0;
+	tag_cfg->tag_fields.input_port = style_alg_reg.s.tag_prt;
+
+	/** vinita_to_do get mask tag*/
+}
+
+
+void cvmx_pki_get_style_config(int node, int style, uint64_t cluster_mask,
+			       struct cvmx_pki_style_config *style_cfg)
+{
+	cvmx_pki_clx_stylex_cfg_t style_cfg_reg;
+	cvmx_pki_clx_stylex_cfg2_t style_cfg2_reg;
+	cvmx_pki_clx_stylex_alg_t style_alg_reg;
+	cvmx_pki_stylex_buf_t     style_buf_reg;
+	int cluster = __builtin_ffsll(cluster_mask) - 1;
+
+	style_cfg_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(style, cluster));
+	style_cfg2_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG2(style, cluster));
+	style_alg_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_ALG(style, cluster));
+        style_buf_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_STYLEX_BUF(style));
+
+	style_cfg->parm_cfg.ip6_udp_opt = style_cfg_reg.s.ip6_udp_opt;
+	style_cfg->parm_cfg.lenerr_en = style_cfg_reg.s.lenerr_en;
+	style_cfg->parm_cfg.lenerr_eqpad = style_cfg_reg.s.lenerr_eqpad;
+	style_cfg->parm_cfg.maxerr_en = style_cfg_reg.s.maxerr_en;
+	style_cfg->parm_cfg.minerr_en = style_cfg_reg.s.minerr_en;
+	style_cfg->parm_cfg.fcs_chk = style_cfg_reg.s.fcs_chk;
+	style_cfg->parm_cfg.fcs_strip = style_cfg_reg.s.fcs_strip;
+	style_cfg->parm_cfg.minmax_sel = style_cfg_reg.s.minmax_sel;
+	style_cfg->parm_cfg.qpg_base = style_cfg_reg.s.qpg_base;
+	style_cfg->parm_cfg.qpg_dis_padd = style_cfg_reg.s.qpg_dis_padd;
+	style_cfg->parm_cfg.qpg_dis_aura = style_cfg_reg.s.qpg_dis_aura;
+	style_cfg->parm_cfg.qpg_dis_grp = style_cfg_reg.s.qpg_dis_grp;
+	style_cfg->parm_cfg.qpg_dis_grptag = style_cfg_reg.s.qpg_dis_grptag;
+	style_cfg->parm_cfg.rawdrp = style_cfg_reg.s.rawdrp;
+	style_cfg->parm_cfg.force_drop = style_cfg_reg.s.drop;
+	style_cfg->parm_cfg.nodrop = style_cfg_reg.s.nodrop;
+
+	style_cfg->parm_cfg.len_lg = style_cfg2_reg.s.len_lg;
+	style_cfg->parm_cfg.len_lf = style_cfg2_reg.s.len_lf;
+	style_cfg->parm_cfg.len_le = style_cfg2_reg.s.len_le;
+	style_cfg->parm_cfg.len_ld = style_cfg2_reg.s.len_ld;
+	style_cfg->parm_cfg.len_lc = style_cfg2_reg.s.len_lc;
+	style_cfg->parm_cfg.len_lb = style_cfg2_reg.s.len_lb;
+	style_cfg->parm_cfg.csum_lg = style_cfg2_reg.s.csum_lg;
+	style_cfg->parm_cfg.csum_lf = style_cfg2_reg.s.csum_lf;
+	style_cfg->parm_cfg.csum_le = style_cfg2_reg.s.csum_le;
+	style_cfg->parm_cfg.csum_ld = style_cfg2_reg.s.csum_ld;
+	style_cfg->parm_cfg.csum_lc = style_cfg2_reg.s.csum_lc;
+	style_cfg->parm_cfg.csum_lb = style_cfg2_reg.s.csum_lb;
+
+	style_cfg->parm_cfg.qpg_qos = style_alg_reg.s.qpg_qos;
+	style_cfg->parm_cfg.tag_type = style_alg_reg.s.tt;
+	style_cfg->parm_cfg.apad_nip = style_alg_reg.s.apad_nip;
+	style_cfg->parm_cfg.qpg_port_sh = style_alg_reg.s.qpg_port_sh;
+	style_cfg->parm_cfg.qpg_port_msb = style_alg_reg.s.qpg_port_msb;
+	style_cfg->parm_cfg.wqe_vs = style_alg_reg.s.wqe_vs;
+
+	style_cfg->parm_cfg.pkt_lend = style_buf_reg.s.pkt_lend;
+	style_cfg->parm_cfg.wqe_hsz = style_buf_reg.s.wqe_hsz;
+	style_cfg->parm_cfg.wqe_skip = style_buf_reg.s.wqe_skip * 128;
+	style_cfg->parm_cfg.first_skip = style_buf_reg.s.first_skip * 8;
+	style_cfg->parm_cfg.later_skip = style_buf_reg.s.later_skip * 8;
+	style_cfg->parm_cfg.cache_mode = style_buf_reg.s.opc_mode;
+	style_cfg->parm_cfg.mbuff_size = style_buf_reg.s.mb_size * 8;
+	style_cfg->parm_cfg.dis_wq_dat = style_buf_reg.s.dis_wq_dat;
+
+	cvmx_pki_get_tag_config(node, style, cluster_mask, &style_cfg->tag_cfg);
+}
+
+int cvmx_pki_get_pkind_config(int node, int pkind, struct cvmx_pki_pkind_config *pkind_cfg)
+{
+	int cluster = 0;
+	uint64_t cl_mask;
+	cvmx_pki_pkindx_icgsel_t pkind_clsel;
+	cvmx_pki_clx_pkindx_style_t pkind_cfg_style;
+	cvmx_pki_icgx_cfg_t pki_cl_grp;
+	cvmx_pki_clx_pkindx_cfg_t pknd_cfg_reg;
+
+	pkind_clsel.u64 = cvmx_read_csr_node(node, CVMX_PKI_PKINDX_ICGSEL(pkind));
+	pki_cl_grp.u64 = cvmx_read_csr_node(node, CVMX_PKI_ICGX_CFG(pkind_clsel.s.icg));
+	pkind_cfg->cluster_grp = (uint8_t)pkind_clsel.s.icg;
+	cl_mask = (uint64_t)pki_cl_grp.s.clusters;
+	cluster = __builtin_ffsll(cl_mask) - 1;
+
+	pkind_cfg_style.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_STYLE(pkind, cluster));
+	pkind_cfg->initial_parse_mode = pkind_cfg_style.s.pm;
+	pkind_cfg->initial_style = pkind_cfg_style.s.style;
+
+	pknd_cfg_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_CFG(pkind, cluster));
+	pkind_cfg->fcs_pres = pknd_cfg_reg.s.fcs_pres;
+	pkind_cfg->parse_en.inst_hdr = pknd_cfg_reg.s.inst_hdr;
+	pkind_cfg->parse_en.mpls_en = pknd_cfg_reg.s.mpls_en;
+	pkind_cfg->parse_en.lg_custom = pknd_cfg_reg.s.lg_custom;
+	pkind_cfg->parse_en.fulc_en = pknd_cfg_reg.s.fulc_en;
+	pkind_cfg->parse_en.dsa_en = pknd_cfg_reg.s.dsa_en;
+	pkind_cfg->parse_en.hg2_en = pknd_cfg_reg.s.hg2_en;
+	pkind_cfg->parse_en.hg_en = pknd_cfg_reg.s.hg_en;
+	return 0;
+}
+
+int cvmx_pki_get_pkind_style(int node, int pkind)
+{
+	int cluster = 0;
+
+	cvmx_pki_clx_pkindx_style_t pkind_style;
+
+	pkind_style.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_STYLE(pkind, cluster));
+
+	return pkind_style.s.style;
+}
+
+void cvmx_pki_config_port(int ipd_port, struct cvmx_pki_port_config *port_cfg)
+{
+	int interface, index, pknd;
+	int style, cl_mask;
+	cvmx_pki_icgx_cfg_t pki_cl_msk;
+	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(ipd_port);
+
+	/* get the pkind used by this ipd port */
+	interface = cvmx_helper_get_interface_num(ipd_port);
+	index = cvmx_helper_get_interface_index_num(ipd_port);
+	pknd = cvmx_helper_get_pknd(interface, index);
+
+	cvmx_pki_set_pkind_config(xp.node, pknd, &port_cfg->pkind_cfg);
+	style = port_cfg->pkind_cfg.initial_style;
+	pki_cl_msk.u64 = cvmx_read_csr_node(xp.node, CVMX_PKI_ICGX_CFG(port_cfg->pkind_cfg.cluster_grp));
+	cl_mask = pki_cl_msk.s.clusters;
+	cvmx_pki_set_style_config(xp.node, style, cl_mask, &port_cfg->style_cfg);
+}
+EXPORT_SYMBOL(cvmx_pki_config_port);
+
+void cvmx_pki_get_port_config(int ipd_port, struct cvmx_pki_port_config *port_cfg)
+{
+	int interface, index, pknd;
+	int style, cl_mask;
+	cvmx_pki_icgx_cfg_t pki_cl_msk;
+	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(ipd_port);
+
+	/* get the pkind used by this ipd port */
+	interface = cvmx_helper_get_interface_num(ipd_port);
+	index = cvmx_helper_get_interface_index_num(ipd_port);
+	pknd = cvmx_helper_get_pknd(interface, index);
+
+	cvmx_pki_get_pkind_config(xp.node, pknd, &port_cfg->pkind_cfg);
+	style = port_cfg->pkind_cfg.initial_style;
+	pki_cl_msk.u64 = cvmx_read_csr_node(xp.node, CVMX_PKI_ICGX_CFG(port_cfg->pkind_cfg.cluster_grp));
+	cl_mask = pki_cl_msk.s.clusters;
+	cvmx_pki_get_style_config(xp.node, style, cl_mask, &port_cfg->style_cfg);
+}
+EXPORT_SYMBOL(cvmx_pki_get_port_config);
+
+/**
+ * This function sets the wqe buffer mode. First packet data buffer can reside
+ * either in same buffer as wqe OR it can go in separate buffer. If used the later mode,
+ * make sure software allocate enough buffers to now have wqe separate from packet data.
+ * @param node	              node number.
+ * @param style		      style to configure.
+ * @param pkt_outside_wqe.	0 = The packet link pointer will be at word [FIRST_SKIP]
+ *				    immediately followed by packet data, in the same buffer
+ *				    as the work queue entry.
+ *				1 = The packet link pointer will be at word [FIRST_SKIP] in a new
+ *				    buffer separate from the work queue entry. Words following the
+ *				    WQE in the same cache line will be zeroed, other lines in the
+ *				    buffer will not be modified and will retain stale data (from the
+ * 				    buffers previous use). This setting may decrease the peak PKI
+ *				    performance by up to half on small packets.
+ */
+void cvmx_pki_set_wqe_mode(int node, uint64_t style, bool pkt_outside_wqe)
+{
+	cvmx_pki_stylex_buf_t     style_buf_reg;
+
+	style_buf_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_STYLEX_BUF(style));
+	style_buf_reg.s.dis_wq_dat = pkt_outside_wqe;
+	cvmx_write_csr_node(node, CVMX_PKI_STYLEX_BUF(style), style_buf_reg.u64);
+}
+
+
+/**
+ * This function writes pcam entry at given offset in pcam table in hardware
+ *
+ * @param node	              node number.
+ * @param index		      offset in pcam table.
+ * @param cluster_mask	      Mask of clusters in which to write pcam entry.
+ * @param input 	      input keys to pcam match passed as struct.
+ * @param action	      pcam match action passed as struct
+ *
+ */
+int cvmx_pki_pcam_write_entry(int node, int index, uint64_t cluster_mask,
+				struct cvmx_pki_pcam_input input,
+				struct cvmx_pki_pcam_action action)
+{
+	int bank;
+	int cluster = 0;
+	cvmx_pki_clx_pcamx_termx_t	pcam_term;
+	cvmx_pki_clx_pcamx_matchx_t	pcam_match;
+	cvmx_pki_clx_pcamx_actionx_t	pcam_action;
+
+	if (index >= CVMX_PKI_TOTAL_PCAM_ENTRY) {
+		cvmx_dprintf("\nERROR: Invalid pcam entry %d", index);
+		return -1;
+	}
+	bank = (int)(input.field & 0x01);
+	while (cluster < CVMX_PKI_NUM_CLUSTER) {
+		if (cluster_mask & (0x01L << cluster)) {
+			pcam_match.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PCAMX_MATCHX(cluster, bank, index));
+			pcam_match.s.data1 = input.data & input.data_mask;
+			pcam_match.s.data0 = (~input.data) & input.data_mask;
+			cvmx_write_csr_node(node, CVMX_PKI_CLX_PCAMX_MATCHX(cluster, bank, index), pcam_match.u64);
+			pcam_action.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PCAMX_ACTIONX(cluster, bank, index));
+			pcam_action.s.pmc = action.parse_mode_chg;
+			pcam_action.s.style_add = action.style_add;
+			pcam_action.s.pf = action.parse_flag_set;
+			pcam_action.s.setty = action.layer_type_set;
+			pcam_action.s.advance = action.pointer_advance;
+			cvmx_write_csr_node(node, CVMX_PKI_CLX_PCAMX_ACTIONX(cluster, bank, index), pcam_action.u64);
+			pcam_term.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PCAMX_TERMX(cluster, bank, index));
+			pcam_term.s.term1 = input.field & input.field_mask;
+			pcam_term.s.term0 = (~input.field) & input.field_mask;
+			pcam_term.s.style1 = input.style & input.style_mask;
+			pcam_term.s.style0 = (~input.style) & input.style_mask;
+			pcam_term.s.valid = 1;
+			cvmx_write_csr_node(node, CVMX_PKI_CLX_PCAMX_TERMX(cluster, bank, index), pcam_term.u64);
+		}
+		cluster++;
+	}
+	return 0;
+
+}
+
+/**
+ * Enables/Disabled QoS (RED Drop, Tail Drop & backpressure) for the
+ * PKI aura.
+ * @param node      node number
+ * @param aura      to enable/disable QoS on.
+ * @param ena_red   Enable/Disable RED drop between pass and drop level
+ *                  1-enable 0-disable
+ * @param ena_drop  Enable/disable tail drop when max drop level exceeds
+ *                  1-enable 0-disable
+ * @param ena_red   Enable/Disable asserting backpressure on bpid when
+ *                  max DROP level exceeds.
+ *                  1-enable 0-disable
+ */
+int cvmx_pki_enable_aura_qos(int node, int aura, bool ena_red,
+			     bool ena_drop, bool ena_bp)
+{
+	cvmx_pki_aurax_cfg_t pki_aura_cfg;
+
+	if (aura >= CVMX_PKI_NUM_AURA) {
+		cvmx_dprintf("ERROR: PKI config aura_qos aura = %d", aura);
+		return -1;
+	}
+	pki_aura_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKI_AURAX_CFG(aura));
+	pki_aura_cfg.s.ena_red = ena_red;
+	pki_aura_cfg.s.ena_drop = ena_drop;
+	pki_aura_cfg.s.ena_bp = ena_bp;
+	cvmx_write_csr_node(node, CVMX_PKI_AURAX_CFG(aura), pki_aura_cfg.u64);
+	return 0;
+}
+
+/**
+ * Configures the bpid on which, specified aura will
+ * assert backpressure.
+ * Each bpid receives backpressure from auras.
+ * Multiple auras can backpressure single bpid.
+ * @param node   node number
+ * @param aura   number which will assert backpressure on that bpid.
+ * @param bpid   to assert backpressure on.
+ */
+int cvmx_pki_write_aura_bpid(int node, int aura, int bpid)
+{
+	cvmx_pki_aurax_cfg_t pki_aura_cfg;
+
+	if (aura >= CVMX_PKI_NUM_AURA || bpid >= CVMX_PKI_NUM_BPID) {
+		cvmx_dprintf("ERROR: PKI config aura_bp aura = %d bpid = %d", aura, bpid);
+		return -1;
+	}
+	pki_aura_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKI_AURAX_CFG(aura));
+	pki_aura_cfg.s.bpid = bpid;
+	cvmx_write_csr_node(node, CVMX_PKI_AURAX_CFG(aura), pki_aura_cfg.u64);
+	return 0;
+}
+
+/**
+ * Configures the channel which will receive backpressure
+ * from the specified bpid.
+ * Each channel listens for backpressure on a specific bpid.
+ * Each bpid can backpressure multiple channels.
+ * @param node    node number
+ * @param bpid    bpid from which, channel will receive backpressure.
+ * @param channel channel numner to receive backpressue.
+ */
+int cvmx_pki_write_channel_bpid(int node, int channel, int bpid)
+{
+	cvmx_pki_chanx_cfg_t pki_chan_cfg;
+
+	if (channel >= CVMX_PKI_NUM_CHANNEL || bpid >= CVMX_PKI_NUM_BPID) {
+		cvmx_dprintf("ERROR: PKI config channel_bp channel = %d bpid = %d", channel, bpid);
+		return -1;
+	}
+
+	pki_chan_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CHANX_CFG(channel));
+	pki_chan_cfg.s.bpid = bpid;
+	cvmx_write_csr_node(node, CVMX_PKI_CHANX_CFG(channel), pki_chan_cfg.u64);
+	return 0;
+}
+
+/**
+ * Enables/Disables fcs check and fcs stripping on the pkind.
+ * @param node		node number
+ * @param pknd		pkind to apply settings on.
+ * @param fcs_chk	enable/disable fcs check.
+ *			1 -- enable fcs error check.
+ *			0 -- disable fcs error check.
+ * @param fcs_strip	Strip L2 FCS bytes from packet, decrease WQE[LEN] by 4 bytes
+ *			1 -- strip L2 FCS.
+ *			0 -- Do not strip L2 FCS.
+ */
+void cvmx_pki_endis_fcs_check(int node, int pknd, bool fcs_chk, bool fcs_strip)
+{
+	int style;
+	int cluster = 0;
+	cvmx_pki_clx_pkindx_style_t pkind_style;
+	cvmx_pki_clx_stylex_cfg_t style_cfg;
+
+        while (cluster < CVMX_PKI_NUM_CLUSTER) {
+            pkind_style.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_STYLE(pknd, cluster));
+            style = pkind_style.s.style;
+            style_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(style, cluster));
+            style_cfg.s.fcs_chk = fcs_chk;
+            style_cfg.s.fcs_strip = fcs_strip;
+            cvmx_write_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(style, cluster), style_cfg.u64);
+            cluster++;
+        }
+}
+
+/**
+ * Enables/Disables l2 length error check and max & min frame length checks
+ * @param node		node number
+ * @param pknd		pkind to disable error for.
+ * @param l2len_err	L2 length error check enable.
+ * @param maxframe_err	Max frame error check enable.
+ * @param minframe_err	Min frame error check enable.
+ *			1 -- Enabel err checks
+ *			0 -- Disable error checks
+ */
+void cvmx_pki_endis_l2_errs(int node, int pknd, bool l2len_err,
+			 bool maxframe_err, bool minframe_err)
+{
+	int style;
+	int cluster = 0;
+	cvmx_pki_clx_pkindx_style_t pkind_style;
+	cvmx_pki_clx_stylex_cfg_t style_cfg;
+
+	while (cluster < CVMX_PKI_NUM_CLUSTER) {
+            pkind_style.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_STYLE(pknd, cluster));
+            style = pkind_style.s.style;
+            style_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(style, cluster));
+            style_cfg.s.lenerr_en = l2len_err;
+            style_cfg.s.maxerr_en = maxframe_err;
+            style_cfg.s.minerr_en = minframe_err;
+            cvmx_write_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(style, cluster), style_cfg.u64);
+            cluster++;
+        }
+}
+
+/**
+ * Disables maximum & minimum frame length checks
+ * @param node   node number
+ * @param pknd	 pkind to disable error for.
+ */
+void cvmx_pki_dis_frame_len_chk(int node, int pknd)
+{
+	int style;
+	int cluster = 0;
+	cvmx_pki_clx_pkindx_style_t pkind_style;
+	cvmx_pki_clx_stylex_cfg_t style_cfg;
+
+	while (cluster < CVMX_PKI_NUM_CLUSTER) {
+            pkind_style.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_STYLE(pknd, cluster));
+            style = pkind_style.s.style;
+            style_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(style, cluster));
+            style_cfg.s.maxerr_en = 0;
+            style_cfg.s.minerr_en = 0;
+            cvmx_write_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(style, cluster), style_cfg.u64);
+            cluster++;
+        }
+}
+
+/**
+ * Modifies maximum frame length to check.
+ * It modifies the global frame length set used by this port, any other
+ * port using the same set will get affected too.
+ * @param node		node number
+ * @param ipd_port	ipd port for which to modify max len.
+ * @param max_size	maximum frame length
+ */
+void cvmx_pki_set_max_frm_len(int node, int ipd_port, uint32_t max_size)
+{
+	/* On CN78XX frame check is enabled for a style n and
+	PKI_CLX_STYLE_CFG[minmax_sel] selects which set of
+	MAXLEN/MINLEN to use. */
+	int interface, index, pknd;
+	cvmx_pki_clx_stylex_cfg_t style_cfg;
+	cvmx_pki_frm_len_chkx_t frame_len;
+	int cluster = 0;
+	int style;
+	int sel;
+
+	/* get the pkind used by this ipd port */
+	interface = cvmx_helper_get_interface_num(ipd_port);
+	index = cvmx_helper_get_interface_index_num(ipd_port);
+	pknd = cvmx_helper_get_pknd(interface, index);
+
+	style = cvmx_pki_get_pkind_style(node, pknd);
+	style_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(style, cluster));
+	sel = style_cfg.s.minmax_sel;
+	frame_len.u64 = cvmx_read_csr(CVMX_PKI_FRM_LEN_CHKX(sel));
+	frame_len.s.maxlen = max_size;
+	cvmx_write_csr_node(node, CVMX_PKI_FRM_LEN_CHKX(sel), frame_len.u64);
+}
+
+/**
+ * This function shows the qpg table entries,
+ * read directly from hardware.
+ * @param node    node number
+ */
+void cvmx_pki_show_qpg_entries(int node, uint16_t num_entry)
+{
+	int index;
+	cvmx_pki_qpg_tblx_t qpg_tbl;
+
+	if (num_entry > CVMX_PKI_NUM_QPG_ENTRY)
+		num_entry = CVMX_PKI_NUM_QPG_ENTRY;
+	for (index = 0; index < num_entry; index++) {
+		qpg_tbl.u64 = cvmx_read_csr_node(node, CVMX_PKI_QPG_TBLX(index));
+		cvmx_dprintf("\n%d	", index);
+		cvmx_dprintf("PADD %-16lu",
+			     (unsigned long)qpg_tbl.s.padd);
+		cvmx_dprintf("GRP_OK %-16lu",
+			     (unsigned long)qpg_tbl.s.grp_ok);
+		cvmx_dprintf("GRP_BAD %-16lu",
+			     (unsigned long)qpg_tbl.s.grp_bad);
+		cvmx_dprintf("LAURA %-16lu",
+			     (unsigned long)qpg_tbl.s.laura);
+	}
+}
+
+/**
+ * This function shows the pcam table in raw format,
+ * read directly from hardware.
+ * @param node    node number
+ */
+void cvmx_pki_show_pcam_entries(int node)
+{
+	int cluster;
+	int index;
+	int bank;
+
+	for (cluster = 0; cluster < 4; cluster++) {
+		for (bank = 0; bank < 2; bank++) {
+			cvmx_dprintf("\n--------------Cluster %1d Bank %1d-------------\n", cluster, bank);
+			cvmx_dprintf("index         TERM                 DATA,                ACTION");
+			for (index = 0; index < CVMX_PKI_NUM_PCAM_ENTRY; index++) {
+				cvmx_dprintf("\n%d", index);
+				cvmx_dprintf("             %-16lx",
+				     (unsigned long)cvmx_read_csr_node(node, CVMX_PKI_CLX_PCAMX_TERMX(cluster, bank, index)));
+				cvmx_dprintf("     %-16lx",
+					     (unsigned long)cvmx_read_csr_node(node, CVMX_PKI_CLX_PCAMX_MATCHX(cluster, bank, index)));
+				cvmx_dprintf("     %-16lx",
+					     (unsigned long)cvmx_read_csr_node(node, CVMX_PKI_CLX_PCAMX_ACTIONX(cluster, bank, index)));
+			}
+		}
+	}
+}
+
+/**
+ * This function shows the valid entries in readable format,
+ * read directly from hardware.
+ * @param node    node number
+ */
+void cvmx_pki_show_valid_pcam_entries(int node)
+{
+	int cluster;
+	int index;
+	int bank;
+	cvmx_pki_clx_pcamx_termx_t	pcam_term;
+	cvmx_pki_clx_pcamx_matchx_t	pcam_match;
+	cvmx_pki_clx_pcamx_actionx_t	pcam_action;
+
+	/*vinita_to_do, later modify to use/t/t etc*/
+	for (cluster = 0; cluster < 4; cluster++) {
+		for (bank = 0; bank < 2; bank++) {
+			cvmx_dprintf("\n--------------Cluster %1d Bank %1d---------------------\n", cluster, bank);
+			cvmx_dprintf("%-10s%-17s%-19s%-18s", "index",
+				     "TERM1:TERM0", "Style1:Style0", "Data1:Data0");
+			cvmx_dprintf("%-6s", "ACTION[pmc:style_add:pf:setty:advance]");
+			for (index = 0; index < CVMX_PKI_NUM_PCAM_ENTRY; index++) {
+				pcam_term.u64 = cvmx_read_csr_node(node,
+						CVMX_PKI_CLX_PCAMX_TERMX(cluster, bank, index));
+				if (pcam_term.s.valid) {
+					pcam_match.u64 = cvmx_read_csr_node(node,
+							CVMX_PKI_CLX_PCAMX_MATCHX(cluster, bank, index));
+					pcam_action.u64 = cvmx_read_csr_node(node,
+							CVMX_PKI_CLX_PCAMX_ACTIONX(cluster, bank, index));
+					cvmx_dprintf("\n%-13d", index);
+					cvmx_dprintf("%-2x:%x", pcam_term.s.term1, pcam_term.s.term0);
+					cvmx_dprintf("     	      %-2x:%x", pcam_term.s.style1, pcam_term.s.style0);
+					cvmx_dprintf("        %-8x:%x", pcam_match.s.data1, pcam_match.s.data0);
+					cvmx_dprintf("            %-2x:%-2x       :%-1x :%2x   :%-2x",
+						pcam_action.s.pmc, pcam_action.s.style_add, pcam_action.s.pf, pcam_action.s.setty, pcam_action.s.advance);
+
+				}
+			}
+		}
+	}
+}
+
+/**
+ * This function shows the pkind attributes in readable format,
+ * read directly from hardware.
+ * @param node    node number
+ */
+void cvmx_pki_show_pkind_attributes(int node, int pkind)
+{
+	int cluster = 0;
+	int index;
+	cvmx_pki_pkindx_icgsel_t pkind_clsel;
+	cvmx_pki_clx_pkindx_style_t pkind_cfg_style;
+	cvmx_pki_icgx_cfg_t pki_cl_grp;
+	cvmx_pki_clx_stylex_cfg_t style_cfg;
+	cvmx_pki_clx_stylex_alg_t style_alg;
+
+	if (pkind >= CVMX_PKI_NUM_PKIND) {
+		cvmx_dprintf("ERROR: PKIND %d is beyond range\n", pkind);
+		return;
+	}
+        cvmx_dprintf("Showing stats for pkind %d------------------\n", pkind);
+	pkind_clsel.u64 = cvmx_read_csr_node(node, CVMX_PKI_PKINDX_ICGSEL(pkind));
+	cvmx_dprintf("cluster group:	%d\n", pkind_clsel.s.icg);
+	pki_cl_grp.u64 = cvmx_read_csr_node(node, CVMX_PKI_ICGX_CFG(pkind_clsel.s.icg));
+	cvmx_dprintf("cluster mask of the group:	0x%x\n", pki_cl_grp.s.clusters);
+
+	while (cluster < CVMX_PKI_NUM_CLUSTER) {
+		if (pki_cl_grp.s.clusters & (0x01L << cluster)) {
+                         cvmx_dprintf("pkind %d config 0x%llx\n", pkind, (unsigned long long)cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_CFG(pkind, cluster)));
+			/*vinita_to_do later modify in human readble format or now just print register value*/
+			pkind_cfg_style.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_STYLE(pkind, cluster));
+			cvmx_dprintf("initial parse Mode: %d\n", pkind_cfg_style.s.pm);
+			cvmx_dprintf("initial_style: %d\n", pkind_cfg_style.s.style);
+			style_alg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_ALG(pkind_cfg_style.s.style, cluster));
+			cvmx_dprintf("style_alg: 0x%llx\n", (unsigned long long)style_alg.u64);
+			style_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(pkind_cfg_style.s.style, cluster));
+			cvmx_dprintf("style_cfg: 0x%llx\n", (unsigned long long)style_cfg.u64);
+			cvmx_dprintf("style_cfg2: 0x%llx\n",
+				     (unsigned long long)cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG2(pkind_cfg_style.s.style, cluster)));
+			cvmx_dprintf("style_buf: 0x%llx\n",
+				     (unsigned long long)cvmx_read_csr_node(node, CVMX_PKI_STYLEX_BUF(pkind_cfg_style.s.style)));
+			break;
+		}
+	}
+	cvmx_dprintf("qpg base: %d\n", style_cfg.s.qpg_base);
+	cvmx_dprintf("qpg qos: %d\n", style_alg.s.qpg_qos);
+	for (index = 0; index < 8; index++) {
+		cvmx_dprintf("qpg index %d: 0x%llx\n", (index+style_cfg.s.qpg_base),
+			     (unsigned long long)cvmx_read_csr_node(node, CVMX_PKI_QPG_TBLX(style_cfg.s.qpg_base+index)));
+	}
+}
+
+void cvmx_pki_show_port(int node, int interface, int index)
+{
+        int pknd = cvmx_helper_get_pknd(interface, index);
+        cvmx_dprintf("Showing stats for intf 0x%x port %d------------------\n", interface, index);
+        cvmx_pki_show_pkind_attributes(node, pknd);
+        cvmx_dprintf("END STAUS------------------------\n\n");
+}
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pko-internal-ports-range.c b/arch/mips/cavium-octeon/executive/cvmx-pko-internal-ports-range.c
new file mode 100644
index 0000000..7e1b6f6
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-pko-internal-ports-range.c
@@ -0,0 +1,170 @@
+/***********************license start***************
+ * Copyright (c) 2012  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include <linux/export.h>
+#include <asm/octeon/cvmx-range.h>
+#include <asm/octeon/cvmx-global-resources.h>
+#include <asm/octeon/cvmx-bootmem.h>
+#include <asm/octeon/cvmx-helper-cfg.h>
+#include <asm/octeon/cvmx-helper.h>
+#include <asm/octeon/cvmx-helper-util.h>
+#else
+#include "cvmx-range.h"
+#include "cvmx-global-resources.h"
+#include "cvmx-bootmem.h"
+#include "cvmx-helper-cfg.h"
+#include "cvmx-helper.h"
+#include "cvmx-helper-util.h"
+#endif
+
+union interface_port
+{
+	struct
+	{
+#ifdef __BIG_ENDIAN_BITFIELD
+		int port;
+		int interface;
+#else
+		int interface;
+		int port;
+#endif
+	} s;
+	uint64_t u64;
+
+};
+
+static int dbg = 0;
+
+static CVMX_SHARED int port_range_init = 0;
+
+int __cvmx_pko_internal_ports_range_init(void)
+{
+	int rv=0;
+
+	if (port_range_init)
+		return 0;
+	port_range_init = 1;
+	rv = cvmx_create_global_resource_range(CVMX_GR_TAG_PKO_IPORTS, CVMX_HELPER_CFG_MAX_PKO_QUEUES);
+	if (rv!=0) {
+		cvmx_dprintf("ERROR : Failed to initalize pko internal port range\n");
+	}
+	return rv;
+}
+
+
+int cvmx_pko_internal_ports_alloc(int xiface, int port, uint64_t count)
+{
+	int ret_val = -1;
+	union interface_port inf_port;
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+
+	__cvmx_pko_internal_ports_range_init();
+	inf_port.s.interface = xi.interface;
+	inf_port.s.port = port;
+	ret_val = cvmx_allocate_global_resource_range(CVMX_GR_TAG_PKO_IPORTS, inf_port.u64, count, 1);
+	if (dbg)
+		cvmx_dprintf("internal port alloc : port=%02d base=%02d count=%02d \n",
+			     (int) port, ret_val, (int) count);
+	if (ret_val == -1)
+		return ret_val;
+	cvmx_cfg_port[xi.node][xi.interface][port].ccpp_pko_port_base = ret_val;
+	cvmx_cfg_port[xi.node][xi.interface][port].ccpp_pko_num_ports  = count;
+	return 0;
+}
+
+/*
+ * Return the internal ports base
+ *
+ * @param  port   the port for which the queues are returned
+ *
+ * @return  0 on success
+ *         -1 on failure
+ */
+int cvmx_pko_internal_ports_free(int xiface, int port)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	int ret_val = -1;
+
+	__cvmx_pko_internal_ports_range_init();
+	ret_val = cvmx_free_global_resource_range_with_base(CVMX_GR_TAG_PKO_IPORTS,
+						    cvmx_cfg_port[xi.node][xi.interface][port].ccpp_pko_port_base,
+						    cvmx_cfg_port[xi.node][xi.interface][port].ccpp_pko_num_ports);
+	if (ret_val != 0)
+		return ret_val;
+	cvmx_cfg_port[xi.node][xi.interface][port].ccpp_pko_port_base = CVMX_HELPER_CFG_INVALID_VALUE;
+	cvmx_cfg_port[xi.node][xi.interface][port].ccpp_pko_num_ports  =  CVMX_HELPER_CFG_INVALID_VALUE;
+
+
+	return 0;
+}
+
+void cvmx_pko_internal_ports_range_free_all(void)
+{
+	int interface, port;
+
+	__cvmx_pko_internal_ports_range_init();
+	for(interface = 0; interface < CVMX_HELPER_MAX_IFACE; interface++)
+		for (port = 0; port < CVMX_HELPER_CFG_MAX_PORT_PER_IFACE;
+			     port++) {
+			if (cvmx_cfg_port[0][interface][port].ccpp_pko_port_base !=
+				    CVMX_HELPER_CFG_INVALID_VALUE)
+				cvmx_pko_internal_ports_free(interface, port);
+		}
+	//cvmx_range_show(pko_internal_ports_range);
+}
+EXPORT_SYMBOL(cvmx_pko_internal_ports_range_free_all);
+
+void cvmx_pko_internal_ports_range_show(void)
+{
+	int interface, port;
+
+	__cvmx_pko_internal_ports_range_init();
+	cvmx_show_global_resource_range(CVMX_GR_TAG_PKO_IPORTS);
+	for(interface = 0; interface < CVMX_HELPER_MAX_IFACE; interface++)
+		for (port = 0; port < CVMX_HELPER_CFG_MAX_PORT_PER_IFACE;
+				port ++) {
+			if (cvmx_cfg_port[0][interface][port].ccpp_pko_port_base !=
+				    CVMX_HELPER_CFG_INVALID_VALUE)
+				cvmx_dprintf("interface=%d port=%d port_base=%d port_cnt=%d\n",
+				     interface, port,
+				     (int)cvmx_cfg_port[0][interface][port].ccpp_pko_port_base,
+				     (int)cvmx_cfg_port[0][interface][port].ccpp_pko_num_ports);
+		}
+}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pko.c b/arch/mips/cavium-octeon/executive/cvmx-pko.c
index 008b881..52a9f29 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pko.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pko.c
@@ -34,6 +34,7 @@
 #include <asm/octeon/cvmx-config.h>
 #include <asm/octeon/cvmx-pko.h>
 #include <asm/octeon/cvmx-helper.h>
+#include <asm/octeon/cvmx-helper-cfg.h>
 
 /**
  * Internal state of packet output
@@ -142,6 +143,24 @@ void cvmx_pko_disable(void)
 }
 EXPORT_SYMBOL_GPL(cvmx_pko_disable);
 
+int __cvmx_pko_get_pipe(int interface, int index)
+{
+        /* The loopback ports do not have pipes */
+        if (cvmx_helper_interface_get_mode(interface) == CVMX_HELPER_INTERFACE_MODE_LOOP)
+                return -1;
+        /* We use pko_port as the pipe. See __cvmx_pko_port_map_o68(). */
+        return cvmx_helper_get_pko_port(interface, index);
+}
+
+int cvmx_pko_get_base_pko_port(int interface, int index)
+{
+        if (octeon_has_feature(OCTEON_FEATURE_PKND))
+                return __cvmx_helper_cfg_pko_port_base(interface, index);
+        else
+                return cvmx_helper_get_ipd_port(interface, index);
+}
+EXPORT_SYMBOL(cvmx_pko_get_base_pko_port);
+
 /**
  * Reset the packet output.
  */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pko3-queue.c b/arch/mips/cavium-octeon/executive/cvmx-pko3-queue.c
new file mode 100644
index 0000000..7d4c6d0
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-pko3-queue.c
@@ -0,0 +1,1212 @@
+/***********************license start***************
+ * Copyright (c) 2003-2013  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+
+/*
+ * File version info: $Rev:$
+ *
+ */
+
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include <linux/module.h>
+#include <asm/octeon/cvmx.h>
+#include <asm/octeon/cvmx-pko3.h>
+#include <asm/octeon/cvmx-helper-pko3.h>
+#include <asm/octeon/cvmx-bootmem.h>
+#include <asm/octeon/cvmx-clock.h>
+#else
+#include "cvmx.h"
+#include "cvmx-pko3.h"
+#include "cvmx-helper-pko3.h"
+#include "cvmx-bootmem.h"
+#endif
+
+
+
+/* Smalles Round-Robin quantum to use +1 */
+#define	CVMX_PKO3_RR_QUANTUM_MIN	0x10
+
+static int debug = 0;
+
+struct cvmx_pko3_dq {
+#ifdef __BIG_ENDIAN_BITFIELD
+	unsigned	
+			dq_count :6,	/* Number of descriptor queues */
+			dq_base :10;	/* Descriptor queue start number */
+#define	CVMX_PKO3_SWIZZLE_IPD	0x0
+#else
+	unsigned	
+			dq_base :10,	/* Descriptor queue start number */
+			dq_count :6;	/* Number of descriptor queues */
+
+#define	CVMX_PKO3_SWIZZLE_IPD	0x3
+#endif
+};
+
+/*
+ * @INTERNAL
+ * Descriptor Queue to IPD port mapping table.
+ *
+ * This pointer is per-core, contains the virtual address
+ * of a global named block which has 2^12 entries per each
+ * possible node.
+ */
+struct cvmx_pko3_dq *__cvmx_pko3_dq_table;
+
+int cvmx_pko3_get_queue_base(int ipd_port)
+{
+	struct cvmx_pko3_dq *dq_table;
+	int ret = -1;
+	unsigned i;
+	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(ipd_port);
+
+	/* get per-node table */
+	if(__cvmx_pko3_dq_table == NULL)
+		__cvmx_pko3_dq_table_setup();
+
+	i = CVMX_PKO3_SWIZZLE_IPD ^ xp.port;
+
+	/* get per-node table */
+	dq_table = __cvmx_pko3_dq_table + CVMX_PKO3_IPD_NUM_MAX * xp.node;
+
+	if(dq_table[i].dq_count > 0)
+		ret = cvmx_helper_node_to_ipd_port(xp.node, dq_table[i].dq_base);
+
+	return ret;
+}
+EXPORT_SYMBOL(cvmx_pko3_get_queue_base);
+
+int cvmx_pko3_get_queue_num(int ipd_port)
+{
+	struct cvmx_pko3_dq *dq_table;
+	int ret = -1;
+	unsigned i;
+	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(ipd_port);
+
+	/* get per-node table */
+	if(__cvmx_pko3_dq_table == NULL)
+		__cvmx_pko3_dq_table_setup();
+
+	i = CVMX_PKO3_SWIZZLE_IPD ^ xp.port;
+
+	/* get per-node table */
+	dq_table = __cvmx_pko3_dq_table + CVMX_PKO3_IPD_NUM_MAX * xp.node;
+
+	if(dq_table[i].dq_count > 0)
+		ret = dq_table[i].dq_count;
+
+	return ret;
+}
+
+/**
+ * @INTERNAL
+ *
+ * Initialize port/dq table contents
+ */
+static void __cvmx_pko3_dq_table_init(void *ptr)
+{
+	unsigned size = sizeof(struct cvmx_pko3_dq) * CVMX_PKO3_IPD_NUM_MAX * CVMX_MAX_NODES;
+
+	memset(ptr, 0, size);
+}
+
+/**
+ * @INTERNAL
+ *
+ * Find or allocate global port/dq map table
+ * which is a named table, contains entries for
+ * all possible OCI nodes.
+ *
+ * The table global pointer is stored in core-local variable
+ * so that every core will call this function once, on first use.
+ */
+int __cvmx_pko3_dq_table_setup(void)
+{
+	void *ptr;
+
+	ptr = cvmx_bootmem_alloc_named_range_once(
+		/* size */
+		sizeof(struct cvmx_pko3_dq) * CVMX_PKO3_IPD_NUM_MAX * CVMX_MAX_NODES,
+		/* min_addr, max_addr, align */
+		0ull, 0ull, sizeof(struct cvmx_pko3_dq),
+		/* name */
+		"cvmx_pko3_global_dq_table",
+		__cvmx_pko3_dq_table_init);
+
+	if(ptr == NULL)
+		return -1;
+
+	__cvmx_pko3_dq_table = ptr;
+	return 0;
+}
+
+/*
+ * @INTERNAL
+ * Register a range of Descriptor Queues wth an interface port
+ *
+ * This function poulates the DQ-to-IPD translation table
+ * used by the application to retreive the DQ range (typically ordered
+ * by priority) for a given IPD-port, which is either a physical port,
+ * or a channel on a channelized interface (i.e. ILK).
+ *
+ * @param interface is the physical interface number
+ * @param port is either a physical port on an interface
+ * @param or a channel of an ILK interface
+ * @param dq_base is the first Descriptor Queue number in a consecutive range
+ * @param dq_count is the number of consecutive Descriptor Queues leading
+ * @param the same channel or port.
+ *
+ * Only a consecurive range of Descriptor Queues can be associated with any
+ * given channel/port, and usually they are ordered from most to least
+ * in terms of scheduling priority.
+ *
+ * Note: thus function only populates the node-local translation table.
+ *
+ * @returns 0 on success, -1 on failure.
+ */
+int __cvmx_pko3_ipd_dq_register(int xiface, int index,
+		unsigned dq_base, unsigned dq_count)
+{
+	struct cvmx_pko3_dq *dq_table;
+	uint16_t ipd_port;
+	unsigned i;
+	struct cvmx_xport xp;
+
+        if(__cvmx_helper_xiface_is_null(xiface))
+		ipd_port = CVMX_PKO3_IPD_PORT_NULL;
+	else
+		ipd_port = cvmx_helper_get_ipd_port(xiface, index);
+
+	xp = cvmx_helper_ipd_port_to_xport(ipd_port);
+	i = CVMX_PKO3_SWIZZLE_IPD ^ xp.port;
+
+	/* get per-node table */
+	if(__cvmx_pko3_dq_table == NULL)
+		__cvmx_pko3_dq_table_setup();
+
+	dq_table = __cvmx_pko3_dq_table + CVMX_PKO3_IPD_NUM_MAX * xp.node;
+
+	if(debug)
+		cvmx_dprintf("%s: ipd=%#x ix=%#x dq %u cnt %u\n",
+			__FUNCTION__, ipd_port, i, dq_base, dq_count);
+
+	/* Check the IPD port has not already been configured */
+	if(dq_table[i].dq_count > 0 ) {
+		cvmx_dprintf("%s: ERROR: IPD %#x already registered\n",
+			__FUNCTION__, ipd_port);
+		return -1;
+	}
+
+	/* Store DQ# range in the queue lookup table */
+	dq_table[i].dq_base = dq_base;
+	dq_table[i].dq_count = dq_count;
+
+	return 0;
+}
+
+/**
+ * @INTERNAL
+ *
+ * Unregister DQs associated with CHAN_E (IPD port)
+ */
+int __cvmx_pko3_ipd_dq_unregister(int xiface, int index)
+{
+	struct cvmx_pko3_dq *dq_table;
+	uint16_t ipd_port;
+	unsigned i;
+	struct cvmx_xport xp;
+
+        if(__cvmx_helper_xiface_is_null(xiface))
+		ipd_port = CVMX_PKO3_IPD_PORT_NULL;
+	else
+		ipd_port = cvmx_helper_get_ipd_port(xiface, index);
+
+	xp = cvmx_helper_ipd_port_to_xport(ipd_port);
+	i = CVMX_PKO3_SWIZZLE_IPD ^ xp.port;
+
+	/* get per-node table */
+	if(__cvmx_pko3_dq_table == NULL)
+		__cvmx_pko3_dq_table_setup();
+
+	/* get per-node table */
+	dq_table = __cvmx_pko3_dq_table + CVMX_PKO3_IPD_NUM_MAX * xp.node;
+
+	if (dq_table[i].dq_count == 0) {
+		cvmx_dprintf("%s:ipd=%#x already released\n",
+			__FUNCTION__, ipd_port);
+		return -1;
+	}
+
+	if(debug)
+		cvmx_dprintf("%s:ipd=%#x release dq %u cnt %u\n",
+			     __FUNCTION__, ipd_port, 
+			     dq_table[i].dq_base, 
+			     dq_table[i].dq_count);
+
+	dq_table[i].dq_count = 0;
+
+	return 0;
+}
+
+/*
+ * @INTERNAL
+ * Convert normal CHAN_E (i.e. IPD port) value to compressed channel form
+ * that is used to populate PKO_LUT.
+ *
+ * Note: This code may be CN78XX specific, not the same for all PKO3
+ * implementations.
+ */
+static uint16_t cvmx_pko3_chan_2_xchan(uint16_t ipd_port)
+{
+	uint16_t xchan;
+	uint8_t off;
+	static const uint8_t xchan_base[16] = {
+		/* IPD 0x000 */ 0x3c0 >> 4,	/* LBK */
+		/* IPD 0x100 */ 0x380 >> 4,	/* DPI */
+		/* IPD 0x200 */ 0xfff >> 4,	/* not used */
+		/* IPD 0x300 */ 0xfff >> 4,	/* not used */
+		/* IPD 0x400 */ 0x000 >> 4,	/* ILK0 */
+		/* IPD 0x500 */ 0x100 >> 4,	/* ILK1 */
+		/* IPD 0x600 */ 0xfff >> 4,	/* not used */
+		/* IPD 0x700 */ 0xfff >> 4,	/* not used */
+		/* IPD 0x800 */ 0x200 >> 4,	/* BGX0 */
+		/* IPD 0x900 */ 0x240 >> 4,	/* BGX1 */
+		/* IPD 0xa00 */ 0x280 >> 4,	/* BGX2 */
+		/* IPD 0xb00 */ 0x2c0 >> 4,	/* BGX3 */
+		/* IPD 0xc00 */ 0x300 >> 4,	/* BGX4 */
+		/* IPD 0xd00 */ 0x340 >> 4,	/* BGX5 */
+		/* IPD 0xe00 */ 0xfff >> 4,	/* not used */
+		/* IPD 0xf00 */ 0xfff >> 4	/* not used */
+	};
+
+	xchan = ipd_port >> 8;
+
+	/* ILKx has 8 bits logical channels, others just 6 */
+	if (((xchan & 0xfe) == 0x04))
+		off = ipd_port & 0xff;
+	else
+		off = ipd_port & 0x3f;
+
+	xchan = xchan_base[ xchan & 0xF ];
+
+	if(xchan == 0xff)
+		return 0xffff;
+	else
+		return (xchan << 4) | off;
+}
+
+/*
+ * Map channel number in PKO 
+ *
+ * @param node is to specify the node to which this configuration is applied.
+ * @param pq_num specifies the Port Queue (i.e. L1) queue number.
+ * @param l2_l3_q_num  specifies L2/L3 queue number.
+ * @param channel specifies the channel number to map to the queue.
+ *
+ * The channel assignment applies to L2 or L3 Shaper Queues depending
+ * on the setting of channel credit level.
+ *
+ * @return returns none.
+ */
+void cvmx_pko3_map_channel(unsigned node,
+	unsigned pq_num, unsigned l2_l3_q_num, uint16_t channel)
+{
+	union cvmx_pko_l3_l2_sqx_channel sqx_channel;
+	cvmx_pko_lutx_t lutx;
+	uint16_t xchan;
+
+	sqx_channel.u64 = cvmx_read_csr_node(node,
+		CVMX_PKO_L3_L2_SQX_CHANNEL(l2_l3_q_num));
+
+	sqx_channel.s.cc_channel = channel;
+
+	cvmx_write_csr_node(node,
+		CVMX_PKO_L3_L2_SQX_CHANNEL(l2_l3_q_num), sqx_channel.u64);
+
+	/* Convert CHAN_E into compressed channel */
+	xchan =  cvmx_pko3_chan_2_xchan(channel);
+
+	if(xchan & 0xf000) {
+		cvmx_dprintf("%s: ERROR: channel %#x not recognized\n",
+			__FUNCTION__, channel);
+		return;
+	}
+
+	lutx.u64 = 0;
+	lutx.s.valid = 1;
+	lutx.s.pq_idx = pq_num;
+	lutx.s.queue_number = l2_l3_q_num;
+
+	cvmx_write_csr_node(node, CVMX_PKO_LUTX(xchan), lutx.u64);
+
+	if (debug)
+		cvmx_dprintf("%s: channel %#x (compressed=%#x) mapped "
+				"L2/L3 SQ=%u, PQ=%u\n",
+			__FUNCTION__, channel, xchan, l2_l3_q_num, pq_num);
+}
+
+/*
+ * @INTERNAL
+ * This function configures port queue scheduling and topology parameters
+ * in hardware.
+ *
+ * @param node is to specify the node to which this configuration is applied.
+ * @param port_queue is the port queue number to be configured.
+ * @param child_base is the first child queue number in the static prioriy childs.
+ * @param child_rr_prio is the round robin childs priority.
+ * @param mac_num is the mac number of the mac that will be tied to this port_queue.
+ * @return returns none.
+ */
+static void cvmx_pko_configure_port_queue(int node, int port_queue,
+					 int child_base, int child_rr_prio,
+					 int mac_num)
+{
+	cvmx_pko_l1_sqx_topology_t pko_l1_topology;
+	cvmx_pko_l1_sqx_shape_t pko_l1_shape;
+	cvmx_pko_l1_sqx_link_t pko_l1_link;
+
+	pko_l1_topology.u64 = 0;
+	pko_l1_topology.s.prio_anchor = child_base;
+	pko_l1_topology.s.link = mac_num;
+	pko_l1_topology.s.rr_prio = child_rr_prio;
+	cvmx_write_csr_node(node, CVMX_PKO_L1_SQX_TOPOLOGY(port_queue), pko_l1_topology.u64);
+
+	pko_l1_shape.u64 = 0;
+	pko_l1_shape.s.link = mac_num;
+	cvmx_write_csr_node(node, CVMX_PKO_L1_SQX_SHAPE(port_queue), pko_l1_shape.u64);
+
+	pko_l1_link.u64 = 0;
+	pko_l1_link.s.link = mac_num;
+	cvmx_write_csr_node(node, CVMX_PKO_L1_SQX_LINK(port_queue), pko_l1_link.u64);
+}
+
+/*
+ * @INTERNAL
+ * This function configures level 2 queues scheduling and topology parameters
+ * in hardware.
+ *
+ * @param node is to specify the node to which this configuration is applied.
+ * @param queue is the level2 queue number to be configured.
+ * @param parent_queue is the parent queue at next level for this l2 queue.
+ * @param prio is this queue's priority in parent's scheduler.
+ * @param rr_quantum is this queue's round robin quantum value.
+ * @return returns none.
+ */
+static void cvmx_pko_configure_l2_queue(int node, int queue, int parent_queue,
+					       int prio, int rr_quantum)
+{
+	cvmx_pko_l2_sqx_schedule_t pko_sq_sched;
+	cvmx_pko_l2_sqx_topology_t pko_sq_topology;
+
+	/* scheduler configuration for this sq in the parent queue */
+	pko_sq_sched.u64 = 0;
+	pko_sq_sched.s.prio = prio;
+	pko_sq_sched.s.rr_quantum = rr_quantum;
+	cvmx_write_csr_node(node, CVMX_PKO_L2_SQX_SCHEDULE(queue), pko_sq_sched.u64);
+
+	/* topology configuration */
+	pko_sq_topology.u64 = 0;
+	pko_sq_topology.s.parent = parent_queue;
+	cvmx_write_csr_node(node, CVMX_PKO_L2_SQX_TOPOLOGY(queue), pko_sq_topology.u64);
+
+}
+
+/*
+ * @INTERNAL
+ * This function configures level 3 queues scheduling and topology parameters
+ * in hardware.
+ *
+ * @param node is to specify the node to which this configuration is applied.
+ * @param queue is the level3 queue number to be configured.
+ * @param parent_queue is the parent queue at next level for this l3 queue.
+ * @param prio is this queue's priority in parent's scheduler.
+ * @param rr_quantum is this queue's round robin quantum value.
+ * @param child_base is the first child queue number in the static prioriy childs.
+ * @param child_rr_prio is the round robin childs priority.
+ * @return returns none.
+ */
+static void cvmx_pko_configure_l3_queue(int node, int queue, int parent_queue,
+					       int prio, int rr_quantum,
+					       int child_base, int child_rr_prio)
+{
+	cvmx_pko_l3_sqx_schedule_t pko_sq_sched;
+	cvmx_pko_l3_sqx_topology_t pko_child_topology;
+	cvmx_pko_l2_sqx_topology_t pko_parent_topology;
+
+	/* parent topology configuration */
+	pko_parent_topology.u64 = cvmx_read_csr_node(node,
+			CVMX_PKO_L2_SQX_TOPOLOGY(parent_queue));
+	pko_parent_topology.s.prio_anchor = child_base;
+	pko_parent_topology.s.rr_prio = child_rr_prio;
+	cvmx_write_csr_node(node,
+			CVMX_PKO_L2_SQX_TOPOLOGY(parent_queue),
+			pko_parent_topology.u64);
+
+	/* scheduler configuration for this sq in the parent queue */
+	pko_sq_sched.u64 = 0;
+	pko_sq_sched.s.prio = prio;
+	pko_sq_sched.s.rr_quantum = rr_quantum;
+	cvmx_write_csr_node(node, CVMX_PKO_L3_SQX_SCHEDULE(queue), pko_sq_sched.u64);
+
+	/* child topology configuration */
+	pko_child_topology.u64 = 0;
+	pko_child_topology.s.parent = parent_queue;
+	cvmx_write_csr_node(node, CVMX_PKO_L3_SQX_TOPOLOGY(queue), pko_child_topology.u64);
+
+}
+
+/*
+ * @INTERNAL
+ * This function configures level 4 queues scheduling and topology parameters
+ * in hardware.
+ *
+ * @param node is to specify the node to which this configuration is applied.
+ * @param queue is the level4 queue number to be configured.
+ * @param parent_queue is the parent queue at next level for this l4 queue.
+ * @param prio is this queue's priority in parent's scheduler.
+ * @param rr_quantum is this queue's round robin quantum value.
+ * @param child_base is the first child queue number in the static prioriy childs.
+ * @param child_rr_prio is the round robin childs priority.
+ * @return returns none.
+ */
+static void cvmx_pko_configure_l4_queue(int node, int queue, int parent_queue,
+					       int prio, int rr_quantum,
+					       int child_base, int child_rr_prio)
+{
+	cvmx_pko_l4_sqx_schedule_t pko_sq_sched;
+	cvmx_pko_l4_sqx_topology_t pko_child_topology;
+	cvmx_pko_l3_sqx_topology_t pko_parent_topology;
+
+	/* parent topology configuration */
+	pko_parent_topology.u64 = cvmx_read_csr_node(node,
+			CVMX_PKO_L3_SQX_TOPOLOGY(parent_queue));
+	pko_parent_topology.s.prio_anchor = child_base;
+	pko_parent_topology.s.rr_prio = child_rr_prio;
+	cvmx_write_csr_node(node,
+			CVMX_PKO_L3_SQX_TOPOLOGY(parent_queue),
+			pko_parent_topology.u64);
+
+	/* scheduler configuration for this sq in the parent queue */
+	pko_sq_sched.u64 = 0;
+	pko_sq_sched.s.prio = prio;
+	pko_sq_sched.s.rr_quantum = rr_quantum;
+	cvmx_write_csr_node(node, CVMX_PKO_L4_SQX_SCHEDULE(queue), pko_sq_sched.u64);
+
+	/* topology configuration */
+	pko_child_topology.u64 = 0;
+	pko_child_topology.s.parent = parent_queue;
+	cvmx_write_csr_node(node, CVMX_PKO_L4_SQX_TOPOLOGY(queue), pko_child_topology.u64);
+}
+
+/*
+ * @INTERNAL
+ * This function configures level 5 queues scheduling and topology parameters
+ * in hardware.
+ *
+ * @param node is to specify the node to which this configuration is applied.
+ * @param queue is the level5 queue number to be configured.
+ * @param parent_queue is the parent queue at next level for this l5 queue.
+ * @param prio is this queue's priority in parent's scheduler.
+ * @param rr_quantum is this queue's round robin quantum value.
+ * @param child_base is the first child queue number in the static prioriy childs.
+ * @param child_rr_prio is the round robin childs priority.
+ * @return returns none.
+ */
+static void cvmx_pko_configure_l5_queue(int node, int queue, int parent_queue,
+					       int prio, int rr_quantum,
+					       int child_base, int child_rr_prio)
+{
+	cvmx_pko_l5_sqx_schedule_t pko_sq_sched;
+	cvmx_pko_l4_sqx_topology_t pko_parent_topology;
+	cvmx_pko_l5_sqx_topology_t pko_child_topology;
+
+	/* parent topology configuration */
+	pko_parent_topology.u64 = cvmx_read_csr_node(node,
+			CVMX_PKO_L4_SQX_TOPOLOGY(parent_queue));
+	pko_parent_topology.s.prio_anchor = child_base;
+	pko_parent_topology.s.rr_prio = child_rr_prio;
+	cvmx_write_csr_node(node,
+			CVMX_PKO_L4_SQX_TOPOLOGY(parent_queue),
+			pko_parent_topology.u64);
+
+	/* scheduler configuration for this sq in the parent queue */
+	pko_sq_sched.u64 = 0;
+	pko_sq_sched.s.prio = prio;
+	pko_sq_sched.s.rr_quantum = rr_quantum;
+	cvmx_write_csr_node(node, CVMX_PKO_L5_SQX_SCHEDULE(queue), pko_sq_sched.u64);
+
+	/* topology configuration */
+	pko_child_topology.u64 = 0;
+	pko_child_topology.s.parent = parent_queue;
+	cvmx_write_csr_node(node, CVMX_PKO_L5_SQX_TOPOLOGY(queue), pko_child_topology.u64);
+}
+
+/*
+ * @INTERNAL
+ * This function configures descriptor queues scheduling and topology parameters
+ * in hardware.
+ *
+ * @param node is to specify the node to which this configuration is applied.
+ * @param dq is the descriptor queue number to be configured.
+ * @param parent_queue is the parent queue at next level for this dq.
+ * @param prio is this queue's priority in parent's scheduler.
+ * @param rr_quantum is this queue's round robin quantum value.
+ * @param child_base is the first child queue number in the static prioriy childs.
+ * @param child_rr_prio is the round robin childs priority.
+ * @return returns none.
+ */
+static void cvmx_pko_configure_dq(int node, int dq, int parent_queue,
+				int prio, int rr_quantum,
+			       	int child_base, int child_rr_prio)
+{
+	cvmx_pko_dqx_schedule_t pko_dq_sched;
+	cvmx_pko_dqx_topology_t pko_dq_topology;
+	cvmx_pko_l5_sqx_topology_t pko_parent_topology;
+	cvmx_pko_dqx_wm_ctl_t pko_dq_wm_ctl;
+
+	/* parent topology configuration */
+	pko_parent_topology.u64 = cvmx_read_csr_node(node,
+			CVMX_PKO_L5_SQX_TOPOLOGY(parent_queue));
+	pko_parent_topology.s.prio_anchor = child_base;
+	pko_parent_topology.s.rr_prio = child_rr_prio;
+	cvmx_write_csr_node(node,
+			CVMX_PKO_L5_SQX_TOPOLOGY(parent_queue),
+			pko_parent_topology.u64);
+
+	/* scheduler configuration for this dq in the parent queue */
+	pko_dq_sched.u64 = 0;
+	pko_dq_sched.s.prio = prio;
+	pko_dq_sched.s.rr_quantum = rr_quantum;
+	cvmx_write_csr_node(node, CVMX_PKO_DQX_SCHEDULE(dq), pko_dq_sched.u64);
+
+	/* topology configuration */
+	pko_dq_topology.u64 = 0;
+	pko_dq_topology.s.parent = parent_queue;
+	cvmx_write_csr_node(node, CVMX_PKO_DQX_TOPOLOGY(dq), pko_dq_topology.u64);
+
+	/* configure for counting packets, not bytes at this level */
+	pko_dq_wm_ctl.u64 = 0;
+	pko_dq_wm_ctl.s.kind = 1;
+	pko_dq_wm_ctl.s.enable = 0;
+	cvmx_write_csr_node(node, CVMX_PKO_DQX_WM_CTL(dq), pko_dq_wm_ctl.u64);
+}
+
+
+/*
+ * @INTERNAL
+ * The following structure selects the Scheduling Queue configuration
+ * routine for each of the supported levels.
+ * The initial content of the table will be setup in accordance
+ * to the specific SoC model and its implemented resources
+ */
+static const struct {
+	unsigned sq_level_base,
+		sq_level_count;
+	/* 4 function pointers for L3 .. L6=DQ */
+	void (*cfg_sq_func[])(
+			int node, int queue, int parent_queue,
+			int prio, int rr_quantum,
+			int child_base, int child_rr_prio);
+} __cvmx_pko3_sq_config_table = {
+	3, 4,
+	{
+	cvmx_pko_configure_l3_queue,
+	cvmx_pko_configure_l4_queue,
+	cvmx_pko_configure_l5_queue,
+	cvmx_pko_configure_dq
+	}
+};
+
+/*
+ * Configure Port Queue and its children Scheduler Queue
+ *
+ * Port Queues (a.k.a L1) are assigned 1-to-1 to MACs.
+ * L2 Scheduler Queues are used for specifying channels, and thus there
+ * could be multiple L2 SQs attached to a single L1 PQ, either in a
+ * fair round-robin scheduling, or with static and/or round-robin priorities.
+ *
+ * @param mac_num is the LMAC number to that is associated with the Port Queue,
+ * @param which is identical to the Port Queue number that is configured
+ * @param child_base is the number of the first L2 SQ attached to the PQ
+ * @param child_count is the number of L2 SQ children to attach to PQ
+ * @param stat_prio_count is the priority setting for the children L2 SQs
+ *
+ * If <stat_prio_count> is -1, the L2 children will have equal Round-Robin
+ * relationship with eachother. If <stat_prio_count> is 0, all L2 children
+ * will be arranged in Weighted-Round-Robin, with the first having the most
+ * precedence. If <stat_prio_count> is between 1 and 8, it indicates how
+ * many children will have static priority settings (with the first having
+ * the most precedence), with the remaining L2 children having WRR scheduling.
+ *
+ * @returns 0 on success, -1 on failure.
+ *
+ * Note: this function supports the configuration of node-local unit.
+ */
+int cvmx_pko3_pq_config_children(unsigned node, unsigned mac_num,
+			 unsigned child_base,
+			unsigned child_count, int stat_prio_count)
+{
+	unsigned pq_num;
+	unsigned rr_quantum, rr_count;
+	unsigned child, prio, rr_prio;
+
+	/* L1/PQ number is 1-to-1 from MAC number */
+	pq_num = mac_num;
+
+	/* First static priority is 0 - wuth the most precedence */
+	prio = 0;
+
+	if (stat_prio_count > (signed) child_count)
+		stat_prio_count = child_count;
+
+	/* Valid PRIO field is 0..9, limit maximum static priorities */
+	if (stat_prio_count > 9)
+		stat_prio_count = 9;
+
+	/* Special case of a single child */
+	if (child_count == 1) {
+		rr_count = 0;
+		rr_prio = 0xF;
+	/* Special case for Fair-RR */
+	} else if (stat_prio_count < 0) {
+		rr_count = child_count;
+		rr_prio = 0;
+	} else {
+		rr_count = child_count - stat_prio_count;
+		rr_prio = stat_prio_count;
+	}
+
+	/* Compute highest RR_QUANTUM */
+	if (stat_prio_count > 0)
+		rr_quantum = CVMX_PKO3_RR_QUANTUM_MIN * rr_count;
+	else
+		rr_quantum = CVMX_PKO3_RR_QUANTUM_MIN;
+
+	if(debug)
+		cvmx_dprintf("%s: L1/PQ%02u MAC%02u child_base %u rr_pri %u\n",
+		__FUNCTION__, pq_num, mac_num, child_base, rr_prio);
+
+	cvmx_pko_configure_port_queue(node,
+		pq_num, child_base, rr_prio, mac_num);
+
+
+	for(child = child_base; child < (child_base + child_count); child ++) {
+		if (debug)
+			cvmx_dprintf("%s: "
+				"L2/SQ%u->PQ%02u prio %u rr_quantum %#x\n",
+				__FUNCTION__,
+				child, pq_num, prio, rr_quantum);
+
+		cvmx_pko_configure_l2_queue(node,
+			child, pq_num, prio, rr_quantum);
+
+		if (prio < rr_prio)
+			prio ++;
+		else if (stat_prio_count > 0)
+			rr_quantum -= CVMX_PKO3_RR_QUANTUM_MIN;
+	} /* for child */
+
+	return 0;
+}
+
+/*
+ * Configure L3 through L5 Scheduler Queues and Descriptor Queues
+ *
+ * The Scheduler Queues in Levels 3 to 5 and Descriptor Queues are
+ * configured one-to-one or many-to-one to a single parent Scheduler
+ * Queues. The level of the parent SQ is specified in an argument,
+ * as well as the number of childer to attach to the specific parent.
+ * The children can have fair round-robin or priority-based scheduling
+ * when multiple children are assigned a single parent.
+ *
+ * @param parent_level is the level of the parent queue, 2 to 5.
+ * @param parent_queue is the number of the parent Scheduler Queue
+ * @param child_base is the number of the first child SQ or DQ to assign to
+ * @param parent
+ * @param child_count is the number of consecutive children to assign
+ * @param stat_prio_count is the priority setting for the children L2 SQs
+ *
+ * If <stat_prio_count> is -1, the Ln children will have equal Round-Robin
+ * relationship with eachother. If <stat_prio_count> is 0, all Ln children
+ * will be arranged in Weighted-Round-Robin, with the first having the most
+ * precedence. If <stat_prio_count> is between 1 and 8, it indicates how
+ * many children will have static priority settings (with the first having
+ * the most precedence), with the remaining Ln children having WRR scheduling.
+ *
+ * @returns 0 on success, -1 on failure.
+ *
+ * Note: this function supports the configuration of node-local unit.
+ */
+int cvmx_pko3_sq_config_children(unsigned int node, unsigned parent_level,
+			unsigned parent_queue, unsigned child_base,
+			unsigned child_count, int stat_prio_count)
+{
+	unsigned child_level;
+	unsigned rr_quantum, rr_count;
+	unsigned child, prio, rr_prio;
+	unsigned func_idx;
+
+	child_level = parent_level + 1;
+
+	if (child_level < __cvmx_pko3_sq_config_table.sq_level_base ||
+	    child_level >= __cvmx_pko3_sq_config_table.sq_level_base +
+			__cvmx_pko3_sq_config_table.sq_level_count)
+		return -1;
+
+	func_idx = child_level - __cvmx_pko3_sq_config_table.sq_level_base;
+
+	/* First static priority is 0 - top precedence */
+	prio = 0;
+
+	if (stat_prio_count > (signed) child_count)
+		stat_prio_count = child_count;
+
+	/* Valid PRIO field is 0..9, limit maximum static priorities */
+	if (stat_prio_count > 9)
+		stat_prio_count = 9;
+
+	/* Special case of a single child */
+	if (child_count == 1) {
+		rr_count = 0;
+		rr_prio = 0xF;
+	/* Special case for Fair-RR */
+	} else if (stat_prio_count < 0) {
+		rr_count = child_count;
+		rr_prio = 0;
+	} else {
+		rr_count = child_count - stat_prio_count;
+		rr_prio = stat_prio_count;
+	}
+
+	/* Compute highest RR_QUANTUM */
+	if (stat_prio_count > 0)
+		rr_quantum = CVMX_PKO3_RR_QUANTUM_MIN * rr_count;
+	else
+		rr_quantum = CVMX_PKO3_RR_QUANTUM_MIN;
+
+	if(debug)
+		cvmx_dprintf("%s: Parent L%u/SQ%02u child_base %u rr_pri %u\n",
+		__FUNCTION__, parent_level, parent_queue, child_base, rr_prio);
+
+	/* Parent is configured with child */
+
+	for(child = child_base; child < (child_base + child_count); child ++) {
+		if (debug)
+			cvmx_dprintf("%s: "
+				"L%u/SQ%u->L%u/SQ%u prio %u rr_quantum %#x\n",
+				__FUNCTION__,
+				child_level, child,
+				parent_level, parent_queue,
+				prio, rr_quantum);
+
+		__cvmx_pko3_sq_config_table.cfg_sq_func[func_idx](
+			node, child, parent_queue, prio, rr_quantum,
+			child_base, rr_prio);
+
+		if (prio < rr_prio)
+			prio ++;
+		else if (stat_prio_count > 0)
+			rr_quantum -= CVMX_PKO3_RR_QUANTUM_MIN;
+	} /* for child */
+
+	return 0;
+}
+
+/**
+ * Convert bitrate and burst size to SQx_xIR register values
+ * 
+ * @INTERNAL
+ *
+ * FIXME:
+ * The calculation is based on partial understanding of 
+ * register fields. Must update when more details become
+ * available.
+ */
+static int cvmx_pko3_shaper_rate_compute(unsigned long tclk,
+		cvmx_pko_l1_sqx_cir_t *reg,
+		unsigned long rate_kbips, unsigned burst_bytes)
+{
+	const unsigned max_exp = 12;	/* maximum exponent */
+	const unsigned tock_bytes_exp = 3;	/* rate in 8-byte words */
+	long long burst_v, rate_v;
+	unsigned long long rate_tocks, burst_tocks;
+	unsigned long min_burst;
+	unsigned div_exp, mant, exp;
+	unsigned long long tmp, fmax;
+	int debug = 1;	// XXX
+
+	if (debug)
+		cvmx_dprintf("%s: tclk=%lu, rate=%lu kbps, burst=%u bytes\n",
+			__func__, tclk, rate_kbips, burst_bytes);
+
+	/* Convert API args into tocks: PSE native units */
+	tmp = (1 << (3 + tock_bytes_exp))-1;
+	rate_tocks = (1000ULL * rate_kbips + tmp) >> (3 + tock_bytes_exp);
+	tmp = (1 << tock_bytes_exp) - 1;
+	burst_tocks = (burst_bytes+tmp) >> tock_bytes_exp;
+
+	/* Compute largest short-float that fits in register fields */
+	fmax = CVMX_SHOFT_TO_U64((1<<CVMX_SHOFT_MANT_BITS)-1, max_exp);
+
+	/* Find the biggest divider that has the short float fit */
+	for (div_exp = 0; div_exp <= max_exp; div_exp++) {
+		tmp = (rate_tocks << div_exp) / tclk;
+		if (tmp > fmax) {
+			div_exp --;
+			break;
+		}
+	}
+
+	/* Make sure exponent is within valid range */
+	if (div_exp > max_exp)
+		div_exp = max_exp;
+
+	/* Store common divider */
+	reg->s.rate_divider_exponent = div_exp;
+
+	/* XXX-
+	 * Assu,ing the BURST field is the actual satiration value
+	 * for the rate accumulator, while the argument is the 
+	 * delta burst amount to add to the accumulator value
+	 * required to maintain the requested rate.
+	 */
+
+	/* Find the minimum burst size needed for rate */
+	min_burst = (rate_tocks << div_exp) / tclk;
+
+	/* Apply the minimum */
+	burst_tocks += min_burst;
+
+	/* Calculate the rate short float */
+	tmp = (rate_tocks << (div_exp + 8)) / tclk;
+	CVMX_SHOFT_FROM_U64(tmp, mant, exp);
+	reg->s.rate_mantissa = mant;
+	reg->s.rate_exponent = exp - 8;
+
+	/* Calculate the BURST short float */
+	tmp = (burst_tocks << 8);
+	CVMX_SHOFT_FROM_U64(tmp, mant, exp);
+	reg->s.burst_mantissa = mant;
+	reg->s.burst_exponent = exp - 8;
+
+	if (debug)
+		cvmx_dprintf("%s: RATE=%llu BURST=%llu DIV_EXP=%d\n",
+			__func__,
+			CVMX_SHOFT_TO_U64(reg->s.rate_mantissa,
+					reg->s.rate_exponent),
+			CVMX_SHOFT_TO_U64(reg->s.burst_mantissa,
+					reg->s.burst_exponent),
+			div_exp);
+
+	/* Validate the resulting rate */
+	rate_v = CVMX_SHOFT_TO_U64(reg->s.rate_mantissa,
+				reg->s.rate_exponent) * tclk;
+	/* Convert to kbips for comaring with argument */
+	rate_v = (rate_v << (3+tock_bytes_exp)) /1000ULL;
+	/* Finally apply divider for best accuracy */
+	rate_v >>= div_exp;
+
+	burst_v = CVMX_SHOFT_TO_U64(reg->s.burst_mantissa,
+				reg->s.burst_exponent);
+	/* Convert in additional bytes as in argument */
+	burst_v = (burst_v - min_burst) << (tock_bytes_exp);
+	
+	if (debug)
+		cvmx_dprintf("%s: result rate=%'llu kbips burst=%llu bytes\n",
+			__func__,rate_v, burst_v);
+	
+	/* Compute error in parts-per-million */
+	rate_v = abs(rate_v - rate_kbips);
+	burst_v = abs(burst_v - burst_bytes);
+
+	if (debug)
+		cvmx_dprintf("%s: diff rate=%llu burst=%llu ppm\n",
+			__func__, rate_v, burst_v);
+
+	rate_v = (rate_v * 1000000ULL) / rate_kbips;
+	burst_v = (burst_v * 1000000ULL) / burst_bytes;
+
+	if (debug)
+		cvmx_dprintf("%s: error rate=%llu burst=%llu ppm\n",
+			__func__, rate_v, burst_v);
+
+	/* Allow ~ 100 ppm error for CIR, and 5% for BURST */
+	if (rate_v > 100)
+		return -rate_v;
+	if (burst_v > 5000)
+		return burst_v;
+
+	return 0;
+}
+
+/**
+ * Configure per-port CIR rate limit parameters
+ *
+ * This function configures rate limit at the L1/PQ level,
+ * i.e. for an entire MAC or physical port.
+ *
+ * @param node The OCI node where the target port is located
+ * @param pq_num The L1/PQ queue number for this setting
+ * @param rate_kbips The desired throughput in kilo-bits-per-second
+ * @param burst_size The size of a burst in bytes above 'rate_kbips' allowed
+ *
+ * @return Returns zero if both settings applied within allowed tolerance,
+ * otherwise the error is returned in parts-per-million.
+ * 'rate_bps" error is e negative number, otherwise 'birst_rate' error
+ * is returned as a positive integer.
+ */
+int cvmx_pko3_port_cir_set(unsigned node, unsigned pq_num,
+		unsigned long rate_kbips, unsigned burst_bytes)
+{
+	const unsigned time_wheel_turn = 96; /* S-Clock cycles */
+	unsigned long tclk;
+	cvmx_pko_l1_sqx_cir_t sqx_cir;
+	int rc;
+
+	if (debug)
+		cvmx_dprintf("%s: pq=%u rate=%lu kbps, burst=%u bytes\n",
+			__func__, pq_num, rate_kbips, burst_bytes);
+
+	sqx_cir.u64 = 0;
+
+	/* When rate == 0, disable the shaper */
+	if( rate_kbips == 0ULL) {
+		/* Disable shaping */
+		sqx_cir.s.enable = 0;
+		cvmx_write_csr_node(node,
+			CVMX_PKO_L1_SQX_CIR(pq_num), sqx_cir.u64);
+		return 0;
+	}
+
+	/* Compute time-wheel frequency */
+	tclk = cvmx_clock_get_rate_node(node, CVMX_CLOCK_SCLK)/
+		time_wheel_turn;
+
+	/* Compute shaper values */
+	rc = cvmx_pko3_shaper_rate_compute(tclk, &sqx_cir,
+		rate_kbips, burst_bytes);
+
+	/* FIXME: should refuse to set register if insane rates ? */
+
+	/* Enable shaping */
+	sqx_cir.s.enable = 1;
+
+	/* Apply new settings */
+	cvmx_write_csr_node(node, CVMX_PKO_L1_SQX_CIR(pq_num), sqx_cir.u64);
+
+	return rc;
+}
+
+/**
+ * Configure per-queue CIR rate limit parameters
+ *
+ * This function configures rate limit at the descriptor queue level.
+ *
+ * @param node The OCI node where the target port is located
+ * @param dq_num The descriptor queue number for this setting
+ * @param rate_kbips The desired throughput in kilo-bits-per-second
+ * @param burst_size The size of a burst in bytes above 'rate_kbips' allowed
+ *
+ * @return Returns zero if both settings applied within allowed tolerance,
+ * otherwise the error is returned in parts-per-million.
+ * 'rate_bps" error is e negative number, otherwise 'birst_rate' error
+ * is returned as a positive integer.
+ */
+int cvmx_pko3_dq_cir_set(unsigned node, unsigned dq_num,
+		unsigned long rate_kbips, unsigned burst_bytes)
+{
+	const unsigned time_wheel_turn = 768; /* S-Clock cycles */
+	unsigned long tclk;
+	cvmx_pko_l1_sqx_cir_t sqx_cir;
+	cvmx_pko_dqx_cir_t dqx_cir;
+	int rc;
+
+	dq_num &= (1<<10)-1;
+
+	if (debug)
+		cvmx_dprintf("%s: dq=%u rate=%lu kbps, burst=%u bytes\n",
+			__func__, dq_num, rate_kbips, burst_bytes);
+
+	dqx_cir.u64 = 0;
+
+	/* When rate == 0, disable the shaper */
+	if( rate_kbips == 0ULL) {
+		/* Disable shaping */
+		dqx_cir.s.enable = 0;
+		cvmx_write_csr_node(node,
+			CVMX_PKO_DQX_CIR(dq_num), dqx_cir.u64);
+		return 0;
+	}
+
+	/* Compute time-wheel frequency */
+	tclk = cvmx_clock_get_rate_node(node, CVMX_CLOCK_SCLK)/
+		time_wheel_turn;
+
+	/* Compute shaper values */
+	rc = cvmx_pko3_shaper_rate_compute(tclk, &sqx_cir,
+		rate_kbips, burst_bytes);
+
+	/* FIXME: should refuse to set register if insane rates ? */
+
+	/* Enable shaping */
+	dqx_cir.s.enable = 1;
+	dqx_cir.s. rate_divider_exponent = sqx_cir.s. rate_divider_exponent;
+	dqx_cir.s. rate_mantissa  = sqx_cir.s. rate_mantissa;
+	dqx_cir.s. rate_exponent  = sqx_cir.s. rate_exponent;
+	dqx_cir.s. burst_mantissa = sqx_cir.s. burst_mantissa;
+	dqx_cir.s. burst_exponent = sqx_cir.s. burst_exponent ;
+
+	/* Apply new settings */
+	cvmx_write_csr_node(node, CVMX_PKO_DQX_CIR(dq_num), dqx_cir.u64);
+
+	return rc;
+}
+
+/**
+ * Configure per-queue PIR rate limit parameters
+ *
+ * This function configures rate limit at the descriptor queue level.
+ *
+ * @param node The OCI node where the target port is located
+ * @param dq_num The descriptor queue number for this setting
+ * @param rate_kbips The desired throughput in kilo-bits-per-second
+ * @param burst_size The size of a burst in bytes above 'rate_kbips' allowed
+ *
+ * @return Returns zero if both settings applied within allowed tolerance,
+ * otherwise the error is returned in parts-per-million.
+ * 'rate_bps" error is e negative number, otherwise 'birst_rate' error
+ * is returned as a positive integer.
+ */
+int cvmx_pko3_dq_pir_set(unsigned node, unsigned dq_num,
+		unsigned long rate_kbips, unsigned burst_bytes)
+{
+	const unsigned time_wheel_turn = 768; /* S-Clock cycles */
+	unsigned long tclk;
+	cvmx_pko_l1_sqx_cir_t sqx_cir;
+	cvmx_pko_dqx_pir_t dqx_pir;
+	int rc;
+
+	dq_num &= (1<<10)-1;
+	if (debug)
+		cvmx_dprintf("%s: dq=%u rate=%lu kbps, burst=%u bytes\n",
+			__func__, dq_num, rate_kbips, burst_bytes);
+
+	dqx_pir.u64 = 0;
+
+	/* When rate == 0, disable the shaper */
+	if( rate_kbips == 0ULL) {
+		/* Disable shaping */
+		dqx_pir.s.enable = 0;
+		cvmx_write_csr_node(node,
+			CVMX_PKO_DQX_PIR(dq_num), dqx_pir.u64);
+		return 0;
+	}
+
+	/* Compute time-wheel frequency */
+	tclk = cvmx_clock_get_rate_node(node, CVMX_CLOCK_SCLK)/
+		time_wheel_turn;
+
+	/* Compute shaper values */
+	rc = cvmx_pko3_shaper_rate_compute(tclk, &sqx_cir,
+		rate_kbips, burst_bytes);
+
+	/* FIXME: should refuse to set register if insane rates ? */
+
+	/* Enable shaping */
+	dqx_pir.s.enable = 1;
+	dqx_pir.s. rate_divider_exponent = sqx_cir.s. rate_divider_exponent;
+	dqx_pir.s. rate_mantissa  = sqx_cir.s. rate_mantissa;
+	dqx_pir.s. rate_exponent  = sqx_cir.s. rate_exponent;
+	dqx_pir.s. burst_mantissa = sqx_cir.s. burst_mantissa;
+	dqx_pir.s. burst_exponent = sqx_cir.s. burst_exponent ;
+
+	/* Apply new settings */
+	cvmx_write_csr_node(node, CVMX_PKO_DQX_PIR(dq_num), dqx_pir.u64);
+
+	return rc;
+}
+
+/**
+ * Configure per-queue treatment of excess traffic
+ *
+ * The default and most sensible behavior is to stall the packets
+ * colored Red (i.e. exceeding the PIR rate in full 3-color mode).
+ * There is also the option to discard excess traffic, which is
+ * the desired action for some applications that do not rely on
+ * back-pressure flow control.
+ * The shaper may be programmed to pass the RED packets onwards,
+ * which may be useful it the color is translated to a change
+ * in packet priority on egress.
+ *
+ * @param node The OCI node where the target port is located
+ * @param dq_num The descriptor queue number for this setting
+ * @param red_act The action required for all packets in excess of PIR
+ * @param len_adjust A 2's complement 8 bit value to add/subtract from
+ * packet length for the purpose of shaping calculations, e.g.
+ * a value of -14 will subtract the length of the Ethernet header
+ * and hence only account IP packet size.
+ *
+ * @return N/A
+ */
+void cvmx_pko3_dq_red(unsigned node, unsigned dq_num, red_action_t red_act,
+	int8_t len_adjust)
+{
+	cvmx_pko_dqx_shape_t dqx_shape;
+
+	dq_num &= (1<<10)-1;
+
+	dqx_shape.u64 = 0;
+        dqx_shape.s.adjust = len_adjust;
+
+	switch(red_act) {
+		default:
+		case CVMX_PKO3_SHAPE_RED_STALL:
+			dqx_shape.s.red_algo = 0x0;
+			break;
+		case CVMX_PKO3_SHAPE_RED_DISCARD:
+			dqx_shape.s.red_algo = 0x3;
+			break;
+		case CVMX_PKO3_SHAPE_RED_PASS:
+			dqx_shape.s.red_algo = 0x1;
+			break;
+		}
+
+	cvmx_write_csr_node(node, CVMX_PKO_DQX_SHAPE(dq_num), dqx_shape.u64);
+}
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pko3-resources.c b/arch/mips/cavium-octeon/executive/cvmx-pko3-resources.c
new file mode 100644
index 0000000..d3043af
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-pko3-resources.c
@@ -0,0 +1,173 @@
+/***********************license start***************
+ * Copyright (c) 2003-2013  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * PKO resources.
+ */
+
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include <linux/module.h>
+#include <asm/octeon/cvmx.h>
+#include <asm/octeon/cvmx-pko3.h>
+#include "asm/octeon/cvmx-global-resources.h"
+#include <asm/octeon/cvmx-pko3-resources.h>
+#include "asm/octeon/cvmx-range.h"
+#else
+#include "cvmx.h"
+#include "cvmx-pko3.h"
+#include "cvmx-global-resources.h"
+#include "cvmx-pko3-resources.h"
+#include "cvmx-range.h"
+#endif
+
+#define CVMX_GR_TAG_PKO_PORT_QUEUES(x)   cvmx_get_gr_tag('c','v','m','_','p','k','o','p','o','q','_',(x+'0'),'.','.','.','.')
+#define CVMX_GR_TAG_PKO_L2_QUEUES(x)   	 cvmx_get_gr_tag('c','v','m','_','p','k','o','l','2','q','_',(x+'0'),'.','.','.','.')
+#define CVMX_GR_TAG_PKO_L3_QUEUES(x)     cvmx_get_gr_tag('c','v','m','_','p','k','o','l','3','q','_',(x+'0'),'.','.','.','.')
+#define CVMX_GR_TAG_PKO_L4_QUEUES(x)     cvmx_get_gr_tag('c','v','m','_','p','k','o','l','4','q','_',(x+'0'),'.','.','.','.')
+#define CVMX_GR_TAG_PKO_L5_QUEUES(x)     cvmx_get_gr_tag('c','v','m','_','p','k','o','l','5','q','_',(x+'0'),'.','.','.','.')
+#define CVMX_GR_TAG_PKO_DESCR_QUEUES(x)  cvmx_get_gr_tag('c','v','m','_','p','k','o','d','e','q','_',(x+'0'),'.','.','.','.')
+#define CVMX_GR_TAG_PKO_PORT_INDEX(x)  	 cvmx_get_gr_tag('c','v','m','_','p','k','o','p','i','d','_',(x+'0'),'.','.','.','.')
+
+const int cvmx_pko_num_queues_78XX[CVMX_PKO_NUM_QUEUE_LEVELS] = 
+{
+	[CVMX_PKO_PORT_QUEUES] = 32,
+	[CVMX_PKO_L2_QUEUES] = 512,
+	[CVMX_PKO_L3_QUEUES] = 512,
+	[CVMX_PKO_L4_QUEUES] = 1024,
+	[CVMX_PKO_L5_QUEUES] = 1024,
+	[CVMX_PKO_DESCR_QUEUES] = 1024
+};
+
+static inline int __cvmx_pko3_get_num_queues(int level)
+{
+	if(OCTEON_IS_MODEL(OCTEON_CN78XX))
+		return cvmx_pko_num_queues_78XX[level];
+	return -1;
+}
+
+static inline struct global_resource_tag __cvmx_pko_get_queues_resource_tag(int node, int queue_level)
+{
+	switch(queue_level) {
+		case CVMX_PKO_PORT_QUEUES:
+			return CVMX_GR_TAG_PKO_PORT_QUEUES(node);
+		case CVMX_PKO_L2_QUEUES:
+			return CVMX_GR_TAG_PKO_L2_QUEUES(node);
+		case CVMX_PKO_L3_QUEUES:
+			return CVMX_GR_TAG_PKO_L3_QUEUES(node);
+		case CVMX_PKO_L4_QUEUES:
+			return CVMX_GR_TAG_PKO_L4_QUEUES(node);
+		case CVMX_PKO_L5_QUEUES:
+			return CVMX_GR_TAG_PKO_L5_QUEUES(node);
+		case CVMX_PKO_DESCR_QUEUES:
+			return CVMX_GR_TAG_PKO_DESCR_QUEUES(node);
+		default:
+			return CVMX_GR_TAG_INVALID;
+	}
+}
+
+/**
+ * Allocate or reserve a pko resource - called by wrapper functions
+ * @param tag processed global resource tag
+ * @param base_queue if specified the queue to reserve
+ * @param owner to be specified for resource
+ * @param num_queues to allocate
+ * @param max_num_queues for global resource
+ */
+int cvmx_pko_alloc_global_resource(struct global_resource_tag tag, int base_queue, int owner, int num_queues, int max_num_queues)
+{
+	int res;
+	if(cvmx_create_global_resource_range(tag, max_num_queues)) {
+		cvmx_dprintf("ERROR: Failed to create PKO3 resource: %lx-%lx\n",
+			(unsigned long) tag.hi, (unsigned long) tag.lo);
+		return -1;
+	}
+	if(base_queue >= 0) {
+		res = cvmx_reserve_global_resource_range(tag, owner, base_queue, num_queues);
+	} else {
+		res = cvmx_allocate_global_resource_range(tag, owner, num_queues, 1);
+	}
+	if(res < 0) {
+		cvmx_dprintf("ERROR: Failed to %s PKO3 tag %lx:%lx, %i %i %i %i.\n",
+			((base_queue < 0) ? "allocate" : "reserve"),
+			(unsigned long) tag.hi, (unsigned long) tag.lo,
+			base_queue, owner, num_queues, max_num_queues);
+		return -1;
+	}
+
+	return res;
+}
+
+/**
+ * Allocate or reserve PKO queues - wrapper for cvmx_pko_alloc_global_resource
+ *
+ * @param node on which to allocate/reserve PKO queues
+ * @param level of PKO queue
+ * @param owner of reserved/allocated resources
+ * @param base_queue to start reservation/allocatation
+ * @param num_queues number of queues to be allocated
+ * @return 0 on success, -1 on failure
+ */
+int cvmx_pko_alloc_queues(int node, int level, int owner, int base_queue, int num_queues)
+{
+	struct global_resource_tag tag = __cvmx_pko_get_queues_resource_tag(node, level);
+	int max_num_queues = __cvmx_pko3_get_num_queues(level);
+
+	return cvmx_pko_alloc_global_resource(tag, base_queue, owner, num_queues, max_num_queues);
+}
+EXPORT_SYMBOL(cvmx_pko_alloc_queues);
+
+/**
+ * Free an allocated/reserved PKO queues for a certain level and owner
+ *
+ * @param node on which to allocate/reserve PKO queues
+ * @param level of PKO queue
+ * @param owner of reserved/allocated resources
+ * @return 0 on success, -1 on failure
+ */
+int cvmx_pko_free_queues(int node, int level, int owner)
+{
+	struct global_resource_tag tag = __cvmx_pko_get_queues_resource_tag(node, level);
+
+	return cvmx_free_global_resource_range_with_owner(tag, owner);
+}
+EXPORT_SYMBOL(cvmx_pko_free_queues);
+
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pko3.c b/arch/mips/cavium-octeon/executive/cvmx-pko3.c
new file mode 100644
index 0000000..e7bbf82
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-pko3.c
@@ -0,0 +1,1613 @@
+/***********************license start***************
+ * Copyright (c) 2003-2013  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+#include <linux/module.h>
+#include <linux/errno.h>
+#include <asm/octeon/cvmx.h>
+#include <asm/octeon/cvmx-config.h>
+#include <asm/octeon/cvmx-fpa3.h>
+#include <asm/octeon/cvmx-clock.h>
+#include <asm/octeon/cvmx-pko.h>
+#include <asm/octeon/cvmx-pko3.h>
+#include <asm/octeon/cvmx-helper-pko3.h>
+
+
+static const int debug = 0;
+#ifdef	__BIG_ENDIAN_BITFIELD
+static const bool __native_le = 0;
+#else
+static const bool __native_le = 1;
+#endif
+
+#define CVMX_DUMP_REGX(reg) 	\
+	if(debug)		\
+	cvmx_dprintf("%s=%#llx\n",#reg,(long long)cvmx_read_csr_node(node,reg))
+
+int32_t __cvmx_pko3_dq_depth[1024];
+
+static int cvmx_pko_setup_macs(int node);
+
+/*
+ * PKO descriptor queue operation error string
+ *
+ * @param dqstatus is the enumeration returned from hardware,
+ * 	  PKO_QUERY_RTN_S[DQSTATUS].
+ *
+ * @return static constant string error description
+ */
+const char * pko_dqstatus_error(pko_query_dqstatus_t dqstatus)
+{
+	char * str = "PKO Undefined error";
+
+	switch(dqstatus) {
+	case PKO_DQSTATUS_PASS :
+		str = "No error";
+		break;
+	case PKO_DQSTATUS_BADSTATE :
+		str = "PKO queue not ready";
+		break;
+	case PKO_DQSTATUS_NOFPABUF :
+		str ="PKO failed to allocate buffer from FPA";
+		break;
+	case PKO_DQSTATUS_NOPKOBUF :
+		str = "PKO out of buffers";
+		break;
+	case PKO_DQSTATUS_FAILRTNPTR :
+		str = "PKO failed to return buffer to FPA";
+		break;
+	case PKO_DQSTATUS_ALREADY :
+		str = "PKO queue already opened";
+		break;
+	case PKO_DQSTATUS_NOTCREATED:
+		str = "PKO queue has not been created";
+		break;
+	case PKO_DQSTATUS_NOTEMPTY :
+		str = "PKO queue is not empty";
+		break;
+	case PKO_DQSTATUS_SENDPKTDROP :
+		str = "Illegal PKO command construct";
+		break;
+	}
+	return str;
+}
+
+/*
+ * PKO global intialization for 78XX.
+ *
+ * @param node is the node on which PKO block is initialized.
+ * @return none.
+ */
+int cvmx_pko3_hw_init_global(int node, uint16_t aura)
+{
+	cvmx_pko_dpfi_flush_t pko_flush;
+	cvmx_pko_dpfi_fpa_aura_t pko_aura;
+	cvmx_pko_dpfi_ena_t dpfi_enable;
+	cvmx_pko_ptf_iobp_cfg_t ptf_iobp_cfg;
+	cvmx_pko_pdm_cfg_t pko_pdm_cfg;
+	cvmx_pko_enable_t pko_enable;
+	cvmx_pko_dpfi_status_t dpfi_status;
+	union cvmx_pko_status pko_status;
+	uint64_t cycles;
+	const unsigned timeout = 100;	/* 100 milliseconds */
+
+	pko_enable.u64 = cvmx_read_csr_node(node, CVMX_PKO_ENABLE);
+	if (pko_enable.s.enable) {
+		cvmx_dprintf("WARNING: %s: PKO already enabled on node %u\n",
+			__func__, node);
+		return 0;
+	}
+
+	/* Clear FLUSH command to be sure */
+	pko_flush.u64 = 0;
+	pko_flush.s.flush_en = 0;
+	cvmx_write_csr_node(node, CVMX_PKO_DPFI_FLUSH, pko_flush.u64);
+
+	/* set the aura number in pko, use aura node from parameter */
+	pko_aura.u64 = 0;
+	pko_aura.s.node = aura >> 10;
+	pko_aura.s.laura = aura & (CVMX_FPA3_AURA_NUM-1);
+	cvmx_write_csr_node(node, CVMX_PKO_DPFI_FPA_AURA, pko_aura.u64);
+
+	CVMX_DUMP_REGX(CVMX_PKO_DPFI_FPA_AURA);
+
+	dpfi_enable.u64 = 0;
+	dpfi_enable.s.enable = 1;
+	cvmx_write_csr_node(node, CVMX_PKO_DPFI_ENA, dpfi_enable.u64);
+
+	/* Prepare timeout */
+        cycles = cvmx_get_cycle();
+        cycles += cvmx_clock_get_rate(CVMX_CLOCK_CORE)/1000 * timeout;
+
+	/* Wait until all pointers have been returned */
+	do {
+		pko_status.u64 = cvmx_read_csr_node(node, CVMX_PKO_STATUS);
+		if (cycles < cvmx_get_cycle())
+			break;
+	} while (!pko_status.s.pko_rdy);
+
+	if (!pko_status.s.pko_rdy) {
+		dpfi_status.u64 = cvmx_read_csr_node(node, CVMX_PKO_DPFI_STATUS);
+		cvmx_dprintf("ERROR: %s: PKO DFPI failed, "
+			"PKO_STATUS=%#llx DPFI_STATUS=%#llx\n", __func__,
+			(unsigned long long) pko_status.u64,
+			(unsigned long long) dpfi_status.u64);
+		return -1;
+	}
+
+	/* set max outstanding requests in IOBP for any FIFO */
+	ptf_iobp_cfg.u64 = 0;
+	ptf_iobp_cfg.s.max_read_size = 72;	/* HRM: typical=0x48 */
+	cvmx_write_csr_node(node, CVMX_PKO_PTF_IOBP_CFG, ptf_iobp_cfg.u64);
+
+	/* Set minimum packet size per Ethernet standard */
+	pko_pdm_cfg.u64 = 0;
+	pko_pdm_cfg.s.pko_pad_minlen = 0x3c;	/* 60 bytes before FCS */
+	cvmx_write_csr_node(node, CVMX_PKO_PDM_CFG, pko_pdm_cfg.u64);
+
+	/* Initialize MACs and FIFOs */
+	cvmx_pko_setup_macs(node);
+
+	/* enable PKO, although interfaces and queues are not up yet */
+	pko_enable.u64 = 0;
+	pko_enable.s.enable = 1;
+	cvmx_write_csr_node(node, CVMX_PKO_ENABLE, pko_enable.u64);
+
+	/* PKO_RDY set indicates succesful initialization */
+	pko_status.u64 = cvmx_read_csr_node(node, CVMX_PKO_STATUS);
+	if (pko_status.s.pko_rdy)
+		return 0;
+
+	cvmx_dprintf("ERROR: %s: failed, PKO_STATUS=%#llx\n", __func__,
+		(unsigned long long) pko_status.u64);
+	return -1;
+}
+
+/**
+ * Shutdown the entire PKO
+ */
+int cvmx_pko3_hw_disable(int node)
+{
+	cvmx_pko_dpfi_flush_t pko_flush;
+	cvmx_pko_dpfi_status_t dpfi_status;
+	cvmx_pko_dpfi_ena_t dpfi_enable;
+	cvmx_pko_enable_t pko_enable;
+	union cvmx_pko_status pko_status;
+	uint64_t cycles;
+	const unsigned timeout = 100;	/* 100 milliseconds */
+	unsigned mac_num, fifo, i;
+
+	(void) pko_status;
+
+	/* Wait until there are no in-flight packets */
+	for(i = mac_num = 0; mac_num < CVMX_PKO_MAX_MACS; mac_num++) {
+		cvmx_pko_ptfx_status_t ptf_status;
+		ptf_status.u64 =
+			cvmx_read_csr_node(node, CVMX_PKO_PTFX_STATUS(mac_num));
+		if (debug)
+			cvmx_dprintf("%s: MAC %u in-flight %u total %u\n",
+				__func__, mac_num,
+				ptf_status.s.in_flight_cnt,
+				ptf_status.s.total_in_flight_cnt);
+		if (ptf_status.s.mac_num == 0x1f)
+			continue;
+		if (ptf_status.s.in_flight_cnt != 0) {
+			cvmx_dprintf("%s: MAC %d in-flight %d\n",
+				__func__, mac_num, ptf_status.s.in_flight_cnt);
+			mac_num --;
+			cvmx_wait(1000);
+		}
+	}
+
+	//XXX- try to disable PKO first, then flush the DPFI
+	/* disable PKO - all packets should be out by now */
+	pko_enable.u64 = 0;
+	pko_enable.s.enable = 0;
+	cvmx_write_csr_node(node, CVMX_PKO_ENABLE, pko_enable.u64);
+
+	/* Reset L1_SQ */
+	for(i = 0; i < 32; i++) {
+		cvmx_pko_l1_sqx_topology_t pko_l1_topology;
+		cvmx_pko_l1_sqx_shape_t pko_l1_shape;
+		cvmx_pko_l1_sqx_link_t pko_l1_link;
+		pko_l1_topology.u64 = 0;
+		pko_l1_topology.s.link = 0x1c;
+		cvmx_write_csr_node(node, CVMX_PKO_L1_SQX_TOPOLOGY(i),
+			pko_l1_topology.u64);
+
+		pko_l1_shape.u64 = 0;
+		pko_l1_shape.s.link = 0x1c;
+		cvmx_write_csr_node(node, CVMX_PKO_L1_SQX_SHAPE(i), pko_l1_shape.u64);
+
+		pko_l1_link.u64 = 0;
+		pko_l1_link.s.link = 0x1c;
+		cvmx_write_csr_node(node, CVMX_PKO_L1_SQX_LINK(i), pko_l1_link.u64);
+
+	}
+
+	/* Reset all MAC configurations */
+	for(mac_num = 0; mac_num < CVMX_PKO_MAX_MACS; mac_num++) {
+		cvmx_pko_macx_cfg_t pko_mac_cfg;
+
+		pko_mac_cfg.u64 = 0;
+		pko_mac_cfg.s.fifo_num = 0x1f;
+		cvmx_write_csr_node(node, CVMX_PKO_MACX_CFG(mac_num),
+			pko_mac_cfg.u64);
+	}
+
+	/* Reset all FIFO groups */
+	for(fifo = 0; fifo < 8; fifo++) {
+		cvmx_pko_ptgfx_cfg_t pko_ptgfx_cfg;
+
+		pko_ptgfx_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKO_PTGFX_CFG(fifo));
+		/* Simulator asserts if an unused group is reset */
+		if (pko_ptgfx_cfg.u64 == 0)
+			continue;
+		pko_ptgfx_cfg.u64 = 0;
+		pko_ptgfx_cfg.s.reset = 1;
+		cvmx_write_csr_node(node, CVMX_PKO_PTGFX_CFG(fifo),
+					pko_ptgfx_cfg.u64);
+	}
+
+	/* Set FLUSH_EN to return cached pointers to FPA */
+	pko_flush.u64 = 0;
+	pko_flush.s.flush_en = 1;
+	cvmx_write_csr_node(node, CVMX_PKO_DPFI_FLUSH, pko_flush.u64);
+
+	/* Prepare timeout */
+        cycles = cvmx_get_cycle();
+        cycles += cvmx_clock_get_rate(CVMX_CLOCK_CORE)/1000 * timeout;
+
+	/* Wait until all pointers have been returned */
+	do {
+		dpfi_status.u64 = cvmx_read_csr_node(node, CVMX_PKO_DPFI_STATUS);
+		if (cycles < cvmx_get_cycle())
+			break;
+	} while (!dpfi_status.s.cache_flushed);
+
+
+
+	/* disable PKO buffer manager, should return all buffers to FPA */
+	dpfi_enable.u64 = 0;
+	dpfi_enable.s.enable = 0;
+	cvmx_write_csr_node(node, CVMX_PKO_DPFI_ENA, dpfi_enable.u64);
+
+	CVMX_DUMP_REGX(CVMX_PKO_DPFI_ENA);
+	CVMX_DUMP_REGX(CVMX_PKO_DPFI_STATUS);
+	CVMX_DUMP_REGX(CVMX_PKO_STATUS);
+
+	/* Clear the FLUSH_EN bit, as we are done */
+	pko_flush.u64 = 0;
+	cvmx_write_csr_node(node, CVMX_PKO_DPFI_FLUSH, pko_flush.u64);
+	CVMX_DUMP_REGX(CVMX_PKO_DPFI_FLUSH);
+
+	if (dpfi_status.s.cache_flushed == 0) {
+		cvmx_dprintf("%s: ERROR: timeout waiting for PKO3 ptr flush\n",
+			__FUNCTION__);
+		return -1;
+	}
+
+	return 0;
+}
+
+ /** Open configured descriptor queues before queueing packets into them.
+ *
+ * @param node is to specify the node to which this configuration is applied.
+ * @param dq is the descriptor queue number to be opened.
+ * @return returns 0 on sucess or -1 on failure.
+ */
+int cvmx_pko_dq_open(int node, int dq)
+{
+	cvmx_pko_query_rtn_t pko_status;
+	pko_query_dqstatus_t dqstatus;
+
+	if(debug)
+		cvmx_dprintf("%s: DEBUG: dq %u\n", __FUNCTION__, dq);
+
+	pko_status = __cvmx_pko3_do_dma(node, dq, NULL, 0, CVMX_PKO_DQ_OPEN);
+
+	dqstatus = pko_status.s.dqstatus;
+
+	if (dqstatus == PKO_DQSTATUS_ALREADY)
+		return 0;
+	if (dqstatus != PKO_DQSTATUS_PASS) {
+		cvmx_dprintf("%s: ERROR: Failed to open dq :%u: %s\n",
+				__FUNCTION__, dq,
+				pko_dqstatus_error(dqstatus));
+		return -1;
+	}
+
+	return 0;
+}
+
+
+ /*
+ * Close a descriptor queue
+ *
+ * @param node is to specify the node to which this configuration is applied.
+ * @param dq is the descriptor queue number to be opened.
+ * @return returns 0 on sucess or -1 on failure.
+ *
+ * This should be called before changing the DQ parent link, topology,
+ * or when shutting down the PKO.
+ */
+int cvmx_pko3_dq_close(int node, int dq)
+{
+	cvmx_pko_query_rtn_t pko_status;
+	pko_query_dqstatus_t dqstatus;
+
+	if(debug)
+		cvmx_dprintf("%s: DEBUG: dq %u\n", __FUNCTION__, dq);
+
+	pko_status = __cvmx_pko3_do_dma(node, dq, NULL, 0, CVMX_PKO_DQ_CLOSE);
+
+	dqstatus = pko_status.s.dqstatus;
+
+#if 0
+	if (dqstatus == PKO_DQSTATUS_NOTCREATED)
+		return 0;
+#endif
+	if (dqstatus != PKO_DQSTATUS_PASS) {
+		cvmx_dprintf("%s: WARNING: Failed to close dq :%u: %s\n",	//XXX temp warn
+				__FUNCTION__, dq,
+				pko_dqstatus_error(dqstatus));
+		cvmx_dprintf("%s: DEBUG: dq %u depth %u\n",
+			__FUNCTION__, dq, (unsigned) pko_status.s.depth);
+//		return -1; XXX- temporarily errors ignored XXX
+	}
+	return 0;
+}
+
+/**
+ * Drain a descriptor queue
+ *
+ * Before closing a DQ, this call will drain all pending traffic
+ * on the DQ to the NULL MAC, which will circumvent any traffic
+ * shaping and flow control to quickly reclaim all packet buffers.
+ */
+void cvmx_pko3_dq_drain(int node, int dq)
+{
+	cvmx_pko_dqx_sw_xoff_t rxoff;
+
+	rxoff.u64 = 0;
+	rxoff.s.drain_null_link = 1;
+	rxoff.s.drain = 1;
+	rxoff.s.xoff = 0;
+
+	cvmx_write_csr_node(node, CVMX_PKO_DQX_SW_XOFF(dq), rxoff.u64);
+
+	cvmx_wait(100);
+
+	rxoff.u64 = 0;
+	cvmx_write_csr_node(node, CVMX_PKO_DQX_SW_XOFF(dq), rxoff.u64);
+}
+
+ /**
+ * Query a descriptor queue
+ *
+ * @param node is to specify the node to which this configuration is applied.
+ * @param dq is the descriptor queue number to be opened.
+ * @return returns the descriptor queue depth on sucess or -1 on failure.
+ *
+ * This should be called before changing the DQ parent link, topology,
+ * or when shutting down the PKO.
+ */
+int cvmx_pko3_dq_query(int node, int dq)
+{
+	cvmx_pko_query_rtn_t pko_status;
+	pko_query_dqstatus_t dqstatus;
+
+	pko_status = __cvmx_pko3_do_dma(node, dq, NULL, 0, CVMX_PKO_DQ_QUERY);
+
+	dqstatus = pko_status.s.dqstatus;
+
+	if (dqstatus != PKO_DQSTATUS_PASS) {
+		cvmx_dprintf("%s: ERROR: Failed to query dq :%u: %s\n",
+				__FUNCTION__, dq,
+				pko_dqstatus_error(dqstatus));
+		return -1;
+	}
+
+	/* Temp: debug for HW */
+	if (pko_status.s.depth > 0)
+		cvmx_dprintf("%s: DEBUG: dq %u depth %u\n",
+			__FUNCTION__, dq, (unsigned) pko_status.s.depth);
+
+	return pko_status.s.depth;
+}
+
+static struct cvmx_pko3_mac_s {
+	cvmx_helper_interface_mode_t mac_mode;
+	uint8_t fifo_cnt;
+	uint8_t fifo_id;
+	uint8_t pri;
+	uint8_t spd;
+	uint8_t mac_fifo_cnt;
+} cvmx_pko3_mac_table[ CVMX_PKO_MAX_MACS ];
+
+/*
+ * PKO initialization of MACs and FIFOs
+ *
+ * All MACs are configured and assigned a specific FIFO,
+ * and each FIFO is configured with size for a best utilization
+ * of available FIFO resources.
+ *
+ * @param node is to specify which node's pko block for this setup.
+ * @return returns 0 if successful and -1 on failure.
+ */
+static int cvmx_pko_setup_macs(int node)
+{
+	unsigned interface;
+	unsigned port, num_ports;
+	unsigned mac_num, fifo, pri, cnt;
+	cvmx_helper_interface_mode_t mode;
+        const unsigned num_interfaces = cvmx_helper_get_number_of_interfaces();
+	uint8_t fifo_group_cfg[8];
+	uint8_t fifo_group_spd[8];
+	unsigned fifo_count = 0;
+
+	/* Initialize FIFO allocation table */
+	memset(&fifo_group_cfg, 0, sizeof(fifo_group_cfg));
+	memset(&fifo_group_spd, 0, sizeof(fifo_group_spd));
+
+	/* Initialize all MACs as disabled */
+	for(mac_num = 0; mac_num < CVMX_PKO_MAX_MACS; mac_num++) {
+		cvmx_pko3_mac_table[mac_num].mac_mode =
+			CVMX_HELPER_INTERFACE_MODE_DISABLED;
+		cvmx_pko3_mac_table[mac_num].pri = 0;
+		cvmx_pko3_mac_table[mac_num].fifo_cnt = 0;
+		cvmx_pko3_mac_table[mac_num].fifo_id = 0x1f;
+	}
+
+	for(interface = 0; interface < num_interfaces; interface ++) {
+		mode = cvmx_helper_interface_get_mode(interface);
+		num_ports = cvmx_helper_interface_enumerate(interface);
+
+		if(mode == CVMX_HELPER_INTERFACE_MODE_DISABLED)
+			continue;
+		/*
+		 * Each of these interfaces has a single MAC really.
+		 */
+		if ((mode == CVMX_HELPER_INTERFACE_MODE_ILK) ||
+			(mode == CVMX_HELPER_INTERFACE_MODE_NPI) ||
+			(mode == CVMX_HELPER_INTERFACE_MODE_LOOP))
+			num_ports = 1;
+
+		for (port = 0; port < num_ports; port++) {
+			int i;
+
+			/* convert interface/port to mac number */
+			i = __cvmx_pko3_get_mac_num(interface, port);
+			if (i < 0 || i>= CVMX_PKO_MAX_MACS) {
+				cvmx_dprintf("%s: ERROR: interface %d port %d has no MAC\n",
+					__func__, interface, port);
+				continue;
+			}
+
+			cvmx_pko3_mac_table[i].mac_mode = mode;
+			if (mode == CVMX_HELPER_INTERFACE_MODE_XAUI) {
+				cvmx_pko3_mac_table[i].fifo_cnt = 4;
+				cvmx_pko3_mac_table[i].pri = 2;
+				/* DXAUI at 20G, or XAU at 10G */
+				cvmx_pko3_mac_table[i].spd = 20;
+				cvmx_pko3_mac_table[i].mac_fifo_cnt = 4;
+			} else if (mode == CVMX_HELPER_INTERFACE_MODE_ILK) {
+				cvmx_pko3_mac_table[i].fifo_cnt = 4;
+				cvmx_pko3_mac_table[i].pri = 3;
+				/* ILK: 40 Gbps or 20 Gbps */
+				cvmx_pko3_mac_table[i].spd = 40;
+				cvmx_pko3_mac_table[i].mac_fifo_cnt = 4;
+			} else {
+				cvmx_pko3_mac_table[i].fifo_cnt = 1;
+				cvmx_pko3_mac_table[i].pri = 1;
+				cvmx_pko3_mac_table[i].spd = 1;
+				cvmx_pko3_mac_table[i].mac_fifo_cnt = 1;
+			}
+
+			if(debug)
+				cvmx_dprintf("%s: intf %u port %u %s "
+					"mac %02u cnt %u spd %u\n",
+				__FUNCTION__, interface, port,
+				cvmx_helper_interface_mode_to_string(mode),
+				i, cvmx_pko3_mac_table[i].fifo_cnt,
+				cvmx_pko3_mac_table[i].spd);
+
+		} /* for port */
+	} /* for interface */
+
+	/* Count the number of requested FIFOs */
+	for(fifo_count = mac_num = 0; mac_num < CVMX_PKO_MAX_MACS; mac_num ++)
+		fifo_count += cvmx_pko3_mac_table[mac_num].fifo_cnt;
+
+	if(debug)
+		cvmx_dprintf("%s: initially reqyested FIFO count %u\n",
+			__FUNCTION__, fifo_count);
+
+	/* Heuristically trim FIFO count to fit in available number */
+	pri = 1; cnt = 4;
+	while(fifo_count > 28) {
+		for(mac_num=0; mac_num < CVMX_PKO_MAX_MACS; mac_num ++) {
+			if (cvmx_pko3_mac_table[mac_num].fifo_cnt == cnt &&
+			    cvmx_pko3_mac_table[mac_num].pri <= pri) {
+				cvmx_pko3_mac_table[mac_num].fifo_cnt >>= 1;
+				fifo_count -=
+					cvmx_pko3_mac_table[mac_num].fifo_cnt;
+			}
+			if (fifo_count <= 28)
+				break;
+		}
+		if (pri >= 4) {
+			pri = 1;
+			cnt >>= 1;
+		} else
+			pri ++;
+		if( cnt == 0)
+			break;
+	}
+
+	if(debug)
+		cvmx_dprintf("%s: adjusted FIFO count %u\n",
+			__FUNCTION__, fifo_count);
+
+
+	/* Special case for NULL Virtual FIFO */
+	fifo_group_cfg[28 >> 2] = 0;
+	/* there is no MAC connected to NULL FIFO */
+
+	/* Configure MAC units, and attach a FIFO to each */
+	for(fifo = 0, cnt = 4; cnt > 0; cnt >>= 1 ) {
+		unsigned g;
+		for(mac_num = 0; mac_num < CVMX_PKO_MAX_MACS; mac_num++) {
+			if(cvmx_pko3_mac_table[mac_num].fifo_cnt < cnt ||
+			  cvmx_pko3_mac_table[mac_num].fifo_id != 0x1f)
+				continue;
+
+			/* Attach FIFO to MAC */
+			cvmx_pko3_mac_table[mac_num].fifo_id = fifo;
+			g = fifo >> 2;
+			/* Sum speed for FIFO group */
+			fifo_group_spd[g] += cvmx_pko3_mac_table[mac_num].spd;
+
+			if(cnt == 4)
+				fifo_group_cfg[g] = 4; /* 10k,0,0,0 */
+			else if(cnt == 2 && (fifo & 0x3) == 0)
+				fifo_group_cfg[g] = 3; /* 5k,0,5k,0 */
+			else if (cnt == 2 && fifo_group_cfg[g] == 3)
+				/* no change */;
+			else if(cnt == 1 && (fifo & 0x2) &&
+				fifo_group_cfg[g] == 3)
+				fifo_group_cfg[g] = 1; /* 5k,0,2.5k 2.5k*/
+			else if(cnt == 1 && (fifo & 0x3)==0x3)
+				/* no change */;
+			else if (cnt == 1)
+				fifo_group_cfg[g] = 0; /* 2.5k x 4 */
+			else
+				cvmx_dprintf("%s: internal error\n",__func__);
+
+			fifo += cnt;
+		}
+	}
+
+	/* Check if there was no error in FIFO allocation */
+	if( fifo > 28 ){
+		cvmx_dprintf("%s: ERROR: Internal error FIFO %u\n",
+			__FUNCTION__, fifo);
+		return -1;
+	}
+
+	if(debug)
+		cvmx_dprintf("%s: used %u of FIFOs\n",
+			__FUNCTION__, fifo);
+
+	/* Now configure all FIFO groups */
+	for(fifo = 0; fifo < 8; fifo++) {
+		cvmx_pko_ptgfx_cfg_t pko_ptgfx_cfg;
+
+		pko_ptgfx_cfg.u64 =
+			cvmx_read_csr_node(node, CVMX_PKO_PTGFX_CFG(fifo));
+		if( pko_ptgfx_cfg.s.size != fifo_group_cfg[fifo])
+			pko_ptgfx_cfg.s.reset = 1;
+		pko_ptgfx_cfg.s.size = fifo_group_cfg[fifo] ;
+		if( fifo_group_spd[fifo] >= 40 )
+			pko_ptgfx_cfg.s.rate = 3;	/* 50 Gbps */
+		else if( fifo_group_spd[fifo] >= 20 )
+			pko_ptgfx_cfg.s.rate = 2;	/* 25 Gbps */
+		else if( fifo_group_spd[fifo] >= 10 )
+			pko_ptgfx_cfg.s.rate = 1;	/* 12.5 Gbps */
+		else
+			pko_ptgfx_cfg.s.rate = 0;	/* 6.25 Gbps */
+
+		if(debug)
+			cvmx_dprintf("%s: FIFO %#x-%#x size=%u "
+				"speed=%d rate=%d\n",
+				__func__, fifo*4, fifo*4+3,
+				 pko_ptgfx_cfg.s.size,
+				fifo_group_spd[fifo],
+				pko_ptgfx_cfg.s.rate);
+
+		cvmx_write_csr_node(node, CVMX_PKO_PTGFX_CFG(fifo),
+					pko_ptgfx_cfg.u64);
+		pko_ptgfx_cfg.s.reset = 0;
+		cvmx_write_csr_node(node, CVMX_PKO_PTGFX_CFG(fifo),
+					pko_ptgfx_cfg.u64);
+	}
+
+	/* Configure all 28 MACs assigned FIFO number */
+	for(mac_num = 0; mac_num < CVMX_PKO_MAX_MACS; mac_num++) {
+		cvmx_pko_macx_cfg_t pko_mac_cfg;
+
+		if(debug)
+			cvmx_dprintf("%s: mac#%02u: fifo=%#x cnt=%u speed=%d\n",
+			__FUNCTION__, mac_num,
+			cvmx_pko3_mac_table[mac_num].fifo_id,
+			cvmx_pko3_mac_table[mac_num].fifo_cnt,
+			cvmx_pko3_mac_table[mac_num].spd);
+
+		pko_mac_cfg.u64 =
+			cvmx_read_csr_node(node, CVMX_PKO_MACX_CFG(mac_num));
+		pko_mac_cfg.s.fifo_num = cvmx_pko3_mac_table[mac_num].fifo_id;
+		cvmx_write_csr_node(node, CVMX_PKO_MACX_CFG(mac_num),
+			pko_mac_cfg.u64);
+	}
+
+
+	/* Setup PKO MCI0/MCI1/SKID credits */
+	for(mac_num = 0; mac_num < CVMX_PKO_MAX_MACS; mac_num++) {
+		cvmx_pko_mci0_max_credx_t pko_mci0_max_cred;
+		cvmx_pko_mci1_max_credx_t pko_mci1_max_cred;
+		cvmx_pko_macx_cfg_t pko_mac_cfg;
+		unsigned fifo_credit, mac_credit, skid_credit;
+		unsigned pko_fifo_cnt, fifo_size;
+		unsigned mac_fifo_cnt;
+		unsigned tmp;
+
+		/* FIXME- this section has no basis in HRM, revisit */
+		/* Loosely based on packet/clear78.x */
+		pko_fifo_cnt = cvmx_pko3_mac_table[mac_num].fifo_cnt;
+		mac_fifo_cnt = cvmx_pko3_mac_table[mac_num].mac_fifo_cnt;
+
+		/* Skip unused MACs */
+		if (pko_fifo_cnt == 0)
+			continue;
+
+		/* Check for sanity */
+		if (pko_fifo_cnt > 4)
+			pko_fifo_cnt = 1;
+
+		fifo_size = (2 * 1024) + (1024 / 2); /* 2.5KiB */
+		fifo_credit = pko_fifo_cnt * fifo_size;
+
+		/* FIXME- This code is chip-dependent, not portable! */
+		switch (mac_num) {
+			case 0: /* loopback */
+				mac_credit = 4096; /* From HRM Sec 13.0 */
+				skid_credit = 0;
+				break;
+			case 1: /* DPI */
+				mac_credit = 2 * 1024;
+				skid_credit = 0;
+				break;
+			case 2: /* ILK0 */
+			case 3: /* ILK1 */
+				mac_credit = 4 * 1024; /* 4KB fifo */
+				skid_credit = 0;
+				break;
+			default: /* BGX */
+				mac_credit = mac_fifo_cnt * 8 * 1024;
+				skid_credit = mac_fifo_cnt * 256;
+				break;
+		} /* switch mac_num */
+
+		if(debug) cvmx_dprintf(
+			"%s: mac %u "
+			"pko_fifo_credit=%u mac_credit=%u\n",
+			__FUNCTION__, mac_num, fifo_credit, mac_credit);
+
+		tmp = (fifo_credit + mac_credit) / 16;
+		pko_mci0_max_cred.u64 = 0;
+		pko_mci0_max_cred.s.max_cred_lim = tmp;
+
+		/* Check for overflow */
+		if (pko_mci0_max_cred.s.max_cred_lim != tmp) {
+			cvmx_dprintf("%s: MCI0 credit overflow\n",__FUNCTION__);
+			pko_mci0_max_cred.s.max_cred_lim = 0xfff;
+		}
+
+		cvmx_write_csr_node(node, CVMX_PKO_MCI0_MAX_CREDX(mac_num),
+					pko_mci0_max_cred.u64);
+
+		/* The original CSR formula is the correct one after all */
+		tmp = (mac_credit) / 16;
+		pko_mci1_max_cred.u64 = 0;
+		pko_mci1_max_cred.s.max_cred_lim = tmp;
+
+		/* Check for overflow */
+		if (pko_mci1_max_cred.s.max_cred_lim != tmp) {
+			cvmx_dprintf("%s: MCI1 credit overflow\n",__FUNCTION__);
+			pko_mci1_max_cred.s.max_cred_lim = 0xfff;
+		}
+
+		cvmx_write_csr_node(node, CVMX_PKO_MCI1_MAX_CREDX(mac_num),
+					pko_mci1_max_cred.u64);
+
+		tmp = (skid_credit / 256) >> 1; /* valid 0,1,2 */
+		pko_mac_cfg.u64 =
+			cvmx_read_csr_node(node, CVMX_PKO_MACX_CFG(mac_num));
+		pko_mac_cfg.s.skid_max_cnt = tmp;
+		cvmx_write_csr_node(node, CVMX_PKO_MACX_CFG(mac_num),
+			pko_mac_cfg.u64);
+
+		if(debug) cvmx_dprintf(
+			"%s: mac %u PKO_MCI0_MAX_CREDX=%u PKO_MCI1_MAX_CREDX=%u PKO_MACX_CFG[SKID_MAX_CNT]=%u\n",
+			__FUNCTION__,  mac_num,
+			pko_mci0_max_cred.s.max_cred_lim,
+			pko_mci1_max_cred.s.max_cred_lim,
+			pko_mac_cfg.s.skid_max_cnt);
+	} /* for mac_num */
+
+	return 0;
+}
+
+/** Set MAC options
+ *
+ * The options supported are the parameters below:
+ *
+ * @param xiface The physical interface number
+ * @param index The physical sub-interface port
+ * @param fcs_enable Enable FCS generation
+ * @param pad_enable Enable padding to minimum packet size
+ * @param fcs_sop_off Number of bytes at start of packet to exclude from FCS
+ *
+ * The typical use for `fcs_sop_off` is when the interface is configured
+ * to use a header such as HighGig to precede every Ethernet packet,
+ * such a header usually does not partake in the CRC32 computation stream,
+ * and its size muet be set with this parameter.
+ *
+ * @return Returns 0 on success, -1 if interface/port is invalid.
+ */
+int cvmx_pko3_interface_options(int xiface, int index,
+			bool fcs_enable, bool pad_enable,
+			unsigned fcs_sop_off)
+{
+	int mac_num;
+	cvmx_pko_macx_cfg_t pko_mac_cfg;
+	unsigned fifo_num;
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+
+	mac_num = __cvmx_pko3_get_mac_num(xiface, index);
+	if(mac_num < 0)
+		return -1;
+
+	pko_mac_cfg.u64 = cvmx_read_csr_node(xi.node, CVMX_PKO_MACX_CFG(mac_num));
+
+	/* If MAC is not assigned, return an error */
+	if (pko_mac_cfg.s.fifo_num == 0x1f)
+		return -1;
+
+	/* WORKAROUND: Pass1 won't allow change any bits unless FIFO_NUM=0x1f */
+	fifo_num = pko_mac_cfg.s.fifo_num;
+	pko_mac_cfg.s.fifo_num = 0x1f;
+
+	pko_mac_cfg.s.min_pad_ena = pad_enable;
+	pko_mac_cfg.s.fcs_ena = fcs_enable;
+	pko_mac_cfg.s.fcs_sop_off = fcs_sop_off;
+
+	cvmx_write_csr_node(xi.node, CVMX_PKO_MACX_CFG(mac_num), pko_mac_cfg.u64);
+
+	pko_mac_cfg.s.fifo_num = fifo_num;
+	cvmx_write_csr_node(xi.node, CVMX_PKO_MACX_CFG(mac_num), pko_mac_cfg.u64);
+
+	if (debug)
+		cvmx_dprintf("%s: PKO_MAC[%u]CFG=%#llx\n",__func__,
+			mac_num, (unsigned long long)
+			cvmx_read_csr_node(xi.node, CVMX_PKO_MACX_CFG(mac_num)));
+
+	return 0;
+}
+EXPORT_SYMBOL(cvmx_pko3_interface_options);
+
+/** Set Descriptor Queue options
+ *
+ * The `min_pad` parameter must be in agreement with the interface-level
+ * padding option for all descriptor queues assigned to that particular
+ * interface/port.
+ */
+void cvmx_pko3_dq_options(unsigned node, unsigned dq, bool min_pad)
+{
+	cvmx_pko_pdm_dqx_minpad_t reg;
+
+	dq &= (1<<10)-1;
+	reg.u64 = cvmx_read_csr_node(node, CVMX_PKO_PDM_DQX_MINPAD(dq));
+	reg.s.minpad = min_pad;
+	cvmx_write_csr_node(node, CVMX_PKO_PDM_DQX_MINPAD(dq), reg.u64);
+}
+
+/**
+ * Get number of PKO internal buffers available
+ *
+ * This function may be used to throttle output processing
+ * when the PKO runs out of internal buffers, to avoid discarding
+ * of packets or returning error results from transmission function.
+ *
+ * Returns negative numbers on error, positive number ti nidicate the
+ * number of buffers available, or 0 when no more buffers are available.
+ *
+ * @INTERNAL
+ */
+int cvmx_pko3_internal_buffer_count(unsigned node)
+{
+	cvmx_pko_dpfi_fpa_aura_t pko_aura;
+	unsigned laura, pool;
+	long long avail1, avail2;
+
+	/* get the aura number in pko, use aura node from parameter */
+	pko_aura.u64 = cvmx_read_csr_node(node, CVMX_PKO_DPFI_FPA_AURA);
+	laura = pko_aura.s.laura;
+
+	/* form here on, node is the AURA node */
+	node = pko_aura.s.node;
+
+	/* get the POOL number for this AURA */
+	pool = cvmx_read_csr_node(node, CVMX_FPA_AURAX_POOL(laura));
+
+	avail1 = cvmx_read_csr_node(node, CVMX_FPA_POOLX_AVAILABLE(pool));
+
+	avail2 = cvmx_read_csr_node(node, CVMX_FPA_AURAX_CNT_LIMIT(laura)) -
+		cvmx_read_csr_node(node, CVMX_FPA_AURAX_CNT(laura));
+
+	if (avail1 < avail2)
+		return avail1;
+
+	return avail2;
+}
+
+/******************************************************************************
+*
+* New PKO3 API - Experimental
+*
+******************************************************************************/
+
+/**
+ * Initialize packet descriptor
+ *
+ * Desciptor storage is provided by the caller,
+ * use this function to initialize the descriptor to a known
+ * empty state.
+ *
+ * @param pdesc Packet Desciptor.
+ *
+ * Do not use this function when creating a descriptor from a
+ * Work Queue Entry.
+ *
+ * The default setting of the 'free_bufs' attribute is 'false'.
+ */
+void cvmx_pko3_pdesc_init(cvmx_pko3_pdesc_t *pdesc)
+{
+	cvmx_pko_send_aura_t *ext_s;
+
+	memset(pdesc, 0, sizeof(*pdesc));
+
+	/* Start with HDR_S and HDR_EXT_S in first two words, all 0's */
+	pdesc->num_words = 2;
+
+	pdesc->hdr_s = (void *) &pdesc->word[0];
+	ext_s = (void *) &pdesc->word[1];
+	ext_s->s.subdc4 = CVMX_PKO_SENDSUBDC_EXT;
+
+	pdesc->last_aura = -1;
+	pdesc->jb_aura = -1;
+
+	/* Empty packets, can not decode header offsets (yet) */
+	pdesc->hdr_offsets = 1;
+}
+
+/**
+ * @INTERNAL
+ *
+ * Add arbitrary subcommand to a packet desciptor.
+ *
+ * This function will also allocate a jump buffer when
+ * the primary LTDMA buffer is exhausted.
+ * The jump buffer is allocated from the internal PKO3 aura
+ * on the node where this function is running.
+ */
+static int cvmx_pko3_pdesc_subdc_add(cvmx_pko3_pdesc_t *pdesc,
+		uint64_t subdc)
+{
+	cvmx_pko_send_hdr_t *hdr_s;
+	cvmx_pko_send_aura_t *ext_s;
+	cvmx_pko_buf_ptr_t *jump_s;
+	const unsigned jump_buf_size = 4*1024 / sizeof(uint64_t);
+	unsigned i;
+
+	/* Simple handling while fitting the command buffer */
+	if (cvmx_likely(pdesc->num_words <= 15 && pdesc->jump_buf == NULL)) {
+		pdesc->word[ pdesc->num_words ] = subdc;
+		pdesc->num_words ++;
+		return pdesc->num_words;
+	}
+
+        /* SEND_JUMP_S broken on Pass1 */
+        if(OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_0)) {
+                cvmx_dprintf("%s: ERROR: too many segments\n",__func__);
+                return -E2BIG;
+        }
+
+	hdr_s = (void *) &pdesc->word[0];
+	ext_s = (void *) &pdesc->word[1];
+
+	/* Allocate jump buffer */
+	if (cvmx_unlikely(pdesc->jump_buf == NULL)) {
+		unsigned pko_gaura;
+		unsigned fpa_node = cvmx_get_node_num();
+
+		/* Allocate jump buffer from PKO internal FPA AURA, size=4KiB */
+		pko_gaura = __cvmx_pko3_aura_get(fpa_node);
+
+		pdesc->jump_buf = cvmx_fpa3_alloc_gaura(pko_gaura);
+                if(pdesc->jump_buf == NULL)
+                        return -EINVAL;
+
+		/* Save the JB aura for later */
+		pdesc->jb_aura = pko_gaura;
+
+		/* Move most of the command to the jump buffer */
+		memcpy(pdesc->jump_buf, &pdesc->word[2],
+			(pdesc->num_words-2)*sizeof(uint64_t));
+		jump_s = (void *) &pdesc->word[2];
+		jump_s->u64 = 0;
+		jump_s->s.addr = cvmx_ptr_to_phys(pdesc->jump_buf);
+		jump_s->s.i = !hdr_s->s.df;	/* F= ~DF */
+		jump_s->s.size = pdesc->num_words - 2;
+		jump_s->s.subdc3 = CVMX_PKO_SENDSUBDC_JUMP;
+
+		/* Now the LMTDMA buffer has only HDR_S, EXT_S, JUMP_S */
+		pdesc->num_words = 3;
+	}
+
+	/* Add the new subcommand to the jump buffer */
+	jump_s = (void *) &pdesc->word[2];
+	i = jump_s->s.size;
+
+	/* Avoid overrunning jump buffer */
+	if (i >= (jump_buf_size-2)) {
+                cvmx_dprintf("%s: ERROR: too many segments\n",__func__);
+		return -E2BIG;
+	}
+
+	pdesc->jump_buf[i] = subdc;
+	jump_s->s.size++;
+
+	(void) ext_s;
+
+	return(i + pdesc->num_words);
+}
+
+/**
+ * Send a packet in a desciptor to an output port via an output queue.
+ *
+ * A call to this function must follow all other functions that
+ * create a packet descriptor from WQE, or after initializing an
+ * empty descriptor and filling it with one or more data fragments.
+ * After this function is called, the content of the packet descriptor
+ * can no longer be used, and are undefined.
+ *
+ * @param pdesc Packet Desciptor.
+ * @param dq Descriptor Queue associated with the desired output port
+ * @return Returns 0 on success, -1 on error.
+ *
+ */
+int cvmx_pko3_pdesc_transmit(cvmx_pko3_pdesc_t *pdesc, uint16_t dq)
+{
+        cvmx_pko_query_rtn_t pko_status;
+	cvmx_pko_send_aura_t aura_s;
+	uint8_t port_node;
+	int rc;
+
+	/* Add last AURA_S for jump_buf, if present */
+	if (cvmx_unlikely(pdesc->jump_buf != NULL) &&
+	    (pdesc->last_aura != pdesc->jb_aura)) {
+		/* The last AURA_S subdc refers to the jump_buf itself */
+		aura_s.s.aura = pdesc->jb_aura;
+		aura_s.s.offset = 0;
+		aura_s.s.alg = AURAALG_NOP;
+		aura_s.s.subdc4 = CVMX_PKO_SENDSUBDC_AURA;
+		pdesc->last_aura = pdesc->jb_aura;
+
+		rc = cvmx_pko3_pdesc_subdc_add(pdesc, aura_s.u64);
+		if (rc < 0)
+			return -1;
+	}
+
+	/* SEND_WORK_S must be the very last subdc */
+	if (cvmx_unlikely(pdesc->send_work_s != 0ULL)) {
+		rc = cvmx_pko3_pdesc_subdc_add(pdesc, pdesc->send_work_s);
+		if (rc < 0)
+			return -1;
+		pdesc->send_work_s = 0ULL;
+	}
+
+        /* Derive destination node from dq */
+        port_node = dq >> 14;
+        dq &= (1<<10)-1;
+
+        /* Send the PKO3 command into the Descriptor Queue */
+        pko_status = __cvmx_pko3_do_dma(port_node, dq,
+                pdesc->word, pdesc->num_words, CVMX_PKO_DQ_SEND);
+
+        /* Map PKO3 result codes to legacy return values */
+        if (pko_status.s.dqstatus == PKO_DQSTATUS_PASS)
+                return 0;
+
+#if 0
+        cvmx_dprintf("%s: ERROR: failed to enqueue: %s\n",
+                                __FUNCTION__,
+                                pko_dqstatus_error(pko_status.s.dqstatus));
+#endif
+
+	return -1;
+}
+
+/**
+ * Append a packet segment to a packet descriptor
+ *
+ * After a packet descriptor is initialized, one or more
+ * packet data segments can be added to the packet,
+ * in the order in which they should be transmitted.
+ *
+ * The size of the resulting packet will be equal to the
+ * sum of the segments appended by thus function.
+ * Every segment may be contained in a buffer that belongs
+ * to a different FPA 'aura', and may be automatically
+ * released back to that aura, if required.
+ *
+ * @param pdesc Packet Desciptor.
+ * @param p_data Address of the segment first byte (virtual).
+ * @param data_bytes Size of the data segment (in bytes).
+ * @param gaura A global FPA 'aura' where the packet buffer was allocated from.
+ *
+ * The 'gaura' parameter contaisn the node number where the buffer pool
+ * is located, and has only a meaning if the 'free_buf' argument is 'true'.
+ * The buffer being added will be automatically freed upon transmission
+ * along with all other buffers in this descriptor, or not, depending
+ * on the descriptor 'free_bufs' attribute that is set during
+ * descriptor creation, or changed subsequently with a call to
+ * 'cvmx_pko3_pdesc_set_free()'.
+ *
+ * @return Returns 0 on success, -1 on error.
+ */
+int cvmx_pko3_pdesc_buf_append(cvmx_pko3_pdesc_t *pdesc, void *p_data,
+		unsigned data_bytes, unsigned gaura)
+{
+	cvmx_pko_send_hdr_t *hdr_s;
+	cvmx_pko_buf_ptr_t gather_s;
+	cvmx_pko_send_aura_t aura_s;
+	int rc;
+
+	if (pdesc->mem_s_ix > 0) {
+		cvmx_dprintf("%s: subcommand restriction violated\n", __func__);
+		return -1;
+	}
+
+	hdr_s = (void *) &pdesc->word[0];
+
+	if(pdesc->last_aura == -1 && gaura != ((unsigned)-1)) {
+		unsigned buf_sz = 128;
+
+		/* First mbuf, calculate headroom */
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+                buf_sz = cvmx_fpa3_get_aura_buf_size(gaura);
+#endif
+		pdesc->headroom = (unsigned long)p_data & (buf_sz-1);
+		pdesc->last_aura = hdr_s->s.aura = gaura;
+	} else if(pdesc->last_aura != (short) gaura) {
+		aura_s.s.aura = gaura;
+		aura_s.s.offset = 0;
+		aura_s.s.alg = AURAALG_NOP;
+		aura_s.s.subdc4 = CVMX_PKO_SENDSUBDC_AURA;
+		pdesc->last_aura = gaura;
+
+		rc = cvmx_pko3_pdesc_subdc_add(pdesc, aura_s.u64);
+		if (rc < 0)
+			return -1;
+	}
+
+	gather_s.u64 = 0;
+	gather_s.s.addr = cvmx_ptr_to_phys(p_data);
+	gather_s.s.size = data_bytes;
+	hdr_s->s.total += data_bytes;
+	gather_s.s.i = 0;	/* follow HDR_S[DF] setting */
+	gather_s.s.subdc3 = CVMX_PKO_SENDSUBDC_GATHER;
+
+	rc = cvmx_pko3_pdesc_subdc_add(pdesc, gather_s.u64);
+	if (rc < 0)
+		return -1;
+
+	return rc;
+}
+
+/**
+ * Request atomic memory decrement at transmission completion
+ *
+ * Each packet descriptor may contain several decrement notification
+ * requests, but these request must only be made after all of the
+ * packet data segments have been added, and before packet transmission
+ * commences.
+ *
+ * Only decrement of a 64-bit memory location is supported.
+ *
+ * @param pdesc Packet Descriptor.
+ * @param p_counter A pointer to an atomic 64-bit memory location.
+ *
+ * @return Returns 0 on success, -1 on failure.
+ */
+int cvmx_pko3_pdesc_notify_decrement(cvmx_pko3_pdesc_t *pdesc,
+	volatile uint64_t *p_counter)
+{
+	int rc;
+	/* 640bit decrement is the only supported operation */
+	cvmx_pko_send_mem_t mem_s = {.s={
+		.subdc4 = CVMX_PKO_SENDSUBDC_MEM,
+		.dsz = MEMDSZ_B64, .alg = MEMALG_SUB,
+		.offset = 1,
+#ifdef	_NOT_IN_SIM_
+		/* Enforce MEM nefore SSO submission if both present */
+		.wmem = 1
+#endif
+		}};
+
+	mem_s.s.addr = cvmx_ptr_to_phys(CASTPTR(void,p_counter));
+
+
+
+	rc = cvmx_pko3_pdesc_subdc_add(pdesc, mem_s.u64);
+
+	/*
+	 * SEND_MEM_S must be after all LINK_S/FATHER_S/IMM_S
+	 * subcommands, set the index to prevent further data
+	 * subcommands.
+	 */
+	if (rc > 0)
+		pdesc->mem_s_ix = rc;
+
+	return rc;
+}
+
+/**
+ * Request atomic memory clear at transmission completion
+ *
+ * Each packet descriptor may contain several notification
+ * requests, but these request must only be made after all of the
+ * packet data segments have been added, and before packet transmission
+ * commences.
+ *
+ * Clearing of a single byte is requested by this function.
+ *
+ * @param pdesc Packet Descriptor.
+ * @param p_counter A pointer to a byte location.
+ *
+ * @return Returns 0 on success, -1 on failure.
+ */
+int cvmx_pko3_pdesc_notify_memclr(cvmx_pko3_pdesc_t *pdesc,
+	volatile uint8_t *p_mem)
+{
+	int rc;
+	/* 640bit decrement is the only supported operation */
+	cvmx_pko_send_mem_t mem_s = {.s={
+		.subdc4 = CVMX_PKO_SENDSUBDC_MEM,
+		.dsz = MEMDSZ_B8, .alg = MEMALG_SET,
+		.offset = 0,
+		}};
+
+	mem_s.s.addr = cvmx_ptr_to_phys(CASTPTR(void,p_mem));
+
+	rc = cvmx_pko3_pdesc_subdc_add(pdesc, mem_s.u64);
+
+	/*
+	 * SEND_MEM_S must be after all LINK_S/FATHER_S/IMM_S
+	 * subcommands, set the index to prevent further data
+	 * subcommands.
+	 */
+	if (rc > 0)
+		pdesc->mem_s_ix = rc;
+
+	return rc;
+}
+
+
+/**
+ * @INTERNAL
+ *
+ * Decode packet header and calculate protocol header offsets
+ *
+ * The protocol information and layer offset is derived
+ * from the results if decoding done by the PKI,
+ * and the appropriate PKO fields are filled.
+ *
+ * The function assumes the headers have not been modified
+ * since converted from WQE, and does not (yet) implement
+ * software-based decoding to handle modified or originated
+ * packets correctly.
+ *
+ * FIXME:
+ * Add simple accessors to read the decoded protocol fields.
+ */
+static int cvmx_pko3_pdesc_hdr_offsets(cvmx_pko3_pdesc_t *pdesc)
+{
+	cvmx_pko_send_hdr_t *hdr_s;
+
+	if (pdesc->hdr_offsets)
+		return 0;
+
+	if (!pdesc->pki_word4_present)
+		return -EINVAL;
+
+	pdesc->hdr_s = hdr_s = (void *) &pdesc->word[0];
+
+	/* Match IPv5/IPv6 protocols with/without options */
+	if ((pdesc->pki_word2.lc_hdr_type & 0x1c)
+		== CVMX_PKI_LTYPE_E_IP4) {
+		hdr_s->s.l3ptr = pdesc->pki_word4.ptr_layer_c;
+
+		/* Match TCP/UDP/SCTP group */
+		if ((pdesc->pki_word2.lf_hdr_type & 0x18) == CVMX_PKI_LTYPE_E_TCP)
+			hdr_s->s.l4ptr = pdesc->pki_word4.ptr_layer_f;
+
+		if (pdesc->pki_word2.lf_hdr_type == CVMX_PKI_LTYPE_E_UDP)
+			pdesc->ckl4_alg = CKL4ALG_UDP;
+		if (pdesc->pki_word2.lf_hdr_type == CVMX_PKI_LTYPE_E_TCP)
+			pdesc->ckl4_alg = CKL4ALG_TCP;
+		if (pdesc->pki_word2.lf_hdr_type == CVMX_PKI_LTYPE_E_SCTP)
+			pdesc->ckl4_alg = CKL4ALG_SCTP;
+	}
+	/* FIXME: consider ARP as L3 too ? what about IPfrag ? */
+
+	pdesc->hdr_offsets = 1;	/* make sure its done once */
+	return 0;
+}
+
+/*
+ * @INTERNAL
+ *
+ * memcpy() a reverse endian memory region.
+ * where both the source and destination are the reverse endianness
+ * with respect to native byte order.
+ */
+static void memcpy_swap(void *dst, const void *src, unsigned bytes)
+{
+	uint8_t *d = dst;
+	const uint8_t *s = src;
+	unsigned i;
+	const unsigned swizzle = 0x7;	/* 64-bit invariant endianness */
+
+	for(i = 0; i < bytes; i++)
+		d[i ^ swizzle] = s[i ^ swizzle];
+}
+
+/*
+ * @INTERNAL
+ *
+ * memcpy() with swizzling, from reverse endianness to native byte order.
+ */
+static void memcpy_from_swap(void *dst, const void *src, unsigned bytes)
+{
+	uint8_t *d = dst;
+	const uint8_t *s = src;
+	unsigned i;
+	const unsigned swizzle = 0x7;	/* 64-bit invariant endianness */
+
+	for(i = 0; i < bytes; i++)
+		d[i] = s[i ^ swizzle];
+}
+
+/*
+ * @INTERNAL
+ *
+ * memcpy() with swizzling, from native byte order to the reverse endianness.
+ */
+static void memcpy_to_swap(void *dst, const void *src, unsigned bytes)
+{
+	uint8_t *d = dst;
+	const uint8_t *s = src;
+	unsigned i;
+	const unsigned swizzle = 0x7;	/* 64-bit invariant endianness */
+
+	for(i = 0; i < bytes; i++)
+		d[i ^ swizzle] = s[i];
+}
+
+/**
+ * Prepend a data segment to the packet descriptor
+ *
+ * Useful for pushing additiona headers
+ *
+ * The initial implementation is confined by the size of the
+ * "headroom" in the first packet buffer attached to the descriptor.
+ * Future version may prepend additional buffers when this head room
+ * is insufficient, but currently will return -1 when headrom is
+ * insufficient.
+ *
+ * On success, the function returns the remaining headroom in the buffer.
+ *
+ */
+int cvmx_pko3_pdesc_hdr_push(cvmx_pko3_pdesc_t *pdesc,
+	const void *p_data, uint8_t data_bytes, uint8_t layer)
+{
+	cvmx_pko_send_hdr_t *hdr_s;
+	cvmx_pko_buf_ptr_t *gather_s;
+	short headroom;
+	void *p;	/* old data location */
+	void *q;	/* new data location */
+	bool endian_swap;
+
+	headroom = pdesc->headroom;
+
+	if ((short)data_bytes > headroom)
+		return -ENOSPC;
+
+	hdr_s = (void *)&pdesc->word[0];
+	endian_swap = (hdr_s->s.le != __native_le);
+
+	/* Get GATTHER_S/LINK_S subcommand location */
+	if (cvmx_likely(pdesc->jump_buf == NULL))
+		/* Without JB, first data buf is in 3rd comand word */
+		gather_s = (void *)&pdesc->word[2];
+	else
+		/* With JB, its first word is the first buffer */
+		gather_s = (void *)pdesc->jump_buf;
+
+	/* Verify the subcommand is of the expected type */
+	if (cvmx_unlikely(gather_s->s.subdc3 != CVMX_PKO_SENDSUBDC_LINK &&
+			gather_s->s.subdc3 != CVMX_PKO_SENDSUBDC_GATHER))
+		return -EINVAL;
+
+	/* adjust  address and size values */
+	p = cvmx_phys_to_ptr(gather_s->s.addr);
+	q			= p - data_bytes;
+	gather_s->s.addr	-= data_bytes;
+	gather_s->s.size	+= data_bytes;
+	hdr_s->s.total		+= data_bytes;
+	headroom		-= data_bytes;
+
+	/* Move link pointer if the descriptor is SEND_LINK_S */
+	if (gather_s->s.subdc3 == CVMX_PKO_SENDSUBDC_LINK) {
+		if (cvmx_likely(!endian_swap))
+			memcpy(q-8, p-8, 8);
+		else
+			memcpy_swap(q-8, p-8, 8);
+	}
+
+	if (cvmx_likely(!endian_swap))
+		memcpy(q, p_data, data_bytes);
+	else
+		memcpy_to_swap(q, p_data, data_bytes);
+
+	pdesc->headroom = headroom;
+
+	/* Adjust higher level protocol header offset */
+	cvmx_pko3_pdesc_hdr_offsets(pdesc);
+	if (layer <= 4 ) {
+		pdesc->hdr_s->s.l4ptr += data_bytes;
+	}
+
+	if (layer <= 3) {
+		pdesc->hdr_s->s.l3ptr += data_bytes;
+	}
+
+	if (layer >= 3) {
+		hdr_s->s.ckl3 = 1;
+		hdr_s->s.ckl4 = pdesc->ckl4_alg;
+		/* FIXME: decode L4 alg in case the header was generated */
+		/* FIXME: CKL4 not supported in simulator */
+	}
+
+	return headroom;
+}
+
+
+/**
+ * Remove some bytes from start of packet
+ *
+ * Useful for popping a header from a packet.
+ * It only eeds to find the first segment, and adjust its address,
+ * as well as segment and total sizes.
+ *
+ * Returns new packet size, or -1 if the trimmed size exceeds the
+ * size of the first data segment.
+ */
+int cvmx_pko3_pdesc_hdr_pop(cvmx_pko3_pdesc_t *pdesc,
+		void *hdr_buf, unsigned num_bytes)
+{
+	cvmx_pko_send_hdr_t *hdr_s;
+	cvmx_pko_buf_ptr_t *gather_s;
+	short headroom;
+	void *p;
+	void *q;
+	bool endian_swap;
+
+	headroom = pdesc->headroom;
+
+	hdr_s = (void *)&pdesc->word[0];
+	endian_swap = (hdr_s->s.le != __native_le);
+
+	if (hdr_s->s.total < num_bytes)
+		return -ENOSPC;
+
+	/* Get GATTHER_S/LINK_S subcommand location */
+	if (cvmx_likely(pdesc->jump_buf == NULL))
+		/* Without JB, first data buf is in 3rd comand word */
+		gather_s = (void *)&pdesc->word[2];
+	else
+		/* With JB, its first word is the first buffer */
+		gather_s = (void *)pdesc->jump_buf;
+
+	/* Verify the subcommand is of the expected type */
+	if (cvmx_unlikely(gather_s->s.subdc3 != CVMX_PKO_SENDSUBDC_LINK &&
+			gather_s->s.subdc3 != CVMX_PKO_SENDSUBDC_GATHER))
+		return -EINVAL;
+
+	/* Can't trim more than the content of the first buffer */
+	if (gather_s->s.size < num_bytes)
+		return -ENOMEM;
+
+	/* adjust  address and size values */
+	p = cvmx_phys_to_ptr(gather_s->s.addr);
+	q			= p + num_bytes;
+	gather_s->s.addr	+= num_bytes;
+	gather_s->s.size	-= num_bytes;
+	hdr_s->s.total		-= num_bytes;
+	headroom		+= num_bytes;
+
+	if (hdr_buf != NULL) {
+		/* Retreive popped header to user buffer */
+		if (cvmx_likely(!endian_swap)) {
+			memcpy(hdr_buf, p, num_bytes);
+		} else {
+			memcpy_from_swap(hdr_buf, p, num_bytes);
+		}
+	}
+
+	/* Move link pointer if the descriptor is SEND_LINK_S */
+	if (gather_s->s.subdc3 == CVMX_PKO_SENDSUBDC_LINK) {
+		if (cvmx_likely(!endian_swap))
+			memcpy(q-8, p-8, 8);
+		else
+			memcpy_swap(q-8, p-8, 8);
+	}
+
+	pdesc->headroom = headroom;
+
+	/* Adjust higher level protocol header offset */
+	cvmx_pko3_pdesc_hdr_offsets(pdesc);
+	if (num_bytes < pdesc->hdr_s->s.l3ptr) {
+		pdesc->hdr_s->s.l3ptr -= num_bytes;
+		pdesc->hdr_s->s.l4ptr -= num_bytes;
+	} else if (num_bytes < pdesc->hdr_s->s.l4ptr) {
+		pdesc->hdr_s->s.l3ptr = 0;
+		pdesc->hdr_s->s.l4ptr -= num_bytes;
+	} else {
+		pdesc->hdr_s->s.l3ptr = 0;
+		pdesc->hdr_s->s.l4ptr = 0;
+		hdr_s->s.ckl4 = CKL4ALG_NONE;
+	}
+
+	return hdr_s->s.total;
+}
+
+/**
+ * Peek into some header field of a packet
+ *
+ * Will return a number of bytes of packet header data at an arbitrary offset
+ * which must reside within the first packet data buffer.
+ *
+ */
+int cvmx_pko3_pdesc_hdr_peek(cvmx_pko3_pdesc_t *pdesc,
+		void *hdr_buf, unsigned num_bytes, unsigned offset)
+{
+	cvmx_pko_send_hdr_t *hdr_s;
+	cvmx_pko_buf_ptr_t *gather_s;
+	void *p;
+	bool endian_swap;
+
+	hdr_s = (void *)&pdesc->word[0];
+	endian_swap = (hdr_s->s.le != __native_le);
+
+	if (hdr_s->s.total < (num_bytes+offset))
+		return -ENOSPC;
+
+	/* Get GATTHER_S/LINK_S subcommand location */
+	if (cvmx_likely(pdesc->jump_buf == NULL))
+		/* Without JB, first data buf is in 3rd command word */
+		gather_s = (void *)&pdesc->word[2];
+	else
+		/* With JB, its first word is the first buffer */
+		gather_s = (void *)pdesc->jump_buf;
+
+	/* Verify the subcommand is of the expected type */
+	if (cvmx_unlikely(gather_s->s.subdc3 != CVMX_PKO_SENDSUBDC_LINK &&
+			gather_s->s.subdc3 != CVMX_PKO_SENDSUBDC_GATHER))
+		return -EINVAL;
+
+	/* Can't peek more than the content of the first buffer */
+	if (gather_s->s.size <= offset)
+		return -ENOMEM;
+	if ((gather_s->s.size-offset) < num_bytes)
+		num_bytes = gather_s->s.size-offset;
+
+	/* adjust address */
+	p = cvmx_phys_to_ptr(gather_s->s.addr) + offset;
+
+	if (hdr_buf == NULL)
+		return -EINVAL;
+
+	/* Copy requested bytes */
+	if (cvmx_likely(!endian_swap)) {
+		memcpy(hdr_buf, p, num_bytes);
+	} else {
+		memcpy_from_swap(hdr_buf, p, num_bytes);
+	}
+
+	return num_bytes;
+}
+
+/**
+ * Set the packet descriptor automatic-free attribute
+ *
+ * Override the 'free_bufs' attribute that was set during
+ * packet descriptor creation, or by an earlier call to
+ * this function.
+ * Setting the 'buf_free" attribute to 'true' will cause
+ * the PKO3 to free all buffers associated with this packet
+ * descriptor to be released upon transmission complete.
+ * Setting this attribute to 'false' allows e.g. using the
+ * same descriptor to transmit a packet out of several ports
+ * with a minimum overhead.
+ */
+void cvmx_pko3_pdesc_set_free(cvmx_pko3_pdesc_t *pdesc, bool free_bufs)
+{
+	cvmx_pko_send_hdr_t *hdr_s;
+	cvmx_pko_buf_ptr_t *jump_s;
+
+	hdr_s = (void *)&pdesc->word[0];
+	hdr_s->s.df = !free_bufs;
+
+	if (cvmx_likely(pdesc->jump_buf == NULL))
+		return;
+	jump_s = (void *) &pdesc->word[2];
+	jump_s->s.i = free_bufs; /* F=free */
+}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-qlm.c b/arch/mips/cavium-octeon/executive/cvmx-qlm.c
new file mode 100644
index 0000000..bafbafc
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-qlm.c
@@ -0,0 +1,1421 @@
+/***********************license start***************
+ * Copyright (c) 2011-2014  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Helper utilities for qlm.
+ *
+ * <hr>$Revision: 99024 $<hr>
+ */
+#include <asm/octeon/cvmx.h>
+#include <asm/octeon/cvmx-bootmem.h>
+#include <asm/octeon/cvmx-helper-jtag.h>
+#include <asm/octeon/cvmx-helper-util.h>
+#include <asm/octeon/cvmx-qlm.h>
+#include <asm/octeon/cvmx-clock.h>
+#include <asm/octeon/cvmx-bgxx-defs.h>
+#include <asm/octeon/cvmx-gmxx-defs.h>
+#include <asm/octeon/cvmx-gserx-defs.h>
+#include <asm/octeon/cvmx-sriox-defs.h>
+#include <asm/octeon/cvmx-sriomaintx-defs.h>
+#include <asm/octeon/cvmx-pciercx-defs.h>
+#include <asm/octeon/cvmx-pemx-defs.h>
+
+#ifdef CVMX_BUILD_FOR_UBOOT
+DECLARE_GLOBAL_DATA_PTR;
+#endif
+
+/* Their is a copy of this in bootloader qlm configuration, make sure
+   to update both the places till i figure out */
+#define R_25G_REFCLK100             0x0
+#define R_5G_REFCLK100              0x1
+#define R_8G_REFCLK100              0x2
+#define R_125G_REFCLK15625_KX       0x3
+#define R_3125G_REFCLK15625_XAUI    0x4
+#define R_103125G_REFCLK15625_KR    0x5
+#define R_125G_REFCLK15625_SGMII    0x6
+#define R_5G_REFCLK15625_QSGMII     0x7
+#define R_625G_REFCLK15625_RXAUI    0x8
+#define R_25G_REFCLK125             0x9
+#define R_5G_REFCLK125              0xa
+#define R_8G_REFCLK125              0xb
+
+static const int REF_100MHZ = 100000000;
+static const int REF_125MHZ = 125000000;
+static const int REF_156MHZ = 156250000;
+
+/**
+ * The JTAG chain for CN52XX and CN56XX is 4 * 268 bits long, or 1072.
+ * CN5XXX full chain shift is:
+ *     new data => lane 3 => lane 2 => lane 1 => lane 0 => data out
+ * The JTAG chain for CN63XX is 4 * 300 bits long, or 1200.
+ * The JTAG chain for CN68XX is 4 * 304 bits long, or 1216.
+ * The JTAG chain for CN66XX/CN61XX/CNF71XX is 4 * 304 bits long, or 1216.
+ * CN6XXX full chain shift is:
+ *     new data => lane 0 => lane 1 => lane 2 => lane 3 => data out
+ * Shift LSB first, get LSB out
+ */
+#define CVMX_QLM_JTAG_UINT32 40
+#ifdef CVMX_BUILD_FOR_LINUX_HOST
+extern void octeon_remote_read_mem(void *buffer, uint64_t physical_address, int length);
+extern void octeon_remote_write_mem(uint64_t physical_address, const void *buffer, int length);
+uint32_t __cvmx_qlm_jtag_xor_ref[5][CVMX_QLM_JTAG_UINT32];
+#else
+typedef uint32_t qlm_jtag_uint32_t[CVMX_QLM_JTAG_UINT32];
+CVMX_SHARED qlm_jtag_uint32_t *__cvmx_qlm_jtag_xor_ref;
+#endif
+
+/**
+ * Return the number of QLMs supported by the chip
+ *
+ * @return  Number of QLMs
+ */
+int cvmx_qlm_get_num(void)
+{
+	if (OCTEON_IS_MODEL(OCTEON_CN68XX))
+		return 5;
+	else if (OCTEON_IS_MODEL(OCTEON_CN66XX))
+		return 3;
+	else if (OCTEON_IS_MODEL(OCTEON_CN63XX))
+		return 3;
+	else if (OCTEON_IS_MODEL(OCTEON_CN61XX))
+		return 3;
+#ifndef _MIPS_ARCH_OCTEON2
+	else if (OCTEON_IS_MODEL(OCTEON_CN56XX))
+		return 4;
+	else if (OCTEON_IS_MODEL(OCTEON_CN52XX))
+		return 2;
+#endif
+	else if (OCTEON_IS_MODEL(OCTEON_CNF71XX))
+		return 2;
+	else if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+		return 8;
+	//cvmx_dprintf("Warning: cvmx_qlm_get_num: This chip does not have QLMs\n");
+	return 0;
+}
+
+/**
+ * Return the qlm number based on the interface
+ *
+ * @param interface  Interface to look up
+ */
+int cvmx_qlm_interface(int xiface)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	if (OCTEON_IS_MODEL(OCTEON_CN61XX)) {
+		return (xi.interface == 0) ? 2 : 0;
+	} else if (OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX)) {
+		return 2 - xi.interface;
+	} else if (OCTEON_IS_MODEL(OCTEON_CNF71XX)) {
+		if (xi.interface == 0)
+			return 0;
+		else
+			cvmx_dprintf("Warning: cvmx_qlm_interface: Invalid interface %d\n", xi.interface);
+	} else if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+		cvmx_bgxx_cmr_global_config_t gconfig;
+		cvmx_gserx_phy_ctl_t phy_ctl;
+		cvmx_gserx_cfg_t gserx_cfg;
+		int qlm;
+
+		if (xi.interface < 6) {
+			if (xi.interface < 2) {
+				gconfig.u64 = cvmx_read_csr_node(xi.node, CVMX_BGXX_CMR_GLOBAL_CONFIG(xi.interface));
+				if (gconfig.s.pmux_sds_sel)
+					qlm = xi.interface + 2; /* QLM 2 or 3 */
+				else
+					qlm = xi.interface; /* QLM 0 or 1 */
+			} else
+				qlm = xi.interface + 2; /* QLM 4-7 */
+
+			/* make sure the QLM is powered up and out of reset */
+			phy_ctl.u64 = cvmx_read_csr_node(xi.node, CVMX_GSERX_PHY_CTL(qlm));
+			if (phy_ctl.s.phy_pd || phy_ctl.s.phy_reset)
+				return -1;
+			gserx_cfg.u64 = cvmx_read_csr_node(xi.node, CVMX_GSERX_CFG(qlm));
+			if (gserx_cfg.s.bgx)
+				return qlm;
+			else
+				return -1;
+		} else if (xi.interface >= 7) { /* ILK */
+			int qlm;
+			for (qlm = 4; qlm < 8; qlm++) {
+				/* Make sure the QLM is powered and out of reset */
+				phy_ctl.u64 = cvmx_read_csr_node(xi.node, CVMX_GSERX_PHY_CTL(qlm));
+				if (phy_ctl.s.phy_pd || phy_ctl.s.phy_reset)
+					continue;
+				/* Make sure the QLM is in ILK mode */
+				gserx_cfg.u64 = cvmx_read_csr_node(xi.node, CVMX_GSERX_CFG(qlm));
+				if (gserx_cfg.s.ila)
+					return qlm;
+			}
+		}
+		return -1;
+	} else {
+		/* Must be cn68XX */
+		switch (xi.interface) {
+		case 1:
+			return 0;
+		default:
+			return xi.interface;
+		}
+	}
+	return -1;
+}
+
+/**
+ * Return number of lanes for a given qlm
+ *
+ * @return  Number of lanes
+ */
+int cvmx_qlm_get_lanes(int qlm)
+{
+	if (OCTEON_IS_MODEL(OCTEON_CN61XX) && qlm == 1)
+		return 2;
+	else if (OCTEON_IS_MODEL(OCTEON_CNF71XX))
+		return 2;
+
+	return 4;
+}
+
+/**
+ * Get the QLM JTAG fields based on Octeon model on the supported chips.
+ *
+ * @return  qlm_jtag_field_t structure
+ */
+const __cvmx_qlm_jtag_field_t *cvmx_qlm_jtag_get_field(void)
+{
+	return NULL;
+}
+
+/**
+ * Get the QLM JTAG length by going through qlm_jtag_field for each
+ * Octeon model that is supported
+ *
+ * @return return the length.
+ */
+int cvmx_qlm_jtag_get_length(void)
+{
+	const __cvmx_qlm_jtag_field_t *qlm_ptr = cvmx_qlm_jtag_get_field();
+	int length = 0;
+
+	/* Figure out how many bits are in the JTAG chain */
+	while (qlm_ptr != NULL && qlm_ptr->name) {
+		if (qlm_ptr->stop_bit > length)
+			length = qlm_ptr->stop_bit + 1;
+		qlm_ptr++;
+	}
+	return length;
+}
+
+/**
+ * Initialize the QLM layer
+ */
+void cvmx_qlm_init(void)
+{
+	int qlm;
+	int qlm_jtag_length;
+	char *qlm_jtag_name = "cvmx_qlm_jtag";
+	int qlm_jtag_size = CVMX_QLM_JTAG_UINT32 * 8 * 4;
+	static uint64_t qlm_base = 0;
+	const cvmx_bootmem_named_block_desc_t *desc;
+
+	if (OCTEON_IS_OCTEON3())
+		return;
+
+#ifndef CVMX_BUILD_FOR_LINUX_HOST
+	/* Skip actual JTAG accesses on simulator */
+	if (cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_SIM)
+		return;
+#endif
+
+	qlm_jtag_length = cvmx_qlm_jtag_get_length();
+
+	if (4 * qlm_jtag_length > (int)sizeof(__cvmx_qlm_jtag_xor_ref[0]) * 8) {
+		cvmx_dprintf("ERROR: cvmx_qlm_init: JTAG chain larger than XOR ref size\n");
+		return;
+	}
+
+	/* No need to initialize the initial JTAG state if cvmx_qlm_jtag
+	   named block is already created. */
+	if ((desc = cvmx_bootmem_find_named_block(qlm_jtag_name)) != NULL) {
+#ifdef CVMX_BUILD_FOR_LINUX_HOST
+		char buffer[qlm_jtag_size];
+
+		octeon_remote_read_mem(buffer, desc->base_addr, qlm_jtag_size);
+		memcpy(__cvmx_qlm_jtag_xor_ref, buffer, qlm_jtag_size);
+#else
+		__cvmx_qlm_jtag_xor_ref = cvmx_phys_to_ptr(desc->base_addr);
+#endif
+		/* Initialize the internal JTAG */
+		cvmx_helper_qlm_jtag_init();
+		return;
+	}
+
+	/* Create named block to store the initial JTAG state. */
+	qlm_base = cvmx_bootmem_phy_named_block_alloc(qlm_jtag_size, 0, 0, 128, qlm_jtag_name, CVMX_BOOTMEM_FLAG_END_ALLOC);
+
+	if (qlm_base == -1ull) {
+		cvmx_dprintf("ERROR: cvmx_qlm_init: Error in creating %s named block\n", qlm_jtag_name);
+		return;
+	}
+#ifndef CVMX_BUILD_FOR_LINUX_HOST
+	__cvmx_qlm_jtag_xor_ref = cvmx_phys_to_ptr(qlm_base);
+#endif
+	memset(__cvmx_qlm_jtag_xor_ref, 0, qlm_jtag_size);
+
+	/* Initialize the internal JTAG */
+	cvmx_helper_qlm_jtag_init();
+
+	/* Read the XOR defaults for the JTAG chain */
+	for (qlm = 0; qlm < cvmx_qlm_get_num(); qlm++) {
+		int i;
+		int num_lanes = cvmx_qlm_get_lanes(qlm);
+		/* Shift all zeros in the chain to make sure all fields are at
+		   reset defaults */
+		cvmx_helper_qlm_jtag_shift_zeros(qlm, qlm_jtag_length * num_lanes);
+		cvmx_helper_qlm_jtag_update(qlm);
+
+		/* Capture the reset defaults */
+		//cvmx_helper_qlm_jtag_capture(qlm);
+		/* Save the reset defaults. This will shift out too much data, but
+		   the extra zeros don't hurt anything */
+		for (i = 0; i < CVMX_QLM_JTAG_UINT32; i++)
+			__cvmx_qlm_jtag_xor_ref[qlm][i] = cvmx_helper_qlm_jtag_shift(qlm, 32, 0);
+	}
+
+#ifdef CVMX_BUILD_FOR_LINUX_HOST
+	/* Update the initial state for oct-remote utils. */
+	{
+		char buffer[qlm_jtag_size];
+
+		memcpy(buffer, &__cvmx_qlm_jtag_xor_ref, qlm_jtag_size);
+		octeon_remote_write_mem(qlm_base, buffer, qlm_jtag_size);
+	}
+#endif
+
+	/* Apply all QLM errata workarounds. */
+	__cvmx_qlm_speed_tweak();
+	__cvmx_qlm_pcie_idle_dac_tweak();
+}
+
+/**
+ * Lookup the bit information for a JTAG field name
+ *
+ * @param name   Name to lookup
+ *
+ * @return Field info, or NULL on failure
+ */
+static const __cvmx_qlm_jtag_field_t *__cvmx_qlm_lookup_field(const char *name)
+{
+	const __cvmx_qlm_jtag_field_t *ptr = cvmx_qlm_jtag_get_field();
+	while (ptr->name) {
+		if (strcmp(name, ptr->name) == 0)
+			return ptr;
+		ptr++;
+	}
+	cvmx_dprintf("__cvmx_qlm_lookup_field: Illegal field name %s\n", name);
+	return NULL;
+}
+
+/**
+ * Get a field in a QLM JTAG chain
+ *
+ * @param qlm    QLM to get
+ * @param lane   Lane in QLM to get
+ * @param name   String name of field
+ *
+ * @return JTAG field value
+ */
+uint64_t cvmx_qlm_jtag_get(int qlm, int lane, const char *name)
+{
+	const __cvmx_qlm_jtag_field_t *field = __cvmx_qlm_lookup_field(name);
+	int qlm_jtag_length = cvmx_qlm_jtag_get_length();
+	int num_lanes = cvmx_qlm_get_lanes(qlm);
+
+	if (!field)
+		return 0;
+
+	/* Capture the current settings */
+	//cvmx_helper_qlm_jtag_capture(qlm);
+	/* Shift past lanes we don't care about. CN6XXX/7XXX shifts lane 0 first, CN3XXX/5XXX shifts lane 3 first */
+	if (OCTEON_IS_MODEL(OCTEON_CN5XXX))
+		cvmx_helper_qlm_jtag_shift_zeros(qlm, qlm_jtag_length * (lane));	/* Shift to the start of the field */
+	else
+		cvmx_helper_qlm_jtag_shift_zeros(qlm, qlm_jtag_length * (num_lanes - 1 - lane));	/* Shift to the start of the field */
+	cvmx_helper_qlm_jtag_shift_zeros(qlm, field->start_bit);
+	/* Shift out the value and return it */
+	return cvmx_helper_qlm_jtag_shift(qlm, field->stop_bit - field->start_bit + 1, 0);
+}
+
+/**
+ * Set a field in a QLM JTAG chain
+ *
+ * @param qlm    QLM to set
+ * @param lane   Lane in QLM to set, or -1 for all lanes
+ * @param name   String name of field
+ * @param value  Value of the field
+ */
+void cvmx_qlm_jtag_set(int qlm, int lane, const char *name, uint64_t value)
+{
+	int i, l;
+	uint32_t shift_values[CVMX_QLM_JTAG_UINT32];
+	int num_lanes = cvmx_qlm_get_lanes(qlm);
+	const __cvmx_qlm_jtag_field_t *field = __cvmx_qlm_lookup_field(name);
+	int qlm_jtag_length = cvmx_qlm_jtag_get_length();
+	int total_length = qlm_jtag_length * num_lanes;
+	int bits = 0;
+
+	if (!field)
+		return;
+
+	/* Get the current state */
+	//cvmx_helper_qlm_jtag_capture(qlm);
+	for (i = 0; i < CVMX_QLM_JTAG_UINT32; i++)
+		shift_values[i] = cvmx_helper_qlm_jtag_shift(qlm, 32, 0);
+
+	/* Put new data in our local array */
+	for (l = 0; l < num_lanes; l++) {
+		uint64_t new_value = value;
+		int bits;
+		int adj_lanes;
+
+		if ((l != lane) && (lane != -1))
+			continue;
+
+		if (OCTEON_IS_MODEL(OCTEON_CN5XXX))
+			adj_lanes = l * qlm_jtag_length;
+		else
+			adj_lanes = (num_lanes - 1 - l) * qlm_jtag_length;
+
+		for (bits = field->start_bit + adj_lanes; bits <= field->stop_bit + adj_lanes; bits++) {
+			if (new_value & 1)
+				shift_values[bits / 32] |= 1 << (bits & 31);
+			else
+				shift_values[bits / 32] &= ~(1 << (bits & 31));
+			new_value >>= 1;
+		}
+	}
+
+	/* Shift out data and xor with reference */
+	while (bits < total_length) {
+		uint32_t shift = shift_values[bits / 32] ^ __cvmx_qlm_jtag_xor_ref[qlm][bits / 32];
+		int width = total_length - bits;
+		if (width > 32)
+			width = 32;
+		cvmx_helper_qlm_jtag_shift(qlm, width, shift);
+		bits += 32;
+	}
+
+	/* Update the new data */
+	cvmx_helper_qlm_jtag_update(qlm);
+	/* Always give the QLM 1ms to settle after every update. This may not
+	   always be needed, but some of the options make significant
+	   electrical changes */
+	//cvmx_wait_usec(1000);
+}
+
+/**
+ * Errata G-16094: QLM Gen2 Equalizer Default Setting Change.
+ * CN68XX pass 1.x and CN66XX pass 1.x QLM tweak. This function tweaks the
+ * JTAG setting for a QLMs to run better at 5 and 6.25Ghz.
+ */
+void __cvmx_qlm_speed_tweak(void)
+{
+	cvmx_mio_qlmx_cfg_t qlm_cfg;
+	int num_qlms = cvmx_qlm_get_num();
+	int qlm;
+
+	/* Workaround for Errata (G-16467) */
+	if (OCTEON_IS_MODEL(OCTEON_CN68XX_PASS2_X)) {
+		for (qlm = 0; qlm < num_qlms; qlm++) {
+			int ir50dac;
+			/* This workaround only applies to QLMs running at 6.25Ghz */
+			if (cvmx_qlm_get_gbaud_mhz(qlm) == 6250) {
+#ifdef CVMX_QLM_DUMP_STATE
+				cvmx_dprintf("%s:%d: QLM%d: Applying workaround for Errata G-16467\n", __func__, __LINE__, qlm);
+				cvmx_qlm_display_registers(qlm);
+				cvmx_dprintf("\n");
+#endif
+				cvmx_qlm_jtag_set(qlm, -1, "cfg_cdr_trunc", 0);
+				/* Hold the QLM in reset */
+				cvmx_qlm_jtag_set(qlm, -1, "cfg_rst_n_set", 0);
+				cvmx_qlm_jtag_set(qlm, -1, "cfg_rst_n_clr", 1);
+				/* Forcfe TX to be idle */
+				cvmx_qlm_jtag_set(qlm, -1, "cfg_tx_idle_clr", 0);
+				cvmx_qlm_jtag_set(qlm, -1, "cfg_tx_idle_set", 1);
+				if (OCTEON_IS_MODEL(OCTEON_CN68XX_PASS2_0)) {
+					ir50dac = cvmx_qlm_jtag_get(qlm, 0, "ir50dac");
+					while (++ir50dac <= 31)
+						cvmx_qlm_jtag_set(qlm, -1, "ir50dac", ir50dac);
+				}
+				cvmx_qlm_jtag_set(qlm, -1, "div4_byp", 0);
+				cvmx_qlm_jtag_set(qlm, -1, "clkf_byp", 16);
+				cvmx_qlm_jtag_set(qlm, -1, "serdes_pll_byp", 1);
+				cvmx_qlm_jtag_set(qlm, -1, "spdsel_byp", 1);
+#ifdef CVMX_QLM_DUMP_STATE
+				cvmx_dprintf("%s:%d: QLM%d: Done applying workaround for Errata G-16467\n", __func__, __LINE__, qlm);
+				cvmx_qlm_display_registers(qlm);
+				cvmx_dprintf("\n\n");
+#endif
+				/* The QLM will be taken out of reset later when ILK/XAUI are initialized. */
+			}
+		}
+
+#ifndef CVMX_BUILD_FOR_LINUX_HOST
+		/* These QLM tuning parameters are specific to EBB6800
+		   eval boards using Cavium QLM cables. These should be
+		   removed or tunned based on customer boards. */
+		if (cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_EBB6800) {
+			for (qlm = 0; qlm < num_qlms; qlm++) {
+#ifdef CVMX_QLM_DUMP_STATE
+				cvmx_dprintf("Setting tunning parameters for QLM%d\n", qlm);
+#endif
+				cvmx_qlm_jtag_set(qlm, -1, "biasdrv_hs_ls_byp", 12);
+				cvmx_qlm_jtag_set(qlm, -1, "biasdrv_hf_byp", 12);
+				cvmx_qlm_jtag_set(qlm, -1, "biasdrv_lf_ls_byp", 12);
+				cvmx_qlm_jtag_set(qlm, -1, "biasdrv_lf_byp", 12);
+				cvmx_qlm_jtag_set(qlm, -1, "tcoeff_hf_byp", 15);
+				cvmx_qlm_jtag_set(qlm, -1, "tcoeff_hf_ls_byp", 15);
+				cvmx_qlm_jtag_set(qlm, -1, "tcoeff_lf_ls_byp", 15);
+				cvmx_qlm_jtag_set(qlm, -1, "tcoeff_lf_byp", 15);
+				cvmx_qlm_jtag_set(qlm, -1, "rx_cap_gen2", 0);
+				cvmx_qlm_jtag_set(qlm, -1, "rx_eq_gen2", 11);
+				cvmx_qlm_jtag_set(qlm, -1, "serdes_tx_byp", 1);
+			}
+		}
+		else if (cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_NIC68_4) {
+			for (qlm = 0; qlm < num_qlms; qlm++) {
+#ifdef CVMX_QLM_DUMP_STATE
+				cvmx_dprintf("Setting tunning parameters for QLM%d\n", qlm);
+#endif
+				cvmx_qlm_jtag_set(qlm, -1, "biasdrv_hs_ls_byp", 30);
+				cvmx_qlm_jtag_set(qlm, -1, "biasdrv_hf_byp", 30);
+				cvmx_qlm_jtag_set(qlm, -1, "biasdrv_lf_ls_byp", 30);
+				cvmx_qlm_jtag_set(qlm, -1, "biasdrv_lf_byp", 30);
+				cvmx_qlm_jtag_set(qlm, -1, "tcoeff_hf_byp", 0);
+				cvmx_qlm_jtag_set(qlm, -1, "tcoeff_hf_ls_byp", 0);
+				cvmx_qlm_jtag_set(qlm, -1, "tcoeff_lf_ls_byp", 0);
+				cvmx_qlm_jtag_set(qlm, -1, "tcoeff_lf_byp", 0);
+				cvmx_qlm_jtag_set(qlm, -1, "rx_cap_gen2", 1);
+				cvmx_qlm_jtag_set(qlm, -1, "rx_eq_gen2", 8);
+				cvmx_qlm_jtag_set(qlm, -1, "serdes_tx_byp", 1);
+			}
+		}
+#endif
+	}
+
+	/* G-16094 QLM Gen2 Equalizer Default Setting Change */
+	else if (OCTEON_IS_MODEL(OCTEON_CN68XX_PASS1_X)
+		 || OCTEON_IS_MODEL(OCTEON_CN66XX_PASS1_X)) {
+		/* Loop through the QLMs */
+		for (qlm = 0; qlm < num_qlms; qlm++) {
+			/* Read the QLM speed */
+			qlm_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(qlm));
+
+			/* If the QLM is at 6.25Ghz or 5Ghz then program JTAG */
+			if ((qlm_cfg.s.qlm_spd == 5) || (qlm_cfg.s.qlm_spd == 12) || (qlm_cfg.s.qlm_spd == 0) || (qlm_cfg.s.qlm_spd == 6) || (qlm_cfg.s.qlm_spd == 11)) {
+				cvmx_qlm_jtag_set(qlm, -1, "rx_cap_gen2", 0x1);
+				cvmx_qlm_jtag_set(qlm, -1, "rx_eq_gen2", 0x8);
+			}
+		}
+	}
+}
+
+/**
+ * Errata G-16174: QLM Gen2 PCIe IDLE DAC change.
+ * CN68XX pass 1.x, CN66XX pass 1.x and CN63XX pass 1.0-2.2 QLM tweak.
+ * This function tweaks the JTAG setting for a QLMs for PCIe to run better.
+ */
+void __cvmx_qlm_pcie_idle_dac_tweak(void)
+{
+	int num_qlms = 0;
+	int qlm;
+
+	if (OCTEON_IS_MODEL(OCTEON_CN68XX_PASS1_X))
+		num_qlms = 5;
+	else if (OCTEON_IS_MODEL(OCTEON_CN66XX_PASS1_X))
+		num_qlms = 3;
+	else if (OCTEON_IS_MODEL(OCTEON_CN63XX_PASS1_X) || OCTEON_IS_MODEL(OCTEON_CN63XX_PASS2_X))
+		num_qlms = 3;
+	else
+		return;
+
+	/* Loop through the QLMs */
+	for (qlm = 0; qlm < num_qlms; qlm++)
+		cvmx_qlm_jtag_set(qlm, -1, "idle_dac", 0x2);
+}
+
+void __cvmx_qlm_pcie_cfg_rxd_set_tweak(int qlm, int lane)
+{
+	if (OCTEON_IS_MODEL(OCTEON_CN6XXX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)) {
+		cvmx_qlm_jtag_set(qlm, lane, "cfg_rxd_set", 0x1);
+	}
+}
+
+/**
+ * Get the speed (Gbaud) of the QLM in Mhz.
+ *
+ * @param qlm    QLM to examine
+ *
+ * @return Speed in Mhz
+ */
+int cvmx_qlm_get_gbaud_mhz(int qlm)
+{
+	if (OCTEON_IS_MODEL(OCTEON_CN63XX)) {
+		if (qlm == 2) {
+			cvmx_gmxx_inf_mode_t inf_mode;
+			inf_mode.u64 = cvmx_read_csr(CVMX_GMXX_INF_MODE(0));
+			switch (inf_mode.s.speed) {
+			case 0:
+				return 5000;	/* 5     Gbaud */
+			case 1:
+				return 2500;	/* 2.5   Gbaud */
+			case 2:
+				return 2500;	/* 2.5   Gbaud */
+			case 3:
+				return 1250;	/* 1.25  Gbaud */
+			case 4:
+				return 1250;	/* 1.25  Gbaud */
+			case 5:
+				return 6250;	/* 6.25  Gbaud */
+			case 6:
+				return 5000;	/* 5     Gbaud */
+			case 7:
+				return 2500;	/* 2.5   Gbaud */
+			case 8:
+				return 3125;	/* 3.125 Gbaud */
+			case 9:
+				return 2500;	/* 2.5   Gbaud */
+			case 10:
+				return 1250;	/* 1.25  Gbaud */
+			case 11:
+				return 5000;	/* 5     Gbaud */
+			case 12:
+				return 6250;	/* 6.25  Gbaud */
+			case 13:
+				return 3750;	/* 3.75  Gbaud */
+			case 14:
+				return 3125;	/* 3.125 Gbaud */
+			default:
+				return 0;	/* Disabled */
+			}
+		} else {
+			cvmx_sriox_status_reg_t status_reg;
+			status_reg.u64 = cvmx_read_csr(CVMX_SRIOX_STATUS_REG(qlm));
+			if (status_reg.s.srio) {
+				cvmx_sriomaintx_port_0_ctl2_t sriomaintx_port_0_ctl2;
+				sriomaintx_port_0_ctl2.u32 = cvmx_read_csr(CVMX_SRIOMAINTX_PORT_0_CTL2(qlm));
+				switch (sriomaintx_port_0_ctl2.s.sel_baud) {
+				case 1:
+					return 1250;	/* 1.25  Gbaud */
+				case 2:
+					return 2500;	/* 2.5   Gbaud */
+				case 3:
+					return 3125;	/* 3.125 Gbaud */
+				case 4:
+					return 5000;	/* 5     Gbaud */
+				case 5:
+					return 6250;	/* 6.250 Gbaud */
+				default:
+					return 0;	/* Disabled */
+				}
+			} else {
+				cvmx_pciercx_cfg032_t pciercx_cfg032;
+				pciercx_cfg032.u32 = cvmx_read_csr(CVMX_PCIERCX_CFG032(qlm));
+				switch (pciercx_cfg032.s.ls) {
+				case 1:
+					return 2500;
+				case 2:
+					return 5000;
+				case 4:
+					return 8000;
+				default:
+					{
+						cvmx_mio_rst_boot_t mio_rst_boot;
+						mio_rst_boot.u64 = cvmx_read_csr(CVMX_MIO_RST_BOOT);
+						if ((qlm == 0) && mio_rst_boot.s.qlm0_spd == 0xf)
+							return 0;
+						if ((qlm == 1) && mio_rst_boot.s.qlm1_spd == 0xf)
+							return 0;
+						return 5000;	/* Best guess I can make */
+					}
+				}
+			}
+		}
+	} else if (OCTEON_IS_OCTEON2()) {
+		cvmx_mio_qlmx_cfg_t qlm_cfg;
+
+		qlm_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(qlm));
+		switch (qlm_cfg.s.qlm_spd) {
+		case 0:
+			return 5000;	/* 5     Gbaud */
+		case 1:
+			return 2500;	/* 2.5   Gbaud */
+		case 2:
+			return 2500;	/* 2.5   Gbaud */
+		case 3:
+			return 1250;	/* 1.25  Gbaud */
+		case 4:
+			return 1250;	/* 1.25  Gbaud */
+		case 5:
+			return 6250;	/* 6.25  Gbaud */
+		case 6:
+			return 5000;	/* 5     Gbaud */
+		case 7:
+			return 2500;	/* 2.5   Gbaud */
+		case 8:
+			return 3125;	/* 3.125 Gbaud */
+		case 9:
+			return 2500;	/* 2.5   Gbaud */
+		case 10:
+			return 1250;	/* 1.25  Gbaud */
+		case 11:
+			return 5000;	/* 5     Gbaud */
+		case 12:
+			return 6250;	/* 6.25  Gbaud */
+		case 13:
+			return 3750;	/* 3.75  Gbaud */
+		case 14:
+			return 3125;	/* 3.125 Gbaud */
+		default:
+			return 0;	/* Disabled */
+		}
+	} else if (OCTEON_IS_MODEL(OCTEON_CN70XX)) {
+		cvmx_gserx_dlmx_mpll_multiplier_t mpll_multiplier;
+		uint64_t meas_refclock;
+		uint64_t freq;
+
+		/* Measure the reference clock */
+		meas_refclock = cvmx_qlm_measure_clock(qlm);
+		/* Multiply to get the final frequency */
+		mpll_multiplier.u64 = cvmx_read_csr(CVMX_GSERX_DLMX_MPLL_MULTIPLIER(qlm, 0));
+		freq = meas_refclock * mpll_multiplier.s.mpll_multiplier;
+		freq = (freq + 500000) / 1000000;
+		return freq;
+	} else if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+		cvmx_gserx_lane_mode_t lane_mode;
+		cvmx_gserx_cfg_t cfg;
+		if (qlm >= 8)
+			return -1;	/* FIXME for OCI */
+		/* Check if QLM is configured */
+		cfg.u64 = cvmx_read_csr(CVMX_GSERX_CFG(qlm));
+		if (cfg.u64 == 0)
+			return -1;
+		if (cfg.s.pcie) {
+			int pem = 0;
+			cvmx_pemx_cfg_t pemx_cfg;
+			switch(qlm) {
+			case 0: /* Either PEM0 x4 of PEM0 x8 */
+				pem = 0;
+				break;
+			case 1: /* Either PEM0 x4 of PEM1 x4 */
+				pemx_cfg.u64 = cvmx_read_csr(CVMX_PEMX_CFG(0));
+				if (pemx_cfg.cn78xx.lanes8)
+					pem = 0;
+				else
+					pem = 1;
+				break;
+			case 2: /* Either PEM2 x4 of PEM2 x8 */
+				pem = 2;
+				break;
+			case 3: /* Either PEM2 x8 of PEM3 x4 or x8 */
+				/* Can be last 4 lanes of PEM2 */
+				pemx_cfg.u64 = cvmx_read_csr(CVMX_PEMX_CFG(2));
+				if (pemx_cfg.cn78xx.lanes8)
+					pem = 2;
+				else {
+					pemx_cfg.u64 = cvmx_read_csr(CVMX_PEMX_CFG(3));
+					if (pemx_cfg.cn78xx.lanes8)
+						pem = 3;
+					else
+						pem = 2;
+				}
+				break;
+			case 4: /* Either PEM3 x8 of PEM3 x4 */
+				pem = 3;
+				break;
+			default:
+				cvmx_dprintf("QLM%d: Should be in PCIe mode\n", qlm);
+				break;
+			}
+			pemx_cfg.u64 = cvmx_read_csr(CVMX_PEMX_CFG(pem));
+			switch(pemx_cfg.s.md) {
+				case 0: /* Gen1 */
+					return 2500;
+				case 1: /* Gen2 */
+					return 5000;
+				case 2: /* Gen3 */
+					return 8000;
+				default:
+					return 0;
+			}
+		} else {
+			lane_mode.u64 = cvmx_read_csr(CVMX_GSERX_LANE_MODE(qlm));
+			switch (lane_mode.s.lmode) {
+			case R_25G_REFCLK100:
+				return 2500;
+			case R_5G_REFCLK100:
+				return 5000;
+			case R_8G_REFCLK100:
+				return 8000;
+			case R_125G_REFCLK15625_KX:
+				return 1250;
+			case R_3125G_REFCLK15625_XAUI:
+				return 3125;
+			case R_103125G_REFCLK15625_KR:
+				return 10312;
+			case R_125G_REFCLK15625_SGMII:
+				return 1250;
+			case R_5G_REFCLK15625_QSGMII:
+				return 5000;
+			case R_625G_REFCLK15625_RXAUI:
+				return 6250;
+			case R_25G_REFCLK125:
+				return 2500;
+			case R_5G_REFCLK125:
+				return 5000;
+			case R_8G_REFCLK125:
+				return 8000;
+			default:
+				return 0;
+			}
+		}
+	}
+	return 0;
+}
+
+static enum cvmx_qlm_mode __cvmx_qlm_get_mode_cn70xx(int qlm)
+{
+#ifndef CVMX_BUILD_FOR_LINUX_HOST
+	if (cvmx_sysinfo_get()->board_type != CVMX_BOARD_TYPE_SIM) {
+		union cvmx_gserx_dlmx_phy_reset phy_reset;
+
+		phy_reset.u64 = cvmx_read_csr(CVMX_GSERX_DLMX_PHY_RESET(qlm, 0));
+		if (phy_reset.s.phy_reset)
+			return CVMX_QLM_MODE_DISABLED;
+
+	}
+#endif
+
+	switch(qlm) {
+	case 0: /* DLM0/DLM1 - SGMII/QSGMII/RXAUI */
+		{
+			union cvmx_gmxx_inf_mode inf_mode0, inf_mode1;
+
+			inf_mode0.u64 = cvmx_read_csr(CVMX_GMXX_INF_MODE(0));
+			inf_mode1.u64 = cvmx_read_csr(CVMX_GMXX_INF_MODE(1));
+
+			/* SGMII0 SGMII1 */
+			switch (inf_mode0.s.mode) {
+			case CVMX_GMX_INF_MODE_SGMII:
+				switch (inf_mode1.s.mode) {
+				case CVMX_GMX_INF_MODE_SGMII:
+					return CVMX_QLM_MODE_SGMII_SGMII;
+				case CVMX_GMX_INF_MODE_QSGMII:
+					return CVMX_QLM_MODE_SGMII_QSGMII;
+				default:
+					return CVMX_QLM_MODE_SGMII_DISABLED;
+				}
+			case CVMX_GMX_INF_MODE_QSGMII:
+				switch (inf_mode1.s.mode) {
+				case CVMX_GMX_INF_MODE_SGMII:
+					return CVMX_QLM_MODE_QSGMII_SGMII;
+				case CVMX_GMX_INF_MODE_QSGMII:
+					return CVMX_QLM_MODE_QSGMII_QSGMII;
+				default:
+					return CVMX_QLM_MODE_QSGMII_DISABLED;
+				}
+			case CVMX_GMX_INF_MODE_RXAUI:
+				return CVMX_QLM_MODE_RXAUI_1X2;
+			default:
+				return CVMX_QLM_MODE_DISABLED;
+			}
+		}
+	case 1:  /* Sata / pem0 */
+		{
+			union cvmx_gserx_sata_cfg sata_cfg;
+			union cvmx_pemx_cfg pem0_cfg;
+
+			sata_cfg.u64 = cvmx_read_csr(CVMX_GSERX_SATA_CFG(0));
+			pem0_cfg.u64 = cvmx_read_csr(CVMX_PEMX_CFG(0));
+
+			switch(pem0_cfg.cn70xx.md) {
+			case CVMX_PEM_MD_GEN2_2LANE:
+			case CVMX_PEM_MD_GEN1_2LANE:
+				return CVMX_QLM_MODE_PCIE_1X2;
+			case CVMX_PEM_MD_GEN2_1LANE:
+			case CVMX_PEM_MD_GEN1_1LANE:
+				if (sata_cfg.s.sata_en)
+					/* Both PEM0 and PEM1 */
+					return CVMX_QLM_MODE_PCIE_2X1;
+				else
+					/* Only PEM0 */
+					return CVMX_QLM_MODE_PCIE_1X1;
+			case CVMX_PEM_MD_GEN2_4LANE:
+			case CVMX_PEM_MD_GEN1_4LANE:
+				return CVMX_QLM_MODE_PCIE;
+			default:
+				return CVMX_QLM_MODE_DISABLED;
+			}
+		}
+	case 2:
+		{
+			union cvmx_gserx_sata_cfg sata_cfg;
+			union cvmx_pemx_cfg pem0_cfg, pem1_cfg, pem2_cfg;
+
+			sata_cfg.u64 = cvmx_read_csr(CVMX_GSERX_SATA_CFG(0));
+			pem0_cfg.u64 = cvmx_read_csr(CVMX_PEMX_CFG(0));
+			pem1_cfg.u64 = cvmx_read_csr(CVMX_PEMX_CFG(1));
+			pem2_cfg.u64 = cvmx_read_csr(CVMX_PEMX_CFG(2));
+
+			if (sata_cfg.s.sata_en)
+				return CVMX_QLM_MODE_SATA_2X1;
+			if (pem0_cfg.cn70xx.md == CVMX_PEM_MD_GEN2_4LANE
+			    || pem0_cfg.cn70xx.md == CVMX_PEM_MD_GEN1_4LANE)
+				return CVMX_QLM_MODE_PCIE;
+			if (pem1_cfg.cn70xx.md == CVMX_PEM_MD_GEN2_2LANE
+			    || pem1_cfg.cn70xx.md == CVMX_PEM_MD_GEN1_2LANE) {
+				return CVMX_QLM_MODE_PCIE_1X2;
+			}
+			if (pem1_cfg.cn70xx.md == CVMX_PEM_MD_GEN2_1LANE
+			    || pem1_cfg.cn70xx.md == CVMX_PEM_MD_GEN1_1LANE) {
+				if (pem2_cfg.cn70xx.md == CVMX_PEM_MD_GEN2_1LANE
+				    || pem2_cfg.cn70xx.md == CVMX_PEM_MD_GEN1_1LANE) {
+					return CVMX_QLM_MODE_PCIE_2X1;
+				} else
+					return CVMX_QLM_MODE_PCIE_1X1;
+			}
+			if (pem2_cfg.cn70xx.md == CVMX_PEM_MD_GEN2_1LANE
+			    || pem2_cfg.cn70xx.md == CVMX_PEM_MD_GEN1_1LANE)
+				return CVMX_QLM_MODE_PCIE_2X1;
+			return CVMX_QLM_MODE_DISABLED;
+		}
+	default:
+		return CVMX_QLM_MODE_DISABLED;
+	}
+
+	return CVMX_QLM_MODE_DISABLED;
+}
+
+/*
+ * Get the DLM mode for the interface based on the interface type.
+ *
+ * @param interface_type   0 - SGMII/QSGMII/RXAUI interface
+ *                         1 - PCIe
+ *                         2 - SATA
+ * @param interface        interface to use
+ * @return  the qlm mode the interface is
+ */
+enum cvmx_qlm_mode cvmx_qlm_get_dlm_mode(int interface_type, int interface)
+{
+	switch (interface_type) {
+	case 0:  /* SGMII/QSGMII/RXAUI */
+	{
+		enum cvmx_qlm_mode qlm_mode = __cvmx_qlm_get_mode_cn70xx(0);
+		switch (interface) {
+		case 0:
+			switch (qlm_mode) {
+			case CVMX_QLM_MODE_SGMII_SGMII:
+			case CVMX_QLM_MODE_SGMII_DISABLED:
+			case CVMX_QLM_MODE_SGMII_QSGMII:
+				return CVMX_QLM_MODE_SGMII;
+			case CVMX_QLM_MODE_QSGMII_QSGMII:
+			case CVMX_QLM_MODE_QSGMII_DISABLED:
+			case CVMX_QLM_MODE_QSGMII_SGMII:
+				return CVMX_QLM_MODE_QSGMII;
+			case CVMX_QLM_MODE_RXAUI_1X2:
+				return CVMX_QLM_MODE_RXAUI;
+			default:
+				return CVMX_QLM_MODE_DISABLED;
+			}
+		case 1:
+			switch (qlm_mode) {
+			case CVMX_QLM_MODE_SGMII_SGMII:
+			case CVMX_QLM_MODE_DISABLED_SGMII:
+			case CVMX_QLM_MODE_QSGMII_SGMII:
+				return CVMX_QLM_MODE_SGMII;
+			case CVMX_QLM_MODE_QSGMII_QSGMII:
+			case CVMX_QLM_MODE_DISABLED_QSGMII:
+			case CVMX_QLM_MODE_SGMII_QSGMII:
+				return CVMX_QLM_MODE_QSGMII;
+			default:
+				return CVMX_QLM_MODE_DISABLED;
+			}
+		default:
+			return qlm_mode;
+		}
+	}
+	case 1:  /* PCIe */
+	{
+		enum cvmx_qlm_mode qlm_mode1 = __cvmx_qlm_get_mode_cn70xx(1);
+		enum cvmx_qlm_mode qlm_mode2 = __cvmx_qlm_get_mode_cn70xx(2);
+
+		switch (interface) {
+		case 0: /* PCIe0 can be DLM1 with 1, 2 or 4 lanes */
+			return qlm_mode1;
+		case 1: /* PCIe1 can be in DLM1 1 lane(1), DLM2 1 lane(0) or 2 lanes(0-1) */
+			if (qlm_mode1 == CVMX_QLM_MODE_PCIE_2X1)
+				return CVMX_QLM_MODE_PCIE_2X1;
+			else if (qlm_mode2 == CVMX_QLM_MODE_PCIE_1X2 ||
+				 qlm_mode2 == CVMX_QLM_MODE_PCIE_2X1)
+				return qlm_mode2;
+			else
+				return CVMX_QLM_MODE_DISABLED;
+		case 2: /* PCIe2 can be DLM2 1 lanes(1) */
+			if (qlm_mode2 == CVMX_QLM_MODE_PCIE_2X1)
+				return qlm_mode2;
+			else
+				return CVMX_QLM_MODE_DISABLED;
+		default:
+			return CVMX_QLM_MODE_DISABLED;
+		}
+	}
+	case 2:  /* SATA */
+	{
+		enum cvmx_qlm_mode qlm_mode = __cvmx_qlm_get_mode_cn70xx(2);
+
+		if (qlm_mode == CVMX_QLM_MODE_SATA_2X1)
+			return CVMX_QLM_MODE_SATA_2X1;
+		else
+			return CVMX_QLM_MODE_DISABLED;
+	}
+	default:
+		return CVMX_QLM_MODE_DISABLED;
+	}
+}
+
+static enum cvmx_qlm_mode __cvmx_qlm_get_mode_cn6xxx(int qlm)
+{
+	cvmx_mio_qlmx_cfg_t qlmx_cfg;
+
+	if (OCTEON_IS_MODEL(OCTEON_CN68XX)) {
+		qlmx_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(qlm));
+		/* QLM is disabled when QLM SPD is 15. */
+		if (qlmx_cfg.s.qlm_spd == 15)
+			return CVMX_QLM_MODE_DISABLED;
+
+		switch (qlmx_cfg.s.qlm_cfg) {
+		case 0:	/* PCIE */
+			return CVMX_QLM_MODE_PCIE;
+		case 1:	/* ILK */
+			return CVMX_QLM_MODE_ILK;
+		case 2:	/* SGMII */
+			return CVMX_QLM_MODE_SGMII;
+		case 3:	/* XAUI */
+			return CVMX_QLM_MODE_XAUI;
+		case 7:	/* RXAUI */
+			return CVMX_QLM_MODE_RXAUI;
+		default:
+			return CVMX_QLM_MODE_DISABLED;
+		}
+	} else if (OCTEON_IS_MODEL(OCTEON_CN66XX)) {
+		qlmx_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(qlm));
+		/* QLM is disabled when QLM SPD is 15. */
+		if (qlmx_cfg.s.qlm_spd == 15)
+			return CVMX_QLM_MODE_DISABLED;
+
+		switch (qlmx_cfg.s.qlm_cfg) {
+		case 0x9:	/* SGMII */
+			return CVMX_QLM_MODE_SGMII;
+		case 0xb:	/* XAUI */
+			return CVMX_QLM_MODE_XAUI;
+		case 0x0:	/* PCIE gen2 */
+		case 0x8:	/* PCIE gen2 (alias) */
+		case 0x2:	/* PCIE gen1 */
+		case 0xa:	/* PCIE gen1 (alias) */
+			return CVMX_QLM_MODE_PCIE;
+		case 0x1:	/* SRIO 1x4 short */
+		case 0x3:	/* SRIO 1x4 long */
+			return CVMX_QLM_MODE_SRIO_1X4;
+		case 0x4:	/* SRIO 2x2 short */
+		case 0x6:	/* SRIO 2x2 long */
+			return CVMX_QLM_MODE_SRIO_2X2;
+		case 0x5:	/* SRIO 4x1 short */
+		case 0x7:	/* SRIO 4x1 long */
+			if (!OCTEON_IS_MODEL(OCTEON_CN66XX_PASS1_0))
+				return CVMX_QLM_MODE_SRIO_4X1;
+		default:
+			return CVMX_QLM_MODE_DISABLED;
+		}
+	} else if (OCTEON_IS_MODEL(OCTEON_CN63XX)) {
+		cvmx_sriox_status_reg_t status_reg;
+		/* For now skip qlm2 */
+		if (qlm == 2) {
+			cvmx_gmxx_inf_mode_t inf_mode;
+			inf_mode.u64 = cvmx_read_csr(CVMX_GMXX_INF_MODE(0));
+			if (inf_mode.s.speed == 15)
+				return CVMX_QLM_MODE_DISABLED;
+			else if (inf_mode.s.mode == 0)
+				return CVMX_QLM_MODE_SGMII;
+			else
+				return CVMX_QLM_MODE_XAUI;
+		}
+		status_reg.u64 = cvmx_read_csr(CVMX_SRIOX_STATUS_REG(qlm));
+		if (status_reg.s.srio)
+			return CVMX_QLM_MODE_SRIO_1X4;
+		else
+			return CVMX_QLM_MODE_PCIE;
+	} else if (OCTEON_IS_MODEL(OCTEON_CN61XX)) {
+		qlmx_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(qlm));
+		/* QLM is disabled when QLM SPD is 15. */
+		if (qlmx_cfg.s.qlm_spd == 15)
+			return CVMX_QLM_MODE_DISABLED;
+
+		switch (qlm) {
+		case 0:
+			switch (qlmx_cfg.s.qlm_cfg) {
+			case 0:	/* PCIe 1x4 gen2 / gen1 */
+				return CVMX_QLM_MODE_PCIE;
+			case 2:	/* SGMII */
+				return CVMX_QLM_MODE_SGMII;
+			case 3:	/* XAUI */
+				return CVMX_QLM_MODE_XAUI;
+			default:
+				return CVMX_QLM_MODE_DISABLED;
+			}
+			break;
+		case 1:
+			switch (qlmx_cfg.s.qlm_cfg) {
+			case 0:	/* PCIe 1x2 gen2 / gen1 */
+				return CVMX_QLM_MODE_PCIE_1X2;
+			case 1:	/* PCIe 2x1 gen2 / gen1 */
+				return CVMX_QLM_MODE_PCIE_2X1;
+			default:
+				return CVMX_QLM_MODE_DISABLED;
+			}
+			break;
+		case 2:
+			switch (qlmx_cfg.s.qlm_cfg) {
+			case 2:	/* SGMII */
+				return CVMX_QLM_MODE_SGMII;
+			case 3:	/* XAUI */
+				return CVMX_QLM_MODE_XAUI;
+			default:
+				return CVMX_QLM_MODE_DISABLED;
+			}
+			break;
+		}
+	} else if (OCTEON_IS_MODEL(OCTEON_CNF71XX)) {
+		qlmx_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(qlm));
+		/* QLM is disabled when QLM SPD is 15. */
+		if (qlmx_cfg.s.qlm_spd == 15)
+			return CVMX_QLM_MODE_DISABLED;
+
+		switch (qlm) {
+		case 0:
+			if (qlmx_cfg.s.qlm_cfg == 2)	/* SGMII */
+				return CVMX_QLM_MODE_SGMII;
+			break;
+		case 1:
+			switch (qlmx_cfg.s.qlm_cfg) {
+			case 0:	/* PCIe 1x2 gen2 / gen1 */
+				return CVMX_QLM_MODE_PCIE_1X2;
+			case 1:	/* PCIe 2x1 gen2 / gen1 */
+				return CVMX_QLM_MODE_PCIE_2X1;
+			default:
+				return CVMX_QLM_MODE_DISABLED;
+			}
+			break;
+		}
+	}
+	return CVMX_QLM_MODE_DISABLED;
+}
+
+enum cvmx_qlm_mode cvmx_qlm_get_mode_cn78xx(int node, int qlm)
+{
+	cvmx_gserx_cfg_t gserx_cfg;
+
+	if (qlm >= 8)
+		return CVMX_QLM_MODE_OCI;
+
+	gserx_cfg.u64 = cvmx_read_csr_node(node, CVMX_GSERX_CFG(qlm));
+	if (gserx_cfg.s.pcie) {
+		switch (qlm) {
+		case 0: /* Either PEM0 x4 or PEM0 x8 */
+		case 1: /* Either PEM0 x8 or PEM1 x4 */
+		{
+			cvmx_pemx_cfg_t pemx_cfg;
+			pemx_cfg.u64 = cvmx_read_csr_node(node, CVMX_PEMX_CFG(0));
+			if (pemx_cfg.cn78xx.lanes8)
+				return CVMX_QLM_MODE_PCIE_1X8; /* PEM0 x8 */
+			else
+				return CVMX_QLM_MODE_PCIE;     /* PEM0 x4 */
+		}
+		case 2: /* Either PEM2 x4 or PEM2 x8 */
+		{
+			cvmx_pemx_cfg_t pemx_cfg;
+			pemx_cfg.u64 = cvmx_read_csr_node(node, CVMX_PEMX_CFG(2));
+			if (pemx_cfg.cn78xx.lanes8)
+				return CVMX_QLM_MODE_PCIE_1X8;  /* PEM2 x8 */
+			else
+				return CVMX_QLM_MODE_PCIE;      /* PEM2 x4 */
+		}
+		case 3: /* Either PEM2 x8 or PEM3 x4 or PEM3 x8 */
+		{
+			cvmx_pemx_cfg_t pemx_cfg;
+			pemx_cfg.u64 = cvmx_read_csr_node(node, CVMX_PEMX_CFG(2));
+			if (pemx_cfg.cn78xx.lanes8)
+				return CVMX_QLM_MODE_PCIE_1X8;  /* PEM2 x8 */
+
+			/* Can be first 4 lanes of PEM3 */
+			pemx_cfg.u64 = cvmx_read_csr_node(node, CVMX_PEMX_CFG(3));
+			if (pemx_cfg.cn78xx.lanes8)
+				return CVMX_QLM_MODE_PCIE_1X8;  /* PEM3 x8 */
+			else
+				return CVMX_QLM_MODE_PCIE; /* PEM2 x4 */
+		}
+		case 4: /* Either PEM3 x8 or PEM3 x4 */
+		{
+			cvmx_pemx_cfg_t pemx_cfg;
+			pemx_cfg.u64 = cvmx_read_csr_node(node, CVMX_PEMX_CFG(3));
+			if (pemx_cfg.cn78xx.lanes8)
+				return CVMX_QLM_MODE_PCIE_1X8; /* PEM3 x8 */
+			else
+				return CVMX_QLM_MODE_PCIE; /* PEM3 x4 */
+		}
+		default:
+			return CVMX_QLM_MODE_DISABLED;
+		}
+	} else if (gserx_cfg.s.ila) {
+		return CVMX_QLM_MODE_ILK;
+	} else if (gserx_cfg.s.bgx) {
+		cvmx_bgxx_cmrx_config_t cmr_config;
+		cvmx_bgxx_spux_br_pmd_control_t pmd_control;
+		int bgx = (qlm < 2) ? qlm : qlm - 2;
+		
+		cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(0, bgx));
+		pmd_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_CONTROL(0, bgx));
+		
+		switch(cmr_config.s.lmac_type) {
+		case 0: return CVMX_QLM_MODE_SGMII;
+		case 1:	return CVMX_QLM_MODE_XAUI;
+		case 2:	return CVMX_QLM_MODE_RXAUI;
+		case 3:	
+			/* Use training to determine if we're in 10GBASE-KR or XFI */
+			if (pmd_control.s.train_en)
+				return CVMX_QLM_MODE_10G_KR;
+			else
+				return CVMX_QLM_MODE_XFI;
+		case 4:	
+			/* Use training to determine if we're in 10GBASE-KR or XFI */
+			if (pmd_control.s.train_en)
+				return CVMX_QLM_MODE_40G_KR4;
+			else
+				return CVMX_QLM_MODE_XLAUI;
+		default: return CVMX_QLM_MODE_DISABLED;
+		}
+	} else
+		return CVMX_QLM_MODE_DISABLED;
+}
+
+/*
+ * Read QLM and return mode.
+ */
+enum cvmx_qlm_mode cvmx_qlm_get_mode(int qlm)
+{
+	if (OCTEON_IS_OCTEON2())
+		return __cvmx_qlm_get_mode_cn6xxx(qlm);
+	else if (OCTEON_IS_MODEL(OCTEON_CN70XX))
+		return __cvmx_qlm_get_mode_cn70xx(qlm);
+	else if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+		return cvmx_qlm_get_mode_cn78xx(cvmx_get_node_num(), qlm);
+
+	return CVMX_QLM_MODE_DISABLED;
+}
+
+int cvmx_qlm_measure_clock_cn78xx(int qlm)
+{
+	cvmx_gserx_cfg_t cfg;
+	cvmx_gserx_refclk_sel_t refclk_sel;
+	cvmx_gserx_lane_mode_t lane_mode;
+
+	if (qlm >= 8)
+		return -1; /* FIXME for OCI */
+
+	cfg.u64 = cvmx_read_csr(CVMX_GSERX_CFG(qlm));
+
+	if (cfg.s.pcie) {
+		refclk_sel.u64 = cvmx_read_csr(CVMX_GSERX_REFCLK_SEL(qlm));
+		if (refclk_sel.s.pcie_refclk125)
+			return REF_125MHZ; /* Ref 125 Mhz */
+		else
+			return REF_100MHZ; /* Ref 100Mhz */
+	}
+
+	lane_mode.u64 = cvmx_read_csr(CVMX_GSERX_LANE_MODE(qlm));
+	switch(lane_mode.s.lmode) {
+	case R_25G_REFCLK100:
+		return REF_100MHZ;
+	case R_5G_REFCLK100:
+		return REF_100MHZ;
+	case R_8G_REFCLK100:
+		return REF_100MHZ;
+	case R_125G_REFCLK15625_KX:
+		return REF_156MHZ;
+	case R_3125G_REFCLK15625_XAUI:
+		return REF_156MHZ;
+	case R_103125G_REFCLK15625_KR:
+		return REF_156MHZ;
+	case R_125G_REFCLK15625_SGMII:
+		return REF_156MHZ;
+	case R_5G_REFCLK15625_QSGMII:
+		return REF_156MHZ;
+	case R_625G_REFCLK15625_RXAUI:
+		return REF_156MHZ;
+	case R_25G_REFCLK125:
+		return REF_125MHZ;
+	case R_5G_REFCLK125:
+		return REF_125MHZ;
+	case R_8G_REFCLK125:
+		return REF_125MHZ;
+	default:
+		return 0;
+	}
+}
+
+/**
+ * Measure the reference clock of a QLM
+ *
+ * @param qlm    QLM to measure
+ *
+ * @return Clock rate in Hz
+ *       */
+int cvmx_qlm_measure_clock(int qlm)
+{
+	cvmx_mio_ptp_clock_cfg_t ptp_clock;
+	uint64_t count;
+	uint64_t start_cycle, stop_cycle;
+	int evcnt_offset = 0x10;
+#ifdef CVMX_BUILD_FOR_UBOOT
+	int ref_clock[16] = {0};
+#else
+	static int ref_clock[16] = {0};
+#endif
+
+	if (ref_clock[qlm])
+		return ref_clock[qlm];
+
+	if (OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN5XXX))
+		return -1;
+
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+		return cvmx_qlm_measure_clock_cn78xx(qlm);
+
+	/* Force the reference to 156.25Mhz when running in simulation.
+	   This supports the most speeds */
+#ifdef CVMX_BUILD_FOR_UBOOT
+	if (gd->board_type == CVMX_BOARD_TYPE_SIM)
+		return 156250000;
+#elif !defined(CVMX_BUILD_FOR_LINUX_HOST)
+	if (cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_SIM)
+		return 156250000;
+#endif
+	/* Fix reference clock for OCI QLMs */
+
+	/* Disable the PTP event counter while we configure it */
+	ptp_clock.u64 = cvmx_read_csr(CVMX_MIO_PTP_CLOCK_CFG);	/* For CN63XXp1 errata */
+	ptp_clock.s.evcnt_en = 0;
+	cvmx_write_csr(CVMX_MIO_PTP_CLOCK_CFG, ptp_clock.u64);
+	/* Count on rising edge, Choose which QLM to count */
+	ptp_clock.u64 = cvmx_read_csr(CVMX_MIO_PTP_CLOCK_CFG);	/* For CN63XXp1 errata */
+	ptp_clock.s.evcnt_edge = 0;
+	ptp_clock.s.evcnt_in = evcnt_offset + qlm;
+	cvmx_write_csr(CVMX_MIO_PTP_CLOCK_CFG, ptp_clock.u64);
+	/* Clear MIO_PTP_EVT_CNT */
+	cvmx_read_csr(CVMX_MIO_PTP_EVT_CNT);	/* For CN63XXp1 errata */
+	count = cvmx_read_csr(CVMX_MIO_PTP_EVT_CNT);
+	cvmx_write_csr(CVMX_MIO_PTP_EVT_CNT, -count);
+	/* Set MIO_PTP_EVT_CNT to 1 billion */
+	cvmx_write_csr(CVMX_MIO_PTP_EVT_CNT, 1000000000);
+	/* Enable the PTP event counter */
+	ptp_clock.u64 = cvmx_read_csr(CVMX_MIO_PTP_CLOCK_CFG);	/* For CN63XXp1 errata */
+	ptp_clock.s.evcnt_en = 1;
+	cvmx_write_csr(CVMX_MIO_PTP_CLOCK_CFG, ptp_clock.u64);
+	start_cycle = cvmx_clock_get_count(CVMX_CLOCK_CORE);
+	/* Wait for 50ms */
+	//cvmx_wait_usec(50000);
+	/* Read the counter */
+	cvmx_read_csr(CVMX_MIO_PTP_EVT_CNT);	/* For CN63XXp1 errata */
+	count = cvmx_read_csr(CVMX_MIO_PTP_EVT_CNT);
+	stop_cycle = cvmx_clock_get_count(CVMX_CLOCK_CORE);
+	/* Disable the PTP event counter */
+	ptp_clock.u64 = cvmx_read_csr(CVMX_MIO_PTP_CLOCK_CFG);	/* For CN63XXp1 errata */
+	ptp_clock.s.evcnt_en = 0;
+	cvmx_write_csr(CVMX_MIO_PTP_CLOCK_CFG, ptp_clock.u64);
+	/* Clock counted down, so reverse it */
+	count = 1000000000 - count;
+	/* Return the rate */
+	ref_clock[qlm] = count * cvmx_clock_get_rate(CVMX_CLOCK_CORE) / (stop_cycle - start_cycle);
+	return ref_clock[qlm];
+}
+
+void cvmx_qlm_display_registers(int qlm)
+{
+	int num_lanes = cvmx_qlm_get_lanes(qlm);
+	int lane;
+	const __cvmx_qlm_jtag_field_t *ptr = cvmx_qlm_jtag_get_field();
+
+	cvmx_dprintf("%29s", "Field[<stop bit>:<start bit>]");
+	for (lane = 0; lane < num_lanes; lane++)
+		cvmx_dprintf("\t      Lane %d", lane);
+	cvmx_dprintf("\n");
+
+	while (ptr != NULL && ptr->name) {
+		cvmx_dprintf("%20s[%3d:%3d]", ptr->name, ptr->stop_bit, ptr->start_bit);
+		for (lane = 0; lane < num_lanes; lane++) {
+			uint64_t val;
+			int tx_byp = 0;
+			/* Make sure serdes_tx_byp is set for displaying
+			   TX amplitude and TX demphasis field values. */
+			if (strncmp(ptr->name, "biasdrv_", 8) == 0 ||
+				strncmp(ptr->name, "tcoeff_", 7) == 0) {
+				tx_byp = cvmx_qlm_jtag_get(qlm, lane, "serdes_tx_byp");
+				if (tx_byp == 0) {
+					cvmx_dprintf("\t \t");
+					continue;
+				}
+			}
+			val = cvmx_qlm_jtag_get(qlm, lane, ptr->name);
+			cvmx_dprintf("\t%4llu (0x%04llx)", (unsigned long long)val, (unsigned long long)val);
+		}
+		cvmx_dprintf("\n");
+		ptr++;
+	}
+}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-range.c b/arch/mips/cavium-octeon/executive/cvmx-range.c
new file mode 100644
index 0000000..7b913c5
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-range.c
@@ -0,0 +1,275 @@
+/***********************license start***************
+ * Copyright (c) 2003-2012  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include <asm/octeon/cvmx-range.h>
+#else
+#include "cvmx-range.h"
+#endif
+
+#define CVMX_RANGE_AVAILABLE ((uint64_t) -88)
+#define addr_of_element(base, index) (1ull << 63 | (base + sizeof(uint64_t) + (index) * sizeof(uint64_t)))
+#define addr_of_size(base) (1ull << 63 | base)
+
+int cvmx_range_memory_size(int nelements)
+{
+	return sizeof(uint64_t) * (nelements + 1);
+}
+
+int cvmx_range_init(uint64_t range_addr, int size)
+{
+	uint64_t i;
+	uint64_t lsize = size;
+
+	cvmx_write64_uint64(addr_of_size(range_addr),lsize);
+	for (i = 0; i < lsize; i++) {
+		cvmx_write64_uint64(addr_of_element(range_addr,i), CVMX_RANGE_AVAILABLE);
+	}
+	return 0;
+}
+
+
+static int64_t cvmx_range_find_next_available(uint64_t range_addr, uint64_t index, int align)
+{
+	uint64_t i;
+	uint64_t size = cvmx_read64_uint64(addr_of_size(range_addr));
+
+	while ((index % align) != 0)
+		index++;
+
+	for (i = index; i < size; i += align) {
+		uint64_t r_owner = cvmx_read64_uint64(addr_of_element(range_addr,i));
+		//cvmx_dprintf("index=%d owner=%llx\n", (int) i, (unsigned long long) r_owner);
+		if (r_owner == CVMX_RANGE_AVAILABLE)
+			return i;
+	}
+	return -1;
+}
+
+int cvmx_range_alloc(uint64_t range_addr, uint64_t owner, uint64_t cnt, int align)
+{
+	uint64_t i=0, size;
+	int64_t first_available;
+
+	//cvmx_dprintf("%s: range_addr=%llx  owner=%llx cnt=%d \n", __FUNCTION__,
+	//	     (unsigned long long) range_addr, (unsigned long long) owner, (int)cnt);
+	size = cvmx_read64_uint64(addr_of_size(range_addr));
+	//cvmx_dprintf("%s: size=%d\n", __FUNCTION__, size);
+	while (i < size) {
+		uint64_t available_cnt=0;
+		first_available = cvmx_range_find_next_available(range_addr, i, align);
+		if (first_available == -1)
+			return -1;
+		i = first_available;
+		//cvmx_dprintf("%s: first_available=%d \n", __FUNCTION__, (int) first_available);
+		while ((available_cnt != cnt) && (i < size)) {
+			uint64_t r_owner = cvmx_read64_uint64(addr_of_element(range_addr,i));
+			if (r_owner == CVMX_RANGE_AVAILABLE)
+				available_cnt++;
+			i++;
+		}
+		if (available_cnt == cnt) {
+			//cvmx_dprintf("%s: first_available=%d available=%d \n", __FUNCTION__,
+			//	     (int) first_available, (int) available_cnt);
+			uint64_t j;
+			for (j = first_available; j < first_available + cnt; j++) {
+				uint64_t a = addr_of_element(range_addr,j);
+				//cvmx_dprintf("%s: j=%d a=%llx \n", __FUNCTION__, (int) j, (unsigned long long) a);
+				cvmx_write64_uint64(a, owner);
+			}
+			return first_available;
+		}
+	}
+	cvmx_dprintf("ERROR: failed to allocate range cnt=%d \n", (int)cnt);
+	cvmx_range_show(range_addr);
+	return -1;
+}
+
+int  cvmx_range_alloc_non_contiguos(uint64_t range_addr, uint64_t owner, uint64_t cnt,
+				    int elements[])
+{
+	uint64_t i=0, size;
+	uint64_t element_index = 0;
+
+	size = cvmx_read64_uint64(addr_of_size(range_addr));
+	for (i = 0; i < size; i++) {
+		uint64_t r_owner = cvmx_read64_uint64(addr_of_element(range_addr,i));
+		//cvmx_dprintf("index=%d owner=%llx\n", (int) i, (unsigned long long) r_owner);
+		if (r_owner == CVMX_RANGE_AVAILABLE)
+			elements[element_index++] = (int) i;
+
+		if (element_index == cnt)
+			break;
+	}
+	if (element_index != cnt) {
+		cvmx_dprintf("ERROR: failed to allocate non contiguos cnt=%d"
+			     " available=%d\n", (int)cnt, (int) element_index);
+		return -1;
+	}
+	for (i = 0; i < cnt; i++) {
+		uint64_t a = addr_of_element(range_addr,elements[i]);
+		cvmx_write64_uint64(a, owner);
+	}
+	return 0;
+
+}
+
+int cvmx_range_reserve(uint64_t range_addr, uint64_t owner, uint64_t base, uint64_t cnt )
+{
+	uint64_t i, size, r_owner;
+	uint64_t up = base + cnt;
+
+	size = cvmx_read64_uint64(addr_of_size(range_addr));
+	if (up > size) {
+		cvmx_dprintf("ERROR: invalid base or cnt size=%d base+cnt=%d \n", (int) size, (int)up);
+		return -1;
+	}
+	for (i = base; i < up; i++) {
+		r_owner = cvmx_read64_uint64(addr_of_element(range_addr,i));
+		//cvmx_dprintf("%d: %llx\n", (int) i,(unsigned long long) r_owner);
+		if (r_owner != CVMX_RANGE_AVAILABLE) {
+			cvmx_dprintf("INFO: resource already reserved base+cnt=%d %llu %llu %llx %llx %llx\n", (int)i, (unsigned long long)cnt, (unsigned long long)base, (unsigned long long)r_owner, (unsigned long long)range_addr, (unsigned long long)owner);
+			return -1;
+		}
+	}
+	for (i = base; i < up; i++) {
+		cvmx_write64_uint64(addr_of_element(range_addr,i), owner);
+	}
+	return base;
+}
+
+int cvmx_range_free_with_owner(uint64_t range_addr, uint64_t owner)
+{
+	uint64_t i, size;
+	int found = -1;
+
+	size = cvmx_read64_uint64(addr_of_size(range_addr));
+	for (i = 0; i < size; i++) {
+		uint64_t r_owner = cvmx_read64_uint64(addr_of_element(range_addr,i));
+		if (r_owner == owner) {
+			cvmx_write64_uint64(addr_of_element(range_addr,i), CVMX_RANGE_AVAILABLE);
+			found = 0;
+		}
+	}
+	return found;
+}
+
+int __cvmx_range_is_allocated(uint64_t range_addr, int bases[], int count)
+{
+	uint64_t i, cnt, size;
+	uint64_t r_owner;
+
+	cnt = count;
+	size = cvmx_read64_uint64(addr_of_size(range_addr));
+	for (i = 0; i < cnt; i++) {
+		uint64_t base = bases[i];
+		if (base >= size) {
+			cvmx_dprintf("ERROR: invalid base or cnt size=%d "
+				     "base=%d \n", (int) size, (int)base);
+			return 0;
+		}
+		r_owner = cvmx_read64_uint64(addr_of_element(range_addr,base));
+		if (r_owner == CVMX_RANGE_AVAILABLE) {
+			cvmx_dprintf("ERROR: i=%d:base=%d is available\n",
+				     (int) i, (int) base);
+			return 0;
+		}
+	}
+	return 1;
+}
+
+int cvmx_range_free_mutiple(uint64_t range_addr, int bases[], int count)
+{
+	uint64_t i, cnt;
+
+	cnt = count;
+	if (__cvmx_range_is_allocated(range_addr, bases, count) != 1) {
+		return -1;
+	}
+	for (i = 0; i < cnt; i++) {
+		uint64_t base = bases[i];
+		cvmx_write64_uint64(addr_of_element(range_addr, base),
+				    CVMX_RANGE_AVAILABLE);
+	}
+	return 0;
+}
+
+int cvmx_range_free_with_base(uint64_t range_addr, int base, int cnt)
+{
+	uint64_t i, size;
+	uint64_t up = base + cnt;
+
+	size = cvmx_read64_uint64(addr_of_size(range_addr));
+	if (up > size) {
+		cvmx_dprintf("ERROR: invalid base or cnt size=%d base+cnt=%d \n", (int) size, (int)up);
+		return -1;
+	}
+	for (i = base; i < up; i++) {
+		cvmx_write64_uint64(addr_of_element(range_addr,i), CVMX_RANGE_AVAILABLE);
+	}
+	return 0;
+}
+
+void cvmx_range_show(uint64_t range_addr)
+{
+	uint64_t pval, val, size, pindex, i;
+
+	size = cvmx_read64_uint64(addr_of_size(range_addr));
+	pval = cvmx_read64_uint64(addr_of_element(range_addr, 0));
+	pindex = 0;
+	for (i = 1; i < size; i++) {
+		val = cvmx_read64_uint64(addr_of_element(range_addr,i));
+		if (val != pval) {
+			cvmx_dprintf ("i=%d : %llx \n", (int) pindex, (unsigned long long)pval);
+			pindex = i;
+			pval = val;
+		}
+	}
+	cvmx_dprintf ("i=%d : %d \n", (int) pindex, (int)pval);
+}
+
+void cvmx_range_cleanup(uint64_t range_addr)
+{
+	uint64_t size, i;
+	size = cvmx_read64_uint64(addr_of_size(range_addr));
+	for (i = 0; i < size; i++) {
+		cvmx_write64_uint64(addr_of_element(range_addr, i),
+			CVMX_RANGE_AVAILABLE);
+	}
+}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-sso-resources.c b/arch/mips/cavium-octeon/executive/cvmx-sso-resources.c
new file mode 100644
index 0000000..852950b
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-sso-resources.c
@@ -0,0 +1,107 @@
+/***********************license start***************
+ * Copyright (c) 2014  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include <asm/octeon/octeon.h>
+#include <asm/octeon/cvmx-pow.h>
+#include <asm/octeon/cvmx-global-resources.h>
+#else
+#include "cvmx.h"
+#include "cvmx-pow.h"
+#include "cvmx-global-resources.h"
+#endif
+
+static struct global_resource_tag get_sso_resource_tag(int node)
+{
+	switch(node) {
+	case 0:
+		return cvmx_get_gr_tag('c','v','m','_','s','s','o','_','0','0','.','.','.','.','.','.');
+	case 1:
+		return cvmx_get_gr_tag('c','v','m','_','s','s','o','_','0','1','.','.','.','.','.','.');
+	case 2:
+		return cvmx_get_gr_tag('c','v','m','_','s','s','o','_','0','2','.','.','.','.','.','.');
+	case 3:
+		return cvmx_get_gr_tag('c','v','m','_','s','s','o','_','0','3','.','.','.','.','.','.');
+	default:
+		/* Add a panic?? */
+		return cvmx_get_gr_tag('i','n','v','a','l','i','d','.','.','.','.','.','.','.','.','.');
+	}
+}
+
+int cvmx_sso_allocate_groups(int node, int groups_allocated[], int count)
+{
+	int num_grp;
+	int rv = -1;
+	uint64_t owner = 0;
+	struct global_resource_tag tag = get_sso_resource_tag(node);
+
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+		num_grp = 256;
+	else if (OCTEON_IS_MODEL(OCTEON_CN68XX))
+		num_grp = 64;
+	else
+		num_grp = 16;
+
+	if (cvmx_create_global_resource_range(tag, num_grp) != 0) {
+		cvmx_dprintf("ERROR: failed to create sso global resource for node=%d\n", node);
+		return -1;
+	}
+
+	if (groups_allocated[0] >= 0) {
+		while (count--) {
+			rv = cvmx_reserve_global_resource_range(tag, owner, groups_allocated[count], 1);
+			if (!rv)
+				return CVMX_RESOURCE_ALREADY_RESERVED;
+		}
+	} else {
+		rv = cvmx_resource_alloc_many(tag, owner, count, groups_allocated);
+	}
+	return rv;
+}
+EXPORT_SYMBOL(cvmx_sso_allocate_groups);
+
+int cvmx_sso_allocate_group(int node)
+{
+	int r;
+	int grp = -1;
+
+	r = cvmx_sso_allocate_groups(node, &grp, 1);
+
+	return r == 0 ? grp : -1;
+}
+EXPORT_SYMBOL(cvmx_sso_allocate_group);
diff --git a/arch/mips/include/asm/octeon/cvmx-address.h b/arch/mips/include/asm/octeon/cvmx-address.h
index 76a5348..46af4dc 100644
--- a/arch/mips/include/asm/octeon/cvmx-address.h
+++ b/arch/mips/include/asm/octeon/cvmx-address.h
@@ -338,4 +338,13 @@ typedef union {
 #define CVMX_OCT_DID_MIS_CSR	    CVMX_FULL_DID(CVMX_OCT_DID_MIS, 7ULL)
 #define CVMX_OCT_DID_ZIP_CSR	    CVMX_FULL_DID(CVMX_OCT_DID_ZIP, 0ULL)
 
+//Cast to unsigned long long, mainly for use in printfs.
+#define CAST_ULL(v) ((unsigned long long) (v))
+
+#ifdef __BIG_ENDIAN_BITFIELD
+#define CVMX_BITFIELD_FIELD(field, more) field; more
+#else
+#define CVMX_BITFIELD_FIELD(field, more) more field;
+#endif
+
 #endif /* __CVMX_ADDRESS_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-atomic.h b/arch/mips/include/asm/octeon/cvmx-atomic.h
new file mode 100644
index 0000000..f4bfb75
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-atomic.h
@@ -0,0 +1,676 @@
+/***********************license start***************
+ * Copyright (c) 2003-2010  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * This file provides atomic operations
+ *
+ * <hr>$Revision: 73845 $<hr>
+ *
+ *
+ */
+
+#ifndef __CVMX_ATOMIC_H__
+#define __CVMX_ATOMIC_H__
+
+#include "cvmx-asm.h"
+
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+extern "C" {
+/* *INDENT-ON* */
+#endif
+
+/**
+ * Atomically adds a signed value to a 32 bit (aligned) memory location.
+ *
+ * This version does not perform 'sync' operations to enforce memory
+ * operations.  This should only be used when there are no memory operation
+ * ordering constraints.  (This should NOT be used for reference counting -
+ * use the standard version instead.)
+ *
+ * @param ptr    address in memory to add incr to
+ * @param incr   amount to increment memory location by (signed)
+ */
+static inline void cvmx_atomic_add32_nosync(int32_t * ptr, int32_t incr)
+{
+
+#ifdef CVMX_CAVIUM_OCTEON2
+	{
+		__asm__ __volatile__("   saa %[inc], (%[base]) \n":"+m"(*ptr)
+				     :[inc] "r"(incr),[base] "r"(ptr)
+				     :"memory");
+	}
+#else
+	if (OCTEON_IS_MODEL(OCTEON_CN3XXX)) {
+		uint32_t tmp;
+
+		__asm__ __volatile__(".set noreorder         \n"
+				     "1: ll   %[tmp], %[val] \n"
+				     "   addu %[tmp], %[inc] \n"
+				     "   sc   %[tmp], %[val] \n"
+				     "   beqz %[tmp], 1b     \n" "   nop                 \n" ".set reorder           \n":[val] "+m"(*ptr),[tmp] "=&r"(tmp)
+				     :[inc] "r"(incr)
+				     :"memory");
+	} else {
+		__asm__ __volatile__("   saa %[inc], (%[base]) \n":"+m"(*ptr)
+				     :[inc] "r"(incr),[base] "r"(ptr)
+				     :"memory");
+	}
+#endif
+}
+
+/**
+ * Atomically adds a signed value to a 32 bit (aligned) memory location.
+ *
+ * Memory access ordering is enforced before/after the atomic operation,
+ * so no additional 'sync' instructions are required.
+ *
+ *
+ * @param ptr    address in memory to add incr to
+ * @param incr   amount to increment memory location by (signed)
+ */
+static inline void cvmx_atomic_add32(int32_t * ptr, int32_t incr)
+{
+	CVMX_SYNCWS;
+	cvmx_atomic_add32_nosync(ptr, incr);
+	CVMX_SYNCWS;
+}
+
+/**
+ * Atomically sets a 32 bit (aligned) memory location to a value
+ *
+ * @param ptr    address of memory to set
+ * @param value  value to set memory location to.
+ */
+static inline void cvmx_atomic_set32(int32_t * ptr, int32_t value)
+{
+	CVMX_SYNCWS;
+	*ptr = value;
+	CVMX_SYNCWS;
+}
+
+/**
+ * Returns the current value of a 32 bit (aligned) memory
+ * location.
+ *
+ * @param ptr    Address of memory to get
+ * @return Value of the memory
+ */
+static inline int32_t cvmx_atomic_get32(int32_t * ptr)
+{
+	return *(volatile int32_t *)ptr;
+}
+
+/**
+ * Atomically adds a signed value to a 64 bit (aligned) memory location.
+ *
+ * This version does not perform 'sync' operations to enforce memory
+ * operations.  This should only be used when there are no memory operation
+ * ordering constraints.  (This should NOT be used for reference counting -
+ * use the standard version instead.)
+ *
+ * @param ptr    address in memory to add incr to
+ * @param incr   amount to increment memory location by (signed)
+ */
+static inline void cvmx_atomic_add64_nosync(int64_t * ptr, int64_t incr)
+{
+#ifdef CVMX_CAVIUM_OCTEON2
+	{
+		__asm__ __volatile__("   saad %[inc], (%[base])  \n":"+m"(*ptr)
+				     :[inc] "r"(incr),[base] "r"(ptr)
+				     :"memory");
+	}
+#else
+	if (OCTEON_IS_MODEL(OCTEON_CN3XXX)) {
+		uint64_t tmp;
+		__asm__ __volatile__(".set noreorder         \n"
+				     "1: lld  %[tmp], %[val] \n"
+				     "   daddu %[tmp], %[inc] \n"
+				     "   scd  %[tmp], %[val] \n"
+				     "   beqz %[tmp], 1b     \n" "   nop                 \n" ".set reorder           \n":[val] "+m"(*ptr),[tmp] "=&r"(tmp)
+				     :[inc] "r"(incr)
+				     :"memory");
+	} else {
+		__asm__ __volatile__("   saad %[inc], (%[base])  \n":"+m"(*ptr)
+				     :[inc] "r"(incr),[base] "r"(ptr)
+				     :"memory");
+	}
+#endif
+}
+
+/**
+ * Atomically adds a signed value to a 64 bit (aligned) memory location.
+ *
+ * Memory access ordering is enforced before/after the atomic operation,
+ * so no additional 'sync' instructions are required.
+ *
+ *
+ * @param ptr    address in memory to add incr to
+ * @param incr   amount to increment memory location by (signed)
+ */
+static inline void cvmx_atomic_add64(int64_t * ptr, int64_t incr)
+{
+	CVMX_SYNCWS;
+	cvmx_atomic_add64_nosync(ptr, incr);
+	CVMX_SYNCWS;
+}
+
+/**
+ * Atomically sets a 64 bit (aligned) memory location to a value
+ *
+ * @param ptr    address of memory to set
+ * @param value  value to set memory location to.
+ */
+static inline void cvmx_atomic_set64(int64_t * ptr, int64_t value)
+{
+	CVMX_SYNCWS;
+	*ptr = value;
+	CVMX_SYNCWS;
+}
+
+/**
+ * Returns the current value of a 64 bit (aligned) memory
+ * location.
+ *
+ * @param ptr    Address of memory to get
+ * @return Value of the memory
+ */
+static inline int64_t cvmx_atomic_get64(int64_t * ptr)
+{
+	return *(volatile int64_t *)ptr;
+}
+
+/**
+ * Atomically compares the old value with the value at ptr, and if they match,
+ * stores new_val to ptr.
+ * If *ptr and old don't match, function returns failure immediately.
+ * If *ptr and old match, function spins until *ptr updated to new atomically, or
+ *  until *ptr and old no longer match
+ *
+ * Does no memory synchronization.
+ *
+ * @return 1 on success (match and store)
+ *         0 on no match
+ */
+static inline uint32_t cvmx_atomic_compare_and_store32_nosync(uint32_t * ptr, uint32_t old_val, uint32_t new_val)
+{
+	uint32_t tmp, ret;
+
+	__asm__ __volatile__(".set noreorder         \n"
+			     "1: ll   %[tmp], %[val] \n"
+			     "   li   %[ret], 0     \n"
+			     "   bne  %[tmp], %[old], 2f \n"
+			     "   move %[tmp], %[new_val] \n"
+			     "   sc   %[tmp], %[val] \n"
+			     "   beqz %[tmp], 1b     \n"
+			     "   li   %[ret], 1      \n" "2: nop               \n" ".set reorder           \n":[val] "+m"(*ptr),[tmp] "=&r"(tmp),[ret] "=&r"(ret)
+			     :[old] "r"(old_val),[new_val] "r"(new_val)
+			     :"memory");
+
+	return (ret);
+
+}
+
+/**
+ * Atomically compares the old value with the value at ptr, and if they match,
+ * stores new_val to ptr.
+ * If *ptr and old don't match, function returns failure immediately.
+ * If *ptr and old match, function spins until *ptr updated to new atomically, or
+ *  until *ptr and old no longer match
+ *
+ * Does memory synchronization that is required to use this as a locking primitive.
+ *
+ * @return 1 on success (match and store)
+ *         0 on no match
+ */
+static inline uint32_t cvmx_atomic_compare_and_store32(uint32_t * ptr, uint32_t old_val, uint32_t new_val)
+{
+	uint32_t ret;
+	CVMX_SYNCWS;
+	ret = cvmx_atomic_compare_and_store32_nosync(ptr, old_val, new_val);
+	CVMX_SYNCWS;
+	return ret;
+
+}
+
+/**
+ * Atomically compares the old value with the value at ptr, and if they match,
+ * stores new_val to ptr.
+ * If *ptr and old don't match, function returns failure immediately.
+ * If *ptr and old match, function spins until *ptr updated to new atomically, or
+ *  until *ptr and old no longer match
+ *
+ * Does no memory synchronization.
+ *
+ * @return 1 on success (match and store)
+ *         0 on no match
+ */
+static inline uint64_t cvmx_atomic_compare_and_store64_nosync(uint64_t * ptr, uint64_t old_val, uint64_t new_val)
+{
+	uint64_t tmp, ret;
+
+	__asm__ __volatile__(".set noreorder         \n"
+			     "1: lld  %[tmp], %[val] \n"
+			     "   li   %[ret], 0     \n"
+			     "   bne  %[tmp], %[old], 2f \n"
+			     "   move %[tmp], %[new_val] \n"
+			     "   scd  %[tmp], %[val] \n"
+			     "   beqz %[tmp], 1b     \n"
+			     "   li   %[ret], 1      \n" "2: nop               \n" ".set reorder           \n":[val] "+m"(*ptr),[tmp] "=&r"(tmp),[ret] "=&r"(ret)
+			     :[old] "r"(old_val),[new_val] "r"(new_val)
+			     :"memory");
+
+	return (ret);
+
+}
+
+/**
+ * Atomically compares the old value with the value at ptr, and if they match,
+ * stores new_val to ptr.
+ * If *ptr and old don't match, function returns failure immediately.
+ * If *ptr and old match, function spins until *ptr updated to new atomically, or
+ *  until *ptr and old no longer match
+ *
+ * Does memory synchronization that is required to use this as a locking primitive.
+ *
+ * @return 1 on success (match and store)
+ *         0 on no match
+ */
+static inline uint64_t cvmx_atomic_compare_and_store64(uint64_t * ptr, uint64_t old_val, uint64_t new_val)
+{
+	uint64_t ret;
+	CVMX_SYNCWS;
+	ret = cvmx_atomic_compare_and_store64_nosync(ptr, old_val, new_val);
+	CVMX_SYNCWS;
+	return ret;
+}
+
+/**
+ * Atomically adds a signed value to a 64 bit (aligned) memory location,
+ * and returns previous value.
+ *
+ * This version does not perform 'sync' operations to enforce memory
+ * operations.  This should only be used when there are no memory operation
+ * ordering constraints.  (This should NOT be used for reference counting -
+ * use the standard version instead.)
+ *
+ * @param ptr    address in memory to add incr to
+ * @param incr   amount to increment memory location by (signed)
+ *
+ * @return Value of memory location before increment
+ */
+static inline int64_t cvmx_atomic_fetch_and_add64_nosync(int64_t * ptr, int64_t incr)
+{
+	uint64_t ret;
+
+#ifdef CVMX_CAVIUM_OCTEON2
+	{
+		CVMX_PUSH_OCTEON2;
+		if (__builtin_constant_p(incr) && incr == 1) {
+			__asm__ __volatile__("laid  %0,(%2)":"=r"(ret), "+m"(ptr):"r"(ptr):"memory");
+		} else if (__builtin_constant_p(incr) && incr == -1) {
+			__asm__ __volatile__("ladd  %0,(%2)":"=r"(ret), "+m"(ptr):"r"(ptr):"memory");
+		} else {
+			__asm__ __volatile__("laad  %0,(%2),%3":"=r"(ret), "+m"(ptr):"r"(ptr), "r"(incr):"memory");
+		}
+		CVMX_POP_OCTEON2;
+	}
+#else
+	{
+		uint64_t tmp;
+		__asm__ __volatile__(".set noreorder          \n"
+				     "1: lld   %[tmp], %[val] \n"
+				     "   move  %[ret], %[tmp] \n"
+				     "   daddu %[tmp], %[inc] \n"
+				     "   scd   %[tmp], %[val] \n"
+				     "   beqz  %[tmp], 1b     \n" "   nop                  \n" ".set reorder            \n":[val] "+m"(*ptr),[tmp] "=&r"(tmp),[ret] "=&r"(ret)
+				     :[inc] "r"(incr)
+				     :"memory");
+	}
+#endif
+
+	return (ret);
+}
+
+/**
+ * Atomically adds a signed value to a 64 bit (aligned) memory location,
+ * and returns previous value.
+ *
+ * Memory access ordering is enforced before/after the atomic operation,
+ * so no additional 'sync' instructions are required.
+ *
+ * @param ptr    address in memory to add incr to
+ * @param incr   amount to increment memory location by (signed)
+ *
+ * @return Value of memory location before increment
+ */
+static inline int64_t cvmx_atomic_fetch_and_add64(int64_t * ptr, int64_t incr)
+{
+	uint64_t ret;
+	CVMX_SYNCWS;
+	ret = cvmx_atomic_fetch_and_add64_nosync(ptr, incr);
+	CVMX_SYNCWS;
+	return ret;
+}
+
+/**
+ * Atomically adds a signed value to a 32 bit (aligned) memory location,
+ * and returns previous value.
+ *
+ * This version does not perform 'sync' operations to enforce memory
+ * operations.  This should only be used when there are no memory operation
+ * ordering constraints.  (This should NOT be used for reference counting -
+ * use the standard version instead.)
+ *
+ * @param ptr    address in memory to add incr to
+ * @param incr   amount to increment memory location by (signed)
+ *
+ * @return Value of memory location before increment
+ */
+static inline int32_t cvmx_atomic_fetch_and_add32_nosync(int32_t * ptr, int32_t incr)
+{
+	uint32_t ret;
+
+#ifdef CVMX_CAVIUM_OCTEON2
+	{
+		CVMX_PUSH_OCTEON2;
+		if (__builtin_constant_p(incr) && incr == 1) {
+			__asm__ __volatile__("lai  %0,(%2)":"=r"(ret), "+m"(ptr):"r"(ptr):"memory");
+		} else if (__builtin_constant_p(incr) && incr == -1) {
+			__asm__ __volatile__("lad  %0,(%2)":"=r"(ret), "+m"(ptr):"r"(ptr):"memory");
+		} else {
+			__asm__ __volatile__("laa  %0,(%2),%3":"=r"(ret), "+m"(ptr):"r"(ptr), "r"(incr):"memory");
+		}
+		CVMX_POP_OCTEON2;
+	}
+#else
+	{
+		uint32_t tmp;
+
+		__asm__ __volatile__(".set noreorder         \n"
+				     "1: ll   %[tmp], %[val] \n"
+				     "   move %[ret], %[tmp] \n"
+				     "   addu %[tmp], %[inc] \n"
+				     "   sc   %[tmp], %[val] \n"
+				     "   beqz %[tmp], 1b     \n" "   nop                 \n" ".set reorder           \n":[val] "+m"(*ptr),[tmp] "=&r"(tmp),[ret] "=&r"(ret)
+				     :[inc] "r"(incr)
+				     :"memory");
+	}
+#endif
+
+	return (ret);
+}
+
+/**
+ * Atomically adds a signed value to a 32 bit (aligned) memory location,
+ * and returns previous value.
+ *
+ * Memory access ordering is enforced before/after the atomic operation,
+ * so no additional 'sync' instructions are required.
+ *
+ * @param ptr    address in memory to add incr to
+ * @param incr   amount to increment memory location by (signed)
+ *
+ * @return Value of memory location before increment
+ */
+static inline int32_t cvmx_atomic_fetch_and_add32(int32_t * ptr, int32_t incr)
+{
+	uint32_t ret;
+	CVMX_SYNCWS;
+	ret = cvmx_atomic_fetch_and_add32_nosync(ptr, incr);
+	CVMX_SYNCWS;
+	return ret;
+}
+
+/**
+ * Atomically set bits in a 64 bit (aligned) memory location,
+ * and returns previous value.
+ *
+ * This version does not perform 'sync' operations to enforce memory
+ * operations.  This should only be used when there are no memory operation
+ * ordering constraints.
+ *
+ * @param ptr    address in memory
+ * @param mask   mask of bits to set
+ *
+ * @return Value of memory location before setting bits
+ */
+static inline uint64_t cvmx_atomic_fetch_and_bset64_nosync(uint64_t * ptr, uint64_t mask)
+{
+	uint64_t tmp, ret;
+
+	__asm__ __volatile__(".set noreorder         \n"
+			     "1: lld  %[tmp], %[val] \n"
+			     "   move %[ret], %[tmp] \n"
+			     "   or   %[tmp], %[msk] \n"
+			     "   scd  %[tmp], %[val] \n"
+			     "   beqz %[tmp], 1b     \n" "   nop                 \n" ".set reorder           \n":[val] "+m"(*ptr),[tmp] "=&r"(tmp),[ret] "=&r"(ret)
+			     :[msk] "r"(mask)
+			     :"memory");
+
+	return (ret);
+}
+
+/**
+ * Atomically set bits in a 32 bit (aligned) memory location,
+ * and returns previous value.
+ *
+ * This version does not perform 'sync' operations to enforce memory
+ * operations.  This should only be used when there are no memory operation
+ * ordering constraints.
+ *
+ * @param ptr    address in memory
+ * @param mask   mask of bits to set
+ *
+ * @return Value of memory location before setting bits
+ */
+static inline uint32_t cvmx_atomic_fetch_and_bset32_nosync(uint32_t * ptr, uint32_t mask)
+{
+	uint32_t tmp, ret;
+
+	__asm__ __volatile__(".set noreorder         \n"
+			     "1: ll   %[tmp], %[val] \n"
+			     "   move %[ret], %[tmp] \n"
+			     "   or   %[tmp], %[msk] \n"
+			     "   sc   %[tmp], %[val] \n"
+			     "   beqz %[tmp], 1b     \n" "   nop                 \n" ".set reorder           \n":[val] "+m"(*ptr),[tmp] "=&r"(tmp),[ret] "=&r"(ret)
+			     :[msk] "r"(mask)
+			     :"memory");
+
+	return (ret);
+}
+
+/**
+ * Atomically clear bits in a 64 bit (aligned) memory location,
+ * and returns previous value.
+ *
+ * This version does not perform 'sync' operations to enforce memory
+ * operations.  This should only be used when there are no memory operation
+ * ordering constraints.
+ *
+ * @param ptr    address in memory
+ * @param mask   mask of bits to clear
+ *
+ * @return Value of memory location before clearing bits
+ */
+static inline uint64_t cvmx_atomic_fetch_and_bclr64_nosync(uint64_t * ptr, uint64_t mask)
+{
+	uint64_t tmp, ret;
+
+	__asm__ __volatile__(".set noreorder         \n"
+			     "   nor  %[msk], 0      \n"
+			     "1: lld  %[tmp], %[val] \n"
+			     "   move %[ret], %[tmp] \n"
+			     "   and  %[tmp], %[msk] \n"
+			     "   scd  %[tmp], %[val] \n"
+			     "   beqz %[tmp], 1b     \n" "   nop                 \n" ".set reorder           \n":[val] "+m"(*ptr),[tmp] "=&r"(tmp),[ret] "=&r"(ret),[msk] "+r"(mask)
+			     ::"memory");
+
+	return (ret);
+}
+
+/**
+ * Atomically clear bits in a 32 bit (aligned) memory location,
+ * and returns previous value.
+ *
+ * This version does not perform 'sync' operations to enforce memory
+ * operations.  This should only be used when there are no memory operation
+ * ordering constraints.
+ *
+ * @param ptr    address in memory
+ * @param mask   mask of bits to clear
+ *
+ * @return Value of memory location before clearing bits
+ */
+static inline uint32_t cvmx_atomic_fetch_and_bclr32_nosync(uint32_t * ptr, uint32_t mask)
+{
+	uint32_t tmp, ret;
+
+	__asm__ __volatile__(".set noreorder         \n"
+			     "   nor  %[msk], 0      \n"
+			     "1: ll   %[tmp], %[val] \n"
+			     "   move %[ret], %[tmp] \n"
+			     "   and  %[tmp], %[msk] \n"
+			     "   sc   %[tmp], %[val] \n"
+			     "   beqz %[tmp], 1b     \n" "   nop                 \n" ".set reorder           \n":[val] "+m"(*ptr),[tmp] "=&r"(tmp),[ret] "=&r"(ret),[msk] "+r"(mask)
+			     ::"memory");
+
+	return (ret);
+}
+
+/**
+ * Atomically swaps value in 64 bit (aligned) memory location,
+ * and returns previous value.
+ *
+ * This version does not perform 'sync' operations to enforce memory
+ * operations.  This should only be used when there are no memory operation
+ * ordering constraints.
+ *
+ * @param ptr       address in memory
+ * @param new_val   new value to write
+ *
+ * @return Value of memory location before swap operation
+ */
+static inline uint64_t cvmx_atomic_swap64_nosync(uint64_t * ptr, uint64_t new_val)
+{
+	uint64_t ret;
+
+#ifdef CVMX_CAVIUM_OCTEON2
+	{
+		CVMX_PUSH_OCTEON2;
+		if (__builtin_constant_p(new_val) && new_val == 0) {
+			__asm__ __volatile__("lacd  %0,(%1)":"=r"(ret):"r"(ptr):"memory");
+		} else if (__builtin_constant_p(new_val) && new_val == ~0ull) {
+			__asm__ __volatile__("lasd  %0,(%1)":"=r"(ret):"r"(ptr):"memory");
+		} else {
+			__asm__ __volatile__("lawd  %0,(%1),%2":"=r"(ret):"r"(ptr), "r"(new_val):"memory");
+		}
+		CVMX_POP_OCTEON2;
+	}
+#else
+	{
+		uint64_t tmp;
+		__asm__ __volatile__(".set noreorder         \n"
+				     "1: lld  %[ret], %[val] \n"
+				     "   move %[tmp], %[new_val] \n"
+				     "   scd  %[tmp], %[val] \n"
+				     "   beqz %[tmp],  1b    \n" "   nop                 \n" ".set reorder           \n":[val] "+m"(*ptr),[tmp] "=&r"(tmp),[ret] "=&r"(ret)
+				     :[new_val] "r"(new_val)
+				     :"memory");
+	}
+#endif
+
+	return (ret);
+}
+
+/**
+ * Atomically swaps value in 32 bit (aligned) memory location,
+ * and returns previous value.
+ *
+ * This version does not perform 'sync' operations to enforce memory
+ * operations.  This should only be used when there are no memory operation
+ * ordering constraints.
+ *
+ * @param ptr       address in memory
+ * @param new_val   new value to write
+ *
+ * @return Value of memory location before swap operation
+ */
+static inline uint32_t cvmx_atomic_swap32_nosync(uint32_t * ptr, uint32_t new_val)
+{
+	uint32_t ret;
+
+#ifdef CVMX_CAVIUM_OCTEON2
+	{
+		CVMX_PUSH_OCTEON2;
+		if (__builtin_constant_p(new_val) && new_val == 0) {
+			__asm__ __volatile__("lac  %0,(%1)":"=r"(ret):"r"(ptr):"memory");
+		} else if (__builtin_constant_p(new_val) && new_val == ~0u) {
+			__asm__ __volatile__("las  %0,(%1)":"=r"(ret):"r"(ptr):"memory");
+		} else {
+			__asm__ __volatile__("law  %0,(%1),%2":"=r"(ret):"r"(ptr), "r"(new_val):"memory");
+		}
+		CVMX_POP_OCTEON2;
+	}
+#else
+	{
+		uint32_t tmp;
+
+		__asm__ __volatile__(".set noreorder         \n"
+				     "1: ll   %[ret], %[val] \n"
+				     "   move %[tmp], %[new_val] \n"
+				     "   sc   %[tmp], %[val] \n"
+				     "   beqz %[tmp],  1b    \n" "   nop                 \n" ".set reorder           \n":[val] "+m"(*ptr),[tmp] "=&r"(tmp),[ret] "=&r"(ret)
+				     :[new_val] "r"(new_val)
+				     :"memory");
+	}
+#endif
+
+	return (ret);
+}
+
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+}
+/* *INDENT-ON* */
+#endif
+
+#endif /* __CVMX_ATOMIC_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-bgx.h b/arch/mips/include/asm/octeon/cvmx-bgx.h
new file mode 100644
index 0000000..71ab334
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-bgx.h
@@ -0,0 +1,64 @@
+/***********************license start***************
+ * Copyright (c) 2013  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Functions to configure the BGX MAC.
+ *
+ * <hr>$Revision$<hr>
+ */
+
+#ifndef __CVMX_BGX_H__
+#define __CVMX_BGX_H__
+
+
+/**
+ * @INTERNAL
+ * Configure the bgx mac.
+ *
+ * @param interface Interface to bring up
+ *
+ * @param mode      Mode to configure the bgx mac as
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int bgx_init(int interface, cvmx_helper_interface_mode_t mode);
+
+#endif /* __CVMX_BGX_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-bgxx-defs.h b/arch/mips/include/asm/octeon/cvmx-bgxx-defs.h
new file mode 100644
index 0000000..81c56ab
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-bgxx-defs.h
@@ -0,0 +1,7495 @@
+/***********************license start***************
+ * Copyright (c) 2003-2014  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+
+/**
+ * cvmx-bgxx-defs.h
+ *
+ * Configuration and status register (CSR) type definitions for
+ * Octeon bgxx.
+ *
+ * This file is auto generated. Do not edit.
+ *
+ * <hr>$Revision$<hr>
+ *
+ */
+#ifndef __CVMX_BGXX_DEFS_H__
+#define __CVMX_BGXX_DEFS_H__
+
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_CONFIG(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_CONFIG(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000000ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_CONFIG(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000000ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_INT(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_INT(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000020ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_INT(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000020ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_PRT_CBFC_CTL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_PRT_CBFC_CTL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000408ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_PRT_CBFC_CTL(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000408ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_RX_ADR_CTL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_RX_ADR_CTL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E00000A0ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_RX_ADR_CTL(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E00000A0ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_RX_BP_DROP(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_RX_BP_DROP(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000080ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_RX_BP_DROP(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000080ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_RX_BP_OFF(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_RX_BP_OFF(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000090ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_RX_BP_OFF(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000090ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_RX_BP_ON(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_RX_BP_ON(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000088ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_RX_BP_ON(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000088ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_RX_BP_STATUS(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_RX_BP_STATUS(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E00000A8ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_RX_BP_STATUS(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E00000A8ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_RX_FIFO_LEN(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_RX_FIFO_LEN(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E00000C0ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_RX_FIFO_LEN(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E00000C0ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_RX_ID_MAP(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_RX_ID_MAP(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000028ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_RX_ID_MAP(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000028ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_RX_LOGL_XOFF(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_RX_LOGL_XOFF(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E00000B0ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_RX_LOGL_XOFF(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E00000B0ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_RX_LOGL_XON(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_RX_LOGL_XON(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E00000B8ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_RX_LOGL_XON(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E00000B8ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_RX_PAUSE_DROP_TIME(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_RX_PAUSE_DROP_TIME(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000030ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_RX_PAUSE_DROP_TIME(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000030ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_RX_STAT0(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_RX_STAT0(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000038ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_RX_STAT0(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000038ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_RX_STAT1(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_RX_STAT1(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000040ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_RX_STAT1(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000040ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_RX_STAT2(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_RX_STAT2(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000048ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_RX_STAT2(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000048ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_RX_STAT3(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_RX_STAT3(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000050ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_RX_STAT3(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000050ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_RX_STAT4(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_RX_STAT4(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000058ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_RX_STAT4(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000058ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_RX_STAT5(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_RX_STAT5(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000060ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_RX_STAT5(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000060ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_RX_STAT6(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_RX_STAT6(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000068ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_RX_STAT6(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000068ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_RX_STAT7(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_RX_STAT7(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000070ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_RX_STAT7(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000070ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_RX_STAT8(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_RX_STAT8(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000078ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_RX_STAT8(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000078ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_RX_WEIGHT(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_RX_WEIGHT(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000098ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_RX_WEIGHT(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000098ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_TX_CHANNEL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_TX_CHANNEL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000400ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_TX_CHANNEL(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000400ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_TX_FIFO_LEN(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_TX_FIFO_LEN(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000418ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_TX_FIFO_LEN(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000418ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_TX_HG2_STATUS(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_TX_HG2_STATUS(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000410ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_TX_HG2_STATUS(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000410ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_TX_OVR_BP(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_TX_OVR_BP(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000420ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_TX_OVR_BP(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000420ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_TX_STAT0(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_TX_STAT0(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000508ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_TX_STAT0(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000508ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_TX_STAT1(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_TX_STAT1(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000510ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_TX_STAT1(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000510ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_TX_STAT10(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_TX_STAT10(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000558ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_TX_STAT10(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000558ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_TX_STAT11(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_TX_STAT11(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000560ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_TX_STAT11(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000560ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_TX_STAT12(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_TX_STAT12(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000568ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_TX_STAT12(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000568ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_TX_STAT13(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_TX_STAT13(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000570ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_TX_STAT13(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000570ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_TX_STAT14(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_TX_STAT14(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000578ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_TX_STAT14(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000578ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_TX_STAT15(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_TX_STAT15(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000580ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_TX_STAT15(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000580ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_TX_STAT16(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_TX_STAT16(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000588ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_TX_STAT16(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000588ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_TX_STAT17(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_TX_STAT17(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000590ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_TX_STAT17(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000590ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_TX_STAT2(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_TX_STAT2(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000518ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_TX_STAT2(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000518ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_TX_STAT3(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_TX_STAT3(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000520ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_TX_STAT3(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000520ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_TX_STAT4(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_TX_STAT4(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000528ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_TX_STAT4(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000528ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_TX_STAT5(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_TX_STAT5(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000530ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_TX_STAT5(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000530ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_TX_STAT6(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_TX_STAT6(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000538ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_TX_STAT6(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000538ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_TX_STAT7(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_TX_STAT7(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000540ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_TX_STAT7(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000540ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_TX_STAT8(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_TX_STAT8(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000548ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_TX_STAT8(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000548ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMRX_TX_STAT9(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMRX_TX_STAT9(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000550ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_CMRX_TX_STAT9(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000550ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMR_BAD(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 5)))))
+		cvmx_warn("CVMX_BGXX_CMR_BAD(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0001020ull) + ((block_id) & 7) * 0x1000000ull;
+}
+#else
+#define CVMX_BGXX_CMR_BAD(block_id) (CVMX_ADD_IO_SEG(0x00011800E0001020ull) + ((block_id) & 7) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMR_BIST_STATUS(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 5)))))
+		cvmx_warn("CVMX_BGXX_CMR_BIST_STATUS(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000300ull) + ((block_id) & 7) * 0x1000000ull;
+}
+#else
+#define CVMX_BGXX_CMR_BIST_STATUS(block_id) (CVMX_ADD_IO_SEG(0x00011800E0000300ull) + ((block_id) & 7) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMR_CHAN_MSK_AND(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 5)))))
+		cvmx_warn("CVMX_BGXX_CMR_CHAN_MSK_AND(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000200ull) + ((block_id) & 7) * 0x1000000ull;
+}
+#else
+#define CVMX_BGXX_CMR_CHAN_MSK_AND(block_id) (CVMX_ADD_IO_SEG(0x00011800E0000200ull) + ((block_id) & 7) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMR_CHAN_MSK_OR(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 5)))))
+		cvmx_warn("CVMX_BGXX_CMR_CHAN_MSK_OR(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000208ull) + ((block_id) & 7) * 0x1000000ull;
+}
+#else
+#define CVMX_BGXX_CMR_CHAN_MSK_OR(block_id) (CVMX_ADD_IO_SEG(0x00011800E0000208ull) + ((block_id) & 7) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMR_GLOBAL_CONFIG(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 5)))))
+		cvmx_warn("CVMX_BGXX_CMR_GLOBAL_CONFIG(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000008ull) + ((block_id) & 7) * 0x1000000ull;
+}
+#else
+#define CVMX_BGXX_CMR_GLOBAL_CONFIG(block_id) (CVMX_ADD_IO_SEG(0x00011800E0000008ull) + ((block_id) & 7) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMR_MEM_CTRL(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 5)))))
+		cvmx_warn("CVMX_BGXX_CMR_MEM_CTRL(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000018ull) + ((block_id) & 7) * 0x1000000ull;
+}
+#else
+#define CVMX_BGXX_CMR_MEM_CTRL(block_id) (CVMX_ADD_IO_SEG(0x00011800E0000018ull) + ((block_id) & 7) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMR_MEM_INT(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 5)))))
+		cvmx_warn("CVMX_BGXX_CMR_MEM_INT(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000010ull) + ((block_id) & 7) * 0x1000000ull;
+}
+#else
+#define CVMX_BGXX_CMR_MEM_INT(block_id) (CVMX_ADD_IO_SEG(0x00011800E0000010ull) + ((block_id) & 7) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMR_NXC_ADR(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 5)))))
+		cvmx_warn("CVMX_BGXX_CMR_NXC_ADR(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0001018ull) + ((block_id) & 7) * 0x1000000ull;
+}
+#else
+#define CVMX_BGXX_CMR_NXC_ADR(block_id) (CVMX_ADD_IO_SEG(0x00011800E0001018ull) + ((block_id) & 7) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMR_RX_ADRX_CAM(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 31)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_CMR_RX_ADRX_CAM(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000100ull) + (((offset) & 31) + ((block_id) & 7) * 0x200000ull) * 8;
+}
+#else
+#define CVMX_BGXX_CMR_RX_ADRX_CAM(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0000100ull) + (((offset) & 31) + ((block_id) & 7) * 0x200000ull) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMR_RX_LMACS(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 5)))))
+		cvmx_warn("CVMX_BGXX_CMR_RX_LMACS(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000308ull) + ((block_id) & 7) * 0x1000000ull;
+}
+#else
+#define CVMX_BGXX_CMR_RX_LMACS(block_id) (CVMX_ADD_IO_SEG(0x00011800E0000308ull) + ((block_id) & 7) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMR_RX_OVR_BP(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 5)))))
+		cvmx_warn("CVMX_BGXX_CMR_RX_OVR_BP(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0000318ull) + ((block_id) & 7) * 0x1000000ull;
+}
+#else
+#define CVMX_BGXX_CMR_RX_OVR_BP(block_id) (CVMX_ADD_IO_SEG(0x00011800E0000318ull) + ((block_id) & 7) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_CMR_TX_LMACS(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 5)))))
+		cvmx_warn("CVMX_BGXX_CMR_TX_LMACS(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0001000ull) + ((block_id) & 7) * 0x1000000ull;
+}
+#else
+#define CVMX_BGXX_CMR_TX_LMACS(block_id) (CVMX_ADD_IO_SEG(0x00011800E0001000ull) + ((block_id) & 7) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_GMI_PRTX_CFG(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_GMI_PRTX_CFG(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0038010ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_GMI_PRTX_CFG(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0038010ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_GMI_RXX_DECISION(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_GMI_RXX_DECISION(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0038040ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_GMI_RXX_DECISION(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0038040ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_GMI_RXX_FRM_CHK(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_GMI_RXX_FRM_CHK(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0038020ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_GMI_RXX_FRM_CHK(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0038020ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_GMI_RXX_FRM_CTL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_GMI_RXX_FRM_CTL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0038018ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_GMI_RXX_FRM_CTL(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0038018ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_GMI_RXX_IFG(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_GMI_RXX_IFG(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0038058ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_GMI_RXX_IFG(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0038058ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_GMI_RXX_INT(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_GMI_RXX_INT(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0038000ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_GMI_RXX_INT(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0038000ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_GMI_RXX_JABBER(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_GMI_RXX_JABBER(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0038038ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_GMI_RXX_JABBER(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0038038ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_GMI_RXX_UDD_SKP(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_GMI_RXX_UDD_SKP(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0038048ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_GMI_RXX_UDD_SKP(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0038048ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_GMI_SMACX(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_GMI_SMACX(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0038230ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_GMI_SMACX(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0038230ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_GMI_TXX_APPEND(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_GMI_TXX_APPEND(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0038218ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_GMI_TXX_APPEND(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0038218ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_GMI_TXX_BURST(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_GMI_TXX_BURST(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0038228ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_GMI_TXX_BURST(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0038228ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_GMI_TXX_CTL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_GMI_TXX_CTL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0038270ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_GMI_TXX_CTL(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0038270ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_GMI_TXX_INT(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_GMI_TXX_INT(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0038500ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_GMI_TXX_INT(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0038500ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_GMI_TXX_MIN_PKT(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_GMI_TXX_MIN_PKT(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0038240ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_GMI_TXX_MIN_PKT(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0038240ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_GMI_TXX_PAUSE_PKT_INTERVAL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_GMI_TXX_PAUSE_PKT_INTERVAL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0038248ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_GMI_TXX_PAUSE_PKT_INTERVAL(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0038248ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_GMI_TXX_PAUSE_PKT_TIME(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_GMI_TXX_PAUSE_PKT_TIME(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0038238ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_GMI_TXX_PAUSE_PKT_TIME(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0038238ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_GMI_TXX_PAUSE_TOGO(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_GMI_TXX_PAUSE_TOGO(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0038258ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_GMI_TXX_PAUSE_TOGO(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0038258ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_GMI_TXX_PAUSE_ZERO(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_GMI_TXX_PAUSE_ZERO(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0038260ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_GMI_TXX_PAUSE_ZERO(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0038260ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_GMI_TXX_SGMII_CTL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_GMI_TXX_SGMII_CTL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0038300ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_GMI_TXX_SGMII_CTL(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0038300ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_GMI_TXX_SLOT(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_GMI_TXX_SLOT(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0038220ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_GMI_TXX_SLOT(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0038220ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_GMI_TXX_SOFT_PAUSE(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_GMI_TXX_SOFT_PAUSE(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0038250ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_GMI_TXX_SOFT_PAUSE(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0038250ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_GMI_TXX_THRESH(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_GMI_TXX_THRESH(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0038210ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_GMI_TXX_THRESH(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0038210ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_GMI_TX_COL_ATTEMPT(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 5)))))
+		cvmx_warn("CVMX_BGXX_GMP_GMI_TX_COL_ATTEMPT(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0039010ull) + ((block_id) & 7) * 0x1000000ull;
+}
+#else
+#define CVMX_BGXX_GMP_GMI_TX_COL_ATTEMPT(block_id) (CVMX_ADD_IO_SEG(0x00011800E0039010ull) + ((block_id) & 7) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_GMI_TX_IFG(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 5)))))
+		cvmx_warn("CVMX_BGXX_GMP_GMI_TX_IFG(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0039000ull) + ((block_id) & 7) * 0x1000000ull;
+}
+#else
+#define CVMX_BGXX_GMP_GMI_TX_IFG(block_id) (CVMX_ADD_IO_SEG(0x00011800E0039000ull) + ((block_id) & 7) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_GMI_TX_JAM(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 5)))))
+		cvmx_warn("CVMX_BGXX_GMP_GMI_TX_JAM(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0039008ull) + ((block_id) & 7) * 0x1000000ull;
+}
+#else
+#define CVMX_BGXX_GMP_GMI_TX_JAM(block_id) (CVMX_ADD_IO_SEG(0x00011800E0039008ull) + ((block_id) & 7) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_GMI_TX_LFSR(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 5)))))
+		cvmx_warn("CVMX_BGXX_GMP_GMI_TX_LFSR(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0039028ull) + ((block_id) & 7) * 0x1000000ull;
+}
+#else
+#define CVMX_BGXX_GMP_GMI_TX_LFSR(block_id) (CVMX_ADD_IO_SEG(0x00011800E0039028ull) + ((block_id) & 7) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_GMI_TX_PAUSE_PKT_DMAC(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 5)))))
+		cvmx_warn("CVMX_BGXX_GMP_GMI_TX_PAUSE_PKT_DMAC(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0039018ull) + ((block_id) & 7) * 0x1000000ull;
+}
+#else
+#define CVMX_BGXX_GMP_GMI_TX_PAUSE_PKT_DMAC(block_id) (CVMX_ADD_IO_SEG(0x00011800E0039018ull) + ((block_id) & 7) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_GMI_TX_PAUSE_PKT_TYPE(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 5)))))
+		cvmx_warn("CVMX_BGXX_GMP_GMI_TX_PAUSE_PKT_TYPE(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0039020ull) + ((block_id) & 7) * 0x1000000ull;
+}
+#else
+#define CVMX_BGXX_GMP_GMI_TX_PAUSE_PKT_TYPE(block_id) (CVMX_ADD_IO_SEG(0x00011800E0039020ull) + ((block_id) & 7) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_PCS_ANX_ADV(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_PCS_ANX_ADV(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0030010ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_PCS_ANX_ADV(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0030010ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_PCS_ANX_EXT_ST(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_PCS_ANX_EXT_ST(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0030028ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_PCS_ANX_EXT_ST(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0030028ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_PCS_ANX_LP_ABIL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_PCS_ANX_LP_ABIL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0030018ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_PCS_ANX_LP_ABIL(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0030018ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_PCS_ANX_RESULTS(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_PCS_ANX_RESULTS(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0030020ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_PCS_ANX_RESULTS(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0030020ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_PCS_INTX(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_PCS_INTX(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0030080ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_PCS_INTX(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0030080ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_PCS_LINKX_TIMER(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_PCS_LINKX_TIMER(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0030040ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_PCS_LINKX_TIMER(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0030040ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_PCS_MISCX_CTL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_PCS_MISCX_CTL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0030078ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_PCS_MISCX_CTL(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0030078ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_PCS_MRX_CONTROL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_PCS_MRX_CONTROL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0030000ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_PCS_MRX_CONTROL(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0030000ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_PCS_MRX_STATUS(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_PCS_MRX_STATUS(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0030008ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_PCS_MRX_STATUS(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0030008ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_PCS_RXX_STATES(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_PCS_RXX_STATES(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0030058ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_PCS_RXX_STATES(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0030058ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_PCS_RXX_SYNC(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_PCS_RXX_SYNC(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0030050ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_PCS_RXX_SYNC(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0030050ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_PCS_SGMX_AN_ADV(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_PCS_SGMX_AN_ADV(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0030068ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_PCS_SGMX_AN_ADV(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0030068ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_PCS_SGMX_LP_ADV(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_PCS_SGMX_LP_ADV(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0030070ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_PCS_SGMX_LP_ADV(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0030070ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_PCS_TXX_STATES(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_PCS_TXX_STATES(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0030060ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_PCS_TXX_STATES(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0030060ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_GMP_PCS_TX_RXX_POLARITY(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_GMP_PCS_TX_RXX_POLARITY(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0030048ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_GMP_PCS_TX_RXX_POLARITY(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0030048ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SMUX_CBFC_CTL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SMUX_CBFC_CTL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0020218ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SMUX_CBFC_CTL(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0020218ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SMUX_CTRL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SMUX_CTRL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0020200ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SMUX_CTRL(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0020200ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SMUX_EXT_LOOPBACK(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SMUX_EXT_LOOPBACK(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0020208ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SMUX_EXT_LOOPBACK(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0020208ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SMUX_HG2_CONTROL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SMUX_HG2_CONTROL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0020210ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SMUX_HG2_CONTROL(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0020210ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SMUX_RX_BAD_COL_HI(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SMUX_RX_BAD_COL_HI(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0020040ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SMUX_RX_BAD_COL_HI(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0020040ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SMUX_RX_BAD_COL_LO(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SMUX_RX_BAD_COL_LO(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0020038ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SMUX_RX_BAD_COL_LO(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0020038ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SMUX_RX_CTL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SMUX_RX_CTL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0020030ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SMUX_RX_CTL(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0020030ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SMUX_RX_DECISION(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SMUX_RX_DECISION(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0020020ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SMUX_RX_DECISION(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0020020ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SMUX_RX_FRM_CHK(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SMUX_RX_FRM_CHK(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0020010ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SMUX_RX_FRM_CHK(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0020010ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SMUX_RX_FRM_CTL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SMUX_RX_FRM_CTL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0020008ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SMUX_RX_FRM_CTL(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0020008ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SMUX_RX_INT(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SMUX_RX_INT(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0020000ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SMUX_RX_INT(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0020000ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SMUX_RX_JABBER(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SMUX_RX_JABBER(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0020018ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SMUX_RX_JABBER(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0020018ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SMUX_RX_UDD_SKP(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SMUX_RX_UDD_SKP(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0020028ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SMUX_RX_UDD_SKP(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0020028ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SMUX_SMAC(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SMUX_SMAC(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0020108ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SMUX_SMAC(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0020108ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SMUX_TX_APPEND(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SMUX_TX_APPEND(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0020100ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SMUX_TX_APPEND(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0020100ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SMUX_TX_CTL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SMUX_TX_CTL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0020160ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SMUX_TX_CTL(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0020160ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SMUX_TX_IFG(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SMUX_TX_IFG(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0020148ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SMUX_TX_IFG(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0020148ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SMUX_TX_INT(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SMUX_TX_INT(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0020140ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SMUX_TX_INT(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0020140ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SMUX_TX_MIN_PKT(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SMUX_TX_MIN_PKT(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0020118ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SMUX_TX_MIN_PKT(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0020118ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SMUX_TX_PAUSE_PKT_DMAC(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SMUX_TX_PAUSE_PKT_DMAC(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0020150ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SMUX_TX_PAUSE_PKT_DMAC(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0020150ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SMUX_TX_PAUSE_PKT_INTERVAL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SMUX_TX_PAUSE_PKT_INTERVAL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0020120ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SMUX_TX_PAUSE_PKT_INTERVAL(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0020120ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SMUX_TX_PAUSE_PKT_TIME(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SMUX_TX_PAUSE_PKT_TIME(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0020110ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SMUX_TX_PAUSE_PKT_TIME(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0020110ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SMUX_TX_PAUSE_PKT_TYPE(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SMUX_TX_PAUSE_PKT_TYPE(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0020158ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SMUX_TX_PAUSE_PKT_TYPE(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0020158ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SMUX_TX_PAUSE_TOGO(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SMUX_TX_PAUSE_TOGO(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0020130ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SMUX_TX_PAUSE_TOGO(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0020130ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SMUX_TX_PAUSE_ZERO(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SMUX_TX_PAUSE_ZERO(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0020138ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SMUX_TX_PAUSE_ZERO(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0020138ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SMUX_TX_SOFT_PAUSE(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SMUX_TX_SOFT_PAUSE(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0020128ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SMUX_TX_SOFT_PAUSE(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0020128ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SMUX_TX_THRESH(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SMUX_TX_THRESH(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0020168ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SMUX_TX_THRESH(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0020168ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_AN_ADV(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_AN_ADV(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E00100D8ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_AN_ADV(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E00100D8ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_AN_BP_STATUS(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_AN_BP_STATUS(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E00100F8ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_AN_BP_STATUS(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E00100F8ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_AN_CONTROL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_AN_CONTROL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E00100C8ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_AN_CONTROL(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E00100C8ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_AN_LP_BASE(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_AN_LP_BASE(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E00100E0ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_AN_LP_BASE(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E00100E0ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_AN_LP_XNP(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_AN_LP_XNP(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E00100F0ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_AN_LP_XNP(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E00100F0ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_AN_STATUS(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_AN_STATUS(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E00100D0ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_AN_STATUS(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E00100D0ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_AN_XNP_TX(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_AN_XNP_TX(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E00100E8ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_AN_XNP_TX(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E00100E8ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_BR_ALGN_STATUS(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_BR_ALGN_STATUS(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0010050ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_BR_ALGN_STATUS(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0010050ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_BR_BIP_ERR_CNT(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_BR_BIP_ERR_CNT(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0010058ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_BR_BIP_ERR_CNT(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0010058ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_BR_LANE_MAP(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_BR_LANE_MAP(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0010060ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_BR_LANE_MAP(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0010060ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_BR_PMD_CONTROL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_BR_PMD_CONTROL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0010068ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_BR_PMD_CONTROL(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0010068ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_BR_PMD_LD_CUP(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_BR_PMD_LD_CUP(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0010088ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_BR_PMD_LD_CUP(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0010088ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_BR_PMD_LD_REP(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_BR_PMD_LD_REP(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0010090ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_BR_PMD_LD_REP(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0010090ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_BR_PMD_LP_CUP(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_BR_PMD_LP_CUP(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0010078ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_BR_PMD_LP_CUP(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0010078ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_BR_PMD_LP_REP(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_BR_PMD_LP_REP(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0010080ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_BR_PMD_LP_REP(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0010080ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_BR_PMD_STATUS(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_BR_PMD_STATUS(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0010070ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_BR_PMD_STATUS(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0010070ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_BR_STATUS1(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_BR_STATUS1(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0010030ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_BR_STATUS1(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0010030ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_BR_STATUS2(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_BR_STATUS2(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0010038ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_BR_STATUS2(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0010038ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_BR_TP_CONTROL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_BR_TP_CONTROL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0010040ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_BR_TP_CONTROL(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0010040ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_BR_TP_ERR_CNT(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_BR_TP_ERR_CNT(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0010048ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_BR_TP_ERR_CNT(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0010048ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_BX_STATUS(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_BX_STATUS(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0010028ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_BX_STATUS(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0010028ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_CONTROL1(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_CONTROL1(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0010000ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_CONTROL1(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0010000ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_CONTROL2(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_CONTROL2(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0010018ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_CONTROL2(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0010018ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_FEC_ABIL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_FEC_ABIL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0010098ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_FEC_ABIL(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0010098ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_FEC_CONTROL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_FEC_CONTROL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E00100A0ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_FEC_CONTROL(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E00100A0ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_FEC_CORR_BLKS01(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_FEC_CORR_BLKS01(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E00100A8ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_FEC_CORR_BLKS01(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E00100A8ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_FEC_CORR_BLKS23(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_FEC_CORR_BLKS23(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E00100B0ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_FEC_CORR_BLKS23(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E00100B0ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_FEC_UNCORR_BLKS01(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_FEC_UNCORR_BLKS01(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E00100B8ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_FEC_UNCORR_BLKS01(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E00100B8ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_FEC_UNCORR_BLKS23(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_FEC_UNCORR_BLKS23(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E00100C0ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_FEC_UNCORR_BLKS23(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E00100C0ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_INT(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_INT(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0010220ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_INT(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0010220ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_LPCS_STATES(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_LPCS_STATES(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0010208ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_LPCS_STATES(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0010208ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_MISC_CONTROL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_MISC_CONTROL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0010218ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_MISC_CONTROL(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0010218ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_SPD_ABIL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_SPD_ABIL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0010010ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_SPD_ABIL(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0010010ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_STATUS1(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_STATUS1(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0010008ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_STATUS1(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0010008ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPUX_STATUS2(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPUX_STATUS2(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0010020ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_BGXX_SPUX_STATUS2(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0010020ull) + (((offset) & 3) + ((block_id) & 7) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPU_BIST_STATUS(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 5)))))
+		cvmx_warn("CVMX_BGXX_SPU_BIST_STATUS(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0010318ull) + ((block_id) & 7) * 0x1000000ull;
+}
+#else
+#define CVMX_BGXX_SPU_BIST_STATUS(block_id) (CVMX_ADD_IO_SEG(0x00011800E0010318ull) + ((block_id) & 7) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPU_DBG_CONTROL(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 5)))))
+		cvmx_warn("CVMX_BGXX_SPU_DBG_CONTROL(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0010300ull) + ((block_id) & 7) * 0x1000000ull;
+}
+#else
+#define CVMX_BGXX_SPU_DBG_CONTROL(block_id) (CVMX_ADD_IO_SEG(0x00011800E0010300ull) + ((block_id) & 7) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPU_MEM_INT(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 5)))))
+		cvmx_warn("CVMX_BGXX_SPU_MEM_INT(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0010310ull) + ((block_id) & 7) * 0x1000000ull;
+}
+#else
+#define CVMX_BGXX_SPU_MEM_INT(block_id) (CVMX_ADD_IO_SEG(0x00011800E0010310ull) + ((block_id) & 7) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPU_MEM_STATUS(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 5)))))
+		cvmx_warn("CVMX_BGXX_SPU_MEM_STATUS(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0010308ull) + ((block_id) & 7) * 0x1000000ull;
+}
+#else
+#define CVMX_BGXX_SPU_MEM_STATUS(block_id) (CVMX_ADD_IO_SEG(0x00011800E0010308ull) + ((block_id) & 7) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPU_SDSX_SKEW_STATUS(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPU_SDSX_SKEW_STATUS(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0010320ull) + (((offset) & 3) + ((block_id) & 7) * 0x200000ull) * 8;
+}
+#else
+#define CVMX_BGXX_SPU_SDSX_SKEW_STATUS(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0010320ull) + (((offset) & 3) + ((block_id) & 7) * 0x200000ull) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_BGXX_SPU_SDSX_STATES(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 5))))))
+		cvmx_warn("CVMX_BGXX_SPU_SDSX_STATES(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800E0010340ull) + (((offset) & 3) + ((block_id) & 7) * 0x200000ull) * 8;
+}
+#else
+#define CVMX_BGXX_SPU_SDSX_STATES(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800E0010340ull) + (((offset) & 3) + ((block_id) & 7) * 0x200000ull) * 8)
+#endif
+
+/**
+ * cvmx_bgx#_cmr#_config
+ *
+ * Logical MAC/PCS configuration registers; one per LMAC. The maximum number of LMACs (and
+ * maximum LMAC ID) that can be enabled by these registers is limited by
+ * BGX()_CMR_RX_LMACS[LMACS] and BGX()_CMR_TX_LMACS[LMACS]. When multiple LMACs are
+ * enabled, they must be configured with the same [LMAC_TYPE] value.
+ *
+ * INTERNAL:
+ * <pre>
+ * Typical configurations:
+ *   ---------------------------------------------------------------------------
+ *   Configuration           LMACS  Register             [ENABLE]    [LMAC_TYPE]
+ *   ---------------------------------------------------------------------------
+ *   1x40GBASE-R4            1      BGXn_CMR0_CONFIG     1           4
+ *                                  BGXn_CMR1_CONFIG     0           --
+ *                                  BGXn_CMR2_CONFIG     0           --
+ *                                  BGXn_CMR3_CONFIG     0           --
+ *   ---------------------------------------------------------------------------
+ *   4x10GBASE-R             4      BGXn_CMR0_CONFIG     1           3
+ *                                  BGXn_CMR1_CONFIG     1           3
+ *                                  BGXn_CMR2_CONFIG     1           3
+ *                                  BGXn_CMR3_CONFIG     1           3
+ *   ---------------------------------------------------------------------------
+ *   2xRXAUI                 2      BGXn_CMR0_CONFIG     1           2
+ *                                  BGXn_CMR1_CONFIG     1           2
+ *                                  BGXn_CMR2_CONFIG     0           --
+ *                                  BGXn_CMR3_CONFIG     0           --
+ *   ---------------------------------------------------------------------------
+ *   1x10GBASE-X/XAUI/DXAUI  1      BGXn_CMR0_CONFIG     1           1
+ *                                  BGXn_CMR1_CONFIG     0           --
+ *                                  BGXn_CMR2_CONFIG     0           --
+ *                                  BGXn_CMR3_CONFIG     0           --
+ *   ---------------------------------------------------------------------------
+ *   4xSGMII/1000BASE-X      4      BGXn_CMR0_CONFIG     1           0
+ *                                  BGXn_CMR1_CONFIG     1           0
+ *                                  BGXn_CMR2_CONFIG     1           0
+ *                                  BGXn_CMR3_CONFIG     1           0
+ *   ---------------------------------------------------------------------------
+ * </pre>
+ */
+union cvmx_bgxx_cmrx_config {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_config_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t enable                       : 1;  /**< Logical MAC/PCS enable. This is the master enable for the LMAC. When clear, all the
+                                                         dedicated BGX context state for the LMAC (state machines, FIFOs, counters, etc.) is reset,
+                                                         and LMAC access to shared BGX resources (SMU/SPU data path, SerDes lanes) is disabled.
+                                                         When set, LMAC operation is enabled, including link bring-up, synchronization, and
+                                                         transmit/receive of idles and fault sequences. Note that configuration registers for an
+                                                         LMAC are not reset when this bit is clear, allowing software to program them before
+                                                         setting this bit to enable the LMAC. This bit together with the LMAC_TYPE is also used to
+                                                         enable the clocking to the GMP and/or blocks of the Super path (SMU and SPU). CMR clocking
+                                                         is enabled when any of the paths are enabled. */
+	uint64_t data_pkt_rx_en               : 1;  /**< Data packet receive enable. When ENABLE=1 and DATA_PKT_RX_EN=1, the reception of data
+                                                         packets is enabled in the MAC layer. When ENABLE=1 and DATA_PKT_RX_EN=0, the MAC layer
+                                                         drops received data and flow control packets. */
+	uint64_t data_pkt_tx_en               : 1;  /**< Data packet transmit enable. When ENABLE=1 and DATA_PKT_TX_EN=1, the transmission of data
+                                                         packets is enabled in the MAC layer. When ENABLE=1 and DATA_PKT_TX_EN=0, the MAC layer
+                                                         suppresses the transmission of new data and packets for the LMAC. */
+	uint64_t int_beat_gen                 : 1;  /**< Internal beat generation. This bit is used for debug/test purposes and should be clear
+                                                         during normal operation. When set, the LMAC's PCS layer ignores RXVALID and
+                                                         TXREADY/TXCREDIT from the associated SerDes lane(s), internally generates fake (idle)
+                                                         RXVALID and TXCREDIT pulses, and suppresses transmission to the SerDes. */
+	uint64_t mix_en                       : 1;  /**< Management enable. This bit is used by LMACs 0 and 1 only, and should be kept clear for
+                                                         LMACs 2 and 3. Setting it will pipe the LMAC to and from the MIX interface (LMAC0 to/from
+                                                         MIX0, LMAC1 to/from MIX1). LMAC_TYPE must be 0 (SGMII) then this bit is set. Note that at
+                                                         most one BGX can be attached to each of MIX0 and MIX1, i.e. at most one
+                                                         BGX(0..5)_CMR(0)_CONFIG[MIX_EN] bit and one BGX(0..5)_CMR(1)_CONFIG[MIX_EN] bit can be
+                                                         set.
+                                                         This field must be programmed to its final value before [ENABLE] is set, and must not
+                                                         be changed when [ENABLE]=1. */
+	uint64_t lmac_type                    : 3;  /**< Logical MAC/PCS/prt type:
+                                                         <pre>
+                                                           LMAC_TYPE  Name       Description            NUM_PCS_LANES
+                                                           ----------------------------------------------------------
+                                                           0x0        SGMII      SGMII/1000BASE-X             1
+                                                           0x1        XAUI       10GBASE-X/XAUI or DXAUI      4
+                                                           0x2        RXAUI      Reduced XAUI                 2
+                                                           0x3        10G_R      10GBASE-R                    1
+                                                           0x4        40G_R      40GBASE-R                    4
+                                                           Other      --         Reserved                     -
+                                                         </pre>
+                                                         NUM_PCS_LANES specifies the number of PCS lanes that are valid for
+                                                         each type. Each valid PCS lane is mapped to a physical SerDes lane
+                                                         based on the programming of [LANE_TO_SDS].
+                                                         This field must be programmed to its final value before [ENABLE] is set, and must not
+                                                         be changed when [ENABLE]=1. */
+	uint64_t lane_to_sds                  : 8;  /**< PCS lane-to-SerDes Mapping.
+                                                         This is an array of 2-bit values that map each logical PCS Lane to a
+                                                         physical SerDes lane, as follows:
+                                                         <pre>
+                                                           Bits    Description            Reset value
+                                                           ------------------------------------------
+                                                           <7:6>   PCS Lane 3 SerDes ID       0x3
+                                                           <5:4>   PCS Lane 2 SerDes ID       0x2
+                                                           <3:2>   PCS Lane 1 SerDes ID       0x1
+                                                           <1:0>   PCS Lane 0 SerDes ID       0x0
+                                                         </pre>
+                                                         PCS lanes 0 through NUM_PCS_LANES-1 are valid, where NUM_PCS_LANES is a function of the
+                                                         logical MAC/PCS type. (See definition of LMAC_TYPE.) For example, when LMAC_TYPE = RXAUI,
+                                                         then NUM_PCS_LANES = 2, PCS lanes 0 and 1 valid and the associated physical SerDes lanes
+                                                         are selected by bits <1:0> and <3:2>, respectively.
+                                                         For 40GBASE-R (LMAC_TYPE = 40G_R), all four PCS lanes are valid, and the PCS lane IDs
+                                                         determine the block distribution order and associated alignment markers on the transmit
+                                                         side. This is not necessarily the order in which PCS lanes receive data because 802.3
+                                                         allows multilane BASE-R receive lanes to be reordered. When a lane (called service
+                                                         interface in 802.3ba-2010) has achieved alignment marker lock on the receive side (i.e.
+                                                         the associated BGX()_SPU()_BR_ALGN_STATUS[MARKER_LOCK] = 1), then the actual
+                                                         detected RX PCS lane number is recorded in the corresponding
+                                                         BGX()_SPU()_BR_LANE_MAP[LNx_MAPPING].
+                                                         This field must be programmed to its final value before [ENABLE] is set, and must not
+                                                         be changed when [ENABLE]=1. */
+#else
+	uint64_t lane_to_sds                  : 8;
+	uint64_t lmac_type                    : 3;
+	uint64_t mix_en                       : 1;
+	uint64_t int_beat_gen                 : 1;
+	uint64_t data_pkt_tx_en               : 1;
+	uint64_t data_pkt_rx_en               : 1;
+	uint64_t enable                       : 1;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_config_s        cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_config cvmx_bgxx_cmrx_config_t;
+
+/**
+ * cvmx_bgx#_cmr#_int
+ */
+union cvmx_bgxx_cmrx_int {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_int_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_3_63                : 61;
+	uint64_t pko_nxc                      : 1;  /**< TX channel out-of-range from PKO interface. Assigned to the LMAC ID based on the lower 2
+                                                         bits of the offending channel. */
+	uint64_t overflw                      : 1;  /**< RX overflow. */
+	uint64_t pause_drp                    : 1;  /**< RX PAUSE packet was dropped due to full RXB FIFO or during partner reset. */
+#else
+	uint64_t pause_drp                    : 1;
+	uint64_t overflw                      : 1;
+	uint64_t pko_nxc                      : 1;
+	uint64_t reserved_3_63                : 61;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_int_s           cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_int cvmx_bgxx_cmrx_int_t;
+
+/**
+ * cvmx_bgx#_cmr#_prt_cbfc_ctl
+ *
+ * See XOFF definition listed under BGX()_SMU()_CBFC_CTL.
+ *
+ */
+union cvmx_bgxx_cmrx_prt_cbfc_ctl {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_prt_cbfc_ctl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_32_63               : 32;
+	uint64_t phys_bp                      : 16; /**< When BGX()_SMU()_CBFC_CTL[RX_EN] is set and the hardware is backpressuring any
+                                                         LMACs (from either PFC/CBFC PAUSE packets or BGX()_CMR()_TX_OVR_BP[TX_CHAN_BP])
+                                                         and all LMACs indicated by PHYS_BP are backpressured, simulate physical backpressure by
+                                                         deferring all packets on the transmitter. */
+	uint64_t reserved_0_15                : 16;
+#else
+	uint64_t reserved_0_15                : 16;
+	uint64_t phys_bp                      : 16;
+	uint64_t reserved_32_63               : 32;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_prt_cbfc_ctl_s  cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_prt_cbfc_ctl cvmx_bgxx_cmrx_prt_cbfc_ctl_t;
+
+/**
+ * cvmx_bgx#_cmr#_rx_adr_ctl
+ */
+union cvmx_bgxx_cmrx_rx_adr_ctl {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_rx_adr_ctl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t cam_accept                   : 1;  /**< Allow or deny DMAC address filter.
+                                                         0 = Reject the packet on DMAC CAM address match.
+                                                         1 = Accept the packet on DMAC CAM address match. */
+	uint64_t mcst_mode                    : 2;  /**< Multicast mode.
+                                                         0x0 = Force reject all multicast packets.
+                                                         0x1 = Force accept all multicast packets.
+                                                         0x2 = Use the address filter CAM.
+                                                         0x3 = Reserved. */
+	uint64_t bcst_accept                  : 1;  /**< Allow or deny broadcast packets.
+                                                         0 = Reject all broadcast packets.
+                                                         1 = Accept all broadcast Packets. */
+#else
+	uint64_t bcst_accept                  : 1;
+	uint64_t mcst_mode                    : 2;
+	uint64_t cam_accept                   : 1;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_rx_adr_ctl_s    cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_rx_adr_ctl cvmx_bgxx_cmrx_rx_adr_ctl_t;
+
+/**
+ * cvmx_bgx#_cmr#_rx_bp_drop
+ */
+union cvmx_bgxx_cmrx_rx_bp_drop {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_rx_bp_drop_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_7_63                : 57;
+	uint64_t mark                         : 7;  /**< Number of eight-byte cycles to reserve in the RX FIFO. When the number of free
+                                                         entries in the RX FIFO is less than or equal to MARK, incoming packet data is
+                                                         dropped. Mark additionally indicates the number of entries to reserve in the RX FIFO for
+                                                         closing partially received packets. MARK should typically be programmed to its reset
+                                                         value; failure to program correctly can lead to system instability. */
+#else
+	uint64_t mark                         : 7;
+	uint64_t reserved_7_63                : 57;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_rx_bp_drop_s    cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_rx_bp_drop cvmx_bgxx_cmrx_rx_bp_drop_t;
+
+/**
+ * cvmx_bgx#_cmr#_rx_bp_off
+ */
+union cvmx_bgxx_cmrx_rx_bp_off {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_rx_bp_off_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_7_63                : 57;
+	uint64_t mark                         : 7;  /**< Low watermark (number of eight-byte cycles to deassert backpressure). Level is also used
+                                                         to exit the overflow dropping state. */
+#else
+	uint64_t mark                         : 7;
+	uint64_t reserved_7_63                : 57;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_rx_bp_off_s     cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_rx_bp_off cvmx_bgxx_cmrx_rx_bp_off_t;
+
+/**
+ * cvmx_bgx#_cmr#_rx_bp_on
+ */
+union cvmx_bgxx_cmrx_rx_bp_on {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_rx_bp_on_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_12_63               : 52;
+	uint64_t mark                         : 12; /**< High watermark. Buffer depth in multiple of 16-bytes, at which BGX will
+                                                         assert backpressure for each individual LMAC.  MARK must satisfy:
+                                                           BGX()_CMR()_RX_BP_OFF[MARK] <= MARK <
+                                                           (FIFO_SIZE - BGX()_CMR()_RX_BP_DROP[MARK]).
+                                                         A value of 0x0 immediately asserts backpressure.
+                                                         The recommended value is 1/4th the size of the per-LMAC RX FIFO_SIZE as
+                                                         determined by BGX()_CMR_RX_LMACS[LMACS]. For example in SGMII mode with
+                                                         four LMACs of type SGMII, where BGX()_CMR_RX_LMACS[LMACS]=0x4, there is
+                                                         16 KB of buffering. The recommended 1/4th size of that 16 KB is 4 KB, which
+                                                         in units of 16 bytes gives MARK = 0x100 (the reset value). */
+#else
+	uint64_t mark                         : 12;
+	uint64_t reserved_12_63               : 52;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_rx_bp_on_s      cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_rx_bp_on cvmx_bgxx_cmrx_rx_bp_on_t;
+
+/**
+ * cvmx_bgx#_cmr#_rx_bp_status
+ */
+union cvmx_bgxx_cmrx_rx_bp_status {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_rx_bp_status_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t bp                           : 1;  /**< Per-LMAC backpressure status.
+                                                         0 = LMAC is not backpressured.
+                                                         1 = LMAC is backpressured. */
+#else
+	uint64_t bp                           : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_rx_bp_status_s  cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_rx_bp_status cvmx_bgxx_cmrx_rx_bp_status_t;
+
+/**
+ * cvmx_bgx#_cmr#_rx_fifo_len
+ */
+union cvmx_bgxx_cmrx_rx_fifo_len {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_rx_fifo_len_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_13_63               : 51;
+	uint64_t fifo_len                     : 13; /**< Per-LMAC FIFO length. Useful for determining if FIFO is empty when bringing an LMAC down. */
+#else
+	uint64_t fifo_len                     : 13;
+	uint64_t reserved_13_63               : 51;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_rx_fifo_len_s   cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_rx_fifo_len cvmx_bgxx_cmrx_rx_fifo_len_t;
+
+/**
+ * cvmx_bgx#_cmr#_rx_id_map
+ *
+ * These registers set the RX LMAC ID mapping for X2P/PKI.
+ *
+ */
+union cvmx_bgxx_cmrx_rx_id_map {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_rx_id_map_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_15_63               : 49;
+	uint64_t rid                          : 7;  /**< Reassembly ID map for this LMAC. A shared pool of 96 reassembly IDs (RIDs) exists for all
+                                                         MACs.
+                                                         The RID for this LMAC must be constrained such that it does not overlap with any other MAC
+                                                         in the system. Its reset value has been chosen such that this condition is satisfied:
+                                                         _ RID reset value = 4*(BGX_ID + 1) + LMAC_ID
+                                                         Changes to RID must only occur when the LMAC is quiescent (i.e. the LMAC receive interface
+                                                         is down and the RX FIFO is empty). */
+	uint64_t pknd                         : 8;  /**< Port kind for this LMAC.  Only bits [5:0] are used.  Bits [7:6] are not used and should
+                                                         not be changed from the reset value of 0. */
+#else
+	uint64_t pknd                         : 8;
+	uint64_t rid                          : 7;
+	uint64_t reserved_15_63               : 49;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_rx_id_map_s     cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_rx_id_map cvmx_bgxx_cmrx_rx_id_map_t;
+
+/**
+ * cvmx_bgx#_cmr#_rx_logl_xoff
+ */
+union cvmx_bgxx_cmrx_rx_logl_xoff {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_rx_logl_xoff_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t xoff                         : 16; /**< Together with BGX()_CMR()_RX_LOGL_XON, defines type of channel backpressure to
+                                                         apply to the SMU. Do not write when HiGig2 is enabled. Writing 1 sets the same physical
+                                                         register as that which is cleared by BGX()_CMR()_RX_LOGL_XON[XON]. An XOFF value
+                                                         of 1 will cause a backpressure on SMU. */
+#else
+	uint64_t xoff                         : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_rx_logl_xoff_s  cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_rx_logl_xoff cvmx_bgxx_cmrx_rx_logl_xoff_t;
+
+/**
+ * cvmx_bgx#_cmr#_rx_logl_xon
+ */
+union cvmx_bgxx_cmrx_rx_logl_xon {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_rx_logl_xon_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t xon                          : 16; /**< Together with BGX()_CMR()_RX_LOGL_XOFF, defines type of channel backpressure to
+                                                         apply. Do not write when HiGig2 is enabled. Writing 1 clears the same physical register as
+                                                         that which is set by BGX()_CMR()_RX_LOGL_XOFF[XOFF]. An XON value of 1 will
+                                                         cause a backpressure on SMU. */
+#else
+	uint64_t xon                          : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_rx_logl_xon_s   cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_rx_logl_xon cvmx_bgxx_cmrx_rx_logl_xon_t;
+
+/**
+ * cvmx_bgx#_cmr#_rx_pause_drop_time
+ */
+union cvmx_bgxx_cmrx_rx_pause_drop_time {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_rx_pause_drop_time_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t pause_time                   : 16; /**< Time extracted from the dropped PAUSE packet dropped due to RXB FIFO full or during partner reset. */
+#else
+	uint64_t pause_time                   : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_rx_pause_drop_time_s cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_rx_pause_drop_time cvmx_bgxx_cmrx_rx_pause_drop_time_t;
+
+/**
+ * cvmx_bgx#_cmr#_rx_stat0
+ *
+ * These registers provide a count of received packets that meet the following conditions:
+ * * are not recognized as PAUSE packets.
+ * * are not dropped due DMAC filtering.
+ * * are not dropped due FIFO full status.
+ * * do not have any other OPCODE (FCS, Length, etc).
+ */
+union cvmx_bgxx_cmrx_rx_stat0 {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_rx_stat0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t cnt                          : 48; /**< Count of received packets. CNT will wrap and is cleared if LMAC is disabled with
+                                                         BGX()_CMR()_CONFIG[ENABLE]=0. */
+#else
+	uint64_t cnt                          : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_rx_stat0_s      cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_rx_stat0 cvmx_bgxx_cmrx_rx_stat0_t;
+
+/**
+ * cvmx_bgx#_cmr#_rx_stat1
+ *
+ * These registers provide a count of octets of received packets.
+ *
+ */
+union cvmx_bgxx_cmrx_rx_stat1 {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_rx_stat1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t cnt                          : 48; /**< Octet count of received packets. CNT will wrap and is cleared if LMAC is disabled with
+                                                         BGX()_CMR()_CONFIG[ENABLE]=0. */
+#else
+	uint64_t cnt                          : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_rx_stat1_s      cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_rx_stat1 cvmx_bgxx_cmrx_rx_stat1_t;
+
+/**
+ * cvmx_bgx#_cmr#_rx_stat2
+ *
+ * These registers provide a count of all packets received that were recognized as flow-control
+ * or PAUSE packets. PAUSE packets with any kind of error are counted in
+ * BGX()_CMR()_RX_STAT8 (error stats register). Pause packets can be optionally dropped
+ * or forwarded based on BGX()_SMU()_RX_FRM_CTL[CTL_DRP]. This count increments
+ * regardless of whether the packet is dropped. PAUSE packets are never counted in
+ * BGX()_CMR()_RX_STAT0.
+ */
+union cvmx_bgxx_cmrx_rx_stat2 {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_rx_stat2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t cnt                          : 48; /**< Count of received PAUSE packets. CNT will wrap and is cleared if LMAC is disabled with
+                                                         BGX()_CMR()_CONFIG[ENABLE]=0. */
+#else
+	uint64_t cnt                          : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_rx_stat2_s      cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_rx_stat2 cvmx_bgxx_cmrx_rx_stat2_t;
+
+/**
+ * cvmx_bgx#_cmr#_rx_stat3
+ *
+ * These registers provide a count of octets of received PAUSE and control packets.
+ *
+ */
+union cvmx_bgxx_cmrx_rx_stat3 {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_rx_stat3_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t cnt                          : 48; /**< Octet count of received PAUSE packets. CNT will wrap and is cleared if LMAC is disabled
+                                                         with BGX()_CMR()_CONFIG[ENABLE]=0. */
+#else
+	uint64_t cnt                          : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_rx_stat3_s      cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_rx_stat3 cvmx_bgxx_cmrx_rx_stat3_t;
+
+/**
+ * cvmx_bgx#_cmr#_rx_stat4
+ *
+ * These registers provide a count of all packets received that were dropped by the DMAC filter.
+ * Packets that match the DMAC are dropped and counted here regardless of whether they were ERR
+ * packets, but does not include those reported in BGX()_CMR()_RX_STAT6. These packets
+ * are never counted in BGX()_CMR()_RX_STAT0. Eight-byte packets as the result of
+ * truncation or other means are not dropped by CNXXXX and will never appear in this count.
+ */
+union cvmx_bgxx_cmrx_rx_stat4 {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_rx_stat4_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t cnt                          : 48; /**< Count of filtered DMAC packets. CNT will wrap and is cleared if LMAC is disabled with
+                                                         BGX()_CMR()_CONFIG[ENABLE]=0. */
+#else
+	uint64_t cnt                          : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_rx_stat4_s      cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_rx_stat4 cvmx_bgxx_cmrx_rx_stat4_t;
+
+/**
+ * cvmx_bgx#_cmr#_rx_stat5
+ *
+ * These registers provide a count of octets of filtered DMAC packets.
+ *
+ */
+union cvmx_bgxx_cmrx_rx_stat5 {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_rx_stat5_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t cnt                          : 48; /**< Octet count of filtered DMAC packets. CNT will wrap and is cleared if LMAC is disabled
+                                                         with BGX()_CMR()_CONFIG[ENABLE]=0. */
+#else
+	uint64_t cnt                          : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_rx_stat5_s      cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_rx_stat5 cvmx_bgxx_cmrx_rx_stat5_t;
+
+/**
+ * cvmx_bgx#_cmr#_rx_stat6
+ *
+ * These registers provide a count of all packets received that were dropped due to a full
+ * receive FIFO. They do not count any packet that is truncated at the point of overflow and sent
+ * on to the PKI. These registers count all entire packets dropped by the FIFO for a given LMAC
+ * regardless of DMAC or PAUSE type.
+ */
+union cvmx_bgxx_cmrx_rx_stat6 {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_rx_stat6_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t cnt                          : 48; /**< Count of dropped packets. CNT will wrap and is cleared if LMAC is disabled with
+                                                         BGX()_CMR()_CONFIG[ENABLE]=0. */
+#else
+	uint64_t cnt                          : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_rx_stat6_s      cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_rx_stat6 cvmx_bgxx_cmrx_rx_stat6_t;
+
+/**
+ * cvmx_bgx#_cmr#_rx_stat7
+ *
+ * These registers provide a count of octets of received packets that were dropped due to a full
+ * receive FIFO.
+ */
+union cvmx_bgxx_cmrx_rx_stat7 {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_rx_stat7_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t cnt                          : 48; /**< Octet count of dropped packets. CNT will wrap and is cleared if LMAC is disabled with
+                                                         BGX()_CMR()_CONFIG[ENABLE]=0. */
+#else
+	uint64_t cnt                          : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_rx_stat7_s      cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_rx_stat7 cvmx_bgxx_cmrx_rx_stat7_t;
+
+/**
+ * cvmx_bgx#_cmr#_rx_stat8
+ *
+ * These registers provide a count of all packets received with some error that were not dropped
+ * either due to the DMAC filter or lack of room in the receive FIFO.
+ * This does not include packets which were counted in
+ * BGX()_CMR()_RX_STAT2, BGX()_CMR()_RX_STAT4 nor
+ * BGX()_CMR()_RX_STAT6.
+ *
+ * Which statistics are updated on control packet errors and drops are shown below:
+ *
+ * <pre>
+ * if dropped [
+ *   if !errored STAT8
+ *   if overflow STAT6
+ *   else if dmac drop STAT4
+ *   else if filter drop STAT2
+ * ] else [
+ *   if errored STAT2
+ *   else STAT8
+ * ]
+ * </pre>
+ */
+union cvmx_bgxx_cmrx_rx_stat8 {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_rx_stat8_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t cnt                          : 48; /**< Count of error packets. CNT will wrap and is cleared if LMAC is disabled with
+                                                         BGX()_CMR()_CONFIG[ENABLE]=0. */
+#else
+	uint64_t cnt                          : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_rx_stat8_s      cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_rx_stat8 cvmx_bgxx_cmrx_rx_stat8_t;
+
+/**
+ * cvmx_bgx#_cmr#_rx_weight
+ */
+union cvmx_bgxx_cmrx_rx_weight {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_rx_weight_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t weight                       : 4;  /**< For the weighted round robin algorithm in CMR RXB, weight to assign for this LMAC relative
+                                                         to other LMAC weights. Defaults to round-robin (non-weighted minimum setting of 0x1). A
+                                                         setting of 0x0 effectively takes the LMAC out of eligibility. */
+#else
+	uint64_t weight                       : 4;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_rx_weight_s     cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_rx_weight cvmx_bgxx_cmrx_rx_weight_t;
+
+/**
+ * cvmx_bgx#_cmr#_tx_channel
+ */
+union cvmx_bgxx_cmrx_tx_channel {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_tx_channel_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_32_63               : 32;
+	uint64_t msk                          : 16; /**< Backpressure channel mask. BGX can completely ignore the channel backpressure for channel
+                                                         specified by this field. Any channel in which MSK<n> is set never sends backpressure
+                                                         information to PKO. */
+	uint64_t dis                          : 16; /**< Credit return backpressure disable. BGX stops returning channel credits for any channel
+                                                         that is backpressured. These bits can be used to override that. If DIS<n> is set, channel
+                                                         credits may flow back regardless of the backpressure for that channel. */
+#else
+	uint64_t dis                          : 16;
+	uint64_t msk                          : 16;
+	uint64_t reserved_32_63               : 32;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_tx_channel_s    cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_tx_channel cvmx_bgxx_cmrx_tx_channel_t;
+
+/**
+ * cvmx_bgx#_cmr#_tx_fifo_len
+ */
+union cvmx_bgxx_cmrx_tx_fifo_len {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_tx_fifo_len_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_14_63               : 50;
+	uint64_t lmac_idle                    : 1;  /**< Idle signal to identify when all credits and other pipeline buffers are also cleared out
+                                                         and LMAC can be considered IDLE in the BGX CMR TX. */
+	uint64_t fifo_len                     : 13; /**< Per-LMAC TXB main FIFO length. Useful for determining if main FIFO is empty when bringing
+                                                         an LMAC down. */
+#else
+	uint64_t fifo_len                     : 13;
+	uint64_t lmac_idle                    : 1;
+	uint64_t reserved_14_63               : 50;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_tx_fifo_len_s   cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_tx_fifo_len cvmx_bgxx_cmrx_tx_fifo_len_t;
+
+/**
+ * cvmx_bgx#_cmr#_tx_hg2_status
+ */
+union cvmx_bgxx_cmrx_tx_hg2_status {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_tx_hg2_status_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_32_63               : 32;
+	uint64_t xof                          : 16; /**< 16-bit XOF back pressure vector from HiGig2 message packet or from PFC/CBFC packets. Non-
+                                                         zero only when logical back pressure is active. All bits are 0 when LGTIM2GO=0x0. */
+	uint64_t lgtim2go                     : 16; /**< Logical packet flow back pressure time remaining. Initial value set from XOF time field of
+                                                         HiGig2 message packet received or a function of the enabled and current timers for
+                                                         PFC/CBFC packets. Non-zero only when logical back pressure is active. */
+#else
+	uint64_t lgtim2go                     : 16;
+	uint64_t xof                          : 16;
+	uint64_t reserved_32_63               : 32;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_tx_hg2_status_s cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_tx_hg2_status cvmx_bgxx_cmrx_tx_hg2_status_t;
+
+/**
+ * cvmx_bgx#_cmr#_tx_ovr_bp
+ */
+union cvmx_bgxx_cmrx_tx_ovr_bp {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_tx_ovr_bp_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t tx_chan_bp                   : 16; /**< Per-channel backpressure status sent to PKO.
+                                                         0 = channel is available.
+                                                         1 = channel should be backpressured. */
+#else
+	uint64_t tx_chan_bp                   : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_tx_ovr_bp_s     cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_tx_ovr_bp cvmx_bgxx_cmrx_tx_ovr_bp_t;
+
+/**
+ * cvmx_bgx#_cmr#_tx_stat0
+ */
+union cvmx_bgxx_cmrx_tx_stat0 {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_tx_stat0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t xscol                        : 48; /**< Number of packets dropped (never successfully sent) due to excessive collision. Defined by
+                                                         BGX()_GMP_GMI_TX_COL_ATTEMPT[LIMIT]. SGMII/1000Base-X half-duplex only.
+                                                         Not cleared on read; cleared on a write with 0x0. Counters will wrap. Cleared if LMAC is
+                                                         disabled with BGX()_CMR()_CONFIG[ENABLE]=0. */
+#else
+	uint64_t xscol                        : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_tx_stat0_s      cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_tx_stat0 cvmx_bgxx_cmrx_tx_stat0_t;
+
+/**
+ * cvmx_bgx#_cmr#_tx_stat1
+ */
+union cvmx_bgxx_cmrx_tx_stat1 {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_tx_stat1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t xsdef                        : 48; /**< Number of packets dropped (never successfully sent) due to excessive deferral.
+                                                         SGMII/1000BASE-X half-duplex only.
+                                                         Not cleared on read; cleared on a write with 0x0. Counters will wrap. Cleared if LMAC is
+                                                         disabled with BGX()_CMR()_CONFIG[ENABLE]=0. */
+#else
+	uint64_t xsdef                        : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_tx_stat1_s      cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_tx_stat1 cvmx_bgxx_cmrx_tx_stat1_t;
+
+/**
+ * cvmx_bgx#_cmr#_tx_stat10
+ */
+union cvmx_bgxx_cmrx_tx_stat10 {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_tx_stat10_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t hist4                        : 48; /**< Number of packets sent with an octet count between 256-511. Packet length is the sum of
+                                                         all data transmitted on the wire for the given packet including packet data, pad bytes,
+                                                         FCS bytes, PAUSE bytes, and JAM bytes. The octet counts do not include PREAMBLE byte or
+                                                         EXTEND cycles.
+                                                         Not cleared on read; cleared on a write with 0x0. Counters will wrap. Cleared if LMAC is
+                                                         disabled with BGX()_CMR()_CONFIG[ENABLE]=0. */
+#else
+	uint64_t hist4                        : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_tx_stat10_s     cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_tx_stat10 cvmx_bgxx_cmrx_tx_stat10_t;
+
+/**
+ * cvmx_bgx#_cmr#_tx_stat11
+ */
+union cvmx_bgxx_cmrx_tx_stat11 {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_tx_stat11_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t hist5                        : 48; /**< Number of packets sent with an octet count between 512-1023. Packet length is the sum of
+                                                         all data transmitted on the wire for the given packet including packet data, pad bytes,
+                                                         FCS bytes, PAUSE bytes, and JAM bytes. The octet counts do not include PREAMBLE byte or
+                                                         EXTEND cycles.
+                                                         Not cleared on read; cleared on a write with 0x0. Counters will wrap. Cleared if LMAC is
+                                                         disabled with BGX()_CMR()_CONFIG[ENABLE]=0. */
+#else
+	uint64_t hist5                        : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_tx_stat11_s     cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_tx_stat11 cvmx_bgxx_cmrx_tx_stat11_t;
+
+/**
+ * cvmx_bgx#_cmr#_tx_stat12
+ */
+union cvmx_bgxx_cmrx_tx_stat12 {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_tx_stat12_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t hist6                        : 48; /**< Number of packets sent with an octet count between 1024-1518. Packet length is the sum of
+                                                         all data transmitted on the wire for the given packet including packet data, pad bytes,
+                                                         FCS bytes, PAUSE bytes, and JAM bytes. The octet counts do not include PREAMBLE byte or
+                                                         EXTEND cycles.
+                                                         Not cleared on read; cleared on a write with 0x0. Counters will wrap. Cleared if LMAC is
+                                                         disabled with BGX()_CMR()_CONFIG[ENABLE]=0. */
+#else
+	uint64_t hist6                        : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_tx_stat12_s     cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_tx_stat12 cvmx_bgxx_cmrx_tx_stat12_t;
+
+/**
+ * cvmx_bgx#_cmr#_tx_stat13
+ */
+union cvmx_bgxx_cmrx_tx_stat13 {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_tx_stat13_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t hist7                        : 48; /**< Number of packets sent with an octet count > 1518. Packet length is the sum of all data
+                                                         transmitted on the wire for the given packet including packet data, pad bytes, FCS bytes,
+                                                         PAUSE bytes, and JAM bytes. The octet counts do not include PREAMBLE byte or EXTEND
+                                                         cycles.
+                                                         Not cleared on read; cleared on a write with 0x0. Counters will wrap. Cleared if LMAC is
+                                                         disabled with BGX()_CMR()_CONFIG[ENABLE]=0. */
+#else
+	uint64_t hist7                        : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_tx_stat13_s     cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_tx_stat13 cvmx_bgxx_cmrx_tx_stat13_t;
+
+/**
+ * cvmx_bgx#_cmr#_tx_stat14
+ */
+union cvmx_bgxx_cmrx_tx_stat14 {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_tx_stat14_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t bcst                         : 48; /**< Number of packets sent to multicast DMAC. Does not include MCST packets.
+                                                         Not cleared on read; cleared on a write with 0x0. Counters will wrap.
+                                                         Note that BGX determines if the packet is MCST or BCST from the DMAC of the packet. BGX
+                                                         assumes that the DMAC lies in the first six bytes of the packet as per the 802.3 frame
+                                                         definition. If the system requires additional data before the L2 header, the MCST and BCST
+                                                         counters may not reflect reality and should be ignored by software. Cleared if LMAC is
+                                                         disabled with BGX()_CMR()_CONFIG[ENABLE]=0. */
+#else
+	uint64_t bcst                         : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_tx_stat14_s     cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_tx_stat14 cvmx_bgxx_cmrx_tx_stat14_t;
+
+/**
+ * cvmx_bgx#_cmr#_tx_stat15
+ */
+union cvmx_bgxx_cmrx_tx_stat15 {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_tx_stat15_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t mcst                         : 48; /**< Number of packets sent to multicast DMAC. Does not include BCST packets.
+                                                         Not cleared on read; cleared on a write with 0x0. Counters will wrap.
+                                                         Note that BGX determines if the packet is MCST or BCST from the DMAC of the packet. BGX
+                                                         assumes that the DMAC lies in the first six bytes of the packet as per the 802.3 frame
+                                                         definition. If the system requires additional data before the L2 header, then the MCST and
+                                                         BCST counters may not reflect reality and should be ignored by software. Cleared if LMAC
+                                                         is disabled with BGX()_CMR()_CONFIG[ENABLE]=0. */
+#else
+	uint64_t mcst                         : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_tx_stat15_s     cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_tx_stat15 cvmx_bgxx_cmrx_tx_stat15_t;
+
+/**
+ * cvmx_bgx#_cmr#_tx_stat16
+ */
+union cvmx_bgxx_cmrx_tx_stat16 {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_tx_stat16_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t undflw                       : 48; /**< Number of underflow packets.
+                                                         Not cleared on read; cleared on a write with 0x0. Counters will wrap. Cleared if LMAC is
+                                                         disabled with BGX()_CMR()_CONFIG[ENABLE]=0. */
+#else
+	uint64_t undflw                       : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_tx_stat16_s     cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_tx_stat16 cvmx_bgxx_cmrx_tx_stat16_t;
+
+/**
+ * cvmx_bgx#_cmr#_tx_stat17
+ */
+union cvmx_bgxx_cmrx_tx_stat17 {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_tx_stat17_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t ctl                          : 48; /**< Number of control packets (PAUSE flow control) generated by BGX. It does not include
+                                                         control packets forwarded or generated by the cores.
+                                                         CTL counts the number of generated PFC frames and does not track the number of generated
+                                                         HG2 messages.
+                                                         Not cleared on read; cleared on a write with 0x0. Counters will wrap. Cleared if LMAC is
+                                                         disabled with BGX()_CMR()_CONFIG[ENABLE]=0. */
+#else
+	uint64_t ctl                          : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_tx_stat17_s     cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_tx_stat17 cvmx_bgxx_cmrx_tx_stat17_t;
+
+/**
+ * cvmx_bgx#_cmr#_tx_stat2
+ */
+union cvmx_bgxx_cmrx_tx_stat2 {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_tx_stat2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t mcol                         : 48; /**< Number of packets sent with multiple collisions. Must be less than
+                                                         BGX()_GMP_GMI_TX_COL_ATTEMPT[LIMIT]. SGMII/1000BASE-X half-duplex only.
+                                                         Not cleared on read; cleared on a write with 0x0. Counters will wrap. Cleared if LMAC is
+                                                         disabled with BGX()_CMR()_CONFIG[ENABLE]=0. */
+#else
+	uint64_t mcol                         : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_tx_stat2_s      cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_tx_stat2 cvmx_bgxx_cmrx_tx_stat2_t;
+
+/**
+ * cvmx_bgx#_cmr#_tx_stat3
+ */
+union cvmx_bgxx_cmrx_tx_stat3 {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_tx_stat3_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t scol                         : 48; /**< Number of packets sent with a single collision. SGMII/1000BASE-X half-duplex only.
+                                                         Not cleared on read; cleared on a write with 0x0. Counters will wrap. Cleared if LMAC is
+                                                         disabled with BGX()_CMR()_CONFIG[ENABLE]=0. */
+#else
+	uint64_t scol                         : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_tx_stat3_s      cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_tx_stat3 cvmx_bgxx_cmrx_tx_stat3_t;
+
+/**
+ * cvmx_bgx#_cmr#_tx_stat4
+ */
+union cvmx_bgxx_cmrx_tx_stat4 {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_tx_stat4_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t octs                         : 48; /**< Number of total octets sent on the interface. Does not count octets from frames that were
+                                                         truncated due to collisions in half-duplex mode.
+                                                         Octet counts are the sum of all data transmitted on the wire including packet data, pad
+                                                         bytes, FCS bytes, PAUSE bytes, and JAM bytes. The octet counts do not include PREAMBLE
+                                                         byte or EXTEND cycles.
+                                                         Not cleared on read; cleared on a write with 0x0. Counters will wrap. Cleared if LMAC is
+                                                         disabled with BGX()_CMR()_CONFIG[ENABLE]=0. */
+#else
+	uint64_t octs                         : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_tx_stat4_s      cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_tx_stat4 cvmx_bgxx_cmrx_tx_stat4_t;
+
+/**
+ * cvmx_bgx#_cmr#_tx_stat5
+ */
+union cvmx_bgxx_cmrx_tx_stat5 {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_tx_stat5_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t pkts                         : 48; /**< Number of total frames sent on the interface. Does not count octets from frames that were
+                                                         truncated due to collisions in half-duplex mode.
+                                                         Not cleared on read; cleared on a write with 0x0. Counters will wrap. Cleared if LMAC is
+                                                         disabled with BGX()_CMR()_CONFIG[ENABLE]=0. */
+#else
+	uint64_t pkts                         : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_tx_stat5_s      cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_tx_stat5 cvmx_bgxx_cmrx_tx_stat5_t;
+
+/**
+ * cvmx_bgx#_cmr#_tx_stat6
+ */
+union cvmx_bgxx_cmrx_tx_stat6 {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_tx_stat6_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t hist0                        : 48; /**< Number of packets sent with an octet count < 64. Packet length is the sum of all data
+                                                         transmitted on the wire for the given packet including packet data, pad bytes, FCS bytes,
+                                                         PAUSE bytes, and JAM bytes. The octet counts do not include PREAMBLE byte or EXTEND
+                                                         cycles.
+                                                         Not cleared on read; cleared on a write with 0x0. Counters will wrap. Cleared if LMAC is
+                                                         disabled with BGX()_CMR()_CONFIG[ENABLE]=0. */
+#else
+	uint64_t hist0                        : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_tx_stat6_s      cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_tx_stat6 cvmx_bgxx_cmrx_tx_stat6_t;
+
+/**
+ * cvmx_bgx#_cmr#_tx_stat7
+ */
+union cvmx_bgxx_cmrx_tx_stat7 {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_tx_stat7_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t hist1                        : 48; /**< Number of packets sent with an octet count of 64. Packet length is the sum of all data
+                                                         transmitted on the wire for the given packet including packet data, pad bytes, FCS bytes,
+                                                         PAUSE bytes, and JAM bytes. The octet counts do not include PREAMBLE byte or EXTEND
+                                                         cycles.
+                                                         Not cleared on read; cleared on a write with 0x0. Counters will wrap. Cleared if LMAC is
+                                                         disabled with BGX()_CMR()_CONFIG[ENABLE]=0. */
+#else
+	uint64_t hist1                        : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_tx_stat7_s      cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_tx_stat7 cvmx_bgxx_cmrx_tx_stat7_t;
+
+/**
+ * cvmx_bgx#_cmr#_tx_stat8
+ */
+union cvmx_bgxx_cmrx_tx_stat8 {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_tx_stat8_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t hist2                        : 48; /**< Number of packets sent with an octet count between 65-127. Packet length is the sum of all
+                                                         data transmitted on the wire for the given packet including packet data, pad bytes, FCS
+                                                         bytes, PAUSE bytes, and JAM bytes. The octet counts do not include PREAMBLE byte or EXTEND
+                                                         cycles.
+                                                         Not cleared on read; cleared on a write with 0x0. Counters will wrap. Cleared if LMAC is
+                                                         disabled with BGX()_CMR()_CONFIG[ENABLE]=0. */
+#else
+	uint64_t hist2                        : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_tx_stat8_s      cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_tx_stat8 cvmx_bgxx_cmrx_tx_stat8_t;
+
+/**
+ * cvmx_bgx#_cmr#_tx_stat9
+ */
+union cvmx_bgxx_cmrx_tx_stat9 {
+	uint64_t u64;
+	struct cvmx_bgxx_cmrx_tx_stat9_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t hist3                        : 48; /**< Number of packets sent with an octet count between 128-255. Packet length is the sum of
+                                                         all data transmitted on the wire for the given packet including packet data, pad bytes,
+                                                         FCS bytes, PAUSE bytes, and JAM bytes. The octet counts do not include PREAMBLE byte or
+                                                         EXTEND cycles.
+                                                         Not cleared on read; cleared on a write with 0x0. Counters will wrap. Cleared if LMAC is
+                                                         disabled with BGX()_CMR()_CONFIG[ENABLE]=0. */
+#else
+	uint64_t hist3                        : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_cmrx_tx_stat9_s      cn78xx;
+};
+typedef union cvmx_bgxx_cmrx_tx_stat9 cvmx_bgxx_cmrx_tx_stat9_t;
+
+/**
+ * cvmx_bgx#_cmr_bad
+ */
+union cvmx_bgxx_cmr_bad {
+	uint64_t u64;
+	struct cvmx_bgxx_cmr_bad_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t rxb_nxl                      : 1;  /**< Receive side LMAC ID > BGX()_CMR_RX_LMACS. */
+#else
+	uint64_t rxb_nxl                      : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_bgxx_cmr_bad_s            cn78xx;
+};
+typedef union cvmx_bgxx_cmr_bad cvmx_bgxx_cmr_bad_t;
+
+/**
+ * cvmx_bgx#_cmr_bist_status
+ */
+union cvmx_bgxx_cmr_bist_status {
+	uint64_t u64;
+	struct cvmx_bgxx_cmr_bist_status_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_25_63               : 39;
+	uint64_t status                       : 25; /**< "BIST results. Hardware sets a bit to 1 for memory that fails; 0 indicates pass or never
+                                                         run. INTERNAL:
+                                                         <0> = bgx#.rxb.infif_gmp
+                                                         <1> = bgx#.rxb.infif_smu
+                                                         <2> = bgx#.rxb.fif_bnk00
+                                                         <3> = bgx#.rxb.fif_bnk01
+                                                         <4> = bgx#.rxb.fif_bnk10
+                                                         <5> = bgx#.rxb.fif_bnk11
+                                                         <6> = bgx#.rxb.skd_fif
+                                                         <7> = bgx#.rxb_mix0_fif
+                                                         <8> = bgx#.rxb_mix1_fif
+                                                         <9> = RAZ
+                                                         <10> = bgx#.txb_fif_bnk0
+                                                         <11> = bgx#.txb_fif_bnk1
+                                                         <12> = bgx#.txb_skd_fif
+                                                         <13> = bgx#.txb_mix0_fif
+                                                         <14> = bgx#.txb_mix1_fif
+                                                         <24:15> = RAZ" */
+#else
+	uint64_t status                       : 25;
+	uint64_t reserved_25_63               : 39;
+#endif
+	} s;
+	struct cvmx_bgxx_cmr_bist_status_s    cn78xx;
+};
+typedef union cvmx_bgxx_cmr_bist_status cvmx_bgxx_cmr_bist_status_t;
+
+/**
+ * cvmx_bgx#_cmr_chan_msk_and
+ */
+union cvmx_bgxx_cmr_chan_msk_and {
+	uint64_t u64;
+	struct cvmx_bgxx_cmr_chan_msk_and_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t msk_and                      : 64; /**< Assert physical backpressure when the backpressure channel vector combined with MSK_AND
+                                                         indicates backpressure as follows:
+                                                         _ phys_bp_msk_and = (CHAN_VECTOR<x:y> & MSK_AND<x:y>) == MSK_AND<x:y>
+                                                         _ phys_bp = phys_bp_msk_or || phys_bp_msk_and
+                                                         In single LMAC configurations, x = 63, y = 0.
+                                                         In multi-LMAC configurations, x/y are set as follows:
+                                                         _ LMAC interface 0, x = 15, y = 0.
+                                                         _ LMAC interface 1, x = 31, y = 16.
+                                                         _ LMAC interface 2, x = 47, y = 32.
+                                                         _ LMAC interface 3, x = 63, y = 48. */
+#else
+	uint64_t msk_and                      : 64;
+#endif
+	} s;
+	struct cvmx_bgxx_cmr_chan_msk_and_s   cn78xx;
+};
+typedef union cvmx_bgxx_cmr_chan_msk_and cvmx_bgxx_cmr_chan_msk_and_t;
+
+/**
+ * cvmx_bgx#_cmr_chan_msk_or
+ */
+union cvmx_bgxx_cmr_chan_msk_or {
+	uint64_t u64;
+	struct cvmx_bgxx_cmr_chan_msk_or_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t msk_or                       : 64; /**< Assert physical backpressure when the backpressure channel vector combined with MSK_OR
+                                                         indicates backpressure as follows:
+                                                         _ phys_bp_msk_or = (CHAN_VECTOR<x:y> & MSK_AND<x:y>) & MSK_OR<x:y>
+                                                         _ phys_bp = phys_bp_msk_or || phys_bp_msk_and
+                                                         In single LMAC configurations, x = 63, y = 0.
+                                                         In multi-LMAC configurations, x/y are set as follows:
+                                                         _ LMAC interface 0, x = 15, y = 0.
+                                                         _ LMAC interface 1, x = 31, y = 16.
+                                                         _ LMAC interface 2, x = 47, y = 32.
+                                                         _ LMAC interface 3, x = 63, y = 48. */
+#else
+	uint64_t msk_or                       : 64;
+#endif
+	} s;
+	struct cvmx_bgxx_cmr_chan_msk_or_s    cn78xx;
+};
+typedef union cvmx_bgxx_cmr_chan_msk_or cvmx_bgxx_cmr_chan_msk_or_t;
+
+/**
+ * cvmx_bgx#_cmr_global_config
+ *
+ * These registers configure the global CMR, PCS, and MAC.
+ *
+ */
+union cvmx_bgxx_cmr_global_config {
+	uint64_t u64;
+	struct cvmx_bgxx_cmr_global_config_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_5_63                : 59;
+	uint64_t cmr_mix1_reset               : 1;  /**< If the MIX1 block is reset, software also needs to reset the MIX interface in the BGX by
+                                                         setting this bit to 1. It resets the MIX interface state in the BGX (mix FIFO and pending
+                                                         requests to MIX) and prevents the RXB FIFOs for all LMACs from pushing data to the
+                                                         interface. Setting this bit to 0 will not reset the MIX interface. After MIX comes out of
+                                                         reset, software should clear CMR_MIX_RESET. */
+	uint64_t cmr_mix0_reset               : 1;  /**< If the MIX0 block is reset, software also needs to reset the MIX interface in the BGX by
+                                                         setting this bit to 1. It resets the MIX interface state in the BGX (mix FIFO and pending
+                                                         requests to MIX) and prevents the RXB FIFOs for all LMACs from pushing data to the
+                                                         interface. Setting this bit to 0 will not reset the MIX interface. After MIX comes out of
+                                                         reset, software should clear CMR_MIX_RESET. */
+	uint64_t cmr_x2p_reset                : 1;  /**< If the PKI block is reset, software also needs to reset the X2P interface in the BGX by
+                                                         setting this bit to 1. It resets the X2P interface state in the BGX (skid FIFO and pending
+                                                         requests to PKI) and prevents the RXB FIFOs for all LMACs from pushing data to the
+                                                         interface. Setting this bit to 0 does not reset the X2P interface. After PKI comes out of
+                                                         reset, software should clear CMR_X2P_RESET. */
+	uint64_t bgx_clk_enable               : 1;  /**< The global clock enable for BGX. Setting this bit overrides clock enables set by
+                                                         BGX()_CMR()_CONFIG[ENABLE] and BGX()_CMR()_CONFIG[LMAC_TYPE], essentially
+                                                         turning on clocks for the entire BGX. Setting this bit to 0 results in not overriding
+                                                         clock enables set by BGX()_CMR()_CONFIG[ENABLE] and
+                                                         BGX()_CMR()_CONFIG[LMAC_TYPE]. */
+	uint64_t pmux_sds_sel                 : 1;  /**< SerDes/QLM output select. Specifies which QLM output is selected as the BGX input, as
+                                                         follows:
+                                                         <pre>
+                                                           Block   PMUX_SDS_SEL=0  PMUX_SDS_SEL=1
+                                                           --------------------------------------
+                                                           BGX0:   QLM0            QLM2
+                                                           BGX1:   QLM1            QLM3
+                                                           BGX2:   QLM4            N/A
+                                                           BGX3:   QLM5            N/A
+                                                           BGX4:   QLM6            N/A
+                                                           BGX5:   QLM7            N/A
+                                                         </pre> */
+#else
+	uint64_t pmux_sds_sel                 : 1;
+	uint64_t bgx_clk_enable               : 1;
+	uint64_t cmr_x2p_reset                : 1;
+	uint64_t cmr_mix0_reset               : 1;
+	uint64_t cmr_mix1_reset               : 1;
+	uint64_t reserved_5_63                : 59;
+#endif
+	} s;
+	struct cvmx_bgxx_cmr_global_config_s  cn78xx;
+};
+typedef union cvmx_bgxx_cmr_global_config cvmx_bgxx_cmr_global_config_t;
+
+/**
+ * cvmx_bgx#_cmr_mem_ctrl
+ */
+union cvmx_bgxx_cmr_mem_ctrl {
+	uint64_t u64;
+	struct cvmx_bgxx_cmr_mem_ctrl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_24_63               : 40;
+	uint64_t txb_skid_synd                : 2;  /**< Syndrome to flip and generate single-bit/double-bit for TXB SKID FIFO. */
+	uint64_t txb_skid_cor_dis             : 1;  /**< ECC-correction disable for the TXB SKID FIFO. */
+	uint64_t txb_fif_bk1_syn              : 2;  /**< Syndrome to flip and generate single-bit/double-bit error for TXB main bank1. */
+	uint64_t txb_fif_bk1_cdis             : 1;  /**< ECC-correction disable for the TXB main bank1. */
+	uint64_t txb_fif_bk0_syn              : 2;  /**< Syndrome to flip and generate single-bit/double-bit error for TXB main bank0. */
+	uint64_t txb_fif_bk0_cdis             : 1;  /**< ECC-correction disable for the TXB main bank0. */
+	uint64_t rxb_skid_synd                : 2;  /**< Syndrome to flip and generate single-bit/double-bit error for RXB SKID FIFO. */
+	uint64_t rxb_skid_cor_dis             : 1;  /**< ECC-correction disable for the RXB SKID FIFO. */
+	uint64_t rxb_fif_bk1_syn1             : 2;  /**< Syndrome to flip and generate single-bit/double-bit error for RXB main bank1 srf1. */
+	uint64_t rxb_fif_bk1_cdis1            : 1;  /**< ECC-correction disable for the RXB main bank1 srf1. */
+	uint64_t rxb_fif_bk1_syn0             : 2;  /**< Syndrome to flip and generate single-bit/double-bit error for RXB main bank1 srf0. */
+	uint64_t rxb_fif_bk1_cdis0            : 1;  /**< ECC-correction disable for the RXB main bank1 srf0. */
+	uint64_t rxb_fif_bk0_syn1             : 2;  /**< Syndrome to flip and generate single-bit/double-bit error for RXB main bank0 srf1. */
+	uint64_t rxb_fif_bk0_cdis1            : 1;  /**< ECC-correction disable for the RXB main bank0 srf1. */
+	uint64_t rxb_fif_bk0_syn0             : 2;  /**< Syndrome to flip and generate single-bit/double-bit error for RXB main bank0 srf0. */
+	uint64_t rxb_fif_bk0_cdis0            : 1;  /**< ECC-correction disable for the RXB main bank0 srf0. */
+#else
+	uint64_t rxb_fif_bk0_cdis0            : 1;
+	uint64_t rxb_fif_bk0_syn0             : 2;
+	uint64_t rxb_fif_bk0_cdis1            : 1;
+	uint64_t rxb_fif_bk0_syn1             : 2;
+	uint64_t rxb_fif_bk1_cdis0            : 1;
+	uint64_t rxb_fif_bk1_syn0             : 2;
+	uint64_t rxb_fif_bk1_cdis1            : 1;
+	uint64_t rxb_fif_bk1_syn1             : 2;
+	uint64_t rxb_skid_cor_dis             : 1;
+	uint64_t rxb_skid_synd                : 2;
+	uint64_t txb_fif_bk0_cdis             : 1;
+	uint64_t txb_fif_bk0_syn              : 2;
+	uint64_t txb_fif_bk1_cdis             : 1;
+	uint64_t txb_fif_bk1_syn              : 2;
+	uint64_t txb_skid_cor_dis             : 1;
+	uint64_t txb_skid_synd                : 2;
+	uint64_t reserved_24_63               : 40;
+#endif
+	} s;
+	struct cvmx_bgxx_cmr_mem_ctrl_s       cn78xx;
+};
+typedef union cvmx_bgxx_cmr_mem_ctrl cvmx_bgxx_cmr_mem_ctrl_t;
+
+/**
+ * cvmx_bgx#_cmr_mem_int
+ */
+union cvmx_bgxx_cmr_mem_int {
+	uint64_t u64;
+	struct cvmx_bgxx_cmr_mem_int_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_18_63               : 46;
+	uint64_t smu_in_overfl                : 1;  /**< RX SMU INFIFO overflow. */
+	uint64_t gmp_in_overfl                : 1;  /**< RX GMP INFIFO overflow. */
+	uint64_t txb_skid_sbe                 : 1;  /**< TXB SKID FIFO single-bit error. */
+	uint64_t txb_skid_dbe                 : 1;  /**< TXB SKID FIFO double-bit error. */
+	uint64_t txb_fif_bk1_sbe              : 1;  /**< TXB Main FIFO Bank1 single-bit error. */
+	uint64_t txb_fif_bk1_dbe              : 1;  /**< TXB Main FIFO Bank1 double-bit error. */
+	uint64_t txb_fif_bk0_sbe              : 1;  /**< TXB Main FIFO Bank0 single-bit error. */
+	uint64_t txb_fif_bk0_dbe              : 1;  /**< TXB Main FIFO Bank0 double-bit error. */
+	uint64_t rxb_skid_sbe                 : 1;  /**< RXB SKID FIFO single-bit error. */
+	uint64_t rxb_skid_dbe                 : 1;  /**< RXB SKID FIFO double-bit error. */
+	uint64_t rxb_fif_bk1_sbe1             : 1;  /**< RXB main FIFO bank1 srf1 single-bit error. */
+	uint64_t rxb_fif_bk1_dbe1             : 1;  /**< RXB main FIFO bank1 srf1 double-bit error. */
+	uint64_t rxb_fif_bk1_sbe0             : 1;  /**< RXB main FIFO bank1 srf0 single-bit error. */
+	uint64_t rxb_fif_bk1_dbe0             : 1;  /**< RXB main FIFO bank1 srf0 double-bit error. */
+	uint64_t rxb_fif_bk0_sbe1             : 1;  /**< RXB main FIFO bank0 srf1 single-bit error. */
+	uint64_t rxb_fif_bk0_dbe1             : 1;  /**< RXB main FIFO bank0 srf1 double-bit error. */
+	uint64_t rxb_fif_bk0_sbe0             : 1;  /**< RXB main FIFO bank0 srf0 single-bit error. */
+	uint64_t rxb_fif_bk0_dbe0             : 1;  /**< RXB main FIFO bank0 srf0 double-bit error. */
+#else
+	uint64_t rxb_fif_bk0_dbe0             : 1;
+	uint64_t rxb_fif_bk0_sbe0             : 1;
+	uint64_t rxb_fif_bk0_dbe1             : 1;
+	uint64_t rxb_fif_bk0_sbe1             : 1;
+	uint64_t rxb_fif_bk1_dbe0             : 1;
+	uint64_t rxb_fif_bk1_sbe0             : 1;
+	uint64_t rxb_fif_bk1_dbe1             : 1;
+	uint64_t rxb_fif_bk1_sbe1             : 1;
+	uint64_t rxb_skid_dbe                 : 1;
+	uint64_t rxb_skid_sbe                 : 1;
+	uint64_t txb_fif_bk0_dbe              : 1;
+	uint64_t txb_fif_bk0_sbe              : 1;
+	uint64_t txb_fif_bk1_dbe              : 1;
+	uint64_t txb_fif_bk1_sbe              : 1;
+	uint64_t txb_skid_dbe                 : 1;
+	uint64_t txb_skid_sbe                 : 1;
+	uint64_t gmp_in_overfl                : 1;
+	uint64_t smu_in_overfl                : 1;
+	uint64_t reserved_18_63               : 46;
+#endif
+	} s;
+	struct cvmx_bgxx_cmr_mem_int_s        cn78xx;
+};
+typedef union cvmx_bgxx_cmr_mem_int cvmx_bgxx_cmr_mem_int_t;
+
+/**
+ * cvmx_bgx#_cmr_nxc_adr
+ */
+union cvmx_bgxx_cmr_nxc_adr {
+	uint64_t u64;
+	struct cvmx_bgxx_cmr_nxc_adr_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t lmac_id                      : 4;  /**< Logged LMAC ID associated with NXC exceptions. */
+	uint64_t channel                      : 12; /**< Logged channel for NXC exceptions. */
+#else
+	uint64_t channel                      : 12;
+	uint64_t lmac_id                      : 4;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_cmr_nxc_adr_s        cn78xx;
+};
+typedef union cvmx_bgxx_cmr_nxc_adr cvmx_bgxx_cmr_nxc_adr_t;
+
+/**
+ * cvmx_bgx#_cmr_rx_adr#_cam
+ *
+ * These registers provide access to the 32 DMAC CAM entries in BGX.
+ *
+ */
+union cvmx_bgxx_cmr_rx_adrx_cam {
+	uint64_t u64;
+	struct cvmx_bgxx_cmr_rx_adrx_cam_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_54_63               : 10;
+	uint64_t id                           : 2;  /**< Logical MAC ID that this DMAC CAM address applies to. BGX has 32 DMAC CAM entries that can
+                                                         be accessed with the BGX*_CMR_RX_ADR_CAM() CSRs. These 32 DMAC entries can be used by
+                                                         any of the four SGMII MACs or the 10G/40G MACs using these register bits.
+                                                         A typical configuration is to provide eight CAM entries per LMAC ID, which is configured
+                                                         using the following settings:
+                                                         * LMAC interface 0: BGX(0..5)_CMR_RX_ADR(0..7)_CAM[ID] = 0x0.
+                                                         * LMAC interface 1: BGX(0..5)_CMR_RX_ADR(8..15)_CAM[ID] = 0x1.
+                                                         * LMAC interface 2: BGX(0..5)_CMR_RX_ADR(16..23)_CAM[ID] = 0x2.
+                                                         * LMAC interface 3: BGX(0..5)_CMR_RX_ADR(24..31)_CAM[ID] = 0x3. */
+	uint64_t reserved_49_51               : 3;
+	uint64_t en                           : 1;  /**< CAM entry enable for this DMAC address.
+                                                         1 = Include this address in the matching algorithm.
+                                                         0 = Don't include this address in the matching algorithm. */
+	uint64_t adr                          : 48; /**< DMAC address in the CAM used for matching. The CAM matches against unicast or multicast
+                                                         DMAC addresses. All BGX()_CMR_RX_ADR()_CAM CSRs can be used in any of the
+                                                         LMAC_TYPE combinations such that any BGX MAC can use any of the 32 common DMAC entries. */
+#else
+	uint64_t adr                          : 48;
+	uint64_t en                           : 1;
+	uint64_t reserved_49_51               : 3;
+	uint64_t id                           : 2;
+	uint64_t reserved_54_63               : 10;
+#endif
+	} s;
+	struct cvmx_bgxx_cmr_rx_adrx_cam_s    cn78xx;
+};
+typedef union cvmx_bgxx_cmr_rx_adrx_cam cvmx_bgxx_cmr_rx_adrx_cam_t;
+
+/**
+ * cvmx_bgx#_cmr_rx_lmacs
+ */
+union cvmx_bgxx_cmr_rx_lmacs {
+	uint64_t u64;
+	struct cvmx_bgxx_cmr_rx_lmacs_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_3_63                : 61;
+	uint64_t lmacs                        : 3;  /**< Number of LMACS. Specifies the number of LMACs that can be enabled.
+                                                         This determines the logical RX buffer size per LMAC and the maximum
+                                                         LMAC ID that can be used:
+                                                         0x0 = Reserved.
+                                                         0x1 = 64 KB per LMAC, maximum LMAC ID is 0.
+                                                         0x2 = 32 KB per LMAC, maximum LMAC ID is 1.
+                                                         0x3 = 16 KB per LMAC, maximum LMAC ID is 2.
+                                                         0x4 = 16 KB per LMAC, maximum LMAC ID is 3.
+                                                         0x5-0x7 = Reserved.
+                                                         Note the maximum LMAC ID is determined by the smaller of
+                                                         BGX()_CMR_RX_LMACS[LMACS] and BGX()_CMR_TX_LMACS[LMACS]. The two fields
+                                                         should be set to the same value for normal operation. */
+#else
+	uint64_t lmacs                        : 3;
+	uint64_t reserved_3_63                : 61;
+#endif
+	} s;
+	struct cvmx_bgxx_cmr_rx_lmacs_s       cn78xx;
+};
+typedef union cvmx_bgxx_cmr_rx_lmacs cvmx_bgxx_cmr_rx_lmacs_t;
+
+/**
+ * cvmx_bgx#_cmr_rx_ovr_bp
+ *
+ * BGX()_CMR_RX_OVR_BP[EN<0>] must be set to one and BGX()_CMR_RX_OVR_BP[BP<0>] must be
+ * cleared to zero (to forcibly disable hardware-automatic 802.3 PAUSE packet generation) with
+ * the HiGig2 Protocol when BGX()_SMU()_HG2_CONTROL[HG2TX_EN]=0. (The HiGig2 protocol is
+ * indicated by BGX()_SMU()_TX_CTL[HG_EN]=1 and BGX()_SMU()_RX_UDD_SKP[LEN]=16).
+ * Hardware can only auto-generate backpressure through HiGig2 messages (optionally, when
+ * BGX()_SMU()_HG2_CONTROL[HG2TX_EN]=1) with the HiGig2 protocol.
+ */
+union cvmx_bgxx_cmr_rx_ovr_bp {
+	uint64_t u64;
+	struct cvmx_bgxx_cmr_rx_ovr_bp_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_12_63               : 52;
+	uint64_t en                           : 4;  /**< Per-LMAC enable backpressure override.
+                                                         0 = Don't enable.
+                                                         1 = Enable override.
+                                                         Bit<8> represents LMAC 0, ..., bit<11> represents LMAC 3. */
+	uint64_t bp                           : 4;  /**< Per-LMAC backpressure status to use:
+                                                         0 = LMAC is available.
+                                                         1 = LMAC should be backpressured.
+                                                         Bit<4> represents LMAC 0, ..., bit<7> represents LMAC 3. */
+	uint64_t ign_fifo_bp                  : 4;  /**< Ignore the RX FIFO BP_ON signal when computing backpressure. CMR does not backpressure the
+                                                         MAC due to the FIFO length passing BP_ON mark. */
+#else
+	uint64_t ign_fifo_bp                  : 4;
+	uint64_t bp                           : 4;
+	uint64_t en                           : 4;
+	uint64_t reserved_12_63               : 52;
+#endif
+	} s;
+	struct cvmx_bgxx_cmr_rx_ovr_bp_s      cn78xx;
+};
+typedef union cvmx_bgxx_cmr_rx_ovr_bp cvmx_bgxx_cmr_rx_ovr_bp_t;
+
+/**
+ * cvmx_bgx#_cmr_tx_lmacs
+ *
+ * This register sets the number of LMACs allowed on the TX interface. The value is important for
+ * defining the partitioning of the transmit FIFO.
+ */
+union cvmx_bgxx_cmr_tx_lmacs {
+	uint64_t u64;
+	struct cvmx_bgxx_cmr_tx_lmacs_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_3_63                : 61;
+	uint64_t lmacs                        : 3;  /**< Number of LMACS: Specifies the number of LMACs that can be enabled.
+                                                         This determines the logical TX buffer size per LMAC and the maximum
+                                                         LMAC ID that can be used:
+                                                         0x0 = Reserved.
+                                                         0x1 = 32 KB per LMAC, maximum LMAC ID is 0.
+                                                         0x2 = 16 KB per LMAC, maximum LMAC ID is 1.
+                                                         0x3 = 8 KB per LMAC, maximum LMAC ID is 2.
+                                                         0x4 = 8 KB per LMAC, maximum LMAC ID is 3.
+                                                         0x5-0x7 = Reserved.
+                                                         The maximum LMAC ID is determined by the smaller of BGX()_CMR_RX_LMACS[LMACS]
+                                                         and BGX()_CMR_TX_LMACS[LMACS]. The two fields should be set to the same value for
+                                                         normal operation.' */
+#else
+	uint64_t lmacs                        : 3;
+	uint64_t reserved_3_63                : 61;
+#endif
+	} s;
+	struct cvmx_bgxx_cmr_tx_lmacs_s       cn78xx;
+};
+typedef union cvmx_bgxx_cmr_tx_lmacs cvmx_bgxx_cmr_tx_lmacs_t;
+
+/**
+ * cvmx_bgx#_gmp_gmi_prt#_cfg
+ *
+ * This register controls the configuration of the LMAC.
+ *
+ */
+union cvmx_bgxx_gmp_gmi_prtx_cfg {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_gmi_prtx_cfg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_14_63               : 50;
+	uint64_t tx_idle                      : 1;  /**< TX machine is idle. */
+	uint64_t rx_idle                      : 1;  /**< RX machine is idle. */
+	uint64_t reserved_9_11                : 3;
+	uint64_t speed_msb                    : 1;  /**< Link speed MSB (SGMII/1000Base-X only). See [SPEED]. */
+	uint64_t reserved_4_7                 : 4;
+	uint64_t slottime                     : 1;  /**< Slot time for half-duplex operation
+                                                         (SGMII/1000Base-X only):
+                                                         0 = 512 bit times (10/100 Mb/s operation)
+                                                         1 = 4096 bit times (1000 Mb/s operation) */
+	uint64_t duplex                       : 1;  /**< Duplex mode
+                                                         (SGMII/1000Base-X only):
+                                                         0 = half-duplex (collisions/extensions/bursts):
+                                                         1 = full-duplex. */
+	uint64_t speed                        : 1;  /**< Link Speed LSB (SGMII/1000Base-X only):
+                                                         _ [SPEED_MSB:SPEED] = 0x0: 100 Mb/s operation.
+                                                         _ [SPEED_MSB:SPEED] = 0x1: 1000 Mb/s operation.
+                                                         _ [SPEED_MSB:SPEED] = 0x2: 10 Mb/s operation.
+                                                         _ [SPEED_MSB:SPEED] = 0x3: Reserved. */
+	uint64_t reserved_0_0                 : 1;
+#else
+	uint64_t reserved_0_0                 : 1;
+	uint64_t speed                        : 1;
+	uint64_t duplex                       : 1;
+	uint64_t slottime                     : 1;
+	uint64_t reserved_4_7                 : 4;
+	uint64_t speed_msb                    : 1;
+	uint64_t reserved_9_11                : 3;
+	uint64_t rx_idle                      : 1;
+	uint64_t tx_idle                      : 1;
+	uint64_t reserved_14_63               : 50;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_gmi_prtx_cfg_s   cn78xx;
+};
+typedef union cvmx_bgxx_gmp_gmi_prtx_cfg cvmx_bgxx_gmp_gmi_prtx_cfg_t;
+
+/**
+ * cvmx_bgx#_gmp_gmi_rx#_decision
+ *
+ * This register specifies the byte count used to determine when to accept or to filter a packet.
+ * As each byte in a packet is received by GMI, the L2 byte count is compared against the
+ * BGX()_GMP_GMI_RX()_DECISION[CNT]. In normal operation, the L2 header begins after the
+ * PREAMBLE + SFD (BGX()_GMP_GMI_RX()_FRM_CTL[PRE_CHK] = 1) and any optional UDD skip
+ * data (BGX()_GMP_GMI_RX()_UDD_SKP[LEN]).
+ *
+ * INTERNAL: Notes:
+ * As each byte in a packet is received by GMI, the L2 byte count is compared
+ * against the BGX()_GMP_GMI_RX()_DECISION[CNT].  The L2 byte count is the number of bytes
+ * from the beginning of the L2 header (DMAC).  In normal operation, the L2
+ * header begins after the PREAMBLE+SFD (BGX()_GMP_GMI_RX()_FRM_CTL[PRE_CHK]=1) and any
+ * optional UDD skip data (BGX()_GMP_GMI_RX()_UDD_SKP[LEN]).
+ * When BGX()_GMP_GMI_RX()_FRM_CTL[PRE_CHK] is clear, PREAMBLE+SFD are prepended to the
+ * packet and would require UDD skip length to account for them.
+ * Port Mode
+ * - Full Duplex
+ *     L2 Size <  BGX_RX_DECISION - Accept packet. No filtering is applied
+ *     L2 Size >= BGX_RX_DECISION - Apply filter. Accept packet based on PAUSE packet filter
+ * - Half Duplex
+ *     L2 Size <  BGX_RX_DECISION - Drop packet. Packet is unconditionally dropped.
+ *     L2 Size >= BGX_RX_DECISION - Accept packet.
+ *
+ * where L2_size = MAX(0, total_packet_size - BGX()_GMP_GMI_RX()_UDD_SKP[LEN] -
+ *                        ((BGX()_GMP_GMI_RX()_FRM_CTL[PRE_CHK]==1)*8))
+ *
+ * BGX()_GMP_GMI_RX()_DECISION = The byte count to decide when to accept or filter a packet
+ */
+union cvmx_bgxx_gmp_gmi_rxx_decision {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_gmi_rxx_decision_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_5_63                : 59;
+	uint64_t cnt                          : 5;  /**< The byte count used to decide when to accept or filter a packet. Refer to GMI Decisions. */
+#else
+	uint64_t cnt                          : 5;
+	uint64_t reserved_5_63                : 59;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_gmi_rxx_decision_s cn78xx;
+};
+typedef union cvmx_bgxx_gmp_gmi_rxx_decision cvmx_bgxx_gmp_gmi_rxx_decision_t;
+
+/**
+ * cvmx_bgx#_gmp_gmi_rx#_frm_chk
+ */
+union cvmx_bgxx_gmp_gmi_rxx_frm_chk {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_gmi_rxx_frm_chk_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t skperr                       : 1;  /**< Skipper error. */
+	uint64_t rcverr                       : 1;  /**< Frame was received with data-reception error. */
+	uint64_t reserved_5_6                 : 2;
+	uint64_t fcserr                       : 1;  /**< Frame was received with FCS/CRC error. */
+	uint64_t jabber                       : 1;  /**< Frame was received with length > sys_length. */
+	uint64_t reserved_2_2                 : 1;
+	uint64_t carext                       : 1;  /**< Carrier extend error. SGMII/1000Base-X only. */
+	uint64_t minerr                       : 1;  /**< PAUSE frame was received with length < minFrameSize. */
+#else
+	uint64_t minerr                       : 1;
+	uint64_t carext                       : 1;
+	uint64_t reserved_2_2                 : 1;
+	uint64_t jabber                       : 1;
+	uint64_t fcserr                       : 1;
+	uint64_t reserved_5_6                 : 2;
+	uint64_t rcverr                       : 1;
+	uint64_t skperr                       : 1;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_gmi_rxx_frm_chk_s cn78xx;
+};
+typedef union cvmx_bgxx_gmp_gmi_rxx_frm_chk cvmx_bgxx_gmp_gmi_rxx_frm_chk_t;
+
+/**
+ * cvmx_bgx#_gmp_gmi_rx#_frm_ctl
+ *
+ * This register controls the handling of the frames.
+ * The CTL_BCK/CTL_DRP bits control how the hardware handles incoming PAUSE packets. The most
+ * common modes of operation:
+ * _ CTL_BCK = 1, CTL_DRP = 1: hardware handles everything.
+ * _ CTL_BCK = 0, CTL_DRP = 0: software sees all PAUSE frames.
+ * _ CTL_BCK = 0, CTL_DRP = 1: all PAUSE frames are completely ignored.
+ *
+ * These control bits should be set to CTL_BCK = 0,CTL_DRP = 0 in half-duplex mode. Since PAUSE
+ * packets only apply to full duplex operation, any PAUSE packet would constitute an exception
+ * which should be handled by the processing cores. PAUSE packets should not be forwarded.
+ *
+ * INTERNAL: Notes:
+ * PRE_STRP
+ * When PRE_CHK is set (indicating that the PREAMBLE will be sent), PRE_STRP
+ * determines if the PREAMBLE+SFD bytes are thrown away or sent to the Octane
+ * core as part of the packet.
+ * In either mode, the PREAMBLE+SFD bytes are not counted toward the packet
+ * size when checking against the MIN and MAX bounds.  Furthermore, the bytes
+ * are skipped when locating the start of the L2 header for DMAC and Control
+ * frame recognition.
+ */
+union cvmx_bgxx_gmp_gmi_rxx_frm_ctl {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_gmi_rxx_frm_ctl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_13_63               : 51;
+	uint64_t ptp_mode                     : 1;  /**< Timestamp mode. When PTP_MODE is set, a 64-bit timestamp is prepended to every incoming
+                                                         packet.
+                                                         The timestamp bytes are added to the packet in such a way as to not modify the packet's
+                                                         receive byte count. This implies that the BGX()_GMP_GMI_RX()_JABBER,
+                                                         BGX()_GMP_GMI_RX()_DECISION, BGX()_GMP_GMI_RX()_UDD_SKP, and
+                                                         BGX()_CMR()_RX_STAT0..BGX()_CMR()_RX_STAT8
+                                                         do not require any adjustment as they operate on the received
+                                                         packet size. When the packet reaches PKI, its size reflects the additional bytes and is
+                                                         subject to the following restrictions:
+                                                         If PTP_MODE = 1 and PRE_CHK = 1, PRE_STRP must be 1.
+                                                         If PTP_MODE = 1:
+                                                         * PKI_CL()_PKIND()_SKIP[FCS_SKIP,INST_SKIP] should be increased by 8.
+                                                         * PKI_CL()_PKIND()_CFG[HG_EN] should be 0.
+                                                         * PKI_FRM_LEN_CHK()[MAXLEN] should be increased by 8.
+                                                         * PKI_FRM_LEN_CHK()[MINLEN] should be increased by 8.
+                                                         * PKI_TAG_INC()_MASK[EN] should be adjusted. */
+	uint64_t reserved_11_11               : 1;
+	uint64_t null_dis                     : 1;  /**< When set, do not modify the MOD bits on NULL ticks due to partial packets. */
+	uint64_t pre_align                    : 1;  /**< When set, PREAMBLE parser aligns the SFD byte regardless of the number of previous
+                                                         PREAMBLE nibbles. In this mode, PRE_STRP should be set to account for the variable nature
+                                                         of the PREAMBLE. PRE_CHK must be set to enable this and all PREAMBLE features.
+                                                         SGMII at 10/100Mbs only. */
+	uint64_t reserved_7_8                 : 2;
+	uint64_t pre_free                     : 1;  /**< When set, PREAMBLE checking is less strict. GMI will begin the frame at the first SFD.
+                                                         PRE_CHK must be set to enable this and all PREAMBLE features. SGMII/1000Base-X only. */
+	uint64_t ctl_smac                     : 1;  /**< Control PAUSE frames can match station SMAC. */
+	uint64_t ctl_mcst                     : 1;  /**< Control PAUSE frames can match globally assign multicast address. */
+	uint64_t ctl_bck                      : 1;  /**< Forward PAUSE information to TX block. */
+	uint64_t ctl_drp                      : 1;  /**< Drop control-PAUSE frames. */
+	uint64_t pre_strp                     : 1;  /**< Strip off the preamble (when present).
+                                                         0 = PREAMBLE + SFD is sent to core as part of frame.
+                                                         1 = PREAMBLE + SFD is dropped.
+                                                         [PRE_CHK] must be set to enable this and all PREAMBLE features.
+                                                         If PTP_MODE=1 and PRE_CHK=1, PRE_STRP must be 1.
+                                                         When PRE_CHK is set (indicating that the PREAMBLE will be sent), PRE_STRP determines if
+                                                         the PREAMBLE+SFD bytes are thrown away or sent to the core as part of the packet. In
+                                                         either mode, the PREAMBLE+SFD bytes are not counted toward the packet size when checking
+                                                         against the MIN and MAX bounds. Furthermore, the bytes are skipped when locating the start
+                                                         of the L2 header for DMAC and Control frame recognition. */
+	uint64_t pre_chk                      : 1;  /**< Check the preamble for correctness. This port is configured to send a valid 802.3 PREAMBLE
+                                                         to begin every frame. GMI checks that a valid PREAMBLE is received (based on PRE_FREE).
+                                                         When a problem does occur within the PREAMBLE sequence, the frame is marked as bad and not
+                                                         sent into the core. The BGX()_SMU()_RX_INT[PCTERR] interrupt is also raised.
+                                                         When BGX()_SMU()_TX_CTL[HG_EN] is set, PRE_CHK must be 0. If PTP_MODE = 1 and
+                                                         PRE_CHK = 1, PRE_STRP must be 1. */
+#else
+	uint64_t pre_chk                      : 1;
+	uint64_t pre_strp                     : 1;
+	uint64_t ctl_drp                      : 1;
+	uint64_t ctl_bck                      : 1;
+	uint64_t ctl_mcst                     : 1;
+	uint64_t ctl_smac                     : 1;
+	uint64_t pre_free                     : 1;
+	uint64_t reserved_7_8                 : 2;
+	uint64_t pre_align                    : 1;
+	uint64_t null_dis                     : 1;
+	uint64_t reserved_11_11               : 1;
+	uint64_t ptp_mode                     : 1;
+	uint64_t reserved_13_63               : 51;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_gmi_rxx_frm_ctl_s cn78xx;
+};
+typedef union cvmx_bgxx_gmp_gmi_rxx_frm_ctl cvmx_bgxx_gmp_gmi_rxx_frm_ctl_t;
+
+/**
+ * cvmx_bgx#_gmp_gmi_rx#_ifg
+ *
+ * This register specifies the minimum number of interframe-gap (IFG) cycles between packets.
+ *
+ */
+union cvmx_bgxx_gmp_gmi_rxx_ifg {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_gmi_rxx_ifg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t ifg                          : 4;  /**< Min IFG (in IFG * 8 bits) between packets used to determine IFGERR. Normally IFG is 96
+                                                         bits.
+                                                         Note that in some operating modes, IFG cycles can be inserted or removed in order to
+                                                         achieve clock rate adaptation. For these reasons, the default value is slightly
+                                                         conservative and does not check up to the full 96 bits of IFG.
+                                                         (SGMII/1000Base-X only) */
+#else
+	uint64_t ifg                          : 4;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_gmi_rxx_ifg_s    cn78xx;
+};
+typedef union cvmx_bgxx_gmp_gmi_rxx_ifg cvmx_bgxx_gmp_gmi_rxx_ifg_t;
+
+/**
+ * cvmx_bgx#_gmp_gmi_rx#_int
+ *
+ * "These registers allow interrupts to be sent to the control processor.
+ * * Exception conditions <10:0> can also set the rcv/opcode in the received packet's work-queue
+ * entry. BGX()_GMP_GMI_RX()_FRM_CHK provides a bit mask for configuring which conditions
+ * set the error.
+ * In half duplex operation, the expectation is that collisions will appear as either MINERR or
+ * CAREXT errors.
+ *
+ * INTERNAL: Notes:
+ * (2) exception conditions 10:0 can also set the rcv/opcode in the received
+ * packet's workQ entry.  The BGX()_GMP_GMI_RX()_FRM_CHK register provides a bit mask
+ * for configuring which conditions set the error.
+ *
+ * (3) in half duplex operation, the expectation is that collisions will appear
+ * as either MINERR o r CAREXT errors.
+ *
+ * (4) JABBER An RX Jabber error indicates that a packet was received which
+ * is longer than the maximum allowed packet as defined by the
+ * system.  GMI will truncate the packet at the JABBER count.
+ * Failure to do so could lead to system instabilty.
+ *
+ * (5) NIBERR This error is illegal at 1000Mbs speeds
+ * (BGX()_GMP_GMI_RX()_PRT_CFG[SPEED]==0) and will never assert.
+ *
+ * (6) MAXERR for untagged frames, the total frame DA+SA+TL+DATA+PAD+FCS >
+ * BGX()_GMP_GMI_RX()_FRM_MAX.  For tagged frames, DA+SA+VLAN+TL+DATA+PAD+FCS
+ * > BGX()_GMP_GMI_RX()_FRM_MAX + 4*VLAN_VAL + 4*VLAN_STACKED.
+ *
+ * (7) MINERR total frame DA+SA+TL+DATA+PAD+FCS < 64
+ *
+ * (8) ALNERR Indicates that the packet received was not an integer number of
+ * bytes.  If FCS checking is enabled, ALNERR will only assert if
+ * the FCS is bad.  If FCS checking is disabled, ALNERR will
+ * assert in all non-integer frame cases.
+ *
+ * (9) Collisions Collisions can only occur in half-duplex mode.  A collision
+ * is assumed by the receiver when the slottime
+ * (BGX()_GMP_GMI_PRT_CFG[SLOTTIME]) is not satisfied.  In 10/100 mode,
+ * this will result in a frame < SLOTTIME.  In 1000 mode, it
+ * could result either in frame < SLOTTIME or a carrier extend
+ * error with the SLOTTIME.  These conditions are visible by...
+ * . transfer ended before slottime COLDET
+ * . carrier extend error           CAREXT
+ *
+ * (A) LENERR Length errors occur when the received packet does not match the
+ * length field.  LENERR is only checked for packets between 64
+ * and 1500 bytes.  For untagged frames, the length must exact
+ * match.  For tagged frames the length or length+4 must match.
+ *
+ * (B) PCTERR checks that the frame begins with a valid PREAMBLE sequence.
+ * Does not check the number of PREAMBLE cycles.
+ *
+ * (C) OVRERR
+ * OVRERR is an architectural assertion check internal to GMI to
+ * make sure no assumption was violated.  In a correctly operating
+ * system, this interrupt can never fire.
+ * GMI has an internal arbiter which selects which of 4 ports to
+ * buffer in the main RX FIFO.  If we normally buffer 8 bytes,
+ * then each port will typically push a tick every 8 cycles if
+ * the packet interface is going as fast as possible.  If there
+ * are four ports, they push every two cycles.  So that's the
+ * assumption.  That the inbound module will always be able to
+ * consume the tick before another is produced.  If that doesn't
+ * happen that's when OVRERR will assert."
+ */
+union cvmx_bgxx_gmp_gmi_rxx_int {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_gmi_rxx_int_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_12_63               : 52;
+	uint64_t ifgerr                       : 1;  /**< Interframe gap violation. Does not necessarily indicate a failure. SGMII/1000Base-X only. */
+	uint64_t coldet                       : 1;  /**< Collision detection. Collisions can only occur in half-duplex mode. A collision is assumed
+                                                         by the receiver when the slottime (BGX()_GMP_GMI_PRT()_CFG[SLOTTIME]) is not
+                                                         satisfied. In 10/100 mode, this will result in a frame < SLOTTIME. In 1000 mode, it could
+                                                         result either in frame < SLOTTIME or a carrier extend error with the SLOTTIME. These
+                                                         conditions are visible by 1) transfer ended before slottime - COLDET or 2) carrier extend
+                                                         error - CAREXT. */
+	uint64_t falerr                       : 1;  /**< False-carrier error, or carrier-extend error after slottime is satisfied. SGMII/1000Base-X only. */
+	uint64_t rsverr                       : 1;  /**< Reserved opcode. */
+	uint64_t pcterr                       : 1;  /**< Bad preamble/protocol error. Checks that the frame begins with a valid PREAMBLE sequence.
+                                                         Does not check the number of PREAMBLE cycles. */
+	uint64_t ovrerr                       : 1;  /**< Internal data aggregation overflow. This interrupt should never assert. SGMII/1000Base-X only. */
+	uint64_t skperr                       : 1;  /**< Skipper error. */
+	uint64_t rcverr                       : 1;  /**< Data-reception error. Frame was received with data-reception error. */
+	uint64_t fcserr                       : 1;  /**< FCS/CRC error. Frame was received with FCS/CRC error. */
+	uint64_t jabber                       : 1;  /**< System-length error: frame was received with length > sys_length.
+                                                         An RX Jabber error indicates that a packet was received which is longer than the maximum
+                                                         allowed packet as defined by the system. GMI truncates the packet at the JABBER count.
+                                                         Failure to do so could lead to system instability. */
+	uint64_t carext                       : 1;  /**< Carrier-extend error. (SGMII/1000Base-X only) */
+	uint64_t minerr                       : 1;  /**< PAUSE frame was received with length < minFrameSize. Frame length checks are typically
+                                                         handled in PKI, but PAUSE frames are normally discarded before being inspected by PKI.
+                                                         Total frame DA+SA+TL+DATA+PAD+FCS < 64. */
+#else
+	uint64_t minerr                       : 1;
+	uint64_t carext                       : 1;
+	uint64_t jabber                       : 1;
+	uint64_t fcserr                       : 1;
+	uint64_t rcverr                       : 1;
+	uint64_t skperr                       : 1;
+	uint64_t ovrerr                       : 1;
+	uint64_t pcterr                       : 1;
+	uint64_t rsverr                       : 1;
+	uint64_t falerr                       : 1;
+	uint64_t coldet                       : 1;
+	uint64_t ifgerr                       : 1;
+	uint64_t reserved_12_63               : 52;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_gmi_rxx_int_s    cn78xx;
+};
+typedef union cvmx_bgxx_gmp_gmi_rxx_int cvmx_bgxx_gmp_gmi_rxx_int_t;
+
+/**
+ * cvmx_bgx#_gmp_gmi_rx#_jabber
+ *
+ * This register specifies the maximum size for packets, beyond which the GMI truncates.
+ *
+ */
+union cvmx_bgxx_gmp_gmi_rxx_jabber {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_gmi_rxx_jabber_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t cnt                          : 16; /**< Byte count for jabber check. Failing packets set the JABBER interrupt and are optionally
+                                                         sent with opcode = JABBER. GMI truncates the packet to CNT bytes.
+                                                         CNT must be 8-byte aligned such that CNT<2:0> = 000. */
+#else
+	uint64_t cnt                          : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_gmi_rxx_jabber_s cn78xx;
+};
+typedef union cvmx_bgxx_gmp_gmi_rxx_jabber cvmx_bgxx_gmp_gmi_rxx_jabber_t;
+
+/**
+ * cvmx_bgx#_gmp_gmi_rx#_udd_skp
+ *
+ * This register specifies the amount of user-defined data (UDD) added before the start of the
+ * L2C data.
+ *
+ * INTERNAL: Notes:
+ * (1) The skip bytes are part of the packet and will be sent down the NCB
+ * packet interface and will be handled by PKI.
+ *
+ * (2) The system can determine if the UDD bytes are included in the FCS check
+ * by using the FCSSEL field - if the FCS check is enabled.
+ *
+ * (3) Assume that the preamble/sfd is always at the start of the frame - even
+ * before UDD bytes.  In most cases, there will be no preamble in these
+ * cases since it will be packet interface in direct communication to
+ * another packet interface (MAC to MAC) without a PHY involved.
+ *
+ * (4) We can still do address filtering and control packet filtering is the
+ * user desires.
+ *
+ * (5) UDD_SKP must be 0 in half-duplex operation unless
+ * BGX()_GMP_GMI_RX()_FRM_CTL[PRE_CHK] is clear.  If BGX()_GMP_GMI_RX()_FRM_CTL[PRE_CHK] is
+ * clear,
+ * then UDD_SKP will normally be 8.
+ *
+ * (6) In all cases, the UDD bytes will be sent down the packet interface as
+ * part of the packet.  The UDD bytes are never stripped from the actual
+ * packet.
+ */
+union cvmx_bgxx_gmp_gmi_rxx_udd_skp {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_gmi_rxx_udd_skp_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t fcssel                       : 1;  /**< Include the skip bytes in the FCS calculation.
+                                                         0 = All skip bytes are included in FCS.
+                                                         1 = The skip bytes are not included in FCS.
+                                                         When BGX()_GMP_GMI_TX()_CTL[HG_EN] is set, this field must be 0.
+                                                         The skip bytes are part of the packet and are sent down the NCB packet interface and are
+                                                         handled by PKI. The system can determine if the UDD bytes are included in the FCS check by
+                                                         using the FCSSEL field, if the FCS check is enabled. */
+	uint64_t reserved_7_7                 : 1;
+	uint64_t len                          : 7;  /**< Amount of user-defined data before the start of the L2C data, in bytes.
+                                                         Setting to 0 means L2C comes first; maximum value is 64.
+                                                         LEN must be 0x0 in half-duplex operation.
+                                                         If LEN != 0, then BGX()_GMP_GMI_RX()_FRM_CHK[MINERR] will be disabled and
+                                                         BGX()_GMP_GMI_RX()_INT[MINERR] will be zero. */
+#else
+	uint64_t len                          : 7;
+	uint64_t reserved_7_7                 : 1;
+	uint64_t fcssel                       : 1;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_gmi_rxx_udd_skp_s cn78xx;
+};
+typedef union cvmx_bgxx_gmp_gmi_rxx_udd_skp cvmx_bgxx_gmp_gmi_rxx_udd_skp_t;
+
+/**
+ * cvmx_bgx#_gmp_gmi_smac#
+ */
+union cvmx_bgxx_gmp_gmi_smacx {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_gmi_smacx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t smac                         : 48; /**< The SMAC field is used for generating and accepting control PAUSE packets. */
+#else
+	uint64_t smac                         : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_gmi_smacx_s      cn78xx;
+};
+typedef union cvmx_bgxx_gmp_gmi_smacx cvmx_bgxx_gmp_gmi_smacx_t;
+
+/**
+ * cvmx_bgx#_gmp_gmi_tx#_append
+ */
+union cvmx_bgxx_gmp_gmi_txx_append {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_gmi_txx_append_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t force_fcs                    : 1;  /**< Append the Ethernet FCS on each PAUSE packet. PAUSE packets are normally padded to 60
+                                                         bytes. If BGX()_GMP_GMI_TX()_MIN_PKT[MIN_SIZE] exceeds 59, then FCS_C is not used. */
+	uint64_t fcs                          : 1;  /**< Append the Ethernet FCS on each packet. */
+	uint64_t pad                          : 1;  /**< Append PAD bytes such that minimum-sized packet is transmitted. */
+	uint64_t preamble                     : 1;  /**< Prepend the Ethernet preamble on each transfer. */
+#else
+	uint64_t preamble                     : 1;
+	uint64_t pad                          : 1;
+	uint64_t fcs                          : 1;
+	uint64_t force_fcs                    : 1;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_gmi_txx_append_s cn78xx;
+};
+typedef union cvmx_bgxx_gmp_gmi_txx_append cvmx_bgxx_gmp_gmi_txx_append_t;
+
+/**
+ * cvmx_bgx#_gmp_gmi_tx#_burst
+ */
+union cvmx_bgxx_gmp_gmi_txx_burst {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_gmi_txx_burst_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t burst                        : 16; /**< Burst (refer to 802.3 to set correctly). Only valid for 1000Mb/s half-duplex operation as
+                                                         follows:
+                                                         half duplex/1000Mb/s: 0x2000
+                                                         all other modes: 0x0
+                                                         SGMII/1000Base-X only. */
+#else
+	uint64_t burst                        : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_gmi_txx_burst_s  cn78xx;
+};
+typedef union cvmx_bgxx_gmp_gmi_txx_burst cvmx_bgxx_gmp_gmi_txx_burst_t;
+
+/**
+ * cvmx_bgx#_gmp_gmi_tx#_ctl
+ */
+union cvmx_bgxx_gmp_gmi_txx_ctl {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_gmi_txx_ctl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_2_63                : 62;
+	uint64_t xsdef_en                     : 1;  /**< Enables the excessive-deferral check for statistics and interrupts. SGMII/1000Base-X half-
+                                                         duplex only. */
+	uint64_t xscol_en                     : 1;  /**< Enables the excessive-collision check for statistics and interrupts. SGMII/1000Base-X
+                                                         half-duplex only. */
+#else
+	uint64_t xscol_en                     : 1;
+	uint64_t xsdef_en                     : 1;
+	uint64_t reserved_2_63                : 62;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_gmi_txx_ctl_s    cn78xx;
+};
+typedef union cvmx_bgxx_gmp_gmi_txx_ctl cvmx_bgxx_gmp_gmi_txx_ctl_t;
+
+/**
+ * cvmx_bgx#_gmp_gmi_tx#_int
+ */
+union cvmx_bgxx_gmp_gmi_txx_int {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_gmi_txx_int_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_5_63                : 59;
+	uint64_t ptp_lost                     : 1;  /**< A packet with a PTP request was not able to be sent due to XSCOL. */
+	uint64_t late_col                     : 1;  /**< TX late collision. (SGMII/1000BASE-X half-duplex only) */
+	uint64_t xsdef                        : 1;  /**< TX excessive deferral. (SGMII/1000BASE-X half-duplex only) */
+	uint64_t xscol                        : 1;  /**< TX excessive collisions. (SGMII/1000BASE-X half-duplex only) */
+	uint64_t undflw                       : 1;  /**< TX underflow. */
+#else
+	uint64_t undflw                       : 1;
+	uint64_t xscol                        : 1;
+	uint64_t xsdef                        : 1;
+	uint64_t late_col                     : 1;
+	uint64_t ptp_lost                     : 1;
+	uint64_t reserved_5_63                : 59;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_gmi_txx_int_s    cn78xx;
+};
+typedef union cvmx_bgxx_gmp_gmi_txx_int cvmx_bgxx_gmp_gmi_txx_int_t;
+
+/**
+ * cvmx_bgx#_gmp_gmi_tx#_min_pkt
+ */
+union cvmx_bgxx_gmp_gmi_txx_min_pkt {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_gmi_txx_min_pkt_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_8_63                : 56;
+	uint64_t min_size                     : 8;  /**< Minimum frame size in bytes before the FCS is applied.
+                                                         Padding is only appended when BGX()_GMP_GMI_TX()_APPEND[PAD] for the corresponding
+                                                         LMAC is set.
+                                                         In SGMII mode, packets are padded to MIN_SIZE+1. The reset value pads to 60 bytes. */
+#else
+	uint64_t min_size                     : 8;
+	uint64_t reserved_8_63                : 56;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_gmi_txx_min_pkt_s cn78xx;
+};
+typedef union cvmx_bgxx_gmp_gmi_txx_min_pkt cvmx_bgxx_gmp_gmi_txx_min_pkt_t;
+
+/**
+ * cvmx_bgx#_gmp_gmi_tx#_pause_pkt_interval
+ *
+ * This register specifies how often PAUSE packets are sent.
+ *
+ * INTERNAL: Notes:
+ * Choosing proper values of BGX()_GMP_GMI_TX_PAUSE_PKT_TIME[PTIME] and
+ * BGX()_GMP_GMI_TX_PAUSE_PKT_INTERVAL[INTERVAL] can be challenging to the system
+ * designer.  It is suggested that TIME be much greater than INTERVAL and
+ * BGX()_GMP_GMI_TX_PAUSE_ZERO[SEND] be set.  This allows a periodic refresh of the PAUSE
+ * count and then when the backpressure condition is lifted, a PAUSE packet
+ * with TIME==0 will be sent indicating that Octane is ready for additional
+ * data.
+ *
+ * If the system chooses to not set BGX()_GMP_GMI_TX_PAUSE_ZERO[SEND], then it is
+ * suggested that TIME and INTERVAL are programmed such that they satisify the
+ * following rule:
+ *
+ * _ INTERVAL <= TIME - (largest_pkt_size + IFG + pause_pkt_size)
+ *
+ * where largest_pkt_size is that largest packet that the system can send
+ * (normally 1518B), IFG is the interframe gap and pause_pkt_size is the size
+ * of the PAUSE packet (normally 64B).
+ */
+union cvmx_bgxx_gmp_gmi_txx_pause_pkt_interval {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_gmi_txx_pause_pkt_interval_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t interval                     : 16; /**< Arbitrate for a 802.3 PAUSE packet or CBFC PAUSE packet every (INTERVAL * 512) bit-times.
+                                                         Normally, 0 < INTERVAL < BGX()_GMP_GMI_TX()_PAUSE_PKT_TIME[PTIME].
+                                                         INTERVAL = 0 only sends a single PAUSE packet for each backpressure event. */
+#else
+	uint64_t interval                     : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_gmi_txx_pause_pkt_interval_s cn78xx;
+};
+typedef union cvmx_bgxx_gmp_gmi_txx_pause_pkt_interval cvmx_bgxx_gmp_gmi_txx_pause_pkt_interval_t;
+
+/**
+ * cvmx_bgx#_gmp_gmi_tx#_pause_pkt_time
+ */
+union cvmx_bgxx_gmp_gmi_txx_pause_pkt_time {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_gmi_txx_pause_pkt_time_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t ptime                        : 16; /**< Provides the pause_time field placed in outbound 802.3 PAUSE packets or CBFC PAUSE packets
+                                                         in 512 bit-times. Normally, P_TIME >
+                                                         BGX()_GMP_GMI_TX()_PAUSE_PKT_INTERVAL[INTERVAL]. For programming information see
+                                                         BGX()_GMP_GMI_TX()_PAUSE_PKT_INTERVAL. */
+#else
+	uint64_t ptime                        : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_gmi_txx_pause_pkt_time_s cn78xx;
+};
+typedef union cvmx_bgxx_gmp_gmi_txx_pause_pkt_time cvmx_bgxx_gmp_gmi_txx_pause_pkt_time_t;
+
+/**
+ * cvmx_bgx#_gmp_gmi_tx#_pause_togo
+ */
+union cvmx_bgxx_gmp_gmi_txx_pause_togo {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_gmi_txx_pause_togo_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t ptime                        : 16; /**< Amount of time remaining to backpressure, from the standard 802.3 PAUSE timer. */
+#else
+	uint64_t ptime                        : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_gmi_txx_pause_togo_s cn78xx;
+};
+typedef union cvmx_bgxx_gmp_gmi_txx_pause_togo cvmx_bgxx_gmp_gmi_txx_pause_togo_t;
+
+/**
+ * cvmx_bgx#_gmp_gmi_tx#_pause_zero
+ */
+union cvmx_bgxx_gmp_gmi_txx_pause_zero {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_gmi_txx_pause_zero_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t send                         : 1;  /**< Send PAUSE-zero enable.When this bit is set, and the backpressure condition is clear, it
+                                                         allows sending a PAUSE packet with pause_time of 0 to enable the channel. */
+#else
+	uint64_t send                         : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_gmi_txx_pause_zero_s cn78xx;
+};
+typedef union cvmx_bgxx_gmp_gmi_txx_pause_zero cvmx_bgxx_gmp_gmi_txx_pause_zero_t;
+
+/**
+ * cvmx_bgx#_gmp_gmi_tx#_sgmii_ctl
+ */
+union cvmx_bgxx_gmp_gmi_txx_sgmii_ctl {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_gmi_txx_sgmii_ctl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t align                        : 1;  /**< Align the transmission to even cycles: (SGMII/1000BASE-X half-duplex only)
+                                                         Recommended value is: ALIGN = !BGX()_GMP_GMI_TX()_APPEND[PREAMBLE].
+                                                         (See Transmit Conversion to Code groups, Transmit Conversion to Code Groups for a complete
+                                                         discussion.)
+                                                         _ 0 = Data can be sent on any cycle. In this mode, the interface functions at maximum
+                                                         bandwidth. It is possible for the TX PCS machine to drop the first byte of the TX frame.
+                                                         When BGX()_GMP_GMI_TX()_APPEND[PREAMBLE] is set, the first byte is a preamble
+                                                         byte, which can be dropped to compensate for an extended IPG.
+                                                         _ 1 = Data is only sent on even cycles. In this mode, there can be bandwidth implications
+                                                         when sending odd-byte packets as the IPG can extend an extra cycle. There will be no loss
+                                                         of data. */
+#else
+	uint64_t align                        : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_gmi_txx_sgmii_ctl_s cn78xx;
+};
+typedef union cvmx_bgxx_gmp_gmi_txx_sgmii_ctl cvmx_bgxx_gmp_gmi_txx_sgmii_ctl_t;
+
+/**
+ * cvmx_bgx#_gmp_gmi_tx#_slot
+ */
+union cvmx_bgxx_gmp_gmi_txx_slot {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_gmi_txx_slot_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_10_63               : 54;
+	uint64_t slot                         : 10; /**< Slottime (refer to Std 802.3 to set correctly):
+                                                         10/100 Mbs: Set SLOT to 0x40.
+                                                         1000 Mbs: Set SLOT to 0x200.
+                                                         SGMII/1000Base-X only. */
+#else
+	uint64_t slot                         : 10;
+	uint64_t reserved_10_63               : 54;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_gmi_txx_slot_s   cn78xx;
+};
+typedef union cvmx_bgxx_gmp_gmi_txx_slot cvmx_bgxx_gmp_gmi_txx_slot_t;
+
+/**
+ * cvmx_bgx#_gmp_gmi_tx#_soft_pause
+ */
+union cvmx_bgxx_gmp_gmi_txx_soft_pause {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_gmi_txx_soft_pause_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t ptime                        : 16; /**< Back off the TX bus for (PTIME * 512) bit-times. */
+#else
+	uint64_t ptime                        : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_gmi_txx_soft_pause_s cn78xx;
+};
+typedef union cvmx_bgxx_gmp_gmi_txx_soft_pause cvmx_bgxx_gmp_gmi_txx_soft_pause_t;
+
+/**
+ * cvmx_bgx#_gmp_gmi_tx#_thresh
+ */
+union cvmx_bgxx_gmp_gmi_txx_thresh {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_gmi_txx_thresh_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_11_63               : 53;
+	uint64_t cnt                          : 11; /**< Number of 128-bit words to accumulate in the TX FIFO before sending on the packet
+                                                         interface. This field should be large enough to prevent underflow on the packet interface
+                                                         and must never be set to 0x0.
+                                                         10G/40G Mode, CNT = 0x100. In all modes, this register cannot exceed the TX FIFO depth as
+                                                         follows:
+                                                         _ BGX()_CMR_TX_LMACS = 0,1:  CNT maximum = 0x7FF.
+                                                         _ BGX()_CMR_TX_LMACS = 2:    CNT maximum = 0x3FF.
+                                                         _ BGX()_CMR_TX_LMACS = 3,4:  CNT maximum = 0x1FF. */
+#else
+	uint64_t cnt                          : 11;
+	uint64_t reserved_11_63               : 53;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_gmi_txx_thresh_s cn78xx;
+};
+typedef union cvmx_bgxx_gmp_gmi_txx_thresh cvmx_bgxx_gmp_gmi_txx_thresh_t;
+
+/**
+ * cvmx_bgx#_gmp_gmi_tx_col_attempt
+ */
+union cvmx_bgxx_gmp_gmi_tx_col_attempt {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_gmi_tx_col_attempt_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_5_63                : 59;
+	uint64_t limit                        : 5;  /**< Number of collision attempts allowed. (SGMII/1000BASE-X half-duplex only.) */
+#else
+	uint64_t limit                        : 5;
+	uint64_t reserved_5_63                : 59;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_gmi_tx_col_attempt_s cn78xx;
+};
+typedef union cvmx_bgxx_gmp_gmi_tx_col_attempt cvmx_bgxx_gmp_gmi_tx_col_attempt_t;
+
+/**
+ * cvmx_bgx#_gmp_gmi_tx_ifg
+ *
+ * Consider the following when programming IFG1 and IFG2:
+ * * For 10/100/1000 Mb/s half-duplex systems that require IEEE 802.3 compatibility, IFG1 must be
+ * in the range of 1-8, IFG2 must be in the range of 4-12, and the IFG1 + IFG2 sum must be 12.
+ * * For 10/100/1000 Mb/s full-duplex systems that require IEEE 802.3 compatibility, IFG1 must be
+ * in the range of 1-11, IFG2 must be in the range of 1-11, and the IFG1 + IFG2 sum must be 12.
+ * For all other systems, IFG1 and IFG2 can be any value in the range of 1-15, allowing for a
+ * total possible IFG sum of 2-30.
+ */
+union cvmx_bgxx_gmp_gmi_tx_ifg {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_gmi_tx_ifg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_8_63                : 56;
+	uint64_t ifg2                         : 4;  /**< Remainder of interFrameGap timing, equal to interFrameGap - IFG1 (in IFG2 * 8 bits). If
+                                                         CRS is detected during IFG2, the interFrameSpacing timer is not reset and a frame is
+                                                         transmitted once the timer expires. */
+	uint64_t ifg1                         : 4;  /**< First portion of interFrameGap timing, in the range of 0 to 2/3 (in IFG2 * 8 bits). If CRS
+                                                         is detected during IFG1, the interFrameSpacing timer is reset and a frame is not
+                                                         transmitted. */
+#else
+	uint64_t ifg1                         : 4;
+	uint64_t ifg2                         : 4;
+	uint64_t reserved_8_63                : 56;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_gmi_tx_ifg_s     cn78xx;
+};
+typedef union cvmx_bgxx_gmp_gmi_tx_ifg cvmx_bgxx_gmp_gmi_tx_ifg_t;
+
+/**
+ * cvmx_bgx#_gmp_gmi_tx_jam
+ *
+ * This register provides the pattern used in JAM bytes.
+ *
+ */
+union cvmx_bgxx_gmp_gmi_tx_jam {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_gmi_tx_jam_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_8_63                : 56;
+	uint64_t jam                          : 8;  /**< JAM pattern. (SGMII/1000BASE-X half-duplex only.) */
+#else
+	uint64_t jam                          : 8;
+	uint64_t reserved_8_63                : 56;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_gmi_tx_jam_s     cn78xx;
+};
+typedef union cvmx_bgxx_gmp_gmi_tx_jam cvmx_bgxx_gmp_gmi_tx_jam_t;
+
+/**
+ * cvmx_bgx#_gmp_gmi_tx_lfsr
+ *
+ * This register shows the contents of the linear feedback shift register (LFSR), which is used
+ * to implement truncated binary exponential backoff.
+ */
+union cvmx_bgxx_gmp_gmi_tx_lfsr {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_gmi_tx_lfsr_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t lfsr                         : 16; /**< Contains the current state of the LFSR, which is used to feed random numbers to compute
+                                                         truncated binary exponential backoff. (SGMII/1000Base-X half-duplex only.) */
+#else
+	uint64_t lfsr                         : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_gmi_tx_lfsr_s    cn78xx;
+};
+typedef union cvmx_bgxx_gmp_gmi_tx_lfsr cvmx_bgxx_gmp_gmi_tx_lfsr_t;
+
+/**
+ * cvmx_bgx#_gmp_gmi_tx_pause_pkt_dmac
+ */
+union cvmx_bgxx_gmp_gmi_tx_pause_pkt_dmac {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_gmi_tx_pause_pkt_dmac_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t dmac                         : 48; /**< The DMAC field, which is placed is outbound PAUSE packets. */
+#else
+	uint64_t dmac                         : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_gmi_tx_pause_pkt_dmac_s cn78xx;
+};
+typedef union cvmx_bgxx_gmp_gmi_tx_pause_pkt_dmac cvmx_bgxx_gmp_gmi_tx_pause_pkt_dmac_t;
+
+/**
+ * cvmx_bgx#_gmp_gmi_tx_pause_pkt_type
+ *
+ * This register provides the PTYPE field that is placed in outbound PAUSE packets.
+ *
+ */
+union cvmx_bgxx_gmp_gmi_tx_pause_pkt_type {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_gmi_tx_pause_pkt_type_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t ptype                        : 16; /**< The PTYPE field placed in outbound PAUSE packets. */
+#else
+	uint64_t ptype                        : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_gmi_tx_pause_pkt_type_s cn78xx;
+};
+typedef union cvmx_bgxx_gmp_gmi_tx_pause_pkt_type cvmx_bgxx_gmp_gmi_tx_pause_pkt_type_t;
+
+/**
+ * cvmx_bgx#_gmp_pcs_an#_adv
+ */
+union cvmx_bgxx_gmp_pcs_anx_adv {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_pcs_anx_adv_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t np                           : 1;  /**< Next page capable. This feature is not supported; this field is always 0. */
+	uint64_t reserved_14_14               : 1;
+	uint64_t rem_flt                      : 2;  /**< Remote fault.
+                                                         0x0 = Link OK, XMIT = DATA.
+                                                         0x1 = Link failure (loss of sync, XMIT !=DATA).
+                                                         0x2 = Local device offline.
+                                                         0x3 = Auto-Negotiation error; failure to complete Auto-Negotiation. AN error is set if
+                                                         resolution function precludes operation with link partner. */
+	uint64_t reserved_9_11                : 3;
+	uint64_t pause                        : 2;  /**< PAUSE frame flow capability across link, exchanged during Auto-Negotiation as follows:
+                                                         0x0 = No PAUSE.
+                                                         0x1 = Symmetric PAUSE.
+                                                         0x2 = Asymmetric PAUSE.
+                                                         0x3 = Both symmetric and asymmetric PAUSE to local device. */
+	uint64_t hfd                          : 1;  /**< Half-duplex. When set, local device is half-duplex capable. */
+	uint64_t fd                           : 1;  /**< Full-duplex. When set, local device is full-duplex capable. */
+	uint64_t reserved_0_4                 : 5;
+#else
+	uint64_t reserved_0_4                 : 5;
+	uint64_t fd                           : 1;
+	uint64_t hfd                          : 1;
+	uint64_t pause                        : 2;
+	uint64_t reserved_9_11                : 3;
+	uint64_t rem_flt                      : 2;
+	uint64_t reserved_14_14               : 1;
+	uint64_t np                           : 1;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_pcs_anx_adv_s    cn78xx;
+};
+typedef union cvmx_bgxx_gmp_pcs_anx_adv cvmx_bgxx_gmp_pcs_anx_adv_t;
+
+/**
+ * cvmx_bgx#_gmp_pcs_an#_ext_st
+ */
+union cvmx_bgxx_gmp_pcs_anx_ext_st {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_pcs_anx_ext_st_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t thou_xfd                     : 1;  /**< When set, PHY is 1000 BASE-X full duplex capable. */
+	uint64_t thou_xhd                     : 1;  /**< When set, PHY is 1000 BASE-X half duplex capable. */
+	uint64_t thou_tfd                     : 1;  /**< When set, PHY is 1000 BASE-T full duplex capable. */
+	uint64_t thou_thd                     : 1;  /**< When set, PHY is 1000 BASE-T half duplex capable. */
+	uint64_t reserved_0_11                : 12;
+#else
+	uint64_t reserved_0_11                : 12;
+	uint64_t thou_thd                     : 1;
+	uint64_t thou_tfd                     : 1;
+	uint64_t thou_xhd                     : 1;
+	uint64_t thou_xfd                     : 1;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_pcs_anx_ext_st_s cn78xx;
+};
+typedef union cvmx_bgxx_gmp_pcs_anx_ext_st cvmx_bgxx_gmp_pcs_anx_ext_st_t;
+
+/**
+ * cvmx_bgx#_gmp_pcs_an#_lp_abil
+ *
+ * This is the Auto-Negotiation Link partner ability register 5 as per IEEE 802.3, Clause 37.
+ *
+ */
+union cvmx_bgxx_gmp_pcs_anx_lp_abil {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_pcs_anx_lp_abil_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t np                           : 1;  /**< 0 = Link partner not next page capable.
+                                                         1 = Link partner next page capable. */
+	uint64_t ack                          : 1;  /**< When set, indicates acknowledgement received. */
+	uint64_t rem_flt                      : 2;  /**< Link partner's link status as follows:
+                                                         0x0 = Link OK.
+                                                         0x1 = Offline.
+                                                         0x2 = Link failure.
+                                                         0x3 = Auto-Negotiation error. */
+	uint64_t reserved_9_11                : 3;
+	uint64_t pause                        : 2;  /**< Link partner PAUSE setting as follows:
+                                                         0x0 = No PAUSE.
+                                                         0x1 = Symmetric PAUSE.
+                                                         0x2 = Asymmetric PAUSE.
+                                                         0x3 = Both symmetric and asymmetric PAUSE to local device. */
+	uint64_t hfd                          : 1;  /**< Half-duplex. When set, link partner is half-duplex capable. */
+	uint64_t fd                           : 1;  /**< Full-duplex. When set, link partner is full-duplex capable. */
+	uint64_t reserved_0_4                 : 5;
+#else
+	uint64_t reserved_0_4                 : 5;
+	uint64_t fd                           : 1;
+	uint64_t hfd                          : 1;
+	uint64_t pause                        : 2;
+	uint64_t reserved_9_11                : 3;
+	uint64_t rem_flt                      : 2;
+	uint64_t ack                          : 1;
+	uint64_t np                           : 1;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_pcs_anx_lp_abil_s cn78xx;
+};
+typedef union cvmx_bgxx_gmp_pcs_anx_lp_abil cvmx_bgxx_gmp_pcs_anx_lp_abil_t;
+
+/**
+ * cvmx_bgx#_gmp_pcs_an#_results
+ *
+ * This register is not valid when BGX()_GMP_PCS_MISC()_CTL[AN_OVRD] is set to 1. If
+ * BGX()_GMP_PCS_MISC()_CTL[AN_OVRD] is set to 0 and
+ * BGX()_GMP_PCS_AN()_RESULTS[AN_CPT] is set to 1, this register is valid.
+ */
+union cvmx_bgxx_gmp_pcs_anx_results {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_pcs_anx_results_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_7_63                : 57;
+	uint64_t pause                        : 2;  /**< PAUSE selection ('don't care' for SGMII) as follows:
+                                                         0x0 = Disable PAUSE, TX and RX.
+                                                         0x1 = Enable PAUSE frames, RX only.
+                                                         0x2 = Enable PAUSE frames, TX only.
+                                                         0x3 = Enable PAUSE frames, TX and RX. */
+	uint64_t spd                          : 2;  /**< Link speed selection as follows:
+                                                         0x0 = 10 Mb/s.
+                                                         0x1 = 100 Mb/s.
+                                                         0x2 = 1000 Mb/s.
+                                                         0x3 = Reserved. */
+	uint64_t an_cpt                       : 1;  /**< Auto-Negotiation completed.
+                                                         1 = Auto-Negotiation completed.
+                                                         0 = Auto-Negotiation not completed or failed. */
+	uint64_t dup                          : 1;  /**< Duplex mode. 1 = full duplex, 0 = half duplex. */
+	uint64_t link_ok                      : 1;  /**< Link status: 1 = link up (OK), 1 = link down. */
+#else
+	uint64_t link_ok                      : 1;
+	uint64_t dup                          : 1;
+	uint64_t an_cpt                       : 1;
+	uint64_t spd                          : 2;
+	uint64_t pause                        : 2;
+	uint64_t reserved_7_63                : 57;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_pcs_anx_results_s cn78xx;
+};
+typedef union cvmx_bgxx_gmp_pcs_anx_results cvmx_bgxx_gmp_pcs_anx_results_t;
+
+/**
+ * cvmx_bgx#_gmp_pcs_int#
+ */
+union cvmx_bgxx_gmp_pcs_intx {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_pcs_intx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_13_63               : 51;
+	uint64_t dbg_sync                     : 1;  /**< Code group sync failure debug help. DBG_SYNC interrupt fires when code group
+                                                         synchronization state machine makes a transition from SYNC_ACQUIRED_1 state to
+                                                         SYNC_ACQUIRED_2 state. (See IEEE 802.3-2005, figure 37-9). It indicates that a bad code
+                                                         group was received after code group synchronization was achieved. This interrupt should be
+                                                         disabled during normal link operation. Use it as a debug help feature only. */
+	uint64_t dup                          : 1;  /**< Set whenever duplex mode changes on the link. */
+	uint64_t sync_bad                     : 1;  /**< Set by hardware whenever RX sync state machine reaches a bad state. Should never be set
+                                                         during normal operation. */
+	uint64_t an_bad                       : 1;  /**< Set by hardware whenever Auto-Negotiation state machine reaches a bad state. Should never
+                                                         be set during normal operation. */
+	uint64_t rxlock                       : 1;  /**< Set by hardware whenever code group sync or bit lock failure occurs. Cannot fire in loopback1 mode. */
+	uint64_t rxbad                        : 1;  /**< Set by hardware whenever RX state machine reaches a bad state. Should never be set during
+                                                         normal operation. */
+	uint64_t rxerr                        : 1;  /**< Set whenever RX receives a code group error in 10-bit to 8-bit decode logic. Cannot fire
+                                                         in loopback1 mode. */
+	uint64_t txbad                        : 1;  /**< Set by hardware whenever TX state machine reaches a bad state. Should never be set during
+                                                         normal operation. */
+	uint64_t txfifo                       : 1;  /**< Set whenever hardware detects a TX FIFO overflow condition. */
+	uint64_t txfifu                       : 1;  /**< Set whenever hardware detects a TX FIFO underflow condition. */
+	uint64_t an_err                       : 1;  /**< Auto-Negotiation error; AN resolution function failed. */
+	uint64_t xmit                         : 1;  /**< Set whenever hardware detects a change in the XMIT variable. XMIT variable states are
+                                                         IDLE, CONFIG and DATA. */
+	uint64_t lnkspd                       : 1;  /**< Set by hardware whenever link speed has changed. */
+#else
+	uint64_t lnkspd                       : 1;
+	uint64_t xmit                         : 1;
+	uint64_t an_err                       : 1;
+	uint64_t txfifu                       : 1;
+	uint64_t txfifo                       : 1;
+	uint64_t txbad                        : 1;
+	uint64_t rxerr                        : 1;
+	uint64_t rxbad                        : 1;
+	uint64_t rxlock                       : 1;
+	uint64_t an_bad                       : 1;
+	uint64_t sync_bad                     : 1;
+	uint64_t dup                          : 1;
+	uint64_t dbg_sync                     : 1;
+	uint64_t reserved_13_63               : 51;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_pcs_intx_s       cn78xx;
+};
+typedef union cvmx_bgxx_gmp_pcs_intx cvmx_bgxx_gmp_pcs_intx_t;
+
+/**
+ * cvmx_bgx#_gmp_pcs_link#_timer
+ *
+ * This is the 1.6 ms nominal Link timer register.
+ *
+ */
+union cvmx_bgxx_gmp_pcs_linkx_timer {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_pcs_linkx_timer_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t count                        : 16; /**< (Coprocessor clock period * 1024) * COUNT should be 1.6 ms for SGMII and 10 ms otherwise,
+                                                         which is the link timer used in Auto-Negotiation. Reset assumes a 700 MHz coprocessor
+                                                         clock for 1.6 ms link timer. */
+#else
+	uint64_t count                        : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_pcs_linkx_timer_s cn78xx;
+};
+typedef union cvmx_bgxx_gmp_pcs_linkx_timer cvmx_bgxx_gmp_pcs_linkx_timer_t;
+
+/**
+ * cvmx_bgx#_gmp_pcs_misc#_ctl
+ *
+ * INTERNAL:
+ * SGMII bit [12] is really a misnomer, it is a decode  of pi_qlm_cfg pins to indicate SGMII or
+ * 1000Base-X modes.
+ *
+ * Repeat note from SGM_AN_ADV register
+ * NOTE: The SGMII AN Advertisement Register above will be sent during Auto Negotiation if the
+ * MAC_PHY mode bit in misc_ctl_reg is set (1=PHY mode). If the bit is not set (0=MAC mode), the
+ * tx_config_reg[14] becomes ACK bit and [0] is always 1.
+ * All other bits in tx_config_reg sent will be 0. The PHY dictates the Auto Negotiation results.
+ * SGMII Misc Control Register
+ */
+union cvmx_bgxx_gmp_pcs_miscx_ctl {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_pcs_miscx_ctl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_13_63               : 51;
+	uint64_t sgmii                        : 1;  /**< SGMII mode.
+                                                         0 = other mode selected.
+                                                         1 = SGMII or 1000BASE-X mode selected.
+                                                         See GSER()_LANE_MODE[LMODE]. */
+	uint64_t gmxeno                       : 1;  /**< GMI enable override. When set, forces GMI to appear disabled. The enable/disable status of
+                                                         GMI is checked only at SOP of every packet. */
+	uint64_t loopbck2                     : 1;  /**< Sets external loopback mode to return RX data back out via the TX data path. 0 = No
+                                                         loopback, 1 = Loopback.
+                                                         LOOPBCK1 and LOOPBCK2 modes may not be supported simultaneously. */
+	uint64_t mac_phy                      : 1;  /**< MAC/PHY.
+                                                         0 = MAC.
+                                                         1 = PHY decides the TX_CONFIG_REG value to be sent during Auto-Negotiation. */
+	uint64_t mode                         : 1;  /**< Mode bit.
+                                                         _ 0 = SGMII mode is selected and the following note applies.
+                                                         The SGMII AN advertisement register (BGX()_GMP_PCS_SGM()_AN_ADV) is sent during
+                                                         Auto-Negotiation if BGX()_GMP_PCS_MISC()_CTL[MAC_PHY] = 1 (PHY mode). If [MAC_PHY]
+                                                         = 0 (MAC mode), the TX_CONFIG_REG<14> becomes ACK bit and <0> is always 1. All other bits
+                                                         in TX_CONFIG_REG sent are 0. The PHY dictates the Auto-Negotiation results.
+                                                         _ 1 = 1000Base-X mode is selected. Auto-Negotiation follows IEEE 802.3 clause 37. */
+	uint64_t an_ovrd                      : 1;  /**< Auto-Negotiation results override:
+                                                         0 = Disable.
+                                                         1 = Enable override. Auto-Negotiation is allowed to happen but the results are ignored
+                                                         when this bit is set.  Duplex and Link speed values are set from BGX()_GMP_PCS_MISC()_CTL. */
+	uint64_t samp_pt                      : 7;  /**< Byte number in elongated frames for 10/100 Mb/s operation for data sampling on RX side in
+                                                         PCS. Recommended values are 0x5 for 100 Mb/s operation and 0x32 for 10 Mb/s operation.
+                                                         For 10 Mb/s operation, this field should be set to a value less than 99 and greater than
+                                                         0.
+                                                         If set out of this range, a value of 50 is used for actual sampling internally without
+                                                         affecting the CSR field.
+                                                         For 100 Mb/s operation this field should be set to a value less than 9 and greater than 0.
+                                                         If set out of this range, a value of 5 is used for actual sampling internally without
+                                                         affecting the CSR field. */
+#else
+	uint64_t samp_pt                      : 7;
+	uint64_t an_ovrd                      : 1;
+	uint64_t mode                         : 1;
+	uint64_t mac_phy                      : 1;
+	uint64_t loopbck2                     : 1;
+	uint64_t gmxeno                       : 1;
+	uint64_t sgmii                        : 1;
+	uint64_t reserved_13_63               : 51;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_pcs_miscx_ctl_s  cn78xx;
+};
+typedef union cvmx_bgxx_gmp_pcs_miscx_ctl cvmx_bgxx_gmp_pcs_miscx_ctl_t;
+
+/**
+ * cvmx_bgx#_gmp_pcs_mr#_control
+ */
+union cvmx_bgxx_gmp_pcs_mrx_control {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_pcs_mrx_control_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t reset                        : 1;  /**< Set to reset.
+                                                         0 = Normal operation.
+                                                         1 = Software PCS reset.
+                                                         The bit returns to 0 after PCS has been reset. Takes 32 coprocessor-clock cycles to reset
+                                                         PCS. */
+	uint64_t loopbck1                     : 1;  /**< Enable loopback:
+                                                           0 = Normal operation.
+                                                           1 = Internal loopback mode.
+                                                         The loopback mode returns loopback TX data from GMII TX back to GMII RX interface. The
+                                                         loopback happens in the PCS module. Auto-Negotiation is disabled even if AN_EN is set
+                                                         during loopback. */
+	uint64_t spdlsb                       : 1;  /**< Least-significant bit of the link-speed field, i.e. SPD<0>. Refer to SPDMSB. */
+	uint64_t an_en                        : 1;  /**< Auto-Negotiation enable. */
+	uint64_t pwr_dn                       : 1;  /**< Power down:
+                                                         0 = Normal operation.
+                                                         1 = Power down (hardware reset). */
+	uint64_t reserved_10_10               : 1;
+	uint64_t rst_an                       : 1;  /**< Reset Auto-Negotiation. When set, if AN_EN = 1 and
+                                                         BGX()_GMP_PCS_MR()_STATUS[AN_ABIL] = 1, Auto-Negotiation begins. Otherwise,
+                                                         software write requests are ignored and this bit remains at 0. This bit clears itself to
+                                                         0, when Auto-Negotiation starts. */
+	uint64_t dup                          : 1;  /**< Duplex mode:
+                                                           0 = half duplex; effective only if Auto-Negotiation is disabled.
+                                                           1 = full duplex.
+                                                         If BGX()_GMP_PCS_MR()_STATUS <15:9> and
+                                                         BGX()_GMP_PCS_AN()_ADV<15:12> allow only one duplex mode, this bit corresponds to
+                                                         that value and any attempts to write are ignored. */
+	uint64_t coltst                       : 1;  /**< Enable COL signal test.
+                                                         During COL test, the COL signal reflects the GMII TX_EN signal with less than 16BT delay. */
+	uint64_t spdmsb                       : 1;  /**< Link speed most-significant bit, i.e SPD<1>; effective only if Auto-Negotiation is
+                                                         disabled.
+                                                         <pre>
+                                                         [SPDMSB]   [SPDLSB]   Link Speed
+                                                          0          0         10 Mb/s
+                                                          0          1         100 Mb/s
+                                                          1          0         1000 Mb/s
+                                                          1          1         reserved
+                                                         </pre> */
+	uint64_t uni                          : 1;  /**< Unidirectional (Std 802.3-2005, Clause 66.2). When set to 1, this bit overrides AN_EN and
+                                                         disables the Auto-Negotiation variable mr_an_enable. Used in both 1000BASE-X and SGMII
+                                                         modes. */
+	uint64_t reserved_0_4                 : 5;
+#else
+	uint64_t reserved_0_4                 : 5;
+	uint64_t uni                          : 1;
+	uint64_t spdmsb                       : 1;
+	uint64_t coltst                       : 1;
+	uint64_t dup                          : 1;
+	uint64_t rst_an                       : 1;
+	uint64_t reserved_10_10               : 1;
+	uint64_t pwr_dn                       : 1;
+	uint64_t an_en                        : 1;
+	uint64_t spdlsb                       : 1;
+	uint64_t loopbck1                     : 1;
+	uint64_t reset                        : 1;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_pcs_mrx_control_s cn78xx;
+};
+typedef union cvmx_bgxx_gmp_pcs_mrx_control cvmx_bgxx_gmp_pcs_mrx_control_t;
+
+/**
+ * cvmx_bgx#_gmp_pcs_mr#_status
+ *
+ * Bits <15:9> in this register indicate the ability to operate when
+ * BGX()_GMP_PCS_MISC()_CTL[MAC_PHY] is set to MAC mode. Bits <15:9> are always read as
+ * 0, indicating that the chip cannot operate in the corresponding modes. The field [RM_FLT] is a
+ * 'don't care' when the selected mode is SGMII.
+ */
+union cvmx_bgxx_gmp_pcs_mrx_status {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_pcs_mrx_status_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t hun_t4                       : 1;  /**< Indicates 100BASE-T4 capable. */
+	uint64_t hun_xfd                      : 1;  /**< Indicates 100BASE-X full duplex. */
+	uint64_t hun_xhd                      : 1;  /**< Indicates 100BASE-X half duplex. */
+	uint64_t ten_fd                       : 1;  /**< Indicates 10Mb/s full duplex. */
+	uint64_t ten_hd                       : 1;  /**< Indicates 10Mb/s half duplex. */
+	uint64_t hun_t2fd                     : 1;  /**< Indicates 100BASE-T2 full duplex. */
+	uint64_t hun_t2hd                     : 1;  /**< Indicates 100BASE-T2 half duplex. */
+	uint64_t ext_st                       : 1;  /**< Extended status information. When set to 1, indicates that additional status data is
+                                                         available in BGX()_GMP_PCS_AN()_EXT_ST. */
+	uint64_t reserved_7_7                 : 1;
+	uint64_t prb_sup                      : 1;  /**< Preamble not needed.
+                                                         0 = Cannot accept frames without preamble bytes.
+                                                         1 = Can work without preamble bytes at the beginning of frames. */
+	uint64_t an_cpt                       : 1;  /**< Indicates Auto-Negotiation is complete; the contents of the
+                                                         BGX()_GMP_PCS_AN()_RESULTS are valid. */
+	uint64_t rm_flt                       : 1;  /**< Indicates remote fault condition occurred. This bit implements a latching-high behavior.
+                                                         It is cleared when software reads this register or when
+                                                         BGX()_GMP_PCS_MR()_CONTROL[RESET] is asserted.
+                                                         See BGX()_GMP_PCS_AN()_ADV[REM_FLT] for fault conditions. */
+	uint64_t an_abil                      : 1;  /**< Indicates Auto-Negotiation capable. */
+	uint64_t lnk_st                       : 1;  /**< Link state:
+                                                           0 = link down.
+                                                           1 = link up.
+                                                         Set during Auto-Negotiation process. Set whenever XMIT = DATA. Latching-low behavior when
+                                                         link goes down. Link down value of the bit stays low until software reads the register. */
+	uint64_t reserved_1_1                 : 1;
+	uint64_t extnd                        : 1;  /**< This field is always 0, extended capability registers not present. */
+#else
+	uint64_t extnd                        : 1;
+	uint64_t reserved_1_1                 : 1;
+	uint64_t lnk_st                       : 1;
+	uint64_t an_abil                      : 1;
+	uint64_t rm_flt                       : 1;
+	uint64_t an_cpt                       : 1;
+	uint64_t prb_sup                      : 1;
+	uint64_t reserved_7_7                 : 1;
+	uint64_t ext_st                       : 1;
+	uint64_t hun_t2hd                     : 1;
+	uint64_t hun_t2fd                     : 1;
+	uint64_t ten_hd                       : 1;
+	uint64_t ten_fd                       : 1;
+	uint64_t hun_xhd                      : 1;
+	uint64_t hun_xfd                      : 1;
+	uint64_t hun_t4                       : 1;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_pcs_mrx_status_s cn78xx;
+};
+typedef union cvmx_bgxx_gmp_pcs_mrx_status cvmx_bgxx_gmp_pcs_mrx_status_t;
+
+/**
+ * cvmx_bgx#_gmp_pcs_rx#_states
+ */
+union cvmx_bgxx_gmp_pcs_rxx_states {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_pcs_rxx_states_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t rx_bad                       : 1;  /**< Receive state machine is in an illegal state. */
+	uint64_t rx_st                        : 5;  /**< Receive state-machine state. */
+	uint64_t sync_bad                     : 1;  /**< Receive synchronization state machine is in an illegal state. */
+	uint64_t sync                         : 4;  /**< Receive synchronization state-machine state. */
+	uint64_t an_bad                       : 1;  /**< Auto-Negotiation state machine is in an illegal state. */
+	uint64_t an_st                        : 4;  /**< Auto-Negotiation state-machine state. */
+#else
+	uint64_t an_st                        : 4;
+	uint64_t an_bad                       : 1;
+	uint64_t sync                         : 4;
+	uint64_t sync_bad                     : 1;
+	uint64_t rx_st                        : 5;
+	uint64_t rx_bad                       : 1;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_pcs_rxx_states_s cn78xx;
+};
+typedef union cvmx_bgxx_gmp_pcs_rxx_states cvmx_bgxx_gmp_pcs_rxx_states_t;
+
+/**
+ * cvmx_bgx#_gmp_pcs_rx#_sync
+ */
+union cvmx_bgxx_gmp_pcs_rxx_sync {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_pcs_rxx_sync_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_2_63                : 62;
+	uint64_t sync                         : 1;  /**< When set, code group synchronization achieved. */
+	uint64_t bit_lock                     : 1;  /**< When set, bit lock achieved. */
+#else
+	uint64_t bit_lock                     : 1;
+	uint64_t sync                         : 1;
+	uint64_t reserved_2_63                : 62;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_pcs_rxx_sync_s   cn78xx;
+};
+typedef union cvmx_bgxx_gmp_pcs_rxx_sync cvmx_bgxx_gmp_pcs_rxx_sync_t;
+
+/**
+ * cvmx_bgx#_gmp_pcs_sgm#_an_adv
+ *
+ * This is the SGMII Auto-Negotiation advertisement register (sent out as TX_CONFIG_REG). This
+ * register is sent during Auto-Negotiation if
+ * BGX()_GMP_PCS_MISC()_CTL[MAC_PHY] is set (1 = PHY mode). If the bit is not set (0 =
+ * MAC mode), the TX_CONFIG_REG<14> becomes ACK bit and <0> is always 1. All other bits in
+ * TX_CONFIG_REG sent will be 0. The PHY dictates the Auto-Negotiation results.
+ */
+union cvmx_bgxx_gmp_pcs_sgmx_an_adv {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_pcs_sgmx_an_adv_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t link                         : 1;  /**< Link status: 1 = Link up, 0 = Link down. */
+	uint64_t ack                          : 1;  /**< Auto-Negotiation acknowledgement. */
+	uint64_t reserved_13_13               : 1;
+	uint64_t dup                          : 1;  /**< Duplex mode: 1 = full duplex, 0 = half duplex */
+	uint64_t speed                        : 2;  /**< Link speed:
+                                                         0x0 = 10 Mb/s.
+                                                         0x1 = 100 Mb/s.
+                                                         0x2 = 1000 Mb/s.
+                                                         0x3 = Reserved. */
+	uint64_t reserved_1_9                 : 9;
+	uint64_t one                          : 1;  /**< Always set to match TX_CONFIG_REG<0>. */
+#else
+	uint64_t one                          : 1;
+	uint64_t reserved_1_9                 : 9;
+	uint64_t speed                        : 2;
+	uint64_t dup                          : 1;
+	uint64_t reserved_13_13               : 1;
+	uint64_t ack                          : 1;
+	uint64_t link                         : 1;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_pcs_sgmx_an_adv_s cn78xx;
+};
+typedef union cvmx_bgxx_gmp_pcs_sgmx_an_adv cvmx_bgxx_gmp_pcs_sgmx_an_adv_t;
+
+/**
+ * cvmx_bgx#_gmp_pcs_sgm#_lp_adv
+ *
+ * This is the SGMII Link partner advertisement register (received as RX_CONFIG_REG).
+ *
+ */
+union cvmx_bgxx_gmp_pcs_sgmx_lp_adv {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_pcs_sgmx_lp_adv_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t link                         : 1;  /**< Link status: 1 = Link up, 0 = Link down. */
+	uint64_t reserved_13_14               : 2;
+	uint64_t dup                          : 1;  /**< Duplex mode: 1 = Full duplex, 0 = Half duplex */
+	uint64_t speed                        : 2;  /**< Link speed:
+                                                         0x0 = 10 Mb/s.
+                                                         0x1 = 100 Mb/s.
+                                                         0x2 = 1000 Mb/s.
+                                                         0x3 = Reserved. */
+	uint64_t reserved_1_9                 : 9;
+	uint64_t one                          : 1;  /**< Always set to match TX_CONFIG_REG<0> */
+#else
+	uint64_t one                          : 1;
+	uint64_t reserved_1_9                 : 9;
+	uint64_t speed                        : 2;
+	uint64_t dup                          : 1;
+	uint64_t reserved_13_14               : 2;
+	uint64_t link                         : 1;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_pcs_sgmx_lp_adv_s cn78xx;
+};
+typedef union cvmx_bgxx_gmp_pcs_sgmx_lp_adv cvmx_bgxx_gmp_pcs_sgmx_lp_adv_t;
+
+/**
+ * cvmx_bgx#_gmp_pcs_tx#_states
+ */
+union cvmx_bgxx_gmp_pcs_txx_states {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_pcs_txx_states_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_7_63                : 57;
+	uint64_t xmit                         : 2;  /**< 0x0 = Undefined.
+                                                         0x1 = Config.
+                                                         0x2 = Idle.
+                                                         0x3 = Data. */
+	uint64_t tx_bad                       : 1;  /**< Transmit state machine in an illegal state. */
+	uint64_t ord_st                       : 4;  /**< Transmit ordered set state-machine state. */
+#else
+	uint64_t ord_st                       : 4;
+	uint64_t tx_bad                       : 1;
+	uint64_t xmit                         : 2;
+	uint64_t reserved_7_63                : 57;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_pcs_txx_states_s cn78xx;
+};
+typedef union cvmx_bgxx_gmp_pcs_txx_states cvmx_bgxx_gmp_pcs_txx_states_t;
+
+/**
+ * cvmx_bgx#_gmp_pcs_tx_rx#_polarity
+ *
+ * BGX()_GMP_PCS_TX_RX()_POLARITY[AUTORXPL] shows correct polarity needed on the link
+ * receive path after code group synchronization is achieved.
+ */
+union cvmx_bgxx_gmp_pcs_tx_rxx_polarity {
+	uint64_t u64;
+	struct cvmx_bgxx_gmp_pcs_tx_rxx_polarity_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t rxovrd                       : 1;  /**< RX polarity override.
+                                                         0 = AUTORXPL determines polarity.
+                                                         1 = RXPLRT determines polarity. */
+	uint64_t autorxpl                     : 1;  /**< Auto RX polarity detected:
+                                                         0 = Normal polarity.
+                                                         1 = Inverted polarity.
+                                                         This bit always represents the correct RX polarity setting needed for successful RX path
+                                                         operation, once a successful code group sync is obtained. */
+	uint64_t rxplrt                       : 1;  /**< RX polarity: 0 = Normal polarity, 1 = Inverted polarity. */
+	uint64_t txplrt                       : 1;  /**< TX polarity: 0 = Normal polarity, 1 = Inverted polarity. */
+#else
+	uint64_t txplrt                       : 1;
+	uint64_t rxplrt                       : 1;
+	uint64_t autorxpl                     : 1;
+	uint64_t rxovrd                       : 1;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_bgxx_gmp_pcs_tx_rxx_polarity_s cn78xx;
+};
+typedef union cvmx_bgxx_gmp_pcs_tx_rxx_polarity cvmx_bgxx_gmp_pcs_tx_rxx_polarity_t;
+
+/**
+ * cvmx_bgx#_smu#_cbfc_ctl
+ */
+union cvmx_bgxx_smux_cbfc_ctl {
+	uint64_t u64;
+	struct cvmx_bgxx_smux_cbfc_ctl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t phys_en                      : 16; /**< Physical backpressure enable. Determines which LMACs will have physical backpressure PAUSE
+                                                         packets. The value placed in the Class Enable Vector field of the PFC/CBFC PAUSE packet is
+                                                         PHYS_EN | LOGL_EN. */
+	uint64_t logl_en                      : 16; /**< Logical backpressure enable. Determines which LMACs will have logical backpressure PAUSE
+                                                         packets. The value placed in the Class Enable Vector field of the PFC/CBFC PAUSE packet is
+                                                         PHYS_EN | LOGL_EN. */
+	uint64_t reserved_4_31                : 28;
+	uint64_t bck_en                       : 1;  /**< Forward PFC/CBFC PAUSE information to the backpressure block. */
+	uint64_t drp_en                       : 1;  /**< Drop-control enable. When set, drop PFC/CBFC PAUSE frames. */
+	uint64_t tx_en                        : 1;  /**< Transmit enable. When set, allow for PFC/CBFC PAUSE packets. Must be clear in HiGig2 mode
+                                                         i.e. when BGX()_SMU()_TX_CTL[HG_EN] = 1 and BGX()_SMU()_RX_UDD_SKP[LEN] =
+                                                         16. */
+	uint64_t rx_en                        : 1;  /**< Receive enable. When set, allow for PFC/CBFC PAUSE packets. Must be clear in HiGig2 mode
+                                                         i.e. when BGX()_SMU()_TX_CTL[HG_EN] = 1 and BGX()_SMU()_RX_UDD_SKP[LEN] =
+                                                         16. */
+#else
+	uint64_t rx_en                        : 1;
+	uint64_t tx_en                        : 1;
+	uint64_t drp_en                       : 1;
+	uint64_t bck_en                       : 1;
+	uint64_t reserved_4_31                : 28;
+	uint64_t logl_en                      : 16;
+	uint64_t phys_en                      : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_smux_cbfc_ctl_s      cn78xx;
+};
+typedef union cvmx_bgxx_smux_cbfc_ctl cvmx_bgxx_smux_cbfc_ctl_t;
+
+/**
+ * cvmx_bgx#_smu#_ctrl
+ */
+union cvmx_bgxx_smux_ctrl {
+	uint64_t u64;
+	struct cvmx_bgxx_smux_ctrl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_2_63                : 62;
+	uint64_t tx_idle                      : 1;  /**< TX machine is idle. This indication pertains to the framer FSM and ignores the effects on
+                                                         the data-path controls or values which occur when BGX()_SMU()_TX_CTL[LS_BYP] is
+                                                         set. */
+	uint64_t rx_idle                      : 1;  /**< RX machine is idle. */
+#else
+	uint64_t rx_idle                      : 1;
+	uint64_t tx_idle                      : 1;
+	uint64_t reserved_2_63                : 62;
+#endif
+	} s;
+	struct cvmx_bgxx_smux_ctrl_s          cn78xx;
+};
+typedef union cvmx_bgxx_smux_ctrl cvmx_bgxx_smux_ctrl_t;
+
+/**
+ * cvmx_bgx#_smu#_ext_loopback
+ *
+ * In loopback mode, the IFG1+IFG2 of local and remote parties must match exactly; otherwise one
+ * of the two sides' loopback FIFO will overrun: BGX()_SMU()_TX_INT[LB_OVRFLW].
+ */
+union cvmx_bgxx_smux_ext_loopback {
+	uint64_t u64;
+	struct cvmx_bgxx_smux_ext_loopback_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_5_63                : 59;
+	uint64_t en                           : 1;  /**< Loopback enable. Puts the packet interface in external loopback mode where the RX lines
+                                                         are reflected on the TX lines. */
+	uint64_t thresh                       : 4;  /**< Threshold on the TX FIFO. Software must only write the typical value. Any other value
+                                                         causes loopback mode not to function correctly. */
+#else
+	uint64_t thresh                       : 4;
+	uint64_t en                           : 1;
+	uint64_t reserved_5_63                : 59;
+#endif
+	} s;
+	struct cvmx_bgxx_smux_ext_loopback_s  cn78xx;
+};
+typedef union cvmx_bgxx_smux_ext_loopback cvmx_bgxx_smux_ext_loopback_t;
+
+/**
+ * cvmx_bgx#_smu#_hg2_control
+ *
+ * HiGig2 TX- and RX-enable are normally set together for HiGig2 messaging. Setting just the TX
+ * or RX bit results in only the HG2 message transmit or receive capability.
+ *
+ * Setting [PHYS_EN] and [LOGL_EN] to 1 allows link PAUSE or backpressure to PKO as per the
+ * received HiGig2 message. Setting these fields to 0 disables link PAUSE and backpressure to PKO
+ * in response to received messages.
+ *
+ * BGX()_SMU()_TX_CTL[HG_EN] must be set (to enable HiGig) whenever either [HG2TX_EN] or
+ * [HG2RX_EN] are set. BGX()_SMU()_RX_UDD_SKP[LEN] must be set to 16 (to select HiGig2)
+ * whenever either [HG2TX_EN] or [HG2RX_EN] are set.
+ *
+ * BGX()_CMR_RX_OVR_BP[EN<0>] must be set and BGX()_CMR_RX_OVR_BP[BP<0>] must be cleared
+ * to 0 (to forcibly disable hardware-automatic 802.3 PAUSE packet generation) with the HiGig2
+ * Protocol when BGX()_SMU()_HG2_CONTROL[HG2TX_EN] = 0. (The HiGig2 protocol is indicated
+ * by BGX()_SMU()_TX_CTL[HG_EN] = 1 and BGX()_SMU()_RX_UDD_SKP[LEN]=16.) Hardware
+ * can only autogenerate backpressure via HiGig2 messages (optionally, when HG2TX_EN = 1) with
+ * the HiGig2 protocol.
+ */
+union cvmx_bgxx_smux_hg2_control {
+	uint64_t u64;
+	struct cvmx_bgxx_smux_hg2_control_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_19_63               : 45;
+	uint64_t hg2tx_en                     : 1;  /**< Enable transmission of HG2 physical and logical messages. When set, also disables hardware
+                                                         autogenerated (802.3 and PFC/CBFC) PAUSE frames. (CNXXXX cannot generate proper 802.3 or
+                                                         PFC/CBFC PAUSE frames in HiGig2 mode.) */
+	uint64_t hg2rx_en                     : 1;  /**< Enable extraction and processing of HG2 message packet from RX flow. Physical and logical
+                                                         PAUSE information is used to PAUSE physical-link, backpressure PKO. This field must be set
+                                                         when HiGig2 messages are present in the receive stream. This bit is also forwarded to CMR
+                                                         so it can generate the required deferring signals to SMU TX and backpressure signals to
+                                                         PKO. */
+	uint64_t phys_en                      : 1;  /**< Physical-link PAUSE enable for received HiGig2 physical PAUSE message. This bit enables
+                                                         the SMU TX to CMR HG2 deferring counter to be set every time SMU RX receives and filters
+                                                         out a valid physical HG2 message. */
+	uint64_t logl_en                      : 16; /**< 16-bit XOF enables for received HiGig2 messages or PFC/CBFC packets. This field is NOT
+                                                         used by SMU at all. It is forwarded to CMR without alteration. It appears here for
+                                                         backward compatibility. */
+#else
+	uint64_t logl_en                      : 16;
+	uint64_t phys_en                      : 1;
+	uint64_t hg2rx_en                     : 1;
+	uint64_t hg2tx_en                     : 1;
+	uint64_t reserved_19_63               : 45;
+#endif
+	} s;
+	struct cvmx_bgxx_smux_hg2_control_s   cn78xx;
+};
+typedef union cvmx_bgxx_smux_hg2_control cvmx_bgxx_smux_hg2_control_t;
+
+/**
+ * cvmx_bgx#_smu#_rx_bad_col_hi
+ */
+union cvmx_bgxx_smux_rx_bad_col_hi {
+	uint64_t u64;
+	struct cvmx_bgxx_smux_rx_bad_col_hi_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_17_63               : 47;
+	uint64_t val                          : 1;  /**< Set when BGX()_SMU()_RX_INT[PCTERR] is set. */
+	uint64_t state                        : 8;  /**< When BGX()_SMU()_RX_INT[PCTERR] is set, contains the receive state at the time of
+                                                         the error. */
+	uint64_t lane_rxc                     : 8;  /**< When BGX()_SMU()_RX_INT[PCTERR] is set, contains the column at the time of the error. */
+#else
+	uint64_t lane_rxc                     : 8;
+	uint64_t state                        : 8;
+	uint64_t val                          : 1;
+	uint64_t reserved_17_63               : 47;
+#endif
+	} s;
+	struct cvmx_bgxx_smux_rx_bad_col_hi_s cn78xx;
+};
+typedef union cvmx_bgxx_smux_rx_bad_col_hi cvmx_bgxx_smux_rx_bad_col_hi_t;
+
+/**
+ * cvmx_bgx#_smu#_rx_bad_col_lo
+ */
+union cvmx_bgxx_smux_rx_bad_col_lo {
+	uint64_t u64;
+	struct cvmx_bgxx_smux_rx_bad_col_lo_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t lane_rxd                     : 64; /**< When BGX()_SMU()_RX_INT[PCTERR] is set, LANE_RXD contains the XAUI/RXAUI column at
+                                                         the time of the error. */
+#else
+	uint64_t lane_rxd                     : 64;
+#endif
+	} s;
+	struct cvmx_bgxx_smux_rx_bad_col_lo_s cn78xx;
+};
+typedef union cvmx_bgxx_smux_rx_bad_col_lo cvmx_bgxx_smux_rx_bad_col_lo_t;
+
+/**
+ * cvmx_bgx#_smu#_rx_ctl
+ */
+union cvmx_bgxx_smux_rx_ctl {
+	uint64_t u64;
+	struct cvmx_bgxx_smux_rx_ctl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_2_63                : 62;
+	uint64_t status                       : 2;  /**< Link status.
+                                                         0x0 = Link OK.
+                                                         0x1 = Local fault.
+                                                         0x2 = Remote fault.
+                                                         0x3 = Reserved. */
+#else
+	uint64_t status                       : 2;
+	uint64_t reserved_2_63                : 62;
+#endif
+	} s;
+	struct cvmx_bgxx_smux_rx_ctl_s        cn78xx;
+};
+typedef union cvmx_bgxx_smux_rx_ctl cvmx_bgxx_smux_rx_ctl_t;
+
+/**
+ * cvmx_bgx#_smu#_rx_decision
+ *
+ * This register specifies the byte count used to determine when to accept or to filter a packet.
+ * As each byte in a packet is received by BGX, the L2 byte count (i.e. the number of bytes from
+ * the beginning of the L2 header (DMAC)) is compared against CNT. In normal operation, the L2
+ * header begins after the PREAMBLE + SFD (BGX()_SMU()_RX_FRM_CTL[PRE_CHK] = 1) and any
+ * optional UDD skip data (BGX()_SMU()_RX_UDD_SKP[LEN]).
+ */
+union cvmx_bgxx_smux_rx_decision {
+	uint64_t u64;
+	struct cvmx_bgxx_smux_rx_decision_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_5_63                : 59;
+	uint64_t cnt                          : 5;  /**< The byte count to decide when to accept or filter a packet. Refer to SMU Decisions. */
+#else
+	uint64_t cnt                          : 5;
+	uint64_t reserved_5_63                : 59;
+#endif
+	} s;
+	struct cvmx_bgxx_smux_rx_decision_s   cn78xx;
+};
+typedef union cvmx_bgxx_smux_rx_decision cvmx_bgxx_smux_rx_decision_t;
+
+/**
+ * cvmx_bgx#_smu#_rx_frm_chk
+ *
+ * The CSRs provide the enable bits for a subset of errors passed to CMR encoded.
+ *
+ */
+union cvmx_bgxx_smux_rx_frm_chk {
+	uint64_t u64;
+	struct cvmx_bgxx_smux_rx_frm_chk_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t skperr                       : 1;  /**< Skipper error. */
+	uint64_t rcverr                       : 1;  /**< Frame was received with data-reception error. */
+	uint64_t reserved_6_6                 : 1;
+	uint64_t fcserr_c                     : 1;  /**< Control frame was received with FCS/CRC error. */
+	uint64_t fcserr_d                     : 1;  /**< Data frame was received with FCS/CRC error. */
+	uint64_t jabber                       : 1;  /**< Frame was received with length > sys_length. */
+	uint64_t reserved_0_2                 : 3;
+#else
+	uint64_t reserved_0_2                 : 3;
+	uint64_t jabber                       : 1;
+	uint64_t fcserr_d                     : 1;
+	uint64_t fcserr_c                     : 1;
+	uint64_t reserved_6_6                 : 1;
+	uint64_t rcverr                       : 1;
+	uint64_t skperr                       : 1;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_bgxx_smux_rx_frm_chk_s    cn78xx;
+};
+typedef union cvmx_bgxx_smux_rx_frm_chk cvmx_bgxx_smux_rx_frm_chk_t;
+
+/**
+ * cvmx_bgx#_smu#_rx_frm_ctl
+ *
+ * This register controls the handling of the frames.
+ * The CTL_BCK/CTL_DRP bits control how the hardware handles incoming PAUSE packets. The most
+ * common modes of operation:
+ * _ CTL_BCK = 1, CTL_DRP = 1: hardware handles everything
+ * _ CTL_BCK = 0, CTL_DRP = 0: software sees all PAUSE frames
+ * _ CTL_BCK = 0, CTL_DRP = 1: all PAUSE frames are completely ignored
+ *
+ * These control bits should be set to CTL_BCK = 0, CTL_DRP = 0 in half-duplex mode. Since PAUSE
+ * packets only apply to full duplex operation, any PAUSE packet would constitute an exception
+ * which should be handled by the processing cores. PAUSE packets should not be forwarded.
+ */
+union cvmx_bgxx_smux_rx_frm_ctl {
+	uint64_t u64;
+	struct cvmx_bgxx_smux_rx_frm_ctl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_13_63               : 51;
+	uint64_t ptp_mode                     : 1;  /**< Timestamp mode. When PTP_MODE is set, a 64-bit timestamp is prepended to every incoming
+                                                         packet.
+                                                         The timestamp bytes are added to the packet in such a way as to not modify the packet's
+                                                         receive byte count. This implies that the BGX()_SMU()_RX_JABBER,
+                                                         BGX()_SMU()_RX_DECISION, and BGX()_SMU()_RX_UDD_SKP do not require any
+                                                         adjustment as they operate on the received packet size. When the packet reaches PKI, its
+                                                         size reflects the additional bytes and is subject to the following restrictions:
+                                                         If PTP_MODE = 1 and PRE_CHK = 1, PRE_STRP must be 1.
+                                                         If PTP_MODE = 1:
+                                                         * PKI_CL()_PKIND()_SKIP[FCS_SKIP,INST_SKIP] should be increased by 8.
+                                                         * PKI_CL()_PKIND()_CFG[HG_EN] should be 0.
+                                                         * PKI_FRM_LEN_CHK()[MAXLEN] should be increased by 8.
+                                                         * PKI_FRM_LEN_CHK()[MINLEN] should be increased by 8.
+                                                         * PKI_TAG_INC()_MASK should be adjusted. */
+	uint64_t reserved_6_11                : 6;
+	uint64_t ctl_smac                     : 1;  /**< Control PAUSE frames can match station SMAC. */
+	uint64_t ctl_mcst                     : 1;  /**< Control PAUSE frames can match globally assign multicast address. */
+	uint64_t ctl_bck                      : 1;  /**< Forward PAUSE information to TX block. */
+	uint64_t ctl_drp                      : 1;  /**< Drop control PAUSE frames. */
+	uint64_t pre_strp                     : 1;  /**< Strip off the preamble (when present).
+                                                         0 = PREAMBLE + SFD is sent to core as part of frame.
+                                                         1 = PREAMBLE + SFD is dropped.
+                                                         [PRE_CHK] must be set to enable this and all PREAMBLE features.
+                                                         If PTP_MODE = 1 and PRE_CHK = 1, PRE_STRP must be 1.
+                                                         When PRE_CHK is set (indicating that the PREAMBLE will be sent), PRE_STRP determines if
+                                                         the PREAMBLE+SFD bytes are thrown away or sent to the core as part of the packet. In
+                                                         either mode, the PREAMBLE+SFD bytes are not counted toward the packet size when checking
+                                                         against the MIN and MAX bounds. Furthermore, the bytes are skipped when locating the start
+                                                         of the L2 header for DMAC and control frame recognition. */
+	uint64_t pre_chk                      : 1;  /**< Check the preamble for correctness.
+                                                         This port is configured to send a valid 802.3 PREAMBLE to begin every frame. BGX checks
+                                                         that a valid PREAMBLE is received (based on PRE_FREE). When a problem does occur within
+                                                         the PREAMBLE sequence, the frame is marked as bad and not sent into the core. The
+                                                         BGX()_SMU()_RX_INT[PCTERR] interrupt is also raised.
+                                                         When BGX()_SMU()_TX_CTL[HG_EN] is set, PRE_CHK must be 0.
+                                                         If PTP_MODE = 1 and PRE_CHK = 1, PRE_STRP must be 1. */
+#else
+	uint64_t pre_chk                      : 1;
+	uint64_t pre_strp                     : 1;
+	uint64_t ctl_drp                      : 1;
+	uint64_t ctl_bck                      : 1;
+	uint64_t ctl_mcst                     : 1;
+	uint64_t ctl_smac                     : 1;
+	uint64_t reserved_6_11                : 6;
+	uint64_t ptp_mode                     : 1;
+	uint64_t reserved_13_63               : 51;
+#endif
+	} s;
+	struct cvmx_bgxx_smux_rx_frm_ctl_s    cn78xx;
+};
+typedef union cvmx_bgxx_smux_rx_frm_ctl cvmx_bgxx_smux_rx_frm_ctl_t;
+
+/**
+ * cvmx_bgx#_smu#_rx_int
+ *
+ * SMU Interrupt Register.
+ *
+ */
+union cvmx_bgxx_smux_rx_int {
+	uint64_t u64;
+	struct cvmx_bgxx_smux_rx_int_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_12_63               : 52;
+	uint64_t hg2cc                        : 1;  /**< HiGig2 received message CRC or control-character error. Set when either a CRC8 error is
+                                                         detected, or when a control character is found in the message bytes after the K.SOM.
+                                                         HG2CC has higher priority than HG2FLD, which means that a HiGig2 message that results in
+                                                         HG2CC getting set never sets HG2FLD. */
+	uint64_t hg2fld                       : 1;  /**< HiGig2 received message field error:
+                                                         MSG_TYPE field not 0x0, i.e. it is not a flow-control message, which is the only defined
+                                                         type for HiGig2.
+                                                         FWD_TYPE field not 0x0, i.e. it is not a link-level message, which is the only defined
+                                                         type for HiGig2.
+                                                         FC_OBJECT field is neither 0x0 for physical link, nor 0x2 for logical link. Those are the
+                                                         only two defined types in HiGig2 */
+	uint64_t bad_term                     : 1;  /**< Frame is terminated by control character other than /T/. (XAUI/RXAUI mode only) The error
+                                                         propagation control character /E/ will be included as part of the frame and does not cause
+                                                         a frame termination. */
+	uint64_t bad_seq                      : 1;  /**< Reserved sequence detected. (XAUI/RXAUI mode only) */
+	uint64_t rem_fault                    : 1;  /**< Remote-fault sequence detected. (XAUI/RXAUI mode only) */
+	uint64_t loc_fault                    : 1;  /**< Local-fault sequence detected. (XAUI/RXAUI mode only) */
+	uint64_t rsverr                       : 1;  /**< Reserved opcodes. */
+	uint64_t pcterr                       : 1;  /**< Bad preamble/protocol. In XAUI/RXAUI mode, the column of data that was bad is logged in
+                                                         BGX()_SMU()_RX_BAD_COL_LO and BGX()_SMU()_RX_BAD_COL_HI.
+                                                         PCTERR checks that the frame begins with a valid
+                                                         PREAMBLE sequence. Does not check the number of PREAMBLE cycles. */
+	uint64_t skperr                       : 1;  /**< Skipper error. */
+	uint64_t rcverr                       : 1;  /**< Frame was received with data-reception error. */
+	uint64_t fcserr                       : 1;  /**< Frame was received with FCS/CRC error */
+	uint64_t jabber                       : 1;  /**< Frame was received with length > sys_length. An RX Jabber error indicates that a packet
+                                                         was received which is longer than the maximum allowed packet as defined by the system. BGX
+                                                         terminates the packet with an EOP on the beat on which JABBER was exceeded. The beat on
+                                                         which JABBER was exceeded is left unchanged and all subsequent data beats are dropped.
+                                                         Failure to truncate could lead to system instability. */
+#else
+	uint64_t jabber                       : 1;
+	uint64_t fcserr                       : 1;
+	uint64_t rcverr                       : 1;
+	uint64_t skperr                       : 1;
+	uint64_t pcterr                       : 1;
+	uint64_t rsverr                       : 1;
+	uint64_t loc_fault                    : 1;
+	uint64_t rem_fault                    : 1;
+	uint64_t bad_seq                      : 1;
+	uint64_t bad_term                     : 1;
+	uint64_t hg2fld                       : 1;
+	uint64_t hg2cc                        : 1;
+	uint64_t reserved_12_63               : 52;
+#endif
+	} s;
+	struct cvmx_bgxx_smux_rx_int_s        cn78xx;
+};
+typedef union cvmx_bgxx_smux_rx_int cvmx_bgxx_smux_rx_int_t;
+
+/**
+ * cvmx_bgx#_smu#_rx_jabber
+ *
+ * This register specifies the maximum size for packets, beyond which the SMU truncates. In
+ * XAUI/RXAUI mode, port 0 is used for checking.
+ *
+ * INTERNAL:
+ * The packet that will be sent to the packet input logic will have an
+ * additionl 8 bytes if BGX()_SMU()_RX_FRM_CTL[PRE_CHK] is set and
+ * BGX()_SMU()_RX_FRM_CTL[PRE_STRP] is clear.  The max packet that will be sent is
+ * defined as:
+ *
+ * _ max_sized_packet = BGX()_SMU()_RX_JABBER[CNT]+((BGX()_SMU()_RX_FRM_CTL[PRE_CHK] &
+ * !BGX()_SMU()_RX_FRM_CTL[PRE_STRP])*8)
+ */
+union cvmx_bgxx_smux_rx_jabber {
+	uint64_t u64;
+	struct cvmx_bgxx_smux_rx_jabber_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t cnt                          : 16; /**< Byte count for jabber check. Failing packets set the JABBER interrupt and are optionally
+                                                         sent with opcode = JABBER. BGX truncates the packet to CNT+1 to CNT+8 bytes.
+                                                         CNT must be 8-byte aligned such that CNT[2:0] = 000. */
+#else
+	uint64_t cnt                          : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_smux_rx_jabber_s     cn78xx;
+};
+typedef union cvmx_bgxx_smux_rx_jabber cvmx_bgxx_smux_rx_jabber_t;
+
+/**
+ * cvmx_bgx#_smu#_rx_udd_skp
+ *
+ * This register specifies the amount of user-defined data (UDD) added before the start of the
+ * L2C data.
+ *
+ * INTERNAL:
+ *
+ * (1) The skip bytes are part of the packet and will be sent down the NCB
+ * packet interface and will be handled by PKI.
+ *
+ * (2) The system can determine if the UDD bytes are included in the FCS check
+ * by using the FCSSEL field if the FCS check is enabled.
+ *
+ * (3) Assume that the preamble/sfd is always at the start of the frame even
+ * before UDD bytes.  In most cases, there will be no preamble in these
+ * cases since it will be packet interface in direct communication to
+ * another packet interface (MAC to MAC) without a PHY involved.
+ *
+ * (4) We can still do address filtering and control packet filtering if the
+ * user desires.
+ *
+ * (6) In all cases, the UDD bytes will be sent down the packet interface as
+ * part of the packet.  The UDD bytes are never stripped from the actual
+ * packet.
+ */
+union cvmx_bgxx_smux_rx_udd_skp {
+	uint64_t u64;
+	struct cvmx_bgxx_smux_rx_udd_skp_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t fcssel                       : 1;  /**< Include the skip bytes in the FCS calculation.
+                                                         0 = All skip bytes are included in FCS.
+                                                         1 = The skip bytes are not included in FCS.
+                                                         When BGX()_SMU()_TX_CTL[HG_EN] is set, this field must be 0.
+                                                         The skip bytes are part of the packet and are sent through the IOI packet interface and
+                                                         are handled by PKI. The system can determine if the UDD bytes are included in the FCS
+                                                         check by using the FCSSEL field, if the FCS check is enabled. */
+	uint64_t reserved_7_7                 : 1;
+	uint64_t len                          : 7;  /**< Amount of user-defined data before the start of the L2C data, in bytes.
+                                                         Setting to 0 means L2C comes first; maximum value is 64.
+                                                         LEN must be 0x0 in half-duplex operation.
+                                                         When BGX()_SMU()_TX_CTL[HG_EN] is set, this field must be set to 12 or 16
+                                                         (depending on HiGig header size) to account for the HiGig header.
+                                                         LEN = 12 selects HiGig/HiGig+; LEN = 16 selects HiGig2. */
+#else
+	uint64_t len                          : 7;
+	uint64_t reserved_7_7                 : 1;
+	uint64_t fcssel                       : 1;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_bgxx_smux_rx_udd_skp_s    cn78xx;
+};
+typedef union cvmx_bgxx_smux_rx_udd_skp cvmx_bgxx_smux_rx_udd_skp_t;
+
+/**
+ * cvmx_bgx#_smu#_smac
+ */
+union cvmx_bgxx_smux_smac {
+	uint64_t u64;
+	struct cvmx_bgxx_smux_smac_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t smac                         : 48; /**< The SMAC field is used for generating and accepting control PAUSE packets. */
+#else
+	uint64_t smac                         : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_smux_smac_s          cn78xx;
+};
+typedef union cvmx_bgxx_smux_smac cvmx_bgxx_smux_smac_t;
+
+/**
+ * cvmx_bgx#_smu#_tx_append
+ *
+ * For more details on the interactions between FCS and PAD, see also the description of
+ * BGX()_SMU()_TX_MIN_PKT[MIN_SIZE].
+ */
+union cvmx_bgxx_smux_tx_append {
+	uint64_t u64;
+	struct cvmx_bgxx_smux_tx_append_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t fcs_c                        : 1;  /**< Append the Ethernet FCS on each PAUSE packet. PAUSE packets are normally padded to 60
+                                                         bytes. If BGX()_SMU()_TX_MIN_PKT[MIN_SIZE] exceeds 59, then FCS_C is not used. */
+	uint64_t fcs_d                        : 1;  /**< Append the Ethernet FCS on each data packet. */
+	uint64_t pad                          : 1;  /**< Append PAD bytes such that minimum-sized packet is transmitted. */
+	uint64_t preamble                     : 1;  /**< Prepend the Ethernet preamble on each transfer. When BGX()_SMU()_TX_CTL[HG_EN] is
+                                                         set, PREAMBLE must be 0. */
+#else
+	uint64_t preamble                     : 1;
+	uint64_t pad                          : 1;
+	uint64_t fcs_d                        : 1;
+	uint64_t fcs_c                        : 1;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_bgxx_smux_tx_append_s     cn78xx;
+};
+typedef union cvmx_bgxx_smux_tx_append cvmx_bgxx_smux_tx_append_t;
+
+/**
+ * cvmx_bgx#_smu#_tx_ctl
+ */
+union cvmx_bgxx_smux_tx_ctl {
+	uint64_t u64;
+	struct cvmx_bgxx_smux_tx_ctl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_31_63               : 33;
+	uint64_t spu_mrk_cnt                  : 20; /**< 40GBASE-R transmit marker interval count. Specifies the interval (number of 66-bit BASE-R
+                                                         blocks) at which the LMAC transmit logic inserts 40GBASE-R alignment markers. An internal
+                                                         counter in SMU is initialized to this value, counts down for each BASE-R block transmitted
+                                                         by the LMAC, and wraps back to the initial value from 0. The LMAC transmit logic inserts
+                                                         alignment markers for lanes 0, 1, 2 and 3, respectively, in the last four BASE-R blocks
+                                                         before the counter wraps (3, 2, 1, 0). The default value corresponds to an alignment
+                                                         marker period of 16363 blocks (exclusive) per lane, as specified in 802.3ba-2010. The
+                                                         default value should always be used for normal operation. */
+	uint64_t hg_pause_hgi                 : 2;  /**< HGI field for hardware-generated HiGig PAUSE packets. */
+	uint64_t hg_en                        : 1;  /**< Enable HiGig mode.
+                                                         When this field is set and BGX()_SMU()_RX_UDD_SKP[LEN] = 12, the interface is in
+                                                         HiGig/HiGig+ mode and the following must be set:
+                                                         * BGX()_SMU()_RX_FRM_CTL[PRE_CHK] = 0.
+                                                         * BGX()_SMU()_RX_UDD_SKP[FCSSEL] = 0.
+                                                         * BGX()_SMU()_RX_UDD_SKP[LEN] = 12.
+                                                         * BGX()_SMU()_TX_APPEND[PREAMBLE] = 0.
+                                                         When this field is set and BGX()_SMU()_RX_UDD_SKP[LEN] = 16, the interface is in
+                                                         HiGig2 mode and the following must be set:
+                                                         * BGX()_SMU()_RX_FRM_CTL[PRE_CHK] = 0.
+                                                         * BGX()_SMU()_RX_UDD_SKP[FCSSEL] = 0.
+                                                         * BGX()_SMU()_RX_UDD_SKP[LEN] = 16.
+                                                         * BGX()_SMU()_TX_APPEND[PREAMBLE] = 0.
+                                                         * BGX()_SMU()_CBFC_CTL[RX_EN] = 0.
+                                                         * BGX()_SMU()_CBFC_CTL[TX_EN] = 0. */
+	uint64_t l2p_bp_conv                  : 1;  /**< If set, causes TX to generate 802.3 pause packets when CMR applies logical backpressure
+                                                         (XOFF), if and only if BGX()_SMU()_CBFC_CTL[TX_EN] is clear and
+                                                         BGX()_SMU()_HG2_CONTROL[HG2TX_EN] is clear. */
+	uint64_t ls_byp                       : 1;  /**< Bypass the link status, as determined by the XGMII receiver, and set the link status of
+                                                         the transmitter to LS. */
+	uint64_t ls                           : 2;  /**< Link status.
+                                                         0 = Link OK; link runs normally. RS passes MAC data to PCS.
+                                                         1 = Local fault. RS layer sends continuous remote fault sequences.
+                                                         2 = Remote fault. RS layer sends continuous idle sequences.
+                                                         3 = Link drain. RS layer drops full packets to allow BGX and PKO to drain their FIFOs. */
+	uint64_t reserved_2_3                 : 2;
+	uint64_t uni_en                       : 1;  /**< Enable unidirectional mode (IEEE Clause 66). */
+	uint64_t dic_en                       : 1;  /**< Enable the deficit idle counter for IFG averaging. */
+#else
+	uint64_t dic_en                       : 1;
+	uint64_t uni_en                       : 1;
+	uint64_t reserved_2_3                 : 2;
+	uint64_t ls                           : 2;
+	uint64_t ls_byp                       : 1;
+	uint64_t l2p_bp_conv                  : 1;
+	uint64_t hg_en                        : 1;
+	uint64_t hg_pause_hgi                 : 2;
+	uint64_t spu_mrk_cnt                  : 20;
+	uint64_t reserved_31_63               : 33;
+#endif
+	} s;
+	struct cvmx_bgxx_smux_tx_ctl_s        cn78xx;
+};
+typedef union cvmx_bgxx_smux_tx_ctl cvmx_bgxx_smux_tx_ctl_t;
+
+/**
+ * cvmx_bgx#_smu#_tx_ifg
+ *
+ * Programming IFG1 and IFG2:
+ * * For XAUI/RXAUI/10Gbs/40Gbs systems that require IEEE 802.3 compatibility, the IFG1+IFG2 sum
+ * must be 12.
+ * * In loopback mode, the IFG1+IFG2 of local and remote parties must match exactly; otherwise
+ * one of the two sides' loopback FIFO will overrun: BGX()_SMU()_TX_INT[LB_OVRFLW].
+ */
+union cvmx_bgxx_smux_tx_ifg {
+	uint64_t u64;
+	struct cvmx_bgxx_smux_tx_ifg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_8_63                : 56;
+	uint64_t ifg2                         : 4;  /**< 1/2 of the interframe gap timing (in IFG2*8 bits). */
+	uint64_t ifg1                         : 4;  /**< 1/2 of the interframe gap timing (in IFG1*8 bits). */
+#else
+	uint64_t ifg1                         : 4;
+	uint64_t ifg2                         : 4;
+	uint64_t reserved_8_63                : 56;
+#endif
+	} s;
+	struct cvmx_bgxx_smux_tx_ifg_s        cn78xx;
+};
+typedef union cvmx_bgxx_smux_tx_ifg cvmx_bgxx_smux_tx_ifg_t;
+
+/**
+ * cvmx_bgx#_smu#_tx_int
+ */
+union cvmx_bgxx_smux_tx_int {
+	uint64_t u64;
+	struct cvmx_bgxx_smux_tx_int_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_5_63                : 59;
+	uint64_t lb_ovrflw                    : 1;  /**< TX loopback overflow. */
+	uint64_t lb_undflw                    : 1;  /**< TX loopback underflow. */
+	uint64_t fake_commit                  : 1;  /**< TX SMU started a packet with PTP on SOP and has not seen a commit for it from TX SPU after
+                                                         2^SMU_TX_PTP_TIMEOUT_WIDTH (2^8) cycles so it faked a commit to CMR. */
+	uint64_t xchange                      : 1;  /**< Link status changed. This denotes a change to BGX()_SMU()_RX_CTL[STATUS]. */
+	uint64_t undflw                       : 1;  /**< TX underflow. */
+#else
+	uint64_t undflw                       : 1;
+	uint64_t xchange                      : 1;
+	uint64_t fake_commit                  : 1;
+	uint64_t lb_undflw                    : 1;
+	uint64_t lb_ovrflw                    : 1;
+	uint64_t reserved_5_63                : 59;
+#endif
+	} s;
+	struct cvmx_bgxx_smux_tx_int_s        cn78xx;
+};
+typedef union cvmx_bgxx_smux_tx_int cvmx_bgxx_smux_tx_int_t;
+
+/**
+ * cvmx_bgx#_smu#_tx_min_pkt
+ */
+union cvmx_bgxx_smux_tx_min_pkt {
+	uint64_t u64;
+	struct cvmx_bgxx_smux_tx_min_pkt_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_8_63                : 56;
+	uint64_t min_size                     : 8;  /**< Min frame in bytes inclusive of FCS, if applied. Padding is only appended when
+                                                         BGX()_SMU()_TX_APPEND[PAD] for the corresponding port is set. When FCS is added to
+                                                         a packet which was padded, the FCS always appears in the 4 octets preceding /T/ or /E/. */
+#else
+	uint64_t min_size                     : 8;
+	uint64_t reserved_8_63                : 56;
+#endif
+	} s;
+	struct cvmx_bgxx_smux_tx_min_pkt_s    cn78xx;
+};
+typedef union cvmx_bgxx_smux_tx_min_pkt cvmx_bgxx_smux_tx_min_pkt_t;
+
+/**
+ * cvmx_bgx#_smu#_tx_pause_pkt_dmac
+ *
+ * This register provides the DMAC value that is placed in outbound PAUSE packets.
+ *
+ */
+union cvmx_bgxx_smux_tx_pause_pkt_dmac {
+	uint64_t u64;
+	struct cvmx_bgxx_smux_tx_pause_pkt_dmac_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t dmac                         : 48; /**< The DMAC field that is placed in outbound PAUSE packets. */
+#else
+	uint64_t dmac                         : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_smux_tx_pause_pkt_dmac_s cn78xx;
+};
+typedef union cvmx_bgxx_smux_tx_pause_pkt_dmac cvmx_bgxx_smux_tx_pause_pkt_dmac_t;
+
+/**
+ * cvmx_bgx#_smu#_tx_pause_pkt_interval
+ *
+ * This register specifies how often PAUSE packets are sent.
+ *
+ */
+union cvmx_bgxx_smux_tx_pause_pkt_interval {
+	uint64_t u64;
+	struct cvmx_bgxx_smux_tx_pause_pkt_interval_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_33_63               : 31;
+	uint64_t hg2_intra_en                 : 1;  /**< Allow intrapacket HiGig2 message generation. Relevant only if HiGig2 message generation is enabled. */
+	uint64_t hg2_intra_interval           : 16; /**< Arbitrate for a HiGig2 message, every (INTERVAL*512) bit-times whilst sending regular
+                                                         packet data. Relevant only if HiGig2 message generation and HG2_INTRA_EN are both set.
+                                                         Normally, 0 < INTERVAL < BGX()_SMU()_TX_PAUSE_PKT_TIME.
+                                                         INTERVAL = 0 only sends a single PAUSE packet for each backpressure event. */
+	uint64_t interval                     : 16; /**< Arbitrate for a 802.3 PAUSE packet, HiGig2 message, or PFC/CBFC PAUSE packet every
+                                                         (INTERVAL * 512) bit-times.
+                                                         Normally, 0 < INTERVAL < BGX()_SMU()_TX_PAUSE_PKT_TIME[PTIME].
+                                                         INTERVAL = 0 only sends a single PAUSE packet for each backpressure event. */
+#else
+	uint64_t interval                     : 16;
+	uint64_t hg2_intra_interval           : 16;
+	uint64_t hg2_intra_en                 : 1;
+	uint64_t reserved_33_63               : 31;
+#endif
+	} s;
+	struct cvmx_bgxx_smux_tx_pause_pkt_interval_s cn78xx;
+};
+typedef union cvmx_bgxx_smux_tx_pause_pkt_interval cvmx_bgxx_smux_tx_pause_pkt_interval_t;
+
+/**
+ * cvmx_bgx#_smu#_tx_pause_pkt_time
+ */
+union cvmx_bgxx_smux_tx_pause_pkt_time {
+	uint64_t u64;
+	struct cvmx_bgxx_smux_tx_pause_pkt_time_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t p_time                       : 16; /**< Provides the pause_time field placed in outbound 802.3 PAUSE packets, HiGig2 messages, or
+                                                         PFC/CBFC PAUSE packets in 512 bit-times. Normally, P_TIME >
+                                                         BGX()_SMU()_TX_PAUSE_PKT_INTERVAL[INTERVAL]. See programming notes in
+                                                         BGX()_SMU()_TX_PAUSE_PKT_INTERVAL. */
+#else
+	uint64_t p_time                       : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_smux_tx_pause_pkt_time_s cn78xx;
+};
+typedef union cvmx_bgxx_smux_tx_pause_pkt_time cvmx_bgxx_smux_tx_pause_pkt_time_t;
+
+/**
+ * cvmx_bgx#_smu#_tx_pause_pkt_type
+ *
+ * This register provides the P_TYPE field that is placed in outbound PAUSE packets.
+ *
+ */
+union cvmx_bgxx_smux_tx_pause_pkt_type {
+	uint64_t u64;
+	struct cvmx_bgxx_smux_tx_pause_pkt_type_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t p_type                       : 16; /**< The P_TYPE field that is placed in outbound PAUSE packets. */
+#else
+	uint64_t p_type                       : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_smux_tx_pause_pkt_type_s cn78xx;
+};
+typedef union cvmx_bgxx_smux_tx_pause_pkt_type cvmx_bgxx_smux_tx_pause_pkt_type_t;
+
+/**
+ * cvmx_bgx#_smu#_tx_pause_togo
+ */
+union cvmx_bgxx_smux_tx_pause_togo {
+	uint64_t u64;
+	struct cvmx_bgxx_smux_tx_pause_togo_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_32_63               : 32;
+	uint64_t msg_time                     : 16; /**< Amount of time remaining to backpressure, from the HiGig2 physical message PAUSE timer
+                                                         (only valid on port0). */
+	uint64_t p_time                       : 16; /**< Amount of time remaining to backpressure, from the standard 802.3 PAUSE timer. */
+#else
+	uint64_t p_time                       : 16;
+	uint64_t msg_time                     : 16;
+	uint64_t reserved_32_63               : 32;
+#endif
+	} s;
+	struct cvmx_bgxx_smux_tx_pause_togo_s cn78xx;
+};
+typedef union cvmx_bgxx_smux_tx_pause_togo cvmx_bgxx_smux_tx_pause_togo_t;
+
+/**
+ * cvmx_bgx#_smu#_tx_pause_zero
+ */
+union cvmx_bgxx_smux_tx_pause_zero {
+	uint64_t u64;
+	struct cvmx_bgxx_smux_tx_pause_zero_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t send                         : 1;  /**< Send PAUSE-zero enable. When this bit is set, and the backpressure condition is clear, it
+                                                         allows sending a PAUSE packet with pause_time of 0 to enable the channel. */
+#else
+	uint64_t send                         : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_bgxx_smux_tx_pause_zero_s cn78xx;
+};
+typedef union cvmx_bgxx_smux_tx_pause_zero cvmx_bgxx_smux_tx_pause_zero_t;
+
+/**
+ * cvmx_bgx#_smu#_tx_soft_pause
+ */
+union cvmx_bgxx_smux_tx_soft_pause {
+	uint64_t u64;
+	struct cvmx_bgxx_smux_tx_soft_pause_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t p_time                       : 16; /**< Back off the TX bus for (P_TIME * 512) bit-times */
+#else
+	uint64_t p_time                       : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_smux_tx_soft_pause_s cn78xx;
+};
+typedef union cvmx_bgxx_smux_tx_soft_pause cvmx_bgxx_smux_tx_soft_pause_t;
+
+/**
+ * cvmx_bgx#_smu#_tx_thresh
+ */
+union cvmx_bgxx_smux_tx_thresh {
+	uint64_t u64;
+	struct cvmx_bgxx_smux_tx_thresh_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_11_63               : 53;
+	uint64_t cnt                          : 11; /**< Number of 128-bit words to accumulate in the TX FIFO before sending on the packet
+                                                         interface. This field should be large enough to prevent underflow on the packet interface
+                                                         and must never be set to 0x0.
+                                                         In 10G/40G mode, CNT = 0x100.
+                                                         In all modes, this register cannot exceed the TX FIFO depth as follows.
+                                                         _ BGX()_CMR_TX_PRTS = 0,1:  CNT maximum = 0x7FF.
+                                                         _ BGX()_CMR_TX_PRTS = 2:    CNT maximum = 0x3FF.
+                                                         _ BGX()_CMR_TX_PRTS = 3,4:  CNT maximum = 0x1FF. */
+#else
+	uint64_t cnt                          : 11;
+	uint64_t reserved_11_63               : 53;
+#endif
+	} s;
+	struct cvmx_bgxx_smux_tx_thresh_s     cn78xx;
+};
+typedef union cvmx_bgxx_smux_tx_thresh cvmx_bgxx_smux_tx_thresh_t;
+
+/**
+ * cvmx_bgx#_spu#_an_adv
+ *
+ * Software programs this register with the contents of the AN-link code word base page to be
+ * transmitted during Auto-Negotiation. (See Std 802.3 section 73.6 for details.) Any write
+ * operations to this register prior to completion of Auto-Negotiation, as indicated by
+ * BGX()_SPU()_AN_STATUS[AN_COMPLETE], should be followed by a renegotiation in order for
+ * the new values to take effect. Renegotiation is initiated by setting
+ * BGX()_SPU()_AN_CONTROL[AN_RESTART]. Once Auto-Negotiation has completed, software can
+ * examine this register along with BGX()_SPU()_AN_LP_BASE to determine the highest
+ * common denominator technology.
+ */
+union cvmx_bgxx_spux_an_adv {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_an_adv_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t fec_req                      : 1;  /**< FEC requested. */
+	uint64_t fec_able                     : 1;  /**< FEC ability. */
+	uint64_t arsv                         : 19; /**< Technology ability. Reserved bits, should always be 0. */
+	uint64_t a100g_cr10                   : 1;  /**< 100GBASE-CR10 ability. Should always be 0; 100GBASE-R is not supported. */
+	uint64_t a40g_cr4                     : 1;  /**< 40GBASE-CR4 ability. */
+	uint64_t a40g_kr4                     : 1;  /**< 40GBASE-KR4 ability. */
+	uint64_t a10g_kr                      : 1;  /**< 10GBASE-KR ability. */
+	uint64_t a10g_kx4                     : 1;  /**< 10GBASE-KX4 ability. */
+	uint64_t a1g_kx                       : 1;  /**< 1000BASE-KX ability. Should always be 0; Auto-Negotiation is not supported for 1000Base-KX. */
+	uint64_t t                            : 5;  /**< Transmitted nonce. This field is automatically updated with a pseudo-random value on entry
+                                                         to the AN ability detect state. */
+	uint64_t np                           : 1;  /**< Next page. Always 0; extended next pages are not used for 10G+ Auto-Negotiation. */
+	uint64_t ack                          : 1;  /**< Acknowledge. Always 0 in this register. */
+	uint64_t rf                           : 1;  /**< Remote fault. */
+	uint64_t xnp_able                     : 1;  /**< Extended next page ability. */
+	uint64_t asm_dir                      : 1;  /**< Asymmetric PAUSE. */
+	uint64_t pause                        : 1;  /**< PAUSE ability. */
+	uint64_t e                            : 5;  /**< Echoed nonce. Provides the echoed-nonce value to use when ACK = 0 in transmitted DME page.
+                                                         Should always be 0x0. */
+	uint64_t s                            : 5;  /**< Selector. Should be 0x1 (encoding for IEEE Std 802.3). */
+#else
+	uint64_t s                            : 5;
+	uint64_t e                            : 5;
+	uint64_t pause                        : 1;
+	uint64_t asm_dir                      : 1;
+	uint64_t xnp_able                     : 1;
+	uint64_t rf                           : 1;
+	uint64_t ack                          : 1;
+	uint64_t np                           : 1;
+	uint64_t t                            : 5;
+	uint64_t a1g_kx                       : 1;
+	uint64_t a10g_kx4                     : 1;
+	uint64_t a10g_kr                      : 1;
+	uint64_t a40g_kr4                     : 1;
+	uint64_t a40g_cr4                     : 1;
+	uint64_t a100g_cr10                   : 1;
+	uint64_t arsv                         : 19;
+	uint64_t fec_able                     : 1;
+	uint64_t fec_req                      : 1;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_an_adv_s        cn78xx;
+};
+typedef union cvmx_bgxx_spux_an_adv cvmx_bgxx_spux_an_adv_t;
+
+/**
+ * cvmx_bgx#_spu#_an_bp_status
+ *
+ * The contents of this register (with the exception of the static BP_AN_ABLE bit) are updated
+ * during Auto-Negotiation and are valid when BGX()_SPU()_AN_STATUS[AN_COMPLETE] is set.
+ * At that time, one of the port type bits (A100G_CR10, A40G_CR4, A40G_KR4, A10G_KR, A10G_KX4,
+ * A1G_KX) will be set depending on the AN priority resolution. If a BASE-R type is negotiated,
+ * then the FEC bit will be set to indicate that FEC operation has been negotiated, and will be
+ * clear otherwise.
+ */
+union cvmx_bgxx_spux_an_bp_status {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_an_bp_status_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t n100g_cr10                   : 1;  /**< 100GBASE-CR10 negotiated; expected to always be 0; 100GBASE-R is not supported. */
+	uint64_t reserved_7_7                 : 1;
+	uint64_t n40g_cr4                     : 1;  /**< 40GBASE-CR4 negotiated. */
+	uint64_t n40g_kr4                     : 1;  /**< 40GBASE-KR4 negotiated. */
+	uint64_t fec                          : 1;  /**< BASE-R FEC negotiated. */
+	uint64_t n10g_kr                      : 1;  /**< 10GBASE-KR negotiated. */
+	uint64_t n10g_kx4                     : 1;  /**< 10GBASE-KX4 or CX4 negotiated (XAUI). */
+	uint64_t n1g_kx                       : 1;  /**< 1000BASE-KX negotiated. */
+	uint64_t bp_an_able                   : 1;  /**< Backplane or BASE-R copper AN Ability; always 1. */
+#else
+	uint64_t bp_an_able                   : 1;
+	uint64_t n1g_kx                       : 1;
+	uint64_t n10g_kx4                     : 1;
+	uint64_t n10g_kr                      : 1;
+	uint64_t fec                          : 1;
+	uint64_t n40g_kr4                     : 1;
+	uint64_t n40g_cr4                     : 1;
+	uint64_t reserved_7_7                 : 1;
+	uint64_t n100g_cr10                   : 1;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_an_bp_status_s  cn78xx;
+};
+typedef union cvmx_bgxx_spux_an_bp_status cvmx_bgxx_spux_an_bp_status_t;
+
+/**
+ * cvmx_bgx#_spu#_an_control
+ */
+union cvmx_bgxx_spux_an_control {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_an_control_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t an_reset                     : 1;  /**< Auto-Negotiation reset. Setting this bit or BGX()_SPU()_CONTROL1[RESET] to 1
+                                                         causes the following to happen:
+                                                         * Resets the logical PCS (LPCS)
+                                                         * Sets the Std 802.3 PCS, FEC and AN registers for the LPCS to their default states
+                                                         * Resets the associated SerDes lanes.
+                                                         It takes up to 32 coprocessor-clock cycles to reset the LPCS, after which RESET is
+                                                         automatically cleared. */
+	uint64_t reserved_14_14               : 1;
+	uint64_t xnp_en                       : 1;  /**< Extended next-page enable. */
+	uint64_t an_en                        : 1;  /**< Auto-Negotiation enable. This bit should not be set when
+                                                         BGX()_CMR()_CONFIG[LMAC_TYPE] is set to RXAUI; auto-negotiation is not supported
+                                                         in RXAUI mode. */
+	uint64_t reserved_10_11               : 2;
+	uint64_t an_restart                   : 1;  /**< Auto-Negotiation restart. Writing a 1 to this bit restarts the Auto-Negotiation process if
+                                                         AN_EN is also set. This is a self-clearing bit. */
+	uint64_t reserved_0_8                 : 9;
+#else
+	uint64_t reserved_0_8                 : 9;
+	uint64_t an_restart                   : 1;
+	uint64_t reserved_10_11               : 2;
+	uint64_t an_en                        : 1;
+	uint64_t xnp_en                       : 1;
+	uint64_t reserved_14_14               : 1;
+	uint64_t an_reset                     : 1;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_an_control_s    cn78xx;
+};
+typedef union cvmx_bgxx_spux_an_control cvmx_bgxx_spux_an_control_t;
+
+/**
+ * cvmx_bgx#_spu#_an_lp_base
+ *
+ * This register captures the contents of the latest AN link code word base page received from
+ * the link partner during Auto-Negotiation. (See Std 802.3 section 73.6 for details.)
+ * BGX()_SPU()_AN_STATUS[PAGE_RX] is set when this register is updated by hardware.
+ */
+union cvmx_bgxx_spux_an_lp_base {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_an_lp_base_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t fec_req                      : 1;  /**< FEC requested. */
+	uint64_t fec_able                     : 1;  /**< FEC ability. */
+	uint64_t arsv                         : 19; /**< Technology ability. Reserved bits, should always be 0. */
+	uint64_t a100g_cr10                   : 1;  /**< 100GBASE-CR10 ability. */
+	uint64_t a40g_cr4                     : 1;  /**< 40GBASE-CR4 ability. */
+	uint64_t a40g_kr4                     : 1;  /**< 40GBASE-KR4 ability. */
+	uint64_t a10g_kr                      : 1;  /**< 10GBASE-KR ability. */
+	uint64_t a10g_kx4                     : 1;  /**< 10GBASE-KX4 ability. */
+	uint64_t a1g_kx                       : 1;  /**< 1000BASE-KX ability. */
+	uint64_t t                            : 5;  /**< Transmitted nonce. */
+	uint64_t np                           : 1;  /**< Next page. */
+	uint64_t ack                          : 1;  /**< Acknowledge. */
+	uint64_t rf                           : 1;  /**< Remote fault. */
+	uint64_t xnp_able                     : 1;  /**< Extended next page ability. */
+	uint64_t asm_dir                      : 1;  /**< Asymmetric PAUSE. */
+	uint64_t pause                        : 1;  /**< PAUSE ability. */
+	uint64_t e                            : 5;  /**< Echoed nonce. */
+	uint64_t s                            : 5;  /**< Selector. */
+#else
+	uint64_t s                            : 5;
+	uint64_t e                            : 5;
+	uint64_t pause                        : 1;
+	uint64_t asm_dir                      : 1;
+	uint64_t xnp_able                     : 1;
+	uint64_t rf                           : 1;
+	uint64_t ack                          : 1;
+	uint64_t np                           : 1;
+	uint64_t t                            : 5;
+	uint64_t a1g_kx                       : 1;
+	uint64_t a10g_kx4                     : 1;
+	uint64_t a10g_kr                      : 1;
+	uint64_t a40g_kr4                     : 1;
+	uint64_t a40g_cr4                     : 1;
+	uint64_t a100g_cr10                   : 1;
+	uint64_t arsv                         : 19;
+	uint64_t fec_able                     : 1;
+	uint64_t fec_req                      : 1;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_an_lp_base_s    cn78xx;
+};
+typedef union cvmx_bgxx_spux_an_lp_base cvmx_bgxx_spux_an_lp_base_t;
+
+/**
+ * cvmx_bgx#_spu#_an_lp_xnp
+ *
+ * This register captures the contents of the latest next page code word received from the link
+ * partner during Auto-Negotiation, if any. See section 802.3 section 73.7.7 for details.
+ */
+union cvmx_bgxx_spux_an_lp_xnp {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_an_lp_xnp_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t u                            : 32; /**< Unformatted code field. */
+	uint64_t np                           : 1;  /**< Next page. */
+	uint64_t ack                          : 1;  /**< Acknowledge. */
+	uint64_t mp                           : 1;  /**< Message page. */
+	uint64_t ack2                         : 1;  /**< Acknowledge 2. */
+	uint64_t toggle                       : 1;  /**< Toggle. */
+	uint64_t m_u                          : 11; /**< Message/unformatted code field. */
+#else
+	uint64_t m_u                          : 11;
+	uint64_t toggle                       : 1;
+	uint64_t ack2                         : 1;
+	uint64_t mp                           : 1;
+	uint64_t ack                          : 1;
+	uint64_t np                           : 1;
+	uint64_t u                            : 32;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_an_lp_xnp_s     cn78xx;
+};
+typedef union cvmx_bgxx_spux_an_lp_xnp cvmx_bgxx_spux_an_lp_xnp_t;
+
+/**
+ * cvmx_bgx#_spu#_an_status
+ */
+union cvmx_bgxx_spux_an_status {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_an_status_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_10_63               : 54;
+	uint64_t prl_flt                      : 1;  /**< Parallel detection fault. Always 0; SPU does not support parallel detection as part of the
+                                                         auto-negotiation protocol. */
+	uint64_t reserved_8_8                 : 1;
+	uint64_t xnp_stat                     : 1;  /**< Extended next-page status. */
+	uint64_t page_rx                      : 1;  /**< Page received. This latching-high bit is set when a new page has been received and stored
+                                                         in BGX()_SPU()_AN_LP_BASE or BGX()_SPU()_AN_LP_XNP; stays set until a 1 is
+                                                         written by software, Auto-Negotiation is disabled or restarted, or next page exchange is
+                                                         initiated. Note that in order to avoid read side effects, this is implemented as a
+                                                         write-1-to-clear bit, rather than latching high read-only as specified in 802.3. */
+	uint64_t an_complete                  : 1;  /**< Auto-Negotiation complete. Set when the Auto-Negotiation process has been completed and
+                                                         the link is up and running using the negotiated highest common denominator (HCD)
+                                                         technology. If AN is enabled (BGX()_SPU()_AN_CONTROL[AN_EN] = 1) and this bit is
+                                                         read as a zero, it indicates that the AN process has not been completed, and the contents
+                                                         of BGX()_SPU()_AN_LP_BASE, BGX()_SPU()_AN_XNP_TX, and
+                                                         BGX()_SPU()_AN_LP_XNP are as defined by the current state of the Auto-Negotiation
+                                                         protocol, or as written for manual configuration. This bit is always zero when AN is
+                                                         disabled (BGX()_SPU()_AN_CONTROL[AN_EN] = 0). */
+	uint64_t rmt_flt                      : 1;  /**< Remote fault: Always 0. */
+	uint64_t an_able                      : 1;  /**< Auto-Negotiation ability: Always 1. */
+	uint64_t link_status                  : 1;  /**< Link status. This bit captures the state of the link_status variable as defined in 802.3
+                                                         section 73.9.1. When set, indicates that a valid link has been established. When clear,
+                                                         indicates that the link has been invalid after this bit was last set by software. Latching
+                                                         low bit; stays clear until a 1 is written by software. Note that in order to avoid read
+                                                         side effects, this is implemented as a write-1-to-set bit, rather than latching low read-
+                                                         only as specified in 802.3. */
+	uint64_t reserved_1_1                 : 1;
+	uint64_t lp_an_able                   : 1;  /**< Link partner Auto-Negotiation ability. Set to indicate that the link partner is able to
+                                                         participate in the Auto-Negotiation function, and cleared otherwise. */
+#else
+	uint64_t lp_an_able                   : 1;
+	uint64_t reserved_1_1                 : 1;
+	uint64_t link_status                  : 1;
+	uint64_t an_able                      : 1;
+	uint64_t rmt_flt                      : 1;
+	uint64_t an_complete                  : 1;
+	uint64_t page_rx                      : 1;
+	uint64_t xnp_stat                     : 1;
+	uint64_t reserved_8_8                 : 1;
+	uint64_t prl_flt                      : 1;
+	uint64_t reserved_10_63               : 54;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_an_status_s     cn78xx;
+};
+typedef union cvmx_bgxx_spux_an_status cvmx_bgxx_spux_an_status_t;
+
+/**
+ * cvmx_bgx#_spu#_an_xnp_tx
+ *
+ * Software programs this register with the contents of the AN message next page or unformatted
+ * next page link code word to be transmitted during auto-negotiation. Next page exchange occurs
+ * after the base link code words have been exchanged if either end of the link segment sets the
+ * NP bit to 1, indicating that it has at least one next page to send. Once initiated, next page
+ * exchange continues until both ends of the link segment set their NP bits to 0. See section
+ * 802.3 section 73.7.7 for details.
+ */
+union cvmx_bgxx_spux_an_xnp_tx {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_an_xnp_tx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t u                            : 32; /**< Unformatted code field. When the MP bit is set, this field contains the 32-bit unformatted
+                                                         code field of the message next page. When MP is clear, this field contains the upper 32
+                                                         bits of the 43-bit unformatted code field of the unformatted next page. */
+	uint64_t np                           : 1;  /**< Next page. */
+	uint64_t ack                          : 1;  /**< Acknowledge: Always 0 in this register. */
+	uint64_t mp                           : 1;  /**< Message page. Set to indicate that this register contains a message next page. Clear to
+                                                         indicate that the register contains an unformatted next page. */
+	uint64_t ack2                         : 1;  /**< Acknowledge 2. Indicates that the receiver is able to act on the information (or perform
+                                                         the task) defined in the message. */
+	uint64_t toggle                       : 1;  /**< This bit is ignored by hardware. The value of the TOGGLE bit in transmitted next pages is
+                                                         automatically generated by hardware. */
+	uint64_t m_u                          : 11; /**< Message/Unformatted code field: When the MP bit is set, this field contains the message
+                                                         code field (M) of the message next page. When MP is clear, this field contains the lower
+                                                         11 bits of the 43-bit unformatted code field of the unformatted next page. */
+#else
+	uint64_t m_u                          : 11;
+	uint64_t toggle                       : 1;
+	uint64_t ack2                         : 1;
+	uint64_t mp                           : 1;
+	uint64_t ack                          : 1;
+	uint64_t np                           : 1;
+	uint64_t u                            : 32;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_an_xnp_tx_s     cn78xx;
+};
+typedef union cvmx_bgxx_spux_an_xnp_tx cvmx_bgxx_spux_an_xnp_tx_t;
+
+/**
+ * cvmx_bgx#_spu#_br_algn_status
+ *
+ * This register implements the Std 802.3 multilane BASE-R PCS alignment status 1-4 registers
+ * (3.50-3.53). It is valid only when the LPCS type is 40GBASE-R
+ * (BGX()_CMR()_CONFIG[LMAC_TYPE] = 0x4), and always returns 0x0 for all other LPCS
+ * types. Std 802.3 bits that are not applicable to 40GBASE-R (e.g. status bits for PCS lanes
+ * 19-4) are not implemented and marked as reserved. PCS lanes 3-0 are valid and are mapped to
+ * physical SerDes lanes based on the programming of BGX()_CMR()_CONFIG[[LANE_TO_SDS].
+ */
+union cvmx_bgxx_spux_br_algn_status {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_br_algn_status_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_36_63               : 28;
+	uint64_t marker_lock                  : 4;  /**< Marker-locked status for PCS lanes 3-0.
+                                                         0 = Not locked.
+                                                         1 = Locked. */
+	uint64_t reserved_13_31               : 19;
+	uint64_t alignd                       : 1;  /**< All lanes are locked and aligned. This bit returns 1 when the logical PCS has locked and
+                                                         aligned all associated receive lanes; returns 0 otherwise. For all other PCS types, this
+                                                         bit always returns 0. */
+	uint64_t reserved_4_11                : 8;
+	uint64_t block_lock                   : 4;  /**< Block-lock status for PCS lanes 3-0:
+                                                         0 = Not locked.
+                                                         1 = Locked. */
+#else
+	uint64_t block_lock                   : 4;
+	uint64_t reserved_4_11                : 8;
+	uint64_t alignd                       : 1;
+	uint64_t reserved_13_31               : 19;
+	uint64_t marker_lock                  : 4;
+	uint64_t reserved_36_63               : 28;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_br_algn_status_s cn78xx;
+};
+typedef union cvmx_bgxx_spux_br_algn_status cvmx_bgxx_spux_br_algn_status_t;
+
+/**
+ * cvmx_bgx#_spu#_br_bip_err_cnt
+ *
+ * This register implements the Std 802.3 BIP error-counter registers for PCS lanes 0-3
+ * (3.200-3.203). It is valid only when the LPCS type is 40GBASE-R
+ * (BGX()_CMR()_CONFIG[LMAC_TYPE] = 0x4), and always returns 0x0 for all other LPCS
+ * types. The counters are indexed by the RX PCS lane number based on the Alignment Marker
+ * detected on each lane and captured in BGX()_SPU()_BR_LANE_MAP. Each counter counts the
+ * BIP errors for its PCS lane, and is held at all ones in case of overflow. The counters are
+ * reset to all 0s when this register is read by software.
+ *
+ * The reset operation takes precedence over the increment operation; if the register is read on
+ * the same clock cycle as an increment operation, the counter is reset to all 0s and the
+ * increment operation is lost. The counters are writable for test purposes, rather than read-
+ * only as specified in Std 802.3.
+ */
+union cvmx_bgxx_spux_br_bip_err_cnt {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_br_bip_err_cnt_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t bip_err_cnt_ln3              : 16; /**< BIP error counter for lane on which PCS lane 3 markers are received. */
+	uint64_t bip_err_cnt_ln2              : 16; /**< BIP error counter for lane on which PCS lane 2 markers are received. */
+	uint64_t bip_err_cnt_ln1              : 16; /**< BIP error counter for lane on which PCS lane 1 markers are received. */
+	uint64_t bip_err_cnt_ln0              : 16; /**< BIP error counter for lane on which PCS lane 0 markers are received. */
+#else
+	uint64_t bip_err_cnt_ln0              : 16;
+	uint64_t bip_err_cnt_ln1              : 16;
+	uint64_t bip_err_cnt_ln2              : 16;
+	uint64_t bip_err_cnt_ln3              : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_br_bip_err_cnt_s cn78xx;
+};
+typedef union cvmx_bgxx_spux_br_bip_err_cnt cvmx_bgxx_spux_br_bip_err_cnt_t;
+
+/**
+ * cvmx_bgx#_spu#_br_lane_map
+ *
+ * This register implements the Std 802.3 lane 0-3 mapping registers (3.400-3.403). It is valid
+ * only when the LPCS type is 40GBASE-R (BGX()_CMR()_CONFIG[LMAC_TYPE] = 0x4), and always
+ * returns 0x0 for all other LPCS types. The LNx_MAPPING field for each programmed PCS lane
+ * (called service interface in 802.3ba-2010) is valid when that lane has achieved alignment
+ * marker lock on the receive side (i.e. the associated
+ * BGX()_SPU()_BR_ALGN_STATUS[MARKER_LOCK] = 1), and is invalid otherwise. When valid, it
+ * returns the actual detected receive PCS lane number based on the received alignment marker
+ * contents received on that service interface.
+ *
+ * The mapping is flexible because Std 802.3 allows multilane BASE-R receive lanes to be re-
+ * ordered. Note that for the transmit side, each PCS lane is mapped to a physical SerDes lane
+ * based on the programming of BGX()_CMR()_CONFIG[LANE_TO_SDS]. For the receive side,
+ * BGX()_CMR()_CONFIG[LANE_TO_SDS] specifies the service interface to physical SerDes
+ * lane mapping, and this register specifies the PCS lane to service interface mapping.
+ */
+union cvmx_bgxx_spux_br_lane_map {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_br_lane_map_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_54_63               : 10;
+	uint64_t ln3_mapping                  : 6;  /**< PCS lane number received on service interface 3 */
+	uint64_t reserved_38_47               : 10;
+	uint64_t ln2_mapping                  : 6;  /**< PCS lane number received on service interface 2 */
+	uint64_t reserved_22_31               : 10;
+	uint64_t ln1_mapping                  : 6;  /**< PCS lane number received on service interface 1 */
+	uint64_t reserved_6_15                : 10;
+	uint64_t ln0_mapping                  : 6;  /**< PCS lane number received on service interface 0 */
+#else
+	uint64_t ln0_mapping                  : 6;
+	uint64_t reserved_6_15                : 10;
+	uint64_t ln1_mapping                  : 6;
+	uint64_t reserved_22_31               : 10;
+	uint64_t ln2_mapping                  : 6;
+	uint64_t reserved_38_47               : 10;
+	uint64_t ln3_mapping                  : 6;
+	uint64_t reserved_54_63               : 10;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_br_lane_map_s   cn78xx;
+};
+typedef union cvmx_bgxx_spux_br_lane_map cvmx_bgxx_spux_br_lane_map_t;
+
+/**
+ * cvmx_bgx#_spu#_br_pmd_control
+ */
+union cvmx_bgxx_spux_br_pmd_control {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_br_pmd_control_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_2_63                : 62;
+	uint64_t train_en                     : 1;  /**< BASE-R training enable */
+	uint64_t train_restart                : 1;  /**< BASE-R training restart. Writing a 1 to this bit restarts the training process if the
+                                                         TRAIN_EN bit in this register is also set. This is a self-clearing bit. Software should
+                                                         wait a minimum of 1.7ms after BGX()_SPU()_INT[TRAINING_FAILURE] is set before
+                                                         restarting the training process. */
+#else
+	uint64_t train_restart                : 1;
+	uint64_t train_en                     : 1;
+	uint64_t reserved_2_63                : 62;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_br_pmd_control_s cn78xx;
+};
+typedef union cvmx_bgxx_spux_br_pmd_control cvmx_bgxx_spux_br_pmd_control_t;
+
+/**
+ * cvmx_bgx#_spu#_br_pmd_ld_cup
+ *
+ * This register implements 802.3 MDIO register 1.153 for 10GBASE-R (when
+ * BGX()_CMR()_CONFIG[LMAC_TYPE] = 10G_R)
+ * and MDIO registers 1.1300-1.1303 for 40GBASE-R (when
+ * LMAC_TYPE = 40G_R). It is automatically cleared at the start of training. When link training
+ * is in progress, each field reflects the contents of the coefficient update field in the
+ * associated lane's outgoing training frame. The fields in this register are read/write even
+ * though they are specified as read-only in 802.3.
+ *
+ * If BGX()_SPU_DBG_CONTROL[BR_PMD_TRAIN_SOFT_EN] is set, then this register must be updated
+ * by software during link training and hardware updates are disabled. If
+ * BGX()_SPU_DBG_CONTROL[BR_PMD_TRAIN_SOFT_EN] is clear, this register is automatically
+ * updated by hardware, and it should not be written by software. The lane fields in this
+ * register are indexed by logical PCS lane ID.
+ *
+ * The lane 0 field (LN0_*) is valid for both
+ * 10GBASE-R and 40GBASE-R. The remaining fields (LN1_*, LN2_*, LN3_*) are only valid for
+ * 40GBASE-R.
+ */
+union cvmx_bgxx_spux_br_pmd_ld_cup {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_br_pmd_ld_cup_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t ln3_cup                      : 16; /**< PCS lane 3 coefficient update: format defined by BGX_SPU_BR_TRAIN_CUP_S. Not valid for
+                                                         10GBASE-R. */
+	uint64_t ln2_cup                      : 16; /**< PCS lane 2 coefficient update: format defined by BGX_SPU_BR_TRAIN_CUP_S. Not valid for
+                                                         10GBASE-R. */
+	uint64_t ln1_cup                      : 16; /**< PCS lane 1 coefficient update: format defined by BGX_SPU_BR_TRAIN_CUP_S. Not valid for
+                                                         10GBASE-R. */
+	uint64_t ln0_cup                      : 16; /**< PCS lane 0 coefficient update: format defined by BGX_SPU_BR_TRAIN_CUP_S. */
+#else
+	uint64_t ln0_cup                      : 16;
+	uint64_t ln1_cup                      : 16;
+	uint64_t ln2_cup                      : 16;
+	uint64_t ln3_cup                      : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_br_pmd_ld_cup_s cn78xx;
+};
+typedef union cvmx_bgxx_spux_br_pmd_ld_cup cvmx_bgxx_spux_br_pmd_ld_cup_t;
+
+/**
+ * cvmx_bgx#_spu#_br_pmd_ld_rep
+ *
+ * This register implements 802.3 MDIO register 1.154 for 10GBASE-R (when
+ * BGX()_CMR()_CONFIG[LMAC_TYPE] = IOG_R) and MDIO registers 1.1400-1.1403 for 40GBASE-R
+ * (when LMAC_TYPE = 40G_R). It is automatically cleared at the start of training. Each field
+ * reflects the contents of the status report field in the associated lane's outgoing training
+ * frame. The fields in this register are read/write even though they are specified as read-only
+ * in 802.3. If BGX()_SPU_DBG_CONTROL[BR_PMD_TRAIN_SOFT_EN] is set, then this register must
+ * be updated by software during link training and hardware updates are disabled. If
+ * BGX()_SPU_DBG_CONTROL[BR_PMD_TRAIN_SOFT_EN] is clear, this register is automatically
+ * updated by hardware, and it should not be written by software. The lane fields in this
+ * register are indexed by logical PCS lane ID.
+ *
+ * The lane 0 field (LN0_*) is valid for both
+ * 10GBASE-R and 40GBASE-R. The remaining fields (LN1_*, LN2_*, LN3_*) are only valid for
+ * 40GBASE-R.
+ */
+union cvmx_bgxx_spux_br_pmd_ld_rep {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_br_pmd_ld_rep_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t ln3_rep                      : 16; /**< PCS lane 3 status report: format defined by BGX_SPU_BR_TRAIN_REP_S. Not valid for
+                                                         10GBASE-R. */
+	uint64_t ln2_rep                      : 16; /**< PCS lane 2 status report: format defined by BGX_SPU_BR_TRAIN_REP_S. Not valid for
+                                                         10GBASE-R. */
+	uint64_t ln1_rep                      : 16; /**< PCS lane 1 status report: format defined by BGX_SPU_BR_TRAIN_REP_S. Not valid for
+                                                         10GBASE-R. */
+	uint64_t ln0_rep                      : 16; /**< PCS lane 0 status report: format defined by BGX_SPU_BR_TRAIN_REP_S. */
+#else
+	uint64_t ln0_rep                      : 16;
+	uint64_t ln1_rep                      : 16;
+	uint64_t ln2_rep                      : 16;
+	uint64_t ln3_rep                      : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_br_pmd_ld_rep_s cn78xx;
+};
+typedef union cvmx_bgxx_spux_br_pmd_ld_rep cvmx_bgxx_spux_br_pmd_ld_rep_t;
+
+/**
+ * cvmx_bgx#_spu#_br_pmd_lp_cup
+ *
+ * This register implements 802.3 MDIO register 1.152 for 10GBASE-R (when
+ * BGX()_CMR()_CONFIG[LMAC_TYPE] = 10G_R)
+ * and MDIO registers 1.1100-1.1103 for 40GBASE-R (when
+ * LMAC_TYPE = 40G_R). It is automatically cleared at the start of training. Each field reflects
+ * the contents of the coefficient update field in the lane's most recently received training
+ * frame. This register should not be written when link training is enabled, i.e. when TRAIN_EN
+ * is set BR_PMD_CONTROL. The lane fields in this register are indexed by logical PCS lane ID.
+ *
+ * The lane 0 field (LN0_*) is valid for both 10GBASE-R and 40GBASE-R. The remaining fields
+ * (LN1_*, LN2_*, LN3_*) are only valid for 40GBASE-R.
+ */
+union cvmx_bgxx_spux_br_pmd_lp_cup {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_br_pmd_lp_cup_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t ln3_cup                      : 16; /**< PCS lane 3 coefficient update: format defined by BGX_SPU_BR_TRAIN_CUP_S. Not valid for
+                                                         10GBASE-R. */
+	uint64_t ln2_cup                      : 16; /**< PCS lane 2 coefficient update: format defined by BGX_SPU_BR_TRAIN_CUP_S. Not valid for
+                                                         10GBASE-R. */
+	uint64_t ln1_cup                      : 16; /**< PCS lane 1 coefficient update: format defined by BGX_SPU_BR_TRAIN_CUP_S. Not valid for
+                                                         10GBASE-R. */
+	uint64_t ln0_cup                      : 16; /**< PCS lane 0 coefficient update: format defined by BGX_SPU_BR_TRAIN_CUP_S. */
+#else
+	uint64_t ln0_cup                      : 16;
+	uint64_t ln1_cup                      : 16;
+	uint64_t ln2_cup                      : 16;
+	uint64_t ln3_cup                      : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_br_pmd_lp_cup_s cn78xx;
+};
+typedef union cvmx_bgxx_spux_br_pmd_lp_cup cvmx_bgxx_spux_br_pmd_lp_cup_t;
+
+/**
+ * cvmx_bgx#_spu#_br_pmd_lp_rep
+ *
+ * This register implements 802.3 MDIO register 1.153 for 10GBASE-R (when
+ * BGX()_CMR()_CONFIG[LMAC_TYPE] = 10G_R)
+ * and MDIO registers 1.1200-1.1203 for 40GBASE-R (when
+ * LMAC_TYPE = 40G_R). It is automatically cleared at the start of training. Each field reflects
+ * the contents of the status report field in the associated lane's most recently received
+ * training frame. The lane fields in this register are indexed by logical PCS lane ID.
+ *
+ * The lane
+ * 0 field (LN0_*) is valid for both 10GBASE-R and 40GBASE-R. The remaining fields (LN1_*, LN2_*,
+ * LN3_*) are only valid for 40GBASE-R.
+ */
+union cvmx_bgxx_spux_br_pmd_lp_rep {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_br_pmd_lp_rep_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t ln3_rep                      : 16; /**< PCS lane 3 status report: format defined by BGX_SPU_BR_TRAIN_REP_S. Not valid for
+                                                         10GBASE-R. */
+	uint64_t ln2_rep                      : 16; /**< PCS lane 2 status report: format defined by BGX_SPU_BR_TRAIN_REP_S. Not valid for
+                                                         10GBASE-R. */
+	uint64_t ln1_rep                      : 16; /**< PCS lane 1 status report: format defined by BGX_SPU_BR_TRAIN_REP_S. Not valid for
+                                                         10GBASE-R. */
+	uint64_t ln0_rep                      : 16; /**< PCS lane 0 status report: format defined by BGX_SPU_BR_TRAIN_REP_S. */
+#else
+	uint64_t ln0_rep                      : 16;
+	uint64_t ln1_rep                      : 16;
+	uint64_t ln2_rep                      : 16;
+	uint64_t ln3_rep                      : 16;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_br_pmd_lp_rep_s cn78xx;
+};
+typedef union cvmx_bgxx_spux_br_pmd_lp_rep cvmx_bgxx_spux_br_pmd_lp_rep_t;
+
+/**
+ * cvmx_bgx#_spu#_br_pmd_status
+ *
+ * The lane fields in this register are indexed by logical PCS lane ID. The lane 0 field (LN0_*)
+ * is valid for both 10GBASE-R and 40GBASE-R. The remaining fields (LN1_*, LN2_*, LN3_*) are only
+ * valid for 40GBASE-R.
+ */
+union cvmx_bgxx_spux_br_pmd_status {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_br_pmd_status_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t ln3_train_status             : 4;  /**< PCS lane 3 link training status. Format defined by BGX_SPU_BR_LANE_TRAIN_STATUS_S. Not
+                                                         valid for 10GBASE-R. */
+	uint64_t ln2_train_status             : 4;  /**< PCS lane 2 link training status. Format defined by BGX_SPU_BR_LANE_TRAIN_STATUS_S. Not
+                                                         valid for 10GBASE-R. */
+	uint64_t ln1_train_status             : 4;  /**< PCS lane 1 link training status. Format defined by BGX_SPU_BR_LANE_TRAIN_STATUS_S. Not
+                                                         valid for 10GBASE-R. */
+	uint64_t ln0_train_status             : 4;  /**< PCS lane 0 link training status. Format defined by BGX_SPU_BR_LANE_TRAIN_STATUS_S. */
+#else
+	uint64_t ln0_train_status             : 4;
+	uint64_t ln1_train_status             : 4;
+	uint64_t ln2_train_status             : 4;
+	uint64_t ln3_train_status             : 4;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_br_pmd_status_s cn78xx;
+};
+typedef union cvmx_bgxx_spux_br_pmd_status cvmx_bgxx_spux_br_pmd_status_t;
+
+/**
+ * cvmx_bgx#_spu#_br_status1
+ */
+union cvmx_bgxx_spux_br_status1 {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_br_status1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_13_63               : 51;
+	uint64_t rcv_lnk                      : 1;  /**< BASE-R receive link status.
+                                                         0 = BASE-R PCS receive-link down.
+                                                         1 = BASE-R PCS receive-link up
+                                                         This bit is a reflection of the PCS_status variable defined in Std 802.3 sections
+                                                         49.2.14.1 and 82.3.1. */
+	uint64_t reserved_4_11                : 8;
+	uint64_t prbs9                        : 1;  /**< 10GBASE-R PRBS9 pattern testing ability. Always 0; PRBS9 pattern testing is not supported. */
+	uint64_t prbs31                       : 1;  /**< 10GBASE-R PRBS31 pattern testing ability. Always 0; PRBS31 pattern testing is not supported. */
+	uint64_t hi_ber                       : 1;  /**< BASE-R PCS high bit-error rate.
+                                                         0 = 64B/66B receiver is detecting a bit-error rate of < 10.4.
+                                                         1 = 64B/66B receiver is detecting a bit-error rate of >= 10.4.
+                                                         This bit is a direct reflection of the state of the HI_BER variable in the 64B/66B state
+                                                         diagram and is defined in Std 802.3 sections 49.2.13.2.2 and 82.2.18.2.2. */
+	uint64_t blk_lock                     : 1;  /**< BASE-R PCS block lock.
+                                                         0 = No block lock.
+                                                         1 = 64B/66B receiver for BASE-R has block lock.
+                                                         This bit is a direct reflection of the state of the BLOCK_LOCK variable in the 64B/66B
+                                                         state diagram and is defined in Std 802.3 sections 49.2.13.2.2 and 82.2.18.2.2.
+                                                         For a multilane logical PCS (i.e. 40GBASE-R), this bit indicates that the receiver has
+                                                         both block lock and alignment for all lanes and is identical to
+                                                         BGX()_SPU()_BR_ALGN_STATUS[ALIGND]. */
+#else
+	uint64_t blk_lock                     : 1;
+	uint64_t hi_ber                       : 1;
+	uint64_t prbs31                       : 1;
+	uint64_t prbs9                        : 1;
+	uint64_t reserved_4_11                : 8;
+	uint64_t rcv_lnk                      : 1;
+	uint64_t reserved_13_63               : 51;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_br_status1_s    cn78xx;
+};
+typedef union cvmx_bgxx_spux_br_status1 cvmx_bgxx_spux_br_status1_t;
+
+/**
+ * cvmx_bgx#_spu#_br_status2
+ *
+ * This register implements a combination of the following Std 802.3 registers:
+ * * BASE-R PCS status 2 (MDIO address 3.33).
+ * * BASE-R BER high-order counter (MDIO address 3.44).
+ * * Errored-blocks high-order counter (MDIO address 3.45).
+ *
+ * Note that the relative locations of some fields have been moved from Std 802.3 in order to
+ * make the register layout more software friendly: the BER counter high-order and low-order bits
+ * from sections 3.44 and 3.33 have been combined into the contiguous, 22-bit BER_CNT field;
+ * likewise, the errored-blocks counter high-order and low-order bits from section 3.45 have been
+ * combined into the contiguous, 22-bit ERR_BLKS field.
+ */
+union cvmx_bgxx_spux_br_status2 {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_br_status2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_62_63               : 2;
+	uint64_t err_blks                     : 22; /**< Errored-blocks counter. This is the BASE-R errored-blocks counter as defined by the
+                                                         errored_block_count variable specified in Std 802.3 sections 49.2.14.2 and 82.2.18.2.4. It
+                                                         increments by 1 on each block for which the BASE-R receive state machine, specified in Std
+                                                         802.3 diagrams 49-15 and 82-15, enters the RX_E state.
+                                                         Back-to-back blocks in the RX_E state are counted as transitions from RX_E to RX_E and
+                                                         keep incrementing the counter. The counter is reset to all 0s after this register is read
+                                                         by software.
+                                                         The reset operation takes precedence over the increment operation: if the register is read
+                                                         on the same clock cycle as an increment operation, the counter is reset to all 0s and the
+                                                         increment operation is lost.
+                                                         This field is writable for test purposes, rather than read-only as specified in Std 802.3. */
+	uint64_t reserved_38_39               : 2;
+	uint64_t ber_cnt                      : 22; /**< Bit-error-rate counter. This is the BASE-R BER counter as defined by the BER_COUNT
+                                                         variable in Std 802.3 sections 49.2.14.2 and 82.2.18.2.4. The counter is reset to all 0s
+                                                         after this register is read by software, and is held at all 1s in case of overflow.
+                                                         The reset operation takes precedence over the increment operation: if the register is read
+                                                         on the same clock cycle an increment operation, the counter is reset to all 0s and the
+                                                         increment operation is lost.
+                                                         This field is writable for test purposes, rather than read-only as specified in Std 802.3. */
+	uint64_t latched_lock                 : 1;  /**< Latched-block lock.
+                                                         0 = No block.
+                                                         1 = 64B/66B receiver for BASE-R has block lock.
+                                                         This is a latching-low version of BGX()_SPU()_BR_STATUS1[BLK_LOCK]; it stays clear
+                                                         until the register is read by software.
+                                                         Note that in order to avoid read side effects, this is implemented as a write-1-to-set
+                                                         bit, rather than latching low read-only as specified in 802.3. */
+	uint64_t latched_ber                  : 1;  /**< Latched-high bit-error rate.
+                                                         0 = Not a high BER.
+                                                         1 = 64B/66B receiver is detecting a high BER.
+                                                         This is a latching-high version of BGX()_SPU()_BR_STATUS1[HI_BER]; it stays set until
+                                                         the register is read by software.
+                                                         Note that in order to avoid read side effects, this is implemented as a write-1-to-clear
+                                                         bit, rather than latching high read-only as specified in 802.3. */
+	uint64_t reserved_0_13                : 14;
+#else
+	uint64_t reserved_0_13                : 14;
+	uint64_t latched_ber                  : 1;
+	uint64_t latched_lock                 : 1;
+	uint64_t ber_cnt                      : 22;
+	uint64_t reserved_38_39               : 2;
+	uint64_t err_blks                     : 22;
+	uint64_t reserved_62_63               : 2;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_br_status2_s    cn78xx;
+};
+typedef union cvmx_bgxx_spux_br_status2 cvmx_bgxx_spux_br_status2_t;
+
+/**
+ * cvmx_bgx#_spu#_br_tp_control
+ *
+ * Refer to the test pattern methodology described in 802.3 sections 49.2.8 and 82.2.10.
+ *
+ */
+union cvmx_bgxx_spux_br_tp_control {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_br_tp_control_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_8_63                : 56;
+	uint64_t scramble_tp                  : 1;  /**< Select scrambled idle test pattern. This bit selects the transmit test pattern used when
+                                                         TX_TP_EN is set:
+                                                         0 = Square wave test pattern.
+                                                         1 = Scrambled idle test pattern. */
+	uint64_t prbs9_tx                     : 1;  /**< 10GBASE-R PRBS9 TP transmit enable. Always 0; PRBS9 pattern testing is not supported. */
+	uint64_t prbs31_rx                    : 1;  /**< 10GBASE-R PRBS31 TP receive enable. Always 0; PRBS31 pattern testing is not supported. */
+	uint64_t prbs31_tx                    : 1;  /**< 10GBASE-R PRBS31 TP transmit enable. Always 0; PRBS31 pattern is not supported. */
+	uint64_t tx_tp_en                     : 1;  /**< Transmit-test-pattern enable. */
+	uint64_t rx_tp_en                     : 1;  /**< Receive-test-pattern enable. The only supported receive test pattern is the scrambled idle
+                                                         test pattern. Setting this bit enables checking of that receive pattern. */
+	uint64_t tp_sel                       : 1;  /**< Square/PRBS test pattern select. Always 1 to select square wave test pattern; PRBS test
+                                                         patterns are not supported. */
+	uint64_t dp_sel                       : 1;  /**< Data pattern select. Always 0; PRBS test patterns are not supported. */
+#else
+	uint64_t dp_sel                       : 1;
+	uint64_t tp_sel                       : 1;
+	uint64_t rx_tp_en                     : 1;
+	uint64_t tx_tp_en                     : 1;
+	uint64_t prbs31_tx                    : 1;
+	uint64_t prbs31_rx                    : 1;
+	uint64_t prbs9_tx                     : 1;
+	uint64_t scramble_tp                  : 1;
+	uint64_t reserved_8_63                : 56;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_br_tp_control_s cn78xx;
+};
+typedef union cvmx_bgxx_spux_br_tp_control cvmx_bgxx_spux_br_tp_control_t;
+
+/**
+ * cvmx_bgx#_spu#_br_tp_err_cnt
+ *
+ * This register provides the BASE-R PCS test-pattern error counter.
+ *
+ */
+union cvmx_bgxx_spux_br_tp_err_cnt {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_br_tp_err_cnt_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t err_cnt                      : 16; /**< Error counter. This 16-bit counter contains the number of errors received during a pattern
+                                                         test. These bits are reset to all 0s when this register is read by software, and they are
+                                                         held at all 1s in the case of overflow.
+                                                         The test pattern methodology is described in Std 802.3, Sections 49.2.12 and 82.2.10. This
+                                                         counter counts either block errors or bit errors dependent on the test mode (see Section
+                                                         49.2.12). The reset operation takes precedence over the increment operation; if the
+                                                         register is read on the same clock cycle as an increment operation, the counter is reset
+                                                         to all 0s and the increment operation is lost. This field is writable for test purposes,
+                                                         rather than read-only as specified in Std 802.3. */
+#else
+	uint64_t err_cnt                      : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_br_tp_err_cnt_s cn78xx;
+};
+typedef union cvmx_bgxx_spux_br_tp_err_cnt cvmx_bgxx_spux_br_tp_err_cnt_t;
+
+/**
+ * cvmx_bgx#_spu#_bx_status
+ */
+union cvmx_bgxx_spux_bx_status {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_bx_status_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_13_63               : 51;
+	uint64_t alignd                       : 1;  /**< 10GBASE-X lane-alignment status.
+                                                         0 = receive lanes not aligned.
+                                                         1 = receive lanes aligned. */
+	uint64_t pattst                       : 1;  /**< Pattern-testing ability. Always 0; 10GBASE-X pattern is testing not supported. */
+	uint64_t reserved_4_10                : 7;
+	uint64_t lsync                        : 4;  /**< Lane synchronization. BASE-X lane synchronization status for PCS lanes 3-0. Each bit is
+                                                         set when the associated lane is code-group synchronized, and clear otherwise. If the PCS
+                                                         type is RXAUI (i.e. the associated BGX()_CMR()_CONFIG[LMAC_TYPE] = RXAUI), then
+                                                         only lanes 1-0 are valid. */
+#else
+	uint64_t lsync                        : 4;
+	uint64_t reserved_4_10                : 7;
+	uint64_t pattst                       : 1;
+	uint64_t alignd                       : 1;
+	uint64_t reserved_13_63               : 51;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_bx_status_s     cn78xx;
+};
+typedef union cvmx_bgxx_spux_bx_status cvmx_bgxx_spux_bx_status_t;
+
+/**
+ * cvmx_bgx#_spu#_control1
+ */
+union cvmx_bgxx_spux_control1 {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_control1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t reset                        : 1;  /**< Reset. Setting this bit or BGX()_SPU()_AN_CONTROL[AN_RESET] to 1 causes the
+                                                         following to happen:
+                                                         * Resets the logical PCS (LPCS)
+                                                         * Sets the Std 802.3 PCS, FEC and AN registers for the LPCS to their default states
+                                                         * Resets the associated SerDes lanes.
+                                                         It takes up to 32 coprocessor-clock cycles to reset the LPCS, after which RESET is
+                                                         automatically cleared. */
+	uint64_t loopbck                      : 1;  /**< TX-to-RX loopback enable. When set, transmit data for each SerDes lane is looped back as
+                                                         receive data. */
+	uint64_t spdsel1                      : 1;  /**< Speed select 1: always 1. */
+	uint64_t reserved_12_12               : 1;
+	uint64_t lo_pwr                       : 1;  /**< Low power enable. When set, the LPCS is disabled (overriding the associated
+                                                         BGX()_CMR()_CONFIG[ENABLE]), and the SerDes lanes associated with the LPCS are
+                                                         reset. */
+	uint64_t reserved_7_10                : 4;
+	uint64_t spdsel0                      : 1;  /**< Speed select 0: always 1. */
+	uint64_t spd                          : 4;  /**< '"Speed selection.
+                                                         Note that this is a read-only field rather than read/write as
+                                                         specified in 802.3.
+                                                         The LPCS speed is instead configured by the associated
+                                                         BGX()_CMR()_CONFIG[LMAC_TYPE]. The read values returned by this field are as
+                                                         follows:
+                                                         <pre>
+                                                           LMAC_TYPE   Speed       SPD Read Value    Comment
+                                                           ------------------------------------------------------
+                                                           XAUI        10G/20G     0x0               20G if DXAUI
+                                                           RXAUI       10G         0x0
+                                                           10G_R       10G         0x0
+                                                           40G_R       40G         0x3
+                                                           Other       -           X
+                                                         </pre>' */
+	uint64_t reserved_0_1                 : 2;
+#else
+	uint64_t reserved_0_1                 : 2;
+	uint64_t spd                          : 4;
+	uint64_t spdsel0                      : 1;
+	uint64_t reserved_7_10                : 4;
+	uint64_t lo_pwr                       : 1;
+	uint64_t reserved_12_12               : 1;
+	uint64_t spdsel1                      : 1;
+	uint64_t loopbck                      : 1;
+	uint64_t reset                        : 1;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_control1_s      cn78xx;
+};
+typedef union cvmx_bgxx_spux_control1 cvmx_bgxx_spux_control1_t;
+
+/**
+ * cvmx_bgx#_spu#_control2
+ */
+union cvmx_bgxx_spux_control2 {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_control2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_3_63                : 61;
+	uint64_t pcs_type                     : 3;  /**< PCS type selection.
+                                                         Note that this is a read-only field rather than read/write as
+                                                         specified in 802.3.
+                                                         The LPCS speed is instead configured by the associated
+                                                         BGX()_CMR()_CONFIG[LMAC_TYPE]. The read values returned by this field are as
+                                                         follows:
+                                                         <pre>
+                                                                       PCS_TYPE
+                                                           LMAC_TYPE   Read Value      Comment
+                                                           -------------------------------------------------
+                                                           XAUI        0x1             10GBASE-X PCS type
+                                                           RXAUI       0x1             10GBASE-X PCS type
+                                                           10G_R       0x0             10GBASE-R PCS type
+                                                           40G_R       0x4             40GBASE-R PCS type
+                                                           Other       Undefined       Reserved
+                                                         </pre> */
+#else
+	uint64_t pcs_type                     : 3;
+	uint64_t reserved_3_63                : 61;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_control2_s      cn78xx;
+};
+typedef union cvmx_bgxx_spux_control2 cvmx_bgxx_spux_control2_t;
+
+/**
+ * cvmx_bgx#_spu#_fec_abil
+ */
+union cvmx_bgxx_spux_fec_abil {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_fec_abil_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_2_63                : 62;
+	uint64_t err_abil                     : 1;  /**< BASE-R FEC error-indication ability. Always 1 when the LPCS type is BASE-R,
+                                                         i.e. BGX()_CMR()_CONFIG[LMAC_TYPE] = 0x3 or 0x4. Always 0 otherwise. */
+	uint64_t fec_abil                     : 1;  /**< BASE-R FEC ability. Always 1 when the LPCS type is BASE-R,
+                                                         i.e. BGX()_CMR()_CONFIG[LMAC_TYPE] = 0x3 or 0x4. Always 0 otherwise. */
+#else
+	uint64_t fec_abil                     : 1;
+	uint64_t err_abil                     : 1;
+	uint64_t reserved_2_63                : 62;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_fec_abil_s      cn78xx;
+};
+typedef union cvmx_bgxx_spux_fec_abil cvmx_bgxx_spux_fec_abil_t;
+
+/**
+ * cvmx_bgx#_spu#_fec_control
+ */
+union cvmx_bgxx_spux_fec_control {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_fec_control_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_2_63                : 62;
+	uint64_t err_en                       : 1;  /**< BASE-R FEC error-indication enable. This bit corresponds to FEC_Enable_Error_to_PCS
+                                                         variable for BASE-R as defined in 802.3 Clause 74. When FEC is enabled (per FEC_EN bit in
+                                                         this register) and this bit is set, the FEC decoder on the receive side signals an
+                                                         uncorrectable FEC error to the BASE-R decoder by driving a value of 2'b11 on the sync bits
+                                                         for some of the 32 64B/66B blocks belonging to the uncorrectable FEC block. See
+                                                         802.3-2008/802.3ba-2010 section 74.7.4.5.1 for more details. */
+	uint64_t fec_en                       : 1;  /**< BASE-R FEC enable. When this bit is set and the LPCS type is BASE-R
+                                                         (BGX()_CMR()_CONFIG[LMAC_TYPE] = 0x4), forward error correction is enabled. FEC is
+                                                         disabled otherwise. Forward error correction is defined in IEEE Std
+                                                         802.3-2008/802.3ba-2010 Clause 74. */
+#else
+	uint64_t fec_en                       : 1;
+	uint64_t err_en                       : 1;
+	uint64_t reserved_2_63                : 62;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_fec_control_s   cn78xx;
+};
+typedef union cvmx_bgxx_spux_fec_control cvmx_bgxx_spux_fec_control_t;
+
+/**
+ * cvmx_bgx#_spu#_fec_corr_blks01
+ *
+ * This register is valid only when the LPCS type is BASE-R
+ * (BGX()_CMR()_CONFIG[LMAC_TYPE] = 0x3 or 0x4). The FEC corrected-block counters are
+ * defined in Std 802.3 section 74.8.4.1. Each corrected-blocks counter increments by 1 for a
+ * corrected FEC block, i.e. an FEC block that has been received with invalid parity on the
+ * associated PCS lane and has been corrected by the FEC decoder. The counter is reset to all 0s
+ * when the register is read, and held at all 1s in case of overflow.
+ *
+ * The reset operation takes precedence over the increment operation; if the register is read on
+ * the same clock cycle as an increment operation, the counter is reset to all 0s and the
+ * increment operation is lost. The counters are writable for test purposes, rather than read-
+ * only as specified in Std 802.3.
+ */
+union cvmx_bgxx_spux_fec_corr_blks01 {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_fec_corr_blks01_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t ln1_corr_blks                : 32; /**< PCS Lane 1 FEC corrected blocks.
+                                                         * For 10GBASE-R, reserved.
+                                                         * For 40GBASE-R, correspond to the Std 802.3 FEC_corrected_blocks_counter_1 variable
+                                                         (registers 1.302-1.303). */
+	uint64_t ln0_corr_blks                : 32; /**< PCS Lane 0 FEC corrected blocks.
+                                                         * For 10GBASE-R, corresponds to the Std 802.3 FEC_corrected_blocks_counter variable
+                                                         (registers 1.172-1.173).
+                                                         * For 40GBASE-R, correspond to the Std 802.3 FEC_corrected_blocks_counter_0 variable
+                                                         (registers 1.300-1.301). */
+#else
+	uint64_t ln0_corr_blks                : 32;
+	uint64_t ln1_corr_blks                : 32;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_fec_corr_blks01_s cn78xx;
+};
+typedef union cvmx_bgxx_spux_fec_corr_blks01 cvmx_bgxx_spux_fec_corr_blks01_t;
+
+/**
+ * cvmx_bgx#_spu#_fec_corr_blks23
+ *
+ * This register is valid only when the LPCS type is 40GBASE-R
+ * (BGX()_CMR()_CONFIG[LMAC_TYPE] = 0x4). The FEC corrected-block counters are defined in
+ * Std 802.3 section 74.8.4.1. Each corrected-blocks counter increments by 1 for a corrected FEC
+ * block, i.e. an FEC block that has been received with invalid parity on the associated PCS lane
+ * and has been corrected by the FEC decoder. The counter is reset to all 0s when the register is
+ * read, and held at all 1s in case of overflow.
+ *
+ * The reset operation takes precedence over the increment operation; if the register is read on
+ * the same clock cycle as an increment operation, the counter is reset to all 0s and the
+ * increment operation is lost. The counters are writable for test purposes, rather than read-
+ * only as specified in Std 802.3.
+ */
+union cvmx_bgxx_spux_fec_corr_blks23 {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_fec_corr_blks23_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t ln3_corr_blks                : 32; /**< PCS Lane 3 FEC corrected blocks. Correspond to the Std 802.3
+                                                         FEC_corrected_blocks_counter_3 variable (registers 1.306-1.307). */
+	uint64_t ln2_corr_blks                : 32; /**< PCS Lane 2 FEC corrected blocks. Correspond to the Std 802.3
+                                                         FEC_corrected_blocks_counter_3 variable (registers 1.304-1.305). */
+#else
+	uint64_t ln2_corr_blks                : 32;
+	uint64_t ln3_corr_blks                : 32;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_fec_corr_blks23_s cn78xx;
+};
+typedef union cvmx_bgxx_spux_fec_corr_blks23 cvmx_bgxx_spux_fec_corr_blks23_t;
+
+/**
+ * cvmx_bgx#_spu#_fec_uncorr_blks01
+ *
+ * This register is valid only when the LPCS type is BASE-R
+ * (BGX()_CMR()_CONFIG[LMAC_TYPE] = 0x3 or 0x4). The FEC corrected-block counters are
+ * defined in Std 802.3 section 74.8.4.2. Each uncorrected-blocks counter increments by 1 for an
+ * uncorrected FEC block, i.e. an FEC block that has been received with invalid parity on the
+ * associated PCS lane and has not been corrected by the FEC decoder. The counter is reset to all
+ * 0s when the register is read, and held at all 1s in case of overflow.
+ *
+ * The reset operation takes precedence over the increment operation; if the register is read on
+ * the same clock cycle as an increment operation, the counter is reset to all 0s and the
+ * increment operation is lost. The counters are writable for test purposes, rather than read-
+ * only as specified in Std 802.3.
+ */
+union cvmx_bgxx_spux_fec_uncorr_blks01 {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_fec_uncorr_blks01_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t ln1_uncorr_blks              : 32; /**< PCS Lane 1 FEC corrected blocks.
+                                                         * For 10GBASE-R, reserved.
+                                                         * For 40GBASE-R, corresponds to the Std 802.3 FEC_uncorrected_blocks_counter_1 variable
+                                                         (registers 1.702-1.703). */
+	uint64_t ln0_uncorr_blks              : 32; /**< PCS Lane 0 FEC uncorrected blocks.
+                                                         * For 10GBASE-R, corresponds to the Std 802.3 FEC_uncorrected_blocks_counter variable
+                                                         (registers 1.174-1.175).
+                                                         * For 40GBASE-R, correspond to the Std 802.3 FEC_uncorrected_blocks_counter_0 variable
+                                                         (registers 1.700-1.701). */
+#else
+	uint64_t ln0_uncorr_blks              : 32;
+	uint64_t ln1_uncorr_blks              : 32;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_fec_uncorr_blks01_s cn78xx;
+};
+typedef union cvmx_bgxx_spux_fec_uncorr_blks01 cvmx_bgxx_spux_fec_uncorr_blks01_t;
+
+/**
+ * cvmx_bgx#_spu#_fec_uncorr_blks23
+ *
+ * This register is valid only when the LPCS type is 40GBASE-R
+ * (BGX()_CMR()_CONFIG[LMAC_TYPE] = 0x4). The FEC uncorrected-block counters are defined
+ * in Std 802.3 section 74.8.4.2. Each corrected-blocks counter increments by 1 for an
+ * uncorrected FEC block, i.e. an FEC block that has been received with invalid parity on the
+ * associated PCS lane and has not been corrected by the FEC decoder. The counter is reset to all
+ * 0s when the register is read, and held at all 1s in case of overflow.
+ *
+ * The reset operation takes precedence over the increment operation; if the register is read on
+ * the same clock cycle as an increment operation, the counter is reset to all 0s and the
+ * increment operation is lost. The counters are writable for test purposes, rather than read-
+ * only as specified in Std 802.3.
+ */
+union cvmx_bgxx_spux_fec_uncorr_blks23 {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_fec_uncorr_blks23_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t ln3_uncorr_blks              : 32; /**< PCS Lane 3 FEC uncorrected blocks. Corresponds to the Std 802.3
+                                                         FEC_uncorrected_blocks_counter_3 variable (registers 1.706-1.707). */
+	uint64_t ln2_uncorr_blks              : 32; /**< PCS Lane 2 FEC uncorrected blocks. Corresponds to the Std 802.3
+                                                         FEC_uncorrected_blocks_counter_3 variable (registers 1.704-1.705). */
+#else
+	uint64_t ln2_uncorr_blks              : 32;
+	uint64_t ln3_uncorr_blks              : 32;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_fec_uncorr_blks23_s cn78xx;
+};
+typedef union cvmx_bgxx_spux_fec_uncorr_blks23 cvmx_bgxx_spux_fec_uncorr_blks23_t;
+
+/**
+ * cvmx_bgx#_spu#_int
+ */
+union cvmx_bgxx_spux_int {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_int_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_15_63               : 49;
+	uint64_t training_failure             : 1;  /**< BASE-R PMD training failure. Set when BASE-R PMD link training has failed on the 10GBASE-R
+                                                         lane or any 40GBASE-R lane. Valid if the LPCS type selected by
+                                                         BGX()_CMR()_CONFIG[LMAC_TYPE] is 10GBASE-R or 40GBASE-R and
+                                                         BGX()_SPU()_BR_PMD_CONTROL[TRAIN_EN] is 1, and never set otherwise. */
+	uint64_t training_done                : 1;  /**< BASE-R PMD training done. Set when the 10GBASE-R lane or all 40GBASE-R lanes have
+                                                         successfully completed BASE-R PMD link training. Valid if the LPCS type selected by
+                                                         BGX()_CMR()_CONFIG[LMAC_TYPE] is 10GBASE-R or 40GBASE-R and
+                                                         BGX()_SPU()_BR_PMD_CONTROL[TRAIN_EN] is 1, and never set otherwise. */
+	uint64_t an_complete                  : 1;  /**< Auto-Negotiation complete. Set when BGX()_SPU()_AN_STATUS[AN_COMPLETE] is set,
+                                                         indicating that the Auto-Negotiation process has been completed and the link is up and
+                                                         running using the negotiated highest common denominator (HCD) technology. */
+	uint64_t an_link_good                 : 1;  /**< Auto-Negotiation link good. Set when the an_link_good variable is set as defined in
+                                                         802.3-2008 Figure 73-11, indicating that Auto-Negotiation has completed. */
+	uint64_t an_page_rx                   : 1;  /**< Auto-Negotiation page received. This bit is set along with
+                                                         BGX()_SPU()_AN_STATUS[PAGE_RX] when a new page has been received and stored in
+                                                         BGX()_SPU()_AN_LP_BASE or BGX()_SPU()_AN_LP_XNP. */
+	uint64_t fec_uncorr                   : 1;  /**< Uncorrectable FEC error. Set when an FEC block with an uncorrectable error is received on
+                                                         the 10GBASE-R lane or any 40GBASE-R lane. Valid if the LPCS type selected by
+                                                         BGX()_CMR()_CONFIG[LMAC_TYPE] is 10GBASE-R or 40GBASE-R, and never set otherwise. */
+	uint64_t fec_corr                     : 1;  /**< Correctable FEC error. Set when an FEC block with a correctable error is received on the
+                                                         10GBASE-R lane or any 40GBASE-R lane. Valid if the LPCS type selected by
+                                                         BGX()_CMR()_CONFIG[LMAC_TYPE] is 10GBASE-R or 40GBASE-R, and never set otherwise. */
+	uint64_t bip_err                      : 1;  /**< 40GBASE-R bit interleaved parity error. Set when a BIP error is detected on any lane.
+                                                         Valid if the LPCS type selected by BGX()_CMR()_CONFIG[LMAC_TYPE] is 40GBASE-R, and
+                                                         never set otherwise. */
+	uint64_t dbg_sync                     : 1;  /**< Sync failure debug. This interrupt is provided for link problem debugging help. It is set
+                                                         as follows based on the LPCS type selected by BGX()_CMR()_CONFIG[LMAC_TYPE], and
+                                                         whether FEC is enabled or disabled by BGX()_SPU()_FEC_CONTROL[FEC_EN]:
+                                                         * XAUI or RXAUI: Set when any lane's PCS synchronization state transitions from
+                                                         SYNC_ACQUIRED_1 to SYNC_ACQUIRED_2 (see 802.3-2008 Figure 48-7).
+                                                         * 10GBASE-R or 40GBASE-R with FEC disabled: Set when sh_invalid_cnt increments to 1 while
+                                                         block_lock is 1 (see 802.3-2008 Figure 49-12 and 802.3ba-2010 Figure 82-20).
+                                                         * 10GBASE-R or 40GBASE-R with FEC enabled: Set when parity_invalid_cnt increments to 1
+                                                         while fec_block_lock is 1 (see 802.3-2008 Figure 74-8). */
+	uint64_t algnlos                      : 1;  /**< Loss of lane alignment. Set when lane-to-lane alignment is lost. This is only valid if the
+                                                         logical PCS is a multilane type (i.e. XAUI, RXAUI or 40GBASE-R is selected by
+                                                         BGX()_CMR()_CONFIG[LMAC_TYPE]), and is never set otherwise. */
+	uint64_t synlos                       : 1;  /**< Loss of lane sync. Lane code-group or block synchronization is lost on one or more lanes
+                                                         associated with the LMAC/LPCS. Set as follows based on the LPCS type selected by
+                                                         BGX()_CMR()_CONFIG[LMAC_TYPE], and whether FEC is enabled or disabled by
+                                                         BGX()_SPU()_FEC_CONTROL[FEC_EN]:
+                                                         * XAUI or RXAUI: Set when any lane's PCS synchronization state transitions to LOSS_OF_SYNC
+                                                         (see 802.3-2008 Figure 48-7)
+                                                         * 10GBASE-R or 40GBASE-R with FEC disabled: set when the block_lock variable is cleared on
+                                                         the 10G lane or any 40G lane (see 802.3-2008 Figure 49-12 and 802.3ba-2010 Figure 82-20).
+                                                         * 10GBASE-R or 40GBASE-R with FEC enabled: set when the fec_block_lock variable is cleared
+                                                         on the 10G lane or any 40G lane (see 802.3-2008 Figure 74-8). */
+	uint64_t bitlckls                     : 1;  /**< Bit lock lost on one or more lanes associated with the LMAC/LPCS. */
+	uint64_t err_blk                      : 1;  /**< Errored block received. Set when an errored BASE-R block is received as described for
+                                                         BGX()_SPU()_BR_STATUS2[ERR_BLKS]. Valid if the LPCS type selected by
+                                                         BGX()_CMR()_CONFIG[LMAC_TYPE] is 10GBASE-R or 40GBASE-R, and never set otherwise. */
+	uint64_t rx_link_down                 : 1;  /**< Set when the receive link goes down, which is the same condition that sets
+                                                         BGX()_SPU()_STATUS2[RCVFLT]. */
+	uint64_t rx_link_up                   : 1;  /**< Set when the receive link comes up, which is the same condition that allows the setting of
+                                                         BGX()_SPU()_STATUS1[RCV_LNK]. */
+#else
+	uint64_t rx_link_up                   : 1;
+	uint64_t rx_link_down                 : 1;
+	uint64_t err_blk                      : 1;
+	uint64_t bitlckls                     : 1;
+	uint64_t synlos                       : 1;
+	uint64_t algnlos                      : 1;
+	uint64_t dbg_sync                     : 1;
+	uint64_t bip_err                      : 1;
+	uint64_t fec_corr                     : 1;
+	uint64_t fec_uncorr                   : 1;
+	uint64_t an_page_rx                   : 1;
+	uint64_t an_link_good                 : 1;
+	uint64_t an_complete                  : 1;
+	uint64_t training_done                : 1;
+	uint64_t training_failure             : 1;
+	uint64_t reserved_15_63               : 49;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_int_s           cn78xx;
+};
+typedef union cvmx_bgxx_spux_int cvmx_bgxx_spux_int_t;
+
+/**
+ * cvmx_bgx#_spu#_lpcs_states
+ */
+union cvmx_bgxx_spux_lpcs_states {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_lpcs_states_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_15_63               : 49;
+	uint64_t br_rx_sm                     : 3;  /**< BASE-R receive state machine state */
+	uint64_t reserved_10_11               : 2;
+	uint64_t bx_rx_sm                     : 2;  /**< BASE-X receive state machine state */
+	uint64_t deskew_am_found              : 4;  /**< 40GBASE-R deskew state machine alignment marker found flag per logical PCS lane ID. */
+	uint64_t reserved_3_3                 : 1;
+	uint64_t deskew_sm                    : 3;  /**< BASE-X and 40GBASE-R deskew state machine state */
+#else
+	uint64_t deskew_sm                    : 3;
+	uint64_t reserved_3_3                 : 1;
+	uint64_t deskew_am_found              : 4;
+	uint64_t bx_rx_sm                     : 2;
+	uint64_t reserved_10_11               : 2;
+	uint64_t br_rx_sm                     : 3;
+	uint64_t reserved_15_63               : 49;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_lpcs_states_s   cn78xx;
+};
+typedef union cvmx_bgxx_spux_lpcs_states cvmx_bgxx_spux_lpcs_states_t;
+
+/**
+ * cvmx_bgx#_spu#_misc_control
+ *
+ * "* RX logical PCS lane polarity vector [3:0] = XOR_RXPLRT[3:0] ^ [4[RXPLRT]].
+ *  * TX logical PCS lane polarity vector [3:0] = XOR_TXPLRT[3:0] ^ [4[TXPLRT]].
+ *
+ *  In short, keep RXPLRT and TXPLRT cleared, and use XOR_RXPLRT and XOR_TXPLRT fields to define
+ *  the polarity per logical PCS lane. Only bit 0 of vector is used for 10GBASE-R, and only bits
+ * - 1:0 of vector are used for RXAUI."
+ */
+union cvmx_bgxx_spux_misc_control {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_misc_control_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_13_63               : 51;
+	uint64_t rx_packet_dis                : 1;  /**< Receive packet disable. Software can set or clear this bit at any time to gracefully
+                                                         disable or re-enable packet reception by the LPCS. If this bit is set while a packet is
+                                                         being received, the packet is completed and all subsequent received packets are discarded
+                                                         by the LPCS. Similarly, if this bit is cleared while a received packet is being discarded,
+                                                         packet reception resumes after the current packet is fully discarded. When set for a
+                                                         40GBASE-R or 10GBASE-R LMAC/LPCS type (selected by BGX()_CMR()_CONFIG[LMAC_TYPE]),
+                                                         received errors and faults will be ignored while receive packets are discarded; idles will
+                                                         be sent to the MAC layer (SMU) and the errored blocks counter
+                                                         (BGX()_SPU()_BR_STATUS2[ERR_BLKS]) will not increment. */
+	uint64_t skip_after_term              : 1;  /**< Enable sending of Idle Skip after Terminate. This bit is meaningful when the logical PCS
+                                                         type is XAUI or RXAUI (selected by BGX()_CMR()_CONFIG[LMAC_TYPE]), and has no
+                                                         effect otherwise. When set, the LMAC/LPCS transmits more Idle Skip columns for clock
+                                                         compensation. Typically set in HiGig/HiGig2 modes; clear otherwise. This field can be set
+                                                         to ensure sufficient density of XAUI Idle Skip (||R||) columns with a small transmit
+                                                         inter-frame gap (IFG) in order to allow the link partner's receiver to delete ||R
+                                                         columns as needed for clock rate compensation. It is usually set when the LMAC's transmit
+                                                         IFG is set to 8 bytes in HiGig/HiGig2 modes (i.e. BGX()_SMU()_TX_IFG[IFG1] +
+                                                         BGX()_SMU()_TX_IFG[IFG2] = 8), and should be cleared when the transmit IFG is
+                                                         greater than 8 bytes. When this bit is set, the SPU will send an ||R|| column after a
+                                                         ||T0|| column (terminate in lane 0) if no ||R|| was sent in the previous IFG. This is a
+                                                         minor deviation from the functionality specified in 802.3-2008 Figure 48-6 (PCS transmit
+                                                         source state diagram), whereby the state will transition directly from SEND_DATA to
+                                                         SEND_RANDOM_R after ||T0|| if no ||R|| was transmitted in the previous IFG. Sending ||R
+                                                         after ||T0|| only (and not ||T1||, |T2|| or ||T3||) ensures that the check_end function at
+                                                         the receiving end, as defined in 802.3-2008 sub-clause 48.2.6.1.4, does not detect an
+                                                         error due to this functional change. When this bit is clear, the LMAC will fully conform
+                                                         to the functionality specified in Figure 48-6. */
+	uint64_t intlv_rdisp                  : 1;  /**< RXAUI interleaved running disparity. This bit is meaningful when the logical PCS type is
+                                                         RXAUI (BGX()_CMR()_CONFIG[LMAC_TYPE] = RXAUI), and has no effect otherwise. It
+                                                         selects which disparity calculation to use when combining or splitting the RXAUI lanes, as
+                                                         follows:
+                                                         _ 0 = Common running disparity. Common running disparity is computed for even and odd
+                                                         code-
+                                                         groups of an RXAUI lane, i.e. interleave lanes before PCS layer as described in the Dune
+                                                         Networks/Broadcom RXAUI v2.1 specification. This obeys 6.25GHz serdes disparity.
+                                                         _ 1 = Interleaved running disparity: Running disparity is computed separately for even and
+                                                         odd code-groups of an RXAUI lane, i.e. interleave lanes after PCS layer as described in
+                                                         the Marvell RXAUI Interface specification. This does not obey 6.25GHz SerDes disparity. */
+	uint64_t xor_rxplrt                   : 4;  /**< RX polarity control per logical PCS lane */
+	uint64_t xor_txplrt                   : 4;  /**< TX polarity control per logical PCS lane */
+	uint64_t rxplrt                       : 1;  /**< Receive polarity. 1 = inverted polarity, 0 = normal polarity. */
+	uint64_t txplrt                       : 1;  /**< Transmit polarity. 1 = inverted polarity, 0 = normal polarity. */
+#else
+	uint64_t txplrt                       : 1;
+	uint64_t rxplrt                       : 1;
+	uint64_t xor_txplrt                   : 4;
+	uint64_t xor_rxplrt                   : 4;
+	uint64_t intlv_rdisp                  : 1;
+	uint64_t skip_after_term              : 1;
+	uint64_t rx_packet_dis                : 1;
+	uint64_t reserved_13_63               : 51;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_misc_control_s  cn78xx;
+};
+typedef union cvmx_bgxx_spux_misc_control cvmx_bgxx_spux_misc_control_t;
+
+/**
+ * cvmx_bgx#_spu#_spd_abil
+ */
+union cvmx_bgxx_spux_spd_abil {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_spd_abil_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t hundredgb                    : 1;  /**< 100G capable. Always 0. */
+	uint64_t fortygb                      : 1;  /**< 40G capable. Always 1. */
+	uint64_t tenpasst                     : 1;  /**< 10PASS-TS/2BASE-TL capable. Always 0. */
+	uint64_t tengb                        : 1;  /**< 10G capable. Always 1. */
+#else
+	uint64_t tengb                        : 1;
+	uint64_t tenpasst                     : 1;
+	uint64_t fortygb                      : 1;
+	uint64_t hundredgb                    : 1;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_spd_abil_s      cn78xx;
+};
+typedef union cvmx_bgxx_spux_spd_abil cvmx_bgxx_spux_spd_abil_t;
+
+/**
+ * cvmx_bgx#_spu#_status1
+ */
+union cvmx_bgxx_spux_status1 {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_status1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_8_63                : 56;
+	uint64_t flt                          : 1;  /**< Fault condition detected.
+                                                         This bit is a logical OR of BGX()_SPU()_STATUS2[XMTFLT, RCVFLT]. */
+	uint64_t reserved_3_6                 : 4;
+	uint64_t rcv_lnk                      : 1;  /**< PCS receive link status:
+                                                           0 = receive link down.
+                                                           1 = receive link up.
+                                                         This is a latching-low bit; it stays clear until the register is read by software.
+                                                         For a BASE-X logical PCS type (in the associated BGX()_CMR()_CONFIG[LMAC_TYPE] =
+                                                         XAUI or RXAUI), this is a latching-low version of BGX()_SPU()_BX_STATUS[ALIGND].
+                                                         For a BASE-R logical PCS type (in the associated BGX()_CMR()_CONFIG[LMAC_TYPE] =
+                                                         10G_R or 40G_R), this is a latching-low version of
+                                                         BGX()_SPU()_BR_STATUS1[RCV_LNK].
+                                                         Note that in order to avoid read side effects, this is implemented as a write-1-to-set
+                                                         bit, rather than latching low read-only as specified in 802.3. */
+	uint64_t lpable                       : 1;  /**< Low-power ability. Always returns 1 to indicate that the LPCS supports low-power mode. */
+	uint64_t reserved_0_0                 : 1;
+#else
+	uint64_t reserved_0_0                 : 1;
+	uint64_t lpable                       : 1;
+	uint64_t rcv_lnk                      : 1;
+	uint64_t reserved_3_6                 : 4;
+	uint64_t flt                          : 1;
+	uint64_t reserved_8_63                : 56;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_status1_s       cn78xx;
+};
+typedef union cvmx_bgxx_spux_status1 cvmx_bgxx_spux_status1_t;
+
+/**
+ * cvmx_bgx#_spu#_status2
+ */
+union cvmx_bgxx_spux_status2 {
+	uint64_t u64;
+	struct cvmx_bgxx_spux_status2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t dev                          : 2;  /**< Device present. Always returns 0x2 to indicate a device is present at this address. */
+	uint64_t reserved_12_13               : 2;
+	uint64_t xmtflt                       : 1;  /**< Transmit fault. Always returns 0. */
+	uint64_t rcvflt                       : 1;  /**< Receive fault: 1 = receive fault, 0 = no receive fault. Latching high bit; stays set until
+                                                         software writes a 1.
+                                                         Note that in order to avoid read side effects, this is implemented as a write-1-to-clear
+                                                         bit, rather than latching high read-only as specified in 802.3. */
+	uint64_t reserved_6_9                 : 4;
+	uint64_t hundredgb_r                  : 1;  /**< 100GBASE-R capable. Always 0. */
+	uint64_t fortygb_r                    : 1;  /**< 40GBASE-R capable. Always 1. */
+	uint64_t tengb_t                      : 1;  /**< 10GBASE-T capable. Always 0. */
+	uint64_t tengb_w                      : 1;  /**< 10GBASE-W capable. Always 0. */
+	uint64_t tengb_x                      : 1;  /**< 10GBASE-X capable. Always 1. */
+	uint64_t tengb_r                      : 1;  /**< 10GBASE-R capable. Always 1. */
+#else
+	uint64_t tengb_r                      : 1;
+	uint64_t tengb_x                      : 1;
+	uint64_t tengb_w                      : 1;
+	uint64_t tengb_t                      : 1;
+	uint64_t fortygb_r                    : 1;
+	uint64_t hundredgb_r                  : 1;
+	uint64_t reserved_6_9                 : 4;
+	uint64_t rcvflt                       : 1;
+	uint64_t xmtflt                       : 1;
+	uint64_t reserved_12_13               : 2;
+	uint64_t dev                          : 2;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_bgxx_spux_status2_s       cn78xx;
+};
+typedef union cvmx_bgxx_spux_status2 cvmx_bgxx_spux_status2_t;
+
+/**
+ * cvmx_bgx#_spu_bist_status
+ *
+ * This register provides memory BIST status from the SPU RX_BUF lane FIFOs.
+ *
+ */
+union cvmx_bgxx_spu_bist_status {
+	uint64_t u64;
+	struct cvmx_bgxx_spu_bist_status_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t rx_buf_bist_status           : 4;  /**< SPU RX_BUF BIST status for lanes 3-0. One bit per SerDes lane, set to indicate BIST
+                                                         failure for the associated RX_BUF lane FIFO. */
+#else
+	uint64_t rx_buf_bist_status           : 4;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_bgxx_spu_bist_status_s    cn78xx;
+};
+typedef union cvmx_bgxx_spu_bist_status cvmx_bgxx_spu_bist_status_t;
+
+/**
+ * cvmx_bgx#_spu_dbg_control
+ */
+union cvmx_bgxx_spu_dbg_control {
+	uint64_t u64;
+	struct cvmx_bgxx_spu_dbg_control_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_56_63               : 8;
+	uint64_t ms_clk_period                : 12; /**< Millisecond clock period. Specifies the number of microsecond clock ticks per millisecond,
+                                                         minus 1. The default value of 999 (0x3E7) should be used during normal operation; other
+                                                         values may be used for test/debug purposes. */
+	uint64_t us_clk_period                : 12; /**< Microsecond clock period. Specifies the number of SCLK cycles per microseconds, minus 1.
+                                                         For example, if SCLK runs at 1.3 GHz, the number of SCLK cycles per microsecond is 1,300
+                                                         so the value of this field should be 1,299 (0x513). This is used by the BASE-R BER monitor
+                                                         timers. */
+	uint64_t reserved_31_31               : 1;
+	uint64_t br_ber_mon_dis               : 1;  /**< BASE-R bit error rate monitor disable. This bit should be clear for normal operation.
+                                                         Setting it disables the BASE-R BER monitor state machine defined in 802.3-2008 Figure
+                                                         49-13 for 10GBASE-R and 802.3ba-2010 Figure 82-13 for 40GBASE-R. */
+	uint64_t an_nonce_match_dis           : 1;  /**< Auto-Negotiation nonce match disable. This bit should be clear for normal operation.
+                                                         Setting it disables Nonce Match check by forcing nonce_match variable to 0 in the Auto-
+                                                         Negotiation arbitration state diagram, as defined in 802.3-2008 Figure 73-11. This bit can
+                                                         be set by software for test purposes, e.g. for running auto-negotiation in loopback mode. */
+	uint64_t timestamp_norm_dis           : 1;  /**< 40GBASE-R RX timestamp normalization disable. This bit controls the generation of the
+                                                         receive SOP timestamp passed to the SMU sub-block for a 40GBASE-R LMAC/LPCS. When this bit
+                                                         is clear, SPU normalizes the receive SOP timestamp in order to compensate for lane-to-lane
+                                                         skew on a 40GBASE-R link, as described below. When this bit is set, timestamp
+                                                         normalization is disabled and SPU directly passes the captured SOP timestamp values to
+                                                         SMU.
+                                                         In 40GBASE-R mode, a packet's SOP block can be transferred on any of the LMAC's lanes. In
+                                                         the presence of lane-to-lane skew, the SOP delay from transmit (by the link partner) to
+                                                         receive by SPU varies depending on which lane is used by the SOP block. This variation
+                                                         reduces the accuracy of the received SOP timestamp relative to when it was transmitted by
+                                                         the link partner.
+                                                         SPU captures the timestamp of the alignment marker received on each SerDes lane during
+                                                         align/skew detection; the captured value can be read from the SerDes lane's
+                                                         BGX()_SPU_SDS()_SKEW_STATUS[SKEW_STATUS] field (AM_TIMESTAMP sub-field). If
+                                                         alignment markers are transmitted at about the same time on all lanes by the link partner,
+                                                         then the difference between the AM_TIMESTAMP values for a pair of lanes represents the
+                                                         approximate skew between those lanes.
+                                                         SPU uses the 40GBASE-R LMAC's programmed PCS lane 0 as a reference and computes the
+                                                         AM_TIMESTAMP delta of every other lane relative to PCS lane 0. When normalization is
+                                                         enabled, SPU adjusts the timestamp of a received SOP by subtracting the receiving lane's
+                                                         AM_TIMESTAMP delta from the captured timestamp value. The adjusted/normalized timestamp
+                                                         value is then passed to SMU along with the SOP.
+                                                         Software can determine the actual maximum skew of a 40GBASE-R link by examining the
+                                                         AM_TIMESTAMP values in the BGX()_SPU_SDS()_SKEW_STATUS registers, and decide if
+                                                         timestamp normalization should be enabled or disabled to improve PTP accuracy.
+                                                         Normalization improves accuracy for larger skew values but reduces the accuracy (due to
+                                                         timestamp measurement errors) for small skew values. */
+	uint64_t rx_buf_flip_synd             : 8;  /**< Flip SPU RX_BUF FIFO ECC bits. Two bits per SerDes lane; used to inject single-bit and
+                                                         double-bit errors into the ECC field on writes to the associated SPU RX_BUF lane FIFO, as
+                                                         follows:
+                                                         0x0 = Normal operation.
+                                                         0x1 = SBE on ECC bit 0.
+                                                         0x2 = SBE on ECC bit 1.
+                                                         0x3 = DBE on ECC bits 1:0. */
+	uint64_t br_pmd_train_soft_en         : 1;  /**< Enable BASE-R PMD software controlled link training. This bit configures the operation
+                                                         mode for BASE-R link training for all LMACs and lanes. When this bit is set along with
+                                                         BGX()_SPU()_BR_PMD_CONTROL[TRAIN_EN] for a given LMAC, the BASE-R link training
+                                                         protocol for that LMAC is executed under software control, whereby the contents the
+                                                         BGX()_SPU()_BR_PMD_LD_CUP and BGX()_SPU()_BR_PMD_LD_REP registers are
+                                                         updated by software. When this bit is clear and
+                                                         BGX()_SPU()_BR_PMD_CONTROL[TRAIN_EN] is set, the link training protocol is fully
+                                                         automated in hardware, whereby the contents BGX()_SPU()_BR_PMD_LD_CUP and
+                                                         BGX()_SPU()_BR_PMD_LD_REP registers are automatically updated by hardware. */
+	uint64_t an_arb_link_chk_en           : 1;  /**< Enable link status checking by Auto-Negotiation arbitration state machine. When Auto-
+                                                         Negotiation is enabled (BGX()_SPU()_AN_CONTROL[AN_EN] is set), this bit controls
+                                                         the behavior of the Auto-Negotiation arbitration state machine when it reaches the AN GOOD
+                                                         CHECK state after DME pages are successfully exchanged, as defined in Figure 73-11 in
+                                                         802.3-2008.
+                                                         When this bit is set and the negotiated highest common denominator (HCD) technology
+                                                         matches BGX()_CMR()_CONFIG[LMAC_TYPE], the Auto-Negotiation arbitration SM
+                                                         performs the actions defined for the AN GOOD CHECK state in Figure 73-11, i.e. run the
+                                                         link_fail_inhibit timer and eventually transition to the AN GOOD or TRANSMIT DISABLE
+                                                         state.
+                                                         When this bit is clear or the HCD technology does not match LMAC_TYPE, the AN arbitration
+                                                         SM stays in the AN GOOD CHECK state, with the expectation that software will perform the
+                                                         appropriate actions to complete the Auto-Negotiation protocol, as follows:
+                                                         * If this bit is clear and the HCD technology matches LMAC_TYPE, clear AN_EN in
+                                                         AN_CONTROL.
+                                                         * Otherwise, disable the LPCS by clearing the BGX()_CMR()_CONFIG[ENABLE], clear
+                                                         BGX()_SPU()_AN_CONTROL[AN_EN], reconfigure the LPCS with the correct LMAC_TYPE,
+                                                         and re-enable the LPCS by setting BGX()_CMR()_CONFIG[ENABLE].
+                                                         In both cases, software should implement the link_fail_inhibit timer and verify the link
+                                                         status as specified for the AN GOOD CHECK state. */
+	uint64_t rx_buf_cor_dis               : 1;  /**< When set, disables ECC correction on all SPU RX_BUF FIFOs. */
+	uint64_t scramble_dis                 : 1;  /**< BASE-R scrambler/descrambler disable. Setting this bit to 1 disables the BASE-R scrambler
+                                                         & descrambler functions and FEC PN-2112 scrambler & descrambler functions for debug
+                                                         purposes. */
+	uint64_t reserved_15_15               : 1;
+	uint64_t marker_rxp                   : 15; /**< BASE-R alignment marker receive period. For a multilane BASE-R logical PCS (i.e.
+                                                         40GBASE-R), this field specifies the expected alignment marker receive period per lane,
+                                                         i.e. the expected number of received 66b non-marker blocks between consecutive markers on
+                                                         the same lane. The default value corresponds to a period of 16363 blocks (exclusive) as
+                                                         specified in 802.3ba-2010. Must be greater than 64. */
+#else
+	uint64_t marker_rxp                   : 15;
+	uint64_t reserved_15_15               : 1;
+	uint64_t scramble_dis                 : 1;
+	uint64_t rx_buf_cor_dis               : 1;
+	uint64_t an_arb_link_chk_en           : 1;
+	uint64_t br_pmd_train_soft_en         : 1;
+	uint64_t rx_buf_flip_synd             : 8;
+	uint64_t timestamp_norm_dis           : 1;
+	uint64_t an_nonce_match_dis           : 1;
+	uint64_t br_ber_mon_dis               : 1;
+	uint64_t reserved_31_31               : 1;
+	uint64_t us_clk_period                : 12;
+	uint64_t ms_clk_period                : 12;
+	uint64_t reserved_56_63               : 8;
+#endif
+	} s;
+	struct cvmx_bgxx_spu_dbg_control_s    cn78xx;
+};
+typedef union cvmx_bgxx_spu_dbg_control cvmx_bgxx_spu_dbg_control_t;
+
+/**
+ * cvmx_bgx#_spu_mem_int
+ */
+union cvmx_bgxx_spu_mem_int {
+	uint64_t u64;
+	struct cvmx_bgxx_spu_mem_int_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_8_63                : 56;
+	uint64_t rx_buf_sbe                   : 4;  /**< SPU RX_BUF single-bit error for lanes 3-0. One bit per physical SerDes lane. Each bit is
+                                                         set when the associated RX_BUF lane FIFO detects a single-bit ECC error. */
+	uint64_t rx_buf_dbe                   : 4;  /**< SPU RX_BUF double-bit error for lanes 3-0. One bit per physical SerDes lane. Each bit is
+                                                         set when the associated RX_BUF lane FIFO detects a double-bit ECC error. */
+#else
+	uint64_t rx_buf_dbe                   : 4;
+	uint64_t rx_buf_sbe                   : 4;
+	uint64_t reserved_8_63                : 56;
+#endif
+	} s;
+	struct cvmx_bgxx_spu_mem_int_s        cn78xx;
+};
+typedef union cvmx_bgxx_spu_mem_int cvmx_bgxx_spu_mem_int_t;
+
+/**
+ * cvmx_bgx#_spu_mem_status
+ *
+ * This register provides memory ECC status from the SPU RX_BUF lane FIFOs.
+ *
+ */
+union cvmx_bgxx_spu_mem_status {
+	uint64_t u64;
+	struct cvmx_bgxx_spu_mem_status_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_32_63               : 32;
+	uint64_t rx_buf_ecc_synd              : 32; /**< SPU RX_BUF ECC syndromes for lanes 3-0. 8-bit syndrome sub-field per SerDes lane. Each
+                                                         8-bit sub-field contains the syndrome of the latest single-bit or double-bit ECC error
+                                                         detected by the associated RX_BUF lane FIFO, i.e. it is loaded when the corresponding
+                                                         RX_BUF_SBE or RX_BUF_DBE bit is set in the SPU MEM_INT register. */
+#else
+	uint64_t rx_buf_ecc_synd              : 32;
+	uint64_t reserved_32_63               : 32;
+#endif
+	} s;
+	struct cvmx_bgxx_spu_mem_status_s     cn78xx;
+};
+typedef union cvmx_bgxx_spu_mem_status cvmx_bgxx_spu_mem_status_t;
+
+/**
+ * cvmx_bgx#_spu_sds#_skew_status
+ *
+ * This register provides SerDes lane skew status. One register per physical SerDes lane.
+ *
+ */
+union cvmx_bgxx_spu_sdsx_skew_status {
+	uint64_t u64;
+	struct cvmx_bgxx_spu_sdsx_skew_status_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_32_63               : 32;
+	uint64_t skew_status                  : 32; /**< Format defined by BGX_SPU_SDS_SKEW_STATUS_S. */
+#else
+	uint64_t skew_status                  : 32;
+	uint64_t reserved_32_63               : 32;
+#endif
+	} s;
+	struct cvmx_bgxx_spu_sdsx_skew_status_s cn78xx;
+};
+typedef union cvmx_bgxx_spu_sdsx_skew_status cvmx_bgxx_spu_sdsx_skew_status_t;
+
+/**
+ * cvmx_bgx#_spu_sds#_states
+ *
+ * This register provides SerDes lane states. One register per physical SerDes lane.
+ *
+ */
+union cvmx_bgxx_spu_sdsx_states {
+	uint64_t u64;
+	struct cvmx_bgxx_spu_sdsx_states_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_52_63               : 12;
+	uint64_t am_lock_invld_cnt            : 2;  /**< 40GBASE-R alignment marker lock state machine invalid AM counter. */
+	uint64_t am_lock_sm                   : 2;  /**< 40GBASE-R alignment marker lock state machine state. */
+	uint64_t reserved_45_47               : 3;
+	uint64_t train_sm                     : 3;  /**< Link training state machine state. */
+	uint64_t train_code_viol              : 1;  /**< Link training code violation in received control channel. */
+	uint64_t train_frame_lock             : 1;  /**< Link training frame lock status. */
+	uint64_t train_lock_found_1st_marker  : 1;  /**< Link training lock state machine found first marker flag. */
+	uint64_t train_lock_bad_markers       : 3;  /**< Link training lock state machine bad markers counter. */
+	uint64_t reserved_35_35               : 1;
+	uint64_t an_arb_sm                    : 3;  /**< Auto-Negotiation arbitration state machine state. */
+	uint64_t an_rx_sm                     : 2;  /**< Auto-Negotiation receive state machine state. */
+	uint64_t reserved_29_29               : 1;
+	uint64_t fec_block_sync               : 1;  /**< FEC block sync status. */
+	uint64_t fec_sync_cnt                 : 4;  /**< FEC block sync state machine good/bad parity block counter. */
+	uint64_t reserved_23_23               : 1;
+	uint64_t br_sh_invld_cnt              : 7;  /**< BASE-R lock state machine invalid sync header counter. */
+	uint64_t br_block_lock                : 1;  /**< BASE-R block lock status. */
+	uint64_t br_sh_cnt                    : 11; /**< BASE-R lock state machine sync header counter */
+	uint64_t bx_sync_sm                   : 4;  /**< BASE-X PCS synchronization state machine state */
+#else
+	uint64_t bx_sync_sm                   : 4;
+	uint64_t br_sh_cnt                    : 11;
+	uint64_t br_block_lock                : 1;
+	uint64_t br_sh_invld_cnt              : 7;
+	uint64_t reserved_23_23               : 1;
+	uint64_t fec_sync_cnt                 : 4;
+	uint64_t fec_block_sync               : 1;
+	uint64_t reserved_29_29               : 1;
+	uint64_t an_rx_sm                     : 2;
+	uint64_t an_arb_sm                    : 3;
+	uint64_t reserved_35_35               : 1;
+	uint64_t train_lock_bad_markers       : 3;
+	uint64_t train_lock_found_1st_marker  : 1;
+	uint64_t train_frame_lock             : 1;
+	uint64_t train_code_viol              : 1;
+	uint64_t train_sm                     : 3;
+	uint64_t reserved_45_47               : 3;
+	uint64_t am_lock_sm                   : 2;
+	uint64_t am_lock_invld_cnt            : 2;
+	uint64_t reserved_52_63               : 12;
+#endif
+	} s;
+	struct cvmx_bgxx_spu_sdsx_states_s    cn78xx;
+};
+typedef union cvmx_bgxx_spu_sdsx_states cvmx_bgxx_spu_sdsx_states_t;
+
+#endif
diff --git a/arch/mips/include/asm/octeon/cvmx-bootmem.h b/arch/mips/include/asm/octeon/cvmx-bootmem.h
index 540c84f..cc14176 100644
--- a/arch/mips/include/asm/octeon/cvmx-bootmem.h
+++ b/arch/mips/include/asm/octeon/cvmx-bootmem.h
@@ -526,5 +526,32 @@ void *__cvmx_bootmem_internal_get_desc_ptr(void);
  * returned.
  */
 void *__cvmx_phys_addr_to_ptr(uint64_t phys, int size);
+const cvmx_bootmem_named_block_desc_t *
+__cvmx_bootmem_find_named_block_flags(char *name, uint32_t flags);
+
+/**
+ * Allocate if needed a block of memory from a specific range of the
+ * free list that was passed to the application by the bootloader, and
+ * assign it a name in the global named block table.  (part of the
+ * cvmx_bootmem_descriptor_t structure) Named blocks can later be
+ * freed.  If the requested name block is already allocated, return
+ * the pointer to block of memory.  If request cannot be satisfied
+ * within the address range specified, NULL is returned
+ *
+ * @param size   Size in bytes of block to allocate
+ * @param min_addr  minimum address of range
+ * @param max_addr  maximum address of range
+ * @param align  Alignment of memory to be allocated. (must be a power of 2)
+ * @param name   name of block - must be less than CVMX_BOOTMEM_NAME_LEN bytes
+ * @param init   Initialization function
+ *
+ * @return pointer to block of memory, NULL on error
+ */
+void *cvmx_bootmem_alloc_named_range_once(uint64_t size,
+                                          uint64_t min_addr,
+                                          uint64_t max_addr,
+                                          uint64_t align,
+                                          const char *name,
+                                          void (*init) (void *));
 
 #endif /*   __CVMX_BOOTMEM_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-ciu-defs.h b/arch/mips/include/asm/octeon/cvmx-ciu-defs.h
index 6d72539..845153f 100644
--- a/arch/mips/include/asm/octeon/cvmx-ciu-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-ciu-defs.h
@@ -8435,6 +8435,7 @@ union cvmx_ciu_qlm_jtgd {
 	struct cvmx_ciu_qlm_jtgd_s cn68xxp1;
 	struct cvmx_ciu_qlm_jtgd_cn61xx cnf71xx;
 };
+typedef union cvmx_ciu_qlm_jtgd cvmx_ciu_qlm_jtgd_t;
 
 union cvmx_ciu_soft_bist {
 	uint64_t u64;
diff --git a/arch/mips/include/asm/octeon/cvmx-clock.h b/arch/mips/include/asm/octeon/cvmx-clock.h
new file mode 100644
index 0000000..96cd088
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-clock.h
@@ -0,0 +1,162 @@
+/***********************license start***************
+ * Copyright (c) 2003-2010  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Interface to Core, IO and DDR Clock.
+ *
+ */
+
+#ifndef __CVMX_CLOCK_H__
+#define __CVMX_CLOCK_H__
+
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include <asm/octeon/octeon.h>
+#include <asm/octeon/cvmx-lmcx-defs.h>
+#include <asm/octeon/cvmx-fpa-defs.h>
+#include <asm/octeon/cvmx-tim-defs.h>
+#else
+#include "cvmx.h"
+#endif
+
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+extern "C" {
+/* *INDENT-ON* */
+#endif
+
+/**
+ * Enumeration of different Clocks in Octeon.
+ */
+typedef enum {
+	CVMX_CLOCK_RCLK,
+			    /**< Clock used by cores, coherent bus and L2 cache. */
+	CVMX_CLOCK_SCLK,
+			    /**< Clock used by IO blocks. */
+	CVMX_CLOCK_DDR,
+			    /**< Clock used by DRAM */
+	CVMX_CLOCK_CORE,
+			    /**< Alias for CVMX_CLOCK_RCLK */
+	CVMX_CLOCK_TIM,
+			    /**< Alias for CVMX_CLOCK_SCLK */
+	CVMX_CLOCK_IPD,
+			    /**< Alias for CVMX_CLOCK_SCLK */
+} cvmx_clock_t;
+
+/**
+ * Get cycle count based on the clock type.
+ *
+ * @param clock - Enumeration of the clock type.
+ * @return      - Get the number of cycles executed so far.
+ */
+static inline uint64_t cvmx_clock_get_count(cvmx_clock_t clock)
+{
+	switch (clock) {
+	case CVMX_CLOCK_RCLK:
+	case CVMX_CLOCK_CORE:
+		{
+#ifndef __mips__
+			return cvmx_read_csr(CVMX_IPD_CLK_COUNT);
+#elif defined(CVMX_ABI_O32)
+			uint32_t tmp_low, tmp_hi;
+
+			asm volatile ("   .set push                    \n"
+				      "   .set mips64r2                \n"
+				      "   .set noreorder               \n"
+				      "   rdhwr %[tmpl], $31           \n"
+				      "   dsrl  %[tmph], %[tmpl], 32   \n"
+				      "   sll   %[tmpl], 0             \n"
+				      "   sll   %[tmph], 0             \n" "   .set pop                 \n":[tmpl] "=&r"(tmp_low),[tmph] "=&r"(tmp_hi):);
+
+			return (((uint64_t) tmp_hi << 32) + tmp_low);
+#else
+			uint64_t cycle;
+			CVMX_RDHWR(cycle, 31);
+			return (cycle);
+#endif
+		}
+	case CVMX_CLOCK_SCLK:
+	case CVMX_CLOCK_IPD:
+		if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+			return cvmx_read_csr(CVMX_FPA_CLK_COUNT);
+		return cvmx_read_csr(CVMX_IPD_CLK_COUNT);
+	case CVMX_CLOCK_TIM:
+		if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+			return cvmx_read_csr(CVMX_TIM_FR_RN_CYCLES);
+		return cvmx_read_csr(CVMX_IPD_CLK_COUNT);
+
+	case CVMX_CLOCK_DDR:
+		if (OCTEON_IS_OCTEON2() || OCTEON_IS_OCTEON3())
+			return cvmx_read_csr(CVMX_LMCX_DCLK_CNT(0));
+		else
+			return ((cvmx_read_csr(CVMX_LMCX_DCLK_CNT_HI(0)) << 32) | cvmx_read_csr(CVMX_LMCX_DCLK_CNT_LO(0)));
+	}
+
+	cvmx_dprintf("cvmx_clock_get_count: Unknown clock type\n");
+	return 0;
+}
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+static inline uint64_t cvmx_clock_get_rate(cvmx_clock_t clock)
+{
+	switch (clock) {
+	case CVMX_CLOCK_RCLK:
+	case CVMX_CLOCK_CORE:
+		return octeon_get_clock_rate();
+	case CVMX_CLOCK_SCLK:
+	case CVMX_CLOCK_TIM:
+	case CVMX_CLOCK_IPD:
+		return octeon_get_io_clock_rate();
+	default:
+		return 0;
+	}
+}
+#define cvmx_clock_get_rate_node(a,b) cvmx_clock_get_rate(b)
+#else
+extern uint64_t cvmx_clock_get_rate(cvmx_clock_t clock);
+extern uint64_t cvmx_clock_get_rate_node(int node, cvmx_clock_t clock);
+#endif
+
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+}
+/* *INDENT-ON* */
+#endif
+
+#endif /* __CVMX_CLOCK_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-fpa1.h b/arch/mips/include/asm/octeon/cvmx-fpa1.h
new file mode 100644
index 0000000..700a576
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-fpa1.h
@@ -0,0 +1,159 @@
+/***********************license start***************
+ * Copyright (c) 2003-2010  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Interface to the hardware Free Pool Allocator on the 78xx.
+ *
+ * <hr>$Revision: 94697 $<hr>
+ *
+ */
+
+#ifndef __CVMX_FPA1_H__
+#define __CVMX_FPA1_H__
+
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include <asm/octeon/cvmx-fpa-defs.h>
+#endif
+
+#define CVMX_FPA_NUM_POOLS      8
+
+/**
+ * Structure to store FPA pool configuration parameters.
+ */
+typedef struct cvmx_fpa_pool_config
+{
+	int64_t pool_num;
+	uint64_t buffer_size;
+	uint64_t buffer_count;
+}cvmx_fpa_pool_config_t;
+
+/**
+ * Allocate or reserve  the specified fpa pool.
+ *
+ * @param pool	  FPA pool to allocate/reserve. If -1 it
+ *                finds an empty pool to allocate.
+ * @return        Alloctaed pool number or -1 if fails to allocate
+                  the pool
+ */
+int cvmx_fpa_alloc_pool(int pool);
+
+/**
+ * Free the specified fpa pool.
+ * @param pool	   Pool to free
+ * @return         0 for success -1 failure
+ */
+int cvmx_fpa_release_pool(int pool);
+
+int cvmx_fpa1_pool_init(int pool_id, int num_blocks, int block_size,
+		       void *buffer, const char *name);
+
+static inline void cvmx_fpa1_free(void *ptr, uint64_t pool,
+	uint64_t num_cache_lines)
+{
+	cvmx_addr_t newptr;
+
+	newptr.u64 = cvmx_ptr_to_phys(ptr);
+	newptr.sfilldidspace.didspace =
+		CVMX_ADDR_DIDSPACE(CVMX_FULL_DID(CVMX_OCT_DID_FPA, pool));
+	/* Make sure that any previous writes to memory go out before we free
+	 * this buffer.  This also serves as a barrier to prevent GCC from
+	 * reordering operations to after the free.
+	 */
+	CVMX_SYNCWS;
+	/* value written is number of cache lines not written back */
+	cvmx_write_io(newptr.u64, num_cache_lines);
+}
+
+static inline void cvmx_fpa1_free_nosync(void *ptr, uint64_t pool,
+					uint64_t num_cache_lines)
+{
+	cvmx_addr_t newptr;
+	newptr.u64 = cvmx_ptr_to_phys(ptr);
+	newptr.sfilldidspace.didspace =
+		CVMX_ADDR_DIDSPACE(CVMX_FULL_DID(CVMX_OCT_DID_FPA, pool));
+	/* Prevent GCC from reordering around free */
+	asm volatile ("":::"memory");
+	/* value written is number of cache lines not written back */
+	cvmx_write_io(newptr.u64, num_cache_lines);
+}
+
+/**
+ * Enable the FPA for use. Must be performed after any CSR
+ * configuration but before any other FPA functions.
+ */
+static inline void cvmx_fpa1_enable(void)
+{
+	cvmx_fpa_ctl_status_t status;
+
+	status.u64 = cvmx_read_csr(CVMX_FPA_CTL_STATUS);
+	if (status.s.enb) {
+		/*
+	 	* CN68XXP1 should not reset the FPA (doing so may break
+		* the SSO, so we may end up enabling it more than once.
+		* Just return and don't spew messages.
+	 	*/
+		return;
+	}
+
+	status.u64 = 0;
+	status.s.enb = 1;
+	cvmx_write_csr(CVMX_FPA_CTL_STATUS, status.u64);
+}
+
+static inline void *cvmx_fpa1_alloc(uint64_t pool)
+{
+	uint64_t address;
+
+	for (;;) {
+		address = cvmx_read_csr(CVMX_ADDR_DID(CVMX_FULL_DID(CVMX_OCT_DID_FPA,
+					pool)));
+		if (cvmx_likely(address)) {
+			return cvmx_phys_to_ptr(address);
+		} else {
+			if (cvmx_read_csr(CVMX_FPA_QUEX_AVAILABLE(pool)) > 0)
+				cvmx_wait(50);
+			else
+				return NULL;
+		}
+	}
+}
+
+#endif
diff --git a/arch/mips/include/asm/octeon/cvmx-fpa3.h b/arch/mips/include/asm/octeon/cvmx-fpa3.h
new file mode 100644
index 0000000..0f1410e
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-fpa3.h
@@ -0,0 +1,415 @@
+/***********************license start***************
+ * Copyright (c) 2014  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Interface to the hardware Free Pool Allocator on the 78xx.
+ *
+ * <hr>$Revision: 94697 $<hr>
+ *
+ */
+
+#ifndef __CVMX_FPA3_H__
+#define __CVMX_FPA3_H__
+
+#include "cvmx.h"
+
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include "cvmx-fpa-defs.h"
+#endif
+
+enum fpa_pool_alignment {
+	FPA_NATURAL_ALIGNMENT,
+	FPA_OFFSET_ALIGNMENT,
+	FPA_OPAQUE_ALIGNMENT
+};
+
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+extern "C" {
+/* *INDENT-ON* */
+#endif
+
+#define CVMX_FPA3_AURA_NUM       	1024
+#define CVMX_FPA3_NUM_POOLS			64
+#define CVMX_FPA3_AURA_NAME_LEN  	32
+
+#define CVMX_FPA3_NULL_POOL 		0
+
+//#define CVMX_DEBUG_FPA3 			1
+/**
+ * Struct describing load allocate operation addresses for FPA pool.
+ */
+typedef union {
+	uint64_t u64;
+	struct {
+		CVMX_BITFIELD_FIELD(uint64_t seg:2,
+		CVMX_BITFIELD_FIELD(uint64_t reserved1:13,
+		CVMX_BITFIELD_FIELD(uint64_t io:1,		/** Indicates I/O space */
+		CVMX_BITFIELD_FIELD(uint64_t did:8,		/**
+					 * the ID of the device on the
+					 * non-coherent bus
+					 */
+		CVMX_BITFIELD_FIELD(uint64_t node:4,	/** OCI node number */
+		CVMX_BITFIELD_FIELD(uint64_t red:1,		/** Perform RED on allocation */
+		CVMX_BITFIELD_FIELD(uint64_t reserved2:9,   /** Reserved */
+		CVMX_BITFIELD_FIELD(uint64_t aura:10,	/** Aura number */
+		CVMX_BITFIELD_FIELD(uint64_t reserved3:16,	/** Reserved */
+		)))))))));
+	};
+} cvmx_fpa3_load_data_t;
+
+/**
+ * Struct describing store free operation addresses from FPA pool.
+ */
+typedef union {
+	uint64_t u64;
+	struct {
+		CVMX_BITFIELD_FIELD(uint64_t seg:2,
+		CVMX_BITFIELD_FIELD(uint64_t reserved1:13,
+		CVMX_BITFIELD_FIELD(uint64_t io:1,		/** Indicates I/O space */
+		CVMX_BITFIELD_FIELD(uint64_t did:8,		/**
+					 * the ID of the device on the
+					 * non-coherent bus
+					 */
+		CVMX_BITFIELD_FIELD(uint64_t node:4,	/** OCI node number */
+		CVMX_BITFIELD_FIELD(uint64_t reserved2:10,  /** Reserved */
+		CVMX_BITFIELD_FIELD(uint64_t aura:10,	/** Aura number */
+		CVMX_BITFIELD_FIELD(uint64_t fabs:1,	/** free absolute */
+		CVMX_BITFIELD_FIELD(uint64_t reserved3:3,	/** Reserved */
+		CVMX_BITFIELD_FIELD(uint64_t dwb_count:9,	/**
+					 * Number of cache lines for which the
+					 * hardware should try to execute
+					 * 'don't-write-back' commands.
+					 */
+		CVMX_BITFIELD_FIELD(uint64_t reserved4:3,	/** Reserved */
+		)))))))))));
+	};
+} cvmx_fpa3_store_addr_t;
+
+int cvmx_fpa3_allocate_auras(int node, int auras_allocated[], int count);
+int cvmx_fpa3_allocate_aura(int node);
+int cvmx_fpa3_free_auras(int node, int *auras_allocated, int count);
+int cvmx_fpa3_allocate_pools(int node, int pools_allocated[], int count);
+int cvmx_fpa3_free_pools(int node, int *pools_allocated, int count);
+
+void cvmx_fpa3_pool_set_stack(int node, int pool_id, uint64_t base, uint64_t len);
+
+static inline int cvmx_fpa3_arua_to_guara(int node, int laura)
+{
+	return node << 10 | laura;
+}
+
+int cvmx_fpa3_aura_to_pool(int node, int aura_id);
+void* cvmx_fpa3_aura_get_base(int node, int aura_id);
+
+/**
+ * Get a new block from the FPA Aura
+ *
+ * @param node  - node number
+ * @param aura  - aura to get the block from
+ * @return pointer to the block or NULL on failure
+ */
+static inline void *cvmx_fpa3_alloc_aura(unsigned int node, unsigned int aura)
+{
+	uint64_t address;
+	cvmx_fpa3_load_data_t load_addr;
+
+	load_addr.u64 = 0;
+	load_addr.seg = CVMX_MIPS_SPACE_XKPHYS;
+	load_addr.io = 1;
+	load_addr.did = 0x29;    /* Device ID. Indicates FPA. */
+	load_addr.node = node;   /* OCI node number */
+	load_addr.red = 0;       /* Perform RED on allocation.
+					 * FIXME to use config option
+					 */
+	load_addr.aura = aura;   /* Aura number */
+
+#ifdef CVMX_DEBUG_FPA3
+	if (cvmx_fpa3_aura_to_pool(node, aura) == CVMX_FPA3_NULL_POOL)
+		cvmx_dprintf("Error: FPA aura %u assigned to null pool.\n", aura);
+#endif
+
+	address = cvmx_read64_uint64(load_addr.u64);
+	if (!address)
+		return NULL;
+	return cvmx_phys_to_ptr(address);
+}
+
+static inline void *cvmx_fpa3_alloc_gaura(unsigned int gaura)
+{
+	return cvmx_fpa3_alloc_aura(gaura >> 10, gaura & 0x3ff);
+}
+
+/**
+ * Free a pointer back to the aura.
+ *
+ * @param node   node number
+ * @param aura   aura number
+ * @param ptr    physical address of block to free.
+ * @param num_cache_lines Cache lines to invalidate
+ */
+static inline void cvmx_fpa3_free_aura(void *ptr, uint64_t node, int aura,
+				       unsigned int num_cache_lines)
+{
+	cvmx_fpa3_store_addr_t newptr;
+	cvmx_addr_t newdata;
+
+	newdata.u64 = cvmx_ptr_to_phys(ptr);
+
+	newptr.u64 = 0;
+	newptr.seg = CVMX_MIPS_SPACE_XKPHYS;
+	newptr.io = 1;
+	newptr.did = 0x29;    /* Device id, indicates FPA */
+	newptr.node = node;   /* OCI node number. */
+	newptr.aura = aura;   /* Aura number */
+	newptr.fabs = 0;	/* Free absolute. FIXME to use config option */
+	newptr.dwb_count = num_cache_lines;
+
+	/*cvmx_dprintf("aura = %d ptr_to_phys(ptr) = 0x%llx newptr.u64 = 0x%llx"
+		     " ptr = %p \n", ptr, aura, (ULL) newptr.u64
+		     (ULL) cvmx_ptr_to_phys(ptr)); */
+	/* Make sure that any previous writes to memory go out before we free
+	   this buffer. This also serves as a barrier to prevent GCC from
+	   reordering operations to after the free. */
+	CVMX_SYNCWS;
+        cvmx_write_io(newptr.u64, newdata.u64);
+}
+
+static inline void cvmx_fpa3_free_gaura(void *ptr, unsigned int gaura,
+				       unsigned int num_cache_lines)
+{
+	cvmx_fpa3_free_aura(ptr, gaura >> 10, gaura & 0x3ff, num_cache_lines);
+}
+
+/**
+ * Free a pointer back to the aura, without flushing the write buffer
+ *
+ * @param node   node number
+ * @param aura   aura number
+ * @param ptr    physical address of block to free.
+ * @param num_cache_lines Cache lines to invalidate
+ */
+static inline void cvmx_fpa3_free_aura_nosync(void *ptr, uint64_t node, int aura,
+					      uint64_t num_cache_lines)
+{
+	cvmx_fpa3_store_addr_t newptr;
+	cvmx_addr_t newdata;
+
+	newdata.u64 = cvmx_ptr_to_phys(ptr);
+
+	newptr.u64 = 0;
+	newptr.seg = CVMX_MIPS_SPACE_XKPHYS;
+	newptr.io = 1;
+	newptr.did = 0x29;    /* Device id, indicates FPA */
+	newptr.node = node;   /* OCI node number. */
+	newptr.aura = aura;   /* Aura number */
+	newptr.fabs = 0;	/* Free absolute. FIXME to use config option */
+	newptr.dwb_count = num_cache_lines;
+	/* Prevent GCC from reordering around free */
+	asm volatile ("":::"memory");
+	cvmx_write_io(newptr.u64, newdata.u64);
+}
+
+static inline void __cvmx_fpa_aura_cfg(int node, int aura, int pool,
+				       int buffers_cnt, int ptr_dis)
+{
+       cvmx_fpa_aurax_cfg_t aura_cfg;
+       uint64_t pool64 = pool;
+
+       aura_cfg.u64 = cvmx_read_csr_node(node, CVMX_FPA_AURAX_CFG(aura));
+       aura_cfg.s.ptr_dis = ptr_dis;
+        /* Configure CVMX_FPA_AURAX_CNT_LEVELS, CVMX_FPA_AURAX_POOL_LEVELS  */
+       cvmx_write_csr_node(node, CVMX_FPA_AURAX_CFG(aura), aura_cfg.u64);
+       cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT_ADD(aura), buffers_cnt);
+       cvmx_write_csr_node(node, CVMX_FPA_AURAX_POOL(aura), pool64);
+
+       /* TODO : config back pressure, RED */
+}
+
+/**
+ * This function sets up QOS related parameter for specified Aura.
+ *
+ * @param node       node number.
+ * @param aura       aura to configure.
+ * @param ena_red       enable RED based on [DROP] and [PASS] levels
+                        1: enable 0:disable
+ * @param pass_thresh   pass threshold for RED.
+ * @param drop_thresh   drop threshold for RED
+ * @param ena_bp        enable backpressure based on [BP] level.
+                        1:enable 0:disable
+ * @param bp_thresh     backpressure threshold.
+ *
+ */
+static inline void cvmx_fpa_setup_aura_qos(int node, int aura, bool ena_red,
+					   uint64_t pass_thresh,
+					   uint64_t drop_thresh,
+					   bool ena_bp,uint64_t bp_thresh)
+{
+	uint64_t shift=0;
+	uint64_t shift_thresh;
+	cvmx_fpa_aurax_cnt_levels_t aura_level;
+
+	shift_thresh = bp_thresh > drop_thresh ? bp_thresh:drop_thresh;
+
+	while ( (shift_thresh & (uint64_t)(~(0xff)))) {
+		shift_thresh = shift_thresh >> 1;
+		shift++;
+	};
+
+	aura_level.u64 = cvmx_read_csr_node(node,CVMX_FPA_AURAX_CNT_LEVELS(aura));
+	aura_level.s.pass = pass_thresh >> shift;
+	aura_level.s.drop = drop_thresh >> shift;
+	aura_level.s.bp = bp_thresh >> shift;
+	aura_level.s.shift = shift;
+	aura_level.s.red_ena = ena_red;
+	aura_level.s.bp_ena = ena_bp;
+	cvmx_write_csr_node(node,CVMX_FPA_AURAX_CNT_LEVELS(aura),aura_level.u64);
+}
+
+/**
+ * This call will initialise the stack of the specified pool. Only the stack
+ * memory which is the memory that holds the buffer pointers is allocated.
+ * For now assume that natural alignment is used. When using natural alignment
+ * the hardware needs to be initialised with the buffer size and hence it is
+ * specified at the time of pool initialisation. The value will be buffered and
+ * used later during cvmx_fpa_aura_init() to allocate buffers.
+ * Before invoking this call the application should already have ownership of
+ * the pool, the ownership is obtained when pool is one of the values in the
+ * pools_allocated array after invocation of cvmx_allocate_fpa_pool()
+ * The linux device driver chooses to not use this call as it would initialise
+ * the FPA pool with kernel memory as opposed to using bootmem.
+ * @param node     - specifies the node of FPA pool.
+ * @parma pool     - Specifies the FPA pool number.
+ * @param name     - Specifies the FPA pool name.
+ * @param mem_node - specifies the node from which the memory for the stack
+ *                   is allocated.
+ * @param max_buffer_cnt - specifies the maximum buffers that FPA pool can hold.
+ * @parm  align          - specifies the alignment type.
+ * @param buffer_sz      - Only when the alignment is natural this field is used
+ *                         to specify the size of each buffer in the FPA .
+ *
+ */
+int cvmx_fpa3_pool_stack_init(int node, int pool, const char *name, int mem_node,
+			     int max_buffer_cnt, enum fpa_pool_alignment align,
+			     int buffer_sz);
+
+/**
+ * This call will allocate buffers_cnt number of buffers from  the bootmemory
+ * of bootmem_node and populate the aura specified with the allocated buffers.
+ * The size of the buffers is obtained from the buffer_sz used to initialise the
+ * stack of the FPA pool associated with the aura. This also means that the aura
+ * has already been mapped to an FPA pool. More parameters is possible to
+ * specify RED and buffer level parameters. This is another that would not be
+ * used by the Linux device driver, the driver would populate the buffers
+ * in the pool using it's own allocation mechanism.
+ * @param node     - specifies the node of aura to be initialized
+ * @parma aura     - specifies the aura to be initalized.
+ * @param name     - specifies the name of aura to be initalized.
+ * @param mem_node - specifies the node from which the memory for the buffers
+ *                   is allocated.
+ * @param buffers  - Block of memory to use for the aura buffers. If NULL,
+ *                   aura memory is allocated.
+ * @param ptr_dis - Need to look into this more but is on the lines of of
+ *		    whether the hardware checks double frees.
+ */
+int cvmx_fpa3_aura_init(int node, int aura, const char *name, int mem_node,
+		       void *buffers, int buffers_cnt, int ptr_dis);
+
+static inline int cvmx_fpa3_config_red_params(int node, int qos_avg_en, int red_lvl_dly,
+			       int avg_dly)
+{
+	cvmx_fpa_gen_cfg_t fpa_cfg;
+	cvmx_fpa_red_delay_t red_delay;
+
+	fpa_cfg.u64 = cvmx_read_csr_node(node,CVMX_FPA_GEN_CFG);
+	fpa_cfg.s.avg_en = qos_avg_en;
+	fpa_cfg.s.lvl_dly = red_lvl_dly;
+	cvmx_write_csr_node(node,CVMX_FPA_GEN_CFG,fpa_cfg.u64);
+
+	red_delay.u64 = cvmx_read_csr_node(node,CVMX_FPA_RED_DELAY);
+	red_delay.s.avg_dly = avg_dly;
+	cvmx_write_csr_node(node,CVMX_FPA_RED_DELAY,red_delay.u64);
+	return 0;
+}
+
+/**
+ * Gets the buffer size of the specified AURA,
+ * which is a 12-bit quantity, with the upper 2 bits
+ * representing the OCI node number, a.k.a. GAURA.
+ *
+ * @param gaura Global aura number
+ * @return Returns size of the buffers in the specified aura.
+ */
+unsigned cvmx_fpa3_get_aura_buf_size(uint16_t gaura);
+
+/**
+ * This will map auras specified in the auras_index[] array to specified
+ * FPA pool_index.
+ * The array is assumed to contain count number of entries.
+ * @param count is the number of entries in the auras_index array.
+ * @pool_index is the index of the fpa pool.
+ * @return 0 on success
+ */
+int cvmx_fpa3_assign_auras(int node, int auras_index[], int count,
+			  int pool_index);
+
+int cvmx_fpa3_assign_aura(int node, int aura, int pool_index);
+
+static inline void cvmx_fpa3_disable_pool(int node, int pool_num)
+{
+	cvmx_fpa_poolx_cfg_t pool_cfg;
+
+	pool_cfg.u64 = 0;
+	pool_cfg.cn78xx.ena = 0;
+
+	cvmx_write_csr_node(node, CVMX_FPA_POOLX_CFG(pool_num), pool_cfg.u64);
+	cvmx_write_csr_node(node, CVMX_FPA_POOLX_STACK_BASE(pool_num), 0);
+	cvmx_write_csr_node(node, CVMX_FPA_POOLX_STACK_ADDR(pool_num), 0);
+	cvmx_write_csr_node(node, CVMX_FPA_POOLX_STACK_END(pool_num), 0);
+}
+
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+}
+/* *INDENT-ON* */
+#endif
+
+#endif /*  __CVM_FPA3_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-global-resources.h b/arch/mips/include/asm/octeon/cvmx-global-resources.h
new file mode 100644
index 0000000..f0aa0e7
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-global-resources.h
@@ -0,0 +1,179 @@
+#ifndef _CVMX_GLOBAL_RESOURCES_T_
+#define _CVMX_GLOBAL_RESOURCES_T_
+
+#define CVMX_GLOBAL_RESOURCES_DATA_NAME "cvmx-global-resources"
+
+/*In macros below abbreviation GR stands for global resources. */
+#define CVMX_GR_TAG_INVALID cvmx_get_gr_tag('i','n','v','a','l','i','d','.','.','.','.','.','.','.','.','.')
+/*Tag for pko que table range. */
+#define CVMX_GR_TAG_PKO_QUEUES cvmx_get_gr_tag('c','v','m','_','p','k','o','_','q','u','e','u','s','.','.','.')
+/*Tag for a pko internal ports range */
+#define CVMX_GR_TAG_PKO_IPORTS cvmx_get_gr_tag('c','v','m','_','p','k','o','_','i','p','o','r','t','.','.','.')
+#define CVMX_GR_TAG_FPA        cvmx_get_gr_tag('c','v','m','_','f','p','a','.','.','.','.','.','.','.','.','.')
+#define CVMX_GR_TAG_FAU        cvmx_get_gr_tag('c','v','m','_','f','a','u','.','.','.','.','.','.','.','.','.')
+#define CVMX_GR_TAG_TIM(n)     cvmx_get_gr_tag('c','v','m','_','t','i','m','_',(n)+'0','.','.','.','.','.','.','.')
+#define CVMX_GR_TAG_CLUSTERS(x)	    cvmx_get_gr_tag('c','v','m','_','c','l','u','s','t','e','r','_',(x+'0'),'.','.','.')
+#define CVMX_GR_TAG_CLUSTER_GRP(x)  cvmx_get_gr_tag('c','v','m','_','c','l','g','r','p','_',(x+'0'),'.','.','.','.','.')
+#define CVMX_GR_TAG_STYLE(x)        cvmx_get_gr_tag('c','v','m','_','s','t','y','l','e','_',(x+'0'),'.','.','.','.','.')
+#define CVMX_GR_TAG_QPG_ENTRY(x)    cvmx_get_gr_tag('c','v','m','_','q','p','g','e','t','_',(x+'0'),'.','.','.','.','.')
+#define CVMX_GR_TAG_PCAM(x,y,z) \
+	cvmx_get_gr_tag('c','v','m','_','p','c','a','m','_',(x+'0'),(y+'0'),(z+'0'),'.','.','.','.')
+
+#define CVMX_GR_TAG_CIU3_IDT(_n) \
+	cvmx_get_gr_tag('c','v','m','_','c','i','u','3','_', ((_n) + '0'),'_','i','d','t','.','.')
+
+
+/* Allocation of the 512 SW INTSTs (in the  12 bit SW INTSN space) */
+#define CVMX_GR_TAG_CIU3_SWINTSN(_n) \
+	cvmx_get_gr_tag('c','v','m','_','c','i','u','3','_', ((_n) + '0'),'_','s','w','i','s','n')
+
+#define TAG_INIT_PART(A,B,C,D,E,F,G,H) ( \
+	(((uint64_t)(A) & 0xff) << 56) | (((uint64_t)(B) & 0xff) << 48) | (((uint64_t)(C) & 0xff) << 40)  | (((uint64_t)(D) & 0xff) << 32) | \
+	(((uint64_t)(E) & 0xff) << 24) | (((uint64_t)(F) & 0xff) << 16) | (((uint64_t)(G) & 0xff) << 8)   | (((uint64_t)(H) & 0xff)))
+
+struct global_resource_tag
+{
+	uint64_t lo;
+	uint64_t hi;
+};
+
+enum cvmx_resource_err
+{
+	CVMX_RESOURCE_ALLOC_FAILED = -1,
+	CVMX_RESOURCE_ALREADY_RESERVED = -2
+};
+
+/*
+ * @INTERNAL
+ * Creates a tag from the specified characters.
+ */
+static inline struct global_resource_tag
+cvmx_get_gr_tag(char a, char b, char c, char d, char e, char f, char g, char h,
+		char i, char j, char k, char l, char m, char n, char o, char p)
+{
+	struct global_resource_tag tag;
+	tag.lo =  TAG_INIT_PART(a,b,c,d,e,f,g,h);
+	tag.hi =  TAG_INIT_PART(i,j,k,l,m,n,o,p);
+	return tag;
+}
+
+/*
+ * @INTERNAL
+ * Creates a global resource range that can hold the specified number of
+ * elements
+ * @param tag is the tag of the range. The taga is created using the method
+ * cvmx_get_gr_tag()
+ * @param nelements is the number of elements to be held in the resource range.
+ */
+int cvmx_create_global_resource_range(struct global_resource_tag tag,int nelements);
+
+/*
+ * @INTERNAL
+ * Allocate nelements in the global resource range with the specified tag. It
+ * is assumed that prior
+ * to calling this the global resource range has already been created using
+ * cvmx_create_global_resource_range().
+ * @param tag is the tag of the global resource range.
+ * @param nelements is the number of elements to be allocated.
+ * @param owner is a 64 bit number that identifes the owner of this range.
+ * @aligment specifes the required alignment of the returned base number.
+ * @return returns the base of the allocated range. -1 return value indicates
+ * failure.
+ */
+int cvmx_allocate_global_resource_range(struct global_resource_tag tag,
+					uint64_t owner, int nelements,
+					int alignment);
+
+/*
+ * @INTERNAL
+ * Allocate nelements in the global resource range with the specified tag.
+ * The elements allocated need not be contiguous. It is assumed that prior to
+ * calling this the global resource range has already
+ * been created using cvmx_create_global_resource_range().
+ * @param tag is the tag of the global resource range.
+ * @param nelements is the number of elements to be allocated.
+ * @param owner is a 64 bit number that identifes the owner of the allocated
+ * elements.
+ * @param allocated_elements returns indexs of the allocated entries.
+ * @return returns 0 on success and -1 on failure.
+ */
+int cvmx_resource_alloc_many(struct global_resource_tag tag,
+			     uint64_t owner,
+			     int nelements,
+			     int allocated_elements[]);
+/*
+ * @INTERNAL
+ * Reserve nelements starting from base in the global resource range with the
+ * specified tag.
+ * It is assumed that prior to calling this the global resource range has
+ * already been created using cvmx_create_global_resource_range().
+ * @param tag is the tag of the global resource range.
+ * @param nelements is the number of elements to be allocated.
+ * @param owner is a 64 bit number that identifes the owner of this range.
+ * @base specifies the base start of nelements.
+ * @return returns the base of the allocated range. -1 return value indicates
+ * failure.
+ */
+int cvmx_reserve_global_resource_range(struct global_resource_tag tag,
+				       uint64_t owner, int base, int nelements);
+/*
+ * @INTERNAL
+ * Free nelements starting at base in the global resource range with the
+ * specified tag.
+ * @param tag is the tag of the global resource range.
+ * @param base is the base number
+ * @param nelements is the number of elements that are to be freed.
+ * @return returns 0 if successful and -1 on failure.
+ */
+int cvmx_free_global_resource_range_with_base(struct global_resource_tag tag,
+						     int base, int nelements);
+
+
+/*
+ * @INTERNAL
+ * Free nelements with the bases specified in bases[] with the
+ * specified tag.
+ * @param tag is the tag of the global resource range.
+ * @param bases is an array containing the bases to be freed.
+ * @param nelements is the number of elements that are to be freed.
+ * @return returns 0 if successful and -1 on failure.
+ */
+int cvmx_free_global_resource_range_multiple(struct global_resource_tag tag,
+					     int bases[], int nelements);
+/*
+ * @INTERNAL
+ * Free elements from the specified owner in the global resource range with the
+ * specified tag.
+ * @param tag is the tag of the global resource range.
+ * @param owner is the owner of resources that are to be freed.
+ * @return returns 0 if successful and -1 on failure.
+ */
+int cvmx_free_global_resource_range_with_owner(struct global_resource_tag tag,
+					       int owner);
+
+/*
+ * @INTERNAL
+ * Frees all the global resources that have been created.
+ * For use only from the bootloader, when it shutdown and boots up the
+ * application or kernel.
+ */
+int free_global_resources(void);
+
+/*
+ * @INTERNAL
+ * Shows the global resource range with the specified tag. Use mainly for debug.
+ */
+void  cvmx_show_global_resource_range(struct global_resource_tag tag);
+
+/*
+ * @INTERNAL
+ * Shows all the global resources. Used mainly for debug.
+ */
+void cvmx_global_resources_show(void);
+
+void cvmx_range_cleanup(uint64_t range_addr);
+
+void octeon_ethernet_resource_cleanup(void);
+
+#endif
+
diff --git a/arch/mips/include/asm/octeon/cvmx-gmxx-defs.h b/arch/mips/include/asm/octeon/cvmx-gmxx-defs.h
index e347496..fd1e37b 100644
--- a/arch/mips/include/asm/octeon/cvmx-gmxx-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-gmxx-defs.h
@@ -2483,6 +2483,7 @@ union cvmx_gmxx_inf_mode {
 	struct cvmx_gmxx_inf_mode_cn68xx cn68xxp1;
 	struct cvmx_gmxx_inf_mode_cn61xx cnf71xx;
 };
+typedef union cvmx_gmxx_inf_mode cvmx_gmxx_inf_mode_t;
 
 union cvmx_gmxx_nxa_adr {
 	uint64_t u64;
diff --git a/arch/mips/include/asm/octeon/cvmx-gserx-defs.h b/arch/mips/include/asm/octeon/cvmx-gserx-defs.h
new file mode 100644
index 0000000..7fce9ac
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-gserx-defs.h
@@ -0,0 +1,6837 @@
+/***********************license start***************
+ * Copyright (c) 2003-2014  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+
+/**
+ * cvmx-gserx-defs.h
+ *
+ * Configuration and status register (CSR) type definitions for
+ * Octeon gserx.
+ *
+ * This file is auto generated. Do not edit.
+ *
+ * <hr>$Revision$<hr>
+ *
+ */
+#ifndef __CVMX_GSERX_DEFS_H__
+#define __CVMX_GSERX_DEFS_H__
+
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_ANA_ATEST(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_ANA_ATEST(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090000800ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_ANA_ATEST(block_id) (CVMX_ADD_IO_SEG(0x0001180090000800ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_ANA_SEL(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_ANA_SEL(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090000808ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_ANA_SEL(block_id) (CVMX_ADD_IO_SEG(0x0001180090000808ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_BR_RXX_CTL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_BR_RXX_CTL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090000400ull) + (((offset) & 3) + ((block_id) & 15) * 0x20000ull) * 128;
+}
+#else
+#define CVMX_GSERX_BR_RXX_CTL(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090000400ull) + (((offset) & 3) + ((block_id) & 15) * 0x20000ull) * 128)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_BR_RXX_EER(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_BR_RXX_EER(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090000418ull) + (((offset) & 3) + ((block_id) & 15) * 0x20000ull) * 128;
+}
+#else
+#define CVMX_GSERX_BR_RXX_EER(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090000418ull) + (((offset) & 3) + ((block_id) & 15) * 0x20000ull) * 128)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_BR_TXX_CTL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_BR_TXX_CTL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090000420ull) + (((offset) & 3) + ((block_id) & 15) * 0x20000ull) * 128;
+}
+#else
+#define CVMX_GSERX_BR_TXX_CTL(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090000420ull) + (((offset) & 3) + ((block_id) & 15) * 0x20000ull) * 128)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_BR_TXX_CUR(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_BR_TXX_CUR(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090000438ull) + (((offset) & 3) + ((block_id) & 15) * 0x20000ull) * 128;
+}
+#else
+#define CVMX_GSERX_BR_TXX_CUR(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090000438ull) + (((offset) & 3) + ((block_id) & 15) * 0x20000ull) * 128)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_CFG(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_CFG(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090000080ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_CFG(block_id) (CVMX_ADD_IO_SEG(0x0001180090000080ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DBG(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_DBG(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090000098ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_DBG(block_id) (CVMX_ADD_IO_SEG(0x0001180090000098ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_LOOPBK_EN(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset == 0)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_LOOPBK_EN(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090001008ull);
+}
+#else
+#define CVMX_GSERX_DLMX_LOOPBK_EN(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090001008ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_LOS_BIAS(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_LOS_BIAS(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090001010ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_DLMX_LOS_BIAS(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090001010ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_LOS_LEVEL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_LOS_LEVEL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090001018ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_DLMX_LOS_LEVEL(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090001018ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_MISC_STATUS(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset == 0)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_MISC_STATUS(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090000000ull);
+}
+#else
+#define CVMX_GSERX_DLMX_MISC_STATUS(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090000000ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_MPLL_EN(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset == 0)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_MPLL_EN(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090001020ull);
+}
+#else
+#define CVMX_GSERX_DLMX_MPLL_EN(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090001020ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_MPLL_HALF_RATE(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_MPLL_HALF_RATE(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090001028ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_DLMX_MPLL_HALF_RATE(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090001028ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_MPLL_MULTIPLIER(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_MPLL_MULTIPLIER(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090001030ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_DLMX_MPLL_MULTIPLIER(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090001030ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_MPLL_STATUS(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_MPLL_STATUS(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090001000ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_DLMX_MPLL_STATUS(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090001000ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_PHY_RESET(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_PHY_RESET(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090001038ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_DLMX_PHY_RESET(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090001038ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_REFCLK_SEL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_REFCLK_SEL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090000008ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_DLMX_REFCLK_SEL(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090000008ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_REF_CLKDIV2(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_REF_CLKDIV2(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090001040ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_DLMX_REF_CLKDIV2(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090001040ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_REF_SSP_EN(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset == 0)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_REF_SSP_EN(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090001048ull);
+}
+#else
+#define CVMX_GSERX_DLMX_REF_SSP_EN(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090001048ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_REF_USE_PAD(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_REF_USE_PAD(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090001050ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_DLMX_REF_USE_PAD(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090001050ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_RX_DATA_EN(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset == 0)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_RX_DATA_EN(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090002008ull);
+}
+#else
+#define CVMX_GSERX_DLMX_RX_DATA_EN(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090002008ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_RX_EQ(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_RX_EQ(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090002010ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_DLMX_RX_EQ(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090002010ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_RX_LOS_EN(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset == 0)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_RX_LOS_EN(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090002018ull);
+}
+#else
+#define CVMX_GSERX_DLMX_RX_LOS_EN(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090002018ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_RX_PLL_EN(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset == 0)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_RX_PLL_EN(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090002020ull);
+}
+#else
+#define CVMX_GSERX_DLMX_RX_PLL_EN(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090002020ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_RX_RATE(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset == 0)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_RX_RATE(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090002028ull);
+}
+#else
+#define CVMX_GSERX_DLMX_RX_RATE(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090002028ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_RX_RESET(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset == 0)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_RX_RESET(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090002030ull);
+}
+#else
+#define CVMX_GSERX_DLMX_RX_RESET(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090002030ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_RX_STATUS(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_RX_STATUS(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090002000ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_DLMX_RX_STATUS(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090002000ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_RX_TERM_EN(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset == 0)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_RX_TERM_EN(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090002038ull);
+}
+#else
+#define CVMX_GSERX_DLMX_RX_TERM_EN(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090002038ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_TEST_BYPASS(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_TEST_BYPASS(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090001058ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_DLMX_TEST_BYPASS(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090001058ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_TEST_POWERDOWN(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_TEST_POWERDOWN(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090001060ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_DLMX_TEST_POWERDOWN(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090001060ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_TX_AMPLITUDE(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset == 0)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_TX_AMPLITUDE(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090003008ull);
+}
+#else
+#define CVMX_GSERX_DLMX_TX_AMPLITUDE(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090003008ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_TX_CM_EN(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset == 0)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_TX_CM_EN(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090003010ull);
+}
+#else
+#define CVMX_GSERX_DLMX_TX_CM_EN(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090003010ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_TX_DATA_EN(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset == 0)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_TX_DATA_EN(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090003018ull);
+}
+#else
+#define CVMX_GSERX_DLMX_TX_DATA_EN(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090003018ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_TX_EN(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset == 0)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_TX_EN(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090003020ull);
+}
+#else
+#define CVMX_GSERX_DLMX_TX_EN(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090003020ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_TX_PREEMPH(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset == 0)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_TX_PREEMPH(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090003028ull);
+}
+#else
+#define CVMX_GSERX_DLMX_TX_PREEMPH(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090003028ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_TX_RATE(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset == 0)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_TX_RATE(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090003030ull);
+}
+#else
+#define CVMX_GSERX_DLMX_TX_RATE(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090003030ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_TX_RESET(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset == 0)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_TX_RESET(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090003038ull);
+}
+#else
+#define CVMX_GSERX_DLMX_TX_RESET(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090003038ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_TX_STATUS(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_TX_STATUS(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090003000ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_DLMX_TX_STATUS(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090003000ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_DLMX_TX_TERM_OFFSET(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_DLMX_TX_TERM_OFFSET(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090003040ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_DLMX_TX_TERM_OFFSET(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090003040ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_GLBL_TAD(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_GLBL_TAD(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090460400ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_GLBL_TAD(block_id) (CVMX_ADD_IO_SEG(0x0001180090460400ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_GLBL_TM_ADMON(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_GLBL_TM_ADMON(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090460408ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_GLBL_TM_ADMON(block_id) (CVMX_ADD_IO_SEG(0x0001180090460408ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_IDDQ_MODE(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_IDDQ_MODE(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090000018ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_IDDQ_MODE(block_id) (CVMX_ADD_IO_SEG(0x0001180090000018ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_LBERT_CFG(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_LBERT_CFG(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904C0020ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_LBERT_CFG(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800904C0020ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_LBERT_ECNT(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_LBERT_ECNT(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904C0028ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_LBERT_ECNT(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800904C0028ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_LBERT_PAT_CFG(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_LBERT_PAT_CFG(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904C0018ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_LBERT_PAT_CFG(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800904C0018ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_MISC_CFG_0(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_MISC_CFG_0(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904C0000ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_MISC_CFG_0(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800904C0000ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_MISC_CFG_1(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_MISC_CFG_1(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904C0008ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_MISC_CFG_1(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800904C0008ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_PCS_CTLIFC_0(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_PCS_CTLIFC_0(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904C0060ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_PCS_CTLIFC_0(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800904C0060ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_PCS_CTLIFC_1(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_PCS_CTLIFC_1(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904C0068ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_PCS_CTLIFC_1(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800904C0068ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_PCS_CTLIFC_2(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_PCS_CTLIFC_2(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904C0070ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_PCS_CTLIFC_2(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800904C0070ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_PMA_LOOPBACK_CTRL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_PMA_LOOPBACK_CTRL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904400D0ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_PMA_LOOPBACK_CTRL(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800904400D0ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_PWR_CTRL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_PWR_CTRL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904400D8ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_PWR_CTRL(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800904400D8ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_RX_AEQ_OUT_0(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_RX_AEQ_OUT_0(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090440280ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_RX_AEQ_OUT_0(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090440280ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_RX_AEQ_OUT_1(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_RX_AEQ_OUT_1(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090440288ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_RX_AEQ_OUT_1(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090440288ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_RX_AEQ_OUT_2(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_RX_AEQ_OUT_2(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090440290ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_RX_AEQ_OUT_2(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090440290ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_RX_CFG_0(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_RX_CFG_0(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090440000ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_RX_CFG_0(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090440000ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_RX_CFG_1(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_RX_CFG_1(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090440008ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_RX_CFG_1(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090440008ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_RX_CFG_2(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_RX_CFG_2(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090440010ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_RX_CFG_2(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090440010ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_RX_CFG_3(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_RX_CFG_3(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090440018ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_RX_CFG_3(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090440018ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_RX_CFG_4(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_RX_CFG_4(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090440020ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_RX_CFG_4(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090440020ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_RX_CFG_5(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_RX_CFG_5(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090440028ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_RX_CFG_5(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090440028ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_RX_CTLE_CTRL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_RX_CTLE_CTRL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090440058ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_RX_CTLE_CTRL(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090440058ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_RX_MISC_OVRRD(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_RX_MISC_OVRRD(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090440258ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_RX_MISC_OVRRD(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090440258ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_RX_PRECORR_CTRL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_RX_PRECORR_CTRL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090440060ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_RX_PRECORR_CTRL(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090440060ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_RX_VALBBD_CTRL_0(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_RX_VALBBD_CTRL_0(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090440240ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_RX_VALBBD_CTRL_0(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090440240ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_RX_VALBBD_CTRL_1(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_RX_VALBBD_CTRL_1(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090440248ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_RX_VALBBD_CTRL_1(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090440248ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_RX_VALBBD_CTRL_2(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_RX_VALBBD_CTRL_2(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090440250ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_RX_VALBBD_CTRL_2(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090440250ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_RX_VMA_CTRL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_RX_VMA_CTRL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090440200ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_RX_VMA_CTRL(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090440200ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_RX_VMA_STATUS_0(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_RX_VMA_STATUS_0(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904402B8ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_RX_VMA_STATUS_0(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800904402B8ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_RX_VMA_STATUS_1(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_RX_VMA_STATUS_1(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904402C0ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_RX_VMA_STATUS_1(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800904402C0ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_TX_CFG_0(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_TX_CFG_0(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904400A8ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_TX_CFG_0(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800904400A8ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_TX_CFG_1(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_TX_CFG_1(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904400B0ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_TX_CFG_1(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800904400B0ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_TX_CFG_2(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_TX_CFG_2(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904400B8ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_TX_CFG_2(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800904400B8ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_TX_CFG_3(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_TX_CFG_3(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904400C0ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_TX_CFG_3(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800904400C0ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANEX_TX_PRE_EMPHASIS(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANEX_TX_PRE_EMPHASIS(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904400C8ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576;
+}
+#else
+#define CVMX_GSERX_LANEX_TX_PRE_EMPHASIS(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800904400C8ull) + (((offset) & 3) + ((block_id) & 15) * 0x10ull) * 1048576)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANE_LPBKEN(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_LANE_LPBKEN(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090000110ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_LANE_LPBKEN(block_id) (CVMX_ADD_IO_SEG(0x0001180090000110ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANE_MODE(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_LANE_MODE(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090000118ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_LANE_MODE(block_id) (CVMX_ADD_IO_SEG(0x0001180090000118ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANE_POFF(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_LANE_POFF(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090000108ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_LANE_POFF(block_id) (CVMX_ADD_IO_SEG(0x0001180090000108ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANE_PX_MODE_0(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 11)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANE_PX_MODE_0(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904E0040ull) + (((offset) & 15) + ((block_id) & 15) * 0x80000ull) * 32;
+}
+#else
+#define CVMX_GSERX_LANE_PX_MODE_0(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800904E0040ull) + (((offset) & 15) + ((block_id) & 15) * 0x80000ull) * 32)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANE_PX_MODE_1(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 11)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_LANE_PX_MODE_1(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904E0048ull) + (((offset) & 15) + ((block_id) & 15) * 0x80000ull) * 32;
+}
+#else
+#define CVMX_GSERX_LANE_PX_MODE_1(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800904E0048ull) + (((offset) & 15) + ((block_id) & 15) * 0x80000ull) * 32)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANE_SRST(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_LANE_SRST(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090000100ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_LANE_SRST(block_id) (CVMX_ADD_IO_SEG(0x0001180090000100ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANE_VMA_COARSE_CTRL_0(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_LANE_VMA_COARSE_CTRL_0(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904E01B0ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_LANE_VMA_COARSE_CTRL_0(block_id) (CVMX_ADD_IO_SEG(0x00011800904E01B0ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANE_VMA_COARSE_CTRL_1(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_LANE_VMA_COARSE_CTRL_1(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904E01B8ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_LANE_VMA_COARSE_CTRL_1(block_id) (CVMX_ADD_IO_SEG(0x00011800904E01B8ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANE_VMA_COARSE_CTRL_2(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_LANE_VMA_COARSE_CTRL_2(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904E01C0ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_LANE_VMA_COARSE_CTRL_2(block_id) (CVMX_ADD_IO_SEG(0x00011800904E01C0ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANE_VMA_FINE_CTRL_0(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_LANE_VMA_FINE_CTRL_0(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904E01C8ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_LANE_VMA_FINE_CTRL_0(block_id) (CVMX_ADD_IO_SEG(0x00011800904E01C8ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANE_VMA_FINE_CTRL_1(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_LANE_VMA_FINE_CTRL_1(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904E01D0ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_LANE_VMA_FINE_CTRL_1(block_id) (CVMX_ADD_IO_SEG(0x00011800904E01D0ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_LANE_VMA_FINE_CTRL_2(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_LANE_VMA_FINE_CTRL_2(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904E01D8ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_LANE_VMA_FINE_CTRL_2(block_id) (CVMX_ADD_IO_SEG(0x00011800904E01D8ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PCIE_PCS_CLK_REQ(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((block_id == 0)))))
+		cvmx_warn("CVMX_GSERX_PCIE_PCS_CLK_REQ(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090080478ull);
+}
+#else
+#define CVMX_GSERX_PCIE_PCS_CLK_REQ(block_id) (CVMX_ADD_IO_SEG(0x0001180090080478ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PCIE_PIPEX_TXDEEMPH(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 3)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_PCIE_PIPEX_TXDEEMPH(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090080480ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 8;
+}
+#else
+#define CVMX_GSERX_PCIE_PIPEX_TXDEEMPH(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090080480ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PCIE_PIPE_COM_CLK(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((block_id == 0)))))
+		cvmx_warn("CVMX_GSERX_PCIE_PIPE_COM_CLK(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090080470ull);
+}
+#else
+#define CVMX_GSERX_PCIE_PIPE_COM_CLK(block_id) (CVMX_ADD_IO_SEG(0x0001180090080470ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PCIE_PIPE_CRST(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((block_id == 0)))))
+		cvmx_warn("CVMX_GSERX_PCIE_PIPE_CRST(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090080458ull);
+}
+#else
+#define CVMX_GSERX_PCIE_PIPE_CRST(block_id) (CVMX_ADD_IO_SEG(0x0001180090080458ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PCIE_PIPE_PORT_LOOPBK(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((block_id == 0)))))
+		cvmx_warn("CVMX_GSERX_PCIE_PIPE_PORT_LOOPBK(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090080468ull);
+}
+#else
+#define CVMX_GSERX_PCIE_PIPE_PORT_LOOPBK(block_id) (CVMX_ADD_IO_SEG(0x0001180090080468ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PCIE_PIPE_PORT_SEL(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((block_id == 0)))))
+		cvmx_warn("CVMX_GSERX_PCIE_PIPE_PORT_SEL(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090080460ull);
+}
+#else
+#define CVMX_GSERX_PCIE_PIPE_PORT_SEL(block_id) (CVMX_ADD_IO_SEG(0x0001180090080460ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PCIE_PIPE_RST(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((block_id == 0)))))
+		cvmx_warn("CVMX_GSERX_PCIE_PIPE_RST(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090080448ull);
+}
+#else
+#define CVMX_GSERX_PCIE_PIPE_RST(block_id) (CVMX_ADD_IO_SEG(0x0001180090080448ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PCIE_PIPE_RST_STS(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((block_id == 0)))))
+		cvmx_warn("CVMX_GSERX_PCIE_PIPE_RST_STS(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090080450ull);
+}
+#else
+#define CVMX_GSERX_PCIE_PIPE_RST_STS(block_id) (CVMX_ADD_IO_SEG(0x0001180090080450ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PCIE_PIPE_STATUS(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((block_id == 0)))))
+		cvmx_warn("CVMX_GSERX_PCIE_PIPE_STATUS(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090080400ull);
+}
+#else
+#define CVMX_GSERX_PCIE_PIPE_STATUS(block_id) (CVMX_ADD_IO_SEG(0x0001180090080400ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PCIE_TX_DEEMPH_GEN1(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((block_id == 0)))))
+		cvmx_warn("CVMX_GSERX_PCIE_TX_DEEMPH_GEN1(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090080408ull);
+}
+#else
+#define CVMX_GSERX_PCIE_TX_DEEMPH_GEN1(block_id) (CVMX_ADD_IO_SEG(0x0001180090080408ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PCIE_TX_DEEMPH_GEN2_3P5DB(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((block_id == 0)))))
+		cvmx_warn("CVMX_GSERX_PCIE_TX_DEEMPH_GEN2_3P5DB(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090080410ull);
+}
+#else
+#define CVMX_GSERX_PCIE_TX_DEEMPH_GEN2_3P5DB(block_id) (CVMX_ADD_IO_SEG(0x0001180090080410ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PCIE_TX_DEEMPH_GEN2_6DB(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((block_id == 0)))))
+		cvmx_warn("CVMX_GSERX_PCIE_TX_DEEMPH_GEN2_6DB(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090080418ull);
+}
+#else
+#define CVMX_GSERX_PCIE_TX_DEEMPH_GEN2_6DB(block_id) (CVMX_ADD_IO_SEG(0x0001180090080418ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PCIE_TX_SWING_FULL(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((block_id == 0)))))
+		cvmx_warn("CVMX_GSERX_PCIE_TX_SWING_FULL(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090080420ull);
+}
+#else
+#define CVMX_GSERX_PCIE_TX_SWING_FULL(block_id) (CVMX_ADD_IO_SEG(0x0001180090080420ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PCIE_TX_SWING_LOW(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((block_id == 0)))))
+		cvmx_warn("CVMX_GSERX_PCIE_TX_SWING_LOW(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090080428ull);
+}
+#else
+#define CVMX_GSERX_PCIE_TX_SWING_LOW(block_id) (CVMX_ADD_IO_SEG(0x0001180090080428ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PCIE_TX_VBOOST_LVL(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((block_id == 0)))))
+		cvmx_warn("CVMX_GSERX_PCIE_TX_VBOOST_LVL(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090080440ull);
+}
+#else
+#define CVMX_GSERX_PCIE_TX_VBOOST_LVL(block_id) (CVMX_ADD_IO_SEG(0x0001180090080440ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PHYX_IDCODE_HI(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_PHYX_IDCODE_HI(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090400008ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_PHYX_IDCODE_HI(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090400008ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PHYX_IDCODE_LO(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_PHYX_IDCODE_LO(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090400000ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_PHYX_IDCODE_LO(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090400000ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PHYX_LANE0_LOOPBACK(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_PHYX_LANE0_LOOPBACK(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090408170ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_PHYX_LANE0_LOOPBACK(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090408170ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PHYX_LANE0_RX_LBERT_CTL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_PHYX_LANE0_RX_LBERT_CTL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904080B0ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_PHYX_LANE0_RX_LBERT_CTL(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800904080B0ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PHYX_LANE0_RX_LBERT_ERR(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_PHYX_LANE0_RX_LBERT_ERR(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904080B8ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_PHYX_LANE0_RX_LBERT_ERR(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800904080B8ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PHYX_LANE0_RX_OVRD_IN_LO(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_PHYX_LANE0_RX_OVRD_IN_LO(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090408028ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_PHYX_LANE0_RX_OVRD_IN_LO(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090408028ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PHYX_LANE0_TXDEBUG(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_PHYX_LANE0_TXDEBUG(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090408080ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_PHYX_LANE0_TXDEBUG(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090408080ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PHYX_LANE0_TX_LBERT_CTL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_PHYX_LANE0_TX_LBERT_CTL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904080A8ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_PHYX_LANE0_TX_LBERT_CTL(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800904080A8ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PHYX_LANE0_TX_OVRD_IN_HI(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_PHYX_LANE0_TX_OVRD_IN_HI(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090408008ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_PHYX_LANE0_TX_OVRD_IN_HI(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090408008ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PHYX_LANE0_TX_OVRD_IN_LO(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_PHYX_LANE0_TX_OVRD_IN_LO(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090408000ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_PHYX_LANE0_TX_OVRD_IN_LO(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090408000ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PHYX_LANE1_LOOPBACK(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_PHYX_LANE1_LOOPBACK(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090408970ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_PHYX_LANE1_LOOPBACK(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090408970ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PHYX_LANE1_RX_LBERT_CTL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_PHYX_LANE1_RX_LBERT_CTL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904088B0ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_PHYX_LANE1_RX_LBERT_CTL(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800904088B0ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PHYX_LANE1_RX_LBERT_ERR(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_PHYX_LANE1_RX_LBERT_ERR(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904088B8ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_PHYX_LANE1_RX_LBERT_ERR(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800904088B8ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PHYX_LANE1_RX_OVRD_IN_LO(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_PHYX_LANE1_RX_OVRD_IN_LO(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090408828ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_PHYX_LANE1_RX_OVRD_IN_LO(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090408828ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PHYX_LANE1_TXDEBUG(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_PHYX_LANE1_TXDEBUG(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090408880ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_PHYX_LANE1_TXDEBUG(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090408880ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PHYX_LANE1_TX_LBERT_CTL(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_PHYX_LANE1_TX_LBERT_CTL(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904088A8ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_PHYX_LANE1_TX_LBERT_CTL(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800904088A8ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PHYX_LANE1_TX_OVRD_IN_HI(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_PHYX_LANE1_TX_OVRD_IN_HI(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090408808ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_PHYX_LANE1_TX_OVRD_IN_HI(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090408808ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PHYX_LANE1_TX_OVRD_IN_LO(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_PHYX_LANE1_TX_OVRD_IN_LO(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090408800ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_PHYX_LANE1_TX_OVRD_IN_LO(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090408800ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PHYX_OVRD_IN_LO(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && (((offset <= 2)) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_PHYX_OVRD_IN_LO(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090400088ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288;
+}
+#else
+#define CVMX_GSERX_PHYX_OVRD_IN_LO(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090400088ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 524288)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PHY_CTL(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_PHY_CTL(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090000000ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_PHY_CTL(block_id) (CVMX_ADD_IO_SEG(0x0001180090000000ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PIPE_LPBK(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_PIPE_LPBK(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090000200ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_PIPE_LPBK(block_id) (CVMX_ADD_IO_SEG(0x0001180090000200ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PLL_PX_MODE_0(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 11)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_PLL_PX_MODE_0(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904E0030ull) + (((offset) & 15) + ((block_id) & 15) * 0x80000ull) * 32;
+}
+#else
+#define CVMX_GSERX_PLL_PX_MODE_0(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800904E0030ull) + (((offset) & 15) + ((block_id) & 15) * 0x80000ull) * 32)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PLL_PX_MODE_1(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 11)) && ((block_id <= 13))))))
+		cvmx_warn("CVMX_GSERX_PLL_PX_MODE_1(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904E0038ull) + (((offset) & 15) + ((block_id) & 15) * 0x80000ull) * 32;
+}
+#else
+#define CVMX_GSERX_PLL_PX_MODE_1(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800904E0038ull) + (((offset) & 15) + ((block_id) & 15) * 0x80000ull) * 32)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_PLL_STAT(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_PLL_STAT(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090000010ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_PLL_STAT(block_id) (CVMX_ADD_IO_SEG(0x0001180090000010ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_QLM_STAT(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_QLM_STAT(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800900000A0ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_QLM_STAT(block_id) (CVMX_ADD_IO_SEG(0x00011800900000A0ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_RDET_TIME(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_RDET_TIME(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904E0008ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_RDET_TIME(block_id) (CVMX_ADD_IO_SEG(0x00011800904E0008ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_REFCLK_SEL(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_REFCLK_SEL(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090000008ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_REFCLK_SEL(block_id) (CVMX_ADD_IO_SEG(0x0001180090000008ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_RX_COAST(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_RX_COAST(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090000138ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_RX_COAST(block_id) (CVMX_ADD_IO_SEG(0x0001180090000138ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_RX_EIE_DETEN(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_RX_EIE_DETEN(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090000148ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_RX_EIE_DETEN(block_id) (CVMX_ADD_IO_SEG(0x0001180090000148ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_RX_EIE_DETSTS(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_RX_EIE_DETSTS(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090000150ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_RX_EIE_DETSTS(block_id) (CVMX_ADD_IO_SEG(0x0001180090000150ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_RX_EIE_FILTER(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_RX_EIE_FILTER(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090000158ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_RX_EIE_FILTER(block_id) (CVMX_ADD_IO_SEG(0x0001180090000158ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_RX_POLARITY(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_RX_POLARITY(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090000160ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_RX_POLARITY(block_id) (CVMX_ADD_IO_SEG(0x0001180090000160ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_RX_PWR_CTRL_P1(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_RX_PWR_CTRL_P1(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x00011800904600B0ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_RX_PWR_CTRL_P1(block_id) (CVMX_ADD_IO_SEG(0x00011800904600B0ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_SATA_CFG(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((block_id == 0)))))
+		cvmx_warn("CVMX_GSERX_SATA_CFG(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090100208ull);
+}
+#else
+#define CVMX_GSERX_SATA_CFG(block_id) (CVMX_ADD_IO_SEG(0x0001180090100208ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_SATA_LANE_RST(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((block_id == 0)))))
+		cvmx_warn("CVMX_GSERX_SATA_LANE_RST(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090100210ull);
+}
+#else
+#define CVMX_GSERX_SATA_LANE_RST(block_id) (CVMX_ADD_IO_SEG(0x0001180090100210ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_SATA_P0_TX_AMP_GENX(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((((offset >= 1) && (offset <= 3))) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_SATA_P0_TX_AMP_GENX(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090100480ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 8;
+}
+#else
+#define CVMX_GSERX_SATA_P0_TX_AMP_GENX(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090100480ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_SATA_P0_TX_PREEMPH_GENX(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((((offset >= 1) && (offset <= 3))) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_SATA_P0_TX_PREEMPH_GENX(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090100400ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 8;
+}
+#else
+#define CVMX_GSERX_SATA_P0_TX_PREEMPH_GENX(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090100400ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_SATA_P1_TX_AMP_GENX(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((((offset >= 1) && (offset <= 3))) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_SATA_P1_TX_AMP_GENX(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x00011800901004A0ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 8;
+}
+#else
+#define CVMX_GSERX_SATA_P1_TX_AMP_GENX(offset, block_id) (CVMX_ADD_IO_SEG(0x00011800901004A0ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_SATA_P1_TX_PREEMPH_GENX(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((((offset >= 1) && (offset <= 3))) && ((block_id == 0))))))
+		cvmx_warn("CVMX_GSERX_SATA_P1_TX_PREEMPH_GENX(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090100420ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 8;
+}
+#else
+#define CVMX_GSERX_SATA_P1_TX_PREEMPH_GENX(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180090100420ull) + (((offset) & 3) + ((block_id) & 0) * 0x0ull) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_SATA_REF_SSP_EN(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((block_id == 0)))))
+		cvmx_warn("CVMX_GSERX_SATA_REF_SSP_EN(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090100600ull);
+}
+#else
+#define CVMX_GSERX_SATA_REF_SSP_EN(block_id) (CVMX_ADD_IO_SEG(0x0001180090100600ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_SATA_RX_INVERT(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((block_id == 0)))))
+		cvmx_warn("CVMX_GSERX_SATA_RX_INVERT(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090100218ull);
+}
+#else
+#define CVMX_GSERX_SATA_RX_INVERT(block_id) (CVMX_ADD_IO_SEG(0x0001180090100218ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_SATA_SSC_CLK_SEL(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((block_id == 0)))))
+		cvmx_warn("CVMX_GSERX_SATA_SSC_CLK_SEL(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090100238ull);
+}
+#else
+#define CVMX_GSERX_SATA_SSC_CLK_SEL(block_id) (CVMX_ADD_IO_SEG(0x0001180090100238ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_SATA_SSC_EN(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((block_id == 0)))))
+		cvmx_warn("CVMX_GSERX_SATA_SSC_EN(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090100228ull);
+}
+#else
+#define CVMX_GSERX_SATA_SSC_EN(block_id) (CVMX_ADD_IO_SEG(0x0001180090100228ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_SATA_SSC_RANGE(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((block_id == 0)))))
+		cvmx_warn("CVMX_GSERX_SATA_SSC_RANGE(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090100230ull);
+}
+#else
+#define CVMX_GSERX_SATA_SSC_RANGE(block_id) (CVMX_ADD_IO_SEG(0x0001180090100230ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_SATA_STATUS(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((block_id == 0)))))
+		cvmx_warn("CVMX_GSERX_SATA_STATUS(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090100200ull);
+}
+#else
+#define CVMX_GSERX_SATA_STATUS(block_id) (CVMX_ADD_IO_SEG(0x0001180090100200ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_SATA_TX_INVERT(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN70XX) && ((block_id == 0)))))
+		cvmx_warn("CVMX_GSERX_SATA_TX_INVERT(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090100220ull);
+}
+#else
+#define CVMX_GSERX_SATA_TX_INVERT(block_id) (CVMX_ADD_IO_SEG(0x0001180090100220ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_SCRATCH(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_SCRATCH(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090000020ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_SCRATCH(block_id) (CVMX_ADD_IO_SEG(0x0001180090000020ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_SLICE_CFG(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_SLICE_CFG(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090460060ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_SLICE_CFG(block_id) (CVMX_ADD_IO_SEG(0x0001180090460060ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_SPD(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_SPD(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090000088ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_SPD(block_id) (CVMX_ADD_IO_SEG(0x0001180090000088ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_SRST(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_SRST(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090000090ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_SRST(block_id) (CVMX_ADD_IO_SEG(0x0001180090000090ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_GSERX_TX_VBOOST(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 13)))))
+		cvmx_warn("CVMX_GSERX_TX_VBOOST(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x0001180090000130ull) + ((block_id) & 15) * 0x1000000ull;
+}
+#else
+#define CVMX_GSERX_TX_VBOOST(block_id) (CVMX_ADD_IO_SEG(0x0001180090000130ull) + ((block_id) & 15) * 0x1000000ull)
+#endif
+
+/**
+ * cvmx_gser#_ana_atest
+ */
+union cvmx_gserx_ana_atest {
+	uint64_t u64;
+	struct cvmx_gserx_ana_atest_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_12_63               : 52;
+	uint64_t ana_dac_b                    : 7;  /**< Used to control the B-side DAC input to the analog test block. Note that the QLM4 register
+                                                         is tied to the analog test block, for non-OCI links. Note that the OCI4 register is tied
+                                                         to the analog test block, for OCI links. The other GSER()_ANA_ATEST registers are
+                                                         unused. For diagnostic use only. */
+	uint64_t ana_dac_a                    : 5;  /**< Used to control A-side DAC input to the analog test block. Note that the QLM4 register is
+                                                         tied to the analog test block, for non-OCI links. Note that the OCI4 register is tied to
+                                                         the analog test block, for OCI links. The other GSER()_ANA_ATEST registers are unused.
+                                                         For diagnostic use only. */
+#else
+	uint64_t ana_dac_a                    : 5;
+	uint64_t ana_dac_b                    : 7;
+	uint64_t reserved_12_63               : 52;
+#endif
+	} s;
+	struct cvmx_gserx_ana_atest_s         cn78xx;
+};
+typedef union cvmx_gserx_ana_atest cvmx_gserx_ana_atest_t;
+
+/**
+ * cvmx_gser#_ana_sel
+ */
+union cvmx_gserx_ana_sel {
+	uint64_t u64;
+	struct cvmx_gserx_ana_sel_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t ana_sel                      : 9;  /**< Used to control the adr_global input to the analog test block. Note that the QLM0 register
+                                                         is tied to the analog test block, for non-OCI links. Note that the QLM8 register is tied
+                                                         to the analog test block, for OCI links. The other GSER()_ANA_SEL registers are unused.
+                                                         For diagnostic use only. */
+#else
+	uint64_t ana_sel                      : 9;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_gserx_ana_sel_s           cn78xx;
+};
+typedef union cvmx_gserx_ana_sel cvmx_gserx_ana_sel_t;
+
+/**
+ * cvmx_gser#_br_rx#_ctl
+ */
+union cvmx_gserx_br_rxx_ctl {
+	uint64_t u64;
+	struct cvmx_gserx_br_rxx_ctl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_3_63                : 61;
+	uint64_t rxt_swm                      : 1;  /**< Set when RX Base-R Link Training is to be performed under software control. For diagnostic
+                                                         use only. */
+	uint64_t rxt_preset                   : 1;  /**< For all link training, this bit determines how to configure the preset bit in the
+                                                         coefficient update message that is sent to the far end transmitter. When set, a one time
+                                                         request is made that the coefficients be set to a state where equalization is turned off.
+                                                         To perform a preset, set this bit prior to link training. Link training needs to be
+                                                         disabled to complete the request and get the rxtrain state machine back to idle. Note that
+                                                         it is illegal to set both the preset and initialize bits at the same time. For diagnostic
+                                                         use only. */
+	uint64_t rxt_initialize               : 1;  /**< For all link training, this bit determines how to configure the initialize bit in the
+                                                         coefficient update message that is sent to the far end transmitter of RX training. When
+                                                         set, a request is made that the coefficients be set to its INITIALIZE state. To perform an
+                                                         initialize prior to link training, set this bit prior to performing link training. Note
+                                                         that it is illegal to set both the preset and initialize bits at the same time. Since the
+                                                         far end transmitter is required to be initialized prior to starting link training, it is
+                                                         not expected that software will need to set this bit. For diagnostic use only. */
+#else
+	uint64_t rxt_initialize               : 1;
+	uint64_t rxt_preset                   : 1;
+	uint64_t rxt_swm                      : 1;
+	uint64_t reserved_3_63                : 61;
+#endif
+	} s;
+	struct cvmx_gserx_br_rxx_ctl_s        cn78xx;
+};
+typedef union cvmx_gserx_br_rxx_ctl cvmx_gserx_br_rxx_ctl_t;
+
+/**
+ * cvmx_gser#_br_rx#_eer
+ *
+ * GSER software Base-R RX Link Training equalization evaluation request (EER). A write to
+ * RXT_EER initiates a equalization request to the RAW PCS. A read of this register returns the
+ * equalization status message and a valid bit indicating it was updated. These registers are for
+ * diagnostic use only.
+ */
+union cvmx_gserx_br_rxx_eer {
+	uint64_t u64;
+	struct cvmx_gserx_br_rxx_eer_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t rxt_eer                      : 1;  /**< When RX Base-R Link Training is being performed under software control,
+                                                         (GSER()_BR_RX()_CTL[RXT_SWM] is set), writing this bit initiates an equalization
+                                                         request to the RAW PCS. Reading this bit always returns a zero. */
+	uint64_t rxt_esv                      : 1;  /**< When performing an equalization request (RXT_EER), this bit, when set, indicates that the
+                                                         Equalization Status (RXT_ESM) is valid. When issuing a RXT_EER request, it is expected
+                                                         that RXT_ESV will get written to zero so that a valid RXT_ESM can be determined. */
+	uint64_t rxt_esm                      : 14; /**< When performing an equalization request (RXT_EER), this is the equalization status message
+                                                         from the RAW PCS. It is valid when RXT_ESV is set.
+                                                         _ <13:6>: Figure of merit. An 8-bit output from the PHY indicating the quality of the
+                                                         received data eye. A higher value indicates better link equalization, with 8'd0 indicating
+                                                         worst equalization setting and 8'd255 indicating the best equalization setting.
+                                                         _ <5:4>: RX recommended TXPOST direction change.
+                                                         _ <3:2>: RX recommended TXMAIN direction change.
+                                                         _ <1:0>: RX recommended TXPRE direction change.
+                                                         Recommended direction change outputs from the PHY for the link partner transmitter
+                                                         coefficients.
+                                                         0x0 = Hold.
+                                                         0x1 = Increment.
+                                                         0x2 = Decrement.
+                                                         0x3 = Hold. */
+#else
+	uint64_t rxt_esm                      : 14;
+	uint64_t rxt_esv                      : 1;
+	uint64_t rxt_eer                      : 1;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_gserx_br_rxx_eer_s        cn78xx;
+};
+typedef union cvmx_gserx_br_rxx_eer cvmx_gserx_br_rxx_eer_t;
+
+/**
+ * cvmx_gser#_br_tx#_ctl
+ */
+union cvmx_gserx_br_txx_ctl {
+	uint64_t u64;
+	struct cvmx_gserx_br_txx_ctl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t txt_swm                      : 1;  /**< Set when TX Base-R Link Training is to be performed under software control. For diagnostic
+                                                         use only. */
+#else
+	uint64_t txt_swm                      : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_gserx_br_txx_ctl_s        cn78xx;
+};
+typedef union cvmx_gserx_br_txx_ctl cvmx_gserx_br_txx_ctl_t;
+
+/**
+ * cvmx_gser#_br_tx#_cur
+ */
+union cvmx_gserx_br_txx_cur {
+	uint64_t u64;
+	struct cvmx_gserx_br_txx_cur_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_14_63               : 50;
+	uint64_t txt_cur                      : 14; /**< When TX Base-R Link Training is being performed under software control,
+                                                         (GSER()_BR_TX()_CTL.TXT_SWM is set), this is the Coefficient Update to be written to the
+                                                         PHY.
+                                                         For diagnostic use only.
+                                                         <13:9> = TX_POST<4:0>.
+                                                         <8:4> = TX_SWING<4:0>.
+                                                         <3:0> = TX_PRE<4:0>. */
+#else
+	uint64_t txt_cur                      : 14;
+	uint64_t reserved_14_63               : 50;
+#endif
+	} s;
+	struct cvmx_gserx_br_txx_cur_s        cn78xx;
+};
+typedef union cvmx_gserx_br_txx_cur cvmx_gserx_br_txx_cur_t;
+
+/**
+ * cvmx_gser#_cfg
+ */
+union cvmx_gserx_cfg {
+	uint64_t u64;
+	struct cvmx_gserx_cfg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_5_63                : 59;
+	uint64_t bgx_quad                     : 1;  /**< For non-OCI links, indicates the BGX is in quad aggregation mode when GSER()_CFG[BGX]
+                                                         is also set. A single controller is used for all four lanes. For OCI links, this bit has
+                                                         no meaning. */
+	uint64_t bgx_dual                     : 1;  /**< For non-OCI links, indicates the BGX is in dual aggregation mode when GSER()_CFG[BGX]
+                                                         is also set. A single controller is used for lanes 0 and 1 and another controller is used
+                                                         for lanes 2 and 3. For OCI links, this bit has no meaning. */
+	uint64_t bgx                          : 1;  /**< For non-OCI links, indicates the GSER is configured for BGX mode. Only one of the BGX,
+                                                         ILA, or PCIE modes can be set at any one time. For OCI links, this bit has no meaning. */
+	uint64_t ila                          : 1;  /**< For non-OCI links, indicates the GSER is configured for ILK/ILA mode. For OCI links this
+                                                         bit will be set. Only one of the BGX, ILA, or PCIE modes can be set at any one time. For
+                                                         OCI links, this bit has no meaning. */
+	uint64_t pcie                         : 1;  /**< For non-OCI links, indicates the GSER is configured for PCIE mode. Only one of the BGX,
+                                                         ILA, or PCIE modes can be set at any one time. For OCI links, this bit has no meaning. */
+#else
+	uint64_t pcie                         : 1;
+	uint64_t ila                          : 1;
+	uint64_t bgx                          : 1;
+	uint64_t bgx_dual                     : 1;
+	uint64_t bgx_quad                     : 1;
+	uint64_t reserved_5_63                : 59;
+#endif
+	} s;
+	struct cvmx_gserx_cfg_s               cn78xx;
+};
+typedef union cvmx_gserx_cfg cvmx_gserx_cfg_t;
+
+/**
+ * cvmx_gser#_dbg
+ */
+union cvmx_gserx_dbg {
+	uint64_t u64;
+	struct cvmx_gserx_dbg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t rxqtm_on                     : 1;  /**< For non-BGX/ILK configurations, setting this bit enables the RX FIFOs. This allows
+                                                         received data to become visible to the RSL debug port. For diagnostic use only. */
+#else
+	uint64_t rxqtm_on                     : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_gserx_dbg_s               cn78xx;
+};
+typedef union cvmx_gserx_dbg cvmx_gserx_dbg_t;
+
+/**
+ * cvmx_gser#_dlm#_loopbk_en
+ *
+ * DLM0 Tx-to-Rx Loopback Enable.
+ *
+ */
+union cvmx_gserx_dlmx_loopbk_en {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_loopbk_en_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t lane1_loopbk_en              : 1;  /**< Lane 1 Tx-to-Rx Loopback Enable.  When this signal is
+                                                         asserted, data from the transmit predriver is looped back
+                                                         to the receive slivers.  LOS is bypassed and based on the
+                                                         txN_en input so that rxN_los = !txN_data_en. */
+	uint64_t reserved_1_7                 : 7;
+	uint64_t lane0_loopbk_en              : 1;  /**< Lane 0 Tx-to-Rx Loopback Enable.  When this signal is
+                                                         asserted, data from the transmit predriver is looped back
+                                                         to the receive slivers.  LOS is bypassed and based on the
+                                                         txN_en input so that rxN_los = !txN_data_en. */
+#else
+	uint64_t lane0_loopbk_en              : 1;
+	uint64_t reserved_1_7                 : 7;
+	uint64_t lane1_loopbk_en              : 1;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_loopbk_en_s    cn70xx;
+	struct cvmx_gserx_dlmx_loopbk_en_s    cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_loopbk_en cvmx_gserx_dlmx_loopbk_en_t;
+
+/**
+ * cvmx_gser#_dlm#_los_bias
+ *
+ * DLM Loss-of-Signal Detector Threshold Level Control.
+ *
+ */
+union cvmx_gserx_dlmx_los_bias {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_los_bias_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_3_63                : 61;
+	uint64_t los_bias                     : 3;  /**< A positive, binary bit setting change results in a
+                                                         +15mVp incremental change in the LOS threshold.  A negative
+                                                         bit setting change results in a -15-mVp incremental change
+                                                         in the LOS threshold.  The 3'b000 setting is reserved and
+                                                         must not be used.
+                                                         0x0: Reserved
+                                                         0x1: 120 mV (default CEI)
+                                                         0x2: 135 mV (default PCIe/SATA)
+                                                         0x3: 150 mV
+                                                         0x4:  45 mV
+                                                         0x5:  60 mV
+                                                         0x6:  75 mV
+                                                         0x7:  90 mV */
+#else
+	uint64_t los_bias                     : 3;
+	uint64_t reserved_3_63                : 61;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_los_bias_s     cn70xx;
+	struct cvmx_gserx_dlmx_los_bias_s     cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_los_bias cvmx_gserx_dlmx_los_bias_t;
+
+/**
+ * cvmx_gser#_dlm#_los_level
+ *
+ * DLM Loss-of-Signal Sensitivity Level Contol.
+ *
+ */
+union cvmx_gserx_dlmx_los_level {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_los_level_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_5_63                : 59;
+	uint64_t los_level                    : 5;  /**< Sets the sesitivity level for the Loss-of-Signal
+                                                         detector.  This signal must be set to 5'b01001. */
+#else
+	uint64_t los_level                    : 5;
+	uint64_t reserved_5_63                : 59;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_los_level_s    cn70xx;
+	struct cvmx_gserx_dlmx_los_level_s    cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_los_level cvmx_gserx_dlmx_los_level_t;
+
+/**
+ * cvmx_gser#_dlm#_misc_status
+ *
+ * DLM0 Miscellaneous Status.
+ *
+ */
+union cvmx_gserx_dlmx_misc_status {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_misc_status_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t tx1_uflow                    : 1;  /**< When set, indicates transmit FIFO underflow
+                                                         has occured on lane 1. */
+	uint64_t reserved_1_7                 : 7;
+	uint64_t tx0_uflow                    : 1;  /**< When set, indicates transmit FIFO underflow
+                                                         has occured on lane 0. */
+#else
+	uint64_t tx0_uflow                    : 1;
+	uint64_t reserved_1_7                 : 7;
+	uint64_t tx1_uflow                    : 1;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_misc_status_s  cn70xx;
+	struct cvmx_gserx_dlmx_misc_status_s  cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_misc_status cvmx_gserx_dlmx_misc_status_t;
+
+/**
+ * cvmx_gser#_dlm#_mpll_en
+ *
+ * DLM0 PHY PLL Enable.
+ *
+ */
+union cvmx_gserx_dlmx_mpll_en {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_mpll_en_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t mpll_en                      : 1;  /**< When deasserted, the MPLL is off and the PHY is in P2 state. */
+#else
+	uint64_t mpll_en                      : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_mpll_en_s      cn70xx;
+	struct cvmx_gserx_dlmx_mpll_en_s      cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_mpll_en cvmx_gserx_dlmx_mpll_en_t;
+
+/**
+ * cvmx_gser#_dlm#_mpll_half_rate
+ *
+ * DLM MPLL Low-Power Mode Enable.
+ *
+ */
+union cvmx_gserx_dlmx_mpll_half_rate {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_mpll_half_rate_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t mpll_half_rate               : 1;  /**< Enables a low-power mode feature for the MPLL block.  This signal
+                                                         should be asserted only when the MPLL is operating at a clock rate
+                                                         less than or equal to 1.5626 GHz. */
+#else
+	uint64_t mpll_half_rate               : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_mpll_half_rate_s cn70xx;
+	struct cvmx_gserx_dlmx_mpll_half_rate_s cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_mpll_half_rate cvmx_gserx_dlmx_mpll_half_rate_t;
+
+/**
+ * cvmx_gser#_dlm#_mpll_multiplier
+ *
+ * DLM MPLL Frequency Multiplier Control.
+ *
+ */
+union cvmx_gserx_dlmx_mpll_multiplier {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_mpll_multiplier_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_7_63                : 57;
+	uint64_t mpll_multiplier              : 7;  /**< Multiples the reference clock to a frequency suitable for
+                                                         intended operating speed. */
+#else
+	uint64_t mpll_multiplier              : 7;
+	uint64_t reserved_7_63                : 57;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_mpll_multiplier_s cn70xx;
+	struct cvmx_gserx_dlmx_mpll_multiplier_s cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_mpll_multiplier cvmx_gserx_dlmx_mpll_multiplier_t;
+
+/**
+ * cvmx_gser#_dlm#_mpll_status
+ *
+ * DLM PLL Lock Status.
+ *
+ */
+union cvmx_gserx_dlmx_mpll_status {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_mpll_status_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t mpll_status                  : 1;  /**< This is the lock status of the PHY PLL.  When asserted,
+                                                         it indicates the PHY's MPLL has reached a stable, running
+                                                         state. */
+#else
+	uint64_t mpll_status                  : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_mpll_status_s  cn70xx;
+	struct cvmx_gserx_dlmx_mpll_status_s  cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_mpll_status cvmx_gserx_dlmx_mpll_status_t;
+
+/**
+ * cvmx_gser#_dlm#_phy_reset
+ *
+ * DLM Core and State Machine Reset.
+ *
+ */
+union cvmx_gserx_dlmx_phy_reset {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_phy_reset_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t phy_reset                    : 1;  /**< Resets the core and all state machines with the exception of the
+                                                         reference clock buffer and JTAG interface.  Asserting PHY_RESET
+                                                         triggers the assertion of teh Tx and Rx reset signals.  Power
+                                                         and clocks are required before deasserting PHY_RESET. */
+#else
+	uint64_t phy_reset                    : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_phy_reset_s    cn70xx;
+	struct cvmx_gserx_dlmx_phy_reset_s    cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_phy_reset cvmx_gserx_dlmx_phy_reset_t;
+
+/**
+ * cvmx_gser#_dlm#_ref_clkdiv2
+ *
+ * DLM Input Reference Clock Divider Control.
+ *
+ */
+union cvmx_gserx_dlmx_ref_clkdiv2 {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_ref_clkdiv2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t ref_clkdiv2                  : 1;  /**< If the input reference clock is greater than 100Mhz, this signal must
+                                                         be asserted.  The reference clock frequency is then divided by 2 to
+                                                         keep it in the range required by the MPLL. */
+#else
+	uint64_t ref_clkdiv2                  : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_ref_clkdiv2_s  cn70xx;
+	struct cvmx_gserx_dlmx_ref_clkdiv2_s  cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_ref_clkdiv2 cvmx_gserx_dlmx_ref_clkdiv2_t;
+
+/**
+ * cvmx_gser#_dlm#_ref_ssp_en
+ *
+ * DLM0 Reference Clock Enable for the PHY.
+ *
+ */
+union cvmx_gserx_dlmx_ref_ssp_en {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_ref_ssp_en_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t ref_ssp_en                   : 1;  /**< Enables the PHY's internal reference clock. */
+#else
+	uint64_t ref_ssp_en                   : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_ref_ssp_en_s   cn70xx;
+	struct cvmx_gserx_dlmx_ref_ssp_en_s   cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_ref_ssp_en cvmx_gserx_dlmx_ref_ssp_en_t;
+
+/**
+ * cvmx_gser#_dlm#_ref_use_pad
+ *
+ * DLM Select Reference Clock.
+ *
+ */
+union cvmx_gserx_dlmx_ref_use_pad {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_ref_use_pad_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t ref_use_pad                  : 1;  /**< When asserted, selects the external ref_pad_clk_[p,m]
+                                                         inputs as the reference clock sourse.  When deasserted,
+                                                         ref_alt_clk_[p,m] are selected from an on-chip
+                                                         source of the reference clock. REF_USE_PAD must be
+                                                         clear for DLM1 and DLM2. */
+#else
+	uint64_t ref_use_pad                  : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_ref_use_pad_s  cn70xx;
+	struct cvmx_gserx_dlmx_ref_use_pad_s  cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_ref_use_pad cvmx_gserx_dlmx_ref_use_pad_t;
+
+/**
+ * cvmx_gser#_dlm#_refclk_sel
+ *
+ * DLM Reference Clock Select.
+ *
+ */
+union cvmx_gserx_dlmx_refclk_sel {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_refclk_sel_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t refclk_sel                   : 1;  /**< When clear, selects common reference clock 0.
+                                                         When set, selects common reference clock 1.
+                                                         GSER0_DLMn_REF_USE_PAD[REF_USE_PAD] must be clear
+                                                         to select either common reference clock. */
+#else
+	uint64_t refclk_sel                   : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_refclk_sel_s   cn70xx;
+	struct cvmx_gserx_dlmx_refclk_sel_s   cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_refclk_sel cvmx_gserx_dlmx_refclk_sel_t;
+
+/**
+ * cvmx_gser#_dlm#_rx_data_en
+ *
+ * DLM Receiver Enable.
+ *
+ */
+union cvmx_gserx_dlmx_rx_data_en {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_rx_data_en_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t rx1_data_en                  : 1;  /**< Enables the clock and data recovery logic fir Lane 1. */
+	uint64_t reserved_1_7                 : 7;
+	uint64_t rx0_data_en                  : 1;  /**< Enables the clock and data recovery logic for Lane 0. */
+#else
+	uint64_t rx0_data_en                  : 1;
+	uint64_t reserved_1_7                 : 7;
+	uint64_t rx1_data_en                  : 1;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_rx_data_en_s   cn70xx;
+	struct cvmx_gserx_dlmx_rx_data_en_s   cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_rx_data_en cvmx_gserx_dlmx_rx_data_en_t;
+
+/**
+ * cvmx_gser#_dlm#_rx_eq
+ *
+ * DLM Receiver Equalization Setting.
+ *
+ */
+union cvmx_gserx_dlmx_rx_eq {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_rx_eq_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_11_63               : 53;
+	uint64_t rx1_eq                       : 3;  /**< Selects the amount of equalization in the Lane 1 receiver. */
+	uint64_t reserved_3_7                 : 5;
+	uint64_t rx0_eq                       : 3;  /**< Selects the amount of equalization in the Lane 0 receiver. */
+#else
+	uint64_t rx0_eq                       : 3;
+	uint64_t reserved_3_7                 : 5;
+	uint64_t rx1_eq                       : 3;
+	uint64_t reserved_11_63               : 53;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_rx_eq_s        cn70xx;
+	struct cvmx_gserx_dlmx_rx_eq_s        cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_rx_eq cvmx_gserx_dlmx_rx_eq_t;
+
+/**
+ * cvmx_gser#_dlm#_rx_los_en
+ *
+ * DLM Loss of Signal Detector Enable.
+ *
+ */
+union cvmx_gserx_dlmx_rx_los_en {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_rx_los_en_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t rx1_los_en                   : 1;  /**< Lane 1 Loss of Signal Detector Enable. */
+	uint64_t reserved_1_7                 : 7;
+	uint64_t rx0_los_en                   : 1;  /**< Lane 0 Loss of Signal Detector Enable. */
+#else
+	uint64_t rx0_los_en                   : 1;
+	uint64_t reserved_1_7                 : 7;
+	uint64_t rx1_los_en                   : 1;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_rx_los_en_s    cn70xx;
+	struct cvmx_gserx_dlmx_rx_los_en_s    cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_rx_los_en cvmx_gserx_dlmx_rx_los_en_t;
+
+/**
+ * cvmx_gser#_dlm#_rx_pll_en
+ *
+ * DLM0 DPLL Enable.
+ *
+ */
+union cvmx_gserx_dlmx_rx_pll_en {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_rx_pll_en_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t rx1_pll_en                   : 1;  /**< Lane 1 Receiver DPLL Enable. */
+	uint64_t reserved_1_7                 : 7;
+	uint64_t rx0_pll_en                   : 1;  /**< Lane 0 Receiver DPLL Enable. */
+#else
+	uint64_t rx0_pll_en                   : 1;
+	uint64_t reserved_1_7                 : 7;
+	uint64_t rx1_pll_en                   : 1;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_rx_pll_en_s    cn70xx;
+	struct cvmx_gserx_dlmx_rx_pll_en_s    cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_rx_pll_en cvmx_gserx_dlmx_rx_pll_en_t;
+
+/**
+ * cvmx_gser#_dlm#_rx_rate
+ *
+ * DLM0 Rx Data Rate.
+ *
+ */
+union cvmx_gserx_dlmx_rx_rate {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_rx_rate_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_10_63               : 54;
+	uint64_t rx1_rate                     : 2;  /**< Lane 1 Rx Data Rate
+                                                         - 00: mpll_baud_clk
+                                                         - 01: mpll_baud_clk / 2
+                                                         - 10: mpll_baud_clk / 4
+                                                         - 11: Not Supported */
+	uint64_t reserved_2_7                 : 6;
+	uint64_t rx0_rate                     : 2;  /**< Lane 0 Rx Data Rate
+                                                         - 00: mpll_baud_clk
+                                                         - 01: mpll_baud_clk / 2
+                                                         - 10: mpll_baud_clk / 4
+                                                         - 11: Not Supported */
+#else
+	uint64_t rx0_rate                     : 2;
+	uint64_t reserved_2_7                 : 6;
+	uint64_t rx1_rate                     : 2;
+	uint64_t reserved_10_63               : 54;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_rx_rate_s      cn70xx;
+	struct cvmx_gserx_dlmx_rx_rate_s      cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_rx_rate cvmx_gserx_dlmx_rx_rate_t;
+
+/**
+ * cvmx_gser#_dlm#_rx_reset
+ *
+ * DLM0 Receiver Reset.
+ *
+ */
+union cvmx_gserx_dlmx_rx_reset {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_rx_reset_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t rx1_reset                    : 1;  /**< Lane 1 Receiver Reset. */
+	uint64_t reserved_1_7                 : 7;
+	uint64_t rx0_reset                    : 1;  /**< Lane 0 Receiver Reset. */
+#else
+	uint64_t rx0_reset                    : 1;
+	uint64_t reserved_1_7                 : 7;
+	uint64_t rx1_reset                    : 1;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_rx_reset_s     cn70xx;
+	struct cvmx_gserx_dlmx_rx_reset_s     cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_rx_reset cvmx_gserx_dlmx_rx_reset_t;
+
+/**
+ * cvmx_gser#_dlm#_rx_status
+ *
+ * DLM Receive DPLL State Indicator.
+ *
+ */
+union cvmx_gserx_dlmx_rx_status {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_rx_status_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t rx1_status                   : 1;  /**< Indicates the current state of the Lane 1 receiver DPLL and clock.
+                                                         When cleared, rxN_clk can be disabled or not running at its
+                                                         target rate. */
+	uint64_t reserved_1_7                 : 7;
+	uint64_t rx0_status                   : 1;  /**< Indicates the current state of the Lane 0 receiver DPLL and clock.
+                                                         When cleared, rxN_clk can be disabled or not running at its
+                                                         target rate. */
+#else
+	uint64_t rx0_status                   : 1;
+	uint64_t reserved_1_7                 : 7;
+	uint64_t rx1_status                   : 1;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_rx_status_s    cn70xx;
+	struct cvmx_gserx_dlmx_rx_status_s    cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_rx_status cvmx_gserx_dlmx_rx_status_t;
+
+/**
+ * cvmx_gser#_dlm#_rx_term_en
+ *
+ * DLM0 PMA Receiver Termination.
+ *
+ */
+union cvmx_gserx_dlmx_rx_term_en {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_rx_term_en_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t rx1_term_en                  : 1;  /**< Lane 1 PMA Receiver Termination.
+                                                         - 0: Terminations removed
+                                                         - 1: Terminations present */
+	uint64_t reserved_1_7                 : 7;
+	uint64_t rx0_term_en                  : 1;  /**< Lane 0 PMA Receiver Termination.
+                                                         - 0: Terminations removed
+                                                         - 1: Terminations present */
+#else
+	uint64_t rx0_term_en                  : 1;
+	uint64_t reserved_1_7                 : 7;
+	uint64_t rx1_term_en                  : 1;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_rx_term_en_s   cn70xx;
+	struct cvmx_gserx_dlmx_rx_term_en_s   cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_rx_term_en cvmx_gserx_dlmx_rx_term_en_t;
+
+/**
+ * cvmx_gser#_dlm#_test_bypass
+ *
+ * DLM Test Bypass.
+ *
+ */
+union cvmx_gserx_dlmx_test_bypass {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_test_bypass_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t test_bypass                  : 1;  /**< When asserted, all circuits Power-Down but leave Reference Clock
+                                                         Active. */
+#else
+	uint64_t test_bypass                  : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_test_bypass_s  cn70xx;
+	struct cvmx_gserx_dlmx_test_bypass_s  cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_test_bypass cvmx_gserx_dlmx_test_bypass_t;
+
+/**
+ * cvmx_gser#_dlm#_test_powerdown
+ *
+ * DLM Test Powerdown.
+ *
+ */
+union cvmx_gserx_dlmx_test_powerdown {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_test_powerdown_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t test_powerdown               : 1;  /**< When asserted, Powers down all circuitry in the PHY for IDDQ testing. */
+#else
+	uint64_t test_powerdown               : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_test_powerdown_s cn70xx;
+	struct cvmx_gserx_dlmx_test_powerdown_s cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_test_powerdown cvmx_gserx_dlmx_test_powerdown_t;
+
+/**
+ * cvmx_gser#_dlm#_tx_amplitude
+ *
+ * DLM0 Tx Amplitude (Full Swing Mode).
+ *
+ */
+union cvmx_gserx_dlmx_tx_amplitude {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_tx_amplitude_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_15_63               : 49;
+	uint64_t tx1_amplitude                : 7;  /**< This static value sets the lanuch amplitude of the Lane 1 transmitter
+                                                         when pipeP_tx_swing is set to 0x7f (default state). */
+	uint64_t reserved_7_7                 : 1;
+	uint64_t tx0_amplitude                : 7;  /**< This static value sets the lanuch amplitude of the Lane 0 transmitter
+                                                         when pipeP_tx_swing is set to 0x7f (default state). */
+#else
+	uint64_t tx0_amplitude                : 7;
+	uint64_t reserved_7_7                 : 1;
+	uint64_t tx1_amplitude                : 7;
+	uint64_t reserved_15_63               : 49;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_tx_amplitude_s cn70xx;
+	struct cvmx_gserx_dlmx_tx_amplitude_s cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_tx_amplitude cvmx_gserx_dlmx_tx_amplitude_t;
+
+/**
+ * cvmx_gser#_dlm#_tx_cm_en
+ *
+ * DLM0 Transmit Common-Mode Control Enable.
+ *
+ */
+union cvmx_gserx_dlmx_tx_cm_en {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_tx_cm_en_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t tx1_cm_en                    : 1;  /**< Enables the Lane 1 transmitter's common mode hold circuitry. */
+	uint64_t reserved_1_7                 : 7;
+	uint64_t tx0_cm_en                    : 1;  /**< Enables the lane 0 transmitter's common mode hold circuitry. */
+#else
+	uint64_t tx0_cm_en                    : 1;
+	uint64_t reserved_1_7                 : 7;
+	uint64_t tx1_cm_en                    : 1;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_tx_cm_en_s     cn70xx;
+	struct cvmx_gserx_dlmx_tx_cm_en_s     cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_tx_cm_en cvmx_gserx_dlmx_tx_cm_en_t;
+
+/**
+ * cvmx_gser#_dlm#_tx_data_en
+ *
+ * DLM0 Transmit Driver Enable.
+ *
+ */
+union cvmx_gserx_dlmx_tx_data_en {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_tx_data_en_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t tx1_data_en                  : 1;  /**< Enables the Lane 1 primary transmitter driver for serial data. */
+	uint64_t reserved_1_7                 : 7;
+	uint64_t tx0_data_en                  : 1;  /**< Enables the Lane 0 primary transmitter driver for serial data. */
+#else
+	uint64_t tx0_data_en                  : 1;
+	uint64_t reserved_1_7                 : 7;
+	uint64_t tx1_data_en                  : 1;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_tx_data_en_s   cn70xx;
+	struct cvmx_gserx_dlmx_tx_data_en_s   cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_tx_data_en cvmx_gserx_dlmx_tx_data_en_t;
+
+/**
+ * cvmx_gser#_dlm#_tx_en
+ *
+ * DLM Transmit Clocking and Data Sampling Enable.
+ *
+ */
+union cvmx_gserx_dlmx_tx_en {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_tx_en_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t tx1_en                       : 1;  /**< Enables the Lane 1 transmit clock path and Tx word alignment. */
+	uint64_t reserved_1_7                 : 7;
+	uint64_t tx0_en                       : 1;  /**< Enables the Lane 0 transmit clock path and Tx word alignment. */
+#else
+	uint64_t tx0_en                       : 1;
+	uint64_t reserved_1_7                 : 7;
+	uint64_t tx1_en                       : 1;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_tx_en_s        cn70xx;
+	struct cvmx_gserx_dlmx_tx_en_s        cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_tx_en cvmx_gserx_dlmx_tx_en_t;
+
+/**
+ * cvmx_gser#_dlm#_tx_preemph
+ *
+ * DLM0 Tx Deemphasis.
+ *
+ */
+union cvmx_gserx_dlmx_tx_preemph {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_tx_preemph_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_15_63               : 49;
+	uint64_t tx1_preemph                  : 7;  /**< Sets the Lane 1 Tx driver de-emphasis value to meet the Tx eye mask. */
+	uint64_t reserved_7_7                 : 1;
+	uint64_t tx0_preemph                  : 7;  /**< Sets the Lane 0 Tx driver de-emphasis value to meet the Tx eye mask. */
+#else
+	uint64_t tx0_preemph                  : 7;
+	uint64_t reserved_7_7                 : 1;
+	uint64_t tx1_preemph                  : 7;
+	uint64_t reserved_15_63               : 49;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_tx_preemph_s   cn70xx;
+	struct cvmx_gserx_dlmx_tx_preemph_s   cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_tx_preemph cvmx_gserx_dlmx_tx_preemph_t;
+
+/**
+ * cvmx_gser#_dlm#_tx_rate
+ *
+ * DLM0 Tx Data Rate.
+ *
+ */
+union cvmx_gserx_dlmx_tx_rate {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_tx_rate_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_10_63               : 54;
+	uint64_t tx1_rate                     : 2;  /**< Selects the Lane 1 baud rate for the transmit path.
+                                                         - 00: baud
+                                                         - 01: baud / 2
+                                                         - 10: baud / 4
+                                                         - 11: Not supported */
+	uint64_t reserved_2_7                 : 6;
+	uint64_t tx0_rate                     : 2;  /**< Selects the Lane 0 baud rate for the transmit path.
+                                                         - 00: baud
+                                                         - 01: baud / 2
+                                                         - 10: baud / 4
+                                                         - 11: Not supported */
+#else
+	uint64_t tx0_rate                     : 2;
+	uint64_t reserved_2_7                 : 6;
+	uint64_t tx1_rate                     : 2;
+	uint64_t reserved_10_63               : 54;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_tx_rate_s      cn70xx;
+	struct cvmx_gserx_dlmx_tx_rate_s      cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_tx_rate cvmx_gserx_dlmx_tx_rate_t;
+
+/**
+ * cvmx_gser#_dlm#_tx_reset
+ *
+ * DLM0 Tx Reset.
+ *
+ */
+union cvmx_gserx_dlmx_tx_reset {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_tx_reset_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t tx1_reset                    : 1;  /**< Resets all Lane 1 transmitter settings and state machines. */
+	uint64_t reserved_1_7                 : 7;
+	uint64_t tx0_reset                    : 1;  /**< Resets all Lane 0 transmitter settings and state machines. */
+#else
+	uint64_t tx0_reset                    : 1;
+	uint64_t reserved_1_7                 : 7;
+	uint64_t tx1_reset                    : 1;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_tx_reset_s     cn70xx;
+	struct cvmx_gserx_dlmx_tx_reset_s     cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_tx_reset cvmx_gserx_dlmx_tx_reset_t;
+
+/**
+ * cvmx_gser#_dlm#_tx_status
+ *
+ * DLM Transmit Common Mode Control Status.
+ *
+ */
+union cvmx_gserx_dlmx_tx_status {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_tx_status_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_10_63               : 54;
+	uint64_t tx1_cm_status                : 1;  /**< When asserted, the Lane 1 transmitter differential pair is held to half
+                                                         of vptxN durring an electrical IDLE.  Otherwise, weakly held to
+                                                         ground through a high impedance connection. */
+	uint64_t tx1_status                   : 1;  /**< Signals when the Lane 1 transmitter is ready to properly sample the
+                                                         incoming data for transmission. */
+	uint64_t reserved_2_7                 : 6;
+	uint64_t tx0_cm_status                : 1;  /**< When asserted, the Lane 0 transmitter differential pair is held to half
+                                                         of vptxN durring an electrical IDLE.  Otherwise, weakly held to
+                                                         ground through a high impedance connection. */
+	uint64_t tx0_status                   : 1;  /**< Signals when the Lane 0 transmitter is ready to properly sample the
+                                                         incoming data for transmission. */
+#else
+	uint64_t tx0_status                   : 1;
+	uint64_t tx0_cm_status                : 1;
+	uint64_t reserved_2_7                 : 6;
+	uint64_t tx1_status                   : 1;
+	uint64_t tx1_cm_status                : 1;
+	uint64_t reserved_10_63               : 54;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_tx_status_s    cn70xx;
+	struct cvmx_gserx_dlmx_tx_status_s    cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_tx_status cvmx_gserx_dlmx_tx_status_t;
+
+/**
+ * cvmx_gser#_dlm#_tx_term_offset
+ *
+ * DLM Transmitter Termination Offset.
+ *
+ */
+union cvmx_gserx_dlmx_tx_term_offset {
+	uint64_t u64;
+	struct cvmx_gserx_dlmx_tx_term_offset_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_13_63               : 51;
+	uint64_t tx1_term_offset              : 5;  /**< Applies an offset to the Lande 1 resistor calibration value.  Not to be
+                                                         used during normal operation. */
+	uint64_t reserved_5_7                 : 3;
+	uint64_t tx0_term_offset              : 5;  /**< Applies an offset to the Lane 0 resistor calibration value.  Not to be
+                                                         used during normal operation. */
+#else
+	uint64_t tx0_term_offset              : 5;
+	uint64_t reserved_5_7                 : 3;
+	uint64_t tx1_term_offset              : 5;
+	uint64_t reserved_13_63               : 51;
+#endif
+	} s;
+	struct cvmx_gserx_dlmx_tx_term_offset_s cn70xx;
+	struct cvmx_gserx_dlmx_tx_term_offset_s cn70xxp1;
+};
+typedef union cvmx_gserx_dlmx_tx_term_offset cvmx_gserx_dlmx_tx_term_offset_t;
+
+/**
+ * cvmx_gser#_glbl_tad
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_glbl_tad {
+	uint64_t u64;
+	struct cvmx_gserx_glbl_tad_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t pcs_sds_tad_8_5              : 4;  /**< AMON Specific mode selection.
+                                                         Set GSER()_GLBL_TM_ADMON[AMON_ON].
+                                                         Decodes 0x0 - 0x4 require GSER()_GLBL_TM_ADMON[LSEL] set.
+                                                         Decodes 0x5 - 0x5 do not require GSER()_GLBL_TM_ADMON[LSEL] set.
+                                                         In both cases, the resulting signals can be observed on the AMON pin.
+                                                         0x0 = TX txdrv DAC 100ua sink current monitor.
+                                                         0x1 = TX vcnt precision dcc.
+                                                         0x2 = RX sdll topregout.
+                                                         0x3 = RX ldll vctrl_i.
+                                                         0x4 = RX RX term VCM voltage.
+                                                         0x5 = Global bandgap voltage.
+                                                         0x6 = Global CTAT voltage.
+                                                         0x7 = Global internal 100ua reference current.
+                                                         0x8 = Global external 100ua reference current.
+                                                         0x9 = Global Rterm calibration reference voltage.
+                                                         0xA = Global Rterm calibration comparator voltage.
+                                                         0xB = Global Force VCNT thru DAC.
+                                                         0xC = Global VDD voltage.
+                                                         0xD = Global VDDCLK voltage.
+                                                         0xE = Global PLL regulate VCO supply.
+                                                         0xF = Global VCTRL for VCO varactor control. */
+	uint64_t pcs_sds_tad_4_0              : 5;  /**< DMON Specific mode selection.
+                                                         Set GSER()_GLBL_TM_ADMON[DMON_ON].
+                                                         Decodes 0x0 - 0xe require GSER()_GLBL_TM_ADMON[LSEL] set.
+                                                         Decodes 0xf - 0x1f do not require GSER()_GLBL_TM_ADMON[LSEL] set.
+                                                         In both cases, the resulting signals can be observed on the DMON pin.
+                                                         0x00 = DFE Data Q.
+                                                         0x01 = DFE Edge I.
+                                                         0x02 = DFE CK Q.
+                                                         0x03 = DFE CK I.
+                                                         0x04 = TBD.
+                                                         0x05-0x7 = Reserved.
+                                                         0x08 = RX ld_rx[0].
+                                                         0x09 = RX rx_clk.
+                                                         0x0A = RX q_error_stg.
+                                                         0x0B = RX q_data_stg.
+                                                         0x0C-0x0E = Reserved.
+                                                         0x0F = Special case to observe supply in global. Sds_vdda and a internal regulated supply
+                                                         can be observed on DMON and DMONB
+                                                         respectively.  sds_vss can be observed on AMON. GSER()_GLBL_TM_ADMON[AMON_ON]
+                                                         must not be set.
+                                                         0x10: PLL_CLK 0 degree.
+                                                         0x11: Sds_tst_fb_clk.
+                                                         0x12: Buffered refclk.
+                                                         0x13: Div 8 of core clock (core_clk_out).
+                                                         0x14-0x1F: Reserved. */
+#else
+	uint64_t pcs_sds_tad_4_0              : 5;
+	uint64_t pcs_sds_tad_8_5              : 4;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_gserx_glbl_tad_s          cn78xx;
+};
+typedef union cvmx_gserx_glbl_tad cvmx_gserx_glbl_tad_t;
+
+/**
+ * cvmx_gser#_glbl_tm_admon
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_glbl_tm_admon {
+	uint64_t u64;
+	struct cvmx_gserx_glbl_tm_admon_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_8_63                : 56;
+	uint64_t amon_on                      : 1;  /**< When set, AMON test mode is enabled; see GSER()_GLBL_TAD. */
+	uint64_t dmon_on                      : 1;  /**< When set, DMON test mode is enabled; see GSER()_GLBL_TAD. */
+	uint64_t reserved_3_5                 : 3;
+	uint64_t lsel                         : 3;  /**< Three bits to select 1 out of 4 lanes for AMON/DMON test.
+                                                         0x0 = Selects lane 0.
+                                                         0x1 = Selects lane 1.
+                                                         0x2 = Selects lane 2.
+                                                         0x3 = Selects lane 3.
+                                                         0x4-0x7 = Reserved. */
+#else
+	uint64_t lsel                         : 3;
+	uint64_t reserved_3_5                 : 3;
+	uint64_t dmon_on                      : 1;
+	uint64_t amon_on                      : 1;
+	uint64_t reserved_8_63                : 56;
+#endif
+	} s;
+	struct cvmx_gserx_glbl_tm_admon_s     cn78xx;
+};
+typedef union cvmx_gserx_glbl_tm_admon cvmx_gserx_glbl_tm_admon_t;
+
+/**
+ * cvmx_gser#_iddq_mode
+ *
+ * These registers are only reset by hardware during chip cold reset. The values of the CSR
+ * fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_iddq_mode {
+	uint64_t u64;
+	struct cvmx_gserx_iddq_mode_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t phy_iddq_mode                : 1;  /**< When set, power downs all circuitry in PHY for IDDQ testing */
+#else
+	uint64_t phy_iddq_mode                : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_gserx_iddq_mode_s         cn78xx;
+};
+typedef union cvmx_gserx_iddq_mode cvmx_gserx_iddq_mode_t;
+
+/**
+ * cvmx_gser#_lane#_lbert_cfg
+ *
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lanex_lbert_cfg {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_lbert_cfg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t lbert_pg_err_insert          : 1;  /**< Insert one bit error into the LSB of the LBERT generated
+                                                         stream.  A single write to this bit inserts a single bit
+                                                         error. */
+	uint64_t lbert_pm_sync_start          : 1;  /**< Synchronize the pattern matcher LFSR with the incoming
+                                                         data.  Writing this bit resets the error counter and
+                                                         starts a synchronization of the PM.  There is no need
+                                                         to write this bit back to a zero to run normally. */
+	uint64_t lbert_pg_en                  : 1;  /**< Enable the LBERT pattern generator. */
+	uint64_t lbert_pg_width               : 2;  /**< LBERT pattern generator data width:
+                                                         0x0 = 8-bit data.
+                                                         0x1 = 10-bit data.
+                                                         0x2 = 16-bit data.
+                                                         0x3 = 20-bit data. */
+	uint64_t lbert_pg_mode                : 4;  /**< LBERT pattern generator mode; when changing modes,
+                                                         must be disabled first:
+                                                         0x0 = Disabled.
+                                                         0x1 = lfsr31 = X^31 + X^28 + 1.
+                                                         0x2 = lfsr23 = X^23 + X^18 + 1.
+                                                         0x3 = lfsr23 = X^23 + X^21 + X^16 + X^8 + X^5 + X^2 + 1.
+                                                         0x4 = lfsr16 = X^16 + X^5 + X^4 + X^3 + 1.
+                                                         0x5 = lfsr15 = X^15 + X^14 + 1.
+                                                         0x6 = lfsr11 = X^11 + X^9 + 1.
+                                                         0x7 = lfsr7  = X^7 + X^6 + 1.
+                                                         0x8 = Fixed word (PAT0).
+                                                         0x9 = DC-balanced word (PAT0, ~PAT0)
+                                                         0xA = Fixed Pattern (000, PAT0, 3ff, ~PAT0).
+                                                         0xB-F = Reserved. */
+	uint64_t lbert_pm_en                  : 1;  /**< Enable LBERT pattern matcher. */
+	uint64_t lbert_pm_width               : 2;  /**< LBERT pattern matcher data width.
+                                                         0x0 = 8-bit data.
+                                                         0x1 = 10-bit data.
+                                                         0x2 = 16-bit data.
+                                                         0x3 = 20-bit data. */
+	uint64_t lbert_pm_mode                : 4;  /**< LBERT pattern matcher mode; when changing modes,
+                                                         must be disabled first:
+                                                         0x0 = Disabled.
+                                                         0x1 = lfsr31 = X^31 + X^28 + 1.
+                                                         0x2 = lfsr23 = X^23 + X^18 + 1.
+                                                         0x3 = lfsr23 = X^23 + X^21 + X^16 + X^8 + X^5 + X^2 + 1.
+                                                         0x4 = lfsr16 = X^16 + X^5 + X^4 + X^3 + 1.
+                                                         0x5 = lfsr15 = X^15 + X^14 + 1.
+                                                         0x6 = lfsr11 = X^11 + X^9 + 1.
+                                                         0x7 = lfsr7  = X^7 + X^6 + 1.
+                                                         0x8 = Fixed word (PAT0).
+                                                         0x9 = DC-balanced word (PAT0, ~PAT0).
+                                                         0xA = Fixed Pattern: (000, PAT0, 3ff, ~PAT0).
+                                                         0xB-F = Reserved. */
+#else
+	uint64_t lbert_pm_mode                : 4;
+	uint64_t lbert_pm_width               : 2;
+	uint64_t lbert_pm_en                  : 1;
+	uint64_t lbert_pg_mode                : 4;
+	uint64_t lbert_pg_width               : 2;
+	uint64_t lbert_pg_en                  : 1;
+	uint64_t lbert_pm_sync_start          : 1;
+	uint64_t lbert_pg_err_insert          : 1;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_lbert_cfg_s   cn78xx;
+};
+typedef union cvmx_gserx_lanex_lbert_cfg cvmx_gserx_lanex_lbert_cfg_t;
+
+/**
+ * cvmx_gser#_lane#_lbert_ecnt
+ *
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ * The error registers are reset on a read-only when the pattern matcher is enabled.
+ * If the pattern matcher is disabled, the registers return the error count that was
+ * indicated when the pattern matcher was disabled and never reset.
+ */
+union cvmx_gserx_lanex_lbert_ecnt {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_lbert_ecnt_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t lbert_err_ovbit14            : 1;  /**< If this bit is set, multiply LBERT_ERR_CNT by 128.
+                                                         If this bit is set and LBERT_ERR_CNT = 2^15-1, signals
+                                                         overflow of the counter. */
+	uint64_t lbert_err_cnt                : 15; /**< Current error count.
+                                                         If LBERT_ERR_OVBIT14 field is active, then multiply
+                                                         count by 128. */
+#else
+	uint64_t lbert_err_cnt                : 15;
+	uint64_t lbert_err_ovbit14            : 1;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_lbert_ecnt_s  cn78xx;
+};
+typedef union cvmx_gserx_lanex_lbert_ecnt cvmx_gserx_lanex_lbert_ecnt_t;
+
+/**
+ * cvmx_gser#_lane#_lbert_pat_cfg
+ *
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lanex_lbert_pat_cfg {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_lbert_pat_cfg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_10_63               : 54;
+	uint64_t lbert_pg_pat                 : 10; /**< Programmable 10-bit pattern to be used in the LBERT pattern mode;
+                                                         applies when GSER()_LANE()_LBERT_CFG[LBERT_PG_MODE]
+                                                         is equal to 8, 9, or 10. */
+#else
+	uint64_t lbert_pg_pat                 : 10;
+	uint64_t reserved_10_63               : 54;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_lbert_pat_cfg_s cn78xx;
+};
+typedef union cvmx_gserx_lanex_lbert_pat_cfg cvmx_gserx_lanex_lbert_pat_cfg_t;
+
+/**
+ * cvmx_gser#_lane#_misc_cfg_0
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lanex_misc_cfg_0 {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_misc_cfg_0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t use_pma_polarity             : 1;  /**< If set, the PMA control is used to define the polarity.
+                                                         In not set, GSER()_LANE()_RX_CFG_0[CFG_RX_POL_INVERT]
+                                                         is used. */
+	uint64_t cfg_pcs_loopback             : 1;  /**< Assert for Parallel Loopback Raw PCS TX to Raw PCS RX. */
+	uint64_t pcs_tx_mode_ovrrd_en         : 1;  /**< Override enable for Raw PCS TX data width. */
+	uint64_t pcs_rx_mode_ovrrd_en         : 1;  /**< Override enable for Raw PCS RX data width. */
+	uint64_t cfg_eie_det_cnt              : 4;  /**< EIE detect state machine required number of consecutive
+                                                         PHY EIE status assertions to determine EIE and assert Raw
+                                                         PCS output pcs_mac_rx_eie_det_sts. */
+	uint64_t eie_det_stl_on_time          : 3;  /**< EIE detec state machine "on" delay prior to sampling
+                                                         PHY EIE status. */
+	uint64_t eie_det_stl_off_time         : 3;  /**< EIE detec state machine "off" delay prior to sampling
+                                                         PHY EIE status. */
+	uint64_t tx_bit_order                 : 1;  /**< 0x1: Reverse bit order of parallel data to SerDes TX.
+                                                         0x0: Maintain bit order of parallel data to SerDes TX. */
+	uint64_t rx_bit_order                 : 1;  /**< 0x1: Reverse bit order of parallel data to SerDes RX.
+                                                         0x0: Maintain bit order of parallel data to SerDes RX. */
+#else
+	uint64_t rx_bit_order                 : 1;
+	uint64_t tx_bit_order                 : 1;
+	uint64_t eie_det_stl_off_time         : 3;
+	uint64_t eie_det_stl_on_time          : 3;
+	uint64_t cfg_eie_det_cnt              : 4;
+	uint64_t pcs_rx_mode_ovrrd_en         : 1;
+	uint64_t pcs_tx_mode_ovrrd_en         : 1;
+	uint64_t cfg_pcs_loopback             : 1;
+	uint64_t use_pma_polarity             : 1;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_misc_cfg_0_s  cn78xx;
+};
+typedef union cvmx_gserx_lanex_misc_cfg_0 cvmx_gserx_lanex_misc_cfg_0_t;
+
+/**
+ * cvmx_gser#_lane#_misc_cfg_1
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lanex_misc_cfg_1 {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_misc_cfg_1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_13_63               : 51;
+	uint64_t par_tx_init                  : 1;  /**< Performs parallel initialization of SerDes interface TX
+                                                         fifo pointers. */
+	uint64_t tx_polarity                  : 1;  /**< Invert polarity of trasmitted bit stream.  Inversion is
+                                                         performed in the SerDes interface transmit datapth. */
+	uint64_t rx_polarity_ovrrd_en         : 1;  /**< Override mac_pcs_rxX_polarity control pin values
+                                                         When set, RX polarity inversion is specified from
+                                                         RX_POLARITY_OVRRD_VAL, and mac_pcs_rxX_polarity is ignored. */
+	uint64_t rx_polarity_ovrrd_val        : 1;  /**< Controls RX polarity inversion when RX_POLARITY_OVRRD_EN
+                                                         is set. Inversion is performed in the SerDes interface receive
+                                                         datapath. */
+	uint64_t reserved_2_8                 : 7;
+	uint64_t mac_tx_fifo_rd_ptr_ival      : 2;  /**< Initial value for MAC to PCS TX FIFO read pointer. */
+#else
+	uint64_t mac_tx_fifo_rd_ptr_ival      : 2;
+	uint64_t reserved_2_8                 : 7;
+	uint64_t rx_polarity_ovrrd_val        : 1;
+	uint64_t rx_polarity_ovrrd_en         : 1;
+	uint64_t tx_polarity                  : 1;
+	uint64_t par_tx_init                  : 1;
+	uint64_t reserved_13_63               : 51;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_misc_cfg_1_s  cn78xx;
+};
+typedef union cvmx_gserx_lanex_misc_cfg_1 cvmx_gserx_lanex_misc_cfg_1_t;
+
+/**
+ * cvmx_gser#_lane#_pcs_ctlifc_0
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lanex_pcs_ctlifc_0 {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_pcs_ctlifc_0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_14_63               : 50;
+	uint64_t cfg_tx_vboost_en_ovrrd_val   : 1;  /**< Specifies TX VBOOST Enable request when its override bit
+                                                         is asserted GSER()_LANE()_PCS_CTLIFC_2[CFG_TX_VBOOST_EN_OVRRD_EN]. */
+	uint64_t cfg_tx_coeff_req_ovrrd_val   : 1;  /**< Specifies TX Coefficient request when its override bit
+                                                         is asserted GSER()_LANE()_PCS_CTLIFC_2[CFG_TX_COEFF_REQ_OVRRD_EN]. */
+	uint64_t cfg_rx_cdr_coast_req_ovrrd_val : 1;/**< Specifies RX CDR Coast request when its override bit
+                                                         is asserted GSER()_LANE()_PCS_CTLIFC_2[CFG_RX_COAST_REQ_OVRRD_EN]. */
+	uint64_t cfg_tx_detrx_en_req_ovrrd_val : 1; /**< Specifies TX Detect RX request when its override bit
+                                                         is asserted GSER()_LANE()_PCS_CTLIFC_2[CFG_TX_DETRX_EN_REQ_OVRRD_EN]. */
+	uint64_t cfg_soft_reset_req_ovrrd_val : 1;  /**< Specifies Soft Reset request when its override bit
+                                                         is asserted GSER()_LANE()_PCS_CTLIFC_2[CFG_SOFT_RESET_REQ_OVRRD_EN]. */
+	uint64_t cfg_lane_pwr_off_ovrrd_val   : 1;  /**< Specifies Lane Power Off Reset request when its override bit
+                                                         is asserted GSER()_LANE()_PCS_CTLIFC_2[CFG_LANE_PWR_OFF_OVRRD_EN]. */
+	uint64_t cfg_tx_mode_ovrrd_val        : 2;  /**< Override PCS TX mode (data width) when its override bit
+                                                         is asserted GSER()_LANE()_PCS_CTLIFC_2[CFG_TX_MODE_OVRRD_EN].
+                                                         0x0 = 8-bit raw data (not supported).
+                                                         0x1 = 10-bit raw data (not supported).
+                                                         0x2 = 16-bit raw data (not supported).
+                                                         0x3 = 20-bit raw data. */
+	uint64_t cfg_tx_pstate_req_ovrrd_val  : 2;  /**< Override TX pstate request when its override bit
+                                                         is asserted GSER()_LANE()_PCS_CTLIFC_2[CFG_TX_PSTATE_REQ_OVRRD_EN]. */
+	uint64_t cfg_lane_mode_req_ovrrd_val  : 4;  /**< Override Lane Mode request when its override bit
+                                                         is asserted GSER()_LANE()_PCS_CTLIFC_2[CFG_LANE_MODE_REQ_OVRRD_EN]. */
+#else
+	uint64_t cfg_lane_mode_req_ovrrd_val  : 4;
+	uint64_t cfg_tx_pstate_req_ovrrd_val  : 2;
+	uint64_t cfg_tx_mode_ovrrd_val        : 2;
+	uint64_t cfg_lane_pwr_off_ovrrd_val   : 1;
+	uint64_t cfg_soft_reset_req_ovrrd_val : 1;
+	uint64_t cfg_tx_detrx_en_req_ovrrd_val : 1;
+	uint64_t cfg_rx_cdr_coast_req_ovrrd_val : 1;
+	uint64_t cfg_tx_coeff_req_ovrrd_val   : 1;
+	uint64_t cfg_tx_vboost_en_ovrrd_val   : 1;
+	uint64_t reserved_14_63               : 50;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_pcs_ctlifc_0_s cn78xx;
+};
+typedef union cvmx_gserx_lanex_pcs_ctlifc_0 cvmx_gserx_lanex_pcs_ctlifc_0_t;
+
+/**
+ * cvmx_gser#_lane#_pcs_ctlifc_1
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lanex_pcs_ctlifc_1 {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_pcs_ctlifc_1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t cfg_rx_pstate_req_ovrrd_val  : 2;  /**< Override RX pstate request when its override bit
+                                                         is asserted GSER()_LANE()_PCS_CTLIFC_2[CFG_TX_PSTATE_REQ_OVRRD_EN]. */
+	uint64_t reserved_2_6                 : 5;
+	uint64_t cfg_rx_mode_ovrrd_val        : 2;  /**< Override PCS RX mode (data width) when its override bit
+                                                         is asserted GSER()_LANE()_PCS_CTLIFC_2[CFG_RX_MODE_OVRRD_EN].
+                                                         0x0 = 8-bit raw data (not supported).
+                                                         0x1 = 10-bit raw data (not supported).
+                                                         0x2 = 16-bit raw data (not supported).
+                                                         0x3 = 20-bit raw data. */
+#else
+	uint64_t cfg_rx_mode_ovrrd_val        : 2;
+	uint64_t reserved_2_6                 : 5;
+	uint64_t cfg_rx_pstate_req_ovrrd_val  : 2;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_pcs_ctlifc_1_s cn78xx;
+};
+typedef union cvmx_gserx_lanex_pcs_ctlifc_1 cvmx_gserx_lanex_pcs_ctlifc_1_t;
+
+/**
+ * cvmx_gser#_lane#_pcs_ctlifc_2
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lanex_pcs_ctlifc_2 {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_pcs_ctlifc_2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t ctlifc_ovrrd_req             : 1;  /**< Writing to set this bit initiates a state machine interface request
+                                                         for GSER()_LANE()_PCS_CTLIFC_0 and GSER()_LANE()_PCS_CTLIFC_1
+                                                         override values. */
+	uint64_t reserved_9_14                : 6;
+	uint64_t cfg_tx_vboost_en_ovrrd_en    : 1;  /**< Override mac_pcs_txX vboost_en signal with the value specified in
+                                                         GSER()_LANE()_PCS_CTLIFC_2[CFG_TX_VBOOST_EN_OVRRD_VAL]. */
+	uint64_t cfg_tx_coeff_req_ovrrd_en    : 1;  /**< Override mac_pcs_txX_coeff_req signal with the value specified in
+                                                         GSER()_LANE()_PCS_CTLIFC_0[CFG_TX_COEFF_REQ_OVRRD_VAL]. */
+	uint64_t cfg_rx_cdr_coast_req_ovrrd_en : 1; /**< Override mac_pcs_rxX_cdr_coast signal with the value specified in
+                                                         GSER()_LANE()_PCS_CTLIFC_2[CFG_RX_COAST_REQ_OVRRD_VAL]. */
+	uint64_t cfg_tx_detrx_en_req_ovrrd_en : 1;  /**< Override mac_pcs_txX_detrx_en signal with the value specified in
+                                                         GSER()_LANE()_PCS_CTLIFC_2[CFG_TX_DETRX_EN_REQ_OVRRD_VAL]. */
+	uint64_t cfg_soft_reset_req_ovrrd_en  : 1;  /**< Override mac_pcs_laneX_soft_rst signal with the value specified in
+                                                         GSER()_LANE()_PCS_CTLIFC_2[CFG_SOFT_RESET_REQ_OVRRD_VAL]. */
+	uint64_t cfg_lane_pwr_off_ovrrd_en    : 1;  /**< Override mac_pcs_laneX_pwr_off signal with the value specified in
+                                                         GSER()_LANE()_PCS_CTLIFC_2[CFG_LANE_PWR_OFF_OVRRD_VAL]. */
+	uint64_t cfg_tx_pstate_req_ovrrd_en   : 1;  /**< Override mac_pcs_txX_pstate[1:0] signal with the value specified in
+                                                         GSER()_LANE()_PCS_CTLIFC_2[CFG_TX_PSTATE_REQ_OVRRD_VAL].
+                                                         When using this field to change the TX Power State, you must also set
+                                                         the override enable bits for the lane_mode, soft_reset and lane_pwr_off
+                                                         fields.  The corresponding orrd_val fields should be programmed so as
+                                                         not to cause undesired changes. */
+	uint64_t cfg_rx_pstate_req_ovrrd_en   : 1;  /**< Override mac_pcs_rxX_pstate[1:0] signal with the value specified in
+                                                         GSER()_LANE()_PCS_CTLIFC_2[CFG_RX_PSTATE_REQ_OVRRD_VAL].
+                                                         When using this field to change the RX Power State, you must also set
+                                                         the override enable bits for the lane_mode, soft_reset and lane_pwr_off
+                                                         fields.  The corresponding orrd_val fields should be programmed so as
+                                                         not to cause undesired changes. */
+	uint64_t cfg_lane_mode_req_ovrrd_en   : 1;  /**< Override mac_pcs_laneX_mode[3:0] signal with the value specified in
+                                                         is asserted GSER()_LANE()_PCS_CTLIFC_2[CFG_LANE_MODE_REQ_OVRRD_VAL]. */
+#else
+	uint64_t cfg_lane_mode_req_ovrrd_en   : 1;
+	uint64_t cfg_rx_pstate_req_ovrrd_en   : 1;
+	uint64_t cfg_tx_pstate_req_ovrrd_en   : 1;
+	uint64_t cfg_lane_pwr_off_ovrrd_en    : 1;
+	uint64_t cfg_soft_reset_req_ovrrd_en  : 1;
+	uint64_t cfg_tx_detrx_en_req_ovrrd_en : 1;
+	uint64_t cfg_rx_cdr_coast_req_ovrrd_en : 1;
+	uint64_t cfg_tx_coeff_req_ovrrd_en    : 1;
+	uint64_t cfg_tx_vboost_en_ovrrd_en    : 1;
+	uint64_t reserved_9_14                : 6;
+	uint64_t ctlifc_ovrrd_req             : 1;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_pcs_ctlifc_2_s cn78xx;
+};
+typedef union cvmx_gserx_lanex_pcs_ctlifc_2 cvmx_gserx_lanex_pcs_ctlifc_2_t;
+
+/**
+ * cvmx_gser#_lane#_pma_loopback_ctrl
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lanex_pma_loopback_ctrl {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_pma_loopback_ctrl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_2_63                : 62;
+	uint64_t cfg_ln_lpbk_mode_ovrrd_en    : 1;  /**< Enable override mac_pcs_loopbk_mode[3:0] with value of FG_LN_LPBK_MODE. */
+	uint64_t cfg_ln_lpbk_mode             : 1;  /**< Override value when CFG_LN_LPBK_MODE_OVRRD_EN is set. */
+#else
+	uint64_t cfg_ln_lpbk_mode             : 1;
+	uint64_t cfg_ln_lpbk_mode_ovrrd_en    : 1;
+	uint64_t reserved_2_63                : 62;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_pma_loopback_ctrl_s cn78xx;
+};
+typedef union cvmx_gserx_lanex_pma_loopback_ctrl cvmx_gserx_lanex_pma_loopback_ctrl_t;
+
+/**
+ * cvmx_gser#_lane#_pwr_ctrl
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lanex_pwr_ctrl {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_pwr_ctrl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_15_63               : 49;
+	uint64_t tx_sds_fifo_reset_ovrrd_en   : 1;  /**< When asserted, TX_SDS_FIFO_RESET_OVVRD_VAL is used to specify the value of the reset
+                                                         signal for the TX FIFO supplying data to the SerDes p2s interface. */
+	uint64_t tx_sds_fifo_reset_ovrrd_val  : 1;  /**< When asserted, TX_SDS_FIFO_RESET_OVVRD_EN is asserted, this field is
+                                                         used to specify the value of the reset
+                                                         signal for the TX FIFO supplying data to the SerDes p2s interface. */
+	uint64_t tx_pcs_reset_ovrrd_val       : 1;  /**< When TX_PCS_RESET_OVRRD_EN is
+                                                         asserted, this field is used to specify the value of
+                                                         the reset signal for PCS TX logic. */
+	uint64_t rx_pcs_reset_ovrrd_val       : 1;  /**< When RX_PCS_RESET_OVRRD_EN is
+                                                         asserted, this field is used to specify the value of
+                                                         the reset signal for PCS RX logic. */
+	uint64_t reserved_9_10                : 2;
+	uint64_t rx_resetn_ovrrd_en           : 1;  /**< Override RX Power State machine rx_resetn
+                                                         control signal.  When set, the rx_resetn control signal is taken
+                                                         from the GSER()_LANE()_RX_CFG_0[RX_RESETN_OVRRD_VAL]
+                                                         control bit. */
+	uint64_t rx_resetn_ovrrd_val          : 1;  /**< Override RX Power State machine reset control
+                                                         signal. When set, reset control signals are specified in
+                                                         [RX_PCS_RESET_OVRRD_VAL]. */
+	uint64_t rx_lctrl_ovrrd_en            : 1;  /**< Override RX Power State machine loop control
+                                                         signals. */
+	uint64_t rx_lctrl_ovrrd_val           : 1;  /**< Override RX Power State machine power down
+                                                         control signal. When set, the power down control signal is
+                                                         specified by GSER()_LANE()_RX_CFG_1[RX_CHPD_OVRRD_VAL]. */
+	uint64_t tx_tristate_en_ovrrd_en      : 1;  /**< Override TX Power State machine TX tristate
+                                                         control signal.  When set, TX tristate control signal is specified
+                                                         in GSER()_LANE()_TX_CFG_0[TX_TRISTATE_EN_OVRRD_VAL]. */
+	uint64_t tx_pcs_reset_ovrrd_en        : 1;  /**< Override TX Power State machine reset control
+                                                         signal.  When set, reset control signals is specified in
+                                                         [TX_PCS_RESET_OVRRD_VAL]. */
+	uint64_t tx_elec_idle_ovrrd_en        : 1;  /**< Override mac_pcs_txX_elec_idle signal
+                                                         When set, TX electrical idle is controlled from
+                                                         GSER()_LANE()_TX_CFG_1[TX_ELEC_IDLE_OVRRD_VAL]
+                                                         mac_pcs_txX_elec_idle signal is ignored. */
+	uint64_t tx_pd_ovrrd_en               : 1;  /**< Override TX Power State machine TX lane
+                                                         power-down control signal
+                                                         When set, TX lane power down is controlled by
+                                                         GSER()_LANE()_TX_CFG_0[TX_CHPD_OVRRD_VAL]. */
+	uint64_t tx_p2s_resetn_ovrrd_en       : 1;  /**< Override TX Power State machine TX reset
+                                                         control signal
+                                                         When set, TX reset is controlled by
+                                                         GSER()_LANE()_TX_CFG_0[TX_RESETN_OVRRD_VAL]. */
+#else
+	uint64_t tx_p2s_resetn_ovrrd_en       : 1;
+	uint64_t tx_pd_ovrrd_en               : 1;
+	uint64_t tx_elec_idle_ovrrd_en        : 1;
+	uint64_t tx_pcs_reset_ovrrd_en        : 1;
+	uint64_t tx_tristate_en_ovrrd_en      : 1;
+	uint64_t rx_lctrl_ovrrd_val           : 1;
+	uint64_t rx_lctrl_ovrrd_en            : 1;
+	uint64_t rx_resetn_ovrrd_val          : 1;
+	uint64_t rx_resetn_ovrrd_en           : 1;
+	uint64_t reserved_9_10                : 2;
+	uint64_t rx_pcs_reset_ovrrd_val       : 1;
+	uint64_t tx_pcs_reset_ovrrd_val       : 1;
+	uint64_t tx_sds_fifo_reset_ovrrd_val  : 1;
+	uint64_t tx_sds_fifo_reset_ovrrd_en   : 1;
+	uint64_t reserved_15_63               : 49;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_pwr_ctrl_s    cn78xx;
+};
+typedef union cvmx_gserx_lanex_pwr_ctrl cvmx_gserx_lanex_pwr_ctrl_t;
+
+/**
+ * cvmx_gser#_lane#_rx_aeq_out_0
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lanex_rx_aeq_out_0 {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_rx_aeq_out_0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_10_63               : 54;
+	uint64_t sds_pcs_rx_aeq_out           : 10; /**< <9:5>: DFE TAP5
+                                                         <4:0>: DFE TAP4 */
+#else
+	uint64_t sds_pcs_rx_aeq_out           : 10;
+	uint64_t reserved_10_63               : 54;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_rx_aeq_out_0_s cn78xx;
+};
+typedef union cvmx_gserx_lanex_rx_aeq_out_0 cvmx_gserx_lanex_rx_aeq_out_0_t;
+
+/**
+ * cvmx_gser#_lane#_rx_aeq_out_1
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lanex_rx_aeq_out_1 {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_rx_aeq_out_1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_15_63               : 49;
+	uint64_t sds_pcs_rx_aeq_out           : 15; /**< <14:10> = DFE TAP3.
+                                                         <9:5> = DFE TAP2.
+                                                         <4:0> = DFE TAP1. */
+#else
+	uint64_t sds_pcs_rx_aeq_out           : 15;
+	uint64_t reserved_15_63               : 49;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_rx_aeq_out_1_s cn78xx;
+};
+typedef union cvmx_gserx_lanex_rx_aeq_out_1 cvmx_gserx_lanex_rx_aeq_out_1_t;
+
+/**
+ * cvmx_gser#_lane#_rx_aeq_out_2
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lanex_rx_aeq_out_2 {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_rx_aeq_out_2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_15_63               : 49;
+	uint64_t sds_pcs_rx_aeq_out           : 15; /**< <9:8> = Reserved.
+                                                         <7:4> = Pre-CTLE gain.
+                                                         <3:0> = Post-CTLE gain. */
+#else
+	uint64_t sds_pcs_rx_aeq_out           : 15;
+	uint64_t reserved_15_63               : 49;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_rx_aeq_out_2_s cn78xx;
+};
+typedef union cvmx_gserx_lanex_rx_aeq_out_2 cvmx_gserx_lanex_rx_aeq_out_2_t;
+
+/**
+ * cvmx_gser#_lane#_rx_cfg_0
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lanex_rx_cfg_0 {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_rx_cfg_0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t rx_datarate_ovrrd_en         : 1;  /**< Override enable for RX Power State Machine data rate signal. */
+	uint64_t pcs_sds_rx_tristate_enable   : 1;  /**< RX termination high-Z enable. */
+	uint64_t rx_resetn_ovvrd_val          : 1;  /**< This value overrides the RX Power State machine rx_resetn control
+                                                         signal when GSER()_LANE()_PWR_CTRL[RX_RESETN_OVRRD_EN] is set. */
+	uint64_t pcs_sds_rx_eyemon_en         : 1;  /**< RX eyemon test enable. */
+	uint64_t pcs_sds_rx_pcm_ctrl          : 4;  /**< <11>: Reserved
+                                                         <10-8>:
+                                                           0x0 = 540mV.
+                                                           0x1 = 540mV + 20mV.
+                                                           0x2-0x3 = Reserved.
+                                                           0x4 = 100-620mV (default).
+                                                           0x5-0x7 = Reserved. */
+	uint64_t rx_datarate_ovrrd_val        : 2;  /**< Specifies the data rate when RX_DATARATE_OVRRD_EN is asserted:
+                                                         0x0 = Full rate.
+                                                         0x1 = 1/2 data rate.
+                                                         0x2 = 1/4 data rate.
+                                                         0x3 = 1/8 data rate. */
+	uint64_t cfg_rx_pol_invert            : 1;  /**< Invert the receive data.  Allies with GSER()_LANE()_MISC_CFG_0[USE_PMA_POLARITY]
+                                                         is deasserted. */
+	uint64_t rx_subblk_pd_ovrrd_val       : 5;  /**< Not supported. */
+#else
+	uint64_t rx_subblk_pd_ovrrd_val       : 5;
+	uint64_t cfg_rx_pol_invert            : 1;
+	uint64_t rx_datarate_ovrrd_val        : 2;
+	uint64_t pcs_sds_rx_pcm_ctrl          : 4;
+	uint64_t pcs_sds_rx_eyemon_en         : 1;
+	uint64_t rx_resetn_ovvrd_val          : 1;
+	uint64_t pcs_sds_rx_tristate_enable   : 1;
+	uint64_t rx_datarate_ovrrd_en         : 1;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_rx_cfg_0_s    cn78xx;
+};
+typedef union cvmx_gserx_lanex_rx_cfg_0 cvmx_gserx_lanex_rx_cfg_0_t;
+
+/**
+ * cvmx_gser#_lane#_rx_cfg_1
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lanex_rx_cfg_1 {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_rx_cfg_1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t rx_chpd_ovrrd_val            : 1;  /**< Not supported. */
+	uint64_t pcs_sds_rx_os_men            : 1;  /**< RX Offset manual enable. */
+	uint64_t eie_en_ovvrd_en              : 1;  /**< Override enable for Electrical-Idle-Exit circuit. */
+	uint64_t eie_en_ovvrd_val             : 1;  /**< Override value for Electrical-Idle-Exit circuit. */
+	uint64_t reserved_11_11               : 1;
+	uint64_t rx_pcie_mode_ovvrd_en        : 1;  /**< Override enable for RX_PCIE_MODE_OVVRD_VAL. */
+	uint64_t rx_pcie_mode_ovvrd_val       : 1;  /**< Override value for RX_PCIE_MODE_OVVRD_VAL;
+                                                         selects between RX terminations.
+                                                         0x0 = pcs_sds_rx_terminate_to_vdda.
+                                                         0x1 = VDDA. */
+	uint64_t cfg_rx_dll_locken            : 1;  /**< Enable DLL lock when GSER()_LANE()_RX_MISC_OVRRD[CFG_RX_DLL_LOCKEN_OVRRD_EN] is asserted. */
+	uint64_t pcs_sds_rx_cdr_ssc_mode      : 8;  /**< Per lane RX CDR SSC control:
+                                                         <7:4> = Resrted.
+                                                         <3> = Clean SSC error flag.
+                                                         <2> = Diable SSC filter.
+                                                         <1> = Enable SSC value usage.
+                                                         <0> = Reserved. */
+#else
+	uint64_t pcs_sds_rx_cdr_ssc_mode      : 8;
+	uint64_t cfg_rx_dll_locken            : 1;
+	uint64_t rx_pcie_mode_ovvrd_val       : 1;
+	uint64_t rx_pcie_mode_ovvrd_en        : 1;
+	uint64_t reserved_11_11               : 1;
+	uint64_t eie_en_ovvrd_val             : 1;
+	uint64_t eie_en_ovvrd_en              : 1;
+	uint64_t pcs_sds_rx_os_men            : 1;
+	uint64_t rx_chpd_ovrrd_val            : 1;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_rx_cfg_1_s    cn78xx;
+};
+typedef union cvmx_gserx_lanex_rx_cfg_1 cvmx_gserx_lanex_rx_cfg_1_t;
+
+/**
+ * cvmx_gser#_lane#_rx_cfg_2
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lanex_rx_cfg_2 {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_rx_cfg_2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_15_63               : 49;
+	uint64_t pcs_sds_rx_terminate_to_vdda : 1;  /**< RX Termination control:
+                                                         0 = Floating.
+                                                         1 = Terminate to sds_vdda. */
+	uint64_t pcs_sds_rx_sampler_boost     : 2;  /**< Controls amount of boost.
+                                                         Note that this control can negatively impact reliability. */
+	uint64_t pcs_sds_rx_sampler_boost_en  : 1;  /**< Faster sampler c2q.
+                                                         For diagnostic use only. */
+	uint64_t reserved_10_10               : 1;
+	uint64_t rx_sds_rx_agc_mval           : 10; /**< AGC manual value only used when GSERX_LANE()_RX_CFG_5[RX_AGC_MEN_OVVRD_VAL] is set.
+                                                         <9:8>: Reserved.
+                                                         <7:4>: Pre-CTL gain
+                                                         - 0 = -6dB
+                                                         - 1 = -5dB
+                                                         - 3 = +5dB.
+                                                         <3:0>: Post-CTL gain (steps of 0.0875)
+                                                         - 0x0 = lowest
+                                                         - 0xf = lowest * 2.3125. */
+#else
+	uint64_t rx_sds_rx_agc_mval           : 10;
+	uint64_t reserved_10_10               : 1;
+	uint64_t pcs_sds_rx_sampler_boost_en  : 1;
+	uint64_t pcs_sds_rx_sampler_boost     : 2;
+	uint64_t pcs_sds_rx_terminate_to_vdda : 1;
+	uint64_t reserved_15_63               : 49;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_rx_cfg_2_s    cn78xx;
+};
+typedef union cvmx_gserx_lanex_rx_cfg_2 cvmx_gserx_lanex_rx_cfg_2_t;
+
+/**
+ * cvmx_gser#_lane#_rx_cfg_3
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lanex_rx_cfg_3 {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_rx_cfg_3_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t cfg_rx_errdet_ctrl           : 16; /**< RX Adaptive Equalizer Control.
+                                                         Value of pcs_sds_rx_err_det_ctrl when
+                                                         GSER()_LANE()_RX_MISC_OVRRD[CFG_RS_ERRDET_CTRL_OVRRD_EN]
+                                                         is set.
+                                                         <15:13>: Starting delta (6.7mV/step, 13.4mV + 6.7mV*N).
+                                                         <12:10>: Minimum delta to adapt to (6.7mV/step, 13.4mV + 6.7mV*N).
+                                                         <9:7>: Window mode (PM) delta (6.7mV/step, 13.4mV + 6.7mV*N).
+                                                         <6>: Enable DFE for edge samplers.
+                                                         <5:4>: Edge sampler DEF alpha:
+                                                         0x0 = 1/4.
+                                                         0x1 = 1/2.
+                                                         0x2 = 3/4.
+                                                         0x3 = 1.
+                                                         <3:0>: Q/QB error sampler 1 threshold, 6.7mV/step. */
+#else
+	uint64_t cfg_rx_errdet_ctrl           : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_rx_cfg_3_s    cn78xx;
+};
+typedef union cvmx_gserx_lanex_rx_cfg_3 cvmx_gserx_lanex_rx_cfg_3_t;
+
+/**
+ * cvmx_gser#_lane#_rx_cfg_4
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lanex_rx_cfg_4 {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_rx_cfg_4_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t cfg_rx_errdet_ctrl           : 16; /**< RX adaptive equalizer control.
+                                                         Value of pcs_sds_rx_err_det_ctrl when
+                                                         GSER()_LANE()_RX_MISC_OVRRD[CFG_RS_ERRDET_CTRL_OVRRD_EN] is set.
+                                                         <15:14>: Reserved
+                                                         <13:8>: Q/QB error sampler 0 threshold, 6.7mV/step, used for training/LMS.
+                                                         <7>: Enable Window mode, after training has finished.
+                                                         <6:5>: Control sds_pcs_rx_vma_status[15:8].
+                                                         0x0 = window counter[19:12] (FOM).
+                                                         0x1 = window ouunter[11:4].
+                                                         0x2 = CTLE pole, SDLL_IQ.
+                                                         0x3 = pre-CTLE gain, CTLE peak.
+                                                         <4>: Offset cancellation enable.
+                                                         <3:0>: Max CTLE peak setting during training when pcs_sds_rx_vma_ctl[7] is set in
+                                                         GSER()_LANE()_RX_VMA_CTRL. */
+#else
+	uint64_t cfg_rx_errdet_ctrl           : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_rx_cfg_4_s    cn78xx;
+};
+typedef union cvmx_gserx_lanex_rx_cfg_4 cvmx_gserx_lanex_rx_cfg_4_t;
+
+/**
+ * cvmx_gser#_lane#_rx_cfg_5
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lanex_rx_cfg_5 {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_rx_cfg_5_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_5_63                : 59;
+	uint64_t rx_agc_men_ovvrd_en          : 1;  /**< Override enable for AGC manual mode. */
+	uint64_t rx_agc_men_ovvrd_val         : 1;  /**< Override value for AGC manual mode. */
+	uint64_t rx_widthsel_ovvrd_en         : 1;  /**< Override enable for RX width select to the SerDes pcs_sds_rx_widthsel. */
+	uint64_t rx_widthsel_ovvrd_val        : 2;  /**< Override value for RX width select to the SerDes pcs_sds_rx_widthsel.
+                                                         0x0 = 8-bit raw data.
+                                                         0x1 = 10-bit raw data.
+                                                         0x2 = 16-bit raw data.
+                                                         0x3 = 20-bit raw data. */
+#else
+	uint64_t rx_widthsel_ovvrd_val        : 2;
+	uint64_t rx_widthsel_ovvrd_en         : 1;
+	uint64_t rx_agc_men_ovvrd_val         : 1;
+	uint64_t rx_agc_men_ovvrd_en          : 1;
+	uint64_t reserved_5_63                : 59;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_rx_cfg_5_s    cn78xx;
+};
+typedef union cvmx_gserx_lanex_rx_cfg_5 cvmx_gserx_lanex_rx_cfg_5_t;
+
+/**
+ * cvmx_gser#_lane#_rx_ctle_ctrl
+ *
+ * These are the RAW PCS per-lane RX CTLE control registers.
+ * These registers are only reset by hardware during chip cold reset. The values of the CSR
+ * fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lanex_rx_ctle_ctrl {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_rx_ctle_ctrl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t pcs_sds_rx_ctle_bias_ctrl    : 2;  /**< CTLE bias trim bits.
+                                                         0x0 = -10%.
+                                                         0x1 =  0%.
+                                                         0x2 = +5%.
+                                                         0x3 = +10%. */
+	uint64_t pcs_sds_rx_ctle_zero         : 4;  /**< Equalizer peaking control. */
+	uint64_t rx_ctle_pole_ovrrd_en        : 1;  /**< Equalizer pole adjustment override enable. */
+	uint64_t rx_ctle_pole_ovrrd_val       : 4;  /**< Equalizer pole adjustment override value.
+                                                         RX pre-correlation sample counter control
+                                                         bit 3: Optimize CTLE during training.
+                                                         bit 2: Turn off DFE1 for edge samplers.
+                                                         bits 1:0:
+                                                         0x0 = ~ 5dB of peaking at 4.0 GHz.
+                                                         0x1 = ~10dB of peaking at 5.0 GHz.
+                                                         0x2 = ~15dB of peaking at 5.5 GHz.
+                                                         0x3 = ~20dB of peaking at 6.0 GHz. */
+	uint64_t pcs_sds_rx_ctle_pole_max     : 2;  /**< Maximum pole value (for VMA adaption, not applicable in manual mode). */
+	uint64_t pcs_sds_rx_ctle_pole_min     : 2;  /**< Minimum pole value (for VMA adaption, not applicable in manual mode). */
+	uint64_t pcs_sds_rx_ctle_pole_step    : 1;  /**< Step pole value (for VMA adaption, not applicable in manual mode). */
+#else
+	uint64_t pcs_sds_rx_ctle_pole_step    : 1;
+	uint64_t pcs_sds_rx_ctle_pole_min     : 2;
+	uint64_t pcs_sds_rx_ctle_pole_max     : 2;
+	uint64_t rx_ctle_pole_ovrrd_val       : 4;
+	uint64_t rx_ctle_pole_ovrrd_en        : 1;
+	uint64_t pcs_sds_rx_ctle_zero         : 4;
+	uint64_t pcs_sds_rx_ctle_bias_ctrl    : 2;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_rx_ctle_ctrl_s cn78xx;
+};
+typedef union cvmx_gserx_lanex_rx_ctle_ctrl cvmx_gserx_lanex_rx_ctle_ctrl_t;
+
+/**
+ * cvmx_gser#_lane#_rx_misc_ovrrd
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lanex_rx_misc_ovrrd {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_rx_misc_ovrrd_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_12_63               : 52;
+	uint64_t cfg_rx_eie_det_ovrrd_val     : 1;  /**< Override value for RX Electrical-Idle-Exit
+                                                         Detect Enable. */
+	uint64_t cfg_rx_eie_det_ovrrd_en      : 1;  /**< Override enable for RX Electrical-Idle-Exit
+                                                         Detect Enable. */
+	uint64_t cfg_rx_cdr_ctrl_ovvrd_en     : 1;  /**< Not supported. */
+	uint64_t cfg_rx_eq_eval_ovrrd_val     : 1;  /**< Training mode control in override mode. */
+	uint64_t cfg_rx_eq_eval_ovrrd_en      : 1;  /**< Override enable for RX-EQ Eval
+                                                         When asserted, training mode is controlled by
+                                                         CFG_RX_EQ_EVAL_OVRRD_VAL. */
+	uint64_t reserved_6_6                 : 1;
+	uint64_t cfg_rx_dll_locken_ovvrd_en   : 1;  /**< When asserted, override DLL lock enable
+                                                         signal from the RX Power State machine with
+                                                         CFG_RX_DLL_LOCKEN in register
+                                                         GSER()_LANE()_RX_CFG_1. */
+	uint64_t cfg_rx_errdet_ctrl_ovvrd_en  : 1;  /**< When asserted, pcs_sds_rx_err_det_ctrl is set
+                                                         to cfg_rx_errdet_ctrl in registers
+                                                         GSER()_LANE()_RX_CFG_3 and GSER()_LANE()_RX_CFG_4. */
+	uint64_t reserved_0_3                 : 4;
+#else
+	uint64_t reserved_0_3                 : 4;
+	uint64_t cfg_rx_errdet_ctrl_ovvrd_en  : 1;
+	uint64_t cfg_rx_dll_locken_ovvrd_en   : 1;
+	uint64_t reserved_6_6                 : 1;
+	uint64_t cfg_rx_eq_eval_ovrrd_en      : 1;
+	uint64_t cfg_rx_eq_eval_ovrrd_val     : 1;
+	uint64_t cfg_rx_cdr_ctrl_ovvrd_en     : 1;
+	uint64_t cfg_rx_eie_det_ovrrd_en      : 1;
+	uint64_t cfg_rx_eie_det_ovrrd_val     : 1;
+	uint64_t reserved_12_63               : 52;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_rx_misc_ovrrd_s cn78xx;
+};
+typedef union cvmx_gserx_lanex_rx_misc_ovrrd cvmx_gserx_lanex_rx_misc_ovrrd_t;
+
+/**
+ * cvmx_gser#_lane#_rx_precorr_ctrl
+ *
+ * These are the RAW PCS per-lane RX precorrelation control registers. These registers are for
+ * diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset. The values of the CSR
+ * fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lanex_rx_precorr_ctrl {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_rx_precorr_ctrl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_5_63                : 59;
+	uint64_t rx_precorr_disable           : 1;  /**< Disable RX precorrelation calculation. */
+	uint64_t rx_precorr_en_ovrrd_en       : 1;  /**< Override enable for RX precorrelation calculation enable. */
+	uint64_t rx_precorr_en_ovrrd_val      : 1;  /**< Override value for RX precorrelation calculation enable. */
+	uint64_t pcs_sds_rx_precorr_scnt_ctrl : 2;  /**< RX precorrelation sample counter control.
+                                                         0x0 = Load max sample counter with 0x1FF.
+                                                         0x1 = Load max sample counter with 0x3FF.
+                                                         0x2 = Load max sample counter with 0x7FF.
+                                                         0x3 = Load max sample counter with 0xFFF. */
+#else
+	uint64_t pcs_sds_rx_precorr_scnt_ctrl : 2;
+	uint64_t rx_precorr_en_ovrrd_val      : 1;
+	uint64_t rx_precorr_en_ovrrd_en       : 1;
+	uint64_t rx_precorr_disable           : 1;
+	uint64_t reserved_5_63                : 59;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_rx_precorr_ctrl_s cn78xx;
+};
+typedef union cvmx_gserx_lanex_rx_precorr_ctrl cvmx_gserx_lanex_rx_precorr_ctrl_t;
+
+/**
+ * cvmx_gser#_lane#_rx_valbbd_ctrl_0
+ *
+ * These registers are only reset by hardware during chip cold reset. The values of the CSR
+ * fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lanex_rx_valbbd_ctrl_0 {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_rx_valbbd_ctrl_0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_14_63               : 50;
+	uint64_t agc_gain                     : 2;  /**< AGC gain. */
+	uint64_t dfe_gain                     : 2;  /**< DFE gain. */
+	uint64_t dfe_c5_mval                  : 4;  /**< DFE Tap5 manual value when GSER()_LANE()_RX_VALBBD_CTRL_2[DFE_OVRD_EN] and
+                                                         GSER()_LANE()_RX_VALBBD_CTRL_2[DFE_C5_OVRD_VAL] are both set. */
+	uint64_t dfe_c5_msgn                  : 1;  /**< DFE Tap5 manual sign when GSER()_LANE()_RX_VALBBD_CTRL_2[DFE_OVRD_EN] and
+                                                         GSER()_LANE()_RX_VALBBD_CTRL_2[DFE_C5_OVRD_VAL] are both set. */
+	uint64_t dfe_c4_mval                  : 4;  /**< DFE Tap4 manual value when GSER()_LANE()_RX_VALBBD_CTRL_2[DFE_OVRD_EN] and
+                                                         GSER()_LANE()_RX_VALBBD_CTRL_2[DFE_C5_OVRD_VAL] are both set. */
+	uint64_t dfe_c4_msgn                  : 1;  /**< DFE Tap4 manual sign when GSER()_LANE()_RX_VALBBD_CTRL_2[DFE_OVRD_EN] and
+                                                         GSER()_LANE()_RX_VALBBD_CTRL_2[DFE_C5_OVRD_VAL] are both set. */
+#else
+	uint64_t dfe_c4_msgn                  : 1;
+	uint64_t dfe_c4_mval                  : 4;
+	uint64_t dfe_c5_msgn                  : 1;
+	uint64_t dfe_c5_mval                  : 4;
+	uint64_t dfe_gain                     : 2;
+	uint64_t agc_gain                     : 2;
+	uint64_t reserved_14_63               : 50;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_rx_valbbd_ctrl_0_s cn78xx;
+};
+typedef union cvmx_gserx_lanex_rx_valbbd_ctrl_0 cvmx_gserx_lanex_rx_valbbd_ctrl_0_t;
+
+/**
+ * cvmx_gser#_lane#_rx_valbbd_ctrl_1
+ *
+ * These registers are only reset by hardware during chip cold reset. The values of the CSR
+ * fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lanex_rx_valbbd_ctrl_1 {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_rx_valbbd_ctrl_1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_15_63               : 49;
+	uint64_t dfe_c3_mval                  : 4;  /**< DFE Tap3 manual value when GSER()_LANE()_RX_VALBBD_CTRL_2[DFE_OVRD_EN] and
+                                                         GSER()_LANE()_RX_VALBBD_CTRL_2[DFE_C5_OVRD_VAL] are both set. */
+	uint64_t dfe_c3_msgn                  : 1;  /**< DFE Tap3 manual sign when GSER()_LANE()_RX_VALBBD_CTRL_2[DFE_OVRD_EN] and
+                                                         GSER()_LANE()_RX_VALBBD_CTRL_2[DFE_C5_OVRD_VAL] are both set. */
+	uint64_t dfe_c2_mval                  : 4;  /**< DFE Tap2 manual value when GSER()_LANE()_RX_VALBBD_CTRL_2[DFE_OVRD_EN] and
+                                                         GSER()_LANE()_RX_VALBBD_CTRL_2[DFE_C5_OVRD_VAL] are both set. */
+	uint64_t dfe_c2_msgn                  : 1;  /**< DFE Tap2 manual sign when GSER()_LANE()_RX_VALBBD_CTRL_2[DFE_OVRD_EN] and
+                                                         GSER()_LANE()_RX_VALBBD_CTRL_2[DFE_C5_OVRD_VAL] are both set. */
+	uint64_t dfe_c1_mval                  : 4;  /**< DFE Tap1 manual value when GSER()_LANE()_RX_VALBBD_CTRL_2[DFE_OVRD_EN] and
+                                                         GSER()_LANE()_RX_VALBBD_CTRL_2[DFE_C5_OVRD_VAL] are both set.
+                                                         Recommended settings: For the following modes:
+                                                         5G_REFCLK100, 5G_REFCLK15625_QSGMII, and 5G_REFCLK125, it is recommended that DFE_C1_MVAL
+                                                         be set to zero after setting GSER()_LANE_P()_MODE_1[VMA_MM] and also after
+                                                         updating the GSER()_LANE()_RX_VALBBD_CTRL_2 register. In all other modes this
+                                                         register can be ignored. */
+	uint64_t dfe_c1_msgn                  : 1;  /**< DFE Tap1 manual sign when GSER()_LANE()_RX_VALBBD_CTRL_2[DFE_OVRD_EN] and
+                                                         GSER()_LANE()_RX_VALBBD_CTRL_2[DFE_C5_OVRD_VAL] are both set. */
+#else
+	uint64_t dfe_c1_msgn                  : 1;
+	uint64_t dfe_c1_mval                  : 4;
+	uint64_t dfe_c2_msgn                  : 1;
+	uint64_t dfe_c2_mval                  : 4;
+	uint64_t dfe_c3_msgn                  : 1;
+	uint64_t dfe_c3_mval                  : 4;
+	uint64_t reserved_15_63               : 49;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_rx_valbbd_ctrl_1_s cn78xx;
+};
+typedef union cvmx_gserx_lanex_rx_valbbd_ctrl_1 cvmx_gserx_lanex_rx_valbbd_ctrl_1_t;
+
+/**
+ * cvmx_gser#_lane#_rx_valbbd_ctrl_2
+ *
+ * These registers are only reset by hardware during chip cold reset. The values of the CSR
+ * fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lanex_rx_valbbd_ctrl_2 {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_rx_valbbd_ctrl_2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_6_63                : 58;
+	uint64_t dfe_ovrd_en                  : 1;  /**< Override enable for DFE tap controls. When asserted, the register bits in
+                                                         GSER()_LANE()_RX_VALBBD_CTRL_0 and GSER()_LANE()_RX_VALBBD_CTRL_1 are
+                                                         used for controlling the DFE tap manual mode, instead the manual mode signal indexed by
+                                                         GSER()_LANE_MODE[LMODE]. Recommended settings: For the following modes: 5G_REFCLK100,
+                                                         5G_REFCLK15625_QSGMII, and 5G_REFCLK125, it is recommended that DFE tap controls be put in
+                                                         manual mode by setting this bit. In all other modes this register can be ignored. */
+	uint64_t dfe_c5_ovrd_val              : 1;  /**< Override value for DFE Tap5 manual enable. Recommended settings: For the following modes;
+                                                         5G_REFCLK100, 5G_REFCLK15625_QSGMII, and 5G_REFCLK125, it is recommended that the DFE Tap5
+                                                         manual enable be set after setting GSER()_LANE_P()_MODE_1[VMA_MM]. In all
+                                                         other modes this register can be ignored. */
+	uint64_t dfe_c4_ovrd_val              : 1;  /**< Override value for DFE Tap4 manual enable. Recommended settings: For the following modes:
+                                                         5G_REFCLK100, 5G_REFCLK15625_QSGMII, and 5G_REFCLK125, it is recommended that the DFE Tap4
+                                                         manual enable be set after setting GSER()_LANE_P()_MODE_1[VMA_MM]. In all
+                                                         other modes this register can be ignored. */
+	uint64_t dfe_c3_ovrd_val              : 1;  /**< Override value for DFE Tap3 manual enable. Recommended settings: For the following modes;
+                                                         5G_REFCLK100, 5G_REFCLK15625_QSGMII, and 5G_REFCLK125, it is recommended that the DFE Tap3
+                                                         manual enable be set after setting GSER()_LANE_P()_MODE_1[VMA_MM]. In all
+                                                         other modes this register can be ignored. */
+	uint64_t dfe_c2_ovrd_val              : 1;  /**< Override value for DFE Tap2 manual enable. Recommended settings: For the following modes;
+                                                         5G_REFCLK100, 5G_REFCLK15625_QSGMII, and 5G_REFCLK125, it is recommended that the DFE Tap2
+                                                         manual enable be set after setting GSER()_LANE_P()_MODE_1[VMA_MM]. In all
+                                                         other modes this register can be ignored. */
+	uint64_t dfe_c1_ovrd_val              : 1;  /**< Override value for DFE Tap1 manual enable. Recommended settings: For the following modes;
+                                                         5G_REFCLK100, 5G_REFCLK15625_QSGMII, and 5G_REFCLK125, it is recommended that the DFE Tap1
+                                                         manual enable be set after setting GSER()_LANE_P()_MODE_1[VMA_MM]. In all
+                                                         other modes this register can be ignored. */
+#else
+	uint64_t dfe_c1_ovrd_val              : 1;
+	uint64_t dfe_c2_ovrd_val              : 1;
+	uint64_t dfe_c3_ovrd_val              : 1;
+	uint64_t dfe_c4_ovrd_val              : 1;
+	uint64_t dfe_c5_ovrd_val              : 1;
+	uint64_t dfe_ovrd_en                  : 1;
+	uint64_t reserved_6_63                : 58;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_rx_valbbd_ctrl_2_s cn78xx;
+};
+typedef union cvmx_gserx_lanex_rx_valbbd_ctrl_2 cvmx_gserx_lanex_rx_valbbd_ctrl_2_t;
+
+/**
+ * cvmx_gser#_lane#_rx_vma_ctrl
+ *
+ * These are the RAW PCS per-lane RX VMA control registers. These registers are for diagnostic
+ * use only.
+ * These registers are only reset by hardware during chip cold reset. The values of the CSR
+ * fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lanex_rx_vma_ctrl {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_rx_vma_ctrl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t vma_fine_cfg_sel_ovrrd_en    : 1;  /**< Enable override of VMA fine configuration selection. */
+	uint64_t vma_fine_cfg_sel_ovrrd_val   : 1;  /**< Override value of VMA fine configuration selection.
+                                                         - 0: Coarse mode.
+                                                         - 1: Fine mode. */
+	uint64_t rx_fom_div_delta             : 1;  /**< TX figure of merit delta division-mode enable. */
+	uint64_t rx_vna_ctrl_18_16            : 3;  /**< RX VMA loop control. */
+	uint64_t rx_vna_ctrl_9_0              : 10; /**< RX VMA loop control.
+                                                         <9:8> = Parameter settling wait time.
+                                                         <7> = Limit CTLE peak to max value.
+                                                         <6> = Long reach enabled.
+                                                         <5> = Short reach enabled.
+                                                         <4> = Training done override enable.
+                                                         <3> = Training done override value.
+                                                         <2:0> = VMA clock modulation. */
+#else
+	uint64_t rx_vna_ctrl_9_0              : 10;
+	uint64_t rx_vna_ctrl_18_16            : 3;
+	uint64_t rx_fom_div_delta             : 1;
+	uint64_t vma_fine_cfg_sel_ovrrd_val   : 1;
+	uint64_t vma_fine_cfg_sel_ovrrd_en    : 1;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_rx_vma_ctrl_s cn78xx;
+};
+typedef union cvmx_gserx_lanex_rx_vma_ctrl cvmx_gserx_lanex_rx_vma_ctrl_t;
+
+/**
+ * cvmx_gser#_lane#_rx_vma_status_0
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lanex_rx_vma_status_0 {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_rx_vma_status_0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_8_63                : 56;
+	uint64_t sds_pcs_rx_vma_status        : 8;  /**< <8> = DFE Powerdown.
+                                                         <7> = Reserved.
+                                                         <6:2> = CTLE Peak.
+                                                         <1:0> = CTLE Pole. */
+#else
+	uint64_t sds_pcs_rx_vma_status        : 8;
+	uint64_t reserved_8_63                : 56;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_rx_vma_status_0_s cn78xx;
+};
+typedef union cvmx_gserx_lanex_rx_vma_status_0 cvmx_gserx_lanex_rx_vma_status_0_t;
+
+/**
+ * cvmx_gser#_lane#_rx_vma_status_1
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lanex_rx_vma_status_1 {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_rx_vma_status_1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t sds_pcs_rx_vma_status        : 16; /**< <15:8>: Output is controlled by GSER()_LANE()_RX_CFG_3[CFG_RX_ERRDET_CTRL[6:5]
+                                                         0x0 = Pre-CTL gain, CTLE Peak.
+                                                         0x1 = CTL pole, SDLL_IQ.
+                                                         0x2 = Window counter[11:3].
+                                                         0x3 = Window counter[19:12] (VMA RAW FOM).
+                                                         <7>: Training done
+                                                         <6>: Internal state machine training done
+                                                         <5:3>: Internal state machine Delta
+                                                         <2:0>: CDR Phase Offset, DLL IQ Training value. */
+#else
+	uint64_t sds_pcs_rx_vma_status        : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_rx_vma_status_1_s cn78xx;
+};
+typedef union cvmx_gserx_lanex_rx_vma_status_1 cvmx_gserx_lanex_rx_vma_status_1_t;
+
+/**
+ * cvmx_gser#_lane#_tx_cfg_0
+ *
+ * These registers are for diagnostic use only. These registers are only reset by hardware during
+ * chip cold reset. The values of the CSR fields in these registers do not change during chip
+ * warm or soft resets.
+ */
+union cvmx_gserx_lanex_tx_cfg_0 {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_tx_cfg_0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t tx_tristate_en_ovrd_val      : 1;  /**< TX termination high-Z enable. */
+	uint64_t tx_chpd_ovrd_val             : 1;  /**< TX lane power down. */
+	uint64_t reserved_10_13               : 4;
+	uint64_t tx_resetn_ovrd_val           : 1;  /**< TX P2S rest. */
+	uint64_t tx_cm_mode                   : 1;  /**< Assert to enable fast Common-Mode charge up. For simulation purposes only. */
+	uint64_t cfg_tx_swing                 : 5;  /**< TX output swing control.
+                                                         Default swing encoding when GSER()_LANE()_TX_CFG_1[TX_SWING_OVRRD_EN] is
+                                                         asserted. */
+	uint64_t fast_rdet_mode               : 1;  /**< Assert to enable fast RX Detection. For simulation purposes only. */
+	uint64_t fast_tristate_mode           : 1;  /**< Assert to enable fast Tristate power up. For simulation purposes only. */
+	uint64_t reserved_0_0                 : 1;
+#else
+	uint64_t reserved_0_0                 : 1;
+	uint64_t fast_tristate_mode           : 1;
+	uint64_t fast_rdet_mode               : 1;
+	uint64_t cfg_tx_swing                 : 5;
+	uint64_t tx_cm_mode                   : 1;
+	uint64_t tx_resetn_ovrd_val           : 1;
+	uint64_t reserved_10_13               : 4;
+	uint64_t tx_chpd_ovrd_val             : 1;
+	uint64_t tx_tristate_en_ovrd_val      : 1;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_tx_cfg_0_s    cn78xx;
+};
+typedef union cvmx_gserx_lanex_tx_cfg_0 cvmx_gserx_lanex_tx_cfg_0_t;
+
+/**
+ * cvmx_gser#_lane#_tx_cfg_1
+ *
+ * These registers are for diagnostic use only. These registers are only reset by hardware during
+ * chip cold reset. The values of the CSR fields in these registers do not change during chip
+ * warm or soft resets.
+ */
+union cvmx_gserx_lanex_tx_cfg_1 {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_tx_cfg_1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_15_63               : 49;
+	uint64_t tx_widthsel_ovrd_en          : 1;  /**< Override enable for pcs_sds_txX_widthsel, TX parallel interface width setting. */
+	uint64_t tx_widthsel_ovrd_val         : 2;  /**< Override value for pcs_sds_widthsel, TX parallel interface width setting.
+                                                         0x0 = 8-bit (not supported).
+                                                         0x1 = 10-bit (not supported).
+                                                         0x2 = 16-bit (not supported).
+                                                         0x3 = 20-bit (not supported). */
+	uint64_t tx_vboost_en_ovrrd_en        : 1;  /**< Override enable for pcs_sds_txX_vboost_en, TX  vboost mode enable. */
+	uint64_t tx_turbo_en_ovrrd_en         : 1;  /**< Override enable for pcs_sds_txX_turbo_en, Turbo mode enable. */
+	uint64_t tx_swing_ovrd_en             : 1;  /**< Override enable for pcs_sds_txX_swing, TX swing. */
+	uint64_t tx_premptap_ovrd_val         : 1;  /**< Override enable for pcs_sds_txX_preemptap, preemphasis control. */
+	uint64_t tx_elec_idle_ovrrd_en        : 1;  /**< Override enable for pcs_sds_txX_elec_idle, TX electrical idle. */
+	uint64_t smpl_rate_ovrd_en            : 1;  /**< Override enable for TX Power state machine sample rate. When asserted, the TX sample is
+                                                         specified from SMPL_RATE_OVRD_VAL and the TX Power state machine control signal is
+                                                         ignored. */
+	uint64_t smpl_rate_ovrd_val           : 3;  /**< Specifies the sample rate (strobe assertion) relative to mac_pcs_txX_clk when
+                                                         SMPL_RATE_OVRD_EN is asserted.
+                                                         0x0 = full rate.
+                                                         0x1 = 1/2 data rate.
+                                                         0x2 = 1/4 data rate.
+                                                         0x3 = 1/8 data rate.
+                                                         0x4 = 1/18 data rate.
+                                                         0x5-7 = Reserved. */
+	uint64_t tx_datarate_ovrd_en          : 1;  /**< Override enable for RX Power state machine data rate signal. When set, rx_datarate is
+                                                         specified from TX_DATA_RATE_OVRD_VAL and the RX Power State Machine control signal is
+                                                         ignored. */
+	uint64_t tx_datarate_ovrd_val         : 2;  /**< Specifies the TX data rate when TX_DATARATE_OVRD_EN is asserted.
+                                                         0x0 = full rate.
+                                                         0x1 = 1/2 data rate.
+                                                         0x2 = 1/4 data rate.
+                                                         0x3 = 1/8 data rate. */
+#else
+	uint64_t tx_datarate_ovrd_val         : 2;
+	uint64_t tx_datarate_ovrd_en          : 1;
+	uint64_t smpl_rate_ovrd_val           : 3;
+	uint64_t smpl_rate_ovrd_en            : 1;
+	uint64_t tx_elec_idle_ovrrd_en        : 1;
+	uint64_t tx_premptap_ovrd_val         : 1;
+	uint64_t tx_swing_ovrd_en             : 1;
+	uint64_t tx_turbo_en_ovrrd_en         : 1;
+	uint64_t tx_vboost_en_ovrrd_en        : 1;
+	uint64_t tx_widthsel_ovrd_val         : 2;
+	uint64_t tx_widthsel_ovrd_en          : 1;
+	uint64_t reserved_15_63               : 49;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_tx_cfg_1_s    cn78xx;
+};
+typedef union cvmx_gserx_lanex_tx_cfg_1 cvmx_gserx_lanex_tx_cfg_1_t;
+
+/**
+ * cvmx_gser#_lane#_tx_cfg_2
+ *
+ * These registers are for diagnostic use only. These registers are only reset by hardware during
+ * chip cold reset. The values of the CSR fields in these registers do not change during chip
+ * warm or soft resets.
+ */
+union cvmx_gserx_lanex_tx_cfg_2 {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_tx_cfg_2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t pcs_sds_tx_dcc_en            : 1;  /**< DCC Enable. */
+	uint64_t reserved_3_14                : 12;
+	uint64_t rcvr_test_ovrd_en            : 1;  /**< Override RX detect disable and test pulse. */
+	uint64_t rcvr_test_ovrd_val           : 1;  /**< Override value for RX detect test pulse; used to create a pulse during which the receiver
+                                                         detect test operation is performed. */
+	uint64_t tx_rx_detect_dis_ovrd_val    : 1;  /**< Override value of RX detect disable. */
+#else
+	uint64_t tx_rx_detect_dis_ovrd_val    : 1;
+	uint64_t rcvr_test_ovrd_val           : 1;
+	uint64_t rcvr_test_ovrd_en            : 1;
+	uint64_t reserved_3_14                : 12;
+	uint64_t pcs_sds_tx_dcc_en            : 1;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_tx_cfg_2_s    cn78xx;
+};
+typedef union cvmx_gserx_lanex_tx_cfg_2 cvmx_gserx_lanex_tx_cfg_2_t;
+
+/**
+ * cvmx_gser#_lane#_tx_cfg_3
+ *
+ * These registers are for diagnostic use only. These registers are only reset by hardware during
+ * chip cold reset. The values of the CSR fields in these registers do not change during chip
+ * warm or soft resets.
+ */
+union cvmx_gserx_lanex_tx_cfg_3 {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_tx_cfg_3_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_15_63               : 49;
+	uint64_t cfg_tx_vboost_en             : 1;  /**< Specifies the value of TX VBoost enable when
+                                                         GSER()_LANE()_TX_CFG_1[TX_VBOOST_EN_OVRRD_EN] is asserted. */
+	uint64_t reserved_7_13                : 7;
+	uint64_t pcs_sds_tx_gain              : 3;  /**< TX Gain. For debug use only. */
+	uint64_t pcs_sds_tx_srate_sel         : 3;  /**< Reserved. */
+	uint64_t cfg_tx_turbo_en              : 1;  /**< Specifies value ot TX turbo enable when GSER()_LANE()_TX_CFG_1[TX_TURBO_EN] is set. */
+#else
+	uint64_t cfg_tx_turbo_en              : 1;
+	uint64_t pcs_sds_tx_srate_sel         : 3;
+	uint64_t pcs_sds_tx_gain              : 3;
+	uint64_t reserved_7_13                : 7;
+	uint64_t cfg_tx_vboost_en             : 1;
+	uint64_t reserved_15_63               : 49;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_tx_cfg_3_s    cn78xx;
+};
+typedef union cvmx_gserx_lanex_tx_cfg_3 cvmx_gserx_lanex_tx_cfg_3_t;
+
+/**
+ * cvmx_gser#_lane#_tx_pre_emphasis
+ *
+ * These registers are for diagnostic use only. These registers are only reset by hardware during
+ * chip cold reset. The values of the CSR fields in these registers do not change during chip
+ * warm or soft resets.
+ */
+union cvmx_gserx_lanex_tx_pre_emphasis {
+	uint64_t u64;
+	struct cvmx_gserx_lanex_tx_pre_emphasis_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t cfg_tx_premptap              : 9;  /**< Override preemphasis control. Applies when
+                                                         GSER()_LANE()_TX_CFG_3[TX_PREMPTAP_OVRD_EN] is asserted.
+                                                         <8:4> = Post-cursor.
+                                                         <3:0> = Pre-cursor. */
+#else
+	uint64_t cfg_tx_premptap              : 9;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_gserx_lanex_tx_pre_emphasis_s cn78xx;
+};
+typedef union cvmx_gserx_lanex_tx_pre_emphasis cvmx_gserx_lanex_tx_pre_emphasis_t;
+
+/**
+ * cvmx_gser#_lane_lpbken
+ *
+ * These registers are only reset by hardware during chip cold reset. The values of the CSR
+ * fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lane_lpbken {
+	uint64_t u64;
+	struct cvmx_gserx_lane_lpbken_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t lpbken                       : 4;  /**< For links that are not in PCIE mode (including all OCI links). When asserted in P0 state,
+                                                         allows per lane TX-to-RX serial loopback activation.
+                                                         <3>: Lane 3.
+                                                         <2>: Lane 2.
+                                                         <1>: Lane 1.
+                                                         <0>: Lane 0. */
+#else
+	uint64_t lpbken                       : 4;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_gserx_lane_lpbken_s       cn78xx;
+};
+typedef union cvmx_gserx_lane_lpbken cvmx_gserx_lane_lpbken_t;
+
+/**
+ * cvmx_gser#_lane_mode
+ *
+ * These registers are only reset by hardware during chip cold reset. The values of the CSR
+ * fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lane_mode {
+	uint64_t u64;
+	struct cvmx_gserx_lane_mode_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t lmode                        : 4;  /**< For links that are not in PCIE mode (including all OCI links), used to index into the PHY
+                                                         table to select electrical specs and link rate. Note that the PHY table can be modified
+                                                         such that any supported link rate can be derived regardless of the configured LMODE.
+                                                         0x0: R_25G_REFCLK100.
+                                                         0x1: R_5G_REFCLK100.
+                                                         0x2: R_8G_REFCLK100.
+                                                         0x3: R_125G_REFCLK15625_KX (not supported).
+                                                         0x4: R_3125G_REFCLK15625_XAUI.
+                                                         For XAUI applications:
+                                                         0x5: R_103125G_REFCLK15625_KR.
+                                                         For XFI, XLAUI, KR applications:
+                                                         0x6: R_125G_REFCLK15625_SGMII.
+                                                         For SGMII applications:
+                                                         0x7: R_5G_REFCLK15625_QSGMII (not supported).
+                                                         0x8: R_625G_REFCLK15625_RXAUI.
+                                                         For RXAUI, DXAUI applications:
+                                                         0x9: R_25G_REFCLK125.
+                                                         0xA: R_5G_REFCLK125.
+                                                         0xB: R_8G_REFCLK125.
+                                                         0xC - 0xF: Reserved.
+                                                         This register is not used for PCIE configurations. For non-OCI links, this register
+                                                         defaults to R_625G_REFCLK15625_RXAUI. For OCI links, the value is mapped at reset from the
+                                                         GSER()_SPD and the appropriate table updates are performed so the rate is obtained for the
+                                                         particular reference clock.
+                                                         It is recommended that the PHY be in reset when reconfiguring the LMODE
+                                                         (GSER()_PHY_CTL[PHY_RESET] is set).
+                                                         Once the LMODE has been configured, and the PHY is out of reset, the table entries for the
+                                                         selected LMODE must be updated to reflect the reference clock speed. Refer to the register
+                                                         description and index into the table using the rate and reference speed to obtain the
+                                                         recommended values.
+                                                         _ Write GSER()_PLL_P()_MODE_0.
+                                                         _ Write GSER()_PLL_P()_MODE_1.
+                                                         _ Write GSER()_LANE_P()_MODE_0.
+                                                         _ Write GSER()_LANE_P()_MODE_1.
+                                                         where in "P(z)", z equals LMODE. */
+#else
+	uint64_t lmode                        : 4;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_gserx_lane_mode_s         cn78xx;
+};
+typedef union cvmx_gserx_lane_mode cvmx_gserx_lane_mode_t;
+
+/**
+ * cvmx_gser#_lane_p#_mode_0
+ *
+ * These are the RAW PCS lane settings mode 0 registers. There is one register per
+ * 4 lanes per GSER per GSER_LMODE_E value (0..11). Only one entry is used at any given time in a
+ * given GSER lane - the one selected by the corresponding GSER()_LANE_MODE[LMODE].
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lane_px_mode_0 {
+	uint64_t u64;
+	struct cvmx_gserx_lane_px_mode_0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_15_63               : 49;
+	uint64_t ctle                         : 2;  /**< Continuous time linear equalizer pole configuration.
+                                                         0x0 = ~5dB of peaking at 4 GHz (Minimum bandwidth).
+                                                         0x1 =~10dB of peaking at 5 GHz
+                                                         0x2 = ~15dB of peaking at 5.5 GHz
+                                                         0x3 = ~20dB of peaking at 6 GHz (Maximum bandwidth).
+                                                         Recommended settings:
+                                                         _ R_25G_REFCLK100:          0x0
+                                                         _ R_5G_REFCLK100:           0x0
+                                                         _ R_8G_REFCLK100:           0x3
+                                                         _ R_125G_REFCLK15625_KX:    0x0
+                                                         _ R_3125G_REFCLK15625_XAUI: 0x0
+                                                         _ R_103125G_REFCLK15625_KR: 0x3
+                                                         _ R_125G_REFCLK15625_SGMII: 0x0
+                                                         _ R_5G_REFCLK15625_QSGMII:  0x0
+                                                         _ R_625G_REFCLK15625_RXAUI: 0x0
+                                                         _ R_25G_REFCLK125:          0x0
+                                                         _ R_5G_REFCLK125:           0x0
+                                                         _ R_8G_REFCLK125:           0x3 */
+	uint64_t pcie                         : 1;  /**< Selects between RX terminations.
+                                                         - 0: Differential termination
+                                                         - 1: Termination between pad and SDS_VDDS.
+                                                          Recommended settings:
+                                                          _ R_25G_REFCLK100:          0x1
+                                                          _ R_5G_REFCLK100:           0x1
+                                                          _ R_8G_REFCLK100:           0x0
+                                                          _ R_125G_REFCLK15625_KX:    0x0
+                                                          _ R_3125G_REFCLK15625_XAUI: 0x0
+                                                          _ R_103125G_REFCLK15625_KR: 0x0
+                                                          _ R_125G_REFCLK15625_SGMII: 0x0
+                                                          _ R_5G_REFCLK15625_QSGMII:  0x0
+                                                          _ R_625G_REFCLK15625_RXAUI: 0x0
+                                                          _ R_25G_REFCLK125:          0x1
+                                                          _ R_5G_REFCLK125:           0x1
+                                                          _ R_8G_REFCLK125:           0x0 */
+	uint64_t tx_ldiv                      : 2;  /**< Configures clock divider used to determine the receive rate.
+                                                         0x0 = full data rate.
+                                                         0x1 = 1/2 data rate.
+                                                         0x2 = 1/4 data rate.
+                                                         0x3 = 1/8 data rate.
+                                                         Recommended settings:
+                                                         _ R_25G_REFCLK100:          0x1
+                                                         _ R_5G_REFCLK100:           0x0
+                                                         _ R_8G_REFCLK100:           0x0
+                                                         _ R_125G_REFCLK15625_KX:    0x2
+                                                         _ R_3125G_REFCLK15625_XAUI: 0x1
+                                                         _ R_103125G_REFCLK15625_KR: 0x0
+                                                         _ R_125G_REFCLK15625_SGMII: 0x2
+                                                         _ R_5G_REFCLK15625_QSGMII:  0x0
+                                                         _ R_625G_REFCLK15625_RXAUI: 0x0
+                                                         _ R_25G_REFCLK125:          0x1
+                                                         _ R_5G_REFCLK125:           0x0
+                                                         _ R_8G_REFCLK125:           0x0 */
+	uint64_t rx_ldiv                      : 2;  /**< Configures clock divider used to determine the receive rate.
+                                                         0x0 = full data rate
+                                                         0x1 = 1/2 data rate
+                                                         0x2 = 1/4 data rate
+                                                         0x3 = 1/8 data rate
+                                                         Recommended settings:
+                                                         _ R_25G_REFCLK100:          0x1
+                                                         _ R_5G_REFCLK100:           0x0
+                                                         _ R_8G_REFCLK100:           0x0
+                                                         _ R_125G_REFCLK15625_KX:    0x2
+                                                         _ R_3125G_REFCLK15625_XAUI: 0x1
+                                                         _ R_103125G_REFCLK15625_KR: 0x0
+                                                         _ R_125G_REFCLK15625_SGMII: 0x2
+                                                         _ R_5G_REFCLK15625_QSGMII:  0x0
+                                                         _ R_625G_REFCLK15625_RXAUI: 0x0
+                                                         _ R_25G_REFCLK125:          0x1
+                                                         _ R_5G_REFCLK125:           0x0
+                                                         _ R_8G_REFCLK125:           0x0 */
+	uint64_t srate                        : 3;  /**< Sample rate, used to generate strobe to effectively divide the clock down to a slower
+                                                         rate.
+                                                         0x0 = Full rate
+                                                         0x1 = 1/2 data rate
+                                                         0x2 = 1/4 data rate
+                                                         0x3 = 1/8 data rate
+                                                         0x4 = 1/16 data rate
+                                                         else = Reserved.
+                                                         This field should always be cleared to zero (full rate). */
+	uint64_t reserved_4_4                 : 1;
+	uint64_t tx_mode                      : 2;  /**< TX data width:
+                                                         0x0 = 8-bit raw data (not supported).
+                                                         0x1 = 10-bit raw data (not supported).
+                                                         0x2 = 16-bit raw data (not supported).
+                                                         0x3 = 20-bit raw data. */
+	uint64_t rx_mode                      : 2;  /**< RX data width:
+                                                         0x0 = 8-bit raw data (not supported).
+                                                         0x1 = 10-bit raw data (not supported).
+                                                         0x2 = 16-bit raw data (not supported).
+                                                         0x3 = 20-bit raw data. */
+#else
+	uint64_t rx_mode                      : 2;
+	uint64_t tx_mode                      : 2;
+	uint64_t reserved_4_4                 : 1;
+	uint64_t srate                        : 3;
+	uint64_t rx_ldiv                      : 2;
+	uint64_t tx_ldiv                      : 2;
+	uint64_t pcie                         : 1;
+	uint64_t ctle                         : 2;
+	uint64_t reserved_15_63               : 49;
+#endif
+	} s;
+	struct cvmx_gserx_lane_px_mode_0_s    cn78xx;
+};
+typedef union cvmx_gserx_lane_px_mode_0 cvmx_gserx_lane_px_mode_0_t;
+
+/**
+ * cvmx_gser#_lane_p#_mode_1
+ *
+ * These are the RAW PCS lane settings mode 1 registers. There is one register per 4 lanes,
+ * (0..3) per GSER per GSER_LMODE_E value (0..11). Only one entry is used at any given time in a
+ * given
+ * GSER lane - the one selected by the corresponding GSER()_LANE_MODE[LMODE].
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lane_px_mode_1 {
+	uint64_t u64;
+	struct cvmx_gserx_lane_px_mode_1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t vma_fine_cfg_sel             : 1;  /**< Recommended settings:
+                                                         1 = Enabled. Fine step adaptation selected (10.3125 Gbaud rate).
+                                                         0 = Disabled. Coarse step adaptation selected (rates lower than 10.3125 Gbaud). */
+	uint64_t vma_mm                       : 1;  /**< Manual DFE verses adaptive DFE mode. Recommended settings:
+                                                         0 = Adaptive DFE (5 Gbaud and higher)
+                                                         1 = Manual DFE, fixed tap (3.125 Gbaud and lower). */
+	uint64_t cdr_fgain                    : 4;  /**< CDR frequency gain.
+                                                         Recommended settings:
+                                                         _ R_25G_REFCLK100:          0xA
+                                                         _ R_5G_REFCLK100:           0xA
+                                                         _ R_8G_REFCLK100:           0xB
+                                                         _ R_125G_REFCLK15625_KX:    0xC
+                                                         _ R_3125G_REFCLK15625_XAUI: 0xC
+                                                         _ R_103125G_REFCLK15625_KR: 0xA
+                                                         _ R_125G_REFCLK15625_SGMII: 0xC
+                                                         _ R_5G_REFCLK15625_QSGMII:  0xC
+                                                         _ R_625G_REFCLK15625_RXAUI: 0xA
+                                                         _ R_25G_REFCLK125:          0xA
+                                                         _ R_5G_REFCLK125:           0xA
+                                                         _ R_8G_REFCLK125:           0xB */
+	uint64_t ph_acc_adj                   : 10; /**< Phase accumulator adjust.
+                                                         Recommended settings:
+                                                         _ R_25G_REFCLK100:          0x14
+                                                         _ R_5G_REFCLK100:           0x14
+                                                         _ R_8G_REFCLK100:           0x23
+                                                         _ R_125G_REFCLK15625_KX:    0x1E
+                                                         _ R_3125G_REFCLK15625_XAUI: 0x1E
+                                                         _ R_103125G_REFCLK15625_KR: 0xF
+                                                         _ R_125G_REFCLK15625_SGMII: 0x1E
+                                                         _ R_5G_REFCLK15625_QSGMII:  0x1E
+                                                         _ R_625G_REFCLK15625_RXAUI: 0x14
+                                                         _ R_25G_REFCLK125:          0x14
+                                                         _ R_5G_REFCLK125:           0x14
+                                                         _ R_8G_REFCLK125:           0x23 */
+#else
+	uint64_t ph_acc_adj                   : 10;
+	uint64_t cdr_fgain                    : 4;
+	uint64_t vma_mm                       : 1;
+	uint64_t vma_fine_cfg_sel             : 1;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_gserx_lane_px_mode_1_s    cn78xx;
+};
+typedef union cvmx_gserx_lane_px_mode_1 cvmx_gserx_lane_px_mode_1_t;
+
+/**
+ * cvmx_gser#_lane_poff
+ *
+ * These registers are only reset by hardware during chip cold reset. The values of the CSR
+ * fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lane_poff {
+	uint64_t u64;
+	struct cvmx_gserx_lane_poff_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t lpoff                        : 4;  /**< For links that are not in PCIE mode (including all OCI links), allows for per lane power
+                                                         down.
+                                                         <3>: Lane 3.
+                                                         <2>: Lane 2.
+                                                         <1>: Lane 1.
+                                                         <0>: Lane 0. */
+#else
+	uint64_t lpoff                        : 4;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_gserx_lane_poff_s         cn78xx;
+};
+typedef union cvmx_gserx_lane_poff cvmx_gserx_lane_poff_t;
+
+/**
+ * cvmx_gser#_lane_srst
+ *
+ * These registers are only reset by hardware during chip cold reset. The values of the CSR
+ * fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lane_srst {
+	uint64_t u64;
+	struct cvmx_gserx_lane_srst_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t lsrst                        : 1;  /**< For links that are not in PCIE mode (including all OCI links), resets all 4 lanes
+                                                         (equivalent to the P2 power state) after any pending requests (power state change, rate
+                                                         change) are complete. The lanes remain in reset state while this signal is asserted. When
+                                                         the signal deasserts, the lanes exit the reset state and the PHY returns to the power
+                                                         state the PHY was in prior. For diagnostic use only. */
+#else
+	uint64_t lsrst                        : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_gserx_lane_srst_s         cn78xx;
+};
+typedef union cvmx_gserx_lane_srst cvmx_gserx_lane_srst_t;
+
+/**
+ * cvmx_gser#_lane_vma_coarse_ctrl_0
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lane_vma_coarse_ctrl_0 {
+	uint64_t u64;
+	struct cvmx_gserx_lane_vma_coarse_ctrl_0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t iq_max                       : 4;  /**< Slice DLL IQ maximum value in VMA coarse mode. */
+	uint64_t iq_min                       : 4;  /**< Slice DLL IQ minimum value in VMA coarse mode. */
+	uint64_t iq_step                      : 2;  /**< Slice DLL IQ step size in VMA coarse mode. */
+	uint64_t window_wait                  : 3;  /**< Adaptation window wait setting in VMA coarse mode. */
+	uint64_t lms_wait                     : 3;  /**< LMS wait time setting used to control the number of samples taken during the collection of
+                                                         statistics in VMA coarse mode. */
+#else
+	uint64_t lms_wait                     : 3;
+	uint64_t window_wait                  : 3;
+	uint64_t iq_step                      : 2;
+	uint64_t iq_min                       : 4;
+	uint64_t iq_max                       : 4;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_gserx_lane_vma_coarse_ctrl_0_s cn78xx;
+};
+typedef union cvmx_gserx_lane_vma_coarse_ctrl_0 cvmx_gserx_lane_vma_coarse_ctrl_0_t;
+
+/**
+ * cvmx_gser#_lane_vma_coarse_ctrl_1
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lane_vma_coarse_ctrl_1 {
+	uint64_t u64;
+	struct cvmx_gserx_lane_vma_coarse_ctrl_1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_10_63               : 54;
+	uint64_t ctle_pmax                    : 4;  /**< RX CTLE peak maximum value in VMA coarse mode. */
+	uint64_t ctle_pmin                    : 4;  /**< RX CTLE peak minimum value in VMA coarse mode. */
+	uint64_t ctle_pstep                   : 2;  /**< CTLE peak step size in VMA coarse mode. */
+#else
+	uint64_t ctle_pstep                   : 2;
+	uint64_t ctle_pmin                    : 4;
+	uint64_t ctle_pmax                    : 4;
+	uint64_t reserved_10_63               : 54;
+#endif
+	} s;
+	struct cvmx_gserx_lane_vma_coarse_ctrl_1_s cn78xx;
+};
+typedef union cvmx_gserx_lane_vma_coarse_ctrl_1 cvmx_gserx_lane_vma_coarse_ctrl_1_t;
+
+/**
+ * cvmx_gser#_lane_vma_coarse_ctrl_2
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lane_vma_coarse_ctrl_2 {
+	uint64_t u64;
+	struct cvmx_gserx_lane_vma_coarse_ctrl_2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_10_63               : 54;
+	uint64_t pctle_gmax                   : 4;  /**< RX PRE-CTLE gain maximum value in VMA coarse mode. */
+	uint64_t pctle_gmin                   : 4;  /**< RX PRE-CTLE gain minimum value in VMA coarse mode. */
+	uint64_t pctle_gstep                  : 2;  /**< CTLE PRE-peak gain step size in VMA coarse mode. */
+#else
+	uint64_t pctle_gstep                  : 2;
+	uint64_t pctle_gmin                   : 4;
+	uint64_t pctle_gmax                   : 4;
+	uint64_t reserved_10_63               : 54;
+#endif
+	} s;
+	struct cvmx_gserx_lane_vma_coarse_ctrl_2_s cn78xx;
+};
+typedef union cvmx_gserx_lane_vma_coarse_ctrl_2 cvmx_gserx_lane_vma_coarse_ctrl_2_t;
+
+/**
+ * cvmx_gser#_lane_vma_fine_ctrl_0
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lane_vma_fine_ctrl_0 {
+	uint64_t u64;
+	struct cvmx_gserx_lane_vma_fine_ctrl_0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t rx_sdll_iq_max_fine          : 4;  /**< RX Slice DLL IQ maximum value in VMA Fine mode (valid when
+                                                         GSER()_LANE_P()_MODE_1[VMA_FINE_CFG_SEL]=1 and
+                                                         GSER()_LANE_P()_MODE_1[VMA_MM]=0). */
+	uint64_t rx_sdll_iq_min_fine          : 4;  /**< RX slice DLL IQ minimum value in VMA fine mode (valid when
+                                                         GSER()_LANE_P()_MODE_1[VMA_FINE_CFG_SEL]=1 and
+                                                         GSER()_LANE_P()_MODE_1[VMA_MM]=0). */
+	uint64_t rx_sdll_iq_step_fine         : 2;  /**< RX Slice DLL IQ step size in VMA Fine mode (valid when
+                                                         GSER()_LANE_P()_MODE_1[VMA_FINE_CFG_SEL]=1 and
+                                                         GSER()_LANE_P()_MODE_1[VMA_MM]=0). */
+	uint64_t vma_window_wait_fine         : 3;  /**< Adaptation window wait setting (in VMA fine mode); used to control the number of samples
+                                                         taken during the collection of statistics (valid when
+                                                         GSER()_LANE_P()_MODE_1[VMA_FINE_CFG_SEL]=1 and GSER()_LANE_P()_MODE_1[VMA_MM]=0). */
+	uint64_t lms_wait_time_fine           : 3;  /**< LMS wait time setting (in VMA fine mode); used to control the number of samples taken
+                                                         during the collection of statistics (valid when
+                                                         GSER()_LANE_P()_MODE_1[VMA_FINE_CFG_SEL]=1 and GSER()_LANE_P()_MODE_1[VMA_MM]=0). */
+#else
+	uint64_t lms_wait_time_fine           : 3;
+	uint64_t vma_window_wait_fine         : 3;
+	uint64_t rx_sdll_iq_step_fine         : 2;
+	uint64_t rx_sdll_iq_min_fine          : 4;
+	uint64_t rx_sdll_iq_max_fine          : 4;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_gserx_lane_vma_fine_ctrl_0_s cn78xx;
+};
+typedef union cvmx_gserx_lane_vma_fine_ctrl_0 cvmx_gserx_lane_vma_fine_ctrl_0_t;
+
+/**
+ * cvmx_gser#_lane_vma_fine_ctrl_1
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lane_vma_fine_ctrl_1 {
+	uint64_t u64;
+	struct cvmx_gserx_lane_vma_fine_ctrl_1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_10_63               : 54;
+	uint64_t rx_ctle_peak_max_fine        : 4;  /**< RX CTLE peak maximum value in VMA fine mode (valid when
+                                                         GSER()_LANE_P()_MODE_1[VMA_FINE_CFG_SEL]=1 and GSER()_LANE_P()_MODE_1[VMA_MM]=0). */
+	uint64_t rx_ctle_peak_min_fine        : 4;  /**< RX CTLE peak minimum value in VMA fine mode (valid when
+                                                         GSER()_LANE_P()_MODE_1[VMA_FINE_CFG_SEL]=1 and GSER()_LANE_P()_MODE_1[VMA_MM]=0). */
+	uint64_t rx_ctle_peak_step_fine       : 2;  /**< RX CTLE Peak step size in VMA Fine mode (valid when
+                                                         GSER()_LANE_P()_MODE_1[VMA_FINE_CFG_SEL]=1 and GSER()_LANE_P()_MODE_1[VMA_MM]=0). */
+#else
+	uint64_t rx_ctle_peak_step_fine       : 2;
+	uint64_t rx_ctle_peak_min_fine        : 4;
+	uint64_t rx_ctle_peak_max_fine        : 4;
+	uint64_t reserved_10_63               : 54;
+#endif
+	} s;
+	struct cvmx_gserx_lane_vma_fine_ctrl_1_s cn78xx;
+};
+typedef union cvmx_gserx_lane_vma_fine_ctrl_1 cvmx_gserx_lane_vma_fine_ctrl_1_t;
+
+/**
+ * cvmx_gser#_lane_vma_fine_ctrl_2
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_lane_vma_fine_ctrl_2 {
+	uint64_t u64;
+	struct cvmx_gserx_lane_vma_fine_ctrl_2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_10_63               : 54;
+	uint64_t rx_prectle_peak_max_fine     : 4;  /**< RX PRE-CTLE peak maximum value in VMA fine mode (valid when
+                                                         GSER()_LANE_P()_MODE_1[VMA_FINE_CFG_SEL]=1 and GSER()_LANE_P()_MODE_1[VMA_MM]=0). */
+	uint64_t rx_prectle_peak_min_fine     : 4;  /**< RX PRE-CTLE peak minimum value in VMA fine mode (valid when
+                                                         GSER()_LANE_P()_MODE_1[VMA_FINE_CFG_SEL]=1 and GSER()_LANE_P()_MODE_1[VMA_MM]=0). */
+	uint64_t rx_prectle_peak_step_fine    : 2;  /**< RX PRE-CTLE peak step size in VMA fine mode (valid when
+                                                         GSER()_LANE_P()_MODE_1[VMA_FINE_CFG_SEL]=1 and GSER()_LANE_P()_MODE_1[VMA_MM]=0). */
+#else
+	uint64_t rx_prectle_peak_step_fine    : 2;
+	uint64_t rx_prectle_peak_min_fine     : 4;
+	uint64_t rx_prectle_peak_max_fine     : 4;
+	uint64_t reserved_10_63               : 54;
+#endif
+	} s;
+	struct cvmx_gserx_lane_vma_fine_ctrl_2_s cn78xx;
+};
+typedef union cvmx_gserx_lane_vma_fine_ctrl_2 cvmx_gserx_lane_vma_fine_ctrl_2_t;
+
+/**
+ * cvmx_gser#_pcie_pcs_clk_req
+ *
+ * PCIE PIPE Clock Required
+ *
+ */
+union cvmx_gserx_pcie_pcs_clk_req {
+	uint64_t u64;
+	struct cvmx_gserx_pcie_pcs_clk_req_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t clk_req                      : 1;  /**< When asserted, indicates that the external logic requires the PHY's
+                                                         PCLK to remain active, preventing the PIPE PCS from powering off
+                                                         that clock while in the P2 state.
+                                                         Note, the PCS hangs if this bit is asserted when a Fundamental Reset
+                                                         is issued to the PEM. */
+#else
+	uint64_t clk_req                      : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_gserx_pcie_pcs_clk_req_s  cn70xx;
+	struct cvmx_gserx_pcie_pcs_clk_req_s  cn70xxp1;
+};
+typedef union cvmx_gserx_pcie_pcs_clk_req cvmx_gserx_pcie_pcs_clk_req_t;
+
+/**
+ * cvmx_gser#_pcie_pipe#_txdeemph
+ *
+ * PCIE PIPE Transmitter De-emphasis.
+ *
+ */
+union cvmx_gserx_pcie_pipex_txdeemph {
+	uint64_t u64;
+	struct cvmx_gserx_pcie_pipex_txdeemph_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t pipe_txdeemph                : 1;  /**< Selects Transmitter De-emphasis.
+                                                         - 0: enabled (6 dB / 3.5 dB)
+                                                         - 1: No de-emphasis */
+#else
+	uint64_t pipe_txdeemph                : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_gserx_pcie_pipex_txdeemph_s cn70xx;
+	struct cvmx_gserx_pcie_pipex_txdeemph_s cn70xxp1;
+};
+typedef union cvmx_gserx_pcie_pipex_txdeemph cvmx_gserx_pcie_pipex_txdeemph_t;
+
+/**
+ * cvmx_gser#_pcie_pipe_com_clk
+ *
+ * PCIE Select Common Clock Mode for Receive Data Path.
+ *
+ */
+union cvmx_gserx_pcie_pipe_com_clk {
+	uint64_t u64;
+	struct cvmx_gserx_pcie_pipe_com_clk_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t com_clk                      : 1;  /**< When both ends of a PCIe link share a common reference clock, the
+                                                         latency through the receiver's elasticity buffer can be shorter,
+                                                         because no frequency offset exists between the two ends of the link.
+                                                         Assert this control only if all physical lanes of the PHY are
+                                                         guaranteed to be connected to links that share a common reference
+                                                         clock. */
+#else
+	uint64_t com_clk                      : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_gserx_pcie_pipe_com_clk_s cn70xx;
+	struct cvmx_gserx_pcie_pipe_com_clk_s cn70xxp1;
+};
+typedef union cvmx_gserx_pcie_pipe_com_clk cvmx_gserx_pcie_pipe_com_clk_t;
+
+/**
+ * cvmx_gser#_pcie_pipe_crst
+ *
+ * PCIE PIPE Cold Reset.
+ *
+ */
+union cvmx_gserx_pcie_pipe_crst {
+	uint64_t u64;
+	struct cvmx_gserx_pcie_pipe_crst_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t pipe_crst                    : 1;  /**< PCIE PIPE Async Cold Reset Contol. */
+#else
+	uint64_t pipe_crst                    : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_gserx_pcie_pipe_crst_s    cn70xx;
+	struct cvmx_gserx_pcie_pipe_crst_s    cn70xxp1;
+};
+typedef union cvmx_gserx_pcie_pipe_crst cvmx_gserx_pcie_pipe_crst_t;
+
+/**
+ * cvmx_gser#_pcie_pipe_port_loopbk
+ *
+ * PCIE Tx-to-Rx Loopback Enable.
+ *
+ */
+union cvmx_gserx_pcie_pipe_port_loopbk {
+	uint64_t u64;
+	struct cvmx_gserx_pcie_pipe_port_loopbk_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t pipe3_loopbk                 : 1;  /**< When this signal is asserted, data from the PIPE3 transmit predriver
+                                                         is looped back to the receive slicers.  LOS is bypassed and based on
+                                                         the TxN_en input so that rxN_los !txN_data_en. */
+	uint64_t pipe2_loopbk                 : 1;  /**< When this signal is asserted, data from the PIPE2 transmit predriver
+                                                         is looped back to the receive slicers.  LOS is bypassed and based on
+                                                         the TxN_en input so that rxN_los !txN_data_en. */
+	uint64_t pipe1_loopbk                 : 1;  /**< When this signal is asserted, data from the PIPE1 transmit predriver
+                                                         is looped back to the receive slicers.  LOS is bypassed and based on
+                                                         the TxN_en input so that rxN_los !txN_data_en. */
+	uint64_t pipe0_loopbk                 : 1;  /**< When this signal is asserted, data from the PIPE0 transmit predriver
+                                                         is looped back to the receive slicers.  LOS is bypassed and based on
+                                                         the TxN_en input so that rxN_los !txN_data_en. */
+#else
+	uint64_t pipe0_loopbk                 : 1;
+	uint64_t pipe1_loopbk                 : 1;
+	uint64_t pipe2_loopbk                 : 1;
+	uint64_t pipe3_loopbk                 : 1;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_gserx_pcie_pipe_port_loopbk_s cn70xx;
+	struct cvmx_gserx_pcie_pipe_port_loopbk_s cn70xxp1;
+};
+typedef union cvmx_gserx_pcie_pipe_port_loopbk cvmx_gserx_pcie_pipe_port_loopbk_t;
+
+/**
+ * cvmx_gser#_pcie_pipe_port_sel
+ *
+ * PCIE PIPE Enable Request.
+ *
+ */
+union cvmx_gserx_pcie_pipe_port_sel {
+	uint64_t u64;
+	struct cvmx_gserx_pcie_pipe_port_sel_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_3_63                : 61;
+	uint64_t cfg_pem1_dlm2                : 1;  /**< The PIPE (Pipe1 or Pipe2) and PHY (DLM1 or DLM2) configuration for PEM1
+                                                          when in 4-Pipe Mode.
+                                                          This bit should not be set in Single Pipe or 2-Pipe Mode.
+                                                         - 0: PEM1 is tied to Pipe1/DLM1.  This is 3x1 PCIe mode when all 4 PIPES are enabled.
+                                                         - 1: PEM1 is tied to Pipe2/DLM2.  This is 2x1 PCIe mode with SATA */
+	uint64_t pipe_port_sel                : 2;  /**< PIPE enable request.  Change only when phy_reset is asserted.
+                                                         - 00: Disables all PIPEs
+                                                         - 01: Single Pipe Mode. Enables PIPE0 (PEM0) only.
+                                                             This is 1x4 PCIe mode.
+                                                         - 10: 2-Pipe Mode.  Enables PIPEs 0 (PEM0) and 1 (PEM1).
+                                                             This is 2x2 PCIe mode or 1x2 PCIe mode with SATA.
+                                                         - 11: 4-Pipe Mode. Enables PIPEs 0 (PEM0), 1, 2 (PEM1), and 3 (PEM2).
+                                                             This is 2x1 PCIe mode with SATA or 3x1 PCIe mode. */
+#else
+	uint64_t pipe_port_sel                : 2;
+	uint64_t cfg_pem1_dlm2                : 1;
+	uint64_t reserved_3_63                : 61;
+#endif
+	} s;
+	struct cvmx_gserx_pcie_pipe_port_sel_s cn70xx;
+	struct cvmx_gserx_pcie_pipe_port_sel_s cn70xxp1;
+};
+typedef union cvmx_gserx_pcie_pipe_port_sel cvmx_gserx_pcie_pipe_port_sel_t;
+
+/**
+ * cvmx_gser#_pcie_pipe_rst
+ *
+ * PCIE PIPE Reset.
+ *
+ */
+union cvmx_gserx_pcie_pipe_rst {
+	uint64_t u64;
+	struct cvmx_gserx_pcie_pipe_rst_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t pipe3_rst                    : 1;  /**< Pipe 3 Reset.  Setting this bit will put Pipe 3 into reset.
+                                                         PEM2 is always tied to Pipe 3. */
+	uint64_t pipe2_rst                    : 1;  /**< Pipe 2 Reset.  Setting this bit will put Pipe 2 into reset.
+                                                         PEM1 is tied to Pipe 2 in 3x1 PCIe mode (GSER_PCIE_PIPE_PORT_SEL.PIPE_PORT_SEL
+                                                         is set to 4-pipe mode, and GSER_PCIE_PIPE_PORT_SEL.CFG_PEM1_DLM2 is also set). */
+	uint64_t pipe1_rst                    : 1;  /**< Pipe 1 Reset.  Setting this bit will put Pipe 1 into reset.
+                                                         PEM1 is tied to Pipe 1 in 2x2 PCIe or 2x1 PCIe with SATA modes. */
+	uint64_t pipe0_rst                    : 1;  /**< Pipe 0 Reset.  Setting this bit will put Pipe 0 into reset.
+                                                         PEM0 is always tied to Pipe 0. */
+#else
+	uint64_t pipe0_rst                    : 1;
+	uint64_t pipe1_rst                    : 1;
+	uint64_t pipe2_rst                    : 1;
+	uint64_t pipe3_rst                    : 1;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_gserx_pcie_pipe_rst_s     cn70xx;
+	struct cvmx_gserx_pcie_pipe_rst_s     cn70xxp1;
+};
+typedef union cvmx_gserx_pcie_pipe_rst cvmx_gserx_pcie_pipe_rst_t;
+
+/**
+ * cvmx_gser#_pcie_pipe_rst_sts
+ *
+ * PCIE PIPE Status Reset.
+ *
+ */
+union cvmx_gserx_pcie_pipe_rst_sts {
+	uint64_t u64;
+	struct cvmx_gserx_pcie_pipe_rst_sts_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t pipe3_rst                    : 1;  /**< Reflects the current state of the pipe3_rst_n which includes
+                                                         the rst__pem2_pcs_rst_n term from the reset controller.  Note that
+                                                         when PIPE3_RST is asserted (active low), no Pipe clocks are generated
+                                                         to PEM3 and any RSL reads to the application side registers will time out. */
+	uint64_t pipe2_rst                    : 1;  /**< Reflects the current state of the pipe2_rst_n which includes
+                                                         the rst__pem2_pcs_rst_n term from the reset controller.  Note that
+                                                         when PIPE2_RST is asserted (active low) and PEM1 is being used in
+                                                         3x1 PCIe mode (4-Pipe Mode with CFG_PEM1_DLM2 set), no Pipe clocks
+                                                         are generated to PEM1 and any RSL reads to the application side
+                                                         registers will time out. */
+	uint64_t pipe1_rst                    : 1;  /**< Reflects the current state of the pipe1_rst_n which includes
+                                                         the rst__pem1_pcs_rst_n term from the reset controller.  Note that
+                                                         when PIPE1_RST is asserted (active low) and PEM1 is being used in
+                                                         2x2 PCIe or 2x1 PCIe with SATA, no Pipe clocks are generated to PEM1
+                                                         and any RSL reads to the application side registers will time out. */
+	uint64_t pipe0_rst                    : 1;  /**< Reflects the current state of the pipe0_rst_n which includes
+                                                         the rst__pem0_pcs_rst_n term from the reset controller.  Note that
+                                                         when PIPE0_RST is asserted (active low), no Pipe clocks are generated
+                                                         to PEM0 and any RSL reads to the application side registers will time out. */
+#else
+	uint64_t pipe0_rst                    : 1;
+	uint64_t pipe1_rst                    : 1;
+	uint64_t pipe2_rst                    : 1;
+	uint64_t pipe3_rst                    : 1;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_gserx_pcie_pipe_rst_sts_s cn70xx;
+	struct cvmx_gserx_pcie_pipe_rst_sts_s cn70xxp1;
+};
+typedef union cvmx_gserx_pcie_pipe_rst_sts cvmx_gserx_pcie_pipe_rst_sts_t;
+
+/**
+ * cvmx_gser#_pcie_pipe_status
+ *
+ * PCIE PIPE Status.
+ *
+ */
+union cvmx_gserx_pcie_pipe_status {
+	uint64_t u64;
+	struct cvmx_gserx_pcie_pipe_status_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t pipe3_clkreqn                : 1;  /**< When asserted, indicates that the PCS/PHY layer is in a mode where
+                                                         reference clocks are required.  When deasserted, the PIPE PCS is
+                                                         powered down into a state where the external reference clocks can
+                                                         be turned off. */
+	uint64_t pipe2_clkreqn                : 1;  /**< When asserted, indicates that the PCS/PHY layer is in a mode where
+                                                         reference clocks are required.  When deasserted, the PIPE PCS is
+                                                         powered down into a state where the external reference clocks can
+                                                         be turned off. */
+	uint64_t pipe1_clkreqn                : 1;  /**< When asserted, indicates that the PCS/PHY layer is in a mode where
+                                                         reference clocks are required.  When deasserted, the PIPE PCS is
+                                                         powered down into a state where the external reference clocks can
+                                                         be turned off. */
+	uint64_t pipe0_clkreqn                : 1;  /**< When asserted, indicates that the PCS/PHY layer is in a mode where
+                                                         reference clocks are required.  When deasserted, the PIPE PCS is
+                                                         powered down into a state where the external reference clocks can
+                                                         be turned off. */
+#else
+	uint64_t pipe0_clkreqn                : 1;
+	uint64_t pipe1_clkreqn                : 1;
+	uint64_t pipe2_clkreqn                : 1;
+	uint64_t pipe3_clkreqn                : 1;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_gserx_pcie_pipe_status_s  cn70xx;
+	struct cvmx_gserx_pcie_pipe_status_s  cn70xxp1;
+};
+typedef union cvmx_gserx_pcie_pipe_status cvmx_gserx_pcie_pipe_status_t;
+
+/**
+ * cvmx_gser#_pcie_tx_deemph_gen1
+ *
+ * PCIE Tx De-emphasis at 3.5 dB.
+ *
+ */
+union cvmx_gserx_pcie_tx_deemph_gen1 {
+	uint64_t u64;
+	struct cvmx_gserx_pcie_tx_deemph_gen1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_6_63                : 58;
+	uint64_t tx_deemph_gen1               : 6;  /**< This static value sets the launch amplitude of the transmitter
+                                                         when pipeP_tx_swing is set to 0x0 (default state). Used for
+                                                         tuning at the board level for Rx eye compliance. */
+#else
+	uint64_t tx_deemph_gen1               : 6;
+	uint64_t reserved_6_63                : 58;
+#endif
+	} s;
+	struct cvmx_gserx_pcie_tx_deemph_gen1_s cn70xx;
+	struct cvmx_gserx_pcie_tx_deemph_gen1_s cn70xxp1;
+};
+typedef union cvmx_gserx_pcie_tx_deemph_gen1 cvmx_gserx_pcie_tx_deemph_gen1_t;
+
+/**
+ * cvmx_gser#_pcie_tx_deemph_gen2_3p5db
+ *
+ * PCIE Tx De-emphasis at 3.5 dB.
+ *
+ */
+union cvmx_gserx_pcie_tx_deemph_gen2_3p5db {
+	uint64_t u64;
+	struct cvmx_gserx_pcie_tx_deemph_gen2_3p5db_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_6_63                : 58;
+	uint64_t tx_deemph_gen2_3p5db         : 6;  /**< This static value sets the Tx driver deemphasis value in the case where
+                                                         pipeP_tx_deemph is set to 1'b1 (default setting) and the PHY is running
+                                                         at the Gen2 rate. Used for tuning at the board level for Rx eye compliance. */
+#else
+	uint64_t tx_deemph_gen2_3p5db         : 6;
+	uint64_t reserved_6_63                : 58;
+#endif
+	} s;
+	struct cvmx_gserx_pcie_tx_deemph_gen2_3p5db_s cn70xx;
+	struct cvmx_gserx_pcie_tx_deemph_gen2_3p5db_s cn70xxp1;
+};
+typedef union cvmx_gserx_pcie_tx_deemph_gen2_3p5db cvmx_gserx_pcie_tx_deemph_gen2_3p5db_t;
+
+/**
+ * cvmx_gser#_pcie_tx_deemph_gen2_6db
+ *
+ * PCIE Tx De-emphasis at 6 dB.
+ *
+ */
+union cvmx_gserx_pcie_tx_deemph_gen2_6db {
+	uint64_t u64;
+	struct cvmx_gserx_pcie_tx_deemph_gen2_6db_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_6_63                : 58;
+	uint64_t tx_deemph_gen2_6db           : 6;  /**< This static value sets the Tx driver deemphasis value in the case where
+                                                         pipeP_tx_deemph is set to 1'b0 and the PHY is running at the Gen2 rate.
+                                                         Used for tuning at the board level for Rx eye compliance. */
+#else
+	uint64_t tx_deemph_gen2_6db           : 6;
+	uint64_t reserved_6_63                : 58;
+#endif
+	} s;
+	struct cvmx_gserx_pcie_tx_deemph_gen2_6db_s cn70xx;
+	struct cvmx_gserx_pcie_tx_deemph_gen2_6db_s cn70xxp1;
+};
+typedef union cvmx_gserx_pcie_tx_deemph_gen2_6db cvmx_gserx_pcie_tx_deemph_gen2_6db_t;
+
+/**
+ * cvmx_gser#_pcie_tx_swing_full
+ *
+ * PCIE Tx Amplitude (Full Swing Mode).
+ *
+ */
+union cvmx_gserx_pcie_tx_swing_full {
+	uint64_t u64;
+	struct cvmx_gserx_pcie_tx_swing_full_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_7_63                : 57;
+	uint64_t tx_swing_hi                  : 7;  /**< This static value sets the launch amplitude of the transmitter when
+                                                         pipeP_tx_swing is set to 1'b0 (default state).  Used for tuning at
+                                                         the board level for Rx eye compliance. */
+#else
+	uint64_t tx_swing_hi                  : 7;
+	uint64_t reserved_7_63                : 57;
+#endif
+	} s;
+	struct cvmx_gserx_pcie_tx_swing_full_s cn70xx;
+	struct cvmx_gserx_pcie_tx_swing_full_s cn70xxp1;
+};
+typedef union cvmx_gserx_pcie_tx_swing_full cvmx_gserx_pcie_tx_swing_full_t;
+
+/**
+ * cvmx_gser#_pcie_tx_swing_low
+ *
+ * PCIE Tx Amplitude (Low Swing Mode).
+ *
+ */
+union cvmx_gserx_pcie_tx_swing_low {
+	uint64_t u64;
+	struct cvmx_gserx_pcie_tx_swing_low_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_7_63                : 57;
+	uint64_t tx_swing_lo                  : 7;  /**< This static value sets the launch amplitude of the transmitter when
+                                                         pipeP_tx_swing is set to 1'b1 (low swing mode).  Used for tuning at
+                                                         the board level for Rx eye compliance. */
+#else
+	uint64_t tx_swing_lo                  : 7;
+	uint64_t reserved_7_63                : 57;
+#endif
+	} s;
+	struct cvmx_gserx_pcie_tx_swing_low_s cn70xx;
+	struct cvmx_gserx_pcie_tx_swing_low_s cn70xxp1;
+};
+typedef union cvmx_gserx_pcie_tx_swing_low cvmx_gserx_pcie_tx_swing_low_t;
+
+/**
+ * cvmx_gser#_pcie_tx_vboost_lvl
+ *
+ * PCIE Tx Voltage Boost Level.
+ *
+ */
+union cvmx_gserx_pcie_tx_vboost_lvl {
+	uint64_t u64;
+	struct cvmx_gserx_pcie_tx_vboost_lvl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_3_63                : 61;
+	uint64_t tx_vboost_lvl                : 3;  /**< Controls the launch amplitude only when VPTX is less than the launch
+                                                         amplitude correspnding to tx_vboost_lvl.  Valid settings:
+                                                         - 011: Corresponds to a launch amplitude of 0.844V
+                                                         - 100: Corresponds to a launch amplitude of 1.008V
+                                                         - 101: Corresponds to a launch amplitude of 1.156V. */
+#else
+	uint64_t tx_vboost_lvl                : 3;
+	uint64_t reserved_3_63                : 61;
+#endif
+	} s;
+	struct cvmx_gserx_pcie_tx_vboost_lvl_s cn70xx;
+	struct cvmx_gserx_pcie_tx_vboost_lvl_s cn70xxp1;
+};
+typedef union cvmx_gserx_pcie_tx_vboost_lvl cvmx_gserx_pcie_tx_vboost_lvl_t;
+
+/**
+ * cvmx_gser#_phy#_idcode_hi
+ *
+ * PHY Version Hi.
+ *
+ */
+union cvmx_gserx_phyx_idcode_hi {
+	uint64_t u64;
+	struct cvmx_gserx_phyx_idcode_hi_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t idcode_hi                    : 16; /**< The PHY version high. */
+#else
+	uint64_t idcode_hi                    : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_gserx_phyx_idcode_hi_s    cn70xx;
+	struct cvmx_gserx_phyx_idcode_hi_s    cn70xxp1;
+};
+typedef union cvmx_gserx_phyx_idcode_hi cvmx_gserx_phyx_idcode_hi_t;
+
+/**
+ * cvmx_gser#_phy#_idcode_lo
+ *
+ * PHY Version Low.
+ *
+ */
+union cvmx_gserx_phyx_idcode_lo {
+	uint64_t u64;
+	struct cvmx_gserx_phyx_idcode_lo_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t idcode_lo                    : 16; /**< The PHY version low. */
+#else
+	uint64_t idcode_lo                    : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_gserx_phyx_idcode_lo_s    cn70xx;
+	struct cvmx_gserx_phyx_idcode_lo_s    cn70xxp1;
+};
+typedef union cvmx_gserx_phyx_idcode_lo cvmx_gserx_phyx_idcode_lo_t;
+
+/**
+ * cvmx_gser#_phy#_lane0_loopback
+ *
+ * PHY Lane 0 Loopback Control.
+ *
+ */
+union cvmx_gserx_phyx_lane0_loopback {
+	uint64_t u64;
+	struct cvmx_gserx_phyx_lane0_loopback_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_8_63                : 56;
+	uint64_t ovrd_tx_lb                   : 1;  /**< Enables override of tx_lb_en pin. */
+	uint64_t tx_lb_en_reg                 : 1;  /**< Value of tx_lb_en pin when OVRD_TX_LB is enabled. */
+	uint64_t atb_vptx                     : 1;  /**< Places vptx0 on atb_s_p and gd on atb_s_m. */
+	uint64_t atb_vreg_tx                  : 1;  /**< Places vreg_tx on atb_s_p and gd on atb_s_m. */
+	uint64_t atb_vdccp                    : 1;  /**< Places vddc_m on atb_s_p. */
+	uint64_t atb_vdccm                    : 1;  /**< Places vdcc_m on atb_s_m. */
+	uint64_t reserved_1_1                 : 1;
+	uint64_t sel_pmix_clk                 : 1;  /**< Selects pmix_clk for Tx clock for ATE test mode. */
+#else
+	uint64_t sel_pmix_clk                 : 1;
+	uint64_t reserved_1_1                 : 1;
+	uint64_t atb_vdccm                    : 1;
+	uint64_t atb_vdccp                    : 1;
+	uint64_t atb_vreg_tx                  : 1;
+	uint64_t atb_vptx                     : 1;
+	uint64_t tx_lb_en_reg                 : 1;
+	uint64_t ovrd_tx_lb                   : 1;
+	uint64_t reserved_8_63                : 56;
+#endif
+	} s;
+	struct cvmx_gserx_phyx_lane0_loopback_s cn70xx;
+	struct cvmx_gserx_phyx_lane0_loopback_s cn70xxp1;
+};
+typedef union cvmx_gserx_phyx_lane0_loopback cvmx_gserx_phyx_lane0_loopback_t;
+
+/**
+ * cvmx_gser#_phy#_lane0_rx_lbert_ctl
+ *
+ * PHY LANE0 RX LBERT Control.
+ *
+ */
+union cvmx_gserx_phyx_lane0_rx_lbert_ctl {
+	uint64_t u64;
+	struct cvmx_gserx_phyx_lane0_rx_lbert_ctl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_5_63                : 59;
+	uint64_t sync                         : 1;  /**< Synchronizes pattern matcher with incoming data.  A write of a 1
+                                                         to this bit resets the error counter and starts a synchronization of
+                                                         the PM.  Once this bit is set, there is no need to write the field back
+                                                         to a zero. */
+	uint64_t mode                         : 4;  /**< Pattern to match.  When changing modes, the field must be set to zero
+                                                          first.  This field should match what was configured for the TX LBERT
+                                                          Control register.
+                                                         - 0: disabled
+                                                         - 1: lfsr31     X^31 + X^28 + 1
+                                                         - 2: lfsr23     X^23 + X^18 + 1
+                                                         - 3: lfsr15     X^15 + X^14 + 1
+                                                         - 4: lfsr7      X^7 + X^6 + 1
+                                                         - 5: d[n] = d[n-10]
+                                                         - 6: d[n] = !d[n-10]
+                                                         - 7: d[n] = !d[n-20]
+                                                          - 15-8: Reserved. */
+#else
+	uint64_t mode                         : 4;
+	uint64_t sync                         : 1;
+	uint64_t reserved_5_63                : 59;
+#endif
+	} s;
+	struct cvmx_gserx_phyx_lane0_rx_lbert_ctl_s cn70xx;
+	struct cvmx_gserx_phyx_lane0_rx_lbert_ctl_s cn70xxp1;
+};
+typedef union cvmx_gserx_phyx_lane0_rx_lbert_ctl cvmx_gserx_phyx_lane0_rx_lbert_ctl_t;
+
+/**
+ * cvmx_gser#_phy#_lane0_rx_lbert_err
+ *
+ * PHY LANE0 RX LBERT Error.
+ * A read of this register, or a SYNC from the RX LBERT Control register
+ * resets the error count.  If all bits in this regisert are set, the
+ * error counter has saturated.
+ */
+union cvmx_gserx_phyx_lane0_rx_lbert_err {
+	uint64_t u64;
+	struct cvmx_gserx_phyx_lane0_rx_lbert_err_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t ov14                         : 1;  /**< If this bit is set, and COUNT[15] is also set, signals a overflow of counter. */
+	uint64_t count                        : 15; /**< Current error count if OV14 field is active, then multiply count
+                                                         by 128 to get the actual count. */
+#else
+	uint64_t count                        : 15;
+	uint64_t ov14                         : 1;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_gserx_phyx_lane0_rx_lbert_err_s cn70xx;
+	struct cvmx_gserx_phyx_lane0_rx_lbert_err_s cn70xxp1;
+};
+typedef union cvmx_gserx_phyx_lane0_rx_lbert_err cvmx_gserx_phyx_lane0_rx_lbert_err_t;
+
+/**
+ * cvmx_gser#_phy#_lane0_rx_ovrd_in_lo
+ *
+ * PHY LANE0 TX Override Input Low
+ *
+ */
+union cvmx_gserx_phyx_lane0_rx_ovrd_in_lo {
+	uint64_t u64;
+	struct cvmx_gserx_phyx_lane0_rx_ovrd_in_lo_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_14_63               : 50;
+	uint64_t rx_los_en_ovrd               : 1;  /**< Override enable for rx_los_en. */
+	uint64_t rx_los_en                    : 1;  /**< Override value for rx_los_en. */
+	uint64_t rx_term_en_ovrd              : 1;  /**< Override enable for rx_term_en. */
+	uint64_t rx_term_en                   : 1;  /**< Override value for rx_term_en. */
+	uint64_t rx_bit_shift_ovrd            : 1;  /**< Override enable for rx_bit_shift. */
+	uint64_t rx_bit_shift_en              : 1;  /**< Override value for rx_bit_shift. */
+	uint64_t rx_align_en_ovrd             : 1;  /**< Override enable for rx_align_en. */
+	uint64_t rx_align_en                  : 1;  /**< Override value for rx_align_en. */
+	uint64_t rx_data_en_ovrd              : 1;  /**< Override enable for rx_data_en. */
+	uint64_t rx_data_en                   : 1;  /**< Override value for rx_data_en. */
+	uint64_t rx_pll_en_ovrd               : 1;  /**< Override enable for rx_pll_en. */
+	uint64_t rx_pll_en                    : 1;  /**< Override value for rx_pll_en. */
+	uint64_t rx_invert_ovrd               : 1;  /**< Override enable for rx_invert. */
+	uint64_t rx_invert                    : 1;  /**< Override value for rx_invert. */
+#else
+	uint64_t rx_invert                    : 1;
+	uint64_t rx_invert_ovrd               : 1;
+	uint64_t rx_pll_en                    : 1;
+	uint64_t rx_pll_en_ovrd               : 1;
+	uint64_t rx_data_en                   : 1;
+	uint64_t rx_data_en_ovrd              : 1;
+	uint64_t rx_align_en                  : 1;
+	uint64_t rx_align_en_ovrd             : 1;
+	uint64_t rx_bit_shift_en              : 1;
+	uint64_t rx_bit_shift_ovrd            : 1;
+	uint64_t rx_term_en                   : 1;
+	uint64_t rx_term_en_ovrd              : 1;
+	uint64_t rx_los_en                    : 1;
+	uint64_t rx_los_en_ovrd               : 1;
+	uint64_t reserved_14_63               : 50;
+#endif
+	} s;
+	struct cvmx_gserx_phyx_lane0_rx_ovrd_in_lo_s cn70xx;
+	struct cvmx_gserx_phyx_lane0_rx_ovrd_in_lo_s cn70xxp1;
+};
+typedef union cvmx_gserx_phyx_lane0_rx_ovrd_in_lo cvmx_gserx_phyx_lane0_rx_ovrd_in_lo_t;
+
+/**
+ * cvmx_gser#_phy#_lane0_tx_lbert_ctl
+ *
+ * PHY LANE0 TX LBERT Control.
+ *
+ */
+union cvmx_gserx_phyx_lane0_tx_lbert_ctl {
+	uint64_t u64;
+	struct cvmx_gserx_phyx_lane0_tx_lbert_ctl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_15_63               : 49;
+	uint64_t pat0                         : 10; /**< 10-bit pattern for modes that use this field.  Ignored for
+                                                         other modes. */
+	uint64_t trig_err                     : 1;  /**< Single shot inversion of the LSB of the current symbol.
+                                                         Any write of 1 to this bit will insert an error. */
+	uint64_t mode                         : 4;  /**< Pattern to generate.  When changing modes, the field must be set to zero
+                                                          first.
+                                                         - 0: disabled
+                                                         - 1: lfsr31     X^31 + X^28 + 1
+                                                         - 2: lfsr23     X^23 + X^18 + 1
+                                                         - 3: lfsr15     X^15 + X^14 + 1
+                                                         - 4: lfsr7      X^7 + X^6 + 1
+                                                         - 5: Fixed word (PAT0)
+                                                         - 6: DC-balanced word (PAT0, ~PAT0)
+                                                         - 7: Word pattern (20-bit)
+                                                          - 15-8: Reserved. */
+#else
+	uint64_t mode                         : 4;
+	uint64_t trig_err                     : 1;
+	uint64_t pat0                         : 10;
+	uint64_t reserved_15_63               : 49;
+#endif
+	} s;
+	struct cvmx_gserx_phyx_lane0_tx_lbert_ctl_s cn70xx;
+	struct cvmx_gserx_phyx_lane0_tx_lbert_ctl_s cn70xxp1;
+};
+typedef union cvmx_gserx_phyx_lane0_tx_lbert_ctl cvmx_gserx_phyx_lane0_tx_lbert_ctl_t;
+
+/**
+ * cvmx_gser#_phy#_lane0_tx_ovrd_in_hi
+ *
+ * PHY LANE0 TX Override Input High
+ *
+ */
+union cvmx_gserx_phyx_lane0_tx_ovrd_in_hi {
+	uint64_t u64;
+	struct cvmx_gserx_phyx_lane0_tx_ovrd_in_hi_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_10_63               : 54;
+	uint64_t tx_vboost_en_ovrd            : 1;  /**< Override enable for tx_vboost_en. */
+	uint64_t tx_vboost_en                 : 1;  /**< Override value for tx_vboost_en. */
+	uint64_t tx_reset_ovrd                : 1;  /**< Override enable for tx_reset. */
+	uint64_t tx_reset                     : 1;  /**< Override value for tx_reset. */
+	uint64_t tx_nyquist_data              : 1;  /**< Overrides incoming data to nyquist. */
+	uint64_t tx_clk_out_en_ovrd           : 1;  /**< Override enable for tx_clk_out_en. */
+	uint64_t tx_clk_out_en                : 1;  /**< Override value for tx_clk_out_en. */
+	uint64_t tx_rate_ovrd                 : 1;  /**< Override enable for tx lane rate. */
+	uint64_t tx_rate                      : 2;  /**< Override value for tx_rate. */
+#else
+	uint64_t tx_rate                      : 2;
+	uint64_t tx_rate_ovrd                 : 1;
+	uint64_t tx_clk_out_en                : 1;
+	uint64_t tx_clk_out_en_ovrd           : 1;
+	uint64_t tx_nyquist_data              : 1;
+	uint64_t tx_reset                     : 1;
+	uint64_t tx_reset_ovrd                : 1;
+	uint64_t tx_vboost_en                 : 1;
+	uint64_t tx_vboost_en_ovrd            : 1;
+	uint64_t reserved_10_63               : 54;
+#endif
+	} s;
+	struct cvmx_gserx_phyx_lane0_tx_ovrd_in_hi_s cn70xx;
+	struct cvmx_gserx_phyx_lane0_tx_ovrd_in_hi_s cn70xxp1;
+};
+typedef union cvmx_gserx_phyx_lane0_tx_ovrd_in_hi cvmx_gserx_phyx_lane0_tx_ovrd_in_hi_t;
+
+/**
+ * cvmx_gser#_phy#_lane0_tx_ovrd_in_lo
+ *
+ * PHY LANE0 TX Override Input Low
+ *
+ */
+union cvmx_gserx_phyx_lane0_tx_ovrd_in_lo {
+	uint64_t u64;
+	struct cvmx_gserx_phyx_lane0_tx_ovrd_in_lo_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_12_63               : 52;
+	uint64_t tx_beacon_en_ovrd            : 1;  /**< Override enable for tx_beacon_en. */
+	uint64_t tx_beacon_en                 : 1;  /**< Override value for tx_beacon_en. */
+	uint64_t tx_cm_en_ovrd                : 1;  /**< Override enable for tx_cm_en. */
+	uint64_t tx_cm_en                     : 1;  /**< Override value for tx_cm_en. */
+	uint64_t tx_en_ovrd                   : 1;  /**< Override enable for tx_en. */
+	uint64_t tx_en                        : 1;  /**< Override value for tx_en. */
+	uint64_t tx_data_en_ovrd              : 1;  /**< Override enable for tx_data_en. */
+	uint64_t tx_data_en                   : 1;  /**< Override value for tx_data_en. */
+	uint64_t tx_invert_ovrd               : 1;  /**< Override enable for tx_invert. */
+	uint64_t tx_invert                    : 1;  /**< Override value for tx_invert. */
+	uint64_t loopbk_en_ovrd               : 1;  /**< Override enable for loopbk_en. */
+	uint64_t loopbk_en                    : 1;  /**< Override value for loopbk_en. */
+#else
+	uint64_t loopbk_en                    : 1;
+	uint64_t loopbk_en_ovrd               : 1;
+	uint64_t tx_invert                    : 1;
+	uint64_t tx_invert_ovrd               : 1;
+	uint64_t tx_data_en                   : 1;
+	uint64_t tx_data_en_ovrd              : 1;
+	uint64_t tx_en                        : 1;
+	uint64_t tx_en_ovrd                   : 1;
+	uint64_t tx_cm_en                     : 1;
+	uint64_t tx_cm_en_ovrd                : 1;
+	uint64_t tx_beacon_en                 : 1;
+	uint64_t tx_beacon_en_ovrd            : 1;
+	uint64_t reserved_12_63               : 52;
+#endif
+	} s;
+	struct cvmx_gserx_phyx_lane0_tx_ovrd_in_lo_s cn70xx;
+	struct cvmx_gserx_phyx_lane0_tx_ovrd_in_lo_s cn70xxp1;
+};
+typedef union cvmx_gserx_phyx_lane0_tx_ovrd_in_lo cvmx_gserx_phyx_lane0_tx_ovrd_in_lo_t;
+
+/**
+ * cvmx_gser#_phy#_lane0_txdebug
+ *
+ * PHY LANE0 TX DEBUG.
+ *
+ */
+union cvmx_gserx_phyx_lane0_txdebug {
+	uint64_t u64;
+	struct cvmx_gserx_phyx_lane0_txdebug_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_12_63               : 52;
+	uint64_t rxdet_meas_time              : 8;  /**< Time to wait for rxdet measurement. */
+	uint64_t detrx_always                 : 1;  /**< Always signals 1 for rx_detect ignoring analog. */
+	uint64_t dtb_sel                      : 3;  /**< Selects data to drive on the DTB. */
+#else
+	uint64_t dtb_sel                      : 3;
+	uint64_t detrx_always                 : 1;
+	uint64_t rxdet_meas_time              : 8;
+	uint64_t reserved_12_63               : 52;
+#endif
+	} s;
+	struct cvmx_gserx_phyx_lane0_txdebug_s cn70xx;
+	struct cvmx_gserx_phyx_lane0_txdebug_s cn70xxp1;
+};
+typedef union cvmx_gserx_phyx_lane0_txdebug cvmx_gserx_phyx_lane0_txdebug_t;
+
+/**
+ * cvmx_gser#_phy#_lane1_loopback
+ *
+ * PHY Lane 1 Loopback Control.
+ *
+ */
+union cvmx_gserx_phyx_lane1_loopback {
+	uint64_t u64;
+	struct cvmx_gserx_phyx_lane1_loopback_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_8_63                : 56;
+	uint64_t ovrd_tx_lb                   : 1;  /**< Enables override of tx_lb_en pin. */
+	uint64_t tx_lb_en_reg                 : 1;  /**< Value of tx_lb_en pin when OVRD_TX_LB is enabled. */
+	uint64_t atb_vptx                     : 1;  /**< Places vptx0 on atb_s_p and gd on atb_s_m. */
+	uint64_t atb_vreg_tx                  : 1;  /**< Places vreg_tx on atb_s_p and gd on atb_s_m. */
+	uint64_t atb_vdccp                    : 1;  /**< Places vddc_m on atb_s_p. */
+	uint64_t atb_vdccm                    : 1;  /**< Places vdcc_m on atb_s_m. */
+	uint64_t reserved_1_1                 : 1;
+	uint64_t sel_pmix_clk                 : 1;  /**< Selects pmix_clk for Tx clock for ATE test mode. */
+#else
+	uint64_t sel_pmix_clk                 : 1;
+	uint64_t reserved_1_1                 : 1;
+	uint64_t atb_vdccm                    : 1;
+	uint64_t atb_vdccp                    : 1;
+	uint64_t atb_vreg_tx                  : 1;
+	uint64_t atb_vptx                     : 1;
+	uint64_t tx_lb_en_reg                 : 1;
+	uint64_t ovrd_tx_lb                   : 1;
+	uint64_t reserved_8_63                : 56;
+#endif
+	} s;
+	struct cvmx_gserx_phyx_lane1_loopback_s cn70xx;
+	struct cvmx_gserx_phyx_lane1_loopback_s cn70xxp1;
+};
+typedef union cvmx_gserx_phyx_lane1_loopback cvmx_gserx_phyx_lane1_loopback_t;
+
+/**
+ * cvmx_gser#_phy#_lane1_rx_lbert_ctl
+ *
+ * PHY LANE1 TX LBERT Control.
+ *
+ */
+union cvmx_gserx_phyx_lane1_rx_lbert_ctl {
+	uint64_t u64;
+	struct cvmx_gserx_phyx_lane1_rx_lbert_ctl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_5_63                : 59;
+	uint64_t sync                         : 1;  /**< Synchronizes pattern matcher with incoming data.  A write of a 1
+                                                         to this bit resets the error counter and starts a synchronization of
+                                                         the PM.  Once this bit is set, there is no need to write the field back
+                                                         to a zero. */
+	uint64_t mode                         : 4;  /**< Pattern to match.  When changing modes, the field must be set to zero
+                                                          first.  This field should match what was configured for the TX LBERT
+                                                          Control register.
+                                                         - 0: disabled
+                                                         - 1: lfsr31     X^31 + X^28 + 1
+                                                         - 2: lfsr23     X^23 + X^18 + 1
+                                                         - 3: lfsr15     X^15 + X^14 + 1
+                                                         - 4: lfsr7      X^7 + X^6 + 1
+                                                         - 5: d[n] = d[n-10]
+                                                         - 6: d[n] = !d[n-10]
+                                                         - 7: d[n] = !d[n-20]
+                                                          - 15-8: Reserved. */
+#else
+	uint64_t mode                         : 4;
+	uint64_t sync                         : 1;
+	uint64_t reserved_5_63                : 59;
+#endif
+	} s;
+	struct cvmx_gserx_phyx_lane1_rx_lbert_ctl_s cn70xx;
+	struct cvmx_gserx_phyx_lane1_rx_lbert_ctl_s cn70xxp1;
+};
+typedef union cvmx_gserx_phyx_lane1_rx_lbert_ctl cvmx_gserx_phyx_lane1_rx_lbert_ctl_t;
+
+/**
+ * cvmx_gser#_phy#_lane1_rx_lbert_err
+ *
+ * PHY LANE1 RX LBERT Error.
+ * A read of this register, or a SYNC from the RX LBERT Control register
+ * resets the error count.  If all bits in this regisert are set, the
+ * error counter has saturated.
+ */
+union cvmx_gserx_phyx_lane1_rx_lbert_err {
+	uint64_t u64;
+	struct cvmx_gserx_phyx_lane1_rx_lbert_err_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t ov14                         : 1;  /**< If this bit is set, and COUNT[15] is also set, signals a overflow of counter. */
+	uint64_t count                        : 15; /**< Current error count if OV14 field is active, then multiply count
+                                                         by 128 to get the actual count. */
+#else
+	uint64_t count                        : 15;
+	uint64_t ov14                         : 1;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_gserx_phyx_lane1_rx_lbert_err_s cn70xx;
+	struct cvmx_gserx_phyx_lane1_rx_lbert_err_s cn70xxp1;
+};
+typedef union cvmx_gserx_phyx_lane1_rx_lbert_err cvmx_gserx_phyx_lane1_rx_lbert_err_t;
+
+/**
+ * cvmx_gser#_phy#_lane1_rx_ovrd_in_lo
+ *
+ * PHY LANE1 TX Override Input Low
+ *
+ */
+union cvmx_gserx_phyx_lane1_rx_ovrd_in_lo {
+	uint64_t u64;
+	struct cvmx_gserx_phyx_lane1_rx_ovrd_in_lo_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_14_63               : 50;
+	uint64_t rx_los_en_ovrd               : 1;  /**< Override enable for rx_los_en. */
+	uint64_t rx_los_en                    : 1;  /**< Override value for rx_los_en. */
+	uint64_t rx_term_en_ovrd              : 1;  /**< Override enable for rx_term_en. */
+	uint64_t rx_term_en                   : 1;  /**< Override value for rx_term_en. */
+	uint64_t rx_bit_shift_ovrd            : 1;  /**< Override enable for rx_bit_shift. */
+	uint64_t rx_bit_shift_en              : 1;  /**< Override value for rx_bit_shift. */
+	uint64_t rx_align_en_ovrd             : 1;  /**< Override enable for rx_align_en. */
+	uint64_t rx_align_en                  : 1;  /**< Override value for rx_align_en. */
+	uint64_t rx_data_en_ovrd              : 1;  /**< Override enable for rx_data_en. */
+	uint64_t rx_data_en                   : 1;  /**< Override value for rx_data_en. */
+	uint64_t rx_pll_en_ovrd               : 1;  /**< Override enable for rx_pll_en. */
+	uint64_t rx_pll_en                    : 1;  /**< Override value for rx_pll_en. */
+	uint64_t rx_invert_ovrd               : 1;  /**< Override enable for rx_invert. */
+	uint64_t rx_invert                    : 1;  /**< Override value for rx_invert. */
+#else
+	uint64_t rx_invert                    : 1;
+	uint64_t rx_invert_ovrd               : 1;
+	uint64_t rx_pll_en                    : 1;
+	uint64_t rx_pll_en_ovrd               : 1;
+	uint64_t rx_data_en                   : 1;
+	uint64_t rx_data_en_ovrd              : 1;
+	uint64_t rx_align_en                  : 1;
+	uint64_t rx_align_en_ovrd             : 1;
+	uint64_t rx_bit_shift_en              : 1;
+	uint64_t rx_bit_shift_ovrd            : 1;
+	uint64_t rx_term_en                   : 1;
+	uint64_t rx_term_en_ovrd              : 1;
+	uint64_t rx_los_en                    : 1;
+	uint64_t rx_los_en_ovrd               : 1;
+	uint64_t reserved_14_63               : 50;
+#endif
+	} s;
+	struct cvmx_gserx_phyx_lane1_rx_ovrd_in_lo_s cn70xx;
+	struct cvmx_gserx_phyx_lane1_rx_ovrd_in_lo_s cn70xxp1;
+};
+typedef union cvmx_gserx_phyx_lane1_rx_ovrd_in_lo cvmx_gserx_phyx_lane1_rx_ovrd_in_lo_t;
+
+/**
+ * cvmx_gser#_phy#_lane1_tx_lbert_ctl
+ *
+ * PHY LANE1 RX LBERT Control.
+ *
+ */
+union cvmx_gserx_phyx_lane1_tx_lbert_ctl {
+	uint64_t u64;
+	struct cvmx_gserx_phyx_lane1_tx_lbert_ctl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_15_63               : 49;
+	uint64_t pat0                         : 10; /**< 10-bit pattern for modes that use this field.  Ignored for
+                                                         other modes. */
+	uint64_t trig_err                     : 1;  /**< Single shot inversion of the LSB of the current symbol.
+                                                         Any write of 1 to this bit will insert an error. */
+	uint64_t mode                         : 4;  /**< Pattern to generate.  When changing modes, the field must be set to zero
+                                                          first.
+                                                         - 0: disabled
+                                                         - 1: lfsr31     X^31 + X^28 + 1
+                                                         - 2: lfsr23     X^23 + X^18 + 1
+                                                         - 3: lfsr15     X^15 + X^14 + 1
+                                                         - 4: lfsr7      X^7 + X^6 + 1
+                                                         - 5: Fixed word (PAT0)
+                                                         - 6: DC-balanced word (PAT0, ~PAT0)
+                                                         - 7: Word pattern (20-bit)
+                                                          - 15-8: Reserved. */
+#else
+	uint64_t mode                         : 4;
+	uint64_t trig_err                     : 1;
+	uint64_t pat0                         : 10;
+	uint64_t reserved_15_63               : 49;
+#endif
+	} s;
+	struct cvmx_gserx_phyx_lane1_tx_lbert_ctl_s cn70xx;
+	struct cvmx_gserx_phyx_lane1_tx_lbert_ctl_s cn70xxp1;
+};
+typedef union cvmx_gserx_phyx_lane1_tx_lbert_ctl cvmx_gserx_phyx_lane1_tx_lbert_ctl_t;
+
+/**
+ * cvmx_gser#_phy#_lane1_tx_ovrd_in_hi
+ *
+ * PHY LANE1 TX Override Input High
+ *
+ */
+union cvmx_gserx_phyx_lane1_tx_ovrd_in_hi {
+	uint64_t u64;
+	struct cvmx_gserx_phyx_lane1_tx_ovrd_in_hi_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_10_63               : 54;
+	uint64_t tx_vboost_en_ovrd            : 1;  /**< Override enable for tx_vboost_en. */
+	uint64_t tx_vboost_en                 : 1;  /**< Override value for tx_vboost_en. */
+	uint64_t tx_reset_ovrd                : 1;  /**< Override enable for tx_reset. */
+	uint64_t tx_reset                     : 1;  /**< Override value for tx_reset. */
+	uint64_t tx_nyquist_data              : 1;  /**< Overrides incoming data to nyquist. */
+	uint64_t tx_clk_out_en_ovrd           : 1;  /**< Override enable for tx_clk_out_en. */
+	uint64_t tx_clk_out_en                : 1;  /**< Override value for tx_clk_out_en. */
+	uint64_t tx_rate_ovrd                 : 1;  /**< Override enable for tx lane rate. */
+	uint64_t tx_rate                      : 2;  /**< Override value for tx_rate. */
+#else
+	uint64_t tx_rate                      : 2;
+	uint64_t tx_rate_ovrd                 : 1;
+	uint64_t tx_clk_out_en                : 1;
+	uint64_t tx_clk_out_en_ovrd           : 1;
+	uint64_t tx_nyquist_data              : 1;
+	uint64_t tx_reset                     : 1;
+	uint64_t tx_reset_ovrd                : 1;
+	uint64_t tx_vboost_en                 : 1;
+	uint64_t tx_vboost_en_ovrd            : 1;
+	uint64_t reserved_10_63               : 54;
+#endif
+	} s;
+	struct cvmx_gserx_phyx_lane1_tx_ovrd_in_hi_s cn70xx;
+	struct cvmx_gserx_phyx_lane1_tx_ovrd_in_hi_s cn70xxp1;
+};
+typedef union cvmx_gserx_phyx_lane1_tx_ovrd_in_hi cvmx_gserx_phyx_lane1_tx_ovrd_in_hi_t;
+
+/**
+ * cvmx_gser#_phy#_lane1_tx_ovrd_in_lo
+ *
+ * PHY LANE1 TX Override Input Low
+ *
+ */
+union cvmx_gserx_phyx_lane1_tx_ovrd_in_lo {
+	uint64_t u64;
+	struct cvmx_gserx_phyx_lane1_tx_ovrd_in_lo_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_12_63               : 52;
+	uint64_t tx_beacon_en_ovrd            : 1;  /**< Override enable for tx_beacon_en. */
+	uint64_t tx_beacon_en                 : 1;  /**< Override value for tx_beacon_en. */
+	uint64_t tx_cm_en_ovrd                : 1;  /**< Override enable for tx_cm_en. */
+	uint64_t tx_cm_en                     : 1;  /**< Override value for tx_cm_en. */
+	uint64_t tx_en_ovrd                   : 1;  /**< Override enable for tx_en. */
+	uint64_t tx_en                        : 1;  /**< Override value for tx_en. */
+	uint64_t tx_data_en_ovrd              : 1;  /**< Override enable for tx_data_en. */
+	uint64_t tx_data_en                   : 1;  /**< Override value for tx_data_en. */
+	uint64_t tx_invert_ovrd               : 1;  /**< Override enable for tx_invert. */
+	uint64_t tx_invert                    : 1;  /**< Override value for tx_invert. */
+	uint64_t loopbk_en_ovrd               : 1;  /**< Override enable for loopbk_en. */
+	uint64_t loopbk_en                    : 1;  /**< Override value for loopbk_en. */
+#else
+	uint64_t loopbk_en                    : 1;
+	uint64_t loopbk_en_ovrd               : 1;
+	uint64_t tx_invert                    : 1;
+	uint64_t tx_invert_ovrd               : 1;
+	uint64_t tx_data_en                   : 1;
+	uint64_t tx_data_en_ovrd              : 1;
+	uint64_t tx_en                        : 1;
+	uint64_t tx_en_ovrd                   : 1;
+	uint64_t tx_cm_en                     : 1;
+	uint64_t tx_cm_en_ovrd                : 1;
+	uint64_t tx_beacon_en                 : 1;
+	uint64_t tx_beacon_en_ovrd            : 1;
+	uint64_t reserved_12_63               : 52;
+#endif
+	} s;
+	struct cvmx_gserx_phyx_lane1_tx_ovrd_in_lo_s cn70xx;
+	struct cvmx_gserx_phyx_lane1_tx_ovrd_in_lo_s cn70xxp1;
+};
+typedef union cvmx_gserx_phyx_lane1_tx_ovrd_in_lo cvmx_gserx_phyx_lane1_tx_ovrd_in_lo_t;
+
+/**
+ * cvmx_gser#_phy#_lane1_txdebug
+ *
+ * PHY LANE1 TX DEBUG.
+ *
+ */
+union cvmx_gserx_phyx_lane1_txdebug {
+	uint64_t u64;
+	struct cvmx_gserx_phyx_lane1_txdebug_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_12_63               : 52;
+	uint64_t rxdet_meas_time              : 8;  /**< Time to wait for rxdet measurement. */
+	uint64_t detrx_always                 : 1;  /**< Always signals 1 for rx_detect ignoring analog. */
+	uint64_t dtb_sel                      : 3;  /**< Selects data to drive on the DTB. */
+#else
+	uint64_t dtb_sel                      : 3;
+	uint64_t detrx_always                 : 1;
+	uint64_t rxdet_meas_time              : 8;
+	uint64_t reserved_12_63               : 52;
+#endif
+	} s;
+	struct cvmx_gserx_phyx_lane1_txdebug_s cn70xx;
+	struct cvmx_gserx_phyx_lane1_txdebug_s cn70xxp1;
+};
+typedef union cvmx_gserx_phyx_lane1_txdebug cvmx_gserx_phyx_lane1_txdebug_t;
+
+/**
+ * cvmx_gser#_phy#_ovrd_in_lo
+ *
+ * PHY Overide Input Low Register.
+ *
+ */
+union cvmx_gserx_phyx_ovrd_in_lo {
+	uint64_t u64;
+	struct cvmx_gserx_phyx_ovrd_in_lo_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t res_ack_in_ovrd              : 1;  /**< Overide enable for RES_ACK_IN input.
+                                                         It is not expected SW will need to set this bit. */
+	uint64_t res_ack_in                   : 1;  /**< Overide value for RES_ACK_IN input.
+                                                         It is not expected SW will need to set this bit. */
+	uint64_t res_req_in_ovrd              : 1;  /**< Overide enable for RES_REW_IN input.
+                                                         It is not expected SW will need to set this bit. */
+	uint64_t res_req_in                   : 1;  /**< Overide value for RES_REQ_IN input.
+                                                         It is not expected SW will need to set this bit. */
+	uint64_t rtune_req_ovrd               : 1;  /**< Overide enable for RTUNE_REQ input.
+                                                         It is not expected SW will need to set this bit. */
+	uint64_t rtune_req                    : 1;  /**< Overide value for RTUNE_REQ input.
+                                                         It is not expected SW will need to set this bit. */
+	uint64_t mpll_multiplier_ovrd         : 1;  /**< Overide enable for MPLL_MULTIPLIER.
+                                                         It is not expected SW will need to set this bit. */
+	uint64_t mpll_multiplier              : 7;  /**< Overide value for MPLL_MULTIPLIER inputs.
+                                                         It is not expected SW will need to set these bits. */
+	uint64_t mpll_en_ovrd                 : 1;  /**< Overide enable for MPLL_EN input.
+                                                         For EP Mode PEMs, SW should set this bit after reset. */
+	uint64_t mpll_en                      : 1;  /**< Overide value for MPLL_EN input.
+                                                         For EP Mode PEMs, SW should set this bit after reset. */
+#else
+	uint64_t mpll_en                      : 1;
+	uint64_t mpll_en_ovrd                 : 1;
+	uint64_t mpll_multiplier              : 7;
+	uint64_t mpll_multiplier_ovrd         : 1;
+	uint64_t rtune_req                    : 1;
+	uint64_t rtune_req_ovrd               : 1;
+	uint64_t res_req_in                   : 1;
+	uint64_t res_req_in_ovrd              : 1;
+	uint64_t res_ack_in                   : 1;
+	uint64_t res_ack_in_ovrd              : 1;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_gserx_phyx_ovrd_in_lo_s   cn70xx;
+	struct cvmx_gserx_phyx_ovrd_in_lo_s   cn70xxp1;
+};
+typedef union cvmx_gserx_phyx_ovrd_in_lo cvmx_gserx_phyx_ovrd_in_lo_t;
+
+/**
+ * cvmx_gser#_phy_ctl
+ *
+ * This register contains general PHY/PLL control of the RAW PCS.
+ * These registers are only reset by hardware during chip cold reset. The values of the CSR
+ * fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_phy_ctl {
+	uint64_t u64;
+	struct cvmx_gserx_phy_ctl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_2_63                : 62;
+	uint64_t phy_reset                    : 1;  /**< When asserted, the PHY is held in reset. This bit is initialized as follows:
+                                                         0 = (not reset) = Bootable PCIe, or OCI when GSER(8..13)_SPD[SPD] comes up in a bootable
+                                                         mode.
+                                                         1 = (reset) =  Non-bootable PCIe, BGX/ILK, or OCI when GSER(8..13)_SPD[SPD] comes up in
+                                                         SW_MODE. */
+	uint64_t phy_pd                       : 1;  /**< When asserted, the PHY is powered down. */
+#else
+	uint64_t phy_pd                       : 1;
+	uint64_t phy_reset                    : 1;
+	uint64_t reserved_2_63                : 62;
+#endif
+	} s;
+	struct cvmx_gserx_phy_ctl_s           cn78xx;
+};
+typedef union cvmx_gserx_phy_ctl cvmx_gserx_phy_ctl_t;
+
+/**
+ * cvmx_gser#_pipe_lpbk
+ */
+union cvmx_gserx_pipe_lpbk {
+	uint64_t u64;
+	struct cvmx_gserx_pipe_lpbk_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t pcie_lpbk                    : 1;  /**< For links that are in PCIE mode, places the PHY in serial loopback mode, where the
+                                                         QLMn_TXN/QLMn_TXP data are looped back to the QLMn_RXN/QLMn_RXP.
+                                                         This register has no meaning for links that don't support PCIe i.e. GSER(5..13). */
+#else
+	uint64_t pcie_lpbk                    : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_gserx_pipe_lpbk_s         cn78xx;
+};
+typedef union cvmx_gserx_pipe_lpbk cvmx_gserx_pipe_lpbk_t;
+
+/**
+ * cvmx_gser#_pll_p#_mode_0
+ *
+ * These are the RAW PCS PLL global settings mode 0 registers. There is one register per GSER per
+ * GSER_LMODE_E value (0..11). Only one entry is used at any given time in a given GSER - the one
+ * selected by the corresponding GSER()_LANE_MODE[LMODE].
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during subsequent chip warm or
+ * soft resets.
+ */
+union cvmx_gserx_pll_px_mode_0 {
+	uint64_t u64;
+	struct cvmx_gserx_pll_px_mode_0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t pll_icp                      : 4;  /**< PLL charge pump enable.
+                                                         Recommended settings, which are based on the reference clock speed:
+                                                         <pre>
+                                                                  100MHz 125MHz 156.25MHz
+                                                         1.25G:    0x1    0x1    0x1
+                                                         2.5G:     0x4    0x3    0x3
+                                                         3.125G:   NS     0x1    0x1
+                                                         5.0G:     0x4    0x3    0x3
+                                                         6.25G:    NS     0x1    0x1
+                                                         8.0G:     0x3    0x2    NS
+                                                         10.3125G: NS     NS     0x1
+                                                         </pre>
+                                                         A 'NS' indicates that the rate is not supported at the specified reference clock. */
+	uint64_t pll_rloop                    : 3;  /**< Loop resistor tuning.
+                                                         Recommended settings:
+                                                         _ 1.25G:    0x3
+                                                         _ 2.5G:     0x3
+                                                         _ 3.125G:   0x3
+                                                         _ 5.0G:     0x3
+                                                         _ 6.25G:    0x3
+                                                         _ 8.0G:     0x5
+                                                         _ 10.3125G: 0x5 */
+	uint64_t pll_pcs_div                  : 9;  /**< The divider that generates PCS_MAC_TX_CLK. The frequency of the clock is (pll_frequency /
+                                                         PLL_PCS_DIV).
+                                                         Recommended settings:
+                                                         _ 1.25G:    0x28
+                                                         _ 2.5G:     0x5
+                                                         _ 3.125G:   0x14
+                                                         _ 5.0G:     0xA
+                                                         _ 6.25G:    0xA
+                                                         _ 8.0G:     0xA
+                                                         _ 10.3125G: 0xA */
+#else
+	uint64_t pll_pcs_div                  : 9;
+	uint64_t pll_rloop                    : 3;
+	uint64_t pll_icp                      : 4;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_gserx_pll_px_mode_0_s     cn78xx;
+};
+typedef union cvmx_gserx_pll_px_mode_0 cvmx_gserx_pll_px_mode_0_t;
+
+/**
+ * cvmx_gser#_pll_p#_mode_1
+ *
+ * These are the RAW PCS PLL global settings mode 1 registers. There is one register per GSER per
+ * GSER_LMODE_E value (0..11). Only one entry is used at any given time in a given GSER - the one
+ * selected by the corresponding GSER()_LANE_MODE[LMODE].
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in this register do not change during subsequent chip warm or
+ * soft resets.
+ */
+union cvmx_gserx_pll_px_mode_1 {
+	uint64_t u64;
+	struct cvmx_gserx_pll_px_mode_1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_14_63               : 50;
+	uint64_t pll_16p5en                   : 1;  /**< Enable for the DIV 16.5 divided down clock.
+                                                         Recommended settings, based on the reference clock speed:
+                                                         <pre>
+                                                                  100MHz 125MHz 156.25MHz
+                                                         1.25G:    0x1    0x1     0x1
+                                                         2.5G:     0x0    0x0     0x0
+                                                         3.125G:   NS     0x1     0x1
+                                                         5.0G:     0x0    0x0     0x0
+                                                         6.25G:    NS     0x0     0x0
+                                                         8.0G:     0x0    0x0     NS
+                                                         10.3125G: NS     NS      0x1
+                                                         </pre>
+                                                         A 'NS' indicates that the rate is not supported at the specified reference clock. */
+	uint64_t pll_cpadj                    : 2;  /**< PLL charge adjust.
+                                                         Recommended settings, based on the reference clock speed:
+                                                         <pre>
+                                                                   100MHz 125MHz 156.25MHz
+                                                         1.25G:     0x2     0x2    0x3
+                                                         2.5G:      0x2     0x1    0x2
+                                                         3.125G:    NS      0x2    0x2
+                                                         5.0G:      0x2     0x1    0x2
+                                                         6.25G:     NS      0x2    0x2
+                                                         8.0G:      0x2     0x1    NS
+                                                         10.3125G:  NS      NS     0x2
+                                                         </pre>
+                                                         A 'NS' indicates that the rate is not supported at the specified reference clock. */
+	uint64_t pll_pcie3en                  : 1;  /**< Enable PCIE3 mode. Recommended settings:
+                                                         0 = Any rate other than 8 Gbaud.
+                                                         1 = Rate is equal to 8 Gbaud. */
+	uint64_t pll_opr                      : 1;  /**< PLL op range:
+                                                         0 = Use Ring Oscillator VCO. Recommended for rates 6.25 Gbaud and lower.
+                                                         1 = Use LC-tank VCO. Recommended for rates 8 Gbaud and higher. */
+	uint64_t pll_div                      : 9;  /**< PLL divider in feedback path which sets the PLL frequency.
+                                                         Recommended settings:
+                                                         <pre>
+                                                                  100MHz 125MHz 156.25MHz
+                                                         1.25G:    0x19   0x14    0x10
+                                                         2.5G:     0x19   0x14    0x10
+                                                         3.125G:   NS     0x19    0x14
+                                                         5.0G:     0x19   0x14    0x10
+                                                         6.25G:    NS     0x19    0x14
+                                                         8.0G:     0x28   0x20    NS
+                                                         10.3125G: NS     NS      0x21
+                                                         </pre>
+                                                         A 'NS' indicates that the rate is not supported at the specified reference clock. */
+#else
+	uint64_t pll_div                      : 9;
+	uint64_t pll_opr                      : 1;
+	uint64_t pll_pcie3en                  : 1;
+	uint64_t pll_cpadj                    : 2;
+	uint64_t pll_16p5en                   : 1;
+	uint64_t reserved_14_63               : 50;
+#endif
+	} s;
+	struct cvmx_gserx_pll_px_mode_1_s     cn78xx;
+};
+typedef union cvmx_gserx_pll_px_mode_1 cvmx_gserx_pll_px_mode_1_t;
+
+/**
+ * cvmx_gser#_pll_stat
+ */
+union cvmx_gserx_pll_stat {
+	uint64_t u64;
+	struct cvmx_gserx_pll_stat_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t pll_lock                     : 1;  /**< When set, indicates that the PHY PLL is locked. */
+#else
+	uint64_t pll_lock                     : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_gserx_pll_stat_s          cn78xx;
+};
+typedef union cvmx_gserx_pll_stat cvmx_gserx_pll_stat_t;
+
+/**
+ * cvmx_gser#_qlm_stat
+ */
+union cvmx_gserx_qlm_stat {
+	uint64_t u64;
+	struct cvmx_gserx_qlm_stat_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_2_63                : 62;
+	uint64_t rst_rdy                      : 1;  /**< When asserted, the QLM is configured and the PLLs are stable. The GSER
+                                                         is ready to accept TX traffic from the MAC. */
+	uint64_t dcok                         : 1;  /**< When asserted, there is a PLL reference clock indicating there is power to the QLM. */
+#else
+	uint64_t dcok                         : 1;
+	uint64_t rst_rdy                      : 1;
+	uint64_t reserved_2_63                : 62;
+#endif
+	} s;
+	struct cvmx_gserx_qlm_stat_s          cn78xx;
+};
+typedef union cvmx_gserx_qlm_stat cvmx_gserx_qlm_stat_t;
+
+/**
+ * cvmx_gser#_rdet_time
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_rdet_time {
+	uint64_t u64;
+	struct cvmx_gserx_rdet_time_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t rdet_time_3                  : 4;  /**< Determines the time allocated for disabling the RX detect
+                                                         circuit, and returning to common mode. */
+	uint64_t rdet_time_2                  : 4;  /**< Determines the time allocated for the RX detect circuit to
+                                                         detect a receiver. */
+	uint64_t rdet_time_1                  : 8;  /**< Determines the time allocated for enabling the RX detect circuit. */
+#else
+	uint64_t rdet_time_1                  : 8;
+	uint64_t rdet_time_2                  : 4;
+	uint64_t rdet_time_3                  : 4;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_gserx_rdet_time_s         cn78xx;
+};
+typedef union cvmx_gserx_rdet_time cvmx_gserx_rdet_time_t;
+
+/**
+ * cvmx_gser#_refclk_sel
+ *
+ * This register selects the reference clock.
+ * These registers are only reset by hardware during chip cold reset. The values of the CSR
+ * fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_refclk_sel {
+	uint64_t u64;
+	struct cvmx_gserx_refclk_sel_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_3_63                : 61;
+	uint64_t pcie_refclk125               : 1;  /**< For bootable PCIe links, this is loaded with
+                                                         PCIE0/2_REFCLK_125 at cold reset and indicates a 125 MHz reference clock when set. For
+                                                         non-bootable PCIe links, this bit is set to zero at cold reset and indicates a 100 MHz
+                                                         reference clock. It is not used for non-PCIe links. */
+	uint64_t com_clk_sel                  : 1;  /**< When set, the reference clock is sourced from the external clock mux. For bootable PCIe
+                                                         links, this bit is loaded with the PCIEn_COM0_CLK_EN pin at cold reset. */
+	uint64_t use_com1                     : 1;  /**< For non-OCI links, this bit controls the external mux select. When set, QLMC_REF_CLK1_N/P
+                                                         are selected as the reference clock. When clear, QLMC_REF_CLK0_N/P are selected as the
+                                                         reference clock. */
+#else
+	uint64_t use_com1                     : 1;
+	uint64_t com_clk_sel                  : 1;
+	uint64_t pcie_refclk125               : 1;
+	uint64_t reserved_3_63                : 61;
+#endif
+	} s;
+	struct cvmx_gserx_refclk_sel_s        cn78xx;
+};
+typedef union cvmx_gserx_refclk_sel cvmx_gserx_refclk_sel_t;
+
+/**
+ * cvmx_gser#_rx_coast
+ *
+ * These registers are only reset by hardware during chip cold reset. The values of the CSR
+ * fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_rx_coast {
+	uint64_t u64;
+	struct cvmx_gserx_rx_coast_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t coast                        : 4;  /**< For links that are not in PCIE mode (including all OCI links), control signals to freeze
+                                                         the frequency of the per lane CDR in the PHY. The COAST signals are only valid in P0
+                                                         state, come up asserted and are deasserted in hardware after detecting the electrical idle
+                                                         exit (GSER()_RX_EIE_DETSTS[EIESTS]). Once the COAST signal deasserts, the CDR is
+                                                         allowed to lock. In BGX mode, the BGX MAC can also control the COAST inputs to the PHY to
+                                                         allow Auto-Negotiation for backplane Ethernet. For diagnostic use only.
+                                                         <3>: Lane 3.
+                                                         <2>: Lane 2.
+                                                         <1>: Lane 1.
+                                                         <0>: Lane 0. */
+#else
+	uint64_t coast                        : 4;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_gserx_rx_coast_s          cn78xx;
+};
+typedef union cvmx_gserx_rx_coast cvmx_gserx_rx_coast_t;
+
+/**
+ * cvmx_gser#_rx_eie_deten
+ *
+ * These registers are only reset by hardware during chip cold reset. The values of the CSR
+ * fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_rx_eie_deten {
+	uint64_t u64;
+	struct cvmx_gserx_rx_eie_deten_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t eiede                        : 4;  /**< For links that are not in PCIE mode (including all OCI links), these bits enable per lane
+                                                         electrical idle exit (EIE) detection. When EIE is detected,
+                                                         GSER()_RX_EIE_DETSTS[EIELTCH] is asserted. EIEDE defaults to the enabled state. Once
+                                                         EIE has been detected, EIEDE must be disabled, and then enabled again to perform another
+                                                         EIE detection.
+                                                         <3>: Lane 3.
+                                                         <2>: Lane 2.
+                                                         <1>: Lane 1.
+                                                         <0>: Lane 0. */
+#else
+	uint64_t eiede                        : 4;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_gserx_rx_eie_deten_s      cn78xx;
+};
+typedef union cvmx_gserx_rx_eie_deten cvmx_gserx_rx_eie_deten_t;
+
+/**
+ * cvmx_gser#_rx_eie_detsts
+ */
+union cvmx_gserx_rx_eie_detsts {
+	uint64_t u64;
+	struct cvmx_gserx_rx_eie_detsts_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_12_63               : 52;
+	uint64_t cdrlock                      : 4;  /**< After an electrical idle exit condition (EIE) has been detected, the CDR needs 10000 UI to
+                                                         lock. During this time, there may be RX bit errors. These bits will set when the CDR is
+                                                         guaranteed to be locked. Note that link training can't start until the lane CDRLOCK is
+                                                         set. Software can use CDRLOCK to determine when to expect error free RX data.
+                                                         <11>: Lane 3.
+                                                         <10>: Lane 2.
+                                                         <9>: Lane 1.
+                                                         <8>: Lane 0. */
+	uint64_t eiests                       : 4;  /**< When electrical idle exit detection is enabled (GSER()_RX_EIE_DETEN[EIEDE] is
+                                                         asserted), indicates that an electrical idle exit condition (EIE) was detected. For higher
+                                                         data rates, the received data needs to have sufficient low frequency content (for example,
+                                                         idle symbols) for data transitions to be detected and for EIESTS to stay set accordingly.
+                                                         Under most conditions, EIESTS
+                                                         will stay asserted until GSER()_RX_EIE_DETEN[EIEDE] is deasserted.
+                                                         <7>: Lane 3.
+                                                         <6>: Lane 2.
+                                                         <5>: Lane 1.
+                                                         <4>: Lane 0. */
+	uint64_t eieltch                      : 4;  /**< When electrical idle exit detection is enabled (GSER()_RX_EIE_DETEN[EIEDE] is
+                                                         asserted), indicates that an electrical idle exit condition (EIE) was detected. Once an
+                                                         EIE condition has been detected, the per-lane EIELTCH will stay set until
+                                                         GSER()_RX_EIE_DETEN[EIEDE] is deasserted. Note that there may be RX bit errors until
+                                                         CDRLOCK
+                                                         is set.
+                                                         <3>: Lane 3.
+                                                         <2>: Lane 2.
+                                                         <1>: Lane 1.
+                                                         <0>: Lane 0. */
+#else
+	uint64_t eieltch                      : 4;
+	uint64_t eiests                       : 4;
+	uint64_t cdrlock                      : 4;
+	uint64_t reserved_12_63               : 52;
+#endif
+	} s;
+	struct cvmx_gserx_rx_eie_detsts_s     cn78xx;
+};
+typedef union cvmx_gserx_rx_eie_detsts cvmx_gserx_rx_eie_detsts_t;
+
+/**
+ * cvmx_gser#_rx_eie_filter
+ */
+union cvmx_gserx_rx_eie_filter {
+	uint64_t u64;
+	struct cvmx_gserx_rx_eie_filter_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t eii_filt                     : 16; /**< The GSER uses electrical idle inference to determine when a RX lane has reentered
+                                                         electrical idle (EI). The PHY electrical idle exit detection supports a minimum pulse
+                                                         width of 400ps, therefore configurations that run faster than 2.5G can indicate EI when
+                                                         the serial lines are still driven. For rates faster than 2.5G, it takes 16K * 8 UI of
+                                                         consecutive deasserted GSER()_RX_EIE_DETSTS[EIESTS] for the GSER to infer EI. In the
+                                                         event of electrical idle inference, the following happens:
+                                                         * GSER()_RX_EIE_DETSTS[CDRLOCK]<lane> is zeroed.
+                                                         * GSER()_RX_EIE_DETSTS[EIELTCH]<lane> is zeroed.
+                                                         * GSER()_RX_EIE_DETSTS[EIESTS]<lane> is zeroed.
+                                                         * GSER()_RX_COAST[COAST]<lane> is asserted to prevent the CDR from trying to lock on
+                                                         the incoming data stream.
+                                                         * GSER()_RX_EIE_DETEN[EIEDE]<lane> deasserts for a short period of time, and then is
+                                                         asserted to begin looking for the Electrical idle Exit condition.
+                                                         Writing this register to a non-zero value causes the electrical idle inference to use the
+                                                         EII_FILT count instead of the default settings. Each EII_FILT count represents 20 ns of
+                                                         incremental EI inference time.
+                                                         It is not expected that software will need to use the Electrical Idle Inference logic. */
+#else
+	uint64_t eii_filt                     : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_gserx_rx_eie_filter_s     cn78xx;
+};
+typedef union cvmx_gserx_rx_eie_filter cvmx_gserx_rx_eie_filter_t;
+
+/**
+ * cvmx_gser#_rx_polarity
+ *
+ * These registers are only reset by hardware during chip cold reset. The values of the CSR
+ * fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_rx_polarity {
+	uint64_t u64;
+	struct cvmx_gserx_rx_polarity_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t rx_inv                       : 4;  /**< For links that are not in PCIE mode (including all OCI links), control signal to invert
+                                                         the polarity of received data. When asserted, the polarity of the received data is
+                                                         inverted.
+                                                         <3>: Lane 3.
+                                                         <2>: Lane 2.
+                                                         <1>: Lane 1.
+                                                         <0>: Lane 0. */
+#else
+	uint64_t rx_inv                       : 4;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_gserx_rx_polarity_s       cn78xx;
+};
+typedef union cvmx_gserx_rx_polarity cvmx_gserx_rx_polarity_t;
+
+/**
+ * cvmx_gser#_rx_pwr_ctrl_p1
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_rx_pwr_ctrl_p1 {
+	uint64_t u64;
+	struct cvmx_gserx_rx_pwr_ctrl_p1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_14_63               : 50;
+	uint64_t p1_rx_resetn                 : 1;  /**< Place the receiver in reset (active low). */
+	uint64_t pq_rx_allow_pll_pd           : 1;  /**< When asserted, permit PLL powerdown (PLL is powered
+                                                         down if all other factors permit). */
+	uint64_t pq_rx_pcs_reset              : 1;  /**< When asserted, the RX power state machine puts the raw PCS RX logic
+                                                         in reset state to save power. */
+	uint64_t p1_rx_agc_en                 : 1;  /**< AGC enable. */
+	uint64_t p1_rx_dfe_en                 : 1;  /**< DFE enable. */
+	uint64_t p1_rx_cdr_en                 : 1;  /**< CDR enable. */
+	uint64_t p1_rx_cdr_coast              : 1;  /**< CDR coast; freezes the frequency of the CDR. */
+	uint64_t p1_rx_cdr_clr                : 1;  /**< CDR clear; clears the frequency of the CDR. */
+	uint64_t p1_rx_subblk_pd              : 5;  /**< RX sub-block powerdown controls to RX:
+                                                         <4> = CTLE.
+                                                         <3> = Reserved.
+                                                         <2> = Lane DLL.
+                                                         <1> = DFE/samplers.
+                                                         <0> = Termination. */
+	uint64_t p1_rx_chpd                   : 1;  /**< RX lane powerdown. */
+#else
+	uint64_t p1_rx_chpd                   : 1;
+	uint64_t p1_rx_subblk_pd              : 5;
+	uint64_t p1_rx_cdr_clr                : 1;
+	uint64_t p1_rx_cdr_coast              : 1;
+	uint64_t p1_rx_cdr_en                 : 1;
+	uint64_t p1_rx_dfe_en                 : 1;
+	uint64_t p1_rx_agc_en                 : 1;
+	uint64_t pq_rx_pcs_reset              : 1;
+	uint64_t pq_rx_allow_pll_pd           : 1;
+	uint64_t p1_rx_resetn                 : 1;
+	uint64_t reserved_14_63               : 50;
+#endif
+	} s;
+	struct cvmx_gserx_rx_pwr_ctrl_p1_s    cn78xx;
+};
+typedef union cvmx_gserx_rx_pwr_ctrl_p1 cvmx_gserx_rx_pwr_ctrl_p1_t;
+
+/**
+ * cvmx_gser#_sata_cfg
+ *
+ * SATA Config Enable.
+ *
+ */
+union cvmx_gserx_sata_cfg {
+	uint64_t u64;
+	struct cvmx_gserx_sata_cfg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t sata_en                      : 1;  /**< When set, DLM2 is configured for SATA (as opposed to PCIE). */
+#else
+	uint64_t sata_en                      : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_gserx_sata_cfg_s          cn70xx;
+	struct cvmx_gserx_sata_cfg_s          cn70xxp1;
+};
+typedef union cvmx_gserx_sata_cfg cvmx_gserx_sata_cfg_t;
+
+/**
+ * cvmx_gser#_sata_lane_rst
+ *
+ * Lane Reset Control.
+ *
+ */
+union cvmx_gserx_sata_lane_rst {
+	uint64_t u64;
+	struct cvmx_gserx_sata_lane_rst_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_2_63                : 62;
+	uint64_t l1_rst                       : 1;  /**< Independent reset for Lane 1. */
+	uint64_t l0_rst                       : 1;  /**< Independent reset for Lane 0. */
+#else
+	uint64_t l0_rst                       : 1;
+	uint64_t l1_rst                       : 1;
+	uint64_t reserved_2_63                : 62;
+#endif
+	} s;
+	struct cvmx_gserx_sata_lane_rst_s     cn70xx;
+	struct cvmx_gserx_sata_lane_rst_s     cn70xxp1;
+};
+typedef union cvmx_gserx_sata_lane_rst cvmx_gserx_sata_lane_rst_t;
+
+/**
+ * cvmx_gser#_sata_p0_tx_amp_gen#
+ *
+ * SATA Lane 0 Tx Launch Amplitude at Gen 1,2 and 3 Speeds.
+ *
+ */
+union cvmx_gserx_sata_p0_tx_amp_genx {
+	uint64_t u64;
+	struct cvmx_gserx_sata_p0_tx_amp_genx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_7_63                : 57;
+	uint64_t tx_amp_gen                   : 7;  /**< This status value sets the Tx driver launch amplitude in the
+                                                         case where the PHY is running at the Gen1, Gen2, and Gen3
+                                                         rates. Used for tuning at the board level for Rx eye compliance. */
+#else
+	uint64_t tx_amp_gen                   : 7;
+	uint64_t reserved_7_63                : 57;
+#endif
+	} s;
+	struct cvmx_gserx_sata_p0_tx_amp_genx_s cn70xx;
+	struct cvmx_gserx_sata_p0_tx_amp_genx_s cn70xxp1;
+};
+typedef union cvmx_gserx_sata_p0_tx_amp_genx cvmx_gserx_sata_p0_tx_amp_genx_t;
+
+/**
+ * cvmx_gser#_sata_p0_tx_preemph_gen#
+ *
+ * SATA Lane 0 Tx Pre-emphasis at Gen 1,2 and 3 Speeds.
+ *
+ */
+union cvmx_gserx_sata_p0_tx_preemph_genx {
+	uint64_t u64;
+	struct cvmx_gserx_sata_p0_tx_preemph_genx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_7_63                : 57;
+	uint64_t tx_preemph                   : 7;  /**< This static value sets the Tx driver de-emphasis value in the
+                                                         case where the PHY is running at the Gen1, Gen2, and Gen3
+                                                         rates. Used for tuning at the board level for Rx eye compliance. */
+#else
+	uint64_t tx_preemph                   : 7;
+	uint64_t reserved_7_63                : 57;
+#endif
+	} s;
+	struct cvmx_gserx_sata_p0_tx_preemph_genx_s cn70xx;
+	struct cvmx_gserx_sata_p0_tx_preemph_genx_s cn70xxp1;
+};
+typedef union cvmx_gserx_sata_p0_tx_preemph_genx cvmx_gserx_sata_p0_tx_preemph_genx_t;
+
+/**
+ * cvmx_gser#_sata_p1_tx_amp_gen#
+ *
+ * SATA Lane 1 Tx Launch Amplitude at Gen 1,2 and 3 Speeds.
+ *
+ */
+union cvmx_gserx_sata_p1_tx_amp_genx {
+	uint64_t u64;
+	struct cvmx_gserx_sata_p1_tx_amp_genx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_7_63                : 57;
+	uint64_t tx_amp_gen                   : 7;  /**< This status value sets the Tx driver launch amplitude in the
+                                                         case where the PHY is running at the Gen1, Gen2, and Gen3
+                                                         rates. Used for tuning at the board level for Rx eye compliance. */
+#else
+	uint64_t tx_amp_gen                   : 7;
+	uint64_t reserved_7_63                : 57;
+#endif
+	} s;
+	struct cvmx_gserx_sata_p1_tx_amp_genx_s cn70xx;
+	struct cvmx_gserx_sata_p1_tx_amp_genx_s cn70xxp1;
+};
+typedef union cvmx_gserx_sata_p1_tx_amp_genx cvmx_gserx_sata_p1_tx_amp_genx_t;
+
+/**
+ * cvmx_gser#_sata_p1_tx_preemph_gen#
+ *
+ * SATA Lane 0 Tx Pre-emphasis at Gen 1,2 and 3 Speeds.
+ *
+ */
+union cvmx_gserx_sata_p1_tx_preemph_genx {
+	uint64_t u64;
+	struct cvmx_gserx_sata_p1_tx_preemph_genx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_7_63                : 57;
+	uint64_t tx_preemph                   : 7;  /**< This static value sets the Tx driver de-emphasis value in the
+                                                         case where the PHY is running at the Gen1, Gen2, and Gen3
+                                                         rates. Used for tuning at the board level for Rx eye compliance. */
+#else
+	uint64_t tx_preemph                   : 7;
+	uint64_t reserved_7_63                : 57;
+#endif
+	} s;
+	struct cvmx_gserx_sata_p1_tx_preemph_genx_s cn70xx;
+	struct cvmx_gserx_sata_p1_tx_preemph_genx_s cn70xxp1;
+};
+typedef union cvmx_gserx_sata_p1_tx_preemph_genx cvmx_gserx_sata_p1_tx_preemph_genx_t;
+
+/**
+ * cvmx_gser#_sata_ref_ssp_en
+ *
+ * SATA Reference Clock Enable for the PHY.
+ *
+ */
+union cvmx_gserx_sata_ref_ssp_en {
+	uint64_t u64;
+	struct cvmx_gserx_sata_ref_ssp_en_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t ref_ssp_en                   : 1;  /**< Reference Clock Enable for the PHY. */
+#else
+	uint64_t ref_ssp_en                   : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_gserx_sata_ref_ssp_en_s   cn70xx;
+	struct cvmx_gserx_sata_ref_ssp_en_s   cn70xxp1;
+};
+typedef union cvmx_gserx_sata_ref_ssp_en cvmx_gserx_sata_ref_ssp_en_t;
+
+/**
+ * cvmx_gser#_sata_rx_invert
+ *
+ * SATA Receive Polarity Inversion.
+ *
+ */
+union cvmx_gserx_sata_rx_invert {
+	uint64_t u64;
+	struct cvmx_gserx_sata_rx_invert_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_2_63                : 62;
+	uint64_t rx1_invert                   : 1;  /**< Instructs the PHY to perform a polarity inversion on the Lane 1
+                                                          received data.
+                                                         - 0: PHY does not performs polarity inversion
+                                                         - 1: PHY performs polarity inversion */
+	uint64_t rx0_invert                   : 1;  /**< 0: PHY does not performs polarity inversion
+                                                         - 1: PHY performs polarity inversion */
+#else
+	uint64_t rx0_invert                   : 1;
+	uint64_t rx1_invert                   : 1;
+	uint64_t reserved_2_63                : 62;
+#endif
+	} s;
+	struct cvmx_gserx_sata_rx_invert_s    cn70xx;
+	struct cvmx_gserx_sata_rx_invert_s    cn70xxp1;
+};
+typedef union cvmx_gserx_sata_rx_invert cvmx_gserx_sata_rx_invert_t;
+
+/**
+ * cvmx_gser#_sata_ssc_clk_sel
+ *
+ * SATA Spread Spectrum Reference Clock Shifting.
+ *
+ */
+union cvmx_gserx_sata_ssc_clk_sel {
+	uint64_t u64;
+	struct cvmx_gserx_sata_ssc_clk_sel_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t ssc_clk_sel                  : 9;  /**< Enables non-standard oscillator frequencies to generate targeted
+                                                         MPLL output rates.  Input corresponds to frequency-synthesis
+                                                         coefficient.
+                                                         [8:6]: modulous - 1
+                                                         [5:0] = 2's compliment push amount. */
+#else
+	uint64_t ssc_clk_sel                  : 9;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_gserx_sata_ssc_clk_sel_s  cn70xx;
+	struct cvmx_gserx_sata_ssc_clk_sel_s  cn70xxp1;
+};
+typedef union cvmx_gserx_sata_ssc_clk_sel cvmx_gserx_sata_ssc_clk_sel_t;
+
+/**
+ * cvmx_gser#_sata_ssc_en
+ *
+ * SATA Spread Spectrum Disable.
+ *
+ */
+union cvmx_gserx_sata_ssc_en {
+	uint64_t u64;
+	struct cvmx_gserx_sata_ssc_en_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t ssc_en                       : 1;  /**< Enables spread spectrum clock production (0.5% down-spread
+                                                         at ~31.5 KHz) in the SATA 6G PHY.  If the reference clock
+                                                         already has spread spectrum applied, this bit must stay
+                                                         deasserted. */
+#else
+	uint64_t ssc_en                       : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_gserx_sata_ssc_en_s       cn70xx;
+	struct cvmx_gserx_sata_ssc_en_s       cn70xxp1;
+};
+typedef union cvmx_gserx_sata_ssc_en cvmx_gserx_sata_ssc_en_t;
+
+/**
+ * cvmx_gser#_sata_ssc_range
+ *
+ * SATA Spread Spectrum Range.
+ *
+ */
+union cvmx_gserx_sata_ssc_range {
+	uint64_t u64;
+	struct cvmx_gserx_sata_ssc_range_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_3_63                : 61;
+	uint64_t ssc_range                    : 3;  /**< Selects the range of spread spectrum modulation when SSC_EN is
+                                                         asserted and the PHY is spreading the high-speed transmit clocks.
+                                                         Applies a fixed offset to the accumulator.
+                                                         - 000: -4.980 ppm
+                                                         - 001: -4.492 ppm
+                                                         - 010: -4.003 ppm
+                                                         - 011: -2.000 ppm
+                                                         - 100:  4.980 ppm
+                                                         - 101:  4.492 ppm
+                                                         - 110:  4.003 ppm
+                                                         - 111:  2.000 ppm */
+#else
+	uint64_t ssc_range                    : 3;
+	uint64_t reserved_3_63                : 61;
+#endif
+	} s;
+	struct cvmx_gserx_sata_ssc_range_s    cn70xx;
+	struct cvmx_gserx_sata_ssc_range_s    cn70xxp1;
+};
+typedef union cvmx_gserx_sata_ssc_range cvmx_gserx_sata_ssc_range_t;
+
+/**
+ * cvmx_gser#_sata_status
+ *
+ * SATA PHY Ready Status.
+ *
+ */
+union cvmx_gserx_sata_status {
+	uint64_t u64;
+	struct cvmx_gserx_sata_status_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_2_63                : 62;
+	uint64_t p1_rdy                       : 1;  /**< PHY Lane 1 is ready to send and receive data. */
+	uint64_t p0_rdy                       : 1;  /**< PHY Lane 0 is ready to send and receive data. */
+#else
+	uint64_t p0_rdy                       : 1;
+	uint64_t p1_rdy                       : 1;
+	uint64_t reserved_2_63                : 62;
+#endif
+	} s;
+	struct cvmx_gserx_sata_status_s       cn70xx;
+	struct cvmx_gserx_sata_status_s       cn70xxp1;
+};
+typedef union cvmx_gserx_sata_status cvmx_gserx_sata_status_t;
+
+/**
+ * cvmx_gser#_sata_tx_invert
+ *
+ * SATA Transmit Polarity Inversion.
+ *
+ */
+union cvmx_gserx_sata_tx_invert {
+	uint64_t u64;
+	struct cvmx_gserx_sata_tx_invert_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_2_63                : 62;
+	uint64_t tx1_invert                   : 1;  /**< Instructs the PHY to perform a polarity inversion on the Lane 1
+                                                          transmitted data.
+                                                         - 0: PHY does not performs polarity inversion
+                                                         - 1: PHY performs polarity inversion */
+	uint64_t tx0_invert                   : 1;  /**< Instructs the PHY to perform a polarity inversion on the Lane 0
+                                                          transmitted data.
+                                                         - 0: PHY does not performs polarity inversion
+                                                         - 1: PHY performs polarity inversion */
+#else
+	uint64_t tx0_invert                   : 1;
+	uint64_t tx1_invert                   : 1;
+	uint64_t reserved_2_63                : 62;
+#endif
+	} s;
+	struct cvmx_gserx_sata_tx_invert_s    cn70xx;
+	struct cvmx_gserx_sata_tx_invert_s    cn70xxp1;
+};
+typedef union cvmx_gserx_sata_tx_invert cvmx_gserx_sata_tx_invert_t;
+
+/**
+ * cvmx_gser#_scratch
+ *
+ * These registers are only reset by hardware during chip cold reset. The values of the CSR
+ * fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_scratch {
+	uint64_t u64;
+	struct cvmx_gserx_scratch_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t scratch                      : 16; /**< General purpose scratch register. */
+#else
+	uint64_t scratch                      : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_gserx_scratch_s           cn78xx;
+};
+typedef union cvmx_gserx_scratch cvmx_gserx_scratch_t;
+
+/**
+ * cvmx_gser#_slice_cfg
+ *
+ * These registers are for diagnostic use only.
+ * These registers are only reset by hardware during chip cold reset.
+ * The values of the CSR fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_slice_cfg {
+	uint64_t u64;
+	struct cvmx_gserx_slice_cfg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_12_63               : 52;
+	uint64_t tx_rx_detect_lvl_enc         : 4;  /**< Determines the RX Detect level, pcs_sds_tx_rx_detect_lvl[9:0],
+                                                         (which is a 1-hot signal), where the level is equal to to
+                                                         2^TX_RX_DETECT_LVL_ENC. */
+	uint64_t reserved_2_7                 : 6;
+	uint64_t pcs_sds_tx_stress_eye        : 2;  /**< Controls TX stress eye. */
+#else
+	uint64_t pcs_sds_tx_stress_eye        : 2;
+	uint64_t reserved_2_7                 : 6;
+	uint64_t tx_rx_detect_lvl_enc         : 4;
+	uint64_t reserved_12_63               : 52;
+#endif
+	} s;
+	struct cvmx_gserx_slice_cfg_s         cn78xx;
+};
+typedef union cvmx_gserx_slice_cfg cvmx_gserx_slice_cfg_t;
+
+/**
+ * cvmx_gser#_spd
+ *
+ * These registers are only reset by hardware during chip cold reset. The values of the CSR
+ * fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_spd {
+	uint64_t u64;
+	struct cvmx_gserx_spd_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t spd                          : 4;  /**< For OCI links (i.e. GSER8..13), the hardware loads this CSR field from the OCI_SPD<3:0>
+                                                         pins during chip cold reset. For non-OCI links, this field is not used.
+                                                         For SPD settings that configure a non-default reference clock, hardware updates the PLL
+                                                         settings of the specific lane mode (LMODE) table entry to derive the correct link rate.
+                                                         <pre>
+                                                         SPD   REFCLK      Link rate   LMODE
+                                                         0x0:  100 MHz     1.25 Gb     R_125G_REFCLK15625_KX
+                                                         0x1:  100 MHz     2.5 Gb      R_25G_REFCLK100
+                                                         0x2:  100 MHz     5 Gb        R_5G_REFCLK100
+                                                         0x3:  100 MHz     8 Gb        R_8G_REFCLK100
+                                                         0x4:  125 MHz     1.25 Gb     R_125G_REFCLK15625_KX
+                                                         0x5:  125 MHz     2.5 Gb      R_25G_REFCLK125
+                                                         0x6:  125 MHz     3.125 Gb    R_3125G_REFCLK15625_XAUI
+                                                         0x7:  125 MHz     5 Gb        R_5G_REFCLK125
+                                                         0x8:  125 MHz     6.25 Gb     R_625G_REFCLK15625_RXAUI
+                                                         0x9:  125 MHz     8 Gb        R_8G_REFCLK125
+                                                         0xA:  156.25 MHz  2.5 Gb      R_25G_REFCLK100
+                                                         0xB:  156.25 MHz  3.125 Gb    R_3125G_REFCLK15625_XAUI
+                                                         0xC:  156.25 MHz  5 Gb        R_5G_REFCLK125
+                                                         0xD:  156.25 MHz  6.25 Gb     R_625G_REFCLK15625_RXAUI
+                                                         0xE:  156.25 MHz  10.3125 Gb  R_103125G_REFCLK15625_KR
+                                                         0xF:                          SW_MODE
+                                                         </pre>
+                                                         Note that a value of 0xF is called SW_MODE. The OCI link does not come up configured in
+                                                         SW_MODE.
+                                                         (Software must do all the OCI GSER configuration to use OCI in the case of SW_MODE.)
+                                                         When SPD!=SW_MODE after a chip cold reset, the hardware has initialized the following
+                                                         registers (based on the OCI_SPD selection):
+                                                          * GSER()_LANE_MODE[LMODE]=Z.
+                                                          * GSER()_PLL_P()_MODE_0.
+                                                          * GSER()_PLL_P()_MODE_1.
+                                                          * GSER()_LANE_P()_MODE_0.
+                                                          * GSER()_LANE_P()_MODE_1.
+                                                          * GSER()_LANE()_RX_VALBBD_CTRL_0.
+                                                          * GSER()_LANE()_RX_VALBBD_CTRL_1.
+                                                          * GSER()_LANE()_RX_VALBBD_CTRL_2.
+                                                          where in "GSER(x)", x is 8..13, and in "P(z)", z equals LMODE. */
+#else
+	uint64_t spd                          : 4;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_gserx_spd_s               cn78xx;
+};
+typedef union cvmx_gserx_spd cvmx_gserx_spd_t;
+
+/**
+ * cvmx_gser#_srst
+ *
+ * These registers are only reset by hardware during chip cold reset. The values of the CSR
+ * fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_srst {
+	uint64_t u64;
+	struct cvmx_gserx_srst_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t srst                         : 1;  /**< When asserted, resets all per-lane state in the GSER with the exception of the PHY and the
+                                                         GSER()_CFG. For diagnostic use only. */
+#else
+	uint64_t srst                         : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_gserx_srst_s              cn78xx;
+};
+typedef union cvmx_gserx_srst cvmx_gserx_srst_t;
+
+/**
+ * cvmx_gser#_tx_vboost
+ *
+ * These registers are only reset by hardware during chip cold reset. The values of the CSR
+ * fields in these registers do not change during chip warm or soft resets.
+ */
+union cvmx_gserx_tx_vboost {
+	uint64_t u64;
+	struct cvmx_gserx_tx_vboost_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t vboost                       : 4;  /**< For links that are not in PCIE mode (including all OCI links), boosts the TX Vswing from
+                                                         VDD to 1.0 VPPD.
+                                                         <3>: Lane 3.
+                                                         <2>: Lane 2.
+                                                         <1>: Lane 1.
+                                                         <0>: Lane 0. */
+#else
+	uint64_t vboost                       : 4;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_gserx_tx_vboost_s         cn78xx;
+};
+typedef union cvmx_gserx_tx_vboost cvmx_gserx_tx_vboost_t;
+
+#endif
diff --git a/arch/mips/include/asm/octeon/cvmx-helper-bgx.h b/arch/mips/include/asm/octeon/cvmx-helper-bgx.h
new file mode 100644
index 0000000..e14cccc
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-helper-bgx.h
@@ -0,0 +1,239 @@
+/***********************license start***************
+ * Copyright (c) 2014  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Functions to configure the BGX MAC.
+ *
+ * <hr>$Revision$<hr>
+ */
+
+#ifndef __CVMX_HELPER_BGX_H__
+#define __CVMX_HELPER_BGX_H__
+
+#define CVMX_BGX_RX_FIFO_SIZE	(64 * 1024)
+
+extern int __cvmx_helper_bgx_enumerate(int interface);
+
+/**
+ * @INTERNAL
+ * Probe a SGMII interface and determine the number of ports
+ * connected to it. The SGMII/XAUI interface should still be down after
+ * this call. This is used by interfaces using the bgx mac.
+ *
+ * @param interface Interface to probe
+ *
+ * @return Number of ports on the interface. Zero to disable.
+ */
+extern int __cvmx_helper_bgx_probe(int interface);
+
+/**
+ * @INTERNAL
+ * Bringup and enable a SGMII interface. After this call packet
+ * I/O should be fully functional. This is called with IPD
+ * enabled but PKO disabled. This is used by interfaces using the
+ * bgx mac.
+ *
+ * @param interface Interface to bring up
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int __cvmx_helper_bgx_sgmii_enable(int interface);
+
+/**
+ * @INTERNAL
+ * Return the link state of an IPD/PKO port as returned by
+ * auto negotiation. The result of this function may not match
+ * Octeon's link config if auto negotiation has changed since
+ * the last call to cvmx_helper_link_set(). This is used by
+ * interfaces using the bgx mac.
+ *
+ * @param ipd_port IPD/PKO port to query
+ *
+ * @return Link state
+ */
+extern cvmx_helper_link_info_t __cvmx_helper_bgx_sgmii_link_get(int ipd_port);
+
+/**
+ * @INTERNAL
+ * Configure an IPD/PKO port for the specified link state. This
+ * function does not influence auto negotiation at the PHY level.
+ * The passed link state must always match the link state returned
+ * by cvmx_helper_link_get(). It is normally best to use
+ * cvmx_helper_link_autoconf() instead. This is used by interfaces
+ * using the bgx mac.
+ *
+ * @param ipd_port  IPD/PKO port to configure
+ * @param link_info The new link state
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int __cvmx_helper_bgx_sgmii_link_set(int ipd_port,
+					    cvmx_helper_link_info_t link_info);
+
+/**
+ * @INTERNAL
+ * Configure a port for internal and/or external loopback. Internal loopback
+ * causes packets sent by the port to be received by Octeon. External loopback
+ * causes packets received from the wire to sent out again. This is used by
+ * interfaces using the bgx mac.
+ *
+ * @param ipd_port IPD/PKO port to loopback.
+ * @param enable_internal
+ *                 Non zero if you want internal loopback
+ * @param enable_external
+ *                 Non zero if you want external loopback
+ *
+ * @return Zero on success, negative on failure.
+ */
+extern int __cvmx_helper_bgx_sgmii_configure_loopback(int ipd_port,
+						      int enable_internal,
+						      int enable_external);
+
+/**
+ * @INTERNAL
+ * Bringup and enable a XAUI interface. After this call packet
+ * I/O should be fully functional. This is called with IPD
+ * enabled but PKO disabled. This is used by interfaces using the
+ * bgx mac.
+ *
+ * @param interface Interface to bring up
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int __cvmx_helper_bgx_xaui_enable(int interface);
+
+/**
+ * @INTERNAL
+ * Return the link state of an IPD/PKO port as returned by
+ * auto negotiation. The result of this function may not match
+ * Octeon's link config if auto negotiation has changed since
+ * the last call to cvmx_helper_link_set(). This is used by
+ * interfaces using the bgx mac.
+ *
+ * @param ipd_port IPD/PKO port to query
+ *
+ * @return Link state
+ */
+extern cvmx_helper_link_info_t __cvmx_helper_bgx_xaui_link_get(int ipd_port);
+
+/**
+ * @INTERNAL
+ * Configure an IPD/PKO port for the specified link state. This
+ * function does not influence auto negotiation at the PHY level.
+ * The passed link state must always match the link state returned
+ * by cvmx_helper_link_get(). It is normally best to use
+ * cvmx_helper_link_autoconf() instead. This is used by interfaces
+ * using the bgx mac.
+ *
+ * @param ipd_port  IPD/PKO port to configure
+ * @param link_info The new link state
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int __cvmx_helper_bgx_xaui_link_set(int ipd_port,
+					   cvmx_helper_link_info_t link_info);
+
+/**
+ * @INTERNAL
+ * Configure a port for internal and/or external loopback. Internal loopback
+ * causes packets sent by the port to be received by Octeon. External loopback
+ * causes packets received from the wire to sent out again. This is used by
+ * interfaces using the bgx mac.
+ *
+ * @param ipd_port IPD/PKO port to loopback.
+ * @param enable_internal
+ *                 Non zero if you want internal loopback
+ * @param enable_external
+ *                 Non zero if you want external loopback
+ *
+ * @return Zero on success, negative on failure.
+ */
+extern int __cvmx_helper_bgx_xaui_configure_loopback(int ipd_port,
+						     int enable_internal,
+						     int enable_external);
+/**
+ * @INTERNAL
+ * Configure Priority-Based Flow Control (a.k.a. PFC/CBFC)
+ * on a specific BGX interface/port.
+ */
+extern void __cvmx_helper_bgx_xaui_config_pfc(unsigned node,
+		unsigned interface, unsigned port, bool pfc_enable);
+/**
+ * This function control how the hardware handles incoming PAUSE
+ * packets. The most common modes of operation:
+ * ctl_bck = 1, ctl_drp = 1: hardware handles everything
+ * ctl_bck = 0, ctl_drp = 0: software sees all PAUSE frames
+ * ctl_bck = 0, ctl_drp = 1: all PAUSE frames are completely ignored
+ * @param node		node number.
+ * @param interface	interface number
+ * @param port		port number
+ * @param ctl_bck	1: Forward PAUSE information to TX block
+ * @param ctl_drp	1: Drop control PAUSE frames.
+ */
+extern void cvmx_helper_bgx_rx_pause_ctl(unsigned node, unsigned interface,
+				  unsigned port, unsigned ctl_bck, unsigned ctl_drp);
+
+/**
+ * This function configures the receive action taken for multicast, broadcast
+ * and dmac filter match packets.
+ * @param node		node number.
+ * @param interface	interface number
+ * @param port		port number
+ * @param cam_accept	0: reject packets on dmac filter match
+ *                      1: accept packet on dmac filter match
+ * @param mcast_mode	0x0 = Force reject all multicast packets
+ *                      0x1 = Force accept all multicast packets
+ *                      0x2 = Use the address filter CAM
+ * @param bcast_accept  0 = Reject all broadcast packets
+ *                      1 = Accept all broadcast packets
+ */
+extern void cvmx_helper_bgx_rx_adr_ctl(unsigned node, unsigned interface, unsigned port,
+                                unsigned cam_accept, unsigned mcast_mode, unsigned bcast_accept);
+
+/**
+ * Function to control the generation of FCS, padding by the BGX
+ *
+ */
+extern void cvmx_helper_bgx_tx_options(unsigned node,
+	unsigned interface, unsigned index,
+	bool fcs_enable, bool pad_enable);
+
+#endif
diff --git a/arch/mips/include/asm/octeon/cvmx-helper-cfg.h b/arch/mips/include/asm/octeon/cvmx-helper-cfg.h
new file mode 100644
index 0000000..62808d5
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-helper-cfg.h
@@ -0,0 +1,575 @@
+/***********************license start***************
+ * Copyright (c) 2003-2013  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Helper Functions for the Configuration Framework
+ *
+ * OCTEON_CN68XX introduces a flexible hw interface configuration
+ * scheme. To cope with this change and the requirements of
+ * configurability for other system resources, e.g., IPD/PIP pknd and
+ * PKO ports and queues, a configuration framework for the SDK is
+ * designed. It has two goals: first to recognize and establish the
+ * default configuration and, second, to allow the user to define key
+ * parameters in a high-level language.
+ *
+ * The helper functions query the QLM setup to help achieving the
+ * first goal.
+ *
+ * The second goal is accomplished by generating
+ * cvmx_helper_cfg_init() from a high-level lanaguage.
+ *
+ * <hr>$Revision: 0 $<hr>
+ */
+
+#ifndef __CVMX_HELPER_CFG_H__
+#define __CVMX_HELPER_CFG_H__
+
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include <asm/octeon/cvmx-helper-util.h>
+#else
+#include "cvmx-helper-util.h"
+#endif
+
+#define CVMX_HELPER_CFG_MAX_PKO_PORT		128
+#define CVMX_HELPER_CFG_MAX_PIP_BPID       	64
+#define CVMX_HELPER_CFG_MAX_PIP_PKND       	64
+#define CVMX_HELPER_CFG_MAX_PKO_QUEUES     	256
+#define CVMX_HELPER_CFG_MAX_PORT_PER_IFACE 	256
+
+#define CVMX_HELPER_CFG_INVALID_VALUE		-1	/* The default return
+							 * value upon failure
+							 */
+
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+extern "C" {
+/* *INDENT-ON* */
+#endif
+
+#define cvmx_helper_cfg_assert(cond)					\
+	do {								\
+	    if (!(cond))						\
+	    {								\
+	        cvmx_dprintf("cvmx_helper_cfg_assert (%s) at %s:%d\n",	\
+		    #cond, __FILE__, __LINE__);				\
+	    }								\
+	} while (0)
+
+/*
+ * Config Options
+ *
+ * These options have to be set via cvmx_helper_cfg_opt_set() before calling the
+ * routines that set up the hw. These routines process the options and set them
+ * correctly to take effect at runtime.
+ */
+enum cvmx_helper_cfg_option {
+	CVMX_HELPER_CFG_OPT_USE_DWB,	/*
+					 * Global option to control if
+					 * the SDK configures units (DMA,
+					 * SSO, and PKO) to send don't
+					 * write back (DWB) requests for
+					 * freed buffers. Set to 1/0 to
+					 * enable/disable DWB.
+					 *
+					 * For programs that fit inside
+					 * L2, sending DWB just causes
+					 * more L2 operations without
+					 * benefit.
+					 */
+
+	CVMX_HELPER_CFG_OPT_MAX
+};
+typedef enum cvmx_helper_cfg_option cvmx_helper_cfg_option_t;
+
+/*
+ * Per physical port
+ */
+struct cvmx_cfg_port_param {
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+	int port_fdt_node;		/** Node offset in FDT of node */
+	int phy_fdt_node;		/** Node offset in FDT of PHY */
+#endif
+	int8_t ccpp_pknd;
+	int8_t ccpp_bpid;
+	int8_t ccpp_pko_port_base;
+	int8_t ccpp_pko_num_ports;
+	bool valid;			/** 1 = port valid, 0 = invalid */
+	bool sgmii_phy_mode;		/** 1 = port in PHY mode, 0 = MAC mode */
+	bool sgmii_1000x_mode;		/** 1 = 1000Base-X mode, 0 = SGMII mode */
+	bool agl_rx_clk_delay_bypass;	/** 1 = use rx clock delay bypass for AGL mode */
+	uint8_t agl_rx_clk_skew;	/** AGL rx clock skew setting (default 0) */
+	bool force_link_up;		/** Ignore PHY and always report link up */
+};
+
+/*
+ * Per pko_port
+ */
+struct cvmx_cfg_pko_port_param {
+	int16_t ccppp_queue_base;
+	int16_t ccppp_num_queues;
+};
+
+/*
+ * A map from pko_port to
+ *     interface,
+ *     index, and
+ *     pko engine id
+ */
+struct cvmx_cfg_pko_port_map {
+	int16_t ccppl_interface;
+	int16_t ccppl_index;
+	int16_t ccppl_eid;
+};
+
+/*
+ * This is for looking up pko_base_port and pko_nport for ipd_port
+ */
+struct cvmx_cfg_pko_port_pair {
+	int8_t ccppp_base_port;
+	int8_t ccppp_nports;
+};
+
+typedef union cvmx_user_static_pko_queue_config
+{
+	struct
+	{
+		struct pko_queues_cfg {
+			unsigned
+				queues_per_port:5,
+				qos_enable:1,
+				pfc_enable:1;
+		} pko_cfg_iface[6];
+		struct pko_queues_cfg pko_cfg_loop;
+		struct pko_queues_cfg pko_cfg_npi;
+	} pknd;
+	struct
+	{
+		uint8_t pko_ports_per_interface[2];
+		uint8_t pko_queues_per_port_interface[2];
+		uint8_t pko_queues_per_port_loop;
+		uint8_t pko_queues_per_port_pci;
+		uint8_t pko_queues_per_port_srio[4];
+	} non_pknd;
+} cvmx_user_static_pko_queue_config_t;
+
+extern CVMX_SHARED cvmx_user_static_pko_queue_config_t __cvmx_pko_queue_static_config;
+extern CVMX_SHARED struct cvmx_cfg_pko_port_map cvmx_cfg_pko_port_map[CVMX_HELPER_CFG_MAX_PKO_PORT];
+extern CVMX_SHARED struct cvmx_cfg_port_param cvmx_cfg_port [CVMX_MAX_NODES][CVMX_HELPER_MAX_IFACE][CVMX_HELPER_CFG_MAX_PORT_PER_IFACE];
+extern CVMX_SHARED struct cvmx_cfg_pko_port_param cvmx_pko_queue_table[];
+extern CVMX_SHARED int cvmx_enable_helper_flag;
+/*
+ * @INTERNAL
+ * Return configured pknd for the port
+ *
+ * @param interface the interface number
+ * @param index the port's index number
+ * @return the pknd
+ */
+extern int __cvmx_helper_cfg_pknd(int interface, int index);
+
+/*
+ * @INTERNAL
+ * Return the configured bpid for the port
+ *
+ * @param interface the interface number
+ * @param index the port's index number
+ * @return the bpid
+ */
+extern int __cvmx_helper_cfg_bpid(int interface, int index);
+
+/**
+ * @INTERNAL
+ * Return the configured pko_port base for the port
+ *
+ * @param interface the interface number
+ * @param index the port's index number
+ * @return the pko_port base
+ */
+extern int __cvmx_helper_cfg_pko_port_base(int interface, int index);
+
+/*
+ * @INTERNAL
+ * Return the configured number of pko_ports for the port
+ *
+ * @param interface the interface number
+ * @param index the port's index number
+ * @return the number of pko_ports
+ */
+extern int __cvmx_helper_cfg_pko_port_num(int interface, int index);
+
+/*
+ * @INTERNAL
+ * Return the configured pko_queue base for the pko_port
+ *
+ * @param pko_port
+ * @return the pko_queue base
+ */
+extern int __cvmx_helper_cfg_pko_queue_base(int pko_port);
+
+/*
+ * @INTERNAL
+ * Return the configured number of pko_queues for the pko_port
+ *
+ * @param pko_port
+ * @return the number of pko_queues
+ */
+extern int __cvmx_helper_cfg_pko_queue_num(int pko_port);
+
+/*
+ * @INTERNAL
+ * Return the interface the pko_port is configured for
+ *
+ * @param pko_port
+ * @return the interface for the pko_port
+ */
+extern int __cvmx_helper_cfg_pko_port_interface(int pko_port);
+
+/*
+ * @INTERNAL
+ * Return the index of the port the pko_port is configured for
+ *
+ * @param pko_port
+ * @return the index of the port
+ */
+extern int __cvmx_helper_cfg_pko_port_index(int pko_port);
+
+/*
+ * @INTERNAL
+ * Return the pko_eid of the pko_port
+ *
+ * @param pko_port
+ * @return the pko_eid
+ */
+extern int __cvmx_helper_cfg_pko_port_eid(int pko_port);
+
+/*
+ * @INTERNAL
+ * Return the max# of pko queues allocated.
+ *
+ * @return the max# of pko queues
+ *
+ * Note: there might be holes in the queue space depending on user
+ * configuration. The function returns the highest queue's index in
+ * use.
+ */
+extern int __cvmx_helper_cfg_pko_max_queue(void);
+
+/*
+ * @INTERNAL
+ * Return the max# of PKO DMA engines allocated.
+ *
+ * @return the max# of DMA engines
+ *
+ * NOTE: the DMA engines are allocated contiguously and starting from
+ * 0.
+ */
+extern int __cvmx_helper_cfg_pko_max_engine(void);
+
+/*
+ * Get the value set for the config option ``opt''.
+ *
+ * @param opt is the config option.
+ * @return the value set for the option
+ *
+ * LR: only used for DWB in NPI, POW, PKO1
+ */
+extern uint64_t cvmx_helper_cfg_opt_get(cvmx_helper_cfg_option_t opt);
+
+/*
+ * Set the value for a config option.
+ *
+ * @param opt is the config option.
+ * @param val is the value to set for the opt.
+ * @return 0 for success and -1 on error
+ *
+ * Note an option here is a config-time parameter and this means that
+ * it has to be set before calling the corresponding setup functions
+ * that actually sets the option in hw.
+ *
+ * LR: Not used.
+ */
+extern int cvmx_helper_cfg_opt_set(cvmx_helper_cfg_option_t opt, uint64_t val);
+
+/*
+ * Retrieve the pko_port base given ipd_port.
+ *
+ * @param ipd_port is the IPD eport
+ * @return the corresponding PKO port base for the physical port
+ * represented by the IPD eport or CVMX_HELPER_CFG_INVALID_VALUE.
+ */
+extern int cvmx_helper_cfg_ipd2pko_port_base(int ipd_port);
+
+/*
+ * Retrieve the number of pko_ports given ipd_port.
+ *
+ * @param ipd_port is the IPD eport
+ * @return the corresponding number of PKO ports for the physical port
+ *  represented by IPD eport or CVMX_HELPER_CFG_INVALID_VALUE.
+ */
+extern int cvmx_helper_cfg_ipd2pko_port_num(int ipd_port);
+
+/*
+ * @INTERNAL
+ * The init function
+ *
+ * @param none
+ * @return 0 for success.
+ *
+ * Note: this function is meant to be called to set the ``configured
+ * parameters,'' e.g., pknd, bpid, etc. and therefore should be before
+ * any of the corresponding cvmx_helper_cfg_xxxx() functions are
+ * called.
+ */
+extern int __cvmx_helper_init_port_config_data(void);
+
+/*
+ * @INTERNAL
+ * The local init function
+ *
+ * @param none
+ * @return 0 for success.
+ *
+ * Note: this function is meant to be called to set the ``configured
+ * parameters locally,'' e.g., pknd, bpid, etc. and therefore should be before
+ * any of the corresponding cvmx_helper_cfg_xxxx() functions are
+ * called.
+ */
+extern int __cvmx_helper_init_port_config_data_local(void);
+
+/*
+ * Set the frame max size and jabber size to 65535.
+ *
+ */
+extern void cvmx_helper_cfg_set_jabber_and_frame_max(void);
+
+/*
+ * Enable storing short packets only in the WQE.
+ */
+extern void cvmx_helper_cfg_store_short_packets_in_wqe(void);
+
+/*
+ * Allocated a block of internal ports and queues for the specified
+ * interface/port
+ *
+ * @param  interface  the interface for which the internal ports and queues
+ *                    are requested
+ * @param  port       the index of the port within in the interface for which
+                      the internal ports and queues are requested.
+ * @param  pot_count  the number of internal ports requested
+ * @param  queue_cnt  the number of queues requested for each of the internal
+ *                    port. This call will allocate a total of
+ *		      (port_cnt * queue_cnt) queues
+ *
+ * @return  0 on success
+ *         -1 on failure
+ *
+ * LR: Called ONLY from comfig-parse!
+ */
+ int cvmx_pko_alloc_iport_and_queues(int interface, int port, int port_cnt,
+				     int queue_cnt);
+
+/*
+ * Allocated a block of queues for the specified port.
+ *
+ * @param  port   the internal port for which the queues are requested
+ * @param  count  the number of queues requested
+ *
+ * @return  0 on success
+ *         -1 on failure
+ */
+int cvmx_pko_queue_alloc(uint64_t port, uint64_t count);
+
+/*
+ * Free the queues that are associated with the specified port
+ *
+ * @param  port   the internal port for which the queues are freed.
+ *
+ * @return  0 on success
+ *         -1 on failure
+ */
+int cvmx_pko_queue_free(uint64_t port);
+
+/*
+ * Initializes the pko queue range data structure.
+ * @return  0 on success
+ *         -1 on failure
+ */
+int init_cvmx_pko_que_range(void);
+
+/*
+ * Frees up all the allocated ques.
+ */
+void cvmx_pko_queue_free_all(void);
+
+/**
+ * Returns if port is valid for a given interface
+ *
+ * @param interface  interface to check
+ * @param index      port index in the interface
+ *
+ * @return status of the port present or not.
+ */
+int cvmx_helper_is_port_valid(int interface, int index);
+
+/**
+ * Set whether or not a port is valid
+ *
+ * @param interface interface to set
+ * @param index     port index to set
+ * @param valid     set 0 to make port invalid, 1 for valid
+ */
+void cvmx_helper_set_port_valid(int interface, int index, bool valid);
+
+/**
+ * @INTERNAL
+ * Return if port is in PHY mode
+ *
+ * @param interface the interface number
+ * @param index the port's index number
+ *
+ * @return 1 if port is in PHY mode, 0 if port is in MAC mode
+ */
+extern bool cvmx_helper_get_mac_phy_mode(int interface, int index);
+extern void cvmx_helper_set_mac_phy_mode(int interface, int index, bool valid);
+
+/**
+ * @INTERNAL
+ * Return if port is in 1000Base X mode
+ *
+ * @param interface the interface number
+ * @param index the port's index number
+ *
+ * @return 1 if port is in 1000Base X mode, 0 if port is in SGMII mode
+ */
+extern bool cvmx_helper_get_1000x_mode(int interface, int index);
+extern void cvmx_helper_set_1000x_mode(int interface, int index, bool valid);
+
+/**
+ * @INTERNAL
+ * Return if an AGL port should bypass the RX clock delay
+ *
+ * @param interface the interface number
+ * @param index the port's index number
+ */
+extern bool cvmx_helper_get_agl_rx_clock_delay_bypass(int interface, int index);
+extern void cvmx_helper_set_agl_rx_clock_delay_bypass(int interface, int index,
+						      bool valid);
+
+/**
+ * @INTERNAL
+ * Forces a link to always return that it is up ignoring the PHY (if present)
+ *
+ * @param interface the interface number
+ * @param index the port's index
+ */
+extern bool cvmx_helper_get_port_force_link_up(int interface, int index);
+extern void cvmx_helper_set_port_force_link_up(int interface, int index,
+					       bool value);
+
+/**
+ * @INTERNAL
+ * Return the AGL port rx clock skew, only used
+ * if agl_rx_clock_delay_bypass is set.
+ *
+ * @param interface the interface number
+ * @param index the port's index number
+ */
+extern uint8_t cvmx_helper_get_agl_rx_clock_skew(int interface, int index);
+extern void cvmx_helper_set_agl_rx_clock_skew(int interface, int index,
+					      uint8_t value);
+
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+/**
+ * @INTERNAL
+ * Store the FDT node offset in the device tree of a port
+ *
+ * @param xiface	node and interface
+ * @param index		port index
+ * @param node_offset	node offset to store
+ */
+extern void cvmx_helper_set_port_fdt_node_offset(int xiface, int index,
+						 int node_offset);
+
+/**
+ * @INTERNAL
+ * Return the FDT node offset in the device tree of a port
+ *
+ * @param xiface	node and interface
+ * @param index		port index
+ * @return		node offset of port or -1 if invalid
+ */
+extern int cvmx_helper_get_port_fdt_node_offset(int xiface, int index);
+
+/**
+ * @INTERNAL
+ * Store the FDT node offset in the device tree of a phy
+ *
+ * @param xiface	node and interface
+ * @param index		port index
+ * @param node_offset	node offset to store
+ */
+extern void cvmx_helper_set_phy_fdt_node_offset(int xiface, int index,
+						int node_offset);
+
+/**
+ * @INTERNAL
+ * Return the FDT node offset in the device tree of a phy
+ *
+ * @param xiface	node and interface
+ * @param index		port index
+ * @return		node offset of phy or -1 if invalid
+ */
+extern int cvmx_helper_get_phy_fdt_node_offset(int xiface, int index);
+#endif /* !CVMX_BUILD_FOR_LINUX_KERNEL */
+
+/*
+ * Initializes cvmx with user specified config info.
+ */
+int cvmx_user_static_config(void);
+void cvmx_pko_queue_show(void);
+int cvmx_fpa_pool_init_from_cvmx_config(void);
+int __cvmx_helper_init_port_valid(void);
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+}
+/* *INDENT-ON* */
+#endif
+#endif /* __CVMX_HELPER_CFG_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-helper-ilk.h b/arch/mips/include/asm/octeon/cvmx-helper-ilk.h
new file mode 100644
index 0000000..4da1c18
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-helper-ilk.h
@@ -0,0 +1,134 @@
+/***********************license start***************
+ * Copyright (c) 2010  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Functions for ILK initialization, configuration,
+ * and monitoring.
+ *
+ * <hr>$Revision: 41586 $<hr>
+ */
+#ifndef __CVMX_HELPER_ILK_H__
+#define __CVMX_HELPER_ILK_H__
+
+extern int __cvmx_helper_ilk_enumerate(int interface);
+
+/**
+ * @INTERNAL
+ * Initialize all calendar entries to the xoff state. This
+ * means no data is sent or received.
+ *
+ * @param interface Interface whose calendar are to be initialized.
+ */
+extern void __cvmx_ilk_init_cal(int interface);
+
+/**
+ * @INTERNAL
+ * Setup the channel's tx calendar entry.
+ *
+ * @param interface Interface channel belongs to
+ * @param channel Channel whose calendar entry is to be updated
+ * @param bpid Bpid assigned to the channel
+ */
+extern void __cvmx_ilk_write_tx_cal_entry(int interface, int channel,
+					  unsigned char bpid);
+
+/**
+ * @INTERNAL
+ * Setup the channel's rx calendar entry.
+ *
+ * @param interface Interface channel belongs to
+ * @param channel Channel whose calendar entry is to be updated
+ * @param pipe PKO assigned to the channel
+ */
+void __cvmx_ilk_write_rx_cal_entry(int interface, int channel,
+				   unsigned char pipe);
+
+/**
+ * @INTERNAL
+ * Probe a ILK interface and determine the number of ports
+ * connected to it. The ILK interface should still be down after
+ * this call.
+ *
+ * @param interface Interface to probe
+ *
+ * @return Number of ports on the interface. Zero to disable.
+ */
+extern int __cvmx_helper_ilk_probe(int xiface);
+
+/**
+ * @INTERNAL
+ * Bringup and enable a ILK interface. After this call packet
+ * I/O should be fully functional. This is called with IPD
+ * enabled but PKO disabled.
+ *
+ * @param interface Interface to bring up
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int __cvmx_helper_ilk_enable(int xiface);
+
+/**
+ * @INTERNAL
+ * Return the link state of an IPD/PKO port as returned by ILK link status.
+ *
+ * @param ipd_port IPD/PKO port to query
+ *
+ * @return Link state
+ */
+extern cvmx_helper_link_info_t __cvmx_helper_ilk_link_get(int ipd_port);
+
+/**
+ * @INTERNAL
+ * Configure an IPD/PKO port for the specified link state. This
+ * function does not influence auto negotiation at the PHY level.
+ * The passed link state must always match the link state returned
+ * by cvmx_helper_link_get(). It is normally best to use
+ * cvmx_helper_link_autoconf() instead.
+ *
+ * @param ipd_port  IPD/PKO port to configure
+ * @param link_info The new link state
+ *
+ * @return Zero on success, negative on failure
+ */
+extern int __cvmx_helper_ilk_link_set(int ipd_port, cvmx_helper_link_info_t link_info);
+
+extern void __cvmx_helper_ilk_show_stats(void);
+#endif
diff --git a/arch/mips/include/asm/octeon/cvmx-helper-pki.h b/arch/mips/include/asm/octeon/cvmx-helper-pki.h
new file mode 100644
index 0000000..abf7645
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-helper-pki.h
@@ -0,0 +1,238 @@
+/***********************license start***************
+ * Copyright (c) 2003-2010  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Helper functions for PKI
+ */
+
+#ifndef __CVMX_HELPER_PKI_H__
+#define __CVMX_HELPER_PKI_H__
+
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include <asm/octeon/cvmx.h>
+#include <asm/octeon/cvmx-pki.h>
+#else
+#include "cvmx-pki.h"
+#endif
+
+#ifdef __cplusplus
+/* *INDENT-OFF* */
+extern "C" {
+/* *INDENT-ON* */
+#endif
+
+/* Modify this if more than 8 ilk channels need to be supported */
+#define CVMX_MAX_PORT_PER_INTERFACE	8
+#define CVMX_MAX_QOS_PRIORITY		64
+#define CVMX_PKI_FIND_AVAILABLE_RSRC    (-1)
+
+struct cvmx_pki_qos_schd {
+	bool pool_per_qos;	/* This qos priority will use its own pool, if FALSE use port pool */
+        int pool;		/* pool number to use, if -1 allocated by sdk otherwise software should alloc it */
+	char *pool_name;
+	uint64_t pool_buff_size;/* size of buffer in pool , if this priority is using its own pool*/
+                                /* it's good to have same buffer size if qos are using separate pools */
+	uint64_t pool_max_buff;	/* number of max buffers allowed in the pool, if this priority is using its own pool*/
+	bool aura_per_qos;	/* This qos priority will use its own aura, if FALSE use port aura */
+        int aura;		/* aura number to use, if -1 allocated by sdk otherwise software should alloc it */
+	char *aura_name;
+	uint64_t aura_buff_cnt;	/* number of buffers in aura, if this priority is using its own aura*/
+	bool sso_grp_per_qos;	/* This qos priority will use its own group, if FALSE use port group */
+        int sso_grp;		/* group number to use, if -1 allocated by sdk otherwise software should alloc it */
+	uint16_t port_add;      /* for BGX super MAC ports which wants to have PFC enabled */
+        int qpg_base;           /* offset in qpg table to use, if -1 allocated by sdk otherwise software should alloc it*/
+};
+
+struct cvmx_pki_prt_schd {
+        bool cfg_port;          /* Set to 1 if this port on the interface is not used */
+	int style;              /* If style_per_prt is TRUE in interface schd */
+	bool pool_per_prt; 	/* Port will use its own pool, if FALSE use interface pool */
+	int pool;		/* pool number to use, if -1 allocated by sdk otherwise software should alloc it*/
+	char *pool_name;
+	uint64_t pool_buff_size;/*size of buffer in pool , if this port is using its own pool*/
+                                /* it's good to have same buffer size if ports are using same style but different pools*/
+	uint64_t pool_max_buff;	/* number of max buffers allowed in the pool, if this port is using its own pool*/
+	bool aura_per_prt;	/* port will use its own aura, if FALSE use interface aura */
+        int aura;		/* aura number to use, if -1 allocated by sdk otherwise software should alloc it */
+	char *aura_name;
+	uint64_t aura_buff_cnt;	/* number of buffers in aura, if this pool is using its own aura*/
+	bool sso_grp_per_prt; 	/* port will use its own sso group, if FALSE use interface group*/
+        int sso_grp;		/* sso group number to use, if -1 allocated by sdk otherwise software should alloc it */
+	enum cvmx_pki_qpg_qos qpg_qos;
+        int qpg_base;           /* offset in qpg table to use, if -1 allocated by sdk otherwise software should alloc it*/
+	struct cvmx_pki_qos_schd qos_s[CVMX_MAX_QOS_PRIORITY];
+};
+
+struct cvmx_pki_intf_schd {
+	bool style_per_prt;	/* Every port will use different style/profile */
+	bool style_per_intf;	/* otherwise all ports on this interface will use same style/profile */
+        int style;              /* style number to use, if -1 allocated by sdk otherwise software should alloc it*/
+	bool pool_per_intf; 	/* Ports will use either this shared pool or their own pool*/
+        int pool;		/* pool number to use, if -1 allocated by sdk otherwise software should alloc it*/
+	char *pool_name;
+	uint64_t pool_buff_size;
+	uint64_t pool_max_buff;
+	bool aura_per_intf; 	/* Ports will use either this shared aura or their own aura */
+        int aura;		/* aura number to use, if -1 allocated by sdk otherwise software should alloc it*/
+	char *aura_name;
+	uint64_t aura_buff_cnt;
+	bool sso_grp_per_intf;	/* Ports will use either this shared group or their own aura */
+        int sso_grp;		/* sso group number to use, if -1 allocated by sdk otherwise software should alloc it */
+	bool qos_share_aura;	/* All ports share the same aura for respective qos if qpg_qos used*/
+	bool qos_share_grp; 	/* All ports share the same sso group for respective qos if qps qos used*/
+        int qpg_base;           /* offset in qpg table to use, if -1 allocated by sdk otherwise software should alloc it*/
+	struct cvmx_pki_prt_schd prt_s[CVMX_MAX_PORT_PER_INTERFACE];
+};
+
+struct cvmx_pki_global_schd {
+	bool setup_pool;
+        int pool;              /* pool number to use, if -1 allocated by sdk otherwise software should alloc it*/
+	char *pool_name;
+	uint64_t pool_buff_size;
+	uint64_t pool_max_buff;
+	bool setup_aura;
+        int aura;              /* aura number to use, if -1 allocated by sdk otherwise software should alloc it*/
+	char *aura_name;
+	uint64_t aura_buff_cnt;
+	bool setup_sso_grp;
+        int sso_grp;            /* sso group number to use, if -1 allocated by sdk otherwise software should alloc it */
+};
+
+extern CVMX_SHARED bool cvmx_pki_dflt_init[CVMX_MAX_NODES];
+
+extern CVMX_SHARED struct cvmx_pki_pool_config pki_dflt_pool[CVMX_MAX_NODES];
+extern CVMX_SHARED struct cvmx_pki_aura_config pki_dflt_aura[CVMX_MAX_NODES];
+extern uint64_t style_qpg_base_map[CVMX_MAX_NODES];
+extern CVMX_SHARED struct cvmx_pki_style_config pki_dflt_style[CVMX_MAX_NODES];
+extern CVMX_SHARED struct cvmx_pki_pkind_config pki_dflt_pkind[CVMX_MAX_NODES];
+extern CVMX_SHARED uint64_t pkind_style_map[CVMX_MAX_NODES][CVMX_PKI_NUM_PKIND];
+
+void cvmx_helper_pki_enable(int node);
+int cvmx_helper_setup_pki_port(int node, int pknd);
+int cvmx_helper_pki_setup_qpg_table(int node, int num_entries,
+				    struct cvmx_pki_qpg_config *qpg_cfg);
+void cvmx_helper_pki_set_fcs_op(int node, int interface, int nports, int has_fcs);
+int cvmx_helper_pki_get_num_qpg_entry(enum cvmx_pki_qpg_qos qpg_qos);
+int __cvmx_helper_pki_port_setup(int node, int ipd_port);
+int __cvmx_helper_pki_global_setup(int node);
+int __cvmx_helper_pki_install_default_vlan(int node);
+void cvmx_helper_pki_set_dflt_pool(int node, int pool,
+				   int buffer_size, int buffer_count);
+void cvmx_helper_pki_set_dflt_aura(int node, int aura,
+				   int pool, int buffer_count);
+void cvmx_helper_pki_set_dflt_pool_buffer(int node, int buffer_count);
+void cvmx_helper_pki_set_dflt_aura_buffer(int node, int buffer_count);
+
+/**
+ * This function sets up aura QOS for RED, backpressure and tail-drop.
+ *
+ * @param node       node number.
+ * @param aura       aura to configure.
+ * @param ena_red       enable RED based on [DROP] and [PASS] levels
+ *			1: enable 0:disable
+ * @param pass_thresh   pass threshold for RED.
+ * @param drop_thresh   drop threshold for RED
+ * @param ena_bp        enable backpressure based on [BP] level.
+ *			1:enable 0:disable
+ * @param bp_thresh     backpressure threshold.
+ * @param ena_drop      enable tail drop.
+ *                      1:enable 0:disable
+ * @return Zero on success. Negative on failure
+ */
+int cvmx_helper_setup_aura_qos(int node, int aura, bool ena_red, bool ena_drop,
+			       uint64_t pass_thresh, uint64_t drop_thresh,
+			       bool ena_bp, uint64_t bp_thresh);
+/**
+ * This function maps specified bpid to all the auras from which it can receive bp and
+ * then maps that bpid to all the channels, that bpid can asserrt bp on.
+ *
+ * @param node          node number.
+ * @param aura          aura number which will back pressure specified bpid.
+ * @param bpid          bpid to map.
+ * @param chl_map       array of channels to map to that bpid.
+ * @param chl_cnt	number of channel/ports to map to that bpid.
+ * @return Zero on success. Negative on failure
+ */
+int cvmx_helper_pki_map_aura_chl_bpid(int node, uint16_t aura, uint16_t bpid,
+                                      uint16_t chl_map[], uint16_t chl_cnt);
+
+void cvmx_helper_pki_set_dflt_pkind_map(int node, int pkind, int style);
+void cvmx_helper_pki_get_dflt_style(int node, struct cvmx_pki_style_config *style_cfg);
+void cvmx_helper_pki_set_dflt_style(int node, struct cvmx_pki_style_config *style_cfg);
+
+
+/* Shutdown complete PKI hardware and software resources */
+void cvmx_helper_pki_shutdown(int node);
+
+int cvmx_helper_pki_set_gbl_schd(int node, struct cvmx_pki_global_schd *gbl_schd);
+
+int cvmx_helper_pki_init_interface(int , struct cvmx_pki_intf_schd *intf, struct cvmx_pki_global_schd *gbl_schd);
+
+int cvmx_helper_pki_init_interface(int xiface,
+				   struct cvmx_pki_intf_schd *intf, struct cvmx_pki_global_schd *gbl_schd);
+int cvmx_helper_pki_init_port(int ipd_port, struct cvmx_pki_prt_schd *prtsch);
+void cvmx_helper_pki_no_dflt_init(int node);
+void cvmx_helper_pki_get_dflt_style(int node, struct cvmx_pki_style_config *style_cfg);
+void cvmx_helper_pki_set_dflt_style(int node, struct cvmx_pki_style_config *style_cfg);
+/**
+ * This function sets the wqe buffer mode of all ports. First packet data buffer can reside
+ * either in same buffer as wqe OR it can go in separate buffer. If used the later mode,
+ * make sure software allocate enough buffers to now have wqe separate from packet data.
+ * @param node	                node number.
+ * @param pkt_outside_wqe.	0 = The packet link pointer will be at word [FIRST_SKIP]
+ *				    immediately followed by packet data, in the same buffer
+ *				    as the work queue entry.
+ *				1 = The packet link pointer will be at word [FIRST_SKIP] in a new
+ *				    buffer separate from the work queue entry. Words following the
+ *				    WQE in the same cache line will be zeroed, other lines in the
+ *				    buffer will not be modified and will retain stale data (from the
+ *				    buffers previous use). This setting may decrease the peak PKI
+ *				    performance by up to half on small packets.
+ */
+void cvmx_helper_pki_set_wqe_mode(int node, bool pkt_outside_wqe);
+void pki_wqe_dump(const cvmx_wqe_78xx_t* wqp);
+
+#ifdef __cplusplus
+/* *INDENT-OFF* */
+}
+/* *INDENT-ON* */
+#endif
+#endif /* __CVMX_HELPER_PKI_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-helper-pko.h b/arch/mips/include/asm/octeon/cvmx-helper-pko.h
new file mode 100644
index 0000000..eff9cfd
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-helper-pko.h
@@ -0,0 +1,100 @@
+/***********************license start***************
+ * Copyright (c) 2003-2014  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * PKO helper, configuration API
+ */
+
+#ifndef __CVMX_HELPER_PKO_H__
+#define __CVMX_HELPER_PKO_H__
+
+/* CSR typedefs have been moved to cvmx-pko-defs.h */
+
+#if 0	// XXX Not clear what this is intended for !
+/**
+ * Definition of internal state for Packet output processing
+ */
+typedef struct {
+	uint64_t *start_ptr;		/**< ptr to start of buffer, offset kept in FAU reg */
+} cvmx_pko_state_elem_t;
+#endif
+
+/**
+ * cvmx_override_pko_queue_priority(int ipd_port, uint64_t
+ * priorities[16]) is a function pointer. It is meant to allow
+ * customization of the PKO queue priorities based on the port
+ * number. Users should set this pointer to a function before
+ * calling any cvmx-helper operations.
+ */
+extern CVMX_SHARED void (*cvmx_override_pko_queue_priority) (int ipd_port, uint8_t * priorities);
+
+/**
+ * Gets the fpa pool number of pko pool
+ */
+int64_t cvmx_fpa_get_pko_pool(void);
+
+/**
+ * Gets the buffer size of pko pool
+ */
+uint64_t cvmx_fpa_get_pko_pool_block_size(void);
+
+/**
+ * Gets the buffer size  of pko pool
+ */
+uint64_t cvmx_fpa_get_pko_pool_buffer_count(void);
+
+
+int cvmx_helper_pko_init(void);
+
+/*
+ * This function is a no-op
+ * included here for backwards compatibility only.
+ */
+static inline  int cvmx_pko_initialize_local(void)
+{
+    return 0;
+}
+
+extern int __cvmx_helper_pko_drain(void);
+
+extern int __cvmx_helper_interface_setup_pko(int interface);
+
+#endif /* __CVMX_HELPER_PKO_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-helper-pko3.h b/arch/mips/include/asm/octeon/cvmx-helper-pko3.h
new file mode 100644
index 0000000..b6ef601
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-helper-pko3.h
@@ -0,0 +1,107 @@
+/***********************license start***************
+ * Copyright (c) 2003-2013  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * <hr>$Revision: 0 $<hr>
+ */
+
+#ifndef __CVMX_HELPER_PKO3_H__
+#define __CVMX_HELPER_PKO3_H__
+
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+extern "C" {
+/* *INDENT-ON* */
+#endif
+
+/*
+ * Initialize PKO3 unit on the current node.
+ *
+ * Covers the common hardware, memory and global configuration.
+ * Per-interface intialization is performed separately.
+ *
+ * @return 0 on success.
+ *
+ */
+extern int cvmx_helper_pko3_init_global(unsigned int node);
+int __cvmx_helper_pko3_init_global(unsigned int node, uint16_t gaura);
+
+/** 
+ * Initialize a simple interface with a a given number of
+ * fair or prioritized queues.
+ * This function will assign one channel per sub-interface.
+ */
+int __cvmx_pko3_config_gen_interface(int xiface, uint8_t subif,
+				     uint8_t num_queues, bool prioritized);
+
+/*
+ * Configure and initialize PKO3 for an interface
+ *
+ * @param interface is the interface number to configure
+ * @return 0 on success.
+ *
+ */
+int cvmx_helper_pko3_init_interface(int xiface);
+int __cvmx_pko3_helper_dqs_activate(int xiface, int index, bool min_pad);
+
+/**
+ * Uninitialize PKO3 interface
+ *
+ * Release all resources held by PKO for an interface.
+ * The shutdown code is the same for all supported interfaces.
+ */
+extern int cvmx_helper_pko3_shut_interface(int xiface);
+
+/**
+ * Shutdown PKO3
+ *
+ * Should be called after all interfaces have been shut down on the PKO3.
+ *
+ * Disables the PKO, frees all its buffers.
+ */
+extern int cvmx_helper_pko3_shutdown(unsigned int node);
+
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+}
+/* *INDENT-ON* */
+#endif
+#endif /* __CVMX_HELPER_PKO3_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-helper-util.h b/arch/mips/include/asm/octeon/cvmx-helper-util.h
index f446f21..c014100 100644
--- a/arch/mips/include/asm/octeon/cvmx-helper-util.h
+++ b/arch/mips/include/asm/octeon/cvmx-helper-util.h
@@ -34,6 +34,98 @@
 #ifndef __CVMX_HELPER_UTIL_H__
 #define __CVMX_HELPER_UTIL_H__
 
+#include "cvmx.h"
+#include "cvmx-mio-defs.h"
+#include "cvmx-helper.h"
+#include "cvmx-fpa3.h"
+
+typedef char cvmx_pknd_t;
+typedef char cvmx_bpid_t;
+
+#define CVMX_INVALID_PKND	((cvmx_pknd_t) -1)
+#define CVMX_INVALID_BPID	((cvmx_bpid_t) -1)
+#define CVMX_MAX_PKND		((cvmx_pknd_t) 64)
+#define CVMX_MAX_BPID		((cvmx_bpid_t) 64)
+
+#define CVMX_HELPER_MAX_IFACE		11
+#define CVMX_HELPER_MAX_PORTS		16
+
+
+/* Maximum range for normalized (a.k.a. IPD) port numbers (12-bit field) */
+#define	CVMX_PKO3_IPD_NUM_MAX	0x1000	//FIXME- take it from someplace else ?
+
+#define CVMX_PKO3_IPD_PORT_NULL (CVMX_PKO3_IPD_NUM_MAX-1)
+
+struct cvmx_xport {
+	int node;
+	int port;
+};
+typedef struct cvmx_xport cvmx_xport_t;
+
+static inline struct cvmx_xport cvmx_helper_ipd_port_to_xport(int ipd_port)
+{
+	struct cvmx_xport r;
+	r.port = ipd_port & (CVMX_PKO3_IPD_NUM_MAX - 1);
+	r.node = (ipd_port >> 12) & CVMX_NODE_MASK;
+	return r;
+}
+
+static inline int cvmx_helper_node_to_ipd_port(int node, int index)
+{
+	return (node << 12) + index;
+}
+
+struct cvmx_xiface {
+        int node;
+        int interface;
+};
+typedef struct cvmx_xiface cvmx_xiface_t;
+
+/**
+ * Return node and interface number from XIFACE.
+ *
+ * @param xiface interface with node information
+ *
+ * @return struct that contains node and interface number.
+ */
+static inline struct cvmx_xiface cvmx_helper_xiface_to_node_interface(int xiface)
+{
+	struct cvmx_xiface interface_node;
+	/*
+	 * If the majic number 0xde0000 is not present in the
+	 * interface, then assume it is node 0.
+	 */
+
+	if (((xiface >> 0x8) & 0xff) == 0xde) {
+		interface_node.node = (xiface >> 16) & CVMX_NODE_MASK;
+		interface_node.interface = xiface & 0xff;
+	} else {
+		interface_node.node = cvmx_get_node_num();
+		interface_node.interface = xiface & 0xff;
+	}
+	return interface_node;
+}
+
+/* Used internally only*/
+static inline bool __cvmx_helper_xiface_is_null(int xiface)
+{
+	return (xiface & 0xff) == 0xff;
+}
+
+#define __CVMX_XIFACE_NULL 0xff
+
+/**
+ * Return interface with majic number and node information (XIFACE)
+ *
+ * @param node       node of the interface referred to
+ * @param interface  interface to use.
+ *
+ * @return
+ */
+static inline int cvmx_helper_node_interface_to_xiface(int node, int interface)
+{
+	return ((node & CVMX_NODE_MASK) << 16) | (0xde << 8) | (interface & 0xff);
+}
 /**
  * Convert a interface mode into a human readable string
  *
@@ -212,4 +304,30 @@ extern int cvmx_helper_get_interface_num(int ipd_port);
  */
 extern int cvmx_helper_get_interface_index_num(int ipd_port);
 
+/**
+ * Get port kind for a given port in an interface.
+ *
+ * @param interface  Interface
+ * @param port       index of the port in the interface
+ *
+ * @return port kind on sucicess  and -1 on failure
+ */
+extern int cvmx_helper_get_pknd(int xiface, int index);
+
+/*
+ * Return number of array alements
+ */
+#define NUM_ELEMENTS(arr) (sizeof(arr)/sizeof((arr)[0]))
+
+/**
+ * Returns the PKO port number for a port on the given interface,
+ * This is the base pko_port for o68 and ipd_port for older models.
+ *
+ * @param interface Interface to use
+ * @param port      Port on the interface
+ *
+ * @return PKO port number and -1 on error.
+ */
+extern int cvmx_helper_get_pko_port(int interface, int port);
+
 #endif /* __CVMX_HELPER_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-helper.h b/arch/mips/include/asm/octeon/cvmx-helper.h
index 5a3090d..9a8bfed 100644
--- a/arch/mips/include/asm/octeon/cvmx-helper.h
+++ b/arch/mips/include/asm/octeon/cvmx-helper.h
@@ -36,7 +36,7 @@
 
 #include <asm/octeon/cvmx-config.h>
 #include <asm/octeon/cvmx-fpa.h>
-#include <asm/octeon/cvmx-wqe.h>
+#include <asm/octeon/cvmx-pow.h>
 
 typedef enum {
 	CVMX_HELPER_INTERFACE_MODE_DISABLED,
@@ -49,6 +49,7 @@ typedef enum {
 	CVMX_HELPER_INTERFACE_MODE_PICMG,
 	CVMX_HELPER_INTERFACE_MODE_NPI,
 	CVMX_HELPER_INTERFACE_MODE_LOOP,
+	CVMX_HELPER_INTERFACE_MODE_ILK,
 } cvmx_helper_interface_mode_t;
 
 typedef union {
@@ -69,17 +70,7 @@ typedef union {
 #include <asm/octeon/cvmx-helper-spi.h>
 #include <asm/octeon/cvmx-helper-util.h>
 #include <asm/octeon/cvmx-helper-xaui.h>
-
-/**
- * cvmx_override_pko_queue_priority(int ipd_port, uint64_t
- * priorities[16]) is a function pointer. It is meant to allow
- * customization of the PKO queue priorities based on the port
- * number. Users should set this pointer to a function before
- * calling any cvmx-helper operations.
- */
-extern void (*cvmx_override_pko_queue_priority) (int pko_port,
-						 uint64_t priorities[16]);
-
+#include <asm/octeon/cvmx-helper-ilk.h>
 /**
  * cvmx_override_ipd_port_setup(int ipd_port) is a function
  * pointer. It is meant to allow customization of the IPD port
@@ -222,5 +213,80 @@ extern int cvmx_helper_interface_enumerate(int interface);
  */
 extern int cvmx_helper_configure_loopback(int ipd_port, int enable_internal,
 					  int enable_external);
+/**
+ * @INTERNAL
+ *
+ * @param interface
+ *
+ * @return 0 if PKO does not do FCS and 1 otherwise.
+ */
+int __cvmx_helper_get_has_fcs(int interface);
+
+/**
+ * Convert system-level priority to Ethernet QoS/PCP value
+ *
+ * Calculate the reverse of cvmx_helper_qos2prio() per IEEE 802.1Q-2005.
+ */
+static inline uint8_t cvmx_helper_prio2qos(uint8_t prio)
+{
+        static const unsigned prio_map =
+                7 << (4 * 0) |
+                6 << (4 * 1) |
+                5 << (4 * 2) |
+                4 << (4 * 3) |
+                3 << (4 * 4) |
+                2 << (4 * 5) |
+                0 << (4 * 6) |
+                1 << (4 * 7);
+
+        return (prio_map >> ((prio & 0x7) << 2)) & 0x7;
+}
+
+/**
+ * @INTERNAL
+ * Get the number of ipd_ports on an interface.
+ *
+ * @param interface
+ *
+ * @return the number of ipd_ports on the interface and -1 for error.
+ */
+int __cvmx_helper_get_num_ipd_ports(int interface);
+enum cvmx_pko_padding __cvmx_helper_get_pko_padding(int xiface);
+
+enum cvmx_pko_padding {
+        CVMX_PKO_PADDING_NONE = 0,
+        CVMX_PKO_PADDING_60 = 1,
+};
+
+/**
+ * Returns the number of ports on the given interface.
+ *
+ * @param interface Which interface to return port count for.
+ *
+ * @return Port count for interface
+ *         -1 for uninitialized interface
+ */
+int __cvmx_helper_early_ports_on_interface(int interface);
+
+/**
+ * @INTERNAL
+ *
+ * @param interface
+ *
+ * @return 0 if PKO does not do FCS and 1 otherwise.
+ */
+int __cvmx_helper_get_has_fcs(int interface);
+
+/*
+ * @INTERNAL
+ * Enable packet input/output from the hardware. This function is
+ * called after all internal setup is complete and IPD is enabled.
+ * After this function completes, packets will be accepted from the
+ * hardware ports. PKO should still be disabled to make sure packets
+ * aren't sent out partially setup hardware.
+ *
+ * @return Zero on success, negative on failure
+ */
+int __cvmx_helper_packet_hardware_enable(int xiface);
 
 #endif /* __CVMX_HELPER_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-ilk-defs.h b/arch/mips/include/asm/octeon/cvmx-ilk-defs.h
new file mode 100644
index 0000000..aa3b709
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-ilk-defs.h
@@ -0,0 +1,4979 @@
+/***********************license start***************
+ * Copyright (c) 2003-2014  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+
+/**
+ * cvmx-ilk-defs.h
+ *
+ * Configuration and status register (CSR) type definitions for
+ * Octeon ilk.
+ *
+ * This file is auto generated. Do not edit.
+ *
+ * <hr>$Revision$<hr>
+ *
+ */
+#ifndef __CVMX_ILK_DEFS_H__
+#define __CVMX_ILK_DEFS_H__
+
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_ILK_BIST_SUM CVMX_ILK_BIST_SUM_FUNC()
+static inline uint64_t CVMX_ILK_BIST_SUM_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_ILK_BIST_SUM not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180014000038ull);
+}
+#else
+#define CVMX_ILK_BIST_SUM (CVMX_ADD_IO_SEG(0x0001180014000038ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_ILK_GBL_CFG CVMX_ILK_GBL_CFG_FUNC()
+static inline uint64_t CVMX_ILK_GBL_CFG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_ILK_GBL_CFG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180014000000ull);
+}
+#else
+#define CVMX_ILK_GBL_CFG (CVMX_ADD_IO_SEG(0x0001180014000000ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_ILK_GBL_ERR_CFG CVMX_ILK_GBL_ERR_CFG_FUNC()
+static inline uint64_t CVMX_ILK_GBL_ERR_CFG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_ILK_GBL_ERR_CFG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180014000058ull);
+}
+#else
+#define CVMX_ILK_GBL_ERR_CFG (CVMX_ADD_IO_SEG(0x0001180014000058ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_ILK_GBL_INT CVMX_ILK_GBL_INT_FUNC()
+static inline uint64_t CVMX_ILK_GBL_INT_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_ILK_GBL_INT not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180014000008ull);
+}
+#else
+#define CVMX_ILK_GBL_INT (CVMX_ADD_IO_SEG(0x0001180014000008ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_ILK_GBL_INT_EN CVMX_ILK_GBL_INT_EN_FUNC()
+static inline uint64_t CVMX_ILK_GBL_INT_EN_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX)))
+		cvmx_warn("CVMX_ILK_GBL_INT_EN not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180014000010ull);
+}
+#else
+#define CVMX_ILK_GBL_INT_EN (CVMX_ADD_IO_SEG(0x0001180014000010ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_ILK_INT_SUM CVMX_ILK_INT_SUM_FUNC()
+static inline uint64_t CVMX_ILK_INT_SUM_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX)))
+		cvmx_warn("CVMX_ILK_INT_SUM not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180014000030ull);
+}
+#else
+#define CVMX_ILK_INT_SUM (CVMX_ADD_IO_SEG(0x0001180014000030ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_LNEX_TRN_CTL(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 15)))))
+		cvmx_warn("CVMX_ILK_LNEX_TRN_CTL(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00011800140380F0ull) + ((offset) & 15) * 1024;
+}
+#else
+#define CVMX_ILK_LNEX_TRN_CTL(offset) (CVMX_ADD_IO_SEG(0x00011800140380F0ull) + ((offset) & 15) * 1024)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_LNEX_TRN_LD(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 15)))))
+		cvmx_warn("CVMX_ILK_LNEX_TRN_LD(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00011800140380E0ull) + ((offset) & 15) * 1024;
+}
+#else
+#define CVMX_ILK_LNEX_TRN_LD(offset) (CVMX_ADD_IO_SEG(0x00011800140380E0ull) + ((offset) & 15) * 1024)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_LNEX_TRN_LP(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 15)))))
+		cvmx_warn("CVMX_ILK_LNEX_TRN_LP(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00011800140380E8ull) + ((offset) & 15) * 1024;
+}
+#else
+#define CVMX_ILK_LNEX_TRN_LP(offset) (CVMX_ADD_IO_SEG(0x00011800140380E8ull) + ((offset) & 15) * 1024)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_ILK_LNE_DBG CVMX_ILK_LNE_DBG_FUNC()
+static inline uint64_t CVMX_ILK_LNE_DBG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_ILK_LNE_DBG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180014030008ull);
+}
+#else
+#define CVMX_ILK_LNE_DBG (CVMX_ADD_IO_SEG(0x0001180014030008ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_ILK_LNE_STS_MSG CVMX_ILK_LNE_STS_MSG_FUNC()
+static inline uint64_t CVMX_ILK_LNE_STS_MSG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_ILK_LNE_STS_MSG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180014030000ull);
+}
+#else
+#define CVMX_ILK_LNE_STS_MSG (CVMX_ADD_IO_SEG(0x0001180014030000ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_ILK_RID_CFG CVMX_ILK_RID_CFG_FUNC()
+static inline uint64_t CVMX_ILK_RID_CFG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_ILK_RID_CFG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180014000050ull);
+}
+#else
+#define CVMX_ILK_RID_CFG (CVMX_ADD_IO_SEG(0x0001180014000050ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_ILK_RXF_IDX_PMAP CVMX_ILK_RXF_IDX_PMAP_FUNC()
+static inline uint64_t CVMX_ILK_RXF_IDX_PMAP_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX)))
+		cvmx_warn("CVMX_ILK_RXF_IDX_PMAP not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180014000020ull);
+}
+#else
+#define CVMX_ILK_RXF_IDX_PMAP (CVMX_ADD_IO_SEG(0x0001180014000020ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_ILK_RXF_MEM_PMAP CVMX_ILK_RXF_MEM_PMAP_FUNC()
+static inline uint64_t CVMX_ILK_RXF_MEM_PMAP_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX)))
+		cvmx_warn("CVMX_ILK_RXF_MEM_PMAP not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180014000028ull);
+}
+#else
+#define CVMX_ILK_RXF_MEM_PMAP (CVMX_ADD_IO_SEG(0x0001180014000028ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_BYTE_CNTX(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 255)) && ((block_id <= 1))))))
+		cvmx_warn("CVMX_ILK_RXX_BYTE_CNTX(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180014023000ull) + (((offset) & 255) + ((block_id) & 1) * 0x800ull) * 8;
+}
+#else
+#define CVMX_ILK_RXX_BYTE_CNTX(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180014023000ull) + (((offset) & 255) + ((block_id) & 1) * 0x800ull) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_CAL_ENTRYX(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 287)) && ((block_id <= 1))))))
+		cvmx_warn("CVMX_ILK_RXX_CAL_ENTRYX(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180014021000ull) + (((offset) & 511) + ((block_id) & 1) * 0x800ull) * 8;
+}
+#else
+#define CVMX_ILK_RXX_CAL_ENTRYX(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180014021000ull) + (((offset) & 511) + ((block_id) & 1) * 0x800ull) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_CFG0(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_RXX_CFG0(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014020000ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_RXX_CFG0(offset) (CVMX_ADD_IO_SEG(0x0001180014020000ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_CFG1(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_RXX_CFG1(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014020008ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_RXX_CFG1(offset) (CVMX_ADD_IO_SEG(0x0001180014020008ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_CHAX(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 255)) && ((block_id <= 1))))))
+		cvmx_warn("CVMX_ILK_RXX_CHAX(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180014002000ull) + (((offset) & 255) + ((block_id) & 1) * 0x200ull) * 8;
+}
+#else
+#define CVMX_ILK_RXX_CHAX(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180014002000ull) + (((offset) & 255) + ((block_id) & 1) * 0x200ull) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_CHA_XONX(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 1))))))
+		cvmx_warn("CVMX_ILK_RXX_CHA_XONX(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180014020400ull) + (((offset) & 3) + ((block_id) & 1) * 0x800ull) * 8;
+}
+#else
+#define CVMX_ILK_RXX_CHA_XONX(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180014020400ull) + (((offset) & 3) + ((block_id) & 1) * 0x800ull) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_ERR_CFG(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_RXX_ERR_CFG(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00011800140200E0ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_RXX_ERR_CFG(offset) (CVMX_ADD_IO_SEG(0x00011800140200E0ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_FLOW_CTL0(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_RXX_FLOW_CTL0(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014020090ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_RXX_FLOW_CTL0(offset) (CVMX_ADD_IO_SEG(0x0001180014020090ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_FLOW_CTL1(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_RXX_FLOW_CTL1(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014020098ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_RXX_FLOW_CTL1(offset) (CVMX_ADD_IO_SEG(0x0001180014020098ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_IDX_CAL(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_RXX_IDX_CAL(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00011800140200A0ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_RXX_IDX_CAL(offset) (CVMX_ADD_IO_SEG(0x00011800140200A0ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_IDX_STAT0(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_RXX_IDX_STAT0(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014020070ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_RXX_IDX_STAT0(offset) (CVMX_ADD_IO_SEG(0x0001180014020070ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_IDX_STAT1(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_RXX_IDX_STAT1(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014020078ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_RXX_IDX_STAT1(offset) (CVMX_ADD_IO_SEG(0x0001180014020078ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_INT(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_RXX_INT(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014020010ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_RXX_INT(offset) (CVMX_ADD_IO_SEG(0x0001180014020010ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_INT_EN(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_RXX_INT_EN(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014020018ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_RXX_INT_EN(offset) (CVMX_ADD_IO_SEG(0x0001180014020018ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_JABBER(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_RXX_JABBER(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00011800140200B8ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_RXX_JABBER(offset) (CVMX_ADD_IO_SEG(0x00011800140200B8ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_MEM_CAL0(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_RXX_MEM_CAL0(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00011800140200A8ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_RXX_MEM_CAL0(offset) (CVMX_ADD_IO_SEG(0x00011800140200A8ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_MEM_CAL1(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_RXX_MEM_CAL1(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00011800140200B0ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_RXX_MEM_CAL1(offset) (CVMX_ADD_IO_SEG(0x00011800140200B0ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_MEM_STAT0(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_RXX_MEM_STAT0(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014020080ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_RXX_MEM_STAT0(offset) (CVMX_ADD_IO_SEG(0x0001180014020080ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_MEM_STAT1(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_RXX_MEM_STAT1(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014020088ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_RXX_MEM_STAT1(offset) (CVMX_ADD_IO_SEG(0x0001180014020088ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_PKT_CNTX(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 255)) && ((block_id <= 1))))))
+		cvmx_warn("CVMX_ILK_RXX_PKT_CNTX(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180014022000ull) + (((offset) & 255) + ((block_id) & 1) * 0x800ull) * 8;
+}
+#else
+#define CVMX_ILK_RXX_PKT_CNTX(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180014022000ull) + (((offset) & 255) + ((block_id) & 1) * 0x800ull) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_RID(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_RXX_RID(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00011800140200C0ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_RXX_RID(offset) (CVMX_ADD_IO_SEG(0x00011800140200C0ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_STAT0(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_RXX_STAT0(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014020020ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_RXX_STAT0(offset) (CVMX_ADD_IO_SEG(0x0001180014020020ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_STAT1(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_RXX_STAT1(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014020028ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_RXX_STAT1(offset) (CVMX_ADD_IO_SEG(0x0001180014020028ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_STAT2(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_RXX_STAT2(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014020030ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_RXX_STAT2(offset) (CVMX_ADD_IO_SEG(0x0001180014020030ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_STAT3(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_RXX_STAT3(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014020038ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_RXX_STAT3(offset) (CVMX_ADD_IO_SEG(0x0001180014020038ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_STAT4(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_RXX_STAT4(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014020040ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_RXX_STAT4(offset) (CVMX_ADD_IO_SEG(0x0001180014020040ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_STAT5(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_RXX_STAT5(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014020048ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_RXX_STAT5(offset) (CVMX_ADD_IO_SEG(0x0001180014020048ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_STAT6(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_RXX_STAT6(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014020050ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_RXX_STAT6(offset) (CVMX_ADD_IO_SEG(0x0001180014020050ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_STAT7(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_RXX_STAT7(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014020058ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_RXX_STAT7(offset) (CVMX_ADD_IO_SEG(0x0001180014020058ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_STAT8(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_RXX_STAT8(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014020060ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_RXX_STAT8(offset) (CVMX_ADD_IO_SEG(0x0001180014020060ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RXX_STAT9(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_RXX_STAT9(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014020068ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_RXX_STAT9(offset) (CVMX_ADD_IO_SEG(0x0001180014020068ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RX_LNEX_CFG(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 7))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 15)))))
+		cvmx_warn("CVMX_ILK_RX_LNEX_CFG(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014038000ull) + ((offset) & 15) * 1024;
+}
+#else
+#define CVMX_ILK_RX_LNEX_CFG(offset) (CVMX_ADD_IO_SEG(0x0001180014038000ull) + ((offset) & 15) * 1024)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RX_LNEX_INT(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 7))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 15)))))
+		cvmx_warn("CVMX_ILK_RX_LNEX_INT(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014038008ull) + ((offset) & 15) * 1024;
+}
+#else
+#define CVMX_ILK_RX_LNEX_INT(offset) (CVMX_ADD_IO_SEG(0x0001180014038008ull) + ((offset) & 15) * 1024)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RX_LNEX_INT_EN(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 7)))))
+		cvmx_warn("CVMX_ILK_RX_LNEX_INT_EN(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014038010ull) + ((offset) & 7) * 1024;
+}
+#else
+#define CVMX_ILK_RX_LNEX_INT_EN(offset) (CVMX_ADD_IO_SEG(0x0001180014038010ull) + ((offset) & 7) * 1024)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RX_LNEX_STAT0(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 7))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 15)))))
+		cvmx_warn("CVMX_ILK_RX_LNEX_STAT0(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014038018ull) + ((offset) & 15) * 1024;
+}
+#else
+#define CVMX_ILK_RX_LNEX_STAT0(offset) (CVMX_ADD_IO_SEG(0x0001180014038018ull) + ((offset) & 15) * 1024)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RX_LNEX_STAT1(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 7))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 15)))))
+		cvmx_warn("CVMX_ILK_RX_LNEX_STAT1(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014038020ull) + ((offset) & 15) * 1024;
+}
+#else
+#define CVMX_ILK_RX_LNEX_STAT1(offset) (CVMX_ADD_IO_SEG(0x0001180014038020ull) + ((offset) & 15) * 1024)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RX_LNEX_STAT10(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 15)))))
+		cvmx_warn("CVMX_ILK_RX_LNEX_STAT10(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014038068ull) + ((offset) & 15) * 1024;
+}
+#else
+#define CVMX_ILK_RX_LNEX_STAT10(offset) (CVMX_ADD_IO_SEG(0x0001180014038068ull) + ((offset) & 15) * 1024)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RX_LNEX_STAT2(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 7))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 15)))))
+		cvmx_warn("CVMX_ILK_RX_LNEX_STAT2(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014038028ull) + ((offset) & 15) * 1024;
+}
+#else
+#define CVMX_ILK_RX_LNEX_STAT2(offset) (CVMX_ADD_IO_SEG(0x0001180014038028ull) + ((offset) & 15) * 1024)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RX_LNEX_STAT3(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 7))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 15)))))
+		cvmx_warn("CVMX_ILK_RX_LNEX_STAT3(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014038030ull) + ((offset) & 15) * 1024;
+}
+#else
+#define CVMX_ILK_RX_LNEX_STAT3(offset) (CVMX_ADD_IO_SEG(0x0001180014038030ull) + ((offset) & 15) * 1024)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RX_LNEX_STAT4(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 7))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 15)))))
+		cvmx_warn("CVMX_ILK_RX_LNEX_STAT4(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014038038ull) + ((offset) & 15) * 1024;
+}
+#else
+#define CVMX_ILK_RX_LNEX_STAT4(offset) (CVMX_ADD_IO_SEG(0x0001180014038038ull) + ((offset) & 15) * 1024)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RX_LNEX_STAT5(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 7))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 15)))))
+		cvmx_warn("CVMX_ILK_RX_LNEX_STAT5(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014038040ull) + ((offset) & 15) * 1024;
+}
+#else
+#define CVMX_ILK_RX_LNEX_STAT5(offset) (CVMX_ADD_IO_SEG(0x0001180014038040ull) + ((offset) & 15) * 1024)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RX_LNEX_STAT6(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 7))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 15)))))
+		cvmx_warn("CVMX_ILK_RX_LNEX_STAT6(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014038048ull) + ((offset) & 15) * 1024;
+}
+#else
+#define CVMX_ILK_RX_LNEX_STAT6(offset) (CVMX_ADD_IO_SEG(0x0001180014038048ull) + ((offset) & 15) * 1024)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RX_LNEX_STAT7(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 7))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 15)))))
+		cvmx_warn("CVMX_ILK_RX_LNEX_STAT7(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014038050ull) + ((offset) & 15) * 1024;
+}
+#else
+#define CVMX_ILK_RX_LNEX_STAT7(offset) (CVMX_ADD_IO_SEG(0x0001180014038050ull) + ((offset) & 15) * 1024)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RX_LNEX_STAT8(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 7))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 15)))))
+		cvmx_warn("CVMX_ILK_RX_LNEX_STAT8(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014038058ull) + ((offset) & 15) * 1024;
+}
+#else
+#define CVMX_ILK_RX_LNEX_STAT8(offset) (CVMX_ADD_IO_SEG(0x0001180014038058ull) + ((offset) & 15) * 1024)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_RX_LNEX_STAT9(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 7))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 15)))))
+		cvmx_warn("CVMX_ILK_RX_LNEX_STAT9(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014038060ull) + ((offset) & 15) * 1024;
+}
+#else
+#define CVMX_ILK_RX_LNEX_STAT9(offset) (CVMX_ADD_IO_SEG(0x0001180014038060ull) + ((offset) & 15) * 1024)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_ILK_SER_CFG CVMX_ILK_SER_CFG_FUNC()
+static inline uint64_t CVMX_ILK_SER_CFG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_ILK_SER_CFG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180014000018ull);
+}
+#else
+#define CVMX_ILK_SER_CFG (CVMX_ADD_IO_SEG(0x0001180014000018ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_TXX_BYTE_CNTX(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 255)) && ((block_id <= 1))))))
+		cvmx_warn("CVMX_ILK_TXX_BYTE_CNTX(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180014013000ull) + (((offset) & 255) + ((block_id) & 1) * 0x800ull) * 8;
+}
+#else
+#define CVMX_ILK_TXX_BYTE_CNTX(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180014013000ull) + (((offset) & 255) + ((block_id) & 1) * 0x800ull) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_TXX_CAL_ENTRYX(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 287)) && ((block_id <= 1))))))
+		cvmx_warn("CVMX_ILK_TXX_CAL_ENTRYX(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180014011000ull) + (((offset) & 511) + ((block_id) & 1) * 0x800ull) * 8;
+}
+#else
+#define CVMX_ILK_TXX_CAL_ENTRYX(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180014011000ull) + (((offset) & 511) + ((block_id) & 1) * 0x800ull) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_TXX_CFG0(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_TXX_CFG0(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014010000ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_TXX_CFG0(offset) (CVMX_ADD_IO_SEG(0x0001180014010000ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_TXX_CFG1(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_TXX_CFG1(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014010008ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_TXX_CFG1(offset) (CVMX_ADD_IO_SEG(0x0001180014010008ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_TXX_CHA_XONX(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 3)) && ((block_id <= 1))))))
+		cvmx_warn("CVMX_ILK_TXX_CHA_XONX(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180014010400ull) + (((offset) & 3) + ((block_id) & 1) * 0x800ull) * 8;
+}
+#else
+#define CVMX_ILK_TXX_CHA_XONX(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180014010400ull) + (((offset) & 3) + ((block_id) & 1) * 0x800ull) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_TXX_DBG(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_TXX_DBG(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014010070ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_TXX_DBG(offset) (CVMX_ADD_IO_SEG(0x0001180014010070ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_TXX_ERR_CFG(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_TXX_ERR_CFG(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00011800140100B0ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_TXX_ERR_CFG(offset) (CVMX_ADD_IO_SEG(0x00011800140100B0ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_TXX_FLOW_CTL0(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_TXX_FLOW_CTL0(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014010048ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_TXX_FLOW_CTL0(offset) (CVMX_ADD_IO_SEG(0x0001180014010048ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_TXX_FLOW_CTL1(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_TXX_FLOW_CTL1(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014010050ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_TXX_FLOW_CTL1(offset) (CVMX_ADD_IO_SEG(0x0001180014010050ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_TXX_IDX_CAL(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_TXX_IDX_CAL(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014010058ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_TXX_IDX_CAL(offset) (CVMX_ADD_IO_SEG(0x0001180014010058ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_TXX_IDX_PMAP(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_TXX_IDX_PMAP(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014010010ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_TXX_IDX_PMAP(offset) (CVMX_ADD_IO_SEG(0x0001180014010010ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_TXX_IDX_STAT0(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_TXX_IDX_STAT0(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014010020ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_TXX_IDX_STAT0(offset) (CVMX_ADD_IO_SEG(0x0001180014010020ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_TXX_IDX_STAT1(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_TXX_IDX_STAT1(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014010028ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_TXX_IDX_STAT1(offset) (CVMX_ADD_IO_SEG(0x0001180014010028ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_TXX_INT(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_TXX_INT(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014010078ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_TXX_INT(offset) (CVMX_ADD_IO_SEG(0x0001180014010078ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_TXX_INT_EN(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_TXX_INT_EN(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014010080ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_TXX_INT_EN(offset) (CVMX_ADD_IO_SEG(0x0001180014010080ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_TXX_MEM_CAL0(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_TXX_MEM_CAL0(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014010060ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_TXX_MEM_CAL0(offset) (CVMX_ADD_IO_SEG(0x0001180014010060ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_TXX_MEM_CAL1(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_TXX_MEM_CAL1(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014010068ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_TXX_MEM_CAL1(offset) (CVMX_ADD_IO_SEG(0x0001180014010068ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_TXX_MEM_PMAP(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_TXX_MEM_PMAP(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014010018ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_TXX_MEM_PMAP(offset) (CVMX_ADD_IO_SEG(0x0001180014010018ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_TXX_MEM_STAT0(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_TXX_MEM_STAT0(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014010030ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_TXX_MEM_STAT0(offset) (CVMX_ADD_IO_SEG(0x0001180014010030ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_TXX_MEM_STAT1(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_TXX_MEM_STAT1(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014010038ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_TXX_MEM_STAT1(offset) (CVMX_ADD_IO_SEG(0x0001180014010038ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_TXX_PIPE(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_TXX_PIPE(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014010088ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_TXX_PIPE(offset) (CVMX_ADD_IO_SEG(0x0001180014010088ull) + ((offset) & 1) * 16384)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_TXX_PKT_CNTX(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 255)) && ((block_id <= 1))))))
+		cvmx_warn("CVMX_ILK_TXX_PKT_CNTX(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180014012000ull) + (((offset) & 255) + ((block_id) & 1) * 0x800ull) * 8;
+}
+#else
+#define CVMX_ILK_TXX_PKT_CNTX(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180014012000ull) + (((offset) & 255) + ((block_id) & 1) * 0x800ull) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_ILK_TXX_RMATCH(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_ILK_TXX_RMATCH(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180014010040ull) + ((offset) & 1) * 16384;
+}
+#else
+#define CVMX_ILK_TXX_RMATCH(offset) (CVMX_ADD_IO_SEG(0x0001180014010040ull) + ((offset) & 1) * 16384)
+#endif
+
+/**
+ * cvmx_ilk_bist_sum
+ */
+union cvmx_ilk_bist_sum {
+	uint64_t u64;
+	struct cvmx_ilk_bist_sum_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t rxf_x2p                      : 1;  /**< BIST result of the X2P memory 0 (rxf.x2p_fif_mem). */
+	uint64_t rxf_mem19                    : 1;  /**< BIST result of the RX FIFO bank3 memory 4 (rxf.rx_fif_bnk3_mem4). */
+	uint64_t rxf_mem18                    : 1;  /**< BIST result of the RX FIFO bank3 memory 3 (rxf.rx_fif_bnk3_mem3. */
+	uint64_t rxf_mem17                    : 1;  /**< BIST result of the RX FIFO bank3 memory 2 (rxf.rx_fif_bnk3_mem2). */
+	uint64_t rxf_mem16                    : 1;  /**< BIST result of the RX FIFO bank3 memory 1 (rxf.rx_fif_bnk3_mem1). */
+	uint64_t rxf_mem15                    : 1;  /**< BIST result of the RX FIFO bank3 memory 0 (rxf.rx_fif_bnk3_mem0). */
+	uint64_t reserved_52_57               : 6;
+	uint64_t rxf_mem8                     : 1;  /**< BIST result of the RX FIFO bank1 memory 3 (rxf.rx_fif_bnk1_mem3). */
+	uint64_t rxf_mem7                     : 1;  /**< BIST result of the RX FIFO bank1 memory 2 (rxf.rx_fif_bnk1_mem2). */
+	uint64_t rxf_mem6                     : 1;  /**< BIST result of the RX FIFO bank1 memory 1 (rxf.rx_fif_bnk1_mem1). */
+	uint64_t rxf_mem5                     : 1;  /**< BIST result of the RX FIFO bank1 memory 0 (rxf.rx_fif_bnk1_mem0). */
+	uint64_t rxf_mem4                     : 1;  /**< BIST result of the RX FIFO bank0 memory 4 (rxf.rx_fif_bnk0_mem4). */
+	uint64_t rxf_mem3                     : 1;  /**< BIST result of the RX FIFO bank0 memory 3 (rxf.rx_fif_bnk0_mem3. */
+	uint64_t reserved_36_45               : 10;
+	uint64_t rle7_dsk1                    : 1;  /**< Reserved. */
+	uint64_t rle7_dsk0                    : 1;  /**< Reserved. */
+	uint64_t rle6_dsk1                    : 1;  /**< Reserved. */
+	uint64_t rle6_dsk0                    : 1;  /**< Reserved. */
+	uint64_t rle5_dsk1                    : 1;  /**< Reserved. */
+	uint64_t rle5_dsk0                    : 1;  /**< Reserved. */
+	uint64_t rle4_dsk1                    : 1;  /**< Reserved. */
+	uint64_t rle4_dsk0                    : 1;  /**< Reserved. */
+	uint64_t rle3_dsk1                    : 1;  /**< Reserved. */
+	uint64_t rle3_dsk0                    : 1;  /**< Reserved. */
+	uint64_t rle2_dsk1                    : 1;  /**< Reserved. */
+	uint64_t rle2_dsk0                    : 1;  /**< Reserved. */
+	uint64_t rle1_dsk1                    : 1;  /**< Reserved. */
+	uint64_t rle1_dsk0                    : 1;  /**< Reserved. */
+	uint64_t rle0_dsk1                    : 1;  /**< Reserved. */
+	uint64_t rle0_dsk0                    : 1;  /**< Reserved. */
+	uint64_t rlk1_pmap                    : 1;  /**< BIST result of the RX link1 port-kind map (rlk1.pmap.pmap_mem_fif). */
+	uint64_t reserved_18_18               : 1;
+	uint64_t rlk1_fwc                     : 1;  /**< BIST result of the RX link1 calendar table memory (rlk1.fwc.cal_mem). */
+	uint64_t reserved_16_16               : 1;
+	uint64_t rlk0_pmap                    : 1;  /**< BIST result of the RX link0 port-kind map (rlk0.pmap.pmap_mem_fif). */
+	uint64_t rlk0_stat1                   : 1;  /**< BIST result of the RX link0 byte count memory (rlk0.csr.byte_cnt_mem). */
+	uint64_t rlk0_fwc                     : 1;  /**< BIST result of the RX link0 calendar table memory (rlk0.fwc.cal_mem). */
+	uint64_t rlk0_stat                    : 1;  /**< BIST result of the RX link0 packet count memory (rlk0.csr.pkt_cnt_mem). */
+	uint64_t tlk1_stat1                   : 1;  /**< BIST result of the TX link1 byte count memory (tlk1.csr.byte_cnt_mem). */
+	uint64_t tlk1_fwc                     : 1;  /**< BIST result of the TX link1 calendar table memory (tlk1.fwc.cal_mem). */
+	uint64_t reserved_9_9                 : 1;
+	uint64_t tlk1_txf2                    : 1;  /**< Reserved. */
+	uint64_t tlk1_txf1                    : 1;  /**< Reserved. */
+	uint64_t tlk1_txf0                    : 1;  /**< BIST result of the TX link1 FIFO memory (tlk1.txf.txf_mem_fif). */
+	uint64_t tlk0_stat1                   : 1;  /**< BIST result of the TX link0 byte count memory (tlk0.csr.byte_cnt_mem). */
+	uint64_t tlk0_fwc                     : 1;  /**< BIST result of the TX link0 calendar table memory (tlk0.fwc.cal_mem). */
+	uint64_t reserved_3_3                 : 1;
+	uint64_t tlk0_txf2                    : 1;  /**< Reserved. */
+	uint64_t tlk0_txf1                    : 1;  /**< Reserved. */
+	uint64_t tlk0_txf0                    : 1;  /**< BIST result of the TX link0 FIFO memory (tlk0.txf.txf_mem_fif). */
+#else
+	uint64_t tlk0_txf0                    : 1;
+	uint64_t tlk0_txf1                    : 1;
+	uint64_t tlk0_txf2                    : 1;
+	uint64_t reserved_3_3                 : 1;
+	uint64_t tlk0_fwc                     : 1;
+	uint64_t tlk0_stat1                   : 1;
+	uint64_t tlk1_txf0                    : 1;
+	uint64_t tlk1_txf1                    : 1;
+	uint64_t tlk1_txf2                    : 1;
+	uint64_t reserved_9_9                 : 1;
+	uint64_t tlk1_fwc                     : 1;
+	uint64_t tlk1_stat1                   : 1;
+	uint64_t rlk0_stat                    : 1;
+	uint64_t rlk0_fwc                     : 1;
+	uint64_t rlk0_stat1                   : 1;
+	uint64_t rlk0_pmap                    : 1;
+	uint64_t reserved_16_16               : 1;
+	uint64_t rlk1_fwc                     : 1;
+	uint64_t reserved_18_18               : 1;
+	uint64_t rlk1_pmap                    : 1;
+	uint64_t rle0_dsk0                    : 1;
+	uint64_t rle0_dsk1                    : 1;
+	uint64_t rle1_dsk0                    : 1;
+	uint64_t rle1_dsk1                    : 1;
+	uint64_t rle2_dsk0                    : 1;
+	uint64_t rle2_dsk1                    : 1;
+	uint64_t rle3_dsk0                    : 1;
+	uint64_t rle3_dsk1                    : 1;
+	uint64_t rle4_dsk0                    : 1;
+	uint64_t rle4_dsk1                    : 1;
+	uint64_t rle5_dsk0                    : 1;
+	uint64_t rle5_dsk1                    : 1;
+	uint64_t rle6_dsk0                    : 1;
+	uint64_t rle6_dsk1                    : 1;
+	uint64_t rle7_dsk0                    : 1;
+	uint64_t rle7_dsk1                    : 1;
+	uint64_t reserved_36_45               : 10;
+	uint64_t rxf_mem3                     : 1;
+	uint64_t rxf_mem4                     : 1;
+	uint64_t rxf_mem5                     : 1;
+	uint64_t rxf_mem6                     : 1;
+	uint64_t rxf_mem7                     : 1;
+	uint64_t rxf_mem8                     : 1;
+	uint64_t reserved_52_57               : 6;
+	uint64_t rxf_mem15                    : 1;
+	uint64_t rxf_mem16                    : 1;
+	uint64_t rxf_mem17                    : 1;
+	uint64_t rxf_mem18                    : 1;
+	uint64_t rxf_mem19                    : 1;
+	uint64_t rxf_x2p                      : 1;
+#endif
+	} s;
+	struct cvmx_ilk_bist_sum_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_58_63               : 6;
+	uint64_t rxf_x2p1                     : 1;  /**< Bist status of rxf.x2p_fif_mem1 */
+	uint64_t rxf_x2p0                     : 1;  /**< Bist status of rxf.x2p_fif_mem0 */
+	uint64_t rxf_pmap                     : 1;  /**< Bist status of rxf.rx_map_mem */
+	uint64_t rxf_mem2                     : 1;  /**< Bist status of rxf.rx_fif_mem2 */
+	uint64_t rxf_mem1                     : 1;  /**< Bist status of rxf.rx_fif_mem1 */
+	uint64_t rxf_mem0                     : 1;  /**< Bist status of rxf.rx_fif_mem0 */
+	uint64_t reserved_36_51               : 16;
+	uint64_t rle7_dsk1                    : 1;  /**< Bist status of lne.rle7.dsk.dsk_fif_mem1 */
+	uint64_t rle7_dsk0                    : 1;  /**< Bist status of lne.rle7.dsk.dsk_fif_mem0 */
+	uint64_t rle6_dsk1                    : 1;  /**< Bist status of lne.rle6.dsk.dsk_fif_mem1 */
+	uint64_t rle6_dsk0                    : 1;  /**< Bist status of lne.rle6.dsk.dsk_fif_mem0 */
+	uint64_t rle5_dsk1                    : 1;  /**< Bist status of lne.rle5.dsk.dsk_fif_mem1 */
+	uint64_t rle5_dsk0                    : 1;  /**< Bist status of lne.rle5.dsk.dsk_fif_mem0 */
+	uint64_t rle4_dsk1                    : 1;  /**< Bist status of lne.rle4.dsk.dsk_fif_mem1 */
+	uint64_t rle4_dsk0                    : 1;  /**< Bist status of lne.rle4.dsk.dsk_fif_mem0 */
+	uint64_t rle3_dsk1                    : 1;  /**< Bist status of lne.rle3.dsk.dsk_fif_mem1 */
+	uint64_t rle3_dsk0                    : 1;  /**< Bist status of lne.rle3.dsk.dsk_fif_mem0 */
+	uint64_t rle2_dsk1                    : 1;  /**< Bist status of lne.rle2.dsk.dsk_fif_mem1 */
+	uint64_t rle2_dsk0                    : 1;  /**< Bist status of lne.rle2.dsk.dsk_fif_mem0 */
+	uint64_t rle1_dsk1                    : 1;  /**< Bist status of lne.rle1.dsk.dsk_fif_mem1 */
+	uint64_t rle1_dsk0                    : 1;  /**< Bist status of lne.rle1.dsk.dsk_fif_mem0 */
+	uint64_t rle0_dsk1                    : 1;  /**< Bist status of lne.rle0.dsk.dsk_fif_mem1 */
+	uint64_t rle0_dsk0                    : 1;  /**< Bist status of lne.rle0.dsk.dsk_fif_mem0 */
+	uint64_t reserved_19_19               : 1;
+	uint64_t rlk1_stat1                   : 1;  /**< Bist status of rlk1.csr.stat_mem1    ***NOTE: Added in pass 2.0 */
+	uint64_t rlk1_fwc                     : 1;  /**< Bist status of rlk1.fwc.cal_chan_ram */
+	uint64_t rlk1_stat                    : 1;  /**< Bist status of rlk1.csr.stat_mem0 */
+	uint64_t reserved_15_15               : 1;
+	uint64_t rlk0_stat1                   : 1;  /**< Bist status of rlk0.csr.stat_mem1    ***NOTE: Added in pass 2.0 */
+	uint64_t rlk0_fwc                     : 1;  /**< Bist status of rlk0.fwc.cal_chan_ram */
+	uint64_t rlk0_stat                    : 1;  /**< Bist status of rlk0.csr.stat_mem0 */
+	uint64_t tlk1_stat1                   : 1;  /**< Bist status of tlk1.csr.stat_mem1 */
+	uint64_t tlk1_fwc                     : 1;  /**< Bist status of tlk1.fwc.cal_chan_ram */
+	uint64_t tlk1_stat0                   : 1;  /**< Bist status of tlk1.csr.stat_mem0 */
+	uint64_t tlk1_txf2                    : 1;  /**< Bist status of tlk1.txf.tx_map_mem */
+	uint64_t tlk1_txf1                    : 1;  /**< Bist status of tlk1.txf.tx_fif_mem1 */
+	uint64_t tlk1_txf0                    : 1;  /**< Bist status of tlk1.txf.tx_fif_mem0 */
+	uint64_t tlk0_stat1                   : 1;  /**< Bist status of tlk0.csr.stat_mem1 */
+	uint64_t tlk0_fwc                     : 1;  /**< Bist status of tlk0.fwc.cal_chan_ram */
+	uint64_t tlk0_stat0                   : 1;  /**< Bist status of tlk0.csr.stat_mem0 */
+	uint64_t tlk0_txf2                    : 1;  /**< Bist status of tlk0.txf.tx_map_mem */
+	uint64_t tlk0_txf1                    : 1;  /**< Bist status of tlk0.txf.tx_fif_mem1 */
+	uint64_t tlk0_txf0                    : 1;  /**< Bist status of tlk0.txf.tx_fif_mem0 */
+#else
+	uint64_t tlk0_txf0                    : 1;
+	uint64_t tlk0_txf1                    : 1;
+	uint64_t tlk0_txf2                    : 1;
+	uint64_t tlk0_stat0                   : 1;
+	uint64_t tlk0_fwc                     : 1;
+	uint64_t tlk0_stat1                   : 1;
+	uint64_t tlk1_txf0                    : 1;
+	uint64_t tlk1_txf1                    : 1;
+	uint64_t tlk1_txf2                    : 1;
+	uint64_t tlk1_stat0                   : 1;
+	uint64_t tlk1_fwc                     : 1;
+	uint64_t tlk1_stat1                   : 1;
+	uint64_t rlk0_stat                    : 1;
+	uint64_t rlk0_fwc                     : 1;
+	uint64_t rlk0_stat1                   : 1;
+	uint64_t reserved_15_15               : 1;
+	uint64_t rlk1_stat                    : 1;
+	uint64_t rlk1_fwc                     : 1;
+	uint64_t rlk1_stat1                   : 1;
+	uint64_t reserved_19_19               : 1;
+	uint64_t rle0_dsk0                    : 1;
+	uint64_t rle0_dsk1                    : 1;
+	uint64_t rle1_dsk0                    : 1;
+	uint64_t rle1_dsk1                    : 1;
+	uint64_t rle2_dsk0                    : 1;
+	uint64_t rle2_dsk1                    : 1;
+	uint64_t rle3_dsk0                    : 1;
+	uint64_t rle3_dsk1                    : 1;
+	uint64_t rle4_dsk0                    : 1;
+	uint64_t rle4_dsk1                    : 1;
+	uint64_t rle5_dsk0                    : 1;
+	uint64_t rle5_dsk1                    : 1;
+	uint64_t rle6_dsk0                    : 1;
+	uint64_t rle6_dsk1                    : 1;
+	uint64_t rle7_dsk0                    : 1;
+	uint64_t rle7_dsk1                    : 1;
+	uint64_t reserved_36_51               : 16;
+	uint64_t rxf_mem0                     : 1;
+	uint64_t rxf_mem1                     : 1;
+	uint64_t rxf_mem2                     : 1;
+	uint64_t rxf_pmap                     : 1;
+	uint64_t rxf_x2p0                     : 1;
+	uint64_t rxf_x2p1                     : 1;
+	uint64_t reserved_58_63               : 6;
+#endif
+	} cn68xx;
+	struct cvmx_ilk_bist_sum_cn68xxp1 {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_58_63               : 6;
+	uint64_t rxf_x2p1                     : 1;  /**< Bist status of rxf.x2p_fif_mem1 */
+	uint64_t rxf_x2p0                     : 1;  /**< Bist status of rxf.x2p_fif_mem0 */
+	uint64_t rxf_pmap                     : 1;  /**< Bist status of rxf.rx_map_mem */
+	uint64_t rxf_mem2                     : 1;  /**< Bist status of rxf.rx_fif_mem2 */
+	uint64_t rxf_mem1                     : 1;  /**< Bist status of rxf.rx_fif_mem1 */
+	uint64_t rxf_mem0                     : 1;  /**< Bist status of rxf.rx_fif_mem0 */
+	uint64_t reserved_36_51               : 16;
+	uint64_t rle7_dsk1                    : 1;  /**< Bist status of lne.rle7.dsk.dsk_fif_mem1 */
+	uint64_t rle7_dsk0                    : 1;  /**< Bist status of lne.rle7.dsk.dsk_fif_mem0 */
+	uint64_t rle6_dsk1                    : 1;  /**< Bist status of lne.rle6.dsk.dsk_fif_mem1 */
+	uint64_t rle6_dsk0                    : 1;  /**< Bist status of lne.rle6.dsk.dsk_fif_mem0 */
+	uint64_t rle5_dsk1                    : 1;  /**< Bist status of lne.rle5.dsk.dsk_fif_mem1 */
+	uint64_t rle5_dsk0                    : 1;  /**< Bist status of lne.rle5.dsk.dsk_fif_mem0 */
+	uint64_t rle4_dsk1                    : 1;  /**< Bist status of lne.rle4.dsk.dsk_fif_mem1 */
+	uint64_t rle4_dsk0                    : 1;  /**< Bist status of lne.rle4.dsk.dsk_fif_mem0 */
+	uint64_t rle3_dsk1                    : 1;  /**< Bist status of lne.rle3.dsk.dsk_fif_mem1 */
+	uint64_t rle3_dsk0                    : 1;  /**< Bist status of lne.rle3.dsk.dsk_fif_mem0 */
+	uint64_t rle2_dsk1                    : 1;  /**< Bist status of lne.rle2.dsk.dsk_fif_mem1 */
+	uint64_t rle2_dsk0                    : 1;  /**< Bist status of lne.rle2.dsk.dsk_fif_mem0 */
+	uint64_t rle1_dsk1                    : 1;  /**< Bist status of lne.rle1.dsk.dsk_fif_mem1 */
+	uint64_t rle1_dsk0                    : 1;  /**< Bist status of lne.rle1.dsk.dsk_fif_mem0 */
+	uint64_t rle0_dsk1                    : 1;  /**< Bist status of lne.rle0.dsk.dsk_fif_mem1 */
+	uint64_t rle0_dsk0                    : 1;  /**< Bist status of lne.rle0.dsk.dsk_fif_mem0 */
+	uint64_t reserved_18_19               : 2;
+	uint64_t rlk1_fwc                     : 1;  /**< Bist status of rlk1.fwc.cal_chan_ram */
+	uint64_t rlk1_stat                    : 1;  /**< Bist status of rlk1.csr.stat_mem */
+	uint64_t reserved_14_15               : 2;
+	uint64_t rlk0_fwc                     : 1;  /**< Bist status of rlk0.fwc.cal_chan_ram */
+	uint64_t rlk0_stat                    : 1;  /**< Bist status of rlk0.csr.stat_mem */
+	uint64_t reserved_11_11               : 1;
+	uint64_t tlk1_fwc                     : 1;  /**< Bist status of tlk1.fwc.cal_chan_ram */
+	uint64_t tlk1_stat                    : 1;  /**< Bist status of tlk1.csr.stat_mem */
+	uint64_t tlk1_txf2                    : 1;  /**< Bist status of tlk1.txf.tx_map_mem */
+	uint64_t tlk1_txf1                    : 1;  /**< Bist status of tlk1.txf.tx_fif_mem1 */
+	uint64_t tlk1_txf0                    : 1;  /**< Bist status of tlk1.txf.tx_fif_mem0 */
+	uint64_t reserved_5_5                 : 1;
+	uint64_t tlk0_fwc                     : 1;  /**< Bist status of tlk0.fwc.cal_chan_ram */
+	uint64_t tlk0_stat                    : 1;  /**< Bist status of tlk0.csr.stat_mem */
+	uint64_t tlk0_txf2                    : 1;  /**< Bist status of tlk0.txf.tx_map_mem */
+	uint64_t tlk0_txf1                    : 1;  /**< Bist status of tlk0.txf.tx_fif_mem1 */
+	uint64_t tlk0_txf0                    : 1;  /**< Bist status of tlk0.txf.tx_fif_mem0 */
+#else
+	uint64_t tlk0_txf0                    : 1;
+	uint64_t tlk0_txf1                    : 1;
+	uint64_t tlk0_txf2                    : 1;
+	uint64_t tlk0_stat                    : 1;
+	uint64_t tlk0_fwc                     : 1;
+	uint64_t reserved_5_5                 : 1;
+	uint64_t tlk1_txf0                    : 1;
+	uint64_t tlk1_txf1                    : 1;
+	uint64_t tlk1_txf2                    : 1;
+	uint64_t tlk1_stat                    : 1;
+	uint64_t tlk1_fwc                     : 1;
+	uint64_t reserved_11_11               : 1;
+	uint64_t rlk0_stat                    : 1;
+	uint64_t rlk0_fwc                     : 1;
+	uint64_t reserved_14_15               : 2;
+	uint64_t rlk1_stat                    : 1;
+	uint64_t rlk1_fwc                     : 1;
+	uint64_t reserved_18_19               : 2;
+	uint64_t rle0_dsk0                    : 1;
+	uint64_t rle0_dsk1                    : 1;
+	uint64_t rle1_dsk0                    : 1;
+	uint64_t rle1_dsk1                    : 1;
+	uint64_t rle2_dsk0                    : 1;
+	uint64_t rle2_dsk1                    : 1;
+	uint64_t rle3_dsk0                    : 1;
+	uint64_t rle3_dsk1                    : 1;
+	uint64_t rle4_dsk0                    : 1;
+	uint64_t rle4_dsk1                    : 1;
+	uint64_t rle5_dsk0                    : 1;
+	uint64_t rle5_dsk1                    : 1;
+	uint64_t rle6_dsk0                    : 1;
+	uint64_t rle6_dsk1                    : 1;
+	uint64_t rle7_dsk0                    : 1;
+	uint64_t rle7_dsk1                    : 1;
+	uint64_t reserved_36_51               : 16;
+	uint64_t rxf_mem0                     : 1;
+	uint64_t rxf_mem1                     : 1;
+	uint64_t rxf_mem2                     : 1;
+	uint64_t rxf_pmap                     : 1;
+	uint64_t rxf_x2p0                     : 1;
+	uint64_t rxf_x2p1                     : 1;
+	uint64_t reserved_58_63               : 6;
+#endif
+	} cn68xxp1;
+	struct cvmx_ilk_bist_sum_cn78xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t rxf_x2p                      : 1;  /**< BIST result of the X2P memory 0 (rxf.x2p_fif_mem). */
+	uint64_t rxf_mem19                    : 1;  /**< BIST result of the RX FIFO bank3 memory 4 (rxf.rx_fif_bnk3_mem4). */
+	uint64_t rxf_mem18                    : 1;  /**< BIST result of the RX FIFO bank3 memory 3 (rxf.rx_fif_bnk3_mem3. */
+	uint64_t rxf_mem17                    : 1;  /**< BIST result of the RX FIFO bank3 memory 2 (rxf.rx_fif_bnk3_mem2). */
+	uint64_t rxf_mem16                    : 1;  /**< BIST result of the RX FIFO bank3 memory 1 (rxf.rx_fif_bnk3_mem1). */
+	uint64_t rxf_mem15                    : 1;  /**< BIST result of the RX FIFO bank3 memory 0 (rxf.rx_fif_bnk3_mem0). */
+	uint64_t rxf_mem14                    : 1;  /**< BIST result of the RX FIFO bank2 memory 4 (rxf.rx_fif_bnk2_mem4). */
+	uint64_t rxf_mem13                    : 1;  /**< BIST result of the RX FIFO bank2 memory 3 (rxf.rx_fif_bnk2_mem3). */
+	uint64_t rxf_mem12                    : 1;  /**< BIST result of the RX FIFO bank2 memory 2 (rxf.rx_fif_bnk2_mem2). */
+	uint64_t rxf_mem11                    : 1;  /**< BIST result of the RX FIFO bank2 memory 1 (rxf.rx_fif_bnk2_mem1). */
+	uint64_t rxf_mem10                    : 1;  /**< BIST result of the RX FIFO bank2 memory 0 (rxf.rx_fif_bnk2_mem0). */
+	uint64_t rxf_mem9                     : 1;  /**< BIST result of the RX FIFO bank1 memory 4 (rxf.rx_fif_bnk1_mem4). */
+	uint64_t rxf_mem8                     : 1;  /**< BIST result of the RX FIFO bank1 memory 3 (rxf.rx_fif_bnk1_mem3). */
+	uint64_t rxf_mem7                     : 1;  /**< BIST result of the RX FIFO bank1 memory 2 (rxf.rx_fif_bnk1_mem2). */
+	uint64_t rxf_mem6                     : 1;  /**< BIST result of the RX FIFO bank1 memory 1 (rxf.rx_fif_bnk1_mem1). */
+	uint64_t rxf_mem5                     : 1;  /**< BIST result of the RX FIFO bank1 memory 0 (rxf.rx_fif_bnk1_mem0). */
+	uint64_t rxf_mem4                     : 1;  /**< BIST result of the RX FIFO bank0 memory 4 (rxf.rx_fif_bnk0_mem4). */
+	uint64_t rxf_mem3                     : 1;  /**< BIST result of the RX FIFO bank0 memory 3 (rxf.rx_fif_bnk0_mem3. */
+	uint64_t rxf_mem2                     : 1;  /**< BIST result of the RX FIFO bank0 memory 2 (rxf.rx_fif_bnk0_mem2). */
+	uint64_t rxf_mem1                     : 1;  /**< BIST result of the RX FIFO bank0 memory 1 (rxf.rx_fif_bnk0_mem1). */
+	uint64_t rxf_mem0                     : 1;  /**< BIST result of the RX FIFO bank0 memory 0 (rxf.rx_fif_bnk0_mem0). */
+	uint64_t reserved_36_42               : 7;
+	uint64_t rle7_dsk1                    : 1;  /**< Reserved. */
+	uint64_t rle7_dsk0                    : 1;  /**< Reserved. */
+	uint64_t rle6_dsk1                    : 1;  /**< Reserved. */
+	uint64_t rle6_dsk0                    : 1;  /**< Reserved. */
+	uint64_t rle5_dsk1                    : 1;  /**< Reserved. */
+	uint64_t rle5_dsk0                    : 1;  /**< Reserved. */
+	uint64_t rle4_dsk1                    : 1;  /**< Reserved. */
+	uint64_t rle4_dsk0                    : 1;  /**< Reserved. */
+	uint64_t rle3_dsk1                    : 1;  /**< Reserved. */
+	uint64_t rle3_dsk0                    : 1;  /**< Reserved. */
+	uint64_t rle2_dsk1                    : 1;  /**< Reserved. */
+	uint64_t rle2_dsk0                    : 1;  /**< Reserved. */
+	uint64_t rle1_dsk1                    : 1;  /**< Reserved. */
+	uint64_t rle1_dsk0                    : 1;  /**< Reserved. */
+	uint64_t rle0_dsk1                    : 1;  /**< Reserved. */
+	uint64_t rle0_dsk0                    : 1;  /**< Reserved. */
+	uint64_t rlk1_pmap                    : 1;  /**< BIST result of the RX link1 port-kind map (rlk1.pmap.pmap_mem_fif). */
+	uint64_t rlk1_stat                    : 1;  /**< BIST result of the RX link1 packet count memory (rlk1.csr.pkt_cnt_mem). */
+	uint64_t rlk1_fwc                     : 1;  /**< BIST result of the RX link1 calendar table memory (rlk1.fwc.cal_mem). */
+	uint64_t rlk1_stat1                   : 1;  /**< BIST result of the RX link1 byte count memory (rlk1.csr.byte_cnt_mem). */
+	uint64_t rlk0_pmap                    : 1;  /**< BIST result of the RX link0 port-kind map (rlk0.pmap.pmap_mem_fif). */
+	uint64_t rlk0_stat1                   : 1;  /**< BIST result of the RX link0 byte count memory (rlk0.csr.byte_cnt_mem). */
+	uint64_t rlk0_fwc                     : 1;  /**< BIST result of the RX link0 calendar table memory (rlk0.fwc.cal_mem). */
+	uint64_t rlk0_stat                    : 1;  /**< BIST result of the RX link0 packet count memory (rlk0.csr.pkt_cnt_mem). */
+	uint64_t tlk1_stat1                   : 1;  /**< BIST result of the TX link1 byte count memory (tlk1.csr.byte_cnt_mem). */
+	uint64_t tlk1_fwc                     : 1;  /**< BIST result of the TX link1 calendar table memory (tlk1.fwc.cal_mem). */
+	uint64_t tlk1_stat0                   : 1;  /**< BIST result of the TX link1 packet count memory (tlk1.csr.pkt_cnt_mem). */
+	uint64_t tlk1_txf2                    : 1;  /**< Reserved. */
+	uint64_t tlk1_txf1                    : 1;  /**< Reserved. */
+	uint64_t tlk1_txf0                    : 1;  /**< BIST result of the TX link1 FIFO memory (tlk1.txf.txf_mem_fif). */
+	uint64_t tlk0_stat1                   : 1;  /**< BIST result of the TX link0 byte count memory (tlk0.csr.byte_cnt_mem). */
+	uint64_t tlk0_fwc                     : 1;  /**< BIST result of the TX link0 calendar table memory (tlk0.fwc.cal_mem). */
+	uint64_t tlk0_stat0                   : 1;  /**< BIST result of the TX link0 packet count memory (tlk0.csr.pkt_cnt_mem). */
+	uint64_t tlk0_txf2                    : 1;  /**< Reserved. */
+	uint64_t tlk0_txf1                    : 1;  /**< Reserved. */
+	uint64_t tlk0_txf0                    : 1;  /**< BIST result of the TX link0 FIFO memory (tlk0.txf.txf_mem_fif). */
+#else
+	uint64_t tlk0_txf0                    : 1;
+	uint64_t tlk0_txf1                    : 1;
+	uint64_t tlk0_txf2                    : 1;
+	uint64_t tlk0_stat0                   : 1;
+	uint64_t tlk0_fwc                     : 1;
+	uint64_t tlk0_stat1                   : 1;
+	uint64_t tlk1_txf0                    : 1;
+	uint64_t tlk1_txf1                    : 1;
+	uint64_t tlk1_txf2                    : 1;
+	uint64_t tlk1_stat0                   : 1;
+	uint64_t tlk1_fwc                     : 1;
+	uint64_t tlk1_stat1                   : 1;
+	uint64_t rlk0_stat                    : 1;
+	uint64_t rlk0_fwc                     : 1;
+	uint64_t rlk0_stat1                   : 1;
+	uint64_t rlk0_pmap                    : 1;
+	uint64_t rlk1_stat1                   : 1;
+	uint64_t rlk1_fwc                     : 1;
+	uint64_t rlk1_stat                    : 1;
+	uint64_t rlk1_pmap                    : 1;
+	uint64_t rle0_dsk0                    : 1;
+	uint64_t rle0_dsk1                    : 1;
+	uint64_t rle1_dsk0                    : 1;
+	uint64_t rle1_dsk1                    : 1;
+	uint64_t rle2_dsk0                    : 1;
+	uint64_t rle2_dsk1                    : 1;
+	uint64_t rle3_dsk0                    : 1;
+	uint64_t rle3_dsk1                    : 1;
+	uint64_t rle4_dsk0                    : 1;
+	uint64_t rle4_dsk1                    : 1;
+	uint64_t rle5_dsk0                    : 1;
+	uint64_t rle5_dsk1                    : 1;
+	uint64_t rle6_dsk0                    : 1;
+	uint64_t rle6_dsk1                    : 1;
+	uint64_t rle7_dsk0                    : 1;
+	uint64_t rle7_dsk1                    : 1;
+	uint64_t reserved_36_42               : 7;
+	uint64_t rxf_mem0                     : 1;
+	uint64_t rxf_mem1                     : 1;
+	uint64_t rxf_mem2                     : 1;
+	uint64_t rxf_mem3                     : 1;
+	uint64_t rxf_mem4                     : 1;
+	uint64_t rxf_mem5                     : 1;
+	uint64_t rxf_mem6                     : 1;
+	uint64_t rxf_mem7                     : 1;
+	uint64_t rxf_mem8                     : 1;
+	uint64_t rxf_mem9                     : 1;
+	uint64_t rxf_mem10                    : 1;
+	uint64_t rxf_mem11                    : 1;
+	uint64_t rxf_mem12                    : 1;
+	uint64_t rxf_mem13                    : 1;
+	uint64_t rxf_mem14                    : 1;
+	uint64_t rxf_mem15                    : 1;
+	uint64_t rxf_mem16                    : 1;
+	uint64_t rxf_mem17                    : 1;
+	uint64_t rxf_mem18                    : 1;
+	uint64_t rxf_mem19                    : 1;
+	uint64_t rxf_x2p                      : 1;
+#endif
+	} cn78xx;
+};
+typedef union cvmx_ilk_bist_sum cvmx_ilk_bist_sum_t;
+
+/**
+ * cvmx_ilk_gbl_cfg
+ */
+union cvmx_ilk_gbl_cfg {
+	uint64_t u64;
+	struct cvmx_ilk_gbl_cfg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t rid_rstdis                   : 1;  /**< Disable automatic reassembly-ID error recovery. For diagnostic use only. */
+	uint64_t reset                        : 1;  /**< Reset ILK. For diagnostic use only. */
+	uint64_t cclk_dis                     : 1;  /**< Disable ILK conditional clocking. For diagnostic use only. */
+	uint64_t rxf_xlink                    : 1;  /**< Causes external loopback traffic to switch links. Enabling this allow simultaneous use of
+                                                         external and internal loopback. */
+#else
+	uint64_t rxf_xlink                    : 1;
+	uint64_t cclk_dis                     : 1;
+	uint64_t reset                        : 1;
+	uint64_t rid_rstdis                   : 1;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_ilk_gbl_cfg_s             cn68xx;
+	struct cvmx_ilk_gbl_cfg_cn68xxp1 {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_2_63                : 62;
+	uint64_t cclk_dis                     : 1;  /**< Disable ILK conditional clocking.   For diagnostic use only. */
+	uint64_t rxf_xlink                    : 1;  /**< Causes external loopback traffic to switch links.  Enabling
+                                                         this allow simultaneous use of external and internal loopback. */
+#else
+	uint64_t rxf_xlink                    : 1;
+	uint64_t cclk_dis                     : 1;
+	uint64_t reserved_2_63                : 62;
+#endif
+	} cn68xxp1;
+	struct cvmx_ilk_gbl_cfg_s             cn78xx;
+};
+typedef union cvmx_ilk_gbl_cfg cvmx_ilk_gbl_cfg_t;
+
+/**
+ * cvmx_ilk_gbl_err_cfg
+ */
+union cvmx_ilk_gbl_err_cfg {
+	uint64_t u64;
+	struct cvmx_ilk_gbl_err_cfg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_20_63               : 44;
+	uint64_t rxf_flip                     : 2;  /**< Testing feature. Flip syndrome bits <1:0> on writes to the RXF RAM to test single-bit or
+                                                         double-bit errors. */
+	uint64_t x2p_flip                     : 2;  /**< Testing feature. Flip syndrome bits <1:0> on writes to the X2P RAM to test single-bit or
+                                                         double-bit errors. */
+	uint64_t reserved_2_15                : 14;
+	uint64_t rxf_cor_dis                  : 1;  /**< Disable ECC corrector on RXF. */
+	uint64_t x2p_cor_dis                  : 1;  /**< Disable ECC corrector on X2P. */
+#else
+	uint64_t x2p_cor_dis                  : 1;
+	uint64_t rxf_cor_dis                  : 1;
+	uint64_t reserved_2_15                : 14;
+	uint64_t x2p_flip                     : 2;
+	uint64_t rxf_flip                     : 2;
+	uint64_t reserved_20_63               : 44;
+#endif
+	} s;
+	struct cvmx_ilk_gbl_err_cfg_s         cn78xx;
+};
+typedef union cvmx_ilk_gbl_err_cfg cvmx_ilk_gbl_err_cfg_t;
+
+/**
+ * cvmx_ilk_gbl_int
+ */
+union cvmx_ilk_gbl_int {
+	uint64_t u64;
+	struct cvmx_ilk_gbl_int_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t x2p_dbe                      : 1;  /**< X2P double-bit error. Throws ILK_INTSN_E::ILK_GBL_X2P_DBE. */
+	uint64_t x2p_sbe                      : 1;  /**< X2P single-bit error. Throws ILK_INTSN_E::ILK_GBL_X2P_SBE. */
+	uint64_t rxf_dbe                      : 1;  /**< RXF double-bit error. Throws ILK_INTSN_E::ILK_GBL_RXF_DBE. */
+	uint64_t rxf_sbe                      : 1;  /**< RXF single-bit error. Throws ILK_INTSN_E::ILK_GBL_RXF_SBE. */
+	uint64_t rxf_push_full                : 1;  /**< RXF overflow. Throws ILK_INTSN_E::ILK_GBL_RXF_PUSH_FULL. */
+	uint64_t rxf_pop_empty                : 1;  /**< RXF underflow. Throws ILK_INTSN_E::ILK_GBL_RXF_POP_EMPTY. */
+	uint64_t rxf_ctl_perr                 : 1;  /**< Reserved. */
+	uint64_t rxf_lnk1_perr                : 1;  /**< Reserved. */
+	uint64_t rxf_lnk0_perr                : 1;  /**< Reserved. */
+#else
+	uint64_t rxf_lnk0_perr                : 1;
+	uint64_t rxf_lnk1_perr                : 1;
+	uint64_t rxf_ctl_perr                 : 1;
+	uint64_t rxf_pop_empty                : 1;
+	uint64_t rxf_push_full                : 1;
+	uint64_t rxf_sbe                      : 1;
+	uint64_t rxf_dbe                      : 1;
+	uint64_t x2p_sbe                      : 1;
+	uint64_t x2p_dbe                      : 1;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_ilk_gbl_int_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_5_63                : 59;
+	uint64_t rxf_push_full                : 1;  /**< RXF overflow */
+	uint64_t rxf_pop_empty                : 1;  /**< RXF underflow */
+	uint64_t rxf_ctl_perr                 : 1;  /**< RXF parity error occurred on sideband control signals.  Data
+                                                         cycle will be dropped. */
+	uint64_t rxf_lnk1_perr                : 1;  /**< RXF parity error occurred on RxLink1 packet data
+                                                         Packet will be marked with error at eop */
+	uint64_t rxf_lnk0_perr                : 1;  /**< RXF parity error occurred on RxLink0 packet data.  Packet will
+                                                         be marked with error at eop */
+#else
+	uint64_t rxf_lnk0_perr                : 1;
+	uint64_t rxf_lnk1_perr                : 1;
+	uint64_t rxf_ctl_perr                 : 1;
+	uint64_t rxf_pop_empty                : 1;
+	uint64_t rxf_push_full                : 1;
+	uint64_t reserved_5_63                : 59;
+#endif
+	} cn68xx;
+	struct cvmx_ilk_gbl_int_cn68xx        cn68xxp1;
+	struct cvmx_ilk_gbl_int_s             cn78xx;
+};
+typedef union cvmx_ilk_gbl_int cvmx_ilk_gbl_int_t;
+
+/**
+ * cvmx_ilk_gbl_int_en
+ */
+union cvmx_ilk_gbl_int_en {
+	uint64_t u64;
+	struct cvmx_ilk_gbl_int_en_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_5_63                : 59;
+	uint64_t rxf_push_full                : 1;  /**< RXF overflow */
+	uint64_t rxf_pop_empty                : 1;  /**< RXF underflow */
+	uint64_t rxf_ctl_perr                 : 1;  /**< RXF parity error occurred on sideband control signals.  Data
+                                                         cycle will be dropped. */
+	uint64_t rxf_lnk1_perr                : 1;  /**< RXF parity error occurred on RxLink1 packet data
+                                                         Packet will be marked with error at eop */
+	uint64_t rxf_lnk0_perr                : 1;  /**< RXF parity error occurred on RxLink0 packet data
+                                                         Packet will be marked with error at eop */
+#else
+	uint64_t rxf_lnk0_perr                : 1;
+	uint64_t rxf_lnk1_perr                : 1;
+	uint64_t rxf_ctl_perr                 : 1;
+	uint64_t rxf_pop_empty                : 1;
+	uint64_t rxf_push_full                : 1;
+	uint64_t reserved_5_63                : 59;
+#endif
+	} s;
+	struct cvmx_ilk_gbl_int_en_s          cn68xx;
+	struct cvmx_ilk_gbl_int_en_s          cn68xxp1;
+};
+typedef union cvmx_ilk_gbl_int_en cvmx_ilk_gbl_int_en_t;
+
+/**
+ * cvmx_ilk_int_sum
+ */
+union cvmx_ilk_int_sum {
+	uint64_t u64;
+	struct cvmx_ilk_int_sum_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_13_63               : 51;
+	uint64_t rle7_int                     : 1;  /**< RxLane7 interrupt status. See ILK_RX_LNE7_INT */
+	uint64_t rle6_int                     : 1;  /**< RxLane6 interrupt status. See ILK_RX_LNE6_INT */
+	uint64_t rle5_int                     : 1;  /**< RxLane5 interrupt status. See ILK_RX_LNE5_INT */
+	uint64_t rle4_int                     : 1;  /**< RxLane4 interrupt status. See ILK_RX_LNE4_INT */
+	uint64_t rle3_int                     : 1;  /**< RxLane3 interrupt status. See ILK_RX_LNE3_INT */
+	uint64_t rle2_int                     : 1;  /**< RxLane2 interrupt status. See ILK_RX_LNE2_INT */
+	uint64_t rle1_int                     : 1;  /**< RxLane1 interrupt status. See ILK_RX_LNE1_INT */
+	uint64_t rle0_int                     : 1;  /**< RxLane0 interrupt status. See ILK_RX_LNE0_INT */
+	uint64_t rlk1_int                     : 1;  /**< RxLink1 interrupt status. See ILK_RX1_INT */
+	uint64_t rlk0_int                     : 1;  /**< RxLink0 interrupt status. See ILK_RX0_INT */
+	uint64_t tlk1_int                     : 1;  /**< TxLink1 interrupt status. See ILK_TX1_INT */
+	uint64_t tlk0_int                     : 1;  /**< TxLink0 interrupt status. See ILK_TX0_INT */
+	uint64_t gbl_int                      : 1;  /**< Global interrupt status. See ILK_GBL_INT */
+#else
+	uint64_t gbl_int                      : 1;
+	uint64_t tlk0_int                     : 1;
+	uint64_t tlk1_int                     : 1;
+	uint64_t rlk0_int                     : 1;
+	uint64_t rlk1_int                     : 1;
+	uint64_t rle0_int                     : 1;
+	uint64_t rle1_int                     : 1;
+	uint64_t rle2_int                     : 1;
+	uint64_t rle3_int                     : 1;
+	uint64_t rle4_int                     : 1;
+	uint64_t rle5_int                     : 1;
+	uint64_t rle6_int                     : 1;
+	uint64_t rle7_int                     : 1;
+	uint64_t reserved_13_63               : 51;
+#endif
+	} s;
+	struct cvmx_ilk_int_sum_s             cn68xx;
+	struct cvmx_ilk_int_sum_s             cn68xxp1;
+};
+typedef union cvmx_ilk_int_sum cvmx_ilk_int_sum_t;
+
+/**
+ * cvmx_ilk_lne#_trn_ctl
+ */
+union cvmx_ilk_lnex_trn_ctl {
+	uint64_t u64;
+	struct cvmx_ilk_lnex_trn_ctl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t trn_lock                     : 1;  /**< Link training RX frame lock. */
+	uint64_t trn_done                     : 1;  /**< Link training done. */
+	uint64_t trn_ena                      : 1;  /**< Link training enable. */
+	uint64_t eie_det                      : 1;  /**< Reserved. */
+#else
+	uint64_t eie_det                      : 1;
+	uint64_t trn_ena                      : 1;
+	uint64_t trn_done                     : 1;
+	uint64_t trn_lock                     : 1;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_ilk_lnex_trn_ctl_s        cn78xx;
+};
+typedef union cvmx_ilk_lnex_trn_ctl cvmx_ilk_lnex_trn_ctl_t;
+
+/**
+ * cvmx_ilk_lne#_trn_ld
+ */
+union cvmx_ilk_lnex_trn_ld {
+	uint64_t u64;
+	struct cvmx_ilk_lnex_trn_ld_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_49_63               : 15;
+	uint64_t ld_cu_val                    : 1;  /**< Local device coefficient update field valid. */
+	uint64_t ld_cu_dat                    : 16; /**< Local device coefficient update field data. */
+	uint64_t reserved_17_31               : 15;
+	uint64_t ld_sr_val                    : 1;  /**< Local device status report field valid. */
+	uint64_t ld_sr_dat                    : 16; /**< Local device status report field data. */
+#else
+	uint64_t ld_sr_dat                    : 16;
+	uint64_t ld_sr_val                    : 1;
+	uint64_t reserved_17_31               : 15;
+	uint64_t ld_cu_dat                    : 16;
+	uint64_t ld_cu_val                    : 1;
+	uint64_t reserved_49_63               : 15;
+#endif
+	} s;
+	struct cvmx_ilk_lnex_trn_ld_s         cn78xx;
+};
+typedef union cvmx_ilk_lnex_trn_ld cvmx_ilk_lnex_trn_ld_t;
+
+/**
+ * cvmx_ilk_lne#_trn_lp
+ */
+union cvmx_ilk_lnex_trn_lp {
+	uint64_t u64;
+	struct cvmx_ilk_lnex_trn_lp_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_49_63               : 15;
+	uint64_t lp_cu_val                    : 1;  /**< Link partner coefficient update field valid. */
+	uint64_t lp_cu_dat                    : 16; /**< Link partner coefficient update field data. */
+	uint64_t reserved_17_31               : 15;
+	uint64_t lp_sr_val                    : 1;  /**< Link partner status report field valid. */
+	uint64_t lp_sr_dat                    : 16; /**< Link partner status report field data. */
+#else
+	uint64_t lp_sr_dat                    : 16;
+	uint64_t lp_sr_val                    : 1;
+	uint64_t reserved_17_31               : 15;
+	uint64_t lp_cu_dat                    : 16;
+	uint64_t lp_cu_val                    : 1;
+	uint64_t reserved_49_63               : 15;
+#endif
+	} s;
+	struct cvmx_ilk_lnex_trn_lp_s         cn78xx;
+};
+typedef union cvmx_ilk_lnex_trn_lp cvmx_ilk_lnex_trn_lp_t;
+
+/**
+ * cvmx_ilk_lne_dbg
+ */
+union cvmx_ilk_lne_dbg {
+	uint64_t u64;
+	struct cvmx_ilk_lne_dbg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_60_63               : 4;
+	uint64_t tx_bad_crc32                 : 1;  /**< Send one diagnostic word with bad CRC32 to the selected lane. Note that it injects just once. */
+	uint64_t tx_bad_6467_cnt              : 5;  /**< Specifies the number of bad 64B/67B code words on the selected lane. */
+	uint64_t tx_bad_sync_cnt              : 3;  /**< Specifies the number of bad sync words on the selected lane. */
+	uint64_t tx_bad_scram_cnt             : 3;  /**< Specifies the number of bad scrambler state on the selected lane. */
+	uint64_t tx_bad_lane_sel              : 16; /**< Select the lane to apply the error-injection counts. */
+	uint64_t tx_dis_dispr                 : 16; /**< Per-lane disparity disable. */
+	uint64_t tx_dis_scram                 : 16; /**< Per-lane scrambler disable. */
+#else
+	uint64_t tx_dis_scram                 : 16;
+	uint64_t tx_dis_dispr                 : 16;
+	uint64_t tx_bad_lane_sel              : 16;
+	uint64_t tx_bad_scram_cnt             : 3;
+	uint64_t tx_bad_sync_cnt              : 3;
+	uint64_t tx_bad_6467_cnt              : 5;
+	uint64_t tx_bad_crc32                 : 1;
+	uint64_t reserved_60_63               : 4;
+#endif
+	} s;
+	struct cvmx_ilk_lne_dbg_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_60_63               : 4;
+	uint64_t tx_bad_crc32                 : 1;  /**< Send 1 diagnostic word with bad CRC32 to the selected lane.
+                                                         Note: injects just once */
+	uint64_t tx_bad_6467_cnt              : 5;  /**< Send N bad 64B/67B codewords on selected lane */
+	uint64_t tx_bad_sync_cnt              : 3;  /**< Send N bad sync words on selected lane */
+	uint64_t tx_bad_scram_cnt             : 3;  /**< Send N bad scram state on selected lane */
+	uint64_t reserved_40_47               : 8;
+	uint64_t tx_bad_lane_sel              : 8;  /**< Select lane to apply error injection counts */
+	uint64_t reserved_24_31               : 8;
+	uint64_t tx_dis_dispr                 : 8;  /**< Per-lane disparity disable */
+	uint64_t reserved_8_15                : 8;
+	uint64_t tx_dis_scram                 : 8;  /**< Per-lane scrambler disable */
+#else
+	uint64_t tx_dis_scram                 : 8;
+	uint64_t reserved_8_15                : 8;
+	uint64_t tx_dis_dispr                 : 8;
+	uint64_t reserved_24_31               : 8;
+	uint64_t tx_bad_lane_sel              : 8;
+	uint64_t reserved_40_47               : 8;
+	uint64_t tx_bad_scram_cnt             : 3;
+	uint64_t tx_bad_sync_cnt              : 3;
+	uint64_t tx_bad_6467_cnt              : 5;
+	uint64_t tx_bad_crc32                 : 1;
+	uint64_t reserved_60_63               : 4;
+#endif
+	} cn68xx;
+	struct cvmx_ilk_lne_dbg_cn68xx        cn68xxp1;
+	struct cvmx_ilk_lne_dbg_s             cn78xx;
+};
+typedef union cvmx_ilk_lne_dbg cvmx_ilk_lne_dbg_t;
+
+/**
+ * cvmx_ilk_lne_sts_msg
+ */
+union cvmx_ilk_lne_sts_msg {
+	uint64_t u64;
+	struct cvmx_ilk_lne_sts_msg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t rx_lnk_stat                  : 16; /**< Link status received in the diagnostic word (per-lane); 1 means healthy (according to the
+                                                         Interlaken specification). */
+	uint64_t rx_lne_stat                  : 16; /**< Lane status received in the diagnostic word (per-lane); 1 means healthy (according to the
+                                                         Interlaken specification). */
+	uint64_t tx_lnk_stat                  : 16; /**< Link status transmitted in the diagnostic word (per-lane); 1 means healthy (according to
+                                                         the Interlaken specification). */
+	uint64_t tx_lne_stat                  : 16; /**< Lane status transmitted in the diagnostic word (per-lane); 1 means healthy (according to
+                                                         the Interlaken specification). */
+#else
+	uint64_t tx_lne_stat                  : 16;
+	uint64_t tx_lnk_stat                  : 16;
+	uint64_t rx_lne_stat                  : 16;
+	uint64_t rx_lnk_stat                  : 16;
+#endif
+	} s;
+	struct cvmx_ilk_lne_sts_msg_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_56_63               : 8;
+	uint64_t rx_lnk_stat                  : 8;  /**< Link status received in the diagnostic word (per-lane)
+                                                         '1' means healthy (according to the Interlaken spec) */
+	uint64_t reserved_40_47               : 8;
+	uint64_t rx_lne_stat                  : 8;  /**< Lane status received in the diagnostic word (per-lane)
+                                                         '1' means healthy (according to the Interlaken spec) */
+	uint64_t reserved_24_31               : 8;
+	uint64_t tx_lnk_stat                  : 8;  /**< Link status transmitted in the diagnostic word (per-lane)
+                                                         '1' means healthy (according to the Interlaken spec) */
+	uint64_t reserved_8_15                : 8;
+	uint64_t tx_lne_stat                  : 8;  /**< Lane status transmitted in the diagnostic word (per-lane)
+                                                         '1' means healthy (according to the Interlaken spec) */
+#else
+	uint64_t tx_lne_stat                  : 8;
+	uint64_t reserved_8_15                : 8;
+	uint64_t tx_lnk_stat                  : 8;
+	uint64_t reserved_24_31               : 8;
+	uint64_t rx_lne_stat                  : 8;
+	uint64_t reserved_40_47               : 8;
+	uint64_t rx_lnk_stat                  : 8;
+	uint64_t reserved_56_63               : 8;
+#endif
+	} cn68xx;
+	struct cvmx_ilk_lne_sts_msg_cn68xx    cn68xxp1;
+	struct cvmx_ilk_lne_sts_msg_s         cn78xx;
+};
+typedef union cvmx_ilk_lne_sts_msg cvmx_ilk_lne_sts_msg_t;
+
+/**
+ * cvmx_ilk_rid_cfg
+ */
+union cvmx_ilk_rid_cfg {
+	uint64_t u64;
+	struct cvmx_ilk_rid_cfg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_39_63               : 25;
+	uint64_t max_cnt                      : 7;  /**< Maximum number of reassembly IDs (RIDs) allowed for both links. If
+                                                         an SOP arrives and the total number of RIDs already allocated to
+                                                         both links is at least MAX_CNT, the packet is dropped.
+                                                         An SOP allocates a RID; an EOP frees a RID. ILK_RX()_RID can be used to further
+                                                         restrict each link individually. */
+	uint64_t reserved_7_31                : 25;
+	uint64_t base                         : 7;  /**< The base RID for ILK. There is a shared pool of 96 RIDs for all MACs.
+                                                         ILK can allocate any RID in the range of
+                                                         _ BASE -> (BASE+(MAX_CNT-1)).
+                                                         BASE and MAX_CNT must be constrained such that:
+                                                         _ 1) BASE >= 2.
+                                                         _ 2) BASE + MAX_CNT <= 96.
+                                                         _ 3) BASE..(BASE+(MAX_CNT-1)) does not overlap with any other MAC programming.
+                                                         The reset value for this CSR has been chosen such that all these conditions are satisfied.
+                                                         The reset value supports up to a total of 64 outstanding incomplete packets between ILK0
+                                                         and ILK1.
+                                                         Changes to BASE must only occur when ILK0 and ILK1 are both quiescent (i.e. Both ILK0 and
+                                                         ILK1 receive interfaces are down and the RX fifo is empty). */
+#else
+	uint64_t base                         : 7;
+	uint64_t reserved_7_31                : 25;
+	uint64_t max_cnt                      : 7;
+	uint64_t reserved_39_63               : 25;
+#endif
+	} s;
+	struct cvmx_ilk_rid_cfg_s             cn78xx;
+};
+typedef union cvmx_ilk_rid_cfg cvmx_ilk_rid_cfg_t;
+
+/**
+ * cvmx_ilk_rx#_byte_cnt#
+ */
+union cvmx_ilk_rxx_byte_cntx {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_byte_cntx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_40_63               : 24;
+	uint64_t rx_bytes                     : 40; /**< Number of bytes received per channel. Wraps on overflow. On overflow, sets
+                                                         ILK_RX()_INT[STAT_CNT_OVFL]. */
+#else
+	uint64_t rx_bytes                     : 40;
+	uint64_t reserved_40_63               : 24;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_byte_cntx_s       cn78xx;
+};
+typedef union cvmx_ilk_rxx_byte_cntx cvmx_ilk_rxx_byte_cntx_t;
+
+/**
+ * cvmx_ilk_rx#_cal_entry#
+ */
+union cvmx_ilk_rxx_cal_entryx {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_cal_entryx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_34_63               : 30;
+	uint64_t ctl                          : 2;  /**< Select source of XON/XOFF for entry (IDX * 8) + 0.
+                                                         0 = PKO backpressure channel.
+                                                         1 = link.
+                                                         2 = XOFF.
+                                                         3 = XON.
+                                                         This field applies to one of bits <55>, <47>, or <31> in the Interlaken control word. */
+	uint64_t reserved_8_31                : 24;
+	uint64_t channel                      : 8;  /**< PKO channel for the calendar table entry. Unused if CTL == 0x1. */
+#else
+	uint64_t channel                      : 8;
+	uint64_t reserved_8_31                : 24;
+	uint64_t ctl                          : 2;
+	uint64_t reserved_34_63               : 30;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_cal_entryx_s      cn78xx;
+};
+typedef union cvmx_ilk_rxx_cal_entryx cvmx_ilk_rxx_cal_entryx_t;
+
+/**
+ * cvmx_ilk_rx#_cfg0
+ */
+union cvmx_ilk_rxx_cfg0 {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_cfg0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t ext_lpbk_fc                  : 1;  /**< Enable RX-TX flow-control external loopback. */
+	uint64_t ext_lpbk                     : 1;  /**< Enable RX-TX data external loopback. Note that with differing transmit and receive clocks,
+                                                         skip word are inserted/deleted. */
+	uint64_t reserved_60_61               : 2;
+	uint64_t lnk_stats_wrap               : 1;  /**< Upon overflow, a statistics counter should wrap instead of
+                                                         saturating. */
+	uint64_t bcw_push                     : 1;  /**< Reserved. */
+	uint64_t mproto_ign                   : 1;  /**< Reserved. */
+	uint64_t ptrn_mode                    : 1;  /**< Reserved. */
+	uint64_t lnk_stats_rdclr              : 1;  /**< Enable that a CSR read operation to ILK_RX()_STAT0..ILK_RX()_STAT9 clears the counter
+                                                         after returning its current value. */
+	uint64_t lnk_stats_ena                : 1;  /**< Enable link-statistics counters. */
+	uint64_t mltuse_fc_ena                : 1;  /**< Use multiuse field for calendar. */
+	uint64_t cal_ena                      : 1;  /**< Enable the RX calendar. When the calendar table is disabled, all port-pipes receive XON. */
+	uint64_t mfrm_len                     : 13; /**< The quantity of data sent on each lane including one sync word, scrambler state,
+                                                         diagnostic word, zero or more skip words, and the data payload. Must be large than
+                                                         _ ILK_TX()_CFG1[SKIP_CNT] + 32.
+                                                         Supported range:
+                                                         _ ILK_TX()_CFG1[SKIP_CNT] + 32 < MFRM_LEN <= 4096 */
+	uint64_t brst_shrt                    : 7;  /**< Minimum interval between burst control words, as a multiple of eight bytes. Supported
+                                                         range from 8 to 512 bytes (i.e. 4 <= BRST_SHRT <= 64).
+                                                         This field affects the ILK_RX()_STAT4[BRST_SHRT_ERR_CNT] counter. It does not affect
+                                                         correct operation of the link. */
+	uint64_t lane_rev                     : 1;  /**< Lane reversal. When enabled, lane destriping is performed from most-significant lane
+                                                         enabled to least-significant lane enabled. LANE_ENA must be 0 before changing LANE_REV. */
+	uint64_t brst_max                     : 5;  /**< Maximum size of a data burst, as a multiple of 64-byte blocks. Supported range is from 64
+                                                         to 1024 bytes
+                                                         (i.e. 0 < BRST_MAX <= 16).
+                                                         This field affects the ILK_RX()_STAT2[BRST_NOT_FULL_CNT] and
+                                                         ILK_RX()_STAT3[BRST_MAX_ERR_CNT] counters. It does not affect correct operation of the
+                                                         link. */
+	uint64_t reserved_25_25               : 1;
+	uint64_t cal_depth                    : 9;  /**< Indicates the number of valid entries in the calendar.   Supported range from 1 to 288. */
+	uint64_t lane_ena                     : 16; /**< Lane-enable mask. The link is enabled if any lane is enabled. The same lane should not be
+                                                         enabled in multiple ILK_RXn_CFG0. Each bit of LANE_ENA maps to an RX lane (RLE) and a QLM
+                                                         lane. Note that LANE_REV has no effect on this mapping.
+                                                         _ LANE_ENA<0> = RLE0 = QLM4 lane 0.
+                                                         _ LANE_ENA<1> = RLE1 = QLM4 lane 1.
+                                                         _ LANE_ENA<2> = RLE2 = QLM4 lane 2.
+                                                         _ LANE_ENA<3> = RLE3 = QLM4 lane 3.
+                                                         _ LANE_ENA<4> = RLE4 = QLM5 lane 0.
+                                                         _ LANE_ENA<5> = RLE5 = QLM5 lane 1.
+                                                         _ LANE_ENA<6> = RLE6 = QLM5 lane 2.
+                                                         _ LANE_ENA<7> = RLE7 = QLM5 lane 3.
+                                                         _ LANE_ENA<8> = RLE8 = QLM6 lane 0.
+                                                         _ LANE_ENA<9> = RLE9 = QLM6 lane 1.
+                                                         _ LANE_ENA<10> = RLE10 = QLM6 lane 2.
+                                                         _ LANE_ENA<11> = RLE11 = QLM6 lane 3.
+                                                         _ LANE_ENA<12> = RLE12 = QLM7 lane 0.
+                                                         _ LANE_ENA<13> = RLE13 = QLM7 lane 1.
+                                                         _ LANE_ENA<14> = RLE14 = QLM7 lane 2.
+                                                         _ LANE_ENA<15> = RLE15 = QLM7 lane 3. */
+#else
+	uint64_t lane_ena                     : 16;
+	uint64_t cal_depth                    : 9;
+	uint64_t reserved_25_25               : 1;
+	uint64_t brst_max                     : 5;
+	uint64_t lane_rev                     : 1;
+	uint64_t brst_shrt                    : 7;
+	uint64_t mfrm_len                     : 13;
+	uint64_t cal_ena                      : 1;
+	uint64_t mltuse_fc_ena                : 1;
+	uint64_t lnk_stats_ena                : 1;
+	uint64_t lnk_stats_rdclr              : 1;
+	uint64_t ptrn_mode                    : 1;
+	uint64_t mproto_ign                   : 1;
+	uint64_t bcw_push                     : 1;
+	uint64_t lnk_stats_wrap               : 1;
+	uint64_t reserved_60_61               : 2;
+	uint64_t ext_lpbk                     : 1;
+	uint64_t ext_lpbk_fc                  : 1;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_cfg0_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t ext_lpbk_fc                  : 1;  /**< Enable Rx-Tx flowcontrol loopback (external) */
+	uint64_t ext_lpbk                     : 1;  /**< Enable Rx-Tx data loopback (external). Note that with differing
+                                                         transmit & receive clocks, skip word are  inserted/deleted */
+	uint64_t reserved_60_61               : 2;
+	uint64_t lnk_stats_wrap               : 1;  /**< Upon overflow, a statistics counter should wrap instead of
+                                                         saturating.
+
+                                                         ***NOTE: Added in pass 2.0 */
+	uint64_t bcw_push                     : 1;  /**< The 8 byte burst control word containing the SOP will be
+                                                         prepended to the corresponding packet.
+
+                                                         ***NOTE: Added in pass 2.0 */
+	uint64_t mproto_ign                   : 1;  /**< When LA_MODE=1 and MPROTO_IGN=0, the multi-protocol bit of the
+                                                         LA control word is used to determine if the burst is an LA or
+                                                         non-LA burst.   When LA_MODE=1 and MPROTO_IGN=1, all bursts
+                                                         are treated LA.   When LA_MODE=0, this field is ignored
+
+                                                         ***NOTE: Added in pass 2.0 */
+	uint64_t ptrn_mode                    : 1;  /**< Reserved */
+	uint64_t lnk_stats_rdclr              : 1;  /**< CSR read to ILK_RXx_STAT* clears the counter after returning
+                                                         its current value. */
+	uint64_t lnk_stats_ena                : 1;  /**< Enable link statistics counters */
+	uint64_t mltuse_fc_ena                : 1;  /**< Use multi-use field for calendar */
+	uint64_t cal_ena                      : 1;  /**< Enable Rx calendar.  When the calendar table is disabled, all
+                                                         port-pipes receive XON. */
+	uint64_t mfrm_len                     : 13; /**< The quantity of data sent on each lane including one sync word,
+                                                         scrambler state, diag word, zero or more skip words, and the
+                                                         data  payload.  Must be large than ILK_RXX_CFG1[SKIP_CNT]+9.
+                                                         Supported range:ILK_RXX_CFG1[SKIP_CNT]+9 < MFRM_LEN <= 4096) */
+	uint64_t brst_shrt                    : 7;  /**< Minimum interval between burst control words, as a multiple of
+                                                         8 bytes.  Supported range from 8 bytes to 512 (ie. 0 <
+                                                         BRST_SHRT <= 64)
+                                                         This field affects the ILK_RX*_STAT4[BRST_SHRT_ERR_CNT]
+                                                         counter. It does not affect correct operation of the link. */
+	uint64_t lane_rev                     : 1;  /**< Lane reversal.   When enabled, lane de-striping is performed
+                                                         from most significant lane enabled to least significant lane
+                                                         enabled.  LANE_ENA must be zero before changing LANE_REV. */
+	uint64_t brst_max                     : 5;  /**< Maximum size of a data burst, as a multiple of 64 byte blocks.
+                                                         Supported range is from 64 bytes to  1024 bytes. (ie. 0 <
+                                                         BRST_MAX <= 16)
+                                                         This field affects the ILK_RX*_STAT2[BRST_NOT_FULL_CNT] and
+                                                         ILK_RX*_STAT3[BRST_MAX_ERR_CNT] counters. It does not affect
+                                                         correct operation of the link. */
+	uint64_t reserved_25_25               : 1;
+	uint64_t cal_depth                    : 9;  /**< Number of valid entries in the calendar.   Supported range from
+                                                         1 to 288. */
+	uint64_t reserved_8_15                : 8;
+	uint64_t lane_ena                     : 8;  /**< Lane enable mask.  Link is enabled if any lane is enabled.  The
+                                                         same lane should not be enabled in multiple ILK_RXx_CFG0.  Each
+                                                         bit of LANE_ENA maps to a RX lane (RLE) and a QLM lane.  NOTE:
+                                                         LANE_REV has no effect on this mapping.
+
+                                                               LANE_ENA[0] = RLE0 = QLM1 lane 0
+                                                               LANE_ENA[1] = RLE1 = QLM1 lane 1
+                                                               LANE_ENA[2] = RLE2 = QLM1 lane 2
+                                                               LANE_ENA[3] = RLE3 = QLM1 lane 3
+                                                               LANE_ENA[4] = RLE4 = QLM2 lane 0
+                                                               LANE_ENA[5] = RLE5 = QLM2 lane 1
+                                                               LANE_ENA[6] = RLE6 = QLM2 lane 2
+                                                               LANE_ENA[7] = RLE7 = QLM2 lane 3 */
+#else
+	uint64_t lane_ena                     : 8;
+	uint64_t reserved_8_15                : 8;
+	uint64_t cal_depth                    : 9;
+	uint64_t reserved_25_25               : 1;
+	uint64_t brst_max                     : 5;
+	uint64_t lane_rev                     : 1;
+	uint64_t brst_shrt                    : 7;
+	uint64_t mfrm_len                     : 13;
+	uint64_t cal_ena                      : 1;
+	uint64_t mltuse_fc_ena                : 1;
+	uint64_t lnk_stats_ena                : 1;
+	uint64_t lnk_stats_rdclr              : 1;
+	uint64_t ptrn_mode                    : 1;
+	uint64_t mproto_ign                   : 1;
+	uint64_t bcw_push                     : 1;
+	uint64_t lnk_stats_wrap               : 1;
+	uint64_t reserved_60_61               : 2;
+	uint64_t ext_lpbk                     : 1;
+	uint64_t ext_lpbk_fc                  : 1;
+#endif
+	} cn68xx;
+	struct cvmx_ilk_rxx_cfg0_cn68xxp1 {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t ext_lpbk_fc                  : 1;  /**< Enable Rx-Tx flowcontrol loopback (external) */
+	uint64_t ext_lpbk                     : 1;  /**< Enable Rx-Tx data loopback (external). Note that with differing
+                                                         transmit & receive clocks, skip word are  inserted/deleted */
+	uint64_t reserved_57_61               : 5;
+	uint64_t ptrn_mode                    : 1;  /**< Enable programmable test pattern mode */
+	uint64_t lnk_stats_rdclr              : 1;  /**< CSR read to ILK_RXx_STAT* clears the counter after returning
+                                                         its current value. */
+	uint64_t lnk_stats_ena                : 1;  /**< Enable link statistics counters */
+	uint64_t mltuse_fc_ena                : 1;  /**< Use multi-use field for calendar */
+	uint64_t cal_ena                      : 1;  /**< Enable Rx calendar.  When the calendar table is disabled, all
+                                                         port-pipes receive XON. */
+	uint64_t mfrm_len                     : 13; /**< The quantity of data sent on each lane including one sync word,
+                                                         scrambler state, diag word, zero or more skip words, and the
+                                                         data  payload.  Must be large than ILK_RXX_CFG1[SKIP_CNT]+9.
+                                                         Supported range:ILK_RXX_CFG1[SKIP_CNT]+9 < MFRM_LEN <= 4096) */
+	uint64_t brst_shrt                    : 7;  /**< Minimum interval between burst control words, as a multiple of
+                                                         8 bytes.  Supported range from 8 bytes to 512 (ie. 0 <
+                                                         BRST_SHRT <= 64)
+                                                         This field affects the ILK_RX*_STAT4[BRST_SHRT_ERR_CNT]
+                                                         counter. It does not affect correct operation of the link. */
+	uint64_t lane_rev                     : 1;  /**< Lane reversal.   When enabled, lane de-striping is performed
+                                                         from most significant lane enabled to least significant lane
+                                                         enabled.  LANE_ENA must be zero before changing LANE_REV. */
+	uint64_t brst_max                     : 5;  /**< Maximum size of a data burst, as a multiple of 64 byte blocks.
+                                                         Supported range is from 64 bytes to  1024 bytes. (ie. 0 <
+                                                         BRST_MAX <= 16)
+                                                         This field affects the ILK_RX*_STAT2[BRST_NOT_FULL_CNT] and
+                                                         ILK_RX*_STAT3[BRST_MAX_ERR_CNT] counters. It does not affect
+                                                         correct operation of the link. */
+	uint64_t reserved_25_25               : 1;
+	uint64_t cal_depth                    : 9;  /**< Number of valid entries in the calendar.   Supported range from
+                                                         1 to 288. */
+	uint64_t reserved_8_15                : 8;
+	uint64_t lane_ena                     : 8;  /**< Lane enable mask.  Link is enabled if any lane is enabled.  The
+                                                         same lane should not be enabled in multiple ILK_RXx_CFG0.  Each
+                                                         bit of LANE_ENA maps to a RX lane (RLE) and a QLM lane.  NOTE:
+                                                         LANE_REV has no effect on this mapping.
+
+                                                               LANE_ENA[0] = RLE0 = QLM1 lane 0
+                                                               LANE_ENA[1] = RLE1 = QLM1 lane 1
+                                                               LANE_ENA[2] = RLE2 = QLM1 lane 2
+                                                               LANE_ENA[3] = RLE3 = QLM1 lane 3
+                                                               LANE_ENA[4] = RLE4 = QLM2 lane 0
+                                                               LANE_ENA[5] = RLE5 = QLM2 lane 1
+                                                               LANE_ENA[6] = RLE6 = QLM2 lane 2
+                                                               LANE_ENA[7] = RLE7 = QLM2 lane 3 */
+#else
+	uint64_t lane_ena                     : 8;
+	uint64_t reserved_8_15                : 8;
+	uint64_t cal_depth                    : 9;
+	uint64_t reserved_25_25               : 1;
+	uint64_t brst_max                     : 5;
+	uint64_t lane_rev                     : 1;
+	uint64_t brst_shrt                    : 7;
+	uint64_t mfrm_len                     : 13;
+	uint64_t cal_ena                      : 1;
+	uint64_t mltuse_fc_ena                : 1;
+	uint64_t lnk_stats_ena                : 1;
+	uint64_t lnk_stats_rdclr              : 1;
+	uint64_t ptrn_mode                    : 1;
+	uint64_t reserved_57_61               : 5;
+	uint64_t ext_lpbk                     : 1;
+	uint64_t ext_lpbk_fc                  : 1;
+#endif
+	} cn68xxp1;
+	struct cvmx_ilk_rxx_cfg0_s            cn78xx;
+};
+typedef union cvmx_ilk_rxx_cfg0 cvmx_ilk_rxx_cfg0_t;
+
+/**
+ * cvmx_ilk_rx#_cfg1
+ */
+union cvmx_ilk_rxx_cfg1 {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_cfg1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_62_63               : 2;
+	uint64_t rx_fifo_cnt                  : 12; /**< Number of 64-bit words currently consumed by this link in the RX FIFO. */
+	uint64_t reserved_49_49               : 1;
+	uint64_t rx_fifo_hwm                  : 13; /**< Number of 64-bit words consumed by this link before switch transmitted link flow-control
+                                                         status from XON to XOFF. LSB must be zero. A typical single-link configuration should set
+                                                         this to 2048. A typical multi-link configuration should set this to NL*128 where NL is the
+                                                         number of lanes enabled for a given link.
+                                                         _ XON = RX_FIFO_CNT < RX_FIFO_HWM
+                                                         _ XOFF = RX_FIFO_CNT >= RX_FIFO_HWM. */
+	uint64_t reserved_35_35               : 1;
+	uint64_t rx_fifo_max                  : 13; /**< Specifies the maximum number of 64-bit words consumed by this link in the RX FIFO. The sum
+                                                         of all links should be equal to 4096 (32KB). LSB must be zero. Typically set to
+                                                         RX_FIFO_HWM * 2. */
+	uint64_t pkt_flush                    : 1;  /**< Packet receive flush. Setting this bit causes all open packets to be error-out, just as
+                                                         though the link went down. */
+	uint64_t pkt_ena                      : 1;  /**< Packet receive enable. When set to 0, any received SOP causes the entire packet to be dropped. */
+	uint64_t la_mode                      : 1;  /**< Reserved. */
+	uint64_t tx_link_fc                   : 1;  /**< Link flow-control status transmitted by the TX-link XON (=1) when RX_FIFO_CNT <=
+                                                         RX_FIFO_HWM and lane alignment is done. */
+	uint64_t rx_link_fc                   : 1;  /**< Link flow-control status received in burst/idle control words. XOFF (=0) causes TX-link to
+                                                         stop transmitting on all channels. */
+	uint64_t rx_align_ena                 : 1;  /**< Enable the lane alignment. This should only be done after all enabled lanes have achieved
+                                                         word boundary lock and scrambler synchronization. Note that hardware clears this when any
+                                                         participating lane loses either word boundary lock or scrambler synchronization. */
+	uint64_t rx_bdry_lock_ena             : 16; /**< Enable word-boundary lock. While disabled, received data is tossed. Once enabled, received
+                                                         data is searched for legal two-bit patterns. Automatically cleared for disabled lanes. */
+#else
+	uint64_t rx_bdry_lock_ena             : 16;
+	uint64_t rx_align_ena                 : 1;
+	uint64_t rx_link_fc                   : 1;
+	uint64_t tx_link_fc                   : 1;
+	uint64_t la_mode                      : 1;
+	uint64_t pkt_ena                      : 1;
+	uint64_t pkt_flush                    : 1;
+	uint64_t rx_fifo_max                  : 13;
+	uint64_t reserved_35_35               : 1;
+	uint64_t rx_fifo_hwm                  : 13;
+	uint64_t reserved_49_49               : 1;
+	uint64_t rx_fifo_cnt                  : 12;
+	uint64_t reserved_62_63               : 2;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_cfg1_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_62_63               : 2;
+	uint64_t rx_fifo_cnt                  : 12; /**< Number of 64-bit words currently consumed by this link in the
+                                                         RX fifo. */
+	uint64_t reserved_48_49               : 2;
+	uint64_t rx_fifo_hwm                  : 12; /**< Number of 64-bit words consumed by this link before switch
+                                                         transmitted link flow control status from XON to XOFF.
+
+                                                         XON  = RX_FIFO_CNT < RX_FIFO_HWM
+                                                         XOFF = RX_FIFO_CNT >= RX_FIFO_HWM. */
+	uint64_t reserved_34_35               : 2;
+	uint64_t rx_fifo_max                  : 12; /**< Maximum number of 64-bit words consumed by this link in the RX
+                                                         fifo.  The sum of all links should be equal to 2048 (16KB) */
+	uint64_t pkt_flush                    : 1;  /**< Packet receive flush.  Writing PKT_FLUSH=1 will cause all open
+                                                         packets to be error-out, just as though the link went down. */
+	uint64_t pkt_ena                      : 1;  /**< Packet receive enable.  When PKT_ENA=0, any received SOP causes
+                                                         the entire packet to be dropped. */
+	uint64_t la_mode                      : 1;  /**< 0 = Interlaken
+                                                         1 = Interlaken Look-Aside */
+	uint64_t tx_link_fc                   : 1;  /**< Link flow control status transmitted by the Tx-Link
+                                                         XON (=1) when RX_FIFO_CNT <= RX_FIFO_HWM and lane alignment is
+                                                         done */
+	uint64_t rx_link_fc                   : 1;  /**< Link flow control status received in burst/idle control words.
+                                                         XOFF (=0) will cause Tx-Link to stop transmitting on all
+                                                         channels. */
+	uint64_t rx_align_ena                 : 1;  /**< Enable the lane alignment.  This should only be done after all
+                                                         enabled lanes have achieved word boundary lock and scrambler
+                                                         synchronization.  Note: Hardware will clear this when any
+                                                         participating lane loses either word boundary lock or scrambler
+                                                         synchronization */
+	uint64_t reserved_8_15                : 8;
+	uint64_t rx_bdry_lock_ena             : 8;  /**< Enable word boundary lock.  While disabled, received data is
+                                                         tossed.  Once enabled,  received data is searched for legal
+                                                         2bit patterns.  Automatically cleared for disabled lanes. */
+#else
+	uint64_t rx_bdry_lock_ena             : 8;
+	uint64_t reserved_8_15                : 8;
+	uint64_t rx_align_ena                 : 1;
+	uint64_t rx_link_fc                   : 1;
+	uint64_t tx_link_fc                   : 1;
+	uint64_t la_mode                      : 1;
+	uint64_t pkt_ena                      : 1;
+	uint64_t pkt_flush                    : 1;
+	uint64_t rx_fifo_max                  : 12;
+	uint64_t reserved_34_35               : 2;
+	uint64_t rx_fifo_hwm                  : 12;
+	uint64_t reserved_48_49               : 2;
+	uint64_t rx_fifo_cnt                  : 12;
+	uint64_t reserved_62_63               : 2;
+#endif
+	} cn68xx;
+	struct cvmx_ilk_rxx_cfg1_cn68xx       cn68xxp1;
+	struct cvmx_ilk_rxx_cfg1_s            cn78xx;
+};
+typedef union cvmx_ilk_rxx_cfg1 cvmx_ilk_rxx_cfg1_t;
+
+/**
+ * cvmx_ilk_rx#_cha#
+ */
+union cvmx_ilk_rxx_chax {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_chax_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_6_63                : 58;
+	uint64_t port_kind                    : 6;  /**< Specify the port kind for the channel. */
+#else
+	uint64_t port_kind                    : 6;
+	uint64_t reserved_6_63                : 58;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_chax_s            cn78xx;
+};
+typedef union cvmx_ilk_rxx_chax cvmx_ilk_rxx_chax_t;
+
+/**
+ * cvmx_ilk_rx#_cha_xon#
+ */
+union cvmx_ilk_rxx_cha_xonx {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_cha_xonx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t xon                          : 64; /**< Flow control status for channels 0-255, where a 0 indicates the presence of backpressure
+                                                         (i.e. XOFF) and 1 indicates the absence of backpressure (i.e. XON).
+                                                         _ ILK_RX(0..1)_CHA_XON[0] -- Channels 63-0.
+                                                         _ ILK_RX(0..1)_CHA_XON[1] -- Channels 127-64.
+                                                         _ ILK_RX(0..1)_CHA_XON[2] -- Channels 191-128.
+                                                         _ ILK_RX(0..1)_CHA_XON[3] -- Channels 255-192. */
+#else
+	uint64_t xon                          : 64;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_cha_xonx_s        cn78xx;
+};
+typedef union cvmx_ilk_rxx_cha_xonx cvmx_ilk_rxx_cha_xonx_t;
+
+/**
+ * cvmx_ilk_rx#_err_cfg
+ */
+union cvmx_ilk_rxx_err_cfg {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_err_cfg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_20_63               : 44;
+	uint64_t fwc_flip                     : 2;  /**< Testing feature. Flip syndrome bits <1:0> on writes to the FWC RAM to test single-bit or
+                                                         double-bit errors. */
+	uint64_t pmap_flip                    : 2;  /**< Testing feature. Flip syndrome bits <1:0> on writes to the PMAP RAM to test single-bit or
+                                                         double-bit errors. */
+	uint64_t reserved_2_15                : 14;
+	uint64_t fwc_cor_dis                  : 1;  /**< Disable ECC corrector on FWC. */
+	uint64_t pmap_cor_dis                 : 1;  /**< Disable ECC corrector on PMAP. */
+#else
+	uint64_t pmap_cor_dis                 : 1;
+	uint64_t fwc_cor_dis                  : 1;
+	uint64_t reserved_2_15                : 14;
+	uint64_t pmap_flip                    : 2;
+	uint64_t fwc_flip                     : 2;
+	uint64_t reserved_20_63               : 44;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_err_cfg_s         cn78xx;
+};
+typedef union cvmx_ilk_rxx_err_cfg cvmx_ilk_rxx_err_cfg_t;
+
+/**
+ * cvmx_ilk_rx#_flow_ctl0
+ */
+union cvmx_ilk_rxx_flow_ctl0 {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_flow_ctl0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t status                       : 64; /**< Flow control status for port-pipes 63-0, where a 1 indicates
+                                                         the presence of backpressure (ie. XOFF) and 0 indicates the
+                                                         absence of backpressure (ie. XON) */
+#else
+	uint64_t status                       : 64;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_flow_ctl0_s       cn68xx;
+	struct cvmx_ilk_rxx_flow_ctl0_s       cn68xxp1;
+};
+typedef union cvmx_ilk_rxx_flow_ctl0 cvmx_ilk_rxx_flow_ctl0_t;
+
+/**
+ * cvmx_ilk_rx#_flow_ctl1
+ */
+union cvmx_ilk_rxx_flow_ctl1 {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_flow_ctl1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t status                       : 64; /**< Flow control status for port-pipes 127-64, where a 1 indicates
+                                                         the presence of backpressure (ie. XOFF) and 0 indicates the
+                                                         absence of backpressure (ie. XON) */
+#else
+	uint64_t status                       : 64;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_flow_ctl1_s       cn68xx;
+	struct cvmx_ilk_rxx_flow_ctl1_s       cn68xxp1;
+};
+typedef union cvmx_ilk_rxx_flow_ctl1 cvmx_ilk_rxx_flow_ctl1_t;
+
+/**
+ * cvmx_ilk_rx#_idx_cal
+ */
+union cvmx_ilk_rxx_idx_cal {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_idx_cal_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_14_63               : 50;
+	uint64_t inc                          : 6;  /**< Increment to add to current index for next index. NOTE:
+                                                         Increment performed after access to   ILK_RXx_MEM_CAL1 */
+	uint64_t reserved_6_7                 : 2;
+	uint64_t index                        : 6;  /**< Specify the group of 8 entries accessed by the next CSR
+                                                         read/write to calendar table memory.  Software must never write
+                                                         IDX >= 36 */
+#else
+	uint64_t index                        : 6;
+	uint64_t reserved_6_7                 : 2;
+	uint64_t inc                          : 6;
+	uint64_t reserved_14_63               : 50;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_idx_cal_s         cn68xx;
+	struct cvmx_ilk_rxx_idx_cal_s         cn68xxp1;
+};
+typedef union cvmx_ilk_rxx_idx_cal cvmx_ilk_rxx_idx_cal_t;
+
+/**
+ * cvmx_ilk_rx#_idx_stat0
+ */
+union cvmx_ilk_rxx_idx_stat0 {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_idx_stat0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_32_63               : 32;
+	uint64_t clr                          : 1;  /**< CSR read to ILK_RXx_MEM_STAT0 clears the selected counter after
+                                                         returning its current value. */
+	uint64_t reserved_24_30               : 7;
+	uint64_t inc                          : 8;  /**< Increment to add to current index for next index */
+	uint64_t reserved_8_15                : 8;
+	uint64_t index                        : 8;  /**< Specify the channel accessed during the next CSR read to the
+                                                         ILK_RXx_MEM_STAT0 */
+#else
+	uint64_t index                        : 8;
+	uint64_t reserved_8_15                : 8;
+	uint64_t inc                          : 8;
+	uint64_t reserved_24_30               : 7;
+	uint64_t clr                          : 1;
+	uint64_t reserved_32_63               : 32;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_idx_stat0_s       cn68xx;
+	struct cvmx_ilk_rxx_idx_stat0_s       cn68xxp1;
+};
+typedef union cvmx_ilk_rxx_idx_stat0 cvmx_ilk_rxx_idx_stat0_t;
+
+/**
+ * cvmx_ilk_rx#_idx_stat1
+ */
+union cvmx_ilk_rxx_idx_stat1 {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_idx_stat1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_32_63               : 32;
+	uint64_t clr                          : 1;  /**< CSR read to ILK_RXx_MEM_STAT1 clears the selected counter after
+                                                         returning its current value. */
+	uint64_t reserved_24_30               : 7;
+	uint64_t inc                          : 8;  /**< Increment to add to current index for next index */
+	uint64_t reserved_8_15                : 8;
+	uint64_t index                        : 8;  /**< Specify the channel accessed during the next CSR read to the
+                                                         ILK_RXx_MEM_STAT1 */
+#else
+	uint64_t index                        : 8;
+	uint64_t reserved_8_15                : 8;
+	uint64_t inc                          : 8;
+	uint64_t reserved_24_30               : 7;
+	uint64_t clr                          : 1;
+	uint64_t reserved_32_63               : 32;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_idx_stat1_s       cn68xx;
+	struct cvmx_ilk_rxx_idx_stat1_s       cn68xxp1;
+};
+typedef union cvmx_ilk_rxx_idx_stat1 cvmx_ilk_rxx_idx_stat1_t;
+
+/**
+ * cvmx_ilk_rx#_int
+ */
+union cvmx_ilk_rxx_int {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_int_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_13_63               : 51;
+	uint64_t pmap_dbe                     : 1;  /**< Port-kind map double-bit error. Throws ILK_INTSN_E::ILK_RX()_PMAP_DBE. */
+	uint64_t pmap_sbe                     : 1;  /**< Port-kind map single-bit error. Throws ILK_INTSN_E::ILK_RX()_PMAP_SBE. */
+	uint64_t fwc_dbe                      : 1;  /**< Flow control calendar table double-bit error. Throws ILK_INTSN_E::ILK_RX()_FWC_DBE. */
+	uint64_t fwc_sbe                      : 1;  /**< Flow control calendar table single-bit error. Throws ILK_INTSN_E::ILK_RX()_FWC_SBE. */
+	uint64_t pkt_drop_sop                 : 1;  /**< Entire packet dropped due to RX_FIFO_CNT == RX_FIFO_MAX, lack of reassembly IDs or because
+                                                         ILK_RX()_CFG1[PKT_ENA]=0. Throws ILK_INTSN_E::ILK_RX()_PKT_DROP_SOP. */
+	uint64_t pkt_drop_rid                 : 1;  /**< Entire packet dropped due to the lack of reassembly IDs or because
+                                                         ILK_RX()_CFG1[PKT_ENA]=0. Throws ILK_INTSN_E::ILK_RX()_PKT_DROP_RID. */
+	uint64_t pkt_drop_rxf                 : 1;  /**< Some/all of a packet dropped due to RX_FIFO_CNT == RX_FIFO_MAX. Throws
+                                                         ILK_INTSN_E::ILK_RX()_PKT_DROP_RXF. */
+	uint64_t lane_bad_word                : 1;  /**< A lane encountered either a bad 64B/67B codeword or an unknown control word type. Throws
+                                                         ILK_INTSN_E::ILK_RX()_LANE_BAD_WORD. */
+	uint64_t stat_cnt_ovfl                : 1;  /**< Statistics counter overflow. Throws ILK_INTSN_E::ILK_RX()_STAT_CNT_OVFL. */
+	uint64_t lane_align_done              : 1;  /**< Lane alignment successful. Throws ILK_INTSN_E::ILK_RX()_LANE_ALIGN_DONE. */
+	uint64_t word_sync_done               : 1;  /**< All enabled lanes have achieved word boundary lock and scrambler synchronization. Lane
+                                                         alignment may now be enabled. Throws ILK_INTSN_E::ILK_RX()_WORD_SYNC_DONE. */
+	uint64_t crc24_err                    : 1;  /**< Burst CRC24 error. All open packets receive an error. Throws
+                                                         ILK_INTSN_E::ILK_RX()_CRC24_ERR. */
+	uint64_t lane_align_fail              : 1;  /**< Lane Alignment fails (4 tries). Hardware repeats lane alignment until is succeeds or until
+                                                         ILK_RX()_CFG1[RX_ALIGN_ENA] is cleared. Throws
+                                                         ILK_INTSN_E::ILK_RX()_LANE_ALIGN_FAIL. */
+#else
+	uint64_t lane_align_fail              : 1;
+	uint64_t crc24_err                    : 1;
+	uint64_t word_sync_done               : 1;
+	uint64_t lane_align_done              : 1;
+	uint64_t stat_cnt_ovfl                : 1;
+	uint64_t lane_bad_word                : 1;
+	uint64_t pkt_drop_rxf                 : 1;
+	uint64_t pkt_drop_rid                 : 1;
+	uint64_t pkt_drop_sop                 : 1;
+	uint64_t fwc_sbe                      : 1;
+	uint64_t fwc_dbe                      : 1;
+	uint64_t pmap_sbe                     : 1;
+	uint64_t pmap_dbe                     : 1;
+	uint64_t reserved_13_63               : 51;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_int_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t pkt_drop_sop                 : 1;  /**< Entire packet dropped due to RX_FIFO_CNT == RX_FIFO_MAX,
+                                                         lack of reassembly-ids or because ILK_RXX_CFG1[PKT_ENA]=0      | $RW
+                                                         because ILK_RXX_CFG1[PKT_ENA]=0
+
+                                                         ***NOTE: Added in pass 2.0 */
+	uint64_t pkt_drop_rid                 : 1;  /**< Entire packet dropped due to the lack of reassembly-ids or
+                                                         because ILK_RXX_CFG1[PKT_ENA]=0 */
+	uint64_t pkt_drop_rxf                 : 1;  /**< Some/all of a packet dropped due to RX_FIFO_CNT == RX_FIFO_MAX */
+	uint64_t lane_bad_word                : 1;  /**< A lane encountered either a bad 64B/67B codeword or an unknown
+                                                         control word type. */
+	uint64_t stat_cnt_ovfl                : 1;  /**< Statistics counter overflow */
+	uint64_t lane_align_done              : 1;  /**< Lane alignment successful */
+	uint64_t word_sync_done               : 1;  /**< All enabled lanes have achieved word boundary lock and
+                                                         scrambler synchronization.  Lane alignment may now be enabled. */
+	uint64_t crc24_err                    : 1;  /**< Burst CRC24 error.  All open packets will be receive an error. */
+	uint64_t lane_align_fail              : 1;  /**< Lane Alignment fails (4 tries).  Hardware will repeat lane
+                                                         alignment until is succeeds or until ILK_RXx_CFG1[RX_ALIGN_ENA]
+                                                         is cleared. */
+#else
+	uint64_t lane_align_fail              : 1;
+	uint64_t crc24_err                    : 1;
+	uint64_t word_sync_done               : 1;
+	uint64_t lane_align_done              : 1;
+	uint64_t stat_cnt_ovfl                : 1;
+	uint64_t lane_bad_word                : 1;
+	uint64_t pkt_drop_rxf                 : 1;
+	uint64_t pkt_drop_rid                 : 1;
+	uint64_t pkt_drop_sop                 : 1;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} cn68xx;
+	struct cvmx_ilk_rxx_int_cn68xxp1 {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_8_63                : 56;
+	uint64_t pkt_drop_rid                 : 1;  /**< Entire packet dropped due to the lack of reassembly-ids or
+                                                         because ILK_RXX_CFG1[PKT_ENA]=0 */
+	uint64_t pkt_drop_rxf                 : 1;  /**< Some/all of a packet dropped due to RX_FIFO_CNT == RX_FIFO_MAX */
+	uint64_t lane_bad_word                : 1;  /**< A lane encountered either a bad 64B/67B codeword or an unknown
+                                                         control word type. */
+	uint64_t stat_cnt_ovfl                : 1;  /**< Statistics counter overflow */
+	uint64_t lane_align_done              : 1;  /**< Lane alignment successful */
+	uint64_t word_sync_done               : 1;  /**< All enabled lanes have achieved word boundary lock and
+                                                         scrambler synchronization.  Lane alignment may now be enabled. */
+	uint64_t crc24_err                    : 1;  /**< Burst CRC24 error.  All open packets will be receive an error. */
+	uint64_t lane_align_fail              : 1;  /**< Lane Alignment fails (4 tries).  Hardware will repeat lane
+                                                         alignment until is succeeds or until ILK_RXx_CFG1[RX_ALIGN_ENA]
+                                                         is cleared. */
+#else
+	uint64_t lane_align_fail              : 1;
+	uint64_t crc24_err                    : 1;
+	uint64_t word_sync_done               : 1;
+	uint64_t lane_align_done              : 1;
+	uint64_t stat_cnt_ovfl                : 1;
+	uint64_t lane_bad_word                : 1;
+	uint64_t pkt_drop_rxf                 : 1;
+	uint64_t pkt_drop_rid                 : 1;
+	uint64_t reserved_8_63                : 56;
+#endif
+	} cn68xxp1;
+	struct cvmx_ilk_rxx_int_s             cn78xx;
+};
+typedef union cvmx_ilk_rxx_int cvmx_ilk_rxx_int_t;
+
+/**
+ * cvmx_ilk_rx#_int_en
+ */
+union cvmx_ilk_rxx_int_en {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_int_en_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t pkt_drop_sop                 : 1;  /**< Entire packet dropped due to RX_FIFO_CNT == RX_FIFO_MAX,
+                                                         lack of reassembly-ids or because ILK_RXX_CFG1[PKT_ENA]=0      | $PRW
+                                                         because ILK_RXX_CFG1[PKT_ENA]=0
+
+                                                         ***NOTE: Added in pass 2.0 */
+	uint64_t pkt_drop_rid                 : 1;  /**< Entire packet dropped due to the lack of reassembly-ids or
+                                                         because ILK_RXX_CFG1[PKT_ENA]=0 */
+	uint64_t pkt_drop_rxf                 : 1;  /**< Some/all of a packet dropped due to RX_FIFO_CNT == RX_FIFO_MAX */
+	uint64_t lane_bad_word                : 1;  /**< A lane encountered either a bad 64B/67B codeword or an unknown
+                                                         control word type. */
+	uint64_t stat_cnt_ovfl                : 1;  /**< Statistics counter overflow */
+	uint64_t lane_align_done              : 1;  /**< Lane alignment successful */
+	uint64_t word_sync_done               : 1;  /**< All enabled lanes have achieved word boundary lock and
+                                                         scrambler synchronization.  Lane alignment may now be enabled. */
+	uint64_t crc24_err                    : 1;  /**< Burst CRC24 error.  All open packets will be receive an error. */
+	uint64_t lane_align_fail              : 1;  /**< Lane Alignment fails (4 tries) */
+#else
+	uint64_t lane_align_fail              : 1;
+	uint64_t crc24_err                    : 1;
+	uint64_t word_sync_done               : 1;
+	uint64_t lane_align_done              : 1;
+	uint64_t stat_cnt_ovfl                : 1;
+	uint64_t lane_bad_word                : 1;
+	uint64_t pkt_drop_rxf                 : 1;
+	uint64_t pkt_drop_rid                 : 1;
+	uint64_t pkt_drop_sop                 : 1;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_int_en_s          cn68xx;
+	struct cvmx_ilk_rxx_int_en_cn68xxp1 {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_8_63                : 56;
+	uint64_t pkt_drop_rid                 : 1;  /**< Entire packet dropped due to the lack of reassembly-ids or
+                                                         because ILK_RXX_CFG1[PKT_ENA]=0 */
+	uint64_t pkt_drop_rxf                 : 1;  /**< Some/all of a packet dropped due to RX_FIFO_CNT == RX_FIFO_MAX */
+	uint64_t lane_bad_word                : 1;  /**< A lane encountered either a bad 64B/67B codeword or an unknown
+                                                         control word type. */
+	uint64_t stat_cnt_ovfl                : 1;  /**< Statistics counter overflow */
+	uint64_t lane_align_done              : 1;  /**< Lane alignment successful */
+	uint64_t word_sync_done               : 1;  /**< All enabled lanes have achieved word boundary lock and
+                                                         scrambler synchronization.  Lane alignment may now be enabled. */
+	uint64_t crc24_err                    : 1;  /**< Burst CRC24 error.  All open packets will be receive an error. */
+	uint64_t lane_align_fail              : 1;  /**< Lane Alignment fails (4 tries) */
+#else
+	uint64_t lane_align_fail              : 1;
+	uint64_t crc24_err                    : 1;
+	uint64_t word_sync_done               : 1;
+	uint64_t lane_align_done              : 1;
+	uint64_t stat_cnt_ovfl                : 1;
+	uint64_t lane_bad_word                : 1;
+	uint64_t pkt_drop_rxf                 : 1;
+	uint64_t pkt_drop_rid                 : 1;
+	uint64_t reserved_8_63                : 56;
+#endif
+	} cn68xxp1;
+};
+typedef union cvmx_ilk_rxx_int_en cvmx_ilk_rxx_int_en_t;
+
+/**
+ * cvmx_ilk_rx#_jabber
+ */
+union cvmx_ilk_rxx_jabber {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_jabber_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t cnt                          : 16; /**< Byte count for jabber check. Failing packets are truncated to CNT bytes.
+                                                         Hardware tracks the size of up to two concurrent packets per link. If using segment mode
+                                                         with more than two channels, some large packets might not be flagged or truncated.
+                                                         CNT must be 8-byte aligned such that CNT[2:0] = 0x0. */
+#else
+	uint64_t cnt                          : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_jabber_s          cn68xx;
+	struct cvmx_ilk_rxx_jabber_s          cn68xxp1;
+	struct cvmx_ilk_rxx_jabber_s          cn78xx;
+};
+typedef union cvmx_ilk_rxx_jabber cvmx_ilk_rxx_jabber_t;
+
+/**
+ * cvmx_ilk_rx#_mem_cal0
+ *
+ * Notes:
+ * Software must program the calendar table prior to enabling the
+ * link.
+ *
+ * Software must always write ILK_RXx_MEM_CAL0 then ILK_RXx_MEM_CAL1.
+ * Software must never write them in reverse order or write one without
+ * writing the other.
+ *
+ * A given calendar table entry has no effect on PKO pipe
+ * backpressure when either:
+ *  - ENTRY_CTLx=Link (1), or
+ *  - ENTRY_CTLx=XON (3) and PORT_PIPEx is outside the range of ILK_TXx_PIPE[BASE/NUMP].
+ *
+ * Within the 8 calendar table entries of one IDX value, if more
+ * than one affects the same PKO pipe, XOFF always wins over XON,
+ * regardless of the calendar table order.
+ *
+ * Software must always read ILK_RXx_MEM_CAL0 then ILK_RXx_MEM_CAL1.  Software
+ * must never read them in reverse order or read one without reading the
+ * other.
+ */
+union cvmx_ilk_rxx_mem_cal0 {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_mem_cal0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_36_63               : 28;
+	uint64_t entry_ctl3                   : 2;  /**< XON/XOFF destination for entry (IDX*8)+3
+
+                                                         - 0: PKO port-pipe  Apply backpressure received from the
+                                                                            remote tranmitter to the PKO pipe selected
+                                                                            by PORT_PIPE3.
+
+                                                         - 1: Link           Apply the backpressure received from the
+                                                                            remote transmitter to link backpressure.
+                                                                            PORT_PIPE3 is unused.
+
+                                                         - 2: XOFF           Apply XOFF to the PKO pipe selected by
+                                                                            PORT_PIPE3.
+
+                                                         - 3: XON            Apply XON to the PKO pipe selected by
+                                                                            PORT_PIPE3. The calendar table entry is
+                                                                            effectively unused if PORT_PIPE3 is out of
+                                                                            range of ILK_TXx_PIPE[BASE/NUMP].
+
+                                                         This field applies to one of bits <52>, <44>, or <28> in the
+                                                         Interlaken control word. */
+	uint64_t port_pipe3                   : 7;  /**< Select PKO port-pipe for calendar table entry (IDX*8)+3
+
+                                                         PORT_PIPE3 must reside in the range of ILK_TXx_PIPE[BASE/NUMP]
+                                                         when ENTRY_CTL3 is "XOFF" (2) or "PKO port-pipe" (0). */
+	uint64_t entry_ctl2                   : 2;  /**< XON/XOFF destination for entry (IDX*8)+2
+
+                                                         - 0: PKO port-pipe  Apply backpressure received from the
+                                                                            remote tranmitter to the PKO pipe selected
+                                                                            by PORT_PIPE2.
+
+                                                         - 1: Link           Apply the backpressure received from the
+                                                                            remote transmitter to link backpressure.
+                                                                            PORT_PIPE2 is unused.
+
+                                                         - 2: XOFF           Apply XOFF to the PKO pipe selected by
+                                                                            PORT_PIPE2.
+
+                                                         - 3: XON            Apply XON to the PKO pipe selected by
+                                                                            PORT_PIPE2. The calendar table entry is
+                                                                            effectively unused if PORT_PIPE2 is out of
+                                                                            range of ILK_TXx_PIPE[BASE/NUMP].
+
+                                                         This field applies to one of bits <53>, <45>, or <29> in the
+                                                         Interlaken control word. */
+	uint64_t port_pipe2                   : 7;  /**< Select PKO port-pipe for calendar table entry (IDX*8)+2
+
+                                                         PORT_PIPE2 must reside in the range of ILK_TXx_PIPE[BASE/NUMP]
+                                                         when ENTRY_CTL2 is "XOFF" (2) or "PKO port-pipe" (0). */
+	uint64_t entry_ctl1                   : 2;  /**< XON/XOFF destination for entry (IDX*8)+1
+
+                                                         - 0: PKO port-pipe  Apply backpressure received from the
+                                                                            remote tranmitter to the PKO pipe selected
+                                                                            by PORT_PIPE1.
+
+                                                         - 1: Link           Apply the backpressure received from the
+                                                                            remote transmitter to link backpressure.
+                                                                            PORT_PIPE1 is unused.
+
+                                                         - 2: XOFF           Apply XOFF to the PKO pipe selected by
+                                                                            PORT_PIPE1.
+
+                                                         - 3: XON            Apply XON to the PKO pipe selected by
+                                                                            PORT_PIPE1. The calendar table entry is
+                                                                            effectively unused if PORT_PIPE1 is out of
+                                                                            range of ILK_TXx_PIPE[BASE/NUMP].
+
+                                                         This field applies to one of bits <54>, <46>, or <30> in the
+                                                         Interlaken control word. */
+	uint64_t port_pipe1                   : 7;  /**< Select PKO port-pipe for calendar table entry (IDX*8)+1
+
+                                                         PORT_PIPE1 must reside in the range of ILK_TXx_PIPE[BASE/NUMP]
+                                                         when ENTRY_CTL1 is "XOFF" (2) or "PKO port-pipe" (0). */
+	uint64_t entry_ctl0                   : 2;  /**< XON/XOFF destination for entry (IDX*8)+0
+
+                                                         - 0: PKO port-pipe  Apply backpressure received from the
+                                                                            remote tranmitter to the PKO pipe selected
+                                                                            by PORT_PIPE0.
+
+                                                         - 1: Link           Apply the backpressure received from the
+                                                                            remote transmitter to link backpressure.
+                                                                            PORT_PIPE0 is unused.
+
+                                                         - 2: XOFF           Apply XOFF to the PKO pipe selected by
+                                                                            PORT_PIPE0.
+
+                                                         - 3: XON            Apply XON to the PKO pipe selected by
+                                                                            PORT_PIPE0. The calendar table entry is
+                                                                            effectively unused if PORT_PIPEx is out of
+                                                                            range of ILK_TXx_PIPE[BASE/NUMP].
+
+                                                         This field applies to one of bits <55>, <47>, or <31> in the
+                                                         Interlaken control word. */
+	uint64_t port_pipe0                   : 7;  /**< Select PKO port-pipe for calendar table entry (IDX*8)+0
+
+                                                         PORT_PIPE0 must reside in the range of ILK_TXx_PIPE[BASE/NUMP]
+                                                         when ENTRY_CTL0 is "XOFF" (2) or "PKO port-pipe" (0). */
+#else
+	uint64_t port_pipe0                   : 7;
+	uint64_t entry_ctl0                   : 2;
+	uint64_t port_pipe1                   : 7;
+	uint64_t entry_ctl1                   : 2;
+	uint64_t port_pipe2                   : 7;
+	uint64_t entry_ctl2                   : 2;
+	uint64_t port_pipe3                   : 7;
+	uint64_t entry_ctl3                   : 2;
+	uint64_t reserved_36_63               : 28;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_mem_cal0_s        cn68xx;
+	struct cvmx_ilk_rxx_mem_cal0_s        cn68xxp1;
+};
+typedef union cvmx_ilk_rxx_mem_cal0 cvmx_ilk_rxx_mem_cal0_t;
+
+/**
+ * cvmx_ilk_rx#_mem_cal1
+ *
+ * Notes:
+ * Software must program the calendar table prior to enabling the
+ * link.
+ *
+ * Software must always write ILK_RXx_MEM_CAL0 then ILK_RXx_MEM_CAL1.
+ * Software must never write them in reverse order or write one without
+ * writing the other.
+ *
+ * A given calendar table entry has no effect on PKO pipe
+ * backpressure when either:
+ *  - ENTRY_CTLx=Link (1), or
+ *  - ENTRY_CTLx=XON (3) and PORT_PIPEx is outside the range of ILK_TXx_PIPE[BASE/NUMP].
+ *
+ * Within the 8 calendar table entries of one IDX value, if more
+ * than one affects the same PKO pipe, XOFF always wins over XON,
+ * regardless of the calendar table order.
+ *
+ * Software must always read ILK_RXx_MEM_CAL0 then ILK_Rx_MEM_CAL1.  Software
+ * must never read them in reverse order or read one without reading the
+ * other.
+ */
+union cvmx_ilk_rxx_mem_cal1 {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_mem_cal1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_36_63               : 28;
+	uint64_t entry_ctl7                   : 2;  /**< XON/XOFF destination for entry (IDX*8)+7
+
+                                                         - 0: PKO port-pipe  Apply backpressure received from the
+                                                                            remote tranmitter to the PKO pipe selected
+                                                                            by PORT_PIPE7.
+
+                                                         - 1: Link           Apply the backpressure received from the
+                                                                            remote transmitter to link backpressure.
+                                                                            PORT_PIPE7 is unused.
+
+                                                         - 2: XOFF           Apply XOFF to the PKO pipe selected by
+                                                                            PORT_PIPE7.
+
+                                                         - 3: XON            Apply XON to the PKO pipe selected by
+                                                                            PORT_PIPE7. The calendar table entry is
+                                                                            effectively unused if PORT_PIPE3 is out of
+                                                                            range of ILK_TXx_PIPE[BASE/NUMP].
+
+                                                         This field applies to one of bits <48>, <40>, or <24> in the
+                                                         Interlaken control word. */
+	uint64_t port_pipe7                   : 7;  /**< Select PKO port-pipe for calendar table entry (IDX*8)+7
+
+                                                         PORT_PIPE7 must reside in the range of ILK_TXx_PIPE[BASE/NUMP]
+                                                         when ENTRY_CTL7 is "XOFF" (2) or "PKO port-pipe" (0). */
+	uint64_t entry_ctl6                   : 2;  /**< XON/XOFF destination for entry (IDX*8)+6
+
+                                                         - 0: PKO port-pipe  Apply backpressure received from the
+                                                                            remote tranmitter to the PKO pipe selected
+                                                                            by PORT_PIPE6.
+
+                                                         - 1: Link           Apply the backpressure received from the
+                                                                            remote transmitter to link backpressure.
+                                                                            PORT_PIPE6 is unused.
+
+                                                         - 2: XOFF           Apply XOFF to the PKO pipe selected by
+                                                                            PORT_PIPE6.
+
+                                                         - 3: XON            Apply XON to the PKO pipe selected by
+                                                                            PORT_PIPE6. The calendar table entry is
+                                                                            effectively unused if PORT_PIPE6 is out of
+                                                                            range of ILK_TXx_PIPE[BASE/NUMP].
+
+                                                         This field applies to one of bits <49>, <41>, or <25> in the
+                                                         Interlaken control word. */
+	uint64_t port_pipe6                   : 7;  /**< Select PKO port-pipe for calendar table entry (IDX*8)+6
+
+                                                         PORT_PIPE6 must reside in the range of ILK_TXx_PIPE[BASE/NUMP]
+                                                         when ENTRY_CTL6 is "XOFF" (2) or "PKO port-pipe" (0). */
+	uint64_t entry_ctl5                   : 2;  /**< XON/XOFF destination for entry (IDX*8)+5
+
+                                                         - 0: PKO port-pipe  Apply backpressure received from the
+                                                                            remote tranmitter to the PKO pipe selected
+                                                                            by PORT_PIPE5.
+
+                                                         - 1: Link           Apply the backpressure received from the
+                                                                            remote transmitter to link backpressure.
+                                                                            PORT_PIPE5 is unused.
+
+                                                         - 2: XOFF           Apply XOFF to the PKO pipe selected by
+                                                                            PORT_PIPE5.
+
+                                                         - 3: XON            Apply XON to the PKO pipe selected by
+                                                                            PORT_PIPE5. The calendar table entry is
+                                                                            effectively unused if PORT_PIPE5 is out of
+                                                                            range of ILK_TXx_PIPE[BASE/NUMP].
+
+                                                         This field applies to one of bits <50>, <42>, or <26> in the
+                                                         Interlaken control word. */
+	uint64_t port_pipe5                   : 7;  /**< Select PKO port-pipe for calendar table entry (IDX*8)+5
+
+                                                         PORT_PIPE5 must reside in the range of ILK_TXx_PIPE[BASE/NUMP]
+                                                         when ENTRY_CTL5 is "XOFF" (2) or "PKO port-pipe" (0). */
+	uint64_t entry_ctl4                   : 2;  /**< XON/XOFF destination for entry (IDX*8)+4
+
+                                                         - 0: PKO port-pipe  Apply backpressure received from the
+                                                                            remote tranmitter to the PKO pipe selected
+                                                                            by PORT_PIPE4.
+
+                                                         - 1: Link           Apply the backpressure received from the
+                                                                            remote transmitter to link backpressure.
+                                                                            PORT_PIPE4 is unused.
+
+                                                         - 2: XOFF           Apply XOFF to the PKO pipe selected by
+                                                                            PORT_PIPE4.
+
+                                                         - 3: XON            Apply XON to the PKO pipe selected by
+                                                                            PORT_PIPE4. The calendar table entry is
+                                                                            effectively unused if PORT_PIPE4 is out of
+                                                                            range of ILK_TXx_PIPE[BASE/NUMP].
+
+                                                         This field applies to one of bits <51>, <43>, or <27> in the
+                                                         Interlaken control word. */
+	uint64_t port_pipe4                   : 7;  /**< Select PKO port-pipe for calendar table entry (IDX*8)+4
+
+                                                         PORT_PIPE4 must reside in the range of ILK_TXx_PIPE[BASE/NUMP]
+                                                         when ENTRY_CTL4 is "XOFF" (2) or "PKO port-pipe" (0). */
+#else
+	uint64_t port_pipe4                   : 7;
+	uint64_t entry_ctl4                   : 2;
+	uint64_t port_pipe5                   : 7;
+	uint64_t entry_ctl5                   : 2;
+	uint64_t port_pipe6                   : 7;
+	uint64_t entry_ctl6                   : 2;
+	uint64_t port_pipe7                   : 7;
+	uint64_t entry_ctl7                   : 2;
+	uint64_t reserved_36_63               : 28;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_mem_cal1_s        cn68xx;
+	struct cvmx_ilk_rxx_mem_cal1_s        cn68xxp1;
+};
+typedef union cvmx_ilk_rxx_mem_cal1 cvmx_ilk_rxx_mem_cal1_t;
+
+/**
+ * cvmx_ilk_rx#_mem_stat0
+ */
+union cvmx_ilk_rxx_mem_stat0 {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_mem_stat0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_28_63               : 36;
+	uint64_t rx_pkt                       : 28; /**< Number of packets received (256M)
+                                                         Channel selected by ILK_RXx_IDX_STAT0[IDX].  Saturates.
+                                                         Interrupt on saturation if ILK_RXX_INT_EN[STAT_CNT_OVFL]=1. */
+#else
+	uint64_t rx_pkt                       : 28;
+	uint64_t reserved_28_63               : 36;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_mem_stat0_s       cn68xx;
+	struct cvmx_ilk_rxx_mem_stat0_s       cn68xxp1;
+};
+typedef union cvmx_ilk_rxx_mem_stat0 cvmx_ilk_rxx_mem_stat0_t;
+
+/**
+ * cvmx_ilk_rx#_mem_stat1
+ */
+union cvmx_ilk_rxx_mem_stat1 {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_mem_stat1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_36_63               : 28;
+	uint64_t rx_bytes                     : 36; /**< Number of bytes received (64GB)
+                                                         Channel selected by ILK_RXx_IDX_STAT1[IDX].    Saturates.
+                                                         Interrupt on saturation if ILK_RXX_INT_EN[STAT_CNT_OVFL]=1. */
+#else
+	uint64_t rx_bytes                     : 36;
+	uint64_t reserved_36_63               : 28;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_mem_stat1_s       cn68xx;
+	struct cvmx_ilk_rxx_mem_stat1_s       cn68xxp1;
+};
+typedef union cvmx_ilk_rxx_mem_stat1 cvmx_ilk_rxx_mem_stat1_t;
+
+/**
+ * cvmx_ilk_rx#_pkt_cnt#
+ */
+union cvmx_ilk_rxx_pkt_cntx {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_pkt_cntx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_34_63               : 30;
+	uint64_t rx_pkt                       : 34; /**< Number of packets received per channel. Wraps on overflow. On overflow, sets
+                                                         ILK_RX()_INT[STAT_CNT_OVFL]. */
+#else
+	uint64_t rx_pkt                       : 34;
+	uint64_t reserved_34_63               : 30;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_pkt_cntx_s        cn78xx;
+};
+typedef union cvmx_ilk_rxx_pkt_cntx cvmx_ilk_rxx_pkt_cntx_t;
+
+/**
+ * cvmx_ilk_rx#_rid
+ */
+union cvmx_ilk_rxx_rid {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_rid_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_7_63                : 57;
+	uint64_t max_cnt                      : 7;  /**< Maximum number of reassembly IDs allowed for a given link. If an SOP arrives and the link
+                                                         has already allocated at least MAX_CNT reassembly IDs, the packet is dropped.
+                                                         An SOP allocates a reassembly ID; an EOP frees a reassembly ID. */
+#else
+	uint64_t max_cnt                      : 7;
+	uint64_t reserved_7_63                : 57;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_rid_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_6_63                : 58;
+	uint64_t max_cnt                      : 6;  /**< Maximum number of reassembly-ids allowed for a given link.  If
+                                                         an SOP arrives and the link has already allocated at least
+                                                         MAX_CNT reassembly-ids, the packet will be dropped.
+
+                                                         Note: An an SOP allocates a reassembly-ids.
+                                                         Note: An an EOP frees a reassembly-ids.
+
+                                                         ***NOTE: Added in pass 2.0 */
+#else
+	uint64_t max_cnt                      : 6;
+	uint64_t reserved_6_63                : 58;
+#endif
+	} cn68xx;
+	struct cvmx_ilk_rxx_rid_s             cn78xx;
+};
+typedef union cvmx_ilk_rxx_rid cvmx_ilk_rxx_rid_t;
+
+/**
+ * cvmx_ilk_rx#_stat0
+ */
+union cvmx_ilk_rxx_stat0 {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_stat0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_35_63               : 29;
+	uint64_t crc24_match_cnt              : 35; /**< Indicates the number of CRC24 matches received. Wraps on overflow if
+                                                         ILK_RX()_CFG0[LNK_STATS_WRAP]=1. Otherwise, saturates. On overflow/saturate, sets
+                                                         ILK_RX()_INT[STAT_CNT_OVFL]. */
+#else
+	uint64_t crc24_match_cnt              : 35;
+	uint64_t reserved_35_63               : 29;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_stat0_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_33_63               : 31;
+	uint64_t crc24_match_cnt              : 33; /**< Number of CRC24 matches received.  Saturates.  Interrupt on
+                                                         saturation if ILK_RXX_INT_EN[STAT_CNT_OVFL]=1. */
+#else
+	uint64_t crc24_match_cnt              : 33;
+	uint64_t reserved_33_63               : 31;
+#endif
+	} cn68xx;
+	struct cvmx_ilk_rxx_stat0_cn68xxp1 {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_27_63               : 37;
+	uint64_t crc24_match_cnt              : 27; /**< Number of CRC24 matches received.  Saturates.  Interrupt on
+                                                         saturation if ILK_RXX_INT_EN[STAT_CNT_OVFL]=1. */
+#else
+	uint64_t crc24_match_cnt              : 27;
+	uint64_t reserved_27_63               : 37;
+#endif
+	} cn68xxp1;
+	struct cvmx_ilk_rxx_stat0_s           cn78xx;
+};
+typedef union cvmx_ilk_rxx_stat0 cvmx_ilk_rxx_stat0_t;
+
+/**
+ * cvmx_ilk_rx#_stat1
+ */
+union cvmx_ilk_rxx_stat1 {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_stat1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_20_63               : 44;
+	uint64_t crc24_err_cnt                : 20; /**< Indicates the number of bursts with a detected CRC error. Wraps on overflow if
+                                                         ILK_RX()_CFG0[LNK_STATS_WRAP]=1. Otherwise, saturates. On overflow/saturate, sets
+                                                         ILK_RX()_INT[STAT_CNT_OVFL]. */
+#else
+	uint64_t crc24_err_cnt                : 20;
+	uint64_t reserved_20_63               : 44;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_stat1_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_18_63               : 46;
+	uint64_t crc24_err_cnt                : 18; /**< Number of bursts with a detected CRC error.  Saturates.
+                                                         Interrupt on saturation if ILK_RXX_INT_EN[STAT_CNT_OVFL]=1. */
+#else
+	uint64_t crc24_err_cnt                : 18;
+	uint64_t reserved_18_63               : 46;
+#endif
+	} cn68xx;
+	struct cvmx_ilk_rxx_stat1_cn68xx      cn68xxp1;
+	struct cvmx_ilk_rxx_stat1_s           cn78xx;
+};
+typedef union cvmx_ilk_rxx_stat1 cvmx_ilk_rxx_stat1_t;
+
+/**
+ * cvmx_ilk_rx#_stat2
+ */
+union cvmx_ilk_rxx_stat2 {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_stat2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_50_63               : 14;
+	uint64_t brst_not_full_cnt            : 18; /**< Indicates the number of bursts received that terminated without an EOP and contained fewer
+                                                         than BurstMax words. Wraps on overflow if ILK_RX()_CFG0[LNK_STATS_WRAP]=1. Otherwise,
+                                                         saturates. On overflow/saturate, sets ILK_RX()_INT[STAT_CNT_OVFL]. */
+	uint64_t reserved_30_31               : 2;
+	uint64_t brst_cnt                     : 30; /**< Indicates the number of bursts correctly received. (i.e. good CRC24, not in violation of
+                                                         BurstMax or BurstShort). Wraps on overflow if ILK_RX()_CFG0[LNK_STATS_WRAP]=1.
+                                                         Otherwise, saturates. On overflow/saturate, sets ILK_RX()_INT[STAT_CNT_OVFL]. */
+#else
+	uint64_t brst_cnt                     : 30;
+	uint64_t reserved_30_31               : 2;
+	uint64_t brst_not_full_cnt            : 18;
+	uint64_t reserved_50_63               : 14;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_stat2_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t brst_not_full_cnt            : 16; /**< Number of bursts received which terminated without an eop and
+                                                         contained fewer than BurstMax words.  Saturates.  Interrupt on
+                                                         saturation if ILK_RXX_INT_EN[STAT_CNT_OVFL]=1. */
+	uint64_t reserved_28_31               : 4;
+	uint64_t brst_cnt                     : 28; /**< Number of bursts correctly received. (ie. good CRC24, not in
+                                                         violation of BurstMax or BurstShort) */
+#else
+	uint64_t brst_cnt                     : 28;
+	uint64_t reserved_28_31               : 4;
+	uint64_t brst_not_full_cnt            : 16;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} cn68xx;
+	struct cvmx_ilk_rxx_stat2_cn68xxp1 {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t brst_not_full_cnt            : 16; /**< Number of bursts received which terminated without an eop and
+                                                         contained fewer than BurstMax words.  Saturates.  Interrupt on
+                                                         saturation if ILK_RXX_INT_EN[STAT_CNT_OVFL]=1. */
+	uint64_t reserved_16_31               : 16;
+	uint64_t brst_cnt                     : 16; /**< Number of bursts correctly received. (ie. good CRC24, not in
+                                                         violation of BurstMax or BurstShort) */
+#else
+	uint64_t brst_cnt                     : 16;
+	uint64_t reserved_16_31               : 16;
+	uint64_t brst_not_full_cnt            : 16;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} cn68xxp1;
+	struct cvmx_ilk_rxx_stat2_s           cn78xx;
+};
+typedef union cvmx_ilk_rxx_stat2 cvmx_ilk_rxx_stat2_t;
+
+/**
+ * cvmx_ilk_rx#_stat3
+ */
+union cvmx_ilk_rxx_stat3 {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_stat3_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_18_63               : 46;
+	uint64_t brst_max_err_cnt             : 18; /**< Indicates the number of bursts received longer than the BurstMax parameter. Wraps on
+                                                         overflow if ILK_RX()_CFG0[LNK_STATS_WRAP]=1. Otherwise, saturates. On
+                                                         overflow/saturate, sets ILK_RX()_INT[STAT_CNT_OVFL]. */
+#else
+	uint64_t brst_max_err_cnt             : 18;
+	uint64_t reserved_18_63               : 46;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_stat3_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t brst_max_err_cnt             : 16; /**< Number of bursts received longer than the BurstMax parameter */
+#else
+	uint64_t brst_max_err_cnt             : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} cn68xx;
+	struct cvmx_ilk_rxx_stat3_cn68xx      cn68xxp1;
+	struct cvmx_ilk_rxx_stat3_s           cn78xx;
+};
+typedef union cvmx_ilk_rxx_stat3 cvmx_ilk_rxx_stat3_t;
+
+/**
+ * cvmx_ilk_rx#_stat4
+ */
+union cvmx_ilk_rxx_stat4 {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_stat4_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_18_63               : 46;
+	uint64_t brst_shrt_err_cnt            : 18; /**< Indicates the number of bursts received that violate the BurstShort parameter. Wraps on
+                                                         overflow if ILK_RX()_CFG0[LNK_STATS_WRAP]=1. Otherwise, saturates. On
+                                                         overflow/saturate, sets ILK_RX()_INT[STAT_CNT_OVFL]. */
+#else
+	uint64_t brst_shrt_err_cnt            : 18;
+	uint64_t reserved_18_63               : 46;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_stat4_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t brst_shrt_err_cnt            : 16; /**< Number of bursts received that violate the BurstShort
+                                                         parameter.  Saturates.  Interrupt on saturation if
+                                                         ILK_RXX_INT_EN[STAT_CNT_OVFL]=1. */
+#else
+	uint64_t brst_shrt_err_cnt            : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} cn68xx;
+	struct cvmx_ilk_rxx_stat4_cn68xx      cn68xxp1;
+	struct cvmx_ilk_rxx_stat4_s           cn78xx;
+};
+typedef union cvmx_ilk_rxx_stat4 cvmx_ilk_rxx_stat4_t;
+
+/**
+ * cvmx_ilk_rx#_stat5
+ */
+union cvmx_ilk_rxx_stat5 {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_stat5_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_25_63               : 39;
+	uint64_t align_cnt                    : 25; /**< Indicates the number of alignment sequences received (i.e. those that do not violate the
+                                                         current alignment). Wraps on overflow if ILK_RX()_CFG0[LNK_STATS_WRAP]=1. Otherwise,
+                                                         saturates. On overflow/saturate, sets ILK_RX()_INT[STAT_CNT_OVFL]. */
+#else
+	uint64_t align_cnt                    : 25;
+	uint64_t reserved_25_63               : 39;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_stat5_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_23_63               : 41;
+	uint64_t align_cnt                    : 23; /**< Number of alignment sequences received  (ie. those that do not
+                                                         violate the current alignment).  Saturates.  Interrupt on
+                                                         saturation if ILK_RXX_INT_EN[STAT_CNT_OVFL]=1. */
+#else
+	uint64_t align_cnt                    : 23;
+	uint64_t reserved_23_63               : 41;
+#endif
+	} cn68xx;
+	struct cvmx_ilk_rxx_stat5_cn68xxp1 {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t align_cnt                    : 16; /**< Number of alignment sequences received  (ie. those that do not
+                                                         violate the current alignment).  Saturates.  Interrupt on
+                                                         saturation if ILK_RXX_INT_EN[STAT_CNT_OVFL]=1. */
+#else
+	uint64_t align_cnt                    : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} cn68xxp1;
+	struct cvmx_ilk_rxx_stat5_s           cn78xx;
+};
+typedef union cvmx_ilk_rxx_stat5 cvmx_ilk_rxx_stat5_t;
+
+/**
+ * cvmx_ilk_rx#_stat6
+ */
+union cvmx_ilk_rxx_stat6 {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_stat6_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_18_63               : 46;
+	uint64_t align_err_cnt                : 18; /**< Indicates the number of alignment sequences received in error (i.e. those that violate the
+                                                         current alignment). Wraps on overflow if ILK_RX()_CFG0[LNK_STATS_WRAP]=1. Otherwise,
+                                                         saturates. On overflow/saturate, sets ILK_RX()_INT[STAT_CNT_OVFL]. */
+#else
+	uint64_t align_err_cnt                : 18;
+	uint64_t reserved_18_63               : 46;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_stat6_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t align_err_cnt                : 16; /**< Number of alignment sequences received in error (ie. those that
+                                                         violate the current alignment).  Saturates.  Interrupt on
+                                                         saturation if ILK_RXX_INT_EN[STAT_CNT_OVFL]=1. */
+#else
+	uint64_t align_err_cnt                : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} cn68xx;
+	struct cvmx_ilk_rxx_stat6_cn68xx      cn68xxp1;
+	struct cvmx_ilk_rxx_stat6_s           cn78xx;
+};
+typedef union cvmx_ilk_rxx_stat6 cvmx_ilk_rxx_stat6_t;
+
+/**
+ * cvmx_ilk_rx#_stat7
+ */
+union cvmx_ilk_rxx_stat7 {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_stat7_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_18_63               : 46;
+	uint64_t bad_64b67b_cnt               : 18; /**< Indicates the number of bad 64B/67B code words.Wraps on overflow if
+                                                         ILK_RX()_CFG0[LNK_STATS_WRAP]=1. Otherwise, saturates. On overflow/saturate, sets
+                                                         ILK_RX()_INT[STAT_CNT_OVFL]. */
+#else
+	uint64_t bad_64b67b_cnt               : 18;
+	uint64_t reserved_18_63               : 46;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_stat7_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t bad_64b67b_cnt               : 16; /**< Number of bad 64B/67B codewords.  Saturates.  Interrupt on
+                                                         saturation if ILK_RXX_INT_EN[STAT_CNT_OVFL]=1. */
+#else
+	uint64_t bad_64b67b_cnt               : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} cn68xx;
+	struct cvmx_ilk_rxx_stat7_cn68xx      cn68xxp1;
+	struct cvmx_ilk_rxx_stat7_s           cn78xx;
+};
+typedef union cvmx_ilk_rxx_stat7 cvmx_ilk_rxx_stat7_t;
+
+/**
+ * cvmx_ilk_rx#_stat8
+ */
+union cvmx_ilk_rxx_stat8 {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_stat8_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_32_63               : 32;
+	uint64_t pkt_drop_rid_cnt             : 16; /**< Indicates the number of packets dropped due to the lack of reassembly IDs or because
+                                                         ILK_RX()_CFG1[PKT_ENA] = 0. Wraps on overflow if ILK_RX()_CFG0[LNK_STATS_WRAP]=1.
+                                                         Otherwise, saturates. On overflow/saturate, sets ILK_RX()_INT[STAT_CNT_OVFL]. */
+	uint64_t pkt_drop_rxf_cnt             : 16; /**< Indicates the number of packets dropped due to RX_FIFO_CNT >= RX_FIFO_MAX. Wraps on
+                                                         overflow if ILK_RX()_CFG0[LNK_STATS_WRAP]=1.Otherwise, saturates. On
+                                                         overflow/saturate, sets ILK_RX()_INT[STAT_CNT_OVFL]. */
+#else
+	uint64_t pkt_drop_rxf_cnt             : 16;
+	uint64_t pkt_drop_rid_cnt             : 16;
+	uint64_t reserved_32_63               : 32;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_stat8_s           cn68xx;
+	struct cvmx_ilk_rxx_stat8_s           cn68xxp1;
+	struct cvmx_ilk_rxx_stat8_s           cn78xx;
+};
+typedef union cvmx_ilk_rxx_stat8 cvmx_ilk_rxx_stat8_t;
+
+/**
+ * cvmx_ilk_rx#_stat9
+ *
+ * This register is reserved.
+ *
+ */
+union cvmx_ilk_rxx_stat9 {
+	uint64_t u64;
+	struct cvmx_ilk_rxx_stat9_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_0_63                : 64;
+#else
+	uint64_t reserved_0_63                : 64;
+#endif
+	} s;
+	struct cvmx_ilk_rxx_stat9_s           cn68xx;
+	struct cvmx_ilk_rxx_stat9_s           cn68xxp1;
+	struct cvmx_ilk_rxx_stat9_s           cn78xx;
+};
+typedef union cvmx_ilk_rxx_stat9 cvmx_ilk_rxx_stat9_t;
+
+/**
+ * cvmx_ilk_rx_lne#_cfg
+ */
+union cvmx_ilk_rx_lnex_cfg {
+	uint64_t u64;
+	struct cvmx_ilk_rx_lnex_cfg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t rx_dis_psh_skip              : 1;  /**< When asserted, skip words are discarded in the lane logic; when deasserted, skip words are
+                                                         destripped.
+                                                         If the lane is in internal loopback mode, this field is ignored and skip words are always
+                                                         discarded in the lane logic. */
+	uint64_t reserved_7_7                 : 1;
+	uint64_t rx_dis_disp_chk              : 1;  /**< Disable the RX disparity check, see ILK_RX_LNE()_INT[DISP_ERR]. */
+	uint64_t rx_scrm_sync                 : 1;  /**< RX scrambler-synchronization status. A 1 means synchronization has been achieved. */
+	uint64_t rx_bdry_sync                 : 1;  /**< RX word-boundary-synchronization status. A 1 means synchronization has been achieved */
+	uint64_t rx_dis_ukwn                  : 1;  /**< Disable normal response to unknown words. Unknown words are still logged but do not cause
+                                                         an error to all open channels. */
+	uint64_t rx_dis_scram                 : 1;  /**< Disable lane scrambler. For diagnostic use only. */
+	uint64_t stat_rdclr                   : 1;  /**< A CSR read operation to ILK_RX_LNEn_STAT* clears the selected counter after returning its
+                                                         current value. */
+	uint64_t stat_ena                     : 1;  /**< Enable RX lane statistics counters. */
+#else
+	uint64_t stat_ena                     : 1;
+	uint64_t stat_rdclr                   : 1;
+	uint64_t rx_dis_scram                 : 1;
+	uint64_t rx_dis_ukwn                  : 1;
+	uint64_t rx_bdry_sync                 : 1;
+	uint64_t rx_scrm_sync                 : 1;
+	uint64_t rx_dis_disp_chk              : 1;
+	uint64_t reserved_7_7                 : 1;
+	uint64_t rx_dis_psh_skip              : 1;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_ilk_rx_lnex_cfg_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t rx_dis_psh_skip              : 1;  /**< When RX_DIS_PSH_SKIP=0, skip words are de-stripped.
+                                                         When RX_DIS_PSH_SKIP=1, skip words are discarded in the lane
+                                                         logic.
+
+                                                         If the lane is in internal loopback mode, RX_DIS_PSH_SKIP
+                                                         is ignored and skip words are always discarded in the lane
+                                                         logic.
+
+                                                         ***NOTE: Added in pass 2.0 */
+	uint64_t reserved_6_7                 : 2;
+	uint64_t rx_scrm_sync                 : 1;  /**< Rx scrambler synchronization status
+                                                         '1' means synchronization achieved
+
+                                                         ***NOTE: Added in pass 2.0 */
+	uint64_t rx_bdry_sync                 : 1;  /**< Rx word boundary sync status
+                                                         '1' means synchronization achieved */
+	uint64_t rx_dis_ukwn                  : 1;  /**< Disable normal response to unknown words.  They are still
+                                                         logged but do not cause an error to all open channels. */
+	uint64_t rx_dis_scram                 : 1;  /**< Disable lane scrambler (debug) */
+	uint64_t stat_rdclr                   : 1;  /**< CSR read to ILK_RX_LNEx_STAT* clears the selected counter after
+                                                         returning its current value. */
+	uint64_t stat_ena                     : 1;  /**< Enable RX lane statistics counters */
+#else
+	uint64_t stat_ena                     : 1;
+	uint64_t stat_rdclr                   : 1;
+	uint64_t rx_dis_scram                 : 1;
+	uint64_t rx_dis_ukwn                  : 1;
+	uint64_t rx_bdry_sync                 : 1;
+	uint64_t rx_scrm_sync                 : 1;
+	uint64_t reserved_6_7                 : 2;
+	uint64_t rx_dis_psh_skip              : 1;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} cn68xx;
+	struct cvmx_ilk_rx_lnex_cfg_cn68xxp1 {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_5_63                : 59;
+	uint64_t rx_bdry_sync                 : 1;  /**< Rx word boundary sync status
+                                                         '1' means synchronization achieved */
+	uint64_t rx_dis_ukwn                  : 1;  /**< Disable normal response to unknown words.  They are still
+                                                         logged but do not cause an error to all open channels */
+	uint64_t rx_dis_scram                 : 1;  /**< Disable lane scrambler (debug) */
+	uint64_t stat_rdclr                   : 1;  /**< CSR read to ILK_RX_LNEx_STAT* clears the selected counter after
+                                                         returning its current value. */
+	uint64_t stat_ena                     : 1;  /**< Enable RX lane statistics counters */
+#else
+	uint64_t stat_ena                     : 1;
+	uint64_t stat_rdclr                   : 1;
+	uint64_t rx_dis_scram                 : 1;
+	uint64_t rx_dis_ukwn                  : 1;
+	uint64_t rx_bdry_sync                 : 1;
+	uint64_t reserved_5_63                : 59;
+#endif
+	} cn68xxp1;
+	struct cvmx_ilk_rx_lnex_cfg_s         cn78xx;
+};
+typedef union cvmx_ilk_rx_lnex_cfg cvmx_ilk_rx_lnex_cfg_t;
+
+/**
+ * cvmx_ilk_rx_lne#_int
+ */
+union cvmx_ilk_rx_lnex_int {
+	uint64_t u64;
+	struct cvmx_ilk_rx_lnex_int_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_10_63               : 54;
+	uint64_t disp_err                     : 1;  /**< RX disparity error encountered. Throws ILK_INTSN_E::ILK_RXLNE()_DISP_ERR. */
+	uint64_t bad_64b67b                   : 1;  /**< Bad 64B/67B code word encountered. Once the bad word reaches the burst control unit (as
+                                                         denoted by ILK_RX()_INT[LANE_BAD_WORD]) it is discarded and all open packets receive
+                                                         an error. Throws ILK_INTSN_E::ILK_RXLNE()_BAD_64B67B. */
+	uint64_t stat_cnt_ovfl                : 1;  /**< Rx lane statistic counter overflow. Throws ILK_INTSN_E::ILK_RXLNE()_STAT_CNT_OVFL. */
+	uint64_t stat_msg                     : 1;  /**< Status bits for the link or a lane transitioned from a 1 (healthy) to a 0 (problem).
+                                                         Throws ILK_INTSN_E::ILK_RXLNE()_STAT_MSG. */
+	uint64_t dskew_fifo_ovfl              : 1;  /**< RX deskew FIFO overflow occurred. Throws ILK_INTSN_E::ILK_RXLNE()_DSKEW_FIFO_OVFL. */
+	uint64_t scrm_sync_loss               : 1;  /**< Four consecutive bad sync words or three consecutive scramble state mismatches. Throws
+                                                         ILK_INTSN_E::ILK_RXLNE()_SCRM_SYNC_LOSS. */
+	uint64_t ukwn_cntl_word               : 1;  /**< Unknown framing control word. Block type does not match any of (SYNC,SCRAM,SKIP,DIAG).
+                                                         Throws ILK_INTSN_E::ILK_RXLNE()_UKWN_CNTL_WORD. */
+	uint64_t crc32_err                    : 1;  /**< Diagnostic CRC32 errors. Throws ILK_INTSN_E::ILK_RXLNE()_CRC32_ERR. */
+	uint64_t bdry_sync_loss               : 1;  /**< RX logic loses word boundary sync (16 tries). Hardware will automatically attempt to
+                                                         regain word boundary sync. Throws ILK_INTSN_E::ILK_RXLNE()_BDRY_SYNC_LOSS. */
+	uint64_t serdes_lock_loss             : 1;  /**< RX SerDes loses lock. Throws ILK_INTSN_E::ILK_RXLNE()_SERDES_LOCK_LOSS. */
+#else
+	uint64_t serdes_lock_loss             : 1;
+	uint64_t bdry_sync_loss               : 1;
+	uint64_t crc32_err                    : 1;
+	uint64_t ukwn_cntl_word               : 1;
+	uint64_t scrm_sync_loss               : 1;
+	uint64_t dskew_fifo_ovfl              : 1;
+	uint64_t stat_msg                     : 1;
+	uint64_t stat_cnt_ovfl                : 1;
+	uint64_t bad_64b67b                   : 1;
+	uint64_t disp_err                     : 1;
+	uint64_t reserved_10_63               : 54;
+#endif
+	} s;
+	struct cvmx_ilk_rx_lnex_int_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t bad_64b67b                   : 1;  /**< Bad 64B/67B codeword encountered.  Once the bad word reaches
+                                                         the burst control unit (as deonted by
+                                                         ILK_RXx_INT[LANE_BAD_WORD]) it will be tossed and all open
+                                                         packets will receive an error. */
+	uint64_t stat_cnt_ovfl                : 1;  /**< Rx lane statistic counter overflow */
+	uint64_t stat_msg                     : 1;  /**< Status bits for the link or a lane transitioned from a '1'
+                                                         (healthy) to a '0' (problem) */
+	uint64_t dskew_fifo_ovfl              : 1;  /**< Rx deskew fifo overflow occurred. */
+	uint64_t scrm_sync_loss               : 1;  /**< 4 consecutive bad sync words or 3 consecutive scramble state
+                                                         mismatches */
+	uint64_t ukwn_cntl_word               : 1;  /**< Unknown framing control word. Block type does not match any of
+                                                         (SYNC,SCRAM,SKIP,DIAG) */
+	uint64_t crc32_err                    : 1;  /**< Diagnostic CRC32 errors */
+	uint64_t bdry_sync_loss               : 1;  /**< Rx logic loses word boundary sync (16 tries).  Hardware will
+                                                         automatically attempt to regain word boundary sync */
+	uint64_t serdes_lock_loss             : 1;  /**< Rx SERDES loses lock */
+#else
+	uint64_t serdes_lock_loss             : 1;
+	uint64_t bdry_sync_loss               : 1;
+	uint64_t crc32_err                    : 1;
+	uint64_t ukwn_cntl_word               : 1;
+	uint64_t scrm_sync_loss               : 1;
+	uint64_t dskew_fifo_ovfl              : 1;
+	uint64_t stat_msg                     : 1;
+	uint64_t stat_cnt_ovfl                : 1;
+	uint64_t bad_64b67b                   : 1;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} cn68xx;
+	struct cvmx_ilk_rx_lnex_int_cn68xx    cn68xxp1;
+	struct cvmx_ilk_rx_lnex_int_s         cn78xx;
+};
+typedef union cvmx_ilk_rx_lnex_int cvmx_ilk_rx_lnex_int_t;
+
+/**
+ * cvmx_ilk_rx_lne#_int_en
+ */
+union cvmx_ilk_rx_lnex_int_en {
+	uint64_t u64;
+	struct cvmx_ilk_rx_lnex_int_en_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t bad_64b67b                   : 1;  /**< Bad 64B/67B codeword encountered.  Once the bad word reaches
+                                                         the burst control unit (as deonted by
+                                                         ILK_RXx_INT[LANE_BAD_WORD]) it will be tossed and all open
+                                                         packets will receive an error. */
+	uint64_t stat_cnt_ovfl                : 1;  /**< Rx lane statistic counter overflow */
+	uint64_t stat_msg                     : 1;  /**< Status bits for the link or a lane transitioned from a '1'
+                                                         (healthy) to a '0' (problem) */
+	uint64_t dskew_fifo_ovfl              : 1;  /**< Rx deskew fifo overflow occurred. */
+	uint64_t scrm_sync_loss               : 1;  /**< 4 consecutive bad sync words or 3 consecutive scramble state
+                                                         mismatches */
+	uint64_t ukwn_cntl_word               : 1;  /**< Unknown framing control word. Block type does not match any of
+                                                         (SYNC,SCRAM,SKIP,DIAG) */
+	uint64_t crc32_err                    : 1;  /**< Diagnostic CRC32 error */
+	uint64_t bdry_sync_loss               : 1;  /**< Rx logic loses word boundary sync (16 tries).  Hardware will
+                                                         automatically attempt to regain word boundary sync */
+	uint64_t serdes_lock_loss             : 1;  /**< Rx SERDES loses lock */
+#else
+	uint64_t serdes_lock_loss             : 1;
+	uint64_t bdry_sync_loss               : 1;
+	uint64_t crc32_err                    : 1;
+	uint64_t ukwn_cntl_word               : 1;
+	uint64_t scrm_sync_loss               : 1;
+	uint64_t dskew_fifo_ovfl              : 1;
+	uint64_t stat_msg                     : 1;
+	uint64_t stat_cnt_ovfl                : 1;
+	uint64_t bad_64b67b                   : 1;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_ilk_rx_lnex_int_en_s      cn68xx;
+	struct cvmx_ilk_rx_lnex_int_en_s      cn68xxp1;
+};
+typedef union cvmx_ilk_rx_lnex_int_en cvmx_ilk_rx_lnex_int_en_t;
+
+/**
+ * cvmx_ilk_rx_lne#_stat0
+ */
+union cvmx_ilk_rx_lnex_stat0 {
+	uint64_t u64;
+	struct cvmx_ilk_rx_lnex_stat0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_18_63               : 46;
+	uint64_t ser_lock_loss_cnt            : 18; /**< Indicates the number of times the lane lost clock-data-recovery. On overflow, saturates
+                                                         and sets ILK_RX_LNE()_INT[STAT_CNT_OVFL]. */
+#else
+	uint64_t ser_lock_loss_cnt            : 18;
+	uint64_t reserved_18_63               : 46;
+#endif
+	} s;
+	struct cvmx_ilk_rx_lnex_stat0_s       cn68xx;
+	struct cvmx_ilk_rx_lnex_stat0_s       cn68xxp1;
+	struct cvmx_ilk_rx_lnex_stat0_s       cn78xx;
+};
+typedef union cvmx_ilk_rx_lnex_stat0 cvmx_ilk_rx_lnex_stat0_t;
+
+/**
+ * cvmx_ilk_rx_lne#_stat1
+ */
+union cvmx_ilk_rx_lnex_stat1 {
+	uint64_t u64;
+	struct cvmx_ilk_rx_lnex_stat1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_18_63               : 46;
+	uint64_t bdry_sync_loss_cnt           : 18; /**< Indicates the number of times a lane lost word-boundary synchronization. On overflow,
+                                                         saturates and sets ILK_RX_LNE()_INT[STAT_CNT_OVFL]. */
+#else
+	uint64_t bdry_sync_loss_cnt           : 18;
+	uint64_t reserved_18_63               : 46;
+#endif
+	} s;
+	struct cvmx_ilk_rx_lnex_stat1_s       cn68xx;
+	struct cvmx_ilk_rx_lnex_stat1_s       cn68xxp1;
+	struct cvmx_ilk_rx_lnex_stat1_s       cn78xx;
+};
+typedef union cvmx_ilk_rx_lnex_stat1 cvmx_ilk_rx_lnex_stat1_t;
+
+/**
+ * cvmx_ilk_rx_lne#_stat10
+ */
+union cvmx_ilk_rx_lnex_stat10 {
+	uint64_t u64;
+	struct cvmx_ilk_rx_lnex_stat10_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_43_63               : 21;
+	uint64_t prbs_bad                     : 11; /**< Indicates the number of training frames with bad PRBS. On overflow, saturates and sets
+                                                         ILK_RX_LNE()_INT[STAT_CNT_OVFL]. */
+	uint64_t reserved_11_31               : 21;
+	uint64_t prbs_good                    : 11; /**< Indicates the number of training frames with correct PRBS. On overflow, saturates and sets
+                                                         ILK_RX_LNE()_INT[STAT_CNT_OVFL]. */
+#else
+	uint64_t prbs_good                    : 11;
+	uint64_t reserved_11_31               : 21;
+	uint64_t prbs_bad                     : 11;
+	uint64_t reserved_43_63               : 21;
+#endif
+	} s;
+	struct cvmx_ilk_rx_lnex_stat10_s      cn78xx;
+};
+typedef union cvmx_ilk_rx_lnex_stat10 cvmx_ilk_rx_lnex_stat10_t;
+
+/**
+ * cvmx_ilk_rx_lne#_stat2
+ */
+union cvmx_ilk_rx_lnex_stat2 {
+	uint64_t u64;
+	struct cvmx_ilk_rx_lnex_stat2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_50_63               : 14;
+	uint64_t syncw_good_cnt               : 18; /**< Indicates the number of good synchronization words. On overflow, saturates and sets
+                                                         ILK_RX_LNE()_INT[STAT_CNT_OVFL]. */
+	uint64_t reserved_18_31               : 14;
+	uint64_t syncw_bad_cnt                : 18; /**< Indicates the number of bad synchronization words. On overflow, saturates and sets
+                                                         ILK_RX_LNE()_INT[STAT_CNT_OVFL]. */
+#else
+	uint64_t syncw_bad_cnt                : 18;
+	uint64_t reserved_18_31               : 14;
+	uint64_t syncw_good_cnt               : 18;
+	uint64_t reserved_50_63               : 14;
+#endif
+	} s;
+	struct cvmx_ilk_rx_lnex_stat2_s       cn68xx;
+	struct cvmx_ilk_rx_lnex_stat2_s       cn68xxp1;
+	struct cvmx_ilk_rx_lnex_stat2_s       cn78xx;
+};
+typedef union cvmx_ilk_rx_lnex_stat2 cvmx_ilk_rx_lnex_stat2_t;
+
+/**
+ * cvmx_ilk_rx_lne#_stat3
+ */
+union cvmx_ilk_rx_lnex_stat3 {
+	uint64_t u64;
+	struct cvmx_ilk_rx_lnex_stat3_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_18_63               : 46;
+	uint64_t bad_64b67b_cnt               : 18; /**< Indicates the number of bad 64B/67B words, meaning bit <65> or bit <64> has been
+                                                         corrupted. On overflow, saturates and sets ILK_RX_LNE()_INT[STAT_CNT_OVFL]. */
+#else
+	uint64_t bad_64b67b_cnt               : 18;
+	uint64_t reserved_18_63               : 46;
+#endif
+	} s;
+	struct cvmx_ilk_rx_lnex_stat3_s       cn68xx;
+	struct cvmx_ilk_rx_lnex_stat3_s       cn68xxp1;
+	struct cvmx_ilk_rx_lnex_stat3_s       cn78xx;
+};
+typedef union cvmx_ilk_rx_lnex_stat3 cvmx_ilk_rx_lnex_stat3_t;
+
+/**
+ * cvmx_ilk_rx_lne#_stat4
+ */
+union cvmx_ilk_rx_lnex_stat4 {
+	uint64_t u64;
+	struct cvmx_ilk_rx_lnex_stat4_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_59_63               : 5;
+	uint64_t cntl_word_cnt                : 27; /**< Indicates the number of control words received. SOn overflow, saturates and sets
+                                                         ILK_RX_LNE()_INT[STAT_CNT_OVFL]. */
+	uint64_t reserved_27_31               : 5;
+	uint64_t data_word_cnt                : 27; /**< Indicates the number of data words received. On overflow, saturates and sets
+                                                         ILK_RX_LNE()_INT[STAT_CNT_OVFL]. */
+#else
+	uint64_t data_word_cnt                : 27;
+	uint64_t reserved_27_31               : 5;
+	uint64_t cntl_word_cnt                : 27;
+	uint64_t reserved_59_63               : 5;
+#endif
+	} s;
+	struct cvmx_ilk_rx_lnex_stat4_s       cn68xx;
+	struct cvmx_ilk_rx_lnex_stat4_s       cn68xxp1;
+	struct cvmx_ilk_rx_lnex_stat4_s       cn78xx;
+};
+typedef union cvmx_ilk_rx_lnex_stat4 cvmx_ilk_rx_lnex_stat4_t;
+
+/**
+ * cvmx_ilk_rx_lne#_stat5
+ */
+union cvmx_ilk_rx_lnex_stat5 {
+	uint64_t u64;
+	struct cvmx_ilk_rx_lnex_stat5_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_18_63               : 46;
+	uint64_t unkwn_word_cnt               : 18; /**< Indicates the number of unknown control words.On overflow, saturates and sets
+                                                         ILK_RX_LNE()_INT[STAT_CNT_OVFL]. */
+#else
+	uint64_t unkwn_word_cnt               : 18;
+	uint64_t reserved_18_63               : 46;
+#endif
+	} s;
+	struct cvmx_ilk_rx_lnex_stat5_s       cn68xx;
+	struct cvmx_ilk_rx_lnex_stat5_s       cn68xxp1;
+	struct cvmx_ilk_rx_lnex_stat5_s       cn78xx;
+};
+typedef union cvmx_ilk_rx_lnex_stat5 cvmx_ilk_rx_lnex_stat5_t;
+
+/**
+ * cvmx_ilk_rx_lne#_stat6
+ */
+union cvmx_ilk_rx_lnex_stat6 {
+	uint64_t u64;
+	struct cvmx_ilk_rx_lnex_stat6_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_18_63               : 46;
+	uint64_t scrm_sync_loss_cnt           : 18; /**< Indicates the number of times scrambler synchronization was lost (due to either four
+                                                         consecutive bad sync words or three consecutive scrambler-state mismatches). On overflow,
+                                                         saturates and sets ILK_RX_LNE()_INT[STAT_CNT_OVFL]. */
+#else
+	uint64_t scrm_sync_loss_cnt           : 18;
+	uint64_t reserved_18_63               : 46;
+#endif
+	} s;
+	struct cvmx_ilk_rx_lnex_stat6_s       cn68xx;
+	struct cvmx_ilk_rx_lnex_stat6_s       cn68xxp1;
+	struct cvmx_ilk_rx_lnex_stat6_s       cn78xx;
+};
+typedef union cvmx_ilk_rx_lnex_stat6 cvmx_ilk_rx_lnex_stat6_t;
+
+/**
+ * cvmx_ilk_rx_lne#_stat7
+ */
+union cvmx_ilk_rx_lnex_stat7 {
+	uint64_t u64;
+	struct cvmx_ilk_rx_lnex_stat7_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_18_63               : 46;
+	uint64_t scrm_match_cnt               : 18; /**< Indicates the number of scrambler-state matches received. On overflow, saturates and sets
+                                                         ILK_RX_LNE()_INT[STAT_CNT_OVFL]. */
+#else
+	uint64_t scrm_match_cnt               : 18;
+	uint64_t reserved_18_63               : 46;
+#endif
+	} s;
+	struct cvmx_ilk_rx_lnex_stat7_s       cn68xx;
+	struct cvmx_ilk_rx_lnex_stat7_s       cn68xxp1;
+	struct cvmx_ilk_rx_lnex_stat7_s       cn78xx;
+};
+typedef union cvmx_ilk_rx_lnex_stat7 cvmx_ilk_rx_lnex_stat7_t;
+
+/**
+ * cvmx_ilk_rx_lne#_stat8
+ */
+union cvmx_ilk_rx_lnex_stat8 {
+	uint64_t u64;
+	struct cvmx_ilk_rx_lnex_stat8_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_18_63               : 46;
+	uint64_t skipw_good_cnt               : 18; /**< Indicates the number of good skip words. On overflow, saturates and sets
+                                                         ILK_RX_LNE()_INT[STAT_CNT_OVFL]. */
+#else
+	uint64_t skipw_good_cnt               : 18;
+	uint64_t reserved_18_63               : 46;
+#endif
+	} s;
+	struct cvmx_ilk_rx_lnex_stat8_s       cn68xx;
+	struct cvmx_ilk_rx_lnex_stat8_s       cn68xxp1;
+	struct cvmx_ilk_rx_lnex_stat8_s       cn78xx;
+};
+typedef union cvmx_ilk_rx_lnex_stat8 cvmx_ilk_rx_lnex_stat8_t;
+
+/**
+ * cvmx_ilk_rx_lne#_stat9
+ */
+union cvmx_ilk_rx_lnex_stat9 {
+	uint64_t u64;
+	struct cvmx_ilk_rx_lnex_stat9_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_50_63               : 14;
+	uint64_t crc32_err_cnt                : 18; /**< Indicates the number of errors in the lane CRC. On overflow, saturates and sets
+                                                         ILK_RX_LNE()_INT[STAT_CNT_OVFL]. */
+	uint64_t reserved_27_31               : 5;
+	uint64_t crc32_match_cnt              : 27; /**< Indicates the number of CRC32 matches received. On overflow, saturates and sets
+                                                         ILK_RX_LNE()_INT[STAT_CNT_OVFL]. */
+#else
+	uint64_t crc32_match_cnt              : 27;
+	uint64_t reserved_27_31               : 5;
+	uint64_t crc32_err_cnt                : 18;
+	uint64_t reserved_50_63               : 14;
+#endif
+	} s;
+	struct cvmx_ilk_rx_lnex_stat9_s       cn68xx;
+	struct cvmx_ilk_rx_lnex_stat9_s       cn68xxp1;
+	struct cvmx_ilk_rx_lnex_stat9_s       cn78xx;
+};
+typedef union cvmx_ilk_rx_lnex_stat9 cvmx_ilk_rx_lnex_stat9_t;
+
+/**
+ * cvmx_ilk_rxf_idx_pmap
+ */
+union cvmx_ilk_rxf_idx_pmap {
+	uint64_t u64;
+	struct cvmx_ilk_rxf_idx_pmap_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_25_63               : 39;
+	uint64_t inc                          : 9;  /**< Increment to add to current index for next index. */
+	uint64_t reserved_9_15                : 7;
+	uint64_t index                        : 9;  /**< Specify the link/channel accessed by the next CSR read/write to
+                                                         port map memory.   IDX[8]=link, IDX[7:0]=channel */
+#else
+	uint64_t index                        : 9;
+	uint64_t reserved_9_15                : 7;
+	uint64_t inc                          : 9;
+	uint64_t reserved_25_63               : 39;
+#endif
+	} s;
+	struct cvmx_ilk_rxf_idx_pmap_s        cn68xx;
+	struct cvmx_ilk_rxf_idx_pmap_s        cn68xxp1;
+};
+typedef union cvmx_ilk_rxf_idx_pmap cvmx_ilk_rxf_idx_pmap_t;
+
+/**
+ * cvmx_ilk_rxf_mem_pmap
+ */
+union cvmx_ilk_rxf_mem_pmap {
+	uint64_t u64;
+	struct cvmx_ilk_rxf_mem_pmap_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_6_63                : 58;
+	uint64_t port_kind                    : 6;  /**< Specify the port-kind for the link/channel selected by
+                                                         ILK_RXF_IDX_PMAP[IDX] */
+#else
+	uint64_t port_kind                    : 6;
+	uint64_t reserved_6_63                : 58;
+#endif
+	} s;
+	struct cvmx_ilk_rxf_mem_pmap_s        cn68xx;
+	struct cvmx_ilk_rxf_mem_pmap_s        cn68xxp1;
+};
+typedef union cvmx_ilk_rxf_mem_pmap cvmx_ilk_rxf_mem_pmap_t;
+
+/**
+ * cvmx_ilk_ser_cfg
+ */
+union cvmx_ilk_ser_cfg {
+	uint64_t u64;
+	struct cvmx_ilk_ser_cfg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_57_63               : 7;
+	uint64_t ser_rxpol_auto               : 1;  /**< SerDes lane receive polarity auto detection mode. */
+	uint64_t ser_rxpol                    : 16; /**< SerDes lane receive polarity.
+                                                         0 = RX without inversion.
+                                                         1 = RX with inversion.
+                                                         _ SER_RXPOL<0>  = QLM4 lane 0.
+                                                         _ SER_RXPOL<1>  = QLM4 lane 1.
+                                                         _ SER_RXPOL<2>  = QLM4 lane 2.
+                                                         _ SER_RXPOL<3>  = QLM4 lane 3.
+                                                         _ SER_RXPOL<4>  = QLM5 lane 0.
+                                                         _ SER_RXPOL<5>  = QLM5 lane 1.
+                                                         _ SER_RXPOL<6>  = QLM5 lane 2.
+                                                         _ SER_RXPOL<7>  = QLM5 lane 3.
+                                                         _ SER_RXPOL<8>  = QLM6 lane 0.
+                                                         _ SER_RXPOL<9>  = QLM6 lane 1.
+                                                         _ SER_RXPOL<10> = QLM6 lane 2.
+                                                         _ SER_RXPOL<11> = QLM6 lane 3.
+                                                         _ SER_RXPOL<12> = QLM7 lane 0.
+                                                         _ SER_RXPOL<13> = QLM7 lane 1.
+                                                         _ SER_RXPOL<14> = QLM7 lane 2.
+                                                         _ SER_RXPOL<15> = QLM7 lane 3. */
+	uint64_t ser_txpol                    : 16; /**< SerDes lane transmit polarity.
+                                                         0 = TX without inversion
+                                                         1 = TX with inversion
+                                                         _ SER_TXPOL<0>  = QLM4 lane 0.
+                                                         _ SER_TXPOL<1>  = QLM4 lane 1.
+                                                         _ SER_TXPOL<2>  = QLM4 lane 2.
+                                                         _ SER_TXPOL<3>  = QLM4 lane 3.
+                                                         _ SER_TXPOL<4>  = QLM5 lane 0.
+                                                         _ SER_TXPOL<5>  = QLM5 lane 1.
+                                                         _ SER_TXPOL<6>  = QLM5 lane 2.
+                                                         _ SER_TXPOL<7>  = QLM5 lane 3.
+                                                         _ SER_TXPOL<8>  = QLM6 lane 0.
+                                                         _ SER_TXPOL<9>  = QLM6 lane 1.
+                                                         _ SER_TXPOL<10> = QLM6 lane 2.
+                                                         _ SER_TXPOL<11> = QLM6 lane 3.
+                                                         _ SER_TXPOL<12> = QLM7 lane 0.
+                                                         _ SER_TXPOL<13> = QLM7 lane 1.
+                                                         _ SER_TXPOL<14> = QLM7 lane 2.
+                                                         _ SER_TXPOL<15> = QLM7 lane 3. */
+	uint64_t ser_reset_n                  : 16; /**< SerDes lane reset. */
+	uint64_t ser_pwrup                    : 4;  /**< Reserved. */
+	uint64_t ser_haul                     : 4;  /**< Reserved. */
+#else
+	uint64_t ser_haul                     : 4;
+	uint64_t ser_pwrup                    : 4;
+	uint64_t ser_reset_n                  : 16;
+	uint64_t ser_txpol                    : 16;
+	uint64_t ser_rxpol                    : 16;
+	uint64_t ser_rxpol_auto               : 1;
+	uint64_t reserved_57_63               : 7;
+#endif
+	} s;
+	struct cvmx_ilk_ser_cfg_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_57_63               : 7;
+	uint64_t ser_rxpol_auto               : 1;  /**< Serdes lane receive polarity auto detection mode */
+	uint64_t reserved_48_55               : 8;
+	uint64_t ser_rxpol                    : 8;  /**< Serdes lane receive polarity
+                                                         - 0: rx without inversion
+                                                         - 1: rx with inversion */
+	uint64_t reserved_32_39               : 8;
+	uint64_t ser_txpol                    : 8;  /**< Serdes lane transmit polarity
+                                                         - 0: tx without inversion
+                                                         - 1: tx with inversion */
+	uint64_t reserved_16_23               : 8;
+	uint64_t ser_reset_n                  : 8;  /**< Serdes lane reset */
+	uint64_t reserved_6_7                 : 2;
+	uint64_t ser_pwrup                    : 2;  /**< Serdes modules (QLM) power up. */
+	uint64_t reserved_2_3                 : 2;
+	uint64_t ser_haul                     : 2;  /**< Serdes module (QLM) haul mode */
+#else
+	uint64_t ser_haul                     : 2;
+	uint64_t reserved_2_3                 : 2;
+	uint64_t ser_pwrup                    : 2;
+	uint64_t reserved_6_7                 : 2;
+	uint64_t ser_reset_n                  : 8;
+	uint64_t reserved_16_23               : 8;
+	uint64_t ser_txpol                    : 8;
+	uint64_t reserved_32_39               : 8;
+	uint64_t ser_rxpol                    : 8;
+	uint64_t reserved_48_55               : 8;
+	uint64_t ser_rxpol_auto               : 1;
+	uint64_t reserved_57_63               : 7;
+#endif
+	} cn68xx;
+	struct cvmx_ilk_ser_cfg_cn68xx        cn68xxp1;
+	struct cvmx_ilk_ser_cfg_s             cn78xx;
+};
+typedef union cvmx_ilk_ser_cfg cvmx_ilk_ser_cfg_t;
+
+/**
+ * cvmx_ilk_tx#_byte_cnt#
+ */
+union cvmx_ilk_txx_byte_cntx {
+	uint64_t u64;
+	struct cvmx_ilk_txx_byte_cntx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_40_63               : 24;
+	uint64_t tx_bytes                     : 40; /**< Number of bytes transmitted per channel. Wraps on overflow. On overflow, sets
+                                                         ILK_TX()_INT[STAT_CNT_OVFL]. */
+#else
+	uint64_t tx_bytes                     : 40;
+	uint64_t reserved_40_63               : 24;
+#endif
+	} s;
+	struct cvmx_ilk_txx_byte_cntx_s       cn78xx;
+};
+typedef union cvmx_ilk_txx_byte_cntx cvmx_ilk_txx_byte_cntx_t;
+
+/**
+ * cvmx_ilk_tx#_cal_entry#
+ */
+union cvmx_ilk_txx_cal_entryx {
+	uint64_t u64;
+	struct cvmx_ilk_txx_cal_entryx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_34_63               : 30;
+	uint64_t ctl                          : 2;  /**< Select source of XON/XOFF for entry (IDX * 8) + 0:
+                                                         0 = PKI backpressure channel.
+                                                         1 = Link.
+                                                         2 = XOFF.
+                                                         3 = XON.
+                                                         This field applies to one of bits <55>, <47>, or <31> in the Interlaken control word. */
+	uint64_t reserved_8_31                : 24;
+	uint64_t channel                      : 8;  /**< PKI channel for the calendar table entry. Unused if CTL != 0. */
+#else
+	uint64_t channel                      : 8;
+	uint64_t reserved_8_31                : 24;
+	uint64_t ctl                          : 2;
+	uint64_t reserved_34_63               : 30;
+#endif
+	} s;
+	struct cvmx_ilk_txx_cal_entryx_s      cn78xx;
+};
+typedef union cvmx_ilk_txx_cal_entryx cvmx_ilk_txx_cal_entryx_t;
+
+/**
+ * cvmx_ilk_tx#_cfg0
+ */
+union cvmx_ilk_txx_cfg0 {
+	uint64_t u64;
+	struct cvmx_ilk_txx_cfg0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t ext_lpbk_fc                  : 1;  /**< Enable RX-TX flow-control external loopback. */
+	uint64_t ext_lpbk                     : 1;  /**< Enable RX-TX data external loopback. Note that with differing transmit and receive clocks,
+                                                         skip word are inserted/deleted */
+	uint64_t int_lpbk                     : 1;  /**< Enable TX-RX internal loopback. */
+	uint64_t txf_byp_dis                  : 1;  /**< Disable TXF bypass. */
+	uint64_t reserved_57_59               : 3;
+	uint64_t ptrn_mode                    : 1;  /**< Reserved. */
+	uint64_t lnk_stats_rdclr              : 1;  /**< CSR read to ILK_TXx_PKT_CNT or ILK_TXx_BYTE_CNT clears the counter after returning its
+                                                         current value. */
+	uint64_t lnk_stats_ena                : 1;  /**< Enable link statistics counters. */
+	uint64_t mltuse_fc_ena                : 1;  /**< When set, the multiuse field of control words contains flow-control status. Otherwise, the
+                                                         multiuse field contains ILK_TX()_CFG1[TX_MLTUSE] */
+	uint64_t cal_ena                      : 1;  /**< Enable TX calendar. When not asserted, the default calendar is used:
+                                                         First control word:
+                                                         _ entry 0 = link
+                                                         _ entry 1 = backpressure ID 0
+                                                         _ entry 2 = backpressure ID 1
+                                                         _ ...
+                                                         _ entry 15 = backpressure ID 14
+                                                         Second control word:
+                                                         _ entry 16 = link
+                                                         _ entry 17 = backpressure ID 15
+                                                         _ entry 18 = backpressure ID 16
+                                                         _ ...
+                                                         This continues until the calendar depth is reached.
+                                                         To disable backpressure completely, enable the calendar table and program each calendar
+                                                         table entry to transmit XON. */
+	uint64_t mfrm_len                     : 13; /**< The quantity of data sent on each lane including one sync word, scrambler state, diag
+                                                         word, zero or more skip words, and the data payload. Must be large than
+                                                         _ ILK_TX()_CFG1[SKIP_CNT] + 32.
+                                                         Supported range:
+                                                         _ ILK_TX()_CFG1[SKIP_CNT] + 32 < MFRM_LEN <= 4096 */
+	uint64_t brst_shrt                    : 7;  /**< Minimum interval between burst control words, as a multiple of eight bytes. Supported
+                                                         range from eight to 512 bytes (i.e. 0 < BRST_SHRT <= 64). */
+	uint64_t lane_rev                     : 1;  /**< Lane reversal. When enabled, lane striping is performed from most significant lane enabled
+                                                         to least significant lane enabled. LANE_ENA must be zero before changing LANE_REV. */
+	uint64_t brst_max                     : 5;  /**< Maximum size of a data burst, as a multiple of 64-byte blocks. Supported range is from 64
+                                                         to 1024 bytes (i.e. 0 < BRST_MAX <= 16). */
+	uint64_t reserved_25_25               : 1;
+	uint64_t cal_depth                    : 9;  /**< Number of valid entries in the calendar. CAL_DEPTH[2:0] must be zero. Supported range is
+                                                         from 0 to 288.
+                                                         If CAL_DEPTH = 0x0, the calendar is completely disabled and all transmit flow control
+                                                         status is XOFF. */
+	uint64_t lane_ena                     : 16; /**< Lane enable mask. Link is enabled if any lane is enabled. The same lane should not be
+                                                         enabled in multiple
+                                                         ILK_TX0/1_CFG0. Each bit of LANE_ENA maps to a TX lane (TLE) and a QLM lane. Note that
+                                                         LANE_REV has no effect on this mapping.
+                                                         _ LANE_ENA<0> = TLE0  =  QLM4 lane 0.
+                                                         _ LANE_ENA<1> = TLE1  =  QLM4 lane 1.
+                                                         _ LANE_ENA<2> = TLE2  =  QLM4 lane 2.
+                                                         _ LANE_ENA<3> = TLE3  =  QLM4 lane 3.
+                                                         _ LANE_ENA<4> = TLE4  =  QLM5 lane 0.
+                                                         _ LANE_ENA<5> = TLE5  =  QLM5 lane 1.
+                                                         _ LANE_ENA<6> = TLE6  =  QLM5 lane 2.
+                                                         _ LANE_ENA<7> = TLE7  =  QLM5 lane 3.
+                                                         _ LANE_ENA<8> = TLE8  =  QLM6 lane 0.
+                                                         _ LANE_ENA<9> = TLE9  =  QLM6 lane 1.
+                                                         _ LANE_ENA<10> = TLE10  =  QLM6 lane 2.
+                                                         _ LANE_ENA<11> = TLE11  =  QLM6 lane 3.
+                                                         _ LANE_ENA<12> = TLE12  =  QLM7 lane 0.
+                                                         _ LANE_ENA<13> = TLE13  =  QLM7 lane 1.
+                                                         _ LANE_ENA<14> = TLE14  =  QLM7 lane 2.
+                                                         _ LANE_ENA<15> = TLE15  =  QLM7 lane 3. */
+#else
+	uint64_t lane_ena                     : 16;
+	uint64_t cal_depth                    : 9;
+	uint64_t reserved_25_25               : 1;
+	uint64_t brst_max                     : 5;
+	uint64_t lane_rev                     : 1;
+	uint64_t brst_shrt                    : 7;
+	uint64_t mfrm_len                     : 13;
+	uint64_t cal_ena                      : 1;
+	uint64_t mltuse_fc_ena                : 1;
+	uint64_t lnk_stats_ena                : 1;
+	uint64_t lnk_stats_rdclr              : 1;
+	uint64_t ptrn_mode                    : 1;
+	uint64_t reserved_57_59               : 3;
+	uint64_t txf_byp_dis                  : 1;
+	uint64_t int_lpbk                     : 1;
+	uint64_t ext_lpbk                     : 1;
+	uint64_t ext_lpbk_fc                  : 1;
+#endif
+	} s;
+	struct cvmx_ilk_txx_cfg0_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t ext_lpbk_fc                  : 1;  /**< Enable Rx-Tx flowcontrol loopback (external) */
+	uint64_t ext_lpbk                     : 1;  /**< Enable Rx-Tx data loopback (external). Note that with differing
+                                                         transmit & receive clocks, skip word are  inserted/deleted */
+	uint64_t int_lpbk                     : 1;  /**< Enable Tx-Rx loopback (internal) */
+	uint64_t reserved_57_60               : 4;
+	uint64_t ptrn_mode                    : 1;  /**< Enable programmable test pattern mode.  This mode allows
+                                                         software to send a packet containing a programmable pattern.
+                                                         While in this mode, the scramblers and disparity inversion will
+                                                         be disabled.  In addition, no framing layer control words will
+                                                         be transmitted (ie. no SYNC, scrambler state, skip, or
+                                                         diagnostic words will be transmitted).
+
+                                                         NOTE: Software must first write ILK_TXX_CFG0[LANE_ENA]=0 before
+                                                         enabling/disabling this mode. */
+	uint64_t reserved_55_55               : 1;
+	uint64_t lnk_stats_ena                : 1;  /**< Enable link statistics counters */
+	uint64_t mltuse_fc_ena                : 1;  /**< When set, the multi-use field of control words will contain
+                                                         flow control status.  Otherwise, the multi-use field will
+                                                         contain ILK_TXX_CFG1[TX_MLTUSE] */
+	uint64_t cal_ena                      : 1;  /**< Enable Tx calendar, else default calendar used:
+                                                              First control word:
+                                                               Entry 0  = link
+                                                               Entry 1  = backpressue id 0
+                                                               Entry 2  = backpressue id 1
+                                                               ...etc.
+                                                            Second control word:
+                                                               Entry 16 = link
+                                                               Entry 17 = backpressue id 15
+                                                               Entry 18 = backpressue id 16
+                                                               ...etc.
+                                                         This continues until the status for all 64 backpressue ids gets
+                                                         transmitted (ie. 0-68 calendar table entries).  The remaining 3
+                                                         calendar table entries (ie. 69-71) will always transmit XOFF.
+
+                                                         To disable backpressure completely, enable the calendar table
+                                                         and program each calendar table entry to transmit XON */
+	uint64_t mfrm_len                     : 13; /**< The quantity of data sent on each lane including one sync word,
+                                                         scrambler state, diag word, zero or more skip words, and the
+                                                         data  payload.  Must be large than ILK_TXX_CFG1[SKIP_CNT]+9.
+                                                         Supported range:ILK_TXX_CFG1[SKIP_CNT]+9 < MFRM_LEN <= 4096) */
+	uint64_t brst_shrt                    : 7;  /**< Minimum interval between burst control words, as a multiple of
+                                                         8 bytes.  Supported range from 8 bytes to 512 (ie. 0 <
+                                                         BRST_SHRT <= 64) */
+	uint64_t lane_rev                     : 1;  /**< Lane reversal.   When enabled, lane striping is performed from
+                                                         most significant lane enabled to least significant lane
+                                                         enabled.  LANE_ENA must be zero before changing LANE_REV. */
+	uint64_t brst_max                     : 5;  /**< Maximum size of a data burst, as a multiple of 64 byte blocks.
+                                                         Supported range is from 64 bytes to 1024 bytes. (ie. 0 <
+                                                         BRST_MAX <= 16) */
+	uint64_t reserved_25_25               : 1;
+	uint64_t cal_depth                    : 9;  /**< Number of valid entries in the calendar.  CAL_DEPTH[2:0] must
+                                                         be zero.  Supported range from 8 to 288.  If CAL_ENA is 0,
+                                                         this field has no effect and the calendar depth is 72 entries. */
+	uint64_t reserved_8_15                : 8;
+	uint64_t lane_ena                     : 8;  /**< Lane enable mask.  Link is enabled if any lane is enabled.  The
+                                                         same lane should not be enabled in multiple ILK_TXx_CFG0.  Each
+                                                         bit of LANE_ENA maps to a TX lane (TLE) and a QLM lane.  NOTE:
+                                                         LANE_REV has no effect on this mapping.
+
+                                                               LANE_ENA[0] = TLE0 = QLM1 lane 0
+                                                               LANE_ENA[1] = TLE1 = QLM1 lane 1
+                                                               LANE_ENA[2] = TLE2 = QLM1 lane 2
+                                                               LANE_ENA[3] = TLE3 = QLM1 lane 3
+                                                               LANE_ENA[4] = TLE4 = QLM2 lane 0
+                                                               LANE_ENA[5] = TLE5 = QLM2 lane 1
+                                                               LANE_ENA[6] = TLE6 = QLM2 lane 2
+                                                               LANE_ENA[7] = TLE7 = QLM2 lane 3 */
+#else
+	uint64_t lane_ena                     : 8;
+	uint64_t reserved_8_15                : 8;
+	uint64_t cal_depth                    : 9;
+	uint64_t reserved_25_25               : 1;
+	uint64_t brst_max                     : 5;
+	uint64_t lane_rev                     : 1;
+	uint64_t brst_shrt                    : 7;
+	uint64_t mfrm_len                     : 13;
+	uint64_t cal_ena                      : 1;
+	uint64_t mltuse_fc_ena                : 1;
+	uint64_t lnk_stats_ena                : 1;
+	uint64_t reserved_55_55               : 1;
+	uint64_t ptrn_mode                    : 1;
+	uint64_t reserved_57_60               : 4;
+	uint64_t int_lpbk                     : 1;
+	uint64_t ext_lpbk                     : 1;
+	uint64_t ext_lpbk_fc                  : 1;
+#endif
+	} cn68xx;
+	struct cvmx_ilk_txx_cfg0_cn68xx       cn68xxp1;
+	struct cvmx_ilk_txx_cfg0_s            cn78xx;
+};
+typedef union cvmx_ilk_txx_cfg0 cvmx_ilk_txx_cfg0_t;
+
+/**
+ * cvmx_ilk_tx#_cfg1
+ */
+union cvmx_ilk_txx_cfg1 {
+	uint64_t u64;
+	struct cvmx_ilk_txx_cfg1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t ser_low                      : 4;  /**< Reserved. */
+	uint64_t reserved_53_59               : 7;
+	uint64_t brst_min                     : 5;  /**< Minimum size of a data burst, as a multiple of 32-byte blocks. 0 disables the scheduling
+                                                         enhancement. When non-zero, must satisfy:
+                                                         _ (BRST_SHRT*8) <= (BRST_MIN*32) <= (BRST_MAX*64)/2. */
+	uint64_t reserved_43_47               : 5;
+	uint64_t ser_limit                    : 10; /**< Reduce latency by limiting the amount of data in flight for each SerDes. If 0x0, hardware
+                                                         will compute it. Otherwise, SER_LIMIT must be set as follows:
+                                                         _ SER_LIMIT >= 148 + (BAUD / SCLK) * (12 + (NUM_LANES/2))
+                                                         For instance, for sclk=1.1GHz,BAUD=10.3125,NUM_LANES=16 :
+                                                         _ SER_LIMIT >= 148 + (10.3125 / 1.1 * (12 + (12/2))
+                                                         _ SER_LIMIT >= 317 */
+	uint64_t pkt_busy                     : 1;  /**< Packet busy. When set to 1, indicates the TX-link is transmitting data. */
+	uint64_t pipe_crd_dis                 : 1;  /**< Disable channel credits. Should be set to 1 when PKO is configured to ignore channel credits. */
+	uint64_t ptp_delay                    : 5;  /**< Reserved. */
+	uint64_t skip_cnt                     : 4;  /**< Number of skip words to insert after the scrambler state. */
+	uint64_t pkt_flush                    : 1;  /**< Packet transmit flush. When asserted, the TxFIFO continuously drains; all data is dropped.
+                                                         Software should first write PKT_ENA = 0 and wait for PKT_BUSY = 0. */
+	uint64_t pkt_ena                      : 1;  /**< Packet transmit enable. When asserted, the TX-link stops transmitting packets, as per
+                                                         RX_LINK_FC_PKT. */
+	uint64_t la_mode                      : 1;  /**< Reserved. */
+	uint64_t tx_link_fc                   : 1;  /**< Link flow-control status transmitted by the TX-link. XON (=1) when RX_FIFO_CNT <=
+                                                         RX_FIFO_HWM and lane alignment is done */
+	uint64_t rx_link_fc                   : 1;  /**< Link flow-control status received in burst/idle control words. When RX_LINK_FC_IGN = 0,
+                                                         XOFF (=0) causes TX-link to stop transmitting on all channels. */
+	uint64_t reserved_12_16               : 5;
+	uint64_t tx_link_fc_jam               : 1;  /**< All flow-control transmitted in burst/idle control words are XOFF whenever TX_LINK_FC = 0
+                                                         (XOFF). Assert this field to allow link XOFF to automatically XOFF all channels. */
+	uint64_t rx_link_fc_pkt               : 1;  /**< Link flow-control received in burst/idle control words causes TX-link to stop transmitting
+                                                         at the end of a packet instead of
+                                                         the end of a burst. */
+	uint64_t rx_link_fc_ign               : 1;  /**< Ignore the link flow-control status received in burst/idle control words */
+	uint64_t rmatch                       : 1;  /**< Enable rate matching circuitry. */
+	uint64_t tx_mltuse                    : 8;  /**< Multiuse bits are used when ILK_TX()_CFG0[MLTUSE_FC_ENA] = 0. */
+#else
+	uint64_t tx_mltuse                    : 8;
+	uint64_t rmatch                       : 1;
+	uint64_t rx_link_fc_ign               : 1;
+	uint64_t rx_link_fc_pkt               : 1;
+	uint64_t tx_link_fc_jam               : 1;
+	uint64_t reserved_12_16               : 5;
+	uint64_t rx_link_fc                   : 1;
+	uint64_t tx_link_fc                   : 1;
+	uint64_t la_mode                      : 1;
+	uint64_t pkt_ena                      : 1;
+	uint64_t pkt_flush                    : 1;
+	uint64_t skip_cnt                     : 4;
+	uint64_t ptp_delay                    : 5;
+	uint64_t pipe_crd_dis                 : 1;
+	uint64_t pkt_busy                     : 1;
+	uint64_t ser_limit                    : 10;
+	uint64_t reserved_43_47               : 5;
+	uint64_t brst_min                     : 5;
+	uint64_t reserved_53_59               : 7;
+	uint64_t ser_low                      : 4;
+#endif
+	} s;
+	struct cvmx_ilk_txx_cfg1_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_33_63               : 31;
+	uint64_t pkt_busy                     : 1;  /**< Tx-Link is transmitting data. */
+	uint64_t pipe_crd_dis                 : 1;  /**< Disable pipe credits.   Should be set when PKO is configure to
+                                                         ignore pipe credits. */
+	uint64_t ptp_delay                    : 5;  /**< Timestamp commit delay.  Must not be zero. */
+	uint64_t skip_cnt                     : 4;  /**< Number of skip words to insert after the scrambler state */
+	uint64_t pkt_flush                    : 1;  /**< Packet transmit flush.  While PKT_FLUSH=1, the TxFifo will
+                                                         continuously drain; all data will be dropped.  Software should
+                                                         first write PKT_ENA=0 and wait for PKT_BUSY=0. */
+	uint64_t pkt_ena                      : 1;  /**< Packet transmit enable.  When PKT_ENA=0, the Tx-Link will stop
+                                                         transmitting packets, as per RX_LINK_FC_PKT */
+	uint64_t la_mode                      : 1;  /**< Enable Interlaken Look-Aside traffic.   When LA_MODE=1 and
+                                                         REMAP=1 for a given port-pipe, bit[39] of the 8-byte header
+                                                         specifies the packet should be decoded as an LA packet. */
+	uint64_t tx_link_fc                   : 1;  /**< Link flow control status transmitted by the Tx-Link
+                                                         XON (=1) when RX_FIFO_CNT <= RX_FIFO_HWM and lane alignment is
+                                                         done */
+	uint64_t rx_link_fc                   : 1;  /**< Link flow control status received in burst/idle control words.
+                                                         When RX_LINK_FC_IGN=0, XOFF (=0) will cause Tx-Link to stop
+                                                         transmitting on all channels. */
+	uint64_t reserved_12_16               : 5;
+	uint64_t tx_link_fc_jam               : 1;  /**< All flow control transmitted in burst/idle control words will
+                                                         be XOFF whenever TX_LINK_FC is XOFF.   Enable this to allow
+                                                         link XOFF to automatically XOFF all channels. */
+	uint64_t rx_link_fc_pkt               : 1;  /**< Link flow control received in burst/idle control words causes
+                                                         Tx-Link to stop transmitting at the end of a packet instead of
+                                                         the end of a burst */
+	uint64_t rx_link_fc_ign               : 1;  /**< Ignore the link flow control status received in burst/idle
+                                                         control words */
+	uint64_t rmatch                       : 1;  /**< Enable rate matching circuitry */
+	uint64_t tx_mltuse                    : 8;  /**< Multiple Use bits used when ILKx_TX_CFG[LA_MODE=0] and
+                                                         ILKx_TX_CFG[MLTUSE_FC_ENA] is zero */
+#else
+	uint64_t tx_mltuse                    : 8;
+	uint64_t rmatch                       : 1;
+	uint64_t rx_link_fc_ign               : 1;
+	uint64_t rx_link_fc_pkt               : 1;
+	uint64_t tx_link_fc_jam               : 1;
+	uint64_t reserved_12_16               : 5;
+	uint64_t rx_link_fc                   : 1;
+	uint64_t tx_link_fc                   : 1;
+	uint64_t la_mode                      : 1;
+	uint64_t pkt_ena                      : 1;
+	uint64_t pkt_flush                    : 1;
+	uint64_t skip_cnt                     : 4;
+	uint64_t ptp_delay                    : 5;
+	uint64_t pipe_crd_dis                 : 1;
+	uint64_t pkt_busy                     : 1;
+	uint64_t reserved_33_63               : 31;
+#endif
+	} cn68xx;
+	struct cvmx_ilk_txx_cfg1_cn68xxp1 {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_32_63               : 32;
+	uint64_t pipe_crd_dis                 : 1;  /**< Disable pipe credits.   Should be set when PKO is configure to
+                                                         ignore pipe credits. */
+	uint64_t ptp_delay                    : 5;  /**< Timestamp commit delay.  Must not be zero. */
+	uint64_t skip_cnt                     : 4;  /**< Number of skip words to insert after the scrambler state */
+	uint64_t pkt_flush                    : 1;  /**< Packet transmit flush.  While PKT_FLUSH=1, the TxFifo will
+                                                         continuously drain; all data will be dropped.  Software should
+                                                         first write PKT_ENA=0 and wait packet transmission to stop. */
+	uint64_t pkt_ena                      : 1;  /**< Packet transmit enable.  When PKT_ENA=0, the Tx-Link will stop
+                                                         transmitting packets, as per RX_LINK_FC_PKT */
+	uint64_t la_mode                      : 1;  /**< 0 = Interlaken
+                                                         1 = Interlaken Look-Aside */
+	uint64_t tx_link_fc                   : 1;  /**< Link flow control status transmitted by the Tx-Link
+                                                         XON (=1) when RX_FIFO_CNT <= RX_FIFO_HWM and lane alignment is
+                                                         done */
+	uint64_t rx_link_fc                   : 1;  /**< Link flow control status received in burst/idle control words.
+                                                         When RX_LINK_FC_IGN=0, XOFF (=0) will cause Tx-Link to stop
+                                                         transmitting on all channels. */
+	uint64_t reserved_12_16               : 5;
+	uint64_t tx_link_fc_jam               : 1;  /**< All flow control transmitted in burst/idle control words will
+                                                         be XOFF whenever TX_LINK_FC is XOFF.   Enable this to allow
+                                                         link XOFF to automatically XOFF all channels. */
+	uint64_t rx_link_fc_pkt               : 1;  /**< Link flow control received in burst/idle control words causes
+                                                         Tx-Link to stop transmitting at the end of a packet instead of
+                                                         the end of a burst */
+	uint64_t rx_link_fc_ign               : 1;  /**< Ignore the link flow control status received in burst/idle
+                                                         control words */
+	uint64_t rmatch                       : 1;  /**< Enable rate matching circuitry */
+	uint64_t tx_mltuse                    : 8;  /**< Multiple Use bits used when ILKx_TX_CFG[LA_MODE=0] and
+                                                         ILKx_TX_CFG[MLTUSE_FC_ENA] is zero */
+#else
+	uint64_t tx_mltuse                    : 8;
+	uint64_t rmatch                       : 1;
+	uint64_t rx_link_fc_ign               : 1;
+	uint64_t rx_link_fc_pkt               : 1;
+	uint64_t tx_link_fc_jam               : 1;
+	uint64_t reserved_12_16               : 5;
+	uint64_t rx_link_fc                   : 1;
+	uint64_t tx_link_fc                   : 1;
+	uint64_t la_mode                      : 1;
+	uint64_t pkt_ena                      : 1;
+	uint64_t pkt_flush                    : 1;
+	uint64_t skip_cnt                     : 4;
+	uint64_t ptp_delay                    : 5;
+	uint64_t pipe_crd_dis                 : 1;
+	uint64_t reserved_32_63               : 32;
+#endif
+	} cn68xxp1;
+	struct cvmx_ilk_txx_cfg1_s            cn78xx;
+};
+typedef union cvmx_ilk_txx_cfg1 cvmx_ilk_txx_cfg1_t;
+
+/**
+ * cvmx_ilk_tx#_cha_xon#
+ */
+union cvmx_ilk_txx_cha_xonx {
+	uint64_t u64;
+	struct cvmx_ilk_txx_cha_xonx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t status                       : 64; /**< PKI flow control status for backpressure ID 255-0, where a 0 indicates the presence of
+                                                         backpressure (i.e. XOFF) and 1 indicates the absence of backpressure (i.e. XON).
+                                                         _ ILK_TX(0..1)_CHA_XON[0] -- Channels 63-0.
+                                                         _ ILK_TX(0..1)_CHA_XON[1] -- Channels 127-64.
+                                                         _ ILK_TX(0..1)_CHA_XON[2] -- Channels 191-128.
+                                                         _ ILK_TX(0..1)_CHA_XON[3] -- Channels 255-192. */
+#else
+	uint64_t status                       : 64;
+#endif
+	} s;
+	struct cvmx_ilk_txx_cha_xonx_s        cn78xx;
+};
+typedef union cvmx_ilk_txx_cha_xonx cvmx_ilk_txx_cha_xonx_t;
+
+/**
+ * cvmx_ilk_tx#_dbg
+ */
+union cvmx_ilk_txx_dbg {
+	uint64_t u64;
+	struct cvmx_ilk_txx_dbg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_29_63               : 35;
+	uint64_t data_rate                    : 13; /**< Reserved. */
+	uint64_t low_delay                    : 6;  /**< Reserved. */
+	uint64_t reserved_3_9                 : 7;
+	uint64_t tx_bad_crc24                 : 1;  /**< Send a control word with bad CRC24. Hardware clears this field once the injection is performed. */
+	uint64_t tx_bad_ctlw2                 : 1;  /**< Send a control word without the control bit set. */
+	uint64_t tx_bad_ctlw1                 : 1;  /**< Send a data word with the control bit set. */
+#else
+	uint64_t tx_bad_ctlw1                 : 1;
+	uint64_t tx_bad_ctlw2                 : 1;
+	uint64_t tx_bad_crc24                 : 1;
+	uint64_t reserved_3_9                 : 7;
+	uint64_t low_delay                    : 6;
+	uint64_t data_rate                    : 13;
+	uint64_t reserved_29_63               : 35;
+#endif
+	} s;
+	struct cvmx_ilk_txx_dbg_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_3_63                : 61;
+	uint64_t tx_bad_crc24                 : 1;  /**< Send a control word with bad CRC24.  Hardware will clear this
+                                                         field once the injection is performed. */
+	uint64_t tx_bad_ctlw2                 : 1;  /**< Send a control word without the control bit set */
+	uint64_t tx_bad_ctlw1                 : 1;  /**< Send a data word with the control bit set */
+#else
+	uint64_t tx_bad_ctlw1                 : 1;
+	uint64_t tx_bad_ctlw2                 : 1;
+	uint64_t tx_bad_crc24                 : 1;
+	uint64_t reserved_3_63                : 61;
+#endif
+	} cn68xx;
+	struct cvmx_ilk_txx_dbg_cn68xx        cn68xxp1;
+	struct cvmx_ilk_txx_dbg_s             cn78xx;
+};
+typedef union cvmx_ilk_txx_dbg cvmx_ilk_txx_dbg_t;
+
+/**
+ * cvmx_ilk_tx#_err_cfg
+ */
+union cvmx_ilk_txx_err_cfg {
+	uint64_t u64;
+	struct cvmx_ilk_txx_err_cfg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_20_63               : 44;
+	uint64_t fwc_flip                     : 2;  /**< Testing feature. Flip syndrome bits <1:0> on writes to the FWC RAM to test single-bit or
+                                                         double-bit errors. */
+	uint64_t txf_flip                     : 2;  /**< Testing feature. Flip syndrome bits <1:0> on writes to the TXF RAM to test single-bit or
+                                                         double-bit errors. */
+	uint64_t reserved_2_15                : 14;
+	uint64_t fwc_cor_dis                  : 1;  /**< Disable ECC corrector on FWC. */
+	uint64_t txf_cor_dis                  : 1;  /**< Disable ECC corrector on TXF. */
+#else
+	uint64_t txf_cor_dis                  : 1;
+	uint64_t fwc_cor_dis                  : 1;
+	uint64_t reserved_2_15                : 14;
+	uint64_t txf_flip                     : 2;
+	uint64_t fwc_flip                     : 2;
+	uint64_t reserved_20_63               : 44;
+#endif
+	} s;
+	struct cvmx_ilk_txx_err_cfg_s         cn78xx;
+};
+typedef union cvmx_ilk_txx_err_cfg cvmx_ilk_txx_err_cfg_t;
+
+/**
+ * cvmx_ilk_tx#_flow_ctl0
+ */
+union cvmx_ilk_txx_flow_ctl0 {
+	uint64_t u64;
+	struct cvmx_ilk_txx_flow_ctl0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t status                       : 64; /**< IPD flow control status for backpressue id 63-0, where a 0
+                                                         indicates the presence of backpressure (ie. XOFF) and 1
+                                                         indicates the absence of backpressure (ie. XON) */
+#else
+	uint64_t status                       : 64;
+#endif
+	} s;
+	struct cvmx_ilk_txx_flow_ctl0_s       cn68xx;
+	struct cvmx_ilk_txx_flow_ctl0_s       cn68xxp1;
+};
+typedef union cvmx_ilk_txx_flow_ctl0 cvmx_ilk_txx_flow_ctl0_t;
+
+/**
+ * cvmx_ilk_tx#_flow_ctl1
+ *
+ * Notes:
+ * Do not publish.
+ *
+ */
+union cvmx_ilk_txx_flow_ctl1 {
+	uint64_t u64;
+	struct cvmx_ilk_txx_flow_ctl1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_0_63                : 64;
+#else
+	uint64_t reserved_0_63                : 64;
+#endif
+	} s;
+	struct cvmx_ilk_txx_flow_ctl1_s       cn68xx;
+	struct cvmx_ilk_txx_flow_ctl1_s       cn68xxp1;
+};
+typedef union cvmx_ilk_txx_flow_ctl1 cvmx_ilk_txx_flow_ctl1_t;
+
+/**
+ * cvmx_ilk_tx#_idx_cal
+ */
+union cvmx_ilk_txx_idx_cal {
+	uint64_t u64;
+	struct cvmx_ilk_txx_idx_cal_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_14_63               : 50;
+	uint64_t inc                          : 6;  /**< Increment to add to current index for next index. NOTE:
+                                                         Increment only performed after *MEM_CAL1 access (ie. not
+                                                         *MEM_CAL0) */
+	uint64_t reserved_6_7                 : 2;
+	uint64_t index                        : 6;  /**< Specify the group of 8 entries accessed by the next CSR
+                                                         read/write to calendar table memory.  Software must ensure IDX
+                                                         is <36 whenever writing to *MEM_CAL1 */
+#else
+	uint64_t index                        : 6;
+	uint64_t reserved_6_7                 : 2;
+	uint64_t inc                          : 6;
+	uint64_t reserved_14_63               : 50;
+#endif
+	} s;
+	struct cvmx_ilk_txx_idx_cal_s         cn68xx;
+	struct cvmx_ilk_txx_idx_cal_s         cn68xxp1;
+};
+typedef union cvmx_ilk_txx_idx_cal cvmx_ilk_txx_idx_cal_t;
+
+/**
+ * cvmx_ilk_tx#_idx_pmap
+ */
+union cvmx_ilk_txx_idx_pmap {
+	uint64_t u64;
+	struct cvmx_ilk_txx_idx_pmap_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_23_63               : 41;
+	uint64_t inc                          : 7;  /**< Increment to add to current index for next index. */
+	uint64_t reserved_7_15                : 9;
+	uint64_t index                        : 7;  /**< Specify the port-pipe accessed by the next CSR read/write to
+                                                         ILK_TXx_MEM_PMAP.   Note that IDX=n is always port-pipe n,
+                                                         regardless of ILK_TXx_PIPE[BASE] */
+#else
+	uint64_t index                        : 7;
+	uint64_t reserved_7_15                : 9;
+	uint64_t inc                          : 7;
+	uint64_t reserved_23_63               : 41;
+#endif
+	} s;
+	struct cvmx_ilk_txx_idx_pmap_s        cn68xx;
+	struct cvmx_ilk_txx_idx_pmap_s        cn68xxp1;
+};
+typedef union cvmx_ilk_txx_idx_pmap cvmx_ilk_txx_idx_pmap_t;
+
+/**
+ * cvmx_ilk_tx#_idx_stat0
+ */
+union cvmx_ilk_txx_idx_stat0 {
+	uint64_t u64;
+	struct cvmx_ilk_txx_idx_stat0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_32_63               : 32;
+	uint64_t clr                          : 1;  /**< CSR read to ILK_TXx_MEM_STAT0 clears the selected counter after
+                                                         returning its current value. */
+	uint64_t reserved_24_30               : 7;
+	uint64_t inc                          : 8;  /**< Increment to add to current index for next index */
+	uint64_t reserved_8_15                : 8;
+	uint64_t index                        : 8;  /**< Specify the channel accessed during the next CSR read to the
+                                                         ILK_TXx_MEM_STAT0 */
+#else
+	uint64_t index                        : 8;
+	uint64_t reserved_8_15                : 8;
+	uint64_t inc                          : 8;
+	uint64_t reserved_24_30               : 7;
+	uint64_t clr                          : 1;
+	uint64_t reserved_32_63               : 32;
+#endif
+	} s;
+	struct cvmx_ilk_txx_idx_stat0_s       cn68xx;
+	struct cvmx_ilk_txx_idx_stat0_s       cn68xxp1;
+};
+typedef union cvmx_ilk_txx_idx_stat0 cvmx_ilk_txx_idx_stat0_t;
+
+/**
+ * cvmx_ilk_tx#_idx_stat1
+ */
+union cvmx_ilk_txx_idx_stat1 {
+	uint64_t u64;
+	struct cvmx_ilk_txx_idx_stat1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_32_63               : 32;
+	uint64_t clr                          : 1;  /**< CSR read to ILK_TXx_MEM_STAT1 clears the selected counter after
+                                                         returning its current value. */
+	uint64_t reserved_24_30               : 7;
+	uint64_t inc                          : 8;  /**< Increment to add to current index for next index */
+	uint64_t reserved_8_15                : 8;
+	uint64_t index                        : 8;  /**< Specify the channel accessed during the next CSR read to the
+                                                         ILK_TXx_MEM_STAT1 */
+#else
+	uint64_t index                        : 8;
+	uint64_t reserved_8_15                : 8;
+	uint64_t inc                          : 8;
+	uint64_t reserved_24_30               : 7;
+	uint64_t clr                          : 1;
+	uint64_t reserved_32_63               : 32;
+#endif
+	} s;
+	struct cvmx_ilk_txx_idx_stat1_s       cn68xx;
+	struct cvmx_ilk_txx_idx_stat1_s       cn68xxp1;
+};
+typedef union cvmx_ilk_txx_idx_stat1 cvmx_ilk_txx_idx_stat1_t;
+
+/**
+ * cvmx_ilk_tx#_int
+ */
+union cvmx_ilk_txx_int {
+	uint64_t u64;
+	struct cvmx_ilk_txx_int_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_8_63                : 56;
+	uint64_t fwc_dbe                      : 1;  /**< Flow control calendar table double-bit error. Throws ILK_INTSN_E::ILK_TX()_FWC_DBE. */
+	uint64_t fwc_sbe                      : 1;  /**< Flow control calendar table single-bit error. Throws ILK_INTSN_E::ILK_TX()_FWC_SBE. */
+	uint64_t txf_dbe                      : 1;  /**< TX FIFO double-bit error. Throws ILK_INTSN_E::ILK_TX()_TXF_DBE. */
+	uint64_t txf_sbe                      : 1;  /**< TX FIFO single-bit error. Throws ILK_INTSN_E::ILK_TX()_TXF_SBE. */
+	uint64_t stat_cnt_ovfl                : 1;  /**< Statistics counter overflow. Throws ILK_INTSN_E::ILK_TX()_STAT_CNT_OVFL. */
+	uint64_t bad_pipe                     : 1;  /**< Reserved. */
+	uint64_t bad_seq                      : 1;  /**< Received sequence is not SOP followed by 0 or more data cycles followed by EOP. Throws
+                                                         ILK_INTSN_E::ILK_TX()_BAD_SEQ. */
+	uint64_t txf_err                      : 1;  /**< Reserved. */
+#else
+	uint64_t txf_err                      : 1;
+	uint64_t bad_seq                      : 1;
+	uint64_t bad_pipe                     : 1;
+	uint64_t stat_cnt_ovfl                : 1;
+	uint64_t txf_sbe                      : 1;
+	uint64_t txf_dbe                      : 1;
+	uint64_t fwc_sbe                      : 1;
+	uint64_t fwc_dbe                      : 1;
+	uint64_t reserved_8_63                : 56;
+#endif
+	} s;
+	struct cvmx_ilk_txx_int_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t stat_cnt_ovfl                : 1;  /**< Statistics counter overflow */
+	uint64_t bad_pipe                     : 1;  /**< Received a PKO port-pipe out of the range specified by
+                                                         ILK_TXX_PIPE */
+	uint64_t bad_seq                      : 1;  /**< Received sequence is not SOP followed by 0 or more data cycles
+                                                         followed by EOP.  PKO config assigned multiple engines to the
+                                                         same ILK Tx Link. */
+	uint64_t txf_err                      : 1;  /**< TX fifo parity error occurred.  At EOP time, EOP_Format will
+                                                         reflect the error. */
+#else
+	uint64_t txf_err                      : 1;
+	uint64_t bad_seq                      : 1;
+	uint64_t bad_pipe                     : 1;
+	uint64_t stat_cnt_ovfl                : 1;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} cn68xx;
+	struct cvmx_ilk_txx_int_cn68xx        cn68xxp1;
+	struct cvmx_ilk_txx_int_s             cn78xx;
+};
+typedef union cvmx_ilk_txx_int cvmx_ilk_txx_int_t;
+
+/**
+ * cvmx_ilk_tx#_int_en
+ */
+union cvmx_ilk_txx_int_en {
+	uint64_t u64;
+	struct cvmx_ilk_txx_int_en_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t stat_cnt_ovfl                : 1;  /**< Statistics counter overflow */
+	uint64_t bad_pipe                     : 1;  /**< Received a PKO port-pipe out of the range specified by
+                                                         ILK_TXX_PIPE. */
+	uint64_t bad_seq                      : 1;  /**< Received sequence is not SOP followed by 0 or more data cycles
+                                                         followed by EOP.  PKO config assigned multiple engines to the
+                                                         same ILK Tx Link. */
+	uint64_t txf_err                      : 1;  /**< TX fifo parity error occurred.  At EOP time, EOP_Format will
+                                                         reflect the error. */
+#else
+	uint64_t txf_err                      : 1;
+	uint64_t bad_seq                      : 1;
+	uint64_t bad_pipe                     : 1;
+	uint64_t stat_cnt_ovfl                : 1;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_ilk_txx_int_en_s          cn68xx;
+	struct cvmx_ilk_txx_int_en_s          cn68xxp1;
+};
+typedef union cvmx_ilk_txx_int_en cvmx_ilk_txx_int_en_t;
+
+/**
+ * cvmx_ilk_tx#_mem_cal0
+ *
+ * Notes:
+ * Software must always read ILK_TXx_MEM_CAL0 then ILK_TXx_MEM_CAL1.  Software
+ * must never read them in reverse order or read one without reading the
+ * other.
+ *
+ * Software must always write ILK_TXx_MEM_CAL0 then ILK_TXx_MEM_CAL1.
+ * Software must never write them in reverse order or write one without
+ * writing the other.
+ */
+union cvmx_ilk_txx_mem_cal0 {
+	uint64_t u64;
+	struct cvmx_ilk_txx_mem_cal0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_36_63               : 28;
+	uint64_t entry_ctl3                   : 2;  /**< Select source of XON/XOFF for entry (IDX*8)+3
+                                                            - 0: IPD backpressue id
+                                                            - 1: Link
+                                                            - 2: XOFF
+                                                            - 3: XON
+                                                         This field applies to one of bits <52>, <44>, or <28> in the
+                                                         Interlaken control word. */
+	uint64_t reserved_33_33               : 1;
+	uint64_t bpid3                        : 6;  /**< Select IPD backpressue id for calendar table entry (IDX*8)+3
+                                                         (unused if ENTRY_CTL3 != 0) */
+	uint64_t entry_ctl2                   : 2;  /**< Select source of XON/XOFF for entry (IDX*8)+2
+                                                            - 0: IPD backpressue id
+                                                            - 1: Link
+                                                            - 2: XOFF
+                                                            - 3: XON
+                                                         This field applies to one of bits <53>, <45>, or <29> in the
+                                                         Interlaken control word. */
+	uint64_t reserved_24_24               : 1;
+	uint64_t bpid2                        : 6;  /**< Select IPD backpressue id for calendar table entry (IDX*8)+2
+                                                         (unused if ENTRY_CTL2 != 0) */
+	uint64_t entry_ctl1                   : 2;  /**< Select source of XON/XOFF for entry (IDX*8)+1
+                                                            - 0: IPD backpressue id
+                                                            - 1: Link
+                                                            - 2: XOFF
+                                                            - 3: XON
+                                                         This field applies to one of bits <54>, <46>, or <30> in the
+                                                         Interlaken control word. */
+	uint64_t reserved_15_15               : 1;
+	uint64_t bpid1                        : 6;  /**< Select IPD backpressue id for calendar table entry (IDX*8)+1
+                                                         (unused if ENTRY_CTL1 != 0) */
+	uint64_t entry_ctl0                   : 2;  /**< Select source of XON/XOFF for entry (IDX*8)+0
+                                                            - 0: IPD backpressue id
+                                                            - 1: Link
+                                                            - 2: XOFF
+                                                            - 3: XON
+                                                         This field applies to one of bits <55>, <47>, or <31> in the
+                                                         Interlaken control word. */
+	uint64_t reserved_6_6                 : 1;
+	uint64_t bpid0                        : 6;  /**< Select IPD backpressue id for calendar table entry (IDX*8)+0
+                                                         (unused if ENTRY_CTL0 != 0) */
+#else
+	uint64_t bpid0                        : 6;
+	uint64_t reserved_6_6                 : 1;
+	uint64_t entry_ctl0                   : 2;
+	uint64_t bpid1                        : 6;
+	uint64_t reserved_15_15               : 1;
+	uint64_t entry_ctl1                   : 2;
+	uint64_t bpid2                        : 6;
+	uint64_t reserved_24_24               : 1;
+	uint64_t entry_ctl2                   : 2;
+	uint64_t bpid3                        : 6;
+	uint64_t reserved_33_33               : 1;
+	uint64_t entry_ctl3                   : 2;
+	uint64_t reserved_36_63               : 28;
+#endif
+	} s;
+	struct cvmx_ilk_txx_mem_cal0_s        cn68xx;
+	struct cvmx_ilk_txx_mem_cal0_s        cn68xxp1;
+};
+typedef union cvmx_ilk_txx_mem_cal0 cvmx_ilk_txx_mem_cal0_t;
+
+/**
+ * cvmx_ilk_tx#_mem_cal1
+ *
+ * Notes:
+ * Software must always read ILK_TXx_MEM_CAL0 then ILK_TXx_MEM_CAL1.  Software
+ * must never read them in reverse order or read one without reading the
+ * other.
+ *
+ * Software must always write ILK_TXx_MEM_CAL0 then ILK_TXx_MEM_CAL1.
+ * Software must never write them in reverse order or write one without
+ * writing the other.
+ */
+union cvmx_ilk_txx_mem_cal1 {
+	uint64_t u64;
+	struct cvmx_ilk_txx_mem_cal1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_36_63               : 28;
+	uint64_t entry_ctl7                   : 2;  /**< Select source of XON/XOFF for entry (IDX*8)+7
+                                                            - 0: IPD backpressue id
+                                                            - 1: Link
+                                                            - 2: XOFF
+                                                            - 3: XON
+                                                         This field applies to one of bits <48>, <40>, or <24> in the
+                                                         Interlaken control word. */
+	uint64_t reserved_33_33               : 1;
+	uint64_t bpid7                        : 6;  /**< Select IPD backpressue id for calendar table entry (IDX*8)+7
+                                                         (unused if ENTRY_CTL7 != 0) */
+	uint64_t entry_ctl6                   : 2;  /**< Select source of XON/XOFF for entry (IDX*8)+6
+                                                            - 0: IPD backpressue id
+                                                            - 1: Link
+                                                            - 2: XOFF
+                                                            - 3: XON
+                                                         This field applies to one of bits <49>, <41>, or <25> in the
+                                                         Interlaken control word. */
+	uint64_t reserved_24_24               : 1;
+	uint64_t bpid6                        : 6;  /**< Select IPD backpressue id for calendar table entry (IDX*8)+6
+                                                         (unused if ENTRY_CTL6 != 0) */
+	uint64_t entry_ctl5                   : 2;  /**< Select source of XON/XOFF for entry (IDX*8)+5
+                                                            - 0: IPD backpressue id
+                                                            - 1: Link
+                                                            - 2: XOFF
+                                                            - 3: XON
+                                                         This field applies to one of bits <50>, <42>, or <26> in the
+                                                         Interlaken control word. */
+	uint64_t reserved_15_15               : 1;
+	uint64_t bpid5                        : 6;  /**< Select IPD backpressue id for calendar table entry (IDX*8)+5
+                                                         (unused if ENTRY_CTL5 != 0) */
+	uint64_t entry_ctl4                   : 2;  /**< Select source of XON/XOFF for entry (IDX*8)+4
+                                                            - 0: IPD backpressue id
+                                                            - 1: Link
+                                                            - 2: XOFF
+                                                            - 3: XON
+                                                         This field applies to one of bits <51>, <43>, or <27> in the
+                                                         Interlaken control word. */
+	uint64_t reserved_6_6                 : 1;
+	uint64_t bpid4                        : 6;  /**< Select IPD backpressue id for calendar table entry (IDX*8)+4
+                                                         (unused if ENTRY_CTL4 != 0) */
+#else
+	uint64_t bpid4                        : 6;
+	uint64_t reserved_6_6                 : 1;
+	uint64_t entry_ctl4                   : 2;
+	uint64_t bpid5                        : 6;
+	uint64_t reserved_15_15               : 1;
+	uint64_t entry_ctl5                   : 2;
+	uint64_t bpid6                        : 6;
+	uint64_t reserved_24_24               : 1;
+	uint64_t entry_ctl6                   : 2;
+	uint64_t bpid7                        : 6;
+	uint64_t reserved_33_33               : 1;
+	uint64_t entry_ctl7                   : 2;
+	uint64_t reserved_36_63               : 28;
+#endif
+	} s;
+	struct cvmx_ilk_txx_mem_cal1_s        cn68xx;
+	struct cvmx_ilk_txx_mem_cal1_s        cn68xxp1;
+};
+typedef union cvmx_ilk_txx_mem_cal1 cvmx_ilk_txx_mem_cal1_t;
+
+/**
+ * cvmx_ilk_tx#_mem_pmap
+ */
+union cvmx_ilk_txx_mem_pmap {
+	uint64_t u64;
+	struct cvmx_ilk_txx_mem_pmap_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_17_63               : 47;
+	uint64_t remap                        : 1;  /**< Dynamically select channel using bits[39:32] of an 8-byte
+                                                         header prepended to any packet transmitted on the port-pipe
+                                                         selected by ILK_TXx_IDX_PMAP[IDX].
+
+                                                         ***NOTE: Added in pass 2.0 */
+	uint64_t reserved_8_15                : 8;
+	uint64_t channel                      : 8;  /**< Specify the channel for the port-pipe selected by
+                                                         ILK_TXx_IDX_PMAP[IDX] */
+#else
+	uint64_t channel                      : 8;
+	uint64_t reserved_8_15                : 8;
+	uint64_t remap                        : 1;
+	uint64_t reserved_17_63               : 47;
+#endif
+	} s;
+	struct cvmx_ilk_txx_mem_pmap_s        cn68xx;
+	struct cvmx_ilk_txx_mem_pmap_cn68xxp1 {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_8_63                : 56;
+	uint64_t channel                      : 8;  /**< Specify the channel for the port-pipe selected by
+                                                         ILK_TXx_IDX_PMAP[IDX] */
+#else
+	uint64_t channel                      : 8;
+	uint64_t reserved_8_63                : 56;
+#endif
+	} cn68xxp1;
+};
+typedef union cvmx_ilk_txx_mem_pmap cvmx_ilk_txx_mem_pmap_t;
+
+/**
+ * cvmx_ilk_tx#_mem_stat0
+ */
+union cvmx_ilk_txx_mem_stat0 {
+	uint64_t u64;
+	struct cvmx_ilk_txx_mem_stat0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_28_63               : 36;
+	uint64_t tx_pkt                       : 28; /**< Number of packets transmitted per channel (256M)
+                                                         Channel selected by ILK_TXx_IDX_STAT0[IDX].  Interrupt on
+                                                         saturation if ILK_TXX_INT_EN[STAT_CNT_OVFL]=1. */
+#else
+	uint64_t tx_pkt                       : 28;
+	uint64_t reserved_28_63               : 36;
+#endif
+	} s;
+	struct cvmx_ilk_txx_mem_stat0_s       cn68xx;
+	struct cvmx_ilk_txx_mem_stat0_s       cn68xxp1;
+};
+typedef union cvmx_ilk_txx_mem_stat0 cvmx_ilk_txx_mem_stat0_t;
+
+/**
+ * cvmx_ilk_tx#_mem_stat1
+ */
+union cvmx_ilk_txx_mem_stat1 {
+	uint64_t u64;
+	struct cvmx_ilk_txx_mem_stat1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_36_63               : 28;
+	uint64_t tx_bytes                     : 36; /**< Number of bytes transmitted per channel (64GB) Channel selected
+                                                         by ILK_TXx_IDX_STAT1[IDX].    Saturates.  Interrupt on
+                                                         saturation if ILK_TXX_INT_EN[STAT_CNT_OVFL]=1. */
+#else
+	uint64_t tx_bytes                     : 36;
+	uint64_t reserved_36_63               : 28;
+#endif
+	} s;
+	struct cvmx_ilk_txx_mem_stat1_s       cn68xx;
+	struct cvmx_ilk_txx_mem_stat1_s       cn68xxp1;
+};
+typedef union cvmx_ilk_txx_mem_stat1 cvmx_ilk_txx_mem_stat1_t;
+
+/**
+ * cvmx_ilk_tx#_pipe
+ */
+union cvmx_ilk_txx_pipe {
+	uint64_t u64;
+	struct cvmx_ilk_txx_pipe_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_24_63               : 40;
+	uint64_t nump                         : 8;  /**< Number of pipes assigned to this Tx Link */
+	uint64_t reserved_7_15                : 9;
+	uint64_t base                         : 7;  /**< When NUMP is non-zero, indicates the base pipe number this
+                                                         Tx link will accept.  This Tx will accept PKO packets from
+                                                         pipes in the range of:  BASE .. (BASE+(NUMP-1))
+
+                                                           BASE and NUMP must be constrained such that
+                                                           1) BASE+(NUMP-1) < 127
+                                                           2) Each used PKO pipe must map to exactly
+                                                              one port|channel
+                                                           3) The pipe ranges must be consistent with
+                                                              the PKO configuration. */
+#else
+	uint64_t base                         : 7;
+	uint64_t reserved_7_15                : 9;
+	uint64_t nump                         : 8;
+	uint64_t reserved_24_63               : 40;
+#endif
+	} s;
+	struct cvmx_ilk_txx_pipe_s            cn68xx;
+	struct cvmx_ilk_txx_pipe_s            cn68xxp1;
+};
+typedef union cvmx_ilk_txx_pipe cvmx_ilk_txx_pipe_t;
+
+/**
+ * cvmx_ilk_tx#_pkt_cnt#
+ */
+union cvmx_ilk_txx_pkt_cntx {
+	uint64_t u64;
+	struct cvmx_ilk_txx_pkt_cntx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_34_63               : 30;
+	uint64_t tx_pkt                       : 34; /**< Number of packets transmitted per channel. Wraps on overflow. On overflow, sets
+                                                         ILK_TX()_INT[STAT_CNT_OVFL]. */
+#else
+	uint64_t tx_pkt                       : 34;
+	uint64_t reserved_34_63               : 30;
+#endif
+	} s;
+	struct cvmx_ilk_txx_pkt_cntx_s        cn78xx;
+};
+typedef union cvmx_ilk_txx_pkt_cntx cvmx_ilk_txx_pkt_cntx_t;
+
+/**
+ * cvmx_ilk_tx#_rmatch
+ */
+union cvmx_ilk_txx_rmatch {
+	uint64_t u64;
+	struct cvmx_ilk_txx_rmatch_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_50_63               : 14;
+	uint64_t grnlrty                      : 2;  /**< Reserved. */
+	uint64_t brst_limit                   : 16; /**< Reserved. */
+	uint64_t time_limit                   : 16; /**< Reserved. */
+	uint64_t rate_limit                   : 16; /**< Reserved. */
+#else
+	uint64_t rate_limit                   : 16;
+	uint64_t time_limit                   : 16;
+	uint64_t brst_limit                   : 16;
+	uint64_t grnlrty                      : 2;
+	uint64_t reserved_50_63               : 14;
+#endif
+	} s;
+	struct cvmx_ilk_txx_rmatch_s          cn68xx;
+	struct cvmx_ilk_txx_rmatch_s          cn68xxp1;
+	struct cvmx_ilk_txx_rmatch_s          cn78xx;
+};
+typedef union cvmx_ilk_txx_rmatch cvmx_ilk_txx_rmatch_t;
+
+#endif
diff --git a/arch/mips/include/asm/octeon/cvmx-ilk.h b/arch/mips/include/asm/octeon/cvmx-ilk.h
new file mode 100644
index 0000000..d4a8afe
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-ilk.h
@@ -0,0 +1,226 @@
+/***********************license start***************
+ * Copyright (c) 2003-2014  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * This file contains defines for the ILK interface
+
+ * <hr>$Revision: 49448 $<hr>
+ *
+ *
+ */
+#ifndef __CVMX_ILK_H__
+#define __CVMX_ILK_H__
+
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include <asm/octeon/cvmx.h>
+#endif
+
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+extern "C" {
+/* *INDENT-ON* */
+#endif
+
+/* CSR typedefs have been moved to cvmx-ilk-defs.h */
+
+/*
+ * Note: this macro must match the first ilk port in the ipd_port_map_68xx[]
+ * and ipd_port_map_78xx[] arrays.
+ */
+static inline int CVMX_ILK_GBL_BASE(void) {
+	if (OCTEON_IS_MODEL(OCTEON_CN68XX))
+		return 5;
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+		return 6;
+	return -1;
+}
+static inline int CVMX_ILK_QLM_BASE(void) {
+	if (OCTEON_IS_MODEL(OCTEON_CN68XX))
+		return 1;
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+		return 4;
+	return -1;
+}
+
+typedef struct {
+	int intf_en:1;
+	int la_mode:1;
+	int reserved:14;		/* unused */
+	int lane_speed:16;
+	/* add more here */
+} cvmx_ilk_intf_t;
+
+#define CVMX_NUM_ILK_INTF  2
+static inline int CVMX_ILK_MAX_LANES(void) {
+	if (OCTEON_IS_MODEL(OCTEON_CN68XX))
+		return 8;
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+		return 16;
+	return -1;
+}
+extern CVMX_SHARED unsigned short cvmx_ilk_lane_mask[CVMX_NUM_ILK_INTF];
+
+typedef struct {
+	unsigned int pipe;
+	unsigned int chan;
+} cvmx_ilk_pipe_chan_t;
+
+#define CVMX_ILK_MAX_PIPES 45
+/* Max number of channels allowed */
+#define CVMX_ILK_MAX_CHANS 8	//FIXME: increase for CN78XX !
+
+extern CVMX_SHARED unsigned char cvmx_ilk_chans[CVMX_NUM_ILK_INTF];
+extern unsigned char cvmx_ilk_chan_map[CVMX_NUM_ILK_INTF][CVMX_ILK_MAX_CHANS];
+
+typedef struct {
+	unsigned int chan;
+	unsigned int pknd;
+} cvmx_ilk_chan_pknd_t;
+
+#define CVMX_ILK_MAX_PKNDS 8	/* must be <45 */
+
+typedef struct {
+	unsigned int *chan_list;	/* for discrete channels. or, must be null */
+	unsigned int num_chans;
+
+	unsigned int chan_start;	/* for continuous channels */
+	unsigned int chan_end;
+	unsigned int chan_step;
+
+	unsigned int clr_on_rd;
+} cvmx_ilk_stats_ctrl_t;
+
+#define CVMX_ILK_MAX_CAL      288
+#define CVMX_ILK_MAX_CAL_IDX  (CVMX_ILK_MAX_CAL / 8)
+#define CVMX_ILK_TX_MIN_CAL   1
+#define CVMX_ILK_RX_MIN_CAL   1
+#define CVMX_ILK_CAL_GRP_SZ   8
+#define CVMX_ILK_PIPE_BPID_SZ 7
+#define CVMX_ILK_ENT_CTRL_SZ  2
+#define CVMX_ILK_RX_FIFO_WM   0x200
+
+typedef enum {
+	PIPE_BPID = 0,
+	LINK,
+	XOFF,
+	XON
+} cvmx_ilk_cal_ent_ctrl_t;
+
+typedef struct {
+	unsigned char pipe_bpid;
+	cvmx_ilk_cal_ent_ctrl_t ent_ctrl;
+} cvmx_ilk_cal_entry_t;
+
+typedef enum {
+	CVMX_ILK_LPBK_DISA = 0,
+	CVMX_ILK_LPBK_ENA
+} cvmx_ilk_lpbk_ena_t;
+
+typedef enum {
+	CVMX_ILK_LPBK_INT = 0,
+	CVMX_ILK_LPBK_EXT
+} cvmx_ilk_lpbk_mode_t;
+
+/**
+ * This header is placed in front of all received ILK look-aside mode packets
+ */
+typedef union {
+	uint64_t u64;
+
+	struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint32_t reserved_63_57:7;	// bits 63...57
+		uint32_t nsp_cmd:5;	// bits 56...52
+		uint32_t nsp_flags:4;	// bits 51...48
+		uint32_t nsp_grp_id_upper:6;	// bits 47...42
+		uint32_t reserved_41_40:2;	// bits 41...40
+		uint32_t la_mode:1;	// bit  39      /* Protocol type, 1 for LA mode packet */
+		uint32_t nsp_grp_id_lower:2;	// bits 38...37
+		uint32_t nsp_xid_upper:4;	// bits 36...33
+		uint32_t ilk_channel:1;	// bit  32      /* ILK channel number, 0 or 1 */
+		uint32_t nsp_xid_lower:8;	// bits 31...24
+		uint32_t reserved_23_0:24;	// bits 23...0  /* Unpredictable, may be any value */
+#else
+		uint32_t reserved_23_0:24;	// bits 23...0
+		uint32_t nsp_xid_lower:8;	// bits 31...24
+		uint32_t ilk_channel:1;	// bit  32
+		uint32_t nsp_xid_upper:4;	// bits 36...33
+		uint32_t nsp_grp_id_lower:2;	// bits 38...37
+		uint32_t la_mode:1;	// bit  39
+		uint32_t reserved_41_40:2;	// bits 41...40
+		uint32_t nsp_grp_id_upper:6;	// bits 47...42
+		uint32_t nsp_flags:4;	// bits 51...48
+		uint32_t nsp_cmd:5;	// bits 56...52
+		uint32_t reserved_63_57:7;	// bits 63...57
+#endif
+	} s;
+} cvmx_ilk_la_nsp_compact_hdr_t;
+
+typedef struct cvmx_ilk_LA_mode_struct
+{
+	int ilk_LA_mode;
+	int ilk_LA_mode_cal_ena;
+} cvmx_ilk_LA_mode_t;
+
+extern CVMX_SHARED cvmx_ilk_LA_mode_t cvmx_ilk_LA_mode[CVMX_NUM_ILK_INTF];
+extern int cvmx_ilk_use_la_mode(int interface, int channel);
+
+extern int cvmx_ilk_start_interface(int interface, unsigned short num_lanes);
+extern int cvmx_ilk_start_interface_la(int interface, unsigned char num_lanes);
+extern int cvmx_ilk_set_pipe(int interface, int pipe_base, unsigned int pipe_len);
+extern int cvmx_ilk_tx_set_channel(int interface, cvmx_ilk_pipe_chan_t * pch, unsigned int num_chs);
+extern int cvmx_ilk_rx_set_pknd(int interface, cvmx_ilk_chan_pknd_t * chpknd, unsigned int num_pknd);
+extern int cvmx_ilk_enable(int interface);
+extern int cvmx_ilk_disable(int interface);
+extern int cvmx_ilk_get_intf_ena(int interface);
+extern int cvmx_ilk_get_chan_info(int interface, unsigned char **chans, unsigned char *num_chan);
+extern cvmx_ilk_la_nsp_compact_hdr_t cvmx_ilk_enable_la_header(int ipd_port, int mode);
+extern void cvmx_ilk_show_stats(int interface, cvmx_ilk_stats_ctrl_t * pstats);
+extern int cvmx_ilk_cal_setup_rx(int interface, int cal_depth, cvmx_ilk_cal_entry_t * pent, int hi_wm, unsigned char cal_ena);
+extern int cvmx_ilk_cal_setup_tx(int interface, int cal_depth, cvmx_ilk_cal_entry_t * pent, unsigned char cal_ena);
+extern int cvmx_ilk_lpbk(int interface, cvmx_ilk_lpbk_ena_t enable, cvmx_ilk_lpbk_mode_t mode);
+extern int cvmx_ilk_la_mode_enable_rx_calendar(int interface);
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+}
+/* *INDENT-ON* */
+#endif
+#endif /* __CVMX_ILK_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-mio-defs.h b/arch/mips/include/asm/octeon/cvmx-mio-defs.h
index ea1086c..922189f 100644
--- a/arch/mips/include/asm/octeon/cvmx-mio-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-mio-defs.h
@@ -2950,6 +2950,7 @@ union cvmx_mio_ptp_clock_cfg {
 	struct cvmx_mio_ptp_clock_cfg_cn63xx cn68xxp1;
 	struct cvmx_mio_ptp_clock_cfg_s cnf71xx;
 };
+typedef union cvmx_mio_ptp_clock_cfg cvmx_mio_ptp_clock_cfg_t;
 
 union cvmx_mio_ptp_clock_comp {
 	uint64_t u64;
@@ -3190,6 +3191,7 @@ union cvmx_mio_qlmx_cfg {
 	struct cvmx_mio_qlmx_cfg_cn68xx cn68xxp1;
 	struct cvmx_mio_qlmx_cfg_cn61xx cnf71xx;
 };
+typedef union cvmx_mio_qlmx_cfg cvmx_mio_qlmx_cfg_t;
 
 union cvmx_mio_rst_boot {
 	uint64_t u64;
@@ -3396,6 +3398,7 @@ union cvmx_mio_rst_boot {
 	} cn68xxp1;
 	struct cvmx_mio_rst_boot_cn61xx cnf71xx;
 };
+typedef union cvmx_mio_rst_boot cvmx_mio_rst_boot_t;
 
 union cvmx_mio_rst_cfg {
 	uint64_t u64;
diff --git a/arch/mips/include/asm/octeon/cvmx-packet.h b/arch/mips/include/asm/octeon/cvmx-packet.h
index d472d6a..95215bb 100644
--- a/arch/mips/include/asm/octeon/cvmx-packet.h
+++ b/arch/mips/include/asm/octeon/cvmx-packet.h
@@ -32,6 +32,23 @@
 #ifndef __CVMX_PACKET_H__
 #define __CVMX_PACKET_H__
 
+#include <asm/octeon/cvmx-address.h>
+
+union cvmx_buf_ptr_pki {
+        uint64_t u64;
+        struct {
+                CVMX_BITFIELD_FIELD(uint64_t size:16,
+                /**< The size of the segment pointed to by addr (in bytes) */
+                CVMX_BITFIELD_FIELD(uint64_t packet_outside_wqe:1,
+                /**< sets is packet is not stored in same buffer as WQE*/
+                CVMX_BITFIELD_FIELD(uint64_t rsvd0:5,
+                CVMX_BITFIELD_FIELD(uint64_t addr:42,   /**< Pointer to the first byte of the data, NOT buffer */
+                ))));
+        };
+};
+
+typedef union cvmx_buf_ptr_pki cvmx_buf_ptr_pki_t;
+
 /**
  * This structure defines a buffer pointer on Octeon
  */
@@ -65,5 +82,5 @@ union cvmx_buf_ptr {
 #endif
 	} s;
 };
-
+typedef union cvmx_buf_ptr cvmx_buf_ptr_t;
 #endif /*  __CVMX_PACKET_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-pciercx-defs.h b/arch/mips/include/asm/octeon/cvmx-pciercx-defs.h
index 4bce393..befd4b4 100644
--- a/arch/mips/include/asm/octeon/cvmx-pciercx-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-pciercx-defs.h
@@ -1077,6 +1077,7 @@ union cvmx_pciercx_cfg032 {
 	struct cvmx_pciercx_cfg032_s cn68xxp1;
 	struct cvmx_pciercx_cfg032_s cnf71xx;
 };
+typedef union cvmx_pciercx_cfg032 cvmx_pciercx_cfg032_t;
 
 union cvmx_pciercx_cfg033 {
 	uint32_t u32;
diff --git a/arch/mips/include/asm/octeon/cvmx-pemx-defs.h b/arch/mips/include/asm/octeon/cvmx-pemx-defs.h
index 50a916f..a4fc713 100644
--- a/arch/mips/include/asm/octeon/cvmx-pemx-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-pemx-defs.h
@@ -33,6 +33,7 @@
 #define CVMX_PEMX_BAR_CTL(block_id) (CVMX_ADD_IO_SEG(0x00011800C0000128ull) + ((block_id) & 1) * 0x1000000ull)
 #define CVMX_PEMX_BIST_STATUS(block_id) (CVMX_ADD_IO_SEG(0x00011800C0000018ull) + ((block_id) & 1) * 0x1000000ull)
 #define CVMX_PEMX_BIST_STATUS2(block_id) (CVMX_ADD_IO_SEG(0x00011800C0000420ull) + ((block_id) & 1) * 0x1000000ull)
+#define CVMX_PEMX_CFG(block_id) (CVMX_ADD_IO_SEG(0x00011800C0000410ull) + ((block_id) & 3) * 0x1000000ull)
 #define CVMX_PEMX_CFG_RD(block_id) (CVMX_ADD_IO_SEG(0x00011800C0000030ull) + ((block_id) & 1) * 0x1000000ull)
 #define CVMX_PEMX_CFG_WR(block_id) (CVMX_ADD_IO_SEG(0x00011800C0000028ull) + ((block_id) & 1) * 0x1000000ull)
 #define CVMX_PEMX_CPL_LUT_VALID(block_id) (CVMX_ADD_IO_SEG(0x00011800C0000098ull) + ((block_id) & 1) * 0x1000000ull)
@@ -212,9 +213,110 @@ union cvmx_pemx_cfg_rd {
 	struct cvmx_pemx_cfg_rd_s cn66xx;
 	struct cvmx_pemx_cfg_rd_s cn68xx;
 	struct cvmx_pemx_cfg_rd_s cn68xxp1;
+	struct cvmx_pemx_cfg_rd_s cn78xx;
 	struct cvmx_pemx_cfg_rd_s cnf71xx;
 };
 
+/**
+ * cvmx_pem#_cfg
+ *
+ * Configuration of the PCIe Application.
+ *
+ */
+union cvmx_pemx_cfg {
+	uint64_t u64;
+	struct cvmx_pemx_cfg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_5_63                : 59;
+	uint64_t laneswap                     : 1;  /**< This field enables overwriting the value for lane swapping. The reset value is captured on
+                                                         cold reset by the pin straps (see PEM(0..3)_STRAP.PILANESWAP). When set, lane swapping is
+                                                         performed to/from the SerDes. When clear, no lane swapping is performed. */
+	uint64_t reserved_2_3                 : 2;
+	uint64_t md                           : 2;  /**< This field enables overwriting the value for speed. The reset value is captured on cold
+                                                         reset by the pin straps (see PEM(0..3)_STRAP.PIMODE). For a root complex configuration
+                                                         that is not running at Gen3 speed, the HOSTMD bit of this register must be set when this
+                                                         field is changed.
+                                                         0x0 = Gen1 speed.
+                                                         0x1 = Gen2 speed.
+                                                         0x2 = Gen3 speed.
+                                                         0x3 = Reserved. */
+#else
+	uint64_t md                           : 2;
+	uint64_t reserved_2_3                 : 2;
+	uint64_t laneswap                     : 1;
+	uint64_t reserved_5_63                : 59;
+#endif
+	} s;
+	struct cvmx_pemx_cfg_cn70xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_5_63                : 59;
+	uint64_t laneswap                     : 1;  /**< This field will overwrite the pin setting for lane swapping.
+                                                         When set, lane swapping is performed to/from the SerDes.
+                                                         When clear, no lane swapping is performed. */
+	uint64_t hostmd                       : 1;  /**< This field will overwrite the pin settings for host mode.
+                                                         When set, the PEM is configured to be a Root Complex.
+                                                         When clear, the PEM is configured to be an End Point. */
+	uint64_t md                           : 3;  /**< This field will overwrite the pin settings for speed and lane
+                                                         configuration. This value is used to set the Maximum Link Width
+                                                         field in the core's Link Capabilities Register (CFG031) to
+                                                         indicate the maximum number of lanes supported. Note that less
+                                                         lanes than the specified maximum can be configured for use via
+                                                         the core's Link Control Register (CFG032) Negotiated Link Width
+                                                         field.
+                                                         NOTE - The lower two bits of the MD field must
+                                                         be the same across all configured PEMs!
+                                                           000 - Gen2 Speed, 2-lanes
+                                                           001 - Gen2 Speed, 1-lane
+                                                           010 - Gen2 Speed, 4-lanes
+                                                           011 - Rsvd
+                                                           100 - Gen1 Speed, 2-lanes
+                                                           101 - Gen1 Speed, 1-lane
+                                                           110 - Gen1 Speed, 4-lanes
+                                                           111 - Rsvd */
+#else
+	uint64_t md                           : 3;
+	uint64_t hostmd                       : 1;
+	uint64_t laneswap                     : 1;
+	uint64_t reserved_5_63                : 59;
+#endif
+	} cn70xx;
+	struct cvmx_pemx_cfg_cn70xx           cn70xxp1;
+	struct cvmx_pemx_cfg_cn78xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_5_63                : 59;
+	uint64_t laneswap                     : 1;  /**< This field enables overwriting the value for lane swapping. The reset value is captured on
+                                                         cold reset by the pin straps (see PEM(0..3)_STRAP.PILANESWAP). When set, lane swapping is
+                                                         performed to/from the SerDes. When clear, no lane swapping is performed. */
+	uint64_t lanes8                       : 1;  /**< This field enables overwriting the value for the maximum number of lanes. The reset value
+                                                         is captured on cold reset by the pin straps (see PEM(0..3)_STRAP.PILANES8). When set, the
+                                                         PEM is configured for a maximum of 8 lanes. When clear, the PEM is configured for a
+                                                         maximum of 4 lanes. This value is used to set the maximum link width field in the core's
+                                                         link capabilities register (CFG031) to indicate the maximum number of lanes
+                                                         supported. Note that less lanes than the specified maximum can be configured for use via
+                                                         the core's link control register (CFG032) negotiated link width field. */
+	uint64_t hostmd                       : 1;  /**< This field enables overwriting the value for host mode. The reset value is captured on
+                                                         cold reset by the pin straps (see PEM(0..3)_STRAP.PIMODE). When set, the PEM is configured
+                                                         to be a root complex.
+                                                         When clear, the PEM is configured to be an end point. */
+	uint64_t md                           : 2;  /**< This field enables overwriting the value for speed. The reset value is captured on cold
+                                                         reset by the pin straps (see PEM(0..3)_STRAP.PIMODE). For a root complex configuration
+                                                         that is not running at Gen3 speed, the HOSTMD bit of this register must be set when this
+                                                         field is changed.
+                                                         0x0 = Gen1 speed.
+                                                         0x1 = Gen2 speed.
+                                                         0x2 = Gen3 speed.
+                                                         0x3 = Reserved. */
+#else
+	uint64_t md                           : 2;
+	uint64_t hostmd                       : 1;
+	uint64_t lanes8                       : 1;
+	uint64_t laneswap                     : 1;
+	uint64_t reserved_5_63                : 59;
+#endif
+	} cn78xx;
+};
+typedef union cvmx_pemx_cfg cvmx_pemx_cfg_t;
+
 union cvmx_pemx_cfg_wr {
 	uint64_t u64;
 	struct cvmx_pemx_cfg_wr_s {
diff --git a/arch/mips/include/asm/octeon/cvmx-pip.h b/arch/mips/include/asm/octeon/cvmx-pip.h
index df69bfd..5d78865 100644
--- a/arch/mips/include/asm/octeon/cvmx-pip.h
+++ b/arch/mips/include/asm/octeon/cvmx-pip.h
@@ -33,7 +33,7 @@
 #ifndef __CVMX_PIP_H__
 #define __CVMX_PIP_H__
 
-#include <asm/octeon/cvmx-wqe.h>
+#include <asm/octeon/cvmx-pow.h>
 #include <asm/octeon/cvmx-fpa.h>
 #include <asm/octeon/cvmx-pip-defs.h>
 
diff --git a/arch/mips/include/asm/octeon/cvmx-pki-cluster.h b/arch/mips/include/asm/octeon/cvmx-pki-cluster.h
new file mode 100644
index 0000000..a1ee9c0
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-pki-cluster.h
@@ -0,0 +1,659 @@
+/* This file is autgenerated from obj/ipemainc.elf */
+const int cvmx_pki_cluster_code_length = 655;
+const uint64_t cvmx_pki_cluster_code_default[] = {
+    0x000000000a000000ull,
+    0x0000413a68024070ull,
+    0x0000813800200020ull,
+    0x900081b800200020ull,
+    0x0004da00ffff0001ull,
+    0x000455ab68010b0eull,
+    0x00045fba46010000ull,
+    0x9046898120002000ull,
+    0x0004418068010028ull,
+    0x90665300680100f0ull,
+    0x0004413f68004070ull,
+    0x00065380680100f0ull,
+    0x00045a346803a0f0ull,
+    0x000401b448000001ull,
+    0x00045cb968030870ull,
+    0x0007debd00100010ull,
+    0x0000813b80008000ull,
+    0x0004413b68004070ull,
+    0x9001c00000000000ull,
+    0x9021c00000000000ull,
+    0x00044180680100f0ull,
+    0x0004c639ff000200ull,
+    0x0004400372010000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x000041ba68034078ull,
+    0x0000512268030870ull,
+    0x000041bc68034070ull,
+    0x00005d3a68030870ull,
+    0x00045cb942080000ull,
+    0x0004552a4e09312dull,
+    0x00045cb968082868ull,
+    0x0004410246090000ull,
+    0x0000813800800080ull,
+    0x000401a486000005ull,
+    0x000615ab74000123ull,
+    0x0007122448000004ull,
+    0x0000813901000000ull,
+    0x000481b800010001ull,
+    0x000685b800020002ull,
+    0xa006823800010001ull,
+    0x0006c639ff000500ull,
+    0xa0685f3e68010405ull,
+    0x0008418368010800ull,
+    0xa0485f3e68010305ull,
+    0xa4085f3e68010028ull,
+    0xa441c00000000000ull,
+    0x0009418368010030ull,
+    0xa466400172030001ull,
+    0x00095f3e68030030ull,
+    0x00095f3e68010416ull,
+    0x0006debd00010001ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x00065cb942080000ull,
+    0x0006552a4e09312dull,
+    0x00065cb968082868ull,
+    0x0006410246090000ull,
+    0x9000813901000000ull,
+    0x0004c639ff000a00ull,
+    0x0004400072010000ull,
+    0x00048181ff00ff00ull,
+    0x0007820101000100ull,
+    0x0006898100ff00ffull,
+    0x00048301ffff0180ull,
+    0x0008d5ab10001000ull,
+    0x0004d4a900010001ull,
+    0x0001c00000000000ull,
+    0x00045cb942080000ull,
+    0x9024552a4e09312dull,
+    0x0004c639ff000b00ull,
+    0x90445f80680100f0ull,
+    0x000459b368020070ull,
+    0x000401024000000cull,
+    0x0006823fffffffffull,
+    0x00088281ffffffffull,
+    0x000ad5ab20002000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0004403f72010001ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x000c8b3fffffc200ull,
+    0x000c8b01ffff0001ull,
+    0x000ddebd00020002ull,
+    0x00045cb942080000ull,
+    0x0004552a4e09312dull,
+    0x00045cb968082868ull,
+    0x0004410246090000ull,
+    0x0000813901000000ull,
+    0x000481b800080008ull,
+    0x9846c639ff001200ull,
+    0x9861c00000000000ull,
+    0x00064180680100f0ull,
+    0x0006400372010000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x000683891f000200ull,
+    0x000ed52a00800080ull,
+    0x000e5e3c68020070ull,
+    0x00065cb942080000ull,
+    0x0006552a4e09312dull,
+    0x00065cb968082868ull,
+    0x0006410246090000ull,
+    0x0000813d00020002ull,
+    0x0004893901000000ull,
+    0x9004893800040004ull,
+    0x9024c639ff001300ull,
+    0x00044180680100f0ull,
+    0x9044400372010001ull,
+    0x0001c00000000000ull,
+    0x00045f3e68010044ull,
+    0x0004debd00040004ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x000483891f000200ull,
+    0x000ed52a00800080ull,
+    0x000e5e3c68020070ull,
+    0x00045cb942080000ull,
+    0x0004552a4e09312dull,
+    0x00045cb968082868ull,
+    0x0004410246090000ull,
+    0x000581b902000000ull,
+    0x9826c639ff001800ull,
+    0x9801c00000000000ull,
+    0x00064180680100f0ull,
+    0x0006400172030000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x000682091f000200ull,
+    0x000883aa00800080ull,
+    0x000ed52a00400040ull,
+    0x000e5e3c68020870ull,
+    0x000fd52a00800080ull,
+    0x000f5e3c68020070ull,
+    0x000983891f000000ull,
+    0x000f54a968090148ull,
+    0x000f59b368020870ull,
+    0x00065cb942080000ull,
+    0x0006552a4e09312dull,
+    0x00065cb968082868ull,
+    0x0006410246090000ull,
+    0x000081b902000000ull,
+    0x9826c639ff001900ull,
+    0x9801c00000000000ull,
+    0x00064180680100f0ull,
+    0x0006400172030001ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x000682091f000200ull,
+    0x000883aa00800080ull,
+    0x000ed52a00400040ull,
+    0x000e5e3c68020870ull,
+    0x000fd52a00800080ull,
+    0x000f5e3c68020070ull,
+    0x000983891f000000ull,
+    0x000f54a968090148ull,
+    0x000f59b368020870ull,
+    0x00065cb942080000ull,
+    0x0006552a4e09312dull,
+    0x00065cb968082868ull,
+    0x0006410246090000ull,
+    0x000081b902000000ull,
+    0x9826c639ff001a00ull,
+    0x9801c00000000000ull,
+    0x00064180680100f0ull,
+    0x0006400172030000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x000682091f000200ull,
+    0x000883aa00800080ull,
+    0x000ed52a00400040ull,
+    0x000e5e3c68020870ull,
+    0x000fd52a00800080ull,
+    0x000f5e3c68020070ull,
+    0x000983891f000000ull,
+    0x000f54a968090148ull,
+    0x000f59b368020870ull,
+    0x00065cb942080000ull,
+    0x0006552a4e09312dull,
+    0x00065cb968082868ull,
+    0x0006410246090000ull,
+    0x000081b902000000ull,
+    0x9826c639ff001b00ull,
+    0x9801c00000000000ull,
+    0x00064180680100f0ull,
+    0x0006400172030001ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x000682091f000200ull,
+    0x000883aa00800080ull,
+    0x000ed52a00400040ull,
+    0x000e5e3c68020870ull,
+    0x000fd52a00800080ull,
+    0x000f5e3c68020070ull,
+    0x000983891f000000ull,
+    0x000f54a968090148ull,
+    0x000f59b368020870ull,
+    0x00065cb942080000ull,
+    0x0006552a4e09312dull,
+    0x00065cb968082868ull,
+    0x0006410246090000ull,
+    0x9000813902000000ull,
+    0x000481b800400040ull,
+    0x00068981ffff8847ull,
+    0x00068581ffff8848ull,
+    0x0006debd00080008ull,
+    0x0006c639ff001e00ull,
+    0x0006010240000002ull,
+    0x9801c00000000000ull,
+    0x9821c00000000000ull,
+    0x00065f80680100f0ull,
+    0x0006403f72010000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x00065cb942080000ull,
+    0x0006552a4e09312dull,
+    0x00065cb968082868ull,
+    0x0006010240000004ull,
+    0x0006823902000000ull,
+    0x00065f3e68010629ull,
+    0xac28828101000100ull,
+    0x000b010240000004ull,
+    0xa42b820101000100ull,
+    0x0009010240000004ull,
+    0xac29828101000100ull,
+    0x000b010240000004ull,
+    0xa42b820101000100ull,
+    0x0009010240000004ull,
+    0xac29828101000100ull,
+    0x000b010240000004ull,
+    0x9000813902000000ull,
+    0x0001c00000000000ull,
+    0x00040181840005ffull,
+    0x0006010240000008ull,
+    0x9801c00000000000ull,
+    0x0006debd00200020ull,
+    0x00048181ffff0806ull,
+    0x0006d4a907c00180ull,
+    0x00048201ffff8035ull,
+    0x00068581ffff8035ull,
+    0x0008d4a907c001c0ull,
+    0x0006dcb97c007c00ull,
+    0x00048201ffff0800ull,
+    0x00088601ffff86ddull,
+    0x00068581ffff0800ull,
+    0x00068581ffff86ddull,
+    0x0008d4a907c00200ull,
+    0x0009dcb97c007c00ull,
+    0x0007823d00200020ull,
+    0x000685bd00200020ull,
+    0x0008d4a907c00140ull,
+    0x0006010240000002ull,
+    0x0006593268020070ull,
+    0x000042a486020000ull,
+    0x000a15ab74000124ull,
+    0x9000813904000000ull,
+    0x0001c00000000000ull,
+    0x00048181f0004000ull,
+    0x9886593268020070ull,
+    0x0006d4a907c00200ull,
+    0x00068201ff000000ull,
+    0xa40815ab74000345ull,
+    0x0009debd01000100ull,
+    0xa429418068010038ull,
+    0x00095a3468010870ull,
+    0x0009028386000005ull,
+    0xac8a068186000014ull,
+    0x000a15ab74000343ull,
+    0x000b5a3468010070ull,
+    0xac6b8203000f0005ull,
+    0x0009d4a907c00240ull,
+    0x000b82013fff0000ull,
+    0x0009d52a00010001ull,
+    0x0009d4a9f8006800ull,
+    0x0009593268020870ull,
+    0x0006418068030230ull,
+    0x0006410240030000ull,
+    0x9c01c00000000000ull,
+    0x0001c00000000000ull,
+    0x00078201f0006000ull,
+    0x0008593268020070ull,
+    0xa068d4a907c00280ull,
+    0x00085a3468010874ull,
+    0x0008818100ff0000ull,
+    0x000615ab74000345ull,
+    0x00075a3468010078ull,
+    0x0007010240000028ull,
+    0xa80782b400ff0000ull,
+    0x000ad4a907c002c0ull,
+    0x000a5a3468010078ull,
+    0x000a410244010000ull,
+    0xa80782b400ff003cull,
+    0x000ad4a907c002c0ull,
+    0x000a5a3468010078ull,
+    0x000a410244010000ull,
+    0xa80782b400ff002bull,
+    0x000ad4a907c002c0ull,
+    0x000a5a3468010078ull,
+    0x000a410244010000ull,
+    0xa80782b400ff002cull,
+    0x000ad4a9ffc06ac0ull,
+    0x000a593268020870ull,
+    0x000ad52a00010001ull,
+    0x000a5a3468010078ull,
+    0x000a410244010000ull,
+    0x0007debd01000100ull,
+    0x000481bd01000100ull,
+    0x0006c639ff002300ull,
+    0x000641aa68034000ull,
+    0x000641a968034846ull,
+    0x0006403472030001ull,
+    0x0004822902000200ull,
+    0x000915ab74000341ull,
+    0x000082aa00010001ull,
+    0x000a86ab00ff0045ull,
+    0x000adcb978007800ull,
+    0x0000822902000200ull,
+    0x00088a3908000000ull,
+    0x00065cb942080000ull,
+    0x0006552a4e09312dull,
+    0x00065cb968082868ull,
+    0x0006410246090000ull,
+    0x000042a486020000ull,
+    0xa02a15ab74000343ull,
+    0x000881b400ff0011ull,
+    0x00068981ffff2118ull,
+    0x0006593268020870ull,
+    0x0006d4a9f8009800ull,
+    0xa026debd02000200ull,
+    0x0008813400ff002full,
+    0x00048901ffff6558ull,
+    0x0004593268020870ull,
+    0x0004d4a9f800a800ull,
+    0x0004debd02000200ull,
+    0x000882bd02000200ull,
+    0xa86ac639ff002800ull,
+    0xa841c00000000000ull,
+    0x000a418368010878ull,
+    0x000a400172030000ull,
+    0x000a5bb768030078ull,
+    0x000a5b00680100f0ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x000a5cb942080000ull,
+    0x000a552a4e09312dull,
+    0x000a5cb968082868ull,
+    0x000a410246090000ull,
+    0x00008129f8009800ull,
+    0x00048529f800a800ull,
+    0x9004893910000000ull,
+    0x0001c00000000000ull,
+    0x00048181f0004000ull,
+    0x988658b168020070ull,
+    0x0006d428001f0008ull,
+    0xa4068201ff000000ull,
+    0x000815ab74000545ull,
+    0x0009418068010038ull,
+    0xa429028384000005ull,
+    0x0009debd04000400ull,
+    0xac8a068184000014ull,
+    0x000a15ab74000543ull,
+    0x000b5a3468010070ull,
+    0xac6b8303000f0005ull,
+    0x000dd428001f0009ull,
+    0x000b830120000000ull,
+    0x000c87011fff0000ull,
+    0x000dd42803e001a0ull,
+    0x000d58b168020870ull,
+    0x000ddcb960006000ull,
+    0x0006418068030230ull,
+    0x0006410240030000ull,
+    0x9c01c00000000000ull,
+    0x0001c00000000000ull,
+    0x00078201f0006000ull,
+    0xa06858b168020070ull,
+    0x0008d428001f000aull,
+    0x0008818100ff0000ull,
+    0x000615ab74000545ull,
+    0x00075a3468010078ull,
+    0x0007010240000028ull,
+    0xa80782b400ff0000ull,
+    0x000ad428001f000bull,
+    0x000a5a3468010078ull,
+    0x000a410244010000ull,
+    0xa80782b400ff003cull,
+    0x000ad428001f000bull,
+    0x000a5a3468010078ull,
+    0x000a410244010000ull,
+    0xa80782b400ff002bull,
+    0x000ad428001f000bull,
+    0x000a5a3468010078ull,
+    0x000a410244010000ull,
+    0xa80782b400ff002cull,
+    0x000ad42803ff01abull,
+    0x000adcb960006000ull,
+    0x000a58b168020870ull,
+    0x000a5a3468010078ull,
+    0x0007debd04000400ull,
+    0x000481bd04000400ull,
+    0x0006c639ff002b00ull,
+    0x000641aa68034000ull,
+    0x000641a868034840ull,
+    0x0006403472030001ull,
+    0x0004822902000200ull,
+    0x000915ab74000541ull,
+    0x000082ab00ff0045ull,
+    0x000adcb960006000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x00065cb942080000ull,
+    0x0006552a4e09312dull,
+    0x00065cb968082868ull,
+    0x0006410246090000ull,
+    0x000042a486020000ull,
+    0x000a15ab74000543ull,
+    0x0000813920000000ull,
+    0x000481b400ff006cull,
+    0x0006d42803e001c0ull,
+    0x000658b168020870ull,
+    0x0007823400ff0032ull,
+    0xa048863400ff0033ull,
+    0x0008d42803e00180ull,
+    0xa0685a80680100f0ull,
+    0x000858b168020870ull,
+    0x00085d80680100f0ull,
+    0x986981b400ff002full,
+    0x0006d42803e00280ull,
+    0x00065a80680100f0ull,
+    0x000658b168020870ull,
+    0x000481b400ff0084ull,
+    0x0006d42803e00240ull,
+    0x0004823400ff0011ull,
+    0x0008d42803e00220ull,
+    0x98c481b400ff0006ull,
+    0x0006d42803e00200ull,
+    0x00065ebd68010b31ull,
+    0x000641806801003cull,
+    0x0006028386000005ull,
+    0x000a15ab74000661ull,
+    0x0006418068030230ull,
+    0x0008c180ffff0008ull,
+    0x0008863400ff0006ull,
+    0x0008418240030000ull,
+    0x000842a486030000ull,
+    0x000a15ab74000661ull,
+    0x9028863400ff0084ull,
+    0x0004c639ff003000ull,
+    0x0004403472010000ull,
+    0xa00858b168020870ull,
+    0x00088181ffff0000ull,
+    0x00088281ffff0000ull,
+    0x000615ab74000664ull,
+    0x000a15ab74000664ull,
+    0x000081b940004000ull,
+    0x00045cb942080000ull,
+    0x0004552a4e09312dull,
+    0x00045cb968082868ull,
+    0x0004410246090000ull,
+    0x000483891f000000ull,
+    0x000f542868090a48ull,
+    0x000f583068020070ull,
+    0x000042a486020000ull,
+    0x000a15ab74000661ull,
+    0x000685a803e00000ull,
+    0x000782b800100010ull,
+    0x000a41b168004078ull,
+    0x000a410040030000ull,
+    0x000a41ba68004078ull,
+    0x000a410240030000ull,
+    0xa801c00000000000ull,
+    0xa821c00000000000ull,
+    0x000a4180680100f0ull,
+    0x000ac639ff003900ull,
+    0x000a400372010001ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x000a83891f000000ull,
+    0x000f542868090a48ull,
+    0x000f583068020070ull,
+    0x000a5cb942080000ull,
+    0x000a552a4e09312dull,
+    0x000a5cb968082868ull,
+    0x000a410246090000ull,
+    0x00005fb968004250ull,
+    0x0000003f70000000ull,
+    0x000041b968034070ull,
+    0x0000512268030070ull,
+    0x0000813800200020ull,
+    0x0004413a68024070ull,
+    0x9001c00000000000ull,
+    0x000081b800200020ull,
+    0x9026898180008000ull,
+    0x0004890110001000ull,
+    0x000456ad680100a0ull,
+    0x0006898180008000ull,
+    0x000652a56801001dull,
+    0x000456ad68090b5bull,
+    0x00055680680900f0ull,
+    0x0005debd00400040ull,
+    0x00005600680800f0ull,
+    0x0000833d00200020ull,
+    0x000c872907c00000ull,
+    0x000dd62c20000000ull,
+    0x0000822902800280ull,
+    0x000841b268034070ull,
+    0x000982a8000a000aull,
+    0x000a41b168034070ull,
+    0x000b822907c00000ull,
+    0x0000003f70000800ull,
+    0x000941b268034070ull,
+    0x0000418048030000ull,
+    0x0000018340000008ull,
+    0x0009018348000004ull,
+    0x000050a168030c20ull,
+    0x000082aa00800080ull,
+    0x000850a168080c2bull,
+    0x000752a56808001eull,
+    0x000a822a00400040ull,
+    0x00088a0900010001ull,
+    0x000841bc68034078ull,
+    0x000941bc68034070ull,
+    0x000a583068030870ull,
+    0x0000813d00400000ull,
+    0x0005c180ffff0000ull,
+    0x00058288001e0000ull,
+    0x000b8208001e0008ull,
+    0x00085d2168004030ull,
+    0x00098308001e0010ull,
+    0x00088608001e0010ull,
+    0x000c5d2168004070ull,
+    0x0008418068080025ull,
+    0x000841ba6803a0f0ull,
+    0x000856ad40030000ull,
+    0x0008c180ffff0000ull,
+    0x0005820807000500ull,
+    0x00088a3d00010001ull,
+    0x000841be68004050ull,
+    0x0005828807000300ull,
+    0x000a8abd00040004ull,
+    0x000a41be68004040ull,
+    0x0005820807000100ull,
+    0x00088a2a00800080ull,
+    0x0008413068004078ull,
+    0xa021c00000000000ull,
+    0x0005828807000200ull,
+    0x000841806801002dull,
+    0x000a8abd00080008ull,
+    0x000a41be68004026ull,
+    0x0005820807000400ull,
+    0x00088a2902000200ull,
+    0x000841b46800405aull,
+    0x000556ad40030000ull,
+    0x000081bd00100010ull,
+    0x0006c180ffff0000ull,
+    0x0006822a00800080ull,
+    0x00088a0900100010ull,
+    0x0008413c68024070ull,
+    0xa021c00000000000ull,
+    0x0006832902000200ull,
+    0x0008c181f0008000ull,
+    0x000841834c00ffffull,
+    0x0006822a00400040ull,
+    0x00088a0900200020ull,
+    0x0008413c68024078ull,
+    0xa021c00000000000ull,
+    0x000c8b0900400040ull,
+    0x0008dc01f0008000ull,
+    0x000841b84c03ffffull,
+    0x000c8b2a00010000ull,
+    0x000c41b44c0300ffull,
+    0x000682a9f800a800ull,
+    0x000a86a9f8009800ull,
+    0x000a8a8904000400ull,
+    0x000a41b74c0300ffull,
+    0x000a41b64c03ffffull,
+    0x0000828901000100ull,
+    0x000a822803e00180ull,
+    0x00088a3400ff0033ull,
+    0x000841bb4c03ffffull,
+    0x0008862803e00280ull,
+    0x000841b54c03ffffull,
+    0x000682287c005800ull,
+    0x00088a0902000200ull,
+    0x0008413068024070ull,
+    0xa001c00000000000ull,
+    0x0006830900020002ull,
+    0x00088281e0002000ull,
+    0xa84a868108000800ull,
+    0xa861c00000000000ull,
+    0x000a41814c03ffffull,
+    0x000a41814c03ffffull,
+    0x00065380680300f0ull,
+    0x000c5321680040b0ull,
+    0x000dd3260fff0fffull,
+    0x0006810900800080ull,
+    0x0000003f70000400ull,
+    0x000082a902000200ull,
+    0x000a413268024070ull,
+    0xa50a822902800280ull,
+    0x0004893d08000800ull,
+    0xb1298301ffffffffull,
+    0x00098381f000e000ull,
+    0xa18c8b01ffffffffull,
+    0x000cd5ab80008000ull,
+    0x00088a01ff00ff00ull,
+    0x0008d5ab40004000ull,
+    0x000ed5ab40004000ull,
+    0x0004893d40000000ull,
+    0x00005700680800f0ull,
+    0x00005780680900f0ull,
+    0x0007d72ef1ff0000ull,
+    0x0007d7aff0000000ull,
+    0x0004d72e00fc0000ull,
+    0x0000000008000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull,
+    0x0001c00000000000ull
+};
diff --git a/arch/mips/include/asm/octeon/cvmx-pki-defs.h b/arch/mips/include/asm/octeon/cvmx-pki-defs.h
new file mode 100644
index 0000000..b422e6b
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-pki-defs.h
@@ -0,0 +1,3701 @@
+/***********************license start***************
+ * Copyright (c) 2003-2014  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+
+/**
+ * cvmx-pki-defs.h
+ *
+ * Configuration and status register (CSR) type definitions for
+ * Octeon pki.
+ *
+ * This file is auto generated. Do not edit.
+ *
+ * <hr>$Revision$<hr>
+ *
+ */
+#ifndef __CVMX_PKI_DEFS_H__
+#define __CVMX_PKI_DEFS_H__
+
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKI_ACTIVE0 CVMX_PKI_ACTIVE0_FUNC()
+static inline uint64_t CVMX_PKI_ACTIVE0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKI_ACTIVE0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180044000220ull);
+}
+#else
+#define CVMX_PKI_ACTIVE0 (CVMX_ADD_IO_SEG(0x0001180044000220ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKI_ACTIVE1 CVMX_PKI_ACTIVE1_FUNC()
+static inline uint64_t CVMX_PKI_ACTIVE1_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKI_ACTIVE1 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180044000230ull);
+}
+#else
+#define CVMX_PKI_ACTIVE1 (CVMX_ADD_IO_SEG(0x0001180044000230ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKI_ACTIVE2 CVMX_PKI_ACTIVE2_FUNC()
+static inline uint64_t CVMX_PKI_ACTIVE2_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKI_ACTIVE2 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180044000240ull);
+}
+#else
+#define CVMX_PKI_ACTIVE2 (CVMX_ADD_IO_SEG(0x0001180044000240ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_AURAX_CFG(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKI_AURAX_CFG(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044900000ull) + ((offset) & 1023) * 8;
+}
+#else
+#define CVMX_PKI_AURAX_CFG(offset) (CVMX_ADD_IO_SEG(0x0001180044900000ull) + ((offset) & 1023) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKI_BIST_STATUS0 CVMX_PKI_BIST_STATUS0_FUNC()
+static inline uint64_t CVMX_PKI_BIST_STATUS0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKI_BIST_STATUS0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180044000080ull);
+}
+#else
+#define CVMX_PKI_BIST_STATUS0 (CVMX_ADD_IO_SEG(0x0001180044000080ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKI_BIST_STATUS1 CVMX_PKI_BIST_STATUS1_FUNC()
+static inline uint64_t CVMX_PKI_BIST_STATUS1_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKI_BIST_STATUS1 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180044000088ull);
+}
+#else
+#define CVMX_PKI_BIST_STATUS1 (CVMX_ADD_IO_SEG(0x0001180044000088ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKI_BIST_STATUS2 CVMX_PKI_BIST_STATUS2_FUNC()
+static inline uint64_t CVMX_PKI_BIST_STATUS2_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKI_BIST_STATUS2 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180044000090ull);
+}
+#else
+#define CVMX_PKI_BIST_STATUS2 (CVMX_ADD_IO_SEG(0x0001180044000090ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_BPIDX_STATE(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKI_BPIDX_STATE(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044B00000ull) + ((offset) & 1023) * 8;
+}
+#else
+#define CVMX_PKI_BPIDX_STATE(offset) (CVMX_ADD_IO_SEG(0x0001180044B00000ull) + ((offset) & 1023) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKI_BUF_CTL CVMX_PKI_BUF_CTL_FUNC()
+static inline uint64_t CVMX_PKI_BUF_CTL_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKI_BUF_CTL not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180044000100ull);
+}
+#else
+#define CVMX_PKI_BUF_CTL (CVMX_ADD_IO_SEG(0x0001180044000100ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_CHANX_CFG(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 4095)))))
+		cvmx_warn("CVMX_PKI_CHANX_CFG(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044A00000ull) + ((offset) & 4095) * 8;
+}
+#else
+#define CVMX_PKI_CHANX_CFG(offset) (CVMX_ADD_IO_SEG(0x0001180044A00000ull) + ((offset) & 4095) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKI_CLKEN CVMX_PKI_CLKEN_FUNC()
+static inline uint64_t CVMX_PKI_CLKEN_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKI_CLKEN not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180044000410ull);
+}
+#else
+#define CVMX_PKI_CLKEN (CVMX_ADD_IO_SEG(0x0001180044000410ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_CLX_ECC_CTL(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 3)))))
+		cvmx_warn("CVMX_PKI_CLX_ECC_CTL(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x000118004400C020ull) + ((block_id) & 3) * 0x10000ull;
+}
+#else
+#define CVMX_PKI_CLX_ECC_CTL(block_id) (CVMX_ADD_IO_SEG(0x000118004400C020ull) + ((block_id) & 3) * 0x10000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_CLX_ECC_INT(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 3)))))
+		cvmx_warn("CVMX_PKI_CLX_ECC_INT(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x000118004400C010ull) + ((block_id) & 3) * 0x10000ull;
+}
+#else
+#define CVMX_PKI_CLX_ECC_INT(block_id) (CVMX_ADD_IO_SEG(0x000118004400C010ull) + ((block_id) & 3) * 0x10000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_CLX_INT(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 3)))))
+		cvmx_warn("CVMX_PKI_CLX_INT(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x000118004400C000ull) + ((block_id) & 3) * 0x10000ull;
+}
+#else
+#define CVMX_PKI_CLX_INT(block_id) (CVMX_ADD_IO_SEG(0x000118004400C000ull) + ((block_id) & 3) * 0x10000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_CLX_PCAMX_ACTIONX(unsigned long a, unsigned long b, unsigned long c)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((a <= 3)) && ((b <= 1)) && ((c <= 191))))))
+		cvmx_warn("CVMX_PKI_CLX_PCAMX_ACTIONX(%lu,%lu,%lu) is invalid on this chip\n", a, b, c);
+	return CVMX_ADD_IO_SEG(0x0001180044708000ull) + ((a) << 16) + ((b) << 12) + ((c) << 3);
+}
+#else
+#define CVMX_PKI_CLX_PCAMX_ACTIONX(a, b, c) (CVMX_ADD_IO_SEG(0x0001180044708000ull) + ((a) << 16) + ((b) << 12) + ((c) << 3))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_CLX_PCAMX_MATCHX(unsigned long a, unsigned long b, unsigned long c)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((a <= 3)) && ((b <= 1)) && ((c <= 191))))))
+		cvmx_warn("CVMX_PKI_CLX_PCAMX_MATCHX(%lu,%lu,%lu) is invalid on this chip\n", a, b, c);
+	return CVMX_ADD_IO_SEG(0x0001180044704000ull) + ((a) << 16) + ((b) << 12) + ((c) << 3);
+}
+#else
+#define CVMX_PKI_CLX_PCAMX_MATCHX(a, b, c) (CVMX_ADD_IO_SEG(0x0001180044704000ull) + ((a) << 16) + ((b) << 12) + ((c) << 3))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_CLX_PCAMX_TERMX(unsigned long a, unsigned long b, unsigned long c)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((a <= 3)) && ((b <= 1)) && ((c <= 191))))))
+		cvmx_warn("CVMX_PKI_CLX_PCAMX_TERMX(%lu,%lu,%lu) is invalid on this chip\n", a, b, c);
+	return CVMX_ADD_IO_SEG(0x0001180044700000ull) + ((a) << 16) + ((b) << 12) + ((c) << 3);
+}
+#else
+#define CVMX_PKI_CLX_PCAMX_TERMX(a, b, c) (CVMX_ADD_IO_SEG(0x0001180044700000ull) + ((a) << 16) + ((b) << 12) + ((c) << 3))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_CLX_PKINDX_CFG(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 63)) && ((block_id <= 3))))))
+		cvmx_warn("CVMX_PKI_CLX_PKINDX_CFG(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180044300040ull) + (((offset) & 63) + ((block_id) & 3) * 0x100ull) * 256;
+}
+#else
+#define CVMX_PKI_CLX_PKINDX_CFG(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180044300040ull) + (((offset) & 63) + ((block_id) & 3) * 0x100ull) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_CLX_PKINDX_KMEMX(unsigned long a, unsigned long b, unsigned long c)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((a <= 3)) && ((b <= 63)) && ((c <= 15))))))
+		cvmx_warn("CVMX_PKI_CLX_PKINDX_KMEMX(%lu,%lu,%lu) is invalid on this chip\n", a, b, c);
+	return CVMX_ADD_IO_SEG(0x0001180044200000ull) + ((a) << 16) + ((b) << 8) + ((c) << 3);
+}
+#else
+#define CVMX_PKI_CLX_PKINDX_KMEMX(a, b, c) (CVMX_ADD_IO_SEG(0x0001180044200000ull) + ((a) << 16) + ((b) << 8) + ((c) << 3))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_CLX_PKINDX_L2_CUSTOM(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 63)) && ((block_id <= 3))))))
+		cvmx_warn("CVMX_PKI_CLX_PKINDX_L2_CUSTOM(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180044300058ull) + (((offset) & 63) + ((block_id) & 3) * 0x100ull) * 256;
+}
+#else
+#define CVMX_PKI_CLX_PKINDX_L2_CUSTOM(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180044300058ull) + (((offset) & 63) + ((block_id) & 3) * 0x100ull) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_CLX_PKINDX_LG_CUSTOM(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 63)) && ((block_id <= 3))))))
+		cvmx_warn("CVMX_PKI_CLX_PKINDX_LG_CUSTOM(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180044300060ull) + (((offset) & 63) + ((block_id) & 3) * 0x100ull) * 256;
+}
+#else
+#define CVMX_PKI_CLX_PKINDX_LG_CUSTOM(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180044300060ull) + (((offset) & 63) + ((block_id) & 3) * 0x100ull) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_CLX_PKINDX_SKIP(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 63)) && ((block_id <= 3))))))
+		cvmx_warn("CVMX_PKI_CLX_PKINDX_SKIP(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180044300050ull) + (((offset) & 63) + ((block_id) & 3) * 0x100ull) * 256;
+}
+#else
+#define CVMX_PKI_CLX_PKINDX_SKIP(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180044300050ull) + (((offset) & 63) + ((block_id) & 3) * 0x100ull) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_CLX_PKINDX_STYLE(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 63)) && ((block_id <= 3))))))
+		cvmx_warn("CVMX_PKI_CLX_PKINDX_STYLE(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180044300048ull) + (((offset) & 63) + ((block_id) & 3) * 0x100ull) * 256;
+}
+#else
+#define CVMX_PKI_CLX_PKINDX_STYLE(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180044300048ull) + (((offset) & 63) + ((block_id) & 3) * 0x100ull) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_CLX_SMEMX(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 2047)) && ((block_id <= 3))))))
+		cvmx_warn("CVMX_PKI_CLX_SMEMX(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180044400000ull) + (((offset) & 2047) + ((block_id) & 3) * 0x2000ull) * 8;
+}
+#else
+#define CVMX_PKI_CLX_SMEMX(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180044400000ull) + (((offset) & 2047) + ((block_id) & 3) * 0x2000ull) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_CLX_START(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((block_id <= 3)))))
+		cvmx_warn("CVMX_PKI_CLX_START(%lu) is invalid on this chip\n", block_id);
+	return CVMX_ADD_IO_SEG(0x000118004400C030ull) + ((block_id) & 3) * 0x10000ull;
+}
+#else
+#define CVMX_PKI_CLX_START(block_id) (CVMX_ADD_IO_SEG(0x000118004400C030ull) + ((block_id) & 3) * 0x10000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_CLX_STYLEX_ALG(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 63)) && ((block_id <= 3))))))
+		cvmx_warn("CVMX_PKI_CLX_STYLEX_ALG(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180044501000ull) + (((offset) & 63) + ((block_id) & 3) * 0x2000ull) * 8;
+}
+#else
+#define CVMX_PKI_CLX_STYLEX_ALG(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180044501000ull) + (((offset) & 63) + ((block_id) & 3) * 0x2000ull) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_CLX_STYLEX_CFG(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 63)) && ((block_id <= 3))))))
+		cvmx_warn("CVMX_PKI_CLX_STYLEX_CFG(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180044500000ull) + (((offset) & 63) + ((block_id) & 3) * 0x2000ull) * 8;
+}
+#else
+#define CVMX_PKI_CLX_STYLEX_CFG(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180044500000ull) + (((offset) & 63) + ((block_id) & 3) * 0x2000ull) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_CLX_STYLEX_CFG2(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && (((offset <= 63)) && ((block_id <= 3))))))
+		cvmx_warn("CVMX_PKI_CLX_STYLEX_CFG2(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return CVMX_ADD_IO_SEG(0x0001180044500800ull) + (((offset) & 63) + ((block_id) & 3) * 0x2000ull) * 8;
+}
+#else
+#define CVMX_PKI_CLX_STYLEX_CFG2(offset, block_id) (CVMX_ADD_IO_SEG(0x0001180044500800ull) + (((offset) & 63) + ((block_id) & 3) * 0x2000ull) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKI_ECC_CTL0 CVMX_PKI_ECC_CTL0_FUNC()
+static inline uint64_t CVMX_PKI_ECC_CTL0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKI_ECC_CTL0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180044000060ull);
+}
+#else
+#define CVMX_PKI_ECC_CTL0 (CVMX_ADD_IO_SEG(0x0001180044000060ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKI_ECC_CTL1 CVMX_PKI_ECC_CTL1_FUNC()
+static inline uint64_t CVMX_PKI_ECC_CTL1_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKI_ECC_CTL1 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180044000068ull);
+}
+#else
+#define CVMX_PKI_ECC_CTL1 (CVMX_ADD_IO_SEG(0x0001180044000068ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKI_ECC_CTL2 CVMX_PKI_ECC_CTL2_FUNC()
+static inline uint64_t CVMX_PKI_ECC_CTL2_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKI_ECC_CTL2 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180044000070ull);
+}
+#else
+#define CVMX_PKI_ECC_CTL2 (CVMX_ADD_IO_SEG(0x0001180044000070ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKI_ECC_INT0 CVMX_PKI_ECC_INT0_FUNC()
+static inline uint64_t CVMX_PKI_ECC_INT0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKI_ECC_INT0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180044000040ull);
+}
+#else
+#define CVMX_PKI_ECC_INT0 (CVMX_ADD_IO_SEG(0x0001180044000040ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKI_ECC_INT1 CVMX_PKI_ECC_INT1_FUNC()
+static inline uint64_t CVMX_PKI_ECC_INT1_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKI_ECC_INT1 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180044000048ull);
+}
+#else
+#define CVMX_PKI_ECC_INT1 (CVMX_ADD_IO_SEG(0x0001180044000048ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKI_ECC_INT2 CVMX_PKI_ECC_INT2_FUNC()
+static inline uint64_t CVMX_PKI_ECC_INT2_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKI_ECC_INT2 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180044000050ull);
+}
+#else
+#define CVMX_PKI_ECC_INT2 (CVMX_ADD_IO_SEG(0x0001180044000050ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_FRM_LEN_CHKX(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_PKI_FRM_LEN_CHKX(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044004000ull) + ((offset) & 1) * 8;
+}
+#else
+#define CVMX_PKI_FRM_LEN_CHKX(offset) (CVMX_ADD_IO_SEG(0x0001180044004000ull) + ((offset) & 1) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKI_GBL_PEN CVMX_PKI_GBL_PEN_FUNC()
+static inline uint64_t CVMX_PKI_GBL_PEN_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKI_GBL_PEN not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180044000200ull);
+}
+#else
+#define CVMX_PKI_GBL_PEN (CVMX_ADD_IO_SEG(0x0001180044000200ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKI_GEN_INT CVMX_PKI_GEN_INT_FUNC()
+static inline uint64_t CVMX_PKI_GEN_INT_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKI_GEN_INT not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180044000020ull);
+}
+#else
+#define CVMX_PKI_GEN_INT (CVMX_ADD_IO_SEG(0x0001180044000020ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_ICGX_CFG(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 3)))))
+		cvmx_warn("CVMX_PKI_ICGX_CFG(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x000118004400A000ull) + ((offset) & 3) * 8;
+}
+#else
+#define CVMX_PKI_ICGX_CFG(offset) (CVMX_ADD_IO_SEG(0x000118004400A000ull) + ((offset) & 3) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_IMEMX(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 2047)))))
+		cvmx_warn("CVMX_PKI_IMEMX(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044100000ull) + ((offset) & 2047) * 8;
+}
+#else
+#define CVMX_PKI_IMEMX(offset) (CVMX_ADD_IO_SEG(0x0001180044100000ull) + ((offset) & 2047) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_LTYPEX_MAP(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
+		cvmx_warn("CVMX_PKI_LTYPEX_MAP(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044005000ull) + ((offset) & 31) * 8;
+}
+#else
+#define CVMX_PKI_LTYPEX_MAP(offset) (CVMX_ADD_IO_SEG(0x0001180044005000ull) + ((offset) & 31) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKI_PCAM_LOOKUP CVMX_PKI_PCAM_LOOKUP_FUNC()
+static inline uint64_t CVMX_PKI_PCAM_LOOKUP_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKI_PCAM_LOOKUP not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180044000500ull);
+}
+#else
+#define CVMX_PKI_PCAM_LOOKUP (CVMX_ADD_IO_SEG(0x0001180044000500ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKI_PCAM_RESULT CVMX_PKI_PCAM_RESULT_FUNC()
+static inline uint64_t CVMX_PKI_PCAM_RESULT_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKI_PCAM_RESULT not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180044000510ull);
+}
+#else
+#define CVMX_PKI_PCAM_RESULT (CVMX_ADD_IO_SEG(0x0001180044000510ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKI_PFE_DIAG CVMX_PKI_PFE_DIAG_FUNC()
+static inline uint64_t CVMX_PKI_PFE_DIAG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKI_PFE_DIAG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180044000560ull);
+}
+#else
+#define CVMX_PKI_PFE_DIAG (CVMX_ADD_IO_SEG(0x0001180044000560ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKI_PIX_CLKEN CVMX_PKI_PIX_CLKEN_FUNC()
+static inline uint64_t CVMX_PKI_PIX_CLKEN_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKI_PIX_CLKEN not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180044000600ull);
+}
+#else
+#define CVMX_PKI_PIX_CLKEN (CVMX_ADD_IO_SEG(0x0001180044000600ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKI_PIX_DIAG CVMX_PKI_PIX_DIAG_FUNC()
+static inline uint64_t CVMX_PKI_PIX_DIAG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKI_PIX_DIAG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180044000580ull);
+}
+#else
+#define CVMX_PKI_PIX_DIAG (CVMX_ADD_IO_SEG(0x0001180044000580ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_PKINDX_ICGSEL(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_PKINDX_ICGSEL(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044010000ull) + ((offset) & 63) * 8;
+}
+#else
+#define CVMX_PKI_PKINDX_ICGSEL(offset) (CVMX_ADD_IO_SEG(0x0001180044010000ull) + ((offset) & 63) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_PKNDX_INB_STAT0(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_PKNDX_INB_STAT0(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044F00000ull) + ((offset) & 63) * 256;
+}
+#else
+#define CVMX_PKI_PKNDX_INB_STAT0(offset) (CVMX_ADD_IO_SEG(0x0001180044F00000ull) + ((offset) & 63) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_PKNDX_INB_STAT1(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_PKNDX_INB_STAT1(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044F00008ull) + ((offset) & 63) * 256;
+}
+#else
+#define CVMX_PKI_PKNDX_INB_STAT1(offset) (CVMX_ADD_IO_SEG(0x0001180044F00008ull) + ((offset) & 63) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_PKNDX_INB_STAT2(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_PKNDX_INB_STAT2(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044F00010ull) + ((offset) & 63) * 256;
+}
+#else
+#define CVMX_PKI_PKNDX_INB_STAT2(offset) (CVMX_ADD_IO_SEG(0x0001180044F00010ull) + ((offset) & 63) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKI_PKT_ERR CVMX_PKI_PKT_ERR_FUNC()
+static inline uint64_t CVMX_PKI_PKT_ERR_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKI_PKT_ERR not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180044000030ull);
+}
+#else
+#define CVMX_PKI_PKT_ERR (CVMX_ADD_IO_SEG(0x0001180044000030ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_QPG_TBLX(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 2047)))))
+		cvmx_warn("CVMX_PKI_QPG_TBLX(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044800000ull) + ((offset) & 2047) * 8;
+}
+#else
+#define CVMX_PKI_QPG_TBLX(offset) (CVMX_ADD_IO_SEG(0x0001180044800000ull) + ((offset) & 2047) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_REASM_SOPX(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_PKI_REASM_SOPX(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044006000ull) + ((offset) & 1) * 8;
+}
+#else
+#define CVMX_PKI_REASM_SOPX(offset) (CVMX_ADD_IO_SEG(0x0001180044006000ull) + ((offset) & 1) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKI_REQ_WGT CVMX_PKI_REQ_WGT_FUNC()
+static inline uint64_t CVMX_PKI_REQ_WGT_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKI_REQ_WGT not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180044000120ull);
+}
+#else
+#define CVMX_PKI_REQ_WGT (CVMX_ADD_IO_SEG(0x0001180044000120ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKI_SFT_RST CVMX_PKI_SFT_RST_FUNC()
+static inline uint64_t CVMX_PKI_SFT_RST_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKI_SFT_RST not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180044000010ull);
+}
+#else
+#define CVMX_PKI_SFT_RST (CVMX_ADD_IO_SEG(0x0001180044000010ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STATX_HIST0(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STATX_HIST0(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044E00000ull) + ((offset) & 63) * 256;
+}
+#else
+#define CVMX_PKI_STATX_HIST0(offset) (CVMX_ADD_IO_SEG(0x0001180044E00000ull) + ((offset) & 63) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STATX_HIST1(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STATX_HIST1(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044E00008ull) + ((offset) & 63) * 256;
+}
+#else
+#define CVMX_PKI_STATX_HIST1(offset) (CVMX_ADD_IO_SEG(0x0001180044E00008ull) + ((offset) & 63) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STATX_HIST2(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STATX_HIST2(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044E00010ull) + ((offset) & 63) * 256;
+}
+#else
+#define CVMX_PKI_STATX_HIST2(offset) (CVMX_ADD_IO_SEG(0x0001180044E00010ull) + ((offset) & 63) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STATX_HIST3(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STATX_HIST3(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044E00018ull) + ((offset) & 63) * 256;
+}
+#else
+#define CVMX_PKI_STATX_HIST3(offset) (CVMX_ADD_IO_SEG(0x0001180044E00018ull) + ((offset) & 63) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STATX_HIST4(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STATX_HIST4(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044E00020ull) + ((offset) & 63) * 256;
+}
+#else
+#define CVMX_PKI_STATX_HIST4(offset) (CVMX_ADD_IO_SEG(0x0001180044E00020ull) + ((offset) & 63) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STATX_HIST5(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STATX_HIST5(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044E00028ull) + ((offset) & 63) * 256;
+}
+#else
+#define CVMX_PKI_STATX_HIST5(offset) (CVMX_ADD_IO_SEG(0x0001180044E00028ull) + ((offset) & 63) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STATX_HIST6(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STATX_HIST6(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044E00030ull) + ((offset) & 63) * 256;
+}
+#else
+#define CVMX_PKI_STATX_HIST6(offset) (CVMX_ADD_IO_SEG(0x0001180044E00030ull) + ((offset) & 63) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STATX_STAT0(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STATX_STAT0(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044E00038ull) + ((offset) & 63) * 256;
+}
+#else
+#define CVMX_PKI_STATX_STAT0(offset) (CVMX_ADD_IO_SEG(0x0001180044E00038ull) + ((offset) & 63) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STATX_STAT1(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STATX_STAT1(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044E00040ull) + ((offset) & 63) * 256;
+}
+#else
+#define CVMX_PKI_STATX_STAT1(offset) (CVMX_ADD_IO_SEG(0x0001180044E00040ull) + ((offset) & 63) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STATX_STAT10(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STATX_STAT10(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044E00088ull) + ((offset) & 63) * 256;
+}
+#else
+#define CVMX_PKI_STATX_STAT10(offset) (CVMX_ADD_IO_SEG(0x0001180044E00088ull) + ((offset) & 63) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STATX_STAT11(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STATX_STAT11(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044E00090ull) + ((offset) & 63) * 256;
+}
+#else
+#define CVMX_PKI_STATX_STAT11(offset) (CVMX_ADD_IO_SEG(0x0001180044E00090ull) + ((offset) & 63) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STATX_STAT12(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STATX_STAT12(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044E00098ull) + ((offset) & 63) * 256;
+}
+#else
+#define CVMX_PKI_STATX_STAT12(offset) (CVMX_ADD_IO_SEG(0x0001180044E00098ull) + ((offset) & 63) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STATX_STAT13(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STATX_STAT13(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044E000A0ull) + ((offset) & 63) * 256;
+}
+#else
+#define CVMX_PKI_STATX_STAT13(offset) (CVMX_ADD_IO_SEG(0x0001180044E000A0ull) + ((offset) & 63) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STATX_STAT14(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STATX_STAT14(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044E000A8ull) + ((offset) & 63) * 256;
+}
+#else
+#define CVMX_PKI_STATX_STAT14(offset) (CVMX_ADD_IO_SEG(0x0001180044E000A8ull) + ((offset) & 63) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STATX_STAT15(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STATX_STAT15(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044E000B0ull) + ((offset) & 63) * 256;
+}
+#else
+#define CVMX_PKI_STATX_STAT15(offset) (CVMX_ADD_IO_SEG(0x0001180044E000B0ull) + ((offset) & 63) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STATX_STAT16(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STATX_STAT16(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044E000B8ull) + ((offset) & 63) * 256;
+}
+#else
+#define CVMX_PKI_STATX_STAT16(offset) (CVMX_ADD_IO_SEG(0x0001180044E000B8ull) + ((offset) & 63) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STATX_STAT17(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STATX_STAT17(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044E000C0ull) + ((offset) & 63) * 256;
+}
+#else
+#define CVMX_PKI_STATX_STAT17(offset) (CVMX_ADD_IO_SEG(0x0001180044E000C0ull) + ((offset) & 63) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STATX_STAT18(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STATX_STAT18(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044E000C8ull) + ((offset) & 63) * 256;
+}
+#else
+#define CVMX_PKI_STATX_STAT18(offset) (CVMX_ADD_IO_SEG(0x0001180044E000C8ull) + ((offset) & 63) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STATX_STAT2(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STATX_STAT2(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044E00048ull) + ((offset) & 63) * 256;
+}
+#else
+#define CVMX_PKI_STATX_STAT2(offset) (CVMX_ADD_IO_SEG(0x0001180044E00048ull) + ((offset) & 63) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STATX_STAT3(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STATX_STAT3(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044E00050ull) + ((offset) & 63) * 256;
+}
+#else
+#define CVMX_PKI_STATX_STAT3(offset) (CVMX_ADD_IO_SEG(0x0001180044E00050ull) + ((offset) & 63) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STATX_STAT4(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STATX_STAT4(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044E00058ull) + ((offset) & 63) * 256;
+}
+#else
+#define CVMX_PKI_STATX_STAT4(offset) (CVMX_ADD_IO_SEG(0x0001180044E00058ull) + ((offset) & 63) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STATX_STAT5(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STATX_STAT5(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044E00060ull) + ((offset) & 63) * 256;
+}
+#else
+#define CVMX_PKI_STATX_STAT5(offset) (CVMX_ADD_IO_SEG(0x0001180044E00060ull) + ((offset) & 63) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STATX_STAT6(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STATX_STAT6(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044E00068ull) + ((offset) & 63) * 256;
+}
+#else
+#define CVMX_PKI_STATX_STAT6(offset) (CVMX_ADD_IO_SEG(0x0001180044E00068ull) + ((offset) & 63) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STATX_STAT7(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STATX_STAT7(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044E00070ull) + ((offset) & 63) * 256;
+}
+#else
+#define CVMX_PKI_STATX_STAT7(offset) (CVMX_ADD_IO_SEG(0x0001180044E00070ull) + ((offset) & 63) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STATX_STAT8(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STATX_STAT8(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044E00078ull) + ((offset) & 63) * 256;
+}
+#else
+#define CVMX_PKI_STATX_STAT8(offset) (CVMX_ADD_IO_SEG(0x0001180044E00078ull) + ((offset) & 63) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STATX_STAT9(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STATX_STAT9(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044E00080ull) + ((offset) & 63) * 256;
+}
+#else
+#define CVMX_PKI_STATX_STAT9(offset) (CVMX_ADD_IO_SEG(0x0001180044E00080ull) + ((offset) & 63) * 256)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKI_STAT_CTL CVMX_PKI_STAT_CTL_FUNC()
+static inline uint64_t CVMX_PKI_STAT_CTL_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKI_STAT_CTL not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180044000110ull);
+}
+#else
+#define CVMX_PKI_STAT_CTL (CVMX_ADD_IO_SEG(0x0001180044000110ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STYLEX_BUF(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STYLEX_BUF(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044024000ull) + ((offset) & 63) * 8;
+}
+#else
+#define CVMX_PKI_STYLEX_BUF(offset) (CVMX_ADD_IO_SEG(0x0001180044024000ull) + ((offset) & 63) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STYLEX_TAG_MASK(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STYLEX_TAG_MASK(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044021000ull) + ((offset) & 63) * 8;
+}
+#else
+#define CVMX_PKI_STYLEX_TAG_MASK(offset) (CVMX_ADD_IO_SEG(0x0001180044021000ull) + ((offset) & 63) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STYLEX_TAG_SEL(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STYLEX_TAG_SEL(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044020000ull) + ((offset) & 63) * 8;
+}
+#else
+#define CVMX_PKI_STYLEX_TAG_SEL(offset) (CVMX_ADD_IO_SEG(0x0001180044020000ull) + ((offset) & 63) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STYLEX_WQ2(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STYLEX_WQ2(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044022000ull) + ((offset) & 63) * 8;
+}
+#else
+#define CVMX_PKI_STYLEX_WQ2(offset) (CVMX_ADD_IO_SEG(0x0001180044022000ull) + ((offset) & 63) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_STYLEX_WQ4(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_PKI_STYLEX_WQ4(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044023000ull) + ((offset) & 63) * 8;
+}
+#else
+#define CVMX_PKI_STYLEX_WQ4(offset) (CVMX_ADD_IO_SEG(0x0001180044023000ull) + ((offset) & 63) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_TAG_INCX_CTL(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
+		cvmx_warn("CVMX_PKI_TAG_INCX_CTL(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044007000ull) + ((offset) & 31) * 8;
+}
+#else
+#define CVMX_PKI_TAG_INCX_CTL(offset) (CVMX_ADD_IO_SEG(0x0001180044007000ull) + ((offset) & 31) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKI_TAG_INCX_MASK(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
+		cvmx_warn("CVMX_PKI_TAG_INCX_MASK(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180044008000ull) + ((offset) & 31) * 8;
+}
+#else
+#define CVMX_PKI_TAG_INCX_MASK(offset) (CVMX_ADD_IO_SEG(0x0001180044008000ull) + ((offset) & 31) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKI_TAG_SECRET CVMX_PKI_TAG_SECRET_FUNC()
+static inline uint64_t CVMX_PKI_TAG_SECRET_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKI_TAG_SECRET not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180044000430ull);
+}
+#else
+#define CVMX_PKI_TAG_SECRET (CVMX_ADD_IO_SEG(0x0001180044000430ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKI_X2P_REQ_OFL CVMX_PKI_X2P_REQ_OFL_FUNC()
+static inline uint64_t CVMX_PKI_X2P_REQ_OFL_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKI_X2P_REQ_OFL not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180044000038ull);
+}
+#else
+#define CVMX_PKI_X2P_REQ_OFL (CVMX_ADD_IO_SEG(0x0001180044000038ull))
+#endif
+
+/**
+ * cvmx_pki_active0
+ */
+union cvmx_pki_active0 {
+	uint64_t u64;
+	struct cvmx_pki_active0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t pfe_active                   : 1;  /**< PFE active. For internal use; software should use PKI_SFT_RST[ACTIVE]. */
+#else
+	uint64_t pfe_active                   : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_pki_active0_s             cn78xx;
+};
+typedef union cvmx_pki_active0 cvmx_pki_active0_t;
+
+/**
+ * cvmx_pki_active1
+ */
+union cvmx_pki_active1 {
+	uint64_t u64;
+	struct cvmx_pki_active1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t fpc_active                   : 1;  /**< PBE FPC and FPA bus active. For internal use; software should use PKI_SFT_RST[ACTIVE]. */
+	uint64_t iobp_active                  : 1;  /**< PBE PMW and IOBP bus active. For internal use; software should use PKI_SFT_RST[ACTIVE]. */
+	uint64_t sws_active                   : 1;  /**< PBE SWS active. For internal use; software should use PKI_SFT_RST[ACTIVE]. */
+	uint64_t pbtag_active                 : 1;  /**< PBE pbtags active. For internal use; software should use PKI_SFT_RST[ACTIVE]. */
+#else
+	uint64_t pbtag_active                 : 1;
+	uint64_t sws_active                   : 1;
+	uint64_t iobp_active                  : 1;
+	uint64_t fpc_active                   : 1;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_pki_active1_s             cn78xx;
+};
+typedef union cvmx_pki_active1 cvmx_pki_active1_t;
+
+/**
+ * cvmx_pki_active2
+ */
+union cvmx_pki_active2 {
+	uint64_t u64;
+	struct cvmx_pki_active2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_5_63                : 59;
+	uint64_t pix_active                   : 5;  /**< PIX control and ICG active. For internal use; software should use PKI_SFT_RST[ACTIVE]. */
+#else
+	uint64_t pix_active                   : 5;
+	uint64_t reserved_5_63                : 59;
+#endif
+	} s;
+	struct cvmx_pki_active2_s             cn78xx;
+};
+typedef union cvmx_pki_active2 cvmx_pki_active2_t;
+
+/**
+ * cvmx_pki_aura#_cfg
+ *
+ * This register configures aura backpressure, etc; see Backpressure.
+ *
+ */
+union cvmx_pki_aurax_cfg {
+	uint64_t u64;
+	struct cvmx_pki_aurax_cfg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_32_63               : 32;
+	uint64_t pkt_add                      : 2;  /**< Specifies what to add to FPA_AURA()_CNT when PKI enqueues a packet:
+                                                         0 = zero.
+                                                         1 = one.
+                                                         2 = The number of FPA buffers allocated; i.e. if PKI_STYLE()_BUF[DIS_WQ_DAT] is set,
+                                                         WQE[BUFS]+1, else WQE[BUFS].
+                                                         3 = WQE[LEN] (i.e. the packet length). */
+	uint64_t reserved_19_29               : 11;
+	uint64_t ena_red                      : 1;  /**< Enable RED drop between PASS and DROP levels. See also
+                                                         FPA_AURA()_POOL_LEVELS[RED_ENA] and FPA_AURA()_CNT_LEVELS[RED_ENA]. */
+	uint64_t ena_drop                     : 1;  /**< Enable tail drop when maximum DROP level exceeded. See also
+                                                         FPA_AURA()_POOL_LEVELS[DROP] and FPA_AURA()_CNT_LEVELS[DROP]. */
+	uint64_t ena_bp                       : 1;  /**< Enable asserting backpressure on BPID when maximum DROP level exceeded. See also
+                                                         FPA_AURA()_POOL_LEVELS[RED_ENA] and FPA_AURA()_CNT_LEVELS[RED_ENA]. */
+	uint64_t reserved_10_15               : 6;
+	uint64_t bpid                         : 10; /**< Bpid to assert backpressure on. */
+#else
+	uint64_t bpid                         : 10;
+	uint64_t reserved_10_15               : 6;
+	uint64_t ena_bp                       : 1;
+	uint64_t ena_drop                     : 1;
+	uint64_t ena_red                      : 1;
+	uint64_t reserved_19_29               : 11;
+	uint64_t pkt_add                      : 2;
+	uint64_t reserved_32_63               : 32;
+#endif
+	} s;
+	struct cvmx_pki_aurax_cfg_s           cn78xx;
+};
+typedef union cvmx_pki_aurax_cfg cvmx_pki_aurax_cfg_t;
+
+/**
+ * cvmx_pki_bist_status0
+ *
+ * BIST status register.
+ *
+ */
+union cvmx_pki_bist_status0 {
+	uint64_t u64;
+	struct cvmx_pki_bist_status0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_31_63               : 33;
+	uint64_t bist                         : 31; /**< BIST results. Hardware sets a bit in BIST for memory that fails BIST. INTERNAL: This
+                                                         register collects status for PKI_PFE.
+                                                         <30> = INB_ERRS.
+                                                         <29> = INB OCTS.
+                                                         <28> = INB PKTS.
+                                                         <27> = LD FIF.
+                                                         <26> = PBE STATE.
+                                                         <25> = WADR STATE.
+                                                         <24> = NXT PTAG.
+                                                         <23> = CUR PTAG.
+                                                         <22> = X2P FIF.
+                                                         <21> = DROP FIF.
+                                                         <20> = NXT BLK.
+                                                         <19..16> = KMEM.
+                                                         <15..0> = ASM BUFF. */
+#else
+	uint64_t bist                         : 31;
+	uint64_t reserved_31_63               : 33;
+#endif
+	} s;
+	struct cvmx_pki_bist_status0_s        cn78xx;
+};
+typedef union cvmx_pki_bist_status0 cvmx_pki_bist_status0_t;
+
+/**
+ * cvmx_pki_bist_status1
+ *
+ * BIST status register.
+ *
+ */
+union cvmx_pki_bist_status1 {
+	uint64_t u64;
+	struct cvmx_pki_bist_status1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_21_63               : 43;
+	uint64_t bist                         : 21; /**< BIST results. Hardware sets a bit in BIST for memory that fails BIST. INTERNAL: This
+                                                         register collects status for PKI_PBE.
+                                                         <20> = STATS_MEM0.
+                                                         <19> = STATS_MEM1.
+                                                         <18> = STATS_MEM2.
+                                                         <17> = STATS_MEM3.
+                                                         <16> = SWS.
+                                                         <15> = WQEOUT.
+                                                         <14> = DOA.
+                                                         <13> = BPID.
+                                                         <12 =10> = Reserved.
+                                                         <9> = PLC.
+                                                         <8> = PKTWQ.
+                                                         <7 =6> = Reserved.
+                                                         <5> = TAG.
+                                                         <4> = AURA.
+                                                         <3> = CHAN.
+                                                         <2> = PBTAG.
+                                                         <1> = STYLEWQ.
+                                                         <0> = QPG. */
+#else
+	uint64_t bist                         : 21;
+	uint64_t reserved_21_63               : 43;
+#endif
+	} s;
+	struct cvmx_pki_bist_status1_s        cn78xx;
+};
+typedef union cvmx_pki_bist_status1 cvmx_pki_bist_status1_t;
+
+/**
+ * cvmx_pki_bist_status2
+ *
+ * BIST status register.
+ *
+ */
+union cvmx_pki_bist_status2 {
+	uint64_t u64;
+	struct cvmx_pki_bist_status2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_25_63               : 39;
+	uint64_t bist                         : 25; /**< BIST results. Hardware sets a bit in BIST for memory that fails BIST. INTERNAL: This
+                                                         register collects status for PKI_PIX (verif/vkits/pki/pki_mem_info_table.sv).
+                                                         24 = IMEM.
+                                                         23 = IPEC3 / IPEs 10 .. 19 (RegFile + DMEM).
+                                                         22 = IPEC3 / IPEs  0 ..  9 (RegFile + DMEM).
+                                                         21 = IPEC2 / IPEs 10 .. 19 (RegFile + DMEM).
+                                                         20 = IPEC2 / IPEs  0 ..  9 (RegFile + DMEM).
+                                                         19 = IPEC1 / IPEs 10 .. 19 (RegFile + DMEM).
+                                                         18 = IPEC1 / IPEs  0 ..  9 (RegFile + DMEM).
+                                                         17 = IPEC0 / IPEs 10 .. 19 (RegFile + DMEM).
+                                                         16 = IPEC0 / IPEs  0 ..  9 (RegFile + DMEM).
+                                                         15..12 = IPEC SMEM.
+                                                         11..8 = IPEC PCAM ECC.
+                                                         7..4 = IPEC PCAM RES.
+                                                         3..0 = IPEC PCAM CAM. */
+#else
+	uint64_t bist                         : 25;
+	uint64_t reserved_25_63               : 39;
+#endif
+	} s;
+	struct cvmx_pki_bist_status2_s        cn78xx;
+};
+typedef union cvmx_pki_bist_status2 cvmx_pki_bist_status2_t;
+
+/**
+ * cvmx_pki_bpid#_state
+ *
+ * This register shows the current bpid state for diagnostics.
+ *
+ */
+union cvmx_pki_bpidx_state {
+	uint64_t u64;
+	struct cvmx_pki_bpidx_state_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t xoff                         : 1;  /**< The corresponding bpid is being backpressured. */
+#else
+	uint64_t xoff                         : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_pki_bpidx_state_s         cn78xx;
+};
+typedef union cvmx_pki_bpidx_state cvmx_pki_bpidx_state_t;
+
+/**
+ * cvmx_pki_buf_ctl
+ */
+union cvmx_pki_buf_ctl {
+	uint64_t u64;
+	struct cvmx_pki_buf_ctl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_11_63               : 53;
+	uint64_t fpa_wait                     : 1;  /**< Policy when FPA runs out of buffers:
+                                                         0 = Drop the remainder of the packet requesting the buffer, and set WQE[OPCODE] to
+                                                         RE_MEMOUT.
+                                                         1 = Wait until buffers become available, only dropping packets if buffering ahead of PKI
+                                                         fills. This may lead to head-of-line blocking of packets on other Auras. */
+	uint64_t fpa_cac_dis                  : 1;  /**< When set, disable caching any FPA buffers, and immediately return any cached buffers to the FPA. */
+	uint64_t reserved_6_8                 : 3;
+	uint64_t pkt_off                      : 1;  /**< Packet buffer off. When this bit is set to 1, the PKI does not buffer the received packet
+                                                         data; when it is clear to 0, the PKI works normally, buffering the received packet data. */
+	uint64_t reserved_3_4                 : 2;
+	uint64_t pbp_en                       : 1;  /**< Bpid enable. When set, enables the sending of bpid level backpressure to the input
+                                                         interface.
+                                                         The application should not de-assert this bit after asserting it. The receivers of this
+                                                         bit may have been put into backpressure mode and can only be released by PKI informing
+                                                         them that the backpressure has been released.
+                                                         INTERNAL: Must be one for PKI HW to assert any output backpressure wires. */
+	uint64_t reserved_1_1                 : 1;
+	uint64_t pki_en                       : 1;  /**< PKI enable. When set to 1, enables the operation of the PKI. When clear to 0, the PKI
+                                                         asserts backpressure on all ports. INTERNAL: Suppresses grants to X2P, not BPID
+                                                         backpressure. */
+#else
+	uint64_t pki_en                       : 1;
+	uint64_t reserved_1_1                 : 1;
+	uint64_t pbp_en                       : 1;
+	uint64_t reserved_3_4                 : 2;
+	uint64_t pkt_off                      : 1;
+	uint64_t reserved_6_8                 : 3;
+	uint64_t fpa_cac_dis                  : 1;
+	uint64_t fpa_wait                     : 1;
+	uint64_t reserved_11_63               : 53;
+#endif
+	} s;
+	struct cvmx_pki_buf_ctl_s             cn78xx;
+};
+typedef union cvmx_pki_buf_ctl cvmx_pki_buf_ctl_t;
+
+/**
+ * cvmx_pki_chan#_cfg
+ *
+ * This register configures each channel.
+ *
+ */
+union cvmx_pki_chanx_cfg {
+	uint64_t u64;
+	struct cvmx_pki_chanx_cfg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_17_63               : 47;
+	uint64_t imp                          : 1;  /**< Implemented. This register is sparse (only indexes with values in PKI_CHAN_E are
+                                                         implemented).
+                                                         0 = this index is read only.
+                                                         1 = this index is read-write.
+                                                         INTERNAL: Write to a non-implemented channel is ignored but returns write commit. Reading
+                                                         non-implemented channel returns all zero data. */
+	uint64_t reserved_10_15               : 6;
+	uint64_t bpid                         : 10; /**< Bpid to receive backpressure from. */
+#else
+	uint64_t bpid                         : 10;
+	uint64_t reserved_10_15               : 6;
+	uint64_t imp                          : 1;
+	uint64_t reserved_17_63               : 47;
+#endif
+	} s;
+	struct cvmx_pki_chanx_cfg_s           cn78xx;
+};
+typedef union cvmx_pki_chanx_cfg cvmx_pki_chanx_cfg_t;
+
+/**
+ * cvmx_pki_cl#_ecc_ctl
+ */
+union cvmx_pki_clx_ecc_ctl {
+	uint64_t u64;
+	struct cvmx_pki_clx_ecc_ctl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t pcam_en                      : 1;  /**< PCAM ECC checking enable. INTERNAL: This enables the PCAM scrubber. */
+	uint64_t reserved_24_62               : 39;
+	uint64_t pcam1_flip                   : 2;  /**< PCAM1 flip syndrome bits on write. */
+	uint64_t pcam0_flip                   : 2;  /**< PCAM  flip syndrome bits on write. */
+	uint64_t smem_flip                    : 2;  /**< SMEM flip syndrome bits on write. Flip syndrome bits <1:0> on writes to the KMEM ram to
+                                                         test single-bit or double-bit error handling. */
+	uint64_t dmem_flip                    : 1;  /**< DMEM flip parity. */
+	uint64_t rf_flip                      : 1;  /**< RF flip parity. */
+	uint64_t reserved_5_15                : 11;
+	uint64_t pcam1_cdis                   : 1;  /**< PCAM1 ECC correction disable. */
+	uint64_t pcam0_cdis                   : 1;  /**< PCAM0 ECC correction disable. */
+	uint64_t smem_cdis                    : 1;  /**< SMEM ECC correction disable. */
+	uint64_t dmem_cdis                    : 1;  /**< DMEM parity poising disable. */
+	uint64_t rf_cdis                      : 1;  /**< RF RAM parity poising disable. */
+#else
+	uint64_t rf_cdis                      : 1;
+	uint64_t dmem_cdis                    : 1;
+	uint64_t smem_cdis                    : 1;
+	uint64_t pcam0_cdis                   : 1;
+	uint64_t pcam1_cdis                   : 1;
+	uint64_t reserved_5_15                : 11;
+	uint64_t rf_flip                      : 1;
+	uint64_t dmem_flip                    : 1;
+	uint64_t smem_flip                    : 2;
+	uint64_t pcam0_flip                   : 2;
+	uint64_t pcam1_flip                   : 2;
+	uint64_t reserved_24_62               : 39;
+	uint64_t pcam_en                      : 1;
+#endif
+	} s;
+	struct cvmx_pki_clx_ecc_ctl_s         cn78xx;
+};
+typedef union cvmx_pki_clx_ecc_ctl cvmx_pki_clx_ecc_ctl_t;
+
+/**
+ * cvmx_pki_cl#_ecc_int
+ */
+union cvmx_pki_clx_ecc_int {
+	uint64_t u64;
+	struct cvmx_pki_clx_ecc_int_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_8_63                : 56;
+	uint64_t pcam1_dbe                    : 1;  /**< PCAM1 ECC double bit error. Throws PKI_INTSN_E::PKI_CL()_ECC_PCAM1_DBE. */
+	uint64_t pcam1_sbe                    : 1;  /**< PCAM1 ECC single bit error. Throws PKI_INTSN_E::PKI_CL()_ECC_PCAM1_SBE. */
+	uint64_t pcam0_dbe                    : 1;  /**< PCAM0 ECC double bit error. Throws PKI_INTSN_E::PKI_CL()_ECC_PCAM0_DBE. */
+	uint64_t pcam0_sbe                    : 1;  /**< PCAM0 ECC single bit error. Throws PKI_INTSN_E::PKI_CL()_ECC_PCAM0_SBE. */
+	uint64_t smem_dbe                     : 1;  /**< SMEM ECC double bit error. Throws PKI_INTSN_E::PKI_CL()_ECC_SMEM_DBE. */
+	uint64_t smem_sbe                     : 1;  /**< SMEM ECC single bit error. Throws PKI_INTSN_E::PKI_CL()_ECC_SMEM_SBE. */
+	uint64_t dmem_perr                    : 1;  /**< DMEM parity error. Throws PKI_INTSN_E::PKI_CL()_ECC_DMEM_PERR. */
+	uint64_t rf_perr                      : 1;  /**< RF RAM parity error. Throws PKI_INTSN_E::PKI_CL()_ECC_RF_PERR. */
+#else
+	uint64_t rf_perr                      : 1;
+	uint64_t dmem_perr                    : 1;
+	uint64_t smem_sbe                     : 1;
+	uint64_t smem_dbe                     : 1;
+	uint64_t pcam0_sbe                    : 1;
+	uint64_t pcam0_dbe                    : 1;
+	uint64_t pcam1_sbe                    : 1;
+	uint64_t pcam1_dbe                    : 1;
+	uint64_t reserved_8_63                : 56;
+#endif
+	} s;
+	struct cvmx_pki_clx_ecc_int_s         cn78xx;
+};
+typedef union cvmx_pki_clx_ecc_int cvmx_pki_clx_ecc_int_t;
+
+/**
+ * cvmx_pki_cl#_int
+ */
+union cvmx_pki_clx_int {
+	uint64_t u64;
+	struct cvmx_pki_clx_int_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_5_63                : 59;
+	uint64_t trapz                        : 1;  /**< Reserved. INTERNAL: Deprecated. PCAM sequencer trapz interrupt. Throws
+                                                         PKI_INTSN_E::PKI_CL()_INT_TRAPZ. Caused by TRAP sequence state, may indicate PKI
+                                                         enabled without proper sequencer code loaded in PKI_IMEM(). Based on PCAM sequencer
+                                                         execution, will likely throw an additional IPTINT interrupt. */
+	uint64_t iptint                       : 1;  /**< PCAM sequencer debug interrupt. Throws PKI_INTSN_E::PKI_CL()_INT_IPTINT. INTERNAL:
+                                                         Caused by TRAP or INTR sequence state. */
+	uint64_t sched_conf                   : 1;  /**< PCAM/SMEM internal port conflict. Internal error, should not occur. Throws
+                                                         PKI_INTSN_E::PKI_CL()_INT_SCHED_CONF. INTERNAL: Sequencer mis-scheduled PCAM or SMEM
+                                                         ops with overlapping accesses. */
+	uint64_t pcam_conf                    : 2;  /**< PCAM() match hit multiple rows, indicating either a soft error in the PCAM or a
+                                                         programming error in PKI_CL()_PCAM()_MATCH() or related registers. Throws
+                                                         PKI_INTSN_E::PKI_CL()_INT_PCAM_CONF(). Once a conflict is detected, the PCAM state
+                                                         is unpredictable and is required to be fully reconfigured before further valid processing
+                                                         can take place. */
+#else
+	uint64_t pcam_conf                    : 2;
+	uint64_t sched_conf                   : 1;
+	uint64_t iptint                       : 1;
+	uint64_t trapz                        : 1;
+	uint64_t reserved_5_63                : 59;
+#endif
+	} s;
+	struct cvmx_pki_clx_int_s             cn78xx;
+};
+typedef union cvmx_pki_clx_int cvmx_pki_clx_int_t;
+
+/**
+ * cvmx_pki_cl#_pcam#_action#
+ *
+ * Action performed based on PCAM lookup using the PKI_CL()_PCAM()_TERM() and
+ * PKI_CL()_PCAM()_MATCH() registers.
+ *
+ * If lookup data matches no PCAM entries, then no action takes place. No matches indicates
+ * normal parsing will continue.
+ *
+ * If data matches multiple PCAM entries, PKI_WQE_S[OPCODE] of the processed packet may be set to
+ * PKI_OPCODE_E::RE_PKIPCAM and the PKI_CL()_INT[PCAM_CONF] error interrupt is signaled.  Once a
+ * conflict is detected, the PCAM state is unpredictable and is required to be fully reconfigured
+ * before further valid processing can take place.
+ */
+union cvmx_pki_clx_pcamx_actionx {
+	uint64_t u64;
+	struct cvmx_pki_clx_pcamx_actionx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_31_63               : 33;
+	uint64_t pmc                          : 7;  /**< Parse mode change. Where to resume parsing after applying the scan offset (if any) as bit
+                                                         mask of which sequence steps to no longer process:
+                                                         _ <0> = LA (L2)
+                                                         _ <1> = LB (Custom)
+                                                         _ <2> = LC (L3)
+                                                         _ <3> = LD (Virt)
+                                                         _ <4> = LE (IL3)
+                                                         _ <5> = LF (L4)
+                                                         _ <6> = LG (Custom/Application)
+                                                         The legal values are:
+                                                         0x0 = no change in parsing.
+                                                         0x1 = Skip further LA parsing; start LB parsing. For TERM==L2_CUSTOM only).
+                                                         0x3 = Skip further LA/LB parsing; start LC parsing. For TERMs through Ethertypes only).
+                                                         0x7 = Skip further LA-LC parsing; start LD parsing. For TERMs through L3FLAGS only).
+                                                         0x7F = Skip all parsing; no further packet inspection. For TERMs through L3FLAGS only).
+                                                         For example an Ethertype match action that wishes to resume with additional Ethertype
+                                                         matches would use a zero PMC to indicate no parse mode change. An Ethertype match action
+                                                         that wishes to not parse any additional Ethertypes and resume at LC would use 0x3.
+                                                         Must be zero for invalid entries, or for TERMs that do not allow a parse mode change as
+                                                         specified in the PKI_PCAM_TERM_E table. */
+	uint64_t style_add                    : 8;  /**< Resulting interim style adder. If this CAM entry matches, the value to add to the current
+                                                         style (may wrap around through 256). Must be zero for invalid entries. */
+	uint64_t pf                           : 3;  /**< Parse flag to set. Specifies the parse flag to set when entry matches, see PCAM actions
+                                                         may also specify setting one of four user flags in the generated work queue entry,
+                                                         WQE[PF1] through WQE[PF4]. These flags are not used by hardware; they are intended to
+                                                         indicate to software that exceptional handling may be required, such as the presence of an
+                                                         encrypted packet:
+                                                         _ 0x0 = no change.
+                                                         _ 0x1 = Set WQE[PF1].
+                                                         _ 0x2 = Set WQE[PF2].
+                                                         _ 0x3 = Set WQE[PF3].
+                                                         _ 0x4 = Set WQE[PF4].
+                                                         _ else = reserved.
+                                                         Must be zero for invalid entries. */
+	uint64_t setty                        : 5;  /**< Set pointer type. If non-zero, indicates the layer type to be set as described under
+                                                         PKI_PCAM_TERM_E. Values are enumerated in PKI_LTYPE_E. Must be zero for invalid entries.
+                                                         The PKI_PCAM_TERM_E table enumerates legal/common SETTY values. */
+	uint64_t advance                      : 8;  /**< Relative number of bytes to advance scan pointer when entry matches.
+                                                         Must be even. Must be zero for invalid entries and for TERMs that do not allow
+                                                         an advance as specified in the PKI_PCAM_TERM_E table. */
+#else
+	uint64_t advance                      : 8;
+	uint64_t setty                        : 5;
+	uint64_t pf                           : 3;
+	uint64_t style_add                    : 8;
+	uint64_t pmc                          : 7;
+	uint64_t reserved_31_63               : 33;
+#endif
+	} s;
+	struct cvmx_pki_clx_pcamx_actionx_s   cn78xx;
+};
+typedef union cvmx_pki_clx_pcamx_actionx cvmx_pki_clx_pcamx_actionx_t;
+
+/**
+ * cvmx_pki_cl#_pcam#_match#
+ */
+union cvmx_pki_clx_pcamx_matchx {
+	uint64_t u64;
+	struct cvmx_pki_clx_pcamx_matchx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t data1                        : 32; /**< See [DATA0]. */
+	uint64_t data0                        : 32; /**< The packet data to compare against. Bits may be ignored in comparison based on [DATA0] and
+                                                         [DATA1]. If the entry matches, PKI_CL()_PCAM()_ACTION() will determine the
+                                                         action to be taken. See PKI_PCAM_TERM_E for comparison bit definitions.
+                                                         The field value is ternary, where each bit matches as follows:
+                                                         _ DATA1<n>=0, DATA0<n>=0: Always match; data<n> don't care.
+                                                         _ DATA1<n>=0, DATA0<n>=1: Match when data<n> == 0.
+                                                         _ DATA1<n>=1, DATA0<n>=0: Match when data<n> == 1.
+                                                         _ DATA1<n>=1, DATA0<n>=1: Reserved. */
+#else
+	uint64_t data0                        : 32;
+	uint64_t data1                        : 32;
+#endif
+	} s;
+	struct cvmx_pki_clx_pcamx_matchx_s    cn78xx;
+};
+typedef union cvmx_pki_clx_pcamx_matchx cvmx_pki_clx_pcamx_matchx_t;
+
+/**
+ * cvmx_pki_cl#_pcam#_term#
+ */
+union cvmx_pki_clx_pcamx_termx {
+	uint64_t u64;
+	struct cvmx_pki_clx_pcamx_termx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t valid                        : 1;  /**< Valid. */
+	uint64_t reserved_48_62               : 15;
+	uint64_t term1                        : 8;  /**< See [TERM0]. */
+	uint64_t style1                       : 8;  /**< See [STYLE0]. */
+	uint64_t reserved_16_31               : 16;
+	uint64_t term0                        : 8;  /**< Comparison type. Enumerated with PKI_PCAM_TERM_E. The field value is ternary, where each
+                                                         bit matches as follows:
+                                                         _ TERM1<n>=0, TERM0<n>=0: Always match; data<n> don't care.
+                                                         _ TERM1<n>=0, TERM0<n>=1: Match when data<n> == 0.
+                                                         _ TERM1<n>=1, TERM0<n>=0: Match when data<n> == 1.
+                                                         _ TERM1<n>=1, TERM0<n>=1: Reserved. */
+	uint64_t style0                       : 8;  /**< Previous interim style. The style that must have been calculated by the port
+                                                         PKI_CL()_PKIND()_STYLE[STYLE] or as modified by previous CAM hits's
+                                                         PKI_CL()_PCAM()_ACTION()[STYLE]. This is used to form AND style matches; see
+                                                         Styles.
+                                                         The field value is ternary, where each bit matches as follows:
+                                                         _ STYLE1<n>=0, STYLE0<n>=0: Always match; data<n> don't care.
+                                                         _ STYLE1<n>=0, STYLE0<n>=1: Match when data<n> == 0.
+                                                         _ STYLE1<n>=1, STYLE0<n>=0: Match when data<n> == 1.
+                                                         _ STYLE1<n>=1, STYLE0<n>=1: Reserved. */
+#else
+	uint64_t style0                       : 8;
+	uint64_t term0                        : 8;
+	uint64_t reserved_16_31               : 16;
+	uint64_t style1                       : 8;
+	uint64_t term1                        : 8;
+	uint64_t reserved_48_62               : 15;
+	uint64_t valid                        : 1;
+#endif
+	} s;
+	struct cvmx_pki_clx_pcamx_termx_s     cn78xx;
+};
+typedef union cvmx_pki_clx_pcamx_termx cvmx_pki_clx_pcamx_termx_t;
+
+/**
+ * cvmx_pki_cl#_pkind#_cfg
+ */
+union cvmx_pki_clx_pkindx_cfg {
+	uint64_t u64;
+	struct cvmx_pki_clx_pkindx_cfg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_8_63                : 56;
+	uint64_t fcs_pres                     : 1;  /**< FCS present.
+                                                         0 = FCS not present. FCS may not be checked nor stripped.
+                                                         1 = FCS present; the last four bytes of the packet are part of the FCS and may not be
+                                                         considered part of a IP, TCP or other header for length error checks.
+                                                         PKI_CL()_STYLE()_CFG[FCS_CHK or FCS_STRIP] may optionally be set. */
+	uint64_t mpls_en                      : 1;  /**< Enable MPLS parsing.
+                                                         0 = Any MPLS labels are ignored, but may be handled by custom Ethertype PCAM matchers.
+                                                         1 = MPLS label stacks are parsed and skipped over. PKI_GBL_PEN[MPLS_PEN] must be set. */
+	uint64_t inst_hdr                     : 1;  /**< INST header. When set, the eight-byte INST_HDR is present on all packets (except incoming
+                                                         packets on the DPI ports). */
+	uint64_t lg_custom                    : 1;  /**< Layer G Custom Match Enable.
+                                                         0 = Disable custom LG header extraction
+                                                         1 = Enable custom LG header extraction.
+                                                         PKI_GBL_PEN[CLG_PEN] must be set. */
+	uint64_t fulc_en                      : 1;  /**< Enable Fulcrum tag parsing.
+                                                         0 = Any Fulcrum header is ignored.
+                                                         1 = Fulcrum header is parsed. PKI_GBL_PEN[FULC_PEN] must be set.
+                                                         At most one of FULC_EN, DSA_EN or HG_EN may be set. */
+	uint64_t dsa_en                       : 1;  /**< Enable DSA parsing. This field should not be set for DPI ports.
+                                                         0 = Any DSA header is ignored.
+                                                         1 = DSA is parsed. PKI_GBL_PEN[DSA_PEN] must be set.
+                                                         At most one of FULC_EN, DSA_EN or HG_EN may be set. */
+	uint64_t hg2_en                       : 1;  /**< Enable HiGig 2 parsing. This field should not be set for DPI ports.
+                                                         0 = Any HiGig2 header is ignored.
+                                                         1 = HiGig2 is parsed. PKI_GBL_PEN[HG_PEN] must be set. */
+	uint64_t hg_en                        : 1;  /**< Enable HiGig parsing. This field should not be set for DPI ports.
+                                                         0 = Any HiGig header is ignored.
+                                                         1 = HiGig is parsed. PKI_GBL_PEN[HG_PEN] must be set.
+                                                         At most one of FULC_EN, DSA_EN or HG_EN may be set. */
+#else
+	uint64_t hg_en                        : 1;
+	uint64_t hg2_en                       : 1;
+	uint64_t dsa_en                       : 1;
+	uint64_t fulc_en                      : 1;
+	uint64_t lg_custom                    : 1;
+	uint64_t inst_hdr                     : 1;
+	uint64_t mpls_en                      : 1;
+	uint64_t fcs_pres                     : 1;
+	uint64_t reserved_8_63                : 56;
+#endif
+	} s;
+	struct cvmx_pki_clx_pkindx_cfg_s      cn78xx;
+};
+typedef union cvmx_pki_clx_pkindx_cfg cvmx_pki_clx_pkindx_cfg_t;
+
+/**
+ * cvmx_pki_cl#_pkind#_kmem#
+ *
+ * The register initialization value for each PKIND and register number (plus 32 or 48 based on
+ * PKI_ICG()_CFG[MLO]). The other PKI_PKND* registers alias inside regions of
+ * PKI_CL()_PKIND()_KMEM(). To avoid confusing tools, these aliases have address
+ * bit 20 set; the PKI address decoder ignores bit 20 when accessing
+ * PKI_CL()_PKIND()_KMEM().
+ */
+union cvmx_pki_clx_pkindx_kmemx {
+	uint64_t u64;
+	struct cvmx_pki_clx_pkindx_kmemx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t data                         : 16; /**< RAM data at given address. */
+#else
+	uint64_t data                         : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_pki_clx_pkindx_kmemx_s    cn78xx;
+};
+typedef union cvmx_pki_clx_pkindx_kmemx cvmx_pki_clx_pkindx_kmemx_t;
+
+/**
+ * cvmx_pki_cl#_pkind#_l2_custom
+ */
+union cvmx_pki_clx_pkindx_l2_custom {
+	uint64_t u64;
+	struct cvmx_pki_clx_pkindx_l2_custom_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t valid                        : 1;  /**< Valid.
+                                                         0 = Disable custom L2 header extraction.
+                                                         1 = Enable custom L2 header extraction.
+                                                         PKI_GBL_PEN[CLG_PEN] must be set. */
+	uint64_t reserved_8_14                : 7;
+	uint64_t offset                       : 8;  /**< Scan offset. Pointer to first byte of 32-bit custom extraction header, as absolute number
+                                                         of bytes from beginning of packet. If PTP_MODE, the 8-byte timestamp is prepended to the
+                                                         packet, and must be included in counting offset bytes. */
+#else
+	uint64_t offset                       : 8;
+	uint64_t reserved_8_14                : 7;
+	uint64_t valid                        : 1;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_pki_clx_pkindx_l2_custom_s cn78xx;
+};
+typedef union cvmx_pki_clx_pkindx_l2_custom cvmx_pki_clx_pkindx_l2_custom_t;
+
+/**
+ * cvmx_pki_cl#_pkind#_lg_custom
+ */
+union cvmx_pki_clx_pkindx_lg_custom {
+	uint64_t u64;
+	struct cvmx_pki_clx_pkindx_lg_custom_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_8_63                : 56;
+	uint64_t offset                       : 8;  /**< Scan offset. Pointer to first byte of 32-bit custom extraction header, as relative number
+                                                         of bytes from WQE[LFPTR]. */
+#else
+	uint64_t offset                       : 8;
+	uint64_t reserved_8_63                : 56;
+#endif
+	} s;
+	struct cvmx_pki_clx_pkindx_lg_custom_s cn78xx;
+};
+typedef union cvmx_pki_clx_pkindx_lg_custom cvmx_pki_clx_pkindx_lg_custom_t;
+
+/**
+ * cvmx_pki_cl#_pkind#_skip
+ */
+union cvmx_pki_clx_pkindx_skip {
+	uint64_t u64;
+	struct cvmx_pki_clx_pkindx_skip_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t fcs_skip                     : 8;  /**< Skip amount from front of packet to first byte covered by FCS start. The skip must be
+                                                         even. If PTP_MODE, the 8-byte timestamp is prepended to the packet, and FCS_SKIP must be
+                                                         at least 8. */
+	uint64_t inst_skip                    : 8;  /**< Skip amount from front of packet to begin parsing at. If
+                                                         PKI_CL()_PKIND()_CFG[INST_HDR] is set, points at the first byte of the
+                                                         instruction header. If INST_HDR is clear, points at the first byte to begin parsing at.
+                                                         The skip must be even. If PTP_MODE, the 8-byte timestamp is prepended to the packet, and
+                                                         INST_SKIP must be at least 8. */
+#else
+	uint64_t inst_skip                    : 8;
+	uint64_t fcs_skip                     : 8;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_pki_clx_pkindx_skip_s     cn78xx;
+};
+typedef union cvmx_pki_clx_pkindx_skip cvmx_pki_clx_pkindx_skip_t;
+
+/**
+ * cvmx_pki_cl#_pkind#_style
+ */
+union cvmx_pki_clx_pkindx_style {
+	uint64_t u64;
+	struct cvmx_pki_clx_pkindx_style_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_15_63               : 49;
+	uint64_t pm                           : 7;  /**< Initial parse mode. Bit mask of which sequence steps to perform, refer to Parse Mode:
+                                                         _ <0> = LA (L2)
+                                                         _ <1> = LB (Custom)
+                                                         _ <2> = LC (L3)
+                                                         _ <3> = LD (L4 Virt)
+                                                         _ <4> = LE (IL3)
+                                                         _ <5> = LF (L4)
+                                                         _ <6> = LG (Custom/Application)
+                                                         The legal values are:
+                                                         _ 0x0 = Parse LA..LG
+                                                         _ 0x1 = Parse LB..LG
+                                                         _ 0x3 = Parse LC..LG
+                                                         _ 0x3F = Parse LG
+                                                         _ 0x7F = Parse nothing
+                                                         _ else = Reserved */
+	uint64_t style                        : 8;  /**< Initial style. Initial style number for packets on this port, will remain as final style
+                                                         if no PCAM entries match the packet. Note only 64 final styles exist, the upper two bits
+                                                         will only be used for PCAM matching. */
+#else
+	uint64_t style                        : 8;
+	uint64_t pm                           : 7;
+	uint64_t reserved_15_63               : 49;
+#endif
+	} s;
+	struct cvmx_pki_clx_pkindx_style_s    cn78xx;
+};
+typedef union cvmx_pki_clx_pkindx_style cvmx_pki_clx_pkindx_style_t;
+
+/**
+ * cvmx_pki_cl#_smem#
+ *
+ * PKI_STYLE* registers alias inside regions of PKI_CL()_SMEM(). To avoid confusing
+ * tools, these aliases have address bit 20 set; the PKI address decoder ignores bit 20 when
+ * accessing PKI_CL()_SMEM().
+ */
+union cvmx_pki_clx_smemx {
+	uint64_t u64;
+	struct cvmx_pki_clx_smemx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_32_63               : 32;
+	uint64_t data                         : 32; /**< RAM data at given address. */
+#else
+	uint64_t data                         : 32;
+	uint64_t reserved_32_63               : 32;
+#endif
+	} s;
+	struct cvmx_pki_clx_smemx_s           cn78xx;
+};
+typedef union cvmx_pki_clx_smemx cvmx_pki_clx_smemx_t;
+
+/**
+ * cvmx_pki_cl#_start
+ */
+union cvmx_pki_clx_start {
+	uint64_t u64;
+	struct cvmx_pki_clx_start_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_11_63               : 53;
+	uint64_t start                        : 11; /**< Cluster start offset. <1:0> must be zero. For diagnostic use only. */
+#else
+	uint64_t start                        : 11;
+	uint64_t reserved_11_63               : 53;
+#endif
+	} s;
+	struct cvmx_pki_clx_start_s           cn78xx;
+};
+typedef union cvmx_pki_clx_start cvmx_pki_clx_start_t;
+
+/**
+ * cvmx_pki_cl#_style#_alg
+ */
+union cvmx_pki_clx_stylex_alg {
+	uint64_t u64;
+	struct cvmx_pki_clx_stylex_alg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_32_63               : 32;
+	uint64_t tt                           : 2;  /**< SSO tag type to schedule to, enumerated with SSO_TT_E. */
+	uint64_t apad_nip                     : 3;  /**< Value for WQE[APAD] when packet is not IP. */
+	uint64_t qpg_qos                      : 3;  /**< Algorithm to select QoS field in QPG calculation. Enumerated with PKI_QPGQOS_E. See QPG. */
+	uint64_t qpg_port_sh                  : 3;  /**< Number of bits to shift port number in QPG calculation. See QPG. */
+	uint64_t qpg_port_msb                 : 4;  /**< MSB to take from port number in QPG calculation. See QPG.
+                                                         0 = Exclude port number from QPG.
+                                                         4 = Include port<3:0>.
+                                                         8 = Include port<7:0>.
+                                                         else Reserved. */
+	uint64_t reserved_11_16               : 6;
+	uint64_t tag_vni                      : 1;  /**< When VXLAN is found, include VNI in tag generation. When NVGRE is found, include TNI. */
+	uint64_t tag_gtp                      : 1;  /**< When GTP is parsed, include GTP's TEID in tag generation. */
+	uint64_t tag_spi                      : 1;  /**< Include AH/GRE in tag generation.
+                                                         0 = Exclude AH/GRE in tag generation.
+                                                         1 = If IP SEC AH is parsed, include AH SPI field in tag generation, or if GRE found,
+                                                         include GRE call number in tag generation. */
+	uint64_t tag_syn                      : 1;  /**< Exclude source address for TCP SYN packets.
+                                                         0 = Include source address if TAG_SRC_* used.
+                                                         1 = Exclude source address for TCP packets with SYN & !ACK, even if TAG_SRC_* set. */
+	uint64_t tag_pctl                     : 1;  /**< Include final IPv4 protocol or IPv6 next header in tag generation. */
+	uint64_t tag_vs1                      : 1;  /**< When VLAN stacking is parsed, include second (network order) VLAN in tuple tag generation. */
+	uint64_t tag_vs0                      : 1;  /**< When VLAN stacking is parsed, include first (network order) VLAN in tuple tag generation. */
+	uint64_t tag_vlan                     : 1;  /**< Reserved. */
+	uint64_t tag_mpls0                    : 1;  /**< Reserved. */
+	uint64_t tag_prt                      : 1;  /**< Include interface port in tag hash. */
+	uint64_t wqe_vs                       : 1;  /**< Which VLAN to put into WQE[VLPTR] when VLAN stacking.
+                                                         0 = Use the first (in network order) VLAN or DSA VID.
+                                                         1 = Use the second (in network order) VLAN. */
+#else
+	uint64_t wqe_vs                       : 1;
+	uint64_t tag_prt                      : 1;
+	uint64_t tag_mpls0                    : 1;
+	uint64_t tag_vlan                     : 1;
+	uint64_t tag_vs0                      : 1;
+	uint64_t tag_vs1                      : 1;
+	uint64_t tag_pctl                     : 1;
+	uint64_t tag_syn                      : 1;
+	uint64_t tag_spi                      : 1;
+	uint64_t tag_gtp                      : 1;
+	uint64_t tag_vni                      : 1;
+	uint64_t reserved_11_16               : 6;
+	uint64_t qpg_port_msb                 : 4;
+	uint64_t qpg_port_sh                  : 3;
+	uint64_t qpg_qos                      : 3;
+	uint64_t apad_nip                     : 3;
+	uint64_t tt                           : 2;
+	uint64_t reserved_32_63               : 32;
+#endif
+	} s;
+	struct cvmx_pki_clx_stylex_alg_s      cn78xx;
+};
+typedef union cvmx_pki_clx_stylex_alg cvmx_pki_clx_stylex_alg_t;
+
+/**
+ * cvmx_pki_cl#_style#_cfg
+ */
+union cvmx_pki_clx_stylex_cfg {
+	uint64_t u64;
+	struct cvmx_pki_clx_stylex_cfg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_31_63               : 33;
+	uint64_t ip6_udp_opt                  : 1;  /**< IPv6/UDP checksum is optional. IPv4 allows an optional UDP checksum by sending the all-0s
+                                                         patterns. IPv6 outlaws this and the spec says to always check UDP checksum.
+                                                         0 = Spec compliant, do not allow optional code.
+                                                         1 = Treat IPv6 as IPv4; the all-0s pattern will cause a UDP checksum pass. */
+	uint64_t lenerr_en                    : 1;  /**< L2 length error check enable. Check if frame was received with L2 length error. This check
+                                                         is typically not enabled for incoming packets on the DPI ports. INTERNAL: Sequencer clears
+                                                         this bit for PKI_BE when SNAP length checks are not appropriate. */
+	uint64_t lenerr_eqpad                 : 1;  /**< L2 length checks exact pad size.
+                                                         0 = Length check uses greater then or equal comparison. Packets must have at least minimum
+                                                         padding, but may have more. This mode must be used when there may be extra Etherypes
+                                                         including VLAN tags.
+                                                         1 = Length check uses equal comparison. Packets must have the exact padding necessary to
+                                                         insure a minimum frame size and no more. */
+	uint64_t minmax_sel                   : 1;  /**< Selects which PKI_FRM_LEN_CHK() register is used for this pkind for MINERR and MAXERR
+                                                         checks.
+                                                         0 = use PKI_FRM_LEN_CHK0.
+                                                         1 = use PKI_FRM_LEN_CHK1. */
+	uint64_t maxerr_en                    : 1;  /**< Max frame error check enable. */
+	uint64_t minerr_en                    : 1;  /**< Min frame error check enable. This check is typically not enabled for incoming packets on
+                                                         the DPI ports. */
+	uint64_t qpg_dis_grptag               : 1;  /**< Disable computing group using WQE[TAG]. */
+	uint64_t fcs_strip                    : 1;  /**< Strip L2 FCS bytes from packet, decrease WQE[LEN] by 4 bytes.
+                                                         PKI_CL()_PKIND()_CFG[FCS_PRES] must be set. */
+	uint64_t fcs_chk                      : 1;  /**< FCS checking enabled. PKI_CL()_PKIND()_CFG[FCS_PRES] must be set. */
+	uint64_t rawdrp                       : 1;  /**< Allow RAW packet drop.
+                                                         0 = Never drop packets with WQE[RAW] set.
+                                                         1 = Allow the PKI to drop RAW packets based on PKI_AURA()_CFG[ENA_RED/ENA_DROP]. */
+	uint64_t drop                         : 1;  /**< Force packet dropping.
+                                                         0 = Drop packet based on PKI_AURA()_CFG[ENA_RED/ENA_DROP].
+                                                         1 = Always drop the packet. Overrides [NODROP], [RAWDRP]. */
+	uint64_t nodrop                       : 1;  /**< Disable QoS packet drop.
+                                                         0 = Allowed to drop packet based on PKI_AURA()_CFG[ENA_RED/ENA_DROP].
+                                                         1 = Never drop the packet. Overrides [RAWDRP]. */
+	uint64_t qpg_dis_padd                 : 1;  /**< Disable computing port adder by QPG algorithm. */
+	uint64_t qpg_dis_grp                  : 1;  /**< Disable computing group by QPG algorithm. */
+	uint64_t qpg_dis_aura                 : 1;  /**< Disable computing aura by QPG algorithm. */
+	uint64_t reserved_11_15               : 5;
+	uint64_t qpg_base                     : 11; /**< Base index into PKI_QPG_TBL(). INTERNAL: Sequencer starts with QPG_BASE, performs
+                                                         the QPG calculation and packs the resulting QPG index back into this field for PKI_BE_S. */
+#else
+	uint64_t qpg_base                     : 11;
+	uint64_t reserved_11_15               : 5;
+	uint64_t qpg_dis_aura                 : 1;
+	uint64_t qpg_dis_grp                  : 1;
+	uint64_t qpg_dis_padd                 : 1;
+	uint64_t nodrop                       : 1;
+	uint64_t drop                         : 1;
+	uint64_t rawdrp                       : 1;
+	uint64_t fcs_chk                      : 1;
+	uint64_t fcs_strip                    : 1;
+	uint64_t qpg_dis_grptag               : 1;
+	uint64_t minerr_en                    : 1;
+	uint64_t maxerr_en                    : 1;
+	uint64_t minmax_sel                   : 1;
+	uint64_t lenerr_eqpad                 : 1;
+	uint64_t lenerr_en                    : 1;
+	uint64_t ip6_udp_opt                  : 1;
+	uint64_t reserved_31_63               : 33;
+#endif
+	} s;
+	struct cvmx_pki_clx_stylex_cfg_s      cn78xx;
+};
+typedef union cvmx_pki_clx_stylex_cfg cvmx_pki_clx_stylex_cfg_t;
+
+/**
+ * cvmx_pki_cl#_style#_cfg2
+ */
+union cvmx_pki_clx_stylex_cfg2 {
+	uint64_t u64;
+	struct cvmx_pki_clx_stylex_cfg2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_32_63               : 32;
+	uint64_t tag_inc                      : 4;  /**< Include masked tags using PKI_TAG_INC()_MASK. Each bit indicates to include the
+                                                         corresponding PKI_TAG_INC_MASK range, see PKI_INST_HDR_S. */
+	uint64_t reserved_25_27               : 3;
+	uint64_t tag_masken                   : 1;  /**< Apply PKI_STYLE()_TAG_MASK to computed tag. INTERNAL: Sequencer must clear for PKI BE
+                                                         when the tag comes from the PKI_INST_HDR_S. */
+	uint64_t tag_src_lg                   : 1;  /**< Include Layer G source address in tuple tag generation. */
+	uint64_t tag_src_lf                   : 1;  /**< Include Layer F source address in tuple tag generation. */
+	uint64_t tag_src_le                   : 1;  /**< Include Layer E source address in tuple tag generation. */
+	uint64_t tag_src_ld                   : 1;  /**< Include Layer D source address in tuple tag generation. */
+	uint64_t tag_src_lc                   : 1;  /**< Include Layer C source address in tuple tag generation. */
+	uint64_t tag_src_lb                   : 1;  /**< Include Layer B source address in tuple tag generation. INTERNAL: Sequencer must clear
+                                                         TAG_SRC_L* for PKI BE when TCP SYNs are not tagged, or when the tag comes from the
+                                                         PKI_INST_HDR_S. */
+	uint64_t tag_dst_lg                   : 1;  /**< Include Layer G destination address in tuple tag generation. */
+	uint64_t tag_dst_lf                   : 1;  /**< Include Layer F destination address in tuple tag generation. */
+	uint64_t tag_dst_le                   : 1;  /**< Include Layer E destination address in tuple tag generation. */
+	uint64_t tag_dst_ld                   : 1;  /**< Include Layer D destination address in tuple tag generation. */
+	uint64_t tag_dst_lc                   : 1;  /**< Include Layer C destination address in tuple tag generation. */
+	uint64_t tag_dst_lb                   : 1;  /**< Include Layer B destination address in tuple tag generation. INTERNAL: Sequencer must
+                                                         clear TAG_SRC_L* for PKI BE when the tag comes from the PKI_INST_HDR_S. */
+	uint64_t len_lg                       : 1;  /**< Check length of Layer G. */
+	uint64_t len_lf                       : 1;  /**< Check length of Layer F. */
+	uint64_t len_le                       : 1;  /**< Check length of Layer E. */
+	uint64_t len_ld                       : 1;  /**< Check length of Layer D. */
+	uint64_t len_lc                       : 1;  /**< Check length of Layer C. */
+	uint64_t len_lb                       : 1;  /**< Check length of Layer B. */
+	uint64_t csum_lg                      : 1;  /**< Compute checksum on Layer G. */
+	uint64_t csum_lf                      : 1;  /**< Compute checksum on Layer F. */
+	uint64_t csum_le                      : 1;  /**< Compute checksum on Layer E. */
+	uint64_t csum_ld                      : 1;  /**< Compute checksum on Layer D. */
+	uint64_t csum_lc                      : 1;  /**< Compute checksum on Layer C. */
+	uint64_t csum_lb                      : 1;  /**< Compute checksum on Layer B. */
+#else
+	uint64_t csum_lb                      : 1;
+	uint64_t csum_lc                      : 1;
+	uint64_t csum_ld                      : 1;
+	uint64_t csum_le                      : 1;
+	uint64_t csum_lf                      : 1;
+	uint64_t csum_lg                      : 1;
+	uint64_t len_lb                       : 1;
+	uint64_t len_lc                       : 1;
+	uint64_t len_ld                       : 1;
+	uint64_t len_le                       : 1;
+	uint64_t len_lf                       : 1;
+	uint64_t len_lg                       : 1;
+	uint64_t tag_dst_lb                   : 1;
+	uint64_t tag_dst_lc                   : 1;
+	uint64_t tag_dst_ld                   : 1;
+	uint64_t tag_dst_le                   : 1;
+	uint64_t tag_dst_lf                   : 1;
+	uint64_t tag_dst_lg                   : 1;
+	uint64_t tag_src_lb                   : 1;
+	uint64_t tag_src_lc                   : 1;
+	uint64_t tag_src_ld                   : 1;
+	uint64_t tag_src_le                   : 1;
+	uint64_t tag_src_lf                   : 1;
+	uint64_t tag_src_lg                   : 1;
+	uint64_t tag_masken                   : 1;
+	uint64_t reserved_25_27               : 3;
+	uint64_t tag_inc                      : 4;
+	uint64_t reserved_32_63               : 32;
+#endif
+	} s;
+	struct cvmx_pki_clx_stylex_cfg2_s     cn78xx;
+};
+typedef union cvmx_pki_clx_stylex_cfg2 cvmx_pki_clx_stylex_cfg2_t;
+
+/**
+ * cvmx_pki_clken
+ */
+union cvmx_pki_clken {
+	uint64_t u64;
+	struct cvmx_pki_clken_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t clken                        : 1;  /**< Controls the conditional clocking within PKI.
+                                                         0 = Allow hardware to control the clocks.
+                                                         1 = Force the clocks to be always on. */
+#else
+	uint64_t clken                        : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_pki_clken_s               cn78xx;
+};
+typedef union cvmx_pki_clken cvmx_pki_clken_t;
+
+/**
+ * cvmx_pki_ecc_ctl0
+ *
+ * This register allows inserting ECC errors for testing.
+ *
+ */
+union cvmx_pki_ecc_ctl0 {
+	uint64_t u64;
+	struct cvmx_pki_ecc_ctl0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_24_63               : 40;
+	uint64_t ldfif_flip                   : 2;  /**< LDFIF RAM flip syndrome bits on write. Flip syndrome bits <1:0> on writes to the LDFIF ram
+                                                         to test single-bit or double-bit error handling. */
+	uint64_t ldfif_cdis                   : 1;  /**< LDFIF RAM ECC correction disable. */
+	uint64_t pbe_flip                     : 2;  /**< PBE state RAM flip syndrome bits on write. Flip syndrome bits <1:0> on writes to the PBE
+                                                         ram to test single-bit or double-bit error handling. */
+	uint64_t pbe_cdis                     : 1;  /**< PBE state RAM ECC correction disable. */
+	uint64_t wadr_flip                    : 2;  /**< WADR RAM flip syndrome bits on write. Flip syndrome bits <1:0> on writes to the WADR ram
+                                                         to test single-bit or double-bit error handling. */
+	uint64_t wadr_cdis                    : 1;  /**< WADR RAM ECC correction disable. */
+	uint64_t nxtptag_flip                 : 2;  /**< NXTPTAG RAM flip syndrome bits on write. Flip syndrome bits <1:0> on writes to the NXTPTAG
+                                                         ram to test single-bit or double-bit error handling. */
+	uint64_t nxtptag_cdis                 : 1;  /**< NXTPTAG RAM ECC correction disable. */
+	uint64_t curptag_flip                 : 2;  /**< CURPTAG RAM flip syndrome bits on write. Flip syndrome bits <1:0> on writes to the CURPTAG
+                                                         ram to test single-bit or double-bit error handling. */
+	uint64_t curptag_cdis                 : 1;  /**< CURPTAG RAM ECC correction disable. */
+	uint64_t nxtblk_flip                  : 2;  /**< NXTBLK RAM flip syndrome bits on write. Flip syndrome bits <1:0> on writes to the NXTBLK
+                                                         ram to test single-bit or double-bit error handling. */
+	uint64_t nxtblk_cdis                  : 1;  /**< NXTBLK RAM ECC correction disable. */
+	uint64_t kmem_flip                    : 2;  /**< KMEM RAM flip syndrome bits on write. Flip syndrome bits <1:0> on writes to the KMEM ram
+                                                         to test single-bit or double-bit error handling. */
+	uint64_t kmem_cdis                    : 1;  /**< KMEM RAM ECC correction disable. */
+	uint64_t asm_flip                     : 2;  /**< ASM RAM flip syndrome bits on write. Flip syndrome bits <1:0> on writes to the ASM ram to
+                                                         test single-bit or double-bit error handling. */
+	uint64_t asm_cdis                     : 1;  /**< ASM RAM ECC correction disable. */
+#else
+	uint64_t asm_cdis                     : 1;
+	uint64_t asm_flip                     : 2;
+	uint64_t kmem_cdis                    : 1;
+	uint64_t kmem_flip                    : 2;
+	uint64_t nxtblk_cdis                  : 1;
+	uint64_t nxtblk_flip                  : 2;
+	uint64_t curptag_cdis                 : 1;
+	uint64_t curptag_flip                 : 2;
+	uint64_t nxtptag_cdis                 : 1;
+	uint64_t nxtptag_flip                 : 2;
+	uint64_t wadr_cdis                    : 1;
+	uint64_t wadr_flip                    : 2;
+	uint64_t pbe_cdis                     : 1;
+	uint64_t pbe_flip                     : 2;
+	uint64_t ldfif_cdis                   : 1;
+	uint64_t ldfif_flip                   : 2;
+	uint64_t reserved_24_63               : 40;
+#endif
+	} s;
+	struct cvmx_pki_ecc_ctl0_s            cn78xx;
+};
+typedef union cvmx_pki_ecc_ctl0 cvmx_pki_ecc_ctl0_t;
+
+/**
+ * cvmx_pki_ecc_ctl1
+ *
+ * This register allows inserting ECC errors for testing.
+ *
+ */
+union cvmx_pki_ecc_ctl1 {
+	uint64_t u64;
+	struct cvmx_pki_ecc_ctl1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_51_63               : 13;
+	uint64_t sws_flip                     : 2;  /**< SWS flip syndrome bits on write. */
+	uint64_t sws_cdis                     : 1;  /**< SWS ECC correction disable. */
+	uint64_t wqeout_flip                  : 2;  /**< WQEOUT flip syndrome bits on write. */
+	uint64_t wqeout_cdis                  : 1;  /**< WQEOUT ECC correction disable. */
+	uint64_t doa_flip                     : 2;  /**< DOA flip syndrome bits on write. */
+	uint64_t doa_cdis                     : 1;  /**< DOA ECC correction disable. */
+	uint64_t bpid_flip                    : 2;  /**< BPID flip syndrome bits on write. */
+	uint64_t bpid_cdis                    : 1;  /**< BPID ECC correction disable. */
+	uint64_t reserved_30_38               : 9;
+	uint64_t plc_flip                     : 2;  /**< PLC flip syndrome bits on write. */
+	uint64_t plc_cdis                     : 1;  /**< PLC ECC correction disable. */
+	uint64_t pktwq_flip                   : 2;  /**< PKTWQ flip syndrome bits on write. */
+	uint64_t pktwq_cdis                   : 1;  /**< PKTWQ ECC correction disable. */
+	uint64_t reserved_18_23               : 6;
+	uint64_t tag_flip                     : 2;  /**< TAG flip syndrome bits on write. */
+	uint64_t tag_cdis                     : 1;  /**< TAG ECC correction disable. */
+	uint64_t aura_flip                    : 2;  /**< AURA flip syndrome bits on write. */
+	uint64_t aura_cdis                    : 1;  /**< AURA ECC correction disable. */
+	uint64_t chan_flip                    : 2;  /**< CHAN flip syndrome bits on write. */
+	uint64_t chan_cdis                    : 1;  /**< CHAN ECC correction disable. */
+	uint64_t pbtag_flip                   : 2;  /**< PBTAG flip syndrome bits on write. */
+	uint64_t pbtag_cdis                   : 1;  /**< PBTAG ECC correction disable. */
+	uint64_t stylewq_flip                 : 2;  /**< STYLEWQ flip syndrome bits on write. */
+	uint64_t stylewq_cdis                 : 1;  /**< STYLEWQ ECC correction disable. */
+	uint64_t qpg_flip                     : 2;  /**< QPG flip syndrome bits on write. */
+	uint64_t qpg_cdis                     : 1;  /**< QPG ECC correction disable. */
+#else
+	uint64_t qpg_cdis                     : 1;
+	uint64_t qpg_flip                     : 2;
+	uint64_t stylewq_cdis                 : 1;
+	uint64_t stylewq_flip                 : 2;
+	uint64_t pbtag_cdis                   : 1;
+	uint64_t pbtag_flip                   : 2;
+	uint64_t chan_cdis                    : 1;
+	uint64_t chan_flip                    : 2;
+	uint64_t aura_cdis                    : 1;
+	uint64_t aura_flip                    : 2;
+	uint64_t tag_cdis                     : 1;
+	uint64_t tag_flip                     : 2;
+	uint64_t reserved_18_23               : 6;
+	uint64_t pktwq_cdis                   : 1;
+	uint64_t pktwq_flip                   : 2;
+	uint64_t plc_cdis                     : 1;
+	uint64_t plc_flip                     : 2;
+	uint64_t reserved_30_38               : 9;
+	uint64_t bpid_cdis                    : 1;
+	uint64_t bpid_flip                    : 2;
+	uint64_t doa_cdis                     : 1;
+	uint64_t doa_flip                     : 2;
+	uint64_t wqeout_cdis                  : 1;
+	uint64_t wqeout_flip                  : 2;
+	uint64_t sws_cdis                     : 1;
+	uint64_t sws_flip                     : 2;
+	uint64_t reserved_51_63               : 13;
+#endif
+	} s;
+	struct cvmx_pki_ecc_ctl1_s            cn78xx;
+};
+typedef union cvmx_pki_ecc_ctl1 cvmx_pki_ecc_ctl1_t;
+
+/**
+ * cvmx_pki_ecc_ctl2
+ *
+ * This register allows inserting ECC errors for testing.
+ *
+ */
+union cvmx_pki_ecc_ctl2 {
+	uint64_t u64;
+	struct cvmx_pki_ecc_ctl2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_3_63                : 61;
+	uint64_t imem_flip                    : 2;  /**< KMEM flip syndrome bits on write. Flip syndrome bits <1:0> on writes to the KMEM ram to
+                                                         test single-bit or double-bit error handling. */
+	uint64_t imem_cdis                    : 1;  /**< IMEM ECC correction disable. */
+#else
+	uint64_t imem_cdis                    : 1;
+	uint64_t imem_flip                    : 2;
+	uint64_t reserved_3_63                : 61;
+#endif
+	} s;
+	struct cvmx_pki_ecc_ctl2_s            cn78xx;
+};
+typedef union cvmx_pki_ecc_ctl2 cvmx_pki_ecc_ctl2_t;
+
+/**
+ * cvmx_pki_ecc_int0
+ */
+union cvmx_pki_ecc_int0 {
+	uint64_t u64;
+	struct cvmx_pki_ecc_int0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t ldfif_dbe                    : 1;  /**< LDFIF ECC double bit error. Throws PKI_INTSN_E::PKI_ECC0_LDFIF_DBE. */
+	uint64_t ldfif_sbe                    : 1;  /**< LDFIF ECC single bit error. Throws PKI_INTSN_E::PKI_ECC0_LDFIF_SBE. */
+	uint64_t pbe_dbe                      : 1;  /**< PBE ECC double bit error. Throws PKI_INTSN_E::PKI_ECC0_PBE_DBE. */
+	uint64_t pbe_sbe                      : 1;  /**< PBE ECC single bit error. Throws PKI_INTSN_E::PKI_ECC0_PBE_SBE. */
+	uint64_t wadr_dbe                     : 1;  /**< WADR ECC double bit error. Throws PKI_INTSN_E::PKI_ECC0_WADR_DBE. */
+	uint64_t wadr_sbe                     : 1;  /**< WADR ECC single bit error. Throws PKI_INTSN_E::PKI_ECC0_WADR_SBE. */
+	uint64_t nxtptag_dbe                  : 1;  /**< NXTPTAG ECC double bit error. Throws PKI_INTSN_E::PKI_ECC0_NXTPTAG_DBE. */
+	uint64_t nxtptag_sbe                  : 1;  /**< NXTPTAG ECC single bit error. Throws PKI_INTSN_E::PKI_ECC0_NXTPTAG_SBE. */
+	uint64_t curptag_dbe                  : 1;  /**< CURPTAG ECC double bit error. Throws PKI_INTSN_E::PKI_ECC0_CURPTAG_DBE. */
+	uint64_t curptag_sbe                  : 1;  /**< CURPTAG ECC single bit error. Throws PKI_INTSN_E::PKI_ECC0_CURPTAG_SBE. */
+	uint64_t nxtblk_dbe                   : 1;  /**< NXTBLK ECC double bit error. Throws PKI_INTSN_E::PKI_ECC0_NXTBLK_DBE. */
+	uint64_t nxtblk_sbe                   : 1;  /**< NXTBLK ECC single bit error. Throws PKI_INTSN_E::PKI_ECC0_NXTBLK_SBE. */
+	uint64_t kmem_dbe                     : 1;  /**< KMEM ECC double bit error. Throws PKI_INTSN_E::PKI_ECC0_KMEM_DBE. */
+	uint64_t kmem_sbe                     : 1;  /**< KMEM ECC single bit error. Throws PKI_INTSN_E::PKI_ECC0_KMEM_SBE. */
+	uint64_t asm_dbe                      : 1;  /**< ASM ECC double bit error. Throws PKI_INTSN_E::PKI_ECC0_ASM_DBE. */
+	uint64_t asm_sbe                      : 1;  /**< ASM ECC single bit error. Throws PKI_INTSN_E::PKI_ECC0_ASM_SBE. */
+#else
+	uint64_t asm_sbe                      : 1;
+	uint64_t asm_dbe                      : 1;
+	uint64_t kmem_sbe                     : 1;
+	uint64_t kmem_dbe                     : 1;
+	uint64_t nxtblk_sbe                   : 1;
+	uint64_t nxtblk_dbe                   : 1;
+	uint64_t curptag_sbe                  : 1;
+	uint64_t curptag_dbe                  : 1;
+	uint64_t nxtptag_sbe                  : 1;
+	uint64_t nxtptag_dbe                  : 1;
+	uint64_t wadr_sbe                     : 1;
+	uint64_t wadr_dbe                     : 1;
+	uint64_t pbe_sbe                      : 1;
+	uint64_t pbe_dbe                      : 1;
+	uint64_t ldfif_sbe                    : 1;
+	uint64_t ldfif_dbe                    : 1;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_pki_ecc_int0_s            cn78xx;
+};
+typedef union cvmx_pki_ecc_int0 cvmx_pki_ecc_int0_t;
+
+/**
+ * cvmx_pki_ecc_int1
+ */
+union cvmx_pki_ecc_int1 {
+	uint64_t u64;
+	struct cvmx_pki_ecc_int1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_34_63               : 30;
+	uint64_t sws_dbe                      : 1;  /**< PLC ECC double bit error. Throws PKI_INTSN_E::PKI_ECC1_SWS_DBE. */
+	uint64_t sws_sbe                      : 1;  /**< PLC ECC single bit error. Throws PKI_INTSN_E::PKI_ECC1_SWS_SBE. */
+	uint64_t wqeout_dbe                   : 1;  /**< PLC ECC double bit error. Throws PKI_INTSN_E::PKI_ECC1_WQEOUT_DBE. */
+	uint64_t wqeout_sbe                   : 1;  /**< PLC ECC single bit error. Throws PKI_INTSN_E::PKI_ECC1_WQEOUT_SBE. */
+	uint64_t doa_dbe                      : 1;  /**< PLC ECC double bit error. Throws PKI_INTSN_E::PKI_ECC1_DOA_DBE. */
+	uint64_t doa_sbe                      : 1;  /**< PLC ECC single bit error. Throws PKI_INTSN_E::PKI_ECC1_DOA_SBE. */
+	uint64_t bpid_dbe                     : 1;  /**< PLC ECC double bit error. Throws PKI_INTSN_E::PKI_ECC1_BPID_DBE. */
+	uint64_t bpid_sbe                     : 1;  /**< PLC ECC single bit error. Throws PKI_INTSN_E::PKI_ECC1_BPID_SBE. */
+	uint64_t reserved_20_25               : 6;
+	uint64_t plc_dbe                      : 1;  /**< PLC ECC double bit error. Throws PKI_INTSN_E::PKI_ECC1_PLC_DBE. */
+	uint64_t plc_sbe                      : 1;  /**< PLC ECC single bit error. Throws PKI_INTSN_E::PKI_ECC1_PLC_SBE. */
+	uint64_t pktwq_dbe                    : 1;  /**< PKTWQ ECC double bit error. Throws PKI_INTSN_E::PKI_ECC1_PKTWQ_DBE. */
+	uint64_t pktwq_sbe                    : 1;  /**< PKTWQ ECC single bit error. Throws PKI_INTSN_E::PKI_ECC1_PKTWQ_SBE. */
+	uint64_t reserved_12_15               : 4;
+	uint64_t tag_dbe                      : 1;  /**< TAG ECC double bit error. Throws PKI_INTSN_E::PKI_ECC1_TAG_DBE. */
+	uint64_t tag_sbe                      : 1;  /**< TAG ECC single bit error. Throws PKI_INTSN_E::PKI_ECC1_TAG_SBE. */
+	uint64_t aura_dbe                     : 1;  /**< AURA ECC double bit error. Throws PKI_INTSN_E::PKI_ECC1_AURA_DBE. */
+	uint64_t aura_sbe                     : 1;  /**< AURA ECC single bit error. Throws PKI_INTSN_E::PKI_ECC1_AURA_SBE. */
+	uint64_t chan_dbe                     : 1;  /**< CHAN ECC double bit error. Throws PKI_INTSN_E::PKI_ECC1_CHAN_DBE. */
+	uint64_t chan_sbe                     : 1;  /**< CHAN ECC single bit error. Throws PKI_INTSN_E::PKI_ECC1_CHAN_SBE. */
+	uint64_t pbtag_dbe                    : 1;  /**< PBTAG ECC double bit error. Throws PKI_INTSN_E::PKI_ECC1_PBTAG_DBE. */
+	uint64_t pbtag_sbe                    : 1;  /**< PBTAG ECC single bit error. Throws PKI_INTSN_E::PKI_ECC1_PBTAG_SBE. */
+	uint64_t stylewq_dbe                  : 1;  /**< STYLEWQ ECC double bit error. Throws PKI_INTSN_E::PKI_ECC1_STYLEWQ_DBE. */
+	uint64_t stylewq_sbe                  : 1;  /**< STYLEWQ ECC single bit error. Throws PKI_INTSN_E::PKI_ECC1_STYLEWQ_SBE. */
+	uint64_t qpg_dbe                      : 1;  /**< QPG ECC double bit error. Throws PKI_INTSN_E::PKI_ECC1_QPG_DBE. */
+	uint64_t qpg_sbe                      : 1;  /**< QPG ECC single bit error. Throws PKI_INTSN_E::PKI_ECC1_QPG_SBE. */
+#else
+	uint64_t qpg_sbe                      : 1;
+	uint64_t qpg_dbe                      : 1;
+	uint64_t stylewq_sbe                  : 1;
+	uint64_t stylewq_dbe                  : 1;
+	uint64_t pbtag_sbe                    : 1;
+	uint64_t pbtag_dbe                    : 1;
+	uint64_t chan_sbe                     : 1;
+	uint64_t chan_dbe                     : 1;
+	uint64_t aura_sbe                     : 1;
+	uint64_t aura_dbe                     : 1;
+	uint64_t tag_sbe                      : 1;
+	uint64_t tag_dbe                      : 1;
+	uint64_t reserved_12_15               : 4;
+	uint64_t pktwq_sbe                    : 1;
+	uint64_t pktwq_dbe                    : 1;
+	uint64_t plc_sbe                      : 1;
+	uint64_t plc_dbe                      : 1;
+	uint64_t reserved_20_25               : 6;
+	uint64_t bpid_sbe                     : 1;
+	uint64_t bpid_dbe                     : 1;
+	uint64_t doa_sbe                      : 1;
+	uint64_t doa_dbe                      : 1;
+	uint64_t wqeout_sbe                   : 1;
+	uint64_t wqeout_dbe                   : 1;
+	uint64_t sws_sbe                      : 1;
+	uint64_t sws_dbe                      : 1;
+	uint64_t reserved_34_63               : 30;
+#endif
+	} s;
+	struct cvmx_pki_ecc_int1_s            cn78xx;
+};
+typedef union cvmx_pki_ecc_int1 cvmx_pki_ecc_int1_t;
+
+/**
+ * cvmx_pki_ecc_int2
+ */
+union cvmx_pki_ecc_int2 {
+	uint64_t u64;
+	struct cvmx_pki_ecc_int2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_2_63                : 62;
+	uint64_t imem_dbe                     : 1;  /**< IMEM ECC double bit error. Throws PKI_INTSN_E::PKI_ECC2_IMEM_DBE. */
+	uint64_t imem_sbe                     : 1;  /**< IMEM ECC single bit error. Throws PKI_INTSN_E::PKI_ECC2_IMEM_SBE. */
+#else
+	uint64_t imem_sbe                     : 1;
+	uint64_t imem_dbe                     : 1;
+	uint64_t reserved_2_63                : 62;
+#endif
+	} s;
+	struct cvmx_pki_ecc_int2_s            cn78xx;
+};
+typedef union cvmx_pki_ecc_int2 cvmx_pki_ecc_int2_t;
+
+/**
+ * cvmx_pki_frm_len_chk#
+ */
+union cvmx_pki_frm_len_chkx {
+	uint64_t u64;
+	struct cvmx_pki_frm_len_chkx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_32_63               : 32;
+	uint64_t maxlen                       : 16; /**< Byte count for max-sized frame check. */
+	uint64_t minlen                       : 16; /**< Byte count for min-sized frame check. */
+#else
+	uint64_t minlen                       : 16;
+	uint64_t maxlen                       : 16;
+	uint64_t reserved_32_63               : 32;
+#endif
+	} s;
+	struct cvmx_pki_frm_len_chkx_s        cn78xx;
+};
+typedef union cvmx_pki_frm_len_chkx cvmx_pki_frm_len_chkx_t;
+
+/**
+ * cvmx_pki_gbl_pen
+ */
+union cvmx_pki_gbl_pen {
+	uint64_t u64;
+	struct cvmx_pki_gbl_pen_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_10_63               : 54;
+	uint64_t virt_pen                     : 1;  /**< Virtualization parsing enable.
+                                                         0 = VXLAN/NVGRE is never used in any style. This enables internal power and latency
+                                                         reductions.
+                                                         1 = VXLAN/NVGRE parsing may be used. */
+	uint64_t clg_pen                      : 1;  /**< Custom LG parsing enable.
+                                                         0 = Custom LG is never used in any style; i.e. PKI_CL()_PKIND()_CFG[LG_CUSTOM] is
+                                                         zero for all indices. This enables internal power and latency reductions.
+                                                         1 = Custom LG parsing may be used. */
+	uint64_t cl2_pen                      : 1;  /**< Custom L2 parsing enable.
+                                                         0 = Custom L2 is never used in any style; i.e. PKI_CL()_PKIND()_L2_CUSTOM[VALID]
+                                                         is zero for all indices. This enables internal power and latency reductions.
+                                                         1 = Custom L2 parsing may be used. */
+	uint64_t l4_pen                       : 1;  /**< L4 parsing enable.
+                                                         0 = L4 parsing is never used in any style. This enables internal power and latency
+                                                         reductions.
+                                                         1 = L4 parsing may be used. */
+	uint64_t il3_pen                      : 1;  /**< L3 inner parsing enable. Must be zero if L3_PEN is zero.
+                                                         0 = L3 inner parsing is never used in any style. This enables internal power and latency
+                                                         reductions.
+                                                         1 = L3 inner (IP-in-IP) parsing may be used. */
+	uint64_t l3_pen                       : 1;  /**< L3 parsing enable.
+                                                         0 = L3 parsing is never used in any style. This enables internal power and latency
+                                                         reductions.
+                                                         1 = L3 parsing may be used. */
+	uint64_t mpls_pen                     : 1;  /**< MPLS parsing enable.
+                                                         0 = MPLS parsing is never used in any style; i.e. PKI_CL()_PKIND()_CFG[MPLS_EN]
+                                                         is zero for all indices. This enables internal power and latency reductions.
+                                                         1 = MPLS parsing may be used. */
+	uint64_t fulc_pen                     : 1;  /**< Fulcrum parsing enable.
+                                                         0 = Fulcrum parsing is never used in any style; i.e.
+                                                         PKI_CL()_PKIND()_CFG[FULC_EN] is zero for all indices. This enables internal
+                                                         power and latency reductions.
+                                                         1 = Fulcrum parsing may be used. */
+	uint64_t dsa_pen                      : 1;  /**< DSA parsing enable. Must be zero if HG_PEN is set.
+                                                         0 = DSA parsing is never used in any style; i.e. PKI_CL()_PKIND()_CFG[DSA_EN] is
+                                                         zero for all indices. This enables internal power and latency reductions.
+                                                         1 = DSA parsing may be used. */
+	uint64_t hg_pen                       : 1;  /**< HiGig parsing enable. Must be zero if DSA_PEN is set.
+                                                         0 = HiGig parsing is never used in any style; i.e. PKI_CL()_PKIND()_CFG[HG2_EN,
+                                                         HG_EN] are zero for all indices. This enables internal power and latency reductions.
+                                                         1 = HiGig parsing may be used. */
+#else
+	uint64_t hg_pen                       : 1;
+	uint64_t dsa_pen                      : 1;
+	uint64_t fulc_pen                     : 1;
+	uint64_t mpls_pen                     : 1;
+	uint64_t l3_pen                       : 1;
+	uint64_t il3_pen                      : 1;
+	uint64_t l4_pen                       : 1;
+	uint64_t cl2_pen                      : 1;
+	uint64_t clg_pen                      : 1;
+	uint64_t virt_pen                     : 1;
+	uint64_t reserved_10_63               : 54;
+#endif
+	} s;
+	struct cvmx_pki_gbl_pen_s             cn78xx;
+};
+typedef union cvmx_pki_gbl_pen cvmx_pki_gbl_pen_t;
+
+/**
+ * cvmx_pki_gen_int
+ */
+union cvmx_pki_gen_int {
+	uint64_t u64;
+	struct cvmx_pki_gen_int_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_8_63                : 56;
+	uint64_t x2p_req_ofl                  : 1;  /**< Set when a device attempts to have more than the allocated requests outstanding to PKI.
+                                                         Throws PKI_INTSN_E::PKI_GEN_X2P_REQ_OFL. */
+	uint64_t drp_noavail                  : 1;  /**< Set when packet dropped due to no FPA pointers available for the aura the packet
+                                                         requested. Throws PKI_INTSN_E::PKI_GEN_DRP_NOAVAIL. */
+	uint64_t dat                          : 1;  /**< Set when data arrives before a SOP for the same reasm-id for a packet. The first detected
+                                                         error associated with bits [DAT,SOP,EOP] of this register is only set here. A new bit can
+                                                         be set when the previous reported bit is cleared. Throws PKI_INTSN_E::PKI_GEN_DAT. */
+	uint64_t eop                          : 1;  /**< Set when an EOP is followed by an EOP for the same reasm-id for a packet. The first
+                                                         detected error associated with bits [DAT,EOP,SOP] of this register is only set here. A new
+                                                         bit can be set when the previous reported bit is cleared. Also see PKI_PKT_ERR. Throws
+                                                         PKI_INTSN_E::PKI_GEN_EOP. */
+	uint64_t sop                          : 1;  /**< Set when a SOP is followed by an SOP for the same reasm-id for a packet. The first
+                                                         detected error associated with bits [DAT,EOP,SOP] of this register is only set here. A new
+                                                         bit can be set when the previous reported bit is cleared. Also see PKI_PKT_ERR. Throws
+                                                         PKI_INTSN_E::PKI_GEN_SOP. */
+	uint64_t bckprs                       : 1;  /**< PKI asserted backpressure. Set when PKI was unable to accept the next valid data from
+                                                         BGX/DPI/ILK etc. over X2P due to all internal resources being used up, and PKI will
+                                                         backpressure X2P. Throws PKI_INTSN_E::PKI_GEN_BCKPRS. */
+	uint64_t crcerr                       : 1;  /**< PKI calculated bad CRC in the L2 frame. Throws PKI_INTSN_E::PKI_GEN_CRCERR. */
+	uint64_t pktdrp                       : 1;  /**< Packet dropped due to QOS. If the QOS algorithm decides to drop a packet, PKI asserts this
+                                                         interrupt. Throws PKI_INTSN_E::PKI_GEN_PKTDRP. */
+#else
+	uint64_t pktdrp                       : 1;
+	uint64_t crcerr                       : 1;
+	uint64_t bckprs                       : 1;
+	uint64_t sop                          : 1;
+	uint64_t eop                          : 1;
+	uint64_t dat                          : 1;
+	uint64_t drp_noavail                  : 1;
+	uint64_t x2p_req_ofl                  : 1;
+	uint64_t reserved_8_63                : 56;
+#endif
+	} s;
+	struct cvmx_pki_gen_int_s             cn78xx;
+};
+typedef union cvmx_pki_gen_int cvmx_pki_gen_int_t;
+
+/**
+ * cvmx_pki_icg#_cfg
+ *
+ * Configures a cluster group.
+ *
+ */
+union cvmx_pki_icgx_cfg {
+	uint64_t u64;
+	struct cvmx_pki_icgx_cfg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_53_63               : 11;
+	uint64_t maxipe_use                   : 5;  /**< Maximum number of IPEs to use in each cluster for this ICG. For diagnostic use only.
+                                                         INTERNAL: Allows reducing the number of IPEs available for debug, characterization,
+                                                         repair, etc. Must be 1-20. Normally, PKI will have all 20 IPEs available in a cluster for
+                                                         packet processing, other values will decrease performance. */
+	uint64_t reserved_36_47               : 12;
+	uint64_t clusters                     : 4;  /**< Bit-mask of clusters in this cluster group. A given cluster can only be enabled in a
+                                                         single cluster group. Behavior is undefined for an ICG which receives traffic with a
+                                                         [CLUSTERS] of 0x0. IGC(0)'s entry resets to 0xF, all other entries to 0x0. */
+	uint64_t reserved_27_31               : 5;
+	uint64_t release_rqd                  : 1;  /**< Release required. For diagnostic use only. INTERNAL:
+                                                         0 = Release of r64 to r95 will occur immediately, no release microop is needed.
+                                                         1 = Release will wait until release microop executes. */
+	uint64_t mlo                          : 1;  /**< Memory low bypass enable. For diagnostic use only. INTERNAL:
+                                                         0 = KMEM specifies contents of r48 to r63. The sequencer code expects this setting.
+                                                         1 = KMEM specifies contents of r32 to r47. This may be desirable when PKIENA=0 to allow
+                                                         direct control over the back end. */
+	uint64_t pena                         : 1;  /**< Parse enable. Must be set after PKI has been initialized.
+                                                         INTERNAL: Software should set after the IMEM and associated state is initialized.
+                                                         0 = IPT transitions from start directly to done without executing a sequence, and the KMEM
+                                                         bits effectively are copied through to the WQ.
+                                                         1 = Normal sequencer operation. */
+	uint64_t timer                        : 12; /**< Current hold-off timer. Enables even spreading of cluster utilization over time; while
+                                                         TIMER is non-zero, a cluster in this group will not start parsing. When a cluster in this
+                                                         group starts parsing, TIMER is set to DELAY, and decrements every coprocessor-clock. TIMER
+                                                         is zeroed if all clusters in this group are idle. */
+	uint64_t delay                        : 12; /**< Delay between cluster starts, as described under TIMER. If zero, a cluster can start at
+                                                         any time relative to other clusters. DELAY should be typically selected to minimize the
+                                                         average observed parser latency by loading with the parsing delay divided by the number of
+                                                         clusters in this cluster group. The smallest useful non-zero value is 0xA0, corresponding
+                                                         to the minimum number of cycles needed to fill one cluster with packets. */
+#else
+	uint64_t delay                        : 12;
+	uint64_t timer                        : 12;
+	uint64_t pena                         : 1;
+	uint64_t mlo                          : 1;
+	uint64_t release_rqd                  : 1;
+	uint64_t reserved_27_31               : 5;
+	uint64_t clusters                     : 4;
+	uint64_t reserved_36_47               : 12;
+	uint64_t maxipe_use                   : 5;
+	uint64_t reserved_53_63               : 11;
+#endif
+	} s;
+	struct cvmx_pki_icgx_cfg_s            cn78xx;
+};
+typedef union cvmx_pki_icgx_cfg cvmx_pki_icgx_cfg_t;
+
+/**
+ * cvmx_pki_imem#
+ */
+union cvmx_pki_imemx {
+	uint64_t u64;
+	struct cvmx_pki_imemx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t data                         : 64; /**< Instruction word at given address. */
+#else
+	uint64_t data                         : 64;
+#endif
+	} s;
+	struct cvmx_pki_imemx_s               cn78xx;
+};
+typedef union cvmx_pki_imemx cvmx_pki_imemx_t;
+
+/**
+ * cvmx_pki_ltype#_map
+ */
+union cvmx_pki_ltypex_map {
+	uint64_t u64;
+	struct cvmx_pki_ltypex_map_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_3_63                : 61;
+	uint64_t beltype                      : 3;  /**< For each given PKI_LTYPE_E, the protocol type backend hardware should assume this layer
+                                                         type corresponds to. Enumerated by PKI_BELTYPE_E. The recommended settings for each
+                                                         PKI_LTYPE_E are shown in the PKI_LTYPE_E table. */
+#else
+	uint64_t beltype                      : 3;
+	uint64_t reserved_3_63                : 61;
+#endif
+	} s;
+	struct cvmx_pki_ltypex_map_s          cn78xx;
+};
+typedef union cvmx_pki_ltypex_map cvmx_pki_ltypex_map_t;
+
+/**
+ * cvmx_pki_pcam_lookup
+ *
+ * For diagnostic use only, perform a PCAM lookup against the provided cluster and PCAM instance
+ * and loads results into PKI_PCAM_RESULT.
+ */
+union cvmx_pki_pcam_lookup {
+	uint64_t u64;
+	struct cvmx_pki_pcam_lookup_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_54_63               : 10;
+	uint64_t cl                           : 2;  /**< Cluster number to lookup within. */
+	uint64_t reserved_49_51               : 3;
+	uint64_t pcam                         : 1;  /**< PCAM number to lookup within. */
+	uint64_t term                         : 8;  /**< Term value to lookup. */
+	uint64_t style                        : 8;  /**< Style value to lookup. */
+	uint64_t data                         : 32; /**< Data to lookup. */
+#else
+	uint64_t data                         : 32;
+	uint64_t style                        : 8;
+	uint64_t term                         : 8;
+	uint64_t pcam                         : 1;
+	uint64_t reserved_49_51               : 3;
+	uint64_t cl                           : 2;
+	uint64_t reserved_54_63               : 10;
+#endif
+	} s;
+	struct cvmx_pki_pcam_lookup_s         cn78xx;
+};
+typedef union cvmx_pki_pcam_lookup cvmx_pki_pcam_lookup_t;
+
+/**
+ * cvmx_pki_pcam_result
+ */
+union cvmx_pki_pcam_result {
+	uint64_t u64;
+	struct cvmx_pki_pcam_result_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_41_63               : 23;
+	uint64_t match                        : 1;  /**< Resulting match. */
+	uint64_t entry                        : 8;  /**< Resulting matching entry number, unpredictable unless [MATCH] set and [CONFLICT] is clear. */
+	uint64_t result                       : 32; /**< Resulting data from matching line's PKI_CL()_PCAM()_ACTION(), or zero if no
+                                                         match. Unpredictable unless [CONFLICT] is clear. */
+#else
+	uint64_t result                       : 32;
+	uint64_t entry                        : 8;
+	uint64_t match                        : 1;
+	uint64_t reserved_41_63               : 23;
+#endif
+	} s;
+	struct cvmx_pki_pcam_result_cn78xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t conflict                     : 1;  /**< Conflict. The lookup resulted in multiple entries matching PKI_PCAM_LOOKUP[DATA], [TERM]
+                                                         and [STYLE], or zero if no conflict. */
+	uint64_t reserved_41_62               : 22;
+	uint64_t match                        : 1;  /**< Resulting match. */
+	uint64_t entry                        : 8;  /**< Resulting matching entry number, unpredictable unless [MATCH] set and [CONFLICT] is clear. */
+	uint64_t result                       : 32; /**< Resulting data from matching line's PKI_CL()_PCAM()_ACTION(), or zero if no
+                                                         match. Unpredictable unless [CONFLICT] is clear. */
+#else
+	uint64_t result                       : 32;
+	uint64_t entry                        : 8;
+	uint64_t match                        : 1;
+	uint64_t reserved_41_62               : 22;
+	uint64_t conflict                     : 1;
+#endif
+	} cn78xx;
+};
+typedef union cvmx_pki_pcam_result cvmx_pki_pcam_result_t;
+
+/**
+ * cvmx_pki_pfe_diag
+ */
+union cvmx_pki_pfe_diag {
+	uint64_t u64;
+	struct cvmx_pki_pfe_diag_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t bad_rid                      : 1;  /**< Asserted when PFE sees and drops an X2P transaction with a RID > 95. */
+#else
+	uint64_t bad_rid                      : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_pki_pfe_diag_s            cn78xx;
+};
+typedef union cvmx_pki_pfe_diag cvmx_pki_pfe_diag_t;
+
+/**
+ * cvmx_pki_pix_clken
+ */
+union cvmx_pki_pix_clken {
+	uint64_t u64;
+	struct cvmx_pki_pix_clken_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_17_63               : 47;
+	uint64_t mech                         : 1;  /**< When set, force the conditional clocks on for mech. */
+	uint64_t reserved_4_15                : 12;
+	uint64_t cls                          : 4;  /**< When set, force the conditional clocks on for this cluster. */
+#else
+	uint64_t cls                          : 4;
+	uint64_t reserved_4_15                : 12;
+	uint64_t mech                         : 1;
+	uint64_t reserved_17_63               : 47;
+#endif
+	} s;
+	struct cvmx_pki_pix_clken_s           cn78xx;
+};
+typedef union cvmx_pki_pix_clken cvmx_pki_pix_clken_t;
+
+/**
+ * cvmx_pki_pix_diag
+ */
+union cvmx_pki_pix_diag {
+	uint64_t u64;
+	struct cvmx_pki_pix_diag_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t nosched                      : 4;  /**< Asserted when PFE requests an ICG with no enabled CLUSTERS. */
+#else
+	uint64_t nosched                      : 4;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_pki_pix_diag_s            cn78xx;
+};
+typedef union cvmx_pki_pix_diag cvmx_pki_pix_diag_t;
+
+/**
+ * cvmx_pki_pkind#_icgsel
+ */
+union cvmx_pki_pkindx_icgsel {
+	uint64_t u64;
+	struct cvmx_pki_pkindx_icgsel_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_2_63                : 62;
+	uint64_t icg                          : 2;  /**< Cluster group that will service traffic on this pkind. See also PKI_ICG()_CFG, the
+                                                         register to which this field indexes. */
+#else
+	uint64_t icg                          : 2;
+	uint64_t reserved_2_63                : 62;
+#endif
+	} s;
+	struct cvmx_pki_pkindx_icgsel_s       cn78xx;
+};
+typedef union cvmx_pki_pkindx_icgsel cvmx_pki_pkindx_icgsel_t;
+
+/**
+ * cvmx_pki_pknd#_inb_stat0
+ *
+ * Inbound packets received by PKI per pkind.
+ *
+ */
+union cvmx_pki_pkndx_inb_stat0 {
+	uint64_t u64;
+	struct cvmx_pki_pkndx_inb_stat0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t pkts                         : 48; /**< Number of packets without errors received by PKI. */
+#else
+	uint64_t pkts                         : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pki_pkndx_inb_stat0_s     cn78xx;
+};
+typedef union cvmx_pki_pkndx_inb_stat0 cvmx_pki_pkndx_inb_stat0_t;
+
+/**
+ * cvmx_pki_pknd#_inb_stat1
+ *
+ * Inbound octets received by PKI per pkind.
+ *
+ */
+union cvmx_pki_pkndx_inb_stat1 {
+	uint64_t u64;
+	struct cvmx_pki_pkndx_inb_stat1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t octs                         : 48; /**< Total number of octets from all packets received by PKI. */
+#else
+	uint64_t octs                         : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pki_pkndx_inb_stat1_s     cn78xx;
+};
+typedef union cvmx_pki_pkndx_inb_stat1 cvmx_pki_pkndx_inb_stat1_t;
+
+/**
+ * cvmx_pki_pknd#_inb_stat2
+ *
+ * Inbound error packets received by PKI per pkind.
+ *
+ */
+union cvmx_pki_pkndx_inb_stat2 {
+	uint64_t u64;
+	struct cvmx_pki_pkndx_inb_stat2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t errs                         : 48; /**< Number of packets with errors received by PKI. */
+#else
+	uint64_t errs                         : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pki_pkndx_inb_stat2_s     cn78xx;
+};
+typedef union cvmx_pki_pkndx_inb_stat2 cvmx_pki_pkndx_inb_stat2_t;
+
+/**
+ * cvmx_pki_pkt_err
+ */
+union cvmx_pki_pkt_err {
+	uint64_t u64;
+	struct cvmx_pki_pkt_err_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_7_63                : 57;
+	uint64_t reasm                        : 7;  /**< When PKI_GEN_INT[DAT,SOP,EOP] is set, this field latches the failing reassembly number
+                                                         associated with the error. */
+#else
+	uint64_t reasm                        : 7;
+	uint64_t reserved_7_63                : 57;
+#endif
+	} s;
+	struct cvmx_pki_pkt_err_s             cn78xx;
+};
+typedef union cvmx_pki_pkt_err cvmx_pki_pkt_err_t;
+
+/**
+ * cvmx_pki_qpg_tbl#
+ *
+ * The QPG table is used to indirectly calculate the Portadd/Aura/Group from the Diffsrv, HiGig
+ * or VLAN information as described in QPG.
+ */
+union cvmx_pki_qpg_tblx {
+	uint64_t u64;
+	struct cvmx_pki_qpg_tblx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_60_63               : 4;
+	uint64_t padd                         : 12; /**< Port to channel adder for calculating WQE[CHAN]. */
+	uint64_t grptag_ok                    : 3;  /**< Number of WQE[TAG] bits to add into WQE[GRP] if no error is detected. */
+	uint64_t reserved_42_44               : 3;
+	uint64_t grp_ok                       : 10; /**< SSO group to schedule packet to and to load WQE[GRP] with if no error is detected. */
+	uint64_t grptag_bad                   : 3;  /**< Number of WQE[TAG] bits to add into WQE[GRP] if an error is detected. */
+	uint64_t reserved_26_28               : 3;
+	uint64_t grp_bad                      : 10; /**< SSO group to schedule packet to and to load WQE[GRP] with if an error is detected. */
+	uint64_t reserved_12_15               : 4;
+	uint64_t aura_node                    : 2;  /**< Aura node number. The node number is part of the upper aura bits, however PKI can only
+                                                         allocate from auras on the local node, therefore these bits are hardcoded to the node
+                                                         number. */
+	uint64_t laura                        : 10; /**< Aura on local node for QOS calculations and loading into WQE[AURA]WQE[AURA]. */
+#else
+	uint64_t laura                        : 10;
+	uint64_t aura_node                    : 2;
+	uint64_t reserved_12_15               : 4;
+	uint64_t grp_bad                      : 10;
+	uint64_t reserved_26_28               : 3;
+	uint64_t grptag_bad                   : 3;
+	uint64_t grp_ok                       : 10;
+	uint64_t reserved_42_44               : 3;
+	uint64_t grptag_ok                    : 3;
+	uint64_t padd                         : 12;
+	uint64_t reserved_60_63               : 4;
+#endif
+	} s;
+	struct cvmx_pki_qpg_tblx_s            cn78xx;
+};
+typedef union cvmx_pki_qpg_tblx cvmx_pki_qpg_tblx_t;
+
+/**
+ * cvmx_pki_reasm_sop#
+ *
+ * Set when a SOP is detected on a Reasm-Id, where the Reasm-ID value sets the bit vector of this
+ * register.
+ */
+union cvmx_pki_reasm_sopx {
+	uint64_t u64;
+	struct cvmx_pki_reasm_sopx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t sop                          : 64; /**< When set, a SOP was detected on a reasm-Id. When clear, a SOP has not yet been received,
+                                                         or an EOP was received on the Reasm-Id. */
+#else
+	uint64_t sop                          : 64;
+#endif
+	} s;
+	struct cvmx_pki_reasm_sopx_s          cn78xx;
+};
+typedef union cvmx_pki_reasm_sopx cvmx_pki_reasm_sopx_t;
+
+/**
+ * cvmx_pki_req_wgt
+ *
+ * Controls the round-robin weights between each PKI requestor. Intended for diagnostic tuning only.
+ *
+ */
+union cvmx_pki_req_wgt {
+	uint64_t u64;
+	struct cvmx_pki_req_wgt_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_36_63               : 28;
+	uint64_t wgt8                         : 4;  /**< Weight for ILK0. */
+	uint64_t wgt7                         : 4;  /**< Weight for LBK. */
+	uint64_t wgt6                         : 4;  /**< Weight for DPI. */
+	uint64_t wgt5                         : 4;  /**< Weight for BGX5. */
+	uint64_t wgt4                         : 4;  /**< Weight for BGX4. */
+	uint64_t wgt3                         : 4;  /**< Weight for BGX3. */
+	uint64_t wgt2                         : 4;  /**< Weight for BGX2. */
+	uint64_t wgt1                         : 4;  /**< Weight for BGX1. */
+	uint64_t wgt0                         : 4;  /**< Weight for BGX0. */
+#else
+	uint64_t wgt0                         : 4;
+	uint64_t wgt1                         : 4;
+	uint64_t wgt2                         : 4;
+	uint64_t wgt3                         : 4;
+	uint64_t wgt4                         : 4;
+	uint64_t wgt5                         : 4;
+	uint64_t wgt6                         : 4;
+	uint64_t wgt7                         : 4;
+	uint64_t wgt8                         : 4;
+	uint64_t reserved_36_63               : 28;
+#endif
+	} s;
+	struct cvmx_pki_req_wgt_s             cn78xx;
+};
+typedef union cvmx_pki_req_wgt cvmx_pki_req_wgt_t;
+
+/**
+ * cvmx_pki_sft_rst
+ */
+union cvmx_pki_sft_rst {
+	uint64_t u64;
+	struct cvmx_pki_sft_rst_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t busy                         : 1;  /**< When set, PKI is busy completing reset. No access except the reading of this bit should
+                                                         occur to the PKI until this is clear. INTERNAL: The BUSY bit for this implementation is a
+                                                         placeholder and is not required to be implemented in HW. The soft reset pulse is short
+                                                         enough that we can guarantee that reset will complete below a subsequent RSL reference can
+                                                         be made. It is still useful for this bit to exist in case that property every changes and
+                                                         the reset requires a longer duration. For this implementation, SW will check the bit which
+                                                         will always report not BUSY allowing SW to proceed with its flow. */
+	uint64_t reserved_33_62               : 30;
+	uint64_t active                       : 1;  /**< When set, PKI is actively processing packet traffic. It is recommenced that software wait
+                                                         until ACTIVE is clear before setting RST. INTERNAL: ACTIVE is an OR of PKI_ACTIVE0..2. */
+	uint64_t reserved_1_31                : 31;
+	uint64_t rst                          : 1;  /**< Reset. When set to 1 by software, PKI will produce an internal reset pulse. */
+#else
+	uint64_t rst                          : 1;
+	uint64_t reserved_1_31                : 31;
+	uint64_t active                       : 1;
+	uint64_t reserved_33_62               : 30;
+	uint64_t busy                         : 1;
+#endif
+	} s;
+	struct cvmx_pki_sft_rst_s             cn78xx;
+};
+typedef union cvmx_pki_sft_rst cvmx_pki_sft_rst_t;
+
+/**
+ * cvmx_pki_stat#_hist0
+ */
+union cvmx_pki_statx_hist0 {
+	uint64_t u64;
+	struct cvmx_pki_statx_hist0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t h1to63                       : 48; /**< Number of non-dropped 1 to 63 byte packets. Packet length includes FCS and any prepended
+                                                         PTP timestamp. */
+#else
+	uint64_t h1to63                       : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pki_statx_hist0_s         cn78xx;
+};
+typedef union cvmx_pki_statx_hist0 cvmx_pki_statx_hist0_t;
+
+/**
+ * cvmx_pki_stat#_hist1
+ */
+union cvmx_pki_statx_hist1 {
+	uint64_t u64;
+	struct cvmx_pki_statx_hist1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t h64to127                     : 48; /**< Number of non-dropped 64 to 127 byte packets. */
+#else
+	uint64_t h64to127                     : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pki_statx_hist1_s         cn78xx;
+};
+typedef union cvmx_pki_statx_hist1 cvmx_pki_statx_hist1_t;
+
+/**
+ * cvmx_pki_stat#_hist2
+ */
+union cvmx_pki_statx_hist2 {
+	uint64_t u64;
+	struct cvmx_pki_statx_hist2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t h128to255                    : 48; /**< Number of non-dropped 128 to 255 byte packets. */
+#else
+	uint64_t h128to255                    : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pki_statx_hist2_s         cn78xx;
+};
+typedef union cvmx_pki_statx_hist2 cvmx_pki_statx_hist2_t;
+
+/**
+ * cvmx_pki_stat#_hist3
+ */
+union cvmx_pki_statx_hist3 {
+	uint64_t u64;
+	struct cvmx_pki_statx_hist3_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t h256to511                    : 48; /**< Number of non-dropped 256 to 511 byte packets. */
+#else
+	uint64_t h256to511                    : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pki_statx_hist3_s         cn78xx;
+};
+typedef union cvmx_pki_statx_hist3 cvmx_pki_statx_hist3_t;
+
+/**
+ * cvmx_pki_stat#_hist4
+ */
+union cvmx_pki_statx_hist4 {
+	uint64_t u64;
+	struct cvmx_pki_statx_hist4_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t h512to1023                   : 48; /**< Number of non-dropped 512 to 1023 byte packets. */
+#else
+	uint64_t h512to1023                   : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pki_statx_hist4_s         cn78xx;
+};
+typedef union cvmx_pki_statx_hist4 cvmx_pki_statx_hist4_t;
+
+/**
+ * cvmx_pki_stat#_hist5
+ */
+union cvmx_pki_statx_hist5 {
+	uint64_t u64;
+	struct cvmx_pki_statx_hist5_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t h1024to1518                  : 48; /**< Number of non-dropped 1024 to 1518 byte packets. */
+#else
+	uint64_t h1024to1518                  : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pki_statx_hist5_s         cn78xx;
+};
+typedef union cvmx_pki_statx_hist5 cvmx_pki_statx_hist5_t;
+
+/**
+ * cvmx_pki_stat#_hist6
+ */
+union cvmx_pki_statx_hist6 {
+	uint64_t u64;
+	struct cvmx_pki_statx_hist6_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t h1519                        : 48; /**< Number of non-dropped 1519 byte and above packets. */
+#else
+	uint64_t h1519                        : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pki_statx_hist6_s         cn78xx;
+};
+typedef union cvmx_pki_statx_hist6 cvmx_pki_statx_hist6_t;
+
+/**
+ * cvmx_pki_stat#_stat0
+ */
+union cvmx_pki_statx_stat0 {
+	uint64_t u64;
+	struct cvmx_pki_statx_stat0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t pkts                         : 48; /**< Number of non-dropped packets processed by PKI. */
+#else
+	uint64_t pkts                         : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pki_statx_stat0_s         cn78xx;
+};
+typedef union cvmx_pki_statx_stat0 cvmx_pki_statx_stat0_t;
+
+/**
+ * cvmx_pki_stat#_stat1
+ */
+union cvmx_pki_statx_stat1 {
+	uint64_t u64;
+	struct cvmx_pki_statx_stat1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t octs                         : 48; /**< Number of non-dropped octets received by PKI (good and bad). */
+#else
+	uint64_t octs                         : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pki_statx_stat1_s         cn78xx;
+};
+typedef union cvmx_pki_statx_stat1 cvmx_pki_statx_stat1_t;
+
+/**
+ * cvmx_pki_stat#_stat10
+ */
+union cvmx_pki_statx_stat10 {
+	uint64_t u64;
+	struct cvmx_pki_statx_stat10_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t jabber                       : 48; /**< Number of non-dropped packets with length > maximum and FCS error. */
+#else
+	uint64_t jabber                       : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pki_statx_stat10_s        cn78xx;
+};
+typedef union cvmx_pki_statx_stat10 cvmx_pki_statx_stat10_t;
+
+/**
+ * cvmx_pki_stat#_stat11
+ */
+union cvmx_pki_statx_stat11 {
+	uint64_t u64;
+	struct cvmx_pki_statx_stat11_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t oversz                       : 48; /**< Number of non-dropped packets with length > maximum and no FCS error. */
+#else
+	uint64_t oversz                       : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pki_statx_stat11_s        cn78xx;
+};
+typedef union cvmx_pki_statx_stat11 cvmx_pki_statx_stat11_t;
+
+/**
+ * cvmx_pki_stat#_stat12
+ */
+union cvmx_pki_statx_stat12 {
+	uint64_t u64;
+	struct cvmx_pki_statx_stat12_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t l2err                        : 48; /**< Number of non-dropped packets with receive errors (WQE[ERRLEV]==RE or L2) not covered by
+                                                         more specific length or FCS statistic error registers. */
+#else
+	uint64_t l2err                        : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pki_statx_stat12_s        cn78xx;
+};
+typedef union cvmx_pki_statx_stat12 cvmx_pki_statx_stat12_t;
+
+/**
+ * cvmx_pki_stat#_stat13
+ */
+union cvmx_pki_statx_stat13 {
+	uint64_t u64;
+	struct cvmx_pki_statx_stat13_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t spec                         : 48; /**< Number of non-dropped packets with special handling. For profiling and diagnostic use
+                                                         only.
+                                                         INTERNAL: Counts packets completing IPE processing with WQE[SH] set. */
+#else
+	uint64_t spec                         : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pki_statx_stat13_s        cn78xx;
+};
+typedef union cvmx_pki_statx_stat13 cvmx_pki_statx_stat13_t;
+
+/**
+ * cvmx_pki_stat#_stat14
+ */
+union cvmx_pki_statx_stat14 {
+	uint64_t u64;
+	struct cvmx_pki_statx_stat14_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t drp_bcast                    : 48; /**< Number of packets with L2 broadcast DMAC that were dropped by RED, buffer exhaustion, or
+                                                         PKI_CL()_STYLE()_CFG[DROP]. See WQE[L2B] for the definition of L2 broadcast. */
+#else
+	uint64_t drp_bcast                    : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pki_statx_stat14_s        cn78xx;
+};
+typedef union cvmx_pki_statx_stat14 cvmx_pki_statx_stat14_t;
+
+/**
+ * cvmx_pki_stat#_stat15
+ */
+union cvmx_pki_statx_stat15 {
+	uint64_t u64;
+	struct cvmx_pki_statx_stat15_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t drp_mcast                    : 48; /**< Number of packets with L2 multicast DMAC that were dropped by RED, buffer exhaustion, or
+                                                         PKI_CL()_STYLE()_CFG[DROP]. See WQE[L2M] for the definition of L2 multicast. */
+#else
+	uint64_t drp_mcast                    : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pki_statx_stat15_s        cn78xx;
+};
+typedef union cvmx_pki_statx_stat15 cvmx_pki_statx_stat15_t;
+
+/**
+ * cvmx_pki_stat#_stat16
+ */
+union cvmx_pki_statx_stat16 {
+	uint64_t u64;
+	struct cvmx_pki_statx_stat16_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t drp_bcast                    : 48; /**< Number of packets with IPv4 L3 broadcast destination address that were dropped due to RED
+                                                         or buffer exhaustion. See WQE[L3B] for the definition of L2 multicast. */
+#else
+	uint64_t drp_bcast                    : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pki_statx_stat16_s        cn78xx;
+};
+typedef union cvmx_pki_statx_stat16 cvmx_pki_statx_stat16_t;
+
+/**
+ * cvmx_pki_stat#_stat17
+ */
+union cvmx_pki_statx_stat17 {
+	uint64_t u64;
+	struct cvmx_pki_statx_stat17_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t drp_mcast                    : 48; /**< Number of packets with IPv4 or IPv6 L3 multicast destination address that were dropped due
+                                                         to RED or buffer exhaustion. See WQE[L3M] for the definition of L3 multicast. */
+#else
+	uint64_t drp_mcast                    : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pki_statx_stat17_s        cn78xx;
+};
+typedef union cvmx_pki_statx_stat17 cvmx_pki_statx_stat17_t;
+
+/**
+ * cvmx_pki_stat#_stat18
+ */
+union cvmx_pki_statx_stat18 {
+	uint64_t u64;
+	struct cvmx_pki_statx_stat18_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t drp_spec                     : 48; /**< Number of packets dropped with special handling. For profiling and diagnostic use only.
+                                                         INTERNAL: Counts packets with dropped after completing IPE processing with WQE[SH] set. */
+#else
+	uint64_t drp_spec                     : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pki_statx_stat18_s        cn78xx;
+};
+typedef union cvmx_pki_statx_stat18 cvmx_pki_statx_stat18_t;
+
+/**
+ * cvmx_pki_stat#_stat2
+ */
+union cvmx_pki_statx_stat2 {
+	uint64_t u64;
+	struct cvmx_pki_statx_stat2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t raw                          : 48; /**< Number of non-dropped packets with WQE[RAW] set. */
+#else
+	uint64_t raw                          : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pki_statx_stat2_s         cn78xx;
+};
+typedef union cvmx_pki_statx_stat2 cvmx_pki_statx_stat2_t;
+
+/**
+ * cvmx_pki_stat#_stat3
+ */
+union cvmx_pki_statx_stat3 {
+	uint64_t u64;
+	struct cvmx_pki_statx_stat3_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t drp_pkts                     : 48; /**< Inbound packets dropped by RED, buffer exhaustion, or PKI_CL()_STYLE()_CFG[DROP]. */
+#else
+	uint64_t drp_pkts                     : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pki_statx_stat3_s         cn78xx;
+};
+typedef union cvmx_pki_statx_stat3 cvmx_pki_statx_stat3_t;
+
+/**
+ * cvmx_pki_stat#_stat4
+ */
+union cvmx_pki_statx_stat4 {
+	uint64_t u64;
+	struct cvmx_pki_statx_stat4_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t drp_octs                     : 48; /**< Inbound octets dropped by RED, buffer exhaustion, or PKI_CL()_STYLE()_CFG[DROP]. */
+#else
+	uint64_t drp_octs                     : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pki_statx_stat4_s         cn78xx;
+};
+typedef union cvmx_pki_statx_stat4 cvmx_pki_statx_stat4_t;
+
+/**
+ * cvmx_pki_stat#_stat5
+ */
+union cvmx_pki_statx_stat5 {
+	uint64_t u64;
+	struct cvmx_pki_statx_stat5_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t bcast                        : 48; /**< Number of non-dropped L2 broadcast packets. Does not include multicast packets. See
+                                                         WQE[L2B] for the definition of L2 broadcast. */
+#else
+	uint64_t bcast                        : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pki_statx_stat5_s         cn78xx;
+};
+typedef union cvmx_pki_statx_stat5 cvmx_pki_statx_stat5_t;
+
+/**
+ * cvmx_pki_stat#_stat6
+ */
+union cvmx_pki_statx_stat6 {
+	uint64_t u64;
+	struct cvmx_pki_statx_stat6_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t mcast                        : 48; /**< Number of non-dropped L2 multicast packets. Does not include broadcast packets. See
+                                                         WQE[L2M] for the definition of L2 multicast. */
+#else
+	uint64_t mcast                        : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pki_statx_stat6_s         cn78xx;
+};
+typedef union cvmx_pki_statx_stat6 cvmx_pki_statx_stat6_t;
+
+/**
+ * cvmx_pki_stat#_stat7
+ */
+union cvmx_pki_statx_stat7 {
+	uint64_t u64;
+	struct cvmx_pki_statx_stat7_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t fcs                          : 48; /**< Number of non-dropped packets with an FCS opcode error, excluding fragments or overruns. */
+#else
+	uint64_t fcs                          : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pki_statx_stat7_s         cn78xx;
+};
+typedef union cvmx_pki_statx_stat7 cvmx_pki_statx_stat7_t;
+
+/**
+ * cvmx_pki_stat#_stat8
+ */
+union cvmx_pki_statx_stat8 {
+	uint64_t u64;
+	struct cvmx_pki_statx_stat8_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t frag                         : 48; /**< Number of non-dropped packets with length < minimum and FCS error */
+#else
+	uint64_t frag                         : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pki_statx_stat8_s         cn78xx;
+};
+typedef union cvmx_pki_statx_stat8 cvmx_pki_statx_stat8_t;
+
+/**
+ * cvmx_pki_stat#_stat9
+ */
+union cvmx_pki_statx_stat9 {
+	uint64_t u64;
+	struct cvmx_pki_statx_stat9_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t undersz                      : 48; /**< Number of non-dropped packets with length < minimum and no FCS error. */
+#else
+	uint64_t undersz                      : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pki_statx_stat9_s         cn78xx;
+};
+typedef union cvmx_pki_statx_stat9 cvmx_pki_statx_stat9_t;
+
+/**
+ * cvmx_pki_stat_ctl
+ *
+ * Controls how the PKI statistics counters are handled.
+ *
+ */
+union cvmx_pki_stat_ctl {
+	uint64_t u64;
+	struct cvmx_pki_stat_ctl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_2_63                : 62;
+	uint64_t mode                         : 2;  /**< The PKI_STAT*_X registers can be indexed either by port kind (pkind), or final style.
+                                                         (Does not apply to the PKI_STAT_INB* registers.)
+                                                         _ 0x0 = X represents the packet's pkind.
+                                                         _ 0x1 = X represents the low 6-bits of packet's final style.
+                                                         _ 0x2-0x3 = Reserved. */
+#else
+	uint64_t mode                         : 2;
+	uint64_t reserved_2_63                : 62;
+#endif
+	} s;
+	struct cvmx_pki_stat_ctl_s            cn78xx;
+};
+typedef union cvmx_pki_stat_ctl cvmx_pki_stat_ctl_t;
+
+/**
+ * cvmx_pki_style#_buf
+ */
+union cvmx_pki_stylex_buf {
+	uint64_t u64;
+	struct cvmx_pki_stylex_buf_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_33_63               : 31;
+	uint64_t pkt_lend                     : 1;  /**< Packet little-endian. Changes write operations of packet data to L2C to be in little-
+                                                         endian. Does not change the WQE header format, which is properly endian neutral. */
+	uint64_t wqe_hsz                      : 2;  /**< Work queue header size:
+                                                         0x0 = WORD0..4, standard WQE_S. Note FIRST_SKIP may be set to not include WORD4 in memory.
+                                                         0x1 = WORD0..5.
+                                                         0x2 = WORD0..6.
+                                                         0x3 = WORD0..7.
+                                                         INTERNAL: Selects which PIX words are transferred to the PKI BE. If a word is not
+                                                         transferred and the word will reach memory (FIRST_SKIP is greater than that word number),
+                                                         then the final WQE memory word will be zero, not the PIX register contents. */
+	uint64_t wqe_skip                     : 2;  /**< WQE start offset. The number of 128-byte cache lines to skip between the buffer pointer
+                                                         and WORD0 of the work-queue entry. See PKI Hardware Allocating Multiple Buffers.
+                                                         If [DIS_WQ_DAT]=1, legal values must satisfy:
+                                                           * MB_SIZE >= (PKI_STYLE()_BUF[WQE_SKIP] * (128/8)) + 18
+                                                         If [DIS_WQ_DAT]=0, legal values must satisfy:
+                                                           * (WQE_SKIP * (128/8)) + 4 <= FIRST_SKIP, to insure the minimum of four work-queue entry
+                                                             words will fit within FIRST_SKIP. */
+	uint64_t first_skip                   : 6;  /**< The number of eight-byte words from the top of the first MBUF that the PKI stores the next
+                                                         pointer.
+                                                         If [DIS_WQ_DAT]=1, legal values must satisfy:
+                                                           * FIRST_SKIP <= PKI_STYLE()_BUF[MB_SIZE] - 18.
+                                                         If [DIS_WQ_DAT]=0, legal values must satisfy:
+                                                           * FIRST_SKIP <= PKI_STYLE()_BUF[MB_SIZE] - 18.
+                                                           * (WQE_SKIP * (128/8)) + X <= FIRST_SKIP,
+                                                             X must be at least 0x4 to insure the minimum of four work-queue entry,
+                                                             but 0x5 is recommended minimum. X=0x4 will drop WQE WORD4, for use in
+                                                             backward compatible applications. */
+	uint64_t later_skip                   : 6;  /**< The number of eight-byte words from the top of any MBUF that is not the first MBUF that
+                                                         PKI writes the next-pointer to. Legal values are 0 to PKI_STYLE()_BUF[MB_SIZE] - 18. */
+	uint64_t opc_mode                     : 2;  /**< Select the style of write to the L2C.
+                                                         0x0 = all packet data and next-buffer pointers are written through to memory.
+                                                         0x1 = all packet data and next-buffer pointers are written into the cache.
+                                                         0x2 = the first aligned cache block holding the WQE and/or front packet data are written
+                                                         to
+                                                         the L2 cache. All remaining cache blocks are not written to the L2 cache.
+                                                         0x3 = the first two aligned cache blocks holding the WQE and front packet data are written
+                                                         to the L2 cache. All remaining cache blocks are not written to the L2 cache. */
+	uint64_t dis_wq_dat                   : 1;  /**< Separate first data buffer from the work queue entry.
+                                                         0 = The packet link pointer will be at word [FIRST_SKIP] immediately followed by packet
+                                                         data, in the same buffer as the work queue entry.
+                                                         1 = The packet link pointer will be at word [FIRST_SKIP] in a new buffer separate from the
+                                                         work queue entry. Words following the WQE in the same cache line will be zeroed, other
+                                                         lines in the buffer will not be modified and will retain stale data (from the buffer's
+                                                         previous use). This setting may decrease the peak PKI performance by up to half on small
+                                                         packets. */
+	uint64_t mb_size                      : 13; /**< The number of eight-byte words between the start of a buffer and the last word
+                                                         that PKI may write into that buffer. The total amount of data stored by PKI into
+                                                         the buffer will be MB_SIZE minus FIRST_SKIP or LATER_SKIP.
+                                                         This must be even, in the range of 32 to 4096. This must be less than or equal
+                                                         to the maximum size of every free page in every FPA pool this style may use.
+                                                         If [DIS_WQ_DAT]=1, legal values must satisfy:
+                                                           * MB_SIZE >= (PKI_STYLE()_BUF[WQE_SKIP] * (128/8)) + 18
+                                                           * MB_SIZE >= PKI_STYLE()_BUF[FIRST_SKIP] + 18.
+                                                           * MB_SIZE >= PKI_STYLE()_BUF[LATER_SKIP] + 18.
+                                                         If [DIS_WQ_DAT]=0, legal values must satisfy:
+                                                           * MB_SIZE >= PKI_STYLE()_BUF[FIRST_SKIP] + 18.
+                                                           * MB_SIZE >= PKI_STYLE()_BUF[LATER_SKIP] + 18. */
+#else
+	uint64_t mb_size                      : 13;
+	uint64_t dis_wq_dat                   : 1;
+	uint64_t opc_mode                     : 2;
+	uint64_t later_skip                   : 6;
+	uint64_t first_skip                   : 6;
+	uint64_t wqe_skip                     : 2;
+	uint64_t wqe_hsz                      : 2;
+	uint64_t pkt_lend                     : 1;
+	uint64_t reserved_33_63               : 31;
+#endif
+	} s;
+	struct cvmx_pki_stylex_buf_s          cn78xx;
+};
+typedef union cvmx_pki_stylex_buf cvmx_pki_stylex_buf_t;
+
+/**
+ * cvmx_pki_style#_tag_mask
+ */
+union cvmx_pki_stylex_tag_mask {
+	uint64_t u64;
+	struct cvmx_pki_stylex_tag_mask_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t mask                         : 16; /**< When set, each bit excludes corresponding bit of the tuple computed tag from being
+                                                         included in the final tag. PKI_CL()_STYLE()_CFG2 [TAG_MASKEN] must be set. Does
+                                                         not affect tags from packets with a PKI_INST_HDR_S when PKI_INST_HDR_S[UTAG] is set. */
+#else
+	uint64_t mask                         : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_pki_stylex_tag_mask_s     cn78xx;
+};
+typedef union cvmx_pki_stylex_tag_mask cvmx_pki_stylex_tag_mask_t;
+
+/**
+ * cvmx_pki_style#_tag_sel
+ */
+union cvmx_pki_stylex_tag_sel {
+	uint64_t u64;
+	struct cvmx_pki_stylex_tag_sel_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_27_63               : 37;
+	uint64_t tag_idx3                     : 3;  /**< Index for TAG_INC<3>. */
+	uint64_t reserved_19_23               : 5;
+	uint64_t tag_idx2                     : 3;  /**< Index for TAG_INC<2>. */
+	uint64_t reserved_11_15               : 5;
+	uint64_t tag_idx1                     : 3;  /**< Index for TAG_INC<1>. */
+	uint64_t reserved_3_7                 : 5;
+	uint64_t tag_idx0                     : 3;  /**< Index for TAG_INC<0>. This value is multipled by 4 to index into PKI_TAG_INC()_MASK.
+                                                         See WQE[TAG]. */
+#else
+	uint64_t tag_idx0                     : 3;
+	uint64_t reserved_3_7                 : 5;
+	uint64_t tag_idx1                     : 3;
+	uint64_t reserved_11_15               : 5;
+	uint64_t tag_idx2                     : 3;
+	uint64_t reserved_19_23               : 5;
+	uint64_t tag_idx3                     : 3;
+	uint64_t reserved_27_63               : 37;
+#endif
+	} s;
+	struct cvmx_pki_stylex_tag_sel_s      cn78xx;
+};
+typedef union cvmx_pki_stylex_tag_sel cvmx_pki_stylex_tag_sel_t;
+
+/**
+ * cvmx_pki_style#_wq2
+ */
+union cvmx_pki_stylex_wq2 {
+	uint64_t u64;
+	struct cvmx_pki_stylex_wq2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t data                         : 64; /**< Data for WQ2<63:0>. This is ORed over any parser calculated WQ2<63:0> fields, and is used
+                                                         to emulate as if the parser set a WQ field such as WQE[PF1]. PKI_INST_HDR_S packets may
+                                                         also want to use this mode to set WQE[LCTY] to IP when PKI parsing IP is disabled. */
+#else
+	uint64_t data                         : 64;
+#endif
+	} s;
+	struct cvmx_pki_stylex_wq2_s          cn78xx;
+};
+typedef union cvmx_pki_stylex_wq2 cvmx_pki_stylex_wq2_t;
+
+/**
+ * cvmx_pki_style#_wq4
+ */
+union cvmx_pki_stylex_wq4 {
+	uint64_t u64;
+	struct cvmx_pki_stylex_wq4_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t data                         : 64; /**< Data for WQ4<63:0>. This is ORed over any parser calculated WQ4<63:0> fields, and is used
+                                                         to emulate as if the parser set a WQ pointer field. PKI_INST_HDR_S packets may also want
+                                                         to use this mode to set WQE[LCPTR] to the start of IP when PKI parsing IP is disabled. */
+#else
+	uint64_t data                         : 64;
+#endif
+	} s;
+	struct cvmx_pki_stylex_wq4_s          cn78xx;
+};
+typedef union cvmx_pki_stylex_wq4 cvmx_pki_stylex_wq4_t;
+
+/**
+ * cvmx_pki_tag_inc#_ctl
+ */
+union cvmx_pki_tag_incx_ctl {
+	uint64_t u64;
+	struct cvmx_pki_tag_incx_ctl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_12_63               : 52;
+	uint64_t ptr_sel                      : 4;  /**< Which pointer to use for the bitmask in PKI_TAG_INC()_MASK.
+                                                         0 = Absolute from start of packet.
+                                                         1-7 = Reserved.
+                                                         8 = Relative to start of WQE[LAPTR]. LAPTR must be valid (see WQE[LAPTR]) or mask is
+                                                         ignored.
+                                                         9 = Relative to start of WQE[LBPTR]. LBPTR must be valid (see WQE[LBPTR]) or mask is
+                                                         ignored.
+                                                         10 = Relative to start of WQE[LCPTR]. LCPTR must be valid (see WQE[LCPTR]) or mask is
+                                                         ignored.
+                                                         11 = Relative to start of WQE[LDPTR]. LDPTR must be valid (see WQE[LDPTR]) or mask is
+                                                         ignored.
+                                                         12 = Relative to start of WQE[LEPTR]. LEPTR must be valid (see WQE[LEPTR]) or mask is
+                                                         ignored.
+                                                         13 = Relative to start of WQE[LFPTR]. LFPTR must be valid (see WQE[LFPTR]) or mask is
+                                                         ignored.
+                                                         14 = Relative to start of WQE[LGPTR]. LGPTR must be valid (see WQE[LGPTR]) or mask is
+                                                         ignored.
+                                                         15 = Relative to start of WQE[VLPTR]. VLPTR must be valid (see WQE[VLPTR]) or mask is
+                                                         ignored.
+                                                         INTERNAL: Note excluding 0, the encoding matches the byte number to read from WQE WORD4. */
+	uint64_t offset                       : 8;  /**< Offset for PKI_TAG_INC()_MASK. Number of bytes to add to the selected pointer before
+                                                         applying the mask. */
+#else
+	uint64_t offset                       : 8;
+	uint64_t ptr_sel                      : 4;
+	uint64_t reserved_12_63               : 52;
+#endif
+	} s;
+	struct cvmx_pki_tag_incx_ctl_s        cn78xx;
+};
+typedef union cvmx_pki_tag_incx_ctl cvmx_pki_tag_incx_ctl_t;
+
+/**
+ * cvmx_pki_tag_inc#_mask
+ */
+union cvmx_pki_tag_incx_mask {
+	uint64_t u64;
+	struct cvmx_pki_tag_incx_mask_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t en                           : 64; /**< Include byte in mask-tag algorithm. Each EN bit corresponds to 64 consecutive bytes in the
+                                                         data stream, as controlled by PKI_TAG_INC()_CTL as described in WQE[TAG]. */
+#else
+	uint64_t en                           : 64;
+#endif
+	} s;
+	struct cvmx_pki_tag_incx_mask_s       cn78xx;
+};
+typedef union cvmx_pki_tag_incx_mask cvmx_pki_tag_incx_mask_t;
+
+/**
+ * cvmx_pki_tag_secret
+ *
+ * The source and destination initial values (IVs) in tag generation provide a mechanism for
+ * seeding with a random initialization value to reduce cache collision attacks.
+ */
+union cvmx_pki_tag_secret {
+	uint64_t u64;
+	struct cvmx_pki_tag_secret_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t dst6                         : 16; /**< Secret for the destination tuple IPv6 tag CRC calculation. Typically identical to SRC6 to
+                                                         insure tagging is symmetric between source/destination flows. Typically different from DST
+                                                         for maximum security. */
+	uint64_t src6                         : 16; /**< Secret for the source tuple IPv6 tag CRC calculation. Typically different from SRC for
+                                                         maximum security. */
+	uint64_t dst                          : 16; /**< Secret for the destination tuple tag CRC calculation. Typically identical to DST6 to
+                                                         insure tagging is symmetric between source/destination flows. */
+	uint64_t src                          : 16; /**< Secret for the source tuple tag CRC calculation. */
+#else
+	uint64_t src                          : 16;
+	uint64_t dst                          : 16;
+	uint64_t src6                         : 16;
+	uint64_t dst6                         : 16;
+#endif
+	} s;
+	struct cvmx_pki_tag_secret_s          cn78xx;
+};
+typedef union cvmx_pki_tag_secret cvmx_pki_tag_secret_t;
+
+/**
+ * cvmx_pki_x2p_req_ofl
+ */
+union cvmx_pki_x2p_req_ofl {
+	uint64_t u64;
+	struct cvmx_pki_x2p_req_ofl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t x2p_did                      : 4;  /**< When PKI_GEN_INT[X2P_REQ_OFL] is set, this field latches the X2P device id number which
+                                                         attempted to overflow the allowed outstanding requests to PKI. */
+#else
+	uint64_t x2p_did                      : 4;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_pki_x2p_req_ofl_s         cn78xx;
+};
+typedef union cvmx_pki_x2p_req_ofl cvmx_pki_x2p_req_ofl_t;
+
+#endif
diff --git a/arch/mips/include/asm/octeon/cvmx-pki-resources.h b/arch/mips/include/asm/octeon/cvmx-pki-resources.h
new file mode 100644
index 0000000..e77792c
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-pki-resources.h
@@ -0,0 +1,162 @@
+/***********************license start***************
+ * Copyright (c) 2003-2010  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Resource management for PKI resources.
+ */
+
+#ifndef __CVMX_PKI_RESOURCES_H__
+#define __CVMX_PKI_RESOURCES_H__
+
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include <asm/octeon/cvmx.h>
+#endif
+
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+extern "C" {
+/* *INDENT-ON* */
+#endif
+
+/**
+ * This function allocates/reserves a style from pool of global styles per node.
+ * @param node	 node to allocate style from.
+ * @param style	 style to allocate, if -1 it will be allocated
+                 first available style from style resource. If index is positive
+		 number and in range, it will try to allocate specified style.
+ * @return 	 style number on success, -1 on failure.
+ */
+int cvmx_pki_style_alloc(int node, int style);
+
+/**
+ * This function allocates/reserves a cluster group from per node
+   cluster group resources.
+ * @param node	 	node to allocate cluster group from.
+   @param cl_grp	cluster group to allocate/reserve, if -1 ,
+                        allocate any available cluster group.
+ * @return 	 	cluster group number or -1 on failure
+ */
+int cvmx_pki_cluster_grp_alloc(int node, int cl_grp);
+
+/**
+ * This function allocates/reserves a cluster from per node
+   cluster resources.
+ * @param node	 	node to allocate cluster group from.
+   @param cluster_mask	mask of clusters  to allocate/reserve, if -1 ,
+                        allocate any available clusters.
+ * @param num_clusters	number of clusters that will be allocated
+ */
+int cvmx_pki_cluster_alloc(int node, int num_clusters, uint64_t *cluster_mask);
+
+
+/**
+ * This function allocates/reserves a pcam entry from node
+ * @param node	 	node to allocate pcam entry from.
+   @param index  	index of pacm entry (0-191), if -1 ,
+                        allocate any available pcam entry.
+ * @param bank		pcam bank where to allocate/reserve pcan entry from
+ * @param cluster_mask  mask of clusters from which pcam entry is needed.
+ * @return 	 	pcam entry of -1 on failure
+ */
+int cvmx_pki_pcam_entry_alloc(int node, int index, int bank, uint64_t cluster_mask);
+
+/**
+ * This function allocates/reserves QPG table entries per node.
+ * @param node	 	node number.
+ * @param base_offset	base_offset in qpg table. If -1, first available
+			qpg base_offset will be allocated. If base_offset is positive
+		 	number and in range, it will try to allocate specified base_offset.
+   @param count		number of consecutive qpg entries to allocate. They will be consecutive
+                        from base offset.
+ * @return 	 	qpg table base offset number on success, -1 on failure.
+ */
+int cvmx_pki_qpg_entry_alloc(int node, int base_offset, int count);
+
+/**
+ * This function frees a style from pool of global styles per node.
+ * @param node	 node to free style from.
+ * @param style	 style to free
+ * @return 	 0 on success, -1 on failure.
+ */
+int cvmx_pki_style_free(int node, int style);
+
+
+/**
+ * This function frees a cluster group from per node
+   cluster group resources.
+ * @param node	 	node to free cluster group from.
+   @param cl_grp	cluster group to free
+ * @return 	 	0 on success or -1 on failure
+ */
+int cvmx_pki_cluster_grp(int node, int cl_grp);
+
+/**
+ * This function frees  clusters  from per node
+   clusters resources.
+ * @param node	 	node to free clusters from.
+ * @param cluster_mask  mask of clusters need freeing
+ * @return 	 	0 on success or -1 on failure
+ */
+int cvmx_pki_cluster_free(int node, uint64_t cluster_mask);
+
+/**
+ * This function frees a pcam entry from node
+ * @param node	 	node to allocate pcam entry from.
+   @param index  	index of pacm entry (0-191) needs to be freed.
+ * @param bank		pcam bank where to free pcam entry from
+ * @param cluster_mask  mask of clusters from which pcam entry is freed.
+ * @return 	 	0 on success OR -1 on failure
+ */
+int cvmx_pki_pcam_entry_free(int node, int index, int bank, uint64_t cluster_mask);
+
+/**
+ * This function frees all the PKI software resources
+ * (clusters, styles, qpg_entry, pcam_entry etc) for the specified node
+ */
+void __cvmx_pki_global_rsrc_free(int node);
+
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+}
+/* *INDENT-ON* */
+#endif
+
+#endif /*  __CVM_PKI_RESOURCES_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-pki.h b/arch/mips/include/asm/octeon/cvmx-pki.h
new file mode 100644
index 0000000..eaf6424
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-pki.h
@@ -0,0 +1,1206 @@
+/***********************license start***************
+ * Copyright (c) 2003-2010  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Interface to the hardware Packet Input Data unit.
+ */
+
+#ifndef __CVMX_PKI_H__
+#define __CVMX_PKI_H__
+
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include <asm/octeon/cvmx.h>
+#include <asm/octeon/cvmx-pki-defs.h>
+#include <asm/octeon/cvmx-fpa3.h>
+#include <asm/octeon/cvmx-helper-util.h>
+#include <asm/octeon/cvmx-helper-cfg.h>
+#else
+#include "cvmx-fpa3.h"
+#include "cvmx-helper-util.h"
+#include "cvmx-helper-cfg.h"
+#endif
+
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+extern "C" {
+/* *INDENT-ON* */
+#endif
+
+#define CVMX_PKI_NUM_CHANNEL		(4096)
+#define CVMX_PKI_NUM_AURA		(1024)
+#define CVMX_PKI_NUM_BPID		(1024)
+#define CVMX_PKI_NUM_PKIND		(64)
+#define CVMX_PKI_NUM_INTERNAL_STYLE     (256)
+#define CVMX_PKI_NUM_FINAL_STYLE 	(64)
+#define CVMX_PKI_NUM_QPG_ENTRY		(2048)
+#define CVMX_PKI_NUM_LTYPE		(32)
+#define CVMX_PKI_NUM_CLUSTER		(4)
+#define CVMX_PKI_NUM_CLUSTER_GROUP      (4)
+#define CVMX_PKI_NUM_PCAM_BANK		(2)
+#define CVMX_PKI_NUM_PCAM_ENTRY		(192)
+#define CVMX_PKI_NUM_FRAME_CHECK	(2)
+#define CVMX_PKI_NUM_BPID		(1024)
+#define CVMX_PKI_NUM_SSO_GROUP		(256)
+#define CVMX_PKI_NUM_BELTYPE		(32)
+#define CVMX_PKI_MAX_FRAME_SIZE		(65535)
+#define CVMX_PKI_FIND_AVAL_ENTRY        (-1)
+#define CVMX_PKI_CLUSTER_ALL		(0xf)
+
+#ifdef CVMX_SUPPORT_SEPARATE_CLUSTER_CONFIG
+#define CVMX_PKI_TOTAL_PCAM_ENTRY	((CVMX_PKI_NUM_CLUSTER) * (CVMX_PKI_NUM_PCAM_BANK) *\
+						(CVMX_PKI_NUM_PCAM_ENTRY))
+#else
+#define CVMX_PKI_TOTAL_PCAM_ENTRY	(CVMX_PKI_NUM_PCAM_BANK * CVMX_PKI_NUM_PCAM_ENTRY)
+#endif
+
+enum cvmx_pki_pkind_parse_mode {
+	CVMX_PKI_PARSE_LA_TO_LG = 0,	/* parse LA(L2) to LG */
+	CVMX_PKI_PARSE_LB_TO_LG = 1,	/* parse LB(custom) to LG */
+	CVMX_PKI_PARSE_LC_TO_LG = 3,	/* parse LC(L3) to LG */
+	CVMX_PKI_PARSE_LG = 0x3f,	/* parse LG */
+	CVMX_PKI_PARSE_NOTHING = 0x7f	/* parse nothing */
+};
+
+enum cvmx_pki_parse_mode_chg {
+	CVMX_PKI_PARSE_NO_CHG = 0x0,
+	CVMX_PKI_PARSE_SKIP_TO_LB = 0x1,
+	CVMX_PKI_PARSE_SKIP_TO_LC = 0x3,
+	CVMX_PKI_PARSE_SKIP_TO_LD = 0x7,
+	CVMX_PKI_PARSE_SKIP_TO_LG = 0x3f,
+	CVMX_PKI_PARSE_SKIP_ALL = 0x7f,
+};
+
+enum cvmx_pki_l2_len_mode {
+	PKI_L2_LENCHK_EQUAL_GREATER = 0,
+	PKI_L2_LENCHK_EQUAL_ONLY
+};
+
+enum cvmx_pki_cache_mode {
+	CVMX_PKI_OPC_MODE_STT = 0LL,	/* All blocks write through DRAM,*/
+	CVMX_PKI_OPC_MODE_STF = 1LL,	/* All blocks into L2 */
+	CVMX_PKI_OPC_MODE_STF1_STT = 2LL,	/* 1st block L2, rest DRAM */
+	CVMX_PKI_OPC_MODE_STF2_STT = 3LL	/* 1st, 2nd blocks L2, rest DRAM */
+};
+
+/**
+ * Tag type definitions
+ */
+enum cvmx_sso_tag_type {
+	CVMX_SSO_TAG_TYPE_ORDERED = 0L,	/**< Tag ordering is maintained */
+	CVMX_SSO_TAG_TYPE_ATOMIC = 1L,	/**< Tag ordering is maintained, and at most one PP has the tag */
+	CVMX_SSO_TAG_TYPE_UNTAGGED = 2L,	/**< The work queue entry from the order
+	- NEVER tag switch from NULL to NULL */
+	CVMX_SSO_TAG_TYPE_EMPTY = 3L/**< A tag switch to NULL, and there is no space reserved in POW
+			- NEVER tag switch to NULL_NULL
+			- NEVER tag switch from NULL_NULL
+			- NULL_NULL is entered at the beginning of time and on a deschedule.
+			- NULL_NULL can be exited by a new work request. A NULL_SWITCH load can also switch the state to NULL */
+};
+
+enum cvmx_pki_qpg_qos {
+	CVMX_PKI_QPG_QOS_NONE = 0,
+	CVMX_PKI_QPG_QOS_VLAN,
+	CVMX_PKI_QPG_QOS_MPLS,
+	CVMX_PKI_QPG_QOS_DSA_SRC,
+	CVMX_PKI_QPG_QOS_DIFFSERV,
+	CVMX_PKI_QPG_QOS_HIGIG,
+};
+
+enum cvmx_pki_wqe_vlan {
+	CVMX_PKI_USE_FIRST_VLAN = 0,
+	CVMX_PKI_USE_SECOND_VLAN
+};
+
+/**
+ * Controls how the PKI statistics counters are handled
+ * The PKI_STAT*_X registers can be indexed either by port kind (pkind), or
+ * final style. (Does not apply to the PKI_STAT_INB* registers.)
+ *    0 = X represents the packets pkind
+ *    1 = X represents the low 6-bits of packets final style
+ */
+enum cvmx_pki_stats_mode {
+	CVMX_PKI_STAT_MODE_PKIND,
+	CVMX_PKI_STAT_MODE_STYLE
+};
+
+#define PKI_BELTYPE_E__NONE_M                              (0x0)
+#define PKI_BELTYPE_E__MISC_M                              (0x1)
+#define PKI_BELTYPE_E__IP4_M                               (0x2)
+#define PKI_BELTYPE_E__IP6_M                               (0x3)
+#define PKI_BELTYPE_E__TCP_M                               (0x4)
+#define PKI_BELTYPE_E__UDP_M                               (0x5)
+#define PKI_BELTYPE_E__SCTP_M                              (0x6)
+#define PKI_BELTYPE_E__SNAP_M                              (0x7)
+
+enum cvmx_pki_beltype { /* PKI_BELTYPE_E_t */
+	CVMX_PKI_BELTYPE_NONE	= PKI_BELTYPE_E__NONE_M,
+	CVMX_PKI_BELTYPE_MISC	= PKI_BELTYPE_E__MISC_M,
+	CVMX_PKI_BELTYPE_IP4	= PKI_BELTYPE_E__IP4_M,
+	CVMX_PKI_BELTYPE_IP6    = PKI_BELTYPE_E__IP6_M,
+	CVMX_PKI_BELTYPE_TCP    = PKI_BELTYPE_E__TCP_M,
+	CVMX_PKI_BELTYPE_UDP    = PKI_BELTYPE_E__UDP_M,
+	CVMX_PKI_BELTYPE_SCTP   = PKI_BELTYPE_E__SCTP_M,
+	CVMX_PKI_BELTYPE_SNAP   = PKI_BELTYPE_E__SNAP_M
+};
+
+struct cvmx_pki_frame_len {
+	uint16_t	maxlen;
+	uint16_t	minlen;
+};
+
+struct cvmx_pki_tag_fields {
+	uint64_t layer_g_src:1;
+	uint64_t layer_f_src:1;
+	uint64_t layer_e_src:1;
+	uint64_t layer_d_src:1;
+	uint64_t layer_c_src:1;
+	uint64_t layer_b_src:1;
+	uint64_t layer_g_dst:1;
+	uint64_t layer_f_dst:1;
+	uint64_t layer_e_dst:1;
+	uint64_t layer_d_dst:1;
+	uint64_t layer_c_dst:1;
+	uint64_t layer_b_dst:1;
+	uint64_t input_port:1;
+	uint64_t mpls_label:1;
+	uint64_t first_vlan:1;
+	uint64_t second_vlan:1;
+	uint64_t ip_prot_nexthdr:1;
+	uint64_t tag_sync:1;
+	uint64_t tag_spi:1;
+	uint64_t tag_gtp:1;
+	uint64_t tag_vni:1;
+};
+
+struct cvmx_pki_pkind_parse {
+	uint64_t mpls_en:1;	/**< Enable MPLS parsing.
+				0 = Any MPLS labels are ignored, but may be handled by custom Ethertype PCAM matchers.
+				1 = MPLS label stacks are parsed and skipped over. PKI_GBL_PEN[MPLS_PEN] must be set. */
+	uint64_t inst_hdr:1;	/**< INST header. When set, the eight-byte INST_HDR is present on all packets (except incoming
+				packets on the DPI ports). */
+	uint64_t lg_custom:1;	/**< Layer G Custom Match Enable.
+				0 = Disable custom LG header extraction
+				1 = Enable custom LG header extraction.*/
+	uint64_t fulc_en:1;	/**< Enable Fulcrum tag parsing.
+				0 = Any Fulcrum header is ignored.
+				1 = Fulcrum header is parsed.*/
+	uint64_t dsa_en:1;	/**< Enable DSA parsing. This field should not be set for DPI ports.
+				0 = Any DSA header is ignored.
+				1 = DSA is parsed. */
+	uint64_t hg2_en:1;	/**< Enable HiGig 2 parsing. This field should not be set for DPI ports.
+				0 = Any HiGig2 header is ignored.
+				1 = HiGig2 is parsed. PKI_GBL_PEN[HG_PEN] must be set. */
+	uint64_t hg_en:1;	/**< Enable HiGig parsing. This field should not be set for DPI ports.
+				0 = Any HiGig header is ignored.
+				1 = HiGig is parsed. PKI_GBL_PEN[HG_PEN] must be set.
+				At most one of FULC_EN, DSA_EN or HG_EN may be set. */
+};
+
+struct cvmx_pki_pool_config {
+	int pool_num;
+	uint64_t buffer_size;
+	uint64_t buffer_count;
+};
+
+struct cvmx_pki_qpg_config {
+	int  qpg_base;
+	int  port_add;
+	int  aura;
+	int  grp_ok;
+	int  grp_bad;
+};
+
+struct cvmx_pki_aura_config {
+	int aura_num;
+	int pool_num;
+	int buffer_count;
+};
+
+/* This is per style structure for configuring port parameters, it is kind of of profile
+   which can be assigned to any port. If multiple ports are assigned same style be aware
+   that modiying that style will modify the respective parameters for all the ports which
+   are using this style */
+struct cvmx_pki_style_parm {
+
+	bool ip6_udp_opt;	/**< IPv6/UDP checksum is optional. IPv4 allows an optional UDP checksum by sending the all-0s
+					patterns. IPv6 outlaws this and the spec says to always check UDP checksum.
+					0 = Spec compliant, do not allow optional code.
+					1 = Treat IPv6 as IPv4; */
+	bool lenerr_en;         /**< L2 length error check enable. Check if frame was received with L2 length error. */
+	bool maxerr_en;         /**< Max frame error check enable. */
+	bool minerr_en;	        /**< Min frame error check enable. */
+
+	uint8_t lenerr_eqpad;	/**< L2 length checks exact pad size.
+					0 = Length check uses greater then or equal comparison.
+					1 = Length check uses equal comparison.*/
+	uint8_t minmax_sel;	/**< Selects which PKI_FRM_LEN_CHK(0..1) register is used for this pkind for MINERR and MAXERR */
+	bool qpg_dis_grptag;	/**< Disable computing group using WQE[TAG]. 1 -- Disable 0 -- Enable */
+	bool fcs_strip;         /**< Strip L2 FCS bytes from packet, decrease WQE[LEN] by 4 bytes.*/
+	bool fcs_chk;           /**< FCS checking enabled.*/
+	bool rawdrp;		/**< Allow RAW packet drop
+					0 = Never drop packets with WQE[RAW] set.
+					1 = Allow the PKI to drop RAW packets based on PKI_AURA(0..1023)_CFG[ENA_RED/ENA_DROP]. */
+	bool force_drop;	/**< Force packet dropping.
+					0 = Drop packet based on PKI_AURA(0..1023)_CFG[ENA_RED/ENA_DROP].
+					1 = Always drop the packet. Overrides NODROP, RAWDRP. */
+	bool nodrop;            /**< Disable QoS packet drop.
+					0 = Allowed to drop packet based on PKI_AURA(0..1023)_CFG[ENA_RED/ENA_DROP].
+					1 = Never drop the packet. Overrides RAWDRP. */
+	bool qpg_dis_padd;	/**< Disable computing port adder by QPG algorithm. */
+	bool qpg_dis_grp;       /**< Disable computing group by QPG algorithm. */
+	bool qpg_dis_aura;      /**< Disable computing aura by QPG algorithm. */
+	uint16_t qpg_base;	/**< Base index into PKI_QPG_TBL(0..2047)*/
+	enum cvmx_pki_qpg_qos	qpg_qos;	/**< Algorithm to select QoS field in QPG calculation */
+	uint8_t			qpg_port_sh;	/**< MSB to take from port number in QPG calculation
+							0 = Exclude port number from QPG.
+							4 = Include port<3:0>.
+							8 = Include port<7:0>.*/
+	uint8_t			qpg_port_msb;	/**< Number of bits to shift port number in QPG */
+	uint8_t apad_nip;			/**< Value for WQE[APAD] when packet is not IP. */
+	uint8_t wqe_vs;				/**< Which VLAN to put into WQE[VLPTR] when VLAN stacking.
+							0 = Use the first (in network order) VLAN or DSA VID.
+							1 = Use the second (in network order) VLAN. */
+
+	enum cvmx_sso_tag_type	tag_type;	/**< SSO tag type to schedule to */
+	bool pkt_lend;				/**< Packet little-endian.write operations of packet data to L2C to be in LE */
+	uint8_t wqe_hsz;			/**< Work queue header size:
+							0x0 = WORD0..4, standard WQE_S. Note FIRST_SKIP may be set to not include WORD4 in memory.
+							0x1 = WORD0..5
+							0x2 = WORD0..6
+							0x3 = WORD0..7
+							else = Reserved */
+	uint8_t wqe_skip;			/**< in bytes, WQE start offset. The number of 128-byte cache lines to skip between the buffer
+							Pointer and WORD0 of the work-queue entry.*/
+	uint8_t first_skip;			/**< in bytes, The number of eight-byte words from the top of the first MBUF
+							that the PKI stores the next pointer.*/
+	uint8_t later_skip;			/**< in bytes, The number of eight-byte words from the top of any MBUF
+							that is not the first MBUF that PKI writes next-pointer to.*/
+	enum cvmx_pki_cache_mode cache_mode;;	/**< Select the style of write to the L2C.
+							0 = all packet data and next-buffer pointers are written through to memory.
+							1 = all packet data and next-buffer pointers are written into the cache.
+							2 = the first aligned cache block holding the WQE and/or front packet data are written to
+							    the L2 cache. All remaining cache blocks are not written to the L2 cache.
+							3 = the first two aligned cache blocks holding the WQE and front packet data are written
+							    to the L2 cache. All remaining cache blocks are not written to the L2 cache. */
+	uint8_t dis_wq_dat;			/**< Separate first data buffer from the work queue entry.
+							0 = The packet link pointer will be at word [FIRST_SKIP] immediately followed by packet
+							    data, in the same buffer as the work queue entry.
+							1 = The packet link pointer will be at word [FIRST_SKIP] in a new buffer separate from the
+							    work queue entry.*/
+	uint64_t mbuff_size;			/**< The number of eight-byte words to store into a buffer. This must be even, in the range of
+						     32 to 4096. This must be less than or equal to the maximum size of every free page in
+						     every FPA pool this style may use. */
+	bool len_lg;				/**< Check length of Layer G. */
+	bool len_lf;				/**< Check length of Layer F. */
+	bool len_le;				/**< Check length of Layer E. */
+	bool len_ld;				/**< Check length of Layer D. */
+	bool len_lc;				/**< Check length of Layer C. */
+	bool len_lb;				/**< Check length of Layer B. */
+	bool csum_lg;				/**< Compute checksum on Layer G. */
+	bool csum_lf;				/**< Compute checksum on Layer F. */
+	bool csum_le;				/**< Compute checksum on Layer E. */
+	bool csum_ld;				/**< Compute checksum on Layer D. */
+	bool csum_lc;				/**< Compute checksum on Layer C. */
+	bool csum_lb;				/**< Compute checksum on Layer B. */
+};
+
+/* This is per style structure for configuring port's tag configuration, it is kind of of profile
+   which can be assigned to any port. If multiple ports are assigned same style be aware
+   that modiying that style will modify the respective parameters for all the ports which
+   are using this style */
+
+struct cvmx_pki_mask_tag {
+	uint64_t tag_inc;			/**< Include masked tags using PKI_TAG_INC(0..31)_MASK. Each bit indicates to include the
+						     corresponding PKI_TAG_INC_MASK range. */
+	uint64_t tag_masken;			/**< Apply PKI_STYLE(0..63)_TAG_MASK to computed tag.*/
+	uint64_t mask;				/**< When set, each bit excludes corresponding bit of the tuple computed tag from being
+						     included in the final tag. PKI_CL(0..3)_STYLE(0..63)_CFG2 [TAG_MASKEN] must be set. Does
+						     not affect tags from packets with a PKI_INST_HDR_S when PKI_INST_HDR[UTAG] is set */
+	uint64_t tag_idx[4];			/**< Index 0-3 for TAG_INC<3>. This value is multipled by 4 to index into PKI_TAG_INC(0..31)_MASK.
+						     See WQE[TAG]. */
+};
+
+struct cvmx_pki_style_tag_cfg {
+	struct cvmx_pki_tag_fields tag_fields;
+	struct cvmx_pki_mask_tag   mask_tag;
+};
+
+struct cvmx_pki_style_config {
+	struct cvmx_pki_style_parm parm_cfg;		/**< General parameter configuration. */
+	struct cvmx_pki_style_tag_cfg  tag_cfg;		/**< Tag parameter configuration. */
+};
+
+struct cvmx_pki_pkind_config {
+	uint8_t cluster_grp;		/**< cluster group that will service traffic on this pkind */
+	bool fcs_pres;			/**< FCS present.
+					0 = FCS not present. FCS may not be checked nor stripped.
+					1 = FCS present; the last four bytes of the packet are part of the FCS and may not be
+					considered part of a IP, TCP or other header for length error checks.*/
+	struct cvmx_pki_pkind_parse parse_en;
+	enum cvmx_pki_pkind_parse_mode	initial_parse_mode;
+	int initial_style;
+	bool custom_l2_hdr;		/**< Valid.custom L2 hesder extraction
+					0 = Disable custom L2 header extraction.
+					1 = Enable custom L2 header extraction.
+					PKI_GBL_PEN[CLG_PEN] must be set. */
+	uint8_t l2_scan_offset;		/**< Scan offset for custom L2 header.
+					Pointer to first byte of 32-bit custom extraction header, as absolute number
+					of bytes from beginning of packet. If PTP_MODE, the 8-byte timestamp is prepended to the
+					packet, and must be included in counting offset bytes. */
+	uint64_t lg_scan_offset;	/**< Scan offset for custom lg header.
+					Pointer to first byte of 32-bit custom extraction header, as relative number
+					of bytes from WQE[LFPTR]. */
+};
+
+struct cvmx_pki_port_config {
+	struct cvmx_pki_pkind_config pkind_cfg;		/**< Parameters can be configure per pkind */
+	struct cvmx_pki_style_config style_cfg;		/**< Parameters are configured per style, style is a profile
+							which can be applied to multiple ports which have same configuration
+							and packet processing */
+};
+
+struct cvmx_pki_global_parse {
+	uint64_t virt_pen:1;	/**< Virtualization parsing enable.*/
+	uint64_t clg_pen:1;	/**< Custom LG parsing enable. */
+	uint64_t cl2_pen:1;	/**< Custom L2 parsing enable.*/
+	uint64_t l4_pen:1;	/**< L4 parsing enable.*/
+	uint64_t il3_pen:1;	/**< L3 inner parsing enable. Must be zero if L3_PEN is zero. */
+	uint64_t l3_pen:1;	/**< L3 parsing enable.*/
+	uint64_t mpls_pen:1;	/**< MPLS parsing enable.*/
+	uint64_t fulc_pen:1;	/**< Fulcrum parsing enable.*/
+	uint64_t dsa_pen:1;	/**< DSA parsing enable. Must be zero if HG_PEN is set.*/
+	uint64_t hg_pen:1;	/**< HiGig parsing enable. Must be zero if DSA_PEN is set.*/
+};
+
+struct cvmx_pki_tag_sec {
+	uint16_t dst6;				/**< Secret for the destination tuple IPv6 tag CRC calculation. Typically identical to SRC6 to						insure tagging is symmetric between source/destination flows. Typically different from DST
+						for maximum security. */
+	uint16_t src6;				/**< Secret for the source tuple IPv6 tag CRC calculation. Typically different from SRC for
+						maximum security. */
+	uint16_t dst;				/**< Secret for the destination tuple tag CRC calculation. Typically identical to DST6 to
+						insure tagging is symmetric between source/destination flows. */
+	uint16_t src;				/**< Secret for the source tuple tag CRC calculation. */
+};
+
+struct cvmx_pki_global_config {
+	uint64_t cluster_mask[CVMX_PKI_NUM_CLUSTER_GROUP];	/**< Mask of clusters associated with that cluster group,
+								there are 4 cluster groups and 4 clusters which can be assigned
+								to cluster groups */
+	enum cvmx_pki_stats_mode stat_mode;			/**< The PKI_STAT*_X registers can be indexed either by pkind or final style.
+								(Does not apply to the PKI_STAT_INB* registers.)
+								0 = X represents the packet's pkind
+								1 = X represents the low 6-bits of packet's final style */
+	struct cvmx_pki_global_parse gbl_pen;			/**< Controls Global parsing options for chip */
+	struct cvmx_pki_tag_sec tag_secret;			/**< Secret value for src/dst tag tuple to reduce cache collision attacks */
+	struct cvmx_pki_frame_len frm_len[CVMX_PKI_NUM_FRAME_CHECK]; /**< values for max and min frame length to check against,2 combination */
+	enum cvmx_pki_beltype ltype_map[CVMX_PKI_NUM_BELTYPE];	/**< Map of which protocol maps to what layer */
+	/* struct cvmx_pki_tag_ctl  tag_ctl[32]; */
+	/* cvmx_pki_tag_incx_mask_t tag_mask[32]; */
+	int pki_enable;
+};
+
+#define CVMX_PKI_PCAM_TERM_E_NONE_M                            (0x0)
+#define CVMX_PKI_PCAM_TERM_E_L2_CUSTOM_M                       (0x2)
+#define CVMX_PKI_PCAM_TERM_E_HIGIG_M                           (0x5)
+#define CVMX_PKI_PCAM_TERM_E_DMACH_M                           (0xA)
+#define CVMX_PKI_PCAM_TERM_E_DMACL_M                           (0xB)
+#define CVMX_PKI_PCAM_TERM_E_GLORT_M                           (0x12)
+#define CVMX_PKI_PCAM_TERM_E_DSA_M                             (0x13)
+#define CVMX_PKI_PCAM_TERM_E_ETHTYPE0_M                        (0x18)
+#define CVMX_PKI_PCAM_TERM_E_ETHTYPE1_M                        (0x19)
+#define CVMX_PKI_PCAM_TERM_E_ETHTYPE2_M                        (0x1A)
+#define CVMX_PKI_PCAM_TERM_E_ETHTYPE3_M                        (0x1B)
+#define CVMX_PKI_PCAM_TERM_E_MPLS0_M                           (0x1E)
+#define CVMX_PKI_PCAM_TERM_E_L3_FLAGS_M                        (0x23)
+#define CVMX_PKI_PCAM_TERM_E_LD_VNI_M                          (0x28)
+#define CVMX_PKI_PCAM_TERM_E_IL3_FLAGS_M                       (0x2B)
+#define CVMX_PKI_PCAM_TERM_E_L4_PORT_M                         (0x30)
+#define CVMX_PKI_PCAM_TERM_E_LG_CUSTOM_M                       (0x39)
+
+enum cvmx_pki_term {
+	CVMX_PKI_PCAM_TERM_E_NONE                    = CVMX_PKI_PCAM_TERM_E_NONE_M,
+	CVMX_PKI_PCAM_TERM_E_L2_CUSTOM               = CVMX_PKI_PCAM_TERM_E_L2_CUSTOM_M,
+	CVMX_PKI_PCAM_TERM_E_HIGIG                   = CVMX_PKI_PCAM_TERM_E_HIGIG_M,
+	CVMX_PKI_PCAM_TERM_E_DMACH                   = CVMX_PKI_PCAM_TERM_E_DMACH_M,
+	CVMX_PKI_PCAM_TERM_E_DMACL                   = CVMX_PKI_PCAM_TERM_E_DMACL_M,
+	CVMX_PKI_PCAM_TERM_E_GLORT                   = CVMX_PKI_PCAM_TERM_E_GLORT_M,
+	CVMX_PKI_PCAM_TERM_E_DSA                     = CVMX_PKI_PCAM_TERM_E_DSA_M,
+	CVMX_PKI_PCAM_TERM_E_ETHTYPE0                = CVMX_PKI_PCAM_TERM_E_ETHTYPE0_M,
+	CVMX_PKI_PCAM_TERM_E_ETHTYPE1                = CVMX_PKI_PCAM_TERM_E_ETHTYPE1_M,
+	CVMX_PKI_PCAM_TERM_E_ETHTYPE2                = CVMX_PKI_PCAM_TERM_E_ETHTYPE2_M,
+	CVMX_PKI_PCAM_TERM_E_ETHTYPE3                = CVMX_PKI_PCAM_TERM_E_ETHTYPE3_M,
+	CVMX_PKI_PCAM_TERM_E_MPLS0                   = CVMX_PKI_PCAM_TERM_E_MPLS0_M,
+	CVMX_PKI_PCAM_TERM_E_L3_FLAGS                = CVMX_PKI_PCAM_TERM_E_L3_FLAGS_M,
+	CVMX_PKI_PCAM_TERM_E_LD_VNI                  = CVMX_PKI_PCAM_TERM_E_LD_VNI_M,
+	CVMX_PKI_PCAM_TERM_E_IL3_FLAGS               = CVMX_PKI_PCAM_TERM_E_IL3_FLAGS_M,
+	CVMX_PKI_PCAM_TERM_E_L4_PORT                 = CVMX_PKI_PCAM_TERM_E_L4_PORT_M,
+	CVMX_PKI_PCAM_TERM_E_LG_CUSTOM               = CVMX_PKI_PCAM_TERM_E_LG_CUSTOM_M
+};
+
+struct cvmx_pki_pcam_input {
+	uint64_t		style;
+	uint64_t		style_mask;
+	enum cvmx_pki_term	field;
+	uint32_t		field_mask;
+	uint64_t		data;
+	uint64_t		data_mask;
+};
+
+struct cvmx_pki_pcam_action {
+	enum cvmx_pki_parse_mode_chg	parse_mode_chg;
+	enum cvmx_pki_layer_type	layer_type_set;
+	int				style_add;
+	int				parse_flag_set;
+	int				pointer_advance;
+};
+
+struct cvmx_pki_pcam_config {
+	int				in_use;
+	int				entry_num;
+	uint64_t			cluster_mask;
+	struct cvmx_pki_pcam_input	pcam_input;
+	struct cvmx_pki_pcam_action	pcam_action;
+};
+
+
+/**
+ * Status statistics for a port
+ */
+struct cvmx_pki_port_stats {
+	uint32_t dropped_octets;	/**< Inbound octets marked to be dropped by the IPD */
+	uint32_t dropped_packets;	/**< Inbound packets marked to be dropped by the IPD */
+	uint32_t pci_raw_packets;	/**< RAW PCI Packets received by PKI per port */
+	uint32_t octets;		/**< Number of octets processed by PKI */
+	uint32_t packets;		/**< Number of packets processed by PKI */
+	uint32_t multicast_packets;	/**< Number of indentified L2 multicast packets.
+					Does not include broadcast packets.
+					Only includes packets whose parse mode is
+					SKIP_TO_L2 */
+	uint32_t broadcast_packets;	/**< Number of indentified L2 broadcast packets.
+					Does not include multicast packets.
+					Only includes packets whose parse mode is
+					SKIP_TO_L2 */
+	uint32_t len_64_packets;	/**< Number of 64B packets */
+	uint32_t len_65_127_packets;	/**< Number of 65-127B packets */
+	uint32_t len_128_255_packets;	/**< Number of 128-255B packets */
+	uint32_t len_256_511_packets;	/**< Number of 256-511B packets */
+	uint32_t len_512_1023_packets;	/**< Number of 512-1023B packets */
+	uint32_t len_1024_1518_packets;	/**< Number of 1024-1518B packets */
+	uint32_t len_1519_max_packets;	/**< Number of 1519-max packets */
+	uint32_t fcs_align_err_packets;	/**< Number of packets with FCS or Align opcode errors */
+	uint32_t runt_packets;		/**< Number of packets with length < min */
+	uint32_t runt_crc_packets;	/**< Number of packets with length < min and FCS error */
+	uint32_t oversize_packets;	/**< Number of packets with length > max */
+	uint32_t oversize_crc_packets;	/**< Number of packets with length > max and FCS error */
+	uint32_t inb_packets;		/**< Number of packets without GMX/SPX/PCI errors received by PKI */
+	uint64_t inb_octets;		/**< Total number of octets from all packets received by PKI, including CRC */
+	uint16_t inb_errors;		/**< Number of packets with GMX/SPX/PCI errors received by PKI */
+	uint32_t mcast_l2_red_packets;	/**< Number of packets with L2 Multicast DMAC
+					that were dropped due to RED.
+					The HW will consider a packet to be an L2
+					multicast packet when the least-significant bit
+					of the first byte of the DMAC is set and the
+					packet is not an L2 broadcast packet.
+					Only applies when the parse mode for the packets
+					is SKIP-TO-L2 */
+	uint32_t bcast_l2_red_packets;	/**< Number of packets with L2 Broadcast DMAC	that were dropped due to RED.
+					The HW will consider a packet to be an L2
+					broadcast packet when the 48-bit DMAC is all 1's.
+					Only applies when the parse mode for the packets
+					is SKIP-TO-L2 */
+	uint32_t mcast_l3_red_packets;	/**< Number of packets with L3 Multicast Dest Address
+					that were dropped due to RED.
+					The HW considers an IPv4 packet to be multicast
+					when the most-significant nibble of the 32-bit
+					destination address is 0xE (i.e it is a class D
+					address). The HW considers an IPv6 packet to be
+					multicast when the most-significant byte of the
+					128-bit destination address is all 1's.
+					Only applies when the parse mode for the packets
+					is SKIP-TO-L2 and the packet is IP or the parse
+					mode for the packet is SKIP-TO-IP */
+	uint32_t bcast_l3_red_packets;	/**< Number of packets with L3 Broadcast Dest Address
+					that were dropped due to RED.
+					The HW considers an IPv4 packet to be broadcast
+					when all bits are set in the MSB of the
+					destination address. IPv6 does not have the
+					concept of a broadcast packets.
+					Only applies when the parse mode for the packet
+					is SKIP-TO-L2 and the packet is IP or the parse
+					mode for the packet is SKIP-TO-IP */
+};
+
+
+/**
+ * This function writes qpg entry at specified offset in hardware
+ *
+ * @param node		node number
+ * @param index		offset in qpg entry to write to.
+ * @param padd		port address for channel calculation
+ * @param aura		aura number to send packet to
+ * @param group_ok	group number to send packet to if there is no error
+ * @param group_bad	group number to send packet  to if there is error
+ */
+static inline void cvmx_pki_write_qpg_entry(int node, int index, int padd, int aura,
+					      int group_ok, int group_bad)
+{
+	cvmx_pki_qpg_tblx_t qpg_tbl;
+	qpg_tbl.u64 = cvmx_read_csr_node(node, CVMX_PKI_QPG_TBLX(index));
+	qpg_tbl.s.padd = padd;
+	qpg_tbl.s.laura = aura;
+	qpg_tbl.s.grp_ok = group_ok;
+	qpg_tbl.s.grp_bad = group_bad;
+	cvmx_write_csr_node(node, CVMX_PKI_QPG_TBLX(index), qpg_tbl.u64);
+}
+
+/**
+ * This function assignes the clusters to a group, later pkind can be
+ * configured to use that group depending on number of clusters pkind
+ * would use. A given cluster can only be enabled in a single cluster group.
+ * Number of clusters assign to that group determines how many engine can work
+ * in parallel to process the packet. Eack cluster can process x MPPS.
+ *
+ * @param cluster_group Group to attach clusters to
+ * @param cluster_mask  It is the mask of clusters which needs to be assigned to group.
+ * to that group
+ *
+ */
+static inline int cvmx_pki_attach_cluster_to_group(int node, uint64_t cluster_group,
+		 uint64_t cluster_mask)
+{
+	cvmx_pki_icgx_cfg_t pki_cl_grp;
+
+	if (cluster_group >= CVMX_PKI_NUM_CLUSTER_GROUP) {
+		cvmx_dprintf("ERROR: config cluster group %d", (int)cluster_group);
+		return -1;
+	}
+	pki_cl_grp.u64 = cvmx_read_csr_node(node, CVMX_PKI_ICGX_CFG(cluster_group));
+	pki_cl_grp.s.clusters = cluster_mask;
+	cvmx_write_csr_node(node, CVMX_PKI_ICGX_CFG(cluster_group), pki_cl_grp.u64);
+	return 0;
+}
+
+static inline void cvmx_pki_write_global_parse(int node, struct cvmx_pki_global_parse gbl_pen)
+{
+	cvmx_pki_gbl_pen_t gbl_pen_reg;
+	gbl_pen_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_GBL_PEN);
+	gbl_pen_reg.s.virt_pen = gbl_pen.virt_pen;
+	gbl_pen_reg.s.clg_pen = gbl_pen.clg_pen;
+	gbl_pen_reg.s.cl2_pen = gbl_pen.cl2_pen;
+	gbl_pen_reg.s.l4_pen = gbl_pen.l4_pen;
+	gbl_pen_reg.s.il3_pen = gbl_pen.il3_pen;
+	gbl_pen_reg.s.l3_pen = gbl_pen.l3_pen;
+	gbl_pen_reg.s.mpls_pen = gbl_pen.mpls_pen;
+	gbl_pen_reg.s.fulc_pen = gbl_pen.fulc_pen;
+	gbl_pen_reg.s.dsa_pen = gbl_pen.dsa_pen;
+	gbl_pen_reg.s.hg_pen = gbl_pen.hg_pen;
+	cvmx_write_csr_node(node, CVMX_PKI_GBL_PEN, gbl_pen_reg.u64);
+}
+
+static inline void cvmx_pki_write_tag_secret(int node, struct cvmx_pki_tag_sec tag_secret)
+{
+	cvmx_pki_tag_secret_t tag_secret_reg;
+	tag_secret_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_TAG_SECRET);
+	tag_secret_reg.s.dst6 = tag_secret.dst6;
+	tag_secret_reg.s.src6 = tag_secret.src6;
+	tag_secret_reg.s.dst = tag_secret.dst;
+	tag_secret_reg.s.src = tag_secret.src;
+	cvmx_write_csr_node(node, CVMX_PKI_TAG_SECRET, tag_secret_reg.u64);
+}
+
+/**
+ * This function writes max and min frame lengths to hardware which can be used
+ * to check the size of frame arrived.There are 2 possible combination which are
+ * indicated by id field.
+ * @param node		node number.
+ * @param id		choose which frame len register to write to
+ * @param len_chk	struct containing Byte count for max-sized/min-sized frame check.
+ *
+ */
+static inline void cvmx_pki_write_frame_len(int node, int id,
+					   struct cvmx_pki_frame_len len_chk)
+{
+	cvmx_pki_frm_len_chkx_t frm_len_chk;
+	frm_len_chk.u64 = cvmx_read_csr_node(node, CVMX_PKI_FRM_LEN_CHKX(id));
+	frm_len_chk.s.maxlen = len_chk.maxlen;
+	frm_len_chk.s.minlen = len_chk.minlen;
+	cvmx_write_csr_node(node, CVMX_PKI_FRM_LEN_CHKX(id), frm_len_chk.u64);
+}
+
+static inline void cvmx_pki_write_ltype_map(int node, enum cvmx_pki_layer_type layer,
+					    enum cvmx_pki_beltype backend)
+{
+	cvmx_pki_ltypex_map_t ltype_map;
+	ltype_map.u64 = cvmx_read_csr_node(node, CVMX_PKI_LTYPEX_MAP(layer));
+	ltype_map.s.beltype = backend;
+	cvmx_write_csr_node(node, CVMX_PKI_LTYPEX_MAP(layer), ltype_map.u64);
+}
+
+/**
+ * This function enables the cluster group to start parsing
+ *
+ * @param node          node number
+ * @param cl_grp        cluster group to enable parsing
+ *
+ */
+static inline int cvmx_pki_parse_enable(int node, int cl_grp)
+{
+	cvmx_pki_icgx_cfg_t pki_cl_grp;
+
+	if (cl_grp >= CVMX_PKI_NUM_CLUSTER_GROUP) {
+		cvmx_dprintf("ERROR: pki parse en group %d", (int)cl_grp);
+		return -1;
+	}
+	pki_cl_grp.u64 = cvmx_read_csr_node(node, CVMX_PKI_ICGX_CFG(cl_grp));
+	pki_cl_grp.s.pena = 1;
+	cvmx_write_csr_node(node, CVMX_PKI_ICGX_CFG(cl_grp), pki_cl_grp.u64);
+	return 0;
+}
+
+/**
+ * This function enables the PKI to send bpid level backpressure
+ * to CN78XX inputs.
+ * @param node       node number
+ */
+static inline void cvmx_pki_enable_backpressure(int node)
+{
+	cvmx_pki_buf_ctl_t pki_buf_ctl;
+
+	pki_buf_ctl.u64 = cvmx_read_csr_node(node, CVMX_PKI_BUF_CTL);
+	pki_buf_ctl.s.pbp_en = 1;
+	cvmx_write_csr_node(node, CVMX_PKI_BUF_CTL, pki_buf_ctl.u64);
+}
+
+#define READCORRECT(cnt, node, value, addr)          \
+{       cnt = 0;    \
+        while(value >= (1ull << 48) && cnt++ < 20) \
+                value = cvmx_read_csr_node(node, addr); \
+        if (cnt >= 20)  \
+                cvmx_dprintf("count stuck for 0x%llx\n", addr);    }
+
+
+/**
+ * Get the statistics counters for a port.
+ *
+ * @param node	   node number
+ * @param port_num Port number (ipd_port) to get statistics for.
+ *		   Make sure PKI_STATS_CTL:mode is set to 0 for
+ *		   collecting per port/pkind stats.
+ *
+ */
+static inline void cvmx_pki_clear_port_stats(int node, uint64_t port)
+{
+	int interface = cvmx_helper_get_interface_num(port);
+	int index = cvmx_helper_get_interface_index_num(port);
+	int pknd = cvmx_helper_get_pknd(interface, index);
+
+	cvmx_pki_statx_stat0_t stat0;
+	cvmx_pki_statx_stat1_t stat1;
+	cvmx_pki_statx_stat2_t stat2;
+	cvmx_pki_statx_stat3_t stat3;
+	cvmx_pki_statx_stat4_t stat4;
+	cvmx_pki_statx_stat5_t stat5;
+	cvmx_pki_statx_stat6_t stat6;
+	cvmx_pki_statx_stat7_t stat7;
+	cvmx_pki_statx_stat8_t stat8;
+	cvmx_pki_statx_stat9_t stat9;
+	cvmx_pki_statx_stat10_t stat10;
+	cvmx_pki_statx_stat11_t stat11;
+	cvmx_pki_statx_stat14_t stat14;
+	cvmx_pki_statx_stat15_t stat15;
+	cvmx_pki_statx_stat16_t stat16;
+	cvmx_pki_statx_stat17_t stat17;
+	cvmx_pki_statx_hist0_t hist0;
+	cvmx_pki_statx_hist1_t hist1;
+	cvmx_pki_statx_hist2_t hist2;
+	cvmx_pki_statx_hist3_t hist3;
+	cvmx_pki_statx_hist4_t hist4;
+	cvmx_pki_statx_hist5_t hist5;
+	cvmx_pki_statx_hist6_t hist6;
+	cvmx_pki_pkndx_inb_stat0_t pki_pknd_inb_stat0;
+	cvmx_pki_pkndx_inb_stat1_t pki_pknd_inb_stat1;
+	cvmx_pki_pkndx_inb_stat2_t pki_pknd_inb_stat2;
+
+	stat0.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT0(pknd));
+	stat1.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT1(pknd));
+	stat2.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT2(pknd));
+	stat3.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT3(pknd));
+	stat4.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT4(pknd));
+	stat5.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT5(pknd));
+	stat6.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT6(pknd));
+	stat7.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT7(pknd));
+	stat8.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT8(pknd));
+	stat9.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT9(pknd));
+	stat10.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT10(pknd));
+	stat11.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT11(pknd));
+	stat14.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT14(pknd));
+	stat15.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT15(pknd));
+	stat16.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT16(pknd));
+	stat17.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT17(pknd));
+	hist0.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST0(pknd));
+	hist1.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST1(pknd));
+	hist2.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST2(pknd));
+	hist3.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST3(pknd));
+	hist4.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST4(pknd));
+	hist5.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST5(pknd));
+	hist6.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST6(pknd));
+	pki_pknd_inb_stat0.u64 = cvmx_read_csr_node(node, CVMX_PKI_PKNDX_INB_STAT0(pknd));
+	pki_pknd_inb_stat1.u64 = cvmx_read_csr_node(node, CVMX_PKI_PKNDX_INB_STAT1(pknd));
+	pki_pknd_inb_stat2.u64 = cvmx_read_csr_node(node, CVMX_PKI_PKNDX_INB_STAT2(pknd));
+
+	stat4.s.drp_octs = 0;
+	stat3.s.drp_pkts = 0;
+	stat1.s.octs = 0;
+	stat2.s.raw = 0;
+	stat0.s.pkts = 0;
+	stat6.s.mcast = 0;
+	stat5.s.bcast = 0;
+	hist0.s.h1to63 = 0;
+	hist1.s.h64to127 = 0;
+	hist2.s.h128to255 = 0;
+	hist3.s.h256to511 = 0;
+	hist4.s.h512to1023 = 0;
+	hist5.s.h1024to1518 = 0;
+	hist6.s.h1519 = 0;
+	stat7.s.fcs = 0;
+	stat9.s.undersz = 0;
+	stat8.s.frag = 0;
+	stat11.s.oversz = 0;
+	stat10.s.jabber = 0;
+	stat15.s.drp_mcast = 0;
+	stat14.s.drp_bcast = 0;
+	stat17.s.drp_mcast = 0;
+	stat16.s.drp_bcast = 0;
+	pki_pknd_inb_stat0.s.pkts = 0;
+	pki_pknd_inb_stat1.s.octs = 0;
+	pki_pknd_inb_stat2.s.errs = 0;
+
+        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT0(pknd), stat0.u64);
+        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT1(pknd), stat1.u64);
+        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT2(pknd), stat2.u64);
+        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT3(pknd), stat3.u64);
+        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT4(pknd), stat4.u64);
+        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT5(pknd), stat5.u64);
+        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT6(pknd), stat6.u64);
+        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT7(pknd), stat7.u64);
+        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT8(pknd), stat8.u64);
+        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT9(pknd), stat9.u64);
+        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT10(pknd), stat10.u64);
+        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT11(pknd), stat11.u64);
+        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT14(pknd), stat14.u64);
+        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT15(pknd), stat15.u64);
+        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT16(pknd), stat16.u64);
+        cvmx_write_csr_node(node, CVMX_PKI_STATX_STAT17(pknd), stat17.u64);
+        cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST0(pknd), hist0.u64);
+        cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST1(pknd), hist1.u64);
+        cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST2(pknd), hist2.u64);
+        cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST3(pknd), hist3.u64);
+        cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST4(pknd), hist4.u64);
+        cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST5(pknd), hist5.u64);
+        cvmx_write_csr_node(node, CVMX_PKI_STATX_HIST6(pknd), hist6.u64);
+        cvmx_write_csr_node(node, CVMX_PKI_PKNDX_INB_STAT0(pknd), pki_pknd_inb_stat0.u64);
+        cvmx_write_csr_node(node, CVMX_PKI_PKNDX_INB_STAT1(pknd), pki_pknd_inb_stat1.u64);
+        cvmx_write_csr_node(node, CVMX_PKI_PKNDX_INB_STAT2(pknd), pki_pknd_inb_stat2.u64);
+
+}
+
+
+/**
+ * Get the status counters for index from PKI.
+ *
+ * @param node	   node number
+ * @param index    pkind number (if PKI_STATS_CTL:mode=0) or
+ *		   style(flow) number (if PKI_STATS_CTL:mode=1)
+ * @param status   Where to put the results.
+ */
+static inline void cvmx_pki_get_stats(int node, int index, struct cvmx_pki_port_stats *status)
+{
+	cvmx_pki_statx_stat0_t stat0;
+	cvmx_pki_statx_stat1_t stat1;
+	cvmx_pki_statx_stat2_t stat2;
+	cvmx_pki_statx_stat3_t stat3;
+	cvmx_pki_statx_stat4_t stat4;
+	cvmx_pki_statx_stat5_t stat5;
+	cvmx_pki_statx_stat6_t stat6;
+	cvmx_pki_statx_stat7_t stat7;
+	cvmx_pki_statx_stat8_t stat8;
+	cvmx_pki_statx_stat9_t stat9;
+	cvmx_pki_statx_stat10_t stat10;
+	cvmx_pki_statx_stat11_t stat11;
+	cvmx_pki_statx_stat14_t stat14;
+	cvmx_pki_statx_stat15_t stat15;
+	cvmx_pki_statx_stat16_t stat16;
+	cvmx_pki_statx_stat17_t stat17;
+	cvmx_pki_statx_hist0_t hist0;
+	cvmx_pki_statx_hist1_t hist1;
+	cvmx_pki_statx_hist2_t hist2;
+	cvmx_pki_statx_hist3_t hist3;
+	cvmx_pki_statx_hist4_t hist4;
+	cvmx_pki_statx_hist5_t hist5;
+	cvmx_pki_statx_hist6_t hist6;
+	cvmx_pki_pkndx_inb_stat0_t pki_pknd_inb_stat0;
+	cvmx_pki_pkndx_inb_stat1_t pki_pknd_inb_stat1;
+	cvmx_pki_pkndx_inb_stat2_t pki_pknd_inb_stat2;
+        int cnt;
+
+	stat0.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT0(index));
+        READCORRECT(cnt, node, stat0.u64, CVMX_PKI_STATX_STAT0(index));
+
+	stat1.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT1(index));
+        READCORRECT(cnt, node, stat1.u64, CVMX_PKI_STATX_STAT1(index));
+
+	stat2.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT2(index));
+        READCORRECT(cnt, node, stat2.u64, CVMX_PKI_STATX_STAT2(index));
+
+	stat3.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT3(index));
+        READCORRECT(cnt, node, stat3.u64, CVMX_PKI_STATX_STAT3(index));
+
+	stat4.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT4(index));
+        READCORRECT(cnt, node, stat4.u64, CVMX_PKI_STATX_STAT4(index));
+
+	stat5.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT5(index));
+        READCORRECT(cnt, node, stat5.u64, CVMX_PKI_STATX_STAT5(index));
+
+	stat6.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT6(index));
+        READCORRECT(cnt, node, stat6.u64, CVMX_PKI_STATX_STAT6(index));
+
+	stat7.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT7(index));
+        READCORRECT(cnt, node, stat7.u64, CVMX_PKI_STATX_STAT7(index));
+
+	stat8.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT8(index));
+        READCORRECT(cnt, node, stat8.u64, CVMX_PKI_STATX_STAT8(index));
+
+	stat9.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT9(index));
+        READCORRECT(cnt, node, stat9.u64, CVMX_PKI_STATX_STAT9(index));
+
+	stat10.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT10(index));
+        READCORRECT(cnt, node, stat10.u64, CVMX_PKI_STATX_STAT10(index));
+
+	stat11.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT11(index));
+        READCORRECT(cnt, node, stat11.u64, CVMX_PKI_STATX_STAT11(index));
+
+	stat14.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT14(index));
+        READCORRECT(cnt, node, stat14.u64, CVMX_PKI_STATX_STAT14(index));
+
+	stat15.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT15(index));
+        READCORRECT(cnt, node, stat15.u64, CVMX_PKI_STATX_STAT15(index));
+
+	stat16.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT16(index));
+        READCORRECT(cnt, node, stat16.u64, CVMX_PKI_STATX_STAT16(index));
+
+	stat17.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_STAT17(index));
+        READCORRECT(cnt, node, stat17.u64, CVMX_PKI_STATX_STAT17(index));
+
+	hist0.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST0(index));
+        READCORRECT(cnt, node, hist0.u64, CVMX_PKI_STATX_HIST0(index));
+
+	hist1.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST1(index));
+        READCORRECT(cnt, node, hist1.u64, CVMX_PKI_STATX_HIST1(index));
+
+	hist2.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST2(index));
+        READCORRECT(cnt, node, hist2.u64, CVMX_PKI_STATX_HIST2(index));
+
+	hist3.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST3(index));
+        READCORRECT(cnt, node, hist3.u64, CVMX_PKI_STATX_HIST3(index));
+
+	hist4.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST4(index));
+        READCORRECT(cnt, node, hist4.u64, CVMX_PKI_STATX_HIST4(index));
+
+	hist5.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST5(index));
+        READCORRECT(cnt, node, hist5.u64, CVMX_PKI_STATX_HIST5(index));
+
+	hist6.u64 = cvmx_read_csr_node(node, CVMX_PKI_STATX_HIST6(index));
+        READCORRECT(cnt, node, hist6.u64, CVMX_PKI_STATX_HIST6(index));
+
+	pki_pknd_inb_stat0.u64 = cvmx_read_csr_node(node, CVMX_PKI_PKNDX_INB_STAT0(index));
+	pki_pknd_inb_stat1.u64 = cvmx_read_csr_node(node, CVMX_PKI_PKNDX_INB_STAT1(index));
+	pki_pknd_inb_stat2.u64 = cvmx_read_csr_node(node, CVMX_PKI_PKNDX_INB_STAT2(index));
+
+	status->dropped_octets = stat4.s.drp_octs;
+	status->dropped_packets = stat3.s.drp_pkts;
+	status->octets = stat1.s.octs;
+	status->pci_raw_packets = stat2.s.raw;
+	status->packets = stat0.s.pkts;
+	status->multicast_packets = stat6.s.mcast;
+	status->broadcast_packets = stat5.s.bcast;
+	status->len_64_packets = hist0.s.h1to63;
+	status->len_65_127_packets = hist1.s.h64to127;
+	status->len_128_255_packets = hist2.s.h128to255;
+	status->len_256_511_packets = hist3.s.h256to511;
+	status->len_512_1023_packets = hist4.s.h512to1023;
+	status->len_1024_1518_packets = hist5.s.h1024to1518;
+	status->len_1519_max_packets = hist6.s.h1519;
+	status->fcs_align_err_packets = stat7.s.fcs;
+	status->runt_packets = stat9.s.undersz;
+	status->runt_crc_packets = stat8.s.frag;
+	status->oversize_packets = stat11.s.oversz;
+	status->oversize_crc_packets = stat10.s.jabber;
+	status->mcast_l2_red_packets = stat15.s.drp_mcast;
+	status->bcast_l2_red_packets = stat14.s.drp_bcast;
+	status->mcast_l3_red_packets = stat17.s.drp_mcast;
+	status->bcast_l3_red_packets = stat16.s.drp_bcast;
+	status->inb_packets = pki_pknd_inb_stat0.s.pkts;
+	status->inb_octets = pki_pknd_inb_stat1.s.octs;
+	status->inb_errors = pki_pknd_inb_stat2.s.errs;
+}
+
+/**
+ * Get the statistics counters for a port.
+ *
+ * @param node	   node number
+ * @param port_num Port number (ipd_port) to get statistics for.
+ *		   Make sure PKI_STATS_CTL:mode is set to 0 for
+ *		   collecting per port/pkind stats.
+ * @param status   Where to put the results.
+ */
+static inline void cvmx_pki_get_port_stats(int node, uint64_t port, struct cvmx_pki_port_stats *status)
+{
+	int interface = cvmx_helper_get_interface_num(port);
+	int index = cvmx_helper_get_interface_index_num(port);
+	int pknd = cvmx_helper_get_pknd(interface, index);
+
+	cvmx_pki_get_stats(node, pknd, status);
+}
+
+/**
+ * Get the statistics counters for a flow represented by style in PKI.
+ *
+ * @param node	   node number
+ * @param style	   style number to get statistics for.
+ *		   Make sure PKI_STATS_CTL:mode is set to 1 for
+ *		   collecting per style/flow stats.
+ * @param status   Where to put the results.
+ */
+static inline void cvmx_pki_get_flow_stats(int node, uint64_t style_num, struct cvmx_pki_port_stats *status)
+{
+	cvmx_pki_get_stats(node, style_num, status);
+}
+
+/**
+ * This function enables pki
+ * @param node	 node to enable pki in.
+ */
+void cvmx_pki_enable(int node);
+
+/**
+ * This function disables pki
+ * @param node	node to disable pki in.
+ */
+void cvmx_pki_disable(int node);
+
+/**
+ * This function writes per pkind parameters in hardware which defines how
+  the incoming packet is processed.
+ * @param node		      node number.
+ * @param pkind               PKI supports a large number of incoming interfaces
+ *                            and packets arriving on different interfaces or channels
+ *                            may want to be processed differently. PKI uses the pkind to
+ *                            determine how the incoming packet is processed.
+ * @param pkind_cfg	      struct conatining pkind configuration need to be written to hw
+ */
+int cvmx_pki_set_pkind_config(int node, int pkind, struct cvmx_pki_pkind_config *pkind_cfg);
+
+int cvmx_pki_get_pkind_config(int node, int pkind, struct cvmx_pki_pkind_config *pkind_cfg);
+
+/**
+ * This function writes/configures parameters associated with tag configuration in hardware.
+ * @param node	              node number.
+ * @param style		      style to configure tag for
+ * @param cluster_mask	      Mask of clusters to configure the style for.
+ * @param tag_cfg	      pointer to taf configuration struct.
+ */
+void cvmx_pki_write_tag_config(int node, int style, uint64_t cluster_mask,
+			       struct cvmx_pki_style_tag_cfg *tag_cfg);
+
+/**
+ * This function writes/configures parameters associated with style in hardware.
+ * @param node	              node to which style belong.
+ * @param style		      style to configure.
+ * @param cluster_mask	      Mask of clusters to configure the style for.
+ * @param style_cfg	      parameters to configure for style passed in struct.
+ */
+void cvmx_pki_set_style_config(int node, uint64_t style, uint64_t cluster_mask,
+			    struct cvmx_pki_style_config *style_cfg);
+
+/**
+ * This function writes pcam entry at given offset in pcam table in hardware
+ *
+ * @param node			node number.
+ * @param index			offset in pcam table.
+ * @param cluster_mask		Mask of clusters in which to write pcam entry.
+ * @param input			input keys to pcam match passed as struct.
+ * @param action		pcam match action passed as struct
+ *
+ */
+int cvmx_pki_pcam_write_entry(int node, int index, uint64_t cluster_mask,
+				struct cvmx_pki_pcam_input input, struct cvmx_pki_pcam_action action);
+
+int cvmx_pki_setup_clusters(int node);
+
+/**
+ * Configures the channel which will receive backpressure
+ * from the specified bpid.
+ * Each channel listens for backpressure on a specific bpid.
+ * Each bpid can backpressure multiple channels.
+ * @param node    node number
+ * @param bpid    bpid from which, channel will receive backpressure.
+ * @param channel channel numner to receive backpressue.
+ */
+int cvmx_pki_write_channel_bpid(int node, int channel, int bpid);
+
+/**
+ * Configures the bpid on which, specified channel will
+ * assert backpressure.
+ * Each bpid receives backpressure from auras.
+ * Multiple auras can backpressure single bpid.
+ * @param node   node number
+ * @param aura   number which will assert backpressure on that bpid.
+ * @param bpid   to assert backpressure on.
+ */
+int cvmx_pki_write_aura_bpid(int node, int aura, int bpid);
+
+/**
+ * Enables/Disabled QoS (RED Drop, Tail Drop & backpressure) for the
+ * PKI aura.
+ * @param node      node number
+ * @param aura      to enable/disable QoS on.
+ * @param ena_red   Enable/Disable RED drop between pass and drop level
+ *                  1-enable 0-disable
+ * @param ena_drop  Enable/disable tail drop when max drop level exceeds
+ *                  1-enable 0-disable
+ * @param ena_red   Enable/Disable asserting backpressure on bpid when
+ *                  max DROP level exceeds.
+ *                  1-enable 0-disable
+ */
+int cvmx_pki_enable_aura_qos(int node, int aura, bool ena_red,
+			      bool ena_drop, bool ena_bp);
+
+/**
+ * This function get the buffer size of the given pool number
+ * @param node  node number
+ * @param pool  fpa pool number
+ * @return	buffer size SUCCESS
+ *		-1 if pool number is not found in pool list
+ */
+int cvmx_pki_get_pool_buffer_size(int node, int pool);
+
+/**
+ * This function get the buffer size of the given aura number
+ * @param node  node number
+ * @param pool  fpa aura number
+ * @return	buffer size SUCCESS
+ *		-1 if aura number is not found in aura list
+ */
+int cvmx_pki_get_aura_buffer_size(int node, int aura);
+
+int cvmx_pki_get_mbuff_size(int node, int base_offset);
+
+
+/**
+ * This function sets the wqe buffer mode. First packet data buffer can reside
+ * either in same buffer as wqe OR it can go in separate buffer. If used the later mode,
+ * make sure software allocate enough buffers to now have wqe separate from packet data.
+ * @param node	              node number.
+ * @param style		      style to configure.
+ * @param pkt_outside_wqe.	0 = The packet link pointer will be at word [FIRST_SKIP]
+ *				    immediately followed by packet data, in the same buffer
+ *				    as the work queue entry.
+ *				1 = The packet link pointer will be at word [FIRST_SKIP] in a new
+ *				    buffer separate from the work queue entry. Words following the
+ *				    WQE in the same cache line will be zeroed, other lines in the
+ *				    buffer will not be modified and will retain stale data (from the
+ *				    buffers previous use). This setting may decrease the peak PKI
+ *				    performance by up to half on small packets.
+ */
+void cvmx_pki_set_wqe_mode(int node, uint64_t style, bool pkt_outside_wqe);
+
+/**
+ * Enables/Disables l2 length error check and max & min frame length checks
+ * @param node		node number
+ * @param pknd		pkind to disable error for.
+ * @param l2len_err	L2 length error check enable.
+ * @param maxframe_err	Max frame error check enable.
+ * @param minframe_err	Min frame error check enable.
+ *			1 -- Enabel err checks
+ *			0 -- Disable error checks
+ */
+void cvmx_pki_endis_l2_errs(int node, int pknd, bool l2len_err,
+			    bool maxframe_err, bool minframe_err);
+
+/**
+ * Enables/Disables fcs check and fcs stripping on the pkind.
+ * @param node		node number
+ * @param pknd		pkind to apply settings on.
+ * @param fcs_chk	enable/disable fcs check.
+ *			1 -- enable fcs error check.
+ *			0 -- disable fcs error check.
+ * @param fcs_strip	Strip L2 FCS bytes from packet, decrease WQE[LEN] by 4 bytes
+ *			1 -- strip L2 FCS.
+ *			0 -- Do not strip L2 FCS.
+ */
+void cvmx_pki_endis_fcs_check(int node, int pknd, bool fcs_chk, bool fcs_strip);
+/**
+ * Modifies maximum frame length to check.
+ * It modifies the global frame length set used by this port, any other
+ * port using the same set will get affected too.
+ * @param node   node number
+ * @param port	 ipd port for which to modify max len.
+ * @param max_size	maximum frame length
+ */
+void cvmx_pki_set_max_frm_len(int node, int port, uint32_t max_size);
+void cvmx_pki_get_style_config(int node, int style, uint64_t cl_mask,
+			       struct cvmx_pki_style_config *style_cfg);
+void cvmx_pki_config_port(int ipd_port, struct cvmx_pki_port_config *port_cfg);
+void cvmx_pki_get_port_config(int ipd_port, struct cvmx_pki_port_config *port_cfg);
+void cvmx_pki_reset(int node);
+int cvmx_pki_get_pkind_style(int node, int pkind);
+void __cvmx_pki_free_ptr(int node);
+void cvmx_pki_show_qpg_entries(int node, uint16_t num_entry);
+void cvmx_pki_show_port(int node, int interface, int index);
+void cvmx_pki_write_global_cfg(int node, struct cvmx_pki_global_config *gbl_cfg);
+void cvmx_pki_read_global_cfg(int node, struct cvmx_pki_global_config *gbl_cfg);
+
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+}
+/* *INDENT-ON* */
+#endif
+#endif
diff --git a/arch/mips/include/asm/octeon/cvmx-pko-defs.h b/arch/mips/include/asm/octeon/cvmx-pko-defs.h
index 87c3b97..41d1e83 100644
--- a/arch/mips/include/asm/octeon/cvmx-pko-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-pko-defs.h
@@ -1,1639 +1,13276 @@
 /***********************license start***************
- * Author: Cavium Networks
+ * Copyright (c) 2003-2014  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
  *
- * Contact: support@caviumnetworks.com
- * This file is part of the OCTEON SDK
  *
- * Copyright (c) 2003-2012 Cavium Networks
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
  *
- * This file is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, Version 2, as
- * published by the Free Software Foundation.
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
  *
- * This file is distributed in the hope that it will be useful, but
- * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
- * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
- * NONINFRINGEMENT.  See the GNU General Public License for more
- * details.
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+
+/**
+ * cvmx-pko-defs.h
+ *
+ * Configuration and status register (CSR) type definitions for
+ * Octeon pko.
+ *
+ * This file is auto generated. Do not edit.
+ *
+ * <hr>$Revision$<hr>
+ *
+ */
+#ifndef __CVMX_PKO_DEFS_H__
+#define __CVMX_PKO_DEFS_H__
+
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_CHANNEL_LEVEL CVMX_PKO_CHANNEL_LEVEL_FUNC()
+static inline uint64_t CVMX_PKO_CHANNEL_LEVEL_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_CHANNEL_LEVEL not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400000800F0ull);
+}
+#else
+#define CVMX_PKO_CHANNEL_LEVEL (CVMX_ADD_IO_SEG(0x00015400000800F0ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_DPFI_ENA CVMX_PKO_DPFI_ENA_FUNC()
+static inline uint64_t CVMX_PKO_DPFI_ENA_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_DPFI_ENA not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000C00018ull);
+}
+#else
+#define CVMX_PKO_DPFI_ENA (CVMX_ADD_IO_SEG(0x0001540000C00018ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_DPFI_FLUSH CVMX_PKO_DPFI_FLUSH_FUNC()
+static inline uint64_t CVMX_PKO_DPFI_FLUSH_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_DPFI_FLUSH not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000C00008ull);
+}
+#else
+#define CVMX_PKO_DPFI_FLUSH (CVMX_ADD_IO_SEG(0x0001540000C00008ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_DPFI_FPA_AURA CVMX_PKO_DPFI_FPA_AURA_FUNC()
+static inline uint64_t CVMX_PKO_DPFI_FPA_AURA_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_DPFI_FPA_AURA not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000C00010ull);
+}
+#else
+#define CVMX_PKO_DPFI_FPA_AURA (CVMX_ADD_IO_SEG(0x0001540000C00010ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_DPFI_STATUS CVMX_PKO_DPFI_STATUS_FUNC()
+static inline uint64_t CVMX_PKO_DPFI_STATUS_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_DPFI_STATUS not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000C00000ull);
+}
+#else
+#define CVMX_PKO_DPFI_STATUS (CVMX_ADD_IO_SEG(0x0001540000C00000ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_DQX_BYTES(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_DQX_BYTES(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00015400000000D8ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_DQX_BYTES(offset) (CVMX_ADD_IO_SEG(0x00015400000000D8ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_DQX_CIR(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_DQX_CIR(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000280018ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_DQX_CIR(offset) (CVMX_ADD_IO_SEG(0x0001540000280018ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_DQX_DROPPED_BYTES(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_DQX_DROPPED_BYTES(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00015400000000C8ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_DQX_DROPPED_BYTES(offset) (CVMX_ADD_IO_SEG(0x00015400000000C8ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_DQX_DROPPED_PACKETS(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_DQX_DROPPED_PACKETS(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00015400000000C0ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_DQX_DROPPED_PACKETS(offset) (CVMX_ADD_IO_SEG(0x00015400000000C0ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_DQX_FIFO(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_DQX_FIFO(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000300078ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_DQX_FIFO(offset) (CVMX_ADD_IO_SEG(0x0001540000300078ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_DQX_PACKETS(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_DQX_PACKETS(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00015400000000D0ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_DQX_PACKETS(offset) (CVMX_ADD_IO_SEG(0x00015400000000D0ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_DQX_PICK(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_DQX_PICK(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000300070ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_DQX_PICK(offset) (CVMX_ADD_IO_SEG(0x0001540000300070ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_DQX_PIR(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_DQX_PIR(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000280020ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_DQX_PIR(offset) (CVMX_ADD_IO_SEG(0x0001540000280020ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_DQX_POINTERS(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_DQX_POINTERS(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000280078ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_DQX_POINTERS(offset) (CVMX_ADD_IO_SEG(0x0001540000280078ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_DQX_SCHEDULE(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_DQX_SCHEDULE(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000280008ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_DQX_SCHEDULE(offset) (CVMX_ADD_IO_SEG(0x0001540000280008ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_DQX_SCHED_STATE(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_DQX_SCHED_STATE(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000280028ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_DQX_SCHED_STATE(offset) (CVMX_ADD_IO_SEG(0x0001540000280028ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_DQX_SHAPE(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_DQX_SHAPE(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000280010ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_DQX_SHAPE(offset) (CVMX_ADD_IO_SEG(0x0001540000280010ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_DQX_SHAPE_STATE(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_DQX_SHAPE_STATE(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000280030ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_DQX_SHAPE_STATE(offset) (CVMX_ADD_IO_SEG(0x0001540000280030ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_DQX_SW_XOFF(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_DQX_SW_XOFF(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00015400002800E0ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_DQX_SW_XOFF(offset) (CVMX_ADD_IO_SEG(0x00015400002800E0ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_DQX_TOPOLOGY(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_DQX_TOPOLOGY(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000300000ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_DQX_TOPOLOGY(offset) (CVMX_ADD_IO_SEG(0x0001540000300000ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_DQX_WM_CNT(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_DQX_WM_CNT(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000000050ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_DQX_WM_CNT(offset) (CVMX_ADD_IO_SEG(0x0001540000000050ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_DQX_WM_CTL(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_DQX_WM_CTL(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000000040ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_DQX_WM_CTL(offset) (CVMX_ADD_IO_SEG(0x0001540000000040ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_DQX_WM_CTL_W1C(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_DQX_WM_CTL_W1C(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000000048ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_DQX_WM_CTL_W1C(offset) (CVMX_ADD_IO_SEG(0x0001540000000048ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_DQ_CSR_BUS_DEBUG CVMX_PKO_DQ_CSR_BUS_DEBUG_FUNC()
+static inline uint64_t CVMX_PKO_DQ_CSR_BUS_DEBUG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_DQ_CSR_BUS_DEBUG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400003001F8ull);
+}
+#else
+#define CVMX_PKO_DQ_CSR_BUS_DEBUG (CVMX_ADD_IO_SEG(0x00015400003001F8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_DQ_DEBUG CVMX_PKO_DQ_DEBUG_FUNC()
+static inline uint64_t CVMX_PKO_DQ_DEBUG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_DQ_DEBUG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000300128ull);
+}
+#else
+#define CVMX_PKO_DQ_DEBUG (CVMX_ADD_IO_SEG(0x0001540000300128ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_DRAIN_IRQ CVMX_PKO_DRAIN_IRQ_FUNC()
+static inline uint64_t CVMX_PKO_DRAIN_IRQ_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_DRAIN_IRQ not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000000140ull);
+}
+#else
+#define CVMX_PKO_DRAIN_IRQ (CVMX_ADD_IO_SEG(0x0001540000000140ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_ENABLE CVMX_PKO_ENABLE_FUNC()
+static inline uint64_t CVMX_PKO_ENABLE_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_ENABLE not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000D00008ull);
+}
+#else
+#define CVMX_PKO_ENABLE (CVMX_ADD_IO_SEG(0x0001540000D00008ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_FORMATX_CTL(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 127)))))
+		cvmx_warn("CVMX_PKO_FORMATX_CTL(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000900800ull) + ((offset) & 127) * 8;
+}
+#else
+#define CVMX_PKO_FORMATX_CTL(offset) (CVMX_ADD_IO_SEG(0x0001540000900800ull) + ((offset) & 127) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_L1_SQA_DEBUG CVMX_PKO_L1_SQA_DEBUG_FUNC()
+static inline uint64_t CVMX_PKO_L1_SQA_DEBUG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_L1_SQA_DEBUG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000080128ull);
+}
+#else
+#define CVMX_PKO_L1_SQA_DEBUG (CVMX_ADD_IO_SEG(0x0001540000080128ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_L1_SQB_DEBUG CVMX_PKO_L1_SQB_DEBUG_FUNC()
+static inline uint64_t CVMX_PKO_L1_SQB_DEBUG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_L1_SQB_DEBUG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000080130ull);
+}
+#else
+#define CVMX_PKO_L1_SQB_DEBUG (CVMX_ADD_IO_SEG(0x0001540000080130ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L1_SQX_CIR(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
+		cvmx_warn("CVMX_PKO_L1_SQX_CIR(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000000018ull) + ((offset) & 31) * 512;
+}
+#else
+#define CVMX_PKO_L1_SQX_CIR(offset) (CVMX_ADD_IO_SEG(0x0001540000000018ull) + ((offset) & 31) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L1_SQX_DROPPED_BYTES(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
+		cvmx_warn("CVMX_PKO_L1_SQX_DROPPED_BYTES(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000000088ull) + ((offset) & 31) * 512;
+}
+#else
+#define CVMX_PKO_L1_SQX_DROPPED_BYTES(offset) (CVMX_ADD_IO_SEG(0x0001540000000088ull) + ((offset) & 31) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L1_SQX_DROPPED_PACKETS(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
+		cvmx_warn("CVMX_PKO_L1_SQX_DROPPED_PACKETS(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000000080ull) + ((offset) & 31) * 512;
+}
+#else
+#define CVMX_PKO_L1_SQX_DROPPED_PACKETS(offset) (CVMX_ADD_IO_SEG(0x0001540000000080ull) + ((offset) & 31) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L1_SQX_GREEN(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
+		cvmx_warn("CVMX_PKO_L1_SQX_GREEN(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000080058ull) + ((offset) & 31) * 512;
+}
+#else
+#define CVMX_PKO_L1_SQX_GREEN(offset) (CVMX_ADD_IO_SEG(0x0001540000080058ull) + ((offset) & 31) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L1_SQX_GREEN_BYTES(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
+		cvmx_warn("CVMX_PKO_L1_SQX_GREEN_BYTES(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00015400000000B8ull) + ((offset) & 31) * 512;
+}
+#else
+#define CVMX_PKO_L1_SQX_GREEN_BYTES(offset) (CVMX_ADD_IO_SEG(0x00015400000000B8ull) + ((offset) & 31) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L1_SQX_GREEN_PACKETS(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
+		cvmx_warn("CVMX_PKO_L1_SQX_GREEN_PACKETS(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00015400000000B0ull) + ((offset) & 31) * 512;
+}
+#else
+#define CVMX_PKO_L1_SQX_GREEN_PACKETS(offset) (CVMX_ADD_IO_SEG(0x00015400000000B0ull) + ((offset) & 31) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L1_SQX_LINK(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
+		cvmx_warn("CVMX_PKO_L1_SQX_LINK(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000000038ull) + ((offset) & 31) * 512;
+}
+#else
+#define CVMX_PKO_L1_SQX_LINK(offset) (CVMX_ADD_IO_SEG(0x0001540000000038ull) + ((offset) & 31) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L1_SQX_PICK(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
+		cvmx_warn("CVMX_PKO_L1_SQX_PICK(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000080070ull) + ((offset) & 31) * 512;
+}
+#else
+#define CVMX_PKO_L1_SQX_PICK(offset) (CVMX_ADD_IO_SEG(0x0001540000080070ull) + ((offset) & 31) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L1_SQX_RED(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
+		cvmx_warn("CVMX_PKO_L1_SQX_RED(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000080068ull) + ((offset) & 31) * 512;
+}
+#else
+#define CVMX_PKO_L1_SQX_RED(offset) (CVMX_ADD_IO_SEG(0x0001540000080068ull) + ((offset) & 31) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L1_SQX_RED_BYTES(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
+		cvmx_warn("CVMX_PKO_L1_SQX_RED_BYTES(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000000098ull) + ((offset) & 31) * 512;
+}
+#else
+#define CVMX_PKO_L1_SQX_RED_BYTES(offset) (CVMX_ADD_IO_SEG(0x0001540000000098ull) + ((offset) & 31) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L1_SQX_RED_PACKETS(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
+		cvmx_warn("CVMX_PKO_L1_SQX_RED_PACKETS(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000000090ull) + ((offset) & 31) * 512;
+}
+#else
+#define CVMX_PKO_L1_SQX_RED_PACKETS(offset) (CVMX_ADD_IO_SEG(0x0001540000000090ull) + ((offset) & 31) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L1_SQX_SCHEDULE(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
+		cvmx_warn("CVMX_PKO_L1_SQX_SCHEDULE(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000000008ull) + ((offset) & 31) * 512;
+}
+#else
+#define CVMX_PKO_L1_SQX_SCHEDULE(offset) (CVMX_ADD_IO_SEG(0x0001540000000008ull) + ((offset) & 31) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L1_SQX_SHAPE(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
+		cvmx_warn("CVMX_PKO_L1_SQX_SHAPE(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000000010ull) + ((offset) & 31) * 512;
+}
+#else
+#define CVMX_PKO_L1_SQX_SHAPE(offset) (CVMX_ADD_IO_SEG(0x0001540000000010ull) + ((offset) & 31) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L1_SQX_SHAPE_STATE(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
+		cvmx_warn("CVMX_PKO_L1_SQX_SHAPE_STATE(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000000030ull) + ((offset) & 31) * 512;
+}
+#else
+#define CVMX_PKO_L1_SQX_SHAPE_STATE(offset) (CVMX_ADD_IO_SEG(0x0001540000000030ull) + ((offset) & 31) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L1_SQX_SW_XOFF(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
+		cvmx_warn("CVMX_PKO_L1_SQX_SW_XOFF(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00015400000000E0ull) + ((offset) & 31) * 512;
+}
+#else
+#define CVMX_PKO_L1_SQX_SW_XOFF(offset) (CVMX_ADD_IO_SEG(0x00015400000000E0ull) + ((offset) & 31) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L1_SQX_TOPOLOGY(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
+		cvmx_warn("CVMX_PKO_L1_SQX_TOPOLOGY(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000080000ull) + ((offset) & 31) * 512;
+}
+#else
+#define CVMX_PKO_L1_SQX_TOPOLOGY(offset) (CVMX_ADD_IO_SEG(0x0001540000080000ull) + ((offset) & 31) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L1_SQX_YELLOW(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
+		cvmx_warn("CVMX_PKO_L1_SQX_YELLOW(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000080060ull) + ((offset) & 31) * 512;
+}
+#else
+#define CVMX_PKO_L1_SQX_YELLOW(offset) (CVMX_ADD_IO_SEG(0x0001540000080060ull) + ((offset) & 31) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L1_SQX_YELLOW_BYTES(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
+		cvmx_warn("CVMX_PKO_L1_SQX_YELLOW_BYTES(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00015400000000A8ull) + ((offset) & 31) * 512;
+}
+#else
+#define CVMX_PKO_L1_SQX_YELLOW_BYTES(offset) (CVMX_ADD_IO_SEG(0x00015400000000A8ull) + ((offset) & 31) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L1_SQX_YELLOW_PACKETS(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 31)))))
+		cvmx_warn("CVMX_PKO_L1_SQX_YELLOW_PACKETS(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00015400000000A0ull) + ((offset) & 31) * 512;
+}
+#else
+#define CVMX_PKO_L1_SQX_YELLOW_PACKETS(offset) (CVMX_ADD_IO_SEG(0x00015400000000A0ull) + ((offset) & 31) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_L1_SQ_CSR_BUS_DEBUG CVMX_PKO_L1_SQ_CSR_BUS_DEBUG_FUNC()
+static inline uint64_t CVMX_PKO_L1_SQ_CSR_BUS_DEBUG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_L1_SQ_CSR_BUS_DEBUG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400000801F8ull);
+}
+#else
+#define CVMX_PKO_L1_SQ_CSR_BUS_DEBUG (CVMX_ADD_IO_SEG(0x00015400000801F8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_L2_SQA_DEBUG CVMX_PKO_L2_SQA_DEBUG_FUNC()
+static inline uint64_t CVMX_PKO_L2_SQA_DEBUG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_L2_SQA_DEBUG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000100128ull);
+}
+#else
+#define CVMX_PKO_L2_SQA_DEBUG (CVMX_ADD_IO_SEG(0x0001540000100128ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_L2_SQB_DEBUG CVMX_PKO_L2_SQB_DEBUG_FUNC()
+static inline uint64_t CVMX_PKO_L2_SQB_DEBUG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_L2_SQB_DEBUG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000100130ull);
+}
+#else
+#define CVMX_PKO_L2_SQB_DEBUG (CVMX_ADD_IO_SEG(0x0001540000100130ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L2_SQX_CIR(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 511)))))
+		cvmx_warn("CVMX_PKO_L2_SQX_CIR(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000080018ull) + ((offset) & 511) * 512;
+}
+#else
+#define CVMX_PKO_L2_SQX_CIR(offset) (CVMX_ADD_IO_SEG(0x0001540000080018ull) + ((offset) & 511) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L2_SQX_GREEN(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 511)))))
+		cvmx_warn("CVMX_PKO_L2_SQX_GREEN(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000100058ull) + ((offset) & 511) * 512;
+}
+#else
+#define CVMX_PKO_L2_SQX_GREEN(offset) (CVMX_ADD_IO_SEG(0x0001540000100058ull) + ((offset) & 511) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L2_SQX_PICK(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 511)))))
+		cvmx_warn("CVMX_PKO_L2_SQX_PICK(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000100070ull) + ((offset) & 511) * 512;
+}
+#else
+#define CVMX_PKO_L2_SQX_PICK(offset) (CVMX_ADD_IO_SEG(0x0001540000100070ull) + ((offset) & 511) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L2_SQX_PIR(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 511)))))
+		cvmx_warn("CVMX_PKO_L2_SQX_PIR(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000080020ull) + ((offset) & 511) * 512;
+}
+#else
+#define CVMX_PKO_L2_SQX_PIR(offset) (CVMX_ADD_IO_SEG(0x0001540000080020ull) + ((offset) & 511) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L2_SQX_POINTERS(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 511)))))
+		cvmx_warn("CVMX_PKO_L2_SQX_POINTERS(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000080078ull) + ((offset) & 511) * 512;
+}
+#else
+#define CVMX_PKO_L2_SQX_POINTERS(offset) (CVMX_ADD_IO_SEG(0x0001540000080078ull) + ((offset) & 511) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L2_SQX_RED(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 511)))))
+		cvmx_warn("CVMX_PKO_L2_SQX_RED(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000100068ull) + ((offset) & 511) * 512;
+}
+#else
+#define CVMX_PKO_L2_SQX_RED(offset) (CVMX_ADD_IO_SEG(0x0001540000100068ull) + ((offset) & 511) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L2_SQX_SCHEDULE(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 511)))))
+		cvmx_warn("CVMX_PKO_L2_SQX_SCHEDULE(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000080008ull) + ((offset) & 511) * 512;
+}
+#else
+#define CVMX_PKO_L2_SQX_SCHEDULE(offset) (CVMX_ADD_IO_SEG(0x0001540000080008ull) + ((offset) & 511) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L2_SQX_SCHED_STATE(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 511)))))
+		cvmx_warn("CVMX_PKO_L2_SQX_SCHED_STATE(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000080028ull) + ((offset) & 511) * 512;
+}
+#else
+#define CVMX_PKO_L2_SQX_SCHED_STATE(offset) (CVMX_ADD_IO_SEG(0x0001540000080028ull) + ((offset) & 511) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L2_SQX_SHAPE(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 511)))))
+		cvmx_warn("CVMX_PKO_L2_SQX_SHAPE(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000080010ull) + ((offset) & 511) * 512;
+}
+#else
+#define CVMX_PKO_L2_SQX_SHAPE(offset) (CVMX_ADD_IO_SEG(0x0001540000080010ull) + ((offset) & 511) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L2_SQX_SHAPE_STATE(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 511)))))
+		cvmx_warn("CVMX_PKO_L2_SQX_SHAPE_STATE(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000080030ull) + ((offset) & 511) * 512;
+}
+#else
+#define CVMX_PKO_L2_SQX_SHAPE_STATE(offset) (CVMX_ADD_IO_SEG(0x0001540000080030ull) + ((offset) & 511) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L2_SQX_SW_XOFF(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 511)))))
+		cvmx_warn("CVMX_PKO_L2_SQX_SW_XOFF(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00015400000800E0ull) + ((offset) & 511) * 512;
+}
+#else
+#define CVMX_PKO_L2_SQX_SW_XOFF(offset) (CVMX_ADD_IO_SEG(0x00015400000800E0ull) + ((offset) & 511) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L2_SQX_TOPOLOGY(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 511)))))
+		cvmx_warn("CVMX_PKO_L2_SQX_TOPOLOGY(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000100000ull) + ((offset) & 511) * 512;
+}
+#else
+#define CVMX_PKO_L2_SQX_TOPOLOGY(offset) (CVMX_ADD_IO_SEG(0x0001540000100000ull) + ((offset) & 511) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L2_SQX_YELLOW(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 511)))))
+		cvmx_warn("CVMX_PKO_L2_SQX_YELLOW(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000100060ull) + ((offset) & 511) * 512;
+}
+#else
+#define CVMX_PKO_L2_SQX_YELLOW(offset) (CVMX_ADD_IO_SEG(0x0001540000100060ull) + ((offset) & 511) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_L2_SQ_CSR_BUS_DEBUG CVMX_PKO_L2_SQ_CSR_BUS_DEBUG_FUNC()
+static inline uint64_t CVMX_PKO_L2_SQ_CSR_BUS_DEBUG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_L2_SQ_CSR_BUS_DEBUG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400001001F8ull);
+}
+#else
+#define CVMX_PKO_L2_SQ_CSR_BUS_DEBUG (CVMX_ADD_IO_SEG(0x00015400001001F8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L3_L2_SQX_CHANNEL(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 511)))))
+		cvmx_warn("CVMX_PKO_L3_L2_SQX_CHANNEL(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000080038ull) + ((offset) & 511) * 512;
+}
+#else
+#define CVMX_PKO_L3_L2_SQX_CHANNEL(offset) (CVMX_ADD_IO_SEG(0x0001540000080038ull) + ((offset) & 511) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_L3_SQA_DEBUG CVMX_PKO_L3_SQA_DEBUG_FUNC()
+static inline uint64_t CVMX_PKO_L3_SQA_DEBUG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_L3_SQA_DEBUG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000180128ull);
+}
+#else
+#define CVMX_PKO_L3_SQA_DEBUG (CVMX_ADD_IO_SEG(0x0001540000180128ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_L3_SQB_DEBUG CVMX_PKO_L3_SQB_DEBUG_FUNC()
+static inline uint64_t CVMX_PKO_L3_SQB_DEBUG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_L3_SQB_DEBUG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000180130ull);
+}
+#else
+#define CVMX_PKO_L3_SQB_DEBUG (CVMX_ADD_IO_SEG(0x0001540000180130ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L3_SQX_CIR(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 511)))))
+		cvmx_warn("CVMX_PKO_L3_SQX_CIR(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000100018ull) + ((offset) & 511) * 512;
+}
+#else
+#define CVMX_PKO_L3_SQX_CIR(offset) (CVMX_ADD_IO_SEG(0x0001540000100018ull) + ((offset) & 511) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L3_SQX_GREEN(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 511)))))
+		cvmx_warn("CVMX_PKO_L3_SQX_GREEN(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000180058ull) + ((offset) & 511) * 512;
+}
+#else
+#define CVMX_PKO_L3_SQX_GREEN(offset) (CVMX_ADD_IO_SEG(0x0001540000180058ull) + ((offset) & 511) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L3_SQX_PICK(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 511)))))
+		cvmx_warn("CVMX_PKO_L3_SQX_PICK(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000180070ull) + ((offset) & 511) * 512;
+}
+#else
+#define CVMX_PKO_L3_SQX_PICK(offset) (CVMX_ADD_IO_SEG(0x0001540000180070ull) + ((offset) & 511) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L3_SQX_PIR(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 511)))))
+		cvmx_warn("CVMX_PKO_L3_SQX_PIR(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000100020ull) + ((offset) & 511) * 512;
+}
+#else
+#define CVMX_PKO_L3_SQX_PIR(offset) (CVMX_ADD_IO_SEG(0x0001540000100020ull) + ((offset) & 511) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L3_SQX_POINTERS(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 511)))))
+		cvmx_warn("CVMX_PKO_L3_SQX_POINTERS(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000100078ull) + ((offset) & 511) * 512;
+}
+#else
+#define CVMX_PKO_L3_SQX_POINTERS(offset) (CVMX_ADD_IO_SEG(0x0001540000100078ull) + ((offset) & 511) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L3_SQX_RED(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 511)))))
+		cvmx_warn("CVMX_PKO_L3_SQX_RED(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000180068ull) + ((offset) & 511) * 512;
+}
+#else
+#define CVMX_PKO_L3_SQX_RED(offset) (CVMX_ADD_IO_SEG(0x0001540000180068ull) + ((offset) & 511) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L3_SQX_SCHEDULE(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 511)))))
+		cvmx_warn("CVMX_PKO_L3_SQX_SCHEDULE(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000100008ull) + ((offset) & 511) * 512;
+}
+#else
+#define CVMX_PKO_L3_SQX_SCHEDULE(offset) (CVMX_ADD_IO_SEG(0x0001540000100008ull) + ((offset) & 511) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L3_SQX_SCHED_STATE(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 511)))))
+		cvmx_warn("CVMX_PKO_L3_SQX_SCHED_STATE(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000100028ull) + ((offset) & 511) * 512;
+}
+#else
+#define CVMX_PKO_L3_SQX_SCHED_STATE(offset) (CVMX_ADD_IO_SEG(0x0001540000100028ull) + ((offset) & 511) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L3_SQX_SHAPE(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 511)))))
+		cvmx_warn("CVMX_PKO_L3_SQX_SHAPE(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000100010ull) + ((offset) & 511) * 512;
+}
+#else
+#define CVMX_PKO_L3_SQX_SHAPE(offset) (CVMX_ADD_IO_SEG(0x0001540000100010ull) + ((offset) & 511) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L3_SQX_SHAPE_STATE(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 511)))))
+		cvmx_warn("CVMX_PKO_L3_SQX_SHAPE_STATE(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000100030ull) + ((offset) & 511) * 512;
+}
+#else
+#define CVMX_PKO_L3_SQX_SHAPE_STATE(offset) (CVMX_ADD_IO_SEG(0x0001540000100030ull) + ((offset) & 511) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L3_SQX_SW_XOFF(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 511)))))
+		cvmx_warn("CVMX_PKO_L3_SQX_SW_XOFF(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00015400001000E0ull) + ((offset) & 511) * 512;
+}
+#else
+#define CVMX_PKO_L3_SQX_SW_XOFF(offset) (CVMX_ADD_IO_SEG(0x00015400001000E0ull) + ((offset) & 511) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L3_SQX_TOPOLOGY(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 511)))))
+		cvmx_warn("CVMX_PKO_L3_SQX_TOPOLOGY(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000180000ull) + ((offset) & 511) * 512;
+}
+#else
+#define CVMX_PKO_L3_SQX_TOPOLOGY(offset) (CVMX_ADD_IO_SEG(0x0001540000180000ull) + ((offset) & 511) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L3_SQX_YELLOW(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 511)))))
+		cvmx_warn("CVMX_PKO_L3_SQX_YELLOW(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000180060ull) + ((offset) & 511) * 512;
+}
+#else
+#define CVMX_PKO_L3_SQX_YELLOW(offset) (CVMX_ADD_IO_SEG(0x0001540000180060ull) + ((offset) & 511) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_L3_SQ_CSR_BUS_DEBUG CVMX_PKO_L3_SQ_CSR_BUS_DEBUG_FUNC()
+static inline uint64_t CVMX_PKO_L3_SQ_CSR_BUS_DEBUG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_L3_SQ_CSR_BUS_DEBUG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400001801F8ull);
+}
+#else
+#define CVMX_PKO_L3_SQ_CSR_BUS_DEBUG (CVMX_ADD_IO_SEG(0x00015400001801F8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_L4_SQA_DEBUG CVMX_PKO_L4_SQA_DEBUG_FUNC()
+static inline uint64_t CVMX_PKO_L4_SQA_DEBUG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_L4_SQA_DEBUG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000200128ull);
+}
+#else
+#define CVMX_PKO_L4_SQA_DEBUG (CVMX_ADD_IO_SEG(0x0001540000200128ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_L4_SQB_DEBUG CVMX_PKO_L4_SQB_DEBUG_FUNC()
+static inline uint64_t CVMX_PKO_L4_SQB_DEBUG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_L4_SQB_DEBUG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000200130ull);
+}
+#else
+#define CVMX_PKO_L4_SQB_DEBUG (CVMX_ADD_IO_SEG(0x0001540000200130ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L4_SQX_CIR(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_L4_SQX_CIR(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000180018ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_L4_SQX_CIR(offset) (CVMX_ADD_IO_SEG(0x0001540000180018ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L4_SQX_GREEN(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_L4_SQX_GREEN(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000200058ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_L4_SQX_GREEN(offset) (CVMX_ADD_IO_SEG(0x0001540000200058ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L4_SQX_PICK(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_L4_SQX_PICK(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000200070ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_L4_SQX_PICK(offset) (CVMX_ADD_IO_SEG(0x0001540000200070ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L4_SQX_PIR(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_L4_SQX_PIR(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000180020ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_L4_SQX_PIR(offset) (CVMX_ADD_IO_SEG(0x0001540000180020ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L4_SQX_POINTERS(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_L4_SQX_POINTERS(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000180078ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_L4_SQX_POINTERS(offset) (CVMX_ADD_IO_SEG(0x0001540000180078ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L4_SQX_RED(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_L4_SQX_RED(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000200068ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_L4_SQX_RED(offset) (CVMX_ADD_IO_SEG(0x0001540000200068ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L4_SQX_SCHEDULE(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_L4_SQX_SCHEDULE(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000180008ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_L4_SQX_SCHEDULE(offset) (CVMX_ADD_IO_SEG(0x0001540000180008ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L4_SQX_SCHED_STATE(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 511)))))
+		cvmx_warn("CVMX_PKO_L4_SQX_SCHED_STATE(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000180028ull) + ((offset) & 511) * 512;
+}
+#else
+#define CVMX_PKO_L4_SQX_SCHED_STATE(offset) (CVMX_ADD_IO_SEG(0x0001540000180028ull) + ((offset) & 511) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L4_SQX_SHAPE(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_L4_SQX_SHAPE(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000180010ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_L4_SQX_SHAPE(offset) (CVMX_ADD_IO_SEG(0x0001540000180010ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L4_SQX_SHAPE_STATE(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 511)))))
+		cvmx_warn("CVMX_PKO_L4_SQX_SHAPE_STATE(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000180030ull) + ((offset) & 511) * 512;
+}
+#else
+#define CVMX_PKO_L4_SQX_SHAPE_STATE(offset) (CVMX_ADD_IO_SEG(0x0001540000180030ull) + ((offset) & 511) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L4_SQX_SW_XOFF(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_L4_SQX_SW_XOFF(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00015400001800E0ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_L4_SQX_SW_XOFF(offset) (CVMX_ADD_IO_SEG(0x00015400001800E0ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L4_SQX_TOPOLOGY(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_L4_SQX_TOPOLOGY(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000200000ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_L4_SQX_TOPOLOGY(offset) (CVMX_ADD_IO_SEG(0x0001540000200000ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L4_SQX_YELLOW(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_L4_SQX_YELLOW(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000200060ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_L4_SQX_YELLOW(offset) (CVMX_ADD_IO_SEG(0x0001540000200060ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_L4_SQ_CSR_BUS_DEBUG CVMX_PKO_L4_SQ_CSR_BUS_DEBUG_FUNC()
+static inline uint64_t CVMX_PKO_L4_SQ_CSR_BUS_DEBUG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_L4_SQ_CSR_BUS_DEBUG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400002001F8ull);
+}
+#else
+#define CVMX_PKO_L4_SQ_CSR_BUS_DEBUG (CVMX_ADD_IO_SEG(0x00015400002001F8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_L5_SQA_DEBUG CVMX_PKO_L5_SQA_DEBUG_FUNC()
+static inline uint64_t CVMX_PKO_L5_SQA_DEBUG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_L5_SQA_DEBUG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000280128ull);
+}
+#else
+#define CVMX_PKO_L5_SQA_DEBUG (CVMX_ADD_IO_SEG(0x0001540000280128ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_L5_SQB_DEBUG CVMX_PKO_L5_SQB_DEBUG_FUNC()
+static inline uint64_t CVMX_PKO_L5_SQB_DEBUG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_L5_SQB_DEBUG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000280130ull);
+}
+#else
+#define CVMX_PKO_L5_SQB_DEBUG (CVMX_ADD_IO_SEG(0x0001540000280130ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L5_SQX_CIR(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_L5_SQX_CIR(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000200018ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_L5_SQX_CIR(offset) (CVMX_ADD_IO_SEG(0x0001540000200018ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L5_SQX_GREEN(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_L5_SQX_GREEN(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000280058ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_L5_SQX_GREEN(offset) (CVMX_ADD_IO_SEG(0x0001540000280058ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L5_SQX_PICK(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_L5_SQX_PICK(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000280070ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_L5_SQX_PICK(offset) (CVMX_ADD_IO_SEG(0x0001540000280070ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L5_SQX_PIR(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_L5_SQX_PIR(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000200020ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_L5_SQX_PIR(offset) (CVMX_ADD_IO_SEG(0x0001540000200020ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L5_SQX_POINTERS(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_L5_SQX_POINTERS(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000200078ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_L5_SQX_POINTERS(offset) (CVMX_ADD_IO_SEG(0x0001540000200078ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L5_SQX_RED(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_L5_SQX_RED(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000280068ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_L5_SQX_RED(offset) (CVMX_ADD_IO_SEG(0x0001540000280068ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L5_SQX_SCHEDULE(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_L5_SQX_SCHEDULE(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000200008ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_L5_SQX_SCHEDULE(offset) (CVMX_ADD_IO_SEG(0x0001540000200008ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L5_SQX_SCHED_STATE(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_L5_SQX_SCHED_STATE(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000200028ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_L5_SQX_SCHED_STATE(offset) (CVMX_ADD_IO_SEG(0x0001540000200028ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L5_SQX_SHAPE(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_L5_SQX_SHAPE(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000200010ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_L5_SQX_SHAPE(offset) (CVMX_ADD_IO_SEG(0x0001540000200010ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L5_SQX_SHAPE_STATE(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_L5_SQX_SHAPE_STATE(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000200030ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_L5_SQX_SHAPE_STATE(offset) (CVMX_ADD_IO_SEG(0x0001540000200030ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L5_SQX_SW_XOFF(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_L5_SQX_SW_XOFF(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00015400002000E0ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_L5_SQX_SW_XOFF(offset) (CVMX_ADD_IO_SEG(0x00015400002000E0ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L5_SQX_TOPOLOGY(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_L5_SQX_TOPOLOGY(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000280000ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_L5_SQX_TOPOLOGY(offset) (CVMX_ADD_IO_SEG(0x0001540000280000ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_L5_SQX_YELLOW(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_L5_SQX_YELLOW(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000280060ull) + ((offset) & 1023) * 512;
+}
+#else
+#define CVMX_PKO_L5_SQX_YELLOW(offset) (CVMX_ADD_IO_SEG(0x0001540000280060ull) + ((offset) & 1023) * 512)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_L5_SQ_CSR_BUS_DEBUG CVMX_PKO_L5_SQ_CSR_BUS_DEBUG_FUNC()
+static inline uint64_t CVMX_PKO_L5_SQ_CSR_BUS_DEBUG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_L5_SQ_CSR_BUS_DEBUG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400002801F8ull);
+}
+#else
+#define CVMX_PKO_L5_SQ_CSR_BUS_DEBUG (CVMX_ADD_IO_SEG(0x00015400002801F8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_LUTX(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_LUTX(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000B00000ull) + ((offset) & 1023) * 8;
+}
+#else
+#define CVMX_PKO_LUTX(offset) (CVMX_ADD_IO_SEG(0x0001540000B00000ull) + ((offset) & 1023) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_LUT_BIST_STATUS CVMX_PKO_LUT_BIST_STATUS_FUNC()
+static inline uint64_t CVMX_PKO_LUT_BIST_STATUS_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_LUT_BIST_STATUS not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000B02018ull);
+}
+#else
+#define CVMX_PKO_LUT_BIST_STATUS (CVMX_ADD_IO_SEG(0x0001540000B02018ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_LUT_ECC_CTL0 CVMX_PKO_LUT_ECC_CTL0_FUNC()
+static inline uint64_t CVMX_PKO_LUT_ECC_CTL0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_LUT_ECC_CTL0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000BFFFD0ull);
+}
+#else
+#define CVMX_PKO_LUT_ECC_CTL0 (CVMX_ADD_IO_SEG(0x0001540000BFFFD0ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_LUT_ECC_DBE_STS0 CVMX_PKO_LUT_ECC_DBE_STS0_FUNC()
+static inline uint64_t CVMX_PKO_LUT_ECC_DBE_STS0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_LUT_ECC_DBE_STS0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000BFFFF0ull);
+}
+#else
+#define CVMX_PKO_LUT_ECC_DBE_STS0 (CVMX_ADD_IO_SEG(0x0001540000BFFFF0ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_LUT_ECC_DBE_STS_CMB0 CVMX_PKO_LUT_ECC_DBE_STS_CMB0_FUNC()
+static inline uint64_t CVMX_PKO_LUT_ECC_DBE_STS_CMB0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_LUT_ECC_DBE_STS_CMB0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000BFFFD8ull);
+}
+#else
+#define CVMX_PKO_LUT_ECC_DBE_STS_CMB0 (CVMX_ADD_IO_SEG(0x0001540000BFFFD8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_LUT_ECC_SBE_STS0 CVMX_PKO_LUT_ECC_SBE_STS0_FUNC()
+static inline uint64_t CVMX_PKO_LUT_ECC_SBE_STS0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_LUT_ECC_SBE_STS0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000BFFFF8ull);
+}
+#else
+#define CVMX_PKO_LUT_ECC_SBE_STS0 (CVMX_ADD_IO_SEG(0x0001540000BFFFF8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_LUT_ECC_SBE_STS_CMB0 CVMX_PKO_LUT_ECC_SBE_STS_CMB0_FUNC()
+static inline uint64_t CVMX_PKO_LUT_ECC_SBE_STS_CMB0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_LUT_ECC_SBE_STS_CMB0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000BFFFE8ull);
+}
+#else
+#define CVMX_PKO_LUT_ECC_SBE_STS_CMB0 (CVMX_ADD_IO_SEG(0x0001540000BFFFE8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_MACX_CFG(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 27)))))
+		cvmx_warn("CVMX_PKO_MACX_CFG(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000900000ull) + ((offset) & 31) * 8;
+}
+#else
+#define CVMX_PKO_MACX_CFG(offset) (CVMX_ADD_IO_SEG(0x0001540000900000ull) + ((offset) & 31) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_MCI0_CRED_CNTX(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 27)))))
+		cvmx_warn("CVMX_PKO_MCI0_CRED_CNTX(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000A40000ull) + ((offset) & 31) * 8;
+}
+#else
+#define CVMX_PKO_MCI0_CRED_CNTX(offset) (CVMX_ADD_IO_SEG(0x0001540000A40000ull) + ((offset) & 31) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_MCI0_MAX_CREDX(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 27)))))
+		cvmx_warn("CVMX_PKO_MCI0_MAX_CREDX(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000A00000ull) + ((offset) & 31) * 8;
+}
+#else
+#define CVMX_PKO_MCI0_MAX_CREDX(offset) (CVMX_ADD_IO_SEG(0x0001540000A00000ull) + ((offset) & 31) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_MCI1_CRED_CNTX(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 27)))))
+		cvmx_warn("CVMX_PKO_MCI1_CRED_CNTX(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000A80100ull) + ((offset) & 31) * 8;
+}
+#else
+#define CVMX_PKO_MCI1_CRED_CNTX(offset) (CVMX_ADD_IO_SEG(0x0001540000A80100ull) + ((offset) & 31) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_MCI1_MAX_CREDX(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 27)))))
+		cvmx_warn("CVMX_PKO_MCI1_MAX_CREDX(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000A80000ull) + ((offset) & 31) * 8;
+}
+#else
+#define CVMX_PKO_MCI1_MAX_CREDX(offset) (CVMX_ADD_IO_SEG(0x0001540000A80000ull) + ((offset) & 31) * 8)
+#endif
+#define CVMX_PKO_MEM_COUNT0 (CVMX_ADD_IO_SEG(0x0001180050001080ull))
+#define CVMX_PKO_MEM_COUNT1 (CVMX_ADD_IO_SEG(0x0001180050001088ull))
+#define CVMX_PKO_MEM_DEBUG0 (CVMX_ADD_IO_SEG(0x0001180050001100ull))
+#define CVMX_PKO_MEM_DEBUG1 (CVMX_ADD_IO_SEG(0x0001180050001108ull))
+#define CVMX_PKO_MEM_DEBUG10 (CVMX_ADD_IO_SEG(0x0001180050001150ull))
+#define CVMX_PKO_MEM_DEBUG11 (CVMX_ADD_IO_SEG(0x0001180050001158ull))
+#define CVMX_PKO_MEM_DEBUG12 (CVMX_ADD_IO_SEG(0x0001180050001160ull))
+#define CVMX_PKO_MEM_DEBUG13 (CVMX_ADD_IO_SEG(0x0001180050001168ull))
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_MEM_DEBUG14 CVMX_PKO_MEM_DEBUG14_FUNC()
+static inline uint64_t CVMX_PKO_MEM_DEBUG14_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN52XX) || OCTEON_IS_MODEL(OCTEON_CN56XX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_PKO_MEM_DEBUG14 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180050001170ull);
+}
+#else
+#define CVMX_PKO_MEM_DEBUG14 (CVMX_ADD_IO_SEG(0x0001180050001170ull))
+#endif
+#define CVMX_PKO_MEM_DEBUG2 (CVMX_ADD_IO_SEG(0x0001180050001110ull))
+#define CVMX_PKO_MEM_DEBUG3 (CVMX_ADD_IO_SEG(0x0001180050001118ull))
+#define CVMX_PKO_MEM_DEBUG4 (CVMX_ADD_IO_SEG(0x0001180050001120ull))
+#define CVMX_PKO_MEM_DEBUG5 (CVMX_ADD_IO_SEG(0x0001180050001128ull))
+#define CVMX_PKO_MEM_DEBUG6 (CVMX_ADD_IO_SEG(0x0001180050001130ull))
+#define CVMX_PKO_MEM_DEBUG7 (CVMX_ADD_IO_SEG(0x0001180050001138ull))
+#define CVMX_PKO_MEM_DEBUG8 (CVMX_ADD_IO_SEG(0x0001180050001140ull))
+#define CVMX_PKO_MEM_DEBUG9 (CVMX_ADD_IO_SEG(0x0001180050001148ull))
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_MEM_IPORT_PTRS CVMX_PKO_MEM_IPORT_PTRS_FUNC()
+static inline uint64_t CVMX_PKO_MEM_IPORT_PTRS_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX)))
+		cvmx_warn("CVMX_PKO_MEM_IPORT_PTRS not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180050001030ull);
+}
+#else
+#define CVMX_PKO_MEM_IPORT_PTRS (CVMX_ADD_IO_SEG(0x0001180050001030ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_MEM_IPORT_QOS CVMX_PKO_MEM_IPORT_QOS_FUNC()
+static inline uint64_t CVMX_PKO_MEM_IPORT_QOS_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX)))
+		cvmx_warn("CVMX_PKO_MEM_IPORT_QOS not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180050001038ull);
+}
+#else
+#define CVMX_PKO_MEM_IPORT_QOS (CVMX_ADD_IO_SEG(0x0001180050001038ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_MEM_IQUEUE_PTRS CVMX_PKO_MEM_IQUEUE_PTRS_FUNC()
+static inline uint64_t CVMX_PKO_MEM_IQUEUE_PTRS_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX)))
+		cvmx_warn("CVMX_PKO_MEM_IQUEUE_PTRS not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180050001040ull);
+}
+#else
+#define CVMX_PKO_MEM_IQUEUE_PTRS (CVMX_ADD_IO_SEG(0x0001180050001040ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_MEM_IQUEUE_QOS CVMX_PKO_MEM_IQUEUE_QOS_FUNC()
+static inline uint64_t CVMX_PKO_MEM_IQUEUE_QOS_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX)))
+		cvmx_warn("CVMX_PKO_MEM_IQUEUE_QOS not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180050001048ull);
+}
+#else
+#define CVMX_PKO_MEM_IQUEUE_QOS (CVMX_ADD_IO_SEG(0x0001180050001048ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_MEM_PORT_PTRS CVMX_PKO_MEM_PORT_PTRS_FUNC()
+static inline uint64_t CVMX_PKO_MEM_PORT_PTRS_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN52XX) || OCTEON_IS_MODEL(OCTEON_CN56XX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_PKO_MEM_PORT_PTRS not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180050001010ull);
+}
+#else
+#define CVMX_PKO_MEM_PORT_PTRS (CVMX_ADD_IO_SEG(0x0001180050001010ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_MEM_PORT_QOS CVMX_PKO_MEM_PORT_QOS_FUNC()
+static inline uint64_t CVMX_PKO_MEM_PORT_QOS_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN52XX) || OCTEON_IS_MODEL(OCTEON_CN56XX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_PKO_MEM_PORT_QOS not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180050001018ull);
+}
+#else
+#define CVMX_PKO_MEM_PORT_QOS (CVMX_ADD_IO_SEG(0x0001180050001018ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_MEM_PORT_RATE0 CVMX_PKO_MEM_PORT_RATE0_FUNC()
+static inline uint64_t CVMX_PKO_MEM_PORT_RATE0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN52XX) || OCTEON_IS_MODEL(OCTEON_CN56XX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_PKO_MEM_PORT_RATE0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180050001020ull);
+}
+#else
+#define CVMX_PKO_MEM_PORT_RATE0 (CVMX_ADD_IO_SEG(0x0001180050001020ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_MEM_PORT_RATE1 CVMX_PKO_MEM_PORT_RATE1_FUNC()
+static inline uint64_t CVMX_PKO_MEM_PORT_RATE1_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN52XX) || OCTEON_IS_MODEL(OCTEON_CN56XX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_PKO_MEM_PORT_RATE1 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180050001028ull);
+}
+#else
+#define CVMX_PKO_MEM_PORT_RATE1 (CVMX_ADD_IO_SEG(0x0001180050001028ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_MEM_QUEUE_PTRS CVMX_PKO_MEM_QUEUE_PTRS_FUNC()
+static inline uint64_t CVMX_PKO_MEM_QUEUE_PTRS_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_PKO_MEM_QUEUE_PTRS not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180050001000ull);
+}
+#else
+#define CVMX_PKO_MEM_QUEUE_PTRS (CVMX_ADD_IO_SEG(0x0001180050001000ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_MEM_QUEUE_QOS CVMX_PKO_MEM_QUEUE_QOS_FUNC()
+static inline uint64_t CVMX_PKO_MEM_QUEUE_QOS_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_PKO_MEM_QUEUE_QOS not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180050001008ull);
+}
+#else
+#define CVMX_PKO_MEM_QUEUE_QOS (CVMX_ADD_IO_SEG(0x0001180050001008ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_MEM_THROTTLE_INT CVMX_PKO_MEM_THROTTLE_INT_FUNC()
+static inline uint64_t CVMX_PKO_MEM_THROTTLE_INT_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX)))
+		cvmx_warn("CVMX_PKO_MEM_THROTTLE_INT not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180050001058ull);
+}
+#else
+#define CVMX_PKO_MEM_THROTTLE_INT (CVMX_ADD_IO_SEG(0x0001180050001058ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_MEM_THROTTLE_PIPE CVMX_PKO_MEM_THROTTLE_PIPE_FUNC()
+static inline uint64_t CVMX_PKO_MEM_THROTTLE_PIPE_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX)))
+		cvmx_warn("CVMX_PKO_MEM_THROTTLE_PIPE not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180050001050ull);
+}
+#else
+#define CVMX_PKO_MEM_THROTTLE_PIPE (CVMX_ADD_IO_SEG(0x0001180050001050ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_NCB_BIST_STATUS CVMX_PKO_NCB_BIST_STATUS_FUNC()
+static inline uint64_t CVMX_PKO_NCB_BIST_STATUS_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_NCB_BIST_STATUS not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000EFFF00ull);
+}
+#else
+#define CVMX_PKO_NCB_BIST_STATUS (CVMX_ADD_IO_SEG(0x0001540000EFFF00ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_NCB_ECC_CTL0 CVMX_PKO_NCB_ECC_CTL0_FUNC()
+static inline uint64_t CVMX_PKO_NCB_ECC_CTL0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_NCB_ECC_CTL0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000EFFFD0ull);
+}
+#else
+#define CVMX_PKO_NCB_ECC_CTL0 (CVMX_ADD_IO_SEG(0x0001540000EFFFD0ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_NCB_ECC_DBE_STS0 CVMX_PKO_NCB_ECC_DBE_STS0_FUNC()
+static inline uint64_t CVMX_PKO_NCB_ECC_DBE_STS0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_NCB_ECC_DBE_STS0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000EFFFF0ull);
+}
+#else
+#define CVMX_PKO_NCB_ECC_DBE_STS0 (CVMX_ADD_IO_SEG(0x0001540000EFFFF0ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_NCB_ECC_DBE_STS_CMB0 CVMX_PKO_NCB_ECC_DBE_STS_CMB0_FUNC()
+static inline uint64_t CVMX_PKO_NCB_ECC_DBE_STS_CMB0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_NCB_ECC_DBE_STS_CMB0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000EFFFD8ull);
+}
+#else
+#define CVMX_PKO_NCB_ECC_DBE_STS_CMB0 (CVMX_ADD_IO_SEG(0x0001540000EFFFD8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_NCB_ECC_SBE_STS0 CVMX_PKO_NCB_ECC_SBE_STS0_FUNC()
+static inline uint64_t CVMX_PKO_NCB_ECC_SBE_STS0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_NCB_ECC_SBE_STS0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000EFFFF8ull);
+}
+#else
+#define CVMX_PKO_NCB_ECC_SBE_STS0 (CVMX_ADD_IO_SEG(0x0001540000EFFFF8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_NCB_ECC_SBE_STS_CMB0 CVMX_PKO_NCB_ECC_SBE_STS_CMB0_FUNC()
+static inline uint64_t CVMX_PKO_NCB_ECC_SBE_STS_CMB0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_NCB_ECC_SBE_STS_CMB0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000EFFFE8ull);
+}
+#else
+#define CVMX_PKO_NCB_ECC_SBE_STS_CMB0 (CVMX_ADD_IO_SEG(0x0001540000EFFFE8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_NCB_INT CVMX_PKO_NCB_INT_FUNC()
+static inline uint64_t CVMX_PKO_NCB_INT_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_NCB_INT not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000E00010ull);
+}
+#else
+#define CVMX_PKO_NCB_INT (CVMX_ADD_IO_SEG(0x0001540000E00010ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_NCB_TX_ERR_INFO CVMX_PKO_NCB_TX_ERR_INFO_FUNC()
+static inline uint64_t CVMX_PKO_NCB_TX_ERR_INFO_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_NCB_TX_ERR_INFO not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000E00008ull);
+}
+#else
+#define CVMX_PKO_NCB_TX_ERR_INFO (CVMX_ADD_IO_SEG(0x0001540000E00008ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_NCB_TX_ERR_WORD CVMX_PKO_NCB_TX_ERR_WORD_FUNC()
+static inline uint64_t CVMX_PKO_NCB_TX_ERR_WORD_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_NCB_TX_ERR_WORD not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000E00000ull);
+}
+#else
+#define CVMX_PKO_NCB_TX_ERR_WORD (CVMX_ADD_IO_SEG(0x0001540000E00000ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_BIST_STATUS CVMX_PKO_PDM_BIST_STATUS_FUNC()
+static inline uint64_t CVMX_PKO_PDM_BIST_STATUS_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_BIST_STATUS not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400008FFF00ull);
+}
+#else
+#define CVMX_PKO_PDM_BIST_STATUS (CVMX_ADD_IO_SEG(0x00015400008FFF00ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_CFG CVMX_PKO_PDM_CFG_FUNC()
+static inline uint64_t CVMX_PKO_PDM_CFG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_CFG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000800000ull);
+}
+#else
+#define CVMX_PKO_PDM_CFG (CVMX_ADD_IO_SEG(0x0001540000800000ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_CFG_DBG CVMX_PKO_PDM_CFG_DBG_FUNC()
+static inline uint64_t CVMX_PKO_PDM_CFG_DBG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_CFG_DBG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000800FF8ull);
+}
+#else
+#define CVMX_PKO_PDM_CFG_DBG (CVMX_ADD_IO_SEG(0x0001540000800FF8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_CP_DBG CVMX_PKO_PDM_CP_DBG_FUNC()
+static inline uint64_t CVMX_PKO_PDM_CP_DBG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_CP_DBG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000800190ull);
+}
+#else
+#define CVMX_PKO_PDM_CP_DBG (CVMX_ADD_IO_SEG(0x0001540000800190ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_PDM_DQX_MINPAD(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 1023)))))
+		cvmx_warn("CVMX_PKO_PDM_DQX_MINPAD(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x00015400008F0000ull) + ((offset) & 1023) * 8;
+}
+#else
+#define CVMX_PKO_PDM_DQX_MINPAD(offset) (CVMX_ADD_IO_SEG(0x00015400008F0000ull) + ((offset) & 1023) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_DRPBUF_DBG CVMX_PKO_PDM_DRPBUF_DBG_FUNC()
+static inline uint64_t CVMX_PKO_PDM_DRPBUF_DBG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_DRPBUF_DBG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400008000B0ull);
+}
+#else
+#define CVMX_PKO_PDM_DRPBUF_DBG (CVMX_ADD_IO_SEG(0x00015400008000B0ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_DWPBUF_DBG CVMX_PKO_PDM_DWPBUF_DBG_FUNC()
+static inline uint64_t CVMX_PKO_PDM_DWPBUF_DBG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_DWPBUF_DBG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400008000A8ull);
+}
+#else
+#define CVMX_PKO_PDM_DWPBUF_DBG (CVMX_ADD_IO_SEG(0x00015400008000A8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_ECC_CTL0 CVMX_PKO_PDM_ECC_CTL0_FUNC()
+static inline uint64_t CVMX_PKO_PDM_ECC_CTL0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_ECC_CTL0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400008FFFD0ull);
+}
+#else
+#define CVMX_PKO_PDM_ECC_CTL0 (CVMX_ADD_IO_SEG(0x00015400008FFFD0ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_ECC_CTL1 CVMX_PKO_PDM_ECC_CTL1_FUNC()
+static inline uint64_t CVMX_PKO_PDM_ECC_CTL1_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_ECC_CTL1 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400008FFFD8ull);
+}
+#else
+#define CVMX_PKO_PDM_ECC_CTL1 (CVMX_ADD_IO_SEG(0x00015400008FFFD8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_ECC_DBE_STS0 CVMX_PKO_PDM_ECC_DBE_STS0_FUNC()
+static inline uint64_t CVMX_PKO_PDM_ECC_DBE_STS0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_ECC_DBE_STS0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400008FFFF0ull);
+}
+#else
+#define CVMX_PKO_PDM_ECC_DBE_STS0 (CVMX_ADD_IO_SEG(0x00015400008FFFF0ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_ECC_DBE_STS_CMB0 CVMX_PKO_PDM_ECC_DBE_STS_CMB0_FUNC()
+static inline uint64_t CVMX_PKO_PDM_ECC_DBE_STS_CMB0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_ECC_DBE_STS_CMB0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400008FFFE0ull);
+}
+#else
+#define CVMX_PKO_PDM_ECC_DBE_STS_CMB0 (CVMX_ADD_IO_SEG(0x00015400008FFFE0ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_ECC_SBE_STS0 CVMX_PKO_PDM_ECC_SBE_STS0_FUNC()
+static inline uint64_t CVMX_PKO_PDM_ECC_SBE_STS0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_ECC_SBE_STS0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400008FFFF8ull);
+}
+#else
+#define CVMX_PKO_PDM_ECC_SBE_STS0 (CVMX_ADD_IO_SEG(0x00015400008FFFF8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_ECC_SBE_STS_CMB0 CVMX_PKO_PDM_ECC_SBE_STS_CMB0_FUNC()
+static inline uint64_t CVMX_PKO_PDM_ECC_SBE_STS_CMB0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_ECC_SBE_STS_CMB0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400008FFFE8ull);
+}
+#else
+#define CVMX_PKO_PDM_ECC_SBE_STS_CMB0 (CVMX_ADD_IO_SEG(0x00015400008FFFE8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_FILLB_DBG0 CVMX_PKO_PDM_FILLB_DBG0_FUNC()
+static inline uint64_t CVMX_PKO_PDM_FILLB_DBG0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_FILLB_DBG0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400008002A0ull);
+}
+#else
+#define CVMX_PKO_PDM_FILLB_DBG0 (CVMX_ADD_IO_SEG(0x00015400008002A0ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_FILLB_DBG1 CVMX_PKO_PDM_FILLB_DBG1_FUNC()
+static inline uint64_t CVMX_PKO_PDM_FILLB_DBG1_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_FILLB_DBG1 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400008002A8ull);
+}
+#else
+#define CVMX_PKO_PDM_FILLB_DBG1 (CVMX_ADD_IO_SEG(0x00015400008002A8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_FILLB_DBG2 CVMX_PKO_PDM_FILLB_DBG2_FUNC()
+static inline uint64_t CVMX_PKO_PDM_FILLB_DBG2_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_FILLB_DBG2 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400008002B0ull);
+}
+#else
+#define CVMX_PKO_PDM_FILLB_DBG2 (CVMX_ADD_IO_SEG(0x00015400008002B0ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_FLSHB_DBG0 CVMX_PKO_PDM_FLSHB_DBG0_FUNC()
+static inline uint64_t CVMX_PKO_PDM_FLSHB_DBG0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_FLSHB_DBG0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400008002B8ull);
+}
+#else
+#define CVMX_PKO_PDM_FLSHB_DBG0 (CVMX_ADD_IO_SEG(0x00015400008002B8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_FLSHB_DBG1 CVMX_PKO_PDM_FLSHB_DBG1_FUNC()
+static inline uint64_t CVMX_PKO_PDM_FLSHB_DBG1_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_FLSHB_DBG1 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400008002C0ull);
+}
+#else
+#define CVMX_PKO_PDM_FLSHB_DBG1 (CVMX_ADD_IO_SEG(0x00015400008002C0ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_ISRD_DBG CVMX_PKO_PDM_ISRD_DBG_FUNC()
+static inline uint64_t CVMX_PKO_PDM_ISRD_DBG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_ISRD_DBG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000800090ull);
+}
+#else
+#define CVMX_PKO_PDM_ISRD_DBG (CVMX_ADD_IO_SEG(0x0001540000800090ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_ISRD_DBG_DQ CVMX_PKO_PDM_ISRD_DBG_DQ_FUNC()
+static inline uint64_t CVMX_PKO_PDM_ISRD_DBG_DQ_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_ISRD_DBG_DQ not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000800290ull);
+}
+#else
+#define CVMX_PKO_PDM_ISRD_DBG_DQ (CVMX_ADD_IO_SEG(0x0001540000800290ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_ISRM_DBG CVMX_PKO_PDM_ISRM_DBG_FUNC()
+static inline uint64_t CVMX_PKO_PDM_ISRM_DBG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_ISRM_DBG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000800098ull);
+}
+#else
+#define CVMX_PKO_PDM_ISRM_DBG (CVMX_ADD_IO_SEG(0x0001540000800098ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_ISRM_DBG_DQ CVMX_PKO_PDM_ISRM_DBG_DQ_FUNC()
+static inline uint64_t CVMX_PKO_PDM_ISRM_DBG_DQ_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_ISRM_DBG_DQ not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000800298ull);
+}
+#else
+#define CVMX_PKO_PDM_ISRM_DBG_DQ (CVMX_ADD_IO_SEG(0x0001540000800298ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_MEM_ADDR CVMX_PKO_PDM_MEM_ADDR_FUNC()
+static inline uint64_t CVMX_PKO_PDM_MEM_ADDR_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_MEM_ADDR not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000800018ull);
+}
+#else
+#define CVMX_PKO_PDM_MEM_ADDR (CVMX_ADD_IO_SEG(0x0001540000800018ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_MEM_DATA CVMX_PKO_PDM_MEM_DATA_FUNC()
+static inline uint64_t CVMX_PKO_PDM_MEM_DATA_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_MEM_DATA not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000800010ull);
+}
+#else
+#define CVMX_PKO_PDM_MEM_DATA (CVMX_ADD_IO_SEG(0x0001540000800010ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_MEM_RW_CTL CVMX_PKO_PDM_MEM_RW_CTL_FUNC()
+static inline uint64_t CVMX_PKO_PDM_MEM_RW_CTL_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_MEM_RW_CTL not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000800020ull);
+}
+#else
+#define CVMX_PKO_PDM_MEM_RW_CTL (CVMX_ADD_IO_SEG(0x0001540000800020ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_MEM_RW_STS CVMX_PKO_PDM_MEM_RW_STS_FUNC()
+static inline uint64_t CVMX_PKO_PDM_MEM_RW_STS_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_MEM_RW_STS not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000800028ull);
+}
+#else
+#define CVMX_PKO_PDM_MEM_RW_STS (CVMX_ADD_IO_SEG(0x0001540000800028ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_MWPBUF_DBG CVMX_PKO_PDM_MWPBUF_DBG_FUNC()
+static inline uint64_t CVMX_PKO_PDM_MWPBUF_DBG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_MWPBUF_DBG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400008000A0ull);
+}
+#else
+#define CVMX_PKO_PDM_MWPBUF_DBG (CVMX_ADD_IO_SEG(0x00015400008000A0ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PDM_STS CVMX_PKO_PDM_STS_FUNC()
+static inline uint64_t CVMX_PKO_PDM_STS_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PDM_STS not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000800008ull);
+}
+#else
+#define CVMX_PKO_PDM_STS (CVMX_ADD_IO_SEG(0x0001540000800008ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PEB_BIST_STATUS CVMX_PKO_PEB_BIST_STATUS_FUNC()
+static inline uint64_t CVMX_PKO_PEB_BIST_STATUS_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PEB_BIST_STATUS not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000900D00ull);
+}
+#else
+#define CVMX_PKO_PEB_BIST_STATUS (CVMX_ADD_IO_SEG(0x0001540000900D00ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PEB_ECC_CTL0 CVMX_PKO_PEB_ECC_CTL0_FUNC()
+static inline uint64_t CVMX_PKO_PEB_ECC_CTL0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PEB_ECC_CTL0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400009FFFD0ull);
+}
+#else
+#define CVMX_PKO_PEB_ECC_CTL0 (CVMX_ADD_IO_SEG(0x00015400009FFFD0ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PEB_ECC_CTL1 CVMX_PKO_PEB_ECC_CTL1_FUNC()
+static inline uint64_t CVMX_PKO_PEB_ECC_CTL1_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PEB_ECC_CTL1 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400009FFFA8ull);
+}
+#else
+#define CVMX_PKO_PEB_ECC_CTL1 (CVMX_ADD_IO_SEG(0x00015400009FFFA8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PEB_ECC_DBE_STS0 CVMX_PKO_PEB_ECC_DBE_STS0_FUNC()
+static inline uint64_t CVMX_PKO_PEB_ECC_DBE_STS0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PEB_ECC_DBE_STS0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400009FFFF0ull);
+}
+#else
+#define CVMX_PKO_PEB_ECC_DBE_STS0 (CVMX_ADD_IO_SEG(0x00015400009FFFF0ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PEB_ECC_DBE_STS_CMB0 CVMX_PKO_PEB_ECC_DBE_STS_CMB0_FUNC()
+static inline uint64_t CVMX_PKO_PEB_ECC_DBE_STS_CMB0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PEB_ECC_DBE_STS_CMB0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400009FFFD8ull);
+}
+#else
+#define CVMX_PKO_PEB_ECC_DBE_STS_CMB0 (CVMX_ADD_IO_SEG(0x00015400009FFFD8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PEB_ECC_SBE_STS0 CVMX_PKO_PEB_ECC_SBE_STS0_FUNC()
+static inline uint64_t CVMX_PKO_PEB_ECC_SBE_STS0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PEB_ECC_SBE_STS0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400009FFFF8ull);
+}
+#else
+#define CVMX_PKO_PEB_ECC_SBE_STS0 (CVMX_ADD_IO_SEG(0x00015400009FFFF8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PEB_ECC_SBE_STS_CMB0 CVMX_PKO_PEB_ECC_SBE_STS_CMB0_FUNC()
+static inline uint64_t CVMX_PKO_PEB_ECC_SBE_STS_CMB0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PEB_ECC_SBE_STS_CMB0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400009FFFE8ull);
+}
+#else
+#define CVMX_PKO_PEB_ECC_SBE_STS_CMB0 (CVMX_ADD_IO_SEG(0x00015400009FFFE8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PEB_ERR_INT CVMX_PKO_PEB_ERR_INT_FUNC()
+static inline uint64_t CVMX_PKO_PEB_ERR_INT_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PEB_ERR_INT not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000900C00ull);
+}
+#else
+#define CVMX_PKO_PEB_ERR_INT (CVMX_ADD_IO_SEG(0x0001540000900C00ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PEB_EXT_HDR_DEF_ERR_INFO CVMX_PKO_PEB_EXT_HDR_DEF_ERR_INFO_FUNC()
+static inline uint64_t CVMX_PKO_PEB_EXT_HDR_DEF_ERR_INFO_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PEB_EXT_HDR_DEF_ERR_INFO not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000900C08ull);
+}
+#else
+#define CVMX_PKO_PEB_EXT_HDR_DEF_ERR_INFO (CVMX_ADD_IO_SEG(0x0001540000900C08ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PEB_FCS_SOP_ERR_INFO CVMX_PKO_PEB_FCS_SOP_ERR_INFO_FUNC()
+static inline uint64_t CVMX_PKO_PEB_FCS_SOP_ERR_INFO_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PEB_FCS_SOP_ERR_INFO not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000900C18ull);
+}
+#else
+#define CVMX_PKO_PEB_FCS_SOP_ERR_INFO (CVMX_ADD_IO_SEG(0x0001540000900C18ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PEB_JUMP_DEF_ERR_INFO CVMX_PKO_PEB_JUMP_DEF_ERR_INFO_FUNC()
+static inline uint64_t CVMX_PKO_PEB_JUMP_DEF_ERR_INFO_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PEB_JUMP_DEF_ERR_INFO not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000900C10ull);
+}
+#else
+#define CVMX_PKO_PEB_JUMP_DEF_ERR_INFO (CVMX_ADD_IO_SEG(0x0001540000900C10ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PEB_MACX_CFG_WR_ERR_INFO CVMX_PKO_PEB_MACX_CFG_WR_ERR_INFO_FUNC()
+static inline uint64_t CVMX_PKO_PEB_MACX_CFG_WR_ERR_INFO_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PEB_MACX_CFG_WR_ERR_INFO not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000900C50ull);
+}
+#else
+#define CVMX_PKO_PEB_MACX_CFG_WR_ERR_INFO (CVMX_ADD_IO_SEG(0x0001540000900C50ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PEB_MAX_LINK_ERR_INFO CVMX_PKO_PEB_MAX_LINK_ERR_INFO_FUNC()
+static inline uint64_t CVMX_PKO_PEB_MAX_LINK_ERR_INFO_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PEB_MAX_LINK_ERR_INFO not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000900C48ull);
+}
+#else
+#define CVMX_PKO_PEB_MAX_LINK_ERR_INFO (CVMX_ADD_IO_SEG(0x0001540000900C48ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PEB_NCB_CFG CVMX_PKO_PEB_NCB_CFG_FUNC()
+static inline uint64_t CVMX_PKO_PEB_NCB_CFG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PEB_NCB_CFG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000900308ull);
+}
+#else
+#define CVMX_PKO_PEB_NCB_CFG (CVMX_ADD_IO_SEG(0x0001540000900308ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PEB_PAD_ERR_INFO CVMX_PKO_PEB_PAD_ERR_INFO_FUNC()
+static inline uint64_t CVMX_PKO_PEB_PAD_ERR_INFO_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PEB_PAD_ERR_INFO not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000900C28ull);
+}
+#else
+#define CVMX_PKO_PEB_PAD_ERR_INFO (CVMX_ADD_IO_SEG(0x0001540000900C28ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PEB_PSE_FIFO_ERR_INFO CVMX_PKO_PEB_PSE_FIFO_ERR_INFO_FUNC()
+static inline uint64_t CVMX_PKO_PEB_PSE_FIFO_ERR_INFO_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PEB_PSE_FIFO_ERR_INFO not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000900C20ull);
+}
+#else
+#define CVMX_PKO_PEB_PSE_FIFO_ERR_INFO (CVMX_ADD_IO_SEG(0x0001540000900C20ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PEB_SUBD_ADDR_ERR_INFO CVMX_PKO_PEB_SUBD_ADDR_ERR_INFO_FUNC()
+static inline uint64_t CVMX_PKO_PEB_SUBD_ADDR_ERR_INFO_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PEB_SUBD_ADDR_ERR_INFO not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000900C38ull);
+}
+#else
+#define CVMX_PKO_PEB_SUBD_ADDR_ERR_INFO (CVMX_ADD_IO_SEG(0x0001540000900C38ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PEB_SUBD_SIZE_ERR_INFO CVMX_PKO_PEB_SUBD_SIZE_ERR_INFO_FUNC()
+static inline uint64_t CVMX_PKO_PEB_SUBD_SIZE_ERR_INFO_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PEB_SUBD_SIZE_ERR_INFO not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000900C40ull);
+}
+#else
+#define CVMX_PKO_PEB_SUBD_SIZE_ERR_INFO (CVMX_ADD_IO_SEG(0x0001540000900C40ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PEB_TRUNC_ERR_INFO CVMX_PKO_PEB_TRUNC_ERR_INFO_FUNC()
+static inline uint64_t CVMX_PKO_PEB_TRUNC_ERR_INFO_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PEB_TRUNC_ERR_INFO not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000900C30ull);
+}
+#else
+#define CVMX_PKO_PEB_TRUNC_ERR_INFO (CVMX_ADD_IO_SEG(0x0001540000900C30ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PQA_DEBUG CVMX_PKO_PQA_DEBUG_FUNC()
+static inline uint64_t CVMX_PKO_PQA_DEBUG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PQA_DEBUG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000000128ull);
+}
+#else
+#define CVMX_PKO_PQA_DEBUG (CVMX_ADD_IO_SEG(0x0001540000000128ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PQB_DEBUG CVMX_PKO_PQB_DEBUG_FUNC()
+static inline uint64_t CVMX_PKO_PQB_DEBUG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PQB_DEBUG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000000130ull);
+}
+#else
+#define CVMX_PKO_PQB_DEBUG (CVMX_ADD_IO_SEG(0x0001540000000130ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PQ_CSR_BUS_DEBUG CVMX_PKO_PQ_CSR_BUS_DEBUG_FUNC()
+static inline uint64_t CVMX_PKO_PQ_CSR_BUS_DEBUG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PQ_CSR_BUS_DEBUG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00015400000001F8ull);
+}
+#else
+#define CVMX_PKO_PQ_CSR_BUS_DEBUG (CVMX_ADD_IO_SEG(0x00015400000001F8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PQ_DEBUG_GREEN CVMX_PKO_PQ_DEBUG_GREEN_FUNC()
+static inline uint64_t CVMX_PKO_PQ_DEBUG_GREEN_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PQ_DEBUG_GREEN not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000000058ull);
+}
+#else
+#define CVMX_PKO_PQ_DEBUG_GREEN (CVMX_ADD_IO_SEG(0x0001540000000058ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PQ_DEBUG_LINKS CVMX_PKO_PQ_DEBUG_LINKS_FUNC()
+static inline uint64_t CVMX_PKO_PQ_DEBUG_LINKS_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PQ_DEBUG_LINKS not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000000068ull);
+}
+#else
+#define CVMX_PKO_PQ_DEBUG_LINKS (CVMX_ADD_IO_SEG(0x0001540000000068ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PQ_DEBUG_YELLOW CVMX_PKO_PQ_DEBUG_YELLOW_FUNC()
+static inline uint64_t CVMX_PKO_PQ_DEBUG_YELLOW_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PQ_DEBUG_YELLOW not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000000060ull);
+}
+#else
+#define CVMX_PKO_PQ_DEBUG_YELLOW (CVMX_ADD_IO_SEG(0x0001540000000060ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_DQ_BIST_STATUS CVMX_PKO_PSE_DQ_BIST_STATUS_FUNC()
+static inline uint64_t CVMX_PKO_PSE_DQ_BIST_STATUS_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_DQ_BIST_STATUS not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000300138ull);
+}
+#else
+#define CVMX_PKO_PSE_DQ_BIST_STATUS (CVMX_ADD_IO_SEG(0x0001540000300138ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_DQ_ECC_CTL0 CVMX_PKO_PSE_DQ_ECC_CTL0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_DQ_ECC_CTL0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_DQ_ECC_CTL0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000300100ull);
+}
+#else
+#define CVMX_PKO_PSE_DQ_ECC_CTL0 (CVMX_ADD_IO_SEG(0x0001540000300100ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_DQ_ECC_DBE_STS0 CVMX_PKO_PSE_DQ_ECC_DBE_STS0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_DQ_ECC_DBE_STS0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_DQ_ECC_DBE_STS0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000300118ull);
+}
+#else
+#define CVMX_PKO_PSE_DQ_ECC_DBE_STS0 (CVMX_ADD_IO_SEG(0x0001540000300118ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_DQ_ECC_DBE_STS_CMB0 CVMX_PKO_PSE_DQ_ECC_DBE_STS_CMB0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_DQ_ECC_DBE_STS_CMB0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_DQ_ECC_DBE_STS_CMB0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000300120ull);
+}
+#else
+#define CVMX_PKO_PSE_DQ_ECC_DBE_STS_CMB0 (CVMX_ADD_IO_SEG(0x0001540000300120ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_DQ_ECC_SBE_STS0 CVMX_PKO_PSE_DQ_ECC_SBE_STS0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_DQ_ECC_SBE_STS0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_DQ_ECC_SBE_STS0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000300108ull);
+}
+#else
+#define CVMX_PKO_PSE_DQ_ECC_SBE_STS0 (CVMX_ADD_IO_SEG(0x0001540000300108ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_DQ_ECC_SBE_STS_CMB0 CVMX_PKO_PSE_DQ_ECC_SBE_STS_CMB0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_DQ_ECC_SBE_STS_CMB0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_DQ_ECC_SBE_STS_CMB0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000300110ull);
+}
+#else
+#define CVMX_PKO_PSE_DQ_ECC_SBE_STS_CMB0 (CVMX_ADD_IO_SEG(0x0001540000300110ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_PQ_BIST_STATUS CVMX_PKO_PSE_PQ_BIST_STATUS_FUNC()
+static inline uint64_t CVMX_PKO_PSE_PQ_BIST_STATUS_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_PQ_BIST_STATUS not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000000138ull);
+}
+#else
+#define CVMX_PKO_PSE_PQ_BIST_STATUS (CVMX_ADD_IO_SEG(0x0001540000000138ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_PQ_ECC_CTL0 CVMX_PKO_PSE_PQ_ECC_CTL0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_PQ_ECC_CTL0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_PQ_ECC_CTL0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000000100ull);
+}
+#else
+#define CVMX_PKO_PSE_PQ_ECC_CTL0 (CVMX_ADD_IO_SEG(0x0001540000000100ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_PQ_ECC_DBE_STS0 CVMX_PKO_PSE_PQ_ECC_DBE_STS0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_PQ_ECC_DBE_STS0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_PQ_ECC_DBE_STS0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000000118ull);
+}
+#else
+#define CVMX_PKO_PSE_PQ_ECC_DBE_STS0 (CVMX_ADD_IO_SEG(0x0001540000000118ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_PQ_ECC_DBE_STS_CMB0 CVMX_PKO_PSE_PQ_ECC_DBE_STS_CMB0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_PQ_ECC_DBE_STS_CMB0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_PQ_ECC_DBE_STS_CMB0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000000120ull);
+}
+#else
+#define CVMX_PKO_PSE_PQ_ECC_DBE_STS_CMB0 (CVMX_ADD_IO_SEG(0x0001540000000120ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_PQ_ECC_SBE_STS0 CVMX_PKO_PSE_PQ_ECC_SBE_STS0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_PQ_ECC_SBE_STS0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_PQ_ECC_SBE_STS0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000000108ull);
+}
+#else
+#define CVMX_PKO_PSE_PQ_ECC_SBE_STS0 (CVMX_ADD_IO_SEG(0x0001540000000108ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_PQ_ECC_SBE_STS_CMB0 CVMX_PKO_PSE_PQ_ECC_SBE_STS_CMB0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_PQ_ECC_SBE_STS_CMB0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_PQ_ECC_SBE_STS_CMB0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000000110ull);
+}
+#else
+#define CVMX_PKO_PSE_PQ_ECC_SBE_STS_CMB0 (CVMX_ADD_IO_SEG(0x0001540000000110ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_SQ1_BIST_STATUS CVMX_PKO_PSE_SQ1_BIST_STATUS_FUNC()
+static inline uint64_t CVMX_PKO_PSE_SQ1_BIST_STATUS_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_SQ1_BIST_STATUS not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000080138ull);
+}
+#else
+#define CVMX_PKO_PSE_SQ1_BIST_STATUS (CVMX_ADD_IO_SEG(0x0001540000080138ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_SQ1_ECC_CTL0 CVMX_PKO_PSE_SQ1_ECC_CTL0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_SQ1_ECC_CTL0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_SQ1_ECC_CTL0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000080100ull);
+}
+#else
+#define CVMX_PKO_PSE_SQ1_ECC_CTL0 (CVMX_ADD_IO_SEG(0x0001540000080100ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_SQ1_ECC_DBE_STS0 CVMX_PKO_PSE_SQ1_ECC_DBE_STS0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_SQ1_ECC_DBE_STS0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_SQ1_ECC_DBE_STS0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000080118ull);
+}
+#else
+#define CVMX_PKO_PSE_SQ1_ECC_DBE_STS0 (CVMX_ADD_IO_SEG(0x0001540000080118ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_SQ1_ECC_DBE_STS_CMB0 CVMX_PKO_PSE_SQ1_ECC_DBE_STS_CMB0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_SQ1_ECC_DBE_STS_CMB0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_SQ1_ECC_DBE_STS_CMB0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000080120ull);
+}
+#else
+#define CVMX_PKO_PSE_SQ1_ECC_DBE_STS_CMB0 (CVMX_ADD_IO_SEG(0x0001540000080120ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_SQ1_ECC_SBE_STS0 CVMX_PKO_PSE_SQ1_ECC_SBE_STS0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_SQ1_ECC_SBE_STS0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_SQ1_ECC_SBE_STS0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000080108ull);
+}
+#else
+#define CVMX_PKO_PSE_SQ1_ECC_SBE_STS0 (CVMX_ADD_IO_SEG(0x0001540000080108ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_SQ1_ECC_SBE_STS_CMB0 CVMX_PKO_PSE_SQ1_ECC_SBE_STS_CMB0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_SQ1_ECC_SBE_STS_CMB0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_SQ1_ECC_SBE_STS_CMB0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000080110ull);
+}
+#else
+#define CVMX_PKO_PSE_SQ1_ECC_SBE_STS_CMB0 (CVMX_ADD_IO_SEG(0x0001540000080110ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_SQ2_BIST_STATUS CVMX_PKO_PSE_SQ2_BIST_STATUS_FUNC()
+static inline uint64_t CVMX_PKO_PSE_SQ2_BIST_STATUS_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_SQ2_BIST_STATUS not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000100138ull);
+}
+#else
+#define CVMX_PKO_PSE_SQ2_BIST_STATUS (CVMX_ADD_IO_SEG(0x0001540000100138ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_SQ2_ECC_CTL0 CVMX_PKO_PSE_SQ2_ECC_CTL0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_SQ2_ECC_CTL0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_SQ2_ECC_CTL0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000100100ull);
+}
+#else
+#define CVMX_PKO_PSE_SQ2_ECC_CTL0 (CVMX_ADD_IO_SEG(0x0001540000100100ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_SQ2_ECC_DBE_STS0 CVMX_PKO_PSE_SQ2_ECC_DBE_STS0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_SQ2_ECC_DBE_STS0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_SQ2_ECC_DBE_STS0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000100118ull);
+}
+#else
+#define CVMX_PKO_PSE_SQ2_ECC_DBE_STS0 (CVMX_ADD_IO_SEG(0x0001540000100118ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_SQ2_ECC_DBE_STS_CMB0 CVMX_PKO_PSE_SQ2_ECC_DBE_STS_CMB0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_SQ2_ECC_DBE_STS_CMB0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_SQ2_ECC_DBE_STS_CMB0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000100120ull);
+}
+#else
+#define CVMX_PKO_PSE_SQ2_ECC_DBE_STS_CMB0 (CVMX_ADD_IO_SEG(0x0001540000100120ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_SQ2_ECC_SBE_STS0 CVMX_PKO_PSE_SQ2_ECC_SBE_STS0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_SQ2_ECC_SBE_STS0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_SQ2_ECC_SBE_STS0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000100108ull);
+}
+#else
+#define CVMX_PKO_PSE_SQ2_ECC_SBE_STS0 (CVMX_ADD_IO_SEG(0x0001540000100108ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_SQ2_ECC_SBE_STS_CMB0 CVMX_PKO_PSE_SQ2_ECC_SBE_STS_CMB0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_SQ2_ECC_SBE_STS_CMB0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_SQ2_ECC_SBE_STS_CMB0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000100110ull);
+}
+#else
+#define CVMX_PKO_PSE_SQ2_ECC_SBE_STS_CMB0 (CVMX_ADD_IO_SEG(0x0001540000100110ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_SQ3_BIST_STATUS CVMX_PKO_PSE_SQ3_BIST_STATUS_FUNC()
+static inline uint64_t CVMX_PKO_PSE_SQ3_BIST_STATUS_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_SQ3_BIST_STATUS not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000180138ull);
+}
+#else
+#define CVMX_PKO_PSE_SQ3_BIST_STATUS (CVMX_ADD_IO_SEG(0x0001540000180138ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_SQ3_ECC_CTL0 CVMX_PKO_PSE_SQ3_ECC_CTL0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_SQ3_ECC_CTL0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_SQ3_ECC_CTL0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000180100ull);
+}
+#else
+#define CVMX_PKO_PSE_SQ3_ECC_CTL0 (CVMX_ADD_IO_SEG(0x0001540000180100ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_SQ3_ECC_DBE_STS0 CVMX_PKO_PSE_SQ3_ECC_DBE_STS0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_SQ3_ECC_DBE_STS0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_SQ3_ECC_DBE_STS0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000180118ull);
+}
+#else
+#define CVMX_PKO_PSE_SQ3_ECC_DBE_STS0 (CVMX_ADD_IO_SEG(0x0001540000180118ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_SQ3_ECC_DBE_STS_CMB0 CVMX_PKO_PSE_SQ3_ECC_DBE_STS_CMB0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_SQ3_ECC_DBE_STS_CMB0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_SQ3_ECC_DBE_STS_CMB0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000180120ull);
+}
+#else
+#define CVMX_PKO_PSE_SQ3_ECC_DBE_STS_CMB0 (CVMX_ADD_IO_SEG(0x0001540000180120ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_SQ3_ECC_SBE_STS0 CVMX_PKO_PSE_SQ3_ECC_SBE_STS0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_SQ3_ECC_SBE_STS0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_SQ3_ECC_SBE_STS0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000180108ull);
+}
+#else
+#define CVMX_PKO_PSE_SQ3_ECC_SBE_STS0 (CVMX_ADD_IO_SEG(0x0001540000180108ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_SQ3_ECC_SBE_STS_CMB0 CVMX_PKO_PSE_SQ3_ECC_SBE_STS_CMB0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_SQ3_ECC_SBE_STS_CMB0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_SQ3_ECC_SBE_STS_CMB0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000180110ull);
+}
+#else
+#define CVMX_PKO_PSE_SQ3_ECC_SBE_STS_CMB0 (CVMX_ADD_IO_SEG(0x0001540000180110ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_SQ4_BIST_STATUS CVMX_PKO_PSE_SQ4_BIST_STATUS_FUNC()
+static inline uint64_t CVMX_PKO_PSE_SQ4_BIST_STATUS_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_SQ4_BIST_STATUS not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000200138ull);
+}
+#else
+#define CVMX_PKO_PSE_SQ4_BIST_STATUS (CVMX_ADD_IO_SEG(0x0001540000200138ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_SQ4_ECC_CTL0 CVMX_PKO_PSE_SQ4_ECC_CTL0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_SQ4_ECC_CTL0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_SQ4_ECC_CTL0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000200100ull);
+}
+#else
+#define CVMX_PKO_PSE_SQ4_ECC_CTL0 (CVMX_ADD_IO_SEG(0x0001540000200100ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_SQ4_ECC_DBE_STS0 CVMX_PKO_PSE_SQ4_ECC_DBE_STS0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_SQ4_ECC_DBE_STS0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_SQ4_ECC_DBE_STS0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000200118ull);
+}
+#else
+#define CVMX_PKO_PSE_SQ4_ECC_DBE_STS0 (CVMX_ADD_IO_SEG(0x0001540000200118ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_SQ4_ECC_DBE_STS_CMB0 CVMX_PKO_PSE_SQ4_ECC_DBE_STS_CMB0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_SQ4_ECC_DBE_STS_CMB0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_SQ4_ECC_DBE_STS_CMB0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000200120ull);
+}
+#else
+#define CVMX_PKO_PSE_SQ4_ECC_DBE_STS_CMB0 (CVMX_ADD_IO_SEG(0x0001540000200120ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_SQ4_ECC_SBE_STS0 CVMX_PKO_PSE_SQ4_ECC_SBE_STS0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_SQ4_ECC_SBE_STS0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_SQ4_ECC_SBE_STS0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000200108ull);
+}
+#else
+#define CVMX_PKO_PSE_SQ4_ECC_SBE_STS0 (CVMX_ADD_IO_SEG(0x0001540000200108ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_SQ4_ECC_SBE_STS_CMB0 CVMX_PKO_PSE_SQ4_ECC_SBE_STS_CMB0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_SQ4_ECC_SBE_STS_CMB0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_SQ4_ECC_SBE_STS_CMB0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000200110ull);
+}
+#else
+#define CVMX_PKO_PSE_SQ4_ECC_SBE_STS_CMB0 (CVMX_ADD_IO_SEG(0x0001540000200110ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_SQ5_BIST_STATUS CVMX_PKO_PSE_SQ5_BIST_STATUS_FUNC()
+static inline uint64_t CVMX_PKO_PSE_SQ5_BIST_STATUS_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_SQ5_BIST_STATUS not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000280138ull);
+}
+#else
+#define CVMX_PKO_PSE_SQ5_BIST_STATUS (CVMX_ADD_IO_SEG(0x0001540000280138ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_SQ5_ECC_CTL0 CVMX_PKO_PSE_SQ5_ECC_CTL0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_SQ5_ECC_CTL0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_SQ5_ECC_CTL0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000280100ull);
+}
+#else
+#define CVMX_PKO_PSE_SQ5_ECC_CTL0 (CVMX_ADD_IO_SEG(0x0001540000280100ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_SQ5_ECC_DBE_STS0 CVMX_PKO_PSE_SQ5_ECC_DBE_STS0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_SQ5_ECC_DBE_STS0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_SQ5_ECC_DBE_STS0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000280118ull);
+}
+#else
+#define CVMX_PKO_PSE_SQ5_ECC_DBE_STS0 (CVMX_ADD_IO_SEG(0x0001540000280118ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_SQ5_ECC_DBE_STS_CMB0 CVMX_PKO_PSE_SQ5_ECC_DBE_STS_CMB0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_SQ5_ECC_DBE_STS_CMB0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_SQ5_ECC_DBE_STS_CMB0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000280120ull);
+}
+#else
+#define CVMX_PKO_PSE_SQ5_ECC_DBE_STS_CMB0 (CVMX_ADD_IO_SEG(0x0001540000280120ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_SQ5_ECC_SBE_STS0 CVMX_PKO_PSE_SQ5_ECC_SBE_STS0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_SQ5_ECC_SBE_STS0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_SQ5_ECC_SBE_STS0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000280108ull);
+}
+#else
+#define CVMX_PKO_PSE_SQ5_ECC_SBE_STS0 (CVMX_ADD_IO_SEG(0x0001540000280108ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PSE_SQ5_ECC_SBE_STS_CMB0 CVMX_PKO_PSE_SQ5_ECC_SBE_STS_CMB0_FUNC()
+static inline uint64_t CVMX_PKO_PSE_SQ5_ECC_SBE_STS_CMB0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PSE_SQ5_ECC_SBE_STS_CMB0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000280110ull);
+}
+#else
+#define CVMX_PKO_PSE_SQ5_ECC_SBE_STS_CMB0 (CVMX_ADD_IO_SEG(0x0001540000280110ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_PTFX_STATUS(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 27)))))
+		cvmx_warn("CVMX_PKO_PTFX_STATUS(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000900100ull) + ((offset) & 31) * 8;
+}
+#else
+#define CVMX_PKO_PTFX_STATUS(offset) (CVMX_ADD_IO_SEG(0x0001540000900100ull) + ((offset) & 31) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_PTF_IOBP_CFG CVMX_PKO_PTF_IOBP_CFG_FUNC()
+static inline uint64_t CVMX_PKO_PTF_IOBP_CFG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_PTF_IOBP_CFG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000900300ull);
+}
+#else
+#define CVMX_PKO_PTF_IOBP_CFG (CVMX_ADD_IO_SEG(0x0001540000900300ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_PTGFX_CFG(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 7)))))
+		cvmx_warn("CVMX_PKO_PTGFX_CFG(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001540000900200ull) + ((offset) & 7) * 8;
+}
+#else
+#define CVMX_PKO_PTGFX_CFG(offset) (CVMX_ADD_IO_SEG(0x0001540000900200ull) + ((offset) & 7) * 8)
+#endif
+#define CVMX_PKO_REG_BIST_RESULT (CVMX_ADD_IO_SEG(0x0001180050000080ull))
+#define CVMX_PKO_REG_CMD_BUF (CVMX_ADD_IO_SEG(0x0001180050000010ull))
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_REG_CRC_CTLX(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN38XX) && ((offset <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN58XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_PKO_REG_CRC_CTLX(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180050000028ull) + ((offset) & 1) * 8;
+}
+#else
+#define CVMX_PKO_REG_CRC_CTLX(offset) (CVMX_ADD_IO_SEG(0x0001180050000028ull) + ((offset) & 1) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_REG_CRC_ENABLE CVMX_PKO_REG_CRC_ENABLE_FUNC()
+static inline uint64_t CVMX_PKO_REG_CRC_ENABLE_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN38XX) || OCTEON_IS_MODEL(OCTEON_CN58XX)))
+		cvmx_warn("CVMX_PKO_REG_CRC_ENABLE not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180050000020ull);
+}
+#else
+#define CVMX_PKO_REG_CRC_ENABLE (CVMX_ADD_IO_SEG(0x0001180050000020ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_REG_CRC_IVX(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN38XX) && ((offset <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN58XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_PKO_REG_CRC_IVX(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180050000038ull) + ((offset) & 1) * 8;
+}
+#else
+#define CVMX_PKO_REG_CRC_IVX(offset) (CVMX_ADD_IO_SEG(0x0001180050000038ull) + ((offset) & 1) * 8)
+#endif
+#define CVMX_PKO_REG_DEBUG0 (CVMX_ADD_IO_SEG(0x0001180050000098ull))
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_REG_DEBUG1 CVMX_PKO_REG_DEBUG1_FUNC()
+static inline uint64_t CVMX_PKO_REG_DEBUG1_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_PKO_REG_DEBUG1 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00011800500000A0ull);
+}
+#else
+#define CVMX_PKO_REG_DEBUG1 (CVMX_ADD_IO_SEG(0x00011800500000A0ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_REG_DEBUG2 CVMX_PKO_REG_DEBUG2_FUNC()
+static inline uint64_t CVMX_PKO_REG_DEBUG2_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_PKO_REG_DEBUG2 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00011800500000A8ull);
+}
+#else
+#define CVMX_PKO_REG_DEBUG2 (CVMX_ADD_IO_SEG(0x00011800500000A8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_REG_DEBUG3 CVMX_PKO_REG_DEBUG3_FUNC()
+static inline uint64_t CVMX_PKO_REG_DEBUG3_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_PKO_REG_DEBUG3 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00011800500000B0ull);
+}
+#else
+#define CVMX_PKO_REG_DEBUG3 (CVMX_ADD_IO_SEG(0x00011800500000B0ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_REG_DEBUG4 CVMX_PKO_REG_DEBUG4_FUNC()
+static inline uint64_t CVMX_PKO_REG_DEBUG4_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX)))
+		cvmx_warn("CVMX_PKO_REG_DEBUG4 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00011800500000B8ull);
+}
+#else
+#define CVMX_PKO_REG_DEBUG4 (CVMX_ADD_IO_SEG(0x00011800500000B8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_REG_ENGINE_INFLIGHT CVMX_PKO_REG_ENGINE_INFLIGHT_FUNC()
+static inline uint64_t CVMX_PKO_REG_ENGINE_INFLIGHT_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN52XX) || OCTEON_IS_MODEL(OCTEON_CN56XX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_PKO_REG_ENGINE_INFLIGHT not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180050000050ull);
+}
+#else
+#define CVMX_PKO_REG_ENGINE_INFLIGHT (CVMX_ADD_IO_SEG(0x0001180050000050ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_REG_ENGINE_INFLIGHT1 CVMX_PKO_REG_ENGINE_INFLIGHT1_FUNC()
+static inline uint64_t CVMX_PKO_REG_ENGINE_INFLIGHT1_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX)))
+		cvmx_warn("CVMX_PKO_REG_ENGINE_INFLIGHT1 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180050000318ull);
+}
+#else
+#define CVMX_PKO_REG_ENGINE_INFLIGHT1 (CVMX_ADD_IO_SEG(0x0001180050000318ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_PKO_REG_ENGINE_STORAGEX(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 1)))))
+		cvmx_warn("CVMX_PKO_REG_ENGINE_STORAGEX(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180050000300ull) + ((offset) & 1) * 8;
+}
+#else
+#define CVMX_PKO_REG_ENGINE_STORAGEX(offset) (CVMX_ADD_IO_SEG(0x0001180050000300ull) + ((offset) & 1) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_REG_ENGINE_THRESH CVMX_PKO_REG_ENGINE_THRESH_FUNC()
+static inline uint64_t CVMX_PKO_REG_ENGINE_THRESH_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN52XX) || OCTEON_IS_MODEL(OCTEON_CN56XX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_PKO_REG_ENGINE_THRESH not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180050000058ull);
+}
+#else
+#define CVMX_PKO_REG_ENGINE_THRESH (CVMX_ADD_IO_SEG(0x0001180050000058ull))
+#endif
+#define CVMX_PKO_REG_ERROR (CVMX_ADD_IO_SEG(0x0001180050000088ull))
+#define CVMX_PKO_REG_FLAGS (CVMX_ADD_IO_SEG(0x0001180050000000ull))
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_REG_GMX_PORT_MODE CVMX_PKO_REG_GMX_PORT_MODE_FUNC()
+static inline uint64_t CVMX_PKO_REG_GMX_PORT_MODE_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_PKO_REG_GMX_PORT_MODE not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180050000018ull);
+}
+#else
+#define CVMX_PKO_REG_GMX_PORT_MODE (CVMX_ADD_IO_SEG(0x0001180050000018ull))
+#endif
+#define CVMX_PKO_REG_INT_MASK (CVMX_ADD_IO_SEG(0x0001180050000090ull))
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_REG_LOOPBACK_BPID CVMX_PKO_REG_LOOPBACK_BPID_FUNC()
+static inline uint64_t CVMX_PKO_REG_LOOPBACK_BPID_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX)))
+		cvmx_warn("CVMX_PKO_REG_LOOPBACK_BPID not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180050000118ull);
+}
+#else
+#define CVMX_PKO_REG_LOOPBACK_BPID (CVMX_ADD_IO_SEG(0x0001180050000118ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_REG_LOOPBACK_PKIND CVMX_PKO_REG_LOOPBACK_PKIND_FUNC()
+static inline uint64_t CVMX_PKO_REG_LOOPBACK_PKIND_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX)))
+		cvmx_warn("CVMX_PKO_REG_LOOPBACK_PKIND not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180050000068ull);
+}
+#else
+#define CVMX_PKO_REG_LOOPBACK_PKIND (CVMX_ADD_IO_SEG(0x0001180050000068ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_REG_MIN_PKT CVMX_PKO_REG_MIN_PKT_FUNC()
+static inline uint64_t CVMX_PKO_REG_MIN_PKT_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX)))
+		cvmx_warn("CVMX_PKO_REG_MIN_PKT not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180050000070ull);
+}
+#else
+#define CVMX_PKO_REG_MIN_PKT (CVMX_ADD_IO_SEG(0x0001180050000070ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_REG_PREEMPT CVMX_PKO_REG_PREEMPT_FUNC()
+static inline uint64_t CVMX_PKO_REG_PREEMPT_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN52XX) || OCTEON_IS_MODEL(OCTEON_CN56XX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_PKO_REG_PREEMPT not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180050000110ull);
+}
+#else
+#define CVMX_PKO_REG_PREEMPT (CVMX_ADD_IO_SEG(0x0001180050000110ull))
+#endif
+#define CVMX_PKO_REG_QUEUE_MODE (CVMX_ADD_IO_SEG(0x0001180050000048ull))
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_REG_QUEUE_PREEMPT CVMX_PKO_REG_QUEUE_PREEMPT_FUNC()
+static inline uint64_t CVMX_PKO_REG_QUEUE_PREEMPT_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN52XX) || OCTEON_IS_MODEL(OCTEON_CN56XX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_PKO_REG_QUEUE_PREEMPT not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180050000108ull);
+}
+#else
+#define CVMX_PKO_REG_QUEUE_PREEMPT (CVMX_ADD_IO_SEG(0x0001180050000108ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_REG_QUEUE_PTRS1 CVMX_PKO_REG_QUEUE_PTRS1_FUNC()
+static inline uint64_t CVMX_PKO_REG_QUEUE_PTRS1_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_PKO_REG_QUEUE_PTRS1 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180050000100ull);
+}
+#else
+#define CVMX_PKO_REG_QUEUE_PTRS1 (CVMX_ADD_IO_SEG(0x0001180050000100ull))
+#endif
+#define CVMX_PKO_REG_READ_IDX (CVMX_ADD_IO_SEG(0x0001180050000008ull))
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_REG_THROTTLE CVMX_PKO_REG_THROTTLE_FUNC()
+static inline uint64_t CVMX_PKO_REG_THROTTLE_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX)))
+		cvmx_warn("CVMX_PKO_REG_THROTTLE not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180050000078ull);
+}
+#else
+#define CVMX_PKO_REG_THROTTLE (CVMX_ADD_IO_SEG(0x0001180050000078ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_REG_TIMESTAMP CVMX_PKO_REG_TIMESTAMP_FUNC()
+static inline uint64_t CVMX_PKO_REG_TIMESTAMP_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_PKO_REG_TIMESTAMP not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180050000060ull);
+}
+#else
+#define CVMX_PKO_REG_TIMESTAMP (CVMX_ADD_IO_SEG(0x0001180050000060ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_PKO_STATUS CVMX_PKO_STATUS_FUNC()
+static inline uint64_t CVMX_PKO_STATUS_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_PKO_STATUS not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001540000D00000ull);
+}
+#else
+#define CVMX_PKO_STATUS (CVMX_ADD_IO_SEG(0x0001540000D00000ull))
+#endif
+
+/**
+ * cvmx_pko_channel_level
+ */
+union cvmx_pko_channel_level {
+	uint64_t u64;
+	struct cvmx_pko_channel_level_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t cc_level                     : 1;  /**< Channel credit level. Channels can be configured at levels 2 or 3 of the PSE hierarchy.
+                                                         0 = Selects the level-2 as the channel level.
+                                                         1 = Selects the level-3 as the channel level. */
+#else
+	uint64_t cc_level                     : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_pko_channel_level_s       cn78xx;
+};
+typedef union cvmx_pko_channel_level cvmx_pko_channel_level_t;
+
+/**
+ * cvmx_pko_dpfi_ena
+ */
+union cvmx_pko_dpfi_ena {
+	uint64_t u64;
+	struct cvmx_pko_dpfi_ena_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t enable                       : 1;  /**< Setting this bit enables the PKO to request/return pointers to FPA. This enable must be
+                                                         set after reset so that the PKO can fill its private pointer cache prior to setting the
+                                                         PKO_RDY flag in the PKO_STATUS register. */
+#else
+	uint64_t enable                       : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_pko_dpfi_ena_s            cn78xx;
+};
+typedef union cvmx_pko_dpfi_ena cvmx_pko_dpfi_ena_t;
+
+/**
+ * cvmx_pko_dpfi_flush
+ */
+union cvmx_pko_dpfi_flush {
+	uint64_t u64;
+	struct cvmx_pko_dpfi_flush_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t flush_en                     : 1;  /**< Pointer cache flush enable. When set, this flag commands the DPFI logic to flush all valid
+                                                         pointers from the pointer cache and return them to the FPA. The flush operation is
+                                                         complete when the CACHE_FLUSHED flag in PKO_DPFI_STATUS is set. Clearing the FLUSH_EN flag
+                                                         results in the DPFI reloading its pointer cache. This flush mechanism should only be
+                                                         enabled when the PKO is quiescent and all DQs have been closed. */
+#else
+	uint64_t flush_en                     : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_pko_dpfi_flush_s          cn78xx;
+};
+typedef union cvmx_pko_dpfi_flush cvmx_pko_dpfi_flush_t;
+
+/**
+ * cvmx_pko_dpfi_fpa_aura
+ */
+union cvmx_pko_dpfi_fpa_aura {
+	uint64_t u64;
+	struct cvmx_pko_dpfi_fpa_aura_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_12_63               : 52;
+	uint64_t node                         : 2;  /**< Node number of current chip, to ensure that the aura is on the local node. */
+	uint64_t laura                        : 10; /**< Local aura to use for PKO command buffering. Must be on local OCI node.
+                                                         The FPA aura selected by LAURA must select an FPA pool whose
+                                                         FPA_POOL()_CFG[NAT_ALIGN]=1, and
+                                                         (FPA_POOL()_CFG[BUF_SIZE] - FPA_POOL()_CFG[BUF_OFFSET]) >= 4 KB/128. */
+#else
+	uint64_t laura                        : 10;
+	uint64_t node                         : 2;
+	uint64_t reserved_12_63               : 52;
+#endif
+	} s;
+	struct cvmx_pko_dpfi_fpa_aura_s       cn78xx;
+};
+typedef union cvmx_pko_dpfi_fpa_aura cvmx_pko_dpfi_fpa_aura_t;
+
+/**
+ * cvmx_pko_dpfi_status
+ */
+union cvmx_pko_dpfi_status {
+	uint64_t u64;
+	struct cvmx_pko_dpfi_status_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t ptr_cnt                      : 32; /**< The number of pointers currently in use for storing descriptors and meta-packets plus
+                                                         those available in the DPFI pointer cache. */
+	uint64_t reserved_13_31               : 19;
+	uint64_t isrd_ptr1_rtn_full           : 1;  /**< ISRD pointer return register 1 contains a valid pointer. */
+	uint64_t isrd_ptr0_rtn_full           : 1;  /**< ISRD pointer return register 0 contains a valid pointer. */
+	uint64_t isrm_ptr1_rtn_full           : 1;  /**< ISRM pointer return register 1 contains a valid pointer. */
+	uint64_t isrm_ptr0_rtn_full           : 1;  /**< ISRM pointer return register 0 contains a valid pointer. */
+	uint64_t isrd_ptr1_val                : 1;  /**< ISRD pointer register 1 contains a valid pointer. */
+	uint64_t isrd_ptr0_val                : 1;  /**< ISRD pointer register 0 contains a valid pointer. */
+	uint64_t isrm_ptr1_val                : 1;  /**< ISRM pointer register 1 contains a valid pointer. */
+	uint64_t isrm_ptr0_val                : 1;  /**< ISRM pointer register 0 contains a valid pointer. */
+	uint64_t ptr_req_pend                 : 1;  /**< DPFI has pointer requests to FPA pending. */
+	uint64_t ptr_rtn_pend                 : 1;  /**< DPFI has pointer returns to FPA pending. */
+	uint64_t fpa_empty                    : 1;  /**< 1 = FPA responded to pointer request with 'no pointers available.'
+                                                         0 = FPA is providing pointers when requested. */
+	uint64_t dpfi_empty                   : 1;  /**< DPFI pointer cache is empty. */
+	uint64_t cache_flushed                : 1;  /**< 1 = Cache flush has completed. PKO_DPFI_STATUS[PTR_CNT] will read zero if all outstanding
+                                                         pointers have been returned to the FPA.
+                                                         0 = Cache flush not enabled or in-progress. */
+#else
+	uint64_t cache_flushed                : 1;
+	uint64_t dpfi_empty                   : 1;
+	uint64_t fpa_empty                    : 1;
+	uint64_t ptr_rtn_pend                 : 1;
+	uint64_t ptr_req_pend                 : 1;
+	uint64_t isrm_ptr0_val                : 1;
+	uint64_t isrm_ptr1_val                : 1;
+	uint64_t isrd_ptr0_val                : 1;
+	uint64_t isrd_ptr1_val                : 1;
+	uint64_t isrm_ptr0_rtn_full           : 1;
+	uint64_t isrm_ptr1_rtn_full           : 1;
+	uint64_t isrd_ptr0_rtn_full           : 1;
+	uint64_t isrd_ptr1_rtn_full           : 1;
+	uint64_t reserved_13_31               : 19;
+	uint64_t ptr_cnt                      : 32;
+#endif
+	} s;
+	struct cvmx_pko_dpfi_status_s         cn78xx;
+};
+typedef union cvmx_pko_dpfi_status cvmx_pko_dpfi_status_t;
+
+/**
+ * cvmx_pko_dq#_bytes
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_GREEN_BYTES.
+ *
+ */
+union cvmx_pko_dqx_bytes {
+	uint64_t u64;
+	struct cvmx_pko_dqx_bytes_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t count                        : 48; /**< Count. The running count of bytes. Note that this count wraps. */
+#else
+	uint64_t count                        : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pko_dqx_bytes_s           cn78xx;
+};
+typedef union cvmx_pko_dqx_bytes cvmx_pko_dqx_bytes_t;
+
+/**
+ * cvmx_pko_dq#_cir
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_CIR.
+ *
+ */
+union cvmx_pko_dqx_cir {
+	uint64_t u64;
+	struct cvmx_pko_dqx_cir_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_41_63               : 23;
+	uint64_t burst_exponent               : 4;  /**< Burst exponent. The burst limit is specified as 1.BURST_MANTISSA << BURST_EXPONENT. */
+	uint64_t burst_mantissa               : 8;  /**< Burst mantissa. The burst limit is specified as 1.BURST_MANTISSA << BURST_EXPONENT. */
+	uint64_t reserved_17_28               : 12;
+	uint64_t rate_divider_exponent        : 4;  /**< Rate divider exponent. This 4-bit base-2 exponent is used to divide the credit rate by
+                                                         specifying the number of time-wheel turns required before the accumulator is increased.
+                                                         The rate count = (1 << RATE_DIVIDER_EXPONENT). The supported range for
+                                                         RATE_DIVIDER_EXPONENT is 0 to 12. Programmed values greater than 12 are treated as 12.
+                                                         Note that for the L1-SQs, a time-wheel turn is 96 clocks (SCLK). For the other levels a
+                                                         time-wheel turn is 768 clocks (SCLK).
+                                                         For L1_SQ: RATE = (SCLK_FREQUENCY / 96) * (1.RATE_MANTISSA << RATE_EXPONENT) /
+                                                         (1 <<RATE_DIVIDER_EXPONENT)
+                                                         For L[5:2]_SQ: RATE = (SCLK_FREQUENCY / 768) * (1.RATE_MANTISSA << RATE_EXPONENT) /
+                                                         (1 << RATE_DIVIDER_EXPONENT) */
+	uint64_t rate_exponent                : 4;  /**< Rate exponent. The rate is specified as 1.RATE_MANTISSA << RATE_EXPONENT. */
+	uint64_t rate_mantissa                : 8;  /**< Rate mantissa. The rate is specified as 1.RATE_MANTISSA << RATE_EXPONENT. */
+	uint64_t enable                       : 1;  /**< Enable. Enables CIR shaping. */
+#else
+	uint64_t enable                       : 1;
+	uint64_t rate_mantissa                : 8;
+	uint64_t rate_exponent                : 4;
+	uint64_t rate_divider_exponent        : 4;
+	uint64_t reserved_17_28               : 12;
+	uint64_t burst_mantissa               : 8;
+	uint64_t burst_exponent               : 4;
+	uint64_t reserved_41_63               : 23;
+#endif
+	} s;
+	struct cvmx_pko_dqx_cir_s             cn78xx;
+};
+typedef union cvmx_pko_dqx_cir cvmx_pko_dqx_cir_t;
+
+/**
+ * cvmx_pko_dq#_dropped_bytes
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_GREEN_BYTES.
+ *
+ */
+union cvmx_pko_dqx_dropped_bytes {
+	uint64_t u64;
+	struct cvmx_pko_dqx_dropped_bytes_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t count                        : 48; /**< Count. The running count of bytes. Note that this count wraps. */
+#else
+	uint64_t count                        : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pko_dqx_dropped_bytes_s   cn78xx;
+};
+typedef union cvmx_pko_dqx_dropped_bytes cvmx_pko_dqx_dropped_bytes_t;
+
+/**
+ * cvmx_pko_dq#_dropped_packets
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_GREEN_PACKETS.
+ *
+ */
+union cvmx_pko_dqx_dropped_packets {
+	uint64_t u64;
+	struct cvmx_pko_dqx_dropped_packets_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_40_63               : 24;
+	uint64_t count                        : 40; /**< Count. The running count of packets. Note that this count wraps. */
+#else
+	uint64_t count                        : 40;
+	uint64_t reserved_40_63               : 24;
+#endif
+	} s;
+	struct cvmx_pko_dqx_dropped_packets_s cn78xx;
+};
+typedef union cvmx_pko_dqx_dropped_packets cvmx_pko_dqx_dropped_packets_t;
+
+/**
+ * cvmx_pko_dq#_fifo
+ */
+union cvmx_pko_dqx_fifo {
+	uint64_t u64;
+	struct cvmx_pko_dqx_fifo_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_15_63               : 49;
+	uint64_t p_con                        : 1;  /**< Reserved. */
+	uint64_t head                         : 7;  /**< See PKO_L2_SQ()_POINTERS[PREV]. */
+	uint64_t tail                         : 7;  /**< See PKO_L2_SQ()_POINTERS[NEXT]. */
+#else
+	uint64_t tail                         : 7;
+	uint64_t head                         : 7;
+	uint64_t p_con                        : 1;
+	uint64_t reserved_15_63               : 49;
+#endif
+	} s;
+	struct cvmx_pko_dqx_fifo_s            cn78xx;
+};
+typedef union cvmx_pko_dqx_fifo cvmx_pko_dqx_fifo_t;
+
+/**
+ * cvmx_pko_dq#_packets
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_GREEN_PACKETS.
+ *
+ */
+union cvmx_pko_dqx_packets {
+	uint64_t u64;
+	struct cvmx_pko_dqx_packets_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_40_63               : 24;
+	uint64_t count                        : 40; /**< Count. The running count of packets. Note that this count wraps. */
+#else
+	uint64_t count                        : 40;
+	uint64_t reserved_40_63               : 24;
+#endif
+	} s;
+	struct cvmx_pko_dqx_packets_s         cn78xx;
+};
+typedef union cvmx_pko_dqx_packets cvmx_pko_dqx_packets_t;
+
+/**
+ * cvmx_pko_dq#_pick
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_PICK.
+ *
+ */
+union cvmx_pko_dqx_pick {
+	uint64_t u64;
+	struct cvmx_pko_dqx_pick_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t dq                           : 10; /**< Descriptor queue. Index of originating descriptor queue. */
+	uint64_t color                        : 2;  /**< See PKO_L2_SQ()_SHAPE[COLOR]. */
+	uint64_t child                        : 10; /**< Child index. When the C_CON bit of this result is set, indicating that this result is
+                                                         connected in a flow that extends through the child result, this is the index of that child
+                                                         result. */
+	uint64_t bubble                       : 1;  /**< This metapacket is a fake passed forward after a prune. */
+	uint64_t p_con                        : 1;  /**< Parent connected flag. This pick has more picks in front of it. */
+	uint64_t c_con                        : 1;  /**< Child connected flag. This pick has more picks behind it. */
+	uint64_t uid                          : 7;  /**< Unique ID. 7-bit unique value assigned at the DQ level, increments for each packet. */
+	uint64_t jump                         : 1;  /**< Jump. Set when metapacket originated from a jump descriptor. */
+	uint64_t fpd                          : 1;  /**< First packet descriptor. Set when metapacket was the first in a cacheline. */
+	uint64_t ds                           : 1;  /**< Don't send. Set when metapacket is not to be sent. */
+	uint64_t adjust                       : 9;  /**< See PKO_L2_SQ()_SHAPE[ADJUST]. */
+	uint64_t pir_dis                      : 1;  /**< PIR disable. Peak shaper disabled. */
+	uint64_t cir_dis                      : 1;  /**< CIR disable. Committed shaper disabled. */
+	uint64_t red_algo_override            : 2;  /**< See PKO_L2_SQ()_SHAPE[RED_ALGO]. */
+	uint64_t length                       : 16; /**< Packet length. The packet length in bytes. */
+#else
+	uint64_t length                       : 16;
+	uint64_t red_algo_override            : 2;
+	uint64_t cir_dis                      : 1;
+	uint64_t pir_dis                      : 1;
+	uint64_t adjust                       : 9;
+	uint64_t ds                           : 1;
+	uint64_t fpd                          : 1;
+	uint64_t jump                         : 1;
+	uint64_t uid                          : 7;
+	uint64_t c_con                        : 1;
+	uint64_t p_con                        : 1;
+	uint64_t bubble                       : 1;
+	uint64_t child                        : 10;
+	uint64_t color                        : 2;
+	uint64_t dq                           : 10;
+#endif
+	} s;
+	struct cvmx_pko_dqx_pick_s            cn78xx;
+};
+typedef union cvmx_pko_dqx_pick cvmx_pko_dqx_pick_t;
+
+/**
+ * cvmx_pko_dq#_pir
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_CIR.
+ *
+ */
+union cvmx_pko_dqx_pir {
+	uint64_t u64;
+	struct cvmx_pko_dqx_pir_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_41_63               : 23;
+	uint64_t burst_exponent               : 4;  /**< Burst exponent. The burst limit is specified as 1.BURST_MANTISSA << BURST_EXPONENT. */
+	uint64_t burst_mantissa               : 8;  /**< Burst mantissa. The burst limit is specified as 1.BURST_MANTISSA << BURST_EXPONENT. */
+	uint64_t reserved_17_28               : 12;
+	uint64_t rate_divider_exponent        : 4;  /**< Rate divider exponent. This 4-bit base-2 exponent is used to divide the credit rate by
+                                                         specifying the number of time-wheel turns required before the accumulator is increased.
+                                                         The rate count = (1 << RATE_DIVIDER_EXPONENT). The supported range for
+                                                         RATE_DIVIDER_EXPONENT is 0 to 12. Programmed values greater than 12 are treated as 12.
+                                                         Note that for the L1-SQs, a time-wheel turn is 96 clocks (SCLK). For the other levels a
+                                                         time-wheel turn is 768 clocks (SCLK).
+                                                         For L1_SQ: RATE = (SCLK_FREQUENCY / 96) * (1.RATE_MANTISSA << RATE_EXPONENT) /
+                                                         (1 <<RATE_DIVIDER_EXPONENT)
+                                                         For L[5:2]_SQ: RATE = (SCLK_FREQUENCY / 768) * (1.RATE_MANTISSA << RATE_EXPONENT) /
+                                                         (1 << RATE_DIVIDER_EXPONENT) */
+	uint64_t rate_exponent                : 4;  /**< Rate exponent. The rate is specified as 1.RATE_MANTISSA << RATE_EXPONENT. */
+	uint64_t rate_mantissa                : 8;  /**< Rate mantissa. The rate is specified as 1.RATE_MANTISSA << RATE_EXPONENT. */
+	uint64_t enable                       : 1;  /**< Enable. Enables CIR shaping. */
+#else
+	uint64_t enable                       : 1;
+	uint64_t rate_mantissa                : 8;
+	uint64_t rate_exponent                : 4;
+	uint64_t rate_divider_exponent        : 4;
+	uint64_t reserved_17_28               : 12;
+	uint64_t burst_mantissa               : 8;
+	uint64_t burst_exponent               : 4;
+	uint64_t reserved_41_63               : 23;
+#endif
+	} s;
+	struct cvmx_pko_dqx_pir_s             cn78xx;
+};
+typedef union cvmx_pko_dqx_pir cvmx_pko_dqx_pir_t;
+
+/**
+ * cvmx_pko_dq#_pointers
+ *
+ * This register has the same bit fields as PKO_L4_SQ()_POINTERS.
+ *
+ */
+union cvmx_pko_dqx_pointers {
+	uint64_t u64;
+	struct cvmx_pko_dqx_pointers_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_26_63               : 38;
+	uint64_t prev                         : 10; /**< See PKO_L2_SQ()_POINTERS[PREV]. */
+	uint64_t reserved_10_15               : 6;
+	uint64_t next                         : 10; /**< See PKO_L2_SQ()_POINTERS[NEXT]. */
+#else
+	uint64_t next                         : 10;
+	uint64_t reserved_10_15               : 6;
+	uint64_t prev                         : 10;
+	uint64_t reserved_26_63               : 38;
+#endif
+	} s;
+	struct cvmx_pko_dqx_pointers_s        cn78xx;
+};
+typedef union cvmx_pko_dqx_pointers cvmx_pko_dqx_pointers_t;
+
+/**
+ * cvmx_pko_dq#_sched_state
+ *
+ * This register has the same bit fields as PKO_L2_SQ()_SCHED_STATE.
+ *
+ */
+union cvmx_pko_dqx_sched_state {
+	uint64_t u64;
+	struct cvmx_pko_dqx_sched_state_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_25_63               : 39;
+	uint64_t rr_count                     : 25; /**< Round-robin (DWRR) deficit counter. A 25-bit signed integer count. For diagnostic use. */
+#else
+	uint64_t rr_count                     : 25;
+	uint64_t reserved_25_63               : 39;
+#endif
+	} s;
+	struct cvmx_pko_dqx_sched_state_s     cn78xx;
+};
+typedef union cvmx_pko_dqx_sched_state cvmx_pko_dqx_sched_state_t;
+
+/**
+ * cvmx_pko_dq#_schedule
+ *
+ * This register has the same bit fields as PKO_L2_SQ()_SCHEDULE.
+ *
+ */
+union cvmx_pko_dqx_schedule {
+	uint64_t u64;
+	struct cvmx_pko_dqx_schedule_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_28_63               : 36;
+	uint64_t prio                         : 4;  /**< Priority. The priority used for this SQ in the (lower-level) parent's scheduling
+                                                         algorithm. When this SQ is not used, we recommend setting PRIO to zero. The legal PRIO
+                                                         values are 0-9 when the SQ is used. In addition to priority, PRIO determines whether the
+                                                         SQ is a static queue or not: If PRIO equals PKO_*_SQn_TOPOLOGY[RR_PRIO], where
+                                                         PKO_*_TOPOLOGY[PARENT] for this SQ equals n, then this is a round-robin child queue into
+                                                         the shaper at the next level. */
+	uint64_t rr_quantum                   : 24; /**< Round-robin (DWRR) quantum. The deficit-weighted round-robin quantum (24-bit unsigned
+                                                         integer).
+                                                         The packet size used in all DWRR calculations is:
+                                                         _  (PKO_Ln_SQm_SHAPE[LENGTH_DISABLE] ? 0 : (PKO_SEND_HDR_S[TOTAL] + CALCPAD)) +
+                                                            PKO_SEND_EXT_S[SHAPECHG] +
+                                                            PKO_Ln_SQm_SHAPE[ADJUST]
+                                                         where n and m correspond to this PKO_Ln_SQm_SCHEDULE CSR. CALCPAD is zero when
+                                                         PKO_PDM_DQd_MINPAD[MINPAD] is clear or when PKO_PDM_CFG[TOTAL]>=PKO_PDM_CFG[MINLEN],
+                                                         else CALCPAD=PKO_PDM_CFG[MINLEN]-PKO_SEND_HDR_S[TOTAL], where d is the DQ the packet used.
+                                                         PKO_SEND_EXT_S[SHAPECHG] is zero when a PKO_SEND_EXT_S is not present in the send
+                                                         descriptor. */
+#else
+	uint64_t rr_quantum                   : 24;
+	uint64_t prio                         : 4;
+	uint64_t reserved_28_63               : 36;
+#endif
+	} s;
+	struct cvmx_pko_dqx_schedule_s        cn78xx;
+};
+typedef union cvmx_pko_dqx_schedule cvmx_pko_dqx_schedule_t;
+
+/**
+ * cvmx_pko_dq#_shape
+ *
+ * This register has the same bit fields as PKO_L5_SQ()_SHAPE.
+ *
+ */
+union cvmx_pko_dqx_shape {
+	uint64_t u64;
+	struct cvmx_pko_dqx_shape_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_25_63               : 39;
+	uint64_t length_disable               : 1;  /**< Length disable. Disables the use of packet lengths in shaping calculations such that only
+                                                         the value of PKO_L5_SQ()_SHAPE[ADJUST]. */
+	uint64_t reserved_13_23               : 11;
+	uint64_t yellow_disable               : 1;  /**< Disable yellow transitions. Disables green-to-yellow packet color marking transitions when set. */
+	uint64_t red_disable                  : 1;  /**< Disable red transitions. Disables green-to-red and yellow-to-red packet color marking
+                                                         transitions when set. */
+	uint64_t red_algo                     : 2;  /**< See PKO_L2_SQ()_SHAPE[RED_ALGO]. */
+	uint64_t adjust                       : 9;  /**< See PKO_L2_SQ()_SHAPE[ADJUST]. */
+#else
+	uint64_t adjust                       : 9;
+	uint64_t red_algo                     : 2;
+	uint64_t red_disable                  : 1;
+	uint64_t yellow_disable               : 1;
+	uint64_t reserved_13_23               : 11;
+	uint64_t length_disable               : 1;
+	uint64_t reserved_25_63               : 39;
+#endif
+	} s;
+	struct cvmx_pko_dqx_shape_s           cn78xx;
+};
+typedef union cvmx_pko_dqx_shape cvmx_pko_dqx_shape_t;
+
+/**
+ * cvmx_pko_dq#_shape_state
+ *
+ * This register has the same bit fields as PKO_L2_SQ()_SHAPE_STATE.
+ *
+ */
+union cvmx_pko_dqx_shape_state {
+	uint64_t u64;
+	struct cvmx_pko_dqx_shape_state_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_60_63               : 4;
+	uint64_t tw_timestamp                 : 6;  /**< Time-wheel timestamp. Debug access to the live time-wheel timestamp. */
+	uint64_t color                        : 2;  /**< Shaper color status. Debug access to the live shaper state.
+                                                         0x0 = Green - operating in 'committed' range.
+                                                         0x1 = Yellow - operating in 'excess/peak' range.
+                                                         0x2 = Red - operating in 'oversubscribed' range.
+                                                         0x3 = Reserved. */
+	uint64_t pir_accum                    : 26; /**< Peak information rate accumulator. Debug access to the live PIR accumulator. */
+	uint64_t cir_accum                    : 26; /**< Committed information rate accumulator. Debug access to the live CIR accumulator. */
+#else
+	uint64_t cir_accum                    : 26;
+	uint64_t pir_accum                    : 26;
+	uint64_t color                        : 2;
+	uint64_t tw_timestamp                 : 6;
+	uint64_t reserved_60_63               : 4;
+#endif
+	} s;
+	struct cvmx_pko_dqx_shape_state_s     cn78xx;
+};
+typedef union cvmx_pko_dqx_shape_state cvmx_pko_dqx_shape_state_t;
+
+/**
+ * cvmx_pko_dq#_sw_xoff
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_SW_XOFF.
+ *
+ */
+union cvmx_pko_dqx_sw_xoff {
+	uint64_t u64;
+	struct cvmx_pko_dqx_sw_xoff_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t drain_irq                    : 1;  /**< Drain IRQ. Enables an interrupt that fires when the drain operation has completed. */
+	uint64_t drain_null_link              : 1;  /**< "Drain null link. Conditions the drain path to drain through the null link (i.e. link
+                                                         28). As such, channel credits, HW_XOFF, and shaping are disabled on the draining path
+                                                         until the path has drained." */
+	uint64_t drain                        : 1;  /**< Drain. This control activates a drain path through the PSE that starts at this node and
+                                                         ends at the SQ1 level. The drain path is prioritized over other paths through PSE and can
+                                                         be used in combination with DRAIN_NULL_LINK and DRAIN_IRQ. */
+	uint64_t xoff                         : 1;  /**< XOFF. The PQ is disabled when XOFF is asserted. PQ is enabled when XOFF is deasserted.
+                                                         NOTE: The associated PKO_L1_SQ()_TOPOLOGY[LINK] must be configured before using this
+                                                         register field. Writing to this register field before the associated
+                                                         PKO_L1_SQ()_TOPOLOGY[LINK] value is configured can result in modifying the software
+                                                         XOFF state of the wrong SQ. */
+#else
+	uint64_t xoff                         : 1;
+	uint64_t drain                        : 1;
+	uint64_t drain_null_link              : 1;
+	uint64_t drain_irq                    : 1;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_pko_dqx_sw_xoff_s         cn78xx;
+};
+typedef union cvmx_pko_dqx_sw_xoff cvmx_pko_dqx_sw_xoff_t;
+
+/**
+ * cvmx_pko_dq#_topology
+ */
+union cvmx_pko_dqx_topology {
+	uint64_t u64;
+	struct cvmx_pko_dqx_topology_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_26_63               : 38;
+	uint64_t parent                       : 10; /**< See PKO_L2_SQ()_TOPOLOGY[PARENT]. */
+	uint64_t reserved_0_15                : 16;
+#else
+	uint64_t reserved_0_15                : 16;
+	uint64_t parent                       : 10;
+	uint64_t reserved_26_63               : 38;
+#endif
+	} s;
+	struct cvmx_pko_dqx_topology_s        cn78xx;
+};
+typedef union cvmx_pko_dqx_topology cvmx_pko_dqx_topology_t;
+
+/**
+ * cvmx_pko_dq#_wm_cnt
+ */
+union cvmx_pko_dqx_wm_cnt {
+	uint64_t u64;
+	struct cvmx_pko_dqx_wm_cnt_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t count                        : 48; /**< Watermark count. The running value of the watermark counter. This value is a count of
+                                                         bytes or packets as specified in PKO_DQ()_WM_CTL[KIND]. */
+#else
+	uint64_t count                        : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pko_dqx_wm_cnt_s          cn78xx;
+};
+typedef union cvmx_pko_dqx_wm_cnt cvmx_pko_dqx_wm_cnt_t;
+
+/**
+ * cvmx_pko_dq#_wm_ctl
+ */
+union cvmx_pko_dqx_wm_ctl {
+	uint64_t u64;
+	struct cvmx_pko_dqx_wm_ctl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_51_63               : 13;
+	uint64_t enable                       : 1;  /**< Watermark enable. */
+	uint64_t kind                         : 1;  /**< Watermark kind. The watermark logic can use a byte count or packet count. 0 = Byte count;
+                                                         1 = Packet count. */
+	uint64_t intr                         : 1;  /**< Watermark Interrupt. The interrupt bit is asserted and an interrupt message to the CIU is
+                                                         generated when the specified threshold is reached or crossed. Subsequent interrupt
+                                                         messages are only generated after this bit has been cleared. */
+	uint64_t threshold                    : 48; /**< Watermark Threshold. This threshold is compared to the watermark count of
+                                                         PKO_DQ()_WM_CNT[COUNT] and an interrupt is generated when the count reaches or
+                                                         crosses the threshold. */
+#else
+	uint64_t threshold                    : 48;
+	uint64_t intr                         : 1;
+	uint64_t kind                         : 1;
+	uint64_t enable                       : 1;
+	uint64_t reserved_51_63               : 13;
+#endif
+	} s;
+	struct cvmx_pko_dqx_wm_ctl_s          cn78xx;
+};
+typedef union cvmx_pko_dqx_wm_ctl cvmx_pko_dqx_wm_ctl_t;
+
+/**
+ * cvmx_pko_dq#_wm_ctl_w1c
+ */
+union cvmx_pko_dqx_wm_ctl_w1c {
+	uint64_t u64;
+	struct cvmx_pko_dqx_wm_ctl_w1c_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_49_63               : 15;
+	uint64_t intr                         : 1;  /**< Interrupt. The interrupt bit is asserted and an interrupt message to the CIU is generated
+                                                         when the specified threshold is crossed. Subsequent interrupt messages are only generated
+                                                         after this bit has been cleared by writing 1. Throws PKO_INTSN_E::PKO_DQ()_WM. */
+	uint64_t reserved_0_47                : 48;
+#else
+	uint64_t reserved_0_47                : 48;
+	uint64_t intr                         : 1;
+	uint64_t reserved_49_63               : 15;
+#endif
+	} s;
+	struct cvmx_pko_dqx_wm_ctl_w1c_s      cn78xx;
+};
+typedef union cvmx_pko_dqx_wm_ctl_w1c cvmx_pko_dqx_wm_ctl_w1c_t;
+
+/**
+ * cvmx_pko_dq_csr_bus_debug
+ */
+union cvmx_pko_dq_csr_bus_debug {
+	uint64_t u64;
+	struct cvmx_pko_dq_csr_bus_debug_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t csr_bus_debug                : 64; /**< -- */
+#else
+	uint64_t csr_bus_debug                : 64;
+#endif
+	} s;
+	struct cvmx_pko_dq_csr_bus_debug_s    cn78xx;
+};
+typedef union cvmx_pko_dq_csr_bus_debug cvmx_pko_dq_csr_bus_debug_t;
+
+/**
+ * cvmx_pko_dq_debug
+ */
+union cvmx_pko_dq_debug {
+	uint64_t u64;
+	struct cvmx_pko_dq_debug_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t dbg_vec                      : 64; /**< Debug Vector. */
+#else
+	uint64_t dbg_vec                      : 64;
+#endif
+	} s;
+	struct cvmx_pko_dq_debug_s            cn78xx;
+};
+typedef union cvmx_pko_dq_debug cvmx_pko_dq_debug_t;
+
+/**
+ * cvmx_pko_drain_irq
+ */
+union cvmx_pko_drain_irq {
+	uint64_t u64;
+	struct cvmx_pko_drain_irq_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t intr                         : 1;  /**< Interrupt. The interrupt bit is asserted and an interrupt message to the CIU is generated
+                                                         when the DRAIN command reaches the PQ level. Subsequent interrupt messages are only
+                                                         generated after this bit has been cleared by writing 1. Throws
+                                                         PKO_INTSN_E::PKO_PSE_PQ_DRAIN. */
+#else
+	uint64_t intr                         : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_pko_drain_irq_s           cn78xx;
+};
+typedef union cvmx_pko_drain_irq cvmx_pko_drain_irq_t;
+
+/**
+ * cvmx_pko_enable
+ */
+union cvmx_pko_enable {
+	uint64_t u64;
+	struct cvmx_pko_enable_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t enable                       : 1;  /**< Enables the PKO. */
+#else
+	uint64_t enable                       : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_pko_enable_s              cn78xx;
+};
+typedef union cvmx_pko_enable cvmx_pko_enable_t;
+
+/**
+ * cvmx_pko_format#_ctl
+ */
+union cvmx_pko_formatx_ctl {
+	uint64_t u64;
+	struct cvmx_pko_formatx_ctl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_28_63               : 36;
+	uint64_t ip4_ck                       : 1;  /**< IPv4 header checksum recalculate */
+	uint64_t offset                       : 11; /**< Bits to add to PKO_SEND_EXT_S[MARKPTR]*8 to determine where to start marking. */
+	uint64_t y_mask                       : 4;  /**< Yellow mark mask. Corresponding bits in packet's data are cleared when marking packet yellow. */
+	uint64_t y_val                        : 4;  /**< Yellow mark value. Corresponding bits in packet's data are set when marking packet yellow. */
+	uint64_t r_mask                       : 4;  /**< Red mark mask. Corresponding bits in packet's data are cleared when marking packet red. */
+	uint64_t r_val                        : 4;  /**< Red mark value. Corresponding bits in packet's data are set when marking packet red. */
+#else
+	uint64_t r_val                        : 4;
+	uint64_t r_mask                       : 4;
+	uint64_t y_val                        : 4;
+	uint64_t y_mask                       : 4;
+	uint64_t offset                       : 11;
+	uint64_t ip4_ck                       : 1;
+	uint64_t reserved_28_63               : 36;
+#endif
+	} s;
+	struct cvmx_pko_formatx_ctl_s         cn78xx;
+};
+typedef union cvmx_pko_formatx_ctl cvmx_pko_formatx_ctl_t;
+
+/**
+ * cvmx_pko_l1_sq#_cir
+ */
+union cvmx_pko_l1_sqx_cir {
+	uint64_t u64;
+	struct cvmx_pko_l1_sqx_cir_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_41_63               : 23;
+	uint64_t burst_exponent               : 4;  /**< Burst exponent. The burst limit is specified as 1.BURST_MANTISSA << BURST_EXPONENT. */
+	uint64_t burst_mantissa               : 8;  /**< Burst mantissa. The burst limit is specified as 1.BURST_MANTISSA << BURST_EXPONENT. */
+	uint64_t reserved_17_28               : 12;
+	uint64_t rate_divider_exponent        : 4;  /**< Rate divider exponent. This 4-bit base-2 exponent is used to divide the credit rate by
+                                                         specifying the number of time-wheel turns required before the accumulator is increased.
+                                                         The rate count = (1 << RATE_DIVIDER_EXPONENT). The supported range for
+                                                         RATE_DIVIDER_EXPONENT is 0 to 12. Programmed values greater than 12 are treated as 12.
+                                                         Note that for the L1-SQs, a time-wheel turn is 96 clocks (SCLK). For the other levels a
+                                                         time-wheel turn is 768 clocks (SCLK).
+                                                         For L1_SQ: RATE = (SCLK_FREQUENCY / 96) * (1.RATE_MANTISSA << RATE_EXPONENT) /
+                                                         (1 <<RATE_DIVIDER_EXPONENT)
+                                                         For L[5:2]_SQ: RATE = (SCLK_FREQUENCY / 768) * (1.RATE_MANTISSA << RATE_EXPONENT) /
+                                                         (1 << RATE_DIVIDER_EXPONENT) */
+	uint64_t rate_exponent                : 4;  /**< Rate exponent. The rate is specified as 1.RATE_MANTISSA << RATE_EXPONENT. */
+	uint64_t rate_mantissa                : 8;  /**< Rate mantissa. The rate is specified as 1.RATE_MANTISSA << RATE_EXPONENT. */
+	uint64_t enable                       : 1;  /**< Enable. Enables CIR shaping. */
+#else
+	uint64_t enable                       : 1;
+	uint64_t rate_mantissa                : 8;
+	uint64_t rate_exponent                : 4;
+	uint64_t rate_divider_exponent        : 4;
+	uint64_t reserved_17_28               : 12;
+	uint64_t burst_mantissa               : 8;
+	uint64_t burst_exponent               : 4;
+	uint64_t reserved_41_63               : 23;
+#endif
+	} s;
+	struct cvmx_pko_l1_sqx_cir_s          cn78xx;
+};
+typedef union cvmx_pko_l1_sqx_cir cvmx_pko_l1_sqx_cir_t;
+
+/**
+ * cvmx_pko_l1_sq#_dropped_bytes
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_GREEN_BYTES.
+ *
+ */
+union cvmx_pko_l1_sqx_dropped_bytes {
+	uint64_t u64;
+	struct cvmx_pko_l1_sqx_dropped_bytes_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t count                        : 48; /**< Count. The running count of bytes. Note that this count wraps. */
+#else
+	uint64_t count                        : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pko_l1_sqx_dropped_bytes_s cn78xx;
+};
+typedef union cvmx_pko_l1_sqx_dropped_bytes cvmx_pko_l1_sqx_dropped_bytes_t;
+
+/**
+ * cvmx_pko_l1_sq#_dropped_packets
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_GREEN_PACKETS.
+ *
+ */
+union cvmx_pko_l1_sqx_dropped_packets {
+	uint64_t u64;
+	struct cvmx_pko_l1_sqx_dropped_packets_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_40_63               : 24;
+	uint64_t count                        : 40; /**< Count. The running count of packets. Note that this count wraps. */
+#else
+	uint64_t count                        : 40;
+	uint64_t reserved_40_63               : 24;
+#endif
+	} s;
+	struct cvmx_pko_l1_sqx_dropped_packets_s cn78xx;
+};
+typedef union cvmx_pko_l1_sqx_dropped_packets cvmx_pko_l1_sqx_dropped_packets_t;
+
+/**
+ * cvmx_pko_l1_sq#_green
+ */
+union cvmx_pko_l1_sqx_green {
+	uint64_t u64;
+	struct cvmx_pko_l1_sqx_green_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_41_63               : 23;
+	uint64_t rr_active                    : 1;  /**< Round-robin red active. Indicates that the round-robin input is mapped to RED. */
+	uint64_t active_vec                   : 20; /**< Active vector. A 10-bit vector, ordered by priority, that indicate which inputs to this
+                                                         scheduling queue are active. For internal use only. */
+	uint64_t reserved_19_19               : 1;
+	uint64_t head                         : 9;  /**< Head pointer. The index of round-robin linked-list head. For internal use only. */
+	uint64_t reserved_9_9                 : 1;
+	uint64_t tail                         : 9;  /**< Tail pointer. The index of round-robin linked-list tail. For internal use only. */
+#else
+	uint64_t tail                         : 9;
+	uint64_t reserved_9_9                 : 1;
+	uint64_t head                         : 9;
+	uint64_t reserved_19_19               : 1;
+	uint64_t active_vec                   : 20;
+	uint64_t rr_active                    : 1;
+	uint64_t reserved_41_63               : 23;
+#endif
+	} s;
+	struct cvmx_pko_l1_sqx_green_s        cn78xx;
+};
+typedef union cvmx_pko_l1_sqx_green cvmx_pko_l1_sqx_green_t;
+
+/**
+ * cvmx_pko_l1_sq#_green_bytes
+ */
+union cvmx_pko_l1_sqx_green_bytes {
+	uint64_t u64;
+	struct cvmx_pko_l1_sqx_green_bytes_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t count                        : 48; /**< Count. The running count of bytes. Note that this count wraps. */
+#else
+	uint64_t count                        : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pko_l1_sqx_green_bytes_s  cn78xx;
+};
+typedef union cvmx_pko_l1_sqx_green_bytes cvmx_pko_l1_sqx_green_bytes_t;
+
+/**
+ * cvmx_pko_l1_sq#_green_packets
+ */
+union cvmx_pko_l1_sqx_green_packets {
+	uint64_t u64;
+	struct cvmx_pko_l1_sqx_green_packets_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_40_63               : 24;
+	uint64_t count                        : 40; /**< Count. The running count of packets. Note that this count wraps. */
+#else
+	uint64_t count                        : 40;
+	uint64_t reserved_40_63               : 24;
+#endif
+	} s;
+	struct cvmx_pko_l1_sqx_green_packets_s cn78xx;
+};
+typedef union cvmx_pko_l1_sqx_green_packets cvmx_pko_l1_sqx_green_packets_t;
+
+/**
+ * cvmx_pko_l1_sq#_link
+ */
+union cvmx_pko_l1_sqx_link {
+	uint64_t u64;
+	struct cvmx_pko_l1_sqx_link_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_49_63               : 15;
+	uint64_t link                         : 5;  /**< Link index. Must match PKO_L1_SQ()_TOPOLOGY[LINK]. */
+	uint64_t reserved_32_43               : 12;
+	uint64_t cc_word_cnt                  : 20; /**< Channel credit word count. This value, plus 1 MTU, represents the maximum outstanding
+                                                         aggregate word count (words are 16 bytes) for all channels feeding into this PQ. Note that
+                                                         this 20-bit field represents a signed value that decrements towards zero as credits are
+                                                         used. Packets are not allowed to flow when the count is less than zero. As such, the most
+                                                         significant bit should normally be programmed as zero (positive count). This gives a
+                                                         maximum value for this field of 2^19 - 1. */
+	uint64_t cc_packet_cnt                : 10; /**< Channel credit packet count. This value, plus 1, represents the maximum outstanding
+                                                         aggregate packet count for all channels feeding into this PQ. Note that this 10-bit field
+                                                         represents a signed value that decrements towards zero as credits are used. Packets are
+                                                         not allowed to flow when the count is less than zero. As such the most significant bit
+                                                         should normally be programmed as zero (positive count). This gives a maximum value for
+                                                         this field of 2^9 - 1. */
+	uint64_t cc_enable                    : 1;  /**< Channel credit enable. Enables CC_WORD_CNT and CC_PACKET_CNT aggregate credit processing. */
+	uint64_t reserved_0_0                 : 1;
+#else
+	uint64_t reserved_0_0                 : 1;
+	uint64_t cc_enable                    : 1;
+	uint64_t cc_packet_cnt                : 10;
+	uint64_t cc_word_cnt                  : 20;
+	uint64_t reserved_32_43               : 12;
+	uint64_t link                         : 5;
+	uint64_t reserved_49_63               : 15;
+#endif
+	} s;
+	struct cvmx_pko_l1_sqx_link_s         cn78xx;
+};
+typedef union cvmx_pko_l1_sqx_link cvmx_pko_l1_sqx_link_t;
+
+/**
+ * cvmx_pko_l1_sq#_pick
+ */
+union cvmx_pko_l1_sqx_pick {
+	uint64_t u64;
+	struct cvmx_pko_l1_sqx_pick_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t dq                           : 10; /**< Descriptor queue. Index of originating descriptor queue. */
+	uint64_t color                        : 2;  /**< See PKO_L2_SQ()_SHAPE[COLOR]. */
+	uint64_t child                        : 10; /**< Child index. When the C_CON bit of this result is set, indicating that this result is
+                                                         connected in a flow that extends through the child result, this is the index of that child
+                                                         result. */
+	uint64_t bubble                       : 1;  /**< This metapacket is a fake passed forward after a prune. */
+	uint64_t p_con                        : 1;  /**< Parent connected flag. This pick has more picks in front of it. */
+	uint64_t c_con                        : 1;  /**< Child connected flag. This pick has more picks behind it. */
+	uint64_t uid                          : 7;  /**< Unique ID. 7-bit unique value assigned at the DQ level, increments for each packet. */
+	uint64_t jump                         : 1;  /**< Jump. Set when metapacket originated from a jump descriptor. */
+	uint64_t fpd                          : 1;  /**< First packet descriptor. Set when metapacket was the first in a cacheline. */
+	uint64_t ds                           : 1;  /**< Don't send. Set when metapacket is not to be sent. */
+	uint64_t adjust                       : 9;  /**< See PKO_L2_SQ()_SHAPE[ADJUST]. */
+	uint64_t pir_dis                      : 1;  /**< PIR disable. Peak shaper disabled. */
+	uint64_t cir_dis                      : 1;  /**< CIR disable. Committed shaper disabled. */
+	uint64_t red_algo_override            : 2;  /**< See PKO_L2_SQ()_SHAPE[RED_ALGO]. */
+	uint64_t length                       : 16; /**< Packet length. The packet length in bytes. */
+#else
+	uint64_t length                       : 16;
+	uint64_t red_algo_override            : 2;
+	uint64_t cir_dis                      : 1;
+	uint64_t pir_dis                      : 1;
+	uint64_t adjust                       : 9;
+	uint64_t ds                           : 1;
+	uint64_t fpd                          : 1;
+	uint64_t jump                         : 1;
+	uint64_t uid                          : 7;
+	uint64_t c_con                        : 1;
+	uint64_t p_con                        : 1;
+	uint64_t bubble                       : 1;
+	uint64_t child                        : 10;
+	uint64_t color                        : 2;
+	uint64_t dq                           : 10;
+#endif
+	} s;
+	struct cvmx_pko_l1_sqx_pick_s         cn78xx;
+};
+typedef union cvmx_pko_l1_sqx_pick cvmx_pko_l1_sqx_pick_t;
+
+/**
+ * cvmx_pko_l1_sq#_red
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_YELLOW.
+ *
+ */
+union cvmx_pko_l1_sqx_red {
+	uint64_t u64;
+	struct cvmx_pko_l1_sqx_red_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_19_63               : 45;
+	uint64_t head                         : 9;  /**< Head pointer. The index of round-robin linked-list head. For internal use only. */
+	uint64_t reserved_9_9                 : 1;
+	uint64_t tail                         : 9;  /**< Tail pointer. The index of round-robin linked-list tail. For internal use only. */
+#else
+	uint64_t tail                         : 9;
+	uint64_t reserved_9_9                 : 1;
+	uint64_t head                         : 9;
+	uint64_t reserved_19_63               : 45;
+#endif
+	} s;
+	struct cvmx_pko_l1_sqx_red_s          cn78xx;
+};
+typedef union cvmx_pko_l1_sqx_red cvmx_pko_l1_sqx_red_t;
+
+/**
+ * cvmx_pko_l1_sq#_red_bytes
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_GREEN_BYTES.
+ *
+ */
+union cvmx_pko_l1_sqx_red_bytes {
+	uint64_t u64;
+	struct cvmx_pko_l1_sqx_red_bytes_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t count                        : 48; /**< Count. The running count of bytes. Note that this count wraps. */
+#else
+	uint64_t count                        : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pko_l1_sqx_red_bytes_s    cn78xx;
+};
+typedef union cvmx_pko_l1_sqx_red_bytes cvmx_pko_l1_sqx_red_bytes_t;
+
+/**
+ * cvmx_pko_l1_sq#_red_packets
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_GREEN_PACKETS.
+ *
+ */
+union cvmx_pko_l1_sqx_red_packets {
+	uint64_t u64;
+	struct cvmx_pko_l1_sqx_red_packets_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_40_63               : 24;
+	uint64_t count                        : 40; /**< Count. The running count of packets. Note that this count wraps. */
+#else
+	uint64_t count                        : 40;
+	uint64_t reserved_40_63               : 24;
+#endif
+	} s;
+	struct cvmx_pko_l1_sqx_red_packets_s  cn78xx;
+};
+typedef union cvmx_pko_l1_sqx_red_packets cvmx_pko_l1_sqx_red_packets_t;
+
+/**
+ * cvmx_pko_l1_sq#_schedule
+ */
+union cvmx_pko_l1_sqx_schedule {
+	uint64_t u64;
+	struct cvmx_pko_l1_sqx_schedule_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t dummy                        : 40; /**< Reserved. */
+	uint64_t rr_quantum                   : 24; /**< Round-robin (DWRR) quantum. The deficit-weighted round-robin quantum (24-bit unsigned integer). */
+#else
+	uint64_t rr_quantum                   : 24;
+	uint64_t dummy                        : 40;
+#endif
+	} s;
+	struct cvmx_pko_l1_sqx_schedule_s     cn78xx;
+};
+typedef union cvmx_pko_l1_sqx_schedule cvmx_pko_l1_sqx_schedule_t;
+
+/**
+ * cvmx_pko_l1_sq#_shape
+ */
+union cvmx_pko_l1_sqx_shape {
+	uint64_t u64;
+	struct cvmx_pko_l1_sqx_shape_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_18_63               : 46;
+	uint64_t link                         : 5;  /**< Link index. Must match PKO_L1_SQ()_TOPOLOGY[LINK]. */
+	uint64_t reserved_0_12                : 13;
+#else
+	uint64_t reserved_0_12                : 13;
+	uint64_t link                         : 5;
+	uint64_t reserved_18_63               : 46;
+#endif
+	} s;
+	struct cvmx_pko_l1_sqx_shape_s        cn78xx;
+};
+typedef union cvmx_pko_l1_sqx_shape cvmx_pko_l1_sqx_shape_t;
+
+/**
+ * cvmx_pko_l1_sq#_shape_state
+ */
+union cvmx_pko_l1_sqx_shape_state {
+	uint64_t u64;
+	struct cvmx_pko_l1_sqx_shape_state_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_60_63               : 4;
+	uint64_t tw_timestamp                 : 6;  /**< Time-wheel timestamp. Debug access to the live time-wheel timestamp. */
+	uint64_t reserved_53_53               : 1;
+	uint64_t color                        : 1;  /**< Shaper color status. Debug access to the live shaper state.
+                                                         0 = Green - operating in 'committed' range.
+                                                         1 = Red - operating in 'oversubscribed' range or inactive. */
+	uint64_t reserved_26_51               : 26;
+	uint64_t cir_accum                    : 26; /**< Committed information rate accumulator. Debug access to the live CIR accumulator. */
+#else
+	uint64_t cir_accum                    : 26;
+	uint64_t reserved_26_51               : 26;
+	uint64_t color                        : 1;
+	uint64_t reserved_53_53               : 1;
+	uint64_t tw_timestamp                 : 6;
+	uint64_t reserved_60_63               : 4;
+#endif
+	} s;
+	struct cvmx_pko_l1_sqx_shape_state_s  cn78xx;
+};
+typedef union cvmx_pko_l1_sqx_shape_state cvmx_pko_l1_sqx_shape_state_t;
+
+/**
+ * cvmx_pko_l1_sq#_sw_xoff
+ */
+union cvmx_pko_l1_sqx_sw_xoff {
+	uint64_t u64;
+	struct cvmx_pko_l1_sqx_sw_xoff_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t drain_irq                    : 1;  /**< Drain IRQ. Enables an interrupt that fires when the drain operation has completed. */
+	uint64_t drain_null_link              : 1;  /**< "Drain null link. Conditions the drain path to drain through the null link (i.e. link
+                                                         28). As such, channel credits, HW_XOFF, and shaping are disabled on the draining path
+                                                         until the path has drained." */
+	uint64_t drain                        : 1;  /**< Drain. This control activates a drain path through the PSE that starts at this node and
+                                                         ends at the SQ1 level. The drain path is prioritized over other paths through PSE and can
+                                                         be used in combination with DRAIN_NULL_LINK and DRAIN_IRQ. */
+	uint64_t xoff                         : 1;  /**< XOFF. The PQ is disabled when XOFF is asserted. PQ is enabled when XOFF is deasserted.
+                                                         NOTE: The associated PKO_L1_SQ()_TOPOLOGY[LINK] must be configured before using this
+                                                         register field. Writing to this register field before the associated
+                                                         PKO_L1_SQ()_TOPOLOGY[LINK] value is configured can result in modifying the software
+                                                         XOFF state of the wrong SQ. */
+#else
+	uint64_t xoff                         : 1;
+	uint64_t drain                        : 1;
+	uint64_t drain_null_link              : 1;
+	uint64_t drain_irq                    : 1;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_pko_l1_sqx_sw_xoff_s      cn78xx;
+};
+typedef union cvmx_pko_l1_sqx_sw_xoff cvmx_pko_l1_sqx_sw_xoff_t;
+
+/**
+ * cvmx_pko_l1_sq#_topology
+ */
+union cvmx_pko_l1_sqx_topology {
+	uint64_t u64;
+	struct cvmx_pko_l1_sqx_topology_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_41_63               : 23;
+	uint64_t prio_anchor                  : 9;  /**< Priority Anchor. The base index positioning the static priority child queues of this
+                                                         shaper. A higher-level queue is a child queue of this shaper when its
+                                                         PKO_*_TOPOLOGY[PARENT] selects this shaper, and it further is a static priority child
+                                                         queue when its PKO_*_SQn_SCHEDULE[PRIO] does not equal RR_PRIO. A static priority child
+                                                         queue with priority PRIO must be located at n=PRIO_ANCHOR+PRIO, where
+                                                         PRIO=PKO_*_SQn_SCHEDULE[PRIO]. There can be at most one static priority child queue at
+                                                         each priority. When there are no static priority child queues attached at any priority, or
+                                                         if this shaper isn't used, the hardware does not use PRIO_ANCHOR. In this case, we
+                                                         recommend PRIO_ANCHOR be zero. Note that there are 10 available priorities, 0 through 9,
+                                                         with priority 0 being the highest and priority 9 being the lowest. */
+	uint64_t reserved_21_31               : 11;
+	uint64_t link                         : 5;  /**< Link index. Index of the link associated with this port queue. */
+	uint64_t reserved_5_15                : 11;
+	uint64_t rr_prio                      : 4;  /**< Round-robin priority. The priority assigned to the round-robin scheduler. A higher-level
+                                                         queue is a child queue of this shaper when its PKO_*_TOPOLOGY[PARENT] selects this shaper,
+                                                         and it further is a round robin child queue when its PKO_*_SQn_SCHEDULE[PRIO] equals
+                                                         RR_PRIO. All round-robin queues attached to this shaper must have the same priority. But
+                                                         the number of round-robin child queues attached (at this priority) is limited only by the
+                                                         number of higher-level queues. When this shaper is not used, we recommend RR_PRIO be zero.
+                                                         When a shaper is used, RR_PRIO should be 0xF when there are no priorities with more than
+                                                         one child queue (i.e. when there are no round-robin child queues), and should otherwise be
+                                                         a legal priority (values 0-9). */
+	uint64_t reserved_0_0                 : 1;
+#else
+	uint64_t reserved_0_0                 : 1;
+	uint64_t rr_prio                      : 4;
+	uint64_t reserved_5_15                : 11;
+	uint64_t link                         : 5;
+	uint64_t reserved_21_31               : 11;
+	uint64_t prio_anchor                  : 9;
+	uint64_t reserved_41_63               : 23;
+#endif
+	} s;
+	struct cvmx_pko_l1_sqx_topology_s     cn78xx;
+};
+typedef union cvmx_pko_l1_sqx_topology cvmx_pko_l1_sqx_topology_t;
+
+/**
+ * cvmx_pko_l1_sq#_yellow
+ */
+union cvmx_pko_l1_sqx_yellow {
+	uint64_t u64;
+	struct cvmx_pko_l1_sqx_yellow_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_19_63               : 45;
+	uint64_t head                         : 9;  /**< Head pointer. The index of round-robin linked-list head. For internal use only. */
+	uint64_t reserved_9_9                 : 1;
+	uint64_t tail                         : 9;  /**< Tail pointer. The index of round-robin linked-list tail. For internal use only. */
+#else
+	uint64_t tail                         : 9;
+	uint64_t reserved_9_9                 : 1;
+	uint64_t head                         : 9;
+	uint64_t reserved_19_63               : 45;
+#endif
+	} s;
+	struct cvmx_pko_l1_sqx_yellow_s       cn78xx;
+};
+typedef union cvmx_pko_l1_sqx_yellow cvmx_pko_l1_sqx_yellow_t;
+
+/**
+ * cvmx_pko_l1_sq#_yellow_bytes
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_GREEN_BYTES.
+ *
+ */
+union cvmx_pko_l1_sqx_yellow_bytes {
+	uint64_t u64;
+	struct cvmx_pko_l1_sqx_yellow_bytes_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t count                        : 48; /**< Count. The running count of bytes. Note that this count wraps. */
+#else
+	uint64_t count                        : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pko_l1_sqx_yellow_bytes_s cn78xx;
+};
+typedef union cvmx_pko_l1_sqx_yellow_bytes cvmx_pko_l1_sqx_yellow_bytes_t;
+
+/**
+ * cvmx_pko_l1_sq#_yellow_packets
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_GREEN_PACKETS.
+ *
+ */
+union cvmx_pko_l1_sqx_yellow_packets {
+	uint64_t u64;
+	struct cvmx_pko_l1_sqx_yellow_packets_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_40_63               : 24;
+	uint64_t count                        : 40; /**< Count. The running count of packets. Note that this count wraps. */
+#else
+	uint64_t count                        : 40;
+	uint64_t reserved_40_63               : 24;
+#endif
+	} s;
+	struct cvmx_pko_l1_sqx_yellow_packets_s cn78xx;
+};
+typedef union cvmx_pko_l1_sqx_yellow_packets cvmx_pko_l1_sqx_yellow_packets_t;
+
+/**
+ * cvmx_pko_l1_sq_csr_bus_debug
+ */
+union cvmx_pko_l1_sq_csr_bus_debug {
+	uint64_t u64;
+	struct cvmx_pko_l1_sq_csr_bus_debug_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t csr_bus_debug                : 64; /**< -- */
+#else
+	uint64_t csr_bus_debug                : 64;
+#endif
+	} s;
+	struct cvmx_pko_l1_sq_csr_bus_debug_s cn78xx;
+};
+typedef union cvmx_pko_l1_sq_csr_bus_debug cvmx_pko_l1_sq_csr_bus_debug_t;
+
+/**
+ * cvmx_pko_l1_sqa_debug
+ *
+ * This register has the same bit fields as PKO_PQA_DEBUG.
+ *
+ */
+union cvmx_pko_l1_sqa_debug {
+	uint64_t u64;
+	struct cvmx_pko_l1_sqa_debug_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t dbg_vec                      : 64; /**< Debug Vector. */
+#else
+	uint64_t dbg_vec                      : 64;
+#endif
+	} s;
+	struct cvmx_pko_l1_sqa_debug_s        cn78xx;
+};
+typedef union cvmx_pko_l1_sqa_debug cvmx_pko_l1_sqa_debug_t;
+
+/**
+ * cvmx_pko_l1_sqb_debug
+ *
+ * This register has the same bit fields as PKO_PQA_DEBUG.
+ *
+ */
+union cvmx_pko_l1_sqb_debug {
+	uint64_t u64;
+	struct cvmx_pko_l1_sqb_debug_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t dbg_vec                      : 64; /**< Debug Vector. */
+#else
+	uint64_t dbg_vec                      : 64;
+#endif
+	} s;
+	struct cvmx_pko_l1_sqb_debug_s        cn78xx;
+};
+typedef union cvmx_pko_l1_sqb_debug cvmx_pko_l1_sqb_debug_t;
+
+/**
+ * cvmx_pko_l2_sq#_cir
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_CIR.
+ *
+ */
+union cvmx_pko_l2_sqx_cir {
+	uint64_t u64;
+	struct cvmx_pko_l2_sqx_cir_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_41_63               : 23;
+	uint64_t burst_exponent               : 4;  /**< Burst exponent. The burst limit is specified as 1.BURST_MANTISSA << BURST_EXPONENT. */
+	uint64_t burst_mantissa               : 8;  /**< Burst mantissa. The burst limit is specified as 1.BURST_MANTISSA << BURST_EXPONENT. */
+	uint64_t reserved_17_28               : 12;
+	uint64_t rate_divider_exponent        : 4;  /**< Rate divider exponent. This 4-bit base-2 exponent is used to divide the credit rate by
+                                                         specifying the number of time-wheel turns required before the accumulator is increased.
+                                                         The rate count = (1 << RATE_DIVIDER_EXPONENT). The supported range for
+                                                         RATE_DIVIDER_EXPONENT is 0 to 12. Programmed values greater than 12 are treated as 12.
+                                                         Note that for the L1-SQs, a time-wheel turn is 96 clocks (SCLK). For the other levels a
+                                                         time-wheel turn is 768 clocks (SCLK).
+                                                         For L1_SQ: RATE = (SCLK_FREQUENCY / 96) * (1.RATE_MANTISSA << RATE_EXPONENT) /
+                                                         (1 <<RATE_DIVIDER_EXPONENT)
+                                                         For L[5:2]_SQ: RATE = (SCLK_FREQUENCY / 768) * (1.RATE_MANTISSA << RATE_EXPONENT) /
+                                                         (1 << RATE_DIVIDER_EXPONENT) */
+	uint64_t rate_exponent                : 4;  /**< Rate exponent. The rate is specified as 1.RATE_MANTISSA << RATE_EXPONENT. */
+	uint64_t rate_mantissa                : 8;  /**< Rate mantissa. The rate is specified as 1.RATE_MANTISSA << RATE_EXPONENT. */
+	uint64_t enable                       : 1;  /**< Enable. Enables CIR shaping. */
+#else
+	uint64_t enable                       : 1;
+	uint64_t rate_mantissa                : 8;
+	uint64_t rate_exponent                : 4;
+	uint64_t rate_divider_exponent        : 4;
+	uint64_t reserved_17_28               : 12;
+	uint64_t burst_mantissa               : 8;
+	uint64_t burst_exponent               : 4;
+	uint64_t reserved_41_63               : 23;
+#endif
+	} s;
+	struct cvmx_pko_l2_sqx_cir_s          cn78xx;
+};
+typedef union cvmx_pko_l2_sqx_cir cvmx_pko_l2_sqx_cir_t;
+
+/**
+ * cvmx_pko_l2_sq#_green
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_GREEN.
+ *
+ */
+union cvmx_pko_l2_sqx_green {
+	uint64_t u64;
+	struct cvmx_pko_l2_sqx_green_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_41_63               : 23;
+	uint64_t rr_active                    : 1;  /**< Round-robin red active. Indicates that the round-robin input is mapped to RED. */
+	uint64_t active_vec                   : 20; /**< Active vector. A 10-bit vector, ordered by priority, that indicate which inputs to this
+                                                         scheduling queue are active. For internal use only. */
+	uint64_t reserved_19_19               : 1;
+	uint64_t head                         : 9;  /**< Head pointer. The index of round-robin linked-list head. For internal use only. */
+	uint64_t reserved_9_9                 : 1;
+	uint64_t tail                         : 9;  /**< Tail pointer. The index of round-robin linked-list tail. For internal use only. */
+#else
+	uint64_t tail                         : 9;
+	uint64_t reserved_9_9                 : 1;
+	uint64_t head                         : 9;
+	uint64_t reserved_19_19               : 1;
+	uint64_t active_vec                   : 20;
+	uint64_t rr_active                    : 1;
+	uint64_t reserved_41_63               : 23;
+#endif
+	} s;
+	struct cvmx_pko_l2_sqx_green_s        cn78xx;
+};
+typedef union cvmx_pko_l2_sqx_green cvmx_pko_l2_sqx_green_t;
+
+/**
+ * cvmx_pko_l2_sq#_pick
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_PICK.
+ *
+ */
+union cvmx_pko_l2_sqx_pick {
+	uint64_t u64;
+	struct cvmx_pko_l2_sqx_pick_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t dq                           : 10; /**< Descriptor queue. Index of originating descriptor queue. */
+	uint64_t color                        : 2;  /**< See PKO_L2_SQ()_SHAPE[COLOR]. */
+	uint64_t child                        : 10; /**< Child index. When the C_CON bit of this result is set, indicating that this result is
+                                                         connected in a flow that extends through the child result, this is the index of that child
+                                                         result. */
+	uint64_t bubble                       : 1;  /**< This metapacket is a fake passed forward after a prune. */
+	uint64_t p_con                        : 1;  /**< Parent connected flag. This pick has more picks in front of it. */
+	uint64_t c_con                        : 1;  /**< Child connected flag. This pick has more picks behind it. */
+	uint64_t uid                          : 7;  /**< Unique ID. 7-bit unique value assigned at the DQ level, increments for each packet. */
+	uint64_t jump                         : 1;  /**< Jump. Set when metapacket originated from a jump descriptor. */
+	uint64_t fpd                          : 1;  /**< First packet descriptor. Set when metapacket was the first in a cacheline. */
+	uint64_t ds                           : 1;  /**< Don't send. Set when metapacket is not to be sent. */
+	uint64_t adjust                       : 9;  /**< See PKO_L2_SQ()_SHAPE[ADJUST]. */
+	uint64_t pir_dis                      : 1;  /**< PIR disable. Peak shaper disabled. */
+	uint64_t cir_dis                      : 1;  /**< CIR disable. Committed shaper disabled. */
+	uint64_t red_algo_override            : 2;  /**< See PKO_L2_SQ()_SHAPE[RED_ALGO]. */
+	uint64_t length                       : 16; /**< Packet length. The packet length in bytes. */
+#else
+	uint64_t length                       : 16;
+	uint64_t red_algo_override            : 2;
+	uint64_t cir_dis                      : 1;
+	uint64_t pir_dis                      : 1;
+	uint64_t adjust                       : 9;
+	uint64_t ds                           : 1;
+	uint64_t fpd                          : 1;
+	uint64_t jump                         : 1;
+	uint64_t uid                          : 7;
+	uint64_t c_con                        : 1;
+	uint64_t p_con                        : 1;
+	uint64_t bubble                       : 1;
+	uint64_t child                        : 10;
+	uint64_t color                        : 2;
+	uint64_t dq                           : 10;
+#endif
+	} s;
+	struct cvmx_pko_l2_sqx_pick_s         cn78xx;
+};
+typedef union cvmx_pko_l2_sqx_pick cvmx_pko_l2_sqx_pick_t;
+
+/**
+ * cvmx_pko_l2_sq#_pir
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_CIR.
+ *
+ */
+union cvmx_pko_l2_sqx_pir {
+	uint64_t u64;
+	struct cvmx_pko_l2_sqx_pir_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_41_63               : 23;
+	uint64_t burst_exponent               : 4;  /**< Burst exponent. The burst limit is specified as 1.BURST_MANTISSA << BURST_EXPONENT. */
+	uint64_t burst_mantissa               : 8;  /**< Burst mantissa. The burst limit is specified as 1.BURST_MANTISSA << BURST_EXPONENT. */
+	uint64_t reserved_17_28               : 12;
+	uint64_t rate_divider_exponent        : 4;  /**< Rate divider exponent. This 4-bit base-2 exponent is used to divide the credit rate by
+                                                         specifying the number of time-wheel turns required before the accumulator is increased.
+                                                         The rate count = (1 << RATE_DIVIDER_EXPONENT). The supported range for
+                                                         RATE_DIVIDER_EXPONENT is 0 to 12. Programmed values greater than 12 are treated as 12.
+                                                         Note that for the L1-SQs, a time-wheel turn is 96 clocks (SCLK). For the other levels a
+                                                         time-wheel turn is 768 clocks (SCLK).
+                                                         For L1_SQ: RATE = (SCLK_FREQUENCY / 96) * (1.RATE_MANTISSA << RATE_EXPONENT) /
+                                                         (1 <<RATE_DIVIDER_EXPONENT)
+                                                         For L[5:2]_SQ: RATE = (SCLK_FREQUENCY / 768) * (1.RATE_MANTISSA << RATE_EXPONENT) /
+                                                         (1 << RATE_DIVIDER_EXPONENT) */
+	uint64_t rate_exponent                : 4;  /**< Rate exponent. The rate is specified as 1.RATE_MANTISSA << RATE_EXPONENT. */
+	uint64_t rate_mantissa                : 8;  /**< Rate mantissa. The rate is specified as 1.RATE_MANTISSA << RATE_EXPONENT. */
+	uint64_t enable                       : 1;  /**< Enable. Enables CIR shaping. */
+#else
+	uint64_t enable                       : 1;
+	uint64_t rate_mantissa                : 8;
+	uint64_t rate_exponent                : 4;
+	uint64_t rate_divider_exponent        : 4;
+	uint64_t reserved_17_28               : 12;
+	uint64_t burst_mantissa               : 8;
+	uint64_t burst_exponent               : 4;
+	uint64_t reserved_41_63               : 23;
+#endif
+	} s;
+	struct cvmx_pko_l2_sqx_pir_s          cn78xx;
+};
+typedef union cvmx_pko_l2_sqx_pir cvmx_pko_l2_sqx_pir_t;
+
+/**
+ * cvmx_pko_l2_sq#_pointers
+ */
+union cvmx_pko_l2_sqx_pointers {
+	uint64_t u64;
+	struct cvmx_pko_l2_sqx_pointers_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_25_63               : 39;
+	uint64_t prev                         : 9;  /**< Previous pointer. The linked-list previous pointer. */
+	uint64_t reserved_9_15                : 7;
+	uint64_t next                         : 9;  /**< Next pointer. The linked-list next pointer. */
+#else
+	uint64_t next                         : 9;
+	uint64_t reserved_9_15                : 7;
+	uint64_t prev                         : 9;
+	uint64_t reserved_25_63               : 39;
+#endif
+	} s;
+	struct cvmx_pko_l2_sqx_pointers_s     cn78xx;
+};
+typedef union cvmx_pko_l2_sqx_pointers cvmx_pko_l2_sqx_pointers_t;
+
+/**
+ * cvmx_pko_l2_sq#_red
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_RED.
+ *
+ */
+union cvmx_pko_l2_sqx_red {
+	uint64_t u64;
+	struct cvmx_pko_l2_sqx_red_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_19_63               : 45;
+	uint64_t head                         : 9;  /**< Head pointer. The index of round-robin linked-list head. For internal use only. */
+	uint64_t reserved_9_9                 : 1;
+	uint64_t tail                         : 9;  /**< Tail pointer. The index of round-robin linked-list tail. For internal use only. */
+#else
+	uint64_t tail                         : 9;
+	uint64_t reserved_9_9                 : 1;
+	uint64_t head                         : 9;
+	uint64_t reserved_19_63               : 45;
+#endif
+	} s;
+	struct cvmx_pko_l2_sqx_red_s          cn78xx;
+};
+typedef union cvmx_pko_l2_sqx_red cvmx_pko_l2_sqx_red_t;
+
+/**
+ * cvmx_pko_l2_sq#_sched_state
+ */
+union cvmx_pko_l2_sqx_sched_state {
+	uint64_t u64;
+	struct cvmx_pko_l2_sqx_sched_state_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_25_63               : 39;
+	uint64_t rr_count                     : 25; /**< Round-robin (DWRR) deficit counter. A 25-bit signed integer count. For diagnostic use. */
+#else
+	uint64_t rr_count                     : 25;
+	uint64_t reserved_25_63               : 39;
+#endif
+	} s;
+	struct cvmx_pko_l2_sqx_sched_state_s  cn78xx;
+};
+typedef union cvmx_pko_l2_sqx_sched_state cvmx_pko_l2_sqx_sched_state_t;
+
+/**
+ * cvmx_pko_l2_sq#_schedule
+ */
+union cvmx_pko_l2_sqx_schedule {
+	uint64_t u64;
+	struct cvmx_pko_l2_sqx_schedule_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_28_63               : 36;
+	uint64_t prio                         : 4;  /**< Priority. The priority used for this SQ in the (lower-level) parent's scheduling
+                                                         algorithm. When this SQ is not used, we recommend setting PRIO to zero. The legal PRIO
+                                                         values are 0-9 when the SQ is used. In addition to priority, PRIO determines whether the
+                                                         SQ is a static queue or not: If PRIO equals PKO_*_SQn_TOPOLOGY[RR_PRIO], where
+                                                         PKO_*_TOPOLOGY[PARENT] for this SQ equals n, then this is a round-robin child queue into
+                                                         the shaper at the next level. */
+	uint64_t rr_quantum                   : 24; /**< Round-robin (DWRR) quantum. The deficit-weighted round-robin quantum (24-bit unsigned
+                                                         integer).
+                                                         The packet size used in all DWRR calculations is:
+                                                         _  (PKO_Ln_SQm_SHAPE[LENGTH_DISABLE] ? 0 : (PKO_SEND_HDR_S[TOTAL] + CALCPAD)) +
+                                                            PKO_SEND_EXT_S[SHAPECHG] +
+                                                            PKO_Ln_SQm_SHAPE[ADJUST]
+                                                         where n and m correspond to this PKO_Ln_SQm_SCHEDULE CSR. CALCPAD is zero when
+                                                         PKO_PDM_DQd_MINPAD[MINPAD] is clear or when PKO_PDM_CFG[TOTAL]>=PKO_PDM_CFG[MINLEN],
+                                                         else CALCPAD=PKO_PDM_CFG[MINLEN]-PKO_SEND_HDR_S[TOTAL], where d is the DQ the packet used.
+                                                         PKO_SEND_EXT_S[SHAPECHG] is zero when a PKO_SEND_EXT_S is not present in the send
+                                                         descriptor. */
+#else
+	uint64_t rr_quantum                   : 24;
+	uint64_t prio                         : 4;
+	uint64_t reserved_28_63               : 36;
+#endif
+	} s;
+	struct cvmx_pko_l2_sqx_schedule_s     cn78xx;
+};
+typedef union cvmx_pko_l2_sqx_schedule cvmx_pko_l2_sqx_schedule_t;
+
+/**
+ * cvmx_pko_l2_sq#_shape
+ */
+union cvmx_pko_l2_sqx_shape {
+	uint64_t u64;
+	struct cvmx_pko_l2_sqx_shape_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_25_63               : 39;
+	uint64_t length_disable               : 1;  /**< Length disable. Disables the use of packet lengths in shaping calculations such that only
+                                                         the values of PKO_L2_SQ()_SHAPE[ADJUST] and PKO_SEND_EXT_S[SHAPECHG] are used. */
+	uint64_t reserved_13_23               : 11;
+	uint64_t yellow_disable               : 1;  /**< Disable yellow transitions. Disables green-to-yellow packet color marking transitions when set. */
+	uint64_t red_disable                  : 1;  /**< Disable red transitions. Disables green-to-red and yellow-to-red packet color marking
+                                                         transitions when set. */
+	uint64_t red_algo                     : 2;  /**< Shaper red state algorithm.
+                                                         0x0 = Stall packets while in RED state until YELLOW or GREEN state is reached (aka never
+                                                         send RED packets).
+                                                         0x1 = Send packets while in RED state.
+                                                         0x2 = Same as 0 above (stall).
+                                                         0x3 = Discard packets while in RED state (red packets are converted to drop packets). */
+	uint64_t adjust                       : 9;  /**< Shaping calculation adjustment. This 9-bit signed values allows +/- 256 bytes to be added
+                                                         to the packet length for the shaping calculations. */
+#else
+	uint64_t adjust                       : 9;
+	uint64_t red_algo                     : 2;
+	uint64_t red_disable                  : 1;
+	uint64_t yellow_disable               : 1;
+	uint64_t reserved_13_23               : 11;
+	uint64_t length_disable               : 1;
+	uint64_t reserved_25_63               : 39;
+#endif
+	} s;
+	struct cvmx_pko_l2_sqx_shape_s        cn78xx;
+};
+typedef union cvmx_pko_l2_sqx_shape cvmx_pko_l2_sqx_shape_t;
+
+/**
+ * cvmx_pko_l2_sq#_shape_state
+ */
+union cvmx_pko_l2_sqx_shape_state {
+	uint64_t u64;
+	struct cvmx_pko_l2_sqx_shape_state_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_60_63               : 4;
+	uint64_t tw_timestamp                 : 6;  /**< Time-wheel timestamp. Debug access to the live time-wheel timestamp. */
+	uint64_t color                        : 2;  /**< Shaper color status. Debug access to the live shaper state.
+                                                         0x0 = Green - operating in 'committed' range.
+                                                         0x1 = Yellow - operating in 'excess/peak' range.
+                                                         0x2 = Red - operating in 'oversubscribed' range.
+                                                         0x3 = Reserved. */
+	uint64_t pir_accum                    : 26; /**< Peak information rate accumulator. Debug access to the live PIR accumulator. */
+	uint64_t cir_accum                    : 26; /**< Committed information rate accumulator. Debug access to the live CIR accumulator. */
+#else
+	uint64_t cir_accum                    : 26;
+	uint64_t pir_accum                    : 26;
+	uint64_t color                        : 2;
+	uint64_t tw_timestamp                 : 6;
+	uint64_t reserved_60_63               : 4;
+#endif
+	} s;
+	struct cvmx_pko_l2_sqx_shape_state_s  cn78xx;
+};
+typedef union cvmx_pko_l2_sqx_shape_state cvmx_pko_l2_sqx_shape_state_t;
+
+/**
+ * cvmx_pko_l2_sq#_sw_xoff
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_SW_XOFF.
+ *
+ */
+union cvmx_pko_l2_sqx_sw_xoff {
+	uint64_t u64;
+	struct cvmx_pko_l2_sqx_sw_xoff_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t drain_irq                    : 1;  /**< Drain IRQ. Enables an interrupt that fires when the drain operation has completed. */
+	uint64_t drain_null_link              : 1;  /**< "Drain null link. Conditions the drain path to drain through the null link (i.e. link
+                                                         28). As such, channel credits, HW_XOFF, and shaping are disabled on the draining path
+                                                         until the path has drained." */
+	uint64_t drain                        : 1;  /**< Drain. This control activates a drain path through the PSE that starts at this node and
+                                                         ends at the SQ1 level. The drain path is prioritized over other paths through PSE and can
+                                                         be used in combination with DRAIN_NULL_LINK and DRAIN_IRQ. */
+	uint64_t xoff                         : 1;  /**< XOFF. The PQ is disabled when XOFF is asserted. PQ is enabled when XOFF is deasserted.
+                                                         NOTE: The associated PKO_L1_SQ()_TOPOLOGY[LINK] must be configured before using this
+                                                         register field. Writing to this register field before the associated
+                                                         PKO_L1_SQ()_TOPOLOGY[LINK] value is configured can result in modifying the software
+                                                         XOFF state of the wrong SQ. */
+#else
+	uint64_t xoff                         : 1;
+	uint64_t drain                        : 1;
+	uint64_t drain_null_link              : 1;
+	uint64_t drain_irq                    : 1;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_pko_l2_sqx_sw_xoff_s      cn78xx;
+};
+typedef union cvmx_pko_l2_sqx_sw_xoff cvmx_pko_l2_sqx_sw_xoff_t;
+
+/**
+ * cvmx_pko_l2_sq#_topology
+ */
+union cvmx_pko_l2_sqx_topology {
+	uint64_t u64;
+	struct cvmx_pko_l2_sqx_topology_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_41_63               : 23;
+	uint64_t prio_anchor                  : 9;  /**< See PKO_L1_SQ()_TOPOLOGY[PRIO_ANCHOR]. */
+	uint64_t reserved_21_31               : 11;
+	uint64_t parent                       : 5;  /**< Parent queue index. The index of the shaping element at the next lower hierarchical level
+                                                         that accepts this shaping element's outputs. Refer to the PKO_*_SQn_TOPOLOGY
+                                                         [PRIO_ANCHOR,RR_PRIO] descriptions for constraints on which child queues can attach to
+                                                         which shapers at the next lower level. When this shaper is unused, we recommend that
+                                                         PARENT be zero. */
+	uint64_t reserved_5_15                : 11;
+	uint64_t rr_prio                      : 4;  /**< See PKO_L1_SQ()_TOPOLOGY[RR_PRIO]. */
+	uint64_t reserved_0_0                 : 1;
+#else
+	uint64_t reserved_0_0                 : 1;
+	uint64_t rr_prio                      : 4;
+	uint64_t reserved_5_15                : 11;
+	uint64_t parent                       : 5;
+	uint64_t reserved_21_31               : 11;
+	uint64_t prio_anchor                  : 9;
+	uint64_t reserved_41_63               : 23;
+#endif
+	} s;
+	struct cvmx_pko_l2_sqx_topology_s     cn78xx;
+};
+typedef union cvmx_pko_l2_sqx_topology cvmx_pko_l2_sqx_topology_t;
+
+/**
+ * cvmx_pko_l2_sq#_yellow
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_YELLOW.
+ *
+ */
+union cvmx_pko_l2_sqx_yellow {
+	uint64_t u64;
+	struct cvmx_pko_l2_sqx_yellow_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_19_63               : 45;
+	uint64_t head                         : 9;  /**< Head pointer. The index of round-robin linked-list head. For internal use only. */
+	uint64_t reserved_9_9                 : 1;
+	uint64_t tail                         : 9;  /**< Tail pointer. The index of round-robin linked-list tail. For internal use only. */
+#else
+	uint64_t tail                         : 9;
+	uint64_t reserved_9_9                 : 1;
+	uint64_t head                         : 9;
+	uint64_t reserved_19_63               : 45;
+#endif
+	} s;
+	struct cvmx_pko_l2_sqx_yellow_s       cn78xx;
+};
+typedef union cvmx_pko_l2_sqx_yellow cvmx_pko_l2_sqx_yellow_t;
+
+/**
+ * cvmx_pko_l2_sq_csr_bus_debug
+ */
+union cvmx_pko_l2_sq_csr_bus_debug {
+	uint64_t u64;
+	struct cvmx_pko_l2_sq_csr_bus_debug_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t csr_bus_debug                : 64; /**< -- */
+#else
+	uint64_t csr_bus_debug                : 64;
+#endif
+	} s;
+	struct cvmx_pko_l2_sq_csr_bus_debug_s cn78xx;
+};
+typedef union cvmx_pko_l2_sq_csr_bus_debug cvmx_pko_l2_sq_csr_bus_debug_t;
+
+/**
+ * cvmx_pko_l2_sqa_debug
+ *
+ * This register has the same bit fields as PKO_PQA_DEBUG.
+ *
+ */
+union cvmx_pko_l2_sqa_debug {
+	uint64_t u64;
+	struct cvmx_pko_l2_sqa_debug_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t dbg_vec                      : 64; /**< Debug Vector. */
+#else
+	uint64_t dbg_vec                      : 64;
+#endif
+	} s;
+	struct cvmx_pko_l2_sqa_debug_s        cn78xx;
+};
+typedef union cvmx_pko_l2_sqa_debug cvmx_pko_l2_sqa_debug_t;
+
+/**
+ * cvmx_pko_l2_sqb_debug
+ *
+ * This register has the same bit fields as PKO_PQA_DEBUG.
+ *
+ */
+union cvmx_pko_l2_sqb_debug {
+	uint64_t u64;
+	struct cvmx_pko_l2_sqb_debug_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t dbg_vec                      : 64; /**< Debug Vector. */
+#else
+	uint64_t dbg_vec                      : 64;
+#endif
+	} s;
+	struct cvmx_pko_l2_sqb_debug_s        cn78xx;
+};
+typedef union cvmx_pko_l2_sqb_debug cvmx_pko_l2_sqb_debug_t;
+
+/**
+ * cvmx_pko_l3_l2_sq#_channel
+ */
+union cvmx_pko_l3_l2_sqx_channel {
+	uint64_t u64;
+	struct cvmx_pko_l3_l2_sqx_channel_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_44_63               : 20;
+	uint64_t cc_channel                   : 12; /**< Channel ID. */
+	uint64_t cc_word_cnt                  : 20; /**< Channel credit word count. This value, plus 1 MTU, represents the maximum outstanding word
+                                                         count for this channel. (Words are 16 bytes.) Note that this 20-bit field represents a
+                                                         signed value that decrements towards zero as credits are used. Packets are not allowed to
+                                                         flow when the count is less than zero. As such, the most significant bit should normally
+                                                         be programmed as zero (positive count). This gives a maximum value for this field of 2^18
+                                                         - 1. */
+	uint64_t cc_packet_cnt                : 10; /**< Channel credit packet count. This value, plus 1, represents the maximum outstanding packet
+                                                         count for this channel. Note that this 10-bit field represents a signed value that
+                                                         decrements towards zero as credits are used. Packets are not allowed to flow when the
+                                                         count is less than zero. As such the most significant bit should normally be programmed as
+                                                         zero (positive count). This gives a maximum value for this field of 2^9 - 1. */
+	uint64_t cc_enable                    : 1;  /**< Channel credit enable. Enables CC_WORD_CNT and CC_PACKET_CNT credit processing. */
+	uint64_t hw_xoff                      : 1;  /**< Hardware XOFF status. The status of hardware XON/XOFF. This is writable to get around LUT
+                                                         issues and for reconfiguration. */
+#else
+	uint64_t hw_xoff                      : 1;
+	uint64_t cc_enable                    : 1;
+	uint64_t cc_packet_cnt                : 10;
+	uint64_t cc_word_cnt                  : 20;
+	uint64_t cc_channel                   : 12;
+	uint64_t reserved_44_63               : 20;
+#endif
+	} s;
+	struct cvmx_pko_l3_l2_sqx_channel_s   cn78xx;
+};
+typedef union cvmx_pko_l3_l2_sqx_channel cvmx_pko_l3_l2_sqx_channel_t;
+
+/**
+ * cvmx_pko_l3_sq#_cir
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_CIR.
+ *
+ */
+union cvmx_pko_l3_sqx_cir {
+	uint64_t u64;
+	struct cvmx_pko_l3_sqx_cir_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_41_63               : 23;
+	uint64_t burst_exponent               : 4;  /**< Burst exponent. The burst limit is specified as 1.BURST_MANTISSA << BURST_EXPONENT. */
+	uint64_t burst_mantissa               : 8;  /**< Burst mantissa. The burst limit is specified as 1.BURST_MANTISSA << BURST_EXPONENT. */
+	uint64_t reserved_17_28               : 12;
+	uint64_t rate_divider_exponent        : 4;  /**< Rate divider exponent. This 4-bit base-2 exponent is used to divide the credit rate by
+                                                         specifying the number of time-wheel turns required before the accumulator is increased.
+                                                         The rate count = (1 << RATE_DIVIDER_EXPONENT). The supported range for
+                                                         RATE_DIVIDER_EXPONENT is 0 to 12. Programmed values greater than 12 are treated as 12.
+                                                         Note that for the L1-SQs, a time-wheel turn is 96 clocks (SCLK). For the other levels a
+                                                         time-wheel turn is 768 clocks (SCLK).
+                                                         For L1_SQ: RATE = (SCLK_FREQUENCY / 96) * (1.RATE_MANTISSA << RATE_EXPONENT) /
+                                                         (1 <<RATE_DIVIDER_EXPONENT)
+                                                         For L[5:2]_SQ: RATE = (SCLK_FREQUENCY / 768) * (1.RATE_MANTISSA << RATE_EXPONENT) /
+                                                         (1 << RATE_DIVIDER_EXPONENT) */
+	uint64_t rate_exponent                : 4;  /**< Rate exponent. The rate is specified as 1.RATE_MANTISSA << RATE_EXPONENT. */
+	uint64_t rate_mantissa                : 8;  /**< Rate mantissa. The rate is specified as 1.RATE_MANTISSA << RATE_EXPONENT. */
+	uint64_t enable                       : 1;  /**< Enable. Enables CIR shaping. */
+#else
+	uint64_t enable                       : 1;
+	uint64_t rate_mantissa                : 8;
+	uint64_t rate_exponent                : 4;
+	uint64_t rate_divider_exponent        : 4;
+	uint64_t reserved_17_28               : 12;
+	uint64_t burst_mantissa               : 8;
+	uint64_t burst_exponent               : 4;
+	uint64_t reserved_41_63               : 23;
+#endif
+	} s;
+	struct cvmx_pko_l3_sqx_cir_s          cn78xx;
+};
+typedef union cvmx_pko_l3_sqx_cir cvmx_pko_l3_sqx_cir_t;
+
+/**
+ * cvmx_pko_l3_sq#_green
+ */
+union cvmx_pko_l3_sqx_green {
+	uint64_t u64;
+	struct cvmx_pko_l3_sqx_green_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_41_63               : 23;
+	uint64_t rr_active                    : 1;  /**< Round-robin red active. Indicates that the round-robin input is mapped to RED. */
+	uint64_t active_vec                   : 20; /**< Active vector. A 10-bit vector, ordered by priority, that indicate which inputs to this
+                                                         scheduling queue are active. For internal use only. */
+	uint64_t head                         : 10; /**< Head pointer. The index of round-robin linked-list head. For internal use only. */
+	uint64_t tail                         : 10; /**< Tail pointer. The index of round-robin linked-list tail. For internal use only. */
+#else
+	uint64_t tail                         : 10;
+	uint64_t head                         : 10;
+	uint64_t active_vec                   : 20;
+	uint64_t rr_active                    : 1;
+	uint64_t reserved_41_63               : 23;
+#endif
+	} s;
+	struct cvmx_pko_l3_sqx_green_s        cn78xx;
+};
+typedef union cvmx_pko_l3_sqx_green cvmx_pko_l3_sqx_green_t;
+
+/**
+ * cvmx_pko_l3_sq#_pick
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_PICK.
+ *
+ */
+union cvmx_pko_l3_sqx_pick {
+	uint64_t u64;
+	struct cvmx_pko_l3_sqx_pick_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t dq                           : 10; /**< Descriptor queue. Index of originating descriptor queue. */
+	uint64_t color                        : 2;  /**< See PKO_L2_SQ()_SHAPE[COLOR]. */
+	uint64_t child                        : 10; /**< Child index. When the C_CON bit of this result is set, indicating that this result is
+                                                         connected in a flow that extends through the child result, this is the index of that child
+                                                         result. */
+	uint64_t bubble                       : 1;  /**< This metapacket is a fake passed forward after a prune. */
+	uint64_t p_con                        : 1;  /**< Parent connected flag. This pick has more picks in front of it. */
+	uint64_t c_con                        : 1;  /**< Child connected flag. This pick has more picks behind it. */
+	uint64_t uid                          : 7;  /**< Unique ID. 7-bit unique value assigned at the DQ level, increments for each packet. */
+	uint64_t jump                         : 1;  /**< Jump. Set when metapacket originated from a jump descriptor. */
+	uint64_t fpd                          : 1;  /**< First packet descriptor. Set when metapacket was the first in a cacheline. */
+	uint64_t ds                           : 1;  /**< Don't send. Set when metapacket is not to be sent. */
+	uint64_t adjust                       : 9;  /**< See PKO_L2_SQ()_SHAPE[ADJUST]. */
+	uint64_t pir_dis                      : 1;  /**< PIR disable. Peak shaper disabled. */
+	uint64_t cir_dis                      : 1;  /**< CIR disable. Committed shaper disabled. */
+	uint64_t red_algo_override            : 2;  /**< See PKO_L2_SQ()_SHAPE[RED_ALGO]. */
+	uint64_t length                       : 16; /**< Packet length. The packet length in bytes. */
+#else
+	uint64_t length                       : 16;
+	uint64_t red_algo_override            : 2;
+	uint64_t cir_dis                      : 1;
+	uint64_t pir_dis                      : 1;
+	uint64_t adjust                       : 9;
+	uint64_t ds                           : 1;
+	uint64_t fpd                          : 1;
+	uint64_t jump                         : 1;
+	uint64_t uid                          : 7;
+	uint64_t c_con                        : 1;
+	uint64_t p_con                        : 1;
+	uint64_t bubble                       : 1;
+	uint64_t child                        : 10;
+	uint64_t color                        : 2;
+	uint64_t dq                           : 10;
+#endif
+	} s;
+	struct cvmx_pko_l3_sqx_pick_s         cn78xx;
+};
+typedef union cvmx_pko_l3_sqx_pick cvmx_pko_l3_sqx_pick_t;
+
+/**
+ * cvmx_pko_l3_sq#_pir
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_CIR.
+ *
+ */
+union cvmx_pko_l3_sqx_pir {
+	uint64_t u64;
+	struct cvmx_pko_l3_sqx_pir_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_41_63               : 23;
+	uint64_t burst_exponent               : 4;  /**< Burst exponent. The burst limit is specified as 1.BURST_MANTISSA << BURST_EXPONENT. */
+	uint64_t burst_mantissa               : 8;  /**< Burst mantissa. The burst limit is specified as 1.BURST_MANTISSA << BURST_EXPONENT. */
+	uint64_t reserved_17_28               : 12;
+	uint64_t rate_divider_exponent        : 4;  /**< Rate divider exponent. This 4-bit base-2 exponent is used to divide the credit rate by
+                                                         specifying the number of time-wheel turns required before the accumulator is increased.
+                                                         The rate count = (1 << RATE_DIVIDER_EXPONENT). The supported range for
+                                                         RATE_DIVIDER_EXPONENT is 0 to 12. Programmed values greater than 12 are treated as 12.
+                                                         Note that for the L1-SQs, a time-wheel turn is 96 clocks (SCLK). For the other levels a
+                                                         time-wheel turn is 768 clocks (SCLK).
+                                                         For L1_SQ: RATE = (SCLK_FREQUENCY / 96) * (1.RATE_MANTISSA << RATE_EXPONENT) /
+                                                         (1 <<RATE_DIVIDER_EXPONENT)
+                                                         For L[5:2]_SQ: RATE = (SCLK_FREQUENCY / 768) * (1.RATE_MANTISSA << RATE_EXPONENT) /
+                                                         (1 << RATE_DIVIDER_EXPONENT) */
+	uint64_t rate_exponent                : 4;  /**< Rate exponent. The rate is specified as 1.RATE_MANTISSA << RATE_EXPONENT. */
+	uint64_t rate_mantissa                : 8;  /**< Rate mantissa. The rate is specified as 1.RATE_MANTISSA << RATE_EXPONENT. */
+	uint64_t enable                       : 1;  /**< Enable. Enables CIR shaping. */
+#else
+	uint64_t enable                       : 1;
+	uint64_t rate_mantissa                : 8;
+	uint64_t rate_exponent                : 4;
+	uint64_t rate_divider_exponent        : 4;
+	uint64_t reserved_17_28               : 12;
+	uint64_t burst_mantissa               : 8;
+	uint64_t burst_exponent               : 4;
+	uint64_t reserved_41_63               : 23;
+#endif
+	} s;
+	struct cvmx_pko_l3_sqx_pir_s          cn78xx;
+};
+typedef union cvmx_pko_l3_sqx_pir cvmx_pko_l3_sqx_pir_t;
+
+/**
+ * cvmx_pko_l3_sq#_pointers
+ *
+ * This register has the same bit fields as PKO_L2_SQ()_POINTERS.
+ *
+ */
+union cvmx_pko_l3_sqx_pointers {
+	uint64_t u64;
+	struct cvmx_pko_l3_sqx_pointers_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_25_63               : 39;
+	uint64_t prev                         : 9;  /**< Previous pointer. The linked-list previous pointer. */
+	uint64_t reserved_9_15                : 7;
+	uint64_t next                         : 9;  /**< Next pointer. The linked-list next pointer. */
+#else
+	uint64_t next                         : 9;
+	uint64_t reserved_9_15                : 7;
+	uint64_t prev                         : 9;
+	uint64_t reserved_25_63               : 39;
+#endif
+	} s;
+	struct cvmx_pko_l3_sqx_pointers_s     cn78xx;
+};
+typedef union cvmx_pko_l3_sqx_pointers cvmx_pko_l3_sqx_pointers_t;
+
+/**
+ * cvmx_pko_l3_sq#_red
+ *
+ * This register has the same bit fields as PKO_L3_SQ()_YELLOW.
+ *
+ */
+union cvmx_pko_l3_sqx_red {
+	uint64_t u64;
+	struct cvmx_pko_l3_sqx_red_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_20_63               : 44;
+	uint64_t head                         : 10; /**< Head pointer. The index of round-robin linked-list head. For internal use only. */
+	uint64_t tail                         : 10; /**< Tail pointer. The index of round-robin linked-list tail. For internal use only. */
+#else
+	uint64_t tail                         : 10;
+	uint64_t head                         : 10;
+	uint64_t reserved_20_63               : 44;
+#endif
+	} s;
+	struct cvmx_pko_l3_sqx_red_s          cn78xx;
+};
+typedef union cvmx_pko_l3_sqx_red cvmx_pko_l3_sqx_red_t;
+
+/**
+ * cvmx_pko_l3_sq#_sched_state
+ *
+ * This register has the same bit fields as PKO_L2_SQ()_SCHED_STATE.
+ *
+ */
+union cvmx_pko_l3_sqx_sched_state {
+	uint64_t u64;
+	struct cvmx_pko_l3_sqx_sched_state_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_25_63               : 39;
+	uint64_t rr_count                     : 25; /**< Round-robin (DWRR) deficit counter. A 25-bit signed integer count. For diagnostic use. */
+#else
+	uint64_t rr_count                     : 25;
+	uint64_t reserved_25_63               : 39;
+#endif
+	} s;
+	struct cvmx_pko_l3_sqx_sched_state_s  cn78xx;
+};
+typedef union cvmx_pko_l3_sqx_sched_state cvmx_pko_l3_sqx_sched_state_t;
+
+/**
+ * cvmx_pko_l3_sq#_schedule
+ *
+ * This register has the same bit fields as PKO_L2_SQ()_SCHEDULE.
+ *
+ */
+union cvmx_pko_l3_sqx_schedule {
+	uint64_t u64;
+	struct cvmx_pko_l3_sqx_schedule_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_28_63               : 36;
+	uint64_t prio                         : 4;  /**< Priority. The priority used for this SQ in the (lower-level) parent's scheduling
+                                                         algorithm. When this SQ is not used, we recommend setting PRIO to zero. The legal PRIO
+                                                         values are 0-9 when the SQ is used. In addition to priority, PRIO determines whether the
+                                                         SQ is a static queue or not: If PRIO equals PKO_*_SQn_TOPOLOGY[RR_PRIO], where
+                                                         PKO_*_TOPOLOGY[PARENT] for this SQ equals n, then this is a round-robin child queue into
+                                                         the shaper at the next level. */
+	uint64_t rr_quantum                   : 24; /**< Round-robin (DWRR) quantum. The deficit-weighted round-robin quantum (24-bit unsigned
+                                                         integer).
+                                                         The packet size used in all DWRR calculations is:
+                                                         _  (PKO_Ln_SQm_SHAPE[LENGTH_DISABLE] ? 0 : (PKO_SEND_HDR_S[TOTAL] + CALCPAD)) +
+                                                            PKO_SEND_EXT_S[SHAPECHG] +
+                                                            PKO_Ln_SQm_SHAPE[ADJUST]
+                                                         where n and m correspond to this PKO_Ln_SQm_SCHEDULE CSR. CALCPAD is zero when
+                                                         PKO_PDM_DQd_MINPAD[MINPAD] is clear or when PKO_PDM_CFG[TOTAL]>=PKO_PDM_CFG[MINLEN],
+                                                         else CALCPAD=PKO_PDM_CFG[MINLEN]-PKO_SEND_HDR_S[TOTAL], where d is the DQ the packet used.
+                                                         PKO_SEND_EXT_S[SHAPECHG] is zero when a PKO_SEND_EXT_S is not present in the send
+                                                         descriptor. */
+#else
+	uint64_t rr_quantum                   : 24;
+	uint64_t prio                         : 4;
+	uint64_t reserved_28_63               : 36;
+#endif
+	} s;
+	struct cvmx_pko_l3_sqx_schedule_s     cn78xx;
+};
+typedef union cvmx_pko_l3_sqx_schedule cvmx_pko_l3_sqx_schedule_t;
+
+/**
+ * cvmx_pko_l3_sq#_shape
+ */
+union cvmx_pko_l3_sqx_shape {
+	uint64_t u64;
+	struct cvmx_pko_l3_sqx_shape_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_25_63               : 39;
+	uint64_t length_disable               : 1;  /**< Length disable. Disables the use of packet lengths in shaping calculations such that only
+                                                         the value of the ADJUST field is used. */
+	uint64_t reserved_13_23               : 11;
+	uint64_t yellow_disable               : 1;  /**< Disable yellow transitions. Disables green-to-yellow packet color marking transitions when set. */
+	uint64_t red_disable                  : 1;  /**< Disable red transitions. Disables green-to-red and yellow-to-red packet color marking
+                                                         transitions when set. */
+	uint64_t red_algo                     : 2;  /**< See PKO_L2_SQ()_SHAPE[RED_ALGO]. */
+	uint64_t adjust                       : 9;  /**< See PKO_L2_SQ()_SHAPE[ADJUST]. */
+#else
+	uint64_t adjust                       : 9;
+	uint64_t red_algo                     : 2;
+	uint64_t red_disable                  : 1;
+	uint64_t yellow_disable               : 1;
+	uint64_t reserved_13_23               : 11;
+	uint64_t length_disable               : 1;
+	uint64_t reserved_25_63               : 39;
+#endif
+	} s;
+	struct cvmx_pko_l3_sqx_shape_s        cn78xx;
+};
+typedef union cvmx_pko_l3_sqx_shape cvmx_pko_l3_sqx_shape_t;
+
+/**
+ * cvmx_pko_l3_sq#_shape_state
+ *
+ * This register has the same bit fields as PKO_L2_SQ()_SHAPE_STATE.
+ *
+ */
+union cvmx_pko_l3_sqx_shape_state {
+	uint64_t u64;
+	struct cvmx_pko_l3_sqx_shape_state_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_60_63               : 4;
+	uint64_t tw_timestamp                 : 6;  /**< Time-wheel timestamp. Debug access to the live time-wheel timestamp. */
+	uint64_t color                        : 2;  /**< Shaper color status. Debug access to the live shaper state.
+                                                         0x0 = Green - operating in 'committed' range.
+                                                         0x1 = Yellow - operating in 'excess/peak' range.
+                                                         0x2 = Red - operating in 'oversubscribed' range.
+                                                         0x3 = Reserved. */
+	uint64_t pir_accum                    : 26; /**< Peak information rate accumulator. Debug access to the live PIR accumulator. */
+	uint64_t cir_accum                    : 26; /**< Committed information rate accumulator. Debug access to the live CIR accumulator. */
+#else
+	uint64_t cir_accum                    : 26;
+	uint64_t pir_accum                    : 26;
+	uint64_t color                        : 2;
+	uint64_t tw_timestamp                 : 6;
+	uint64_t reserved_60_63               : 4;
+#endif
+	} s;
+	struct cvmx_pko_l3_sqx_shape_state_s  cn78xx;
+};
+typedef union cvmx_pko_l3_sqx_shape_state cvmx_pko_l3_sqx_shape_state_t;
+
+/**
+ * cvmx_pko_l3_sq#_sw_xoff
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_SW_XOFF
+ *
+ */
+union cvmx_pko_l3_sqx_sw_xoff {
+	uint64_t u64;
+	struct cvmx_pko_l3_sqx_sw_xoff_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t drain_irq                    : 1;  /**< Drain IRQ. Enables an interrupt that fires when the drain operation has completed. */
+	uint64_t drain_null_link              : 1;  /**< "Drain null link. Conditions the drain path to drain through the null link (i.e. link
+                                                         28). As such, channel credits, HW_XOFF, and shaping are disabled on the draining path
+                                                         until the path has drained." */
+	uint64_t drain                        : 1;  /**< Drain. This control activates a drain path through the PSE that starts at this node and
+                                                         ends at the SQ1 level. The drain path is prioritized over other paths through PSE and can
+                                                         be used in combination with DRAIN_NULL_LINK and DRAIN_IRQ. */
+	uint64_t xoff                         : 1;  /**< XOFF. The PQ is disabled when XOFF is asserted. PQ is enabled when XOFF is deasserted.
+                                                         NOTE: The associated PKO_L1_SQ()_TOPOLOGY[LINK] must be configured before using this
+                                                         register field. Writing to this register field before the associated
+                                                         PKO_L1_SQ()_TOPOLOGY[LINK] value is configured can result in modifying the software
+                                                         XOFF state of the wrong SQ. */
+#else
+	uint64_t xoff                         : 1;
+	uint64_t drain                        : 1;
+	uint64_t drain_null_link              : 1;
+	uint64_t drain_irq                    : 1;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_pko_l3_sqx_sw_xoff_s      cn78xx;
+};
+typedef union cvmx_pko_l3_sqx_sw_xoff cvmx_pko_l3_sqx_sw_xoff_t;
+
+/**
+ * cvmx_pko_l3_sq#_topology
+ */
+union cvmx_pko_l3_sqx_topology {
+	uint64_t u64;
+	struct cvmx_pko_l3_sqx_topology_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_42_63               : 22;
+	uint64_t prio_anchor                  : 10; /**< See PKO_L1_SQ()_TOPOLOGY[PRIO_ANCHOR]. */
+	uint64_t reserved_25_31               : 7;
+	uint64_t parent                       : 9;  /**< See PKO_L2_SQ()_TOPOLOGY[PARENT]. */
+	uint64_t reserved_5_15                : 11;
+	uint64_t rr_prio                      : 4;  /**< See PKO_L1_SQ()_TOPOLOGY[RR_PRIO]. */
+	uint64_t reserved_0_0                 : 1;
+#else
+	uint64_t reserved_0_0                 : 1;
+	uint64_t rr_prio                      : 4;
+	uint64_t reserved_5_15                : 11;
+	uint64_t parent                       : 9;
+	uint64_t reserved_25_31               : 7;
+	uint64_t prio_anchor                  : 10;
+	uint64_t reserved_42_63               : 22;
+#endif
+	} s;
+	struct cvmx_pko_l3_sqx_topology_s     cn78xx;
+};
+typedef union cvmx_pko_l3_sqx_topology cvmx_pko_l3_sqx_topology_t;
+
+/**
+ * cvmx_pko_l3_sq#_yellow
+ */
+union cvmx_pko_l3_sqx_yellow {
+	uint64_t u64;
+	struct cvmx_pko_l3_sqx_yellow_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_20_63               : 44;
+	uint64_t head                         : 10; /**< Head pointer. The index of round-robin linked-list head. For internal use only. */
+	uint64_t tail                         : 10; /**< Tail pointer. The index of round-robin linked-list tail. For internal use only. */
+#else
+	uint64_t tail                         : 10;
+	uint64_t head                         : 10;
+	uint64_t reserved_20_63               : 44;
+#endif
+	} s;
+	struct cvmx_pko_l3_sqx_yellow_s       cn78xx;
+};
+typedef union cvmx_pko_l3_sqx_yellow cvmx_pko_l3_sqx_yellow_t;
+
+/**
+ * cvmx_pko_l3_sq_csr_bus_debug
+ */
+union cvmx_pko_l3_sq_csr_bus_debug {
+	uint64_t u64;
+	struct cvmx_pko_l3_sq_csr_bus_debug_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t csr_bus_debug                : 64; /**< -- */
+#else
+	uint64_t csr_bus_debug                : 64;
+#endif
+	} s;
+	struct cvmx_pko_l3_sq_csr_bus_debug_s cn78xx;
+};
+typedef union cvmx_pko_l3_sq_csr_bus_debug cvmx_pko_l3_sq_csr_bus_debug_t;
+
+/**
+ * cvmx_pko_l3_sqa_debug
+ *
+ * This register has the same bit fields as PKO_PQA_DEBUG.
+ *
+ */
+union cvmx_pko_l3_sqa_debug {
+	uint64_t u64;
+	struct cvmx_pko_l3_sqa_debug_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t dbg_vec                      : 64; /**< Debug Vector. */
+#else
+	uint64_t dbg_vec                      : 64;
+#endif
+	} s;
+	struct cvmx_pko_l3_sqa_debug_s        cn78xx;
+};
+typedef union cvmx_pko_l3_sqa_debug cvmx_pko_l3_sqa_debug_t;
+
+/**
+ * cvmx_pko_l3_sqb_debug
+ *
+ * This register has the same bit fields as PKO_PQA_DEBUG.
+ *
+ */
+union cvmx_pko_l3_sqb_debug {
+	uint64_t u64;
+	struct cvmx_pko_l3_sqb_debug_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t dbg_vec                      : 64; /**< Debug Vector. */
+#else
+	uint64_t dbg_vec                      : 64;
+#endif
+	} s;
+	struct cvmx_pko_l3_sqb_debug_s        cn78xx;
+};
+typedef union cvmx_pko_l3_sqb_debug cvmx_pko_l3_sqb_debug_t;
+
+/**
+ * cvmx_pko_l4_sq#_cir
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_CIR.
+ *
+ */
+union cvmx_pko_l4_sqx_cir {
+	uint64_t u64;
+	struct cvmx_pko_l4_sqx_cir_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_41_63               : 23;
+	uint64_t burst_exponent               : 4;  /**< Burst exponent. The burst limit is specified as 1.BURST_MANTISSA << BURST_EXPONENT. */
+	uint64_t burst_mantissa               : 8;  /**< Burst mantissa. The burst limit is specified as 1.BURST_MANTISSA << BURST_EXPONENT. */
+	uint64_t reserved_17_28               : 12;
+	uint64_t rate_divider_exponent        : 4;  /**< Rate divider exponent. This 4-bit base-2 exponent is used to divide the credit rate by
+                                                         specifying the number of time-wheel turns required before the accumulator is increased.
+                                                         The rate count = (1 << RATE_DIVIDER_EXPONENT). The supported range for
+                                                         RATE_DIVIDER_EXPONENT is 0 to 12. Programmed values greater than 12 are treated as 12.
+                                                         Note that for the L1-SQs, a time-wheel turn is 96 clocks (SCLK). For the other levels a
+                                                         time-wheel turn is 768 clocks (SCLK).
+                                                         For L1_SQ: RATE = (SCLK_FREQUENCY / 96) * (1.RATE_MANTISSA << RATE_EXPONENT) /
+                                                         (1 <<RATE_DIVIDER_EXPONENT)
+                                                         For L[5:2]_SQ: RATE = (SCLK_FREQUENCY / 768) * (1.RATE_MANTISSA << RATE_EXPONENT) /
+                                                         (1 << RATE_DIVIDER_EXPONENT) */
+	uint64_t rate_exponent                : 4;  /**< Rate exponent. The rate is specified as 1.RATE_MANTISSA << RATE_EXPONENT. */
+	uint64_t rate_mantissa                : 8;  /**< Rate mantissa. The rate is specified as 1.RATE_MANTISSA << RATE_EXPONENT. */
+	uint64_t enable                       : 1;  /**< Enable. Enables CIR shaping. */
+#else
+	uint64_t enable                       : 1;
+	uint64_t rate_mantissa                : 8;
+	uint64_t rate_exponent                : 4;
+	uint64_t rate_divider_exponent        : 4;
+	uint64_t reserved_17_28               : 12;
+	uint64_t burst_mantissa               : 8;
+	uint64_t burst_exponent               : 4;
+	uint64_t reserved_41_63               : 23;
+#endif
+	} s;
+	struct cvmx_pko_l4_sqx_cir_s          cn78xx;
+};
+typedef union cvmx_pko_l4_sqx_cir cvmx_pko_l4_sqx_cir_t;
+
+/**
+ * cvmx_pko_l4_sq#_green
+ *
+ * This register has the same bit fields as PKO_L3_SQ()_GREEN.
+ *
+ */
+union cvmx_pko_l4_sqx_green {
+	uint64_t u64;
+	struct cvmx_pko_l4_sqx_green_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_41_63               : 23;
+	uint64_t rr_active                    : 1;  /**< Round-robin red active. Indicates that the round-robin input is mapped to RED. */
+	uint64_t active_vec                   : 20; /**< Active vector. A 10-bit vector, ordered by priority, that indicate which inputs to this
+                                                         scheduling queue are active. For internal use only. */
+	uint64_t head                         : 10; /**< Head pointer. The index of round-robin linked-list head. For internal use only. */
+	uint64_t tail                         : 10; /**< Tail pointer. The index of round-robin linked-list tail. For internal use only. */
+#else
+	uint64_t tail                         : 10;
+	uint64_t head                         : 10;
+	uint64_t active_vec                   : 20;
+	uint64_t rr_active                    : 1;
+	uint64_t reserved_41_63               : 23;
+#endif
+	} s;
+	struct cvmx_pko_l4_sqx_green_s        cn78xx;
+};
+typedef union cvmx_pko_l4_sqx_green cvmx_pko_l4_sqx_green_t;
+
+/**
+ * cvmx_pko_l4_sq#_pick
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_PICK.
+ *
+ */
+union cvmx_pko_l4_sqx_pick {
+	uint64_t u64;
+	struct cvmx_pko_l4_sqx_pick_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t dq                           : 10; /**< Descriptor queue. Index of originating descriptor queue. */
+	uint64_t color                        : 2;  /**< See PKO_L2_SQ()_SHAPE[COLOR]. */
+	uint64_t child                        : 10; /**< Child index. When the C_CON bit of this result is set, indicating that this result is
+                                                         connected in a flow that extends through the child result, this is the index of that child
+                                                         result. */
+	uint64_t bubble                       : 1;  /**< This metapacket is a fake passed forward after a prune. */
+	uint64_t p_con                        : 1;  /**< Parent connected flag. This pick has more picks in front of it. */
+	uint64_t c_con                        : 1;  /**< Child connected flag. This pick has more picks behind it. */
+	uint64_t uid                          : 7;  /**< Unique ID. 7-bit unique value assigned at the DQ level, increments for each packet. */
+	uint64_t jump                         : 1;  /**< Jump. Set when metapacket originated from a jump descriptor. */
+	uint64_t fpd                          : 1;  /**< First packet descriptor. Set when metapacket was the first in a cacheline. */
+	uint64_t ds                           : 1;  /**< Don't send. Set when metapacket is not to be sent. */
+	uint64_t adjust                       : 9;  /**< See PKO_L2_SQ()_SHAPE[ADJUST]. */
+	uint64_t pir_dis                      : 1;  /**< PIR disable. Peak shaper disabled. */
+	uint64_t cir_dis                      : 1;  /**< CIR disable. Committed shaper disabled. */
+	uint64_t red_algo_override            : 2;  /**< See PKO_L2_SQ()_SHAPE[RED_ALGO]. */
+	uint64_t length                       : 16; /**< Packet length. The packet length in bytes. */
+#else
+	uint64_t length                       : 16;
+	uint64_t red_algo_override            : 2;
+	uint64_t cir_dis                      : 1;
+	uint64_t pir_dis                      : 1;
+	uint64_t adjust                       : 9;
+	uint64_t ds                           : 1;
+	uint64_t fpd                          : 1;
+	uint64_t jump                         : 1;
+	uint64_t uid                          : 7;
+	uint64_t c_con                        : 1;
+	uint64_t p_con                        : 1;
+	uint64_t bubble                       : 1;
+	uint64_t child                        : 10;
+	uint64_t color                        : 2;
+	uint64_t dq                           : 10;
+#endif
+	} s;
+	struct cvmx_pko_l4_sqx_pick_s         cn78xx;
+};
+typedef union cvmx_pko_l4_sqx_pick cvmx_pko_l4_sqx_pick_t;
+
+/**
+ * cvmx_pko_l4_sq#_pir
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_CIR.
+ *
+ */
+union cvmx_pko_l4_sqx_pir {
+	uint64_t u64;
+	struct cvmx_pko_l4_sqx_pir_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_41_63               : 23;
+	uint64_t burst_exponent               : 4;  /**< Burst exponent. The burst limit is specified as 1.BURST_MANTISSA << BURST_EXPONENT. */
+	uint64_t burst_mantissa               : 8;  /**< Burst mantissa. The burst limit is specified as 1.BURST_MANTISSA << BURST_EXPONENT. */
+	uint64_t reserved_17_28               : 12;
+	uint64_t rate_divider_exponent        : 4;  /**< Rate divider exponent. This 4-bit base-2 exponent is used to divide the credit rate by
+                                                         specifying the number of time-wheel turns required before the accumulator is increased.
+                                                         The rate count = (1 << RATE_DIVIDER_EXPONENT). The supported range for
+                                                         RATE_DIVIDER_EXPONENT is 0 to 12. Programmed values greater than 12 are treated as 12.
+                                                         Note that for the L1-SQs, a time-wheel turn is 96 clocks (SCLK). For the other levels a
+                                                         time-wheel turn is 768 clocks (SCLK).
+                                                         For L1_SQ: RATE = (SCLK_FREQUENCY / 96) * (1.RATE_MANTISSA << RATE_EXPONENT) /
+                                                         (1 <<RATE_DIVIDER_EXPONENT)
+                                                         For L[5:2]_SQ: RATE = (SCLK_FREQUENCY / 768) * (1.RATE_MANTISSA << RATE_EXPONENT) /
+                                                         (1 << RATE_DIVIDER_EXPONENT) */
+	uint64_t rate_exponent                : 4;  /**< Rate exponent. The rate is specified as 1.RATE_MANTISSA << RATE_EXPONENT. */
+	uint64_t rate_mantissa                : 8;  /**< Rate mantissa. The rate is specified as 1.RATE_MANTISSA << RATE_EXPONENT. */
+	uint64_t enable                       : 1;  /**< Enable. Enables CIR shaping. */
+#else
+	uint64_t enable                       : 1;
+	uint64_t rate_mantissa                : 8;
+	uint64_t rate_exponent                : 4;
+	uint64_t rate_divider_exponent        : 4;
+	uint64_t reserved_17_28               : 12;
+	uint64_t burst_mantissa               : 8;
+	uint64_t burst_exponent               : 4;
+	uint64_t reserved_41_63               : 23;
+#endif
+	} s;
+	struct cvmx_pko_l4_sqx_pir_s          cn78xx;
+};
+typedef union cvmx_pko_l4_sqx_pir cvmx_pko_l4_sqx_pir_t;
+
+/**
+ * cvmx_pko_l4_sq#_pointers
+ */
+union cvmx_pko_l4_sqx_pointers {
+	uint64_t u64;
+	struct cvmx_pko_l4_sqx_pointers_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_26_63               : 38;
+	uint64_t prev                         : 10; /**< See PKO_L2_SQ()_POINTERS[PREV]. */
+	uint64_t reserved_10_15               : 6;
+	uint64_t next                         : 10; /**< See PKO_L2_SQ()_POINTERS[NEXT]. */
+#else
+	uint64_t next                         : 10;
+	uint64_t reserved_10_15               : 6;
+	uint64_t prev                         : 10;
+	uint64_t reserved_26_63               : 38;
+#endif
+	} s;
+	struct cvmx_pko_l4_sqx_pointers_s     cn78xx;
+};
+typedef union cvmx_pko_l4_sqx_pointers cvmx_pko_l4_sqx_pointers_t;
+
+/**
+ * cvmx_pko_l4_sq#_red
+ *
+ * This register has the same bit fields as PKO_L3_SQ()_YELLOW.
+ *
+ */
+union cvmx_pko_l4_sqx_red {
+	uint64_t u64;
+	struct cvmx_pko_l4_sqx_red_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_20_63               : 44;
+	uint64_t head                         : 10; /**< Head pointer. The index of round-robin linked-list head. For internal use only. */
+	uint64_t tail                         : 10; /**< Tail pointer. The index of round-robin linked-list tail. For internal use only. */
+#else
+	uint64_t tail                         : 10;
+	uint64_t head                         : 10;
+	uint64_t reserved_20_63               : 44;
+#endif
+	} s;
+	struct cvmx_pko_l4_sqx_red_s          cn78xx;
+};
+typedef union cvmx_pko_l4_sqx_red cvmx_pko_l4_sqx_red_t;
+
+/**
+ * cvmx_pko_l4_sq#_sched_state
+ *
+ * This register has the same bit fields as PKO_L2_SQ()_SCHED_STATE.
+ *
+ */
+union cvmx_pko_l4_sqx_sched_state {
+	uint64_t u64;
+	struct cvmx_pko_l4_sqx_sched_state_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_25_63               : 39;
+	uint64_t rr_count                     : 25; /**< Round-robin (DWRR) deficit counter. A 25-bit signed integer count. For diagnostic use. */
+#else
+	uint64_t rr_count                     : 25;
+	uint64_t reserved_25_63               : 39;
+#endif
+	} s;
+	struct cvmx_pko_l4_sqx_sched_state_s  cn78xx;
+};
+typedef union cvmx_pko_l4_sqx_sched_state cvmx_pko_l4_sqx_sched_state_t;
+
+/**
+ * cvmx_pko_l4_sq#_schedule
+ *
+ * This register has the same bit fields as PKO_L2_SQ()_SCHEDULE.
+ *
+ */
+union cvmx_pko_l4_sqx_schedule {
+	uint64_t u64;
+	struct cvmx_pko_l4_sqx_schedule_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_28_63               : 36;
+	uint64_t prio                         : 4;  /**< Priority. The priority used for this SQ in the (lower-level) parent's scheduling
+                                                         algorithm. When this SQ is not used, we recommend setting PRIO to zero. The legal PRIO
+                                                         values are 0-9 when the SQ is used. In addition to priority, PRIO determines whether the
+                                                         SQ is a static queue or not: If PRIO equals PKO_*_SQn_TOPOLOGY[RR_PRIO], where
+                                                         PKO_*_TOPOLOGY[PARENT] for this SQ equals n, then this is a round-robin child queue into
+                                                         the shaper at the next level. */
+	uint64_t rr_quantum                   : 24; /**< Round-robin (DWRR) quantum. The deficit-weighted round-robin quantum (24-bit unsigned
+                                                         integer).
+                                                         The packet size used in all DWRR calculations is:
+                                                         _  (PKO_Ln_SQm_SHAPE[LENGTH_DISABLE] ? 0 : (PKO_SEND_HDR_S[TOTAL] + CALCPAD)) +
+                                                            PKO_SEND_EXT_S[SHAPECHG] +
+                                                            PKO_Ln_SQm_SHAPE[ADJUST]
+                                                         where n and m correspond to this PKO_Ln_SQm_SCHEDULE CSR. CALCPAD is zero when
+                                                         PKO_PDM_DQd_MINPAD[MINPAD] is clear or when PKO_PDM_CFG[TOTAL]>=PKO_PDM_CFG[MINLEN],
+                                                         else CALCPAD=PKO_PDM_CFG[MINLEN]-PKO_SEND_HDR_S[TOTAL], where d is the DQ the packet used.
+                                                         PKO_SEND_EXT_S[SHAPECHG] is zero when a PKO_SEND_EXT_S is not present in the send
+                                                         descriptor. */
+#else
+	uint64_t rr_quantum                   : 24;
+	uint64_t prio                         : 4;
+	uint64_t reserved_28_63               : 36;
+#endif
+	} s;
+	struct cvmx_pko_l4_sqx_schedule_s     cn78xx;
+};
+typedef union cvmx_pko_l4_sqx_schedule cvmx_pko_l4_sqx_schedule_t;
+
+/**
+ * cvmx_pko_l4_sq#_shape
+ *
+ * This register has the same bit fields as PKO_L3_SQ()_SHAPE.
+ *
+ */
+union cvmx_pko_l4_sqx_shape {
+	uint64_t u64;
+	struct cvmx_pko_l4_sqx_shape_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_25_63               : 39;
+	uint64_t length_disable               : 1;  /**< Length disable. Disables the use of packet lengths in shaping calculations such that only
+                                                         the value of the ADJUST field is used. */
+	uint64_t reserved_13_23               : 11;
+	uint64_t yellow_disable               : 1;  /**< Disable yellow transitions. Disables green-to-yellow packet color marking transitions when set. */
+	uint64_t red_disable                  : 1;  /**< Disable red transitions. Disables green-to-red and yellow-to-red packet color marking
+                                                         transitions when set. */
+	uint64_t red_algo                     : 2;  /**< See PKO_L2_SQ()_SHAPE[RED_ALGO]. */
+	uint64_t adjust                       : 9;  /**< See PKO_L2_SQ()_SHAPE[ADJUST]. */
+#else
+	uint64_t adjust                       : 9;
+	uint64_t red_algo                     : 2;
+	uint64_t red_disable                  : 1;
+	uint64_t yellow_disable               : 1;
+	uint64_t reserved_13_23               : 11;
+	uint64_t length_disable               : 1;
+	uint64_t reserved_25_63               : 39;
+#endif
+	} s;
+	struct cvmx_pko_l4_sqx_shape_s        cn78xx;
+};
+typedef union cvmx_pko_l4_sqx_shape cvmx_pko_l4_sqx_shape_t;
+
+/**
+ * cvmx_pko_l4_sq#_shape_state
+ *
+ * This register has the same bit fields as PKO_L2_SQ()_SHAPE_STATE.
+ *
+ */
+union cvmx_pko_l4_sqx_shape_state {
+	uint64_t u64;
+	struct cvmx_pko_l4_sqx_shape_state_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_60_63               : 4;
+	uint64_t tw_timestamp                 : 6;  /**< Time-wheel timestamp. Debug access to the live time-wheel timestamp. */
+	uint64_t color                        : 2;  /**< Shaper color status. Debug access to the live shaper state.
+                                                         0x0 = Green - operating in 'committed' range.
+                                                         0x1 = Yellow - operating in 'excess/peak' range.
+                                                         0x2 = Red - operating in 'oversubscribed' range.
+                                                         0x3 = Reserved. */
+	uint64_t pir_accum                    : 26; /**< Peak information rate accumulator. Debug access to the live PIR accumulator. */
+	uint64_t cir_accum                    : 26; /**< Committed information rate accumulator. Debug access to the live CIR accumulator. */
+#else
+	uint64_t cir_accum                    : 26;
+	uint64_t pir_accum                    : 26;
+	uint64_t color                        : 2;
+	uint64_t tw_timestamp                 : 6;
+	uint64_t reserved_60_63               : 4;
+#endif
+	} s;
+	struct cvmx_pko_l4_sqx_shape_state_s  cn78xx;
+};
+typedef union cvmx_pko_l4_sqx_shape_state cvmx_pko_l4_sqx_shape_state_t;
+
+/**
+ * cvmx_pko_l4_sq#_sw_xoff
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_SW_XOFF.
+ *
+ */
+union cvmx_pko_l4_sqx_sw_xoff {
+	uint64_t u64;
+	struct cvmx_pko_l4_sqx_sw_xoff_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t drain_irq                    : 1;  /**< Drain IRQ. Enables an interrupt that fires when the drain operation has completed. */
+	uint64_t drain_null_link              : 1;  /**< "Drain null link. Conditions the drain path to drain through the null link (i.e. link
+                                                         28). As such, channel credits, HW_XOFF, and shaping are disabled on the draining path
+                                                         until the path has drained." */
+	uint64_t drain                        : 1;  /**< Drain. This control activates a drain path through the PSE that starts at this node and
+                                                         ends at the SQ1 level. The drain path is prioritized over other paths through PSE and can
+                                                         be used in combination with DRAIN_NULL_LINK and DRAIN_IRQ. */
+	uint64_t xoff                         : 1;  /**< XOFF. The PQ is disabled when XOFF is asserted. PQ is enabled when XOFF is deasserted.
+                                                         NOTE: The associated PKO_L1_SQ()_TOPOLOGY[LINK] must be configured before using this
+                                                         register field. Writing to this register field before the associated
+                                                         PKO_L1_SQ()_TOPOLOGY[LINK] value is configured can result in modifying the software
+                                                         XOFF state of the wrong SQ. */
+#else
+	uint64_t xoff                         : 1;
+	uint64_t drain                        : 1;
+	uint64_t drain_null_link              : 1;
+	uint64_t drain_irq                    : 1;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_pko_l4_sqx_sw_xoff_s      cn78xx;
+};
+typedef union cvmx_pko_l4_sqx_sw_xoff cvmx_pko_l4_sqx_sw_xoff_t;
+
+/**
+ * cvmx_pko_l4_sq#_topology
+ */
+union cvmx_pko_l4_sqx_topology {
+	uint64_t u64;
+	struct cvmx_pko_l4_sqx_topology_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_42_63               : 22;
+	uint64_t prio_anchor                  : 10; /**< See PKO_L1_SQ()_TOPOLOGY[PRIO_ANCHOR]. */
+	uint64_t reserved_25_31               : 7;
+	uint64_t parent                       : 9;  /**< See PKO_L2_SQ()_TOPOLOGY[PARENT]. */
+	uint64_t reserved_5_15                : 11;
+	uint64_t rr_prio                      : 4;  /**< See PKO_L1_SQ()_TOPOLOGY[RR_PRIO]. */
+	uint64_t reserved_0_0                 : 1;
+#else
+	uint64_t reserved_0_0                 : 1;
+	uint64_t rr_prio                      : 4;
+	uint64_t reserved_5_15                : 11;
+	uint64_t parent                       : 9;
+	uint64_t reserved_25_31               : 7;
+	uint64_t prio_anchor                  : 10;
+	uint64_t reserved_42_63               : 22;
+#endif
+	} s;
+	struct cvmx_pko_l4_sqx_topology_s     cn78xx;
+};
+typedef union cvmx_pko_l4_sqx_topology cvmx_pko_l4_sqx_topology_t;
+
+/**
+ * cvmx_pko_l4_sq#_yellow
+ *
+ * This register has the same bit fields as PKO_L3_SQ()_YELLOW.
+ *
+ */
+union cvmx_pko_l4_sqx_yellow {
+	uint64_t u64;
+	struct cvmx_pko_l4_sqx_yellow_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_20_63               : 44;
+	uint64_t head                         : 10; /**< Head pointer. The index of round-robin linked-list head. For internal use only. */
+	uint64_t tail                         : 10; /**< Tail pointer. The index of round-robin linked-list tail. For internal use only. */
+#else
+	uint64_t tail                         : 10;
+	uint64_t head                         : 10;
+	uint64_t reserved_20_63               : 44;
+#endif
+	} s;
+	struct cvmx_pko_l4_sqx_yellow_s       cn78xx;
+};
+typedef union cvmx_pko_l4_sqx_yellow cvmx_pko_l4_sqx_yellow_t;
+
+/**
+ * cvmx_pko_l4_sq_csr_bus_debug
+ */
+union cvmx_pko_l4_sq_csr_bus_debug {
+	uint64_t u64;
+	struct cvmx_pko_l4_sq_csr_bus_debug_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t csr_bus_debug                : 64; /**< -- */
+#else
+	uint64_t csr_bus_debug                : 64;
+#endif
+	} s;
+	struct cvmx_pko_l4_sq_csr_bus_debug_s cn78xx;
+};
+typedef union cvmx_pko_l4_sq_csr_bus_debug cvmx_pko_l4_sq_csr_bus_debug_t;
+
+/**
+ * cvmx_pko_l4_sqa_debug
+ *
+ * This register has the same bit fields as PKO_PQA_DEBUG.
+ *
+ */
+union cvmx_pko_l4_sqa_debug {
+	uint64_t u64;
+	struct cvmx_pko_l4_sqa_debug_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t dbg_vec                      : 64; /**< Debug Vector. */
+#else
+	uint64_t dbg_vec                      : 64;
+#endif
+	} s;
+	struct cvmx_pko_l4_sqa_debug_s        cn78xx;
+};
+typedef union cvmx_pko_l4_sqa_debug cvmx_pko_l4_sqa_debug_t;
+
+/**
+ * cvmx_pko_l4_sqb_debug
+ *
+ * This register has the same bit fields as PKO_PQA_DEBUG.
+ *
+ */
+union cvmx_pko_l4_sqb_debug {
+	uint64_t u64;
+	struct cvmx_pko_l4_sqb_debug_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t dbg_vec                      : 64; /**< Debug Vector. */
+#else
+	uint64_t dbg_vec                      : 64;
+#endif
+	} s;
+	struct cvmx_pko_l4_sqb_debug_s        cn78xx;
+};
+typedef union cvmx_pko_l4_sqb_debug cvmx_pko_l4_sqb_debug_t;
+
+/**
+ * cvmx_pko_l5_sq#_cir
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_CIR.
+ *
+ */
+union cvmx_pko_l5_sqx_cir {
+	uint64_t u64;
+	struct cvmx_pko_l5_sqx_cir_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_41_63               : 23;
+	uint64_t burst_exponent               : 4;  /**< Burst exponent. The burst limit is specified as 1.BURST_MANTISSA << BURST_EXPONENT. */
+	uint64_t burst_mantissa               : 8;  /**< Burst mantissa. The burst limit is specified as 1.BURST_MANTISSA << BURST_EXPONENT. */
+	uint64_t reserved_17_28               : 12;
+	uint64_t rate_divider_exponent        : 4;  /**< Rate divider exponent. This 4-bit base-2 exponent is used to divide the credit rate by
+                                                         specifying the number of time-wheel turns required before the accumulator is increased.
+                                                         The rate count = (1 << RATE_DIVIDER_EXPONENT). The supported range for
+                                                         RATE_DIVIDER_EXPONENT is 0 to 12. Programmed values greater than 12 are treated as 12.
+                                                         Note that for the L1-SQs, a time-wheel turn is 96 clocks (SCLK). For the other levels a
+                                                         time-wheel turn is 768 clocks (SCLK).
+                                                         For L1_SQ: RATE = (SCLK_FREQUENCY / 96) * (1.RATE_MANTISSA << RATE_EXPONENT) /
+                                                         (1 <<RATE_DIVIDER_EXPONENT)
+                                                         For L[5:2]_SQ: RATE = (SCLK_FREQUENCY / 768) * (1.RATE_MANTISSA << RATE_EXPONENT) /
+                                                         (1 << RATE_DIVIDER_EXPONENT) */
+	uint64_t rate_exponent                : 4;  /**< Rate exponent. The rate is specified as 1.RATE_MANTISSA << RATE_EXPONENT. */
+	uint64_t rate_mantissa                : 8;  /**< Rate mantissa. The rate is specified as 1.RATE_MANTISSA << RATE_EXPONENT. */
+	uint64_t enable                       : 1;  /**< Enable. Enables CIR shaping. */
+#else
+	uint64_t enable                       : 1;
+	uint64_t rate_mantissa                : 8;
+	uint64_t rate_exponent                : 4;
+	uint64_t rate_divider_exponent        : 4;
+	uint64_t reserved_17_28               : 12;
+	uint64_t burst_mantissa               : 8;
+	uint64_t burst_exponent               : 4;
+	uint64_t reserved_41_63               : 23;
+#endif
+	} s;
+	struct cvmx_pko_l5_sqx_cir_s          cn78xx;
+};
+typedef union cvmx_pko_l5_sqx_cir cvmx_pko_l5_sqx_cir_t;
+
+/**
+ * cvmx_pko_l5_sq#_green
+ *
+ * This register has the same bit fields as PKO_L3_SQ()_GREEN.
+ *
+ */
+union cvmx_pko_l5_sqx_green {
+	uint64_t u64;
+	struct cvmx_pko_l5_sqx_green_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_41_63               : 23;
+	uint64_t rr_active                    : 1;  /**< Round-robin red active. Indicates that the round-robin input is mapped to RED. */
+	uint64_t active_vec                   : 20; /**< Active vector. A 10-bit vector, ordered by priority, that indicate which inputs to this
+                                                         scheduling queue are active. For internal use only. */
+	uint64_t head                         : 10; /**< Head pointer. The index of round-robin linked-list head. For internal use only. */
+	uint64_t tail                         : 10; /**< Tail pointer. The index of round-robin linked-list tail. For internal use only. */
+#else
+	uint64_t tail                         : 10;
+	uint64_t head                         : 10;
+	uint64_t active_vec                   : 20;
+	uint64_t rr_active                    : 1;
+	uint64_t reserved_41_63               : 23;
+#endif
+	} s;
+	struct cvmx_pko_l5_sqx_green_s        cn78xx;
+};
+typedef union cvmx_pko_l5_sqx_green cvmx_pko_l5_sqx_green_t;
+
+/**
+ * cvmx_pko_l5_sq#_pick
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_PICK.
+ *
+ */
+union cvmx_pko_l5_sqx_pick {
+	uint64_t u64;
+	struct cvmx_pko_l5_sqx_pick_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t dq                           : 10; /**< Descriptor queue. Index of originating descriptor queue. */
+	uint64_t color                        : 2;  /**< See PKO_L2_SQ()_SHAPE[COLOR]. */
+	uint64_t child                        : 10; /**< Child index. When the C_CON bit of this result is set, indicating that this result is
+                                                         connected in a flow that extends through the child result, this is the index of that child
+                                                         result. */
+	uint64_t bubble                       : 1;  /**< This metapacket is a fake passed forward after a prune. */
+	uint64_t p_con                        : 1;  /**< Parent connected flag. This pick has more picks in front of it. */
+	uint64_t c_con                        : 1;  /**< Child connected flag. This pick has more picks behind it. */
+	uint64_t uid                          : 7;  /**< Unique ID. 7-bit unique value assigned at the DQ level, increments for each packet. */
+	uint64_t jump                         : 1;  /**< Jump. Set when metapacket originated from a jump descriptor. */
+	uint64_t fpd                          : 1;  /**< First packet descriptor. Set when metapacket was the first in a cacheline. */
+	uint64_t ds                           : 1;  /**< Don't send. Set when metapacket is not to be sent. */
+	uint64_t adjust                       : 9;  /**< See PKO_L2_SQ()_SHAPE[ADJUST]. */
+	uint64_t pir_dis                      : 1;  /**< PIR disable. Peak shaper disabled. */
+	uint64_t cir_dis                      : 1;  /**< CIR disable. Committed shaper disabled. */
+	uint64_t red_algo_override            : 2;  /**< See PKO_L2_SQ()_SHAPE[RED_ALGO]. */
+	uint64_t length                       : 16; /**< Packet length. The packet length in bytes. */
+#else
+	uint64_t length                       : 16;
+	uint64_t red_algo_override            : 2;
+	uint64_t cir_dis                      : 1;
+	uint64_t pir_dis                      : 1;
+	uint64_t adjust                       : 9;
+	uint64_t ds                           : 1;
+	uint64_t fpd                          : 1;
+	uint64_t jump                         : 1;
+	uint64_t uid                          : 7;
+	uint64_t c_con                        : 1;
+	uint64_t p_con                        : 1;
+	uint64_t bubble                       : 1;
+	uint64_t child                        : 10;
+	uint64_t color                        : 2;
+	uint64_t dq                           : 10;
+#endif
+	} s;
+	struct cvmx_pko_l5_sqx_pick_s         cn78xx;
+};
+typedef union cvmx_pko_l5_sqx_pick cvmx_pko_l5_sqx_pick_t;
+
+/**
+ * cvmx_pko_l5_sq#_pir
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_CIR.
+ *
+ */
+union cvmx_pko_l5_sqx_pir {
+	uint64_t u64;
+	struct cvmx_pko_l5_sqx_pir_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_41_63               : 23;
+	uint64_t burst_exponent               : 4;  /**< Burst exponent. The burst limit is specified as 1.BURST_MANTISSA << BURST_EXPONENT. */
+	uint64_t burst_mantissa               : 8;  /**< Burst mantissa. The burst limit is specified as 1.BURST_MANTISSA << BURST_EXPONENT. */
+	uint64_t reserved_17_28               : 12;
+	uint64_t rate_divider_exponent        : 4;  /**< Rate divider exponent. This 4-bit base-2 exponent is used to divide the credit rate by
+                                                         specifying the number of time-wheel turns required before the accumulator is increased.
+                                                         The rate count = (1 << RATE_DIVIDER_EXPONENT). The supported range for
+                                                         RATE_DIVIDER_EXPONENT is 0 to 12. Programmed values greater than 12 are treated as 12.
+                                                         Note that for the L1-SQs, a time-wheel turn is 96 clocks (SCLK). For the other levels a
+                                                         time-wheel turn is 768 clocks (SCLK).
+                                                         For L1_SQ: RATE = (SCLK_FREQUENCY / 96) * (1.RATE_MANTISSA << RATE_EXPONENT) /
+                                                         (1 <<RATE_DIVIDER_EXPONENT)
+                                                         For L[5:2]_SQ: RATE = (SCLK_FREQUENCY / 768) * (1.RATE_MANTISSA << RATE_EXPONENT) /
+                                                         (1 << RATE_DIVIDER_EXPONENT) */
+	uint64_t rate_exponent                : 4;  /**< Rate exponent. The rate is specified as 1.RATE_MANTISSA << RATE_EXPONENT. */
+	uint64_t rate_mantissa                : 8;  /**< Rate mantissa. The rate is specified as 1.RATE_MANTISSA << RATE_EXPONENT. */
+	uint64_t enable                       : 1;  /**< Enable. Enables CIR shaping. */
+#else
+	uint64_t enable                       : 1;
+	uint64_t rate_mantissa                : 8;
+	uint64_t rate_exponent                : 4;
+	uint64_t rate_divider_exponent        : 4;
+	uint64_t reserved_17_28               : 12;
+	uint64_t burst_mantissa               : 8;
+	uint64_t burst_exponent               : 4;
+	uint64_t reserved_41_63               : 23;
+#endif
+	} s;
+	struct cvmx_pko_l5_sqx_pir_s          cn78xx;
+};
+typedef union cvmx_pko_l5_sqx_pir cvmx_pko_l5_sqx_pir_t;
+
+/**
+ * cvmx_pko_l5_sq#_pointers
+ *
+ * This register has the same bit fields as PKO_L4_SQ()_POINTERS.
+ *
+ */
+union cvmx_pko_l5_sqx_pointers {
+	uint64_t u64;
+	struct cvmx_pko_l5_sqx_pointers_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_26_63               : 38;
+	uint64_t prev                         : 10; /**< See PKO_L2_SQ()_POINTERS[PREV]. */
+	uint64_t reserved_10_15               : 6;
+	uint64_t next                         : 10; /**< See PKO_L2_SQ()_POINTERS[NEXT]. */
+#else
+	uint64_t next                         : 10;
+	uint64_t reserved_10_15               : 6;
+	uint64_t prev                         : 10;
+	uint64_t reserved_26_63               : 38;
+#endif
+	} s;
+	struct cvmx_pko_l5_sqx_pointers_s     cn78xx;
+};
+typedef union cvmx_pko_l5_sqx_pointers cvmx_pko_l5_sqx_pointers_t;
+
+/**
+ * cvmx_pko_l5_sq#_red
+ *
+ * This register has the same bit fields as PKO_L3_SQ()_YELLOW.
+ *
+ */
+union cvmx_pko_l5_sqx_red {
+	uint64_t u64;
+	struct cvmx_pko_l5_sqx_red_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_20_63               : 44;
+	uint64_t head                         : 10; /**< Head pointer. The index of round-robin linked-list head. For internal use only. */
+	uint64_t tail                         : 10; /**< Tail pointer. The index of round-robin linked-list tail. For internal use only. */
+#else
+	uint64_t tail                         : 10;
+	uint64_t head                         : 10;
+	uint64_t reserved_20_63               : 44;
+#endif
+	} s;
+	struct cvmx_pko_l5_sqx_red_s          cn78xx;
+};
+typedef union cvmx_pko_l5_sqx_red cvmx_pko_l5_sqx_red_t;
+
+/**
+ * cvmx_pko_l5_sq#_sched_state
+ *
+ * This register has the same bit fields as PKO_L2_SQ()_SCHED_STATE.
+ *
+ */
+union cvmx_pko_l5_sqx_sched_state {
+	uint64_t u64;
+	struct cvmx_pko_l5_sqx_sched_state_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_25_63               : 39;
+	uint64_t rr_count                     : 25; /**< Round-robin (DWRR) deficit counter. A 25-bit signed integer count. For diagnostic use. */
+#else
+	uint64_t rr_count                     : 25;
+	uint64_t reserved_25_63               : 39;
+#endif
+	} s;
+	struct cvmx_pko_l5_sqx_sched_state_s  cn78xx;
+};
+typedef union cvmx_pko_l5_sqx_sched_state cvmx_pko_l5_sqx_sched_state_t;
+
+/**
+ * cvmx_pko_l5_sq#_schedule
+ *
+ * This register has the same bit fields as PKO_L2_SQ()_SCHEDULE.
+ *
+ */
+union cvmx_pko_l5_sqx_schedule {
+	uint64_t u64;
+	struct cvmx_pko_l5_sqx_schedule_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_28_63               : 36;
+	uint64_t prio                         : 4;  /**< Priority. The priority used for this SQ in the (lower-level) parent's scheduling
+                                                         algorithm. When this SQ is not used, we recommend setting PRIO to zero. The legal PRIO
+                                                         values are 0-9 when the SQ is used. In addition to priority, PRIO determines whether the
+                                                         SQ is a static queue or not: If PRIO equals PKO_*_SQn_TOPOLOGY[RR_PRIO], where
+                                                         PKO_*_TOPOLOGY[PARENT] for this SQ equals n, then this is a round-robin child queue into
+                                                         the shaper at the next level. */
+	uint64_t rr_quantum                   : 24; /**< Round-robin (DWRR) quantum. The deficit-weighted round-robin quantum (24-bit unsigned
+                                                         integer).
+                                                         The packet size used in all DWRR calculations is:
+                                                         _  (PKO_Ln_SQm_SHAPE[LENGTH_DISABLE] ? 0 : (PKO_SEND_HDR_S[TOTAL] + CALCPAD)) +
+                                                            PKO_SEND_EXT_S[SHAPECHG] +
+                                                            PKO_Ln_SQm_SHAPE[ADJUST]
+                                                         where n and m correspond to this PKO_Ln_SQm_SCHEDULE CSR. CALCPAD is zero when
+                                                         PKO_PDM_DQd_MINPAD[MINPAD] is clear or when PKO_PDM_CFG[TOTAL]>=PKO_PDM_CFG[MINLEN],
+                                                         else CALCPAD=PKO_PDM_CFG[MINLEN]-PKO_SEND_HDR_S[TOTAL], where d is the DQ the packet used.
+                                                         PKO_SEND_EXT_S[SHAPECHG] is zero when a PKO_SEND_EXT_S is not present in the send
+                                                         descriptor. */
+#else
+	uint64_t rr_quantum                   : 24;
+	uint64_t prio                         : 4;
+	uint64_t reserved_28_63               : 36;
+#endif
+	} s;
+	struct cvmx_pko_l5_sqx_schedule_s     cn78xx;
+};
+typedef union cvmx_pko_l5_sqx_schedule cvmx_pko_l5_sqx_schedule_t;
+
+/**
+ * cvmx_pko_l5_sq#_shape
+ */
+union cvmx_pko_l5_sqx_shape {
+	uint64_t u64;
+	struct cvmx_pko_l5_sqx_shape_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_25_63               : 39;
+	uint64_t length_disable               : 1;  /**< Length disable. Disables the use of packet lengths in shaping calculations such that only
+                                                         the value of PKO_L5_SQ()_SHAPE[ADJUST]. */
+	uint64_t reserved_13_23               : 11;
+	uint64_t yellow_disable               : 1;  /**< Disable yellow transitions. Disables green-to-yellow packet color marking transitions when set. */
+	uint64_t red_disable                  : 1;  /**< Disable red transitions. Disables green-to-red and yellow-to-red packet color marking
+                                                         transitions when set. */
+	uint64_t red_algo                     : 2;  /**< See PKO_L2_SQ()_SHAPE[RED_ALGO]. */
+	uint64_t adjust                       : 9;  /**< See PKO_L2_SQ()_SHAPE[ADJUST]. */
+#else
+	uint64_t adjust                       : 9;
+	uint64_t red_algo                     : 2;
+	uint64_t red_disable                  : 1;
+	uint64_t yellow_disable               : 1;
+	uint64_t reserved_13_23               : 11;
+	uint64_t length_disable               : 1;
+	uint64_t reserved_25_63               : 39;
+#endif
+	} s;
+	struct cvmx_pko_l5_sqx_shape_s        cn78xx;
+};
+typedef union cvmx_pko_l5_sqx_shape cvmx_pko_l5_sqx_shape_t;
+
+/**
+ * cvmx_pko_l5_sq#_shape_state
+ *
+ * This register has the same bit fields as PKO_L2_SQ()_SHAPE_STATE.
+ *
+ */
+union cvmx_pko_l5_sqx_shape_state {
+	uint64_t u64;
+	struct cvmx_pko_l5_sqx_shape_state_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_60_63               : 4;
+	uint64_t tw_timestamp                 : 6;  /**< Time-wheel timestamp. Debug access to the live time-wheel timestamp. */
+	uint64_t color                        : 2;  /**< Shaper color status. Debug access to the live shaper state.
+                                                         0x0 = Green - operating in 'committed' range.
+                                                         0x1 = Yellow - operating in 'excess/peak' range.
+                                                         0x2 = Red - operating in 'oversubscribed' range.
+                                                         0x3 = Reserved. */
+	uint64_t pir_accum                    : 26; /**< Peak information rate accumulator. Debug access to the live PIR accumulator. */
+	uint64_t cir_accum                    : 26; /**< Committed information rate accumulator. Debug access to the live CIR accumulator. */
+#else
+	uint64_t cir_accum                    : 26;
+	uint64_t pir_accum                    : 26;
+	uint64_t color                        : 2;
+	uint64_t tw_timestamp                 : 6;
+	uint64_t reserved_60_63               : 4;
+#endif
+	} s;
+	struct cvmx_pko_l5_sqx_shape_state_s  cn78xx;
+};
+typedef union cvmx_pko_l5_sqx_shape_state cvmx_pko_l5_sqx_shape_state_t;
+
+/**
+ * cvmx_pko_l5_sq#_sw_xoff
+ *
+ * This register has the same bit fields as PKO_L1_SQ()_SW_XOFF.
+ *
+ */
+union cvmx_pko_l5_sqx_sw_xoff {
+	uint64_t u64;
+	struct cvmx_pko_l5_sqx_sw_xoff_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t drain_irq                    : 1;  /**< Drain IRQ. Enables an interrupt that fires when the drain operation has completed. */
+	uint64_t drain_null_link              : 1;  /**< "Drain null link. Conditions the drain path to drain through the null link (i.e. link
+                                                         28). As such, channel credits, HW_XOFF, and shaping are disabled on the draining path
+                                                         until the path has drained." */
+	uint64_t drain                        : 1;  /**< Drain. This control activates a drain path through the PSE that starts at this node and
+                                                         ends at the SQ1 level. The drain path is prioritized over other paths through PSE and can
+                                                         be used in combination with DRAIN_NULL_LINK and DRAIN_IRQ. */
+	uint64_t xoff                         : 1;  /**< XOFF. The PQ is disabled when XOFF is asserted. PQ is enabled when XOFF is deasserted.
+                                                         NOTE: The associated PKO_L1_SQ()_TOPOLOGY[LINK] must be configured before using this
+                                                         register field. Writing to this register field before the associated
+                                                         PKO_L1_SQ()_TOPOLOGY[LINK] value is configured can result in modifying the software
+                                                         XOFF state of the wrong SQ. */
+#else
+	uint64_t xoff                         : 1;
+	uint64_t drain                        : 1;
+	uint64_t drain_null_link              : 1;
+	uint64_t drain_irq                    : 1;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_pko_l5_sqx_sw_xoff_s      cn78xx;
+};
+typedef union cvmx_pko_l5_sqx_sw_xoff cvmx_pko_l5_sqx_sw_xoff_t;
+
+/**
+ * cvmx_pko_l5_sq#_topology
+ */
+union cvmx_pko_l5_sqx_topology {
+	uint64_t u64;
+	struct cvmx_pko_l5_sqx_topology_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_42_63               : 22;
+	uint64_t prio_anchor                  : 10; /**< See PKO_L1_SQ()_TOPOLOGY[PRIO_ANCHOR]. */
+	uint64_t reserved_26_31               : 6;
+	uint64_t parent                       : 10; /**< See PKO_L2_SQ()_TOPOLOGY[PARENT]. */
+	uint64_t reserved_5_15                : 11;
+	uint64_t rr_prio                      : 4;  /**< See PKO_L1_SQ()_TOPOLOGY[RR_PRIO]. */
+	uint64_t reserved_0_0                 : 1;
+#else
+	uint64_t reserved_0_0                 : 1;
+	uint64_t rr_prio                      : 4;
+	uint64_t reserved_5_15                : 11;
+	uint64_t parent                       : 10;
+	uint64_t reserved_26_31               : 6;
+	uint64_t prio_anchor                  : 10;
+	uint64_t reserved_42_63               : 22;
+#endif
+	} s;
+	struct cvmx_pko_l5_sqx_topology_s     cn78xx;
+};
+typedef union cvmx_pko_l5_sqx_topology cvmx_pko_l5_sqx_topology_t;
+
+/**
+ * cvmx_pko_l5_sq#_yellow
+ *
+ * This register has the same bit fields as PKO_L3_SQ()_YELLOW.
+ *
+ */
+union cvmx_pko_l5_sqx_yellow {
+	uint64_t u64;
+	struct cvmx_pko_l5_sqx_yellow_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_20_63               : 44;
+	uint64_t head                         : 10; /**< Head pointer. The index of round-robin linked-list head. For internal use only. */
+	uint64_t tail                         : 10; /**< Tail pointer. The index of round-robin linked-list tail. For internal use only. */
+#else
+	uint64_t tail                         : 10;
+	uint64_t head                         : 10;
+	uint64_t reserved_20_63               : 44;
+#endif
+	} s;
+	struct cvmx_pko_l5_sqx_yellow_s       cn78xx;
+};
+typedef union cvmx_pko_l5_sqx_yellow cvmx_pko_l5_sqx_yellow_t;
+
+/**
+ * cvmx_pko_l5_sq_csr_bus_debug
+ */
+union cvmx_pko_l5_sq_csr_bus_debug {
+	uint64_t u64;
+	struct cvmx_pko_l5_sq_csr_bus_debug_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t csr_bus_debug                : 64; /**< -- */
+#else
+	uint64_t csr_bus_debug                : 64;
+#endif
+	} s;
+	struct cvmx_pko_l5_sq_csr_bus_debug_s cn78xx;
+};
+typedef union cvmx_pko_l5_sq_csr_bus_debug cvmx_pko_l5_sq_csr_bus_debug_t;
+
+/**
+ * cvmx_pko_l5_sqa_debug
+ *
+ * This register has the same bit fields as PKO_PQA_DEBUG.
+ *
+ */
+union cvmx_pko_l5_sqa_debug {
+	uint64_t u64;
+	struct cvmx_pko_l5_sqa_debug_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t dbg_vec                      : 64; /**< Debug Vector. */
+#else
+	uint64_t dbg_vec                      : 64;
+#endif
+	} s;
+	struct cvmx_pko_l5_sqa_debug_s        cn78xx;
+};
+typedef union cvmx_pko_l5_sqa_debug cvmx_pko_l5_sqa_debug_t;
+
+/**
+ * cvmx_pko_l5_sqb_debug
+ *
+ * This register has the same bit fields as PKO_PQA_DEBUG.
+ *
+ */
+union cvmx_pko_l5_sqb_debug {
+	uint64_t u64;
+	struct cvmx_pko_l5_sqb_debug_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t dbg_vec                      : 64; /**< Debug Vector. */
+#else
+	uint64_t dbg_vec                      : 64;
+#endif
+	} s;
+	struct cvmx_pko_l5_sqb_debug_s        cn78xx;
+};
+typedef union cvmx_pko_l5_sqb_debug cvmx_pko_l5_sqb_debug_t;
+
+/**
+ * cvmx_pko_lut#
+ */
+union cvmx_pko_lutx {
+	uint64_t u64;
+	struct cvmx_pko_lutx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t valid                        : 1;  /**< Declares if the index in the LUT is valid. */
+	uint64_t reserved_14_14               : 1;
+	uint64_t pq_idx                       : 5;  /**< PQ index for channel return processing in the PSE. */
+	uint64_t queue_number                 : 9;  /**< Mapping from this channel to the programmed queue number. */
+#else
+	uint64_t queue_number                 : 9;
+	uint64_t pq_idx                       : 5;
+	uint64_t reserved_14_14               : 1;
+	uint64_t valid                        : 1;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_pko_lutx_s                cn78xx;
+};
+typedef union cvmx_pko_lutx cvmx_pko_lutx_t;
+
+/**
+ * cvmx_pko_lut_bist_status
+ */
+union cvmx_pko_lut_bist_status {
+	uint64_t u64;
+	struct cvmx_pko_lut_bist_status_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t bist_status                  : 1;  /**< C2Q LUT BIST status. */
+#else
+	uint64_t bist_status                  : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_pko_lut_bist_status_s     cn78xx;
+};
+typedef union cvmx_pko_lut_bist_status cvmx_pko_lut_bist_status_t;
+
+/**
+ * cvmx_pko_lut_ecc_ctl0
+ */
+union cvmx_pko_lut_ecc_ctl0 {
+	uint64_t u64;
+	struct cvmx_pko_lut_ecc_ctl0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t c2q_lut_ram_flip             : 2;  /**< C2Q_LUT_RAM flip syndrome bits on write. */
+	uint64_t c2q_lut_ram_cdis             : 1;  /**< C2Q_LUT_RAM ECC correction disable. */
+	uint64_t reserved_0_60                : 61;
+#else
+	uint64_t reserved_0_60                : 61;
+	uint64_t c2q_lut_ram_cdis             : 1;
+	uint64_t c2q_lut_ram_flip             : 2;
+#endif
+	} s;
+	struct cvmx_pko_lut_ecc_ctl0_s        cn78xx;
+};
+typedef union cvmx_pko_lut_ecc_ctl0 cvmx_pko_lut_ecc_ctl0_t;
+
+/**
+ * cvmx_pko_lut_ecc_dbe_sts0
+ */
+union cvmx_pko_lut_ecc_dbe_sts0 {
+	uint64_t u64;
+	struct cvmx_pko_lut_ecc_dbe_sts0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t c2q_lut_ram_dbe              : 1;  /**< Double-bit error for C2Q_LUT_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.nonpse.pko_c2q_lut.pko_c2q_lut_ram_i */
+	uint64_t reserved_0_62                : 63;
+#else
+	uint64_t reserved_0_62                : 63;
+	uint64_t c2q_lut_ram_dbe              : 1;
+#endif
+	} s;
+	struct cvmx_pko_lut_ecc_dbe_sts0_s    cn78xx;
+};
+typedef union cvmx_pko_lut_ecc_dbe_sts0 cvmx_pko_lut_ecc_dbe_sts0_t;
+
+/**
+ * cvmx_pko_lut_ecc_dbe_sts_cmb0
+ */
+union cvmx_pko_lut_ecc_dbe_sts_cmb0 {
+	uint64_t u64;
+	struct cvmx_pko_lut_ecc_dbe_sts_cmb0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t lut_dbe_cmb0                 : 1;  /**< This bit is the logical OR of all bits in PKO_LUT_ECC_DBE_STS0. To clear this bit,
+                                                         software
+                                                         must clear bits in PKO_LUT_ECC_DBE_STS0. When this bit is set, the corresponding interrupt
+                                                         is set. Throws PKO_INTSN_E::PKO_LUT_DBE_CMB0. INTERNAL: Instances:
+                                                         pko_pnr2.nonpse.pko_c2q_lut.pko_c2q_lut_ram_i */
+	uint64_t reserved_0_62                : 63;
+#else
+	uint64_t reserved_0_62                : 63;
+	uint64_t lut_dbe_cmb0                 : 1;
+#endif
+	} s;
+	struct cvmx_pko_lut_ecc_dbe_sts_cmb0_s cn78xx;
+};
+typedef union cvmx_pko_lut_ecc_dbe_sts_cmb0 cvmx_pko_lut_ecc_dbe_sts_cmb0_t;
+
+/**
+ * cvmx_pko_lut_ecc_sbe_sts0
+ */
+union cvmx_pko_lut_ecc_sbe_sts0 {
+	uint64_t u64;
+	struct cvmx_pko_lut_ecc_sbe_sts0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t c2q_lut_ram_sbe              : 1;  /**< Single-bit error for C2Q_LUT_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.nonpse.pko_c2q_lut.pko_c2q_lut_ram_i */
+	uint64_t reserved_0_62                : 63;
+#else
+	uint64_t reserved_0_62                : 63;
+	uint64_t c2q_lut_ram_sbe              : 1;
+#endif
+	} s;
+	struct cvmx_pko_lut_ecc_sbe_sts0_s    cn78xx;
+};
+typedef union cvmx_pko_lut_ecc_sbe_sts0 cvmx_pko_lut_ecc_sbe_sts0_t;
+
+/**
+ * cvmx_pko_lut_ecc_sbe_sts_cmb0
+ */
+union cvmx_pko_lut_ecc_sbe_sts_cmb0 {
+	uint64_t u64;
+	struct cvmx_pko_lut_ecc_sbe_sts_cmb0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t lut_sbe_cmb0                 : 1;  /**< This bit is the logical OR of all bits in PKO_LUT_ECC_SBE_STS0. To clear this bit,
+                                                         software
+                                                         must clear bits in PKO_LUT_ECC_SBE_STS0. When this bit is set, the corresponding interrupt
+                                                         is set. Throws PKO_INTSN_E::PKO_LUT_SBE_CMB0. INTERNAL: Instances:
+                                                         pko_pnr2.nonpse.pko_c2q_lut.pko_c2q_lut_ram_i */
+	uint64_t reserved_0_62                : 63;
+#else
+	uint64_t reserved_0_62                : 63;
+	uint64_t lut_sbe_cmb0                 : 1;
+#endif
+	} s;
+	struct cvmx_pko_lut_ecc_sbe_sts_cmb0_s cn78xx;
+};
+typedef union cvmx_pko_lut_ecc_sbe_sts_cmb0 cvmx_pko_lut_ecc_sbe_sts_cmb0_t;
+
+/**
+ * cvmx_pko_mac#_cfg
+ */
+union cvmx_pko_macx_cfg {
+	uint64_t u64;
+	struct cvmx_pko_macx_cfg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_17_63               : 47;
+	uint64_t min_pad_ena                  : 1;  /**< Minimum padding is enabled for this MAC/FIFO. */
+	uint64_t fcs_ena                      : 1;  /**< Enable outside FCS for this MAC/FIFO. */
+	uint64_t fcs_sop_off                  : 8;  /**< FCS start of packet offset. For this MAC, the number of bytes in the front of each packet
+                                                         to exclude from FCS. */
+	uint64_t skid_max_cnt                 : 2;  /**< Maximum number of SKID credits. 0x0 = 16; 0x1 = 32; 0x2 = 64. */
+	uint64_t fifo_num                     : 5;  /**< The PEB TX FIFO number assigned to the given MAC. A value of 0x1F means unassigned. Unused
+                                                         MACs must be assigned a FIFO_NUM = 0x1F. For each active MAC, a unique FIFO_NUM must be
+                                                         assigned. Legal values depend on the values in PKO_PTGF()_CFG[SIZE]. Assigning the
+                                                         same FIFO_NUM to more than a single active MAC will have unpredictable results. FIFOs 0x1E
+                                                         and 0x1D are invalid and will cause unpredictable results if used. */
+#else
+	uint64_t fifo_num                     : 5;
+	uint64_t skid_max_cnt                 : 2;
+	uint64_t fcs_sop_off                  : 8;
+	uint64_t fcs_ena                      : 1;
+	uint64_t min_pad_ena                  : 1;
+	uint64_t reserved_17_63               : 47;
+#endif
+	} s;
+	struct cvmx_pko_macx_cfg_s            cn78xx;
+};
+typedef union cvmx_pko_macx_cfg cvmx_pko_macx_cfg_t;
+
+/**
+ * cvmx_pko_mci0_cred_cnt#
+ */
+union cvmx_pko_mci0_cred_cntx {
+	uint64_t u64;
+	struct cvmx_pko_mci0_cred_cntx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_13_63               : 51;
+	uint64_t cred_cnt                     : 13; /**< Credit count. */
+#else
+	uint64_t cred_cnt                     : 13;
+	uint64_t reserved_13_63               : 51;
+#endif
+	} s;
+	struct cvmx_pko_mci0_cred_cntx_s      cn78xx;
+};
+typedef union cvmx_pko_mci0_cred_cntx cvmx_pko_mci0_cred_cntx_t;
+
+/**
+ * cvmx_pko_mci0_max_cred#
+ */
+union cvmx_pko_mci0_max_credx {
+	uint64_t u64;
+	struct cvmx_pko_mci0_max_credx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_12_63               : 52;
+	uint64_t max_cred_lim                 : 12; /**< Max credit limit. */
+#else
+	uint64_t max_cred_lim                 : 12;
+	uint64_t reserved_12_63               : 52;
+#endif
+	} s;
+	struct cvmx_pko_mci0_max_credx_s      cn78xx;
+};
+typedef union cvmx_pko_mci0_max_credx cvmx_pko_mci0_max_credx_t;
+
+/**
+ * cvmx_pko_mci1_cred_cnt#
+ */
+union cvmx_pko_mci1_cred_cntx {
+	uint64_t u64;
+	struct cvmx_pko_mci1_cred_cntx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_13_63               : 51;
+	uint64_t cred_cnt                     : 13; /**< Credit count. */
+#else
+	uint64_t cred_cnt                     : 13;
+	uint64_t reserved_13_63               : 51;
+#endif
+	} s;
+	struct cvmx_pko_mci1_cred_cntx_s      cn78xx;
+};
+typedef union cvmx_pko_mci1_cred_cntx cvmx_pko_mci1_cred_cntx_t;
+
+/**
+ * cvmx_pko_mci1_max_cred#
+ */
+union cvmx_pko_mci1_max_credx {
+	uint64_t u64;
+	struct cvmx_pko_mci1_max_credx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_12_63               : 52;
+	uint64_t max_cred_lim                 : 12; /**< Max credit limit. */
+#else
+	uint64_t max_cred_lim                 : 12;
+	uint64_t reserved_12_63               : 52;
+#endif
+	} s;
+	struct cvmx_pko_mci1_max_credx_s      cn78xx;
+};
+typedef union cvmx_pko_mci1_max_credx cvmx_pko_mci1_max_credx_t;
+
+/**
+ * cvmx_pko_mem_count0
+ *
+ * Notes:
+ * Total number of packets seen by PKO, per port
+ * A write to this address will clear the entry whose index is specified as COUNT[5:0].
+ * This CSR is a memory of 44 entries, and thus, the PKO_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.  A read of any entry that has not been
+ * previously written is illegal and will result in unpredictable CSR read data.
+ */
+union cvmx_pko_mem_count0 {
+	uint64_t u64;
+	struct cvmx_pko_mem_count0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_32_63               : 32;
+	uint64_t count                        : 32; /**< Total number of packets seen by PKO */
+#else
+	uint64_t count                        : 32;
+	uint64_t reserved_32_63               : 32;
+#endif
+	} s;
+	struct cvmx_pko_mem_count0_s          cn30xx;
+	struct cvmx_pko_mem_count0_s          cn31xx;
+	struct cvmx_pko_mem_count0_s          cn38xx;
+	struct cvmx_pko_mem_count0_s          cn38xxp2;
+	struct cvmx_pko_mem_count0_s          cn50xx;
+	struct cvmx_pko_mem_count0_s          cn52xx;
+	struct cvmx_pko_mem_count0_s          cn52xxp1;
+	struct cvmx_pko_mem_count0_s          cn56xx;
+	struct cvmx_pko_mem_count0_s          cn56xxp1;
+	struct cvmx_pko_mem_count0_s          cn58xx;
+	struct cvmx_pko_mem_count0_s          cn58xxp1;
+	struct cvmx_pko_mem_count0_s          cn61xx;
+	struct cvmx_pko_mem_count0_s          cn63xx;
+	struct cvmx_pko_mem_count0_s          cn63xxp1;
+	struct cvmx_pko_mem_count0_s          cn66xx;
+	struct cvmx_pko_mem_count0_s          cn68xx;
+	struct cvmx_pko_mem_count0_s          cn68xxp1;
+	struct cvmx_pko_mem_count0_s          cn70xx;
+	struct cvmx_pko_mem_count0_s          cn70xxp1;
+	struct cvmx_pko_mem_count0_s          cnf71xx;
+};
+typedef union cvmx_pko_mem_count0 cvmx_pko_mem_count0_t;
+
+/**
+ * cvmx_pko_mem_count1
+ *
+ * Notes:
+ * Total number of bytes seen by PKO, per port
+ * A write to this address will clear the entry whose index is specified as COUNT[5:0].
+ * This CSR is a memory of 44 entries, and thus, the PKO_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.  A read of any entry that has not been
+ * previously written is illegal and will result in unpredictable CSR read data.
+ */
+union cvmx_pko_mem_count1 {
+	uint64_t u64;
+	struct cvmx_pko_mem_count1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t count                        : 48; /**< Total number of bytes seen by PKO */
+#else
+	uint64_t count                        : 48;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_pko_mem_count1_s          cn30xx;
+	struct cvmx_pko_mem_count1_s          cn31xx;
+	struct cvmx_pko_mem_count1_s          cn38xx;
+	struct cvmx_pko_mem_count1_s          cn38xxp2;
+	struct cvmx_pko_mem_count1_s          cn50xx;
+	struct cvmx_pko_mem_count1_s          cn52xx;
+	struct cvmx_pko_mem_count1_s          cn52xxp1;
+	struct cvmx_pko_mem_count1_s          cn56xx;
+	struct cvmx_pko_mem_count1_s          cn56xxp1;
+	struct cvmx_pko_mem_count1_s          cn58xx;
+	struct cvmx_pko_mem_count1_s          cn58xxp1;
+	struct cvmx_pko_mem_count1_s          cn61xx;
+	struct cvmx_pko_mem_count1_s          cn63xx;
+	struct cvmx_pko_mem_count1_s          cn63xxp1;
+	struct cvmx_pko_mem_count1_s          cn66xx;
+	struct cvmx_pko_mem_count1_s          cn68xx;
+	struct cvmx_pko_mem_count1_s          cn68xxp1;
+	struct cvmx_pko_mem_count1_s          cn70xx;
+	struct cvmx_pko_mem_count1_s          cn70xxp1;
+	struct cvmx_pko_mem_count1_s          cnf71xx;
+};
+typedef union cvmx_pko_mem_count1 cvmx_pko_mem_count1_t;
+
+/**
+ * cvmx_pko_mem_debug0
+ *
+ * Notes:
+ * Internal per-port state intended for debug use only - pko_prt_psb.cmnd[63:0]
+ * This CSR is a memory of 12 entries, and thus, the PKO_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.
+ */
+union cvmx_pko_mem_debug0 {
+	uint64_t u64;
+	struct cvmx_pko_mem_debug0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t fau                          : 28; /**< Fetch and add command words */
+	uint64_t cmd                          : 14; /**< Command word */
+	uint64_t segs                         : 6;  /**< Number of segments/gather size */
+	uint64_t size                         : 16; /**< Packet length in bytes */
+#else
+	uint64_t size                         : 16;
+	uint64_t segs                         : 6;
+	uint64_t cmd                          : 14;
+	uint64_t fau                          : 28;
+#endif
+	} s;
+	struct cvmx_pko_mem_debug0_s          cn30xx;
+	struct cvmx_pko_mem_debug0_s          cn31xx;
+	struct cvmx_pko_mem_debug0_s          cn38xx;
+	struct cvmx_pko_mem_debug0_s          cn38xxp2;
+	struct cvmx_pko_mem_debug0_s          cn50xx;
+	struct cvmx_pko_mem_debug0_s          cn52xx;
+	struct cvmx_pko_mem_debug0_s          cn52xxp1;
+	struct cvmx_pko_mem_debug0_s          cn56xx;
+	struct cvmx_pko_mem_debug0_s          cn56xxp1;
+	struct cvmx_pko_mem_debug0_s          cn58xx;
+	struct cvmx_pko_mem_debug0_s          cn58xxp1;
+	struct cvmx_pko_mem_debug0_s          cn61xx;
+	struct cvmx_pko_mem_debug0_s          cn63xx;
+	struct cvmx_pko_mem_debug0_s          cn63xxp1;
+	struct cvmx_pko_mem_debug0_s          cn66xx;
+	struct cvmx_pko_mem_debug0_s          cn68xx;
+	struct cvmx_pko_mem_debug0_s          cn68xxp1;
+	struct cvmx_pko_mem_debug0_s          cn70xx;
+	struct cvmx_pko_mem_debug0_s          cn70xxp1;
+	struct cvmx_pko_mem_debug0_s          cnf71xx;
+};
+typedef union cvmx_pko_mem_debug0 cvmx_pko_mem_debug0_t;
+
+/**
+ * cvmx_pko_mem_debug1
+ *
+ * Notes:
+ * Internal per-port state intended for debug use only - pko_prt_psb.curr[63:0]
+ * This CSR is a memory of 12 entries, and thus, the PKO_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.
+ */
+union cvmx_pko_mem_debug1 {
+	uint64_t u64;
+	struct cvmx_pko_mem_debug1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t i                            : 1;  /**< "I"  value used for free operation */
+	uint64_t back                         : 4;  /**< Back value used for free operation */
+	uint64_t pool                         : 3;  /**< Pool value used for free operation */
+	uint64_t size                         : 16; /**< Size in bytes */
+	uint64_t ptr                          : 40; /**< Data pointer */
+#else
+	uint64_t ptr                          : 40;
+	uint64_t size                         : 16;
+	uint64_t pool                         : 3;
+	uint64_t back                         : 4;
+	uint64_t i                            : 1;
+#endif
+	} s;
+	struct cvmx_pko_mem_debug1_s          cn30xx;
+	struct cvmx_pko_mem_debug1_s          cn31xx;
+	struct cvmx_pko_mem_debug1_s          cn38xx;
+	struct cvmx_pko_mem_debug1_s          cn38xxp2;
+	struct cvmx_pko_mem_debug1_s          cn50xx;
+	struct cvmx_pko_mem_debug1_s          cn52xx;
+	struct cvmx_pko_mem_debug1_s          cn52xxp1;
+	struct cvmx_pko_mem_debug1_s          cn56xx;
+	struct cvmx_pko_mem_debug1_s          cn56xxp1;
+	struct cvmx_pko_mem_debug1_s          cn58xx;
+	struct cvmx_pko_mem_debug1_s          cn58xxp1;
+	struct cvmx_pko_mem_debug1_s          cn61xx;
+	struct cvmx_pko_mem_debug1_s          cn63xx;
+	struct cvmx_pko_mem_debug1_s          cn63xxp1;
+	struct cvmx_pko_mem_debug1_s          cn66xx;
+	struct cvmx_pko_mem_debug1_s          cn68xx;
+	struct cvmx_pko_mem_debug1_s          cn68xxp1;
+	struct cvmx_pko_mem_debug1_s          cn70xx;
+	struct cvmx_pko_mem_debug1_s          cn70xxp1;
+	struct cvmx_pko_mem_debug1_s          cnf71xx;
+};
+typedef union cvmx_pko_mem_debug1 cvmx_pko_mem_debug1_t;
+
+/**
+ * cvmx_pko_mem_debug10
+ *
+ * Notes:
+ * Internal per-engine state intended for debug use only - pko.dat.ptr.ptrs1, pko.dat.ptr.ptrs2
+ * This CSR is a memory of 10 entries, and thus, the PKO_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.
+ */
+union cvmx_pko_mem_debug10 {
+	uint64_t u64;
+	struct cvmx_pko_mem_debug10_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_0_63                : 64;
+#else
+	uint64_t reserved_0_63                : 64;
+#endif
+	} s;
+	struct cvmx_pko_mem_debug10_cn30xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t fau                          : 28; /**< Fetch and add command words */
+	uint64_t cmd                          : 14; /**< Command word */
+	uint64_t segs                         : 6;  /**< Number of segments/gather size */
+	uint64_t size                         : 16; /**< Packet length in bytes */
+#else
+	uint64_t size                         : 16;
+	uint64_t segs                         : 6;
+	uint64_t cmd                          : 14;
+	uint64_t fau                          : 28;
+#endif
+	} cn30xx;
+	struct cvmx_pko_mem_debug10_cn30xx    cn31xx;
+	struct cvmx_pko_mem_debug10_cn30xx    cn38xx;
+	struct cvmx_pko_mem_debug10_cn30xx    cn38xxp2;
+	struct cvmx_pko_mem_debug10_cn50xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_49_63               : 15;
+	uint64_t ptrs1                        : 17; /**< Internal state */
+	uint64_t reserved_17_31               : 15;
+	uint64_t ptrs2                        : 17; /**< Internal state */
+#else
+	uint64_t ptrs2                        : 17;
+	uint64_t reserved_17_31               : 15;
+	uint64_t ptrs1                        : 17;
+	uint64_t reserved_49_63               : 15;
+#endif
+	} cn50xx;
+	struct cvmx_pko_mem_debug10_cn50xx    cn52xx;
+	struct cvmx_pko_mem_debug10_cn50xx    cn52xxp1;
+	struct cvmx_pko_mem_debug10_cn50xx    cn56xx;
+	struct cvmx_pko_mem_debug10_cn50xx    cn56xxp1;
+	struct cvmx_pko_mem_debug10_cn50xx    cn58xx;
+	struct cvmx_pko_mem_debug10_cn50xx    cn58xxp1;
+	struct cvmx_pko_mem_debug10_cn50xx    cn61xx;
+	struct cvmx_pko_mem_debug10_cn50xx    cn63xx;
+	struct cvmx_pko_mem_debug10_cn50xx    cn63xxp1;
+	struct cvmx_pko_mem_debug10_cn50xx    cn66xx;
+	struct cvmx_pko_mem_debug10_cn50xx    cn68xx;
+	struct cvmx_pko_mem_debug10_cn50xx    cn68xxp1;
+	struct cvmx_pko_mem_debug10_cn50xx    cn70xx;
+	struct cvmx_pko_mem_debug10_cn50xx    cn70xxp1;
+	struct cvmx_pko_mem_debug10_cn50xx    cnf71xx;
+};
+typedef union cvmx_pko_mem_debug10 cvmx_pko_mem_debug10_t;
+
+/**
+ * cvmx_pko_mem_debug11
+ *
+ * Notes:
+ * Internal per-engine state intended for debug use only - pko.out.sta.state[22:0]
+ * This CSR is a memory of 10 entries, and thus, the PKO_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.
+ */
+union cvmx_pko_mem_debug11 {
+	uint64_t u64;
+	struct cvmx_pko_mem_debug11_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t i                            : 1;  /**< "I"  value used for free operation */
+	uint64_t back                         : 4;  /**< Back value used for free operation */
+	uint64_t pool                         : 3;  /**< Pool value used for free operation */
+	uint64_t size                         : 16; /**< Size in bytes */
+	uint64_t reserved_0_39                : 40;
+#else
+	uint64_t reserved_0_39                : 40;
+	uint64_t size                         : 16;
+	uint64_t pool                         : 3;
+	uint64_t back                         : 4;
+	uint64_t i                            : 1;
+#endif
+	} s;
+	struct cvmx_pko_mem_debug11_cn30xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t i                            : 1;  /**< "I"  value used for free operation */
+	uint64_t back                         : 4;  /**< Back value used for free operation */
+	uint64_t pool                         : 3;  /**< Pool value used for free operation */
+	uint64_t size                         : 16; /**< Size in bytes */
+	uint64_t ptr                          : 40; /**< Data pointer */
+#else
+	uint64_t ptr                          : 40;
+	uint64_t size                         : 16;
+	uint64_t pool                         : 3;
+	uint64_t back                         : 4;
+	uint64_t i                            : 1;
+#endif
+	} cn30xx;
+	struct cvmx_pko_mem_debug11_cn30xx    cn31xx;
+	struct cvmx_pko_mem_debug11_cn30xx    cn38xx;
+	struct cvmx_pko_mem_debug11_cn30xx    cn38xxp2;
+	struct cvmx_pko_mem_debug11_cn50xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_23_63               : 41;
+	uint64_t maj                          : 1;  /**< Internal state */
+	uint64_t uid                          : 3;  /**< Internal state */
+	uint64_t sop                          : 1;  /**< Internal state */
+	uint64_t len                          : 1;  /**< Internal state */
+	uint64_t chk                          : 1;  /**< Internal state */
+	uint64_t cnt                          : 13; /**< Internal state */
+	uint64_t mod                          : 3;  /**< Internal state */
+#else
+	uint64_t mod                          : 3;
+	uint64_t cnt                          : 13;
+	uint64_t chk                          : 1;
+	uint64_t len                          : 1;
+	uint64_t sop                          : 1;
+	uint64_t uid                          : 3;
+	uint64_t maj                          : 1;
+	uint64_t reserved_23_63               : 41;
+#endif
+	} cn50xx;
+	struct cvmx_pko_mem_debug11_cn50xx    cn52xx;
+	struct cvmx_pko_mem_debug11_cn50xx    cn52xxp1;
+	struct cvmx_pko_mem_debug11_cn50xx    cn56xx;
+	struct cvmx_pko_mem_debug11_cn50xx    cn56xxp1;
+	struct cvmx_pko_mem_debug11_cn50xx    cn58xx;
+	struct cvmx_pko_mem_debug11_cn50xx    cn58xxp1;
+	struct cvmx_pko_mem_debug11_cn50xx    cn61xx;
+	struct cvmx_pko_mem_debug11_cn50xx    cn63xx;
+	struct cvmx_pko_mem_debug11_cn50xx    cn63xxp1;
+	struct cvmx_pko_mem_debug11_cn50xx    cn66xx;
+	struct cvmx_pko_mem_debug11_cn50xx    cn68xx;
+	struct cvmx_pko_mem_debug11_cn50xx    cn68xxp1;
+	struct cvmx_pko_mem_debug11_cn50xx    cn70xx;
+	struct cvmx_pko_mem_debug11_cn50xx    cn70xxp1;
+	struct cvmx_pko_mem_debug11_cn50xx    cnf71xx;
+};
+typedef union cvmx_pko_mem_debug11 cvmx_pko_mem_debug11_t;
+
+/**
+ * cvmx_pko_mem_debug12
+ *
+ * Notes:
+ * Internal per-engine x4 state intended for debug use only - pko.out.ctl.cmnd[63:0]
+ * This CSR is a memory of 40 entries, and thus, the PKO_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.
+ */
+union cvmx_pko_mem_debug12 {
+	uint64_t u64;
+	struct cvmx_pko_mem_debug12_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_0_63                : 64;
+#else
+	uint64_t reserved_0_63                : 64;
+#endif
+	} s;
+	struct cvmx_pko_mem_debug12_cn30xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t data                         : 64; /**< WorkQ data or Store0 pointer */
+#else
+	uint64_t data                         : 64;
+#endif
+	} cn30xx;
+	struct cvmx_pko_mem_debug12_cn30xx    cn31xx;
+	struct cvmx_pko_mem_debug12_cn30xx    cn38xx;
+	struct cvmx_pko_mem_debug12_cn30xx    cn38xxp2;
+	struct cvmx_pko_mem_debug12_cn50xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t fau                          : 28; /**< Fetch and add command words */
+	uint64_t cmd                          : 14; /**< Command word */
+	uint64_t segs                         : 6;  /**< Number of segments/gather size */
+	uint64_t size                         : 16; /**< Packet length in bytes */
+#else
+	uint64_t size                         : 16;
+	uint64_t segs                         : 6;
+	uint64_t cmd                          : 14;
+	uint64_t fau                          : 28;
+#endif
+	} cn50xx;
+	struct cvmx_pko_mem_debug12_cn50xx    cn52xx;
+	struct cvmx_pko_mem_debug12_cn50xx    cn52xxp1;
+	struct cvmx_pko_mem_debug12_cn50xx    cn56xx;
+	struct cvmx_pko_mem_debug12_cn50xx    cn56xxp1;
+	struct cvmx_pko_mem_debug12_cn50xx    cn58xx;
+	struct cvmx_pko_mem_debug12_cn50xx    cn58xxp1;
+	struct cvmx_pko_mem_debug12_cn50xx    cn61xx;
+	struct cvmx_pko_mem_debug12_cn50xx    cn63xx;
+	struct cvmx_pko_mem_debug12_cn50xx    cn63xxp1;
+	struct cvmx_pko_mem_debug12_cn50xx    cn66xx;
+	struct cvmx_pko_mem_debug12_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t state                        : 64; /**< Internal state */
+#else
+	uint64_t state                        : 64;
+#endif
+	} cn68xx;
+	struct cvmx_pko_mem_debug12_cn68xx    cn68xxp1;
+	struct cvmx_pko_mem_debug12_cn50xx    cn70xx;
+	struct cvmx_pko_mem_debug12_cn50xx    cn70xxp1;
+	struct cvmx_pko_mem_debug12_cn50xx    cnf71xx;
+};
+typedef union cvmx_pko_mem_debug12 cvmx_pko_mem_debug12_t;
+
+/**
+ * cvmx_pko_mem_debug13
+ *
+ * Notes:
+ * Internal per-engine x4 state intended for debug use only - pko.out.ctl.head[63:0]
+ * This CSR is a memory of 40 entries, and thus, the PKO_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.
+ */
+union cvmx_pko_mem_debug13 {
+	uint64_t u64;
+	struct cvmx_pko_mem_debug13_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_0_63                : 64;
+#else
+	uint64_t reserved_0_63                : 64;
+#endif
+	} s;
+	struct cvmx_pko_mem_debug13_cn30xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_51_63               : 13;
+	uint64_t widx                         : 17; /**< PDB widx */
+	uint64_t ridx2                        : 17; /**< PDB ridx2 */
+	uint64_t widx2                        : 17; /**< PDB widx2 */
+#else
+	uint64_t widx2                        : 17;
+	uint64_t ridx2                        : 17;
+	uint64_t widx                         : 17;
+	uint64_t reserved_51_63               : 13;
+#endif
+	} cn30xx;
+	struct cvmx_pko_mem_debug13_cn30xx    cn31xx;
+	struct cvmx_pko_mem_debug13_cn30xx    cn38xx;
+	struct cvmx_pko_mem_debug13_cn30xx    cn38xxp2;
+	struct cvmx_pko_mem_debug13_cn50xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t i                            : 1;  /**< "I"  value used for free operation */
+	uint64_t back                         : 4;  /**< Back value used for free operation */
+	uint64_t pool                         : 3;  /**< Pool value used for free operation */
+	uint64_t size                         : 16; /**< Size in bytes */
+	uint64_t ptr                          : 40; /**< Data pointer */
+#else
+	uint64_t ptr                          : 40;
+	uint64_t size                         : 16;
+	uint64_t pool                         : 3;
+	uint64_t back                         : 4;
+	uint64_t i                            : 1;
+#endif
+	} cn50xx;
+	struct cvmx_pko_mem_debug13_cn50xx    cn52xx;
+	struct cvmx_pko_mem_debug13_cn50xx    cn52xxp1;
+	struct cvmx_pko_mem_debug13_cn50xx    cn56xx;
+	struct cvmx_pko_mem_debug13_cn50xx    cn56xxp1;
+	struct cvmx_pko_mem_debug13_cn50xx    cn58xx;
+	struct cvmx_pko_mem_debug13_cn50xx    cn58xxp1;
+	struct cvmx_pko_mem_debug13_cn50xx    cn61xx;
+	struct cvmx_pko_mem_debug13_cn50xx    cn63xx;
+	struct cvmx_pko_mem_debug13_cn50xx    cn63xxp1;
+	struct cvmx_pko_mem_debug13_cn50xx    cn66xx;
+	struct cvmx_pko_mem_debug13_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t state                        : 64; /**< Internal state */
+#else
+	uint64_t state                        : 64;
+#endif
+	} cn68xx;
+	struct cvmx_pko_mem_debug13_cn68xx    cn68xxp1;
+	struct cvmx_pko_mem_debug13_cn50xx    cn70xx;
+	struct cvmx_pko_mem_debug13_cn50xx    cn70xxp1;
+	struct cvmx_pko_mem_debug13_cn50xx    cnf71xx;
+};
+typedef union cvmx_pko_mem_debug13 cvmx_pko_mem_debug13_t;
+
+/**
+ * cvmx_pko_mem_debug14
+ *
+ * Notes:
+ * Internal per-port state intended for debug use only - pko.prt.psb.save[63:0]
+ * This CSR is a memory of 132 entries, and thus, the PKO_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.
+ */
+union cvmx_pko_mem_debug14 {
+	uint64_t u64;
+	struct cvmx_pko_mem_debug14_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_0_63                : 64;
+#else
+	uint64_t reserved_0_63                : 64;
+#endif
+	} s;
+	struct cvmx_pko_mem_debug14_cn30xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_17_63               : 47;
+	uint64_t ridx                         : 17; /**< PDB ridx */
+#else
+	uint64_t ridx                         : 17;
+	uint64_t reserved_17_63               : 47;
+#endif
+	} cn30xx;
+	struct cvmx_pko_mem_debug14_cn30xx    cn31xx;
+	struct cvmx_pko_mem_debug14_cn30xx    cn38xx;
+	struct cvmx_pko_mem_debug14_cn30xx    cn38xxp2;
+	struct cvmx_pko_mem_debug14_cn52xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t data                         : 64; /**< Command words */
+#else
+	uint64_t data                         : 64;
+#endif
+	} cn52xx;
+	struct cvmx_pko_mem_debug14_cn52xx    cn52xxp1;
+	struct cvmx_pko_mem_debug14_cn52xx    cn56xx;
+	struct cvmx_pko_mem_debug14_cn52xx    cn56xxp1;
+	struct cvmx_pko_mem_debug14_cn52xx    cn61xx;
+	struct cvmx_pko_mem_debug14_cn52xx    cn63xx;
+	struct cvmx_pko_mem_debug14_cn52xx    cn63xxp1;
+	struct cvmx_pko_mem_debug14_cn52xx    cn66xx;
+	struct cvmx_pko_mem_debug14_cn52xx    cn70xx;
+	struct cvmx_pko_mem_debug14_cn52xx    cn70xxp1;
+	struct cvmx_pko_mem_debug14_cn52xx    cnf71xx;
+};
+typedef union cvmx_pko_mem_debug14 cvmx_pko_mem_debug14_t;
+
+/**
+ * cvmx_pko_mem_debug2
+ *
+ * Notes:
+ * Internal per-port state intended for debug use only - pko_prt_psb.head[63:0]
+ * This CSR is a memory of 12 entries, and thus, the PKO_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.
+ */
+union cvmx_pko_mem_debug2 {
+	uint64_t u64;
+	struct cvmx_pko_mem_debug2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t i                            : 1;  /**< "I"  value used for free operation */
+	uint64_t back                         : 4;  /**< Back value used for free operation */
+	uint64_t pool                         : 3;  /**< Pool value used for free operation */
+	uint64_t size                         : 16; /**< Size in bytes */
+	uint64_t ptr                          : 40; /**< Data pointer */
+#else
+	uint64_t ptr                          : 40;
+	uint64_t size                         : 16;
+	uint64_t pool                         : 3;
+	uint64_t back                         : 4;
+	uint64_t i                            : 1;
+#endif
+	} s;
+	struct cvmx_pko_mem_debug2_s          cn30xx;
+	struct cvmx_pko_mem_debug2_s          cn31xx;
+	struct cvmx_pko_mem_debug2_s          cn38xx;
+	struct cvmx_pko_mem_debug2_s          cn38xxp2;
+	struct cvmx_pko_mem_debug2_s          cn50xx;
+	struct cvmx_pko_mem_debug2_s          cn52xx;
+	struct cvmx_pko_mem_debug2_s          cn52xxp1;
+	struct cvmx_pko_mem_debug2_s          cn56xx;
+	struct cvmx_pko_mem_debug2_s          cn56xxp1;
+	struct cvmx_pko_mem_debug2_s          cn58xx;
+	struct cvmx_pko_mem_debug2_s          cn58xxp1;
+	struct cvmx_pko_mem_debug2_s          cn61xx;
+	struct cvmx_pko_mem_debug2_s          cn63xx;
+	struct cvmx_pko_mem_debug2_s          cn63xxp1;
+	struct cvmx_pko_mem_debug2_s          cn66xx;
+	struct cvmx_pko_mem_debug2_s          cn68xx;
+	struct cvmx_pko_mem_debug2_s          cn68xxp1;
+	struct cvmx_pko_mem_debug2_s          cn70xx;
+	struct cvmx_pko_mem_debug2_s          cn70xxp1;
+	struct cvmx_pko_mem_debug2_s          cnf71xx;
+};
+typedef union cvmx_pko_mem_debug2 cvmx_pko_mem_debug2_t;
+
+/**
+ * cvmx_pko_mem_debug3
+ *
+ * Notes:
+ * Internal per-port state intended for debug use only - pko_prt_psb.resp[63:0]
+ * This CSR is a memory of 12 entries, and thus, the PKO_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.
+ */
+union cvmx_pko_mem_debug3 {
+	uint64_t u64;
+	struct cvmx_pko_mem_debug3_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_0_63                : 64;
+#else
+	uint64_t reserved_0_63                : 64;
+#endif
+	} s;
+	struct cvmx_pko_mem_debug3_cn30xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t i                            : 1;  /**< "I"  value used for free operation */
+	uint64_t back                         : 4;  /**< Back value used for free operation */
+	uint64_t pool                         : 3;  /**< Pool value used for free operation */
+	uint64_t size                         : 16; /**< Size in bytes */
+	uint64_t ptr                          : 40; /**< Data pointer */
+#else
+	uint64_t ptr                          : 40;
+	uint64_t size                         : 16;
+	uint64_t pool                         : 3;
+	uint64_t back                         : 4;
+	uint64_t i                            : 1;
+#endif
+	} cn30xx;
+	struct cvmx_pko_mem_debug3_cn30xx     cn31xx;
+	struct cvmx_pko_mem_debug3_cn30xx     cn38xx;
+	struct cvmx_pko_mem_debug3_cn30xx     cn38xxp2;
+	struct cvmx_pko_mem_debug3_cn50xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t data                         : 64; /**< WorkQ data or Store0 pointer */
+#else
+	uint64_t data                         : 64;
+#endif
+	} cn50xx;
+	struct cvmx_pko_mem_debug3_cn50xx     cn52xx;
+	struct cvmx_pko_mem_debug3_cn50xx     cn52xxp1;
+	struct cvmx_pko_mem_debug3_cn50xx     cn56xx;
+	struct cvmx_pko_mem_debug3_cn50xx     cn56xxp1;
+	struct cvmx_pko_mem_debug3_cn50xx     cn58xx;
+	struct cvmx_pko_mem_debug3_cn50xx     cn58xxp1;
+	struct cvmx_pko_mem_debug3_cn50xx     cn61xx;
+	struct cvmx_pko_mem_debug3_cn50xx     cn63xx;
+	struct cvmx_pko_mem_debug3_cn50xx     cn63xxp1;
+	struct cvmx_pko_mem_debug3_cn50xx     cn66xx;
+	struct cvmx_pko_mem_debug3_cn50xx     cn68xx;
+	struct cvmx_pko_mem_debug3_cn50xx     cn68xxp1;
+	struct cvmx_pko_mem_debug3_cn50xx     cn70xx;
+	struct cvmx_pko_mem_debug3_cn50xx     cn70xxp1;
+	struct cvmx_pko_mem_debug3_cn50xx     cnf71xx;
+};
+typedef union cvmx_pko_mem_debug3 cvmx_pko_mem_debug3_t;
+
+/**
+ * cvmx_pko_mem_debug4
+ *
+ * Notes:
+ * Internal per-port state intended for debug use only - pko_prt_psb.state[63:0]
+ * This CSR is a memory of 12 entries, and thus, the PKO_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.
+ */
+union cvmx_pko_mem_debug4 {
+	uint64_t u64;
+	struct cvmx_pko_mem_debug4_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_0_63                : 64;
+#else
+	uint64_t reserved_0_63                : 64;
+#endif
+	} s;
+	struct cvmx_pko_mem_debug4_cn30xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t data                         : 64; /**< WorkQ data or Store0 pointer */
+#else
+	uint64_t data                         : 64;
+#endif
+	} cn30xx;
+	struct cvmx_pko_mem_debug4_cn30xx     cn31xx;
+	struct cvmx_pko_mem_debug4_cn30xx     cn38xx;
+	struct cvmx_pko_mem_debug4_cn30xx     cn38xxp2;
+	struct cvmx_pko_mem_debug4_cn50xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t cmnd_segs                    : 3;  /**< Internal state */
+	uint64_t cmnd_siz                     : 16; /**< Internal state */
+	uint64_t cmnd_off                     : 6;  /**< Internal state */
+	uint64_t uid                          : 3;  /**< Internal state */
+	uint64_t dread_sop                    : 1;  /**< Internal state */
+	uint64_t init_dwrite                  : 1;  /**< Internal state */
+	uint64_t chk_once                     : 1;  /**< Internal state */
+	uint64_t chk_mode                     : 1;  /**< Internal state */
+	uint64_t active                       : 1;  /**< Internal state */
+	uint64_t static_p                     : 1;  /**< Internal state */
+	uint64_t qos                          : 3;  /**< Internal state */
+	uint64_t qcb_ridx                     : 5;  /**< Internal state */
+	uint64_t qid_off_max                  : 4;  /**< Internal state */
+	uint64_t qid_off                      : 4;  /**< Internal state */
+	uint64_t qid_base                     : 8;  /**< Internal state */
+	uint64_t wait                         : 1;  /**< Internal state */
+	uint64_t minor                        : 2;  /**< Internal state */
+	uint64_t major                        : 3;  /**< Internal state */
+#else
+	uint64_t major                        : 3;
+	uint64_t minor                        : 2;
+	uint64_t wait                         : 1;
+	uint64_t qid_base                     : 8;
+	uint64_t qid_off                      : 4;
+	uint64_t qid_off_max                  : 4;
+	uint64_t qcb_ridx                     : 5;
+	uint64_t qos                          : 3;
+	uint64_t static_p                     : 1;
+	uint64_t active                       : 1;
+	uint64_t chk_mode                     : 1;
+	uint64_t chk_once                     : 1;
+	uint64_t init_dwrite                  : 1;
+	uint64_t dread_sop                    : 1;
+	uint64_t uid                          : 3;
+	uint64_t cmnd_off                     : 6;
+	uint64_t cmnd_siz                     : 16;
+	uint64_t cmnd_segs                    : 3;
+#endif
+	} cn50xx;
+	struct cvmx_pko_mem_debug4_cn52xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t curr_siz                     : 8;  /**< Internal state */
+	uint64_t curr_off                     : 16; /**< Internal state */
+	uint64_t cmnd_segs                    : 6;  /**< Internal state */
+	uint64_t cmnd_siz                     : 16; /**< Internal state */
+	uint64_t cmnd_off                     : 6;  /**< Internal state */
+	uint64_t uid                          : 2;  /**< Internal state */
+	uint64_t dread_sop                    : 1;  /**< Internal state */
+	uint64_t init_dwrite                  : 1;  /**< Internal state */
+	uint64_t chk_once                     : 1;  /**< Internal state */
+	uint64_t chk_mode                     : 1;  /**< Internal state */
+	uint64_t wait                         : 1;  /**< Internal state */
+	uint64_t minor                        : 2;  /**< Internal state */
+	uint64_t major                        : 3;  /**< Internal state */
+#else
+	uint64_t major                        : 3;
+	uint64_t minor                        : 2;
+	uint64_t wait                         : 1;
+	uint64_t chk_mode                     : 1;
+	uint64_t chk_once                     : 1;
+	uint64_t init_dwrite                  : 1;
+	uint64_t dread_sop                    : 1;
+	uint64_t uid                          : 2;
+	uint64_t cmnd_off                     : 6;
+	uint64_t cmnd_siz                     : 16;
+	uint64_t cmnd_segs                    : 6;
+	uint64_t curr_off                     : 16;
+	uint64_t curr_siz                     : 8;
+#endif
+	} cn52xx;
+	struct cvmx_pko_mem_debug4_cn52xx     cn52xxp1;
+	struct cvmx_pko_mem_debug4_cn52xx     cn56xx;
+	struct cvmx_pko_mem_debug4_cn52xx     cn56xxp1;
+	struct cvmx_pko_mem_debug4_cn50xx     cn58xx;
+	struct cvmx_pko_mem_debug4_cn50xx     cn58xxp1;
+	struct cvmx_pko_mem_debug4_cn52xx     cn61xx;
+	struct cvmx_pko_mem_debug4_cn52xx     cn63xx;
+	struct cvmx_pko_mem_debug4_cn52xx     cn63xxp1;
+	struct cvmx_pko_mem_debug4_cn52xx     cn66xx;
+	struct cvmx_pko_mem_debug4_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t curr_siz                     : 9;  /**< Internal state */
+	uint64_t curr_off                     : 16; /**< Internal state */
+	uint64_t cmnd_segs                    : 6;  /**< Internal state */
+	uint64_t cmnd_siz                     : 16; /**< Internal state */
+	uint64_t cmnd_off                     : 6;  /**< Internal state */
+	uint64_t dread_sop                    : 1;  /**< Internal state */
+	uint64_t init_dwrite                  : 1;  /**< Internal state */
+	uint64_t chk_once                     : 1;  /**< Internal state */
+	uint64_t chk_mode                     : 1;  /**< Internal state */
+	uint64_t reserved_6_6                 : 1;
+	uint64_t minor                        : 2;  /**< Internal state */
+	uint64_t major                        : 4;  /**< Internal state */
+#else
+	uint64_t major                        : 4;
+	uint64_t minor                        : 2;
+	uint64_t reserved_6_6                 : 1;
+	uint64_t chk_mode                     : 1;
+	uint64_t chk_once                     : 1;
+	uint64_t init_dwrite                  : 1;
+	uint64_t dread_sop                    : 1;
+	uint64_t cmnd_off                     : 6;
+	uint64_t cmnd_siz                     : 16;
+	uint64_t cmnd_segs                    : 6;
+	uint64_t curr_off                     : 16;
+	uint64_t curr_siz                     : 9;
+#endif
+	} cn68xx;
+	struct cvmx_pko_mem_debug4_cn68xx     cn68xxp1;
+	struct cvmx_pko_mem_debug4_cn52xx     cn70xx;
+	struct cvmx_pko_mem_debug4_cn52xx     cn70xxp1;
+	struct cvmx_pko_mem_debug4_cn52xx     cnf71xx;
+};
+typedef union cvmx_pko_mem_debug4 cvmx_pko_mem_debug4_t;
+
+/**
+ * cvmx_pko_mem_debug5
+ *
+ * Notes:
+ * Internal per-port state intended for debug use only - pko_prt_psb.state[127:64]
+ * This CSR is a memory of 12 entries, and thus, the PKO_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.
+ */
+union cvmx_pko_mem_debug5 {
+	uint64_t u64;
+	struct cvmx_pko_mem_debug5_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_0_63                : 64;
+#else
+	uint64_t reserved_0_63                : 64;
+#endif
+	} s;
+	struct cvmx_pko_mem_debug5_cn30xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t dwri_mod                     : 1;  /**< Dwrite mod */
+	uint64_t dwri_sop                     : 1;  /**< Dwrite sop needed */
+	uint64_t dwri_len                     : 1;  /**< Dwrite len */
+	uint64_t dwri_cnt                     : 13; /**< Dwrite count */
+	uint64_t cmnd_siz                     : 16; /**< Copy of cmnd.size */
+	uint64_t uid                          : 1;  /**< UID */
+	uint64_t xfer_wor                     : 1;  /**< Transfer work needed */
+	uint64_t xfer_dwr                     : 1;  /**< Transfer dwrite needed */
+	uint64_t cbuf_fre                     : 1;  /**< Cbuf needs free */
+	uint64_t reserved_27_27               : 1;
+	uint64_t chk_mode                     : 1;  /**< Checksum mode */
+	uint64_t active                       : 1;  /**< Port is active */
+	uint64_t qos                          : 3;  /**< Current QOS round */
+	uint64_t qcb_ridx                     : 5;  /**< Buffer read  index for QCB */
+	uint64_t qid_off                      : 3;  /**< Offset to be added to QID_BASE for current queue */
+	uint64_t qid_base                     : 7;  /**< Absolute QID of the queue array base = &QUEUES[0] */
+	uint64_t wait                         : 1;  /**< State wait when set */
+	uint64_t minor                        : 2;  /**< State minor code */
+	uint64_t major                        : 4;  /**< State major code */
+#else
+	uint64_t major                        : 4;
+	uint64_t minor                        : 2;
+	uint64_t wait                         : 1;
+	uint64_t qid_base                     : 7;
+	uint64_t qid_off                      : 3;
+	uint64_t qcb_ridx                     : 5;
+	uint64_t qos                          : 3;
+	uint64_t active                       : 1;
+	uint64_t chk_mode                     : 1;
+	uint64_t reserved_27_27               : 1;
+	uint64_t cbuf_fre                     : 1;
+	uint64_t xfer_dwr                     : 1;
+	uint64_t xfer_wor                     : 1;
+	uint64_t uid                          : 1;
+	uint64_t cmnd_siz                     : 16;
+	uint64_t dwri_cnt                     : 13;
+	uint64_t dwri_len                     : 1;
+	uint64_t dwri_sop                     : 1;
+	uint64_t dwri_mod                     : 1;
+#endif
+	} cn30xx;
+	struct cvmx_pko_mem_debug5_cn30xx     cn31xx;
+	struct cvmx_pko_mem_debug5_cn30xx     cn38xx;
+	struct cvmx_pko_mem_debug5_cn30xx     cn38xxp2;
+	struct cvmx_pko_mem_debug5_cn50xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t curr_ptr                     : 29; /**< Internal state */
+	uint64_t curr_siz                     : 16; /**< Internal state */
+	uint64_t curr_off                     : 16; /**< Internal state */
+	uint64_t cmnd_segs                    : 3;  /**< Internal state */
+#else
+	uint64_t cmnd_segs                    : 3;
+	uint64_t curr_off                     : 16;
+	uint64_t curr_siz                     : 16;
+	uint64_t curr_ptr                     : 29;
+#endif
+	} cn50xx;
+	struct cvmx_pko_mem_debug5_cn52xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_54_63               : 10;
+	uint64_t nxt_inflt                    : 6;  /**< Internal state */
+	uint64_t curr_ptr                     : 40; /**< Internal state */
+	uint64_t curr_siz                     : 8;  /**< Internal state */
+#else
+	uint64_t curr_siz                     : 8;
+	uint64_t curr_ptr                     : 40;
+	uint64_t nxt_inflt                    : 6;
+	uint64_t reserved_54_63               : 10;
+#endif
+	} cn52xx;
+	struct cvmx_pko_mem_debug5_cn52xx     cn52xxp1;
+	struct cvmx_pko_mem_debug5_cn52xx     cn56xx;
+	struct cvmx_pko_mem_debug5_cn52xx     cn56xxp1;
+	struct cvmx_pko_mem_debug5_cn50xx     cn58xx;
+	struct cvmx_pko_mem_debug5_cn50xx     cn58xxp1;
+	struct cvmx_pko_mem_debug5_cn61xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_56_63               : 8;
+	uint64_t ptp                          : 1;  /**< Internal state */
+	uint64_t major_3                      : 1;  /**< Internal state */
+	uint64_t nxt_inflt                    : 6;  /**< Internal state */
+	uint64_t curr_ptr                     : 40; /**< Internal state */
+	uint64_t curr_siz                     : 8;  /**< Internal state */
+#else
+	uint64_t curr_siz                     : 8;
+	uint64_t curr_ptr                     : 40;
+	uint64_t nxt_inflt                    : 6;
+	uint64_t major_3                      : 1;
+	uint64_t ptp                          : 1;
+	uint64_t reserved_56_63               : 8;
+#endif
+	} cn61xx;
+	struct cvmx_pko_mem_debug5_cn61xx     cn63xx;
+	struct cvmx_pko_mem_debug5_cn61xx     cn63xxp1;
+	struct cvmx_pko_mem_debug5_cn61xx     cn66xx;
+	struct cvmx_pko_mem_debug5_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_57_63               : 7;
+	uint64_t uid                          : 3;  /**< Internal state */
+	uint64_t ptp                          : 1;  /**< Internal state */
+	uint64_t nxt_inflt                    : 6;  /**< Internal state */
+	uint64_t curr_ptr                     : 40; /**< Internal state */
+	uint64_t curr_siz                     : 7;  /**< Internal state */
+#else
+	uint64_t curr_siz                     : 7;
+	uint64_t curr_ptr                     : 40;
+	uint64_t nxt_inflt                    : 6;
+	uint64_t ptp                          : 1;
+	uint64_t uid                          : 3;
+	uint64_t reserved_57_63               : 7;
+#endif
+	} cn68xx;
+	struct cvmx_pko_mem_debug5_cn68xx     cn68xxp1;
+	struct cvmx_pko_mem_debug5_cn61xx     cn70xx;
+	struct cvmx_pko_mem_debug5_cn61xx     cn70xxp1;
+	struct cvmx_pko_mem_debug5_cn61xx     cnf71xx;
+};
+typedef union cvmx_pko_mem_debug5 cvmx_pko_mem_debug5_t;
+
+/**
+ * cvmx_pko_mem_debug6
+ *
+ * Notes:
+ * Internal per-port state intended for debug use only - pko_prt_psb.port[63:0]
+ * This CSR is a memory of 44 entries, and thus, the PKO_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.
+ */
+union cvmx_pko_mem_debug6 {
+	uint64_t u64;
+	struct cvmx_pko_mem_debug6_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_38_63               : 26;
+	uint64_t qos_active                   : 1;  /**< Internal state */
+	uint64_t reserved_0_36                : 37;
+#else
+	uint64_t reserved_0_36                : 37;
+	uint64_t qos_active                   : 1;
+	uint64_t reserved_38_63               : 26;
+#endif
+	} s;
+	struct cvmx_pko_mem_debug6_cn30xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_11_63               : 53;
+	uint64_t qid_offm                     : 3;  /**< Qid offset max */
+	uint64_t static_p                     : 1;  /**< Static port when set */
+	uint64_t work_min                     : 3;  /**< Work minor */
+	uint64_t dwri_chk                     : 1;  /**< Dwrite checksum mode */
+	uint64_t dwri_uid                     : 1;  /**< Dwrite UID */
+	uint64_t dwri_mod                     : 2;  /**< Dwrite mod */
+#else
+	uint64_t dwri_mod                     : 2;
+	uint64_t dwri_uid                     : 1;
+	uint64_t dwri_chk                     : 1;
+	uint64_t work_min                     : 3;
+	uint64_t static_p                     : 1;
+	uint64_t qid_offm                     : 3;
+	uint64_t reserved_11_63               : 53;
+#endif
+	} cn30xx;
+	struct cvmx_pko_mem_debug6_cn30xx     cn31xx;
+	struct cvmx_pko_mem_debug6_cn30xx     cn38xx;
+	struct cvmx_pko_mem_debug6_cn30xx     cn38xxp2;
+	struct cvmx_pko_mem_debug6_cn50xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_11_63               : 53;
+	uint64_t curr_ptr                     : 11; /**< Internal state */
+#else
+	uint64_t curr_ptr                     : 11;
+	uint64_t reserved_11_63               : 53;
+#endif
+	} cn50xx;
+	struct cvmx_pko_mem_debug6_cn52xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_37_63               : 27;
+	uint64_t qid_offres                   : 4;  /**< Internal state */
+	uint64_t qid_offths                   : 4;  /**< Internal state */
+	uint64_t preempter                    : 1;  /**< Internal state */
+	uint64_t preemptee                    : 1;  /**< Internal state */
+	uint64_t preempted                    : 1;  /**< Internal state */
+	uint64_t active                       : 1;  /**< Internal state */
+	uint64_t statc                        : 1;  /**< Internal state */
+	uint64_t qos                          : 3;  /**< Internal state */
+	uint64_t qcb_ridx                     : 5;  /**< Internal state */
+	uint64_t qid_offmax                   : 4;  /**< Internal state */
+	uint64_t qid_off                      : 4;  /**< Internal state */
+	uint64_t qid_base                     : 8;  /**< Internal state */
+#else
+	uint64_t qid_base                     : 8;
+	uint64_t qid_off                      : 4;
+	uint64_t qid_offmax                   : 4;
+	uint64_t qcb_ridx                     : 5;
+	uint64_t qos                          : 3;
+	uint64_t statc                        : 1;
+	uint64_t active                       : 1;
+	uint64_t preempted                    : 1;
+	uint64_t preemptee                    : 1;
+	uint64_t preempter                    : 1;
+	uint64_t qid_offths                   : 4;
+	uint64_t qid_offres                   : 4;
+	uint64_t reserved_37_63               : 27;
+#endif
+	} cn52xx;
+	struct cvmx_pko_mem_debug6_cn52xx     cn52xxp1;
+	struct cvmx_pko_mem_debug6_cn52xx     cn56xx;
+	struct cvmx_pko_mem_debug6_cn52xx     cn56xxp1;
+	struct cvmx_pko_mem_debug6_cn50xx     cn58xx;
+	struct cvmx_pko_mem_debug6_cn50xx     cn58xxp1;
+	struct cvmx_pko_mem_debug6_cn52xx     cn61xx;
+	struct cvmx_pko_mem_debug6_cn52xx     cn63xx;
+	struct cvmx_pko_mem_debug6_cn52xx     cn63xxp1;
+	struct cvmx_pko_mem_debug6_cn52xx     cn66xx;
+	struct cvmx_pko_mem_debug6_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_38_63               : 26;
+	uint64_t qos_active                   : 1;  /**< Internal state */
+	uint64_t qid_offths                   : 5;  /**< Internal state */
+	uint64_t preempter                    : 1;  /**< Internal state */
+	uint64_t preemptee                    : 1;  /**< Internal state */
+	uint64_t active                       : 1;  /**< Internal state */
+	uint64_t static_p                     : 1;  /**< Internal state */
+	uint64_t qos                          : 3;  /**< Internal state */
+	uint64_t qcb_ridx                     : 7;  /**< Internal state */
+	uint64_t qid_offmax                   : 5;  /**< Internal state */
+	uint64_t qid_off                      : 5;  /**< Internal state */
+	uint64_t qid_base                     : 8;  /**< Internal state */
+#else
+	uint64_t qid_base                     : 8;
+	uint64_t qid_off                      : 5;
+	uint64_t qid_offmax                   : 5;
+	uint64_t qcb_ridx                     : 7;
+	uint64_t qos                          : 3;
+	uint64_t static_p                     : 1;
+	uint64_t active                       : 1;
+	uint64_t preemptee                    : 1;
+	uint64_t preempter                    : 1;
+	uint64_t qid_offths                   : 5;
+	uint64_t qos_active                   : 1;
+	uint64_t reserved_38_63               : 26;
+#endif
+	} cn68xx;
+	struct cvmx_pko_mem_debug6_cn68xx     cn68xxp1;
+	struct cvmx_pko_mem_debug6_cn70xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_37_63               : 27;
+	uint64_t qid_offres                   : 4;  /**< Internal state */
+	uint64_t qid_offths                   : 4;  /**< Internal state */
+	uint64_t preempter                    : 1;  /**< Internal state */
+	uint64_t preemptee                    : 1;  /**< Internal state */
+	uint64_t preempted                    : 1;  /**< Internal state */
+	uint64_t active                       : 1;  /**< Internal state */
+	uint64_t staticb                      : 1;  /**< Internal state */
+	uint64_t qos                          : 3;  /**< Internal state */
+	uint64_t qcb_ridx                     : 5;  /**< Internal state */
+	uint64_t qid_offmax                   : 4;  /**< Internal state */
+	uint64_t qid_off                      : 4;  /**< Internal state */
+	uint64_t qid_base                     : 8;  /**< Internal state */
+#else
+	uint64_t qid_base                     : 8;
+	uint64_t qid_off                      : 4;
+	uint64_t qid_offmax                   : 4;
+	uint64_t qcb_ridx                     : 5;
+	uint64_t qos                          : 3;
+	uint64_t staticb                      : 1;
+	uint64_t active                       : 1;
+	uint64_t preempted                    : 1;
+	uint64_t preemptee                    : 1;
+	uint64_t preempter                    : 1;
+	uint64_t qid_offths                   : 4;
+	uint64_t qid_offres                   : 4;
+	uint64_t reserved_37_63               : 27;
+#endif
+	} cn70xx;
+	struct cvmx_pko_mem_debug6_cn70xx     cn70xxp1;
+	struct cvmx_pko_mem_debug6_cn52xx     cnf71xx;
+};
+typedef union cvmx_pko_mem_debug6 cvmx_pko_mem_debug6_t;
+
+/**
+ * cvmx_pko_mem_debug7
+ *
+ * Notes:
+ * Internal per-queue state intended for debug use only - pko_prt_qsb.state[63:0]
+ * This CSR is a memory of 256 entries, and thus, the PKO_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.
+ */
+union cvmx_pko_mem_debug7 {
+	uint64_t u64;
+	struct cvmx_pko_mem_debug7_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_0_63                : 64;
+#else
+	uint64_t reserved_0_63                : 64;
+#endif
+	} s;
+	struct cvmx_pko_mem_debug7_cn30xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_58_63               : 6;
+	uint64_t dwb                          : 9;  /**< Calculated DWB count used for free operation */
+	uint64_t start                        : 33; /**< Calculated start address used for free operation */
+	uint64_t size                         : 16; /**< Packet length in bytes */
+#else
+	uint64_t size                         : 16;
+	uint64_t start                        : 33;
+	uint64_t dwb                          : 9;
+	uint64_t reserved_58_63               : 6;
+#endif
+	} cn30xx;
+	struct cvmx_pko_mem_debug7_cn30xx     cn31xx;
+	struct cvmx_pko_mem_debug7_cn30xx     cn38xx;
+	struct cvmx_pko_mem_debug7_cn30xx     cn38xxp2;
+	struct cvmx_pko_mem_debug7_cn50xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t qos                          : 5;  /**< QOS mask to enable the queue when set */
+	uint64_t tail                         : 1;  /**< This queue is the last (tail) in the queue array */
+	uint64_t buf_siz                      : 13; /**< Command buffer remaining size in words */
+	uint64_t buf_ptr                      : 33; /**< Command word pointer */
+	uint64_t qcb_widx                     : 6;  /**< Buffer write index for QCB */
+	uint64_t qcb_ridx                     : 6;  /**< Buffer read  index for QCB */
+#else
+	uint64_t qcb_ridx                     : 6;
+	uint64_t qcb_widx                     : 6;
+	uint64_t buf_ptr                      : 33;
+	uint64_t buf_siz                      : 13;
+	uint64_t tail                         : 1;
+	uint64_t qos                          : 5;
+#endif
+	} cn50xx;
+	struct cvmx_pko_mem_debug7_cn50xx     cn52xx;
+	struct cvmx_pko_mem_debug7_cn50xx     cn52xxp1;
+	struct cvmx_pko_mem_debug7_cn50xx     cn56xx;
+	struct cvmx_pko_mem_debug7_cn50xx     cn56xxp1;
+	struct cvmx_pko_mem_debug7_cn50xx     cn58xx;
+	struct cvmx_pko_mem_debug7_cn50xx     cn58xxp1;
+	struct cvmx_pko_mem_debug7_cn50xx     cn61xx;
+	struct cvmx_pko_mem_debug7_cn50xx     cn63xx;
+	struct cvmx_pko_mem_debug7_cn50xx     cn63xxp1;
+	struct cvmx_pko_mem_debug7_cn50xx     cn66xx;
+	struct cvmx_pko_mem_debug7_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t buf_siz                      : 11; /**< Command buffer remaining size in words */
+	uint64_t buf_ptr                      : 37; /**< Command word pointer */
+	uint64_t qcb_widx                     : 8;  /**< Buffer write index for QCB */
+	uint64_t qcb_ridx                     : 8;  /**< Buffer read  index for QCB */
+#else
+	uint64_t qcb_ridx                     : 8;
+	uint64_t qcb_widx                     : 8;
+	uint64_t buf_ptr                      : 37;
+	uint64_t buf_siz                      : 11;
+#endif
+	} cn68xx;
+	struct cvmx_pko_mem_debug7_cn68xx     cn68xxp1;
+	struct cvmx_pko_mem_debug7_cn50xx     cn70xx;
+	struct cvmx_pko_mem_debug7_cn50xx     cn70xxp1;
+	struct cvmx_pko_mem_debug7_cn50xx     cnf71xx;
+};
+typedef union cvmx_pko_mem_debug7 cvmx_pko_mem_debug7_t;
+
+/**
+ * cvmx_pko_mem_debug8
+ *
+ * Notes:
+ * Internal per-queue state intended for debug use only - pko_prt_qsb.state[91:64]
+ * This CSR is a memory of 256 entries, and thus, the PKO_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.
+ */
+union cvmx_pko_mem_debug8 {
+	uint64_t u64;
+	struct cvmx_pko_mem_debug8_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_59_63               : 5;
+	uint64_t tail                         : 1;  /**< This queue is the last (tail) in the queue array */
+	uint64_t reserved_0_57                : 58;
+#else
+	uint64_t reserved_0_57                : 58;
+	uint64_t tail                         : 1;
+	uint64_t reserved_59_63               : 5;
+#endif
+	} s;
+	struct cvmx_pko_mem_debug8_cn30xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t qos                          : 5;  /**< QOS mask to enable the queue when set */
+	uint64_t tail                         : 1;  /**< This queue is the last (tail) in the queue array */
+	uint64_t buf_siz                      : 13; /**< Command buffer remaining size in words */
+	uint64_t buf_ptr                      : 33; /**< Command word pointer */
+	uint64_t qcb_widx                     : 6;  /**< Buffer write index for QCB */
+	uint64_t qcb_ridx                     : 6;  /**< Buffer read  index for QCB */
+#else
+	uint64_t qcb_ridx                     : 6;
+	uint64_t qcb_widx                     : 6;
+	uint64_t buf_ptr                      : 33;
+	uint64_t buf_siz                      : 13;
+	uint64_t tail                         : 1;
+	uint64_t qos                          : 5;
+#endif
+	} cn30xx;
+	struct cvmx_pko_mem_debug8_cn30xx     cn31xx;
+	struct cvmx_pko_mem_debug8_cn30xx     cn38xx;
+	struct cvmx_pko_mem_debug8_cn30xx     cn38xxp2;
+	struct cvmx_pko_mem_debug8_cn50xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_28_63               : 36;
+	uint64_t doorbell                     : 20; /**< Doorbell count */
+	uint64_t reserved_6_7                 : 2;
+	uint64_t static_p                     : 1;  /**< Static priority */
+	uint64_t s_tail                       : 1;  /**< Static tail */
+	uint64_t static_q                     : 1;  /**< Static priority */
+	uint64_t qos                          : 3;  /**< QOS mask to enable the queue when set */
+#else
+	uint64_t qos                          : 3;
+	uint64_t static_q                     : 1;
+	uint64_t s_tail                       : 1;
+	uint64_t static_p                     : 1;
+	uint64_t reserved_6_7                 : 2;
+	uint64_t doorbell                     : 20;
+	uint64_t reserved_28_63               : 36;
+#endif
+	} cn50xx;
+	struct cvmx_pko_mem_debug8_cn52xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_29_63               : 35;
+	uint64_t preempter                    : 1;  /**< Preempter */
+	uint64_t doorbell                     : 20; /**< Doorbell count */
+	uint64_t reserved_7_7                 : 1;
+	uint64_t preemptee                    : 1;  /**< Preemptee */
+	uint64_t static_p                     : 1;  /**< Static priority */
+	uint64_t s_tail                       : 1;  /**< Static tail */
+	uint64_t static_q                     : 1;  /**< Static priority */
+	uint64_t qos                          : 3;  /**< QOS mask to enable the queue when set */
+#else
+	uint64_t qos                          : 3;
+	uint64_t static_q                     : 1;
+	uint64_t s_tail                       : 1;
+	uint64_t static_p                     : 1;
+	uint64_t preemptee                    : 1;
+	uint64_t reserved_7_7                 : 1;
+	uint64_t doorbell                     : 20;
+	uint64_t preempter                    : 1;
+	uint64_t reserved_29_63               : 35;
+#endif
+	} cn52xx;
+	struct cvmx_pko_mem_debug8_cn52xx     cn52xxp1;
+	struct cvmx_pko_mem_debug8_cn52xx     cn56xx;
+	struct cvmx_pko_mem_debug8_cn52xx     cn56xxp1;
+	struct cvmx_pko_mem_debug8_cn50xx     cn58xx;
+	struct cvmx_pko_mem_debug8_cn50xx     cn58xxp1;
+	struct cvmx_pko_mem_debug8_cn61xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_42_63               : 22;
+	uint64_t qid_qqos                     : 8;  /**< QOS_MASK */
+	uint64_t reserved_33_33               : 1;
+	uint64_t qid_idx                      : 4;  /**< IDX */
+	uint64_t preempter                    : 1;  /**< Preempter */
+	uint64_t doorbell                     : 20; /**< Doorbell count */
+	uint64_t reserved_7_7                 : 1;
+	uint64_t preemptee                    : 1;  /**< Preemptee */
+	uint64_t static_p                     : 1;  /**< Static priority */
+	uint64_t s_tail                       : 1;  /**< Static tail */
+	uint64_t static_q                     : 1;  /**< Static priority */
+	uint64_t qos                          : 3;  /**< QOS mask to enable the queue when set */
+#else
+	uint64_t qos                          : 3;
+	uint64_t static_q                     : 1;
+	uint64_t s_tail                       : 1;
+	uint64_t static_p                     : 1;
+	uint64_t preemptee                    : 1;
+	uint64_t reserved_7_7                 : 1;
+	uint64_t doorbell                     : 20;
+	uint64_t preempter                    : 1;
+	uint64_t qid_idx                      : 4;
+	uint64_t reserved_33_33               : 1;
+	uint64_t qid_qqos                     : 8;
+	uint64_t reserved_42_63               : 22;
+#endif
+	} cn61xx;
+	struct cvmx_pko_mem_debug8_cn52xx     cn63xx;
+	struct cvmx_pko_mem_debug8_cn52xx     cn63xxp1;
+	struct cvmx_pko_mem_debug8_cn61xx     cn66xx;
+	struct cvmx_pko_mem_debug8_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_50_63               : 14;
+	uint64_t qid_qqos                     : 8;  /**< Queue QOS */
+	uint64_t qid_idx                      : 5;  /**< Queue index in queue array for the port */
+	uint64_t preempter                    : 1;  /**< Preempter */
+	uint64_t doorbell                     : 20; /**< Doorbell count */
+	uint64_t reserved_9_15                : 7;
+	uint64_t qid_qos                      : 6;  /**< QOS mask[5:0] to enable the queue when set */
+	uint64_t qid_tail                     : 1;  /**< This queue is the last (tail) in the queue array */
+	uint64_t buf_siz                      : 2;  /**< Command buffer remaining size in words */
+#else
+	uint64_t buf_siz                      : 2;
+	uint64_t qid_tail                     : 1;
+	uint64_t qid_qos                      : 6;
+	uint64_t reserved_9_15                : 7;
+	uint64_t doorbell                     : 20;
+	uint64_t preempter                    : 1;
+	uint64_t qid_idx                      : 5;
+	uint64_t qid_qqos                     : 8;
+	uint64_t reserved_50_63               : 14;
+#endif
+	} cn68xx;
+	struct cvmx_pko_mem_debug8_cn68xx     cn68xxp1;
+	struct cvmx_pko_mem_debug8_cn61xx     cn70xx;
+	struct cvmx_pko_mem_debug8_cn61xx     cn70xxp1;
+	struct cvmx_pko_mem_debug8_cn61xx     cnf71xx;
+};
+typedef union cvmx_pko_mem_debug8 cvmx_pko_mem_debug8_t;
+
+/**
+ * cvmx_pko_mem_debug9
+ *
+ * Notes:
+ * Internal per-engine state intended for debug use only - pko.dat.ptr.ptrs0, pko.dat.ptr.ptrs3
+ * This CSR is a memory of 10 entries, and thus, the PKO_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.
+ */
+union cvmx_pko_mem_debug9 {
+	uint64_t u64;
+	struct cvmx_pko_mem_debug9_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_49_63               : 15;
+	uint64_t ptrs0                        : 17; /**< Internal state */
+	uint64_t reserved_0_31                : 32;
+#else
+	uint64_t reserved_0_31                : 32;
+	uint64_t ptrs0                        : 17;
+	uint64_t reserved_49_63               : 15;
+#endif
+	} s;
+	struct cvmx_pko_mem_debug9_cn30xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_28_63               : 36;
+	uint64_t doorbell                     : 20; /**< Doorbell count */
+	uint64_t reserved_5_7                 : 3;
+	uint64_t s_tail                       : 1;  /**< reads as zero (S_TAIL cannot be read) */
+	uint64_t static_q                     : 1;  /**< reads as zero (STATIC_Q cannot be read) */
+	uint64_t qos                          : 3;  /**< QOS mask to enable the queue when set */
+#else
+	uint64_t qos                          : 3;
+	uint64_t static_q                     : 1;
+	uint64_t s_tail                       : 1;
+	uint64_t reserved_5_7                 : 3;
+	uint64_t doorbell                     : 20;
+	uint64_t reserved_28_63               : 36;
+#endif
+	} cn30xx;
+	struct cvmx_pko_mem_debug9_cn30xx     cn31xx;
+	struct cvmx_pko_mem_debug9_cn38xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_28_63               : 36;
+	uint64_t doorbell                     : 20; /**< Doorbell count */
+	uint64_t reserved_6_7                 : 2;
+	uint64_t static_p                     : 1;  /**< Static priority (port) */
+	uint64_t s_tail                       : 1;  /**< Static tail */
+	uint64_t static_q                     : 1;  /**< Static priority */
+	uint64_t qos                          : 3;  /**< QOS mask to enable the queue when set */
+#else
+	uint64_t qos                          : 3;
+	uint64_t static_q                     : 1;
+	uint64_t s_tail                       : 1;
+	uint64_t static_p                     : 1;
+	uint64_t reserved_6_7                 : 2;
+	uint64_t doorbell                     : 20;
+	uint64_t reserved_28_63               : 36;
+#endif
+	} cn38xx;
+	struct cvmx_pko_mem_debug9_cn38xx     cn38xxp2;
+	struct cvmx_pko_mem_debug9_cn50xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_49_63               : 15;
+	uint64_t ptrs0                        : 17; /**< Internal state */
+	uint64_t reserved_17_31               : 15;
+	uint64_t ptrs3                        : 17; /**< Internal state */
+#else
+	uint64_t ptrs3                        : 17;
+	uint64_t reserved_17_31               : 15;
+	uint64_t ptrs0                        : 17;
+	uint64_t reserved_49_63               : 15;
+#endif
+	} cn50xx;
+	struct cvmx_pko_mem_debug9_cn50xx     cn52xx;
+	struct cvmx_pko_mem_debug9_cn50xx     cn52xxp1;
+	struct cvmx_pko_mem_debug9_cn50xx     cn56xx;
+	struct cvmx_pko_mem_debug9_cn50xx     cn56xxp1;
+	struct cvmx_pko_mem_debug9_cn50xx     cn58xx;
+	struct cvmx_pko_mem_debug9_cn50xx     cn58xxp1;
+	struct cvmx_pko_mem_debug9_cn50xx     cn61xx;
+	struct cvmx_pko_mem_debug9_cn50xx     cn63xx;
+	struct cvmx_pko_mem_debug9_cn50xx     cn63xxp1;
+	struct cvmx_pko_mem_debug9_cn50xx     cn66xx;
+	struct cvmx_pko_mem_debug9_cn50xx     cn68xx;
+	struct cvmx_pko_mem_debug9_cn50xx     cn68xxp1;
+	struct cvmx_pko_mem_debug9_cn50xx     cn70xx;
+	struct cvmx_pko_mem_debug9_cn50xx     cn70xxp1;
+	struct cvmx_pko_mem_debug9_cn50xx     cnf71xx;
+};
+typedef union cvmx_pko_mem_debug9 cvmx_pko_mem_debug9_t;
+
+/**
+ * cvmx_pko_mem_iport_ptrs
+ *
+ * Notes:
+ * This CSR is a memory of 128 entries, and thus, the PKO_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.  The index to this CSR is an IPORT.  A read of any
+ * entry that has not been previously written is illegal and will result in unpredictable CSR read data.
+ */
+union cvmx_pko_mem_iport_ptrs {
+	uint64_t u64;
+	struct cvmx_pko_mem_iport_ptrs_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_63_63               : 1;
+	uint64_t crc                          : 1;  /**< Set if this IPID uses CRC */
+	uint64_t static_p                     : 1;  /**< Set if this IPID has static priority */
+	uint64_t qos_mask                     : 8;  /**< Mask to control priority across 8 QOS rounds */
+	uint64_t min_pkt                      : 3;  /**< Min packet size specified by PKO_REG_MIN_PKT[MIN_PKT] */
+	uint64_t reserved_31_49               : 19;
+	uint64_t pipe                         : 7;  /**< The PKO pipe or loopback port
+                                                         When INT != PIP/IPD:
+                                                          PIPE is the PKO pipe to which this port is mapped
+                                                          All used PKO-internal ports that map to the same
+                                                          PIPE must also map to the same INT and EID in
+                                                          this case.
+                                                         When INT == PIP/IPD:
+                                                          PIPE must be in the range
+                                                                  0..PKO_REG_LOOPBACK[NUM_PORTS]-1
+                                                          in this case and selects one of the loopback
+                                                          ports. */
+	uint64_t reserved_21_23               : 3;
+	uint64_t intr                         : 5;  /**< The interface to which this port is mapped
+                                                         All used PKO-internal ports that map to the same EID
+                                                         must also map to the same INT. All used PKO-internal
+                                                         ports that map to the same INT must also map to the
+                                                         same EID.
+                                                         Encoding:
+                                                           0 = GMX0 XAUI/DXAUI/RXAUI0 or SGMII0
+                                                           1 = GMX0 SGMII1
+                                                           2 = GMX0 SGMII2
+                                                           3 = GMX0 SGMII3
+                                                           4 = GMX1 RXAUI
+                                                           8 = GMX2 XAUI/DXAUI or SGMII0
+                                                           9 = GMX2 SGMII1
+                                                          10 = GMX2 SGMII2
+                                                          11 = GMX2 SGMII3
+                                                          12 = GMX3 XAUI/DXAUI or SGMII0
+                                                          13 = GMX3 SGMII1
+                                                          14 = GMX3 SGMII2
+                                                          15 = GMX3 SGMII3
+                                                          16 = GMX4 XAUI/DXAUI or SGMII0
+                                                          17 = GMX4 SGMII1
+                                                          18 = GMX4 SGMII2
+                                                          19 = GMX4 SGMII3
+                                                          28 = ILK interface 0
+                                                          29 = ILK interface 1
+                                                          30 = DPI
+                                                          31 = PIP/IPD
+                                                          others = reserved */
+	uint64_t reserved_13_15               : 3;
+	uint64_t eid                          : 5;  /**< Engine ID to which this port is mapped
+                                                         EID==31 can be used with unused PKO-internal ports.
+                                                         Otherwise, 0-19 are legal EID values. */
+	uint64_t reserved_7_7                 : 1;
+	uint64_t ipid                         : 7;  /**< PKO-internal Port ID to be accessed */
+#else
+	uint64_t ipid                         : 7;
+	uint64_t reserved_7_7                 : 1;
+	uint64_t eid                          : 5;
+	uint64_t reserved_13_15               : 3;
+	uint64_t intr                         : 5;
+	uint64_t reserved_21_23               : 3;
+	uint64_t pipe                         : 7;
+	uint64_t reserved_31_49               : 19;
+	uint64_t min_pkt                      : 3;
+	uint64_t qos_mask                     : 8;
+	uint64_t static_p                     : 1;
+	uint64_t crc                          : 1;
+	uint64_t reserved_63_63               : 1;
+#endif
+	} s;
+	struct cvmx_pko_mem_iport_ptrs_s      cn68xx;
+	struct cvmx_pko_mem_iport_ptrs_s      cn68xxp1;
+};
+typedef union cvmx_pko_mem_iport_ptrs cvmx_pko_mem_iport_ptrs_t;
+
+/**
+ * cvmx_pko_mem_iport_qos
+ *
+ * Notes:
+ * Sets the QOS mask, per port.  These QOS_MASK bits are logically and physically the same QOS_MASK
+ * bits in PKO_MEM_IPORT_PTRS.  This CSR address allows the QOS_MASK bits to be written during PKO
+ * operation without affecting any other port state.  The engine to which port PID is mapped is engine
+ * EID.  Note that the port to engine mapping must be the same as was previously programmed via the
+ * PKO_MEM_IPORT_PTRS CSR.
+ * This CSR is a memory of 128 entries, and thus, the PKO_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.  The index to this CSR is an IPORT.  A read of
+ * any entry that has not been previously written is illegal and will result in unpredictable CSR read data.
+ */
+union cvmx_pko_mem_iport_qos {
+	uint64_t u64;
+	struct cvmx_pko_mem_iport_qos_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_61_63               : 3;
+	uint64_t qos_mask                     : 8;  /**< Mask to control priority across 8 QOS rounds */
+	uint64_t reserved_13_52               : 40;
+	uint64_t eid                          : 5;  /**< Engine ID to which this port is mapped */
+	uint64_t reserved_7_7                 : 1;
+	uint64_t ipid                         : 7;  /**< PKO-internal Port ID */
+#else
+	uint64_t ipid                         : 7;
+	uint64_t reserved_7_7                 : 1;
+	uint64_t eid                          : 5;
+	uint64_t reserved_13_52               : 40;
+	uint64_t qos_mask                     : 8;
+	uint64_t reserved_61_63               : 3;
+#endif
+	} s;
+	struct cvmx_pko_mem_iport_qos_s       cn68xx;
+	struct cvmx_pko_mem_iport_qos_s       cn68xxp1;
+};
+typedef union cvmx_pko_mem_iport_qos cvmx_pko_mem_iport_qos_t;
+
+/**
+ * cvmx_pko_mem_iqueue_ptrs
+ *
+ * Notes:
+ * Sets the queue to port mapping and the initial command buffer pointer, per queue.  Unused queues must
+ * set BUF_PTR=0.  Each queue may map to at most one port.  No more than 32 queues may map to a port.
+ * The set of queues that is mapped to a port must be a contiguous array of queues.  The port to which
+ * queue QID is mapped is port IPID.  The index of queue QID in port IPID's queue list is IDX.  The last
+ * queue in port IPID's queue array must have its TAIL bit set.
+ * STATIC_Q marks queue QID as having static priority.  STATIC_P marks the port IPID to which QID is
+ * mapped as having at least one queue with static priority.  If any QID that maps to IPID has static
+ * priority, then all QID that map to IPID must have STATIC_P set.  Queues marked as static priority
+ * must be contiguous and begin at IDX 0.  The last queue that is marked as having static priority
+ * must have its S_TAIL bit set.
+ * This CSR is a memory of 256 entries, and thus, the PKO_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.  The index to this CSR is an IQUEUE.  A read of any
+ * entry that has not been previously written is illegal and will result in unpredictable CSR read data.
+ */
+union cvmx_pko_mem_iqueue_ptrs {
+	uint64_t u64;
+	struct cvmx_pko_mem_iqueue_ptrs_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t s_tail                       : 1;  /**< Set if this QID is the tail of the static queues */
+	uint64_t static_p                     : 1;  /**< Set if any QID in this IPID has static priority */
+	uint64_t static_q                     : 1;  /**< Set if this QID has static priority */
+	uint64_t qos_mask                     : 8;  /**< Mask to control priority across 8 QOS rounds */
+	uint64_t buf_ptr                      : 31; /**< Command buffer pointer[37:7] */
+	uint64_t tail                         : 1;  /**< Set if this QID is the tail of the queue array */
+	uint64_t index                        : 5;  /**< Index (distance from head) in the queue array */
+	uint64_t reserved_15_15               : 1;
+	uint64_t ipid                         : 7;  /**< PKO-Internal Port ID to which this queue is mapped */
+	uint64_t qid                          : 8;  /**< Queue ID */
+#else
+	uint64_t qid                          : 8;
+	uint64_t ipid                         : 7;
+	uint64_t reserved_15_15               : 1;
+	uint64_t index                        : 5;
+	uint64_t tail                         : 1;
+	uint64_t buf_ptr                      : 31;
+	uint64_t qos_mask                     : 8;
+	uint64_t static_q                     : 1;
+	uint64_t static_p                     : 1;
+	uint64_t s_tail                       : 1;
+#endif
+	} s;
+	struct cvmx_pko_mem_iqueue_ptrs_s     cn68xx;
+	struct cvmx_pko_mem_iqueue_ptrs_s     cn68xxp1;
+};
+typedef union cvmx_pko_mem_iqueue_ptrs cvmx_pko_mem_iqueue_ptrs_t;
+
+/**
+ * cvmx_pko_mem_iqueue_qos
+ *
+ * Notes:
+ * Sets the QOS mask, per queue.  These QOS_MASK bits are logically and physically the same QOS_MASK
+ * bits in PKO_MEM_IQUEUE_PTRS.  This CSR address allows the QOS_MASK bits to be written during PKO
+ * operation without affecting any other queue state.  The port to which queue QID is mapped is port
+ * IPID.  Note that the queue to port mapping must be the same as was previously programmed via the
+ * PKO_MEM_IQUEUE_PTRS CSR.
+ * This CSR is a memory of 256 entries, and thus, the PKO_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.  The index to this CSR is an IQUEUE.  A read of any
+ * entry that has not been previously written is illegal and will result in unpredictable CSR read data.
+ */
+union cvmx_pko_mem_iqueue_qos {
+	uint64_t u64;
+	struct cvmx_pko_mem_iqueue_qos_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_61_63               : 3;
+	uint64_t qos_mask                     : 8;  /**< Mask to control priority across 8 QOS rounds */
+	uint64_t reserved_15_52               : 38;
+	uint64_t ipid                         : 7;  /**< PKO-Internal Port ID to which this queue is mapped */
+	uint64_t qid                          : 8;  /**< Queue ID */
+#else
+	uint64_t qid                          : 8;
+	uint64_t ipid                         : 7;
+	uint64_t reserved_15_52               : 38;
+	uint64_t qos_mask                     : 8;
+	uint64_t reserved_61_63               : 3;
+#endif
+	} s;
+	struct cvmx_pko_mem_iqueue_qos_s      cn68xx;
+	struct cvmx_pko_mem_iqueue_qos_s      cn68xxp1;
+};
+typedef union cvmx_pko_mem_iqueue_qos cvmx_pko_mem_iqueue_qos_t;
+
+/**
+ * cvmx_pko_mem_port_ptrs
+ *
+ * Notes:
+ * Sets the port to engine mapping, per port.  Ports marked as static priority need not be contiguous,
+ * but they must be the lowest numbered PIDs mapped to this EID and must have QOS_MASK=0xff.  If EID==8
+ * or EID==9, then PID[1:0] is used to direct the packet to the correct port on that interface.
+ * EID==15 can be used for unused PKO-internal ports.
+ * The reset configuration is the following:
+ *   PID EID(ext port) BP_PORT QOS_MASK STATIC_P
+ *   -------------------------------------------
+ *     0   0( 0)             0     0xff        0
+ *     1   1( 1)             1     0xff        0
+ *     2   2( 2)             2     0xff        0
+ *     3   3( 3)             3     0xff        0
+ *     4   0( 0)             4     0xff        0
+ *     5   1( 1)             5     0xff        0
+ *     6   2( 2)             6     0xff        0
+ *     7   3( 3)             7     0xff        0
+ *     8   0( 0)             8     0xff        0
+ *     9   1( 1)             9     0xff        0
+ *    10   2( 2)            10     0xff        0
+ *    11   3( 3)            11     0xff        0
+ *    12   0( 0)            12     0xff        0
+ *    13   1( 1)            13     0xff        0
+ *    14   2( 2)            14     0xff        0
+ *    15   3( 3)            15     0xff        0
+ *   -------------------------------------------
+ *    16   4(16)            16     0xff        0
+ *    17   5(17)            17     0xff        0
+ *    18   6(18)            18     0xff        0
+ *    19   7(19)            19     0xff        0
+ *    20   4(16)            20     0xff        0
+ *    21   5(17)            21     0xff        0
+ *    22   6(18)            22     0xff        0
+ *    23   7(19)            23     0xff        0
+ *    24   4(16)            24     0xff        0
+ *    25   5(17)            25     0xff        0
+ *    26   6(18)            26     0xff        0
+ *    27   7(19)            27     0xff        0
+ *    28   4(16)            28     0xff        0
+ *    29   5(17)            29     0xff        0
+ *    30   6(18)            30     0xff        0
+ *    31   7(19)            31     0xff        0
+ *   -------------------------------------------
+ *    32   8(32)            32     0xff        0
+ *    33   8(33)            33     0xff        0
+ *    34   8(34)            34     0xff        0
+ *    35   8(35)            35     0xff        0
+ *   -------------------------------------------
+ *    36   9(36)            36     0xff        0
+ *    37   9(37)            37     0xff        0
+ *    38   9(38)            38     0xff        0
+ *    39   9(39)            39     0xff        0
+ *
+ * This CSR is a memory of 48 entries, and thus, the PKO_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.  A read of any entry that has not been
+ * previously written is illegal and will result in unpredictable CSR read data.
+ */
+union cvmx_pko_mem_port_ptrs {
+	uint64_t u64;
+	struct cvmx_pko_mem_port_ptrs_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_62_63               : 2;
+	uint64_t static_p                     : 1;  /**< Set if this PID has static priority */
+	uint64_t qos_mask                     : 8;  /**< Mask to control priority across 8 QOS rounds */
+	uint64_t reserved_16_52               : 37;
+	uint64_t bp_port                      : 6;  /**< RESERVED, must be 63 */
+	uint64_t eid                          : 4;  /**< Engine ID to which this port is mapped
+                                                         Legal EIDs: 0-1, 8-9, 15 (15 only if port not used) */
+	uint64_t pid                          : 6;  /**< Port ID[5:0] */
+#else
+	uint64_t pid                          : 6;
+	uint64_t eid                          : 4;
+	uint64_t bp_port                      : 6;
+	uint64_t reserved_16_52               : 37;
+	uint64_t qos_mask                     : 8;
+	uint64_t static_p                     : 1;
+	uint64_t reserved_62_63               : 2;
+#endif
+	} s;
+	struct cvmx_pko_mem_port_ptrs_s       cn52xx;
+	struct cvmx_pko_mem_port_ptrs_s       cn52xxp1;
+	struct cvmx_pko_mem_port_ptrs_s       cn56xx;
+	struct cvmx_pko_mem_port_ptrs_s       cn56xxp1;
+	struct cvmx_pko_mem_port_ptrs_s       cn61xx;
+	struct cvmx_pko_mem_port_ptrs_s       cn63xx;
+	struct cvmx_pko_mem_port_ptrs_s       cn63xxp1;
+	struct cvmx_pko_mem_port_ptrs_s       cn66xx;
+	struct cvmx_pko_mem_port_ptrs_s       cn70xx;
+	struct cvmx_pko_mem_port_ptrs_s       cn70xxp1;
+	struct cvmx_pko_mem_port_ptrs_s       cnf71xx;
+};
+typedef union cvmx_pko_mem_port_ptrs cvmx_pko_mem_port_ptrs_t;
+
+/**
+ * cvmx_pko_mem_port_qos
+ *
+ * Notes:
+ * Sets the QOS mask, per port.  These QOS_MASK bits are logically and physically the same QOS_MASK
+ * bits in PKO_MEM_PORT_PTRS.  This CSR address allows the QOS_MASK bits to be written during PKO
+ * operation without affecting any other port state.  The engine to which port PID is mapped is engine
+ * EID.  Note that the port to engine mapping must be the same as was previously programmed via the
+ * PKO_MEM_PORT_PTRS CSR.
+ * This CSR is a memory of 44 entries, and thus, the PKO_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.  A read of any entry that has not been
+ * previously written is illegal and will result in unpredictable CSR read data.
+ */
+union cvmx_pko_mem_port_qos {
+	uint64_t u64;
+	struct cvmx_pko_mem_port_qos_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_61_63               : 3;
+	uint64_t qos_mask                     : 8;  /**< Mask to control priority across 8 QOS rounds */
+	uint64_t reserved_10_52               : 43;
+	uint64_t eid                          : 4;  /**< Engine ID to which this port is mapped
+                                                         Legal EIDs: 0-1, 8-9 */
+	uint64_t pid                          : 6;  /**< Port ID[5:0] */
+#else
+	uint64_t pid                          : 6;
+	uint64_t eid                          : 4;
+	uint64_t reserved_10_52               : 43;
+	uint64_t qos_mask                     : 8;
+	uint64_t reserved_61_63               : 3;
+#endif
+	} s;
+	struct cvmx_pko_mem_port_qos_s        cn52xx;
+	struct cvmx_pko_mem_port_qos_s        cn52xxp1;
+	struct cvmx_pko_mem_port_qos_s        cn56xx;
+	struct cvmx_pko_mem_port_qos_s        cn56xxp1;
+	struct cvmx_pko_mem_port_qos_s        cn61xx;
+	struct cvmx_pko_mem_port_qos_s        cn63xx;
+	struct cvmx_pko_mem_port_qos_s        cn63xxp1;
+	struct cvmx_pko_mem_port_qos_s        cn66xx;
+	struct cvmx_pko_mem_port_qos_s        cn70xx;
+	struct cvmx_pko_mem_port_qos_s        cn70xxp1;
+	struct cvmx_pko_mem_port_qos_s        cnf71xx;
+};
+typedef union cvmx_pko_mem_port_qos cvmx_pko_mem_port_qos_t;
+
+/**
+ * cvmx_pko_mem_port_rate0
+ *
+ * Notes:
+ * This CSR is a memory of 44 entries, and thus, the PKO_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.  A read of any entry that has not been
+ * previously written is illegal and will result in unpredictable CSR read data.
+ */
+union cvmx_pko_mem_port_rate0 {
+	uint64_t u64;
+	struct cvmx_pko_mem_port_rate0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_51_63               : 13;
+	uint64_t rate_word                    : 19; /**< Rate limiting adder per 8 byte */
+	uint64_t rate_pkt                     : 24; /**< Rate limiting adder per packet */
+	uint64_t reserved_7_7                 : 1;
+	uint64_t pid                          : 7;  /**< Port ID[5:0] */
+#else
+	uint64_t pid                          : 7;
+	uint64_t reserved_7_7                 : 1;
+	uint64_t rate_pkt                     : 24;
+	uint64_t rate_word                    : 19;
+	uint64_t reserved_51_63               : 13;
+#endif
+	} s;
+	struct cvmx_pko_mem_port_rate0_cn52xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_51_63               : 13;
+	uint64_t rate_word                    : 19; /**< Rate limiting adder per 8 byte */
+	uint64_t rate_pkt                     : 24; /**< Rate limiting adder per packet */
+	uint64_t reserved_6_7                 : 2;
+	uint64_t pid                          : 6;  /**< Port ID[5:0] */
+#else
+	uint64_t pid                          : 6;
+	uint64_t reserved_6_7                 : 2;
+	uint64_t rate_pkt                     : 24;
+	uint64_t rate_word                    : 19;
+	uint64_t reserved_51_63               : 13;
+#endif
+	} cn52xx;
+	struct cvmx_pko_mem_port_rate0_cn52xx cn52xxp1;
+	struct cvmx_pko_mem_port_rate0_cn52xx cn56xx;
+	struct cvmx_pko_mem_port_rate0_cn52xx cn56xxp1;
+	struct cvmx_pko_mem_port_rate0_cn52xx cn61xx;
+	struct cvmx_pko_mem_port_rate0_cn52xx cn63xx;
+	struct cvmx_pko_mem_port_rate0_cn52xx cn63xxp1;
+	struct cvmx_pko_mem_port_rate0_cn52xx cn66xx;
+	struct cvmx_pko_mem_port_rate0_s      cn68xx;
+	struct cvmx_pko_mem_port_rate0_s      cn68xxp1;
+	struct cvmx_pko_mem_port_rate0_cn52xx cn70xx;
+	struct cvmx_pko_mem_port_rate0_cn52xx cn70xxp1;
+	struct cvmx_pko_mem_port_rate0_cn52xx cnf71xx;
+};
+typedef union cvmx_pko_mem_port_rate0 cvmx_pko_mem_port_rate0_t;
+
+/**
+ * cvmx_pko_mem_port_rate1
+ *
+ * Notes:
+ * Writing PKO_MEM_PORT_RATE1[PID,RATE_LIM] has the side effect of setting the corresponding
+ * accumulator to zero.
+ * This CSR is a memory of 44 entries, and thus, the PKO_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.  A read of any entry that has not been
+ * previously written is illegal and will result in unpredictable CSR read data.
+ */
+union cvmx_pko_mem_port_rate1 {
+	uint64_t u64;
+	struct cvmx_pko_mem_port_rate1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_32_63               : 32;
+	uint64_t rate_lim                     : 24; /**< Rate limiting accumulator limit */
+	uint64_t reserved_7_7                 : 1;
+	uint64_t pid                          : 7;  /**< Port ID[5:0] */
+#else
+	uint64_t pid                          : 7;
+	uint64_t reserved_7_7                 : 1;
+	uint64_t rate_lim                     : 24;
+	uint64_t reserved_32_63               : 32;
+#endif
+	} s;
+	struct cvmx_pko_mem_port_rate1_cn52xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_32_63               : 32;
+	uint64_t rate_lim                     : 24; /**< Rate limiting accumulator limit */
+	uint64_t reserved_6_7                 : 2;
+	uint64_t pid                          : 6;  /**< Port ID[5:0] */
+#else
+	uint64_t pid                          : 6;
+	uint64_t reserved_6_7                 : 2;
+	uint64_t rate_lim                     : 24;
+	uint64_t reserved_32_63               : 32;
+#endif
+	} cn52xx;
+	struct cvmx_pko_mem_port_rate1_cn52xx cn52xxp1;
+	struct cvmx_pko_mem_port_rate1_cn52xx cn56xx;
+	struct cvmx_pko_mem_port_rate1_cn52xx cn56xxp1;
+	struct cvmx_pko_mem_port_rate1_cn52xx cn61xx;
+	struct cvmx_pko_mem_port_rate1_cn52xx cn63xx;
+	struct cvmx_pko_mem_port_rate1_cn52xx cn63xxp1;
+	struct cvmx_pko_mem_port_rate1_cn52xx cn66xx;
+	struct cvmx_pko_mem_port_rate1_s      cn68xx;
+	struct cvmx_pko_mem_port_rate1_s      cn68xxp1;
+	struct cvmx_pko_mem_port_rate1_cn52xx cn70xx;
+	struct cvmx_pko_mem_port_rate1_cn52xx cn70xxp1;
+	struct cvmx_pko_mem_port_rate1_cn52xx cnf71xx;
+};
+typedef union cvmx_pko_mem_port_rate1 cvmx_pko_mem_port_rate1_t;
+
+/**
+ * cvmx_pko_mem_queue_ptrs
+ *
+ * Notes:
+ * Sets the queue to port mapping and the initial command buffer pointer, per queue
+ * Each queue may map to at most one port.  No more than 16 queues may map to a port.  The set of
+ * queues that is mapped to a port must be a contiguous array of queues.  The port to which queue QID
+ * is mapped is port PID.  The index of queue QID in port PID's queue list is IDX.  The last queue in
+ * port PID's queue array must have its TAIL bit set.  Unused queues must be mapped to port 63.
+ * STATIC_Q marks queue QID as having static priority.  STATIC_P marks the port PID to which QID is
+ * mapped as having at least one queue with static priority.  If any QID that maps to PID has static
+ * priority, then all QID that map to PID must have STATIC_P set.  Queues marked as static priority
+ * must be contiguous and begin at IDX 0.  The last queue that is marked as having static priority
+ * must have its S_TAIL bit set.
+ * This CSR is a memory of 256 entries, and thus, the PKO_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.  A read of any entry that has not been
+ * previously written is illegal and will result in unpredictable CSR read data.
+ */
+union cvmx_pko_mem_queue_ptrs {
+	uint64_t u64;
+	struct cvmx_pko_mem_queue_ptrs_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t s_tail                       : 1;  /**< Set if this QID is the tail of the static queues */
+	uint64_t static_p                     : 1;  /**< Set if any QID in this PID has static priority */
+	uint64_t static_q                     : 1;  /**< Set if this QID has static priority */
+	uint64_t qos_mask                     : 8;  /**< Mask to control priority across 8 QOS rounds */
+	uint64_t buf_ptr                      : 36; /**< Command buffer pointer, <23:17> MBZ */
+	uint64_t tail                         : 1;  /**< Set if this QID is the tail of the queue array */
+	uint64_t index                        : 3;  /**< Index[2:0] (distance from head) in the queue array */
+	uint64_t port                         : 6;  /**< Port ID to which this queue is mapped */
+	uint64_t queue                        : 7;  /**< Queue ID[6:0] */
+#else
+	uint64_t queue                        : 7;
+	uint64_t port                         : 6;
+	uint64_t index                        : 3;
+	uint64_t tail                         : 1;
+	uint64_t buf_ptr                      : 36;
+	uint64_t qos_mask                     : 8;
+	uint64_t static_q                     : 1;
+	uint64_t static_p                     : 1;
+	uint64_t s_tail                       : 1;
+#endif
+	} s;
+	struct cvmx_pko_mem_queue_ptrs_s      cn30xx;
+	struct cvmx_pko_mem_queue_ptrs_s      cn31xx;
+	struct cvmx_pko_mem_queue_ptrs_s      cn38xx;
+	struct cvmx_pko_mem_queue_ptrs_s      cn38xxp2;
+	struct cvmx_pko_mem_queue_ptrs_s      cn50xx;
+	struct cvmx_pko_mem_queue_ptrs_s      cn52xx;
+	struct cvmx_pko_mem_queue_ptrs_s      cn52xxp1;
+	struct cvmx_pko_mem_queue_ptrs_s      cn56xx;
+	struct cvmx_pko_mem_queue_ptrs_s      cn56xxp1;
+	struct cvmx_pko_mem_queue_ptrs_s      cn58xx;
+	struct cvmx_pko_mem_queue_ptrs_s      cn58xxp1;
+	struct cvmx_pko_mem_queue_ptrs_s      cn61xx;
+	struct cvmx_pko_mem_queue_ptrs_s      cn63xx;
+	struct cvmx_pko_mem_queue_ptrs_s      cn63xxp1;
+	struct cvmx_pko_mem_queue_ptrs_s      cn66xx;
+	struct cvmx_pko_mem_queue_ptrs_s      cn70xx;
+	struct cvmx_pko_mem_queue_ptrs_s      cn70xxp1;
+	struct cvmx_pko_mem_queue_ptrs_s      cnf71xx;
+};
+typedef union cvmx_pko_mem_queue_ptrs cvmx_pko_mem_queue_ptrs_t;
+
+/**
+ * cvmx_pko_mem_queue_qos
+ *
+ * Notes:
+ * Sets the QOS mask, per queue.  These QOS_MASK bits are logically and physically the same QOS_MASK
+ * bits in PKO_MEM_QUEUE_PTRS.  This CSR address allows the QOS_MASK bits to be written during PKO
+ * operation without affecting any other queue state.  The port to which queue QID is mapped is port
+ * PID.  Note that the queue to port mapping must be the same as was previously programmed via the
+ * PKO_MEM_QUEUE_PTRS CSR.
+ * This CSR is a memory of 256 entries, and thus, the PKO_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.  A read of any entry that has not been
+ * previously written is illegal and will result in unpredictable CSR read data.
+ */
+union cvmx_pko_mem_queue_qos {
+	uint64_t u64;
+	struct cvmx_pko_mem_queue_qos_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_61_63               : 3;
+	uint64_t qos_mask                     : 8;  /**< Mask to control priority across 8 QOS rounds */
+	uint64_t reserved_13_52               : 40;
+	uint64_t pid                          : 6;  /**< Port ID to which this queue is mapped */
+	uint64_t qid                          : 7;  /**< Queue ID */
+#else
+	uint64_t qid                          : 7;
+	uint64_t pid                          : 6;
+	uint64_t reserved_13_52               : 40;
+	uint64_t qos_mask                     : 8;
+	uint64_t reserved_61_63               : 3;
+#endif
+	} s;
+	struct cvmx_pko_mem_queue_qos_s       cn30xx;
+	struct cvmx_pko_mem_queue_qos_s       cn31xx;
+	struct cvmx_pko_mem_queue_qos_s       cn38xx;
+	struct cvmx_pko_mem_queue_qos_s       cn38xxp2;
+	struct cvmx_pko_mem_queue_qos_s       cn50xx;
+	struct cvmx_pko_mem_queue_qos_s       cn52xx;
+	struct cvmx_pko_mem_queue_qos_s       cn52xxp1;
+	struct cvmx_pko_mem_queue_qos_s       cn56xx;
+	struct cvmx_pko_mem_queue_qos_s       cn56xxp1;
+	struct cvmx_pko_mem_queue_qos_s       cn58xx;
+	struct cvmx_pko_mem_queue_qos_s       cn58xxp1;
+	struct cvmx_pko_mem_queue_qos_s       cn61xx;
+	struct cvmx_pko_mem_queue_qos_s       cn63xx;
+	struct cvmx_pko_mem_queue_qos_s       cn63xxp1;
+	struct cvmx_pko_mem_queue_qos_s       cn66xx;
+	struct cvmx_pko_mem_queue_qos_s       cn70xx;
+	struct cvmx_pko_mem_queue_qos_s       cn70xxp1;
+	struct cvmx_pko_mem_queue_qos_s       cnf71xx;
+};
+typedef union cvmx_pko_mem_queue_qos cvmx_pko_mem_queue_qos_t;
+
+/**
+ * cvmx_pko_mem_throttle_int
+ *
+ * Notes:
+ * Writing PACKET and WORD with 0 resets both counts for INT to 0 rather than add 0.
+ * Otherwise, writes to this CSR add to the existing WORD/PACKET counts for the interface INT.
+ *
+ * PKO tracks the number of (8-byte) WORD's and PACKET's in-flight (sum total in both PKO
+ * and the interface MAC) on the interface. (When PKO first selects a packet from a PKO queue, it
+ * increments the counts appropriately. When the interface MAC has (largely) completed sending
+ * the words/packet, PKO decrements the count appropriately.) When PKO_REG_FLAGS[ENA_THROTTLE]
+ * is set and the most-significant bit of the WORD or packet count for a interface is set,
+ * PKO will not transfer any packets over the interface. Software can limit the amount of
+ * packet data and/or the number of packets that OCTEON can send out the chip after receiving backpressure
+ * from the interface/pipe via these per-pipe throttle counts when PKO_REG_FLAGS[ENA_THROTTLE]=1.
+ * For example, to limit the number of packets outstanding in the interface to N, preset PACKET for
+ * the pipe to the value 0x20-N (0x20 is the smallest PACKET value with the most-significant bit set).
+ *
+ * This CSR is a memory of 32 entries, and thus, the PKO_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.  The index to this CSR is an INTERFACE.  A read of any
+ * entry that has not been previously written is illegal and will result in unpredictable CSR read data.
+ */
+union cvmx_pko_mem_throttle_int {
+	uint64_t u64;
+	struct cvmx_pko_mem_throttle_int_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_47_63               : 17;
+	uint64_t word                         : 15; /**< On a write, the amount to add to the interface
+                                                         throttle word count selected by INT. On a read,
+                                                         returns the current value of the interface throttle
+                                                         word count selected by PKO_REG_READ_IDX[IDX]. */
+	uint64_t reserved_14_31               : 18;
+	uint64_t packet                       : 6;  /**< On a write, the amount to add to the interface
+                                                         throttle packet count selected by INT. On a read,
+                                                         returns the current value of the interface throttle
+                                                         packet count selected by PKO_REG_READ_IDX[IDX]. */
+	uint64_t reserved_5_7                 : 3;
+	uint64_t intr                         : 5;  /**< Selected interface for writes. Undefined on a read.
+                                                         See PKO_MEM_IPORT_PTRS[INT] for encoding. */
+#else
+	uint64_t intr                         : 5;
+	uint64_t reserved_5_7                 : 3;
+	uint64_t packet                       : 6;
+	uint64_t reserved_14_31               : 18;
+	uint64_t word                         : 15;
+	uint64_t reserved_47_63               : 17;
+#endif
+	} s;
+	struct cvmx_pko_mem_throttle_int_s    cn68xx;
+	struct cvmx_pko_mem_throttle_int_s    cn68xxp1;
+};
+typedef union cvmx_pko_mem_throttle_int cvmx_pko_mem_throttle_int_t;
+
+/**
+ * cvmx_pko_mem_throttle_pipe
+ *
+ * Notes:
+ * Writing PACKET and WORD with 0 resets both counts for PIPE to 0 rather than add 0.
+ * Otherwise, writes to this CSR add to the existing WORD/PACKET counts for the PKO pipe PIPE.
+ *
+ * PKO tracks the number of (8-byte) WORD's and PACKET's in-flight (sum total in both PKO
+ * and the interface MAC) on the pipe. (When PKO first selects a packet from a PKO queue, it
+ * increments the counts appropriately. When the interface MAC has (largely) completed sending
+ * the words/packet, PKO decrements the count appropriately.) When PKO_REG_FLAGS[ENA_THROTTLE]
+ * is set and the most-significant bit of the WORD or packet count for a PKO pipe is set,
+ * PKO will not transfer any packets over the PKO pipe. Software can limit the amount of
+ * packet data and/or the number of packets that OCTEON can send out the chip after receiving backpressure
+ * from the interface/pipe via these per-pipe throttle counts when PKO_REG_FLAGS[ENA_THROTTLE]=1.
+ * For example, to limit the number of packets outstanding in the pipe to N, preset PACKET for
+ * the pipe to the value 0x20-N (0x20 is the smallest PACKET value with the most-significant bit set).
+ *
+ * This CSR is a memory of 128 entries, and thus, the PKO_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.  The index to this CSR is a PIPE.  A read of any
+ * entry that has not been previously written is illegal and will result in unpredictable CSR read data.
+ */
+union cvmx_pko_mem_throttle_pipe {
+	uint64_t u64;
+	struct cvmx_pko_mem_throttle_pipe_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_47_63               : 17;
+	uint64_t word                         : 15; /**< On a write, the amount to add to the pipe throttle
+                                                         word count selected by PIPE. On a read, returns
+                                                         the current value of the pipe throttle word count
+                                                         selected by PKO_REG_READ_IDX[IDX]. */
+	uint64_t reserved_14_31               : 18;
+	uint64_t packet                       : 6;  /**< On a write, the amount to add to the pipe throttle
+                                                         packet count selected by PIPE. On a read, returns
+                                                         the current value of the pipe throttle packet count
+                                                         selected by PKO_REG_READ_IDX[IDX]. */
+	uint64_t reserved_7_7                 : 1;
+	uint64_t pipe                         : 7;  /**< Selected PKO pipe for writes. Undefined on a read. */
+#else
+	uint64_t pipe                         : 7;
+	uint64_t reserved_7_7                 : 1;
+	uint64_t packet                       : 6;
+	uint64_t reserved_14_31               : 18;
+	uint64_t word                         : 15;
+	uint64_t reserved_47_63               : 17;
+#endif
+	} s;
+	struct cvmx_pko_mem_throttle_pipe_s   cn68xx;
+	struct cvmx_pko_mem_throttle_pipe_s   cn68xxp1;
+};
+typedef union cvmx_pko_mem_throttle_pipe cvmx_pko_mem_throttle_pipe_t;
+
+/**
+ * cvmx_pko_ncb_bist_status
+ *
+ * Each bit is the BIST result of an individual memory (per bit, 0 = pass and 1 = fail).
+ *
+ */
+union cvmx_pko_ncb_bist_status {
+	uint64_t u64;
+	struct cvmx_pko_ncb_bist_status_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t ncbi_l2_out_ram_bist_status  : 1;  /**< BIST status for NCBI_L2_OUT_RAM. */
+	uint64_t ncbi_pp_out_ram_bist_status  : 1;  /**< BIST status for NCBI_PP_OUT_RAM. */
+	uint64_t ncbo_pdm_cmd_dat_ram_bist_status : 1;/**< BIST status for NCBO_PDM_CMD_DAT_RAM. */
+	uint64_t ncbi_l2_pdm_pref_ram_bist_status : 1;/**< BIST status for NCBI_L2_PDM_PREF_RAM. */
+	uint64_t ncbo_pp_fif_ram_bist_status  : 1;  /**< BIST status for NCBO_PP_FIF_RAM. */
+	uint64_t reserved_0_58                : 59;
+#else
+	uint64_t reserved_0_58                : 59;
+	uint64_t ncbo_pp_fif_ram_bist_status  : 1;
+	uint64_t ncbi_l2_pdm_pref_ram_bist_status : 1;
+	uint64_t ncbo_pdm_cmd_dat_ram_bist_status : 1;
+	uint64_t ncbi_pp_out_ram_bist_status  : 1;
+	uint64_t ncbi_l2_out_ram_bist_status  : 1;
+#endif
+	} s;
+	struct cvmx_pko_ncb_bist_status_s     cn78xx;
+};
+typedef union cvmx_pko_ncb_bist_status cvmx_pko_ncb_bist_status_t;
+
+/**
+ * cvmx_pko_ncb_ecc_ctl0
+ */
+union cvmx_pko_ncb_ecc_ctl0 {
+	uint64_t u64;
+	struct cvmx_pko_ncb_ecc_ctl0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t ncbi_l2_out_ram_flip         : 2;  /**< NCBI_L2_OUT_RAM flip syndrome bits on write. */
+	uint64_t ncbi_l2_out_ram_cdis         : 1;  /**< NCBI_L2_OUT_RAM ECC correction disable. */
+	uint64_t ncbi_pp_out_ram_flip         : 2;  /**< NCBI_PP_OUT_RAM flip syndrome bits on write. */
+	uint64_t ncbi_pp_out_ram_cdis         : 1;  /**< NCBI_PP_OUT_RAM ECC correction disable. */
+	uint64_t ncbo_pdm_cmd_dat_ram_flip    : 2;  /**< NCBO_PDM_CMD_DAT_RAM flip syndrome bits on write. */
+	uint64_t ncbo_pdm_cmd_dat_ram_cdis    : 1;  /**< NCBO_PDM_CMD_DAT_RAM ECC correction disable. */
+	uint64_t ncbi_l2_pdm_pref_ram_flip    : 2;  /**< NCBI_L2_PDM_PREF_RAM flip syndrome bits on write. */
+	uint64_t ncbi_l2_pdm_pref_ram_cdis    : 1;  /**< NCBI_L2_PDM_PREF_RAM ECC correction disable. */
+	uint64_t ncbo_pp_fif_ram_flip         : 2;  /**< NCBO_PP_FIF_RAM flip syndrome bits on write. */
+	uint64_t ncbo_pp_fif_ram_cdis         : 1;  /**< NCBO_PP_FIF_RAM ECC correction disable. */
+	uint64_t reserved_0_48                : 49;
+#else
+	uint64_t reserved_0_48                : 49;
+	uint64_t ncbo_pp_fif_ram_cdis         : 1;
+	uint64_t ncbo_pp_fif_ram_flip         : 2;
+	uint64_t ncbi_l2_pdm_pref_ram_cdis    : 1;
+	uint64_t ncbi_l2_pdm_pref_ram_flip    : 2;
+	uint64_t ncbo_pdm_cmd_dat_ram_cdis    : 1;
+	uint64_t ncbo_pdm_cmd_dat_ram_flip    : 2;
+	uint64_t ncbi_pp_out_ram_cdis         : 1;
+	uint64_t ncbi_pp_out_ram_flip         : 2;
+	uint64_t ncbi_l2_out_ram_cdis         : 1;
+	uint64_t ncbi_l2_out_ram_flip         : 2;
+#endif
+	} s;
+	struct cvmx_pko_ncb_ecc_ctl0_s        cn78xx;
+};
+typedef union cvmx_pko_ncb_ecc_ctl0 cvmx_pko_ncb_ecc_ctl0_t;
+
+/**
+ * cvmx_pko_ncb_ecc_dbe_sts0
+ */
+union cvmx_pko_ncb_ecc_dbe_sts0 {
+	uint64_t u64;
+	struct cvmx_pko_ncb_ecc_dbe_sts0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t ncbi_l2_out_ram_dbe          : 1;  /**< Double-bit error for NCBI_L2_OUT_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.nonpse.ncb.pko_ncbi_outb.ncbi_txr.l2_out_fifo */
+	uint64_t ncbi_pp_out_ram_dbe          : 1;  /**< Double-bit error for NCBI_PP_OUT_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.nonpse.ncb.pko_ncbi_outb.ncbi_txr.pp_out_fifo */
+	uint64_t ncbo_pdm_cmd_dat_ram_dbe     : 1;  /**< Double-bit error for NCBO_PDM_CMD_DAT_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.nonpse.ncb.pko_ncbo_inb.splitter.ncb__pdm_cmnd_data_fifo */
+	uint64_t ncbi_l2_pdm_pref_ram_dbe     : 1;  /**< Double-bit error for NCBI_L2_PDM_PREF_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.nonpse.ncb.pko_ncbi_outb.ncbi_l2_pipe.pdm_prefbuf_fifo */
+	uint64_t ncbo_pp_fif_ram_dbe          : 1;  /**< Double-bit error for NCBO_PP_FIF_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.nonpse.ncb.pko_ncbo_inb.splitter.pp_fifo */
+	uint64_t reserved_0_58                : 59;
+#else
+	uint64_t reserved_0_58                : 59;
+	uint64_t ncbo_pp_fif_ram_dbe          : 1;
+	uint64_t ncbi_l2_pdm_pref_ram_dbe     : 1;
+	uint64_t ncbo_pdm_cmd_dat_ram_dbe     : 1;
+	uint64_t ncbi_pp_out_ram_dbe          : 1;
+	uint64_t ncbi_l2_out_ram_dbe          : 1;
+#endif
+	} s;
+	struct cvmx_pko_ncb_ecc_dbe_sts0_s    cn78xx;
+};
+typedef union cvmx_pko_ncb_ecc_dbe_sts0 cvmx_pko_ncb_ecc_dbe_sts0_t;
+
+/**
+ * cvmx_pko_ncb_ecc_dbe_sts_cmb0
+ */
+union cvmx_pko_ncb_ecc_dbe_sts_cmb0 {
+	uint64_t u64;
+	struct cvmx_pko_ncb_ecc_dbe_sts_cmb0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t ncb_dbe_cmb0                 : 1;  /**< This bit is the logical OR of all bits in PKO_NCB_ECC_DBE_STS0. To clear this bit,
+                                                         software
+                                                         must clear bits in PKO_NCB_ECC_DBE_STS0. When this bit is set, the corresponding interrupt
+                                                         is set. Throws PKO_INTSN_E::PKO_NCB_DBE_CMB0. INTERNAL: Instances:
+                                                         pko_pnr2.nonpse.ncb.pko_ncbi_outb.ncbi_txr.l2_out_fifo
+                                                         pko_pnr2.nonpse.ncb.pko_ncbi_outb.ncbi_txr.pp_out_fifo
+                                                         pko_pnr2.nonpse.ncb.pko_ncbo_inb.splitter.ncb__pdm_cmnd_data_fifo
+                                                         pko_pnr2.nonpse.ncb.pko_ncbi_outb.ncbi_l2_pipe.pdm_prefbuf_fifo
+                                                         pko_pnr2.nonpse.ncb.pko_ncbo_inb.splitter.pp_fifo */
+	uint64_t reserved_0_62                : 63;
+#else
+	uint64_t reserved_0_62                : 63;
+	uint64_t ncb_dbe_cmb0                 : 1;
+#endif
+	} s;
+	struct cvmx_pko_ncb_ecc_dbe_sts_cmb0_s cn78xx;
+};
+typedef union cvmx_pko_ncb_ecc_dbe_sts_cmb0 cvmx_pko_ncb_ecc_dbe_sts_cmb0_t;
+
+/**
+ * cvmx_pko_ncb_ecc_sbe_sts0
+ */
+union cvmx_pko_ncb_ecc_sbe_sts0 {
+	uint64_t u64;
+	struct cvmx_pko_ncb_ecc_sbe_sts0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t ncbi_l2_out_ram_sbe          : 1;  /**< Single-bit error for NCBI_L2_OUT_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.nonpse.ncb.pko_ncbi_outb.ncbi_txr.l2_out_fifo */
+	uint64_t ncbi_pp_out_ram_sbe          : 1;  /**< Single-bit error for NCBI_PP_OUT_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.nonpse.ncb.pko_ncbi_outb.ncbi_txr.pp_out_fifo */
+	uint64_t ncbo_pdm_cmd_dat_ram_sbe     : 1;  /**< Single-bit error for NCBO_PDM_CMD_DAT_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.nonpse.ncb.pko_ncbo_inb.splitter.ncb__pdm_cmnd_data_fifo */
+	uint64_t ncbi_l2_pdm_pref_ram_sbe     : 1;  /**< Single-bit error for NCBI_L2_PDM_PREF_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.nonpse.ncb.pko_ncbi_outb.ncbi_l2_pipe.pdm_prefbuf_fifo */
+	uint64_t ncbo_pp_fif_ram_sbe          : 1;  /**< Single-bit error for NCBO_PP_FIF_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.nonpse.ncb.pko_ncbo_inb.splitter.pp_fifo */
+	uint64_t reserved_0_58                : 59;
+#else
+	uint64_t reserved_0_58                : 59;
+	uint64_t ncbo_pp_fif_ram_sbe          : 1;
+	uint64_t ncbi_l2_pdm_pref_ram_sbe     : 1;
+	uint64_t ncbo_pdm_cmd_dat_ram_sbe     : 1;
+	uint64_t ncbi_pp_out_ram_sbe          : 1;
+	uint64_t ncbi_l2_out_ram_sbe          : 1;
+#endif
+	} s;
+	struct cvmx_pko_ncb_ecc_sbe_sts0_s    cn78xx;
+};
+typedef union cvmx_pko_ncb_ecc_sbe_sts0 cvmx_pko_ncb_ecc_sbe_sts0_t;
+
+/**
+ * cvmx_pko_ncb_ecc_sbe_sts_cmb0
+ */
+union cvmx_pko_ncb_ecc_sbe_sts_cmb0 {
+	uint64_t u64;
+	struct cvmx_pko_ncb_ecc_sbe_sts_cmb0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t ncb_sbe_cmb0                 : 1;  /**< This bit is the logical OR of all bits in PKO_NCB_ECC_SBE_STS0. To clear this bit,
+                                                         software
+                                                         must clear bits in PKO_NCB_ECC_SBE_STS0. When this bit is set, the corresponding interrupt
+                                                         is set. Throws PKO_INTSN_E::PKO_NCB_SBE_CMB0. INTERNAL: Instances:
+                                                         pko_pnr2.nonpse.ncb.pko_ncbi_outb.ncbi_txr.l2_out_fifo
+                                                         pko_pnr2.nonpse.ncb.pko_ncbi_outb.ncbi_txr.pp_out_fifo
+                                                         pko_pnr2.nonpse.ncb.pko_ncbo_inb.splitter.ncb__pdm_cmnd_data_fifo
+                                                         pko_pnr2.nonpse.ncb.pko_ncbi_outb.ncbi_l2_pipe.pdm_prefbuf_fifo
+                                                         pko_pnr2.nonpse.ncb.pko_ncbo_inb.splitter.pp_fifo */
+	uint64_t reserved_0_62                : 63;
+#else
+	uint64_t reserved_0_62                : 63;
+	uint64_t ncb_sbe_cmb0                 : 1;
+#endif
+	} s;
+	struct cvmx_pko_ncb_ecc_sbe_sts_cmb0_s cn78xx;
+};
+typedef union cvmx_pko_ncb_ecc_sbe_sts_cmb0 cvmx_pko_ncb_ecc_sbe_sts_cmb0_t;
+
+/**
+ * cvmx_pko_ncb_int
+ */
+union cvmx_pko_ncb_int {
+	uint64_t u64;
+	struct cvmx_pko_ncb_int_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t ncb_tx_error                 : 1;  /**< NCB transaction error occurred (error/unpredictable/undefined). Throws
+                                                         PKO_INTSN_E::PKO_NCB_TX_ERR. */
+#else
+	uint64_t ncb_tx_error                 : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_pko_ncb_int_s             cn78xx;
+};
+typedef union cvmx_pko_ncb_int cvmx_pko_ncb_int_t;
+
+/**
+ * cvmx_pko_ncb_tx_err_info
+ */
+union cvmx_pko_ncb_tx_err_info {
+	uint64_t u64;
+	struct cvmx_pko_ncb_tx_err_info_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_32_63               : 32;
+	uint64_t wcnt                         : 5;  /**< Word count. */
+	uint64_t src                          : 12; /**< Source of the IOI outbound transaction. */
+	uint64_t dst                          : 8;  /**< Destination of the IOI outbound transaction. */
+	uint64_t tag                          : 4;  /**< Tag of the IOI outbound transaction. */
+	uint64_t eot                          : 1;  /**< EOT bit of the NCBO transaction. */
+	uint64_t sot                          : 1;  /**< SOT bit of the NCBO transaction. */
+	uint64_t valid                        : 1;  /**< Valid bit for transaction (should always be 1 on capture). */
+#else
+	uint64_t valid                        : 1;
+	uint64_t sot                          : 1;
+	uint64_t eot                          : 1;
+	uint64_t tag                          : 4;
+	uint64_t dst                          : 8;
+	uint64_t src                          : 12;
+	uint64_t wcnt                         : 5;
+	uint64_t reserved_32_63               : 32;
+#endif
+	} s;
+	struct cvmx_pko_ncb_tx_err_info_s     cn78xx;
+};
+typedef union cvmx_pko_ncb_tx_err_info cvmx_pko_ncb_tx_err_info_t;
+
+/**
+ * cvmx_pko_ncb_tx_err_word
+ */
+union cvmx_pko_ncb_tx_err_word {
+	uint64_t u64;
+	struct cvmx_pko_ncb_tx_err_word_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t err_word                     : 64; /**< PKO NCB error word (first word of erroneous transaction).
+                                                         Note: this is only the 64-bit data word; the NCB info that goes with it is in
+                                                         PKO_NCB_TX_ERR_INFO. */
+#else
+	uint64_t err_word                     : 64;
+#endif
+	} s;
+	struct cvmx_pko_ncb_tx_err_word_s     cn78xx;
+};
+typedef union cvmx_pko_ncb_tx_err_word cvmx_pko_ncb_tx_err_word_t;
+
+/**
+ * cvmx_pko_pdm_bist_status
  *
- * You should have received a copy of the GNU General Public License
- * along with this file; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- * or visit http://www.gnu.org/licenses/.
+ * Each bit is the BIST result of an individual memory (per bit, 0 = pass and 1 = fail).
  *
- * This file may also be available under a different license from Cavium.
- * Contact Cavium Networks for more information
- ***********************license end**************************************/
+ */
+union cvmx_pko_pdm_bist_status {
+	uint64_t u64;
+	struct cvmx_pko_pdm_bist_status_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t flshb_cache_lo_ram_bist_status : 1;/**< BIST status for FLSHB_CACHE_LO_RAM. */
+	uint64_t flshb_cache_hi_ram_bist_status : 1;/**< BIST status for FLSHB_CACHE_HI_RAM. */
+	uint64_t isrm_ca_iinst_ram_bist_status : 1; /**< BIST status for ISRM_CA_IINST_RAM. */
+	uint64_t isrm_ca_cm_ram_bist_status   : 1;  /**< BIST status for ISRM_CA_CM_RAM. */
+	uint64_t isrm_st_ram2_bist_status     : 1;  /**< BIST status for ISRM_ST_RAM2. */
+	uint64_t isrm_st_ram1_bist_status     : 1;  /**< BIST status for ISRM_ST_RAM1. */
+	uint64_t isrm_st_ram0_bist_status     : 1;  /**< BIST status for ISRM_ST_RAM0. */
+	uint64_t isrd_st_ram3_bist_status     : 1;  /**< BIST status for ISRD_ST_RAM3. */
+	uint64_t isrd_st_ram2_bist_status     : 1;  /**< BIST status for ISRD_ST_RAM2. */
+	uint64_t isrd_st_ram1_bist_status     : 1;  /**< BIST status for ISRD_ST_RAM1. */
+	uint64_t isrd_st_ram0_bist_status     : 1;  /**< BIST status for ISRD_ST_RAM0. */
+	uint64_t drp_hi_ram_bist_status       : 1;  /**< BIST status for DRP_HI_RAM. */
+	uint64_t drp_lo_ram_bist_status       : 1;  /**< BIST status for DRP_LO_RAM. */
+	uint64_t dwp_hi_ram_bist_status       : 1;  /**< BIST status for DWP_HI_RAM. */
+	uint64_t dwp_lo_ram_bist_status       : 1;  /**< BIST status for DWP_LO_RAM. */
+	uint64_t mwp_hi_ram_bist_status       : 1;  /**< BIST status for MWP_HI_RAM. */
+	uint64_t mwp_lo_ram_bist_status       : 1;  /**< BIST status for MWP_LO_RAM. */
+	uint64_t fillb_m_rsp_ram_hi_bist_status : 1;/**< BIST status for FILLB_M_RSP_RAM_HI. */
+	uint64_t fillb_m_rsp_ram_lo_bist_status : 1;/**< BIST status for FILLB_M_RSP_RAM_LO. */
+	uint64_t fillb_d_rsp_ram_hi_bist_status : 1;/**< BIST status for FILLB_D_RSP_RAM_HI. */
+	uint64_t fillb_d_rsp_ram_lo_bist_status : 1;/**< BIST status for FILLB_D_RSP_RAM_LO. */
+	uint64_t fillb_d_rsp_dat_fifo_bist_status : 1;/**< BIST status for FILLB_FLSHB_M_DAT_RAM. */
+	uint64_t fillb_m_rsp_dat_fifo_bist_status : 1;/**< BIST status for FILLB_M_DAT_FIFO. */
+	uint64_t flshb_m_dat_ram_bist_status  : 1;  /**< BIST status for FLSHB_M_DAT_RAM. */
+	uint64_t flshb_d_dat_ram_bist_status  : 1;  /**< BIST status for FLSHB_M_DAT_RAM. */
+	uint64_t minpad_ram_bist_status       : 1;  /**< BIST status for MINPAD_RAM. */
+	uint64_t reserved_0_37                : 38;
+#else
+	uint64_t reserved_0_37                : 38;
+	uint64_t minpad_ram_bist_status       : 1;
+	uint64_t flshb_d_dat_ram_bist_status  : 1;
+	uint64_t flshb_m_dat_ram_bist_status  : 1;
+	uint64_t fillb_m_rsp_dat_fifo_bist_status : 1;
+	uint64_t fillb_d_rsp_dat_fifo_bist_status : 1;
+	uint64_t fillb_d_rsp_ram_lo_bist_status : 1;
+	uint64_t fillb_d_rsp_ram_hi_bist_status : 1;
+	uint64_t fillb_m_rsp_ram_lo_bist_status : 1;
+	uint64_t fillb_m_rsp_ram_hi_bist_status : 1;
+	uint64_t mwp_lo_ram_bist_status       : 1;
+	uint64_t mwp_hi_ram_bist_status       : 1;
+	uint64_t dwp_lo_ram_bist_status       : 1;
+	uint64_t dwp_hi_ram_bist_status       : 1;
+	uint64_t drp_lo_ram_bist_status       : 1;
+	uint64_t drp_hi_ram_bist_status       : 1;
+	uint64_t isrd_st_ram0_bist_status     : 1;
+	uint64_t isrd_st_ram1_bist_status     : 1;
+	uint64_t isrd_st_ram2_bist_status     : 1;
+	uint64_t isrd_st_ram3_bist_status     : 1;
+	uint64_t isrm_st_ram0_bist_status     : 1;
+	uint64_t isrm_st_ram1_bist_status     : 1;
+	uint64_t isrm_st_ram2_bist_status     : 1;
+	uint64_t isrm_ca_cm_ram_bist_status   : 1;
+	uint64_t isrm_ca_iinst_ram_bist_status : 1;
+	uint64_t flshb_cache_hi_ram_bist_status : 1;
+	uint64_t flshb_cache_lo_ram_bist_status : 1;
+#endif
+	} s;
+	struct cvmx_pko_pdm_bist_status_s     cn78xx;
+};
+typedef union cvmx_pko_pdm_bist_status cvmx_pko_pdm_bist_status_t;
+
+/**
+ * cvmx_pko_pdm_cfg
+ */
+union cvmx_pko_pdm_cfg {
+	uint64_t u64;
+	struct cvmx_pko_pdm_cfg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_13_63               : 51;
+	uint64_t dis_lpd_w2r_fill             : 1;  /**< Set to disable the write to read fill caused by LPD in the PDM. If disabled, must wait for
+                                                         FPD bit from PEB, which is a performance penalty when the time is large for the PEB
+                                                         request to make it back to PDM. For diagnostic use only. */
+	uint64_t en_fr_w2r_ptr_swp            : 1;  /**< Set to enable pointer swap on a fill response when we go in-sync (only one cacheline in
+                                                         DQ). For diagnostic use only. */
+	uint64_t dis_flsh_cache               : 1;  /**< Set to disable the flush buffer's cache. This makes all fills require full memory latency.
+                                                         For diagnostic use only. */
+	uint64_t pko_pad_minlen               : 7;  /**< Minimum frame padding min length. */
+	uint64_t diag_mode                    : 1;  /**< Set to enable read/write to memories in PDM through CSR interface. For diagnostic use only. */
+	uint64_t alloc_lds                    : 1;  /**< Allocate LDS. This signal prevents the loads to IOBP from being allocated in on-chip cache
+                                                         (LDWB vs. LDD). Two modes as follows: 0 = No allocate (LDWB); 1 = Allocate (LDD). */
+	uint64_t alloc_sts                    : 1;  /**< Allocate STS. This signal prevents the stores to NCB from being allocated in on-chip cache
+                                                         (STF vs. STT). Two modes as follows: 0 = No allocate (STT); 1 = Allocate (STF). */
+#else
+	uint64_t alloc_sts                    : 1;
+	uint64_t alloc_lds                    : 1;
+	uint64_t diag_mode                    : 1;
+	uint64_t pko_pad_minlen               : 7;
+	uint64_t dis_flsh_cache               : 1;
+	uint64_t en_fr_w2r_ptr_swp            : 1;
+	uint64_t dis_lpd_w2r_fill             : 1;
+	uint64_t reserved_13_63               : 51;
+#endif
+	} s;
+	struct cvmx_pko_pdm_cfg_s             cn78xx;
+};
+typedef union cvmx_pko_pdm_cfg cvmx_pko_pdm_cfg_t;
+
+/**
+ * cvmx_pko_pdm_cfg_dbg
+ */
+union cvmx_pko_pdm_cfg_dbg {
+	uint64_t u64;
+	struct cvmx_pko_pdm_cfg_dbg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_32_63               : 32;
+	uint64_t cp_stall_thrshld             : 32; /**< Program this register to the 32-bit number of cycles to test for the PDM(CP) stalled on
+                                                         inputs going into the ISRs. PKO_PDM_STS[CP_STALL_THRSHLD_HIT] indicates the threshold has
+                                                         been hit. INTERNAL: Do not list field in HRM. For lab debug only; will likely disappear in
+                                                         pass 2. */
+#else
+	uint64_t cp_stall_thrshld             : 32;
+	uint64_t reserved_32_63               : 32;
+#endif
+	} s;
+	struct cvmx_pko_pdm_cfg_dbg_s         cn78xx;
+};
+typedef union cvmx_pko_pdm_cfg_dbg cvmx_pko_pdm_cfg_dbg_t;
+
+/**
+ * cvmx_pko_pdm_cp_dbg
+ */
+union cvmx_pko_pdm_cp_dbg {
+	uint64_t u64;
+	struct cvmx_pko_pdm_cp_dbg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t stateless_fif_cnt            : 6;  /**< Stateless fifo count. */
+	uint64_t reserved_5_9                 : 5;
+	uint64_t op_fif_not_full              : 5;  /**< Output fifo not full signals. The order of the bits is:
+                                                         0x4 = ISR CMD FIFO not full.
+                                                         0x3 = DESC DAT FIFO HIGH not full.
+                                                         0x2 = DESC DAT FIFO LOW not full.
+                                                         0x1 = MP DAT FIFO not full.
+                                                         0x0 = PSE CMD RESP FIFO has credit. */
+#else
+	uint64_t op_fif_not_full              : 5;
+	uint64_t reserved_5_9                 : 5;
+	uint64_t stateless_fif_cnt            : 6;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_pko_pdm_cp_dbg_s          cn78xx;
+};
+typedef union cvmx_pko_pdm_cp_dbg cvmx_pko_pdm_cp_dbg_t;
+
+/**
+ * cvmx_pko_pdm_dq#_minpad
+ */
+union cvmx_pko_pdm_dqx_minpad {
+	uint64_t u64;
+	struct cvmx_pko_pdm_dqx_minpad_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t minpad                       : 1;  /**< MINPAD setting per DQ. Each DQ has a separate CSR address; and bit 0 of the data
+                                                         read/write value is the MINPAD bit. When MINPAD is set, the send-packet header has the
+                                                         total field adjusted by MINLEN (PKO_PDM_CFG[PKO_PAD_MINLEN]) as follows:
+                                                         if (MINPAD)
+                                                         if (send_hdr.total < MINLEN) send_hdr.total = MINLEN */
+#else
+	uint64_t minpad                       : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_pko_pdm_dqx_minpad_s      cn78xx;
+};
+typedef union cvmx_pko_pdm_dqx_minpad cvmx_pko_pdm_dqx_minpad_t;
+
+/**
+ * cvmx_pko_pdm_drpbuf_dbg
+ */
+union cvmx_pko_pdm_drpbuf_dbg {
+	uint64_t u64;
+	struct cvmx_pko_pdm_drpbuf_dbg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_43_63               : 21;
+	uint64_t sel_nxt_ptr                  : 1;  /**< Sel_nxt_ptr signal. */
+	uint64_t load_val                     : 1;  /**< Load valid signal. */
+	uint64_t rdy                          : 1;  /**< Ready signal. */
+	uint64_t cur_state                    : 3;  /**< Current state from the pbuf controller. */
+	uint64_t reserved_33_36               : 4;
+	uint64_t track_rd_cnt                 : 6;  /**< Track read count value. */
+	uint64_t track_wr_cnt                 : 6;  /**< Track write count value. */
+	uint64_t reserved_17_20               : 4;
+	uint64_t mem_addr                     : 13; /**< Memory address for pbuf ram. */
+	uint64_t mem_en                       : 4;  /**< Memory write/chip enable signals. The order of the bits is:
+                                                         0x3 = Low wen.
+                                                         0x2 = Low cen.
+                                                         0x1 = High wen.
+                                                         0x0 = High cen. */
+#else
+	uint64_t mem_en                       : 4;
+	uint64_t mem_addr                     : 13;
+	uint64_t reserved_17_20               : 4;
+	uint64_t track_wr_cnt                 : 6;
+	uint64_t track_rd_cnt                 : 6;
+	uint64_t reserved_33_36               : 4;
+	uint64_t cur_state                    : 3;
+	uint64_t rdy                          : 1;
+	uint64_t load_val                     : 1;
+	uint64_t sel_nxt_ptr                  : 1;
+	uint64_t reserved_43_63               : 21;
+#endif
+	} s;
+	struct cvmx_pko_pdm_drpbuf_dbg_s      cn78xx;
+};
+typedef union cvmx_pko_pdm_drpbuf_dbg cvmx_pko_pdm_drpbuf_dbg_t;
+
+/**
+ * cvmx_pko_pdm_dwpbuf_dbg
+ */
+union cvmx_pko_pdm_dwpbuf_dbg {
+	uint64_t u64;
+	struct cvmx_pko_pdm_dwpbuf_dbg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_43_63               : 21;
+	uint64_t sel_nxt_ptr                  : 1;  /**< Sel_nxt_ptr signal. */
+	uint64_t load_val                     : 1;  /**< Load valid signal. */
+	uint64_t rdy                          : 1;  /**< Ready signal. */
+	uint64_t cur_state                    : 3;  /**< Current state from the pbuf controller. */
+	uint64_t reserved_33_36               : 4;
+	uint64_t track_rd_cnt                 : 6;  /**< Track read count value. */
+	uint64_t track_wr_cnt                 : 6;  /**< Track write count value. */
+	uint64_t reserved_17_20               : 4;
+	uint64_t mem_addr                     : 13; /**< Memory address for pbuf ram. */
+	uint64_t mem_en                       : 4;  /**< Memory write/chip enable signals. The order of the bits is:
+                                                         0x3 = Low wen.
+                                                         0x2 = Low cen.
+                                                         0x1 = High wen.
+                                                         0x0 = High cen. */
+#else
+	uint64_t mem_en                       : 4;
+	uint64_t mem_addr                     : 13;
+	uint64_t reserved_17_20               : 4;
+	uint64_t track_wr_cnt                 : 6;
+	uint64_t track_rd_cnt                 : 6;
+	uint64_t reserved_33_36               : 4;
+	uint64_t cur_state                    : 3;
+	uint64_t rdy                          : 1;
+	uint64_t load_val                     : 1;
+	uint64_t sel_nxt_ptr                  : 1;
+	uint64_t reserved_43_63               : 21;
+#endif
+	} s;
+	struct cvmx_pko_pdm_dwpbuf_dbg_s      cn78xx;
+};
+typedef union cvmx_pko_pdm_dwpbuf_dbg cvmx_pko_pdm_dwpbuf_dbg_t;
+
+/**
+ * cvmx_pko_pdm_ecc_ctl0
+ */
+union cvmx_pko_pdm_ecc_ctl0 {
+	uint64_t u64;
+	struct cvmx_pko_pdm_ecc_ctl0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t flshb_cache_lo_ram_flip      : 2;  /**< FLSHB_CACHE_LO_RAM flip syndrome bits on write. */
+	uint64_t flshb_cache_lo_ram_cdis      : 1;  /**< FLSHB_CACHE_LO_RAM ECC correction disable. */
+	uint64_t flshb_cache_hi_ram_flip      : 2;  /**< FLSHB_CACHE_HI_RAM flip syndrome bits on write. */
+	uint64_t flshb_cache_hi_ram_cdis      : 1;  /**< FLSHB_CACHE_HI_RAM ECC correction disable. */
+	uint64_t isrm_ca_iinst_ram_flip       : 2;  /**< ISRM_CA_IINST_RAM flip syndrome bits on write. */
+	uint64_t isrm_ca_iinst_ram_cdis       : 1;  /**< ISRM_CA_IINST_RAM ECC correction disable. */
+	uint64_t isrm_ca_cm_ram_flip          : 2;  /**< ISRM_CA_CM_RAM flip syndrome bits on write. */
+	uint64_t isrm_ca_cm_ram_cdis          : 1;  /**< ISRM_CA_CM_RAM ECC correction disable. */
+	uint64_t isrm_st_ram2_flip            : 2;  /**< ISRM_ST_RAM2 flip syndrome bits on write. */
+	uint64_t isrm_st_ram2_cdis            : 1;  /**< ISRM_ST_RAM2 ECC correction disable. */
+	uint64_t isrm_st_ram1_flip            : 2;  /**< ISRM_ST_RAM1 flip syndrome bits on write. */
+	uint64_t isrm_st_ram1_cdis            : 1;  /**< ISRM_ST_RAM1 ECC correction disable. */
+	uint64_t isrm_st_ram0_flip            : 2;  /**< ISRM_ST_RAM0 flip syndrome bits on write. */
+	uint64_t isrm_st_ram0_cdis            : 1;  /**< ISRM_ST_RAM0 ECC correction disable. */
+	uint64_t isrd_st_ram3_flip            : 2;  /**< ISRD_ST_RAM3 flip syndrome bits on write. */
+	uint64_t isrd_st_ram3_cdis            : 1;  /**< ISRD_ST_RAM3 ECC correction disable. */
+	uint64_t isrd_st_ram2_flip            : 2;  /**< ISRD_ST_RAM2 flip syndrome bits on write. */
+	uint64_t isrd_st_ram2_cdis            : 1;  /**< ISRD_ST_RAM2 ECC correction disable. */
+	uint64_t isrd_st_ram1_flip            : 2;  /**< ISRD_ST_RAM1 flip syndrome bits on write. */
+	uint64_t isrd_st_ram1_cdis            : 1;  /**< ISRD_ST_RAM1 ECC correction disable. */
+	uint64_t isrd_st_ram0_flip            : 2;  /**< ISRD_ST_RAM0 flip syndrome bits on write. */
+	uint64_t isrd_st_ram0_cdis            : 1;  /**< ISRD_ST_RAM0 ECC correction disable. */
+	uint64_t drp_hi_ram_flip              : 2;  /**< DRP_HI_RAM flip syndrome bits on write. */
+	uint64_t drp_hi_ram_cdis              : 1;  /**< DRP_HI_RAM ECC correction disable. */
+	uint64_t drp_lo_ram_flip              : 2;  /**< DRP_LO_RAM flip syndrome bits on write. */
+	uint64_t drp_lo_ram_cdis              : 1;  /**< DRP_LO_RAM ECC correction disable. */
+	uint64_t dwp_hi_ram_flip              : 2;  /**< DWP_HI_RAM flip syndrome bits on write. */
+	uint64_t dwp_hi_ram_cdis              : 1;  /**< DWP_HI_RAM ECC correction disable. */
+	uint64_t dwp_lo_ram_flip              : 2;  /**< DWP_LO_RAM flip syndrome bits on write. */
+	uint64_t dwp_lo_ram_cdis              : 1;  /**< DWP_LO_RAM ECC correction disable. */
+	uint64_t mwp_hi_ram_flip              : 2;  /**< MWP_HI_RAM flip syndrome bits on write. */
+	uint64_t mwp_hi_ram_cdis              : 1;  /**< MWP_HI_RAM ECC correction disable. */
+	uint64_t mwp_lo_ram_flip              : 2;  /**< MWP_LO_RAM flip syndrome bits on write. */
+	uint64_t mwp_lo_ram_cdis              : 1;  /**< MWP_LO_RAM ECC correction disable. */
+	uint64_t fillb_m_rsp_ram_hi_flip      : 2;  /**< FILLB_M_RSP_RAM_HI flip syndrome bits on write. */
+	uint64_t fillb_m_rsp_ram_hi_cdis      : 1;  /**< FILLB_M_RSP_RAM_HI ECC correction disable. */
+	uint64_t fillb_m_rsp_ram_lo_flip      : 2;  /**< FILLB_M_RSP_RAM_LO flip syndrome bits on write. */
+	uint64_t fillb_m_rsp_ram_lo_cdis      : 1;  /**< FILLB_M_RSP_RAM_LO ECC correction disable. */
+	uint64_t fillb_d_rsp_ram_hi_flip      : 2;  /**< FILLB_D_RSP_RAM_LO flip syndrome bits on write. */
+	uint64_t fillb_d_rsp_ram_hi_cdis      : 1;  /**< FILLB_D_RSP_RAM_HI ECC correction disable. */
+	uint64_t fillb_d_rsp_ram_lo_flip      : 2;  /**< FILLB_D_DAT_RAM_LO flip syndrome bits on write. */
+	uint64_t fillb_d_rsp_ram_lo_cdis      : 1;  /**< FILLB_D_RSP_RAM_LO ECC correction disable. */
+	uint64_t reserved_0_0                 : 1;
+#else
+	uint64_t reserved_0_0                 : 1;
+	uint64_t fillb_d_rsp_ram_lo_cdis      : 1;
+	uint64_t fillb_d_rsp_ram_lo_flip      : 2;
+	uint64_t fillb_d_rsp_ram_hi_cdis      : 1;
+	uint64_t fillb_d_rsp_ram_hi_flip      : 2;
+	uint64_t fillb_m_rsp_ram_lo_cdis      : 1;
+	uint64_t fillb_m_rsp_ram_lo_flip      : 2;
+	uint64_t fillb_m_rsp_ram_hi_cdis      : 1;
+	uint64_t fillb_m_rsp_ram_hi_flip      : 2;
+	uint64_t mwp_lo_ram_cdis              : 1;
+	uint64_t mwp_lo_ram_flip              : 2;
+	uint64_t mwp_hi_ram_cdis              : 1;
+	uint64_t mwp_hi_ram_flip              : 2;
+	uint64_t dwp_lo_ram_cdis              : 1;
+	uint64_t dwp_lo_ram_flip              : 2;
+	uint64_t dwp_hi_ram_cdis              : 1;
+	uint64_t dwp_hi_ram_flip              : 2;
+	uint64_t drp_lo_ram_cdis              : 1;
+	uint64_t drp_lo_ram_flip              : 2;
+	uint64_t drp_hi_ram_cdis              : 1;
+	uint64_t drp_hi_ram_flip              : 2;
+	uint64_t isrd_st_ram0_cdis            : 1;
+	uint64_t isrd_st_ram0_flip            : 2;
+	uint64_t isrd_st_ram1_cdis            : 1;
+	uint64_t isrd_st_ram1_flip            : 2;
+	uint64_t isrd_st_ram2_cdis            : 1;
+	uint64_t isrd_st_ram2_flip            : 2;
+	uint64_t isrd_st_ram3_cdis            : 1;
+	uint64_t isrd_st_ram3_flip            : 2;
+	uint64_t isrm_st_ram0_cdis            : 1;
+	uint64_t isrm_st_ram0_flip            : 2;
+	uint64_t isrm_st_ram1_cdis            : 1;
+	uint64_t isrm_st_ram1_flip            : 2;
+	uint64_t isrm_st_ram2_cdis            : 1;
+	uint64_t isrm_st_ram2_flip            : 2;
+	uint64_t isrm_ca_cm_ram_cdis          : 1;
+	uint64_t isrm_ca_cm_ram_flip          : 2;
+	uint64_t isrm_ca_iinst_ram_cdis       : 1;
+	uint64_t isrm_ca_iinst_ram_flip       : 2;
+	uint64_t flshb_cache_hi_ram_cdis      : 1;
+	uint64_t flshb_cache_hi_ram_flip      : 2;
+	uint64_t flshb_cache_lo_ram_cdis      : 1;
+	uint64_t flshb_cache_lo_ram_flip      : 2;
+#endif
+	} s;
+	struct cvmx_pko_pdm_ecc_ctl0_s        cn78xx;
+};
+typedef union cvmx_pko_pdm_ecc_ctl0 cvmx_pko_pdm_ecc_ctl0_t;
+
+/**
+ * cvmx_pko_pdm_ecc_ctl1
+ */
+union cvmx_pko_pdm_ecc_ctl1 {
+	uint64_t u64;
+	struct cvmx_pko_pdm_ecc_ctl1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_3_63                : 61;
+	uint64_t minpad_ram_flip              : 2;  /**< MINPAD_RAM flip syndrome bits on write. */
+	uint64_t minpad_ram_cdis              : 1;  /**< MINPAD_RAM ECC correction disable. */
+#else
+	uint64_t minpad_ram_cdis              : 1;
+	uint64_t minpad_ram_flip              : 2;
+	uint64_t reserved_3_63                : 61;
+#endif
+	} s;
+	struct cvmx_pko_pdm_ecc_ctl1_s        cn78xx;
+};
+typedef union cvmx_pko_pdm_ecc_ctl1 cvmx_pko_pdm_ecc_ctl1_t;
+
+/**
+ * cvmx_pko_pdm_ecc_dbe_sts0
+ */
+union cvmx_pko_pdm_ecc_dbe_sts0 {
+	uint64_t u64;
+	struct cvmx_pko_pdm_ecc_dbe_sts0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t flshb_cache_lo_ram_dbe       : 1;  /**< Double-bit error for FLSHB_CACHE_LO_RAM. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.flshb.flshb_cache_lo */
+	uint64_t flshb_cache_hi_ram_dbe       : 1;  /**< Double-bit error for FLSHB_CACHE_HI_RAM. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.flshb.flshb_cache_hi */
+	uint64_t isrm_ca_iinst_ram_dbe        : 1;  /**< Double-bit error for ISRM_CA_IINST_RAM. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.isr.mp_isr.cred_accum.iinst_in_fif */
+	uint64_t isrm_ca_cm_ram_dbe           : 1;  /**< Double-bit error for ISRM_CA_CM_RAM. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.isr.mp_isr.cred_accum.cred_accum_ctrlr_and_mem.cred_accum_spr */
+	uint64_t isrm_st_ram2_dbe             : 1;  /**< Double-bit error for ISRM_ST_RAM2. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.isr.mp_isr.st_mem2 */
+	uint64_t isrm_st_ram1_dbe             : 1;  /**< Double-bit error for ISRM_ST_RAM1. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.isr.mp_isr.st_mem1 */
+	uint64_t isrm_st_ram0_dbe             : 1;  /**< Double-bit error for ISRM_ST_RAM0. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.isr.mp_isr.st_mem0 */
+	uint64_t isrd_st_ram3_dbe             : 1;  /**< Double-bit error for ISRD_ST_RAM3. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.isr.d_isr.st_mem3 */
+	uint64_t isrd_st_ram2_dbe             : 1;  /**< Double-bit error for ISRD_ST_RAM2. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.isr.d_isr.st_mem2 */
+	uint64_t isrd_st_ram1_dbe             : 1;  /**< Double-bit error for ISRD_ST_RAM1. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.isr.d_isr.st_mem1 */
+	uint64_t isrd_st_ram0_dbe             : 1;  /**< Double-bit error for ISRD_ST_RAM0. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.isr.d_isr.st_mem0 */
+	uint64_t drp_hi_ram_dbe               : 1;  /**< Double-bit error for DRP_HI_RAM. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.drpbuf.ram_128k_pbuf_1 */
+	uint64_t drp_lo_ram_dbe               : 1;  /**< Double-bit error for DRP_LO_RAM. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.drpbuf.ram_128k_pbuf_2 */
+	uint64_t dwp_hi_ram_dbe               : 1;  /**< Double-bit error for DWP_HI_RAM. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.dwpbuf.ram_128k_pbuf_high */
+	uint64_t dwp_lo_ram_dbe               : 1;  /**< Double-bit error for DWP_LO_RAM. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.dwpbuf.ram_128k_pbuf_low */
+	uint64_t mwp_hi_ram_dbe               : 1;  /**< Double-bit error for MWP_HI_RAM. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.mwpbuf.ram_128k_pbuf_high */
+	uint64_t mwp_lo_ram_dbe               : 1;  /**< Double-bit error for MWP_LO_RAM. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.mwpbuf.ram_128k_pbuf_low */
+	uint64_t fillb_m_rsp_ram_hi_dbe       : 1;  /**< Double-bit error for FILLB_M_DAT_RAM_HI. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.fillb.m_rsp_ram_hi */
+	uint64_t fillb_m_rsp_ram_lo_dbe       : 1;  /**< Double-bit error for FILLB_D_DAT_RAM_LO. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.fillb.m_rsp_ram_lo */
+	uint64_t fillb_d_rsp_ram_hi_dbe       : 1;  /**< Double-bit error for FILLB_D_DAT_RAM_HI. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.fillb.d_rsp_ram_hi */
+	uint64_t fillb_d_rsp_ram_lo_dbe       : 1;  /**< Double-bit error for FILLB_D_DAT_RAM_LO. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.fillb.d_rsp_ram_lo */
+	uint64_t minpad_ram_dbe               : 1;  /**< Double-bit error for MINPAD_RAM. INTERNAL: Instances: pko_pnr1.pko_pnr1_pdm.cp.minpad_ram */
+	uint64_t reserved_0_41                : 42;
+#else
+	uint64_t reserved_0_41                : 42;
+	uint64_t minpad_ram_dbe               : 1;
+	uint64_t fillb_d_rsp_ram_lo_dbe       : 1;
+	uint64_t fillb_d_rsp_ram_hi_dbe       : 1;
+	uint64_t fillb_m_rsp_ram_lo_dbe       : 1;
+	uint64_t fillb_m_rsp_ram_hi_dbe       : 1;
+	uint64_t mwp_lo_ram_dbe               : 1;
+	uint64_t mwp_hi_ram_dbe               : 1;
+	uint64_t dwp_lo_ram_dbe               : 1;
+	uint64_t dwp_hi_ram_dbe               : 1;
+	uint64_t drp_lo_ram_dbe               : 1;
+	uint64_t drp_hi_ram_dbe               : 1;
+	uint64_t isrd_st_ram0_dbe             : 1;
+	uint64_t isrd_st_ram1_dbe             : 1;
+	uint64_t isrd_st_ram2_dbe             : 1;
+	uint64_t isrd_st_ram3_dbe             : 1;
+	uint64_t isrm_st_ram0_dbe             : 1;
+	uint64_t isrm_st_ram1_dbe             : 1;
+	uint64_t isrm_st_ram2_dbe             : 1;
+	uint64_t isrm_ca_cm_ram_dbe           : 1;
+	uint64_t isrm_ca_iinst_ram_dbe        : 1;
+	uint64_t flshb_cache_hi_ram_dbe       : 1;
+	uint64_t flshb_cache_lo_ram_dbe       : 1;
+#endif
+	} s;
+	struct cvmx_pko_pdm_ecc_dbe_sts0_s    cn78xx;
+};
+typedef union cvmx_pko_pdm_ecc_dbe_sts0 cvmx_pko_pdm_ecc_dbe_sts0_t;
+
+/**
+ * cvmx_pko_pdm_ecc_dbe_sts_cmb0
+ */
+union cvmx_pko_pdm_ecc_dbe_sts_cmb0 {
+	uint64_t u64;
+	struct cvmx_pko_pdm_ecc_dbe_sts_cmb0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t pdm_dbe_cmb0                 : 1;  /**< This bit is the logical OR of all bits in PKO_PDM_ECC_DBE_STS0. To clear this bit,
+                                                         software
+                                                         must clear bits in PKO_PDM_ECC_DBE_STS0. When this bit is set, the corresponding interrupt
+                                                         is set. Throws PKO_INTSN_E::PKO_PDM_DBE_CMB0. INTERNAL: Instances:
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.flshb.flshb_cache_hi
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.flshb.flshb_cache_lo
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.isr.mp_isr.cred_accum.iinst_in_fif
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.isr.mp_isr.cred_accum.cred_accum_ctrlr_and_mem.cred_
+                                                         accum_spr fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.isr.mp_isr.st_mem0
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.isr.mp_isr.st_mem1
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.isr.mp_isr.st_mem2
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.isr.d_isr.st_mem0
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.isr.d_isr.st_mem1
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.isr.d_isr.st_mem2
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.isr.d_isr.st_mem3
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.drpbuf.ram_128k_pbuf_1
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.drpbuf.ram_128k_pbuf_2
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.dwpbuf.ram_128k_pbuf_low
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.dwpbuf.ram_128k_pbuf_high
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.mwpbuf.ram_128k_pbuf_low
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.mwpbuf.ram_128k_pbuf_high
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.fillb.d_rsp_ram_hi
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.fillb.d_rsp_ram_lo
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.fillb.m_rsp_dat_fifo
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.cp.minpad_ram */
+	uint64_t reserved_0_62                : 63;
+#else
+	uint64_t reserved_0_62                : 63;
+	uint64_t pdm_dbe_cmb0                 : 1;
+#endif
+	} s;
+	struct cvmx_pko_pdm_ecc_dbe_sts_cmb0_s cn78xx;
+};
+typedef union cvmx_pko_pdm_ecc_dbe_sts_cmb0 cvmx_pko_pdm_ecc_dbe_sts_cmb0_t;
+
+/**
+ * cvmx_pko_pdm_ecc_sbe_sts0
+ */
+union cvmx_pko_pdm_ecc_sbe_sts0 {
+	uint64_t u64;
+	struct cvmx_pko_pdm_ecc_sbe_sts0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t flshb_cache_lo_ram_sbe       : 1;  /**< Single-bit error for FLSHB_CACHE_LO_RAM. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.flshb.flshb_cache_lo */
+	uint64_t flshb_cache_hi_ram_sbe       : 1;  /**< Single-bit error for FLSHB_CACHE_HI_RAM. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.flshb.flshb_cache_hi */
+	uint64_t isrm_ca_iinst_ram_sbe        : 1;  /**< Single-bit error for ISRM_CA_IINST_RAM. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.isr.mp_isr.cred_accum.iinst_in_fif */
+	uint64_t isrm_ca_cm_ram_sbe           : 1;  /**< Single-bit error for ISRM_CA_CM_RAM. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.isr.mp_isr.cred_accum.cred_accum_ctrlr_and_mem.cred_accum_spr */
+	uint64_t isrm_st_ram2_sbe             : 1;  /**< Single-bit error for ISRM_ST_RAM2. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.isr.mp_isr.st_mem2 */
+	uint64_t isrm_st_ram1_sbe             : 1;  /**< Single-bit error for ISRM_ST_RAM1. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.isr.mp_isr.st_mem1 */
+	uint64_t isrm_st_ram0_sbe             : 1;  /**< Single-bit error for ISRM_ST_RAM0. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.isr.mp_isr.st_mem0 */
+	uint64_t isrd_st_ram3_sbe             : 1;  /**< Single-bit error for ISRD_ST_RAM3. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.isr.d_isr.st_mem3 */
+	uint64_t isrd_st_ram2_sbe             : 1;  /**< Single-bit error for ISRD_ST_RAM2. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.isr.d_isr.st_mem2 */
+	uint64_t isrd_st_ram1_sbe             : 1;  /**< Single-bit error for ISRD_ST_RAM1. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.isr.d_isr.st_mem1 */
+	uint64_t isrd_st_ram0_sbe             : 1;  /**< Single-bit error for ISRD_ST_RAM0. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.isr.d_isr.st_mem0 */
+	uint64_t drp_hi_ram_sbe               : 1;  /**< Single-bit error for DRP_HI_RAM. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.drpbuf.ram_128k_pbuf_1 */
+	uint64_t drp_lo_ram_sbe               : 1;  /**< Single-bit error for DRP_LO_RAM. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.drpbuf.ram_128k_pbuf_2 */
+	uint64_t dwp_hi_ram_sbe               : 1;  /**< Single-bit error for DWP_HI_RAM. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.dwpbuf.ram_128k_pbuf_high */
+	uint64_t dwp_lo_ram_sbe               : 1;  /**< Single-bit error for DWP_LO_RAM. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.dwpbuf.ram_128k_pbuf_low */
+	uint64_t mwp_hi_ram_sbe               : 1;  /**< Single-bit error for MWP_HI_RAM. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.mwpbuf.ram_128k_pbuf_high */
+	uint64_t mwp_lo_ram_sbe               : 1;  /**< Single-bit error for MWP_LO_RAM. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.mwpbuf.ram_128k_pbuf_low */
+	uint64_t fillb_m_rsp_ram_hi_sbe       : 1;  /**< Single-bit error for FILLB_M_RSP_RAM_HI. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.fillb.m_rsp_ram_hi */
+	uint64_t fillb_m_rsp_ram_lo_sbe       : 1;  /**< Single-bit error for FILLB_M_RSP_RAM_LO. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.fillb.m_rsp_ram_lo */
+	uint64_t fillb_d_rsp_ram_hi_sbe       : 1;  /**< Single-bit error for FILLB_D_RSP_RAM_HI. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.fillb.d_rsp_ram_hi */
+	uint64_t fillb_d_rsp_ram_lo_sbe       : 1;  /**< Single-bit error for FILLB_D_RSP_RAM_LO. INTERNAL: Instances:
+                                                         pko_pnr1.pko_pnr1_pdm.fillb.d_rsp_ram_lo */
+	uint64_t minpad_ram_sbe               : 1;  /**< Single-bit error for MINPAD_RAM. INTERNAL: Instances: pko_pnr1.pko_pnr1_pdm.cp.minpad_ram */
+	uint64_t reserved_0_41                : 42;
+#else
+	uint64_t reserved_0_41                : 42;
+	uint64_t minpad_ram_sbe               : 1;
+	uint64_t fillb_d_rsp_ram_lo_sbe       : 1;
+	uint64_t fillb_d_rsp_ram_hi_sbe       : 1;
+	uint64_t fillb_m_rsp_ram_lo_sbe       : 1;
+	uint64_t fillb_m_rsp_ram_hi_sbe       : 1;
+	uint64_t mwp_lo_ram_sbe               : 1;
+	uint64_t mwp_hi_ram_sbe               : 1;
+	uint64_t dwp_lo_ram_sbe               : 1;
+	uint64_t dwp_hi_ram_sbe               : 1;
+	uint64_t drp_lo_ram_sbe               : 1;
+	uint64_t drp_hi_ram_sbe               : 1;
+	uint64_t isrd_st_ram0_sbe             : 1;
+	uint64_t isrd_st_ram1_sbe             : 1;
+	uint64_t isrd_st_ram2_sbe             : 1;
+	uint64_t isrd_st_ram3_sbe             : 1;
+	uint64_t isrm_st_ram0_sbe             : 1;
+	uint64_t isrm_st_ram1_sbe             : 1;
+	uint64_t isrm_st_ram2_sbe             : 1;
+	uint64_t isrm_ca_cm_ram_sbe           : 1;
+	uint64_t isrm_ca_iinst_ram_sbe        : 1;
+	uint64_t flshb_cache_hi_ram_sbe       : 1;
+	uint64_t flshb_cache_lo_ram_sbe       : 1;
+#endif
+	} s;
+	struct cvmx_pko_pdm_ecc_sbe_sts0_s    cn78xx;
+};
+typedef union cvmx_pko_pdm_ecc_sbe_sts0 cvmx_pko_pdm_ecc_sbe_sts0_t;
+
+/**
+ * cvmx_pko_pdm_ecc_sbe_sts_cmb0
+ */
+union cvmx_pko_pdm_ecc_sbe_sts_cmb0 {
+	uint64_t u64;
+	struct cvmx_pko_pdm_ecc_sbe_sts_cmb0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t pdm_sbe_cmb0                 : 1;  /**< This bit is the logical OR of all bits in PKO_PDM_ECC_SBE_STS0. To clear this bit,
+                                                         software
+                                                         must clear bits in PKO_PDM_ECC_SBE_STS0. When this bit is set, the corresponding interrupt
+                                                         is set. Throws PKO_INTSN_E::PKO_PDM_SBE_CMB0. INTERNAL: Instances:
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.flshb.flshb_cache_hi
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.flshb.flshb_cache_lo
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.isr.mp_isr.cred_accum.iinst_in_fif
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.isr.mp_isr.cred_accum.cred_accum_ctrlr_and_mem.cred_
+                                                         accum_spr fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.isr.mp_isr.st_mem0
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.isr.mp_isr.st_mem1
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.isr.mp_isr.st_mem2
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.isr.d_isr.st_mem0
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.isr.d_isr.st_mem1
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.isr.d_isr.st_mem2
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.isr.d_isr.st_mem3
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.drpbuf.ram_128k_pbuf_1
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.drpbuf.ram_128k_pbuf_2
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.dwpbuf.ram_128k_pbuf_low
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.dwpbuf.ram_128k_pbuf_high
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.mwpbuf.ram_128k_pbuf_low
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.mwpbuf.ram_128k_pbuf_high
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.fillb.d_rsp_ram_hi
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.fillb.d_rsp_ram_lo
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.fillb.m_rsp_dat_fifo
+                                                         fc.core.roc.pko.pko_pnr1.pko_pnr1_pdm.cp.minpad_ram */
+	uint64_t reserved_0_62                : 63;
+#else
+	uint64_t reserved_0_62                : 63;
+	uint64_t pdm_sbe_cmb0                 : 1;
+#endif
+	} s;
+	struct cvmx_pko_pdm_ecc_sbe_sts_cmb0_s cn78xx;
+};
+typedef union cvmx_pko_pdm_ecc_sbe_sts_cmb0 cvmx_pko_pdm_ecc_sbe_sts_cmb0_t;
+
+/**
+ * cvmx_pko_pdm_fillb_dbg0
+ */
+union cvmx_pko_pdm_fillb_dbg0 {
+	uint64_t u64;
+	struct cvmx_pko_pdm_fillb_dbg0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_57_63               : 7;
+	uint64_t pd_seq                       : 5;  /**< Sequence number for next packet descriptor fill request */
+	uint64_t resp_pd_seq                  : 5;  /**< Sequence number for next response to be written into packet descriptor buffer RAM */
+	uint64_t d_rsp_lo_ram_addr_sel        : 2;  /**< Source of read/write address to low PD fill buffer RAM.
+                                                         0x0 = No access.
+                                                         0x1 = Read access sourced by PD fill buffer response FIFO (feeding DRPBUF).
+                                                         0x2 = Write access sourced by IOBP0.
+                                                         0x3 = Write access sourced by flush buffer. */
+	uint64_t d_rsp_hi_ram_addr_sel        : 2;  /**< Source of read/write address to high PD fill buffer RAM.
+                                                         0x0 = No access.
+                                                         0x1 = Read access sourced by PD fill buffer response FIFO (feeding DRPBUF).
+                                                         0x2 = Write access sourced by IOBP0.
+                                                         0x3 = Write access sourced by flush buffer. */
+	uint64_t d_rsp_rd_seq                 : 5;  /**< Sequence number for next response to be read from packet descriptor buffer RAM */
+	uint64_t d_rsp_fifo_rd_seq            : 5;  /**< Sequence number for next PD fill response to be sent to DRPBUF */
+	uint64_t d_fill_req_fifo_val          : 1;  /**< Fill buffer PD read request FIFO has a valid entry */
+	uint64_t d_rsp_ram_valid              : 32; /**< Fill buffer packet descriptor RAM valid flags */
+#else
+	uint64_t d_rsp_ram_valid              : 32;
+	uint64_t d_fill_req_fifo_val          : 1;
+	uint64_t d_rsp_fifo_rd_seq            : 5;
+	uint64_t d_rsp_rd_seq                 : 5;
+	uint64_t d_rsp_hi_ram_addr_sel        : 2;
+	uint64_t d_rsp_lo_ram_addr_sel        : 2;
+	uint64_t resp_pd_seq                  : 5;
+	uint64_t pd_seq                       : 5;
+	uint64_t reserved_57_63               : 7;
+#endif
+	} s;
+	struct cvmx_pko_pdm_fillb_dbg0_s      cn78xx;
+};
+typedef union cvmx_pko_pdm_fillb_dbg0 cvmx_pko_pdm_fillb_dbg0_t;
+
+/**
+ * cvmx_pko_pdm_fillb_dbg1
+ */
+union cvmx_pko_pdm_fillb_dbg1 {
+	uint64_t u64;
+	struct cvmx_pko_pdm_fillb_dbg1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_57_63               : 7;
+	uint64_t mp_seq                       : 5;  /**< Sequence number for next meta packet cache line fill request */
+	uint64_t resp_mp_seq                  : 5;  /**< Sequence number for next response to be written into meta packet buffer RAM */
+	uint64_t m_rsp_lo_ram_addr_sel        : 2;  /**< Source of read/write address to low MP fill buffer RAM.
+                                                         0x0 = No access.
+                                                         0x1 = Read access sourced by MP fill buffer response FIFO (feeding DRPBUF).
+                                                         0x2 = Write access sourced by IOBP0.
+                                                         0x3 = Write access sourced by flush buffer. */
+	uint64_t m_rsp_hi_ram_addr_sel        : 2;  /**< Source of read/write address to high MP fill buffer RAM.
+                                                         0x0 = No access.
+                                                         0x1 = Read access sourced by MP fill buffer response FIFO (feeding DRPBUF).
+                                                         0x2 = Write access sourced by IOBP0.
+                                                         0x3 = Write access sourced by flush buffer. */
+	uint64_t m_rsp_rd_seq                 : 5;  /**< Sequence number for next response to be read from meta packet buffer RAM */
+	uint64_t m_rsp_fifo_rd_seq            : 5;  /**< Sequence number for next MP fill response to be sent to DRPBUF */
+	uint64_t m_fill_req_fifo_val          : 1;  /**< Fill buffer MP read request FIFO has a valid entry */
+	uint64_t m_rsp_ram_valid              : 32; /**< Fill buffer meta packet RAM valid flags */
+#else
+	uint64_t m_rsp_ram_valid              : 32;
+	uint64_t m_fill_req_fifo_val          : 1;
+	uint64_t m_rsp_fifo_rd_seq            : 5;
+	uint64_t m_rsp_rd_seq                 : 5;
+	uint64_t m_rsp_hi_ram_addr_sel        : 2;
+	uint64_t m_rsp_lo_ram_addr_sel        : 2;
+	uint64_t resp_mp_seq                  : 5;
+	uint64_t mp_seq                       : 5;
+	uint64_t reserved_57_63               : 7;
+#endif
+	} s;
+	struct cvmx_pko_pdm_fillb_dbg1_s      cn78xx;
+};
+typedef union cvmx_pko_pdm_fillb_dbg1 cvmx_pko_pdm_fillb_dbg1_t;
+
+/**
+ * cvmx_pko_pdm_fillb_dbg2
+ */
+union cvmx_pko_pdm_fillb_dbg2 {
+	uint64_t u64;
+	struct cvmx_pko_pdm_fillb_dbg2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t fillb_sm                     : 5;  /**< Fill buffer state machine */
+	uint64_t reserved_3_3                 : 1;
+	uint64_t iobp0_credit_cntr            : 3;  /**< IOBP0 read request credit counter */
+#else
+	uint64_t iobp0_credit_cntr            : 3;
+	uint64_t reserved_3_3                 : 1;
+	uint64_t fillb_sm                     : 5;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_pko_pdm_fillb_dbg2_s      cn78xx;
+};
+typedef union cvmx_pko_pdm_fillb_dbg2 cvmx_pko_pdm_fillb_dbg2_t;
+
+/**
+ * cvmx_pko_pdm_flshb_dbg0
+ */
+union cvmx_pko_pdm_flshb_dbg0 {
+	uint64_t u64;
+	struct cvmx_pko_pdm_flshb_dbg0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_44_63               : 20;
+	uint64_t flshb_sm                     : 7;  /**< Flush buffer state machine */
+	uint64_t flshb_ctl_sm                 : 9;  /**< Flush buffer control state machine */
+	uint64_t cam_hptr                     : 5;  /**< Flush buffer CAM head pointer */
+	uint64_t cam_tptr                     : 5;  /**< Flush buffer CAM tail pointer */
+	uint64_t expected_stdns               : 6;  /**< Number of store done responses still pending */
+	uint64_t d_flshb_eot_cntr             : 3;  /**< Number of packet descriptor flush requests pending */
+	uint64_t m_flshb_eot_cntr             : 3;  /**< Number of meta packet flush requests pending */
+	uint64_t ncbi_credit_cntr             : 6;  /**< NCBI FIFO credit counter */
+#else
+	uint64_t ncbi_credit_cntr             : 6;
+	uint64_t m_flshb_eot_cntr             : 3;
+	uint64_t d_flshb_eot_cntr             : 3;
+	uint64_t expected_stdns               : 6;
+	uint64_t cam_tptr                     : 5;
+	uint64_t cam_hptr                     : 5;
+	uint64_t flshb_ctl_sm                 : 9;
+	uint64_t flshb_sm                     : 7;
+	uint64_t reserved_44_63               : 20;
+#endif
+	} s;
+	struct cvmx_pko_pdm_flshb_dbg0_s      cn78xx;
+};
+typedef union cvmx_pko_pdm_flshb_dbg0 cvmx_pko_pdm_flshb_dbg0_t;
+
+/**
+ * cvmx_pko_pdm_flshb_dbg1
+ */
+union cvmx_pko_pdm_flshb_dbg1 {
+	uint64_t u64;
+	struct cvmx_pko_pdm_flshb_dbg1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t cam_stdn                     : 32; /**< Flush buffer entry store done request flags */
+	uint64_t cam_valid                    : 32; /**< Flush buffer entry valid flags */
+#else
+	uint64_t cam_valid                    : 32;
+	uint64_t cam_stdn                     : 32;
+#endif
+	} s;
+	struct cvmx_pko_pdm_flshb_dbg1_s      cn78xx;
+};
+typedef union cvmx_pko_pdm_flshb_dbg1 cvmx_pko_pdm_flshb_dbg1_t;
+
+/**
+ * cvmx_pko_pdm_isrd_dbg
+ */
+union cvmx_pko_pdm_isrd_dbg {
+	uint64_t u64;
+	struct cvmx_pko_pdm_isrd_dbg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_44_63               : 20;
+	uint64_t in_arb_reqs                  : 8;  /**< Input arbitration request signals. The order of the bits is:
+                                                         0x2B = Fill response - normal path request.
+                                                         0x2A = Fill response - flushb path request.
+                                                         0x29 = CP queue-open request.
+                                                         0x28 = CP queue-closed request.
+                                                         0x27 = CP queue-query request.
+                                                         0x26 = CP send-packet request.
+                                                         0x25 = PEB fill request.
+                                                         0x24 = PEB read request. */
+	uint64_t in_arb_gnts                  : 7;  /**< Input arbitration grant signals. The order of the bits is:
+                                                         0x23 = Fill response grant.
+                                                         0x22 = CP - queue-open grant.
+                                                         0x21 = CP - queue-close grant.
+                                                         0x20 = CP - queue-query grant.
+                                                         0x1F = CP - send-packet grant.
+                                                         0x1E = PEB fill grant.
+                                                         0x1D = PEB read grant. */
+	uint64_t cmt_arb_reqs                 : 7;  /**< Commit arbitration request signals. The order of the bits is:
+                                                         0x1C = Fill response grant.
+                                                         0x1B = CP - queue-open grant.
+                                                         0x1A = CP - queue-close grant.
+                                                         0x19 = CP - queue-query grant.
+                                                         0x18 = CP - send-packet grant.
+                                                         0x17 = PEB fill grant.
+                                                         0x16 = PEB read grant. */
+	uint64_t cmt_arb_gnts                 : 7;  /**< Commit arbitration grant signals. The order of the bits is:
+                                                         0x15 = Fill response grant.
+                                                         0x14 = CP - queue-open grant.
+                                                         0x13 = CP - queue-close grant.
+                                                         0x12 = CP - queue-query grant.
+                                                         0x11 = CP - send-packet grant.
+                                                         0x10 = PEB fill grant.
+                                                         0xF = PEB read grant. */
+	uint64_t in_use                       : 4;  /**< In use signals indicate the execution units are in use. The order of the bits is:
+                                                         0xE = PEB fill unit.
+                                                         0xD = PEB read unit.
+                                                         0xC = CP unit.
+                                                         0xB = Fill response unit. */
+	uint64_t has_cred                     : 4;  /**< Has credit signals indicate there is sufficient credit to commit. The order of the bits
+                                                         is:
+                                                         0xA = Flush buffer has credit
+                                                         0x9 = Fill buffer has credit
+                                                         0x8 = DW command output FIFO has credit
+                                                         0x7 = DR command output FIFO has credit */
+	uint64_t val_exec                     : 7;  /**< Valid bits for the execution units; means the unit can commit if it gets the grant of the
+                                                         commit arb and other conditions are met. The order of the bits is:
+                                                         0x6 = Fill response unit.
+                                                         0x5 = CP unit - queue-open.
+                                                         0x4 = CP unit - queue-close.
+                                                         0x3 = CP unit - queue-probe.
+                                                         0x2 = CP unit - send-packet.
+                                                         0x1 = PEB fill unit.
+                                                         0x0 = PEB read unit. */
+#else
+	uint64_t val_exec                     : 7;
+	uint64_t has_cred                     : 4;
+	uint64_t in_use                       : 4;
+	uint64_t cmt_arb_gnts                 : 7;
+	uint64_t cmt_arb_reqs                 : 7;
+	uint64_t in_arb_gnts                  : 7;
+	uint64_t in_arb_reqs                  : 8;
+	uint64_t reserved_44_63               : 20;
+#endif
+	} s;
+	struct cvmx_pko_pdm_isrd_dbg_s        cn78xx;
+};
+typedef union cvmx_pko_pdm_isrd_dbg cvmx_pko_pdm_isrd_dbg_t;
+
+/**
+ * cvmx_pko_pdm_isrd_dbg_dq
+ */
+union cvmx_pko_pdm_isrd_dbg_dq {
+	uint64_t u64;
+	struct cvmx_pko_pdm_isrd_dbg_dq_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_46_63               : 18;
+	uint64_t pebrd_sic_dq                 : 10; /**< CP SIC's DQ number. */
+	uint64_t reserved_34_35               : 2;
+	uint64_t pebfill_sic_dq               : 10; /**< CP SIC's DQ number. */
+	uint64_t reserved_22_23               : 2;
+	uint64_t fr_sic_dq                    : 10; /**< CP SIC's DQ number. */
+	uint64_t reserved_10_11               : 2;
+	uint64_t cp_sic_dq                    : 10; /**< CP SIC's DQ number. */
+#else
+	uint64_t cp_sic_dq                    : 10;
+	uint64_t reserved_10_11               : 2;
+	uint64_t fr_sic_dq                    : 10;
+	uint64_t reserved_22_23               : 2;
+	uint64_t pebfill_sic_dq               : 10;
+	uint64_t reserved_34_35               : 2;
+	uint64_t pebrd_sic_dq                 : 10;
+	uint64_t reserved_46_63               : 18;
+#endif
+	} s;
+	struct cvmx_pko_pdm_isrd_dbg_dq_s     cn78xx;
+};
+typedef union cvmx_pko_pdm_isrd_dbg_dq cvmx_pko_pdm_isrd_dbg_dq_t;
+
+/**
+ * cvmx_pko_pdm_isrm_dbg
+ */
+union cvmx_pko_pdm_isrm_dbg {
+	uint64_t u64;
+	struct cvmx_pko_pdm_isrm_dbg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_34_63               : 30;
+	uint64_t in_arb_reqs                  : 7;  /**< Input arbitration request signals. The order of the bits is:
+                                                         0x21 = PSE ACK.
+                                                         0x20 = Fill Response - normal path request.
+                                                         0x1F = Fill Response - flushb path request.
+                                                         0x1E = CP queue-open.
+                                                         0x1D = CP queue-closed.
+                                                         0x1C = CP queue-query.
+                                                         0x1B = CP send-packet. */
+	uint64_t in_arb_gnts                  : 6;  /**< Input arbitration grant signals. The order of the bits is:
+                                                         0x1A = PSE ACK.
+                                                         0x19 = Fill Response.
+                                                         0x18 = CP - queue-open.
+                                                         0x17 = CP - queue-close.
+                                                         0x16 = CP - queue-query.
+                                                         0x15 = CP - send-packet. */
+	uint64_t cmt_arb_reqs                 : 6;  /**< Commit arbitration request signals. The order of the bits is:
+                                                         0x14 = PSE ACK.
+                                                         0x13 = Fill Response.
+                                                         0x12 = CP - queue-open.
+                                                         0x11 = CP - queue-close.
+                                                         0x10 = CP - queue-query.
+                                                         0xF CP - send-packet. */
+	uint64_t cmt_arb_gnts                 : 6;  /**< Commit arbitration grant signals. The order of the bits is:
+                                                         0xE = PSE ACK.
+                                                         0xD = Fill Response.
+                                                         0xC = CP - queue-open.
+                                                         0xB = CP - queue-close.
+                                                         0xA = CP - queue-query.
+                                                         0x9 = CP - send-packet. */
+	uint64_t in_use                       : 3;  /**< In use signals indicate the execution units are in use. The order of the bits is:
+                                                         0x8 = (PSE) ACK unit.
+                                                         0x7 = Fill response unit.
+                                                         0x6 = CP unit. */
+	uint64_t has_cred                     : 3;  /**< Has credit signals indicate there is sufficient credit to commit. The order of the bits
+                                                         is:
+                                                         0x5 = Flush buffer has credit.
+                                                         0x4 = Fill buffer has credit.
+                                                         0x3 = MWP command output FIFO has credit. */
+	uint64_t val_exec                     : 3;  /**< Valid bits for the execution units; means the unit can commit if it gets the grant of the
+                                                         commit arb and other conditions are met. The order of the bits is:
+                                                         0x2 = (PSE) ACK unit.
+                                                         0x1 = Fill response unit.
+                                                         0x0 = CP unit - ALL. */
+#else
+	uint64_t val_exec                     : 3;
+	uint64_t has_cred                     : 3;
+	uint64_t in_use                       : 3;
+	uint64_t cmt_arb_gnts                 : 6;
+	uint64_t cmt_arb_reqs                 : 6;
+	uint64_t in_arb_gnts                  : 6;
+	uint64_t in_arb_reqs                  : 7;
+	uint64_t reserved_34_63               : 30;
+#endif
+	} s;
+	struct cvmx_pko_pdm_isrm_dbg_s        cn78xx;
+};
+typedef union cvmx_pko_pdm_isrm_dbg cvmx_pko_pdm_isrm_dbg_t;
+
+/**
+ * cvmx_pko_pdm_isrm_dbg_dq
+ */
+union cvmx_pko_pdm_isrm_dbg_dq {
+	uint64_t u64;
+	struct cvmx_pko_pdm_isrm_dbg_dq_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_34_63               : 30;
+	uint64_t ack_sic_dq                   : 10; /**< CP SIC's DQ number. */
+	uint64_t reserved_22_23               : 2;
+	uint64_t fr_sic_dq                    : 10; /**< CP SIC's DQ number. */
+	uint64_t reserved_10_11               : 2;
+	uint64_t cp_sic_dq                    : 10; /**< CP SIC's DQ number. */
+#else
+	uint64_t cp_sic_dq                    : 10;
+	uint64_t reserved_10_11               : 2;
+	uint64_t fr_sic_dq                    : 10;
+	uint64_t reserved_22_23               : 2;
+	uint64_t ack_sic_dq                   : 10;
+	uint64_t reserved_34_63               : 30;
+#endif
+	} s;
+	struct cvmx_pko_pdm_isrm_dbg_dq_s     cn78xx;
+};
+typedef union cvmx_pko_pdm_isrm_dbg_dq cvmx_pko_pdm_isrm_dbg_dq_t;
+
+/**
+ * cvmx_pko_pdm_mem_addr
+ */
+union cvmx_pko_pdm_mem_addr {
+	uint64_t u64;
+	struct cvmx_pko_pdm_mem_addr_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t memsel                       : 3;  /**< Memory select. Selects the RAM to read or write to.
+                                                         0 = Invalid.
+                                                         1 = ISRM states.
+                                                         2 = ISRD states.
+                                                         3 = DWPBUF.
+                                                         4 = DRPBUF.
+                                                         5 = MWPBUF. */
+	uint64_t reserved_17_60               : 44;
+	uint64_t memaddr                      : 14; /**< Memory address for the RAM. */
+	uint64_t reserved_2_2                 : 1;
+	uint64_t membanksel                   : 2;  /**< Memory bank select. Selects the bank to write to. Note that bit 0 is the only bit used in
+                                                         the PBUF's because there are only 2 banks per each PBUF. In the ISRM bank sel 3 is
+                                                         illegal. */
+#else
+	uint64_t membanksel                   : 2;
+	uint64_t reserved_2_2                 : 1;
+	uint64_t memaddr                      : 14;
+	uint64_t reserved_17_60               : 44;
+	uint64_t memsel                       : 3;
+#endif
+	} s;
+	struct cvmx_pko_pdm_mem_addr_s        cn78xx;
+};
+typedef union cvmx_pko_pdm_mem_addr cvmx_pko_pdm_mem_addr_t;
+
+/**
+ * cvmx_pko_pdm_mem_data
+ */
+union cvmx_pko_pdm_mem_data {
+	uint64_t u64;
+	struct cvmx_pko_pdm_mem_data_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t data                         : 64; /**< Raw data to write into the memory, or the raw data read out from the memory. Note that the
+                                                         ISR RAMs are only 57 bits wide, so [56:0] are the only bits that can be read or written to
+                                                         them. The PBUFs are 64 bits wide. */
+#else
+	uint64_t data                         : 64;
+#endif
+	} s;
+	struct cvmx_pko_pdm_mem_data_s        cn78xx;
+};
+typedef union cvmx_pko_pdm_mem_data cvmx_pko_pdm_mem_data_t;
+
+/**
+ * cvmx_pko_pdm_mem_rw_ctl
+ */
+union cvmx_pko_pdm_mem_rw_ctl {
+	uint64_t u64;
+	struct cvmx_pko_pdm_mem_rw_ctl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_2_63                : 62;
+	uint64_t read                         : 1;  /**< Set to 1 to read memory. */
+	uint64_t write                        : 1;  /**< Set to 1 to write memory. */
+#else
+	uint64_t write                        : 1;
+	uint64_t read                         : 1;
+	uint64_t reserved_2_63                : 62;
+#endif
+	} s;
+	struct cvmx_pko_pdm_mem_rw_ctl_s      cn78xx;
+};
+typedef union cvmx_pko_pdm_mem_rw_ctl cvmx_pko_pdm_mem_rw_ctl_t;
+
+/**
+ * cvmx_pko_pdm_mem_rw_sts
+ */
+union cvmx_pko_pdm_mem_rw_sts {
+	uint64_t u64;
+	struct cvmx_pko_pdm_mem_rw_sts_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_1_63                : 63;
+	uint64_t readdone                     : 1;  /**< This bit is set to 1 when the read is complete and the data is valid in the data register. */
+#else
+	uint64_t readdone                     : 1;
+	uint64_t reserved_1_63                : 63;
+#endif
+	} s;
+	struct cvmx_pko_pdm_mem_rw_sts_s      cn78xx;
+};
+typedef union cvmx_pko_pdm_mem_rw_sts cvmx_pko_pdm_mem_rw_sts_t;
+
+/**
+ * cvmx_pko_pdm_mwpbuf_dbg
+ */
+union cvmx_pko_pdm_mwpbuf_dbg {
+	uint64_t u64;
+	struct cvmx_pko_pdm_mwpbuf_dbg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_43_63               : 21;
+	uint64_t sel_nxt_ptr                  : 1;  /**< Sel_nxt_ptr signal. */
+	uint64_t load_val                     : 1;  /**< Load valid signal. */
+	uint64_t rdy                          : 1;  /**< Ready signal. */
+	uint64_t cur_state                    : 3;  /**< Current state from the pbuf controller. */
+	uint64_t reserved_33_36               : 4;
+	uint64_t track_rd_cnt                 : 6;  /**< Track read count value. */
+	uint64_t track_wr_cnt                 : 6;  /**< Track write count value. */
+	uint64_t reserved_17_20               : 4;
+	uint64_t mem_addr                     : 13; /**< Memory address for pbuf ram. */
+	uint64_t mem_en                       : 4;  /**< Memory write/chip enable signals. The order of the bits is:
+                                                         0x3 = Low wen.
+                                                         0x2 = Low cen.
+                                                         0x1 = High wen.
+                                                         0x0 = High cen. */
+#else
+	uint64_t mem_en                       : 4;
+	uint64_t mem_addr                     : 13;
+	uint64_t reserved_17_20               : 4;
+	uint64_t track_wr_cnt                 : 6;
+	uint64_t track_rd_cnt                 : 6;
+	uint64_t reserved_33_36               : 4;
+	uint64_t cur_state                    : 3;
+	uint64_t rdy                          : 1;
+	uint64_t load_val                     : 1;
+	uint64_t sel_nxt_ptr                  : 1;
+	uint64_t reserved_43_63               : 21;
+#endif
+	} s;
+	struct cvmx_pko_pdm_mwpbuf_dbg_s      cn78xx;
+};
+typedef union cvmx_pko_pdm_mwpbuf_dbg cvmx_pko_pdm_mwpbuf_dbg_t;
+
+/**
+ * cvmx_pko_pdm_sts
+ */
+union cvmx_pko_pdm_sts {
+	uint64_t u64;
+	struct cvmx_pko_pdm_sts_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_38_63               : 26;
+	uint64_t cp_stalled_thrshld_hit       : 1;  /**< Reserved. INTERNAL: This register is set to 1 if the PDM stalls the inputs for more than
+                                                         PKO_PDM_CFG_DBG[CP_STALL_THRSHLD]: Do not list field in HRM. For lab debug only; will
+                                                         likely disappear in pass 2. */
+	uint64_t reserved_35_36               : 2;
+	uint64_t mwpbuf_data_val_err          : 1;  /**< Received signal that MWPBUF had data valid error. Throws
+                                                         PKO_INTSN_E::PKO_MWPBUF_DATA_VAL_ERR. */
+	uint64_t drpbuf_data_val_err          : 1;  /**< Received signal that DRPBUF had data valid error. Throws
+                                                         PKO_INTSN_E::PKO_DRPBUF_DATA_VAL_ERR. */
+	uint64_t dwpbuf_data_val_err          : 1;  /**< Received signal that DWPBUF had data valid error. Throws
+                                                         PKO_INTSN_E::PKO_DWPBUF_DATA_VAL_ERR. */
+	uint64_t reserved_30_31               : 2;
+	uint64_t qcmd_iobx_err_sts            : 4;  /**< When PKO_PDM_STS[QCMD_IOBX_ERR] is set, this contains the queue command response's status
+                                                         field for the response causing the error. Note that if multiple errors occur, only the
+                                                         first error status is captured here until PKO_PDM_STS[QCMD_IOBX_ERR] is cleared.
+                                                         Enumerated by PKO_DQSTATUS_E. */
+	uint64_t qcmd_iobx_err                : 1;  /**< Queue command IOBDMA/IOBLD error status occurred in PKO/PDM.
+                                                         PKO_PDM_STS[QCMD_IOBX_ERR_STS] contains the status code. Note that FPA being out of
+                                                         pointers does not set this bit. (See PKO_FPA_NO_PTRS.) Throws
+                                                         PKO_INTSN_E::PKO_QCMD_IOBX_ERR. */
+	uint64_t sendpkt_lmtdma_err_sts       : 4;  /**< This is the status field of the command response on the LMTDMA failure indicated by
+                                                         PKO_PDM_STS[SENDPKT_LMTDMA_ERR] bits being asserted. Note that if multiple errors occur,
+                                                         only the first error status is captured here until PKO_PDM_STS[SENDPKT_LMTDMA_ERR] is
+                                                         cleared. Enumerated by PKO_DQSTATUS_E. */
+	uint64_t sendpkt_lmtdma_err           : 1;  /**< Send-packet of type LMTDMA error status occurred in PKO/PDM.
+                                                         PKO_PDM_STS[SENDPKT_LMTDMA_ERR_STS] contains the status code. Note that FPA being out of
+                                                         pointers does not set this bit. (See PKO_FPA_NO_PTRS.) Throws
+                                                         PKO_INTSN_E::PKO_SENDPKT_LMTDMA_ERR. */
+	uint64_t sendpkt_lmtst_err_sts        : 4;  /**< This is the status field of the command response on the LMTST failure indicated by
+                                                         PKO_PDM_STS[SENDPKT_LMTST_ERR] bits being asserted. Note that if multiple errors occur
+                                                         only the first error status will be captured here until PKO_PDM_STS[SENDPKT_LMTST_ERR] is
+                                                         cleared. Enumerated by PKO_DQSTATUS_E. */
+	uint64_t sendpkt_lmtst_err            : 1;  /**< Send-packet of type LMTST error status occurred in PKO/PDM.
+                                                         PKO_PDM_STS[SENDPKT_LMTST_ERR_STS] contains the status code. Note that FPA being out of
+                                                         pointers does not set this bit. (See PKO_FPA_NO_PTRS.) Throws
+                                                         PKO_INTSN_E::PKO_SENDPKT_LMTST_ERR. */
+	uint64_t fpa_no_ptrs                  : 1;  /**< FPA signaled PKO that FPA can not allocate pointers. This is a fatal error. Throws
+                                                         PKO_INTSN_E::PKO_FPA_NO_PTRS. */
+	uint64_t reserved_12_13               : 2;
+	uint64_t cp_sendpkt_err_no_drp_code   : 2;  /**< This field stores the error code for illegally constructed send-packets that did not drop.
+                                                         Note that if multiple errors occur, only the first error code is captured here until
+                                                         PKO_PDM_STS[CP_SENDPKT_ERR_NO_DRP] is cleared. Codes: 0x0 = NO ERROR CODE. 0x1 = SEND_JUMP
+                                                         not at end of descriptor. */
+	uint64_t cp_sendpkt_err_no_drp        : 1;  /**< PKO/PDM/CP did not drop a send-packet; however, the SEND_JUMP command is not at end of the
+                                                         descriptor. The error code is captured in PKO_PDM_STS[CP_SENDPKT_ERR_NO_DRP_CODE]. Throws
+                                                         PKO_INTSN_E::PKO_CP_SENDPKT_ERR_NO_DRP. */
+	uint64_t reserved_7_8                 : 2;
+	uint64_t cp_sendpkt_err_drop_code     : 3;  /**< This field stores the error code for illegally constructed send-packet drops. Note that if
+                                                         multiple errors occur, only the first error code is captured here until
+                                                         PKO_PDM_STS[CP_SENDPKT_ERR_DROP] is cleared. PKO_CPSENDDROP_E enumerates the codes and
+                                                         conditions. */
+	uint64_t cp_sendpkt_err_drop          : 1;  /**< Dropped a send-packet in PDM/CP due to a rule violation. The error code is captured in
+                                                         PKO_PDM_STS[CP_SENDPKT_ERR_DROP_CODE]. Throws PKO_INTSN_E::PKO_CP_SENDPKT_ERR_DROP. */
+	uint64_t reserved_1_2                 : 2;
+	uint64_t desc_crc_err                 : 1;  /**< CRC error occurred in a descriptor. (State may have been corrupted.) Throws
+                                                         PKO_INTSN_E::PKO_DESC_CRC_ERR. INTERNAL: Note that this is a pass 2 feature. */
+#else
+	uint64_t desc_crc_err                 : 1;
+	uint64_t reserved_1_2                 : 2;
+	uint64_t cp_sendpkt_err_drop          : 1;
+	uint64_t cp_sendpkt_err_drop_code     : 3;
+	uint64_t reserved_7_8                 : 2;
+	uint64_t cp_sendpkt_err_no_drp        : 1;
+	uint64_t cp_sendpkt_err_no_drp_code   : 2;
+	uint64_t reserved_12_13               : 2;
+	uint64_t fpa_no_ptrs                  : 1;
+	uint64_t sendpkt_lmtst_err            : 1;
+	uint64_t sendpkt_lmtst_err_sts        : 4;
+	uint64_t sendpkt_lmtdma_err           : 1;
+	uint64_t sendpkt_lmtdma_err_sts       : 4;
+	uint64_t qcmd_iobx_err                : 1;
+	uint64_t qcmd_iobx_err_sts            : 4;
+	uint64_t reserved_30_31               : 2;
+	uint64_t dwpbuf_data_val_err          : 1;
+	uint64_t drpbuf_data_val_err          : 1;
+	uint64_t mwpbuf_data_val_err          : 1;
+	uint64_t reserved_35_36               : 2;
+	uint64_t cp_stalled_thrshld_hit       : 1;
+	uint64_t reserved_38_63               : 26;
+#endif
+	} s;
+	struct cvmx_pko_pdm_sts_s             cn78xx;
+};
+typedef union cvmx_pko_pdm_sts cvmx_pko_pdm_sts_t;
+
+/**
+ * cvmx_pko_peb_bist_status
+ *
+ * Each bit is the BIST result of an individual memory (per bit, 0 = pass and 1 = fail).
+ *
+ */
+union cvmx_pko_peb_bist_status {
+	uint64_t u64;
+	struct cvmx_pko_peb_bist_status_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_26_63               : 38;
+	uint64_t add_work_fifo                : 1;  /**< ADD_WORK_FIFO RAM BIST status. */
+	uint64_t pdm_pse_buf_ram              : 1;  /**< PDM_PSE_BUF RAM BIST status. */
+	uint64_t iobp0_fifo_ram               : 1;  /**< IOBP0_FIFO RAM BIST status. */
+	uint64_t iobp1_fifo_ram               : 1;  /**< IOBP1_FIFO RAM BIST status. */
+	uint64_t state_mem0                   : 1;  /**< STATE_MEM0 RAM BIST status. */
+	uint64_t state_mem1                   : 1;  /**< STATE_MEM1 RAM BIST status. */
+	uint64_t state_mem2                   : 1;  /**< STATE_MEM2 RAM BIST status. */
+	uint64_t state_mem3                   : 1;  /**< STATE_MEM3 RAM BIST status. */
+	uint64_t iobp1_uid_fifo_ram           : 1;  /**< IOBP1_UID_FIFO RAM BIST status. */
+	uint64_t nxt_link_ptr_ram             : 1;  /**< NXT_LINK_PTR RAM BIST status. */
+	uint64_t pd_bank0_ram                 : 1;  /**< PD_BANK0 RAM BIST status. */
+	uint64_t pd_bank1_ram                 : 1;  /**< PD_BANK1 RAM BIST status. */
+	uint64_t pd_bank2_ram                 : 1;  /**< PD_BANK2 RAM BIST status. */
+	uint64_t pd_bank3_ram                 : 1;  /**< PD_BANK3 RAM BIST status. */
+	uint64_t pd_var_bank_ram              : 1;  /**< PD_VAR_BANK RAM BIST status. */
+	uint64_t pdm_resp_buf_ram             : 1;  /**< PDM_RESP_BUF RAM BIST status. */
+	uint64_t tx_fifo_pkt_ram              : 1;  /**< TX_FIFO_PKT RAM BIST status. */
+	uint64_t tx_fifo_hdr_ram              : 1;  /**< TX_FIFO_HDR RAM BIST status. */
+	uint64_t tx_fifo_crc_ram              : 1;  /**< TX_FIFO_CRC RAM BIST status. */
+	uint64_t ts_addwork_ram               : 1;  /**< TS_ADDWORK RAM BIST status. */
+	uint64_t send_mem_ts_fifo             : 1;  /**< SEND_MEM_TS_FIFO RAM BIST status. */
+	uint64_t send_mem_stdn_fifo           : 1;  /**< SEND_MEM_STDN_FIFO RAM BIST status. */
+	uint64_t send_mem_fifo                : 1;  /**< SEND_MEM_FIFO RAM BIST status. */
+	uint64_t pkt_mrk_ram                  : 1;  /**< PKT_MRK RAM BIST status. */
+	uint64_t peb_st_inf_ram               : 1;  /**< PEB_ST_INF RAM BIST status. */
+	uint64_t reserved_0_0                 : 1;
+#else
+	uint64_t reserved_0_0                 : 1;
+	uint64_t peb_st_inf_ram               : 1;
+	uint64_t pkt_mrk_ram                  : 1;
+	uint64_t send_mem_fifo                : 1;
+	uint64_t send_mem_stdn_fifo           : 1;
+	uint64_t send_mem_ts_fifo             : 1;
+	uint64_t ts_addwork_ram               : 1;
+	uint64_t tx_fifo_crc_ram              : 1;
+	uint64_t tx_fifo_hdr_ram              : 1;
+	uint64_t tx_fifo_pkt_ram              : 1;
+	uint64_t pdm_resp_buf_ram             : 1;
+	uint64_t pd_var_bank_ram              : 1;
+	uint64_t pd_bank3_ram                 : 1;
+	uint64_t pd_bank2_ram                 : 1;
+	uint64_t pd_bank1_ram                 : 1;
+	uint64_t pd_bank0_ram                 : 1;
+	uint64_t nxt_link_ptr_ram             : 1;
+	uint64_t iobp1_uid_fifo_ram           : 1;
+	uint64_t state_mem3                   : 1;
+	uint64_t state_mem2                   : 1;
+	uint64_t state_mem1                   : 1;
+	uint64_t state_mem0                   : 1;
+	uint64_t iobp1_fifo_ram               : 1;
+	uint64_t iobp0_fifo_ram               : 1;
+	uint64_t pdm_pse_buf_ram              : 1;
+	uint64_t add_work_fifo                : 1;
+	uint64_t reserved_26_63               : 38;
+#endif
+	} s;
+	struct cvmx_pko_peb_bist_status_s     cn78xx;
+};
+typedef union cvmx_pko_peb_bist_status cvmx_pko_peb_bist_status_t;
+
+/**
+ * cvmx_pko_peb_ecc_ctl0
+ */
+union cvmx_pko_peb_ecc_ctl0 {
+	uint64_t u64;
+	struct cvmx_pko_peb_ecc_ctl0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t iobp1_uid_fifo_ram_flip      : 2;  /**< IOBP1_UID_FIFO_RAM flip syndrome bits on write. */
+	uint64_t iobp1_uid_fifo_ram_cdis      : 1;  /**< IOBP1_UID_FIFO_RAM ECC correction disable. */
+	uint64_t iobp0_fifo_ram_flip          : 2;  /**< IOBP0_FIFO_RAM flip syndrome bits on write. */
+	uint64_t iobp0_fifo_ram_cdis          : 1;  /**< IOBP0_FIFO_RAM ECC correction disable. */
+	uint64_t iobp1_fifo_ram_flip          : 2;  /**< IOBP1_FIFO_RAM flip syndrome bits on write. */
+	uint64_t iobp1_fifo_ram_cdis          : 1;  /**< IOBP1_FIFO_RAM ECC correction disable. */
+	uint64_t pdm_resp_buf_ram_flip        : 2;  /**< PDM_RESP_BUF_RAM flip syndrome bits on write. */
+	uint64_t pdm_resp_buf_ram_cdis        : 1;  /**< PDM_RESP_BUF_RAM ECC correction disable. */
+	uint64_t pdm_pse_buf_ram_flip         : 2;  /**< PDM_PSE_BUF_RAM flip syndrome bits on write. */
+	uint64_t pdm_pse_buf_ram_cdis         : 1;  /**< PDM_PSE_BUF_RAM ECC correction disable. */
+	uint64_t reserved_46_48               : 3;
+	uint64_t peb_st_inf_ram_flip          : 2;  /**< PEB_ST_INF_RAM flip syndrome bits on write. */
+	uint64_t peb_st_inf_ram_cdis          : 1;  /**< PEB_ST_INF_RAM ECC correction disable. */
+	uint64_t pd_bank3_ram_flip            : 2;  /**< PD_BANK3_RAM flip syndrome bits on write. */
+	uint64_t pd_bank3_ram_cdis            : 1;  /**< PD_BANK3_RAM ECC correction disable. */
+	uint64_t pd_bank2_ram_flip            : 2;  /**< PD_BANK2_RAM flip syndrome bits on write. */
+	uint64_t pd_bank2_ram_cdis            : 1;  /**< PD_BANK2_RAM ECC correction disable. */
+	uint64_t pd_bank1_ram_flip            : 2;  /**< PD_BANK1_RAM flip syndrome bits on write. */
+	uint64_t pd_bank1_ram_cdis            : 1;  /**< PD_BANK1_RAM ECC correction disable. */
+	uint64_t pd_bank0_ram_flip            : 2;  /**< PD_BANK0_RAM flip syndrome bits on write. */
+	uint64_t pd_bank0_ram_cdis            : 1;  /**< PD_BANK0_RAM ECC correction disable. */
+	uint64_t pd_var_bank_ram_flip         : 2;  /**< PD_VAR_BANK_RAM flip syndrome bits on write. */
+	uint64_t pd_var_bank_ram_cdis         : 1;  /**< PD_VAR_BANK_RAM ECC correction disable. */
+	uint64_t tx_fifo_crc_ram_flip         : 2;  /**< TX_FIFO_CRC_RAM flip syndrome bits on write. */
+	uint64_t tx_fifo_crc_ram_cdis         : 1;  /**< TX_FIFO_CRC_RAM ECC correction disable. */
+	uint64_t tx_fifo_hdr_ram_flip         : 2;  /**< TX_FIFO_HDR_RAM flip syndrome bits on write. */
+	uint64_t tx_fifo_hdr_ram_cdis         : 1;  /**< TX_FIFO_HDR_RAM ECC correction disable. */
+	uint64_t tx_fifo_pkt_ram_flip         : 2;  /**< TX_FIFO_PKT_RAM flip syndrome bits on write. */
+	uint64_t tx_fifo_pkt_ram_cdis         : 1;  /**< TX_FIFO_PKT_RAM ECC correction disable. */
+	uint64_t add_work_fifo_flip           : 2;  /**< ADD_WORK_FIFO flip syndrome bits on write. */
+	uint64_t add_work_fifo_cdis           : 1;  /**< ADD_WORK_FIFO ECC correction disable. */
+	uint64_t send_mem_fifo_flip           : 2;  /**< SEND_MEM_FIFO flip syndrome bits on write. */
+	uint64_t send_mem_fifo_cdis           : 1;  /**< SEND_MEM_FIFO ECC correction disable. */
+	uint64_t send_mem_stdn_fifo_flip      : 2;  /**< SEND_MEM_STDN_FIFO flip syndrome bits on write. */
+	uint64_t send_mem_stdn_fifo_cdis      : 1;  /**< SEND_MEM_STDN_FIFO ECC correction disable. */
+	uint64_t send_mem_ts_fifo_flip        : 2;  /**< SEND_MEM_TS_FIFO flip syndrome bits on write. */
+	uint64_t send_mem_ts_fifo_cdis        : 1;  /**< SEND_MEM_TS_FIFO ECC correction disable. */
+	uint64_t nxt_link_ptr_ram_flip        : 2;  /**< NXT_LINK_PTR_RAM flip syndrome bits on write. */
+	uint64_t nxt_link_ptr_ram_cdis        : 1;  /**< NXT_LINK_PTR_RAM ECC correction disable. */
+	uint64_t pkt_mrk_ram_flip             : 2;  /**< PKT_MRK_RAM flip syndrome bits on write. */
+	uint64_t pkt_mrk_ram_cdis             : 1;  /**< PKT_MRK_RAM ECC correction disable. */
+	uint64_t reserved_0_0                 : 1;
+#else
+	uint64_t reserved_0_0                 : 1;
+	uint64_t pkt_mrk_ram_cdis             : 1;
+	uint64_t pkt_mrk_ram_flip             : 2;
+	uint64_t nxt_link_ptr_ram_cdis        : 1;
+	uint64_t nxt_link_ptr_ram_flip        : 2;
+	uint64_t send_mem_ts_fifo_cdis        : 1;
+	uint64_t send_mem_ts_fifo_flip        : 2;
+	uint64_t send_mem_stdn_fifo_cdis      : 1;
+	uint64_t send_mem_stdn_fifo_flip      : 2;
+	uint64_t send_mem_fifo_cdis           : 1;
+	uint64_t send_mem_fifo_flip           : 2;
+	uint64_t add_work_fifo_cdis           : 1;
+	uint64_t add_work_fifo_flip           : 2;
+	uint64_t tx_fifo_pkt_ram_cdis         : 1;
+	uint64_t tx_fifo_pkt_ram_flip         : 2;
+	uint64_t tx_fifo_hdr_ram_cdis         : 1;
+	uint64_t tx_fifo_hdr_ram_flip         : 2;
+	uint64_t tx_fifo_crc_ram_cdis         : 1;
+	uint64_t tx_fifo_crc_ram_flip         : 2;
+	uint64_t pd_var_bank_ram_cdis         : 1;
+	uint64_t pd_var_bank_ram_flip         : 2;
+	uint64_t pd_bank0_ram_cdis            : 1;
+	uint64_t pd_bank0_ram_flip            : 2;
+	uint64_t pd_bank1_ram_cdis            : 1;
+	uint64_t pd_bank1_ram_flip            : 2;
+	uint64_t pd_bank2_ram_cdis            : 1;
+	uint64_t pd_bank2_ram_flip            : 2;
+	uint64_t pd_bank3_ram_cdis            : 1;
+	uint64_t pd_bank3_ram_flip            : 2;
+	uint64_t peb_st_inf_ram_cdis          : 1;
+	uint64_t peb_st_inf_ram_flip          : 2;
+	uint64_t reserved_46_48               : 3;
+	uint64_t pdm_pse_buf_ram_cdis         : 1;
+	uint64_t pdm_pse_buf_ram_flip         : 2;
+	uint64_t pdm_resp_buf_ram_cdis        : 1;
+	uint64_t pdm_resp_buf_ram_flip        : 2;
+	uint64_t iobp1_fifo_ram_cdis          : 1;
+	uint64_t iobp1_fifo_ram_flip          : 2;
+	uint64_t iobp0_fifo_ram_cdis          : 1;
+	uint64_t iobp0_fifo_ram_flip          : 2;
+	uint64_t iobp1_uid_fifo_ram_cdis      : 1;
+	uint64_t iobp1_uid_fifo_ram_flip      : 2;
+#endif
+	} s;
+	struct cvmx_pko_peb_ecc_ctl0_s        cn78xx;
+};
+typedef union cvmx_pko_peb_ecc_ctl0 cvmx_pko_peb_ecc_ctl0_t;
 
-#ifndef __CVMX_PKO_DEFS_H__
-#define __CVMX_PKO_DEFS_H__
+/**
+ * cvmx_pko_peb_ecc_ctl1
+ */
+union cvmx_pko_peb_ecc_ctl1 {
+	uint64_t u64;
+	struct cvmx_pko_peb_ecc_ctl1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t ts_addwork_ram_flip          : 2;  /**< TS_ADDWORK_RAM flip syndrome bits on write. */
+	uint64_t ts_addwork_ram_cdis          : 1;  /**< TS_ADDWORK_RAM ECC correction disable. */
+	uint64_t reserved_0_60                : 61;
+#else
+	uint64_t reserved_0_60                : 61;
+	uint64_t ts_addwork_ram_cdis          : 1;
+	uint64_t ts_addwork_ram_flip          : 2;
+#endif
+	} s;
+	struct cvmx_pko_peb_ecc_ctl1_s        cn78xx;
+};
+typedef union cvmx_pko_peb_ecc_ctl1 cvmx_pko_peb_ecc_ctl1_t;
 
-#define CVMX_PKO_MEM_COUNT0 (CVMX_ADD_IO_SEG(0x0001180050001080ull))
-#define CVMX_PKO_MEM_COUNT1 (CVMX_ADD_IO_SEG(0x0001180050001088ull))
-#define CVMX_PKO_MEM_DEBUG0 (CVMX_ADD_IO_SEG(0x0001180050001100ull))
-#define CVMX_PKO_MEM_DEBUG1 (CVMX_ADD_IO_SEG(0x0001180050001108ull))
-#define CVMX_PKO_MEM_DEBUG10 (CVMX_ADD_IO_SEG(0x0001180050001150ull))
-#define CVMX_PKO_MEM_DEBUG11 (CVMX_ADD_IO_SEG(0x0001180050001158ull))
-#define CVMX_PKO_MEM_DEBUG12 (CVMX_ADD_IO_SEG(0x0001180050001160ull))
-#define CVMX_PKO_MEM_DEBUG13 (CVMX_ADD_IO_SEG(0x0001180050001168ull))
-#define CVMX_PKO_MEM_DEBUG14 (CVMX_ADD_IO_SEG(0x0001180050001170ull))
-#define CVMX_PKO_MEM_DEBUG2 (CVMX_ADD_IO_SEG(0x0001180050001110ull))
-#define CVMX_PKO_MEM_DEBUG3 (CVMX_ADD_IO_SEG(0x0001180050001118ull))
-#define CVMX_PKO_MEM_DEBUG4 (CVMX_ADD_IO_SEG(0x0001180050001120ull))
-#define CVMX_PKO_MEM_DEBUG5 (CVMX_ADD_IO_SEG(0x0001180050001128ull))
-#define CVMX_PKO_MEM_DEBUG6 (CVMX_ADD_IO_SEG(0x0001180050001130ull))
-#define CVMX_PKO_MEM_DEBUG7 (CVMX_ADD_IO_SEG(0x0001180050001138ull))
-#define CVMX_PKO_MEM_DEBUG8 (CVMX_ADD_IO_SEG(0x0001180050001140ull))
-#define CVMX_PKO_MEM_DEBUG9 (CVMX_ADD_IO_SEG(0x0001180050001148ull))
-#define CVMX_PKO_MEM_IPORT_PTRS (CVMX_ADD_IO_SEG(0x0001180050001030ull))
-#define CVMX_PKO_MEM_IPORT_QOS (CVMX_ADD_IO_SEG(0x0001180050001038ull))
-#define CVMX_PKO_MEM_IQUEUE_PTRS (CVMX_ADD_IO_SEG(0x0001180050001040ull))
-#define CVMX_PKO_MEM_IQUEUE_QOS (CVMX_ADD_IO_SEG(0x0001180050001048ull))
-#define CVMX_PKO_MEM_PORT_PTRS (CVMX_ADD_IO_SEG(0x0001180050001010ull))
-#define CVMX_PKO_MEM_PORT_QOS (CVMX_ADD_IO_SEG(0x0001180050001018ull))
-#define CVMX_PKO_MEM_PORT_RATE0 (CVMX_ADD_IO_SEG(0x0001180050001020ull))
-#define CVMX_PKO_MEM_PORT_RATE1 (CVMX_ADD_IO_SEG(0x0001180050001028ull))
-#define CVMX_PKO_MEM_QUEUE_PTRS (CVMX_ADD_IO_SEG(0x0001180050001000ull))
-#define CVMX_PKO_MEM_QUEUE_QOS (CVMX_ADD_IO_SEG(0x0001180050001008ull))
-#define CVMX_PKO_MEM_THROTTLE_INT (CVMX_ADD_IO_SEG(0x0001180050001058ull))
-#define CVMX_PKO_MEM_THROTTLE_PIPE (CVMX_ADD_IO_SEG(0x0001180050001050ull))
-#define CVMX_PKO_REG_BIST_RESULT (CVMX_ADD_IO_SEG(0x0001180050000080ull))
-#define CVMX_PKO_REG_CMD_BUF (CVMX_ADD_IO_SEG(0x0001180050000010ull))
-#define CVMX_PKO_REG_CRC_CTLX(offset) (CVMX_ADD_IO_SEG(0x0001180050000028ull) + ((offset) & 1) * 8)
-#define CVMX_PKO_REG_CRC_ENABLE (CVMX_ADD_IO_SEG(0x0001180050000020ull))
-#define CVMX_PKO_REG_CRC_IVX(offset) (CVMX_ADD_IO_SEG(0x0001180050000038ull) + ((offset) & 1) * 8)
-#define CVMX_PKO_REG_DEBUG0 (CVMX_ADD_IO_SEG(0x0001180050000098ull))
-#define CVMX_PKO_REG_DEBUG1 (CVMX_ADD_IO_SEG(0x00011800500000A0ull))
-#define CVMX_PKO_REG_DEBUG2 (CVMX_ADD_IO_SEG(0x00011800500000A8ull))
-#define CVMX_PKO_REG_DEBUG3 (CVMX_ADD_IO_SEG(0x00011800500000B0ull))
-#define CVMX_PKO_REG_DEBUG4 (CVMX_ADD_IO_SEG(0x00011800500000B8ull))
-#define CVMX_PKO_REG_ENGINE_INFLIGHT (CVMX_ADD_IO_SEG(0x0001180050000050ull))
-#define CVMX_PKO_REG_ENGINE_INFLIGHT1 (CVMX_ADD_IO_SEG(0x0001180050000318ull))
-#define CVMX_PKO_REG_ENGINE_STORAGEX(offset) (CVMX_ADD_IO_SEG(0x0001180050000300ull) + ((offset) & 1) * 8)
-#define CVMX_PKO_REG_ENGINE_THRESH (CVMX_ADD_IO_SEG(0x0001180050000058ull))
-#define CVMX_PKO_REG_ERROR (CVMX_ADD_IO_SEG(0x0001180050000088ull))
-#define CVMX_PKO_REG_FLAGS (CVMX_ADD_IO_SEG(0x0001180050000000ull))
-#define CVMX_PKO_REG_GMX_PORT_MODE (CVMX_ADD_IO_SEG(0x0001180050000018ull))
-#define CVMX_PKO_REG_INT_MASK (CVMX_ADD_IO_SEG(0x0001180050000090ull))
-#define CVMX_PKO_REG_LOOPBACK_BPID (CVMX_ADD_IO_SEG(0x0001180050000118ull))
-#define CVMX_PKO_REG_LOOPBACK_PKIND (CVMX_ADD_IO_SEG(0x0001180050000068ull))
-#define CVMX_PKO_REG_MIN_PKT (CVMX_ADD_IO_SEG(0x0001180050000070ull))
-#define CVMX_PKO_REG_PREEMPT (CVMX_ADD_IO_SEG(0x0001180050000110ull))
-#define CVMX_PKO_REG_QUEUE_MODE (CVMX_ADD_IO_SEG(0x0001180050000048ull))
-#define CVMX_PKO_REG_QUEUE_PREEMPT (CVMX_ADD_IO_SEG(0x0001180050000108ull))
-#define CVMX_PKO_REG_QUEUE_PTRS1 (CVMX_ADD_IO_SEG(0x0001180050000100ull))
-#define CVMX_PKO_REG_READ_IDX (CVMX_ADD_IO_SEG(0x0001180050000008ull))
-#define CVMX_PKO_REG_THROTTLE (CVMX_ADD_IO_SEG(0x0001180050000078ull))
-#define CVMX_PKO_REG_TIMESTAMP (CVMX_ADD_IO_SEG(0x0001180050000060ull))
+/**
+ * cvmx_pko_peb_ecc_dbe_sts0
+ */
+union cvmx_pko_peb_ecc_dbe_sts0 {
+	uint64_t u64;
+	struct cvmx_pko_peb_ecc_dbe_sts0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t iobp1_uid_fifo_ram_dbe       : 1;  /**< Double-bit error for IOBP1_UID_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_proc_i.pko_peb_iobp1_uid_fifo_i */
+	uint64_t iobp0_fifo_ram_dbe           : 1;  /**< Double-bit error for IOBP0_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_iobp0_fifo_i */
+	uint64_t iobp1_fifo_ram_dbe           : 1;  /**< Double-bit error for IOBP1_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_proc_i.pko_peb_iobp1_fifo_i */
+	uint64_t pdm_resp_buf_ram_dbe         : 1;  /**< Double-bit error for PDM_RESP_BUF_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_pdm_intf_i.pko_peb_pdm_resp_buf_i */
+	uint64_t pdm_pse_buf_ram_dbe          : 1;  /**< Double-bit error for PDM_PSE_BUF_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_pdm_intf_i.pko_peb_pse_buf_i */
+	uint64_t reserved_58_58               : 1;
+	uint64_t peb_st_inf_ram_dbe           : 1;  /**< Double-bit error for PEB_ST_INF_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_state_info_mem_i */
+	uint64_t pd_bank3_ram_dbe             : 1;  /**< Double-bit error for PD_BANK3_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_pd_mem_bank3_i */
+	uint64_t pd_bank2_ram_dbe             : 1;  /**< Double-bit error for PD_BANK2_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_pd_mem_bank2_i */
+	uint64_t pd_bank1_ram_dbe             : 1;  /**< Double-bit error for PD_BANK1_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_pd_mem_bank1_i */
+	uint64_t pd_bank0_ram_dbe             : 1;  /**< Double-bit error for PD_BANK0_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_pd_mem_bank0_i */
+	uint64_t pd_var_bank_ram_dbe          : 1;  /**< Double-bit error for PD_VAR_BANK_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_pd_var_mem_bank_i */
+	uint64_t tx_fifo_crc_ram_dbe          : 1;  /**< Double-bit error for TX_FIFO_CRC_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_tx_fifo_i.pko_peb_tx_fifo_crc_i */
+	uint64_t tx_fifo_hdr_ram_dbe          : 1;  /**< Double-bit error for TX_FIFO_HDR_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_tx_fifo_i.pko_peb_tx_fifo_hdr_i */
+	uint64_t tx_fifo_pkt_ram_dbe          : 1;  /**< Double-bit error for TX_FIFO_PKT_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_tx_fifo_i.pko_peb_tx_fifo_pkt_i */
+	uint64_t add_work_fifo_dbe            : 1;  /**< Double-bit error for ADD_WORK_FIFO. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_proc_i.pko_peb_addwork_proc_i.pko_peb_add_work_fifo_i */
+	uint64_t send_mem_fifo_dbe            : 1;  /**< Double-bit error for SEND_MEM_FIFO. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_proc_i.pko_peb_sendmem_proc_i.pko_peb_send_mem_fifo_i */
+	uint64_t send_mem_stdn_fifo_dbe       : 1;  /**< Double-bit error for SEND_MEM_STDN_FIFO. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_proc_i.pko_peb_sendmem_proc_i.pko_peb_send_mem_stdn_fifo_i */
+	uint64_t send_mem_ts_fifo_dbe         : 1;  /**< Double-bit error for SEND_MEM_TS_FIFO. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_proc_i.pko_peb_sendmem_proc_i.pko_peb_send_mem_ts_fifo_i */
+	uint64_t nxt_link_ptr_ram_dbe         : 1;  /**< Double-bit error for NXT_LINK_PTR_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_nxt_link_ptr_mem_i */
+	uint64_t pkt_mrk_ram_dbe              : 1;  /**< Double-bit error for PKT_MRK_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_pkt_mrk_mem_i */
+	uint64_t ts_addwork_ram_dbe           : 1;  /**< Double-bit error for TS_ADDWORK_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_proc_i.pko_peb_addwork_proc_i.pko_peb_ts_addwork_mem_i */
+	uint64_t reserved_0_41                : 42;
+#else
+	uint64_t reserved_0_41                : 42;
+	uint64_t ts_addwork_ram_dbe           : 1;
+	uint64_t pkt_mrk_ram_dbe              : 1;
+	uint64_t nxt_link_ptr_ram_dbe         : 1;
+	uint64_t send_mem_ts_fifo_dbe         : 1;
+	uint64_t send_mem_stdn_fifo_dbe       : 1;
+	uint64_t send_mem_fifo_dbe            : 1;
+	uint64_t add_work_fifo_dbe            : 1;
+	uint64_t tx_fifo_pkt_ram_dbe          : 1;
+	uint64_t tx_fifo_hdr_ram_dbe          : 1;
+	uint64_t tx_fifo_crc_ram_dbe          : 1;
+	uint64_t pd_var_bank_ram_dbe          : 1;
+	uint64_t pd_bank0_ram_dbe             : 1;
+	uint64_t pd_bank1_ram_dbe             : 1;
+	uint64_t pd_bank2_ram_dbe             : 1;
+	uint64_t pd_bank3_ram_dbe             : 1;
+	uint64_t peb_st_inf_ram_dbe           : 1;
+	uint64_t reserved_58_58               : 1;
+	uint64_t pdm_pse_buf_ram_dbe          : 1;
+	uint64_t pdm_resp_buf_ram_dbe         : 1;
+	uint64_t iobp1_fifo_ram_dbe           : 1;
+	uint64_t iobp0_fifo_ram_dbe           : 1;
+	uint64_t iobp1_uid_fifo_ram_dbe       : 1;
+#endif
+	} s;
+	struct cvmx_pko_peb_ecc_dbe_sts0_s    cn78xx;
+};
+typedef union cvmx_pko_peb_ecc_dbe_sts0 cvmx_pko_peb_ecc_dbe_sts0_t;
 
-union cvmx_pko_mem_count0 {
+/**
+ * cvmx_pko_peb_ecc_dbe_sts_cmb0
+ */
+union cvmx_pko_peb_ecc_dbe_sts_cmb0 {
 	uint64_t u64;
-	struct cvmx_pko_mem_count0_s {
+	struct cvmx_pko_peb_ecc_dbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_32_63:32;
-		uint64_t count:32;
-#else
-		uint64_t count:32;
-		uint64_t reserved_32_63:32;
-#endif
-	} s;
-	struct cvmx_pko_mem_count0_s cn30xx;
-	struct cvmx_pko_mem_count0_s cn31xx;
-	struct cvmx_pko_mem_count0_s cn38xx;
-	struct cvmx_pko_mem_count0_s cn38xxp2;
-	struct cvmx_pko_mem_count0_s cn50xx;
-	struct cvmx_pko_mem_count0_s cn52xx;
-	struct cvmx_pko_mem_count0_s cn52xxp1;
-	struct cvmx_pko_mem_count0_s cn56xx;
-	struct cvmx_pko_mem_count0_s cn56xxp1;
-	struct cvmx_pko_mem_count0_s cn58xx;
-	struct cvmx_pko_mem_count0_s cn58xxp1;
-	struct cvmx_pko_mem_count0_s cn61xx;
-	struct cvmx_pko_mem_count0_s cn63xx;
-	struct cvmx_pko_mem_count0_s cn63xxp1;
-	struct cvmx_pko_mem_count0_s cn66xx;
-	struct cvmx_pko_mem_count0_s cn68xx;
-	struct cvmx_pko_mem_count0_s cn68xxp1;
-	struct cvmx_pko_mem_count0_s cnf71xx;
+	uint64_t peb_dbe_cmb0                 : 1;  /**< This bit is the logical OR of all bits in PKO_PEB_ECC_DBE_STS0. To clear this bit,
+                                                         software
+                                                         must clear bits in PKO_PEB_ECC_DBE_STS0. When this bit is set, the corresponding interrupt
+                                                         is set. Throws PKO_INTSN_E::PKO_PEB_DBE_CMB0. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_proc_i.pko_peb_iobp1_uid_fifo_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_iobp0_fifo_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_proc_i.pko_peb_iobp1_fifo_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_pdm_intf_i.pko_peb_pdm_resp_buf_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_pdm_intf_i.pko_peb_pse_buf_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_state_info_mem_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_pd_mem_bank3_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_pd_mem_bank0_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_pd_mem_bank1_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_pd_mem_bank2_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_pd_var_mem_bank_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_tx_fifo_i.pko_peb_tx_fifo_crc_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_tx_fifo_i.pko_peb_tx_fifo_hdr_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_tx_fifo_i.pko_peb_tx_fifo_pkt_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_proc_i.pko_peb_addwork_proc_i.pko_peb_add_work_fifo_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_proc_i.pko_peb_sendmem_proc_i.pko_peb_send_mem_fifo_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_proc_i.pko_peb_sendmem_proc_i.pko_peb_send_mem_stdn_fifo_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_proc_i.pko_peb_sendmem_proc_i.pko_peb_send_mem_ts_fifo_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_nxt_link_ptr_mem_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_pkt_mrk_mem_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_proc_i.pko_peb_addwork_proc_i.pko_peb_ts_addwork_mem_i */
+	uint64_t reserved_0_62                : 63;
+#else
+	uint64_t reserved_0_62                : 63;
+	uint64_t peb_dbe_cmb0                 : 1;
+#endif
+	} s;
+	struct cvmx_pko_peb_ecc_dbe_sts_cmb0_s cn78xx;
 };
+typedef union cvmx_pko_peb_ecc_dbe_sts_cmb0 cvmx_pko_peb_ecc_dbe_sts_cmb0_t;
 
-union cvmx_pko_mem_count1 {
+/**
+ * cvmx_pko_peb_ecc_sbe_sts0
+ */
+union cvmx_pko_peb_ecc_sbe_sts0 {
 	uint64_t u64;
-	struct cvmx_pko_mem_count1_s {
+	struct cvmx_pko_peb_ecc_sbe_sts0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_48_63:16;
-		uint64_t count:48;
-#else
-		uint64_t count:48;
-		uint64_t reserved_48_63:16;
-#endif
-	} s;
-	struct cvmx_pko_mem_count1_s cn30xx;
-	struct cvmx_pko_mem_count1_s cn31xx;
-	struct cvmx_pko_mem_count1_s cn38xx;
-	struct cvmx_pko_mem_count1_s cn38xxp2;
-	struct cvmx_pko_mem_count1_s cn50xx;
-	struct cvmx_pko_mem_count1_s cn52xx;
-	struct cvmx_pko_mem_count1_s cn52xxp1;
-	struct cvmx_pko_mem_count1_s cn56xx;
-	struct cvmx_pko_mem_count1_s cn56xxp1;
-	struct cvmx_pko_mem_count1_s cn58xx;
-	struct cvmx_pko_mem_count1_s cn58xxp1;
-	struct cvmx_pko_mem_count1_s cn61xx;
-	struct cvmx_pko_mem_count1_s cn63xx;
-	struct cvmx_pko_mem_count1_s cn63xxp1;
-	struct cvmx_pko_mem_count1_s cn66xx;
-	struct cvmx_pko_mem_count1_s cn68xx;
-	struct cvmx_pko_mem_count1_s cn68xxp1;
-	struct cvmx_pko_mem_count1_s cnf71xx;
+	uint64_t iobp1_uid_fifo_ram_sbe       : 1;  /**< Single-bit error for IOBP1_UID_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_proc_i.pko_peb_iobp1_uid_fifo_i */
+	uint64_t iobp0_fifo_ram_sbe           : 1;  /**< Single-bit error for IOBP0_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_iobp0_fifo_i */
+	uint64_t iobp1_fifo_ram_sbe           : 1;  /**< Single-bit error for IOBP1_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_proc_i.pko_peb_iobp1_fifo_i */
+	uint64_t pdm_resp_buf_ram_sbe         : 1;  /**< Single-bit error for PDM_RESP_BUF_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_pdm_intf_i.pko_peb_pdm_resp_buf_i */
+	uint64_t pdm_pse_buf_ram_sbe          : 1;  /**< Single-bit error for PDM_PSE_BUF_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_pdm_intf_i.pko_peb_pse_buf_i */
+	uint64_t reserved_58_58               : 1;
+	uint64_t peb_st_inf_ram_sbe           : 1;  /**< Single-bit error for PEB_ST_INF_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_state_info_mem_i */
+	uint64_t pd_bank3_ram_sbe             : 1;  /**< Single-bit error for PD_BANK3_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_pd_mem_bank3_i */
+	uint64_t pd_bank2_ram_sbe             : 1;  /**< Single-bit error for PD_BANK2_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_pd_mem_bank2_i */
+	uint64_t pd_bank1_ram_sbe             : 1;  /**< Single-bit error for PD_BANK1_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_pd_mem_bank1_i */
+	uint64_t pd_bank0_ram_sbe             : 1;  /**< Single-bit error for PD_BANK1_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_pd_mem_bank0_i */
+	uint64_t pd_var_bank_ram_sbe          : 1;  /**< Single-bit error for PD_VAR_BANK_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_pd_var_mem_bank_i */
+	uint64_t tx_fifo_crc_ram_sbe          : 1;  /**< Single-bit error for TX_FIFO_CRC_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_tx_fifo_i.pko_peb_tx_fifo_crc_i */
+	uint64_t tx_fifo_hdr_ram_sbe          : 1;  /**< Single-bit error for TX_FIFO_HDR_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_tx_fifo_i.pko_peb_tx_fifo_hdr_i */
+	uint64_t tx_fifo_pkt_ram_sbe          : 1;  /**< Single-bit error for TX_FIFO_PKT_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_tx_fifo_i.pko_peb_tx_fifo_pkt_i */
+	uint64_t add_work_fifo_sbe            : 1;  /**< Single-bit error for ADD_WORK_FIFO. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_proc_i.pko_peb_addwork_proc_i.pko_peb_add_work_fifo_i */
+	uint64_t send_mem_fifo_sbe            : 1;  /**< Single-bit error for SEND_MEM_FIFO. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_proc_i.pko_peb_sendmem_proc_i.pko_peb_send_mem_fifo_i */
+	uint64_t send_mem_stdn_fifo_sbe       : 1;  /**< Single-bit error for SEND_MEM_STDN_FIFO. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_proc_i.pko_peb_sendmem_proc_i.pko_peb_send_mem_stdn_fifo_i */
+	uint64_t send_mem_ts_fifo_sbe         : 1;  /**< Single-bit error for SEND_MEM_TS_FIFO. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_proc_i.pko_peb_sendmem_proc_i.pko_peb_send_mem_ts_fifo_i */
+	uint64_t nxt_link_ptr_ram_sbe         : 1;  /**< Single-bit error for NXT_LINK_PTR_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_nxt_link_ptr_mem_i */
+	uint64_t pkt_mrk_ram_sbe              : 1;  /**< Single-bit error for PKT_MRK_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_pkt_mrk_mem_i */
+	uint64_t ts_addwork_ram_sbe           : 1;  /**< Single-bit error for TS_ADDWORK_RAM. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_proc_i.pko_peb_addwork_proc_i.pko_peb_ts_addwork_mem_i */
+	uint64_t reserved_0_41                : 42;
+#else
+	uint64_t reserved_0_41                : 42;
+	uint64_t ts_addwork_ram_sbe           : 1;
+	uint64_t pkt_mrk_ram_sbe              : 1;
+	uint64_t nxt_link_ptr_ram_sbe         : 1;
+	uint64_t send_mem_ts_fifo_sbe         : 1;
+	uint64_t send_mem_stdn_fifo_sbe       : 1;
+	uint64_t send_mem_fifo_sbe            : 1;
+	uint64_t add_work_fifo_sbe            : 1;
+	uint64_t tx_fifo_pkt_ram_sbe          : 1;
+	uint64_t tx_fifo_hdr_ram_sbe          : 1;
+	uint64_t tx_fifo_crc_ram_sbe          : 1;
+	uint64_t pd_var_bank_ram_sbe          : 1;
+	uint64_t pd_bank0_ram_sbe             : 1;
+	uint64_t pd_bank1_ram_sbe             : 1;
+	uint64_t pd_bank2_ram_sbe             : 1;
+	uint64_t pd_bank3_ram_sbe             : 1;
+	uint64_t peb_st_inf_ram_sbe           : 1;
+	uint64_t reserved_58_58               : 1;
+	uint64_t pdm_pse_buf_ram_sbe          : 1;
+	uint64_t pdm_resp_buf_ram_sbe         : 1;
+	uint64_t iobp1_fifo_ram_sbe           : 1;
+	uint64_t iobp0_fifo_ram_sbe           : 1;
+	uint64_t iobp1_uid_fifo_ram_sbe       : 1;
+#endif
+	} s;
+	struct cvmx_pko_peb_ecc_sbe_sts0_s    cn78xx;
 };
+typedef union cvmx_pko_peb_ecc_sbe_sts0 cvmx_pko_peb_ecc_sbe_sts0_t;
 
-union cvmx_pko_mem_debug0 {
+/**
+ * cvmx_pko_peb_ecc_sbe_sts_cmb0
+ */
+union cvmx_pko_peb_ecc_sbe_sts_cmb0 {
 	uint64_t u64;
-	struct cvmx_pko_mem_debug0_s {
+	struct cvmx_pko_peb_ecc_sbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t fau:28;
-		uint64_t cmd:14;
-		uint64_t segs:6;
-		uint64_t size:16;
-#else
-		uint64_t size:16;
-		uint64_t segs:6;
-		uint64_t cmd:14;
-		uint64_t fau:28;
-#endif
-	} s;
-	struct cvmx_pko_mem_debug0_s cn30xx;
-	struct cvmx_pko_mem_debug0_s cn31xx;
-	struct cvmx_pko_mem_debug0_s cn38xx;
-	struct cvmx_pko_mem_debug0_s cn38xxp2;
-	struct cvmx_pko_mem_debug0_s cn50xx;
-	struct cvmx_pko_mem_debug0_s cn52xx;
-	struct cvmx_pko_mem_debug0_s cn52xxp1;
-	struct cvmx_pko_mem_debug0_s cn56xx;
-	struct cvmx_pko_mem_debug0_s cn56xxp1;
-	struct cvmx_pko_mem_debug0_s cn58xx;
-	struct cvmx_pko_mem_debug0_s cn58xxp1;
-	struct cvmx_pko_mem_debug0_s cn61xx;
-	struct cvmx_pko_mem_debug0_s cn63xx;
-	struct cvmx_pko_mem_debug0_s cn63xxp1;
-	struct cvmx_pko_mem_debug0_s cn66xx;
-	struct cvmx_pko_mem_debug0_s cn68xx;
-	struct cvmx_pko_mem_debug0_s cn68xxp1;
-	struct cvmx_pko_mem_debug0_s cnf71xx;
+	uint64_t peb_sbe_cmb0                 : 1;  /**< This bit is the logical OR of all bits in PKO_PEB_ECC_SBE_STS0. To clear this bit,
+                                                         software
+                                                         must clear bits in PKO_PEB_ECC_SBE_STS0. When this bit is set, the corresponding interrupt
+                                                         is set. Throws PKO_INTSN_E::PKO_PEB_SBE_CMB0. INTERNAL: Instances:
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_proc_i.pko_peb_iobp1_uid_fifo_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_iobp0_fifo_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_proc_i.pko_peb_iobp1_fifo_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_pdm_intf_i.pko_peb_pdm_resp_buf_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_pdm_intf_i.pko_peb_pse_buf_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_state_info_mem_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_pd_mem_bank3_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_pd_mem_bank0_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_pd_mem_bank1_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_pd_mem_bank2_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_pd_var_mem_bank_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_tx_fifo_i.pko_peb_tx_fifo_crc_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_tx_fifo_i.pko_peb_tx_fifo_hdr_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_tx_fifo_i.pko_peb_tx_fifo_pkt_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_proc_i.pko_peb_addwork_proc_i.pko_peb_add_work_fifo_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_proc_i.pko_peb_sendmem_proc_i.pko_peb_send_mem_fifo_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_proc_i.pko_peb_sendmem_proc_i.pko_peb_send_mem_stdn_fifo_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_proc_i.pko_peb_sendmem_proc_i.pko_peb_send_mem_ts_fifo_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_nxt_link_ptr_mem_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_state_mem_i.pko_peb_pkt_mrk_mem_i
+                                                         pko_pnr3.pko_pnr3_peb.pko_peb_proc_i.pko_peb_addwork_proc_i.pko_peb_ts_addwork_mem_i */
+	uint64_t reserved_0_62                : 63;
+#else
+	uint64_t reserved_0_62                : 63;
+	uint64_t peb_sbe_cmb0                 : 1;
+#endif
+	} s;
+	struct cvmx_pko_peb_ecc_sbe_sts_cmb0_s cn78xx;
 };
+typedef union cvmx_pko_peb_ecc_sbe_sts_cmb0 cvmx_pko_peb_ecc_sbe_sts_cmb0_t;
 
-union cvmx_pko_mem_debug1 {
+/**
+ * cvmx_pko_peb_err_int
+ */
+union cvmx_pko_peb_err_int {
 	uint64_t u64;
-	struct cvmx_pko_mem_debug1_s {
+	struct cvmx_pko_peb_err_int_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t i:1;
-		uint64_t back:4;
-		uint64_t pool:3;
-		uint64_t size:16;
-		uint64_t ptr:40;
-#else
-		uint64_t ptr:40;
-		uint64_t size:16;
-		uint64_t pool:3;
-		uint64_t back:4;
-		uint64_t i:1;
-#endif
-	} s;
-	struct cvmx_pko_mem_debug1_s cn30xx;
-	struct cvmx_pko_mem_debug1_s cn31xx;
-	struct cvmx_pko_mem_debug1_s cn38xx;
-	struct cvmx_pko_mem_debug1_s cn38xxp2;
-	struct cvmx_pko_mem_debug1_s cn50xx;
-	struct cvmx_pko_mem_debug1_s cn52xx;
-	struct cvmx_pko_mem_debug1_s cn52xxp1;
-	struct cvmx_pko_mem_debug1_s cn56xx;
-	struct cvmx_pko_mem_debug1_s cn56xxp1;
-	struct cvmx_pko_mem_debug1_s cn58xx;
-	struct cvmx_pko_mem_debug1_s cn58xxp1;
-	struct cvmx_pko_mem_debug1_s cn61xx;
-	struct cvmx_pko_mem_debug1_s cn63xx;
-	struct cvmx_pko_mem_debug1_s cn63xxp1;
-	struct cvmx_pko_mem_debug1_s cn66xx;
-	struct cvmx_pko_mem_debug1_s cn68xx;
-	struct cvmx_pko_mem_debug1_s cn68xxp1;
-	struct cvmx_pko_mem_debug1_s cnf71xx;
+	uint64_t reserved_10_63               : 54;
+	uint64_t peb_macx_cfg_wr_err          : 1;  /**< Asserted when software writes a FIFO number to PKO_MAC()_CFG when that FIFO is
+                                                         already assigned. Throws PKO_INTSN_E::PEB_MACX_CFG_WR_ERR. */
+	uint64_t peb_max_link_err             : 1;  /**< Asserted when 200 LINK segments have been followed. Indicates likelihood of infinite loop.
+                                                         Throws PKO_INTSN_E::PEB_MAX_LINK_ERR. */
+	uint64_t peb_subd_size_err            : 1;  /**< Asserted when a SEND_LINK/GATHER/IMM/JUMP subD has size=0. Throws
+                                                         PKO_INTSN_E::PEB_SUBD_SIZE_ERR. */
+	uint64_t peb_subd_addr_err            : 1;  /**< Asserted when the address of a FREE/MEM/LINK/LINK segment/JUMP/GATHER subD is 0x0. Throws
+                                                         PKO_INTSN_E::PEB_SUBD_ADDR_ERR. */
+	uint64_t peb_trunc_err                : 1;  /**< Asserted when a PD has truncated data. Throws PKO_INTSN_E::PEB_TRUNC_ERR. */
+	uint64_t peb_pad_err                  : 1;  /**< Asserted when a PD has data padded to it (SEND_HDR[TOTAL] < sum(SEND_DATA[size])). Throws
+                                                         PKO_INTSN_E::PEB_PAD_ERR. */
+	uint64_t peb_pse_fifo_err             : 1;  /**< Asserted when PSE sends PD information for a nonconfigured FIFO. Throws
+                                                         PKO_INTSN_E::PEB_PSE_FIFO_ERR. */
+	uint64_t peb_fcs_sop_err              : 1;  /**< Asserted when FCS SOP value greater than packet size detected. Throws
+                                                         PKO_INTSN_E::PEB_FCS_SOP_ERR. */
+	uint64_t peb_jump_def_err             : 1;  /**< Asserted when JUMP subdescriptor is not last in a PD. Throws
+                                                         PKO_INTSN_E::PEB_JUMP_DEF_ERR. */
+	uint64_t peb_ext_hdr_def_err          : 1;  /**< Asserted when EXT_HDR is not the second sub-descriptor in a PD. Throws
+                                                         PKO_INTSN_E::PEB_EXT_HDR_DEF_ERR. */
+#else
+	uint64_t peb_ext_hdr_def_err          : 1;
+	uint64_t peb_jump_def_err             : 1;
+	uint64_t peb_fcs_sop_err              : 1;
+	uint64_t peb_pse_fifo_err             : 1;
+	uint64_t peb_pad_err                  : 1;
+	uint64_t peb_trunc_err                : 1;
+	uint64_t peb_subd_addr_err            : 1;
+	uint64_t peb_subd_size_err            : 1;
+	uint64_t peb_max_link_err             : 1;
+	uint64_t peb_macx_cfg_wr_err          : 1;
+	uint64_t reserved_10_63               : 54;
+#endif
+	} s;
+	struct cvmx_pko_peb_err_int_s         cn78xx;
 };
+typedef union cvmx_pko_peb_err_int cvmx_pko_peb_err_int_t;
 
-union cvmx_pko_mem_debug10 {
+/**
+ * cvmx_pko_peb_ext_hdr_def_err_info
+ */
+union cvmx_pko_peb_ext_hdr_def_err_info {
 	uint64_t u64;
-	struct cvmx_pko_mem_debug10_s {
+	struct cvmx_pko_peb_ext_hdr_def_err_info_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_0_63:64;
+	uint64_t reserved_20_63               : 44;
+	uint64_t val                          : 1;  /**< Asserted when PKO_PEB_ERR_INT[PEB_EXT_HDR_DEF_ERR] is set. */
+	uint64_t fifo                         : 7;  /**< FIFO number associated with the captured PEB_EXT_HDR_DEF_ERR. */
+	uint64_t chan                         : 12; /**< Channel number associated with the captured PEB_EXT_HDR_DEF_ERR. */
 #else
-		uint64_t reserved_0_63:64;
+	uint64_t chan                         : 12;
+	uint64_t fifo                         : 7;
+	uint64_t val                          : 1;
+	uint64_t reserved_20_63               : 44;
 #endif
 	} s;
-	struct cvmx_pko_mem_debug10_cn30xx {
+	struct cvmx_pko_peb_ext_hdr_def_err_info_s cn78xx;
+};
+typedef union cvmx_pko_peb_ext_hdr_def_err_info cvmx_pko_peb_ext_hdr_def_err_info_t;
+
+/**
+ * cvmx_pko_peb_fcs_sop_err_info
+ */
+union cvmx_pko_peb_fcs_sop_err_info {
+	uint64_t u64;
+	struct cvmx_pko_peb_fcs_sop_err_info_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t fau:28;
-		uint64_t cmd:14;
-		uint64_t segs:6;
-		uint64_t size:16;
+	uint64_t reserved_20_63               : 44;
+	uint64_t val                          : 1;  /**< Asserted when PKO_PEB_ERR_INT[PEB_FCS_SOP_ERR] is set. */
+	uint64_t fifo                         : 7;  /**< FIFO number associated with the captured PEB_FCS_SOP_ERR. */
+	uint64_t chan                         : 12; /**< Channel number associated with the captured PEB_FCS_SOP_ERR. */
 #else
-		uint64_t size:16;
-		uint64_t segs:6;
-		uint64_t cmd:14;
-		uint64_t fau:28;
+	uint64_t chan                         : 12;
+	uint64_t fifo                         : 7;
+	uint64_t val                          : 1;
+	uint64_t reserved_20_63               : 44;
 #endif
-	} cn30xx;
-	struct cvmx_pko_mem_debug10_cn30xx cn31xx;
-	struct cvmx_pko_mem_debug10_cn30xx cn38xx;
-	struct cvmx_pko_mem_debug10_cn30xx cn38xxp2;
-	struct cvmx_pko_mem_debug10_cn50xx {
+	} s;
+	struct cvmx_pko_peb_fcs_sop_err_info_s cn78xx;
+};
+typedef union cvmx_pko_peb_fcs_sop_err_info cvmx_pko_peb_fcs_sop_err_info_t;
+
+/**
+ * cvmx_pko_peb_jump_def_err_info
+ */
+union cvmx_pko_peb_jump_def_err_info {
+	uint64_t u64;
+	struct cvmx_pko_peb_jump_def_err_info_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_49_63:15;
-		uint64_t ptrs1:17;
-		uint64_t reserved_17_31:15;
-		uint64_t ptrs2:17;
+	uint64_t reserved_20_63               : 44;
+	uint64_t val                          : 1;  /**< Asserted when PKO_PEB_ERR_INT[PEB_JUMP_DEF_ERR] is set. */
+	uint64_t fifo                         : 7;  /**< FIFO number associated with the captured PEB_JUMP_DEF_ERR. */
+	uint64_t chan                         : 12; /**< Channel number associated with the captured PEB_JUMP_DEF_ERR. */
 #else
-		uint64_t ptrs2:17;
-		uint64_t reserved_17_31:15;
-		uint64_t ptrs1:17;
-		uint64_t reserved_49_63:15;
+	uint64_t chan                         : 12;
+	uint64_t fifo                         : 7;
+	uint64_t val                          : 1;
+	uint64_t reserved_20_63               : 44;
 #endif
-	} cn50xx;
-	struct cvmx_pko_mem_debug10_cn50xx cn52xx;
-	struct cvmx_pko_mem_debug10_cn50xx cn52xxp1;
-	struct cvmx_pko_mem_debug10_cn50xx cn56xx;
-	struct cvmx_pko_mem_debug10_cn50xx cn56xxp1;
-	struct cvmx_pko_mem_debug10_cn50xx cn58xx;
-	struct cvmx_pko_mem_debug10_cn50xx cn58xxp1;
-	struct cvmx_pko_mem_debug10_cn50xx cn61xx;
-	struct cvmx_pko_mem_debug10_cn50xx cn63xx;
-	struct cvmx_pko_mem_debug10_cn50xx cn63xxp1;
-	struct cvmx_pko_mem_debug10_cn50xx cn66xx;
-	struct cvmx_pko_mem_debug10_cn50xx cn68xx;
-	struct cvmx_pko_mem_debug10_cn50xx cn68xxp1;
-	struct cvmx_pko_mem_debug10_cn50xx cnf71xx;
+	} s;
+	struct cvmx_pko_peb_jump_def_err_info_s cn78xx;
 };
+typedef union cvmx_pko_peb_jump_def_err_info cvmx_pko_peb_jump_def_err_info_t;
 
-union cvmx_pko_mem_debug11 {
+/**
+ * cvmx_pko_peb_macx_cfg_wr_err_info
+ */
+union cvmx_pko_peb_macx_cfg_wr_err_info {
 	uint64_t u64;
-	struct cvmx_pko_mem_debug11_s {
+	struct cvmx_pko_peb_macx_cfg_wr_err_info_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t i:1;
-		uint64_t back:4;
-		uint64_t pool:3;
-		uint64_t size:16;
-		uint64_t reserved_0_39:40;
+	uint64_t reserved_8_63                : 56;
+	uint64_t val                          : 1;  /**< Asserted when PKO_PEB_ERR_INT[PEB_MACX_CFG_WR_ERR] is set. */
+	uint64_t mac                          : 7;  /**< MAC number associated with the captured PEB_MACX_CFG_WR_ERR. */
 #else
-		uint64_t reserved_0_39:40;
-		uint64_t size:16;
-		uint64_t pool:3;
-		uint64_t back:4;
-		uint64_t i:1;
+	uint64_t mac                          : 7;
+	uint64_t val                          : 1;
+	uint64_t reserved_8_63                : 56;
 #endif
 	} s;
-	struct cvmx_pko_mem_debug11_cn30xx {
+	struct cvmx_pko_peb_macx_cfg_wr_err_info_s cn78xx;
+};
+typedef union cvmx_pko_peb_macx_cfg_wr_err_info cvmx_pko_peb_macx_cfg_wr_err_info_t;
+
+/**
+ * cvmx_pko_peb_max_link_err_info
+ */
+union cvmx_pko_peb_max_link_err_info {
+	uint64_t u64;
+	struct cvmx_pko_peb_max_link_err_info_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t i:1;
-		uint64_t back:4;
-		uint64_t pool:3;
-		uint64_t size:16;
-		uint64_t ptr:40;
+	uint64_t reserved_20_63               : 44;
+	uint64_t val                          : 1;  /**< Asserted when PKO_PEB_ERR_INT[PEB_MAX_LINK_ERR] is set. */
+	uint64_t fifo                         : 7;  /**< FIFO number associated with the captured PEB_MAX_LINK_ERR. */
+	uint64_t chan                         : 12; /**< Channel number associated with the captured PEB_MAX_LINK_ERR. */
 #else
-		uint64_t ptr:40;
-		uint64_t size:16;
-		uint64_t pool:3;
-		uint64_t back:4;
-		uint64_t i:1;
+	uint64_t chan                         : 12;
+	uint64_t fifo                         : 7;
+	uint64_t val                          : 1;
+	uint64_t reserved_20_63               : 44;
 #endif
-	} cn30xx;
-	struct cvmx_pko_mem_debug11_cn30xx cn31xx;
-	struct cvmx_pko_mem_debug11_cn30xx cn38xx;
-	struct cvmx_pko_mem_debug11_cn30xx cn38xxp2;
-	struct cvmx_pko_mem_debug11_cn50xx {
+	} s;
+	struct cvmx_pko_peb_max_link_err_info_s cn78xx;
+};
+typedef union cvmx_pko_peb_max_link_err_info cvmx_pko_peb_max_link_err_info_t;
+
+/**
+ * cvmx_pko_peb_ncb_cfg
+ */
+union cvmx_pko_peb_ncb_cfg {
+	uint64_t u64;
+	struct cvmx_pko_peb_ncb_cfg_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_23_63:41;
-		uint64_t maj:1;
-		uint64_t uid:3;
-		uint64_t sop:1;
-		uint64_t len:1;
-		uint64_t chk:1;
-		uint64_t cnt:13;
-		uint64_t mod:3;
-#else
-		uint64_t mod:3;
-		uint64_t cnt:13;
-		uint64_t chk:1;
-		uint64_t len:1;
-		uint64_t sop:1;
-		uint64_t uid:3;
-		uint64_t maj:1;
-		uint64_t reserved_23_63:41;
+	uint64_t reserved_1_63                : 63;
+	uint64_t rstp                         : 1;  /**< Reserved. */
+#else
+	uint64_t rstp                         : 1;
+	uint64_t reserved_1_63                : 63;
 #endif
-	} cn50xx;
-	struct cvmx_pko_mem_debug11_cn50xx cn52xx;
-	struct cvmx_pko_mem_debug11_cn50xx cn52xxp1;
-	struct cvmx_pko_mem_debug11_cn50xx cn56xx;
-	struct cvmx_pko_mem_debug11_cn50xx cn56xxp1;
-	struct cvmx_pko_mem_debug11_cn50xx cn58xx;
-	struct cvmx_pko_mem_debug11_cn50xx cn58xxp1;
-	struct cvmx_pko_mem_debug11_cn50xx cn61xx;
-	struct cvmx_pko_mem_debug11_cn50xx cn63xx;
-	struct cvmx_pko_mem_debug11_cn50xx cn63xxp1;
-	struct cvmx_pko_mem_debug11_cn50xx cn66xx;
-	struct cvmx_pko_mem_debug11_cn50xx cn68xx;
-	struct cvmx_pko_mem_debug11_cn50xx cn68xxp1;
-	struct cvmx_pko_mem_debug11_cn50xx cnf71xx;
+	} s;
+	struct cvmx_pko_peb_ncb_cfg_s         cn78xx;
 };
+typedef union cvmx_pko_peb_ncb_cfg cvmx_pko_peb_ncb_cfg_t;
 
-union cvmx_pko_mem_debug12 {
+/**
+ * cvmx_pko_peb_pad_err_info
+ */
+union cvmx_pko_peb_pad_err_info {
 	uint64_t u64;
-	struct cvmx_pko_mem_debug12_s {
+	struct cvmx_pko_peb_pad_err_info_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_0_63:64;
+	uint64_t reserved_20_63               : 44;
+	uint64_t val                          : 1;  /**< Asserted when PKO_PEB_ERR_INT[PEB_PAD_ERR] is set. */
+	uint64_t fifo                         : 7;  /**< FIFO number associated with the captured PEB_PAD_ERR. */
+	uint64_t chan                         : 12; /**< Channel number associated with the captured PEB_PAD_ERR. */
 #else
-		uint64_t reserved_0_63:64;
+	uint64_t chan                         : 12;
+	uint64_t fifo                         : 7;
+	uint64_t val                          : 1;
+	uint64_t reserved_20_63               : 44;
 #endif
 	} s;
-	struct cvmx_pko_mem_debug12_cn30xx {
+	struct cvmx_pko_peb_pad_err_info_s    cn78xx;
+};
+typedef union cvmx_pko_peb_pad_err_info cvmx_pko_peb_pad_err_info_t;
+
+/**
+ * cvmx_pko_peb_pse_fifo_err_info
+ */
+union cvmx_pko_peb_pse_fifo_err_info {
+	uint64_t u64;
+	struct cvmx_pko_peb_pse_fifo_err_info_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t data:64;
+	uint64_t reserved_20_63               : 44;
+	uint64_t val                          : 1;  /**< Asserted when PKO_PEB_ERR_INT[PEB_PSE_FIFO_ERR] is set. */
+	uint64_t fifo                         : 7;  /**< FIFO number associated with the captured PEB_PSE_FIFO_ERR. */
+	uint64_t chan                         : 12; /**< Channel number associated with the captured PEB_PSE_FIFO_ERR. */
 #else
-		uint64_t data:64;
+	uint64_t chan                         : 12;
+	uint64_t fifo                         : 7;
+	uint64_t val                          : 1;
+	uint64_t reserved_20_63               : 44;
 #endif
-	} cn30xx;
-	struct cvmx_pko_mem_debug12_cn30xx cn31xx;
-	struct cvmx_pko_mem_debug12_cn30xx cn38xx;
-	struct cvmx_pko_mem_debug12_cn30xx cn38xxp2;
-	struct cvmx_pko_mem_debug12_cn50xx {
+	} s;
+	struct cvmx_pko_peb_pse_fifo_err_info_s cn78xx;
+};
+typedef union cvmx_pko_peb_pse_fifo_err_info cvmx_pko_peb_pse_fifo_err_info_t;
+
+/**
+ * cvmx_pko_peb_subd_addr_err_info
+ */
+union cvmx_pko_peb_subd_addr_err_info {
+	uint64_t u64;
+	struct cvmx_pko_peb_subd_addr_err_info_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t fau:28;
-		uint64_t cmd:14;
-		uint64_t segs:6;
-		uint64_t size:16;
+	uint64_t reserved_20_63               : 44;
+	uint64_t val                          : 1;  /**< Asserted when PKO_PEB_ERR_INT[PEB_SUBD_ADDR_ERR] is set. */
+	uint64_t fifo                         : 7;  /**< FIFO number associated with the captured PEB_SUBD_ADDR_ERR. */
+	uint64_t chan                         : 12; /**< Channel number associated with the captured PEB_SUBD_ADDR_ERR. */
 #else
-		uint64_t size:16;
-		uint64_t segs:6;
-		uint64_t cmd:14;
-		uint64_t fau:28;
+	uint64_t chan                         : 12;
+	uint64_t fifo                         : 7;
+	uint64_t val                          : 1;
+	uint64_t reserved_20_63               : 44;
 #endif
-	} cn50xx;
-	struct cvmx_pko_mem_debug12_cn50xx cn52xx;
-	struct cvmx_pko_mem_debug12_cn50xx cn52xxp1;
-	struct cvmx_pko_mem_debug12_cn50xx cn56xx;
-	struct cvmx_pko_mem_debug12_cn50xx cn56xxp1;
-	struct cvmx_pko_mem_debug12_cn50xx cn58xx;
-	struct cvmx_pko_mem_debug12_cn50xx cn58xxp1;
-	struct cvmx_pko_mem_debug12_cn50xx cn61xx;
-	struct cvmx_pko_mem_debug12_cn50xx cn63xx;
-	struct cvmx_pko_mem_debug12_cn50xx cn63xxp1;
-	struct cvmx_pko_mem_debug12_cn50xx cn66xx;
-	struct cvmx_pko_mem_debug12_cn68xx {
+	} s;
+	struct cvmx_pko_peb_subd_addr_err_info_s cn78xx;
+};
+typedef union cvmx_pko_peb_subd_addr_err_info cvmx_pko_peb_subd_addr_err_info_t;
+
+/**
+ * cvmx_pko_peb_subd_size_err_info
+ */
+union cvmx_pko_peb_subd_size_err_info {
+	uint64_t u64;
+	struct cvmx_pko_peb_subd_size_err_info_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t state:64;
+	uint64_t reserved_20_63               : 44;
+	uint64_t val                          : 1;  /**< Asserted when PKO_PEB_ERR_INT[PEB_SUBD_SIZE_ERR] is set. */
+	uint64_t fifo                         : 7;  /**< FIFO number associated with the captured PEB_SUBD_SIZE_ERR. */
+	uint64_t chan                         : 12; /**< Channel number associated with the captured PEB_SUBD_SIZE_ERR. */
 #else
-		uint64_t state:64;
+	uint64_t chan                         : 12;
+	uint64_t fifo                         : 7;
+	uint64_t val                          : 1;
+	uint64_t reserved_20_63               : 44;
 #endif
-	} cn68xx;
-	struct cvmx_pko_mem_debug12_cn68xx cn68xxp1;
-	struct cvmx_pko_mem_debug12_cn50xx cnf71xx;
+	} s;
+	struct cvmx_pko_peb_subd_size_err_info_s cn78xx;
 };
+typedef union cvmx_pko_peb_subd_size_err_info cvmx_pko_peb_subd_size_err_info_t;
 
-union cvmx_pko_mem_debug13 {
+/**
+ * cvmx_pko_peb_trunc_err_info
+ */
+union cvmx_pko_peb_trunc_err_info {
 	uint64_t u64;
-	struct cvmx_pko_mem_debug13_s {
+	struct cvmx_pko_peb_trunc_err_info_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_0_63:64;
+	uint64_t reserved_20_63               : 44;
+	uint64_t val                          : 1;  /**< Asserted when PKO_PEB_ERR_INT[PEB_TRUNC_ERR] is set. */
+	uint64_t fifo                         : 7;  /**< FIFO number associated with the captured PEB_TRUNC_ERR. */
+	uint64_t chan                         : 12; /**< Channel number associated with the captured PEB_TRUNC_ERR. */
 #else
-		uint64_t reserved_0_63:64;
+	uint64_t chan                         : 12;
+	uint64_t fifo                         : 7;
+	uint64_t val                          : 1;
+	uint64_t reserved_20_63               : 44;
 #endif
 	} s;
-	struct cvmx_pko_mem_debug13_cn30xx {
+	struct cvmx_pko_peb_trunc_err_info_s  cn78xx;
+};
+typedef union cvmx_pko_peb_trunc_err_info cvmx_pko_peb_trunc_err_info_t;
+
+/**
+ * cvmx_pko_pq_csr_bus_debug
+ */
+union cvmx_pko_pq_csr_bus_debug {
+	uint64_t u64;
+	struct cvmx_pko_pq_csr_bus_debug_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_51_63:13;
-		uint64_t widx:17;
-		uint64_t ridx2:17;
-		uint64_t widx2:17;
+	uint64_t csr_bus_debug                : 64; /**< N/A */
 #else
-		uint64_t widx2:17;
-		uint64_t ridx2:17;
-		uint64_t widx:17;
-		uint64_t reserved_51_63:13;
+	uint64_t csr_bus_debug                : 64;
 #endif
-	} cn30xx;
-	struct cvmx_pko_mem_debug13_cn30xx cn31xx;
-	struct cvmx_pko_mem_debug13_cn30xx cn38xx;
-	struct cvmx_pko_mem_debug13_cn30xx cn38xxp2;
-	struct cvmx_pko_mem_debug13_cn50xx {
+	} s;
+	struct cvmx_pko_pq_csr_bus_debug_s    cn78xx;
+};
+typedef union cvmx_pko_pq_csr_bus_debug cvmx_pko_pq_csr_bus_debug_t;
+
+/**
+ * cvmx_pko_pq_debug_green
+ */
+union cvmx_pko_pq_debug_green {
+	uint64_t u64;
+	struct cvmx_pko_pq_debug_green_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t i:1;
-		uint64_t back:4;
-		uint64_t pool:3;
-		uint64_t size:16;
-		uint64_t ptr:40;
+	uint64_t g_valid                      : 32; /**< g_valid vector. */
+	uint64_t cred_ok_n                    : 32; /**< cred_ok_n vector. */
 #else
-		uint64_t ptr:40;
-		uint64_t size:16;
-		uint64_t pool:3;
-		uint64_t back:4;
-		uint64_t i:1;
+	uint64_t cred_ok_n                    : 32;
+	uint64_t g_valid                      : 32;
 #endif
-	} cn50xx;
-	struct cvmx_pko_mem_debug13_cn50xx cn52xx;
-	struct cvmx_pko_mem_debug13_cn50xx cn52xxp1;
-	struct cvmx_pko_mem_debug13_cn50xx cn56xx;
-	struct cvmx_pko_mem_debug13_cn50xx cn56xxp1;
-	struct cvmx_pko_mem_debug13_cn50xx cn58xx;
-	struct cvmx_pko_mem_debug13_cn50xx cn58xxp1;
-	struct cvmx_pko_mem_debug13_cn50xx cn61xx;
-	struct cvmx_pko_mem_debug13_cn50xx cn63xx;
-	struct cvmx_pko_mem_debug13_cn50xx cn63xxp1;
-	struct cvmx_pko_mem_debug13_cn50xx cn66xx;
-	struct cvmx_pko_mem_debug13_cn68xx {
+	} s;
+	struct cvmx_pko_pq_debug_green_s      cn78xx;
+};
+typedef union cvmx_pko_pq_debug_green cvmx_pko_pq_debug_green_t;
+
+/**
+ * cvmx_pko_pq_debug_links
+ */
+union cvmx_pko_pq_debug_links {
+	uint64_t u64;
+	struct cvmx_pko_pq_debug_links_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t state:64;
+	uint64_t links_ready                  : 32; /**< links_ready vector. */
+	uint64_t peb_lnk_rdy_ir               : 32; /**< peb_lnk_rdy_ir vector. */
 #else
-		uint64_t state:64;
+	uint64_t peb_lnk_rdy_ir               : 32;
+	uint64_t links_ready                  : 32;
 #endif
-	} cn68xx;
-	struct cvmx_pko_mem_debug13_cn68xx cn68xxp1;
-	struct cvmx_pko_mem_debug13_cn50xx cnf71xx;
+	} s;
+	struct cvmx_pko_pq_debug_links_s      cn78xx;
 };
+typedef union cvmx_pko_pq_debug_links cvmx_pko_pq_debug_links_t;
 
-union cvmx_pko_mem_debug14 {
+/**
+ * cvmx_pko_pq_debug_yellow
+ */
+union cvmx_pko_pq_debug_yellow {
 	uint64_t u64;
-	struct cvmx_pko_mem_debug14_s {
+	struct cvmx_pko_pq_debug_yellow_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_0_63:64;
+	uint64_t y_valid                      : 32; /**< y_valid vector. */
+	uint64_t reserved_28_31               : 4;
+	uint64_t link_vv                      : 28; /**< link_vv vector. */
 #else
-		uint64_t reserved_0_63:64;
+	uint64_t link_vv                      : 28;
+	uint64_t reserved_28_31               : 4;
+	uint64_t y_valid                      : 32;
 #endif
 	} s;
-	struct cvmx_pko_mem_debug14_cn30xx {
+	struct cvmx_pko_pq_debug_yellow_s     cn78xx;
+};
+typedef union cvmx_pko_pq_debug_yellow cvmx_pko_pq_debug_yellow_t;
+
+/**
+ * cvmx_pko_pqa_debug
+ */
+union cvmx_pko_pqa_debug {
+	uint64_t u64;
+	struct cvmx_pko_pqa_debug_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_17_63:47;
-		uint64_t ridx:17;
+	uint64_t dbg_vec                      : 64; /**< Debug Vector. */
 #else
-		uint64_t ridx:17;
-		uint64_t reserved_17_63:47;
+	uint64_t dbg_vec                      : 64;
 #endif
-	} cn30xx;
-	struct cvmx_pko_mem_debug14_cn30xx cn31xx;
-	struct cvmx_pko_mem_debug14_cn30xx cn38xx;
-	struct cvmx_pko_mem_debug14_cn30xx cn38xxp2;
-	struct cvmx_pko_mem_debug14_cn52xx {
+	} s;
+	struct cvmx_pko_pqa_debug_s           cn78xx;
+};
+typedef union cvmx_pko_pqa_debug cvmx_pko_pqa_debug_t;
+
+/**
+ * cvmx_pko_pqb_debug
+ *
+ * This register has the same bit fields as PKO_PQA_DEBUG.
+ *
+ */
+union cvmx_pko_pqb_debug {
+	uint64_t u64;
+	struct cvmx_pko_pqb_debug_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t data:64;
+	uint64_t dbg_vec                      : 64; /**< Debug Vector. */
 #else
-		uint64_t data:64;
+	uint64_t dbg_vec                      : 64;
 #endif
-	} cn52xx;
-	struct cvmx_pko_mem_debug14_cn52xx cn52xxp1;
-	struct cvmx_pko_mem_debug14_cn52xx cn56xx;
-	struct cvmx_pko_mem_debug14_cn52xx cn56xxp1;
-	struct cvmx_pko_mem_debug14_cn52xx cn61xx;
-	struct cvmx_pko_mem_debug14_cn52xx cn63xx;
-	struct cvmx_pko_mem_debug14_cn52xx cn63xxp1;
-	struct cvmx_pko_mem_debug14_cn52xx cn66xx;
-	struct cvmx_pko_mem_debug14_cn52xx cnf71xx;
+	} s;
+	struct cvmx_pko_pqb_debug_s           cn78xx;
 };
+typedef union cvmx_pko_pqb_debug cvmx_pko_pqb_debug_t;
 
-union cvmx_pko_mem_debug2 {
+/**
+ * cvmx_pko_pse_dq_bist_status
+ *
+ * Each bit is the BIST result of an individual memory (per bit, 0 = pass and 1 = fail).
+ *
+ */
+union cvmx_pko_pse_dq_bist_status {
 	uint64_t u64;
-	struct cvmx_pko_mem_debug2_s {
+	struct cvmx_pko_pse_dq_bist_status_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t i:1;
-		uint64_t back:4;
-		uint64_t pool:3;
-		uint64_t size:16;
-		uint64_t ptr:40;
-#else
-		uint64_t ptr:40;
-		uint64_t size:16;
-		uint64_t pool:3;
-		uint64_t back:4;
-		uint64_t i:1;
-#endif
-	} s;
-	struct cvmx_pko_mem_debug2_s cn30xx;
-	struct cvmx_pko_mem_debug2_s cn31xx;
-	struct cvmx_pko_mem_debug2_s cn38xx;
-	struct cvmx_pko_mem_debug2_s cn38xxp2;
-	struct cvmx_pko_mem_debug2_s cn50xx;
-	struct cvmx_pko_mem_debug2_s cn52xx;
-	struct cvmx_pko_mem_debug2_s cn52xxp1;
-	struct cvmx_pko_mem_debug2_s cn56xx;
-	struct cvmx_pko_mem_debug2_s cn56xxp1;
-	struct cvmx_pko_mem_debug2_s cn58xx;
-	struct cvmx_pko_mem_debug2_s cn58xxp1;
-	struct cvmx_pko_mem_debug2_s cn61xx;
-	struct cvmx_pko_mem_debug2_s cn63xx;
-	struct cvmx_pko_mem_debug2_s cn63xxp1;
-	struct cvmx_pko_mem_debug2_s cn66xx;
-	struct cvmx_pko_mem_debug2_s cn68xx;
-	struct cvmx_pko_mem_debug2_s cn68xxp1;
-	struct cvmx_pko_mem_debug2_s cnf71xx;
+	uint64_t reserved_9_63                : 55;
+	uint64_t wt_sram                      : 1;  /**< Work table. */
+	uint64_t rt7_sram                     : 1;  /**< Result table 7 - DQ FIFO[1023:896]. */
+	uint64_t rt6_sram                     : 1;  /**< Result table 6 - DQ FIFO[895:768]. */
+	uint64_t rt5_sram                     : 1;  /**< Result table 5 - DQ FIFO[767:640]. */
+	uint64_t rt4_sram                     : 1;  /**< Result table 4 - DQ FIFO[639:512]. */
+	uint64_t rt3_sram                     : 1;  /**< Result table 3 - DQ FIFO[511:384]. */
+	uint64_t rt2_sram                     : 1;  /**< Result table 2 - DQ FIFO[383:256]. */
+	uint64_t rt1_sram                     : 1;  /**< Result table 1 - DQ FIFO[255:128]. */
+	uint64_t rt0_sram                     : 1;  /**< Result table 0 - DQ FIFO[127:0]. */
+#else
+	uint64_t rt0_sram                     : 1;
+	uint64_t rt1_sram                     : 1;
+	uint64_t rt2_sram                     : 1;
+	uint64_t rt3_sram                     : 1;
+	uint64_t rt4_sram                     : 1;
+	uint64_t rt5_sram                     : 1;
+	uint64_t rt6_sram                     : 1;
+	uint64_t rt7_sram                     : 1;
+	uint64_t wt_sram                      : 1;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_pko_pse_dq_bist_status_s  cn78xx;
 };
+typedef union cvmx_pko_pse_dq_bist_status cvmx_pko_pse_dq_bist_status_t;
 
-union cvmx_pko_mem_debug3 {
+/**
+ * cvmx_pko_pse_dq_ecc_ctl0
+ */
+union cvmx_pko_pse_dq_ecc_ctl0 {
 	uint64_t u64;
-	struct cvmx_pko_mem_debug3_s {
+	struct cvmx_pko_pse_dq_ecc_ctl0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_0_63:64;
+	uint64_t dq_wt_ram_flip               : 2;  /**< DQ_WT_RAM flip syndrome bits on write. */
+	uint64_t dq_wt_ram_cdis               : 1;  /**< DQ_WT_RAM ECC correction disable. */
+	uint64_t dq_rt7_flip                  : 2;  /**< DQ_RT7 flip syndrome bits on write. */
+	uint64_t dq_rt7_cdis                  : 1;  /**< DQ_RT7 ECC correction disable. */
+	uint64_t dq_rt6_flip                  : 2;  /**< DQ_RT6 flip syndrome bits on write. */
+	uint64_t dq_rt6_cdis                  : 1;  /**< DQ_RT6 ECC correction disable. */
+	uint64_t dq_rt5_flip                  : 2;  /**< DQ_RT5 flip syndrome bits on write. */
+	uint64_t dq_rt5_cdis                  : 1;  /**< DQ_RT5 ECC correction disable. */
+	uint64_t dq_rt4_flip                  : 2;  /**< DQ_RT4 flip syndrome bits on write. */
+	uint64_t dq_rt4_cdis                  : 1;  /**< DQ_RT4 ECC correction disable. */
+	uint64_t dq_rt3_flip                  : 2;  /**< DQ_RT3 flip syndrome bits on write. */
+	uint64_t dq_rt3_cdis                  : 1;  /**< DQ_RT3 ECC correction disable. */
+	uint64_t dq_rt2_flip                  : 2;  /**< DQ_RT2 flip syndrome bits on write. */
+	uint64_t dq_rt2_cdis                  : 1;  /**< DQ_RT2 ECC correction disable. */
+	uint64_t dq_rt1_flip                  : 2;  /**< DQ_RT1 flip syndrome bits on write. */
+	uint64_t dq_rt1_cdis                  : 1;  /**< DQ_RT1 ECC correction disable. */
+	uint64_t dq_rt0_flip                  : 2;  /**< DQ_RT0 flip syndrome bits on write. */
+	uint64_t dq_rt0_cdis                  : 1;  /**< DQ_RT0 ECC correction disable. */
+	uint64_t reserved_0_36                : 37;
 #else
-		uint64_t reserved_0_63:64;
+	uint64_t reserved_0_36                : 37;
+	uint64_t dq_rt0_cdis                  : 1;
+	uint64_t dq_rt0_flip                  : 2;
+	uint64_t dq_rt1_cdis                  : 1;
+	uint64_t dq_rt1_flip                  : 2;
+	uint64_t dq_rt2_cdis                  : 1;
+	uint64_t dq_rt2_flip                  : 2;
+	uint64_t dq_rt3_cdis                  : 1;
+	uint64_t dq_rt3_flip                  : 2;
+	uint64_t dq_rt4_cdis                  : 1;
+	uint64_t dq_rt4_flip                  : 2;
+	uint64_t dq_rt5_cdis                  : 1;
+	uint64_t dq_rt5_flip                  : 2;
+	uint64_t dq_rt6_cdis                  : 1;
+	uint64_t dq_rt6_flip                  : 2;
+	uint64_t dq_rt7_cdis                  : 1;
+	uint64_t dq_rt7_flip                  : 2;
+	uint64_t dq_wt_ram_cdis               : 1;
+	uint64_t dq_wt_ram_flip               : 2;
 #endif
 	} s;
-	struct cvmx_pko_mem_debug3_cn30xx {
+	struct cvmx_pko_pse_dq_ecc_ctl0_s     cn78xx;
+};
+typedef union cvmx_pko_pse_dq_ecc_ctl0 cvmx_pko_pse_dq_ecc_ctl0_t;
+
+/**
+ * cvmx_pko_pse_dq_ecc_dbe_sts0
+ */
+union cvmx_pko_pse_dq_ecc_dbe_sts0 {
+	uint64_t u64;
+	struct cvmx_pko_pse_dq_ecc_dbe_sts0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t i:1;
-		uint64_t back:4;
-		uint64_t pool:3;
-		uint64_t size:16;
-		uint64_t ptr:40;
+	uint64_t dq_wt_ram_dbe                : 1;  /**< Double-bit error for DQ_WT_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_dq.wt_sram */
+	uint64_t dq_rt7_dbe                   : 1;  /**< Double-bit error for DQ_RT7_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_dq.rt7 */
+	uint64_t dq_rt6_dbe                   : 1;  /**< Double-bit error for DQ_RT6_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_dq.rt6 */
+	uint64_t dq_rt5_dbe                   : 1;  /**< Double-bit error for DQ_RT5_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_dq.rt5 */
+	uint64_t dq_rt4_dbe                   : 1;  /**< Double-bit error for DQ_RT4_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_dq.rt4 */
+	uint64_t dq_rt3_dbe                   : 1;  /**< Double-bit error for DQ_RT3_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_dq.rt3 */
+	uint64_t dq_rt2_dbe                   : 1;  /**< Double-bit error for DQ_RT2_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_dq.rt2 */
+	uint64_t dq_rt1_dbe                   : 1;  /**< Double-bit error for DQ_RT1_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_dq.rt1 */
+	uint64_t dq_rt0_dbe                   : 1;  /**< Double-bit error for DQ_RT0_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_dq.rt0 */
+	uint64_t reserved_0_54                : 55;
 #else
-		uint64_t ptr:40;
-		uint64_t size:16;
-		uint64_t pool:3;
-		uint64_t back:4;
-		uint64_t i:1;
+	uint64_t reserved_0_54                : 55;
+	uint64_t dq_rt0_dbe                   : 1;
+	uint64_t dq_rt1_dbe                   : 1;
+	uint64_t dq_rt2_dbe                   : 1;
+	uint64_t dq_rt3_dbe                   : 1;
+	uint64_t dq_rt4_dbe                   : 1;
+	uint64_t dq_rt5_dbe                   : 1;
+	uint64_t dq_rt6_dbe                   : 1;
+	uint64_t dq_rt7_dbe                   : 1;
+	uint64_t dq_wt_ram_dbe                : 1;
 #endif
-	} cn30xx;
-	struct cvmx_pko_mem_debug3_cn30xx cn31xx;
-	struct cvmx_pko_mem_debug3_cn30xx cn38xx;
-	struct cvmx_pko_mem_debug3_cn30xx cn38xxp2;
-	struct cvmx_pko_mem_debug3_cn50xx {
+	} s;
+	struct cvmx_pko_pse_dq_ecc_dbe_sts0_s cn78xx;
+};
+typedef union cvmx_pko_pse_dq_ecc_dbe_sts0 cvmx_pko_pse_dq_ecc_dbe_sts0_t;
+
+/**
+ * cvmx_pko_pse_dq_ecc_dbe_sts_cmb0
+ */
+union cvmx_pko_pse_dq_ecc_dbe_sts_cmb0 {
+	uint64_t u64;
+	struct cvmx_pko_pse_dq_ecc_dbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t data:64;
+	uint64_t pse_dq_dbe_cmb0              : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_DQ_ECC_DBE_STS0.
+                                                         To clear this bit, software must clear bits in PKO_PSE_DQ_ECC_DBE_STS0.
+                                                         When this bit is set, the corresponding interrupt is set.
+                                                         Throws PKO_INTSN_E::PKO_PSE_DQ_DBE_CMB0.
+                                                         INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_dq.wt_sram
+                                                         pko_pnr2.pko_pse.pse_dq.rt0
+                                                         pko_pnr2.pko_pse.pse_dq.rt1
+                                                         pko_pnr2.pko_pse.pse_dq.rt2
+                                                         pko_pnr2.pko_pse.pse_dq.rt3
+                                                         pko_pnr2.pko_pse.pse_dq.rt4
+                                                         pko_pnr2.pko_pse.pse_dq.rt5
+                                                         pko_pnr2.pko_pse.pse_dq.rt6
+                                                         pko_pnr2.pko_pse.pse_dq.rt7 */
+	uint64_t reserved_0_62                : 63;
 #else
-		uint64_t data:64;
+	uint64_t reserved_0_62                : 63;
+	uint64_t pse_dq_dbe_cmb0              : 1;
 #endif
-	} cn50xx;
-	struct cvmx_pko_mem_debug3_cn50xx cn52xx;
-	struct cvmx_pko_mem_debug3_cn50xx cn52xxp1;
-	struct cvmx_pko_mem_debug3_cn50xx cn56xx;
-	struct cvmx_pko_mem_debug3_cn50xx cn56xxp1;
-	struct cvmx_pko_mem_debug3_cn50xx cn58xx;
-	struct cvmx_pko_mem_debug3_cn50xx cn58xxp1;
-	struct cvmx_pko_mem_debug3_cn50xx cn61xx;
-	struct cvmx_pko_mem_debug3_cn50xx cn63xx;
-	struct cvmx_pko_mem_debug3_cn50xx cn63xxp1;
-	struct cvmx_pko_mem_debug3_cn50xx cn66xx;
-	struct cvmx_pko_mem_debug3_cn50xx cn68xx;
-	struct cvmx_pko_mem_debug3_cn50xx cn68xxp1;
-	struct cvmx_pko_mem_debug3_cn50xx cnf71xx;
+	} s;
+	struct cvmx_pko_pse_dq_ecc_dbe_sts_cmb0_s cn78xx;
 };
+typedef union cvmx_pko_pse_dq_ecc_dbe_sts_cmb0 cvmx_pko_pse_dq_ecc_dbe_sts_cmb0_t;
 
-union cvmx_pko_mem_debug4 {
+/**
+ * cvmx_pko_pse_dq_ecc_sbe_sts0
+ */
+union cvmx_pko_pse_dq_ecc_sbe_sts0 {
 	uint64_t u64;
-	struct cvmx_pko_mem_debug4_s {
+	struct cvmx_pko_pse_dq_ecc_sbe_sts0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_0_63:64;
+	uint64_t dq_wt_ram_sbe                : 1;  /**< Single-bit error for DQ_WT_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_dq.wt_sram */
+	uint64_t dq_rt7_sbe                   : 1;  /**< Single-bit error for DQ_RT7_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_dq.rt7 */
+	uint64_t dq_rt6_sbe                   : 1;  /**< Single-bit error for DQ_RT6_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_dq.rt6 */
+	uint64_t dq_rt5_sbe                   : 1;  /**< Single-bit error for DQ_RT5_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_dq.rt5 */
+	uint64_t dq_rt4_sbe                   : 1;  /**< Single-bit error for DQ_RT4_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_dq.rt4 */
+	uint64_t dq_rt3_sbe                   : 1;  /**< Single-bit error for DQ_RT3_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_dq.rt3 */
+	uint64_t dq_rt2_sbe                   : 1;  /**< Single-bit error for DQ_RT2_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_dq.rt2 */
+	uint64_t dq_rt1_sbe                   : 1;  /**< Single-bit error for DQ_RT1_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_dq.rt1 */
+	uint64_t dq_rt0_sbe                   : 1;  /**< Single-bit error for DQ_RT0_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_dq.rt0 */
+	uint64_t reserved_0_54                : 55;
 #else
-		uint64_t reserved_0_63:64;
+	uint64_t reserved_0_54                : 55;
+	uint64_t dq_rt0_sbe                   : 1;
+	uint64_t dq_rt1_sbe                   : 1;
+	uint64_t dq_rt2_sbe                   : 1;
+	uint64_t dq_rt3_sbe                   : 1;
+	uint64_t dq_rt4_sbe                   : 1;
+	uint64_t dq_rt5_sbe                   : 1;
+	uint64_t dq_rt6_sbe                   : 1;
+	uint64_t dq_rt7_sbe                   : 1;
+	uint64_t dq_wt_ram_sbe                : 1;
 #endif
 	} s;
-	struct cvmx_pko_mem_debug4_cn30xx {
+	struct cvmx_pko_pse_dq_ecc_sbe_sts0_s cn78xx;
+};
+typedef union cvmx_pko_pse_dq_ecc_sbe_sts0 cvmx_pko_pse_dq_ecc_sbe_sts0_t;
+
+/**
+ * cvmx_pko_pse_dq_ecc_sbe_sts_cmb0
+ */
+union cvmx_pko_pse_dq_ecc_sbe_sts_cmb0 {
+	uint64_t u64;
+	struct cvmx_pko_pse_dq_ecc_sbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t data:64;
+	uint64_t pse_dq_sbe_cmb0              : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_DQ_ECC_SBE_STS0.
+                                                         To clear this bit, software must clear bits in PKO_PSE_DQ_ECC_SBE_STS0.
+                                                         When this bit is set, the corresponding interrupt is set.
+                                                         Throws PKO_INTSN_E::PKO_PSE_DQ_SBE_CMB0.
+                                                         INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_dq.wt_sram
+                                                         pko_pnr2.pko_pse.pse_dq.rt0
+                                                         pko_pnr2.pko_pse.pse_dq.rt1
+                                                         pko_pnr2.pko_pse.pse_dq.rt2
+                                                         pko_pnr2.pko_pse.pse_dq.rt3
+                                                         pko_pnr2.pko_pse.pse_dq.rt4
+                                                         pko_pnr2.pko_pse.pse_dq.rt5
+                                                         pko_pnr2.pko_pse.pse_dq.rt6
+                                                         pko_pnr2.pko_pse.pse_dq.rt7 */
+	uint64_t reserved_0_62                : 63;
 #else
-		uint64_t data:64;
+	uint64_t reserved_0_62                : 63;
+	uint64_t pse_dq_sbe_cmb0              : 1;
 #endif
-	} cn30xx;
-	struct cvmx_pko_mem_debug4_cn30xx cn31xx;
-	struct cvmx_pko_mem_debug4_cn30xx cn38xx;
-	struct cvmx_pko_mem_debug4_cn30xx cn38xxp2;
-	struct cvmx_pko_mem_debug4_cn50xx {
+	} s;
+	struct cvmx_pko_pse_dq_ecc_sbe_sts_cmb0_s cn78xx;
+};
+typedef union cvmx_pko_pse_dq_ecc_sbe_sts_cmb0 cvmx_pko_pse_dq_ecc_sbe_sts_cmb0_t;
+
+/**
+ * cvmx_pko_pse_pq_bist_status
+ *
+ * Each bit is the BIST result of an individual memory (per bit, 0 = pass and 1 = fail).
+ *
+ */
+union cvmx_pko_pse_pq_bist_status {
+	uint64_t u64;
+	struct cvmx_pko_pse_pq_bist_status_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t cmnd_segs:3;
-		uint64_t cmnd_siz:16;
-		uint64_t cmnd_off:6;
-		uint64_t uid:3;
-		uint64_t dread_sop:1;
-		uint64_t init_dwrite:1;
-		uint64_t chk_once:1;
-		uint64_t chk_mode:1;
-		uint64_t active:1;
-		uint64_t static_p:1;
-		uint64_t qos:3;
-		uint64_t qcb_ridx:5;
-		uint64_t qid_off_max:4;
-		uint64_t qid_off:4;
-		uint64_t qid_base:8;
-		uint64_t wait:1;
-		uint64_t minor:2;
-		uint64_t major:3;
-#else
-		uint64_t major:3;
-		uint64_t minor:2;
-		uint64_t wait:1;
-		uint64_t qid_base:8;
-		uint64_t qid_off:4;
-		uint64_t qid_off_max:4;
-		uint64_t qcb_ridx:5;
-		uint64_t qos:3;
-		uint64_t static_p:1;
-		uint64_t active:1;
-		uint64_t chk_mode:1;
-		uint64_t chk_once:1;
-		uint64_t init_dwrite:1;
-		uint64_t dread_sop:1;
-		uint64_t uid:3;
-		uint64_t cmnd_off:6;
-		uint64_t cmnd_siz:16;
-		uint64_t cmnd_segs:3;
+	uint64_t reserved_15_63               : 49;
+	uint64_t tp_sram                      : 1;  /**< Topology parent - pko_pse_pq_srf32x5e */
+	uint64_t irq_fifo_sram                : 1;  /**< Interrupt message FIFO - pko_pse_pq_srf1024x10e */
+	uint64_t wmd_sram                     : 1;  /**< Dynamic watermark state - pko_pse_wmd_srf1024x49e */
+	uint64_t wms_sram                     : 1;  /**< Static watermark configuration - pko_pse_wms_srf1024x50e */
+	uint64_t cxd_sram                     : 1;  /**< Dynamic channel state - pko_pse_cxd_srf32x31e */
+	uint64_t dqd_sram                     : 1;  /**< DQ dropped stats - pko_pse_stats_srf1024x88 */
+	uint64_t dqs_sram                     : 1;  /**< DQ sent stats - pko_pse_stats_srf1024x88 */
+	uint64_t pqd_sram                     : 1;  /**< PQ dropped stats - pko_pse_stats_srf32x88 */
+	uint64_t pqr_sram                     : 1;  /**< PQ read stats - pko_pse_stats_srf32x88 */
+	uint64_t pqy_sram                     : 1;  /**< PQ yellow stats - pko_pse_stats_srf32x88 */
+	uint64_t pqg_sram                     : 1;  /**< PQ green stats - pko_pse_stats_srf32x88 */
+	uint64_t std_sram                     : 1;  /**< Dynamic shaping state - pko_pse_std_srf32x105e */
+	uint64_t st_sram                      : 1;  /**< Static shaping configuration - pko_pse_sts_srf32x74e */
+	uint64_t reserved_1_1                 : 1;
+	uint64_t cxs_sram                     : 1;  /**< Static channel credit configuration - pko_pse_cx0_srf32x6e */
+#else
+	uint64_t cxs_sram                     : 1;
+	uint64_t reserved_1_1                 : 1;
+	uint64_t st_sram                      : 1;
+	uint64_t std_sram                     : 1;
+	uint64_t pqg_sram                     : 1;
+	uint64_t pqy_sram                     : 1;
+	uint64_t pqr_sram                     : 1;
+	uint64_t pqd_sram                     : 1;
+	uint64_t dqs_sram                     : 1;
+	uint64_t dqd_sram                     : 1;
+	uint64_t cxd_sram                     : 1;
+	uint64_t wms_sram                     : 1;
+	uint64_t wmd_sram                     : 1;
+	uint64_t irq_fifo_sram                : 1;
+	uint64_t tp_sram                      : 1;
+	uint64_t reserved_15_63               : 49;
 #endif
-	} cn50xx;
-	struct cvmx_pko_mem_debug4_cn52xx {
+	} s;
+	struct cvmx_pko_pse_pq_bist_status_s  cn78xx;
+};
+typedef union cvmx_pko_pse_pq_bist_status cvmx_pko_pse_pq_bist_status_t;
+
+/**
+ * cvmx_pko_pse_pq_ecc_ctl0
+ */
+union cvmx_pko_pse_pq_ecc_ctl0 {
+	uint64_t u64;
+	struct cvmx_pko_pse_pq_ecc_ctl0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t curr_siz:8;
-		uint64_t curr_off:16;
-		uint64_t cmnd_segs:6;
-		uint64_t cmnd_siz:16;
-		uint64_t cmnd_off:6;
-		uint64_t uid:2;
-		uint64_t dread_sop:1;
-		uint64_t init_dwrite:1;
-		uint64_t chk_once:1;
-		uint64_t chk_mode:1;
-		uint64_t wait:1;
-		uint64_t minor:2;
-		uint64_t major:3;
-#else
-		uint64_t major:3;
-		uint64_t minor:2;
-		uint64_t wait:1;
-		uint64_t chk_mode:1;
-		uint64_t chk_once:1;
-		uint64_t init_dwrite:1;
-		uint64_t dread_sop:1;
-		uint64_t uid:2;
-		uint64_t cmnd_off:6;
-		uint64_t cmnd_siz:16;
-		uint64_t cmnd_segs:6;
-		uint64_t curr_off:16;
-		uint64_t curr_siz:8;
+	uint64_t pq_cxs_ram_flip              : 2;  /**< PQ_CXS_RAM flip syndrome bits on write. */
+	uint64_t pq_cxs_ram_cdis              : 1;  /**< PQ_CXS_RAM ECC correction disable. */
+	uint64_t pq_cxd_ram_flip              : 2;  /**< PQ_CXD_RAM flip syndrome bits on write. */
+	uint64_t pq_cxd_ram_cdis              : 1;  /**< PQ_CXD_RAM ECC correction disable. */
+	uint64_t irq_fifo_sram_flip           : 2;  /**< IRQ_FIFO_SRAM flip syndrome bits on write. */
+	uint64_t irq_fifo_sram_cdis           : 1;  /**< IRQ_FIFO_SRAM ECC correction disable. */
+	uint64_t tp_sram_flip                 : 2;  /**< TP_SRAM flip syndrome bits on write. */
+	uint64_t tp_sram_cdis                 : 1;  /**< TP_SRAM ECC correction disable. */
+	uint64_t pq_std_ram_flip              : 2;  /**< PQ_STD_RAM flip syndrome bits on write. */
+	uint64_t pq_std_ram_cdis              : 1;  /**< PQ_STD_RAM ECC correction disable. */
+	uint64_t pq_st_ram_flip               : 2;  /**< PQ_ST_RAM flip syndrome bits on write. */
+	uint64_t pq_st_ram_cdis               : 1;  /**< PQ_ST_RAM ECC correction disable. */
+	uint64_t pq_wmd_ram_flip              : 2;  /**< PQ_WMD_RAM flip syndrome bits on write. */
+	uint64_t pq_wmd_ram_cdis              : 1;  /**< PQ_WMD_RAM ECC correction disable. */
+	uint64_t pq_wms_ram_flip              : 2;  /**< PQ_WMS_RAM flip syndrome bits on write. */
+	uint64_t pq_wms_ram_cdis              : 1;  /**< PQ_WMS_RAM ECC correction disable. */
+	uint64_t reserved_0_39                : 40;
+#else
+	uint64_t reserved_0_39                : 40;
+	uint64_t pq_wms_ram_cdis              : 1;
+	uint64_t pq_wms_ram_flip              : 2;
+	uint64_t pq_wmd_ram_cdis              : 1;
+	uint64_t pq_wmd_ram_flip              : 2;
+	uint64_t pq_st_ram_cdis               : 1;
+	uint64_t pq_st_ram_flip               : 2;
+	uint64_t pq_std_ram_cdis              : 1;
+	uint64_t pq_std_ram_flip              : 2;
+	uint64_t tp_sram_cdis                 : 1;
+	uint64_t tp_sram_flip                 : 2;
+	uint64_t irq_fifo_sram_cdis           : 1;
+	uint64_t irq_fifo_sram_flip           : 2;
+	uint64_t pq_cxd_ram_cdis              : 1;
+	uint64_t pq_cxd_ram_flip              : 2;
+	uint64_t pq_cxs_ram_cdis              : 1;
+	uint64_t pq_cxs_ram_flip              : 2;
 #endif
-	} cn52xx;
-	struct cvmx_pko_mem_debug4_cn52xx cn52xxp1;
-	struct cvmx_pko_mem_debug4_cn52xx cn56xx;
-	struct cvmx_pko_mem_debug4_cn52xx cn56xxp1;
-	struct cvmx_pko_mem_debug4_cn50xx cn58xx;
-	struct cvmx_pko_mem_debug4_cn50xx cn58xxp1;
-	struct cvmx_pko_mem_debug4_cn52xx cn61xx;
-	struct cvmx_pko_mem_debug4_cn52xx cn63xx;
-	struct cvmx_pko_mem_debug4_cn52xx cn63xxp1;
-	struct cvmx_pko_mem_debug4_cn52xx cn66xx;
-	struct cvmx_pko_mem_debug4_cn52xx cn68xx;
-	struct cvmx_pko_mem_debug4_cn52xx cn68xxp1;
-	struct cvmx_pko_mem_debug4_cn52xx cnf71xx;
+	} s;
+	struct cvmx_pko_pse_pq_ecc_ctl0_s     cn78xx;
 };
+typedef union cvmx_pko_pse_pq_ecc_ctl0 cvmx_pko_pse_pq_ecc_ctl0_t;
 
-union cvmx_pko_mem_debug5 {
+/**
+ * cvmx_pko_pse_pq_ecc_dbe_sts0
+ */
+union cvmx_pko_pse_pq_ecc_dbe_sts0 {
 	uint64_t u64;
-	struct cvmx_pko_mem_debug5_s {
+	struct cvmx_pko_pse_pq_ecc_dbe_sts0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_0_63:64;
+	uint64_t pq_cxs_ram_dbe               : 1;  /**< Double-bit error for PQ_CXS_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.pq.cxs_sram */
+	uint64_t pq_cxd_ram_dbe               : 1;  /**< Double-bit error for PQ_CXD_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.pq.cxd_sram */
+	uint64_t irq_fifo_sram_dbe            : 1;  /**< Double-bit error for IRQ_FIFO_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.pq.irq_fifo_sram */
+	uint64_t tp_sram_dbe                  : 1;  /**< Double-bit error for TP_SRAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq2_pq.pq.tp_sram */
+	uint64_t pq_std_ram_dbe               : 1;  /**< Double-bit error for PQ_STD_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.pq.std_sram */
+	uint64_t pq_st_ram_dbe                : 1;  /**< Double-bit error for PQ_ST_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.pq.st_sram */
+	uint64_t pq_wmd_ram_dbe               : 1;  /**< Double-bit error for PQ_WMD_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.pq.wmd_sram */
+	uint64_t pq_wms_ram_dbe               : 1;  /**< Double-bit error for PQ_WMS_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.pq.wms_sram */
+	uint64_t reserved_0_55                : 56;
 #else
-		uint64_t reserved_0_63:64;
+	uint64_t reserved_0_55                : 56;
+	uint64_t pq_wms_ram_dbe               : 1;
+	uint64_t pq_wmd_ram_dbe               : 1;
+	uint64_t pq_st_ram_dbe                : 1;
+	uint64_t pq_std_ram_dbe               : 1;
+	uint64_t tp_sram_dbe                  : 1;
+	uint64_t irq_fifo_sram_dbe            : 1;
+	uint64_t pq_cxd_ram_dbe               : 1;
+	uint64_t pq_cxs_ram_dbe               : 1;
 #endif
 	} s;
-	struct cvmx_pko_mem_debug5_cn30xx {
+	struct cvmx_pko_pse_pq_ecc_dbe_sts0_s cn78xx;
+};
+typedef union cvmx_pko_pse_pq_ecc_dbe_sts0 cvmx_pko_pse_pq_ecc_dbe_sts0_t;
+
+/**
+ * cvmx_pko_pse_pq_ecc_dbe_sts_cmb0
+ */
+union cvmx_pko_pse_pq_ecc_dbe_sts_cmb0 {
+	uint64_t u64;
+	struct cvmx_pko_pse_pq_ecc_dbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t dwri_mod:1;
-		uint64_t dwri_sop:1;
-		uint64_t dwri_len:1;
-		uint64_t dwri_cnt:13;
-		uint64_t cmnd_siz:16;
-		uint64_t uid:1;
-		uint64_t xfer_wor:1;
-		uint64_t xfer_dwr:1;
-		uint64_t cbuf_fre:1;
-		uint64_t reserved_27_27:1;
-		uint64_t chk_mode:1;
-		uint64_t active:1;
-		uint64_t qos:3;
-		uint64_t qcb_ridx:5;
-		uint64_t qid_off:3;
-		uint64_t qid_base:7;
-		uint64_t wait:1;
-		uint64_t minor:2;
-		uint64_t major:4;
-#else
-		uint64_t major:4;
-		uint64_t minor:2;
-		uint64_t wait:1;
-		uint64_t qid_base:7;
-		uint64_t qid_off:3;
-		uint64_t qcb_ridx:5;
-		uint64_t qos:3;
-		uint64_t active:1;
-		uint64_t chk_mode:1;
-		uint64_t reserved_27_27:1;
-		uint64_t cbuf_fre:1;
-		uint64_t xfer_dwr:1;
-		uint64_t xfer_wor:1;
-		uint64_t uid:1;
-		uint64_t cmnd_siz:16;
-		uint64_t dwri_cnt:13;
-		uint64_t dwri_len:1;
-		uint64_t dwri_sop:1;
-		uint64_t dwri_mod:1;
+	uint64_t pse_pq_dbe_cmb0              : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_PQ_ECC_DBE_STS0.
+                                                         To clear this bit, software must clear bits in PKO_PSE_PQ_ECC_DBE_STS0.
+                                                         When this bit is set, the corresponding interrupt is set.
+                                                         Throws PKO_INTSN_E::PKO_PSE_PQ_DBE_CMB0.
+                                                         INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.pq.cxs_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.pq.cxd_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.pq.irq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.pq.tp_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.pq.std_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.pq.st_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.pq.wmd_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.pq.wms_sram */
+	uint64_t reserved_0_62                : 63;
+#else
+	uint64_t reserved_0_62                : 63;
+	uint64_t pse_pq_dbe_cmb0              : 1;
 #endif
-	} cn30xx;
-	struct cvmx_pko_mem_debug5_cn30xx cn31xx;
-	struct cvmx_pko_mem_debug5_cn30xx cn38xx;
-	struct cvmx_pko_mem_debug5_cn30xx cn38xxp2;
-	struct cvmx_pko_mem_debug5_cn50xx {
+	} s;
+	struct cvmx_pko_pse_pq_ecc_dbe_sts_cmb0_s cn78xx;
+};
+typedef union cvmx_pko_pse_pq_ecc_dbe_sts_cmb0 cvmx_pko_pse_pq_ecc_dbe_sts_cmb0_t;
+
+/**
+ * cvmx_pko_pse_pq_ecc_sbe_sts0
+ */
+union cvmx_pko_pse_pq_ecc_sbe_sts0 {
+	uint64_t u64;
+	struct cvmx_pko_pse_pq_ecc_sbe_sts0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t curr_ptr:29;
-		uint64_t curr_siz:16;
-		uint64_t curr_off:16;
-		uint64_t cmnd_segs:3;
+	uint64_t pq_cxs_ram_sbe               : 1;  /**< Single-bit error for PQ_CXS_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.pq.cxs_sram */
+	uint64_t pq_cxd_ram_sbe               : 1;  /**< Single-bit error for PQ_CXD_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.pq.cxd_sram */
+	uint64_t irq_fifo_sram_sbe            : 1;  /**< Single-bit error for IRQ_FIFO_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.pq.irq_fifo_sram */
+	uint64_t tp_sram_sbe                  : 1;  /**< Single-bit error for TP_SRAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq2_pq.pq.tp_sram */
+	uint64_t pq_std_ram_sbe               : 1;  /**< Single-bit error for PQ_STD_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.pq.std_sram */
+	uint64_t pq_st_ram_sbe                : 1;  /**< Single-bit error for PQ_ST_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.pq.st_sram */
+	uint64_t pq_wmd_ram_sbe               : 1;  /**< Single-bit error for PQ_WMD_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.pq.wmd_sram */
+	uint64_t pq_wms_ram_sbe               : 1;  /**< Single-bit error for PQ_WMS_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.pq.wms_sram */
+	uint64_t reserved_0_55                : 56;
 #else
-		uint64_t cmnd_segs:3;
-		uint64_t curr_off:16;
-		uint64_t curr_siz:16;
-		uint64_t curr_ptr:29;
+	uint64_t reserved_0_55                : 56;
+	uint64_t pq_wms_ram_sbe               : 1;
+	uint64_t pq_wmd_ram_sbe               : 1;
+	uint64_t pq_st_ram_sbe                : 1;
+	uint64_t pq_std_ram_sbe               : 1;
+	uint64_t tp_sram_sbe                  : 1;
+	uint64_t irq_fifo_sram_sbe            : 1;
+	uint64_t pq_cxd_ram_sbe               : 1;
+	uint64_t pq_cxs_ram_sbe               : 1;
 #endif
-	} cn50xx;
-	struct cvmx_pko_mem_debug5_cn52xx {
+	} s;
+	struct cvmx_pko_pse_pq_ecc_sbe_sts0_s cn78xx;
+};
+typedef union cvmx_pko_pse_pq_ecc_sbe_sts0 cvmx_pko_pse_pq_ecc_sbe_sts0_t;
+
+/**
+ * cvmx_pko_pse_pq_ecc_sbe_sts_cmb0
+ */
+union cvmx_pko_pse_pq_ecc_sbe_sts_cmb0 {
+	uint64_t u64;
+	struct cvmx_pko_pse_pq_ecc_sbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_54_63:10;
-		uint64_t nxt_inflt:6;
-		uint64_t curr_ptr:40;
-		uint64_t curr_siz:8;
+	uint64_t pse_pq_sbe_cmb0              : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_PQ_ECC_SBE_STS0.
+                                                         To clear this bit, software must clear bits in PKO_PSE_PQ_ECC_SBE_STS0.
+                                                         When this bit is set, the corresponding interrupt is set.
+                                                         Throws PKO_INTSN_E::PKO_PSE_PQ_SBE_CMB0.
+                                                         INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.pq.cxs_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.pq.cxd_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.pq.irq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.pq.tp_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.pq.std_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.pq.st_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.pq.wmd_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.pq.wms_sram */
+	uint64_t reserved_0_62                : 63;
 #else
-		uint64_t curr_siz:8;
-		uint64_t curr_ptr:40;
-		uint64_t nxt_inflt:6;
-		uint64_t reserved_54_63:10;
+	uint64_t reserved_0_62                : 63;
+	uint64_t pse_pq_sbe_cmb0              : 1;
 #endif
-	} cn52xx;
-	struct cvmx_pko_mem_debug5_cn52xx cn52xxp1;
-	struct cvmx_pko_mem_debug5_cn52xx cn56xx;
-	struct cvmx_pko_mem_debug5_cn52xx cn56xxp1;
-	struct cvmx_pko_mem_debug5_cn50xx cn58xx;
-	struct cvmx_pko_mem_debug5_cn50xx cn58xxp1;
-	struct cvmx_pko_mem_debug5_cn61xx {
+	} s;
+	struct cvmx_pko_pse_pq_ecc_sbe_sts_cmb0_s cn78xx;
+};
+typedef union cvmx_pko_pse_pq_ecc_sbe_sts_cmb0 cvmx_pko_pse_pq_ecc_sbe_sts_cmb0_t;
+
+/**
+ * cvmx_pko_pse_sq1_bist_status
+ *
+ * Each bit is the BIST result of an individual memory (per bit, 0 = pass and 1 = fail).
+ *
+ */
+union cvmx_pko_pse_sq1_bist_status {
+	uint64_t u64;
+	struct cvmx_pko_pse_sq1_bist_status_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_56_63:8;
-		uint64_t ptp:1;
-		uint64_t major_3:1;
-		uint64_t nxt_inflt:6;
-		uint64_t curr_ptr:40;
-		uint64_t curr_siz:8;
+	uint64_t reserved_29_63               : 35;
+	uint64_t sc_sram                      : 1;  /**< SQ[5:1] scheduling configuration */
+	uint64_t pc_sram                      : 1;  /**< SQ[1] physical channel - pko_pse_pc_srf32x12e */
+	uint64_t xon_sram                     : 1;  /**< XON SRAM */
+	uint64_t cc_sram                      : 1;  /**< SQ[1] channel credit OK state array */
+	uint64_t vc1_sram                     : 1;  /**< SQ[1] virtual channel - pko_pse_sq1_vc_srf256x9e */
+	uint64_t vc0_sram                     : 1;  /**< SQ[1] virtual channel - pko_pse_sq1_vc_srf256x9e */
+	uint64_t reserved_21_22               : 2;
+	uint64_t tp1_sram                     : 1;  /**< SQ[5:1] topology parent configuration */
+	uint64_t tp0_sram                     : 1;  /**< SQ[5:1] topology parent configuration */
+	uint64_t xo_sram                      : 1;  /**< XOFF SRAM */
+	uint64_t rt_sram                      : 1;  /**< Result table */
+	uint64_t reserved_9_16                : 8;
+	uint64_t tw1_cmd_fifo                 : 1;  /**< SQ[5:1] time wheel 1 command FIFO SRAM */
+	uint64_t std_sram                     : 1;  /**< Dynamic shaping state */
+	uint64_t sts_sram                     : 1;  /**< Static shaping configuration */
+	uint64_t tw0_cmd_fifo                 : 1;  /**< SQ[5:1] time wheel 0 command FIFO SRAM */
+	uint64_t cxd_sram                     : 1;  /**< SQ[1] dynamic channel credit state */
+	uint64_t cxs_sram                     : 1;  /**< SQ[1] static channel credit configuration */
+	uint64_t nt_sram                      : 1;  /**< SQ[5:1] next pointer table */
+	uint64_t pt_sram                      : 1;  /**< SQ[5:1] previous pointer table */
+	uint64_t wt_sram                      : 1;  /**< SQ[5:1] work table */
 #else
-		uint64_t curr_siz:8;
-		uint64_t curr_ptr:40;
-		uint64_t nxt_inflt:6;
-		uint64_t major_3:1;
-		uint64_t ptp:1;
-		uint64_t reserved_56_63:8;
+	uint64_t wt_sram                      : 1;
+	uint64_t pt_sram                      : 1;
+	uint64_t nt_sram                      : 1;
+	uint64_t cxs_sram                     : 1;
+	uint64_t cxd_sram                     : 1;
+	uint64_t tw0_cmd_fifo                 : 1;
+	uint64_t sts_sram                     : 1;
+	uint64_t std_sram                     : 1;
+	uint64_t tw1_cmd_fifo                 : 1;
+	uint64_t reserved_9_16                : 8;
+	uint64_t rt_sram                      : 1;
+	uint64_t xo_sram                      : 1;
+	uint64_t tp0_sram                     : 1;
+	uint64_t tp1_sram                     : 1;
+	uint64_t reserved_21_22               : 2;
+	uint64_t vc0_sram                     : 1;
+	uint64_t vc1_sram                     : 1;
+	uint64_t cc_sram                      : 1;
+	uint64_t xon_sram                     : 1;
+	uint64_t pc_sram                      : 1;
+	uint64_t sc_sram                      : 1;
+	uint64_t reserved_29_63               : 35;
 #endif
-	} cn61xx;
-	struct cvmx_pko_mem_debug5_cn61xx cn63xx;
-	struct cvmx_pko_mem_debug5_cn61xx cn63xxp1;
-	struct cvmx_pko_mem_debug5_cn61xx cn66xx;
-	struct cvmx_pko_mem_debug5_cn68xx {
+	} s;
+	struct cvmx_pko_pse_sq1_bist_status_s cn78xx;
+};
+typedef union cvmx_pko_pse_sq1_bist_status cvmx_pko_pse_sq1_bist_status_t;
+
+/**
+ * cvmx_pko_pse_sq1_ecc_ctl0
+ */
+union cvmx_pko_pse_sq1_ecc_ctl0 {
+	uint64_t u64;
+	struct cvmx_pko_pse_sq1_ecc_ctl0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_57_63:7;
-		uint64_t uid_2:1;
-		uint64_t ptp:1;
-		uint64_t major_3:1;
-		uint64_t nxt_inflt:6;
-		uint64_t curr_ptr:40;
-		uint64_t curr_siz:8;
-#else
-		uint64_t curr_siz:8;
-		uint64_t curr_ptr:40;
-		uint64_t nxt_inflt:6;
-		uint64_t major_3:1;
-		uint64_t ptp:1;
-		uint64_t uid_2:1;
-		uint64_t reserved_57_63:7;
+	uint64_t cxs_ram_flip                 : 2;  /**< CXS_RAM flip syndrome bits on write. */
+	uint64_t cxs_ram_cdis                 : 1;  /**< CXS_RAM ECC correction disable. */
+	uint64_t cxd_ram_flip                 : 2;  /**< CXD_RAM flip syndrome bits on write. */
+	uint64_t cxd_ram_cdis                 : 1;  /**< CXD_RAM ECC correction disable. */
+	uint64_t vc1_sram_flip                : 2;  /**< VC1_SRAM flip syndrome bits on write. */
+	uint64_t vc1_sram_cdis                : 1;  /**< VC1_SRAM ECC correction disable. */
+	uint64_t vc0_sram_flip                : 2;  /**< VC0_SRAM flip syndrome bits on write. */
+	uint64_t vc0_sram_cdis                : 1;  /**< VC0_SRAM ECC correction disable. */
+	uint64_t sq_pt_ram_flip               : 2;  /**< SQ_PT_RAM flip syndrome bits on write. */
+	uint64_t sq_pt_ram_cdis               : 1;  /**< SQ_PT_RAM ECC correction disable. */
+	uint64_t sq_nt_ram_flip               : 2;  /**< SQ_NT_RAM flip syndrome bits on write. */
+	uint64_t sq_nt_ram_cdis               : 1;  /**< SQ_NT_RAM ECC correction disable. */
+	uint64_t rt_ram_flip                  : 2;  /**< RT_RAM flip syndrome bits on write. */
+	uint64_t rt_ram_cdis                  : 1;  /**< RT_RAM ECC correction disable. */
+	uint64_t pc_ram_flip                  : 2;  /**< PC_RAM flip syndrome bits on write. */
+	uint64_t pc_ram_cdis                  : 1;  /**< PC_RAM ECC correction disable. */
+	uint64_t tw1_cmd_fifo_ram_flip        : 2;  /**< TW1_CMD_FIFO_RAM flip syndrome bits on write. */
+	uint64_t tw1_cmd_fifo_ram_cdis        : 1;  /**< TW1_CMD_FIFO_RAM ECC correction disable. */
+	uint64_t tw0_cmd_fifo_ram_flip        : 2;  /**< TW0_CMD_FIFO_RAM flip syndrome bits on write. */
+	uint64_t tw0_cmd_fifo_ram_cdis        : 1;  /**< TW0_CMD_FIFO_RAM ECC correction disable. */
+	uint64_t tp1_sram_flip                : 2;  /**< TP1_SRAM flip syndrome bits on write. */
+	uint64_t tp1_sram_cdis                : 1;  /**< TP1_SRAM ECC correction disable. */
+	uint64_t tp0_sram_flip                : 2;  /**< TP0_SRAM flip syndrome bits on write. */
+	uint64_t tp0_sram_cdis                : 1;  /**< TP0_SRAM ECC correction disable. */
+	uint64_t sts1_ram_flip                : 2;  /**< STS1_RAM flip syndrome bits on write. */
+	uint64_t sts1_ram_cdis                : 1;  /**< STS1_RAM ECC correction disable. */
+	uint64_t sts0_ram_flip                : 2;  /**< STS0_RAM flip syndrome bits on write. */
+	uint64_t sts0_ram_cdis                : 1;  /**< STS0_RAM ECC correction disable. */
+	uint64_t std1_ram_flip                : 2;  /**< STD1_RAM flip syndrome bits on write. */
+	uint64_t std1_ram_cdis                : 1;  /**< STD1_RAM ECC correction disable. */
+	uint64_t std0_ram_flip                : 2;  /**< STD0_RAM flip syndrome bits on write. */
+	uint64_t std0_ram_cdis                : 1;  /**< STD0_RAM ECC correction disable. */
+	uint64_t wt_ram_flip                  : 2;  /**< WT_RAM flip syndrome bits on write. */
+	uint64_t wt_ram_cdis                  : 1;  /**< WT_RAM ECC correction disable. */
+	uint64_t sc_ram_flip                  : 2;  /**< SC_RAM flip syndrome bits on write. */
+	uint64_t sc_ram_cdis                  : 1;  /**< SC_RAM ECC correction disable. */
+	uint64_t reserved_0_9                 : 10;
+#else
+	uint64_t reserved_0_9                 : 10;
+	uint64_t sc_ram_cdis                  : 1;
+	uint64_t sc_ram_flip                  : 2;
+	uint64_t wt_ram_cdis                  : 1;
+	uint64_t wt_ram_flip                  : 2;
+	uint64_t std0_ram_cdis                : 1;
+	uint64_t std0_ram_flip                : 2;
+	uint64_t std1_ram_cdis                : 1;
+	uint64_t std1_ram_flip                : 2;
+	uint64_t sts0_ram_cdis                : 1;
+	uint64_t sts0_ram_flip                : 2;
+	uint64_t sts1_ram_cdis                : 1;
+	uint64_t sts1_ram_flip                : 2;
+	uint64_t tp0_sram_cdis                : 1;
+	uint64_t tp0_sram_flip                : 2;
+	uint64_t tp1_sram_cdis                : 1;
+	uint64_t tp1_sram_flip                : 2;
+	uint64_t tw0_cmd_fifo_ram_cdis        : 1;
+	uint64_t tw0_cmd_fifo_ram_flip        : 2;
+	uint64_t tw1_cmd_fifo_ram_cdis        : 1;
+	uint64_t tw1_cmd_fifo_ram_flip        : 2;
+	uint64_t pc_ram_cdis                  : 1;
+	uint64_t pc_ram_flip                  : 2;
+	uint64_t rt_ram_cdis                  : 1;
+	uint64_t rt_ram_flip                  : 2;
+	uint64_t sq_nt_ram_cdis               : 1;
+	uint64_t sq_nt_ram_flip               : 2;
+	uint64_t sq_pt_ram_cdis               : 1;
+	uint64_t sq_pt_ram_flip               : 2;
+	uint64_t vc0_sram_cdis                : 1;
+	uint64_t vc0_sram_flip                : 2;
+	uint64_t vc1_sram_cdis                : 1;
+	uint64_t vc1_sram_flip                : 2;
+	uint64_t cxd_ram_cdis                 : 1;
+	uint64_t cxd_ram_flip                 : 2;
+	uint64_t cxs_ram_cdis                 : 1;
+	uint64_t cxs_ram_flip                 : 2;
 #endif
-	} cn68xx;
-	struct cvmx_pko_mem_debug5_cn68xx cn68xxp1;
-	struct cvmx_pko_mem_debug5_cn61xx cnf71xx;
+	} s;
+	struct cvmx_pko_pse_sq1_ecc_ctl0_s    cn78xx;
 };
+typedef union cvmx_pko_pse_sq1_ecc_ctl0 cvmx_pko_pse_sq1_ecc_ctl0_t;
 
-union cvmx_pko_mem_debug6 {
+/**
+ * cvmx_pko_pse_sq1_ecc_dbe_sts0
+ */
+union cvmx_pko_pse_sq1_ecc_dbe_sts0 {
 	uint64_t u64;
-	struct cvmx_pko_mem_debug6_s {
+	struct cvmx_pko_pse_sq1_ecc_dbe_sts0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_37_63:27;
-		uint64_t qid_offres:4;
-		uint64_t qid_offths:4;
-		uint64_t preempter:1;
-		uint64_t preemptee:1;
-		uint64_t preempted:1;
-		uint64_t active:1;
-		uint64_t statc:1;
-		uint64_t qos:3;
-		uint64_t qcb_ridx:5;
-		uint64_t qid_offmax:4;
-		uint64_t reserved_0_11:12;
-#else
-		uint64_t reserved_0_11:12;
-		uint64_t qid_offmax:4;
-		uint64_t qcb_ridx:5;
-		uint64_t qos:3;
-		uint64_t statc:1;
-		uint64_t active:1;
-		uint64_t preempted:1;
-		uint64_t preemptee:1;
-		uint64_t preempter:1;
-		uint64_t qid_offths:4;
-		uint64_t qid_offres:4;
-		uint64_t reserved_37_63:27;
+	uint64_t cxs_ram_dbe                  : 1;  /**< Double-bit error for CXS_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.cxs_sram */
+	uint64_t cxd_ram_dbe                  : 1;  /**< Double-bit error for CXD_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.cxd_sram */
+	uint64_t vc1_sram_dbe                 : 1;  /**< Double-bit error for VC1_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.vc1_sram */
+	uint64_t vc0_sram_dbe                 : 1;  /**< Double-bit error for VC0_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.vc0_sram */
+	uint64_t sq_pt_ram_dbe                : 1;  /**< Double-bit error for SQ_PT_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.nt_sram */
+	uint64_t sq_nt_ram_dbe                : 1;  /**< Double-bit error for SQ_NT_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.pt_sram */
+	uint64_t rt_ram_dbe                   : 1;  /**< Double-bit error for RT_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq2_pq.sq1.rt_sram */
+	uint64_t pc_ram_dbe                   : 1;  /**< Double-bit error for PC_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq2_pq.sq1.pc_sram */
+	uint64_t tw1_cmd_fifo_ram_dbe         : 1;  /**< Double-bit error for TW1_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.tw_0.sq_fifo_sram */
+	uint64_t tw0_cmd_fifo_ram_dbe         : 1;  /**< Double-bit error for TW0_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.tw_1.sq_fifo_sram */
+	uint64_t tp1_sram_dbe                 : 1;  /**< Double-bit error for TP1_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.tp0_sram */
+	uint64_t tp0_sram_dbe                 : 1;  /**< Double-bit error for TP0_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.tp1_sram */
+	uint64_t sts1_ram_dbe                 : 1;  /**< Double-bit error for STS1_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.sts1_sram */
+	uint64_t sts0_ram_dbe                 : 1;  /**< Double-bit error for STS0_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.sts0_sram */
+	uint64_t std1_ram_dbe                 : 1;  /**< Double-bit error for STD1_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.std1_sram */
+	uint64_t std0_ram_dbe                 : 1;  /**< Double-bit error for STD0_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.std0_sram */
+	uint64_t wt_ram_dbe                   : 1;  /**< Double-bit error for WT_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq2_pq.sq1.wt_sram */
+	uint64_t sc_ram_dbe                   : 1;  /**< Double-bit error for SC_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq2_pq.sq1.sc_sram */
+	uint64_t reserved_0_45                : 46;
+#else
+	uint64_t reserved_0_45                : 46;
+	uint64_t sc_ram_dbe                   : 1;
+	uint64_t wt_ram_dbe                   : 1;
+	uint64_t std0_ram_dbe                 : 1;
+	uint64_t std1_ram_dbe                 : 1;
+	uint64_t sts0_ram_dbe                 : 1;
+	uint64_t sts1_ram_dbe                 : 1;
+	uint64_t tp0_sram_dbe                 : 1;
+	uint64_t tp1_sram_dbe                 : 1;
+	uint64_t tw0_cmd_fifo_ram_dbe         : 1;
+	uint64_t tw1_cmd_fifo_ram_dbe         : 1;
+	uint64_t pc_ram_dbe                   : 1;
+	uint64_t rt_ram_dbe                   : 1;
+	uint64_t sq_nt_ram_dbe                : 1;
+	uint64_t sq_pt_ram_dbe                : 1;
+	uint64_t vc0_sram_dbe                 : 1;
+	uint64_t vc1_sram_dbe                 : 1;
+	uint64_t cxd_ram_dbe                  : 1;
+	uint64_t cxs_ram_dbe                  : 1;
 #endif
 	} s;
-	struct cvmx_pko_mem_debug6_cn30xx {
-#ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_11_63:53;
-		uint64_t qid_offm:3;
-		uint64_t static_p:1;
-		uint64_t work_min:3;
-		uint64_t dwri_chk:1;
-		uint64_t dwri_uid:1;
-		uint64_t dwri_mod:2;
-#else
-		uint64_t dwri_mod:2;
-		uint64_t dwri_uid:1;
-		uint64_t dwri_chk:1;
-		uint64_t work_min:3;
-		uint64_t static_p:1;
-		uint64_t qid_offm:3;
-		uint64_t reserved_11_63:53;
-#endif
-	} cn30xx;
-	struct cvmx_pko_mem_debug6_cn30xx cn31xx;
-	struct cvmx_pko_mem_debug6_cn30xx cn38xx;
-	struct cvmx_pko_mem_debug6_cn30xx cn38xxp2;
-	struct cvmx_pko_mem_debug6_cn50xx {
+	struct cvmx_pko_pse_sq1_ecc_dbe_sts0_s cn78xx;
+};
+typedef union cvmx_pko_pse_sq1_ecc_dbe_sts0 cvmx_pko_pse_sq1_ecc_dbe_sts0_t;
+
+/**
+ * cvmx_pko_pse_sq1_ecc_dbe_sts_cmb0
+ */
+union cvmx_pko_pse_sq1_ecc_dbe_sts_cmb0 {
+	uint64_t u64;
+	struct cvmx_pko_pse_sq1_ecc_dbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_11_63:53;
-		uint64_t curr_ptr:11;
+	uint64_t pse_sq1_dbe_cmb0             : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_SQ1_ECC_DBE_STS0.
+                                                         To clear this bit, software must clear bits in PKO_PSE_SQ1_ECC_DBE_STS0.
+                                                         When this bit is set, the corresponding interrupt is set.
+                                                         Throws PKO_INTSN_E::PKO_PSE_SQ1_DBE_CMB0.
+                                                         INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.cxs_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.cxd_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.vc0_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.vc1_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.nt_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.pt_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.pc_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.rt_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.tw_0.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.tw_1.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.tp0_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.tp1_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.std0_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.std1_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.sts0_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.sts1_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.wt_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.sc_sram */
+	uint64_t reserved_0_62                : 63;
 #else
-		uint64_t curr_ptr:11;
-		uint64_t reserved_11_63:53;
+	uint64_t reserved_0_62                : 63;
+	uint64_t pse_sq1_dbe_cmb0             : 1;
 #endif
-	} cn50xx;
-	struct cvmx_pko_mem_debug6_cn52xx {
+	} s;
+	struct cvmx_pko_pse_sq1_ecc_dbe_sts_cmb0_s cn78xx;
+};
+typedef union cvmx_pko_pse_sq1_ecc_dbe_sts_cmb0 cvmx_pko_pse_sq1_ecc_dbe_sts_cmb0_t;
+
+/**
+ * cvmx_pko_pse_sq1_ecc_sbe_sts0
+ */
+union cvmx_pko_pse_sq1_ecc_sbe_sts0 {
+	uint64_t u64;
+	struct cvmx_pko_pse_sq1_ecc_sbe_sts0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_37_63:27;
-		uint64_t qid_offres:4;
-		uint64_t qid_offths:4;
-		uint64_t preempter:1;
-		uint64_t preemptee:1;
-		uint64_t preempted:1;
-		uint64_t active:1;
-		uint64_t statc:1;
-		uint64_t qos:3;
-		uint64_t qcb_ridx:5;
-		uint64_t qid_offmax:4;
-		uint64_t qid_off:4;
-		uint64_t qid_base:8;
-#else
-		uint64_t qid_base:8;
-		uint64_t qid_off:4;
-		uint64_t qid_offmax:4;
-		uint64_t qcb_ridx:5;
-		uint64_t qos:3;
-		uint64_t statc:1;
-		uint64_t active:1;
-		uint64_t preempted:1;
-		uint64_t preemptee:1;
-		uint64_t preempter:1;
-		uint64_t qid_offths:4;
-		uint64_t qid_offres:4;
-		uint64_t reserved_37_63:27;
+	uint64_t cxs_ram_sbe                  : 1;  /**< Single-bit error for CXS_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.cxs_sram */
+	uint64_t cxd_ram_sbe                  : 1;  /**< Single-bit error for CXD_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.cxd_sram */
+	uint64_t vc1_sram_sbe                 : 1;  /**< Single-bit error for VC1_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.vc0_sram */
+	uint64_t vc0_sram_sbe                 : 1;  /**< Single-bit error for VC0_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.vc1_sram */
+	uint64_t sq_pt_ram_sbe                : 1;  /**< Single-bit error for SQ_PT_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.nt_sram */
+	uint64_t sq_nt_ram_sbe                : 1;  /**< Single-bit error for SQ_NT_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.pt_sram */
+	uint64_t rt_ram_sbe                   : 1;  /**< Single-bit error for RT_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq2_pq.sq1.rt_sram */
+	uint64_t pc_ram_sbe                   : 1;  /**< Single-bit error for PC_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq2_pq.sq1.pc_sram */
+	uint64_t tw1_cmd_fifo_ram_sbe         : 1;  /**< Single-bit error for TW1_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.tw_0.sq_fifo_sram */
+	uint64_t tw0_cmd_fifo_ram_sbe         : 1;  /**< Single-bit error for TW0_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.tw_1.sq_fifo_sram */
+	uint64_t tp1_sram_sbe                 : 1;  /**< Single-bit error for TP1_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.tp0_sram */
+	uint64_t tp0_sram_sbe                 : 1;  /**< Single-bit error for TP0_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.tp1_sram */
+	uint64_t sts1_ram_sbe                 : 1;  /**< Single-bit error for STS1_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.sts0_sram */
+	uint64_t sts0_ram_sbe                 : 1;  /**< Single-bit error for STS0_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.sts1_sram */
+	uint64_t std1_ram_sbe                 : 1;  /**< Single-bit error for STD1_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.std0_sram */
+	uint64_t std0_ram_sbe                 : 1;  /**< Single-bit error for STD0_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.std1_sram */
+	uint64_t wt_ram_sbe                   : 1;  /**< Single-bit error for WT_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq2_pq.sq1.wt_sram */
+	uint64_t sc_ram_sbe                   : 1;  /**< Single-bit error for SC_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq2_pq.sq1.sc_sram */
+	uint64_t reserved_0_45                : 46;
+#else
+	uint64_t reserved_0_45                : 46;
+	uint64_t sc_ram_sbe                   : 1;
+	uint64_t wt_ram_sbe                   : 1;
+	uint64_t std0_ram_sbe                 : 1;
+	uint64_t std1_ram_sbe                 : 1;
+	uint64_t sts0_ram_sbe                 : 1;
+	uint64_t sts1_ram_sbe                 : 1;
+	uint64_t tp0_sram_sbe                 : 1;
+	uint64_t tp1_sram_sbe                 : 1;
+	uint64_t tw0_cmd_fifo_ram_sbe         : 1;
+	uint64_t tw1_cmd_fifo_ram_sbe         : 1;
+	uint64_t pc_ram_sbe                   : 1;
+	uint64_t rt_ram_sbe                   : 1;
+	uint64_t sq_nt_ram_sbe                : 1;
+	uint64_t sq_pt_ram_sbe                : 1;
+	uint64_t vc0_sram_sbe                 : 1;
+	uint64_t vc1_sram_sbe                 : 1;
+	uint64_t cxd_ram_sbe                  : 1;
+	uint64_t cxs_ram_sbe                  : 1;
 #endif
-	} cn52xx;
-	struct cvmx_pko_mem_debug6_cn52xx cn52xxp1;
-	struct cvmx_pko_mem_debug6_cn52xx cn56xx;
-	struct cvmx_pko_mem_debug6_cn52xx cn56xxp1;
-	struct cvmx_pko_mem_debug6_cn50xx cn58xx;
-	struct cvmx_pko_mem_debug6_cn50xx cn58xxp1;
-	struct cvmx_pko_mem_debug6_cn52xx cn61xx;
-	struct cvmx_pko_mem_debug6_cn52xx cn63xx;
-	struct cvmx_pko_mem_debug6_cn52xx cn63xxp1;
-	struct cvmx_pko_mem_debug6_cn52xx cn66xx;
-	struct cvmx_pko_mem_debug6_cn52xx cn68xx;
-	struct cvmx_pko_mem_debug6_cn52xx cn68xxp1;
-	struct cvmx_pko_mem_debug6_cn52xx cnf71xx;
+	} s;
+	struct cvmx_pko_pse_sq1_ecc_sbe_sts0_s cn78xx;
 };
+typedef union cvmx_pko_pse_sq1_ecc_sbe_sts0 cvmx_pko_pse_sq1_ecc_sbe_sts0_t;
 
-union cvmx_pko_mem_debug7 {
+/**
+ * cvmx_pko_pse_sq1_ecc_sbe_sts_cmb0
+ */
+union cvmx_pko_pse_sq1_ecc_sbe_sts_cmb0 {
 	uint64_t u64;
-	struct cvmx_pko_mem_debug7_s {
+	struct cvmx_pko_pse_sq1_ecc_sbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_0_63:64;
+	uint64_t pse_sq1_sbe_cmb0             : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_SQ1_ECC_SBE_STS0.
+                                                         To clear this bit, software must clear bits in PKO_PSE_SQ1_ECC_SBE_STS0.
+                                                         When this bit is set, the corresponding interrupt is set.
+                                                         Throws PKO_INTSN_E::PKO_PSE_SQ1_SBE_CMB0.
+                                                         INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.cxs_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.cxd_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.vc0_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.vc1_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.nt_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.pt_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.pc_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.rt_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.tw_0.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.tw_1.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.tp0_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.tp1_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.std0_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.std1_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.sts0_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.sts1_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.wt_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.sc_sram */
+	uint64_t reserved_0_62                : 63;
 #else
-		uint64_t reserved_0_63:64;
+	uint64_t reserved_0_62                : 63;
+	uint64_t pse_sq1_sbe_cmb0             : 1;
 #endif
 	} s;
-	struct cvmx_pko_mem_debug7_cn30xx {
+	struct cvmx_pko_pse_sq1_ecc_sbe_sts_cmb0_s cn78xx;
+};
+typedef union cvmx_pko_pse_sq1_ecc_sbe_sts_cmb0 cvmx_pko_pse_sq1_ecc_sbe_sts_cmb0_t;
+
+/**
+ * cvmx_pko_pse_sq2_bist_status
+ *
+ * Each bit is the BIST result of an individual memory (per bit, 0 = pass and 1 = fail).
+ *
+ */
+union cvmx_pko_pse_sq2_bist_status {
+	uint64_t u64;
+	struct cvmx_pko_pse_sq2_bist_status_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_58_63:6;
-		uint64_t dwb:9;
-		uint64_t start:33;
-		uint64_t size:16;
+	uint64_t reserved_29_63               : 35;
+	uint64_t sc_sram                      : 1;  /**< Scheduling configuration. */
+	uint64_t reserved_21_27               : 7;
+	uint64_t tp1_sram                     : 1;  /**< SQ[5:1] topology parent configuration. */
+	uint64_t tp0_sram                     : 1;  /**< SQ[5:1] topology parent configuration. */
+	uint64_t reserved_18_18               : 1;
+	uint64_t rt_sram                      : 1;  /**< Result table. */
+	uint64_t reserved_9_16                : 8;
+	uint64_t tw1_cmd_fifo                 : 1;  /**< SQ[5:1] time wheel 1 command FIFO SRAM. */
+	uint64_t std_sram                     : 1;  /**< Dynamic shaping state */
+	uint64_t sts_sram                     : 1;  /**< Static shaping configuration. */
+	uint64_t tw0_cmd_fifo                 : 1;  /**< SQ[5:1] time wheel 0 command FIFO SRAM. */
+	uint64_t reserved_3_4                 : 2;
+	uint64_t nt_sram                      : 1;  /**< Next pointer table. */
+	uint64_t pt_sram                      : 1;  /**< Previous pointer table. */
+	uint64_t wt_sram                      : 1;  /**< Work table. */
 #else
-		uint64_t size:16;
-		uint64_t start:33;
-		uint64_t dwb:9;
-		uint64_t reserved_58_63:6;
+	uint64_t wt_sram                      : 1;
+	uint64_t pt_sram                      : 1;
+	uint64_t nt_sram                      : 1;
+	uint64_t reserved_3_4                 : 2;
+	uint64_t tw0_cmd_fifo                 : 1;
+	uint64_t sts_sram                     : 1;
+	uint64_t std_sram                     : 1;
+	uint64_t tw1_cmd_fifo                 : 1;
+	uint64_t reserved_9_16                : 8;
+	uint64_t rt_sram                      : 1;
+	uint64_t reserved_18_18               : 1;
+	uint64_t tp0_sram                     : 1;
+	uint64_t tp1_sram                     : 1;
+	uint64_t reserved_21_27               : 7;
+	uint64_t sc_sram                      : 1;
+	uint64_t reserved_29_63               : 35;
 #endif
-	} cn30xx;
-	struct cvmx_pko_mem_debug7_cn30xx cn31xx;
-	struct cvmx_pko_mem_debug7_cn30xx cn38xx;
-	struct cvmx_pko_mem_debug7_cn30xx cn38xxp2;
-	struct cvmx_pko_mem_debug7_cn50xx {
+	} s;
+	struct cvmx_pko_pse_sq2_bist_status_s cn78xx;
+};
+typedef union cvmx_pko_pse_sq2_bist_status cvmx_pko_pse_sq2_bist_status_t;
+
+/**
+ * cvmx_pko_pse_sq2_ecc_ctl0
+ */
+union cvmx_pko_pse_sq2_ecc_ctl0 {
+	uint64_t u64;
+	struct cvmx_pko_pse_sq2_ecc_ctl0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t qos:5;
-		uint64_t tail:1;
-		uint64_t buf_siz:13;
-		uint64_t buf_ptr:33;
-		uint64_t qcb_widx:6;
-		uint64_t qcb_ridx:6;
+	uint64_t sq_pt_ram_flip               : 2;  /**< SQ_PT_RAM flip syndrome bits on write. */
+	uint64_t sq_pt_ram_cdis               : 1;  /**< SQ_PT_RAM ECC correction disable. */
+	uint64_t sq_nt_ram_flip               : 2;  /**< SQ_NT_RAM flip syndrome bits on write. */
+	uint64_t sq_nt_ram_cdis               : 1;  /**< SQ_NT_RAM ECC correction disable. */
+	uint64_t rt_ram_flip                  : 2;  /**< RT_RAM flip syndrome bits on write. */
+	uint64_t rt_ram_cdis                  : 1;  /**< RT_RAM ECC correction disable. */
+	uint64_t tw1_cmd_fifo_ram_flip        : 2;  /**< TW1_CMD_FIFO_RAM flip syndrome bits on write. */
+	uint64_t tw1_cmd_fifo_ram_cdis        : 1;  /**< TW1_CMD_FIFO_RAM ECC correction disable. */
+	uint64_t tw0_cmd_fifo_ram_flip        : 2;  /**< TW0_CMD_FIFO_RAM flip syndrome bits on write. */
+	uint64_t tw0_cmd_fifo_ram_cdis        : 1;  /**< TW0_CMD_FIFO_RAM ECC correction disable. */
+	uint64_t tp1_sram_flip                : 2;  /**< TP1_SRAM flip syndrome bits on write. */
+	uint64_t tp1_sram_cdis                : 1;  /**< TP1_SRAM ECC correction disable. */
+	uint64_t tp0_sram_flip                : 2;  /**< TP0_SRAM flip syndrome bits on write. */
+	uint64_t tp0_sram_cdis                : 1;  /**< TP0_SRAM ECC correction disable. */
+	uint64_t sts1_ram_flip                : 2;  /**< STS1_RAM flip syndrome bits on write. */
+	uint64_t sts1_ram_cdis                : 1;  /**< STS1_RAM ECC correction disable. */
+	uint64_t sts0_ram_flip                : 2;  /**< STS0_RAM flip syndrome bits on write. */
+	uint64_t sts0_ram_cdis                : 1;  /**< STS0_RAM ECC correction disable. */
+	uint64_t std1_ram_flip                : 2;  /**< STD1_RAM flip syndrome bits on write. */
+	uint64_t std1_ram_cdis                : 1;  /**< STD1_RAM ECC correction disable. */
+	uint64_t std0_ram_flip                : 2;  /**< STD0_RAM flip syndrome bits on write. */
+	uint64_t std0_ram_cdis                : 1;  /**< STD0_RAM ECC correction disable. */
+	uint64_t wt_ram_flip                  : 2;  /**< WT_RAM flip syndrome bits on write. */
+	uint64_t wt_ram_cdis                  : 1;  /**< WT_RAM ECC correction disable. */
+	uint64_t sc_ram_flip                  : 2;  /**< SC_RAM flip syndrome bits on write. */
+	uint64_t sc_ram_cdis                  : 1;  /**< SC_RAM ECC correction disable. */
+	uint64_t reserved_0_24                : 25;
 #else
-		uint64_t qcb_ridx:6;
-		uint64_t qcb_widx:6;
-		uint64_t buf_ptr:33;
-		uint64_t buf_siz:13;
-		uint64_t tail:1;
-		uint64_t qos:5;
+	uint64_t reserved_0_24                : 25;
+	uint64_t sc_ram_cdis                  : 1;
+	uint64_t sc_ram_flip                  : 2;
+	uint64_t wt_ram_cdis                  : 1;
+	uint64_t wt_ram_flip                  : 2;
+	uint64_t std0_ram_cdis                : 1;
+	uint64_t std0_ram_flip                : 2;
+	uint64_t std1_ram_cdis                : 1;
+	uint64_t std1_ram_flip                : 2;
+	uint64_t sts0_ram_cdis                : 1;
+	uint64_t sts0_ram_flip                : 2;
+	uint64_t sts1_ram_cdis                : 1;
+	uint64_t sts1_ram_flip                : 2;
+	uint64_t tp0_sram_cdis                : 1;
+	uint64_t tp0_sram_flip                : 2;
+	uint64_t tp1_sram_cdis                : 1;
+	uint64_t tp1_sram_flip                : 2;
+	uint64_t tw0_cmd_fifo_ram_cdis        : 1;
+	uint64_t tw0_cmd_fifo_ram_flip        : 2;
+	uint64_t tw1_cmd_fifo_ram_cdis        : 1;
+	uint64_t tw1_cmd_fifo_ram_flip        : 2;
+	uint64_t rt_ram_cdis                  : 1;
+	uint64_t rt_ram_flip                  : 2;
+	uint64_t sq_nt_ram_cdis               : 1;
+	uint64_t sq_nt_ram_flip               : 2;
+	uint64_t sq_pt_ram_cdis               : 1;
+	uint64_t sq_pt_ram_flip               : 2;
 #endif
-	} cn50xx;
-	struct cvmx_pko_mem_debug7_cn50xx cn52xx;
-	struct cvmx_pko_mem_debug7_cn50xx cn52xxp1;
-	struct cvmx_pko_mem_debug7_cn50xx cn56xx;
-	struct cvmx_pko_mem_debug7_cn50xx cn56xxp1;
-	struct cvmx_pko_mem_debug7_cn50xx cn58xx;
-	struct cvmx_pko_mem_debug7_cn50xx cn58xxp1;
-	struct cvmx_pko_mem_debug7_cn50xx cn61xx;
-	struct cvmx_pko_mem_debug7_cn50xx cn63xx;
-	struct cvmx_pko_mem_debug7_cn50xx cn63xxp1;
-	struct cvmx_pko_mem_debug7_cn50xx cn66xx;
-	struct cvmx_pko_mem_debug7_cn68xx {
+	} s;
+	struct cvmx_pko_pse_sq2_ecc_ctl0_s    cn78xx;
+};
+typedef union cvmx_pko_pse_sq2_ecc_ctl0 cvmx_pko_pse_sq2_ecc_ctl0_t;
+
+/**
+ * cvmx_pko_pse_sq2_ecc_dbe_sts0
+ */
+union cvmx_pko_pse_sq2_ecc_dbe_sts0 {
+	uint64_t u64;
+	struct cvmx_pko_pse_sq2_ecc_dbe_sts0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t qos:3;
-		uint64_t tail:1;
-		uint64_t buf_siz:13;
-		uint64_t buf_ptr:33;
-		uint64_t qcb_widx:7;
-		uint64_t qcb_ridx:7;
+	uint64_t sq_pt_ram_dbe                : 1;  /**< Double-bit error for SQ_PT_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.nt_sram */
+	uint64_t sq_nt_ram_dbe                : 1;  /**< Double-bit error for SQ_NT_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.pt_sram */
+	uint64_t rt_ram_dbe                   : 1;  /**< Double-bit error for RT_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq2_pq.sq2.rt_sram */
+	uint64_t tw1_cmd_fifo_ram_dbe         : 1;  /**< Double-bit error for TW1_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.tw_1.sq_fifo_sram */
+	uint64_t tw0_cmd_fifo_ram_dbe         : 1;  /**< Double-bit error for TW0_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.tw_0.sq_fifo_sram */
+	uint64_t tp1_sram_dbe                 : 1;  /**< Double-bit error for TP1_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.tp1_sram */
+	uint64_t tp0_sram_dbe                 : 1;  /**< Double-bit error for TP0_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.tp0_sram */
+	uint64_t sts1_ram_dbe                 : 1;  /**< Double-bit error for STS1_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.sts1_sram */
+	uint64_t sts0_ram_dbe                 : 1;  /**< Double-bit error for STS0_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.sts0_sram */
+	uint64_t std1_ram_dbe                 : 1;  /**< Double-bit error for STD1_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.std1_sram */
+	uint64_t std0_ram_dbe                 : 1;  /**< Double-bit error for STD0_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.std0_sram */
+	uint64_t wt_ram_dbe                   : 1;  /**< Double-bit error for WT_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq2_pq.sq2.wt_sram */
+	uint64_t sc_ram_dbe                   : 1;  /**< Double-bit error for SC_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq2_pq.sq2.sc_sram */
+	uint64_t reserved_0_50                : 51;
 #else
-		uint64_t qcb_ridx:7;
-		uint64_t qcb_widx:7;
-		uint64_t buf_ptr:33;
-		uint64_t buf_siz:13;
-		uint64_t tail:1;
-		uint64_t qos:3;
+	uint64_t reserved_0_50                : 51;
+	uint64_t sc_ram_dbe                   : 1;
+	uint64_t wt_ram_dbe                   : 1;
+	uint64_t std0_ram_dbe                 : 1;
+	uint64_t std1_ram_dbe                 : 1;
+	uint64_t sts0_ram_dbe                 : 1;
+	uint64_t sts1_ram_dbe                 : 1;
+	uint64_t tp0_sram_dbe                 : 1;
+	uint64_t tp1_sram_dbe                 : 1;
+	uint64_t tw0_cmd_fifo_ram_dbe         : 1;
+	uint64_t tw1_cmd_fifo_ram_dbe         : 1;
+	uint64_t rt_ram_dbe                   : 1;
+	uint64_t sq_nt_ram_dbe                : 1;
+	uint64_t sq_pt_ram_dbe                : 1;
 #endif
-	} cn68xx;
-	struct cvmx_pko_mem_debug7_cn68xx cn68xxp1;
-	struct cvmx_pko_mem_debug7_cn50xx cnf71xx;
+	} s;
+	struct cvmx_pko_pse_sq2_ecc_dbe_sts0_s cn78xx;
 };
+typedef union cvmx_pko_pse_sq2_ecc_dbe_sts0 cvmx_pko_pse_sq2_ecc_dbe_sts0_t;
 
-union cvmx_pko_mem_debug8 {
+/**
+ * cvmx_pko_pse_sq2_ecc_dbe_sts_cmb0
+ */
+union cvmx_pko_pse_sq2_ecc_dbe_sts_cmb0 {
 	uint64_t u64;
-	struct cvmx_pko_mem_debug8_s {
+	struct cvmx_pko_pse_sq2_ecc_dbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_59_63:5;
-		uint64_t tail:1;
-		uint64_t buf_siz:13;
-		uint64_t reserved_0_44:45;
+	uint64_t pse_sq2_dbe_cmb0             : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_SQ2_ECC_DBE_STS0.
+                                                         To clear this bit, software must clear bits in PKO_PSE_SQ2_ECC_DBE_STS0.
+                                                         When this bit is set, the corresponding interrupt is set.
+                                                         Throws PKO_INTSN_E::PKO_PSE_SQ2_DBE_CMB0.
+                                                         INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.nt_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.pt_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.rt_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.tw_0.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.tw_1.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.tp0_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.tp1_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.std0_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.std1_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.sts0_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.sts1_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.wt_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.sc_sram */
+	uint64_t reserved_0_62                : 63;
 #else
-		uint64_t reserved_0_44:45;
-		uint64_t buf_siz:13;
-		uint64_t tail:1;
-		uint64_t reserved_59_63:5;
+	uint64_t reserved_0_62                : 63;
+	uint64_t pse_sq2_dbe_cmb0             : 1;
 #endif
 	} s;
-	struct cvmx_pko_mem_debug8_cn30xx {
+	struct cvmx_pko_pse_sq2_ecc_dbe_sts_cmb0_s cn78xx;
+};
+typedef union cvmx_pko_pse_sq2_ecc_dbe_sts_cmb0 cvmx_pko_pse_sq2_ecc_dbe_sts_cmb0_t;
+
+/**
+ * cvmx_pko_pse_sq2_ecc_sbe_sts0
+ */
+union cvmx_pko_pse_sq2_ecc_sbe_sts0 {
+	uint64_t u64;
+	struct cvmx_pko_pse_sq2_ecc_sbe_sts0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t qos:5;
-		uint64_t tail:1;
-		uint64_t buf_siz:13;
-		uint64_t buf_ptr:33;
-		uint64_t qcb_widx:6;
-		uint64_t qcb_ridx:6;
+	uint64_t sq_pt_ram_sbe                : 1;  /**< Single-bit error for SQ_PT_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.nt_sram */
+	uint64_t sq_nt_ram_sbe                : 1;  /**< Single-bit error for SQ_NT_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.pt_sram */
+	uint64_t rt_ram_sbe                   : 1;  /**< Single-bit error for RT_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq2_pq.sq2.rt_sram */
+	uint64_t tw1_cmd_fifo_ram_sbe         : 1;  /**< Single-bit error for TW1_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.tw_1.sq_fifo_sram */
+	uint64_t tw0_cmd_fifo_ram_sbe         : 1;  /**< Single-bit error for TW0_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.tw_0.sq_fifo_sram */
+	uint64_t tp1_sram_sbe                 : 1;  /**< Single-bit error for TP1_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.tp1_sram */
+	uint64_t tp0_sram_sbe                 : 1;  /**< Single-bit error for TP0_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.tp0_sram */
+	uint64_t sts1_ram_sbe                 : 1;  /**< Single-bit error for STS1_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.sts1_sram */
+	uint64_t sts0_ram_sbe                 : 1;  /**< Single-bit error for STS0_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.sts0_sram */
+	uint64_t std1_ram_sbe                 : 1;  /**< Single-bit error for STD1_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.std1_sram */
+	uint64_t std0_ram_sbe                 : 1;  /**< Single-bit error for STD0_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.std0_sram */
+	uint64_t wt_ram_sbe                   : 1;  /**< Single-bit error for WT_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq2_pq.sq2.wt_sram */
+	uint64_t sc_ram_sbe                   : 1;  /**< Single-bit error for SC_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq2_pq.sq2.sc_sram */
+	uint64_t reserved_0_50                : 51;
 #else
-		uint64_t qcb_ridx:6;
-		uint64_t qcb_widx:6;
-		uint64_t buf_ptr:33;
-		uint64_t buf_siz:13;
-		uint64_t tail:1;
-		uint64_t qos:5;
+	uint64_t reserved_0_50                : 51;
+	uint64_t sc_ram_sbe                   : 1;
+	uint64_t wt_ram_sbe                   : 1;
+	uint64_t std0_ram_sbe                 : 1;
+	uint64_t std1_ram_sbe                 : 1;
+	uint64_t sts0_ram_sbe                 : 1;
+	uint64_t sts1_ram_sbe                 : 1;
+	uint64_t tp0_sram_sbe                 : 1;
+	uint64_t tp1_sram_sbe                 : 1;
+	uint64_t tw0_cmd_fifo_ram_sbe         : 1;
+	uint64_t tw1_cmd_fifo_ram_sbe         : 1;
+	uint64_t rt_ram_sbe                   : 1;
+	uint64_t sq_nt_ram_sbe                : 1;
+	uint64_t sq_pt_ram_sbe                : 1;
 #endif
-	} cn30xx;
-	struct cvmx_pko_mem_debug8_cn30xx cn31xx;
-	struct cvmx_pko_mem_debug8_cn30xx cn38xx;
-	struct cvmx_pko_mem_debug8_cn30xx cn38xxp2;
-	struct cvmx_pko_mem_debug8_cn50xx {
+	} s;
+	struct cvmx_pko_pse_sq2_ecc_sbe_sts0_s cn78xx;
+};
+typedef union cvmx_pko_pse_sq2_ecc_sbe_sts0 cvmx_pko_pse_sq2_ecc_sbe_sts0_t;
+
+/**
+ * cvmx_pko_pse_sq2_ecc_sbe_sts_cmb0
+ */
+union cvmx_pko_pse_sq2_ecc_sbe_sts_cmb0 {
+	uint64_t u64;
+	struct cvmx_pko_pse_sq2_ecc_sbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_28_63:36;
-		uint64_t doorbell:20;
-		uint64_t reserved_6_7:2;
-		uint64_t static_p:1;
-		uint64_t s_tail:1;
-		uint64_t static_q:1;
-		uint64_t qos:3;
-#else
-		uint64_t qos:3;
-		uint64_t static_q:1;
-		uint64_t s_tail:1;
-		uint64_t static_p:1;
-		uint64_t reserved_6_7:2;
-		uint64_t doorbell:20;
-		uint64_t reserved_28_63:36;
+	uint64_t pse_sq2_sbe_cmb0             : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_SQ2_ECC_SBE_STS0.
+                                                         To clear this bit, software must clear bits in PKO_PSE_SQ2_ECC_SBE_STS0.
+                                                         When this bit is set, the corresponding interrupt is set.
+                                                         Throws PKO_INTSN_E::PKO_PSE_SQ2_SBE_CMB0.
+                                                         INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.nt_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.pt_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.rt_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.tw_0.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.tw_1.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.tp0_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.tp1_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.std0_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.std1_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.sts0_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq1.sts1_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.wt_sram
+                                                         pko_pnr2.pko_pse.pse_sq2_pq.sq2.sc_sram */
+	uint64_t reserved_0_62                : 63;
+#else
+	uint64_t reserved_0_62                : 63;
+	uint64_t pse_sq2_sbe_cmb0             : 1;
 #endif
-	} cn50xx;
-	struct cvmx_pko_mem_debug8_cn52xx {
+	} s;
+	struct cvmx_pko_pse_sq2_ecc_sbe_sts_cmb0_s cn78xx;
+};
+typedef union cvmx_pko_pse_sq2_ecc_sbe_sts_cmb0 cvmx_pko_pse_sq2_ecc_sbe_sts_cmb0_t;
+
+/**
+ * cvmx_pko_pse_sq3_bist_status
+ *
+ * Each bit is the BIST result of an individual memory (per bit, 0 = pass and 1 = fail).
+ *
+ */
+union cvmx_pko_pse_sq3_bist_status {
+	uint64_t u64;
+	struct cvmx_pko_pse_sq3_bist_status_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_29_63:35;
-		uint64_t preempter:1;
-		uint64_t doorbell:20;
-		uint64_t reserved_7_7:1;
-		uint64_t preemptee:1;
-		uint64_t static_p:1;
-		uint64_t s_tail:1;
-		uint64_t static_q:1;
-		uint64_t qos:3;
-#else
-		uint64_t qos:3;
-		uint64_t static_q:1;
-		uint64_t s_tail:1;
-		uint64_t static_p:1;
-		uint64_t preemptee:1;
-		uint64_t reserved_7_7:1;
-		uint64_t doorbell:20;
-		uint64_t preempter:1;
-		uint64_t reserved_29_63:35;
+	uint64_t reserved_29_63               : 35;
+	uint64_t sc_sram                      : 1;  /**< Scheduling configuration */
+	uint64_t reserved_23_27               : 5;
+	uint64_t tp3_sram                     : 1;  /**< SQ[5:3] topology parent configuration */
+	uint64_t tp2_sram                     : 1;  /**< SQ[5:3] topology parent configuration */
+	uint64_t tp1_sram                     : 1;  /**< SQ[5:1] topology parent configuration */
+	uint64_t tp0_sram                     : 1;  /**< SQ[5:1] topology parent configuration */
+	uint64_t reserved_18_18               : 1;
+	uint64_t rt_sram                      : 1;  /**< Result table */
+	uint64_t reserved_15_16               : 2;
+	uint64_t tw3_cmd_fifo                 : 1;  /**< SQ[5:3] time wheel 3 command FIFO SRAM */
+	uint64_t reserved_12_13               : 2;
+	uint64_t tw2_cmd_fifo                 : 1;  /**< SQ[5:3] time wheel 2 command FIFO SRAM */
+	uint64_t reserved_9_10                : 2;
+	uint64_t tw1_cmd_fifo                 : 1;  /**< SQ[5:1] time wheel 1 command FIFO SRAM */
+	uint64_t std_sram                     : 1;  /**< Dynamic shaping state */
+	uint64_t sts_sram                     : 1;  /**< Static shaping configuration */
+	uint64_t tw0_cmd_fifo                 : 1;  /**< SQ[5:1] time wheel 0 command FIFO SRAM */
+	uint64_t reserved_3_4                 : 2;
+	uint64_t nt_sram                      : 1;  /**< Next pointer table */
+	uint64_t pt_sram                      : 1;  /**< Previous pointer table */
+	uint64_t wt_sram                      : 1;  /**< Work table */
+#else
+	uint64_t wt_sram                      : 1;
+	uint64_t pt_sram                      : 1;
+	uint64_t nt_sram                      : 1;
+	uint64_t reserved_3_4                 : 2;
+	uint64_t tw0_cmd_fifo                 : 1;
+	uint64_t sts_sram                     : 1;
+	uint64_t std_sram                     : 1;
+	uint64_t tw1_cmd_fifo                 : 1;
+	uint64_t reserved_9_10                : 2;
+	uint64_t tw2_cmd_fifo                 : 1;
+	uint64_t reserved_12_13               : 2;
+	uint64_t tw3_cmd_fifo                 : 1;
+	uint64_t reserved_15_16               : 2;
+	uint64_t rt_sram                      : 1;
+	uint64_t reserved_18_18               : 1;
+	uint64_t tp0_sram                     : 1;
+	uint64_t tp1_sram                     : 1;
+	uint64_t tp2_sram                     : 1;
+	uint64_t tp3_sram                     : 1;
+	uint64_t reserved_23_27               : 5;
+	uint64_t sc_sram                      : 1;
+	uint64_t reserved_29_63               : 35;
 #endif
-	} cn52xx;
-	struct cvmx_pko_mem_debug8_cn52xx cn52xxp1;
-	struct cvmx_pko_mem_debug8_cn52xx cn56xx;
-	struct cvmx_pko_mem_debug8_cn52xx cn56xxp1;
-	struct cvmx_pko_mem_debug8_cn50xx cn58xx;
-	struct cvmx_pko_mem_debug8_cn50xx cn58xxp1;
-	struct cvmx_pko_mem_debug8_cn61xx {
+	} s;
+	struct cvmx_pko_pse_sq3_bist_status_s cn78xx;
+};
+typedef union cvmx_pko_pse_sq3_bist_status cvmx_pko_pse_sq3_bist_status_t;
+
+/**
+ * cvmx_pko_pse_sq3_ecc_ctl0
+ */
+union cvmx_pko_pse_sq3_ecc_ctl0 {
+	uint64_t u64;
+	struct cvmx_pko_pse_sq3_ecc_ctl0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_42_63:22;
-		uint64_t qid_qqos:8;
-		uint64_t reserved_33_33:1;
-		uint64_t qid_idx:4;
-		uint64_t preempter:1;
-		uint64_t doorbell:20;
-		uint64_t reserved_7_7:1;
-		uint64_t preemptee:1;
-		uint64_t static_p:1;
-		uint64_t s_tail:1;
-		uint64_t static_q:1;
-		uint64_t qos:3;
-#else
-		uint64_t qos:3;
-		uint64_t static_q:1;
-		uint64_t s_tail:1;
-		uint64_t static_p:1;
-		uint64_t preemptee:1;
-		uint64_t reserved_7_7:1;
-		uint64_t doorbell:20;
-		uint64_t preempter:1;
-		uint64_t qid_idx:4;
-		uint64_t reserved_33_33:1;
-		uint64_t qid_qqos:8;
-		uint64_t reserved_42_63:22;
+	uint64_t sq_pt_ram_flip               : 2;  /**< SQ_PT_RAM flip syndrome bits on write. */
+	uint64_t sq_pt_ram_cdis               : 1;  /**< SQ_PT_RAM ECC correction disable. */
+	uint64_t sq_nt_ram_flip               : 2;  /**< SQ_NT_RAM flip syndrome bits on write. */
+	uint64_t sq_nt_ram_cdis               : 1;  /**< SQ_NT_RAM ECC correction disable. */
+	uint64_t rt_ram_flip                  : 2;  /**< RT_RAM flip syndrome bits on write. */
+	uint64_t rt_ram_cdis                  : 1;  /**< RT_RAM ECC correction disable. */
+	uint64_t tw3_cmd_fifo_ram_flip        : 2;  /**< TW3_CMD_FIFO_RAM flip syndrome bits on write. */
+	uint64_t tw3_cmd_fifo_ram_cdis        : 1;  /**< TW3_CMD_FIFO_RAM ECC correction disable. */
+	uint64_t tw2_cmd_fifo_ram_flip        : 2;  /**< TW2_CMD_FIFO_RAM flip syndrome bits on write. */
+	uint64_t tw2_cmd_fifo_ram_cdis        : 1;  /**< TW2_CMD_FIFO_RAM ECC correction disable. */
+	uint64_t tw1_cmd_fifo_ram_flip        : 2;  /**< TW1_CMD_FIFO_RAM flip syndrome bits on write. */
+	uint64_t tw1_cmd_fifo_ram_cdis        : 1;  /**< TW1_CMD_FIFO_RAM ECC correction disable. */
+	uint64_t tw0_cmd_fifo_ram_flip        : 2;  /**< TW0_CMD_FIFO_RAM flip syndrome bits on write. */
+	uint64_t tw0_cmd_fifo_ram_cdis        : 1;  /**< TW0_CMD_FIFO_RAM ECC correction disable. */
+	uint64_t tp3_sram_flip                : 2;  /**< TP3_SRAM flip syndrome bits on write. */
+	uint64_t tp3_sram_cdis                : 1;  /**< TP3_SRAM ECC correction disable. */
+	uint64_t tp2_sram_flip                : 2;  /**< TP2_SRAM flip syndrome bits on write. */
+	uint64_t tp2_sram_cdis                : 1;  /**< TP2_SRAM ECC correction disable. */
+	uint64_t tp1_sram_flip                : 2;  /**< TP1_SRAM flip syndrome bits on write. */
+	uint64_t tp1_sram_cdis                : 1;  /**< TP1_SRAM ECC correction disable. */
+	uint64_t tp0_sram_flip                : 2;  /**< TP0_SRAM flip syndrome bits on write. */
+	uint64_t tp0_sram_cdis                : 1;  /**< TP0_SRAM ECC correction disable. */
+	uint64_t sts3_ram_flip                : 2;  /**< STS3_RAM flip syndrome bits on write. */
+	uint64_t sts3_ram_cdis                : 1;  /**< STS3_RAM ECC correction disable. */
+	uint64_t sts2_ram_flip                : 2;  /**< STS2_RAM flip syndrome bits on write. */
+	uint64_t sts2_ram_cdis                : 1;  /**< STS2_RAM ECC correction disable. */
+	uint64_t sts1_ram_flip                : 2;  /**< STS1_RAM flip syndrome bits on write. */
+	uint64_t sts1_ram_cdis                : 1;  /**< STS1_RAM ECC correction disable. */
+	uint64_t sts0_ram_flip                : 2;  /**< STS0_RAM flip syndrome bits on write. */
+	uint64_t sts0_ram_cdis                : 1;  /**< STS0_RAM ECC correction disable. */
+	uint64_t std3_ram_flip                : 2;  /**< STD3_RAM flip syndrome bits on write. */
+	uint64_t std3_ram_cdis                : 1;  /**< STD3_RAM ECC correction disable. */
+	uint64_t std2_ram_flip                : 2;  /**< STD2_RAM flip syndrome bits on write. */
+	uint64_t std2_ram_cdis                : 1;  /**< STD2_RAM ECC correction disable. */
+	uint64_t std1_ram_flip                : 2;  /**< STD1_RAM flip syndrome bits on write. */
+	uint64_t std1_ram_cdis                : 1;  /**< STD1_RAM ECC correction disable. */
+	uint64_t std0_ram_flip                : 2;  /**< STD0_RAM flip syndrome bits on write. */
+	uint64_t std0_ram_cdis                : 1;  /**< STD0_RAM ECC correction disable. */
+	uint64_t wt_ram_flip                  : 2;  /**< WT_RAM flip syndrome bits on write. */
+	uint64_t wt_ram_cdis                  : 1;  /**< WT_RAM ECC correction disable. */
+	uint64_t sc_ram_flip                  : 2;  /**< SC_RAM flip syndrome bits on write. */
+	uint64_t sc_ram_cdis                  : 1;  /**< SC_RAM ECC correction disable. */
+	uint64_t reserved_0_0                 : 1;
+#else
+	uint64_t reserved_0_0                 : 1;
+	uint64_t sc_ram_cdis                  : 1;
+	uint64_t sc_ram_flip                  : 2;
+	uint64_t wt_ram_cdis                  : 1;
+	uint64_t wt_ram_flip                  : 2;
+	uint64_t std0_ram_cdis                : 1;
+	uint64_t std0_ram_flip                : 2;
+	uint64_t std1_ram_cdis                : 1;
+	uint64_t std1_ram_flip                : 2;
+	uint64_t std2_ram_cdis                : 1;
+	uint64_t std2_ram_flip                : 2;
+	uint64_t std3_ram_cdis                : 1;
+	uint64_t std3_ram_flip                : 2;
+	uint64_t sts0_ram_cdis                : 1;
+	uint64_t sts0_ram_flip                : 2;
+	uint64_t sts1_ram_cdis                : 1;
+	uint64_t sts1_ram_flip                : 2;
+	uint64_t sts2_ram_cdis                : 1;
+	uint64_t sts2_ram_flip                : 2;
+	uint64_t sts3_ram_cdis                : 1;
+	uint64_t sts3_ram_flip                : 2;
+	uint64_t tp0_sram_cdis                : 1;
+	uint64_t tp0_sram_flip                : 2;
+	uint64_t tp1_sram_cdis                : 1;
+	uint64_t tp1_sram_flip                : 2;
+	uint64_t tp2_sram_cdis                : 1;
+	uint64_t tp2_sram_flip                : 2;
+	uint64_t tp3_sram_cdis                : 1;
+	uint64_t tp3_sram_flip                : 2;
+	uint64_t tw0_cmd_fifo_ram_cdis        : 1;
+	uint64_t tw0_cmd_fifo_ram_flip        : 2;
+	uint64_t tw1_cmd_fifo_ram_cdis        : 1;
+	uint64_t tw1_cmd_fifo_ram_flip        : 2;
+	uint64_t tw2_cmd_fifo_ram_cdis        : 1;
+	uint64_t tw2_cmd_fifo_ram_flip        : 2;
+	uint64_t tw3_cmd_fifo_ram_cdis        : 1;
+	uint64_t tw3_cmd_fifo_ram_flip        : 2;
+	uint64_t rt_ram_cdis                  : 1;
+	uint64_t rt_ram_flip                  : 2;
+	uint64_t sq_nt_ram_cdis               : 1;
+	uint64_t sq_nt_ram_flip               : 2;
+	uint64_t sq_pt_ram_cdis               : 1;
+	uint64_t sq_pt_ram_flip               : 2;
 #endif
-	} cn61xx;
-	struct cvmx_pko_mem_debug8_cn52xx cn63xx;
-	struct cvmx_pko_mem_debug8_cn52xx cn63xxp1;
-	struct cvmx_pko_mem_debug8_cn61xx cn66xx;
-	struct cvmx_pko_mem_debug8_cn68xx {
+	} s;
+	struct cvmx_pko_pse_sq3_ecc_ctl0_s    cn78xx;
+};
+typedef union cvmx_pko_pse_sq3_ecc_ctl0 cvmx_pko_pse_sq3_ecc_ctl0_t;
+
+/**
+ * cvmx_pko_pse_sq3_ecc_dbe_sts0
+ */
+union cvmx_pko_pse_sq3_ecc_dbe_sts0 {
+	uint64_t u64;
+	struct cvmx_pko_pse_sq3_ecc_dbe_sts0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_37_63:27;
-		uint64_t preempter:1;
-		uint64_t doorbell:20;
-		uint64_t reserved_9_15:7;
-		uint64_t preemptee:1;
-		uint64_t static_p:1;
-		uint64_t s_tail:1;
-		uint64_t static_q:1;
-		uint64_t qos:5;
-#else
-		uint64_t qos:5;
-		uint64_t static_q:1;
-		uint64_t s_tail:1;
-		uint64_t static_p:1;
-		uint64_t preemptee:1;
-		uint64_t reserved_9_15:7;
-		uint64_t doorbell:20;
-		uint64_t preempter:1;
-		uint64_t reserved_37_63:27;
+	uint64_t sq_pt_ram_dbe                : 1;  /**< Double-bit error for SQ_PT_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.nt_sram */
+	uint64_t sq_nt_ram_dbe                : 1;  /**< Double-bit error for SQ_NT_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.pt_sram */
+	uint64_t rt_ram_dbe                   : 1;  /**< Double-bit error for RT_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq5_sq3.sq3.rt_sram */
+	uint64_t tw3_cmd_fifo_ram_dbe         : 1;  /**< Double-bit error for TW3_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tw_3.sq_fifo_sram */
+	uint64_t tw2_cmd_fifo_ram_dbe         : 1;  /**< Double-bit error for TW2_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tw_2.sq_fifo_sram */
+	uint64_t tw1_cmd_fifo_ram_dbe         : 1;  /**< Double-bit error for TW1_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tw_1.sq_fifo_sram */
+	uint64_t tw0_cmd_fifo_ram_dbe         : 1;  /**< Double-bit error for TW0_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tw_0.sq_fifo_sram */
+	uint64_t tp3_sram_dbe                 : 1;  /**< Double-bit error for TP3_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tp3_sram */
+	uint64_t tp2_sram_dbe                 : 1;  /**< Double-bit error for TP2_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tp2_sram */
+	uint64_t tp1_sram_dbe                 : 1;  /**< Double-bit error for TP1_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tp1_sram */
+	uint64_t tp0_sram_dbe                 : 1;  /**< Double-bit error for TP0_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tp0_sram */
+	uint64_t sts3_ram_dbe                 : 1;  /**< Double-bit error for STS3_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.sts3_sram */
+	uint64_t sts2_ram_dbe                 : 1;  /**< Double-bit error for STS2_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.sts2_sram */
+	uint64_t sts1_ram_dbe                 : 1;  /**< Double-bit error for STS1_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.sts1_sram */
+	uint64_t sts0_ram_dbe                 : 1;  /**< Double-bit error for STS0_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.sts0_sram */
+	uint64_t std3_ram_dbe                 : 1;  /**< Double-bit error for STD3_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.std3_sram */
+	uint64_t std2_ram_dbe                 : 1;  /**< Double-bit error for STD2_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.std2_sram */
+	uint64_t std1_ram_dbe                 : 1;  /**< Double-bit error for STD1_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.std1_sram */
+	uint64_t std0_ram_dbe                 : 1;  /**< Double-bit error for STD0_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.std0_sram */
+	uint64_t wt_ram_dbe                   : 1;  /**< Double-bit error for WT_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq5_sq3.sq3.wt_sram */
+	uint64_t sc_ram_dbe                   : 1;  /**< Double-bit error for SC_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq5_sq3.sq3.sc_sram */
+	uint64_t reserved_0_42                : 43;
+#else
+	uint64_t reserved_0_42                : 43;
+	uint64_t sc_ram_dbe                   : 1;
+	uint64_t wt_ram_dbe                   : 1;
+	uint64_t std0_ram_dbe                 : 1;
+	uint64_t std1_ram_dbe                 : 1;
+	uint64_t std2_ram_dbe                 : 1;
+	uint64_t std3_ram_dbe                 : 1;
+	uint64_t sts0_ram_dbe                 : 1;
+	uint64_t sts1_ram_dbe                 : 1;
+	uint64_t sts2_ram_dbe                 : 1;
+	uint64_t sts3_ram_dbe                 : 1;
+	uint64_t tp0_sram_dbe                 : 1;
+	uint64_t tp1_sram_dbe                 : 1;
+	uint64_t tp2_sram_dbe                 : 1;
+	uint64_t tp3_sram_dbe                 : 1;
+	uint64_t tw0_cmd_fifo_ram_dbe         : 1;
+	uint64_t tw1_cmd_fifo_ram_dbe         : 1;
+	uint64_t tw2_cmd_fifo_ram_dbe         : 1;
+	uint64_t tw3_cmd_fifo_ram_dbe         : 1;
+	uint64_t rt_ram_dbe                   : 1;
+	uint64_t sq_nt_ram_dbe                : 1;
+	uint64_t sq_pt_ram_dbe                : 1;
 #endif
-	} cn68xx;
-	struct cvmx_pko_mem_debug8_cn68xx cn68xxp1;
-	struct cvmx_pko_mem_debug8_cn61xx cnf71xx;
+	} s;
+	struct cvmx_pko_pse_sq3_ecc_dbe_sts0_s cn78xx;
 };
+typedef union cvmx_pko_pse_sq3_ecc_dbe_sts0 cvmx_pko_pse_sq3_ecc_dbe_sts0_t;
 
-union cvmx_pko_mem_debug9 {
+/**
+ * cvmx_pko_pse_sq3_ecc_dbe_sts_cmb0
+ */
+union cvmx_pko_pse_sq3_ecc_dbe_sts_cmb0 {
 	uint64_t u64;
-	struct cvmx_pko_mem_debug9_s {
+	struct cvmx_pko_pse_sq3_ecc_dbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_49_63:15;
-		uint64_t ptrs0:17;
-		uint64_t reserved_0_31:32;
+	uint64_t pse_sq3_dbe_cmb0             : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_SQ3_ECC_DBE_STS0.
+                                                         To clear this bit, software must clear bits in PKO_PSE_SQ3_ECC_DBE_STS0.
+                                                         When this bit is set, the corresponding interrupt is set.
+                                                         Throws PKO_INTSN_E::PKO_PSE_SQ3_DBE_CMB0.
+                                                         INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.nt_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.pt_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.rt_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tw_0.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tw_1.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tw_2.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tw_3.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tp0_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tp1_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tp2_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tp3_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.std0_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.std1_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.std2_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.std3_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.sts0_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.sts1_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.sts2_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.sts3_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.wt_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.sc_sram */
+	uint64_t reserved_0_62                : 63;
 #else
-		uint64_t reserved_0_31:32;
-		uint64_t ptrs0:17;
-		uint64_t reserved_49_63:15;
+	uint64_t reserved_0_62                : 63;
+	uint64_t pse_sq3_dbe_cmb0             : 1;
 #endif
 	} s;
-	struct cvmx_pko_mem_debug9_cn30xx {
+	struct cvmx_pko_pse_sq3_ecc_dbe_sts_cmb0_s cn78xx;
+};
+typedef union cvmx_pko_pse_sq3_ecc_dbe_sts_cmb0 cvmx_pko_pse_sq3_ecc_dbe_sts_cmb0_t;
+
+/**
+ * cvmx_pko_pse_sq3_ecc_sbe_sts0
+ */
+union cvmx_pko_pse_sq3_ecc_sbe_sts0 {
+	uint64_t u64;
+	struct cvmx_pko_pse_sq3_ecc_sbe_sts0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_28_63:36;
-		uint64_t doorbell:20;
-		uint64_t reserved_5_7:3;
-		uint64_t s_tail:1;
-		uint64_t static_q:1;
-		uint64_t qos:3;
+	uint64_t sq_pt_ram_sbe                : 1;  /**< Single-bit error for SQ_PT_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.nt_sram */
+	uint64_t sq_nt_ram_sbe                : 1;  /**< Single-bit error for SQ_NT_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.pt_sram */
+	uint64_t rt_ram_sbe                   : 1;  /**< Single-bit error for RT_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq5_sq3.sq3.rt_sram */
+	uint64_t tw3_cmd_fifo_ram_sbe         : 1;  /**< Single-bit error for TW3_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tw_3.sq_fifo_sram */
+	uint64_t tw2_cmd_fifo_ram_sbe         : 1;  /**< Single-bit error for TW2_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tw_2.sq_fifo_sram */
+	uint64_t tw1_cmd_fifo_ram_sbe         : 1;  /**< Single-bit error for TW1_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tw_1.sq_fifo_sram */
+	uint64_t tw0_cmd_fifo_ram_sbe         : 1;  /**< Single-bit error for TW0_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tw_0.sq_fifo_sram */
+	uint64_t tp3_sram_sbe                 : 1;  /**< Single-bit error for TP3_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tp3_sram */
+	uint64_t tp2_sram_sbe                 : 1;  /**< Single-bit error for TP2_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tp2_sram */
+	uint64_t tp1_sram_sbe                 : 1;  /**< Single-bit error for TP1_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tp1_sram */
+	uint64_t tp0_sram_sbe                 : 1;  /**< Single-bit error for TP0_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tp0_sram */
+	uint64_t sts3_ram_sbe                 : 1;  /**< Single-bit error for STS3_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.sts3_sram */
+	uint64_t sts2_ram_sbe                 : 1;  /**< Single-bit error for STS2_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.sts2_sram */
+	uint64_t sts1_ram_sbe                 : 1;  /**< Single-bit error for STS1_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.sts1_sram */
+	uint64_t sts0_ram_sbe                 : 1;  /**< Single-bit error for STS0_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.sts0_sram */
+	uint64_t std3_ram_sbe                 : 1;  /**< Single-bit error for STD3_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.std3_sram */
+	uint64_t std2_ram_sbe                 : 1;  /**< Single-bit error for STD2_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.std2_sram */
+	uint64_t std1_ram_sbe                 : 1;  /**< Single-bit error for STD1_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.std1_sram */
+	uint64_t std0_ram_sbe                 : 1;  /**< Single-bit error for STD0_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.std0_sram */
+	uint64_t wt_ram_sbe                   : 1;  /**< Single-bit error for WT_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq5_sq3.sq3.wt_sram */
+	uint64_t sc_ram_sbe                   : 1;  /**< Single-bit error for SC_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq5_sq3.sq3.sc_sram */
+	uint64_t reserved_0_42                : 43;
 #else
-		uint64_t qos:3;
-		uint64_t static_q:1;
-		uint64_t s_tail:1;
-		uint64_t reserved_5_7:3;
-		uint64_t doorbell:20;
-		uint64_t reserved_28_63:36;
+	uint64_t reserved_0_42                : 43;
+	uint64_t sc_ram_sbe                   : 1;
+	uint64_t wt_ram_sbe                   : 1;
+	uint64_t std0_ram_sbe                 : 1;
+	uint64_t std1_ram_sbe                 : 1;
+	uint64_t std2_ram_sbe                 : 1;
+	uint64_t std3_ram_sbe                 : 1;
+	uint64_t sts0_ram_sbe                 : 1;
+	uint64_t sts1_ram_sbe                 : 1;
+	uint64_t sts2_ram_sbe                 : 1;
+	uint64_t sts3_ram_sbe                 : 1;
+	uint64_t tp0_sram_sbe                 : 1;
+	uint64_t tp1_sram_sbe                 : 1;
+	uint64_t tp2_sram_sbe                 : 1;
+	uint64_t tp3_sram_sbe                 : 1;
+	uint64_t tw0_cmd_fifo_ram_sbe         : 1;
+	uint64_t tw1_cmd_fifo_ram_sbe         : 1;
+	uint64_t tw2_cmd_fifo_ram_sbe         : 1;
+	uint64_t tw3_cmd_fifo_ram_sbe         : 1;
+	uint64_t rt_ram_sbe                   : 1;
+	uint64_t sq_nt_ram_sbe                : 1;
+	uint64_t sq_pt_ram_sbe                : 1;
 #endif
-	} cn30xx;
-	struct cvmx_pko_mem_debug9_cn30xx cn31xx;
-	struct cvmx_pko_mem_debug9_cn38xx {
+	} s;
+	struct cvmx_pko_pse_sq3_ecc_sbe_sts0_s cn78xx;
+};
+typedef union cvmx_pko_pse_sq3_ecc_sbe_sts0 cvmx_pko_pse_sq3_ecc_sbe_sts0_t;
+
+/**
+ * cvmx_pko_pse_sq3_ecc_sbe_sts_cmb0
+ */
+union cvmx_pko_pse_sq3_ecc_sbe_sts_cmb0 {
+	uint64_t u64;
+	struct cvmx_pko_pse_sq3_ecc_sbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_28_63:36;
-		uint64_t doorbell:20;
-		uint64_t reserved_6_7:2;
-		uint64_t static_p:1;
-		uint64_t s_tail:1;
-		uint64_t static_q:1;
-		uint64_t qos:3;
-#else
-		uint64_t qos:3;
-		uint64_t static_q:1;
-		uint64_t s_tail:1;
-		uint64_t static_p:1;
-		uint64_t reserved_6_7:2;
-		uint64_t doorbell:20;
-		uint64_t reserved_28_63:36;
+	uint64_t pse_sq3_sbe_cmb0             : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_SQ3_ECC_SBE_STS0.
+                                                         To clear this bit, software must clear bits in PKO_PSE_SQ3_ECC_SBE_STS0.
+                                                         When this bit is set, the corresponding interrupt is set.
+                                                         Throws PKO_INTSN_E::PKO_PSE_SQ3_SBE_CMB0.
+                                                         INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.nt_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.pt_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.rt_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tw_0.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tw_1.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tw_2.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tw_3.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tp0_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tp1_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tp2_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tp3_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.std0_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.std1_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.std2_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.std3_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.sts0_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.sts1_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.sts2_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.sts3_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.wt_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.sc_sram */
+	uint64_t reserved_0_62                : 63;
+#else
+	uint64_t reserved_0_62                : 63;
+	uint64_t pse_sq3_sbe_cmb0             : 1;
 #endif
-	} cn38xx;
-	struct cvmx_pko_mem_debug9_cn38xx cn38xxp2;
-	struct cvmx_pko_mem_debug9_cn50xx {
+	} s;
+	struct cvmx_pko_pse_sq3_ecc_sbe_sts_cmb0_s cn78xx;
+};
+typedef union cvmx_pko_pse_sq3_ecc_sbe_sts_cmb0 cvmx_pko_pse_sq3_ecc_sbe_sts_cmb0_t;
+
+/**
+ * cvmx_pko_pse_sq4_bist_status
+ *
+ * Each bit is the BIST result of an individual memory (per bit, 0 = pass and 1 = fail).
+ *
+ */
+union cvmx_pko_pse_sq4_bist_status {
+	uint64_t u64;
+	struct cvmx_pko_pse_sq4_bist_status_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_49_63:15;
-		uint64_t ptrs0:17;
-		uint64_t reserved_17_31:15;
-		uint64_t ptrs3:17;
+	uint64_t reserved_29_63               : 35;
+	uint64_t sc_sram                      : 1;  /**< Scheduling configuration */
+	uint64_t reserved_23_27               : 5;
+	uint64_t tp3_sram                     : 1;  /**< SQ[5:3] topology parent configuration */
+	uint64_t tp2_sram                     : 1;  /**< SQ[5:3] topology parent configuration */
+	uint64_t tp1_sram                     : 1;  /**< SQ[5:1] topology parent configuration */
+	uint64_t tp0_sram                     : 1;  /**< SQ[5:1] topology parent configuration */
+	uint64_t reserved_18_18               : 1;
+	uint64_t rt_sram                      : 1;  /**< Result table */
+	uint64_t reserved_15_16               : 2;
+	uint64_t tw3_cmd_fifo                 : 1;  /**< SQ[5:3] time wheel 3 command FIFO SRAM */
+	uint64_t reserved_12_13               : 2;
+	uint64_t tw2_cmd_fifo                 : 1;  /**< SQ[5:3] time wheel 2 command FIFO SRAM. */
+	uint64_t reserved_9_10                : 2;
+	uint64_t tw1_cmd_fifo                 : 1;  /**< SQ[5:1] time wheel 1 command FIFO SRAM. */
+	uint64_t std_sram                     : 1;  /**< Dynamic shaping state. */
+	uint64_t sts_sram                     : 1;  /**< Static shaping configuration. */
+	uint64_t tw0_cmd_fifo                 : 1;  /**< SQ[5:1] time wheel 0 command FIFO SRAM. */
+	uint64_t reserved_3_4                 : 2;
+	uint64_t nt_sram                      : 1;  /**< Next pointer table. */
+	uint64_t pt_sram                      : 1;  /**< Previous pointer table. */
+	uint64_t wt_sram                      : 1;  /**< Work table. */
 #else
-		uint64_t ptrs3:17;
-		uint64_t reserved_17_31:15;
-		uint64_t ptrs0:17;
-		uint64_t reserved_49_63:15;
+	uint64_t wt_sram                      : 1;
+	uint64_t pt_sram                      : 1;
+	uint64_t nt_sram                      : 1;
+	uint64_t reserved_3_4                 : 2;
+	uint64_t tw0_cmd_fifo                 : 1;
+	uint64_t sts_sram                     : 1;
+	uint64_t std_sram                     : 1;
+	uint64_t tw1_cmd_fifo                 : 1;
+	uint64_t reserved_9_10                : 2;
+	uint64_t tw2_cmd_fifo                 : 1;
+	uint64_t reserved_12_13               : 2;
+	uint64_t tw3_cmd_fifo                 : 1;
+	uint64_t reserved_15_16               : 2;
+	uint64_t rt_sram                      : 1;
+	uint64_t reserved_18_18               : 1;
+	uint64_t tp0_sram                     : 1;
+	uint64_t tp1_sram                     : 1;
+	uint64_t tp2_sram                     : 1;
+	uint64_t tp3_sram                     : 1;
+	uint64_t reserved_23_27               : 5;
+	uint64_t sc_sram                      : 1;
+	uint64_t reserved_29_63               : 35;
 #endif
-	} cn50xx;
-	struct cvmx_pko_mem_debug9_cn50xx cn52xx;
-	struct cvmx_pko_mem_debug9_cn50xx cn52xxp1;
-	struct cvmx_pko_mem_debug9_cn50xx cn56xx;
-	struct cvmx_pko_mem_debug9_cn50xx cn56xxp1;
-	struct cvmx_pko_mem_debug9_cn50xx cn58xx;
-	struct cvmx_pko_mem_debug9_cn50xx cn58xxp1;
-	struct cvmx_pko_mem_debug9_cn50xx cn61xx;
-	struct cvmx_pko_mem_debug9_cn50xx cn63xx;
-	struct cvmx_pko_mem_debug9_cn50xx cn63xxp1;
-	struct cvmx_pko_mem_debug9_cn50xx cn66xx;
-	struct cvmx_pko_mem_debug9_cn50xx cn68xx;
-	struct cvmx_pko_mem_debug9_cn50xx cn68xxp1;
-	struct cvmx_pko_mem_debug9_cn50xx cnf71xx;
+	} s;
+	struct cvmx_pko_pse_sq4_bist_status_s cn78xx;
 };
+typedef union cvmx_pko_pse_sq4_bist_status cvmx_pko_pse_sq4_bist_status_t;
 
-union cvmx_pko_mem_iport_ptrs {
+/**
+ * cvmx_pko_pse_sq4_ecc_ctl0
+ */
+union cvmx_pko_pse_sq4_ecc_ctl0 {
 	uint64_t u64;
-	struct cvmx_pko_mem_iport_ptrs_s {
+	struct cvmx_pko_pse_sq4_ecc_ctl0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_63_63:1;
-		uint64_t crc:1;
-		uint64_t static_p:1;
-		uint64_t qos_mask:8;
-		uint64_t min_pkt:3;
-		uint64_t reserved_31_49:19;
-		uint64_t pipe:7;
-		uint64_t reserved_21_23:3;
-		uint64_t intr:5;
-		uint64_t reserved_13_15:3;
-		uint64_t eid:5;
-		uint64_t reserved_7_7:1;
-		uint64_t ipid:7;
-#else
-		uint64_t ipid:7;
-		uint64_t reserved_7_7:1;
-		uint64_t eid:5;
-		uint64_t reserved_13_15:3;
-		uint64_t intr:5;
-		uint64_t reserved_21_23:3;
-		uint64_t pipe:7;
-		uint64_t reserved_31_49:19;
-		uint64_t min_pkt:3;
-		uint64_t qos_mask:8;
-		uint64_t static_p:1;
-		uint64_t crc:1;
-		uint64_t reserved_63_63:1;
-#endif
-	} s;
-	struct cvmx_pko_mem_iport_ptrs_s cn68xx;
-	struct cvmx_pko_mem_iport_ptrs_s cn68xxp1;
+	uint64_t sq_pt_ram_flip               : 2;  /**< SQ_PT_RAM flip syndrome bits on write. */
+	uint64_t sq_pt_ram_cdis               : 1;  /**< SQ_PT_RAM ECC correction disable. */
+	uint64_t sq_nt_ram_flip               : 2;  /**< SQ_NT_RAM flip syndrome bits on write. */
+	uint64_t sq_nt_ram_cdis               : 1;  /**< SQ_NT_RAM ECC correction disable. */
+	uint64_t rt_ram_flip                  : 2;  /**< RT_RAM flip syndrome bits on write. */
+	uint64_t rt_ram_cdis                  : 1;  /**< RT_RAM ECC correction disable. */
+	uint64_t tw3_cmd_fifo_ram_flip        : 2;  /**< TW3_CMD_FIFO_RAM flip syndrome bits on write. */
+	uint64_t tw3_cmd_fifo_ram_cdis        : 1;  /**< TW3_CMD_FIFO_RAM ECC correction disable. */
+	uint64_t tw2_cmd_fifo_ram_flip        : 2;  /**< TW2_CMD_FIFO_RAM flip syndrome bits on write. */
+	uint64_t tw2_cmd_fifo_ram_cdis        : 1;  /**< TW2_CMD_FIFO_RAM ECC correction disable. */
+	uint64_t tw1_cmd_fifo_ram_flip        : 2;  /**< TW1_CMD_FIFO_RAM flip syndrome bits on write. */
+	uint64_t tw1_cmd_fifo_ram_cdis        : 1;  /**< TW1_CMD_FIFO_RAM ECC correction disable. */
+	uint64_t tw0_cmd_fifo_ram_flip        : 2;  /**< TW0_CMD_FIFO_RAM flip syndrome bits on write. */
+	uint64_t tw0_cmd_fifo_ram_cdis        : 1;  /**< TW0_CMD_FIFO_RAM ECC correction disable. */
+	uint64_t tp3_sram_flip                : 2;  /**< TP3_SRAM flip syndrome bits on write. */
+	uint64_t tp3_sram_cdis                : 1;  /**< TP3_SRAM ECC correction disable. */
+	uint64_t tp2_sram_flip                : 2;  /**< TP2_SRAM flip syndrome bits on write. */
+	uint64_t tp2_sram_cdis                : 1;  /**< TP2_SRAM ECC correction disable. */
+	uint64_t tp1_sram_flip                : 2;  /**< TP1_SRAM flip syndrome bits on write. */
+	uint64_t tp1_sram_cdis                : 1;  /**< TP1_SRAM ECC correction disable. */
+	uint64_t tp0_sram_flip                : 2;  /**< TP0_SRAM flip syndrome bits on write. */
+	uint64_t tp0_sram_cdis                : 1;  /**< TP0_SRAM ECC correction disable. */
+	uint64_t sts3_ram_flip                : 2;  /**< STS3_RAM flip syndrome bits on write. */
+	uint64_t sts3_ram_cdis                : 1;  /**< STS3_RAM ECC correction disable. */
+	uint64_t sts2_ram_flip                : 2;  /**< STS2_RAM flip syndrome bits on write. */
+	uint64_t sts2_ram_cdis                : 1;  /**< STS2_RAM ECC correction disable. */
+	uint64_t sts1_ram_flip                : 2;  /**< STS1_RAM flip syndrome bits on write. */
+	uint64_t sts1_ram_cdis                : 1;  /**< STS1_RAM ECC correction disable. */
+	uint64_t sts0_ram_flip                : 2;  /**< STS0_RAM flip syndrome bits on write. */
+	uint64_t sts0_ram_cdis                : 1;  /**< STS0_RAM ECC correction disable. */
+	uint64_t std3_ram_flip                : 2;  /**< STD3_RAM flip syndrome bits on write. */
+	uint64_t std3_ram_cdis                : 1;  /**< STD3_RAM ECC correction disable. */
+	uint64_t std2_ram_flip                : 2;  /**< STD2_RAM flip syndrome bits on write. */
+	uint64_t std2_ram_cdis                : 1;  /**< STD2_RAM ECC correction disable. */
+	uint64_t std1_ram_flip                : 2;  /**< STD1_RAM flip syndrome bits on write. */
+	uint64_t std1_ram_cdis                : 1;  /**< STD1_RAM ECC correction disable. */
+	uint64_t std0_ram_flip                : 2;  /**< STD0_RAM flip syndrome bits on write. */
+	uint64_t std0_ram_cdis                : 1;  /**< STD0_RAM ECC correction disable. */
+	uint64_t wt_ram_flip                  : 2;  /**< WT_RAM flip syndrome bits on write. */
+	uint64_t wt_ram_cdis                  : 1;  /**< WT_RAM ECC correction disable. */
+	uint64_t sc_ram_flip                  : 2;  /**< SC_RAM flip syndrome bits on write. */
+	uint64_t sc_ram_cdis                  : 1;  /**< SC_RAM ECC correction disable. */
+	uint64_t reserved_0_0                 : 1;
+#else
+	uint64_t reserved_0_0                 : 1;
+	uint64_t sc_ram_cdis                  : 1;
+	uint64_t sc_ram_flip                  : 2;
+	uint64_t wt_ram_cdis                  : 1;
+	uint64_t wt_ram_flip                  : 2;
+	uint64_t std0_ram_cdis                : 1;
+	uint64_t std0_ram_flip                : 2;
+	uint64_t std1_ram_cdis                : 1;
+	uint64_t std1_ram_flip                : 2;
+	uint64_t std2_ram_cdis                : 1;
+	uint64_t std2_ram_flip                : 2;
+	uint64_t std3_ram_cdis                : 1;
+	uint64_t std3_ram_flip                : 2;
+	uint64_t sts0_ram_cdis                : 1;
+	uint64_t sts0_ram_flip                : 2;
+	uint64_t sts1_ram_cdis                : 1;
+	uint64_t sts1_ram_flip                : 2;
+	uint64_t sts2_ram_cdis                : 1;
+	uint64_t sts2_ram_flip                : 2;
+	uint64_t sts3_ram_cdis                : 1;
+	uint64_t sts3_ram_flip                : 2;
+	uint64_t tp0_sram_cdis                : 1;
+	uint64_t tp0_sram_flip                : 2;
+	uint64_t tp1_sram_cdis                : 1;
+	uint64_t tp1_sram_flip                : 2;
+	uint64_t tp2_sram_cdis                : 1;
+	uint64_t tp2_sram_flip                : 2;
+	uint64_t tp3_sram_cdis                : 1;
+	uint64_t tp3_sram_flip                : 2;
+	uint64_t tw0_cmd_fifo_ram_cdis        : 1;
+	uint64_t tw0_cmd_fifo_ram_flip        : 2;
+	uint64_t tw1_cmd_fifo_ram_cdis        : 1;
+	uint64_t tw1_cmd_fifo_ram_flip        : 2;
+	uint64_t tw2_cmd_fifo_ram_cdis        : 1;
+	uint64_t tw2_cmd_fifo_ram_flip        : 2;
+	uint64_t tw3_cmd_fifo_ram_cdis        : 1;
+	uint64_t tw3_cmd_fifo_ram_flip        : 2;
+	uint64_t rt_ram_cdis                  : 1;
+	uint64_t rt_ram_flip                  : 2;
+	uint64_t sq_nt_ram_cdis               : 1;
+	uint64_t sq_nt_ram_flip               : 2;
+	uint64_t sq_pt_ram_cdis               : 1;
+	uint64_t sq_pt_ram_flip               : 2;
+#endif
+	} s;
+	struct cvmx_pko_pse_sq4_ecc_ctl0_s    cn78xx;
 };
+typedef union cvmx_pko_pse_sq4_ecc_ctl0 cvmx_pko_pse_sq4_ecc_ctl0_t;
 
-union cvmx_pko_mem_iport_qos {
+/**
+ * cvmx_pko_pse_sq4_ecc_dbe_sts0
+ */
+union cvmx_pko_pse_sq4_ecc_dbe_sts0 {
 	uint64_t u64;
-	struct cvmx_pko_mem_iport_qos_s {
+	struct cvmx_pko_pse_sq4_ecc_dbe_sts0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_61_63:3;
-		uint64_t qos_mask:8;
-		uint64_t reserved_13_52:40;
-		uint64_t eid:5;
-		uint64_t reserved_7_7:1;
-		uint64_t ipid:7;
+	uint64_t sq_pt_ram_dbe                : 1;  /**< Double-bit error for SQ_PT_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.pt_sram */
+	uint64_t sq_nt_ram_dbe                : 1;  /**< Double-bit error for SQ_NT_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.nt_sram */
+	uint64_t rt_ram_dbe                   : 1;  /**< Double-bit error for RT_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq5_sq3.sq5.rt_sram */
+	uint64_t tw3_cmd_fifo_ram_dbe         : 1;  /**< Double-bit error for TW3_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tw_3.sq_fifo_sram */
+	uint64_t tw2_cmd_fifo_ram_dbe         : 1;  /**< Double-bit error for TW2_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tw_2.sq_fifo_sram */
+	uint64_t tw1_cmd_fifo_ram_dbe         : 1;  /**< Double-bit error for TW1_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tw_1.sq_fifo_sram */
+	uint64_t tw0_cmd_fifo_ram_dbe         : 1;  /**< Double-bit error for TW0_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tw_0.sq_fifo_sram */
+	uint64_t tp3_sram_dbe                 : 1;  /**< Double-bit error for TP3_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tp3_sram */
+	uint64_t tp2_sram_dbe                 : 1;  /**< Double-bit error for TP2_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tp2_sram */
+	uint64_t tp1_sram_dbe                 : 1;  /**< Double-bit error for TP1_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tp1_sram */
+	uint64_t tp0_sram_dbe                 : 1;  /**< Double-bit error for TP0_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tp0_sram */
+	uint64_t sts3_ram_dbe                 : 1;  /**< Double-bit error for STS3_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.sts3_sram */
+	uint64_t sts2_ram_dbe                 : 1;  /**< Double-bit error for STS2_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.sts2_sram */
+	uint64_t sts1_ram_dbe                 : 1;  /**< Double-bit error for STS1_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.sts1_sram */
+	uint64_t sts0_ram_dbe                 : 1;  /**< Double-bit error for STS0_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.sts0_sram */
+	uint64_t std3_ram_dbe                 : 1;  /**< Double-bit error for STD3_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.std3_sram */
+	uint64_t std2_ram_dbe                 : 1;  /**< Double-bit error for STD2_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.std2_sram */
+	uint64_t std1_ram_dbe                 : 1;  /**< Double-bit error for STD1_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.std1_sram */
+	uint64_t std0_ram_dbe                 : 1;  /**< Double-bit error for STD0_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.std0_sram */
+	uint64_t wt_ram_dbe                   : 1;  /**< Double-bit error for WT_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq5_sq3.sq4.wt_sram */
+	uint64_t sc_ram_dbe                   : 1;  /**< Double-bit error for SC_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq5_sq3.sq4.sc_sram */
+	uint64_t reserved_0_42                : 43;
 #else
-		uint64_t ipid:7;
-		uint64_t reserved_7_7:1;
-		uint64_t eid:5;
-		uint64_t reserved_13_52:40;
-		uint64_t qos_mask:8;
-		uint64_t reserved_61_63:3;
+	uint64_t reserved_0_42                : 43;
+	uint64_t sc_ram_dbe                   : 1;
+	uint64_t wt_ram_dbe                   : 1;
+	uint64_t std0_ram_dbe                 : 1;
+	uint64_t std1_ram_dbe                 : 1;
+	uint64_t std2_ram_dbe                 : 1;
+	uint64_t std3_ram_dbe                 : 1;
+	uint64_t sts0_ram_dbe                 : 1;
+	uint64_t sts1_ram_dbe                 : 1;
+	uint64_t sts2_ram_dbe                 : 1;
+	uint64_t sts3_ram_dbe                 : 1;
+	uint64_t tp0_sram_dbe                 : 1;
+	uint64_t tp1_sram_dbe                 : 1;
+	uint64_t tp2_sram_dbe                 : 1;
+	uint64_t tp3_sram_dbe                 : 1;
+	uint64_t tw0_cmd_fifo_ram_dbe         : 1;
+	uint64_t tw1_cmd_fifo_ram_dbe         : 1;
+	uint64_t tw2_cmd_fifo_ram_dbe         : 1;
+	uint64_t tw3_cmd_fifo_ram_dbe         : 1;
+	uint64_t rt_ram_dbe                   : 1;
+	uint64_t sq_nt_ram_dbe                : 1;
+	uint64_t sq_pt_ram_dbe                : 1;
 #endif
 	} s;
-	struct cvmx_pko_mem_iport_qos_s cn68xx;
-	struct cvmx_pko_mem_iport_qos_s cn68xxp1;
+	struct cvmx_pko_pse_sq4_ecc_dbe_sts0_s cn78xx;
 };
+typedef union cvmx_pko_pse_sq4_ecc_dbe_sts0 cvmx_pko_pse_sq4_ecc_dbe_sts0_t;
 
-union cvmx_pko_mem_iqueue_ptrs {
+/**
+ * cvmx_pko_pse_sq4_ecc_dbe_sts_cmb0
+ */
+union cvmx_pko_pse_sq4_ecc_dbe_sts_cmb0 {
 	uint64_t u64;
-	struct cvmx_pko_mem_iqueue_ptrs_s {
+	struct cvmx_pko_pse_sq4_ecc_dbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t s_tail:1;
-		uint64_t static_p:1;
-		uint64_t static_q:1;
-		uint64_t qos_mask:8;
-		uint64_t buf_ptr:31;
-		uint64_t tail:1;
-		uint64_t index:5;
-		uint64_t reserved_15_15:1;
-		uint64_t ipid:7;
-		uint64_t qid:8;
-#else
-		uint64_t qid:8;
-		uint64_t ipid:7;
-		uint64_t reserved_15_15:1;
-		uint64_t index:5;
-		uint64_t tail:1;
-		uint64_t buf_ptr:31;
-		uint64_t qos_mask:8;
-		uint64_t static_q:1;
-		uint64_t static_p:1;
-		uint64_t s_tail:1;
-#endif
-	} s;
-	struct cvmx_pko_mem_iqueue_ptrs_s cn68xx;
-	struct cvmx_pko_mem_iqueue_ptrs_s cn68xxp1;
+	uint64_t pse_sq4_dbe_cmb0             : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_SQ4_ECC_DBE_STS0.
+                                                         To clear this bit, software must clear bits in PKO_PSE_SQ4_ECC_DBE_STS0.
+                                                         When this bit is set, the corresponding interrupt is set.
+                                                         Throws PKO_INTSN_E::PKO_PSE_SQ4_DBE_CMB0.
+                                                         INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.nt_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.pt_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.rt_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tw_0.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tw_1.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tw_2.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tw_3.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tp0_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tp1_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tp2_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tp3_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.std0_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.std1_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.std2_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.std3_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.sts0_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.sts1_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.sts2_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.sts3_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.wt_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.sc_sram */
+	uint64_t reserved_0_62                : 63;
+#else
+	uint64_t reserved_0_62                : 63;
+	uint64_t pse_sq4_dbe_cmb0             : 1;
+#endif
+	} s;
+	struct cvmx_pko_pse_sq4_ecc_dbe_sts_cmb0_s cn78xx;
 };
+typedef union cvmx_pko_pse_sq4_ecc_dbe_sts_cmb0 cvmx_pko_pse_sq4_ecc_dbe_sts_cmb0_t;
 
-union cvmx_pko_mem_iqueue_qos {
+/**
+ * cvmx_pko_pse_sq4_ecc_sbe_sts0
+ */
+union cvmx_pko_pse_sq4_ecc_sbe_sts0 {
 	uint64_t u64;
-	struct cvmx_pko_mem_iqueue_qos_s {
+	struct cvmx_pko_pse_sq4_ecc_sbe_sts0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_61_63:3;
-		uint64_t qos_mask:8;
-		uint64_t reserved_15_52:38;
-		uint64_t ipid:7;
-		uint64_t qid:8;
+	uint64_t sq_pt_ram_sbe                : 1;  /**< Single-bit error for SQ_PT_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.nt_sram */
+	uint64_t sq_nt_ram_sbe                : 1;  /**< Single-bit error for SQ_NT_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.pt_sram */
+	uint64_t rt_ram_sbe                   : 1;  /**< Single-bit error for RT_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq5_sq3.sq5.rt_sram */
+	uint64_t tw3_cmd_fifo_ram_sbe         : 1;  /**< Single-bit error for TW3_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tw_3.sq_fifo_sram */
+	uint64_t tw2_cmd_fifo_ram_sbe         : 1;  /**< Single-bit error for TW2_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tw_2.sq_fifo_sram */
+	uint64_t tw1_cmd_fifo_ram_sbe         : 1;  /**< Single-bit error for TW1_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tw_1.sq_fifo_sram */
+	uint64_t tw0_cmd_fifo_ram_sbe         : 1;  /**< Single-bit error for TW0_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tw_0.sq_fifo_sram */
+	uint64_t tp3_sram_sbe                 : 1;  /**< Single-bit error for TP3_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tp3_sram */
+	uint64_t tp2_sram_sbe                 : 1;  /**< Single-bit error for TP2_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tp2_sram */
+	uint64_t tp1_sram_sbe                 : 1;  /**< Single-bit error for TP1_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tp1_sram */
+	uint64_t tp0_sram_sbe                 : 1;  /**< Single-bit error for TP0_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tp0_sram */
+	uint64_t sts3_ram_sbe                 : 1;  /**< Single-bit error for STS3_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.sts3_sram */
+	uint64_t sts2_ram_sbe                 : 1;  /**< Single-bit error for STS2_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.sts2_sram */
+	uint64_t sts1_ram_sbe                 : 1;  /**< Single-bit error for STS1_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.sts1_sram */
+	uint64_t sts0_ram_sbe                 : 1;  /**< Single-bit error for STS0_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.sts0_sram */
+	uint64_t std3_ram_sbe                 : 1;  /**< Single-bit error for STD3_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.std3_sram */
+	uint64_t std2_ram_sbe                 : 1;  /**< Single-bit error for STD2_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.std2_sram */
+	uint64_t std1_ram_sbe                 : 1;  /**< Single-bit error for STD1_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.std1_sram */
+	uint64_t std0_ram_sbe                 : 1;  /**< Single-bit error for STD0_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.std0_sram */
+	uint64_t wt_ram_sbe                   : 1;  /**< Single-bit error for WT_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq5_sq3.sq4.wt_sram */
+	uint64_t sc_ram_sbe                   : 1;  /**< Single-bit error for SC_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq5_sq3.sq4.sc_sram */
+	uint64_t reserved_0_42                : 43;
 #else
-		uint64_t qid:8;
-		uint64_t ipid:7;
-		uint64_t reserved_15_52:38;
-		uint64_t qos_mask:8;
-		uint64_t reserved_61_63:3;
+	uint64_t reserved_0_42                : 43;
+	uint64_t sc_ram_sbe                   : 1;
+	uint64_t wt_ram_sbe                   : 1;
+	uint64_t std0_ram_sbe                 : 1;
+	uint64_t std1_ram_sbe                 : 1;
+	uint64_t std2_ram_sbe                 : 1;
+	uint64_t std3_ram_sbe                 : 1;
+	uint64_t sts0_ram_sbe                 : 1;
+	uint64_t sts1_ram_sbe                 : 1;
+	uint64_t sts2_ram_sbe                 : 1;
+	uint64_t sts3_ram_sbe                 : 1;
+	uint64_t tp0_sram_sbe                 : 1;
+	uint64_t tp1_sram_sbe                 : 1;
+	uint64_t tp2_sram_sbe                 : 1;
+	uint64_t tp3_sram_sbe                 : 1;
+	uint64_t tw0_cmd_fifo_ram_sbe         : 1;
+	uint64_t tw1_cmd_fifo_ram_sbe         : 1;
+	uint64_t tw2_cmd_fifo_ram_sbe         : 1;
+	uint64_t tw3_cmd_fifo_ram_sbe         : 1;
+	uint64_t rt_ram_sbe                   : 1;
+	uint64_t sq_nt_ram_sbe                : 1;
+	uint64_t sq_pt_ram_sbe                : 1;
 #endif
 	} s;
-	struct cvmx_pko_mem_iqueue_qos_s cn68xx;
-	struct cvmx_pko_mem_iqueue_qos_s cn68xxp1;
+	struct cvmx_pko_pse_sq4_ecc_sbe_sts0_s cn78xx;
 };
+typedef union cvmx_pko_pse_sq4_ecc_sbe_sts0 cvmx_pko_pse_sq4_ecc_sbe_sts0_t;
 
-union cvmx_pko_mem_port_ptrs {
+/**
+ * cvmx_pko_pse_sq4_ecc_sbe_sts_cmb0
+ */
+union cvmx_pko_pse_sq4_ecc_sbe_sts_cmb0 {
 	uint64_t u64;
-	struct cvmx_pko_mem_port_ptrs_s {
+	struct cvmx_pko_pse_sq4_ecc_sbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_62_63:2;
-		uint64_t static_p:1;
-		uint64_t qos_mask:8;
-		uint64_t reserved_16_52:37;
-		uint64_t bp_port:6;
-		uint64_t eid:4;
-		uint64_t pid:6;
-#else
-		uint64_t pid:6;
-		uint64_t eid:4;
-		uint64_t bp_port:6;
-		uint64_t reserved_16_52:37;
-		uint64_t qos_mask:8;
-		uint64_t static_p:1;
-		uint64_t reserved_62_63:2;
-#endif
-	} s;
-	struct cvmx_pko_mem_port_ptrs_s cn52xx;
-	struct cvmx_pko_mem_port_ptrs_s cn52xxp1;
-	struct cvmx_pko_mem_port_ptrs_s cn56xx;
-	struct cvmx_pko_mem_port_ptrs_s cn56xxp1;
-	struct cvmx_pko_mem_port_ptrs_s cn61xx;
-	struct cvmx_pko_mem_port_ptrs_s cn63xx;
-	struct cvmx_pko_mem_port_ptrs_s cn63xxp1;
-	struct cvmx_pko_mem_port_ptrs_s cn66xx;
-	struct cvmx_pko_mem_port_ptrs_s cnf71xx;
+	uint64_t pse_sq4_sbe_cmb0             : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_SQ4_ECC_SBE_STS0.
+                                                         To clear this bit, software must clear bits in PKO_PSE_SQ4_ECC_SBE_STS0.
+                                                         When this bit is set, the corresponding interrupt is set.
+                                                         Throws PKO_INTSN_E::PKO_PSE_SQ4_SBE_CMB0.
+                                                         INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.nt_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.pt_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.rt_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tw_0.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tw_1.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tw_2.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tw_3.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tp0_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tp1_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tp2_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.tp3_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.std0_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.std1_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.std2_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.std3_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.sts0_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.sts1_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.sts2_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.sts3_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.wt_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.sc_sram */
+	uint64_t reserved_0_62                : 63;
+#else
+	uint64_t reserved_0_62                : 63;
+	uint64_t pse_sq4_sbe_cmb0             : 1;
+#endif
+	} s;
+	struct cvmx_pko_pse_sq4_ecc_sbe_sts_cmb0_s cn78xx;
 };
+typedef union cvmx_pko_pse_sq4_ecc_sbe_sts_cmb0 cvmx_pko_pse_sq4_ecc_sbe_sts_cmb0_t;
 
-union cvmx_pko_mem_port_qos {
+/**
+ * cvmx_pko_pse_sq5_bist_status
+ *
+ * Each bit is the BIST result of an individual memory (per bit, 0 = pass and 1 = fail).
+ *
+ */
+union cvmx_pko_pse_sq5_bist_status {
 	uint64_t u64;
-	struct cvmx_pko_mem_port_qos_s {
+	struct cvmx_pko_pse_sq5_bist_status_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_61_63:3;
-		uint64_t qos_mask:8;
-		uint64_t reserved_10_52:43;
-		uint64_t eid:4;
-		uint64_t pid:6;
+	uint64_t reserved_29_63               : 35;
+	uint64_t sc_sram                      : 1;  /**< Scheduling configuration. */
+	uint64_t reserved_23_27               : 5;
+	uint64_t tp3_sram                     : 1;  /**< SQ[5:3] topology parent configuration. */
+	uint64_t tp2_sram                     : 1;  /**< SQ[5:3] topology parent configuration. */
+	uint64_t tp1_sram                     : 1;  /**< SQ[5:1] topology parent configuration. */
+	uint64_t tp0_sram                     : 1;  /**< SQ[5:1] topology parent configuration. */
+	uint64_t reserved_18_18               : 1;
+	uint64_t rt_sram                      : 1;  /**< Result table. */
+	uint64_t reserved_15_16               : 2;
+	uint64_t tw3_cmd_fifo                 : 1;  /**< SQ[5:3] time wheel 3 command FIFO SRAM. */
+	uint64_t reserved_12_13               : 2;
+	uint64_t tw2_cmd_fifo                 : 1;  /**< SQ[5:3] time wheel 2 command FIFO SRAM. */
+	uint64_t reserved_9_10                : 2;
+	uint64_t tw1_cmd_fifo                 : 1;  /**< SQ[5:1] time wheel 1 command FIFO SRAM. */
+	uint64_t std_sram                     : 1;  /**< Dynamic shaping state. */
+	uint64_t sts_sram                     : 1;  /**< Static shaping configuration. */
+	uint64_t tw0_cmd_fifo                 : 1;  /**< SQ[5:1] time wheel 0 command FIFO SRAM. */
+	uint64_t reserved_3_4                 : 2;
+	uint64_t nt_sram                      : 1;  /**< Next pointer table. */
+	uint64_t pt_sram                      : 1;  /**< Previous pointer table. */
+	uint64_t wt_sram                      : 1;  /**< Work table. */
 #else
-		uint64_t pid:6;
-		uint64_t eid:4;
-		uint64_t reserved_10_52:43;
-		uint64_t qos_mask:8;
-		uint64_t reserved_61_63:3;
+	uint64_t wt_sram                      : 1;
+	uint64_t pt_sram                      : 1;
+	uint64_t nt_sram                      : 1;
+	uint64_t reserved_3_4                 : 2;
+	uint64_t tw0_cmd_fifo                 : 1;
+	uint64_t sts_sram                     : 1;
+	uint64_t std_sram                     : 1;
+	uint64_t tw1_cmd_fifo                 : 1;
+	uint64_t reserved_9_10                : 2;
+	uint64_t tw2_cmd_fifo                 : 1;
+	uint64_t reserved_12_13               : 2;
+	uint64_t tw3_cmd_fifo                 : 1;
+	uint64_t reserved_15_16               : 2;
+	uint64_t rt_sram                      : 1;
+	uint64_t reserved_18_18               : 1;
+	uint64_t tp0_sram                     : 1;
+	uint64_t tp1_sram                     : 1;
+	uint64_t tp2_sram                     : 1;
+	uint64_t tp3_sram                     : 1;
+	uint64_t reserved_23_27               : 5;
+	uint64_t sc_sram                      : 1;
+	uint64_t reserved_29_63               : 35;
 #endif
 	} s;
-	struct cvmx_pko_mem_port_qos_s cn52xx;
-	struct cvmx_pko_mem_port_qos_s cn52xxp1;
-	struct cvmx_pko_mem_port_qos_s cn56xx;
-	struct cvmx_pko_mem_port_qos_s cn56xxp1;
-	struct cvmx_pko_mem_port_qos_s cn61xx;
-	struct cvmx_pko_mem_port_qos_s cn63xx;
-	struct cvmx_pko_mem_port_qos_s cn63xxp1;
-	struct cvmx_pko_mem_port_qos_s cn66xx;
-	struct cvmx_pko_mem_port_qos_s cnf71xx;
+	struct cvmx_pko_pse_sq5_bist_status_s cn78xx;
 };
+typedef union cvmx_pko_pse_sq5_bist_status cvmx_pko_pse_sq5_bist_status_t;
 
-union cvmx_pko_mem_port_rate0 {
+/**
+ * cvmx_pko_pse_sq5_ecc_ctl0
+ */
+union cvmx_pko_pse_sq5_ecc_ctl0 {
 	uint64_t u64;
-	struct cvmx_pko_mem_port_rate0_s {
+	struct cvmx_pko_pse_sq5_ecc_ctl0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_51_63:13;
-		uint64_t rate_word:19;
-		uint64_t rate_pkt:24;
-		uint64_t reserved_7_7:1;
-		uint64_t pid:7;
+	uint64_t sq_pt_ram_flip               : 2;  /**< SQ_PT_RAM flip syndrome bits on write. */
+	uint64_t sq_pt_ram_cdis               : 1;  /**< SQ_PT_RAM ECC correction disable. */
+	uint64_t sq_nt_ram_flip               : 2;  /**< SQ_NT_RAM flip syndrome bits on write. */
+	uint64_t sq_nt_ram_cdis               : 1;  /**< SQ_NT_RAM ECC correction disable. */
+	uint64_t rt_ram_flip                  : 2;  /**< RT_RAM flip syndrome bits on write. */
+	uint64_t rt_ram_cdis                  : 1;  /**< RT_RAM ECC correction disable. */
+	uint64_t tw3_cmd_fifo_ram_flip        : 2;  /**< TW3_CMD_FIFO_RAM flip syndrome bits on write. */
+	uint64_t tw3_cmd_fifo_ram_cdis        : 1;  /**< TW3_CMD_FIFO_RAM ECC correction disable. */
+	uint64_t tw2_cmd_fifo_ram_flip        : 2;  /**< TW2_CMD_FIFO_RAM flip syndrome bits on write. */
+	uint64_t tw2_cmd_fifo_ram_cdis        : 1;  /**< TW2_CMD_FIFO_RAM ECC correction disable. */
+	uint64_t tw1_cmd_fifo_ram_flip        : 2;  /**< TW1_CMD_FIFO_RAM flip syndrome bits on write. */
+	uint64_t tw1_cmd_fifo_ram_cdis        : 1;  /**< TW1_CMD_FIFO_RAM ECC correction disable. */
+	uint64_t tw0_cmd_fifo_ram_flip        : 2;  /**< TW0_CMD_FIFO_RAM flip syndrome bits on write. */
+	uint64_t tw0_cmd_fifo_ram_cdis        : 1;  /**< TW0_CMD_FIFO_RAM ECC correction disable. */
+	uint64_t tp3_sram_flip                : 2;  /**< TP3_SRAM flip syndrome bits on write. */
+	uint64_t tp3_sram_cdis                : 1;  /**< TP3_SRAM ECC correction disable. */
+	uint64_t tp2_sram_flip                : 2;  /**< TP2_SRAM flip syndrome bits on write. */
+	uint64_t tp2_sram_cdis                : 1;  /**< TP2_SRAM ECC correction disable. */
+	uint64_t tp1_sram_flip                : 2;  /**< TP1_SRAM flip syndrome bits on write. */
+	uint64_t tp1_sram_cdis                : 1;  /**< TP1_SRAM ECC correction disable. */
+	uint64_t tp0_sram_flip                : 2;  /**< TP0_SRAM flip syndrome bits on write. */
+	uint64_t tp0_sram_cdis                : 1;  /**< TP0_SRAM ECC correction disable. */
+	uint64_t sts3_ram_flip                : 2;  /**< STS3_RAM flip syndrome bits on write. */
+	uint64_t sts3_ram_cdis                : 1;  /**< STS3_RAM ECC correction disable. */
+	uint64_t sts2_ram_flip                : 2;  /**< STS2_RAM flip syndrome bits on write. */
+	uint64_t sts2_ram_cdis                : 1;  /**< STS2_RAM ECC correction disable. */
+	uint64_t sts1_ram_flip                : 2;  /**< STS1_RAM flip syndrome bits on write. */
+	uint64_t sts1_ram_cdis                : 1;  /**< STS1_RAM ECC correction disable. */
+	uint64_t sts0_ram_flip                : 2;  /**< STS0_RAM flip syndrome bits on write. */
+	uint64_t sts0_ram_cdis                : 1;  /**< STS0_RAM ECC correction disable. */
+	uint64_t std3_ram_flip                : 2;  /**< STD3_RAM flip syndrome bits on write. */
+	uint64_t std3_ram_cdis                : 1;  /**< STD3_RAM ECC correction disable. */
+	uint64_t std2_ram_flip                : 2;  /**< STD2_RAM flip syndrome bits on write. */
+	uint64_t std2_ram_cdis                : 1;  /**< STD2_RAM ECC correction disable. */
+	uint64_t std1_ram_flip                : 2;  /**< STD1_RAM flip syndrome bits on write. */
+	uint64_t std1_ram_cdis                : 1;  /**< STD1_RAM ECC correction disable. */
+	uint64_t std0_ram_flip                : 2;  /**< STD0_RAM flip syndrome bits on write. */
+	uint64_t std0_ram_cdis                : 1;  /**< STD0_RAM ECC correction disable. */
+	uint64_t wt_ram_flip                  : 2;  /**< WT_RAM flip syndrome bits on write. */
+	uint64_t wt_ram_cdis                  : 1;  /**< WT_RAM ECC correction disable. */
+	uint64_t sc_ram_flip                  : 2;  /**< SC_RAM flip syndrome bits on write. */
+	uint64_t sc_ram_cdis                  : 1;  /**< SC_RAM ECC correction disable. */
+	uint64_t reserved_0_0                 : 1;
 #else
-		uint64_t pid:7;
-		uint64_t reserved_7_7:1;
-		uint64_t rate_pkt:24;
-		uint64_t rate_word:19;
-		uint64_t reserved_51_63:13;
+	uint64_t reserved_0_0                 : 1;
+	uint64_t sc_ram_cdis                  : 1;
+	uint64_t sc_ram_flip                  : 2;
+	uint64_t wt_ram_cdis                  : 1;
+	uint64_t wt_ram_flip                  : 2;
+	uint64_t std0_ram_cdis                : 1;
+	uint64_t std0_ram_flip                : 2;
+	uint64_t std1_ram_cdis                : 1;
+	uint64_t std1_ram_flip                : 2;
+	uint64_t std2_ram_cdis                : 1;
+	uint64_t std2_ram_flip                : 2;
+	uint64_t std3_ram_cdis                : 1;
+	uint64_t std3_ram_flip                : 2;
+	uint64_t sts0_ram_cdis                : 1;
+	uint64_t sts0_ram_flip                : 2;
+	uint64_t sts1_ram_cdis                : 1;
+	uint64_t sts1_ram_flip                : 2;
+	uint64_t sts2_ram_cdis                : 1;
+	uint64_t sts2_ram_flip                : 2;
+	uint64_t sts3_ram_cdis                : 1;
+	uint64_t sts3_ram_flip                : 2;
+	uint64_t tp0_sram_cdis                : 1;
+	uint64_t tp0_sram_flip                : 2;
+	uint64_t tp1_sram_cdis                : 1;
+	uint64_t tp1_sram_flip                : 2;
+	uint64_t tp2_sram_cdis                : 1;
+	uint64_t tp2_sram_flip                : 2;
+	uint64_t tp3_sram_cdis                : 1;
+	uint64_t tp3_sram_flip                : 2;
+	uint64_t tw0_cmd_fifo_ram_cdis        : 1;
+	uint64_t tw0_cmd_fifo_ram_flip        : 2;
+	uint64_t tw1_cmd_fifo_ram_cdis        : 1;
+	uint64_t tw1_cmd_fifo_ram_flip        : 2;
+	uint64_t tw2_cmd_fifo_ram_cdis        : 1;
+	uint64_t tw2_cmd_fifo_ram_flip        : 2;
+	uint64_t tw3_cmd_fifo_ram_cdis        : 1;
+	uint64_t tw3_cmd_fifo_ram_flip        : 2;
+	uint64_t rt_ram_cdis                  : 1;
+	uint64_t rt_ram_flip                  : 2;
+	uint64_t sq_nt_ram_cdis               : 1;
+	uint64_t sq_nt_ram_flip               : 2;
+	uint64_t sq_pt_ram_cdis               : 1;
+	uint64_t sq_pt_ram_flip               : 2;
 #endif
 	} s;
-	struct cvmx_pko_mem_port_rate0_cn52xx {
+	struct cvmx_pko_pse_sq5_ecc_ctl0_s    cn78xx;
+};
+typedef union cvmx_pko_pse_sq5_ecc_ctl0 cvmx_pko_pse_sq5_ecc_ctl0_t;
+
+/**
+ * cvmx_pko_pse_sq5_ecc_dbe_sts0
+ */
+union cvmx_pko_pse_sq5_ecc_dbe_sts0 {
+	uint64_t u64;
+	struct cvmx_pko_pse_sq5_ecc_dbe_sts0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_51_63:13;
-		uint64_t rate_word:19;
-		uint64_t rate_pkt:24;
-		uint64_t reserved_6_7:2;
-		uint64_t pid:6;
+	uint64_t sq_pt_ram_dbe                : 1;  /**< Double-bit error for SQ_PT_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.pt_sram */
+	uint64_t sq_nt_ram_dbe                : 1;  /**< Double-bit error for SQ_NT_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.nt_sram */
+	uint64_t rt_ram_dbe                   : 1;  /**< Double-bit error for RT_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq5_sq3.sq4.rt_sram */
+	uint64_t tw3_cmd_fifo_ram_dbe         : 1;  /**< Double-bit error for TW3_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tw_3.sq_fifo_sram */
+	uint64_t tw2_cmd_fifo_ram_dbe         : 1;  /**< Double-bit error for TW2_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tw_2.sq_fifo_sram */
+	uint64_t tw1_cmd_fifo_ram_dbe         : 1;  /**< Double-bit error for TW1_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tw_1.sq_fifo_sram */
+	uint64_t tw0_cmd_fifo_ram_dbe         : 1;  /**< Double-bit error for TW0_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tw_0.sq_fifo_sram */
+	uint64_t tp3_sram_dbe                 : 1;  /**< Double-bit error for TP3_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tp3_sram */
+	uint64_t tp2_sram_dbe                 : 1;  /**< Double-bit error for TP2_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tp2_sram */
+	uint64_t tp1_sram_dbe                 : 1;  /**< Double-bit error for TP1_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tp1_sram */
+	uint64_t tp0_sram_dbe                 : 1;  /**< Double-bit error for TP0_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tp0_sram */
+	uint64_t sts3_ram_dbe                 : 1;  /**< Double-bit error for STS3_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.sts3_sram */
+	uint64_t sts2_ram_dbe                 : 1;  /**< Double-bit error for STS2_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.sts2_sram */
+	uint64_t sts1_ram_dbe                 : 1;  /**< Double-bit error for STS1_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.sts1_sram */
+	uint64_t sts0_ram_dbe                 : 1;  /**< Double-bit error for STS0_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.sts0_sram */
+	uint64_t std3_ram_dbe                 : 1;  /**< Double-bit error for STD3_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.std3_sram */
+	uint64_t std2_ram_dbe                 : 1;  /**< Double-bit error for STD2_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.std2_sram */
+	uint64_t std1_ram_dbe                 : 1;  /**< Double-bit error for STD1_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.std1_sram */
+	uint64_t std0_ram_dbe                 : 1;  /**< Double-bit error for STD0_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.std0_sram */
+	uint64_t wt_ram_dbe                   : 1;  /**< Double-bit error for WT_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq5_sq3.sq5.wt_sram */
+	uint64_t sc_ram_dbe                   : 1;  /**< Double-bit error for SC_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq5_sq3.sq5.sc_sram */
+	uint64_t reserved_0_42                : 43;
 #else
-		uint64_t pid:6;
-		uint64_t reserved_6_7:2;
-		uint64_t rate_pkt:24;
-		uint64_t rate_word:19;
-		uint64_t reserved_51_63:13;
+	uint64_t reserved_0_42                : 43;
+	uint64_t sc_ram_dbe                   : 1;
+	uint64_t wt_ram_dbe                   : 1;
+	uint64_t std0_ram_dbe                 : 1;
+	uint64_t std1_ram_dbe                 : 1;
+	uint64_t std2_ram_dbe                 : 1;
+	uint64_t std3_ram_dbe                 : 1;
+	uint64_t sts0_ram_dbe                 : 1;
+	uint64_t sts1_ram_dbe                 : 1;
+	uint64_t sts2_ram_dbe                 : 1;
+	uint64_t sts3_ram_dbe                 : 1;
+	uint64_t tp0_sram_dbe                 : 1;
+	uint64_t tp1_sram_dbe                 : 1;
+	uint64_t tp2_sram_dbe                 : 1;
+	uint64_t tp3_sram_dbe                 : 1;
+	uint64_t tw0_cmd_fifo_ram_dbe         : 1;
+	uint64_t tw1_cmd_fifo_ram_dbe         : 1;
+	uint64_t tw2_cmd_fifo_ram_dbe         : 1;
+	uint64_t tw3_cmd_fifo_ram_dbe         : 1;
+	uint64_t rt_ram_dbe                   : 1;
+	uint64_t sq_nt_ram_dbe                : 1;
+	uint64_t sq_pt_ram_dbe                : 1;
 #endif
-	} cn52xx;
-	struct cvmx_pko_mem_port_rate0_cn52xx cn52xxp1;
-	struct cvmx_pko_mem_port_rate0_cn52xx cn56xx;
-	struct cvmx_pko_mem_port_rate0_cn52xx cn56xxp1;
-	struct cvmx_pko_mem_port_rate0_cn52xx cn61xx;
-	struct cvmx_pko_mem_port_rate0_cn52xx cn63xx;
-	struct cvmx_pko_mem_port_rate0_cn52xx cn63xxp1;
-	struct cvmx_pko_mem_port_rate0_cn52xx cn66xx;
-	struct cvmx_pko_mem_port_rate0_s cn68xx;
-	struct cvmx_pko_mem_port_rate0_s cn68xxp1;
-	struct cvmx_pko_mem_port_rate0_cn52xx cnf71xx;
+	} s;
+	struct cvmx_pko_pse_sq5_ecc_dbe_sts0_s cn78xx;
 };
+typedef union cvmx_pko_pse_sq5_ecc_dbe_sts0 cvmx_pko_pse_sq5_ecc_dbe_sts0_t;
 
-union cvmx_pko_mem_port_rate1 {
+/**
+ * cvmx_pko_pse_sq5_ecc_dbe_sts_cmb0
+ */
+union cvmx_pko_pse_sq5_ecc_dbe_sts_cmb0 {
 	uint64_t u64;
-	struct cvmx_pko_mem_port_rate1_s {
+	struct cvmx_pko_pse_sq5_ecc_dbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_32_63:32;
-		uint64_t rate_lim:24;
-		uint64_t reserved_7_7:1;
-		uint64_t pid:7;
+	uint64_t pse_sq5_dbe_cmb0             : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_SQ5_ECC_DBE_STS0.
+                                                         To clear this bit, software must clear bits in PKO_PSE_SQ5_ECC_DBE_STS0.
+                                                         When this bit is set, the corresponding interrupt is set.
+                                                         Throws PKO_INTSN_E::PKO_PSE_SQ5_DBE_CMB0.
+                                                         INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.nt_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.pt_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.rt_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tw_0.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tw_1.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tw_2.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tw_3.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tp0_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tp1_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tp2_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tp3_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.std0_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.std1_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.std2_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.std3_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.sts0_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.sts1_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.sts2_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.sts3_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.wt_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.sc_sram */
+	uint64_t reserved_0_62                : 63;
 #else
-		uint64_t pid:7;
-		uint64_t reserved_7_7:1;
-		uint64_t rate_lim:24;
-		uint64_t reserved_32_63:32;
+	uint64_t reserved_0_62                : 63;
+	uint64_t pse_sq5_dbe_cmb0             : 1;
 #endif
 	} s;
-	struct cvmx_pko_mem_port_rate1_cn52xx {
+	struct cvmx_pko_pse_sq5_ecc_dbe_sts_cmb0_s cn78xx;
+};
+typedef union cvmx_pko_pse_sq5_ecc_dbe_sts_cmb0 cvmx_pko_pse_sq5_ecc_dbe_sts_cmb0_t;
+
+/**
+ * cvmx_pko_pse_sq5_ecc_sbe_sts0
+ */
+union cvmx_pko_pse_sq5_ecc_sbe_sts0 {
+	uint64_t u64;
+	struct cvmx_pko_pse_sq5_ecc_sbe_sts0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_32_63:32;
-		uint64_t rate_lim:24;
-		uint64_t reserved_6_7:2;
-		uint64_t pid:6;
+	uint64_t sq_pt_ram_sbe                : 1;  /**< Single-bit error for SQ_PT_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.pt_sram */
+	uint64_t sq_nt_ram_sbe                : 1;  /**< Single-bit error for SQ_NT_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.nt_sram */
+	uint64_t rt_ram_sbe                   : 1;  /**< Single-bit error for RT_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq5_sq3.sq4.rt_sram */
+	uint64_t tw3_cmd_fifo_ram_sbe         : 1;  /**< Single-bit error for TW3_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tw_3.sq_fifo_sram */
+	uint64_t tw2_cmd_fifo_ram_sbe         : 1;  /**< Single-bit error for TW2_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tw_2.sq_fifo_sram */
+	uint64_t tw1_cmd_fifo_ram_sbe         : 1;  /**< Single-bit error for TW1_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tw_1.sq_fifo_sram */
+	uint64_t tw0_cmd_fifo_ram_sbe         : 1;  /**< Single-bit error for TW0_CMD_FIFO_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tw_0.sq_fifo_sram */
+	uint64_t tp3_sram_sbe                 : 1;  /**< Single-bit error for TP3_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tp3_sram */
+	uint64_t tp2_sram_sbe                 : 1;  /**< Single-bit error for TP2_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tp2_sram */
+	uint64_t tp1_sram_sbe                 : 1;  /**< Single-bit error for TP1_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tp1_sram */
+	uint64_t tp0_sram_sbe                 : 1;  /**< Single-bit error for TP0_SRAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tp0_sram */
+	uint64_t sts3_ram_sbe                 : 1;  /**< Single-bit error for STS3_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.sts3_sram */
+	uint64_t sts2_ram_sbe                 : 1;  /**< Single-bit error for STS2_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.sts2_sram */
+	uint64_t sts1_ram_sbe                 : 1;  /**< Single-bit error for STS1_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.sts1_sram */
+	uint64_t sts0_ram_sbe                 : 1;  /**< Single-bit error for STS0_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.sts0_sram */
+	uint64_t std3_ram_sbe                 : 1;  /**< Single-bit error for STD3_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.std3_sram */
+	uint64_t std2_ram_sbe                 : 1;  /**< Single-bit error for STD2_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.std2_sram */
+	uint64_t std1_ram_sbe                 : 1;  /**< Single-bit error for STD1_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.std1_sram */
+	uint64_t std0_ram_sbe                 : 1;  /**< Single-bit error for STD0_RAM. INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.std0_sram */
+	uint64_t wt_ram_sbe                   : 1;  /**< Single-bit error for WT_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq5_sq3.sq5.wt_sram */
+	uint64_t sc_ram_sbe                   : 1;  /**< Single-bit error for SC_RAM. INTERNAL: Instances: pko_pnr2.pko_pse.pse_sq5_sq3.sq5.sc_sram */
+	uint64_t reserved_0_42                : 43;
 #else
-		uint64_t pid:6;
-		uint64_t reserved_6_7:2;
-		uint64_t rate_lim:24;
-		uint64_t reserved_32_63:32;
+	uint64_t reserved_0_42                : 43;
+	uint64_t sc_ram_sbe                   : 1;
+	uint64_t wt_ram_sbe                   : 1;
+	uint64_t std0_ram_sbe                 : 1;
+	uint64_t std1_ram_sbe                 : 1;
+	uint64_t std2_ram_sbe                 : 1;
+	uint64_t std3_ram_sbe                 : 1;
+	uint64_t sts0_ram_sbe                 : 1;
+	uint64_t sts1_ram_sbe                 : 1;
+	uint64_t sts2_ram_sbe                 : 1;
+	uint64_t sts3_ram_sbe                 : 1;
+	uint64_t tp0_sram_sbe                 : 1;
+	uint64_t tp1_sram_sbe                 : 1;
+	uint64_t tp2_sram_sbe                 : 1;
+	uint64_t tp3_sram_sbe                 : 1;
+	uint64_t tw0_cmd_fifo_ram_sbe         : 1;
+	uint64_t tw1_cmd_fifo_ram_sbe         : 1;
+	uint64_t tw2_cmd_fifo_ram_sbe         : 1;
+	uint64_t tw3_cmd_fifo_ram_sbe         : 1;
+	uint64_t rt_ram_sbe                   : 1;
+	uint64_t sq_nt_ram_sbe                : 1;
+	uint64_t sq_pt_ram_sbe                : 1;
 #endif
-	} cn52xx;
-	struct cvmx_pko_mem_port_rate1_cn52xx cn52xxp1;
-	struct cvmx_pko_mem_port_rate1_cn52xx cn56xx;
-	struct cvmx_pko_mem_port_rate1_cn52xx cn56xxp1;
-	struct cvmx_pko_mem_port_rate1_cn52xx cn61xx;
-	struct cvmx_pko_mem_port_rate1_cn52xx cn63xx;
-	struct cvmx_pko_mem_port_rate1_cn52xx cn63xxp1;
-	struct cvmx_pko_mem_port_rate1_cn52xx cn66xx;
-	struct cvmx_pko_mem_port_rate1_s cn68xx;
-	struct cvmx_pko_mem_port_rate1_s cn68xxp1;
-	struct cvmx_pko_mem_port_rate1_cn52xx cnf71xx;
+	} s;
+	struct cvmx_pko_pse_sq5_ecc_sbe_sts0_s cn78xx;
 };
+typedef union cvmx_pko_pse_sq5_ecc_sbe_sts0 cvmx_pko_pse_sq5_ecc_sbe_sts0_t;
 
-union cvmx_pko_mem_queue_ptrs {
+/**
+ * cvmx_pko_pse_sq5_ecc_sbe_sts_cmb0
+ */
+union cvmx_pko_pse_sq5_ecc_sbe_sts_cmb0 {
 	uint64_t u64;
-	struct cvmx_pko_mem_queue_ptrs_s {
+	struct cvmx_pko_pse_sq5_ecc_sbe_sts_cmb0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t s_tail:1;
-		uint64_t static_p:1;
-		uint64_t static_q:1;
-		uint64_t qos_mask:8;
-		uint64_t buf_ptr:36;
-		uint64_t tail:1;
-		uint64_t index:3;
-		uint64_t port:6;
-		uint64_t queue:7;
-#else
-		uint64_t queue:7;
-		uint64_t port:6;
-		uint64_t index:3;
-		uint64_t tail:1;
-		uint64_t buf_ptr:36;
-		uint64_t qos_mask:8;
-		uint64_t static_q:1;
-		uint64_t static_p:1;
-		uint64_t s_tail:1;
-#endif
-	} s;
-	struct cvmx_pko_mem_queue_ptrs_s cn30xx;
-	struct cvmx_pko_mem_queue_ptrs_s cn31xx;
-	struct cvmx_pko_mem_queue_ptrs_s cn38xx;
-	struct cvmx_pko_mem_queue_ptrs_s cn38xxp2;
-	struct cvmx_pko_mem_queue_ptrs_s cn50xx;
-	struct cvmx_pko_mem_queue_ptrs_s cn52xx;
-	struct cvmx_pko_mem_queue_ptrs_s cn52xxp1;
-	struct cvmx_pko_mem_queue_ptrs_s cn56xx;
-	struct cvmx_pko_mem_queue_ptrs_s cn56xxp1;
-	struct cvmx_pko_mem_queue_ptrs_s cn58xx;
-	struct cvmx_pko_mem_queue_ptrs_s cn58xxp1;
-	struct cvmx_pko_mem_queue_ptrs_s cn61xx;
-	struct cvmx_pko_mem_queue_ptrs_s cn63xx;
-	struct cvmx_pko_mem_queue_ptrs_s cn63xxp1;
-	struct cvmx_pko_mem_queue_ptrs_s cn66xx;
-	struct cvmx_pko_mem_queue_ptrs_s cnf71xx;
+	uint64_t pse_sq5_sbe_cmb0             : 1;  /**< This bit is the logical OR of all bits in PKO_PSE_SQ5_ECC_SBE_STS0.
+                                                         To clear this bit, software must clear bits in PKO_PSE_SQ5_ECC_SBE_STS0.
+                                                         When this bit is set, the corresponding interrupt is set.
+                                                         Throws PKO_INTSN_E::PKO_PSE_SQ5_SBE_CMB0.
+                                                         INTERNAL: Instances:
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.nt_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.pt_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.rt_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tw_0.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tw_1.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tw_2.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.tw_3.sq_fifo_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tp0_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tp1_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tp2_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq4.tp3_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.std0_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.std1_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.std2_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.std3_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.sts0_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.sts1_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.sts2_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq3.sts3_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.wt_sram
+                                                         pko_pnr2.pko_pse.pse_sq5_sq3.sq5.sc_sram */
+	uint64_t reserved_0_62                : 63;
+#else
+	uint64_t reserved_0_62                : 63;
+	uint64_t pse_sq5_sbe_cmb0             : 1;
+#endif
+	} s;
+	struct cvmx_pko_pse_sq5_ecc_sbe_sts_cmb0_s cn78xx;
 };
+typedef union cvmx_pko_pse_sq5_ecc_sbe_sts_cmb0 cvmx_pko_pse_sq5_ecc_sbe_sts_cmb0_t;
 
-union cvmx_pko_mem_queue_qos {
+/**
+ * cvmx_pko_ptf#_status
+ */
+union cvmx_pko_ptfx_status {
 	uint64_t u64;
-	struct cvmx_pko_mem_queue_qos_s {
+	struct cvmx_pko_ptfx_status_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_61_63:3;
-		uint64_t qos_mask:8;
-		uint64_t reserved_13_52:40;
-		uint64_t pid:6;
-		uint64_t qid:7;
-#else
-		uint64_t qid:7;
-		uint64_t pid:6;
-		uint64_t reserved_13_52:40;
-		uint64_t qos_mask:8;
-		uint64_t reserved_61_63:3;
-#endif
-	} s;
-	struct cvmx_pko_mem_queue_qos_s cn30xx;
-	struct cvmx_pko_mem_queue_qos_s cn31xx;
-	struct cvmx_pko_mem_queue_qos_s cn38xx;
-	struct cvmx_pko_mem_queue_qos_s cn38xxp2;
-	struct cvmx_pko_mem_queue_qos_s cn50xx;
-	struct cvmx_pko_mem_queue_qos_s cn52xx;
-	struct cvmx_pko_mem_queue_qos_s cn52xxp1;
-	struct cvmx_pko_mem_queue_qos_s cn56xx;
-	struct cvmx_pko_mem_queue_qos_s cn56xxp1;
-	struct cvmx_pko_mem_queue_qos_s cn58xx;
-	struct cvmx_pko_mem_queue_qos_s cn58xxp1;
-	struct cvmx_pko_mem_queue_qos_s cn61xx;
-	struct cvmx_pko_mem_queue_qos_s cn63xx;
-	struct cvmx_pko_mem_queue_qos_s cn63xxp1;
-	struct cvmx_pko_mem_queue_qos_s cn66xx;
-	struct cvmx_pko_mem_queue_qos_s cnf71xx;
+	uint64_t reserved_20_63               : 44;
+	uint64_t total_in_flight_cnt          : 8;  /**< Total number of packets currently in-flight within PEB. Useful both for reconfiguration
+                                                         (able to disable a FIFO when it is empty) and debugging. */
+	uint64_t in_flight_cnt                : 7;  /**< Number of packets currently in-flight within PEB for this link. Useful both for
+                                                         reconfiguration (able to disable a FIFO when it is empty) and debugging. */
+	uint64_t mac_num                      : 5;  /**< MAC assigned to the given PKO TX FIFO. A value of 0x1F means unassigned. These register
+                                                         values are derived automatically by the hardware from the PKO_MAC()_CFG[FIFO_NUM]
+                                                         settings. */
+#else
+	uint64_t mac_num                      : 5;
+	uint64_t in_flight_cnt                : 7;
+	uint64_t total_in_flight_cnt          : 8;
+	uint64_t reserved_20_63               : 44;
+#endif
+	} s;
+	struct cvmx_pko_ptfx_status_s         cn78xx;
 };
+typedef union cvmx_pko_ptfx_status cvmx_pko_ptfx_status_t;
 
-union cvmx_pko_mem_throttle_int {
+/**
+ * cvmx_pko_ptf_iobp_cfg
+ */
+union cvmx_pko_ptf_iobp_cfg {
 	uint64_t u64;
-	struct cvmx_pko_mem_throttle_int_s {
+	struct cvmx_pko_ptf_iobp_cfg_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_47_63:17;
-		uint64_t word:15;
-		uint64_t reserved_14_31:18;
-		uint64_t packet:6;
-		uint64_t reserved_5_7:3;
-		uint64_t intr:5;
+	uint64_t reserved_43_63               : 21;
+	uint64_t iobp0_l2_allocate            : 1;  /**< Determine L2 allocation (1 = no allocation) when performing IOBP0 requests. */
+	uint64_t iobp1_magic_addr             : 35; /**< IOBP1 read address to be used for any dummy reads */
+	uint64_t max_read_size                : 7;  /**< Maximum number of IOBP1 read requests outstanding to be allowed by any given PEB TX FIFO. */
 #else
-		uint64_t intr:5;
-		uint64_t reserved_5_7:3;
-		uint64_t packet:6;
-		uint64_t reserved_14_31:18;
-		uint64_t word:15;
-		uint64_t reserved_47_63:17;
+	uint64_t max_read_size                : 7;
+	uint64_t iobp1_magic_addr             : 35;
+	uint64_t iobp0_l2_allocate            : 1;
+	uint64_t reserved_43_63               : 21;
 #endif
 	} s;
-	struct cvmx_pko_mem_throttle_int_s cn68xx;
-	struct cvmx_pko_mem_throttle_int_s cn68xxp1;
+	struct cvmx_pko_ptf_iobp_cfg_s        cn78xx;
 };
+typedef union cvmx_pko_ptf_iobp_cfg cvmx_pko_ptf_iobp_cfg_t;
 
-union cvmx_pko_mem_throttle_pipe {
+/**
+ * cvmx_pko_ptgf#_cfg
+ *
+ * This register configurations specific to the PKO TX FIFO groups (clusters of four PKO TX FIFOs).
+ *
+ */
+union cvmx_pko_ptgfx_cfg {
 	uint64_t u64;
-	struct cvmx_pko_mem_throttle_pipe_s {
+	struct cvmx_pko_ptgfx_cfg_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_47_63:17;
-		uint64_t word:15;
-		uint64_t reserved_14_31:18;
-		uint64_t packet:6;
-		uint64_t reserved_7_7:1;
-		uint64_t pipe:7;
+	uint64_t reserved_7_63                : 57;
+	uint64_t reset                        : 1;  /**< This bit resets the address pointers for the FIFOs in this group. This should only be
+                                                         performed when a PTGF is empty and the SIZE field is to be being changed. */
+	uint64_t rate                         : 3;  /**< Each PTGF can support up to 100 Gb/s. The total aggregate rate across all FIFOs (including
+                                                         the NULL) should never exceed 250 Gb/s. This field represents the rate for each active
+                                                         FIFO
+                                                         in PEB; thus the calculation for throughput is a function of the SIZE field and whether or
+                                                         not the FIFO is assigned to a MAC in PKO_MAC()_CFG.
+                                                         0x0 = 6.25 Gb/s.
+                                                         0x1 = 12.5 Gb/s.
+                                                         0x2 = 25 Gb/s.
+                                                         0x3 = 50 Gb/s.
+                                                         0x4 = 100 Gb/s.
+                                                         else reserved. */
+	uint64_t size                         : 3;  /**< "PKO supports up to 29 independent TX FIFOs where 0-27 are physical and 28 is virtual. The
+                                                         FIFOs are grouped into 8 sets of four contiguously numbered queues where each FIFO has a
+                                                         base storage amount of 2.5K bytes of buffering.
+                                                         _ PKO_PTGF(0)_CFG -> FIFO 0-3
+                                                         _ PKO_PTGF(1)_CFG -> FIFO 4-7
+                                                         _ PKO_PTGF(2)_CFG -> FIFO 8-11
+                                                         _ PKO_PTGF(3)_CFG -> FIFO 12-15
+                                                         _ PKO_PTGF(4)_CFG -> FIFO 16-19
+                                                         _ PKO_PTGF(5)_CFG -> FIFO 20-23
+                                                         _ PKO_PTGF(6)_CFG -> FIFO 24-27
+                                                         _ PKO_PTGF(7)_CFG -> FIFO 28 (Virtual/NULL)
+                                                         Within each set, 2 or 4 FIFOs can be combined to produce a larger FIFO if desired. The
+                                                         SIZE field is used to configure the number and depth of the FIFOs in a set. The supported
+                                                         options for a FIFO set are as follows:
+                                                         _ SIZE: Set of 4 Contiguously Numbered FIFOs
+                                                         _ ------------------------------------------
+                                                         _ xxx      Queue0  Queue1  Queue2  Queue3
+                                                         _ 000 :     2.5k    2.5k    2.5k    2.5k
+                                                         _ 001 :     5.0k    0.0k    2.5k    2.5k
+                                                         _ 010 :     2.5k    2.5k    5.0k    0.0k
+                                                         _ 011 :     5.0k    0.0k    5.0k    0.0k
+                                                         _ 100 :    10.0k    0.0k    0.0k    0.0k
+                                                         Note: 101-111 are illegal SIZE values and should not be used.
+                                                         Note that when a FIFO is set to a size of 0K bytes, FIFO_NUM is no longer legal and cannot
+                                                         be assigned to an active MAC. For example, for the set of FIFOs 8-11, if the
+                                                         PKO_PTGF(2)_CFG[SIZE] = 3'b100, then FIFO_NUMs 9, 10 and 11 are no longer valid. Only
+                                                         FIFO_NUM = 8 is available from this set for assignment to a MAC because all of the 10
+                                                         Kbytes of buffering was configured to FIFO 8.
+                                                         FIFO_NUM = 28 is a virtual FIFO and is used exclusively to indicate the NULL FIFO. Packets
+                                                         targeting the NULL FIFO are dropped by PKO and their buffers returned to the FPA. The SIZE
+                                                         field for PKO_PTGF(7) should always be set to zero. Modifications to this field require
+                                                         two writes. The first write must assert PKO_PTGF()_CFG[RESET] to reset the address
+                                                         pointers for the FIFOS in this group. The second write clears the RESET bit as well as
+                                                         configures the new SIZE values." */
 #else
-		uint64_t pipe:7;
-		uint64_t reserved_7_7:1;
-		uint64_t packet:6;
-		uint64_t reserved_14_31:18;
-		uint64_t word:15;
-		uint64_t reserved_47_63:17;
+	uint64_t size                         : 3;
+	uint64_t rate                         : 3;
+	uint64_t reset                        : 1;
+	uint64_t reserved_7_63                : 57;
 #endif
 	} s;
-	struct cvmx_pko_mem_throttle_pipe_s cn68xx;
-	struct cvmx_pko_mem_throttle_pipe_s cn68xxp1;
+	struct cvmx_pko_ptgfx_cfg_s           cn78xx;
 };
+typedef union cvmx_pko_ptgfx_cfg cvmx_pko_ptgfx_cfg_t;
 
+/**
+ * cvmx_pko_reg_bist_result
+ *
+ * Notes:
+ * Access to the internal BiST results
+ * Each bit is the BiST result of an individual memory (per bit, 0=pass and 1=fail).
+ */
 union cvmx_pko_reg_bist_result {
 	uint64_t u64;
 	struct cvmx_pko_reg_bist_result_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_0_63:64;
+	uint64_t reserved_0_63                : 64;
 #else
-		uint64_t reserved_0_63:64;
+	uint64_t reserved_0_63                : 64;
 #endif
 	} s;
 	struct cvmx_pko_reg_bist_result_cn30xx {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_27_63:37;
-		uint64_t psb2:5;
-		uint64_t count:1;
-		uint64_t rif:1;
-		uint64_t wif:1;
-		uint64_t ncb:1;
-		uint64_t out:1;
-		uint64_t crc:1;
-		uint64_t chk:1;
-		uint64_t qsb:2;
-		uint64_t qcb:2;
-		uint64_t pdb:4;
-		uint64_t psb:7;
-#else
-		uint64_t psb:7;
-		uint64_t pdb:4;
-		uint64_t qcb:2;
-		uint64_t qsb:2;
-		uint64_t chk:1;
-		uint64_t crc:1;
-		uint64_t out:1;
-		uint64_t ncb:1;
-		uint64_t wif:1;
-		uint64_t rif:1;
-		uint64_t count:1;
-		uint64_t psb2:5;
-		uint64_t reserved_27_63:37;
+	uint64_t reserved_27_63               : 37;
+	uint64_t psb2                         : 5;  /**< BiST result of the PSB   memories (0=pass, !0=fail) */
+	uint64_t count                        : 1;  /**< BiST result of the COUNT memories (0=pass, !0=fail) */
+	uint64_t rif                          : 1;  /**< BiST result of the RIF   memories (0=pass, !0=fail) */
+	uint64_t wif                          : 1;  /**< BiST result of the WIF   memories (0=pass, !0=fail) */
+	uint64_t ncb                          : 1;  /**< BiST result of the NCB   memories (0=pass, !0=fail) */
+	uint64_t out                          : 1;  /**< BiST result of the OUT   memories (0=pass, !0=fail) */
+	uint64_t crc                          : 1;  /**< BiST result of the CRC   memories (0=pass, !0=fail) */
+	uint64_t chk                          : 1;  /**< BiST result of the CHK   memories (0=pass, !0=fail) */
+	uint64_t qsb                          : 2;  /**< BiST result of the QSB   memories (0=pass, !0=fail) */
+	uint64_t qcb                          : 2;  /**< BiST result of the QCB   memories (0=pass, !0=fail) */
+	uint64_t pdb                          : 4;  /**< BiST result of the PDB   memories (0=pass, !0=fail) */
+	uint64_t psb                          : 7;  /**< BiST result of the PSB   memories (0=pass, !0=fail) */
+#else
+	uint64_t psb                          : 7;
+	uint64_t pdb                          : 4;
+	uint64_t qcb                          : 2;
+	uint64_t qsb                          : 2;
+	uint64_t chk                          : 1;
+	uint64_t crc                          : 1;
+	uint64_t out                          : 1;
+	uint64_t ncb                          : 1;
+	uint64_t wif                          : 1;
+	uint64_t rif                          : 1;
+	uint64_t count                        : 1;
+	uint64_t psb2                         : 5;
+	uint64_t reserved_27_63               : 37;
 #endif
 	} cn30xx;
 	struct cvmx_pko_reg_bist_result_cn30xx cn31xx;
@@ -1641,74 +13278,74 @@ union cvmx_pko_reg_bist_result {
 	struct cvmx_pko_reg_bist_result_cn30xx cn38xxp2;
 	struct cvmx_pko_reg_bist_result_cn50xx {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_33_63:31;
-		uint64_t csr:1;
-		uint64_t iob:1;
-		uint64_t out_crc:1;
-		uint64_t out_ctl:3;
-		uint64_t out_sta:1;
-		uint64_t out_wif:1;
-		uint64_t prt_chk:3;
-		uint64_t prt_nxt:1;
-		uint64_t prt_psb:6;
-		uint64_t ncb_inb:2;
-		uint64_t prt_qcb:2;
-		uint64_t prt_qsb:3;
-		uint64_t dat_dat:4;
-		uint64_t dat_ptr:4;
-#else
-		uint64_t dat_ptr:4;
-		uint64_t dat_dat:4;
-		uint64_t prt_qsb:3;
-		uint64_t prt_qcb:2;
-		uint64_t ncb_inb:2;
-		uint64_t prt_psb:6;
-		uint64_t prt_nxt:1;
-		uint64_t prt_chk:3;
-		uint64_t out_wif:1;
-		uint64_t out_sta:1;
-		uint64_t out_ctl:3;
-		uint64_t out_crc:1;
-		uint64_t iob:1;
-		uint64_t csr:1;
-		uint64_t reserved_33_63:31;
+	uint64_t reserved_33_63               : 31;
+	uint64_t csr                          : 1;  /**< BiST result of CSR      memories (0=pass, !0=fail) */
+	uint64_t iob                          : 1;  /**< BiST result of IOB      memories (0=pass, !0=fail) */
+	uint64_t out_crc                      : 1;  /**< BiST result of OUT_CRC  memories (0=pass, !0=fail) */
+	uint64_t out_ctl                      : 3;  /**< BiST result of OUT_CTL  memories (0=pass, !0=fail) */
+	uint64_t out_sta                      : 1;  /**< BiST result of OUT_STA  memories (0=pass, !0=fail) */
+	uint64_t out_wif                      : 1;  /**< BiST result of OUT_WIF  memories (0=pass, !0=fail) */
+	uint64_t prt_chk                      : 3;  /**< BiST result of PRT_CHK  memories (0=pass, !0=fail) */
+	uint64_t prt_nxt                      : 1;  /**< BiST result of PRT_NXT  memories (0=pass, !0=fail) */
+	uint64_t prt_psb                      : 6;  /**< BiST result of PRT_PSB  memories (0=pass, !0=fail) */
+	uint64_t ncb_inb                      : 2;  /**< BiST result of NCB_INB  memories (0=pass, !0=fail) */
+	uint64_t prt_qcb                      : 2;  /**< BiST result of PRT_QCB  memories (0=pass, !0=fail) */
+	uint64_t prt_qsb                      : 3;  /**< BiST result of PRT_QSB  memories (0=pass, !0=fail) */
+	uint64_t dat_dat                      : 4;  /**< BiST result of DAT_DAT  memories (0=pass, !0=fail) */
+	uint64_t dat_ptr                      : 4;  /**< BiST result of DAT_PTR  memories (0=pass, !0=fail) */
+#else
+	uint64_t dat_ptr                      : 4;
+	uint64_t dat_dat                      : 4;
+	uint64_t prt_qsb                      : 3;
+	uint64_t prt_qcb                      : 2;
+	uint64_t ncb_inb                      : 2;
+	uint64_t prt_psb                      : 6;
+	uint64_t prt_nxt                      : 1;
+	uint64_t prt_chk                      : 3;
+	uint64_t out_wif                      : 1;
+	uint64_t out_sta                      : 1;
+	uint64_t out_ctl                      : 3;
+	uint64_t out_crc                      : 1;
+	uint64_t iob                          : 1;
+	uint64_t csr                          : 1;
+	uint64_t reserved_33_63               : 31;
 #endif
 	} cn50xx;
 	struct cvmx_pko_reg_bist_result_cn52xx {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_35_63:29;
-		uint64_t csr:1;
-		uint64_t iob:1;
-		uint64_t out_dat:1;
-		uint64_t out_ctl:3;
-		uint64_t out_sta:1;
-		uint64_t out_wif:1;
-		uint64_t prt_chk:3;
-		uint64_t prt_nxt:1;
-		uint64_t prt_psb:8;
-		uint64_t ncb_inb:2;
-		uint64_t prt_qcb:2;
-		uint64_t prt_qsb:3;
-		uint64_t prt_ctl:2;
-		uint64_t dat_dat:2;
-		uint64_t dat_ptr:4;
-#else
-		uint64_t dat_ptr:4;
-		uint64_t dat_dat:2;
-		uint64_t prt_ctl:2;
-		uint64_t prt_qsb:3;
-		uint64_t prt_qcb:2;
-		uint64_t ncb_inb:2;
-		uint64_t prt_psb:8;
-		uint64_t prt_nxt:1;
-		uint64_t prt_chk:3;
-		uint64_t out_wif:1;
-		uint64_t out_sta:1;
-		uint64_t out_ctl:3;
-		uint64_t out_dat:1;
-		uint64_t iob:1;
-		uint64_t csr:1;
-		uint64_t reserved_35_63:29;
+	uint64_t reserved_35_63               : 29;
+	uint64_t csr                          : 1;  /**< BiST result of CSR      memories (0=pass, !0=fail) */
+	uint64_t iob                          : 1;  /**< BiST result of IOB      memories (0=pass, !0=fail) */
+	uint64_t out_dat                      : 1;  /**< BiST result of OUT_DAT  memories (0=pass, !0=fail) */
+	uint64_t out_ctl                      : 3;  /**< BiST result of OUT_CTL  memories (0=pass, !0=fail) */
+	uint64_t out_sta                      : 1;  /**< BiST result of OUT_STA  memories (0=pass, !0=fail) */
+	uint64_t out_wif                      : 1;  /**< BiST result of OUT_WIF  memories (0=pass, !0=fail) */
+	uint64_t prt_chk                      : 3;  /**< BiST result of PRT_CHK  memories (0=pass, !0=fail) */
+	uint64_t prt_nxt                      : 1;  /**< BiST result of PRT_NXT  memories (0=pass, !0=fail) */
+	uint64_t prt_psb                      : 8;  /**< BiST result of PRT_PSB  memories (0=pass, !0=fail) */
+	uint64_t ncb_inb                      : 2;  /**< BiST result of NCB_INB  memories (0=pass, !0=fail) */
+	uint64_t prt_qcb                      : 2;  /**< BiST result of PRT_QCB  memories (0=pass, !0=fail) */
+	uint64_t prt_qsb                      : 3;  /**< BiST result of PRT_QSB  memories (0=pass, !0=fail) */
+	uint64_t prt_ctl                      : 2;  /**< BiST result of PRT_CTL  memories (0=pass, !0=fail) */
+	uint64_t dat_dat                      : 2;  /**< BiST result of DAT_DAT  memories (0=pass, !0=fail) */
+	uint64_t dat_ptr                      : 4;  /**< BiST result of DAT_PTR  memories (0=pass, !0=fail) */
+#else
+	uint64_t dat_ptr                      : 4;
+	uint64_t dat_dat                      : 2;
+	uint64_t prt_ctl                      : 2;
+	uint64_t prt_qsb                      : 3;
+	uint64_t prt_qcb                      : 2;
+	uint64_t ncb_inb                      : 2;
+	uint64_t prt_psb                      : 8;
+	uint64_t prt_nxt                      : 1;
+	uint64_t prt_chk                      : 3;
+	uint64_t out_wif                      : 1;
+	uint64_t out_sta                      : 1;
+	uint64_t out_ctl                      : 3;
+	uint64_t out_dat                      : 1;
+	uint64_t iob                          : 1;
+	uint64_t csr                          : 1;
+	uint64_t reserved_35_63               : 29;
 #endif
 	} cn52xx;
 	struct cvmx_pko_reg_bist_result_cn52xx cn52xxp1;
@@ -1722,372 +13359,519 @@ union cvmx_pko_reg_bist_result {
 	struct cvmx_pko_reg_bist_result_cn52xx cn66xx;
 	struct cvmx_pko_reg_bist_result_cn68xx {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_36_63:28;
-		uint64_t crc:1;
-		uint64_t csr:1;
-		uint64_t iob:1;
-		uint64_t out_dat:1;
-		uint64_t reserved_31_31:1;
-		uint64_t out_ctl:2;
-		uint64_t out_sta:1;
-		uint64_t out_wif:1;
-		uint64_t prt_chk:3;
-		uint64_t prt_nxt:1;
-		uint64_t prt_psb7:1;
-		uint64_t reserved_21_21:1;
-		uint64_t prt_psb:6;
-		uint64_t ncb_inb:2;
-		uint64_t prt_qcb:2;
-		uint64_t prt_qsb:3;
-		uint64_t prt_ctl:2;
-		uint64_t dat_dat:2;
-		uint64_t dat_ptr:4;
-#else
-		uint64_t dat_ptr:4;
-		uint64_t dat_dat:2;
-		uint64_t prt_ctl:2;
-		uint64_t prt_qsb:3;
-		uint64_t prt_qcb:2;
-		uint64_t ncb_inb:2;
-		uint64_t prt_psb:6;
-		uint64_t reserved_21_21:1;
-		uint64_t prt_psb7:1;
-		uint64_t prt_nxt:1;
-		uint64_t prt_chk:3;
-		uint64_t out_wif:1;
-		uint64_t out_sta:1;
-		uint64_t out_ctl:2;
-		uint64_t reserved_31_31:1;
-		uint64_t out_dat:1;
-		uint64_t iob:1;
-		uint64_t csr:1;
-		uint64_t crc:1;
-		uint64_t reserved_36_63:28;
+	uint64_t reserved_36_63               : 28;
+	uint64_t crc                          : 1;  /**< BiST result of CRC      memories (0=pass, !0=fail) */
+	uint64_t csr                          : 1;  /**< BiST result of CSR      memories (0=pass, !0=fail) */
+	uint64_t iob                          : 1;  /**< BiST result of IOB      memories (0=pass, !0=fail) */
+	uint64_t out_dat                      : 1;  /**< BiST result of OUT_DAT  memories (0=pass, !0=fail) */
+	uint64_t reserved_31_31               : 1;
+	uint64_t out_ctl                      : 2;  /**< BiST result of OUT_CTL  memories (0=pass, !0=fail) */
+	uint64_t out_sta                      : 1;  /**< BiST result of OUT_STA  memories (0=pass, !0=fail) */
+	uint64_t out_wif                      : 1;  /**< BiST result of OUT_WIF  memories (0=pass, !0=fail) */
+	uint64_t prt_chk                      : 3;  /**< BiST result of PRT_CHK  memories (0=pass, !0=fail) */
+	uint64_t prt_nxt                      : 1;  /**< BiST result of PRT_NXT  memories (0=pass, !0=fail) */
+	uint64_t prt_psb7                     : 1;  /**< BiST result of PRT_PSB  memories (0=pass, !0=fail) */
+	uint64_t reserved_21_21               : 1;
+	uint64_t prt_psb                      : 6;  /**< BiST result of PRT_PSB  memories (0=pass, !0=fail) */
+	uint64_t ncb_inb                      : 2;  /**< BiST result of NCB_INB  memories (0=pass, !0=fail) */
+	uint64_t prt_qcb                      : 2;  /**< BiST result of PRT_QCB  memories (0=pass, !0=fail) */
+	uint64_t prt_qsb                      : 3;  /**< BiST result of PRT_QSB  memories (0=pass, !0=fail) */
+	uint64_t prt_ctl                      : 2;  /**< BiST result of PRT_CTL  memories (0=pass, !0=fail) */
+	uint64_t dat_dat                      : 2;  /**< BiST result of DAT_DAT  memories (0=pass, !0=fail) */
+	uint64_t dat_ptr                      : 4;  /**< BiST result of DAT_PTR  memories (0=pass, !0=fail) */
+#else
+	uint64_t dat_ptr                      : 4;
+	uint64_t dat_dat                      : 2;
+	uint64_t prt_ctl                      : 2;
+	uint64_t prt_qsb                      : 3;
+	uint64_t prt_qcb                      : 2;
+	uint64_t ncb_inb                      : 2;
+	uint64_t prt_psb                      : 6;
+	uint64_t reserved_21_21               : 1;
+	uint64_t prt_psb7                     : 1;
+	uint64_t prt_nxt                      : 1;
+	uint64_t prt_chk                      : 3;
+	uint64_t out_wif                      : 1;
+	uint64_t out_sta                      : 1;
+	uint64_t out_ctl                      : 2;
+	uint64_t reserved_31_31               : 1;
+	uint64_t out_dat                      : 1;
+	uint64_t iob                          : 1;
+	uint64_t csr                          : 1;
+	uint64_t crc                          : 1;
+	uint64_t reserved_36_63               : 28;
 #endif
 	} cn68xx;
 	struct cvmx_pko_reg_bist_result_cn68xxp1 {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_35_63:29;
-		uint64_t csr:1;
-		uint64_t iob:1;
-		uint64_t out_dat:1;
-		uint64_t reserved_31_31:1;
-		uint64_t out_ctl:2;
-		uint64_t out_sta:1;
-		uint64_t out_wif:1;
-		uint64_t prt_chk:3;
-		uint64_t prt_nxt:1;
-		uint64_t prt_psb7:1;
-		uint64_t reserved_21_21:1;
-		uint64_t prt_psb:6;
-		uint64_t ncb_inb:2;
-		uint64_t prt_qcb:2;
-		uint64_t prt_qsb:3;
-		uint64_t prt_ctl:2;
-		uint64_t dat_dat:2;
-		uint64_t dat_ptr:4;
-#else
-		uint64_t dat_ptr:4;
-		uint64_t dat_dat:2;
-		uint64_t prt_ctl:2;
-		uint64_t prt_qsb:3;
-		uint64_t prt_qcb:2;
-		uint64_t ncb_inb:2;
-		uint64_t prt_psb:6;
-		uint64_t reserved_21_21:1;
-		uint64_t prt_psb7:1;
-		uint64_t prt_nxt:1;
-		uint64_t prt_chk:3;
-		uint64_t out_wif:1;
-		uint64_t out_sta:1;
-		uint64_t out_ctl:2;
-		uint64_t reserved_31_31:1;
-		uint64_t out_dat:1;
-		uint64_t iob:1;
-		uint64_t csr:1;
-		uint64_t reserved_35_63:29;
+	uint64_t reserved_35_63               : 29;
+	uint64_t csr                          : 1;  /**< BiST result of CSR      memories (0=pass, !0=fail) */
+	uint64_t iob                          : 1;  /**< BiST result of IOB      memories (0=pass, !0=fail) */
+	uint64_t out_dat                      : 1;  /**< BiST result of OUT_DAT  memories (0=pass, !0=fail) */
+	uint64_t reserved_31_31               : 1;
+	uint64_t out_ctl                      : 2;  /**< BiST result of OUT_CTL  memories (0=pass, !0=fail) */
+	uint64_t out_sta                      : 1;  /**< BiST result of OUT_STA  memories (0=pass, !0=fail) */
+	uint64_t out_wif                      : 1;  /**< BiST result of OUT_WIF  memories (0=pass, !0=fail) */
+	uint64_t prt_chk                      : 3;  /**< BiST result of PRT_CHK  memories (0=pass, !0=fail) */
+	uint64_t prt_nxt                      : 1;  /**< BiST result of PRT_NXT  memories (0=pass, !0=fail) */
+	uint64_t prt_psb7                     : 1;  /**< BiST result of PRT_PSB  memories (0=pass, !0=fail) */
+	uint64_t reserved_21_21               : 1;
+	uint64_t prt_psb                      : 6;  /**< BiST result of PRT_PSB  memories (0=pass, !0=fail) */
+	uint64_t ncb_inb                      : 2;  /**< BiST result of NCB_INB  memories (0=pass, !0=fail) */
+	uint64_t prt_qcb                      : 2;  /**< BiST result of PRT_QCB  memories (0=pass, !0=fail) */
+	uint64_t prt_qsb                      : 3;  /**< BiST result of PRT_QSB  memories (0=pass, !0=fail) */
+	uint64_t prt_ctl                      : 2;  /**< BiST result of PRT_CTL  memories (0=pass, !0=fail) */
+	uint64_t dat_dat                      : 2;  /**< BiST result of DAT_DAT  memories (0=pass, !0=fail) */
+	uint64_t dat_ptr                      : 4;  /**< BiST result of DAT_PTR  memories (0=pass, !0=fail) */
+#else
+	uint64_t dat_ptr                      : 4;
+	uint64_t dat_dat                      : 2;
+	uint64_t prt_ctl                      : 2;
+	uint64_t prt_qsb                      : 3;
+	uint64_t prt_qcb                      : 2;
+	uint64_t ncb_inb                      : 2;
+	uint64_t prt_psb                      : 6;
+	uint64_t reserved_21_21               : 1;
+	uint64_t prt_psb7                     : 1;
+	uint64_t prt_nxt                      : 1;
+	uint64_t prt_chk                      : 3;
+	uint64_t out_wif                      : 1;
+	uint64_t out_sta                      : 1;
+	uint64_t out_ctl                      : 2;
+	uint64_t reserved_31_31               : 1;
+	uint64_t out_dat                      : 1;
+	uint64_t iob                          : 1;
+	uint64_t csr                          : 1;
+	uint64_t reserved_35_63               : 29;
 #endif
 	} cn68xxp1;
+	struct cvmx_pko_reg_bist_result_cn70xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_30_63               : 34;
+	uint64_t csr                          : 1;  /**< BiST result of CSR      memories (0=pass, !0=fail) */
+	uint64_t iob                          : 1;  /**< BiST result of IOB      memories (0=pass, !0=fail) */
+	uint64_t out_dat                      : 1;  /**< BiST result of OUT_DAT  memories (0=pass, !0=fail) */
+	uint64_t out_ctl                      : 1;  /**< BiST result of OUT_CTL  memories (0=pass, !0=fail) */
+	uint64_t out_sta                      : 1;  /**< BiST result of OUT_STA  memories (0=pass, !0=fail) */
+	uint64_t out_wif                      : 1;  /**< BiST result of OUT_WIF  memories (0=pass, !0=fail) */
+	uint64_t prt_chk                      : 3;  /**< BiST result of PRT_CHK  memories (0=pass, !0=fail) */
+	uint64_t prt_nxt                      : 1;  /**< BiST result of PRT_NXT  memories (0=pass, !0=fail) */
+	uint64_t prt_psb                      : 8;  /**< BiST result of PRT_PSB  memories (0=pass, !0=fail) */
+	uint64_t ncb_inb                      : 1;  /**< BiST result of NCB_INB  memories (0=pass, !0=fail) */
+	uint64_t prt_qcb                      : 1;  /**< BiST result of PRT_QCB  memories (0=pass, !0=fail) */
+	uint64_t prt_qsb                      : 2;  /**< BiST result of PRT_QSB  memories (0=pass, !0=fail) */
+	uint64_t prt_ctl                      : 2;  /**< BiST result of PRT_CTL  memories (0=pass, !0=fail) */
+	uint64_t dat_dat                      : 2;  /**< BiST result of DAT_DAT  memories (0=pass, !0=fail) */
+	uint64_t dat_ptr                      : 4;  /**< BiST result of DAT_PTR  memories (0=pass, !0=fail) */
+#else
+	uint64_t dat_ptr                      : 4;
+	uint64_t dat_dat                      : 2;
+	uint64_t prt_ctl                      : 2;
+	uint64_t prt_qsb                      : 2;
+	uint64_t prt_qcb                      : 1;
+	uint64_t ncb_inb                      : 1;
+	uint64_t prt_psb                      : 8;
+	uint64_t prt_nxt                      : 1;
+	uint64_t prt_chk                      : 3;
+	uint64_t out_wif                      : 1;
+	uint64_t out_sta                      : 1;
+	uint64_t out_ctl                      : 1;
+	uint64_t out_dat                      : 1;
+	uint64_t iob                          : 1;
+	uint64_t csr                          : 1;
+	uint64_t reserved_30_63               : 34;
+#endif
+	} cn70xx;
+	struct cvmx_pko_reg_bist_result_cn70xx cn70xxp1;
 	struct cvmx_pko_reg_bist_result_cn52xx cnf71xx;
 };
+typedef union cvmx_pko_reg_bist_result cvmx_pko_reg_bist_result_t;
 
+/**
+ * cvmx_pko_reg_cmd_buf
+ *
+ * Notes:
+ * Sets the command buffer parameters
+ * The size of the command buffer segments is measured in uint64s.  The pool specifies (1 of 8 free
+ * lists to be used when freeing command buffer segments.
+ */
 union cvmx_pko_reg_cmd_buf {
 	uint64_t u64;
 	struct cvmx_pko_reg_cmd_buf_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_23_63:41;
-		uint64_t pool:3;
-		uint64_t reserved_13_19:7;
-		uint64_t size:13;
-#else
-		uint64_t size:13;
-		uint64_t reserved_13_19:7;
-		uint64_t pool:3;
-		uint64_t reserved_23_63:41;
-#endif
-	} s;
-	struct cvmx_pko_reg_cmd_buf_s cn30xx;
-	struct cvmx_pko_reg_cmd_buf_s cn31xx;
-	struct cvmx_pko_reg_cmd_buf_s cn38xx;
-	struct cvmx_pko_reg_cmd_buf_s cn38xxp2;
-	struct cvmx_pko_reg_cmd_buf_s cn50xx;
-	struct cvmx_pko_reg_cmd_buf_s cn52xx;
-	struct cvmx_pko_reg_cmd_buf_s cn52xxp1;
-	struct cvmx_pko_reg_cmd_buf_s cn56xx;
-	struct cvmx_pko_reg_cmd_buf_s cn56xxp1;
-	struct cvmx_pko_reg_cmd_buf_s cn58xx;
-	struct cvmx_pko_reg_cmd_buf_s cn58xxp1;
-	struct cvmx_pko_reg_cmd_buf_s cn61xx;
-	struct cvmx_pko_reg_cmd_buf_s cn63xx;
-	struct cvmx_pko_reg_cmd_buf_s cn63xxp1;
-	struct cvmx_pko_reg_cmd_buf_s cn66xx;
-	struct cvmx_pko_reg_cmd_buf_s cn68xx;
-	struct cvmx_pko_reg_cmd_buf_s cn68xxp1;
-	struct cvmx_pko_reg_cmd_buf_s cnf71xx;
+	uint64_t reserved_23_63               : 41;
+	uint64_t pool                         : 3;  /**< Free list used to free command buffer segments */
+	uint64_t reserved_13_19               : 7;
+	uint64_t size                         : 13; /**< Number of uint64s per command buffer segment */
+#else
+	uint64_t size                         : 13;
+	uint64_t reserved_13_19               : 7;
+	uint64_t pool                         : 3;
+	uint64_t reserved_23_63               : 41;
+#endif
+	} s;
+	struct cvmx_pko_reg_cmd_buf_s         cn30xx;
+	struct cvmx_pko_reg_cmd_buf_s         cn31xx;
+	struct cvmx_pko_reg_cmd_buf_s         cn38xx;
+	struct cvmx_pko_reg_cmd_buf_s         cn38xxp2;
+	struct cvmx_pko_reg_cmd_buf_s         cn50xx;
+	struct cvmx_pko_reg_cmd_buf_s         cn52xx;
+	struct cvmx_pko_reg_cmd_buf_s         cn52xxp1;
+	struct cvmx_pko_reg_cmd_buf_s         cn56xx;
+	struct cvmx_pko_reg_cmd_buf_s         cn56xxp1;
+	struct cvmx_pko_reg_cmd_buf_s         cn58xx;
+	struct cvmx_pko_reg_cmd_buf_s         cn58xxp1;
+	struct cvmx_pko_reg_cmd_buf_s         cn61xx;
+	struct cvmx_pko_reg_cmd_buf_s         cn63xx;
+	struct cvmx_pko_reg_cmd_buf_s         cn63xxp1;
+	struct cvmx_pko_reg_cmd_buf_s         cn66xx;
+	struct cvmx_pko_reg_cmd_buf_s         cn68xx;
+	struct cvmx_pko_reg_cmd_buf_s         cn68xxp1;
+	struct cvmx_pko_reg_cmd_buf_s         cn70xx;
+	struct cvmx_pko_reg_cmd_buf_s         cn70xxp1;
+	struct cvmx_pko_reg_cmd_buf_s         cnf71xx;
 };
+typedef union cvmx_pko_reg_cmd_buf cvmx_pko_reg_cmd_buf_t;
 
+/**
+ * cvmx_pko_reg_crc_ctl#
+ *
+ * Notes:
+ * Controls datapath reflection when calculating CRC
+ *
+ */
 union cvmx_pko_reg_crc_ctlx {
 	uint64_t u64;
 	struct cvmx_pko_reg_crc_ctlx_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_2_63:62;
-		uint64_t invres:1;
-		uint64_t refin:1;
+	uint64_t reserved_2_63                : 62;
+	uint64_t invres                       : 1;  /**< Invert the result */
+	uint64_t refin                        : 1;  /**< Reflect the bits in each byte.
+                                                          Byte order does not change.
+                                                         - 0: CRC is calculated MSB to LSB
+                                                         - 1: CRC is calculated MLB to MSB */
 #else
-		uint64_t refin:1;
-		uint64_t invres:1;
-		uint64_t reserved_2_63:62;
+	uint64_t refin                        : 1;
+	uint64_t invres                       : 1;
+	uint64_t reserved_2_63                : 62;
 #endif
 	} s;
-	struct cvmx_pko_reg_crc_ctlx_s cn38xx;
-	struct cvmx_pko_reg_crc_ctlx_s cn38xxp2;
-	struct cvmx_pko_reg_crc_ctlx_s cn58xx;
-	struct cvmx_pko_reg_crc_ctlx_s cn58xxp1;
+	struct cvmx_pko_reg_crc_ctlx_s        cn38xx;
+	struct cvmx_pko_reg_crc_ctlx_s        cn38xxp2;
+	struct cvmx_pko_reg_crc_ctlx_s        cn58xx;
+	struct cvmx_pko_reg_crc_ctlx_s        cn58xxp1;
 };
+typedef union cvmx_pko_reg_crc_ctlx cvmx_pko_reg_crc_ctlx_t;
 
+/**
+ * cvmx_pko_reg_crc_enable
+ *
+ * Notes:
+ * Enables CRC for the GMX ports.
+ *
+ */
 union cvmx_pko_reg_crc_enable {
 	uint64_t u64;
 	struct cvmx_pko_reg_crc_enable_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_32_63:32;
-		uint64_t enable:32;
+	uint64_t reserved_32_63               : 32;
+	uint64_t enable                       : 32; /**< Mask for ports 31-0 to enable CRC
+                                                         Mask bit==0 means CRC not enabled
+                                                         Mask bit==1 means CRC     enabled
+                                                         Note that CRC should be enabled only when using SPI4.2 */
 #else
-		uint64_t enable:32;
-		uint64_t reserved_32_63:32;
+	uint64_t enable                       : 32;
+	uint64_t reserved_32_63               : 32;
 #endif
 	} s;
-	struct cvmx_pko_reg_crc_enable_s cn38xx;
-	struct cvmx_pko_reg_crc_enable_s cn38xxp2;
-	struct cvmx_pko_reg_crc_enable_s cn58xx;
-	struct cvmx_pko_reg_crc_enable_s cn58xxp1;
+	struct cvmx_pko_reg_crc_enable_s      cn38xx;
+	struct cvmx_pko_reg_crc_enable_s      cn38xxp2;
+	struct cvmx_pko_reg_crc_enable_s      cn58xx;
+	struct cvmx_pko_reg_crc_enable_s      cn58xxp1;
 };
+typedef union cvmx_pko_reg_crc_enable cvmx_pko_reg_crc_enable_t;
 
+/**
+ * cvmx_pko_reg_crc_iv#
+ *
+ * Notes:
+ * Determines the IV used by the CRC algorithm
+ * * PKO_CRC_IV
+ *  PKO_CRC_IV controls the initial state of the CRC algorithm.  Octane can
+ *  support a wide range of CRC algorithms and as such, the IV must be
+ *  carefully constructed to meet the specific algorithm.  The code below
+ *  determines the value to program into Octane based on the algorthim's IV
+ *  and width.  In the case of Octane, the width should always be 32.
+ *
+ *  PKO_CRC_IV0 sets the IV for ports 0-15 while PKO_CRC_IV1 sets the IV for
+ *  ports 16-31.
+ *
+ *   @verbatim
+ *   unsigned octane_crc_iv(unsigned algorithm_iv, unsigned poly, unsigned w)
+ *   [
+ *     int i;
+ *     int doit;
+ *     unsigned int current_val = algorithm_iv;
+ *
+ *     for(i = 0; i < w; i++) [
+ *       doit = current_val & 0x1;
+ *
+ *       if(doit) current_val ^= poly;
+ *       assert(!(current_val & 0x1));
+ *
+ *       current_val = (current_val >> 1) | (doit << (w-1));
+ *     ]
+ *
+ *     return current_val;
+ *   ]
+ *   @endverbatim
+ */
 union cvmx_pko_reg_crc_ivx {
 	uint64_t u64;
 	struct cvmx_pko_reg_crc_ivx_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_32_63:32;
-		uint64_t iv:32;
+	uint64_t reserved_32_63               : 32;
+	uint64_t iv                           : 32; /**< IV used by the CRC algorithm.  Default is FCS32. */
 #else
-		uint64_t iv:32;
-		uint64_t reserved_32_63:32;
+	uint64_t iv                           : 32;
+	uint64_t reserved_32_63               : 32;
 #endif
 	} s;
-	struct cvmx_pko_reg_crc_ivx_s cn38xx;
-	struct cvmx_pko_reg_crc_ivx_s cn38xxp2;
-	struct cvmx_pko_reg_crc_ivx_s cn58xx;
-	struct cvmx_pko_reg_crc_ivx_s cn58xxp1;
+	struct cvmx_pko_reg_crc_ivx_s         cn38xx;
+	struct cvmx_pko_reg_crc_ivx_s         cn38xxp2;
+	struct cvmx_pko_reg_crc_ivx_s         cn58xx;
+	struct cvmx_pko_reg_crc_ivx_s         cn58xxp1;
 };
+typedef union cvmx_pko_reg_crc_ivx cvmx_pko_reg_crc_ivx_t;
 
+/**
+ * cvmx_pko_reg_debug0
+ *
+ * Notes:
+ * Note that this CSR is present only in chip revisions beginning with pass2.
+ *
+ */
 union cvmx_pko_reg_debug0 {
 	uint64_t u64;
 	struct cvmx_pko_reg_debug0_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t asserts:64;
+	uint64_t asserts                      : 64; /**< Various assertion checks */
 #else
-		uint64_t asserts:64;
+	uint64_t asserts                      : 64;
 #endif
 	} s;
 	struct cvmx_pko_reg_debug0_cn30xx {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_17_63:47;
-		uint64_t asserts:17;
+	uint64_t reserved_17_63               : 47;
+	uint64_t asserts                      : 17; /**< Various assertion checks */
 #else
-		uint64_t asserts:17;
-		uint64_t reserved_17_63:47;
+	uint64_t asserts                      : 17;
+	uint64_t reserved_17_63               : 47;
 #endif
 	} cn30xx;
-	struct cvmx_pko_reg_debug0_cn30xx cn31xx;
-	struct cvmx_pko_reg_debug0_cn30xx cn38xx;
-	struct cvmx_pko_reg_debug0_cn30xx cn38xxp2;
-	struct cvmx_pko_reg_debug0_s cn50xx;
-	struct cvmx_pko_reg_debug0_s cn52xx;
-	struct cvmx_pko_reg_debug0_s cn52xxp1;
-	struct cvmx_pko_reg_debug0_s cn56xx;
-	struct cvmx_pko_reg_debug0_s cn56xxp1;
-	struct cvmx_pko_reg_debug0_s cn58xx;
-	struct cvmx_pko_reg_debug0_s cn58xxp1;
-	struct cvmx_pko_reg_debug0_s cn61xx;
-	struct cvmx_pko_reg_debug0_s cn63xx;
-	struct cvmx_pko_reg_debug0_s cn63xxp1;
-	struct cvmx_pko_reg_debug0_s cn66xx;
-	struct cvmx_pko_reg_debug0_s cn68xx;
-	struct cvmx_pko_reg_debug0_s cn68xxp1;
-	struct cvmx_pko_reg_debug0_s cnf71xx;
+	struct cvmx_pko_reg_debug0_cn30xx     cn31xx;
+	struct cvmx_pko_reg_debug0_cn30xx     cn38xx;
+	struct cvmx_pko_reg_debug0_cn30xx     cn38xxp2;
+	struct cvmx_pko_reg_debug0_s          cn50xx;
+	struct cvmx_pko_reg_debug0_s          cn52xx;
+	struct cvmx_pko_reg_debug0_s          cn52xxp1;
+	struct cvmx_pko_reg_debug0_s          cn56xx;
+	struct cvmx_pko_reg_debug0_s          cn56xxp1;
+	struct cvmx_pko_reg_debug0_s          cn58xx;
+	struct cvmx_pko_reg_debug0_s          cn58xxp1;
+	struct cvmx_pko_reg_debug0_s          cn61xx;
+	struct cvmx_pko_reg_debug0_s          cn63xx;
+	struct cvmx_pko_reg_debug0_s          cn63xxp1;
+	struct cvmx_pko_reg_debug0_s          cn66xx;
+	struct cvmx_pko_reg_debug0_s          cn68xx;
+	struct cvmx_pko_reg_debug0_s          cn68xxp1;
+	struct cvmx_pko_reg_debug0_s          cn70xx;
+	struct cvmx_pko_reg_debug0_s          cn70xxp1;
+	struct cvmx_pko_reg_debug0_s          cnf71xx;
 };
+typedef union cvmx_pko_reg_debug0 cvmx_pko_reg_debug0_t;
 
+/**
+ * cvmx_pko_reg_debug1
+ */
 union cvmx_pko_reg_debug1 {
 	uint64_t u64;
 	struct cvmx_pko_reg_debug1_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t asserts:64;
+	uint64_t asserts                      : 64; /**< Various assertion checks */
 #else
-		uint64_t asserts:64;
+	uint64_t asserts                      : 64;
 #endif
 	} s;
-	struct cvmx_pko_reg_debug1_s cn50xx;
-	struct cvmx_pko_reg_debug1_s cn52xx;
-	struct cvmx_pko_reg_debug1_s cn52xxp1;
-	struct cvmx_pko_reg_debug1_s cn56xx;
-	struct cvmx_pko_reg_debug1_s cn56xxp1;
-	struct cvmx_pko_reg_debug1_s cn58xx;
-	struct cvmx_pko_reg_debug1_s cn58xxp1;
-	struct cvmx_pko_reg_debug1_s cn61xx;
-	struct cvmx_pko_reg_debug1_s cn63xx;
-	struct cvmx_pko_reg_debug1_s cn63xxp1;
-	struct cvmx_pko_reg_debug1_s cn66xx;
-	struct cvmx_pko_reg_debug1_s cn68xx;
-	struct cvmx_pko_reg_debug1_s cn68xxp1;
-	struct cvmx_pko_reg_debug1_s cnf71xx;
+	struct cvmx_pko_reg_debug1_s          cn50xx;
+	struct cvmx_pko_reg_debug1_s          cn52xx;
+	struct cvmx_pko_reg_debug1_s          cn52xxp1;
+	struct cvmx_pko_reg_debug1_s          cn56xx;
+	struct cvmx_pko_reg_debug1_s          cn56xxp1;
+	struct cvmx_pko_reg_debug1_s          cn58xx;
+	struct cvmx_pko_reg_debug1_s          cn58xxp1;
+	struct cvmx_pko_reg_debug1_s          cn61xx;
+	struct cvmx_pko_reg_debug1_s          cn63xx;
+	struct cvmx_pko_reg_debug1_s          cn63xxp1;
+	struct cvmx_pko_reg_debug1_s          cn66xx;
+	struct cvmx_pko_reg_debug1_s          cn68xx;
+	struct cvmx_pko_reg_debug1_s          cn68xxp1;
+	struct cvmx_pko_reg_debug1_s          cn70xx;
+	struct cvmx_pko_reg_debug1_s          cn70xxp1;
+	struct cvmx_pko_reg_debug1_s          cnf71xx;
 };
+typedef union cvmx_pko_reg_debug1 cvmx_pko_reg_debug1_t;
 
+/**
+ * cvmx_pko_reg_debug2
+ */
 union cvmx_pko_reg_debug2 {
 	uint64_t u64;
 	struct cvmx_pko_reg_debug2_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t asserts:64;
+	uint64_t asserts                      : 64; /**< Various assertion checks */
 #else
-		uint64_t asserts:64;
+	uint64_t asserts                      : 64;
 #endif
 	} s;
-	struct cvmx_pko_reg_debug2_s cn50xx;
-	struct cvmx_pko_reg_debug2_s cn52xx;
-	struct cvmx_pko_reg_debug2_s cn52xxp1;
-	struct cvmx_pko_reg_debug2_s cn56xx;
-	struct cvmx_pko_reg_debug2_s cn56xxp1;
-	struct cvmx_pko_reg_debug2_s cn58xx;
-	struct cvmx_pko_reg_debug2_s cn58xxp1;
-	struct cvmx_pko_reg_debug2_s cn61xx;
-	struct cvmx_pko_reg_debug2_s cn63xx;
-	struct cvmx_pko_reg_debug2_s cn63xxp1;
-	struct cvmx_pko_reg_debug2_s cn66xx;
-	struct cvmx_pko_reg_debug2_s cn68xx;
-	struct cvmx_pko_reg_debug2_s cn68xxp1;
-	struct cvmx_pko_reg_debug2_s cnf71xx;
+	struct cvmx_pko_reg_debug2_s          cn50xx;
+	struct cvmx_pko_reg_debug2_s          cn52xx;
+	struct cvmx_pko_reg_debug2_s          cn52xxp1;
+	struct cvmx_pko_reg_debug2_s          cn56xx;
+	struct cvmx_pko_reg_debug2_s          cn56xxp1;
+	struct cvmx_pko_reg_debug2_s          cn58xx;
+	struct cvmx_pko_reg_debug2_s          cn58xxp1;
+	struct cvmx_pko_reg_debug2_s          cn61xx;
+	struct cvmx_pko_reg_debug2_s          cn63xx;
+	struct cvmx_pko_reg_debug2_s          cn63xxp1;
+	struct cvmx_pko_reg_debug2_s          cn66xx;
+	struct cvmx_pko_reg_debug2_s          cn68xx;
+	struct cvmx_pko_reg_debug2_s          cn68xxp1;
+	struct cvmx_pko_reg_debug2_s          cn70xx;
+	struct cvmx_pko_reg_debug2_s          cn70xxp1;
+	struct cvmx_pko_reg_debug2_s          cnf71xx;
 };
+typedef union cvmx_pko_reg_debug2 cvmx_pko_reg_debug2_t;
 
+/**
+ * cvmx_pko_reg_debug3
+ */
 union cvmx_pko_reg_debug3 {
 	uint64_t u64;
 	struct cvmx_pko_reg_debug3_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t asserts:64;
+	uint64_t asserts                      : 64; /**< Various assertion checks */
 #else
-		uint64_t asserts:64;
+	uint64_t asserts                      : 64;
 #endif
 	} s;
-	struct cvmx_pko_reg_debug3_s cn50xx;
-	struct cvmx_pko_reg_debug3_s cn52xx;
-	struct cvmx_pko_reg_debug3_s cn52xxp1;
-	struct cvmx_pko_reg_debug3_s cn56xx;
-	struct cvmx_pko_reg_debug3_s cn56xxp1;
-	struct cvmx_pko_reg_debug3_s cn58xx;
-	struct cvmx_pko_reg_debug3_s cn58xxp1;
-	struct cvmx_pko_reg_debug3_s cn61xx;
-	struct cvmx_pko_reg_debug3_s cn63xx;
-	struct cvmx_pko_reg_debug3_s cn63xxp1;
-	struct cvmx_pko_reg_debug3_s cn66xx;
-	struct cvmx_pko_reg_debug3_s cn68xx;
-	struct cvmx_pko_reg_debug3_s cn68xxp1;
-	struct cvmx_pko_reg_debug3_s cnf71xx;
+	struct cvmx_pko_reg_debug3_s          cn50xx;
+	struct cvmx_pko_reg_debug3_s          cn52xx;
+	struct cvmx_pko_reg_debug3_s          cn52xxp1;
+	struct cvmx_pko_reg_debug3_s          cn56xx;
+	struct cvmx_pko_reg_debug3_s          cn56xxp1;
+	struct cvmx_pko_reg_debug3_s          cn58xx;
+	struct cvmx_pko_reg_debug3_s          cn58xxp1;
+	struct cvmx_pko_reg_debug3_s          cn61xx;
+	struct cvmx_pko_reg_debug3_s          cn63xx;
+	struct cvmx_pko_reg_debug3_s          cn63xxp1;
+	struct cvmx_pko_reg_debug3_s          cn66xx;
+	struct cvmx_pko_reg_debug3_s          cn68xx;
+	struct cvmx_pko_reg_debug3_s          cn68xxp1;
+	struct cvmx_pko_reg_debug3_s          cn70xx;
+	struct cvmx_pko_reg_debug3_s          cn70xxp1;
+	struct cvmx_pko_reg_debug3_s          cnf71xx;
 };
+typedef union cvmx_pko_reg_debug3 cvmx_pko_reg_debug3_t;
 
+/**
+ * cvmx_pko_reg_debug4
+ */
 union cvmx_pko_reg_debug4 {
 	uint64_t u64;
 	struct cvmx_pko_reg_debug4_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t asserts:64;
+	uint64_t asserts                      : 64; /**< Various assertion checks */
 #else
-		uint64_t asserts:64;
+	uint64_t asserts                      : 64;
 #endif
 	} s;
-	struct cvmx_pko_reg_debug4_s cn68xx;
-	struct cvmx_pko_reg_debug4_s cn68xxp1;
+	struct cvmx_pko_reg_debug4_s          cn68xx;
+	struct cvmx_pko_reg_debug4_s          cn68xxp1;
 };
+typedef union cvmx_pko_reg_debug4 cvmx_pko_reg_debug4_t;
 
+/**
+ * cvmx_pko_reg_engine_inflight
+ *
+ * Notes:
+ * Sets the maximum number of inflight packets, per engine.  Values greater than 4 are illegal.
+ * Setting an engine's value to 0 effectively stops the engine.
+ */
 union cvmx_pko_reg_engine_inflight {
 	uint64_t u64;
 	struct cvmx_pko_reg_engine_inflight_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t engine15:4;
-		uint64_t engine14:4;
-		uint64_t engine13:4;
-		uint64_t engine12:4;
-		uint64_t engine11:4;
-		uint64_t engine10:4;
-		uint64_t engine9:4;
-		uint64_t engine8:4;
-		uint64_t engine7:4;
-		uint64_t engine6:4;
-		uint64_t engine5:4;
-		uint64_t engine4:4;
-		uint64_t engine3:4;
-		uint64_t engine2:4;
-		uint64_t engine1:4;
-		uint64_t engine0:4;
-#else
-		uint64_t engine0:4;
-		uint64_t engine1:4;
-		uint64_t engine2:4;
-		uint64_t engine3:4;
-		uint64_t engine4:4;
-		uint64_t engine5:4;
-		uint64_t engine6:4;
-		uint64_t engine7:4;
-		uint64_t engine8:4;
-		uint64_t engine9:4;
-		uint64_t engine10:4;
-		uint64_t engine11:4;
-		uint64_t engine12:4;
-		uint64_t engine13:4;
-		uint64_t engine14:4;
-		uint64_t engine15:4;
+	uint64_t engine15                     : 4;  /**< Maximum number of inflight packets for engine15 */
+	uint64_t engine14                     : 4;  /**< Maximum number of inflight packets for engine14 */
+	uint64_t engine13                     : 4;  /**< Reserved */
+	uint64_t engine12                     : 4;  /**< Reserved */
+	uint64_t engine11                     : 4;  /**< Reserved */
+	uint64_t engine10                     : 4;  /**< Reserved */
+	uint64_t engine9                      : 4;  /**< Maximum number of inflight packets for engine9 */
+	uint64_t engine8                      : 4;  /**< Maximum number of inflight packets for engine8 */
+	uint64_t engine7                      : 4;  /**< Reserved */
+	uint64_t engine6                      : 4;  /**< Reserved */
+	uint64_t engine5                      : 4;  /**< Reserved */
+	uint64_t engine4                      : 4;  /**< Reserved */
+	uint64_t engine3                      : 4;  /**< Reserved */
+	uint64_t engine2                      : 4;  /**< Reserved */
+	uint64_t engine1                      : 4;  /**< Maximum number of inflight packets for engine1 */
+	uint64_t engine0                      : 4;  /**< Maximum number of inflight packets for engine0 */
+#else
+	uint64_t engine0                      : 4;
+	uint64_t engine1                      : 4;
+	uint64_t engine2                      : 4;
+	uint64_t engine3                      : 4;
+	uint64_t engine4                      : 4;
+	uint64_t engine5                      : 4;
+	uint64_t engine6                      : 4;
+	uint64_t engine7                      : 4;
+	uint64_t engine8                      : 4;
+	uint64_t engine9                      : 4;
+	uint64_t engine10                     : 4;
+	uint64_t engine11                     : 4;
+	uint64_t engine12                     : 4;
+	uint64_t engine13                     : 4;
+	uint64_t engine14                     : 4;
+	uint64_t engine15                     : 4;
 #endif
 	} s;
 	struct cvmx_pko_reg_engine_inflight_cn52xx {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_40_63:24;
-		uint64_t engine9:4;
-		uint64_t engine8:4;
-		uint64_t engine7:4;
-		uint64_t engine6:4;
-		uint64_t engine5:4;
-		uint64_t engine4:4;
-		uint64_t engine3:4;
-		uint64_t engine2:4;
-		uint64_t engine1:4;
-		uint64_t engine0:4;
-#else
-		uint64_t engine0:4;
-		uint64_t engine1:4;
-		uint64_t engine2:4;
-		uint64_t engine3:4;
-		uint64_t engine4:4;
-		uint64_t engine5:4;
-		uint64_t engine6:4;
-		uint64_t engine7:4;
-		uint64_t engine8:4;
-		uint64_t engine9:4;
-		uint64_t reserved_40_63:24;
+	uint64_t reserved_40_63               : 24;
+	uint64_t engine9                      : 4;  /**< Maximum number of inflight packets for engine9 */
+	uint64_t engine8                      : 4;  /**< Maximum number of inflight packets for engine8 */
+	uint64_t engine7                      : 4;  /**< MBZ */
+	uint64_t engine6                      : 4;  /**< MBZ */
+	uint64_t engine5                      : 4;  /**< MBZ */
+	uint64_t engine4                      : 4;  /**< MBZ */
+	uint64_t engine3                      : 4;  /**< Maximum number of inflight packets for engine3 */
+	uint64_t engine2                      : 4;  /**< Maximum number of inflight packets for engine2 */
+	uint64_t engine1                      : 4;  /**< Maximum number of inflight packets for engine1 */
+	uint64_t engine0                      : 4;  /**< Maximum number of inflight packets for engine0 */
+#else
+	uint64_t engine0                      : 4;
+	uint64_t engine1                      : 4;
+	uint64_t engine2                      : 4;
+	uint64_t engine3                      : 4;
+	uint64_t engine4                      : 4;
+	uint64_t engine5                      : 4;
+	uint64_t engine6                      : 4;
+	uint64_t engine7                      : 4;
+	uint64_t engine8                      : 4;
+	uint64_t engine9                      : 4;
+	uint64_t reserved_40_63               : 24;
 #endif
 	} cn52xx;
 	struct cvmx_pko_reg_engine_inflight_cn52xx cn52xxp1;
@@ -2095,159 +13879,234 @@ union cvmx_pko_reg_engine_inflight {
 	struct cvmx_pko_reg_engine_inflight_cn52xx cn56xxp1;
 	struct cvmx_pko_reg_engine_inflight_cn61xx {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_56_63:8;
-		uint64_t engine13:4;
-		uint64_t engine12:4;
-		uint64_t engine11:4;
-		uint64_t engine10:4;
-		uint64_t engine9:4;
-		uint64_t engine8:4;
-		uint64_t engine7:4;
-		uint64_t engine6:4;
-		uint64_t engine5:4;
-		uint64_t engine4:4;
-		uint64_t engine3:4;
-		uint64_t engine2:4;
-		uint64_t engine1:4;
-		uint64_t engine0:4;
-#else
-		uint64_t engine0:4;
-		uint64_t engine1:4;
-		uint64_t engine2:4;
-		uint64_t engine3:4;
-		uint64_t engine4:4;
-		uint64_t engine5:4;
-		uint64_t engine6:4;
-		uint64_t engine7:4;
-		uint64_t engine8:4;
-		uint64_t engine9:4;
-		uint64_t engine10:4;
-		uint64_t engine11:4;
-		uint64_t engine12:4;
-		uint64_t engine13:4;
-		uint64_t reserved_56_63:8;
+	uint64_t reserved_56_63               : 8;
+	uint64_t engine13                     : 4;  /**< Reserved */
+	uint64_t engine12                     : 4;  /**< Reserved */
+	uint64_t engine11                     : 4;  /**< Reserved */
+	uint64_t engine10                     : 4;  /**< Reserved */
+	uint64_t engine9                      : 4;  /**< Maximum number of inflight packets for engine9 */
+	uint64_t engine8                      : 4;  /**< Maximum number of inflight packets for engine8 */
+	uint64_t engine7                      : 4;  /**< Maximum number of inflight packets for engine7 */
+	uint64_t engine6                      : 4;  /**< Maximum number of inflight packets for engine6 */
+	uint64_t engine5                      : 4;  /**< Maximum number of inflight packets for engine5 */
+	uint64_t engine4                      : 4;  /**< Maximum number of inflight packets for engine4 */
+	uint64_t engine3                      : 4;  /**< Maximum number of inflight packets for engine3 */
+	uint64_t engine2                      : 4;  /**< Maximum number of inflight packets for engine2 */
+	uint64_t engine1                      : 4;  /**< Maximum number of inflight packets for engine1 */
+	uint64_t engine0                      : 4;  /**< Maximum number of inflight packets for engine0 */
+#else
+	uint64_t engine0                      : 4;
+	uint64_t engine1                      : 4;
+	uint64_t engine2                      : 4;
+	uint64_t engine3                      : 4;
+	uint64_t engine4                      : 4;
+	uint64_t engine5                      : 4;
+	uint64_t engine6                      : 4;
+	uint64_t engine7                      : 4;
+	uint64_t engine8                      : 4;
+	uint64_t engine9                      : 4;
+	uint64_t engine10                     : 4;
+	uint64_t engine11                     : 4;
+	uint64_t engine12                     : 4;
+	uint64_t engine13                     : 4;
+	uint64_t reserved_56_63               : 8;
 #endif
 	} cn61xx;
 	struct cvmx_pko_reg_engine_inflight_cn63xx {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_48_63:16;
-		uint64_t engine11:4;
-		uint64_t engine10:4;
-		uint64_t engine9:4;
-		uint64_t engine8:4;
-		uint64_t engine7:4;
-		uint64_t engine6:4;
-		uint64_t engine5:4;
-		uint64_t engine4:4;
-		uint64_t engine3:4;
-		uint64_t engine2:4;
-		uint64_t engine1:4;
-		uint64_t engine0:4;
-#else
-		uint64_t engine0:4;
-		uint64_t engine1:4;
-		uint64_t engine2:4;
-		uint64_t engine3:4;
-		uint64_t engine4:4;
-		uint64_t engine5:4;
-		uint64_t engine6:4;
-		uint64_t engine7:4;
-		uint64_t engine8:4;
-		uint64_t engine9:4;
-		uint64_t engine10:4;
-		uint64_t engine11:4;
-		uint64_t reserved_48_63:16;
+	uint64_t reserved_48_63               : 16;
+	uint64_t engine11                     : 4;  /**< Maximum number of inflight packets for engine11 */
+	uint64_t engine10                     : 4;  /**< Maximum number of inflight packets for engine10 */
+	uint64_t engine9                      : 4;  /**< Maximum number of inflight packets for engine9 */
+	uint64_t engine8                      : 4;  /**< Maximum number of inflight packets for engine8 */
+	uint64_t engine7                      : 4;  /**< MBZ */
+	uint64_t engine6                      : 4;  /**< MBZ */
+	uint64_t engine5                      : 4;  /**< MBZ */
+	uint64_t engine4                      : 4;  /**< MBZ */
+	uint64_t engine3                      : 4;  /**< Maximum number of inflight packets for engine3 */
+	uint64_t engine2                      : 4;  /**< Maximum number of inflight packets for engine2 */
+	uint64_t engine1                      : 4;  /**< Maximum number of inflight packets for engine1 */
+	uint64_t engine0                      : 4;  /**< Maximum number of inflight packets for engine0 */
+#else
+	uint64_t engine0                      : 4;
+	uint64_t engine1                      : 4;
+	uint64_t engine2                      : 4;
+	uint64_t engine3                      : 4;
+	uint64_t engine4                      : 4;
+	uint64_t engine5                      : 4;
+	uint64_t engine6                      : 4;
+	uint64_t engine7                      : 4;
+	uint64_t engine8                      : 4;
+	uint64_t engine9                      : 4;
+	uint64_t engine10                     : 4;
+	uint64_t engine11                     : 4;
+	uint64_t reserved_48_63               : 16;
 #endif
 	} cn63xx;
 	struct cvmx_pko_reg_engine_inflight_cn63xx cn63xxp1;
 	struct cvmx_pko_reg_engine_inflight_cn61xx cn66xx;
 	struct cvmx_pko_reg_engine_inflight_s cn68xx;
 	struct cvmx_pko_reg_engine_inflight_s cn68xxp1;
+	struct cvmx_pko_reg_engine_inflight_cn61xx cn70xx;
+	struct cvmx_pko_reg_engine_inflight_cn61xx cn70xxp1;
 	struct cvmx_pko_reg_engine_inflight_cn61xx cnf71xx;
 };
+typedef union cvmx_pko_reg_engine_inflight cvmx_pko_reg_engine_inflight_t;
 
+/**
+ * cvmx_pko_reg_engine_inflight1
+ *
+ * Notes:
+ * Sets the maximum number of inflight packets, per engine.  Values greater than 8 are illegal.
+ * Setting an engine's value to 0 effectively stops the engine.
+ */
 union cvmx_pko_reg_engine_inflight1 {
 	uint64_t u64;
 	struct cvmx_pko_reg_engine_inflight1_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_16_63:48;
-		uint64_t engine19:4;
-		uint64_t engine18:4;
-		uint64_t engine17:4;
-		uint64_t engine16:4;
+	uint64_t reserved_16_63               : 48;
+	uint64_t engine19                     : 4;  /**< Maximum number of inflight packets for engine19 */
+	uint64_t engine18                     : 4;  /**< Maximum number of inflight packets for engine18 */
+	uint64_t engine17                     : 4;  /**< Maximum number of inflight packets for engine17 */
+	uint64_t engine16                     : 4;  /**< Maximum number of inflight packets for engine16 */
 #else
-		uint64_t engine16:4;
-		uint64_t engine17:4;
-		uint64_t engine18:4;
-		uint64_t engine19:4;
-		uint64_t reserved_16_63:48;
+	uint64_t engine16                     : 4;
+	uint64_t engine17                     : 4;
+	uint64_t engine18                     : 4;
+	uint64_t engine19                     : 4;
+	uint64_t reserved_16_63               : 48;
 #endif
 	} s;
 	struct cvmx_pko_reg_engine_inflight1_s cn68xx;
 	struct cvmx_pko_reg_engine_inflight1_s cn68xxp1;
 };
+typedef union cvmx_pko_reg_engine_inflight1 cvmx_pko_reg_engine_inflight1_t;
 
+/**
+ * cvmx_pko_reg_engine_storage#
+ *
+ * Notes:
+ * The PKO has 40KB of local storage, consisting of 20, 2KB chunks.  Up to 15 contiguous chunks may be mapped per engine.
+ * The total of all mapped storage must not exceed 40KB.
+ */
 union cvmx_pko_reg_engine_storagex {
 	uint64_t u64;
 	struct cvmx_pko_reg_engine_storagex_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t engine15:4;
-		uint64_t engine14:4;
-		uint64_t engine13:4;
-		uint64_t engine12:4;
-		uint64_t engine11:4;
-		uint64_t engine10:4;
-		uint64_t engine9:4;
-		uint64_t engine8:4;
-		uint64_t engine7:4;
-		uint64_t engine6:4;
-		uint64_t engine5:4;
-		uint64_t engine4:4;
-		uint64_t engine3:4;
-		uint64_t engine2:4;
-		uint64_t engine1:4;
-		uint64_t engine0:4;
-#else
-		uint64_t engine0:4;
-		uint64_t engine1:4;
-		uint64_t engine2:4;
-		uint64_t engine3:4;
-		uint64_t engine4:4;
-		uint64_t engine5:4;
-		uint64_t engine6:4;
-		uint64_t engine7:4;
-		uint64_t engine8:4;
-		uint64_t engine9:4;
-		uint64_t engine10:4;
-		uint64_t engine11:4;
-		uint64_t engine12:4;
-		uint64_t engine13:4;
-		uint64_t engine14:4;
-		uint64_t engine15:4;
+	uint64_t engine15                     : 4;  /**< Number of contiguous 2KB chunks allocated to
+                                                         engine (X * 16) + 15.
+                                                         ENGINE15 does not exist and is reserved in
+                                                         PKO_REG_ENGINE_STORAGE1. */
+	uint64_t engine14                     : 4;  /**< Number of contiguous 2KB chunks allocated to
+                                                         engine (X * 16) + 14.
+                                                         ENGINE14 does not exist and is reserved in
+                                                         PKO_REG_ENGINE_STORAGE1. */
+	uint64_t engine13                     : 4;  /**< Number of contiguous 2KB chunks allocated to
+                                                         engine (X * 16) + 13.
+                                                         ENGINE13 does not exist and is reserved in
+                                                         PKO_REG_ENGINE_STORAGE1. */
+	uint64_t engine12                     : 4;  /**< Number of contiguous 2KB chunks allocated to
+                                                         engine (X * 16) + 12.
+                                                         ENGINE12 does not exist and is reserved in
+                                                         PKO_REG_ENGINE_STORAGE1. */
+	uint64_t engine11                     : 4;  /**< Number of contiguous 2KB chunks allocated to
+                                                         engine (X * 16) + 11.
+                                                         ENGINE11 does not exist and is reserved in
+                                                         PKO_REG_ENGINE_STORAGE1. */
+	uint64_t engine10                     : 4;  /**< Number of contiguous 2KB chunks allocated to
+                                                         engine (X * 16) + 10.
+                                                         ENGINE10 does not exist and is reserved in
+                                                         PKO_REG_ENGINE_STORAGE1. */
+	uint64_t engine9                      : 4;  /**< Number of contiguous 2KB chunks allocated to
+                                                         engine (X * 16) + 9.
+                                                         ENGINE9 does not exist and is reserved in
+                                                         PKO_REG_ENGINE_STORAGE1. */
+	uint64_t engine8                      : 4;  /**< Number of contiguous 2KB chunks allocated to
+                                                         engine (X * 16) + 8.
+                                                         ENGINE8 does not exist and is reserved in
+                                                         PKO_REG_ENGINE_STORAGE1. */
+	uint64_t engine7                      : 4;  /**< Number of contiguous 2KB chunks allocated to
+                                                         engine (X * 16) + 7.
+                                                         ENGINE7 does not exist and is reserved in
+                                                         PKO_REG_ENGINE_STORAGE1. */
+	uint64_t engine6                      : 4;  /**< Number of contiguous 2KB chunks allocated to
+                                                         engine (X * 16) + 6.
+                                                         ENGINE6 does not exist and is reserved in
+                                                         PKO_REG_ENGINE_STORAGE1. */
+	uint64_t engine5                      : 4;  /**< Number of contiguous 2KB chunks allocated to
+                                                         engine (X * 16) + 5.
+                                                         ENGINE5 does not exist and is reserved in
+                                                         PKO_REG_ENGINE_STORAGE1. */
+	uint64_t engine4                      : 4;  /**< Number of contiguous 2KB chunks allocated to
+                                                         engine (X * 16) + 4.
+                                                         ENGINE4 does not exist and is reserved in
+                                                         PKO_REG_ENGINE_STORAGE1. */
+	uint64_t engine3                      : 4;  /**< Number of contiguous 2KB chunks allocated to
+                                                         engine (X * 16) + 3. */
+	uint64_t engine2                      : 4;  /**< Number of contiguous 2KB chunks allocated to
+                                                         engine (X * 16) + 2. */
+	uint64_t engine1                      : 4;  /**< Number of contiguous 2KB chunks allocated to
+                                                         engine (X * 16) + 1. */
+	uint64_t engine0                      : 4;  /**< Number of contiguous 2KB chunks allocated to
+                                                         engine (X * 16) + 0. */
+#else
+	uint64_t engine0                      : 4;
+	uint64_t engine1                      : 4;
+	uint64_t engine2                      : 4;
+	uint64_t engine3                      : 4;
+	uint64_t engine4                      : 4;
+	uint64_t engine5                      : 4;
+	uint64_t engine6                      : 4;
+	uint64_t engine7                      : 4;
+	uint64_t engine8                      : 4;
+	uint64_t engine9                      : 4;
+	uint64_t engine10                     : 4;
+	uint64_t engine11                     : 4;
+	uint64_t engine12                     : 4;
+	uint64_t engine13                     : 4;
+	uint64_t engine14                     : 4;
+	uint64_t engine15                     : 4;
 #endif
 	} s;
 	struct cvmx_pko_reg_engine_storagex_s cn68xx;
 	struct cvmx_pko_reg_engine_storagex_s cn68xxp1;
 };
+typedef union cvmx_pko_reg_engine_storagex cvmx_pko_reg_engine_storagex_t;
 
+/**
+ * cvmx_pko_reg_engine_thresh
+ *
+ * Notes:
+ * When not enabled, packet data may be sent as soon as it is written into PKO's internal buffers.
+ * When enabled and the packet fits entirely in the PKO's internal buffer, none of the packet data will
+ * be sent until all of it has been written into the PKO's internal buffer.  Note that a packet is
+ * considered to fit entirely only if the packet's size is <= BUFFER_SIZE-8.  When enabled and the
+ * packet does not fit entirely in the PKO's internal buffer, none of the packet data will be sent until
+ * at least BUFFER_SIZE-256 bytes of the packet have been written into the PKO's internal buffer
+ * (note that BUFFER_SIZE is a function of PKO_REG_GMX_PORT_MODE above)
+ */
 union cvmx_pko_reg_engine_thresh {
 	uint64_t u64;
 	struct cvmx_pko_reg_engine_thresh_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_20_63:44;
-		uint64_t mask:20;
+	uint64_t reserved_20_63               : 44;
+	uint64_t mask                         : 20; /**< Mask[n]=0 disables packet send threshold for engine n
+                                                         Mask[n]=1 enables  packet send threshold for engine n  $PR       NS
+                                                         Bits <13:10,7:2> are unused */
 #else
-		uint64_t mask:20;
-		uint64_t reserved_20_63:44;
+	uint64_t mask                         : 20;
+	uint64_t reserved_20_63               : 44;
 #endif
 	} s;
 	struct cvmx_pko_reg_engine_thresh_cn52xx {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_10_63:54;
-		uint64_t mask:10;
+	uint64_t reserved_10_63               : 54;
+	uint64_t mask                         : 10; /**< Mask[n]=0 disables packet send threshold for eng n
+                                                         Mask[n]=1 enables  packet send threshold for eng n     $PR       NS
+                                                         Mask[n] MBZ for n = 4-7, as engines 4-7 dont exist */
 #else
-		uint64_t mask:10;
-		uint64_t reserved_10_63:54;
+	uint64_t mask                         : 10;
+	uint64_t reserved_10_63               : 54;
 #endif
 	} cn52xx;
 	struct cvmx_pko_reg_engine_thresh_cn52xx cn52xxp1;
@@ -2255,570 +14114,807 @@ union cvmx_pko_reg_engine_thresh {
 	struct cvmx_pko_reg_engine_thresh_cn52xx cn56xxp1;
 	struct cvmx_pko_reg_engine_thresh_cn61xx {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_14_63:50;
-		uint64_t mask:14;
+	uint64_t reserved_14_63               : 50;
+	uint64_t mask                         : 14; /**< Mask[n]=0 disables packet send threshold for engine n
+                                                         Mask[n]=1 enables  packet send threshold for engine n  $PR       NS
+                                                         Bits <13:10> are unused */
 #else
-		uint64_t mask:14;
-		uint64_t reserved_14_63:50;
+	uint64_t mask                         : 14;
+	uint64_t reserved_14_63               : 50;
 #endif
 	} cn61xx;
 	struct cvmx_pko_reg_engine_thresh_cn63xx {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_12_63:52;
-		uint64_t mask:12;
+	uint64_t reserved_12_63               : 52;
+	uint64_t mask                         : 12; /**< Mask[n]=0 disables packet send threshold for engine n
+                                                         Mask[n]=1 enables  packet send threshold for engine n  $PR       NS
+                                                         Mask[n] MBZ for n = 4-7, as engines 4-7 dont exist */
 #else
-		uint64_t mask:12;
-		uint64_t reserved_12_63:52;
+	uint64_t mask                         : 12;
+	uint64_t reserved_12_63               : 52;
 #endif
 	} cn63xx;
 	struct cvmx_pko_reg_engine_thresh_cn63xx cn63xxp1;
 	struct cvmx_pko_reg_engine_thresh_cn61xx cn66xx;
-	struct cvmx_pko_reg_engine_thresh_s cn68xx;
-	struct cvmx_pko_reg_engine_thresh_s cn68xxp1;
+	struct cvmx_pko_reg_engine_thresh_s   cn68xx;
+	struct cvmx_pko_reg_engine_thresh_s   cn68xxp1;
+	struct cvmx_pko_reg_engine_thresh_cn61xx cn70xx;
+	struct cvmx_pko_reg_engine_thresh_cn61xx cn70xxp1;
 	struct cvmx_pko_reg_engine_thresh_cn61xx cnf71xx;
 };
+typedef union cvmx_pko_reg_engine_thresh cvmx_pko_reg_engine_thresh_t;
 
+/**
+ * cvmx_pko_reg_error
+ *
+ * Notes:
+ * Note that this CSR is present only in chip revisions beginning with pass2.
+ *
+ */
 union cvmx_pko_reg_error {
 	uint64_t u64;
 	struct cvmx_pko_reg_error_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_4_63:60;
-		uint64_t loopback:1;
-		uint64_t currzero:1;
-		uint64_t doorbell:1;
-		uint64_t parity:1;
+	uint64_t reserved_4_63                : 60;
+	uint64_t loopback                     : 1;  /**< A packet was sent to an illegal loopback port */
+	uint64_t currzero                     : 1;  /**< A packet data pointer has size=0 */
+	uint64_t doorbell                     : 1;  /**< A doorbell count has overflowed */
+	uint64_t parity                       : 1;  /**< Read parity error at port data buffer */
 #else
-		uint64_t parity:1;
-		uint64_t doorbell:1;
-		uint64_t currzero:1;
-		uint64_t loopback:1;
-		uint64_t reserved_4_63:60;
+	uint64_t parity                       : 1;
+	uint64_t doorbell                     : 1;
+	uint64_t currzero                     : 1;
+	uint64_t loopback                     : 1;
+	uint64_t reserved_4_63                : 60;
 #endif
 	} s;
 	struct cvmx_pko_reg_error_cn30xx {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_2_63:62;
-		uint64_t doorbell:1;
-		uint64_t parity:1;
+	uint64_t reserved_2_63                : 62;
+	uint64_t doorbell                     : 1;  /**< A doorbell count has overflowed */
+	uint64_t parity                       : 1;  /**< Read parity error at port data buffer */
 #else
-		uint64_t parity:1;
-		uint64_t doorbell:1;
-		uint64_t reserved_2_63:62;
+	uint64_t parity                       : 1;
+	uint64_t doorbell                     : 1;
+	uint64_t reserved_2_63                : 62;
 #endif
 	} cn30xx;
-	struct cvmx_pko_reg_error_cn30xx cn31xx;
-	struct cvmx_pko_reg_error_cn30xx cn38xx;
-	struct cvmx_pko_reg_error_cn30xx cn38xxp2;
+	struct cvmx_pko_reg_error_cn30xx      cn31xx;
+	struct cvmx_pko_reg_error_cn30xx      cn38xx;
+	struct cvmx_pko_reg_error_cn30xx      cn38xxp2;
 	struct cvmx_pko_reg_error_cn50xx {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_3_63:61;
-		uint64_t currzero:1;
-		uint64_t doorbell:1;
-		uint64_t parity:1;
+	uint64_t reserved_3_63                : 61;
+	uint64_t currzero                     : 1;  /**< A packet data pointer has size=0 */
+	uint64_t doorbell                     : 1;  /**< A doorbell count has overflowed */
+	uint64_t parity                       : 1;  /**< Read parity error at port data buffer */
 #else
-		uint64_t parity:1;
-		uint64_t doorbell:1;
-		uint64_t currzero:1;
-		uint64_t reserved_3_63:61;
+	uint64_t parity                       : 1;
+	uint64_t doorbell                     : 1;
+	uint64_t currzero                     : 1;
+	uint64_t reserved_3_63                : 61;
 #endif
 	} cn50xx;
-	struct cvmx_pko_reg_error_cn50xx cn52xx;
-	struct cvmx_pko_reg_error_cn50xx cn52xxp1;
-	struct cvmx_pko_reg_error_cn50xx cn56xx;
-	struct cvmx_pko_reg_error_cn50xx cn56xxp1;
-	struct cvmx_pko_reg_error_cn50xx cn58xx;
-	struct cvmx_pko_reg_error_cn50xx cn58xxp1;
-	struct cvmx_pko_reg_error_cn50xx cn61xx;
-	struct cvmx_pko_reg_error_cn50xx cn63xx;
-	struct cvmx_pko_reg_error_cn50xx cn63xxp1;
-	struct cvmx_pko_reg_error_cn50xx cn66xx;
-	struct cvmx_pko_reg_error_s cn68xx;
-	struct cvmx_pko_reg_error_s cn68xxp1;
-	struct cvmx_pko_reg_error_cn50xx cnf71xx;
+	struct cvmx_pko_reg_error_cn50xx      cn52xx;
+	struct cvmx_pko_reg_error_cn50xx      cn52xxp1;
+	struct cvmx_pko_reg_error_cn50xx      cn56xx;
+	struct cvmx_pko_reg_error_cn50xx      cn56xxp1;
+	struct cvmx_pko_reg_error_cn50xx      cn58xx;
+	struct cvmx_pko_reg_error_cn50xx      cn58xxp1;
+	struct cvmx_pko_reg_error_cn50xx      cn61xx;
+	struct cvmx_pko_reg_error_cn50xx      cn63xx;
+	struct cvmx_pko_reg_error_cn50xx      cn63xxp1;
+	struct cvmx_pko_reg_error_cn50xx      cn66xx;
+	struct cvmx_pko_reg_error_s           cn68xx;
+	struct cvmx_pko_reg_error_s           cn68xxp1;
+	struct cvmx_pko_reg_error_cn50xx      cn70xx;
+	struct cvmx_pko_reg_error_cn50xx      cn70xxp1;
+	struct cvmx_pko_reg_error_cn50xx      cnf71xx;
 };
+typedef union cvmx_pko_reg_error cvmx_pko_reg_error_t;
 
+/**
+ * cvmx_pko_reg_flags
+ *
+ * Notes:
+ * When set, ENA_PKO enables the PKO picker and places the PKO in normal operation.  When set, ENA_DWB
+ * enables the use of DontWriteBacks during the buffer freeing operations.  When not set, STORE_BE inverts
+ * bits[2:0] of the STORE0 byte write address.  When set, RESET causes a 4-cycle reset pulse to the
+ * entire box.
+ */
 union cvmx_pko_reg_flags {
 	uint64_t u64;
 	struct cvmx_pko_reg_flags_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_9_63:55;
-		uint64_t dis_perf3:1;
-		uint64_t dis_perf2:1;
-		uint64_t dis_perf1:1;
-		uint64_t dis_perf0:1;
-		uint64_t ena_throttle:1;
-		uint64_t reset:1;
-		uint64_t store_be:1;
-		uint64_t ena_dwb:1;
-		uint64_t ena_pko:1;
-#else
-		uint64_t ena_pko:1;
-		uint64_t ena_dwb:1;
-		uint64_t store_be:1;
-		uint64_t reset:1;
-		uint64_t ena_throttle:1;
-		uint64_t dis_perf0:1;
-		uint64_t dis_perf1:1;
-		uint64_t dis_perf2:1;
-		uint64_t dis_perf3:1;
-		uint64_t reserved_9_63:55;
+	uint64_t reserved_9_63                : 55;
+	uint64_t dis_perf3                    : 1;  /**< Set to disable inactive queue QOS skipping */
+	uint64_t dis_perf2                    : 1;  /**< Set to disable inactive queue skipping */
+	uint64_t dis_perf1                    : 1;  /**< Set to disable command word prefetching */
+	uint64_t dis_perf0                    : 1;  /**< Set to disable read performance optimizations */
+	uint64_t ena_throttle                 : 1;  /**< Set to enable the PKO picker throttle logic
+                                                         When ENA_THROTTLE=1 and the most-significant
+                                                         bit of any of the pipe or interface, word or
+                                                         packet throttle count is set, then PKO will
+                                                         not output any packets to the interface/pipe.
+                                                         See PKO_MEM_THROTTLE_PIPE and
+                                                         PKO_MEM_THROTTLE_INT. */
+	uint64_t reset                        : 1;  /**< Reset oneshot pulse */
+	uint64_t store_be                     : 1;  /**< Force STORE0 byte write address to big endian */
+	uint64_t ena_dwb                      : 1;  /**< Set to enable DontWriteBacks */
+	uint64_t ena_pko                      : 1;  /**< Set to enable the PKO picker */
+#else
+	uint64_t ena_pko                      : 1;
+	uint64_t ena_dwb                      : 1;
+	uint64_t store_be                     : 1;
+	uint64_t reset                        : 1;
+	uint64_t ena_throttle                 : 1;
+	uint64_t dis_perf0                    : 1;
+	uint64_t dis_perf1                    : 1;
+	uint64_t dis_perf2                    : 1;
+	uint64_t dis_perf3                    : 1;
+	uint64_t reserved_9_63                : 55;
 #endif
 	} s;
 	struct cvmx_pko_reg_flags_cn30xx {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_4_63:60;
-		uint64_t reset:1;
-		uint64_t store_be:1;
-		uint64_t ena_dwb:1;
-		uint64_t ena_pko:1;
+	uint64_t reserved_4_63                : 60;
+	uint64_t reset                        : 1;  /**< Reset oneshot pulse */
+	uint64_t store_be                     : 1;  /**< Force STORE0 byte write address to big endian */
+	uint64_t ena_dwb                      : 1;  /**< Set to enable DontWriteBacks */
+	uint64_t ena_pko                      : 1;  /**< Set to enable the PKO picker */
 #else
-		uint64_t ena_pko:1;
-		uint64_t ena_dwb:1;
-		uint64_t store_be:1;
-		uint64_t reset:1;
-		uint64_t reserved_4_63:60;
+	uint64_t ena_pko                      : 1;
+	uint64_t ena_dwb                      : 1;
+	uint64_t store_be                     : 1;
+	uint64_t reset                        : 1;
+	uint64_t reserved_4_63                : 60;
 #endif
 	} cn30xx;
-	struct cvmx_pko_reg_flags_cn30xx cn31xx;
-	struct cvmx_pko_reg_flags_cn30xx cn38xx;
-	struct cvmx_pko_reg_flags_cn30xx cn38xxp2;
-	struct cvmx_pko_reg_flags_cn30xx cn50xx;
-	struct cvmx_pko_reg_flags_cn30xx cn52xx;
-	struct cvmx_pko_reg_flags_cn30xx cn52xxp1;
-	struct cvmx_pko_reg_flags_cn30xx cn56xx;
-	struct cvmx_pko_reg_flags_cn30xx cn56xxp1;
-	struct cvmx_pko_reg_flags_cn30xx cn58xx;
-	struct cvmx_pko_reg_flags_cn30xx cn58xxp1;
+	struct cvmx_pko_reg_flags_cn30xx      cn31xx;
+	struct cvmx_pko_reg_flags_cn30xx      cn38xx;
+	struct cvmx_pko_reg_flags_cn30xx      cn38xxp2;
+	struct cvmx_pko_reg_flags_cn30xx      cn50xx;
+	struct cvmx_pko_reg_flags_cn30xx      cn52xx;
+	struct cvmx_pko_reg_flags_cn30xx      cn52xxp1;
+	struct cvmx_pko_reg_flags_cn30xx      cn56xx;
+	struct cvmx_pko_reg_flags_cn30xx      cn56xxp1;
+	struct cvmx_pko_reg_flags_cn30xx      cn58xx;
+	struct cvmx_pko_reg_flags_cn30xx      cn58xxp1;
 	struct cvmx_pko_reg_flags_cn61xx {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_9_63:55;
-		uint64_t dis_perf3:1;
-		uint64_t dis_perf2:1;
-		uint64_t reserved_4_6:3;
-		uint64_t reset:1;
-		uint64_t store_be:1;
-		uint64_t ena_dwb:1;
-		uint64_t ena_pko:1;
-#else
-		uint64_t ena_pko:1;
-		uint64_t ena_dwb:1;
-		uint64_t store_be:1;
-		uint64_t reset:1;
-		uint64_t reserved_4_6:3;
-		uint64_t dis_perf2:1;
-		uint64_t dis_perf3:1;
-		uint64_t reserved_9_63:55;
+	uint64_t reserved_9_63                : 55;
+	uint64_t dis_perf3                    : 1;  /**< Set to disable inactive queue QOS skipping */
+	uint64_t dis_perf2                    : 1;  /**< Set to disable inactive queue skipping */
+	uint64_t reserved_4_6                 : 3;
+	uint64_t reset                        : 1;  /**< Reset oneshot pulse */
+	uint64_t store_be                     : 1;  /**< Force STORE0 byte write address to big endian */
+	uint64_t ena_dwb                      : 1;  /**< Set to enable DontWriteBacks */
+	uint64_t ena_pko                      : 1;  /**< Set to enable the PKO picker */
+#else
+	uint64_t ena_pko                      : 1;
+	uint64_t ena_dwb                      : 1;
+	uint64_t store_be                     : 1;
+	uint64_t reset                        : 1;
+	uint64_t reserved_4_6                 : 3;
+	uint64_t dis_perf2                    : 1;
+	uint64_t dis_perf3                    : 1;
+	uint64_t reserved_9_63                : 55;
 #endif
 	} cn61xx;
-	struct cvmx_pko_reg_flags_cn30xx cn63xx;
-	struct cvmx_pko_reg_flags_cn30xx cn63xxp1;
-	struct cvmx_pko_reg_flags_cn61xx cn66xx;
-	struct cvmx_pko_reg_flags_s cn68xx;
+	struct cvmx_pko_reg_flags_cn30xx      cn63xx;
+	struct cvmx_pko_reg_flags_cn30xx      cn63xxp1;
+	struct cvmx_pko_reg_flags_cn61xx      cn66xx;
+	struct cvmx_pko_reg_flags_s           cn68xx;
 	struct cvmx_pko_reg_flags_cn68xxp1 {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_7_63:57;
-		uint64_t dis_perf1:1;
-		uint64_t dis_perf0:1;
-		uint64_t ena_throttle:1;
-		uint64_t reset:1;
-		uint64_t store_be:1;
-		uint64_t ena_dwb:1;
-		uint64_t ena_pko:1;
-#else
-		uint64_t ena_pko:1;
-		uint64_t ena_dwb:1;
-		uint64_t store_be:1;
-		uint64_t reset:1;
-		uint64_t ena_throttle:1;
-		uint64_t dis_perf0:1;
-		uint64_t dis_perf1:1;
-		uint64_t reserved_7_63:57;
+	uint64_t reserved_7_63                : 57;
+	uint64_t dis_perf1                    : 1;  /**< Set to disable command word prefetching */
+	uint64_t dis_perf0                    : 1;  /**< Set to disable read performance optimizations */
+	uint64_t ena_throttle                 : 1;  /**< Set to enable the PKO picker throttle logic
+                                                         When ENA_THROTTLE=1 and the most-significant
+                                                         bit of any of the pipe or interface, word or
+                                                         packet throttle count is set, then PKO will
+                                                         not output any packets to the interface/pipe.
+                                                         See PKO_MEM_THROTTLE_PIPE and
+                                                         PKO_MEM_THROTTLE_INT. */
+	uint64_t reset                        : 1;  /**< Reset oneshot pulse */
+	uint64_t store_be                     : 1;  /**< Force STORE0 byte write address to big endian */
+	uint64_t ena_dwb                      : 1;  /**< Set to enable DontWriteBacks */
+	uint64_t ena_pko                      : 1;  /**< Set to enable the PKO picker */
+#else
+	uint64_t ena_pko                      : 1;
+	uint64_t ena_dwb                      : 1;
+	uint64_t store_be                     : 1;
+	uint64_t reset                        : 1;
+	uint64_t ena_throttle                 : 1;
+	uint64_t dis_perf0                    : 1;
+	uint64_t dis_perf1                    : 1;
+	uint64_t reserved_7_63                : 57;
 #endif
 	} cn68xxp1;
-	struct cvmx_pko_reg_flags_cn61xx cnf71xx;
+	struct cvmx_pko_reg_flags_cn61xx      cn70xx;
+	struct cvmx_pko_reg_flags_cn61xx      cn70xxp1;
+	struct cvmx_pko_reg_flags_cn61xx      cnf71xx;
 };
+typedef union cvmx_pko_reg_flags cvmx_pko_reg_flags_t;
 
+/**
+ * cvmx_pko_reg_gmx_port_mode
+ *
+ * Notes:
+ * The system has a total of 2 + 4 + 4 ports and 2 + 1 + 1 engines (GM0 + PCI + LOOP).
+ * This CSR sets the number of GMX0 ports and amount of local storage per engine.
+ * It has no effect on the number of ports or amount of local storage per engine for PCI and LOOP.
+ * When both GMX ports are used (MODE0=3), each GMX engine has 10kB of local
+ * storage.  Increasing MODE0 to 4 decreases the number of GMX ports to 1 and
+ * increases the local storage for the one remaining PKO GMX engine to 20kB.
+ * MODE0 value 0, 1, and 2, or greater than 4 are illegal.
+ *
+ * MODE0   GMX0  PCI   LOOP  GMX0                       PCI            LOOP
+ *         ports ports ports storage/engine             storage/engine storage/engine
+ * 3       2     4     4      10.0kB                    2.5kB          2.5kB
+ * 4       1     4     4      20.0kB                    2.5kB          2.5kB
+ */
 union cvmx_pko_reg_gmx_port_mode {
 	uint64_t u64;
 	struct cvmx_pko_reg_gmx_port_mode_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_6_63:58;
-		uint64_t mode1:3;
-		uint64_t mode0:3;
-#else
-		uint64_t mode0:3;
-		uint64_t mode1:3;
-		uint64_t reserved_6_63:58;
-#endif
-	} s;
-	struct cvmx_pko_reg_gmx_port_mode_s cn30xx;
-	struct cvmx_pko_reg_gmx_port_mode_s cn31xx;
-	struct cvmx_pko_reg_gmx_port_mode_s cn38xx;
-	struct cvmx_pko_reg_gmx_port_mode_s cn38xxp2;
-	struct cvmx_pko_reg_gmx_port_mode_s cn50xx;
-	struct cvmx_pko_reg_gmx_port_mode_s cn52xx;
-	struct cvmx_pko_reg_gmx_port_mode_s cn52xxp1;
-	struct cvmx_pko_reg_gmx_port_mode_s cn56xx;
-	struct cvmx_pko_reg_gmx_port_mode_s cn56xxp1;
-	struct cvmx_pko_reg_gmx_port_mode_s cn58xx;
-	struct cvmx_pko_reg_gmx_port_mode_s cn58xxp1;
-	struct cvmx_pko_reg_gmx_port_mode_s cn61xx;
-	struct cvmx_pko_reg_gmx_port_mode_s cn63xx;
-	struct cvmx_pko_reg_gmx_port_mode_s cn63xxp1;
-	struct cvmx_pko_reg_gmx_port_mode_s cn66xx;
-	struct cvmx_pko_reg_gmx_port_mode_s cnf71xx;
+	uint64_t reserved_6_63                : 58;
+	uint64_t mode1                        : 3;  /**< RESERVED, must be 5 */
+	uint64_t mode0                        : 3;  /**< # of GM0 ports = 16 >> MODE0 */
+#else
+	uint64_t mode0                        : 3;
+	uint64_t mode1                        : 3;
+	uint64_t reserved_6_63                : 58;
+#endif
+	} s;
+	struct cvmx_pko_reg_gmx_port_mode_s   cn30xx;
+	struct cvmx_pko_reg_gmx_port_mode_s   cn31xx;
+	struct cvmx_pko_reg_gmx_port_mode_s   cn38xx;
+	struct cvmx_pko_reg_gmx_port_mode_s   cn38xxp2;
+	struct cvmx_pko_reg_gmx_port_mode_s   cn50xx;
+	struct cvmx_pko_reg_gmx_port_mode_s   cn52xx;
+	struct cvmx_pko_reg_gmx_port_mode_s   cn52xxp1;
+	struct cvmx_pko_reg_gmx_port_mode_s   cn56xx;
+	struct cvmx_pko_reg_gmx_port_mode_s   cn56xxp1;
+	struct cvmx_pko_reg_gmx_port_mode_s   cn58xx;
+	struct cvmx_pko_reg_gmx_port_mode_s   cn58xxp1;
+	struct cvmx_pko_reg_gmx_port_mode_s   cn61xx;
+	struct cvmx_pko_reg_gmx_port_mode_s   cn63xx;
+	struct cvmx_pko_reg_gmx_port_mode_s   cn63xxp1;
+	struct cvmx_pko_reg_gmx_port_mode_s   cn66xx;
+	struct cvmx_pko_reg_gmx_port_mode_s   cn70xx;
+	struct cvmx_pko_reg_gmx_port_mode_s   cn70xxp1;
+	struct cvmx_pko_reg_gmx_port_mode_s   cnf71xx;
 };
+typedef union cvmx_pko_reg_gmx_port_mode cvmx_pko_reg_gmx_port_mode_t;
 
+/**
+ * cvmx_pko_reg_int_mask
+ *
+ * Notes:
+ * When a mask bit is set, the corresponding interrupt is enabled.
+ *
+ */
 union cvmx_pko_reg_int_mask {
 	uint64_t u64;
 	struct cvmx_pko_reg_int_mask_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_4_63:60;
-		uint64_t loopback:1;
-		uint64_t currzero:1;
-		uint64_t doorbell:1;
-		uint64_t parity:1;
+	uint64_t reserved_4_63                : 60;
+	uint64_t loopback                     : 1;  /**< Bit mask corresponding to PKO_REG_ERROR[3] above */
+	uint64_t currzero                     : 1;  /**< Bit mask corresponding to PKO_REG_ERROR[2] above */
+	uint64_t doorbell                     : 1;  /**< Bit mask corresponding to PKO_REG_ERROR[1] above */
+	uint64_t parity                       : 1;  /**< Bit mask corresponding to PKO_REG_ERROR[0] above */
 #else
-		uint64_t parity:1;
-		uint64_t doorbell:1;
-		uint64_t currzero:1;
-		uint64_t loopback:1;
-		uint64_t reserved_4_63:60;
+	uint64_t parity                       : 1;
+	uint64_t doorbell                     : 1;
+	uint64_t currzero                     : 1;
+	uint64_t loopback                     : 1;
+	uint64_t reserved_4_63                : 60;
 #endif
 	} s;
 	struct cvmx_pko_reg_int_mask_cn30xx {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_2_63:62;
-		uint64_t doorbell:1;
-		uint64_t parity:1;
+	uint64_t reserved_2_63                : 62;
+	uint64_t doorbell                     : 1;  /**< Bit mask corresponding to PKO_REG_ERROR[1] above */
+	uint64_t parity                       : 1;  /**< Bit mask corresponding to PKO_REG_ERROR[0] above */
 #else
-		uint64_t parity:1;
-		uint64_t doorbell:1;
-		uint64_t reserved_2_63:62;
+	uint64_t parity                       : 1;
+	uint64_t doorbell                     : 1;
+	uint64_t reserved_2_63                : 62;
 #endif
 	} cn30xx;
-	struct cvmx_pko_reg_int_mask_cn30xx cn31xx;
-	struct cvmx_pko_reg_int_mask_cn30xx cn38xx;
-	struct cvmx_pko_reg_int_mask_cn30xx cn38xxp2;
+	struct cvmx_pko_reg_int_mask_cn30xx   cn31xx;
+	struct cvmx_pko_reg_int_mask_cn30xx   cn38xx;
+	struct cvmx_pko_reg_int_mask_cn30xx   cn38xxp2;
 	struct cvmx_pko_reg_int_mask_cn50xx {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_3_63:61;
-		uint64_t currzero:1;
-		uint64_t doorbell:1;
-		uint64_t parity:1;
+	uint64_t reserved_3_63                : 61;
+	uint64_t currzero                     : 1;  /**< Bit mask corresponding to PKO_REG_ERROR[2] above */
+	uint64_t doorbell                     : 1;  /**< Bit mask corresponding to PKO_REG_ERROR[1] above */
+	uint64_t parity                       : 1;  /**< Bit mask corresponding to PKO_REG_ERROR[0] above */
 #else
-		uint64_t parity:1;
-		uint64_t doorbell:1;
-		uint64_t currzero:1;
-		uint64_t reserved_3_63:61;
+	uint64_t parity                       : 1;
+	uint64_t doorbell                     : 1;
+	uint64_t currzero                     : 1;
+	uint64_t reserved_3_63                : 61;
 #endif
 	} cn50xx;
-	struct cvmx_pko_reg_int_mask_cn50xx cn52xx;
-	struct cvmx_pko_reg_int_mask_cn50xx cn52xxp1;
-	struct cvmx_pko_reg_int_mask_cn50xx cn56xx;
-	struct cvmx_pko_reg_int_mask_cn50xx cn56xxp1;
-	struct cvmx_pko_reg_int_mask_cn50xx cn58xx;
-	struct cvmx_pko_reg_int_mask_cn50xx cn58xxp1;
-	struct cvmx_pko_reg_int_mask_cn50xx cn61xx;
-	struct cvmx_pko_reg_int_mask_cn50xx cn63xx;
-	struct cvmx_pko_reg_int_mask_cn50xx cn63xxp1;
-	struct cvmx_pko_reg_int_mask_cn50xx cn66xx;
-	struct cvmx_pko_reg_int_mask_s cn68xx;
-	struct cvmx_pko_reg_int_mask_s cn68xxp1;
-	struct cvmx_pko_reg_int_mask_cn50xx cnf71xx;
+	struct cvmx_pko_reg_int_mask_cn50xx   cn52xx;
+	struct cvmx_pko_reg_int_mask_cn50xx   cn52xxp1;
+	struct cvmx_pko_reg_int_mask_cn50xx   cn56xx;
+	struct cvmx_pko_reg_int_mask_cn50xx   cn56xxp1;
+	struct cvmx_pko_reg_int_mask_cn50xx   cn58xx;
+	struct cvmx_pko_reg_int_mask_cn50xx   cn58xxp1;
+	struct cvmx_pko_reg_int_mask_cn50xx   cn61xx;
+	struct cvmx_pko_reg_int_mask_cn50xx   cn63xx;
+	struct cvmx_pko_reg_int_mask_cn50xx   cn63xxp1;
+	struct cvmx_pko_reg_int_mask_cn50xx   cn66xx;
+	struct cvmx_pko_reg_int_mask_s        cn68xx;
+	struct cvmx_pko_reg_int_mask_s        cn68xxp1;
+	struct cvmx_pko_reg_int_mask_cn50xx   cn70xx;
+	struct cvmx_pko_reg_int_mask_cn50xx   cn70xxp1;
+	struct cvmx_pko_reg_int_mask_cn50xx   cnf71xx;
 };
+typedef union cvmx_pko_reg_int_mask cvmx_pko_reg_int_mask_t;
 
+/**
+ * cvmx_pko_reg_loopback_bpid
+ *
+ * Notes:
+ * None.
+ *
+ */
 union cvmx_pko_reg_loopback_bpid {
 	uint64_t u64;
 	struct cvmx_pko_reg_loopback_bpid_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_59_63:5;
-		uint64_t bpid7:6;
-		uint64_t reserved_52_52:1;
-		uint64_t bpid6:6;
-		uint64_t reserved_45_45:1;
-		uint64_t bpid5:6;
-		uint64_t reserved_38_38:1;
-		uint64_t bpid4:6;
-		uint64_t reserved_31_31:1;
-		uint64_t bpid3:6;
-		uint64_t reserved_24_24:1;
-		uint64_t bpid2:6;
-		uint64_t reserved_17_17:1;
-		uint64_t bpid1:6;
-		uint64_t reserved_10_10:1;
-		uint64_t bpid0:6;
-		uint64_t reserved_0_3:4;
-#else
-		uint64_t reserved_0_3:4;
-		uint64_t bpid0:6;
-		uint64_t reserved_10_10:1;
-		uint64_t bpid1:6;
-		uint64_t reserved_17_17:1;
-		uint64_t bpid2:6;
-		uint64_t reserved_24_24:1;
-		uint64_t bpid3:6;
-		uint64_t reserved_31_31:1;
-		uint64_t bpid4:6;
-		uint64_t reserved_38_38:1;
-		uint64_t bpid5:6;
-		uint64_t reserved_45_45:1;
-		uint64_t bpid6:6;
-		uint64_t reserved_52_52:1;
-		uint64_t bpid7:6;
-		uint64_t reserved_59_63:5;
-#endif
-	} s;
-	struct cvmx_pko_reg_loopback_bpid_s cn68xx;
-	struct cvmx_pko_reg_loopback_bpid_s cn68xxp1;
+	uint64_t reserved_59_63               : 5;
+	uint64_t bpid7                        : 6;  /**< Loopback port 7 backpressure-ID */
+	uint64_t reserved_52_52               : 1;
+	uint64_t bpid6                        : 6;  /**< Loopback port 6 backpressure-ID */
+	uint64_t reserved_45_45               : 1;
+	uint64_t bpid5                        : 6;  /**< Loopback port 5 backpressure-ID */
+	uint64_t reserved_38_38               : 1;
+	uint64_t bpid4                        : 6;  /**< Loopback port 4 backpressure-ID */
+	uint64_t reserved_31_31               : 1;
+	uint64_t bpid3                        : 6;  /**< Loopback port 3 backpressure-ID */
+	uint64_t reserved_24_24               : 1;
+	uint64_t bpid2                        : 6;  /**< Loopback port 2 backpressure-ID */
+	uint64_t reserved_17_17               : 1;
+	uint64_t bpid1                        : 6;  /**< Loopback port 1 backpressure-ID */
+	uint64_t reserved_10_10               : 1;
+	uint64_t bpid0                        : 6;  /**< Loopback port 0 backpressure-ID */
+	uint64_t reserved_0_3                 : 4;
+#else
+	uint64_t reserved_0_3                 : 4;
+	uint64_t bpid0                        : 6;
+	uint64_t reserved_10_10               : 1;
+	uint64_t bpid1                        : 6;
+	uint64_t reserved_17_17               : 1;
+	uint64_t bpid2                        : 6;
+	uint64_t reserved_24_24               : 1;
+	uint64_t bpid3                        : 6;
+	uint64_t reserved_31_31               : 1;
+	uint64_t bpid4                        : 6;
+	uint64_t reserved_38_38               : 1;
+	uint64_t bpid5                        : 6;
+	uint64_t reserved_45_45               : 1;
+	uint64_t bpid6                        : 6;
+	uint64_t reserved_52_52               : 1;
+	uint64_t bpid7                        : 6;
+	uint64_t reserved_59_63               : 5;
+#endif
+	} s;
+	struct cvmx_pko_reg_loopback_bpid_s   cn68xx;
+	struct cvmx_pko_reg_loopback_bpid_s   cn68xxp1;
 };
+typedef union cvmx_pko_reg_loopback_bpid cvmx_pko_reg_loopback_bpid_t;
 
+/**
+ * cvmx_pko_reg_loopback_pkind
+ *
+ * Notes:
+ * None.
+ *
+ */
 union cvmx_pko_reg_loopback_pkind {
 	uint64_t u64;
 	struct cvmx_pko_reg_loopback_pkind_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_59_63:5;
-		uint64_t pkind7:6;
-		uint64_t reserved_52_52:1;
-		uint64_t pkind6:6;
-		uint64_t reserved_45_45:1;
-		uint64_t pkind5:6;
-		uint64_t reserved_38_38:1;
-		uint64_t pkind4:6;
-		uint64_t reserved_31_31:1;
-		uint64_t pkind3:6;
-		uint64_t reserved_24_24:1;
-		uint64_t pkind2:6;
-		uint64_t reserved_17_17:1;
-		uint64_t pkind1:6;
-		uint64_t reserved_10_10:1;
-		uint64_t pkind0:6;
-		uint64_t num_ports:4;
-#else
-		uint64_t num_ports:4;
-		uint64_t pkind0:6;
-		uint64_t reserved_10_10:1;
-		uint64_t pkind1:6;
-		uint64_t reserved_17_17:1;
-		uint64_t pkind2:6;
-		uint64_t reserved_24_24:1;
-		uint64_t pkind3:6;
-		uint64_t reserved_31_31:1;
-		uint64_t pkind4:6;
-		uint64_t reserved_38_38:1;
-		uint64_t pkind5:6;
-		uint64_t reserved_45_45:1;
-		uint64_t pkind6:6;
-		uint64_t reserved_52_52:1;
-		uint64_t pkind7:6;
-		uint64_t reserved_59_63:5;
-#endif
-	} s;
-	struct cvmx_pko_reg_loopback_pkind_s cn68xx;
-	struct cvmx_pko_reg_loopback_pkind_s cn68xxp1;
+	uint64_t reserved_59_63               : 5;
+	uint64_t pkind7                       : 6;  /**< Loopback port 7 port-kind */
+	uint64_t reserved_52_52               : 1;
+	uint64_t pkind6                       : 6;  /**< Loopback port 6 port-kind */
+	uint64_t reserved_45_45               : 1;
+	uint64_t pkind5                       : 6;  /**< Loopback port 5 port-kind */
+	uint64_t reserved_38_38               : 1;
+	uint64_t pkind4                       : 6;  /**< Loopback port 4 port-kind */
+	uint64_t reserved_31_31               : 1;
+	uint64_t pkind3                       : 6;  /**< Loopback port 3 port-kind */
+	uint64_t reserved_24_24               : 1;
+	uint64_t pkind2                       : 6;  /**< Loopback port 2 port-kind */
+	uint64_t reserved_17_17               : 1;
+	uint64_t pkind1                       : 6;  /**< Loopback port 1 port-kind */
+	uint64_t reserved_10_10               : 1;
+	uint64_t pkind0                       : 6;  /**< Loopback port 0 port-kind */
+	uint64_t num_ports                    : 4;  /**< Number of loopback ports, 0 <= NUM_PORTS <= 8 */
+#else
+	uint64_t num_ports                    : 4;
+	uint64_t pkind0                       : 6;
+	uint64_t reserved_10_10               : 1;
+	uint64_t pkind1                       : 6;
+	uint64_t reserved_17_17               : 1;
+	uint64_t pkind2                       : 6;
+	uint64_t reserved_24_24               : 1;
+	uint64_t pkind3                       : 6;
+	uint64_t reserved_31_31               : 1;
+	uint64_t pkind4                       : 6;
+	uint64_t reserved_38_38               : 1;
+	uint64_t pkind5                       : 6;
+	uint64_t reserved_45_45               : 1;
+	uint64_t pkind6                       : 6;
+	uint64_t reserved_52_52               : 1;
+	uint64_t pkind7                       : 6;
+	uint64_t reserved_59_63               : 5;
+#endif
+	} s;
+	struct cvmx_pko_reg_loopback_pkind_s  cn68xx;
+	struct cvmx_pko_reg_loopback_pkind_s  cn68xxp1;
 };
+typedef union cvmx_pko_reg_loopback_pkind cvmx_pko_reg_loopback_pkind_t;
 
+/**
+ * cvmx_pko_reg_min_pkt
+ *
+ * Notes:
+ * This CSR is used with PKO_MEM_IPORT_PTRS[MIN_PKT] to select the minimum packet size.  Packets whose
+ * size in bytes < (SIZEn+1) are zero-padded to (SIZEn+1) bytes.  Note that this does not include CRC bytes.
+ * SIZE0=0 is read-only and is used when no padding is desired.
+ */
 union cvmx_pko_reg_min_pkt {
 	uint64_t u64;
 	struct cvmx_pko_reg_min_pkt_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t size7:8;
-		uint64_t size6:8;
-		uint64_t size5:8;
-		uint64_t size4:8;
-		uint64_t size3:8;
-		uint64_t size2:8;
-		uint64_t size1:8;
-		uint64_t size0:8;
+	uint64_t size7                        : 8;  /**< Minimum packet size-1 in bytes                                NS */
+	uint64_t size6                        : 8;  /**< Minimum packet size-1 in bytes                                NS */
+	uint64_t size5                        : 8;  /**< Minimum packet size-1 in bytes                                NS */
+	uint64_t size4                        : 8;  /**< Minimum packet size-1 in bytes                                NS */
+	uint64_t size3                        : 8;  /**< Minimum packet size-1 in bytes                                NS */
+	uint64_t size2                        : 8;  /**< Minimum packet size-1 in bytes                                NS */
+	uint64_t size1                        : 8;  /**< Minimum packet size-1 in bytes                                NS */
+	uint64_t size0                        : 8;  /**< Minimum packet size-1 in bytes                                NS */
 #else
-		uint64_t size0:8;
-		uint64_t size1:8;
-		uint64_t size2:8;
-		uint64_t size3:8;
-		uint64_t size4:8;
-		uint64_t size5:8;
-		uint64_t size6:8;
-		uint64_t size7:8;
+	uint64_t size0                        : 8;
+	uint64_t size1                        : 8;
+	uint64_t size2                        : 8;
+	uint64_t size3                        : 8;
+	uint64_t size4                        : 8;
+	uint64_t size5                        : 8;
+	uint64_t size6                        : 8;
+	uint64_t size7                        : 8;
 #endif
 	} s;
-	struct cvmx_pko_reg_min_pkt_s cn68xx;
-	struct cvmx_pko_reg_min_pkt_s cn68xxp1;
+	struct cvmx_pko_reg_min_pkt_s         cn68xx;
+	struct cvmx_pko_reg_min_pkt_s         cn68xxp1;
 };
+typedef union cvmx_pko_reg_min_pkt cvmx_pko_reg_min_pkt_t;
 
+/**
+ * cvmx_pko_reg_preempt
+ */
 union cvmx_pko_reg_preempt {
 	uint64_t u64;
 	struct cvmx_pko_reg_preempt_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_16_63:48;
-		uint64_t min_size:16;
+	uint64_t reserved_16_63               : 48;
+	uint64_t min_size                     : 16; /**< Threshhold for packet preemption, measured in bytes.
+                                                         Only packets which have at least MIN_SIZE bytes
+                                                         remaining to be read can be preempted. */
 #else
-		uint64_t min_size:16;
-		uint64_t reserved_16_63:48;
+	uint64_t min_size                     : 16;
+	uint64_t reserved_16_63               : 48;
 #endif
 	} s;
-	struct cvmx_pko_reg_preempt_s cn52xx;
-	struct cvmx_pko_reg_preempt_s cn52xxp1;
-	struct cvmx_pko_reg_preempt_s cn56xx;
-	struct cvmx_pko_reg_preempt_s cn56xxp1;
-	struct cvmx_pko_reg_preempt_s cn61xx;
-	struct cvmx_pko_reg_preempt_s cn63xx;
-	struct cvmx_pko_reg_preempt_s cn63xxp1;
-	struct cvmx_pko_reg_preempt_s cn66xx;
-	struct cvmx_pko_reg_preempt_s cn68xx;
-	struct cvmx_pko_reg_preempt_s cn68xxp1;
-	struct cvmx_pko_reg_preempt_s cnf71xx;
+	struct cvmx_pko_reg_preempt_s         cn52xx;
+	struct cvmx_pko_reg_preempt_s         cn52xxp1;
+	struct cvmx_pko_reg_preempt_s         cn56xx;
+	struct cvmx_pko_reg_preempt_s         cn56xxp1;
+	struct cvmx_pko_reg_preempt_s         cn61xx;
+	struct cvmx_pko_reg_preempt_s         cn63xx;
+	struct cvmx_pko_reg_preempt_s         cn63xxp1;
+	struct cvmx_pko_reg_preempt_s         cn66xx;
+	struct cvmx_pko_reg_preempt_s         cn68xx;
+	struct cvmx_pko_reg_preempt_s         cn68xxp1;
+	struct cvmx_pko_reg_preempt_s         cn70xx;
+	struct cvmx_pko_reg_preempt_s         cn70xxp1;
+	struct cvmx_pko_reg_preempt_s         cnf71xx;
 };
+typedef union cvmx_pko_reg_preempt cvmx_pko_reg_preempt_t;
 
+/**
+ * cvmx_pko_reg_queue_mode
+ *
+ * Notes:
+ * Sets the number of queues and amount of local storage per queue
+ * The system has a total of 256 queues and (256*8) words of local command storage.  This CSR sets the
+ * number of queues that are used.  Increasing the value of MODE by 1 decreases the number of queues
+ * by a power of 2 and increases the local storage per queue by a power of 2.
+ * MODEn queues storage/queue
+ * 0     256     64B ( 8 words)
+ * 1     128    128B (16 words)
+ * 2      64    256B (32 words)
+ */
 union cvmx_pko_reg_queue_mode {
 	uint64_t u64;
 	struct cvmx_pko_reg_queue_mode_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_2_63:62;
-		uint64_t mode:2;
-#else
-		uint64_t mode:2;
-		uint64_t reserved_2_63:62;
-#endif
-	} s;
-	struct cvmx_pko_reg_queue_mode_s cn30xx;
-	struct cvmx_pko_reg_queue_mode_s cn31xx;
-	struct cvmx_pko_reg_queue_mode_s cn38xx;
-	struct cvmx_pko_reg_queue_mode_s cn38xxp2;
-	struct cvmx_pko_reg_queue_mode_s cn50xx;
-	struct cvmx_pko_reg_queue_mode_s cn52xx;
-	struct cvmx_pko_reg_queue_mode_s cn52xxp1;
-	struct cvmx_pko_reg_queue_mode_s cn56xx;
-	struct cvmx_pko_reg_queue_mode_s cn56xxp1;
-	struct cvmx_pko_reg_queue_mode_s cn58xx;
-	struct cvmx_pko_reg_queue_mode_s cn58xxp1;
-	struct cvmx_pko_reg_queue_mode_s cn61xx;
-	struct cvmx_pko_reg_queue_mode_s cn63xx;
-	struct cvmx_pko_reg_queue_mode_s cn63xxp1;
-	struct cvmx_pko_reg_queue_mode_s cn66xx;
-	struct cvmx_pko_reg_queue_mode_s cn68xx;
-	struct cvmx_pko_reg_queue_mode_s cn68xxp1;
-	struct cvmx_pko_reg_queue_mode_s cnf71xx;
+	uint64_t reserved_2_63                : 62;
+	uint64_t mode                         : 2;  /**< # of queues = 256 >> MODE, 0 <= MODE <=2 */
+#else
+	uint64_t mode                         : 2;
+	uint64_t reserved_2_63                : 62;
+#endif
+	} s;
+	struct cvmx_pko_reg_queue_mode_s      cn30xx;
+	struct cvmx_pko_reg_queue_mode_s      cn31xx;
+	struct cvmx_pko_reg_queue_mode_s      cn38xx;
+	struct cvmx_pko_reg_queue_mode_s      cn38xxp2;
+	struct cvmx_pko_reg_queue_mode_s      cn50xx;
+	struct cvmx_pko_reg_queue_mode_s      cn52xx;
+	struct cvmx_pko_reg_queue_mode_s      cn52xxp1;
+	struct cvmx_pko_reg_queue_mode_s      cn56xx;
+	struct cvmx_pko_reg_queue_mode_s      cn56xxp1;
+	struct cvmx_pko_reg_queue_mode_s      cn58xx;
+	struct cvmx_pko_reg_queue_mode_s      cn58xxp1;
+	struct cvmx_pko_reg_queue_mode_s      cn61xx;
+	struct cvmx_pko_reg_queue_mode_s      cn63xx;
+	struct cvmx_pko_reg_queue_mode_s      cn63xxp1;
+	struct cvmx_pko_reg_queue_mode_s      cn66xx;
+	struct cvmx_pko_reg_queue_mode_s      cn68xx;
+	struct cvmx_pko_reg_queue_mode_s      cn68xxp1;
+	struct cvmx_pko_reg_queue_mode_s      cn70xx;
+	struct cvmx_pko_reg_queue_mode_s      cn70xxp1;
+	struct cvmx_pko_reg_queue_mode_s      cnf71xx;
 };
+typedef union cvmx_pko_reg_queue_mode cvmx_pko_reg_queue_mode_t;
 
+/**
+ * cvmx_pko_reg_queue_preempt
+ *
+ * Notes:
+ * Per QID, setting both PREEMPTER=1 and PREEMPTEE=1 is illegal and sets only PREEMPTER=1.
+ * This CSR is used with PKO_MEM_QUEUE_PTRS and PKO_REG_QUEUE_PTRS1.  When programming queues, the
+ * programming sequence must first write PKO_REG_QUEUE_PREEMPT, then PKO_REG_QUEUE_PTRS1 and then
+ * PKO_MEM_QUEUE_PTRS for each queue.  Preemption is supported only on queues that are ultimately
+ * mapped to engines 0-7.  It is illegal to set preemptee or preempter for a queue that is ultimately
+ * mapped to engines 8-11.
+ *
+ * Also, PKO_REG_ENGINE_INFLIGHT must be at least 2 for any engine on which preemption is enabled.
+ *
+ * See the descriptions of PKO_MEM_QUEUE_PTRS for further explanation of queue programming.
+ */
 union cvmx_pko_reg_queue_preempt {
 	uint64_t u64;
 	struct cvmx_pko_reg_queue_preempt_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_2_63:62;
-		uint64_t preemptee:1;
-		uint64_t preempter:1;
+	uint64_t reserved_2_63                : 62;
+	uint64_t preemptee                    : 1;  /**< Allow this QID to be preempted.
+                                                         0=cannot be preempted, 1=can be preempted */
+	uint64_t preempter                    : 1;  /**< Preempts the servicing of packet on PID to
+                                                         allow this QID immediate servicing.  0=do not cause
+                                                         preemption, 1=cause preemption.  Per PID, at most
+                                                         1 QID can have this bit set. */
 #else
-		uint64_t preempter:1;
-		uint64_t preemptee:1;
-		uint64_t reserved_2_63:62;
+	uint64_t preempter                    : 1;
+	uint64_t preemptee                    : 1;
+	uint64_t reserved_2_63                : 62;
 #endif
 	} s;
-	struct cvmx_pko_reg_queue_preempt_s cn52xx;
-	struct cvmx_pko_reg_queue_preempt_s cn52xxp1;
-	struct cvmx_pko_reg_queue_preempt_s cn56xx;
-	struct cvmx_pko_reg_queue_preempt_s cn56xxp1;
-	struct cvmx_pko_reg_queue_preempt_s cn61xx;
-	struct cvmx_pko_reg_queue_preempt_s cn63xx;
-	struct cvmx_pko_reg_queue_preempt_s cn63xxp1;
-	struct cvmx_pko_reg_queue_preempt_s cn66xx;
-	struct cvmx_pko_reg_queue_preempt_s cn68xx;
-	struct cvmx_pko_reg_queue_preempt_s cn68xxp1;
-	struct cvmx_pko_reg_queue_preempt_s cnf71xx;
+	struct cvmx_pko_reg_queue_preempt_s   cn52xx;
+	struct cvmx_pko_reg_queue_preempt_s   cn52xxp1;
+	struct cvmx_pko_reg_queue_preempt_s   cn56xx;
+	struct cvmx_pko_reg_queue_preempt_s   cn56xxp1;
+	struct cvmx_pko_reg_queue_preempt_s   cn61xx;
+	struct cvmx_pko_reg_queue_preempt_s   cn63xx;
+	struct cvmx_pko_reg_queue_preempt_s   cn63xxp1;
+	struct cvmx_pko_reg_queue_preempt_s   cn66xx;
+	struct cvmx_pko_reg_queue_preempt_s   cn68xx;
+	struct cvmx_pko_reg_queue_preempt_s   cn68xxp1;
+	struct cvmx_pko_reg_queue_preempt_s   cn70xx;
+	struct cvmx_pko_reg_queue_preempt_s   cn70xxp1;
+	struct cvmx_pko_reg_queue_preempt_s   cnf71xx;
 };
+typedef union cvmx_pko_reg_queue_preempt cvmx_pko_reg_queue_preempt_t;
 
+/**
+ * cvmx_pko_reg_queue_ptrs1
+ *
+ * Notes:
+ * This CSR is used with PKO_MEM_QUEUE_PTRS and PKO_MEM_QUEUE_QOS to allow access to queues 128-255
+ * and to allow up mapping of up to 16 queues per port.  When programming queues 128-255, the
+ * programming sequence must first write PKO_REG_QUEUE_PTRS1 and then write PKO_MEM_QUEUE_PTRS or
+ * PKO_MEM_QUEUE_QOS for each queue.
+ * See the descriptions of PKO_MEM_QUEUE_PTRS and PKO_MEM_QUEUE_QOS for further explanation of queue
+ * programming.
+ */
 union cvmx_pko_reg_queue_ptrs1 {
 	uint64_t u64;
 	struct cvmx_pko_reg_queue_ptrs1_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_2_63:62;
-		uint64_t idx3:1;
-		uint64_t qid7:1;
+	uint64_t reserved_2_63                : 62;
+	uint64_t idx3                         : 1;  /**< [3] of Index (distance from head) in the queue array */
+	uint64_t qid7                         : 1;  /**< [7] of Queue ID */
 #else
-		uint64_t qid7:1;
-		uint64_t idx3:1;
-		uint64_t reserved_2_63:62;
+	uint64_t qid7                         : 1;
+	uint64_t idx3                         : 1;
+	uint64_t reserved_2_63                : 62;
 #endif
 	} s;
-	struct cvmx_pko_reg_queue_ptrs1_s cn50xx;
-	struct cvmx_pko_reg_queue_ptrs1_s cn52xx;
-	struct cvmx_pko_reg_queue_ptrs1_s cn52xxp1;
-	struct cvmx_pko_reg_queue_ptrs1_s cn56xx;
-	struct cvmx_pko_reg_queue_ptrs1_s cn56xxp1;
-	struct cvmx_pko_reg_queue_ptrs1_s cn58xx;
-	struct cvmx_pko_reg_queue_ptrs1_s cn58xxp1;
-	struct cvmx_pko_reg_queue_ptrs1_s cn61xx;
-	struct cvmx_pko_reg_queue_ptrs1_s cn63xx;
-	struct cvmx_pko_reg_queue_ptrs1_s cn63xxp1;
-	struct cvmx_pko_reg_queue_ptrs1_s cn66xx;
-	struct cvmx_pko_reg_queue_ptrs1_s cnf71xx;
+	struct cvmx_pko_reg_queue_ptrs1_s     cn50xx;
+	struct cvmx_pko_reg_queue_ptrs1_s     cn52xx;
+	struct cvmx_pko_reg_queue_ptrs1_s     cn52xxp1;
+	struct cvmx_pko_reg_queue_ptrs1_s     cn56xx;
+	struct cvmx_pko_reg_queue_ptrs1_s     cn56xxp1;
+	struct cvmx_pko_reg_queue_ptrs1_s     cn58xx;
+	struct cvmx_pko_reg_queue_ptrs1_s     cn58xxp1;
+	struct cvmx_pko_reg_queue_ptrs1_s     cn61xx;
+	struct cvmx_pko_reg_queue_ptrs1_s     cn63xx;
+	struct cvmx_pko_reg_queue_ptrs1_s     cn63xxp1;
+	struct cvmx_pko_reg_queue_ptrs1_s     cn66xx;
+	struct cvmx_pko_reg_queue_ptrs1_s     cn70xx;
+	struct cvmx_pko_reg_queue_ptrs1_s     cn70xxp1;
+	struct cvmx_pko_reg_queue_ptrs1_s     cnf71xx;
 };
+typedef union cvmx_pko_reg_queue_ptrs1 cvmx_pko_reg_queue_ptrs1_t;
 
+/**
+ * cvmx_pko_reg_read_idx
+ *
+ * Notes:
+ * Provides the read index during a CSR read operation to any of the CSRs that are physically stored
+ * as memories.  The names of these CSRs begin with the prefix "PKO_MEM_".
+ * IDX[7:0] is the read index.  INC[7:0] is an increment that is added to IDX[7:0] after any CSR read.
+ * The intended use is to initially write this CSR such that IDX=0 and INC=1.  Then, the entire
+ * contents of a CSR memory can be read with consecutive CSR read commands.
+ */
 union cvmx_pko_reg_read_idx {
 	uint64_t u64;
 	struct cvmx_pko_reg_read_idx_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_16_63:48;
-		uint64_t inc:8;
-		uint64_t index:8;
-#else
-		uint64_t index:8;
-		uint64_t inc:8;
-		uint64_t reserved_16_63:48;
-#endif
-	} s;
-	struct cvmx_pko_reg_read_idx_s cn30xx;
-	struct cvmx_pko_reg_read_idx_s cn31xx;
-	struct cvmx_pko_reg_read_idx_s cn38xx;
-	struct cvmx_pko_reg_read_idx_s cn38xxp2;
-	struct cvmx_pko_reg_read_idx_s cn50xx;
-	struct cvmx_pko_reg_read_idx_s cn52xx;
-	struct cvmx_pko_reg_read_idx_s cn52xxp1;
-	struct cvmx_pko_reg_read_idx_s cn56xx;
-	struct cvmx_pko_reg_read_idx_s cn56xxp1;
-	struct cvmx_pko_reg_read_idx_s cn58xx;
-	struct cvmx_pko_reg_read_idx_s cn58xxp1;
-	struct cvmx_pko_reg_read_idx_s cn61xx;
-	struct cvmx_pko_reg_read_idx_s cn63xx;
-	struct cvmx_pko_reg_read_idx_s cn63xxp1;
-	struct cvmx_pko_reg_read_idx_s cn66xx;
-	struct cvmx_pko_reg_read_idx_s cn68xx;
-	struct cvmx_pko_reg_read_idx_s cn68xxp1;
-	struct cvmx_pko_reg_read_idx_s cnf71xx;
+	uint64_t reserved_16_63               : 48;
+	uint64_t inc                          : 8;  /**< Increment to add to current index for next index */
+	uint64_t index                        : 8;  /**< Index to use for next memory CSR read */
+#else
+	uint64_t index                        : 8;
+	uint64_t inc                          : 8;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_pko_reg_read_idx_s        cn30xx;
+	struct cvmx_pko_reg_read_idx_s        cn31xx;
+	struct cvmx_pko_reg_read_idx_s        cn38xx;
+	struct cvmx_pko_reg_read_idx_s        cn38xxp2;
+	struct cvmx_pko_reg_read_idx_s        cn50xx;
+	struct cvmx_pko_reg_read_idx_s        cn52xx;
+	struct cvmx_pko_reg_read_idx_s        cn52xxp1;
+	struct cvmx_pko_reg_read_idx_s        cn56xx;
+	struct cvmx_pko_reg_read_idx_s        cn56xxp1;
+	struct cvmx_pko_reg_read_idx_s        cn58xx;
+	struct cvmx_pko_reg_read_idx_s        cn58xxp1;
+	struct cvmx_pko_reg_read_idx_s        cn61xx;
+	struct cvmx_pko_reg_read_idx_s        cn63xx;
+	struct cvmx_pko_reg_read_idx_s        cn63xxp1;
+	struct cvmx_pko_reg_read_idx_s        cn66xx;
+	struct cvmx_pko_reg_read_idx_s        cn68xx;
+	struct cvmx_pko_reg_read_idx_s        cn68xxp1;
+	struct cvmx_pko_reg_read_idx_s        cn70xx;
+	struct cvmx_pko_reg_read_idx_s        cn70xxp1;
+	struct cvmx_pko_reg_read_idx_s        cnf71xx;
 };
+typedef union cvmx_pko_reg_read_idx cvmx_pko_reg_read_idx_t;
 
+/**
+ * cvmx_pko_reg_throttle
+ *
+ * Notes:
+ * This CSR is used with PKO_MEM_THROTTLE_PIPE and PKO_MEM_THROTTLE_INT.  INT_MASK corresponds to the
+ * interfaces listed in the description for PKO_MEM_IPORT_PTRS[INT].  Set INT_MASK[N] to enable the
+ * updating of PKO_MEM_THROTTLE_PIPE and PKO_MEM_THROTTLE_INT counts for packets destined for
+ * interface N.  INT_MASK has no effect on the updates caused by CSR writes to PKO_MEM_THROTTLE_PIPE
+ * and PKO_MEM_THROTTLE_INT.  Note that this does not disable the throttle logic, just the updating of
+ * the interface counts.
+ */
 union cvmx_pko_reg_throttle {
 	uint64_t u64;
 	struct cvmx_pko_reg_throttle_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_32_63:32;
-		uint64_t int_mask:32;
+	uint64_t reserved_32_63               : 32;
+	uint64_t int_mask                     : 32; /**< Mask to enable THROTTLE count updates per interface           NS */
 #else
-		uint64_t int_mask:32;
-		uint64_t reserved_32_63:32;
+	uint64_t int_mask                     : 32;
+	uint64_t reserved_32_63               : 32;
 #endif
 	} s;
-	struct cvmx_pko_reg_throttle_s cn68xx;
-	struct cvmx_pko_reg_throttle_s cn68xxp1;
+	struct cvmx_pko_reg_throttle_s        cn68xx;
+	struct cvmx_pko_reg_throttle_s        cn68xxp1;
 };
+typedef union cvmx_pko_reg_throttle cvmx_pko_reg_throttle_t;
 
+/**
+ * cvmx_pko_reg_timestamp
+ *
+ * Notes:
+ * None.
+ *
+ */
 union cvmx_pko_reg_timestamp {
 	uint64_t u64;
 	struct cvmx_pko_reg_timestamp_s {
 #ifdef __BIG_ENDIAN_BITFIELD
-		uint64_t reserved_4_63:60;
-		uint64_t wqe_word:4;
+	uint64_t reserved_4_63                : 60;
+	uint64_t wqe_word                     : 4;  /**< Specifies the 8-byte word in the WQE to which a PTP
+                                                         timestamp is written.  Values 0 and 1 are illegal. */
+#else
+	uint64_t wqe_word                     : 4;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_pko_reg_timestamp_s       cn61xx;
+	struct cvmx_pko_reg_timestamp_s       cn63xx;
+	struct cvmx_pko_reg_timestamp_s       cn63xxp1;
+	struct cvmx_pko_reg_timestamp_s       cn66xx;
+	struct cvmx_pko_reg_timestamp_s       cn68xx;
+	struct cvmx_pko_reg_timestamp_s       cn68xxp1;
+	struct cvmx_pko_reg_timestamp_s       cn70xx;
+	struct cvmx_pko_reg_timestamp_s       cn70xxp1;
+	struct cvmx_pko_reg_timestamp_s       cnf71xx;
+};
+typedef union cvmx_pko_reg_timestamp cvmx_pko_reg_timestamp_t;
+
+/**
+ * cvmx_pko_status
+ */
+union cvmx_pko_status {
+	uint64_t u64;
+	struct cvmx_pko_status_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t pko_rdy                      : 1;  /**< PKO ready for configuration. */
+	uint64_t reserved_24_62               : 39;
+	uint64_t c2qlut_rdy                   : 1;  /**< PKO C2Q LUT block ready for configuration. */
+	uint64_t ppfi_rdy                     : 1;  /**< PKO PPFI block ready for configuration. */
+	uint64_t iobp1_rdy                    : 1;  /**< PKO IOBP1 block ready for configuration. */
+	uint64_t ncb_rdy                      : 1;  /**< PKO NCB block ready for configuration. */
+	uint64_t pse_rdy                      : 1;  /**< PKO PSE block ready for configuration. */
+	uint64_t pdm_rdy                      : 1;  /**< PKO PDM block ready for configuration. */
+	uint64_t peb_rdy                      : 1;  /**< PKO PEB block ready for configuration. */
+	uint64_t csi_rdy                      : 1;  /**< PKO CSI block ready for configuration. */
+	uint64_t reserved_5_15                : 11;
+	uint64_t ncb_bist_status              : 1;  /**< PKO NCB block BIST status. 0 = BIST passed; 1 = BIST failed. */
+	uint64_t c2qlut_bist_status           : 1;  /**< PKO C2QLUT block BIST status. 0 = BIST passed; 1 = BIST failed. */
+	uint64_t pdm_bist_status              : 1;  /**< PKO PDM block BIST status. 0 = BIST passed; 1 = BIST failed. */
+	uint64_t peb_bist_status              : 1;  /**< PKO PEB block BIST status. 0 = BIST passed; 1 = BIST failed. */
+	uint64_t pse_bist_status              : 1;  /**< PKO PSE block BIST status. 0 = BIST passed; 1 = BIST failed. */
 #else
-		uint64_t wqe_word:4;
-		uint64_t reserved_4_63:60;
+	uint64_t pse_bist_status              : 1;
+	uint64_t peb_bist_status              : 1;
+	uint64_t pdm_bist_status              : 1;
+	uint64_t c2qlut_bist_status           : 1;
+	uint64_t ncb_bist_status              : 1;
+	uint64_t reserved_5_15                : 11;
+	uint64_t csi_rdy                      : 1;
+	uint64_t peb_rdy                      : 1;
+	uint64_t pdm_rdy                      : 1;
+	uint64_t pse_rdy                      : 1;
+	uint64_t ncb_rdy                      : 1;
+	uint64_t iobp1_rdy                    : 1;
+	uint64_t ppfi_rdy                     : 1;
+	uint64_t c2qlut_rdy                   : 1;
+	uint64_t reserved_24_62               : 39;
+	uint64_t pko_rdy                      : 1;
 #endif
 	} s;
-	struct cvmx_pko_reg_timestamp_s cn61xx;
-	struct cvmx_pko_reg_timestamp_s cn63xx;
-	struct cvmx_pko_reg_timestamp_s cn63xxp1;
-	struct cvmx_pko_reg_timestamp_s cn66xx;
-	struct cvmx_pko_reg_timestamp_s cn68xx;
-	struct cvmx_pko_reg_timestamp_s cn68xxp1;
-	struct cvmx_pko_reg_timestamp_s cnf71xx;
+	struct cvmx_pko_status_s              cn78xx;
 };
 
 #endif
diff --git a/arch/mips/include/asm/octeon/cvmx-pko-internal-ports-range.h b/arch/mips/include/asm/octeon/cvmx-pko-internal-ports-range.h
new file mode 100644
index 0000000..466f865
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-pko-internal-ports-range.h
@@ -0,0 +1,79 @@
+/***********************license start***************
+ * Copyright (c) 2012 Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+#ifndef __CVMX_INTERNAL_PORTS_RANGE__
+#define __CVMX_INTERNAL_PORTS_RANGE__
+
+#include "cvmx.h"
+
+/*
+ * Allocated a block of internal ports for the specified interface/port
+ *
+ * @param  interface  the interface for which the internal ports are requested
+ * @param  port       the index of the port within in the interface for which the internal ports
+ *                    are requested.
+ * @param  count      the number of internal ports requested
+ *
+ * @return  0 on success
+ *         -1 on failure
+ */
+int cvmx_pko_internal_ports_alloc(int interface, int port, uint64_t count);
+
+/*
+ * Free the internal ports associated with the specified interface/port
+ *
+ * @param  interface  the interface for which the internal ports are requested
+ * @param  port       the index of the port within in the interface for which the internal ports
+ *                    are requested.
+ *
+ * @return  0 on success
+ *         -1 on failure
+ */
+int cvmx_pko_internal_ports_free(int interface, int port);
+
+/*
+ * Frees up all the allocated internal ports.
+ */
+void cvmx_pko_internal_ports_range_free_all(void);
+
+void cvmx_pko_internal_ports_range_show(void);
+
+extern int __cvmx_pko_internal_ports_range_init(void);
+
+#endif
diff --git a/arch/mips/include/asm/octeon/cvmx-pko.h b/arch/mips/include/asm/octeon/cvmx-pko.h
index f7d2a67..7be16fe 100644
--- a/arch/mips/include/asm/octeon/cvmx-pko.h
+++ b/arch/mips/include/asm/octeon/cvmx-pko.h
@@ -115,7 +115,7 @@ typedef enum {
 	CVMX_PKO_LOCK_CMD_QUEUE = 2,
 } cvmx_pko_lock_t;
 
-typedef struct {
+typedef struct cvmx_pko_port_status{
 	uint32_t packets;
 	uint64_t octets;
 	uint64_t doorbell;
@@ -607,4 +607,27 @@ extern int cvmx_pko_rate_limit_packets(int port, int packets_s, int burst);
  */
 extern int cvmx_pko_rate_limit_bits(int port, uint64_t bits_s, int burst);
 
+/**
+ * @INTERNAL
+ *
+ * Retrieve the PKO pipe number for a port
+ *
+ * @param interface
+ * @param index
+ *
+ * @return negative on error.
+ *
+ * This applies only to the non-loopback interfaces.
+ *
+ */
+extern int __cvmx_pko_get_pipe(int interface, int index);
+
+/**
+ * Get the first pko_port for the (interface, index)
+ *
+ * @param interface
+ * @param index
+ */
+extern int cvmx_pko_get_base_pko_port(int interface, int index);
+
 #endif /* __CVMX_PKO_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-pko3-queue.h b/arch/mips/include/asm/octeon/cvmx-pko3-queue.h
new file mode 100644
index 0000000..60c5f10
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-pko3-queue.h
@@ -0,0 +1,245 @@
+/***********************license start***************
+ * Copyright (c) 2003-2013  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * <hr>$Revision: 0 $<hr>
+ */
+
+#ifndef __CVMX_PKO3_QUEUE_H__
+#define __CVMX_PKO3_QUEUE_H__
+
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+extern "C" {
+/* *INDENT-ON* */
+#endif
+
+/**
+ * @INTERNAL
+ *
+ * Find or allocate global port/dq map table
+ * which is a named table, contains entries for
+ * all possible OCI nodes.
+ *
+ * The table global pointer is stored in core-local variable
+ * so that every core will call this function once, on first use.
+ */
+int __cvmx_pko3_dq_table_setup(void);
+
+/*
+ * Get the base Descriptor Queue number for an IPD port on the local node
+ */
+int cvmx_pko3_get_queue_base(int ipd_port);
+
+/*
+ * Get the number of Descriptor Queues assigned for an IPD port
+ */
+int cvmx_pko3_get_queue_num(int ipd_port);
+
+/*
+ * Configure Port Queue and its children Scheduler Queue
+ *
+ * Port Queues (a.k.a L1) are assigned 1-to-1 to MACs.
+ * L2 Scheduler Queues are used for specifying channels, and thus there
+ * could be multiple L2 SQs attached to a single L1 PQ, either in a
+ * fair round-robin scheduling, or with static and/or round-robin priorities.
+ *
+ * @param node is the OCI node location for the queues to be configured
+ * @param mac_num is the LMAC number to that is associated with the Port Queue,
+ * @param which is identical to the Port Queue number that is configured
+ * @param child_base is the number of the first L2 SQ attached to the PQ
+ * @param child_count is the number of L2 SQ children to attach to PQ
+ * @param stat_prio_count is the priority setting for the children L2 SQs
+ *
+ * If <stat_prio_count> is -1, the L2 children will have equal Round-Robin
+ * relationship with eachother. If <stat_prio_count> is 0, all L2 children
+ * will be arranged in Weighted-Round-Robin, with the first having the most
+ * precedence. If <stat_prio_count> is between 1 and 8, it indicates how
+ * many children will have static priority settings (with the first having
+ * the most precedence), with the remaining L2 children having WRR scheduling.
+ *
+ * @returns 0 on success, -1 on failure.
+ *
+ * Note: this function supports the configuration of node-local unit.
+ */
+int cvmx_pko3_pq_config_children(unsigned node, unsigned mac_num,
+			unsigned child_base,
+			unsigned child_count, int stat_prio_count);
+
+/*
+ * Configure L3 through L5 Scheduler Queues and Descriptor Queues
+ *
+ * The Scheduler Queues in Levels 3 to 5 and Descriptor Queues are
+ * configured one-to-one or many-to-one to a single parent Scheduler
+ * Queues. The level of the parent SQ is specified in an argument,
+ * as well as the number of childer to attach to the specific parent.
+ * The children can have fair round-robin or priority-based scheduling
+ * when multiple children are assigned a single parent.
+ *
+ * @param node is the OCI node location for the queues to be configured
+ * @param parent_level is the level of the parent queue, 2 to 5.
+ * @param parent_queue is the number of the parent Scheduler Queue
+ * @param child_base is the number of the first child SQ or DQ to assign to
+ * @param parent
+ * @param child_count is the number of consecutive children to assign
+ * @param stat_prio_count is the priority setting for the children L2 SQs
+ *
+ * If <stat_prio_count> is -1, the Ln children will have equal Round-Robin
+ * relationship with eachother. If <stat_prio_count> is 0, all Ln children
+ * will be arranged in Weighted-Round-Robin, with the first having the most
+ * precedence. If <stat_prio_count> is between 1 and 8, it indicates how
+ * many children will have static priority settings (with the first having
+ * the most precedence), with the remaining Ln children having WRR scheduling.
+ *
+ * @returns 0 on success, -1 on failure.
+ *
+ * Note: this function supports the configuration of node-local unit.
+ */
+int cvmx_pko3_sq_config_children(unsigned int node, unsigned parent_level,
+			unsigned parent_queue, unsigned child_base,
+			unsigned child_count, int stat_prio_count);
+
+/*
+ * @INTERNAL
+ * Register a range of Descriptor Queues wth an interface port
+ *
+ * This function poulates the DQ-to-IPD translation table
+ * used by the application to retreive the DQ range (typically ordered
+ * by priority) for a given IPD-port, which is either a physical port,
+ * or a channel on a channelized interface (i.e. ILK).
+ *
+ * @param xiface is the physical interface number
+ * @param index is either a physical port on an interface
+ * @param or a channel of an ILK interface
+ * @param dq_base is the first Descriptor Queue number in a consecutive range
+ * @param dq_count is the number of consecutive Descriptor Queues leading
+ * @param the same channel or port.
+ *
+ * Only a consecurive range of Descriptor Queues can be associated with any
+ * given channel/port, and usually they are ordered from most to least
+ * in terms of scheduling priority.
+ *
+ * Note: thus function only populates the node-local translation table.
+ *
+ * @returns 0 on success, -1 on failure.
+ */
+int __cvmx_pko3_ipd_dq_register(int xiface, int index,
+		unsigned dq_base, unsigned dq_count);
+
+
+/**
+ * @INTERNAL
+ *
+ * Unregister DQs associated with CHAN_E (IPD port)
+ */
+int __cvmx_pko3_ipd_dq_unregister(int xiface, int index);
+
+/*
+ * Map channel number in PKO 
+ *
+ * @param node is to specify the node to which this configuration is applied.
+ * @param pq_num specifies the Port Queue (i.e. L1) queue number.
+ * @param l2_l3_q_num  specifies L2/L3 queue number.
+ * @param channel specifies the channel number to map to the queue.
+ *
+ * The channel assignment applies to L2 or L3 Shaper Queues depending
+ * on the setting of channel credit level.
+ *
+ * @return returns none.
+ */
+void cvmx_pko3_map_channel(unsigned node,
+	unsigned pq_num, unsigned l2_l3_q_num, uint16_t channel);
+
+extern int cvmx_pko3_port_cir_set(unsigned node, unsigned pq_num,
+		unsigned long rate_kbips, unsigned burst_bytes);
+extern int cvmx_pko3_dq_cir_set(unsigned node, unsigned pq_num,
+		unsigned long rate_kbips, unsigned burst_bytes);
+extern int cvmx_pko3_dq_pir_set(unsigned node, unsigned pq_num,
+		unsigned long rate_kbips, unsigned burst_bytes);
+typedef enum {
+	CVMX_PKO3_SHAPE_RED_STALL,
+	CVMX_PKO3_SHAPE_RED_DISCARD,
+	CVMX_PKO3_SHAPE_RED_PASS
+} red_action_t;
+
+extern void cvmx_pko3_dq_red(unsigned node, unsigned dq_num,
+	red_action_t red_act, int8_t len_adjust);
+
+/**
+ * Macros to deal with short floating point numbers,
+ * where unsigned exponent, and an unsigned normalized
+ * mantissa are represented each with a defined field width.
+ *
+ */
+#define	CVMX_SHOFT_MANT_BITS	8
+#define	CVMX_SHOFT_EXP_BITS	4
+
+/**
+ * Convert short-float to an unsigned integer
+ * Note that it will lose precision.
+ */
+#define	CVMX_SHOFT_TO_U64(m,e) \
+	((((1ull<<CVMX_SHOFT_MANT_BITS) | (m)) << (e))>>CVMX_SHOFT_MANT_BITS)
+
+/**
+ * Convert to short-float from an unsigned integer
+ */
+#define	CVMX_SHOFT_FROM_U64(ui,m,e) do { \
+		unsigned long long u; unsigned k;		\
+		k = (1ull << (CVMX_SHOFT_MANT_BITS+1)) -1;		\
+		(e) = 0; u = (ui) << CVMX_SHOFT_MANT_BITS;	\
+		while((u) > k) {				\
+			u >>=1; (e)++;				\
+		}						\
+		(m) = u & (k>>1);				\
+	} while(0);
+
+#define	CVMX_SHOFT_MAX()	CVMX_SHOFT_TO_U64((1<<CVMX_SHOFT_MANT_BITS)-1, \
+						(1<<CVMX_SHOFT_EXP_BITS)-1)
+#define	CVMX_SHOFT_MIN()	CVMX_SHOFT_TO_U64(0, 0)
+
+
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+}
+/* *INDENT-ON* */
+#endif
+#endif /* __CVMX_PKO3_QUEUE_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-pko3-resources.h b/arch/mips/include/asm/octeon/cvmx-pko3-resources.h
new file mode 100644
index 0000000..c5bad29
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-pko3-resources.h
@@ -0,0 +1,82 @@
+/***********************license start***************
+ * Copyright (c) 2003-2013  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * <hr>$Revision: 0 $<hr>
+ */
+
+#ifndef __CVMX_PKO3_RESOURCES_H__
+#define __CVMX_PKO3_RESOURCES_H__
+
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+extern "C" {
+/* *INDENT-ON* */
+#endif
+
+/*
+ * Allocate or reserve contiguous list of PKO queues.
+ *
+ * @param node is the node number for PKO queues.
+ * @param level is the PKO queue level.
+ * @param owner is the owner of PKO queue resources.
+ * @param base_queue is the PKO queue base number(specify -1 to allocate).
+ * @param num_queues is the number of PKO queues that have to be reserved or allocated.
+ * @return returns queue_base if successful or -1 on failure.
+ */
+extern int cvmx_pko_alloc_queues(int node, int level, int owner, int base_queue, int num_queues);
+
+/**
+ * Free an allocated/reserved PKO queues for a certain level and owner
+ *
+ * @param node on which to allocate/reserve PKO queues
+ * @param level of PKO queue
+ * @param owner of reserved/allocated resources
+ * @return 0 on success, -1 on failure
+ */
+extern int cvmx_pko_free_queues(int node, int level, int owner);
+
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+}
+/* *INDENT-ON* */
+#endif
+#endif /* __CVMX_PKO3_RESOURCES_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-pko3.h b/arch/mips/include/asm/octeon/cvmx-pko3.h
new file mode 100644
index 0000000..c70f9d3
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-pko3.h
@@ -0,0 +1,829 @@
+/***********************license start***************
+ * Copyright (c) 2003-2014  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * <hr>$Revision: 0 $<hr>
+ */
+
+#ifndef __CVMX_PKO3_H__
+#define __CVMX_PKO3_H__
+
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+extern "C" {
+/* *INDENT-ON* */
+#endif
+
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include <asm/octeon/cvmx-pko-defs.h>
+#include <asm/octeon/cvmx-helper.h>
+#include <asm/octeon/cvmx-pko3-queue.h>
+#include <asm/octeon/cvmx-ilk.h>
+#include <asm/octeon/cvmx-scratch.h>
+#include <asm/octeon/cvmx-atomic.h>
+#else
+#include "cvmx-pko-defs.h"
+#include "cvmx-pko3-queue.h"
+#include "cvmx-helper.h"
+#include "cvmx-scratch.h"
+#include "cvmx-atomic.h"
+#endif
+
+/* Use full LMTDMA when PARAMETER_CHECKINS is enabled */
+#undef	CVMX_ENABLE_PARAMETER_CHECKING
+#define	CVMX_ENABLE_PARAMETER_CHECKING 0
+
+#define	CVMX_PKO3_DQ_MAX_DEPTH	(48*256)
+
+/* dwords are from 1-16 */
+/* scratch line for LMT operations */
+#define CVMX_PKO_LMTLINE 2ull	//FIXME- should go somewhere else ?
+
+enum {
+	CVMX_PKO_PORT_QUEUES = 0,
+	CVMX_PKO_L2_QUEUES,
+	CVMX_PKO_L3_QUEUES,
+	CVMX_PKO_L4_QUEUES,
+	CVMX_PKO_L5_QUEUES,
+	CVMX_PKO_DESCR_QUEUES,
+	CVMX_PKO_NUM_QUEUE_LEVELS
+};
+
+#define CVMX_PKO_MAX_MACS 28
+
+enum cvmx_pko_dqop {
+	CVMX_PKO_DQ_SEND = 0ULL,
+	CVMX_PKO_DQ_OPEN = 1ULL,
+	CVMX_PKO_DQ_CLOSE = 2ULL,
+	CVMX_PKO_DQ_QUERY = 3ULL
+};
+
+union cvmx_pko_query_rtn {
+	uint64_t u64;
+	struct {
+		CVMX_BITFIELD_FIELD(uint64_t dqstatus	: 4,
+		CVMX_BITFIELD_FIELD(uint64_t rsvd_50_59	:10,
+		CVMX_BITFIELD_FIELD(uint64_t dqop	: 2,
+		CVMX_BITFIELD_FIELD(uint64_t depth	:48,
+			))));
+	} s;
+};
+typedef union cvmx_pko_query_rtn cvmx_pko_query_rtn_t;
+
+/* PKO_QUERY_RTN_S[DQSTATUS] - cvmx_pko_query_rtn_t->s.dqstatus */
+enum pko_query_dqstatus {
+	PKO_DQSTATUS_PASS = 0,		/* No error */
+	PKO_DQSTATUS_BADSTATE = 0x8,	/* queue was not ready to enqueue */
+	PKO_DQSTATUS_NOFPABUF = 0x9,	/* FPA out of buffers */
+	PKO_DQSTATUS_NOPKOBUF = 0xA,	/* PKO out of buffers */
+	PKO_DQSTATUS_FAILRTNPTR = 0xB,	/* can't return buffer ptr to FPA */
+	PKO_DQSTATUS_ALREADY = 0xC,	/* already created */
+	PKO_DQSTATUS_NOTCREATED = 0xD,	/* not created */
+	PKO_DQSTATUS_NOTEMPTY = 0xE,	/* queue not empty */
+	PKO_DQSTATUS_SENDPKTDROP = 0xF	/* packet dropped, illegal construct */
+};
+typedef	enum pko_query_dqstatus pko_query_dqstatus_t;
+
+/* Sub-command three bit codes (SUBDC3) */
+#define CVMX_PKO_SENDSUBDC_LINK		0x0
+#define CVMX_PKO_SENDSUBDC_GATHER	0x1
+#define CVMX_PKO_SENDSUBDC_JUMP		0x2
+/* Sub-command four bit codes (SUBDC4) */
+#define CVMX_PKO_SENDSUBDC_FREE		0x9
+#define CVMX_PKO_SENDSUBDC_WORK		0xA
+#define CVMX_PKO_SENDSUBDC_AURA		0xB
+#define CVMX_PKO_SENDSUBDC_MEM		0xC
+#define CVMX_PKO_SENDSUBDC_EXT		0xD
+#define CVMX_PKO_SENDSUBDC_CRC		0xE
+#define CVMX_PKO_SENDSUBDC_IMM		0xF
+
+/**
+ * pko buf ptr
+ * This is good for LINK_S, GATHER_S and PKI_BUFLINK_S structure use.
+ * It can also be used for JUMP_S with F-bit represented by "i" field,
+ * and the size limited to 8-bit.
+*/
+
+union cvmx_pko_buf_ptr {
+	uint64_t u64;
+	struct {
+		CVMX_BITFIELD_FIELD(uint64_t size	:16,
+		CVMX_BITFIELD_FIELD(uint64_t subdc3	: 3,
+		CVMX_BITFIELD_FIELD(uint64_t i		: 1,
+		CVMX_BITFIELD_FIELD(uint64_t rsvd_42_43	: 2,
+		CVMX_BITFIELD_FIELD(uint64_t addr	:42,
+			)))));
+	} s;
+};
+typedef union cvmx_pko_buf_ptr cvmx_pko_buf_ptr_t;
+
+/**
+ * pko_auraalg_e
+ */
+enum pko_auraalg_e {
+	AURAALG_NOP = 0x0,	/* aura_cnt = No change */
+	AURAALG_SUB = 0x3,	/* aura_cnt -= pko_send_aura_t.offset */
+	AURAALG_SUBLEN = 0x7,	/* aura_cnt -= pko_send_aura_t.offset +
+						pko_send_hdr_t.total_bytes */
+	AURAALG_SUBMBUF = 0xB	/* aura_cnt -= pko_send_aura_t.offset +
+						mbufs_freed */
+};
+
+/**
+ * PKO_CKL4ALG_E
+ */
+enum pko_clk4alg_e {
+	CKL4ALG_NONE = 0x0,	/* No checksum. */
+	CKL4ALG_UDP = 0x1,	/* UDP L4 checksum. */
+	CKL4ALG_TCP = 0x2,	/* TCP L4 checksum. */
+	CKL4ALG_SCTP = 0x3,	/* SCTP L4 checksum. */
+};
+
+/**
+ * pko_send_aura
+ */
+union cvmx_pko_send_aura {
+	uint64_t u64;
+	struct {
+                CVMX_BITFIELD_FIELD(uint64_t rsvd_60_63 : 4,
+                CVMX_BITFIELD_FIELD(uint64_t aura 	: 12, /* NODE+LAURA */
+                CVMX_BITFIELD_FIELD(uint64_t subdc4 	: 4,
+                CVMX_BITFIELD_FIELD(uint64_t alg 	: 4, /* pko_auraalg_e */
+                CVMX_BITFIELD_FIELD(uint64_t rsvd_08_39 : 32,
+                CVMX_BITFIELD_FIELD(uint64_t offset 	: 8,
+			))))));
+	} s;
+};
+typedef union cvmx_pko_send_aura cvmx_pko_send_aura_t;
+
+/* PKO_SEND_HDR_S - PKO header subcommand */
+union cvmx_pko_send_hdr {
+	uint64_t u64;
+	struct {
+		CVMX_BITFIELD_FIELD(uint64_t rsvd_60_63	:4,
+		CVMX_BITFIELD_FIELD(uint64_t aura	:12,
+		CVMX_BITFIELD_FIELD(uint64_t ckl4	:2, /* PKO_CKL4ALG_E */
+		CVMX_BITFIELD_FIELD(uint64_t ckl3	:1,
+		CVMX_BITFIELD_FIELD(uint64_t ds		:1,
+		CVMX_BITFIELD_FIELD(uint64_t le		:1,
+		CVMX_BITFIELD_FIELD(uint64_t n2		:1,
+		CVMX_BITFIELD_FIELD(uint64_t ii		:1,
+		CVMX_BITFIELD_FIELD(uint64_t df		:1,
+		CVMX_BITFIELD_FIELD(uint64_t rsvd_39	:1,
+		CVMX_BITFIELD_FIELD(uint64_t format	:7,
+		CVMX_BITFIELD_FIELD(uint64_t l4ptr	:8,
+		CVMX_BITFIELD_FIELD(uint64_t l3ptr	:8,
+		CVMX_BITFIELD_FIELD(uint64_t total	:16,
+			))))))))))))));
+	} s;
+};
+typedef union cvmx_pko_send_hdr cvmx_pko_send_hdr_t;
+
+/* PKO_SEND_EXT_S - extended header subcommand */
+union cvmx_pko_send_ext {
+	uint64_t u64;
+	struct {
+		CVMX_BITFIELD_FIELD(uint64_t rsvd_48_63	:16,
+		CVMX_BITFIELD_FIELD(uint64_t subdc4	:4, /* _SENDSUBDC_EXT */
+		CVMX_BITFIELD_FIELD(uint64_t col	:2, /* _COLORALG_E */
+		CVMX_BITFIELD_FIELD(uint64_t ra		:2, /* _REDALG_E */
+		CVMX_BITFIELD_FIELD(uint64_t tstmp	:1,
+		CVMX_BITFIELD_FIELD(uint64_t rsvd_24_38:15,
+		CVMX_BITFIELD_FIELD(uint64_t markptr	:8,
+		CVMX_BITFIELD_FIELD(uint64_t rsvd_9_15	:7,
+		CVMX_BITFIELD_FIELD(uint64_t shapechg	:9,
+			)))))))));
+	} s;
+};
+typedef union cvmx_pko_send_ext cvmx_pko_send_ext_t;
+
+/* PKO_MEMDSZ_E */
+enum cvmx_pko_memdsz_e {
+	MEMDSZ_B64 = 0,
+	MEMDSZ_B32 = 1,
+	MEMDSZ_B16 = 2,		/* Not in HRM, assumed unsupported */
+	MEMDSZ_B8 = 3
+};
+
+/* PKO_MEMALG_E */
+enum cvmx_pko_memalg_e {
+	MEMALG_SET = 0,		/* Set mem = PKO_SEND_MEM_S[OFFSET] */
+	MEMALG_SETTSTMP = 1,	/* Set the memory location to the timestamp
+				   PKO_SEND_MEM_S[DSZ] must be B64 and a
+				   PKO_SEND_EXT_S subdescriptor must be in
+				   the descriptor with PKO_SEND_EXT_S[TSTMP]=1
+				 */
+	MEMALG_SETRSLT = 2,	/* [DSZ] = B64; mem = PKO_MEM_RESULT_S.  */
+	MEMALG_ADD = 8,		/* mem = mem + PKO_SEND_MEM_S[OFFSET] */
+	MEMALG_SUB = 9,		/* mem = mem  PKO_SEND_MEM_S[OFFSET] */
+	MEMALG_ADDLEN = 0xA,	/* mem += [OFFSET] + PKO_SEND_HDR_S[TOTAL] */
+	MEMALG_SUBLEN = 0xB,	/* mem -= [OFFSET] + PKO_SEND_HDR_S[TOTAL] */
+	MEMALG_ADDMBUF = 0xC,	/* mem += [OFFSET] + mbufs_freed */
+	MEMALG_SUBMBUF = 0xD	/* mem -= [OFFSET] + mbufs_freed */
+};
+
+union cvmx_pko_send_mem {
+	uint64_t u64;
+	struct {
+		CVMX_BITFIELD_FIELD(uint64_t rsvd_63	:1,
+		CVMX_BITFIELD_FIELD(uint64_t wmem	:1,
+		CVMX_BITFIELD_FIELD(uint64_t dsz	:2, /* PKO_MEMDSZ_E */
+		CVMX_BITFIELD_FIELD(uint64_t alg	:4, /* PKO_MEMALG_E */
+		CVMX_BITFIELD_FIELD(uint64_t offset	:8,
+		CVMX_BITFIELD_FIELD(uint64_t subdc4	:4,
+		CVMX_BITFIELD_FIELD(uint64_t rsvd_42_43	:2,
+		CVMX_BITFIELD_FIELD(uint64_t addr	:42,
+		))))))));
+	} s;
+};
+
+typedef union cvmx_pko_send_mem cvmx_pko_send_mem_t;
+
+union cvmx_pko_send_work {
+	uint64_t u64;
+	struct {
+		CVMX_BITFIELD_FIELD(uint64_t rsvd_62_63	:2,
+		CVMX_BITFIELD_FIELD(uint64_t grp	:10,
+		CVMX_BITFIELD_FIELD(uint64_t tt		:2,
+		CVMX_BITFIELD_FIELD(uint64_t rsvd_48_49	:2,
+		CVMX_BITFIELD_FIELD(uint64_t subdc4	:4,
+		CVMX_BITFIELD_FIELD(uint64_t rsvd_42_43	:2,
+		CVMX_BITFIELD_FIELD(uint64_t addr	:42,
+			)))))));
+	} s;
+};
+
+typedef union cvmx_pko_send_work cvmx_pko_send_work_t;
+
+/*** PKO_SEND_DMA_S - format of IOBDMA/LTDMA data word ***/
+union cvmx_pko_lmtdma_data {
+	uint64_t u64;
+	struct {
+		CVMX_BITFIELD_FIELD(uint64_t scraddr	: 8,
+		CVMX_BITFIELD_FIELD(uint64_t rtnlen	: 8,
+		CVMX_BITFIELD_FIELD(uint64_t did	: 8, /* 0x51 */
+		CVMX_BITFIELD_FIELD(uint64_t node	: 4,
+		CVMX_BITFIELD_FIELD(uint64_t rsvd_34_35	: 2,
+		CVMX_BITFIELD_FIELD(uint64_t dqop	: 2, /* PKO_DQOP_E */
+		CVMX_BITFIELD_FIELD(uint64_t rsvd_26_31	: 6,
+		CVMX_BITFIELD_FIELD(uint64_t dq		: 10,
+		CVMX_BITFIELD_FIELD(uint64_t rsvd_0_15	: 16,
+			)))))))));
+	} s;
+};
+typedef union cvmx_pko_lmtdma_data cvmx_pko_lmtdma_data_t;
+
+/* per-core DQ depth cached value */
+extern int32_t __cvmx_pko3_dq_depth[1024];
+
+extern int cvmx_pko3_internal_buffer_count(unsigned node);
+
+/*
+ * PKO descriptor queue operation error string
+ *
+ * @param dqstatus is the enumeration returned from hardware, 
+ * 	  PKO_QUERY_RTN_S[DQSTATUS].
+ *
+ * @return static constant string error description
+ */
+const char * pko_dqstatus_error(pko_query_dqstatus_t dqstatus);
+
+
+/*
+ * This function gets PKO mac num for a interface/port.
+ *
+ * @param interface is the interface number.
+ * @param index is the port number.
+ * @return returns mac number if successful or -1 on failure.
+ */
+static inline int __cvmx_pko3_get_mac_num(int xiface, int index)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	cvmx_helper_interface_mode_t mode;
+	int interface_index;
+
+	mode = cvmx_helper_interface_get_mode(xiface);
+	switch (mode) {
+		case CVMX_HELPER_INTERFACE_MODE_LOOP:
+			return 0;
+		case CVMX_HELPER_INTERFACE_MODE_NPI:
+			return 1;
+		case CVMX_HELPER_INTERFACE_MODE_ILK:
+			interface_index = (xi.interface - CVMX_ILK_GBL_BASE());
+			if (interface_index < 0)
+				return -1;
+			return (2 + interface_index);
+		default:
+			if (xi.interface >= CVMX_ILK_GBL_BASE())
+				return -1;
+			/* All other modes belong to BGX */
+			return (4 + 4 * xi.interface + index);
+	}
+}
+
+/*
+ * Configure Channel credit level in PKO.
+ *
+ * @param node is to specify the node to which this configuration is applied.
+ * @param level specifies the level at which pko channel queues will be configured,
+ *              level : 0 -> L2, level : 1 -> L3 queues.
+ * @return returns 0 if successful and -1 on failure.
+ */
+static inline int cvmx_pko_setup_channel_credit_level(int node, int level)
+{
+	union cvmx_pko_channel_level channel_level;
+
+	if (level != 0 || level != 1)
+		return -1;
+
+	channel_level.u64 = 0;
+	channel_level.s.cc_level = level;
+	cvmx_write_csr_node(node, CVMX_PKO_CHANNEL_LEVEL, channel_level.u64);
+
+	return 0;
+
+}
+
+/**
+ * @INTERNAL
+ *
+ * Get address for LMTDMA/LMTST data buffer
+ *
+ */
+static inline unsigned
+cvmx_pko3_lmtdma_scr_base(void)
+{
+	return CVMX_PKO_LMTLINE * CVMX_CACHE_LINE_SIZE;
+}
+
+/*
+ * @INTERNAL  
+ * Deliver PKO SEND commands via CVMSEG LM and LMTDMA/LMTTST.
+ * The command should be already stored in the address
+ * returned by 'cvmx_pko3_lmtdma_bufaddr()'.
+ *
+ * @param node is the destination node
+ * @param dq is the destonation descriptor queue.
+ * @param numworkds is the number of outgoing words
+ * @return the PKO3 native query result structure.
+ *
+ * <numwords> must be between 1 and 15 for CVMX_PKO_DQ_SEND command
+ *
+ * NOTE: Internal use only.
+ */
+static inline cvmx_pko_query_rtn_t 
+__cvmx_pko3_lmtdma(uint8_t node, uint16_t dq, unsigned numwords)
+{
+	const enum cvmx_pko_dqop dqop = CVMX_PKO_DQ_SEND;
+	cvmx_pko_query_rtn_t pko_status;
+	cvmx_pko_lmtdma_data_t pko_send_dma_data;
+	uint64_t dma_addr;
+	unsigned scr_base = cvmx_pko3_lmtdma_scr_base();
+	unsigned scr_off;
+
+	pko_status.u64 = 0;
+
+	/* LMTDMA address offset is (nWords-1) */
+	dma_addr = 0xffffffffffffa400ull; 
+	dma_addr += (numwords - 1) << 3;
+
+#ifdef	CVMX_PKO3_DQ_MAX_DEPTH
+	/* If cached depth exceeds limit, check the real depth */
+	if (cvmx_unlikely(__cvmx_pko3_dq_depth[dq] > CVMX_PKO3_DQ_MAX_DEPTH)) {
+		cvmx_pko_dqx_wm_cnt_t wm_cnt;
+		wm_cnt.u64 = cvmx_read_csr_node(node,CVMX_PKO_DQX_WM_CNT(dq));
+		__cvmx_pko3_dq_depth[dq] = pko_status.s.depth = wm_cnt.s.count;
+
+		if (pko_status.s.depth > CVMX_PKO3_DQ_MAX_DEPTH) {
+			pko_status.s.dqop = dqop;
+			pko_status.s.dqstatus = PKO_DQSTATUS_NOFPABUF;
+			return pko_status;
+		}
+	}
+#endif	/* CVMX_PKO3_DQ_MAX_DEPTH */
+
+	if (cvmx_unlikely(numwords < 1 || numwords > 15)) {
+		cvmx_dprintf("%s: ERROR: Internal error\n",
+				__FUNCTION__);
+		pko_status.u64 = ~0ull;
+		return pko_status;
+	}
+
+	scr_off = scr_base + numwords * sizeof(uint64_t);
+	pko_send_dma_data.u64 = 0;
+
+	if (CVMX_ENABLE_PARAMETER_CHECKING) {
+		/* Request one return word */
+		pko_send_dma_data.s.rtnlen = 1;
+
+		/* Write all-ones into the return area */
+		cvmx_scratch_write64(scr_off, ~0ull);
+	} else {
+		/* Do not expext a return word */
+		pko_send_dma_data.s.rtnlen = 0;
+	}
+
+	/* build store data for DMA */
+	pko_send_dma_data.s.scraddr = scr_off >> 3;
+	pko_send_dma_data.s.did = 0x51;
+	pko_send_dma_data.s.node = node;
+	pko_send_dma_data.s.dqop = dqop;
+	pko_send_dma_data.s.dq = dq;
+
+	/* Push all data into CVMSEG LM */
+	CVMX_SYNCWS;
+
+	/* issue PKO DMA */
+	cvmx_write64_uint64(dma_addr, pko_send_dma_data.u64);
+
+	if (cvmx_unlikely(pko_send_dma_data.s.rtnlen)) {
+		/* Wait for completion */
+		CVMX_SYNCIOBDMA;
+
+		/* Retreive real result */
+		pko_status.u64 = cvmx_scratch_read64(scr_off);
+#ifdef	CVMX_PKO3_DQ_MAX_DEPTH
+		__cvmx_pko3_dq_depth[dq] = pko_status.s.depth;
+#endif	/* CVMX_PKO3_DQ_MAX_DEPTH */
+	} else {
+		/* Fake positive result */
+		pko_status.s.dqop = dqop;
+		pko_status.s.dqstatus = PKO_DQSTATUS_PASS;
+#ifdef	CVMX_PKO3_DQ_MAX_DEPTH
+		__cvmx_pko3_dq_depth[dq] += 48;
+#endif	/* CVMX_PKO3_DQ_MAX_DEPTH */
+	}
+
+	return pko_status;
+}
+
+
+/*
+ * @INTERNAL  
+ * Sends PKO descriptor commands via CVMSEG LM and LMTDMA.
+ * @param node is the destination node
+ * @param dq is the destonation descriptor queue.
+ * @param cmds[] is an array of 64-bit PKO3 headers/subheaders
+ * @param numworkds is the number of outgoing words
+ * @param dqop is the operation code 
+ * @return the PKO3 native query result structure.
+ *
+ * <numwords> must be between 1 and 15 for CVMX_PKO_DQ_SEND command
+ * otherwise it must be 0.
+ *
+ * NOTE: Internal use only.
+ */
+static inline cvmx_pko_query_rtn_t 
+__cvmx_pko3_do_dma(uint8_t node, uint16_t dq, uint64_t cmds[],
+	unsigned numwords, enum cvmx_pko_dqop dqop)
+{
+	const unsigned scr_base = CVMX_PKO_LMTLINE * CVMX_CACHE_LINE_SIZE;
+	cvmx_pko_query_rtn_t pko_status;
+	cvmx_pko_lmtdma_data_t pko_send_dma_data;
+	uint64_t dma_addr;
+	unsigned i, scr_off;
+
+	pko_status.u64 = 0;
+
+	/* With 0 data to send, this is an IOBDMA, else LMTDMA operation */
+	if(numwords == 0) {
+		dma_addr = 0xffffffffffffa200ull;
+	} else {
+		/* LMTDMA address offset is (nWords-1) */
+		dma_addr = 0xffffffffffffa400ull; 
+		dma_addr += (numwords - 1) << 3;
+	}
+
+#ifdef	CVMX_PKO3_DQ_MAX_DEPTH
+	if (cvmx_unlikely(__cvmx_pko3_dq_depth[dq] > CVMX_PKO3_DQ_MAX_DEPTH) &&
+	    dqop == CVMX_PKO_DQ_SEND) {
+		cvmx_pko_dqx_wm_cnt_t wm_cnt;
+		wm_cnt.u64 = cvmx_read_csr_node(node,CVMX_PKO_DQX_WM_CNT(dq));
+		__cvmx_pko3_dq_depth[dq] = pko_status.s.depth = wm_cnt.s.count;
+
+		if (pko_status.s.depth > CVMX_PKO3_DQ_MAX_DEPTH) {
+			pko_status.s.dqop = dqop;
+			pko_status.s.dqstatus = PKO_DQSTATUS_NOFPABUF;
+			return pko_status;
+		}
+	}
+#endif
+
+	if (numwords > 15) {
+		cvmx_dprintf("%s: ERROR: Internal error\n",
+				__FUNCTION__);
+		pko_status.u64 = ~0ull;
+		return pko_status;
+	}
+
+	/* Store the command words into CVMSEG LM */
+	for(i = 0, scr_off = scr_base; i < numwords; i++) {
+		cvmx_scratch_write64(scr_off, cmds[i]);
+		scr_off += sizeof(cmds[0]);
+	}
+
+	pko_send_dma_data.u64 = 0;
+
+	if (dqop != CVMX_PKO_DQ_SEND || CVMX_ENABLE_PARAMETER_CHECKING) {
+		/* Request one return word */
+		pko_send_dma_data.s.rtnlen = 1;
+		/* Write all-ones into the return area */
+		cvmx_scratch_write64(scr_off, ~0ull);
+	} else {
+		/* Do not expext a return word */
+		pko_send_dma_data.s.rtnlen = 0;
+	}
+
+	/* build store data for DMA */
+	pko_send_dma_data.s.scraddr = scr_off >> 3;
+	pko_send_dma_data.s.did = 0x51;
+	pko_send_dma_data.s.node = node;
+	pko_send_dma_data.s.dqop = dqop;
+	pko_send_dma_data.s.dq = dq;
+
+	/* Push all data into CVMSEG LM */
+	CVMX_SYNCWS;
+
+	/* issue PKO DMA */
+	cvmx_write64_uint64(dma_addr, pko_send_dma_data.u64);
+
+	if (pko_send_dma_data.s.rtnlen) {
+		/* Wait for completion */
+		CVMX_SYNCIOBDMA;
+
+		/* Retreive real result */
+		pko_status.u64 = cvmx_scratch_read64(scr_off);
+#ifdef	CVMX_PKO3_DQ_MAX_DEPTH
+		__cvmx_pko3_dq_depth[dq] = pko_status.s.depth;
+#endif	/* CVMX_PKO3_DQ_MAX_DEPTH */
+	} else {
+		/* Fake positive result */
+		pko_status.s.dqop = dqop;
+		pko_status.s.dqstatus = PKO_DQSTATUS_PASS;
+#ifdef	CVMX_PKO3_DQ_MAX_DEPTH
+		__cvmx_pko3_dq_depth[dq] += 48;
+#endif	/* CVMX_PKO3_DQ_MAX_DEPTH */
+	}
+
+	return pko_status;
+}
+
+
+/*
+ * Transmit packets through PKO, simplified API
+ *
+ * @INTERNAL
+ *
+ * @param dq is a global destination queue number 
+ * @param pki_ptr specifies packet first linked pointer as returned from
+ * 'cvmx_wqe_get_pki_pkt_ptr()'.
+ * @param len is the total number of bytes in the packet.
+ * @param gaura is the aura to free packet buffers after trasnmit.
+ * @param pCounter is an address of a 64-bit counter to atomically
+ * decrement when packet transmission is complete.
+ *
+ * @return returns 0 if successful and -1 on failure.
+ *
+ *
+ * NOTE: This is a provisional API, and is subject to change.
+ */
+static inline int
+cvmx_pko3_xmit_link_buf(int dq,cvmx_buf_ptr_pki_t pki_ptr,
+	unsigned len, int gaura, uint64_t *pCounter)
+{
+	cvmx_pko_query_rtn_t pko_status;
+	cvmx_pko_send_hdr_t hdr_s;
+	cvmx_pko_buf_ptr_t gtr_s;
+	unsigned node, nwords;
+	unsigned scr_base = cvmx_pko3_lmtdma_scr_base();
+
+	/* Separa global DQ# into node and local DQ */
+	node = dq >> 10;
+	dq &= (1 << 10)-1;
+
+	/* Fill in header */
+	hdr_s.u64 = 0;
+	hdr_s.s.total = len;
+	hdr_s.s.df = (gaura < 0);
+	hdr_s.s.ii = 1;
+	hdr_s.s.aura = (gaura >= 0)? gaura: 0;
+
+	/* Fill in gather */
+	gtr_s.u64 = 0;
+	gtr_s.s.subdc3 = CVMX_PKO_SENDSUBDC_LINK;
+	gtr_s.s.addr = pki_ptr.addr;
+	gtr_s.s.size = pki_ptr.size;
+
+	/* Setup command word pointers */
+	cvmx_scratch_write64(scr_base+sizeof(uint64_t)*0, hdr_s.u64);
+	cvmx_scratch_write64(scr_base+sizeof(uint64_t)*1, gtr_s.u64);
+	nwords = 2;
+
+	/* Conditionally setup an atomic decrement counter */
+	if (pCounter != NULL) {
+		cvmx_pko_send_mem_t mem_s = {.s={
+			.subdc4 = CVMX_PKO_SENDSUBDC_MEM,
+			.dsz = MEMDSZ_B64, .alg = MEMALG_SUB,
+			.offset = 1,
+			}};
+		mem_s.s.addr = cvmx_ptr_to_phys(CASTPTR(void,pCounter));
+		cvmx_scratch_write64(
+			scr_base + sizeof(uint64_t) * nwords++,
+			mem_s.u64);
+	}
+
+	/* Do LMTDMA */
+	pko_status = __cvmx_pko3_lmtdma(node, dq, nwords);
+
+	if (cvmx_likely(pko_status.s.dqstatus == PKO_DQSTATUS_PASS))
+		return 0;
+	else
+		return -1;
+}
+
+/**
+ * @INTERNAL
+ *
+ * Retreive PKO internal AURA from register.
+ */
+static inline unsigned __cvmx_pko3_aura_get(unsigned node)
+{
+	static int16_t aura = -1;
+	cvmx_pko_dpfi_fpa_aura_t pko_aura;
+
+	if (aura >= 0)
+		return aura;
+
+	pko_aura.u64 = cvmx_read_csr_node(node, CVMX_PKO_DPFI_FPA_AURA);
+
+	aura =  pko_aura.s.node << 10 | pko_aura.s.laura;
+	return aura;
+}
+
+ /** Open configured descriptor queues before queueing packets into them.
+ *
+ * @param node is to specify the node to which this configuration is applied.
+ * @param dq is the descriptor queue number to be opened.
+ * @return returns 0 on sucess or -1 on failure.
+ */
+int cvmx_pko_dq_open(int node, int dq);
+
+ /** Close a descriptor queue
+ *
+ * @param node is to specify the node to which this configuration is applied.
+ * @param dq is the descriptor queue number to be opened.
+ * @return returns 0 on sucess or -1 on failure.
+ *
+ * This should be called before changing the DQ parent link, topology,
+ * or when shutting down the PKO.
+ */
+int cvmx_pko3_dq_close(int node, int dq);
+
+ /** Query a descriptor queue
+ *
+ * @param node is to specify the node to which this configuration is applied.
+ * @param dq is the descriptor queue number to be opened.
+ * @return returns the descriptor queue depth on sucess or -1 on failure.
+ *
+ * This should be called before changing the DQ parent link, topology,
+ * or when shutting down the PKO.
+ */
+int cvmx_pko3_dq_query(int node, int dq);
+
+/** Drain a descriptor queue
+ *
+ * Before closing a DQ, this call will drain all pending traffic
+ * on the DQ to the NULL MAC, which will circumvent any traffic
+ * shaping and flow control to quickly reclaim all packet buffers.
+ */
+void cvmx_pko3_dq_drain(int node, int dq);
+
+/*
+ * PKO global intialization for 78XX.
+ *
+ * @param node is the node on which PKO block is initialized.
+ * @param aura is the 12-bit AURA (including node) for PKO internal use.
+ * @return none.
+ */
+int cvmx_pko3_hw_init_global(int node, uint16_t aura);
+
+/**
+ * Shutdown the entire PKO
+ */
+int cvmx_pko3_hw_disable(int node);
+
+/** Set MAC options
+ *
+ * The options supported are the parameters below:
+ *
+ * @param xiface The physical interface number
+ * @param index The physical sub-interface port
+ * @param fcs_enable Enable FCS generation
+ * @param pad_enable Enable padding to minimum packet size
+ * @param fcs_sop_off Number of bytes at start of packet to exclude from FCS
+ *
+ * The typical use for `fcs_sop_off` is when the interface is configured
+ * to use a header such as HighGig to precede every Ethernet packet,
+ * such a header usually does not partake in the CRC32 computation stream,
+ * and its size muet be set with this parameter.
+ *
+ * @return Returns 0 on success, -1 if interface/port is invalid.
+ */
+extern int cvmx_pko3_interface_options(int xiface, int index,
+			bool fcs_enable, bool pad_enable,
+			unsigned fcs_sop_off);
+
+/** Set Descriptor Queue options
+ *
+ * The `min_pad` parameter must be in agreement with the interface-level
+ * padding option for all descriptor queues assigned to that particular
+ * interface/port.
+ */
+extern void cvmx_pko3_dq_options(unsigned node, unsigned dq, bool min_pad);
+
+
+/* Packet descriptor - PKO3 command buffer + internal state */
+typedef struct cvmx_pko3_pdesc_s {
+	uint64_t *jump_buf;	/**< jump buffer vaddr */
+	int16_t last_aura;	/**< AURA of the latest LINK_S/GATHER_S */
+	unsigned
+		num_words:5,	/**< valid words in word array 2..16 */
+		headroom:10,	/**< free bytes at start of 1st buf */
+		hdr_offsets:1,
+		pki_word4_present : 1;
+	/* PKO3 command buffer: */
+	cvmx_pko_send_hdr_t *hdr_s;
+	uint64_t word[16];	/**< header and subcommands buffer */
+	/* Bookkeeping fields: */
+	uint64_t send_work_s;	/**< SEND_WORK_S must be the very last subdc */
+	int16_t jb_aura;	/**< AURA where the jump buffer belongs */
+	uint16_t mem_s_ix;	/**< index of first MEM_S subcommand */
+	uint8_t ckl4_alg;	/**< L3/L4 alg to use if recalc is needed */
+	/* Fields saved from WQE for later inspection */
+	cvmx_pki_wqe_word4_t pki_word4;
+	cvmx_pki_wqe_word2_t pki_word2;
+} cvmx_pko3_pdesc_t;
+
+void cvmx_pko3_pdesc_init(cvmx_pko3_pdesc_t *pdesc);
+int cvmx_pko3_pdesc_from_wqe(cvmx_pko3_pdesc_t *pdesc, cvmx_wqe_78xx_t *wqe,
+	bool free_bufs);
+int cvmx_pko3_pdesc_transmit(cvmx_pko3_pdesc_t *pdesc, uint16_t dq);
+int cvmx_pko3_pdesc_notify_decrement(cvmx_pko3_pdesc_t *pdesc,
+        volatile uint64_t *p_counter);
+int cvmx_pko3_pdesc_notify_wqe(cvmx_pko3_pdesc_t *pdesc, cvmx_wqe_78xx_t *wqe,
+	uint8_t node, uint8_t group, uint8_t tt, uint32_t tag);
+int cvmx_pko3_pdesc_buf_append(cvmx_pko3_pdesc_t *pdesc, void *p_data,
+		unsigned data_bytes, unsigned gaura);
+int cvmx_pko3_pdesc_hdr_push(cvmx_pko3_pdesc_t *pdesc,
+	const void *p_data, uint8_t data_bytes, uint8_t layer);
+int cvmx_pko3_pdesc_hdr_pop(cvmx_pko3_pdesc_t *pdesc,
+		void *hdr_buf, unsigned num_bytes);
+int cvmx_pko3_pdesc_hdr_peek(cvmx_pko3_pdesc_t *pdesc,
+		void *hdr_buf, unsigned num_bytes, unsigned offset);
+void cvmx_pko3_pdesc_set_free(cvmx_pko3_pdesc_t *pdesc, bool free_bufs);
+
+#ifdef	__cplusplus
+/* *INDENT-OFF* */
+}
+/* *INDENT-ON* */
+#endif
+#endif /* __CVMX_PKO3_H__ */
+
diff --git a/arch/mips/include/asm/octeon/cvmx-pow.h b/arch/mips/include/asm/octeon/cvmx-pow.h
index 4b4d0ec..049115a 100644
--- a/arch/mips/include/asm/octeon/cvmx-pow.h
+++ b/arch/mips/include/asm/octeon/cvmx-pow.h
@@ -52,16 +52,14 @@
 #define __CVMX_POW_H__
 
 #include <asm/octeon/cvmx-pow-defs.h>
-
 #include <asm/octeon/cvmx-scratch.h>
-#include <asm/octeon/cvmx-wqe.h>
 
 /* Default to having all POW constancy checks turned on */
 #ifndef CVMX_ENABLE_POW_CHECKS
 #define CVMX_ENABLE_POW_CHECKS 1
 #endif
 
-enum cvmx_pow_tag_type {
+enum cvmx_pow_tag_type{
 	/* Tag ordering is maintained */
 	CVMX_POW_TAG_TYPE_ORDERED   = 0L,
 	/* Tag ordering is maintained, and at most one PP has the tag */
@@ -81,6 +79,10 @@ enum cvmx_pow_tag_type {
 	CVMX_POW_TAG_TYPE_NULL_NULL = 3L
 };
 
+typedef	enum cvmx_pow_tag_type cvmx_pow_tag_type_t;
+
+#include <asm/octeon/cvmx-wqe.h>
+
 /**
  * Wait flag values for pow functions.
  */
@@ -89,6 +91,21 @@ typedef enum {
 	CVMX_POW_NO_WAIT = 0,
 } cvmx_pow_wait_t;
 
+typedef union {
+        uint8_t         xgrp;
+        /* Fields that map XGRP for backwards compatibility */
+        struct __attribute__ ((__packed__)) {
+#ifdef __BIG_ENDIAN_BITFIELD
+                uint8_t group: 5,
+                        qus: 3;
+#else
+                uint8_t qus: 3,
+                        group: 5;
+#endif  
+        };
+} cvmx_xgrp_t;
+
+
 /**
  *  POW tag operations.	 These are used in the data stored to the POW.
  */
@@ -249,6 +266,23 @@ typedef union {
 		uint64_t reserved_0_2:3;
 	} swork;
 
+	struct {
+		CVMX_BITFIELD_FIELD(uint64_t mem_region:2,      /**< Mips64 address region. Should be CVMX_IO_SEG */
+		CVMX_BITFIELD_FIELD(uint64_t reserved_49_61:13, /**< Must be zero */
+		CVMX_BITFIELD_FIELD(uint64_t is_io:1,           /**< Must be one */
+		CVMX_BITFIELD_FIELD(uint64_t did:8,             /**< the ID of POW -- did<2:0> == 0 in this case */
+		CVMX_BITFIELD_FIELD(uint64_t node:4,            /**< OCI Node number */
+		CVMX_BITFIELD_FIELD(uint64_t reserved_32_35:4,  /**< Must be zero */
+		CVMX_BITFIELD_FIELD(uint64_t indexed:1,  /**< Indexed get_work if set */
+		CVMX_BITFIELD_FIELD(uint64_t grouped:1,  /**< get_work for group specified in index */
+		CVMX_BITFIELD_FIELD(uint64_t rtngrp:1,  /**< Return group and tt in the return if set */
+		CVMX_BITFIELD_FIELD(uint64_t reserved_16_28:13,  /**< Must be zero */
+		CVMX_BITFIELD_FIELD(uint64_t index:12,  /**< mask/grp/index of the request */
+		CVMX_BITFIELD_FIELD(uint64_t wait:1,            /**< If set, don't return load response until work is available */
+		CVMX_BITFIELD_FIELD(uint64_t reserved_0_2:3,    /**< Must be zero */
+		)))))))))))));
+	} swork_78xx;
+
     /**
      * Address for loads to get POW internal status
      */
@@ -401,24 +435,22 @@ typedef union {
      * Response to new work request loads
      */
 	struct {
-		/*
-		 * Set when no new work queue entry was returned.  *
-		 * If there was de-scheduled work, the HW will
-		 * definitely return it. When this bit is set, it
-		 * could mean either mean:
-		 *
-		 * - There was no work, or
-		 *
-		 * - There was no work that the HW could find. This
-		 *   case can happen, regardless of the wait bit value
-		 *   in the original request, when there is work in
-		 *   the IQ's that is too deep down the list.
-		 */
-		uint64_t no_work:1;
-		/* Must be zero */
-		uint64_t reserved_40_62:23;
-		/* 36 in O1 -- the work queue pointer */
-		uint64_t addr:40;
+		CVMX_BITFIELD_FIELD(uint64_t no_work:1,	    /**< Set when no new work queue entry was returned.
+                                                If there was de-scheduled work, the HW will definitely
+                                                return it. When this bit is set, it could mean
+                                                either mean:
+                                                - There was no work, or
+                                                - There was no work that the HW could find. This
+                                                    case can happen, regardless of the wait bit value
+                                                    in the original request, when there is work
+                                                    in the IQ's that is too deep down the list. */
+		CVMX_BITFIELD_FIELD(uint64_t pend_switch:1, /**< cn68XX and above, set if there was a pending tag switch*/
+		CVMX_BITFIELD_FIELD(uint64_t tt:2,
+		CVMX_BITFIELD_FIELD(uint64_t reserved_58_59:2,
+		CVMX_BITFIELD_FIELD(uint64_t grp:10,
+		CVMX_BITFIELD_FIELD(uint64_t reserved_42_47:6,    /**< Must be zero */
+		CVMX_BITFIELD_FIELD(uint64_t addr:42,	    /**< 36 in O1 -- the work queue pointer */
+		)))))));
 	} s_work;
 
     /**
@@ -1000,6 +1032,13 @@ typedef union {
 
 /* CSR typedefs have been moved to cvmx-csr-*.h */
 
+/*enum for group priority parameters which needs modification*/
+enum cvmx_sso_group_modify_mask{
+        CVMX_SSO_MODIFY_GROUP_PRIORITY = 0x01,
+        CVMX_SSO_MODIFY_GROUP_WEIGHT = 0x02,
+        CVMX_SSO_MODIFY_GROUP_AFFINITY = 0x04
+};
+
 /**
  * Get the POW tag for this core. This returns the current
  * tag type, tag, group, and POW entry index associated with
@@ -1979,4 +2018,71 @@ extern void cvmx_pow_display(void *buffer, int buffer_size);
  */
 extern int cvmx_pow_get_num_entries(void);
 
+/**
+ * This will allocate count number of SSO groups on the specified node to the
+ * calling application. These groups will be for exclusive use of the application
+ * until they are freed.
+ * @param groups_allocated is an array of length count allocated by
+ *                        the application before invoking the
+ *                        cvmx_sso_allocate_groups.  On return it will
+ *                        contain the index numbers of the groups allocated.
+ * @return 0 on success and -1 on failure.
+ */
+int cvmx_sso_allocate_groups(int node, int groups_allocated[], int count);
+int cvmx_sso_allocate_group(int node);
+
+/**
+ * This function sets the the affinity of group to the cores in 78xx.
+ * It sets up all the cores in core_mask to accept work from the specified group.
+ *
+ * @param xgrp          group to accept work from, 0 - 255.
+ * @param core_mask     mask of all the cores which will accept work from this group
+ * @param mask_set      every core has set of 2 masks which can be set to accept work
+ *                      from 256 groups. At the time of get_work, cores can choose which
+ *                      mask_set to get work from.
+ *                      <mask_set> values range from 0 to 3, where
+ *                      each of the two bits represents a mask set.
+ *                      Cores will be added to the mask set whith corresponding
+ *                      bit set, and removed from the mask set with 
+ *                      corresponding bit clear.
+ *
+ * Note: cores can only accept work from SSO groups on the same node,
+ * so the node number for the group is derived from the core number.
+ *
+ */
+static inline void cvmx_sso_set_group_core_affinity(cvmx_xgrp_t xgrp,
+                const cvmx_coremask_t * core_mask, uint8_t mask_set)
+{
+
+	return;
+}
+
+/**
+ * This function sets the priority and group affinity arbitration for each group.
+ *
+ * @param node          node number
+ * @param xgrp          group 0 - 255 to apply mask parameters to
+ * @param priority      priority of the group relative to other groups
+ *                      0x0 - highest priority
+ *                      0x7 - lowest priority
+ * @param weight        Cross-group arbitration weight to apply to this group.
+ *                      valid values are 1-63
+ *                      h/w default is 0x3f
+ * @param affinity      Processor affinity arbitration weight to apply to this group.
+ *                      If zero, affinity is disabled.
+ *                      valid values are 0-15
+ *                      h/w default which is 0xf.
+ * @param modify_mask   mask of the parameters which needs to be modified.
+ *                      enum cvmx_sso_group_modify_mask
+ *                      to modify only priority -- set bit0
+ *                      to modify only weight   -- set bit1
+ *                      to modify only affinity -- set bit2
+ */
+static inline void cvmx_sso_set_group_priority(int node, cvmx_xgrp_t xgrp,
+                        int priority, int weight, int affinity,
+                       enum cvmx_sso_group_modify_mask modify_mask)
+{
+	return;
+}
+
 #endif /* __CVMX_POW_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-qlm.h b/arch/mips/include/asm/octeon/cvmx-qlm.h
new file mode 100644
index 0000000..e8b9633
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-qlm.h
@@ -0,0 +1,235 @@
+/***********************license start***************
+ * Copyright (c) 2011-2014  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Helper utilities for qlm.
+ *
+ * <hr>$Revision: 98310 $<hr>
+ */
+
+#ifndef __CVMX_QLM_H__
+#define __CVMX_QLM_H__
+
+#include "cvmx.h"
+
+/*
+ * Interface 0 on the 78xx can be connected to qlm 0 or qlm 2. When interface
+ * 0 is connected to qlm 0, this macro must be set to 0. When interface 0 is
+ * connected to qlm 2, this macro must be set to 1.
+ */
+#define MUX_78XX_IFACE0		0
+
+/*
+ * Interface 1 on the 78xx can be connected to qlm 1 or qlm 3. When interface
+ * 1 is connected to qlm 1, this macro must be set to 0. When interface 1 is
+ * connected to qlm 3, this macro must be set to 1.
+ */
+#define MUX_78XX_IFACE1		0
+
+/* Uncomment this line to print QLM JTAG state */
+/* #define CVMX_QLM_DUMP_STATE 1 */
+
+typedef struct {
+	const char *name;
+	int stop_bit;
+	int start_bit;
+} __cvmx_qlm_jtag_field_t;
+
+/**
+ * Return the number of QLMs supported by the chip
+ * 
+ * @return  Number of QLMs
+ */
+extern int cvmx_qlm_get_num(void);
+
+/**
+ * Return the qlm number based on the interface
+ *
+ * @param interface  Interface to look up
+ */
+extern int cvmx_qlm_interface(int interface);
+
+/**
+ * Return number of lanes for a given qlm
+ * 
+ * @return  Number of lanes
+ */
+extern int cvmx_qlm_get_lanes(int qlm);
+
+/**
+ * Get the QLM JTAG fields based on Octeon model on the supported chips. 
+ *
+ * @return  qlm_jtag_field_t structure
+ */
+extern const __cvmx_qlm_jtag_field_t *cvmx_qlm_jtag_get_field(void);
+
+/**
+ * Get the QLM JTAG length by going through qlm_jtag_field for each
+ * Octeon model that is supported
+ *
+ * @return return the length.
+ */
+extern int cvmx_qlm_jtag_get_length(void);
+
+/**
+ * Initialize the QLM layer
+ */
+extern void cvmx_qlm_init(void);
+
+/**
+ * Get a field in a QLM JTAG chain
+ *
+ * @param qlm    QLM to get
+ * @param lane   Lane in QLM to get
+ * @param name   String name of field
+ *
+ * @return JTAG field value
+ */
+extern uint64_t cvmx_qlm_jtag_get(int qlm, int lane, const char *name);
+
+/**
+ * Set a field in a QLM JTAG chain
+ *
+ * @param qlm    QLM to set
+ * @param lane   Lane in QLM to set, or -1 for all lanes
+ * @param name   String name of field
+ * @param value  Value of the field
+ */
+extern void cvmx_qlm_jtag_set(int qlm, int lane, const char *name,
+			      uint64_t value);
+
+/**
+ * Errata G-16094: QLM Gen2 Equalizer Default Setting Change.
+ * CN68XX pass 1.x and CN66XX pass 1.x QLM tweak. This function tweaks the
+ * JTAG setting for a QLMs to run better at 5 and 6.25Ghz.
+ */
+extern void __cvmx_qlm_speed_tweak(void);
+
+/**
+ * Errata G-16174: QLM Gen2 PCIe IDLE DAC change.
+ * CN68XX pass 1.x, CN66XX pass 1.x and CN63XX pass 1.0-2.2 QLM tweak.
+ * This function tweaks the JTAG setting for a QLMs for PCIe to run better.
+ */
+extern void __cvmx_qlm_pcie_idle_dac_tweak(void);
+
+extern void __cvmx_qlm_pcie_cfg_rxd_set_tweak(int qlm, int lane);
+
+/**
+ * Get the speed (Gbaud) of the QLM in Mhz.
+ *
+ * @param qlm    QLM to examine
+ *
+ * @return Speed in Mhz
+ */
+extern int cvmx_qlm_get_gbaud_mhz(int qlm);
+
+enum cvmx_qlm_mode {
+	CVMX_QLM_MODE_DISABLED = -1,
+	CVMX_QLM_MODE_SGMII = 1,
+	CVMX_QLM_MODE_XAUI,
+	CVMX_QLM_MODE_RXAUI,
+	CVMX_QLM_MODE_PCIE,	/* gen3 / gen2 / gen1 */
+	CVMX_QLM_MODE_PCIE_1X2,	/* 1x2 gen2 / gen1 */
+	CVMX_QLM_MODE_PCIE_2X1,	/* 2x1 gen2 / gen1 */
+	CVMX_QLM_MODE_PCIE_1X1,	/* 1x1 gen2 / gen1 */
+	CVMX_QLM_MODE_SRIO_1X4,	/* 1x4 short / long */
+	CVMX_QLM_MODE_SRIO_2X2,	/* 2x2 short / long */
+	CVMX_QLM_MODE_SRIO_4X1,	/* 4x1 short / long */
+	CVMX_QLM_MODE_ILK,
+	CVMX_QLM_MODE_QSGMII,
+	CVMX_QLM_MODE_SGMII_SGMII,
+	CVMX_QLM_MODE_SGMII_DISABLED,
+	CVMX_QLM_MODE_DISABLED_SGMII,
+	CVMX_QLM_MODE_SGMII_QSGMII,
+	CVMX_QLM_MODE_QSGMII_QSGMII,
+	CVMX_QLM_MODE_QSGMII_DISABLED,
+	CVMX_QLM_MODE_DISABLED_QSGMII,
+	CVMX_QLM_MODE_QSGMII_SGMII,
+	CVMX_QLM_MODE_RXAUI_1X2,
+	CVMX_QLM_MODE_SATA_2X1,
+	CVMX_QLM_MODE_XLAUI,
+	CVMX_QLM_MODE_XFI,
+	CVMX_QLM_MODE_10G_KR,
+	CVMX_QLM_MODE_40G_KR4,
+	CVMX_QLM_MODE_PCIE_1X8,  /* 1x8 gen3 / gen2 / gen1 */
+	CVMX_QLM_MODE_OCI
+};
+
+enum cvmx_gmx_inf_mode {
+	CVMX_GMX_INF_MODE_DISABLED = 0,
+	CVMX_GMX_INF_MODE_SGMII = 1,     /* Other interface can be SGMII or QSGMII */
+	CVMX_GMX_INF_MODE_QSGMII = 2,    /* Other interface can be SGMII or QSGMII */
+	CVMX_GMX_INF_MODE_RXAUI = 3,     /* Only interface 0, interface 1 must be DISABLED */
+};
+
+/**
+ * These apply to DLM1 and DLM2 if its not in SATA mode
+ * Manual refers to lanes as follows:
+ *  DML 0 lane 0 == GSER0 lane 0
+ *  DML 0 lane 1 == GSER0 lane 1
+ *  DML 1 lane 2 == GSER1 lane 0
+ *  DML 1 lane 3 == GSER1 lane 1
+ *  DML 2 lane 4 == GSER2 lane 0
+ *  DML 2 lane 5 == GSER2 lane 1
+ */
+enum cvmx_pemx_cfg_mode {
+	CVMX_PEM_MD_GEN2_2LANE = 0,	/* Valid for PEM0(DLM1), PEM1(DLM2) */
+	CVMX_PEM_MD_GEN2_1LANE = 1,	/* Valid for PEM0(DLM1.0), PEM1(DLM1.1,DLM2.0), PEM2(DLM2.1) */
+	CVMX_PEM_MD_GEN2_4LANE = 2,	/* Valid for PEM0(DLM1-2) */
+	/* Reserved */
+	CVMX_PEM_MD_GEN1_2LANE = 4,	/* Valid for PEM0(DLM1), PEM1(DLM2) */
+	CVMX_PEM_MD_GEN1_1LANE = 5,	/* Valid for PEM0(DLM1.0), PEM1(DLM1.1,DLM2.0), PEM2(DLM2.1) */
+	CVMX_PEM_MD_GEN1_4LANE = 6,	/* Valid for PEM0(DLM1-2) */
+	/* Reserved */
+};
+
+/*
+ * Read QLM and return mode.
+ */
+extern enum cvmx_qlm_mode cvmx_qlm_get_mode(int qlm);
+enum cvmx_qlm_mode cvmx_qlm_get_mode_cn78xx(int node, int qlm);
+extern enum cvmx_qlm_mode cvmx_qlm_get_dlm_mode(int dlm_mode, int interface);
+
+extern void cvmx_qlm_display_registers(int qlm);
+
+extern int cvmx_qlm_measure_clock(int qlm);
+
+#endif /* __CVMX_QLM_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-range.h b/arch/mips/include/asm/octeon/cvmx-range.h
new file mode 100644
index 0000000..71b7f08
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-range.h
@@ -0,0 +1,55 @@
+/***********************license start***************
+ * Copyright (c) 2003-2012  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+#include "cvmx.h"
+
+#ifndef __CVMX_RANGE_H__
+#define __CVMX_RANGE_H__
+
+extern int  cvmx_range_init(uint64_t range_addr, int size);
+extern int  cvmx_range_alloc(uint64_t range_addr, uint64_t owner, uint64_t cnt, int align);
+extern int  cvmx_range_alloc_non_contiguos(uint64_t range_addr, uint64_t owner, uint64_t cnt,
+					   int elements[]);
+extern int  cvmx_range_reserve(uint64_t range_addr, uint64_t owner, uint64_t base, uint64_t cnt);
+extern int  cvmx_range_free_with_base(uint64_t range_addr, int base, int cnt);
+extern int  cvmx_range_free_with_owner(uint64_t range_addr, uint64_t owner);
+extern void cvmx_range_show(uint64_t range_addr);
+extern int  cvmx_range_memory_size(int nelements);
+extern int cvmx_range_free_mutiple(uint64_t range_addr, int bases[], int count);
+
+#endif // __CVMX_RANGE_H__
diff --git a/arch/mips/include/asm/octeon/cvmx-sriomaintx-defs.h b/arch/mips/include/asm/octeon/cvmx-sriomaintx-defs.h
new file mode 100644
index 0000000..f382a85
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-sriomaintx-defs.h
@@ -0,0 +1,4401 @@
+/***********************license start***************
+ * Copyright (c) 2003-2014  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+
+/**
+ * cvmx-sriomaintx-defs.h
+ *
+ * Configuration and status register (CSR) type definitions for
+ * Octeon sriomaintx.
+ *
+ * This file is auto generated. Do not edit.
+ *
+ * <hr>$Revision$<hr>
+ *
+ */
+#ifndef __CVMX_SRIOMAINTX_DEFS_H__
+#define __CVMX_SRIOMAINTX_DEFS_H__
+
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_ASMBLY_ID(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_ASMBLY_ID(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000000008ull;
+}
+#else
+#define CVMX_SRIOMAINTX_ASMBLY_ID(block_id) (0x0000000000000008ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_ASMBLY_INFO(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_ASMBLY_INFO(%lu) is invalid on this chip\n", block_id);
+	return 0x000000000000000Cull;
+}
+#else
+#define CVMX_SRIOMAINTX_ASMBLY_INFO(block_id) (0x000000000000000Cull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_BAR1_IDXX(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && (((offset <= 15)) && ((block_id <= 1)))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && (((offset <= 15)) && ((block_id == 0) || (block_id == 2) || (block_id == 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_BAR1_IDXX(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return 0x0000000000200010ull + (((offset) & 15) + ((block_id) & 3) * 0x0ull) * 4;
+}
+#else
+#define CVMX_SRIOMAINTX_BAR1_IDXX(offset, block_id) (0x0000000000200010ull + (((offset) & 15) + ((block_id) & 3) * 0x0ull) * 4)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_BELL_STATUS(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_BELL_STATUS(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000200080ull;
+}
+#else
+#define CVMX_SRIOMAINTX_BELL_STATUS(block_id) (0x0000000000200080ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_COMP_TAG(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_COMP_TAG(%lu) is invalid on this chip\n", block_id);
+	return 0x000000000000006Cull;
+}
+#else
+#define CVMX_SRIOMAINTX_COMP_TAG(block_id) (0x000000000000006Cull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_CORE_ENABLES(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_CORE_ENABLES(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000200070ull;
+}
+#else
+#define CVMX_SRIOMAINTX_CORE_ENABLES(block_id) (0x0000000000200070ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_DEV_ID(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_DEV_ID(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000000000ull;
+}
+#else
+#define CVMX_SRIOMAINTX_DEV_ID(block_id) (0x0000000000000000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_DEV_REV(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_DEV_REV(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000000004ull;
+}
+#else
+#define CVMX_SRIOMAINTX_DEV_REV(block_id) (0x0000000000000004ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_DST_OPS(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_DST_OPS(%lu) is invalid on this chip\n", block_id);
+	return 0x000000000000001Cull;
+}
+#else
+#define CVMX_SRIOMAINTX_DST_OPS(block_id) (0x000000000000001Cull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_ERB_ATTR_CAPT(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_ERB_ATTR_CAPT(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000002048ull;
+}
+#else
+#define CVMX_SRIOMAINTX_ERB_ATTR_CAPT(block_id) (0x0000000000002048ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_ERB_ERR_DET(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_ERB_ERR_DET(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000002040ull;
+}
+#else
+#define CVMX_SRIOMAINTX_ERB_ERR_DET(block_id) (0x0000000000002040ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_ERB_ERR_RATE(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_ERB_ERR_RATE(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000002068ull;
+}
+#else
+#define CVMX_SRIOMAINTX_ERB_ERR_RATE(block_id) (0x0000000000002068ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_ERB_ERR_RATE_EN(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_ERB_ERR_RATE_EN(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000002044ull;
+}
+#else
+#define CVMX_SRIOMAINTX_ERB_ERR_RATE_EN(block_id) (0x0000000000002044ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_ERB_ERR_RATE_THR(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_ERB_ERR_RATE_THR(%lu) is invalid on this chip\n", block_id);
+	return 0x000000000000206Cull;
+}
+#else
+#define CVMX_SRIOMAINTX_ERB_ERR_RATE_THR(block_id) (0x000000000000206Cull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_ERB_HDR(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_ERB_HDR(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000002000ull;
+}
+#else
+#define CVMX_SRIOMAINTX_ERB_HDR(block_id) (0x0000000000002000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_ERB_LT_ADDR_CAPT_H(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_ERB_LT_ADDR_CAPT_H(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000002010ull;
+}
+#else
+#define CVMX_SRIOMAINTX_ERB_LT_ADDR_CAPT_H(block_id) (0x0000000000002010ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_ERB_LT_ADDR_CAPT_L(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_ERB_LT_ADDR_CAPT_L(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000002014ull;
+}
+#else
+#define CVMX_SRIOMAINTX_ERB_LT_ADDR_CAPT_L(block_id) (0x0000000000002014ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_ERB_LT_CTRL_CAPT(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_ERB_LT_CTRL_CAPT(%lu) is invalid on this chip\n", block_id);
+	return 0x000000000000201Cull;
+}
+#else
+#define CVMX_SRIOMAINTX_ERB_LT_CTRL_CAPT(block_id) (0x000000000000201Cull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_ERB_LT_DEV_ID(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_ERB_LT_DEV_ID(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000002028ull;
+}
+#else
+#define CVMX_SRIOMAINTX_ERB_LT_DEV_ID(block_id) (0x0000000000002028ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_ERB_LT_DEV_ID_CAPT(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_ERB_LT_DEV_ID_CAPT(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000002018ull;
+}
+#else
+#define CVMX_SRIOMAINTX_ERB_LT_DEV_ID_CAPT(block_id) (0x0000000000002018ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_ERB_LT_ERR_DET(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_ERB_LT_ERR_DET(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000002008ull;
+}
+#else
+#define CVMX_SRIOMAINTX_ERB_LT_ERR_DET(block_id) (0x0000000000002008ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_ERB_LT_ERR_EN(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_ERB_LT_ERR_EN(%lu) is invalid on this chip\n", block_id);
+	return 0x000000000000200Cull;
+}
+#else
+#define CVMX_SRIOMAINTX_ERB_LT_ERR_EN(block_id) (0x000000000000200Cull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_ERB_PACK_CAPT_1(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_ERB_PACK_CAPT_1(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000002050ull;
+}
+#else
+#define CVMX_SRIOMAINTX_ERB_PACK_CAPT_1(block_id) (0x0000000000002050ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_ERB_PACK_CAPT_2(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_ERB_PACK_CAPT_2(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000002054ull;
+}
+#else
+#define CVMX_SRIOMAINTX_ERB_PACK_CAPT_2(block_id) (0x0000000000002054ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_ERB_PACK_CAPT_3(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_ERB_PACK_CAPT_3(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000002058ull;
+}
+#else
+#define CVMX_SRIOMAINTX_ERB_PACK_CAPT_3(block_id) (0x0000000000002058ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_ERB_PACK_SYM_CAPT(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_ERB_PACK_SYM_CAPT(%lu) is invalid on this chip\n", block_id);
+	return 0x000000000000204Cull;
+}
+#else
+#define CVMX_SRIOMAINTX_ERB_PACK_SYM_CAPT(block_id) (0x000000000000204Cull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_HB_DEV_ID_LOCK(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_HB_DEV_ID_LOCK(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000000068ull;
+}
+#else
+#define CVMX_SRIOMAINTX_HB_DEV_ID_LOCK(block_id) (0x0000000000000068ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_IR_BUFFER_CONFIG(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_IR_BUFFER_CONFIG(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000102000ull;
+}
+#else
+#define CVMX_SRIOMAINTX_IR_BUFFER_CONFIG(block_id) (0x0000000000102000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_IR_BUFFER_CONFIG2(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_IR_BUFFER_CONFIG2(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000102004ull;
+}
+#else
+#define CVMX_SRIOMAINTX_IR_BUFFER_CONFIG2(block_id) (0x0000000000102004ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_IR_PD_PHY_CTRL(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_IR_PD_PHY_CTRL(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000107028ull;
+}
+#else
+#define CVMX_SRIOMAINTX_IR_PD_PHY_CTRL(block_id) (0x0000000000107028ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_IR_PD_PHY_STAT(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_IR_PD_PHY_STAT(%lu) is invalid on this chip\n", block_id);
+	return 0x000000000010702Cull;
+}
+#else
+#define CVMX_SRIOMAINTX_IR_PD_PHY_STAT(block_id) (0x000000000010702Cull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_IR_PI_PHY_CTRL(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_IR_PI_PHY_CTRL(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000107020ull;
+}
+#else
+#define CVMX_SRIOMAINTX_IR_PI_PHY_CTRL(block_id) (0x0000000000107020ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_IR_PI_PHY_STAT(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_IR_PI_PHY_STAT(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000107024ull;
+}
+#else
+#define CVMX_SRIOMAINTX_IR_PI_PHY_STAT(block_id) (0x0000000000107024ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_IR_SP_RX_CTRL(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_IR_SP_RX_CTRL(%lu) is invalid on this chip\n", block_id);
+	return 0x000000000010700Cull;
+}
+#else
+#define CVMX_SRIOMAINTX_IR_SP_RX_CTRL(block_id) (0x000000000010700Cull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_IR_SP_RX_DATA(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_IR_SP_RX_DATA(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000107014ull;
+}
+#else
+#define CVMX_SRIOMAINTX_IR_SP_RX_DATA(block_id) (0x0000000000107014ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_IR_SP_RX_STAT(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_IR_SP_RX_STAT(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000107010ull;
+}
+#else
+#define CVMX_SRIOMAINTX_IR_SP_RX_STAT(block_id) (0x0000000000107010ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_IR_SP_TX_CTRL(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_IR_SP_TX_CTRL(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000107000ull;
+}
+#else
+#define CVMX_SRIOMAINTX_IR_SP_TX_CTRL(block_id) (0x0000000000107000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_IR_SP_TX_DATA(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_IR_SP_TX_DATA(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000107008ull;
+}
+#else
+#define CVMX_SRIOMAINTX_IR_SP_TX_DATA(block_id) (0x0000000000107008ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_IR_SP_TX_STAT(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_IR_SP_TX_STAT(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000107004ull;
+}
+#else
+#define CVMX_SRIOMAINTX_IR_SP_TX_STAT(block_id) (0x0000000000107004ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_LANE_X_STATUS_0(unsigned long offset, unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && (((offset <= 3)) && ((block_id <= 1)))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && (((offset <= 3)) && ((block_id == 0) || (block_id == 2) || (block_id == 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_LANE_X_STATUS_0(%lu,%lu) is invalid on this chip\n", offset, block_id);
+	return 0x0000000000001010ull + (((offset) & 3) + ((block_id) & 3) * 0x0ull) * 32;
+}
+#else
+#define CVMX_SRIOMAINTX_LANE_X_STATUS_0(offset, block_id) (0x0000000000001010ull + (((offset) & 3) + ((block_id) & 3) * 0x0ull) * 32)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_LCS_BA0(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_LCS_BA0(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000000058ull;
+}
+#else
+#define CVMX_SRIOMAINTX_LCS_BA0(block_id) (0x0000000000000058ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_LCS_BA1(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_LCS_BA1(%lu) is invalid on this chip\n", block_id);
+	return 0x000000000000005Cull;
+}
+#else
+#define CVMX_SRIOMAINTX_LCS_BA1(block_id) (0x000000000000005Cull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_M2S_BAR0_START0(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_M2S_BAR0_START0(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000200000ull;
+}
+#else
+#define CVMX_SRIOMAINTX_M2S_BAR0_START0(block_id) (0x0000000000200000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_M2S_BAR0_START1(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_M2S_BAR0_START1(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000200004ull;
+}
+#else
+#define CVMX_SRIOMAINTX_M2S_BAR0_START1(block_id) (0x0000000000200004ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_M2S_BAR1_START0(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_M2S_BAR1_START0(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000200008ull;
+}
+#else
+#define CVMX_SRIOMAINTX_M2S_BAR1_START0(block_id) (0x0000000000200008ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_M2S_BAR1_START1(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_M2S_BAR1_START1(%lu) is invalid on this chip\n", block_id);
+	return 0x000000000020000Cull;
+}
+#else
+#define CVMX_SRIOMAINTX_M2S_BAR1_START1(block_id) (0x000000000020000Cull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_M2S_BAR2_START(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_M2S_BAR2_START(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000200050ull;
+}
+#else
+#define CVMX_SRIOMAINTX_M2S_BAR2_START(block_id) (0x0000000000200050ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_MAC_CTRL(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_MAC_CTRL(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000200068ull;
+}
+#else
+#define CVMX_SRIOMAINTX_MAC_CTRL(block_id) (0x0000000000200068ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_PE_FEAT(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_PE_FEAT(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000000010ull;
+}
+#else
+#define CVMX_SRIOMAINTX_PE_FEAT(block_id) (0x0000000000000010ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_PE_LLC(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_PE_LLC(%lu) is invalid on this chip\n", block_id);
+	return 0x000000000000004Cull;
+}
+#else
+#define CVMX_SRIOMAINTX_PE_LLC(block_id) (0x000000000000004Cull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_PORT_0_CTL(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_PORT_0_CTL(%lu) is invalid on this chip\n", block_id);
+	return 0x000000000000015Cull;
+}
+#else
+#define CVMX_SRIOMAINTX_PORT_0_CTL(block_id) (0x000000000000015Cull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_PORT_0_CTL2(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_PORT_0_CTL2(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000000154ull;
+}
+#else
+#define CVMX_SRIOMAINTX_PORT_0_CTL2(block_id) (0x0000000000000154ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_PORT_0_ERR_STAT(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_PORT_0_ERR_STAT(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000000158ull;
+}
+#else
+#define CVMX_SRIOMAINTX_PORT_0_ERR_STAT(block_id) (0x0000000000000158ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_PORT_0_LINK_REQ(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_PORT_0_LINK_REQ(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000000140ull;
+}
+#else
+#define CVMX_SRIOMAINTX_PORT_0_LINK_REQ(block_id) (0x0000000000000140ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_PORT_0_LINK_RESP(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_PORT_0_LINK_RESP(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000000144ull;
+}
+#else
+#define CVMX_SRIOMAINTX_PORT_0_LINK_RESP(block_id) (0x0000000000000144ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_PORT_0_LOCAL_ACKID(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_PORT_0_LOCAL_ACKID(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000000148ull;
+}
+#else
+#define CVMX_SRIOMAINTX_PORT_0_LOCAL_ACKID(block_id) (0x0000000000000148ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_PORT_GEN_CTL(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_PORT_GEN_CTL(%lu) is invalid on this chip\n", block_id);
+	return 0x000000000000013Cull;
+}
+#else
+#define CVMX_SRIOMAINTX_PORT_GEN_CTL(block_id) (0x000000000000013Cull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_PORT_LT_CTL(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_PORT_LT_CTL(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000000120ull;
+}
+#else
+#define CVMX_SRIOMAINTX_PORT_LT_CTL(block_id) (0x0000000000000120ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_PORT_MBH0(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_PORT_MBH0(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000000100ull;
+}
+#else
+#define CVMX_SRIOMAINTX_PORT_MBH0(block_id) (0x0000000000000100ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_PORT_RT_CTL(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_PORT_RT_CTL(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000000124ull;
+}
+#else
+#define CVMX_SRIOMAINTX_PORT_RT_CTL(block_id) (0x0000000000000124ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_PORT_TTL_CTL(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_PORT_TTL_CTL(%lu) is invalid on this chip\n", block_id);
+	return 0x000000000000012Cull;
+}
+#else
+#define CVMX_SRIOMAINTX_PORT_TTL_CTL(block_id) (0x000000000000012Cull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_PRI_DEV_ID(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_PRI_DEV_ID(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000000060ull;
+}
+#else
+#define CVMX_SRIOMAINTX_PRI_DEV_ID(block_id) (0x0000000000000060ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_SEC_DEV_CTRL(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_SEC_DEV_CTRL(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000200064ull;
+}
+#else
+#define CVMX_SRIOMAINTX_SEC_DEV_CTRL(block_id) (0x0000000000200064ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_SEC_DEV_ID(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_SEC_DEV_ID(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000200060ull;
+}
+#else
+#define CVMX_SRIOMAINTX_SEC_DEV_ID(block_id) (0x0000000000200060ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_SERIAL_LANE_HDR(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_SERIAL_LANE_HDR(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000001000ull;
+}
+#else
+#define CVMX_SRIOMAINTX_SERIAL_LANE_HDR(block_id) (0x0000000000001000ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_SRC_OPS(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_SRC_OPS(%lu) is invalid on this chip\n", block_id);
+	return 0x0000000000000018ull;
+}
+#else
+#define CVMX_SRIOMAINTX_SRC_OPS(block_id) (0x0000000000000018ull)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_SRIOMAINTX_TX_DROP(unsigned long block_id)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN63XX) && ((block_id <= 1))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN66XX) && ((block_id == 0) || ((block_id >= 2) && (block_id <= 3))))))
+		cvmx_warn("CVMX_SRIOMAINTX_TX_DROP(%lu) is invalid on this chip\n", block_id);
+	return 0x000000000020006Cull;
+}
+#else
+#define CVMX_SRIOMAINTX_TX_DROP(block_id) (0x000000000020006Cull)
+#endif
+
+/**
+ * cvmx_sriomaint#_asmbly_id
+ *
+ * SRIOMAINT_ASMBLY_ID = SRIO Assembly ID
+ *
+ * The Assembly ID register shows the Assembly ID and Vendor
+ *
+ * Notes:
+ * The Assembly ID register shows the Assembly ID and Vendor specified in $SRIO_ASMBLY_ID.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_ASMBLY_ID     hclk    hrst_n
+ */
+union cvmx_sriomaintx_asmbly_id {
+	uint32_t u32;
+	struct cvmx_sriomaintx_asmbly_id_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t assy_id                      : 16; /**< Assembly Identifer */
+	uint32_t assy_ven                     : 16; /**< Assembly Vendor Identifer */
+#else
+	uint32_t assy_ven                     : 16;
+	uint32_t assy_id                      : 16;
+#endif
+	} s;
+	struct cvmx_sriomaintx_asmbly_id_s    cn63xx;
+	struct cvmx_sriomaintx_asmbly_id_s    cn63xxp1;
+	struct cvmx_sriomaintx_asmbly_id_s    cn66xx;
+};
+typedef union cvmx_sriomaintx_asmbly_id cvmx_sriomaintx_asmbly_id_t;
+
+/**
+ * cvmx_sriomaint#_asmbly_info
+ *
+ * SRIOMAINT_ASMBLY_INFO = SRIO Assembly Information
+ *
+ * The Assembly Info register shows the Assembly Revision specified in $SRIO_ASMBLY_INFO
+ *
+ * Notes:
+ * The Assembly Info register shows the Assembly Revision specified in $SRIO_ASMBLY_INFO and Extended
+ *  Feature Pointer.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_ASMBLY_INFO   hclk    hrst_n
+ */
+union cvmx_sriomaintx_asmbly_info {
+	uint32_t u32;
+	struct cvmx_sriomaintx_asmbly_info_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t assy_rev                     : 16; /**< Assembly Revision */
+	uint32_t ext_fptr                     : 16; /**< Pointer to the first entry in the extended feature
+                                                         list. */
+#else
+	uint32_t ext_fptr                     : 16;
+	uint32_t assy_rev                     : 16;
+#endif
+	} s;
+	struct cvmx_sriomaintx_asmbly_info_s  cn63xx;
+	struct cvmx_sriomaintx_asmbly_info_s  cn63xxp1;
+	struct cvmx_sriomaintx_asmbly_info_s  cn66xx;
+};
+typedef union cvmx_sriomaintx_asmbly_info cvmx_sriomaintx_asmbly_info_t;
+
+/**
+ * cvmx_sriomaint#_bar1_idx#
+ *
+ * SRIOMAINT_BAR1_IDXX = SRIO BAR1 IndexX Register
+ *
+ * Contains address index and control bits for access to memory ranges of BAR1.
+ *
+ * Notes:
+ * This register specifies the Octeon address, endian swap and cache status associated with each of
+ *  the 16 BAR1 entries.  The local address bits used are based on the BARSIZE field located in the
+ *  SRIOMAINT(0,2..3)_M2S_BAR1_START0 register.  This register is only writeable over SRIO if the
+ *  SRIO(0,2..3)_ACC_CTRL.DENY_BAR1 bit is zero.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_BAR1_IDX[0:15]        hclk    hrst_n
+ */
+union cvmx_sriomaintx_bar1_idxx {
+	uint32_t u32;
+	struct cvmx_sriomaintx_bar1_idxx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t reserved_30_31               : 2;
+	uint32_t la                           : 22; /**< L2/DRAM Address bits [37:16]
+                                                         Not all LA[21:0] bits are used by SRIO hardware,
+                                                         depending on SRIOMAINT(0,2..3)_M2S_BAR1_START1[BARSIZE].
+
+                                                                                 Become
+                                                                                 L2/DRAM
+                                                                                 Address  Entry
+                                                         BARSIZE   LA Bits Used   Bits    Size
+                                                            0        LA[21:0]    [37:16]   64KB
+                                                            1        LA[21:1]    [37:17]  128KB
+                                                            2        LA[21:2]    [37:18]  256KB
+                                                            3        LA[21:3]    [37:19]  512KB
+                                                            4        LA[21:4]    [37:20]    1MB
+                                                            5        LA[21:5]    [37:21]    2MB
+                                                            6        LA[21:6]    [37:22]    4MB
+                                                            7        LA[21:7]    [37:23]    8MB
+                                                            8        LA[21:8]    [37:24]   16MB
+                                                            9        LA[21:9]    [37:25]   32MB
+                                                           10        LA[21:10]   [37:26]   64MB
+                                                           11        LA[21:11]   [37:27]  128MB
+                                                           12        LA[21:12]   [37:28]  256MB
+                                                           13        LA[21:13]   [37:29]  512MB */
+	uint32_t reserved_6_7                 : 2;
+	uint32_t es                           : 2;  /**< Endian Swap Mode.
+                                                         0 = No Swap
+                                                         1 = 64-bit Swap Bytes [ABCD_EFGH] -> [HGFE_DCBA]
+                                                         2 = 32-bit Swap Words [ABCD_EFGH] -> [DCBA_HGFE]
+                                                         3 = 32-bit Word Exch  [ABCD_EFGH] -> [EFGH_ABCD] */
+	uint32_t nca                          : 1;  /**< Non-Cacheable Access Mode.  When set, transfers
+                                                         through this window are not cacheable. */
+	uint32_t reserved_1_2                 : 2;
+	uint32_t enable                       : 1;  /**< When set the selected index address is valid. */
+#else
+	uint32_t enable                       : 1;
+	uint32_t reserved_1_2                 : 2;
+	uint32_t nca                          : 1;
+	uint32_t es                           : 2;
+	uint32_t reserved_6_7                 : 2;
+	uint32_t la                           : 22;
+	uint32_t reserved_30_31               : 2;
+#endif
+	} s;
+	struct cvmx_sriomaintx_bar1_idxx_s    cn63xx;
+	struct cvmx_sriomaintx_bar1_idxx_s    cn63xxp1;
+	struct cvmx_sriomaintx_bar1_idxx_s    cn66xx;
+};
+typedef union cvmx_sriomaintx_bar1_idxx cvmx_sriomaintx_bar1_idxx_t;
+
+/**
+ * cvmx_sriomaint#_bell_status
+ *
+ * SRIOMAINT_BELL_STATUS = SRIO Incoming Doorbell Status
+ *
+ * The SRIO Incoming (RX) Doorbell Status
+ *
+ * Notes:
+ * This register displays the status of the doorbells received.  If FULL is set the SRIO device will
+ *  retry incoming transactions.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_BELL_STATUS   hclk    hrst_n
+ */
+union cvmx_sriomaintx_bell_status {
+	uint32_t u32;
+	struct cvmx_sriomaintx_bell_status_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t reserved_1_31                : 31;
+	uint32_t full                         : 1;  /**< Not able to receive Doorbell Transactions */
+#else
+	uint32_t full                         : 1;
+	uint32_t reserved_1_31                : 31;
+#endif
+	} s;
+	struct cvmx_sriomaintx_bell_status_s  cn63xx;
+	struct cvmx_sriomaintx_bell_status_s  cn63xxp1;
+	struct cvmx_sriomaintx_bell_status_s  cn66xx;
+};
+typedef union cvmx_sriomaintx_bell_status cvmx_sriomaintx_bell_status_t;
+
+/**
+ * cvmx_sriomaint#_comp_tag
+ *
+ * SRIOMAINT_COMP_TAG = SRIO Component Tag
+ *
+ * Component Tag
+ *
+ * Notes:
+ * This register contains a component tag value for the processing element and the value can be
+ *  assigned by software when the device is initialized.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_COMP_TAG      hclk    hrst_n
+ */
+union cvmx_sriomaintx_comp_tag {
+	uint32_t u32;
+	struct cvmx_sriomaintx_comp_tag_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t comp_tag                     : 32; /**< Component Tag for Firmware Use */
+#else
+	uint32_t comp_tag                     : 32;
+#endif
+	} s;
+	struct cvmx_sriomaintx_comp_tag_s     cn63xx;
+	struct cvmx_sriomaintx_comp_tag_s     cn63xxp1;
+	struct cvmx_sriomaintx_comp_tag_s     cn66xx;
+};
+typedef union cvmx_sriomaintx_comp_tag cvmx_sriomaintx_comp_tag_t;
+
+/**
+ * cvmx_sriomaint#_core_enables
+ *
+ * SRIOMAINT_CORE_ENABLES = SRIO Core Control
+ *
+ * Core Control
+ *
+ * Notes:
+ * This register displays the reset state of the Octeon Core Logic while the SRIO Link is running.
+ *  The bit should be set after the software has initialized the chip to allow memory operations.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_CORE_ENABLES  hclk    hrst_n, srst_n
+ */
+union cvmx_sriomaintx_core_enables {
+	uint32_t u32;
+	struct cvmx_sriomaintx_core_enables_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t reserved_5_31                : 27;
+	uint32_t halt                         : 1;  /**< OCTEON currently in Reset
+                                                         0 = All OCTEON resources are available.
+                                                         1 = The OCTEON is in reset. When this bit is set,
+                                                             SRIO maintenance registers can be accessed,
+                                                             but BAR0, BAR1, and BAR2 cannot be. */
+	uint32_t imsg1                        : 1;  /**< Allow Incoming Message Unit 1 Operations
+                                                         Note: This bit is cleared when the C63XX is reset
+                                                          0 = SRIO Incoming Messages to Unit 1 ignored and
+                                                              return error response
+                                                          1 = SRIO Incoming Messages to Unit 1 */
+	uint32_t imsg0                        : 1;  /**< Allow Incoming Message Unit 0 Operations
+                                                         Note: This bit is cleared when the C63XX is reset
+                                                          0 = SRIO Incoming Messages to Unit 0 ignored and
+                                                              return error response
+                                                          1 = SRIO Incoming Messages to Unit 0 */
+	uint32_t doorbell                     : 1;  /**< Allow Inbound Doorbell Operations
+                                                         Note: This bit is cleared when the C63XX is reset
+                                                          0 = SRIO Doorbell OPs ignored and return error
+                                                              response
+                                                          1 = SRIO Doorbell OPs Allowed */
+	uint32_t memory                       : 1;  /**< Allow Inbound/Outbound Memory Operations
+                                                         Note: This bit is cleared when the C63XX is reset
+                                                          0 = SRIO Incoming Nwrites and Swrites are
+                                                              dropped.  Incoming Nreads, Atomics and
+                                                              NwriteRs return responses with ERROR status.
+                                                              SRIO Incoming Maintenance BAR Memory Accesses
+                                                              are processed normally.
+                                                              Outgoing Store Operations are Dropped
+                                                              Outgoing Load Operations are not issued and
+                                                              return all 1's with an ERROR status.
+                                                              In Flight Operations started while the bit is
+                                                              set in both directions will complete normally.
+                                                          1 = SRIO Memory Read/Write OPs Allowed */
+#else
+	uint32_t memory                       : 1;
+	uint32_t doorbell                     : 1;
+	uint32_t imsg0                        : 1;
+	uint32_t imsg1                        : 1;
+	uint32_t halt                         : 1;
+	uint32_t reserved_5_31                : 27;
+#endif
+	} s;
+	struct cvmx_sriomaintx_core_enables_s cn63xx;
+	struct cvmx_sriomaintx_core_enables_s cn63xxp1;
+	struct cvmx_sriomaintx_core_enables_s cn66xx;
+};
+typedef union cvmx_sriomaintx_core_enables cvmx_sriomaintx_core_enables_t;
+
+/**
+ * cvmx_sriomaint#_dev_id
+ *
+ * SRIOMAINT_DEV_ID = SRIO Device ID
+ *
+ * The DeviceVendor Identity field identifies the vendor that manufactured the device
+ *
+ * Notes:
+ * This register identifies Cavium Networks and the Product ID.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_DEV_ID        hclk    hrst_n
+ */
+union cvmx_sriomaintx_dev_id {
+	uint32_t u32;
+	struct cvmx_sriomaintx_dev_id_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t device                       : 16; /**< Product Identity */
+	uint32_t vendor                       : 16; /**< Cavium Vendor Identity */
+#else
+	uint32_t vendor                       : 16;
+	uint32_t device                       : 16;
+#endif
+	} s;
+	struct cvmx_sriomaintx_dev_id_s       cn63xx;
+	struct cvmx_sriomaintx_dev_id_s       cn63xxp1;
+	struct cvmx_sriomaintx_dev_id_s       cn66xx;
+};
+typedef union cvmx_sriomaintx_dev_id cvmx_sriomaintx_dev_id_t;
+
+/**
+ * cvmx_sriomaint#_dev_rev
+ *
+ * SRIOMAINT_DEV_REV = SRIO Device Revision
+ *
+ * The Device Revision register identifies the chip pass and revision
+ *
+ * Notes:
+ * This register identifies the chip pass and revision derived from the fuses.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_DEV_REV       hclk    hrst_n
+ */
+union cvmx_sriomaintx_dev_rev {
+	uint32_t u32;
+	struct cvmx_sriomaintx_dev_rev_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t reserved_8_31                : 24;
+	uint32_t revision                     : 8;  /**< Chip Pass/Revision */
+#else
+	uint32_t revision                     : 8;
+	uint32_t reserved_8_31                : 24;
+#endif
+	} s;
+	struct cvmx_sriomaintx_dev_rev_s      cn63xx;
+	struct cvmx_sriomaintx_dev_rev_s      cn63xxp1;
+	struct cvmx_sriomaintx_dev_rev_s      cn66xx;
+};
+typedef union cvmx_sriomaintx_dev_rev cvmx_sriomaintx_dev_rev_t;
+
+/**
+ * cvmx_sriomaint#_dst_ops
+ *
+ * SRIOMAINT_DST_OPS = SRIO Source Operations
+ *
+ * The logical operations supported from external devices.
+ *
+ * Notes:
+ * The logical operations supported from external devices.   The Destination OPs register shows the
+ *  operations specified in the SRIO(0,2..3)_IP_FEATURE.OPS register.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_DST_OPS       hclk    hrst_n
+ */
+union cvmx_sriomaintx_dst_ops {
+	uint32_t u32;
+	struct cvmx_sriomaintx_dst_ops_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t gsm_read                     : 1;  /**< PE does not support Read Home operations.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<31>] */
+	uint32_t i_read                       : 1;  /**< PE does not support Instruction Read.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<30>] */
+	uint32_t rd_own                       : 1;  /**< PE does not support Read for Ownership.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<29>] */
+	uint32_t d_invald                     : 1;  /**< PE does not support Data Cache Invalidate.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<28>] */
+	uint32_t castout                      : 1;  /**< PE does not support Castout Operations.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<27>] */
+	uint32_t d_flush                      : 1;  /**< PE does not support Data Cache Flush.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<26>] */
+	uint32_t io_read                      : 1;  /**< PE does not support IO Read.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<25>] */
+	uint32_t i_invald                     : 1;  /**< PE does not support Instruction Cache Invalidate.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<24>] */
+	uint32_t tlb_inv                      : 1;  /**< PE does not support TLB Entry Invalidate.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<23>] */
+	uint32_t tlb_invs                     : 1;  /**< PE does not support TLB Entry Invalidate Sync.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<22>] */
+	uint32_t reserved_16_21               : 6;
+	uint32_t read                         : 1;  /**< PE can support Nread operations.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<15>] */
+	uint32_t write                        : 1;  /**< PE can support Nwrite operations.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<14>] */
+	uint32_t swrite                       : 1;  /**< PE can support Swrite operations.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<13>] */
+	uint32_t write_r                      : 1;  /**< PE can support Write with Response operations.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<12>] */
+	uint32_t msg                          : 1;  /**< PE can support Data Message operations.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<11>] */
+	uint32_t doorbell                     : 1;  /**< PE can support Doorbell operations.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<10>] */
+	uint32_t compswap                     : 1;  /**< PE does not support Atomic Compare and Swap.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<9>] */
+	uint32_t testswap                     : 1;  /**< PE does not support Atomic Test and Swap.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<8>] */
+	uint32_t atom_inc                     : 1;  /**< PE can support Atomic increment operations.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<7>] */
+	uint32_t atom_dec                     : 1;  /**< PE can support Atomic decrement operations.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<6>] */
+	uint32_t atom_set                     : 1;  /**< PE can support Atomic set operations.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<5>] */
+	uint32_t atom_clr                     : 1;  /**< PE can support Atomic clear operations.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<4>] */
+	uint32_t atom_swp                     : 1;  /**< PE does not support Atomic Swap.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<3>] */
+	uint32_t port_wr                      : 1;  /**< PE can Port Write operations.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<2>] */
+	uint32_t reserved_0_1                 : 2;
+#else
+	uint32_t reserved_0_1                 : 2;
+	uint32_t port_wr                      : 1;
+	uint32_t atom_swp                     : 1;
+	uint32_t atom_clr                     : 1;
+	uint32_t atom_set                     : 1;
+	uint32_t atom_dec                     : 1;
+	uint32_t atom_inc                     : 1;
+	uint32_t testswap                     : 1;
+	uint32_t compswap                     : 1;
+	uint32_t doorbell                     : 1;
+	uint32_t msg                          : 1;
+	uint32_t write_r                      : 1;
+	uint32_t swrite                       : 1;
+	uint32_t write                        : 1;
+	uint32_t read                         : 1;
+	uint32_t reserved_16_21               : 6;
+	uint32_t tlb_invs                     : 1;
+	uint32_t tlb_inv                      : 1;
+	uint32_t i_invald                     : 1;
+	uint32_t io_read                      : 1;
+	uint32_t d_flush                      : 1;
+	uint32_t castout                      : 1;
+	uint32_t d_invald                     : 1;
+	uint32_t rd_own                       : 1;
+	uint32_t i_read                       : 1;
+	uint32_t gsm_read                     : 1;
+#endif
+	} s;
+	struct cvmx_sriomaintx_dst_ops_s      cn63xx;
+	struct cvmx_sriomaintx_dst_ops_s      cn63xxp1;
+	struct cvmx_sriomaintx_dst_ops_s      cn66xx;
+};
+typedef union cvmx_sriomaintx_dst_ops cvmx_sriomaintx_dst_ops_t;
+
+/**
+ * cvmx_sriomaint#_erb_attr_capt
+ *
+ * SRIOMAINT_ERB_ATTR_CAPT = SRIO Attributes Capture
+ *
+ * Attributes Capture
+ *
+ * Notes:
+ * This register contains the information captured during the error.
+ *  The HW will not update this register (i.e. this register is locked) while
+ *  VALID is set in this CSR.
+ *  The HW sets SRIO_INT_REG[PHY_ERB] every time it sets VALID in this CSR.
+ *  To handle the interrupt, the following procedure may be best:
+ *       (1) clear SRIO_INT_REG[PHY_ERB],
+ *       (2) read this CSR, corresponding SRIOMAINT*_ERB_ERR_DET, SRIOMAINT*_ERB_PACK_SYM_CAPT,
+ *           SRIOMAINT*_ERB_PACK_CAPT_1, SRIOMAINT*_ERB_PACK_CAPT_2, and SRIOMAINT*_ERB_PACK_CAPT_3
+ *       (3) Write VALID in this CSR to 0.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_ERB_ATTR_CAPT hclk    hrst_n
+ */
+union cvmx_sriomaintx_erb_attr_capt {
+	uint32_t u32;
+	struct cvmx_sriomaintx_erb_attr_capt_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t inf_type                     : 3;  /**< Type of Information Logged.
+                                                         000 - Packet
+                                                         010 - Short Control Symbol
+                                                               (use only first capture register)
+                                                         100 - Implementation Specific Error Reporting
+                                                         All Others Reserved */
+	uint32_t err_type                     : 5;  /**< The encoded value of the 31 minus the bit in
+                                                         SRIOMAINT(0,2..3)_ERB_ERR_DET that describes the error
+                                                         captured in SRIOMAINT(0,2..3)_ERB_*CAPT Registers.
+                                                         (For example a value of 5 indicates 31-5 = bit 26) */
+	uint32_t err_info                     : 20; /**< Error Info.
+                                                         ERR_TYPE Bits   Description
+                                                            0     23     TX Protocol Error
+                                                                  22     RX Protocol Error
+                                                                  21     TX Link Response Timeout
+                                                                  20     TX ACKID Timeout
+                                                                  - 19:16  Reserved
+                                                                  - 15:12  TX Protocol ID
+                                                                         1 = Rcvd Unexpected Link Response
+                                                                         2 = Rcvd Link Response before Req
+                                                                         3 = Rcvd NACK servicing NACK
+                                                                         4 = Rcvd NACK
+                                                                         5 = Rcvd RETRY servicing RETRY
+                                                                         6 = Rcvd RETRY servicing NACK
+                                                                         7 = Rcvd ACK servicing RETRY
+                                                                         8 = Rcvd ACK servicing NACK
+                                                                         9 = Unexp ACKID on ACK or RETRY
+                                                                        10 = Unexp ACK or RETRY
+                                                                  - 11:8   Reserved
+                                                                  - 7:4   RX Protocol ID
+                                                                         1 = Rcvd EOP w/o Prev SOP
+                                                                         2 = Rcvd STOMP w/o Prev SOP
+                                                                         3 = Unexp RESTART
+                                                                         4 = Redundant Status from LinkReq
+                                                          9-16    23:20  RX K Bits
+                                                                  - 19:0   Reserved
+                                                           26     23:20  RX K Bits
+                                                                  - 19:0   Reserved
+                                                           27     23:12  Type
+                                                                           0x000 TX
+                                                                           0x010 RX
+                                                                  - 11:8   RX or TX Protocol ID (see above)
+                                                                  - 7:4   Reserved
+                                                           30     23:20  RX K Bits
+                                                                  - 19:0   Reserved
+                                                           31     23:16  ACKID Timeout 0x2
+                                                                  - 15:14  Reserved
+                                                                  - 13:8   AckID
+                                                                  - 7:4   Reserved
+                                                           All others ERR_TYPEs are reserved. */
+	uint32_t reserved_1_3                 : 3;
+	uint32_t valid                        : 1;  /**< This bit is set by hardware to indicate that the
+                                                         Packet/control symbol capture registers contain
+                                                         valid information. For control symbols, only
+                                                         capture register 0 will contain meaningful
+                                                         information.  This bit must be cleared by software
+                                                         to allow capture of other errors. */
+#else
+	uint32_t valid                        : 1;
+	uint32_t reserved_1_3                 : 3;
+	uint32_t err_info                     : 20;
+	uint32_t err_type                     : 5;
+	uint32_t inf_type                     : 3;
+#endif
+	} s;
+	struct cvmx_sriomaintx_erb_attr_capt_s cn63xx;
+	struct cvmx_sriomaintx_erb_attr_capt_cn63xxp1 {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t inf_type                     : 3;  /**< Type of Information Logged.
+                                                         000 - Packet
+                                                         010 - Short Control Symbol
+                                                               (use only first capture register)
+                                                         All Others Reserved */
+	uint32_t err_type                     : 5;  /**< The encoded value of the 31 minus the bit in
+                                                         SRIOMAINT(0..1)_ERB_ERR_DET that describes the error
+                                                         captured in SRIOMAINT(0..1)_ERB_*CAPT Registers.
+                                                         (For example a value of 5 indicates 31-5 = bit 26) */
+	uint32_t reserved_1_23                : 23;
+	uint32_t valid                        : 1;  /**< This bit is set by hardware to indicate that the
+                                                         Packet/control symbol capture registers contain
+                                                         valid information. For control symbols, only
+                                                         capture register 0 will contain meaningful
+                                                         information.  This bit must be cleared by software
+                                                         to allow capture of other errors. */
+#else
+	uint32_t valid                        : 1;
+	uint32_t reserved_1_23                : 23;
+	uint32_t err_type                     : 5;
+	uint32_t inf_type                     : 3;
+#endif
+	} cn63xxp1;
+	struct cvmx_sriomaintx_erb_attr_capt_s cn66xx;
+};
+typedef union cvmx_sriomaintx_erb_attr_capt cvmx_sriomaintx_erb_attr_capt_t;
+
+/**
+ * cvmx_sriomaint#_erb_err_det
+ *
+ * SRIOMAINT_ERB_ERR_DET = SRIO Error Detect
+ *
+ * Error Detect
+ *
+ * Notes:
+ * The Error Detect Register indicates physical layer transmission errors detected by the hardware.
+ *  The HW will not update this register (i.e. this register is locked) while
+ *  SRIOMAINT*_ERB_ATTR_CAPT[VALID] is set.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_ERB_ERR_DET   hclk    hrst_n
+ */
+union cvmx_sriomaintx_erb_err_det {
+	uint32_t u32;
+	struct cvmx_sriomaintx_erb_err_det_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t imp_err                      : 1;  /**< Implementation Specific Error. */
+	uint32_t reserved_23_30               : 8;
+	uint32_t ctl_crc                      : 1;  /**< Received a control symbol with a bad CRC value
+                                                         Complete Symbol in SRIOMAINT(0,2..3)_ERB_PACK_SYM_CAPT */
+	uint32_t uns_id                       : 1;  /**< Received an acknowledge control symbol with an
+                                                         unexpected ackID (packet-accepted or packet_retry)
+                                                         Partial Symbol in SRIOMAINT(0,2..3)_ERB_PACK_SYM_CAPT */
+	uint32_t nack                         : 1;  /**< Received packet-not-accepted acknowledge control
+                                                         symbols.
+                                                         Partial Symbol in SRIOMAINT(0,2..3)_ERB_PACK_SYM_CAPT */
+	uint32_t out_ack                      : 1;  /**< Received packet with unexpected ackID value
+                                                         Header in SRIOMAINT(0,2..3)_ERB_PACK_SYM_CAPT */
+	uint32_t pkt_crc                      : 1;  /**< Received a packet with a bad CRC value
+                                                         Header in SRIOMAINT(0,2..3)_ERB_PACK_SYM_CAPT */
+	uint32_t size                         : 1;  /**< Received packet which exceeds the maximum allowed
+                                                         size of 276 bytes.
+                                                         Header in SRIOMAINT(0,2..3)_ERB_PACK_SYM_CAPT */
+	uint32_t inv_char                     : 1;  /**< Received illegal, 8B/10B error  or undefined
+                                                         codegroup within a packet.
+                                                         Header in SRIOMAINT(0,2..3)_ERB_PACK_SYM_CAPT */
+	uint32_t inv_data                     : 1;  /**< Received data codegroup or 8B/10B error within an
+                                                         IDLE sequence.
+                                                         Header in SRIOMAINT(0,2..3)_ERB_PACK_SYM_CAPT */
+	uint32_t reserved_6_14                : 9;
+	uint32_t bad_ack                      : 1;  /**< Link_response received with an ackID that is not
+                                                         outstanding.
+                                                         Partial Symbol in SRIOMAINT(0,2..3)_ERB_PACK_SYM_CAPT */
+	uint32_t proterr                      : 1;  /**< An unexpected packet or control symbol was
+                                                         received.
+                                                         Partial Symbol in SRIOMAINT(0,2..3)_ERB_PACK_SYM_CAPT */
+	uint32_t f_toggle                     : 1;  /**< Reserved. */
+	uint32_t del_err                      : 1;  /**< Received illegal or undefined codegroup.
+                                                         (either INV_DATA or INV_CHAR)
+                                                         Complete Symbol in SRIOMAINT(0,2..3)_ERB_PACK_SYM_CAPT */
+	uint32_t uns_ack                      : 1;  /**< An unexpected acknowledge control symbol was
+                                                         received.
+                                                         Partial Symbol in SRIOMAINT(0,2..3)_ERB_PACK_SYM_CAPT */
+	uint32_t lnk_tout                     : 1;  /**< An acknowledge or link-response control symbol is
+                                                         not received within the specified timeout interval
+                                                         Partial Header in SRIOMAINT(0,2..3)_ERB_PACK_SYM_CAPT */
+#else
+	uint32_t lnk_tout                     : 1;
+	uint32_t uns_ack                      : 1;
+	uint32_t del_err                      : 1;
+	uint32_t f_toggle                     : 1;
+	uint32_t proterr                      : 1;
+	uint32_t bad_ack                      : 1;
+	uint32_t reserved_6_14                : 9;
+	uint32_t inv_data                     : 1;
+	uint32_t inv_char                     : 1;
+	uint32_t size                         : 1;
+	uint32_t pkt_crc                      : 1;
+	uint32_t out_ack                      : 1;
+	uint32_t nack                         : 1;
+	uint32_t uns_id                       : 1;
+	uint32_t ctl_crc                      : 1;
+	uint32_t reserved_23_30               : 8;
+	uint32_t imp_err                      : 1;
+#endif
+	} s;
+	struct cvmx_sriomaintx_erb_err_det_s  cn63xx;
+	struct cvmx_sriomaintx_erb_err_det_cn63xxp1 {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t reserved_23_31               : 9;
+	uint32_t ctl_crc                      : 1;  /**< Received a control symbol with a bad CRC value
+                                                         Complete Symbol in SRIOMAINT(0..1)_ERB_PACK_SYM_CAPT */
+	uint32_t uns_id                       : 1;  /**< Received an acknowledge control symbol with an
+                                                         unexpected ackID (packet-accepted or packet_retry)
+                                                         Partial Symbol in SRIOMAINT(0..1)_ERB_PACK_SYM_CAPT */
+	uint32_t nack                         : 1;  /**< Received packet-not-accepted acknowledge control
+                                                         symbols.
+                                                         Partial Symbol in SRIOMAINT(0..1)_ERB_PACK_SYM_CAPT */
+	uint32_t out_ack                      : 1;  /**< Received packet with unexpected ackID value
+                                                         Header in SRIOMAINT(0..1)_ERB_PACK_SYM_CAPT */
+	uint32_t pkt_crc                      : 1;  /**< Received a packet with a bad CRC value
+                                                         Header in SRIOMAINT(0..1)_ERB_PACK_SYM_CAPT */
+	uint32_t size                         : 1;  /**< Received packet which exceeds the maximum allowed
+                                                         size of 276 bytes.
+                                                         Header in SRIOMAINT(0..1)_ERB_PACK_SYM_CAPT */
+	uint32_t reserved_6_16                : 11;
+	uint32_t bad_ack                      : 1;  /**< Link_response received with an ackID that is not
+                                                         outstanding.
+                                                         Partial Symbol in SRIOMAINT(0..1)_ERB_PACK_SYM_CAPT */
+	uint32_t proterr                      : 1;  /**< An unexpected packet or control symbol was
+                                                         received.
+                                                         Partial Symbol in SRIOMAINT(0..1)_ERB_PACK_SYM_CAPT */
+	uint32_t f_toggle                     : 1;  /**< Reserved. */
+	uint32_t del_err                      : 1;  /**< Received illegal or undefined codegroup.
+                                                         (either INV_DATA or INV_CHAR) (Pass 2)
+                                                         Complete Symbol in SRIOMAINT(0..1)_ERB_PACK_SYM_CAPT */
+	uint32_t uns_ack                      : 1;  /**< An unexpected acknowledge control symbol was
+                                                         received.
+                                                         Partial Symbol in SRIOMAINT(0..1)_ERB_PACK_SYM_CAPT */
+	uint32_t lnk_tout                     : 1;  /**< An acknowledge or link-response control symbol is
+                                                         not received within the specified timeout interval
+                                                         Partial Header in SRIOMAINT(0..1)_ERB_PACK_SYM_CAPT */
+#else
+	uint32_t lnk_tout                     : 1;
+	uint32_t uns_ack                      : 1;
+	uint32_t del_err                      : 1;
+	uint32_t f_toggle                     : 1;
+	uint32_t proterr                      : 1;
+	uint32_t bad_ack                      : 1;
+	uint32_t reserved_6_16                : 11;
+	uint32_t size                         : 1;
+	uint32_t pkt_crc                      : 1;
+	uint32_t out_ack                      : 1;
+	uint32_t nack                         : 1;
+	uint32_t uns_id                       : 1;
+	uint32_t ctl_crc                      : 1;
+	uint32_t reserved_23_31               : 9;
+#endif
+	} cn63xxp1;
+	struct cvmx_sriomaintx_erb_err_det_s  cn66xx;
+};
+typedef union cvmx_sriomaintx_erb_err_det cvmx_sriomaintx_erb_err_det_t;
+
+/**
+ * cvmx_sriomaint#_erb_err_rate
+ *
+ * SRIOMAINT_ERB_ERR_RATE = SRIO Error Rate
+ *
+ * Error Rate
+ *
+ * Notes:
+ * The Error Rate register is used with the Error Rate Threshold register to monitor and control the
+ *  reporting of transmission errors.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_ERB_ERR_RATE  hclk    hrst_n
+ */
+union cvmx_sriomaintx_erb_err_rate {
+	uint32_t u32;
+	struct cvmx_sriomaintx_erb_err_rate_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t err_bias                     : 8;  /**< These bits provide the error rate bias value.
+                                                         0x00 - do not decrement the error rate counter
+                                                         0x01 - decrement every 1ms (+/-34%)
+                                                         0x02 - decrement every 10ms (+/-34%)
+                                                         0x04 - decrement every 100ms (+/-34%)
+                                                         0x08 - decrement every 1s (+/-34%)
+                                                         0x10 - decrement every 10s (+/-34%)
+                                                         0x20 - decrement every 100s (+/-34%)
+                                                         0x40 - decrement every 1000s (+/-34%)
+                                                         0x80 - decrement every 10000s (+/-34%)
+                                                         All other values are reserved */
+	uint32_t reserved_18_23               : 6;
+	uint32_t rate_lim                     : 2;  /**< These bits limit the incrementing of the error
+                                                         rate counter above the failed threshold trigger.
+                                                           00 - only count 2 errors above
+                                                           01 - only count 4 errors above
+                                                           10 - only count 16 error above
+                                                           11 - do not limit incrementing the error rate ct */
+	uint32_t pk_rate                      : 8;  /**< Peak Value attainted by the error rate counter */
+	uint32_t rate_cnt                     : 8;  /**< These bits maintain a count of the number of
+                                                         transmission errors that have been detected by the
+                                                         port, decremented by the Error Rate Bias
+                                                         mechanism, to create an indication of the link
+                                                         error rate. */
+#else
+	uint32_t rate_cnt                     : 8;
+	uint32_t pk_rate                      : 8;
+	uint32_t rate_lim                     : 2;
+	uint32_t reserved_18_23               : 6;
+	uint32_t err_bias                     : 8;
+#endif
+	} s;
+	struct cvmx_sriomaintx_erb_err_rate_s cn63xx;
+	struct cvmx_sriomaintx_erb_err_rate_s cn63xxp1;
+	struct cvmx_sriomaintx_erb_err_rate_s cn66xx;
+};
+typedef union cvmx_sriomaintx_erb_err_rate cvmx_sriomaintx_erb_err_rate_t;
+
+/**
+ * cvmx_sriomaint#_erb_err_rate_en
+ *
+ * SRIOMAINT_ERB_ERR_RATE_EN = SRIO Error Rate Enable
+ *
+ * Error Rate Enable
+ *
+ * Notes:
+ * This register contains the bits that control when an error condition is allowed to increment the
+ *  error rate counter in the Error Rate Threshold Register and lock the Error Capture registers.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_ERB_ERR_RATE_EN       hclk    hrst_n
+ */
+union cvmx_sriomaintx_erb_err_rate_en {
+	uint32_t u32;
+	struct cvmx_sriomaintx_erb_err_rate_en_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t imp_err                      : 1;  /**< Enable Implementation Specific Error. */
+	uint32_t reserved_23_30               : 8;
+	uint32_t ctl_crc                      : 1;  /**< Enable error rate counting of control symbols with
+                                                         bad CRC values */
+	uint32_t uns_id                       : 1;  /**< Enable error rate counting of acknowledge control
+                                                         symbol with unexpected ackIDs
+                                                         (packet-accepted or packet_retry) */
+	uint32_t nack                         : 1;  /**< Enable error rate counting of packet-not-accepted
+                                                         acknowledge control symbols. */
+	uint32_t out_ack                      : 1;  /**< Enable error rate counting of received packet with
+                                                         unexpected ackID value */
+	uint32_t pkt_crc                      : 1;  /**< Enable error rate counting of received a packet
+                                                         with a bad CRC value */
+	uint32_t size                         : 1;  /**< Enable error rate counting of received packet
+                                                         which exceeds the maximum size of 276 bytes. */
+	uint32_t inv_char                     : 1;  /**< Enable error rate counting of received illegal
+                                                         illegal, 8B/10B error or undefined codegroup
+                                                         within a packet. */
+	uint32_t inv_data                     : 1;  /**< Enable error rate counting of received data
+                                                         codegroup or 8B/10B error within IDLE sequence. */
+	uint32_t reserved_6_14                : 9;
+	uint32_t bad_ack                      : 1;  /**< Enable error rate counting of link_responses with
+                                                         an ackID that is not outstanding. */
+	uint32_t proterr                      : 1;  /**< Enable error rate counting of unexpected packet or
+                                                         control symbols received. */
+	uint32_t f_toggle                     : 1;  /**< Reserved. */
+	uint32_t del_err                      : 1;  /**< Enable error rate counting of illegal or undefined
+                                                         codegroups (either INV_DATA or INV_CHAR). */
+	uint32_t uns_ack                      : 1;  /**< Enable error rate counting of unexpected
+                                                         acknowledge control symbols received. */
+	uint32_t lnk_tout                     : 1;  /**< Enable error rate counting of acknowledge or
+                                                         link-response control symbols not received within
+                                                         the specified timeout interval */
+#else
+	uint32_t lnk_tout                     : 1;
+	uint32_t uns_ack                      : 1;
+	uint32_t del_err                      : 1;
+	uint32_t f_toggle                     : 1;
+	uint32_t proterr                      : 1;
+	uint32_t bad_ack                      : 1;
+	uint32_t reserved_6_14                : 9;
+	uint32_t inv_data                     : 1;
+	uint32_t inv_char                     : 1;
+	uint32_t size                         : 1;
+	uint32_t pkt_crc                      : 1;
+	uint32_t out_ack                      : 1;
+	uint32_t nack                         : 1;
+	uint32_t uns_id                       : 1;
+	uint32_t ctl_crc                      : 1;
+	uint32_t reserved_23_30               : 8;
+	uint32_t imp_err                      : 1;
+#endif
+	} s;
+	struct cvmx_sriomaintx_erb_err_rate_en_s cn63xx;
+	struct cvmx_sriomaintx_erb_err_rate_en_cn63xxp1 {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t reserved_23_31               : 9;
+	uint32_t ctl_crc                      : 1;  /**< Enable error rate counting of control symbols with
+                                                         bad CRC values */
+	uint32_t uns_id                       : 1;  /**< Enable error rate counting of acknowledge control
+                                                         symbol with unexpected ackIDs
+                                                         (packet-accepted or packet_retry) */
+	uint32_t nack                         : 1;  /**< Enable error rate counting of packet-not-accepted
+                                                         acknowledge control symbols. */
+	uint32_t out_ack                      : 1;  /**< Enable error rate counting of received packet with
+                                                         unexpected ackID value */
+	uint32_t pkt_crc                      : 1;  /**< Enable error rate counting of received a packet
+                                                         with a bad CRC value */
+	uint32_t size                         : 1;  /**< Enable error rate counting of received packet
+                                                         which exceeds the maximum size of 276 bytes. */
+	uint32_t reserved_6_16                : 11;
+	uint32_t bad_ack                      : 1;  /**< Enable error rate counting of link_responses with
+                                                         an ackID that is not outstanding. */
+	uint32_t proterr                      : 1;  /**< Enable error rate counting of unexpected packet or
+                                                         control symbols received. */
+	uint32_t f_toggle                     : 1;  /**< Reserved. */
+	uint32_t del_err                      : 1;  /**< Enable error rate counting of illegal or undefined
+                                                         codegroups (either INV_DATA or INV_CHAR). (Pass 2) */
+	uint32_t uns_ack                      : 1;  /**< Enable error rate counting of unexpected
+                                                         acknowledge control symbols received. */
+	uint32_t lnk_tout                     : 1;  /**< Enable error rate counting of acknowledge or
+                                                         link-response control symbols not received within
+                                                         the specified timeout interval */
+#else
+	uint32_t lnk_tout                     : 1;
+	uint32_t uns_ack                      : 1;
+	uint32_t del_err                      : 1;
+	uint32_t f_toggle                     : 1;
+	uint32_t proterr                      : 1;
+	uint32_t bad_ack                      : 1;
+	uint32_t reserved_6_16                : 11;
+	uint32_t size                         : 1;
+	uint32_t pkt_crc                      : 1;
+	uint32_t out_ack                      : 1;
+	uint32_t nack                         : 1;
+	uint32_t uns_id                       : 1;
+	uint32_t ctl_crc                      : 1;
+	uint32_t reserved_23_31               : 9;
+#endif
+	} cn63xxp1;
+	struct cvmx_sriomaintx_erb_err_rate_en_s cn66xx;
+};
+typedef union cvmx_sriomaintx_erb_err_rate_en cvmx_sriomaintx_erb_err_rate_en_t;
+
+/**
+ * cvmx_sriomaint#_erb_err_rate_thr
+ *
+ * SRIOMAINT_ERB_ERR_RATE_THR = SRIO Error Rate Threshold
+ *
+ * Error Rate Threshold
+ *
+ * Notes:
+ * The Error Rate Threshold register is used to control the reporting of errors to the link status.
+ *  Typically the Degraded Threshold is less than the Fail Threshold.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_ERB_ERR_RATE_THR      hclk    hrst_n
+ */
+union cvmx_sriomaintx_erb_err_rate_thr {
+	uint32_t u32;
+	struct cvmx_sriomaintx_erb_err_rate_thr_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t fail_th                      : 8;  /**< These bits provide the threshold value for
+                                                         reporting an error condition due to a possibly
+                                                         broken link.
+                                                           0x00 - Disable the Error Rate Failed Threshold
+                                                                  Trigger
+                                                           0x01 - Set the error reporting threshold to 1
+                                                           0x02 - Set the error reporting threshold to 2
+                                                           - ...
+                                                           0xFF - Set the error reporting threshold to 255 */
+	uint32_t dgrad_th                     : 8;  /**< These bits provide the threshold value for
+                                                         reporting an error condition due to a possibly
+                                                         degrading link.
+                                                           0x00 - Disable the Degrade Rate Failed Threshold
+                                                                  Trigger
+                                                           0x01 - Set the error reporting threshold to 1
+                                                           0x02 - Set the error reporting threshold to 2
+                                                           - ...
+                                                           0xFF - Set the error reporting threshold to 255 */
+	uint32_t reserved_0_15                : 16;
+#else
+	uint32_t reserved_0_15                : 16;
+	uint32_t dgrad_th                     : 8;
+	uint32_t fail_th                      : 8;
+#endif
+	} s;
+	struct cvmx_sriomaintx_erb_err_rate_thr_s cn63xx;
+	struct cvmx_sriomaintx_erb_err_rate_thr_s cn63xxp1;
+	struct cvmx_sriomaintx_erb_err_rate_thr_s cn66xx;
+};
+typedef union cvmx_sriomaintx_erb_err_rate_thr cvmx_sriomaintx_erb_err_rate_thr_t;
+
+/**
+ * cvmx_sriomaint#_erb_hdr
+ *
+ * SRIOMAINT_ERB_HDR = SRIO Error Reporting Block Header
+ *
+ * Error Reporting Block Header
+ *
+ * Notes:
+ * The error management extensions block header register contains the EF_PTR to the next EF_BLK and
+ *  the EF_ID that identifies this as the error management extensions block header. In this
+ *  implementation this is the last block and therefore the EF_PTR is a NULL pointer.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_ERB_HDR       hclk    hrst_n
+ */
+union cvmx_sriomaintx_erb_hdr {
+	uint32_t u32;
+	struct cvmx_sriomaintx_erb_hdr_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t ef_ptr                       : 16; /**< Pointer to the next block in the extended features
+                                                         data structure. */
+	uint32_t ef_id                        : 16; /**< Single Port ID */
+#else
+	uint32_t ef_id                        : 16;
+	uint32_t ef_ptr                       : 16;
+#endif
+	} s;
+	struct cvmx_sriomaintx_erb_hdr_s      cn63xx;
+	struct cvmx_sriomaintx_erb_hdr_s      cn63xxp1;
+	struct cvmx_sriomaintx_erb_hdr_s      cn66xx;
+};
+typedef union cvmx_sriomaintx_erb_hdr cvmx_sriomaintx_erb_hdr_t;
+
+/**
+ * cvmx_sriomaint#_erb_lt_addr_capt_h
+ *
+ * SRIOMAINT_ERB_LT_ADDR_CAPT_H = SRIO Logical/Transport Layer High Address Capture
+ *
+ * Logical/Transport Layer High Address Capture
+ *
+ * Notes:
+ * This register contains error information. It is locked when a Logical/Transport error is detected
+ *  and unlocked when the SRIOMAINT(0,2..3)_ERB_LT_ERR_DET is written to zero. This register should be
+ *  written only when error detection is disabled.  This register is only required for end point
+ *  transactions of 50 or 66 bits.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_ERB_LT_ADDR_CAPT_H    hclk    hrst_n
+ */
+union cvmx_sriomaintx_erb_lt_addr_capt_h {
+	uint32_t u32;
+	struct cvmx_sriomaintx_erb_lt_addr_capt_h_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t addr                         : 32; /**< Most significant 32 bits of the address associated
+                                                         with the error. Information supplied for requests
+                                                         and responses if available. */
+#else
+	uint32_t addr                         : 32;
+#endif
+	} s;
+	struct cvmx_sriomaintx_erb_lt_addr_capt_h_s cn63xx;
+	struct cvmx_sriomaintx_erb_lt_addr_capt_h_s cn63xxp1;
+	struct cvmx_sriomaintx_erb_lt_addr_capt_h_s cn66xx;
+};
+typedef union cvmx_sriomaintx_erb_lt_addr_capt_h cvmx_sriomaintx_erb_lt_addr_capt_h_t;
+
+/**
+ * cvmx_sriomaint#_erb_lt_addr_capt_l
+ *
+ * SRIOMAINT_ERB_LT_ADDR_CAPT_L = SRIO Logical/Transport Layer Low Address Capture
+ *
+ * Logical/Transport Layer Low Address Capture
+ *
+ * Notes:
+ * This register contains error information. It is locked when a Logical/Transport error is detected
+ *  and unlocked when the SRIOMAINT(0,2..3)_ERB_LT_ERR_DET is written to zero.  This register should be
+ *  written only when error detection is disabled.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_ERB_LT_ADDR_CAPT_L    hclk    hrst_n
+ */
+union cvmx_sriomaintx_erb_lt_addr_capt_l {
+	uint32_t u32;
+	struct cvmx_sriomaintx_erb_lt_addr_capt_l_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t addr                         : 29; /**< Least significant 29 bits of the address
+                                                         associated with the error.  Bits 31:24 specify the
+                                                         request HOP count for Maintenance Operations.
+                                                         Information supplied for requests and responses if
+                                                         available. */
+	uint32_t reserved_2_2                 : 1;
+	uint32_t xaddr                        : 2;  /**< Extended address bits of the address associated
+                                                         with the error.  Information supplied for requests
+                                                         and responses if available. */
+#else
+	uint32_t xaddr                        : 2;
+	uint32_t reserved_2_2                 : 1;
+	uint32_t addr                         : 29;
+#endif
+	} s;
+	struct cvmx_sriomaintx_erb_lt_addr_capt_l_s cn63xx;
+	struct cvmx_sriomaintx_erb_lt_addr_capt_l_s cn63xxp1;
+	struct cvmx_sriomaintx_erb_lt_addr_capt_l_s cn66xx;
+};
+typedef union cvmx_sriomaintx_erb_lt_addr_capt_l cvmx_sriomaintx_erb_lt_addr_capt_l_t;
+
+/**
+ * cvmx_sriomaint#_erb_lt_ctrl_capt
+ *
+ * SRIOMAINT_ERB_LT_CTRL_CAPT = SRIO Logical/Transport Layer Control Capture
+ *
+ * Logical/Transport Layer Control Capture
+ *
+ * Notes:
+ * This register contains error information. It is locked when a Logical/Transport error is detected
+ *  and unlocked when the SRIOMAINT(0,2..3)_ERB_LT_ERR_DET is written to zero.  This register should be
+ *  written only when error detection is disabled.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_ERB_LT_CTRL_CAPT      hclk    hrst_n
+ */
+union cvmx_sriomaintx_erb_lt_ctrl_capt {
+	uint32_t u32;
+	struct cvmx_sriomaintx_erb_lt_ctrl_capt_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t ftype                        : 4;  /**< Format Type associated with the error */
+	uint32_t ttype                        : 4;  /**< Transaction Type associated with the error
+                                                         (For Messages)
+                                                         Message Length */
+	uint32_t extra                        : 8;  /**< Additional Information
+                                                         (For Messages)
+                                                         - 23:22 Letter
+                                                         - 21:20 Mbox
+                                                         - 19:16 Msgseg/xmbox
+                                                         Information for the last message request sent
+                                                         for the mailbox that had an error
+                                                         (For Responses)
+                                                         - 23:20 Response Request FTYPE
+                                                         - 19:16 Response Request TTYPE
+                                                         (For all other types)
+                                                         Reserved. */
+	uint32_t status                       : 4;  /**< Response Status.
+                                                         (For all other Requests)
+                                                         Reserved. */
+	uint32_t size                         : 4;  /**< Size associated with the transaction. */
+	uint32_t tt                           : 1;  /**< Transfer Type 0=ID8, 1=ID16. */
+	uint32_t wdptr                        : 1;  /**< Word Pointer associated with the error. */
+	uint32_t reserved_5_5                 : 1;
+	uint32_t capt_idx                     : 5;  /**< Capture Index. 31 - Bit set in
+                                                         SRIOMAINT(0,2..3)_ERB_LT_ERR_DET. */
+#else
+	uint32_t capt_idx                     : 5;
+	uint32_t reserved_5_5                 : 1;
+	uint32_t wdptr                        : 1;
+	uint32_t tt                           : 1;
+	uint32_t size                         : 4;
+	uint32_t status                       : 4;
+	uint32_t extra                        : 8;
+	uint32_t ttype                        : 4;
+	uint32_t ftype                        : 4;
+#endif
+	} s;
+	struct cvmx_sriomaintx_erb_lt_ctrl_capt_s cn63xx;
+	struct cvmx_sriomaintx_erb_lt_ctrl_capt_s cn63xxp1;
+	struct cvmx_sriomaintx_erb_lt_ctrl_capt_s cn66xx;
+};
+typedef union cvmx_sriomaintx_erb_lt_ctrl_capt cvmx_sriomaintx_erb_lt_ctrl_capt_t;
+
+/**
+ * cvmx_sriomaint#_erb_lt_dev_id
+ *
+ * SRIOMAINT_ERB_LT_DEV_ID = SRIO Port-write Target deviceID
+ *
+ * Port-write Target deviceID
+ *
+ * Notes:
+ * This SRIO interface does not support generating Port-Writes based on ERB Errors.  This register is
+ *  currently unused and should be treated as reserved.
+ *
+ * Clk_Rst:        SRIOMAINT_ERB_LT_DEV_ID hclk    hrst_n
+ */
+union cvmx_sriomaintx_erb_lt_dev_id {
+	uint32_t u32;
+	struct cvmx_sriomaintx_erb_lt_dev_id_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t id16                         : 8;  /**< This is the most significant byte of the
+                                                         port-write destination deviceID (large transport
+                                                         systems only)
+                                                         destination ID used for Port Write errors */
+	uint32_t id8                          : 8;  /**< This is the port-write destination deviceID */
+	uint32_t tt                           : 1;  /**< Transport Type used for Port Write
+                                                         0 = Small Transport, ID8 Only
+                                                         1 = Large Transport, ID16 and ID8 */
+	uint32_t reserved_0_14                : 15;
+#else
+	uint32_t reserved_0_14                : 15;
+	uint32_t tt                           : 1;
+	uint32_t id8                          : 8;
+	uint32_t id16                         : 8;
+#endif
+	} s;
+	struct cvmx_sriomaintx_erb_lt_dev_id_s cn63xx;
+	struct cvmx_sriomaintx_erb_lt_dev_id_s cn63xxp1;
+	struct cvmx_sriomaintx_erb_lt_dev_id_s cn66xx;
+};
+typedef union cvmx_sriomaintx_erb_lt_dev_id cvmx_sriomaintx_erb_lt_dev_id_t;
+
+/**
+ * cvmx_sriomaint#_erb_lt_dev_id_capt
+ *
+ * SRIOMAINT_ERB_LT_DEV_ID_CAPT = SRIO Logical/Transport Layer Device ID Capture
+ *
+ * Logical/Transport Layer Device ID Capture
+ *
+ * Notes:
+ * This register contains error information. It is locked when a Logical/Transport error is detected
+ *  and unlocked when the SRIOMAINT(0,2..3)_ERB_LT_ERR_DET is written to zero.  This register should be
+ *  written only when error detection is disabled.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_ERB_LT_DEV_ID_CAPT    hclk    hrst_n
+ */
+union cvmx_sriomaintx_erb_lt_dev_id_capt {
+	uint32_t u32;
+	struct cvmx_sriomaintx_erb_lt_dev_id_capt_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t dst_id16                     : 8;  /**< Most significant byte of the large transport
+                                                         destination ID associated with the error */
+	uint32_t dst_id8                      : 8;  /**< Least significant byte of the large transport
+                                                         destination ID or the 8-bit small transport
+                                                         destination ID associated with the error */
+	uint32_t src_id16                     : 8;  /**< Most significant byte of the large transport
+                                                         source ID associated with the error */
+	uint32_t src_id8                      : 8;  /**< Least significant byte of the large transport
+                                                         source ID or the 8-bit small transport source ID
+                                                         associated with the error */
+#else
+	uint32_t src_id8                      : 8;
+	uint32_t src_id16                     : 8;
+	uint32_t dst_id8                      : 8;
+	uint32_t dst_id16                     : 8;
+#endif
+	} s;
+	struct cvmx_sriomaintx_erb_lt_dev_id_capt_s cn63xx;
+	struct cvmx_sriomaintx_erb_lt_dev_id_capt_s cn63xxp1;
+	struct cvmx_sriomaintx_erb_lt_dev_id_capt_s cn66xx;
+};
+typedef union cvmx_sriomaintx_erb_lt_dev_id_capt cvmx_sriomaintx_erb_lt_dev_id_capt_t;
+
+/**
+ * cvmx_sriomaint#_erb_lt_err_det
+ *
+ * SRIOMAINT_ERB_LT_ERR_DET = SRIO Logical/Transport Layer Error Detect
+ *
+ * SRIO Logical/Transport Layer Error Detect
+ *
+ * Notes:
+ * This register indicates the error that was detected by the Logical or Transport logic layer.
+ *  Once a bit is set in this CSR, HW will lock the register until SW writes a zero to clear all the
+ *  fields.  The HW sets SRIO_INT_REG[LOG_ERB] every time it sets one of the bits.
+ *  To handle the interrupt, the following procedure may be best:
+ *       (1) clear SRIO_INT_REG[LOG_ERB],
+ *       (2) read this CSR, corresponding SRIOMAINT*_ERB_LT_ADDR_CAPT_H, SRIOMAINT*_ERB_LT_ADDR_CAPT_L,
+ *           SRIOMAINT*_ERB_LT_DEV_ID_CAPT, and SRIOMAINT*_ERB_LT_CTRL_CAPT
+ *       (3) Write this CSR to 0.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_ERB_LT_ERR_DET        hclk    hrst_n
+ */
+union cvmx_sriomaintx_erb_lt_err_det {
+	uint32_t u32;
+	struct cvmx_sriomaintx_erb_lt_err_det_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t io_err                       : 1;  /**< Received a response of ERROR for an IO Logical
+                                                         Layer Request.  This includes all Maintenance and
+                                                         Memory Responses not destined for the RX Soft
+                                                         Packet FIFO. When SRIO receives an ERROR response
+                                                         for a read, the issuing core or DPI DMA engine
+                                                         receives result bytes with all bits set. In the
+                                                         case of writes with response, this bit is the only
+                                                         indication of failure. */
+	uint32_t msg_err                      : 1;  /**< Received a response of ERROR for an outgoing
+                                                         message segment. This bit is the only direct
+                                                         indication of a MSG_ERR. When a MSG_ERR occurs,
+                                                         SRIO drops the message segment and will not set
+                                                         SRIO*_INT_REG[OMSG*] after the message
+                                                         "transfer". NOTE: SRIO can continue to send or
+                                                         retry other segments from the same message after
+                                                         a MSG_ERR. */
+	uint32_t gsm_err                      : 1;  /**< Received a response of ERROR for an GSM Logical
+                                                         Request.  SRIO hardware never sets this bit. GSM
+                                                         operations are not supported (outside of the Soft
+                                                         Packet FIFO). */
+	uint32_t msg_fmt                      : 1;  /**< Received an incoming Message Segment with a
+                                                         formating error.  A MSG_FMT error occurs when SRIO
+                                                         receives a message segment with a reserved SSIZE,
+                                                         or a illegal data payload size, or a MSGSEG greater
+                                                         than MSGLEN, or a MSGSEG that is the duplicate of
+                                                         one already received by an inflight message.
+                                                         When a non-duplicate MSG_FMT error occurs, SRIO
+                                                         drops the segment and sends an ERROR response.
+                                                         When a duplicate MSG_FMT error occurs, SRIO
+                                                         (internally) terminates the currently-inflight
+                                                         message with an error and processes the duplicate,
+                                                         which may result in a new message being generated
+                                                         internally for the duplicate. */
+	uint32_t ill_tran                     : 1;  /**< Received illegal fields in the request/response
+                                                         packet for a supported transaction or any packet
+                                                         with a reserved transaction type. When an ILL_TRAN
+                                                         error occurs, SRIO ignores the packet. ILL_TRAN
+                                                         errors are 2nd priority after ILL_TGT and may mask
+                                                         other problems. Packets with ILL_TRAN errors cannot
+                                                         enter the RX Soft Packet FIFO.
+                                                         There are two things that can set ILL_TRAN:
+                                                         (1) SRIO received a packet with a tt value is not
+                                                         0 or 1, or (2) SRIO received a response to an
+                                                         outstanding message segment whose status was not
+                                                         DONE, RETRY, or ERROR. */
+	uint32_t ill_tgt                      : 1;  /**< Received a packet that contained a destination ID
+                                                         other than SRIOMAINT*_PRI_DEV_ID or
+                                                         SRIOMAINT*_SEC_DEV_ID. When an ILL_TGT error
+                                                         occurs, SRIO drops the packet. ILL_TGT errors are
+                                                         highest priority, so may mask other problems.
+                                                         Packets with ILL_TGT errors cannot enter the RX
+                                                         soft packet fifo. */
+	uint32_t msg_tout                     : 1;  /**< An expected incoming message request has not been
+                                                         received within the time-out interval specified in
+                                                         SRIOMAINT(0,2..3)_PORT_RT_CTL. When a MSG_TOUT occurs,
+                                                         SRIO (internally) terminates the inflight message
+                                                         with an error. */
+	uint32_t pkt_tout                     : 1;  /**< A required response has not been received to an
+                                                         outgoing memory, maintenance or message request
+                                                         before the time-out interval specified in
+                                                         SRIOMAINT(0,2..3)_PORT_RT_CTL.  When an IO or maintenance
+                                                         read request operation has a PKT_TOUT, the issuing
+                                                         core load or DPI DMA engine receive all ones for
+                                                         the result. When an IO NWRITE_R has a PKT_TOUT,
+                                                         this bit is the only indication of failure. When a
+                                                         message request operation has a PKT_TOUT, SRIO
+                                                         discards the the outgoing message segment,  and
+                                                         this bit is the only direct indication of failure.
+                                                         NOTE: SRIO may continue to send or retry other
+                                                         segments from the same message. When one or more of
+                                                         the segments in an outgoing message have a
+                                                         PKT_TOUT, SRIO will not set SRIO*_INT_REG[OMSG*]
+                                                         after the message "transfer". */
+	uint32_t uns_resp                     : 1;  /**< An unsolicited/unexpected memory, maintenance or
+                                                         message response packet was received that was not
+                                                         destined for the RX Soft Packet FIFO.  When this
+                                                         condition is detected, the packet is dropped. */
+	uint32_t uns_tran                     : 1;  /**< A transaction is received that is not supported.
+                                                         SRIO HW will never set this bit - SRIO routes all
+                                                         unsupported transactions to the RX soft packet
+                                                         FIFO. */
+	uint32_t reserved_1_21                : 21;
+	uint32_t resp_sz                      : 1;  /**< Received an incoming Memory or Maintenance
+                                                         Read response packet with a DONE status and less
+                                                         data then expected.  This condition causes the
+                                                         Read to be completed and an error response to be
+                                                         returned with all the data bits set to the issuing
+                                                         Core or DMA Engine. */
+#else
+	uint32_t resp_sz                      : 1;
+	uint32_t reserved_1_21                : 21;
+	uint32_t uns_tran                     : 1;
+	uint32_t uns_resp                     : 1;
+	uint32_t pkt_tout                     : 1;
+	uint32_t msg_tout                     : 1;
+	uint32_t ill_tgt                      : 1;
+	uint32_t ill_tran                     : 1;
+	uint32_t msg_fmt                      : 1;
+	uint32_t gsm_err                      : 1;
+	uint32_t msg_err                      : 1;
+	uint32_t io_err                       : 1;
+#endif
+	} s;
+	struct cvmx_sriomaintx_erb_lt_err_det_s cn63xx;
+	struct cvmx_sriomaintx_erb_lt_err_det_s cn63xxp1;
+	struct cvmx_sriomaintx_erb_lt_err_det_s cn66xx;
+};
+typedef union cvmx_sriomaintx_erb_lt_err_det cvmx_sriomaintx_erb_lt_err_det_t;
+
+/**
+ * cvmx_sriomaint#_erb_lt_err_en
+ *
+ * SRIOMAINT_ERB_LT_ERR_EN = SRIO Logical/Transport Layer Error Enable
+ *
+ * SRIO Logical/Transport Layer Error Enable
+ *
+ * Notes:
+ * This register contains the bits that control if an error condition locks the Logical/Transport
+ *  Layer Error Detect and Capture registers and is reported to the system host.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_ERB_LT_ERR_EN hclk    hrst_n
+ */
+union cvmx_sriomaintx_erb_lt_err_en {
+	uint32_t u32;
+	struct cvmx_sriomaintx_erb_lt_err_en_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t io_err                       : 1;  /**< Enable reporting of an IO error response. Save and
+                                                         lock original request transaction information in
+                                                         all Logical/Transport Layer Capture CSRs. */
+	uint32_t msg_err                      : 1;  /**< Enable reporting of a Message error response. Save
+                                                         and lock original request transaction information
+                                                         in all Logical/Transport Layer Capture CSRs. */
+	uint32_t gsm_err                      : 1;  /**< Enable reporting of a GSM error response. Save and
+                                                         lock original request transaction capture
+                                                         information in all Logical/Transport Layer Capture
+                                                         CSRs. */
+	uint32_t msg_fmt                      : 1;  /**< Enable reporting of a message format error. Save
+                                                         and lock transaction capture information in
+                                                         Logical/Transport Layer Device ID and Control
+                                                         Capture CSRs. */
+	uint32_t ill_tran                     : 1;  /**< Enable reporting of an illegal transaction decode
+                                                         error Save and lock transaction capture
+                                                         information in Logical/Transport Layer Device ID
+                                                         and Control Capture CSRs. */
+	uint32_t ill_tgt                      : 1;  /**< Enable reporting of an illegal transaction target
+                                                         error. Save and lock transaction capture
+                                                         information in Logical/Transport Layer Device ID
+                                                         and Control Capture CSRs. */
+	uint32_t msg_tout                     : 1;  /**< Enable reporting of a Message Request time-out
+                                                         error. Save and lock transaction capture
+                                                         information in Logical/Transport Layer Device ID
+                                                         and Control Capture CSRs for the last Message
+                                                         request segment packet received. */
+	uint32_t pkt_tout                     : 1;  /**< Enable reporting of a packet response time-out
+                                                         error.  Save and lock original request address in
+                                                         Logical/Transport Layer Address Capture CSRs.
+                                                         Save and lock original request Destination ID in
+                                                         Logical/Transport Layer Device ID Capture CSR. */
+	uint32_t uns_resp                     : 1;  /**< Enable reporting of an unsolicited response error.
+                                                         Save and lock transaction capture information in
+                                                         Logical/Transport Layer Device ID and Control
+                                                         Capture CSRs. */
+	uint32_t uns_tran                     : 1;  /**< Enable reporting of an unsupported transaction
+                                                         error.  Save and lock transaction capture
+                                                         information in Logical/Transport Layer Device ID
+                                                         and Control Capture CSRs. */
+	uint32_t reserved_1_21                : 21;
+	uint32_t resp_sz                      : 1;  /**< Enable reporting of an incoming response with
+                                                         unexpected data size */
+#else
+	uint32_t resp_sz                      : 1;
+	uint32_t reserved_1_21                : 21;
+	uint32_t uns_tran                     : 1;
+	uint32_t uns_resp                     : 1;
+	uint32_t pkt_tout                     : 1;
+	uint32_t msg_tout                     : 1;
+	uint32_t ill_tgt                      : 1;
+	uint32_t ill_tran                     : 1;
+	uint32_t msg_fmt                      : 1;
+	uint32_t gsm_err                      : 1;
+	uint32_t msg_err                      : 1;
+	uint32_t io_err                       : 1;
+#endif
+	} s;
+	struct cvmx_sriomaintx_erb_lt_err_en_s cn63xx;
+	struct cvmx_sriomaintx_erb_lt_err_en_s cn63xxp1;
+	struct cvmx_sriomaintx_erb_lt_err_en_s cn66xx;
+};
+typedef union cvmx_sriomaintx_erb_lt_err_en cvmx_sriomaintx_erb_lt_err_en_t;
+
+/**
+ * cvmx_sriomaint#_erb_pack_capt_1
+ *
+ * SRIOMAINT_ERB_PACK_CAPT_1 = SRIO Packet Capture 1
+ *
+ * Packet Capture 1
+ *
+ * Notes:
+ * Error capture register 1 contains either long symbol capture information or bytes 4 through 7 of
+ *  the packet header.
+ *  The HW will not update this register (i.e. this register is locked) while
+ *  SRIOMAINT*_ERB_ATTR_CAPT[VALID] is set.  This register should only be read while this bit is set.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_ERB_PACK_CAPT_1       hclk    hrst_n
+ */
+union cvmx_sriomaintx_erb_pack_capt_1 {
+	uint32_t u32;
+	struct cvmx_sriomaintx_erb_pack_capt_1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t capture                      : 32; /**< Bytes 4 thru 7 of the packet header. */
+#else
+	uint32_t capture                      : 32;
+#endif
+	} s;
+	struct cvmx_sriomaintx_erb_pack_capt_1_s cn63xx;
+	struct cvmx_sriomaintx_erb_pack_capt_1_s cn63xxp1;
+	struct cvmx_sriomaintx_erb_pack_capt_1_s cn66xx;
+};
+typedef union cvmx_sriomaintx_erb_pack_capt_1 cvmx_sriomaintx_erb_pack_capt_1_t;
+
+/**
+ * cvmx_sriomaint#_erb_pack_capt_2
+ *
+ * SRIOMAINT_ERB_PACK_CAPT_2 = SRIO Packet Capture 2
+ *
+ * Packet Capture 2
+ *
+ * Notes:
+ * Error capture register 2 contains bytes 8 through 11 of the packet header.
+ *  The HW will not update this register (i.e. this register is locked) while
+ *  SRIOMAINT*_ERB_ATTR_CAPT[VALID] is set.  This register should only be read while this bit is set.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_ERB_PACK_CAPT_2       hclk    hrst_n
+ */
+union cvmx_sriomaintx_erb_pack_capt_2 {
+	uint32_t u32;
+	struct cvmx_sriomaintx_erb_pack_capt_2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t capture                      : 32; /**< Bytes 8 thru 11 of the packet header. */
+#else
+	uint32_t capture                      : 32;
+#endif
+	} s;
+	struct cvmx_sriomaintx_erb_pack_capt_2_s cn63xx;
+	struct cvmx_sriomaintx_erb_pack_capt_2_s cn63xxp1;
+	struct cvmx_sriomaintx_erb_pack_capt_2_s cn66xx;
+};
+typedef union cvmx_sriomaintx_erb_pack_capt_2 cvmx_sriomaintx_erb_pack_capt_2_t;
+
+/**
+ * cvmx_sriomaint#_erb_pack_capt_3
+ *
+ * SRIOMAINT_ERB_PACK_CAPT_3 = SRIO Packet Capture 3
+ *
+ * Packet Capture 3
+ *
+ * Notes:
+ * Error capture register 3 contains bytes 12 through 15 of the packet header.
+ *  The HW will not update this register (i.e. this register is locked) while
+ *  SRIOMAINT*_ERB_ATTR_CAPT[VALID] is set.  This register should only be read while this bit is set.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_ERB_PACK_CAPT_3       hclk    hrst_n
+ */
+union cvmx_sriomaintx_erb_pack_capt_3 {
+	uint32_t u32;
+	struct cvmx_sriomaintx_erb_pack_capt_3_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t capture                      : 32; /**< Bytes 12 thru 15 of the packet header. */
+#else
+	uint32_t capture                      : 32;
+#endif
+	} s;
+	struct cvmx_sriomaintx_erb_pack_capt_3_s cn63xx;
+	struct cvmx_sriomaintx_erb_pack_capt_3_s cn63xxp1;
+	struct cvmx_sriomaintx_erb_pack_capt_3_s cn66xx;
+};
+typedef union cvmx_sriomaintx_erb_pack_capt_3 cvmx_sriomaintx_erb_pack_capt_3_t;
+
+/**
+ * cvmx_sriomaint#_erb_pack_sym_capt
+ *
+ * SRIOMAINT_ERB_PACK_SYM_CAPT = SRIO Packet/Control Symbol Capture
+ *
+ * Packet/Control Symbol Capture
+ *
+ * Notes:
+ * This register contains either captured control symbol information or the first 4 bytes of captured
+ *  packet information.  The Errors that generate Partial Control Symbols can be found in
+ *  SRIOMAINT*_ERB_ERR_DET.  The HW will not update this register (i.e. this register is locked) while
+ *  SRIOMAINT*_ERB_ATTR_CAPT[VALID] is set.  This register should only be read while this bit is set.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_ERB_PACK_SYM_CAPT     hclk    hrst_n
+ */
+union cvmx_sriomaintx_erb_pack_sym_capt {
+	uint32_t u32;
+	struct cvmx_sriomaintx_erb_pack_sym_capt_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t capture                      : 32; /**< Control Character and Control Symbol or Bytes 0 to
+                                                         3 of Packet Header
+                                                         The Control Symbol consists of
+                                                           - 31:24 - SC Character (0 in Partial Symbol)
+                                                           - 23:21 - Stype 0
+                                                           - 20:16 - Parameter 0
+                                                           - 15:11 - Parameter 1
+                                                           - 10: 8 - Stype 1 (0 in Partial Symbol)
+                                                           - 7: 5 - Command (0 in Partial Symbol)
+                                                           - 4: 0 - CRC5    (0 in Partial Symbol) */
+#else
+	uint32_t capture                      : 32;
+#endif
+	} s;
+	struct cvmx_sriomaintx_erb_pack_sym_capt_s cn63xx;
+	struct cvmx_sriomaintx_erb_pack_sym_capt_s cn63xxp1;
+	struct cvmx_sriomaintx_erb_pack_sym_capt_s cn66xx;
+};
+typedef union cvmx_sriomaintx_erb_pack_sym_capt cvmx_sriomaintx_erb_pack_sym_capt_t;
+
+/**
+ * cvmx_sriomaint#_hb_dev_id_lock
+ *
+ * SRIOMAINT_HB_DEV_ID_LOCK = SRIO Host Device ID Lock
+ *
+ * The Host Base Device ID
+ *
+ * Notes:
+ * This register contains the Device ID of the Host responsible for initializing this SRIO device.
+ *  The register contains a special write once function that captures the first HOSTID written to it
+ *  after reset.  The function allows several potential hosts to write to this register and then read
+ *  it to see if they have responsibility for initialization.  The register can be unlocked by
+ *  rewriting the current host value.  This will reset the lock and restore the value to 0xFFFF.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_HB_DEV_ID_LOCK        hclk    hrst_n
+ */
+union cvmx_sriomaintx_hb_dev_id_lock {
+	uint32_t u32;
+	struct cvmx_sriomaintx_hb_dev_id_lock_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t reserved_16_31               : 16;
+	uint32_t hostid                       : 16; /**< Primary 16-bit Device ID */
+#else
+	uint32_t hostid                       : 16;
+	uint32_t reserved_16_31               : 16;
+#endif
+	} s;
+	struct cvmx_sriomaintx_hb_dev_id_lock_s cn63xx;
+	struct cvmx_sriomaintx_hb_dev_id_lock_s cn63xxp1;
+	struct cvmx_sriomaintx_hb_dev_id_lock_s cn66xx;
+};
+typedef union cvmx_sriomaintx_hb_dev_id_lock cvmx_sriomaintx_hb_dev_id_lock_t;
+
+/**
+ * cvmx_sriomaint#_ir_buffer_config
+ *
+ * SRIOMAINT_IR_BUFFER_CONFIG = SRIO Buffer Configuration
+ *
+ * Buffer Configuration
+ *
+ * Notes:
+ * This register controls the operation of the SRIO Core buffer mux logic.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_IR_BUFFER_CONFIG      hclk    hrst_n
+ */
+union cvmx_sriomaintx_ir_buffer_config {
+	uint32_t u32;
+	struct cvmx_sriomaintx_ir_buffer_config_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t tx_wm0                       : 4;  /**< Reserved. (See SRIOMAINT(0,2..3)_IR_BUFFER_CONFIG2) */
+	uint32_t tx_wm1                       : 4;  /**< Reserved. (See SRIOMAINT(0,2..3)_IR_BUFFER_CONFIG2) */
+	uint32_t tx_wm2                       : 4;  /**< Reserved. (See SRIOMAINT(0,2..3)_IR_BUFFER_CONFIG2) */
+	uint32_t reserved_3_19                : 17;
+	uint32_t tx_flow                      : 1;  /**< Controls whether Transmitter Flow Control is
+                                                         permitted on this device.
+                                                           0 - Disabled
+                                                           1 - Permitted
+                                                         The reset value of this field is
+                                                         SRIO*_IP_FEATURE[TX_FLOW]. */
+	uint32_t tx_sync                      : 1;  /**< Reserved. */
+	uint32_t rx_sync                      : 1;  /**< Reserved. */
+#else
+	uint32_t rx_sync                      : 1;
+	uint32_t tx_sync                      : 1;
+	uint32_t tx_flow                      : 1;
+	uint32_t reserved_3_19                : 17;
+	uint32_t tx_wm2                       : 4;
+	uint32_t tx_wm1                       : 4;
+	uint32_t tx_wm0                       : 4;
+#endif
+	} s;
+	struct cvmx_sriomaintx_ir_buffer_config_s cn63xx;
+	struct cvmx_sriomaintx_ir_buffer_config_s cn63xxp1;
+	struct cvmx_sriomaintx_ir_buffer_config_s cn66xx;
+};
+typedef union cvmx_sriomaintx_ir_buffer_config cvmx_sriomaintx_ir_buffer_config_t;
+
+/**
+ * cvmx_sriomaint#_ir_buffer_config2
+ *
+ * SRIOMAINT_IR_BUFFER_CONFIG2 = SRIO Buffer Configuration 2
+ *
+ * Buffer Configuration 2
+ *
+ * Notes:
+ * This register controls the RX and TX Buffer availablility by priority.  The typical values are
+ *  optimized for normal operation.  Care must be taken when changing these values to avoid values
+ *  which can result in deadlocks.  Disabling a priority is not recommended and can result in system
+ *  level failures.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_IR_BUFFER_CONFIG2     hclk    hrst_n
+ */
+union cvmx_sriomaintx_ir_buffer_config2 {
+	uint32_t u32;
+	struct cvmx_sriomaintx_ir_buffer_config2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t tx_wm3                       : 4;  /**< Number of buffers free before a priority 3 packet
+                                                         will be transmitted.  A value of 9 will disable
+                                                         this priority. */
+	uint32_t tx_wm2                       : 4;  /**< Number of buffers free before a priority 2 packet
+                                                         will be transmitted.  A value of 9 will disable
+                                                         this priority. */
+	uint32_t tx_wm1                       : 4;  /**< Number of buffers free before a priority 1 packet
+                                                         will be transmitted.  A value of 9 will disable
+                                                         this priority. */
+	uint32_t tx_wm0                       : 4;  /**< Number of buffers free before a priority 0 packet
+                                                         will be transmitted.  A value of 9 will disable
+                                                         this priority. */
+	uint32_t rx_wm3                       : 4;  /**< Number of buffers free before a priority 3 packet
+                                                         will be accepted.  A value of 9 will disable this
+                                                         priority and always cause a physical layer RETRY. */
+	uint32_t rx_wm2                       : 4;  /**< Number of buffers free before a priority 2 packet
+                                                         will be accepted.  A value of 9 will disable this
+                                                         priority and always cause a physical layer RETRY. */
+	uint32_t rx_wm1                       : 4;  /**< Number of buffers free before a priority 1 packet
+                                                         will be accepted.  A value of 9 will disable this
+                                                         priority and always cause a physical layer RETRY. */
+	uint32_t rx_wm0                       : 4;  /**< Number of buffers free before a priority 0 packet
+                                                         will be accepted.  A value of 9 will disable this
+                                                         priority and always cause a physical layer RETRY. */
+#else
+	uint32_t rx_wm0                       : 4;
+	uint32_t rx_wm1                       : 4;
+	uint32_t rx_wm2                       : 4;
+	uint32_t rx_wm3                       : 4;
+	uint32_t tx_wm0                       : 4;
+	uint32_t tx_wm1                       : 4;
+	uint32_t tx_wm2                       : 4;
+	uint32_t tx_wm3                       : 4;
+#endif
+	} s;
+	struct cvmx_sriomaintx_ir_buffer_config2_s cn63xx;
+	struct cvmx_sriomaintx_ir_buffer_config2_s cn66xx;
+};
+typedef union cvmx_sriomaintx_ir_buffer_config2 cvmx_sriomaintx_ir_buffer_config2_t;
+
+/**
+ * cvmx_sriomaint#_ir_pd_phy_ctrl
+ *
+ * SRIOMAINT_IR_PD_PHY_CTRL = SRIO Platform Dependent PHY Control
+ *
+ * Platform Dependent PHY Control
+ *
+ * Notes:
+ * This register can be used for testing.  The register is otherwise unused by the hardware.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_IR_PD_PHY_CTRL        hclk    hrst_n
+ */
+union cvmx_sriomaintx_ir_pd_phy_ctrl {
+	uint32_t u32;
+	struct cvmx_sriomaintx_ir_pd_phy_ctrl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t pd_ctrl                      : 32; /**< Unused Register available for testing */
+#else
+	uint32_t pd_ctrl                      : 32;
+#endif
+	} s;
+	struct cvmx_sriomaintx_ir_pd_phy_ctrl_s cn63xx;
+	struct cvmx_sriomaintx_ir_pd_phy_ctrl_s cn63xxp1;
+	struct cvmx_sriomaintx_ir_pd_phy_ctrl_s cn66xx;
+};
+typedef union cvmx_sriomaintx_ir_pd_phy_ctrl cvmx_sriomaintx_ir_pd_phy_ctrl_t;
+
+/**
+ * cvmx_sriomaint#_ir_pd_phy_stat
+ *
+ * SRIOMAINT_IR_PD_PHY_STAT = SRIO Platform Dependent PHY Status
+ *
+ * Platform Dependent PHY Status
+ *
+ * Notes:
+ * This register is used to monitor PHY status on each lane.  They are documented here to assist in
+ *  debugging only.  The lane numbers take into account the lane swap pin.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_IR_PD_PHY_STAT        hclk    hrst_n
+ */
+union cvmx_sriomaintx_ir_pd_phy_stat {
+	uint32_t u32;
+	struct cvmx_sriomaintx_ir_pd_phy_stat_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t reserved_16_31               : 16;
+	uint32_t ln3_rx                       : 3;  /**< Phy Lane 3 RX Status
+                                                         0XX = Normal Operation
+                                                         100 = 8B/10B Error
+                                                         101 = Elastic Buffer Overflow (Data Lost)
+                                                         110 = Elastic Buffer Underflow (Data Corrupted)
+                                                         111 = Disparity Error */
+	uint32_t ln3_dis                      : 1;  /**< Lane 3 Phy Clock Disabled
+                                                         0 = Phy Clock Valid
+                                                         1 = Phy Clock InValid */
+	uint32_t ln2_rx                       : 3;  /**< Phy Lane 2 RX Status
+                                                         0XX = Normal Operation
+                                                         100 = 8B/10B Error
+                                                         101 = Elastic Buffer Overflow (Data Lost)
+                                                         110 = Elastic Buffer Underflow (Data Corrupted)
+                                                         111 = Disparity Error */
+	uint32_t ln2_dis                      : 1;  /**< Lane 2 Phy Clock Disabled
+                                                         0 = Phy Clock Valid
+                                                         1 = Phy Clock InValid */
+	uint32_t ln1_rx                       : 3;  /**< Phy Lane 1 RX Status
+                                                         0XX = Normal Operation
+                                                         100 = 8B/10B Error
+                                                         101 = Elastic Buffer Overflow (Data Lost)
+                                                         110 = Elastic Buffer Underflow (Data Corrupted)
+                                                         111 = Disparity Error */
+	uint32_t ln1_dis                      : 1;  /**< Lane 1 Phy Clock Disabled
+                                                         0 = Phy Clock Valid
+                                                         1 = Phy Clock InValid */
+	uint32_t ln0_rx                       : 3;  /**< Phy Lane 0 RX Status
+                                                         0XX = Normal Operation
+                                                         100 = 8B/10B Error
+                                                         101 = Elastic Buffer Overflow (Data Lost)
+                                                         110 = Elastic Buffer Underflow (Data Corrupted)
+                                                         111 = Disparity Error */
+	uint32_t ln0_dis                      : 1;  /**< Lane 0 Phy Clock Disabled
+                                                         0 = Phy Clock Valid
+                                                         1 = Phy Clock InValid */
+#else
+	uint32_t ln0_dis                      : 1;
+	uint32_t ln0_rx                       : 3;
+	uint32_t ln1_dis                      : 1;
+	uint32_t ln1_rx                       : 3;
+	uint32_t ln2_dis                      : 1;
+	uint32_t ln2_rx                       : 3;
+	uint32_t ln3_dis                      : 1;
+	uint32_t ln3_rx                       : 3;
+	uint32_t reserved_16_31               : 16;
+#endif
+	} s;
+	struct cvmx_sriomaintx_ir_pd_phy_stat_s cn63xx;
+	struct cvmx_sriomaintx_ir_pd_phy_stat_s cn63xxp1;
+	struct cvmx_sriomaintx_ir_pd_phy_stat_s cn66xx;
+};
+typedef union cvmx_sriomaintx_ir_pd_phy_stat cvmx_sriomaintx_ir_pd_phy_stat_t;
+
+/**
+ * cvmx_sriomaint#_ir_pi_phy_ctrl
+ *
+ * SRIOMAINT_IR_PI_PHY_CTRL = SRIO Platform Independent PHY Control
+ *
+ * Platform Independent PHY Control
+ *
+ * Notes:
+ * This register is used to control platform independent operating modes of the transceivers. These
+ *  control bits are uniform across all platforms.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_IR_PI_PHY_CTRL        hclk    hrst_n
+ */
+union cvmx_sriomaintx_ir_pi_phy_ctrl {
+	uint32_t u32;
+	struct cvmx_sriomaintx_ir_pi_phy_ctrl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t tx_reset                     : 1;  /**< Outgoing PHY Logic Reset.  0=Reset, 1=Normal Op */
+	uint32_t rx_reset                     : 1;  /**< Incoming PHY Logic Reset.  0=Reset, 1=Normal Op */
+	uint32_t reserved_29_29               : 1;
+	uint32_t loopback                     : 2;  /**< These bits control the state of the loopback
+                                                         control vector on the transceiver interface.  The
+                                                         loopback modes are enumerated as follows:
+                                                           00 - No Loopback
+                                                           01 - Near End PCS Loopback
+                                                           10 - Far End PCS Loopback
+                                                           11 - Both Near and Far End PCS Loopback */
+	uint32_t reserved_0_26                : 27;
+#else
+	uint32_t reserved_0_26                : 27;
+	uint32_t loopback                     : 2;
+	uint32_t reserved_29_29               : 1;
+	uint32_t rx_reset                     : 1;
+	uint32_t tx_reset                     : 1;
+#endif
+	} s;
+	struct cvmx_sriomaintx_ir_pi_phy_ctrl_s cn63xx;
+	struct cvmx_sriomaintx_ir_pi_phy_ctrl_s cn63xxp1;
+	struct cvmx_sriomaintx_ir_pi_phy_ctrl_s cn66xx;
+};
+typedef union cvmx_sriomaintx_ir_pi_phy_ctrl cvmx_sriomaintx_ir_pi_phy_ctrl_t;
+
+/**
+ * cvmx_sriomaint#_ir_pi_phy_stat
+ *
+ * SRIOMAINT_IR_PI_PHY_STAT = SRIO Platform Independent PHY Status
+ *
+ * Platform Independent PHY Status
+ *
+ * Notes:
+ * This register displays the status of the link initialization state machine.  Changes to this state
+ *  cause the SRIO(0,2..3)_INT_REG.LINK_UP or SRIO(0,2..3)_INT_REG.LINK_DOWN interrupts.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_IR_PI_PHY_STAT        hclk    hrst_n
+ */
+union cvmx_sriomaintx_ir_pi_phy_stat {
+	uint32_t u32;
+	struct cvmx_sriomaintx_ir_pi_phy_stat_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t reserved_12_31               : 20;
+	uint32_t tx_rdy                       : 1;  /**< Minimum number of Status Transmitted */
+	uint32_t rx_rdy                       : 1;  /**< Minimum number of Good Status Received */
+	uint32_t init_sm                      : 10; /**< Initialization State Machine
+                                                         001 - Silent
+                                                         002 - Seek
+                                                         004 - Discovery
+                                                         008 - 1x_Mode_Lane0
+                                                         010 - 1x_Mode_Lane1
+                                                         020 - 1x_Mode_Lane2
+                                                         040 - 1x_Recovery
+                                                         080 - 2x_Mode
+                                                         100 - 2x_Recovery
+                                                         200 - 4x_Mode
+                                                         All others are reserved */
+#else
+	uint32_t init_sm                      : 10;
+	uint32_t rx_rdy                       : 1;
+	uint32_t tx_rdy                       : 1;
+	uint32_t reserved_12_31               : 20;
+#endif
+	} s;
+	struct cvmx_sriomaintx_ir_pi_phy_stat_s cn63xx;
+	struct cvmx_sriomaintx_ir_pi_phy_stat_cn63xxp1 {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t reserved_10_31               : 22;
+	uint32_t init_sm                      : 10; /**< Initialization State Machine
+                                                         001 - Silent
+                                                         002 - Seek
+                                                         004 - Discovery
+                                                         008 - 1x_Mode_Lane0
+                                                         010 - 1x_Mode_Lane1
+                                                         020 - 1x_Mode_Lane2
+                                                         040 - 1x_Recovery
+                                                         080 - 2x_Mode
+                                                         100 - 2x_Recovery
+                                                         200 - 4x_Mode
+                                                         All others are reserved */
+#else
+	uint32_t init_sm                      : 10;
+	uint32_t reserved_10_31               : 22;
+#endif
+	} cn63xxp1;
+	struct cvmx_sriomaintx_ir_pi_phy_stat_s cn66xx;
+};
+typedef union cvmx_sriomaintx_ir_pi_phy_stat cvmx_sriomaintx_ir_pi_phy_stat_t;
+
+/**
+ * cvmx_sriomaint#_ir_sp_rx_ctrl
+ *
+ * SRIOMAINT_IR_SP_RX_CTRL = SRIO Soft Packet FIFO Receive Control
+ *
+ * Soft Packet FIFO Receive Control
+ *
+ * Notes:
+ * This register is used to configure events generated by the reception of packets using the soft
+ * packet FIFO.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_IR_SP_RX_CTRL hclk    hrst_n
+ */
+union cvmx_sriomaintx_ir_sp_rx_ctrl {
+	uint32_t u32;
+	struct cvmx_sriomaintx_ir_sp_rx_ctrl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t reserved_1_31                : 31;
+	uint32_t overwrt                      : 1;  /**< When clear, SRIO drops received packets that should
+                                                         enter the soft packet FIFO when the FIFO is full.
+                                                         In this case, SRIO also increments
+                                                         SRIOMAINT(0,2..3)_IR_SP_RX_STAT.DROP_CNT. When set, SRIO
+                                                         stalls received packets that should enter the soft
+                                                         packet FIFO when the FIFO is full. SRIO may stop
+                                                         receiving any packets in this stall case if
+                                                         software does not drain the receive soft packet
+                                                         FIFO. */
+#else
+	uint32_t overwrt                      : 1;
+	uint32_t reserved_1_31                : 31;
+#endif
+	} s;
+	struct cvmx_sriomaintx_ir_sp_rx_ctrl_s cn63xx;
+	struct cvmx_sriomaintx_ir_sp_rx_ctrl_s cn63xxp1;
+	struct cvmx_sriomaintx_ir_sp_rx_ctrl_s cn66xx;
+};
+typedef union cvmx_sriomaintx_ir_sp_rx_ctrl cvmx_sriomaintx_ir_sp_rx_ctrl_t;
+
+/**
+ * cvmx_sriomaint#_ir_sp_rx_data
+ *
+ * SRIOMAINT_IR_SP_RX_DATA = SRIO Soft Packet FIFO Receive Data
+ *
+ * Soft Packet FIFO Receive Data
+ *
+ * Notes:
+ * This register is used to read data from the soft packet FIFO.  The Soft Packet FIFO contains the
+ *  majority of the packet data received from the SRIO link.  The packet does not include the Control
+ *  Symbols or the initial byte containing AckId, 2 Reserved Bits and the CRF.  In the case of packets
+ *  with less than 80 bytes (including AckId byte) both the trailing CRC and Pad (if present) are
+ *  included in the FIFO and Octet Count.  In the case of a packet with exactly 80 bytes (including
+ *  the AckId byte) the CRC is removed and the Pad is maintained so the Octet Count will read 81 bytes
+ *  instead of the expected 83.  In cases over 80 bytes the CRC at 80 bytes is removed but the
+ *  trailing CRC and Pad (if necessary) are present.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_IR_SP_RX_DATA hclk    hrst_n
+ */
+union cvmx_sriomaintx_ir_sp_rx_data {
+	uint32_t u32;
+	struct cvmx_sriomaintx_ir_sp_rx_data_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t pkt_data                     : 32; /**< This register is used to read packet data from the
+                                                         RX FIFO. */
+#else
+	uint32_t pkt_data                     : 32;
+#endif
+	} s;
+	struct cvmx_sriomaintx_ir_sp_rx_data_s cn63xx;
+	struct cvmx_sriomaintx_ir_sp_rx_data_s cn63xxp1;
+	struct cvmx_sriomaintx_ir_sp_rx_data_s cn66xx;
+};
+typedef union cvmx_sriomaintx_ir_sp_rx_data cvmx_sriomaintx_ir_sp_rx_data_t;
+
+/**
+ * cvmx_sriomaint#_ir_sp_rx_stat
+ *
+ * SRIOMAINT_IR_SP_RX_STAT = SRIO Soft Packet FIFO Receive Status
+ *
+ * Soft Packet FIFO Receive Status
+ *
+ * Notes:
+ * This register is used to monitor the reception of packets using the soft packet FIFO.
+ *  The HW sets SRIO_INT_REG[SOFT_RX] every time a packet arrives in the soft packet FIFO. To read
+ *  out (one or more) packets, the following procedure may be best:
+ *       (1) clear SRIO_INT_REG[SOFT_RX],
+ *       (2) read this CSR to determine how many packets there are,
+ *       (3) read the packets out (via SRIOMAINT*_IR_SP_RX_DATA).
+ *  This procedure could lead to situations where SOFT_RX will be set even though there are currently
+ *  no packets - the SW interrupt handler would need to properly handle this case
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_IR_SP_RX_STAT hclk    hrst_n
+ */
+union cvmx_sriomaintx_ir_sp_rx_stat {
+	uint32_t u32;
+	struct cvmx_sriomaintx_ir_sp_rx_stat_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t octets                       : 16; /**< This field shows how many octets are remaining
+                                                         in the current packet in the RX FIFO. */
+	uint32_t buffers                      : 4;  /**< This field indicates how many complete packets are
+                                                         stored in the Rx FIFO. */
+	uint32_t drop_cnt                     : 7;  /**< Number of Packets Received when the RX FIFO was
+                                                         full and then discarded. */
+	uint32_t full                         : 1;  /**< This bit is set when the value of Buffers Filled
+                                                         equals the number of available reception buffers. */
+	uint32_t fifo_st                      : 4;  /**< These bits display the state of the state machine
+                                                         that controls loading of packet data into the RX
+                                                         FIFO. The enumeration of states are as follows:
+                                                           0000 - Idle
+                                                           0001 - Armed
+                                                           0010 - Active
+                                                           All other states are reserved. */
+#else
+	uint32_t fifo_st                      : 4;
+	uint32_t full                         : 1;
+	uint32_t drop_cnt                     : 7;
+	uint32_t buffers                      : 4;
+	uint32_t octets                       : 16;
+#endif
+	} s;
+	struct cvmx_sriomaintx_ir_sp_rx_stat_s cn63xx;
+	struct cvmx_sriomaintx_ir_sp_rx_stat_cn63xxp1 {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t octets                       : 16; /**< This field shows how many octets are remaining
+                                                         in the current packet in the RX FIFO. */
+	uint32_t buffers                      : 4;  /**< This field indicates how many complete packets are
+                                                         stored in the Rx FIFO. */
+	uint32_t reserved_5_11                : 7;
+	uint32_t full                         : 1;  /**< This bit is set when the value of Buffers Filled
+                                                         equals the number of available reception buffers.
+                                                         This bit always reads zero in Pass 1 */
+	uint32_t fifo_st                      : 4;  /**< These bits display the state of the state machine
+                                                         that controls loading of packet data into the RX
+                                                         FIFO. The enumeration of states are as follows:
+                                                           0000 - Idle
+                                                           0001 - Armed
+                                                           0010 - Active
+                                                           All other states are reserved. */
+#else
+	uint32_t fifo_st                      : 4;
+	uint32_t full                         : 1;
+	uint32_t reserved_5_11                : 7;
+	uint32_t buffers                      : 4;
+	uint32_t octets                       : 16;
+#endif
+	} cn63xxp1;
+	struct cvmx_sriomaintx_ir_sp_rx_stat_s cn66xx;
+};
+typedef union cvmx_sriomaintx_ir_sp_rx_stat cvmx_sriomaintx_ir_sp_rx_stat_t;
+
+/**
+ * cvmx_sriomaint#_ir_sp_tx_ctrl
+ *
+ * SRIOMAINT_IR_SP_TX_CTRL = SRIO Soft Packet FIFO Transmit Control
+ *
+ * Soft Packet FIFO Transmit Control
+ *
+ * Notes:
+ * This register is used to configure and control the transmission of packets using the soft packet
+ *  FIFO.
+ *
+ * Clk_Rst:        SRIOMAINT_IR_SP_TX_CTRL hclk    hrst_n
+ */
+union cvmx_sriomaintx_ir_sp_tx_ctrl {
+	uint32_t u32;
+	struct cvmx_sriomaintx_ir_sp_tx_ctrl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t octets                       : 16; /**< Writing a non-zero value (N) to this field arms
+                                                         the packet FIFO for packet transmission. The FIFO
+                                                         control logic will transmit the next N bytes
+                                                         written 4-bytes at a time to the
+                                                         SRIOMAINT(0,2..3)_IR_SP_TX_DATA Register and create a
+                                                         single RapidIO packet. */
+	uint32_t reserved_0_15                : 16;
+#else
+	uint32_t reserved_0_15                : 16;
+	uint32_t octets                       : 16;
+#endif
+	} s;
+	struct cvmx_sriomaintx_ir_sp_tx_ctrl_s cn63xx;
+	struct cvmx_sriomaintx_ir_sp_tx_ctrl_s cn63xxp1;
+	struct cvmx_sriomaintx_ir_sp_tx_ctrl_s cn66xx;
+};
+typedef union cvmx_sriomaintx_ir_sp_tx_ctrl cvmx_sriomaintx_ir_sp_tx_ctrl_t;
+
+/**
+ * cvmx_sriomaint#_ir_sp_tx_data
+ *
+ * SRIOMAINT_IR_SP_TX_DATA = SRIO Soft Packet FIFO Transmit Data
+ *
+ * Soft Packet FIFO Transmit Data
+ *
+ * Notes:
+ * This register is used to write data to the soft packet FIFO.  The format of the packet follows the
+ * Internal Packet Format (add link here).  Care must be taken on creating TIDs for the packets which
+ * generate a response.  Bits [7:6] of the 8 bit TID must be set for all Soft Packet FIFO generated
+ * packets.  TID values of 0x00 - 0xBF are reserved for hardware generated Tags.  The remainer of the
+ * TID[5:0] must be unique for each packet in flight and cannot be reused until a response is received
+ * in the SRIOMAINT(0,2..3)_IR_SP_RX_DATA register.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_IR_SP_TX_DATA hclk    hrst_n
+ */
+union cvmx_sriomaintx_ir_sp_tx_data {
+	uint32_t u32;
+	struct cvmx_sriomaintx_ir_sp_tx_data_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t pkt_data                     : 32; /**< This register is used to write packet data to the
+                                                         Tx FIFO. Reads of this register will return zero. */
+#else
+	uint32_t pkt_data                     : 32;
+#endif
+	} s;
+	struct cvmx_sriomaintx_ir_sp_tx_data_s cn63xx;
+	struct cvmx_sriomaintx_ir_sp_tx_data_s cn63xxp1;
+	struct cvmx_sriomaintx_ir_sp_tx_data_s cn66xx;
+};
+typedef union cvmx_sriomaintx_ir_sp_tx_data cvmx_sriomaintx_ir_sp_tx_data_t;
+
+/**
+ * cvmx_sriomaint#_ir_sp_tx_stat
+ *
+ * SRIOMAINT_IR_SP_TX_STAT = SRIO Soft Packet FIFO Transmit Status
+ *
+ * Soft Packet FIFO Transmit Status
+ *
+ * Notes:
+ * This register is used to monitor the transmission of packets using the soft packet FIFO.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_IR_SP_TX_STAT hclk    hrst_n
+ */
+union cvmx_sriomaintx_ir_sp_tx_stat {
+	uint32_t u32;
+	struct cvmx_sriomaintx_ir_sp_tx_stat_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t octets                       : 16; /**< This field shows how many octets are still to be
+                                                         loaded in the current packet. */
+	uint32_t buffers                      : 4;  /**< This field indicates how many complete packets are
+                                                         stored in the Tx FIFO.  The field always reads
+                                                         zero in the current hardware. */
+	uint32_t reserved_5_11                : 7;
+	uint32_t full                         : 1;  /**< This bit is set when the value of Buffers Filled
+                                                         equals the number of available transmission
+                                                         buffers. */
+	uint32_t fifo_st                      : 4;  /**< These bits display the state of the state machine
+                                                         that controls loading of packet data into the TX
+                                                         FIFO. The enumeration of states are as follows:
+                                                           0000 - Idle
+                                                           0001 - Armed
+                                                           0010 - Active
+                                                           All other states are reserved. */
+#else
+	uint32_t fifo_st                      : 4;
+	uint32_t full                         : 1;
+	uint32_t reserved_5_11                : 7;
+	uint32_t buffers                      : 4;
+	uint32_t octets                       : 16;
+#endif
+	} s;
+	struct cvmx_sriomaintx_ir_sp_tx_stat_s cn63xx;
+	struct cvmx_sriomaintx_ir_sp_tx_stat_s cn63xxp1;
+	struct cvmx_sriomaintx_ir_sp_tx_stat_s cn66xx;
+};
+typedef union cvmx_sriomaintx_ir_sp_tx_stat cvmx_sriomaintx_ir_sp_tx_stat_t;
+
+/**
+ * cvmx_sriomaint#_lane_#_status_0
+ *
+ * SRIOMAINT_LANE_X_STATUS_0 = SRIO Lane X Status 0
+ *
+ * SRIO Lane Status 0
+ *
+ * Notes:
+ * This register contains status information about the local lane transceiver.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_LANE_[0:3]_STATUS_0   hclk    hrst_n
+ */
+union cvmx_sriomaintx_lane_x_status_0 {
+	uint32_t u32;
+	struct cvmx_sriomaintx_lane_x_status_0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t port                         : 8;  /**< The number of the port within the device to which
+                                                         the lane is assigned. */
+	uint32_t lane                         : 4;  /**< Lane Number within the port. */
+	uint32_t tx_type                      : 1;  /**< Transmitter Type
+                                                         0 = Short Run
+                                                         1 = Long Run */
+	uint32_t tx_mode                      : 1;  /**< Transmitter Operating Mode
+                                                         0 = Short Run
+                                                         1 = Long Run */
+	uint32_t rx_type                      : 2;  /**< Receiver Type
+                                                         0 = Short Run
+                                                         1 = Medium Run
+                                                         2 = Long Run
+                                                         3 = Reserved */
+	uint32_t rx_inv                       : 1;  /**< Receiver Input Inverted
+                                                         0 = No Inversion
+                                                         1 = Input Inverted */
+	uint32_t rx_adapt                     : 1;  /**< Receiver Trained
+                                                         0 = One or more adaptive equalizers are
+                                                             controlled by the lane receiver and at least
+                                                             one is not trained.
+                                                         1 = The lane receiver controls no adaptive
+                                                             equalizers or all the equalizers are trained. */
+	uint32_t rx_sync                      : 1;  /**< Receiver Lane Sync'd */
+	uint32_t rx_train                     : 1;  /**< Receiver Lane Trained */
+	uint32_t dec_err                      : 4;  /**< 8Bit/10Bit Decoding Errors
+                                                         0    = No Errors since last read
+                                                         1-14 = Number of Errors since last read
+                                                         15   = Fifteen or more Errors since last read */
+	uint32_t xsync                        : 1;  /**< Receiver Lane Sync Change
+                                                         0 = Lane Sync has not changed since last read
+                                                         1 = Lane Sync has changed since last read */
+	uint32_t xtrain                       : 1;  /**< Receiver Training Change
+                                                         0 = Training has not changed since last read
+                                                         1 = Training has changed since last read */
+	uint32_t reserved_4_5                 : 2;
+	uint32_t status1                      : 1;  /**< Status 1 CSR Implemented */
+	uint32_t statusn                      : 3;  /**< Status 2-7 Not Implemented */
+#else
+	uint32_t statusn                      : 3;
+	uint32_t status1                      : 1;
+	uint32_t reserved_4_5                 : 2;
+	uint32_t xtrain                       : 1;
+	uint32_t xsync                        : 1;
+	uint32_t dec_err                      : 4;
+	uint32_t rx_train                     : 1;
+	uint32_t rx_sync                      : 1;
+	uint32_t rx_adapt                     : 1;
+	uint32_t rx_inv                       : 1;
+	uint32_t rx_type                      : 2;
+	uint32_t tx_mode                      : 1;
+	uint32_t tx_type                      : 1;
+	uint32_t lane                         : 4;
+	uint32_t port                         : 8;
+#endif
+	} s;
+	struct cvmx_sriomaintx_lane_x_status_0_s cn63xx;
+	struct cvmx_sriomaintx_lane_x_status_0_s cn63xxp1;
+	struct cvmx_sriomaintx_lane_x_status_0_s cn66xx;
+};
+typedef union cvmx_sriomaintx_lane_x_status_0 cvmx_sriomaintx_lane_x_status_0_t;
+
+/**
+ * cvmx_sriomaint#_lcs_ba0
+ *
+ * SRIOMAINT_LCS_BA0 = SRIO Local Configuration Space MSB Base Address
+ *
+ * MSBs of SRIO Address Space mapped to Maintenance BAR.
+ *
+ * Notes:
+ * The double word aligned SRIO address window mapped to the SRIO Maintenance BAR.  This window has
+ *  the highest priority and eclipses matches to the BAR0, BAR1 and BAR2 windows.  Note:  Address bits
+ *  not supplied in the transfer are considered zero.  For example, SRIO Address 65:35 must be set to
+ *  zero to match in a 34-bit access.  SRIO Address 65:50 must be set to zero to match in a 50-bit
+ *  access.  This coding allows the Maintenance Bar window to appear in specific address spaces. The
+ *  remaining bits are located in SRIOMAINT(0,2..3)_LCS_BA1. This SRIO maintenance BAR is effectively
+ *  disabled when LCSBA[30] is set with 34 or 50-bit addressing.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_LCS_BA0       hclk    hrst_n
+ */
+union cvmx_sriomaintx_lcs_ba0 {
+	uint32_t u32;
+	struct cvmx_sriomaintx_lcs_ba0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t reserved_31_31               : 1;
+	uint32_t lcsba                        : 31; /**< SRIO Address 65:35 */
+#else
+	uint32_t lcsba                        : 31;
+	uint32_t reserved_31_31               : 1;
+#endif
+	} s;
+	struct cvmx_sriomaintx_lcs_ba0_s      cn63xx;
+	struct cvmx_sriomaintx_lcs_ba0_s      cn63xxp1;
+	struct cvmx_sriomaintx_lcs_ba0_s      cn66xx;
+};
+typedef union cvmx_sriomaintx_lcs_ba0 cvmx_sriomaintx_lcs_ba0_t;
+
+/**
+ * cvmx_sriomaint#_lcs_ba1
+ *
+ * SRIOMAINT_LCS_BA1 = SRIO Local Configuration Space LSB Base Address
+ *
+ * LSBs of SRIO Address Space mapped to Maintenance BAR.
+ *
+ * Notes:
+ * The double word aligned SRIO address window mapped to the SRIO Maintenance BAR.  This window has
+ *  the highest priority and eclipses matches to the BAR0, BAR1 and BAR2 windows. Address bits not
+ *  supplied in the transfer are considered zero.  For example, SRIO Address 65:35 must be set to zero
+ *  to match in a 34-bit access and SRIO Address 65:50 must be set to zero to match in a 50-bit access.
+ *  This coding allows the Maintenance Bar window to appear in specific address spaces. Accesses
+ *  through this BAR are limited to single word (32-bit) aligned transfers of one to four bytes.
+ *  Accesses which violate this rule will return an error response if possible and be otherwise
+ *  ignored.  The remaining bits are located in SRIOMAINT(0,2..3)_LCS_BA0.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_LCS_BA1       hclk    hrst_n
+ */
+union cvmx_sriomaintx_lcs_ba1 {
+	uint32_t u32;
+	struct cvmx_sriomaintx_lcs_ba1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t lcsba                        : 11; /**< SRIO Address 34:24 */
+	uint32_t reserved_0_20                : 21;
+#else
+	uint32_t reserved_0_20                : 21;
+	uint32_t lcsba                        : 11;
+#endif
+	} s;
+	struct cvmx_sriomaintx_lcs_ba1_s      cn63xx;
+	struct cvmx_sriomaintx_lcs_ba1_s      cn63xxp1;
+	struct cvmx_sriomaintx_lcs_ba1_s      cn66xx;
+};
+typedef union cvmx_sriomaintx_lcs_ba1 cvmx_sriomaintx_lcs_ba1_t;
+
+/**
+ * cvmx_sriomaint#_m2s_bar0_start0
+ *
+ * SRIOMAINT_M2S_BAR0_START0 = SRIO Device Access BAR0 MSB Start
+ *
+ * The starting SRIO address to forwarded to the NPEI Configuration Space.
+ *
+ * Notes:
+ * This register specifies the 50-bit and 66-bit SRIO Address mapped to the BAR0 Space.  See
+ *  SRIOMAINT(0,2..3)_M2S_BAR0_START1 for more details. This register is only writeable over SRIO if the
+ *  SRIO(0,2..3)_ACC_CTRL.DENY_BAR0 bit is zero.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_M2S_BAR0_START0       hclk    hrst_n
+ */
+union cvmx_sriomaintx_m2s_bar0_start0 {
+	uint32_t u32;
+	struct cvmx_sriomaintx_m2s_bar0_start0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t addr64                       : 16; /**< SRIO Address 63:48 */
+	uint32_t addr48                       : 16; /**< SRIO Address 47:32 */
+#else
+	uint32_t addr48                       : 16;
+	uint32_t addr64                       : 16;
+#endif
+	} s;
+	struct cvmx_sriomaintx_m2s_bar0_start0_s cn63xx;
+	struct cvmx_sriomaintx_m2s_bar0_start0_s cn63xxp1;
+	struct cvmx_sriomaintx_m2s_bar0_start0_s cn66xx;
+};
+typedef union cvmx_sriomaintx_m2s_bar0_start0 cvmx_sriomaintx_m2s_bar0_start0_t;
+
+/**
+ * cvmx_sriomaint#_m2s_bar0_start1
+ *
+ * SRIOMAINT_M2S_BAR0_START1 = SRIO Device Access BAR0 LSB Start
+ *
+ * The starting SRIO address to forwarded to the NPEI Configuration Space.
+ *
+ * Notes:
+ * This register specifies the SRIO Address mapped to the BAR0 RSL Space.  If the transaction has not
+ *  already been mapped to SRIO Maintenance Space through the SRIOMAINT_LCS_BA[1:0] registers, if
+ *  ENABLE is set and the address bits match then the SRIO Memory transactions will map to Octeon SLI
+ *  Registers.  34-bit address transactions require a match in SRIO Address 33:14 and require all the
+ *  other bits in ADDR48, ADDR64 and ADDR66 fields to be zero.  50-bit address transactions a match of
+ *  SRIO Address 49:14 and require all the other bits of ADDR64 and ADDR66 to be zero.  66-bit address
+ *  transactions require matches of all valid address field bits.  Reads and  Writes through Bar0
+ *  have a size limit of 8 bytes and cannot cross a 64-bit boundry.  All accesses with sizes greater
+ *  than this limit will be ignored and return an error on any SRIO responses.  Note: ADDR48 and
+ *  ADDR64 fields are located in SRIOMAINT(0,2..3)_M2S_BAR0_START0.  The ADDR32/66 fields of this register
+ *  are writeable over SRIO if the SRIO(0,2..3)_ACC_CTRL.DENY_ADR0 bit is zero.  The ENABLE field is
+ *  writeable over SRIO if the SRIO(0,2..3)_ACC_CTRL.DENY_BAR0 bit is zero.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_M2S_BAR0_START1       hclk    hrst_n
+ */
+union cvmx_sriomaintx_m2s_bar0_start1 {
+	uint32_t u32;
+	struct cvmx_sriomaintx_m2s_bar0_start1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t addr32                       : 18; /**< SRIO Address 31:14 */
+	uint32_t reserved_3_13                : 11;
+	uint32_t addr66                       : 2;  /**< SRIO Address 65:64 */
+	uint32_t enable                       : 1;  /**< Enable BAR0 Access */
+#else
+	uint32_t enable                       : 1;
+	uint32_t addr66                       : 2;
+	uint32_t reserved_3_13                : 11;
+	uint32_t addr32                       : 18;
+#endif
+	} s;
+	struct cvmx_sriomaintx_m2s_bar0_start1_s cn63xx;
+	struct cvmx_sriomaintx_m2s_bar0_start1_s cn63xxp1;
+	struct cvmx_sriomaintx_m2s_bar0_start1_s cn66xx;
+};
+typedef union cvmx_sriomaintx_m2s_bar0_start1 cvmx_sriomaintx_m2s_bar0_start1_t;
+
+/**
+ * cvmx_sriomaint#_m2s_bar1_start0
+ *
+ * SRIOMAINT_M2S_BAR1_START0 = SRIO Device Access BAR1 MSB Start
+ *
+ * The starting SRIO address to forwarded to the BAR1 Memory Space.
+ *
+ * Notes:
+ * This register specifies the 50-bit and 66-bit SRIO Address mapped to the BAR1 Space.  See
+ *  SRIOMAINT(0,2..3)_M2S_BAR1_START1 for more details.  This register is only writeable over SRIO if the
+ *  SRIO(0,2..3)_ACC_CTRL.DENY_ADR1 bit is zero.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_M2S_BAR1_START0       hclk    hrst_n
+ */
+union cvmx_sriomaintx_m2s_bar1_start0 {
+	uint32_t u32;
+	struct cvmx_sriomaintx_m2s_bar1_start0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t addr64                       : 16; /**< SRIO Address 63:48 */
+	uint32_t addr48                       : 16; /**< SRIO Address 47:32
+                                                         The SRIO hardware does not use the low order
+                                                         one or two bits of this field when BARSIZE is 12
+                                                         or 13, respectively.
+                                                         (BARSIZE is SRIOMAINT(0,2..3)_M2S_BAR1_START1[BARSIZE].) */
+#else
+	uint32_t addr48                       : 16;
+	uint32_t addr64                       : 16;
+#endif
+	} s;
+	struct cvmx_sriomaintx_m2s_bar1_start0_s cn63xx;
+	struct cvmx_sriomaintx_m2s_bar1_start0_s cn63xxp1;
+	struct cvmx_sriomaintx_m2s_bar1_start0_s cn66xx;
+};
+typedef union cvmx_sriomaintx_m2s_bar1_start0 cvmx_sriomaintx_m2s_bar1_start0_t;
+
+/**
+ * cvmx_sriomaint#_m2s_bar1_start1
+ *
+ * SRIOMAINT_M2S_BAR1_START1 = SRIO Device to BAR1 Start
+ *
+ * The starting SRIO address to forwarded to the BAR1 Memory Space.
+ *
+ * Notes:
+ * This register specifies the SRIO Address mapped to the BAR1 Space.  If the transaction has not
+ *  already been mapped to SRIO Maintenance Space through the SRIOMAINT_LCS_BA[1:0] registers and the
+ *  address bits do not match enabled BAR0 addresses and if ENABLE is set and the addresses match the
+ *  BAR1 addresses then SRIO Memory transactions will map to Octeon Memory Space specified by
+ *  SRIOMAINT(0,2..3)_BAR1_IDX[31:0] registers.  The BARSIZE field determines the size of BAR1, the entry
+ *  select bits, and the size of each entry. A 34-bit address matches BAR1 when it matches
+ *  SRIO_Address[33:20+BARSIZE] while all the other bits in ADDR48, ADDR64 and ADDR66 are zero.
+ *  A 50-bit address matches BAR1 when it matches SRIO_Address[49:20+BARSIZE] while all the
+ *  other bits of ADDR64 and ADDR66 are zero.  A 66-bit address matches BAR1 when all of
+ *  SRIO_Address[65:20+BARSIZE] match all corresponding address CSR field bits.  Note: ADDR48 and
+ *  ADDR64 fields are located in SRIOMAINT(0,2..3)_M2S_BAR1_START0. The ADDR32/66 fields of this register
+ *  are writeable over SRIO if the SRIO(0,2..3)_ACC_CTRL.DENY_ADR1 bit is zero.  The remaining fields are
+ *  writeable over SRIO if the SRIO(0,2..3)_ACC_CTRL.DENY_BAR1 bit is zero.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_M2S_BAR1_START1       hclk    hrst_n
+ */
+union cvmx_sriomaintx_m2s_bar1_start1 {
+	uint32_t u32;
+	struct cvmx_sriomaintx_m2s_bar1_start1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t addr32                       : 12; /**< SRIO Address 31:20
+                                                         This field is not used by the SRIO hardware for
+                                                         BARSIZE values 12 or 13.
+                                                         With BARSIZE < 12, the upper 12-BARSIZE
+                                                         bits of this field are used, and the lower BARSIZE
+                                                         bits of this field are unused by the SRIO hardware. */
+	uint32_t reserved_7_19                : 13;
+	uint32_t barsize                      : 4;  /**< Bar Size.
+                                                                              SRIO_Address*
+                                                                         ---------------------
+                                                                        /                     \
+                                                         BARSIZE         BAR     Entry   Entry    Entry
+                                                         Value   BAR    compare  Select  Offset   Size
+                                                                 Size    bits    bits    bits
+                                                          0       1MB    65:20   19:16   15:0     64KB
+                                                          1       2MB    65:21   20:17   16:0    128KB
+                                                          2       4MB    65:22   21:18   17:0    256KB
+                                                          3       8MB    65:23   22:19   18:0    512KB
+                                                          4      16MB    65:24   23:20   19:0      1MB
+                                                          5      32MB    65:25   24:21   20:0      2MB
+                                                          6      64MB    65:26   25:22   21:0      4MB
+                                                          7     128MB    65:27   26:23   22:0      8MB
+                                                          8     256MB    65:28   27:24   23:0     16MB
+                                                          9     512MB    65:29   28:25   24:0     32MB
+                                                         10    1024MB    65:30   29:26   25:0     64MB
+                                                         11    2048MB    65:31   30:27   26:0    128MB
+                                                         12    4096MB    65:32   31:28   27:0    256MB
+                                                         13    8192MB    65:33   32:29   28:0    512MB
+
+                                                         *The SRIO Transaction Address
+                                                         The entry select bits is the X that  select an
+                                                         SRIOMAINT(0,2..3)_BAR1_IDXX entry. */
+	uint32_t addr66                       : 2;  /**< SRIO Address 65:64 */
+	uint32_t enable                       : 1;  /**< Enable BAR1 Access */
+#else
+	uint32_t enable                       : 1;
+	uint32_t addr66                       : 2;
+	uint32_t barsize                      : 4;
+	uint32_t reserved_7_19                : 13;
+	uint32_t addr32                       : 12;
+#endif
+	} s;
+	struct cvmx_sriomaintx_m2s_bar1_start1_s cn63xx;
+	struct cvmx_sriomaintx_m2s_bar1_start1_cn63xxp1 {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t addr32                       : 12; /**< SRIO Address 31:20
+                                                         With BARSIZE < 12, the upper 12-BARSIZE
+                                                         bits of this field are used, and the lower BARSIZE
+                                                         bits of this field are unused by the SRIO hardware. */
+	uint32_t reserved_6_19                : 14;
+	uint32_t barsize                      : 3;  /**< Bar Size.
+                                                                              SRIO_Address*
+                                                                         ---------------------
+                                                                        /                     \
+                                                         BARSIZE         BAR     Entry   Entry    Entry
+                                                         Value   BAR    compare  Select  Offset   Size
+                                                                 Size    bits    bits    bits
+                                                          0       1MB    65:20   19:16   15:0     64KB
+                                                          1       2MB    65:21   20:17   16:0    128KB
+                                                          2       4MB    65:22   21:18   17:0    256KB
+                                                          3       8MB    65:23   22:19   18:0    512KB
+                                                          4      16MB    65:24   23:20   19:0      1MB
+                                                          5      32MB    65:25   24:21   20:0      2MB
+                                                          6      64MB    65:26   25:22   21:0      4MB
+                                                          7     128MB    65:27   26:23   22:0      8MB
+                                                          8     256MB  ** not in pass 1
+                                                          9     512MB  ** not in pass 1
+                                                         10       1GB  ** not in pass 1
+                                                         11       2GB  ** not in pass 1
+                                                         12       4GB  ** not in pass 1
+                                                         13       8GB  ** not in pass 1
+
+                                                         *The SRIO Transaction Address
+                                                         The entry select bits is the X that  select an
+                                                         SRIOMAINT(0..1)_BAR1_IDXX entry.
+
+                                                         In O63 pass 2, BARSIZE is 4 bits (6:3 in this
+                                                         CSR), and BARSIZE values 8-13 are implemented,
+                                                         providing a total possible BAR1 size range from
+                                                         1MB up to 8GB. */
+	uint32_t addr66                       : 2;  /**< SRIO Address 65:64 */
+	uint32_t enable                       : 1;  /**< Enable BAR1 Access */
+#else
+	uint32_t enable                       : 1;
+	uint32_t addr66                       : 2;
+	uint32_t barsize                      : 3;
+	uint32_t reserved_6_19                : 14;
+	uint32_t addr32                       : 12;
+#endif
+	} cn63xxp1;
+	struct cvmx_sriomaintx_m2s_bar1_start1_s cn66xx;
+};
+typedef union cvmx_sriomaintx_m2s_bar1_start1 cvmx_sriomaintx_m2s_bar1_start1_t;
+
+/**
+ * cvmx_sriomaint#_m2s_bar2_start
+ *
+ * SRIOMAINT_M2S_BAR2_START = SRIO Device to BAR2 Start
+ *
+ * The starting SRIO address to forwarded to the BAR2 Memory Space.
+ *
+ * Notes:
+ * This register specifies the SRIO Address mapped to the BAR2 Space.  If ENABLE is set and the
+ *  address bits do not match and other enabled BAR address and match the BAR2 addresses then the SRIO
+ *  Memory transactions will map to Octeon BAR2 Memory Space.  34-bit address transactions require
+ *  ADDR66, ADDR64 and ADDR48 fields set to zero and supplies zeros for unused addresses 40:34.
+ *  50-bit address transactions a match of SRIO Address 49:41 and require all the other bits of ADDR64
+ *  and ADDR66 to be zero.  66-bit address transactions require matches of all valid address field
+ *  bits.  The ADDR32/48/64/66 fields of this register are writeable over SRIO if the
+ *  SRIO(0,2..3)_ACC_CTRL.DENY_ADR2 bit is zero.  The remaining fields are writeable over SRIO if the
+ *  SRIO(0,2..3)_ACC_CTRL.DENY_BAR2 bit is zero.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_M2S_BAR2_START        hclk    hrst_n
+ */
+union cvmx_sriomaintx_m2s_bar2_start {
+	uint32_t u32;
+	struct cvmx_sriomaintx_m2s_bar2_start_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t addr64                       : 16; /**< SRIO Address 63:48 */
+	uint32_t addr48                       : 7;  /**< SRIO Address 47:41 */
+	uint32_t reserved_6_8                 : 3;
+	uint32_t esx                          : 2;  /**< Endian Swap Mode used for SRIO 34-bit access.
+                                                         For 50/66-bit assesses Endian Swap is determine
+                                                         by ESX XOR'd with SRIO Addr 39:38.
+                                                         0 = No Swap
+                                                         1 = 64-bit Swap Bytes [ABCD_EFGH] -> [HGFE_DCBA]
+                                                         2 = 32-bit Swap Words [ABCD_EFGH] -> [DCBA_HGFE]
+                                                         3 = 32-bit Word Exch  [ABCD_EFGH] -> [EFGH_ABCD] */
+	uint32_t cax                          : 1;  /**< Cacheable Access Mode.  When set transfer is
+                                                         cached.  This bit is used for SRIO 34-bit access.
+                                                         For 50/66-bit accessas NCA is determine by CAX
+                                                         XOR'd with SRIO Addr 40. */
+	uint32_t addr66                       : 2;  /**< SRIO Address 65:64 */
+	uint32_t enable                       : 1;  /**< Enable BAR2 Access */
+#else
+	uint32_t enable                       : 1;
+	uint32_t addr66                       : 2;
+	uint32_t cax                          : 1;
+	uint32_t esx                          : 2;
+	uint32_t reserved_6_8                 : 3;
+	uint32_t addr48                       : 7;
+	uint32_t addr64                       : 16;
+#endif
+	} s;
+	struct cvmx_sriomaintx_m2s_bar2_start_s cn63xx;
+	struct cvmx_sriomaintx_m2s_bar2_start_s cn63xxp1;
+	struct cvmx_sriomaintx_m2s_bar2_start_s cn66xx;
+};
+typedef union cvmx_sriomaintx_m2s_bar2_start cvmx_sriomaintx_m2s_bar2_start_t;
+
+/**
+ * cvmx_sriomaint#_mac_ctrl
+ *
+ * SRIOMAINT_MAC_CTRL = SRIO MAC Control
+ *
+ * Control for MAC Features
+ *
+ * Notes:
+ * This register enables MAC optimizations that may not be supported by all SRIO devices.  The
+ *  default values should be supported.  This register can be changed at any time while the MAC is
+ *  out of reset.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_MAC_CTRL      hclk    hrst_n
+ */
+union cvmx_sriomaintx_mac_ctrl {
+	uint32_t u32;
+	struct cvmx_sriomaintx_mac_ctrl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t reserved_21_31               : 11;
+	uint32_t sec_spf                      : 1;  /**< Send all Incoming Packets matching Secondary ID to
+                                                         RX Soft Packet FIFO.  This bit is ignored if
+                                                         RX_SPF is set. */
+	uint32_t ack_zero                     : 1;  /**< Generate ACKs for all incoming Zero Byte packets.
+                                                         Default behavior is to issue a NACK.  Regardless
+                                                         of this setting the SRIO(0,2..3)_INT_REG.ZERO_PKT
+                                                         interrupt is generated.
+                                                         SRIO(0,2..3)_INT_REG. */
+	uint32_t rx_spf                       : 1;  /**< Route all received packets to RX Soft Packet FIFO.
+                                                         No logical layer ERB Errors will be reported.
+                                                         Used for Diagnostics Only. */
+	uint32_t eop_mrg                      : 1;  /**< Transmitted Packets can eliminate EOP Symbol on
+                                                         back to back packets. */
+	uint32_t type_mrg                     : 1;  /**< Allow STYPE Merging on Transmit. */
+	uint32_t lnk_rtry                     : 16; /**< Number of times MAC will reissue Link Request
+                                                         after timeout.  If retry count is exceeded Fatal
+                                                         Port Error will occur (see SRIO(0,2..3)_INT_REG.F_ERROR) */
+#else
+	uint32_t lnk_rtry                     : 16;
+	uint32_t type_mrg                     : 1;
+	uint32_t eop_mrg                      : 1;
+	uint32_t rx_spf                       : 1;
+	uint32_t ack_zero                     : 1;
+	uint32_t sec_spf                      : 1;
+	uint32_t reserved_21_31               : 11;
+#endif
+	} s;
+	struct cvmx_sriomaintx_mac_ctrl_cn63xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t reserved_20_31               : 12;
+	uint32_t ack_zero                     : 1;  /**< Generate ACKs for all incoming Zero Byte packets.
+                                                         Default behavior is to issue a NACK.  Regardless
+                                                         of this setting the SRIO(0..1)_INT_REG.ZERO_PKT
+                                                         interrupt is generated.
+                                                         SRIO(0..1)_INT_REG. */
+	uint32_t rx_spf                       : 1;  /**< Route all received packets to RX Soft Packet FIFO.
+                                                         No logical layer ERB Errors will be reported.
+                                                         Used for Diagnostics Only. */
+	uint32_t eop_mrg                      : 1;  /**< Transmitted Packets can eliminate EOP Symbol on
+                                                         back to back packets. */
+	uint32_t type_mrg                     : 1;  /**< Allow STYPE Merging on Transmit. */
+	uint32_t lnk_rtry                     : 16; /**< Number of times MAC will reissue Link Request
+                                                         after timeout.  If retry count is exceeded Fatal
+                                                         Port Error will occur (see SRIO(0..1)_INT_REG.F_ERROR) */
+#else
+	uint32_t lnk_rtry                     : 16;
+	uint32_t type_mrg                     : 1;
+	uint32_t eop_mrg                      : 1;
+	uint32_t rx_spf                       : 1;
+	uint32_t ack_zero                     : 1;
+	uint32_t reserved_20_31               : 12;
+#endif
+	} cn63xx;
+	struct cvmx_sriomaintx_mac_ctrl_s     cn66xx;
+};
+typedef union cvmx_sriomaintx_mac_ctrl cvmx_sriomaintx_mac_ctrl_t;
+
+/**
+ * cvmx_sriomaint#_pe_feat
+ *
+ * SRIOMAINT_PE_FEAT = SRIO Processing Element Features
+ *
+ * The Supported Processing Element Features.
+ *
+ * Notes:
+ * The Processing Element Feature register describes the major functionality provided by the SRIO
+ *  device.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_PE_FEAT       hclk    hrst_n
+ */
+union cvmx_sriomaintx_pe_feat {
+	uint32_t u32;
+	struct cvmx_sriomaintx_pe_feat_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t bridge                       : 1;  /**< Bridge Functions not supported. */
+	uint32_t memory                       : 1;  /**< PE contains addressable memory. */
+	uint32_t proc                         : 1;  /**< PE contains a local processor. */
+	uint32_t switchf                      : 1;  /**< Switch Functions not supported. */
+	uint32_t mult_prt                     : 1;  /**< Multiport Functions not supported. */
+	uint32_t reserved_7_26                : 20;
+	uint32_t suppress                     : 1;  /**< Error Recovery Suppression not supported. */
+	uint32_t crf                          : 1;  /**< Critical Request Flow not supported. */
+	uint32_t lg_tran                      : 1;  /**< Large Transport (16-bit Device IDs) supported. */
+	uint32_t ex_feat                      : 1;  /**< Extended Feature Pointer is valid. */
+	uint32_t ex_addr                      : 3;  /**< PE supports 66, 50 and 34-bit addresses.
+                                                         [2:1] are a RO copy of SRIO*_IP_FEATURE[A66,A50]. */
+#else
+	uint32_t ex_addr                      : 3;
+	uint32_t ex_feat                      : 1;
+	uint32_t lg_tran                      : 1;
+	uint32_t crf                          : 1;
+	uint32_t suppress                     : 1;
+	uint32_t reserved_7_26                : 20;
+	uint32_t mult_prt                     : 1;
+	uint32_t switchf                      : 1;
+	uint32_t proc                         : 1;
+	uint32_t memory                       : 1;
+	uint32_t bridge                       : 1;
+#endif
+	} s;
+	struct cvmx_sriomaintx_pe_feat_s      cn63xx;
+	struct cvmx_sriomaintx_pe_feat_s      cn63xxp1;
+	struct cvmx_sriomaintx_pe_feat_s      cn66xx;
+};
+typedef union cvmx_sriomaintx_pe_feat cvmx_sriomaintx_pe_feat_t;
+
+/**
+ * cvmx_sriomaint#_pe_llc
+ *
+ * SRIOMAINT_PE_LLC = SRIO Processing Element Logical Layer Control
+ *
+ * Addresses supported by the SRIO Device.
+ *
+ * Notes:
+ * The Processing Element Logical Layer is used for general configuration for the logical interface.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_PE_LLC        hclk    hrst_n
+ */
+union cvmx_sriomaintx_pe_llc {
+	uint32_t u32;
+	struct cvmx_sriomaintx_pe_llc_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t reserved_3_31                : 29;
+	uint32_t ex_addr                      : 3;  /**< Controls the number of address bits generated by
+                                                         PE as a source and processed by the PE as a
+                                                         target of an operation.
+                                                          001 = 34-bit Addresses
+                                                          010 = 50-bit Addresses
+                                                          100 = 66-bit Addresses
+                                                          All other encodings are reserved. */
+#else
+	uint32_t ex_addr                      : 3;
+	uint32_t reserved_3_31                : 29;
+#endif
+	} s;
+	struct cvmx_sriomaintx_pe_llc_s       cn63xx;
+	struct cvmx_sriomaintx_pe_llc_s       cn63xxp1;
+	struct cvmx_sriomaintx_pe_llc_s       cn66xx;
+};
+typedef union cvmx_sriomaintx_pe_llc cvmx_sriomaintx_pe_llc_t;
+
+/**
+ * cvmx_sriomaint#_port_0_ctl
+ *
+ * SRIOMAINT_PORT_0_CTL = SRIO Port 0 Control
+ *
+ * Port 0 Control
+ *
+ * Notes:
+ * This register contains assorted control bits.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_PORT_0_CTL    hclk    hrst_n
+ */
+union cvmx_sriomaintx_port_0_ctl {
+	uint32_t u32;
+	struct cvmx_sriomaintx_port_0_ctl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t pt_width                     : 2;  /**< Hardware Port Width.
+                                                         00 = One Lane supported.
+                                                         01 = One/Four Lanes supported.
+                                                         10 = One/Two Lanes supported.
+                                                         11 = One/Two/Four Lanes supported.
+                                                         This value is a copy of SRIO*_IP_FEATURE[PT_WIDTH]
+                                                         limited by the number of lanes the MAC has. */
+	uint32_t it_width                     : 3;  /**< Initialized Port Width
+                                                         000 = Single-lane, Lane 0
+                                                         001 = Single-lane, Lane 1 or 2
+                                                         010 = Four-lane
+                                                         011 = Two-lane
+                                                         111 = Link Uninitialized
+                                                         Others = Reserved */
+	uint32_t ov_width                     : 3;  /**< Override Port Width.  Writing this register causes
+                                                         the port to reinitialize.
+                                                         000 = No Override all lanes possible
+                                                         001 = Reserved
+                                                         010 = Force Single-lane, Lane 0
+                                                               If Ln 0 is unavailable try Ln 2 then Ln 1
+                                                         011 = Force Single-lane, Lane 2
+                                                               If Ln 2 is unavailable try Ln 1 then Ln 0
+                                                         100 = Reserved
+                                                         101 = Enable Two-lane, Disable Four-Lane
+                                                         110 = Enable Four-lane, Disable Two-Lane
+                                                         111 = All lanes sizes enabled */
+	uint32_t disable                      : 1;  /**< Port Disable.  Setting this bit should disable
+                                                         SERDES drivers and receivers.  On this chip it
+                                                         disables SERDES receivers only. */
+	uint32_t o_enable                     : 1;  /**< Port Output Enable.  When cleared, port will
+                                                         generate control symbols and respond to
+                                                         maintenance transactions only.  When set, all
+                                                         transactions are allowed. */
+	uint32_t i_enable                     : 1;  /**< Port Input Enable.  When cleared, port will
+                                                         generate control symbols and respond to
+                                                         maintenance packets only.  All other packets will
+                                                         not be accepted. */
+	uint32_t dis_err                      : 1;  /**< Disable Error Checking.  Diagnostic Only. */
+	uint32_t mcast                        : 1;  /**< Reserved. */
+	uint32_t reserved_18_18               : 1;
+	uint32_t enumb                        : 1;  /**< Enumeration Boundry. SW can use this bit to
+                                                         determine port enumeration. */
+	uint32_t reserved_16_16               : 1;
+	uint32_t ex_width                     : 2;  /**< Extended Port Width not supported. */
+	uint32_t ex_stat                      : 2;  /**< Extended Port Width Status. 00 = not supported */
+	uint32_t suppress                     : 8;  /**< Retransmit Suppression Mask.  CRF not Supported. */
+	uint32_t stp_port                     : 1;  /**< Stop on Failed Port.  This bit is used with the
+                                                         DROP_PKT bit to force certain behavior when the
+                                                         Error Rate Failed Threshold has been met or
+                                                         exceeded. */
+	uint32_t drop_pkt                     : 1;  /**< Drop on Failed Port.  This bit is used with the
+                                                         STP_PORT bit to force certain behavior when the
+                                                         Error Rate Failed Threshold has been met or
+                                                         exceeded. */
+	uint32_t prt_lock                     : 1;  /**< When this bit is cleared, the packets that may be
+                                                         received and issued are controlled by the state of
+                                                         the O_ENABLE and I_ENABLE bits.  When this bit is
+                                                         set, this port is stopped and is not enabled to
+                                                         receive any packets; the input port can still
+                                                         follow the training procedure and can still send
+                                                         respond to link-requests; all received packets
+                                                         return packet-not-accepted control symbols to
+                                                         force an error condition to be signaled by the
+                                                         sending device.  The O_ENABLE bit should also be
+                                                         cleared to disable packet output. */
+	uint32_t pt_type                      : 1;  /**< Port Type.  1 = Serial port. */
+#else
+	uint32_t pt_type                      : 1;
+	uint32_t prt_lock                     : 1;
+	uint32_t drop_pkt                     : 1;
+	uint32_t stp_port                     : 1;
+	uint32_t suppress                     : 8;
+	uint32_t ex_stat                      : 2;
+	uint32_t ex_width                     : 2;
+	uint32_t reserved_16_16               : 1;
+	uint32_t enumb                        : 1;
+	uint32_t reserved_18_18               : 1;
+	uint32_t mcast                        : 1;
+	uint32_t dis_err                      : 1;
+	uint32_t i_enable                     : 1;
+	uint32_t o_enable                     : 1;
+	uint32_t disable                      : 1;
+	uint32_t ov_width                     : 3;
+	uint32_t it_width                     : 3;
+	uint32_t pt_width                     : 2;
+#endif
+	} s;
+	struct cvmx_sriomaintx_port_0_ctl_s   cn63xx;
+	struct cvmx_sriomaintx_port_0_ctl_s   cn63xxp1;
+	struct cvmx_sriomaintx_port_0_ctl_s   cn66xx;
+};
+typedef union cvmx_sriomaintx_port_0_ctl cvmx_sriomaintx_port_0_ctl_t;
+
+/**
+ * cvmx_sriomaint#_port_0_ctl2
+ *
+ * SRIOMAINT_PORT_0_CTL2 = SRIO Port 0 Control 2
+ *
+ * Port 0 Control 2
+ *
+ * Notes:
+ * These registers are accessed when a local processor or an external device wishes to examine the
+ *  port baudrate information.  The Automatic Baud Rate Feature is not available on this device.  The
+ *  SUP_* and ENB_* fields are set directly by the QLM_SPD bits as a reference but otherwise have
+ *  no effect.  WARNING:  Writes to this register will reinitialize the SRIO link.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_PORT_0_CTL2   hclk    hrst_n
+ */
+union cvmx_sriomaintx_port_0_ctl2 {
+	uint32_t u32;
+	struct cvmx_sriomaintx_port_0_ctl2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t sel_baud                     : 4;  /**< Link Baud Rate Selected.
+                                                           0000 - No rate selected
+                                                           0001 - 1.25 GBaud
+                                                           0010 - 2.5 GBaud
+                                                           0011 - 3.125 GBaud
+                                                           0100 - 5.0 GBaud
+                                                           0101 - 6.25 GBaud (reserved)
+                                                           0110 - 0b1111 - Reserved
+                                                         Indicates the speed of the interface SERDES lanes
+                                                         (selected by the QLM*_SPD straps). */
+	uint32_t baud_sup                     : 1;  /**< Automatic Baud Rate Discovery not supported. */
+	uint32_t baud_enb                     : 1;  /**< Auto Baud Rate Discovery Enable. */
+	uint32_t sup_125g                     : 1;  /**< 1.25GB Rate Operation supported.
+                                                         Set when the interface SERDES lanes are operating
+                                                         at 1.25 Gbaud (as selected by QLM*_SPD straps). */
+	uint32_t enb_125g                     : 1;  /**< 1.25GB Rate Operation enable.
+                                                         Reset to 1 when the interface SERDES lanes are
+                                                         operating at 1.25 Gbaud (as selected by QLM*_SPD
+                                                         straps). Reset to 0 otherwise. */
+	uint32_t sup_250g                     : 1;  /**< 2.50GB Rate Operation supported.
+                                                         Set when the interface SERDES lanes are operating
+                                                         at 2.5 Gbaud (as selected by QLM*_SPD straps). */
+	uint32_t enb_250g                     : 1;  /**< 2.50GB Rate Operation enable.
+                                                         Reset to 1 when the interface SERDES lanes are
+                                                         operating at 2.5 Gbaud (as selected by QLM*_SPD
+                                                         straps). Reset to 0 otherwise. */
+	uint32_t sup_312g                     : 1;  /**< 3.125GB Rate Operation supported.
+                                                         Set when the interface SERDES lanes are operating
+                                                         at 3.125 Gbaud (as selected by QLM*_SPD straps). */
+	uint32_t enb_312g                     : 1;  /**< 3.125GB Rate Operation enable.
+                                                         Reset to 1 when the interface SERDES lanes are
+                                                         operating at 3.125 Gbaud (as selected by QLM*_SPD
+                                                         straps). Reset to 0 otherwise. */
+	uint32_t sub_500g                     : 1;  /**< 5.0GB Rate Operation supported.
+                                                         Set when the interface SERDES lanes are operating
+                                                         at 5.0 Gbaud (as selected by QLM*_SPD straps). */
+	uint32_t enb_500g                     : 1;  /**< 5.0GB Rate Operation enable.
+                                                         Reset to 1 when the interface SERDES lanes are
+                                                         operating at 5.0 Gbaud (as selected by QLM*_SPD
+                                                         straps). Reset to 0 otherwise. */
+	uint32_t sup_625g                     : 1;  /**< 6.25GB Rate Operation (not supported). */
+	uint32_t enb_625g                     : 1;  /**< 6.25GB Rate Operation enable. */
+	uint32_t reserved_2_15                : 14;
+	uint32_t tx_emph                      : 1;  /**< Indicates whether is port is able to transmit
+                                                         commands to control the transmit emphasis in the
+                                                         connected port. */
+	uint32_t emph_en                      : 1;  /**< Controls whether a port may adjust the
+                                                         transmit emphasis in the connected port.  This bit
+                                                         should be cleared for normal operation. */
+#else
+	uint32_t emph_en                      : 1;
+	uint32_t tx_emph                      : 1;
+	uint32_t reserved_2_15                : 14;
+	uint32_t enb_625g                     : 1;
+	uint32_t sup_625g                     : 1;
+	uint32_t enb_500g                     : 1;
+	uint32_t sub_500g                     : 1;
+	uint32_t enb_312g                     : 1;
+	uint32_t sup_312g                     : 1;
+	uint32_t enb_250g                     : 1;
+	uint32_t sup_250g                     : 1;
+	uint32_t enb_125g                     : 1;
+	uint32_t sup_125g                     : 1;
+	uint32_t baud_enb                     : 1;
+	uint32_t baud_sup                     : 1;
+	uint32_t sel_baud                     : 4;
+#endif
+	} s;
+	struct cvmx_sriomaintx_port_0_ctl2_s  cn63xx;
+	struct cvmx_sriomaintx_port_0_ctl2_s  cn63xxp1;
+	struct cvmx_sriomaintx_port_0_ctl2_s  cn66xx;
+};
+typedef union cvmx_sriomaintx_port_0_ctl2 cvmx_sriomaintx_port_0_ctl2_t;
+
+/**
+ * cvmx_sriomaint#_port_0_err_stat
+ *
+ * SRIOMAINT_PORT_0_ERR_STAT = SRIO Port 0 Error and Status
+ *
+ * Port 0 Error and Status
+ *
+ * Notes:
+ * This register displays port error and status information.  Several port error conditions are
+ *  captured here and must be cleared by writing 1's to the individual bits.
+ *  Bits are R/W on 65/66xx pass 1 and R/W1C on pass 1.2
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_PORT_0_ERR_STAT       hclk    hrst_n
+ */
+union cvmx_sriomaintx_port_0_err_stat {
+	uint32_t u32;
+	struct cvmx_sriomaintx_port_0_err_stat_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t reserved_27_31               : 5;
+	uint32_t pkt_drop                     : 1;  /**< Output Packet Dropped. */
+	uint32_t o_fail                       : 1;  /**< Output Port has encountered a failure condition,
+                                                         meaning the port's failed error threshold has
+                                                         reached SRIOMAINT(0,2..3)_ERB_ERR_RATE_THR.ER_FAIL value. */
+	uint32_t o_dgrad                      : 1;  /**< Output Port has encountered a degraded condition,
+                                                         meaning the port's degraded threshold has
+                                                         reached SRIOMAINT(0,2..3)_ERB_ERR_RATE_THR.ER_DGRAD
+                                                         value. */
+	uint32_t reserved_21_23               : 3;
+	uint32_t o_retry                      : 1;  /**< Output Retry Encountered.  This bit is set when
+                                                         bit 18 is set. */
+	uint32_t o_rtried                     : 1;  /**< Output Port has received a packet-retry condition
+                                                         and cannot make forward progress.  This bit is set
+                                                         when  bit 18 is set and is cleared when a packet-
+                                                         accepted or a packet-not-accepted control symbol
+                                                         is received. */
+	uint32_t o_sm_ret                     : 1;  /**< Output Port State Machine has received a
+                                                         packet-retry control symbol and is retrying the
+                                                         packet. */
+	uint32_t o_error                      : 1;  /**< Output Error Encountered and possibly recovered
+                                                         from.  This sticky bit is set with bit 16. */
+	uint32_t o_sm_err                     : 1;  /**< Output Port State Machine has encountered an
+                                                         error. */
+	uint32_t reserved_11_15               : 5;
+	uint32_t i_sm_ret                     : 1;  /**< Input Port State Machine has received a
+                                                         packet-retry control symbol and is retrying the
+                                                         packet. */
+	uint32_t i_error                      : 1;  /**< Input Error Encountered and possibly recovered
+                                                         from.  This sticky bit is set with bit 8. */
+	uint32_t i_sm_err                     : 1;  /**< Input Port State Machine has encountered an
+                                                         error. */
+	uint32_t reserved_5_7                 : 3;
+	uint32_t pt_write                     : 1;  /**< Port has encountered a condition which required it
+                                                         initiate a Maintenance Port-Write Operation.
+                                                         Never set by hardware. */
+	uint32_t reserved_3_3                 : 1;
+	uint32_t pt_error                     : 1;  /**< Input or Output Port has encountered an
+                                                         unrecoverable error condition. */
+	uint32_t pt_ok                        : 1;  /**< Input or Output Port are intitialized and the port
+                                                         is exchanging error free control symbols with
+                                                         attached device. */
+	uint32_t pt_uinit                     : 1;  /**< Port is uninitialized.  This bit and bit 1 are
+                                                         mutually exclusive. */
+#else
+	uint32_t pt_uinit                     : 1;
+	uint32_t pt_ok                        : 1;
+	uint32_t pt_error                     : 1;
+	uint32_t reserved_3_3                 : 1;
+	uint32_t pt_write                     : 1;
+	uint32_t reserved_5_7                 : 3;
+	uint32_t i_sm_err                     : 1;
+	uint32_t i_error                      : 1;
+	uint32_t i_sm_ret                     : 1;
+	uint32_t reserved_11_15               : 5;
+	uint32_t o_sm_err                     : 1;
+	uint32_t o_error                      : 1;
+	uint32_t o_sm_ret                     : 1;
+	uint32_t o_rtried                     : 1;
+	uint32_t o_retry                      : 1;
+	uint32_t reserved_21_23               : 3;
+	uint32_t o_dgrad                      : 1;
+	uint32_t o_fail                       : 1;
+	uint32_t pkt_drop                     : 1;
+	uint32_t reserved_27_31               : 5;
+#endif
+	} s;
+	struct cvmx_sriomaintx_port_0_err_stat_s cn63xx;
+	struct cvmx_sriomaintx_port_0_err_stat_s cn63xxp1;
+	struct cvmx_sriomaintx_port_0_err_stat_s cn66xx;
+};
+typedef union cvmx_sriomaintx_port_0_err_stat cvmx_sriomaintx_port_0_err_stat_t;
+
+/**
+ * cvmx_sriomaint#_port_0_link_req
+ *
+ * SRIOMAINT_PORT_0_LINK_REQ = SRIO Port 0 Link Request
+ *
+ * Port 0 Manual Link Request
+ *
+ * Notes:
+ * Writing this register generates the link request symbol or eight device reset symbols.   The
+ *  progress of the request can be determined by reading SRIOMAINT(0,2..3)_PORT_0_LINK_RESP.  Only a single
+ *  request should be generated at a time.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_PORT_0_LINK_REQ       hclk    hrst_n
+ */
+union cvmx_sriomaintx_port_0_link_req {
+	uint32_t u32;
+	struct cvmx_sriomaintx_port_0_link_req_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t reserved_3_31                : 29;
+	uint32_t cmd                          : 3;  /**< Link Request Command.
+                                                         011 - Reset Device
+                                                         100 - Link Request
+                                                         All other values reserved. */
+#else
+	uint32_t cmd                          : 3;
+	uint32_t reserved_3_31                : 29;
+#endif
+	} s;
+	struct cvmx_sriomaintx_port_0_link_req_s cn63xx;
+	struct cvmx_sriomaintx_port_0_link_req_s cn66xx;
+};
+typedef union cvmx_sriomaintx_port_0_link_req cvmx_sriomaintx_port_0_link_req_t;
+
+/**
+ * cvmx_sriomaint#_port_0_link_resp
+ *
+ * SRIOMAINT_PORT_0_LINK_RESP = SRIO Port 0 Link Response
+ *
+ * Port 0 Manual Link Response
+ *
+ * Notes:
+ * This register only returns responses generated by writes to SRIOMAINT(0,2..3)_PORT_0_LINK_REQ.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_PORT_0_LINK_RESP      hclk    hrst_n
+ */
+union cvmx_sriomaintx_port_0_link_resp {
+	uint32_t u32;
+	struct cvmx_sriomaintx_port_0_link_resp_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t valid                        : 1;  /**< Link Response Valid.
+                                                         1 = Link Response Received or Reset Device
+                                                             Symbols Transmitted.  Value cleared on read.
+                                                         0 = No response received. */
+	uint32_t reserved_11_30               : 20;
+	uint32_t ackid                        : 6;  /**< AckID received from link response.
+                                                         Reset Device symbol response is always zero.
+                                                         Bit 10 is used for IDLE2 and always reads zero. */
+	uint32_t status                       : 5;  /**< Link Response Status.
+                                                         Status supplied by link response.
+                                                         Reset Device symbol response is always zero. */
+#else
+	uint32_t status                       : 5;
+	uint32_t ackid                        : 6;
+	uint32_t reserved_11_30               : 20;
+	uint32_t valid                        : 1;
+#endif
+	} s;
+	struct cvmx_sriomaintx_port_0_link_resp_s cn63xx;
+	struct cvmx_sriomaintx_port_0_link_resp_s cn66xx;
+};
+typedef union cvmx_sriomaintx_port_0_link_resp cvmx_sriomaintx_port_0_link_resp_t;
+
+/**
+ * cvmx_sriomaint#_port_0_local_ackid
+ *
+ * SRIOMAINT_PORT_0_LOCAL_ACKID = SRIO Port 0 Local AckID
+ *
+ * Port 0 Local AckID Control
+ *
+ * Notes:
+ * This register is typically only written when recovering from a failed link.  It may be read at any
+ *  time the MAC is out of reset.  Writes to the O_ACKID field will be used for both the O_ACKID and
+ *  E_ACKID.  Care must be taken to ensure that no packets are pending at the time of a write.  The
+ *  number of pending packets can be read in the TX_INUSE field of SRIO(0,2..3)_MAC_BUFFERS.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_PORT_0_LOCAL_ACKID    hclk    hrst_n
+ */
+union cvmx_sriomaintx_port_0_local_ackid {
+	uint32_t u32;
+	struct cvmx_sriomaintx_port_0_local_ackid_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t reserved_30_31               : 2;
+	uint32_t i_ackid                      : 6;  /**< Next Expected Inbound AckID.
+                                                         Bit 29 is used for IDLE2 and should be zero. */
+	uint32_t reserved_14_23               : 10;
+	uint32_t e_ackid                      : 6;  /**< Next Expected Unacknowledged AckID.
+                                                         Bit 13 is used for IDLE2 and should be zero. */
+	uint32_t reserved_6_7                 : 2;
+	uint32_t o_ackid                      : 6;  /**< Next Outgoing Packet AckID.
+                                                         Bit 5 is used for IDLE2 and should be zero. */
+#else
+	uint32_t o_ackid                      : 6;
+	uint32_t reserved_6_7                 : 2;
+	uint32_t e_ackid                      : 6;
+	uint32_t reserved_14_23               : 10;
+	uint32_t i_ackid                      : 6;
+	uint32_t reserved_30_31               : 2;
+#endif
+	} s;
+	struct cvmx_sriomaintx_port_0_local_ackid_s cn63xx;
+	struct cvmx_sriomaintx_port_0_local_ackid_s cn66xx;
+};
+typedef union cvmx_sriomaintx_port_0_local_ackid cvmx_sriomaintx_port_0_local_ackid_t;
+
+/**
+ * cvmx_sriomaint#_port_gen_ctl
+ *
+ * SRIOMAINT_PORT_GEN_CTL = SRIO Port General Control
+ *
+ * Port General Control
+ *
+ * Notes:
+ * Clk_Rst:        SRIOMAINT(0,2..3)_PORT_GEN_CTL  hclk    hrst_n
+ *
+ */
+union cvmx_sriomaintx_port_gen_ctl {
+	uint32_t u32;
+	struct cvmx_sriomaintx_port_gen_ctl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t host                         : 1;  /**< Host Device.
+                                                         The HOST reset value is based on corresponding
+                                                         MIO_RST_CTL*[PRTMODE].  HOST resets to 1 when
+                                                         this field selects RC (i.e. host) mode, else 0. */
+	uint32_t menable                      : 1;  /**< Master Enable.  Must be set for device to issue
+                                                         read, write, doorbell, message requests. */
+	uint32_t discover                     : 1;  /**< Discovered. The device has been discovered by the
+                                                         host responsible for initialization. */
+	uint32_t reserved_0_28                : 29;
+#else
+	uint32_t reserved_0_28                : 29;
+	uint32_t discover                     : 1;
+	uint32_t menable                      : 1;
+	uint32_t host                         : 1;
+#endif
+	} s;
+	struct cvmx_sriomaintx_port_gen_ctl_s cn63xx;
+	struct cvmx_sriomaintx_port_gen_ctl_s cn63xxp1;
+	struct cvmx_sriomaintx_port_gen_ctl_s cn66xx;
+};
+typedef union cvmx_sriomaintx_port_gen_ctl cvmx_sriomaintx_port_gen_ctl_t;
+
+/**
+ * cvmx_sriomaint#_port_lt_ctl
+ *
+ * SRIOMAINT_PORT_LT_CTL = SRIO Link Layer Timeout Control
+ *
+ * Link Layer Timeout Control
+ *
+ * Notes:
+ * This register controls the timeout for link layer transactions.  It is used as the timeout between
+ *  sending a packet (of any type) or link request to receiving the corresponding link acknowledge or
+ *  link-response.  Each count represents 200ns.  The minimum timeout period is the TIMEOUT x 200nS
+ *  and the maximum is twice that number.  A value less than 32 may not guarantee that all timeout
+ *  errors will be reported correctly.  When the timeout period expires the packet or link request is
+ *  dropped and the error is logged in the LNK_TOUT field of the SRIOMAINT(0,2..3)_ERB_ERR_DET register.  A
+ *  value of 0 in this register will allow the packet or link request to be issued but it will timeout
+ *  immediately.  This value is not recommended for normal operation.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_PORT_LT_CTL   hclk    hrst_n
+ */
+union cvmx_sriomaintx_port_lt_ctl {
+	uint32_t u32;
+	struct cvmx_sriomaintx_port_lt_ctl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t timeout                      : 24; /**< Timeout Value */
+	uint32_t reserved_0_7                 : 8;
+#else
+	uint32_t reserved_0_7                 : 8;
+	uint32_t timeout                      : 24;
+#endif
+	} s;
+	struct cvmx_sriomaintx_port_lt_ctl_s  cn63xx;
+	struct cvmx_sriomaintx_port_lt_ctl_s  cn63xxp1;
+	struct cvmx_sriomaintx_port_lt_ctl_s  cn66xx;
+};
+typedef union cvmx_sriomaintx_port_lt_ctl cvmx_sriomaintx_port_lt_ctl_t;
+
+/**
+ * cvmx_sriomaint#_port_mbh0
+ *
+ * SRIOMAINT_PORT_MBH0 = SRIO Port Maintenance Block Header 0
+ *
+ * Port Maintenance Block Header 0
+ *
+ * Notes:
+ * Clk_Rst:        SRIOMAINT(0,2..3)_PORT_MBH0     hclk    hrst_n
+ *
+ */
+union cvmx_sriomaintx_port_mbh0 {
+	uint32_t u32;
+	struct cvmx_sriomaintx_port_mbh0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t ef_ptr                       : 16; /**< Pointer to Error Management Block. */
+	uint32_t ef_id                        : 16; /**< Extended Feature ID (Generic Endpoint Device) */
+#else
+	uint32_t ef_id                        : 16;
+	uint32_t ef_ptr                       : 16;
+#endif
+	} s;
+	struct cvmx_sriomaintx_port_mbh0_s    cn63xx;
+	struct cvmx_sriomaintx_port_mbh0_s    cn63xxp1;
+	struct cvmx_sriomaintx_port_mbh0_s    cn66xx;
+};
+typedef union cvmx_sriomaintx_port_mbh0 cvmx_sriomaintx_port_mbh0_t;
+
+/**
+ * cvmx_sriomaint#_port_rt_ctl
+ *
+ * SRIOMAINT_PORT_RT_CTL = SRIO Logical Layer Timeout Control
+ *
+ * Logical Layer Timeout Control
+ *
+ * Notes:
+ * This register controls the timeout for logical layer transactions.  It is used under two
+ *  conditions.  First, it is used as the timeout period between sending a packet requiring a packet
+ *  response being sent to receiving the corresponding response.  This is used for all outgoing packet
+ *  types including memory, maintenance, doorbells and message operations.  When the timeout period
+ *  expires the packet is disgarded and the error is logged in the PKT_TOUT field of the
+ *  SRIOMAINT(0,2..3)_ERB_LT_ERR_DET register.  The second use of this register is as a timeout period
+ *  between incoming message segments of the same message.  If a message segment is received then the
+ *  MSG_TOUT field of the SRIOMAINT(0,2..3)_ERB_LT_ERR_DET register is set if the next segment has not been
+ *  received before the time expires.  In both cases, each count represents 200ns.  The minimum
+ *  timeout period is the TIMEOUT x 200nS and the maximum is twice that number.  A value less than 32
+ *  may not guarantee that all timeout errors will be reported correctly.  A value of 0 disables the
+ *  logical layer timeouts and is not recommended for normal operation.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_PORT_RT_CTL   hclk    hrst_n
+ */
+union cvmx_sriomaintx_port_rt_ctl {
+	uint32_t u32;
+	struct cvmx_sriomaintx_port_rt_ctl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t timeout                      : 24; /**< Timeout Value */
+	uint32_t reserved_0_7                 : 8;
+#else
+	uint32_t reserved_0_7                 : 8;
+	uint32_t timeout                      : 24;
+#endif
+	} s;
+	struct cvmx_sriomaintx_port_rt_ctl_s  cn63xx;
+	struct cvmx_sriomaintx_port_rt_ctl_s  cn63xxp1;
+	struct cvmx_sriomaintx_port_rt_ctl_s  cn66xx;
+};
+typedef union cvmx_sriomaintx_port_rt_ctl cvmx_sriomaintx_port_rt_ctl_t;
+
+/**
+ * cvmx_sriomaint#_port_ttl_ctl
+ *
+ * SRIOMAINT_PORT_TTL_CTL = SRIO Packet Time to Live Control
+ *
+ * Packet Time to Live
+ *
+ * Notes:
+ * This register controls the timeout for outgoing packets.  It is used to make sure packets are
+ *  being transmitted and acknowledged within a reasonable period of time.   The timeout value
+ *  corresponds to TIMEOUT x 200ns and a value of 0 disables the timer.  The actualy value of the
+ *  should be greater than the physical layer timout specified in SRIOMAINT(0,2..3)_PORT_LT_CTL and is
+ *  typically a less SRIOMAINT(0,2..3)_PORT_LT_CTL timeout than the response timeout specified in
+ *  SRIOMAINT(0,2..3)_PORT_RT_CTL.  A second application of this timer is to remove all the packets waiting
+ *  to be transmitted including those already in flight.  This may necessary in the case of a link
+ *  going down (see SRIO(0,2..3)_INT_REG.LINK_DWN).  This can accomplished by setting the TIMEOUT to small
+ *  value all so that all TX packets can be dropped.  In either case, when the timeout expires the TTL
+ *  interrupt is asserted, any packets currently being transmitted are dropped, the
+ *  SRIOMAINT(0,2..3)_TX_DROP.DROP bit is set (causing any scheduled packets to be dropped), the
+ *  SRIOMAINT(0,2..3)_TX_DROP.DROP_CNT is incremented for each packet and the SRIO output state is set to
+ *  IDLE (all errors are cleared).  Software must clear the SRIOMAINT(0,2..3)_TX_DROP.DROP bit to resume
+ *  transmitting packets.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_PORT_RT_CTL   hclk    hrst_n
+ */
+union cvmx_sriomaintx_port_ttl_ctl {
+	uint32_t u32;
+	struct cvmx_sriomaintx_port_ttl_ctl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t timeout                      : 24; /**< Timeout Value */
+	uint32_t reserved_0_7                 : 8;
+#else
+	uint32_t reserved_0_7                 : 8;
+	uint32_t timeout                      : 24;
+#endif
+	} s;
+	struct cvmx_sriomaintx_port_ttl_ctl_s cn63xx;
+	struct cvmx_sriomaintx_port_ttl_ctl_s cn66xx;
+};
+typedef union cvmx_sriomaintx_port_ttl_ctl cvmx_sriomaintx_port_ttl_ctl_t;
+
+/**
+ * cvmx_sriomaint#_pri_dev_id
+ *
+ * SRIOMAINT_PRI_DEV_ID = SRIO Primary Device ID
+ *
+ * Primary 8 and 16 bit Device IDs
+ *
+ * Notes:
+ * This register defines the primary 8 and 16 bit device IDs used for large and small transport.  An
+ *  optional secondary set of device IDs are located in SRIOMAINT(0,2..3)_SEC_DEV_ID.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_PRI_DEV_ID    hclk    hrst_n
+ */
+union cvmx_sriomaintx_pri_dev_id {
+	uint32_t u32;
+	struct cvmx_sriomaintx_pri_dev_id_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t reserved_24_31               : 8;
+	uint32_t id8                          : 8;  /**< Primary 8-bit Device ID */
+	uint32_t id16                         : 16; /**< Primary 16-bit Device ID */
+#else
+	uint32_t id16                         : 16;
+	uint32_t id8                          : 8;
+	uint32_t reserved_24_31               : 8;
+#endif
+	} s;
+	struct cvmx_sriomaintx_pri_dev_id_s   cn63xx;
+	struct cvmx_sriomaintx_pri_dev_id_s   cn63xxp1;
+	struct cvmx_sriomaintx_pri_dev_id_s   cn66xx;
+};
+typedef union cvmx_sriomaintx_pri_dev_id cvmx_sriomaintx_pri_dev_id_t;
+
+/**
+ * cvmx_sriomaint#_sec_dev_ctrl
+ *
+ * SRIOMAINT_SEC_DEV_CTRL = SRIO Secondary Device ID Control
+ *
+ * Control for Secondary Device IDs
+ *
+ * Notes:
+ * This register enables the secondary 8 and 16 bit device IDs used for large and small transport.
+ *  The corresponding secondary ID must be written before the ID is enabled.  The secondary IDs should
+ *  not be enabled if the values of the primary and secondary IDs are identical.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_SEC_DEV_CTRL  hclk    hrst_n
+ */
+union cvmx_sriomaintx_sec_dev_ctrl {
+	uint32_t u32;
+	struct cvmx_sriomaintx_sec_dev_ctrl_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t reserved_2_31                : 30;
+	uint32_t enable8                      : 1;  /**< Enable matches to secondary 8-bit Device ID */
+	uint32_t enable16                     : 1;  /**< Enable matches to secondary 16-bit Device ID */
+#else
+	uint32_t enable16                     : 1;
+	uint32_t enable8                      : 1;
+	uint32_t reserved_2_31                : 30;
+#endif
+	} s;
+	struct cvmx_sriomaintx_sec_dev_ctrl_s cn63xx;
+	struct cvmx_sriomaintx_sec_dev_ctrl_s cn63xxp1;
+	struct cvmx_sriomaintx_sec_dev_ctrl_s cn66xx;
+};
+typedef union cvmx_sriomaintx_sec_dev_ctrl cvmx_sriomaintx_sec_dev_ctrl_t;
+
+/**
+ * cvmx_sriomaint#_sec_dev_id
+ *
+ * SRIOMAINT_SEC_DEV_ID = SRIO Secondary Device ID
+ *
+ * Secondary 8 and 16 bit Device IDs
+ *
+ * Notes:
+ * This register defines the secondary 8 and 16 bit device IDs used for large and small transport.
+ *  The corresponding secondary ID must be written before the ID is enabled in the
+ *  SRIOMAINT(0,2..3)_SEC_DEV_CTRL register.  The primary set of device IDs are located in
+ *  SRIOMAINT(0,2..3)_PRI_DEV_ID register.  The secondary IDs should not be written to the same values as the
+ *  corresponding primary IDs.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_SEC_DEV_ID    hclk    hrst_n
+ */
+union cvmx_sriomaintx_sec_dev_id {
+	uint32_t u32;
+	struct cvmx_sriomaintx_sec_dev_id_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t reserved_24_31               : 8;
+	uint32_t id8                          : 8;  /**< Secondary 8-bit Device ID */
+	uint32_t id16                         : 16; /**< Secondary 16-bit Device ID */
+#else
+	uint32_t id16                         : 16;
+	uint32_t id8                          : 8;
+	uint32_t reserved_24_31               : 8;
+#endif
+	} s;
+	struct cvmx_sriomaintx_sec_dev_id_s   cn63xx;
+	struct cvmx_sriomaintx_sec_dev_id_s   cn63xxp1;
+	struct cvmx_sriomaintx_sec_dev_id_s   cn66xx;
+};
+typedef union cvmx_sriomaintx_sec_dev_id cvmx_sriomaintx_sec_dev_id_t;
+
+/**
+ * cvmx_sriomaint#_serial_lane_hdr
+ *
+ * SRIOMAINT_SERIAL_LANE_HDR = SRIO Serial Lane Header
+ *
+ * SRIO Serial Lane Header
+ *
+ * Notes:
+ * The error management extensions block header register contains the EF_PTR to the next EF_BLK and
+ *  the EF_ID that identifies this as the Serial Lane Status Block.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_SERIAL_LANE_HDR       hclk    hrst_n
+ */
+union cvmx_sriomaintx_serial_lane_hdr {
+	uint32_t u32;
+	struct cvmx_sriomaintx_serial_lane_hdr_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t ef_ptr                       : 16; /**< Pointer to the next block in the extended features
+                                                         data structure. */
+	uint32_t ef_id                        : 16;
+#else
+	uint32_t ef_id                        : 16;
+	uint32_t ef_ptr                       : 16;
+#endif
+	} s;
+	struct cvmx_sriomaintx_serial_lane_hdr_s cn63xx;
+	struct cvmx_sriomaintx_serial_lane_hdr_s cn63xxp1;
+	struct cvmx_sriomaintx_serial_lane_hdr_s cn66xx;
+};
+typedef union cvmx_sriomaintx_serial_lane_hdr cvmx_sriomaintx_serial_lane_hdr_t;
+
+/**
+ * cvmx_sriomaint#_src_ops
+ *
+ * SRIOMAINT_SRC_OPS = SRIO Source Operations
+ *
+ * The logical operations initiated by the Octeon.
+ *
+ * Notes:
+ * The logical operations initiated by the Cores.   The Source OPs register shows the operations
+ *  specified in the SRIO(0,2..3)_IP_FEATURE.OPS register.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_SRC_OPS       hclk    hrst_n
+ */
+union cvmx_sriomaintx_src_ops {
+	uint32_t u32;
+	struct cvmx_sriomaintx_src_ops_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t gsm_read                     : 1;  /**< PE does not support Read Home operations.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<31>] */
+	uint32_t i_read                       : 1;  /**< PE does not support Instruction Read.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<30>] */
+	uint32_t rd_own                       : 1;  /**< PE does not support Read for Ownership.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<29>] */
+	uint32_t d_invald                     : 1;  /**< PE does not support Data Cache Invalidate.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<28>] */
+	uint32_t castout                      : 1;  /**< PE does not support Castout Operations.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<27>] */
+	uint32_t d_flush                      : 1;  /**< PE does not support Data Cache Flush.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<26>] */
+	uint32_t io_read                      : 1;  /**< PE does not support IO Read.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<25>] */
+	uint32_t i_invald                     : 1;  /**< PE does not support Instruction Cache Invalidate.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<24>] */
+	uint32_t tlb_inv                      : 1;  /**< PE does not support TLB Entry Invalidate.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<23>] */
+	uint32_t tlb_invs                     : 1;  /**< PE does not support TLB Entry Invalidate Sync.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<22>] */
+	uint32_t reserved_16_21               : 6;
+	uint32_t read                         : 1;  /**< PE can support Nread operations.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<15>] */
+	uint32_t write                        : 1;  /**< PE can support Nwrite operations.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<14>] */
+	uint32_t swrite                       : 1;  /**< PE can support Swrite operations.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<13>] */
+	uint32_t write_r                      : 1;  /**< PE can support Write with Response operations.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<12>] */
+	uint32_t msg                          : 1;  /**< PE can support Data Message operations.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<11>] */
+	uint32_t doorbell                     : 1;  /**< PE can support Doorbell operations.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<10>] */
+	uint32_t compswap                     : 1;  /**< PE does not support Atomic Compare and Swap.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<9>] */
+	uint32_t testswap                     : 1;  /**< PE does not support Atomic Test and Swap.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<8>] */
+	uint32_t atom_inc                     : 1;  /**< PE can support Atomic increment operations.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<7>] */
+	uint32_t atom_dec                     : 1;  /**< PE can support Atomic decrement operations.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<6>] */
+	uint32_t atom_set                     : 1;  /**< PE can support Atomic set operations.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<5>] */
+	uint32_t atom_clr                     : 1;  /**< PE can support Atomic clear operations.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<4>] */
+	uint32_t atom_swp                     : 1;  /**< PE does not support Atomic Swap.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<3>] */
+	uint32_t port_wr                      : 1;  /**< PE can Port Write operations.
+                                                         This is a RO copy of SRIO*_IP_FEATURE[OPS<2>] */
+	uint32_t reserved_0_1                 : 2;
+#else
+	uint32_t reserved_0_1                 : 2;
+	uint32_t port_wr                      : 1;
+	uint32_t atom_swp                     : 1;
+	uint32_t atom_clr                     : 1;
+	uint32_t atom_set                     : 1;
+	uint32_t atom_dec                     : 1;
+	uint32_t atom_inc                     : 1;
+	uint32_t testswap                     : 1;
+	uint32_t compswap                     : 1;
+	uint32_t doorbell                     : 1;
+	uint32_t msg                          : 1;
+	uint32_t write_r                      : 1;
+	uint32_t swrite                       : 1;
+	uint32_t write                        : 1;
+	uint32_t read                         : 1;
+	uint32_t reserved_16_21               : 6;
+	uint32_t tlb_invs                     : 1;
+	uint32_t tlb_inv                      : 1;
+	uint32_t i_invald                     : 1;
+	uint32_t io_read                      : 1;
+	uint32_t d_flush                      : 1;
+	uint32_t castout                      : 1;
+	uint32_t d_invald                     : 1;
+	uint32_t rd_own                       : 1;
+	uint32_t i_read                       : 1;
+	uint32_t gsm_read                     : 1;
+#endif
+	} s;
+	struct cvmx_sriomaintx_src_ops_s      cn63xx;
+	struct cvmx_sriomaintx_src_ops_s      cn63xxp1;
+	struct cvmx_sriomaintx_src_ops_s      cn66xx;
+};
+typedef union cvmx_sriomaintx_src_ops cvmx_sriomaintx_src_ops_t;
+
+/**
+ * cvmx_sriomaint#_tx_drop
+ *
+ * SRIOMAINT_TX_DROP = SRIO MAC Outgoing Packet Drop
+ *
+ * Outging SRIO Packet Drop Control/Status
+ *
+ * Notes:
+ * This register controls and provides status for dropping outgoing SRIO packets.  The DROP bit
+ *  should only be cleared when no packets are currently being dropped.  This can be guaranteed by
+ *  clearing the SRIOMAINT(0,2..3)_PORT_0_CTL.O_ENABLE bit before changing the DROP bit and restoring the
+ *  O_ENABLE afterwards.
+ *
+ * Clk_Rst:        SRIOMAINT(0,2..3)_MAC_CTRL      hclk    hrst_n
+ */
+union cvmx_sriomaintx_tx_drop {
+	uint32_t u32;
+	struct cvmx_sriomaintx_tx_drop_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint32_t reserved_17_31               : 15;
+	uint32_t drop                         : 1;  /**< All outgoing packets are dropped.  Any packets
+                                                         requiring a response will return 1's after the
+                                                         SRIOMAINT(0,2..3)_PORT_RT_CTL Timeout expires.  This bit
+                                                         is set automatically when the TTL Timeout occurs
+                                                         or can be set by software and must always be
+                                                         cleared by software. */
+	uint32_t drop_cnt                     : 16; /**< Number of packets dropped by transmit logic.
+                                                         Packets are dropped whenever a packet is ready to
+                                                         be transmitted and a TTL Timeouts occur, the  DROP
+                                                         bit is set or the SRIOMAINT(0,2..3)_ERB_ERR_RATE_THR
+                                                         FAIL_TH has been reached and the DROP_PKT bit is
+                                                         set in SRIOMAINT(0,2..3)_PORT_0_CTL.  This counter wraps
+                                                         on overflow and is cleared only on reset. */
+#else
+	uint32_t drop_cnt                     : 16;
+	uint32_t drop                         : 1;
+	uint32_t reserved_17_31               : 15;
+#endif
+	} s;
+	struct cvmx_sriomaintx_tx_drop_s      cn63xx;
+	struct cvmx_sriomaintx_tx_drop_s      cn66xx;
+};
+typedef union cvmx_sriomaintx_tx_drop cvmx_sriomaintx_tx_drop_t;
+
+#endif
diff --git a/arch/mips/include/asm/octeon/cvmx-sriox-defs.h b/arch/mips/include/asm/octeon/cvmx-sriox-defs.h
index 5140f2d..514a401f 100644
--- a/arch/mips/include/asm/octeon/cvmx-sriox-defs.h
+++ b/arch/mips/include/asm/octeon/cvmx-sriox-defs.h
@@ -1529,6 +1529,7 @@ union cvmx_sriox_status_reg {
 	struct cvmx_sriox_status_reg_s cn63xxp1;
 	struct cvmx_sriox_status_reg_s cn66xx;
 };
+typedef union cvmx_sriox_status_reg cvmx_sriox_status_reg_t;
 
 union cvmx_sriox_tag_ctrl {
 	uint64_t u64;
diff --git a/arch/mips/include/asm/octeon/cvmx-sysinfo.h b/arch/mips/include/asm/octeon/cvmx-sysinfo.h
index 2131197..486a50a 100644
--- a/arch/mips/include/asm/octeon/cvmx-sysinfo.h
+++ b/arch/mips/include/asm/octeon/cvmx-sysinfo.h
@@ -149,4 +149,63 @@ extern int cvmx_sysinfo_minimal_initialize(void *phy_mem_desc_ptr,
 					   uint8_t board_rev_minor,
 					   uint32_t cpu_clock_hz);
 
+
+#define CVMX_MIPS_MAX_CORE_BITS (10)    /** Maximum # of bits to define cores */
+#define CVMX_MIPS_MAX_CORES     (1 << CVMX_MIPS_MAX_CORE_BITS)
+
+typedef uint64_t cvmx_coremask_holder_t;        /* basic type to hold the
+                                                   coremask bits */
+/* bits per holder */
+#define CVMX_COREMASK_HLDRSZ ((int)(sizeof(cvmx_coremask_holder_t) * 8))
+
+/** Maximum allowed cores per node */
+#define CVMX_COREMASK_MAX_CORES_PER_NODE (1 << CVMX_NODE_NO_SHIFT)
+
+/** Maximum number of bits actually used in the coremask */
+#define CVMX_MAX_USED_CORES_BMP (1 << (CVMX_NODE_NO_SHIFT + CVMX_NODE_BITS))
+
+/* the number of valid bits in and the mask of the most significant holder */
+#define CVMX_COREMASK_MSHLDR_NBITS (CVMX_MIPS_MAX_CORES % CVMX_COREMASK_HLDRSZ)
+#define CVMX_COREMASK_MSHLDR_MASK                                       \
+        ((CVMX_COREMASK_MSHLDR_NBITS) ?                                 \
+            (((cvmx_coremask_holder_t) 1 << CVMX_COREMASK_MSHLDR_NBITS) - 1) : \
+            ((cvmx_coremask_holder_t) -1))
+
+/* cvmx_coremask_t's size in cvmx_coremask_holder_t */
+#define CVMX_COREMASK_BMPSZ                                             \
+        ((int)(CVMX_MIPS_MAX_CORES / CVMX_COREMASK_HLDRSZ +             \
+                (CVMX_COREMASK_MSHLDR_NBITS != 0)))
+
+#define CVMX_COREMASK_BMP_NODE_CORE_IDX(node, core)     \
+        ((((node) << CVMX_NODE_NO_SHIFT) + (core)) / CVMX_COREMASK_HLDRSZ)
+
+/* cvmx_coremask_t */
+struct cvmx_coremask {
+        cvmx_coremask_holder_t coremask_bitmap[CVMX_COREMASK_BMPSZ];
+};
+
+typedef struct cvmx_coremask cvmx_coremask_t;
+
+/**
+ * Empty coremask
+ */
+#define CVMX_COREMASK_EMPTY             \
+        {{ 0, 0, 0, 0, 0, 0, 0, 0,      \
+           0, 0, 0, 0, 0, 0, 0, 0}}
+
+/**
+ * Set the 64-bit of the coremask for a particular node.
+ * @param pcm   pointer to coremask
+ * @param node  node to set
+ * @param coremask_64   64-bit coremask to apply to the specified node
+ */
+static inline void cvmx_coremask_set64_node(cvmx_coremask_t *pcm,
+                                            uint8_t node,
+                                            uint64_t coremask_64)
+{
+
+        pcm->coremask_bitmap[CVMX_COREMASK_BMP_NODE_CORE_IDX(node, 0)] =
+                                                                coremask_64;
+}
+
 #endif /* __CVMX_SYSINFO_H__ */
diff --git a/arch/mips/include/asm/octeon/cvmx-tim-defs.h b/arch/mips/include/asm/octeon/cvmx-tim-defs.h
new file mode 100644
index 0000000..ac0354f
--- /dev/null
+++ b/arch/mips/include/asm/octeon/cvmx-tim-defs.h
@@ -0,0 +1,1816 @@
+/***********************license start***************
+ * Copyright (c) 2003-2014  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+
+/**
+ * cvmx-tim-defs.h
+ *
+ * Configuration and status register (CSR) type definitions for
+ * Octeon tim.
+ *
+ * This file is auto generated. Do not edit.
+ *
+ * <hr>$Revision$<hr>
+ *
+ */
+#ifndef __CVMX_TIM_DEFS_H__
+#define __CVMX_TIM_DEFS_H__
+
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_BIST_RESULT CVMX_TIM_BIST_RESULT_FUNC()
+static inline uint64_t CVMX_TIM_BIST_RESULT_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_TIM_BIST_RESULT not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058000020ull);
+}
+#else
+#define CVMX_TIM_BIST_RESULT (CVMX_ADD_IO_SEG(0x0001180058000020ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_DBG2 CVMX_TIM_DBG2_FUNC()
+static inline uint64_t CVMX_TIM_DBG2_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_TIM_DBG2 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00011800580000A0ull);
+}
+#else
+#define CVMX_TIM_DBG2 (CVMX_ADD_IO_SEG(0x00011800580000A0ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_DBG3 CVMX_TIM_DBG3_FUNC()
+static inline uint64_t CVMX_TIM_DBG3_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_TIM_DBG3 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00011800580000A8ull);
+}
+#else
+#define CVMX_TIM_DBG3 (CVMX_ADD_IO_SEG(0x00011800580000A8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_ECC_CFG CVMX_TIM_ECC_CFG_FUNC()
+static inline uint64_t CVMX_TIM_ECC_CFG_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_TIM_ECC_CFG not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058000018ull);
+}
+#else
+#define CVMX_TIM_ECC_CFG (CVMX_ADD_IO_SEG(0x0001180058000018ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_TIM_ENGX_ACTIVE(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 3)))))
+		cvmx_warn("CVMX_TIM_ENGX_ACTIVE(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180058001000ull) + ((offset) & 3) * 8;
+}
+#else
+#define CVMX_TIM_ENGX_ACTIVE(offset) (CVMX_ADD_IO_SEG(0x0001180058001000ull) + ((offset) & 3) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_FR_RN_CYCLES CVMX_TIM_FR_RN_CYCLES_FUNC()
+static inline uint64_t CVMX_TIM_FR_RN_CYCLES_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_TIM_FR_RN_CYCLES not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00011800580000C0ull);
+}
+#else
+#define CVMX_TIM_FR_RN_CYCLES (CVMX_ADD_IO_SEG(0x00011800580000C0ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_FR_RN_GPIOS CVMX_TIM_FR_RN_GPIOS_FUNC()
+static inline uint64_t CVMX_TIM_FR_RN_GPIOS_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_TIM_FR_RN_GPIOS not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x00011800580000C8ull);
+}
+#else
+#define CVMX_TIM_FR_RN_GPIOS (CVMX_ADD_IO_SEG(0x00011800580000C8ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_FR_RN_TT CVMX_TIM_FR_RN_TT_FUNC()
+static inline uint64_t CVMX_TIM_FR_RN_TT_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX)))
+		cvmx_warn("CVMX_TIM_FR_RN_TT not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058000010ull);
+}
+#else
+#define CVMX_TIM_FR_RN_TT (CVMX_ADD_IO_SEG(0x0001180058000010ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_GPIO_EN CVMX_TIM_GPIO_EN_FUNC()
+static inline uint64_t CVMX_TIM_GPIO_EN_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_TIM_GPIO_EN not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058000080ull);
+}
+#else
+#define CVMX_TIM_GPIO_EN (CVMX_ADD_IO_SEG(0x0001180058000080ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_INT0 CVMX_TIM_INT0_FUNC()
+static inline uint64_t CVMX_TIM_INT0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_TIM_INT0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058000030ull);
+}
+#else
+#define CVMX_TIM_INT0 (CVMX_ADD_IO_SEG(0x0001180058000030ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_INT0_EN CVMX_TIM_INT0_EN_FUNC()
+static inline uint64_t CVMX_TIM_INT0_EN_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX)))
+		cvmx_warn("CVMX_TIM_INT0_EN not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058000038ull);
+}
+#else
+#define CVMX_TIM_INT0_EN (CVMX_ADD_IO_SEG(0x0001180058000038ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_INT0_EVENT CVMX_TIM_INT0_EVENT_FUNC()
+static inline uint64_t CVMX_TIM_INT0_EVENT_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX)))
+		cvmx_warn("CVMX_TIM_INT0_EVENT not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058000040ull);
+}
+#else
+#define CVMX_TIM_INT0_EVENT (CVMX_ADD_IO_SEG(0x0001180058000040ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_INT_ECCERR CVMX_TIM_INT_ECCERR_FUNC()
+static inline uint64_t CVMX_TIM_INT_ECCERR_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_TIM_INT_ECCERR not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058000060ull);
+}
+#else
+#define CVMX_TIM_INT_ECCERR (CVMX_ADD_IO_SEG(0x0001180058000060ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_INT_ECCERR_EN CVMX_TIM_INT_ECCERR_EN_FUNC()
+static inline uint64_t CVMX_TIM_INT_ECCERR_EN_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX)))
+		cvmx_warn("CVMX_TIM_INT_ECCERR_EN not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058000068ull);
+}
+#else
+#define CVMX_TIM_INT_ECCERR_EN (CVMX_ADD_IO_SEG(0x0001180058000068ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_INT_ECCERR_EVENT0 CVMX_TIM_INT_ECCERR_EVENT0_FUNC()
+static inline uint64_t CVMX_TIM_INT_ECCERR_EVENT0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_TIM_INT_ECCERR_EVENT0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058000070ull);
+}
+#else
+#define CVMX_TIM_INT_ECCERR_EVENT0 (CVMX_ADD_IO_SEG(0x0001180058000070ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_INT_ECCERR_EVENT1 CVMX_TIM_INT_ECCERR_EVENT1_FUNC()
+static inline uint64_t CVMX_TIM_INT_ECCERR_EVENT1_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN78XX)))
+		cvmx_warn("CVMX_TIM_INT_ECCERR_EVENT1 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058000078ull);
+}
+#else
+#define CVMX_TIM_INT_ECCERR_EVENT1 (CVMX_ADD_IO_SEG(0x0001180058000078ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_MEM_DEBUG0 CVMX_TIM_MEM_DEBUG0_FUNC()
+static inline uint64_t CVMX_TIM_MEM_DEBUG0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_TIM_MEM_DEBUG0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058001100ull);
+}
+#else
+#define CVMX_TIM_MEM_DEBUG0 (CVMX_ADD_IO_SEG(0x0001180058001100ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_MEM_DEBUG1 CVMX_TIM_MEM_DEBUG1_FUNC()
+static inline uint64_t CVMX_TIM_MEM_DEBUG1_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_TIM_MEM_DEBUG1 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058001108ull);
+}
+#else
+#define CVMX_TIM_MEM_DEBUG1 (CVMX_ADD_IO_SEG(0x0001180058001108ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_MEM_DEBUG2 CVMX_TIM_MEM_DEBUG2_FUNC()
+static inline uint64_t CVMX_TIM_MEM_DEBUG2_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_TIM_MEM_DEBUG2 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058001110ull);
+}
+#else
+#define CVMX_TIM_MEM_DEBUG2 (CVMX_ADD_IO_SEG(0x0001180058001110ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_MEM_RING0 CVMX_TIM_MEM_RING0_FUNC()
+static inline uint64_t CVMX_TIM_MEM_RING0_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_TIM_MEM_RING0 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058001000ull);
+}
+#else
+#define CVMX_TIM_MEM_RING0 (CVMX_ADD_IO_SEG(0x0001180058001000ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_MEM_RING1 CVMX_TIM_MEM_RING1_FUNC()
+static inline uint64_t CVMX_TIM_MEM_RING1_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_TIM_MEM_RING1 not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058001008ull);
+}
+#else
+#define CVMX_TIM_MEM_RING1 (CVMX_ADD_IO_SEG(0x0001180058001008ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_REG_BIST_RESULT CVMX_TIM_REG_BIST_RESULT_FUNC()
+static inline uint64_t CVMX_TIM_REG_BIST_RESULT_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_TIM_REG_BIST_RESULT not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058000080ull);
+}
+#else
+#define CVMX_TIM_REG_BIST_RESULT (CVMX_ADD_IO_SEG(0x0001180058000080ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_REG_ERROR CVMX_TIM_REG_ERROR_FUNC()
+static inline uint64_t CVMX_TIM_REG_ERROR_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_TIM_REG_ERROR not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058000088ull);
+}
+#else
+#define CVMX_TIM_REG_ERROR (CVMX_ADD_IO_SEG(0x0001180058000088ull))
+#endif
+#define CVMX_TIM_REG_FLAGS (CVMX_ADD_IO_SEG(0x0001180058000000ull))
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_REG_INT_MASK CVMX_TIM_REG_INT_MASK_FUNC()
+static inline uint64_t CVMX_TIM_REG_INT_MASK_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_TIM_REG_INT_MASK not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058000090ull);
+}
+#else
+#define CVMX_TIM_REG_INT_MASK (CVMX_ADD_IO_SEG(0x0001180058000090ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+#define CVMX_TIM_REG_READ_IDX CVMX_TIM_REG_READ_IDX_FUNC()
+static inline uint64_t CVMX_TIM_REG_READ_IDX_FUNC(void)
+{
+	if (!(OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN5XXX) || OCTEON_IS_MODEL(OCTEON_CN61XX) || OCTEON_IS_MODEL(OCTEON_CN63XX) || OCTEON_IS_MODEL(OCTEON_CN66XX) || OCTEON_IS_MODEL(OCTEON_CN70XX) || OCTEON_IS_MODEL(OCTEON_CNF71XX)))
+		cvmx_warn("CVMX_TIM_REG_READ_IDX not supported on this chip\n");
+	return CVMX_ADD_IO_SEG(0x0001180058000008ull);
+}
+#else
+#define CVMX_TIM_REG_READ_IDX (CVMX_ADD_IO_SEG(0x0001180058000008ull))
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_TIM_RINGX_AURA(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_TIM_RINGX_AURA(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180058002C00ull) + ((offset) & 63) * 8;
+}
+#else
+#define CVMX_TIM_RINGX_AURA(offset) (CVMX_ADD_IO_SEG(0x0001180058002C00ull) + ((offset) & 63) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_TIM_RINGX_CTL0(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 63))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_TIM_RINGX_CTL0(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180058002000ull) + ((offset) & 63) * 8;
+}
+#else
+#define CVMX_TIM_RINGX_CTL0(offset) (CVMX_ADD_IO_SEG(0x0001180058002000ull) + ((offset) & 63) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_TIM_RINGX_CTL1(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 63))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_TIM_RINGX_CTL1(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180058002400ull) + ((offset) & 63) * 8;
+}
+#else
+#define CVMX_TIM_RINGX_CTL1(offset) (CVMX_ADD_IO_SEG(0x0001180058002400ull) + ((offset) & 63) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_TIM_RINGX_CTL2(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 63))) ||
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_TIM_RINGX_CTL2(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180058002800ull) + ((offset) & 63) * 8;
+}
+#else
+#define CVMX_TIM_RINGX_CTL2(offset) (CVMX_ADD_IO_SEG(0x0001180058002800ull) + ((offset) & 63) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_TIM_RINGX_DBG0(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_TIM_RINGX_DBG0(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180058003000ull) + ((offset) & 63) * 8;
+}
+#else
+#define CVMX_TIM_RINGX_DBG0(offset) (CVMX_ADD_IO_SEG(0x0001180058003000ull) + ((offset) & 63) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_TIM_RINGX_DBG1(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN68XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_TIM_RINGX_DBG1(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180058001200ull) + ((offset) & 63) * 8;
+}
+#else
+#define CVMX_TIM_RINGX_DBG1(offset) (CVMX_ADD_IO_SEG(0x0001180058001200ull) + ((offset) & 63) * 8)
+#endif
+#if CVMX_ENABLE_CSR_ADDRESS_CHECKING
+static inline uint64_t CVMX_TIM_RINGX_REL(unsigned long offset)
+{
+	if (!(
+	      (OCTEON_IS_MODEL(OCTEON_CN78XX) && ((offset <= 63)))))
+		cvmx_warn("CVMX_TIM_RINGX_REL(%lu) is invalid on this chip\n", offset);
+	return CVMX_ADD_IO_SEG(0x0001180058003000ull) + ((offset) & 63) * 8;
+}
+#else
+#define CVMX_TIM_RINGX_REL(offset) (CVMX_ADD_IO_SEG(0x0001180058003000ull) + ((offset) & 63) * 8)
+#endif
+
+/**
+ * cvmx_tim_bist_result
+ *
+ * This register provides access to the internal timer BIST results. Each bit is the BIST result
+ * of an individual memory (per bit, 0 = pass and 1 = fail).
+ */
+union cvmx_tim_bist_result {
+	uint64_t u64;
+	struct cvmx_tim_bist_result_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t tstmp_mem                    : 1;  /**< BIST result of the Time Stamp memory. */
+	uint64_t wqe_fifo                     : 1;  /**< BIST result of the NCB_WQE FIFO. */
+	uint64_t lslr_fifo                    : 1;  /**< BIST result of the NCB_LSLR FIFO. */
+	uint64_t rds_mem                      : 1;  /**< BIST result of the RDS memory. */
+#else
+	uint64_t rds_mem                      : 1;
+	uint64_t lslr_fifo                    : 1;
+	uint64_t wqe_fifo                     : 1;
+	uint64_t tstmp_mem                    : 1;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_tim_bist_result_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_3_63                : 61;
+	uint64_t wqe_fifo                     : 1;  /**< BIST result of the NCB_WQE FIFO (0=pass, !0=fail) */
+	uint64_t lslr_fifo                    : 1;  /**< BIST result of the NCB_LSLR FIFO (0=pass, !0=fail) */
+	uint64_t rds_mem                      : 1;  /**< BIST result of the RDS memory (0=pass, !0=fail) */
+#else
+	uint64_t rds_mem                      : 1;
+	uint64_t lslr_fifo                    : 1;
+	uint64_t wqe_fifo                     : 1;
+	uint64_t reserved_3_63                : 61;
+#endif
+	} cn68xx;
+	struct cvmx_tim_bist_result_cn68xx    cn68xxp1;
+	struct cvmx_tim_bist_result_s         cn78xx;
+};
+typedef union cvmx_tim_bist_result cvmx_tim_bist_result_t;
+
+/**
+ * cvmx_tim_dbg2
+ */
+union cvmx_tim_dbg2 {
+	uint64_t u64;
+	struct cvmx_tim_dbg2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t mem_alloc_reg                : 8;  /**< IOI load memory allocation status. */
+	uint64_t reserved_46_55               : 10;
+	uint64_t rwf_fifo_level               : 6;  /**< IOI requests FIFO level. */
+	uint64_t wqe_fifo_level               : 8;  /**< IOI WQE LD FIFO level. */
+	uint64_t reserved_16_31               : 16;
+	uint64_t fsm3_state                   : 4;  /**< FSM 3 current state. */
+	uint64_t fsm2_state                   : 4;  /**< FSM 2 current state. */
+	uint64_t fsm1_state                   : 4;  /**< FSM 1 current state. */
+	uint64_t fsm0_state                   : 4;  /**< FSM 0 current state. */
+#else
+	uint64_t fsm0_state                   : 4;
+	uint64_t fsm1_state                   : 4;
+	uint64_t fsm2_state                   : 4;
+	uint64_t fsm3_state                   : 4;
+	uint64_t reserved_16_31               : 16;
+	uint64_t wqe_fifo_level               : 8;
+	uint64_t rwf_fifo_level               : 6;
+	uint64_t reserved_46_55               : 10;
+	uint64_t mem_alloc_reg                : 8;
+#endif
+	} s;
+	struct cvmx_tim_dbg2_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t mem_alloc_reg                : 8;  /**< NCB Load Memory Allocation status */
+	uint64_t reserved_51_55               : 5;
+	uint64_t gnt_fifo_level               : 3;  /**< NCB GRANT FIFO level */
+	uint64_t reserved_45_47               : 3;
+	uint64_t rwf_fifo_level               : 5;  /**< NCB requests FIFO level */
+	uint64_t wqe_fifo_level               : 8;  /**< NCB WQE LD FIFO level */
+	uint64_t reserved_16_31               : 16;
+	uint64_t fsm3_state                   : 4;  /**< FSM 3 current state */
+	uint64_t fsm2_state                   : 4;  /**< FSM 2 current state */
+	uint64_t fsm1_state                   : 4;  /**< FSM 1 current state */
+	uint64_t fsm0_state                   : 4;  /**< FSM 0 current state */
+#else
+	uint64_t fsm0_state                   : 4;
+	uint64_t fsm1_state                   : 4;
+	uint64_t fsm2_state                   : 4;
+	uint64_t fsm3_state                   : 4;
+	uint64_t reserved_16_31               : 16;
+	uint64_t wqe_fifo_level               : 8;
+	uint64_t rwf_fifo_level               : 5;
+	uint64_t reserved_45_47               : 3;
+	uint64_t gnt_fifo_level               : 3;
+	uint64_t reserved_51_55               : 5;
+	uint64_t mem_alloc_reg                : 8;
+#endif
+	} cn68xx;
+	struct cvmx_tim_dbg2_cn68xx           cn68xxp1;
+	struct cvmx_tim_dbg2_cn78xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t mem_alloc_reg                : 8;  /**< IOI load memory allocation status. */
+	uint64_t reserved_53_55               : 3;
+	uint64_t gnt_fifo_level               : 4;  /**< IOI grant FIFO level. */
+	uint64_t fpa_fifo_level               : 3;  /**< FPA FIFO level. */
+	uint64_t rwf_fifo_level               : 6;  /**< IOI requests FIFO level. */
+	uint64_t wqe_fifo_level               : 8;  /**< IOI WQE LD FIFO level. */
+	uint64_t reserved_16_31               : 16;
+	uint64_t fsm3_state                   : 4;  /**< FSM 3 current state. */
+	uint64_t fsm2_state                   : 4;  /**< FSM 2 current state. */
+	uint64_t fsm1_state                   : 4;  /**< FSM 1 current state. */
+	uint64_t fsm0_state                   : 4;  /**< FSM 0 current state. */
+#else
+	uint64_t fsm0_state                   : 4;
+	uint64_t fsm1_state                   : 4;
+	uint64_t fsm2_state                   : 4;
+	uint64_t fsm3_state                   : 4;
+	uint64_t reserved_16_31               : 16;
+	uint64_t wqe_fifo_level               : 8;
+	uint64_t rwf_fifo_level               : 6;
+	uint64_t fpa_fifo_level               : 3;
+	uint64_t gnt_fifo_level               : 4;
+	uint64_t reserved_53_55               : 3;
+	uint64_t mem_alloc_reg                : 8;
+#endif
+	} cn78xx;
+};
+typedef union cvmx_tim_dbg2 cvmx_tim_dbg2_t;
+
+/**
+ * cvmx_tim_dbg3
+ */
+union cvmx_tim_dbg3 {
+	uint64_t u64;
+	struct cvmx_tim_dbg3_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t rings_pending_vec            : 64; /**< Pending rings vector. Indicates which ring in TIM are pending traversal. Bit 0 represents
+                                                         ring 0 while bit 63 represents ring 63. */
+#else
+	uint64_t rings_pending_vec            : 64;
+#endif
+	} s;
+	struct cvmx_tim_dbg3_s                cn68xx;
+	struct cvmx_tim_dbg3_s                cn68xxp1;
+	struct cvmx_tim_dbg3_s                cn78xx;
+};
+typedef union cvmx_tim_dbg3 cvmx_tim_dbg3_t;
+
+/**
+ * cvmx_tim_ecc_cfg
+ */
+union cvmx_tim_ecc_cfg {
+	uint64_t u64;
+	struct cvmx_tim_ecc_cfg_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_3_63                : 61;
+	uint64_t ecc_flp_syn                  : 2;  /**< ECC flip syndrome. Flip the ECC's syndrome for testing purposes, to test SBE and DBE ECC
+                                                         interrupts. */
+	uint64_t ecc_en                       : 1;  /**< Enable ECC correction of the ring data structure memory. */
+#else
+	uint64_t ecc_en                       : 1;
+	uint64_t ecc_flp_syn                  : 2;
+	uint64_t reserved_3_63                : 61;
+#endif
+	} s;
+	struct cvmx_tim_ecc_cfg_s             cn68xx;
+	struct cvmx_tim_ecc_cfg_s             cn68xxp1;
+	struct cvmx_tim_ecc_cfg_s             cn78xx;
+};
+typedef union cvmx_tim_ecc_cfg cvmx_tim_ecc_cfg_t;
+
+/**
+ * cvmx_tim_eng#_active
+ */
+union cvmx_tim_engx_active {
+	uint64_t u64;
+	struct cvmx_tim_engx_active_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_9_63                : 55;
+	uint64_t act                          : 1;  /**< Engine active. For diagnostic use. */
+	uint64_t reserved_6_7                 : 2;
+	uint64_t ring_id                      : 6;  /**< Current ring ID. For diagnostic use. */
+#else
+	uint64_t ring_id                      : 6;
+	uint64_t reserved_6_7                 : 2;
+	uint64_t act                          : 1;
+	uint64_t reserved_9_63                : 55;
+#endif
+	} s;
+	struct cvmx_tim_engx_active_s         cn78xx;
+};
+typedef union cvmx_tim_engx_active cvmx_tim_engx_active_t;
+
+/**
+ * cvmx_tim_fr_rn_cycles
+ */
+union cvmx_tim_fr_rn_cycles {
+	uint64_t u64;
+	struct cvmx_tim_fr_rn_cycles_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t count                        : 64; /**< Count of system coprocessor-clock cycles. This register is only writable when
+                                                         TIM_REG_FLAGS[ENA_TIM] = 0. */
+#else
+	uint64_t count                        : 64;
+#endif
+	} s;
+	struct cvmx_tim_fr_rn_cycles_s        cn78xx;
+};
+typedef union cvmx_tim_fr_rn_cycles cvmx_tim_fr_rn_cycles_t;
+
+/**
+ * cvmx_tim_fr_rn_gpios
+ */
+union cvmx_tim_fr_rn_gpios {
+	uint64_t u64;
+	struct cvmx_tim_fr_rn_gpios_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t count                        : 64; /**< Count of GPIO cycles. This register is only writable when TIM_REG_FLAGS[ENA_TIM] = 0. */
+#else
+	uint64_t count                        : 64;
+#endif
+	} s;
+	struct cvmx_tim_fr_rn_gpios_s         cn78xx;
+};
+typedef union cvmx_tim_fr_rn_gpios cvmx_tim_fr_rn_gpios_t;
+
+/**
+ * cvmx_tim_fr_rn_tt
+ *
+ * Notes:
+ * For every 64 entries in a bucket interval should be at
+ * least 1us.
+ * Minimal recommended value for Threshold register is 1us
+ */
+union cvmx_tim_fr_rn_tt {
+	uint64_t u64;
+	struct cvmx_tim_fr_rn_tt_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_54_63               : 10;
+	uint64_t thld_gp                      : 22; /**< Free Running Timer Threshold. Defines the reset value
+                                                         for the free running timer when it reaches zero during
+                                                         it's count down. This threshold only applies to the
+                                                         timer that is driven by GPIO edge as defined at
+                                                         TIM_REG_FLAGS.GPIO_EDGE
+                                                         ***NOTE: Added in pass 2.0 */
+	uint64_t reserved_22_31               : 10;
+	uint64_t fr_rn_tt                     : 22; /**< Free Running Timer Threshold. Defines the reset value
+                                                         for the free running timer when it reaches zero during
+                                                         it's count down.
+                                                         FR_RN_TT will be used in both cases where free running
+                                                         clock is driven externally or internally.
+                                                         Interval programming guidelines:
+                                                         For every 64 entries in a bucket interval should be at
+                                                         least 1us.
+                                                         Minimal recommended value for FR_RN_TT is 1us. */
+#else
+	uint64_t fr_rn_tt                     : 22;
+	uint64_t reserved_22_31               : 10;
+	uint64_t thld_gp                      : 22;
+	uint64_t reserved_54_63               : 10;
+#endif
+	} s;
+	struct cvmx_tim_fr_rn_tt_s            cn68xx;
+	struct cvmx_tim_fr_rn_tt_cn68xxp1 {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_22_63               : 42;
+	uint64_t fr_rn_tt                     : 22; /**< Free Running Timer Threshold. Defines the reset value
+                                                         for the free running timer when it reaches zero during
+                                                         it's count down.
+                                                         FR_RN_TT will be used in both cases where free running
+                                                         clock is driven externally or internally.
+                                                         Interval programming guidelines:
+                                                         For every 64 entries in a bucket interval should be at
+                                                         least 1us.
+                                                         Minimal recommended value for FR_RN_TT is 1us. */
+#else
+	uint64_t fr_rn_tt                     : 22;
+	uint64_t reserved_22_63               : 42;
+#endif
+	} cn68xxp1;
+};
+typedef union cvmx_tim_fr_rn_tt cvmx_tim_fr_rn_tt_t;
+
+/**
+ * cvmx_tim_gpio_en
+ */
+union cvmx_tim_gpio_en {
+	uint64_t u64;
+	struct cvmx_tim_gpio_en_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t gpio_en                      : 64; /**< Each bit corresponds to rings [63:0] respectively. This register reflects the values
+                                                         written to TIM_RING()_CTL1 [ENA_GPIO]. For debug only; Reserved. */
+#else
+	uint64_t gpio_en                      : 64;
+#endif
+	} s;
+	struct cvmx_tim_gpio_en_s             cn68xx;
+	struct cvmx_tim_gpio_en_s             cn78xx;
+};
+typedef union cvmx_tim_gpio_en cvmx_tim_gpio_en_t;
+
+/**
+ * cvmx_tim_int0
+ *
+ * A ring is in error if its interval has elapsed more than once without having been serviced,
+ * either due to too many events in this ring's previous interval, or another ring having too
+ * many events to process within this ring's interval. This is usually a programming error where
+ * the number of entries in the bucket is too large for the interval specified across the rings.
+ * When in error, TIM may process events in an interval later than requested. Any bit in the INT
+ * field should be cleared by writing one to it.
+ */
+union cvmx_tim_int0 {
+	uint64_t u64;
+	struct cvmx_tim_int0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t int0                         : 64; /**< Interrupt bit per ring. Each bit indicates the ring number in error. Throws INTSN
+                                                         TIM_INTSN_E::TIM_RING()_SLOW. */
+#else
+	uint64_t int0                         : 64;
+#endif
+	} s;
+	struct cvmx_tim_int0_s                cn68xx;
+	struct cvmx_tim_int0_s                cn68xxp1;
+	struct cvmx_tim_int0_s                cn78xx;
+};
+typedef union cvmx_tim_int0 cvmx_tim_int0_t;
+
+/**
+ * cvmx_tim_int0_en
+ *
+ * Notes:
+ * When bit at TIM_INT0_EN is set it enables the corresponding TIM_INTO's bit for interrupt generation
+ * If enable bit is cleared the corresponding bit at TIM_INT0 will still be set.
+ * Interrupt to the cores is generated by : |(TIM_INT0 & TIM_INT0_EN0)
+ */
+union cvmx_tim_int0_en {
+	uint64_t u64;
+	struct cvmx_tim_int0_en_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t int0_en                      : 64; /**< Bit enable corresponding to TIM_INT0. */
+#else
+	uint64_t int0_en                      : 64;
+#endif
+	} s;
+	struct cvmx_tim_int0_en_s             cn68xx;
+	struct cvmx_tim_int0_en_s             cn68xxp1;
+};
+typedef union cvmx_tim_int0_en cvmx_tim_int0_en_t;
+
+/**
+ * cvmx_tim_int0_event
+ */
+union cvmx_tim_int0_event {
+	uint64_t u64;
+	struct cvmx_tim_int0_event_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_6_63                : 58;
+	uint64_t ring_id                      : 6;  /**< The first Ring ID where an interrupt occurred. */
+#else
+	uint64_t ring_id                      : 6;
+	uint64_t reserved_6_63                : 58;
+#endif
+	} s;
+	struct cvmx_tim_int0_event_s          cn68xx;
+	struct cvmx_tim_int0_event_s          cn68xxp1;
+};
+typedef union cvmx_tim_int0_event cvmx_tim_int0_event_t;
+
+/**
+ * cvmx_tim_int_eccerr
+ *
+ * Notes:
+ * Each bit in this reg is set regardless of TIM_INT_ECCERR_EN value.
+ *
+ */
+union cvmx_tim_int_eccerr {
+	uint64_t u64;
+	struct cvmx_tim_int_eccerr_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t ctl_dbe                      : 1;  /**< TIM CTL memory had a double-bit error. Throws INTSN TIM_INTSN_E::TIM_ECCERR_CTL_DBE. */
+	uint64_t ctl_sbe                      : 1;  /**< TIM CTL memory had a single-bit error. Throws INTSN TIM_INTSN_E::TIM_ECCERR_CTL_SBE. */
+	uint64_t dbe                          : 1;  /**< TIM RDS memory had a double-bit error. Throws INTSN TIM_INTSN_E::TIM_ECCERR_DBE. */
+	uint64_t sbe                          : 1;  /**< TIM RDS memory had a single-bit error. Throws INTSN TIM_INTSN_E::TIM_ECCERR_SBE. */
+#else
+	uint64_t sbe                          : 1;
+	uint64_t dbe                          : 1;
+	uint64_t ctl_sbe                      : 1;
+	uint64_t ctl_dbe                      : 1;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_tim_int_eccerr_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_2_63                : 62;
+	uint64_t dbe                          : 1;  /**< TIM RDS memory had a Double Bit Error */
+	uint64_t sbe                          : 1;  /**< TIM RDS memory had a Single Bit Error */
+#else
+	uint64_t sbe                          : 1;
+	uint64_t dbe                          : 1;
+	uint64_t reserved_2_63                : 62;
+#endif
+	} cn68xx;
+	struct cvmx_tim_int_eccerr_cn68xx     cn68xxp1;
+	struct cvmx_tim_int_eccerr_s          cn78xx;
+};
+typedef union cvmx_tim_int_eccerr cvmx_tim_int_eccerr_t;
+
+/**
+ * cvmx_tim_int_eccerr_en
+ *
+ * Notes:
+ * When mask bit is set, the corresponding bit in TIM_INT_ECCERR is enabled. If mask bit is cleared the
+ * corresponding bit in TIM_INT_ECCERR will still be set but interrupt will not be reported.
+ */
+union cvmx_tim_int_eccerr_en {
+	uint64_t u64;
+	struct cvmx_tim_int_eccerr_en_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_2_63                : 62;
+	uint64_t dbe_en                       : 1;  /**< Bit mask corresponding to TIM_REG_ECCERR.DBE */
+	uint64_t sbe_en                       : 1;  /**< Bit mask corresponding to TIM_REG_ECCERR.SBE */
+#else
+	uint64_t sbe_en                       : 1;
+	uint64_t dbe_en                       : 1;
+	uint64_t reserved_2_63                : 62;
+#endif
+	} s;
+	struct cvmx_tim_int_eccerr_en_s       cn68xx;
+	struct cvmx_tim_int_eccerr_en_s       cn68xxp1;
+};
+typedef union cvmx_tim_int_eccerr_en cvmx_tim_int_eccerr_en_t;
+
+/**
+ * cvmx_tim_int_eccerr_event0
+ */
+union cvmx_tim_int_eccerr_event0 {
+	uint64_t u64;
+	struct cvmx_tim_int_eccerr_event0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_7_63                : 57;
+	uint64_t add                          : 7;  /**< Memory address where the error occurred. */
+#else
+	uint64_t add                          : 7;
+	uint64_t reserved_7_63                : 57;
+#endif
+	} s;
+	struct cvmx_tim_int_eccerr_event0_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_15_63               : 49;
+	uint64_t synd                         : 7;  /**< ECC Syndrome */
+	uint64_t add                          : 8;  /**< Memory address where the Error occurred. */
+#else
+	uint64_t add                          : 8;
+	uint64_t synd                         : 7;
+	uint64_t reserved_15_63               : 49;
+#endif
+	} cn68xx;
+	struct cvmx_tim_int_eccerr_event0_cn68xx cn68xxp1;
+	struct cvmx_tim_int_eccerr_event0_cn78xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_14_63               : 50;
+	uint64_t synd                         : 7;  /**< ECC syndrome bits. */
+	uint64_t add                          : 7;  /**< Memory address where the error occurred. */
+#else
+	uint64_t add                          : 7;
+	uint64_t synd                         : 7;
+	uint64_t reserved_14_63               : 50;
+#endif
+	} cn78xx;
+};
+typedef union cvmx_tim_int_eccerr_event0 cvmx_tim_int_eccerr_event0_t;
+
+/**
+ * cvmx_tim_int_eccerr_event1
+ */
+union cvmx_tim_int_eccerr_event1 {
+	uint64_t u64;
+	struct cvmx_tim_int_eccerr_event1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_55_63               : 9;
+	uint64_t org_ecc                      : 7;  /**< Original ECC bits where the error occurred. */
+	uint64_t org_rds_dat                  : 48; /**< Memory original data where the error occurred. */
+#else
+	uint64_t org_rds_dat                  : 48;
+	uint64_t org_ecc                      : 7;
+	uint64_t reserved_55_63               : 9;
+#endif
+	} s;
+	struct cvmx_tim_int_eccerr_event1_s   cn68xx;
+	struct cvmx_tim_int_eccerr_event1_s   cn68xxp1;
+	struct cvmx_tim_int_eccerr_event1_s   cn78xx;
+};
+typedef union cvmx_tim_int_eccerr_event1 cvmx_tim_int_eccerr_event1_t;
+
+/**
+ * cvmx_tim_mem_debug0
+ *
+ * Notes:
+ * Internal per-ring state intended for debug use only - tim.ctl[47:0]
+ * This CSR is a memory of 16 entries, and thus, the TIM_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.
+ */
+union cvmx_tim_mem_debug0 {
+	uint64_t u64;
+	struct cvmx_tim_mem_debug0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_48_63               : 16;
+	uint64_t ena                          : 1;  /**< Ring timer enable */
+	uint64_t reserved_46_46               : 1;
+	uint64_t count                        : 22; /**< Time offset for the ring
+                                                         Set to INTERVAL and counts down by 1 every 1024
+                                                         cycles when ENA==1. The HW forces a bucket
+                                                         traversal (and resets COUNT to INTERVAL) whenever
+                                                         the decrement would cause COUNT to go negative.
+                                                         COUNT is unpredictable whenever ENA==0.
+                                                         COUNT is reset to INTERVAL whenever TIM_MEM_RING1
+                                                         is written for the ring. */
+	uint64_t reserved_22_23               : 2;
+	uint64_t interval                     : 22; /**< Timer interval - 1 */
+#else
+	uint64_t interval                     : 22;
+	uint64_t reserved_22_23               : 2;
+	uint64_t count                        : 22;
+	uint64_t reserved_46_46               : 1;
+	uint64_t ena                          : 1;
+	uint64_t reserved_48_63               : 16;
+#endif
+	} s;
+	struct cvmx_tim_mem_debug0_s          cn30xx;
+	struct cvmx_tim_mem_debug0_s          cn31xx;
+	struct cvmx_tim_mem_debug0_s          cn38xx;
+	struct cvmx_tim_mem_debug0_s          cn38xxp2;
+	struct cvmx_tim_mem_debug0_s          cn50xx;
+	struct cvmx_tim_mem_debug0_s          cn52xx;
+	struct cvmx_tim_mem_debug0_s          cn52xxp1;
+	struct cvmx_tim_mem_debug0_s          cn56xx;
+	struct cvmx_tim_mem_debug0_s          cn56xxp1;
+	struct cvmx_tim_mem_debug0_s          cn58xx;
+	struct cvmx_tim_mem_debug0_s          cn58xxp1;
+	struct cvmx_tim_mem_debug0_s          cn61xx;
+	struct cvmx_tim_mem_debug0_s          cn63xx;
+	struct cvmx_tim_mem_debug0_s          cn63xxp1;
+	struct cvmx_tim_mem_debug0_s          cn66xx;
+	struct cvmx_tim_mem_debug0_s          cn70xx;
+	struct cvmx_tim_mem_debug0_s          cn70xxp1;
+	struct cvmx_tim_mem_debug0_s          cnf71xx;
+};
+typedef union cvmx_tim_mem_debug0 cvmx_tim_mem_debug0_t;
+
+/**
+ * cvmx_tim_mem_debug1
+ *
+ * Notes:
+ * Internal per-ring state intended for debug use only - tim.sta[63:0]
+ * This CSR is a memory of 16 entries, and thus, the TIM_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.
+ */
+union cvmx_tim_mem_debug1 {
+	uint64_t u64;
+	struct cvmx_tim_mem_debug1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t bucket                       : 13; /**< Current bucket[12:0]
+                                                         Reset to 0 whenever TIM_MEM_RING0 is written for
+                                                         the ring. Incremented (modulo BSIZE) once per
+                                                         bucket traversal.
+                                                         See TIM_MEM_DEBUG2[BUCKET]. */
+	uint64_t base                         : 31; /**< Pointer[35:5] to bucket[0] */
+	uint64_t bsize                        : 20; /**< Number of buckets - 1 */
+#else
+	uint64_t bsize                        : 20;
+	uint64_t base                         : 31;
+	uint64_t bucket                       : 13;
+#endif
+	} s;
+	struct cvmx_tim_mem_debug1_s          cn30xx;
+	struct cvmx_tim_mem_debug1_s          cn31xx;
+	struct cvmx_tim_mem_debug1_s          cn38xx;
+	struct cvmx_tim_mem_debug1_s          cn38xxp2;
+	struct cvmx_tim_mem_debug1_s          cn50xx;
+	struct cvmx_tim_mem_debug1_s          cn52xx;
+	struct cvmx_tim_mem_debug1_s          cn52xxp1;
+	struct cvmx_tim_mem_debug1_s          cn56xx;
+	struct cvmx_tim_mem_debug1_s          cn56xxp1;
+	struct cvmx_tim_mem_debug1_s          cn58xx;
+	struct cvmx_tim_mem_debug1_s          cn58xxp1;
+	struct cvmx_tim_mem_debug1_s          cn61xx;
+	struct cvmx_tim_mem_debug1_s          cn63xx;
+	struct cvmx_tim_mem_debug1_s          cn63xxp1;
+	struct cvmx_tim_mem_debug1_s          cn66xx;
+	struct cvmx_tim_mem_debug1_s          cn70xx;
+	struct cvmx_tim_mem_debug1_s          cn70xxp1;
+	struct cvmx_tim_mem_debug1_s          cnf71xx;
+};
+typedef union cvmx_tim_mem_debug1 cvmx_tim_mem_debug1_t;
+
+/**
+ * cvmx_tim_mem_debug2
+ *
+ * Notes:
+ * Internal per-ring state intended for debug use only - tim.sta[95:64]
+ * This CSR is a memory of 16 entries, and thus, the TIM_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.
+ */
+union cvmx_tim_mem_debug2 {
+	uint64_t u64;
+	struct cvmx_tim_mem_debug2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_24_63               : 40;
+	uint64_t cpool                        : 3;  /**< Free list used to free chunks */
+	uint64_t csize                        : 13; /**< Number of words per chunk */
+	uint64_t reserved_7_7                 : 1;
+	uint64_t bucket                       : 7;  /**< Current bucket[19:13]
+                                                         See TIM_MEM_DEBUG1[BUCKET]. */
+#else
+	uint64_t bucket                       : 7;
+	uint64_t reserved_7_7                 : 1;
+	uint64_t csize                        : 13;
+	uint64_t cpool                        : 3;
+	uint64_t reserved_24_63               : 40;
+#endif
+	} s;
+	struct cvmx_tim_mem_debug2_s          cn30xx;
+	struct cvmx_tim_mem_debug2_s          cn31xx;
+	struct cvmx_tim_mem_debug2_s          cn38xx;
+	struct cvmx_tim_mem_debug2_s          cn38xxp2;
+	struct cvmx_tim_mem_debug2_s          cn50xx;
+	struct cvmx_tim_mem_debug2_s          cn52xx;
+	struct cvmx_tim_mem_debug2_s          cn52xxp1;
+	struct cvmx_tim_mem_debug2_s          cn56xx;
+	struct cvmx_tim_mem_debug2_s          cn56xxp1;
+	struct cvmx_tim_mem_debug2_s          cn58xx;
+	struct cvmx_tim_mem_debug2_s          cn58xxp1;
+	struct cvmx_tim_mem_debug2_s          cn61xx;
+	struct cvmx_tim_mem_debug2_s          cn63xx;
+	struct cvmx_tim_mem_debug2_s          cn63xxp1;
+	struct cvmx_tim_mem_debug2_s          cn66xx;
+	struct cvmx_tim_mem_debug2_s          cn70xx;
+	struct cvmx_tim_mem_debug2_s          cn70xxp1;
+	struct cvmx_tim_mem_debug2_s          cnf71xx;
+};
+typedef union cvmx_tim_mem_debug2 cvmx_tim_mem_debug2_t;
+
+/**
+ * cvmx_tim_mem_ring0
+ *
+ * Notes:
+ * TIM_MEM_RING0 must not be written for a ring when TIM_MEM_RING1[ENA] is set for the ring.
+ * Every write to TIM_MEM_RING0 clears the current bucket for the ring. (The current bucket is
+ * readable via TIM_MEM_DEBUG2[BUCKET],TIM_MEM_DEBUG1[BUCKET].)
+ * BASE is a 32-byte aligned pointer[35:0].  Only pointer[35:5] are stored because pointer[4:0] = 0.
+ * This CSR is a memory of 16 entries, and thus, the TIM_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.
+ */
+union cvmx_tim_mem_ring0 {
+	uint64_t u64;
+	struct cvmx_tim_mem_ring0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_55_63               : 9;
+	uint64_t first_bucket                 : 31; /**< Pointer[35:5] to bucket[0] */
+	uint64_t num_buckets                  : 20; /**< Number of buckets - 1 */
+	uint64_t ring                         : 4;  /**< Ring ID */
+#else
+	uint64_t ring                         : 4;
+	uint64_t num_buckets                  : 20;
+	uint64_t first_bucket                 : 31;
+	uint64_t reserved_55_63               : 9;
+#endif
+	} s;
+	struct cvmx_tim_mem_ring0_s           cn30xx;
+	struct cvmx_tim_mem_ring0_s           cn31xx;
+	struct cvmx_tim_mem_ring0_s           cn38xx;
+	struct cvmx_tim_mem_ring0_s           cn38xxp2;
+	struct cvmx_tim_mem_ring0_s           cn50xx;
+	struct cvmx_tim_mem_ring0_s           cn52xx;
+	struct cvmx_tim_mem_ring0_s           cn52xxp1;
+	struct cvmx_tim_mem_ring0_s           cn56xx;
+	struct cvmx_tim_mem_ring0_s           cn56xxp1;
+	struct cvmx_tim_mem_ring0_s           cn58xx;
+	struct cvmx_tim_mem_ring0_s           cn58xxp1;
+	struct cvmx_tim_mem_ring0_s           cn61xx;
+	struct cvmx_tim_mem_ring0_s           cn63xx;
+	struct cvmx_tim_mem_ring0_s           cn63xxp1;
+	struct cvmx_tim_mem_ring0_s           cn66xx;
+	struct cvmx_tim_mem_ring0_s           cn70xx;
+	struct cvmx_tim_mem_ring0_s           cn70xxp1;
+	struct cvmx_tim_mem_ring0_s           cnf71xx;
+};
+typedef union cvmx_tim_mem_ring0 cvmx_tim_mem_ring0_t;
+
+/**
+ * cvmx_tim_mem_ring1
+ *
+ * Notes:
+ * After a 1->0 transition on ENA, the HW will still complete a bucket traversal for the ring
+ * if it was pending or active prior to the transition. (SW must delay to ensure the completion
+ * of the traversal before reprogramming the ring.)
+ * Every write to TIM_MEM_RING1 resets the current time offset for the ring to the INTERVAL value.
+ * (The current time offset for the ring is readable via TIM_MEM_DEBUG0[COUNT].)
+ * CSIZE must be at least 16.  It is illegal to program CSIZE to a value that is less than 16.
+ * This CSR is a memory of 16 entries, and thus, the TIM_REG_READ_IDX CSR must be written before any
+ * CSR read operations to this address can be performed.
+ */
+union cvmx_tim_mem_ring1 {
+	uint64_t u64;
+	struct cvmx_tim_mem_ring1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_43_63               : 21;
+	uint64_t enable                       : 1;  /**< Ring timer enable
+                                                         When clear, the ring is disabled and TIM
+                                                         will not traverse any new buckets for the ring. */
+	uint64_t pool                         : 3;  /**< Free list used to free chunks */
+	uint64_t words_per_chunk              : 13; /**< Number of words per chunk */
+	uint64_t interval                     : 22; /**< Timer interval - 1, measured in 1024 cycle ticks */
+	uint64_t ring                         : 4;  /**< Ring ID */
+#else
+	uint64_t ring                         : 4;
+	uint64_t interval                     : 22;
+	uint64_t words_per_chunk              : 13;
+	uint64_t pool                         : 3;
+	uint64_t enable                       : 1;
+	uint64_t reserved_43_63               : 21;
+#endif
+	} s;
+	struct cvmx_tim_mem_ring1_s           cn30xx;
+	struct cvmx_tim_mem_ring1_s           cn31xx;
+	struct cvmx_tim_mem_ring1_s           cn38xx;
+	struct cvmx_tim_mem_ring1_s           cn38xxp2;
+	struct cvmx_tim_mem_ring1_s           cn50xx;
+	struct cvmx_tim_mem_ring1_s           cn52xx;
+	struct cvmx_tim_mem_ring1_s           cn52xxp1;
+	struct cvmx_tim_mem_ring1_s           cn56xx;
+	struct cvmx_tim_mem_ring1_s           cn56xxp1;
+	struct cvmx_tim_mem_ring1_s           cn58xx;
+	struct cvmx_tim_mem_ring1_s           cn58xxp1;
+	struct cvmx_tim_mem_ring1_s           cn61xx;
+	struct cvmx_tim_mem_ring1_s           cn63xx;
+	struct cvmx_tim_mem_ring1_s           cn63xxp1;
+	struct cvmx_tim_mem_ring1_s           cn66xx;
+	struct cvmx_tim_mem_ring1_s           cn70xx;
+	struct cvmx_tim_mem_ring1_s           cn70xxp1;
+	struct cvmx_tim_mem_ring1_s           cnf71xx;
+};
+typedef union cvmx_tim_mem_ring1 cvmx_tim_mem_ring1_t;
+
+/**
+ * cvmx_tim_reg_bist_result
+ *
+ * Notes:
+ * Access to the internal BiST results
+ * Each bit is the BiST result of an individual memory (per bit, 0=pass and 1=fail).
+ */
+union cvmx_tim_reg_bist_result {
+	uint64_t u64;
+	struct cvmx_tim_reg_bist_result_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_4_63                : 60;
+	uint64_t sta                          : 2;  /**< BiST result of the STA   memories (0=pass, !0=fail) */
+	uint64_t ncb                          : 1;  /**< BiST result of the NCB   memories (0=pass, !0=fail) */
+	uint64_t ctl                          : 1;  /**< BiST result of the CTL   memories (0=pass, !0=fail) */
+#else
+	uint64_t ctl                          : 1;
+	uint64_t ncb                          : 1;
+	uint64_t sta                          : 2;
+	uint64_t reserved_4_63                : 60;
+#endif
+	} s;
+	struct cvmx_tim_reg_bist_result_s     cn30xx;
+	struct cvmx_tim_reg_bist_result_s     cn31xx;
+	struct cvmx_tim_reg_bist_result_s     cn38xx;
+	struct cvmx_tim_reg_bist_result_s     cn38xxp2;
+	struct cvmx_tim_reg_bist_result_s     cn50xx;
+	struct cvmx_tim_reg_bist_result_s     cn52xx;
+	struct cvmx_tim_reg_bist_result_s     cn52xxp1;
+	struct cvmx_tim_reg_bist_result_s     cn56xx;
+	struct cvmx_tim_reg_bist_result_s     cn56xxp1;
+	struct cvmx_tim_reg_bist_result_s     cn58xx;
+	struct cvmx_tim_reg_bist_result_s     cn58xxp1;
+	struct cvmx_tim_reg_bist_result_s     cn61xx;
+	struct cvmx_tim_reg_bist_result_s     cn63xx;
+	struct cvmx_tim_reg_bist_result_s     cn63xxp1;
+	struct cvmx_tim_reg_bist_result_s     cn66xx;
+	struct cvmx_tim_reg_bist_result_s     cn70xx;
+	struct cvmx_tim_reg_bist_result_s     cn70xxp1;
+	struct cvmx_tim_reg_bist_result_s     cnf71xx;
+};
+typedef union cvmx_tim_reg_bist_result cvmx_tim_reg_bist_result_t;
+
+/**
+ * cvmx_tim_reg_error
+ *
+ * A ring is in error if its interval has elapsed more than once without having been serviced,
+ * either due to too many events in this ring's previous interval, or another ring having too
+ * many events to process within this ring's interval. This is usually a programming error where
+ * the number of entries in the bucket is too large for the interval specified across the rings.
+ * When in error, TIM may process events in an interval later than requested. Any bit in the INT
+ * field should be cleared by writing one to it.
+ */
+union cvmx_tim_reg_error {
+	uint64_t u64;
+	struct cvmx_tim_reg_error_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t mask                         : 16; /**< Bit mask indicating the rings in error */
+#else
+	uint64_t mask                         : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_tim_reg_error_s           cn30xx;
+	struct cvmx_tim_reg_error_s           cn31xx;
+	struct cvmx_tim_reg_error_s           cn38xx;
+	struct cvmx_tim_reg_error_s           cn38xxp2;
+	struct cvmx_tim_reg_error_s           cn50xx;
+	struct cvmx_tim_reg_error_s           cn52xx;
+	struct cvmx_tim_reg_error_s           cn52xxp1;
+	struct cvmx_tim_reg_error_s           cn56xx;
+	struct cvmx_tim_reg_error_s           cn56xxp1;
+	struct cvmx_tim_reg_error_s           cn58xx;
+	struct cvmx_tim_reg_error_s           cn58xxp1;
+	struct cvmx_tim_reg_error_s           cn61xx;
+	struct cvmx_tim_reg_error_s           cn63xx;
+	struct cvmx_tim_reg_error_s           cn63xxp1;
+	struct cvmx_tim_reg_error_s           cn66xx;
+	struct cvmx_tim_reg_error_s           cn70xx;
+	struct cvmx_tim_reg_error_s           cn70xxp1;
+	struct cvmx_tim_reg_error_s           cnf71xx;
+};
+typedef union cvmx_tim_reg_error cvmx_tim_reg_error_t;
+
+/**
+ * cvmx_tim_reg_flags
+ *
+ * This register provides flags for TIM.
+ *
+ */
+union cvmx_tim_reg_flags {
+	uint64_t u64;
+	struct cvmx_tim_reg_flags_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_7_63                : 57;
+	uint64_t gpio_edge                    : 2;  /**< Edge used for GPIO timing.
+                                                         0x0 = no edges and the timer tick is not generated.
+                                                         0x1 = TIM counts low to high transitions.
+                                                         0x2 = TIM counts high to low transitions.
+                                                         0x3 = TIM counts both low to high and high to low transitions. */
+	uint64_t ena_gpio                     : 1;  /**< Enable the external control of GPIO over the free
+                                                         running timer.
+                                                         When set, free running timer will be driven by GPIO.
+                                                         Free running timer will count posedge or negedge of the
+                                                         GPIO pin based on GPIO_EDGE register. */
+	uint64_t ena_dfb                      : 1;  /**< Enable Don't Free Buffer. When set chunk buffer
+                                                         would not be released by the TIM back to FPA. */
+	uint64_t reset                        : 1;  /**< Reset oneshot pulse for free-running structures */
+	uint64_t enable_dwb                   : 1;  /**< Enables non-zero DonwWriteBacks when set
+                                                         When set, enables the use of
+                                                         DontWriteBacks during the buffer freeing
+                                                         operations. */
+	uint64_t enable_timers                : 1;  /**< Enables the TIM section when set
+                                                         When set, TIM is in normal operation.
+                                                         When clear, time is effectively stopped for all
+                                                         rings in TIM. */
+#else
+	uint64_t enable_timers                : 1;
+	uint64_t enable_dwb                   : 1;
+	uint64_t reset                        : 1;
+	uint64_t ena_dfb                      : 1;
+	uint64_t ena_gpio                     : 1;
+	uint64_t gpio_edge                    : 2;
+	uint64_t reserved_7_63                : 57;
+#endif
+	} s;
+	struct cvmx_tim_reg_flags_cn30xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_3_63                : 61;
+	uint64_t reset                        : 1;  /**< Reset oneshot pulse for free-running structures */
+	uint64_t enable_dwb                   : 1;  /**< Enables non-zero DonwWriteBacks when set
+                                                         When set, enables the use of
+                                                         DontWriteBacks during the buffer freeing
+                                                         operations. */
+	uint64_t enable_timers                : 1;  /**< Enables the TIM section when set
+                                                         When set, TIM is in normal operation.
+                                                         When clear, time is effectively stopped for all
+                                                         rings in TIM. */
+#else
+	uint64_t enable_timers                : 1;
+	uint64_t enable_dwb                   : 1;
+	uint64_t reset                        : 1;
+	uint64_t reserved_3_63                : 61;
+#endif
+	} cn30xx;
+	struct cvmx_tim_reg_flags_cn30xx      cn31xx;
+	struct cvmx_tim_reg_flags_cn30xx      cn38xx;
+	struct cvmx_tim_reg_flags_cn30xx      cn38xxp2;
+	struct cvmx_tim_reg_flags_cn30xx      cn50xx;
+	struct cvmx_tim_reg_flags_cn30xx      cn52xx;
+	struct cvmx_tim_reg_flags_cn30xx      cn52xxp1;
+	struct cvmx_tim_reg_flags_cn30xx      cn56xx;
+	struct cvmx_tim_reg_flags_cn30xx      cn56xxp1;
+	struct cvmx_tim_reg_flags_cn30xx      cn58xx;
+	struct cvmx_tim_reg_flags_cn30xx      cn58xxp1;
+	struct cvmx_tim_reg_flags_cn30xx      cn61xx;
+	struct cvmx_tim_reg_flags_cn30xx      cn63xx;
+	struct cvmx_tim_reg_flags_cn30xx      cn63xxp1;
+	struct cvmx_tim_reg_flags_cn30xx      cn66xx;
+	struct cvmx_tim_reg_flags_s           cn68xx;
+	struct cvmx_tim_reg_flags_s           cn68xxp1;
+	struct cvmx_tim_reg_flags_cn30xx      cn70xx;
+	struct cvmx_tim_reg_flags_cn30xx      cn70xxp1;
+	struct cvmx_tim_reg_flags_cn78xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_7_63                : 57;
+	uint64_t gpio_edge                    : 2;  /**< Edge used for GPIO timing.
+                                                         0x0 = no edges and the timer tick is not generated.
+                                                         0x1 = TIM counts low to high transitions.
+                                                         0x2 = TIM counts high to low transitions.
+                                                         0x3 = TIM counts both low to high and high to low transitions. */
+	uint64_t reserved_3_4                 : 2;
+	uint64_t reset                        : 1;  /**< Reset. One-shot pulse for free-running timer FR_RN_HT. */
+	uint64_t reserved_1_1                 : 1;
+	uint64_t enable_timers                : 1;  /**< When set, TIM is in normal operation. When clear, time is effectively stopped for all
+                                                         rings in TIM.
+                                                         TIM has a counter (see FR_RN_HT) that causes a periodic tick. This counter is shared by
+                                                         all rings. Each Timer tick causes the hardware to decrement the time count for all enabled
+                                                         rings.
+                                                         When ENA_TIM = 0, the hardware stops the shared periodic counter, FR_RN_HT, so there are
+                                                         no more ticks, and there are no more new bucket traversals.
+                                                         If ENA_TIM transitions 1->0, TIM longer creates new bucket traversals, but does traverse
+                                                         any rings that previously expired and are pending hardware traversal. */
+#else
+	uint64_t enable_timers                : 1;
+	uint64_t reserved_1_1                 : 1;
+	uint64_t reset                        : 1;
+	uint64_t reserved_3_4                 : 2;
+	uint64_t gpio_edge                    : 2;
+	uint64_t reserved_7_63                : 57;
+#endif
+	} cn78xx;
+	struct cvmx_tim_reg_flags_cn30xx      cnf71xx;
+};
+typedef union cvmx_tim_reg_flags cvmx_tim_reg_flags_t;
+
+/**
+ * cvmx_tim_reg_int_mask
+ *
+ * Notes:
+ * Note that this CSR is present only in chip revisions beginning with pass2.
+ * When mask bit is set, the interrupt is enabled.
+ */
+union cvmx_tim_reg_int_mask {
+	uint64_t u64;
+	struct cvmx_tim_reg_int_mask_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t mask                         : 16; /**< Bit mask corresponding to TIM_REG_ERROR.MASK above */
+#else
+	uint64_t mask                         : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_tim_reg_int_mask_s        cn30xx;
+	struct cvmx_tim_reg_int_mask_s        cn31xx;
+	struct cvmx_tim_reg_int_mask_s        cn38xx;
+	struct cvmx_tim_reg_int_mask_s        cn38xxp2;
+	struct cvmx_tim_reg_int_mask_s        cn50xx;
+	struct cvmx_tim_reg_int_mask_s        cn52xx;
+	struct cvmx_tim_reg_int_mask_s        cn52xxp1;
+	struct cvmx_tim_reg_int_mask_s        cn56xx;
+	struct cvmx_tim_reg_int_mask_s        cn56xxp1;
+	struct cvmx_tim_reg_int_mask_s        cn58xx;
+	struct cvmx_tim_reg_int_mask_s        cn58xxp1;
+	struct cvmx_tim_reg_int_mask_s        cn61xx;
+	struct cvmx_tim_reg_int_mask_s        cn63xx;
+	struct cvmx_tim_reg_int_mask_s        cn63xxp1;
+	struct cvmx_tim_reg_int_mask_s        cn66xx;
+	struct cvmx_tim_reg_int_mask_s        cn70xx;
+	struct cvmx_tim_reg_int_mask_s        cn70xxp1;
+	struct cvmx_tim_reg_int_mask_s        cnf71xx;
+};
+typedef union cvmx_tim_reg_int_mask cvmx_tim_reg_int_mask_t;
+
+/**
+ * cvmx_tim_reg_read_idx
+ *
+ * Notes:
+ * Provides the read index during a CSR read operation to any of the CSRs that are physically stored
+ * as memories.  The names of these CSRs begin with the prefix "TIM_MEM_".
+ * IDX[7:0] is the read index.  INC[7:0] is an increment that is added to IDX[7:0] after any CSR read.
+ * The intended use is to initially write this CSR such that IDX=0 and INC=1.  Then, the entire
+ * contents of a CSR memory can be read with consecutive CSR read commands.
+ */
+union cvmx_tim_reg_read_idx {
+	uint64_t u64;
+	struct cvmx_tim_reg_read_idx_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t inc                          : 8;  /**< Increment to add to current index for next index */
+	uint64_t index                        : 8;  /**< Index to use for next memory CSR read */
+#else
+	uint64_t index                        : 8;
+	uint64_t inc                          : 8;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_tim_reg_read_idx_s        cn30xx;
+	struct cvmx_tim_reg_read_idx_s        cn31xx;
+	struct cvmx_tim_reg_read_idx_s        cn38xx;
+	struct cvmx_tim_reg_read_idx_s        cn38xxp2;
+	struct cvmx_tim_reg_read_idx_s        cn50xx;
+	struct cvmx_tim_reg_read_idx_s        cn52xx;
+	struct cvmx_tim_reg_read_idx_s        cn52xxp1;
+	struct cvmx_tim_reg_read_idx_s        cn56xx;
+	struct cvmx_tim_reg_read_idx_s        cn56xxp1;
+	struct cvmx_tim_reg_read_idx_s        cn58xx;
+	struct cvmx_tim_reg_read_idx_s        cn58xxp1;
+	struct cvmx_tim_reg_read_idx_s        cn61xx;
+	struct cvmx_tim_reg_read_idx_s        cn63xx;
+	struct cvmx_tim_reg_read_idx_s        cn63xxp1;
+	struct cvmx_tim_reg_read_idx_s        cn66xx;
+	struct cvmx_tim_reg_read_idx_s        cn70xx;
+	struct cvmx_tim_reg_read_idx_s        cn70xxp1;
+	struct cvmx_tim_reg_read_idx_s        cnf71xx;
+};
+typedef union cvmx_tim_reg_read_idx cvmx_tim_reg_read_idx_t;
+
+/**
+ * cvmx_tim_ring#_aura
+ */
+union cvmx_tim_ringx_aura {
+	uint64_t u64;
+	struct cvmx_tim_ringx_aura_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_16_63               : 48;
+	uint64_t aura                         : 16; /**< Aura number used to free and return chucks to. Bits <15:12> must be zero. */
+#else
+	uint64_t aura                         : 16;
+	uint64_t reserved_16_63               : 48;
+#endif
+	} s;
+	struct cvmx_tim_ringx_aura_s          cn78xx;
+};
+typedef union cvmx_tim_ringx_aura cvmx_tim_ringx_aura_t;
+
+/**
+ * cvmx_tim_ring#_ctl0
+ *
+ * Notes:
+ * This CSR is a memory of 64 entries
+ * After a 1 to 0 transition on ENA, the HW will still complete a bucket traversal for the ring
+ * if it was pending or active prior to the transition. (SW must delay to ensure the completion
+ * of the traversal before reprogramming the ring.)
+ */
+union cvmx_tim_ringx_ctl0 {
+	uint64_t u64;
+	struct cvmx_tim_ringx_ctl0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_0_63                : 64;
+#else
+	uint64_t reserved_0_63                : 64;
+#endif
+	} s;
+	struct cvmx_tim_ringx_ctl0_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_47_63               : 17;
+	uint64_t ena                          : 1;  /**< Ring timer enable */
+	uint64_t intc                         : 2;  /**< Interval count for Error. Defines how many intervals
+                                                         could elapse from bucket expiration till actual
+                                                         bucket traversal before HW asserts an error.
+                                                         Typical value is 0,1,2. */
+	uint64_t timercount                   : 22; /**< Timer Count represents the ring offset; how many timer
+                                                         ticks have left till the interval expiration.
+                                                         Typical initialization value should be Interval/Constant,
+                                                         it is recommended that constant should be unique per ring
+                                                         This will create an offset between the rings.
+                                                         Once ENA is set,
+                                                         TIMERCOUNT counts down timer ticks. When TIMERCOUNT
+                                                         reaches zero, ring's interval expired and the HW forces
+                                                         a bucket traversal (and resets TIMERCOUNT to INTERVAL)
+                                                         TIMERCOUNT is unpredictable whenever ENA==0.
+                                                         It is SW responsibility to set TIMERCOUNT before
+                                                         TIM_RINGX_CTL0.ENA transitions from 0 to 1.
+                                                         When the field is set to X it would take X+1 timer tick
+                                                         for the interval to expire. */
+	uint64_t interval                     : 22; /**< Timer interval. Measured in Timer Ticks, where timer
+                                                         ticks are defined by TIM_FR_RN_TT.FR_RN_TT. */
+#else
+	uint64_t interval                     : 22;
+	uint64_t timercount                   : 22;
+	uint64_t intc                         : 2;
+	uint64_t ena                          : 1;
+	uint64_t reserved_47_63               : 17;
+#endif
+	} cn68xx;
+	struct cvmx_tim_ringx_ctl0_cn68xx     cn68xxp1;
+	struct cvmx_tim_ringx_ctl0_cn78xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t expire_offset                : 32; /**< Time at which the next bucket will be serviced, or offset. See also TIM_RING()_REL
+                                                         for the position relative to current time.
+                                                         If TIM_RING()_CTL1[ENA] = 0, then contains an offset. When ENA transitions from a
+                                                         zero to a one this offset will be added to the current time and loaded back into
+                                                         EXPIRE_OFFSET. Thus the offset sets the delta time between ENA transitioning to one and
+                                                         the very first time the ring will be serviced. Software should program different offsets
+                                                         on each ring to reduce congestion to prevent many rings from otherwise expiring
+                                                         concurrently.
+                                                         If TIM_RING()_CTL1[ENA] = 1, then contains the time the next bucket will be serviced.
+                                                         When EXPIRE_OFFSET reaches the current time (TIM_FR_RN_CYCLES or TIM_FR_RN_GPIOS),
+                                                         EXPIRE_OFFSET is set to the next expiration time (current time plus
+                                                         TIM_RING()_CTL0[INTERVAL]).
+                                                         EXPIRE_OFFSET is unpredictable after ENA_GPIO changes or TIM_RING()_CTL1[ENA]
+                                                         transitions from 1 to 0, and must be reprogrammed before (re-) setting
+                                                         TIM_RING()_CTL1[ENA]. */
+	uint64_t interval                     : 32; /**< Timer interval, measured in cycles or GPIO transitions.
+                                                         For every 64 entries in a bucket, the interval should be at least 1u. Minimal recommended
+                                                         value is 1u. */
+#else
+	uint64_t interval                     : 32;
+	uint64_t expire_offset                : 32;
+#endif
+	} cn78xx;
+};
+typedef union cvmx_tim_ringx_ctl0 cvmx_tim_ringx_ctl0_t;
+
+/**
+ * cvmx_tim_ring#_ctl1
+ *
+ * Notes:
+ * This CSR is a memory of 64 entries
+ * ***NOTE: Added fields in pass 2.0
+ */
+union cvmx_tim_ringx_ctl1 {
+	uint64_t u64;
+	struct cvmx_tim_ringx_ctl1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_51_63               : 13;
+	uint64_t rcf_busy                     : 1;  /**< Ring reconfiguration busy. When ENA is cleared, this bit will remain set until hardware
+                                                         completes the idling of the ring. ENA must not be re-enabled until clear. */
+	uint64_t intc                         : 2;  /**< Interval count for error. Defines how many intervals could elapse from bucket expiration
+                                                         until actual bucket traversal before hardware asserts an error. Typical value is 0x0, 0x1,
+                                                         0x2. */
+	uint64_t ena                          : 1;  /**< Ring timer enable. After a 1 to 0 transition on ENA, the hardware still completes a bucket
+                                                         traversal for the ring if it were pending or active prior to the transition. When
+                                                         clearing, software must delay until TIM_RING()_REL[RING_ESR] = 0 to ensure the
+                                                         completion of the traversal before reprogramming the ring. When setting, RCF_BUSY must be
+                                                         clear. */
+	uint64_t ena_gpio                     : 1;  /**< When set, ring's timer tick is generated by the GPIO timer. The GPIO edge is defined by
+                                                         TIM_REG_FLAGS[GPIO_EDGE]. The default value (zero) means that timer ticks are generated
+                                                         from the internal timer. To change ENA_GPIO:
+                                                         1. TIM_RING()_CTL1[ENA] is cleared.
+                                                         2. [ENA_GPIO] is changed.
+                                                         3. TIM_RING()_CTL0[EXPIRE_OFFSET] is reprogrammed appropriately.
+                                                         4. TIM_RING()_CTL1[ENA] is set. */
+	uint64_t ena_prd                      : 1;  /**< Enable periodic mode, which disables the memory write of zeros to NUM_ENTRIES and
+                                                         CHUNK_REMAINDER when a bucket is traversed. In periodic mode ENA_DFB and ENA_LDWB must
+                                                         also be clear. */
+	uint64_t reserved_44_44               : 1;
+	uint64_t ena_dfb                      : 1;  /**< Enable don't free buffer. When set, chunk buffer is not released by the TIM back to FPA. */
+	uint64_t cpool                        : 3;  /**< FPA Free list to free chunks to. */
+	uint64_t bucket                       : 20; /**< Current bucket. Should be set to 0x0 by software at enable time. Incremented once per
+                                                         bucket traversal. */
+	uint64_t bsize                        : 20; /**< Number of buckets minus one. If BSIZE = 0, there is only one bucket in the ring. */
+#else
+	uint64_t bsize                        : 20;
+	uint64_t bucket                       : 20;
+	uint64_t cpool                        : 3;
+	uint64_t ena_dfb                      : 1;
+	uint64_t reserved_44_44               : 1;
+	uint64_t ena_prd                      : 1;
+	uint64_t ena_gpio                     : 1;
+	uint64_t ena                          : 1;
+	uint64_t intc                         : 2;
+	uint64_t rcf_busy                     : 1;
+	uint64_t reserved_51_63               : 13;
+#endif
+	} s;
+	struct cvmx_tim_ringx_ctl1_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_47_63               : 17;
+	uint64_t ena_gpio                     : 1;  /**< When set, ring's timer tick will be generated by the
+                                                         GPIO Timer. GPIO edge is defined by
+                                                         TIM_REG_FLAGS.GPIO_EDGE
+                                                         Default value zero means that timer ticks will
+                                                         be genearated from the Internal Timer */
+	uint64_t ena_prd                      : 1;  /**< Enable Periodic Mode which would disable the memory
+                                                         write of zeros to num_entries and chunk_remainder
+                                                         when a bucket is traveresed. */
+	uint64_t ena_dwb                      : 1;  /**< When set, enables the use of Dont Write Back during
+                                                         FPA buffer freeing operations */
+	uint64_t ena_dfb                      : 1;  /**< Enable Don't Free Buffer. When set chunk buffer
+                                                         would not be released by the TIM back to FPA. */
+	uint64_t cpool                        : 3;  /**< FPA Free list to free chunks to. */
+	uint64_t bucket                       : 20; /**< Current bucket. Should be set to zero by SW at
+                                                         enable time.
+                                                         Incremented once per bucket traversal. */
+	uint64_t bsize                        : 20; /**< Number of buckets minus one. If BSIZE==0 there is
+                                                         only one bucket in the ring. */
+#else
+	uint64_t bsize                        : 20;
+	uint64_t bucket                       : 20;
+	uint64_t cpool                        : 3;
+	uint64_t ena_dfb                      : 1;
+	uint64_t ena_dwb                      : 1;
+	uint64_t ena_prd                      : 1;
+	uint64_t ena_gpio                     : 1;
+	uint64_t reserved_47_63               : 17;
+#endif
+	} cn68xx;
+	struct cvmx_tim_ringx_ctl1_cn68xxp1 {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_43_63               : 21;
+	uint64_t cpool                        : 3;  /**< FPA Free list to free chunks to. */
+	uint64_t bucket                       : 20; /**< Current bucket. Should be set to zero by SW at
+                                                         enable time.
+                                                         Incremented once per bucket traversal. */
+	uint64_t bsize                        : 20; /**< Number of buckets minus one. If BSIZE==0 there is
+                                                         only one bucket in the ring. */
+#else
+	uint64_t bsize                        : 20;
+	uint64_t bucket                       : 20;
+	uint64_t cpool                        : 3;
+	uint64_t reserved_43_63               : 21;
+#endif
+	} cn68xxp1;
+	struct cvmx_tim_ringx_ctl1_cn78xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_51_63               : 13;
+	uint64_t rcf_busy                     : 1;  /**< Ring reconfiguration busy. When ENA is cleared, this bit will remain set until hardware
+                                                         completes the idling of the ring. ENA must not be re-enabled until clear. */
+	uint64_t intc                         : 2;  /**< Interval count for error. Defines how many intervals could elapse from bucket expiration
+                                                         until actual bucket traversal before hardware asserts an error. Typical value is 0x0, 0x1,
+                                                         0x2. */
+	uint64_t ena                          : 1;  /**< Ring timer enable. After a 1 to 0 transition on ENA, the hardware still completes a bucket
+                                                         traversal for the ring if it were pending or active prior to the transition. When
+                                                         clearing, software must delay until TIM_RING()_REL[RING_ESR] = 0 to ensure the
+                                                         completion of the traversal before reprogramming the ring. When setting, RCF_BUSY must be
+                                                         clear. */
+	uint64_t ena_gpio                     : 1;  /**< When set, ring's timer tick is generated by the GPIO timer. The GPIO edge is defined by
+                                                         TIM_REG_FLAGS[GPIO_EDGE]. The default value (zero) means that timer ticks are generated
+                                                         from the internal timer. To change ENA_GPIO:
+                                                         1. TIM_RING()_CTL1[ENA] is cleared.
+                                                         2. [ENA_GPIO] is changed.
+                                                         3. TIM_RING()_CTL0[EXPIRE_OFFSET] is reprogrammed appropriately.
+                                                         4. TIM_RING()_CTL1[ENA] is set. */
+	uint64_t ena_prd                      : 1;  /**< Enable periodic mode, which disables the memory write of zeros to NUM_ENTRIES and
+                                                         CHUNK_REMAINDER when a bucket is traversed. In periodic mode ENA_DFB and ENA_LDWB must
+                                                         also be clear. */
+	uint64_t ena_ldwb                     : 1;  /**< When set, enables the use of Load and Don't-Write-Back when reading timer entry cache lines. */
+	uint64_t ena_dfb                      : 1;  /**< Enable don't free buffer. When set, chunk buffer is not released by the TIM back to FPA. */
+	uint64_t reserved_40_42               : 3;
+	uint64_t bucket                       : 20; /**< Current bucket. Should be set to 0x0 by software at enable time. Incremented once per
+                                                         bucket traversal. */
+	uint64_t bsize                        : 20; /**< Number of buckets minus one. If BSIZE = 0, there is only one bucket in the ring. */
+#else
+	uint64_t bsize                        : 20;
+	uint64_t bucket                       : 20;
+	uint64_t reserved_40_42               : 3;
+	uint64_t ena_dfb                      : 1;
+	uint64_t ena_ldwb                     : 1;
+	uint64_t ena_prd                      : 1;
+	uint64_t ena_gpio                     : 1;
+	uint64_t ena                          : 1;
+	uint64_t intc                         : 2;
+	uint64_t rcf_busy                     : 1;
+	uint64_t reserved_51_63               : 13;
+#endif
+	} cn78xx;
+};
+typedef union cvmx_tim_ringx_ctl1 cvmx_tim_ringx_ctl1_t;
+
+/**
+ * cvmx_tim_ring#_ctl2
+ *
+ * Notes:
+ * BASE is a 32-byte aligned pointer[35:0].  Only pointer[35:5] are stored because pointer[4:0] = 0.
+ * This CSR is a memory of 64 entries
+ */
+union cvmx_tim_ringx_ctl2 {
+	uint64_t u64;
+	struct cvmx_tim_ringx_ctl2_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_0_63                : 64;
+#else
+	uint64_t reserved_0_63                : 64;
+#endif
+	} s;
+	struct cvmx_tim_ringx_ctl2_cn68xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_47_63               : 17;
+	uint64_t csize                        : 13; /**< Number of words per chunk. CSIZE mod(16) should be
+                                                         zero. */
+	uint64_t reserved_31_33               : 3;
+	uint64_t base                         : 31; /**< Pointer[35:5] to bucket[0] */
+#else
+	uint64_t base                         : 31;
+	uint64_t reserved_31_33               : 3;
+	uint64_t csize                        : 13;
+	uint64_t reserved_47_63               : 17;
+#endif
+	} cn68xx;
+	struct cvmx_tim_ringx_ctl2_cn68xx     cn68xxp1;
+	struct cvmx_tim_ringx_ctl2_cn78xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_53_63               : 11;
+	uint64_t csize                        : 13; /**< Number of eight-byte words per chunk. CSIZE mod(16) should be zero. */
+	uint64_t reserved_37_39               : 3;
+	uint64_t base                         : 37; /**< Pointer<41:5> to bucket<0>. */
+#else
+	uint64_t base                         : 37;
+	uint64_t reserved_37_39               : 3;
+	uint64_t csize                        : 13;
+	uint64_t reserved_53_63               : 11;
+#endif
+	} cn78xx;
+};
+typedef union cvmx_tim_ringx_ctl2 cvmx_tim_ringx_ctl2_t;
+
+/**
+ * cvmx_tim_ring#_dbg0
+ */
+union cvmx_tim_ringx_dbg0 {
+	uint64_t u64;
+	struct cvmx_tim_ringx_dbg0_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t fr_rn_ht                     : 22; /**< Free Running Hardware Timer. Shared by all rings and is
+                                                         used to generate the Timer Tick based on
+                                                         FR_RN_TT. */
+	uint64_t timercount                   : 22; /**< Timer Count represents the ring's offset.
+                                                         Refer to TIM_RINGX_CTL0. */
+	uint64_t cur_bucket                   : 20; /**< Current bucket. Indicates the ring's current bucket.
+                                                         Refer to TIM_RINGX_CTL1.BUCKET. */
+#else
+	uint64_t cur_bucket                   : 20;
+	uint64_t timercount                   : 22;
+	uint64_t fr_rn_ht                     : 22;
+#endif
+	} s;
+	struct cvmx_tim_ringx_dbg0_s          cn68xx;
+	struct cvmx_tim_ringx_dbg0_s          cn68xxp1;
+};
+typedef union cvmx_tim_ringx_dbg0 cvmx_tim_ringx_dbg0_t;
+
+/**
+ * cvmx_tim_ring#_dbg1
+ */
+union cvmx_tim_ringx_dbg1 {
+	uint64_t u64;
+	struct cvmx_tim_ringx_dbg1_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t reserved_2_63                : 62;
+	uint64_t ring_esr                     : 2;  /**< Ring Expiration Status Register.
+                                                         This register hold the expiration status of the ring.
+                                                         2'b00 - Ring was recently traversed.
+                                                         2'b01 - Interval expired. Ring is queued to be traversed.
+                                                         2'b10 - 1st interval expiration while ring is queued to be
+                                                         traversed.
+                                                         2'b11 - 2nd interval expiration while ring is queued to be
+                                                         traversed. */
+#else
+	uint64_t ring_esr                     : 2;
+	uint64_t reserved_2_63                : 62;
+#endif
+	} s;
+	struct cvmx_tim_ringx_dbg1_s          cn68xx;
+	struct cvmx_tim_ringx_dbg1_s          cn68xxp1;
+};
+typedef union cvmx_tim_ringx_dbg1 cvmx_tim_ringx_dbg1_t;
+
+/**
+ * cvmx_tim_ring#_rel
+ *
+ * Current positions of the TIM walker in both time and ring position, for easy synchronization
+ * with software.
+ */
+union cvmx_tim_ringx_rel {
+	uint64_t u64;
+	struct cvmx_tim_ringx_rel_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+	uint64_t cur_bucket                   : 20; /**< Current bucket. Indicates the ring's current bucket. See TIM_RING()_CTL1[BUCKET]. */
+	uint64_t reserved_34_43               : 10;
+	uint64_t ring_esr                     : 2;  /**< Ring expiration status register. These registers hold the expiration status of the ring.
+                                                         0x0 = Ring has not expired.
+                                                         0x1 = Interval expired. Ring is queued to be traversed.
+                                                         0x2 = First interval expiration while ring is queued to be traversed.
+                                                         0x3 = Second interval expiration while ring is queued to be traversed.
+                                                         This field is zeroed when TIM_RING()_CTL1[ENA] transitions from 0 to 1. */
+	uint64_t timercount                   : 32; /**< Timer count indicates how many timer ticks are left until the interval expiration,
+                                                         calculated as TIM_RING()_CTL0[EXPIRE_OFFSET] minus current time (TIM_FR_RN_CYCLES or
+                                                         TIM_FR_RN_GPIOS).
+                                                         Once ENA = 1, TIMERCOUNT will be observed to count down timer ticks. When TIMERCOUNT
+                                                         reaches 0x0, the ring's interval expired and the hardware forces a bucket traversal (and
+                                                         increments RING_ESR).
+                                                         Typical initialization value should be interval/constant; Cavium recommends that the
+                                                         constant be unique per ring. This creates an offset between the rings.
+                                                         TIMERCOUNT becomes and remains unpredictable whenever ENA = 0 or ENA_GPIO changes. It is
+                                                         software's responsibility to set TIMERCOUNT before TIM_RING()_CTL1[ENA] transitions
+                                                         from 0 -> 1. */
+#else
+	uint64_t timercount                   : 32;
+	uint64_t ring_esr                     : 2;
+	uint64_t reserved_34_43               : 10;
+	uint64_t cur_bucket                   : 20;
+#endif
+	} s;
+	struct cvmx_tim_ringx_rel_s           cn78xx;
+};
+typedef union cvmx_tim_ringx_rel cvmx_tim_ringx_rel_t;
+
+#endif
diff --git a/arch/mips/include/asm/octeon/cvmx-wqe.h b/arch/mips/include/asm/octeon/cvmx-wqe.h
index aa0d3d0..db3d59d 100644
--- a/arch/mips/include/asm/octeon/cvmx-wqe.h
+++ b/arch/mips/include/asm/octeon/cvmx-wqe.h
@@ -42,13 +42,286 @@
 
 #include <asm/octeon/cvmx-packet.h>
 
-
 #define OCT_TAG_TYPE_STRING(x)						\
 	(((x) == CVMX_POW_TAG_TYPE_ORDERED) ?  "ORDERED" :		\
 		(((x) == CVMX_POW_TAG_TYPE_ATOMIC) ?  "ATOMIC" :	\
 			(((x) == CVMX_POW_TAG_TYPE_NULL) ?  "NULL" :	\
 				"NULL_NULL")))
 
+/* Error levels in WQE entry word2 */
+#define PKI_ERRLEV_E__RE_M                                 (0x0)
+#define PKI_ERRLEV_E__LA_M                                 (0x1)
+#define PKI_ERRLEV_E__LB_M                                 (0x2)
+#define PKI_ERRLEV_E__LC_M                                 (0x3)
+#define PKI_ERRLEV_E__LD_M                                 (0x4)
+#define PKI_ERRLEV_E__LE_M                                 (0x5)
+#define PKI_ERRLEV_E__LF_M                                 (0x6)
+#define PKI_ERRLEV_E__LG_M                                 (0x7)
+
+enum cvmx_pki_errlevel {
+	CVMX_PKI_ERRLEV_E_RE                         = PKI_ERRLEV_E__RE_M,
+	CVMX_PKI_ERRLEV_E_LA                         = PKI_ERRLEV_E__LA_M,
+	CVMX_PKI_ERRLEV_E_LB                         = PKI_ERRLEV_E__LB_M,
+	CVMX_PKI_ERRLEV_E_LC                         = PKI_ERRLEV_E__LC_M,
+	CVMX_PKI_ERRLEV_E_LD                         = PKI_ERRLEV_E__LD_M,
+	CVMX_PKI_ERRLEV_E_LE                         = PKI_ERRLEV_E__LE_M,
+	CVMX_PKI_ERRLEV_E_LF                         = PKI_ERRLEV_E__LF_M,
+	CVMX_PKI_ERRLEV_E_LG                         = PKI_ERRLEV_E__LG_M
+};
+
+/* Layer types in pki */
+#define CVMX_PKI_LTYPE_E_NONE_M                                (0x0)
+#define CVMX_PKI_LTYPE_E_ENET_M                                (0x1)
+#define CVMX_PKI_LTYPE_E_VLAN_M                                (0x2)
+#define CVMX_PKI_LTYPE_E_SNAP_PAYLD_M                          (0x5)
+#define CVMX_PKI_LTYPE_E_ARP_M                                 (0x6)
+#define CVMX_PKI_LTYPE_E_RARP_M                                (0x7)
+#define CVMX_PKI_LTYPE_E_IP4_M                                 (0x8)
+#define CVMX_PKI_LTYPE_E_IP4_OPT_M                             (0x9)
+#define CVMX_PKI_LTYPE_E_IP6_M                                 (0xA)
+#define CVMX_PKI_LTYPE_E_IP6_OPT_M                             (0xB)
+#define CVMX_PKI_LTYPE_E_IPSEC_ESP_M                           (0xC)
+#define CVMX_PKI_LTYPE_E_IPFRAG_M                              (0xD)
+#define CVMX_PKI_LTYPE_E_IPCOMP_M                              (0xE)
+#define CVMX_PKI_LTYPE_E_TCP_M                                 (0x10)
+#define CVMX_PKI_LTYPE_E_UDP_M                                 (0x11)
+#define CVMX_PKI_LTYPE_E_SCTP_M                                (0x12)
+#define CVMX_PKI_LTYPE_E_UDP_VXLAN_M                           (0x13)
+#define CVMX_PKI_LTYPE_E_GRE_M                                 (0x14)
+#define CVMX_PKI_LTYPE_E_NVGRE_M                               (0x15)
+#define CVMX_PKI_LTYPE_E_GTP_M                                 (0x16)
+#define CVMX_PKI_LTYPE_E_SW28_M                                (0x1C)
+#define CVMX_PKI_LTYPE_E_SW29_M                                (0x1D)
+#define CVMX_PKI_LTYPE_E_SW30_M                                (0x1E)
+#define CVMX_PKI_LTYPE_E_SW31_M                                (0x1F)
+
+enum cvmx_pki_layer_type {
+	CVMX_PKI_LTYPE_E_NONE                        = CVMX_PKI_LTYPE_E_NONE_M,
+	CVMX_PKI_LTYPE_E_ENET                        = CVMX_PKI_LTYPE_E_ENET_M,
+	CVMX_PKI_LTYPE_E_VLAN                        = CVMX_PKI_LTYPE_E_VLAN_M,
+	CVMX_PKI_LTYPE_E_SNAP_PAYLD                  = CVMX_PKI_LTYPE_E_SNAP_PAYLD_M,
+	CVMX_PKI_LTYPE_E_ARP                         = CVMX_PKI_LTYPE_E_ARP_M,
+	CVMX_PKI_LTYPE_E_RARP                        = CVMX_PKI_LTYPE_E_RARP_M,
+	CVMX_PKI_LTYPE_E_IP4                         = CVMX_PKI_LTYPE_E_IP4_M,
+	CVMX_PKI_LTYPE_E_IP4_OPT                     = CVMX_PKI_LTYPE_E_IP4_OPT_M,
+	CVMX_PKI_LTYPE_E_IP6                         = CVMX_PKI_LTYPE_E_IP6_M,
+	CVMX_PKI_LTYPE_E_IP6_OPT                     = CVMX_PKI_LTYPE_E_IP6_OPT_M,
+	CVMX_PKI_LTYPE_E_IPSEC_ESP                   = CVMX_PKI_LTYPE_E_IPSEC_ESP_M,
+	CVMX_PKI_LTYPE_E_IPFRAG                      = CVMX_PKI_LTYPE_E_IPFRAG_M,
+	CVMX_PKI_LTYPE_E_IPCOMP                      = CVMX_PKI_LTYPE_E_IPCOMP_M,
+	CVMX_PKI_LTYPE_E_TCP                         = CVMX_PKI_LTYPE_E_TCP_M,
+	CVMX_PKI_LTYPE_E_UDP                         = CVMX_PKI_LTYPE_E_UDP_M,
+	CVMX_PKI_LTYPE_E_SCTP                        = CVMX_PKI_LTYPE_E_SCTP_M,
+	CVMX_PKI_LTYPE_E_UDP_VXLAN                   = CVMX_PKI_LTYPE_E_UDP_VXLAN_M,
+	CVMX_PKI_LTYPE_E_GRE                         = CVMX_PKI_LTYPE_E_GRE_M,
+	CVMX_PKI_LTYPE_E_NVGRE                       = CVMX_PKI_LTYPE_E_NVGRE_M,
+	CVMX_PKI_LTYPE_E_GTP                         = CVMX_PKI_LTYPE_E_GTP_M,
+	CVMX_PKI_LTYPE_E_SW28                        = CVMX_PKI_LTYPE_E_SW28_M,
+	CVMX_PKI_LTYPE_E_SW29                        = CVMX_PKI_LTYPE_E_SW29_M,
+	CVMX_PKI_LTYPE_E_SW30                        = CVMX_PKI_LTYPE_E_SW30_M,
+	CVMX_PKI_LTYPE_E_SW31                        = CVMX_PKI_LTYPE_E_SW31_M
+};
+
+typedef union {
+	uint64_t u64;
+#ifdef __BIG_ENDIAN_BITFIELD
+	struct {
+		uint64_t rsvd_0:4;
+		uint64_t aura:12;
+		uint64_t rsvd_1:1;
+		uint64_t apad:3;
+		uint64_t channel:12;
+		uint64_t bufs:8;
+		uint64_t style:8;
+		uint64_t rsvd_2:10;
+		uint64_t pknd:6;
+	};
+#else
+	struct {
+		uint64_t pknd:6;
+		uint64_t rsvd_2:10;
+		uint64_t style:8;
+		uint64_t bufs:8;
+		uint64_t channel:12;
+		uint64_t apad:3;
+		uint64_t rsvd_1:1;
+		uint64_t aura:12;
+		uint64_t rsvd_0:4;
+	};
+#endif
+} cvmx_pki_wqe_word0_t;
+
+typedef union {
+	uint64_t u64;
+	struct {
+		CVMX_BITFIELD_FIELD(uint64_t len:16,
+		CVMX_BITFIELD_FIELD(uint64_t rsvd_0:2,
+		CVMX_BITFIELD_FIELD(uint64_t rsvd_1:2,
+		/**
+		 * the group that the work queue entry will be scheduled to
+		 */
+		CVMX_BITFIELD_FIELD(uint64_t grp:10,
+		CVMX_BITFIELD_FIELD(cvmx_pow_tag_type_t tag_type:2,
+		CVMX_BITFIELD_FIELD(uint64_t tag:32,
+		))))));
+	};
+} cvmx_pki_wqe_word1_t;
+
+typedef union {
+	uint64_t u64;
+#ifdef __BIG_ENDIAN_BITFIELD
+	struct {
+		uint64_t software:1;		  /**< reserved for software use, hardware always writes 0 to this bit */
+		uint64_t lg_hdr_type:5;		  /**< Indicates the Layer G header typed parsed,PKI_LTYPE_E */
+		uint64_t lf_hdr_type:5;		  /**< Indicates the Layer F header typed parsed, PKI_LTYPE_E */
+		uint64_t le_hdr_type:5;        	  /**< Indicates the Layer E header typed parsed, PKI_LTYPE_E */
+		uint64_t ld_hdr_type:5;		  /**< Indicates the Layer D header typed parsed, PKI_LTYPE_E */
+		uint64_t lc_hdr_type:5;		  /**< Indicates the Layer C header typed parsed, PKI_LTYPE_E */
+		uint64_t lb_hdr_type:5;		  /**< Indicates the Layer B header typed parsed, PKI_LTYPE_E */
+		uint64_t is_la_ether:1;	          /**< Indicates that Layer A Ethernet was parsed */
+		uint64_t rsvd_0:8;
+		uint64_t vlan_valid:1;		  /**< set to 1 if we found VLAN in the L2 */
+		uint64_t vlan_stacked:1;	  /**< set to 1 if the VLAN tag is stacked */
+		uint64_t stat_inc:1;              /**< stat increment Reserved for Statistics hardware */
+		uint64_t pcam_flag4:1;            /**< indicates if PCAM entry has set the flag */
+		uint64_t pcam_flag3:1;            /**< indicates if PCAM entry has set the flag */
+		uint64_t pcam_flag2:1;            /**< indicates if PCAM entry has set the flag */
+		uint64_t pcam_flag1:1;            /**< indicates if PCAM entry has set the flag */
+		uint64_t is_frag:1;		  /**< set when the outer IP indicates a fragment */
+		uint64_t is_l3_bcast:1;           /**< set when outer ip indicates broadcast */
+		uint64_t is_l3_mcast:1;           /**< set when outer ipv4 indicates multicast */
+		uint64_t is_l2_bcast:1;           /**< set when the packet?s destination MAC address
+							field in the L2 HDR is the broadcast address */
+		uint64_t is_l2_mcast:1; 	  /**< set when the packet?s destination MAC address
+							field in the L2 HDR is a multicast address */
+		uint64_t is_raw:1;                /**< set when PKI_INST_HDR[RAW] was set */
+		uint64_t err_level:3;             /**< contains the lowest protocol layer containing error,
+							when errors are detected, normally 0; PKI_ERROR_LEVEL_E */
+		uint64_t err_code:8;              /**< normally 0, but contains a (non-zero) exception opcode
+							enumerated by PKI_OPCODE_E when WQE[ERRLEV] is non-zero */
+	};
+#else
+	struct {
+		uint64_t err_code:8;
+		uint64_t err_level:3;
+		uint64_t is_raw:1;
+		uint64_t is_l2_mcast:1;
+		uint64_t is_l2_bcast:1;
+		uint64_t is_l3_mcast:1;
+		uint64_t is_l3_bcast:1;
+		uint64_t is_frag:1;
+		uint64_t pcam_flag1:1;
+		uint64_t pcam_flag2:1;
+		uint64_t pcam_flag3:1;
+		uint64_t pcam_flag4:1;
+		uint64_t stat_inc:1;
+		uint64_t vlan_stacked:1;
+		uint64_t vlan_valid:1;
+		uint64_t rsvd_0:8;
+		uint64_t is_la_ether:1;
+		uint64_t lb_hdr_type:5;
+		uint64_t lc_hdr_type:5;
+		uint64_t ld_hdr_type:5;
+		uint64_t le_hdr_type:5;
+		uint64_t lf_hdr_type:5;
+		uint64_t lg_hdr_type:5;
+		uint64_t software:1;
+	};
+#endif
+} cvmx_pki_wqe_word2_t;
+
+#define CVMX_PKI_OPCODE_RE_NONE		0x0
+#define CVMX_PKI_OPCODE_RE_PARTIAL	0x1
+#define CVMX_PKI_OPCODE_RE_JABBER	0x2
+#define CVMX_PKI_OPCODE_RE_FCS		0x7
+#define CVMX_PKI_OPCODE_RE_FCS_RCV	0x8
+#define CVMX_PKI_OPCODE_RE_TERMINATE	0x9
+#define CVMX_PKI_OPCODE_RE_RX_CTL	0xb
+#define CVMX_PKI_OPCODE_RE_SKIP		0xc
+#define CVMX_PKI_OPCODE_RE_DMAPKT	0xf
+#define CVMX_PKI_OPCODE_RE_PKIPAR	0x13
+#define CVMX_PKI_OPCODE_RE_PKIPCAM	0x14
+#define CVMX_PKI_OPCODE_RE_MEMOUT	0x15
+
+
+typedef union {
+	uint64_t u64;
+#ifdef __BIG_ENDIAN_BITFIELD
+	struct {
+		uint64_t ptr_vlan:8;              /**< Contains a byte pointer to the first byte of the
+					               VLAN ID field for the first or second VLAN. */
+		uint64_t ptr_layer_g:8;           /**< Contains a byte pointer to the start of layer G */
+		uint64_t ptr_layer_f:8;           /**< Contains a byte pointer to the start of layer F */
+		uint64_t ptr_layer_e:8;           /**< Contains a byte pointer to the start of layer E */
+		uint64_t ptr_layer_d:8;           /**< Contains a byte pointer to the start of layer D */
+		uint64_t ptr_layer_c:8;           /**< Contains a byte pointer to the start of layer C */
+		uint64_t ptr_layer_b:8;           /**< Contains a byte pointer to the start of layer B */
+		uint64_t ptr_layer_a:8;           /**< Contains a byte pointer to the start of layer A */
+	};
+#else
+	struct {
+		uint64_t ptr_layer_a:8;
+		uint64_t ptr_layer_b:8;
+		uint64_t ptr_layer_c:8;
+		uint64_t ptr_layer_d:8;
+		uint64_t ptr_layer_e:8;
+		uint64_t ptr_layer_f:8;
+		uint64_t ptr_layer_g:8;
+		uint64_t ptr_vlan:8;
+
+	};
+#endif
+} cvmx_pki_wqe_word4_t;
+
+/**
+ * Work queue entry format for 78XX
+ * In 78XX packet data always resides in WQE buffer unless
+ * option DIS_WQ_DAT=1 in PKI_STYLE_BUF, which causes packet data to use
+ * separate buffer.
+ *
+ * must be 8-byte aligned
+ */
+typedef struct {
+	/*****************************************************************
+	 * WORD 0
+	 *  HW WRITE: the following 64 bits are filled by HW when a packet arrives
+	 */
+	cvmx_pki_wqe_word0_t word0;
+
+	/*****************************************************************
+	 * WORD 1
+	 *  HW WRITE: the following 64 bits are filled by HW when a packet arrives
+	 */
+
+	cvmx_pki_wqe_word1_t word1;
+
+	/**
+	 * WORD 2
+	 *   HW WRITE: the following 64-bits are filled in by hardware when a packet arrives
+	 *   This indicates a variety of status and error conditions.
+	 */
+	cvmx_pki_wqe_word2_t word2;
+
+	/**
+	 * Pointer to the first segment of the packet.
+         * WORD 3
+	 */
+	cvmx_buf_ptr_pki_t packet_ptr;
+
+	/**
+         * WORD 4
+         *   HW WRITE: the following 64-bits are filled in by hardware when a packet arrives
+	 *   contains a byte pointer to the start of Layer A/B/C/D/E/F/G relative of start of packet
+	 */
+
+	cvmx_pki_wqe_word4_t word4;
+
+	/**
+         * WORD 5/6/7 may be extended here if WQE_HSZ is set to !0
+	 */
+        uint64_t        wqe_data[11];
+
+} CVMX_CACHE_LINE_ALIGNED cvmx_wqe_78xx_t;
 /**
  * HW decode / err_code in work queue entry
  */
diff --git a/arch/mips/include/asm/octeon/cvmx.h b/arch/mips/include/asm/octeon/cvmx.h
index 41da7f6..f4da884 100644
--- a/arch/mips/include/asm/octeon/cvmx.h
+++ b/arch/mips/include/asm/octeon/cvmx.h
@@ -31,12 +31,16 @@
 #include <linux/kernel.h>
 #include <linux/string.h>
 
-enum cvmx_mips_space {
+#define CVMX_SHARED
+#define cvmx_unlikely unlikely
+#define cvmx_likely likely
+
+typedef enum cvmx_mips_space {
 	CVMX_MIPS_SPACE_XKSEG = 3LL,
 	CVMX_MIPS_SPACE_XKPHYS = 2LL,
 	CVMX_MIPS_SPACE_XSSEG = 1LL,
 	CVMX_MIPS_SPACE_XUSEG = 0LL
-};
+} cvmx_mips_space_t;
 
 /* These macros for use when using 32 bit pointers. */
 #define CVMX_MIPS32_SPACE_KSEG0 1l
@@ -440,7 +444,7 @@ static inline uint64_t cvmx_get_cycle_global(void)
  * 2) Check if ("type".s."field" "op" "value")
  * 3) If #2 isn't true loop to #1 unless too much time has passed.
  */
-#define CVMX_WAIT_FOR_FIELD64(address, type, field, op, value, timeout_usec)\
+#define CVMX_WAIT_FOR_FIELD64_NODE(node, address, type, field, op, value, timeout_usec) \
     (									\
 {									\
 	int result;							\
@@ -449,7 +453,7 @@ static inline uint64_t cvmx_get_cycle_global(void)
 			octeon_get_clock_rate() / 1000000;		\
 		type c;							\
 		while (1) {						\
-			c.u64 = cvmx_read_csr(address);			\
+			c.u64 = cvmx_read_csr_node(node, address);	\
 			if ((c.s.field) op(value)) {			\
 				result = 0;				\
 				break;					\
@@ -476,6 +480,9 @@ static inline void cvmx_reset_octeon(void)
 		cvmx_write_csr(CVMX_CIU_SOFT_RST, ciu_soft_rst.u64);
 }
 
+#define CVMX_WAIT_FOR_FIELD64(address, type, field, op, value, timeout_usec) \
+	CVMX_WAIT_FOR_FIELD64_NODE(0, address, type, field, op, value, timeout_usec)
+
 /* Return the number of cores available in the chip */
 static inline uint32_t cvmx_octeon_num_cores(void)
 {
diff --git a/arch/mips/include/asm/octeon/octeon-feature.h b/arch/mips/include/asm/octeon/octeon-feature.h
index 3ccc236..5329def 100644
--- a/arch/mips/include/asm/octeon/octeon-feature.h
+++ b/arch/mips/include/asm/octeon/octeon-feature.h
@@ -84,6 +84,8 @@ enum octeon_feature {
 	OCTEON_FEATURE_DFM,
 	OCTEON_FEATURE_CIU2,
 	OCTEON_FEATURE_CIU3,
+	OCTEON_FEATURE_FPA3,
+   	/**<  Octeon has FPA first seen on 78XX */
 	OCTEON_MAX_FEATURE
 };
 
@@ -222,7 +224,8 @@ static inline int octeon_has_feature(enum octeon_feature feature)
 			|| OCTEON_IS_MODEL(OCTEON_CN52XX);
 
 	case OCTEON_FEATURE_PKND:
-		return OCTEON_IS_MODEL(OCTEON_CN68XX);
+		return (OCTEON_IS_MODEL(OCTEON_CN68XX)
+			|| OCTEON_IS_MODEL(OCTEON_CN78XX));
 
 	case OCTEON_FEATURE_CN68XX_WQE:
 		return OCTEON_IS_MODEL(OCTEON_CN68XX);
diff --git a/drivers/staging/Makefile b/drivers/staging/Makefile
index f933a29..8840b1d 100644
--- a/drivers/staging/Makefile
+++ b/drivers/staging/Makefile
@@ -27,6 +27,7 @@ obj-$(CONFIG_LINE6_USB)		+= line6/
 obj-$(CONFIG_NETLOGIC_XLR_NET)	+= netlogic/
 obj-$(CONFIG_USB_SERIAL_QUATECH2)	+= serqt_usb2/
 obj-$(CONFIG_OCTEON_ETHERNET)	+= octeon/
+obj-$(CONFIG_OCTEON3_ETHERNET)	+= octeon/
 obj-$(CONFIG_OCTEON_USB)	+= octeon-usb/
 obj-$(CONFIG_VT6655)		+= vt6655/
 obj-$(CONFIG_VT6656)		+= vt6656/
diff --git a/drivers/staging/octeon/Kconfig b/drivers/staging/octeon/Kconfig
index 6e1d5f8..44d5688 100644
--- a/drivers/staging/octeon/Kconfig
+++ b/drivers/staging/octeon/Kconfig
@@ -11,3 +11,19 @@ config OCTEON_ETHERNET
 	  To compile this driver as a module, choose M here.  The module
 	  will be called octeon-ethernet.
 
+config OCTEON_BGX_NEXUS
+	tristate
+	depends on CAVIUM_OCTEON_SOC
+
+config OCTEON_BGX_PORT
+	tristate
+	depends on CAVIUM_OCTEON_SOC
+	select OCTEON_BGX_NEXUS
+
+config OCTEON3_ETHERNET
+	tristate "Cavium Inc. OCTEON III PKI/PKO Ethernet support (not cn70xx)"
+	depends on CAVIUM_OCTEON_SOC
+	select OCTEON_BGX_PORT
+	help
+	  Support for 'BGX' Ethernet via PKI/PKO units.  No support
+	  for cn70xx chips (use OCTEON_ETHERNET for cn70xx)
diff --git a/drivers/staging/octeon/Makefile b/drivers/staging/octeon/Makefile
index 9012dee..ee263b3 100644
--- a/drivers/staging/octeon/Makefile
+++ b/drivers/staging/octeon/Makefile
@@ -10,6 +10,9 @@
 #
 
 obj-${CONFIG_OCTEON_ETHERNET} :=  octeon-ethernet.o
+obj-$(CONFIG_OCTEON_BGX_NEXUS) += octeon-bgx-nexus.o
+obj-$(CONFIG_OCTEON_BGX_PORT)  += octeon-bgx-port.o
+obj-$(CONFIG_OCTEON3_ETHERNET) += octeon3-ethernet.o
 
 octeon-ethernet-y := ethernet.o
 octeon-ethernet-y += ethernet-mdio.o
diff --git a/drivers/staging/octeon/octeon-bgx-nexus.c b/drivers/staging/octeon/octeon-bgx-nexus.c
new file mode 100644
index 0000000..cde2157
--- /dev/null
+++ b/drivers/staging/octeon/octeon-bgx-nexus.c
@@ -0,0 +1,160 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2014 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+#include <linux/platform_device.h>
+#include <linux/of_platform.h>
+#include <linux/of_address.h>
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/list.h>
+
+#include <asm/octeon/cvmx.h>
+#include <asm/octeon/octeon-model.h>
+#include <asm/octeon/cvmx-helper.h>
+#include <asm/octeon/cvmx-helper-util.h>
+#include <asm/octeon/cvmx-helper-cfg.h>
+#include <asm/octeon/cvmx-bgxx-defs.h>
+
+#include "octeon-bgx.h"
+
+static int bgx_probe(struct platform_device *pdev)
+{
+	struct bgx_platform_data platform_data;
+	const __be32 *reg;
+	u32 port;
+	u64 addr;
+	struct device_node *child;
+	struct platform_device *new_dev;
+	struct platform_device *pki_dev;
+	static int pki_id;
+	int numa_node, interface;
+	int i;
+	int r = 0;
+	char id[64];
+
+	reg = of_get_property(pdev->dev.of_node, "reg", NULL);
+	addr = of_translate_address(pdev->dev.of_node, reg);
+	interface = (addr >> 24) & 0xf;
+	numa_node = (addr >> 36) & 0x7;
+
+	__cvmx_helper_packet_hardware_enable(cvmx_helper_node_interface_to_xiface(numa_node, interface));
+	/* Assign 8 CAM entries per LMAC */
+	for (i = 0; i < 32; i++) {
+		union cvmx_bgxx_cmr_rx_adrx_cam adr_cam;
+		adr_cam.u64 = 0;
+		adr_cam.s.id = i >> 3;
+		cvmx_write_csr_node(numa_node, CVMX_BGXX_CMR_RX_ADRX_CAM(i, interface), adr_cam.u64);
+	}
+
+	for_each_available_child_of_node(pdev->dev.of_node, child) {
+		union cvmx_bgxx_cmrx_config cmr_config;
+		if (!of_device_is_compatible(child, "cavium,octeon-7890-bgx-port"))
+			continue;
+		r = of_property_read_u32(child, "reg", &port);
+		if (r)
+			return -ENODEV;
+
+		/* Connect to PKI/PKO */
+		cmr_config.u64 = cvmx_read_csr_node(numa_node, CVMX_BGXX_CMRX_CONFIG(port, interface));
+		cmr_config.s.mix_en = 0;
+		cvmx_write_csr_node(numa_node, CVMX_BGXX_CMRX_CONFIG(port, interface), cmr_config.u64);
+
+		snprintf(id, sizeof(id), "%llx.%u.ethernet-mac", (unsigned long long)addr, port);
+		new_dev = of_platform_device_create(child, id, &pdev->dev);
+		if (!new_dev) {
+			dev_err(&pdev->dev, "Error creating %s\n", id);
+			continue;
+		}
+		platform_data.numa_node = numa_node;
+		platform_data.interface = interface;
+		platform_data.port = port;
+
+		pki_dev = platform_device_register_data(&new_dev->dev, "ethernet-mac-pki", pki_id++,
+							&platform_data, sizeof(platform_data));
+		dev_info(&pdev->dev, "Created PKI %u: %p\n", pki_dev->id, pki_dev);
+#ifdef CONFIG_NUMA
+		new_dev->dev.numa_node = pdev->dev.numa_node;
+		pki_dev->dev.numa_node = pdev->dev.numa_node;
+#endif
+	}
+
+	dev_info(&pdev->dev, "Probed\n");
+	return 0;
+}
+
+static int bgx_remove(struct platform_device *pdev)
+{
+	return 0;
+}
+
+static void bgx_shutdown(struct platform_device *pdev)
+{
+	return;
+}
+
+static struct of_device_id bgx_match[] = {
+	{
+		.compatible = "cavium,octeon-7890-bgx",
+	},
+	{},
+};
+MODULE_DEVICE_TABLE(of, bgx_match);
+
+static struct platform_driver bgx_driver = {
+	.probe		= bgx_probe,
+	.remove		= bgx_remove,
+	.shutdown       = bgx_shutdown,
+	.driver		= {
+		.owner	= THIS_MODULE,
+		.name	= KBUILD_MODNAME,
+		.of_match_table = bgx_match,
+	},
+};
+
+/* Allow bgx_port driver to force this driver to load */
+void bgx_nexus_load(void)
+{
+}
+EXPORT_SYMBOL(bgx_nexus_load);
+
+static int __init bgx_driver_init(void)
+{
+	int r;
+	__cvmx_helper_init_port_config_data();
+	r =  platform_driver_register(&bgx_driver);
+	return r;
+}
+module_init(bgx_driver_init);
+
+static void __exit bgx_driver_exit(void)
+{
+	platform_driver_unregister(&bgx_driver);
+}
+module_exit(bgx_driver_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Cavium Networks <support@caviumnetworks.com>");
+MODULE_DESCRIPTION("Cavium Networks BGX MAC Nexus driver.");
diff --git a/drivers/staging/octeon/octeon-bgx-port.c b/drivers/staging/octeon/octeon-bgx-port.c
new file mode 100644
index 0000000..912993e
--- /dev/null
+++ b/drivers/staging/octeon/octeon-bgx-port.c
@@ -0,0 +1,415 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2014 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+#include <linux/platform_device.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/of_platform.h>
+#include <linux/of_address.h>
+#include <linux/of_mdio.h>
+#include <linux/of_net.h>
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/list.h>
+
+#include <asm/octeon/cvmx.h>
+#include <asm/octeon/octeon-model.h>
+#include <asm/octeon/cvmx-helper.h>
+#include <asm/octeon/cvmx-helper-util.h>
+#include <asm/octeon/cvmx-helper-cfg.h>
+#include <asm/octeon/cvmx-bgxx-defs.h>
+
+#include "octeon-bgx.h"
+
+struct bgx_port_priv {
+	int numa_node;
+	int bgx_interface;
+	int index; /* Port index on BGX block*/
+	int ipd_port;
+	int xiface;
+	const u8 *mac_addr;
+	struct phy_device *phydev;
+	struct device_node *phy_np;
+	spinlock_t lock;
+	unsigned int last_duplex;
+	unsigned int last_link;
+	unsigned int last_speed;
+};
+
+static struct bgx_port_priv *bgx_port_netdev2priv(struct net_device *netdev)
+{
+	struct bgx_port_netdev_priv *nd_priv = netdev_priv(netdev);
+	return nd_priv->bgx_priv;
+}
+
+void bgx_port_set_netdev(struct device *dev, struct net_device *netdev)
+{
+	struct bgx_port_netdev_priv *nd_priv = netdev_priv(netdev);
+	struct bgx_port_priv *priv = dev_get_drvdata(dev);
+	nd_priv->bgx_priv = priv;
+}
+EXPORT_SYMBOL(bgx_port_set_netdev);
+
+const u8 *bgx_port_get_mac(struct net_device *netdev)
+{
+	struct bgx_port_priv *priv = bgx_port_netdev2priv(netdev);
+	return priv->mac_addr;
+}
+EXPORT_SYMBOL(bgx_port_get_mac);
+
+static void bgx_port_write_cam(int numa_node, int interface, int index, int cam, const u8 *mac)
+{
+	int i;
+	union cvmx_bgxx_cmr_rx_adrx_cam adr_cam;
+	u64 m = 0;
+	if (mac)
+		for (i = 0; i < 6; i++)
+			m |= (((u64)mac[i]) << ((5 - i) * 8));
+	adr_cam.u64 = m;
+	adr_cam.s.en = mac ? 1 : 0;
+	adr_cam.s.id = index;
+	cvmx_write_csr_node(numa_node, CVMX_BGXX_CMR_RX_ADRX_CAM(index * 8 + cam, interface), adr_cam.u64);
+}
+
+/* Set MAC address for the net_device that is attached. */
+void bgx_port_set_rx_filtering(struct net_device *netdev)
+{
+	union cvmx_bgxx_cmrx_rx_adr_ctl adr_ctl;
+	struct bgx_port_priv *priv = bgx_port_netdev2priv(netdev);
+	int available_cam_entries, current_cam_entry;
+	struct netdev_hw_addr *ha;
+
+	available_cam_entries = 8;
+	adr_ctl.u64 = 0;
+	adr_ctl.s.bcst_accept = 1; /* Accept all Broadcast*/
+
+	if ((netdev->flags & IFF_PROMISC) || netdev->uc.count > 7) {
+		adr_ctl.s.cam_accept = 0; /* Reject CAM match */
+		available_cam_entries = 0;
+	} else {
+		/* One CAM entry for the primary address, leaves seven
+		 * for the secondary addresses.
+		 */
+		adr_ctl.s.cam_accept = 1; /* Accept CAM match */
+		available_cam_entries = 7 - netdev->uc.count;
+	}
+
+	if (netdev->flags & IFF_PROMISC) {
+		adr_ctl.s.mcst_mode = 1; /* Accept all Multicast */
+	} else {
+		if (netdev->flags & IFF_MULTICAST) {
+			if ((netdev->flags & IFF_ALLMULTI) || netdev_mc_count(netdev) > available_cam_entries)
+				adr_ctl.s.mcst_mode = 1; /* Accept all Multicast */
+			else
+				adr_ctl.s.mcst_mode = 2; /* Accept all Multicast via CAM */
+		}
+	}
+	current_cam_entry = 0;
+	if (adr_ctl.s.cam_accept) {
+		bgx_port_write_cam(priv->numa_node, priv->bgx_interface, priv->index,
+				   current_cam_entry, netdev->dev_addr);
+		current_cam_entry++;
+		netdev_for_each_uc_addr(ha, netdev) {
+			bgx_port_write_cam(priv->numa_node, priv->bgx_interface, priv->index,
+					   current_cam_entry, ha->addr);
+			current_cam_entry++;
+		}
+	}
+	if (adr_ctl.s.mcst_mode == 2) {/* Accept all Multicast via CAM */
+		netdev_for_each_mc_addr(ha, netdev) {
+			bgx_port_write_cam(priv->numa_node, priv->bgx_interface, priv->index,
+					   current_cam_entry, ha->addr);
+			current_cam_entry++;
+		}
+	}
+	while (current_cam_entry < 8) {
+		bgx_port_write_cam(priv->numa_node, priv->bgx_interface, priv->index,
+				   current_cam_entry, NULL);
+		current_cam_entry++;
+	}
+	cvmx_write_csr_node(priv->numa_node,
+			    CVMX_BGXX_CMRX_RX_ADR_CTL(priv->index, priv->bgx_interface),
+			    adr_ctl.u64);
+}
+EXPORT_SYMBOL(bgx_port_set_rx_filtering);
+
+static void bgx_port_adjust_link(struct net_device *netdev)
+{
+	struct bgx_port_priv *p = bgx_port_netdev2priv(netdev);
+	int link_changed = 0;
+	unsigned int link, speed, duplex;
+	unsigned long flags;
+
+	spin_lock_irqsave(&p->lock, flags);
+
+	if (!p->phydev->link && p->last_link)
+		link_changed = -1;
+
+	if (p->phydev->link
+	    && (p->last_duplex != p->phydev->duplex
+		|| p->last_link != p->phydev->link
+		|| p->last_speed != p->phydev->speed)) {
+		link_changed = 1;
+	}
+
+	link = p->last_link = p->phydev->link;
+	speed = p->last_speed = p->phydev->speed;
+	duplex = p->last_duplex = p->phydev->duplex;
+
+	spin_unlock_irqrestore(&p->lock, flags);
+
+	if (link_changed != 0) {
+		cvmx_helper_link_info_t link_info;
+		if (link_changed > 0) {
+			pr_info("%s: Link is up - %d/%s\n", netdev->name,
+				p->phydev->speed,
+				DUPLEX_FULL == p->phydev->duplex ?
+				"Full" : "Half");
+		} else {
+			pr_info("%s: Link is down\n", netdev->name);
+		}
+		link_info.u64 = 0;
+		link_info.s.link_up = link ? 1 : 0;
+		link_info.s.full_duplex = duplex ? 1 : 0;
+		link_info.s.speed = speed;
+		cvmx_helper_link_set(p->ipd_port, link_info);
+		if (link)
+			netif_carrier_on(netdev);
+		else
+			netif_carrier_off(netdev);
+	}
+}
+
+int bgx_port_enable(struct net_device *netdev)
+{
+	union cvmx_bgxx_cmrx_config cfg;
+	struct bgx_port_priv *priv = bgx_port_netdev2priv(netdev);
+
+	cvmx_helper_set_1000x_mode(priv->xiface, priv->index, false);
+	cvmx_helper_set_mac_phy_mode(priv->xiface, priv->index, false);
+
+	cfg.u64 = cvmx_read_csr_node(priv->numa_node, CVMX_BGXX_CMRX_CONFIG(priv->index, priv->bgx_interface));
+	if (cfg.s.lmac_type == 0) {
+		/* 1G */
+		union cvmx_bgxx_gmp_gmi_txx_append tx_append;
+		union cvmx_bgxx_gmp_gmi_txx_min_pkt min_pkt;
+
+		tx_append.u64 = cvmx_read_csr_node(priv->numa_node,
+						   CVMX_BGXX_GMP_GMI_TXX_APPEND(priv->index, priv->bgx_interface));
+		tx_append.s.fcs = 1;
+		tx_append.s.pad = 1;
+
+		cvmx_write_csr_node(priv->numa_node,
+				    CVMX_BGXX_GMP_GMI_TXX_APPEND(priv->index, priv->bgx_interface),
+				    tx_append.u64);
+
+		min_pkt.u64 = 0;
+		/* packets are padded (without FCS) to MIN_SIZE + 1 in SGMII */
+		min_pkt.s.min_size = 60 - 1;
+		cvmx_write_csr_node(priv->numa_node,
+				    CVMX_BGXX_GMP_GMI_TXX_MIN_PKT(priv->index, priv->bgx_interface),
+				    min_pkt.u64);
+	} else {
+		/* 10G or higher */
+		union cvmx_bgxx_smux_tx_append tx_append;
+		union cvmx_bgxx_smux_tx_min_pkt min_pkt;
+
+		tx_append.u64 = cvmx_read_csr_node(priv->numa_node,
+						   CVMX_BGXX_SMUX_TX_APPEND(priv->index, priv->bgx_interface));
+		tx_append.s.fcs_d = 1;
+		tx_append.s.pad = 1;
+
+		cvmx_write_csr_node(priv->numa_node,
+				    CVMX_BGXX_SMUX_TX_APPEND(priv->index, priv->bgx_interface),
+				    tx_append.u64);
+
+		min_pkt.u64 = 0;
+		/* packets are padded(with FCS) to MIN_SIZE  in non-SGMII */
+		min_pkt.s.min_size = 60 + 4;
+		cvmx_write_csr_node(priv->numa_node,
+				    CVMX_BGXX_SMUX_TX_MIN_PKT(priv->index, priv->bgx_interface),
+				    min_pkt.u64);
+
+	}
+
+	if (priv->phy_np == NULL) {
+		cvmx_helper_link_autoconf(priv->ipd_port);
+		netif_carrier_on(netdev);
+		return 0;
+	}
+	priv->phydev = of_phy_connect(netdev, priv->phy_np,
+				      bgx_port_adjust_link, 0,
+				      PHY_INTERFACE_MODE_SGMII);
+	if (!priv->phydev)
+		return -ENODEV;
+
+	netif_carrier_off(netdev);
+	phy_start_aneg(priv->phydev);
+
+	return 0;
+}
+EXPORT_SYMBOL(bgx_port_enable);
+
+int bgx_port_disable(struct net_device *netdev)
+{
+	struct bgx_port_priv *priv = bgx_port_netdev2priv(netdev);
+	cvmx_helper_link_info_t link_info;
+
+	if (priv->phydev)
+		phy_disconnect(priv->phydev);
+	priv->phydev = NULL;
+
+	netif_carrier_off(netdev);
+	link_info.u64 = 0;
+	cvmx_helper_link_set(priv->ipd_port, link_info);
+
+	return 0;
+}
+EXPORT_SYMBOL(bgx_port_disable);
+
+int bgx_port_change_mtu(struct net_device *netdev, int new_mtu)
+{
+	union cvmx_bgxx_cmrx_config cfg;
+	struct bgx_port_priv *priv = bgx_port_netdev2priv(netdev);
+	int max_frame;
+
+	if (new_mtu < 60 || new_mtu > 65392)
+		return -EINVAL;
+
+	netdev->mtu = new_mtu;
+
+	max_frame = round_up(new_mtu + ETH_HLEN + ETH_FCS_LEN, 8);
+
+	cfg.u64 = cvmx_read_csr_node(priv->numa_node, CVMX_BGXX_CMRX_CONFIG(priv->index, priv->bgx_interface));
+	if (cfg.s.lmac_type == 0)
+		cvmx_write_csr_node(priv->numa_node,		/* 1G */
+				    CVMX_BGXX_GMP_GMI_RXX_JABBER(priv->index, priv->bgx_interface),
+				    max_frame);
+	else
+		cvmx_write_csr_node(priv->numa_node,		/* 10G or higher */
+				    CVMX_BGXX_SMUX_RX_JABBER(priv->index, priv->bgx_interface),
+				    max_frame);
+
+
+	return 0;
+}
+EXPORT_SYMBOL(bgx_port_change_mtu);
+
+static int bgx_port_probe(struct platform_device *pdev)
+{
+	u64 addr;
+	const u8 *mac;
+	const __be32 *reg;
+	u32 index;
+	int r;
+	struct bgx_port_priv *priv;
+	int numa_node;
+
+	reg = of_get_property(pdev->dev.parent->of_node, "reg", NULL);
+	addr = of_translate_address(pdev->dev.parent->of_node, reg);
+	mac = of_get_mac_address(pdev->dev.of_node);
+
+	numa_node = (addr >> 36) & 0x7;
+
+	r = of_property_read_u32(pdev->dev.of_node, "reg", &index);
+	if (r)
+		return -ENODEV;
+	priv = kzalloc_node(sizeof(*priv), GFP_KERNEL, numa_node);
+	if (!priv)
+		return -ENOMEM;
+
+	spin_lock_init(&priv->lock);
+	priv->numa_node = numa_node;
+	priv->bgx_interface = (addr >> 24) & 0xf;
+	priv->index = index;
+	priv->xiface = cvmx_helper_node_interface_to_xiface(numa_node, priv->bgx_interface);
+	priv->ipd_port = cvmx_helper_get_ipd_port(priv->xiface, index);
+	if (mac)
+		priv->mac_addr = mac;
+
+	priv->phy_np = of_parse_phandle(pdev->dev.of_node, "phy-handle", 0);
+
+	r = dev_set_drvdata(&pdev->dev, priv);
+	if (r)
+		goto err;
+
+	dev_info(&pdev->dev, "Probed\n");
+	return 0;
+err:
+	kfree(priv);
+	return r;
+}
+
+static int bgx_port_remove(struct platform_device *pdev)
+{
+	struct bgx_port_priv *priv = dev_get_drvdata(&pdev->dev);
+	kfree(priv);
+	return 0;
+}
+
+static void bgx_port_shutdown(struct platform_device *pdev)
+{
+	return;
+}
+
+static struct of_device_id bgx_port_match[] = {
+	{
+		.compatible = "cavium,octeon-7890-bgx-port",
+	},
+	{},
+};
+MODULE_DEVICE_TABLE(of, bgx_port_match);
+
+static struct platform_driver bgx_port_driver = {
+	.probe		= bgx_port_probe,
+	.remove		= bgx_port_remove,
+	.shutdown       = bgx_port_shutdown,
+	.driver		= {
+		.owner	= THIS_MODULE,
+		.name	= KBUILD_MODNAME,
+		.of_match_table = bgx_port_match,
+	},
+};
+
+static int __init bgx_port_driver_init(void)
+{
+	int r;
+
+	bgx_nexus_load();
+	r =  platform_driver_register(&bgx_port_driver);
+	return r;
+}
+module_init(bgx_port_driver_init);
+
+static void __exit bgx_port_driver_exit(void)
+{
+	platform_driver_unregister(&bgx_port_driver);
+}
+module_exit(bgx_port_driver_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Cavium Networks <support@caviumnetworks.com>");
+MODULE_DESCRIPTION("Cavium Networks BGX Ethernet MAC driver.");
diff --git a/drivers/staging/octeon/octeon-bgx.h b/drivers/staging/octeon/octeon-bgx.h
new file mode 100644
index 0000000..7fa10b2
--- /dev/null
+++ b/drivers/staging/octeon/octeon-bgx.h
@@ -0,0 +1,53 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2014 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+#ifndef _OCTEON_BGX_H_
+#define _OCTEON_BGX_H_
+
+struct device;
+struct net_device;
+struct bgx_port_priv;
+
+struct bgx_port_netdev_priv {
+	struct bgx_port_priv *bgx_priv;
+};
+
+void bgx_nexus_load(void);
+
+void bgx_port_set_netdev(struct device *dev, struct net_device *netdev);
+int bgx_port_enable(struct net_device *netdev);
+int bgx_port_disable(struct net_device *netdev);
+const u8 *bgx_port_get_mac(struct net_device *netdev);
+void bgx_port_set_rx_filtering(struct net_device *netdev);
+int bgx_port_change_mtu(struct net_device *netdev, int new_mtu);
+
+struct bgx_platform_data {
+	int numa_node;
+	int interface;
+	int port;
+};
+
+#endif /* _OCTEON_BGX_H_ */
diff --git a/drivers/staging/octeon/octeon3-ethernet.c b/drivers/staging/octeon/octeon3-ethernet.c
new file mode 100644
index 0000000..244f77b
--- /dev/null
+++ b/drivers/staging/octeon/octeon3-ethernet.c
@@ -0,0 +1,1147 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2014 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+#include <linux/module.h>
+#include <linux/atomic.h>
+#include <linux/kthread.h>
+#include <linux/interrupt.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/platform_device.h>
+#include <linux/ip.h>
+#include <linux/ipv6.h>
+
+#include <asm/octeon/octeon.h>
+#include <asm/octeon/cvmx-helper-cfg.h>
+#include <asm/octeon/cvmx-helper-pko3.h>
+#include <asm/octeon/cvmx-helper-pki.h>
+#include <asm/octeon/cvmx-pow.h>
+#include <asm/octeon/cvmx-pko3.h>
+#include <asm/octeon/cvmx-fpa3.h>
+#include <asm/octeon/cvmx-fpa1.h>
+
+#include <asm/octeon/cvmx-fpa-defs.h>
+#include <asm/octeon/cvmx-sso-defs.h>
+
+#include "octeon-bgx.h"
+
+#define MAX_TX_QUEUE_DEPTH 512
+#define SSO_INTSN_EXE 0x61
+#define OCTEON3_ETH_MAX_NUMA_NODES 2
+
+struct octeon3_ethernet {
+	struct bgx_port_netdev_priv bgx_priv;
+	struct net_device *netdev;
+	struct napi_struct napi;
+	int pki_laura;
+	int pki_pkind;
+	int pko_queue;
+	int numa_node;
+	int xiface;
+	int port_index;
+	int rx_buf_count;
+	int rx_grp;
+	int rx_irq;
+	cpumask_t rx_affinity_hint;
+	int tx_complete_grp;
+	atomic64_t tx_backlog;
+	spinlock_t stat_lock;
+	u64 last_packets;
+	u64 last_octets;
+	u64 last_dropped;
+	atomic64_t rx_packets;
+	atomic64_t rx_octets;
+	atomic64_t rx_dropped;
+	atomic64_t rx_errors;
+	atomic64_t rx_length_errors;
+	atomic64_t rx_crc_errors;
+	atomic64_t tx_packets;
+	atomic64_t tx_octets;
+	atomic64_t tx_dropped;
+};
+
+static DEFINE_MUTEX(octeon3_eth_init_mutex);
+
+struct octeon3_ethernet_node {
+	bool init_done;
+	int next_cpu_irq_affinity;
+	int numa_node;
+	int pki_packet_pool;
+	int sso_pool;
+	int pko_pool;
+	int sso_aura;
+	int pko_aura;
+	int tx_complete_grp;
+	int tx_irq;
+	cpumask_t tx_affinity_hint;
+	struct task_struct *tx_complete_task;
+	struct kthread_worker tx_complete_worker;
+	struct kthread_work tx_complete_work;
+};
+
+static int num_packet_buffers = 512;
+module_param(num_packet_buffers, int, S_IRUGO);
+MODULE_PARM_DESC(num_packet_buffers, "Number of packet buffers to allocate per port.");
+
+static int packet_buffer_size = 2048;
+module_param(packet_buffer_size, int, S_IRUGO);
+MODULE_PARM_DESC(packet_buffer_size, "Size of each RX packet buffer.");
+
+
+static struct octeon3_ethernet_node octeon3_eth_node[OCTEON3_ETH_MAX_NUMA_NODES];
+static struct kmem_cache *octeon3_eth_sso_pko_cache;
+
+static void octeon3_eth_gen_affinity(int node, cpumask_t *mask)
+{
+	int cpu;
+
+	do {
+		cpu = cpumask_next(octeon3_eth_node[node].next_cpu_irq_affinity,
+				   cpu_online_mask);
+		octeon3_eth_node[node].next_cpu_irq_affinity++;
+		if (cpu >= nr_cpu_ids) {
+			octeon3_eth_node[node].next_cpu_irq_affinity = -1;
+			continue;
+		}
+	} while (false);
+	cpumask_clear(mask);
+	cpumask_set_cpu(cpu, mask);
+}
+
+static int octeon3_eth_fpa_pool_init(unsigned int node, unsigned int pool, int num_ptrs)
+{
+	void *pool_stack;
+	u64 pool_stack_start, pool_stack_end;
+	union cvmx_fpa_poolx_end_addr limit_addr;
+	union cvmx_fpa_poolx_cfg cfg;
+	int stack_size = (DIV_ROUND_UP(num_ptrs, 29) + 1) * 128;
+
+	cvmx_write_csr_node(node, CVMX_FPA_POOLX_CFG(pool), 0);
+	cvmx_write_csr_node(node, CVMX_FPA_POOLX_START_ADDR(pool), 128);
+	limit_addr.u64 = 0;
+	limit_addr.cn78xx.addr = ~0ll;
+	cvmx_write_csr_node(node, CVMX_FPA_POOLX_END_ADDR(pool), limit_addr.u64);
+
+	pool_stack = kmalloc_node(stack_size, GFP_KERNEL, node);
+	if (!pool_stack)
+		return -ENOMEM;
+
+	pool_stack_start = virt_to_phys(pool_stack);
+	pool_stack_end = round_down(pool_stack_start + stack_size, 128);
+	pool_stack_start = round_up(pool_stack_start, 128);
+
+	cvmx_write_csr_node(node, CVMX_FPA_POOLX_STACK_BASE(pool), pool_stack_start);
+	cvmx_write_csr_node(node, CVMX_FPA_POOLX_STACK_ADDR(pool), pool_stack_start);
+	cvmx_write_csr_node(node, CVMX_FPA_POOLX_STACK_END(pool), pool_stack_end);
+
+	cfg.u64 = 0;
+	cfg.s.l_type = 2; /* Load with DWB */
+	cfg.s.ena = 1;
+	cvmx_write_csr_node(node, CVMX_FPA_POOLX_CFG(pool), cfg.u64);
+	return 0;
+}
+
+static int octeon3_eth_fpa_aura_init(unsigned int node, unsigned int pool, int aura, int limit)
+{
+	cvmx_write_csr_node(node, CVMX_FPA_AURAX_CFG(aura), 0);
+	cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT_LIMIT(aura), limit * 2);
+	cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT(aura), limit * 2);
+	cvmx_write_csr_node(node, CVMX_FPA_AURAX_POOL(aura), pool);
+	return 0;
+}
+
+static int octeon3_eth_sso_init(unsigned int node, int aura)
+{
+	union cvmx_sso_aw_cfg cfg;
+	union cvmx_sso_xaq_aura xaq_aura;
+	union cvmx_sso_err0 err0;
+	union cvmx_sso_grpx_pri grp_pri;
+	int i;
+	int rv = 0;
+
+	grp_pri.u64 = 0;
+	grp_pri.s.weight = 0x3f;
+
+	cfg.u64 = 0;
+	cfg.s.stt = 1;
+	cfg.s.ldt = 1;
+	cfg.s.ldwb = 1;
+	cvmx_write_csr_node(node, CVMX_SSO_AW_CFG, cfg.u64);
+
+	xaq_aura.u64 = 0;
+	xaq_aura.s.laura = aura;
+	xaq_aura.s.node = node;
+	cvmx_write_csr_node(node, CVMX_SSO_XAQ_AURA, xaq_aura.u64);
+
+	for (i = 0; i < 256; i++) {
+		u64 phys;
+		void *mem = cvmx_fpa3_alloc_aura(node, aura);
+		if (!mem) {
+			rv = -ENOMEM;
+			goto err;
+		}
+		phys = virt_to_phys(mem);
+		cvmx_write_csr_node(node, CVMX_SSO_XAQX_HEAD_PTR(i), phys);
+		cvmx_write_csr_node(node, CVMX_SSO_XAQX_TAIL_PTR(i), phys);
+		/* SSO-18678 */
+		cvmx_write_csr_node(node, CVMX_SSO_GRPX_PRI(i), grp_pri.u64);
+	}
+	err0.u64 = 0;
+	err0.s.fpe = 1;
+	cvmx_write_csr_node(node, CVMX_SSO_ERR0, err0.u64);
+
+	cfg.s.rwen = 1;
+	cvmx_write_csr_node(node, CVMX_SSO_AW_CFG, cfg.u64);
+
+err:
+	return rv;
+}
+
+static void octeon3_eth_sso_irq_set_armed(int node, int grp, bool v)
+{
+	union cvmx_sso_grpx_int_thr grp_int_thr;
+	union cvmx_sso_grpx_int grp_int;
+
+	/* Disarm/Rearm the irq. */
+	grp_int_thr.u64 = 0;
+	grp_int_thr.s.iaq_thr = v ? 1 : 0;
+	cvmx_write_csr_node(node, CVMX_SSO_GRPX_INT_THR(grp), grp_int_thr.u64);
+
+	grp_int.u64 = 0;
+	grp_int.s.exe_int = 1;
+	cvmx_write_csr_node(node, CVMX_SSO_GRPX_INT(grp), grp_int.u64);
+}
+
+struct wr_ret {
+	void *work;
+	u16 grp;
+};
+
+static inline struct wr_ret octeon3_eth_work_request_grp_sync(unsigned int lgrp, cvmx_pow_wait_t wait)
+{
+	cvmx_pow_load_addr_t ptr;
+	cvmx_pow_tag_load_resp_t result;
+	struct wr_ret r;
+	unsigned int node = cvmx_get_node_num() & 3;
+
+	ptr.u64 = 0;
+
+	ptr.swork_78xx.mem_region = CVMX_IO_SEG;
+	ptr.swork_78xx.is_io = 1;
+	ptr.swork_78xx.did = CVMX_OCT_DID_TAG_SWTAG;
+	ptr.swork_78xx.node = node;
+	ptr.swork_78xx.rtngrp = 1;
+	ptr.swork_78xx.grouped = 1;
+	ptr.swork_78xx.index = (lgrp & 0xff) | node << 8;
+	ptr.swork_78xx.wait = wait;
+
+	result.u64 = cvmx_read_csr(ptr.u64);
+	r.grp = result.s_work.grp;
+	if (result.s_work.no_work)
+		r.work = NULL;
+	else
+		r.work = cvmx_phys_to_ptr(result.s_work.addr);
+	return r;
+}
+
+static void octeon3_eth_tx_complete_worker(struct kthread_work *txcw)
+{
+	struct octeon3_ethernet_node *oen = container_of(txcw, struct octeon3_ethernet_node, tx_complete_work);
+
+	for (;;) {
+		void *work;
+		struct sk_buff *skb;
+		struct wr_ret r;
+
+		r = octeon3_eth_work_request_grp_sync(oen->tx_complete_grp, CVMX_POW_NO_WAIT);
+		work = r.work;
+		if (work == NULL)
+			break;
+		skb = container_of(work, struct sk_buff, cb);
+		dev_kfree_skb(skb);
+	}
+	octeon3_eth_sso_irq_set_armed(oen->numa_node, oen->tx_complete_grp, true);
+}
+
+static irqreturn_t octeon3_eth_tx_handler(int irq, void *info)
+{
+	struct octeon3_ethernet_node *oen = info;
+	/* Disarm the irq. */
+	octeon3_eth_sso_irq_set_armed(oen->numa_node, oen->tx_complete_grp, false);
+
+	queue_kthread_work(&oen->tx_complete_worker, &oen->tx_complete_work);
+
+	return IRQ_HANDLED;
+}
+
+static int octeon3_eth_global_init(unsigned int node)
+{
+	int i;
+	int rv = 0;
+	struct irq_domain *d;
+	unsigned int sso_intsn;
+	struct octeon3_ethernet_node *nd;
+	mutex_lock(&octeon3_eth_init_mutex);
+
+	nd = octeon3_eth_node + node;
+
+	if (nd->init_done)
+		goto done;
+
+	nd->numa_node = node;
+	for (i = 0; i < 1024; i++) {
+		cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT(i), 0x100000000ull);
+		cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT_LIMIT(i), 0xfffffffffull);
+		cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT_THRESHOLD(i), 0xffffffffeull);
+	}
+	nd->sso_pool = cvmx_fpa_alloc_pool(-1);
+	if (nd->sso_pool < 0) {
+		rv = -ENODEV;
+		goto done;
+	}
+	nd->pko_pool = cvmx_fpa_alloc_pool(-1);
+	if (nd->pko_pool < 0) {
+		rv = -ENODEV;
+		goto done;
+	}
+	nd->pki_packet_pool = cvmx_fpa_alloc_pool(-1);
+	if (nd->pki_packet_pool < 0) {
+		rv = -ENODEV;
+		goto done;
+	}
+	nd->sso_aura = cvmx_fpa3_allocate_aura(node);
+	nd->pko_aura = cvmx_fpa3_allocate_aura(node);
+
+	pr_err("octeon3_eth_global_init  SSO:%d:%d, PKO:%d:%d\n",
+	       nd->sso_pool, nd->sso_aura,  nd->pko_pool, nd->pko_aura);
+
+	octeon3_eth_fpa_pool_init(node, nd->sso_pool, 40960);
+	octeon3_eth_fpa_pool_init(node, nd->pko_pool, 40960);
+	octeon3_eth_fpa_pool_init(node, nd->pki_packet_pool, 64 * num_packet_buffers);
+	octeon3_eth_fpa_aura_init(node, nd->sso_pool, nd->sso_aura, 20480);
+	octeon3_eth_fpa_aura_init(node, nd->pko_pool, nd->pko_aura, 20480);
+
+	if (!octeon3_eth_sso_pko_cache) {
+		octeon3_eth_sso_pko_cache = kmem_cache_create("sso_pko", 4096, 128, 0, NULL);
+		if (!octeon3_eth_sso_pko_cache) {
+			rv = -ENOMEM;
+			goto done;
+		}
+	}
+	for (i = 0; i < 1024; i++) {
+		void *mem;
+		mem = kmem_cache_alloc_node(octeon3_eth_sso_pko_cache, GFP_KERNEL, node);
+		if (!mem) {
+			rv = -ENOMEM;
+			goto done;
+		}
+		cvmx_fpa3_free_aura(mem, node, nd->sso_aura, 0);
+		mem = kmem_cache_alloc_node(octeon3_eth_sso_pko_cache, GFP_KERNEL, node);
+		if (!mem) {
+			rv = -ENOMEM;
+			goto done;
+		}
+		cvmx_fpa3_free_aura(mem, node, nd->pko_aura, 0);
+	}
+
+	rv = octeon3_eth_sso_init(node, nd->sso_aura);
+	if (rv)
+		goto done;
+
+	nd->tx_complete_grp = cvmx_sso_allocate_group(node);
+	d = octeon_irq_get_block_domain(nd->numa_node, SSO_INTSN_EXE);
+	sso_intsn = SSO_INTSN_EXE << 12 | nd->tx_complete_grp;
+	nd->tx_irq = irq_create_mapping(d, sso_intsn);
+	if (!nd->tx_irq) {
+		rv = -ENODEV;
+		goto done;
+	}
+
+	__cvmx_helper_init_port_config_data();
+	rv = __cvmx_helper_pko3_init_global(node, cvmx_fpa3_arua_to_guara(node, nd->pko_aura));
+	if (rv) {
+		pr_err("cvmx_helper_pko3_init_global failed\n");
+		rv = -ENODEV;
+		goto done;
+	}
+	__cvmx_helper_pki_install_default_vlan(node);
+	cvmx_pki_setup_clusters(node);
+	cvmx_pki_enable_backpressure(node);
+	cvmx_pki_parse_enable(node, 0);
+	cvmx_pki_enable(node);
+	/* Statistics per pkind */
+	cvmx_write_csr_node(node, CVMX_PKI_STAT_CTL, 0);
+
+	init_kthread_worker(&nd->tx_complete_worker);
+	init_kthread_work(&nd->tx_complete_work, octeon3_eth_tx_complete_worker);
+	nd->tx_complete_task = kthread_create_on_node(kthread_worker_fn, &nd->tx_complete_worker, node,
+						      "oct3_eth_tx_done[%d]", node);
+	if (IS_ERR(nd->tx_complete_task)) {
+		rv = PTR_ERR(nd->tx_complete_task);
+		goto done;
+	} else
+		wake_up_process(nd->tx_complete_task);
+
+
+	rv = request_irq(nd->tx_irq, octeon3_eth_tx_handler, 0, "oct3_eth_tx_done", nd);
+	if (rv)
+		goto done;
+	octeon3_eth_gen_affinity(node, &nd->tx_affinity_hint);
+	irq_set_affinity_hint(nd->tx_irq, &nd->tx_affinity_hint);
+
+	octeon3_eth_sso_irq_set_armed(node, nd->tx_complete_grp, true);
+
+	nd->init_done = true;
+done:
+	mutex_unlock(&octeon3_eth_init_mutex);
+	return rv;
+}
+
+static struct sk_buff *octeon3_eth_work_to_skb(void *w)
+{
+	struct sk_buff *skb;
+	void **f = w;
+	skb = f[-16];
+	return skb;
+}
+
+/* Receive one packet.
+ * returns the number of RX buffers consumed.
+ */
+static int octeon3_eth_rx_one(struct octeon3_ethernet *priv)
+{
+	int segments;
+	int ret;
+	unsigned int packet_len;
+	cvmx_wqe_78xx_t *work;
+	u8 *data;
+	int len_remaining;
+	struct sk_buff *skb;
+	union cvmx_buf_ptr_pki packet_ptr;
+	struct wr_ret r;
+
+	r = octeon3_eth_work_request_grp_sync(priv->rx_grp, CVMX_POW_NO_WAIT);
+	work = r.work;
+	if (!work)
+		return 0;
+	skb = octeon3_eth_work_to_skb(work);
+	segments = work->word0.bufs;
+	ret = segments;
+	packet_ptr = work->packet_ptr;
+	if (unlikely(work->word2.err_level <= CVMX_PKI_ERRLEV_E_LA &&
+		     work->word2.err_code != CVMX_PKI_OPCODE_RE_NONE)) {
+		atomic64_inc(&priv->rx_errors);
+		switch (work->word2.err_code) {
+		case CVMX_PKI_OPCODE_RE_JABBER:
+			atomic64_inc(&priv->rx_length_errors);
+			break;
+		case CVMX_PKI_OPCODE_RE_FCS:
+			atomic64_inc(&priv->rx_crc_errors);
+			break;
+		}
+		data = phys_to_virt(packet_ptr.addr);
+		for (;;) {
+			dev_kfree_skb_any(skb);
+			segments--;
+			if (segments <= 0)
+				break;
+			packet_ptr.u64 = *(u64 *)(data - 8);
+			/* PKI-20776 PKI_BUFLINK_S's are endian-swapped */
+			packet_ptr.u64 = swab64(packet_ptr.u64);
+			data = phys_to_virt(packet_ptr.addr);
+			skb = octeon3_eth_work_to_skb((void *)round_down((unsigned long)data, 128ul));
+		}
+		goto out;
+	}
+
+	packet_len = work->word1.len;
+	data = phys_to_virt(packet_ptr.addr);
+	skb->data = data;
+	skb->len = packet_len;
+	len_remaining = packet_len;
+	if (segments == 1) {
+		skb_set_tail_pointer(skb, skb->len);
+	} else {
+		bool first_frag = true;
+		struct sk_buff *current_skb = skb;
+		struct sk_buff *next_skb = NULL;
+		unsigned int segment_size;
+
+		skb_frag_list_init(skb);
+		for (;;) {
+			segment_size = (segments == 1) ? len_remaining : packet_ptr.size;
+			len_remaining -= segment_size;
+			if (!first_frag) {
+				current_skb->len = segment_size;
+				skb->data_len += segment_size;
+				skb->truesize += current_skb->truesize;
+			}
+			skb_set_tail_pointer(current_skb, segment_size);
+			segments--;
+			if (segments == 0)
+				break;
+			packet_ptr.u64 = *(u64 *)(data - 8);
+			/* PKI-20776 PKI_BUFLINK_S's are endian-swapped */
+			packet_ptr.u64 = swab64(packet_ptr.u64);
+
+			data = phys_to_virt(packet_ptr.addr);
+			next_skb = octeon3_eth_work_to_skb((void *)round_down((unsigned long)data, 128ul));
+			if (first_frag) {
+				skb_frag_add_head(current_skb, next_skb);
+			} else {
+				current_skb->next = next_skb;
+				next_skb->next = NULL;
+			}
+			current_skb = next_skb;
+			first_frag = false;
+			current_skb->data = data;
+		}
+	}
+	if (likely(priv->netdev->flags & IFF_UP)) {
+		skb_checksum_none_assert(skb);
+		skb->protocol = eth_type_trans(skb, priv->netdev);
+		skb->dev = priv->netdev;
+		if (priv->netdev->features & NETIF_F_RXCSUM) {
+			if ((work->word2.lc_hdr_type == CVMX_PKI_LTYPE_E_IP4 ||
+			     work->word2.lc_hdr_type == CVMX_PKI_LTYPE_E_IP6) &&
+			    (work->word2.lf_hdr_type == CVMX_PKI_LTYPE_E_TCP ||
+			     work->word2.lf_hdr_type == CVMX_PKI_LTYPE_E_UDP ||
+			     work->word2.lf_hdr_type == CVMX_PKI_LTYPE_E_SCTP))
+				if (work->word2.err_code == 0)
+					skb->ip_summed = CHECKSUM_UNNECESSARY;
+
+
+		}
+		netif_receive_skb(skb);
+	} else {
+		/* Drop any packet received for a device that isn't up */
+		atomic64_inc(&priv->rx_dropped);
+		dev_kfree_skb_any(skb);
+	}
+out:
+	return ret;
+}
+
+static void octeon3_eth_replentish_rx(struct octeon3_ethernet *priv, int count)
+{
+	struct sk_buff *skb;
+	int i;
+
+	for (i = 0; i < count; i++) {
+		void **buf;
+		skb = __alloc_skb(packet_buffer_size, GFP_ATOMIC, 0, priv->numa_node);
+		if (!skb) {
+			pr_err("WARNING: octeon3_eth_replentish_rx out of memory\n");
+			break;
+		}
+		buf = (void **)PTR_ALIGN(skb->head, 128);
+		buf[0] = skb;
+		cvmx_fpa3_free_aura(buf, priv->numa_node, priv->pki_laura, 0);
+	}
+}
+
+static int octeon3_eth_napi(struct napi_struct *napi, int budget)
+{
+	int rx_count = 0;
+	int bufs_consumed = 0;
+	struct octeon3_ethernet *priv = container_of(napi, struct octeon3_ethernet, napi);
+
+	while (rx_count < budget) {
+		int n = octeon3_eth_rx_one(priv);
+		if (n == 0) {
+			napi_complete(napi);
+			octeon3_eth_sso_irq_set_armed(priv->numa_node, priv->rx_grp, true);
+			break;
+		}
+		rx_count++;
+		bufs_consumed += n;
+	}
+
+	if (bufs_consumed)
+		octeon3_eth_replentish_rx(priv, bufs_consumed);
+
+	return rx_count;
+}
+
+//#define BROKEN_SIMULATOR_CSUM 1
+
+static int octeon3_eth_ndo_init(struct net_device *netdev)
+{
+	struct octeon3_ethernet *priv = netdev_priv(netdev);
+	struct octeon3_ethernet_node *nd = octeon3_eth_node + priv->numa_node;
+	struct cvmx_pki_port_config pki_prt_cfg;
+	struct cvmx_pki_prt_schd *prt_schd = NULL;
+	int ipd_port, node_dq;
+	int first_skip, later_skip;
+	struct cvmx_xport xdq;
+	int r;
+	const u8 *mac;
+
+	netif_carrier_off(netdev);
+
+	netdev->features |=
+		NETIF_F_SG |
+		NETIF_F_FRAGLIST |
+		NETIF_F_RXCSUM
+#ifndef BROKEN_SIMULATOR_CSUM
+		|
+		NETIF_F_IP_CSUM |
+		NETIF_F_IPV6_CSUM
+#endif
+		;
+
+	if (!OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X)) /* PKO-18824 */
+		netdev->features |= NETIF_F_SCTP_CSUM;
+
+	priv->rx_buf_count = num_packet_buffers;
+
+	ipd_port = cvmx_helper_get_ipd_port(priv->xiface, priv->port_index);
+
+	r =  __cvmx_pko3_config_gen_interface(priv->xiface, priv->port_index, 1, false);
+	if (r)
+		return -ENODEV;
+
+	r = __cvmx_pko3_helper_dqs_activate(priv->xiface, priv->port_index, false);
+	if (r < 0)
+		return -ENODEV;
+
+	/* Padding and FCS are done in BGX */
+	r = cvmx_pko3_interface_options(priv->xiface, priv->port_index, false, false, 0);
+	if (r)
+		return -ENODEV;
+
+	node_dq = cvmx_pko3_get_queue_base(ipd_port);
+	xdq = cvmx_helper_ipd_port_to_xport(node_dq);
+
+	priv->pko_queue = xdq.port;
+	priv->pki_laura = cvmx_fpa3_allocate_aura(priv->numa_node);
+	octeon3_eth_fpa_aura_init(priv->numa_node, nd->pki_packet_pool, priv->pki_laura,
+				  num_packet_buffers * 2);
+
+	priv->rx_grp = cvmx_sso_allocate_group(priv->numa_node);
+	priv->tx_complete_grp = nd->tx_complete_grp;
+	priv->pki_pkind = cvmx_helper_get_pknd(priv->xiface, priv->port_index);
+	dev_err(netdev->dev.parent, "rx sso grp:%d aura:%d pknd:%d\n",
+		priv->rx_grp, priv->pki_laura, priv->pki_pkind);
+	if (priv->rx_grp < 0) {
+		dev_err(netdev->dev.parent, "Failed to allocated SSO group\n");
+		return -ENODEV;
+	}
+
+	prt_schd = kzalloc(sizeof(*prt_schd), GFP_KERNEL);
+	if (!prt_schd) {
+		r = -ENOMEM;
+		goto err;
+	}
+	prt_schd->style = -1; /* Allocate net style per port */
+	prt_schd->qpg_base = -1;
+	prt_schd->aura_per_prt = true;
+	prt_schd->aura = priv->pki_laura;
+	prt_schd->sso_grp_per_prt = true;
+	prt_schd->sso_grp = priv->rx_grp;
+	prt_schd->qpg_qos = CVMX_PKI_QPG_QOS_NONE;
+
+	cvmx_helper_pki_init_port(ipd_port, prt_schd);
+	cvmx_pki_get_port_config(ipd_port, &pki_prt_cfg);
+
+	pki_prt_cfg.style_cfg.parm_cfg.ip6_udp_opt = false;
+	pki_prt_cfg.style_cfg.parm_cfg.lenerr_en = true;
+	pki_prt_cfg.style_cfg.parm_cfg.maxerr_en = false;
+	pki_prt_cfg.style_cfg.parm_cfg.minerr_en = false;
+	pki_prt_cfg.style_cfg.parm_cfg.ip6_udp_opt = false;
+	pki_prt_cfg.style_cfg.parm_cfg.ip6_udp_opt = false;
+	pki_prt_cfg.style_cfg.parm_cfg.wqe_skip = 1 * 128;
+	first_skip = 8 * 21;
+	later_skip = 8 * 16;
+	pki_prt_cfg.style_cfg.parm_cfg.first_skip = first_skip;
+	pki_prt_cfg.style_cfg.parm_cfg.later_skip = later_skip;
+#ifdef __LITTLE_ENDIAN
+	pki_prt_cfg.style_cfg.parm_cfg.pkt_lend = true;
+#else
+	pki_prt_cfg.style_cfg.parm_cfg.pkt_lend = false;
+#endif
+	pki_prt_cfg.style_cfg.parm_cfg.tag_type = CVMX_SSO_TAG_TYPE_UNTAGGED;
+	pki_prt_cfg.style_cfg.parm_cfg.qpg_dis_grptag = true;
+	pki_prt_cfg.style_cfg.parm_cfg.dis_wq_dat = false;
+
+	pki_prt_cfg.style_cfg.parm_cfg.csum_lb = true;
+	pki_prt_cfg.style_cfg.parm_cfg.csum_lc = true;
+	pki_prt_cfg.style_cfg.parm_cfg.csum_ld = true;
+	pki_prt_cfg.style_cfg.parm_cfg.csum_le = true;
+	pki_prt_cfg.style_cfg.parm_cfg.csum_lf = true;
+	pki_prt_cfg.style_cfg.parm_cfg.csum_lg = false;
+
+	pki_prt_cfg.style_cfg.parm_cfg.len_lb = false;
+	pki_prt_cfg.style_cfg.parm_cfg.len_lc = true;
+	pki_prt_cfg.style_cfg.parm_cfg.len_ld = false;
+	pki_prt_cfg.style_cfg.parm_cfg.len_le = false;
+	pki_prt_cfg.style_cfg.parm_cfg.len_lf = true;
+	pki_prt_cfg.style_cfg.parm_cfg.len_lg = false;
+
+	pki_prt_cfg.style_cfg.parm_cfg.mbuff_size = (packet_buffer_size - 128) & ~0xf;
+
+	cvmx_pki_config_port(ipd_port, &pki_prt_cfg);
+
+	cvmx_write_csr_node(priv->numa_node, CVMX_PKI_STATX_STAT0(priv->pki_pkind), 0);
+	priv->last_packets = 0;
+
+	cvmx_write_csr_node(priv->numa_node, CVMX_PKI_STATX_STAT1(priv->pki_pkind), 0);
+	priv->last_octets = 0;
+
+	cvmx_write_csr_node(priv->numa_node, CVMX_PKI_STATX_STAT3(priv->pki_pkind), 0);
+	priv->last_dropped = 0;
+
+	mac = bgx_port_get_mac(netdev);
+	if (mac && is_valid_ether_addr(mac)) {
+		memcpy(netdev->dev_addr, mac, ETH_ALEN);
+		netdev->addr_assign_type &= ~NET_ADDR_RANDOM;
+	} else {
+		eth_hw_addr_random(netdev);
+	}
+	bgx_port_set_rx_filtering(netdev);
+	bgx_port_change_mtu(netdev, netdev->mtu);
+	netif_napi_add(netdev, &priv->napi, octeon3_eth_napi, 32);
+	napi_enable(&priv->napi);
+
+	netdev_info(netdev, "octeon3_eth_ndo_init\n");
+	return 0;
+err:
+	kfree(prt_schd);
+	return r;
+}
+
+static void octeon3_eth_ndo_uninit(struct net_device *netdev)
+{
+	return;
+}
+
+static irqreturn_t octeon3_eth_rx_handler(int irq, void *info)
+{
+	struct net_device *netdev = info;
+	struct octeon3_ethernet *priv = netdev_priv(netdev);
+
+	/* Disarm the irq. */
+	octeon3_eth_sso_irq_set_armed(priv->numa_node, priv->rx_grp, false);
+
+	napi_schedule(&priv->napi);
+	return IRQ_HANDLED;
+}
+
+static int octeon3_eth_ndo_open(struct net_device *netdev)
+{
+	struct octeon3_ethernet *priv = netdev_priv(netdev);
+	struct irq_domain *d = octeon_irq_get_block_domain(priv->numa_node, SSO_INTSN_EXE);
+	unsigned int sso_intsn = SSO_INTSN_EXE << 12 | priv->rx_grp;
+	int r;
+
+	priv->rx_irq = irq_create_mapping(d, sso_intsn);
+	if (!priv->rx_irq) {
+		netdev_err(netdev, "ERROR: Couldn't map hwirq: %x\n", sso_intsn);
+		return -EINVAL;
+	}
+
+	/* Arm the irq. */
+	octeon3_eth_sso_irq_set_armed(priv->numa_node, priv->rx_grp, true);
+
+	r = request_irq(priv->rx_irq, octeon3_eth_rx_handler, 0, netdev_name(netdev), netdev);
+	if (r)
+		goto err;
+
+	octeon3_eth_gen_affinity(priv->numa_node, &priv->rx_affinity_hint);
+	irq_set_affinity_hint(priv->rx_irq, &priv->rx_affinity_hint);
+
+	octeon3_eth_replentish_rx(priv, priv->rx_buf_count);
+
+	r = bgx_port_enable(netdev);
+	return r;
+err:
+	/* Cleanup mapping ?? */
+	return r;
+}
+
+static int octeon3_eth_ndo_stop(struct net_device *netdev)
+{
+	struct octeon3_ethernet *priv = netdev_priv(netdev);
+	void **w;
+	struct sk_buff *skb;
+	int r;
+
+	r = bgx_port_disable(netdev);
+	if (r)
+		goto err;
+
+	msleep(20);
+
+	/* Wait for SSO to drain */
+	while (cvmx_read_csr_node(priv->numa_node, CVMX_SSO_GRPX_AQ_CNT(priv->rx_grp)))
+		msleep(20);
+
+	octeon3_eth_sso_irq_set_armed(priv->numa_node, priv->rx_grp, false);
+
+	irq_set_affinity_hint(priv->rx_irq, NULL);
+	free_irq(priv->rx_irq, netdev);
+	priv->rx_irq = 0;
+
+	msleep(20);
+
+	/* Free the packet buffers */
+	for (;;) {
+		w = cvmx_fpa3_alloc_aura(priv->numa_node, priv->pki_laura);
+		if (!w)
+			break;
+		skb = w[0];
+		dev_kfree_skb(skb);
+	}
+
+
+err:
+	return r;
+}
+
+static int octeon3_eth_ndo_start_xmit(struct sk_buff *skb, struct net_device *netdev)
+{
+	struct sk_buff *skb_tmp;
+	struct octeon3_ethernet *priv = netdev_priv(netdev);
+	unsigned int scr_off = CVMX_PKO_LMTLINE * CVMX_CACHE_LINE_SIZE;
+	unsigned int ret_off = scr_off;
+	union cvmx_pko_send_hdr send_hdr;
+	union cvmx_pko_buf_ptr buf_ptr;
+	union cvmx_pko_send_work send_work;
+	union cvmx_pko_send_mem send_mem;
+	union cvmx_pko_lmtdma_data lmtdma_data;
+	union cvmx_pko_query_rtn query_rtn;
+	u8 l4_hdr = 0;
+	unsigned int checksum_alg;
+	long backlog;
+	int frag_count;
+	int head_len, i;
+	u64 dma_addr;
+	void **work;
+
+	frag_count = 0;
+	if (skb_has_frag_list(skb))
+		skb_walk_frags(skb, skb_tmp)
+			frag_count++;
+	/* We have space for 13 segment pointers, If there will be
+	 * more than that, we must linearize.  The count is: 1 (base
+	 * SKB) + frag_count + nr_frags.
+	 */
+	if (unlikely(skb_shinfo(skb)->nr_frags + frag_count > 12)) {
+		if (unlikely(__skb_linearize(skb)))
+			goto skip_xmit;
+		frag_count = 0;
+	}
+
+	work = (void **)skb->cb;
+	work[0] = NULL;
+	work[1] = NULL;
+
+	backlog = atomic64_inc_return(&priv->tx_backlog);
+	if (backlog > MAX_TX_QUEUE_DEPTH)
+		netif_stop_queue(netdev);
+
+	/* Adjust the port statistics. */
+	atomic64_inc(&priv->tx_packets);
+	atomic64_add(skb->len, &priv->tx_octets);
+
+	/* Make sure packet data writes are committed before
+	 * submitting the command below
+	 */
+	wmb();
+	send_hdr.u64 = 0;
+#ifdef __LITTLE_ENDIAN
+	send_hdr.s.le = 1;
+#endif
+/* broken in sim */	send_hdr.s.n2 = 1; /* Don't allocate to L2 */
+	send_hdr.s.df = 1; /* Don't automatically free to FPA */
+	send_hdr.s.total = skb->len;
+
+#ifndef BROKEN_SIMULATOR_CSUM
+	switch (skb->protocol) {
+	case __constant_htons(ETH_P_IP):
+		send_hdr.s.l3ptr = ETH_HLEN;
+		send_hdr.s.ckl3 = 1;
+		l4_hdr = ip_hdr(skb)->protocol;
+		send_hdr.s.l4ptr = ETH_HLEN + (4 * ip_hdr(skb)->ihl);
+		break;
+	case __constant_htons(ETH_P_IPV6):
+		l4_hdr = ipv6_hdr(skb)->nexthdr;
+		break;
+	default:
+		break;
+	}
+#endif
+	checksum_alg = 1; /* UDP == 1 */
+	switch (l4_hdr) {
+	case IPPROTO_SCTP:
+		if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X)) /* PKO-18824 */
+			break;
+		checksum_alg++; /* SCTP == 3 */
+		/* Fall through */
+	case IPPROTO_TCP: /* TCP == 2 */
+		checksum_alg++;
+		/* Fall through */
+	case IPPROTO_UDP:
+		if (skb_transport_header_was_set(skb)) {
+			int l4ptr = skb_transport_header(skb) - skb->data;
+			send_hdr.s.l4ptr = l4ptr;
+			send_hdr.s.ckl4 = checksum_alg;
+		}
+		break;
+	default:
+		break;
+	}
+
+	cvmx_scratch_write64(scr_off, send_hdr.u64);
+	scr_off += sizeof(send_hdr);
+
+	buf_ptr.u64 = 0;
+	buf_ptr.s.subdc3 = CVMX_PKO_SENDSUBDC_GATHER;
+
+	/* Add a Gather entry for each segment. */
+	head_len = skb_headlen(skb);
+	if (head_len > 0) {
+		buf_ptr.s.size = head_len;
+		buf_ptr.s.addr = virt_to_phys(skb->data);
+		cvmx_scratch_write64(scr_off, buf_ptr.u64);
+		scr_off += sizeof(buf_ptr);
+	}
+	for (i = 1; i <= skb_shinfo(skb)->nr_frags; i++) {
+		struct skb_frag_struct *fs = skb_shinfo(skb)->frags + i - 1;
+		buf_ptr.s.size = fs->size;
+		buf_ptr.s.addr = virt_to_phys((u8 *)page_address(fs->page.p) + fs->page_offset);
+		cvmx_scratch_write64(scr_off, buf_ptr.u64);
+		scr_off += sizeof(buf_ptr);
+	}
+	skb_walk_frags(skb, skb_tmp) {
+		buf_ptr.s.addr = virt_to_phys(skb_tmp->data);
+		buf_ptr.s.size = skb_tmp->len;
+		cvmx_scratch_write64(scr_off, buf_ptr.u64);
+		scr_off += sizeof(buf_ptr);
+	}
+
+	/* Subtract 1 from the tx_backlog. */
+	send_mem.u64 = 0;
+	send_mem.s.subdc4 = CVMX_PKO_SENDSUBDC_MEM;
+	send_mem.s.dsz = MEMDSZ_B64;
+	send_mem.s.alg = MEMALG_SUB;
+	send_mem.s.offset = 1;
+	send_mem.s.addr = virt_to_phys(&priv->tx_backlog);
+	cvmx_scratch_write64(scr_off, send_mem.u64);
+	scr_off += sizeof(buf_ptr);
+
+	/* Send work when finished with the packet. */
+	send_work.u64 = 0;
+	send_work.s.subdc4 = CVMX_PKO_SENDSUBDC_WORK;
+	send_work.s.addr = virt_to_phys(work);
+	send_work.s.tt = CVMX_POW_TAG_TYPE_NULL;
+	send_work.s.grp = priv->tx_complete_grp;
+	cvmx_scratch_write64(scr_off, send_work.u64);
+	scr_off += sizeof(buf_ptr);
+
+	lmtdma_data.u64 = 0;
+	lmtdma_data.s.scraddr = ret_off >> 3;
+	lmtdma_data.s.rtnlen = 1;
+	lmtdma_data.s.did = 0x51;
+	lmtdma_data.s.node = priv->numa_node;
+	lmtdma_data.s.dq = priv->pko_queue;
+	dma_addr = 0xffffffffffffa400ull | ((scr_off & 0x78) - 8);
+	cvmx_write64_uint64(dma_addr, lmtdma_data.u64);
+
+	CVMX_SYNCIOBDMA;
+
+	query_rtn.u64 = cvmx_scratch_read64(ret_off);
+	if (unlikely(query_rtn.s.dqstatus != PKO_DQSTATUS_PASS)) {
+		netdev_err(netdev, "PKO enqueue failed %llx\n",
+			   (unsigned long long)query_rtn.u64);
+		dev_kfree_skb_any(skb);
+	}
+
+	return NETDEV_TX_OK;
+skip_xmit:
+	atomic64_inc(&priv->tx_dropped);
+	dev_kfree_skb_any(skb);
+	return NETDEV_TX_OK;
+}
+
+static u64 read_pki_stat(int numa_node, u64 csr)
+{
+	u64 v;
+
+	/* PKI-20775, must read until not all ones. */
+	do {
+		v = cvmx_read_csr_node(numa_node, csr);
+	} while (v == 0xffffffffffffffffull);
+	return v;
+}
+
+static struct rtnl_link_stats64 *octeon3_eth_ndo_get_stats64(struct net_device *netdev,
+							     struct rtnl_link_stats64 *s)
+{
+	struct octeon3_ethernet *priv = netdev_priv(netdev);
+	u64 packets, octets, dropped;
+	u64 delta_packets, delta_octets, delta_dropped;
+
+	spin_lock(&priv->stat_lock);
+
+	packets = read_pki_stat(priv->numa_node, CVMX_PKI_STATX_STAT0(priv->pki_pkind));
+	octets = read_pki_stat(priv->numa_node, CVMX_PKI_STATX_STAT1(priv->pki_pkind));
+	dropped = read_pki_stat(priv->numa_node, CVMX_PKI_STATX_STAT3(priv->pki_pkind));
+
+	delta_packets = (packets - priv->last_packets) & ((1ull << 48) - 1);
+	delta_octets = (octets - priv->last_octets) & ((1ull << 48) - 1);
+	delta_dropped = (dropped - priv->last_dropped) & ((1ull << 48) - 1);
+
+	priv->last_packets = packets;
+	priv->last_octets = octets;
+	priv->last_dropped = dropped;
+
+	spin_unlock(&priv->stat_lock);
+
+	atomic64_add(delta_packets, &priv->rx_packets);
+	atomic64_add(delta_octets, &priv->rx_octets);
+	atomic64_add(delta_dropped, &priv->rx_dropped);
+
+	s->rx_packets = atomic64_read(&priv->rx_packets);
+	s->rx_bytes = atomic64_read(&priv->rx_octets);
+	s->rx_dropped = atomic64_read(&priv->rx_dropped);
+	s->rx_errors = atomic64_read(&priv->rx_errors);
+	s->rx_length_errors = atomic64_read(&priv->rx_length_errors);
+	s->rx_crc_errors = atomic64_read(&priv->rx_crc_errors);
+
+	s->tx_packets = atomic64_read(&priv->tx_packets);
+	s->tx_bytes = atomic64_read(&priv->tx_octets);
+	s->tx_dropped = atomic64_read(&priv->tx_dropped);
+	return s;
+}
+
+static int octeon3_eth_set_mac_address(struct net_device *netdev, void *addr)
+{
+	int r = eth_mac_addr(netdev, addr);
+
+	if (r)
+		return r;
+
+	bgx_port_set_rx_filtering(netdev);
+
+	return 0;
+}
+
+static const struct net_device_ops octeon3_eth_netdev_ops = {
+	.ndo_init		= octeon3_eth_ndo_init,
+	.ndo_uninit		= octeon3_eth_ndo_uninit,
+	.ndo_open		= octeon3_eth_ndo_open,
+	.ndo_stop		= octeon3_eth_ndo_stop,
+	.ndo_start_xmit		= octeon3_eth_ndo_start_xmit,
+	.ndo_get_stats64	= octeon3_eth_ndo_get_stats64,
+	.ndo_set_rx_mode	= bgx_port_set_rx_filtering,
+	.ndo_set_mac_address	= octeon3_eth_set_mac_address,
+	.ndo_change_mtu		= bgx_port_change_mtu
+};
+
+static int octeon3_eth_probe(struct platform_device *pdev)
+{
+	struct octeon3_ethernet *priv;
+	struct net_device *netdev;
+	int r;
+
+	struct bgx_platform_data *pd = dev_get_platdata(&pdev->dev);
+
+	r = octeon3_eth_global_init(pd->numa_node);
+	if (r)
+		return r;
+
+	dev_err(&pdev->dev, "Probing %d-%d:%d\n", pd->numa_node, pd->interface, pd->port);
+	netdev = alloc_etherdev(sizeof(struct octeon3_ethernet));
+	if (!netdev) {
+		dev_err(&pdev->dev, "Failed to allocated ethernet device\n");
+		return -ENOMEM;
+	}
+	SET_NETDEV_DEV(netdev, &pdev->dev);
+	dev_set_drvdata(&pdev->dev, netdev);
+
+	bgx_port_set_netdev(pdev->dev.parent, netdev);
+	priv = netdev_priv(netdev);
+	priv->netdev = netdev;
+	priv->numa_node = pd->numa_node;
+	priv->xiface = cvmx_helper_node_interface_to_xiface(pd->numa_node, pd->interface);
+	priv->port_index = pd->port;
+	spin_lock_init(&priv->stat_lock);
+	netdev->netdev_ops = &octeon3_eth_netdev_ops;
+
+	if (register_netdev(netdev) < 0) {
+		dev_err(&pdev->dev, "Failed to register ethernet device\n");
+		free_netdev(netdev);
+	}
+	return 0;
+}
+
+static int octeon3_eth_remove(struct platform_device *pdev)
+{
+//	struct net_device *netdev = dev_get_drvdata(&pdev->dev);
+//	struct octeon3_ethernet *priv = netdev_priv(netdev);
+	return 0;
+}
+
+static void octeon3_eth_shutdown(struct platform_device *pdev)
+{
+	return;
+}
+
+
+static struct platform_driver octeon3_eth_driver = {
+	.probe		= octeon3_eth_probe,
+	.remove		= octeon3_eth_remove,
+	.shutdown       = octeon3_eth_shutdown,
+	.driver		= {
+		.owner	= THIS_MODULE,
+		.name	= "ethernet-mac-pki",
+	},
+};
+
+
+
+static int __init octeon3_eth_init(void)
+{
+	if (!OCTEON_IS_MODEL(OCTEON_CN78XX))
+		return 0;
+
+	return platform_driver_register(&octeon3_eth_driver);
+}
+module_init(octeon3_eth_init);
+
+static void __exit octeon3_eth_exit(void)
+{
+	if (!OCTEON_IS_MODEL(OCTEON_CN78XX))
+		return;
+
+	platform_driver_unregister(&octeon3_eth_driver);
+}
+module_exit(octeon3_eth_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Cavium Networks <support@caviumnetworks.com>");
+MODULE_DESCRIPTION("Cavium Networks PKI/PKO Ethernet driver.");
-- 
1.8.2.1

