From d1b49ce72f92c661bb7a5d462694eb735953d6b7 Mon Sep 17 00:00:00 2001
From: Leonid Rosenboim <lrosenboim@caviumnetworks.com>
Date: Wed, 3 Sep 2014 16:40:58 -0700
Subject: [PATCH 176/202] MIPS: Octeon: Update ethernet driver to match

Update of Octeon and Octeon3 ethernet drivers to use native FPA
API layer for the appropriate chip family.

Signed-off-by: Leonid Rosenboim <lrosenboim@caviumnetworks.com>
[Original patch taken from Cavium SDK 3.1.1-544]
Signed-off-by: Bin Jiang <bin.jiang@windriver.com>
---
 drivers/net/ethernet/octeon/ethernet-mem.c     |  10 ++-
 drivers/net/ethernet/octeon/ethernet.c         |   8 ++
 drivers/net/ethernet/octeon/octeon3-ethernet.c | 105 ++++++++++++++-----------
 3 files changed, 71 insertions(+), 52 deletions(-)

diff --git a/drivers/net/ethernet/octeon/ethernet-mem.c b/drivers/net/ethernet/octeon/ethernet-mem.c
index 01fb90b..770212d 100644
--- a/drivers/net/ethernet/octeon/ethernet-mem.c
+++ b/drivers/net/ethernet/octeon/ethernet-mem.c
@@ -237,6 +237,8 @@ int cvm_oct_alloc_fpa_pool(int pool, int size)
 	if (pool >= (int)ARRAY_SIZE(cvm_oct_pools) || size < 128)
 		return -EINVAL;
 
+	BUG_ON(octeon_has_feature(OCTEON_FEATURE_FPA3));
+
 	mutex_lock(&cvm_oct_pools_lock);
 
 	if (pool >= 0) {
@@ -253,7 +255,7 @@ int cvm_oct_alloc_fpa_pool(int pool, int size)
 			}
 		}
 		/* reserve/alloc fpa pool */
-		pool = cvmx_fpa_alloc_pool(pool);
+		pool = cvmx_fpa1_reserve_pool(pool);
 		if (pool < 0) {
 			ret = -EINVAL;
 			goto out;
@@ -269,7 +271,7 @@ int cvm_oct_alloc_fpa_pool(int pool, int size)
 			}
 
 		/* Alloc fpa pool */
-		pool = cvmx_fpa_alloc_pool(pool);
+		pool = cvmx_fpa1_reserve_pool(pool);
 		if (pool < 0) {
 			/* No empties. */
 			ret = -EINVAL;
@@ -299,7 +301,7 @@ int cvm_oct_alloc_fpa_pool(int pool, int size)
 		if (!cvm_oct_pools[pool].kmem) {
 			ret = -ENOMEM;
 			cvm_oct_pools[pool].pool = -1;
-			cvmx_fpa_release_pool(pool);
+			cvmx_fpa1_release_pool(pool);
 			goto out;
 		}
 	}
@@ -334,7 +336,7 @@ int cvm_oct_release_fpa_pool(int pool)
 	cvm_oct_pools[pool].users--;
 
 	if (cvm_oct_pools[pool].users == 0)
-		cvmx_fpa_release_pool(pool);
+		cvmx_fpa1_release_pool(pool);
 
 	ret = 0;
 out:
diff --git a/drivers/net/ethernet/octeon/ethernet.c b/drivers/net/ethernet/octeon/ethernet.c
index 9db6578..0ed97dc0 100644
--- a/drivers/net/ethernet/octeon/ethernet.c
+++ b/drivers/net/ethernet/octeon/ethernet.c
@@ -944,6 +944,14 @@ static int cvm_oct_probe(struct platform_device *pdev)
 				priv->num_tx_queues = cvmx_pko_get_num_queues(priv->ipd_port);
 			}
 
+			if (priv->num_tx_queues == 0) {
+				dev_err(&pdev->dev,
+					"tx_queue count not configured for port %d:%d\n",
+					interface, interface_port);
+				free_netdev(dev);
+				continue;
+			}
+
 			BUG_ON(priv->num_tx_queues < 1);
 			BUG_ON(priv->num_tx_queues > 32);
 
diff --git a/drivers/net/ethernet/octeon/octeon3-ethernet.c b/drivers/net/ethernet/octeon/octeon3-ethernet.c
index 42ed117..a3ca279 100644
--- a/drivers/net/ethernet/octeon/octeon3-ethernet.c
+++ b/drivers/net/ethernet/octeon/octeon3-ethernet.c
@@ -228,11 +228,11 @@ struct octeon3_ethernet_node {
 	bool napi_init_done;
 	int next_cpu_irq_affinity;
 	int numa_node;
-	int pki_packet_pool;
-	int sso_pool;
-	int pko_pool;
-	int sso_aura;
-	int pko_aura;
+	cvmx_fpa3_pool_t  pki_packet_pool;
+	cvmx_fpa3_pool_t sso_pool;
+	cvmx_fpa3_pool_t pko_pool;
+	cvmx_fpa3_gaura_t sso_aura;
+	cvmx_fpa3_gaura_t pko_aura;
 	int tx_complete_grp;
 	int tx_irq;
 	cpumask_t tx_affinity_hint;
@@ -295,7 +295,7 @@ static void octeon3_eth_gen_affinity(int node, cpumask_t *mask)
 	cpumask_set_cpu(cpu, mask);
 }
 
-static int octeon3_eth_fpa_pool_init(unsigned int node, unsigned int pool, int num_ptrs)
+static int octeon3_eth_fpa_pool_init(cvmx_fpa3_pool_t pool, int num_ptrs)
 {
 	void *pool_stack;
 	u64 pool_stack_start, pool_stack_end;
@@ -303,13 +303,13 @@ static int octeon3_eth_fpa_pool_init(unsigned int node, unsigned int pool, int n
 	union cvmx_fpa_poolx_cfg cfg;
 	int stack_size = (DIV_ROUND_UP(num_ptrs, 29) + 1) * 128;
 
-	cvmx_write_csr_node(node, CVMX_FPA_POOLX_CFG(pool), 0);
-	cvmx_write_csr_node(node, CVMX_FPA_POOLX_START_ADDR(pool), 128);
+	cvmx_write_csr_node(pool.node, CVMX_FPA_POOLX_CFG(pool.lpool), 0);
+	cvmx_write_csr_node(pool.node, CVMX_FPA_POOLX_START_ADDR(pool.lpool), 128);
 	limit_addr.u64 = 0;
 	limit_addr.cn78xx.addr = ~0ll;
-	cvmx_write_csr_node(node, CVMX_FPA_POOLX_END_ADDR(pool), limit_addr.u64);
+	cvmx_write_csr_node(pool.node, CVMX_FPA_POOLX_END_ADDR(pool.lpool), limit_addr.u64);
 
-	pool_stack = kmalloc_node(stack_size, GFP_KERNEL, node);
+	pool_stack = kmalloc_node(stack_size, GFP_KERNEL, pool.node);
 	if (!pool_stack)
 		return -ENOMEM;
 
@@ -317,30 +317,32 @@ static int octeon3_eth_fpa_pool_init(unsigned int node, unsigned int pool, int n
 	pool_stack_end = round_down(pool_stack_start + stack_size, 128);
 	pool_stack_start = round_up(pool_stack_start, 128);
 
-	cvmx_write_csr_node(node, CVMX_FPA_POOLX_STACK_BASE(pool), pool_stack_start);
-	cvmx_write_csr_node(node, CVMX_FPA_POOLX_STACK_ADDR(pool), pool_stack_start);
-	cvmx_write_csr_node(node, CVMX_FPA_POOLX_STACK_END(pool), pool_stack_end);
+	cvmx_write_csr_node(pool.node, CVMX_FPA_POOLX_STACK_BASE(pool.lpool), pool_stack_start);
+	cvmx_write_csr_node(pool.node, CVMX_FPA_POOLX_STACK_ADDR(pool.lpool), pool_stack_start);
+	cvmx_write_csr_node(pool.node, CVMX_FPA_POOLX_STACK_END(pool.lpool), pool_stack_end);
 
 	cfg.u64 = 0;
 	cfg.s.l_type = 2; /* Load with DWB */
 	cfg.s.ena = 1;
-	cvmx_write_csr_node(node, CVMX_FPA_POOLX_CFG(pool), cfg.u64);
+	cvmx_write_csr_node(pool.node, CVMX_FPA_POOLX_CFG(pool.lpool), cfg.u64);
 	return 0;
 }
 
-static int octeon3_eth_fpa_aura_init(unsigned int node, unsigned int pool, int aura, unsigned int limit)
+static int octeon3_eth_fpa_aura_init(cvmx_fpa3_pool_t pool, cvmx_fpa3_gaura_t aura, unsigned int limit)
 {
 	int shift;
 	union cvmx_fpa_aurax_cnt_levels cnt_levels;
 	unsigned int drop, pass;
 
+	BUG_ON(aura.node != pool.node);
+
 	limit *= 2; /* allow twice the limit before saturation at zero. */
 
-	cvmx_write_csr_node(node, CVMX_FPA_AURAX_CFG(aura), 0);
-	cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT_LIMIT(aura), limit);
-	cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT(aura), limit);
-	cvmx_write_csr_node(node, CVMX_FPA_AURAX_POOL(aura), pool);
-	cvmx_write_csr_node(node, CVMX_FPA_AURAX_POOL_LEVELS(aura), 0); /* No per-pool RED/Drop */
+	cvmx_write_csr_node(aura.node, CVMX_FPA_AURAX_CFG(aura.laura), 0);
+	cvmx_write_csr_node(aura.node, CVMX_FPA_AURAX_CNT_LIMIT(aura.laura), limit);
+	cvmx_write_csr_node(aura.node, CVMX_FPA_AURAX_CNT(aura.laura), limit);
+	cvmx_write_csr_node(aura.node, CVMX_FPA_AURAX_POOL(aura.laura), pool.lpool);
+	cvmx_write_csr_node(aura.node, CVMX_FPA_AURAX_POOL_LEVELS(aura.laura), 0); /* No per-pool RED/Drop */
 
 	shift = 0;
 	while ((limit >> shift) > 255)
@@ -354,7 +356,7 @@ static int octeon3_eth_fpa_aura_init(unsigned int node, unsigned int pool, int a
 	cnt_levels.s.red_ena = 1;
 	cnt_levels.s.drop = drop;
 	cnt_levels.s.pass = pass;
-	cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT_LEVELS(aura), cnt_levels.u64);
+	cvmx_write_csr_node(aura.node, CVMX_FPA_AURAX_CNT_LEVELS(aura.laura), cnt_levels.u64);
 
 	return 0;
 }
@@ -384,7 +386,7 @@ static int octeon3_eth_sso_init(unsigned int node, int aura)
 
 	for (i = 0; i < 256; i++) {
 		u64 phys;
-		void *mem = cvmx_fpa3_alloc_aura(node, aura);
+		void *mem = cvmx_fpa3_alloc(__cvmx_fpa3_gaura(node, aura));
 		if (!mem) {
 			rv = -ENOMEM;
 			goto err;
@@ -467,7 +469,9 @@ static void octeon3_eth_replenish_rx(struct octeon3_ethernet *priv, int count)
 		}
 		buf = (void **)PTR_ALIGN(skb->head, 128);
 		buf[SKB_PTR_OFFSET] = skb;
-		cvmx_fpa3_free_aura(buf, priv->numa_node, priv->pki_laura, 0);
+		cvmx_fpa3_free(buf,
+			__cvmx_fpa3_gaura(priv->numa_node, priv->pki_laura),
+			0);
 	}
 }
 
@@ -603,32 +607,33 @@ static int octeon3_eth_global_init(unsigned int node)
 	fpa_cfg.s.lvl_dly = 3;
 	cvmx_write_csr_node(node, CVMX_FPA_GEN_CFG, fpa_cfg.u64);
 
-	oen->sso_pool = cvmx_fpa_alloc_pool(-1);
-	if (oen->sso_pool < 0) {
+	oen->sso_pool = cvmx_fpa3_reserve_pool(node, -1);
+	if (!__cvmx_fpa3_pool_valid(oen->sso_pool)) {
 		rv = -ENODEV;
 		goto done;
 	}
-	oen->pko_pool = cvmx_fpa_alloc_pool(-1);
-	if (oen->pko_pool < 0) {
+	oen->pko_pool = cvmx_fpa3_reserve_pool(node, -1);
+	if (!__cvmx_fpa3_pool_valid(oen->pko_pool)) {
 		rv = -ENODEV;
 		goto done;
 	}
-	oen->pki_packet_pool = cvmx_fpa_alloc_pool(-1);
-	if (oen->pki_packet_pool < 0) {
+	oen->pki_packet_pool = cvmx_fpa3_reserve_pool(node, -1);
+	if (!__cvmx_fpa3_pool_valid(oen->pki_packet_pool)) {
 		rv = -ENODEV;
 		goto done;
 	}
-	oen->sso_aura = cvmx_fpa3_allocate_aura(node);
-	oen->pko_aura = cvmx_fpa3_allocate_aura(node);
+	oen->sso_aura = cvmx_fpa3_reserve_aura(node, -1);
+	oen->pko_aura = cvmx_fpa3_reserve_aura(node, -1);
 
 	pr_err("octeon3_eth_global_init  SSO:%d:%d, PKO:%d:%d\n",
-	       oen->sso_pool, oen->sso_aura,  oen->pko_pool, oen->pko_aura);
+	       oen->sso_pool.lpool, oen->sso_aura.laura,
+	       oen->pko_pool.lpool, oen->pko_aura.laura);
 
-	octeon3_eth_fpa_pool_init(node, oen->sso_pool, 40960);
-	octeon3_eth_fpa_pool_init(node, oen->pko_pool, 40960);
-	octeon3_eth_fpa_pool_init(node, oen->pki_packet_pool, 64 * num_packet_buffers);
-	octeon3_eth_fpa_aura_init(node, oen->sso_pool, oen->sso_aura, 20480);
-	octeon3_eth_fpa_aura_init(node, oen->pko_pool, oen->pko_aura, 20480);
+	octeon3_eth_fpa_pool_init(oen->sso_pool, 40960);
+	octeon3_eth_fpa_pool_init(oen->pko_pool, 40960);
+	octeon3_eth_fpa_pool_init(oen->pki_packet_pool, 64 * num_packet_buffers);
+	octeon3_eth_fpa_aura_init(oen->sso_pool, oen->sso_aura, 20480);
+	octeon3_eth_fpa_aura_init(oen->pko_pool, oen->pko_aura, 20480);
 
 	if (!octeon3_eth_sso_pko_cache) {
 		octeon3_eth_sso_pko_cache = kmem_cache_create("sso_pko", 4096, 128, 0, NULL);
@@ -644,16 +649,17 @@ static int octeon3_eth_global_init(unsigned int node)
 			rv = -ENOMEM;
 			goto done;
 		}
-		cvmx_fpa3_free_aura(mem, node, oen->sso_aura, 0);
+		cvmx_fpa3_free(mem, oen->sso_aura, 0);
 		mem = kmem_cache_alloc_node(octeon3_eth_sso_pko_cache, GFP_KERNEL, node);
 		if (!mem) {
 			rv = -ENOMEM;
 			goto done;
 		}
-		cvmx_fpa3_free_aura(mem, node, oen->pko_aura, 0);
+		cvmx_fpa3_free(mem, oen->pko_aura, 0);
 	}
 
-	rv = octeon3_eth_sso_init(node, oen->sso_aura);
+	BUG_ON(node != oen->sso_aura.node);
+	rv = octeon3_eth_sso_init(node, oen->sso_aura.laura);
 	if (rv)
 		goto done;
 
@@ -667,15 +673,16 @@ static int octeon3_eth_global_init(unsigned int node)
 	}
 
 	__cvmx_helper_init_port_config_data();
-	rv = __cvmx_helper_pko3_init_global(node, cvmx_fpa3_arua_to_guara(node, oen->pko_aura));
+	rv = __cvmx_helper_pko3_init_global(node, oen->pko_aura.laura | (node << 10));
 	if (rv) {
 		pr_err("cvmx_helper_pko3_init_global failed\n");
 		rv = -ENODEV;
 		goto done;
 	}
-	__cvmx_helper_pki_install_default_vlan(node);
+
+	__cvmx_helper_pki_install_dflt_vlan(node);
 	cvmx_pki_setup_clusters(node);
-	__cvmx_helper_pki_set_ltype_map(node);
+	__cvmx_helper_pki_set_dflt_ltype_map(node);
 	cvmx_pki_enable_backpressure(node);
 	cvmx_pki_parse_enable(node, 0);
 	cvmx_pki_enable(node);
@@ -1131,6 +1138,7 @@ static int octeon3_eth_ndo_init(struct net_device *netdev)
 	struct cvmx_xport xdq;
 	int r, i;
 	const u8 *mac;
+	cvmx_fpa3_gaura_t aura;
 
 	netif_carrier_off(netdev);
 
@@ -1170,8 +1178,9 @@ static int octeon3_eth_ndo_init(struct net_device *netdev)
 	xdq = cvmx_helper_ipd_port_to_xport(node_dq);
 
 	priv->pko_queue = xdq.port;
-	priv->pki_laura = cvmx_fpa3_allocate_aura(priv->numa_node);
-	octeon3_eth_fpa_aura_init(priv->numa_node, oen->pki_packet_pool, priv->pki_laura,
+	aura = cvmx_fpa3_reserve_aura(priv->numa_node, -1);
+	priv->pki_laura = aura.laura;
+	octeon3_eth_fpa_aura_init(oen->pki_packet_pool, aura,
 				  num_packet_buffers * 2);
 	aura2buffers_needed[priv->pki_laura] = &priv->buffers_needed;
 
@@ -1201,7 +1210,7 @@ static int octeon3_eth_ndo_init(struct net_device *netdev)
 	prt_schd->style = -1; /* Allocate net style per port */
 	prt_schd->qpg_base = -1;
 	prt_schd->aura_per_prt = true;
-	prt_schd->aura = priv->pki_laura;
+	prt_schd->aura_num = priv->pki_laura;
 	prt_schd->sso_grp_per_prt = true;
 	prt_schd->sso_grp = base_rx_grp;
 	prt_schd->qpg_qos = CVMX_PKI_QPG_QOS_NONE;
@@ -1267,7 +1276,7 @@ static int octeon3_eth_ndo_init(struct net_device *netdev)
 	pki_prt_cfg.style_cfg.tag_cfg.tag_fields.layer_f_src = 1;
 	pki_prt_cfg.style_cfg.tag_cfg.tag_fields.layer_g_src = 1;
 
-	cvmx_pki_config_port(ipd_port, &pki_prt_cfg);
+	cvmx_pki_set_port_config(ipd_port, &pki_prt_cfg);
 
 	i = 0;
 	while ((priv->num_rx_cxt & (1 << i)) == 0)
@@ -1421,7 +1430,7 @@ static int octeon3_eth_ndo_stop(struct net_device *netdev)
 
 	/* Free the packet buffers */
 	for (;;) {
-		w = cvmx_fpa3_alloc_aura(priv->numa_node, priv->pki_laura);
+		w = cvmx_fpa3_alloc(__cvmx_fpa3_gaura(priv->numa_node, priv->pki_laura));
 		if (!w)
 			break;
 		skb = w[0];
-- 
1.8.2.1

