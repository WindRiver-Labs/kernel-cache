From 89097e52cb238c00c4a1aaeb00ed50b27851646b Mon Sep 17 00:00:00 2001
From: Chandrakala Chavva <cchavva@caviumnetworks.com>
Date: Fri, 24 Apr 2015 09:59:48 +0530
Subject: [PATCH 106/132] MIPS:OCTEON: Sync-up SE files.

Signed-off-by: Chandrakala Chavva <cchavva@caviumnetworks.com>
Signed-off-by: Abhishek Paliwal <abhishekpaliwal@aricent.com>
[Original patch taken from OCTEON-SDK 3.1.1-544.]
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 arch/mips/cavium-octeon/executive/cvmx-bootmem.c   |   6 +-
 arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c | 274 ++++++++++---
 .../mips/cavium-octeon/executive/cvmx-dma-engine.c |   6 +-
 arch/mips/cavium-octeon/executive/cvmx-ipd.c       |   4 +-
 arch/mips/cavium-octeon/executive/cvmx-l2c.c       |  26 +-
 arch/mips/cavium-octeon/executive/cvmx-pko3.c      |   6 +-
 arch/mips/include/asm/octeon/cvmx-app-init.h       |   4 +-
 arch/mips/include/asm/octeon/cvmx-bootmem.h        |   5 +-
 arch/mips/include/asm/octeon/cvmx-cmd-queue.h      | 436 ++++++++++-----------
 arch/mips/include/asm/octeon/cvmx-fpa.h            |   4 +-
 arch/mips/include/asm/octeon/cvmx-pko3.h           |  10 +-
 11 files changed, 468 insertions(+), 313 deletions(-)

diff --git a/arch/mips/cavium-octeon/executive/cvmx-bootmem.c b/arch/mips/cavium-octeon/executive/cvmx-bootmem.c
index 46ee518..a99b7e4 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-bootmem.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-bootmem.c
@@ -43,7 +43,7 @@
  * Simple allocate only memory allocator.  Used to allocate memory at
  * application start time.
  *
- * <hr>$Revision: 106502 $<hr>
+ * <hr>$Revision: 113619 $<hr>
  *
  */
 
@@ -591,8 +591,12 @@ void *cvmx_bootmem_alloc_named_range_once(uint64_t size, uint64_t min_addr,
 		return NULL;
 	}
 	ptr = cvmx_phys_to_ptr(addr);
+
 	if (init)
 		init(ptr);
+	else
+		memset(ptr, 0, size);
+
 	__cvmx_bootmem_unlock(0);
 	return ptr;
 }
diff --git a/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c b/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c
index 7170672..5089e87 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c
@@ -43,7 +43,7 @@
  * Support functions for managing command queues used for
  * various hardware blocks.
  *
- * <hr>$Revision: 106617 $<hr>
+ * <hr>$Revision: 114199 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <linux/export.h>
@@ -66,8 +66,8 @@
  * This application uses this pointer to access the global queue
  * state. It points to a bootmem named block.
  */
-CVMX_SHARED __cvmx_cmd_queue_all_state_t *__cvmx_cmd_queue_state_ptr;
-EXPORT_SYMBOL(__cvmx_cmd_queue_state_ptr);
+CVMX_SHARED __cvmx_cmd_queue_all_state_t *__cvmx_cmd_queue_state_ptrs[CVMX_MAX_NODES];
+EXPORT_SYMBOL(__cvmx_cmd_queue_state_ptrs);
 
 /**
  * @INTERNAL
@@ -75,42 +75,66 @@ EXPORT_SYMBOL(__cvmx_cmd_queue_state_ptr);
  *
  * @return CVMX_CMD_QUEUE_SUCCESS or a failure code
  */
-static cvmx_cmd_queue_result_t __cvmx_cmd_queue_init_state_ptr(void)
+cvmx_cmd_queue_result_t __cvmx_cmd_queue_init_state_ptr(unsigned node)
 {
-	char *alloc_name = "cvmx_cmd_queues";
+	char *alloc_name = "cvmx_cmd_queues\0\0";
+	char s[4] = "_0";
+	const cvmx_bootmem_named_block_desc_t *block_desc = NULL;
+	unsigned size;
+	uint64_t paddr_min = 0, paddr_max = 0;
+	void *ptr;
+
 #if defined(CONFIG_CAVIUM_RESERVE32) && CONFIG_CAVIUM_RESERVE32
-	extern uint64_t octeon_reserve32_memory;
 #endif
 
-	if (cvmx_likely(__cvmx_cmd_queue_state_ptr))
+	if (cvmx_likely(__cvmx_cmd_queue_state_ptrs[node]))
+		return CVMX_CMD_QUEUE_SUCCESS;
+
+	/* Add node# to block name */
+	if (node > 0) {
+		s[1] += node;
+		strcat(alloc_name, s);
+	}
+
+	/* Find the named block in case it has been created already */
+	block_desc = cvmx_bootmem_find_named_block(alloc_name);
+	if (block_desc) {
+		__cvmx_cmd_queue_state_ptrs[node] =
+			cvmx_phys_to_ptr(block_desc->base_addr);
 		return CVMX_CMD_QUEUE_SUCCESS;
+	}
 
+	size = sizeof(*__cvmx_cmd_queue_state_ptrs[node]);
+
+	/* Rest f the code is to allocate a new named block */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #if defined(CONFIG_CAVIUM_RESERVE32) && CONFIG_CAVIUM_RESERVE32
-	if (octeon_reserve32_memory)
-		__cvmx_cmd_queue_state_ptr = cvmx_bootmem_alloc_named_range(sizeof(*__cvmx_cmd_queue_state_ptr),
-									    octeon_reserve32_memory,
-									    octeon_reserve32_memory + (CONFIG_CAVIUM_RESERVE32 << 20) - 1,
-									    128, alloc_name);
-	else
+	{
+		/* Special address range for SE-UM apps in 32-bit mode */
+		extern uint64_t octeon_reserve32_memory;
+		if (octeon_reserve32_memory) {
+			paddr_min = octeon_reserve32_memory;
+			paddr_max = octeon_reserve32_memory +
+				(CONFIG_CAVIUM_RESERVE32 << 20) - 1;
+		}
+	}
 #endif
-		__cvmx_cmd_queue_state_ptr = cvmx_bootmem_alloc_named(sizeof(*__cvmx_cmd_queue_state_ptr), 128, alloc_name);
-#else
-	__cvmx_cmd_queue_state_ptr = cvmx_bootmem_alloc_named(sizeof(*__cvmx_cmd_queue_state_ptr), 128, alloc_name);
 #endif
-	if (__cvmx_cmd_queue_state_ptr)
-		memset(__cvmx_cmd_queue_state_ptr, 0, sizeof(*__cvmx_cmd_queue_state_ptr));
-	else {
-		const cvmx_bootmem_named_block_desc_t *block_desc = cvmx_bootmem_find_named_block(alloc_name);
-		if (block_desc)
-			__cvmx_cmd_queue_state_ptr = cvmx_phys_to_ptr(block_desc->base_addr);
-		else {
-			cvmx_dprintf("ERROR: cvmx_cmd_queue_initialize: Unable to get named block %s.\n", alloc_name);
-			return CVMX_CMD_QUEUE_NO_MEMORY;
-		}
+
+	/* Atomically allocate named block once, and zero it by default */
+	ptr = cvmx_bootmem_alloc_named_range_once(size, paddr_min, paddr_max, 
+			128, alloc_name, NULL);
+
+	if (ptr != NULL) {
+		__cvmx_cmd_queue_state_ptrs[node] = ptr;
+	} else {
+		cvmx_dprintf("ERROR: %s: Unable to get named block %s.\n", 
+			__func__, alloc_name);
+		return CVMX_CMD_QUEUE_NO_MEMORY;
 	}
 	return CVMX_CMD_QUEUE_SUCCESS;
 }
+EXPORT_SYMBOL(__cvmx_cmd_queue_init_state_ptr);
 
 /**
  * Initialize a command queue for use. The initial FPA buffer is
@@ -129,7 +153,21 @@ cvmx_cmd_queue_result_t cvmx_cmd_queue_initialize(cvmx_cmd_queue_id_t queue_id,
 						  int pool_size)
 {
 	__cvmx_cmd_queue_state_t *qstate;
-	cvmx_cmd_queue_result_t result = __cvmx_cmd_queue_init_state_ptr();
+	cvmx_cmd_queue_result_t result;
+	unsigned node;
+	unsigned index;
+	int fpa_pool_min, fpa_pool_max;
+
+	node = __cvmx_cmd_queue_get_node(queue_id);
+
+	index = __cvmx_cmd_queue_get_index(queue_id);
+	if (index >= NUM_ELEMENTS(__cvmx_cmd_queue_state_ptrs[node]->state)){
+		cvmx_printf("ERROR: %s: queue %#x out of range\n",
+			__func__, queue_id);
+		return CVMX_CMD_QUEUE_INVALID_PARAM;
+	}
+
+	result = __cvmx_cmd_queue_init_state_ptr(node);
 	if (result != CVMX_CMD_QUEUE_SUCCESS)
 		return result;
 
@@ -147,29 +185,45 @@ cvmx_cmd_queue_result_t cvmx_cmd_queue_initialize(cvmx_cmd_queue_id_t queue_id,
 	} else if (max_depth != 0)
 		return CVMX_CMD_QUEUE_INVALID_PARAM;
 
-	if ((fpa_pool < 0) || (fpa_pool >= CVMX_FPA_NUM_POOLS))
+	/* CVMX_FPA_NUM_POOLS maps to cvmx_fpa3_num_auras for FPA3 */
+	fpa_pool_min = node << 10;
+	fpa_pool_max = fpa_pool_min + CVMX_FPA_NUM_POOLS;
+
+	if ((fpa_pool < fpa_pool_min) || (fpa_pool >= fpa_pool_max))
 		return CVMX_CMD_QUEUE_INVALID_PARAM;
-	if ((pool_size < 128) || (pool_size > 65536))
+
+	if ((pool_size < 128) || (pool_size > (1<<17)))
 		return CVMX_CMD_QUEUE_INVALID_PARAM;
 
+	if (pool_size & 3)
+		cvmx_dprintf("WARNING: %s: pool_size %d not multiple of 8\n",
+			__func__, pool_size);
+
 	/* See if someone else has already initialized the queue */
-	if (qstate->base_ptr_div128) {
-		if (max_depth != (int)qstate->max_depth) {
-			cvmx_dprintf("ERROR: cvmx_cmd_queue_initialize: Queue already initialized with different max_depth (%d).\n",
-				     (int)qstate->max_depth);
+	if (qstate->base_paddr) {
+		int depth;
+		static const char emsg[] = /* Common error message part */
+			"Queue already initialized with different ";
+
+		depth = (max_depth + qstate->pool_size_m1 - 1) /
+			qstate->pool_size_m1;
+		if (depth != qstate->max_depth) {
+			depth = qstate->max_depth * qstate->pool_size_m1;
+			cvmx_dprintf("ERROR: %s: %s max_depth (%d).\n",
+				__func__, emsg, depth);
 			return CVMX_CMD_QUEUE_INVALID_PARAM;
 		}
 		if (fpa_pool != qstate->fpa_pool) {
-			cvmx_dprintf("ERROR: cvmx_cmd_queue_initialize: Queue already initialized with different FPA pool (%u).\n",
-				     qstate->fpa_pool);
+			cvmx_dprintf("ERROR: %s: %s FPA pool (%u).\n",
+				__func__, emsg, qstate->fpa_pool);
 			return CVMX_CMD_QUEUE_INVALID_PARAM;
 		}
 		if ((pool_size >> 3) - 1 != qstate->pool_size_m1) {
-			cvmx_dprintf("ERROR: cvmx_cmd_queue_initialize: Queue already initialized with different FPA pool size (%u).\n",
-				     (qstate->pool_size_m1 + 1) << 3);
+			cvmx_dprintf("ERROR: %s: %s FPA pool size (%u).\n",
+				__func__, emsg,
+				(qstate->pool_size_m1 + 1) << 3);
 			return CVMX_CMD_QUEUE_INVALID_PARAM;
 		}
-		CVMX_SYNCWS;
 		return CVMX_CMD_QUEUE_ALREADY_SETUP;
 	} else {
 		union cvmx_fpa_ctl_status status;
@@ -178,31 +232,31 @@ cvmx_cmd_queue_result_t cvmx_cmd_queue_initialize(cvmx_cmd_queue_id_t queue_id,
 		if (!(octeon_has_feature(OCTEON_FEATURE_FPA3))) {
 			status.u64 = cvmx_read_csr(CVMX_FPA_CTL_STATUS);
 			if (!status.s.enb) {
-				cvmx_dprintf("ERROR: cvmx_cmd_queue_initialize:"
-					     " FPA is not enabled.\n");
+				cvmx_dprintf("ERROR: %s: FPA is not enabled.\n",
+					__func__);
 				return CVMX_CMD_QUEUE_NO_MEMORY;
 			}
 		}
-		buffer = __cvmx_cmd_queue_alloc_buffer(fpa_pool);
+		buffer = cvmx_fpa_alloc(fpa_pool);
 		if (buffer == NULL) {
-			cvmx_dprintf("ERROR: cvmx_cmd_queue_initialize: Unable to allocate initial buffer.\n");
+			cvmx_dprintf("ERROR: %s: allocating first buffer.\n",
+				__func__);
 			return CVMX_CMD_QUEUE_NO_MEMORY;
 		}
 
-		memset(qstate, 0, sizeof(*qstate));
-		qstate->max_depth = max_depth;
+		index = (pool_size >> 3) - 1;
+		qstate->pool_size_m1 = index;
+		qstate->max_depth = (max_depth + index -1) / index;
+		qstate->index = 0;
 		qstate->fpa_pool = fpa_pool;
-		qstate->pool_size_m1 = (pool_size >> 3) - 1;
-		qstate->base_ptr_div128 = cvmx_ptr_to_phys(buffer) / 128;
-		/*
-		 * We zeroed the now serving field so we need to also
-		 * zero the ticket.
-		 */
-		__cvmx_cmd_queue_state_ptr->ticket[__cvmx_cmd_queue_get_index(queue_id)] = 0;
-		CVMX_SYNCWS;
+		qstate->base_paddr = cvmx_ptr_to_phys(buffer);
+
+		/* Initialize lock */
+		__cvmx_cmd_queue_lock_init(queue_id);
 		return CVMX_CMD_QUEUE_SUCCESS;
 	}
 }
+
 EXPORT_SYMBOL(cvmx_cmd_queue_initialize);
 
 /**
@@ -217,6 +271,8 @@ EXPORT_SYMBOL(cvmx_cmd_queue_initialize);
 cvmx_cmd_queue_result_t cvmx_cmd_queue_shutdown(cvmx_cmd_queue_id_t queue_id)
 {
 	__cvmx_cmd_queue_state_t *qptr = __cvmx_cmd_queue_get_state(queue_id);
+
+	/* FIXME: This will not complain if the queue was never initialized */
 	if (qptr == NULL) {
 		cvmx_dprintf("ERROR: cvmx_cmd_queue_shutdown: Unable to get queue information.\n");
 		return CVMX_CMD_QUEUE_INVALID_PARAM;
@@ -227,14 +283,14 @@ cvmx_cmd_queue_result_t cvmx_cmd_queue_shutdown(cvmx_cmd_queue_id_t queue_id)
 		return CVMX_CMD_QUEUE_FULL;
 	}
 
-	__cvmx_cmd_queue_lock(queue_id, qptr);
-	if (qptr->base_ptr_div128) {
+	__cvmx_cmd_queue_lock(queue_id);
+	if (qptr->base_paddr) {
 		cvmx_fpa_free(cvmx_phys_to_ptr(
-				       (uint64_t) qptr->base_ptr_div128 << 7),
+				       (uint64_t) qptr->base_paddr),
 				       qptr->fpa_pool, 0);
-		qptr->base_ptr_div128 = 0;
+		qptr->base_paddr = 0;
 	}
-	__cvmx_cmd_queue_unlock(qptr);
+	__cvmx_cmd_queue_unlock(queue_id);
 
 	return CVMX_CMD_QUEUE_SUCCESS;
 }
@@ -318,9 +374,109 @@ int cvmx_cmd_queue_length(cvmx_cmd_queue_id_t queue_id)
 void *cvmx_cmd_queue_buffer(cvmx_cmd_queue_id_t queue_id)
 {
 	__cvmx_cmd_queue_state_t *qptr = __cvmx_cmd_queue_get_state(queue_id);
-	if (qptr && qptr->base_ptr_div128)
-		return cvmx_phys_to_ptr((uint64_t) qptr->base_ptr_div128 << 7);
+	if (qptr && qptr->base_paddr)
+		return cvmx_phys_to_ptr((uint64_t) qptr->base_paddr);
 	else
 		return NULL;
 }
 EXPORT_SYMBOL(cvmx_cmd_queue_buffer);
+
+static uint64_t *__cvmx_cmd_queue_add_blk(__cvmx_cmd_queue_state_t *qptr)
+{
+	uint64_t *cmd_ptr;
+	uint64_t *new_buffer;
+	uint64_t new_paddr;
+
+	/* Get base vaddr of current (full) block */
+	cmd_ptr = cvmx_phys_to_ptr((uint64_t) qptr->base_paddr);
+
+	/* Allocate a new block from the per-queue pool */
+	new_buffer = cvmx_fpa_alloc(qptr->fpa_pool);
+
+	/* Check for allocation failure */
+	if (cvmx_unlikely(new_buffer == NULL))
+		return NULL;
+
+	/* Zero out the new block link pointer,
+	 * in case this block will be filled to the rim
+	 */
+	new_buffer[ qptr->pool_size_m1 ] = ~0ull;
+
+	/* Get physical address of the new buffer */
+	new_paddr = cvmx_ptr_to_phys(new_buffer);
+
+	/* Store the physical link address at the end of current full block */
+	cmd_ptr[ qptr->pool_size_m1] = new_paddr;
+
+	/* Store the physical address in the queue state structure */
+	qptr->base_paddr = new_paddr;
+	qptr->index = 0;
+
+	/* Return the virtual base of the new block */
+	return new_buffer;
+}
+
+/**
+ * @INTERNAL
+ * Add command words into a queue, handles all the corener cases
+ * where only some of the words might fit into the current block,
+ * and a new block may need to be allocated.
+ * Locking and argument checks are done in the front-end in-line
+ * functions that call this one for the rare corner cases.
+ */
+cvmx_cmd_queue_result_t
+__cvmx_cmd_queue_write_raw(cvmx_cmd_queue_id_t queue_id,
+	__cvmx_cmd_queue_state_t *qptr,
+	int cmd_count, const uint64_t *cmds)
+{
+	uint64_t *cmd_ptr;
+	unsigned index;
+
+	cmd_ptr = cvmx_phys_to_ptr((uint64_t) qptr->base_paddr);
+	index = qptr->index;
+
+	/* Enforce queue depth limit, if enabled, once per block */
+	if (CVMX_CMD_QUEUE_ENABLE_MAX_DEPTH &&
+	    cvmx_unlikely(qptr->max_depth)) {
+		unsigned depth = cvmx_cmd_queue_length(queue_id);
+		depth /= qptr->pool_size_m1;
+
+		if (cvmx_unlikely(depth > qptr->max_depth)) {
+			return CVMX_CMD_QUEUE_FULL;
+		}
+	}
+
+	/*
+	 * If the block allocation fails, even the words that we wrote
+	 * to the current block will not count because the 'index' will
+	 * not be comitted.
+	 * The loop is run 'count + 1' times to take care of the tail
+	 * case, where the buffer is full to the rim, so the link
+	 * pointer must be filled with a valid address.
+	 */
+	while (cmd_count >= 0) {
+		if (index >= qptr->pool_size_m1) {
+			/* Block is full, get another one and proceed */
+			cmd_ptr = __cvmx_cmd_queue_add_blk(qptr);
+
+			/* Baul on allocation error w/o comitting anything */
+			if (cvmx_unlikely(cmd_ptr == NULL))
+				return CVMX_CMD_QUEUE_NO_MEMORY;
+
+			/* Reset index for start of new block */
+			index = 0;
+		}
+		/* Exit Loop on 'count + 1' iterations */
+		if (cmd_count <= 0)
+			break;
+		/* Store commands into queue block while there is space */
+		cmd_ptr[ index ++ ] = *cmds++;
+		cmd_count --;
+	} /* while cmd_count */
+
+	/* Commit added words if all is well */
+	qptr->index = index;
+
+	return CVMX_CMD_QUEUE_SUCCESS;
+}
+EXPORT_SYMBOL(__cvmx_cmd_queue_write_raw);
diff --git a/arch/mips/cavium-octeon/executive/cvmx-dma-engine.c b/arch/mips/cavium-octeon/executive/cvmx-dma-engine.c
index dfcc12a..f4751f5 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-dma-engine.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-dma-engine.c
@@ -43,7 +43,7 @@
  * Interface to the PCI / PCIe DMA engines. These are only avialable
  * on chips with PCI / PCIe.
  *
- * <hr>$Revision: 103836 $<hr>
+ * <hr>$Revision: 113619 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <linux/export.h>
@@ -333,7 +333,7 @@ int cvmx_dma_engine_submit(int engine, cvmx_dma_engine_header_t header, int num_
 	   ring the doorbell for the DMA engines. This prevents doorbells from
 	   possibly arriving out of order with respect to the command queue
 	   entries */
-	__cvmx_cmd_queue_lock(CVMX_CMD_QUEUE_DMA(engine), __cvmx_cmd_queue_get_state(CVMX_CMD_QUEUE_DMA(engine)));
+	__cvmx_cmd_queue_lock(CVMX_CMD_QUEUE_DMA(engine));
 	result = cvmx_cmd_queue_write(CVMX_CMD_QUEUE_DMA(engine), 0, cmd_count, cmds);
 	/* This SYNCWS is needed since the command queue didn't do locking, which
 	   normally implies the SYNCWS. This one makes sure the command queue
@@ -354,7 +354,7 @@ int cvmx_dma_engine_submit(int engine, cvmx_dma_engine_header_t header, int num_
 		}
 	}
 	/* Here is the unlock for the above errata workaround */
-	__cvmx_cmd_queue_unlock(__cvmx_cmd_queue_get_state(CVMX_CMD_QUEUE_DMA(engine)));
+	__cvmx_cmd_queue_unlock((CVMX_CMD_QUEUE_DMA(engine)));
 	return result;
 }
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-ipd.c b/arch/mips/cavium-octeon/executive/cvmx-ipd.c
index 71cf0f6..7fe8e4b 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-ipd.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-ipd.c
@@ -106,9 +106,9 @@ void cvmx_ipd_convert_to_newcfg(cvmx_ipd_config_t ipd_config)
 			ipd_config.port_config.tag_fields.ipv4_dst_ip;
 	pki_dflt_style[node].tag_cfg.tag_fields.ip_prot_nexthdr = ipd_config.port_config.tag_fields.ipv6_next_header |
 			ipd_config.port_config.tag_fields.ipv4_protocol;
-	pki_dflt_style[node].tag_cfg.tag_fields.layer_d_src = ipd_config.port_config.tag_fields.ipv6_src_port |
+	pki_dflt_style[node].tag_cfg.tag_fields.layer_f_src = ipd_config.port_config.tag_fields.ipv6_src_port |
 			ipd_config.port_config.tag_fields.ipv4_src_port;
-	pki_dflt_style[node].tag_cfg.tag_fields.layer_d_dst = ipd_config.port_config.tag_fields.ipv6_dst_port |
+	pki_dflt_style[node].tag_cfg.tag_fields.layer_f_dst = ipd_config.port_config.tag_fields.ipv6_dst_port |
 			ipd_config.port_config.tag_fields.ipv4_dst_port;
 	pki_dflt_style[node].tag_cfg.tag_fields.input_port = ipd_config.port_config.tag_fields.input_port;
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-l2c.c b/arch/mips/cavium-octeon/executive/cvmx-l2c.c
index fa653df..762bf17 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-l2c.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-l2c.c
@@ -43,7 +43,7 @@
  * Implementation of the Level 2 Cache (L2C) control,
  * measurement, and debugging facilities.
  *
- * <hr>$Revision: 106657 $<hr>
+ * <hr>$Revision: 113624 $<hr>
  *
  */
 
@@ -114,13 +114,15 @@ int cvmx_l2c_set_core_way_partition(uint32_t core, uint32_t mask)
 	uint32_t valid_mask;
 	int node = cvmx_get_node_num();
 
-	valid_mask = (0x1 << cvmx_l2c_get_num_assoc()) - 1;
+	if (OCTEON_IS_OCTEON1PLUS()) {
+		valid_mask = (0x1 << cvmx_l2c_get_num_assoc()) - 1;
 
-	mask &= valid_mask;
+		mask &= valid_mask;
 
-	/* A UMSK setting which blocks all L2C Ways is an error on some chips */
-	if (mask == valid_mask && (OCTEON_IS_OCTEON1PLUS()))
-		return -1;
+		/* A UMSK setting which blocks all L2C Ways is an error on some chips */
+		if (mask == valid_mask)
+			return -1;
+	}
 
 	/* Validate the core number */
 	if (core >= cvmx_octeon_num_cores())
@@ -164,12 +166,14 @@ int cvmx_l2c_set_hw_way_partition(uint32_t mask)
 	uint32_t valid_mask;
 	int node = cvmx_get_node_num();
 
-	valid_mask = (0x1 << cvmx_l2c_get_num_assoc()) - 1;
-	mask &= valid_mask;
+	if (OCTEON_IS_OCTEON1PLUS()) {
+		valid_mask = (0x1 << cvmx_l2c_get_num_assoc()) - 1;
+		mask &= 0xffff;
 
-	/* A UMSK setting which blocks all L2C Ways is an error on some chips */
-	if (mask == valid_mask && OCTEON_IS_OCTEON1PLUS())
-		return -1;
+		/* A UMSK setting which blocks all L2C Ways is an error on some chips */
+		if (mask == valid_mask)
+			return -1;
+	}
 
 	if (OCTEON_IS_OCTEON2() || OCTEON_IS_OCTEON3())
 		cvmx_write_csr_node(node, CVMX_L2C_WPAR_IOBX(0), mask);
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pko3.c b/arch/mips/cavium-octeon/executive/cvmx-pko3.c
index fa11065..a8a3091 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pko3.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pko3.c
@@ -707,7 +707,6 @@ static int cvmx_pko_setup_macs(int node)
 			pko_mac_cfg.u64);
 	}
 
-
 	/* Setup PKO MCI0/MCI1/SKID credits */
 	for(mac_num = 0; mac_num < CVMX_PKO_MAX_MACS; mac_num++) {
 		cvmx_pko_mci0_max_credx_t pko_mci0_max_cred;
@@ -767,7 +766,10 @@ static int cvmx_pko_setup_macs(int node)
 			pko_mci0_max_cred.s.max_cred_lim = 0xfff;
 		}
 
-		cvmx_write_csr_node(node, CVMX_PKO_MCI0_MAX_CREDX(mac_num),
+		/* Pass 2 PKO hardware does not use the MCI0 credits */
+		if(OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X))
+			cvmx_write_csr_node(node,
+					CVMX_PKO_MCI0_MAX_CREDX(mac_num),
 					pko_mci0_max_cred.u64);
 
 		/* The original CSR formula is the correct one after all */
diff --git a/arch/mips/include/asm/octeon/cvmx-app-init.h b/arch/mips/include/asm/octeon/cvmx-app-init.h
index 25f04ff..1e872b3 100644
--- a/arch/mips/include/asm/octeon/cvmx-app-init.h
+++ b/arch/mips/include/asm/octeon/cvmx-app-init.h
@@ -41,7 +41,7 @@
  * @file
  * Header file for simple executive application initialization.  This defines
  * part of the ABI between the bootloader and the application.
- * <hr>$Revision: 105233 $<hr>
+ * <hr>$Revision: 113622 $<hr>
  *
  */
 
@@ -288,6 +288,7 @@ enum cvmx_board_types_enum {
 	CVMX_BOARD_TYPE_EBB7800_CFG0 = 70, /* Only required to support cn78xx p1.0 */
 	CVMX_BOARD_TYPE_EBB7804_CFG0 = 71, /* Only required to support cn78xx p1.0 */
 	CVMX_BOARD_TYPE_SWORDFISH = 72,
+	CVMX_BOARD_TYPE_SFF7800 = 73,   /* Embedded Planet board */
 	CVMX_BOARD_TYPE_MAX,
 	/* NOTE:  256-257 are being used by a customer. */
 
@@ -429,6 +430,7 @@ static inline const char *cvmx_board_type_to_string(enum cvmx_board_types_enum t
 		ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_EBB7800_CFG0)
 		ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_EBB7804_CFG0)
 		ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_SWORDFISH)
+		ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_SFF7800)
 		ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_MAX)
 
 		/* Customer boards listed here */
diff --git a/arch/mips/include/asm/octeon/cvmx-bootmem.h b/arch/mips/include/asm/octeon/cvmx-bootmem.h
index ab3374a..d10baf8 100644
--- a/arch/mips/include/asm/octeon/cvmx-bootmem.h
+++ b/arch/mips/include/asm/octeon/cvmx-bootmem.h
@@ -42,7 +42,7 @@
  * Simple allocate only memory allocator.  Used to allocate memory at application
  * start time.
  *
- * <hr>$Revision: 96253 $<hr>
+ * <hr>$Revision: 113619 $<hr>
  *
  */
 
@@ -329,6 +329,9 @@ extern void *cvmx_bootmem_alloc_named_range(uint64_t size, uint64_t min_addr,
  * @param name   name of block - must be less than CVMX_BOOTMEM_NAME_LEN bytes
  * @param init   Initialization function
  *
+ * The initialization function is optional, if omitted the named block
+ * is initialized to all zeros when it is created, i.e. once.
+ *
  * @return pointer to block of memory, NULL on error
  */
 void *cvmx_bootmem_alloc_named_range_once(uint64_t size,
diff --git a/arch/mips/include/asm/octeon/cvmx-cmd-queue.h b/arch/mips/include/asm/octeon/cvmx-cmd-queue.h
index 657e1c6..1d5b8ab 100644
--- a/arch/mips/include/asm/octeon/cvmx-cmd-queue.h
+++ b/arch/mips/include/asm/octeon/cvmx-cmd-queue.h
@@ -82,12 +82,14 @@
  * internal cycle counter to completely eliminate any causes of
  * bus traffic.
  *
- * <hr> $Revision: 103822 $ <hr>
+ * <hr> $Revision: 113619 $ <hr>
  */
 
 #ifndef __CVMX_CMD_QUEUE_H__
 #define __CVMX_CMD_QUEUE_H__
 
+#include "cvmx-atomic.h"
+
 #ifdef	__cplusplus
 /* *INDENT-OFF* */
 extern "C" {
@@ -103,6 +105,8 @@ extern "C" {
 #define CVMX_CMD_QUEUE_ENABLE_MAX_DEPTH 0
 #endif
 
+#define	NUM_ELEMENTS(arr) (sizeof(arr)/sizeof((arr)[0]))
+
 /**
  * Enumeration representing all hardware blocks that use command
  * queues. Each hardware block has up to 65536 sub identifiers for
@@ -124,6 +128,9 @@ typedef enum {
 	CVMX_CMD_QUEUE_END = 0x70000,
 } cvmx_cmd_queue_id_t;
 
+#define CVMX_CMD_QUEUE_ZIP3_QUE(node,queue) \
+  ((cvmx_cmd_queue_id_t)((node) << 24 | CVMX_CMD_QUEUE_ZIP | (0xffff&(queue))))
+
 /**
  * Command write operations can fail if the command queue needs
  * a new buffer and the associated FPA pool is empty. It can also
@@ -140,29 +147,37 @@ typedef enum {
 
 typedef struct {
 #ifdef __BIG_ENDIAN_BITFIELD
-	uint32_t now_serving;	    /**< You have lock when this is your ticket */
-	uint32_t max_depth;	    /**< Maximum outstanding command words */
-
-	uint64_t fpa_pool:3;	    /**< FPA pool buffers come from */
-	uint64_t base_ptr_div128:33;
-				    /**< Top of command buffer pointer shifted 7 */
-	uint64_t unused2:2;
-	uint64_t pool_size_m1:13;
-				    /**< FPA buffer size in 64bit words minus 1 */
-	uint64_t index:13;	    /**< Number of commands already used in buffer */
+	/* First 64-bit word: */
+	uint64_t fpa_pool:16;		/**< FPA1:POOL/FPA3:GAURA for buffers */
+	uint64_t base_paddr:48;		/**< command buffer physical address */
+	/* Second 64-bit word: */
+	int32_t index;			/**< Number of cmd words in buffer */
+	uint16_t max_depth;		/**< Maximum outstanding blocks */
+	uint16_t pool_size_m1;		/**< FPA buffer size in dwords - 1 */
 #else
-	uint32_t max_depth;
-	uint32_t now_serving;
-
-	uint64_t index:13;
-	uint64_t pool_size_m1:13;
-	uint64_t unused2:2;
-	uint64_t base_ptr_div128:33;
-	uint64_t fpa_pool:3;
+	/* First 64-bit word: */
+	uint64_t base_paddr:48;
+	uint64_t fpa_pool:16;
+	/* Second 64-bit word: */
+	uint16_t pool_size_m1;
+	uint16_t max_depth;
+	int32_t index;
 #endif
 } __cvmx_cmd_queue_state_t;
 
 /**
+ * command-queue locking uses a fair ticket spinlock algo,
+ * with 64-bit tickets for endianness-neutrality and
+ * counter overflow protection.
+ * Lock is free when both counters are of equal value.
+ */
+typedef struct {
+	uint64_t ticket;
+	uint64_t now_serving;
+} __cvmx_cmd_queue_lock_t;
+
+/**
+ * @INTERNAL
  * This structure contains the global state of all command queues.
  * It is stored in a bootmem named block and shared by all
  * applications running on Octeon. Tickets are stored in a different
@@ -171,10 +186,25 @@ typedef struct {
  * of queue state causes the ll/sc to fail quite often.
  */
 typedef struct {
-	uint64_t ticket[(CVMX_CMD_QUEUE_END >> 16) * 256];
+	__cvmx_cmd_queue_lock_t lock[(CVMX_CMD_QUEUE_END >> 16) * 256];
 	__cvmx_cmd_queue_state_t state[(CVMX_CMD_QUEUE_END >> 16) * 256];
 } __cvmx_cmd_queue_all_state_t;
 
+extern CVMX_SHARED __cvmx_cmd_queue_all_state_t *
+__cvmx_cmd_queue_state_ptrs[CVMX_MAX_NODES];
+
+/**
+ * @INTERNAL
+ * Internal function to handle the corner cases
+ * of adding command words to a queue when the current
+ * block is getting full.
+ */
+extern cvmx_cmd_queue_result_t
+		__cvmx_cmd_queue_write_raw(cvmx_cmd_queue_id_t queue_id,
+			__cvmx_cmd_queue_state_t *qptr,
+			int cmd_count, const uint64_t *cmds);
+
+
 /**
  * Initialize a command queue for use. The initial FPA buffer is
  * allocated and the hardware unit is configured to point to the
@@ -226,13 +256,19 @@ void *cvmx_cmd_queue_buffer(cvmx_cmd_queue_id_t queue_id);
 
 /**
  * @INTERNAL
+ * Retreive or allocate command queue state named block
+ */
+extern cvmx_cmd_queue_result_t __cvmx_cmd_queue_init_state_ptr(unsigned node);
+
+/**
+ * @INTERNAL
  * Get the index into the state arrays for the supplied queue id.
  *
  * @param queue_id Queue ID to get an index for
  *
  * @return Index into the state arrays
  */
-static inline int __cvmx_cmd_queue_get_index(cvmx_cmd_queue_id_t queue_id)
+static inline unsigned __cvmx_cmd_queue_get_index(cvmx_cmd_queue_id_t queue_id)
 {
 	/* Warning: This code currently only works with devices that have 256
 	 * queues or less.  Devices with more than 16 queues are laid out in
@@ -240,10 +276,16 @@ static inline int __cvmx_cmd_queue_get_index(cvmx_cmd_queue_id_t queue_id)
 	 * cache thrashing when you are running 16 queues per port to support
 	 * lockless operation
 	 */
-	int unit = queue_id >> 16;
-	int q = (queue_id >> 4) & 0xf;
-	int core = queue_id & 0xf;
-	return unit * 256 + core * 16 + q;
+	unsigned unit = (queue_id >> 16) & 0xff;
+	unsigned q = (queue_id >> 4) & 0xf;
+	unsigned core = queue_id & 0xf;
+	return (unit << 8) | (core << 4) | q;
+}
+
+static inline int __cvmx_cmd_queue_get_node(cvmx_cmd_queue_id_t queue_id)
+{
+	unsigned node = queue_id >> 24;
+	return node;
 }
 
 /**
@@ -254,41 +296,39 @@ static inline int __cvmx_cmd_queue_get_index(cvmx_cmd_queue_id_t queue_id)
  * @param queue_id Queue ID to lock
  * @param qptr     Pointer to the queue's global state
  */
-static inline void __cvmx_cmd_queue_lock(cvmx_cmd_queue_id_t queue_id,
-					 __cvmx_cmd_queue_state_t *qptr)
+static inline void __cvmx_cmd_queue_lock(cvmx_cmd_queue_id_t queue_id)
 {
 #ifndef __U_BOOT__
-	extern CVMX_SHARED __cvmx_cmd_queue_all_state_t *__cvmx_cmd_queue_state_ptr;
-	int tmp;
-	int my_ticket;
-	CVMX_PREFETCH(qptr, 0);
+	__cvmx_cmd_queue_lock_t *lock_ptr;
+	unsigned node;
+	uint64_t tmp;
+	uint64_t my_ticket;
+
+	tmp = __cvmx_cmd_queue_get_index(queue_id);
+	node = __cvmx_cmd_queue_get_node(queue_id);
+	lock_ptr = &__cvmx_cmd_queue_state_ptrs[node]->lock[tmp];
+
 	asm volatile (".set push\n"
 		      ".set noreorder\n"
+		      /* Atomic incremebt of 'ticket' with LL/SC */
 		      "1:\n"
 		      "lld     %[my_ticket], %[ticket_ptr]\n"
-		      /* Atomic add one to ticket_ptr 64-bit operation for
-		       * endian nutral access.
-		       */
 		      "daddiu  %[ticket], %[my_ticket], 1\n"
-		      /*    and store the original value  in my_ticket */
 		      "scd     %[ticket], %[ticket_ptr]\n"
-		      "beqz   %[ticket], 1b\n"
-		      " sll	%[my_ticket],%[my_ticket],0\n"        /* truncate to 32 bits */
-		      "lw    %[ticket], %[now_serving]\n"	/* Load the current now_serving ticket */
+		      "beqz    %[ticket], 1b\n"
+		      " lld    %[ticket], %[now_serving]\n"
 		      "2:\n"
-		      "beq    %[ticket], %[my_ticket], 4f\n"	/* Jump out if now_serving == my_ticket */
-		      " subu   %[ticket], %[my_ticket], %[ticket]\n"	/* Find out how many tickets are in front of me */
-		      "subu  %[ticket], 1\n"	/* Use tickets in front of me minus one to delay */
-		      "sll   %[ticket], %[ticket], 5\n"	/* Delay will be ((tickets in front)-1)*32 loops */
-		      "3:\n"
-		      "bnez   %[ticket], 3b\n"	/* Loop here until our ticket might be up */
-		      " subu  %[ticket], 1\n"
-		      "b      2b\n"	/* Jump back up to check out ticket again */
-		      " lw    %[ticket], %[now_serving]\n"	/* Load the current now_serving ticket */
-		      "4:\n"
+                      /* Wait until 'now_serving == ticket' with LL/PAUSE */
+                      "beq    %[ticket], %[my_ticket], 3f\n"
+                      " nop\n pause\n"  /* PAUSE is not allowed in delay slot */
+                      "b      2b\n"     /* check now_serving again */
+                      " lld    %[ticket], %[now_serving]\n"
+	              "3:\n"
 		      ".set pop\n"
-		      : [ticket_ptr] "=m"(__cvmx_cmd_queue_state_ptr->ticket[__cvmx_cmd_queue_get_index(queue_id)]),
-		      [now_serving] "=m"(qptr->now_serving),[ticket] "=&r"(tmp),[my_ticket] "=&r"(my_ticket)
+                      : [ticket_ptr] "=m"(lock_ptr->ticket),
+                      [now_serving] "=m"(lock_ptr->now_serving),
+                      [ticket] "=&r"(tmp),
+                      [my_ticket] "=&r"(my_ticket)
 	    );
 #endif
 }
@@ -299,16 +339,43 @@ static inline void __cvmx_cmd_queue_lock(cvmx_cmd_queue_id_t queue_id,
  *
  * @param qptr   Queue to unlock
  */
-static inline void __cvmx_cmd_queue_unlock(__cvmx_cmd_queue_state_t *qptr)
+static inline void __cvmx_cmd_queue_unlock(cvmx_cmd_queue_id_t queue_id)
 {
 #ifndef __U_BOOT__
-	uint32_t ns;
+	__cvmx_cmd_queue_lock_t *lock_ptr;
+	uint64_t *ns_ptr;
+	uint64_t ns;
+	unsigned node;
+	int tmp;
 
-	ns = qptr->now_serving + 1;
-	CVMX_SYNCWS;		/* Order queue manipulation with respect to the unlock.  */
-	qptr->now_serving = ns;
-	CVMX_SYNCWS;		/* nudge out the unlock. */
+	/* Order queue manipulation with respect to the unlock.  */
+	CVMX_SYNCWS;
+
+	tmp = __cvmx_cmd_queue_get_index(queue_id);
+	node = __cvmx_cmd_queue_get_node(queue_id);
+
+	lock_ptr = &__cvmx_cmd_queue_state_ptrs[node]->lock[tmp];
+	ns_ptr = &lock_ptr->now_serving;
+
+	/* Incremebt 'now_sercving to allow next contender in */
+	ns = (*ns_ptr) + 1;
+	(*ns_ptr) = ns;
 #endif
+	CVMX_SYNCWS;            /* nudge out the unlock. */
+}
+
+/**
+ * @INTERNAL
+ * Initialize a command-queue lock to "unlocked" state.
+ */
+static inline void __cvmx_cmd_queue_lock_init(cvmx_cmd_queue_id_t queue_id)
+{
+	unsigned index = __cvmx_cmd_queue_get_index(queue_id);
+	unsigned node = __cvmx_cmd_queue_get_node(queue_id);
+
+	__cvmx_cmd_queue_state_ptrs[node]->lock[index] =
+			(__cvmx_cmd_queue_lock_t){0, 0};
+	CVMX_SYNCWS;
 }
 
 /**
@@ -322,21 +389,24 @@ static inline void __cvmx_cmd_queue_unlock(__cvmx_cmd_queue_state_t *qptr)
 static inline __cvmx_cmd_queue_state_t *
 __cvmx_cmd_queue_get_state(cvmx_cmd_queue_id_t queue_id)
 {
-	extern CVMX_SHARED __cvmx_cmd_queue_all_state_t *__cvmx_cmd_queue_state_ptr;
+	unsigned index;
+	unsigned node;
+	__cvmx_cmd_queue_state_t *qptr;
+
 	if (CVMX_ENABLE_PARAMETER_CHECKING) {
 		if (cvmx_unlikely(queue_id >= CVMX_CMD_QUEUE_END))
 			return NULL;
 		if (cvmx_unlikely((queue_id & 0xffff) >= 256))
 			return NULL;
 	}
-	return &__cvmx_cmd_queue_state_ptr->state[__cvmx_cmd_queue_get_index(queue_id)];
-}
+	node = __cvmx_cmd_queue_get_node(queue_id);
+	index = __cvmx_cmd_queue_get_index(queue_id);
 
-static inline uint64_t *__cvmx_cmd_queue_alloc_buffer(int pool)
-{
-	uint64_t *new_buffer;
-	new_buffer = cvmx_fpa_alloc(pool);
-	return new_buffer;
+	if (cvmx_unlikely(__cvmx_cmd_queue_state_ptrs[node] == NULL))
+		__cvmx_cmd_queue_init_state_ptr(node);
+
+	qptr = &__cvmx_cmd_queue_state_ptrs[node]->state[index];
+	return qptr;
 }
 
 /**
@@ -355,9 +425,12 @@ static inline uint64_t *__cvmx_cmd_queue_alloc_buffer(int pool)
  * @return CVMX_CMD_QUEUE_SUCCESS or a failure code
  */
 static inline cvmx_cmd_queue_result_t
-cvmx_cmd_queue_write(cvmx_cmd_queue_id_t queue_id, int use_locking,
-		     int cmd_count, uint64_t *cmds)
+cvmx_cmd_queue_write(cvmx_cmd_queue_id_t queue_id, bool use_locking,
+		    int cmd_count, const uint64_t *cmds)
 {
+	cvmx_cmd_queue_result_t ret = CVMX_CMD_QUEUE_SUCCESS;
+	uint64_t *cmd_ptr;
+
 	__cvmx_cmd_queue_state_t *qptr = __cvmx_cmd_queue_get_state(queue_id);
 
 	if (CVMX_ENABLE_PARAMETER_CHECKING) {
@@ -371,68 +444,34 @@ cvmx_cmd_queue_write(cvmx_cmd_queue_id_t queue_id, int use_locking,
 
 	/* Make sure nobody else is updating the same queue */
 	if (cvmx_likely(use_locking))
-		__cvmx_cmd_queue_lock(queue_id, qptr);
-
-	/* If a max queue length was specified then make sure we don't
-	   exceed it. If any part of the command would be below the limit
-	   we allow it */
-	if (CVMX_CMD_QUEUE_ENABLE_MAX_DEPTH && cvmx_unlikely(qptr->max_depth)) {
-		if (cvmx_unlikely(cvmx_cmd_queue_length(queue_id) > (int)qptr->max_depth)) {
-			if (cvmx_likely(use_locking))
-				__cvmx_cmd_queue_unlock(qptr);
-			return CVMX_CMD_QUEUE_FULL;
-		}
-	}
-
-	/* Normally there is plenty of room in the current buffer for the command */
-	if (cvmx_likely(qptr->index + cmd_count < qptr->pool_size_m1)) {
-		uint64_t *ptr = (uint64_t *) cvmx_phys_to_ptr((uint64_t) qptr->base_ptr_div128 << 7);
-		ptr += qptr->index;
-		qptr->index += cmd_count;
-		while (cmd_count--)
-			*ptr++ = *cmds++;
+		__cvmx_cmd_queue_lock(queue_id);
+
+	/* Most of the time there is lots of free words in current block */
+	if (cvmx_unlikely(
+	    (qptr->index + cmd_count) >= qptr->pool_size_m1)) {
+		/* The rare case when nearing end of block */
+		ret = __cvmx_cmd_queue_write_raw(queue_id, qptr,
+			cmd_count, cmds);
 	} else {
-		uint64_t *ptr;
-		int count;
-		/* We need a new command buffer. Fail if there isn't one available */
-		uint64_t *new_buffer = __cvmx_cmd_queue_alloc_buffer(qptr->fpa_pool);
-		if (cvmx_unlikely(new_buffer == NULL)) {
-			if (cvmx_likely(use_locking))
-				__cvmx_cmd_queue_unlock(qptr);
-			return CVMX_CMD_QUEUE_NO_MEMORY;
+		cmd_ptr = cvmx_phys_to_ptr((uint64_t) qptr->base_paddr);
+		/* Loop easy for compiler to unroll for the likely case */
+		while (cmd_count > 0) {
+			cmd_ptr[ qptr->index ++ ] = *cmds++;
+			cmd_count --;
 		}
-		ptr = (uint64_t *) cvmx_phys_to_ptr((uint64_t) qptr->base_ptr_div128 << 7);
-		/* Figure out how many command words will fit in this buffer.
-		 * One location will be needed for the next buffer pointer
-		 */
-		count = qptr->pool_size_m1 - qptr->index;
-		ptr += qptr->index;
-		cmd_count -= count;
-		while (count--)
-			*ptr++ = *cmds++;
-		*ptr = cvmx_ptr_to_phys(new_buffer);
-		/* The current buffer is full and has a link to the next buffer.
-		 * Time to write the rest of the commands into the new buffer
-		 */
-		qptr->base_ptr_div128 = *ptr >> 7;
-		qptr->index = cmd_count;
-		ptr = new_buffer;
-		while (cmd_count--)
-			*ptr++ = *cmds++;
 	}
 
 	/* All updates are complete. Release the lock and return */
 	if (cvmx_likely(use_locking))
-		__cvmx_cmd_queue_unlock(qptr);
-#ifdef __U_BOOT__
-	CVMX_SYNCWS;
-#endif
-	return CVMX_CMD_QUEUE_SUCCESS;
+		__cvmx_cmd_queue_unlock(queue_id);
+	else
+		CVMX_SYNCWS;
+
+	return ret;
 }
 
 /**
- * Simple function to write two command words to a command
- * queue.
+ * Simple function to write two command words to a command queue.
  *
  * @param queue_id Hardware command queue to write to
  * @param use_locking
@@ -445,9 +484,12 @@ cvmx_cmd_queue_write(cvmx_cmd_queue_id_t queue_id, int use_locking,
  * @return CVMX_CMD_QUEUE_SUCCESS or a failure code
  */
 static inline cvmx_cmd_queue_result_t
-cvmx_cmd_queue_write2(cvmx_cmd_queue_id_t queue_id, int use_locking,
+cvmx_cmd_queue_write2(cvmx_cmd_queue_id_t queue_id, bool use_locking,
 		      uint64_t cmd1, uint64_t cmd2)
 {
+	cvmx_cmd_queue_result_t ret = CVMX_CMD_QUEUE_SUCCESS;
+	uint64_t *cmd_ptr;
+
 	__cvmx_cmd_queue_state_t *qptr = __cvmx_cmd_queue_get_state(queue_id);
 
 	if (CVMX_ENABLE_PARAMETER_CHECKING) {
@@ -457,64 +499,34 @@ cvmx_cmd_queue_write2(cvmx_cmd_queue_id_t queue_id, int use_locking,
 
 	/* Make sure nobody else is updating the same queue */
 	if (cvmx_likely(use_locking))
-		__cvmx_cmd_queue_lock(queue_id, qptr);
-
-	/* If a max queue length was specified then make sure we don't
-	   exceed it. If any part of the command would be below the limit
-	   we allow it */
-	if (CVMX_CMD_QUEUE_ENABLE_MAX_DEPTH && cvmx_unlikely(qptr->max_depth)) {
-		if (cvmx_unlikely(cvmx_cmd_queue_length(queue_id) > (int)qptr->max_depth)) {
-			if (cvmx_likely(use_locking))
-				__cvmx_cmd_queue_unlock(qptr);
-			return CVMX_CMD_QUEUE_FULL;
-		}
-	}
-
-	/* Normally there is plenty of room in the current buffer for the command */
-	if (cvmx_likely(qptr->index + 2 < qptr->pool_size_m1)) {
-		uint64_t *ptr = (uint64_t *) cvmx_phys_to_ptr((uint64_t) qptr->base_ptr_div128 << 7);
-		ptr += qptr->index;
-		qptr->index += 2;
-		ptr[0] = cmd1;
-		ptr[1] = cmd2;
+		__cvmx_cmd_queue_lock(queue_id);
+
+	if (cvmx_unlikely((qptr->index + 2) >= qptr->pool_size_m1)) {
+		/* The rare case when nearing end of block */
+		uint64_t cmds[2];
+		cmds[0] = cmd1;
+		cmds[1] = cmd2;
+		ret = __cvmx_cmd_queue_write_raw(queue_id, qptr, 2, cmds);
 	} else {
-		uint64_t *ptr;
-		/* Figure out how many command words will fit in this buffer. One
-		   location will be needed for the next buffer pointer */
-		int count = qptr->pool_size_m1 - qptr->index;
-		/* We need a new command buffer. Fail if there isn't one available */
-		uint64_t *new_buffer = __cvmx_cmd_queue_alloc_buffer(qptr->fpa_pool);
-		if (cvmx_unlikely(new_buffer == NULL)) {
-			if (cvmx_likely(use_locking))
-				__cvmx_cmd_queue_unlock(qptr);
-			return CVMX_CMD_QUEUE_NO_MEMORY;
-		}
-		count--;
-		ptr = (uint64_t *) cvmx_phys_to_ptr((uint64_t) qptr->base_ptr_div128 << 7);
-		ptr += qptr->index;
-		*ptr++ = cmd1;
-		if (cvmx_likely(count))
-			*ptr++ = cmd2;
-		*ptr = cvmx_ptr_to_phys(new_buffer);
-		/* The current buffer is full and has a link to the next buffer. Time
-		   to write the rest of the commands into the new buffer */
-		qptr->base_ptr_div128 = *ptr >> 7;
-		qptr->index = 0;
-		if (cvmx_unlikely(count == 0)) {
-			qptr->index = 1;
-			new_buffer[0] = cmd2;
-		}
+		/* Likely case to work fast */
+		cmd_ptr = cvmx_phys_to_ptr((uint64_t) qptr->base_paddr);
+		cmd_ptr += qptr->index;
+		qptr->index += 2;
+		cmd_ptr[0] = cmd1;
+		cmd_ptr[1] = cmd2;
 	}
 
 	/* All updates are complete. Release the lock and return */
 	if (cvmx_likely(use_locking))
-		__cvmx_cmd_queue_unlock(qptr);
-	return CVMX_CMD_QUEUE_SUCCESS;
+		__cvmx_cmd_queue_unlock(queue_id);
+	else
+		CVMX_SYNCWS;
+
+	return ret;
 }
 
 /**
- * Simple function to write three command words to a command
- * queue.
+ * Simple function to write three command words to a command queue.
  *
  * @param queue_id Hardware command queue to write to
  * @param use_locking
@@ -528,9 +540,12 @@ cvmx_cmd_queue_write2(cvmx_cmd_queue_id_t queue_id, int use_locking,
  * @return CVMX_CMD_QUEUE_SUCCESS or a failure code
  */
 static inline cvmx_cmd_queue_result_t
-cvmx_cmd_queue_write3(cvmx_cmd_queue_id_t queue_id, int use_locking,
+cvmx_cmd_queue_write3(cvmx_cmd_queue_id_t queue_id, bool use_locking,
 		      uint64_t cmd1, uint64_t cmd2, uint64_t cmd3)
 {
+	cvmx_cmd_queue_result_t ret = CVMX_CMD_QUEUE_SUCCESS;
+	uint64_t *cmd_ptr;
+
 	__cvmx_cmd_queue_state_t *qptr = __cvmx_cmd_queue_get_state(queue_id);
 
 	if (CVMX_ENABLE_PARAMETER_CHECKING) {
@@ -540,70 +555,31 @@ cvmx_cmd_queue_write3(cvmx_cmd_queue_id_t queue_id, int use_locking,
 
 	/* Make sure nobody else is updating the same queue */
 	if (cvmx_likely(use_locking))
-		__cvmx_cmd_queue_lock(queue_id, qptr);
-
-	/* If a max queue length was specified then make sure we don't
-	   exceed it. If any part of the command would be below the limit
-	   we allow it */
-	if (CVMX_CMD_QUEUE_ENABLE_MAX_DEPTH && cvmx_unlikely(qptr->max_depth)) {
-		if (cvmx_unlikely(cvmx_cmd_queue_length(queue_id) > (int)qptr->max_depth)) {
-			if (cvmx_likely(use_locking))
-				__cvmx_cmd_queue_unlock(qptr);
-			return CVMX_CMD_QUEUE_FULL;
-		}
-	}
-
-	/* Normally there is plenty of room in the current buffer for the command */
-	if (cvmx_likely(qptr->index + 3 < qptr->pool_size_m1)) {
-		uint64_t *ptr = (uint64_t *) cvmx_phys_to_ptr((uint64_t) qptr->base_ptr_div128 << 7);
-		ptr += qptr->index;
-		qptr->index += 3;
-		ptr[0] = cmd1;
-		ptr[1] = cmd2;
-		ptr[2] = cmd3;
+		__cvmx_cmd_queue_lock(queue_id);
+
+	if (cvmx_unlikely((qptr->index + 3) >= qptr->pool_size_m1)) {
+	/* Most of the time there is lots of free words in current block */
+		uint64_t cmds[3];
+		cmds[0] = cmd1;
+		cmds[1] = cmd2;
+		cmds[2] = cmd3;
+		ret = __cvmx_cmd_queue_write_raw(queue_id, qptr, 3, cmds);
 	} else {
-		uint64_t *ptr;
-		/* Figure out how many command words will fit in this buffer.
-		 * One location will be needed for the next buffer pointer
-		 */
-		int count = qptr->pool_size_m1 - qptr->index;
-		/* We need a new command buffer. Fail if there isn't one available */
-		uint64_t *new_buffer = __cvmx_cmd_queue_alloc_buffer(qptr->fpa_pool);
-		if (cvmx_unlikely(new_buffer == NULL)) {
-			if (cvmx_likely(use_locking))
-				__cvmx_cmd_queue_unlock(qptr);
-			return CVMX_CMD_QUEUE_NO_MEMORY;
-		}
-		count--;
-		ptr = (uint64_t *) cvmx_phys_to_ptr((uint64_t) qptr->base_ptr_div128 << 7);
-		ptr += qptr->index;
-		*ptr++ = cmd1;
-		if (count) {
-			*ptr++ = cmd2;
-			if (count > 1)
-				*ptr++ = cmd3;
-		}
-		*ptr = cvmx_ptr_to_phys(new_buffer);
-		/* The current buffer is full and has a link to the next buffer.
-		 * Time to write the rest of the commands into the new buffer
-		 */
-		qptr->base_ptr_div128 = *ptr >> 7;
-		qptr->index = 0;
-		ptr = new_buffer;
-		if (count == 0) {
-			*ptr++ = cmd2;
-			qptr->index++;
-		}
-		if (count < 2) {
-			*ptr++ = cmd3;
-			qptr->index++;
-		}
+		cmd_ptr = cvmx_phys_to_ptr((uint64_t) qptr->base_paddr);
+		cmd_ptr += qptr->index;
+		qptr->index += 3;
+		cmd_ptr[0] = cmd1;
+		cmd_ptr[1] = cmd2;
+		cmd_ptr[2] = cmd3;
 	}
 
 	/* All updates are complete. Release the lock and return */
 	if (cvmx_likely(use_locking))
-		__cvmx_cmd_queue_unlock(qptr);
-	return CVMX_CMD_QUEUE_SUCCESS;
+		__cvmx_cmd_queue_unlock(queue_id);
+	else
+		CVMX_SYNCWS;
+
+	return ret;
 }
 
 #ifdef	__cplusplus
diff --git a/arch/mips/include/asm/octeon/cvmx-fpa.h b/arch/mips/include/asm/octeon/cvmx-fpa.h
index a6adbbd..45065fa 100644
--- a/arch/mips/include/asm/octeon/cvmx-fpa.h
+++ b/arch/mips/include/asm/octeon/cvmx-fpa.h
@@ -42,7 +42,7 @@
  *
  * Interface to the hardware Free Pool Allocator.
  *
- * <hr>$Revision: 104152 $<hr>
+ * <hr>$Revision: 113671 $<hr>
  *
  */
 
@@ -292,6 +292,8 @@ extern int cvmx_fpa_is_pool_available(int pool_num);
 extern uint64_t cvmx_fpa_get_pool_owner(int pool_num);
 extern int cvmx_fpa_get_max_pools(void);
 extern int cvmx_fpa_get_current_count(int pool_num);
+extern int cvmx_fpa_validate_pool(int pool);
+
 
 #ifdef	__cplusplus
 /* *INDENT-OFF* */
diff --git a/arch/mips/include/asm/octeon/cvmx-pko3.h b/arch/mips/include/asm/octeon/cvmx-pko3.h
index fdd7427..1a1facd 100644
--- a/arch/mips/include/asm/octeon/cvmx-pko3.h
+++ b/arch/mips/include/asm/octeon/cvmx-pko3.h
@@ -57,6 +57,7 @@ extern "C" {
 #include <asm/octeon/cvmx-helper.h>
 #include <asm/octeon/cvmx-pko3-queue.h>
 #include <asm/octeon/cvmx-ilk.h>
+#include <asm/octeon/cvmx-pow.h>
 #include <asm/octeon/cvmx-scratch.h>
 #include <asm/octeon/cvmx-atomic.h>
 #else
@@ -64,6 +65,7 @@ extern "C" {
 #include "cvmx-pko3-queue.h"
 #include "cvmx-helper.h"
 #include "cvmx-ilk.h"
+#include "cvmx-pow.h"
 #include "cvmx-scratch.h"
 #include "cvmx-atomic.h"
 #endif
@@ -439,7 +441,7 @@ cvmx_pko3_cvmseg_addr(void)
  * NOTE: Internal use only.
  */
 static inline cvmx_pko_query_rtn_t
-__cvmx_pko3_lmtdma(uint8_t node, uint16_t dq, unsigned numwords)
+__cvmx_pko3_lmtdma(uint8_t node, uint16_t dq, unsigned numwords, bool tag_wait)
 {
 	const enum cvmx_pko_dqop dqop = CVMX_PKO_DQ_SEND;
 	cvmx_pko_query_rtn_t pko_status;
@@ -500,6 +502,10 @@ __cvmx_pko3_lmtdma(uint8_t node, uint16_t dq, unsigned numwords)
 	/* Barrier: make sure all prior writes complete before the following */
 	CVMX_SYNCWS;
 
+	/* Wait to finish tag switch just before issueing LMTDMA */
+	if (tag_wait)
+		cvmx_pow_tag_sw_wait();
+
 	/* issue PKO DMA */
 	cvmx_write64_uint64(dma_addr, pko_send_dma_data.u64);
 
@@ -704,7 +710,7 @@ cvmx_pko3_xmit_link_buf(int dq,cvmx_buf_ptr_pki_t pki_ptr,
 	}
 
 	/* Do LMTDMA */
-	pko_status = __cvmx_pko3_lmtdma(node, dq, nwords);
+	pko_status = __cvmx_pko3_lmtdma(node, dq, nwords, false);
 
 	if (cvmx_likely(pko_status.s.dqstatus == PKO_DQSTATUS_PASS))
 		return 0;
-- 
1.9.1

