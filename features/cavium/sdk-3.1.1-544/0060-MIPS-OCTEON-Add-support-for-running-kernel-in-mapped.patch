From eb47c395230d91a96d218bb97803db13de7d754f Mon Sep 17 00:00:00 2001
From: David Daney <david.daney@cavium.com>
Date: Fri, 13 Feb 2015 14:30:14 +0530
Subject: [PATCH 060/132] MIPS: OCTEON: Add support for running kernel in
 mapped address space.

Commit 8b11e3f6cc0944f8a4f53be699ddf4b876676544 from
git://git.yoctoproject.org/linux-yocto-3.14

Allow the kernel to run in mapped address space via a wired TLB entry.
This allows the same kernel binary to run as more than a single
execution instance in the same system.  Also module function calls can
be done with a single instruction for higher performance.

Make __phys_addr work for addresses in mapped kernel images.
Make virt_to_phys() work for all unmapped addresses.

Signed-off-by: David Daney <david.daney@cavium.com>
Signed-off-by: Abhishek Paliwal <abhishek.paliwal@aricent.com>
[Original patch taken from OCTEON-SDK 3.1.1-544.]
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 arch/mips/Kconfig                  | 18 ++++++++++++++++++
 arch/mips/Makefile                 |  3 +++
 arch/mips/include/asm/page.h       | 17 +++++++++++------
 arch/mips/include/asm/pgtable-64.h |  7 ++++++-
 arch/mips/kernel/Makefile          |  6 +++++-
 arch/mips/kernel/vmlinux.lds.S     | 15 +++++++++++++--
 arch/mips/mm/tlb-r4k.c             |  2 ++
 arch/mips/mm/tlbex.c               | 13 +++++++++++++
 8 files changed, 71 insertions(+), 10 deletions(-)

diff --git a/arch/mips/Kconfig b/arch/mips/Kconfig
index 778ee0e..92a07dc 100644
--- a/arch/mips/Kconfig
+++ b/arch/mips/Kconfig
@@ -2061,6 +2061,24 @@ config FORCE_MAX_ZONEORDER
 	  The page size is not necessarily 4KB.  Keep this in mind
 	  when choosing a value for this option.
 
+config MAPPED_KERNEL
+	bool "Mapped kernel"
+	depends on CPU_CAVIUM_OCTEON
+	help
+	  Select this option if you want the kernel's code and data to
+	  be in mapped memory.  The kernel will be mapped using a
+	  single wired TLB entry, thus reducing the number of
+	  available TLB entries by one.  Kernel modules will be able
+	  to use a more efficient calling convention.
+
+config PHYS_LOAD_ADDRESS
+	hex "Physical load address"
+	depends on MAPPED_KERNEL
+	default 0xffffffff81000000
+	help
+	  The physical load address reflected as the program header
+	  physical address in the kernel ELF image.
+
 config BOARD_SCACHE
 	bool
 
diff --git a/arch/mips/Makefile b/arch/mips/Makefile
index c7f028c..d1d117d 100644
--- a/arch/mips/Makefile
+++ b/arch/mips/Makefile
@@ -90,8 +90,11 @@ all-$(CONFIG_SYS_SUPPORTS_ZBOOT)+= vmlinuz
 cflags-y			+= -G 0 -mno-abicalls -fno-pic -pipe
 cflags-y			+= -msoft-float
 LDFLAGS_vmlinux			+= -G 0 -static -n -nostdlib
+
+ifndef CONFIG_MAPPED_KERNEL
 KBUILD_AFLAGS_MODULE		+= -mlong-calls
 KBUILD_CFLAGS_MODULE		+= -mlong-calls
+endif
 
 #
 # pass -msoft-float to GAS if it supports it.  However on newer binutils
diff --git a/arch/mips/include/asm/page.h b/arch/mips/include/asm/page.h
index 89dd7fe..e38afec 100644
--- a/arch/mips/include/asm/page.h
+++ b/arch/mips/include/asm/page.h
@@ -70,6 +70,8 @@ static inline unsigned int page_size_ftlb(unsigned int mmuextdef)
 #define HUGETLB_PAGE_ORDER	({BUILD_BUG(); 0; })
 #endif /* CONFIG_MIPS_HUGE_TLB_SUPPORT */
 
+#ifndef __ASSEMBLY__
+
 #include <linux/pfn.h>
 
 extern void build_clear_page(void);
@@ -163,11 +165,8 @@ typedef struct { unsigned long pgprot; } pgprot_t;
  * __pa()/__va() should be used only during mem init.
  */
 #ifdef CONFIG_64BIT
-#define __pa(x)								\
-({									\
-    unsigned long __x = (unsigned long)(x);				\
-    __x < CKSEG0 ? XPHYSADDR(__x) : CPHYSADDR(__x);			\
-})
+unsigned long __phys_addr(unsigned long x);
+#define __pa(x)        __phys_addr((unsigned long)x)
 #else
 #define __pa(x)								\
     ((unsigned long)(x) - PAGE_OFFSET + PHYS_OFFSET)
@@ -189,7 +188,13 @@ typedef struct { unsigned long pgprot; } pgprot_t;
  */
 
 #ifndef __pa_symbol
-#define __pa_symbol(x)	__pa(RELOC_HIDE((unsigned long)(x), 0))
+# ifdef CONFIG_MAPPED_KERNEL
+extern unsigned long phys_to_kernel_offset;
+#  define __pa_symbol(x)	(RELOC_HIDE((unsigned long)(x), 0) - phys_to_kernel_offset)
+# else
+#  define __pa_symbol(x)	__pa(RELOC_HIDE((unsigned long)(x), 0))
+# endif
+#endif /*__ASSEMBLY__*/
 #endif
 
 #define pfn_to_kaddr(pfn)	__va((pfn) << PAGE_SHIFT)
diff --git a/arch/mips/include/asm/pgtable-64.h b/arch/mips/include/asm/pgtable-64.h
index cf661a2..bbcd466 100644
--- a/arch/mips/include/asm/pgtable-64.h
+++ b/arch/mips/include/asm/pgtable-64.h
@@ -135,7 +135,12 @@
 #if defined(CONFIG_MODULES) && defined(KBUILD_64BIT_SYM32) && \
 	VMALLOC_START != CKSSEG
 /* Load modules into 32bit-compatible segment. */
-#define MODULE_START	CKSSEG
+#ifdef CONFIG_MAPPED_KERNEL
+extern unsigned long kernel_image_end;
+#define MODULE_START   kernel_image_end
+#else
+#define MODULE_START   CKSSEG
+#endif
 #define MODULE_END	(FIXADDR_START-2*PAGE_SIZE)
 #endif
 
diff --git a/arch/mips/kernel/Makefile b/arch/mips/kernel/Makefile
index 15c019b..2f2cec3 100644
--- a/arch/mips/kernel/Makefile
+++ b/arch/mips/kernel/Makefile
@@ -126,4 +126,8 @@ CFLAGS_branch.o			= $(CFLAGS_DSP)
 CFLAGS_ptrace.o			= $(CFLAGS_DSP)
 endif
 
-CPPFLAGS_vmlinux.lds		:= $(KBUILD_CFLAGS)
+ifdef CONFIG_MAPPED_KERNEL
+  PHYS_LOAD_ADDRESS = -D"PHYSADDR=$(CONFIG_PHYS_LOAD_ADDRESS)"
+endif
+
+CPPFLAGS_vmlinux.lds           := $(KBUILD_CFLAGS) $(PHYS_LOAD_ADDRESS)
diff --git a/arch/mips/kernel/vmlinux.lds.S b/arch/mips/kernel/vmlinux.lds.S
index 89debf0..620050b 100644
--- a/arch/mips/kernel/vmlinux.lds.S
+++ b/arch/mips/kernel/vmlinux.lds.S
@@ -3,6 +3,11 @@
 
 #define PAGE_SIZE _PAGE_SIZE
 
+#ifdef PHYSADDR
+phys_entry = kernel_entry - VMLINUX_LOAD_ADDRESS + PHYSADDR;
+#define LOAD_OFFSET (VMLINUX_LOAD_ADDRESS - PHYSADDR)
+#endif
+
 /*
  * Put .bss..swapper_pg_dir as the first thing in .bss. This will
  * ensure that it has .bss alignment (64K).
@@ -14,9 +19,15 @@
 #undef mips
 #define mips mips
 OUTPUT_ARCH(mips)
+#ifdef PHYSADDR
+ENTRY(phys_entry)
+#define AT_LOCATION AT(PHYSADDR)
+#else
 ENTRY(kernel_entry)
+#define AT_LOCATION
+#endif
 PHDRS {
-	text PT_LOAD FLAGS(7);	/* RWX */
+	text PT_LOAD AT_LOCATION FLAGS(7);	/* RWX */
 #ifndef CONFIG_DISABLE_ELF_NOTE_HEADER
 	note PT_NOTE FLAGS(4);	/* R__ */
 #endif
@@ -52,7 +63,7 @@ SECTIONS
 	. = VMLINUX_LOAD_ADDRESS;
 	/* read-only */
 	_text = .;	/* Text and read-only data */
-	.text : {
+	.text : AT_LOCATION {
 		TEXT_TEXT
 		SCHED_TEXT
 		LOCK_TEXT
diff --git a/arch/mips/mm/tlb-r4k.c b/arch/mips/mm/tlb-r4k.c
index 08318ec..dcd7d99 100644
--- a/arch/mips/mm/tlb-r4k.c
+++ b/arch/mips/mm/tlb-r4k.c
@@ -486,7 +486,9 @@ static void r4k_tlb_configure(void)
 	 *     be set to fixed-size pages.
 	 */
 	write_c0_pagemask(PM_DEFAULT_MASK);
+#ifndef CONFIG_MAPPED_KERNEL
 	write_c0_wired(0);
+#endif
 	if (current_cpu_type() == CPU_R10000 ||
 	    current_cpu_type() == CPU_R12000 ||
 	    current_cpu_type() == CPU_R14000 ||
diff --git a/arch/mips/mm/tlbex.c b/arch/mips/mm/tlbex.c
index b7f007d..a2436ed 100644
--- a/arch/mips/mm/tlbex.c
+++ b/arch/mips/mm/tlbex.c
@@ -1476,6 +1476,18 @@ static void build_setup_pgd(void)
 		struct uasm_reloc *r = relocs;
 
 		/* PGD << 11 in c0_Context */
+#ifdef CONFIG_MAPPED_KERNEL
+		/*
+		 * if (&swapper_pg_dir == pgd)
+		 *     pgd = pgd - phys_to_kernel_offset;
+		 */
+		UASM_i_LA(&p, a1, (long)&swapper_pg_dir);
+		uasm_il_bne(&p, &r, a0, a1, label_tlbl_goaround1);
+		UASM_i_LA_mostly(&p, a1, (long)&phys_to_kernel_offset);
+		UASM_i_LW(&p, a1, uasm_rel_lo((long)&phys_to_kernel_offset), a1);
+		UASM_i_SUBU(&p, a0, a0, a1);
+		/* Fall through to tlbl_goaround1. */
+#else
 		/*
 		 * If it is a ckseg0 address, convert to a physical
 		 * address.  Shifting right by 29 and adding 4 will
@@ -1487,6 +1499,7 @@ static void build_setup_pgd(void)
 		uasm_il_bnez(&p, &r, a1, label_tlbl_goaround1);
 		uasm_i_nop(&p);
 		uasm_i_dinsm(&p, a0, 0, 29, 64 - 29);
+#endif /* CONFIG_MAPPED_KERNEL */
 		uasm_l_tlbl_goaround1(&l, p);
 		UASM_i_SLL(&p, a0, a0, 11);
 		uasm_i_jr(&p, 31);
-- 
1.9.1

