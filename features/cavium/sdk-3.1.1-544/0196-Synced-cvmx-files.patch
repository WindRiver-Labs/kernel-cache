From 943d4c80472c690a7bdd553e9055db5ba5091778 Mon Sep 17 00:00:00 2001
From: Tsvetan Erenditsov <terenditsov@caviumnetworks.com>
Date: Fri, 31 Oct 2014 14:19:04 -0700
Subject: [PATCH 196/202] Synced cvmx files

Signed-off-by: Tsvetan Erenditsov <terenditsov@caviumnetworks.com>
[Original patch taken from Cavium SDK 3.1.1-544]
Signed-off-by: Bin Jiang <bin.jiang@windriver.com>
---
 .../executive/cvmx-appcfg-transport.c              |  12 +
 arch/mips/cavium-octeon/executive/cvmx-bootmem.c   |  14 +-
 arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c |   6 +-
 .../cavium-octeon/executive/cvmx-debug-remote.c    |   2 +-
 arch/mips/cavium-octeon/executive/cvmx-debug.c     |   9 +-
 .../cavium-octeon/executive/cvmx-fpa-resource.c    |  13 +-
 .../executive/cvmx-global-resources.c              |  36 +-
 .../mips/cavium-octeon/executive/cvmx-helper-agl.c |   2 +-
 .../mips/cavium-octeon/executive/cvmx-helper-bgx.c | 107 ++----
 .../cavium-octeon/executive/cvmx-helper-board.c    | 145 +++----
 .../mips/cavium-octeon/executive/cvmx-helper-cfg.c |  11 +-
 .../mips/cavium-octeon/executive/cvmx-helper-ilk.c |   2 +-
 .../mips/cavium-octeon/executive/cvmx-helper-pki.c |  82 ++--
 .../mips/cavium-octeon/executive/cvmx-helper-pko.c |   6 +-
 .../cavium-octeon/executive/cvmx-helper-pko3.c     |   7 +-
 .../cavium-octeon/executive/cvmx-helper-sgmii.c    |   6 +-
 .../cavium-octeon/executive/cvmx-helper-util.c     |  63 +--
 .../cavium-octeon/executive/cvmx-helper-xaui.c     |   7 +-
 arch/mips/cavium-octeon/executive/cvmx-helper.c    |  21 +-
 arch/mips/cavium-octeon/executive/cvmx-ipd.c       |   2 -
 arch/mips/cavium-octeon/executive/cvmx-l2c.c       |  21 +-
 arch/mips/cavium-octeon/executive/cvmx-pcie.c      | 428 +++++++++++----------
 .../cavium-octeon/executive/cvmx-pki-resources.c   |  66 +++-
 arch/mips/cavium-octeon/executive/cvmx-pki.c       |   4 +-
 arch/mips/cavium-octeon/executive/cvmx-pko.c       |   6 +-
 .../mips/cavium-octeon/executive/cvmx-pko3-queue.c |  12 +-
 arch/mips/cavium-octeon/executive/cvmx-pko3.c      |  51 ++-
 arch/mips/cavium-octeon/executive/cvmx-qlm.c       |   4 +-
 arch/mips/cavium-octeon/executive/cvmx-range.c     |   8 +-
 arch/mips/cavium-octeon/executive/cvmx-srio.c      |   4 +-
 arch/mips/cavium-octeon/executive/cvmx-usb.c       |   2 +-
 arch/mips/cavium-octeon/executive/octeon-model.c   |  22 +-
 arch/mips/include/asm/octeon/cvmx-app-config.h     |   2 +
 arch/mips/include/asm/octeon/cvmx-app-init.h       |  16 +-
 arch/mips/include/asm/octeon/cvmx-fpa3.h           |  31 +-
 .../include/asm/octeon/cvmx-global-resources.h     |   1 +
 arch/mips/include/asm/octeon/cvmx-helper-pki.h     |   5 +-
 arch/mips/include/asm/octeon/cvmx-hwpko.h          |   6 +-
 arch/mips/include/asm/octeon/cvmx-l2c.h            |  31 +-
 arch/mips/include/asm/octeon/cvmx-pcie.h           |  20 +-
 arch/mips/include/asm/octeon/cvmx-pip.h            |  10 +-
 arch/mips/include/asm/octeon/cvmx-pki-cluster.h    |   2 +-
 arch/mips/include/asm/octeon/cvmx-pki-resources.h  |  20 +
 arch/mips/include/asm/octeon/cvmx-pko3.h           |  11 +-
 arch/mips/include/asm/octeon/cvmx-usb.h            |   2 +-
 arch/mips/include/asm/octeon/cvmx-wqe.h            |  30 +-
 arch/mips/include/asm/octeon/octeon-boot-info.h    |   1 -
 47 files changed, 817 insertions(+), 552 deletions(-)

diff --git a/arch/mips/cavium-octeon/executive/cvmx-appcfg-transport.c b/arch/mips/cavium-octeon/executive/cvmx-appcfg-transport.c
index 3b14fc6..b8f1e32 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-appcfg-transport.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-appcfg-transport.c
@@ -332,6 +332,18 @@ void __cvmx_export_app_config_cleanup(void)
 EXPORT_SYMBOL(__cvmx_export_app_config_cleanup);
 
 /**
+ * Called by kernel modules to update appconfig
+ * @return 0 if export successful, -1 on failure
+ */
+int __cvmx_export_config(void)
+{
+	if (cvmx_export_app_config)
+		return (*cvmx_export_app_config)();
+	return -1;
+}
+EXPORT_SYMBOL(__cvmx_export_config);
+
+/**
  * @INTERNAL
  * Imports fpa config using named block.
  *
diff --git a/arch/mips/cavium-octeon/executive/cvmx-bootmem.c b/arch/mips/cavium-octeon/executive/cvmx-bootmem.c
index 86b61db..13817fa 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-bootmem.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-bootmem.c
@@ -43,7 +43,7 @@
  * Simple allocate only memory allocator.  Used to allocate memory at
  * application start time.
  *
- * <hr>$Revision: 104081 $<hr>
+ * <hr>$Revision: 106502 $<hr>
  *
  */
 
@@ -648,10 +648,20 @@ int cvmx_bootmem_free_named(const char *name)
 }
 #endif
 
+/**
+ * Find a named block with flags
+ *
+ * @param name is the block name
+ * @param flags indicates the need to use locking during search
+ * @return pointer to named block descriptor
+ *
+ * Note: this function returns a pointer to a static structure,
+ * and is therefore not re-entrant.
+ * Making this function re-entrant will break backward compatibility.
+ */
 const cvmx_bootmem_named_block_desc_t *
 __cvmx_bootmem_find_named_block_flags(const char *name, uint32_t flags)
 {
-	/* FIXME: Returning a single static object is probably a bad thing */
 	static cvmx_bootmem_named_block_desc_t desc;
 	uint64_t named_addr = cvmx_bootmem_phy_named_block_find(name, flags);
 	if (named_addr) {
diff --git a/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c b/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c
index c213b22..7170672 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c
@@ -43,7 +43,7 @@
  * Support functions for managing command queues used for
  * various hardware blocks.
  *
- * <hr>$Revision: 103822 $<hr>
+ * <hr>$Revision: 106617 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <linux/export.h>
@@ -262,7 +262,7 @@ int cvmx_cmd_queue_length(cvmx_cmd_queue_id_t queue_id)
 	switch ((cvmx_cmd_queue_id_t) (queue_id & 0xff0000)) {
 	case CVMX_CMD_QUEUE_PKO_BASE:
 		/*
-		 * FIXME: Need atomic lock on
+		 * Really need atomic lock on
 		 * CVMX_PKO_REG_READ_IDX. Right now we are normally
 		 * called with the queue lock, so that is a SLIGHT
 		 * amount of protection.
@@ -284,7 +284,7 @@ int cvmx_cmd_queue_length(cvmx_cmd_queue_id_t queue_id)
 	case CVMX_CMD_QUEUE_DFA:
 	case CVMX_CMD_QUEUE_HNA:
 	case CVMX_CMD_QUEUE_RAID:
-		/* FIXME: Implement other lengths */
+		/* Still need to implement other lengths */
 		return 0;
 	case CVMX_CMD_QUEUE_DMA_BASE:
 		if (octeon_has_feature(OCTEON_FEATURE_NPEI)) {
diff --git a/arch/mips/cavium-octeon/executive/cvmx-debug-remote.c b/arch/mips/cavium-octeon/executive/cvmx-debug-remote.c
index 43dc232..ebaed84 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-debug-remote.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-debug-remote.c
@@ -71,7 +71,7 @@ static int cvmx_debug_remote_mem_wait_for_resume(volatile cvmx_debug_core_contex
 
 static void cvmx_debug_memory_change_core(int oldcore, int newcore)
 {
-	/* FIXME, this should change the core on the host side too. */
+	/* This should cause the host gdb to change the core but there is no way to signal to it, the core has changed. */
 }
 
 cvmx_debug_comm_t cvmx_debug_remote_comm = {
diff --git a/arch/mips/cavium-octeon/executive/cvmx-debug.c b/arch/mips/cavium-octeon/executive/cvmx-debug.c
index 756599c..2db5409 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-debug.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-debug.c
@@ -335,6 +335,9 @@ void cvmx_debug_init(void)
 	cvmx_spinlock_t *lock;
 	cvmx_coremask_t *pcm = cvmx_debug_core_mask();
 
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+	  return;
+
 	if (!cvmx_debug_enabled())
 		return;
 
@@ -1327,9 +1330,9 @@ static int cvmx_debug_event_loop(cvmx_debug_register_t * debug_reg, volatile cvm
 #ifndef CVMX_BUILD_FOR_LINUX_KERNEL
 			cvmx_coremask_t cm = CVMX_COREMASK_EMPTY;
 
-			/* FIXME: Debugger is limited at 64 cores */
+			/* Note: Debugger is limited at 64 cores */
 			cvmx_coremask_set64(&cm, state.handler_cores);
-			/* FIXME, this should a sync not based on cvmx_coremask_barrier_sync.  */
+			/* This should a sync not based on cvmx_coremask_barrier_sync.  */
 			/* Sync up.  */
 			cvmx_coremask_barrier_sync(&cm);
 #endif
@@ -1624,7 +1627,7 @@ void cvmx_debug_finish(void)
 	if (state.ever_been_in_debug)
 		cvmx_debug_putcorepacket("finished.", coreid);
 
-	/* FIXME: Debugger is limited at 64 cores */
+	/* Note: The Debugger is limited at 64 cores */
 	cvmx_coremask_set64(&cm, state.core_finished);
 
 	/* Notify the debugger if all cores have completed the program */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-fpa-resource.c b/arch/mips/cavium-octeon/executive/cvmx-fpa-resource.c
index 1306a53..09deb3f 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-fpa-resource.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-fpa-resource.c
@@ -198,6 +198,12 @@ int cvmx_fpa3_release_pool(cvmx_fpa3_pool_t pool)
 	if (!__cvmx_fpa3_pool_valid(pool))
 		return -1;
 
+	if (cvmx_create_global_resource_range(tag, CVMX_FPA3_NUM_POOLX) != 0) {
+		cvmx_printf("ERROR: %s: global resource create node=%u\n",
+			__func__, pool.node);
+		return -1;
+	}
+
 	return
 		cvmx_free_global_resource_range_multiple(tag, &lpool, 1);
 }
@@ -242,10 +248,9 @@ int cvmx_fpa1_release_pool(cvmx_fpa1_pool_t pool)
 		cvmx_free_global_resource_range_multiple(tag, &pool, 1);
 }
 
-/* 
- * FIXME:
- * An easier way to acheive the same would be to
- * query the block size of a "pool"
+/**
+ * Query if an FPA pool is available for reservation
+ * using global resources
  */
 int cvmx_fpa1_is_pool_available(cvmx_fpa1_pool_t pool)
 {
diff --git a/arch/mips/cavium-octeon/executive/cvmx-global-resources.c b/arch/mips/cavium-octeon/executive/cvmx-global-resources.c
index 8b76f48..c1a1db4 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-global-resources.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-global-resources.c
@@ -254,7 +254,8 @@ static uint64_t __cvmx_global_resources_init(void)
 							      CVMX_GLOBAL_RESOURCES_DATA_NAME,
 							      CVMX_BOOTMEM_FLAG_NO_LOCKING);
 		if (tmp_phys < 0) {
-			cvmx_dprintf("ERROR : failed to allocate global resource name block. sz=%d \n", sz);
+			cvmx_printf("ERROR: %s: failed to allocate global resource name block. sz=%d\n",
+				__func__, sz);
 			goto end;
 		}
 		__cvmx_global_resources_addr = (uint64_t) tmp_phys;
@@ -346,7 +347,11 @@ uint64_t cvmx_create_global_resource(struct global_resource_tag tag, uint64_t si
 	*new = 1;
 	entry_count = CVMX_GLOBAL_RESOURCES_GET_FIELD(entry_cnt);
 	if (entry_count >= CVMX_MAX_GLOBAL_RESOURCES) {
-		cvmx_dprintf("ERROR: reached max global resources limit\n");
+		char tagname[MAX_RESOURCE_TAG_LEN+1];
+
+		__cvmx_get_tagname(&tag, tagname);
+		cvmx_printf("ERROR: %s: reached global resources limit for %s\n",
+			__func__, tagname);
 		phys_addr = 0;
 		goto end;
 	}
@@ -354,7 +359,11 @@ uint64_t cvmx_create_global_resource(struct global_resource_tag tag, uint64_t si
         /* Allocate bootmem for the resource*/
 	phys_addr = __cvmx_alloc_bootmem_for_global_resources(size);
 	if (!phys_addr) {
-		cvmx_dprintf("ERROR: unable to bootmem size=%d \n", (int) size);
+		char tagname[MAX_RESOURCE_TAG_LEN+1];
+
+		__cvmx_get_tagname(&tag, tagname);
+		cvmx_dprintf("ERROR: %s: out of memory %s, size=%d\n",
+			__func__, tagname, (int) size);
 		goto end;
 	}
 
@@ -407,7 +416,8 @@ int cvmx_allocate_global_resource_range(struct global_resource_tag tag, uint64_t
 	if (addr == 0) {
 		char tagname[256];
 		__cvmx_get_tagname(&tag, tagname);
-		cvmx_dprintf("ERROR: cannot find resource %s\n", tagname);
+		cvmx_printf("ERROR: %s: cannot find resource %s\n", 
+			__func__, tagname);
 		return -1;
 	}
 	__cvmx_global_resource_lock();
@@ -472,6 +482,10 @@ int cvmx_free_global_resource_range_with_base(struct global_resource_tag tag,
 	uint64_t addr = cvmx_get_global_resource(tag,1);
 	int rv;
 
+	/* Resource was not created, nothing to release */
+	if (addr == 0)
+		return 0;
+
 	__cvmx_global_resource_lock();
 	rv = cvmx_range_free_with_base(addr, base, nelements);
 	__cvmx_global_resource_unlock();
@@ -484,6 +498,10 @@ int cvmx_free_global_resource_range_multiple(struct global_resource_tag tag,
 	uint64_t addr = cvmx_get_global_resource(tag,1);
 	int rv;
 
+	/* Resource was not created, nothing to release */
+	if (addr == 0)
+		return 0;
+
 	__cvmx_global_resource_lock();
 	rv = cvmx_range_free_mutiple(addr, bases, nelements);
 	__cvmx_global_resource_unlock();
@@ -496,6 +514,10 @@ int cvmx_free_global_resource_range_with_owner(struct global_resource_tag tag,
 	uint64_t addr = cvmx_get_global_resource(tag,1);
 	int rv;
 
+	/* Resource was not created, nothing to release */
+	if (addr == 0)
+		return 0;
+
 	__cvmx_global_resource_lock();
 	rv = cvmx_range_free_with_owner(addr, owner);
 	__cvmx_global_resource_unlock();
@@ -550,6 +572,10 @@ uint64_t cvmx_get_global_resource_owner(struct global_resource_tag tag, int base
 {
 	uint64_t addr = cvmx_get_global_resource(tag, 1);
 
+	/* Resource was not created, return "available" special owner code */
+	if (addr == 0)
+		return -88LL;
+
 	return cvmx_range_get_owner(addr, base);
 }
 
@@ -570,8 +596,6 @@ void cvmx_global_resources_show(void)
 	entry_cnt = CVMX_GLOBAL_RESOURCES_GET_FIELD(entry_cnt);
 	memset (tagname, 0, MAX_RESOURCE_TAG_LEN + 1);
 
-	if (dbg)
-		cvmx_dprintf("%s: cvmx-global-resources: \n", __func__);
 	for (count = 0; count < entry_cnt; count++) {
 		p = CVMX_GET_RESOURCE_ENTRY(count);
 		phys_addr = CVMX_RESOURCE_ENTRY_GET_FIELD(p, phys_addr);
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-agl.c b/arch/mips/cavium-octeon/executive/cvmx-helper-agl.c
index 1d43c49..928e105 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-agl.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-agl.c
@@ -129,7 +129,7 @@ int __cvmx_helper_agl_probe(int interface)
 	/* MII clocks counts are based on the 125Mhz reference, so our
 	 * delays need to be scaled to match the core clock rate. The
 	 * "+1" is to make sure rounding always waits a little too
-	 * long. FIXME.
+	 * long.
 	 */
 	clock_scale = cvmx_clock_get_rate(CVMX_CLOCK_CORE) / 125000000 + 1;
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-bgx.c b/arch/mips/cavium-octeon/executive/cvmx-helper-bgx.c
index 5e27fe9..72ef11f 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-bgx.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-bgx.c
@@ -80,7 +80,6 @@ int __cvmx_helper_bgx_enumerate(int xiface)
 
 	mode = cvmx_qlm_get_mode_cn78xx(node, qlm);
 	if (mode == CVMX_QLM_MODE_SGMII) {
-	/* FIXME: Check here if SGMII is a MIX interface */
 		return 4;
 	} else if (mode == CVMX_QLM_MODE_XAUI
 		   || mode == CVMX_QLM_MODE_XLAUI
@@ -129,14 +128,13 @@ void cvmx_helper_bgx_disable(int xipd_port)
  *
  * @param mode      Mode to configure the bgx mac as
  */
-static void __cvmx_helper_bgx_common_init(int xiface)
+static void __cvmx_bgx_common_init(int xiface, int index)
 {
 	cvmx_bgxx_cmrx_config_t	cmr_config;
 	cvmx_bgxx_cmr_rx_lmacs_t bgx_cmr_rx_lmacs;
 	cvmx_bgxx_cmr_tx_lmacs_t bgx_cmr_tx_lmacs;
 	cvmx_helper_interface_mode_t mode;
 	int num_ports;
-	int index;
 	int lmac_type = 0;
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
 	int interface = xi.interface;
@@ -173,27 +171,26 @@ static void __cvmx_helper_bgx_common_init(int xiface)
 	}
 
 	/* Set mode and lanes for all interface ports */
-	for (index = 0; index < num_ports; index++) {
-		cmr_config.u64 =
-			cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface));
-		cmr_config.s.enable = 0;
-		cmr_config.s.data_pkt_tx_en = 0;
-		cmr_config.s.data_pkt_rx_en = 0;
-		cmr_config.s.lmac_type = lmac_type;
-		cmr_config.s.lane_to_sds = ((lane_to_sds == 1) ? index
-					     : ((lane_to_sds == 0)
-						 ? (index ? 0xe : 4) : lane_to_sds));
-		cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
-	}
-
-	bgx_cmr_rx_lmacs.u64 = 0;
-	bgx_cmr_rx_lmacs.s.lmacs = num_ports;
-	cvmx_write_csr_node(node, CVMX_BGXX_CMR_RX_LMACS(interface), bgx_cmr_rx_lmacs.u64);
+	cmr_config.u64 =
+		cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface));
+	cmr_config.s.enable = 0;
+	cmr_config.s.data_pkt_tx_en = 0;
+	cmr_config.s.data_pkt_rx_en = 0;
+	cmr_config.s.lmac_type = lmac_type;
+	cmr_config.s.lane_to_sds = ((lane_to_sds == 1) ? index
+				: ((lane_to_sds == 0)
+				? (index ? 0xe : 4) : lane_to_sds));
+	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
 
-	bgx_cmr_tx_lmacs.u64 = 0;
-	bgx_cmr_tx_lmacs.s.lmacs = num_ports;
-	cvmx_write_csr_node(node, CVMX_BGXX_CMR_TX_LMACS(interface), bgx_cmr_tx_lmacs.u64);
+	if (index == 0) {
+		bgx_cmr_rx_lmacs.u64 = 0;
+		bgx_cmr_rx_lmacs.s.lmacs = num_ports;
+		cvmx_write_csr_node(node, CVMX_BGXX_CMR_RX_LMACS(interface), bgx_cmr_rx_lmacs.u64);
 
+		bgx_cmr_tx_lmacs.u64 = 0;
+		bgx_cmr_tx_lmacs.s.lmacs = num_ports;
+		cvmx_write_csr_node(node, CVMX_BGXX_CMR_TX_LMACS(interface), bgx_cmr_tx_lmacs.u64);
+	}
 }
 
 static void __cvmx_bgx_common_init_pknd(int xiface, int index)
@@ -244,7 +241,11 @@ static void __cvmx_bgx_common_init_pknd(int xiface, int index)
  */
 int __cvmx_helper_bgx_probe(int xiface)
 {
-	__cvmx_helper_bgx_common_init(xiface);
+	int num_ports = cvmx_helper_ports_on_interface(xiface);
+	int index;
+
+	for (index = 0; index < num_ports; index++)
+		__cvmx_bgx_common_init(xiface, index);
 	return __cvmx_helper_bgx_enumerate(xiface);
 }
 EXPORT_SYMBOL(__cvmx_helper_bgx_probe);
@@ -581,8 +582,10 @@ static int __cvmx_helper_bgx_sgmii_hardware_init_link_speed(int xiface,
 	cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface));
 	if (!OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X) || index)
 		cmr_config.s.enable = is_enabled;
+#ifndef CVMX_BUILD_FOR_UBOOT
 	cmr_config.s.data_pkt_tx_en = 1;
 	cmr_config.s.data_pkt_rx_en = 1;
+#endif
 	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
 
 	return 0;
@@ -787,7 +790,6 @@ static int __cvmx_helper_bgx_xaui_init(int index, int xiface)
 	if (mode == CVMX_HELPER_INTERFACE_MODE_10G_KR
 	    || mode == CVMX_HELPER_INTERFACE_MODE_40G_KR4) {
 		use_training = 1;
-		/* FIXME: disabled as it currently doesn't work */
 		use_auto_neg = 0;
 	}
 
@@ -847,7 +849,6 @@ static int __cvmx_helper_bgx_xaui_init(int index, int xiface)
 
 		/* 4b. Write BGX(0..5)_SPU(0..3)_CONTROL1[LO_PWR] = 1 and
 		     BGX(0..5)_SPU(0..3)_MISC_CONTROL[RX_PACKET_DIS] = 1. */
-		/* FIXME it is already dine in step 2 */
 		spu_control1.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, interface));
 		spu_control1.s.lo_pwr = 1;
 		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, interface), spu_control1.u64);
@@ -869,6 +870,10 @@ static int __cvmx_helper_bgx_xaui_init(int index, int xiface)
 			cvmx_write_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_CONTROL(index, interface), spu_br_pmd_control.u64);
 
 		}
+	} else { /* enable for simulator */
+		cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface));
+		cmr_config.s.enable = 1;
+		cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
 	}
 
 	/* 4d. Program all other relevant BGX configuration while
@@ -957,46 +962,9 @@ int __cvmx_helper_bgx_port_init(int xipd_port, int phy_pres)
 	int interface = xi.interface;
 	int node = xi.node;
 	int index = cvmx_helper_get_interface_index_num(xp.port);
-	int lmac_type = 0;
-	int lane_to_sds = 0;
 	cvmx_helper_interface_mode_t mode;
-	cvmx_bgxx_cmrx_config_t cmr_config;
 
 	mode = cvmx_helper_interface_get_mode(xiface);
-	switch (mode) {
-	case CVMX_HELPER_INTERFACE_MODE_SGMII:
-		lmac_type = 0;
-		lane_to_sds = 1;
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_XAUI:
-		lmac_type = 1;
-		lane_to_sds = 0xe4;
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_RXAUI:
-		lmac_type = 2;
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_XFI:
-	case CVMX_HELPER_INTERFACE_MODE_10G_KR:
-		lmac_type = 3;
-		lane_to_sds = 1;
-		break;
-	case CVMX_HELPER_INTERFACE_MODE_XLAUI:
-	case CVMX_HELPER_INTERFACE_MODE_40G_KR4:
-		lmac_type = 4;
-		lane_to_sds = 0xe4;
-		break;
-	default:
-		break;
-	}
-	cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface));
-	cmr_config.s.enable = 0;
-	cmr_config.s.data_pkt_tx_en = 0;
-	cmr_config.s.data_pkt_rx_en = 0;
-	cmr_config.s.lmac_type = lmac_type;
-	cmr_config.s.lane_to_sds = ((lane_to_sds == 1) ? index
-	: ((lane_to_sds == 0)
-			? (index ? 0xe : 4) : lane_to_sds));
-	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
 
 	__cvmx_bgx_common_init_pknd(xiface, index);
 
@@ -1010,7 +978,7 @@ int __cvmx_helper_bgx_port_init(int xipd_port, int phy_pres)
 		gmi_tx_thresh.s.cnt = 0x20;
 		cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_THRESH(index, interface),
 				    gmi_tx_thresh.u64);
-		__cvmx_helper_bgx_sgmii_hardware_init_one_time(interface, index);
+		__cvmx_helper_bgx_sgmii_hardware_init_one_time(xiface, index);
 		gmp_txx_append.u64 = cvmx_read_csr_node(node,
 					CVMX_BGXX_GMP_GMI_TXX_APPEND(index, interface));
 		gmp_sgmii_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_SGMII_CTL(index, interface));
@@ -1022,6 +990,12 @@ int __cvmx_helper_bgx_port_init(int xipd_port, int phy_pres)
 		int res;
 		cvmx_bgxx_smux_tx_thresh_t smu_tx_thresh;
 
+		res = __cvmx_helper_bgx_xaui_init(index, xiface);
+		if (res == -1) {
+			cvmx_dprintf("Failed to enable XAUI for %d:BGX(%d,%d)\n", node, interface, index);
+			return res;
+		}
+
 		smu_tx_thresh.u64 = 0;
 		/* Hopefully big enough to avoid underrun, but not too
 		* big to adversly effect shaping.
@@ -1039,11 +1013,6 @@ int __cvmx_helper_bgx_port_init(int xipd_port, int phy_pres)
 			cvmx_write_csr_node(node, CVMX_BGXX_SPUX_MISC_CONTROL(index, interface),
 					    misc_control.u64);
 		}
-		res = __cvmx_helper_bgx_xaui_init(index, xiface);
-		if (res == -1) {
-			cvmx_dprintf("Failed to enable XAUI for %d:BGX(%d,%d)\n", node, interface, index);
-			return res;
-		}
 	}
 	return 0;
 }
@@ -1261,7 +1230,7 @@ int __cvmx_helper_bgx_xaui_enable(int xiface)
 		int phy_pres;
 
 		/* Set disparity for RXAUI interface as described in the
-		Marvell RXAUI Interface specification. */
+		   Marvell RXAUI Interface specification. */
 		if (mode == CVMX_HELPER_INTERFACE_MODE_RXAUI &&
 				  (cvmx_helper_get_port_phy_present(xiface, index)))
 			phy_pres = 1;
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-board.c b/arch/mips/cavium-octeon/executive/cvmx-helper-board.c
index 0c9aec0..57f5ea4 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-board.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-board.c
@@ -90,7 +90,7 @@
 CVMX_SHARED cvmx_helper_link_info_t(*cvmx_override_board_link_get)(int ipd_port) = NULL;
 
 #ifndef CVMX_BUILD_FOR_LINUX_KERNEL
-
+/** Set this to 1 to enable lots of debugging output */
 static const int device_tree_dbg = 0;
 #endif
 
@@ -195,7 +195,8 @@ int __pip_eth_node(const void *fdt_addr, int aliases, int ipd_port)
 	}
 	pip = fdt_path_offset(fdt_addr, pip_path);
 	if (dbg)
-		cvmx_dprintf("ipdd_port=%d pip_path=%s pip=%d ", ipd_port, pip_path, pip);
+		cvmx_dprintf("ipdd_port=%d pip_path=%s pip=%d ",
+			     ipd_port, pip_path, pip);
 	if (pip < 0) {
 		cvmx_dprintf("ERROR: pip not found in device tree\n");
 		if (dbg)
@@ -272,51 +273,38 @@ static int __get_muxed_mdio_info_from_dt(cvmx_phy_info_t *phy_info,
 					 int mdio_offset, int mux_offset)
 {
 	static void *fdt_addr = 0;
-	uint32_t *psmi_handle;
 	int phandle;
-	uint32_t *pgpio_handle;
 	int smi_offset;
 	int gpio_offset;
-	uint64_t *smi_addrp;
 	uint64_t smi_addr = 0;
 	int len;
+	uint32_t *pgpio_handle;
 	int gpio_count = 0;
 	uint32_t *prop_val;
 	int offset;
 	const char *prop_name;
 
+	if (device_tree_dbg)
+		cvmx_dprintf("%s(%p, 0x%x, 0x%x)\n", __func__, phy_info,
+			     mdio_offset, mux_offset);
 	if (fdt_addr == 0)
 		fdt_addr = __cvmx_phys_addr_to_ptr(cvmx_sysinfo_get()->fdt_addr,
 						   OCTEON_FDT_MAX_SIZE);
 
-	prop_val = (uint32_t *)fdt_getprop(fdt_addr, mdio_offset, "reg", NULL);
-	if (!prop_val) {
-		cvmx_dprintf("Could not get register value for muxed MDIO bus from DT\n");
-		return -1;
-	}
 	/* Get register value to put onto the GPIO lines to select */
-	phy_info->gpio_value = fdt32_to_cpu(*prop_val);
-
-	psmi_handle = (uint32_t *)fdt_getprop(fdt_addr, mux_offset,
-					      "mdio-parent-bus", NULL);
-	if (psmi_handle == NULL) {
-		cvmx_dprintf("Could not get MDIO parent bus for multiplexed bus from device tree\n");
+	phy_info->gpio_value = cvmx_fdt_get_int(fdt_addr, mdio_offset, "reg", -1);
+	if (phy_info->gpio_value < 0) {
+		cvmx_dprintf("Could not get register value for muxed MDIO bus from DT\n");
 		return -1;
 	}
 
-	phandle = fdt32_to_cpu(*psmi_handle);
-	smi_offset = fdt_node_offset_by_phandle(fdt_addr, phandle);
+	smi_offset = cvmx_fdt_lookup_phandle(fdt_addr, mux_offset,
+					   "mdio-parent-bus");
 	if (smi_offset < 0) {
 		cvmx_dprintf("Invalid SMI offset for muxed MDIO interface in device tree\n");
 		return -1;
 	}
-	smi_addrp = (uint64_t *)fdt_getprop(fdt_addr, smi_offset, "reg", &len);
-	if ((len < (int)sizeof(uint64_t)) || smi_addrp == NULL) {
-		cvmx_dprintf("Could not get register information for SMI interface from DT\n");
-		return -1;
-	}
-	memcpy(&smi_addr, smi_addrp, sizeof(uint64_t));
-	smi_addr = fdt64_to_cpu(smi_addr);
+	smi_addr = cvmx_fdt_get_uint64(fdt_addr, smi_offset, "reg", 0);
 
 	/* Convert SMI address to a MDIO interface */
 	switch (smi_addr) {
@@ -510,7 +498,7 @@ static int __cvmx_helper_dt_process_mdio_mux(void *fdt_addr, int mdio_offset,
 	}
 	mux_info->direct_connect = 0;
 
-	smi_offset = cvmx_fdt_lookup_phandle(fdt_addr, mux_offset,
+	smi_offset = (fdt_addr, mux_offset,
 					     "mdio-parent-bus");
 	if (smi_offset < 0) {
 		cvmx_printf("Could not get parent mdio bus\n");
@@ -972,20 +960,15 @@ int __cvmx_helper_board_get_port_from_dt(void *fdt_addr, int ipd_port)
 int __get_phy_info_from_dt(cvmx_phy_info_t *phy_info, int ipd_port)
 {
 	static void *fdt_addr = 0;
-	uint32_t *phy_handle;
-	int aliases, eth, phy, phy_parent, phandle, ret, len, i;
+	int aliases, eth, phy, phy_parent, ret, i;
 	int mdio_parent;
 	const char *phy_compatible_str;
 	const char *host_mode_str = NULL;
-	uint32_t *phy_addr_ptr;
-	uint32_t *psmi_handle;
-	int smi_offset;
-	uint64_t *smi_addrp;
-	uint64_t smi_addr = 0;
 	int dbg = device_tree_dbg;
 	int interface;
+	int phy_addr_offset = 0;
 
-	if (device_tree_dbg)
+	if (dbg)
 		cvmx_dprintf("%s(%p, %d)\n", __func__, phy_info, ipd_port);
 
 	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
@@ -995,8 +978,6 @@ int __get_phy_info_from_dt(cvmx_phy_info_t *phy_info, int ipd_port)
 		fdt_addr = __cvmx_phys_addr_to_ptr(cvmx_sysinfo_get()->fdt_addr,
 						   OCTEON_FDT_MAX_SIZE);
 
-	if (device_tree_dbg)
-		cvmx_dprintf("%s(%p, %d)\n", __func__, phy_info, ipd_port);
 	phy_info->phy_addr = -1;
 	phy_info->phy_sub_addr = 0;
 	phy_info->ipd_port = ipd_port;
@@ -1034,12 +1015,15 @@ int __get_phy_info_from_dt(cvmx_phy_info_t *phy_info, int ipd_port)
 
 	interface = cvmx_helper_get_interface_num(ipd_port);
 	/* Get handle to phy */
-	phy_handle = (uint32_t *) fdt_getprop(fdt_addr, eth, "phy-handle", NULL);
-	if (!phy_handle) {
+	phy = cvmx_fdt_lookup_phandle(fdt_addr, eth, "phy-handle");
+	if (phy < 0) {
 		cvmx_helper_interface_mode_t if_mode;
 		/* Note that it's OK for RXAUI and ILK to not have a PHY
 		 * connected (i.e. EBB boards in loopback).
 		 */
+		if (dbg)
+			cvmx_dprintf("Cannot get phy-handle for ipd_port: %d\n",
+				     ipd_port);
 		if_mode = cvmx_helper_interface_get_mode(interface);
 		if (if_mode != CVMX_HELPER_INTERFACE_MODE_RXAUI &&
 		    if_mode != CVMX_HELPER_INTERFACE_MODE_ILK) {
@@ -1051,13 +1035,6 @@ int __get_phy_info_from_dt(cvmx_phy_info_t *phy_info, int ipd_port)
 			return -2;
 		}
 	}
-	phandle = fdt32_to_cpu(*phy_handle);
-	phy = fdt_node_offset_by_phandle(fdt_addr, phandle);
-	if (phy < 0) {
-		cvmx_dprintf("ERROR : cannot find phy for ipd_port=%d ret=%d\n",
-			     ipd_port, phy);
-		return -1;
-	}
 
 	phy_compatible_str = (const char *)fdt_getprop(fdt_addr, phy,
 						       "compatible", NULL);
@@ -1065,11 +1042,11 @@ int __get_phy_info_from_dt(cvmx_phy_info_t *phy_info, int ipd_port)
 		cvmx_dprintf("ERROR: no compatible prop in phy\n");
 		return -1;
 	}
-	if (device_tree_dbg)
+	if (dbg)
 		cvmx_dprintf("Checking compatible string \"%s\" for ipd port %d\n",
 			     phy_compatible_str, ipd_port);
 	if (!memcmp("marvell", phy_compatible_str, strlen("marvell"))) {
-		if (device_tree_dbg)
+		if (dbg)
 			cvmx_dprintf("Marvell PHY detected for ipd_port %d\n",
 				     ipd_port);
 		phy_info->phy_type = MARVELL_GENERIC_PHY;
@@ -1077,25 +1054,25 @@ int __get_phy_info_from_dt(cvmx_phy_info_t *phy_info, int ipd_port)
 	} else if (!memcmp("broadcom", phy_compatible_str, strlen("broadcom"))) {
 		phy_info->phy_type = BROADCOM_GENERIC_PHY;
 		phy_info->link_function = __get_broadcom_phy_link_state;
-		if (device_tree_dbg)
+		if (dbg)
 			cvmx_dprintf("Broadcom PHY detected for ipd_port %d\n",
 				     ipd_port);
 	} else if (!memcmp("vitesse", phy_compatible_str, strlen("vitesse"))) {
-		if (device_tree_dbg)
+		if (dbg)
 			cvmx_dprintf("Vitesse PHY detected for ipd_port %d\n",
 				     ipd_port);
 		if (!fdt_node_check_compatible(fdt_addr, phy,
 					       "ethernet-phy-ieee802.3-c22")) {
 			phy_info->phy_type = GENERIC_8023_C22_PHY;
 			phy_info->link_function =
-					__get_generic_8023_c45_phy_link_state;
-			if (device_tree_dbg)
+					__cvmx_get_generic_8023_c22_phy_link_state;
+			if (dbg)
 				cvmx_dprintf("Vitesse 802.3 c22 detected\n");
 		} else {
 			phy_info->phy_type = GENERIC_8023_C45_PHY;
 			phy_info->link_function =
-				__cvmx_get_generic_8023_c22_phy_link_state;
-			if (device_tree_dbg)
+				__get_generic_8023_c45_phy_link_state;
+			if (dbg)
 				cvmx_dprintf("Vitesse 802.3 c45 detected\n");
 		}
 	} else if (!memcmp("cortina", phy_compatible_str, strlen("cortina"))) {
@@ -1104,13 +1081,13 @@ int __get_phy_info_from_dt(cvmx_phy_info_t *phy_info, int ipd_port)
 		host_mode_str = (const char *)fdt_getprop(fdt_addr, phy,
 							  "cortina,host-mode",
 							  NULL);
-		if (device_tree_dbg)
+		if (dbg)
 			cvmx_dprintf("Cortina PHY detected for ipd_port %d\n",
 				     ipd_port);
 	} else if (!memcmp("ti", phy_compatible_str, strlen("ti"))) {
 		phy_info->phy_type = GENERIC_8023_C45_PHY;
 		phy_info->link_function = __get_generic_8023_c45_phy_link_state;
-		if (device_tree_dbg)
+		if (dbg)
 			cvmx_dprintf("TI PHY detected for ipd_port %d\n",
 				     ipd_port);
 	} else if (!fdt_node_check_compatible(fdt_addr, phy, "atheros,ar8334") ||
@@ -1119,20 +1096,20 @@ int __get_phy_info_from_dt(cvmx_phy_info_t *phy_info, int ipd_port)
 		   !fdt_node_check_compatible(fdt_addr, phy, "qualcomm,qca8337")) {
 		phy_info->phy_type = QUALCOMM_S17;
 		phy_info->link_function = __cvmx_get_qualcomm_s17_phy_link_state;
-		if (device_tree_dbg)
+		if (dbg)
 			cvmx_dprintf("Qualcomm QCA833X switch detected\n");
 	} else if (!fdt_node_check_compatible(fdt_addr, phy,
 					      "ethernet-phy-ieee802.3-c22")) {
 		phy_info->phy_type = GENERIC_8023_C22_PHY;
 		phy_info->link_function =
 				__cvmx_get_generic_8023_c22_phy_link_state;
-		if (device_tree_dbg)
+		if (dbg)
 			cvmx_dprintf("Generic 802.3 c22 PHY detected\n");
 	} else if (!fdt_node_check_compatible(fdt_addr, phy,
 					      "ethernet-phy-ieee802.3-c45")) {
 		phy_info->phy_type = GENERIC_8023_C45_PHY;
 		phy_info->link_function = __get_generic_8023_c45_phy_link_state;
-		if (device_tree_dbg)
+		if (dbg)
 			cvmx_dprintf("Generic 802.3 c45 PHY detected\n");
 	} else {
 		cvmx_dprintf("Unknown PHY compatibility\n");
@@ -1166,8 +1143,12 @@ int __get_phy_info_from_dt(cvmx_phy_info_t *phy_info, int ipd_port)
 	/* For multi-phy devices and devices on a MUX, go to the parent */
 	ret = fdt_node_check_compatible(fdt_addr, phy_parent,
 					"ethernet-phy-nexus");
-	if (ret == 0)
+	if (ret == 0) {
+		/* It's a nexus so check the grandparent. */
+		phy_addr_offset = cvmx_fdt_get_int(fdt_addr, phy_parent,
+						 "reg", 0);
 		phy_parent = fdt_parent_offset(fdt_addr, phy_parent);
+	}
 
 	/* Check for a muxed MDIO interface */
 	mdio_parent = fdt_parent_offset(fdt_addr, phy_parent);
@@ -1176,36 +1157,18 @@ int __get_phy_info_from_dt(cvmx_phy_info_t *phy_info, int ipd_port)
 	if (ret == 0) {
 		ret = __get_muxed_mdio_info_from_dt(phy_info, phy_parent,
 						    mdio_parent);
-		/* Find the parent MDIO bus */
-		psmi_handle = (uint32_t *)fdt_getprop(fdt_addr, mdio_parent,
-						      "mdio-parent-bus", NULL);
-		if (psmi_handle) {
-			phandle = fdt32_to_cpu(*psmi_handle);
-			smi_offset = fdt_node_offset_by_phandle(fdt_addr,
-								phandle);
-			if (smi_offset > 0) {
-				smi_addrp = (uint64_t *)fdt_getprop(fdt_addr,
-								    smi_offset,
-								    "reg",
-								    &len);
-				if (smi_addrp != NULL && len > 8) {
-					memcpy(&smi_addr, smi_addrp,
-					       sizeof(uint64_t));
-					smi_addr = fdt64_to_cpu(smi_addr);
-				}
-			} else {
-				cvmx_dprintf("Could not find SMI handler for mux\n");
-			}
-		} else {
-			cvmx_dprintf("%s: Could not get parent mdio bus\n",
-				     __func__);
+		if (ret) {
+			printf("Error reading mdio mux information for ipd port %d\n",
+			       ipd_port);
+			return -1;
 		}
-		/* Find the GPIO MUX controller */
 	}
 	ret = fdt_node_check_compatible(fdt_addr, phy_parent,
 					"cavium,octeon-3860-mdio");
 	if (ret == 0) {
-		uint32_t *mdio_reg_base = (uint32_t *) fdt_getprop(fdt_addr, phy_parent, "reg", 0);
+		uint32_t *mdio_reg_base = (uint32_t *) fdt_getprop(fdt_addr,
+								   phy_parent,
+								   "reg", 0);
 		phy_info->direct_connect = 1;
 		if (mdio_reg_base == 0) {
 			cvmx_dprintf("ERROR : unable to get reg property in phy mdio\n");
@@ -1230,13 +1193,16 @@ int __get_phy_info_from_dt(cvmx_phy_info_t *phy_info, int ipd_port)
 				      __func__, phy_info->phy_addr);
 	}
 
-	phy_addr_ptr = (uint32_t *) fdt_getprop(fdt_addr, phy, "reg", NULL);
-	if (!phy_addr_ptr) {
+	phy_info->phy_addr = cvmx_fdt_get_int(fdt_addr, phy, "reg", -1);
+	if (phy_info->phy_addr < 0) {
 		cvmx_dprintf("ERROR: Could not read phy address from reg in DT\n");
 		return -1;
 	}
-	phy_info->phy_addr = fdt32_to_cpu(*phy_addr_ptr) |
-				phy_info->mdio_unit << 8;
+	phy_info->phy_addr += phy_addr_offset;
+	phy_info->phy_addr |= phy_info->mdio_unit << 8;
+	if (dbg)
+		cvmx_dprintf("%s(%p, %d) => 0x%x\n", __func__,
+			     phy_info, ipd_port, phy_info->phy_addr);
 	return phy_info->phy_addr;
 }
 
@@ -2120,6 +2086,7 @@ cvmx_helper_link_info_t __cvmx_helper_board_link_get_from_dt(int ipd_port)
 			case CVMX_HELPER_INTERFACE_MODE_GMII:
 			case CVMX_HELPER_INTERFACE_MODE_SGMII:
 			case CVMX_HELPER_INTERFACE_MODE_QSGMII:
+			case CVMX_HELPER_INTERFACE_MODE_AGL:
 			case CVMX_HELPER_INTERFACE_MODE_SPI:
 				result.s.speed = 1000;
 				break;
@@ -2129,6 +2096,7 @@ cvmx_helper_link_info_t __cvmx_helper_board_link_get_from_dt(int ipd_port)
 				result.s.speed = 10000;
 				break;
 			case CVMX_HELPER_INTERFACE_MODE_XFI:
+			case CVMX_HELPER_INTERFACE_MODE_XLAUI:
 			case CVMX_HELPER_INTERFACE_MODE_40G_KR4:
 				result.s.speed = 40000;
 				break;
@@ -2779,7 +2747,6 @@ cvmx_helper_board_usb_clock_types_t __cvmx_helper_board_usb_get_clock_type(void)
 	return USB_CLOCK_TYPE_REF_48;
 }
 EXPORT_SYMBOL(__cvmx_helper_board_usb_get_clock_type);
-
 /**
  * @INTERNAL
  * Adjusts the number of available USB ports on Octeon based on board
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c b/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c
index 4832c43..a1cc29e 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c
@@ -472,8 +472,10 @@ void cvmx_helper_cfg_init_pko_port_map(void)
 		for (j = 0; j < cvmx_helper_interface_enumerate(i); j++) {
 			pko_port_base = cvmx_cfg_port[0][i][j].ccpp_pko_port_base;
 			pko_port_max = pko_port_base + cvmx_cfg_port[0][i][j].ccpp_pko_num_ports;
-			cvmx_helper_cfg_assert(pko_port_base != CVMX_HELPER_CFG_INVALID_VALUE);
-			cvmx_helper_cfg_assert(pko_port_max >= pko_port_base);
+			if (!octeon_has_feature(OCTEON_FEATURE_PKO3)) {
+				cvmx_helper_cfg_assert(pko_port_base != CVMX_HELPER_CFG_INVALID_VALUE);
+				cvmx_helper_cfg_assert(pko_port_max >= pko_port_base);
+			}
 			for (k = pko_port_base; k < pko_port_max; k++) {
 				cvmx_cfg_pko_port_map[k].ccppl_interface = i;
 				cvmx_cfg_pko_port_map[k].ccppl_index = j;
@@ -493,7 +495,8 @@ void cvmx_helper_cfg_init_pko_port_map(void)
 	/*
 	 * Legal pko_eids [0, 0x13] should not be exhausted.
 	 */
-	cvmx_helper_cfg_assert(pko_eid <= 0x14);
+	if (!octeon_has_feature(OCTEON_FEATURE_PKO3))
+		cvmx_helper_cfg_assert(pko_eid <= 0x14);
 
 	cvmx_cfg_max_pko_engines = pko_eid;
 }
@@ -700,7 +703,7 @@ int cvmx_helper_cfg_ipd2pko_port_num(int ipd_port)
 	int ipd_y, ipd_x;
 
 	ipd_y = IPD2PKO_CACHE_Y(ipd_port);
-	ipd_x = IPD2PKO_CACHE_X(ipd_port);
+	ipd_x = __cvmx_helper_cfg_ipd2pko_cachex(ipd_port);
 
 	return ipd2pko_port_cache[ipd_y][ipd_x].ccppp_nports;
 }
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-ilk.c b/arch/mips/cavium-octeon/executive/cvmx-helper-ilk.c
index edce8bd..3071b44 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-ilk.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-ilk.c
@@ -781,7 +781,7 @@ retry:
 			 */
 			cvmx_write_csr(CVMX_ILK_RXX_INT_EN(interface), 0x1e2);
 		}
-		/* FIXME: Enable ILK interrupts for 78xx */
+		/* Need to enable ILK interrupts for 78xx */
 
 		for (i = 0; i < CVMX_ILK_MAX_LANES(); i++) {
 			if ((1 << i) & lane_mask) {
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-pki.c b/arch/mips/cavium-octeon/executive/cvmx-helper-pki.c
index 553b042..ebdca57 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-pki.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-pki.c
@@ -66,7 +66,7 @@
 #include "cvmx-global-resources.h"
 #endif
 
-static CVMX_SHARED int pki_helper_debug = 0;
+static CVMX_SHARED int pki_helper_debug;
 
 CVMX_SHARED bool cvmx_pki_dflt_init[CVMX_MAX_NODES] = {[0 ... CVMX_MAX_NODES-1] = 1};
 
@@ -217,7 +217,7 @@ int __cvmx_helper_pki_install_dflt_vlan(int node)
 		pcam_action.style_add = 0;
 		pcam_action.pointer_advance = 4;
 		cvmx_pki_pcam_write_entry(node, index, cl_mask,
-			pcam_input, pcam_action);/*vinita_to_do, cluster_mask*/
+			pcam_input, pcam_action);/*cluster_mask in pass2*/
 
 		index = cvmx_pki_pcam_entry_alloc(node,
 			CVMX_PKI_FIND_AVAL_ENTRY, bank, cl_mask);
@@ -228,7 +228,7 @@ int __cvmx_helper_pki_install_dflt_vlan(int node)
 		}
 		pcam_input.data = 0x88a80000;
 		cvmx_pki_pcam_write_entry(node, index, cl_mask, pcam_input,
-			pcam_action);/*vinita_to_do, cluster_mask*/
+			pcam_action);
 
 		index = cvmx_pki_pcam_entry_alloc(node,
 			CVMX_PKI_FIND_AVAL_ENTRY, bank, cl_mask);
@@ -239,7 +239,7 @@ int __cvmx_helper_pki_install_dflt_vlan(int node)
 		}
 		pcam_input.data = 0x92000000;
 		cvmx_pki_pcam_write_entry(node, index, cl_mask, pcam_input,
-			pcam_action);/*vinita_to_do, cluster_mask*/
+			pcam_action);/* cluster_mask in pass2*/
 
 		index = cvmx_pki_pcam_entry_alloc(node,
 			CVMX_PKI_FIND_AVAL_ENTRY, bank, cl_mask);
@@ -250,7 +250,7 @@ int __cvmx_helper_pki_install_dflt_vlan(int node)
 		}
 		pcam_input.data = 0x91000000;
 		cvmx_pki_pcam_write_entry(node, index, cl_mask, pcam_input,
-			pcam_action);/*vinita_to_do, cluster_mask*/
+			pcam_action);
 	}
 	return 0;
 }
@@ -538,7 +538,7 @@ void cvmx_helper_pki_enable(int node)
 int cvmx_helper_pki_port_shutdown(int ipd_port)
 {
 	/* remove pcam entries */
-	/* vinita_to_do implemet later */
+	/* implemet if needed */
 	/* __cvmx_pki_port_rsrc_free(node); */
 	return 0;
 }
@@ -573,7 +573,7 @@ int cvmx_helper_pki_get_num_qpg_entry(enum cvmx_pki_qpg_qos qpg_qos)
 		return 1;
 	else if (qpg_qos == CVMX_PKI_QPG_QOS_VLAN || qpg_qos == CVMX_PKI_QPG_QOS_MPLS)
 		return 8;
-	else if (qpg_qos == CVMX_PKI_QPG_QOS_DSA_SRC) /*vinita_to_do for higig2*/
+	else if (qpg_qos == CVMX_PKI_QPG_QOS_DSA_SRC)
 		return 32;
 	else if (qpg_qos == CVMX_PKI_QPG_QOS_DIFFSERV || qpg_qos == CVMX_PKI_QPG_QOS_HIGIG)
 		return 64;
@@ -595,7 +595,7 @@ int cvmx_helper_pki_set_qpg_entry(int node, struct cvmx_pki_qpg_config *qpg_cfg)
 
 	offset = cvmx_pki_qpg_entry_alloc(node, qpg_cfg->qpg_base, 1);
 	if (pki_helper_debug)
-		cvmx_dprintf("at offset %d \n", offset);
+		cvmx_dprintf("pki-helper:set qpg entry at offset %d \n", offset);
 	if (offset == CVMX_RESOURCE_ALREADY_RESERVED) {
 		cvmx_dprintf("INFO:setup_qpg_table: offset %d already reserved\n", qpg_cfg->qpg_base);
 		return CVMX_RESOURCE_ALREADY_RESERVED;
@@ -623,6 +623,8 @@ int cvmx_helper_pki_set_qpg_entry(int node, struct cvmx_pki_qpg_config *qpg_cfg)
  * @param ena_drop      enable tail drop.
  *			1:enable 0:disable
  * @return Zero on success. Negative on failure
+ * @note the 'node' and 'aura' arguments may be combined in the future
+ * to use a compaund cvmx_fpa3_gaura_t structure argument.
  */
 int cvmx_helper_setup_aura_qos(int node, int aura, bool ena_red, bool ena_drop,
 			       uint64_t pass_thresh, uint64_t drop_thresh,
@@ -630,7 +632,6 @@ int cvmx_helper_setup_aura_qos(int node, int aura, bool ena_red, bool ena_drop,
 {
 	cvmx_fpa3_gaura_t gaura;
 
-	/* FIXME: change upper-layer arguments to new handle types */
 	gaura = __cvmx_fpa3_gaura(node, aura);
 
 	ena_red = ena_red | ena_drop;
@@ -708,7 +709,7 @@ int __cvmx_helper_pki_qos_rsrcs(int node, struct cvmx_pki_qos_schd *qossch)
 	/* Reserve pool resources */
 	if (qossch->pool_per_qos && qossch->pool_num < 0) {
 		if (pki_helper_debug)
-			cvmx_dprintf("pki-helper:qos: setup pool %d buff_size %d blocks %d\n",
+			cvmx_dprintf("pki-helper:qos-rsrc: setup pool %d buff_size %d blocks %d\n",
 				     qossch->pool_num, (int)qossch->pool_buff_size, (int)qossch->pool_max_buff);
 
 		qossch->_pool = cvmx_fpa3_setup_fill_pool(node,
@@ -729,7 +730,7 @@ int __cvmx_helper_pki_qos_rsrcs(int node, struct cvmx_pki_qos_schd *qossch)
 	/* Reserve aura resources */
 	if (qossch->aura_per_qos && qossch->aura_num < 0) {
 		if (pki_helper_debug)
-			cvmx_dprintf("pki-helper:qos setup aura %d pool %d blocks %d\n",
+			cvmx_dprintf("pki-helper:qos-rsrc: setup aura %d pool %d blocks %d\n",
 				     qossch->aura_num, qossch->pool_num,
 				     (int)qossch->aura_buff_cnt);
 
@@ -753,12 +754,12 @@ int __cvmx_helper_pki_qos_rsrcs(int node, struct cvmx_pki_qos_schd *qossch)
 	if (qossch->sso_grp_per_qos && qossch->sso_grp < 0) {
 		rs = cvmx_sso_allocate_group(node);
 		if (rs < 0) {
-			cvmx_dprintf("pki-helper:qos ERROR: sso grp not available\n");
+			cvmx_dprintf("pki-helper:qos-rsrc: ERROR: sso grp not available\n");
 			return rs;
 		}
 		qossch->sso_grp = rs;
 		if (pki_helper_debug)
-			cvmx_dprintf("pki-helper:qos: sso grp alloced is %d\n", qossch->sso_grp);
+			cvmx_dprintf("pki-helper:qos-rsrc: sso grp alloced is %d\n", qossch->sso_grp);
 	}
 #endif /* CVMX_BUILD_FOR_LINUX_KERNEL */
 	return 0;
@@ -772,7 +773,7 @@ int __cvmx_helper_pki_port_rsrcs(int node, struct cvmx_pki_prt_schd *prtsch)
 	/* Reserve pool resources */
 	if (prtsch->pool_per_prt && prtsch->pool_num < 0) {
 		if (pki_helper_debug)
-			cvmx_dprintf("pki-helper:port setup pool %d buff_size %d blocks %d\n",
+			cvmx_dprintf("pki-helper:port-rsrc: setup pool %d buff_size %d blocks %d\n",
 				     prtsch->pool_num, (int)prtsch->pool_buff_size, (int)prtsch->pool_max_buff);
 
 		prtsch->_pool = cvmx_fpa3_setup_fill_pool(node,
@@ -791,7 +792,7 @@ int __cvmx_helper_pki_port_rsrcs(int node, struct cvmx_pki_prt_schd *prtsch)
 	/* Reserve aura resources */
 	if (prtsch->aura_per_prt && prtsch->aura_num < 0) {
 		if (pki_helper_debug)
-			cvmx_dprintf("pki-helper:port setup aura %d pool %d blocks %d\n",
+			cvmx_dprintf("pki-helper:port-rsrc; setup aura %d pool %d blocks %d\n",
 				     prtsch->aura_num, prtsch->pool_num, (int)prtsch->aura_buff_cnt);
 		prtsch->_aura = cvmx_fpa3_set_aura_for_pool(prtsch->_pool,
 			prtsch->aura_num, prtsch->aura_name,
@@ -803,7 +804,6 @@ int __cvmx_helper_pki_port_rsrcs(int node, struct cvmx_pki_prt_schd *prtsch)
 				__func__, prtsch->aura_num);
 			return -1;
 		}
-
 		prtsch->aura_num = prtsch->_aura.laura;
 
 		if (pki_helper_debug)
@@ -813,12 +813,12 @@ int __cvmx_helper_pki_port_rsrcs(int node, struct cvmx_pki_prt_schd *prtsch)
 	if (prtsch->sso_grp_per_prt && prtsch->sso_grp < 0) {
 		rs = cvmx_sso_allocate_group(node);
 		if (rs < 0) {
-			cvmx_dprintf("pki-helper:port:ERROR: sso grp not available\n");
+			cvmx_printf("ERROR: %s: sso grp not available\n", __func__);
 			return rs;
 		}
 		prtsch->sso_grp = rs;
 		if (pki_helper_debug)
-			cvmx_dprintf("pki-helper:port: sso grp alloced is %d\n", prtsch->sso_grp);
+			cvmx_dprintf("pki-helper:port-rsrc: sso grp alloced is %d\n", prtsch->sso_grp);
 	}
 #endif /* CVMX_BUILD_FOR_LINUX_KERNEL */
 	return 0;
@@ -831,7 +831,7 @@ int __cvmx_helper_pki_intf_rsrcs(int node, struct cvmx_pki_intf_schd *intf)
 
 	if (intf->pool_per_intf && intf->pool_num < 0) {
 		if (pki_helper_debug)
-			cvmx_dprintf("pki-helper:intf: setup pool %d buff_size %d blocks %d\n",
+			cvmx_dprintf("pki-helper:intf-rsrc: setup pool %d buff_size %d blocks %d\n",
 				     intf->pool_num, (int)intf->pool_buff_size, (int)intf->pool_max_buff);
 		intf->_pool = cvmx_fpa3_setup_fill_pool(node, intf->pool_num,
 			intf->pool_name, intf->pool_buff_size,
@@ -850,7 +850,7 @@ int __cvmx_helper_pki_intf_rsrcs(int node, struct cvmx_pki_intf_schd *intf)
 	}
 	if (intf->aura_per_intf && intf->aura_num < 0) {
 		if (pki_helper_debug)
-			cvmx_dprintf("pki-helper:intf: setup aura %d pool %d blocks %d\n",
+			cvmx_dprintf("pki-helper:intf-rsrc: setup aura %d pool %d blocks %d\n",
 			     intf->aura_num, intf->pool_num, (int)intf->aura_buff_cnt);
 		intf->_aura = cvmx_fpa3_set_aura_for_pool(intf->_pool,
 			intf->aura_num, intf->aura_name,
@@ -872,7 +872,7 @@ int __cvmx_helper_pki_intf_rsrcs(int node, struct cvmx_pki_intf_schd *intf)
 	if (intf->sso_grp_per_intf && intf->sso_grp < 0) {
 		rs = cvmx_sso_allocate_group(node);
 		if (rs < 0) {
-			cvmx_dprintf("pki-helper:intf:ERROR: sso grp not available\n");
+			cvmx_printf("ERROR: %s: sso grp not available\n", __func__);
 			return rs;
 		}
 		intf->sso_grp = rs;
@@ -976,7 +976,7 @@ int cvmx_helper_pki_set_gbl_schd(int node, struct cvmx_pki_global_schd *gblsch)
 	if (gblsch->setup_sso_grp) {
 		rs = cvmx_sso_allocate_group(node);
 		if (rs < 0) {
-			cvmx_dprintf("pki-helper:gbl ERROR: sso grp not available\n");
+			cvmx_dprintf("pki-helper:gbl: ERROR: sso grp not available\n");
 			return rs;
 		}
 		gblsch->sso_grp = rs;
@@ -1020,12 +1020,12 @@ int cvmx_helper_pki_init_port(int ipd_port, struct cvmx_pki_prt_schd *prtsch)
 	if (prtsch->qpg_base < 0) {
 		rs = cvmx_pki_qpg_entry_alloc(xp.node, prtsch->qpg_base, num_qos);
 		if (rs < 0) {
-			cvmx_dprintf("pki-helper:port:ERROR: qpg entries not available\n");
+			cvmx_dprintf("pki-helper:port%d:ERROR: qpg entries not available\n", ipd_port);
 			return CVMX_RESOURCE_ALLOC_FAILED;
 		}
 		prtsch->qpg_base = rs;
 		if (pki_helper_debug)
-			cvmx_dprintf("port %d qpg_base %d allocated\n",
+			cvmx_dprintf("pki-helper:port-init: to port %d, qpg_base %d allocated\n",
 				ipd_port, prtsch->qpg_base);
 	}
 
@@ -1051,7 +1051,7 @@ int cvmx_helper_pki_init_port(int ipd_port, struct cvmx_pki_prt_schd *prtsch)
 			qpg_cfg.grp_bad = qossch->sso_grp;
 			cvmx_pki_write_qpg_entry(xp.node, prtsch->qpg_base + qos, &qpg_cfg);
 			if (pki_helper_debug)
-				cvmx_dprintf("port %d qos %d has port_add %d aura %d grp %d\n",
+				cvmx_dprintf("pki-helper:port-init: port %d qos %d has port_add %d aura %d grp %d\n",
 				ipd_port, qos, qossch->port_add,
 				qossch->aura_num, qossch->sso_grp);
 		}
@@ -1071,7 +1071,7 @@ int cvmx_helper_pki_init_port(int ipd_port, struct cvmx_pki_prt_schd *prtsch)
 		} else {
 			prtsch->style = rs;
 			if (pki_helper_debug)
-				cvmx_dprintf("port %d has style %d\n",
+				cvmx_dprintf("pki-helper:port-init: port %d has style %d\n",
 					ipd_port, prtsch->style);
 			style_cfg = pki_dflt_style[xp.node];
 			style_cfg.parm_cfg.qpg_qos = prtsch->qpg_qos;
@@ -1127,17 +1127,20 @@ int cvmx_helper_pki_init_interface(const int xiface, struct cvmx_pki_intf_schd *
 	has_fcs = __cvmx_helper_get_has_fcs(xiface);
 	memset(&qpg_cfg, 0, sizeof(qpg_cfg));
 
+	if (pki_helper_debug)
+		cvmx_dprintf("pki-helper:intf-init:intf%d initialize\n", xiface);
+
 	if (!intfsch->pool_per_intf) {
 		if (gblsch != NULL) {
 			intfsch->_pool = gblsch->_pool;
 			intfsch->pool_num = gblsch->pool_num;
 		} else {
-			cvmx_dprintf("ERROR: global scheduling is in use but is NULL\n");
+			cvmx_dprintf("ERROR:pki-helper:intf-init:intf%d: global scheduling is in use but is NULL\n", xiface);
 			return -1;
 		}
 	} else {
 		if (intfsch == NULL) {
-			cvmx_dprintf("ERROR: interface scheduling pointer is NULL\n");
+			cvmx_dprintf("ERROR:pki-helper:intf-init:intf%d: interface scheduling pointer is NULL\n", xiface);
 			return -1;
 		}
 		mbuff_size = intfsch->pool_buff_size;
@@ -1179,12 +1182,15 @@ int cvmx_helper_pki_init_interface(const int xiface, struct cvmx_pki_intf_schd *
 
 		/* Port is using qpg qos to schedule packets to differnet aura or sso group */
 		num_qos = cvmx_helper_pki_get_num_qpg_entry(prtsch->qpg_qos);
+		if (pki_helper_debug)
+			cvmx_dprintf("pki-helper:intf-init:intf%d: port %d used qpg_qos=%d\n",
+				     xiface, port, prtsch->qpg_qos);
 
 		/* All ports will share the aura from port 0 for the respective qos */
 		/* Port 0 should never have this set to TRUE **/
 		if (intfsch->qos_share_aura && (port != 0)) {
 			if (pki_helper_debug)
-				cvmx_dprintf("All ports will share same aura for all qos\n");
+				cvmx_dprintf("pki-helper:intf-init:intf%d All ports will share same aura for all qos\n", xiface);
 			for (qos = 0; qos < num_qos; qos++) {
 				qossch = &prtsch->qos_s[qos];
 				prtsch->qpg_qos = intfsch->prt_s[0].qpg_qos;
@@ -1199,7 +1205,7 @@ int cvmx_helper_pki_init_interface(const int xiface, struct cvmx_pki_intf_schd *
 		}
 		if (intfsch->qos_share_grp && (port != 0)) {
 			if (pki_helper_debug)
-				cvmx_dprintf("All ports will share same sso group for all qos\n");
+				cvmx_dprintf("pki-helper:intf-init:intf%d: All ports will share same sso group for all qos\n",xiface);
 			for (qos = 0; qos < num_qos; qos++) {
 				qossch = &prtsch->qos_s[qos];
 				qossch->sso_grp_per_qos =
@@ -1214,8 +1220,8 @@ int cvmx_helper_pki_init_interface(const int xiface, struct cvmx_pki_intf_schd *
 				qossch->pool_num = prtsch->pool_num;
 				qossch->_pool = prtsch->_pool;
 				if (pki_helper_debug)
-					cvmx_dprintf("qos %d pool %d\n",
-						qos, prtsch->pool_num);
+					cvmx_dprintf("pki-helper:intf-init:intf%d: qos %d has pool %d\n",
+						xiface, qos, prtsch->pool_num);
 			} else if (qossch->pool_buff_size < mbuff_size ||
 				    !mbuff_size)
 				mbuff_size = qossch->pool_buff_size;
@@ -1261,7 +1267,7 @@ int cvmx_helper_pki_init_interface(const int xiface, struct cvmx_pki_intf_schd *
 				port_shift = __cvmx_helper_pki_port_shift(xiface, intfsch->prt_s[0].qpg_qos);
 				if (pki_helper_debug) {
 					cvmx_dprintf("pki-helper: num qpg entry needed %d\n", (int)num_entry);
-					cvmx_dprintf("pki-helper:port_msb= %d port_shift=%d\n", port_msb, port_shift);
+					cvmx_dprintf("pki-helper:port_msb=%d port_shift=%d\n", port_msb, port_shift);
 				}
 				num_entry = num_qos;
 				for (port = 0; port < num_ports; port++) {
@@ -1344,7 +1350,7 @@ int cvmx_helper_pki_init_interface(const int xiface, struct cvmx_pki_intf_schd *
 				return -1;
 			}
 			mbuff_size = gblsch->pool_buff_size;
-			cvmx_dprintf("interface %d is using global pool\n", xiface);
+			cvmx_dprintf("interface %d on node %d is using global pool\n", xi.interface, xi.node);
 		}
 		/* Allocate style here and map it to all ports on interface */
 		rs = cvmx_pki_style_alloc(xi.node, intfsch->style);
@@ -1380,7 +1386,7 @@ int cvmx_helper_pki_init_interface(const int xiface, struct cvmx_pki_intf_schd *
 			pknd_cfg.fcs_pres = has_fcs;
 			cvmx_pki_write_pkind_config(xi.node, pknd, &pknd_cfg);
 		}
-	} else if (intfsch->style_per_prt) {
+	} else {
 		port_msb = 0;
 		port_shift = 0;
 		for (port = 0; port < num_ports; port++) {
@@ -1408,7 +1414,7 @@ int cvmx_helper_pki_init_interface(const int xiface, struct cvmx_pki_intf_schd *
 /**
  * This function gets all the PKI parameters related to that
  * particular port from hardware.
- * @param ipd_port	ipd port number to get parameter of
+ * @param ipd_port	ipd port number with node to get parameter of
  * @param port_cfg	pointer to structure where to store read parameters
  */
 void cvmx_pki_get_port_config(int ipd_port, struct cvmx_pki_port_config *port_cfg)
@@ -1434,7 +1440,7 @@ EXPORT_SYMBOL(cvmx_pki_get_port_config);
 /**
  * This function sets all the PKI parameters related to that
  * particular port in hardware.
- * @param ipd_port	ipd port number to get parameter of
+ * @param ipd_port	ipd port number with node to get parameter of
  * @param port_cfg	pointer to structure containing port parameters
  */
 void cvmx_pki_set_port_config(int ipd_port, struct cvmx_pki_port_config *port_cfg)
@@ -1679,7 +1685,7 @@ void cvmx_helper_pki_set_fcs_op(int node, int interface, int nports, int has_fcs
 	for (index = 0; index < nports; index++) {
 		pknd = cvmx_helper_get_pknd(interface, index);
 		while (cluster < CVMX_PKI_NUM_CLUSTER) {
-			/*vinita_to_do; find the cluster in use*/
+			/*find the cluster in use pass2*/
 			pkind_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_CFG(pknd, cluster));
 			pkind_cfg.s.fcs_pres = has_fcs;
 			cvmx_write_csr_node(node, CVMX_PKI_CLX_PKINDX_CFG(pknd, cluster), pkind_cfg.u64);
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-pko.c b/arch/mips/cavium-octeon/executive/cvmx-helper-pko.c
index 5643c4f..0d0f82a 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-pko.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-pko.c
@@ -42,7 +42,7 @@
  *
  * Helper Functions for the PKO
  *
- * $Id: cvmx-helper-pko.c 103836 2014-09-03 02:00:52Z lrosenboim $
+ * $Id: cvmx-helper-pko.c 106502 2014-10-22 16:28:44Z cchavva $
  */
 
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
@@ -143,7 +143,7 @@ static int cvmx_helper_pko_pool_init(void)
 			"pool %d already initialized\n",
 			__func__, pool);
 #endif
-		/* FIXME: Should check available buffer count */
+		/* It is up to the app to have sufficient buffer count */
 		return pool;
 	}
 
@@ -201,7 +201,7 @@ int cvmx_helper_pko_init(void)
  *
  * @return Zero on success, negative on failure
  *
- * FIXME: This is for PKO1 only.
+ * @note This is for PKO1/PKO2, and is not used for PKO3.
  */
 int __cvmx_helper_interface_setup_pko(int interface)
 {
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-pko3.c b/arch/mips/cavium-octeon/executive/cvmx-helper-pko3.c
index a2bab6c..ae77de4 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-pko3.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-pko3.c
@@ -276,9 +276,6 @@ static int __cvmx_pko3_config_ilk_interface(int xiface,
 		/* map channels to l2 queues */
 		cvmx_pko3_map_channel(xi.node, l1_q_num, l2_q_num+i, ipd_port);
 
-		//FIXME- can not convert it to a loop because
-		// of CVMX_PKO_Lx_QUEUES are enumerated
-
 		l3_q = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_L3_QUEUES,
 			res_owner, -1, 1);
 		if(l3_q < 0) goto _fail;
@@ -322,8 +319,8 @@ static int __cvmx_pko3_config_ilk_interface(int xiface,
 /** Initialize a channelized port
  * This is intended for LOOP and NPI interfaces which have one MAC
  * per interface and need a channel per subinterface (e.g. ring).
- *
- * FIXME: Consider merging this function with the ILK configuration code
+ * This function is somewhat similar to __cvmx_pko3_config_ilk_interface()
+ * but are kept separate for easier maintenance.
  */
 static int __cvmx_pko3_config_chan_interface( int xiface, unsigned num_chans,
 	uint8_t num_queues, bool prioritized)
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.c b/arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.c
index 68238f2..7832f55 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.c
@@ -43,7 +43,7 @@
  * Functions for SGMII initialization, configuration,
  * and monitoring.
  *
- * <hr>$Revision: 102466 $<hr>
+ * <hr>$Revision: 105303 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
@@ -156,9 +156,7 @@ static int __cvmx_helper_sgmii_hardware_init_one_time(int interface, int index)
 static int __cvmx_helper_need_g15618(void)
 {
 	if (cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_SIM ||
-	    OCTEON_IS_MODEL(OCTEON_CN63XX_PASS1_X) ||
-	    OCTEON_IS_MODEL(OCTEON_CN63XX_PASS2_0) ||
-	    OCTEON_IS_MODEL(OCTEON_CN63XX_PASS2_1) ||
+	    OCTEON_IS_MODEL(OCTEON_CN63XX) ||
 	    OCTEON_IS_MODEL(OCTEON_CN66XX_PASS1_X) ||
 	    OCTEON_IS_MODEL(OCTEON_CN68XX_PASS1_X))
 		return 1;
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-util.c b/arch/mips/cavium-octeon/executive/cvmx-helper-util.c
index 025f397..0649216 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-util.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-util.c
@@ -364,7 +364,7 @@ static int __cvmx_ipd_mode_no_wptr(void)
 }
 
 static CVMX_TLS cvmx_buf_ptr_t __cvmx_packet_short_ptr[4];
-static CVMX_TLS uint8_t __cvmx_wqe_pool;
+static CVMX_TLS int8_t __cvmx_wqe_pool = -1;
 
 /**
  * @INTERNAL
@@ -560,26 +560,26 @@ cvmx_buf_ptr_t cvmx_wqe_get_packet_ptr(cvmx_wqe_t *work)
 
 void cvmx_wqe_free(cvmx_wqe_t *work)
 {
-	unsigned ncl = 1;
-	cvmx_wqe_78xx_t * wqe = (void *) work;
+	unsigned bufs, ncl = 1;
+	uint64_t paddr, paddr1;
 
-	/* Free native untranslated 78xx WQE */
-	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE) &&
-		!wqe->pki_wqe_translated) {
+	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
+		cvmx_wqe_78xx_t * wqe = (void *) work;
+		cvmx_fpa3_gaura_t aura;
 		cvmx_buf_ptr_pki_t bptr;
 
-		bptr = wqe->packet_ptr;
+		bufs = wqe->word0.bufs;
 
-		/* Do nothing if the first packet buffer shares WQE buffer */
-		if (!bptr.packet_outside_wqe)
-			return;
-	} else {
-		uint64_t paddr, paddr1;
+		if (!wqe->pki_wqe_translated && bufs != 0) {
+			/* Handle cn78xx native untralsated WQE */
 
-		/* check for unconverted RS */
-		if (cvmx_likely(work->word2.s_cn38xx.bufs != 0)) {
+			bptr = wqe->packet_ptr;
 
-			/* Check if the first data buffer is inside WQE */
+			/* Do nothing - first packet buffer shares WQE buffer */
+			if (!bptr.packet_outside_wqe)
+				return;
+		} else if (cvmx_likely(bufs != 0)) {
+			/* Handle translated 78XX WQE */
 			paddr = (work->packet_ptr.s.addr & (~0x7full)) -
 				(work->packet_ptr.s.back << 7);
 			paddr1 = cvmx_ptr_to_phys(work);
@@ -589,20 +589,31 @@ void cvmx_wqe_free(cvmx_wqe_t *work)
 				return;
 		}
 
-		cvmx_fpa1_free(work, __cvmx_wqe_pool, ncl);
-		return;
-	}
-
-	/* At this point it is clear the WQE needs to be freed */
-	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
-		cvmx_fpa3_gaura_t aura =
-			__cvmx_fpa3_gaura(
+		/* WQE is separate from packet buffer, free it */
+		aura = __cvmx_fpa3_gaura(
 				wqe->word0.aura >> 10,
 				wqe->word0.aura * 0x3ff);
-		/* First buffer outside WQE, but WQE comes from the same AURA */
-		/* Only a few words have been touched, not entire buf */
-		ncl = 1;
+
 		cvmx_fpa3_free(work, aura, ncl);
+	} else {
+		/* handle legacy WQE */
+		bufs = work->word2.s_cn38xx.bufs;
+
+		if (cvmx_likely(bufs != 0)) {
+			/* Check if the first data buffer is inside WQE */
+			paddr = (work->packet_ptr.s.addr & (~0x7full)) -
+				(work->packet_ptr.s.back << 7);
+			paddr1 = cvmx_ptr_to_phys(work);
+
+			/* do not free WQE if contains first data buffer */
+			if (paddr == paddr1)
+				return;
+		}
+
+		/* precalculate packet_ptr, WQE pool number */
+		if (cvmx_unlikely(__cvmx_wqe_pool < 0))
+			cvmx_packet_short_ptr_calculate();
+		cvmx_fpa1_free(work, __cvmx_wqe_pool, ncl);
 	}
 }
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-xaui.c b/arch/mips/cavium-octeon/executive/cvmx-helper-xaui.c
index 8cdd9b5..0953b5b 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-xaui.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-xaui.c
@@ -43,7 +43,7 @@
  * Functions for XAUI initialization, configuration,
  * and monitoring.
  *
- * <hr>$Revision: 100545 $<hr>
+ * <hr>$Revision: 106617 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
@@ -77,7 +77,6 @@ int __cvmx_helper_xaui_enumerate(int xiface)
 		if (qlm_mode == CVMX_QLM_MODE_RXAUI)
 			return 1;
 		return 0;
-		/* FIXME for higig2 */
 	}
 	/* If HiGig2 is enabled return 16 ports, otherwise return 1 port */
 	gmx_hg2_control.u64 = cvmx_read_csr(CVMX_GMXX_HG2_CONTROL(interface));
@@ -238,9 +237,7 @@ int __cvmx_helper_xaui_link_init(int interface)
 	 * Errata G-15618 requires disabling PCS soft reset in some
 	 * OCTEON II models.
 	 */
-	if (!OCTEON_IS_MODEL(OCTEON_CN63XX_PASS1_X) &&
-	    !OCTEON_IS_MODEL(OCTEON_CN63XX_PASS2_0) &&
-	    !OCTEON_IS_MODEL(OCTEON_CN63XX_PASS2_1) &&
+	if (!OCTEON_IS_MODEL(OCTEON_CN63XX) &&
 	    !OCTEON_IS_MODEL(OCTEON_CN66XX_PASS1_X) &&
 	    !OCTEON_IS_MODEL(OCTEON_CN68XX))
 		xaui_ctl.s.reset = 1;
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper.c b/arch/mips/cavium-octeon/executive/cvmx-helper.c
index b219e16..348d2ad 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper.c
@@ -57,6 +57,7 @@
 #include <asm/octeon/cvmx-pip-defs.h>
 #include <asm/octeon/cvmx-asxx-defs.h>
 #include <asm/octeon/cvmx-gmxx-defs.h>
+#include <asm/octeon/cvmx-gserx-defs.h>
 #include <asm/octeon/cvmx-smix-defs.h>
 #include <asm/octeon/cvmx-dbg-defs.h>
 #include <asm/octeon/cvmx-sso-defs.h>
@@ -804,7 +805,15 @@ static cvmx_helper_interface_mode_t __cvmx_get_mode_cn78xx(int xiface)
 				iface_node_ops[xi.node][xi.interface] = &iface_ops_dis;
 		}
 	} else if (xi.interface == 8) { /* DPI */
-		iface_node_ops[xi.node][xi.interface] = &iface_ops_npi;
+		int qlm = 0;
+		for (qlm = 0; qlm < 5; qlm++) {
+			/* if GSERX_CFG[pcie] == 1, then enable npi */
+			if (cvmx_read_csr_node(xi.node, CVMX_GSERX_CFG(qlm)) & 0x1) {
+				iface_node_ops[xi.node][xi.interface] = &iface_ops_npi;
+				return iface_node_ops[xi.node][xi.interface]->mode;
+			}
+		}
+		iface_node_ops[xi.node][xi.interface] = &iface_ops_dis;
 	} else if (xi.interface == 9) { /* LOOP */
 		iface_node_ops[xi.node][xi.interface] = &iface_ops_loop;
 	} else
@@ -1514,10 +1523,6 @@ int cvmx_helper_ipd_and_packet_input_enable_node(int node)
 	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
 		; // cvmx_pko_enable_78xx(0); already enabled
 	} else {
-		/* FIXME:
-		 * 		 This call was in cvmx-pko.c,
-		 * 		 not sure if this is right either
-		 */
 #ifdef CVMX_BUILD_FOR_STANDALONE
 		__cvmx_install_gmx_error_handler_for_xaui();
 #endif
@@ -1610,7 +1615,6 @@ int cvmx_helper_initialize_packet_io_node(unsigned int node)
 
 	/* PKO3 init precedes that of interfaces */
 	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
-		//FIXME- ILK needs this config data for now - must fix!
 		__cvmx_helper_init_port_config_data();
 		result = cvmx_helper_pko3_init_global(node);
 	}
@@ -1630,7 +1634,7 @@ int cvmx_helper_initialize_packet_io_node(unsigned int node)
 			    cvmx_helper_ports_on_interface(interface),
 			    cvmx_helper_interface_mode_to_string(cvmx_helper_interface_get_mode(interface)));
 
-		result |= __cvmx_helper_ipd_setup_interface(interface);/* vinita_to_do separate pki */
+		result |= __cvmx_helper_ipd_setup_interface(interface);
 		if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE))
 			result |= cvmx_helper_pko3_init_interface(cvmx_helper_node_interface_to_xiface(node, interface));
 		else
@@ -1839,7 +1843,6 @@ static int __cvmx_helper_shutdown_packet_io_global_cn78xx(int node)
 			for (index = 0; index < num_ports; index++) {
 				if (!cvmx_helper_is_port_valid(interface, index))
 					continue;
-		//FIXME: Move this code to cvmx-bgxx.c
 				/* Disable GMX before we make any changes. Remember the enable state */
 				cmr_config.u64 = cvmx_read_csr(CVMX_BGXX_CMRX_CONFIG(index, interface));
 				cmr_config.s.enable = 0;
@@ -2503,7 +2506,7 @@ void cvmx_helper_setup_simulator_io_buffer_counts(int node, int num_packet_buffe
 void *cvmx_helper_mem_alloc(int node, uint64_t alloc_size, uint64_t align)
 {
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
-	return kmalloc(alloc_size, GFP_NOIO | GFP_DMA); //FIXME alignment
+	return kmalloc(alloc_size, GFP_NOIO | GFP_DMA);
 #else
 	return cvmx_phys_to_ptr(cvmx_bootmem_phy_alloc_range(alloc_size, align,
 							     cvmx_addr_on_node(node, 0ull),
diff --git a/arch/mips/cavium-octeon/executive/cvmx-ipd.c b/arch/mips/cavium-octeon/executive/cvmx-ipd.c
index 7bdb397..71cf0f6 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-ipd.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-ipd.c
@@ -85,7 +85,6 @@ CVMX_SHARED cvmx_ipd_config_t cvmx_ipd_cfg = {.first_mbuf_skip = 184,
 					};
 EXPORT_SYMBOL(cvmx_ipd_cfg);
 
-/* FIXME- review these values, convert to params ? */
 #define IPD_RED_AVG_DLY	1000
 #define IPD_RED_PRB_DLY	1000
 
@@ -525,7 +524,6 @@ int cvmx_ipd_setup_red(int pass_thresh, int drop_thresh)
 	int interface;
 	int port;
 
-	/*vinita_to_do modify for 78xx*/
 	if (octeon_has_feature(OCTEON_FEATURE_PKI))
 		return -1;
 	/*
diff --git a/arch/mips/cavium-octeon/executive/cvmx-l2c.c b/arch/mips/cavium-octeon/executive/cvmx-l2c.c
index c5ef6b7..0314338 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-l2c.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-l2c.c
@@ -43,7 +43,7 @@
  * Implementation of the Level 2 Cache (L2C) control,
  * measurement, and debugging facilities.
  *
- * <hr>$Revision: 97778 $<hr>
+ * <hr>$Revision: 106657 $<hr>
  *
  */
 
@@ -1062,6 +1062,7 @@ int cvmx_l2c_get_num_sets(void)
 int cvmx_l2c_get_num_assoc(void)
 {
 	int l2_assoc;
+
 	if (OCTEON_IS_MODEL(OCTEON_CN56XX)
 		|| OCTEON_IS_MODEL(OCTEON_CN52XX)
 		|| OCTEON_IS_MODEL(OCTEON_CN58XX)
@@ -1202,10 +1203,11 @@ void cvmx_l2c_flush_line(uint32_t assoc, uint32_t index)
  * Initialize the BIG address in L2C+DRAM to generate proper error
  * on reading/writing to an non-existant memory location.
  *
+ * @param node      OCX CPU node number
  * @param mem_size  Amount of DRAM configured in MB.
  * @param mode      Allow/Disallow reporting errors L2C_INT_SUM[BIGRD,BIGWR].
  */
-void cvmx_l2c_set_big_size(uint64_t mem_size, int mode)
+void cvmx_l2c_set_big_size_node(int node, uint64_t mem_size, int mode)
 {
 	if ((OCTEON_IS_OCTEON2() || OCTEON_IS_OCTEON3())
 	    && !OCTEON_IS_MODEL(OCTEON_CN63XX_PASS1_X)) {
@@ -1242,10 +1244,23 @@ void cvmx_l2c_set_big_size(uint64_t mem_size, int mode)
 		big_ctl.u64 = 0;
 		big_ctl.s.maxdram = bits - 9;
 		big_ctl.cn61xx.disable = mode;
-		cvmx_write_csr(CVMX_L2C_BIG_CTL, big_ctl.u64);
+		cvmx_write_csr_node(node, CVMX_L2C_BIG_CTL, big_ctl.u64);
 	}
 }
 
+/**
+ * Initialize the BIG address in L2C+DRAM to generate proper error
+ * on reading/writing to an non-existant memory location.
+ *
+ * @param mem_size  Amount of DRAM configured in MB.
+ * @param mode      Allow/Disallow reporting errors L2C_INT_SUM[BIGRD,BIGWR].
+ */
+void cvmx_l2c_set_big_size(uint64_t mem_size, int mode)
+{
+	cvmx_l2c_set_big_size_node(0, mem_size, mode);
+}
+
+
 #if !defined(CVMX_BUILD_FOR_LINUX_HOST) && !defined(CVMX_BUILD_FOR_LINUX_KERNEL)
 /* L2C Virtualization APIs. These APIs are based on Octeon II documentation. */
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pcie.c b/arch/mips/cavium-octeon/executive/cvmx-pcie.c
index b369cc5..157adad 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pcie.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pcie.c
@@ -42,7 +42,7 @@
  *
  * Interface to PCIe as a host(RC) or target(EP)
  *
- * <hr>$Revision: 103829 $<hr>
+ * <hr>$Revision: 106368 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
@@ -109,6 +109,11 @@
 #define _CVMX_PCIE_ES 0
 #endif
 
+#define CVMX_READ_CSR(addr)		cvmx_read_csr_node(node,addr)
+#define CVMX_WRITE_CSR(addr,val)	cvmx_write_csr_node(node,addr,val)
+#define CVMX_PCIE_CFGX_READ(p,addr)	cvmx_pcie_cfgx_read_node(node,p,addr)
+#define CVMX_PCIE_CFGX_WRITE(p,addr,val)	cvmx_pcie_cfgx_write_node(node,p,addr,val)
+
 /**
  * Return the Core virtual base address for PCIe IO access. IOs are
  * read/written as an offset from this address.
@@ -125,8 +130,9 @@ uint64_t cvmx_pcie_get_io_base_address(int pcie_port)
 	pcie_addr.io.io = 1;
 	pcie_addr.io.did = 3;
 	pcie_addr.io.subdid = 2;
+	pcie_addr.io.node = (pcie_port >> 4) & 0x3;
 	pcie_addr.io.es = _CVMX_PCIE_ES;
-	pcie_addr.io.port = pcie_port;
+	pcie_addr.io.port = (pcie_port & 0x3);
 	return pcie_addr.u64;
 }
 
@@ -158,7 +164,8 @@ uint64_t cvmx_pcie_get_mem_base_address(int pcie_port)
 	pcie_addr.mem.upper = 0;
 	pcie_addr.mem.io = 1;
 	pcie_addr.mem.did = 3;
-	pcie_addr.mem.subdid = 3 + pcie_port;
+	pcie_addr.mem.subdid = 3 + (pcie_port & 0x3);
+	pcie_addr.mem.node = (pcie_port >> 4) & 0x3;
 	return pcie_addr.u64;
 }
 
@@ -181,7 +188,7 @@ uint64_t cvmx_pcie_get_mem_size(int pcie_port)
  *
  * @param pcie_port PCIe port to initialize
  */
-static void __cvmx_pcie_rc_initialize_config_space(int pcie_port)
+static void __cvmx_pcie_rc_initialize_config_space(int node, int pcie_port)
 {
 	/* Max Payload Size (PCIE*_CFG030[MPS]) */
 	/* Max Read Request Size (PCIE*_CFG030[MRRS]) */
@@ -189,7 +196,7 @@ static void __cvmx_pcie_rc_initialize_config_space(int pcie_port)
 	/* Error Message Enables (PCIE*_CFG030[CE_EN,NFE_EN,FE_EN,UR_EN]) */
 	{
 		cvmx_pciercx_cfg030_t pciercx_cfg030;
-		pciercx_cfg030.u32 = cvmx_pcie_cfgx_read(pcie_port,
+		pciercx_cfg030.u32 = CVMX_PCIE_CFGX_READ(pcie_port,
 							 CVMX_PCIERCX_CFG030(pcie_port));
 		if (OCTEON_IS_MODEL(OCTEON_CN5XXX)) {
 			pciercx_cfg030.s.mps = MPS_CN5XXX;
@@ -204,7 +211,7 @@ static void __cvmx_pcie_rc_initialize_config_space(int pcie_port)
 		pciercx_cfg030.s.nfe_en = 1;	/* Non-fatal error reporting enable. */
 		pciercx_cfg030.s.fe_en = 1;	/* Fatal error reporting enable. */
 		pciercx_cfg030.s.ur_en = 1;	/* Unsupported request reporting enable. */
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG030(pcie_port),
+		CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG030(pcie_port),
 				     pciercx_cfg030.u32);
 	}
 
@@ -212,7 +219,7 @@ static void __cvmx_pcie_rc_initialize_config_space(int pcie_port)
 		/* Max Payload Size (NPEI_CTL_STATUS2[MPS]) must match PCIE*_CFG030[MPS] */
 		/* Max Read Request Size (NPEI_CTL_STATUS2[MRRS]) must not exceed PCIE*_CFG030[MRRS] */
 		cvmx_npei_ctl_status2_t npei_ctl_status2;
-		npei_ctl_status2.u64 = cvmx_read_csr(CVMX_PEXP_NPEI_CTL_STATUS2);
+		npei_ctl_status2.u64 = CVMX_READ_CSR(CVMX_PEXP_NPEI_CTL_STATUS2);
 		npei_ctl_status2.s.mps = MPS_CN5XXX;	/* Max payload size = 128 bytes for best Octeon DMA performance */
 		npei_ctl_status2.s.mrrs = MRRS_CN5XXX;	/* Max read request size = 128 bytes for best Octeon DMA performance */
 		if (pcie_port)
@@ -220,32 +227,32 @@ static void __cvmx_pcie_rc_initialize_config_space(int pcie_port)
 		else
 			npei_ctl_status2.s.c0_b1_s = 3;	/* Port0 BAR1 Size 256MB */
 
-		cvmx_write_csr(CVMX_PEXP_NPEI_CTL_STATUS2, npei_ctl_status2.u64);
+		CVMX_WRITE_CSR(CVMX_PEXP_NPEI_CTL_STATUS2, npei_ctl_status2.u64);
 	} else {
 		/* Max Payload Size (DPI_SLI_PRTX_CFG[MPS]) must match PCIE*_CFG030[MPS] */
 		/* Max Read Request Size (DPI_SLI_PRTX_CFG[MRRS]) must not exceed PCIE*_CFG030[MRRS] */
 		cvmx_dpi_sli_prtx_cfg_t prt_cfg;
 		cvmx_sli_s2m_portx_ctl_t sli_s2m_portx_ctl;
-		prt_cfg.u64 = cvmx_read_csr(CVMX_DPI_SLI_PRTX_CFG(pcie_port));
+		prt_cfg.u64 = CVMX_READ_CSR(CVMX_DPI_SLI_PRTX_CFG(pcie_port));
 		prt_cfg.s.mps = MPS_CN6XXX;
 		prt_cfg.s.mrrs = MRRS_CN6XXX;
 		/* Max outstanding load request. */
 		prt_cfg.s.molr = 32;
-		cvmx_write_csr(CVMX_DPI_SLI_PRTX_CFG(pcie_port), prt_cfg.u64);
+		CVMX_WRITE_CSR(CVMX_DPI_SLI_PRTX_CFG(pcie_port), prt_cfg.u64);
 
-		sli_s2m_portx_ctl.u64 = cvmx_read_csr(CVMX_PEXP_SLI_S2M_PORTX_CTL(pcie_port));
+		sli_s2m_portx_ctl.u64 = CVMX_READ_CSR(CVMX_PEXP_SLI_S2M_PORTX_CTL(pcie_port));
 		sli_s2m_portx_ctl.cn61xx.mrrs = MRRS_CN6XXX;
-		cvmx_write_csr(CVMX_PEXP_SLI_S2M_PORTX_CTL(pcie_port), sli_s2m_portx_ctl.u64);
+		CVMX_WRITE_CSR(CVMX_PEXP_SLI_S2M_PORTX_CTL(pcie_port), sli_s2m_portx_ctl.u64);
 	}
 
 	/* ECRC Generation (PCIE*_CFG070[GE,CE]) */
 	{
 		cvmx_pciercx_cfg070_t pciercx_cfg070;
-		pciercx_cfg070.u32 = cvmx_pcie_cfgx_read(pcie_port,
+		pciercx_cfg070.u32 = CVMX_PCIE_CFGX_READ(pcie_port,
 							 CVMX_PCIERCX_CFG070(pcie_port));
 		pciercx_cfg070.s.ge = 1;	/* ECRC generation enable. */
 		pciercx_cfg070.s.ce = 1;	/* ECRC check enable. */
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG070(pcie_port),
+		CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG070(pcie_port),
 				     pciercx_cfg070.u32);
 	}
 
@@ -255,29 +262,29 @@ static void __cvmx_pcie_rc_initialize_config_space(int pcie_port)
 	/* System Error Message Enable (PCIE*_CFG001[SEE]) */
 	{
 		cvmx_pciercx_cfg001_t pciercx_cfg001;
-		pciercx_cfg001.u32 = cvmx_pcie_cfgx_read(pcie_port,
+		pciercx_cfg001.u32 = CVMX_PCIE_CFGX_READ(pcie_port,
 							 CVMX_PCIERCX_CFG001(pcie_port));
 		pciercx_cfg001.s.msae = 1;	/* Memory space enable. */
 		pciercx_cfg001.s.me = 1;	/* Bus master enable. */
 		pciercx_cfg001.s.i_dis = 1;	/* INTx assertion disable. */
 		pciercx_cfg001.s.see = 1;	/* SERR# enable */
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG001(pcie_port),
+		CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG001(pcie_port),
 				     pciercx_cfg001.u32);
 	}
 
 	/* Advanced Error Recovery Message Enables */
 	/* (PCIE*_CFG066,PCIE*_CFG067,PCIE*_CFG069) */
-	cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG066(pcie_port), 0);
+	CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG066(pcie_port), 0);
 	/* Use CVMX_PCIERCX_CFG067 hardware default */
-	cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG069(pcie_port), 0);
+	CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG069(pcie_port), 0);
 
 	/* Active State Power Management (PCIE*_CFG032[ASLPC]) */
 	{
 		cvmx_pciercx_cfg032_t pciercx_cfg032;
-		pciercx_cfg032.u32 = cvmx_pcie_cfgx_read(pcie_port,
+		pciercx_cfg032.u32 = CVMX_PCIE_CFGX_READ(pcie_port,
 							 CVMX_PCIERCX_CFG032(pcie_port));
 		pciercx_cfg032.s.aslpc = 0;	/* Active state Link PM control. */
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG032(pcie_port),
+		CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG032(pcie_port),
 				     pciercx_cfg032.u32);
 	}
 
@@ -294,7 +301,7 @@ static void __cvmx_pcie_rc_initialize_config_space(int pcie_port)
 		pciercx_cfg006.s.pbnum = 1;
 		pciercx_cfg006.s.sbnum = 1;
 		pciercx_cfg006.s.subbnum = 1;
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG006(pcie_port),
+		CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG006(pcie_port),
 				     pciercx_cfg006.u32);
 	}
 
@@ -306,7 +313,7 @@ static void __cvmx_pcie_rc_initialize_config_space(int pcie_port)
 		pciercx_cfg008.u32 = 0;
 		pciercx_cfg008.s.mb_addr = 0x100;
 		pciercx_cfg008.s.ml_addr = 0;
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG008(pcie_port),
+		CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG008(pcie_port),
 				     pciercx_cfg008.u32);
 	}
 
@@ -318,21 +325,21 @@ static void __cvmx_pcie_rc_initialize_config_space(int pcie_port)
 		cvmx_pciercx_cfg009_t pciercx_cfg009;
 		cvmx_pciercx_cfg010_t pciercx_cfg010;
 		cvmx_pciercx_cfg011_t pciercx_cfg011;
-		pciercx_cfg009.u32 = cvmx_pcie_cfgx_read(pcie_port,
+		pciercx_cfg009.u32 = CVMX_PCIE_CFGX_READ(pcie_port,
 							 CVMX_PCIERCX_CFG009(pcie_port));
-		pciercx_cfg010.u32 = cvmx_pcie_cfgx_read(pcie_port,
+		pciercx_cfg010.u32 = CVMX_PCIE_CFGX_READ(pcie_port,
 							 CVMX_PCIERCX_CFG010(pcie_port));
-		pciercx_cfg011.u32 = cvmx_pcie_cfgx_read(pcie_port,
+		pciercx_cfg011.u32 = CVMX_PCIE_CFGX_READ(pcie_port,
 							 CVMX_PCIERCX_CFG011(pcie_port));
 		pciercx_cfg009.s.lmem_base = 0x100;
 		pciercx_cfg009.s.lmem_limit = 0;
 		pciercx_cfg010.s.umem_base = 0x100;
 		pciercx_cfg011.s.umem_limit = 0;
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG009(pcie_port),
+		CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG009(pcie_port),
 				     pciercx_cfg009.u32);
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG010(pcie_port),
+		CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG010(pcie_port),
 				     pciercx_cfg010.u32);
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG011(pcie_port),
+		CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG011(pcie_port),
 				     pciercx_cfg011.u32);
 	}
 
@@ -340,13 +347,13 @@ static void __cvmx_pcie_rc_initialize_config_space(int pcie_port)
 	/* PME Interrupt Enables (PCIERCn_CFG035[PMEIE]) */
 	{
 		cvmx_pciercx_cfg035_t pciercx_cfg035;
-		pciercx_cfg035.u32 = cvmx_pcie_cfgx_read(pcie_port,
+		pciercx_cfg035.u32 = CVMX_PCIE_CFGX_READ(pcie_port,
 							 CVMX_PCIERCX_CFG035(pcie_port));
 		pciercx_cfg035.s.secee = 1;	/* System error on correctable error enable. */
 		pciercx_cfg035.s.sefee = 1;	/* System error on fatal error enable. */
 		pciercx_cfg035.s.senfee = 1;	/* System error on non-fatal error enable. */
 		pciercx_cfg035.s.pmeie = 1;	/* PME interrupt enable. */
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG035(pcie_port),
+		CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG035(pcie_port),
 				     pciercx_cfg035.u32);
 	}
 
@@ -354,12 +361,12 @@ static void __cvmx_pcie_rc_initialize_config_space(int pcie_port)
 	/* (PCIERCn_CFG075[CERE,NFERE,FERE]) */
 	{
 		cvmx_pciercx_cfg075_t pciercx_cfg075;
-		pciercx_cfg075.u32 = cvmx_pcie_cfgx_read(pcie_port,
+		pciercx_cfg075.u32 = CVMX_PCIE_CFGX_READ(pcie_port,
 							 CVMX_PCIERCX_CFG075(pcie_port));
 		pciercx_cfg075.s.cere = 1;	/* Correctable error reporting enable. */
 		pciercx_cfg075.s.nfere = 1;	/* Non-fatal error reporting enable. */
 		pciercx_cfg075.s.fere = 1;	/* Fatal error reporting enable. */
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG075(pcie_port),
+		CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG075(pcie_port),
 				     pciercx_cfg075.u32);
 	}
 
@@ -367,17 +374,17 @@ static void __cvmx_pcie_rc_initialize_config_space(int pcie_port)
 	/* PCIERCn_CFG034[DLLS_EN,CCINT_EN]) */
 	{
 		cvmx_pciercx_cfg034_t pciercx_cfg034;
-		pciercx_cfg034.u32 = cvmx_pcie_cfgx_read(pcie_port,
+		pciercx_cfg034.u32 = CVMX_PCIE_CFGX_READ(pcie_port,
 							 CVMX_PCIERCX_CFG034(pcie_port));
 		pciercx_cfg034.s.hpint_en = 1;	/* Hot-plug interrupt enable. */
 		pciercx_cfg034.s.dlls_en = 1;	/* Data Link Layer state changed enable */
 		pciercx_cfg034.s.ccint_en = 1;	/* Command completed interrupt enable. */
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG034(pcie_port),
+		CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG034(pcie_port),
 				     pciercx_cfg034.u32);
 	}
 
 	if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
-		int qlm = pcie_port; /* FIXME */
+		int qlm = pcie_port;
 		int speed = cvmx_qlm_get_gbaud_mhz(qlm);
 		cvmx_pemx_cfg_t pem_cfg;
 		cvmx_pciercx_cfg040_t cfg040;
@@ -387,39 +394,39 @@ static void __cvmx_pcie_rc_initialize_config_space(int pcie_port)
   		   its going to try */
 		switch(speed) {
 		case 2500: /* Gen1 */
-			pem_cfg.u64 = cvmx_read_csr(CVMX_PEMX_CFG(pcie_port));
+			pem_cfg.u64 = CVMX_READ_CSR(CVMX_PEMX_CFG(pcie_port));
 			pem_cfg.s.md = 0;
-			cvmx_write_csr(CVMX_PEMX_CFG(pcie_port), pem_cfg.u64);
+			CVMX_WRITE_CSR(CVMX_PEMX_CFG(pcie_port), pem_cfg.u64);
 
 			/* Set the target link speed */
-			cfg040.u32 = cvmx_pcie_cfgx_read(pcie_port,
+			cfg040.u32 = CVMX_PCIE_CFGX_READ(pcie_port,
 						CVMX_PCIERCX_CFG040(pcie_port));	
 			cfg040.s.tls = 1;
-			cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG040(pcie_port),
+			CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG040(pcie_port),
 						cfg040.u32);
 			break;
 		case 5000: /* Gen2 */
-			pem_cfg.u64 = cvmx_read_csr(CVMX_PEMX_CFG(pcie_port));
+			pem_cfg.u64 = CVMX_READ_CSR(CVMX_PEMX_CFG(pcie_port));
 			pem_cfg.s.md = 1;
-			cvmx_write_csr(CVMX_PEMX_CFG(pcie_port), pem_cfg.u64);
+			CVMX_WRITE_CSR(CVMX_PEMX_CFG(pcie_port), pem_cfg.u64);
 
 			/* Set the target link speed */
-			cfg040.u32 = cvmx_pcie_cfgx_read(pcie_port,
+			cfg040.u32 = CVMX_PCIE_CFGX_READ(pcie_port,
 						CVMX_PCIERCX_CFG040(pcie_port));	
 			cfg040.s.tls = 2;
-			cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG040(pcie_port),
+			CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG040(pcie_port),
 						cfg040.u32);
 			break;
 		case 8000: /* Gen3 */
-			pem_cfg.u64 = cvmx_read_csr(CVMX_PEMX_CFG(pcie_port));
+			pem_cfg.u64 = CVMX_READ_CSR(CVMX_PEMX_CFG(pcie_port));
 			pem_cfg.s.md = 2;
-			cvmx_write_csr(CVMX_PEMX_CFG(pcie_port), pem_cfg.u64);
+			CVMX_WRITE_CSR(CVMX_PEMX_CFG(pcie_port), pem_cfg.u64);
 
 			/* Set the target link speed */
-			cfg040.u32 = cvmx_pcie_cfgx_read(pcie_port,
+			cfg040.u32 = CVMX_PCIE_CFGX_READ(pcie_port,
 						CVMX_PCIERCX_CFG040(pcie_port));	
 			cfg040.s.tls = 3;
-			cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG040(pcie_port),
+			CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG040(pcie_port),
 						cfg040.u32);
 			break;
 		default:
@@ -427,10 +434,10 @@ static void __cvmx_pcie_rc_initialize_config_space(int pcie_port)
 		}
 		
 		/* Link Width Mode (PCIERCn_CFG452[LME]) */
-		pem_cfg.u64 = cvmx_read_csr(CVMX_PEMX_CFG(pcie_port));
-		cfg452.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG452(pcie_port));	
+		pem_cfg.u64 = CVMX_READ_CSR(CVMX_PEMX_CFG(pcie_port));
+		cfg452.u32 = CVMX_PCIE_CFGX_READ(pcie_port, CVMX_PCIERCX_CFG452(pcie_port));	
 		cfg452.s.lme = (pem_cfg.cn78xx.lanes8) ? 0xf : 0x7;
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG452(pcie_port), cfg452.u32);
+		CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG452(pcie_port), cfg452.u32);
 	}
 
 	if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_0)) {
@@ -443,45 +450,45 @@ static void __cvmx_pcie_rc_initialize_config_space(int pcie_port)
 		/* The starting equalization hints are incorrect on CN78XX pass 1.x. Fix
 		them for the 8 possible lanes. It doesn't hurt to program them even for
 		lanes not in use */
-		cfg089.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG089(pcie_port));
+		cfg089.u32 = CVMX_PCIE_CFGX_READ(pcie_port, CVMX_PCIERCX_CFG089(pcie_port));
 		cfg089.s.l1urph= 2;
 		cfg089.s.l1utp = 7;
 		cfg089.s.l0urph = 2;
 		cfg089.s.l0utp = 7;
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG089(pcie_port), cfg089.u32);
-		cfg090.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG090(pcie_port));
+		CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG089(pcie_port), cfg089.u32);
+		cfg090.u32 = CVMX_PCIE_CFGX_READ(pcie_port, CVMX_PCIERCX_CFG090(pcie_port));
 		cfg090.s.l3urph= 2;
 		cfg090.s.l3utp = 7;
 		cfg090.s.l2urph = 2;
 		cfg090.s.l2utp = 7;
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG090(pcie_port), cfg090.u32);
-		cfg091.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG091(pcie_port));
+		CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG090(pcie_port), cfg090.u32);
+		cfg091.u32 = CVMX_PCIE_CFGX_READ(pcie_port, CVMX_PCIERCX_CFG091(pcie_port));
 		cfg091.s.l5urph= 2;
 		cfg091.s.l5utp = 7;
 		cfg091.s.l4urph = 2;
 		cfg091.s.l4utp = 7;
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG091(pcie_port), cfg091.u32);
-		cfg092.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG092(pcie_port));
+		CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG091(pcie_port), cfg091.u32);
+		cfg092.u32 = CVMX_PCIE_CFGX_READ(pcie_port, CVMX_PCIERCX_CFG092(pcie_port));
 		cfg092.s.l7urph= 2;
 		cfg092.s.l7utp = 7;
 		cfg092.s.l6urph = 2;
 		cfg092.s.l6utp = 7;
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG092(pcie_port), cfg092.u32);
-		/* FIXME: Disable phase 2 and phase 3 equalization */
-		cfg548.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG548(pcie_port));
+		CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG092(pcie_port), cfg092.u32);
+		/* Disable phase 2 and phase 3 equalization */
+		cfg548.u32 = CVMX_PCIE_CFGX_READ(pcie_port, CVMX_PCIERCX_CFG548(pcie_port));
 		cfg548.s.ep2p3d = 1;
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG548(pcie_port), cfg548.u32);
+		CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG548(pcie_port), cfg548.u32);
 	}
 
 	if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X)) {
 		cvmx_pciercx_cfg554_t cfg554;
 		/* Errata (GSER-21331) GEN3 Equalization may fail */
 		/* Disable preset #10 and disable the 2ms timeout */
-		cfg554.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG554(pcie_port));
+		cfg554.u32 = CVMX_PCIE_CFGX_READ(pcie_port, CVMX_PCIERCX_CFG554(pcie_port));
 		if (OCTEON_IS_MODEL(OCTEON_CN78XX))
 			cfg554.s.p23td = 1;
 		cfg554.s.prv = 0x3ff;
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG554(pcie_port), cfg554.u32);
+		CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG554(pcie_port), cfg554.u32);
 	}
 }
 
@@ -788,7 +795,7 @@ retry:
 			    pcie_port, CAST64(pescx_bist_status.u64));
 
 	/* Initialize the config space CSRs */
-	__cvmx_pcie_rc_initialize_config_space(pcie_port);
+	__cvmx_pcie_rc_initialize_config_space(0, pcie_port);
 
 	/* Bring the link up */
 	if (__cvmx_pcie_rc_initialize_link_gen1(pcie_port)) {
@@ -996,7 +1003,7 @@ retry:
  *
  * @return Zero on success
  */
-static int __cvmx_pcie_rc_initialize_link_gen2(int pcie_port)
+static int __cvmx_pcie_rc_initialize_link_gen2(int node, int pcie_port)
 {
 	uint64_t start_cycle;
 	int try_gen3;
@@ -1008,32 +1015,33 @@ static int __cvmx_pcie_rc_initialize_link_gen2(int pcie_port)
 	cvmx_pciercx_cfg448_t pciercx_cfg448;
 
 	if (OCTEON_IS_OCTEON3()) {
-		if (CVMX_WAIT_FOR_FIELD64(CVMX_PEMX_ON(pcie_port), cvmx_pemx_on_t, pemoor, ==, 1, 100000)) {
-			cvmx_printf("PCIe: Port %d PEM not on, skipping\n", pcie_port);
+		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_PEMX_ON(pcie_port), cvmx_pemx_on_t, pemoor, ==, 1, 100000)) {
+			cvmx_printf("%d:PCIe: Port %d PEM not on, skipping\n", node, pcie_port);
 			return -1;
 		}
 	}
 
 	/* Remember if the link should try Gen3. This is needed for the CN78XX
 	pass 1.x workaround below */
-	pciercx_cfg031.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG031(pcie_port));
+	pciercx_cfg031.u32 = CVMX_PCIE_CFGX_READ(pcie_port, CVMX_PCIERCX_CFG031(pcie_port));
 	try_gen3 = (pciercx_cfg031.s.mls == 3);
 
 	/* Errata (GSER-21178) PCIe gen3 doesn't work */
 	if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_0) && try_gen3) {
 		/* Force Gen1 for initial link bringup. We'll fix it later */
-		pciercx_cfg031.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG031(pcie_port));
+		pciercx_cfg031.u32 = CVMX_PCIE_CFGX_READ(pcie_port, CVMX_PCIERCX_CFG031(pcie_port));
 		pciercx_cfg031.s.mls = 1;
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG031(pcie_port), pciercx_cfg031.u32);
-		pciercx_cfg040.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG040(pcie_port));
+		CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG031(pcie_port), pciercx_cfg031.u32);
+		pciercx_cfg040.u32 = CVMX_PCIE_CFGX_READ(pcie_port, CVMX_PCIERCX_CFG040(pcie_port));
 		pciercx_cfg040.s.tls = 1;
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG040(pcie_port), pciercx_cfg040.u32);
+		CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG040(pcie_port), pciercx_cfg040.u32);
 	}
 
 	/* Bring up the link */
-	pem_ctl_status.u64 = cvmx_read_csr(CVMX_PEMX_CTL_STATUS(pcie_port));
+	pem_ctl_status.u64 = CVMX_READ_CSR(CVMX_PEMX_CTL_STATUS(pcie_port));
 	pem_ctl_status.s.lnk_enb = 1;
-	cvmx_write_csr(CVMX_PEMX_CTL_STATUS(pcie_port), pem_ctl_status.u64);
+	CVMX_WRITE_CSR(CVMX_PEMX_CTL_STATUS(pcie_port), pem_ctl_status.u64);
+//printf("try_gen3 = %d, lnk_en = 0x%llx\n", try_gen3, CVMX_READ_CSR(CVMX_PEMX_CTL_STATUS(pcie_port)));
 
 	/* Wait for the link to come up */
 	start_cycle = cvmx_get_cycle();
@@ -1041,7 +1049,7 @@ static int __cvmx_pcie_rc_initialize_link_gen2(int pcie_port)
 		if (cvmx_get_cycle() - start_cycle > cvmx_clock_get_rate(CVMX_CLOCK_CORE))
 			return -1;
 		cvmx_wait(10000);
-		pciercx_cfg032.u32 = cvmx_pcie_cfgx_read(pcie_port,
+		pciercx_cfg032.u32 = CVMX_PCIE_CFGX_READ(pcie_port,
 							 CVMX_PCIERCX_CFG032(pcie_port));
 	} while ((pciercx_cfg032.s.dlla == 0) || (pciercx_cfg032.s.lt == 1));
 
@@ -1056,16 +1064,16 @@ static int __cvmx_pcie_rc_initialize_link_gen2(int pcie_port)
 		int qlm, lane;
 
 		/* Enable gen3 speed selection */
-		cfg031.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG031(pcie_port));
+		cfg031.u32 = CVMX_PCIE_CFGX_READ(pcie_port, CVMX_PCIERCX_CFG031(pcie_port));
 		cfg031.s.mls = 3;
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG031(pcie_port), cfg031.u32);
-		cfg040.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG040(pcie_port));
+		CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG031(pcie_port), cfg031.u32);
+		cfg040.u32 = CVMX_PCIE_CFGX_READ(pcie_port, CVMX_PCIERCX_CFG040(pcie_port));
 		cfg040.s.tls = 3;
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG040(pcie_port), cfg040.u32);
+		CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG040(pcie_port), cfg040.u32);
 		/* Force a demand speed change */
-		cfg515.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG515(pcie_port));
+		cfg515.u32 = CVMX_PCIE_CFGX_READ(pcie_port, CVMX_PCIERCX_CFG515(pcie_port));
 		cfg515.s.dsc = 1;
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG515(pcie_port), cfg515.u32);
+		CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG515(pcie_port), cfg515.u32);
 		cvmx_wait_usec(500);
 
 		/* Wait up to 10ms for the link speed change to complete */
@@ -1074,12 +1082,12 @@ static int __cvmx_pcie_rc_initialize_link_gen2(int pcie_port)
 			if (cvmx_get_cycle() - start_cycle > cvmx_clock_get_rate(CVMX_CLOCK_CORE))
 				return -1;
 			cvmx_wait(10000);
-			pciercx_cfg032.u32 = cvmx_pcie_cfgx_read(pcie_port,
+			pciercx_cfg032.u32 = CVMX_PCIE_CFGX_READ(pcie_port,
 							 CVMX_PCIERCX_CFG032(pcie_port));
 		} while (pciercx_cfg032.s.ls != 3);
 
-		pem_cfg.u64 = cvmx_read_csr(CVMX_PEMX_CFG(pcie_port));
-		low_qlm = pcie_port;  /* FIXME */
+		pem_cfg.u64 = CVMX_READ_CSR(CVMX_PEMX_CFG(pcie_port));
+		low_qlm = pcie_port;
 		high_qlm = (pem_cfg.cn78xx.lanes8) ? low_qlm+1 : low_qlm;
 
 		/* Toggle cfg_rx_dll_locken_ovvrd_en and rx_resetn_ovrrd_en across
@@ -1089,12 +1097,12 @@ static int __cvmx_pcie_rc_initialize_link_gen2(int pcie_port)
 				cvmx_gserx_lanex_rx_misc_ovrrd_t misc_ovrrd;
 				cvmx_gserx_lanex_pwr_ctrl_t pwr_ctrl;
 
-				misc_ovrrd.u64 = cvmx_read_csr(CVMX_GSERX_LANEX_RX_MISC_OVRRD(lane, qlm));
+				misc_ovrrd.u64 = CVMX_READ_CSR(CVMX_GSERX_LANEX_RX_MISC_OVRRD(lane, qlm));
 				misc_ovrrd.s.cfg_rx_dll_locken_ovvrd_en = 1;
-				cvmx_write_csr(CVMX_GSERX_LANEX_RX_MISC_OVRRD(lane, qlm), misc_ovrrd.u64);
-				pwr_ctrl.u64 = cvmx_read_csr(CVMX_GSERX_LANEX_PWR_CTRL(lane, qlm));
+				CVMX_WRITE_CSR(CVMX_GSERX_LANEX_RX_MISC_OVRRD(lane, qlm), misc_ovrrd.u64);
+				pwr_ctrl.u64 = CVMX_READ_CSR(CVMX_GSERX_LANEX_PWR_CTRL(lane, qlm));
 				pwr_ctrl.s.rx_resetn_ovrrd_en = 1;
-				cvmx_write_csr(CVMX_GSERX_LANEX_PWR_CTRL(lane, qlm), pwr_ctrl.u64);
+				CVMX_WRITE_CSR(CVMX_GSERX_LANEX_PWR_CTRL(lane, qlm), pwr_ctrl.u64);
 			}
 		}
 		for (qlm = low_qlm; qlm <= high_qlm; qlm++) {
@@ -1102,12 +1110,12 @@ static int __cvmx_pcie_rc_initialize_link_gen2(int pcie_port)
 				cvmx_gserx_lanex_rx_misc_ovrrd_t misc_ovrrd;
 				cvmx_gserx_lanex_pwr_ctrl_t pwr_ctrl;
 
-				misc_ovrrd.u64 = cvmx_read_csr(CVMX_GSERX_LANEX_RX_MISC_OVRRD(lane, qlm));
+				misc_ovrrd.u64 = CVMX_READ_CSR(CVMX_GSERX_LANEX_RX_MISC_OVRRD(lane, qlm));
 				misc_ovrrd.s.cfg_rx_dll_locken_ovvrd_en = 0;
-				cvmx_write_csr(CVMX_GSERX_LANEX_RX_MISC_OVRRD(lane, qlm), misc_ovrrd.u64);
-				pwr_ctrl.u64 = cvmx_read_csr(CVMX_GSERX_LANEX_PWR_CTRL(lane, qlm));
+				CVMX_WRITE_CSR(CVMX_GSERX_LANEX_RX_MISC_OVRRD(lane, qlm), misc_ovrrd.u64);
+				pwr_ctrl.u64 = CVMX_READ_CSR(CVMX_GSERX_LANEX_PWR_CTRL(lane, qlm));
 				pwr_ctrl.s.rx_resetn_ovrrd_en = 0;
-				cvmx_write_csr(CVMX_GSERX_LANEX_PWR_CTRL(lane, qlm), pwr_ctrl.u64);
+				CVMX_WRITE_CSR(CVMX_GSERX_LANEX_PWR_CTRL(lane, qlm), pwr_ctrl.u64);
 			}
 		}
 
@@ -1117,7 +1125,7 @@ static int __cvmx_pcie_rc_initialize_link_gen2(int pcie_port)
 			if (cvmx_clock_get_count(CVMX_CLOCK_CORE) - start_cycle > cvmx_clock_get_rate(CVMX_CLOCK_CORE))
 				return -1;
 			cvmx_wait_usec(1000);
-			cfg032.u32 = cvmx_pcie_cfgx_read(pcie_port, CVMX_PCIERCX_CFG032(pcie_port));
+			cfg032.u32 = CVMX_PCIE_CFGX_READ(pcie_port, CVMX_PCIERCX_CFG032(pcie_port));
 		} while ((cfg032.s.dlla == 0) || (cfg032.s.lt == 1));
 	}
 
@@ -1127,7 +1135,7 @@ static int __cvmx_pcie_rc_initialize_link_gen2(int pcie_port)
 	 * for a 512 byte MPS instead of our actual 256 byte MPS. The numbers
 	 * below are directly from the PCIe spec table 3-4
 	 */
-	pciercx_cfg448.u32 = cvmx_pcie_cfgx_read(pcie_port,
+	pciercx_cfg448.u32 = CVMX_PCIE_CFGX_READ(pcie_port,
 						 CVMX_PCIERCX_CFG448(pcie_port));
 	switch (pciercx_cfg032.s.nlw) {
 	case 1:		/* 1 lane */
@@ -1143,7 +1151,7 @@ static int __cvmx_pcie_rc_initialize_link_gen2(int pcie_port)
 		pciercx_cfg448.s.rtl = 258;
 		break;
 	}
-	cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG448(pcie_port),
+	CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG448(pcie_port),
 			     pciercx_cfg448.u32);
 
 	return 0;
@@ -1218,12 +1226,15 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 	cvmx_pemx_bar1_indexx_t bar1_index;
 	uint64_t ciu_soft_prst_reg, rst_ctl_reg;
 	int ep_mode;
-	int qlm = pcie_port;
+	int qlm;
+	int node = (pcie_port >> 4) & 0x3;
 	int connected_pcie_reset = -1;
 	enum cvmx_qlm_mode mode = CVMX_QLM_MODE_DISABLED;
 #ifndef CVMX_BUILD_FOR_LINUX_KERNEL
 	static void *fdt_addr = 0;
 #endif
+	pcie_port &= 0x3;
+	qlm = pcie_port;
 
 	if (pcie_port >= CVMX_PCIE_PORTS) {
 		//cvmx_dprintf("Invalid PCIe%d port\n", pcie_port);
@@ -1254,33 +1265,36 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 		else if (OCTEON_IS_MODEL(OCTEON_CNF71XX))
 			qlm = 1;
 
-		mode = cvmx_qlm_get_mode(qlm);
+		if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+			mode = cvmx_qlm_get_mode_cn78xx(node, qlm);
+		else
+			mode = cvmx_qlm_get_mode(qlm);
 		if (mode == CVMX_QLM_MODE_SRIO_1X4 ||
 		    mode == CVMX_QLM_MODE_SRIO_2X2 ||
 		    mode == CVMX_QLM_MODE_SRIO_4X1) {
-			cvmx_printf("PCIe: Port %d is SRIO, skipping.\n",
-				    pcie_port);
+			cvmx_printf("%d:PCIe: Port %d is SRIO, skipping.\n",
+				    node, pcie_port);
 			return -1;
 		} else if (mode == CVMX_QLM_MODE_SGMII) {
-			cvmx_printf("PCIe: Port %d is SGMII, skipping.\n",
-				    pcie_port);
+			cvmx_printf("%d:PCIe: Port %d is SGMII, skipping.\n",
+				    node, pcie_port);
 			return -1;
 		} else if (mode == CVMX_QLM_MODE_XAUI ||
 			   mode == CVMX_QLM_MODE_RXAUI) {
-			cvmx_printf("PCIe: Port %d is XAUI, skipping.\n",
-				    pcie_port);
+			cvmx_printf("%d:PCIe: Port %d is XAUI, skipping.\n",
+				    node, pcie_port);
 			return -1;
 		} else if (mode == CVMX_QLM_MODE_ILK) {
-			cvmx_printf("PCIe: Port %d is ILK, skipping.\n",
-				    pcie_port);
+			cvmx_printf("%d:PCIe: Port %d is ILK, skipping.\n",
+				    node, pcie_port);
 			return -1;
 		} else if (mode != CVMX_QLM_MODE_PCIE &&
 			   mode != CVMX_QLM_MODE_PCIE_1X8 &&
 			   mode != CVMX_QLM_MODE_PCIE_1X2 &&
 			   mode != CVMX_QLM_MODE_PCIE_2X1 &&
 			   mode != CVMX_QLM_MODE_PCIE_1X1) {
-			cvmx_printf("PCIe: Port %d is unknown, skipping.\n",
-				    pcie_port);
+			cvmx_printf("%d:PCIe: Port %d is unknown, skipping.\n",
+				    node, pcie_port);
 			return -1;
 		}
 	}
@@ -1315,7 +1329,7 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 		rst_ctl_reg = CVMX_MIO_RST_CTLX(pcie_port);
 	}
 
-	mio_rst_ctl.u64 = cvmx_read_csr(rst_ctl_reg);
+	mio_rst_ctl.u64 = CVMX_READ_CSR(rst_ctl_reg);
 	ep_mode = ((OCTEON_IS_MODEL(OCTEON_CN61XX) ||
 		    OCTEON_IS_MODEL(OCTEON_CNF71XX))
 		? (mio_rst_ctl.s.prtmode != 1) : (!mio_rst_ctl.s.host_mode));
@@ -1330,12 +1344,12 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 	}
 
 	if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
-		cvmx_write_csr(CVMX_DTX_PEMX_SELX(0, pcie_port), 0x17);
-		cvmx_write_csr(CVMX_DTX_PEMX_SELX(1, pcie_port), 0);
+		CVMX_WRITE_CSR(CVMX_DTX_PEMX_SELX(0, pcie_port), 0x17);
+		CVMX_WRITE_CSR(CVMX_DTX_PEMX_SELX(1, pcie_port), 0);
 	}
 
 	if (ep_mode) {
-		cvmx_printf("PCIe: Port %d in endpoint mode.\n", pcie_port);
+		cvmx_printf("%d:PCIe: Port %d in endpoint mode.\n", node, pcie_port);
 		return -1;
 	}
 #if 0
@@ -1418,56 +1432,56 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 	switch (connected_pcie_reset) {
 	case 0:
 		if (pcie_port == 1 &&
-		    (cvmx_read_csr(CVMX_MIO_QLMX_CFG(1)) & 0x3) == 1) {
-			ciu_soft_prst.u64 = cvmx_read_csr(CVMX_CIU_SOFT_PRST);
+		    (CVMX_READ_CSR(CVMX_MIO_QLMX_CFG(1)) & 0x3) == 1) {
+			ciu_soft_prst.u64 = CVMX_READ_CSR(CVMX_CIU_SOFT_PRST);
 			if (ciu_soft_prst.s.soft_prst == 0) {
 				/* Reset the port */
 				ciu_soft_prst.s.soft_prst = 1;
-				cvmx_write_csr(CVMX_CIU_SOFT_PRST,
+				CVMX_WRITE_CSR(CVMX_CIU_SOFT_PRST,
 					ciu_soft_prst.u64);
 				/* Wait until pcie resets the ports. */
 				cvmx_wait_usec(2000);
-				cvmx_write_csr(CVMX_CIU_SOFT_PRST1,
+				CVMX_WRITE_CSR(CVMX_CIU_SOFT_PRST1,
 					ciu_soft_prst.u64);
 				/* Wait until pcie resets the ports. */
 				cvmx_wait_usec(2000);
 			}
-			ciu_soft_prst.u64 = cvmx_read_csr(CVMX_CIU_SOFT_PRST);
+			ciu_soft_prst.u64 = CVMX_READ_CSR(CVMX_CIU_SOFT_PRST);
 			ciu_soft_prst.s.soft_prst = 0;
-			cvmx_write_csr(CVMX_CIU_SOFT_PRST, ciu_soft_prst.u64);
-			ciu_soft_prst.u64 = cvmx_read_csr(CVMX_CIU_SOFT_PRST1);
+			CVMX_WRITE_CSR(CVMX_CIU_SOFT_PRST, ciu_soft_prst.u64);
+			ciu_soft_prst.u64 = CVMX_READ_CSR(CVMX_CIU_SOFT_PRST1);
 			ciu_soft_prst.s.soft_prst = 0;
-			cvmx_write_csr(CVMX_CIU_SOFT_PRST1, ciu_soft_prst.u64);
+			CVMX_WRITE_CSR(CVMX_CIU_SOFT_PRST1, ciu_soft_prst.u64);
 		}
 		break;
 	case 1:
 		if (pcie_port == 0 &&
-		    (cvmx_read_csr(CVMX_MIO_QLMX_CFG(1)) & 0x3) == 1) {
-			ciu_soft_prst.u64 = cvmx_read_csr(CVMX_CIU_SOFT_PRST1);
+		    (CVMX_READ_CSR(CVMX_MIO_QLMX_CFG(1)) & 0x3) == 1) {
+			ciu_soft_prst.u64 = CVMX_READ_CSR(CVMX_CIU_SOFT_PRST1);
 			if (ciu_soft_prst.s.soft_prst == 0) {
 				/* Reset the port */
 				ciu_soft_prst.s.soft_prst = 1;
-				cvmx_write_csr(CVMX_CIU_SOFT_PRST1,
+				CVMX_WRITE_CSR(CVMX_CIU_SOFT_PRST1,
 					ciu_soft_prst.u64);
 				/* Wait until pcie resets the ports. */
 				cvmx_wait_usec(2000);
-				cvmx_write_csr(CVMX_CIU_SOFT_PRST,
+				CVMX_WRITE_CSR(CVMX_CIU_SOFT_PRST,
 					ciu_soft_prst.u64);
 				/* Wait until pcie resets the ports. */
 				cvmx_wait_usec(2000);
 			}
-			ciu_soft_prst.u64 = cvmx_read_csr(CVMX_CIU_SOFT_PRST1);
+			ciu_soft_prst.u64 = CVMX_READ_CSR(CVMX_CIU_SOFT_PRST1);
 			ciu_soft_prst.s.soft_prst = 0;
-			cvmx_write_csr(CVMX_CIU_SOFT_PRST1, ciu_soft_prst.u64);
-			ciu_soft_prst.u64 = cvmx_read_csr(CVMX_CIU_SOFT_PRST);
+			CVMX_WRITE_CSR(CVMX_CIU_SOFT_PRST1, ciu_soft_prst.u64);
+			ciu_soft_prst.u64 = CVMX_READ_CSR(CVMX_CIU_SOFT_PRST);
 			ciu_soft_prst.s.soft_prst = 0;
-			cvmx_write_csr(CVMX_CIU_SOFT_PRST, ciu_soft_prst.u64);
+			CVMX_WRITE_CSR(CVMX_CIU_SOFT_PRST, ciu_soft_prst.u64);
 		}
 		break;
 	case -1:
 	default:
 		/* Bring the PCIe out of reset */
-		ciu_soft_prst.u64 = cvmx_read_csr(ciu_soft_prst_reg);
+		ciu_soft_prst.u64 = CVMX_READ_CSR(ciu_soft_prst_reg);
 		/* After a chip reset the PCIe will also be in reset. If it
 		 * isn't, most likely someone is trying to init it again
 		 * without a proper PCIe reset.
@@ -1475,13 +1489,13 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 		if (ciu_soft_prst.s.soft_prst == 0) {
 			/* Reset the port */
 			ciu_soft_prst.s.soft_prst = 1;
-			cvmx_write_csr(ciu_soft_prst_reg, ciu_soft_prst.u64);
+			CVMX_WRITE_CSR(ciu_soft_prst_reg, ciu_soft_prst.u64);
 			/* Wait until pcie resets the ports. */
 			cvmx_wait_usec(2000);
 		}
-		ciu_soft_prst.u64 = cvmx_read_csr(ciu_soft_prst_reg);
+		ciu_soft_prst.u64 = CVMX_READ_CSR(ciu_soft_prst_reg);
 		ciu_soft_prst.s.soft_prst = 0;
-		cvmx_write_csr(ciu_soft_prst_reg, ciu_soft_prst.u64);
+		CVMX_WRITE_CSR(ciu_soft_prst_reg, ciu_soft_prst.u64);
 	}
 
 	/* Wait for PCIe reset to complete */
@@ -1496,57 +1510,57 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 	/* Check and make sure PCIe came out of reset. If it doesn't the board
 	   probably hasn't wired the clocks up and the interface should be
 	   skipped */
-	if (CVMX_WAIT_FOR_FIELD64(rst_ctl_reg, cvmx_mio_rst_ctlx_t,
+	if (CVMX_WAIT_FOR_FIELD64_NODE(node, rst_ctl_reg, cvmx_mio_rst_ctlx_t,
 				  rst_done, ==, 1, 10000)) {
-		cvmx_printf("PCIe: Port %d stuck in reset, skipping.\n",
-			    pcie_port);
+		cvmx_printf("%d:PCIe: Port %d stuck in reset, skipping.\n",
+			    node, pcie_port);
 		return -1;
 	}
 
 	/* Check BIST status */
-	pemx_bist_status.u64 = cvmx_read_csr(CVMX_PEMX_BIST_STATUS(pcie_port));
+	pemx_bist_status.u64 = CVMX_READ_CSR(CVMX_PEMX_BIST_STATUS(pcie_port));
 	if (pemx_bist_status.u64)
-		cvmx_printf("PCIe: BIST FAILED for port %d (0x%016llx)\n",
-			    pcie_port, CAST64(pemx_bist_status.u64));
+		cvmx_printf("%d:PCIe: BIST FAILED for port %d (0x%016llx)\n",
+			    node, pcie_port, CAST64(pemx_bist_status.u64));
 	/* BIST_STATUS2 is not present on 78xx */
 	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
 		pemx_bist_status2.u64 = 0;
 	else
-		pemx_bist_status2.u64 = cvmx_read_csr(CVMX_PEMX_BIST_STATUS2(pcie_port));
+		pemx_bist_status2.u64 = CVMX_READ_CSR(CVMX_PEMX_BIST_STATUS2(pcie_port));
 	/* Errata PCIE-14766 may cause the lower 6 bits to be randomly set on CN63XXp1 */
 	if (OCTEON_IS_MODEL(OCTEON_CN63XX_PASS1_X))
 		pemx_bist_status2.u64 &= ~0x3full;
 	if (pemx_bist_status2.u64)
-		cvmx_printf("PCIe: BIST2 FAILED for port %d (0x%016llx)\n",
-			    pcie_port, CAST64(pemx_bist_status2.u64));
+		cvmx_printf("%d:PCIe: BIST2 FAILED for port %d (0x%016llx)\n",
+			    node, pcie_port, CAST64(pemx_bist_status2.u64));
 
 	/* Initialize the config space CSRs */
-	__cvmx_pcie_rc_initialize_config_space(pcie_port);
+	__cvmx_pcie_rc_initialize_config_space(node, pcie_port);
 
 	/* Enable gen2 speed selection */
-	pciercx_cfg515.u32 = cvmx_pcie_cfgx_read(pcie_port,
+	pciercx_cfg515.u32 = CVMX_PCIE_CFGX_READ(pcie_port,
 						 CVMX_PCIERCX_CFG515(pcie_port));
 	pciercx_cfg515.s.dsc = 1;
-	cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG515(pcie_port),
+	CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG515(pcie_port),
 			     pciercx_cfg515.u32);
 
 	/* Bring the link up */
-	if (__cvmx_pcie_rc_initialize_link_gen2(pcie_port)) {
+	if (__cvmx_pcie_rc_initialize_link_gen2(node, pcie_port)) {
 		/* Some gen1 devices don't handle the gen 2 training correctly.
 		 * Disable gen2 and try again with only gen1
 		 */
 		if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
-			cvmx_printf("PCIe: Link timeout on port %d, probably the slot is empty\n",
-				    pcie_port);
+			cvmx_printf("%d:PCIe: Link timeout on port %d, probably the slot is empty\n",
+				    node, pcie_port);
 			return -1;
 		} else {
 			cvmx_pciercx_cfg031_t pciercx_cfg031;
-			pciercx_cfg031.u32 = cvmx_pcie_cfgx_read(pcie_port,
+			pciercx_cfg031.u32 = CVMX_PCIE_CFGX_READ(pcie_port,
 							 CVMX_PCIERCX_CFG031(pcie_port));
 			pciercx_cfg031.s.mls = 1;
-			cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIERCX_CFG031(pcie_port),
+			CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG031(pcie_port),
 				     		pciercx_cfg031.u32);
-			if (__cvmx_pcie_rc_initialize_link_gen2(pcie_port)) {
+			if (__cvmx_pcie_rc_initialize_link_gen2(node, pcie_port)) {
 				cvmx_printf("PCIe: Link timeout on port %d, probably the slot is empty\n",
 					    pcie_port);
 				return -1;
@@ -1555,10 +1569,10 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 	}
 
 	/* Store merge control (SLI_MEM_ACCESS_CTL[TIMER,MAX_WORD]) */
-	sli_mem_access_ctl.u64 = cvmx_read_csr(CVMX_PEXP_SLI_MEM_ACCESS_CTL);
+	sli_mem_access_ctl.u64 = CVMX_READ_CSR(CVMX_PEXP_SLI_MEM_ACCESS_CTL);
 	sli_mem_access_ctl.s.max_word = 0;	/* Allow 16 words to combine */
 	sli_mem_access_ctl.s.timer = 127;	/* Wait up to 127 cycles for more data */
-	cvmx_write_csr(CVMX_PEXP_SLI_MEM_ACCESS_CTL, sli_mem_access_ctl.u64);
+	CVMX_WRITE_CSR(CVMX_PEXP_SLI_MEM_ACCESS_CTL, sli_mem_access_ctl.u64);
 
 	/* Setup Mem access SubDIDs */
 	mem_access_subid.u64 = 0;
@@ -1578,7 +1592,7 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 	 * bits of address space
 	 */
 	for (i = 12 + pcie_port * 4; i < 16 + pcie_port * 4; i++) {
-		cvmx_write_csr(CVMX_PEXP_SLI_MEM_ACCESS_SUBIDX(i),
+		CVMX_WRITE_CSR(CVMX_PEXP_SLI_MEM_ACCESS_SUBIDX(i),
 			       mem_access_subid.u64);
 		/* Set each SUBID to extend the addressable range */
 		__cvmx_increment_ba(&mem_access_subid);
@@ -1593,40 +1607,40 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 		 * addresses to the PCIe busses
 		 */
 		for (i = 0; i < 4; i++) {
-			cvmx_write_csr(CVMX_PEMX_P2P_BARX_START(i, pcie_port), -1);
-			cvmx_write_csr(CVMX_PEMX_P2P_BARX_END(i, pcie_port), -1);
+			CVMX_WRITE_CSR(CVMX_PEMX_P2P_BARX_START(i, pcie_port), -1);
+			CVMX_WRITE_CSR(CVMX_PEMX_P2P_BARX_END(i, pcie_port), -1);
 		}
 	}
 
 	/* Set Octeon's BAR0 to decode 0-16KB. It overlaps with Bar2 */
-	cvmx_write_csr(CVMX_PEMX_P2N_BAR0_START(pcie_port), 0);
+	CVMX_WRITE_CSR(CVMX_PEMX_P2N_BAR0_START(pcie_port), 0);
 
 	/* Set Octeon's BAR2 to decode 0-2^41. Bar0 and Bar1 take precedence
 	 * where they overlap. It also overlaps with the device addresses, so
 	 * make sure the peer to peer forwarding is set right
 	 */
-	cvmx_write_csr(CVMX_PEMX_P2N_BAR2_START(pcie_port), 0);
+	CVMX_WRITE_CSR(CVMX_PEMX_P2N_BAR2_START(pcie_port), 0);
 
 	/* Setup BAR2 attributes */
 	/* Relaxed Ordering (NPEI_CTL_PORTn[PTLP_RO,CTLP_RO, WAIT_COM]) */
 	/*  PTLP_RO,CTLP_RO should normally be set (except for debug). */
 	/*  WAIT_COM=0 will likely work for all applications. */
 	/* Load completion relaxed ordering (NPEI_CTL_PORTn[WAITL_COM]) */
-	pemx_bar_ctl.u64 = cvmx_read_csr(CVMX_PEMX_BAR_CTL(pcie_port));
+	pemx_bar_ctl.u64 = CVMX_READ_CSR(CVMX_PEMX_BAR_CTL(pcie_port));
 	pemx_bar_ctl.s.bar1_siz = 3;	/* 256MB BAR1 */
 	pemx_bar_ctl.s.bar2_enb = 1;
 	pemx_bar_ctl.s.bar2_esx = _CVMX_PCIE_ES;
 	pemx_bar_ctl.s.bar2_cax = 0;
-	cvmx_write_csr(CVMX_PEMX_BAR_CTL(pcie_port), pemx_bar_ctl.u64);
-	sli_ctl_portx.u64 = cvmx_read_csr(CVMX_PEXP_SLI_CTL_PORTX(pcie_port));
+	CVMX_WRITE_CSR(CVMX_PEMX_BAR_CTL(pcie_port), pemx_bar_ctl.u64);
+	sli_ctl_portx.u64 = CVMX_READ_CSR(CVMX_PEXP_SLI_CTL_PORTX(pcie_port));
 	sli_ctl_portx.s.ptlp_ro = 1;
 	sli_ctl_portx.s.ctlp_ro = 1;
 	sli_ctl_portx.s.wait_com = 0;
 	sli_ctl_portx.s.waitl_com = 0;
-	cvmx_write_csr(CVMX_PEXP_SLI_CTL_PORTX(pcie_port), sli_ctl_portx.u64);
+	CVMX_WRITE_CSR(CVMX_PEXP_SLI_CTL_PORTX(pcie_port), sli_ctl_portx.u64);
 
 	/* BAR1 follows BAR2 */
-	cvmx_write_csr(CVMX_PEMX_P2N_BAR1_START(pcie_port),
+	CVMX_WRITE_CSR(CVMX_PEMX_P2N_BAR1_START(pcie_port),
 		       CVMX_PCIE_BAR1_RC_BASE);
 
 	bar1_index.u64 = 0;
@@ -1636,22 +1650,22 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 	bar1_index.s.addr_v = 1;	/* Valid entry */
 
 	for (i = 0; i < 16; i++) {
-		cvmx_write_csr(CVMX_PEMX_BAR1_INDEXX(i, pcie_port),
+		CVMX_WRITE_CSR(CVMX_PEMX_BAR1_INDEXX(i, pcie_port),
 			       bar1_index.u64);
 		/* 256MB / 16 >> 22 == 4 */
 		bar1_index.s.addr_idx += (((1ull << 28) / 16ull) >> 22);
 	}
 
 	/* Value is recommended in CSR files */
-	pemx_ctl_status.u64 = cvmx_read_csr(CVMX_PEMX_CTL_STATUS(pcie_port));
+	pemx_ctl_status.u64 = CVMX_READ_CSR(CVMX_PEMX_CTL_STATUS(pcie_port));
 	pemx_ctl_status.cn63xx.cfg_rtry = 32;
-	cvmx_write_csr(CVMX_PEMX_CTL_STATUS(pcie_port), pemx_ctl_status.u64);
+	CVMX_WRITE_CSR(CVMX_PEMX_CTL_STATUS(pcie_port), pemx_ctl_status.u64);
 
 	/* Display the link status */
-	pciercx_cfg032.u32 = cvmx_pcie_cfgx_read(pcie_port,
+	pciercx_cfg032.u32 = CVMX_PCIE_CFGX_READ(pcie_port,
 						 CVMX_PCIERCX_CFG032(pcie_port));
-	cvmx_printf("PCIe: Port %d link active, %d lanes, speed gen%d\n",
-		    pcie_port, pciercx_cfg032.s.nlw, pciercx_cfg032.s.ls);
+	cvmx_printf("%d:PCIe: Port %d link active, %d lanes, speed gen%d\n",
+		    node, pcie_port, pciercx_cfg032.s.nlw, pciercx_cfg032.s.ls);
 
 	return 0;
 }
@@ -1659,7 +1673,7 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 /**
  * Initialize a PCIe port for use in host(RC) mode. It doesn't enumerate the bus.
  *
- * @param pcie_port PCIe port to initialize
+ * @param pcie_port PCIe port to initialize for a node
  *
  * @return Zero on success
  */
@@ -1681,12 +1695,14 @@ int cvmx_pcie_rc_initialize(int pcie_port)
 /**
  * Shutdown a PCIe port and put it in reset
  *
- * @param pcie_port PCIe port to shutdown
+ * @param pcie_port PCIe port to shutdown for a node
  *
  * @return Zero on success
  */
 int cvmx_pcie_rc_shutdown(int pcie_port)
 {
+	int node = (pcie_port >> 4) & 0x3;
+	pcie_port &= 0x3;
 #if !defined(CVMX_BUILD_FOR_LINUX_KERNEL) || defined(CONFIG_CAVIUM_DECODE_RSL)
 	cvmx_error_disable_group(CVMX_ERROR_GROUP_PCI, pcie_port);
 #endif
@@ -1697,7 +1713,7 @@ int cvmx_pcie_rc_shutdown(int pcie_port)
 			cvmx_dprintf("PCIe: Port %d shutdown timeout\n",
 				     pcie_port);
 	} else {
-		if (CVMX_WAIT_FOR_FIELD64(CVMX_PEMX_CPL_LUT_VALID(pcie_port),
+		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_PEMX_CPL_LUT_VALID(pcie_port),
 					  cvmx_pemx_cpl_lut_valid_t, tag, ==,
 					  0, 2000))
 			cvmx_dprintf("PCIe: Port %d shutdown timeout\n",
@@ -1707,14 +1723,14 @@ int cvmx_pcie_rc_shutdown(int pcie_port)
 	/* Force reset */
 	if (pcie_port) {
 		cvmx_ciu_soft_prst_t ciu_soft_prst;
-		ciu_soft_prst.u64 = cvmx_read_csr(CVMX_CIU_SOFT_PRST1);
+		ciu_soft_prst.u64 = CVMX_READ_CSR(CVMX_CIU_SOFT_PRST1);
 		ciu_soft_prst.s.soft_prst = 1;
-		cvmx_write_csr(CVMX_CIU_SOFT_PRST1, ciu_soft_prst.u64);
+		CVMX_WRITE_CSR(CVMX_CIU_SOFT_PRST1, ciu_soft_prst.u64);
 	} else {
 		cvmx_ciu_soft_prst_t ciu_soft_prst;
-		ciu_soft_prst.u64 = cvmx_read_csr(CVMX_CIU_SOFT_PRST);
+		ciu_soft_prst.u64 = CVMX_READ_CSR(CVMX_CIU_SOFT_PRST);
 		ciu_soft_prst.s.soft_prst = 1;
-		cvmx_write_csr(CVMX_CIU_SOFT_PRST, ciu_soft_prst.u64);
+		CVMX_WRITE_CSR(CVMX_CIU_SOFT_PRST, ciu_soft_prst.u64);
 	}
 	return 0;
 }
@@ -1731,7 +1747,7 @@ int cvmx_pcie_rc_shutdown(int pcie_port)
  *
  * @return 64bit Octeon IO address
  */
-static inline uint64_t __cvmx_pcie_build_config_addr(int pcie_port, int bus,
+static inline uint64_t __cvmx_pcie_build_config_addr(int node, int pcie_port, int bus,
 						     int dev, int fn, int reg)
 {
 	cvmx_pcie_address_t pcie_addr;
@@ -1747,6 +1763,7 @@ static inline uint64_t __cvmx_pcie_build_config_addr(int pcie_port, int bus,
 	pcie_addr.config.io = 1;
 	pcie_addr.config.did = 3;
 	pcie_addr.config.subdid = 1;
+	pcie_addr.config.node = node;
 	pcie_addr.config.es = _CVMX_PCIE_ES;
 	pcie_addr.config.port = pcie_port;
 	pcie_addr.config.ty = (bus > pciercx_cfg006.s.pbnum);
@@ -1770,7 +1787,10 @@ static inline uint64_t __cvmx_pcie_build_config_addr(int pcie_port, int bus,
  */
 uint8_t cvmx_pcie_config_read8(int pcie_port, int bus, int dev, int fn, int reg)
 {
-	uint64_t address = __cvmx_pcie_build_config_addr(pcie_port, bus, dev,
+	uint64_t address;
+	int node = (pcie_port >> 4) & 0x3;
+	pcie_port &= 0x3;
+	address = __cvmx_pcie_build_config_addr(node, pcie_port, bus, dev,
 							 fn, reg);
 	if (address)
 		return cvmx_read64_uint8(address);
@@ -1792,7 +1812,10 @@ uint8_t cvmx_pcie_config_read8(int pcie_port, int bus, int dev, int fn, int reg)
 uint16_t cvmx_pcie_config_read16(int pcie_port, int bus, int dev,
 				 int fn, int reg)
 {
-	uint64_t address = __cvmx_pcie_build_config_addr(pcie_port, bus, dev,
+	uint64_t address;
+	int node = (pcie_port >> 4) & 0x3;
+	pcie_port &= 0x3;
+	address = __cvmx_pcie_build_config_addr(node, pcie_port, bus, dev,
 							 fn, reg);
 	if (address)
 		return cvmx_le16_to_cpu(cvmx_read64_uint16(address));
@@ -1814,7 +1837,10 @@ uint16_t cvmx_pcie_config_read16(int pcie_port, int bus, int dev,
 uint32_t cvmx_pcie_config_read32(int pcie_port, int bus, int dev,
 				 int fn, int reg)
 {
-	uint64_t address = __cvmx_pcie_build_config_addr(pcie_port, bus, dev,
+	uint64_t address;
+	int node = (pcie_port >> 4) & 0x3;
+	pcie_port &= 0x3;
+	address = __cvmx_pcie_build_config_addr(node, pcie_port, bus, dev,
 							 fn, reg);
 	if (address)
 		return cvmx_le32_to_cpu(cvmx_read64_uint32(address));
@@ -1835,7 +1861,10 @@ uint32_t cvmx_pcie_config_read32(int pcie_port, int bus, int dev,
 void cvmx_pcie_config_write8(int pcie_port, int bus, int dev, int fn,
 			     int reg, uint8_t val)
 {
-	uint64_t address = __cvmx_pcie_build_config_addr(pcie_port, bus, dev,
+	uint64_t address;
+	int node = (pcie_port >> 4) & 0x3;
+	pcie_port &= 0x3;
+	address = __cvmx_pcie_build_config_addr(node, pcie_port, bus, dev,
 							 fn, reg);
 	if (address)
 		cvmx_write64_uint8(address, val);
@@ -1854,7 +1883,10 @@ void cvmx_pcie_config_write8(int pcie_port, int bus, int dev, int fn,
 void cvmx_pcie_config_write16(int pcie_port, int bus, int dev, int fn,
 			      int reg, uint16_t val)
 {
-	uint64_t address = __cvmx_pcie_build_config_addr(pcie_port, bus, dev,
+	uint64_t address;
+	int node = (pcie_port >> 4) & 0x3;
+	pcie_port &= 0x3;
+	address = __cvmx_pcie_build_config_addr(node, pcie_port, bus, dev,
 							 fn, reg);
 	if (address)
 		cvmx_write64_uint16(address, cvmx_cpu_to_le16(val));
@@ -1873,7 +1905,10 @@ void cvmx_pcie_config_write16(int pcie_port, int bus, int dev, int fn,
 void cvmx_pcie_config_write32(int pcie_port, int bus, int dev, int fn,
 			      int reg, uint32_t val)
 {
-	uint64_t address = __cvmx_pcie_build_config_addr(pcie_port, bus, dev,
+	uint64_t address;
+	int node = (pcie_port >> 4) & 0x3;
+	pcie_port &= 0x3;
+	address = __cvmx_pcie_build_config_addr(node, pcie_port, bus, dev,
 							 fn, reg);
 	if (address)
 		cvmx_write64_uint32(address, cvmx_cpu_to_le32(val));
@@ -1906,8 +1941,8 @@ uint32_t cvmx_pcie_cfgx_read_node(int node, int pcie_port, uint32_t cfg_offset)
 		cvmx_pemx_cfg_rd_t pemx_cfg_rd;
 		pemx_cfg_rd.u64 = 0;
 		pemx_cfg_rd.s.addr = cfg_offset;
-		cvmx_write_csr_node(node, CVMX_PEMX_CFG_RD(pcie_port), pemx_cfg_rd.u64);
-		pemx_cfg_rd.u64 = cvmx_read_csr_node(node, CVMX_PEMX_CFG_RD(pcie_port));
+		CVMX_WRITE_CSR(CVMX_PEMX_CFG_RD(pcie_port), pemx_cfg_rd.u64);
+		pemx_cfg_rd.u64 = CVMX_READ_CSR(CVMX_PEMX_CFG_RD(pcie_port));
 		return pemx_cfg_rd.s.data;
 	}
 }
@@ -1938,7 +1973,7 @@ void cvmx_pcie_cfgx_write_node(int node, int pcie_port, uint32_t cfg_offset, uin
 		pemx_cfg_wr.u64 = 0;
 		pemx_cfg_wr.s.addr = cfg_offset;
 		pemx_cfg_wr.s.data = val;
-		cvmx_write_csr_node(node, CVMX_PEMX_CFG_WR(pcie_port), pemx_cfg_wr.u64);
+		CVMX_WRITE_CSR(CVMX_PEMX_CFG_WR(pcie_port), pemx_cfg_wr.u64);
 	}
 }
 
@@ -1947,15 +1982,18 @@ extern int cvmx_pcie_is_host_mode(int pcie_port);
 /**
  * Initialize a PCIe port for use in target(EP) mode.
  *
- * @param pcie_port PCIe port to initialize
+ * @param pcie_port PCIe port to initialize for a node
  *
  * @return Zero on success
  */
 int cvmx_pcie_ep_initialize(int pcie_port)
 {
+	int node = (pcie_port >> 4) & 0x3;
 	if (cvmx_pcie_is_host_mode(pcie_port))
 		return -1;
 
+	pcie_port &= 0x3;
+
 	/* CN63XX Pass 1.0 errata G-14395 requires the QLM De-emphasis be
 	 * programmed
 	 */
@@ -1978,7 +2016,7 @@ int cvmx_pcie_ep_initialize(int pcie_port)
 	}
 
 	/* Enable bus master and memory */
-	cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIEEPX_CFG001(pcie_port), 0x6);
+	CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIEEPX_CFG001(pcie_port), 0x6);
 
 	/* Max Payload Size (PCIE*_CFG030[MPS]) */
 	/* Max Read Request Size (PCIE*_CFG030[MRRS]) */
@@ -1986,7 +2024,7 @@ int cvmx_pcie_ep_initialize(int pcie_port)
 	/* Error Message Enables (PCIE*_CFG030[CE_EN,NFE_EN,FE_EN,UR_EN]) */
 	{
 		cvmx_pcieepx_cfg030_t pcieepx_cfg030;
-		pcieepx_cfg030.u32 = cvmx_pcie_cfgx_read(pcie_port,
+		pcieepx_cfg030.u32 = CVMX_PCIE_CFGX_READ(pcie_port,
 							 CVMX_PCIEEPX_CFG030(pcie_port));
 		if (OCTEON_IS_MODEL(OCTEON_CN5XXX)) {
 			pcieepx_cfg030.s.mps = MPS_CN5XXX;
@@ -2001,7 +2039,7 @@ int cvmx_pcie_ep_initialize(int pcie_port)
 		pcieepx_cfg030.s.nfe_en = 1;	/* Non-fatal error reporting enable. */
 		pcieepx_cfg030.s.fe_en = 1;	/* Fatal error reporting enable. */
 		pcieepx_cfg030.s.ur_en = 1;	/* Unsupported request reporting enable. */
-		cvmx_pcie_cfgx_write(pcie_port, CVMX_PCIEEPX_CFG030(pcie_port),
+		CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIEEPX_CFG030(pcie_port),
 				     pcieepx_cfg030.u32);
 	}
 
@@ -2026,16 +2064,16 @@ int cvmx_pcie_ep_initialize(int pcie_port)
 		 */
 		cvmx_dpi_sli_prtx_cfg_t prt_cfg;
 		cvmx_sli_s2m_portx_ctl_t sli_s2m_portx_ctl;
-		prt_cfg.u64 = cvmx_read_csr(CVMX_DPI_SLI_PRTX_CFG(pcie_port));
+		prt_cfg.u64 = CVMX_READ_CSR(CVMX_DPI_SLI_PRTX_CFG(pcie_port));
 		prt_cfg.s.mps = MPS_CN6XXX;
 		prt_cfg.s.mrrs = MRRS_CN6XXX;
 		/* Max outstanding load request. */
 		prt_cfg.s.molr = 32;
-		cvmx_write_csr(CVMX_DPI_SLI_PRTX_CFG(pcie_port), prt_cfg.u64);
+		CVMX_WRITE_CSR(CVMX_DPI_SLI_PRTX_CFG(pcie_port), prt_cfg.u64);
 
-		sli_s2m_portx_ctl.u64 = cvmx_read_csr(CVMX_PEXP_SLI_S2M_PORTX_CTL(pcie_port));
+		sli_s2m_portx_ctl.u64 = CVMX_READ_CSR(CVMX_PEXP_SLI_S2M_PORTX_CTL(pcie_port));
 		sli_s2m_portx_ctl.cn61xx.mrrs = MRRS_CN6XXX;
-		cvmx_write_csr(CVMX_PEXP_SLI_S2M_PORTX_CTL(pcie_port), sli_s2m_portx_ctl.u64);
+		CVMX_WRITE_CSR(CVMX_PEXP_SLI_S2M_PORTX_CTL(pcie_port), sli_s2m_portx_ctl.u64);
 	}
 
 	/* Setup Mem access SubDID 12 to access Host memory */
@@ -2066,7 +2104,7 @@ int cvmx_pcie_ep_initialize(int pcie_port)
 			mem_access_subid.cn68xx.ba = 0;
 		else
 			mem_access_subid.cn63xx.ba = 0;
-		cvmx_write_csr(CVMX_PEXP_SLI_MEM_ACCESS_SUBIDX(12 + pcie_port * 4), mem_access_subid.u64);
+		CVMX_WRITE_CSR(CVMX_PEXP_SLI_MEM_ACCESS_SUBIDX(12 + pcie_port * 4), mem_access_subid.u64);
 	}
 	return 0;
 }
@@ -2157,9 +2195,11 @@ void cvmx_pcie_wait_for_pending(int pcie_port)
  */
 int cvmx_pcie_is_host_mode(int pcie_port)
 {
+	int node = (pcie_port >> 4) & 0x3;
+	pcie_port &= 0x3;
 	if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
 		cvmx_pemx_strap_t strap;
-		strap.u64 = cvmx_read_csr(CVMX_PEMX_STRAP(pcie_port));
+		strap.u64 = CVMX_READ_CSR(CVMX_PEMX_STRAP(pcie_port));
 		return (strap.cn78xx.pimode != 3);
 	} else if (OCTEON_IS_MODEL(OCTEON_CN70XX)) {
 		cvmx_rst_ctlx_t rst_ctl;
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pki-resources.c b/arch/mips/cavium-octeon/executive/cvmx-pki-resources.c
index 5138aaf..e03b6a1 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pki-resources.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pki-resources.c
@@ -298,7 +298,7 @@ int cvmx_pki_pcam_entry_alloc(int node, int index, int bank, uint64_t cluster_ma
 		}
 	}
 	index = rs;
-	/*vinita to_do , implement cluster handle, for now assume
+	/* implement cluster handle for pass2, for now assume
 	all clusters will have same base index*/
 	return index;
 }
@@ -417,6 +417,11 @@ void __cvmx_pki_global_rsrc_free(int node)
 		cvmx_printf("ERROR pki-rsrc:Failed to release all qpg entries\n");
 	}
 
+	cnt = CVMX_PKI_NUM_BPID;
+	if (cvmx_free_global_resource_range_with_base(CVMX_GR_TAG_BPID(node), 0, cnt) == -1) {
+		cvmx_printf("ERROR pki-rsrc:Failed to release all bpids\n");
+	}
+
 	cnt = CVMX_PKI_NUM_PCAM_ENTRY;
 	for (cluster = 0; cluster < CVMX_PKI_NUM_CLUSTER; cluster++) {
 		for (bank = 0; bank < CVMX_PKI_NUM_PCAM_BANK; bank++) {
@@ -428,3 +433,62 @@ void __cvmx_pki_global_rsrc_free(int node)
 	}
 
 }
+
+/**
+ * This function allocates/reserves a bpid from pool of global bpid per node.
+ * @param node	node to allocate bpid from.
+ * @param bpid	bpid  to allocate, if -1 it will be allocated
+ *		first available boid from bpid resource. If index is positive
+ *		number and in range, it will try to allocate specified bpid.
+ * @return 	bpid number on success,
+ *		-1 on alloc failure.
+ *		-2 on resource already reserved.
+ */
+int cvmx_pki_bpid_alloc(int node, int bpid)
+{
+	int rs;
+
+	if (cvmx_create_global_resource_range(CVMX_GR_TAG_BPID(node), CVMX_PKI_NUM_BPID)) {
+		cvmx_printf("ERROR: Failed to create bpid global resource\n");
+		return -1;
+	}
+	if (bpid >= 0) {
+		rs = cvmx_reserve_global_resource_range(CVMX_GR_TAG_BPID(node),
+				bpid, bpid, 1);
+		if (rs == -1) {
+			cvmx_dprintf("INFO: bpid %d is already reserved\n", (int)bpid);
+			return CVMX_RESOURCE_ALREADY_RESERVED;
+		}
+	} else {
+		rs = cvmx_allocate_global_resource_range(CVMX_GR_TAG_BPID(node), bpid, 1, 1);
+		if (rs == -1) {
+			cvmx_printf("ERROR: Failed to allocate bpid\n");
+			return CVMX_RESOURCE_ALLOC_FAILED;
+		}
+		if (rs == 0) { /* don't use bpid 0 */
+			rs = cvmx_allocate_global_resource_range(CVMX_GR_TAG_BPID(node), bpid, 1, 1);
+			if (rs == -1) {
+				cvmx_printf("ERROR: Failed to allocate bpid\n");
+				return CVMX_RESOURCE_ALLOC_FAILED;
+			}
+		}
+	}
+	bpid = rs;
+	return bpid;
+}
+
+/**
+ * This function frees a bpid from pool of global bpid per node.
+ * @param node	 node to free bpid from.
+ * @param style	 bpid to free
+ * @return 	 0 on success, -1 on failure or
+ */
+int cvmx_pki_bpid_free(int node, int bpid)
+{
+	if (cvmx_free_global_resource_range_with_base(CVMX_GR_TAG_BPID(node), bpid, 1) == -1) {
+		cvmx_printf("ERROR Failed to release bpid %d\n", (int)bpid);
+		return -1;
+	}
+	return 0;
+}
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pki.c b/arch/mips/cavium-octeon/executive/cvmx-pki.c
index a2bb7a3..97fc298 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pki.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pki.c
@@ -415,7 +415,7 @@ void cvmx_pki_read_tag_config(int node, int style, uint64_t cluster_mask,
 	tag_cfg->tag_fields.mpls_label = style_alg_reg.s.tag_mpls0;
 	tag_cfg->tag_fields.input_port = style_alg_reg.s.tag_prt;
 
-	/** vinita_to_do get mask tag*/
+	/** TO_DO get mask tag*/
 }
 
  /** This function writes/configures parameters associated with tag configuration in hardware.
@@ -462,7 +462,7 @@ void cvmx_pki_write_tag_config(int node, int style, uint64_t cluster_mask,
 			style_alg_reg.s.tag_prt = tag_cfg->tag_fields.input_port;
 			cvmx_write_csr_node(node, CVMX_PKI_CLX_STYLEX_ALG(style, cluster), style_alg_reg.u64);
 
-			/* vinita_to_do add mask tag */
+			/* TO_DO add mask tag */
 		}
 		cluster++;
 	}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pko.c b/arch/mips/cavium-octeon/executive/cvmx-pko.c
index 2e5ac5a..e5a68f2 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pko.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pko.c
@@ -603,7 +603,7 @@ cvmx_pko_return_value_t cvmx_pko_config_port(int port, int base_queue,
 			port, base_queue, (base_queue+num_queues-1),
 			priority[0],priority[1], priority[2], priority[3]);
 
-	/* FIXME: the need to handle ILLEGAL_PID port argument
+	/* The need to handle ILLEGAL_PID port argument
 	 * is obsolete now, the code here can be simplified.
 	 */
 
@@ -783,7 +783,7 @@ cvmx_pko_return_value_t cvmx_pko_config_port(int port, int base_queue,
  * Configure queues for an internal port.
  * @INTERNAL
  * @param pko_port PKO internal port number
- * FIXME: for PKO2 only, equivalent to cvmx_pko_config_port()
+ * @note this is the PKO2 equivalent to cvmx_pko_config_port()
  */
 static cvmx_pko_return_value_t cvmx_pko2_config_port(short ipd_port, int base_queue,
 				       int num_queues,
@@ -931,7 +931,7 @@ static cvmx_pko_return_value_t cvmx_pko2_config_port(short ipd_port, int base_qu
 		cvmx_write_csr(CVMX_PKO_MEM_IQUEUE_PTRS, config.u64);
 	}
 
-// FIXME: detect errors
+	/* Error detection is resirable here */
 	return 0;
 }
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pko3-queue.c b/arch/mips/cavium-octeon/executive/cvmx-pko3-queue.c
index 52c2167..02bb0b3 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pko3-queue.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pko3-queue.c
@@ -65,8 +65,7 @@
 static int debug = 0;	/* 1 for basic, 2 for detailed trace */
 
 /* Minimum MTU assumed for shaping configuration */
-static unsigned __pko3_min_mtu = 9080;
-/* FIXME: The above could be made a per-port config param */
+static unsigned __pko3_min_mtu = 9080;	/* Could be per-port in the future */
 
 struct cvmx_pko3_dq {
 #ifdef __BIG_ENDIAN_BITFIELD
@@ -108,7 +107,8 @@ int cvmx_pko3_get_queue_base(int ipd_port)
 	dq_table = __cvmx_pko3_dq_table + CVMX_PKO3_IPD_NUM_MAX * xp.node;
 
 	if(dq_table[i].dq_count > 0)
-		ret = cvmx_helper_node_to_ipd_port(xp.node, dq_table[i].dq_base);
+		ret = xp.node << 10 |
+		    cvmx_helper_node_to_ipd_port(xp.node, dq_table[i].dq_base);
 
 	return ret;
 }
@@ -1261,6 +1261,12 @@ void cvmx_pko3_dq_red(unsigned node, unsigned dq_num, red_action_t red_act,
 	dq_num &= (1<<10)-1;
 
 	dqx_shape.u64 = 0;
+
+        if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X)) {
+		if (len_adjust < 0)
+			len_adjust = 0;
+	}
+
         dqx_shape.s.adjust = len_adjust;
 
 	switch(red_act) {
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pko3.c b/arch/mips/cavium-octeon/executive/cvmx-pko3.c
index 487471e..fa11065 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pko3.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pko3.c
@@ -1,5 +1,5 @@
 /***********************license start***************
- * Copyright (c) 2003-2013  Cavium Inc. (support@cavium.com). All rights
+ * Copyright (c) 2014  Cavium Inc. (support@cavium.com). All rights
  * reserved.
  *
  *
@@ -718,8 +718,6 @@ static int cvmx_pko_setup_macs(int node)
 		unsigned mac_fifo_cnt;
 		unsigned tmp;
 
-		/* FIXME- this section has no basis in HRM, revisit */
-		/* Loosely based on packet/clear78.x */
 		pko_fifo_cnt = cvmx_pko3_mac_table[mac_num].fifo_cnt;
 		mac_fifo_cnt = cvmx_pko3_mac_table[mac_num].mac_fifo_cnt;
 
@@ -734,7 +732,6 @@ static int cvmx_pko_setup_macs(int node)
 		fifo_size = (2 * 1024) + (1024 / 2); /* 2.5KiB */
 		fifo_credit = pko_fifo_cnt * fifo_size;
 
-		/* FIXME- This code is chip-dependent, not portable! */
 		switch (mac_num) {
 			case 0: /* loopback */
 				mac_credit = 4096; /* From HRM Sec 13.0 */
@@ -1156,8 +1153,10 @@ int cvmx_pko3_pdesc_from_wqe(cvmx_pko3_pdesc_t *pdesc, cvmx_wqe_78xx_t *wqe,
 	hdr_s->s.format = 0;	/* Only 0 works for Pass1 */
 	hdr_s->s.ds = 0;	/* don't send, never used */
 
-	/* TODO: n2 is not currently supported in simulator */
-	hdr_s->s.n2 = 0;	/* No L2 allocate */
+        if(OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X))
+		hdr_s->s.n2 = 0;	/* L2 allocate everything */
+	else
+		hdr_s->s.n2 = 1;	/* No L2 allocate works faster */
 
 	/* Default buffer freeing setting, may be overriden by "i" */
 	hdr_s->s.df = !free_bufs;
@@ -1358,8 +1357,8 @@ int cvmx_pko3_pdesc_transmit(cvmx_pko3_pdesc_t *pdesc, uint16_t dq)
 	}
 
         /* Derive destination node from dq */
-        port_node = dq >> 14;
-        dq &= (1<<10)-1;
+	port_node = dq >> 10;
+	dq &= (1<<10)-1;
 
         /* Send the PKO3 command into the Descriptor Queue */
         pko_status = __cvmx_pko3_do_dma(port_node, dq,
@@ -1378,6 +1377,34 @@ int cvmx_pko3_pdesc_transmit(cvmx_pko3_pdesc_t *pdesc, uint16_t dq)
 	return -1;
 }
 
+int cvmx_pko3_pdesc_append_free(cvmx_pko3_pdesc_t *pdesc, uint64_t addr,
+			     unsigned gaura)
+{
+	cvmx_pko_send_hdr_t *hdr_s;
+	cvmx_pko_send_free_t free_s;
+	cvmx_pko_send_aura_t aura_s;
+
+	hdr_s = (void *) &pdesc->word[0];
+
+	if (pdesc->last_aura == -1) {
+		pdesc->last_aura = hdr_s->s.aura = gaura;
+	} else if (pdesc->last_aura != (short) gaura) {
+		aura_s.s.aura = gaura;
+		aura_s.s.offset = 0;
+		aura_s.s.alg = AURAALG_NOP;
+		aura_s.s.subdc4 = CVMX_PKO_SENDSUBDC_AURA;
+		pdesc->last_aura = gaura;
+		if (cvmx_pko3_pdesc_subdc_add(pdesc, aura_s.u64) < 0)
+			return -1;
+	}
+
+	free_s.u64 = 0;
+	free_s.s.subdc4 = CVMX_PKO_SENDSUBDC_FREE;
+	free_s.s.addr = addr;
+
+	return cvmx_pko3_pdesc_subdc_add(pdesc, free_s.u64);
+}
+
 /**
  * Append a packet segment to a packet descriptor
  *
@@ -1617,8 +1644,8 @@ int cvmx_pko3_pdesc_notify_memclr(cvmx_pko3_pdesc_t *pdesc,
  * software-based decoding to handle modified or originated
  * packets correctly.
  *
- * FIXME:
- * Add simple accessors to read the decoded protocol fields.
+ * @note
+ * Need to add simple accessors to read the decoded protocol fields.
  */
 static int cvmx_pko3_pdesc_hdr_offsets(cvmx_pko3_pdesc_t *pdesc)
 {
@@ -1648,7 +1675,7 @@ static int cvmx_pko3_pdesc_hdr_offsets(cvmx_pko3_pdesc_t *pdesc)
 		if (pdesc->pki_word2.lf_hdr_type == CVMX_PKI_LTYPE_E_SCTP)
 			pdesc->ckl4_alg = CKL4ALG_SCTP;
 	}
-	/* FIXME: consider ARP as L3 too ? what about IPfrag ? */
+	/* May need to add logic for ARP, IPfrag packets here */
 
 	pdesc->hdr_offsets = 1;	/* make sure its done once */
 	return 0;
@@ -1785,8 +1812,6 @@ int cvmx_pko3_pdesc_hdr_push(cvmx_pko3_pdesc_t *pdesc,
 	if (layer >= 3) {
 		hdr_s->s.ckl3 = 1;
 		hdr_s->s.ckl4 = pdesc->ckl4_alg;
-		/* FIXME: decode L4 alg in case the header was generated */
-		/* FIXME: CKL4 not supported in simulator */
 	}
 
 	return headroom;
diff --git a/arch/mips/cavium-octeon/executive/cvmx-qlm.c b/arch/mips/cavium-octeon/executive/cvmx-qlm.c
index 80c2858..930f7f9 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-qlm.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-qlm.c
@@ -42,7 +42,7 @@
  *
  * Helper utilities for qlm.
  *
- * <hr>$Revision: 103883 $<hr>
+ * <hr>$Revision: 105521 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
@@ -293,7 +293,7 @@ void cvmx_qlm_init(void)
 	int qlm;
 	int qlm_jtag_length;
 	char *qlm_jtag_name = "cvmx_qlm_jtag";
-	int qlm_jtag_size = CVMX_QLM_JTAG_UINT32 * 8 * sizeof(uint32_t);
+	int qlm_jtag_size = CVMX_QLM_JTAG_UINT32 * 8 * sizeof(uint32_t) * 5;
 	static uint64_t qlm_base = 0;
 	const cvmx_bootmem_named_block_desc_t *desc;
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-range.c b/arch/mips/cavium-octeon/executive/cvmx-range.c
index cdbce31..12450eb 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-range.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-range.c
@@ -203,8 +203,7 @@ int cvmx_range_reserve(uint64_t range_addr, uint64_t owner, uint64_t base, uint6
 
 	size = cvmx_read64_uint64(addr_of_size(range_addr));
 	if (up > size) {
-		if (debug)
-			cvmx_dprintf("ERROR: invalid base or cnt. "
+		cvmx_dprintf("ERROR: invalid base or cnt. "
 			    "range_addr=0x%llx, owner=0x%llx, size=%d base+cnt=%d\n",
 			     (unsigned long long)range_addr,
 			     (unsigned long long)owner,
@@ -261,8 +260,9 @@ int __cvmx_range_is_allocated(uint64_t range_addr, int bases[], int count)
 		}
 		r_owner = cvmx_read64_uint64(addr_of_element(range_addr,base));
 		if (r_owner == CVMX_RANGE_AVAILABLE) {
-			cvmx_dprintf("ERROR: i=%d:base=%d is available\n",
-				     (int) i, (int) base);
+		    if (debug)
+			cvmx_dprintf("%s: i=%d:base=%d is available\n",
+				     __func__, (int) i, (int) base);
 			return 0;
 		}
 	}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-srio.c b/arch/mips/cavium-octeon/executive/cvmx-srio.c
index 4b0553b..3bf4b7f 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-srio.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-srio.c
@@ -745,7 +745,7 @@ int cvmx_srio_initialize(int srio_port, cvmx_srio_initialize_flags_t flags)
 	sli_mem_access_ctl.s.timer = 127;	/* Wait up to 127 cycles for more data */
 	cvmx_write_csr(CVMX_PEXP_SLI_MEM_ACCESS_CTL, sli_mem_access_ctl.u64);
 
-	/* FIXME: Disable sending a link request when the SRIO link is
+	/* Disable sending a link request when the SRIO link is
 	   brought up. For unknown reasons this code causes issues with some SRIO
 	   devices. As we currently don't support hotplug in software, this code
 	   should never be needed.  Without link down/up events, the ACKs should
@@ -1479,7 +1479,7 @@ uint64_t cvmx_srio_physical_map(int srio_port, cvmx_srio_write_mode_t write_op,
 	needed_subid.s.port = srio_port;
 	needed_subid.s.nmerge = 0;
 
-	/* FIXME: We might want to use the device ID swapping modes so the device
+	/* We might want to use the device ID swapping modes so the device
 	   ID is part of the lower address bits. This would allow many more
 	   devices to share S2M_TYPE indexes. This would require "base+size-1"
 	   to fit in bits [17:0] or bits[25:0] for 8 bits of device ID */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-usb.c b/arch/mips/cavium-octeon/executive/cvmx-usb.c
index 79558be..da6940a 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-usb.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-usb.c
@@ -2400,7 +2400,7 @@ static int __cvmx_usb_submit_transaction(cvmx_usb_internal_state_t * usb,
 	transaction->buffer = buffer;
 	transaction->buffer_length = buffer_length;
 	transaction->control_header = control_header;
-	transaction->iso_start_frame = iso_start_frame;	// FIXME: This is not used, implement it
+	transaction->iso_start_frame = iso_start_frame;	// This is not used, need to implement it.
 	transaction->iso_number_packets = iso_number_packets;
 	transaction->iso_packets = iso_packets;
 	transaction->callback = callback;
diff --git a/arch/mips/cavium-octeon/executive/octeon-model.c b/arch/mips/cavium-octeon/executive/octeon-model.c
index af95856..008c7c7 100644
--- a/arch/mips/cavium-octeon/executive/octeon-model.c
+++ b/arch/mips/cavium-octeon/executive/octeon-model.c
@@ -43,7 +43,7 @@
  * File defining functions for working with different Octeon
  * models.
  *
- * <hr>$Revision: 102057 $<hr>
+ * <hr>$Revision: 105060 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/octeon.h>
@@ -188,6 +188,12 @@ const char *octeon_model_get_string_buffer(uint32_t chip_id, char *buffer)
 	case 48:
 		core_model = "90";
 		break;
+	case 44:
+		core_model = "88";
+		break;
+	case 40:
+		core_model = "85";
+		break;
 	case 32:
 		core_model = "80";
 		break;
@@ -442,8 +448,18 @@ const char *octeon_model_get_string_buffer(uint32_t chip_id, char *buffer)
 			family = "77";
 		if (fus_dat3.cn78xx.nozip
 		    && fus_dat3.cn78xx.nodfa_dte
-		    && fus_dat3.cn78xx.nohna_dte)
-			suffix = "SCP";
+		    && fus_dat3.cn78xx.nohna_dte) {
+			if (fus_dat3.cn78xx.nozip && 
+				!fus_dat2.cn78xx.raid_en &&
+				fus_dat3.cn78xx.nohna_dte) {
+				suffix = "CP";
+			}
+			else {
+				suffix = "SCP";
+			}
+		}
+		else if (fus_dat2.cn78xx.raid_en == 0)
+			suffix = "HCP";
 		else
 			suffix = "AAP";
 		break;
diff --git a/arch/mips/include/asm/octeon/cvmx-app-config.h b/arch/mips/include/asm/octeon/cvmx-app-config.h
index 75dd539..de8424a 100644
--- a/arch/mips/include/asm/octeon/cvmx-app-config.h
+++ b/arch/mips/include/asm/octeon/cvmx-app-config.h
@@ -89,6 +89,8 @@ int __cvmx_import_app_config_from_named_block(char * block_name);
  */
 void __cvmx_export_app_config_cleanup(void);
 
+int __cvmx_export_config(void);
+
 #ifdef  __cplusplus
 /* *INDENT-OFF* */
 }
diff --git a/arch/mips/include/asm/octeon/cvmx-app-init.h b/arch/mips/include/asm/octeon/cvmx-app-init.h
index f3eb084..dbe212e 100644
--- a/arch/mips/include/asm/octeon/cvmx-app-init.h
+++ b/arch/mips/include/asm/octeon/cvmx-app-init.h
@@ -41,7 +41,7 @@
  * @file
  * Header file for simple executive application initialization.  This defines
  * part of the ABI between the bootloader and the application.
- * <hr>$Revision: 101905 $<hr>
+ * <hr>$Revision: 105233 $<hr>
  *
  */
 
@@ -280,8 +280,14 @@ enum cvmx_board_types_enum {
 	CVMX_BOARD_TYPE_NIC401NVG = 62,
 	CVMX_BOARD_TYPE_NIC210NVG = 63,
 	CVMX_BOARD_TYPE_SFF7000 = 64,
-	CVMX_BOARD_TYPE_EBB7800_CFG1 = 65,
+	CVMX_BOARD_TYPE_EBB7800_CFG1 = 65, /* Only required to support cn78xx p1.0 */
 	CVMX_BOARD_TYPE_TB7600 = 66,
+	CVMX_BOARD_TYPE_EBB7804 = 67,
+	CVMX_BOARD_TYPE_EBB7804_CFG1 = 68, /* Only required to support cn78xx p1.0 */
+	CVMX_BOARD_TYPE_TB7000 = 69,
+	CVMX_BOARD_TYPE_EBB7800_CFG0 = 70, /* Only required to support cn78xx p1.0 */
+	CVMX_BOARD_TYPE_EBB7804_CFG0 = 71, /* Only required to support cn78xx p1.0 */
+	CVMX_BOARD_TYPE_SWORDFISH = 72,
 	CVMX_BOARD_TYPE_MAX,
 	/* NOTE:  256-257 are being used by a customer. */
 
@@ -416,6 +422,12 @@ static inline const char *cvmx_board_type_to_string(enum cvmx_board_types_enum t
 		ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_SFF7000)
 		ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_EBB7800_CFG1)
 		ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_TB7600)
+		ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_EBB7804)
+		ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_EBB7804_CFG1)
+		ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_TB7000)
+		ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_EBB7800_CFG0)
+		ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_EBB7804_CFG0)
+		ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_SWORDFISH)
 		ENUM_BRD_TYPE_CASE(CVMX_BOARD_TYPE_MAX)
 
 		/* Customer boards listed here */
diff --git a/arch/mips/include/asm/octeon/cvmx-fpa3.h b/arch/mips/include/asm/octeon/cvmx-fpa3.h
index 224b756..7f7ad3a 100644
--- a/arch/mips/include/asm/octeon/cvmx-fpa3.h
+++ b/arch/mips/include/asm/octeon/cvmx-fpa3.h
@@ -42,7 +42,7 @@
  *
  * Interface to the CN78XX Free Pool Allocator, a.k.a. FPA3
  *
- * <hr>$Revision: 104195 $<hr>
+ * <hr>$Revision: 106503 $<hr>
  *
  */
 
@@ -529,13 +529,12 @@ static inline long long cvmx_fpa3_get_available(cvmx_fpa3_gaura_t aura)
 /**
  * Configure the QoS parameters of an FPA3 AURA
  *
- * @para aura is the FPA3 AURA handle
+ * @param aura is the FPA3 AURA handle
  * @param ena_bp enables backpressure when outstanding count exceeds 'bp_thresh'
- * @param ena_red enables random early discard when outstanding count
- * exceeds 'pass_thresh'
+ * @param ena_red enables random early discard when outstanding count exceeds 'pass_thresh'
  * @param pass_thresh is the maximum count to invoke flow control
  * @param drop_thresh is the count threshold to begin dropping packets
- * @para, bp_thresh is the back-pressure threshold
+ * @param bp_thresh is the back-pressure threshold
  *
  */
 static inline void cvmx_fpa3_setup_aura_qos(cvmx_fpa3_gaura_t aura, bool ena_red,
@@ -595,10 +594,32 @@ extern cvmx_fpa3_pool_t cvmx_fpa3_setup_fill_pool(
 	int node, int desired_pool, const char *name,
 	unsigned block_size, unsigned num_blocks, void *buffer);
 
+/**
+ * Function to attach an aura to an existing pool
+ *
+ * @param node - configure fpa on this node
+ * @param pool - configured pool to attach aura to
+ * @param desired_aura - pointer to aura to use, set to -1 to allocate
+ * @param name - name to register
+ * @param block_size - size of buffers to use
+ * @param num_blocks - number of blocks to allocate
+ *
+ * @return configured gaura on success, CVMX_FPA3_INVALID_GAURA on failure
+ */
 extern cvmx_fpa3_gaura_t cvmx_fpa3_set_aura_for_pool
 	(cvmx_fpa3_pool_t pool, int desired_aura,
 	const char *name, unsigned block_size, unsigned num_blocks);
 
+
+/**
+ * Function to setup and initialize a pool.
+ *
+ * @param node - configure fpa on this node
+ * @param desired_aura - aura to use, -1 for dynamic allocation
+ * @param name - name to register
+ * @param block_size - size of buffers in pool
+ * @param num_blocks - max number of buffers allowed
+ */
 extern cvmx_fpa3_gaura_t cvmx_fpa3_setup_aura_and_pool(
 		int node, int desired_aura,
 		const char *name, void *buffer,
diff --git a/arch/mips/include/asm/octeon/cvmx-global-resources.h b/arch/mips/include/asm/octeon/cvmx-global-resources.h
index 6fefcf0..52fb6b1 100644
--- a/arch/mips/include/asm/octeon/cvmx-global-resources.h
+++ b/arch/mips/include/asm/octeon/cvmx-global-resources.h
@@ -16,6 +16,7 @@
 #define CVMX_GR_TAG_CLUSTER_GRP(x)  cvmx_get_gr_tag('c','v','m','_','c','l','g','r','p','_',(x+'0'),'.','.','.','.','.')
 #define CVMX_GR_TAG_STYLE(x)        cvmx_get_gr_tag('c','v','m','_','s','t','y','l','e','_',(x+'0'),'.','.','.','.','.')
 #define CVMX_GR_TAG_QPG_ENTRY(x)    cvmx_get_gr_tag('c','v','m','_','q','p','g','e','t','_',(x+'0'),'.','.','.','.','.')
+#define CVMX_GR_TAG_BPID(x)         cvmx_get_gr_tag('c','v','m','_','b','p','i','d','s','_',(x+'0'),'.','.','.','.','.')
 #define CVMX_GR_TAG_PCAM(x,y,z) \
 	cvmx_get_gr_tag('c','v','m','_','p','c','a','m','_',(x+'0'),(y+'0'),(z+'0'),'.','.','.','.')
 
diff --git a/arch/mips/include/asm/octeon/cvmx-helper-pki.h b/arch/mips/include/asm/octeon/cvmx-helper-pki.h
index e1278da..ac6dce3 100644
--- a/arch/mips/include/asm/octeon/cvmx-helper-pki.h
+++ b/arch/mips/include/asm/octeon/cvmx-helper-pki.h
@@ -87,7 +87,7 @@ struct cvmx_pki_prt_schd {
 	cvmx_fpa3_pool_t _pool;	/* FPA3 POOL handle */
 	cvmx_fpa3_gaura_t _aura;/* FPA3 AURA handle */
 	bool cfg_port;          /* Set to 1 if this port on the interface is not used */
-	int style;              /* If style_per_prt is TRUE in interface schd */
+	int style;              /* If port is using its own style and not interfcae style */
 	bool pool_per_prt; 	/* Port will use its own pool, if FALSE use interface pool */
 	int pool_num;		/* pool number to use, if -1 allocated by sdk otherwise software should alloc it*/
 	char *pool_name;
@@ -108,8 +108,7 @@ struct cvmx_pki_prt_schd {
 struct cvmx_pki_intf_schd {
 	cvmx_fpa3_pool_t _pool;	/* FPA3 POOL handle */
 	cvmx_fpa3_gaura_t _aura;/* FPA3 AURA handle */
-	bool style_per_prt;	/* Every port will use different style/profile */
-	bool style_per_intf;	/* otherwise all ports on this interface will use same style/profile */
+	bool style_per_intf;	/* 1: all ports on this interface will use same style; 0:ports on this intf will use their own style */
 	int style;              /* style number to use, if -1 allocated by sdk otherwise software should alloc it*/
 	bool pool_per_intf; 	/* Ports will use either this shared pool or their own pool*/
 	int pool_num;		/* pool number to use, if -1 allocated by sdk otherwise software should alloc it*/
diff --git a/arch/mips/include/asm/octeon/cvmx-hwpko.h b/arch/mips/include/asm/octeon/cvmx-hwpko.h
index f5d93b0..eaa946c 100644
--- a/arch/mips/include/asm/octeon/cvmx-hwpko.h
+++ b/arch/mips/include/asm/octeon/cvmx-hwpko.h
@@ -473,12 +473,12 @@ extern int cvmx_pko_get_num_queues(int port);
  * @param pool	fpa pool number yo use
  * @param buffer_size	buffer size of pool
  * @param buffer_count	number of buufers to allocate to pool
+ *
+ * @note the caller is responsable for setting up the pool with
+ * an appropriate buffer size and sufficient buffer count.
  */
 void cvmx_pko_set_cmd_que_pool_config(int64_t pool, uint64_t buffer_size,
 					   uint64_t buffer_count);
-//FIXME- reconsider:
-// Helper should setup the pool, then call PKO (pko_init?) with the
-// pool number to use.
 
 /**
  * Get the status counters for a port.
diff --git a/arch/mips/include/asm/octeon/cvmx-l2c.h b/arch/mips/include/asm/octeon/cvmx-l2c.h
index f89ab21..4eed9b6 100644
--- a/arch/mips/include/asm/octeon/cvmx-l2c.h
+++ b/arch/mips/include/asm/octeon/cvmx-l2c.h
@@ -43,7 +43,7 @@
  * Interface to the Level 2 Cache (L2C) control, measurement, and debugging
  * facilities.
  *
- * <hr>$Revision: 92683 $<hr>
+ * <hr>$Revision: 106657 $<hr>
  *
  */
 
@@ -376,8 +376,8 @@ int cvmx_l2c_unlock_mem_region(uint64_t start, uint64_t len);
  * @param index  Which way to read from.
  *
  * @return l2c tag structure for line requested.
- * 
- * NOTE: This function is deprecated and cannot be used on devices with 
+ *
+ * NOTE: This function is deprecated and cannot be used on devices with
  *       multiple L2C interfaces such as the OCTEON CN68XX.
  *       Please use cvmx_l2c_get_tag_v2 instead.
  */
@@ -391,7 +391,7 @@ cvmx_l2c_tag_t cvmx_l2c_get_tag(uint32_t association, uint32_t index)
  * @param association
  *               Which association to read line from
  * @param index  Which way to read from.
- * 
+ *
  * @param tad    Which TAD to read from, set to 0 except on OCTEON CN68XX.
  *
  * @return l2c tag structure for line requested.
@@ -402,7 +402,7 @@ cvmx_l2c_tag_t cvmx_l2c_get_tag_v2(uint32_t association, uint32_t index, uint32_
  * Find the TAD for the specified address
  *
  * @param addr   physical address to get TAD for
- * 
+ *
  * @return TAD number for address.
  */
 int cvmx_l2c_address_to_tad(uint64_t addr);
@@ -433,6 +433,14 @@ uint32_t cvmx_l2c_v2_address_to_tag(uint64_t addr);
 void cvmx_l2c_flush(void);
 
 /**
+ * Flushes (and unlocks) the entire L2 cache.  Unlike cvmx_l2c_flush this
+ * function also flushes and unlocks the remote L2 cache.
+ * IMPORTANT: Must only be run by one core at a time due to use
+ * of L2C debug features.
+ */
+void cvmx_l2c_flush_ocx(void);
+
+/**
  *
  * @return Returns the size of the L2 cache in bytes,
  * -1 on error (unrecognized model)
@@ -451,6 +459,7 @@ int cvmx_l2c_get_num_sets(void);
  * @return
  */
 int cvmx_l2c_get_set_bits(void);
+
 /**
  * Return the number of associations in the L2 Cache
  *
@@ -470,13 +479,23 @@ void cvmx_l2c_flush_line(uint32_t assoc, uint32_t index);
 
 /**
  * Initialize the BIG address in L2C+DRAM to generate proper error
- * on reading/writing to an non-existant memory location. 
+ * on reading/writing to an non-existant memory location.
  *
  * @param mem_size  Amount of DRAM configured in MB.
  * @param mode      Allow/Disallow reporting errors L2C_INT_SUM[BIGRD,BIGWR].
  */
 void cvmx_l2c_set_big_size(uint64_t mem_size, int mode);
 
+/**
+ * Initialize the BIG address in L2C+DRAM to generate proper error
+ * on reading/writing to an non-existant memory location.
+ *
+ * @param node      OCX CPU node number
+ * @param mem_size  Amount of DRAM configured in MB.
+ * @param mode      Allow/Disallow reporting errors L2C_INT_SUM[BIGRD,BIGWR].
+ */
+void cvmx_l2c_set_big_size_node(int node, uint64_t mem_size, int mode);
+
 #if !defined(CVMX_BUILD_FOR_LINUX_HOST) && !defined(CVMX_BUILD_FOR_LINUX_KERNEL)
 
 /*
diff --git a/arch/mips/include/asm/octeon/cvmx-pcie.h b/arch/mips/include/asm/octeon/cvmx-pcie.h
index 69440b0..093c132 100644
--- a/arch/mips/include/asm/octeon/cvmx-pcie.h
+++ b/arch/mips/include/asm/octeon/cvmx-pcie.h
@@ -42,7 +42,7 @@
  *
  * Interface to PCIe as a host(RC) or target(EP)
  *
- * <hr>$Revision: 102522 $<hr>
+ * <hr>$Revision: 104992 $<hr>
  */
 
 #ifndef __CVMX_PCIE_H__
@@ -89,7 +89,8 @@ typedef union {
 		uint64_t io:1;	/* 1 for IO space access */
 		uint64_t did:5;	/* PCIe DID = 3 */
 		uint64_t subdid:3;	/* PCIe SubDID = 1 */
-		uint64_t reserved_36_39:4;	/* Must be zero */
+		uint64_t reserved_38_39:2;	/* Must be zero */
+		uint64_t node:2;		/* Numa node number */
 		uint64_t es:2;	/* Endian swap = 1 */
 		uint64_t port:2;	/* PCIe port 0,1 */
 		uint64_t reserved_29_31:3;	/* Must be zero */
@@ -106,7 +107,8 @@ typedef union {
 		uint64_t io:1;	/* 1 for IO space access */
 		uint64_t did:5;	/* PCIe DID = 3 */
 		uint64_t subdid:3;	/* PCIe SubDID = 2 */
-		uint64_t reserved_36_39:4;	/* Must be zero */
+		uint64_t reserved_38_39:4;	/* Must be zero */
+		uint64_t node:2;		/* Numa node number */
 		uint64_t es:2;	/* Endian swap = 1 */
 		uint64_t port:2;	/* PCIe port 0,1 */
 		uint64_t address:32;	/* PCIe IO address */
@@ -117,7 +119,8 @@ typedef union {
 		uint64_t io:1;	/* 1 for IO space access */
 		uint64_t did:5;	/* PCIe DID = 3 */
 		uint64_t subdid:3;	/* PCIe SubDID = 3-6 */
-		uint64_t reserved_36_39:4;	/* Must be zero */
+		uint64_t reserved_38_39:2;	/* Must be zero */
+		uint64_t node:2;		/* Numa node number */
 		uint64_t address:36;	/* PCIe Mem address */
 	} mem;
 #else
@@ -130,7 +133,8 @@ typedef union {
 		uint64_t reserved_29_31:3;
 		uint64_t port:2;
 		uint64_t es:2;
-		uint64_t reserved_36_39:4;
+		uint64_t node:2;
+		uint64_t reserved_38_39:4;
 		uint64_t subdid:3;
 		uint64_t did:5;
 		uint64_t io:1;
@@ -141,7 +145,8 @@ typedef union {
 		uint64_t address:32;
 		uint64_t port:2;
 		uint64_t es:2;
-		uint64_t reserved_36_39:4;
+		uint64_t node:2;
+		uint64_t reserved_38_39:4;
 		uint64_t subdid:3;
 		uint64_t did:5;
 		uint64_t io:1;
@@ -150,7 +155,8 @@ typedef union {
 	} io;
 	struct {
 		uint64_t address:36;
-		uint64_t reserved_36_39:4;
+		uint64_t node:2;
+		uint64_t reserved_38_39:4;
 		uint64_t subdid:3;
 		uint64_t did:5;
 		uint64_t io:1;
diff --git a/arch/mips/include/asm/octeon/cvmx-pip.h b/arch/mips/include/asm/octeon/cvmx-pip.h
index 6797430..b8b2669 100644
--- a/arch/mips/include/asm/octeon/cvmx-pip.h
+++ b/arch/mips/include/asm/octeon/cvmx-pip.h
@@ -42,7 +42,7 @@
  *
  * Interface to the hardware Packet Input Processing unit.
  *
- * <hr>$Revision: 103836 $<hr>
+ * <hr>$Revision: 106617 $<hr>
  */
 
 #ifndef __CVMX_PIP_H__
@@ -666,9 +666,7 @@ static inline void cvmx_pip_config_port(uint64_t ipd_port, cvmx_pip_prt_cfgx_t p
  */
 static inline void cvmx_pip_config_vlan_qos(uint64_t vlan_priority, uint64_t qos)
 {
-	if (octeon_has_feature(OCTEON_FEATURE_PKND)) {
-		/* FIXME for 68xx. */
-	} else {
+	if (!octeon_has_feature(OCTEON_FEATURE_PKND)) {
 		cvmx_pip_qos_vlanx_t pip_qos_vlanx;
 		pip_qos_vlanx.u64 = 0;
 		pip_qos_vlanx.s.qos = qos;
@@ -684,9 +682,7 @@ static inline void cvmx_pip_config_vlan_qos(uint64_t vlan_priority, uint64_t qos
  */
 static inline void cvmx_pip_config_diffserv_qos(uint64_t diffserv, uint64_t qos)
 {
-	if (octeon_has_feature(OCTEON_FEATURE_PKND)) {
-		/* FIXME for 68xx. */
-	} else {
+	if (!octeon_has_feature(OCTEON_FEATURE_PKND)) {
 		cvmx_pip_qos_diffx_t pip_qos_diffx;
 		pip_qos_diffx.u64 = 0;
 		pip_qos_diffx.s.qos = qos;
diff --git a/arch/mips/include/asm/octeon/cvmx-pki-cluster.h b/arch/mips/include/asm/octeon/cvmx-pki-cluster.h
index 15541a1..186ec45 100644
--- a/arch/mips/include/asm/octeon/cvmx-pki-cluster.h
+++ b/arch/mips/include/asm/octeon/cvmx-pki-cluster.h
@@ -1,4 +1,4 @@
-/* This file is autgenerated from obj/ipemainc.elf */
+/* This file is autogenerated from obj/ipemainc.elf */
 const int cvmx_pki_cluster_code_length = 673;
 const uint64_t cvmx_pki_cluster_code_default[] = {
     0x000000000a000000ull,
diff --git a/arch/mips/include/asm/octeon/cvmx-pki-resources.h b/arch/mips/include/asm/octeon/cvmx-pki-resources.h
index edb3dd0..64aebb8 100644
--- a/arch/mips/include/asm/octeon/cvmx-pki-resources.h
+++ b/arch/mips/include/asm/octeon/cvmx-pki-resources.h
@@ -161,6 +161,26 @@ int cvmx_pki_cluster_free(int node, uint64_t cluster_mask);
 int cvmx_pki_pcam_entry_free(int node, int index, int bank, uint64_t cluster_mask);
 
 /**
+ * This function allocates/reserves a bpid from pool of global bpid per node.
+ * @param node	node to allocate bpid from.
+ * @param bpid	bpid  to allocate, if -1 it will be allocated
+ *		first available boid from bpid resource. If index is positive
+ *		number and in range, it will try to allocate specified bpid.
+ * @return 	bpid number on success,
+ *		-1 on alloc failure.
+ *		-2 on resource already reserved.
+ */
+int cvmx_pki_bpid_alloc(int node, int bpid);
+
+/**
+ * This function frees a bpid from pool of global bpid per node.
+ * @param node	 node to free bpid from.
+ * @param style	 bpid to free
+ * @return 	 0 on success, -1 on failure or
+ */
+int cvmx_pki_bpid_free(int node, int bpid);
+
+/**
  * This function frees all the PKI software resources
  * (clusters, styles, qpg_entry, pcam_entry etc) for the specified node
  */
diff --git a/arch/mips/include/asm/octeon/cvmx-pko3.h b/arch/mips/include/asm/octeon/cvmx-pko3.h
index e020067..fdd7427 100644
--- a/arch/mips/include/asm/octeon/cvmx-pko3.h
+++ b/arch/mips/include/asm/octeon/cvmx-pko3.h
@@ -74,9 +74,10 @@ extern "C" {
 
 #define	CVMX_PKO3_DQ_MAX_DEPTH	(48*256)
 
-/* dwords are from 1-16 */
+/* dwords count from 1-16 */
 /* scratch line for LMT operations */
-#define CVMX_PKO_LMTLINE 2ull	//FIXME- should go somewhere else ?
+/* Should be unique wrt other uses of CVMSEG, e.g. IOBDMA */
+#define CVMX_PKO_LMTLINE 2ull
 
 enum {
 	CVMX_PKO_PORT_QUEUES = 0,
@@ -202,9 +203,9 @@ union cvmx_pko_send_free {
 	uint64_t u64;
 	struct {
                 CVMX_BITFIELD_FIELD(uint64_t rsvd_48_63 : 16,
-                CVMX_BITFIELD_FIELD(uint64_t subdc4 	: 4, /* NODE+LAURA */
-                CVMX_BITFIELD_FIELD(uint64_t ns 	: 2,
-                CVMX_BITFIELD_FIELD(uint64_t addr 	: 42,
+                CVMX_BITFIELD_FIELD(uint64_t subdc4	: 4, /* 0x9 */
+                CVMX_BITFIELD_FIELD(uint64_t rsvd	: 2,
+                CVMX_BITFIELD_FIELD(uint64_t addr	: 42,
 			))));
 	} s;
 };
diff --git a/arch/mips/include/asm/octeon/cvmx-usb.h b/arch/mips/include/asm/octeon/cvmx-usb.h
index 44121cc..ed1f340 100644
--- a/arch/mips/include/asm/octeon/cvmx-usb.h
+++ b/arch/mips/include/asm/octeon/cvmx-usb.h
@@ -516,7 +516,7 @@ typedef enum {
 	CVMX_USB_COMPLETE_SUCCESS,
 				    /**< The transaction / operation finished without any errors */
 	CVMX_USB_COMPLETE_SHORT,
-				    /**< FIXME: This is currently not implemented */
+				    /**< This is currently not implemented */
 	CVMX_USB_COMPLETE_CANCEL,
 				    /**< The transaction was canceled while in flight by a user call to cvmx_usb_cancel* */
 	CVMX_USB_COMPLETE_ERROR,
diff --git a/arch/mips/include/asm/octeon/cvmx-wqe.h b/arch/mips/include/asm/octeon/cvmx-wqe.h
index c2cc82e..7fcc53c 100644
--- a/arch/mips/include/asm/octeon/cvmx-wqe.h
+++ b/arch/mips/include/asm/octeon/cvmx-wqe.h
@@ -1079,7 +1079,7 @@ static inline void cvmx_wqe_set_xgrp(cvmx_wqe_t *work, int grp)
 {
 	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE))
 		work->word1.cn78xx.grp = grp;
-	if (octeon_has_feature(OCTEON_FEATURE_CN68XX_WQE))
+	else if (octeon_has_feature(OCTEON_FEATURE_CN68XX_WQE))
 		work->word1.cn68xx.grp = grp;
 	else
 		work->word1.cn38xx.grp = grp;
@@ -1395,13 +1395,32 @@ static inline int cvmx_wqe_is_l2_mcast(cvmx_wqe_t *work)
 		return (work->word2.s_cn38xx.is_mcast);
 }
 
+static inline void cvmx_wqe_set_l2_bcast(cvmx_wqe_t *work, bool bcast)
+{
+	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
+		cvmx_wqe_78xx_t* wqe = (void *)work;
+		wqe->word2.is_l2_bcast = bcast;
+	} else
+		work->word2.s_cn38xx.is_bcast = bcast;
+}
+
+static inline void cvmx_wqe_set_l2_mcast(cvmx_wqe_t *work, bool mcast)
+{
+	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
+		cvmx_wqe_78xx_t* wqe = (void *)work;
+		wqe->word2.is_l2_mcast = mcast;
+	} else
+		work->word2.s_cn38xx.is_mcast = mcast;
+}
+
 static inline int cvmx_wqe_is_l3_bcast(cvmx_wqe_t *work)
 {
 	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
 		cvmx_wqe_78xx_t* wqe = (void *)work;
 		return (wqe->word2.is_l3_bcast);
-	} else
-		cvmx_dprintf("%s: ERROR: not supported for model\n",__func__);
+	}
+	cvmx_dprintf("%s: ERROR: not supported for model\n",__func__);
+	return 0;
 }
 
 static inline int cvmx_wqe_is_l3_mcast(cvmx_wqe_t *work)
@@ -1409,8 +1428,9 @@ static inline int cvmx_wqe_is_l3_mcast(cvmx_wqe_t *work)
 	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)){
 		cvmx_wqe_78xx_t* wqe = (void *)work;
 		return (wqe->word2.is_l3_mcast);
-	} else
-		cvmx_dprintf("%s: ERROR: not supported for model\n",__func__);
+	}
+	cvmx_dprintf("%s: ERROR: not supported for model\n",__func__);
+	return 0;
 }
 
 /**
diff --git a/arch/mips/include/asm/octeon/octeon-boot-info.h b/arch/mips/include/asm/octeon/octeon-boot-info.h
index c0c6df5..4233d11 100644
--- a/arch/mips/include/asm/octeon/octeon-boot-info.h
+++ b/arch/mips/include/asm/octeon/octeon-boot-info.h
@@ -122,7 +122,6 @@ typedef struct  boot_init_vector boot_init_vector_t;
 #undef GD_TMP_STR_SIZE
 #define GD_TMP_STR_SIZE 32
 
-/* FIXME: This structure is not endianness-neutral */
 /* This structure is deprecated, use sysinfo instead */
 struct linux_app_global_data {
 #if	_MIPS_SZPTR == 32
-- 
1.8.2.1

