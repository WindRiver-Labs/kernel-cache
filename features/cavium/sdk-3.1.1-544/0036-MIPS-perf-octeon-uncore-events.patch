From 3ba8e2254ba722dd53d002d9cac0609c19d97b15 Mon Sep 17 00:00:00 2001
From: Peter Swain <peter.swain@cavium.com>
Date: Tue, 19 Nov 2013 16:04:34 -0800
Subject: [PATCH 036/202] MIPS: perf: octeon uncore events

Add support for the uncore (non-cpu) performance counters
of Cavium Octeon processors.

Each SoC has one or more Local Memory Controllers (LMC),
and Level-2 Cache Controllers (L2C).

Octeon1/1-Plus (cn3x/cn5x) have fixed-function LMC counters
accessed as _HI/_LO 32bit halves.
Octeon2/Oceton3 (cn6x/cn7x) access LMC counters as 64bit.

Octeon1(-plus) has 4 variable-function counters for L2C,
selectable across various events like the mips cpu perf counters.

Octeon2/3 have a collection of fixed-function counters on L2C,
plus 4 variable-function counters for each TagAndData (TAD) unit
attached to each L2C.

The fixed-function registers are polled here (UNC_DIRECT), and
variable-function counters (UNC_MAPPED events) are handled by callout
to cvmx executive functions shared with stand-alone environment.

Because kernel build is generic across several chips, and L2C/LMC
register count is dependent on boot-time LMC config options (1/2/4 banks),
each UNC_DIRECT event's properties are explored at runtime event->pmu->init(),
where register addresses, count & stride are saved in per-event structure.
Hence the periodic sampling calls to event->pmu->read() are simplified,
reducing sampling overhead.

To allow for easy extension to further non-cpu performance counters,
much of the per-event detail is captured in perf_uncore_events.h,
as a set ov EV() macro invocations passing all the properties of the event.
This file is included in multiple contexts, each with different EV() define.
I don't like this structure, but it was better than the alternative while code
was evolving.

Further development possibilities:
- Register some of the uncore_l2c events within perf's standard namespace
  (like LLC-loads, LLC-store-misses, full list shown by "perf list")

- Fix perf's event-name parser so event-names like uncore_l2c/store/ are
  allowed (currently "cycles", "store", "miss" and "ops" are reserved
  event names, even _within_ a subsystem/.../ construct)

- Consider dropping EV() macros (useful while evolving), open-coding now-stable framework.
  The multi-include EV() structure was for flexibility while adding other unc_xxx classes
  (like pow/pki/...), but their footprint is probably unsurprising.
  Perhaps should try adding one (pow-stats? ocla??) to confirm framework adequate
  before tearing down EV() wrappers?
  Or if we _do_ want to add more classes, perhaps this EV() is already nice abstraction,
  allows perf_uncore_events.h to include uncore_pow.h, uncore_pki.h etc.

- Consider widening to cn78-ready NUMA version (just rename /sys/devices/uncore_%s%d,
  with node always zero for now

- Consider^2 (eg) uncore_mc/0 .../1 .../sum for direct stats access
  where /sum is what's now produces

- FIXME: propagate CVMX_xxxx_amax() _bmax() for all interesting alpha,beta
  (HRM's terminology) into cvmx*.h to simplify the CVMX_xxxx_CNTX(offset,block)
  definitions, so all the limit checking is done in these functions,
  which can be called from drivers.
  This would remove the need to cut+paste limit-checks into drivers.
  (must build unchanged SE+Linux+usercode)

Signed-off-by: Peter Swain <peter.swain@cavium.com>
[Original patch taken from Cavium SDK 3.1.1-544]
Signed-off-by: Bin Jiang <bin.jiang@windriver.com>
---
 arch/mips/cavium-octeon/Kconfig              |   7 +
 arch/mips/cavium-octeon/Makefile             |   5 +-
 arch/mips/cavium-octeon/perf_uncore.c        | 915 +++++++++++++++++++++++++++
 arch/mips/cavium-octeon/perf_uncore_events.h | 207 ++++++
 4 files changed, 1133 insertions(+), 1 deletion(-)
 create mode 100644 arch/mips/cavium-octeon/perf_uncore.c
 create mode 100644 arch/mips/cavium-octeon/perf_uncore_events.h

diff --git a/arch/mips/cavium-octeon/Kconfig b/arch/mips/cavium-octeon/Kconfig
index eed2e3a..963cf77 100644
--- a/arch/mips/cavium-octeon/Kconfig
+++ b/arch/mips/cavium-octeon/Kconfig
@@ -199,6 +199,13 @@ config CAVIUM_OCTEON_KERNEL_CRYPTO
 	  crypto state.  If OCTEON crypto instruction support is
 	  needed, select this option.
 
+config CAVIUM_OCTEON_PERF
+	bool "OCTEON-specific hardware performance counters"
+	default y
+	depends on PERF_EVENTS
+	help
+	  support extra performance counters, including L2 cache & DRAM controller
+
 config ARCH_SPARSEMEM_ENABLE
 	def_bool y
 	select SPARSEMEM_STATIC
diff --git a/arch/mips/cavium-octeon/Makefile b/arch/mips/cavium-octeon/Makefile
index b773990..e17f4ae 100644
--- a/arch/mips/cavium-octeon/Makefile
+++ b/arch/mips/cavium-octeon/Makefile
@@ -17,7 +17,6 @@ obj-y := setup.o serial.o octeon-platform.o octeon-irq.o csrc-octeon.o \
 obj-y += dma-octeon.o
 obj-y += csrc-octeon-ptp.o
 obj-y += octeon-pci-console.o
-obj-y += perf_counters.o
 obj-y += octeon-memcpy.o
 obj-y += executive/
 
@@ -32,6 +31,10 @@ obj-$(CONFIG_CAVIUM_OCTEON_KERNEL_CRYPTO) += octeon-crypto.o
 obj-$(CONFIG_OCTEON_ERROR_INJECTOR)	+= octeon-error-injector.o
 obj-$(CONFIG_CAVIUM_OCTEON_RAPIDIO)	+= octeon-rapidio.o
 
+obj-$(CONFIG_PERF_EVENTS)		+= perf_counters.o
+obj-$(CONFIG_CAVIUM_OCTEON_PERF)	+= perf_uncore.o
+
+
 DTS_FILES = octeon_3xxx.dts octeon_68xx.dts
 DTB_FILES = $(patsubst %.dts, %.dtb, $(DTS_FILES))
 
diff --git a/arch/mips/cavium-octeon/perf_uncore.c b/arch/mips/cavium-octeon/perf_uncore.c
new file mode 100644
index 0000000..74afc15
--- /dev/null
+++ b/arch/mips/cavium-octeon/perf_uncore.c
@@ -0,0 +1,915 @@
+#ifdef CONFIG_DEBUG_KERNEL
+# define DEBUG
+#endif
+#define pr_fmt(f) "uncore: " f
+/*
+ * Copyright (C) 2013 Cavium Networks
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * Inspired by the AMD uncore handling
+ */
+
+#include <linux/perf_event.h>
+#include <linux/types.h>
+#include <linux/slab.h>
+#include <linux/init.h>
+#include <linux/cpu.h>
+#include <linux/cpumask.h>
+#include <linux/module.h>
+#include <linux/perf_event.h>
+#include <linux/stringify.h>
+#include <linux/printk.h>
+
+#include <asm/cpu-features.h>
+#include <asm/octeon/octeon.h>
+
+/*
+ * perf_uncore.debug=1 enables tracing here and cvmx-*.h
+ */
+static bool debug;
+module_param(debug, bool, 0644);
+
+static int unc_traces = 100; /* lines to log */
+module_param(unc_traces, int, 0644);
+
+static void safe_printk(const char *fmt, ...);
+#undef pr_debug
+#define pr_debug(fmt, ...) safe_printk(fmt, __VA_ARGS__)
+
+#include <asm/octeon/cvmx-core.h>
+#include <asm/octeon/cvmx-l2c.h>
+#include <asm/octeon/cvmx-l2c-defs.h>
+#include <asm/octeon/cvmx-lmcx-defs.h>
+
+/* see also:
+ * arch/mips/cavium-octeon/executive/cvmx-l2c.c
+ *
+ * sdk/bootloader/u-boot/arch/mips/cpu/octeon/octeon3_lmc.h
+ * sdk/examples/lmc-dump/lmc-dump.c
+ *
+ * These are the un-core counters, those not associated with a cpu core.
+ * (for per-core counters, see arch/mips/cavium-octeon/perf_counters.c)
+ *
+ * We have a mix of fixed-function counters (which count (if enabled)
+ * a pre-determined event type), and variable-function counters which
+ * are each configured to count a dynamically-selectable event class.
+ *
+ * A Mem-controller (LMC) has 3 fixed-function counters.
+ * A Level-2-cache-controller has:
+ *  4 variable-function counters for each tag-and-data (TAD) unit
+ *  7 fixed-function counters (XMC/XMD/RSC/RSD/INV/IOC/IOR)
+ *  We program each TAD's counters identically & sum their counts.
+ */
+
+#ifndef CONFIG_EARLY_PRINTK
+/* if not supported, just stub out ... */
+static void early_vprintk(const char *fmt, va_list ap) {}
+#endif
+
+/* use early_printk() for safe visibility in perf irq context ... */
+static void safe_printk(const char *fmt, ...)
+{
+	va_list ap;
+
+	if (!debug || unc_traces-- < 0)
+		return;
+	va_start(ap, fmt);
+	if (irqs_disabled())
+		early_vprintk(fmt, ap);
+	else
+		vprintk(fmt, ap);
+	va_end(ap);
+}
+
+/*
+ * Many of the fixed-function perf counters have multiple
+ * instances, depending on chip-type and runtime config.
+ * These 4 lim_xxx() functions cover all varieties ...
+ */
+
+/* register count for O23 L2C_IO[CR]X_PFC */
+static int lim_one(void) { return 1; }
+
+/* register count for CVMX_LMCX_DCLK/_OPS/_IFB */
+static int lim_cvmx_lmcx(void)
+{
+	int lim = 0;
+	int tad;
+
+	/*
+	 * Prepare to walk over all _enabled_ LMC banks:
+	 * - o1/o1p have _exactly_ one, with no quad_dll_ena bit;
+	 * - o2/o3 have 1/2/4 LMCs defined, but upper LMCs may be disabled.
+	 * LMC banks will never be sparse (eg: 0+2 enabled, 1+3 disabled)
+	 */
+	if (OCTEON_IS_OCTEON1PLUS())
+		return 1;
+	for (tad = 0; tad < CVMX_L2C_TADS; tad++) {
+		union cvmx_lmcx_dll_ctl2 ctl2;
+		ctl2.u64 = cvmx_read_csr(CVMX_LMCX_DLL_CTL2(tad));
+		if (current_cpu_type() == CPU_CAVIUM_OCTEON3) {
+			if (ctl2.cn70xx.quad_dll_ena == 0)
+				continue;
+		} else if (ctl2.cn63xx.quad_dll_ena == 0)
+			continue;
+		lim++;
+	}
+	return lim;
+}
+
+/* register count for O23 L2C_XM[CD]X_PFC _RS[CD]X_PFC */
+static int lim_o23_xmcd_rsdc(void)
+{
+	switch (cvmx_get_octeon_family()) {
+	case OCTEON_CN68XX & OCTEON_FAMILY_MASK:
+		return 4;
+	case OCTEON_CN78XX & OCTEON_FAMILY_MASK:
+		return 10;
+	}
+	return 1;
+}
+
+/* register count for O3-only L2C_INVX_PFC */
+static int lim_o3_l2c_inv(void)
+{
+	switch (cvmx_get_octeon_family()) {
+	case OCTEON_CN70XX & OCTEON_FAMILY_MASK:
+		return 1;
+	case OCTEON_CN78XX & OCTEON_FAMILY_MASK:
+		return 8;
+	}
+	return 0;
+}
+
+/* find our extended state from a generic perf_event ... */
+#define e_unc(e) (*(struct oct_uncore **)&(e)->hw.event_base)
+#define e_uev(e) (&(e_unc(e)->uevent0[(e)->attr.config]))
+
+/* set up unique enum ids within each scope, to tie to switch() in _init() */
+#undef EV
+#define EV(_family, _name, ...) EVID(_family, _name),
+#include "perf_uncore_events.h"
+enum { LMC_EVENTS };
+enum { O1P_EVENTS };
+enum { O23_EVENTS };
+enum { TAD_EVENTS };
+
+enum unc_flags { /* event attributes passed in EV() _flags */
+	UNC_TYPES	= 0x1f, /* mask of PMU types */
+	UNC_LMC		= 0x1,
+	UNC_O1P		= 0x2,
+	UNC_O2		= 0x4,
+	UNC_O3		= 0x8,
+	UNC_O23		= (UNC_O2|UNC_O3),
+	UNC_ANY		= (UNC_O1P|UNC_O23),
+	UNC_TAD		= 0x10,
+	UNC_DIRECT	= 0, /* fixed counter/event mapping */
+	UNC_MAPPED	= 0x80, /* selectable counter/event mapping */
+};
+
+#define NUM_COUNTERS_L2C	CVMX_L2C_MAX_PCNT /* 4 o1p or tad ctrs*/
+
+/*
+ * struct uncore_event_desc - a per-event-type object, statically allocated
+ *
+ * Why not use perf_event.hw.event_base_rdpmc for register addresses?
+ * Pre-computed regs make _read faster, but the usual event_base_rdpmc
+ * is 'int', and for Octeon1 we need 2 64bit counters addrs (_CNT_LO &
+ * _CNT_HI), so we could choose 2 ulong objects (event_base & extra_reg.config)
+ * to hold our counter-addresses, and squeeze .stride & .counters into other
+ * standard members.
+ * But it gets ugly & complicated, so break them out into a custom
+ * struct oct_uncore, and just misuse one perf_event element (.hw.event_base)
+ * to point to that.  Could be simplified.
+ */
+struct uncore_event_desc {
+#define UEV_MAGIC 74548027
+#ifdef UEV_MAGIC
+	int magic; /* sanity check */
+#endif
+	int attno; /* linear attribute#, set in _init scan */
+	enum unc_flags flags;
+	enum cvmx_l2c_event cvmx_event; /* mapped events */
+	struct kobj_attribute attr;
+	u16 counters;
+	u32 stride;
+	u64 reglo;
+	u64 reghi;
+};
+
+static ssize_t uncore_event_show(struct kobject *kobj,
+	struct kobj_attribute *attr, char *buf)
+{
+	struct uncore_event_desc *uevent =
+		container_of(attr, struct uncore_event_desc, attr);
+	return sprintf(buf, "event=0x%x", uevent->attno);
+}
+
+#undef EV
+#define EV(_family, _name, _cvmx, _flags, ...)			\
+{								\
+	.magic = UEV_MAGIC,					\
+	.attno = EVID(_family, _name),				\
+	.attr	= __ATTR(_name, 0444, uncore_event_show, NULL),	\
+	.flags = _flags,					\
+	/* mapped-only */ .cvmx_event = _cvmx,			\
+}, /*end*/
+
+#include "perf_uncore_events.h"
+
+static struct uncore_event_desc lmc_events[] = {
+	LMC_EVENTS
+	{ /* zeros */ },
+};
+static struct uncore_event_desc o1p_events[] = {
+	O1P_EVENTS
+	{ /* zeros */ },
+};
+static struct uncore_event_desc o23_events[] = {
+	O23_EVENTS
+	{ /* zeros */ },
+};
+static struct uncore_event_desc tad_events[] = {
+	TAD_EVENTS
+	{ /* zeros */ },
+};
+
+static struct pmu oct_lmc_pmu;
+static struct pmu o1p_l2c_pmu;
+static struct pmu o23_l2c_pmu;
+static struct pmu tad_l2c_pmu;
+
+
+struct oct_uncore {
+	int id;
+	int refcnt;
+	unsigned num_shared_regs:8;
+	unsigned single_fixed:1;
+	struct pmu *pmu;
+	const struct attribute_group *attr_groups[4];
+	int num_counters; /* dimensions uevents[] */
+	struct uncore_event_desc *uevent0; /* static order, indexed by evno */
+	struct uncore_event_desc *uevents[0]; /* current mapping */
+};
+
+#define events_group attr_groups[0]
+#define format_group attr_groups[1]
+#define pmu_group attr_groups[2]
+
+/* all counters are 64bit, whole word */
+PMU_FORMAT_ATTR(event, "config:0-63");
+/* sample from just 1 cpu, always cpu0 for simplicity */
+PMU_FORMAT_ATTR(cpumask, "0"); /* not a mask, but a list of cpu numbers */
+
+static struct attribute *uncore_format_attrs[] = {
+	&format_attr_event.attr,
+	NULL,
+};
+
+
+static struct attribute_group uncore_format_group = {
+	.name = "format",
+	.attrs = uncore_format_attrs,
+};
+
+static struct attribute *uncore_pmu_attrs[] = {
+	&format_attr_cpumask.attr,
+	NULL,
+};
+static struct attribute_group uncore_pmu_attr_group = {
+	.attrs = uncore_pmu_attrs,
+};
+
+
+static struct oct_uncore *oct_uncore_lmc;
+static struct oct_uncore *o1p_uncore_l2c;
+static struct oct_uncore *o23_uncore_l2c;
+static struct oct_uncore *tad_uncore_l2c;
+static struct oct_uncore *live_uncores[4+1]; /* list for teardown, simplify? */
+
+static inline bool is_lmc_event(struct perf_event *event)
+{
+	/* FIXME: speedup by testing e_uev(e)_>flags & UNC_xxx... */
+	return event->pmu == &oct_lmc_pmu;
+}
+
+static inline bool is_o1p_l2c_event(struct perf_event *event)
+{
+	return event->pmu == &o1p_l2c_pmu;
+}
+
+static inline bool is_o23_l2c_event(struct perf_event *event)
+{
+	return event->pmu == &o23_l2c_pmu;
+}
+
+static inline bool is_tad_l2c_event(struct perf_event *event)
+{
+	return event->pmu == &tad_l2c_pmu;
+}
+
+static inline bool is_mapped_event(struct perf_event *event)
+{
+	return is_o1p_l2c_event(event) || is_tad_l2c_event(event);
+}
+
+static struct oct_uncore *event_to_oct_uncore(struct perf_event *event)
+{
+	if (!event)
+		return NULL;
+	else if (e_unc(event))
+		return e_unc(event);
+	/* these are only for _init, should separate 2 use-cases ... */
+	/* FIXME: faster with UNC_xxx flags */
+	else if (is_lmc_event(event) && oct_uncore_lmc)
+		return oct_uncore_lmc;
+	else if (is_o1p_l2c_event(event) && o1p_uncore_l2c)
+		return o1p_uncore_l2c;
+	else if (is_o23_l2c_event(event) && o23_uncore_l2c)
+		return o23_uncore_l2c;
+	else if (is_tad_l2c_event(event) && tad_uncore_l2c)
+		return tad_uncore_l2c;
+
+	return NULL;
+}
+
+static inline
+struct uncore_event_desc *event_to_oct_uevent(struct perf_event *event)
+{
+	struct uncore_event_desc *uev;
+
+	if (!event)
+		return NULL;
+	uev = e_uev(event);
+#ifdef UEV_MAGIC
+	BUG_ON(uev->magic != UEV_MAGIC);
+#endif
+	return uev;
+}
+
+/*
+ * o1 l2c_pfc(0..3) 36bits, so shift left/right (64-36) to sign-extend diff
+ *    or use clear-on-read to avoid need, just dribble in increments
+ * o1 lmc_xxx_cnt_lo/hi 32+32, 8 bytes apart (build [0],[8] in)
+ * o1p (same)
+ *
+ * 6x lmc_xxx(0..3)_cnt 64bit
+ * 6x l2c_xxx(0..3)_cnt 64bit
+ * 6x l2c_tad(0..3)_pfc(0..3) 64bit
+ * 0..3 on 68, 0-only on some
+ *
+ * 71 lmc_xxx(0)_cnt 64bit
+ * 71 l2c_xxx(0)_cnt 64bit
+ * 71 l2c_tad(0)_pfc(0..3) 64bit
+ *
+ * 78 lmc_xxx(0..3)_cnt 64bit
+ * 78 l2c_xxx(0..9)_cnt 64bit
+ * 78 l2c_tad(0..7)_pfc(0..3) 64bit
+ */
+
+/* mapped samples sign-extended by <<,>> ? */
+static int mapped_sextend_bits;
+
+static void new_sample(struct perf_event *event, u64 new, int shift)
+{
+	struct hw_perf_event *hwc = &event->hw;
+	u64 prev = local64_read(&hwc->prev_count);
+	s64 delta;
+
+	local64_set(&hwc->prev_count, new);
+	delta = (new << shift) - (prev << shift);
+	delta >>= shift;
+	local64_add(delta, &event->count);
+}
+
+static void unc_direct_read(struct perf_event *event)
+{
+	struct uncore_event_desc *uev = e_uev(event);
+	uint64_t lo = uev->reglo;
+	uint64_t hi = uev->reghi;
+	u64 new = 0;
+	int i;
+	int counters = uev->counters;
+	int stride = uev->stride;
+
+	/*
+	 * since we do not enable counter overflow interrupts,
+	 * we do not have to worry about prev_count changing on us
+	 */
+	for (i = 0; i < counters; i++) {
+		u64 inc = 0;
+		if (lo)
+			inc = cvmx_read_csr(lo+i*stride);
+		else
+			pr_debug("no cvmx mapping for ev=%llx\n",
+				event->attr.config);
+		if (hi)
+			inc |= (cvmx_read_csr(hi+i*stride) << 32);
+		new += inc;
+	}
+
+	new_sample(event, new, 0);
+	pr_debug("%s direct ev=%d:%d"
+		" new %llx, lo %llx, hi %llx, n %d += %d\n",
+		__func__, event->pmu->type, event->hw.idx,
+		new, lo, hi, counters, stride);
+}
+
+static void unc_mapped_read(struct perf_event *event)
+{
+	u64 new = cvmx_l2c_read_perf(event->hw.idx);
+
+	/*
+	 * since we do not enable counter overflow interrupts,
+	 * we do not have to worry about prev_count changing on us
+	 */
+	new_sample(event, new, mapped_sextend_bits);
+}
+
+static void oct_uncore_start(struct perf_event *event, int flags)
+{
+	struct hw_perf_event *hwc = &event->hw;
+
+	if (flags & PERF_EF_RELOAD)
+		event->pmu->read(event);
+	local64_set(&event->count, 0);
+
+	hwc->state = 0;
+}
+
+static void oct_uncore_stop(struct perf_event *event, int flags)
+{
+	struct hw_perf_event *hwc = &event->hw;
+
+	hwc->state |= PERF_HES_STOPPED;
+
+	if ((flags & PERF_EF_UPDATE) && !(hwc->state & PERF_HES_UPTODATE)) {
+		event->pmu->read(event);
+		hwc->state |= PERF_HES_UPTODATE;
+	}
+}
+
+/*
+ * oct_uncore_add(event, flags)
+ *
+ * [maybe split into per-type _add_lmc/_add_tad/_add_o1p/_add_o23 ??
+ * no, too much clutter]
+ *
+ * For PMDs which map N events down to C counters ( C < N )
+ * MIPS-core perf counters are like this (but use own code),
+ * as is L2C's 4-of-many event structure.
+ * But LMC has only 3 fixed-function counters,
+ * and L2C also has some fixed-function counters.
+ * Leave this general, for later handling uncore mapped-counters from
+ * SSO/PKI/PKO/ASE/OCLA-stack/... ???
+ * OTOH, these have _mapped_ regs:
+ * 4-way map poking via L2C_TADa_PFCb for a in 0..9, b in 0..3
+ */
+static int unc_mapped_add(struct perf_event *event,
+	int flags, struct oct_uncore *uncore)
+{
+	int i;
+	struct uncore_event_desc *uevent = event_to_oct_uevent(event);
+	struct hw_perf_event *hwc = &event->hw;
+
+	/* are we already assigned? */
+	if (hwc->idx != -1 && uncore->uevents[hwc->idx] == uevent)
+		goto out;
+
+	for (i = 0; i < uncore->num_counters; i++) {
+		if (uncore->uevents[i] == uevent) {
+			hwc->idx = i;
+			goto out;
+		}
+	}
+
+	/* if not, take the first available counter */
+	hwc->idx = -1;
+	for (i = 0; i < uncore->num_counters; i++) {
+		if (uncore->uevents[i] == uevent) {
+			hwc->idx = i;
+			break;
+		} else if (cmpxchg(&uncore->uevents[i], NULL, uevent) == NULL) {
+			hwc->idx = i;
+			break;
+		}
+	}
+
+out:
+	if (hwc->idx < 0)
+		return -EBUSY;
+
+	cvmx_l2c_config_perf(hwc->idx, uevent->cvmx_event, 0);
+	hwc->state = PERF_HES_UPTODATE | PERF_HES_STOPPED;
+
+	if (flags & PERF_EF_START)
+		oct_uncore_start(event, PERF_EF_RELOAD);
+
+	perf_event_update_userpage(event);
+	return 0;
+}
+
+static int unc_o1p_add(struct perf_event *event, int flags)
+{
+	return unc_mapped_add(event, flags, o1p_uncore_l2c);
+}
+
+static int unc_tad_add(struct perf_event *event, int flags)
+{
+	return unc_mapped_add(event, flags, tad_uncore_l2c);
+}
+
+static int unc_direct_add(struct perf_event *event,
+	int flags, struct uncore_event_desc *uevents)
+{
+	struct hw_perf_event *hwc = &event->hw;
+
+	hwc->state = PERF_HES_UPTODATE | PERF_HES_STOPPED;
+
+	if (flags & PERF_EF_START)
+		oct_uncore_start(event, PERF_EF_RELOAD);
+
+	perf_event_update_userpage(event);
+	return 0;
+}
+
+static int unc_lmc_add(struct perf_event *event, int flags)
+{
+	return unc_direct_add(event, flags, lmc_events);
+}
+
+static int unc_o23_add(struct perf_event *event, int flags)
+{
+	return unc_direct_add(event, flags, o23_events);
+}
+
+static void unc_direct_del(struct perf_event *event, int flags)
+{
+	oct_uncore_stop(event, PERF_EF_UPDATE);
+	perf_event_update_userpage(event);
+}
+
+static void unc_mapped_del(struct perf_event *event,
+	int flags, struct oct_uncore *uncore)
+{
+	int i;
+	struct uncore_event_desc *uevent = event_to_oct_uevent(event);
+	struct hw_perf_event *hwc = &event->hw;
+
+	oct_uncore_stop(event, PERF_EF_UPDATE);
+
+	for (i = 0; i < uncore->num_counters; i++) {
+		if (cmpxchg(&uncore->uevents[i], uevent, NULL) == uevent)
+			break;
+	}
+
+	hwc->idx = -1;
+	perf_event_update_userpage(event);
+}
+
+static void unc_o1p_del(struct perf_event *event, int flags)
+{
+	unc_mapped_del(event, flags, o1p_uncore_l2c);
+}
+
+static void unc_tad_del(struct perf_event *event, int flags)
+{
+	unc_mapped_del(event, flags, tad_uncore_l2c);
+}
+
+static struct pmu oct_lmc_pmu = {
+	.add		= unc_lmc_add,
+	.del		= unc_direct_del,
+	.read		= unc_direct_read,
+};
+static struct pmu o1p_l2c_pmu = {
+	.add		= unc_o1p_add,
+	.del		= unc_o1p_del,
+	.read		= unc_mapped_read,
+};
+static struct pmu o23_l2c_pmu = {
+	.add		= unc_o23_add,
+	.del		= unc_direct_del,
+	.read		= unc_direct_read,
+};
+static struct pmu tad_l2c_pmu = {
+	.add		= unc_tad_add,
+	.del		= unc_tad_del,
+	.read		= unc_mapped_read,
+};
+
+static int oct_uncore_event_init(struct perf_event *event)
+{
+	struct oct_uncore *uncore;
+	struct uncore_event_desc *uev;
+	struct perf_event_attr *attr = &event->attr;
+	struct hw_perf_event *hwc = &event->hw;
+
+	if (!event || !event->pmu || !attr || attr->type != event->pmu->type)
+		return -ENOENT;
+
+	/*
+	 * Octeon has a single coherent Level-2 cache, shared by all cores
+	 * and all other DMAing hardware units, so L2C/LMC counters are shared.
+	 * Interrupts can be directed to a single target core, however, event
+	 * counts generated by processes running on other cores cannot be
+	 * masked out. So we do not support sampling and * per-thread events.
+	 */
+	if (is_sampling_event(event) || event->attach_state & PERF_ATTACH_TASK)
+		return -EINVAL;
+
+	/* uncore counters do not have usr/os/guest/host bits */
+	if (attr->exclude_user || attr->exclude_kernel ||
+	    attr->exclude_host || attr->exclude_guest)
+		return -EINVAL;
+	pr_debug("new ev ty:%d eb:%p ac:%llx\n",
+		attr->type, (void *)event->hw.event_base, attr->config);
+
+	e_unc(event) = NULL; /* force lookup in ... */
+	e_unc(event) = event_to_oct_uncore(event);
+
+	pr_debug("link ev unc@%p pmu@%p ty:%d ev0@%p\n",
+		e_unc(event),
+		(e_unc(event) ? e_unc(event)->pmu : NULL),
+		(e_unc(event) && e_unc(event)->pmu
+				? e_unc(event)->pmu->type : 0),
+		(e_unc(event) && e_unc(event)->uevent0
+				? e_unc(event)->uevent0 : NULL));
+	uev = event_to_oct_uevent(event);
+	if (uev) {
+		pr_debug("ev0[%lld]=%p\n", attr->config, uev);
+		pr_debug(".magic %d .attno %d, f %x, cvmx_event %x,"
+			" r %llx/%llx, n %d, stride %d\n",
+			uev->magic, uev->attno, uev->flags, uev->cvmx_event,
+			uev->reglo, uev->reghi, uev->counters, uev->stride);
+	}
+	uncore = event_to_oct_uncore(event);
+
+	if (!uncore)
+		return -ENODEV;
+
+	BUG_ON(!uev);
+	if (is_mapped_event(event)) {
+		hwc->idx = -1;
+	} else {
+		int ev = event->attr.config;
+		struct hw_perf_event *hwc = &event->hw;
+		struct uncore_event_desc *uev = e_uev(event);
+		int eflags = 0;
+		int (*lim)(void) = NULL;
+
+		hwc->idx = uev->attno; /* linearly mapped */
+
+		/*
+		 * UGLY, but is there a better way??
+		 * Build-time enumeration of all events, deferring until runtime
+		 * the eval of CVMX*_CNT & _CNT_LO/_CNT_HI macros
+		 */
+#undef EV
+#define EV(_family, _name, _cvmx, _flags, _lo, _hi, _stride, _lim) \
+	case EVID(_family, _name): \
+		pr_debug("ev:%d _lo:%s _hi:%s _stride:%s _lim:%s\n", \
+			ev, __stringify(_lo), __stringify(_hi), \
+			__stringify(_stride), __stringify(_lim)); \
+		hwc->idx = ev; \
+		eflags = (_flags); \
+		lim = (_lim); \
+		uev->reglo = (_lo); \
+		uev->reghi = (_hi); \
+		uev->stride = (_stride); \
+		break; /*end*/
+
+#include "perf_uncore_events.h"
+
+		if (is_lmc_event(event)) {
+			switch (ev) {
+				LMC_EVENTS /* one "case...break;" for each */
+			default:
+				WARN(true, "unexpected LMC event %d\n",
+					ev);
+				break;
+			}
+		}
+		if (is_o23_l2c_event(event)) {
+			switch (ev) {
+				O23_EVENTS /* one "case...break;" for each */
+			default:
+				WARN(true, "unexpected o2/o3 L2C event %d\n",
+					ev);
+				break;
+			}
+		}
+		uev->counters = (lim ? lim() : 1);
+		pr_debug("e%d l:%llx h:%llx stride:%d lim:%d\n",
+			ev, uev->reglo, uev->reghi,
+			uev->stride, uev->counters);
+
+
+		WARN_ONCE(!uev->reglo, "no counter for event %x\n", ev);
+		if (!uev->reglo)
+			return -ENOENT;
+
+		if (lim)
+			uev->counters = lim();
+
+		if (hwc->idx == -1)
+			return -EBUSY;
+	}
+
+	/* set counter baseline */
+	local64_set(&event->count, 0);
+	local64_set(&hwc->prev_count, 0);
+
+	return 0;
+}
+
+static void __init uncore_type_exit(struct oct_uncore *type)
+{
+	kfree(type->events_group);
+	type->events_group = NULL;
+}
+
+static int __init uncore_type_init(struct oct_uncore *type,
+		struct uncore_event_desc *uevents, const char *name)
+{
+	struct attribute_group *attr_group;
+	struct attribute **attrs;
+	bool o1p = OCTEON_IS_OCTEON1PLUS();
+	bool o2 = OCTEON_IS_OCTEON2();
+	bool o3 = OCTEON_IS_OCTEON3();
+	int i, j, k;
+
+	type->pmu->event_init	= oct_uncore_event_init;
+	type->pmu->start	= oct_uncore_start;
+	type->pmu->stop		= oct_uncore_stop;
+	type->pmu->task_ctx_nr	= perf_invalid_context;
+
+	if (uevents) {
+		for (i = 0; uevents[i].attr.attr.name; i++)
+			; /* just count */
+
+		attr_group = kzalloc(sizeof(struct attribute *) * i +
+					sizeof(*attr_group), GFP_KERNEL);
+		WARN_ON(!attr_group);
+		if (!attr_group)
+			goto fail;
+
+		attrs = (struct attribute **)(attr_group + 1);
+		attr_group->name = "events";
+		attr_group->attrs = attrs;
+
+		for (j = k = 0; j < i; j++) {
+			enum unc_flags f = uevents[j].flags;
+
+			/* skip counters not present on this chip */
+			if (!((o1p && (f & UNC_O1P)) ||
+			      (o2 && (f & UNC_O2)) ||
+			      (o3 && (f & UNC_O3))))
+				continue;
+
+			attrs[k] = &uevents[j].attr.attr;
+			pr_debug("attach %d:%s/%s:%s\n",
+				k, name, attr_group->name,
+				(attrs[k] ? attrs[k]->name : NULL));
+			k++;
+		}
+
+		type->events_group = attr_group;
+		type->format_group = &uncore_format_group;
+	}
+
+	type->pmu->attr_groups = type->attr_groups;
+	type->pmu_group = &uncore_pmu_attr_group;
+
+	return 0;
+fail:
+	pr_debug(pr_fmt("uncore_type_init(%s) ENOMEM\n"), name);
+	uncore_type_exit(type);
+	return -ENOMEM;
+}
+
+static struct oct_uncore *
+__init oct_uncore_register(char *name, struct pmu *pmu,
+		int num_counters, /* -ve for one-per uevent */
+		struct uncore_event_desc *uevents)
+{
+	struct uncore_event_desc *e;
+	struct oct_uncore *uncore;
+	struct oct_uncore **next;
+	size_t size;
+	bool fixed_map = (num_counters < 0);
+	int nevents;
+	int n;
+
+	/* count events */
+	for (nevents = 0, e = uevents; e->attr.attr.name; nevents++, e++)
+		;
+
+	if (fixed_map)
+		num_counters = nevents;
+
+	size = sizeof(struct oct_uncore) +
+		(num_counters+1) * sizeof(struct uncore_event_desc *);
+	uncore = kzalloc(size, GFP_KERNEL);
+	if (!uncore)
+		return NULL;
+
+	uncore->pmu = pmu;
+	uncore->num_counters = num_counters;
+
+	/* fixed list of possible events */
+	uncore->uevent0 = uevents;
+
+	/* count & (if possible) map */
+	for (n = 0, e = uevents; e->attr.attr.name; n++, e++)
+		if (fixed_map)
+			uncore->uevents[n] = e;
+
+	/* merge this into loop above ... */
+	n = uncore_type_init(uncore, uevents, name);
+	if (n) {
+		kfree(uncore);
+		return NULL;
+	}
+
+	n = perf_pmu_register(uncore->pmu, name, -1);
+	if (n) {
+		pr_debug(pr_fmt("perf_pmu_register(%s) err %d\n"),
+			name, n);
+		uncore_type_exit(uncore);
+		return NULL;
+	}
+
+	/* all is complete, add to list */
+	for (next = live_uncores; *next; next++)
+		;
+	*next = uncore;
+
+	pr_info(pr_fmt("%s counters detected\n"), name);
+
+	return uncore;
+}
+
+static void __init uncore_types_exit(struct oct_uncore **types)
+{
+	int i;
+
+	for (i = 0; types[i]; i++) {
+		uncore_type_exit(types[i]);
+		kfree(types[i]);
+		types[i] = NULL;
+	}
+}
+
+static void oct_uncore_exit(void)
+{
+	uncore_types_exit(live_uncores);
+	kfree(oct_uncore_lmc);
+	kfree(o1p_uncore_l2c);
+	kfree(o23_uncore_l2c);
+	kfree(tad_uncore_l2c);
+}
+module_exit(oct_uncore_exit);
+
+static int __init oct_uncore_init(void)
+{
+	/* octeon-1/2/3 Local Memory Controller */
+	oct_uncore_lmc = oct_uncore_register("uncore_mc",
+			&oct_lmc_pmu, -1, lmc_events);
+
+	if (OCTEON_IS_OCTEON1PLUS()) {
+		/* must sign-extend 36-bit O1P mapped counters */
+		mapped_sextend_bits = (64 - 36);
+
+		o1p_uncore_l2c = oct_uncore_register("uncore_l2c",
+				&o1p_l2c_pmu, NUM_COUNTERS_L2C, o1p_events);
+
+	} else {
+		int tad;
+
+		o23_uncore_l2c = oct_uncore_register("uncore_l2c",
+				&o23_l2c_pmu, -1, o23_events);
+
+		/* associate each TAD's 4 counters with the "NONE" event */
+		for (tad = 0; tad < CVMX_L2C_TADS; tad++)
+			cvmx_write_csr(CVMX_L2C_TADX_PRF(tad), 0);
+
+		tad_uncore_l2c = oct_uncore_register("uncore_tad",
+				&tad_l2c_pmu, NUM_COUNTERS_L2C, tad_events);
+	}
+
+	/* FIXME: generalize to fit existing perf_hw_cache_id hierarchy:
+	 * add some l2c events as PERF_COUNT_HW_CACHE_LL
+	 * and some perf_event_mipsxx.c events as other PERF_COUNT_HW_xxx
+	 */
+
+	return 0;
+
+	uncore_types_exit(live_uncores);
+
+	return -ENOMEM;
+}
+device_initcall(oct_uncore_init);
diff --git a/arch/mips/cavium-octeon/perf_uncore_events.h b/arch/mips/cavium-octeon/perf_uncore_events.h
new file mode 100644
index 0000000..33709cb
--- /dev/null
+++ b/arch/mips/cavium-octeon/perf_uncore_events.h
@@ -0,0 +1,207 @@
+/*
+ * Per-event defines for arch/mips/cavium-octeon/perf_events_uncore.c
+ *
+ * Included multiple times, because some properties are setup cleaner
+ * at build time, but others (like the _CNT/(_CNT_LO,_CNT_HI)) accessors
+ * cannot be resolved at run-time.
+ *
+ * Different calls can make use of different mix of properties,
+ * but they're all summarized here in one place.
+ *
+ * Each inclusion can differently define
+ *    EV(_family, _name, _cvmx, _flags, ...)
+ * to extract different subsets of properties.
+ *
+ * Where tables cannot be resolved at build time (octeon-model-specific
+ * addresses and/or existence), tie different pieces together at runtime
+ * with identifier EVID(f,n), which creates an enum used for runtime switch.
+ * This is carried in event->attr.config (notionally u64, but something zeros
+ * upper bits during perf-tool negotiation, so just use lower byte!)
+ */
+#ifndef EVID
+# define EVID(_family, _name) UNC_##_family##_##_name
+#endif
+
+/*
+ * Event-naming issues...
+ * "cycles", "store", "miss" and "ops" are reserved words to perf,
+ * they are arch-independent event names with fixed bindings,
+ * so not re-usable in (eg) uncore_xxx/cycles/ context.
+ * Until this is fixed in tools/perf/util/parse-events.l grammar,
+ * pointely mis-spell cycles as cyc, store as stor, ops as op, miss as mis.
+ * [all instances marked RENAME:xxx below]
+ */
+
+/* (re)undef to allow re-include, with varying EV() definitions */
+#undef OCT_EVENTS
+#undef LMC_EVENTS
+#undef O1P_EVENTS
+#undef O23_EVENTS
+#undef TAD_EVENTS
+
+/*
+ * LMCX: fixed-function counters for mem-controller stats:
+ * - Octeon1 has _HI/_LO, 2/3 has just _CNT
+ */
+#undef LMC_EV
+#define LMC_EV(_name, _cvmx) EV(lmc, _name, EVID(lmc, _name), \
+	UNC_DIRECT|UNC_ANY, \
+	/* _lo */ (OCTEON_IS_OCTEON1PLUS() \
+		? _cvmx##_CNT_LO(0) : _cvmx##_CNT(0)), \
+	/* _hi */ (OCTEON_IS_OCTEON1PLUS() \
+		? _cvmx##_CNT_HI(0) : 0), \
+	/* _stride */ (OCTEON_IS_OCTEON1PLUS() \
+		? (_cvmx##_CNT_LO(1) - _cvmx##_CNT_LO(0)) \
+		: (_cvmx##_CNT(1) - _cvmx##_CNT(0))), \
+	/* _lim */ lim_cvmx_lmcx)
+#define LMC_EVENTS \
+	LMC_EV(dclk, CVMX_LMCX_DCLK) \
+	LMC_EV(op, CVMX_LMCX_OPS) /*RENAME:op*/ \
+	LMC_EV(ifb, CVMX_LMCX_IFB) \
+	/*end*/
+
+/* Octeon1/1-plus L2-cache counters, mapped into 4 variable-func counters */
+#undef O1P_EV
+#define O1P_EV(_name, _cvmx) EV(o1p, _name, _cvmx, \
+	UNC_MAPPED|UNC_O1P, 0, 0, 0, NULL)
+#define O1P_EVENTS \
+	O1P_EV(cyc, CVMX_L2C_EVENT_CYCLES) /*RENAME:cycles*/ \
+	O1P_EV(imiss, CVMX_L2C_EVENT_INSTRUCTION_MISS) \
+	O1P_EV(ihit, CVMX_L2C_EVENT_INSTRUCTION_HIT) \
+	O1P_EV(dmiss, CVMX_L2C_EVENT_DATA_MISS)\
+	O1P_EV(dhit, CVMX_L2C_EVENT_DATA_HIT) \
+	O1P_EV(mis, CVMX_L2C_EVENT_MISS) /*RENAME:miss*/ \
+	O1P_EV(hit, CVMX_L2C_EVENT_HIT) \
+	O1P_EV(victim_buffer_hit, CVMX_L2C_EVENT_VICTIM_HIT) \
+	O1P_EV(lfb_nq_index_conflict, CVMX_L2C_EVENT_INDEX_CONFLICT) \
+	O1P_EV(tag_probe, CVMX_L2C_EVENT_TAG_PROBE) \
+	O1P_EV(tag_update, CVMX_L2C_EVENT_TAG_UPDATE) \
+	O1P_EV(tag_probe_completed, CVMX_L2C_EVENT_TAG_COMPLETE) \
+	O1P_EV(tag_dirty_victim, CVMX_L2C_EVENT_TAG_DIRTY) \
+	O1P_EV(data_store_nop, CVMX_L2C_EVENT_DATA_STORE_NOP) \
+	O1P_EV(data_store_read, CVMX_L2C_EVENT_DATA_STORE_READ) \
+	O1P_EV(data_store_write, CVMX_L2C_EVENT_DATA_STORE_WRITE) \
+	O1P_EV(memory_fill_data_valid, CVMX_L2C_EVENT_FILL_DATA_VALID) \
+	O1P_EV(memory_write_request, CVMX_L2C_EVENT_WRITE_REQUEST) \
+	O1P_EV(memory_read_request, CVMX_L2C_EVENT_READ_REQUEST) \
+	O1P_EV(memory_write_data_valid, CVMX_L2C_EVENT_WRITE_DATA_VALID) \
+	O1P_EV(xmc_nop, CVMX_L2C_EVENT_XMC_NOP) \
+	O1P_EV(xmc_ldt, CVMX_L2C_EVENT_XMC_LDT) \
+	O1P_EV(xmc_ldi, CVMX_L2C_EVENT_XMC_LDI) \
+	O1P_EV(xmc_ldd, CVMX_L2C_EVENT_XMC_LDD) \
+	O1P_EV(xmc_stf, CVMX_L2C_EVENT_XMC_STF) \
+	O1P_EV(xmc_stt, CVMX_L2C_EVENT_XMC_STT) \
+	O1P_EV(xmc_stp, CVMX_L2C_EVENT_XMC_STP) \
+	O1P_EV(xmc_stc, CVMX_L2C_EVENT_XMC_STC) \
+	O1P_EV(xmc_dwb, CVMX_L2C_EVENT_XMC_DWB) \
+	O1P_EV(xmc_pl2, CVMX_L2C_EVENT_XMC_PL2) \
+	O1P_EV(xmc_psl1, CVMX_L2C_EVENT_XMC_PSL1) \
+	O1P_EV(xmc_iobld, CVMX_L2C_EVENT_XMC_IOBLD) \
+	O1P_EV(xmc_iobst, CVMX_L2C_EVENT_XMC_IOBST) \
+	O1P_EV(xmc_iobdma, CVMX_L2C_EVENT_XMC_IOBDMA) \
+	O1P_EV(xmc_iobrsp, CVMX_L2C_EVENT_XMC_IOBRSP) \
+	O1P_EV(xmd_bus_valid, CVMX_L2C_EVENT_XMC_BUS_VALID) \
+	O1P_EV(xmd_bus_valid_dst_l2c, CVMX_L2C_EVENT_XMC_MEM_DATA) \
+	O1P_EV(xmd_bus_valid_dst_iob, CVMX_L2C_EVENT_XMC_REFL_DATA) \
+	O1P_EV(xmd_bus_valid_dst_pp, CVMX_L2C_EVENT_XMC_IOBRSP_DATA) \
+	O1P_EV(rsc_nop, CVMX_L2C_EVENT_RSC_NOP) \
+	O1P_EV(rsc_stdn, CVMX_L2C_EVENT_RSC_STDN) \
+	O1P_EV(rsc_fill, CVMX_L2C_EVENT_RSC_FILL) \
+	O1P_EV(rsc_refl, CVMX_L2C_EVENT_RSC_REFL) \
+	O1P_EV(rsc_stin, CVMX_L2C_EVENT_RSC_STIN) \
+	O1P_EV(rsc_scin, CVMX_L2C_EVENT_RSC_SCIN) \
+	O1P_EV(rsc_scfl, CVMX_L2C_EVENT_RSC_SCFL) \
+	O1P_EV(rsc_scdn, CVMX_L2C_EVENT_RSC_SCDN) \
+	O1P_EV(rsd_data_valid, CVMX_L2C_EVENT_RSC_DATA_VALID) \
+	O1P_EV(rsd_data_valid_fill, CVMX_L2C_EVENT_RSC_VALID_FILL) \
+	O1P_EV(rsd_data_valid_strsp, CVMX_L2C_EVENT_RSC_VALID_STRSP) \
+	O1P_EV(rsd_data_valid_refl, CVMX_L2C_EVENT_RSC_VALID_REFL) \
+	O1P_EV(lrf_req, CVMX_L2C_EVENT_LRF_REQ) \
+	O1P_EV(dt_rd_alloc, CVMX_L2C_EVENT_DT_RD_ALLOC) \
+	O1P_EV(dt_wr_inva, CVMX_L2C_EVENT_DT_WR_INVAL) \
+	/*end*/
+
+/*
+ * o23: L2C fixed-function counters for Octeon2/3 L2-cache
+ * - Values summed over N banks as 'offset' increments
+ * - no _HI/_LO needed, but passed as params for symmetric macros
+ */
+#undef O23_EV
+#define O23_EV(_name, _cvmx, _f, _lim) EV(o23, _name, EVID(o23, _name), \
+	UNC_DIRECT|(_f), \
+	/* _lo */ _cvmx##X_PFC(0), \
+	/* _hi */ 0, \
+	/* _stride */ (_cvmx##X_PFC(1) - _cvmx##X_PFC(0)), \
+	_lim)
+#define O23_EVENTS \
+	O23_EV(add, CVMX_L2C_XMC, UNC_O23, lim_o23_xmcd_rsdc) \
+	O23_EV(stor, CVMX_L2C_XMD, UNC_O23, lim_o23_xmcd_rsdc) /*RENAME:store*/\
+	O23_EV(commit, CVMX_L2C_RSC, UNC_O23, lim_o23_xmcd_rsdc) \
+	O23_EV(fill, CVMX_L2C_RSD, UNC_O23, lim_o23_xmcd_rsdc) \
+	O23_EV(inval, CVMX_L2C_INV, UNC_O3, lim_o3_l2c_inv) \
+	O23_EV(ioc, CVMX_L2C_IOC, UNC_O23, lim_one) \
+	O23_EV(ior, CVMX_L2C_IOR, UNC_O23, lim_one) \
+	/*end*/
+
+/*
+ * Octeon2/3 L2-cache TAG-and-DATA counters:
+ * - mapped into 4 variable-function counters spanning the 4-or-8 "quadrants"
+ * - the per-quad counters should be summed over the 4-or-8 "quadrants"
+ *   and for meaningful results should be sampled as perf -e group
+ *   {u_tad/q0index/,u_tad/q0read,...},{u_tad/q1index/,u_tad/q1read/,...},..
+ *   so event-multiplexing pulls in a coherent sample-set
+ */
+#undef TAD_EV
+#define TAD_EV(_name, _f, _cvmx) \
+	EV(tad, _name, _cvmx, UNC_MAPPED|(_f), 0, 0, 0, NULL)
+#define TAD_EVENTS \
+	/* omitted: TAD_EV(none, UNC_O23, CVMX_L2C_TAD_EVENT_NONE)*/ \
+	TAD_EV(hit, UNC_O23, CVMX_L2C_TAD_EVENT_TAG_HIT) \
+	TAD_EV(mis, UNC_O23, CVMX_L2C_TAD_EVENT_TAG_MISS) /*RENAME:miss*/ \
+	TAD_EV(no_alloc, UNC_O23, CVMX_L2C_TAD_EVENT_TAG_NOALLOC) \
+	TAD_EV(victim, UNC_O23, CVMX_L2C_TAD_EVENT_TAG_VICTIM) \
+	TAD_EV(sc_fail, UNC_O23, CVMX_L2C_TAD_EVENT_SC_FAIL) \
+	TAD_EV(sc_pass, UNC_O23, CVMX_L2C_TAD_EVENT_SC_PASS) \
+	TAD_EV(lfb_valid, UNC_O23, CVMX_L2C_TAD_EVENT_LFB_VALID) \
+	TAD_EV(lfb_wait_lfb, UNC_O23, CVMX_L2C_TAD_EVENT_LFB_WAIT_LFB) \
+	TAD_EV(lfb_wait_vab, UNC_O23, CVMX_L2C_TAD_EVENT_LFB_WAIT_VAB) \
+	TAD_EV(quad0_index, UNC_O23, CVMX_L2C_TAD_EVENT_QUAD0_INDEX) \
+	TAD_EV(quad0_read, UNC_O23, CVMX_L2C_TAD_EVENT_QUAD0_READ) \
+	TAD_EV(quad0_bank, UNC_O23, CVMX_L2C_TAD_EVENT_QUAD0_BANK) \
+	TAD_EV(quad0_wdat, UNC_O23, CVMX_L2C_TAD_EVENT_QUAD0_WDAT) \
+	TAD_EV(quad1_index, UNC_O23, CVMX_L2C_TAD_EVENT_QUAD1_INDEX) \
+	TAD_EV(quad1_read, UNC_O23, CVMX_L2C_TAD_EVENT_QUAD1_READ) \
+	TAD_EV(quad1_bank, UNC_O23, CVMX_L2C_TAD_EVENT_QUAD1_BANK) \
+	TAD_EV(quad1_wdat, UNC_O23, CVMX_L2C_TAD_EVENT_QUAD1_WDAT) \
+	TAD_EV(quad2_index, UNC_O23, CVMX_L2C_TAD_EVENT_QUAD2_INDEX) \
+	TAD_EV(quad2_read, UNC_O23, CVMX_L2C_TAD_EVENT_QUAD2_READ) \
+	TAD_EV(quad2_bank, UNC_O23, CVMX_L2C_TAD_EVENT_QUAD2_BANK) \
+	TAD_EV(quad2_wdat, UNC_O23, CVMX_L2C_TAD_EVENT_QUAD2_WDAT) \
+	TAD_EV(quad3_index, UNC_O23, CVMX_L2C_TAD_EVENT_QUAD3_INDEX) \
+	TAD_EV(quad3_read, UNC_O23, CVMX_L2C_TAD_EVENT_QUAD3_READ) \
+	TAD_EV(quad3_bank, UNC_O23, CVMX_L2C_TAD_EVENT_QUAD3_BANK) \
+	TAD_EV(quad3_wdat, UNC_O23, CVMX_L2C_TAD_EVENT_QUAD3_WDAT) \
+	TAD_EV(quad4_index, UNC_O3, CVMX_L2C_TAD_EVENT_QUAD4_INDEX) \
+	TAD_EV(quad4_read, UNC_O3, CVMX_L2C_TAD_EVENT_QUAD4_READ) \
+	TAD_EV(quad4_bank, UNC_O3, CVMX_L2C_TAD_EVENT_QUAD4_BANK) \
+	TAD_EV(quad4_wdat, UNC_O3, CVMX_L2C_TAD_EVENT_QUAD4_WDAT) \
+	TAD_EV(quad5_index, UNC_O3, CVMX_L2C_TAD_EVENT_QUAD5_INDEX) \
+	TAD_EV(quad5_read, UNC_O3, CVMX_L2C_TAD_EVENT_QUAD5_READ) \
+	TAD_EV(quad5_bank, UNC_O3, CVMX_L2C_TAD_EVENT_QUAD5_BANK) \
+	TAD_EV(quad5_wdat, UNC_O3, CVMX_L2C_TAD_EVENT_QUAD5_WDAT) \
+	TAD_EV(quad6_index, UNC_O3, CVMX_L2C_TAD_EVENT_QUAD6_INDEX) \
+	TAD_EV(quad6_read, UNC_O3, CVMX_L2C_TAD_EVENT_QUAD6_READ) \
+	TAD_EV(quad6_bank, UNC_O3, CVMX_L2C_TAD_EVENT_QUAD6_BANK) \
+	TAD_EV(quad6_wdat, UNC_O3, CVMX_L2C_TAD_EVENT_QUAD6_WDAT) \
+	TAD_EV(quad7_index, UNC_O3, CVMX_L2C_TAD_EVENT_QUAD7_INDEX) \
+	TAD_EV(quad7_read, UNC_O3, CVMX_L2C_TAD_EVENT_QUAD7_READ) \
+	TAD_EV(quad7_bank, UNC_O3, CVMX_L2C_TAD_EVENT_QUAD7_BANK) \
+	TAD_EV(quad7_wdat, UNC_O3, CVMX_L2C_TAD_EVENT_QUAD7_WDAT) \
+	/*end*/
+
+#define OCT_EVENTS	\
+	 LMC_EVENTS	\
+	 O1P_EVENTS	\
+	 O23_EVENTS	\
+	 TAD_EVENTS	\
+	/*end*/
-- 
1.8.2.1

