From c876ffd584ca398f508700e85059b8bd00faf6b8 Mon Sep 17 00:00:00 2001
From: David Daney <david.daney@cavium.com>
Date: Fri, 13 Feb 2015 12:57:20 +0530
Subject: [PATCH 025/132] netdev: octeon: Move and update OCTEON network
 drivers from staging.

Commit 7a1eecada888113bd9b4e325f227630e63e1b5f4 from
git://git.yoctoproject.org/linux-yocto-3.14

netdev: octeon-ethernet: Enable SKB recycling of transmitted packets.

New module parameter "octeon_recycle_tx" turns it on and off.

net: Export skb_release_head_state.

This function will be needed by octeon-ethernet driver

Signed-off-by: David Daney <david.daney@cavium.com>
Signed-off-by: Abhishek Paliwal <abhishek.paliwal@aricent.com>
[Set buf to NULL in octeon3_eth_ndo_start_xmit() to fix
 a compile warning]
Signed-off-by: Bin Jiang <bin.jiang@windriver.com>
[Original patch taken from OCTEON-SDK 3.1.1-544.
 Remove zombie codes in octeon3_eth_remove().]
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 drivers/net/ethernet/octeon/Makefile              |   16 +
 drivers/net/ethernet/octeon/ethernet-defines.h    |  109 ++
 drivers/net/ethernet/octeon/ethernet-mdio.c       |  514 ++++++
 drivers/net/ethernet/octeon/ethernet-mem.c        |  326 ++++
 drivers/net/ethernet/octeon/ethernet-napi.c       |  472 +++++
 drivers/net/ethernet/octeon/ethernet-rgmii.c      |  298 +++
 drivers/net/ethernet/octeon/ethernet-rx.c         |  601 ++++++
 drivers/net/ethernet/octeon/ethernet-sgmii.c      |  231 +++
 drivers/net/ethernet/octeon/ethernet-spi.c        |  239 +++
 drivers/net/ethernet/octeon/ethernet-tx.c         |  229 +++
 drivers/net/ethernet/octeon/ethernet-xmit.c       |  393 ++++
 drivers/net/ethernet/octeon/ethernet.c            | 1178 ++++++++++++
 drivers/net/ethernet/octeon/octeon-ethernet.h     |  228 +++
 drivers/net/ethernet/octeon/octeon-pow-ethernet.c |  816 ++++++++
 drivers/net/ethernet/octeon/octeon3-ethernet.c    | 2041 +++++++++++++++++++++
 drivers/net/ethernet/octeon/octeon_common.h       |   40 +
 drivers/net/ethernet/octeon/octeon_mgmt.c         |    8 +-
 drivers/staging/Kconfig                           |    2 -
 drivers/staging/Makefile                          |    1 -
 drivers/staging/octeon/Kconfig                    |   13 -
 drivers/staging/octeon/Makefile                   |   23 -
 drivers/staging/octeon/ethernet-mem.h             |   29 -
 drivers/staging/octeon/ethernet-rx.h              |   52 -
 drivers/staging/octeon/ethernet-spi.c             |  292 ---
 drivers/staging/octeon/ethernet-tx.h              |   34 -
 include/linux/skbuff.h                            |    1 +
 net/core/skbuff.c                                 |    3 +-
 27 files changed, 7738 insertions(+), 451 deletions(-)
 create mode 100644 drivers/net/ethernet/octeon/ethernet-defines.h
 create mode 100644 drivers/net/ethernet/octeon/ethernet-mdio.c
 create mode 100644 drivers/net/ethernet/octeon/ethernet-mem.c
 create mode 100644 drivers/net/ethernet/octeon/ethernet-napi.c
 create mode 100644 drivers/net/ethernet/octeon/ethernet-rgmii.c
 create mode 100644 drivers/net/ethernet/octeon/ethernet-rx.c
 create mode 100644 drivers/net/ethernet/octeon/ethernet-sgmii.c
 create mode 100644 drivers/net/ethernet/octeon/ethernet-spi.c
 create mode 100644 drivers/net/ethernet/octeon/ethernet-tx.c
 create mode 100644 drivers/net/ethernet/octeon/ethernet-xmit.c
 create mode 100644 drivers/net/ethernet/octeon/ethernet.c
 create mode 100644 drivers/net/ethernet/octeon/octeon-ethernet.h
 create mode 100644 drivers/net/ethernet/octeon/octeon-pow-ethernet.c
 create mode 100644 drivers/net/ethernet/octeon/octeon3-ethernet.c
 create mode 100644 drivers/net/ethernet/octeon/octeon_common.h
 delete mode 100644 drivers/staging/octeon/Kconfig
 delete mode 100644 drivers/staging/octeon/Makefile
 delete mode 100644 drivers/staging/octeon/ethernet-mem.h
 delete mode 100644 drivers/staging/octeon/ethernet-rx.h
 delete mode 100644 drivers/staging/octeon/ethernet-spi.c
 delete mode 100644 drivers/staging/octeon/ethernet-tx.h

diff --git a/drivers/net/ethernet/octeon/Makefile b/drivers/net/ethernet/octeon/Makefile
index efa41c1..df1bb419 100644
--- a/drivers/net/ethernet/octeon/Makefile
+++ b/drivers/net/ethernet/octeon/Makefile
@@ -2,4 +2,20 @@
 # Makefile for the Cavium network device drivers.
 #
 
+obj-$(CONFIG_OCTEON3_ETHERNET)          += octeon3-ethernet.o
 obj-$(CONFIG_OCTEON_MGMT_ETHERNET)	+= octeon_mgmt.o
+obj-$(CONFIG_OCTEON_POW_ONLY_ETHERNET)  += octeon-pow-ethernet.o
+obj-$(CONFIG_OCTEON_ETHERNET) +=  octeon-ethernet.o
+obj-$(CONFIG_OCTEON_ETHERNET_MEM) += ethernet-mem.o
+obj-$(CONFIG_OCTEON_ETHERNET_MEM) += ethernet-mem.o
+obj-$(CONFIG_OCTEON_ETHERNET_COMMON) += octeon_common.o
+
+octeon-ethernet-objs := ethernet.o
+octeon-ethernet-objs += ethernet-mdio.o
+octeon-ethernet-objs += ethernet-rgmii.o
+octeon-ethernet-objs += ethernet-rx.o
+octeon-ethernet-objs += ethernet-sgmii.o
+octeon-ethernet-objs += ethernet-spi.o
+octeon-ethernet-objs += ethernet-tx.o
+
+
diff --git a/drivers/net/ethernet/octeon/ethernet-defines.h b/drivers/net/ethernet/octeon/ethernet-defines.h
new file mode 100644
index 0000000..45c55a6
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-defines.h
@@ -0,0 +1,109 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+
+/*
+ * A few defines are used to control the operation of this driver:
+ *  CONFIG_CAVIUM_RESERVE32
+ *      This kernel config options controls the amount of memory configured
+ *      in a wired TLB entry for all processes to share. If this is set, the
+ *      driver will use this memory instead of kernel memory for pools. This
+ *      allows 32bit userspace application to access the buffers, but also
+ *      requires all received packets to be copied.
+ *  USE_SKBUFFS_IN_HW
+ *      Tells the driver to populate the packet buffers with kernel skbuffs.
+ *      This allows the driver to receive packets without copying them. It also
+ *      means that 32bit userspace can't access the packet buffers.
+ *  USE_HW_TCPUDP_CHECKSUM
+ *      Controls if the Octeon TCP/UDP checksum engine is used for packet
+ *      output. If this is zero, the kernel will perform the checksum in
+ *      software.
+ *  USE_ASYNC_IOBDMA
+ *      Use asynchronous IO access to hardware. This uses Octeon's asynchronous
+ *      IOBDMAs to issue IO accesses without stalling. Set this to zero
+ *      to disable this. Note that IOBDMAs require CVMSEG.
+ *  REUSE_SKBUFFS_WITHOUT_FREE
+ *      Allows the TX path to free an skbuff into the FPA hardware pool. This
+ *      can significantly improve performance for forwarding and bridging, but
+ *      may be somewhat dangerous. Checks are made, but if any buffer is reused
+ *      without the proper Linux cleanup, the networking stack may have very
+ *      bizarre bugs.
+ */
+#ifndef __ETHERNET_DEFINES_H__
+#define __ETHERNET_DEFINES_H__
+
+#define OCTEON_ETHERNET_VERSION "2.0"
+
+/* FAU */
+#define FAU_REG_END (2048)
+
+/* FPA defines */
+#define FPA_WQE_POOL_SIZE (1 * CVMX_CACHE_LINE_SIZE)
+#define FPA_PACKET_POOL_SIZE (16 * CVMX_CACHE_LINE_SIZE)
+#define FPA_OUTPUT_BUFFER_POOL_SIZE (8 * CVMX_CACHE_LINE_SIZE)
+
+/* TODO: replace this */
+#define CVMX_SCR_SCRATCH (0)
+
+#ifndef CONFIG_CAVIUM_RESERVE32
+#define CONFIG_CAVIUM_RESERVE32 0
+#endif
+
+#define USE_SKBUFFS_IN_HW           1
+#ifdef CONFIG_NETFILTER
+#define REUSE_SKBUFFS_WITHOUT_FREE  0
+#else
+#define REUSE_SKBUFFS_WITHOUT_FREE  1
+#endif
+
+#define USE_HW_TCPUDP_CHECKSUM      1
+
+/* Enable Random Early Dropping under load */
+#define USE_RED                     1
+#define USE_ASYNC_IOBDMA            1
+
+/*
+ * Allow SW based preamble removal at 10Mbps to workaround PHYs giving
+ * us bad preambles.
+ */
+#define USE_10MBPS_PREAMBLE_WORKAROUND 1
+/*
+ * Use this to have all FPA frees also tell the L2 not to write data
+ * to memory.
+ */
+#define DONT_WRITEBACK(x)           (x)
+/* Use this to not have FPA frees control L2 */
+/*#define DONT_WRITEBACK(x)         0   */
+
+/* Maximum number of SKBs to try to free per xmit packet. */
+#define MAX_OUT_QUEUE_DEPTH 1000
+
+#define FAU_NUM_PACKET_BUFFERS_TO_FREE (FAU_REG_END - sizeof(u32))
+
+#define TOTAL_NUMBER_OF_PORTS       (CVMX_PIP_NUM_INPUT_PORTS+1)
+
+
+#endif /* __ETHERNET_DEFINES_H__ */
diff --git a/drivers/net/ethernet/octeon/ethernet-mdio.c b/drivers/net/ethernet/octeon/ethernet-mdio.c
new file mode 100644
index 0000000..9c48519
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-mdio.c
@@ -0,0 +1,514 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+#include <linux/kernel.h>
+#include <linux/ethtool.h>
+#include <linux/phy.h>
+#include <linux/ratelimit.h>
+#include <linux/of_mdio.h>
+#include <linux/net_tstamp.h>
+
+#include <net/dst.h>
+
+#include <asm/octeon/octeon.h>
+#include <asm/octeon/cvmx-srio.h>
+#include <asm/octeon/octeon-ethernet-user.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+
+#include <asm/octeon/cvmx-helper-board.h>
+
+#include <asm/octeon/cvmx-smix-defs.h>
+#include <asm/octeon/cvmx-gmxx-defs.h>
+#include <asm/octeon/cvmx-pip-defs.h>
+#include <asm/octeon/cvmx-pko-defs.h>
+#include <asm/octeon/cvmx-pcsx-defs.h>
+
+static void cvm_oct_get_drvinfo(struct net_device *dev,
+				struct ethtool_drvinfo *info)
+{
+	strcpy(info->driver, "octeon-ethernet");
+	strcpy(info->version, OCTEON_ETHERNET_VERSION);
+	strcpy(info->bus_info, "Builtin");
+}
+
+static int cvm_oct_get_settings(struct net_device *dev, struct ethtool_cmd *cmd)
+{
+	cvmx_helper_link_info_t link_info;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	if (priv->phydev)
+		return phy_ethtool_gset(priv->phydev, cmd);
+
+	link_info = cvmx_helper_link_get(priv->ipd_port);
+	cmd->speed = link_info.s.link_up ? link_info.s.speed : 0;
+	cmd->duplex = link_info.s.full_duplex ? DUPLEX_FULL : DUPLEX_HALF;
+
+	return 0;
+}
+
+static int cvm_oct_set_settings(struct net_device *dev, struct ethtool_cmd *cmd)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	if (priv->phydev)
+		return phy_ethtool_sset(priv->phydev, cmd);
+
+	return -EOPNOTSUPP;
+}
+
+static int cvm_oct_nway_reset(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	if (priv->phydev)
+		return phy_start_aneg(priv->phydev);
+
+	return -EOPNOTSUPP;
+}
+
+/* for qca833x & at803x, unhang phy by PCS down/up */
+/* PHY-specific code should save/restore state, this just zaps it */
+static inline void set_port_pcs(struct net_device *dev, bool up)
+{
+      struct octeon_ethernet *priv = netdev_priv(dev);
+      int interface = cvmx_helper_get_interface_num(priv->ipd_port);
+      int index = cvmx_helper_get_interface_index_num(priv->ipd_port);
+      union cvmx_pcsx_mrx_control_reg control_reg;
+      union cvmx_gmxx_prtx_cfg gmxx_prtx_cfg;
+      union cvmx_pcsx_miscx_ctl_reg pcsx_miscx_ctl_reg;
+
+      control_reg.u64 = cvmx_read_csr(CVMX_PCSX_MRX_CONTROL_REG(index, interface));
+      control_reg.s.an_en = 1;
+      control_reg.s.pwr_dn = 0;
+
+      if (up) {
+	  control_reg.s.pwr_dn = 0;
+	  control_reg.s.spdmsb = 1;
+	  control_reg.s.spdlsb = 0;
+	  control_reg.s.dup = 1;
+
+	  gmxx_prtx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+	  gmxx_prtx_cfg.s.speed = 1;
+	  gmxx_prtx_cfg.s.speed_msb = 0;
+	  gmxx_prtx_cfg.s.slottime = 1;
+	  cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmxx_prtx_cfg.u64);
+
+	  pcsx_miscx_ctl_reg.u64 = cvmx_read_csr(CVMX_PCSX_MISCX_CTL_REG(index, interface));
+	  pcsx_miscx_ctl_reg.s.samp_pt = 1;
+	  cvmx_write_csr(CVMX_PCSX_MISCX_CTL_REG(index, interface),pcsx_miscx_ctl_reg.u64);
+
+      } else {
+	  control_reg.s.pwr_dn = 1;
+	  control_reg.s.spdmsb = 1;
+	  control_reg.s.spdlsb = 0;
+	  control_reg.s.dup = 1;
+      }
+      cvmx_write_csr(CVMX_PCSX_MRX_CONTROL_REG(index, interface), control_reg.u64);
+}
+
+static int cvm_eth_reset(struct net_device *dev, u32 *maskp)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	if (!priv->phydev)
+		return -EOPNOTSUPP;
+
+	if (!maskp)
+		return 0;
+
+	/* for qca833x & friends, where PHY must be reset by PCS cycling */
+	if (*maskp & ETH_RESET_PHY) {
+		struct octeon_ethernet *priv = netdev_priv(dev);
+		cvmx_helper_link_info_t link_info = (cvmx_helper_link_info_t) priv->link_info;
+
+		if (priv->last_link)
+		{
+			cvmx_helper_link_info_t down_info = link_info;
+
+			down_info.s.link_up = 0;
+			cvmx_helper_link_set(priv->ipd_port, down_info);
+			set_port_pcs(dev, 0);
+			msleep(30);
+			set_port_pcs(dev, 1);
+			link_info.s.link_up = 1;
+			cvmx_helper_link_set(priv->ipd_port, link_info);
+		}
+		*maskp &= ~ETH_RESET_PHY;
+	}
+
+	return 0;
+}
+
+const struct ethtool_ops cvm_oct_ethtool_ops = {
+	.get_drvinfo = cvm_oct_get_drvinfo,
+	.get_settings = cvm_oct_get_settings,
+	.set_settings = cvm_oct_set_settings,
+	.nway_reset = cvm_oct_nway_reset,
+	.get_link = ethtool_op_get_link,
+	.reset = cvm_eth_reset,
+};
+
+/**
+ * cvm_oct_ioctl_hwtstamp - IOCTL support for timestamping
+ * @dev:    Device to change
+ * @rq:     the request
+ * @cmd:    the command
+ *
+ * Returns Zero on success
+ */
+static int cvm_oct_ioctl_hwtstamp(struct net_device *dev,
+				  struct ifreq *rq, int cmd)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	struct hwtstamp_config config;
+	union cvmx_mio_ptp_clock_cfg ptp;
+	union cvmx_gmxx_rxx_frm_ctl frm_ctl;
+	union cvmx_pip_prt_cfgx prt_cfg;
+
+	if (copy_from_user(&config, rq->ifr_data, sizeof(config)))
+		return -EFAULT;
+
+	if (config.flags) /* reserved for future extensions */
+		return -EINVAL;
+
+	/* Check the status of hardware for tiemstamps */
+	if (OCTEON_IS_MODEL(OCTEON_CN6XXX) || OCTEON_IS_MODEL(OCTEON_CNF7XXX)) {
+		/* Write TX timestamp into word 4 */
+		cvmx_write_csr(CVMX_PKO_REG_TIMESTAMP, 4);
+
+		switch (priv->imode) {
+		case CVMX_HELPER_INTERFACE_MODE_XAUI:
+		case CVMX_HELPER_INTERFACE_MODE_RXAUI:
+		case CVMX_HELPER_INTERFACE_MODE_SGMII:
+			break;
+		default:
+			/* No timestamp support*/
+			return -EOPNOTSUPP;
+		}
+
+		ptp.u64 = octeon_read_ptp_csr(CVMX_MIO_PTP_CLOCK_CFG);
+		if (!ptp.s.ptp_en) {
+			/* It should have been enabled by csrc-octeon-ptp */
+			netdev_err(dev, "Error: PTP clock not enabled\n");
+			/* No timestamp support*/
+			return -EOPNOTSUPP;
+		}
+	} else {
+			/* No timestamp support*/
+			return -EOPNOTSUPP;
+	}
+
+	switch (config.tx_type) {
+	case HWTSTAMP_TX_OFF:
+		priv->tx_timestamp_hw = 0;
+		break;
+	case HWTSTAMP_TX_ON:
+		priv->tx_timestamp_hw = 1;
+		break;
+	default:
+		return -ERANGE;
+	}
+
+	switch (config.rx_filter) {
+	case HWTSTAMP_FILTER_NONE:
+		priv->rx_timestamp_hw = 0;
+
+		frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface));
+		frm_ctl.s.ptp_mode = 0;
+		cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface), frm_ctl.u64);
+
+		prt_cfg.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(priv->ipd_pkind));
+		prt_cfg.s.skip = 0;
+		cvmx_write_csr(CVMX_PIP_PRT_CFGX(priv->ipd_pkind), prt_cfg.u64);
+		break;
+	case HWTSTAMP_FILTER_ALL:
+	case HWTSTAMP_FILTER_SOME:
+	case HWTSTAMP_FILTER_PTP_V1_L4_EVENT:
+	case HWTSTAMP_FILTER_PTP_V1_L4_SYNC:
+	case HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ:
+	case HWTSTAMP_FILTER_PTP_V2_L4_EVENT:
+	case HWTSTAMP_FILTER_PTP_V2_L4_SYNC:
+	case HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ:
+	case HWTSTAMP_FILTER_PTP_V2_L2_EVENT:
+	case HWTSTAMP_FILTER_PTP_V2_L2_SYNC:
+	case HWTSTAMP_FILTER_PTP_V2_L2_DELAY_REQ:
+	case HWTSTAMP_FILTER_PTP_V2_EVENT:
+	case HWTSTAMP_FILTER_PTP_V2_SYNC:
+	case HWTSTAMP_FILTER_PTP_V2_DELAY_REQ:
+		priv->rx_timestamp_hw = 1;
+		config.rx_filter = HWTSTAMP_FILTER_ALL;
+		frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface));
+		frm_ctl.s.ptp_mode = 1;
+		cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface), frm_ctl.u64);
+
+		prt_cfg.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(priv->ipd_pkind));
+		prt_cfg.s.skip = 8;
+		cvmx_write_csr(CVMX_PIP_PRT_CFGX(priv->ipd_pkind), prt_cfg.u64);
+		break;
+	default:
+		return -ERANGE;
+	}
+
+	if (copy_to_user(rq->ifr_data, &config, sizeof(config)))
+		return -EFAULT;
+
+	return 0;
+}
+
+/**
+ * cvm_oct_ioctl - IOCTL support for PHY control
+ * @dev:    Device to change
+ * @rq:     the request
+ * @cmd:    the command
+ *
+ * Returns Zero on success
+ */
+int cvm_oct_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	union cvmx_srio_tx_message_header tx_header;
+	int ivalue;
+
+	switch (cmd) {
+	case CAVIUM_NET_IOCTL_SETPRIO:
+		ivalue = rq->ifr_ifru.ifru_ivalue;
+		if ((ivalue >= 0) && (ivalue < 4)) {
+			tx_header.u64 = priv->srio_tx_header;
+			tx_header.s.prio = ivalue;
+			priv->srio_tx_header = tx_header.u64;
+			return 0;
+		}
+		return -EINVAL;
+
+	case CAVIUM_NET_IOCTL_GETPRIO:
+		tx_header.u64 = priv->srio_tx_header;
+		rq->ifr_ifru.ifru_ivalue = tx_header.s.prio;
+		return 0;
+
+	case CAVIUM_NET_IOCTL_SETIDSIZE:
+		ivalue = rq->ifr_ifru.ifru_ivalue;
+		if ((ivalue >= 0) && (ivalue < 2)) {
+			tx_header.u64 = priv->srio_tx_header;
+			tx_header.s.tt = ivalue;
+			priv->srio_tx_header = tx_header.u64;
+			return 0;
+		}
+		return -EINVAL;
+
+	case CAVIUM_NET_IOCTL_GETIDSIZE:
+		tx_header.u64 = priv->srio_tx_header;
+		rq->ifr_ifru.ifru_ivalue = tx_header.s.tt;
+		return 0;
+
+	case CAVIUM_NET_IOCTL_SETSRCID:
+		ivalue = rq->ifr_ifru.ifru_ivalue;
+		if ((ivalue >= 0) && (ivalue < 2)) {
+			tx_header.u64 = priv->srio_tx_header;
+			tx_header.s.sis = ivalue;
+			priv->srio_tx_header = tx_header.u64;
+			return 0;
+		}
+		return -EINVAL;
+
+	case CAVIUM_NET_IOCTL_GETSRCID:
+		tx_header.u64 = priv->srio_tx_header;
+		rq->ifr_ifru.ifru_ivalue = tx_header.s.sis;
+		return 0;
+
+	case CAVIUM_NET_IOCTL_SETLETTER:
+		ivalue = rq->ifr_ifru.ifru_ivalue;
+		if ((ivalue >= -1) && (ivalue < 4)) {
+			tx_header.u64 = priv->srio_tx_header;
+			tx_header.s.lns = (ivalue == -1);
+			if (tx_header.s.lns)
+				tx_header.s.letter = 0;
+			else
+				tx_header.s.letter = ivalue;
+			priv->srio_tx_header = tx_header.u64;
+			return 0;
+		}
+		return -EINVAL;
+
+	case CAVIUM_NET_IOCTL_GETLETTER:
+		tx_header.u64 = priv->srio_tx_header;
+		if (tx_header.s.lns)
+			rq->ifr_ifru.ifru_ivalue = -1;
+		else
+			rq->ifr_ifru.ifru_ivalue = tx_header.s.letter;
+		return 0;
+
+	case SIOCSHWTSTAMP:
+		return cvm_oct_ioctl_hwtstamp(dev, rq, cmd);
+
+	default:
+		if (priv->phydev)
+			return phy_mii_ioctl(priv->phydev, rq, cmd);
+	}
+	return -EOPNOTSUPP;
+}
+
+static void cvm_oct_note_carrier(struct octeon_ethernet *priv,
+				 cvmx_helper_link_info_t li)
+{
+	if (li.s.link_up) {
+		pr_notice_ratelimited("%s: %u Mbps %s duplex, port %d\n",
+				      netdev_name(priv->netdev), li.s.speed,
+				      (li.s.full_duplex) ? "Full" : "Half",
+				      priv->ipd_port);
+	} else {
+		pr_notice_ratelimited("%s: Link down\n",
+				      netdev_name(priv->netdev));
+	}
+}
+
+/**
+ * cvm_oct_set_carrier - common wrapper of netif_carrier_{on,off}
+ *
+ * @priv: Device struct.
+ * @link_info: Current state.
+ */
+void cvm_oct_set_carrier(struct octeon_ethernet *priv,
+			 cvmx_helper_link_info_t link_info)
+{
+	cvm_oct_note_carrier(priv, link_info);
+	if (link_info.s.link_up) {
+		if (!netif_carrier_ok(priv->netdev))
+			netif_carrier_on(priv->netdev);
+	} else {
+		if (netif_carrier_ok(priv->netdev))
+			netif_carrier_off(priv->netdev);
+	}
+}
+
+void cvm_oct_adjust_link(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	cvmx_helper_link_info_t link_info;
+
+	if (priv->last_link != priv->phydev->link) {
+		priv->last_link = priv->phydev->link;
+		link_info.u64 = 0;
+		link_info.s.link_up = priv->last_link ? 1 : 0;
+		link_info.s.full_duplex = priv->phydev->duplex ? 1 : 0;
+		link_info.s.speed = priv->phydev->speed;
+
+		cvmx_helper_link_set(priv->ipd_port, link_info);
+
+		if (priv->link_change)
+			priv->link_change(priv, link_info);
+
+		cvm_oct_note_carrier(priv, link_info);
+	}
+}
+
+int cvm_oct_common_stop(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	cvmx_helper_link_info_t link_info;
+
+	priv->poll = NULL;
+
+	if (priv->phydev)
+		phy_disconnect(priv->phydev);
+	priv->phydev = NULL;
+
+	if (priv->last_link) {
+		link_info.u64 = 0;
+		priv->last_link = 0;
+
+		cvmx_helper_link_set(priv->ipd_port, link_info);
+
+		if (priv->link_change)
+			priv->link_change(priv, link_info);
+
+		cvm_oct_note_carrier(priv, link_info);
+	}
+	return 0;
+}
+
+/**
+ * cvm_oct_phy_setup_device - setup the PHY
+ *
+ * @dev:    Device to setup
+ *
+ * Returns Zero on success, negative on failure
+ */
+int cvm_oct_phy_setup_device(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	struct device_node *phy_node;
+	phy_interface_t iface;
+
+	if (!priv->of_node)
+		goto no_phy;
+
+	phy_node = of_parse_phandle(priv->of_node, "phy-handle", 0);
+	if (!phy_node)
+		goto no_phy;
+
+	switch (priv->imode) {
+	case CVMX_HELPER_INTERFACE_MODE_GMII:
+		iface = PHY_INTERFACE_MODE_GMII;
+		break;
+	case CVMX_HELPER_INTERFACE_MODE_RGMII:
+		iface = PHY_INTERFACE_MODE_RGMII;
+		break;
+	case CVMX_HELPER_INTERFACE_MODE_SGMII:
+		iface = PHY_INTERFACE_MODE_SGMII;
+		break;
+	default:
+		iface = PHY_INTERFACE_MODE_NA;
+		break;
+	}
+
+	priv->phydev = of_phy_connect(dev, phy_node, cvm_oct_adjust_link, 0,
+				      iface);
+
+	if (priv->phydev == NULL)
+		return -ENODEV;
+
+	priv->last_link = 0;
+	phy_start_aneg(priv->phydev);
+
+	return 0;
+no_phy:
+	/* If there is no phy, assume a direct MAC connection and that
+	 * the link is up.
+	 */
+	netif_carrier_on(dev);
+	return 0;
+}
diff --git a/drivers/net/ethernet/octeon/ethernet-mem.c b/drivers/net/ethernet/octeon/ethernet-mem.c
new file mode 100644
index 0000000..d069167
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-mem.c
@@ -0,0 +1,326 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+#include <linux/netdevice.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/slab.h>
+
+#include <asm/octeon/octeon.h>
+#include <asm/octeon/cvmx-fpa1.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+
+struct fpa_pool {
+	int pool;
+	int users;
+	int size;
+	char kmem_name[20];
+	struct kmem_cache *kmem;
+	int (*fill)(struct fpa_pool *p, int num);
+	int (*empty)(struct fpa_pool *p, int num);
+};
+
+static DEFINE_MUTEX(cvm_oct_pools_lock);
+static struct fpa_pool cvm_oct_pools[CVMX_FPA1_NUM_POOLS] = {
+	[0 ... CVMX_FPA1_NUM_POOLS - 1] = {-1,},
+};
+
+/**
+ * cvm_oct_fill_hw_skbuff - fill the supplied hardware pool with skbuffs
+ * @pool:     Pool to allocate an skbuff for
+ * @size:     Size of the buffer needed for the pool
+ * @elements: Number of buffers to allocate
+ *
+ * Returns the actual number of buffers allocated.
+ */
+static int cvm_oct_fill_hw_skbuff(struct fpa_pool *pool, int elements)
+{
+	int freed = elements;
+	int size = pool->size;
+	int pool_num = pool->pool;
+	while (freed) {
+		int extra_reserve;
+		u8 *desired_data;
+		struct sk_buff *skb = alloc_skb(size + CVM_OCT_SKB_TO_FPA_PADDING,
+						GFP_ATOMIC);
+		if (skb == NULL) {
+			pr_err("Failed to allocate skb for hardware pool %d\n",
+			       pool_num);
+			break;
+		}
+		desired_data = cvm_oct_get_fpa_head(skb);
+		extra_reserve = desired_data - skb->data;
+		skb_reserve(skb, extra_reserve);
+		*(struct sk_buff **)(skb->data - sizeof(void *)) = skb;
+		cvmx_fpa1_free(skb->data, pool_num, DONT_WRITEBACK(size / 128));
+		freed--;
+	}
+	return elements - freed;
+}
+
+/**
+ * cvm_oct_free_hw_skbuff- free hardware pool skbuffs
+ * @pool:     Pool to allocate an skbuff for
+ * @size:     Size of the buffer needed for the pool
+ * @elements: Number of buffers to allocate
+ */
+static int cvm_oct_free_hw_skbuff(struct fpa_pool *pool, int elements)
+{
+	struct sk_buff *skb;
+	char *memory;
+	int pool_num = pool->pool;
+
+	while (elements) {
+		memory = cvmx_fpa1_alloc(pool_num);
+		if (!memory)
+			break;
+		skb = *cvm_oct_packet_to_skb(memory);
+		elements--;
+		dev_kfree_skb(skb);
+	}
+
+	if (elements > 0)
+		pr_err("Freeing of pool %u is missing %d skbuffs\n",
+		       pool_num, elements);
+
+	return elements;
+}
+
+/**
+ * cvm_oct_fill_hw_memory - fill a hardware pool with memory.
+ * @pool:     Pool to populate
+ * @size:     Size of each buffer in the pool
+ * @elements: Number of buffers to allocate
+ *
+ * Returns the actual number of buffers allocated.
+ */
+static int cvm_oct_fill_hw_kmem(struct fpa_pool *pool, int elements)
+{
+	char *memory;
+	int freed = elements;
+
+	while (freed) {
+		memory = kmem_cache_alloc(pool->kmem, GFP_KERNEL);
+		if (unlikely(memory == NULL)) {
+			pr_err("Unable to allocate %u bytes for FPA pool %d\n",
+			       elements * pool->size, pool->pool);
+			break;
+		}
+		cvmx_fpa1_free(memory, pool->pool, 0);
+		freed--;
+	}
+	return elements - freed;
+}
+
+/**
+ * cvm_oct_free_hw_memory - Free memory allocated by cvm_oct_fill_hw_memory
+ * @pool:     FPA pool to free
+ * @size:     Size of each buffer in the pool
+ * @elements: Number of buffers that should be in the pool
+ */
+static int cvm_oct_free_hw_kmem(struct fpa_pool *pool, int elements)
+{
+	char *fpa;
+	while (elements) {
+		fpa = cvmx_fpa1_alloc(pool->pool);
+		if (!fpa)
+			break;
+		elements--;
+		kmem_cache_free(pool->kmem, fpa);
+	}
+
+	if (elements > 0)
+		pr_err("Warning: Freeing of pool %u is missing %d buffers\n",
+		       pool->pool, elements);
+	return elements;
+}
+
+int cvm_oct_mem_fill_fpa(int pool, int elements)
+{
+	struct fpa_pool *p;
+
+	if (pool < 0 || pool >= ARRAY_SIZE(cvm_oct_pools))
+		return -EINVAL;
+
+	p = cvm_oct_pools + pool;
+
+	return p->fill(p, elements);
+}
+EXPORT_SYMBOL(cvm_oct_mem_fill_fpa);
+
+int cvm_oct_mem_empty_fpa(int pool, int elements)
+{
+	struct fpa_pool *p;
+
+	if (pool < 0 || pool >= ARRAY_SIZE(cvm_oct_pools))
+		return -EINVAL;
+
+	p = cvm_oct_pools + pool;
+	if (p->empty)
+		return p->empty(p, elements);
+
+	return 0;
+}
+EXPORT_SYMBOL(cvm_oct_mem_empty_fpa);
+
+void cvm_oct_mem_cleanup(void)
+{
+	int i;
+
+	mutex_lock(&cvm_oct_pools_lock);
+
+	for (i = 0; i < ARRAY_SIZE(cvm_oct_pools); i++)
+		if (cvm_oct_pools[i].kmem)
+			kmem_cache_shrink(cvm_oct_pools[i].kmem);
+	mutex_unlock(&cvm_oct_pools_lock);
+}
+EXPORT_SYMBOL(cvm_oct_mem_cleanup);
+
+/**
+ * cvm_oct_alloc_fpa_pool() - Allocate an FPA pool of the given size
+ * @pool:  Requested pool number (-1 for don't care).
+ * @size:  The size of the pool elements.
+ *
+ * Returns the pool number or a negative number on error.
+ */
+int cvm_oct_alloc_fpa_pool(int pool, int size)
+{
+	int i;
+	int ret = 0;
+
+	if (pool >= (int)ARRAY_SIZE(cvm_oct_pools) || size < 128)
+		return -EINVAL;
+
+	BUG_ON(octeon_has_feature(OCTEON_FEATURE_FPA3));
+
+	mutex_lock(&cvm_oct_pools_lock);
+
+	if (pool >= 0) {
+		if (cvm_oct_pools[pool].pool != -1) {
+			if (cvm_oct_pools[pool].size == size) {
+				/* Already allocated */
+				cvm_oct_pools[pool].users++;
+				ret = pool;
+				goto out;
+			} else {
+				/* conflict */
+				ret = -EINVAL;
+				goto out;
+			}
+		}
+		/* reserve/alloc fpa pool */
+		pool = cvmx_fpa1_reserve_pool(pool);
+		if (pool < 0) {
+			ret = -EINVAL;
+			goto out;
+		}
+	} else {
+		/* Find an established pool */
+		for (i = 0; i < ARRAY_SIZE(cvm_oct_pools); i++)
+			if (cvm_oct_pools[i].pool >= 0 &&
+			    cvm_oct_pools[i].size == size) {
+				cvm_oct_pools[i].users++;
+				ret = i;
+				goto out;
+			}
+
+		/* Alloc fpa pool */
+		pool = cvmx_fpa1_reserve_pool(pool);
+		if (pool < 0) {
+			/* No empties. */
+			ret = -EINVAL;
+			goto out;
+		}
+	}
+
+	/* Setup the pool */
+	cvm_oct_pools[pool].pool = pool;
+	cvm_oct_pools[pool].users++;
+	cvm_oct_pools[pool].size = size;
+	if (USE_SKBUFFS_IN_HW && pool == 0) {
+		/* Special packet pool */
+		cvm_oct_pools[pool].fill = cvm_oct_fill_hw_skbuff;
+		cvm_oct_pools[pool].empty = cvm_oct_free_hw_skbuff;
+	} else {
+		snprintf(cvm_oct_pools[pool].kmem_name,
+			 sizeof(cvm_oct_pools[pool].kmem_name),
+			 "oct-fpa-%d", size);
+		cvm_oct_pools[pool].fill = cvm_oct_fill_hw_kmem;
+		cvm_oct_pools[pool].empty = cvm_oct_free_hw_kmem;
+		cvm_oct_pools[pool].kmem =
+			kmem_cache_create(cvm_oct_pools[pool].kmem_name,
+					  size, 128, 0, NULL);
+		if (!cvm_oct_pools[pool].kmem) {
+			ret = -ENOMEM;
+			cvm_oct_pools[pool].pool = -1;
+			cvmx_fpa1_release_pool(pool);
+			goto out;
+		}
+	}
+	ret = pool;
+out:
+	mutex_unlock(&cvm_oct_pools_lock);
+	return ret;
+}
+EXPORT_SYMBOL(cvm_oct_alloc_fpa_pool);
+
+/**
+ * cvm_oct_release_fpa_pool() - Releases an FPA pool
+ * @pool:  Pool number.
+ *
+ * This undoes the action of cvm_oct_alloc_fpa_pool().
+ *
+ * Returns zero on success.
+ */
+int cvm_oct_release_fpa_pool(int pool)
+{
+	int ret = -EINVAL;
+
+	if (pool < 0 || pool >= (int)ARRAY_SIZE(cvm_oct_pools))
+		return ret;
+
+	mutex_lock(&cvm_oct_pools_lock);
+
+	if (cvm_oct_pools[pool].users <= 0) {
+		pr_err("Error: Unbalanced FPA pool allocation\n");
+		goto out;
+	}
+	cvm_oct_pools[pool].users--;
+
+	if (cvm_oct_pools[pool].users == 0)
+		cvmx_fpa1_release_pool(pool);
+
+	ret = 0;
+out:
+	mutex_unlock(&cvm_oct_pools_lock);
+	return ret;
+}
+EXPORT_SYMBOL(cvm_oct_release_fpa_pool);
+
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Cavium, Inc. Ethernet/FPA memory allocator.");
diff --git a/drivers/net/ethernet/octeon/ethernet-napi.c b/drivers/net/ethernet/octeon/ethernet-napi.c
new file mode 100644
index 0000000..d0d5c83
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-napi.c
@@ -0,0 +1,472 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+
+/* This file is included in ethernet-rx.c *twice* */
+
+#undef CVM_OCT_NAPI_POLL
+#undef CVM_OCT_NAPI_HAS_CN68XX_SSO
+
+#ifdef CVM_OCT_NAPI_68
+#define CVM_OCT_NAPI_POLL cvm_oct_napi_poll_68
+#define CVM_OCT_NAPI_HAS_CN68XX_SSO 1
+#else
+#define CVM_OCT_NAPI_POLL cvm_oct_napi_poll_38
+#define CVM_OCT_NAPI_HAS_CN68XX_SSO 0
+#endif
+
+/**
+ * cvm_oct_napi_poll - the NAPI poll function.
+ * @napi: The NAPI instance, or null if called from cvm_oct_poll_controller
+ * @budget: Maximum number of packets to receive.
+ *
+ * Returns the number of packets processed.
+ */
+static int CVM_OCT_NAPI_POLL(struct napi_struct *napi, int budget)
+{
+	const int	coreid = cvmx_get_core_num();
+	int		no_work_count = 0;
+	u64		old_group_mask;
+	u64		old_scratch;
+	int		rx_count = 0;
+	bool		did_work_request = false;
+	bool		packet_copied;
+
+	char		*p = (char *)cvm_oct_by_pkind;
+
+	/* Prefetch cvm_oct_device since we know we need it soon */
+	prefetch(&p[0]);
+	prefetch(&p[SMP_CACHE_BYTES]);
+	prefetch(&p[2 * SMP_CACHE_BYTES]);
+
+	if (USE_ASYNC_IOBDMA) {
+		/* Save scratch in case userspace is using it */
+		CVMX_SYNCIOBDMA;
+		old_scratch = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
+	}
+
+	/* Only allow work for our group (and preserve priorities) */
+	if (CVM_OCT_NAPI_HAS_CN68XX_SSO) {
+		old_group_mask = cvmx_read_csr(CVMX_SSO_PPX_GRP_MSK(coreid));
+		cvmx_write_csr(CVMX_SSO_PPX_GRP_MSK(coreid),
+			       1ull << pow_receive_group);
+		/* Read it back so it takes effect before we request work */
+		cvmx_read_csr(CVMX_SSO_PPX_GRP_MSK(coreid));
+	} else {
+		old_group_mask = cvmx_read_csr(CVMX_POW_PP_GRP_MSKX(coreid));
+		cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(coreid),
+			       (old_group_mask & ~0xFFFFull) | 1 << pow_receive_group);
+	}
+
+	if (USE_ASYNC_IOBDMA) {
+		cvmx_pow_work_request_async(CVMX_SCR_SCRATCH, CVMX_POW_NO_WAIT);
+		did_work_request = true;
+	}
+
+	while (rx_count < budget) {
+		struct sk_buff *skb = NULL;
+		struct sk_buff **pskb = NULL;
+		struct octeon_ethernet *priv;
+		enum cvm_oct_callback_result callback_result;
+		bool skb_in_hw;
+		cvmx_wqe_t *work;
+		int port;
+		unsigned int segments;
+		int packets_to_replace = 0;
+		unsigned int packet_len;
+
+		union cvmx_buf_ptr  packet_ptr;
+
+		if (USE_ASYNC_IOBDMA && did_work_request)
+			work = cvmx_pow_work_response_async(CVMX_SCR_SCRATCH);
+		else
+			work = cvmx_pow_work_request_sync(CVMX_POW_NO_WAIT);
+
+		did_work_request = false;
+		if (unlikely(work == NULL)) {
+			/* It takes so long to get here, so lets wait
+			 * around a little to see if another packet
+			 * comes in.
+			 */
+			if (no_work_count >= 2)
+				break;
+			no_work_count++;
+			ndelay(500);
+			continue;
+		}
+		packet_ptr = work->packet_ptr;
+		pskb = cvm_oct_packet_to_skb(cvm_oct_get_buffer_ptr(packet_ptr));
+		prefetch(pskb);
+
+		if (likely(USE_ASYNC_IOBDMA && rx_count < (budget - 1))) {
+			cvmx_pow_work_request_async_nocheck(CVMX_SCR_SCRATCH, CVMX_POW_NO_WAIT);
+			did_work_request = true;
+		}
+
+		if (unlikely(rx_count == 0)) {
+			/* First time through, see if there is enough
+			 * work waiting to merit waking another
+			 * CPU.
+			 */
+			int backlog;
+			int cores_in_use = core_state.active_cores;
+			if (CVM_OCT_NAPI_HAS_CN68XX_SSO) {
+				union cvmx_sso_wq_int_cntx counts;
+				counts.u64 = cvmx_read_csr(CVMX_SSO_WQ_INT_CNTX(pow_receive_group));
+				backlog = counts.s.iq_cnt + counts.s.ds_cnt;
+			} else {
+				union cvmx_pow_wq_int_cntx counts;
+				counts.u64 = cvmx_read_csr(CVMX_POW_WQ_INT_CNTX(pow_receive_group));
+				backlog = counts.s.iq_cnt + counts.s.ds_cnt;
+			}
+			if (backlog > rx_cpu_factor * cores_in_use &&
+			    napi != NULL &&
+			    cores_in_use < core_state.baseline_cores)
+				cvm_oct_enable_one_cpu();
+		}
+		rx_count++;
+
+		/* If WORD2[SOFTWARE] then this WQE is a complete for
+		 * a TX packet.
+		 */
+		if (work->word2.s.software) {
+			struct octeon_ethernet *priv;
+			int packet_qos = work->word0.raw.unused;
+
+			skb = (struct sk_buff *)packet_ptr.u64;
+			priv = netdev_priv(skb->dev);
+			if (netif_queue_stopped(skb->dev))
+				netif_wake_queue(skb->dev);
+			if (unlikely((skb_shinfo(skb)->tx_flags | SKBTX_IN_PROGRESS) != 0 &&
+				     priv->tx_timestamp_hw)) {
+					u64 ns = *(u64 *)work->packet_data;
+					struct skb_shared_hwtstamps ts;
+					ts.syststamp = cvm_oct_ptp_to_ktime(ns);
+					ts.hwtstamp = ns_to_ktime(ns);
+					skb_tstamp_tx(skb, &ts);
+			}
+			dev_kfree_skb_any(skb);
+
+			cvmx_fpa1_free(work, wqe_pool, DONT_WRITEBACK(1));
+
+			/* We are done with this one, adjust the queue
+			 * depth.
+			 */
+			cvmx_hwfau_atomic_add32(priv->tx_queue[packet_qos].fau, -1);
+			continue;
+		}
+		segments = work->word2.s.bufs;
+		skb_in_hw = USE_SKBUFFS_IN_HW && segments > 0;
+		if (likely(skb_in_hw)) {
+			skb = *pskb;
+			prefetch(&skb->head);
+			prefetch(&skb->len);
+		}
+
+		if (CVM_OCT_NAPI_HAS_CN68XX_SSO)
+			port = work->word0.pip.cn68xx.pknd;
+		else
+			port = work->word1.cn38xx.ipprt;
+
+		prefetch(cvm_oct_by_pkind[port]);
+
+		/* Immediately throw away all packets with receive errors */
+		if (unlikely(work->word2.snoip.rcv_error)) {
+			if (cvm_oct_check_rcv_error(work))
+				continue;
+		}
+
+		if (CVM_OCT_NAPI_HAS_CN68XX_SSO) {
+			if (unlikely(cvm_oct_by_pkind[port] == NULL))
+				priv = cvm_oct_dev_for_port(work->word2.s_cn68xx.port);
+			else
+				priv = cvm_oct_by_pkind[port];
+		} else {
+			/* srio priv is based on mbox, not port */
+			if (port >= 40 && port <= 47)
+				priv = NULL;
+			else
+				priv = cvm_oct_by_pkind[port];
+		}
+
+		if (likely(priv) && priv->rx_strip_fcs)
+			work->word1.len -= 4;
+
+		packet_len = work->word1.len;
+		/* We can only use the zero copy path if skbuffs are
+		 * in the FPA pool and the packet fits in a single
+		 * buffer.
+		 */
+		if (likely(skb_in_hw)) {
+			skb->data = phys_to_virt(packet_ptr.s.addr);
+			prefetch(skb->data);
+			skb->len = packet_len;
+			packets_to_replace = segments;
+			if (likely(segments == 1)) {
+				skb_set_tail_pointer(skb, skb->len);
+			} else {
+				struct sk_buff *current_skb = skb;
+				struct sk_buff *next_skb = NULL;
+				unsigned int segment_size;
+				bool first_frag = true;
+
+				skb_frag_list_init(skb);
+				/* Multi-segment packet. */
+				for (;;) {
+					/* Octeon Errata PKI-100: The segment size is
+					 * wrong. Until it is fixed, calculate the
+					 * segment size based on the packet pool
+					 * buffer size. When it is fixed, the
+					 * following line should be replaced with this
+					 * one: int segment_size =
+					 * segment_ptr.s.size;
+					 */
+					segment_size = FPA_PACKET_POOL_SIZE -
+						(packet_ptr.s.addr - (((packet_ptr.s.addr >> 7) - packet_ptr.s.back) << 7));
+					if (segment_size > packet_len)
+						segment_size = packet_len;
+					if (!first_frag) {
+						current_skb->len = segment_size;
+						skb->data_len += segment_size;
+						skb->truesize += current_skb->truesize;
+					}
+					skb_set_tail_pointer(current_skb, segment_size);
+					packet_len -= segment_size;
+					segments--;
+					if (segments == 0)
+						break;
+					packet_ptr = *(union cvmx_buf_ptr *)phys_to_virt(packet_ptr.s.addr - 8);
+					next_skb = *cvm_oct_packet_to_skb(cvm_oct_get_buffer_ptr(packet_ptr));
+					if (first_frag) {
+						skb_frag_add_head(current_skb, next_skb);
+					} else {
+						current_skb->next = next_skb;
+						next_skb->next = NULL;
+					}
+					current_skb = next_skb;
+					first_frag = false;
+					current_skb->data = phys_to_virt(packet_ptr.s.addr);
+				}
+			}
+			packet_copied = false;
+		} else {
+			/* We have to copy the packet. First allocate
+			 * an skbuff for it.
+			 */
+			skb = dev_alloc_skb(packet_len);
+			if (!skb) {
+				printk_ratelimited("Port %d failed to allocate skbuff, packet dropped\n",
+						   port);
+				cvm_oct_free_work(work);
+				continue;
+			}
+
+			/* Check if we've received a packet that was
+			 * entirely stored in the work entry.
+			 */
+			if (unlikely(work->word2.s.bufs == 0)) {
+				u8 *ptr = work->packet_data;
+
+				if (likely(!work->word2.s.not_IP)) {
+					/* The beginning of the packet
+					 * moves for IP packets.
+					 */
+					if (work->word2.s.is_v6)
+						ptr += 2;
+					else
+						ptr += 6;
+				}
+				memcpy(skb_put(skb, packet_len), ptr, packet_len);
+				/* No packet buffers to free */
+			} else {
+				int segments = work->word2.s.bufs;
+				union cvmx_buf_ptr segment_ptr = work->packet_ptr;
+
+				while (segments--) {
+					union cvmx_buf_ptr next_ptr =
+					    *(union cvmx_buf_ptr *)phys_to_virt(segment_ptr.s.addr - 8);
+
+			/* Octeon Errata PKI-100: The segment size is
+			 * wrong. Until it is fixed, calculate the
+			 * segment size based on the packet pool
+			 * buffer size. When it is fixed, the
+			 * following line should be replaced with this
+			 * one: int segment_size =
+			 * segment_ptr.s.size;
+			 */
+					int segment_size = FPA_PACKET_POOL_SIZE -
+						(segment_ptr.s.addr - (((segment_ptr.s.addr >> 7) - segment_ptr.s.back) << 7));
+					/* Don't copy more than what
+					 * is left in the packet.
+					 */
+					if (segment_size > packet_len)
+						segment_size = packet_len;
+					/* Copy the data into the packet */
+					memcpy(skb_put(skb, segment_size),
+					       phys_to_virt(segment_ptr.s.addr),
+					       segment_size);
+					packet_len -= segment_size;
+					segment_ptr = next_ptr;
+				}
+			}
+			packet_copied = true;
+		}
+		/* srio priv is based on mbox, not port */
+		if (!CVM_OCT_NAPI_HAS_CN68XX_SSO && unlikely(priv == NULL)) {
+			const struct cvmx_srio_rx_message_header *rx_header =
+				(const struct cvmx_srio_rx_message_header *)skb->data;
+			*(u64 *)rx_header = be64_to_cpu(*(u64 *)rx_header);
+			priv = cvm_oct_by_srio_mbox[(port - 40) >> 1][rx_header->word0.s.mbox];
+		}
+
+		if (likely(priv)) {
+#ifdef CONFIG_RAPIDIO
+			if (unlikely(priv->imode == CVMX_HELPER_INTERFACE_MODE_SRIO)) {
+				__skb_pull(skb, sizeof(struct cvmx_srio_rx_message_header));
+
+				atomic64_add(1, (atomic64_t *)&priv->netdev->stats.rx_packets);
+				atomic64_add(skb->len, (atomic64_t *)&priv->netdev->stats.rx_bytes);
+			}
+#endif
+			/* Only accept packets for devices that are
+			 * currently up.
+			 */
+			if (likely(priv->netdev->flags & IFF_UP)) {
+				if (priv->rx_timestamp_hw) {
+					/* The first 8 bytes are the timestamp */
+					u64 ns = *(u64 *)skb->data;
+					struct skb_shared_hwtstamps *ts;
+					ts = skb_hwtstamps(skb);
+					ts->hwtstamp = ns_to_ktime(ns);
+					ts->syststamp = cvm_oct_ptp_to_ktime(ns);
+					__skb_pull(skb, 8);
+				}
+				skb->protocol = eth_type_trans(skb, priv->netdev);
+				skb->dev = priv->netdev;
+
+				if (unlikely(work->word2.s.not_IP || work->word2.s.IP_exc ||
+					work->word2.s.L4_error || !work->word2.s.tcp_or_udp))
+					skb->ip_summed = CHECKSUM_NONE;
+				else
+					skb->ip_summed = CHECKSUM_UNNECESSARY;
+
+				/* Increment RX stats for virtual ports */
+				if (port >= CVMX_PIP_NUM_INPUT_PORTS) {
+					atomic64_add(1, (atomic64_t *)&priv->netdev->stats.rx_packets);
+					atomic64_add(skb->len, (atomic64_t *)&priv->netdev->stats.rx_bytes);
+				}
+				if (priv->intercept_cb) {
+					callback_result = priv->intercept_cb(priv->netdev, work, skb);
+					switch (callback_result) {
+					case CVM_OCT_PASS:
+						netif_receive_skb(skb);
+						break;
+					case CVM_OCT_DROP:
+						dev_kfree_skb_any(skb);
+						atomic64_add(1, (atomic64_t *)&priv->netdev->stats.rx_dropped);
+						break;
+					case CVM_OCT_TAKE_OWNERSHIP_WORK:
+						/*
+						 * Interceptor took
+						 * our work, but we
+						 * need to free the
+						 * skbuff
+						 */
+						if (USE_SKBUFFS_IN_HW && likely(!packet_copied)) {
+							/*
+							 * We can't free the skbuff since its data is
+							 * the same as the work. In this case we don't
+							 * do anything
+							 */
+						} else {
+							dev_kfree_skb_any(skb);
+						}
+						break;
+					case CVM_OCT_TAKE_OWNERSHIP_SKB:
+						/* Interceptor took our packet */
+						break;
+					}
+				} else {
+					netif_receive_skb(skb);
+					callback_result = CVM_OCT_PASS;
+				}
+			} else {
+				/* Drop any packet received for a device that isn't up */
+				atomic64_add(1, (atomic64_t *)&priv->netdev->stats.rx_dropped);
+				dev_kfree_skb_any(skb);
+				callback_result = CVM_OCT_DROP;
+
+			}
+		} else {
+			/* Drop any packet received for a device that
+			 * doesn't exist.
+			 */
+			printk_ratelimited("Port %d not controlled by Linux, packet dropped\n",
+					   port);
+			dev_kfree_skb_any(skb);
+			callback_result = CVM_OCT_DROP;
+		}
+		/* We only need to free the work if the interceptor didn't
+		   take over ownership of it */
+		if (callback_result != CVM_OCT_TAKE_OWNERSHIP_WORK) {
+			/* Check to see if the skbuff and work share the same
+			 * packet buffer.
+			 */
+			if (USE_SKBUFFS_IN_HW && likely(!packet_copied)) {
+				/* This buffer needs to be replaced, increment
+				 * the number of buffers we need to free by
+				 * one.
+				 */
+				cvmx_hwfau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE,
+						      packets_to_replace);
+
+				cvmx_fpa1_free(work, wqe_pool, DONT_WRITEBACK(1));
+			} else {
+				cvm_oct_free_work(work);
+			}
+		}
+	}
+	/* Restore the original POW group mask */
+	if (CVM_OCT_NAPI_HAS_CN68XX_SSO) {
+		cvmx_write_csr(CVMX_SSO_PPX_GRP_MSK(coreid), old_group_mask);
+		/* Read it back so it takes effect before ?? */
+		cvmx_read_csr(CVMX_SSO_PPX_GRP_MSK(coreid));
+	} else {
+		cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(coreid), old_group_mask);
+	}
+	if (USE_ASYNC_IOBDMA) {
+		/* Restore the scratch area */
+		cvmx_scratch_write64(CVMX_SCR_SCRATCH, old_scratch);
+	}
+	cvm_oct_rx_refill_pool(0);
+
+	if (rx_count < budget && napi != NULL) {
+		/* No more work */
+		napi_complete(napi);
+		cvm_oct_no_more_work(napi);
+	}
+	return rx_count;
+}
diff --git a/drivers/net/ethernet/octeon/ethernet-rgmii.c b/drivers/net/ethernet/octeon/ethernet-rgmii.c
new file mode 100644
index 0000000..a88dcd6
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-rgmii.c
@@ -0,0 +1,298 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/interrupt.h>
+#include <linux/phy.h>
+#include <linux/ratelimit.h>
+#include <net/dst.h>
+
+#include <asm/octeon/octeon-hw-status.h>
+#include <asm/octeon/octeon.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+
+#include <asm/octeon/cvmx-helper.h>
+
+#include <asm/octeon/cvmx-ipd-defs.h>
+#include <asm/octeon/cvmx-npi-defs.h>
+#include <asm/octeon/cvmx-gmxx-defs.h>
+
+#define INT_BIT_PHY_LINK 16
+#define INT_BIT_PHY_SPD 17
+#define INT_BIT_PHY_DUPX 18
+
+DEFINE_SPINLOCK(global_register_lock);
+
+static void cvm_oct_rgmii_poll(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	cvmx_helper_link_info_t link_info;
+
+	if (priv->phydev) {
+		link_info.u64 = 0;
+		link_info.s.link_up = priv->last_link ? 1 : 0;
+		link_info.s.full_duplex = priv->phydev->duplex ? 1 : 0;
+		link_info.s.speed = priv->phydev->speed;
+	} else {
+		link_info = cvmx_helper_link_get(priv->ipd_port);
+	}
+	if (link_info.u64 == priv->link_info) {
+		/* If the 10Mbps preamble workaround is supported and we're
+		 * at 10Mbps we may need to do some special checking.
+		 */
+		if (USE_10MBPS_PREAMBLE_WORKAROUND && (link_info.s.speed == 10)) {
+			/* Read the GMXX_RXX_INT_REG[PCTERR] bit and
+			 * see if we are getting preamble errors.
+			 */
+			union cvmx_gmxx_rxx_int_reg gmxx_rxx_int_reg;
+			gmxx_rxx_int_reg.u64 =
+				cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(priv->interface_port, priv->interface));
+			if (gmxx_rxx_int_reg.s.pcterr) {
+				/* We are getting preamble errors at
+				 * 10Mbps.  Most likely the PHY is
+				 * giving us packets with mis aligned
+				 * preambles. In order to get these
+				 * packets we need to disable preamble
+				 * checking and do it in software.
+				 */
+				union cvmx_gmxx_rxx_frm_ctl gmxx_rxx_frm_ctl;
+
+				/* Disable preamble checking */
+				gmxx_rxx_frm_ctl.u64 =
+					cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface));
+				gmxx_rxx_frm_ctl.s.pre_chk = 0;
+				cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface),
+					       gmxx_rxx_frm_ctl.u64);
+
+				/* Clear any error bits */
+				cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(priv->interface_port, priv->interface),
+					       gmxx_rxx_int_reg.u64);
+				printk_ratelimited("%s: Using 10Mbps with software preamble removal\n",
+						   dev->name);
+			}
+		}
+		return;
+	}
+
+	/* If the 10Mbps preamble workaround is allowed we need to on
+	 * preamble checking, FCS stripping, and clear error bits on
+	 * every speed change. If errors occur during 10Mbps operation
+	 * the above code will change this stuff.
+	 */
+	if (USE_10MBPS_PREAMBLE_WORKAROUND) {
+		union cvmx_gmxx_rxx_frm_ctl gmxx_rxx_frm_ctl;
+		union cvmx_gmxx_rxx_int_reg gmxx_rxx_int_reg;
+
+		/* Enable preamble checking */
+		gmxx_rxx_frm_ctl.u64 =
+		    cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface));
+		gmxx_rxx_frm_ctl.s.pre_chk = 1;
+		cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(priv->interface_port, priv->interface),
+			       gmxx_rxx_frm_ctl.u64);
+		/* Clear any error bits */
+		gmxx_rxx_int_reg.u64 =
+			cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(priv->interface_port, priv->interface));
+		cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(priv->interface_port, priv->interface),
+			       gmxx_rxx_int_reg.u64);
+	}
+	if (priv->phydev == NULL) {
+		link_info = cvmx_helper_link_autoconf(priv->ipd_port);
+		priv->link_info = link_info.u64;
+	}
+
+	if (priv->phydev == NULL)
+		cvm_oct_set_carrier(priv, link_info);
+}
+
+static int cvm_oct_rgmii_hw_status(struct notifier_block *nb, unsigned long val, void *v)
+{
+	struct octeon_ethernet *priv = container_of(nb, struct octeon_ethernet, hw_status_notifier);
+
+	if (val == OCTEON_HW_STATUS_SOURCE_ASSERTED) {
+		struct octeon_hw_status_data *d = v;
+		if (d->reg == CVMX_GMXX_RXX_INT_REG(priv->interface_port, priv->interface) &&
+		    (d->bit == INT_BIT_PHY_LINK ||
+		     d->bit == INT_BIT_PHY_SPD ||
+		     d->bit == INT_BIT_PHY_DUPX)) {
+			if (!atomic_read(&cvm_oct_poll_queue_stopping))
+				queue_work(cvm_oct_poll_queue, &priv->port_work);
+			return NOTIFY_STOP;
+		}
+	}
+	return NOTIFY_DONE;
+}
+
+static void cvm_oct_rgmii_immediate_poll(struct work_struct *work)
+{
+	struct octeon_ethernet *priv = container_of(work, struct octeon_ethernet, port_work);
+	cvm_oct_rgmii_poll(priv->netdev);
+}
+
+int cvm_oct_rgmii_open(struct net_device *dev)
+{
+	union cvmx_ipd_sub_port_fcs ipd_sub_port_fcs;
+	struct octeon_hw_status_reg sr[3];
+	u64 en_mask;
+	union cvmx_gmxx_prtx_cfg gmx_cfg;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	cvmx_helper_link_info_t link_info;
+	int rv;
+
+	rv = cvm_oct_phy_setup_device(dev);
+	if (rv)
+		return rv;
+
+
+	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
+	gmx_cfg.s.en = 1;
+	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface), gmx_cfg.u64);
+
+	if (!octeon_is_simulation()) {
+		if (priv->phydev) {
+			int r = phy_read_status(priv->phydev);
+			if (r == 0 && priv->phydev->link == 0)
+				netif_carrier_off(dev);
+			cvm_oct_adjust_link(dev);
+		} else {
+			link_info = cvmx_helper_link_get(priv->ipd_port);
+			if (!link_info.s.link_up)
+				netif_carrier_off(dev);
+			priv->poll = cvm_oct_rgmii_poll;
+		}
+	}
+
+	INIT_WORK(&priv->port_work, cvm_oct_rgmii_immediate_poll);
+
+
+	/* Only true RGMII ports need to be polled. In GMII mode, port
+	 * 0 is really a RGMII port.
+	 */
+	if ((priv->imode == CVMX_HELPER_INTERFACE_MODE_GMII && priv->ipd_port != 0) ||
+	    octeon_is_simulation())
+		return 0;
+
+	/* Due to GMX errata in CN3XXX series chips, it is necessary
+	 * to take the link down immediately when the PHY changes
+	 * state. In order to do this we call the poll function every
+	 * time the RGMII inband status changes.  This may cause
+	 * problems if the PHY doesn't implement inband status
+	 * properly.
+	 */
+	priv->hw_status_notifier.priority = 10;
+	priv->hw_status_notifier.notifier_call = cvm_oct_rgmii_hw_status;
+	octeon_hw_status_notifier_register(&priv->hw_status_notifier);
+
+	en_mask = 0;
+	memset(sr, 0, sizeof(sr));
+	sr[0].reg = 46; /* RML */
+	sr[0].reg_is_hwint = 1;
+	sr[0].has_child = 1;
+	sr[1].reg = CVMX_NPI_RSL_INT_BLOCKS;
+	sr[1].bit = priv->interface + 1;
+	sr[1].has_child = 1;
+	sr[2].reg = CVMX_GMXX_RXX_INT_REG(priv->interface_port, priv->interface);
+	sr[2].mask_reg = CVMX_GMXX_RXX_INT_EN(priv->interface_port, priv->interface);
+	sr[2].ack_w1c = 1;
+
+	sr[2].bit = INT_BIT_PHY_LINK;
+	en_mask |= 1ull << sr[2].bit;
+	octeon_hw_status_add_source(sr);
+
+	sr[2].bit = INT_BIT_PHY_SPD;
+	en_mask |= 1ull << sr[2].bit;
+	octeon_hw_status_add_source(sr);
+
+	sr[2].bit = INT_BIT_PHY_DUPX;
+	en_mask |= 1ull << sr[2].bit;
+	octeon_hw_status_add_source(sr);
+
+	octeon_hw_status_enable(sr[2].reg, en_mask);
+
+
+	/* Disable FCS stripping for PKI-602*/
+	ipd_sub_port_fcs.u64 = cvmx_read_csr(CVMX_IPD_SUB_PORT_FCS);
+	ipd_sub_port_fcs.s.port_bit &= 0xffffffffull ^ (1ull << priv->ipd_port);
+	cvmx_write_csr(CVMX_IPD_SUB_PORT_FCS, ipd_sub_port_fcs.u64);
+	priv->rx_strip_fcs = 1;
+
+	return 0;
+}
+
+int cvm_oct_rgmii_stop(struct net_device *dev)
+{
+	union cvmx_ipd_sub_port_fcs ipd_sub_port_fcs;
+	union cvmx_gmxx_prtx_cfg gmx_cfg;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
+	gmx_cfg.s.en = 0;
+	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface), gmx_cfg.u64);
+
+
+	if (priv->hw_status_notifier.notifier_call) {
+		struct octeon_hw_status_reg sr;
+		memset(&sr, 0, sizeof(sr));
+
+		sr.reg = CVMX_GMXX_RXX_INT_REG(priv->interface_port, priv->interface);
+		sr.mask_reg = CVMX_GMXX_RXX_INT_EN(priv->interface_port, priv->interface);
+		sr.ack_w1c = 1;
+		sr.bit = INT_BIT_PHY_LINK;
+		octeon_hw_status_remove_source(&sr);
+		sr.bit = INT_BIT_PHY_SPD;
+		octeon_hw_status_remove_source(&sr);
+		sr.bit = INT_BIT_PHY_DUPX;
+		octeon_hw_status_remove_source(&sr);
+		octeon_hw_status_notifier_unregister(&priv->hw_status_notifier);
+		priv->hw_status_notifier.notifier_call = NULL;
+	}
+
+	cancel_work_sync(&priv->port_work);
+
+	/* re-Enable FCS stripping */
+	ipd_sub_port_fcs.u64 = cvmx_read_csr(CVMX_IPD_SUB_PORT_FCS);
+	ipd_sub_port_fcs.s.port_bit |= 1ull << priv->ipd_port;
+	cvmx_write_csr(CVMX_IPD_SUB_PORT_FCS, ipd_sub_port_fcs.u64);
+	priv->rx_strip_fcs = 0;
+
+	return cvm_oct_common_stop(dev);
+}
+
+int cvm_oct_rgmii_init(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	union cvmx_gmxx_prtx_cfg gmx_cfg;
+
+	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
+	gmx_cfg.s.en = 0;
+	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface), gmx_cfg.u64);
+
+	cvm_oct_common_init(dev);
+
+	return 0;
+}
diff --git a/drivers/net/ethernet/octeon/ethernet-rx.c b/drivers/net/ethernet/octeon/ethernet-rx.c
new file mode 100644
index 0000000..45eeab0
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-rx.c
@@ -0,0 +1,601 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/cache.h>
+#include <linux/cpumask.h>
+#include <linux/netdevice.h>
+#include <linux/init.h>
+#include <linux/etherdevice.h>
+#include <linux/ip.h>
+#include <linux/string.h>
+#include <linux/prefetch.h>
+#include <linux/ratelimit.h>
+#include <linux/smp.h>
+#include <linux/interrupt.h>
+#include <net/dst.h>
+#ifdef CONFIG_XFRM
+#include <linux/xfrm.h>
+#include <net/xfrm.h>
+#endif /* CONFIG_XFRM */
+
+#include <asm/octeon/octeon.h>
+#include <asm/octeon/octeon-hw-status.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+
+#include <asm/octeon/cvmx-helper.h>
+#include <asm/octeon/cvmx-wqe.h>
+#include <asm/octeon/cvmx-hwfau.h>
+#include <asm/octeon/cvmx-pow.h>
+#include <asm/octeon/cvmx-pip.h>
+#include <asm/octeon/cvmx-ipd.h>
+#include <asm/octeon/cvmx-srio.h>
+#include <asm/octeon/cvmx-scratch.h>
+
+#include <asm/octeon/cvmx-gmxx-defs.h>
+#include <asm/octeon/cvmx-sso-defs.h>
+
+struct cvm_napi_wrapper {
+	struct napi_struct napi;
+	int available;
+} ____cacheline_aligned_in_smp;
+
+static struct cvm_napi_wrapper cvm_oct_napi[NR_CPUS] __cacheline_aligned_in_smp;
+
+struct cvm_oct_core_state {
+	int baseline_cores;
+	/* We want to read this without having to acquire the lock,
+	 * make it volatile so we are likely to get a fairly current
+	 * value.
+	 */
+	volatile int active_cores;
+	/* cvm_napi_wrapper.available and active_cores must be kept
+	 * consistent with this lock.
+	 */
+	spinlock_t lock;
+} ____cacheline_aligned_in_smp;
+
+static struct cvm_oct_core_state core_state __cacheline_aligned_in_smp;
+
+#ifdef CONFIG_SMP
+static int cvm_oct_enable_one_message;
+#endif
+
+static void cvm_oct_enable_napi(void)
+{
+	int cpu = smp_processor_id();
+	napi_schedule(&cvm_oct_napi[cpu].napi);
+}
+
+static void cvm_oct_enable_one_cpu(void)
+{
+	int cpu;
+	unsigned long flags;
+	spin_lock_irqsave(&core_state.lock, flags);
+	/* ... if a CPU is available, Turn on NAPI polling for that CPU.  */
+	for_each_online_cpu(cpu) {
+		if (cvm_oct_napi[cpu].available > 0) {
+			cvm_oct_napi[cpu].available--;
+			core_state.active_cores++;
+			spin_unlock_irqrestore(&core_state.lock, flags);
+			if (cpu == smp_processor_id()) {
+				cvm_oct_enable_napi();
+			} else {
+#ifdef CONFIG_SMP
+				octeon_send_ipi_single(cpu, cvm_oct_enable_one_message);
+#else
+				BUG();
+#endif
+			}
+			goto out;
+		}
+	}
+	spin_unlock_irqrestore(&core_state.lock, flags);
+out:
+	return;
+}
+
+static void cvm_oct_no_more_work(struct napi_struct *napi)
+{
+	struct cvm_napi_wrapper *nr = container_of(napi, struct cvm_napi_wrapper, napi);
+	int current_active;
+	unsigned long flags;
+
+	spin_lock_irqsave(&core_state.lock, flags);
+
+	core_state.active_cores--;
+	current_active = core_state.active_cores;
+	nr->available++;
+	BUG_ON(nr->available != 1);
+
+	spin_unlock_irqrestore(&core_state.lock, flags);
+
+	if (current_active == 0) {
+		/* No more CPUs doing processing, enable interrupts so
+		 * we can start processing again when there is
+		 * something to do.
+		 */
+		if (OCTEON_IS_MODEL(OCTEON_CN68XX)) {
+			union cvmx_sso_wq_int_thrx int_thr;
+			int_thr.u64 = 0;
+			int_thr.s.iq_thr = 1;
+			int_thr.s.ds_thr = 1;
+			/*
+			 * Enable SSO interrupt when our port has at
+			 * least one packet.
+			 */
+			cvmx_write_csr(CVMX_SSO_WQ_INT_THRX(pow_receive_group),
+				       int_thr.u64);
+		} else {
+			union cvmx_pow_wq_int_thrx int_thr;
+			int_thr.u64 = 0;
+			int_thr.s.iq_thr = 1;
+			int_thr.s.ds_thr = 1;
+			/* Enable POW interrupt when our port has at
+			 * least one packet.
+			 */
+			cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group),
+				       int_thr.u64);
+		}
+	}
+}
+
+/**
+ * cvm_oct_do_interrupt - interrupt handler.
+ *
+ * The interrupt occurs whenever the POW has packets in our group.
+ *
+ */
+static irqreturn_t cvm_oct_do_interrupt(int cpl, void *dev_id)
+{
+	int cpu = smp_processor_id();
+	unsigned long flags;
+
+	/* Disable the IRQ and start napi_poll. */
+	if (OCTEON_IS_MODEL(OCTEON_CN68XX)) {
+		cvmx_write_csr(CVMX_SSO_WQ_INT_THRX(pow_receive_group), 0);
+		cvmx_write_csr(CVMX_SSO_WQ_INT, 1ULL << pow_receive_group);
+	} else {
+		union cvmx_pow_wq_int wq_int;
+
+		cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group), 0);
+
+		wq_int.u64 = 0;
+		wq_int.s.wq_int = 1 << pow_receive_group;
+		cvmx_write_csr(CVMX_POW_WQ_INT, wq_int.u64);
+	}
+
+	spin_lock_irqsave(&core_state.lock, flags);
+
+	/* ... and NAPI better not be running on this CPU.  */
+	BUG_ON(cvm_oct_napi[cpu].available != 1);
+	cvm_oct_napi[cpu].available--;
+
+	/* There better be cores available...  */
+	core_state.active_cores++;
+	BUG_ON(core_state.active_cores > core_state.baseline_cores);
+
+	spin_unlock_irqrestore(&core_state.lock, flags);
+
+	cvm_oct_enable_napi();
+
+	return IRQ_HANDLED;
+}
+
+/**
+ * cvm_oct_check_rcv_error - process receive errors
+ * @work: Work queue entry pointing to the packet.
+ *
+ * Returns Non-zero if the packet can be dropped, zero otherwise.
+ */
+static int cvm_oct_check_rcv_error(cvmx_wqe_t *work)
+{
+	bool err  = false;
+	int port = cvmx_wqe_get_port(work);
+
+	if ((work->word2.snoip.err_code == 10) && (work->word1.len <= 64)) {
+		/* Ignore length errors on min size packets. Some
+		 * equipment incorrectly pads packets to 64+4FCS
+		 * instead of 60+4FCS.  Note these packets still get
+		 * counted as frame errors.
+		 */
+	} else if (USE_10MBPS_PREAMBLE_WORKAROUND &&
+		   ((work->word2.snoip.err_code == 5)
+		    || (work->word2.snoip.err_code == 7))) {
+
+		/* We received a packet with either an alignment error
+		 * or a FCS error. This may be signalling that we are
+		 * running 10Mbps with GMXX_RXX_FRM_CTL[PRE_CHK}
+		 * off. If this is the case we need to parse the
+		 * packet to determine if we can remove a non spec
+		 * preamble and generate a correct packet.
+		 */
+		int interface = cvmx_helper_get_interface_num(port);
+		int index = cvmx_helper_get_interface_index_num(port);
+		union cvmx_gmxx_rxx_frm_ctl gmxx_rxx_frm_ctl;
+		uint64_t frm_ctl_reg;
+
+		if (cvmx_helper_interface_get_mode(interface) ==
+			CVMX_HELPER_INTERFACE_MODE_AGL)
+			frm_ctl_reg = CVMX_AGL_GMX_RXX_FRM_CTL(index);
+		else
+			frm_ctl_reg = CVMX_GMXX_RXX_FRM_CTL(index, interface);
+
+		gmxx_rxx_frm_ctl.u64 = cvmx_read_csr(frm_ctl_reg);
+		if (gmxx_rxx_frm_ctl.s.pre_chk == 0) {
+
+			u8 *ptr = phys_to_virt(work->packet_ptr.s.addr);
+			int i = 0;
+
+			while (i < work->word1.len - 1) {
+				if (*ptr != 0x55)
+					break;
+				ptr++;
+				i++;
+			}
+
+			if (*ptr == 0xd5) {
+				work->packet_ptr.s.addr += i + 1;
+				work->word1.len -= i + 5;
+			} else if ((*ptr & 0xf) == 0xd) {
+				work->packet_ptr.s.addr += i;
+				work->word1.len -= i + 4;
+				for (i = 0; i < work->word1.len; i++) {
+					*ptr = ((*ptr & 0xf0) >> 4) | ((*(ptr + 1) & 0xf) << 4);
+					ptr++;
+				}
+			} else {
+				printk_ratelimited("Port %d unknown preamble, packet dropped\n",
+						   port);
+				/* cvmx_helper_dump_packet(work); */
+				cvm_oct_free_work(work);
+				return 1;
+			}
+		} else {
+			err  = true;
+		}
+	} else {
+			err  = true;
+	}
+	if (err) {
+		printk_ratelimited("Port %d receive error code %d, packet dropped\n",
+				   port, work->word2.snoip.err_code);
+		cvm_oct_free_work(work);
+		return 1;
+	}
+
+	return 0;
+}
+
+/**
+ * cvm_oct_ptp_to_ktime - Convert a hardware PTP timestamp into a
+ * kernel timestamp.
+ *
+ * @ptptime: 64 bit PTP timestamp, normally in nanoseconds
+ *
+ * Return ktime_t
+ */
+static ktime_t cvm_oct_ptp_to_ktime(u64 ptptime)
+{
+	ktime_t ktimebase;
+	u64 ptpbase;
+	unsigned long flags;
+
+	local_irq_save(flags);
+	/* Fill the icache with the code */
+	ktime_get_real();
+	/* Flush all pending operations */
+	mb();
+	/* Read the time and PTP clock as close together as
+	 * possible. It is important that this sequence take the same
+	 * amount of time to reduce jitter
+	 */
+	ktimebase = ktime_get_real();
+	ptpbase = octeon_read_ptp_csr(CVMX_MIO_PTP_CLOCK_HI);
+	local_irq_restore(flags);
+
+	return ktime_sub_ns(ktimebase, ptpbase - ptptime);
+}
+
+#undef CVM_OCT_NAPI_68
+#include "ethernet-napi.c"
+
+#define CVM_OCT_NAPI_68
+#include "ethernet-napi.c"
+
+static int (*cvm_oct_napi_poll)(struct napi_struct *, int);
+
+#ifdef CONFIG_NET_POLL_CONTROLLER
+
+/**
+ * cvm_oct_poll_controller - poll for receive packets
+ * device.
+ *
+ * @dev:    Device to poll. Unused
+ */
+void cvm_oct_poll_controller(struct net_device *dev)
+{
+	cvm_oct_napi_poll(NULL, 16);
+}
+#endif
+
+static struct kmem_cache *cvm_oct_kmem_sso;
+static int cvm_oct_sso_fptr_count;
+
+static int cvm_oct_sso_initialize(int num_wqe)
+{
+	union cvmx_sso_cfg sso_cfg;
+	union cvmx_fpa_fpfx_marks fpa_marks;
+	int i;
+	int rwq_bufs;
+
+	if (!OCTEON_IS_MODEL(OCTEON_CN68XX))
+		return 0;
+
+	rwq_bufs = 48 + DIV_ROUND_UP(num_wqe, 26);
+	cvm_oct_sso_fptr_count = rwq_bufs;
+	cvm_oct_kmem_sso = kmem_cache_create("octeon_ethernet_sso", 256, 128, 0, NULL);
+	if (cvm_oct_kmem_sso == NULL) {
+		pr_err("cannot create kmem_cache for octeon_ethernet_sso\n");
+		return -ENOMEM;
+	}
+
+	/*
+	 * CN68XX-P1 may reset with the wrong values, put in
+	 * the correct values.
+	 */
+	fpa_marks.u64 = 0;
+	fpa_marks.s.fpf_wr = 0xa4;
+	fpa_marks.s.fpf_rd = 0x40;
+	cvmx_write_csr(CVMX_FPA_FPF8_MARKS, fpa_marks.u64);
+
+	/* Make sure RWI/RWO is disabled. */
+	sso_cfg.u64 = cvmx_read_csr(CVMX_SSO_CFG);
+	sso_cfg.s.rwen = 0;
+	cvmx_write_csr(CVMX_SSO_CFG, sso_cfg.u64);
+
+	while (rwq_bufs) {
+		union cvmx_sso_rwq_psh_fptr fptr;
+		void *mem;
+
+		mem = kmem_cache_alloc(cvm_oct_kmem_sso, GFP_KERNEL);
+		if (mem == NULL) {
+			pr_err("cannot allocate memory from octeon_ethernet_sso\n");
+			return -ENOMEM;
+		}
+		for (;;) {
+			fptr.u64 = cvmx_read_csr(CVMX_SSO_RWQ_PSH_FPTR);
+			if (!fptr.s.full)
+				break;
+			__delay(1000);
+		}
+		fptr.s.fptr = virt_to_phys(mem) >> 7;
+		cvmx_write_csr(CVMX_SSO_RWQ_PSH_FPTR, fptr.u64);
+		rwq_bufs--;
+	}
+	for (i = 0; i < 8; i++) {
+		union cvmx_sso_rwq_head_ptrx head_ptr;
+		union cvmx_sso_rwq_tail_ptrx tail_ptr;
+		void *mem;
+
+		mem = kmem_cache_alloc(cvm_oct_kmem_sso, GFP_KERNEL);
+		if (mem == NULL) {
+			pr_err("cannot allocate memory from octeon_ethernet_sso\n");
+			return -ENOMEM;
+		}
+
+		head_ptr.u64 = 0;
+		tail_ptr.u64 = 0;
+		head_ptr.s.ptr = virt_to_phys(mem) >> 7;
+		tail_ptr.s.ptr = head_ptr.s.ptr;
+		cvmx_write_csr(CVMX_SSO_RWQ_HEAD_PTRX(i), head_ptr.u64);
+		cvmx_write_csr(CVMX_SSO_RWQ_TAIL_PTRX(i), tail_ptr.u64);
+	}
+	/* Now enable the SS0  RWI/RWO */
+	sso_cfg.u64 = cvmx_read_csr(CVMX_SSO_CFG);
+	sso_cfg.s.rwen = 1;
+	sso_cfg.s.rwq_byp_dis = 0;
+	sso_cfg.s.rwio_byp_dis = 0;
+	cvmx_write_csr(CVMX_SSO_CFG, sso_cfg.u64);
+
+	return 0;
+}
+
+void cvm_oct_rx_initialize(int num_wqe)
+{
+	int i;
+	struct net_device *dev_for_napi = NULL;
+
+	if (list_empty(&cvm_oct_list))
+		panic("No net_devices were allocated.");
+
+#ifdef CONFIG_SMP
+	cvm_oct_enable_one_message = octeon_request_ipi_handler(cvm_oct_enable_napi);
+	if (cvm_oct_enable_one_message < 0)
+		panic("cvm_oct_rx_initialize: No IPI handler handles available\n");
+#endif
+
+	if (OCTEON_IS_MODEL(OCTEON_CN68XX))
+		cvm_oct_napi_poll = cvm_oct_napi_poll_68;
+	else
+		cvm_oct_napi_poll = cvm_oct_napi_poll_38;
+
+	dev_for_napi = list_first_entry(&cvm_oct_list,
+					struct octeon_ethernet,
+					list)->netdev;
+
+	if (max_rx_cpus >= 1  && max_rx_cpus < num_online_cpus())
+		core_state.baseline_cores = max_rx_cpus;
+	else
+		core_state.baseline_cores = num_online_cpus();
+
+	for_each_possible_cpu(i) {
+		cvm_oct_napi[i].available = 1;
+		netif_napi_add(dev_for_napi, &cvm_oct_napi[i].napi,
+			       cvm_oct_napi_poll, rx_napi_weight);
+		napi_enable(&cvm_oct_napi[i].napi);
+	}
+	/* Before interrupts are enabled, no RX processing will occur,
+	 * so we can initialize all those things out side of the
+	 * lock.
+	 */
+	spin_lock_init(&core_state.lock);
+
+	/* Register an IRQ hander for to receive POW interrupts */
+	i = request_irq(OCTEON_IRQ_WORKQ0 + pow_receive_group,
+			cvm_oct_do_interrupt, 0, dev_for_napi->name, &cvm_oct_list);
+
+	if (i)
+		panic("Could not acquire Ethernet IRQ %d\n",
+		      OCTEON_IRQ_WORKQ0 + pow_receive_group);
+
+	if (cvm_oct_sso_initialize(num_wqe))
+		goto err;
+
+	/* Scheduld NAPI now.  This will indirectly enable interrupts. */
+	preempt_disable();
+	cvm_oct_enable_one_cpu();
+	preempt_enable();
+	return;
+err:
+	free_irq(OCTEON_IRQ_WORKQ0 + pow_receive_group, &cvm_oct_list);
+	return;
+}
+
+void cvm_oct_rx_shutdown0(void)
+{
+	int i;
+
+	/* Disable POW/SSO interrupt */
+	if (OCTEON_IS_MODEL(OCTEON_CN68XX))
+		cvmx_write_csr(CVMX_SSO_WQ_INT_THRX(pow_receive_group), 0);
+	else
+		cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group), 0);
+
+	/* Free the interrupt handler */
+	free_irq(OCTEON_IRQ_WORKQ0 + pow_receive_group, &cvm_oct_list);
+
+#ifdef CONFIG_SMP
+	octeon_release_ipi_handler(cvm_oct_enable_one_message);
+#endif
+
+	/* Shutdown all of the NAPIs */
+	for_each_possible_cpu(i)
+		netif_napi_del(&cvm_oct_napi[i].napi);
+}
+
+void cvm_oct_rx_shutdown1(void)
+{
+	union cvmx_fpa_quex_available queue_available;
+	union cvmx_sso_cfg sso_cfg;
+	union cvmx_sso_rwq_pop_fptr pop_fptr;
+	union cvmx_sso_rwq_psh_fptr fptr;
+	union cvmx_sso_fpage_cnt fpage_cnt;
+	int num_to_transfer, count, i;
+	void *mem;
+	const int sso_fpe_bit = 45;
+
+	if (!OCTEON_IS_MODEL(OCTEON_CN68XX))
+		return;
+
+	/* Spurious FPE errors will happen doing this cleanup.
+	 * Disable the indication.
+	 */
+	octeon_hw_status_disable(CVMX_SSO_ERR, 1ull << sso_fpe_bit);
+
+	sso_cfg.u64 = cvmx_read_csr(CVMX_SSO_CFG);
+	sso_cfg.s.rwen = 0;
+	sso_cfg.s.rwq_byp_dis = 1;
+	cvmx_write_csr(CVMX_SSO_CFG, sso_cfg.u64);
+	cvmx_read_csr(CVMX_SSO_CFG);
+	queue_available.u64 = cvmx_read_csr(CVMX_FPA_QUEX_AVAILABLE(8));
+
+	/* Make CVMX_FPA_QUEX_AVAILABLE(8) % 16 == 0*/
+	for (num_to_transfer = (16 - queue_available.s.que_siz) % 16;
+	     num_to_transfer > 0; num_to_transfer--) {
+		do {
+			pop_fptr.u64 = cvmx_read_csr(CVMX_SSO_RWQ_POP_FPTR);
+		} while (!pop_fptr.s.val);
+		for (;;) {
+			fptr.u64 = cvmx_read_csr(CVMX_SSO_RWQ_PSH_FPTR);
+			if (!fptr.s.full)
+				break;
+			__delay(1000);
+		}
+		fptr.s.fptr = pop_fptr.s.fptr;
+		cvmx_write_csr(CVMX_SSO_RWQ_PSH_FPTR, fptr.u64);
+	}
+	cvmx_read_csr(CVMX_SSO_CFG);
+
+	do {
+		queue_available.u64 = cvmx_read_csr(CVMX_FPA_QUEX_AVAILABLE(8));
+	} while (queue_available.s.que_siz % 16);
+
+	sso_cfg.s.rwen = 1;
+	sso_cfg.s.rwq_byp_dis = 0;
+	cvmx_write_csr(CVMX_SSO_CFG, sso_cfg.u64);
+
+	for (i = 0; i < 8; i++) {
+		union cvmx_sso_rwq_head_ptrx head_ptr;
+		union cvmx_sso_rwq_tail_ptrx tail_ptr;
+
+		head_ptr.u64 = cvmx_read_csr(CVMX_SSO_RWQ_HEAD_PTRX(i));
+		tail_ptr.u64 = cvmx_read_csr(CVMX_SSO_RWQ_TAIL_PTRX(i));
+		WARN_ON(head_ptr.s.ptr != tail_ptr.s.ptr);
+
+		mem = phys_to_virt(((u64)head_ptr.s.ptr) << 7);
+		kmem_cache_free(cvm_oct_kmem_sso, mem);
+	}
+
+	count = 0;
+
+	do {
+		do {
+			pop_fptr.u64 = cvmx_read_csr(CVMX_SSO_RWQ_POP_FPTR);
+			if (pop_fptr.s.val) {
+				mem = phys_to_virt(((u64)pop_fptr.s.fptr) << 7);
+				kmem_cache_free(cvm_oct_kmem_sso, mem);
+				count++;
+			}
+		} while (pop_fptr.s.val);
+		fpage_cnt.u64 = cvmx_read_csr(CVMX_SSO_FPAGE_CNT);
+	} while (fpage_cnt.s.fpage_cnt);
+
+	WARN_ON(count != cvm_oct_sso_fptr_count);
+
+	sso_cfg.s.rwen = 0;
+	sso_cfg.s.rwq_byp_dis = 0;
+	cvmx_write_csr(CVMX_SSO_CFG, sso_cfg.u64);
+	kmem_cache_destroy(cvm_oct_kmem_sso);
+	cvm_oct_kmem_sso = NULL;
+
+	/* Clear any FPE indicators, and reenable. */
+	cvmx_write_csr(CVMX_SSO_ERR, 1ull << sso_fpe_bit);
+	octeon_hw_status_enable(CVMX_SSO_ERR, 1ull << sso_fpe_bit);
+}
diff --git a/drivers/net/ethernet/octeon/ethernet-sgmii.c b/drivers/net/ethernet/octeon/ethernet-sgmii.c
new file mode 100644
index 0000000..c214703
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-sgmii.c
@@ -0,0 +1,231 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+**********************************************************************/
+#include <linux/phy.h>
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/ratelimit.h>
+#include <net/dst.h>
+
+#include <asm/octeon/octeon.h>
+#include <asm/octeon/octeon-hw-status.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+
+#include <asm/octeon/cvmx-helper.h>
+
+#include <asm/octeon/cvmx-gmxx-defs.h>
+#include <asm/octeon/cvmx-npei-defs.h>
+
+#define INT_BIT_LOC_FAULT 20
+#define INT_BIT_REM_FAULT 21
+
+/* Although these functions are called cvm_oct_sgmii_*, they also
+ * happen to be used for the XAUI ports as well.
+ */
+
+static void cvm_oct_sgmii_poll(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	cvmx_helper_link_info_t link_info;
+
+	link_info = cvmx_helper_link_get(priv->ipd_port);
+	if (link_info.u64 == priv->link_info)
+		return;
+
+	link_info = cvmx_helper_link_autoconf(priv->ipd_port);
+	priv->link_info = link_info.u64;
+
+	/* Tell the core */
+	cvm_oct_set_carrier(priv, link_info);
+}
+static int cvm_oct_sgmii_hw_status(struct notifier_block *nb,
+				   unsigned long val, void *v)
+{
+	struct octeon_ethernet *priv = container_of(nb, struct octeon_ethernet,
+						    hw_status_notifier);
+
+	if (val == OCTEON_HW_STATUS_SOURCE_ASSERTED) {
+		struct octeon_hw_status_data *d = v;
+		if (d->reg == CVMX_GMXX_RXX_INT_REG(priv->interface_port,
+						    priv->interface) &&
+		    (d->bit == INT_BIT_LOC_FAULT ||
+		     d->bit == INT_BIT_REM_FAULT)) {
+			cvmx_helper_link_autoconf(priv->ipd_port);
+			return NOTIFY_STOP;
+		}
+	}
+	return NOTIFY_DONE;
+}
+
+int cvm_oct_sgmii_open(struct net_device *dev)
+{
+	struct octeon_hw_status_reg sr[3];
+	union cvmx_gmxx_prtx_cfg gmx_cfg;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	cvmx_helper_link_info_t link_info;
+	cvmx_helper_interface_mode_t imode;
+	int rv, i;
+	u64 en_mask;
+
+	rv = cvm_oct_phy_setup_device(dev);
+	if (rv)
+		return rv;
+
+	gmx_cfg.u64 = cvmx_read_csr(priv->gmx_base + GMX_PRT_CFG);
+	gmx_cfg.s.en = 1;
+	cvmx_write_csr(priv->gmx_base + GMX_PRT_CFG, gmx_cfg.u64);
+
+	if (octeon_is_simulation())
+		return 0;
+
+	if (priv->phydev) {
+		int r = phy_read_status(priv->phydev);
+		if (r == 0 && priv->phydev->link == 0)
+			netif_carrier_off(dev);
+		cvm_oct_adjust_link(dev);
+	} else {
+		link_info = cvmx_helper_link_get(priv->ipd_port);
+		if (!link_info.s.link_up)
+			netif_carrier_off(dev);
+		priv->poll = cvm_oct_sgmii_poll;
+		cvm_oct_sgmii_poll(dev);
+	}
+	imode = cvmx_helper_interface_get_mode(priv->interface);
+	switch (imode) {
+	case CVMX_HELPER_INTERFACE_MODE_XAUI:
+	case CVMX_HELPER_INTERFACE_MODE_RXAUI:
+		/* Handle GMXX_RXX_INT_REG[LOC_FAULT,REM_FAULT]*/
+		priv->hw_status_notifier.priority = 10;
+		priv->hw_status_notifier.notifier_call = cvm_oct_sgmii_hw_status;
+		octeon_hw_status_notifier_register(&priv->hw_status_notifier);
+		memset(sr, 0, sizeof(sr));
+		i = 0;
+		en_mask = 0;
+		if (OCTEON_IS_OCTEONPLUS()) {
+			sr[i].reg = 46; /* RML */
+			sr[i].reg_is_hwint = 1;
+			sr[i].has_child = 1;
+			i++;
+			sr[i].reg = CVMX_NPEI_RSL_INT_BLOCKS;
+			/* GMX[priv->interface]*/
+			sr[i].bit = priv->interface + 1;
+			sr[i].has_child = 1;
+			i++;
+		} else if (octeon_has_feature(OCTEON_FEATURE_CIU2)) {
+			/* PKT[AGX[priv->interface]]*/
+			sr[i].reg = (6 << 6) | priv->interface;
+			sr[i].reg_is_hwint = 1;
+			sr[i].has_child = 1;
+			i++;
+		} else {
+			/* INT_SUM1[AGX[priv->interface]]*/
+			sr[i].reg = (1 << 6) | (priv->interface + 36);
+			sr[i].reg_is_hwint = 1;
+			sr[i].has_child = 1;
+			i++;
+		}
+		sr[i].reg = CVMX_GMXX_RXX_INT_REG(priv->interface_port,
+						  priv->interface);
+		sr[i].mask_reg = CVMX_GMXX_RXX_INT_EN(priv->interface_port,
+						      priv->interface);
+		sr[i].ack_w1c = 1;
+
+		sr[i].bit = INT_BIT_LOC_FAULT;
+		en_mask |= 1ull << sr[i].bit;
+		octeon_hw_status_add_source(sr);
+
+		sr[i].bit = INT_BIT_REM_FAULT;
+		en_mask |= 1ull << sr[i].bit;
+		octeon_hw_status_add_source(sr);
+
+		octeon_hw_status_enable(sr[i].reg, en_mask);
+		break;
+	default:
+		break;
+	}
+	return 0;
+}
+
+int cvm_oct_sgmii_stop(struct net_device *dev)
+{
+	union cvmx_gmxx_prtx_cfg gmx_cfg;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	gmx_cfg.u64 = cvmx_read_csr(priv->gmx_base + GMX_PRT_CFG);
+	gmx_cfg.s.en = 0;
+	cvmx_write_csr(priv->gmx_base + GMX_PRT_CFG, gmx_cfg.u64);
+
+	if (priv->hw_status_notifier.notifier_call) {
+		struct octeon_hw_status_reg sr;
+		memset(&sr, 0, sizeof(sr));
+
+		sr.reg = CVMX_GMXX_RXX_INT_REG(priv->interface_port,
+					       priv->interface);
+		sr.mask_reg = CVMX_GMXX_RXX_INT_EN(priv->interface_port,
+						   priv->interface);
+		sr.ack_w1c = 1;
+		sr.bit = INT_BIT_LOC_FAULT;
+		octeon_hw_status_remove_source(&sr);
+		sr.bit = INT_BIT_REM_FAULT;
+		octeon_hw_status_remove_source(&sr);
+		octeon_hw_status_notifier_unregister(&priv->hw_status_notifier);
+		priv->hw_status_notifier.notifier_call = NULL;
+	}
+
+	return cvm_oct_common_stop(dev);
+}
+
+static void cvm_oct_sgmii_link_change(struct octeon_ethernet *priv,
+				      cvmx_helper_link_info_t link_info)
+{
+	if (link_info.s.link_up)
+		octeon_error_tree_enable(CVMX_ERROR_GROUP_ETHERNET,
+					 priv->ipd_port);
+	else
+		octeon_error_tree_disable(CVMX_ERROR_GROUP_ETHERNET,
+					  priv->ipd_port);
+}
+
+int cvm_oct_sgmii_init(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	cvm_oct_common_init(dev);
+	dev->netdev_ops->ndo_stop(dev);
+	priv->link_change = cvm_oct_sgmii_link_change;
+
+	return 0;
+}
+
+void cvm_oct_sgmii_uninit(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	octeon_error_tree_disable(CVMX_ERROR_GROUP_ETHERNET,
+				  priv->ipd_port);
+}
diff --git a/drivers/net/ethernet/octeon/ethernet-spi.c b/drivers/net/ethernet/octeon/ethernet-spi.c
new file mode 100644
index 0000000..64e30b2
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-spi.c
@@ -0,0 +1,239 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/interrupt.h>
+#include <net/dst.h>
+
+#include <asm/octeon/octeon-hw-status.h>
+#include <asm/octeon/octeon.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+
+#include <asm/octeon/cvmx-spi.h>
+
+#include <asm/octeon/cvmx-npi-defs.h>
+#include <asm/octeon/cvmx-spxx-defs.h>
+#include <asm/octeon/cvmx-stxx-defs.h>
+
+#define MASK_FOR_BIT(_X) (1ull << (_X))
+
+#define INT_BIT_SPX_PRTNXA 0
+#define INT_BIT_SPX_ABNORM 1
+#define INT_BIT_SPX_SPIOVR 4
+#define INT_BIT_SPX_CLSERR 5
+#define INT_BIT_SPX_DRWNNG 6
+#define INT_BIT_SPX_RSVERR 7
+#define INT_BIT_SPX_TPAOVR 8
+#define INT_BIT_SPX_DIPERR 9
+#define INT_BIT_SPX_SYNCERR 10
+#define INT_BIT_SPX_CALERR 11
+
+#define SPX_MASK (				\
+	MASK_FOR_BIT(INT_BIT_SPX_PRTNXA) |	\
+	MASK_FOR_BIT(INT_BIT_SPX_ABNORM) |	\
+	MASK_FOR_BIT(INT_BIT_SPX_SPIOVR) |	\
+	MASK_FOR_BIT(INT_BIT_SPX_CLSERR) |	\
+	MASK_FOR_BIT(INT_BIT_SPX_DRWNNG) |	\
+	MASK_FOR_BIT(INT_BIT_SPX_RSVERR) |	\
+	MASK_FOR_BIT(INT_BIT_SPX_TPAOVR) |	\
+	MASK_FOR_BIT(INT_BIT_SPX_DIPERR) |	\
+	MASK_FOR_BIT(INT_BIT_SPX_CALERR))
+
+#define INT_BIT_SPX_SPF 31
+
+#define INT_BIT_STX_CALPAR0 0
+#define INT_BIT_STX_CALPAR1 1
+#define INT_BIT_STX_OVRBST 2
+#define INT_BIT_STX_DATOVR 3
+#define INT_BIT_STX_DIPERR 4
+#define INT_BIT_STX_NOSYNC 5
+#define INT_BIT_STX_UNXFRM 6
+#define INT_BIT_STX_FRMERR 7
+#define INT_BIT_STX_SYNCERR 8
+#define STX_MASK (					\
+		MASK_FOR_BIT(INT_BIT_STX_CALPAR0) |	\
+		MASK_FOR_BIT(INT_BIT_STX_CALPAR1) |	\
+		MASK_FOR_BIT(INT_BIT_STX_OVRBST) |	\
+		MASK_FOR_BIT(INT_BIT_STX_DATOVR) |	\
+		MASK_FOR_BIT(INT_BIT_STX_DIPERR) |	\
+		MASK_FOR_BIT(INT_BIT_STX_NOSYNC) |	\
+		MASK_FOR_BIT(INT_BIT_STX_UNXFRM) |	\
+		MASK_FOR_BIT(INT_BIT_STX_FRMERR) |	\
+		MASK_FOR_BIT(INT_BIT_STX_SYNCERR))
+
+static int need_retrain[2] = { 0, 0 };
+
+static int cvm_oct_spi_hw_status(struct notifier_block *nb, unsigned long val, void *v)
+{
+	struct octeon_ethernet *priv = container_of(nb, struct octeon_ethernet, hw_status_notifier);
+
+	if (val == OCTEON_HW_STATUS_SOURCE_ASSERTED) {
+		struct octeon_hw_status_data *d = v;
+		if (d->reg == CVMX_SPXX_INT_REG(priv->interface) ||
+		    d->reg == CVMX_STXX_INT_REG(priv->interface)) {
+			if (need_retrain[priv->interface])
+				return NOTIFY_STOP;
+			need_retrain[priv->interface] = 1;
+			octeon_hw_status_disable(CVMX_SPXX_INT_REG(priv->interface), SPX_MASK);
+			octeon_hw_status_disable(CVMX_STXX_INT_REG(priv->interface), STX_MASK);
+		}
+	}
+	return NOTIFY_DONE;
+}
+static void cvm_oct_spi_poll(struct net_device *dev)
+{
+	static int spi4000_port;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	int interface;
+
+	for (interface = 0; interface < 2; interface++) {
+		if ((priv->ipd_port == interface * 16) && need_retrain[interface]) {
+			if (cvmx_spi_restart_interface(interface, CVMX_SPI_MODE_DUPLEX, 10) == 0) {
+				need_retrain[interface] = 0;
+				octeon_hw_status_enable(CVMX_SPXX_INT_REG(priv->interface), SPX_MASK);
+				octeon_hw_status_enable(CVMX_STXX_INT_REG(priv->interface), STX_MASK);
+			}
+		}
+
+		/* The SPI4000 TWSI interface is very slow. In order
+		 * not to bring the system to a crawl, we only poll a
+		 * single port every second. This means negotiation
+		 * speed changes take up to 10 seconds, but at least
+		 * we don't waste absurd amounts of time waiting for
+		 * TWSI.
+		 */
+		if (priv->ipd_port == spi4000_port) {
+			/* This function does nothing if it is called on an
+			 * interface without a SPI4000.
+			 */
+			cvmx_spi4000_check_speed(interface, priv->ipd_port);
+			/* Normal ordering increments. By decrementing
+			 * we only match once per iteration.
+			 */
+			spi4000_port--;
+			if (spi4000_port < 0)
+				spi4000_port = 10;
+		}
+	}
+}
+
+int cvm_oct_spi_init(struct net_device *dev)
+{
+	int r, i;
+	struct octeon_hw_status_reg sr[3];
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	if ((priv->ipd_port == 0) || (priv->ipd_port == 16)) {
+
+		priv->hw_status_notifier.priority = 10;
+		priv->hw_status_notifier.notifier_call = cvm_oct_spi_hw_status;
+		r = octeon_hw_status_notifier_register(&priv->hw_status_notifier);
+		if (r)
+			return r;
+
+		memset(sr, 0, sizeof(sr));
+		sr[0].reg = 46; /* RML */
+		sr[0].reg_is_hwint = 1;
+		sr[0].has_child = 1;
+
+		sr[1].reg = CVMX_NPI_RSL_INT_BLOCKS;
+		sr[1].bit = priv->interface + 18;
+		sr[1].has_child = 1;
+
+		sr[2].reg = CVMX_SPXX_INT_REG(priv->interface);
+		sr[2].mask_reg = CVMX_SPXX_INT_MSK(priv->interface);
+		sr[2].ack_w1c = 1;
+
+		for (i = 0; i < 32; i++) {
+			if ((1ull << i) & SPX_MASK) {
+				sr[2].bit = i;
+				octeon_hw_status_add_source(sr);
+			}
+		}
+
+		sr[2].bit = INT_BIT_SPX_SPF;
+		sr[2].ack_w1c = 0;
+		octeon_hw_status_add_source(sr);
+		octeon_hw_status_enable(sr[2].reg, SPX_MASK);
+
+		sr[2].reg = CVMX_STXX_INT_REG(priv->interface);
+		sr[2].mask_reg = CVMX_STXX_INT_MSK(priv->interface);
+		sr[2].ack_w1c = 1;
+
+		for (i = 0; i < 32; i++) {
+			if ((1ull << i) & STX_MASK) {
+				sr[2].bit = i;
+				octeon_hw_status_add_source(sr);
+			}
+		}
+		octeon_hw_status_enable(sr[2].reg, STX_MASK);
+
+		priv->poll = cvm_oct_spi_poll;
+	}
+	cvm_oct_common_init(dev);
+	return 0;
+}
+
+void cvm_oct_spi_uninit(struct net_device *dev)
+{
+	int i;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	if (priv->hw_status_notifier.notifier_call) {
+		struct octeon_hw_status_reg sr;
+		memset(&sr, 0, sizeof(sr));
+
+		sr.reg = CVMX_SPXX_INT_REG(priv->interface);
+		sr.mask_reg = CVMX_SPXX_INT_MSK(priv->interface);
+		sr.ack_w1c = 1;
+
+		for (i = 0; i < 32; i++) {
+			if ((1ull << i) & SPX_MASK) {
+				sr.bit = i;
+				octeon_hw_status_remove_source(&sr);
+			}
+		}
+		sr.bit = INT_BIT_SPX_SPF;
+		sr.ack_w1c = 0;
+		octeon_hw_status_remove_source(&sr);
+
+		sr.reg = CVMX_STXX_INT_REG(priv->interface);
+		sr.mask_reg = CVMX_STXX_INT_MSK(priv->interface);
+		sr.ack_w1c = 1;
+		for (i = 0; i < 32; i++) {
+			if ((1ull << i) & STX_MASK) {
+				sr.bit = i;
+				octeon_hw_status_remove_source(&sr);
+			}
+		}
+
+		octeon_hw_status_notifier_unregister(&priv->hw_status_notifier);
+		priv->hw_status_notifier.notifier_call = NULL;
+	}
+}
diff --git a/drivers/net/ethernet/octeon/ethernet-tx.c b/drivers/net/ethernet/octeon/ethernet-tx.c
new file mode 100644
index 0000000..26f9340
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-tx.c
@@ -0,0 +1,229 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/skbuff.h>
+#include <linux/init.h>
+#include <linux/etherdevice.h>
+#include <linux/ip.h>
+#include <linux/ratelimit.h>
+#include <linux/string.h>
+#include <linux/interrupt.h>
+#include <net/dst.h>
+
+#include <asm/octeon/octeon.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+
+#include <asm/octeon/cvmx-wqe.h>
+#include <asm/octeon/cvmx-hwfau.h>
+#include <asm/octeon/cvmx-ipd.h>
+#include <asm/octeon/cvmx-pip.h>
+#include <asm/octeon/cvmx-hwpko.h>
+#include <asm/octeon/cvmx-helper.h>
+
+#include <asm/octeon/cvmx-gmxx-defs.h>
+
+/*
+ * You can define GET_SKBUFF_QOS() to override how the skbuff output
+ * function determines which output queue is used. The default
+ * implementation always uses the base queue for the port. If, for
+ * example, you wanted to use the skb->priority fieid, define
+ * GET_SKBUFF_QOS as: #define GET_SKBUFF_QOS(skb) ((skb)->priority)
+ */
+#ifndef GET_SKBUFF_QOS
+#define GET_SKBUFF_QOS(skb) 0
+#endif
+
+static bool cvm_oct_skb_ok_for_reuse(struct sk_buff *skb)
+{
+	unsigned char *fpa_head = cvm_oct_get_fpa_head(skb);
+
+	if (unlikely(skb->data < fpa_head))
+		return false;
+
+	if (unlikely(fpa_head - skb->head < sizeof(void *)))
+		return false;
+
+	if (unlikely((skb_end_pointer(skb) - fpa_head) < FPA_PACKET_POOL_SIZE))
+		return false;
+
+	if (unlikely(skb_shared(skb)) ||
+	    unlikely(skb_cloned(skb)) ||
+	    unlikely(skb->fclone != SKB_FCLONE_UNAVAILABLE))
+		return false;
+
+	return true;
+}
+
+static void skb_recycle(struct sk_buff *skb)
+{
+	struct skb_shared_info *shinfo;
+
+	skb_release_head_state(skb);
+
+	shinfo = skb_shinfo(skb);
+	memset(shinfo, 0, offsetof(struct skb_shared_info, dataref));
+	atomic_set(&shinfo->dataref, 1);
+
+	memset(skb, 0, offsetof(struct sk_buff, tail));
+	skb->data = skb->head + NET_SKB_PAD;
+	skb_reset_tail_pointer(skb);
+}
+
+static void cvm_oct_skb_prepare_for_reuse(struct sk_buff *skb)
+{
+	unsigned char *fpa_head = cvm_oct_get_fpa_head(skb);
+
+	skb->data_len = 0;
+	skb_frag_list_init(skb);
+
+	/* The check also resets all the fields. */
+	skb_recycle(skb);
+
+	*(struct sk_buff **)(fpa_head - sizeof(void *)) = skb;
+	skb->truesize = sizeof(*skb) + skb_end_pointer(skb) - skb->head;
+}
+
+static inline void cvm_oct_set_back(struct sk_buff *skb,
+				    union cvmx_buf_ptr *hw_buffer)
+{
+	unsigned char *fpa_head = cvm_oct_get_fpa_head(skb);
+
+	hw_buffer->s.back = ((unsigned long)skb->data >> 7) - ((unsigned long)fpa_head >> 7);
+}
+
+#define CVM_OCT_LOCKLESS 1
+#include "ethernet-xmit.c"
+
+#undef CVM_OCT_LOCKLESS
+#include "ethernet-xmit.c"
+
+/**
+ * cvm_oct_transmit_qos - transmit a work queue entry out of the ethernet port.
+ *
+ * Both the work queue entry and the packet data can optionally be
+ * freed. The work will be freed on error as well.
+ *
+ * @dev: Device to transmit out.
+ * @work_queue_entry: Work queue entry to send
+ * @do_free: True if the work queue entry and packet data should be
+ *           freed. If false, neither will be freed.
+ * @qos: Index into the queues for this port to transmit on. This is
+ *       used to implement QoS if their are multiple queues per
+ *       port. This parameter must be between 0 and the number of
+ *       queues per port minus 1. Values outside of this range will be
+ *       change to zero.
+ *
+ * Returns Zero on success, negative on failure.
+ */
+int cvm_oct_transmit_qos(struct net_device *dev,
+			 void *work_queue_entry,
+			 int do_free,
+			 int qos)
+{
+	unsigned long			flags;
+	cvmx_buf_ptr_t			hw_buffer;
+	cvmx_pko_command_word0_t	pko_command;
+	int				dropped;
+	struct octeon_ethernet		*priv = netdev_priv(dev);
+	cvmx_wqe_t			*work = work_queue_entry;
+	cvmx_pko_lock_t lock_type;
+
+	if (!(dev->flags & IFF_UP)) {
+		netdev_err(dev, "Error: Device not up\n");
+		if (do_free)
+			cvm_oct_free_work(work);
+		return -1;
+	}
+
+	if (priv->tx_lockless) {
+		qos = cvmx_get_core_num();
+		lock_type = CVMX_PKO_LOCK_NONE;
+	} else {
+		/*
+		 * The check on CVMX_PKO_QUEUES_PER_PORT_* is designed to
+		 * completely remove "qos" in the event neither interface
+		 * supports multiple queues per port
+		 */
+		if (priv->tx_multiple_queues) {
+			if (qos <= 0)
+				qos = 0;
+			else if (qos >= priv->num_tx_queues)
+				qos = 0;
+		} else
+			qos = 0;
+		lock_type = CVMX_PKO_LOCK_CMD_QUEUE;
+	}
+
+	/* Start off assuming no drop */
+	dropped = 0;
+
+	local_irq_save(flags);
+
+	cvmx_pko_send_packet_prepare_pkoid(priv->pko_port, priv->tx_queue[qos].queue, lock_type);
+
+	/* Build the PKO buffer pointer */
+	hw_buffer.u64 = 0;
+	hw_buffer.s.addr = work->packet_ptr.s.addr;
+	hw_buffer.s.pool = packet_pool;
+	hw_buffer.s.size = FPA_PACKET_POOL_SIZE;
+	hw_buffer.s.back = work->packet_ptr.s.back;
+
+	/* Build the PKO command */
+	pko_command.u64 = 0;
+	pko_command.s.n2 = 1; /* Don't pollute L2 with the outgoing packet */
+	pko_command.s.dontfree = !do_free;
+	pko_command.s.segs = work->word2.s.bufs;
+	pko_command.s.total_bytes = work->word1.len;
+
+	/* Check if we can use the hardware checksumming */
+	if (unlikely(work->word2.s.not_IP || work->word2.s.IP_exc))
+		pko_command.s.ipoffp1 = 0;
+	else
+		pko_command.s.ipoffp1 = sizeof(struct ethhdr) + 1;
+
+	/* Send the packet to the output queue */
+	if (unlikely(cvmx_hwpko_send_packet_finish_pkoid(priv->pko_port, priv->tx_queue[qos].queue, pko_command, hw_buffer, lock_type))) {
+		netdev_err(dev, "Error: Failed to send the packet\n");
+		dropped = -1;
+	}
+	local_irq_restore(flags);
+
+	if (unlikely(dropped)) {
+		if (do_free)
+			cvm_oct_free_work(work);
+		dev->stats.tx_dropped++;
+	} else
+	if (do_free)
+		cvmx_fpa1_free(work, wqe_pool, DONT_WRITEBACK(1));
+
+	return dropped;
+}
+EXPORT_SYMBOL(cvm_oct_transmit_qos);
diff --git a/drivers/net/ethernet/octeon/ethernet-xmit.c b/drivers/net/ethernet/octeon/ethernet-xmit.c
new file mode 100644
index 0000000..397bef7
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet-xmit.c
@@ -0,0 +1,393 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+
+#undef CVM_OCT_XMIT
+#undef CVM_OCT_PKO_LOCK_TYPE
+
+#ifdef CVM_OCT_LOCKLESS
+#define CVM_OCT_XMIT cvm_oct_xmit_lockless
+#define CVM_OCT_PKO_LOCK_TYPE CVMX_PKO_LOCK_NONE
+#else
+#define CVM_OCT_XMIT cvm_oct_xmit
+#define CVM_OCT_PKO_LOCK_TYPE CVMX_PKO_LOCK_CMD_QUEUE
+#endif
+
+/**
+ * cvm_oct_xmit - transmit a packet
+ * @skb:    Packet to send
+ * @dev:    Device info structure
+ *
+ * Returns Always returns NETDEV_TX_OK
+ */
+int
+CVM_OCT_XMIT
+(struct sk_buff *skb, struct net_device *dev)
+{
+	struct sk_buff *skb_tmp;
+	cvmx_pko_command_word0_t pko_command;
+	union cvmx_buf_ptr hw_buffer;
+	u64 old_scratch;
+	u64 old_scratch2;
+	int qos;
+	int i;
+	int frag_count;
+	enum {QUEUE_HW, QUEUE_WQE, QUEUE_DROP, QUEUE_DROP_NO_DEC} queue_type;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	s32 queue_depth;
+	s32 buffers_to_free;
+	s32 buffers_being_recycled;
+	unsigned long flags;
+	cvmx_wqe_t *work = NULL;
+	bool timestamp_this_skb = false;
+	bool can_do_reuse = true;
+
+	/* Prefetch the private data structure.  It is larger than one
+	 * cache line.
+	 */
+	prefetch(priv);
+
+	if (USE_ASYNC_IOBDMA) {
+		/* Save scratch in case userspace is using it */
+		CVMX_SYNCIOBDMA;
+		old_scratch = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
+		old_scratch2 = cvmx_scratch_read64(CVMX_SCR_SCRATCH + 8);
+
+		/* Fetch and increment the number of packets to be
+		 * freed.
+		 */
+		cvmx_hwfau_async_fetch_and_add32(CVMX_SCR_SCRATCH + 8,
+					       FAU_NUM_PACKET_BUFFERS_TO_FREE,
+					       0);
+	}
+
+	frag_count = 0;
+	if (skb_has_frag_list(skb))
+		skb_walk_frags(skb, skb_tmp)
+			frag_count++;
+	/* We have space for 12 segment pointers, If there will be
+	 * more than that, we must linearize.  The count is: 1 (base
+	 * SKB) + frag_count + nr_frags.
+	 */
+	if (unlikely(skb_shinfo(skb)->nr_frags + frag_count > 11)) {
+		if (unlikely(__skb_linearize(skb))) {
+			dev_kfree_skb_any(skb);
+			dev->stats.tx_dropped++;
+			goto post_preempt_out;
+		}
+		frag_count = 0;
+	}
+
+	/* We cannot move to a different CPU once we determine our
+	 * queue number/qos
+	 */
+	preempt_disable();
+#ifdef CVM_OCT_LOCKLESS
+	qos = cvmx_get_core_num();
+#else
+	/* The check on CVMX_PKO_QUEUES_PER_PORT_* is designed to
+	 * completely remove "qos" in the event neither interface
+	 * supports multiple queues per port.
+	 */
+	if (priv->tx_multiple_queues) {
+		qos = GET_SKBUFF_QOS(skb);
+		if (qos <= 0)
+			qos = 0;
+		else if (qos >= priv->num_tx_queues)
+			qos = 0;
+	} else
+		qos = 0;
+#endif
+	if (USE_ASYNC_IOBDMA) {
+		cvmx_hwfau_async_fetch_and_add32(CVMX_SCR_SCRATCH,
+					       priv->tx_queue[qos].fau, 1);
+	}
+
+#ifndef CVM_OCT_LOCKLESS
+	/* The CN3XXX series of parts has an errata (GMX-401) which
+	 * causes the GMX block to hang if a collision occurs towards
+	 * the end of a <68 byte packet. As a workaround for this, we
+	 * pad packets to be 68 bytes whenever we are in half duplex
+	 * mode. We don't handle the case of having a small packet but
+	 * no room to add the padding.  The kernel should always give
+	 * us at least a cache line
+	 */
+	if ((skb->len < 64) && OCTEON_IS_MODEL(OCTEON_CN3XXX)) {
+		union cvmx_gmxx_prtx_cfg gmx_prt_cfg;
+
+		if (priv->interface < 2) {
+			/* We only need to pad packet in half duplex mode */
+			gmx_prt_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(priv->interface_port, priv->interface));
+			if (gmx_prt_cfg.s.duplex == 0) {
+				int add_bytes = 64 - skb->len;
+				if ((skb_tail_pointer(skb) + add_bytes) <= skb_end_pointer(skb))
+					memset(__skb_put(skb, add_bytes), 0, add_bytes);
+			}
+		}
+	}
+#endif
+	/* Build the PKO command */
+	pko_command.u64 = 0;
+#ifdef __LITTLE_ENDIAN
+	pko_command.s.le = 1;
+#endif
+	/* Don't pollute L2 with the outgoing packet */
+	pko_command.s.n2 = 1;
+	pko_command.s.segs = 1;
+	pko_command.s.total_bytes = skb->len;
+	/* Use fau0 to decrement the number of packets queued */
+	pko_command.s.size0 = CVMX_FAU_OP_SIZE_32;
+	pko_command.s.subone0 = 1;
+	pko_command.s.reg0 = priv->tx_queue[qos].fau;
+	pko_command.s.dontfree = 1;
+
+	/* Build the PKO buffer pointer */
+	hw_buffer.u64 = 0; /* Implies pool == 0, i == 0 */
+	if (skb_shinfo(skb)->nr_frags == 0 && frag_count == 0) {
+		hw_buffer.s.addr = virt_to_phys(skb->data);
+		hw_buffer.s.size = skb->len;
+		cvm_oct_set_back(skb, &hw_buffer);
+		buffers_being_recycled = 1;
+	} else {
+		u64 *hw_buffer_list;
+
+		work = cvmx_fpa1_alloc(wqe_pool);
+		if (unlikely(!work)) {
+			netdev_err(dev, "Failed WQE allocate\n");
+			queue_type = USE_ASYNC_IOBDMA ? QUEUE_DROP : QUEUE_DROP_NO_DEC;
+			goto skip_xmit;
+		}
+		hw_buffer_list = (u64 *)work->packet_data;
+		hw_buffer.s.addr = virt_to_phys(skb->data);
+		hw_buffer.s.size = skb_headlen(skb);
+		if (skb_shinfo(skb)->nr_frags == 0 && cvm_oct_skb_ok_for_reuse(skb)) {
+			cvm_oct_set_back(skb, &hw_buffer);
+		} else {
+			hw_buffer.s.back = 0;
+			can_do_reuse = false;
+		}
+		hw_buffer_list[0] = hw_buffer.u64;
+		for (i = 1; i <= skb_shinfo(skb)->nr_frags; i++) {
+			struct skb_frag_struct *fs = skb_shinfo(skb)->frags + i - 1;
+			hw_buffer.s.addr = virt_to_phys((u8 *)page_address(fs->page.p) + fs->page_offset);
+			hw_buffer.s.size = fs->size;
+			hw_buffer_list[i] = hw_buffer.u64;
+			can_do_reuse = false;
+		}
+		skb_walk_frags(skb, skb_tmp) {
+			hw_buffer.s.addr = virt_to_phys(skb_tmp->data);
+			hw_buffer.s.size = skb_tmp->len;
+			if (cvm_oct_skb_ok_for_reuse(skb_tmp)) {
+				cvm_oct_set_back(skb_tmp, &hw_buffer);
+			} else {
+				hw_buffer.s.back = 0;
+				can_do_reuse = false;
+			}
+			hw_buffer_list[i] = hw_buffer.u64;
+			i++;
+		}
+		hw_buffer.s.addr = virt_to_phys(hw_buffer_list);
+		hw_buffer.s.size = i;
+		hw_buffer.s.back = 0;
+		hw_buffer.s.pool = wqe_pool;
+		buffers_being_recycled = i;
+		pko_command.s.segs = hw_buffer.s.size;
+		pko_command.s.gather = 1;
+	}
+
+	if (unlikely(priv->tx_timestamp_hw) &&
+	    (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP)) {
+		timestamp_this_skb = true;
+		skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+		can_do_reuse = false;
+	}
+
+	/* See if we can put this skb in the FPA pool. Any strange
+	 * behavior from the Linux networking stack will most likely
+	 * be caused by a bug in the following code. If some field is
+	 * in use by the network stack and get carried over when a
+	 * buffer is reused, bad thing may happen.  If in doubt and
+	 * you dont need the absolute best performance, set recycle_tx
+	 * to zero . The reuse of buffers has shown a 25% increase in
+	 * performance under some loads.
+	 */
+	if (octeon_recycle_tx && can_do_reuse &&
+	    cvm_oct_skb_ok_for_reuse(skb) &&
+	    likely(!skb_header_cloned(skb)) &&
+	    likely(!skb->destructor))
+		/* We can use this buffer in the FPA.  We don't need
+		 * the FAU update anymore
+		 */
+		pko_command.s.dontfree = 0;
+
+	/* Check if we can use the hardware checksumming */
+	if (USE_HW_TCPUDP_CHECKSUM && (skb->protocol == htons(ETH_P_IP)) &&
+	    (ip_hdr(skb)->version == 4) && (ip_hdr(skb)->ihl == 5) &&
+	    ((ip_hdr(skb)->frag_off == 0) || (ip_hdr(skb)->frag_off == htons(1 << 14)))
+	    && ((ip_hdr(skb)->protocol == IPPROTO_TCP) || (ip_hdr(skb)->protocol == IPPROTO_UDP))) {
+		/* Use hardware checksum calc */
+		pko_command.s.ipoffp1 = sizeof(struct ethhdr) + 1;
+		if (unlikely(priv->imode == CVMX_HELPER_INTERFACE_MODE_SRIO))
+			pko_command.s.ipoffp1 += 8;
+	}
+
+	if (USE_ASYNC_IOBDMA) {
+		/* Get the number of skbuffs in use by the hardware */
+		CVMX_SYNCIOBDMA;
+		queue_depth = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
+		buffers_to_free = cvmx_scratch_read64(CVMX_SCR_SCRATCH + 8);
+	} else {
+		/* Get the number of skbuffs in use by the hardware */
+		queue_depth = cvmx_hwfau_fetch_and_add32(priv->tx_queue[qos].fau, 1);
+		buffers_to_free = cvmx_hwfau_fetch_and_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);
+	}
+
+	/* If we're sending faster than the receive can free them then
+	 * don't do the HW free.
+	 */
+	if (unlikely(buffers_to_free < -100))
+		pko_command.s.dontfree = 1;
+
+	/* Drop this packet if we have too many already queued to the HW */
+	if (unlikely(queue_depth >= MAX_OUT_QUEUE_DEPTH)) {
+		if (dev->tx_queue_len != 0) {
+			netif_stop_queue(dev);
+		} else {
+			/* If not using normal queueing.  */
+			queue_type = QUEUE_DROP;
+			goto skip_xmit;
+		}
+	}
+
+	if (pko_command.s.dontfree) {
+		queue_type = QUEUE_WQE;
+	} else {
+		queue_type = QUEUE_HW;
+		if (buffers_being_recycled > 1) {
+			struct sk_buff *tskb, *nskb;
+			/* We are committed to use hardware free, restore the
+			 * frag list to empty on the first SKB
+			 */
+			tskb = skb_shinfo(skb)->frag_list;
+			while (tskb) {
+				nskb = tskb->next;
+				cvm_oct_skb_prepare_for_reuse(tskb);
+				tskb = nskb;
+			}
+		}
+		cvm_oct_skb_prepare_for_reuse(skb);
+	}
+
+	if (queue_type == QUEUE_WQE) {
+		if (!work) {
+			work = cvmx_fpa1_alloc(wqe_pool);
+			if (unlikely(!work)) {
+				netdev_err(dev, "Failed WQE allocate\n");
+				queue_type = QUEUE_DROP;
+				goto skip_xmit;
+			}
+		}
+
+		pko_command.s.rsp = 1;
+		pko_command.s.wqp = 1;
+		/* work->unused will carry the qos for this packet,
+		 * this allows us to find the proper FAU when freeing
+		 * the packet.  We decrement the FAU when the WQE is
+		 * replaced in the pool.
+		 */
+		pko_command.s.reg0 = 0;
+		work->word0.u64 = 0;
+		work->word0.raw.unused = (u8)qos;
+
+		work->word1.u64 = 0;
+		work->word1.tag_type = CVMX_POW_TAG_TYPE_NULL;
+		work->word1.tag = 0;
+		work->word2.u64 = 0;
+		work->word2.s.software = 1;
+		cvmx_wqe_set_grp(work, pow_receive_group);
+		work->packet_ptr.u64 = (unsigned long)skb;
+	}
+
+	local_irq_save(flags);
+
+	cvmx_pko_send_packet_prepare_pkoid(priv->pko_port,
+					   priv->tx_queue[qos].queue,
+					   CVM_OCT_PKO_LOCK_TYPE);
+
+	/* Send the packet to the output queue */
+	if (queue_type == QUEUE_WQE) {
+		u64 word2 = virt_to_phys(work);
+		if (timestamp_this_skb)
+			word2 |= 1ull << 40; /* Bit 40 controls timestamps */
+
+		if (unlikely(cvmx_hwpko_send_packet_finish3_pkoid(priv->pko_port,
+							  priv->tx_queue[qos].queue, pko_command, hw_buffer,
+							  word2, CVM_OCT_PKO_LOCK_TYPE))) {
+				queue_type = QUEUE_DROP;
+				netdev_err(dev, "Failed to send the packet with wqe\n");
+		}
+	} else {
+		if (unlikely(cvmx_hwpko_send_packet_finish_pkoid(priv->pko_port,
+							 priv->tx_queue[qos].queue,
+							 pko_command, hw_buffer,
+							 CVM_OCT_PKO_LOCK_TYPE))) {
+			netdev_err(dev, "Failed to send the packet\n");
+			queue_type = QUEUE_DROP;
+		}
+	}
+	local_irq_restore(flags);
+
+skip_xmit:
+	switch (queue_type) {
+	case QUEUE_DROP:
+		cvmx_hwfau_atomic_add32(priv->tx_queue[qos].fau, -1);
+		/* Fall through */
+	case QUEUE_DROP_NO_DEC:
+		dev_kfree_skb_any(skb);
+		dev->stats.tx_dropped++;
+		if (work)
+			cvmx_fpa1_free(work, wqe_pool, DONT_WRITEBACK(1));
+		break;
+	case QUEUE_HW:
+		cvmx_hwfau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, -buffers_being_recycled);
+		break;
+	case QUEUE_WQE:
+		/* Cleanup is done on the RX path when the WQE returns */
+		break;
+	default:
+		BUG();
+	}
+	preempt_enable();
+post_preempt_out:
+	if (USE_ASYNC_IOBDMA) {
+		CVMX_SYNCIOBDMA;
+		/* Restore the scratch area */
+		cvmx_scratch_write64(CVMX_SCR_SCRATCH, old_scratch);
+		cvmx_scratch_write64(CVMX_SCR_SCRATCH + 8, old_scratch2);
+	}
+
+	return NETDEV_TX_OK;
+}
diff --git a/drivers/net/ethernet/octeon/ethernet.c b/drivers/net/ethernet/octeon/ethernet.c
new file mode 100644
index 0000000..cd94908
--- /dev/null
+++ b/drivers/net/ethernet/octeon/ethernet.c
@@ -0,0 +1,1178 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+#include <linux/platform_device.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/phy.h>
+#include <linux/slab.h>
+#include <linux/of_net.h>
+#include <linux/interrupt.h>
+
+#include <net/dst.h>
+
+#include <asm/octeon/octeon.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+
+#include <asm/octeon/cvmx-pip.h>
+#include <asm/octeon/cvmx-hwpko.h>
+#include <asm/octeon/cvmx-hwfau.h>
+#include <asm/octeon/cvmx-ipd.h>
+#include <asm/octeon/cvmx-srio.h>
+#include <asm/octeon/cvmx-helper.h>
+#include <asm/octeon/cvmx-helper-cfg.h>
+#include <asm/octeon/cvmx-helper-util.h>
+#include <asm/octeon/cvmx-pko-internal-ports-range.h>
+#include <asm/octeon/cvmx-app-config.h>
+
+#include <asm/octeon/cvmx-gmxx-defs.h>
+#include <asm/octeon/cvmx-smix-defs.h>
+
+int rx_cpu_factor = 8;
+module_param(rx_cpu_factor, int, S_IRUGO | S_IWUSR | S_IWGRP);
+MODULE_PARM_DESC(rx_cpu_factor, "Control how many CPUs are used for packet reception.\n"
+		 "\tLarger numbers result in fewer CPUs used.");
+
+int octeon_recycle_tx = REUSE_SKBUFFS_WITHOUT_FREE;
+module_param_named(recycle_tx, octeon_recycle_tx, int, S_IRUGO | S_IWUSR | S_IWGRP);
+MODULE_PARM_DESC(recycle_tx, "Allow hardware SKB recycling.");
+
+int num_packet_buffers = 1024;
+module_param(num_packet_buffers, int, 0444);
+MODULE_PARM_DESC(num_packet_buffers, "\n"
+	"\tNumber of packet buffers to allocate and store in the\n"
+	"\tFPA. By default, 1024 packet buffers are used.");
+
+int pow_receive_group = 15;
+module_param(pow_receive_group, int, 0444);
+MODULE_PARM_DESC(pow_receive_group, "\n"
+	"\tPOW group to receive packets from. All ethernet hardware\n"
+	"\twill be configured to send incomming packets to this POW\n"
+	"\tgroup. Also any other software can submit packets to this\n"
+	"\tgroup for the kernel to process.");
+
+static int disable_core_queueing = 1;
+module_param(disable_core_queueing, int, S_IRUGO);
+MODULE_PARM_DESC(disable_core_queueing, "\n"
+		"\t\tWhen set the networking core's tx_queue_len is set to zero.  This\n"
+		"\t\tallows packets to be sent without lock contention in the packet scheduler\n"
+		"\t\tresulting in some cases in improved throughput.");
+
+int max_rx_cpus = -1;
+module_param(max_rx_cpus, int, 0444);
+MODULE_PARM_DESC(max_rx_cpus, "\n"
+	"\t\tThe maximum number of CPUs to use for packet reception.\n"
+	"\t\tUse -1 to use all available CPUs.");
+
+int rx_napi_weight = 32;
+module_param(rx_napi_weight, int, 0444);
+MODULE_PARM_DESC(rx_napi_weight, "The NAPI WEIGHT parameter.");
+
+static int disable_lockless_pko;
+module_param(disable_lockless_pko, int, S_IRUGO);
+MODULE_PARM_DESC(disable_lockless_pko, "Disable lockless PKO access (use locking for queues instead).");
+
+/* packet pool */
+int packet_pool = 0;
+/* wqe pool */
+int wqe_pool = -1;
+/* output pool */
+int output_pool = -1;
+
+/**
+ * cvm_oct_poll_queue - Workqueue for polling operations.
+ */
+struct workqueue_struct *cvm_oct_poll_queue;
+
+/**
+ * cvm_oct_poll_queue_stopping - flag to indicate polling should stop.
+ *
+ * Set to one right before cvm_oct_poll_queue is destroyed.
+ */
+atomic_t cvm_oct_poll_queue_stopping = ATOMIC_INIT(0);
+
+/* cvm_oct_by_pkind is an array of every ethernet device owned by this
+ * driver indexed by the IPD pkind/port_number.  If an entry is empty
+ * (NULL) it either doesn't exist, or there was a collision.  The two
+ * cases can be distinguished by trying to look up via
+ * cvm_oct_dev_for_port();
+ */
+struct octeon_ethernet *cvm_oct_by_pkind[64] __cacheline_aligned;
+
+/*
+ * cvm_oct_by_srio_mbox is indexed by the SRIO mailbox.
+ */
+struct octeon_ethernet *cvm_oct_by_srio_mbox[4][4];
+
+/* cvm_oct_list is a list of all cvm_oct_private_t created by this driver. */
+LIST_HEAD(cvm_oct_list);
+
+static void cvm_oct_rx_refill_worker(struct work_struct *work);
+static DECLARE_DELAYED_WORK(cvm_oct_rx_refill_work, cvm_oct_rx_refill_worker);
+
+static void cvm_oct_rx_refill_worker(struct work_struct *work)
+{
+	/* FPA 0 may have been drained, try to refill it if we need
+	 * more than num_packet_buffers / 2, otherwise normal receive
+	 * processing will refill it.  If it were drained, no packets
+	 * could be received so cvm_oct_napi_poll would never be
+	 * invoked to do the refill.
+	 */
+	cvm_oct_rx_refill_pool(num_packet_buffers / 2);
+
+	if (!atomic_read(&cvm_oct_poll_queue_stopping))
+		queue_delayed_work(cvm_oct_poll_queue,
+				   &cvm_oct_rx_refill_work, HZ);
+}
+
+static void cvm_oct_periodic_worker(struct work_struct *work)
+{
+	struct octeon_ethernet *priv = container_of(work,
+						    struct octeon_ethernet,
+						    port_periodic_work.work);
+	void (*poll_fn) (struct net_device *);
+
+	poll_fn = ACCESS_ONCE(priv->poll);
+
+	if (poll_fn)
+		poll_fn(priv->netdev);
+
+	priv->netdev->netdev_ops->ndo_get_stats(priv->netdev);
+
+	if (!atomic_read(&cvm_oct_poll_queue_stopping))
+		queue_delayed_work(cvm_oct_poll_queue, &priv->port_periodic_work, HZ);
+}
+
+static int cvm_oct_num_output_buffers;
+
+static int cvm_oct_get_total_pko_queues(void)
+{
+	if (OCTEON_IS_MODEL(OCTEON_CN38XX))
+		return 128;
+	else if (OCTEON_IS_MODEL(OCTEON_CN3XXX))
+		return 32;
+	else if (OCTEON_IS_MODEL(OCTEON_CN50XX))
+		return 32;
+	else
+		return 256;
+}
+
+static bool cvm_oct_pko_lockless(void)
+{
+	int interface, num_interfaces;
+	int queues = 0;
+
+	if (disable_lockless_pko)
+		return false;
+
+	/* CN3XXX require workarounds in xmit.  Disable lockless for
+	 * CN3XXX to optimize the lockless case with out the
+	 * workarounds.
+	 */
+	if (OCTEON_IS_MODEL(OCTEON_CN3XXX))
+		return false;
+
+	num_interfaces = cvmx_helper_get_number_of_interfaces();
+	for (interface = 0; interface < num_interfaces; interface++) {
+		int num_ports, port;
+		cvmx_helper_interface_mode_t imode = cvmx_helper_interface_get_mode(interface);
+
+		num_ports = cvmx_helper_interface_enumerate(interface);
+		for (port = 0; port < num_ports; port++) {
+			if (!cvmx_helper_is_port_valid(interface, port))
+				continue;
+			switch (imode) {
+			case CVMX_HELPER_INTERFACE_MODE_XAUI:
+			case CVMX_HELPER_INTERFACE_MODE_RXAUI:
+			case CVMX_HELPER_INTERFACE_MODE_SGMII:
+			case CVMX_HELPER_INTERFACE_MODE_QSGMII:
+			case CVMX_HELPER_INTERFACE_MODE_RGMII:
+			case CVMX_HELPER_INTERFACE_MODE_GMII:
+			case CVMX_HELPER_INTERFACE_MODE_SPI:
+				queues += max(8u, num_possible_cpus());
+				break;
+			case CVMX_HELPER_INTERFACE_MODE_NPI:
+			case CVMX_HELPER_INTERFACE_MODE_LOOP:
+#ifdef CONFIG_RAPIDIO
+			case CVMX_HELPER_INTERFACE_MODE_SRIO:
+#endif
+				queues += 1;
+				break;
+			default:
+				break;
+			}
+		}
+	}
+	return queues <= cvm_oct_get_total_pko_queues();
+}
+
+static void cvm_oct_set_pko_multiqueue(void)
+{
+	int interface, num_interfaces, rv;
+
+	num_interfaces = cvmx_helper_get_number_of_interfaces();
+	for (interface = 0; interface < num_interfaces; interface++) {
+		int num_ports, port;
+		cvmx_helper_interface_mode_t imode = cvmx_helper_interface_get_mode(interface);
+
+		num_ports = cvmx_helper_interface_enumerate(interface);
+		for (port = 0; port < num_ports; port++) {
+			if (!cvmx_helper_is_port_valid(interface, port))
+				continue;
+			switch (imode) {
+			case CVMX_HELPER_INTERFACE_MODE_XAUI:
+			case CVMX_HELPER_INTERFACE_MODE_RXAUI:
+			case CVMX_HELPER_INTERFACE_MODE_SGMII:
+			case CVMX_HELPER_INTERFACE_MODE_QSGMII:
+			case CVMX_HELPER_INTERFACE_MODE_AGL:
+			case CVMX_HELPER_INTERFACE_MODE_RGMII:
+			case CVMX_HELPER_INTERFACE_MODE_GMII:
+			case CVMX_HELPER_INTERFACE_MODE_SPI:
+				rv = cvmx_pko_alloc_iport_and_queues(interface, port, 1,
+								     max(8u, num_possible_cpus()));
+				WARN(rv, "cvmx_pko_alloc_iport_and_queues failed");
+				if (rv)
+					return;
+				break;
+			default:
+				break;
+			}
+		}
+	}
+}
+
+static int cvm_oct_configure_common_hw(void)
+{
+	/* Setup the FPA */
+	cvmx_fpa1_enable();
+
+	/* allocate packet pool */
+	packet_pool = cvm_oct_alloc_fpa_pool(packet_pool, FPA_PACKET_POOL_SIZE);
+	if (packet_pool < 0) {
+		pr_err("cvm_oct_alloc_fpa_pool(%d, FPA_PACKET_POOL_SIZE) failed.\n", packet_pool);
+		return -ENOMEM;
+	}
+	cvm_oct_mem_fill_fpa(packet_pool, num_packet_buffers);
+
+	/* communicate packet pool number to ipd */
+	cvmx_ipd_set_packet_pool_config(packet_pool, FPA_PACKET_POOL_SIZE,
+					num_packet_buffers);
+
+	/* allocate wqe pool */
+	wqe_pool = cvm_oct_alloc_fpa_pool(-1, FPA_WQE_POOL_SIZE);
+	if (wqe_pool < 0) {
+		pr_err("cvm_oct_alloc_fpa_pool(-1, FPA_WQE_POOL_SIZE) failed.\n");
+		return -ENOMEM;;
+	}
+	cvm_oct_mem_fill_fpa(wqe_pool, num_packet_buffers);
+
+	/* communicate wqe pool to ipd */
+	cvmx_ipd_set_wqe_pool_config(wqe_pool, FPA_WQE_POOL_SIZE,
+				     num_packet_buffers);
+
+	if (cvm_oct_pko_lockless()) {
+		cvm_oct_set_pko_multiqueue();
+		cvm_oct_num_output_buffers = 4 * cvm_oct_get_total_pko_queues();
+	} else {
+		cvm_oct_num_output_buffers = 128;
+	}
+
+	/* alloc fpa pool for output buffers */
+	output_pool = cvm_oct_alloc_fpa_pool(-1, FPA_OUTPUT_BUFFER_POOL_SIZE);
+	if (output_pool < 0) {
+		pr_err("cvm_oct_alloc_fpa_pool(-1, FPA_OUTPUT_BUFFER_POOL_SIZE) failed.\n");
+		return -ENOMEM;;
+	}
+	cvm_oct_mem_fill_fpa(output_pool, cvm_oct_num_output_buffers);
+
+	/* communicate output pool no. to pko */
+	cvmx_pko_set_cmd_que_pool_config(output_pool,
+					 FPA_OUTPUT_BUFFER_POOL_SIZE,
+					 cvm_oct_num_output_buffers);
+
+	/* more configuration needs to be done, so enable ipd seperately */
+	cvmx_ipd_cfg.ipd_enable = 0;
+
+	__cvmx_export_app_config_to_named_block(CVMX_APP_CONFIG);
+
+	cvmx_helper_initialize_packet_io_global();
+
+#ifdef __LITTLE_ENDIAN
+	{
+		union cvmx_ipd_ctl_status ipd_ctl_status;
+		ipd_ctl_status.u64 = cvmx_read_csr(CVMX_IPD_CTL_STATUS);
+		ipd_ctl_status.s.pkt_lend = 1;
+		ipd_ctl_status.s.wqe_lend = 1;
+		cvmx_write_csr(CVMX_IPD_CTL_STATUS, ipd_ctl_status.u64);
+	}
+#endif
+	/* Enable red after interface is initialized */
+	if (USE_RED)
+		cvmx_helper_setup_red(num_packet_buffers / 4,
+				      num_packet_buffers / 8);
+
+	return 0;
+}
+
+/**
+ * cvm_oct_register_callback -  Register a intercept callback for the named device.
+ *
+ * It returns the net_device structure for the ethernet port. Usign a
+ * callback of NULL will remove the callback. Note that this callback
+ * must not disturb scratch. It will be called with SYNCIOBDMAs in
+ * progress and userspace may be using scratch. It also must not
+ * disturb the group mask.
+ *
+ * @device_name: Device name to register for. (Example: "eth0")
+ * @callback: Intercept callback to set.
+ *
+ * Returns the net_device structure for the ethernet port or NULL on failure.
+ */
+struct net_device *cvm_oct_register_callback(const char *device_name, cvm_oct_callback_t callback)
+{
+	struct octeon_ethernet *priv;
+
+	list_for_each_entry(priv, &cvm_oct_list, list) {
+		if (strcmp(device_name, priv->netdev->name) == 0) {
+			priv->intercept_cb = callback;
+			wmb();
+			return priv->netdev;
+		}
+	}
+	return NULL;
+}
+EXPORT_SYMBOL(cvm_oct_register_callback);
+
+/**
+ * cvm_oct_free_work- Free a work queue entry
+ *
+ * @work_queue_entry: Work queue entry to free
+ *
+ * Returns Zero on success, Negative on failure.
+ */
+int cvm_oct_free_work(void *work_queue_entry)
+{
+	cvmx_wqe_t *work = work_queue_entry;
+
+	int segments = work->word2.s.bufs;
+	union cvmx_buf_ptr segment_ptr = work->packet_ptr;
+
+	while (segments--) {
+		union cvmx_buf_ptr next_ptr = *(union cvmx_buf_ptr *)phys_to_virt(segment_ptr.s.addr - 8);
+		if (!segment_ptr.s.i)
+			cvmx_fpa1_free(cvm_oct_get_buffer_ptr(segment_ptr),
+				      segment_ptr.s.pool,
+				      DONT_WRITEBACK(FPA_PACKET_POOL_SIZE / 128));
+		segment_ptr = next_ptr;
+	}
+	cvmx_fpa1_free(work, wqe_pool, DONT_WRITEBACK(1));
+
+	return 0;
+}
+EXPORT_SYMBOL(cvm_oct_free_work);
+
+/* Lock to protect racy cvmx_pko_get_port_status() */
+static DEFINE_SPINLOCK(cvm_oct_tx_stat_lock);
+
+/**
+ * cvm_oct_common_get_stats - get the low level ethernet statistics
+ * @dev:    Device to get the statistics from
+ *
+ * Returns Pointer to the statistics
+ */
+static struct net_device_stats *cvm_oct_common_get_stats(struct net_device *dev)
+{
+	unsigned long flags;
+	cvmx_pip_port_status_t rx_status;
+	cvmx_pko_port_status_t tx_status;
+	u64 current_tx_octets;
+	u32 current_tx_packets;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	if (octeon_is_simulation()) {
+		/* The simulator doesn't support statistics */
+		memset(&rx_status, 0, sizeof(rx_status));
+		memset(&tx_status, 0, sizeof(tx_status));
+	} else {
+		cvmx_pip_get_port_status(priv->ipd_port, 1, &rx_status);
+
+		spin_lock_irqsave(&cvm_oct_tx_stat_lock, flags);
+		cvmx_pko_get_port_status(priv->ipd_port, 0, &tx_status);
+		current_tx_packets = tx_status.packets;
+		current_tx_octets = tx_status.octets;
+		/* The tx_packets counter is 32-bits as are all these
+		 * variables.  No truncation necessary.
+		 */
+		tx_status.packets = current_tx_packets - priv->last_tx_packets;
+		/* The tx_octets counter is only 48-bits, so we need
+		 * to truncate in case there was a wrap-around
+		 */
+		tx_status.octets = (current_tx_octets - priv->last_tx_octets) & 0xffffffffffffull;
+		priv->last_tx_packets = current_tx_packets;
+		priv->last_tx_octets = current_tx_octets;
+		spin_unlock_irqrestore(&cvm_oct_tx_stat_lock, flags);
+	}
+
+	dev->stats.rx_packets += rx_status.inb_packets;
+	dev->stats.tx_packets += tx_status.packets;
+	dev->stats.rx_bytes += rx_status.inb_octets;
+	dev->stats.tx_bytes += tx_status.octets;
+	dev->stats.multicast += rx_status.multicast_packets;
+	dev->stats.rx_crc_errors += rx_status.inb_errors;
+	dev->stats.rx_frame_errors += rx_status.fcs_align_err_packets;
+
+	/* The drop counter must be incremented atomically since the
+	 * RX tasklet also increments it.
+	 */
+	atomic64_add(rx_status.dropped_packets,
+		     (atomic64_t *)&dev->stats.rx_dropped);
+
+	return &dev->stats;
+}
+
+/**
+ * cvm_oct_change_mtu - change the link MTU
+ * @dev:     Device to change
+ * @new_mtu: The new MTU
+ *
+ * Returns Zero on success
+ */
+static int cvm_oct_change_mtu(struct net_device *dev, int new_mtu)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	int ret;
+	u64 ipd_reg;
+
+	if (OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN58XX))
+		ipd_reg = 0;
+	else
+		ipd_reg = CVMX_PIP_PRT_CFGX(priv->ipd_pkind);
+
+	ret = cvm_oct_common_change_mtu(dev, new_mtu, priv->gmx_base, ipd_reg, 65392);
+
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+/**
+ * Set RX filtering
+ * @dev:    Device to work on
+ */
+static void cvm_oct_set_rx_filter(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	if (priv->gmx_base)
+		cvm_oct_common_set_rx_filtering(dev, priv->gmx_base,
+					 &priv->poll_lock);
+}
+
+/**
+ * Set the hardware MAC address for a device
+ * @dev:    The device in question.
+ * @addr:   Address structure to change it too.
+ *
+ * Returns Zero on success
+ */
+static int cvm_oct_set_mac_address(struct net_device *dev, void *addr)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	if (priv->gmx_base)
+		cvm_oct_common_set_mac_address(dev, addr, priv->gmx_base,
+					 &priv->poll_lock);
+
+	return 0;
+}
+
+/**
+ * cvm_oct_common_init - per network device initialization
+ * @dev:    Device to initialize
+ *
+ * Returns Zero on success
+ */
+int cvm_oct_common_init(struct net_device *dev)
+{
+	unsigned long flags;
+	cvmx_pko_port_status_t tx_status;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	const u8 *mac = NULL;
+	struct sockaddr sa;
+
+	if (priv->of_node)
+		mac = of_get_mac_address(priv->of_node);
+
+	if (mac && is_valid_ether_addr(mac)) {
+		memcpy(dev->dev_addr, mac, ETH_ALEN);
+		dev->addr_assign_type &= ~NET_ADDR_RANDOM;
+	} else {
+		eth_hw_addr_random(dev);
+	}
+
+	if (priv->num_tx_queues != -1) {
+		dev->features |= NETIF_F_SG | NETIF_F_FRAGLIST;
+		if (USE_HW_TCPUDP_CHECKSUM)
+			dev->features |= NETIF_F_IP_CSUM;
+	}
+
+	/* We do our own locking, Linux doesn't need to */
+	dev->features |= NETIF_F_LLTX;
+	SET_ETHTOOL_OPS(dev, &cvm_oct_ethtool_ops);
+
+	memcpy(sa.sa_data, dev->dev_addr, ETH_ALEN);
+	cvm_oct_common_set_mac_address(dev, &sa, priv->gmx_base, &priv->poll_lock);
+	dev->netdev_ops->ndo_change_mtu(dev, dev->mtu);
+
+	spin_lock_irqsave(&cvm_oct_tx_stat_lock, flags);
+	cvmx_pko_get_port_status(priv->ipd_port, 0, &tx_status);
+	priv->last_tx_packets = tx_status.packets;
+	priv->last_tx_octets = tx_status.octets;
+	/* Zero out stats for port so we won't mistakenly show
+	 * counters from the bootloader.
+	 */
+	memset(&dev->stats, 0, sizeof(struct net_device_stats));
+	spin_unlock_irqrestore(&cvm_oct_tx_stat_lock, flags);
+
+	return 0;
+}
+
+static const struct net_device_ops cvm_oct_npi_netdev_ops = {
+	.ndo_init		= cvm_oct_common_init,
+	.ndo_start_xmit		= cvm_oct_xmit,
+	.ndo_set_rx_mode	= cvm_oct_set_rx_filter,
+	.ndo_set_mac_address	= cvm_oct_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+
+/* SGMII, AGL and XAUI handled the same so they both use this. */
+static const struct net_device_ops cvm_oct_sgmii_netdev_ops = {
+	.ndo_init		= cvm_oct_sgmii_init,
+	.ndo_uninit		= cvm_oct_sgmii_uninit,
+	.ndo_open		= cvm_oct_sgmii_open,
+	.ndo_stop		= cvm_oct_sgmii_stop,
+	.ndo_start_xmit		= cvm_oct_xmit,
+	.ndo_set_rx_mode	= cvm_oct_set_rx_filter,
+	.ndo_set_mac_address	= cvm_oct_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+static const struct net_device_ops cvm_oct_sgmii_lockless_netdev_ops = {
+	.ndo_init		= cvm_oct_sgmii_init,
+	.ndo_uninit		= cvm_oct_sgmii_uninit,
+	.ndo_open		= cvm_oct_sgmii_open,
+	.ndo_stop		= cvm_oct_sgmii_stop,
+	.ndo_start_xmit		= cvm_oct_xmit_lockless,
+	.ndo_set_rx_mode	= cvm_oct_set_rx_filter,
+	.ndo_set_mac_address	= cvm_oct_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+static const struct net_device_ops cvm_oct_spi_netdev_ops = {
+	.ndo_init		= cvm_oct_spi_init,
+	.ndo_uninit		= cvm_oct_spi_uninit,
+	.ndo_open		= cvm_oct_phy_setup_device,
+	.ndo_stop		= cvm_oct_common_stop,
+	.ndo_start_xmit		= cvm_oct_xmit,
+	.ndo_set_rx_mode	= cvm_oct_set_rx_filter,
+	.ndo_set_mac_address	= cvm_oct_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+static const struct net_device_ops cvm_oct_spi_lockless_netdev_ops = {
+	.ndo_init		= cvm_oct_spi_init,
+	.ndo_uninit		= cvm_oct_spi_uninit,
+	.ndo_open		= cvm_oct_phy_setup_device,
+	.ndo_start_xmit		= cvm_oct_xmit_lockless,
+	.ndo_set_rx_mode	= cvm_oct_set_rx_filter,
+	.ndo_set_mac_address	= cvm_oct_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+static const struct net_device_ops cvm_oct_rgmii_netdev_ops = {
+	.ndo_init		= cvm_oct_rgmii_init,
+	.ndo_open		= cvm_oct_rgmii_open,
+	.ndo_stop		= cvm_oct_rgmii_stop,
+	.ndo_start_xmit		= cvm_oct_xmit,
+	.ndo_set_rx_mode	= cvm_oct_set_rx_filter,
+	.ndo_set_mac_address	= cvm_oct_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+static const struct net_device_ops cvm_oct_rgmii_lockless_netdev_ops = {
+	.ndo_init		= cvm_oct_rgmii_init,
+	.ndo_open		= cvm_oct_rgmii_open,
+	.ndo_stop		= cvm_oct_rgmii_stop,
+	.ndo_start_xmit		= cvm_oct_xmit_lockless,
+	.ndo_set_rx_mode	= cvm_oct_set_rx_filter,
+	.ndo_set_mac_address	= cvm_oct_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+#ifdef CONFIG_RAPIDIO
+static const struct net_device_ops cvm_oct_srio_netdev_ops = {
+	.ndo_init		= cvm_oct_srio_init,
+	.ndo_open		= cvm_oct_srio_open,
+	.ndo_stop		= cvm_oct_srio_stop,
+	.ndo_start_xmit		= cvm_oct_xmit_srio,
+	.ndo_set_mac_address	= cvm_oct_srio_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_srio_change_mtu,
+	.ndo_get_stats		= cvm_oct_srio_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+#endif
+
+extern void octeon_mdiobus_force_mod_depencency(void);
+
+static int num_devices_extra_wqe;
+#define PER_DEVICE_EXTRA_WQE (MAX_OUT_QUEUE_DEPTH)
+
+static struct rb_root cvm_oct_ipd_tree = RB_ROOT;
+
+void cvm_oct_add_ipd_port(struct octeon_ethernet *port)
+{
+	struct rb_node **link = &cvm_oct_ipd_tree.rb_node;
+	struct rb_node *parent = NULL;
+	struct octeon_ethernet *n;
+	int value = port->key;
+
+	while (*link) {
+		parent = *link;
+		n = rb_entry(parent, struct octeon_ethernet, ipd_tree);
+
+		if (value < n->key)
+			link = &(*link)->rb_left;
+		else if (value > n->key)
+			link = &(*link)->rb_right;
+		else
+			BUG();
+	}
+	rb_link_node(&port->ipd_tree, parent, link);
+	rb_insert_color(&port->ipd_tree, &cvm_oct_ipd_tree);
+}
+
+struct octeon_ethernet *cvm_oct_dev_for_port(int port_number)
+{
+	struct rb_node *n = cvm_oct_ipd_tree.rb_node;
+	while (n) {
+		struct octeon_ethernet *s = rb_entry(n, struct octeon_ethernet, ipd_tree);
+
+		if (s->key > port_number)
+			n = n->rb_left;
+		else if (s->key < port_number)
+			n = n->rb_left;
+		else
+			return s;
+	}
+	return NULL;
+}
+
+static struct device_node *cvm_oct_of_get_child(const struct device_node *parent,
+						int reg_val)
+{
+	struct device_node *node = NULL;
+	int size;
+	const __be32 *addr;
+
+	for (;;) {
+		node = of_get_next_child(parent, node);
+		if (!node)
+			break;
+		addr = of_get_property(node, "reg", &size);
+		if (addr && (be32_to_cpu(*addr) == reg_val))
+			break;
+	}
+	return node;
+}
+
+static struct device_node *cvm_oct_node_for_port(struct device_node *pip,
+						 int interface, int port)
+{
+	struct device_node *ni, *np;
+
+	ni = cvm_oct_of_get_child(pip, interface);
+	if (!ni)
+		return NULL;
+
+	np = cvm_oct_of_get_child(ni, port);
+	of_node_put(ni);
+
+	return np;
+}
+
+static int cvm_oct_get_port_status(struct device_node *pip)
+{
+	int i, j;
+	int num_interfaces = cvmx_helper_get_number_of_interfaces();
+
+	for (i = 0; i < num_interfaces; i++) {
+		int num_ports = cvmx_helper_interface_enumerate(i);
+		int mode = cvmx_helper_interface_get_mode(i);
+
+		for (j = 0; j < num_ports; j++) {
+			switch (mode) {
+			case CVMX_HELPER_INTERFACE_MODE_RGMII:
+			case CVMX_HELPER_INTERFACE_MODE_GMII:
+			case CVMX_HELPER_INTERFACE_MODE_XAUI:
+			case CVMX_HELPER_INTERFACE_MODE_RXAUI:
+			case CVMX_HELPER_INTERFACE_MODE_SPI:
+			case CVMX_HELPER_INTERFACE_MODE_AGL:
+				if (cvm_oct_node_for_port(pip, i, j) != NULL)
+					cvmx_helper_set_port_valid(i, j, true);
+				else
+					cvmx_helper_set_port_valid(i, j, false);
+				cvmx_helper_set_mac_phy_mode(i, j, false);
+				cvmx_helper_set_1000x_mode(i, j, false);
+				break;
+			case CVMX_HELPER_INTERFACE_MODE_SGMII:
+			case CVMX_HELPER_INTERFACE_MODE_QSGMII:
+			{
+				struct device_node *port_node;
+				port_node = cvm_oct_node_for_port(pip, i, j);
+				if (port_node != NULL)
+					cvmx_helper_set_port_valid(i, j, true);
+				else
+					cvmx_helper_set_port_valid(i, j, false);
+				cvmx_helper_set_mac_phy_mode(i, j, false);
+				cvmx_helper_set_1000x_mode(i, j, false);
+				if (port_node) {
+					if (of_get_property(port_node, 
+					    "cavium,sgmii-mac-phy-mode", NULL) != NULL)
+						cvmx_helper_set_mac_phy_mode(i, j, true);
+					if (of_get_property(port_node, 
+					    "cavium,sgmii-mac-1000x-mode", NULL) 
+					    != NULL)
+						cvmx_helper_set_1000x_mode(i, j, true);
+				}
+				break;
+			}
+			default:
+				cvmx_helper_set_port_valid(i, j, true);
+				cvmx_helper_set_mac_phy_mode(i, j, false);
+				cvmx_helper_set_1000x_mode(i, j, false);
+				break;
+			}
+		}
+	}
+	return 0;
+}
+
+static int cvm_oct_probe(struct platform_device *pdev)
+{
+	int num_interfaces;
+	int interface;
+	int fau = FAU_NUM_PACKET_BUFFERS_TO_FREE;
+	int qos, r;
+	struct device_node *pip;
+
+	octeon_mdiobus_force_mod_depencency();
+	pr_notice("octeon-ethernet %s\n", OCTEON_ETHERNET_VERSION);
+
+	pip = pdev->dev.of_node;
+	if (!pip) {
+		dev_err(&pdev->dev, "No of_node.\n");
+		return -EINVAL;
+	}
+
+	cvm_oct_get_port_status(pip);
+
+	cvm_oct_poll_queue = create_singlethread_workqueue("octeon-ethernet");
+	if (cvm_oct_poll_queue == NULL) {
+		dev_err(&pdev->dev, "Cannot create workqueue");
+		return -ENOMEM;
+	}
+
+	r = cvm_oct_configure_common_hw();
+	if (r)
+		return r;
+
+	/* Change the input group for all ports before input is enabled */
+	num_interfaces = cvmx_helper_get_number_of_interfaces();
+	for (interface = 0; interface < num_interfaces; interface++) {
+		int num_ports = cvmx_helper_ports_on_interface(interface);
+		int index;
+
+		for (index = 0; index < num_ports; index++) {
+			union cvmx_pip_prt_tagx pip_prt_tagx;
+			int port = cvmx_helper_get_ipd_port(interface, index);
+
+			if (octeon_has_feature(OCTEON_FEATURE_PKND))
+				port = cvmx_helper_get_pknd(interface, index);
+
+			if (!cvmx_helper_is_port_valid(interface, index))
+				continue;
+			pip_prt_tagx.u64 = cvmx_read_csr(CVMX_PIP_PRT_TAGX(port));
+			pip_prt_tagx.s.grp = pow_receive_group;
+			cvmx_write_csr(CVMX_PIP_PRT_TAGX(port), pip_prt_tagx.u64);
+		}
+	}
+
+	cvmx_helper_ipd_and_packet_input_enable_node(0);
+
+	/* Initialize the FAU used for counting packet buffers that
+	 * need to be freed.
+	 */
+	cvmx_hwfau_atomic_write32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);
+
+	num_interfaces = cvmx_helper_get_number_of_interfaces();
+	for (interface = 0; interface < num_interfaces; interface++) {
+		cvmx_helper_interface_mode_t imode = cvmx_helper_interface_get_mode(interface);
+		int num_ports = cvmx_helper_ports_on_interface(interface);
+		int interface_port;
+
+		if (imode == CVMX_HELPER_INTERFACE_MODE_SRIO)
+			num_ports = 2; /* consistent with se apps. could be 4 */
+
+		for (interface_port = 0; interface_port < num_ports;
+		     interface_port++) {
+			struct octeon_ethernet *priv;
+			int base_queue;
+			struct net_device *dev;
+
+			if (!cvmx_helper_is_port_valid(interface,
+						      interface_port))
+				continue;
+
+			dev = alloc_etherdev(sizeof(struct octeon_ethernet));
+			if (!dev) {
+				dev_err(&pdev->dev,
+					"Failed to allocate ethernet device for port %d:%d\n",
+					interface, interface_port);
+				continue;
+			}
+
+			if (disable_core_queueing)
+				dev->tx_queue_len = 0;
+
+			/* Initialize the device private structure. */
+			priv = netdev_priv(dev);
+			INIT_LIST_HEAD(&priv->srio_bcast);
+			priv->of_node = cvm_oct_node_for_port(pip, interface, interface_port);
+			priv->netdev = dev;
+			priv->interface = interface;
+			priv->interface_port = interface_port;
+			priv->gmx_base = 0;
+			spin_lock_init(&priv->poll_lock);
+			INIT_DELAYED_WORK(&priv->port_periodic_work,
+					  cvm_oct_periodic_worker);
+			priv->imode = imode;
+
+			if (imode == CVMX_HELPER_INTERFACE_MODE_SRIO) {
+				int mbox = cvmx_helper_get_ipd_port(interface, interface_port) - cvmx_helper_get_ipd_port(interface, 0);
+				union cvmx_srio_tx_message_header tx_header;
+				tx_header.u64 = 0;
+				tx_header.s.tt = 0;
+				tx_header.s.ssize = 0xe;
+				tx_header.s.mbox = mbox;
+				tx_header.s.lns = 1;
+				tx_header.s.intr = 1;
+				priv->srio_tx_header = tx_header.u64;
+				priv->ipd_port = cvmx_helper_get_ipd_port(interface, mbox);
+				priv->pko_port = priv->ipd_port;
+				priv->key = priv->ipd_port + (0x10000 * mbox);
+				base_queue = cvmx_pko_get_base_queue(priv->ipd_port);
+				priv->num_tx_queues = 1;
+				cvm_oct_by_srio_mbox[interface - 4][mbox] = priv;
+			} else {
+				priv->ipd_port = cvmx_helper_get_ipd_port(interface, interface_port);
+				priv->key = priv->ipd_port;
+				priv->pko_port = cvmx_helper_get_pko_port(interface, interface_port);
+				base_queue = cvmx_pko_get_base_queue(priv->ipd_port);
+				priv->num_tx_queues = cvmx_pko_get_num_queues(priv->ipd_port);
+			}
+
+			if (priv->num_tx_queues == 0) {
+				dev_err(&pdev->dev,
+					"tx_queue count not configured for port %d:%d\n",
+					interface, interface_port);
+				free_netdev(dev);
+				continue;
+			}
+
+			BUG_ON(priv->num_tx_queues < 1);
+			BUG_ON(priv->num_tx_queues > 32);
+
+			if (octeon_has_feature(OCTEON_FEATURE_PKND))
+				priv->ipd_pkind = cvmx_helper_get_pknd(interface, interface_port);
+			else
+				priv->ipd_pkind = priv->ipd_port;
+
+			for (qos = 0; qos < priv->num_tx_queues; qos++) {
+				priv->tx_queue[qos].queue = base_queue + qos;
+				fau = fau - sizeof(u32);
+				priv->tx_queue[qos].fau = fau;
+				cvmx_hwfau_atomic_write32(priv->tx_queue[qos].fau, 0);
+			}
+
+			/* Cache the fact that there may be multiple queues */
+			priv->tx_multiple_queues = (priv->num_tx_queues > 1);
+
+			switch (priv->imode) {
+			/* These types don't support ports to IPD/PKO */
+			case CVMX_HELPER_INTERFACE_MODE_DISABLED:
+			case CVMX_HELPER_INTERFACE_MODE_PCIE:
+			case CVMX_HELPER_INTERFACE_MODE_PICMG:
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_NPI:
+				dev->netdev_ops = &cvm_oct_npi_netdev_ops;
+				strcpy(dev->name, "npi%d");
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_XAUI:
+			case CVMX_HELPER_INTERFACE_MODE_RXAUI:
+				priv->tx_lockless = priv->tx_multiple_queues && !disable_lockless_pko;
+				dev->netdev_ops = priv->tx_lockless ?
+					&cvm_oct_sgmii_lockless_netdev_ops : &cvm_oct_sgmii_netdev_ops;
+				dev->priv_flags |= IFF_UNICAST_FLT;
+				priv->gmx_base = CVMX_GMXX_RXX_INT_REG(interface_port, interface);
+				strcpy(dev->name, "xaui%d");
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_LOOP:
+				dev->netdev_ops = &cvm_oct_npi_netdev_ops;
+				strcpy(dev->name, "loop%d");
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_SGMII:
+			case CVMX_HELPER_INTERFACE_MODE_QSGMII:
+				priv->tx_lockless = priv->tx_multiple_queues && !disable_lockless_pko;
+				dev->netdev_ops = priv->tx_lockless ?
+					&cvm_oct_sgmii_lockless_netdev_ops : &cvm_oct_sgmii_netdev_ops;
+				dev->priv_flags |= IFF_UNICAST_FLT;
+				priv->gmx_base = CVMX_GMXX_RXX_INT_REG(interface_port, interface);
+				strcpy(dev->name, "eth%d");
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_AGL:
+				priv->tx_lockless = priv->tx_multiple_queues && !disable_lockless_pko;
+				dev->netdev_ops = priv->tx_lockless ?
+					&cvm_oct_sgmii_lockless_netdev_ops : &cvm_oct_sgmii_netdev_ops;
+				dev->priv_flags |= IFF_UNICAST_FLT;
+				priv->gmx_base = CVMX_AGL_GMX_RXX_INT_REG(0);
+				strcpy(dev->name, "agl%d");
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_SPI:
+				priv->tx_lockless = priv->tx_multiple_queues && !disable_lockless_pko;
+				dev->netdev_ops = priv->tx_lockless ?
+					&cvm_oct_spi_lockless_netdev_ops : &cvm_oct_spi_netdev_ops;
+				dev->priv_flags |= IFF_UNICAST_FLT;
+				priv->gmx_base = CVMX_GMXX_RXX_INT_REG(interface_port, interface);
+				strcpy(dev->name, "spi%d");
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_RGMII:
+			case CVMX_HELPER_INTERFACE_MODE_GMII:
+				priv->tx_lockless = priv->tx_multiple_queues && !disable_lockless_pko;
+				dev->netdev_ops = priv->tx_lockless ?
+					&cvm_oct_rgmii_lockless_netdev_ops : &cvm_oct_rgmii_netdev_ops;
+				dev->priv_flags |= IFF_UNICAST_FLT;
+				priv->gmx_base = CVMX_GMXX_RXX_INT_REG(interface_port, interface);
+				strcpy(dev->name, "eth%d");
+				break;
+#ifdef CONFIG_RAPIDIO
+			case CVMX_HELPER_INTERFACE_MODE_SRIO:
+				dev->netdev_ops = &cvm_oct_srio_netdev_ops;
+				strcpy(dev->name, "rio%d");
+				break;
+#endif
+			}
+
+			netif_carrier_off(dev);
+			if (!dev->netdev_ops) {
+				free_netdev(dev);
+			} else if (register_netdev(dev) < 0) {
+				dev_err(&pdev->dev,
+					"Failed to register ethernet device for interface %d, port %d\n",
+					interface, priv->ipd_port);
+				free_netdev(dev);
+			} else {
+				list_add_tail(&priv->list, &cvm_oct_list);
+				if (cvm_oct_by_pkind[priv->ipd_pkind] == NULL)
+					cvm_oct_by_pkind[priv->ipd_pkind] = priv;
+				else
+					cvm_oct_by_pkind[priv->ipd_pkind] = (void *)-1L;
+
+				cvm_oct_add_ipd_port(priv);
+				/* Each transmit queue will need its
+				 * own MAX_OUT_QUEUE_DEPTH worth of
+				 * WQE to track the transmit skbs.
+				 */
+				cvm_oct_mem_fill_fpa(wqe_pool,
+						     PER_DEVICE_EXTRA_WQE);
+				num_devices_extra_wqe++;
+				queue_delayed_work(cvm_oct_poll_queue,
+						   &priv->port_periodic_work, HZ);
+			}
+		}
+	}
+
+	cvm_oct_rx_initialize(num_packet_buffers + num_devices_extra_wqe * PER_DEVICE_EXTRA_WQE);
+
+	queue_delayed_work(cvm_oct_poll_queue, &cvm_oct_rx_refill_work, HZ);
+
+	return 0;
+}
+
+static int cvm_oct_remove(struct platform_device *pdev)
+{
+	struct octeon_ethernet *priv;
+	struct octeon_ethernet *tmp;
+
+	/* Put both taking the interface down and unregistering it
+	 * under the lock.  That way the devices cannot be taken back
+	 * up in the middle of everything.
+	 */
+	rtnl_lock();
+
+	/* Take down all the interfaces, this disables the GMX and
+	 * prevents it from getting into a Bad State when IPD is
+	 * disabled.
+	 */
+	list_for_each_entry(priv, &cvm_oct_list, list) {
+		unsigned int f = dev_get_flags(priv->netdev);
+		dev_change_flags(priv->netdev, f & ~IFF_UP);
+	}
+
+	mdelay(10);
+
+	cvmx_ipd_disable();
+
+	mdelay(10);
+
+	atomic_inc_return(&cvm_oct_poll_queue_stopping);
+	cancel_delayed_work_sync(&cvm_oct_rx_refill_work);
+
+	cvm_oct_rx_shutdown0();
+
+	/* unregister the ethernet devices */
+	list_for_each_entry(priv, &cvm_oct_list, list) {
+		cancel_delayed_work_sync(&priv->port_periodic_work);
+		unregister_netdevice(priv->netdev);
+	}
+
+	rtnl_unlock();
+
+	/* Free the ethernet devices */
+	list_for_each_entry_safe_reverse(priv, tmp, &cvm_oct_list, list) {
+		list_del(&priv->list);
+		free_netdev(priv->netdev);
+	}
+
+	cvmx_helper_shutdown_packet_io_global();
+
+	cvm_oct_rx_shutdown1();
+
+	destroy_workqueue(cvm_oct_poll_queue);
+
+	/* Free the HW pools */
+	cvm_oct_mem_empty_fpa(packet_pool, num_packet_buffers);
+	cvm_oct_release_fpa_pool(packet_pool);
+
+	cvm_oct_mem_empty_fpa(wqe_pool,
+			      num_packet_buffers + num_devices_extra_wqe * PER_DEVICE_EXTRA_WQE);
+	cvm_oct_release_fpa_pool(wqe_pool);
+
+	cvm_oct_mem_empty_fpa(output_pool,
+				cvm_oct_num_output_buffers);
+	cvm_oct_release_fpa_pool(output_pool);
+
+	cvm_oct_mem_cleanup();
+
+	cvmx_pko_queue_free_all();
+	cvmx_pko_internal_ports_range_free_all();
+
+	__cvmx_export_app_config_cleanup();
+
+	return 0;
+}
+
+static void cvm_oct_shutdown(struct platform_device *pdev)
+{
+	cvm_oct_remove(pdev);
+}
+
+static struct of_device_id cvm_oct_match[] = {
+	{
+		.compatible = "cavium,octeon-3860-pip",
+	},
+	{},
+};
+MODULE_DEVICE_TABLE(of, cvm_oct_match);
+
+static struct platform_driver cvm_oct_driver = {
+	.probe		= cvm_oct_probe,
+	.remove		= cvm_oct_remove,
+	.shutdown       = cvm_oct_shutdown,
+	.driver		= {
+		.owner	= THIS_MODULE,
+		.name	= KBUILD_MODNAME,
+		.of_match_table = cvm_oct_match,
+	},
+};
+
+module_platform_driver(cvm_oct_driver);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Cavium Networks <support@caviumnetworks.com>");
+MODULE_DESCRIPTION("Cavium Networks Octeon ethernet driver.");
diff --git a/drivers/net/ethernet/octeon/octeon-ethernet.h b/drivers/net/ethernet/octeon/octeon-ethernet.h
new file mode 100644
index 0000000..10ffdea
--- /dev/null
+++ b/drivers/net/ethernet/octeon/octeon-ethernet.h
@@ -0,0 +1,228 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+
+/*
+ * External interface for the Cavium Octeon ethernet driver.
+ */
+#ifndef OCTEON_ETHERNET_H
+#define OCTEON_ETHERNET_H
+
+#include <linux/of.h>
+
+#include "octeon_common.h"
+
+#include <asm/octeon/cvmx-helper.h>
+#include <asm/octeon/cvmx-hwfau.h>
+#include <asm/octeon/octeon-ethernet-user.h>
+
+/**
+ * This is the definition of the Ethernet driver's private
+ * driver state stored in netdev_priv(dev).
+ */
+struct octeon_ethernet {
+	struct rb_node ipd_tree;
+	int key;
+	int ipd_port;
+	int pko_port;
+	int ipd_pkind;
+	int interface;
+	int interface_port;
+
+	/* My netdev. */
+	struct net_device *netdev;
+	/* My location in the cvm_oct_list */
+	struct list_head list;
+
+	/* Type of port. This is one of the enums in
+	 * cvmx_helper_interface_mode_t
+	 */
+	int imode;
+
+	unsigned int rx_strip_fcs:1;
+	unsigned int tx_timestamp_hw:1;
+	unsigned int rx_timestamp_hw:1;
+	unsigned int tx_multiple_queues:1;
+	unsigned int tx_lockless:1;
+
+	/* Base address for accessing GMX registers */
+	u64 gmx_base;
+
+	/* Optional intecept callback defined above */
+	cvm_oct_callback_t      intercept_cb;
+
+	/* Number of elements in tx_queue below */
+	int                     num_tx_queues;
+
+	/* SRIO ports add this header for the SRIO block */
+	u64 srio_tx_header;
+
+	struct {
+		/* PKO hardware queue for the port */
+		int	queue;
+		/* Hardware fetch and add to count outstanding tx buffers */
+		int	fau;
+	} tx_queue[32];
+
+	struct phy_device *phydev;
+	int last_link;
+	/* Last negotiated link state */
+	u64 link_info;
+	/* Called periodically to check link status */
+	spinlock_t poll_lock;
+	void (*poll) (struct net_device *dev);
+	void (*link_change) (struct octeon_ethernet *, cvmx_helper_link_info_t);
+	struct delayed_work	port_periodic_work;
+	struct work_struct	port_work;	/* may be unused. */
+	struct device_node	*of_node;
+	u64 last_tx_octets;
+	u32 last_tx_packets;
+	struct list_head srio_bcast;
+	struct notifier_block	hw_status_notifier;
+};
+
+struct octeon_ethernet_srio_bcast_target {
+	struct list_head list;
+	u16 destid;
+};
+
+struct octeon_ethernet *cvm_oct_dev_for_port(int);
+
+int cvm_oct_free_work(void *work_queue_entry);
+
+int cvm_oct_rgmii_init(struct net_device *dev);
+int cvm_oct_rgmii_open(struct net_device *dev);
+int cvm_oct_rgmii_stop(struct net_device *dev);
+
+int cvm_oct_sgmii_init(struct net_device *dev);
+void cvm_oct_sgmii_uninit(struct net_device *dev);
+int cvm_oct_sgmii_open(struct net_device *dev);
+int cvm_oct_sgmii_stop(struct net_device *dev);
+
+int cvm_oct_spi_init(struct net_device *dev);
+void cvm_oct_spi_uninit(struct net_device *dev);
+
+int cvm_oct_xaui_init(struct net_device *dev);
+int cvm_oct_xaui_open(struct net_device *dev);
+int cvm_oct_xaui_stop(struct net_device *dev);
+
+int cvm_oct_srio_init(struct net_device *dev);
+int cvm_oct_srio_open(struct net_device *dev);
+int cvm_oct_srio_stop(struct net_device *dev);
+int cvm_oct_xmit_srio(struct sk_buff *skb, struct net_device *dev);
+int cvm_oct_srio_set_mac_address(struct net_device *dev, void *addr);
+int cvm_oct_srio_change_mtu(struct net_device *dev, int new_mtu);
+struct net_device_stats *cvm_oct_srio_get_stats(struct net_device *dev);
+
+int cvm_oct_common_init(struct net_device *dev);
+int cvm_oct_common_stop(struct net_device *dev);
+
+void cvm_oct_set_carrier(struct octeon_ethernet *priv,
+				cvmx_helper_link_info_t link_info);
+void cvm_oct_adjust_link(struct net_device *dev);
+
+int cvm_oct_ioctl(struct net_device *dev, struct ifreq *rq, int cmd);
+int cvm_oct_phy_setup_device(struct net_device *dev);
+
+int cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev);
+int cvm_oct_xmit_lockless(struct sk_buff *skb, struct net_device *dev);
+
+void cvm_oct_poll_controller(struct net_device *dev);
+void cvm_oct_rx_initialize(int);
+void cvm_oct_rx_shutdown0(void);
+void cvm_oct_rx_shutdown1(void);
+
+int cvm_oct_mem_fill_fpa(int pool, int elements);
+int cvm_oct_mem_empty_fpa(int pool, int elements);
+int cvm_oct_alloc_fpa_pool(int pool, int size);
+int cvm_oct_release_fpa_pool(int pool);
+void cvm_oct_mem_cleanup(void);
+
+extern const struct ethtool_ops cvm_oct_ethtool_ops;
+
+extern int rx_cpu_factor;
+extern int octeon_recycle_tx;
+extern int packet_pool;
+extern int wqe_pool;
+extern int output_pool;
+extern int always_use_pow;
+extern int pow_send_group;
+extern int pow_receive_group;
+extern char pow_send_list[];
+extern struct list_head cvm_oct_list;
+extern struct octeon_ethernet *cvm_oct_by_pkind[];
+extern struct octeon_ethernet *cvm_oct_by_srio_mbox[4][4];
+
+extern struct workqueue_struct *cvm_oct_poll_queue;
+extern atomic_t cvm_oct_poll_queue_stopping;
+
+extern int max_rx_cpus;
+extern int rx_napi_weight;
+
+static inline void cvm_oct_rx_refill_pool(int fill_threshold)
+{
+	int number_to_free;
+	int num_freed;
+	/* Refill the packet buffer pool */
+	number_to_free =
+		cvmx_hwfau_fetch_and_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);
+
+	if (number_to_free > fill_threshold) {
+		cvmx_hwfau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE,
+				      -number_to_free);
+		num_freed = cvm_oct_mem_fill_fpa(packet_pool, number_to_free);
+		if (num_freed != number_to_free) {
+			cvmx_hwfau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE,
+					number_to_free - num_freed);
+		}
+	}
+}
+
+/**
+ * cvm_oct_get_buffer_ptr - convert packet data address to pointer
+ * @pd: Packet data hardware address
+ *
+ * Returns Packet buffer pointer
+ */
+static inline void *cvm_oct_get_buffer_ptr(union cvmx_buf_ptr pd)
+{
+	return phys_to_virt(((pd.s.addr >> 7) - pd.s.back) << 7);
+}
+
+static inline struct sk_buff **cvm_oct_packet_to_skb(void *packet)
+{
+	char *p = packet;
+	return (struct sk_buff **)(p - sizeof(void *));
+}
+
+#define CVM_OCT_SKB_TO_FPA_PADDING (128 + sizeof(void *) - 1)
+
+static inline u8 *cvm_oct_get_fpa_head(struct sk_buff *skb)
+{
+	return (u8 *)((unsigned long)(skb->head + CVM_OCT_SKB_TO_FPA_PADDING) & ~0x7ful);
+}
+
+#endif
diff --git a/drivers/net/ethernet/octeon/octeon-pow-ethernet.c b/drivers/net/ethernet/octeon/octeon-pow-ethernet.c
new file mode 100644
index 0000000..28bfa2c
--- /dev/null
+++ b/drivers/net/ethernet/octeon/octeon-pow-ethernet.c
@@ -0,0 +1,816 @@
+/*
+ *   Octeon POW Ethernet Driver
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2005-2012 Cavium, Inc.
+ */
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/interrupt.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/in.h>
+#include <linux/ip.h>
+#include <linux/string.h>
+#include <linux/delay.h>
+
+#include <asm/octeon/octeon.h>
+#include <asm/octeon/cvmx.h>
+#include <asm/octeon/cvmx-fpa1.h>
+#include <asm/octeon/cvmx-fpa3.h>
+#include <asm/octeon/cvmx-fpa.h>
+#include <asm/octeon/cvmx-pow.h>
+#include <asm/octeon/cvmx-wqe.h>
+#include <asm/octeon/cvmx-pow-defs.h>
+#include <asm/octeon/cvmx-sso-defs.h>
+
+#define VIRTUAL_PORT    63	/* Value to put in work->ipprt */
+#define CN78XX_SSO_INTSN_EXE 0x61
+
+#define DEBUGPRINT(format, ...) do {					\
+		if (printk_ratelimit())					\
+			printk(format, ##__VA_ARGS__);			\
+	} while (0)
+
+#define DEV_NAME "octeon-pow-ethernet"
+
+static int receive_group = -1;
+module_param(receive_group, int, 0444);
+MODULE_PARM_DESC(receive_group,
+		 " 0-16 POW group to receive packets from. This must be unique in\n"
+		 "\t\tthe system. If you don't specify a value, the core ID will\n"
+		 "\t\tbe used.");
+
+static int broadcast_groups;
+module_param(broadcast_groups, int, 0644);
+MODULE_PARM_DESC(broadcast_groups,
+		 " Bitmask of groups to send broadcasts to. This MUST be specified.\n"
+		 "\t\tWARNING: Be careful to not send broadcasts to groups that aren't\n"
+		 "\t\tread otherwise you may fill the POW and stop receiving packets.\n");
+
+
+
+static int ptp_rx_group = -1;
+module_param(ptp_rx_group, int, 0444);
+MODULE_PARM_DESC(ptp_rx_group,
+		 "For the PTP POW device, 0-64 POW group to receive packets from.\n"
+		 "\t\tIf you don't specify a value, the 'pow0' device will not be created\n.");
+
+static int ptp_tx_group = -1;
+module_param(ptp_tx_group, int, 0444);
+MODULE_PARM_DESC(ptp_tx_group,
+		 "For the PTP POW device, 0-64 POW group to transmit packets to.\n"
+		 "\t\tIf you don't specify a value, the 'pow0' device will not be created\n.");
+
+static int pki_packet_pool = 0;
+module_param(pki_packet_pool, int, 0644);
+MODULE_PARM_DESC(pki_packet_pool,
+		 "Pool to use for transmit/receive buffer alloc/frees.\n");
+
+static int reverse_endian;
+module_param(reverse_endian, int, 0444);
+MODULE_PARM_DESC(reverse_endian,
+		 "Link partner is running with different endianness (set on only one end of the link).\n");
+
+static void *memcpy_re_to(void *d, const void *s, size_t n)
+{
+	u8 *dst = d;
+	const u8 *src = s;
+	while (n) {
+		u8 *pd = (u8 *)((unsigned long)dst ^ 7);
+		*pd = *src;
+		n--;
+		dst++;
+		src++;
+	}
+	return d;
+}
+static void *memcpy_re_from(void *d, const void *s, size_t n)
+{
+	u8 *dst = d;
+	const u8 *src = s;
+	while (n) {
+		u8 *ps = (u8 *)((unsigned long)src ^ 7);
+		*dst = *ps;
+		n--;
+		dst++;
+		src++;
+	}
+	return d;
+}
+static void * (*octeon_pow_copy_to)(void *d, const void *s, size_t n);
+static void * (*octeon_pow_copy_from)(void *d, const void *s, size_t n);
+
+/*
+ * This is the definition of the Ethernet driver's private
+ * driver state.
+ */
+struct octeon_pow {
+	u64 tx_mask;
+	int rx_group;
+	bool is_ptp;
+	int rx_irq;
+	int numa_node;
+};
+
+static int fpa_wqe_pool = 1;	/* HW FPA pool to use for work queue entries */
+static int fpa_packet_pool;	/* HW FPA pool to use for packet buffers */
+static int fpa_packet_pool_size = 2048;	/* Size of the packet buffers */
+static struct net_device *octeon_pow_oct_dev;
+static struct net_device *octeon_pow_ptp_dev;
+static int octeon_pow_num_groups;
+
+void *work_to_skb(void *work)
+{
+	return work - (octeon_has_feature(OCTEON_FEATURE_PKI) ? 0x80 : 0);
+}
+
+/*
+ * Given a packet data address, return a pointer to the
+ * beginning of the packet buffer.
+ */
+static void *get_buffer_ptr(union cvmx_buf_ptr packet_ptr)
+{
+	return phys_to_virt(((packet_ptr.s.addr >> 7) -
+			     packet_ptr.s.back) << 7);
+}
+
+uint64_t oct_get_packet_ptr(cvmx_wqe_t *work)
+{
+	if (octeon_has_feature(OCTEON_FEATURE_PKI)) {
+		return cvmx_wqe_get_pki_pkt_ptr(work).u64;
+	} else {
+		if ((work->word2.s.bufs > 0) || (work->word2.s.software))
+		    return work->packet_ptr.s.addr;
+		return (cvmx_ptr_to_phys(work) + 32);
+	}
+}
+
+static int octeon_pow_free_work(cvmx_wqe_t *work)
+{
+	int segments = cvmx_wqe_get_bufs(work);
+
+	if (octeon_has_feature(OCTEON_FEATURE_PKI)) {
+		/* FIXME */
+		int segments = cvmx_wqe_get_bufs(work);
+		if (segments > 1) pr_warn(DEV_NAME " WARNING: segments > 1 not yet supported.\n");
+
+		cvmx_fpa_free(work_to_skb(work), cvmx_wqe_get_aura(work), 0);
+	} else {
+		while (segments--) {
+			union cvmx_buf_ptr segment_ptr = work->packet_ptr;
+			union cvmx_buf_ptr next_ptr =
+				*(union cvmx_buf_ptr *)phys_to_virt(segment_ptr.s.addr - 8);
+			if (unlikely(!segment_ptr.s.i))
+				cvmx_fpa_free(get_buffer_ptr(segment_ptr),
+					     segment_ptr.s.pool, 0);
+			segment_ptr = next_ptr;
+		}
+		cvmx_fpa_free(work, fpa_wqe_pool, 0);
+	}
+
+	return 0;
+}
+
+static int octeon_pow_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+	struct octeon_pow *priv;
+	cvmx_wqe_t *work = NULL;
+	void *packet_buffer = NULL;
+	void *copy_location;
+	u64 send_group_mask;
+	int send_group;
+
+	priv = netdev_priv(dev);
+
+	/* Any unknown MAC address goes to all groups in the module
+	 * param broadcast_groups. Known MAC addresses use the low
+	 * order dest mac byte as the group number.
+	 */
+	if (!priv->is_ptp && ((*(uint64_t *) (skb->data) >> 16) < 0x01ff))
+		send_group_mask = 1ull << (skb->data[5] & (octeon_pow_num_groups - 1));
+	else
+		send_group_mask = priv->tx_mask;
+
+	/* Remove self from group mask */
+	send_group_mask &= ~(1 << priv->rx_group);
+
+	/* It is ugly, but we need to send multiple times for
+	 * broadcast packets. The hardware doesn't support submitting
+	 * work to multiple groups
+	 */
+	for (send_group = 0; send_group < octeon_pow_num_groups; send_group++) {
+		/* Don't transmit to groups not in our send_group_mask */
+		if (likely((send_group_mask & (1ULL << send_group)) == 0))
+			continue;
+
+		/* Get a work queue entry */
+		work = cvmx_fpa_alloc(fpa_wqe_pool) +
+			(octeon_has_feature(OCTEON_FEATURE_FPA3) ? 0x80 : 0);
+		if (unlikely(work == NULL)) {
+			DEBUGPRINT("%s: Failed to allocate a work queue entry\n",
+				   dev->name);
+			goto fail;
+		}
+
+		/* Get a packet buffer */
+		if (octeon_has_feature(OCTEON_FEATURE_PKI))
+			/* octeon3-ethernet uses a different fpa/packet system */
+			packet_buffer = ((void*)work) + 0x80;
+		else
+			packet_buffer = cvmx_fpa_alloc(fpa_packet_pool);
+		if (unlikely(packet_buffer == NULL)) {
+			DEBUGPRINT("%s: Failed to allocate a packet buffer\n",
+				   dev->name);
+			goto fail;
+		}
+
+		/* Calculate where we need to copy the data to. We
+		 * need to leave 8 bytes for a next pointer
+		 * (unused). Then we need to align the IP packet src
+		 * and dest into the same 64bit word.
+		 */
+		copy_location = packet_buffer + sizeof(uint64_t) + 6;
+
+		/* Fail if the packet won't fit in a single buffer */
+		if (unlikely
+		    (copy_location + skb->len >
+		     packet_buffer + fpa_packet_pool_size)) {
+			DEBUGPRINT("%s: Packet too large for FPA buffer\n",
+				   dev->name);
+			goto fail;
+		}
+
+		octeon_pow_copy_to(copy_location, skb->data, skb->len);
+
+		/* Fill in some of the work queue fields. We may need
+		 * to add more if the software at the other end needs
+		 * them.
+		 */
+		work->word0.u64 = 0;
+		work->word2.u64 = 0;	/* Default to zero. Sets of zero later
+					   are commented out */
+#if 0
+		work->hw_chksum = skb->csum;
+#endif
+
+		cvmx_wqe_set_len(work, skb->len);
+		cvmx_wqe_set_port(work, VIRTUAL_PORT);
+		cvmx_wqe_set_qos(work, 0);
+		cvmx_wqe_set_grp(work, send_group);
+		cvmx_wqe_set_tt(work, 2);
+		cvmx_wqe_set_tag(work, 0);
+
+		cvmx_wqe_set_bufs(work, 1);
+		cvmx_wqe_set_aura(work, fpa_wqe_pool);
+
+		if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
+			/* We use a 78xx format wqe where necessary */
+			union cvmx_buf_ptr_pki pki_ptr;
+			cvmx_wqe_78xx_t *wqe = (cvmx_wqe_78xx_t*) work;
+			pki_ptr.u64 = 0;
+			pki_ptr.addr = virt_to_phys(copy_location);
+			pki_ptr.packet_outside_wqe = 0;
+			pki_ptr.size = skb->len;
+			wqe->packet_ptr.u64 = pki_ptr.u64;
+			/* Mark errata as handled to prevent additional byteswap */
+			wqe->pki_errata20776 = 1;
+		} else {
+			work->packet_ptr.u64 = 0;
+			work->packet_ptr.s.addr = virt_to_phys(copy_location);
+			work->packet_ptr.s.pool = fpa_packet_pool;
+			work->packet_ptr.s.size = fpa_packet_pool_size;
+			work->packet_ptr.s.back = (copy_location - packet_buffer) >> 7;
+		}
+		if (skb->protocol == htons(ETH_P_IP)) {
+			cvmx_wqe_set_l3_offset(work, 14);
+			cvmx_wqe_set_l4_udp(work, ip_hdr(skb)->protocol == IPPROTO_UDP);
+			if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE) ||
+				(ip_hdr(skb)->protocol == IPPROTO_TCP))
+				cvmx_wqe_set_l4_tcp(work, ip_hdr(skb)->protocol == IPPROTO_TCP);
+			cvmx_wqe_set_l3_frag(work, !((ip_hdr(skb)->frag_off == 0)
+						     || (ip_hdr(skb)->frag_off ==
+							 1 << 14)));
+			cvmx_wqe_set_l2_bcast(work, skb->pkt_type == PACKET_BROADCAST);
+			cvmx_wqe_set_l2_mcast(work, skb->pkt_type == PACKET_MULTICAST);
+#if 0
+			work->word2.s.vlan_valid = 0;	/* FIXME */
+			work->word2.s.vlan_cfi = 0;	/* FIXME */
+			work->word2.s.vlan_id = 0;	/* FIXME */
+			work->word2.s.dec_ipcomp = 0;	/* FIXME */
+			if (!octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE))
+				work->word2.s.IP_exc = 0; /* Assume Linux is sending
+							     a good packet */
+			work->word2.s.IP_exc = 0; /* Assume Linux is sending
+						     a good packet */
+			work->word2.s.not_IP = 0; /* This is an IP packet */
+			work->word2.s.rcv_error = 0; /* No error, packet is
+							internal */
+			work->word2.s.err_code = 0;  /* No error, packet is
+							internal */
+#endif
+
+			/* When copying the data, include 4 bytes of the
+			   ethernet header to align the same way hardware does */
+			octeon_pow_copy_to(work->packet_data, skb->data + 10,
+			       sizeof(work->packet_data));
+		} else {
+#if 0
+			work->word2.snoip.is_rarp =
+				skb->protocol == htons(ETH_P_RARP);
+			work->word2.snoip.is_arp =
+				skb->protocol == htons(ETH_P_ARP);
+			work->word2.snoip.is_bcast =
+				(skb->pkt_type == PACKET_BROADCAST);
+			work->word2.snoip.is_mcast =
+				(skb->pkt_type == PACKET_MULTICAST);
+#endif
+			cvmx_wqe_set_l3_ipv4(work, 0);
+			octeon_pow_copy_to(work->packet_data, skb->data,
+			       sizeof(work->packet_data));
+		}
+
+		/*
+		 * Submit the packet to the POW
+		 * tag: 0
+		 * tag_type: 2
+		 * qos: 0
+		 * grp: send_group
+		 */
+		if (octeon_has_feature(OCTEON_FEATURE_PKI))
+			cvmx_pow_work_submit_node(work, 0, 2, send_group, priv->numa_node);
+		else
+			cvmx_pow_work_submit(work, 0, 2, 0, send_group);
+		work = NULL;
+		packet_buffer = NULL;
+	}
+
+	dev->stats.tx_packets++;
+	dev->stats.tx_bytes += skb->len;
+	dev_kfree_skb(skb);
+	return NETDEV_TX_OK;
+
+fail:
+	if (work)
+		cvmx_fpa_free(work_to_skb(work), fpa_wqe_pool, 0);
+	if (packet_buffer && !octeon_has_feature(OCTEON_FEATURE_PKI))
+		cvmx_fpa_free(packet_buffer, fpa_packet_pool, 0);
+	dev->stats.tx_dropped++;
+	dev_kfree_skb(skb);
+	return NETDEV_TX_OK;
+}
+
+
+/**
+ * Interrupt handler. The interrupt occurs whenever the POW
+ * transitions from 0->1 packets in our group.
+ *
+ * @param cpl
+ * @param dev_id
+ * @param regs
+ * @return
+ */
+static irqreturn_t octeon_pow_interrupt(int cpl, void *dev_id)
+{
+	const uint64_t coreid = cvmx_get_core_num();
+	struct net_device *dev = (struct net_device *) dev_id;
+	struct octeon_pow *priv;
+	uint64_t old_group_mask = 0;
+	cvmx_wqe_t *work;
+	struct sk_buff *skb;
+
+	priv = netdev_priv(dev);
+
+	/* Make sure any userspace operations are complete */
+	asm volatile ("synciobdma" : : : "memory");
+
+	if (octeon_has_feature(OCTEON_FEATURE_PKI)) {
+		/* Can get-work from group explicitly here */
+	} else if (OCTEON_IS_MODEL(OCTEON_CN68XX)) {
+		/* Only allow work for our group */
+		old_group_mask = cvmx_read_csr(CVMX_SSO_PPX_GRP_MSK(coreid));
+		cvmx_write_csr(CVMX_SSO_PPX_GRP_MSK(coreid),
+			       1ull << priv->rx_group);
+		/* Read it back so it takes effect before we request work */
+		cvmx_read_csr(CVMX_SSO_PPX_GRP_MSK(coreid));
+	} else {
+		/* Only allow work for our group */
+		old_group_mask = cvmx_read_csr(CVMX_POW_PP_GRP_MSKX(coreid));
+		cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(coreid), 1 << priv->rx_group);
+	}
+	while (1) {
+		if (octeon_has_feature(OCTEON_FEATURE_PKI))
+			work = cvmx_sso_work_request_grp_sync_nocheck(priv->rx_group,
+				CVMX_POW_NO_WAIT);
+		else
+			work = cvmx_pow_work_request_sync(0);
+		if (work == NULL)
+			break;
+
+		/* Silently drop packets if we aren't up */
+		if ((dev->flags & IFF_UP) == 0) {
+			octeon_pow_free_work(work);
+			continue;
+		}
+
+		/* Throw away all packets with receive errors */
+		if (unlikely(cvmx_wqe_get_rcv_err(work))) {
+			DEBUGPRINT("%s: Receive error code %d, packet dropped\n",
+				   dev->name, cvmx_wqe_get_rcv_err(work));
+			octeon_pow_free_work(work);
+			dev->stats.rx_errors++;
+			continue;
+		}
+
+		/* We have to copy the packet. First allocate an skbuff for it */
+		skb = dev_alloc_skb(cvmx_wqe_get_len(work));
+		if (!skb) {
+			DEBUGPRINT("%s: Failed to allocate skbuff, packet dropped\n",
+				   dev->name);
+			octeon_pow_free_work(work);
+			dev->stats.rx_dropped++;
+			continue;
+		}
+
+		/* Check if we've received a packet that was entirely
+		 * stored the work entry. This is untested
+		 */
+		if (unlikely(cvmx_wqe_get_bufs(work) == 0)) {
+			int len = cvmx_wqe_get_len(work);
+			DEBUGPRINT("%s: Received a work with work->word2.s.bufs=0, untested\n",
+				   dev->name);
+			octeon_pow_copy_from(skb_put(skb, len),
+					     phys_to_virt(oct_get_packet_ptr(work)), len);
+		} else {
+			int segments = cvmx_wqe_get_bufs(work);
+			uint64_t buf_desc = oct_get_packet_ptr(work);
+			int len = cvmx_wqe_get_len(work);
+			while (segments--) {
+				int segment_size;
+				uint64_t pkt_ptr;
+				if (octeon_has_feature(OCTEON_FEATURE_PKI)) {
+					cvmx_buf_ptr_pki_t pki_ptr;
+					pki_ptr.u64 = buf_desc;
+					segment_size = pki_ptr.size;
+					pkt_ptr = pki_ptr.addr;
+					buf_desc = *((uint64_t*)phys_to_virt(pki_ptr.addr - 8));
+				} else {
+					cvmx_buf_ptr_t buf_ptr;
+					union cvmx_buf_ptr next_ptr;
+					buf_ptr.u64 = buf_desc;
+					next_ptr = *(union cvmx_buf_ptr *)
+						phys_to_virt(buf_ptr.s.addr - 8);
+					/* Octeon Errata PKI-100: The segment size is
+					   wrong. Until it is fixed, calculate the
+					   segment size based on the packet pool buffer
+					   size. When it is fixed, the following line
+					   should be replaced with this one: int
+					   segment_size = segment_ptr.s.size; */
+					segment_size =
+						fpa_packet_pool_size -
+						(buf_ptr.s.addr -
+						 (((buf_ptr.s.addr >> 7) -
+						   buf_ptr.s.back) << 7));
+					/* Don't copy more than what is left in the
+					   packet */
+					pkt_ptr = buf_ptr.s.addr;
+					buf_desc = next_ptr.u64;
+				}
+				if (segment_size > len)
+					segment_size = len;
+				/* Copy the data into the packet */
+				octeon_pow_copy_from(skb_put(skb, segment_size),
+						     phys_to_virt(pkt_ptr),
+						     segment_size);
+				/* Reduce the amount of bytes left to copy */
+				len -= segment_size;
+			}
+		}
+		octeon_pow_free_work(work);
+		skb->protocol = eth_type_trans(skb, dev);
+		skb->dev = dev;
+		skb->ip_summed = CHECKSUM_NONE;
+		dev->stats.rx_bytes += skb->len;
+		dev->stats.rx_packets++;
+		netif_rx(skb);
+	}
+
+	if (octeon_has_feature(OCTEON_FEATURE_PKI)) {
+		/* Clear interrupt */
+		cvmx_write_csr_node(priv->numa_node, CVMX_SSO_GRPX_INT(priv->rx_group), 2);
+	} else if (OCTEON_IS_MODEL(OCTEON_CN68XX)) {
+		/* Restore the original POW group mask */
+		cvmx_write_csr(CVMX_SSO_PPX_GRP_MSK(coreid), old_group_mask);
+		/* Read it back so it takes effect before ?? */
+		cvmx_read_csr(CVMX_SSO_PPX_GRP_MSK(coreid));
+
+		/* Acknowledge the interrupt */
+		cvmx_write_csr(CVMX_SSO_WQ_INT, 1ull << priv->rx_group);
+	} else {
+		/* Restore the original POW group mask */
+		cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(coreid), old_group_mask);
+
+		/* Acknowledge the interrupt */
+		cvmx_write_csr(CVMX_POW_WQ_INT, 1ull << priv->rx_group);
+	}
+	return IRQ_HANDLED;
+}
+
+#ifdef CONFIG_NET_POLL_CONTROLLER
+
+static void octeon_pow_poll(struct net_device *dev)
+{
+	octeon_pow_interrupt(0, dev);
+}
+#endif
+
+static int octeon_pow_open(struct net_device *dev)
+{
+	int r;
+	struct octeon_pow *priv = netdev_priv(dev);
+
+	/* Clear the statistics whenever the interface is brought up */
+	memset(&dev->stats, 0, sizeof(dev->stats));
+
+	if (octeon_has_feature(OCTEON_FEATURE_PKI)) {
+		int sso_intsn = (CN78XX_SSO_INTSN_EXE << 12) | priv->rx_group;
+		struct irq_domain *d = octeon_irq_get_block_domain(priv->numa_node,
+								   sso_intsn);
+		priv->rx_irq = irq_create_mapping(d, sso_intsn);
+		if (!priv->rx_irq) {
+			netdev_err(dev, "ERROR: Couldn't map hwirq: %x\n",
+				   sso_intsn);
+			return -EINVAL;
+		}
+	} else
+		priv->rx_irq = OCTEON_IRQ_WORKQ0 + priv->rx_group;
+	/* Register an IRQ hander for to receive POW interrupts */
+	r = request_irq(priv->rx_irq, octeon_pow_interrupt, 0, dev->name, dev);
+	if (r)
+		return r;
+
+	/* Enable POW interrupt when our port has at least one packet */
+	if (octeon_has_feature(OCTEON_FEATURE_PKI)) {
+		union cvmx_sso_grpx_int_thr thr;
+		union cvmx_sso_grpx_int grp_int;
+		thr.u64 = 0;
+		thr.cn78xx.ds_thr = 1;
+		thr.cn78xx.iaq_thr = 1;
+		cvmx_write_csr_node(priv->numa_node, CVMX_SSO_GRPX_INT_THR(priv->rx_group),
+				    thr.u64);
+		grp_int.u64 = 0;
+		grp_int.s.exe_int = 1;
+		cvmx_write_csr_node(priv->numa_node, CVMX_SSO_GRPX_INT(priv->rx_group), grp_int.u64);
+	} else if (OCTEON_IS_MODEL(OCTEON_CN68XX)) {
+		union cvmx_sso_wq_int_thrx thr;
+		thr.u64 = 0;
+		thr.s.iq_thr = 1;
+		thr.s.ds_thr = 1;
+		cvmx_write_csr(CVMX_SSO_WQ_INT_THRX(priv->rx_group), thr.u64);
+	} else {
+		union cvmx_pow_wq_int_thrx thr;
+		thr.u64 = 0;
+		thr.s.iq_thr = 1;
+		thr.s.ds_thr = 1;
+		cvmx_write_csr(CVMX_POW_WQ_INT_THRX(priv->rx_group), thr.u64);
+	}
+
+	return 0;
+}
+
+static int octeon_pow_stop(struct net_device *dev)
+{
+	struct octeon_pow *priv = netdev_priv(dev);
+
+	/* Disable POW interrupt */
+	if (octeon_has_feature(OCTEON_FEATURE_PKI))
+		cvmx_write_csr_node(priv->numa_node, CVMX_SSO_GRPX_INT_THR(priv->rx_group), 0);
+	else if (OCTEON_IS_MODEL(OCTEON_CN68XX))
+		cvmx_write_csr(CVMX_SSO_WQ_INT_THRX(priv->rx_group), 0);
+	else
+		cvmx_write_csr(CVMX_POW_WQ_INT_THRX(priv->rx_group), 0);
+
+	/* Free the interrupt handler */
+	free_irq(priv->rx_irq, dev);
+	return 0;
+}
+
+/**
+ * Per network device initialization
+ *
+ * @param dev    Device to initialize
+ * @return Zero on success
+ */
+static int octeon_pow_init(struct net_device *dev)
+{
+	struct octeon_pow *priv = netdev_priv(dev);
+
+	dev->features |= NETIF_F_LLTX;	/* We do our own locking, Linux doesn't
+					   need to */
+	dev->dev_addr[0] = 0;
+	dev->dev_addr[1] = 0;
+	dev->dev_addr[2] = 0;
+	dev->dev_addr[3] = 0;
+	dev->dev_addr[4] = priv->is_ptp ? 3 : 1;
+	dev->dev_addr[5] = priv->rx_group;
+	priv->numa_node = cvmx_get_node_num();
+	return 0;
+}
+
+static const struct net_device_ops octeon_pow_netdev_ops = {
+	.ndo_init = octeon_pow_init,
+	.ndo_open = octeon_pow_open,
+	.ndo_stop = octeon_pow_stop,
+	.ndo_start_xmit = octeon_pow_xmit,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller =  octeon_pow_poll,
+#endif
+};
+
+
+/*
+ * Module/ driver initialization. Creates the linux network
+ * devices.
+ *
+ * @return Zero on success
+ */
+static int __init octeon_pow_mod_init(void)
+{
+	struct octeon_pow *priv;
+	u64 allowed_group_mask;
+
+	if (reverse_endian) {
+		octeon_pow_copy_to = memcpy_re_to;
+		octeon_pow_copy_from = memcpy_re_from;
+	} else {
+		octeon_pow_copy_to = memcpy;
+		octeon_pow_copy_from = memcpy;
+	}
+
+	if (octeon_has_feature(OCTEON_FEATURE_PKI)) {
+		/* Actually 256 groups total, only 64 currently supported */
+		octeon_pow_num_groups = 64;
+		allowed_group_mask = 0xffffffffffffffffull;
+	} else if (OCTEON_IS_MODEL(OCTEON_CN68XX)) {
+		octeon_pow_num_groups = 64;
+		allowed_group_mask = 0xffffffffffffffffull;
+	} else {
+		octeon_pow_num_groups = 16;
+		allowed_group_mask = 0xffffull;
+	}
+
+	/* If a receive group isn't specified, default to the core id */
+	if (receive_group < 0)
+		receive_group = cvmx_get_core_num();
+
+
+	if ((receive_group > octeon_pow_num_groups)) {
+		pr_err(DEV_NAME " ERROR: Invalid receive group. Must be 0-%d\n",
+		       octeon_pow_num_groups - 1);
+		return -1;
+	}
+
+	if (!broadcast_groups) {
+		pr_err(DEV_NAME " ERROR: You must specify a broadcast group mask.\n");
+		return -1;
+	}
+
+	if ((broadcast_groups & allowed_group_mask) != broadcast_groups) {
+		pr_err(DEV_NAME " ERROR: Invalid broadcast group mask.\n");
+		return -1;
+	}
+
+	if ((ptp_rx_group >= 0 && ptp_tx_group < 0) || (ptp_rx_group < 0 && ptp_tx_group >= 0)) {
+		pr_err(DEV_NAME " ERROR: Both ptp_rx_group AND ptp_tx_group must be set.\n");
+		return -1;
+	}
+
+	if (ptp_rx_group >= 0 && ptp_tx_group == ptp_rx_group) {
+		pr_err(DEV_NAME " ERROR: ptp_rx_group and ptp_tx_group must differ.\n");
+		return -1;
+	}
+
+	if (ptp_rx_group >= octeon_pow_num_groups || ptp_tx_group >= octeon_pow_num_groups) {
+		pr_err(DEV_NAME " ERROR: ptp_rx_group and ptp_tx_group. Must be 0-%d\n",
+		       octeon_pow_num_groups - 1);
+		return -1;
+	}
+
+	if (receive_group == ptp_rx_group) {
+		pr_err(DEV_NAME " ERROR: ptp_rx_group(%d) and  receive_group(%d) must differ.\n",
+			ptp_rx_group, receive_group);
+		return -1;
+	}
+
+	if (octeon_has_feature(OCTEON_FEATURE_PKI) && (pki_packet_pool == 0)) {
+		pr_err(DEV_NAME " ERROR: pki_packet_pool must be specified for CN78XX.\n");
+	}
+
+	pr_info("Octeon POW only ethernet driver\n");
+
+	/* Setup is complete, create the virtual ethernet devices */
+	octeon_pow_oct_dev = alloc_etherdev(sizeof(struct octeon_pow));
+	if (octeon_pow_oct_dev == NULL) {
+		pr_err(DEV_NAME " ERROR: Failed to allocate ethernet device\n");
+		return -1;
+	}
+
+	octeon_pow_oct_dev->netdev_ops = &octeon_pow_netdev_ops;
+	strcpy(octeon_pow_oct_dev->name, "oct%d");
+
+	/* Initialize the device private structure. */
+	priv = netdev_priv(octeon_pow_oct_dev);
+	priv->rx_group = receive_group;
+	priv->tx_mask = broadcast_groups;
+	priv->numa_node = cvmx_get_node_num();
+
+
+	if (octeon_has_feature(OCTEON_FEATURE_PKI)) {
+		/* Spin waiting for another core to setup all the hardware */
+		printk("Waiting for another core to setup the PKI hardware...");
+		while ((cvmx_read_csr_node(priv->numa_node, CVMX_PKI_BUF_CTL) & 1) == 0)
+			mdelay(100);
+
+		printk("Done\n");
+		fpa_packet_pool_size = 2048;
+		fpa_packet_pool = fpa_wqe_pool = pki_packet_pool;
+
+		if (fpa_packet_pool < 0) {
+			netdev_err(octeon_pow_oct_dev, "ERROR: Failed to initialize fpa pool\n");
+			return -1;
+		}
+	} else {
+		/* Spin waiting for another core to setup all the hardware */
+		printk("Waiting for another core to setup the IPD hardware...");
+		while ((cvmx_read_csr(CVMX_IPD_CTL_STATUS) & 1) == 0)
+			mdelay(100);
+
+		printk("Done\n");
+		/* Read the configured size of the FPA packet buffers. This
+		 * way we don't need changes if someone chooses to use a
+		 * different buffer size
+		 */
+		fpa_packet_pool_size = (cvmx_read_csr(CVMX_IPD_PACKET_MBUFF_SIZE) & 0xfff) * 8;
+
+		/* Read the work queue pool */
+		fpa_packet_pool = fpa_wqe_pool = cvmx_read_csr(CVMX_IPD_WQE_FPA_QUEUE) & 7;
+	}
+
+	if (register_netdev(octeon_pow_oct_dev) < 0) {
+		netdev_err(octeon_pow_oct_dev, "ERROR: Failed to register ethernet device\n");
+		free_netdev(octeon_pow_oct_dev);
+		return -1;
+	}
+
+	if (ptp_rx_group < 0)
+		return 0;
+
+	/* Else create a ptp device. */
+	octeon_pow_ptp_dev = alloc_etherdev(sizeof(struct octeon_pow));
+	if (octeon_pow_ptp_dev == NULL) {
+		pr_err(DEV_NAME " ERROR: Failed to allocate ethernet device\n");
+		return -1;
+	}
+
+	octeon_pow_ptp_dev->netdev_ops = &octeon_pow_netdev_ops;
+	strcpy(octeon_pow_ptp_dev->name, "pow%d");
+
+	/* Initialize the device private structure. */
+	priv = netdev_priv(octeon_pow_ptp_dev);
+	priv->rx_group = ptp_rx_group;
+	priv->tx_mask = 1ull << ptp_tx_group;
+	priv->is_ptp = true;
+
+	if (register_netdev(octeon_pow_ptp_dev) < 0) {
+		netdev_err(octeon_pow_ptp_dev, "ERROR: Failed to register ethernet device\n");
+		free_netdev(octeon_pow_ptp_dev);
+		return -1;
+	}
+
+	return 0;
+}
+
+/**
+ * Module / driver shutdown
+ *
+ * @return Zero on success
+ */
+static void __exit octeon_pow_mod_exit(void)
+{
+	/* Free the ethernet devices */
+	unregister_netdev(octeon_pow_oct_dev);
+	free_netdev(octeon_pow_oct_dev);
+	if (octeon_pow_ptp_dev) {
+		unregister_netdev(octeon_pow_ptp_dev);
+		free_netdev(octeon_pow_ptp_dev);
+	}
+}
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Cavium Inc. <support@cavium.com>");
+MODULE_DESCRIPTION("Cavium Inc. OCTEON internal only POW ethernet driver.");
+module_init(octeon_pow_mod_init);
+module_exit(octeon_pow_mod_exit);
diff --git a/drivers/net/ethernet/octeon/octeon3-ethernet.c b/drivers/net/ethernet/octeon/octeon3-ethernet.c
new file mode 100644
index 0000000..a26d445
--- /dev/null
+++ b/drivers/net/ethernet/octeon/octeon3-ethernet.c
@@ -0,0 +1,2041 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2014 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+#include <linux/module.h>
+#include <linux/wait.h>
+#include <linux/rculist.h>
+#include <linux/atomic.h>
+#include <linux/kthread.h>
+#include <linux/interrupt.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/platform_device.h>
+#include <linux/ip.h>
+#include <linux/ipv6.h>
+
+#include <asm/octeon/octeon.h>
+#include <asm/octeon/cvmx-helper-cfg.h>
+#include <asm/octeon/cvmx-helper-pko3.h>
+#include <asm/octeon/cvmx-helper-pki.h>
+#include <asm/octeon/cvmx-pow.h>
+#include <asm/octeon/cvmx-pko3.h>
+#include <asm/octeon/cvmx-fpa3.h>
+#include <asm/octeon/cvmx-app-config.h>
+
+#include <asm/octeon/cvmx-fpa-defs.h>
+#include <asm/octeon/cvmx-sso-defs.h>
+
+#include "octeon-bgx.h"
+
+
+/*
+
+ First buffer:
+
+                              +---SKB---------+
+                              |               |
+                              |               |
+                           +--+--*data        |
+                           |  |               |
+                           |  |               |
+                           |  +---------------+
+                           |       /|\
+                           |        |
+                           |        |
+                          \|/       |
+  WQE - 128 --+-----> +-------------+-------+     -+-
+              |       |    *skb ----+       |      |
+              |       |                     |      |
+              |       |                     |      |
+    WQE_SKIP = 128    |                     |      |
+              |       |                     |      |
+              |       |                     |      |
+              |       |                     |      |
+              |       |                     |      First Skip
+  WQE    -----+-----> +---------------------+      |
+                      |   word 0            |      |
+                      |   word 1            |      |
+                      |   word 2            |      |
+                      |   word 3            |      |
+                      |   word 4            |      |
+                      +---------------------+     -+-
+                 +----+- packet link        |
+                 |    |  packet data        |
+                 |    |                     |
+                 |    |                     |
+                 |    |         .           |
+                 |    |         .           |
+                 |    |         .           |
+                 |    +---------------------+
+                 |
+                 |
+ Later buffers:  |
+                 |
+                 |
+                 |
+                 |
+                 |
+                 |            +---SKB---------+
+                 |            |               |
+                 |            |               |
+                 |         +--+--*data        |
+                 |         |  |               |
+                 |         |  |               |
+                 |         |  +---------------+
+                 |         |       /|\
+                 |         |        |
+                 |         |        |
+                 |        \|/       |
+  WQE - 128 -----+--> +-------------+-------+     -+-
+                 |    |    *skb ----+       |      |
+                 |    |                     |      |
+                 |    |                     |      |
+                 |    |                     |      |
+                 |    |                     |      LATER_SKIP = 128
+                 |    |                     |      |
+                 |    |                     |      |
+                 |    |                     |      |
+                 |    +---------------------+     -+-
+                 |    |  packet link        |
+                 +--> |  packet data        |
+                      |                     |
+                      |                     |
+                      |         .           |
+                      |         .           |
+                      |         .           |
+                      +---------------------+
+ */
+
+#define MAX_TX_QUEUE_DEPTH 512
+#define SSO_INTSN_EXE 0x61
+#define OCTEON3_ETH_MAX_NUMA_NODES 2
+#define MAX_RX_CONTEXTS 32
+
+#define SKB_PTR_OFFSET		0
+#define SKB_AURA_OFFSET		1
+#define SKB_AURA_MAGIC		0xbadc0ffee4dad000UL
+
+#define USE_ASYNC_IOBDMA	1
+#define CVMX_SCR_SCRATCH	0
+
+/*  octeon3_napi_wrapper:	Structure containing the napi structure. This
+ *				structure is added to receive contexts to
+ *				increase the number of threads (napis) receiving
+ *				packets.
+ *
+ *  napi:			Used by the kernel napi core.
+ *  available:			0 = This napi instance is in use.
+ *				1 = This napi instance is available.
+ *  idx:			Napi index per context.
+ *  cpu:			CPU this napi should run on.
+ *  ctx:			Receive context this napi belongs to.
+ */
+struct octeon3_napi_wrapper {
+	struct napi_struct	napi;
+	int			available;
+	int			idx;
+	int			cpu;
+	struct octeon3_rx	*cxt;
+} ____cacheline_aligned_in_smp;
+
+static struct octeon3_napi_wrapper
+napi_wrapper[OCTEON3_ETH_MAX_NUMA_NODES][CVMX_MAX_CORES]
+__cacheline_aligned_in_smp;
+
+struct octeon3_ethernet;
+
+struct octeon3_rx {
+	struct octeon3_napi_wrapper *napiw;
+	DECLARE_BITMAP(napi_idx_bitmap, CVMX_MAX_CORES);
+	spinlock_t napi_idx_lock;
+	struct octeon3_ethernet *parent;
+	int rx_grp;
+	int rx_irq;
+	cpumask_t rx_affinity_hint;
+};
+
+struct octeon3_ethernet {
+	struct bgx_port_netdev_priv bgx_priv; /* Must be first element. */
+	struct list_head list;
+	struct net_device *netdev;
+	struct octeon3_rx rx_cxt[MAX_RX_CONTEXTS];
+	int num_rx_cxt;
+	int pki_laura;
+	int pki_pkind;
+	int pko_queue;
+	int numa_node;
+	int xiface;
+	int port_index;
+	int rx_buf_count;
+	int tx_complete_grp;
+	spinlock_t stat_lock;
+	u64 last_packets;
+	u64 last_octets;
+	u64 last_dropped;
+	atomic64_t rx_packets;
+	atomic64_t rx_octets;
+	atomic64_t rx_dropped;
+	atomic64_t rx_errors;
+	atomic64_t rx_length_errors;
+	atomic64_t rx_crc_errors;
+	atomic64_t tx_packets;
+	atomic64_t tx_octets;
+	atomic64_t tx_dropped;
+	/*
+	 * The following two fields need to be on a different cache line as
+	 * they are updated by pko which invalidates the cache every time it
+	 * updates them. The idea is to prevent other fields from being
+	 * invalidated unnecessarily.
+	 */
+	char cacheline_pad1[CVMX_CACHE_LINE_SIZE];
+	atomic64_t buffers_needed;
+	atomic64_t tx_backlog;
+	char cacheline_pad2[CVMX_CACHE_LINE_SIZE];
+};
+
+static DEFINE_MUTEX(octeon3_eth_init_mutex);
+
+struct octeon3_ethernet_node;
+
+struct octeon3_ethernet_worker {
+	wait_queue_head_t queue;
+	struct task_struct *task;
+	struct octeon3_ethernet_node *oen;
+	atomic_t kick;;
+	int order;
+};
+
+struct octeon3_ethernet_node {
+	bool init_done;
+	bool napi_init_done;
+	int next_cpu_irq_affinity;
+	int numa_node;
+	cvmx_fpa3_pool_t  pki_packet_pool;
+	cvmx_fpa3_pool_t sso_pool;
+	cvmx_fpa3_pool_t pko_pool;
+	cvmx_fpa3_gaura_t sso_aura;
+	cvmx_fpa3_gaura_t pko_aura;
+	int tx_complete_grp;
+	int tx_irq;
+	cpumask_t tx_affinity_hint;
+	struct octeon3_ethernet_worker workers[8];
+	struct mutex device_list_lock;
+	struct list_head device_list;
+	DECLARE_BITMAP(napi_cpu_bitmap, CVMX_MAX_CORES);
+	int napi_max_cpus;
+	spinlock_t napi_alloc_lock;
+};
+
+static int recycle_skbs = 1;
+module_param(recycle_skbs, int, 0644);
+MODULE_PARM_DESC(recycle_skbs, "Allow recycling skbs back to fpa auras.");
+
+static int use_tx_queues;
+module_param(use_tx_queues, int, 0644);
+MODULE_PARM_DESC(use_tx_queues, "Use network layer transmit queues.");
+
+static int wait_pko_response;
+module_param(wait_pko_response, int, 0644);
+MODULE_PARM_DESC(use_tx_queues, "Wait for response after each pko command.");
+
+static int num_packet_buffers = 4096;
+module_param(num_packet_buffers, int, S_IRUGO);
+MODULE_PARM_DESC(num_packet_buffers, "Number of packet buffers to allocate per port.");
+
+static int packet_buffer_size = 2048;
+module_param(packet_buffer_size, int, S_IRUGO);
+MODULE_PARM_DESC(packet_buffer_size, "Size of each RX packet buffer.");
+
+static int rx_contexts = 1;
+module_param(rx_contexts, int, S_IRUGO);
+MODULE_PARM_DESC(rx_contexts, "Number of RX threads per port.");
+
+static struct octeon3_ethernet_node octeon3_eth_node[OCTEON3_ETH_MAX_NUMA_NODES];
+static struct kmem_cache *octeon3_eth_sso_pko_cache;
+
+/* octeon3_eth_sso_pass1_limit:	Near full TAQ can cause hang. When the TAQ
+ *				(Transitory Admission Queue) is near-full, it is
+ *				possible for SSO to hang.
+ *				Workaround: Ensure that the sum of
+ *				SSO_GRP(0..255)_TAQ_THR[MAX_THR] of all used
+ *				groups is <= 1264. This may reduce single-group
+ *				performance when many groups are used.
+ *
+ *  node:			Node to update.
+ *  grp:			SSO group to update.
+ */
+static void octeon3_eth_sso_pass1_limit(int node, int	grp)
+{
+	cvmx_sso_grpx_taq_thr_t	taq_thr;
+	cvmx_sso_taq_add_t	taq_add;
+	int			max_thr;
+	int			rsvd_thr;
+
+	/* Ideally, we would like to divide the maximum number of TAQ buffers
+	 * (1264) among the sso groups in use. However, since we don't know how
+	 * many sso groups are used by code outside this driver we take the
+	 * worst case approach and assume all 256 sso groups must be supported.
+	 */
+	max_thr = 1264 / CVMX_SSO_NUM_XGRP;
+	if (max_thr < 4)
+		max_thr = 4;
+	rsvd_thr = max_thr - 1;
+
+	/* Changes to CVMX_SSO_GRPX_TAQ_THR[rsvd_thr] must also update
+	 * SSO_TAQ_ADD[RSVD_FREE].
+	 */
+	taq_thr.u64 = cvmx_read_csr_node(node, CVMX_SSO_GRPX_TAQ_THR(grp));
+	taq_add.u64 = 0;
+	taq_add.s.rsvd_free = rsvd_thr - taq_thr.s.rsvd_thr;
+
+	taq_thr.s.max_thr = max_thr;
+	taq_thr.s.rsvd_thr = rsvd_thr;
+
+	cvmx_write_csr_node(node, CVMX_SSO_GRPX_TAQ_THR(grp), taq_thr.u64);
+	cvmx_write_csr_node(node, CVMX_SSO_TAQ_ADD, taq_add.u64);
+}
+
+/*
+ * Map auras to the field priv->buffers_needed. Used to speed up packet
+ * transmission.
+ */
+static void *aura2buffers_needed[1024];
+
+static void octeon3_eth_gen_affinity(int node, cpumask_t *mask)
+{
+	int cpu;
+
+	do {
+		cpu = cpumask_next(octeon3_eth_node[node].next_cpu_irq_affinity,
+				   cpu_online_mask);
+		octeon3_eth_node[node].next_cpu_irq_affinity++;
+		if (cpu >= nr_cpu_ids) {
+			octeon3_eth_node[node].next_cpu_irq_affinity = -1;
+			continue;
+		}
+	} while (false);
+	cpumask_clear(mask);
+	cpumask_set_cpu(cpu, mask);
+}
+
+static int octeon3_eth_fpa_pool_init(cvmx_fpa3_pool_t pool, int num_ptrs)
+{
+	void *pool_stack;
+	u64 pool_stack_start, pool_stack_end;
+	union cvmx_fpa_poolx_end_addr limit_addr;
+	union cvmx_fpa_poolx_cfg cfg;
+	int stack_size = (DIV_ROUND_UP(num_ptrs, 29) + 1) * 128;
+
+	cvmx_write_csr_node(pool.node, CVMX_FPA_POOLX_CFG(pool.lpool), 0);
+	cvmx_write_csr_node(pool.node, CVMX_FPA_POOLX_START_ADDR(pool.lpool), 128);
+	limit_addr.u64 = 0;
+	limit_addr.cn78xx.addr = ~0ll;
+	cvmx_write_csr_node(pool.node, CVMX_FPA_POOLX_END_ADDR(pool.lpool), limit_addr.u64);
+
+	pool_stack = kmalloc_node(stack_size, GFP_KERNEL, pool.node);
+	if (!pool_stack)
+		return -ENOMEM;
+
+	pool_stack_start = virt_to_phys(pool_stack);
+	pool_stack_end = round_down(pool_stack_start + stack_size, 128);
+	pool_stack_start = round_up(pool_stack_start, 128);
+
+	cvmx_write_csr_node(pool.node, CVMX_FPA_POOLX_STACK_BASE(pool.lpool), pool_stack_start);
+	cvmx_write_csr_node(pool.node, CVMX_FPA_POOLX_STACK_ADDR(pool.lpool), pool_stack_start);
+	cvmx_write_csr_node(pool.node, CVMX_FPA_POOLX_STACK_END(pool.lpool), pool_stack_end);
+
+	cfg.u64 = 0;
+	cfg.s.l_type = 2; /* Load with DWB */
+	cfg.s.ena = 1;
+	cvmx_write_csr_node(pool.node, CVMX_FPA_POOLX_CFG(pool.lpool), cfg.u64);
+	return 0;
+}
+
+static int octeon3_eth_fpa_aura_init(cvmx_fpa3_pool_t pool, cvmx_fpa3_gaura_t aura, unsigned int limit)
+{
+	int shift;
+	union cvmx_fpa_aurax_cnt_levels cnt_levels;
+	unsigned int drop, pass;
+
+	BUG_ON(aura.node != pool.node);
+
+	limit *= 2; /* allow twice the limit before saturation at zero. */
+
+	cvmx_write_csr_node(aura.node, CVMX_FPA_AURAX_CFG(aura.laura), 0);
+	cvmx_write_csr_node(aura.node, CVMX_FPA_AURAX_CNT_LIMIT(aura.laura), limit);
+	cvmx_write_csr_node(aura.node, CVMX_FPA_AURAX_CNT(aura.laura), limit);
+	cvmx_write_csr_node(aura.node, CVMX_FPA_AURAX_POOL(aura.laura), pool.lpool);
+	cvmx_write_csr_node(aura.node, CVMX_FPA_AURAX_POOL_LEVELS(aura.laura), 0); /* No per-pool RED/Drop */
+
+	shift = 0;
+	while ((limit >> shift) > 255)
+		shift++;
+
+	drop = (limit - num_packet_buffers / 20) >> shift;	  /* 95% */
+	pass = (limit - (num_packet_buffers * 3) / 20) >> shift;  /* 85% */
+
+	cnt_levels.u64 = 0;
+	cnt_levels.s.shift = shift;
+	cnt_levels.s.red_ena = 1;
+	cnt_levels.s.drop = drop;
+	cnt_levels.s.pass = pass;
+	cvmx_write_csr_node(aura.node, CVMX_FPA_AURAX_CNT_LEVELS(aura.laura), cnt_levels.u64);
+
+	return 0;
+}
+
+static int octeon3_eth_sso_init(unsigned int node, int aura)
+{
+	union cvmx_sso_aw_cfg cfg;
+	union cvmx_sso_xaq_aura xaq_aura;
+	union cvmx_sso_err0 err0;
+	union cvmx_sso_grpx_pri grp_pri;
+	int i;
+	int rv = 0;
+
+	grp_pri.u64 = 0;
+	grp_pri.s.weight = 0x3f;
+
+	cfg.u64 = 0;
+	cfg.s.stt = 1;
+	cfg.s.ldt = 1;
+	cfg.s.ldwb = 1;
+	cvmx_write_csr_node(node, CVMX_SSO_AW_CFG, cfg.u64);
+
+	xaq_aura.u64 = 0;
+	xaq_aura.s.laura = aura;
+	xaq_aura.s.node = node;
+	cvmx_write_csr_node(node, CVMX_SSO_XAQ_AURA, xaq_aura.u64);
+
+	for (i = 0; i < 256; i++) {
+		u64 phys;
+		void *mem = cvmx_fpa3_alloc(__cvmx_fpa3_gaura(node, aura));
+		if (!mem) {
+			rv = -ENOMEM;
+			goto err;
+		}
+		phys = virt_to_phys(mem);
+		cvmx_write_csr_node(node, CVMX_SSO_XAQX_HEAD_PTR(i), phys);
+		cvmx_write_csr_node(node, CVMX_SSO_XAQX_TAIL_PTR(i), phys);
+		/* SSO-18678 */
+		cvmx_write_csr_node(node, CVMX_SSO_GRPX_PRI(i), grp_pri.u64);
+	}
+	err0.u64 = 0;
+	err0.s.fpe = 1;
+	cvmx_write_csr_node(node, CVMX_SSO_ERR0, err0.u64);
+
+	cfg.s.rwen = 1;
+	cvmx_write_csr_node(node, CVMX_SSO_AW_CFG, cfg.u64);
+
+err:
+	return rv;
+}
+
+static void octeon3_eth_sso_irq_set_armed(int node, int grp, bool v)
+{
+	union cvmx_sso_grpx_int_thr grp_int_thr;
+	union cvmx_sso_grpx_int grp_int;
+
+	/* Disarm/Rearm the irq. */
+	grp_int_thr.u64 = 0;
+	grp_int_thr.s.iaq_thr = v ? 1 : 0;
+	cvmx_write_csr_node(node, CVMX_SSO_GRPX_INT_THR(grp), grp_int_thr.u64);
+
+	grp_int.u64 = 0;
+	grp_int.s.exe_int = 1;
+	cvmx_write_csr_node(node, CVMX_SSO_GRPX_INT(grp), grp_int.u64);
+}
+
+struct wr_ret {
+	void *work;
+	u16 grp;
+};
+
+static inline struct wr_ret octeon3_eth_work_request_grp_sync(unsigned int lgrp, cvmx_pow_wait_t wait)
+{
+	cvmx_pow_load_addr_t ptr;
+	cvmx_pow_tag_load_resp_t result;
+	struct wr_ret r;
+	unsigned int node = cvmx_get_node_num() & 3;
+
+	ptr.u64 = 0;
+
+	ptr.swork_78xx.mem_region = CVMX_IO_SEG;
+	ptr.swork_78xx.is_io = 1;
+	ptr.swork_78xx.did = CVMX_OCT_DID_TAG_SWTAG;
+	ptr.swork_78xx.node = node;
+	ptr.swork_78xx.rtngrp = 1;
+	ptr.swork_78xx.grouped = 1;
+	ptr.swork_78xx.index = (lgrp & 0xff) | node << 8;
+	ptr.swork_78xx.wait = wait;
+
+	result.u64 = cvmx_read_csr(ptr.u64);
+	r.grp = result.s_work.grp;
+	if (result.s_work.no_work)
+		r.work = NULL;
+	else
+		r.work = cvmx_phys_to_ptr(result.s_work.addr);
+	return r;
+}
+
+/* octeon3_eth_get_work_async:	Request work via a iobdma command. Doesn't wait
+ *				for the response.
+ *
+ *  lgrp:			Group to request work for.
+ */
+static inline void octeon3_eth_get_work_async(unsigned int lgrp)
+{
+	cvmx_pow_iobdma_store_t	data;
+	int			node = cvmx_get_node_num();
+
+	data.u64 = 0;
+	data.s_cn78xx.scraddr = CVMX_SCR_SCRATCH;
+	data.s_cn78xx.len = 1;
+	data.s_cn78xx.did = CVMX_OCT_DID_TAG_SWTAG;
+	data.s_cn78xx.node = node;
+	data.s_cn78xx.grouped = 1;
+	data.s_cn78xx.rtngrp = 1;
+	data.s_cn78xx.index_grp_mask = (lgrp & 0xff) | node << 8;
+	data.s_cn78xx.wait = CVMX_POW_NO_WAIT;
+
+	cvmx_send_single(data.u64);
+}
+
+/* octeon3_eth_get_work_response_async:	Read the request work response. Must
+ *					be called after calling
+ *					octeon3_eth_get_work_async().
+ *
+ *  Returns:				Work queue entry.
+ */
+static inline struct wr_ret octeon3_eth_get_work_response_async(void)
+{
+	cvmx_pow_tag_load_resp_t	result;
+	struct wr_ret			r;
+
+	CVMX_SYNCIOBDMA;
+	result.u64 = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
+
+	r.grp = result.s_work.grp;
+	if (result.s_work.no_work)
+		r.work = NULL;
+	else
+		r.work = cvmx_phys_to_ptr(result.s_work.addr);
+	return r;
+}
+
+static void octeon3_eth_replenish_rx(struct octeon3_ethernet *priv, int count)
+{
+	struct sk_buff *skb;
+	int i;
+
+	for (i = 0; i < count; i++) {
+		void **buf;
+		skb = __alloc_skb(packet_buffer_size, GFP_ATOMIC, 0, priv->numa_node);
+		if (!skb) {
+			pr_err("WARNING: octeon3_eth_replenish_rx out of memory\n");
+			break;
+		}
+		buf = (void **)PTR_ALIGN(skb->head, 128);
+		buf[SKB_PTR_OFFSET] = skb;
+		cvmx_fpa3_free(buf,
+			__cvmx_fpa3_gaura(priv->numa_node, priv->pki_laura),
+			0);
+	}
+}
+
+static bool octeon3_eth_tx_complete_runnable(struct octeon3_ethernet_worker *worker)
+{
+	return atomic_read(&worker->kick) != 0;
+}
+
+static int octeon3_eth_replenish_all(struct octeon3_ethernet_node *oen)
+{
+	int pending = 0;
+	int batch_size = 32;
+	struct octeon3_ethernet *priv;
+
+	rcu_read_lock();
+	list_for_each_entry_rcu(priv, &oen->device_list, list) {
+		int amount = atomic64_sub_if_positive(batch_size,
+						      &priv->buffers_needed);
+		if (amount >= 0) {
+			octeon3_eth_replenish_rx(priv, batch_size);
+			pending += amount;
+		}
+	}
+	rcu_read_unlock();
+	return pending;
+}
+
+static int octeon3_eth_tx_complete_worker(void *data)
+{
+	union cvmx_sso_grpx_aq_cnt aq_cnt;
+	struct octeon3_ethernet_worker *worker = data;
+	struct octeon3_ethernet_node *oen = worker->oen;
+	int backlog;
+	int loops;
+	const int tx_complete_batch = 100;
+	int backlog_stop_thresh = worker->order == 0 ? 0 : (worker->order - 1) * 80;
+	int i;
+#ifndef CONFIG_PREEMPT
+	unsigned long last_jiffies = jiffies;
+#endif
+	for (;;) {
+		/*
+		 * replaced by wait_event to avoid warnings like
+		 * "task oct3_eth/0:2:1250 blocked for more than 120 seconds."
+		 */
+		wait_event_interruptible(worker->queue, octeon3_eth_tx_complete_runnable(worker));
+		atomic_dec_if_positive(&worker->kick); /* clear the flag */
+		loops = 0;
+		do {
+		re_enter:
+			loops++;
+			backlog = octeon3_eth_replenish_all(oen);
+			if (loops > 3 && backlog > 100 * worker->order &&
+			    worker->order < ARRAY_SIZE(oen->workers) - 1) {
+				atomic_set(&oen->workers[worker->order + 1].kick, 1);
+				wake_up(&oen->workers[worker->order + 1].queue);
+			}
+			for (i = 0; i < tx_complete_batch; i++) {
+				void **work;
+				struct net_device *tx_netdev;
+				struct octeon3_ethernet *tx_priv;
+				struct sk_buff *skb;
+				struct wr_ret r;
+
+#ifndef CONFIG_PREEMPT
+				if (jiffies != last_jiffies) {
+					schedule();
+					last_jiffies = jiffies;
+				}
+#endif
+				r = octeon3_eth_work_request_grp_sync(oen->tx_complete_grp, CVMX_POW_NO_WAIT);
+				work = r.work;
+				if (work == NULL)
+					break;
+				tx_netdev = work[0];
+				tx_priv = netdev_priv(tx_netdev);
+				if (unlikely(netif_queue_stopped(tx_netdev)) &&
+				    atomic64_read(&tx_priv->tx_backlog) < MAX_TX_QUEUE_DEPTH)
+					netif_wake_queue(tx_netdev);
+				skb = container_of((void *)work, struct sk_buff, cb);
+				dev_kfree_skb(skb);
+			}
+			aq_cnt.u64 = cvmx_read_csr_node(oen->numa_node, CVMX_SSO_GRPX_AQ_CNT(oen->tx_complete_grp));
+		} while (backlog > backlog_stop_thresh   && aq_cnt.s.aq_cnt > worker->order * tx_complete_batch);
+		if (!octeon3_eth_tx_complete_runnable(worker))
+			octeon3_eth_sso_irq_set_armed(oen->numa_node, oen->tx_complete_grp, true);
+		aq_cnt.u64 = cvmx_read_csr_node(oen->numa_node, CVMX_SSO_GRPX_AQ_CNT(oen->tx_complete_grp));
+		if (aq_cnt.s.aq_cnt > worker->order * tx_complete_batch)
+			goto re_enter;
+	}
+	return 0;
+}
+
+static irqreturn_t octeon3_eth_tx_handler(int irq, void *info)
+{
+	struct octeon3_ethernet_node *oen = info;
+	/* Disarm the irq. */
+	octeon3_eth_sso_irq_set_armed(oen->numa_node, oen->tx_complete_grp, false);
+	atomic_set(&oen->workers[0].kick, 1);
+	wake_up(&oen->workers[0].queue);
+	return IRQ_HANDLED;
+}
+
+static int octeon3_eth_global_init(unsigned int node)
+{
+	int i;
+	int rv = 0;
+	struct irq_domain *d;
+	unsigned int sso_intsn;
+	struct octeon3_ethernet_node *oen;
+	union cvmx_fpa_gen_cfg fpa_cfg;
+	mutex_lock(&octeon3_eth_init_mutex);
+
+	oen = octeon3_eth_node + node;
+
+	if (oen->init_done)
+		goto done;
+
+	__cvmx_export_app_config_to_named_block(CVMX_APP_CONFIG);
+
+	INIT_LIST_HEAD(&oen->device_list);
+	mutex_init(&oen->device_list_lock);
+	spin_lock_init(&oen->napi_alloc_lock);
+
+	oen->numa_node = node;
+	for (i = 0; i < 1024; i++) {
+		cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT(i), 0x100000000ull);
+		cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT_LIMIT(i), 0xfffffffffull);
+		cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT_THRESHOLD(i), 0xffffffffeull);
+	}
+
+	fpa_cfg.u64 = cvmx_read_csr_node(node, CVMX_FPA_GEN_CFG);
+	fpa_cfg.s.lvl_dly = 3;
+	cvmx_write_csr_node(node, CVMX_FPA_GEN_CFG, fpa_cfg.u64);
+
+	oen->sso_pool = cvmx_fpa3_reserve_pool(node, -1);
+	if (!__cvmx_fpa3_pool_valid(oen->sso_pool)) {
+		rv = -ENODEV;
+		goto done;
+	}
+	oen->pko_pool = cvmx_fpa3_reserve_pool(node, -1);
+	if (!__cvmx_fpa3_pool_valid(oen->pko_pool)) {
+		rv = -ENODEV;
+		goto done;
+	}
+	oen->pki_packet_pool = cvmx_fpa3_reserve_pool(node, -1);
+	if (!__cvmx_fpa3_pool_valid(oen->pki_packet_pool)) {
+		rv = -ENODEV;
+		goto done;
+	}
+	oen->sso_aura = cvmx_fpa3_reserve_aura(node, -1);
+	oen->pko_aura = cvmx_fpa3_reserve_aura(node, -1);
+
+	pr_err("octeon3_eth_global_init  SSO:%d:%d, PKO:%d:%d\n",
+	       oen->sso_pool.lpool, oen->sso_aura.laura,
+	       oen->pko_pool.lpool, oen->pko_aura.laura);
+
+	octeon3_eth_fpa_pool_init(oen->sso_pool, 40960);
+	octeon3_eth_fpa_pool_init(oen->pko_pool, 40960);
+	octeon3_eth_fpa_pool_init(oen->pki_packet_pool, 64 * num_packet_buffers);
+	octeon3_eth_fpa_aura_init(oen->sso_pool, oen->sso_aura, 20480);
+	octeon3_eth_fpa_aura_init(oen->pko_pool, oen->pko_aura, 20480);
+
+	if (!octeon3_eth_sso_pko_cache) {
+		octeon3_eth_sso_pko_cache = kmem_cache_create("sso_pko", 4096, 128, 0, NULL);
+		if (!octeon3_eth_sso_pko_cache) {
+			rv = -ENOMEM;
+			goto done;
+		}
+	}
+	for (i = 0; i < 1024; i++) {
+		void *mem;
+		mem = kmem_cache_alloc_node(octeon3_eth_sso_pko_cache, GFP_KERNEL, node);
+		if (!mem) {
+			rv = -ENOMEM;
+			goto done;
+		}
+		cvmx_fpa3_free(mem, oen->sso_aura, 0);
+		mem = kmem_cache_alloc_node(octeon3_eth_sso_pko_cache, GFP_KERNEL, node);
+		if (!mem) {
+			rv = -ENOMEM;
+			goto done;
+		}
+		cvmx_fpa3_free(mem, oen->pko_aura, 0);
+	}
+
+	BUG_ON(node != oen->sso_aura.node);
+	rv = octeon3_eth_sso_init(node, oen->sso_aura.laura);
+	if (rv)
+		goto done;
+
+	oen->tx_complete_grp = cvmx_sso_allocate_group(node);
+	d = octeon_irq_get_block_domain(oen->numa_node, SSO_INTSN_EXE);
+	sso_intsn = SSO_INTSN_EXE << 12 | oen->tx_complete_grp;
+	oen->tx_irq = irq_create_mapping(d, sso_intsn);
+	if (!oen->tx_irq) {
+		rv = -ENODEV;
+		goto done;
+	}
+
+	__cvmx_helper_init_port_config_data();
+	rv = __cvmx_helper_pko3_init_global(node, oen->pko_aura.laura | (node << 10));
+	if (rv) {
+		pr_err("cvmx_helper_pko3_init_global failed\n");
+		rv = -ENODEV;
+		goto done;
+	}
+
+	__cvmx_helper_pki_install_dflt_vlan(node);
+	cvmx_pki_setup_clusters(node);
+	__cvmx_helper_pki_set_dflt_ltype_map(node);
+	cvmx_pki_enable_backpressure(node);
+	cvmx_pki_parse_enable(node, 0);
+	cvmx_pki_enable(node);
+	/* Statistics per pkind */
+	cvmx_write_csr_node(node, CVMX_PKI_STAT_CTL, 0);
+
+	for (i = 0; i < ARRAY_SIZE(oen->workers); i++) {
+		oen->workers[i].oen = oen;
+		init_waitqueue_head(&oen->workers[i].queue);
+		oen->workers[i].order = i;
+	}
+	for (i = 0; i < ARRAY_SIZE(oen->workers); i++) {
+		oen->workers[i].task = kthread_create_on_node(octeon3_eth_tx_complete_worker,
+							      oen->workers + i, node,
+							      "oct3_eth/%d:%d", node, i);
+		if (IS_ERR(oen->workers[i].task)) {
+			rv = PTR_ERR(oen->workers[i].task);
+			goto done;
+		} else {
+			wake_up_process(oen->workers[i].task);
+		}
+	}
+
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X))
+		octeon3_eth_sso_pass1_limit(node, oen->tx_complete_grp);
+
+	rv = request_irq(oen->tx_irq, octeon3_eth_tx_handler, 0, "oct3_eth_tx_done", oen);
+	if (rv)
+		goto done;
+	octeon3_eth_gen_affinity(node, &oen->tx_affinity_hint);
+	irq_set_affinity_hint(oen->tx_irq, &oen->tx_affinity_hint);
+
+	octeon3_eth_sso_irq_set_armed(node, oen->tx_complete_grp, true);
+
+	__cvmx_export_config();
+
+	oen->init_done = true;
+done:
+	mutex_unlock(&octeon3_eth_init_mutex);
+	return rv;
+}
+
+static struct sk_buff *octeon3_eth_work_to_skb(void *w)
+{
+	struct sk_buff *skb;
+	void **f = w;
+	skb = f[-16];
+	return skb;
+}
+
+/* octeon3_napi_alloc:		Allocate a napi.
+ *
+ *  cxt:			Receive context the napi will be added to.
+ *  idx:			Napi index within the receive context.
+ *  cpu:			Cpu to bind the napi to:
+ *					<  0: use any cpu.
+ *					>= 0: use requested cpu.
+ *
+ *  Returns:			Pointer to napi wrapper or NULL on error.
+ */
+static struct octeon3_napi_wrapper *octeon3_napi_alloc(struct octeon3_rx *cxt,
+						       int		  idx,
+						       int		  cpu)
+{
+	struct octeon3_ethernet_node	*oen;
+	struct octeon3_ethernet		*priv = cxt->parent;
+	int				node = priv->numa_node;
+	int				i;
+
+	oen = octeon3_eth_node + node;
+	spin_lock(&oen->napi_alloc_lock);
+
+	/* Find a free napi wrapper */
+	for (i = 0; i < CVMX_MAX_CORES; i++) {
+		if (napi_wrapper[node][i].available) {
+			/* Assign a cpu to use (a free cpu if possible) */
+			if (cpu < 0) {
+				cpu = find_first_zero_bit(oen->napi_cpu_bitmap,
+							  oen->napi_max_cpus);
+				if (cpu < oen->napi_max_cpus) {
+					bitmap_set(oen->napi_cpu_bitmap,
+						   cpu, 1);
+				} else {
+					/* Choose a random cpu */
+					get_random_bytes(&cpu, sizeof(int));
+					cpu = cpu % oen->napi_max_cpus;
+				}
+			} else
+				bitmap_set(oen->napi_cpu_bitmap, cpu, 1);
+
+			napi_wrapper[node][i].available = 0;
+			napi_wrapper[node][i].idx = idx;
+			napi_wrapper[node][i].cpu = cpu;
+			napi_wrapper[node][i].cxt = cxt;
+			spin_unlock(&oen->napi_alloc_lock);
+			return &napi_wrapper[node][i];
+		}
+	}
+
+	spin_unlock(&oen->napi_alloc_lock);
+	return NULL;
+}
+
+/* octeon_cpu_napi_sched:	Schedule a napi for execution. The napi will
+ *				start executing on the cpu calling this
+ *				function.
+ *
+ *  info:			Pointer to the napi to schedule for execution.
+ */
+static void octeon_cpu_napi_sched(void	*info)
+{
+	struct napi_struct		*napi = info;
+	napi_schedule(napi);
+}
+
+/* octeon3_rm_napi_from_cxt:	Remove a napi from a receive context.
+ *
+ *  node:			Node napi belongs to.
+ *  napiw:			Pointer to napi to remove.
+ *
+ *  returns:			0 on success, otherwise a negative error code.
+ */
+static int octeon3_rm_napi_from_cxt(int				node,
+				    struct octeon3_napi_wrapper	*napiw)
+{
+	struct octeon3_ethernet_node	*oen;
+	struct octeon3_rx		*cxt;
+	int				idx;
+
+	oen = octeon3_eth_node + node;
+	cxt = napiw->cxt;
+	idx = napiw->idx;
+
+	/* Free the napi block */
+	spin_lock(&oen->napi_alloc_lock);
+	bitmap_clear(oen->napi_cpu_bitmap, napiw->cpu, 1);
+	napiw->available = 1;
+	napiw->idx = -1;
+	napiw->cpu = -1;
+	napiw->cxt = NULL;
+	spin_unlock(&oen->napi_alloc_lock);
+
+	/* Free the napi idx */
+	spin_lock(&cxt->napi_idx_lock);
+	bitmap_clear(cxt->napi_idx_bitmap, idx, 1);
+	spin_unlock(&cxt->napi_idx_lock);
+
+	return 0;
+}
+
+/* octeon3_add_napi_to_cxt:	Add a napi to a receive context.
+ *
+ *  cxt:			Pointer to receive context.
+ *
+ *  returns:			0 on success, otherwise a negative error code.
+ */
+static int octeon3_add_napi_to_cxt(struct octeon3_rx *cxt)
+{
+	struct octeon3_napi_wrapper	*napiw;
+	struct octeon3_ethernet		*priv = cxt->parent;
+	int				idx;
+	int				rc;
+
+	/* Get a free napi idx */
+	spin_lock(&cxt->napi_idx_lock);
+	idx = find_first_zero_bit(cxt->napi_idx_bitmap, CVMX_MAX_CORES);
+	if (unlikely(idx >= CVMX_MAX_CORES)) {
+		spin_unlock(&cxt->napi_idx_lock);
+		return -ENOMEM;
+	}
+	bitmap_set(cxt->napi_idx_bitmap, idx, 1);
+	spin_unlock(&cxt->napi_idx_lock);
+
+	/* Get a free napi block */
+	napiw = octeon3_napi_alloc(cxt, idx, -1);
+	if (unlikely(napiw == NULL)) {
+		spin_lock(&cxt->napi_idx_lock);
+		bitmap_clear(cxt->napi_idx_bitmap, idx, 1);
+		spin_unlock(&cxt->napi_idx_lock);
+		return -ENOMEM;
+	}
+
+	rc = smp_call_function_single(napiw->cpu, octeon_cpu_napi_sched,
+				      &napiw->napi, 0);
+	if (unlikely(rc))
+		octeon3_rm_napi_from_cxt(priv->numa_node, napiw);
+
+	return rc;
+}
+
+/* Receive one packet.
+ * returns the number of RX buffers consumed.
+ */
+static int octeon3_eth_rx_one(struct octeon3_rx *rx, bool is_async,
+			      bool req_next)
+{
+	int segments;
+	int ret;
+	unsigned int packet_len;
+	cvmx_wqe_78xx_t *work;
+	u8 *data;
+	int len_remaining;
+	struct sk_buff *skb;
+	union cvmx_buf_ptr_pki packet_ptr;
+	struct wr_ret r;
+	struct octeon3_ethernet *priv = rx->parent;
+	void **buf;
+
+	if (is_async == true)
+		r = octeon3_eth_get_work_response_async();
+	else
+		r = octeon3_eth_work_request_grp_sync(rx->rx_grp,
+						      CVMX_POW_NO_WAIT);
+	work = r.work;
+	if (!work)
+		return 0;
+
+	/* Request the next work so it'll be ready when we need it */
+	if (is_async == true && req_next)
+		octeon3_eth_get_work_async(rx->rx_grp);
+
+	skb = octeon3_eth_work_to_skb(work);
+
+	/* Save the aura this skb came from to allow the pko to free the skb
+	 * back to the correct aura.
+	 */
+	buf = (void **)PTR_ALIGN(skb->head, 128);
+	buf[SKB_AURA_OFFSET] = (void *)(SKB_AURA_MAGIC | priv->pki_laura);
+
+	segments = work->word0.bufs;
+	ret = segments;
+	packet_ptr = work->packet_ptr;
+//	pr_err("RX %016lx.%016lx.%016lx\n",
+//	       (unsigned long)work->word0.u64, (unsigned long)work->word1.u64, (unsigned long)work->word2.u64);
+	if (unlikely(work->word2.err_level <= CVMX_PKI_ERRLEV_E_LA &&
+		     work->word2.err_code != CVMX_PKI_OPCODE_RE_NONE)) {
+		atomic64_inc(&priv->rx_errors);
+		switch (work->word2.err_code) {
+		case CVMX_PKI_OPCODE_RE_JABBER:
+			atomic64_inc(&priv->rx_length_errors);
+			break;
+		case CVMX_PKI_OPCODE_RE_FCS:
+			atomic64_inc(&priv->rx_crc_errors);
+			break;
+		}
+		data = phys_to_virt(packet_ptr.addr);
+		for (;;) {
+			dev_kfree_skb_any(skb);
+			segments--;
+			if (segments <= 0)
+				break;
+			packet_ptr.u64 = *(u64 *)(data - 8);
+			/* PKI_BUFLINK_S's are endian-swapped */
+			packet_ptr.u64 = swab64(packet_ptr.u64);
+			data = phys_to_virt(packet_ptr.addr);
+			skb = octeon3_eth_work_to_skb((void *)round_down((unsigned long)data, 128ul));
+		}
+		goto out;
+	}
+
+	packet_len = work->word1.len;
+	data = phys_to_virt(packet_ptr.addr);
+	skb->data = data;
+	skb->len = packet_len;
+	len_remaining = packet_len;
+	if (segments == 1) {
+		skb_set_tail_pointer(skb, skb->len);
+	} else {
+		bool first_frag = true;
+		struct sk_buff *current_skb = skb;
+		struct sk_buff *next_skb = NULL;
+		unsigned int segment_size;
+
+		skb_frag_list_init(skb);
+		for (;;) {
+			segment_size = (segments == 1) ? len_remaining : packet_ptr.size;
+			len_remaining -= segment_size;
+			if (!first_frag) {
+				current_skb->len = segment_size;
+				skb->data_len += segment_size;
+				skb->truesize += current_skb->truesize;
+			}
+			skb_set_tail_pointer(current_skb, segment_size);
+			segments--;
+			if (segments == 0)
+				break;
+			packet_ptr.u64 = *(u64 *)(data - 8);
+			/* PKI_BUFLINK_S's are endian-swapped */
+			packet_ptr.u64 = swab64(packet_ptr.u64);
+
+			data = phys_to_virt(packet_ptr.addr);
+			next_skb = octeon3_eth_work_to_skb((void *)round_down((unsigned long)data, 128ul));
+			if (first_frag) {
+				skb_frag_add_head(current_skb, next_skb);
+			} else {
+				current_skb->next = next_skb;
+				next_skb->next = NULL;
+			}
+			current_skb = next_skb;
+			first_frag = false;
+			current_skb->data = data;
+		}
+	}
+	if (likely(priv->netdev->flags & IFF_UP)) {
+		skb_checksum_none_assert(skb);
+		skb->protocol = eth_type_trans(skb, priv->netdev);
+		skb->dev = priv->netdev;
+		if (priv->netdev->features & NETIF_F_RXCSUM) {
+			if ((work->word2.lc_hdr_type == CVMX_PKI_LTYPE_E_IP4 ||
+			     work->word2.lc_hdr_type == CVMX_PKI_LTYPE_E_IP6) &&
+			    (work->word2.lf_hdr_type == CVMX_PKI_LTYPE_E_TCP ||
+			     work->word2.lf_hdr_type == CVMX_PKI_LTYPE_E_UDP ||
+			     work->word2.lf_hdr_type == CVMX_PKI_LTYPE_E_SCTP))
+				if (work->word2.err_code == 0)
+					skb->ip_summed = CHECKSUM_UNNECESSARY;
+
+
+		}
+		netif_receive_skb(skb);
+	} else {
+		/* Drop any packet received for a device that isn't up */
+		atomic64_inc(&priv->rx_dropped);
+		dev_kfree_skb_any(skb);
+	}
+out:
+	return ret;
+}
+
+static int octeon3_eth_napi(struct napi_struct *napi, int budget)
+{
+	int rx_count = 0;
+	struct octeon3_napi_wrapper *napiw;
+	struct octeon3_rx *cxt;
+	struct octeon3_ethernet *priv;
+	union cvmx_sso_grpx_aq_cnt aq_cnt;
+	int idx;
+	int napis_inuse;
+	int n = 0;
+	int n_bufs = 0;
+	u64 old_scratch;
+
+	napiw = container_of(napi, struct octeon3_napi_wrapper, napi);
+	cxt = napiw->cxt;
+	priv = cxt->parent;
+
+	/* Get the amount of work pending */
+	aq_cnt.u64 = cvmx_read_csr_node(priv->numa_node,
+					CVMX_SSO_GRPX_AQ_CNT(cxt->rx_grp));
+
+	/* Allow the last thread to add/remove threads if the work
+	 * incremented/decremented by more than what the current number
+	 * of threads can support.
+	 */
+	idx = find_last_bit(cxt->napi_idx_bitmap, CVMX_MAX_CORES);
+	napis_inuse = bitmap_weight(cxt->napi_idx_bitmap, CVMX_MAX_CORES);
+
+	if (napiw->idx == idx) {
+		if (aq_cnt.s.aq_cnt > napis_inuse * 128)
+			octeon3_add_napi_to_cxt(cxt);
+		else if (napiw->idx > 0 &&
+			 aq_cnt.s.aq_cnt < (napis_inuse - 1) * 128) {
+			napi_complete(napi);
+			octeon3_rm_napi_from_cxt(priv->numa_node, napiw);
+			return 0;
+		}
+	}
+
+	if (likely(USE_ASYNC_IOBDMA)) {
+		/* Save scratch in case userspace is using it */
+		CVMX_SYNCIOBDMA;
+		old_scratch = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
+
+		octeon3_eth_get_work_async(cxt->rx_grp);
+	}
+
+	while (rx_count < budget) {
+		n = 0;
+
+		if (likely(USE_ASYNC_IOBDMA)) {
+			bool req_next = rx_count < (budget - 1) ? true : false;
+
+			n = octeon3_eth_rx_one(cxt, true, req_next);
+		} else
+			n = octeon3_eth_rx_one(cxt, false, false);
+
+		if (n == 0) {
+			break;
+		}
+
+		n_bufs += n;
+		rx_count++;
+	}
+
+	/* Wake up worker threads */
+	n_bufs = atomic64_add_return(n_bufs, &priv->buffers_needed);
+	if (n_bufs >= 32) {
+		struct octeon3_ethernet_node *oen;
+
+		oen = octeon3_eth_node + priv->numa_node;
+		atomic_set(&oen->workers[0].kick, 1);
+		wake_up(&oen->workers[0].queue);
+	}
+
+	/* Stop the thread when no work is pending */
+	if (rx_count < budget) {
+		napi_complete(napi);
+
+		if (napiw->idx > 0) {
+			octeon3_rm_napi_from_cxt(priv->numa_node, napiw);
+		} else {
+			octeon3_eth_sso_irq_set_armed(cxt->parent->numa_node,
+						      cxt->rx_grp, true);
+		}
+	}
+
+	if (likely(USE_ASYNC_IOBDMA)) {
+		/* Restore the scratch area */
+		cvmx_scratch_write64(CVMX_SCR_SCRATCH, old_scratch);
+	}
+
+	return rx_count;
+}
+
+/* octeon3_napi_init_node:	Initialize the node napis.
+ *
+ *  node:			Node napis belong to.
+ *  netdev:			Default network device used to initialize the
+ *				napis.
+ *
+ *  returns:			0 on success, otherwise a negative error code.
+ */
+static int octeon3_napi_init_node(int node, struct net_device *netdev)
+{
+	struct octeon3_ethernet_node	*oen;
+	int				i;
+
+	oen = octeon3_eth_node + node;
+	spin_lock(&oen->napi_alloc_lock);
+
+	if (oen->napi_init_done)
+		goto done;
+
+	bitmap_zero(oen->napi_cpu_bitmap, CVMX_MAX_CORES);
+	oen->napi_max_cpus = nr_cpus_node(node);
+
+	for (i = 0; i < CVMX_MAX_CORES; i++) {
+		netif_napi_add(netdev, &napi_wrapper[node][i].napi,
+			       octeon3_eth_napi, 32);
+		napi_enable(&napi_wrapper[node][i].napi);
+		napi_wrapper[node][i].available = 1;
+		napi_wrapper[node][i].idx = 0;
+		napi_wrapper[node][i].cpu = -1;
+		napi_wrapper[node][i].cxt = NULL;
+	}
+
+	oen->napi_init_done = true;
+ done:
+	spin_unlock(&oen->napi_alloc_lock);
+	return 0;
+
+}
+
+//#define BROKEN_SIMULATOR_CSUM 1
+
+static void ethtool_get_drvinfo(struct net_device *netdev,
+				struct ethtool_drvinfo *info)
+{
+	strcpy(info->driver, "octeon3-ethernet");
+	strcpy(info->version, "1.0");
+	strcpy(info->bus_info, "Builtin");
+}
+
+static const struct ethtool_ops octeon3_ethtool_ops = {
+	.get_drvinfo = ethtool_get_drvinfo,
+	.get_settings = bgx_port_ethtool_get_settings,
+	.set_settings = bgx_port_ethtool_set_settings,
+	.nway_reset = bgx_port_ethtool_nway_reset,
+	.get_link = ethtool_op_get_link,
+};
+
+static int octeon3_eth_ndo_init(struct net_device *netdev)
+{
+	struct octeon3_ethernet *priv = netdev_priv(netdev);
+	struct octeon3_ethernet_node *oen = octeon3_eth_node + priv->numa_node;
+	struct cvmx_pki_port_config pki_prt_cfg;
+	struct cvmx_pki_prt_schd *prt_schd = NULL;
+	union cvmx_pki_aurax_cfg pki_aura_cfg;
+	union cvmx_pki_qpg_tblx qpg_tbl;
+	int ipd_port, node_dq;
+	int base_rx_grp;
+	int first_skip, later_skip;
+	struct cvmx_xport xdq;
+	int r, i;
+	const u8 *mac;
+	cvmx_fpa3_gaura_t aura;
+
+	netif_carrier_off(netdev);
+
+	netdev->features |=
+		NETIF_F_SG |
+		NETIF_F_FRAGLIST |
+		NETIF_F_RXCSUM |
+		NETIF_F_LLTX
+#ifndef BROKEN_SIMULATOR_CSUM
+		|
+		NETIF_F_IP_CSUM |
+		NETIF_F_IPV6_CSUM
+#endif
+		;
+
+	if (!OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X))
+		netdev->features |= NETIF_F_SCTP_CSUM;
+
+	priv->rx_buf_count = num_packet_buffers;
+
+	ipd_port = cvmx_helper_get_ipd_port(priv->xiface, priv->port_index);
+
+	r =  __cvmx_pko3_config_gen_interface(priv->xiface, priv->port_index, 1, false);
+	if (r)
+		return -ENODEV;
+
+	r = __cvmx_pko3_helper_dqs_activate(priv->xiface, priv->port_index, false);
+	if (r < 0)
+		return -ENODEV;
+
+	/* Padding and FCS are done in BGX */
+	r = cvmx_pko3_interface_options(priv->xiface, priv->port_index, false, false, 0);
+	if (r)
+		return -ENODEV;
+
+	node_dq = cvmx_pko3_get_queue_base(ipd_port);
+	xdq = cvmx_helper_ipd_port_to_xport(node_dq);
+
+	priv->pko_queue = xdq.port;
+	aura = cvmx_fpa3_reserve_aura(priv->numa_node, -1);
+	priv->pki_laura = aura.laura;
+	octeon3_eth_fpa_aura_init(oen->pki_packet_pool, aura,
+				  num_packet_buffers * 2);
+	aura2buffers_needed[priv->pki_laura] = &priv->buffers_needed;
+
+	base_rx_grp = -1;
+	r = cvmx_sso_allocate_group_range(priv->numa_node, &base_rx_grp, rx_contexts);
+	if (r) {
+		dev_err(netdev->dev.parent, "Failed to allocated SSO group\n");
+		return -ENODEV;
+	}
+	for (i = 0; i < rx_contexts; i++) {
+		priv->rx_cxt[i].rx_grp = base_rx_grp + i;
+		priv->rx_cxt[i].parent = priv;
+
+		if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X)) {
+			octeon3_eth_sso_pass1_limit(priv->numa_node,
+						    priv->rx_cxt[i].rx_grp);
+		}
+	}
+	priv->num_rx_cxt = rx_contexts;
+
+	priv->tx_complete_grp = oen->tx_complete_grp;
+	priv->pki_pkind = cvmx_helper_get_pknd(priv->xiface, priv->port_index);
+	dev_err(netdev->dev.parent, "rx sso grp:%d..%d aura:%d pknd:%d pko_queue:%d\n",
+		base_rx_grp, base_rx_grp + priv->num_rx_cxt, priv->pki_laura, priv->pki_pkind, priv->pko_queue);
+
+	prt_schd = kzalloc(sizeof(*prt_schd), GFP_KERNEL);
+	if (!prt_schd) {
+		r = -ENOMEM;
+		goto err;
+	}
+
+	prt_schd->style = -1; /* Allocate net style per port */
+	prt_schd->qpg_base = -1;
+	prt_schd->aura_per_prt = true;
+	prt_schd->aura_num = priv->pki_laura;
+	prt_schd->sso_grp_per_prt = true;
+	prt_schd->sso_grp = base_rx_grp;
+	prt_schd->qpg_qos = CVMX_PKI_QPG_QOS_NONE;
+
+	cvmx_helper_pki_init_port(ipd_port, prt_schd);
+	cvmx_pki_get_port_config(ipd_port, &pki_prt_cfg);
+
+	pki_prt_cfg.style_cfg.parm_cfg.ip6_udp_opt = false;
+	pki_prt_cfg.style_cfg.parm_cfg.lenerr_en = true;
+	pki_prt_cfg.style_cfg.parm_cfg.maxerr_en = false;
+	pki_prt_cfg.style_cfg.parm_cfg.minerr_en = false;
+	pki_prt_cfg.style_cfg.parm_cfg.ip6_udp_opt = false;
+	pki_prt_cfg.style_cfg.parm_cfg.ip6_udp_opt = false;
+	pki_prt_cfg.style_cfg.parm_cfg.wqe_skip = 1 * 128;
+	first_skip = 8 * 21;
+	later_skip = 8 * 16;
+	pki_prt_cfg.style_cfg.parm_cfg.first_skip = first_skip;
+	pki_prt_cfg.style_cfg.parm_cfg.later_skip = later_skip;
+#ifdef __LITTLE_ENDIAN
+	pki_prt_cfg.style_cfg.parm_cfg.pkt_lend = true;
+#else
+	pki_prt_cfg.style_cfg.parm_cfg.pkt_lend = false;
+#endif
+	pki_prt_cfg.style_cfg.parm_cfg.tag_type = CVMX_SSO_TAG_TYPE_UNTAGGED;
+	pki_prt_cfg.style_cfg.parm_cfg.qpg_dis_grptag = false;
+	pki_prt_cfg.style_cfg.parm_cfg.dis_wq_dat = false;
+
+	pki_prt_cfg.style_cfg.parm_cfg.csum_lb = true;
+	pki_prt_cfg.style_cfg.parm_cfg.csum_lc = true;
+	pki_prt_cfg.style_cfg.parm_cfg.csum_ld = true;
+	pki_prt_cfg.style_cfg.parm_cfg.csum_le = true;
+	pki_prt_cfg.style_cfg.parm_cfg.csum_lf = true;
+	pki_prt_cfg.style_cfg.parm_cfg.csum_lg = false;
+
+	pki_prt_cfg.style_cfg.parm_cfg.len_lb = false;
+	pki_prt_cfg.style_cfg.parm_cfg.len_lc = true;
+	pki_prt_cfg.style_cfg.parm_cfg.len_ld = false;
+	pki_prt_cfg.style_cfg.parm_cfg.len_le = false;
+	pki_prt_cfg.style_cfg.parm_cfg.len_lf = true;
+	pki_prt_cfg.style_cfg.parm_cfg.len_lg = false;
+
+	pki_prt_cfg.style_cfg.parm_cfg.mbuff_size = (packet_buffer_size - 128) & ~0xf;
+
+	pki_prt_cfg.style_cfg.tag_cfg.tag_fields.tag_vni = 0;
+	pki_prt_cfg.style_cfg.tag_cfg.tag_fields.tag_gtp = 0;
+	pki_prt_cfg.style_cfg.tag_cfg.tag_fields.tag_spi = 0;
+	pki_prt_cfg.style_cfg.tag_cfg.tag_fields.tag_sync = 0;
+	pki_prt_cfg.style_cfg.tag_cfg.tag_fields.ip_prot_nexthdr = 0;
+	pki_prt_cfg.style_cfg.tag_cfg.tag_fields.second_vlan = 0;
+	pki_prt_cfg.style_cfg.tag_cfg.tag_fields.first_vlan = 0;
+	pki_prt_cfg.style_cfg.tag_cfg.tag_fields.mpls_label = 0;
+	pki_prt_cfg.style_cfg.tag_cfg.tag_fields.input_port = 0;
+	pki_prt_cfg.style_cfg.tag_cfg.tag_fields.layer_b_dst = 1;
+	pki_prt_cfg.style_cfg.tag_cfg.tag_fields.layer_c_dst = 1;
+	pki_prt_cfg.style_cfg.tag_cfg.tag_fields.layer_d_dst = 1;
+	pki_prt_cfg.style_cfg.tag_cfg.tag_fields.layer_e_dst = 1;
+	pki_prt_cfg.style_cfg.tag_cfg.tag_fields.layer_f_dst = 1;
+	pki_prt_cfg.style_cfg.tag_cfg.tag_fields.layer_g_dst = 1;
+	pki_prt_cfg.style_cfg.tag_cfg.tag_fields.layer_b_src = 1;
+	pki_prt_cfg.style_cfg.tag_cfg.tag_fields.layer_c_src = 1;
+	pki_prt_cfg.style_cfg.tag_cfg.tag_fields.layer_d_src = 1;
+	pki_prt_cfg.style_cfg.tag_cfg.tag_fields.layer_e_src = 1;
+	pki_prt_cfg.style_cfg.tag_cfg.tag_fields.layer_f_src = 1;
+	pki_prt_cfg.style_cfg.tag_cfg.tag_fields.layer_g_src = 1;
+
+	cvmx_pki_set_port_config(ipd_port, &pki_prt_cfg);
+
+	i = 0;
+	while ((priv->num_rx_cxt & (1 << i)) == 0)
+		i++;
+
+	qpg_tbl.u64 = cvmx_read_csr_node(priv->numa_node, CVMX_PKI_QPG_TBLX(pki_prt_cfg.style_cfg.parm_cfg.qpg_base));
+	qpg_tbl.s.grptag_ok = i;
+	qpg_tbl.s.grptag_bad = i;
+	cvmx_write_csr_node(priv->numa_node,
+			    CVMX_PKI_QPG_TBLX(pki_prt_cfg.style_cfg.parm_cfg.qpg_base),
+			    qpg_tbl.u64);
+	pki_aura_cfg.u64 = 0;
+	pki_aura_cfg.s.ena_red = 1;
+	cvmx_write_csr_node(priv->numa_node, CVMX_PKI_AURAX_CFG(priv->pki_laura), pki_aura_cfg.u64);
+
+	cvmx_write_csr_node(priv->numa_node, CVMX_PKI_STATX_STAT0(priv->pki_pkind), 0);
+	priv->last_packets = 0;
+
+	cvmx_write_csr_node(priv->numa_node, CVMX_PKI_STATX_STAT1(priv->pki_pkind), 0);
+	priv->last_octets = 0;
+
+	cvmx_write_csr_node(priv->numa_node, CVMX_PKI_STATX_STAT3(priv->pki_pkind), 0);
+	priv->last_dropped = 0;
+
+	mac = bgx_port_get_mac(netdev);
+	if (mac && is_valid_ether_addr(mac)) {
+		memcpy(netdev->dev_addr, mac, ETH_ALEN);
+		netdev->addr_assign_type &= ~NET_ADDR_RANDOM;
+	} else {
+		eth_hw_addr_random(netdev);
+	}
+	bgx_port_set_rx_filtering(netdev);
+	bgx_port_change_mtu(netdev, netdev->mtu);
+
+	octeon3_napi_init_node(priv->numa_node, netdev);
+
+	/* Register ethtool methods */
+	SET_ETHTOOL_OPS(netdev, &octeon3_ethtool_ops);
+
+	__cvmx_export_config();
+
+	return 0;
+err:
+	kfree(prt_schd);
+	return r;
+}
+
+static void octeon3_eth_ndo_uninit(struct net_device *netdev)
+{
+	return;
+}
+
+static irqreturn_t octeon3_eth_rx_handler(int irq, void *info)
+{
+	struct octeon3_rx *rx = info;
+
+	/* Disarm the irq. */
+	octeon3_eth_sso_irq_set_armed(rx->parent->numa_node, rx->rx_grp, false);
+
+	napi_schedule(&rx->napiw->napi);
+	return IRQ_HANDLED;
+}
+
+static int octeon3_eth_ndo_open(struct net_device *netdev)
+{
+	struct octeon3_ethernet *priv = netdev_priv(netdev);
+	struct irq_domain *d = octeon_irq_get_block_domain(priv->numa_node, SSO_INTSN_EXE);
+	struct octeon3_rx *rx;
+	int idx;
+	int cpu;
+	int i;
+	int r;
+
+	for (i = 0; i < priv->num_rx_cxt; i++) {
+		unsigned int sso_intsn;
+
+		rx = priv->rx_cxt + i;
+		sso_intsn = SSO_INTSN_EXE << 12 | rx->rx_grp;
+
+		spin_lock_init(&rx->napi_idx_lock);
+
+		rx->rx_irq = irq_create_mapping(d, sso_intsn);
+		if (!rx->rx_irq) {
+			netdev_err(netdev, "ERROR: Couldn't map hwirq: %x\n",
+				   sso_intsn);
+			r = -EINVAL;
+			goto err1;
+		}
+		r = request_irq(rx->rx_irq, octeon3_eth_rx_handler, 0,
+				netdev_name(netdev), rx);
+		if (r) {
+			netdev_err(netdev, "ERROR: Couldn't request irq: %d\n",
+				   rx->rx_irq);
+			r = -ENOMEM;
+			goto err2;
+		}
+
+		octeon3_eth_gen_affinity(priv->numa_node, &rx->rx_affinity_hint);
+		irq_set_affinity_hint(rx->rx_irq, &rx->rx_affinity_hint);
+
+		/* Allocate a napi index for this receive context */
+		bitmap_zero(priv->rx_cxt[i].napi_idx_bitmap, CVMX_MAX_CORES);
+		idx = find_first_zero_bit(priv->rx_cxt[i].napi_idx_bitmap,
+					 CVMX_MAX_CORES);
+		if (idx >= CVMX_MAX_CORES) {
+			netdev_err(netdev, "ERROR: Couldn't get napi index\n");
+			r = -ENOMEM;
+			goto err3;
+		}
+		bitmap_set(priv->rx_cxt[i].napi_idx_bitmap, idx, 1);
+
+		cpu = cpumask_first(&rx->rx_affinity_hint);
+		priv->rx_cxt[i].napiw = octeon3_napi_alloc(&priv->rx_cxt[i],
+							   idx, cpu);
+		if (priv->rx_cxt[i].napiw == NULL) {
+			r = -ENOMEM;
+			goto err4;
+		}
+
+		/* Arm the irq. */
+		octeon3_eth_sso_irq_set_armed(priv->numa_node, rx->rx_grp,
+					      true);
+	}
+	octeon3_eth_replenish_rx(priv, priv->rx_buf_count);
+
+	r = bgx_port_enable(netdev);
+	__cvmx_export_config();
+
+	return r;
+
+ err4:
+	bitmap_clear(priv->rx_cxt[i].napi_idx_bitmap, idx, 1);
+ err3:
+	free_irq(rx->rx_irq, rx);
+ err2:
+	irq_dispose_mapping(rx->rx_irq);
+ err1:
+	for (i--; i >= 0; i--) {
+		rx = priv->rx_cxt + i;
+		irq_dispose_mapping(rx->rx_irq);
+		free_irq(rx->rx_irq, rx);
+		octeon3_rm_napi_from_cxt(priv->numa_node,
+					 priv->rx_cxt[i].napiw);
+		priv->rx_cxt[i].napiw = NULL;
+	}
+
+	return r;
+}
+
+static int octeon3_eth_ndo_stop(struct net_device *netdev)
+{
+	struct octeon3_ethernet *priv = netdev_priv(netdev);
+	void **w;
+	struct sk_buff *skb;
+	struct octeon3_rx *rx;
+	int i;
+	int r;
+
+	r = bgx_port_disable(netdev);
+	if (r)
+		goto err;
+
+	msleep(20);
+
+	for (i = 0; i < priv->num_rx_cxt; i++) {
+		rx = priv->rx_cxt + i;
+		/* Wait for SSO to drain */
+		while (cvmx_read_csr_node(priv->numa_node, CVMX_SSO_GRPX_AQ_CNT(rx->rx_grp)))
+			msleep(20);
+	}
+
+	for (i = 0; i < priv->num_rx_cxt; i++) {
+		rx = priv->rx_cxt + i;
+		octeon3_eth_sso_irq_set_armed(priv->numa_node, rx->rx_grp, false);
+
+		irq_set_affinity_hint(rx->rx_irq, NULL);
+		free_irq(rx->rx_irq, rx);
+		rx->rx_irq = 0;
+	}
+	msleep(20);
+
+	/* Free the packet buffers */
+	for (;;) {
+		w = cvmx_fpa3_alloc(__cvmx_fpa3_gaura(priv->numa_node, priv->pki_laura));
+		if (!w)
+			break;
+		skb = w[0];
+		dev_kfree_skb(skb);
+	}
+
+	/* Free the napis */
+	for (i = 0; i < priv->num_rx_cxt; i++) {
+		octeon3_rm_napi_from_cxt(priv->numa_node,
+					 priv->rx_cxt[i].napiw);
+		priv->rx_cxt[i].napiw = NULL;
+	}
+
+err:
+	return r;
+}
+
+/* octeon3_prepare_skb_to_recycle:	Reset all the skb fields to default
+ *					values so that the skb can be reused.
+ *					Note: the data buffer is not touched.
+ *
+ *  skb:				skb to reset.
+ */
+static inline void octeon3_prepare_skb_to_recycle(struct sk_buff *skb)
+{
+	struct skb_shared_info	*shinfo;
+
+	/* Prepare the skb to be recycled */
+	skb->data_len = 0;
+	skb_frag_list_init(skb);
+	skb_release_head_state(skb);
+
+	shinfo = skb_shinfo(skb);
+	memset(shinfo, 0, offsetof(struct skb_shared_info, dataref));
+	atomic_set(&shinfo->dataref, 1);
+
+	memset(skb, 0, offsetof(struct sk_buff, tail));
+	skb->data = skb->head + NET_SKB_PAD;
+	skb_reset_tail_pointer(skb);
+	skb->truesize = sizeof(*skb) + skb_end_pointer(skb) - skb->head;
+}
+
+static int octeon3_eth_ndo_start_xmit(struct sk_buff *skb, struct net_device *netdev)
+{
+	struct sk_buff *skb_tmp;
+	struct octeon3_ethernet *priv = netdev_priv(netdev);
+	unsigned int scr_off = CVMX_PKO_LMTLINE * CVMX_CACHE_LINE_SIZE;
+	unsigned int ret_off = scr_off;
+	union cvmx_pko_send_hdr send_hdr;
+	union cvmx_pko_buf_ptr buf_ptr;
+	union cvmx_pko_send_work send_work;
+	union cvmx_pko_send_mem send_mem;
+	union cvmx_pko_lmtdma_data lmtdma_data;
+	union cvmx_pko_query_rtn query_rtn;
+	u8 l4_hdr = 0;
+	unsigned int checksum_alg;
+	long backlog;
+	int frag_count;
+	int head_len, i;
+	u64 dma_addr;
+	void **work;
+	bool can_recycle_skb = false;
+	int aura = 0;
+	void *buffers_needed = NULL;
+	void **buf = NULL;
+
+	frag_count = 0;
+	if (skb_has_frag_list(skb))
+		skb_walk_frags(skb, skb_tmp)
+			frag_count++;
+
+	/* Check if the skb can be recycled (freed back to the fpa) */
+	if (likely(recycle_skbs) &&
+	    likely(skb_shinfo(skb)->nr_frags == 0) &&
+	    likely(skb_shared(skb) == 0) &&
+	    likely(skb_cloned(skb) == 0) &&
+	    likely(frag_count == 0) &&
+	    likely(skb->fclone == SKB_FCLONE_UNAVAILABLE)) {
+		uint64_t	magic;
+
+		buf = (void **)PTR_ALIGN(skb->head, 128);
+		magic = (uint64_t)buf[SKB_AURA_OFFSET];
+		if (likely(buf[SKB_PTR_OFFSET] == skb) &&
+		    likely((magic & 0xfffffffffffff000) == SKB_AURA_MAGIC)) {
+			can_recycle_skb = true;
+			aura = magic & 0xfff;
+			buffers_needed = aura2buffers_needed[aura];
+		}
+	}
+
+	/* We have space for 13 segment pointers, If there will be
+	 * more than that, we must linearize.  The count is: 1 (base
+	 * SKB) + frag_count + nr_frags.
+	 */
+	if (unlikely(skb_shinfo(skb)->nr_frags + frag_count > 12)) {
+		if (unlikely(__skb_linearize(skb)))
+			goto skip_xmit;
+		frag_count = 0;
+	}
+
+	work = (void **)skb->cb;
+	work[0] = netdev;
+	work[1] = NULL;
+
+	backlog = atomic64_inc_return(&priv->tx_backlog);
+	if (unlikely(backlog > MAX_TX_QUEUE_DEPTH)) {
+		if (use_tx_queues) {
+			netif_stop_queue(netdev);
+		} else {
+			atomic64_dec(&priv->tx_backlog);
+			goto skip_xmit;
+		}
+	}
+
+	/* Adjust the port statistics. */
+	atomic64_inc(&priv->tx_packets);
+	atomic64_add(skb->len, &priv->tx_octets);
+
+	/* Make sure packet data writes are committed before
+	 * submitting the command below
+	 */
+	wmb();
+	send_hdr.u64 = 0;
+#ifdef __LITTLE_ENDIAN
+	send_hdr.s.le = 1;
+#endif
+	if (!OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X))
+		send_hdr.s.n2 = 1; /* Don't allocate to L2 */
+	send_hdr.s.df = 1; /* Don't automatically free to FPA */
+	send_hdr.s.total = skb->len;
+	send_hdr.s.aura = aura;
+
+	if (skb->ip_summed != CHECKSUM_NONE) {
+#ifndef BROKEN_SIMULATOR_CSUM
+		switch (skb->protocol) {
+		case __constant_htons(ETH_P_IP):
+			send_hdr.s.l3ptr = ETH_HLEN;
+			send_hdr.s.ckl3 = 1;
+			l4_hdr = ip_hdr(skb)->protocol;
+			send_hdr.s.l4ptr = ETH_HLEN + (4 * ip_hdr(skb)->ihl);
+			break;
+		case __constant_htons(ETH_P_IPV6):
+			l4_hdr = ipv6_hdr(skb)->nexthdr;
+			break;
+		default:
+			break;
+		}
+#endif
+		checksum_alg = 1; /* UDP == 1 */
+		switch (l4_hdr) {
+		case IPPROTO_SCTP:
+			if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X))
+				break;
+			checksum_alg++; /* SCTP == 3 */
+			/* Fall through */
+		case IPPROTO_TCP: /* TCP == 2 */
+			checksum_alg++;
+			/* Fall through */
+		case IPPROTO_UDP:
+			if (skb_transport_header_was_set(skb)) {
+				int l4ptr = skb_transport_header(skb) -
+					skb->data;
+				send_hdr.s.l4ptr = l4ptr;
+				send_hdr.s.ckl4 = checksum_alg;
+			}
+			break;
+		default:
+			break;
+		}
+	}
+	cvmx_scratch_write64(scr_off, send_hdr.u64);
+	scr_off += sizeof(send_hdr);
+
+	buf_ptr.u64 = 0;
+	buf_ptr.s.subdc3 = CVMX_PKO_SENDSUBDC_GATHER;
+
+	/* Add a Gather entry for each segment. */
+	head_len = skb_headlen(skb);
+	if (head_len > 0) {
+		buf_ptr.s.size = head_len;
+		buf_ptr.s.addr = virt_to_phys(skb->data);
+		cvmx_scratch_write64(scr_off, buf_ptr.u64);
+		scr_off += sizeof(buf_ptr);
+	}
+	for (i = 1; i <= skb_shinfo(skb)->nr_frags; i++) {
+		struct skb_frag_struct *fs = skb_shinfo(skb)->frags + i - 1;
+		buf_ptr.s.size = fs->size;
+		buf_ptr.s.addr = virt_to_phys((u8 *)page_address(fs->page.p) + fs->page_offset);
+		cvmx_scratch_write64(scr_off, buf_ptr.u64);
+		scr_off += sizeof(buf_ptr);
+	}
+	skb_walk_frags(skb, skb_tmp) {
+		buf_ptr.s.addr = virt_to_phys(skb_tmp->data);
+		buf_ptr.s.size = skb_tmp->len;
+		cvmx_scratch_write64(scr_off, buf_ptr.u64);
+		scr_off += sizeof(buf_ptr);
+	}
+
+	/* Subtract 1 from the tx_backlog. */
+	send_mem.u64 = 0;
+	send_mem.s.subdc4 = CVMX_PKO_SENDSUBDC_MEM;
+	send_mem.s.dsz = MEMDSZ_B64;
+	send_mem.s.alg = MEMALG_SUB;
+	send_mem.s.offset = 1;
+	send_mem.s.addr = virt_to_phys(&priv->tx_backlog);
+	cvmx_scratch_write64(scr_off, send_mem.u64);
+	scr_off += sizeof(buf_ptr);
+
+	if (likely(can_recycle_skb)) {
+		cvmx_pko_send_free_t	send_free;
+
+		/* Subtract 1 from buffers_needed. */
+		send_mem.u64 = 0;
+		send_mem.s.subdc4 = CVMX_PKO_SENDSUBDC_MEM;
+		send_mem.s.dsz = MEMDSZ_B64;
+		send_mem.s.alg = MEMALG_SUB;
+		send_mem.s.offset = 1;
+		send_mem.s.addr = virt_to_phys(buffers_needed);
+		cvmx_scratch_write64(scr_off, send_mem.u64);
+		scr_off += sizeof(buf_ptr);
+
+		/* Free buffer when finished with the packet */
+		send_free.u64 = 0;
+		send_free.s.subdc4 = CVMX_PKO_SENDSUBDC_FREE;
+		buf[SKB_PTR_OFFSET] = skb;
+		send_free.s.addr = virt_to_phys(buf);
+		cvmx_scratch_write64(scr_off, send_free.u64);
+		scr_off += sizeof(buf_ptr);
+
+		/* Reset skb before it's freed back to the fpa */
+		octeon3_prepare_skb_to_recycle(skb);
+	} else {
+		/* Send work when finished with the packet. */
+		send_work.u64 = 0;
+		send_work.s.subdc4 = CVMX_PKO_SENDSUBDC_WORK;
+		send_work.s.addr = virt_to_phys(work);
+		send_work.s.tt = CVMX_POW_TAG_TYPE_NULL;
+		send_work.s.grp = priv->tx_complete_grp;
+		cvmx_scratch_write64(scr_off, send_work.u64);
+		scr_off += sizeof(buf_ptr);
+	}
+
+	lmtdma_data.u64 = 0;
+	lmtdma_data.s.scraddr = ret_off >> 3;
+	lmtdma_data.s.rtnlen = wait_pko_response ? 1 : 0;
+	lmtdma_data.s.did = 0x51;
+	lmtdma_data.s.node = priv->numa_node;
+	lmtdma_data.s.dq = priv->pko_queue;
+	dma_addr = 0xffffffffffffa400ull | ((scr_off & 0x78) - 8);
+	cvmx_write64_uint64(dma_addr, lmtdma_data.u64);
+
+	if (wait_pko_response) {
+		CVMX_SYNCIOBDMA;
+
+		query_rtn.u64 = cvmx_scratch_read64(ret_off);
+		if (unlikely(query_rtn.s.dqstatus != PKO_DQSTATUS_PASS)) {
+			netdev_err(netdev, "PKO enqueue failed %llx\n",
+				   (unsigned long long)query_rtn.u64);
+			dev_kfree_skb_any(skb);
+		}
+	}
+
+	return NETDEV_TX_OK;
+skip_xmit:
+	atomic64_inc(&priv->tx_dropped);
+	dev_kfree_skb_any(skb);
+	return NETDEV_TX_OK;
+}
+
+static u64 read_pki_stat(int numa_node, u64 csr)
+{
+	u64 v;
+
+	/* PKI-20775, must read until not all ones. */
+	do {
+		v = cvmx_read_csr_node(numa_node, csr);
+	} while (v == 0xffffffffffffffffull);
+	return v;
+}
+
+static struct rtnl_link_stats64 *octeon3_eth_ndo_get_stats64(struct net_device *netdev,
+							     struct rtnl_link_stats64 *s)
+{
+	struct octeon3_ethernet *priv = netdev_priv(netdev);
+	u64 packets, octets, dropped;
+	u64 delta_packets, delta_octets, delta_dropped;
+
+	spin_lock(&priv->stat_lock);
+
+	packets = read_pki_stat(priv->numa_node, CVMX_PKI_STATX_STAT0(priv->pki_pkind));
+	octets = read_pki_stat(priv->numa_node, CVMX_PKI_STATX_STAT1(priv->pki_pkind));
+	dropped = read_pki_stat(priv->numa_node, CVMX_PKI_STATX_STAT3(priv->pki_pkind));
+
+	delta_packets = (packets - priv->last_packets) & ((1ull << 48) - 1);
+	delta_octets = (octets - priv->last_octets) & ((1ull << 48) - 1);
+	delta_dropped = (dropped - priv->last_dropped) & ((1ull << 48) - 1);
+
+	priv->last_packets = packets;
+	priv->last_octets = octets;
+	priv->last_dropped = dropped;
+
+	spin_unlock(&priv->stat_lock);
+
+	atomic64_add(delta_packets, &priv->rx_packets);
+	atomic64_add(delta_octets, &priv->rx_octets);
+	atomic64_add(delta_dropped, &priv->rx_dropped);
+
+	s->rx_packets = atomic64_read(&priv->rx_packets);
+	s->rx_bytes = atomic64_read(&priv->rx_octets);
+	s->rx_dropped = atomic64_read(&priv->rx_dropped);
+	s->rx_errors = atomic64_read(&priv->rx_errors);
+	s->rx_length_errors = atomic64_read(&priv->rx_length_errors);
+	s->rx_crc_errors = atomic64_read(&priv->rx_crc_errors);
+
+	s->tx_packets = atomic64_read(&priv->tx_packets);
+	s->tx_bytes = atomic64_read(&priv->tx_octets);
+	s->tx_dropped = atomic64_read(&priv->tx_dropped);
+	return s;
+}
+
+static int octeon3_eth_set_mac_address(struct net_device *netdev, void *addr)
+{
+	int r = eth_mac_addr(netdev, addr);
+
+	if (r)
+		return r;
+
+	bgx_port_set_rx_filtering(netdev);
+
+	return 0;
+}
+
+static const struct net_device_ops octeon3_eth_netdev_ops = {
+	.ndo_init		= octeon3_eth_ndo_init,
+	.ndo_uninit		= octeon3_eth_ndo_uninit,
+	.ndo_open		= octeon3_eth_ndo_open,
+	.ndo_stop		= octeon3_eth_ndo_stop,
+	.ndo_start_xmit		= octeon3_eth_ndo_start_xmit,
+	.ndo_get_stats64	= octeon3_eth_ndo_get_stats64,
+	.ndo_set_rx_mode	= bgx_port_set_rx_filtering,
+	.ndo_set_mac_address	= octeon3_eth_set_mac_address,
+	.ndo_change_mtu		= bgx_port_change_mtu
+};
+
+static int octeon3_eth_probe(struct platform_device *pdev)
+{
+	struct octeon3_ethernet *priv;
+	struct net_device *netdev;
+	int r;
+
+	struct bgx_platform_data *pd = dev_get_platdata(&pdev->dev);
+
+	r = octeon3_eth_global_init(pd->numa_node);
+	if (r)
+		return r;
+
+	dev_err(&pdev->dev, "Probing %d-%d:%d\n", pd->numa_node, pd->interface, pd->port);
+	netdev = alloc_etherdev(sizeof(struct octeon3_ethernet));
+	if (!netdev) {
+		dev_err(&pdev->dev, "Failed to allocated ethernet device\n");
+		return -ENOMEM;
+	}
+
+	/* Using transmit queues degrades performance significantly */
+	if (!use_tx_queues)
+		netdev->tx_queue_len = 0;
+
+	SET_NETDEV_DEV(netdev, &pdev->dev);
+	dev_set_drvdata(&pdev->dev, netdev);
+
+	bgx_port_set_netdev(pdev->dev.parent, netdev);
+	priv = netdev_priv(netdev);
+	priv->netdev = netdev;
+	INIT_LIST_HEAD(&priv->list);
+	priv->numa_node = pd->numa_node;
+
+	mutex_lock(&octeon3_eth_node[priv->numa_node].device_list_lock);
+	list_add_tail_rcu(&priv->list, &octeon3_eth_node[priv->numa_node].device_list);
+	mutex_unlock(&octeon3_eth_node[priv->numa_node].device_list_lock);
+
+	priv->xiface = cvmx_helper_node_interface_to_xiface(pd->numa_node, pd->interface);
+	priv->port_index = pd->port;
+	spin_lock_init(&priv->stat_lock);
+	netdev->netdev_ops = &octeon3_eth_netdev_ops;
+
+	if (register_netdev(netdev) < 0) {
+		dev_err(&pdev->dev, "Failed to register ethernet device\n");
+		list_del(&priv->list);
+		free_netdev(netdev);
+	}
+	netdev_info(netdev, "Registered\n");
+	return 0;
+}
+
+static int octeon3_eth_remove(struct platform_device *pdev)
+{
+	return 0;
+}
+
+static void octeon3_eth_shutdown(struct platform_device *pdev)
+{
+	return;
+}
+
+
+static struct platform_driver octeon3_eth_driver = {
+	.probe		= octeon3_eth_probe,
+	.remove		= octeon3_eth_remove,
+	.shutdown       = octeon3_eth_shutdown,
+	.driver		= {
+		.owner	= THIS_MODULE,
+		.name	= "ethernet-mac-pki",
+	},
+};
+
+
+
+static int __init octeon3_eth_init(void)
+{
+	if (!OCTEON_IS_MODEL(OCTEON_CN78XX))
+		return 0;
+
+	if (rx_contexts <= 0)
+		rx_contexts = 1;
+	if (rx_contexts > MAX_RX_CONTEXTS)
+		rx_contexts = MAX_RX_CONTEXTS;
+
+	return platform_driver_register(&octeon3_eth_driver);
+}
+module_init(octeon3_eth_init);
+
+static void __exit octeon3_eth_exit(void)
+{
+	if (!OCTEON_IS_MODEL(OCTEON_CN78XX))
+		return;
+
+	platform_driver_unregister(&octeon3_eth_driver);
+}
+module_exit(octeon3_eth_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Cavium Networks <support@caviumnetworks.com>");
+MODULE_DESCRIPTION("Cavium Networks PKI/PKO Ethernet driver.");
diff --git a/drivers/net/ethernet/octeon/octeon_common.h b/drivers/net/ethernet/octeon/octeon_common.h
new file mode 100644
index 0000000..4e642a7
--- /dev/null
+++ b/drivers/net/ethernet/octeon/octeon_common.h
@@ -0,0 +1,40 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2013 Cavium, Inc.
+ */
+#ifndef _ETHERNET_OCTEON_OCTEON_COMMON_H
+#define _ETHERNET_OCTEON_OCTEON_COMMON_H
+
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/if_vlan.h>
+
+/* Allow 8 bytes for vlan and FCS. */
+#define OCTEON_FRAME_HEADER_LEN	(ETH_HLEN + ETH_FCS_LEN + VLAN_HLEN)
+
+#define GMX_PRT_CFG                 0x10
+
+#define GMX_RX_FRM_MAX              0x30
+#define GMX_RX_JABBER               0x38
+
+#define GMX_RX_ADR_CTL              0x100
+#define GMX_RX_ADR_CAM_EN           0x108
+#define GMX_RX_ADR_CAM0             0x180
+#define GMX_RX_ADR_CAM1             0x188
+#define GMX_RX_ADR_CAM2             0x190
+#define GMX_RX_ADR_CAM3             0x198
+#define GMX_RX_ADR_CAM4             0x1a0
+#define GMX_RX_ADR_CAM5             0x1a8
+
+extern void cvm_oct_common_set_rx_filtering(struct net_device *dev, u64 base_reg,
+					spinlock_t *lock);
+
+extern int cvm_oct_common_set_mac_address(struct net_device *dev, void *addr,
+					u64 base_reg, spinlock_t *lock);
+
+extern int cvm_oct_common_change_mtu(struct net_device *dev, int mtu, u64 base_reg,
+		u64 pip_reg, int mtu_limit);
+#endif
diff --git a/drivers/net/ethernet/octeon/octeon_mgmt.c b/drivers/net/ethernet/octeon/octeon_mgmt.c
index 7bf9c02..ce764aa 100644
--- a/drivers/net/ethernet/octeon/octeon_mgmt.c
+++ b/drivers/net/ethernet/octeon/octeon_mgmt.c
@@ -1045,13 +1045,13 @@ static int octeon_mgmt_open(struct net_device *netdev)
 	}
 
 	oring1.u64 = 0;
-	oring1.s.obase = p->tx_ring_handle >> 3;
-	oring1.s.osize = OCTEON_MGMT_TX_RING_SIZE;
+	oring1.cn63xx.obase = p->tx_ring_handle >> 3;
+	oring1.cn63xx.osize = OCTEON_MGMT_TX_RING_SIZE;
 	cvmx_write_csr(p->mix + MIX_ORING1, oring1.u64);
 
 	iring1.u64 = 0;
-	iring1.s.ibase = p->rx_ring_handle >> 3;
-	iring1.s.isize = OCTEON_MGMT_RX_RING_SIZE;
+	iring1.cn63xx.ibase = p->rx_ring_handle >> 3;
+	iring1.cn63xx.isize = OCTEON_MGMT_RX_RING_SIZE;
 	cvmx_write_csr(p->mix + MIX_IRING1, iring1.u64);
 
 	memcpy(sa.sa_data, netdev->dev_addr, ETH_ALEN);
diff --git a/drivers/staging/Kconfig b/drivers/staging/Kconfig
index bfacf69..73cfa7e 100644
--- a/drivers/staging/Kconfig
+++ b/drivers/staging/Kconfig
@@ -46,8 +46,6 @@ source "drivers/staging/rtl8723au/Kconfig"
 
 source "drivers/staging/rts5208/Kconfig"
 
-source "drivers/staging/octeon/Kconfig"
-
 source "drivers/staging/octeon-usb/Kconfig"
 
 source "drivers/staging/vt6655/Kconfig"
diff --git a/drivers/staging/Makefile b/drivers/staging/Makefile
index 2bbd1bf..997d81b 100644
--- a/drivers/staging/Makefile
+++ b/drivers/staging/Makefile
@@ -16,7 +16,6 @@ obj-$(CONFIG_R8188EU)		+= rtl8188eu/
 obj-$(CONFIG_R8723AU)		+= rtl8723au/
 obj-$(CONFIG_RTS5208)		+= rts5208/
 obj-$(CONFIG_NETLOGIC_XLR_NET)	+= netlogic/
-obj-$(CONFIG_OCTEON_ETHERNET)	+= octeon/
 obj-$(CONFIG_OCTEON_USB)	+= octeon-usb/
 obj-$(CONFIG_VT6655)		+= vt6655/
 obj-$(CONFIG_VT6656)		+= vt6656/
diff --git a/drivers/staging/octeon/Kconfig b/drivers/staging/octeon/Kconfig
deleted file mode 100644
index 6e1d5f8..0000000
--- a/drivers/staging/octeon/Kconfig
+++ /dev/null
@@ -1,13 +0,0 @@
-config OCTEON_ETHERNET
-	tristate "Cavium Networks Octeon Ethernet support"
-	depends on CAVIUM_OCTEON_SOC && NETDEVICES
-	select PHYLIB
-	select MDIO_OCTEON
-	help
-	  This driver supports the builtin ethernet ports on Cavium
-	  Networks' products in the Octeon family. This driver supports the
-	  CN3XXX and CN5XXX Octeon processors.
-
-	  To compile this driver as a module, choose M here.  The module
-	  will be called octeon-ethernet.
-
diff --git a/drivers/staging/octeon/Makefile b/drivers/staging/octeon/Makefile
deleted file mode 100644
index 9012dee..0000000
--- a/drivers/staging/octeon/Makefile
+++ /dev/null
@@ -1,23 +0,0 @@
-# This file is subject to the terms and conditions of the GNU General Public
-# License.  See the file "COPYING" in the main directory of this archive
-# for more details.
-#
-# Copyright (C) 2005-2009 Cavium Networks
-#
-
-#
-# Makefile for Cavium OCTEON on-board ethernet driver
-#
-
-obj-${CONFIG_OCTEON_ETHERNET} :=  octeon-ethernet.o
-
-octeon-ethernet-y := ethernet.o
-octeon-ethernet-y += ethernet-mdio.o
-octeon-ethernet-y += ethernet-mem.o
-octeon-ethernet-y += ethernet-rgmii.o
-octeon-ethernet-y += ethernet-rx.o
-octeon-ethernet-y += ethernet-sgmii.o
-octeon-ethernet-y += ethernet-spi.o
-octeon-ethernet-y += ethernet-tx.o
-octeon-ethernet-y += ethernet-xaui.o
-
diff --git a/drivers/staging/octeon/ethernet-mem.h b/drivers/staging/octeon/ethernet-mem.h
deleted file mode 100644
index 713f2ed..0000000
--- a/drivers/staging/octeon/ethernet-mem.h
+++ /dev/null
@@ -1,29 +0,0 @@
-/*********************************************************************
- * Author: Cavium Networks
- *
- * Contact: support@caviumnetworks.com
- * This file is part of the OCTEON SDK
- *
- * Copyright (c) 2003-2007 Cavium Networks
- *
- * This file is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, Version 2, as
- * published by the Free Software Foundation.
- *
- * This file is distributed in the hope that it will be useful, but
- * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
- * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
- * NONINFRINGEMENT.  See the GNU General Public License for more
- * details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this file; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- * or visit http://www.gnu.org/licenses/.
- *
- * This file may also be available under a different license from Cavium.
- * Contact Cavium Networks for more information
-********************************************************************/
-
-int cvm_oct_mem_fill_fpa(int pool, int size, int elements);
-void cvm_oct_mem_empty_fpa(int pool, int size, int elements);
diff --git a/drivers/staging/octeon/ethernet-rx.h b/drivers/staging/octeon/ethernet-rx.h
deleted file mode 100644
index 9240c85..0000000
--- a/drivers/staging/octeon/ethernet-rx.h
+++ /dev/null
@@ -1,52 +0,0 @@
-/*********************************************************************
- * Author: Cavium Networks
- *
- * Contact: support@caviumnetworks.com
- * This file is part of the OCTEON SDK
- *
- * Copyright (c) 2003-2007 Cavium Networks
- *
- * This file is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, Version 2, as
- * published by the Free Software Foundation.
- *
- * This file is distributed in the hope that it will be useful, but
- * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
- * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
- * NONINFRINGEMENT.  See the GNU General Public License for more
- * details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this file; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- * or visit http://www.gnu.org/licenses/.
- *
- * This file may also be available under a different license from Cavium.
- * Contact Cavium Networks for more information
-*********************************************************************/
-#include <asm/octeon/cvmx-fau.h>
-
-void cvm_oct_poll_controller(struct net_device *dev);
-void cvm_oct_rx_initialize(void);
-void cvm_oct_rx_shutdown(void);
-
-static inline void cvm_oct_rx_refill_pool(int fill_threshold)
-{
-	int number_to_free;
-	int num_freed;
-	/* Refill the packet buffer pool */
-	number_to_free =
-		cvmx_fau_fetch_and_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);
-
-	if (number_to_free > fill_threshold) {
-		cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE,
-				      -number_to_free);
-		num_freed = cvm_oct_mem_fill_fpa(CVMX_FPA_PACKET_POOL,
-						 CVMX_FPA_PACKET_POOL_SIZE,
-						 number_to_free);
-		if (num_freed != number_to_free) {
-			cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE,
-					number_to_free - num_freed);
-		}
-	}
-}
diff --git a/drivers/staging/octeon/ethernet-spi.c b/drivers/staging/octeon/ethernet-spi.c
deleted file mode 100644
index 5108bc0..0000000
--- a/drivers/staging/octeon/ethernet-spi.c
+++ /dev/null
@@ -1,292 +0,0 @@
-/**********************************************************************
- * Author: Cavium Networks
- *
- * Contact: support@caviumnetworks.com
- * This file is part of the OCTEON SDK
- *
- * Copyright (c) 2003-2007 Cavium Networks
- *
- * This file is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, Version 2, as
- * published by the Free Software Foundation.
- *
- * This file is distributed in the hope that it will be useful, but
- * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
- * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
- * NONINFRINGEMENT.  See the GNU General Public License for more
- * details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this file; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- * or visit http://www.gnu.org/licenses/.
- *
- * This file may also be available under a different license from Cavium.
- * Contact Cavium Networks for more information
-**********************************************************************/
-#include <linux/kernel.h>
-#include <linux/netdevice.h>
-#include <linux/interrupt.h>
-#include <net/dst.h>
-
-#include <asm/octeon/octeon.h>
-
-#include "ethernet-defines.h"
-#include "octeon-ethernet.h"
-#include "ethernet-util.h"
-
-#include <asm/octeon/cvmx-spi.h>
-
-#include <asm/octeon/cvmx-npi-defs.h>
-#include <asm/octeon/cvmx-spxx-defs.h>
-#include <asm/octeon/cvmx-stxx-defs.h>
-
-static int number_spi_ports;
-static int need_retrain[2] = { 0, 0 };
-
-static irqreturn_t cvm_oct_spi_rml_interrupt(int cpl, void *dev_id)
-{
-	irqreturn_t return_status = IRQ_NONE;
-	union cvmx_npi_rsl_int_blocks rsl_int_blocks;
-
-	/* Check and see if this interrupt was caused by the GMX block */
-	rsl_int_blocks.u64 = cvmx_read_csr(CVMX_NPI_RSL_INT_BLOCKS);
-	if (rsl_int_blocks.s.spx1) {	/* 19 - SPX1_INT_REG & STX1_INT_REG */
-
-		union cvmx_spxx_int_reg spx_int_reg;
-		union cvmx_stxx_int_reg stx_int_reg;
-
-		spx_int_reg.u64 = cvmx_read_csr(CVMX_SPXX_INT_REG(1));
-		cvmx_write_csr(CVMX_SPXX_INT_REG(1), spx_int_reg.u64);
-		if (!need_retrain[1]) {
-
-			spx_int_reg.u64 &= cvmx_read_csr(CVMX_SPXX_INT_MSK(1));
-			if (spx_int_reg.s.spf)
-				pr_err("SPI1: SRX Spi4 interface down\n");
-			if (spx_int_reg.s.calerr)
-				pr_err("SPI1: SRX Spi4 Calendar table parity error\n");
-			if (spx_int_reg.s.syncerr)
-				pr_err("SPI1: SRX Consecutive Spi4 DIP4 errors have exceeded SPX_ERR_CTL[ERRCNT]\n");
-			if (spx_int_reg.s.diperr)
-				pr_err("SPI1: SRX Spi4 DIP4 error\n");
-			if (spx_int_reg.s.tpaovr)
-				pr_err("SPI1: SRX Selected port has hit TPA overflow\n");
-			if (spx_int_reg.s.rsverr)
-				pr_err("SPI1: SRX Spi4 reserved control word detected\n");
-			if (spx_int_reg.s.drwnng)
-				pr_err("SPI1: SRX Spi4 receive FIFO drowning/overflow\n");
-			if (spx_int_reg.s.clserr)
-				pr_err("SPI1: SRX Spi4 packet closed on non-16B alignment without EOP\n");
-			if (spx_int_reg.s.spiovr)
-				pr_err("SPI1: SRX Spi4 async FIFO overflow\n");
-			if (spx_int_reg.s.abnorm)
-				pr_err("SPI1: SRX Abnormal packet termination (ERR bit)\n");
-			if (spx_int_reg.s.prtnxa)
-				pr_err("SPI1: SRX Port out of range\n");
-		}
-
-		stx_int_reg.u64 = cvmx_read_csr(CVMX_STXX_INT_REG(1));
-		cvmx_write_csr(CVMX_STXX_INT_REG(1), stx_int_reg.u64);
-		if (!need_retrain[1]) {
-
-			stx_int_reg.u64 &= cvmx_read_csr(CVMX_STXX_INT_MSK(1));
-			if (stx_int_reg.s.syncerr)
-				pr_err("SPI1: STX Interface encountered a fatal error\n");
-			if (stx_int_reg.s.frmerr)
-				pr_err("SPI1: STX FRMCNT has exceeded STX_DIP_CNT[MAXFRM]\n");
-			if (stx_int_reg.s.unxfrm)
-				pr_err("SPI1: STX Unexpected framing sequence\n");
-			if (stx_int_reg.s.nosync)
-				pr_err("SPI1: STX ERRCNT has exceeded STX_DIP_CNT[MAXDIP]\n");
-			if (stx_int_reg.s.diperr)
-				pr_err("SPI1: STX DIP2 error on the Spi4 Status channel\n");
-			if (stx_int_reg.s.datovr)
-				pr_err("SPI1: STX Spi4 FIFO overflow error\n");
-			if (stx_int_reg.s.ovrbst)
-				pr_err("SPI1: STX Transmit packet burst too big\n");
-			if (stx_int_reg.s.calpar1)
-				pr_err("SPI1: STX Calendar Table Parity Error Bank1\n");
-			if (stx_int_reg.s.calpar0)
-				pr_err("SPI1: STX Calendar Table Parity Error Bank0\n");
-		}
-
-		cvmx_write_csr(CVMX_SPXX_INT_MSK(1), 0);
-		cvmx_write_csr(CVMX_STXX_INT_MSK(1), 0);
-		need_retrain[1] = 1;
-		return_status = IRQ_HANDLED;
-	}
-
-	if (rsl_int_blocks.s.spx0) {	/* 18 - SPX0_INT_REG & STX0_INT_REG */
-		union cvmx_spxx_int_reg spx_int_reg;
-		union cvmx_stxx_int_reg stx_int_reg;
-
-		spx_int_reg.u64 = cvmx_read_csr(CVMX_SPXX_INT_REG(0));
-		cvmx_write_csr(CVMX_SPXX_INT_REG(0), spx_int_reg.u64);
-		if (!need_retrain[0]) {
-
-			spx_int_reg.u64 &= cvmx_read_csr(CVMX_SPXX_INT_MSK(0));
-			if (spx_int_reg.s.spf)
-				pr_err("SPI0: SRX Spi4 interface down\n");
-			if (spx_int_reg.s.calerr)
-				pr_err("SPI0: SRX Spi4 Calendar table parity error\n");
-			if (spx_int_reg.s.syncerr)
-				pr_err("SPI0: SRX Consecutive Spi4 DIP4 errors have exceeded SPX_ERR_CTL[ERRCNT]\n");
-			if (spx_int_reg.s.diperr)
-				pr_err("SPI0: SRX Spi4 DIP4 error\n");
-			if (spx_int_reg.s.tpaovr)
-				pr_err("SPI0: SRX Selected port has hit TPA overflow\n");
-			if (spx_int_reg.s.rsverr)
-				pr_err("SPI0: SRX Spi4 reserved control word detected\n");
-			if (spx_int_reg.s.drwnng)
-				pr_err("SPI0: SRX Spi4 receive FIFO drowning/overflow\n");
-			if (spx_int_reg.s.clserr)
-				pr_err("SPI0: SRX Spi4 packet closed on non-16B alignment without EOP\n");
-			if (spx_int_reg.s.spiovr)
-				pr_err("SPI0: SRX Spi4 async FIFO overflow\n");
-			if (spx_int_reg.s.abnorm)
-				pr_err("SPI0: SRX Abnormal packet termination (ERR bit)\n");
-			if (spx_int_reg.s.prtnxa)
-				pr_err("SPI0: SRX Port out of range\n");
-		}
-
-		stx_int_reg.u64 = cvmx_read_csr(CVMX_STXX_INT_REG(0));
-		cvmx_write_csr(CVMX_STXX_INT_REG(0), stx_int_reg.u64);
-		if (!need_retrain[0]) {
-
-			stx_int_reg.u64 &= cvmx_read_csr(CVMX_STXX_INT_MSK(0));
-			if (stx_int_reg.s.syncerr)
-				pr_err("SPI0: STX Interface encountered a fatal error\n");
-			if (stx_int_reg.s.frmerr)
-				pr_err("SPI0: STX FRMCNT has exceeded STX_DIP_CNT[MAXFRM]\n");
-			if (stx_int_reg.s.unxfrm)
-				pr_err("SPI0: STX Unexpected framing sequence\n");
-			if (stx_int_reg.s.nosync)
-				pr_err("SPI0: STX ERRCNT has exceeded STX_DIP_CNT[MAXDIP]\n");
-			if (stx_int_reg.s.diperr)
-				pr_err("SPI0: STX DIP2 error on the Spi4 Status channel\n");
-			if (stx_int_reg.s.datovr)
-				pr_err("SPI0: STX Spi4 FIFO overflow error\n");
-			if (stx_int_reg.s.ovrbst)
-				pr_err("SPI0: STX Transmit packet burst too big\n");
-			if (stx_int_reg.s.calpar1)
-				pr_err("SPI0: STX Calendar Table Parity Error Bank1\n");
-			if (stx_int_reg.s.calpar0)
-				pr_err("SPI0: STX Calendar Table Parity Error Bank0\n");
-		}
-
-		cvmx_write_csr(CVMX_SPXX_INT_MSK(0), 0);
-		cvmx_write_csr(CVMX_STXX_INT_MSK(0), 0);
-		need_retrain[0] = 1;
-		return_status = IRQ_HANDLED;
-	}
-
-	return return_status;
-}
-
-static void cvm_oct_spi_enable_error_reporting(int interface)
-{
-	union cvmx_spxx_int_msk spxx_int_msk;
-	union cvmx_stxx_int_msk stxx_int_msk;
-
-	spxx_int_msk.u64 = cvmx_read_csr(CVMX_SPXX_INT_MSK(interface));
-	spxx_int_msk.s.calerr = 1;
-	spxx_int_msk.s.syncerr = 1;
-	spxx_int_msk.s.diperr = 1;
-	spxx_int_msk.s.tpaovr = 1;
-	spxx_int_msk.s.rsverr = 1;
-	spxx_int_msk.s.drwnng = 1;
-	spxx_int_msk.s.clserr = 1;
-	spxx_int_msk.s.spiovr = 1;
-	spxx_int_msk.s.abnorm = 1;
-	spxx_int_msk.s.prtnxa = 1;
-	cvmx_write_csr(CVMX_SPXX_INT_MSK(interface), spxx_int_msk.u64);
-
-	stxx_int_msk.u64 = cvmx_read_csr(CVMX_STXX_INT_MSK(interface));
-	stxx_int_msk.s.frmerr = 1;
-	stxx_int_msk.s.unxfrm = 1;
-	stxx_int_msk.s.nosync = 1;
-	stxx_int_msk.s.diperr = 1;
-	stxx_int_msk.s.datovr = 1;
-	stxx_int_msk.s.ovrbst = 1;
-	stxx_int_msk.s.calpar1 = 1;
-	stxx_int_msk.s.calpar0 = 1;
-	cvmx_write_csr(CVMX_STXX_INT_MSK(interface), stxx_int_msk.u64);
-}
-
-static void cvm_oct_spi_poll(struct net_device *dev)
-{
-	static int spi4000_port;
-	struct octeon_ethernet *priv = netdev_priv(dev);
-	int interface;
-
-	for (interface = 0; interface < 2; interface++) {
-
-		if ((priv->port == interface * 16) && need_retrain[interface]) {
-
-			if (cvmx_spi_restart_interface
-			    (interface, CVMX_SPI_MODE_DUPLEX, 10) == 0) {
-				need_retrain[interface] = 0;
-				cvm_oct_spi_enable_error_reporting(interface);
-			}
-		}
-
-		/*
-		 * The SPI4000 TWSI interface is very slow. In order
-		 * not to bring the system to a crawl, we only poll a
-		 * single port every second. This means negotiation
-		 * speed changes take up to 10 seconds, but at least
-		 * we don't waste absurd amounts of time waiting for
-		 * TWSI.
-		 */
-		if (priv->port == spi4000_port) {
-			/*
-			 * This function does nothing if it is called on an
-			 * interface without a SPI4000.
-			 */
-			cvmx_spi4000_check_speed(interface, priv->port);
-			/*
-			 * Normal ordering increments. By decrementing
-			 * we only match once per iteration.
-			 */
-			spi4000_port--;
-			if (spi4000_port < 0)
-				spi4000_port = 10;
-		}
-	}
-}
-
-int cvm_oct_spi_init(struct net_device *dev)
-{
-	int r;
-	struct octeon_ethernet *priv = netdev_priv(dev);
-
-	if (number_spi_ports == 0) {
-		r = request_irq(OCTEON_IRQ_RML, cvm_oct_spi_rml_interrupt,
-				IRQF_SHARED, "SPI", &number_spi_ports);
-		if (r)
-			return r;
-	}
-	number_spi_ports++;
-
-	if ((priv->port == 0) || (priv->port == 16)) {
-		cvm_oct_spi_enable_error_reporting(INTERFACE(priv->port));
-		priv->poll = cvm_oct_spi_poll;
-	}
-	cvm_oct_common_init(dev);
-	return 0;
-}
-
-void cvm_oct_spi_uninit(struct net_device *dev)
-{
-	int interface;
-
-	cvm_oct_common_uninit(dev);
-	number_spi_ports--;
-	if (number_spi_ports == 0) {
-		for (interface = 0; interface < 2; interface++) {
-			cvmx_write_csr(CVMX_SPXX_INT_MSK(interface), 0);
-			cvmx_write_csr(CVMX_STXX_INT_MSK(interface), 0);
-		}
-		free_irq(OCTEON_IRQ_RML, &number_spi_ports);
-	}
-}
diff --git a/drivers/staging/octeon/ethernet-tx.h b/drivers/staging/octeon/ethernet-tx.h
deleted file mode 100644
index 547680c..0000000
--- a/drivers/staging/octeon/ethernet-tx.h
+++ /dev/null
@@ -1,34 +0,0 @@
-/*********************************************************************
- * Author: Cavium Networks
- *
- * Contact: support@caviumnetworks.com
- * This file is part of the OCTEON SDK
- *
- * Copyright (c) 2003-2007 Cavium Networks
- *
- * This file is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, Version 2, as
- * published by the Free Software Foundation.
- *
- * This file is distributed in the hope that it will be useful, but
- * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
- * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
- * NONINFRINGEMENT.  See the GNU General Public License for more
- * details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this file; if not, write to the Free Software
- * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
- * or visit http://www.gnu.org/licenses/.
- *
- * This file may also be available under a different license from Cavium.
- * Contact Cavium Networks for more information
-*********************************************************************/
-
-int cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev);
-int cvm_oct_xmit_pow(struct sk_buff *skb, struct net_device *dev);
-int cvm_oct_transmit_qos(struct net_device *dev, void *work_queue_entry,
-			 int do_free, int qos);
-void cvm_oct_tx_initialize(void);
-void cvm_oct_tx_shutdown(void);
-void cvm_oct_tx_shutdown_dev(struct net_device *dev);
diff --git a/include/linux/skbuff.h b/include/linux/skbuff.h
index f15154a..ec6ce5c 100644
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@ -761,6 +761,7 @@ static inline struct rtable *skb_rtable(const struct sk_buff *skb)
 	return (struct rtable *)skb_dst(skb);
 }
 
+void skb_release_head_state(struct sk_buff *skb);
 void kfree_skb(struct sk_buff *skb);
 void kfree_skb_list(struct sk_buff *segs);
 void skb_tx_error(struct sk_buff *skb);
diff --git a/net/core/skbuff.c b/net/core/skbuff.c
index 41ec022..3b2eeed 100644
--- a/net/core/skbuff.c
+++ b/net/core/skbuff.c
@@ -681,7 +681,7 @@ fastpath:
 	kmem_cache_free(skbuff_fclone_cache, fclones);
 }
 
-static void skb_release_head_state(struct sk_buff *skb)
+void skb_release_head_state(struct sk_buff *skb)
 {
 	skb_dst_drop(skb);
 #ifdef CONFIG_XFRM
@@ -698,6 +698,7 @@ static void skb_release_head_state(struct sk_buff *skb)
 	nf_bridge_put(skb->nf_bridge);
 #endif
 }
+EXPORT_SYMBOL(skb_release_head_state);
 
 /* Free everything but the sk_buff shell. */
 static void skb_release_all(struct sk_buff *skb)
-- 
1.9.1

