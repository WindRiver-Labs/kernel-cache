From 0d125307106cb3450f93dd14e3de12ffc7b83c98 Mon Sep 17 00:00:00 2001
From: David Daney <david.daney@cavium.com>
Date: Sat, 19 Apr 2014 16:04:59 -0700
Subject: [PATCH 049/202] netdev: Preliminary Ethernet driver for Cavium
 cn78XX.

It can ping, but buffer management and resource management, needs
improvement.

Signed-off-by: David Daney <david.daney@cavium.com>
[Original patch taken from Cavium SDK 3.1.1-544]
Signed-off-by: Bin Jiang <bin.jiang@windriver.com>
---
 drivers/net/ethernet/octeon/Kconfig            |  14 +
 drivers/net/ethernet/octeon/Makefile           |   3 +-
 drivers/net/ethernet/octeon/octeon-bgx.c       | 311 ++++++++++
 drivers/net/ethernet/octeon/octeon-bgx.h       |  45 ++
 drivers/net/ethernet/octeon/octeon3-ethernet.c | 820 +++++++++++++++++++++++++
 5 files changed, 1192 insertions(+), 1 deletion(-)
 create mode 100644 drivers/net/ethernet/octeon/octeon-bgx.c
 create mode 100644 drivers/net/ethernet/octeon/octeon-bgx.h
 create mode 100644 drivers/net/ethernet/octeon/octeon3-ethernet.c

diff --git a/drivers/net/ethernet/octeon/Kconfig b/drivers/net/ethernet/octeon/Kconfig
index 172a10e..0e8fafe 100644
--- a/drivers/net/ethernet/octeon/Kconfig
+++ b/drivers/net/ethernet/octeon/Kconfig
@@ -1,6 +1,20 @@
 #
 # Cavium network device configuration
 #
+
+config OCTEON_BGX
+	tristate "Cavium Inc. OCTEON III BGX support"
+	depends on CAVIUM_OCTEON_SOC
+	help
+	  Support for 'BGX' Ethernet MAC on cn78XX.
+
+config OCTEON3_ETHERNET
+	tristate "Cavium Inc. OCTEON III Ethernet support"
+	depends on CAVIUM_OCTEON_SOC
+	select OCTEON_BGX
+	help
+	  Support for 'BGX' Ethernet via PKI/PKO units.
+
 config OCTEON_ETHERNET
 	tristate "Cavium Inc. OCTEON Ethernet support"
 	depends on CPU_CAVIUM_OCTEON
diff --git a/drivers/net/ethernet/octeon/Makefile b/drivers/net/ethernet/octeon/Makefile
index fd2cd24..e0d2db8 100644
--- a/drivers/net/ethernet/octeon/Makefile
+++ b/drivers/net/ethernet/octeon/Makefile
@@ -1,7 +1,8 @@
 #
 # Makefile for the Cavium network device drivers.
 #
-
+obj-$(CONFIG_OCTEON_BGX)		+= octeon-bgx.o
+obj-$(CONFIG_OCTEON3_ETHERNET)		+= octeon3-ethernet.o
 obj-$(CONFIG_OCTEON_MGMT_ETHERNET)	+= octeon_mgmt.o
 obj-$(CONFIG_OCTEON_POW_ONLY_ETHERNET)	+= octeon-pow-ethernet.o
 obj-$(CONFIG_OCTEON_ETHERNET) +=  octeon-ethernet.o
diff --git a/drivers/net/ethernet/octeon/octeon-bgx.c b/drivers/net/ethernet/octeon/octeon-bgx.c
new file mode 100644
index 0000000..7ce27b8
--- /dev/null
+++ b/drivers/net/ethernet/octeon/octeon-bgx.c
@@ -0,0 +1,311 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2014 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+#include <linux/platform_device.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/of_platform.h>
+#include <linux/of_address.h>
+#include <linux/of_net.h>
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/list.h>
+
+#include <asm/octeon/cvmx-helper.h>
+#include <asm/octeon/cvmx-helper-util.h>
+#include <asm/octeon/cvmx-helper-cfg.h>
+#include <asm/octeon/cvmx-bgxx-defs.h>
+
+#include "octeon-bgx.h"
+
+struct bgx_port_priv {
+	int numa_node;
+	int bgx_interface;
+	int index; /* Port index on BGX block*/
+	const u8 *mac_addr;
+};
+
+const u8 *bgx_port_get_mac(struct device *dev)
+{
+	struct bgx_port_priv *priv = dev_get_drvdata(dev);
+	return priv->mac_addr;
+}
+static void bgx_port_write_cam(int numa_node, int interface, int index, int cam, const u8 *mac)
+{
+	int i;
+	union cvmx_bgxx_cmr_rx_adrx_cam adr_cam;
+	u64 m = 0;
+	if (mac)
+		for (i = 0; i < 6; i++)
+			m |= ((u64)mac[i]) << (6 * i);
+	adr_cam.u64 = m;
+	adr_cam.s.en = mac ? 1 : 0;
+	adr_cam.s.id = index >> 3;
+	cvmx_write_csr_node(numa_node, CVMX_BGXX_CMR_RX_ADRX_CAM(index * 8 + cam, interface), adr_cam.u64);
+}
+
+/* Set MAC address for the ned_device that is attached. */
+void bgx_port_set_rx_filtering(struct net_device *netdev, struct device *dev)
+{
+	union cvmx_bgxx_cmrx_rx_adr_ctl adr_ctl;
+	struct bgx_port_priv *priv = dev_get_drvdata(dev);
+	int available_cam_entries, current_cam_entry;
+	struct netdev_hw_addr *ha;
+
+	available_cam_entries = 8;
+	adr_ctl.u64 = 0;
+	adr_ctl.s.bcst_accept = 1; /* Accept all Broadcast*/
+
+	if ((netdev->flags & IFF_PROMISC) || netdev->uc.count > 7) {
+		adr_ctl.s.cam_accept = 0; /* Reject CAM match */
+		available_cam_entries = 0;
+	} else {
+		/* One CAM entry for the primary address, leaves seven
+		 * for the secondary addresses.
+		 */
+		adr_ctl.s.cam_accept = 1; /* Accept CAM match */
+		available_cam_entries = 7 - netdev->uc.count;
+	}
+
+	if (netdev->flags & IFF_MULTICAST) {
+		if ((netdev->flags & IFF_ALLMULTI) || netdev_mc_count(netdev) > available_cam_entries)
+			adr_ctl.s.mcst_mode = 1; /* Accept all Multicast */
+		else
+			adr_ctl.s.mcst_mode = 2; /* Accept all Multicast via CAM */
+	}
+	current_cam_entry = 0;
+	if (adr_ctl.s.cam_accept) {
+		bgx_port_write_cam(priv->numa_node, priv->bgx_interface, priv->index,
+				   current_cam_entry, netdev->dev_addr);
+		current_cam_entry++;
+		netdev_for_each_uc_addr(ha, netdev) {
+			bgx_port_write_cam(priv->numa_node, priv->bgx_interface, priv->index,
+					   current_cam_entry, ha->addr);
+			current_cam_entry++;
+		}
+	}
+	if (adr_ctl.s.mcst_mode == 2) {/* Accept all Multicast via CAM */
+		netdev_for_each_mc_addr(ha, netdev) {
+			bgx_port_write_cam(priv->numa_node, priv->bgx_interface, priv->index,
+					   current_cam_entry, ha->addr);
+			current_cam_entry++;
+		}
+	}
+	while (current_cam_entry < 8) {
+		bgx_port_write_cam(priv->numa_node, priv->bgx_interface, priv->index,
+				   current_cam_entry, NULL);
+		current_cam_entry++;
+	}
+	cvmx_write_csr_node(priv->numa_node,
+			    CVMX_BGXX_CMRX_RX_ADR_CTL(priv->index, priv->bgx_interface),
+			    adr_ctl.u64);
+}
+
+int bgx_port_enable(struct device *dev)
+{
+	return 0;
+}
+
+int bgx_port_disable(struct device *dev)
+{
+	return 0;
+}
+
+static int bgx_port_probe(struct platform_device *pdev)
+{
+	u64 addr;
+	const u8 *mac;
+	const __be32 *reg;
+	u32 index;
+	int r;
+	struct bgx_port_priv *priv;
+	int numa_node;
+
+	reg = of_get_property(pdev->dev.parent->of_node, "reg", NULL);
+	addr = of_translate_address(pdev->dev.parent->of_node, reg);
+	mac = of_get_mac_address(pdev->dev.parent->of_node);
+
+	numa_node = (addr >> 36) & 0x7;
+
+	r = of_property_read_u32(pdev->dev.of_node, "reg", &index);
+	if (r)
+		return -ENODEV;
+	priv = kzalloc_node(sizeof(*priv), GFP_KERNEL, numa_node);
+	if (!priv)
+		return -ENOMEM;
+
+	priv->numa_node = numa_node;
+	priv->bgx_interface = (addr >> 24) & 0xf;
+	priv->index = index;
+	if (mac)
+		priv->mac_addr = mac;
+
+	r = dev_set_drvdata(&pdev->dev, priv);
+	if (r)
+		goto err;
+
+	dev_info(&pdev->dev, "Probed\n");
+	return 0;
+err:
+	kfree(priv);
+	return r;
+}
+
+static int bgx_port_remove(struct platform_device *pdev)
+{
+	struct bgx_port_priv *priv = dev_get_drvdata(&pdev->dev);
+	kfree(priv);
+	return 0;
+}
+
+static void bgx_port_shutdown(struct platform_device *pdev)
+{
+	return;
+}
+
+static struct of_device_id bgx_port_match[] = {
+	{
+		.compatible = "cavium,octeon-7880-bgx-port",
+	},
+	{},
+};
+MODULE_DEVICE_TABLE(of, bgx_port_match);
+
+static struct platform_driver bgx_port_driver = {
+	.probe		= bgx_port_probe,
+	.remove		= bgx_port_remove,
+	.shutdown       = bgx_port_shutdown,
+	.driver		= {
+		.owner	= THIS_MODULE,
+		.name	= KBUILD_MODNAME "_port",
+		.of_match_table = bgx_port_match,
+	},
+};
+module_platform_driver(bgx_port_driver);
+
+static int bgx_probe(struct platform_device *pdev)
+{
+	struct bgx_platform_data platform_data;
+	const __be32 *reg;
+	u32 port;
+	u64 addr;
+	struct device_node *child;
+	struct platform_device *new_dev;
+	struct platform_device *pki_dev;
+	static int pki_id;
+	int numa_node, interface;
+	int i;
+	int r = 0;
+	char id[64];
+
+	reg = of_get_property(pdev->dev.of_node, "reg", NULL);
+	addr = of_translate_address(pdev->dev.of_node, reg);
+	interface = (addr >> 24) & 0xf;
+	numa_node = (addr >> 36) & 0x7;
+	for_each_available_child_of_node(pdev->dev.of_node, child) {
+		if (!of_device_is_compatible(child, "cavium,octeon-7880-bgx-port"))
+			continue;
+		r = of_property_read_u32(child, "reg", &port);
+		if (r)
+			return -ENODEV;
+		snprintf(id, sizeof(id), "%llx.%u.ethernet-mac", (unsigned long long)addr, port);
+		new_dev = of_platform_device_create(child, id, &pdev->dev);
+		if (!new_dev) {
+			dev_err(&pdev->dev, "Error creating %s\n", id);
+			continue;
+		}
+		platform_data.numa_node = numa_node;
+		platform_data.interface = interface;
+		platform_data.port = port;
+
+		pki_dev = platform_device_register_data(&new_dev->dev, "ethernet-mac-pki", pki_id++,
+							&platform_data, sizeof(platform_data));
+		dev_info(&pdev->dev, "Created PKI %u: %p\n", pki_dev->id, pki_dev);
+#ifdef CONFIG_NUMA
+		new_dev->dev.numa_node = pdev->dev.numa_node;
+		pki_dev->dev.numa_node = pdev->dev.numa_node;
+#endif
+	}
+	__cvmx_helper_packet_hardware_enable(cvmx_helper_node_interface_to_xiface(numa_node, interface));
+	/* Assign 8 CAM entries per LMAC */
+	for (i = 0; i < 32; i++) {
+		union cvmx_bgxx_cmr_rx_adrx_cam adr_cam;
+		adr_cam.u64 = 0;
+		adr_cam.s.id = i >> 3;
+		cvmx_write_csr_node(numa_node, CVMX_BGXX_CMR_RX_ADRX_CAM(i, interface), adr_cam.u64);
+	}
+
+	dev_info(&pdev->dev, "Probed\n");
+	return 0;
+}
+
+static int bgx_remove(struct platform_device *pdev)
+{
+	return 0;
+}
+
+static void bgx_shutdown(struct platform_device *pdev)
+{
+	return;
+}
+
+static struct of_device_id bgx_match[] = {
+	{
+		.compatible = "cavium,octeon-7880-bgx",
+	},
+	{},
+};
+MODULE_DEVICE_TABLE(of, bgx_match);
+
+static struct platform_driver bgx_driver = {
+	.probe		= bgx_probe,
+	.remove		= bgx_remove,
+	.shutdown       = bgx_shutdown,
+	.driver		= {
+		.owner	= THIS_MODULE,
+		.name	= KBUILD_MODNAME,
+		.of_match_table = bgx_match,
+	},
+};
+
+static int __init bgx_driver_init(void)
+{
+	int r;
+	__cvmx_helper_init_port_config_data();
+	r =  platform_driver_register(&bgx_driver);
+	return r;
+}
+module_init(bgx_driver_init);
+
+static void __exit bgx_driver_exit(void)
+{
+	platform_driver_unregister(&bgx_driver);
+}
+module_exit(bgx_driver_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Cavium Networks <support@caviumnetworks.com>");
+MODULE_DESCRIPTION("Cavium Networks BGX Ethernet MAC driver.");
diff --git a/drivers/net/ethernet/octeon/octeon-bgx.h b/drivers/net/ethernet/octeon/octeon-bgx.h
new file mode 100644
index 0000000..be326a2
--- /dev/null
+++ b/drivers/net/ethernet/octeon/octeon-bgx.h
@@ -0,0 +1,45 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2014 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+#ifndef _OCTEON_BGX_H_
+#define _OCTEON_BGX_H_
+
+struct device;
+struct net_device;
+
+int bgx_port_enable(struct device *dev);
+int bgx_port_disable(struct device *dev);
+const u8 *bgx_port_get_mac(struct device *dev);
+void bgx_port_set_rx_filtering(struct net_device *netdev, struct device *dev);
+
+
+struct bgx_platform_data {
+	int numa_node;
+	int interface;
+	int port;
+};
+
+#endif /* _OCTEON_BGX_H_ */
diff --git a/drivers/net/ethernet/octeon/octeon3-ethernet.c b/drivers/net/ethernet/octeon/octeon3-ethernet.c
new file mode 100644
index 0000000..93bf70f
--- /dev/null
+++ b/drivers/net/ethernet/octeon/octeon3-ethernet.c
@@ -0,0 +1,820 @@
+/**********************************************************************
+ * Author: Cavium, Inc.
+ *
+ * Contact: support@cavium.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2014 Cavium, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium, Inc. for more information
+ **********************************************************************/
+#include <linux/init.h>
+#include <linux/atomic.h>
+#include <linux/kthread.h>
+#include <linux/interrupt.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/platform_device.h>
+
+#include <asm/octeon/octeon.h>
+#include <asm/octeon/cvmx-helper-cfg.h>
+#include <asm/octeon/cvmx-helper-pko3.h>
+#include <asm/octeon/cvmx-helper-pki.h>
+#include <asm/octeon/cvmx-pow.h>
+#include <asm/octeon/cvmx-pko3.h>
+#include <asm/octeon/cvmx-fpa3.h>
+
+#include <asm/octeon/cvmx-fpa-defs.h>
+#include <asm/octeon/cvmx-sso-defs.h>
+
+#include "octeon-bgx.h"
+
+#define MAX_TX_QUEUE_DEPTH 512
+#define SSO_INTSN_EXE 0x61
+#define OCTEON3_ETH_MAX_NUMA_NODES 2
+
+struct octeon3_ethernet {
+	struct device *bgx_dev;
+	struct net_device *netdev;
+	struct napi_struct napi;
+	int pki_channel;
+	int pki_laura;
+	int pko_queue;
+	int numa_node;
+	int tx_complete_gaura;
+	int rx_grp;
+	int rx_irq;
+	int tx_complete_grp;
+	atomic64_t tx_backlog;
+};
+
+static DEFINE_MUTEX(octeon3_eth_init_mutex);
+
+struct octeon3_ethernet_node {
+	bool init_done;
+	int numa_node;
+	int pki_packet_pool;
+	int sso_pko_pool;
+	int tx_complete_pool;
+	int sso_aura;
+	int pko_aura;
+	int tx_complete_grp;
+	int tx_irq;
+	struct task_struct *tx_complete_task;
+	struct kthread_worker tx_complete_worker;
+	struct kthread_work tx_complete_work;
+};
+
+struct octeon3_ethernet_node octeon3_eth_node[OCTEON3_ETH_MAX_NUMA_NODES];
+static struct kmem_cache *octeon3_eth_sso_pko_cache;
+static struct kmem_cache *octeon3_eth_tx_complete_cache;
+
+static int octeon3_eth_fpa_pool_init(unsigned int node, unsigned int pool, int num_ptrs)
+{
+	void *pool_stack;
+	u64 pool_stack_start, pool_stack_end;
+	union cvmx_fpa_poolx_end_addr limit_addr;
+	union cvmx_fpa_poolx_cfg cfg;
+	int stack_size = (DIV_ROUND_UP(num_ptrs, 29) + 1) * 128;
+
+	cvmx_write_csr_node(node, CVMX_FPA_POOLX_CFG(pool), 0);
+	cvmx_write_csr_node(node, CVMX_FPA_POOLX_START_ADDR(pool), 128);
+	limit_addr.u64 = 0;
+	limit_addr.cn78xx.addr = ~0ll;
+	cvmx_write_csr_node(node, CVMX_FPA_POOLX_END_ADDR(pool), limit_addr.u64);
+
+	pool_stack = kmalloc_node(stack_size, GFP_KERNEL, node);
+	if (!pool_stack)
+		return -ENOMEM;
+
+	pool_stack_start = virt_to_phys(pool_stack);
+	pool_stack_end = round_down(pool_stack_start + stack_size, 128);
+	pool_stack_start = round_up(pool_stack_start, 128);
+
+	cvmx_write_csr_node(node, CVMX_FPA_POOLX_STACK_BASE(pool), pool_stack_start);
+	cvmx_write_csr_node(node, CVMX_FPA_POOLX_STACK_ADDR(pool), pool_stack_start);
+	cvmx_write_csr_node(node, CVMX_FPA_POOLX_STACK_END(pool), pool_stack_end);
+
+	cfg.u64 = 0;
+	cfg.s.l_type = 2; /* Load with DWB */
+	cfg.s.ena = 1;
+	cvmx_write_csr_node(node, CVMX_FPA_POOLX_CFG(pool), cfg.u64);
+	return 0;
+}
+
+static int octeon3_eth_fpa_aura_init(unsigned int node, unsigned int pool, int aura, int limit)
+{
+	cvmx_write_csr_node(node, CVMX_FPA_AURAX_CFG(aura), 0);
+	cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT_LIMIT(aura), limit * 2);
+	cvmx_write_csr_node(node, CVMX_FPA_AURAX_CNT(aura), limit * 2);
+	cvmx_write_csr_node(node, CVMX_FPA_AURAX_POOL(aura), pool);
+	return 0;
+}
+
+static int octeon3_eth_sso_init(unsigned int node, int aura)
+{
+	union cvmx_sso_aw_cfg cfg;
+	union cvmx_sso_xaq_aura xaq_aura;
+	union cvmx_sso_err0 err0;
+	int i;
+	int rv = 0;
+
+	cfg.u64 = 0;
+	cfg.s.stt = 1;
+	cfg.s.ldt = 1;
+	cfg.s.ldwb = 1;
+	cvmx_write_csr_node(node, CVMX_SSO_AW_CFG, cfg.u64);
+
+	xaq_aura.u64 = 0;
+	xaq_aura.s.laura = aura;
+	xaq_aura.s.node = node;
+	cvmx_write_csr_node(node, CVMX_SSO_XAQ_AURA, xaq_aura.u64);
+
+	for (i = 0; i < 256; i++) {
+		u64 phys;
+		void *mem = cvmx_fpa3_alloc_aura(node, aura);
+		if (!mem) {
+			rv = -ENOMEM;
+			goto err;
+		}
+		phys = virt_to_phys(mem);
+		cvmx_write_csr_node(node, CVMX_SSO_XAQX_HEAD_PTR(i), phys);
+		cvmx_write_csr_node(node, CVMX_SSO_XAQX_TAIL_PTR(i), phys);
+	}
+	err0.u64 = 0;
+	err0.s.fpe = 1;
+	cvmx_write_csr_node(node, CVMX_SSO_ERR0, err0.u64);
+
+	cfg.s.rwen = 1;
+	cvmx_write_csr_node(node, CVMX_SSO_AW_CFG, cfg.u64);
+
+err:
+	return rv;
+}
+
+static void octeon3_eth_sso_irq_set_armed(int node, int grp, bool v)
+{
+	union cvmx_sso_grpx_int_thr grp_int_thr;
+	union cvmx_sso_grpx_int grp_int;
+
+	/* Disarm/Rearm the irq. */
+	grp_int_thr.u64 = 0;
+	grp_int_thr.s.iaq_thr = v ? 1 : 0;
+	cvmx_write_csr_node(node, CVMX_SSO_GRPX_INT_THR(grp), grp_int_thr.u64);
+
+	grp_int.u64 = 0;
+	grp_int.s.exe_int = 1;
+	cvmx_write_csr_node(node, CVMX_SSO_GRPX_INT(grp), grp_int.u64);
+}
+
+static void octeon3_eth_tx_complete_worker(struct kthread_work *txcw)
+{
+	struct octeon3_ethernet_node *oen = container_of(txcw, struct octeon3_ethernet_node, tx_complete_work);
+
+	preempt_disable();
+	for (;;) {
+		void **work;
+		struct sk_buff *skb;
+		struct octeon3_ethernet *priv;
+		work = cvmx_sso_work_request_grp_sync_nocheck(oen->tx_complete_grp, CVMX_POW_NO_WAIT);
+		if (work == NULL)
+			break;
+		skb = work[2];
+		priv = work[0];
+		dev_kfree_skb(skb);
+		cvmx_fpa3_free_gaura(work, priv->tx_complete_gaura, 1);
+	}
+	preempt_enable();
+	octeon3_eth_sso_irq_set_armed(oen->numa_node, oen->tx_complete_grp, true);
+}
+
+static irqreturn_t octeon3_eth_tx_handler(int irq, void *info)
+{
+	struct octeon3_ethernet_node *oen = info;
+	/* Disarm the irq. */
+	octeon3_eth_sso_irq_set_armed(oen->numa_node, oen->tx_complete_grp, false);
+
+	queue_kthread_work(&oen->tx_complete_worker, &oen->tx_complete_work);
+
+	return IRQ_HANDLED;
+}
+
+static int octeon3_eth_global_init(unsigned int node)
+{
+	int i;
+	int rv = 0;
+	struct irq_domain *d;
+	unsigned int sso_intsn;
+	struct octeon3_ethernet_node *nd;
+	mutex_lock(&octeon3_eth_init_mutex);
+
+	nd = octeon3_eth_node + node;
+
+	if (nd->init_done)
+		goto done;
+
+	nd->numa_node = node;
+	nd->sso_pko_pool = cvmx_fpa_alloc_pool(-1);
+	if (nd->sso_pko_pool < 0) {
+		rv = -ENODEV;
+		goto done;
+	}
+	nd->pki_packet_pool = cvmx_fpa_alloc_pool(-1);
+	if (nd->pki_packet_pool < 0) {
+		rv = -ENODEV;
+		goto done;
+	}
+	nd->tx_complete_pool = cvmx_fpa_alloc_pool(-1);
+	if (nd->tx_complete_pool < 0) {
+		rv = -ENODEV;
+		goto done;
+	}
+	nd->sso_aura = cvmx_fpa3_allocate_aura(node);
+	nd->pko_aura = cvmx_fpa3_allocate_aura(node);
+
+	pr_err("octeon3_eth_global_init  Pool:%d, SSO:%d, PKO:%d\n",
+	       nd->sso_pko_pool, nd->sso_aura, nd->pko_aura);
+
+	octeon3_eth_fpa_pool_init(node, nd->sso_pko_pool, 40960);
+	octeon3_eth_fpa_pool_init(node, nd->pki_packet_pool, 40960);
+	octeon3_eth_fpa_pool_init(node, nd->tx_complete_pool, 40960);
+	octeon3_eth_fpa_aura_init(node, nd->sso_pko_pool, nd->sso_aura, 20480);
+	octeon3_eth_fpa_aura_init(node, nd->sso_pko_pool, nd->pko_aura, 20480);
+
+	if (!octeon3_eth_tx_complete_cache) {
+		octeon3_eth_tx_complete_cache = kmem_cache_create("tx_complete", 128, 128, 0, NULL);
+		if (!octeon3_eth_tx_complete_cache) {
+			rv = -ENOMEM;
+			goto done;
+		}
+	}
+	if (!octeon3_eth_sso_pko_cache) {
+		octeon3_eth_sso_pko_cache = kmem_cache_create("sso_pko", 4096, 128, 0, NULL);
+		if (!octeon3_eth_sso_pko_cache) {
+			rv = -ENOMEM;
+			goto done;
+		}
+	}
+	for (i = 0; i < 1024; i++) {
+		void *mem;
+		mem = kmem_cache_alloc_node(octeon3_eth_sso_pko_cache, GFP_KERNEL, node);
+		if (!mem) {
+			rv = -ENOMEM;
+			goto done;
+		}
+		cvmx_fpa3_free_aura(mem, node, nd->sso_aura, 0);
+		mem = kmem_cache_alloc_node(octeon3_eth_sso_pko_cache, GFP_KERNEL, node);
+		if (!mem) {
+			rv = -ENOMEM;
+			goto done;
+		}
+		cvmx_fpa3_free_aura(mem, node, nd->pko_aura, 0);
+	}
+
+	rv = octeon3_eth_sso_init(node, nd->sso_aura);
+	if (rv)
+		goto done;
+
+	nd->tx_complete_grp = cvmx_sso_allocate_group(node);
+	d = octeon_irq_get_block_domain(nd->numa_node, SSO_INTSN_EXE);
+	sso_intsn = SSO_INTSN_EXE << 12 | nd->tx_complete_grp;
+	nd->tx_irq = irq_create_mapping(d, sso_intsn);
+	if (!nd->tx_irq) {
+		rv = -ENODEV;
+		goto done;
+	}
+
+	__cvmx_helper_init_port_config_data();
+	rv = __cvmx_helper_pko3_init_global(node, cvmx_fpa3_arua_to_guara(node, nd->pko_aura));
+	if (rv) {
+		pr_err("cvmx_helper_pko3_init_global failed\n");
+		rv = -ENODEV;
+		goto done;
+	}
+	__cvmx_helper_pki_install_default_vlan(node);
+	cvmx_pki_setup_clusters(node);
+	cvmx_pki_enable_backpressure(node);
+	cvmx_pki_parse_enable(node, 0);
+	cvmx_pki_enable(node);
+
+	init_kthread_worker(&nd->tx_complete_worker);
+	init_kthread_work(&nd->tx_complete_work, octeon3_eth_tx_complete_worker);
+	nd->tx_complete_task = kthread_create_on_node(kthread_worker_fn, &nd->tx_complete_worker, node,
+						      "oct3_eth_tx_done[%d]", node);
+	if (IS_ERR(nd->tx_complete_task)) {
+		rv = PTR_ERR(nd->tx_complete_task);
+		goto done;
+	} else
+		wake_up_process(nd->tx_complete_task);
+
+
+	rv = request_irq(nd->tx_irq, octeon3_eth_tx_handler, 0, "oct3_eth_tx_done", nd);
+	if (rv)
+		goto done;
+
+	octeon3_eth_sso_irq_set_armed(node, nd->tx_complete_grp, true);
+
+	nd->init_done = true;
+done:
+	mutex_unlock(&octeon3_eth_init_mutex);
+	return rv;
+}
+
+static struct sk_buff *octeon3_eth_work_to_skb(void *w)
+{
+	struct sk_buff *skb;
+	void **f = w;
+	skb = f[-16];
+	return skb;
+}
+
+/* Receive one packet.
+ * returns the number of RX buffers consumed.
+ */
+static int octeon3_eth_rx_one(struct octeon3_ethernet *priv)
+{
+	unsigned int segments;
+	int ret;
+	unsigned int packet_len;
+	cvmx_wqe_78xx_t *work;
+	u8 *data;
+	struct sk_buff *skb;
+	union cvmx_buf_ptr_pki packet_ptr;
+
+	work = cvmx_sso_work_request_grp_sync_nocheck(priv->rx_grp, CVMX_POW_NO_WAIT);
+	prefetch(work);
+	if (!work)
+		return 0;
+	skb = octeon3_eth_work_to_skb(work);
+	segments = work->word0.bufs;
+	ret = segments;
+	packet_len = work->word1.len;
+	packet_ptr = work->packet_ptr;
+	data = phys_to_virt(packet_ptr.addr);
+	skb->data = data;
+	skb->len = packet_len;
+	if (segments == 1) {
+		skb_set_tail_pointer(skb, skb->len);
+	} else {
+		bool first_frag = true;
+		struct sk_buff *current_skb = skb;
+		struct sk_buff *next_skb = NULL;
+		unsigned int segment_size;
+
+		skb_frag_list_init(skb);
+		for (;;) {
+			segment_size = packet_ptr.size;
+			if (!first_frag) {
+				current_skb->len = segment_size;
+				skb->data_len += segment_size;
+				skb->truesize += current_skb->truesize;
+			}
+			skb_set_tail_pointer(current_skb, segment_size);
+			segments--;
+			if (segments == 0)
+				break;
+			packet_ptr = *(union cvmx_buf_ptr_pki *)(data - 8);
+			data = phys_to_virt(packet_ptr.addr);
+			next_skb = octeon3_eth_work_to_skb((void *)round_down((unsigned long)data, 128ul));
+			if (first_frag) {
+				skb_frag_add_head(current_skb, next_skb);
+			} else {
+				current_skb->next = next_skb;
+				next_skb->next = NULL;
+			}
+			current_skb = next_skb;
+			first_frag = false;
+			current_skb->data = phys_to_virt(packet_ptr.addr);
+		}
+	}
+	if (likely(priv->netdev->flags & IFF_UP)) {
+		skb->protocol = eth_type_trans(skb, priv->netdev);
+		skb->dev = priv->netdev;
+		skb->ip_summed = CHECKSUM_NONE;
+		netif_receive_skb(skb);
+	} else {
+		/* Drop any packet received for a device that isn't up */
+		atomic64_add(1, (atomic64_t *)&priv->netdev->stats.rx_dropped);
+		dev_kfree_skb_any(skb);
+	}
+	return ret;
+}
+
+static int octeon3_eth_napi(struct napi_struct *napi, int budget)
+{
+	int rx_count = 0;
+	struct octeon3_ethernet *priv = container_of(napi, struct octeon3_ethernet, napi);
+
+	while (rx_count < budget) {
+		int n = octeon3_eth_rx_one(priv);
+		if (n == 0) {
+			napi_complete(napi);
+			octeon3_eth_sso_irq_set_armed(priv->numa_node, priv->rx_grp, true);
+			break;
+		}
+		rx_count++;
+	}
+
+	return rx_count;
+}
+
+static int octeon3_eth_ndo_init(struct net_device *netdev)
+{
+	struct octeon3_ethernet *priv = netdev_priv(netdev);
+	const u8 *mac;
+
+	netif_carrier_off(netdev);
+
+	netdev->features |= NETIF_F_SG | NETIF_F_FRAGLIST;
+
+	netif_napi_add(netdev, &priv->napi, octeon3_eth_napi, 32);
+	napi_enable(&priv->napi);
+
+	mac = bgx_port_get_mac(priv->bgx_dev);
+	if (mac && is_valid_ether_addr(mac)) {
+		memcpy(netdev->dev_addr, mac, ETH_ALEN);
+		netdev->addr_assign_type &= ~NET_ADDR_RANDOM;
+	} else {
+		eth_hw_addr_random(netdev);
+	}
+	bgx_port_set_rx_filtering(netdev, priv->bgx_dev);
+
+	netdev_info(netdev, "octeon3_eth_ndo_init\n");
+	return 0;
+}
+
+static void octeon3_eth_ndo_uninit(struct net_device *netdev)
+{
+	return;
+}
+
+static irqreturn_t octeon3_eth_rx_handler(int irq, void *info)
+{
+	struct net_device *netdev = info;
+	struct octeon3_ethernet *priv = netdev_priv(netdev);
+
+	/* Disarm the irq. */
+	octeon3_eth_sso_irq_set_armed(priv->numa_node, priv->rx_grp, false);
+
+	napi_schedule(&priv->napi);
+	return IRQ_HANDLED;
+}
+
+static int octeon3_eth_ndo_open(struct net_device *netdev)
+{
+	struct octeon3_ethernet *priv = netdev_priv(netdev);
+	struct irq_domain *d = octeon_irq_get_block_domain(priv->numa_node, SSO_INTSN_EXE);
+	unsigned int sso_intsn = SSO_INTSN_EXE << 12 | priv->rx_grp;
+	int i;
+	int r;
+	struct sk_buff *skb;
+
+	priv->rx_irq = irq_create_mapping(d, sso_intsn);
+	if (!priv->rx_irq) {
+		netdev_err(netdev, "ERROR: Couldn't map hwirq: %x\n", sso_intsn);
+		return -EINVAL;
+	}
+
+	/* Arm the irq. */
+	octeon3_eth_sso_irq_set_armed(priv->numa_node, priv->rx_grp, true);
+
+	r = request_irq(priv->rx_irq, octeon3_eth_rx_handler, 0, netdev_name(netdev), netdev);
+	if (r)
+		goto err;
+
+	for (i = 0; i < 100; i++) {
+		void **buf;
+		skb = __alloc_skb(2048 + 128, GFP_KERNEL, 0, priv->numa_node);
+		if (!skb) {
+			r = -ENOMEM;
+			goto err;
+		}
+		buf = (void **)PTR_ALIGN(skb->head, 128);
+		buf[0] = skb;
+		cvmx_fpa3_free_aura(buf, priv->numa_node, priv->pki_laura, 0);
+		pr_err("%p -> %p\n", skb->head, buf);
+	}
+	netdev_err(netdev, "Populated aura:%d\n", priv->pki_laura);
+
+	netif_carrier_on(netdev);
+	return 0;
+err:
+	/* Cleanup mapping ?? */
+	return r;
+}
+
+static int octeon3_eth_ndo_start_xmit(struct sk_buff *skb, struct net_device *netdev)
+{
+	struct sk_buff *skb_tmp;
+	struct octeon3_ethernet *priv = netdev_priv(netdev);
+	unsigned int scr_off = CVMX_PKO_LMTLINE * CVMX_CACHE_LINE_SIZE;
+	unsigned int ret_off = scr_off;
+	union cvmx_pko_send_hdr send_hdr;
+	union cvmx_pko_buf_ptr buf_ptr;
+	union cvmx_pko_send_work send_work;
+	union cvmx_pko_send_mem send_mem;
+	union cvmx_pko_lmtdma_data lmtdma_data;
+	union cvmx_pko_query_rtn query_rtn;
+	long backlog;
+	int frag_count;
+	int head_len, i;
+	u64 dma_addr;
+	void **work;
+
+	frag_count = 0;
+	if (skb_has_frag_list(skb))
+		skb_walk_frags(skb, skb_tmp)
+			frag_count++;
+	/* We have space for 13 segment pointers, If there will be
+	 * more than that, we must linearize.  The count is: 1 (base
+	 * SKB) + frag_count + nr_frags.
+	 */
+	if (unlikely(skb_shinfo(skb)->nr_frags + frag_count > 12)) {
+		if (unlikely(__skb_linearize(skb)))
+			goto skip_xmit;
+		frag_count = 0;
+	}
+	work = cvmx_fpa3_alloc_gaura(priv->tx_complete_gaura);
+	if (!work)
+		goto skip_xmit;
+	work[0] = priv;
+	work[1] = NULL;
+	work[2] = skb;
+
+	backlog = atomic64_inc_return(&priv->tx_backlog);
+	if (backlog > MAX_TX_QUEUE_DEPTH)
+		netif_stop_queue(netdev);
+	wmb();
+
+	send_hdr.u64 = 0;
+#ifdef __LITTLE_ENDIAN
+	send_hdr.s.le = 1;
+#endif
+// broken in sim	send_hdr.s.n2 = 1; /* Don't allocate to L2 */
+	send_hdr.s.df = 1; /* Don't automatically free to FPA */
+	send_hdr.s.total = skb->len;
+
+	cvmx_scratch_write64(scr_off, send_hdr.u64);
+	scr_off += sizeof(send_hdr);
+
+	buf_ptr.u64 = 0;
+	buf_ptr.s.subdc3 = CVMX_PKO_SENDSUBDC_GATHER;
+
+	/* Add a Gather entry for each segment. */
+	head_len = skb_headlen(skb);
+	if (head_len > 0) {
+		buf_ptr.s.size = head_len;
+		buf_ptr.s.addr = virt_to_phys(skb->data);
+		cvmx_scratch_write64(scr_off, buf_ptr.u64);
+		scr_off += sizeof(buf_ptr);
+	}
+	for (i = 1; i <= skb_shinfo(skb)->nr_frags; i++) {
+		struct skb_frag_struct *fs = skb_shinfo(skb)->frags + i - 1;
+		buf_ptr.s.size = fs->size;
+		buf_ptr.s.addr = virt_to_phys((u8 *)page_address(fs->page.p) + fs->page_offset);
+		cvmx_scratch_write64(scr_off, buf_ptr.u64);
+		scr_off += sizeof(buf_ptr);
+	}
+	skb_walk_frags(skb, skb_tmp) {
+		buf_ptr.s.addr = virt_to_phys(skb_tmp->data);
+		buf_ptr.s.size = skb_tmp->len;
+		cvmx_scratch_write64(scr_off, buf_ptr.u64);
+		scr_off += sizeof(buf_ptr);
+	}
+
+	/* Subtract 1 from the tx_backlog. */
+	send_mem.u64 = 0;
+	send_mem.s.subdc4 = CVMX_PKO_SENDSUBDC_MEM;
+	send_mem.s.dsz = MEMDSZ_B64;
+	send_mem.s.alg = MEMALG_SUB;
+	send_mem.s.offset = 1;
+	send_mem.s.addr = virt_to_phys(&priv->tx_backlog);
+	cvmx_scratch_write64(scr_off, send_mem.u64);
+	scr_off += sizeof(buf_ptr);
+
+	/* Send work when finished with the packet. */
+	send_work.u64 = 0;
+	send_work.s.subdc4 = CVMX_PKO_SENDSUBDC_WORK;
+	send_work.s.addr = virt_to_phys(work);
+	send_work.s.tt = CVMX_POW_TAG_TYPE_NULL;
+	send_work.s.grp = priv->tx_complete_grp;
+	cvmx_scratch_write64(scr_off, send_work.u64);
+	scr_off += sizeof(buf_ptr);
+
+	lmtdma_data.u64 = 0;
+	lmtdma_data.s.scraddr = ret_off >> 3;
+	lmtdma_data.s.rtnlen = 1;
+	lmtdma_data.s.did = 0x51;
+	lmtdma_data.s.node = priv->numa_node;
+	lmtdma_data.s.dq = priv->pko_queue;
+	dma_addr = 0xffffffffffffa400ull | ((scr_off & 0x78) - 8);
+	cvmx_write64_uint64(dma_addr, lmtdma_data.u64);
+
+	CVMX_SYNCIOBDMA;
+
+	query_rtn.u64 = cvmx_scratch_read64(ret_off);
+	if (unlikely(query_rtn.s.dqstatus != PKO_DQSTATUS_PASS)) {
+		netdev_err(netdev, "PKO enqueue failed %llx\n",
+			   (unsigned long long)query_rtn.u64);
+		dev_kfree_skb_any(skb);
+		cvmx_fpa3_free_gaura(work, priv->tx_complete_gaura, 1);
+	}
+
+	return NETDEV_TX_OK;
+skip_xmit:
+	dev_kfree_skb_any(skb);
+	return NETDEV_TX_OK;
+}
+
+static const struct net_device_ops octeon3_eth_netdev_ops = {
+	.ndo_init		= octeon3_eth_ndo_init,
+	.ndo_uninit		= octeon3_eth_ndo_uninit,
+	.ndo_open		= octeon3_eth_ndo_open,
+	.ndo_start_xmit		= octeon3_eth_ndo_start_xmit,
+};
+
+static int octeon3_eth_probe(struct platform_device *pdev)
+{
+	struct cvmx_pki_port_config pki_prt_cfg;
+	struct cvmx_pki_prt_schd *prt_schd = NULL;
+	struct octeon3_ethernet *priv;
+	struct net_device *netdev;
+	struct octeon3_ethernet_node *nd;
+	int xiface, ipd_port, node_dq;
+	struct cvmx_xport xdq;
+	int laura;
+	int i, r;
+
+	struct bgx_platform_data *pd = dev_get_platdata(&pdev->dev);
+
+	r = octeon3_eth_global_init(pd->numa_node);
+	if (r)
+		return r;
+
+	nd = octeon3_eth_node + pd->numa_node;
+
+	dev_err(&pdev->dev, "Probing %d-%d:%d\n", pd->numa_node, pd->interface, pd->port);
+	xiface = cvmx_helper_node_interface_to_xiface(pd->numa_node, pd->interface);
+	ipd_port = cvmx_helper_get_ipd_port(xiface, pd->port);
+	cvmx_helper_interface_enumerate(xiface);
+	cvmx_helper_interface_probe(xiface);
+	r =  __cvmx_pko3_config_gen_interface(xiface, pd->port, 1, false);
+	if (r)
+		return r;
+
+	r = __cvmx_pko3_helper_dqs_activate(xiface, pd->port);
+	if (r < 0)
+		return r;
+
+	r = cvmx_pko3_interface_options(xiface, pd->port,
+					__cvmx_helper_get_has_fcs(xiface),
+					__cvmx_helper_get_pko_padding(xiface), 0);
+	if (r)
+		return r;
+
+	netdev = alloc_etherdev(sizeof(struct octeon3_ethernet));
+	if (!netdev) {
+		dev_err(&pdev->dev, "Failed to allocated ethernet device\n");
+		return -ENOMEM;
+	}
+	SET_NETDEV_DEV(netdev, &pdev->dev);
+	dev_set_drvdata(&pdev->dev, netdev);
+
+	node_dq = cvmx_pko3_get_queue_base(ipd_port);
+	xdq = cvmx_helper_ipd_port_to_xport(node_dq);
+
+
+	priv = netdev_priv(netdev);
+	priv->bgx_dev = pdev->dev.parent;
+	priv->netdev = netdev;
+	priv->pki_channel = 0;
+	priv->pko_queue = xdq.port;
+	priv->pki_laura = cvmx_fpa3_allocate_aura(pd->numa_node);
+	octeon3_eth_fpa_aura_init(pd->numa_node, nd->pki_packet_pool, priv->pki_laura, 1024);
+	priv->rx_grp = cvmx_sso_allocate_group(pd->numa_node);
+	priv->tx_complete_grp = octeon3_eth_node[pd->numa_node].tx_complete_grp;
+	dev_err(&pdev->dev, "rx sso grp:%d aura:%d pknd:%d\n", priv->rx_grp, priv->pki_laura, cvmx_helper_get_pknd(xiface, pd->port));
+	if (priv->rx_grp < 0) {
+		dev_err(&pdev->dev, "Failed to allocated SSO group\n");
+		return -ENODEV;
+	}
+
+	prt_schd = kzalloc(sizeof(*prt_schd), GFP_KERNEL);
+	if (!prt_schd) {
+		r = -ENOMEM;
+		goto err;
+	}
+	prt_schd->style = -1; /* Allocate net style per port */
+	prt_schd->aura_per_prt = true;
+	prt_schd->aura = priv->pki_laura;
+	prt_schd->sso_grp_per_prt = true;
+	prt_schd->sso_grp = priv->rx_grp;
+	prt_schd->qpg_qos = CVMX_PKI_QPG_QOS_NONE;
+
+	cvmx_helper_pki_init_port(ipd_port, prt_schd);
+	cvmx_pki_get_port_config(ipd_port, &pki_prt_cfg);
+
+	pki_prt_cfg.style_cfg.parm_cfg.ip6_udp_opt = false;
+	pki_prt_cfg.style_cfg.parm_cfg.lenerr_en = true;
+	pki_prt_cfg.style_cfg.parm_cfg.maxerr_en = true;
+	pki_prt_cfg.style_cfg.parm_cfg.minerr_en = true;
+	pki_prt_cfg.style_cfg.parm_cfg.ip6_udp_opt = false;
+	pki_prt_cfg.style_cfg.parm_cfg.ip6_udp_opt = false;
+	pki_prt_cfg.style_cfg.parm_cfg.wqe_skip = 1 * 128;
+	pki_prt_cfg.style_cfg.parm_cfg.first_skip = 8 * 21;
+	pki_prt_cfg.style_cfg.parm_cfg.later_skip = 8 * 16;
+	pki_prt_cfg.style_cfg.parm_cfg.pkt_lend = false;
+	pki_prt_cfg.style_cfg.parm_cfg.tag_type = CVMX_SSO_TAG_TYPE_UNTAGGED;
+	pki_prt_cfg.style_cfg.parm_cfg.qpg_dis_grptag = true;
+	pki_prt_cfg.style_cfg.parm_cfg.dis_wq_dat = false;
+	pki_prt_cfg.style_cfg.parm_cfg.mbuff_size = 2048;
+
+	cvmx_pki_config_port(ipd_port, &pki_prt_cfg);
+
+	netdev->netdev_ops = &octeon3_eth_netdev_ops;
+
+	laura = cvmx_fpa3_allocate_aura(pd->numa_node);
+	if (laura < 0) {
+		r = -ENODEV;
+		goto err;
+	}
+	priv->tx_complete_gaura = cvmx_fpa3_arua_to_guara(pd->numa_node, laura);
+	octeon3_eth_fpa_aura_init(pd->numa_node, nd->tx_complete_pool, laura, 1024);
+	for (i = 0; i < 1024; i++) {
+		void *mem;
+		mem = kmem_cache_alloc_node(octeon3_eth_tx_complete_cache, GFP_KERNEL, pd->numa_node);
+		if (!mem) {
+			r = -ENOMEM;
+			goto err;
+		}
+		cvmx_fpa3_free_aura(mem, pd->numa_node, laura, 0);
+	}
+
+	if (register_netdev(netdev) < 0) {
+		dev_err(&pdev->dev, "Failed to register ethernet device\n");
+		free_netdev(netdev);
+	}
+	r = 0;
+err:
+	kfree(prt_schd);
+	return r;
+}
+
+static int octeon3_eth_remove(struct platform_device *pdev)
+{
+//	struct net_device *netdev = dev_get_drvdata(&pdev->dev);
+//	struct octeon3_ethernet *priv = netdev_priv(netdev);
+	return 0;
+}
+
+static void octeon3_eth_shutdown(struct platform_device *pdev)
+{
+	return;
+}
+
+
+static struct platform_driver octeon3_eth_driver = {
+	.probe		= octeon3_eth_probe,
+	.remove		= octeon3_eth_remove,
+	.shutdown       = octeon3_eth_shutdown,
+	.driver		= {
+		.owner	= THIS_MODULE,
+		.name	= "ethernet-mac-pki",
+	},
+};
+
+
+
+static int __init octeon3_eth_init(void)
+{
+	if (!OCTEON_IS_MODEL(OCTEON_CN78XX))
+		return 0;
+
+	return platform_driver_register(&octeon3_eth_driver);
+}
+module_init(octeon3_eth_init);
+
+static void __exit octeon3_eth_exit(void)
+{
+	if (!OCTEON_IS_MODEL(OCTEON_CN78XX))
+		return;
+
+	platform_driver_unregister(&octeon3_eth_driver);
+}
+module_exit(octeon3_eth_exit);
-- 
1.8.2.1

