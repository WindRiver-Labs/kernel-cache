From 18b9408eada2ea660e4721228febb17688b27b27 Mon Sep 17 00:00:00 2001
From: Carlos Munoz <carlos.munoz@caviumnetworks.com>
Date: Fri, 13 Feb 2015 14:04:49 +0530
Subject: [PATCH 043/132] MIPS/OCTEON: Add msi/msi-x support for ciu3.

Commit ed8c5e4ea1718cae7c1b4fb889686575be9acda0 from
git://git.yoctoproject.org/linux-yocto-3.14

Signed-off-by: Carlos Munoz <carlos.munoz@caviumnetworks.com>
Signed-off-by: Abhishek Paliwal <abhishek.paliwal@aricent.com>
[Original patch taken from OCTEON-SDK 3.1.1-544.]
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 arch/mips/pci/msi-octeon.c | 840 ++++++++++++++++++++++++++++++++-------------
 1 file changed, 594 insertions(+), 246 deletions(-)

diff --git a/arch/mips/pci/msi-octeon.c b/arch/mips/pci/msi-octeon.c
index cffaaf4..bb92ec2 100644
--- a/arch/mips/pci/msi-octeon.c
+++ b/arch/mips/pci/msi-octeon.c
@@ -3,303 +3,277 @@
  * License.  See the file "COPYING" in the main directory of this archive
  * for more details.
  *
- * Copyright (C) 2005-2009, 2010 Cavium Networks
+ * Copyright (C) 2005-2012 Cavium Inc.
  */
+#include <linux/interrupt.h>
+#include <linux/spinlock.h>
 #include <linux/kernel.h>
 #include <linux/init.h>
+#include <linux/cpu.h>
 #include <linux/msi.h>
-#include <linux/spinlock.h>
-#include <linux/interrupt.h>
 
 #include <asm/octeon/octeon.h>
 #include <asm/octeon/cvmx-npi-defs.h>
 #include <asm/octeon/cvmx-pci-defs.h>
 #include <asm/octeon/cvmx-npei-defs.h>
-#include <asm/octeon/cvmx-sli-defs.h>
 #include <asm/octeon/cvmx-pexp-defs.h>
+#include <asm/octeon/cvmx-sli-defs.h>
+#include <asm/octeon/cvmx-ciu2-defs.h>
 #include <asm/octeon/pci-octeon.h>
 
+/* MSI major block number (8 MSBs of intsn) */
+#define MSI_BLOCK_NUMBER	0x1e
+
+#define MSI_IRQ_SIZE		256
+
 /*
- * Each bit in msi_free_irq_bitmask represents a MSI interrupt that is
- * in use.
+ * Data to save in the chip_data field of the irq description.
  */
-static u64 msi_free_irq_bitmask[4];
+struct msi_chip_data {
+	int msi;
+	int hwmsi;
+};
 
 /*
- * Each bit in msi_multiple_irq_bitmask tells that the device using
- * this bit in msi_free_irq_bitmask is also using the next bit. This
- * is used so we can disable all of the MSI interrupts when a device
- * uses multiple.
+ * Each bit in msi_free_irq_bitmap represents a MSI interrupt that is
+ * in use. Each node requires its own set of bits.
  */
-static u64 msi_multiple_irq_bitmask[4];
+static DECLARE_BITMAP(msi_free_irq_bitmap[CVMX_MAX_NODES], MSI_IRQ_SIZE);
 
 /*
- * This lock controls updates to msi_free_irq_bitmask and
- * msi_multiple_irq_bitmask.
+ * This lock controls updates to msi_free_irq_bitmap.
  */
-static DEFINE_SPINLOCK(msi_free_irq_bitmask_lock);
+static DEFINE_SPINLOCK(msi_free_irq_bitmap_lock);
+
+/* MSI to IRQ lookup */
+static int msi_to_irq[MSI_IRQ_SIZE];
 
 /*
- * Number of MSI IRQs used. This variable is set up in
- * the module init time.
+ * Find a contiguous aligned block of free msi interrupts and allocate
+ * them (set them to one).
+ *
+ * @node:      Node to allocate msi interrupts for.
+ * @nvec:      Number of msi interrupts to allocate.
+ *
+ * Returns:    Zero on success, error otherwise.
  */
-static int msi_irq_size;
+static int msi_bitmap_alloc_hwirqs(int node, int nvec)
+{
+	unsigned long	flags;
+	int		offset;
+	int		order = get_count_order(nvec);
 
-/**
- * Called when a driver request MSI interrupts instead of the
- * legacy INT A-D. This routine will allocate multiple interrupts
- * for MSI devices that support them. A device can override this by
- * programming the MSI control bits [6:4] before calling
- * pci_enable_msi().
- *
- * @dev:    Device requesting MSI interrupts
- * @desc:   MSI descriptor
+	spin_lock_irqsave(&msi_free_irq_bitmap_lock, flags);
+	offset = bitmap_find_free_region(msi_free_irq_bitmap[node],
+					 MSI_IRQ_SIZE, order);
+
+	spin_unlock_irqrestore(&msi_free_irq_bitmap_lock, flags);
+
+	if (unlikely(offset < 0)) {
+		WARN(1, "Unable to find a free MSI interrupt");
+		return offset;
+	}
+
+	return offset;
+}
+
+/*
+ * Free a contiguous block of msi interrupts (set them to zero).
  *
- * Returns 0 on success.
+ * @node:      Node to allocate msi interrupts for.
+ * @offset:    Beginnning of msi interrupts to release.
+ * @nvec:      Number of msi interrupts to release.
  */
-int arch_setup_msi_irq(struct pci_dev *dev, struct msi_desc *desc)
+static void msi_bitmap_free_hwirqs(int node, int offset, int nvec)
 {
-	struct msi_msg msg;
-	u16 control;
-	int configured_private_bits;
-	int request_private_bits;
-	int irq = 0;
-	int irq_step;
-	u64 search_mask;
-	int index;
+	unsigned long	flags;
+	int		order = get_count_order(nvec);
 
-	/*
-	 * Read the MSI config to figure out how many IRQs this device
-	 * wants.  Most devices only want 1, which will give
-	 * configured_private_bits and request_private_bits equal 0.
-	 */
-	pci_read_config_word(dev, dev->msi_cap + PCI_MSI_FLAGS, &control);
+	spin_lock_irqsave(&msi_free_irq_bitmap_lock, flags);
+	bitmap_release_region(msi_free_irq_bitmap[node], offset, order);
+	spin_unlock_irqrestore(&msi_free_irq_bitmap_lock, flags);
+}
 
-	/*
-	 * If the number of private bits has been configured then use
-	 * that value instead of the requested number. This gives the
-	 * driver the chance to override the number of interrupts
-	 * before calling pci_enable_msi().
-	 */
-	configured_private_bits = (control & PCI_MSI_FLAGS_QSIZE) >> 4;
-	if (configured_private_bits == 0) {
-		/* Nothing is configured, so use the hardware requested size */
-		request_private_bits = (control & PCI_MSI_FLAGS_QMASK) >> 1;
+/**
+ * Called when a device no longer needs its MSI interrupts. All
+ * MSI interrupts for the device are freed.
+ *
+ * @irq:    The devices first irq number. There may be multple in sequence.
+ */
+void arch_teardown_msi_irq(unsigned int irq)
+{
+	int msi;
+	int node = 0; /* Must use node device is in. TODO */
+
+	if (octeon_has_feature(OCTEON_FEATURE_CIU3)) {
+		struct octeon_ciu_chip_data *cd3 = irq_get_chip_data(irq);
+		msi = cd3->intsn & 0xff;
 	} else {
-		/*
-		 * Use the number of configured bits, assuming the
-		 * driver wanted to override the hardware request
-		 * value.
-		 */
-		request_private_bits = configured_private_bits;
+		struct msi_chip_data *cd = irq_get_chip_data(irq);
+		msi = cd->msi;
+		irq_free_descs(irq, 1);
 	}
 
-	/*
-	 * The PCI 2.3 spec mandates that there are at most 32
-	 * interrupts. If this device asks for more, only give it one.
-	 */
-	if (request_private_bits > 5)
-		request_private_bits = 0;
+	msi_bitmap_free_hwirqs(node, msi, 1);
+}
 
-try_only_one:
-	/*
-	 * The IRQs have to be aligned on a power of two based on the
-	 * number being requested.
-	 */
-	irq_step = 1 << request_private_bits;
+static DEFINE_RAW_SPINLOCK(octeon_irq_msi_lock);
 
-	/* Mask with one bit for each IRQ */
-	search_mask = (1 << irq_step) - 1;
+static u64 msi_rcv_reg[4];
+static u64 msi_ena_reg[4];
 
-	/*
-	 * We're going to search msi_free_irq_bitmask_lock for zero
-	 * bits. This represents an MSI interrupt number that isn't in
-	 * use.
-	 */
-	spin_lock(&msi_free_irq_bitmask_lock);
-	for (index = 0; index < msi_irq_size/64; index++) {
-		for (irq = 0; irq < 64; irq += irq_step) {
-			if ((msi_free_irq_bitmask[index] & (search_mask << irq)) == 0) {
-				msi_free_irq_bitmask[index] |= search_mask << irq;
-				msi_multiple_irq_bitmask[index] |= (search_mask >> 1) << irq;
-				goto msi_irq_allocated;
-			}
-		}
-	}
-msi_irq_allocated:
-	spin_unlock(&msi_free_irq_bitmask_lock);
-
-	/* Make sure the search for available interrupts didn't fail */
-	if (irq >= 64) {
-		if (request_private_bits) {
-			pr_err("arch_setup_msi_irq: Unable to find %d free interrupts, trying just one",
-			       1 << request_private_bits);
-			request_private_bits = 0;
-			goto try_only_one;
-		} else
-			panic("arch_setup_msi_irq: Unable to find a free MSI interrupt");
-	}
+/*
+ * Up to 256 MSIs are supported. MSIs are allocated sequencially from 0 to 255.
+ * The CIU has 4 interrupt lines each supporting 64 MSIs to handle the 256 MSI
+ * interrupts.
+ * Software might desire to map MSIs to different CIU interrupt lines to share
+ * the load. For example, MSI 0 might be mapped to CIU interrupt line 0, MSI 1
+ * to CIU interrupt line 1, and so on.
+ * Hardware MSIs indicate the CIU interrupt line and the bit within the line a
+ * particular MSI is mapped to.
+ * These pointers point to the methods that performs the mapping to use.
+ */
+static int (*octeon_irq_msi_to_hwmsi)(int);
+static int (*octeon_irq_hwmsi_to_msi)(int);
 
-	/* MSI interrupts start at logical IRQ OCTEON_IRQ_MSI_BIT0 */
-	irq += index*64;
-	irq += OCTEON_IRQ_MSI_BIT0;
+/*
+ * MSI to hardware MSI linear mapping. No load sharing. First 64 allocated MSIs
+ * go to CIU interrupt line 0, next 64 to the next CIU line and so on.
+ */
+static int octeon_irq_msi_to_hwmsi_linear(int msi)
+{
+	return msi;
+}
 
-	switch (octeon_dma_bar_type) {
-	case OCTEON_DMA_BAR_TYPE_SMALL:
-		/* When not using big bar, Bar 0 is based at 128MB */
-		msg.address_lo =
-			((128ul << 20) + CVMX_PCI_MSI_RCV) & 0xffffffff;
-		msg.address_hi = ((128ul << 20) + CVMX_PCI_MSI_RCV) >> 32;
-		break;
-	case OCTEON_DMA_BAR_TYPE_BIG:
-		/* When using big bar, Bar 0 is based at 0 */
-		msg.address_lo = (0 + CVMX_PCI_MSI_RCV) & 0xffffffff;
-		msg.address_hi = (0 + CVMX_PCI_MSI_RCV) >> 32;
-		break;
-	case OCTEON_DMA_BAR_TYPE_PCIE:
-		/* When using PCIe, Bar 0 is based at 0 */
-		/* FIXME CVMX_NPEI_MSI_RCV* other than 0? */
-		msg.address_lo = (0 + CVMX_NPEI_PCIE_MSI_RCV) & 0xffffffff;
-		msg.address_hi = (0 + CVMX_NPEI_PCIE_MSI_RCV) >> 32;
-		break;
-	case OCTEON_DMA_BAR_TYPE_PCIE2:
-		/* When using PCIe2, Bar 0 is based at 0 */
-		msg.address_lo = (0 + CVMX_SLI_PCIE_MSI_RCV) & 0xffffffff;
-		msg.address_hi = (0 + CVMX_SLI_PCIE_MSI_RCV) >> 32;
-		break;
-	default:
-		panic("arch_setup_msi_irq: Invalid octeon_dma_bar_type");
-	}
-	msg.data = irq - OCTEON_IRQ_MSI_BIT0;
+static int octeon_irq_hwmsi_to_msi_linear(int hwmsi)
+{
+	return hwmsi;
+}
 
-	/* Update the number of IRQs the device has available to it */
-	control &= ~PCI_MSI_FLAGS_QSIZE;
-	control |= request_private_bits << 4;
-	pci_write_config_word(dev, dev->msi_cap + PCI_MSI_FLAGS, control);
+/*
+ * MSI to hardware MSI scatter mapping. MSI interrupt load is spread among all
+ * CIU interrupt lines. MSI 0 goes to CIU line 0, MSI 1 to CIU line 1 and so on.
+ */
+static int octeon_irq_msi_to_hwmsi_scatter(int msi)
+{
+	return ((msi << 6) & 0xc0) | ((msi >> 2) & 0x3f);
+}
 
-	irq_set_msi_desc(irq, desc);
-	pci_write_msi_msg(irq, &msg);
-	return 0;
+static int octeon_irq_hwmsi_to_msi_scatter(int hwmsi)
+{
+	return (((hwmsi >> 6) & 0x3) | ((hwmsi << 2) & 0xfc));
 }
 
-int arch_setup_msi_irqs(struct pci_dev *dev, int nvec, int type)
+#ifdef CONFIG_SMP
+
+static atomic_t affinity_in_progress[4] = {
+	ATOMIC_INIT(1),
+	ATOMIC_INIT(1),
+	ATOMIC_INIT(1),
+	ATOMIC_INIT(1)};
+
+static int octeon_irq_msi_set_affinity_pcie(struct irq_data *data,
+					    const struct cpumask *dest,
+					    bool force)
 {
-	struct msi_desc *entry;
-	int ret;
+	struct msi_chip_data *cd = irq_get_chip_data(data->irq);
+	int hwmsi = cd->hwmsi;
+	int index = (hwmsi >> 6) & 0x3;
+	int bit;
+	int r;
 
 	/*
-	 * MSI-X is not supported.
+	 * If we are in the middle of updating the set, the first call
+	 * takes care of everything, do nothing successfully.
 	 */
-	if (type == PCI_CAP_ID_MSIX)
-		return -EINVAL;
+	if (atomic_sub_if_positive(1, affinity_in_progress + index) < 0)
+		return 0;
 
-	/*
-	 * If an architecture wants to support multiple MSI, it needs to
-	 * override arch_setup_msi_irqs()
-	 */
-	if (type == PCI_CAP_ID_MSI && nvec > 1)
-		return 1;
-
-	list_for_each_entry(entry, &dev->msi_list, list) {
-		ret = arch_setup_msi_irq(dev, entry);
-		if (ret < 0)
-			return ret;
-		if (ret > 0)
-			return -ENOSPC;
-	}
+	r = irq_set_affinity(OCTEON_IRQ_PCI_MSI0 + index, dest);
 
-	return 0;
+	for (bit = 0; bit < 64; bit++) {
+		int msi = octeon_irq_hwmsi_to_msi(64 * index + bit);
+		int partner = msi_to_irq[msi];
+		if (partner && partner != data->irq)
+			irq_set_affinity(partner, dest);
+	}
+	atomic_add(1, affinity_in_progress + index);
+	return r;
 }
 
-/**
- * Called when a device no longer needs its MSI interrupts. All
- * MSI interrupts for the device are freed.
- *
- * @irq:    The devices first irq number. There may be multple in sequence.
- */
-void arch_teardown_msi_irq(unsigned int irq)
+static int octeon_irq_msi_set_affinity_pci(struct irq_data *data,
+					   const struct cpumask *dest,
+					   bool force)
 {
-	int number_irqs;
-	u64 bitmask;
-	int index = 0;
-	int irq0;
-
-	if ((irq < OCTEON_IRQ_MSI_BIT0)
-		|| (irq > msi_irq_size + OCTEON_IRQ_MSI_BIT0))
-		panic("arch_teardown_msi_irq: Attempted to teardown illegal "
-		      "MSI interrupt (%d)", irq);
-
-	irq -= OCTEON_IRQ_MSI_BIT0;
-	index = irq / 64;
-	irq0 = irq % 64;
+	struct msi_chip_data *cd = irq_get_chip_data(data->irq);
+	int hwmsi = cd->hwmsi;
+	int index = hwmsi >> 4;
+	int bit;
+	int r;
 
 	/*
-	 * Count the number of IRQs we need to free by looking at the
-	 * msi_multiple_irq_bitmask. Each bit set means that the next
-	 * IRQ is also owned by this device.
+	 * If we are in the middle of updating the set, the first call
+	 * takes care of everything, do nothing successfully.
 	 */
-	number_irqs = 0;
-	while ((irq0 + number_irqs < 64) &&
-	       (msi_multiple_irq_bitmask[index]
-		& (1ull << (irq0 + number_irqs))))
-		number_irqs++;
-	number_irqs++;
-	/* Mask with one bit for each IRQ */
-	bitmask = (1 << number_irqs) - 1;
-	/* Shift the mask to the correct bit location */
-	bitmask <<= irq0;
-	if ((msi_free_irq_bitmask[index] & bitmask) != bitmask)
-		panic("arch_teardown_msi_irq: Attempted to teardown MSI "
-		      "interrupt (%d) not in use", irq);
-
-	/* Checks are done, update the in use bitmask */
-	spin_lock(&msi_free_irq_bitmask_lock);
-	msi_free_irq_bitmask[index] &= ~bitmask;
-	msi_multiple_irq_bitmask[index] &= ~bitmask;
-	spin_unlock(&msi_free_irq_bitmask_lock);
-}
+	if (atomic_sub_if_positive(1, affinity_in_progress + index) < 0)
+		return 0;
 
-static DEFINE_RAW_SPINLOCK(octeon_irq_msi_lock);
+	r = irq_set_affinity(OCTEON_IRQ_PCI_MSI0 + index, dest);
 
-static u64 msi_rcv_reg[4];
-static u64 mis_ena_reg[4];
+	for (bit = 0; bit < 16; bit++) {
+		int msi = octeon_irq_hwmsi_to_msi(64 * index + bit);
+		int partner = msi_to_irq[msi];
+		if (partner && partner != data->irq)
+			irq_set_affinity(partner, dest);
+	}
+	atomic_add(1, affinity_in_progress + index);
+	return r;
+}
+#endif /* CONFIG_SMP */
 
 static void octeon_irq_msi_enable_pcie(struct irq_data *data)
 {
 	u64 en;
 	unsigned long flags;
-	int msi_number = data->irq - OCTEON_IRQ_MSI_BIT0;
-	int irq_index = msi_number >> 6;
-	int irq_bit = msi_number & 0x3f;
+	struct msi_chip_data *cd = irq_get_chip_data(data->irq);
+	int hwmsi = cd->hwmsi;
+	int irq_index = hwmsi >> 6;
+	int irq_bit = hwmsi & 0x3f;
 
 	raw_spin_lock_irqsave(&octeon_irq_msi_lock, flags);
-	en = cvmx_read_csr(mis_ena_reg[irq_index]);
+	en = cvmx_read_csr(msi_ena_reg[irq_index]);
 	en |= 1ull << irq_bit;
-	cvmx_write_csr(mis_ena_reg[irq_index], en);
-	cvmx_read_csr(mis_ena_reg[irq_index]);
+	cvmx_write_csr(msi_ena_reg[irq_index], en);
+	cvmx_read_csr(msi_ena_reg[irq_index]);
 	raw_spin_unlock_irqrestore(&octeon_irq_msi_lock, flags);
+	unmask_msi_irq(data);
 }
 
 static void octeon_irq_msi_disable_pcie(struct irq_data *data)
 {
 	u64 en;
 	unsigned long flags;
-	int msi_number = data->irq - OCTEON_IRQ_MSI_BIT0;
-	int irq_index = msi_number >> 6;
-	int irq_bit = msi_number & 0x3f;
+	struct msi_chip_data *cd = irq_get_chip_data(data->irq);
+	int hwmsi = cd->hwmsi;
+	int irq_index = hwmsi >> 6;
+	int irq_bit = hwmsi & 0x3f;
 
 	raw_spin_lock_irqsave(&octeon_irq_msi_lock, flags);
-	en = cvmx_read_csr(mis_ena_reg[irq_index]);
+	en = cvmx_read_csr(msi_ena_reg[irq_index]);
 	en &= ~(1ull << irq_bit);
-	cvmx_write_csr(mis_ena_reg[irq_index], en);
-	cvmx_read_csr(mis_ena_reg[irq_index]);
+	cvmx_write_csr(msi_ena_reg[irq_index], en);
+	cvmx_read_csr(msi_ena_reg[irq_index]);
 	raw_spin_unlock_irqrestore(&octeon_irq_msi_lock, flags);
+	mask_msi_irq(data);
 }
 
 static struct irq_chip octeon_irq_chip_msi_pcie = {
 	.name = "MSI",
 	.irq_enable = octeon_irq_msi_enable_pcie,
 	.irq_disable = octeon_irq_msi_disable_pcie,
+#ifdef CONFIG_SMP
+	.irq_set_affinity = octeon_irq_msi_set_affinity_pcie,
+#endif
 };
 
 static void octeon_irq_msi_enable_pci(struct irq_data *data)
@@ -322,16 +296,271 @@ static struct irq_chip octeon_irq_chip_msi_pci = {
 	.name = "MSI",
 	.irq_enable = octeon_irq_msi_enable_pci,
 	.irq_disable = octeon_irq_msi_disable_pci,
+#ifdef CONFIG_SMP
+	.irq_set_affinity = octeon_irq_msi_set_affinity_pci,
+#endif
 };
 
 /*
+ * Update msg with the system specific address where the msi data is to be
+ * written.
+ *
+ * @msg:    Updated with the mis message address.
+ */
+static void setup_msi_msg_address(struct msi_msg *msg)
+{
+	switch (octeon_dma_bar_type) {
+	case OCTEON_DMA_BAR_TYPE_SMALL:
+		/* When not using big bar, Bar 0 is based at 128MB */
+		msg->address_lo =
+			((128ul << 20) + CVMX_PCI_MSI_RCV) & 0xffffffff;
+		msg->address_hi = ((128ul << 20) + CVMX_PCI_MSI_RCV) >> 32;
+		break;
+	case OCTEON_DMA_BAR_TYPE_BIG:
+		/* When using big bar, Bar 0 is based at 0 */
+		msg->address_lo = (0 + CVMX_PCI_MSI_RCV) & 0xffffffff;
+		msg->address_hi = (0 + CVMX_PCI_MSI_RCV) >> 32;
+		break;
+	case OCTEON_DMA_BAR_TYPE_PCIE:
+		/* When using PCIe, Bar 0 is based at 0 */
+		/* FIXME CVMX_NPEI_MSI_RCV* other than 0? */
+		msg->address_lo = (0 + CVMX_NPEI_PCIE_MSI_RCV) & 0xffffffff;
+		msg->address_hi = (0 + CVMX_NPEI_PCIE_MSI_RCV) >> 32;
+		break;
+	case OCTEON_DMA_BAR_TYPE_PCIE2:
+		/* When using PCIe2, Bar 0 is based at 0 */
+		msg->address_lo = (0 + CVMX_SLI_PCIE_MSI_RCV) & 0xffffffff;
+		msg->address_hi = (0 + CVMX_SLI_PCIE_MSI_RCV) >> 32;
+		break;
+	default:
+		panic("setup_msi_msg_address: Invalid octeon_dma_bar_type");
+	}
+}
+
+/*
+ * Allocate and configure multiple irqs for MSI interrupts for OCTEON.
+ *
+ * @node:      Node to configure interrupts for.
+ * @desc:      MSI descriptor.
+ * @msi_base:  First msi number.
+ * @nvec:      Number of MSI interrupts requested.
+ *
+ * Returns:    First irq number.
+ */
+static int arch_setup_msi_irq_ciu(int node, struct msi_desc *desc, int msi_base,
+				  int nvec)
+{
+	int irq_base;
+	struct irq_chip *chip;
+	struct msi_chip_data *cd;
+	int hwmsi;
+	int i;
+
+	irq_base = irq_alloc_descs(-1, 1, nvec, node);
+	if (irq_base < 0) {
+		WARN(1, "Unable to allocate %d irq(s)", nvec);
+		return -ENOSPC;
+	}
+
+	for (i = 0; i < nvec; i++) {
+		cd = kzalloc_node(sizeof(*cd), GFP_KERNEL, node);
+		if (!cd) {
+			for (i--; i >= 0; i--) {
+				cd = irq_get_chip_data(irq_base + i);
+				irq_set_chip_and_handler(irq_base + i, NULL,
+							 NULL);
+				irq_set_chip_data(irq_base + i, NULL);
+				kfree(cd);
+			}
+			irq_free_descs(irq_base, nvec);
+			return -ENOMEM;
+		}
+
+		cd->msi = msi_base + i;
+		hwmsi = octeon_irq_msi_to_hwmsi(msi_base + i);
+		cd->hwmsi = hwmsi;
+		msi_to_irq[msi_base + i] = irq_base + i;
+
+		/* Initialize the irq description */
+		if (octeon_dma_bar_type == OCTEON_DMA_BAR_TYPE_PCIE2)
+			chip = &octeon_irq_chip_msi_pcie;
+		else if (octeon_dma_bar_type == OCTEON_DMA_BAR_TYPE_PCIE)
+			chip = &octeon_irq_chip_msi_pcie;
+		else
+			chip = &octeon_irq_chip_msi_pci;
+
+		irq_set_chip_and_handler(irq_base + i, chip, handle_simple_irq);
+		irq_set_chip_data(irq_base + i, cd);
+		irq_set_msi_desc(irq_base + i, desc);
+	}
+
+	return irq_base;
+}
+
+/*
+ * Allocate and configure multiple irqs for MSI interrupts for OCTEON III.
+ *
+ * @node:      Node to configure interrupts for.
+ * @desc:      MSI descriptor.
+ * @msi_base:  First msi number.
+ * @nvec:      Number of MSI interrupts requested.
+ *
+ * Returns:    First irq number.
+ */
+static int arch_setup_msi_irq_ciu3(int node, struct msi_desc *desc,
+				   int msi_base, int nvec)
+{
+	struct irq_domain *domain;
+	int irq_base = -1;
+	int irq;
+	int hwirq;
+	int i;
+
+	/* Get the domain for the msi interrupts */
+	domain = octeon_irq_get_block_domain(node, MSI_BLOCK_NUMBER);
+
+	for (i = 0; i < nvec; i++) {
+		/* Get a irq for the msi intsn (hardware interrupt) */
+		hwirq = MSI_BLOCK_NUMBER << 12 | (msi_base + i);
+		irq = irq_create_mapping(domain, hwirq);
+		irqd_set_trigger_type(irq_get_irq_data(irq),
+				      IRQ_TYPE_EDGE_RISING);
+		irq_set_msi_desc(irq, desc);
+
+		if (i == 0)
+			irq_base = irq;
+	}
+
+	return irq_base;
+}
+
+/*
+ * Called when a driver request MSI interrupts instead of the
+ * legacy INT A-D. This routine will allocate multiple MSI interrupts
+ * for MSI devices that support them.
+ *
+ * @dev:       Device requesting MSI interrupts.
+ * @desc:      MSI descriptor.
+ * @nvec:      Number of interrupts requested.
+ *
+ * Returns 0 on success, error otherwise.
+ */
+static int arch_setup_multi_msi_irq(struct pci_dev *dev, struct msi_desc *desc,
+				    int nvec)
+{
+	struct msi_msg msg;
+	int irq_base;
+	int msi_base;
+	int node = 0; /* Must use the correct node. TODO */
+
+	/* Get a free msi interrupt block */
+	msi_base = msi_bitmap_alloc_hwirqs(node, nvec);
+	if (msi_base < 0)
+		return msi_base;
+
+	if (octeon_has_feature(OCTEON_FEATURE_CIU3))
+		irq_base = arch_setup_msi_irq_ciu3(node, desc, msi_base, nvec);
+	else
+		irq_base = arch_setup_msi_irq_ciu(node, desc, msi_base, nvec);
+
+	if (irq_base < 0) {
+		msi_bitmap_free_hwirqs(node, msi_base, 1);
+		return irq_base;
+	}
+
+	/* Set the base of the irqs used by this device */
+	irq_set_msi_desc(irq_base, desc);
+
+	/* Update the config space msi(x) capability structure */
+	desc->msi_attrib.multiple = ilog2(nvec);
+	msg.data = msi_base;
+	setup_msi_msg_address(&msg);
+	write_msi_msg(irq_base, &msg);
+
+	return 0;
+}
+
+/**
+ * Called when a driver request MSI/MSIX interrupts instead of the
+ * legacy INT A-D. This routine will allocate a single MSI/MSIX interrupt
+ * for MSI devices that support them.
+ *
+ * @dev:    Device requesting MSI interrupts
+ * @desc:   MSI descriptor
+ *
+ * Returns 0 on success, error otherwise.
+ */
+int arch_setup_msi_irq(struct pci_dev *dev, struct msi_desc *desc)
+{
+	struct msi_msg msg;
+	int irq;
+	int msi;
+	int node = 0; /* Must use the correct node. TODO */
+
+	/* Get a free msi interrupt */
+	msi = msi_bitmap_alloc_hwirqs(node, 1);
+	if (msi < 0)
+		return msi;
+
+	if (octeon_has_feature(OCTEON_FEATURE_CIU3))
+		irq = arch_setup_msi_irq_ciu3(node, desc, msi, 1);
+	else
+		irq = arch_setup_msi_irq_ciu(node, desc, msi, 1);
+
+	if (irq < 0) {
+		msi_bitmap_free_hwirqs(node, msi, 1);
+		return irq;
+	}
+
+	/* Update the config space msi(x) capability structure */
+	desc->msi_attrib.multiple = 0;
+	msg.data = msi;
+	setup_msi_msg_address(&msg);
+	write_msi_msg(irq, &msg);
+
+	return 0;
+}
+
+/**
+ * Called when a driver request MSI/MSIX interrupts instead of the
+ * legacy INT A-D. This routine will allocate multiple MSI/MSIX interrupts
+ * for MSI devices that support them.
+ *
+ * @dev:    Device requesting MSI interrupts
+ * @nvec:   Number of MSI interrupts requested.
+ * @type:   Interrupt type, MSI or MSIX.
+ *
+ * Returns 0 on success.
+ */
+int arch_setup_msi_irqs(struct pci_dev *dev, int nvec, int type)
+{
+	struct msi_desc *entry;
+	int rc = -1;
+
+	if (type == PCI_CAP_ID_MSI && nvec > 1) {
+		entry = list_first_entry(&dev->msi_list,
+					 struct msi_desc, list);
+		rc = arch_setup_multi_msi_irq(dev, entry, nvec);
+	} else {
+		list_for_each_entry(entry, &dev->msi_list, list) {
+			rc = arch_setup_msi_irq(dev, entry);
+			if (rc)
+				return rc;
+		}
+	}
+
+	return rc;
+}
+
+/*
  * Called by the interrupt handling code when an MSI interrupt
  * occurs.
  */
 static irqreturn_t __octeon_msi_do_interrupt(int index, u64 msi_bits)
 {
-	int irq;
 	int bit;
+	int msi;
+	int irq;
 
 	bit = fls64(msi_bits);
 	if (bit) {
@@ -339,8 +568,10 @@ static irqreturn_t __octeon_msi_do_interrupt(int index, u64 msi_bits)
 		/* Acknowledge it first. */
 		cvmx_write_csr(msi_rcv_reg[index], 1ull << bit);
 
-		irq = bit + OCTEON_IRQ_MSI_BIT0 + 64 * index;
-		do_IRQ(irq);
+		msi = octeon_irq_hwmsi_to_msi(bit + 64 * index);
+		irq = msi_to_irq[msi];
+
+		generic_handle_irq(irq);
 		return IRQ_HANDLED;
 	}
 	return IRQ_NONE;
@@ -361,76 +592,193 @@ OCTEON_MSI_INT_HANDLER_X(1);
 OCTEON_MSI_INT_HANDLER_X(2);
 OCTEON_MSI_INT_HANDLER_X(3);
 
+static void octeon_irq_msi_ciu3_ack(struct irq_data *data)
+{
+	u64 csr_addr;
+	struct octeon_ciu_chip_data *cd;
+	int msi;
+
+	octeon_irq_ciu3_ack(data);
+
+	cd = irq_data_get_irq_chip_data(data);
+
+	/* Acknowledge lsi (msi) interrupt (get the node from the ciu3 addr) */
+	msi = cd->intsn & 0xff;
+	csr_addr = (cd->ciu3_addr & CVMX_NODE_MASK) | msi_rcv_reg[msi >> 6];
+	cvmx_write_csr(csr_addr, 1 << (msi & 0x3f));
+}
+
+static void octeon_irq_msi_ciu3_mask_ack(struct irq_data *data)
+{
+	u64 csr_addr;
+	struct octeon_ciu_chip_data *cd;
+	int msi;
+
+	octeon_irq_ciu3_mask_ack(data);
+
+	cd = irq_data_get_irq_chip_data(data);
+
+	/* Acknowledge lsi (msi) interrupt (get the node from the ciu3 addr) */
+	msi = cd->intsn & 0xff;
+	csr_addr = (cd->ciu3_addr & CVMX_NODE_MASK) | msi_rcv_reg[msi >> 6];
+	cvmx_write_csr(csr_addr, 1 << (msi & 0x3f));
+}
+
+static void octeon_irq_msi_ciu3_enable(struct irq_data *data)
+{
+	octeon_irq_ciu3_enable(data);
+	unmask_msi_irq(data);
+}
+
+static void octeon_irq_msi_ciu3_disable(struct irq_data *data)
+{
+	octeon_irq_ciu3_disable(data);
+	mask_msi_irq(data);
+}
+
+static struct irq_chip octeon_irq_msi_chip_ciu3 = {
+	.name = "MSI-X",
+	.irq_enable = octeon_irq_msi_ciu3_enable,
+	.irq_disable = octeon_irq_msi_ciu3_disable,
+	.irq_ack = octeon_irq_msi_ciu3_ack,
+	.irq_mask = octeon_irq_ciu3_mask,
+	.irq_mask_ack = octeon_irq_msi_ciu3_mask_ack,
+	.irq_unmask = octeon_irq_ciu3_enable,
+#ifdef CONFIG_SMP
+	.irq_set_affinity = octeon_irq_ciu3_set_affinity,
+#endif
+};
+
+static int octeon_irq_msi_ciu3_map(struct irq_domain *d,
+				   unsigned int virq, irq_hw_number_t hw)
+{
+	return octeon_irq_ciu3_mapx(d, virq, hw, &octeon_irq_msi_chip_ciu3);
+}
+
+struct irq_domain_ops octeon_msi_domain_ciu3_ops = {
+	.map = octeon_irq_msi_ciu3_map,
+	.unmap = octeon_irq_free_cd,
+	.xlate = octeon_irq_ciu3_xlat,
+};
+
 /*
  * Initializes the MSI interrupt handling code
  */
 int __init octeon_msi_initialize(void)
 {
-	int irq;
-	struct irq_chip *msi;
+	struct irq_domain *domain;
+	u64 msi_map_reg;
+	int i;
+	int node = 0; /* Must use correct node. TODO */
+
+	/* Clear msi irq bitmap */
+	for (i = 0; i < CVMX_MAX_NODES; i++)
+		bitmap_zero(msi_free_irq_bitmap[i], MSI_IRQ_SIZE);
+
+	if (octeon_has_feature(OCTEON_FEATURE_CIU3)) {
+		int	irq_base;
+
+		/* MSI interrupts use their own domain */
+		irq_base = irq_alloc_descs(-1, 0, MSI_IRQ_SIZE, 0);
+		domain = irq_domain_add_legacy(NULL, MSI_IRQ_SIZE, irq_base,
+					       MSI_BLOCK_NUMBER << 12,
+					       &octeon_msi_domain_ciu3_ops,
+					       octeon_irq_get_ciu3_info(node));
+		octeon_irq_add_block_domain(node, MSI_BLOCK_NUMBER, domain);
+
+		/* Registers to acknowledge msi interrupts */
+		msi_rcv_reg[0] = CVMX_PEXP_SLI_MSI_RCV0;
+		msi_rcv_reg[1] = CVMX_PEXP_SLI_MSI_RCV1;
+		msi_rcv_reg[2] = CVMX_PEXP_SLI_MSI_RCV2;
+		msi_rcv_reg[3] = CVMX_PEXP_SLI_MSI_RCV3;
+		return 0;
+	}
 
-	if (octeon_dma_bar_type == OCTEON_DMA_BAR_TYPE_PCIE) {
+	if (octeon_dma_bar_type == OCTEON_DMA_BAR_TYPE_PCIE2) {
+		msi_rcv_reg[0] = CVMX_PEXP_SLI_MSI_RCV0;
+		msi_rcv_reg[1] = CVMX_PEXP_SLI_MSI_RCV1;
+		msi_rcv_reg[2] = CVMX_PEXP_SLI_MSI_RCV2;
+		msi_rcv_reg[3] = CVMX_PEXP_SLI_MSI_RCV3;
+		msi_ena_reg[0] = CVMX_PEXP_SLI_MSI_ENB0;
+		msi_ena_reg[1] = CVMX_PEXP_SLI_MSI_ENB1;
+		msi_ena_reg[2] = CVMX_PEXP_SLI_MSI_ENB2;
+		msi_ena_reg[3] = CVMX_PEXP_SLI_MSI_ENB3;
+		octeon_irq_msi_to_hwmsi = octeon_irq_msi_to_hwmsi_scatter;
+		octeon_irq_hwmsi_to_msi = octeon_irq_hwmsi_to_msi_scatter;
+		msi_map_reg = CVMX_PEXP_SLI_MSI_WR_MAP;
+	} else if (octeon_dma_bar_type == OCTEON_DMA_BAR_TYPE_PCIE) {
 		msi_rcv_reg[0] = CVMX_PEXP_NPEI_MSI_RCV0;
 		msi_rcv_reg[1] = CVMX_PEXP_NPEI_MSI_RCV1;
 		msi_rcv_reg[2] = CVMX_PEXP_NPEI_MSI_RCV2;
 		msi_rcv_reg[3] = CVMX_PEXP_NPEI_MSI_RCV3;
-		mis_ena_reg[0] = CVMX_PEXP_NPEI_MSI_ENB0;
-		mis_ena_reg[1] = CVMX_PEXP_NPEI_MSI_ENB1;
-		mis_ena_reg[2] = CVMX_PEXP_NPEI_MSI_ENB2;
-		mis_ena_reg[3] = CVMX_PEXP_NPEI_MSI_ENB3;
-		msi = &octeon_irq_chip_msi_pcie;
+		msi_ena_reg[0] = CVMX_PEXP_NPEI_MSI_ENB0;
+		msi_ena_reg[1] = CVMX_PEXP_NPEI_MSI_ENB1;
+		msi_ena_reg[2] = CVMX_PEXP_NPEI_MSI_ENB2;
+		msi_ena_reg[3] = CVMX_PEXP_NPEI_MSI_ENB3;
+		octeon_irq_msi_to_hwmsi = octeon_irq_msi_to_hwmsi_scatter;
+		octeon_irq_hwmsi_to_msi = octeon_irq_hwmsi_to_msi_scatter;
+		msi_map_reg = CVMX_PEXP_NPEI_MSI_WR_MAP;
 	} else {
 		msi_rcv_reg[0] = CVMX_NPI_NPI_MSI_RCV;
 #define INVALID_GENERATE_ADE 0x8700000000000000ULL;
 		msi_rcv_reg[1] = INVALID_GENERATE_ADE;
 		msi_rcv_reg[2] = INVALID_GENERATE_ADE;
 		msi_rcv_reg[3] = INVALID_GENERATE_ADE;
-		mis_ena_reg[0] = INVALID_GENERATE_ADE;
-		mis_ena_reg[1] = INVALID_GENERATE_ADE;
-		mis_ena_reg[2] = INVALID_GENERATE_ADE;
-		mis_ena_reg[3] = INVALID_GENERATE_ADE;
-		msi = &octeon_irq_chip_msi_pci;
+		msi_ena_reg[0] = INVALID_GENERATE_ADE;
+		msi_ena_reg[1] = INVALID_GENERATE_ADE;
+		msi_ena_reg[2] = INVALID_GENERATE_ADE;
+		msi_ena_reg[3] = INVALID_GENERATE_ADE;
+		octeon_irq_msi_to_hwmsi = octeon_irq_msi_to_hwmsi_linear;
+		octeon_irq_hwmsi_to_msi = octeon_irq_hwmsi_to_msi_linear;
+		msi_map_reg = 0;
 	}
 
-	for (irq = OCTEON_IRQ_MSI_BIT0; irq <= OCTEON_IRQ_MSI_LAST; irq++)
-		irq_set_chip_and_handler(irq, msi, handle_simple_irq);
+	if (msi_map_reg) {
+		int msi;
+		int ciu;
+		u64 e;
+
+		for (msi = 0; msi < 256; msi++) {
+			ciu = (msi >> 2) | ((msi << 6) & 0xc0);
+			e = (ciu << 8) | msi;
+			cvmx_write_csr(msi_map_reg, e);
+		}
+	}
 
 	if (octeon_has_feature(OCTEON_FEATURE_PCIE)) {
 		if (request_irq(OCTEON_IRQ_PCI_MSI0, octeon_msi_interrupt0,
-				0, "MSI[0:63]", octeon_msi_interrupt0))
+				IRQF_NO_THREAD, "MSI[0:63]", octeon_msi_interrupt0))
 			panic("request_irq(OCTEON_IRQ_PCI_MSI0) failed");
 
 		if (request_irq(OCTEON_IRQ_PCI_MSI1, octeon_msi_interrupt1,
-				0, "MSI[64:127]", octeon_msi_interrupt1))
+				IRQF_NO_THREAD, "MSI[64:127]", octeon_msi_interrupt1))
 			panic("request_irq(OCTEON_IRQ_PCI_MSI1) failed");
 
 		if (request_irq(OCTEON_IRQ_PCI_MSI2, octeon_msi_interrupt2,
-				0, "MSI[127:191]", octeon_msi_interrupt2))
+				IRQF_NO_THREAD, "MSI[127:191]", octeon_msi_interrupt2))
 			panic("request_irq(OCTEON_IRQ_PCI_MSI2) failed");
 
 		if (request_irq(OCTEON_IRQ_PCI_MSI3, octeon_msi_interrupt3,
-				0, "MSI[192:255]", octeon_msi_interrupt3))
+				IRQF_NO_THREAD, "MSI[192:255]", octeon_msi_interrupt3))
 			panic("request_irq(OCTEON_IRQ_PCI_MSI3) failed");
-
-		msi_irq_size = 256;
 	} else if (octeon_is_pci_host()) {
 		if (request_irq(OCTEON_IRQ_PCI_MSI0, octeon_msi_interrupt0,
-				0, "MSI[0:15]", octeon_msi_interrupt0))
+				IRQF_NO_THREAD, "MSI[0:15]", octeon_msi_interrupt0))
 			panic("request_irq(OCTEON_IRQ_PCI_MSI0) failed");
 
 		if (request_irq(OCTEON_IRQ_PCI_MSI1, octeon_msi_interrupt0,
-				0, "MSI[16:31]", octeon_msi_interrupt0))
+				IRQF_NO_THREAD, "MSI[16:31]", octeon_msi_interrupt0))
 			panic("request_irq(OCTEON_IRQ_PCI_MSI1) failed");
 
 		if (request_irq(OCTEON_IRQ_PCI_MSI2, octeon_msi_interrupt0,
-				0, "MSI[32:47]", octeon_msi_interrupt0))
+				IRQF_NO_THREAD, "MSI[32:47]", octeon_msi_interrupt0))
 			panic("request_irq(OCTEON_IRQ_PCI_MSI2) failed");
 
 		if (request_irq(OCTEON_IRQ_PCI_MSI3, octeon_msi_interrupt0,
-				0, "MSI[48:63]", octeon_msi_interrupt0))
+				IRQF_NO_THREAD, "MSI[48:63]", octeon_msi_interrupt0))
 			panic("request_irq(OCTEON_IRQ_PCI_MSI3) failed");
-		msi_irq_size = 64;
 	}
 	return 0;
 }
 subsys_initcall(octeon_msi_initialize);
+
-- 
1.9.1

