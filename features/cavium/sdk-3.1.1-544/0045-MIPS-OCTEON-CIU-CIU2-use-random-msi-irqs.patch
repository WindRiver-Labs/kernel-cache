From cb049adb2e3c208dcc528f4c6805ad58852ffca4 Mon Sep 17 00:00:00 2001
From: Carlos Munoz <carlos.munoz@caviumnetworks.com>
Date: Mon, 24 Mar 2014 17:40:15 -0700
Subject: [PATCH 045/202] MIPS/OCTEON: CIU/CIU2 use random msi irqs.

Signed-off-by: Carlos Munoz <carlos.munoz@caviumnetworks.com>
[Original patch taken from Cavium SDK 3.1.1-544]
Signed-off-by: Bin Jiang <bin.jiang@windriver.com>
---
 arch/mips/include/asm/mach-cavium-octeon/irq.h |  12 +-
 arch/mips/pci/msi-octeon.c                     | 492 ++++++++++---------------
 2 files changed, 193 insertions(+), 311 deletions(-)

diff --git a/arch/mips/include/asm/mach-cavium-octeon/irq.h b/arch/mips/include/asm/mach-cavium-octeon/irq.h
index 6a53ba1..87adfc7 100644
--- a/arch/mips/include/asm/mach-cavium-octeon/irq.h
+++ b/arch/mips/include/asm/mach-cavium-octeon/irq.h
@@ -44,17 +44,7 @@ enum octeon_irq {
 	OCTEON_IRQ_SRIO1,
 	OCTEON_IRQ_SRIO2,
 	OCTEON_IRQ_SRIO3,
-#ifndef CONFIG_PCI_MSI
-	OCTEON_IRQ_LAST = 255
-#endif
+	OCTEON_IRQ_LAST = 511
 };
 
-#ifdef CONFIG_PCI_MSI
-/* 256 - 511 represent the MSI interrupts 0-255 */
-#define OCTEON_IRQ_MSI_BIT0	(256)
-
-#define OCTEON_IRQ_MSI_LAST	 (OCTEON_IRQ_MSI_BIT0 + 255)
-#define OCTEON_IRQ_LAST		 (OCTEON_IRQ_MSI_LAST + 1)
-#endif
-
 #endif
diff --git a/arch/mips/pci/msi-octeon.c b/arch/mips/pci/msi-octeon.c
index 291fdec..d976187 100644
--- a/arch/mips/pci/msi-octeon.c
+++ b/arch/mips/pci/msi-octeon.c
@@ -27,6 +27,14 @@
 #define MSI_IRQ_SIZE		256
 
 /*
+ * Data to save in the chip_data field of the irq description.
+ */
+struct msi_chip_data {
+	int msi;
+	int hwmsi;
+};
+
+/*
  * Each bit in msi_free_irq_bitmap represents a MSI interrupt that is
  * in use. Each node requires its own set of bits.
  */
@@ -37,95 +45,8 @@ static DECLARE_BITMAP(msi_free_irq_bitmap[CVMX_MAX_NODES], MSI_IRQ_SIZE);
  */
 static DEFINE_SPINLOCK(msi_free_irq_bitmap_lock);
 
-/*
- * Number of MSI IRQs used. This variable is set up in
- * the module init time.
- */
-static int msi_irq_size;
-
-/**
- * Called when a driver request MSI interrupts instead of the
- * legacy INT A-D. This routine will allocate multiple interrupts
- * for MSI devices that support them. A device can override this by
- * programming the MSI control bits [6:4] before calling
- * pci_enable_msi().
- *
- * @dev:    Device requesting MSI interrupts
- * @desc:   MSI descriptor
- *
- * Returns 0 on success.
- */
-int arch_setup_msi_irq(struct pci_dev *dev, struct msi_desc *desc)
-{
-	struct msi_msg msg;
-	int irq;
-	int hwirq;
-	int msi;
-	struct irq_domain *domain;
-	int node = 0; /* Must use the correct node. TODO */
-
-	/*
-	 * We're going to search msi_free_irq_bitmap for zero bits. This
-	 * represents an MSI interrupt number that isn't in use.
-	 */
-	spin_lock(&msi_free_irq_bitmap_lock);
-	msi = find_next_zero_bit(msi_free_irq_bitmap[node], MSI_IRQ_SIZE, 0);
-	if (msi >= MSI_IRQ_SIZE) {
-		spin_unlock(&msi_free_irq_bitmap_lock);
-		WARN(1, "arch_setup_msi_irq: Unable to find a free MSI "
-		     "interrupt");
-		return -ENOSPC;
-	}
-
-	set_bit(msi, msi_free_irq_bitmap[node]);
-	spin_unlock(&msi_free_irq_bitmap_lock);
-	msg.data = msi;
-
-	if (octeon_has_feature(OCTEON_FEATURE_CIU3)) {
-		/* Get the domain for the msi interrupts */
-		domain = octeon_irq_get_block_domain(node, MSI_BLOCK_NUMBER);
-
-		/* Get a irq for the msi intsn (hardware interrupt) */
-		hwirq = MSI_BLOCK_NUMBER << 12 | msi;
-		irq = irq_create_mapping(domain, hwirq);
-		irqd_set_trigger_type(irq_get_irq_data(irq),
-				      IRQ_TYPE_EDGE_RISING);
-	} else {
-		/* MSI interrupts start at logical IRQ OCTEON_IRQ_MSI_BIT0 */
-		irq = msi + OCTEON_IRQ_MSI_BIT0;
-	}
-
-	switch (octeon_dma_bar_type) {
-	case OCTEON_DMA_BAR_TYPE_SMALL:
-		/* When not using big bar, Bar 0 is based at 128MB */
-		msg.address_lo =
-			((128ul << 20) + CVMX_PCI_MSI_RCV) & 0xffffffff;
-		msg.address_hi = ((128ul << 20) + CVMX_PCI_MSI_RCV) >> 32;
-		break;
-	case OCTEON_DMA_BAR_TYPE_BIG:
-		/* When using big bar, Bar 0 is based at 0 */
-		msg.address_lo = (0 + CVMX_PCI_MSI_RCV) & 0xffffffff;
-		msg.address_hi = (0 + CVMX_PCI_MSI_RCV) >> 32;
-		break;
-	case OCTEON_DMA_BAR_TYPE_PCIE:
-		/* When using PCIe, Bar 0 is based at 0 */
-		/* FIXME CVMX_NPEI_MSI_RCV* other than 0? */
-		msg.address_lo = (0 + CVMX_NPEI_PCIE_MSI_RCV) & 0xffffffff;
-		msg.address_hi = (0 + CVMX_NPEI_PCIE_MSI_RCV) >> 32;
-		break;
-	case OCTEON_DMA_BAR_TYPE_PCIE2:
-		/* When using PCIe2, Bar 0 is based at 0 */
-		msg.address_lo = (0 + CVMX_SLI_PCIE_MSI_RCV) & 0xffffffff;
-		msg.address_hi = (0 + CVMX_SLI_PCIE_MSI_RCV) >> 32;
-		break;
-	default:
-		panic("arch_setup_msi_irq: Invalid octeon_dma_bar_type");
-	}
-
-	irq_set_msi_desc(irq, desc);
-	write_msi_msg(irq, &msg);
-	return 0;
-}
+/* MSI to IRQ lookup */
+static int msi_to_irq[MSI_IRQ_SIZE];
 
 /**
  * Called when a device no longer needs its MSI interrupts. All
@@ -136,16 +57,19 @@ int arch_setup_msi_irq(struct pci_dev *dev, struct msi_desc *desc)
 void arch_teardown_msi_irq(unsigned int irq)
 {
 	int old;
+	int msi;
 	int node = 0; /* Must use node device is in. TODO */
 
-	if ((irq < OCTEON_IRQ_MSI_BIT0)
-		|| (irq > msi_irq_size + OCTEON_IRQ_MSI_BIT0 - 1))
-		panic("arch_teardown_msi_irq: Attempted to teardown illegal MSI interrupt (%d)",
-		      irq);
+	if (octeon_has_feature(OCTEON_FEATURE_CIU3)) {
+		struct octeon_ciu_chip_data *cd3 = irq_get_chip_data(irq);
+		msi = cd3->intsn & 0xff;
+	} else {
+		struct msi_chip_data *cd = irq_get_chip_data(irq);
+		msi = cd->msi;
+	}
 
-	irq -= OCTEON_IRQ_MSI_BIT0;
 	spin_lock(&msi_free_irq_bitmap_lock);
-	old = test_and_clear_bit(irq, msi_free_irq_bitmap[node]);
+	old = test_and_clear_bit(msi, msi_free_irq_bitmap[node]);
 	spin_unlock(&msi_free_irq_bitmap_lock);
 
 	if (!old) {
@@ -159,28 +83,46 @@ static DEFINE_RAW_SPINLOCK(octeon_irq_msi_lock);
 static u64 msi_rcv_reg[4];
 static u64 msi_ena_reg[4];
 
-static int (*octeon_irq_msi_to_irq)(int);
-static int (*octeon_irq_irq_to_msi)(int);
+/*
+ * Up to 256 MSIs are supported. MSIs are allocated sequencially from 0 to 255.
+ * The CIU has 4 interrupt lines each supporting 64 MSIs to handle the 256 MSI
+ * interrupts.
+ * Software might desire to map MSIs to different CIU interrupt lines to share
+ * the load. For example, MSI 0 might be mapped to CIU interrupt line 0, MSI 1
+ * to CIU interrupt line 1, and so on.
+ * Hardware MSIs indicate the CIU interrupt line and the bit within the line a
+ * particular MSI is mapped to.
+ * These pointers point to the methods that performs the mapping to use.
+ */
+static int (*octeon_irq_msi_to_hwmsi)(int);
+static int (*octeon_irq_hwmsi_to_msi)(int);
 
-static int octeon_irq_msi_to_irq_linear(int msi)
+/*
+ * MSI to hardware MSI linear mapping. No load sharing. First 64 allocated MSIs
+ * go to CIU interrupt line 0, next 64 to the next CIU line and so on.
+ */
+static int octeon_irq_msi_to_hwmsi_linear(int msi)
 {
-	return msi + OCTEON_IRQ_MSI_BIT0;
+	return msi;
 }
 
-static int octeon_irq_irq_to_msi_linear(int irq)
+static int octeon_irq_hwmsi_to_msi_linear(int hwmsi)
 {
-	return irq - OCTEON_IRQ_MSI_BIT0;
+	return hwmsi;
 }
 
-static int octeon_irq_msi_to_irq_scatter(int msi)
+/*
+ * MSI to hardware MSI scatter mapping. MSI interrupt load is spread among all
+ * CIU interrupt lines. MSI 0 goes to CIU line 0, MSI 1 to CIU line 1 and so on.
+ */
+static int octeon_irq_msi_to_hwmsi_scatter(int msi)
 {
-	return (((msi >> 6) & 0x3) | ((msi << 2) & 0xfc)) + OCTEON_IRQ_MSI_BIT0;
+	return ((msi << 6) & 0xc0) | ((msi >> 2) & 0x3f);
 }
 
-static int octeon_irq_irq_to_msi_scatter(int irq)
+static int octeon_irq_hwmsi_to_msi_scatter(int hwmsi)
 {
-	int t = irq - OCTEON_IRQ_MSI_BIT0;
-	return ((t << 6) & 0xc0) | ((t >> 2) & 0x3f);
+	return (((hwmsi >> 6) & 0x3) | ((hwmsi << 2) & 0xfc));
 }
 
 #ifdef CONFIG_SMP
@@ -195,8 +137,9 @@ static int octeon_irq_msi_set_affinity_pcie(struct irq_data *data,
 					    const struct cpumask *dest,
 					    bool force)
 {
-	int msi = octeon_irq_irq_to_msi(data->irq);
-	int index = msi >> 6;
+	struct msi_chip_data *cd = irq_get_chip_data(data->irq);
+	int hwmsi = cd->hwmsi;
+	int index = (hwmsi >> 6) & 0x3;
 	int bit;
 	int r;
 
@@ -210,8 +153,9 @@ static int octeon_irq_msi_set_affinity_pcie(struct irq_data *data,
 	r = irq_set_affinity(OCTEON_IRQ_PCI_MSI0 + index, dest);
 
 	for (bit = 0; bit < 64; bit++) {
-		int partner = octeon_irq_msi_to_irq(64 * index + bit);
-		if (partner != data->irq)
+		int msi = octeon_irq_hwmsi_to_msi(64 * index + bit);
+		int partner = msi_to_irq[msi];
+		if (partner && partner != data->irq)
 			irq_set_affinity(partner, dest);
 	}
 	atomic_add(1, affinity_in_progress + index);
@@ -222,8 +166,9 @@ static int octeon_irq_msi_set_affinity_pci(struct irq_data *data,
 					   const struct cpumask *dest,
 					   bool force)
 {
-	int msi = octeon_irq_irq_to_msi(data->irq);
-	int index = msi >> 4;
+	struct msi_chip_data *cd = irq_get_chip_data(data->irq);
+	int hwmsi = cd->hwmsi;
+	int index = hwmsi >> 4;
 	int bit;
 	int r;
 
@@ -237,8 +182,9 @@ static int octeon_irq_msi_set_affinity_pci(struct irq_data *data,
 	r = irq_set_affinity(OCTEON_IRQ_PCI_MSI0 + index, dest);
 
 	for (bit = 0; bit < 16; bit++) {
-		int partner = octeon_irq_msi_to_irq(16 * index + bit);
-		if (partner != data->irq)
+		int msi = octeon_irq_hwmsi_to_msi(64 * index + bit);
+		int partner = msi_to_irq[msi];
+		if (partner && partner != data->irq)
 			irq_set_affinity(partner, dest);
 	}
 	atomic_add(1, affinity_in_progress + index);
@@ -250,9 +196,10 @@ static void octeon_irq_msi_enable_pcie(struct irq_data *data)
 {
 	u64 en;
 	unsigned long flags;
-	int msi_number = octeon_irq_irq_to_msi(data->irq);
-	int irq_index = msi_number >> 6;
-	int irq_bit = msi_number & 0x3f;
+	struct msi_chip_data *cd = irq_get_chip_data(data->irq);
+	int hwmsi = cd->hwmsi;
+	int irq_index = hwmsi >> 6;
+	int irq_bit = hwmsi & 0x3f;
 
 	raw_spin_lock_irqsave(&octeon_irq_msi_lock, flags);
 	en = cvmx_read_csr(msi_ena_reg[irq_index]);
@@ -267,9 +214,10 @@ static void octeon_irq_msi_disable_pcie(struct irq_data *data)
 {
 	u64 en;
 	unsigned long flags;
-	int msi_number = octeon_irq_irq_to_msi(data->irq);
-	int irq_index = msi_number >> 6;
-	int irq_bit = msi_number & 0x3f;
+	struct msi_chip_data *cd = irq_get_chip_data(data->irq);
+	int hwmsi = cd->hwmsi;
+	int irq_index = hwmsi >> 6;
+	int irq_bit = hwmsi & 0x3f;
 
 	raw_spin_lock_irqsave(&octeon_irq_msi_lock, flags);
 	en = cvmx_read_csr(msi_ena_reg[irq_index]);
@@ -314,14 +262,129 @@ static struct irq_chip octeon_irq_chip_msi_pci = {
 #endif
 };
 
+/**
+ * Called when a driver request MSI interrupts instead of the
+ * legacy INT A-D. This routine will allocate multiple interrupts
+ * for MSI devices that support them. A device can override this by
+ * programming the MSI control bits [6:4] before calling
+ * pci_enable_msi().
+ *
+ * @dev:    Device requesting MSI interrupts
+ * @desc:   MSI descriptor
+ *
+ * Returns 0 on success.
+ */
+int arch_setup_msi_irq(struct pci_dev *dev, struct msi_desc *desc)
+{
+	struct msi_msg msg;
+	int irq;
+	int hwirq;
+	int msi;
+	struct irq_chip *chip;
+	struct irq_domain *domain;
+	int node = 0; /* Must use the correct node. TODO */
+
+	/*
+	 * We're going to search msi_free_irq_bitmap for zero bits. This
+	 * represents an MSI interrupt number that isn't in use.
+	 */
+	spin_lock(&msi_free_irq_bitmap_lock);
+	msi = find_next_zero_bit(msi_free_irq_bitmap[node], MSI_IRQ_SIZE, 0);
+	if (msi >= MSI_IRQ_SIZE) {
+		spin_unlock(&msi_free_irq_bitmap_lock);
+		WARN(1, "arch_setup_msi_irq: Unable to find a free MSI "
+		     "interrupt");
+		return -ENOSPC;
+	}
+
+	set_bit(msi, msi_free_irq_bitmap[node]);
+	spin_unlock(&msi_free_irq_bitmap_lock);
+	msg.data = msi;
+
+	if (octeon_has_feature(OCTEON_FEATURE_CIU3)) {
+		/* Get the domain for the msi interrupts */
+		domain = octeon_irq_get_block_domain(node, MSI_BLOCK_NUMBER);
+
+		/* Get a irq for the msi intsn (hardware interrupt) */
+		hwirq = MSI_BLOCK_NUMBER << 12 | msi;
+		irq = irq_create_mapping(domain, hwirq);
+		irqd_set_trigger_type(irq_get_irq_data(irq),
+				      IRQ_TYPE_EDGE_RISING);
+	} else {
+		struct msi_chip_data *cd;
+		int hwmsi = octeon_irq_msi_to_hwmsi(msi);
+
+		/* Reuse the irq if already assigned to the msi */
+		if (msi_to_irq[msi])
+			irq = msi_to_irq[msi];
+		else {
+			cd = kzalloc(sizeof(*cd), GFP_KERNEL);
+			cd->msi = msi;
+			cd->hwmsi = hwmsi;
+
+			if ((irq = irq_alloc_descs(-1, 1, 1, node)) < 0) {
+				WARN(1, "arch_setup_msi_irq: Unable to find a "
+				     "free irq\n");
+				clear_bit(msi, msi_free_irq_bitmap[node]);
+				kfree(cd);
+				return -ENOSPC;
+			}
+			msi_to_irq[msi] = irq;
+
+			/* Initialize the irq description */
+			if (octeon_dma_bar_type == OCTEON_DMA_BAR_TYPE_PCIE2)
+				chip = &octeon_irq_chip_msi_pcie;
+			else if (octeon_dma_bar_type == OCTEON_DMA_BAR_TYPE_PCIE)
+				chip = &octeon_irq_chip_msi_pcie;
+			else
+				chip = &octeon_irq_chip_msi_pci;
+
+			irq_set_chip_and_handler(irq, chip, handle_simple_irq);
+			irq_set_chip_data(irq, cd);
+		}
+	}
+
+	switch (octeon_dma_bar_type) {
+	case OCTEON_DMA_BAR_TYPE_SMALL:
+		/* When not using big bar, Bar 0 is based at 128MB */
+		msg.address_lo =
+			((128ul << 20) + CVMX_PCI_MSI_RCV) & 0xffffffff;
+		msg.address_hi = ((128ul << 20) + CVMX_PCI_MSI_RCV) >> 32;
+		break;
+	case OCTEON_DMA_BAR_TYPE_BIG:
+		/* When using big bar, Bar 0 is based at 0 */
+		msg.address_lo = (0 + CVMX_PCI_MSI_RCV) & 0xffffffff;
+		msg.address_hi = (0 + CVMX_PCI_MSI_RCV) >> 32;
+		break;
+	case OCTEON_DMA_BAR_TYPE_PCIE:
+		/* When using PCIe, Bar 0 is based at 0 */
+		/* FIXME CVMX_NPEI_MSI_RCV* other than 0? */
+		msg.address_lo = (0 + CVMX_NPEI_PCIE_MSI_RCV) & 0xffffffff;
+		msg.address_hi = (0 + CVMX_NPEI_PCIE_MSI_RCV) >> 32;
+		break;
+	case OCTEON_DMA_BAR_TYPE_PCIE2:
+		/* When using PCIe2, Bar 0 is based at 0 */
+		msg.address_lo = (0 + CVMX_SLI_PCIE_MSI_RCV) & 0xffffffff;
+		msg.address_hi = (0 + CVMX_SLI_PCIE_MSI_RCV) >> 32;
+		break;
+	default:
+		panic("arch_setup_msi_irq: Invalid octeon_dma_bar_type");
+	}
+
+	irq_set_msi_desc(irq, desc);
+	write_msi_msg(irq, &msg);
+	return 0;
+}
+
 /*
  * Called by the interrupt handling code when an MSI interrupt
  * occurs.
  */
 static irqreturn_t __octeon_msi_do_interrupt(int index, u64 msi_bits)
 {
-	int irq;
 	int bit;
+	int msi;
+	int irq;
 
 	bit = fls64(msi_bits);
 	if (bit) {
@@ -329,7 +392,8 @@ static irqreturn_t __octeon_msi_do_interrupt(int index, u64 msi_bits)
 		/* Acknowledge it first. */
 		cvmx_write_csr(msi_rcv_reg[index], 1ull << bit);
 
-		irq = octeon_irq_msi_to_irq(bit + 64 * index);
+		msi = octeon_irq_hwmsi_to_msi(bit + 64 * index);
+		irq = msi_to_irq[msi];
 
 		generic_handle_irq(irq);
 		return IRQ_HANDLED;
@@ -352,163 +416,6 @@ OCTEON_MSI_INT_HANDLER_X(1);
 OCTEON_MSI_INT_HANDLER_X(2);
 OCTEON_MSI_INT_HANDLER_X(3);
 
-#if 0
-static void octeon_msi_ciu2_enable_on_cpu(unsigned int irq, int cpu)
-{
-	int core;
-	int msi = irq - OCTEON_IRQ_MSI_BIT0;
-	union cvmx_ciu2_msi_selx sel;
-
-	core = octeon_coreid_for_cpu(cpu);
-
-	sel.u64 = 0;
-	sel.s.pp_num = core;
-	sel.s.ip_num = 2; /* IP4 */
-	sel.s.en = 1;
-	cvmx_write_csr(CVMX_CIU2_MSI_SELX(msi), sel.u64);
-	/* Read back some CSR for write to complete. */
-	cvmx_read_csr(CVMX_CIU2_SUM_PPX_IP2(core));
-}
-
-static void octeon_msi_ciu2_enable(struct irq_data *data)
-{
-	int cpu;
-
-#ifdef CONFIG_SMP
-	cpu = cpumask_first(data->affinity);
-#else
-	cpu = 0;
-#endif
-	octeon_msi_ciu2_enable_on_cpu(data->irq, cpu);
-}
-
-static void octeon_msi_ciu2_disable(struct irq_data *data)
-{
-	int msi = data->irq - OCTEON_IRQ_MSI_BIT0;
-	union cvmx_ciu2_msi_selx sel;
-
-	sel.u64 = cvmx_read_csr(CVMX_CIU2_MSI_SELX(msi));
-	sel.s.en = 0;
-	cvmx_write_csr(CVMX_CIU2_MSI_SELX(msi), sel.u64);
-	cvmx_read_csr(CVMX_CIU2_INTR_CIU_READY);
-}
-
-static void octeon_msi_ciu2_ack(struct irq_data *data)
-{
-	int msi = data->irq - OCTEON_IRQ_MSI_BIT0;
-
-	cvmx_write_csr(CVMX_CIU2_MSI_RCVX(msi), 0);
-	cvmx_read_csr(CVMX_CIU2_INTR_CIU_READY);
-}
-
-#ifdef CONFIG_SMP
-static int octeon_msi_ciu2_set_affinity(struct irq_data *data,
-					const struct cpumask *dest,
-					bool force)
-{
-	int cpu = cpumask_first(dest);
-
-	/*
-	 * For CIU2-MSI, we will allow only single CPU affinity.
-	 * This .
-	 */
-	if (cpumask_weight(dest) != 1)
-		return -EINVAL;
-
-	octeon_msi_ciu2_enable_on_cpu(data->irq, cpu);
-
-	return 0;
-}
-#endif
-
-static struct irq_chip octeon_msi_chip_ciu2 = {
-	.name = "CIU2-MSI",
-	.irq_enable = octeon_msi_ciu2_enable,
-	.irq_disable = octeon_msi_ciu2_disable,
-	.irq_ack = octeon_msi_ciu2_ack,
-	.irq_mask = octeon_msi_ciu2_disable,
-	.irq_unmask = octeon_msi_ciu2_enable,
-#ifdef CONFIG_SMP
-	.irq_set_affinity = octeon_msi_ciu2_set_affinity,
-	.irq_cpu_offline = octeon_irq_cpu_offline_ciu,
-#endif
-};
-
-static void octeon_msi_ip4(void)
-{
-	union cvmx_ciu2_msired_ppx_ip4 msired;
-	int core = cvmx_get_core_num();
-
-	msired.u64 = cvmx_read_csr(CVMX_CIU2_MSIRED_PPX_IP4(core));
-
-	if (msired.s.intr)
-		do_IRQ(msired.s.msi_num + OCTEON_IRQ_MSI_BIT0);
-	else
-		spurious_interrupt();
-}
-
-static int octeon_msi_cpu_callback(struct notifier_block *nfb,
-				   unsigned long action, void *hcpu)
-{
-	unsigned int cpu = (unsigned long)hcpu;
-
-	switch (action) {
-	case CPU_DOWN_PREPARE:
-		cvmx_write_csr(CVMX_CIU2_EN_PPX_IP4_IO_W1C(cpu_logical_map(cpu)),
-			       1<<12);
-		break;
-	case CPU_ONLINE:
-	case CPU_DOWN_FAILED:
-		cvmx_write_csr(CVMX_CIU2_EN_PPX_IP4_IO_W1S(cpu_logical_map(cpu)),
-			       1<<12);
-		break;
-	default:
-		break;
-	}
-	return NOTIFY_OK;
-}
-
-static struct notifier_block octeon_msi_cpu_notifier = {
-	.notifier_call = octeon_msi_cpu_callback,
-};
-
-static int __init octeon_msi_68XX_init(void)
-{
-	int i;
-	int cpu;
-
-	/* Disable legacy handling. */
-	cvmx_write_csr(CVMX_PEXP_SLI_MSI_ENB0, 0);
-	cvmx_write_csr(CVMX_PEXP_SLI_MSI_ENB1, 0);
-	cvmx_write_csr(CVMX_PEXP_SLI_MSI_ENB2, 0);
-	cvmx_write_csr(CVMX_PEXP_SLI_MSI_ENB3, 0);
-
-	/* Disable CIU2_MSI */
-	for (i = 0; i < 256; i++)
-		cvmx_write_csr(CVMX_CIU2_MSI_SELX(i), 0);
-
-	for (i = OCTEON_IRQ_MSI_BIT0; i <= OCTEON_IRQ_MSI_LAST; i++)
-		irq_set_chip_and_handler(i, &octeon_msi_chip_ciu2, handle_edge_irq);
-
-	octeon_irq_set_ip4_handler(octeon_msi_ip4);
-	/* Enable MSIRED interrupt */
-#ifdef CONFIG_SMP
-	for_each_online_cpu(cpu)
-		cvmx_write_csr(CVMX_CIU2_EN_PPX_IP4_IO_W1S(cpu_logical_map(cpu)),
-			       1<<12);
-#else
-	cvmx_write_csr(CVMX_CIU2_EN_PPX_IP4_IO_W1S(cvmx_get_core_num()),
-			       1<<12);
-#endif
-	cvmx_read_csr(CVMX_CIU2_SUM_PPX_IP2(cvmx_get_core_num()));
-
-	register_hotcpu_notifier(&octeon_msi_cpu_notifier);
-
-	msi_irq_size = 256;
-	return 0;
-}
-#endif /* 0 */
-
 static void octeon_irq_msi_ciu3_ack(struct irq_data *data)
 {
 	u64 csr_addr;
@@ -571,9 +478,7 @@ struct irq_domain_ops octeon_msi_domain_ciu3_ops = {
  */
 int __init octeon_msi_initialize(void)
 {
-	int irq;
 	struct irq_domain *domain;
-	struct irq_chip *msi;
 	u64 msi_map_reg;
 	int i;
 	int node = 0; /* Must use correct node. TODO */
@@ -595,10 +500,6 @@ int __init octeon_msi_initialize(void)
 		msi_rcv_reg[3] = CVMX_PEXP_SLI_MSI_RCV3;
 		return 0;
 	}
-#if 0
-	if (OCTEON_IS_MODEL(OCTEON_CN68XX) && !OCTEON_IS_MODEL(OCTEON_CN68XX_PASS1_X))
-		return octeon_msi_68XX_init();
-#endif
 
 	if (octeon_dma_bar_type == OCTEON_DMA_BAR_TYPE_PCIE2) {
 		msi_rcv_reg[0] = CVMX_PEXP_SLI_MSI_RCV0;
@@ -609,9 +510,8 @@ int __init octeon_msi_initialize(void)
 		msi_ena_reg[1] = CVMX_PEXP_SLI_MSI_ENB1;
 		msi_ena_reg[2] = CVMX_PEXP_SLI_MSI_ENB2;
 		msi_ena_reg[3] = CVMX_PEXP_SLI_MSI_ENB3;
-		msi = &octeon_irq_chip_msi_pcie;
-		octeon_irq_msi_to_irq = octeon_irq_msi_to_irq_scatter;
-		octeon_irq_irq_to_msi = octeon_irq_irq_to_msi_scatter;
+		octeon_irq_msi_to_hwmsi = octeon_irq_msi_to_hwmsi_scatter;
+		octeon_irq_hwmsi_to_msi = octeon_irq_hwmsi_to_msi_scatter;
 		msi_map_reg = CVMX_PEXP_SLI_MSI_WR_MAP;
 	} else if (octeon_dma_bar_type == OCTEON_DMA_BAR_TYPE_PCIE) {
 		msi_rcv_reg[0] = CVMX_PEXP_NPEI_MSI_RCV0;
@@ -622,9 +522,8 @@ int __init octeon_msi_initialize(void)
 		msi_ena_reg[1] = CVMX_PEXP_NPEI_MSI_ENB1;
 		msi_ena_reg[2] = CVMX_PEXP_NPEI_MSI_ENB2;
 		msi_ena_reg[3] = CVMX_PEXP_NPEI_MSI_ENB3;
-		msi = &octeon_irq_chip_msi_pcie;
-		octeon_irq_msi_to_irq = octeon_irq_msi_to_irq_scatter;
-		octeon_irq_irq_to_msi = octeon_irq_irq_to_msi_scatter;
+		octeon_irq_msi_to_hwmsi = octeon_irq_msi_to_hwmsi_scatter;
+		octeon_irq_hwmsi_to_msi = octeon_irq_hwmsi_to_msi_scatter;
 		msi_map_reg = CVMX_PEXP_NPEI_MSI_WR_MAP;
 	} else {
 		msi_rcv_reg[0] = CVMX_NPI_NPI_MSI_RCV;
@@ -636,9 +535,8 @@ int __init octeon_msi_initialize(void)
 		msi_ena_reg[1] = INVALID_GENERATE_ADE;
 		msi_ena_reg[2] = INVALID_GENERATE_ADE;
 		msi_ena_reg[3] = INVALID_GENERATE_ADE;
-		msi = &octeon_irq_chip_msi_pci;
-		octeon_irq_msi_to_irq = octeon_irq_msi_to_irq_linear;
-		octeon_irq_irq_to_msi = octeon_irq_irq_to_msi_linear;
+		octeon_irq_msi_to_hwmsi = octeon_irq_msi_to_hwmsi_linear;
+		octeon_irq_hwmsi_to_msi = octeon_irq_hwmsi_to_msi_linear;
 		msi_map_reg = 0;
 	}
 
@@ -654,9 +552,6 @@ int __init octeon_msi_initialize(void)
 		}
 	}
 
-	for (irq = OCTEON_IRQ_MSI_BIT0; irq <= OCTEON_IRQ_MSI_LAST; irq++)
-		irq_set_chip_and_handler(irq, msi, handle_simple_irq);
-
 	if (octeon_has_feature(OCTEON_FEATURE_PCIE)) {
 		if (request_irq(OCTEON_IRQ_PCI_MSI0, octeon_msi_interrupt0,
 				IRQF_NO_THREAD, "MSI[0:63]", octeon_msi_interrupt0))
@@ -673,8 +568,6 @@ int __init octeon_msi_initialize(void)
 		if (request_irq(OCTEON_IRQ_PCI_MSI3, octeon_msi_interrupt3,
 				IRQF_NO_THREAD, "MSI[192:255]", octeon_msi_interrupt3))
 			panic("request_irq(OCTEON_IRQ_PCI_MSI3) failed");
-
-		msi_irq_size = 256;
 	} else if (octeon_is_pci_host()) {
 		if (request_irq(OCTEON_IRQ_PCI_MSI0, octeon_msi_interrupt0,
 				IRQF_NO_THREAD, "MSI[0:15]", octeon_msi_interrupt0))
@@ -691,7 +584,6 @@ int __init octeon_msi_initialize(void)
 		if (request_irq(OCTEON_IRQ_PCI_MSI3, octeon_msi_interrupt0,
 				IRQF_NO_THREAD, "MSI[48:63]", octeon_msi_interrupt0))
 			panic("request_irq(OCTEON_IRQ_PCI_MSI3) failed");
-		msi_irq_size = 64;
 	}
 	return 0;
 }
-- 
1.8.2.1

