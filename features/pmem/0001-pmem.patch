From 3abd4ef43f2fc191f0e296e0ee4a1b7874481846 Mon Sep 17 00:00:00 2001
From: Yongli He <yongli.he@windriver.com>
Date: Wed, 3 Sep 2008 16:57:48 +0800
Subject: [PATCH] pmem

Provide a framework for storing information in non-volatile memory.
A simulation mode uses RAM instead of non-volatile memory.

There is a true persistent mode for certain boards that have hardware
support for it.  The current board list is ATCA-6101, ATCA-f101, ATCA-7101, ATCA-717.

For user space, a file system interface is provided by the PMEMFS pseudo-filesystem.
This filesystem will not be visible in menuconfig or xconfig unless PMEM is enabled.
When PMEM is enabled, PMEMFS is included as a module by default.

Signed-off-by: Yongli he   <yongli.he@windriver.com>
Signed-off-by: MacDonald, Joe <Joe.MacDonald@windriver.com>
index a69e034..bbe14e3 100644
---
 arch/powerpc/include/asm/reg.h    |    5 +
 arch/powerpc/include/asm/system.h |    3 +
 arch/powerpc/kernel/misc.S        |   56 +
 arch/powerpc/kernel/pci-common.c  |    6 +-
 arch/powerpc/mm/init_32.c         |   13 +
 arch/powerpc/platforms/Makefile   |    1 +
 arch/x86/Kconfig                  |    6 +
 arch/x86/mm/init_32.c             |    8 +
 arch/x86/pci/i386.c               |    1 +
 drivers/Makefile                  |    1 +
 drivers/pmem/Makefile             |    4 +
 drivers/pmem/pmem_init.c          | 1162 ++++++++++++++
 drivers/pmem/pmem_pci.c           |  327 ++++
 fs/Kconfig                        |    8 +
 fs/Makefile                       |    1 +
 fs/pmemfs/Makefile                |   10 +
 fs/pmemfs/inode.c                 | 2992 +++++++++++++++++++++++++++++++++++++
 fs/proc/proc_misc.c               |    3 +
 include/linux/pmem.h              |  778 ++++++++++
 include/linux/schedhist.h         |   95 ++
 init/Kconfig                      |   16 +
 init/main.c                       |    6 +
 kernel/sched.c                    |   80 +
 mm/Makefile                       |    1 +
 mm/pmem/Kconfig                   |  129 ++
 mm/pmem/Makefile                  |    5 +
 mm/pmem/cmds.c                    | 1077 +++++++++++++
 mm/pmem/handle.c                  |  473 ++++++
 mm/pmem/io.c                      |  605 ++++++++
 mm/pmem/reg.c                     |  725 +++++++++
 30 files changed, 8596 insertions(+), 1 deletions(-)
 create mode 100644 drivers/pmem/Makefile
 create mode 100644 drivers/pmem/pmem_init.c
 create mode 100644 drivers/pmem/pmem_pci.c
 create mode 100644 fs/pmemfs/Makefile
 create mode 100644 fs/pmemfs/inode.c
 create mode 100644 include/linux/pmem.h
 create mode 100644 include/linux/schedhist.h
 create mode 100644 mm/pmem/Kconfig
 create mode 100644 mm/pmem/Makefile
 create mode 100644 mm/pmem/cmds.c
 create mode 100644 mm/pmem/handle.c
 create mode 100644 mm/pmem/io.c
 create mode 100644 mm/pmem/reg.c

diff --git a/arch/powerpc/include/asm/reg.h b/arch/powerpc/include/asm/reg.h
index c6d1ab6..ad4c240 100644
--- a/arch/powerpc/include/asm/reg.h
+++ b/arch/powerpc/include/asm/reg.h
@@ -624,6 +624,11 @@
 #define MTFSF_L(REG)	mtfsf	0xff, (REG)
 #endif
 
+#ifdef CONFIG_UCACHE
+#define MSSCR0         1014    /* Memory susbsystem control register */
+#define MSSCR0_DL1HWF  (1<<23) /* L1 data cache hardware flush */
+#endif
+
 /* Processor Version Register (PVR) field extraction */
 
 #define PVR_VER(pvr)	(((pvr) >>  16) & 0xFFFF)	/* Version field */
diff --git a/arch/powerpc/include/asm/system.h b/arch/powerpc/include/asm/system.h
index d6648c1..d40f5d0 100644
--- a/arch/powerpc/include/asm/system.h
+++ b/arch/powerpc/include/asm/system.h
@@ -115,6 +115,9 @@ extern void do_dabr(struct pt_regs *regs, unsigned long address,
 extern void print_backtrace(unsigned long *);
 extern void show_regs(struct pt_regs * regs);
 extern void flush_instruction_cache(void);
+extern void flush_data_cache(void);
+extern void flush_data_cache_soft(void);
+extern void flush_data_cache_hard(void);
 extern void hard_reset_now(void);
 extern void poweroff_now(void);
 
diff --git a/arch/powerpc/kernel/misc.S b/arch/powerpc/kernel/misc.S
index 85cb6f3..b908e11 100644
--- a/arch/powerpc/kernel/misc.S
+++ b/arch/powerpc/kernel/misc.S
@@ -19,6 +19,7 @@
 #include <asm/unistd.h>
 #include <asm/asm-compat.h>
 #include <asm/asm-offsets.h>
+#include <asm/page.h>
 
 	.text
 
@@ -116,6 +117,61 @@ _GLOBAL(longjmp)
 	mtlr	r0
 	mr	r3,r4
 	blr
+/*
+ * Flush the L1 data cache by reading the first 64kB of RAM
+ * and then flushing the same area with the dcbf instruction.
+ * The L2 cache has already been disabled.
+ */
+_GLOBAL(flush_data_cache)
+	li	r4,0x0800	/* 64kB / 32B */
+	mtctr	r4
+	lis	r4,KERNELBASE@h
+1:
+	lwz	r0,0(r4)
+	addi	r4,r4,0x0020	/* Go to start of next cache line */
+	bdnz	1b
+	sync
+
+	li	r4,0x0800	/* 64k */
+	mtctr	r4
+	lis	r4,KERNELBASE@h
+1:
+	dcbf	r0,r4
+	addi	r4,r4,0x0020	/* Go to start of next cache line */
+	bdnz	1b
+	sync
+10:	blr
+
+#ifdef CONFIG_UCACHE
+/*
+ *	flush_data_cache_soft:
+ *		Flush the L1 data cache by loading the
+ *	first 64Kb from the kernel base, then flushing
+ *	it.
+ */
+
+_GLOBAL(flush_data_cache_soft)
+	b	flush_data_cache
+
+
+/*
+ *	flush_data_cache_hard:
+ *		Flush the L1 data cache using the 7400
+ *	hardware flush parameter in MSSCR0.
+ */
+
+_GLOBAL(flush_data_cache_hard)
+	mfspr	r3,MSSCR0
+	oris	r3,r3,MSSCR0_DL1HWF@h
+
+	sync
+	mtspr	MSSCR0,r3
+
+	sync
+	blr
+
+#endif /* CONFIG_UCACHE */
+
 
 _GLOBAL(__setup_cpu_power7)
 _GLOBAL(__restore_cpu_power7)
diff --git a/arch/powerpc/kernel/pci-common.c b/arch/powerpc/kernel/pci-common.c
index ea0c61e..78acf1f 100644
--- a/arch/powerpc/kernel/pci-common.c
+++ b/arch/powerpc/kernel/pci-common.c
@@ -316,8 +316,11 @@ static struct resource *__pci_mmap_make_offset(struct pci_dev *dev,
 			continue;
 
 		/* found it! construct the final physical address */
-		if (mmap_state == pci_mmap_io)
+		if (mmap_state == pci_mmap_io) {
+			printk(KERN_INFO "%s (%d): offset %lx\n", __func__, __LINE__, offset);
 			*offset += hose->io_base_phys - io_offset;
+			printk(KERN_INFO "%s (%d): offset %lx\n", __func__, __LINE__, offset);
+		}
 		return rp;
 	}
 
@@ -440,6 +443,7 @@ int pci_mmap_page_range(struct pci_dev *dev, struct vm_area_struct *vma,
 
 	return ret;
 }
+EXPORT_SYMBOL(pci_mmap_page_range);
 
 void pci_resource_to_user(const struct pci_dev *dev, int bar,
 			  const struct resource *rsrc,
diff --git a/arch/powerpc/mm/init_32.c b/arch/powerpc/mm/init_32.c
index 4ac0e4e..f4cb986 100644
--- a/arch/powerpc/mm/init_32.c
+++ b/arch/powerpc/mm/init_32.c
@@ -45,6 +45,10 @@
 #include <asm/sections.h>
 #include <asm/system.h>
 
+#if defined CONFIG_PMEM
+#include <linux/pmem.h>
+#endif
+
 #include "mmu_decl.h"
 
 #if defined(CONFIG_KERNEL_START_BOOL) || defined(CONFIG_LOWMEM_SIZE_BOOL)
@@ -113,6 +117,15 @@ void MMU_setup(void)
 #endif
 }
 
+ 
+#if defined CONFIG_PMEM
+unsigned long pmem_arch_pgprot_noncached(unsigned long prot)
+{
+ 	prot |= _PAGE_NO_CACHE | _PAGE_GUARDED;
+ 	return prot;
+}
+#endif
+ 
 /*
  * MMU_init sets up the basic memory mappings for the kernel,
  * including both RAM and possibly some I/O regions,
diff --git a/arch/powerpc/platforms/Makefile b/arch/powerpc/platforms/Makefile
index 8079e0b..1c5ec93 100644
--- a/arch/powerpc/platforms/Makefile
+++ b/arch/powerpc/platforms/Makefile
@@ -19,3 +19,4 @@ obj-$(CONFIG_PPC_PASEMI)	+= pasemi/
 obj-$(CONFIG_PPC_CELL)		+= cell/
 obj-$(CONFIG_PPC_PS3)		+= ps3/
 obj-$(CONFIG_EMBEDDED6xx)	+= embedded6xx/
+ 
diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
index a50d3e1..bfa26cb 100644
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -982,6 +982,12 @@ config X86_64_ACPI_NUMA
 	help
 	  Enable ACPI SRAT based node topology detection.
 
+config INTEL_ATCA7101
+         bool "Intel ATCA-7101 Blade"
+         default n
+  	 help
+ 	  Enable this option for running on the Motorola ATCA 7101
+
 # Some NUMA nodes have memory ranges that span
 # other nodes.  Even though a pfn is valid and
 # between a node's start and end pfns, it may not
diff --git a/arch/x86/mm/init_32.c b/arch/x86/mm/init_32.c
index ed45162..3830fe7 100644
--- a/arch/x86/mm/init_32.c
+++ b/arch/x86/mm/init_32.c
@@ -66,6 +66,14 @@ static unsigned long __meminitdata table_top;
 
 static int __initdata after_init_bootmem;
 
+#ifdef CONFIG_PMEM
+unsigned long pmem_arch_pgprot_noncached(unsigned long prot)
+{
+	prot |= _PAGE_PCD | _PAGE_PWT;
+	return prot;
+}
+#endif	/* CONFIG_PMEM */
+
 static __init void *alloc_low_page(unsigned long *phys)
 {
 	unsigned long pfn = table_end++;
diff --git a/arch/x86/pci/i386.c b/arch/x86/pci/i386.c
index 8791fc5..b081562 100644
--- a/arch/x86/pci/i386.c
+++ b/arch/x86/pci/i386.c
@@ -345,3 +345,4 @@ int pci_mmap_page_range(struct pci_dev *dev, struct vm_area_struct *vma,
 
 	return 0;
 }
+EXPORT_SYMBOL(pci_mmap_page_range);
diff --git a/drivers/Makefile b/drivers/Makefile
index 39cfe40..ed2e1df 100644
--- a/drivers/Makefile
+++ b/drivers/Makefile
@@ -93,6 +93,7 @@ obj-$(CONFIG_INFINIBAND)	+= infiniband/
 obj-$(CONFIG_SGI_SN)		+= sn/
 obj-y				+= firmware/
 obj-$(CONFIG_CRYPTO)		+= crypto/
+obj-$(CONFIG_PMEM)		+= pmem/
 obj-$(CONFIG_SUPERH)		+= sh/
 obj-$(CONFIG_GENERIC_TIME)	+= clocksource/
 obj-$(CONFIG_DMA_ENGINE)	+= dma/
diff --git a/drivers/pmem/Makefile b/drivers/pmem/Makefile
new file mode 100644
index 0000000..8df36a9
--- /dev/null
+++ b/drivers/pmem/Makefile
@@ -0,0 +1,4 @@
+ifeq ($(CONFIG_PMEM_PCI_DRIVER),y)
+obj-y	+= pmem_pci.o
+endif
+obj-y	+= pmem_init.o
diff --git a/drivers/pmem/pmem_init.c b/drivers/pmem/pmem_init.c
new file mode 100644
index 0000000..6576d74
--- /dev/null
+++ b/drivers/pmem/pmem_init.c
@@ -0,0 +1,1162 @@
+/*
+ * Pmem initialization routines.
+ *
+ * These are broken out into the drivers/pmem tree to allow them to be
+ * linked into the kernel after the PCI drivers.  The PCI drivers need to 
+ * be able to find the memory to be used by pmem and then the initialization
+ * can occur.
+ *
+ * This initialization code used to be mm/pmem/init.c and init/main.c
+ *
+ * WindRiver Systems
+ * September 2005
+ */
+
+#include <linux/string.h>
+#include <linux/errno.h>
+#include <linux/slab.h>
+#include <linux/vmalloc.h>
+#include <linux/spinlock.h>
+#include <linux/kernel.h>
+#include <linux/time.h>
+#include <linux/module.h>
+#include <linux/pmem.h>
+#include <linux/zlib.h>
+#include <linux/zutil.h>
+#include <linux/bootmem.h>	/* alloc_bootmem_pages() */
+#include <linux/init.h>		/* __setup */
+#include <linux/notifier.h>
+#include <linux/reboot.h>	/* register_reboot_notifier() */
+#include <linux/mm.h>
+#include <linux/sched.h>
+
+#include <asm/types.h>
+
+#ifdef CONFIG_SCHED_HIST_BUF
+#include <linux/schedhist.h>
+extern struct pmem_ptr_block sched_hist_pmem_block;
+#endif /* CONFIG_SCHED_HIST_BUF */
+
+/* Allow the kernel command line to force a pmem format */
+static int __initdata pmem_force_clear = 0;
+
+static int __init pmem_force_clear_setup(char *str)
+{
+	pmem_force_clear = 1;
+	return 1;
+}
+__setup("pmem_force_clear", pmem_force_clear_setup);
+
+/* the Global pmem structure */
+struct pmem_cb_data pmem = {
+	.pmem 		= NULL,
+	.size 		= 0,
+	.lock	 	= SPIN_LOCK_UNLOCKED,
+	.shadow		= NULL,
+	.enabled	= 0,
+	.using_hardware	= 0,
+	.using_io_mem   = 0,
+};
+EXPORT_SYMBOL(pmem);
+
+/* Reboot notification block for pmem */
+#ifdef CONFIG_PMEM_HARDWARE
+static int pmem_reboot_handler(struct notifier_block *this, 
+                               unsigned long code, void *unused);
+static struct notifier_block pmem_reboot_notifier = {
+	pmem_reboot_handler,
+	NULL,
+	99
+};
+#endif
+
+
+/* initialize the callbacks to empty */
+struct pmem_event_block pmem_events = {
+	.create_partition	= &pmem_default_create_partition,
+	.create_region		= &pmem_default_create_region,
+};
+EXPORT_SYMBOL(pmem_events);
+
+struct persistent_control{
+	unsigned long magic;
+	unsigned long reboot_chksum;
+	unsigned long reboot_reason;
+	unsigned long reboot_counter;
+	unsigned long test;
+	unsigned long trigger0;
+	unsigned long trigger1;
+};
+
+#define PERSISTENT_CONTROL_MAGIC 		0xdeedbeef
+#define PERSISTENT_CONTROL_COUNTER_MASK		0xffff0000
+#define PERSISTENT_CONTROL_COUNTER_VALUE	0x12340000
+
+#define MR_CLEAN_CONTROL(persistent_control_st)	do{\
+	persistent_control_st->reboot_counter=PERSISTENT_CONTROL_COUNTER_VALUE;\
+	persistent_control_st->test=0;\
+	persistent_control_st->magic=0;\
+	persistent_control_st->reboot_reason=0;\
+	persistent_control_st->reboot_chksum=0;\
+}while(0)
+
+#define CLEAN_CONTROL(persistent_control_st)	do{\
+	persistent_control_st->reboot_reason=0;\
+	persistent_control_st->reboot_chksum=0;\
+}while(0)
+
+#define INIT_CONTROL(persistent_control_st)	do{\
+	persistent_control_st->reboot_counter=PERSISTENT_CONTROL_COUNTER_VALUE;\
+	persistent_control_st->magic=PERSISTENT_CONTROL_MAGIC;\
+}while(0)
+
+/* Returns true if the two areas overlap */
+int pmem_is_overlapping(__u32 offset_a, __u32 size_a, __u32 offset_b,
+		__u32 size_b)
+{
+	/* Check if above */
+	if (offset_a + size_a <= offset_b)
+		return 0;
+	/* Check if below */
+	if (offset_b + size_b <= offset_a)
+		return 0;
+
+	/* Above are false therefore it must overlap */
+	return 1;
+}
+
+/* Validate the data in the allocation table entry */
+static int pmem_validate_alloc_entry(int alloc_index)
+{
+	struct pmem_alloc_desc *alloc_desc;
+	int rc = 0;
+
+	alloc_desc = PMEM_GET_ALLOC_DESC(alloc_index);
+
+	if (PMEM_VALIDATE_CHECKSUM(alloc_desc) < 0) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	/* Range check the offset. Ensure that it is in persistent memory */
+	if ((alloc_desc->data.offset + alloc_desc->data.size) > pmem.size) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	/* Ensure that it is not inside the allocation table */
+	if (alloc_desc->data.offset <
+	    (sizeof (struct pmem_cb_hdr) +
+	     (pmem.pmem->data.num_alloc * sizeof (struct pmem_alloc_desc)))) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	switch (alloc_desc->data.type) {
+	case PMEM_ALLOC_TYPE_LOG:
+		break;
+	default:
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	PMEM_DPRINT("Partition: desc=%s, offset=%d, size=%d, type=%d\n",
+		    alloc_desc->data.desc, alloc_desc->data.offset,
+		    alloc_desc->data.size, alloc_desc->data.type);
+
+      done:
+	return (rc);
+}
+
+
+static int pmem_validate_block(struct pmem_part_hdr *part_hdr,
+		    struct pmem_block_hdr *block_hdr)
+{
+	int rc = 0;
+
+	if (PMEM_VALIDATE_CHECKSUM(block_hdr) < 0) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	if (block_hdr->data.size == 0) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	if (block_hdr->data.flags >> PMEM_BLOCK_NUM_FLAGS) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	/* Range check the offset. Ensure it is inside the partition data */
+	if (block_hdr->data.offset < PMEM_PART_HDR_MAX_SIZE) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	if ((block_hdr->data.offset + block_hdr->data.size) >
+	    (part_hdr->data.size + PMEM_PART_HDR_MAX_SIZE)) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	PMEM_DPRINT("Log block: hdr_offset=%ld, offset=%d, size=%d, flags=%d\n",
+		    (unsigned long) block_hdr - (unsigned long) pmem.pmem,
+		    block_hdr->data.offset, block_hdr->data.size,
+		    block_hdr->data.flags);
+
+      done:
+	return (rc);
+}
+
+static int pmem_validate_region(struct pmem_part_hdr *part_hdr,
+		struct pmem_region_hdr *region_hdr)
+{
+	struct pmem_block_hdr *block_hdr;
+	int rc = 0;
+
+	if (PMEM_VALIDATE_CHECKSUM(region_hdr) < 0) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	if (region_hdr->data.size == 0) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	if (region_hdr->data.flags >> PMEM_REGION_NUM_FLAGS) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	/* Get any block since they all have the same size information */
+	block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, 0);
+
+	/* Range check the offset. Ensure it is inside the block */
+	if ((region_hdr->data.offset + region_hdr->data.size) >
+	    block_hdr->data.size) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	PMEM_DPRINT
+	    ("Log region: desc=%s, offset=%d, size=%d, flags=%d, fixed_size=%d, num_log_desc=%d\n",
+	     region_hdr->data.desc, region_hdr->data.offset,
+	     region_hdr->data.size, region_hdr->data.flags,
+	     region_hdr->data.fixed_size, region_hdr->data.num_log_desc);
+
+      done:
+	return (rc);
+}
+
+#ifdef CONFIG_PMEM_HARDWARE
+/*
+ * There is 3 way to take the system down. Power down, Spurious reset and Controlled 
+ * reboot.
+ * - Power down wipe off all persistent memory.
+ * - Spurious reset can be WatchDog or Blade reset initiated by another processor 
+ *   (Ex 405 reboot...).  The Spurious reset conserve persistent memory, in the case
+ *   of the 280, some special trick need to be used to tell the Firmware todo so.
+ * - Controlled reboot can be a reboot cmd (system call), panic, bug, oops...
+ *
+ * The following code try to determine why the system went down and if the reason is
+ * a Controlled reboot then a Chksum is performed and match against the control 
+ * information.
+ * That functionality will detect a possible Firmware memory trampler over the pmem...
+ * On a Controlled reboot, machine_restart() or machine_power_off() or machine_halt() 
+ * are used.  So here the idea is simple, in those above function simply update the 
+ * control information to know at the next boot time where we come from.
+ *
+ * On Neptune platform the Persistent memory is not  entirely validated. With the help
+ * of this code {reboot counter, magic value and trigger} there should be enough to 
+ * move forward...
+ *
+ * The Check Sum is the whole persistent memory except over the LOGBUFFER as it may 
+ * change at boot/reset time i.e. modified before this code run from a Reset/Reboot
+ *
+ * On the 280 the only way to add persistent storage is at the bottom.
+ * On 3PC the Firmware size is persistent, let use some of it.
+ * If the bottom of the Firmware size is corrupted, chance are the pmem is also 
+ * corrupted!  mem_pieces_remove add PERSISTENT_CONTROL_SIZE bytes to
+ * the bottom and the top of the Persistent storage to handle 280 and TPC and the 
+ * same time
+ * -etiennem
+ */
+
+static int persistent_mem_crc(int reason)
+{
+	static int init=0;
+	static char *persistent_mem = NULL ;
+	static struct persistent_control *persistent_control_st =NULL;
+	uLong adler = zlib_adler32(0L, NULL, 0);
+	static int pmem_size = 0;
+	struct part_list_elem	*part_elem;
+	struct list_head	*elem, *temp;
+	pmem_handle_t part_handle = NULL;
+	int i;
+
+	if(!init){
+		/* Acquire architecture specific pointers */
+		persistent_mem = (char*)pmem_arch_get_start_ptr();
+		if(!persistent_mem){
+			printk(KERN_ERR "Cannot get persistent_mem pointer !\n");
+			return -1;
+		}
+		persistent_control_st = (struct persistent_control*)pmem_arch_get_checksum_start_ptr();
+		if(!persistent_control_st){
+			printk(KERN_ERR "Cannot get persistent_control_st !\n");
+			return -1;
+		}
+
+		pmem_size = pmem_arch_get_size();
+		if (pmem_size <= 0) {
+			printk(KERN_ERR "Cant get pmem size from arch driver\n");
+			return -1;
+		}
+		init=1;
+
+	} else {
+		/* Stop the kernel users from writing to persistent memory
+		 * so that the checksum is valid */
+		list_for_each_safe(elem, temp, &pmem.part_list) {
+			part_elem = list_entry(elem, struct part_list_elem, 
+			                       list_elem);
+			if (pmem_partition_get(part_elem->hdr->data.desc, &part_handle) != 0) {
+				PMEM_DPRINT("ERROR: Cant get handle for part [%s]\n", part_elem->hdr->data.desc);
+				continue;
+			}
+			if (pmem_disable_ptr_blocks(part_handle) < 0) {
+				printk(KERN_ERR "Failed to disable pointer block for partition [%s]\n", part_elem->hdr->data.desc);
+			} else {
+#ifdef CONFIG_PMEM_DEBUG
+				printk(KERN_INFO "Kernel pmem pointer blocks disabled for [%s]\n", part_elem->hdr->data.desc);
+#endif
+			}
+			pmem_release_handle(&part_handle);
+		} /* for each pmem partition */
+		pmem.enabled = 0;
+	}
+	if (pmem_size % PAGE_SIZE != 0) {
+		printk(KERN_WARNING "PMEM: Warning - size not multiple of PAGE_SIZE\n");
+	}
+        /*Compute checksum -- do it in blocks so that the soft lockup
+	 detection doesn't trip and belch complaints... */
+	printk(KERN_INFO "PMEM CRC starting (this may take a while)\n");
+	for (i = 0 ; i < pmem_size/PAGE_SIZE ; i++) {
+		adler = zlib_adler32(adler, persistent_mem + (i*PAGE_SIZE), PAGE_SIZE);
+		schedule();
+	}
+	printk(KERN_INFO "Persistent Memory chkSum %lx\n",adler);
+
+	switch(reason){
+	/*****BOOT-UP time*****/
+	case PERSISTENT_CONTROL_BOOTUP:/*Boot-up, Let's check against the Last reset value*/
+#if 0
+		/*Visual Check*/
+		printk("Persistent Memory test %lx\n",persistent_control_st->test);
+		persistent_control_st->test=0x11223344;
+		printk("After Persistent Memory test %lx\n",persistent_control_st->test);
+#endif
+		/*Trigger use by UserLand dd if=/dev/mem bs=1M | od -t x 4 |grep 0x40414243
+		 * or the ASCII string "ABCDEFGH"*/
+		persistent_control_st->trigger0=0x41424344;
+		persistent_control_st->trigger1=0x45464748;
+
+		if(persistent_control_st->magic != PERSISTENT_CONTROL_MAGIC){/*Nothing to do here*/
+			printk(KERN_INFO "Persistent Memory Power Down\n");
+			/*Should also clear Pmem as well, later...*/
+			MR_CLEAN_CONTROL(persistent_control_st);
+			INIT_CONTROL(persistent_control_st);
+			return 0;
+		}
+		switch(persistent_control_st->reboot_reason){
+		case PERSISTENT_CONTROL_RESTART:
+		case PERSISTENT_CONTROL_HALT:
+		case PERSISTENT_CONTROL_PWR_OFF:
+			printk(KERN_INFO "Persistent Memory Controlled reboot\n");
+			if((persistent_control_st->reboot_counter & PERSISTENT_CONTROL_COUNTER_MASK)==PERSISTENT_CONTROL_COUNTER_VALUE){
+				printk(KERN_INFO "Persistent Memory reboot_counter=%ld\n",persistent_control_st->reboot_counter &~PERSISTENT_CONTROL_COUNTER_MASK);
+				persistent_control_st->reboot_counter++;
+			}
+			if(persistent_control_st->reboot_chksum != adler){
+				printk(KERN_INFO "WARNING Persistent Memory corrupted : new=%lx, old=%lx\n", persistent_control_st->reboot_chksum, adler);
+			}
+			CLEAN_CONTROL(persistent_control_st);
+			break;
+		default:
+			printk(KERN_INFO "Persistent Memory Spurious reset\n");
+			if((persistent_control_st->reboot_counter & PERSISTENT_CONTROL_COUNTER_MASK)==PERSISTENT_CONTROL_COUNTER_VALUE){
+				printk(KERN_INFO "Persistent Memory reboot_counter=%ld\n",persistent_control_st->reboot_counter &~PERSISTENT_CONTROL_COUNTER_MASK);
+				persistent_control_st->reboot_counter++;
+			}
+			printk(KERN_INFO "WARNING Persistent Memory NO chkSum validation\n");
+			/*This is the case for Neptune. 405 reset the 280*/
+			CLEAN_CONTROL(persistent_control_st);
+			break;
+		}
+		break;
+
+	/*****Controlled reboot time*****/
+	case PERSISTENT_CONTROL_RESTART:
+	case PERSISTENT_CONTROL_HALT:
+	case PERSISTENT_CONTROL_PWR_OFF:
+		persistent_control_st->reboot_reason = reason;
+		persistent_control_st->reboot_chksum = adler;
+		break;
+	default:
+		BUG();
+	}
+	PMEM_FLUSH_CACHE(persistent_control_st, sizeof(struct persistent_control));
+
+	return 0;
+}
+#endif /* CONFIG_PMEM_HARDWARE */
+
+static int pmem_validate_partition(struct pmem_alloc_desc *alloc_desc)
+{
+	struct pmem_part_hdr *part_hdr;
+	struct pmem_block_hdr *block_hdr;
+	struct pmem_region_hdr *region_hdr;
+	__u32 block_size = 0;
+	__u32 newest_sec = 0;
+	__u32 newest_usec = 0;
+	int index = 0;
+	int block_active = 0;
+	int newest_block_index = 0;
+	int rc = 0;
+
+	if (alloc_desc->data.size < sizeof (struct pmem_part_hdr)) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	part_hdr = PMEM_GET_PART_HDR(alloc_desc);
+
+	if (PMEM_VALIDATE_CHECKSUM(part_hdr) < 0) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	if (part_hdr->data.size !=
+			alloc_desc->data.size - PMEM_PART_HDR_MAX_SIZE) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	if (strncmp(part_hdr->data.desc, alloc_desc->data.desc,
+	            PMEM_DESC_MAX)) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	if (part_hdr->data.flags != 0) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	PMEM_DPRINT("Log partition: desc=%s, size=%d, blocks=%d, regions=%d\n",
+		    part_hdr->data.desc, part_hdr->data.size,
+		    part_hdr->data.num_blocks, part_hdr->data.num_regions);
+
+	/* Check that each individual block is valid */
+	for (index = 0; index < part_hdr->data.num_blocks; index++) {
+		block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, index);
+
+		if (pmem_validate_block(part_hdr, block_hdr) < 0) {
+			rc = -1;
+			PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+			goto done;
+		}
+	}
+
+	/* Check that the blocks do not conflict with each other */
+	for (index = 0; index < part_hdr->data.num_blocks; index++) {
+		/* Get the first element for the overlap macro */
+		block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, 0);
+
+		if (PMEM_CHECK_FOR_OVERLAP
+		    (block_hdr, index, part_hdr->data.num_blocks) < 0) {
+			rc = -1;
+			PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+			goto done;
+		}
+
+		block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, index);
+
+		/* Ensure that all block sizes are the same */
+		if (block_size == 0)
+			block_size = block_hdr->data.size;
+		else if (block_hdr->data.size != block_size) {
+			rc = -1;
+			PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+			goto done;
+		}
+
+		/* Ensure that only one block was active */
+		if (block_hdr->data.flags & PMEM_BLOCK_FLAG_ACTIVE) {
+			if (block_active == 0)
+				block_active = 1;
+			else {
+				rc = -1;
+				PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+				goto done;
+			}
+		}
+
+		/* Check for the newest block index */
+		if ((block_hdr->data.sec > newest_sec) &&
+				(block_hdr->data.usec > newest_usec)) {
+			newest_sec = block_hdr->data.sec;
+			newest_usec = block_hdr->data.usec;
+			newest_block_index = index;
+		}
+	}
+
+	/* Check that one block is active */
+	if ((part_hdr->data.num_blocks > 0) && (block_active == 0)) {
+	/* Nothing was active. Just make the newest block active
+	   so that it will get rotated out */
+		PMEM_DPRINT("PMEM: Nothing active. Setting block %d active\n",
+				newest_block_index);
+
+		block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, newest_block_index);
+		block_hdr->data.flags |= PMEM_BLOCK_FLAG_ACTIVE;
+		PMEM_UPDATE_CHECKSUM(block_hdr);
+	}
+
+	/* Check that each individual region is valid */
+	for (index = 0; index < part_hdr->data.num_regions; index++) {
+		region_hdr = PMEM_GET_REGION_HDR(part_hdr, index);
+
+		if (pmem_validate_region(part_hdr, region_hdr) < 0) {
+			rc = -1;
+			PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+			goto done;
+		}
+	}
+
+	/* Check that the regions do not conflict with each other */
+	for (index = 0; index < part_hdr->data.num_regions; index++) {
+		/* Get the first element for the overlap macro */
+		region_hdr = PMEM_GET_REGION_HDR(part_hdr, 0);
+
+		if (PMEM_CHECK_FOR_OVERLAP
+		    (region_hdr, index, part_hdr->data.num_regions) < 0) {
+			rc = -1;
+			PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+			goto done;
+		}
+	}
+
+      done:
+	return (rc);
+}
+
+static int pmem_validate_control_block(void)
+{
+	struct pmem_cb_hdr *pmem_cbh;
+	int desc_index;
+	int rc = 0;
+
+	pmem_cbh = pmem.pmem;
+
+	if (!pmem_cbh) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	if (PMEM_VALIDATE_CB_CHECKSUM(pmem_cbh) < 0) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		PMEM_DPRINT("Chksum should be %d\n", (int)pmem_cbh->checksum);
+		goto done;
+	}
+
+	if (ntohl(pmem_cbh->data.size) != pmem.size) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, size=%d\n", ntohl(pmem_cbh->data.size));
+		goto done;
+	}
+
+	if (pmem_cbh->data.version != PMEM_VERSION_CURRENT) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, version=%d\n", pmem_cbh->data.version);
+		goto done;
+	}
+
+	if (pmem_cbh->data.num_cpus != PMEM_NUM_SUPPORT_CPUS) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, num_cpus=%d\n", pmem_cbh->data.num_cpus);
+		goto done;
+	}
+
+	//PMEM_DPRINT("INFO: Loading pmem with arch=%d, endian=%d\n", PMEM_GET_ARCH(pmem_cbh), PMEM_GET_ENDIANESS(pmem_cbh));
+	if (pmem_cbh->data.arch_info != PMEM_ARCH_INFO) {
+		rc = -1;
+		/* FIXME: Some cases may not be fatal here */
+		PMEM_DPRINT("Validation failed, arch=%d, endianess=%d\n",
+		            PMEM_GET_ARCH(pmem_cbh),
+		            PMEM_GET_ENDIANESS(pmem_cbh));
+		goto done;
+	}
+
+	if (ntohl(pmem_cbh->data.flags) >> PMEM_CB_NUM_FLAGS) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, flags=%d\n", ntohl(pmem_cbh->data.flags));
+		goto done;
+	}
+
+	/* Validate the allocation table */
+
+	/* Check that each individual entry is valid */
+	for (desc_index = 0; desc_index < pmem.pmem->data.num_alloc;
+	     desc_index++) {
+		if (pmem_validate_alloc_entry(desc_index) < 0) {
+			rc = -1;
+			PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+			goto done;
+		}
+	}
+
+	/* Check that the entries do not conflict with each other */
+	for (desc_index = 0; desc_index < pmem.pmem->data.num_alloc;
+	     desc_index++) {
+		/* Get the first element for the overlap macro */
+		struct pmem_alloc_desc *alloc_desc = PMEM_GET_ALLOC_DESC(0);
+
+		/* Ensure that no one else is using the same offsets */
+		if (PMEM_CHECK_FOR_OVERLAP
+		    (alloc_desc, desc_index, pmem.pmem->data.num_alloc) < 0) {
+			rc = -1;
+			PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+			goto done;
+		}
+	}
+
+	PMEM_DPRINT("Control block is valid, num_alloc = %d\n",
+		    pmem.pmem->data.num_alloc);
+
+      done:
+	return (rc);
+}
+
+static int pmem_validate_pmem(void)
+{
+	struct pmem_alloc_desc *alloc_desc;
+	int desc_index;
+
+	if (pmem_validate_control_block() < 0)
+		return -1;
+
+	/* Loop through each partition and check its headers */
+	for (desc_index = 0; desc_index < pmem.pmem->data.num_alloc;
+	     desc_index++) {
+		alloc_desc = PMEM_GET_ALLOC_DESC(desc_index);
+
+		switch (alloc_desc->data.type) {
+		case PMEM_ALLOC_TYPE_LOG:
+			if (pmem_validate_partition(alloc_desc) < 0)
+				return -1;
+			break;
+		default:
+			return -1;
+		}
+
+		PMEM_DPRINT("Done validating partition %d\n", desc_index);
+	}
+
+	return 0;
+}
+
+
+static void pmem_shadowcopy(void)
+{
+#ifdef CONFIG_PMEM_SHADOW
+	/* Copy the memory to a shadow area */
+	if (pmem.shadow) {
+		PMEM_DPRINT("shadow not null\n");
+		return;
+	}
+
+	pmem.shadow = vmalloc(pmem.size);
+	if (!pmem.shadow) {
+		printk(KERN_ERR "PMEM: could not allocate %d shadow bytes\n",
+		       pmem.size);
+		return;
+	}
+
+	pmem_memcpy_fromio(pmem.shadow, (void*)pmem.pmem, pmem.size);
+	PMEM_DPRINT("pmem shadow copy at %p\n", pmem.shadow);
+#endif
+}
+
+/* Clears persistent memory and sets up a fresh control block */
+static int pmem_reinit(void)
+{
+	int rc = 0;
+	struct pmem_cb_hdr_data hdr_data;
+
+#ifdef CONFIG_PMEM_DEBUG
+	PMEM_DPRINT("pmem_reinit() running on %p with hw=%d:%d\n", pmem.pmem, pmem.using_hardware, pmem.using_io_mem);
+#endif
+	/* Only save a shadow copy if we're using hardware pmem.*/
+	if (pmem.using_hardware)
+		pmem_shadowcopy();
+
+	/* Clear all of persistent memory */
+	pmem_memset((void *) pmem.pmem, 0, pmem.size);
+	
+	
+	/* Create a new control block from scratch */
+	pmem.pmem->data.size = htonl(pmem.size);
+	pmem.pmem->data.flags = htonl(0);
+	pmem.pmem->data.version = PMEM_VERSION_CURRENT;
+	pmem.pmem->data.num_cpus = PMEM_NUM_SUPPORT_CPUS;
+	pmem.pmem->data.arch_info = PMEM_ARCH_INFO;
+	pmem.pmem->data.num_alloc = 0;
+
+	PMEM_UPDATE_CB_CHECKSUM(pmem.pmem);
+
+	/* If we are using hardware - make sure that the values just written
+	 * can be read back.  Sometimes if the hardware is misconfigured or
+	 * has errors, we can read back garbage values. */
+	if (pmem.using_hardware) {
+		pmem_memset(&hdr_data, 0, sizeof(struct pmem_cb_hdr_data));
+		hdr_data.size = htonl(pmem.size);
+		hdr_data.flags = htonl(0);
+		hdr_data.version = PMEM_VERSION_CURRENT;
+		hdr_data.num_cpus = PMEM_NUM_SUPPORT_CPUS;
+		hdr_data.arch_info = PMEM_ARCH_INFO;
+		hdr_data.num_alloc = 0;
+
+		rc = memcmp(&(pmem.pmem->data), &hdr_data,
+		            sizeof(struct pmem_cb_hdr_data));
+		if (0 != rc) {
+			printk(KERN_ERR "ERROR: Hardware pmem is inconsitent rc=%d\n", rc);
+			return -EIO;
+		}
+	}
+	
+	return 0;
+}
+
+/* Create the elements needed for operation */
+static int pmem_volatile_init(void)
+{
+	struct pmem_alloc_desc *alloc_desc;
+	int desc_index;
+
+	INIT_LIST_HEAD(&pmem.part_list);
+
+	for (desc_index = 0; desc_index < pmem.pmem->data.num_alloc;
+	     desc_index++) {
+		alloc_desc = PMEM_GET_ALLOC_DESC(desc_index);
+
+		switch (alloc_desc->data.type) {
+		case PMEM_ALLOC_TYPE_LOG:
+			{
+				struct part_list_elem *part_elem;
+				struct region_list_elem *region_elem;
+				struct pmem_block_hdr *block_hdr = NULL;
+				int index;
+
+				part_elem =
+				    kmalloc(sizeof (struct part_list_elem),
+					    GFP_KERNEL);
+				if (!part_elem) {
+					printk(KERN_ERR
+					       "PMEM: Unable to allocate memory\n");
+					return (-ENOMEM);
+				}
+
+				part_elem->hdr = PMEM_GET_PART_HDR(alloc_desc);
+				part_elem->active_block_offset = 0;
+				INIT_LIST_HEAD(&part_elem->region_list);
+
+				/* Figure out which block was active. */
+				for (index = 0;
+				     index < part_elem->hdr->data.num_blocks;
+				     index++) {
+					block_hdr =
+					    PMEM_GET_BLOCK_HDR(part_elem->hdr,
+							       index);
+
+					if (block_hdr->data.
+					    flags & PMEM_BLOCK_FLAG_ACTIVE)
+						part_elem->active_block_offset =
+						    block_hdr->data.offset;
+				}
+
+				/* Ensure that one block is active if there are blocks */
+				if ((part_elem->active_block_offset == 0)
+				    && (block_hdr)) {
+					part_elem->active_block_offset =
+					    block_hdr->data.offset;
+					block_hdr->data.flags &=
+					    PMEM_BLOCK_FLAG_ACTIVE;
+					PMEM_UPDATE_CHECKSUM(block_hdr);
+				}
+
+				for (index = 0;
+				     index < part_elem->hdr->data.num_regions;
+				     index++) {
+					region_elem =
+					    kmalloc(sizeof
+						    (struct region_list_elem),
+						    GFP_KERNEL);
+					if (!region_elem) {
+						printk(KERN_ERR
+						       "PMEM: Unable to allocate memory\n");
+						return (-ENOMEM);
+					}
+
+					region_elem->hdr =
+					    PMEM_GET_REGION_HDR(part_elem->hdr,
+								index);
+					region_elem->data_lock =
+					    SPIN_LOCK_UNLOCKED;
+					region_elem->ptr_block = NULL;
+
+					list_add(&(region_elem->list_elem),
+						 &(part_elem->region_list));
+				}
+
+				list_add(&(part_elem->list_elem),
+					 &pmem.part_list);
+				break;
+			}
+		}
+
+		PMEM_DPRINT("Created elements for partition %d\n", desc_index);
+	}
+
+	return 0;
+}
+
+/* Performs extra partition initialization such as rotating blocks */
+static void pmem_rotate_partitions(void)
+{
+	struct part_list_elem *part_elem;
+	struct list_head *elem, *temp;
+
+	list_for_each_safe(elem, temp, &pmem.part_list) {
+		part_elem = list_entry(elem, struct part_list_elem, list_elem);
+
+		if (part_elem->hdr->data.num_blocks > 1)
+			if (pmem_rotate_block_data(part_elem, 0) < 0)
+				printk(KERN_ERR
+				       "PMEM: unable to rotate partition\n");
+	}
+	PMEM_DPRINT("INFO: pmem_rotate_partitions() done.\n");
+}
+
+static int pmem_reg_users(void)
+{
+	struct pmem_reg_part part;
+	struct pmem_reg_region region;
+	pmem_handle_t log_part_hdl = NULL;
+#if defined(CONFIG_SCHED_HIST_BUF) || defined(CONFIG_LHB)
+	pmem_handle_t region_hdl = NULL;
+#endif
+	int rc;
+
+	/* Initialize the various pointer blocks */
+#ifdef CONFIG_SCHED_HIST_BUF
+	memset(&sched_hist_pmem_block, '\0', sizeof(struct pmem_ptr_block));
+#endif
+	/* Register the log partition */
+	strncpy(part.desc, PMEM_PART_LOG_DESC, PMEM_DESC_MAX);
+	part.size = CONFIG_PMEM_LOG_PART_SIZE;
+	part.num_blocks = CONFIG_PMEM_LOG_PART_SEGMENTS;
+	part.version = PMEM_PART_LOG_VERSION;
+
+	rc = pmem_partition_reg(&part, &log_part_hdl);
+	if (rc < 0) {
+		printk(KERN_ERR "ERROR: **********************************\n");
+		printk(KERN_ERR "ERROR: Unable to register log partition\n");
+		printk(KERN_ERR "ERROR: Kernel regions are DISABLED (LHB, etc)!\n");
+		printk(KERN_ERR "ERROR: **********************************\n");
+		return rc;
+	}
+
+#ifdef CONFIG_PMEM_LOG_REG
+	/* Register the log region and release the handle */
+	strncpy(region.desc, PMEM_REG_GENERAL_DESC, PMEM_DESC_MAX);
+	region.size = CONFIG_PMEM_LOG_REG_SIZE;
+	region.flags = PMEM_REG_GENERAL_FLAGS;
+	region.fixed_size = PMEM_REG_GENERAL_FIXED_SIZE;
+	region.num_log_desc = CONFIG_PMEM_LOG_REG_LOGS;
+	region.version = PMEM_REG_GENERAL_VERSION;
+	region.block_id = PMEM_ACTIVE_BLOCK;
+
+	rc = pmem_region_reg(log_part_hdl, &region, &region_hdl);
+	if (rc < 0) {
+		printk(KERN_ERR "ERROR: **********************************\n");
+		printk(KERN_ERR "ERROR: Unable to get general logs region\n");
+		printk(KERN_ERR "ERROR: Kernelspace access DISABLED\n");
+		printk(KERN_ERR "ERROR: **********************************\n");
+	}
+	else {
+		pmem_release_handle(&region_hdl);
+	}
+#endif
+
+#ifdef CONFIG_SCHED_HIST_BUF 
+	strncpy(region.desc, PMEM_REG_SCHED_DESC, PMEM_DESC_MAX);
+	region.size = CONFIG_SCHED_HIST_SIZE;
+	region.flags = PMEM_REG_SCHED_FLAGS;
+	region.fixed_size = PMEM_REG_SCHED_FIXED_SIZE;
+	region.num_log_desc = PMEM_REG_SCHED_LOG_DESC;
+	region.version = PMEM_REG_SCHED_VERSION;
+	region.block_id = PMEM_ACTIVE_BLOCK;
+
+	if (pmem_region_reg(log_part_hdl, &region, &region_hdl) < 0) {
+		printk(KERN_ERR "ERROR: **********************************\n");
+		printk(KERN_ERR "ERROR: Unable to get sched history region\n");
+		printk(KERN_ERR "ERROR: Scheduler History is DISABLED.\n");
+		printk(KERN_ERR "ERROR: **********************************\n");
+	}
+	else {
+		if (pmem_register_ptr_block(region_hdl, &sched_hist_pmem_block) < 0) {
+			printk(KERN_ERR "ERROR: *****************************\n");
+			printk(KERN_ERR "ERORR: Unable to get sched history ptr block\n");
+			printk(KERN_ERR "ERROR: Scheduler History is DISABLED.\n");
+			printk(KERN_ERR "ERROR: *****************************\n");
+		}
+
+		pmem_release_handle(&region_hdl);
+	}
+	
+#endif /* CONFIG_SCHED_HIST_BUF */
+
+	/* Release the log partition */
+	pmem_release_handle(&log_part_hdl);
+
+	printk(KERN_INFO "PMEM: pmem_reg_users() done.\n");
+	return 0;
+}
+
+/* Setup pmem to use simulated memory.  A chunk of vmalloc()d ram.
+ * Returns 0 on successful allocated of the simulated pmem
+ */
+static int alloc_simulated_pmem(void)
+{
+	unsigned long len;
+	unsigned long end;
+	int rc = 0;
+	int i;
+
+	/* Round off required pmem size to PAGE_SIZE */
+	len = CONFIG_PMEM_SIZE + (PAGE_SIZE -1);
+	len &= ~(PAGE_SIZE - 1);
+	if ((pmem.pmem = vmalloc(len))) {
+		pmem.size = CONFIG_PMEM_SIZE;
+		pmem.using_hardware = 0;
+		pmem.using_io_mem = 0;
+
+		end = (unsigned long)pmem.pmem + len;
+		for (i = 0; i < len ; i += PAGE_SIZE) {
+			SetPageReserved(vmalloc_to_page((void *)((unsigned long)pmem.pmem + i)));
+		}
+
+		printk(KERN_ERR "PMEM: Using simulated pmem\n");
+	} else {
+		printk(KERN_ERR "PMEM: No simulated pmem available\n");
+		rc = -ENOMEM;
+	}
+
+	return rc;
+}
+
+/* Generic initialization function for persistent memory.
+ * When this function has completed, users can start using
+ * persistent memory. */
+static int pmem_init(void)
+{
+	int rc = -EINVAL;
+	int do_reinit=1;
+
+	if (pmem.enabled) {
+		/* Already initialized */
+		printk(KERN_ERR "WARNING: Calling pmem_init with pmem already"
+		                " initialized\n");
+		return rc;
+	}
+
+#ifdef CONFIG_PMEM_HARDWARE
+	/* Get the memory.  If this passes, it is assumed that
+	 * hardware pmem is available. */
+	if (pmem_arch_get_persistent_memory((void **) &pmem.pmem, &pmem.size) >= 0) {
+		pmem.using_hardware = 1;
+		pmem.using_io_mem = (__u8)pmem_arch_uses_ioremap();
+
+		/* notify the pmem crc checker that the system is coming up.
+		 * On PPC32 this used to be in the setup_arch call, but 
+		 * on Xscale, the system is not yet ready to execute this
+		 * code, so its moved to here */
+		persistent_mem_crc(PERSISTENT_CONTROL_BOOTUP);
+	} else {
+		printk(KERN_ERR "PMEM: Unable to get hardware persistent memory\n");
+		pmem.using_hardware = 0;
+		pmem.using_io_mem = 0;
+	}
+#endif
+
+alloc_simulated:
+	if (!pmem.using_hardware) {
+		if (0 != alloc_simulated_pmem()) {
+			printk(KERN_ERR "PMEM: Cant get simulated pmem !\n");
+			return rc;
+		}
+		goto reinit;
+	}
+
+	if (!pmem.pmem) {
+		printk(KERN_ERR "ERROR: pmem memory hasnt been setup!\n");
+		return rc;
+	}
+
+	PMEM_DPRINT("PMEM: phys=%p, virt=%p, size=%d hardware=%d:%d\n",
+		    (void *) __pa(pmem.pmem), pmem.pmem, pmem.size,
+		    pmem.using_hardware, pmem.using_io_mem);
+
+	/* check if the kernel asked to forcibly format pmem */
+	if (pmem_force_clear) {
+		printk(KERN_INFO "PMEM: Asked to forcibly format pmem\n");
+		goto reinit;
+	}
+	/* Validate the headers in persistent memory
+	 * If any corruption exists, or the memory is uninitialized
+	 * then clear the memory and start over */
+	if (pmem_validate_pmem() < 0) {
+		printk(KERN_ERR "PMEM: Structure is corrupt\n");
+		goto reinit;
+	}
+
+	/* Check if the user requested to clear all persistent memory */
+	if (ntohl(pmem.pmem->data.flags) & PMEM_CB_FLAG_RESET) {
+		printk(KERN_NOTICE "PMEM: Clearing persistent memory based on user flags\n");
+		goto reinit;
+	}
+
+	do_reinit=0;
+reinit:
+	if (do_reinit) {
+		if (0 != pmem_reinit()) {
+			if (pmem.using_hardware) {
+				/* fall back to simulated pmem */
+				pmem.using_hardware = 0;
+				pmem.using_io_mem = 0;
+				goto alloc_simulated;
+			} else {
+				printk(KERN_ERR "ERROR: Cant init pmem !\n");
+				return rc;
+			}
+		}
+	}
+
+	/* Create the dynamic structures for existing data */
+	if (pmem_volatile_init() < 0) {
+		printk(KERN_ERR "PMEM: Unable to create dynamic structures\n");
+		return rc;
+	}
+
+	/* Initialize the existing partitions if we didn't wipe the memory. */
+	if (!do_reinit) {
+		/* ensure that the active partition is locked from the previous
+		 * boot */
+		if (pmem.using_hardware) {
+			pmem_lock_all_active_segments();
+		}
+		pmem_rotate_partitions();
+	}
+
+#ifdef CONFIG_PMEM_DEBUG
+	if (pmem_validate_pmem() < 0) {
+		printk(KERN_ERR
+		       "PMEM: Partition initialization caused corruption\n");
+	}
+#endif
+
+	printk(KERN_NOTICE "PMEM: initialization completed\n");
+	pmem.enabled = 1;
+
+
+	/* install shutdown/reboot CRC routine only if using hardware pmem */
+#ifdef CONFIG_PMEM_HARDWARE
+	if (pmem.using_hardware) {
+		register_reboot_notifier(&pmem_reboot_notifier);
+	}
+#endif
+	/* Now call the function that will setup common kernel areas */
+	rc = pmem_reg_users();
+
+	return rc;
+}
+__initcall(pmem_init);
+
+#ifdef CONFIG_PMEM_HARDWARE
+/* Hardware pmem wants to update the CRC on pmem on reboot */
+static int pmem_reboot_handler(struct notifier_block *this,
+			       unsigned long code, void *unused)
+{
+	int reason;
+	int rc = -EFAULT;
+
+	printk(KERN_INFO "INFO: Starting pmem_reboot_handler\n");
+	
+	/* lock the current active segment to prevent accidental 
+	 * overwrite in the case of a reboot loop */
+	pmem_lock_all_active_segments();
+
+	switch (code) {
+		case SYS_RESTART:
+			/* also handles SYS_DOWN  */
+			reason = PERSISTENT_CONTROL_RESTART;
+			break;
+		case SYS_HALT:
+			reason = PERSISTENT_CONTROL_HALT;
+			break;
+		case SYS_POWER_OFF:
+			reason = PERSISTENT_CONTROL_PWR_OFF;
+			break;
+		default:
+			reason = 0xffffffff;
+			printk(KERN_ERR "WARN: Unknown reboot code %ld\n", code);
+			break;
+	}
+	
+	/* always going to be a reboot on NCGL platform */
+	rc = persistent_mem_crc(reason);
+	if (rc) {
+		printk(KERN_ERR "ERROR: Failed to calculate pmem CRC err=%d\n", rc);
+	}
+
+	printk(KERN_INFO "INFO: Finishing pmem_reboot_handler\n");
+	return rc;
+}
+#endif 
+
+
+
diff --git a/drivers/pmem/pmem_pci.c b/drivers/pmem/pmem_pci.c
new file mode 100644
index 0000000..7df3dd0
--- /dev/null
+++ b/drivers/pmem/pmem_pci.c
@@ -0,0 +1,327 @@
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/pci.h>
+#include <linux/init.h>
+
+#include <linux/pmem.h>
+
+#if defined(CONFIG_PMEM_HW_ATCAF101)
+#define PCI_DEVICE_ID_MOTOROLA_9030	0x4824
+#define PCI_9030_PMEM_RESOURCE		2
+#elif defined(CONFIG_PMEM_HW_ATCA7101)
+#define PCI_DEVICE_ID_MOTOROLA_9030	0x4825
+#define PCI_9030_PMEM_RESOURCE		2
+#endif
+
+#define PCI_PMC815_PMEM_RESOURCE        2
+
+/* GWC will have 4 PMC280s.  MC will have 1 PMC280 on bus-4 device-4 */
+/* We'll try to use that PMC for both card types */
+#define PCI_ATCA717_PMC4_BUS            4
+#define PCI_ATCA717_PMC4_DEVFN          4
+#define PCI_MARVELL_CS0_BAR             0
+#define PCI_ATCA717_PMEM_SIZE           0x01000000
+#define PCI_ATCA717_PMEM_PCI_OFFSET     0x06000000
+/* flag indicating that favourite PMC is present */
+static unsigned short PCI_ATCA717_PMC4_PRESENT = 0;
+
+#define DEV_TYPE_9030	1
+
+/* 'global' values that the pmem pci driver will set.
+ *
+ * These will be read by the pmem_arch_get_persistent_memory() routine 
+ */
+static unsigned long pmem_pci_start = 0;
+static unsigned long pmem_pci_length = 0;
+static unsigned short pmem_pci_vendor_id = 0;
+static unsigned short pmem_pci_device_id = 0;
+static unsigned short pmem_pci_resource_num = 0;
+
+/* single access point to the ioremaped pmem pointers.  Removes the need for
+ * the crc code to iounmap its pointers */
+static void *pmem_start_ptr = (void *)0;
+static void *pmem_checksum_ptr = (void *)0;
+
+/* PCI based pmem is always ioremapped in to the kernels address space */
+int pmem_arch_uses_ioremap(void)
+{
+	return 1;
+}
+
+int pmem_arch_get_size(void)
+{
+	unsigned long size;
+
+	/* Start with user requested size, and round down required to
+	   PAGE_SIZE.  This allows partitions to be page aligned for MMAP to
+	   work. */
+	size = (CONFIG_PMEM_SIZE >> PAGE_SHIFT) << PAGE_SHIFT;
+
+	if (!pmem_pci_start) {
+		printk(KERN_ERR
+		       "Error: Cant determine size before PCI mapping\n");
+		return -1;
+	}
+
+	if (size + PERSISTENT_CONTROL_SIZE > pmem_pci_length) {
+#ifdef CONFIG_PMEM_DEBUG
+		PMEM_DPRINT("INFO: reducing pmem size to meet hw limits\n");
+#endif
+		size = ((pmem_pci_length - PERSISTENT_CONTROL_SIZE)
+		        >> PAGE_SHIFT) << PAGE_SHIFT;
+	}
+
+	if (size % PAGE_SIZE) {
+		PMEM_DPRINT("WARNING: PMEM size (%lu) is not a multiple of "
+		            "PAGE_SIZE\n", size);
+	}
+	return size;
+}
+
+struct pci_dev *pmem_arch_get_pci_dev(struct pci_dev *dev)
+{
+	if (!pmem_pci_start) {
+		PMEM_DPRINT("ERROR: Asked for pmem pci_dev before PCI init done\n");
+	}
+	
+	return pci_get_device(pmem_pci_vendor_id, pmem_pci_device_id, NULL);
+}
+EXPORT_SYMBOL(pmem_arch_get_pci_dev);
+
+void *pmem_arch_get_pci_start_ptr(void)
+{
+	if (!pmem_pci_start) {
+		PMEM_DPRINT("ERROR: Asked for pmem pci ptr before PCI init done\n");
+	}
+
+	return (void *)pmem_pci_start;
+}
+EXPORT_SYMBOL(pmem_arch_get_pci_start_ptr);
+
+int pmem_arch_get_pci_resource_num(void)
+{
+	if (!pmem_pci_start) {
+		PMEM_DPRINT("ERROR: Asked for pmem pci resource num before PCI init done\n");
+	}
+
+	return pmem_pci_resource_num;
+}
+EXPORT_SYMBOL(pmem_arch_get_pci_resource_num);
+
+void *pmem_arch_get_start_ptr(void)
+{
+	int pmem_size = pmem_arch_get_size();
+
+	if (!pmem_pci_start) {
+		PMEM_DPRINT("ERROR: Asked for pmem ptr before PCI init done\n");
+		return 0;
+	}
+
+	if (!pmem_start_ptr) {
+
+#ifdef CONFIG_PMEM_DEBUG
+		printk(KERN_INFO "PCI Persistent Memory located at %lx "
+		       "size %x\n", pmem_pci_start + PERSISTENT_CONTROL_SIZE,
+		       pmem_size);
+#endif
+		pmem_start_ptr = (void *)ioremap_nocache(pmem_pci_start +
+							 PERSISTENT_CONTROL_SIZE,
+							 pmem_size);
+		if (!pmem_pci_start) {
+			printk(KERN_ERR "ERROR: Cant get pci pmem \n");
+		}
+	}
+
+	return pmem_start_ptr;
+}
+EXPORT_SYMBOL(pmem_arch_get_start_ptr);
+
+void *pmem_arch_get_checksum_start_ptr(void)
+{
+	if (!pmem_pci_start)
+		return 0;
+
+	if (!pmem_checksum_ptr) {
+#ifdef CONFIG_PMEM_DEBUG
+		printk("PCI Persistent chkSum located at %lx size %x\n",
+		       pmem_pci_start, (int)PERSISTENT_CONTROL_SIZE);
+#endif
+		pmem_checksum_ptr = ioremap_nocache(pmem_pci_start,
+						    PERSISTENT_CONTROL_SIZE);
+		if (!pmem_checksum_ptr) {
+			printk(KERN_ERR "ERROR: Cant get pci pmem CRC mem\n");
+		}
+	}
+
+	return pmem_checksum_ptr;
+}
+
+int pmem_arch_get_persistent_memory(void **ptr, __u32 * size)
+{
+	unsigned long pmem_ptr;
+
+	pmem_ptr = (unsigned long)pmem_arch_get_start_ptr();
+	if (!pmem_ptr) {
+		printk(KERN_ERR "ERROR: Cant find PCI pmem segment\n");
+		return -EFAULT;
+	}
+
+	*ptr = (void *)pmem_ptr;
+	*size = pmem_arch_get_size();
+
+#ifdef CONFIG_PMEM_DEBUG
+	PMEM_DPRINT("INFO: raw size = %ld, crc_size=%d, pmem.size=%d\n",
+		    pmem_pci_length, (int)PERSISTENT_CONTROL_SIZE, pmem.size);
+#endif
+	return 0;
+}
+EXPORT_SYMBOL(pmem_arch_get_persistent_memory);
+
+#ifdef CONFIG_PMEM_HW_ATCA6101
+/* Initialize the PMC815 device to be used for Pmem */
+static int probe_harrier(struct pci_dev *dev, const struct pci_device_id *id)
+{
+	//pmem_pci_start = pci_resource_start(dev, PCI_PMC815_PMEM_RESOURCE) + 0x800000;
+	pmem_pci_start =
+	    pci_resource_start(dev,
+			       PCI_PMC815_PMEM_RESOURCE) + CONFIG_FIRMWARE_SIZE;
+	pmem_pci_length =
+	    pci_resource_end(dev, PCI_PMC815_PMEM_RESOURCE) - pmem_pci_start;
+	pmem_pci_resource_num = PCI_PMC815_PMEM_RESOURCE;
+
+#ifdef CONFIG_PMEM_DEBUG
+	printk(KERN_INFO
+	       "INFO: Found PCI memory start=%lx, len %ld w/ devid=[%s]\n",
+	       pmem_pci_start, pmem_pci_length, pci_name(dev));
+#endif
+	return 0;
+}
+#endif
+
+#if defined(CONFIG_PMEM_HW_ATCA7101) || defined(CONFIG_PMEM_HW_ATCAF101)
+/* Initialize the 9030 device to be used for Pmem */
+static int probe_9030(struct pci_dev *dev, const struct pci_device_id *id)
+{
+	pmem_pci_start = pci_resource_start(dev, PCI_9030_PMEM_RESOURCE);
+	pmem_pci_length =
+	    pci_resource_end(dev, PCI_9030_PMEM_RESOURCE) - pmem_pci_start;
+	pmem_pci_resource_num = PCI_9030_PMEM_RESOURCE;
+
+#ifdef CONFIG_PMEM_DEBUG
+	printk(KERN_INFO
+	       "INFO: Found PCI memory start=%lx, len %ld w/ devid=[%s]\n",
+	       pmem_pci_start, pmem_pci_length, pci_name(dev));
+#endif
+	return 0;
+}
+#endif
+
+/* Initialize the 64360 device to be used for Pmem */
+#ifdef CONFIG_PMEM_HW_ATCA717
+static int probe_64360(struct pci_dev *dev, const struct pci_device_id *id)
+{
+	if (PCI_ATCA717_PMC4_PRESENT != 0) {
+		return 0;
+	}
+	if (PCI_FUNC(dev->devfn) == PCI_MARVELL_CS0_BAR) {
+		unsigned long tmp_pmem_pci_start = 0;
+		unsigned long tmp_pmem_pci_length = 0;
+
+		tmp_pmem_pci_start =
+		    pci_resource_start(dev, PCI_MARVELL_CS0_BAR);
+		tmp_pmem_pci_length =
+		    pci_resource_end(dev, PCI_MARVELL_CS0_BAR) - pmem_pci_start;
+
+		if (tmp_pmem_pci_length >=
+		    (PCI_ATCA717_PMEM_PCI_OFFSET + PCI_ATCA717_PMEM_SIZE)) {
+
+			pmem_pci_start =
+			    (tmp_pmem_pci_start + PCI_ATCA717_PMEM_PCI_OFFSET);
+			pmem_pci_length = PCI_ATCA717_PMEM_SIZE;
+			pmem_pci_resource_num = PCI_MARVELL_CS0_BAR;
+
+			if ((dev->bus->number == PCI_ATCA717_PMC4_BUS) &&
+			    (PCI_SLOT(dev->devfn) == PCI_ATCA717_PMC4_DEVFN)) {
+				PCI_ATCA717_PMC4_PRESENT = 1;
+			}
+#ifdef CONFIG_PMEM_DEBUG
+			printk(KERN_INFO
+			       "INFO: Found PCI memory start=%lx, len %ld w/ devid=[%s]\n",
+			       pmem_pci_start, pmem_pci_length, pci_name(dev));
+#endif
+		}
+	}
+	return 0;
+}
+#endif
+
+static int probe(struct pci_dev *dev, const struct pci_device_id *id)
+{
+	pci_enable_device(dev);
+
+	pmem_pci_vendor_id = dev->vendor;
+	pmem_pci_device_id = dev->device;
+#ifdef CONFIG_PMEM_DEBUG
+	printk(KERN_INFO "INFO: PCI vendorid=%d, deviceid=%d\n", pmem_pci_vendor_id, pmem_pci_device_id);
+#endif
+	
+	/* pass off init to the function stored in driver_data */
+	if (!id->driver_data) {
+		printk(KERN_ERR
+		       "ERROR: NULL data field in pmem_pci device table\n");
+		return -1;
+	}
+
+	return ((int (*)(struct pci_dev *, const struct pci_device_id *))
+		(id->driver_data)) (dev, id);
+}
+
+static void remove(struct pci_dev *dev)
+{
+	/* clean up any allocated resources and stuff here.
+	 * like call release_region();
+	 */
+}
+
+/* PCI driver boiler plate */
+static struct pci_device_id ids[] = {
+#if defined(CONFIG_PMEM_HW_ATCA7101) || defined(CONFIG_PMEM_HW_ATCAF101)
+	{PCI_DEVICE(PCI_VENDOR_ID_MOTOROLA, PCI_DEVICE_ID_MOTOROLA_9030),
+	 0, 0, (kernel_ulong_t) & probe_9030},
+#endif
+#ifdef CONFIG_PMEM_HW_ATCA717
+	{PCI_DEVICE(PCI_VENDOR_ID_MARVELL, PCI_DEVICE_ID_MARVELL_MV64360),
+	 0, 0, (kernel_ulong_t) & probe_64360},
+#endif
+#ifdef CONFIG_PMEM_HW_ATCA6101
+	{PCI_DEVICE(PCI_VENDOR_ID_MOTOROLA, PCI_DEVICE_ID_MOTOROLA_HARRIER),
+	 0, 0, (kernel_ulong_t) & probe_harrier},
+#endif
+	{0,}
+};
+
+MODULE_DEVICE_TABLE(pci, ids);
+
+static struct pci_driver pci_driver = {
+	.name = "pci_pmem",
+	.id_table = ids,
+	.probe = probe,
+	.remove = remove,
+};
+
+static int __init pmem_pci_init(void)
+{
+	return pci_register_driver(&pci_driver);
+}
+
+/* should never be called */
+static void __exit pmem_pci_exit(void)
+{
+	pci_unregister_driver(&pci_driver);
+}
+
+MODULE_LICENSE("GPL");
+
+module_init(pmem_pci_init);
+module_exit(pmem_pci_exit);
+
diff --git a/fs/Kconfig b/fs/Kconfig
index a7225f1..3628a24 100644
--- a/fs/Kconfig
+++ b/fs/Kconfig
@@ -1020,6 +1020,14 @@ config UNION_FS_DEBUG
 	help
 	  If you say Y here, you can turn on debugging output from Unionfs.
 
+config PMEMFS
+	tristate "Persistent Memory filesystem"
+	depends on PMEM
+	default m
+	help
+	  The persistent memory filesystem provides userspace access to 
+	  the kernel's persistent memory subsystem.
+
 endmenu
 
 menu "Miscellaneous filesystems"
diff --git a/fs/Makefile b/fs/Makefile
index 1198ae7..5c9ddec 100644
--- a/fs/Makefile
+++ b/fs/Makefile
@@ -124,6 +124,7 @@ obj-$(CONFIG_AFS_FS)		+= afs/
 obj-$(CONFIG_BEFS_FS)		+= befs/
 obj-$(CONFIG_HOSTFS)		+= hostfs/
 obj-$(CONFIG_HPPFS)		+= hppfs/
+obj-$(CONFIG_PMEM)		+= pmemfs/
 obj-$(CONFIG_DEBUG_FS)		+= debugfs/
 obj-$(CONFIG_OCFS2_FS)		+= ocfs2/
 obj-$(CONFIG_GFS2_FS)           += gfs2/
diff --git a/fs/pmemfs/Makefile b/fs/pmemfs/Makefile
new file mode 100644
index 0000000..ca9b052
--- /dev/null
+++ b/fs/pmemfs/Makefile
@@ -0,0 +1,10 @@
+#
+# Makefile for the pmemfs-filesystem routines.
+#
+
+#ifeq ($(CONFIG_PMEM),y)
+#    obj-m += pmemfs.o
+#endif
+
+obj-$(CONFIG_PMEMFS)	+= pmemfs.o
+pmemfs-objs := inode.o
diff --git a/fs/pmemfs/inode.c b/fs/pmemfs/inode.c
new file mode 100644
index 0000000..0af0352
--- /dev/null
+++ b/fs/pmemfs/inode.c
@@ -0,0 +1,2992 @@
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/vmalloc.h>
+#include <linux/pagemap.h> 	/* PAGE_CACHE_SIZE */
+#include <linux/backing-dev.h>	/* struct backing_dev_info */
+#include <linux/fs.h>		/* This is where libfs stuff is declared */
+#include <linux/mount.h>
+#include <linux/ioport.h>		
+#include <linux/pmem.h>		
+#include <asm/atomic.h>
+#include <asm/uaccess.h>	/* copy_to_user */
+
+#ifdef CONFIG_PMEM_PCI_DRIVER
+#include <linux/pci.h>
+#endif
+
+
+/*
+ * Persistent Memory filesystem.
+ *
+ * A simple filesystem that provides userspace access to the persistent
+ * memory area in CGL.
+ *
+ * Loosely based on the following filesystem samples :
+ * 	- lwnfs (Copyright 2002 Jonathan Corbet <corbet@lwn.net>)
+ * 	- ramfs, tmpfs
+ * 	- smbfs for the readpage and writepage templates
+ *
+ * WindRiver 
+ * May 2005
+ */
+
+MODULE_LICENSE("Dual BSD/GPL");
+MODULE_AUTHOR("WindRiver");
+
+/* pmemfs globals, constants and types {{{ */
+
+#define PMEMFS_MAGIC 0x82640149  /* random magic for superblock */
+/* Number of bytes to read before calling schedule() */
+#define PMEMFS_RAW_CHUNK 16384
+
+/* This makes a dentry parent/child name pair. Useful for debugging printk's */
+#define DENTRY_PATH(dentry) \
+	(dentry)->d_parent->d_name.name,(dentry)->d_name.name
+
+/* file permissions */
+const int PMEMFS_PERM_NONE =		0000;
+const int PMEMFS_PERM_RO =		0444;
+const int PMEMFS_PERM_RW =	 	0644;
+
+
+/* different types of 'files' that pmemfs exports */
+typedef enum {
+	PMEMFS_RAW_PMEM=1,	/* Raw access to all of pmem */
+	PMEMFS_RAW_PART,	/* Raw access to a pmem partition */
+	PMEMFS_RAW_SEG,		/* Deprecated: Raw access to a pmem segment */
+	PMEMFS_RAW_REG,		/* Deprecated: Raw access to a pmem region */
+	PMEMFS_REG_RECORD,	/* Access to a log descriptor based region */
+	PMEMFS_REG_BYTE,	/* Access to fixed length region */
+	PMEMFS_BACKUP,		/* In kernel backup of corrupted pmem */
+	PMEMFS_HW_INDICATOR,	/* Indicator file for Hardware backed pmem */
+	/* "Control" file types after here */
+	PMEMFS_NEW_PART,	/* Create a new partition */
+	PMEMFS_PART_NEW_REG,	/* Create a new region on a partition */ 
+	PMEMFS_LOCK_PMEM,	/* Lock or unlock all of pmem */
+	PMEMFS_LOCK_ALL_ACTIVE,	/* Lock or unlock all active segments in pmem */
+	PMEMFS_LOCK_PART,	/* Lock or unlock a pmem partition */
+	PMEMFS_LOCK_SEG,	/* Lock of unlock a pmem segment */
+	PMEMFS_PMEM_ROTATE,	/* Rotate on all partitions in pmem */
+	PMEMFS_PART_ROTATE,	/* Rotate on a pmem partition */
+	PMEMFS_ACTIVE_SEGMENT,	/* Read a partitions active segment  */
+	PMEMFS_RESET_PMEM,	/* Access to the Reset pmem flag */
+	/* Extended logical file types */
+	PMEMFS_PART_HEADER,	/* Access to a pmem partition header area */
+	PMEMFS_PART_DATA,	/* Access to a pmem partition data area */
+	PMEMFS_SEG_HEADER,	/* Access to a pmem segment header area */
+	PMEMFS_SEG_DATA,	/* Access to a pmem segment data area */
+	PMEMFS_REG_HEADER,	/* Access to a pmem region header area */
+	PMEMFS_REG_RAW_DATA,	/* Raw access to a pmem region data area */
+	PMEMFS_REG_DATA,	/* Access to a pmem region data area */
+} pmemfs_file_t;
+
+/* typedefs for the read and write function handlers used for access to files
+ * on pmemfs.  typedefs are "evil",but we need a type for pmemfs_alloc_finfo 
+ * Notice that theses routines differ from the standard libfs read/write 
+ * signarutes by having the pmem_handle and the offset is not changed by
+ * these routines (not a ptr) */
+typedef ssize_t (*pmem_read_fn) (pmem_handle_t, char *, size_t, loff_t);
+typedef ssize_t (*pmem_write_fn) (pmem_handle_t, const char *, size_t, loff_t);
+
+/* private file handle for pmemfs 'files'.  We attach this to a inode's
+ * private data field so that we know what kind of file the user is 
+ * attempting to access and any data we need to access the appropriate
+ * section of pmem. read and write functions are also defined for each 
+ * file. The function pointers for read/write can be null if this file 
+ * doesnt support read or write. */
+struct pmemfs_file_info {
+	pmemfs_file_t	type;		/* type of pmemfs file this is */
+	pmem_handle_t	handle;		/* pmem handle for this file */
+	pmem_read_fn	read;		/* read function for this file */
+	pmem_write_fn	write;		/* write function */
+};
+
+/********************************************************************
+ *
+ * Persistent memory filesystem globals (not globally visible)
+ *
+ *******************************************************************/
+/* the pmemfs superblock ptr  - it is cached so we can create inodes 
+ * when the kernel callbacks are called (not a VFS call, so no VFS
+ * context) */
+static struct super_block *pmemfs_super = NULL;
+
+
+
+/********************************************************************
+ *
+ * VFS structure pre-declarations
+ *
+ *******************************************************************/
+static struct file_system_type pmemfs_type;
+static struct super_operations pmemfs_sb_ops;
+static struct file_operations pmemfs_file_ops;
+static struct inode_operations pmemfs_inode_ops;
+static struct backing_dev_info pmemfs_backing_dev_info;
+static struct address_space_operations pmemfs_aops;
+static struct vm_operations_struct pmemfs_file_vm_ops;
+
+
+/********************************************************************
+ *
+ * pmemfs miscellaneous routines
+ *
+ *******************************************************************/
+#ifdef pgprot_noncached
+/*
+ * Architectures vary in how they handle caching for addresses
+ * outside of main memory.
+ * The definition is moved to  mm/pmem/cmds.c since page_is_ram is no
+ * longer exported.
+ */
+extern int pmem_uncached_access(unsigned long addr);
+
+#endif /* pgprot_noncached */
+
+
+/********************************************************************
+ *
+ * pmemfs filesystem routines
+ *
+ *******************************************************************/
+/* static struct dentry *pmemfs_find_dentry {{{
+ *
+ * Find the child dentry of the given parent, with the specified name.  
+ * Returns a pointer to the dentry for the requested partition, or 
+ * NULL if the child node does not exist
+ * 
+ * NOTE: Since this routine calls d_lookup to find the resulting 
+ * dentry, the caller must call d_put on the result once they are done
+ * using it to decrement the refcount.
+ */
+static struct dentry *pmemfs_find_dentry(struct dentry *parent,
+		const char *name)
+{
+	struct dentry *result = NULL;
+	struct qstr qname;
+
+	qname.name = name;
+	qname.len = strlen(name);
+	qname.hash = full_name_hash(name, qname.len);
+
+	result = d_lookup(parent, &qname);
+	//PMEM_DPRINT("INFO: find_dentry returns %p\n", result);
+	return result;
+}
+
+/* static struct inode *pmemfs_create_inode {{{
+ * This function creates an inode to represent the structure of
+ * pmem to the VFS layer.  It is called for each directory and file
+ * that exists in the filesystem.
+ */
+static struct inode *pmemfs_create_inode(struct super_block *sb, int mode,
+                const loff_t size)
+{
+	struct inode *ret = new_inode(sb);
+
+	if (ret) {
+		ret->i_mode = mode;
+		ret->i_uid = current->fsuid;
+		ret->i_gid = current->fsgid;
+		ret->i_size = size;
+		ret->i_blocks = (long)(size / PAGE_CACHE_SIZE);
+		if (size % PAGE_CACHE_SIZE)
+			ret->i_blocks += 1;
+
+		ret->i_mapping->a_ops = &pmemfs_aops;
+		ret->i_mapping->backing_dev_info = &pmemfs_backing_dev_info;
+		/* FIXME: Do we want to have true values for a_time/c_time ? */
+		ret->i_atime = ret->i_mtime = ret->i_ctime = CURRENT_TIME;
+	}
+	return ret;
+} 
+
+/* static struct dentry *pmemfs_create_file {{{
+ * Create a file mapping a name to a partition/segment/region/log entry.
+ * Attach the given file info to the file's inode
+ */
+static struct dentry *pmemfs_create_file(struct super_block *sb,
+		struct dentry *dir, const char *name,
+		struct pmemfs_file_info *f_info, const int mode,
+		const loff_t size)
+{
+	struct dentry *dentry;
+	struct inode *inode;
+	struct qstr qname;
+	
+	/* Make a hashed version of the name to go with the dentry. */
+	qname.name = name;
+	qname.len = strlen(name);
+	qname.hash = full_name_hash(name, qname.len);
+	/* Now we can create our dentry and the inode to go with it.  */
+	dentry = d_alloc(dir, &qname);
+	if (!dentry) {
+		PMEM_DPRINT("ERROR: Failed to alloc dentry for [%s]\n", name);
+		return NULL;
+	}
+	inode = pmemfs_create_inode(sb, S_IFREG | mode, size);
+	if (!inode) {
+		PMEM_DPRINT("ERROR: Failed to create inode for [%s]\n", name);
+		dput(dentry);
+		return NULL;
+	}
+	inode->i_fop = &pmemfs_file_ops;
+	/* assign the magic private inode data so that later we know what kind
+	 * of pmemfs file this is */
+	inode->i_private = f_info;
+	/* Put it all into the dentry cache and we're done. */
+	d_add(dentry, inode);
+
+	//PMEM_DPRINT("INFO: CF [%s] dentry=%p, inode=%p\n", name, dentry, dentry->d_inode);
+	return dentry;
+} 
+
+/* static struct dentry *pmemfs_create_dir {{{
+ * Create a directory which can be used to hold files.  This code is
+ * almost identical to the "create file" logic, except that we create
+ * the inode with a different mode, and use the libfs "simple" operations.
+ */
+static struct dentry *pmemfs_create_dir(struct super_block *sb,
+		struct dentry *parent, const char *name)
+{
+	struct dentry *dentry;
+	struct inode *inode;
+	struct qstr qname;
+
+	qname.name = name;
+	qname.len = strlen(name);
+	qname.hash = full_name_hash(name, qname.len);
+	dentry = d_alloc(parent, &qname);
+	if (!dentry) {
+		PMEM_DPRINT("ERROR: Failed to alloc dentry for [%s]\n", name);
+		return 0;
+	}
+	inode = pmemfs_create_inode(sb, S_IFDIR | 0644, 0);
+	if (!inode) {
+		PMEM_DPRINT("ERROR: Failed to create inode for [%s]\n", name);
+		dput(dentry);
+		return 0;
+	}
+	/* use the default inode ops here since we dont support unlink other
+	 * than the top level dir */
+	inode->i_op = &simple_dir_inode_operations;
+	inode->i_fop = &simple_dir_operations;
+
+	d_add(dentry, inode);
+	return dentry;
+}
+
+/********************************************************************
+ *
+ * Pmemfs internal I/O routines. 
+ *
+ *******************************************************************/
+/* static ssize_t pmemfs_read_ptr {{{
+ *
+ * Read from an in kernel pointer.  Abstraction funciton used
+ * by pmemfs_read_raw_pmem and pmemfs_read_shadow.
+ *
+ * Returns number of bytes read on success, -errno on error
+ */
+   
+static ssize_t pmemfs_read_ptr(void *start, int len, char *buf,
+                size_t count, const loff_t offset)
+{
+	ssize_t	bytes_read;
+	ssize_t	bytes_left;
+	void *_start;
+
+	bytes_read = count;
+	bytes_left = count;
+	_start = start + offset;
+
+	if (offset + count > len) {
+		bytes_read = len - offset;
+		bytes_left = len - offset;
+	}
+
+	/* Read just PMEMFS_RAW_CHUNK bytes at a time. This should
+	 * reduce the scheduling latency.
+	 */
+	for (;;) {
+		if (bytes_left > PMEMFS_RAW_CHUNK) {
+			__pmem_memcpy_fromio(buf, _start, PMEMFS_RAW_CHUNK);
+			buf += PMEMFS_RAW_CHUNK;
+			bytes_left -= PMEMFS_RAW_CHUNK;
+			_start += PMEMFS_RAW_CHUNK;
+			schedule();
+		} else {
+			__pmem_memcpy_fromio(buf, _start, bytes_left);
+			break;
+		}
+	}
+
+	return bytes_read;
+} 
+
+/* static ssize_t pmemfs_write_ptr {{{
+ *
+ * Write directly to a location in kernel memory.  Abstraction function
+ * used by pmemfs_write_raw_pmem.
+ *
+ * Returns number of bytes read on success, -errno on error
+ */
+static ssize_t pmemfs_write_ptr(void *start, int len, const char *buf,
+                size_t count, const loff_t offset)
+{
+	ssize_t	bytes_written;
+
+	bytes_written = count;
+	if (offset + count > len) {
+		bytes_written = len - offset;
+	}
+
+	__pmem_memcpy_toio(start + offset, buf, bytes_written);
+
+	return bytes_written;
+} 
+
+/* static ssize_t pmemfs_read_raw_pmem {{{
+ *
+ * Read from the raw persistent memory
+ * Returns number of bytes read on success, -errno on error
+ */
+   
+static ssize_t pmemfs_read_raw_pmem(pmem_handle_t handle, char *buf,
+                size_t count, const loff_t offset)
+{
+	/* nothing fancy here - start at the beginning and allow the user
+	 * to read all the way to the end of pmem */
+	return pmemfs_read_ptr((void*)pmem.pmem, pmem.size, buf, count, offset);
+} 
+
+/* 
+ *
+ * Read from the raw shadow copy.
+ *
+ * Returns number of bytes read on success, -errno on error
+ */
+static ssize_t pmemfs_read_raw_shadow(pmem_handle_t handle, char *buf,
+                size_t count, const loff_t offset)
+{
+	/* nothing fancy here - start at the beginning and allow the user
+	 * to read all the way to the end of pmem */
+	return pmemfs_read_ptr((void*)pmem.shadow, pmem.size, buf, count,
+	                       offset);
+} 
+
+/* static ssize_t pmemfs_read_raw_partition 
+ *
+ * Read from the raw partition memory
+ * Returns number of bytes read on success, -errno on error
+ */
+   
+static ssize_t pmemfs_read_raw_partition(pmem_handle_t handle, char *buf,
+                size_t count, const loff_t offset)
+{
+	struct pmem_handle	*hdl = (struct pmem_handle*)handle;
+	struct part_handle	*part_hdl;
+	struct pmem_part_hdr	*part_hdr;
+
+	if (PMEM_HANDLE_TYPE_PART != hdl->type) {
+		/* handle should always be null here */
+		PMEM_DPRINT("ERROR: Invalid handle type %d for raw partition\n",
+		            hdl->type);
+		return -EINVAL;
+	}
+	part_hdl = (struct part_handle*)hdl;
+	part_hdr = (struct pmem_part_hdr*) pmem_get_hdr_ptr(hdl);
+
+	/* nothing fancy here - start at the beginning and allow the user
+	 * to read all the way to the end of pmem */
+	return pmemfs_read_ptr((void*)part_hdl->elem->hdr, part_hdr->data.size + PMEM_PART_HDR_MAX_SIZE, buf, count, offset);
+} 
+
+/* static ssize_t pmemfs_write_raw_pmem 
+ *
+ * Write directly to raw persistent memory.  Headers and all.  
+ * This routine wont stop you from doing somethind bad, so use caution.
+ * Returns number of bytes read on success, -errno on error
+ */
+static ssize_t pmemfs_write_raw_pmem(pmem_handle_t handle, const char *buf,
+                size_t count, const loff_t offset)
+{
+	int rc = -EINVAL;
+
+	/* WARNING:  Removing this return code check may cause a compiler
+	 * bug on Xscale.  Observed on October 25, 2005
+	 * Adding the rc variable and using it removes the bad register
+	 * optimization
+	 */
+	rc = pmemfs_write_ptr((void*)pmem.pmem, pmem.size, buf, count, offset);
+	if (rc < 0) {
+		PMEM_DPRINT("ERROR: Failed to write to raw pmem\n");
+	}
+	return rc;
+} 
+
+/* static ssize_t pmemfs_write_raw_partition 
+ *
+ * Write directly to raw partition memory.  Headers and all.  
+ * This routine wont stop you from doing somethind bad, so use caution.
+ * Returns number of bytes read on success, -errno on error
+ */
+static ssize_t pmemfs_write_raw_partition(pmem_handle_t handle, const char *buf,
+                size_t count, const loff_t offset)
+{
+	struct pmem_handle	*hdl = (struct pmem_handle*)handle;
+	struct part_handle	*part_hdl;
+	struct pmem_part_hdr	*part_hdr;
+	int rc = -EINVAL;
+
+	/* WARNING:  Removing this return code check may cause a compiler
+	 * bug on Xscale.  Observed on October 25, 2005
+	 * Adding the rc variable and using it removes the bad register
+	 * optimization
+	 */
+	part_hdl = (struct part_handle*)hdl;
+	part_hdr = (struct pmem_part_hdr*) pmem_get_hdr_ptr(hdl);
+
+	rc = pmemfs_write_ptr((void*)part_hdl->elem->hdr, part_hdr->data.size + PMEM_PART_HDR_MAX_SIZE, buf, count, offset);
+	if (rc < 0) {
+		PMEM_DPRINT("ERROR: Failed to write to raw pmem partition\n");
+	}
+	return rc;
+} 
+
+/* static ssize_t pmemfs_read_header {{{
+ * Read from a header area of pmem.  This covers partitions, segments and
+ * regions.  Returns the number of bytes read into buf on success and
+ * -errno on error.
+ */
+static ssize_t pmemfs_read_header(pmem_handle_t handle, char *buf,
+                size_t count, const loff_t offset)
+{
+	ssize_t			bytes_read = 0;
+	int			hdr_size;
+	int 			rc;
+
+	hdr_size = pmem_get_hdr_size(handle);
+	if (hdr_size < 0) {
+		PMEM_DPRINT("ERROR: Cant get header size\n");
+		return -EINVAL;
+	}
+
+	/* read any parts of the header that are requested */
+	if ((int)offset <= hdr_size) {
+		bytes_read = min((int)((int)hdr_size - (int)offset),
+				  (int)count);
+		rc = pmem_read_header(handle, buf, bytes_read, offset);
+		if (rc < 0) {
+			PMEM_DPRINT("ERROR: read %d bytes from hdr rc=%d\n",
+			               (int)bytes_read, rc);
+			return rc;
+		}
+	}
+	return bytes_read;
+} 
+
+/* static ssize_t pmemfs_read_data 
+ * Read from a data area of pmem.  This covers partitions, segments and
+ * regions.  Returns the number of bytes read into buf on success and
+ * -errno on error.
+ */
+static ssize_t pmemfs_read_data(pmem_handle_t handle, char *buf,
+                size_t count, const loff_t offset)
+{
+	struct pmem_handle	*hdl = (struct pmem_handle*)handle;
+
+	/* update the offset in the opaque handle since pmem_write_data_user()
+	 * relies on it being properly set */
+	hdl->offset = offset;
+
+	return pmem_read_data(handle, buf, count);
+} 
+
+/* static ssize_t pmemfs_read_region 
+ *
+ * Read data from a regions data space.  This function handles both 
+ * record based and byte based regions by calling pmem_read_user()
+ * Returns number of bytes read into buf, -errno on error.
+ */
+static ssize_t pmemfs_read_region(pmem_handle_t handle, char *buf,
+                size_t count, const loff_t offset)
+{
+	struct pmem_handle	*hdl = (struct pmem_handle*)handle;
+
+	if (PMEM_HANDLE_TYPE_REG != hdl->type) {
+		PMEM_DPRINT("ERROR: Invalid handle type %d\n", hdl->type);
+		return -EINVAL;
+	}
+
+	/* update the offset in the opaque handle since pmem_read_data_user()
+	 * relies on it being properly set */
+	hdl->offset = offset;
+
+	return pmem_read_data(handle, buf, count);
+} 
+
+/* static ssize_t pmemfs_write_header 
+ *
+ * Write header to a pmem element.
+ *
+ * Returns number of bytes written to the pmem element on success and
+ * -errno on error.
+ */
+static ssize_t pmemfs_write_header(pmem_handle_t handle, const char *buf,
+                size_t count, const loff_t offset)
+{
+	return pmem_write_header(handle, buf, count, offset);
+} 
+
+/* static ssize_t pmemfs_write_data 
+ *
+ * Write data to a pmem element.
+ *
+ * Returns number of bytes written to the pmem element on success and
+ * -errno on error.
+ */
+static ssize_t pmemfs_write_data(pmem_handle_t handle, const char *buf,
+                size_t count, const loff_t offset)
+{
+	struct pmem_handle	*hdl = (struct pmem_handle*)handle;
+
+	/* update the offset in the opaque handle since pmem_write_data_user()
+	 * relies on it being properly set */
+	hdl->offset = offset;
+
+	return pmem_write_data(handle, buf, count);
+} 
+
+
+/* static ssize_t pmemfs_write_raw_region
+ *
+ * Write to a region in "raw" mode.  This call will not call the pmem io.c
+ * write implementation since that method interprets the write as a log
+ * data append write, which is not what would be desired when the user thinks
+ * they are manipulating the region in raw mode.
+ *
+ * Returns the number of bytes written on success and -errno on 
+ * error.
+ */
+static ssize_t pmemfs_write_raw_region(pmem_handle_t handle, const char *buf,
+                size_t count, const loff_t offset)
+{
+	struct pmem_handle	*hdl = (struct pmem_handle*)handle;
+	void			*data_start;
+	int 			rc;
+
+	/* This routine shares a common set of code with 
+	 * pmemfs_write_split_area() - just the data segment write
+	 * doesnt go through pmem_write_data() */
+
+	if (PMEM_HANDLE_TYPE_REG != hdl->type) {
+		PMEM_DPRINT("ERROR: Invalid handle type %d\n", hdl->type);
+		return -EINVAL;
+	}
+
+	data_start = pmem_get_data_ptr(handle);
+	if (!data_start)  {
+		PMEM_DPRINT("ERROR: Cant get data ptr for region!\n");
+		return -EINVAL;
+	}
+
+	rc = pmemfs_write_ptr(data_start, hdl->size, buf, count, offset);
+
+	if (rc < 0) {
+		PMEM_DPRINT("ERROR: write %d bytes to data rc=%d\n",
+		            (int) count, rc);
+		return rc;
+	}
+
+	return rc;
+} 
+
+/* static ssize_t pmemfs_write_region 
+ *
+ * Write to a regions file - this will pass the call onto the 
+ * pmem_write_data call that knows how to write to both types of
+ * regions.
+ * Returns the number of bytes written on success and -errno on 
+ * error.
+ */
+static ssize_t pmemfs_write_region(pmem_handle_t handle, const char *buf,
+                size_t count, const loff_t offset)
+{
+	struct pmem_handle	*hdl = (struct pmem_handle*)handle;
+
+	if (PMEM_HANDLE_TYPE_REG != hdl->type) {
+		PMEM_DPRINT("ERROR: Invalid handle type %d\n", hdl->type);
+		return -EINVAL;
+	}
+
+	/* update the offset in the opaque handle since pmem_write_data_user()
+	 * relies on it being properly set */
+	hdl->offset = offset;
+
+	return pmem_write_data(handle, buf, count);
+} 
+
+/* static ssize_t pmemfs_write_new_part 
+ *
+ * User is writing in a pmem_reg_part structure in binary mode to the
+ * file.  Will create a new partition with the given data.  This routine
+ * will accept only an exact write of sizeof(struct pmem_reg_part).
+ *
+ * Returns the number of bytes written on success and -errno on 
+ * error.
+ */
+static ssize_t pmemfs_write_new_part(pmem_handle_t handle, const char *buf,
+                size_t count, const loff_t offset)
+{
+	pmem_handle_t		temp_hdl = NULL;    /* needed for pmem_partition_reg */
+	ssize_t			rc = -EINVAL;
+
+	if (NULL != handle) {
+		/* handle should always be null here */
+		PMEM_DPRINT("ERROR: Invalid handle found \n");
+		return rc;
+	}
+	/* only accept _exactly_ one struct of input */
+	if (count != sizeof(struct pmem_reg_part)) {
+		PMEM_DPRINT("ERROR: Invalid size %d for new partition\n",
+		            (int)count);
+		return rc;
+	}
+	/* and only accept write calls starting at offset 0 */
+	if (0 != offset) {
+		PMEM_DPRINT("ERROR: Invalid offset %d for new partition\n",
+		            (int)offset);
+		return rc;
+	}
+
+	/*
+	PMEM_DPRINT("INFO: New part, name=[%s], size=%d, blocks=%d, ver=%d\n",
+	            input.desc, input.size, input.num_blocks, input.version);
+	*/
+	rc = pmem_partition_reg((struct pmem_reg_part*)buf, &temp_hdl);
+	if (0 != rc) {
+		PMEM_DPRINT("ERROR: Unable to register new partition\n");
+		return rc;
+	}
+
+	/* success - but we need to dispose of this kernel handle now */
+	pmem_release_handle(&temp_hdl);
+			
+	return count;	
+} 
+
+/* static ssize_t pmemfs_write_new_region 
+ *
+ * User is writing in a pmem_reg_region structure in binary mode to the
+ * file.  Will create a new region with the given data.  This routine
+ * will accept only an exact write of sizeof(struct pmem_reg_region) at 
+ * offset 0.
+ *
+ * Returns the number of bytes written on success and -errno on 
+ * error.
+ */
+static ssize_t pmemfs_write_new_region(pmem_handle_t handle, const char *buf,
+                size_t count, const loff_t offset)
+{
+	struct pmem_handle	*hdl = (struct pmem_handle*)handle;
+	pmem_handle_t		temp_hdl = NULL;    /* needed for pmem_region_reg */
+	ssize_t			rc = -EINVAL;
+#ifdef CONFIG_PMEM_DEBUG
+	struct pmem_reg_region	*reg_ptr;
+#endif
+
+	if (PMEM_HANDLE_TYPE_PART != hdl->type) {
+		/* handle must be partition handle type */
+		PMEM_DPRINT("ERROR: Invalid handle type %d for new_region\n",
+		             hdl->type);
+		return rc;
+	}
+	/* only accept _exactly_ one struct of input */
+	if (count != sizeof(struct pmem_reg_region)) {
+		PMEM_DPRINT("ERROR: Invalid size %d for new region\n",
+		            (int)count);
+		return rc;
+	}
+	/* and only accept write calls starting at offset 0 */
+	if (0 != offset) {
+		PMEM_DPRINT("ERROR: Invalid offset %d for new region\n",
+		            (int)offset);
+		return rc;
+	}
+
+#ifdef CONFIG_PMEM_DEBUG
+	reg_ptr = (struct pmem_reg_region*) buf;
+
+	PMEM_DPRINT("INFO: New region, name=[%s], size=%d, flags=%d, "
+	            "fixed_size=%d, num_log_desc=%d, ver=%d, block_id=%d\n",
+	            reg_ptr->desc, reg_ptr->size, reg_ptr->flags, reg_ptr->fixed_size,
+	            (int)reg_ptr->num_log_desc, (int)reg_ptr->version,
+	            (int)reg_ptr->block_id);
+#endif
+
+	rc = pmem_region_reg(handle, (struct pmem_reg_region*)buf, &temp_hdl);
+	if (0 != rc) {
+		PMEM_DPRINT("ERROR: Unable to register new region\n");
+		return rc;
+	}
+
+	/* success - but we need to dispose of this kernel handle now */
+	pmem_release_handle(&temp_hdl);
+			
+	return count;	
+} 
+
+/* static ssize_t pmemfs_write_lock_pmem
+ *
+ * User will write a text string  to this control file that will
+ * either lock or unlock all of pmem.  Write a "0\n" (character zero, ascii 48)
+ * to unlock, or a "1\n" to lock.  Anything else will be rejected. 
+ *
+ * Returns the number of bytes written on success and -errno on 
+ * error.
+ */
+static ssize_t pmemfs_write_lock_pmem(pmem_handle_t handle, const char *buf,
+                size_t count, const loff_t offset)
+{
+	ssize_t		rc = -EINVAL;
+	unsigned char	input[3];
+
+	if (NULL != handle) {
+		/* handle should always be null here */
+		PMEM_DPRINT("ERROR: Invalid handle found \n");
+		return rc;
+	}
+	/* only accept one char of input, plus one trailing \n */
+	if (count != sizeof(unsigned char) + 1) {
+		PMEM_DPRINT("ERROR: Invalid size %d for lock pmem\n", 
+		            (int)count);
+		return rc;
+	}
+	/* and only accept write calls starting at offset 0 */
+	if (0 != offset) {
+		PMEM_DPRINT("ERROR: Invalid offset %d for write pmem lock\n",
+		            (int)offset);
+		return rc;
+	}
+
+	/* pull in the input string */
+	memset(input, '\0', 3);
+	memcpy(&input, buf, count);
+
+
+	/* test input against the unlock constant */
+	if (0 == strncmp("1\n", input, 3)) {
+		//PMEM_DPRINT("INFO: User asked to lock global pmem.\n");
+		rc = pmem_lock();
+	} else if (0 == strncmp("0\n", input, 3)){
+		//PMEM_DPRINT("INFO: User asked to unlock global pmem.\n");
+		rc = pmem_unlock();
+	} else {
+		PMEM_DPRINT("ERROR: Unrecognized input for global lock [%s]\n",
+		            input);
+	}
+
+	if (rc < 0) {
+		PMEM_DPRINT("ERROR: lock/unlock failed rc=%d\n", (int)rc);
+		return rc;
+	}
+	
+	return count;	
+} 
+
+/* static ssize_t pmemfs_write_lock_all_active 
+ *
+ * User will write a single text character to this control file that will
+ * either lock or unlock all active segments in pmem. 
+ * Write a 0 (character zero, ascii 48) to unlock, anything else will be
+ * taken as a lock command. A newline character at the end of the input 
+ * will be ignored.
+ *
+ * Returns the number of bytes written on success and -errno on 
+ * error.
+ */
+static ssize_t pmemfs_write_lock_all_active(pmem_handle_t handle,
+		const char *buf, size_t count, const loff_t offset)
+{
+	ssize_t		rc = -EINVAL;
+	unsigned char	input[3];
+
+	if (NULL != handle) {
+		/* handle should always be null here */
+		PMEM_DPRINT("ERROR: Invalid handle found \n");
+		return rc;
+	}
+	/* only accept one char of input +  a trailing \n  */
+	if (count != sizeof(unsigned char) + 1) {
+		PMEM_DPRINT("ERROR: Invalid size %d for lock active pmem\n",
+		            (int)count);
+		return rc;
+	}
+	/* and only accept write calls starting at offset 0 */
+	if (0 != offset) {
+		PMEM_DPRINT("ERROR: Invalid offset %d for write active locks\n",
+		            (int)offset);
+		return rc;
+	}
+
+	/* pull in the input string */
+	memset(input, '\0', 3);
+	memcpy(&input, buf, 2);
+
+	/* test input against the unlock constant */
+	if (0 == strncmp("0\n", input, 3)) {
+		//PMEM_DPRINT("INFO: asked to unlock all active segments.\n");
+		rc = pmem_unlock_all_active_segments();
+	} else if (0 == strncmp("1\n", input, 3)) {
+		//PMEM_DPRINT("INFO: asked to lock all active segments.\n");
+		rc = pmem_lock_all_active_segments();
+	} else {
+		PMEM_DPRINT("ERROR: Invalid lock active segments input [%s]\n",
+		            input);
+	}
+
+	if (rc < 0) {
+		PMEM_DPRINT("ERROR:(un)lock active segments failed rc=%d\n",
+		            (int)rc);
+		return rc;
+	}
+	
+	return count;	
+} 
+
+/* static ssize_t pmemfs_write_rotate_pmem 
+ *
+ * Write the text string "1\n" to this file to rotate all partitions in
+ * pmem and lock the previously active segment. Write the text string 
+ * "0\n" to this file to rotate without locking the currently active segments.
+ * Any other input will be rejected. 
+ *
+ * Returns the number of bytes written on success (2) and -errno on 
+ * error.
+ */
+static ssize_t pmemfs_write_rotate_pmem(pmem_handle_t handle, const char *buf,
+                size_t count, const loff_t offset)
+{
+	ssize_t		rc = -EINVAL;
+	unsigned char	input[3];
+
+	if (NULL != handle) {
+		/* handle should always be null here */
+		PMEM_DPRINT("ERROR: Invalid handle found \n");
+		return rc;
+	}
+
+	/* "0\n" or "1\n" */	
+	if (count != sizeof(unsigned char) + 1) {
+		PMEM_DPRINT("ERROR: Invalid size %d for all part rotate\n",
+		            (int)count);
+		return rc;
+	}
+	/* and only accept write calls starting at offset 0 */
+	if (0 != offset) {
+		PMEM_DPRINT("ERROR: Invalid offset %d for write rotate pmem\n",
+		            (int)offset);
+		return rc;
+	}
+
+	/* grab the input string */
+	memset(&input, '\0', 3);
+	memcpy(&input, buf, 2);
+
+	/* test input against the constant */
+	if (0 == strncmp("1\n", input, 3)) {
+		//PMEM_DPRINT("INFO: asked to rotate all parts with lock.\n");
+		rc = pmem_rotate(1);
+	} else if (0 == strncmp("0\n", input, 3)){
+		//PMEM_DPRINT("INFO: asked to rotate all parts with lock.\n");
+		rc = pmem_rotate(0);
+	} else {
+		PMEM_DPRINT("ERROR: unknown rotate command [%s].\n", input);
+		return rc;
+	}
+
+	if (rc < 0) {
+		PMEM_DPRINT("ERROR: pmem_rotate() failed\n");
+		return rc;
+	}
+	
+	return count;	
+} 
+
+/* static ssize_t pmemfs_write_reset_pmem
+ *
+ * Write the text string "1\n" to this file to set the pmem 'reset' flag
+ * and "0\n" to clear the reset flag.
+ * Any other input will be rejected. 
+ *
+ * Returns the number of bytes written on success (2) and -errno on 
+ * error.
+ */
+static ssize_t pmemfs_write_reset_pmem(pmem_handle_t handle, const char *buf,
+                size_t count, const loff_t offset)
+{
+	const char	*valid = "1\n";		/* valid input string */
+	unsigned char	input[3];
+
+	if (NULL != handle) {
+		/* handle should always be null here */
+		PMEM_DPRINT("ERROR: Invalid handle found \n");
+		return -EINVAL;
+	}
+
+	/* only accept the exact input we are looking for */	
+	if (count != strlen(valid)) {
+		PMEM_DPRINT("ERROR: Invalid size %d for reset pmem.\n",
+		            (int)count);
+		return -EINVAL;
+	}
+	if (0 != offset) {
+		PMEM_DPRINT("ERROR: Invalid offset %d for write reset pmem\n",
+		            (int)offset);
+		return -EINVAL;
+	}
+
+	memset(&input, '\0', 3);
+	memcpy(&input, buf, 2);
+
+	/* test input against the constant */
+	if (0 == strncmp("1\n", input, 3)) {
+		//PMEM_DPRINT("INFO: asked to set reset flag.\n");
+		pmem_set_cb_reset_flag();
+	} else if (0 == strncmp("0\n", input, 3)) {
+		//PMEM_DPRINT("INFO: asked to un-set reset flag.\n");
+		/* Clear the reset flag */
+		pmem_clear_cb_reset_flag();
+	} else {
+		PMEM_DPRINT("ERROR: unknown reset command [%s].\n", input);
+	}
+	
+	return count;	
+} 
+
+/* static ssize_t pmemfs_write_part_lock 
+ *
+ * User will write a text string  to this control file that will
+ * either lock or unlock the partition associated with handle.
+ * Write a "0\n" (character zero, ascii 48) to unlock, or a "1\n" to lock.
+ * Anything else will be rejected. 
+ *
+ * Returns the number of bytes written on success (2) and -errno on 
+ * error.
+ */
+static ssize_t pmemfs_write_part_lock(pmem_handle_t handle, const char *buf,
+                size_t count, const loff_t offset)
+{
+	struct pmem_handle	*hdl = (struct pmem_handle*)handle;
+	struct part_handle	*part;
+	ssize_t			rc = -EINVAL;
+	unsigned char		input[3];
+
+	if (PMEM_HANDLE_TYPE_PART != hdl->type) {
+		/* handle should always be null here */
+		PMEM_DPRINT("ERROR: Invalid handle type %d for lock_part\n",
+		            hdl->type);
+		return rc;
+	}
+	part = (struct part_handle*)hdl;
+
+	/* only accept one char of input, plus one trailing \n */
+	if (count != sizeof(unsigned char) + 1) {
+		PMEM_DPRINT("ERROR: Invalid size %d for lock part\n",
+		            (int)count);
+		return rc;
+	}
+	/* and only accept write calls starting at offset 0 */
+	if (0 != offset) {
+		PMEM_DPRINT("ERROR: Invalid offset %d for write lock part\n",
+		            (int)offset);
+		return rc;
+	}
+
+	/* pull in the input string */
+	memset(input, '\0', 3);
+	memcpy(&input, buf, count);
+
+	/* test input against the unlock constant */
+	if (0 == strncmp("1\n", input, 3)) {
+		//PMEM_DPRINT("INFO: User asked to lock part.\n");
+		rc = pmem_lock_part(handle);
+	} else if (0 == strncmp("0\n", input, 3)){
+		//PMEM_DPRINT("INFO: User asked to unlock part.\n");
+		rc = pmem_unlock_part(handle);
+	} else {
+		PMEM_DPRINT("ERROR: Unrecognized input for part lock [%s]\n",
+		            input);
+	}
+
+	if (rc < 0) {
+		PMEM_DPRINT("ERROR: lock/unlock part failed rc=%d\n", (int)rc);
+		return rc;
+	}
+	
+	return count;	
+} 
+
+/* static ssize_t pmemfs_write_part_rotate 
+ *
+ * Write the text string "1\n" to this file to rotate the partition 
+ * represented by handle and lock the previously active segment.
+ * Write the text string "0\n" to this file to rotate without locking
+ * the currently active segments.
+ * Any other input will be rejected. 
+ *
+ * Returns the number of bytes written on success (2) and -errno on 
+ * error.
+ */
+static ssize_t pmemfs_write_part_rotate(pmem_handle_t handle, const char *buf,
+                size_t count, const loff_t offset)
+{
+	struct pmem_handle	*hdl = (struct pmem_handle*)handle;
+	struct part_handle	*part;
+	ssize_t			rc = -EINVAL;
+	unsigned char		input[3];
+
+	if (PMEM_HANDLE_TYPE_PART != hdl->type) {
+		PMEM_DPRINT("ERROR: Invalid handle type %d\n", hdl->type);
+		return rc;
+	}
+	part = (struct part_handle*)hdl;
+
+	/* "0\n" or "1\n" */
+	if (count != sizeof(unsigned char) + 1) {
+		PMEM_DPRINT("ERROR: Invalid size %d for all part rotate\n",
+		            (int)count);
+		return rc;
+	}
+	/* only accept write calls starting at offset 0 */
+	if (0 != offset) {
+		PMEM_DPRINT("ERROR: Invalid offset %d for write rotate part\n",
+		            (int)offset);
+		return rc;
+	}
+
+	/* grab the input string */
+	memset(&input, '\0', 3);
+	memcpy(&input, buf, 2);
+
+	/* test input against the constant */
+	if (0 == strncmp("1\n", input, 3)) {
+		//PMEM_DPRINT("INFO: asked to rotate part with lock.\n");
+		rc = pmem_part_rotate(handle, 1);
+	} else if (0 == strncmp("0\n", input, 3)){
+		//PMEM_DPRINT("INFO: asked to rotate part without lock.\n");
+		rc = pmem_part_rotate(handle, 0);
+	} else {
+		PMEM_DPRINT("ERROR: unknown rotate command [%s].\n", input);
+		return rc;
+	}
+
+	if (rc < 0) {
+		PMEM_DPRINT("ERROR: pmem_rotate() failed = %d\n", (int)rc);
+		return rc;
+	}
+	
+	return count;	
+} 
+
+/* static ssize_t pmemfs_read_active_segment 
+ *
+ * Deposit the currently active segment in text format into the supplied 
+ * kernel space buffer.
+ *
+ * Returns number of bytes read into buf (3) on success, -errno on error.
+ */
+static ssize_t pmemfs_read_active_segment(pmem_handle_t handle, char *buf,
+                size_t count, const loff_t offset)
+{
+	struct pmem_handle	*hdl = (struct pmem_handle*)handle;
+	struct part_handle	*part;
+	char 			id[32];
+	ssize_t			len;
+	ssize_t			bytes_read = -EINVAL;
+
+	if (PMEM_HANDLE_TYPE_PART != hdl->type) {
+		PMEM_DPRINT("ERROR: Invalid handle type %d\n", hdl->type);
+		return bytes_read;
+	}
+	part = (struct part_handle*)hdl;
+
+	len = snprintf(id, 31, "%d\n",
+	               pmem_get_active_block_index(part->elem->hdr));
+	id[len++] = '\0'; 		/* ensure null termination */
+
+	if (offset > len) {
+		return 0;
+	}
+	
+	bytes_read = count;
+	if (offset + count > len) {
+		bytes_read = len - offset;
+	}
+
+	memcpy(buf, id + offset, bytes_read);
+
+	return bytes_read;
+} 
+
+/* static ssize_t pmemfs_write_seg_lock 
+ *
+ * User will write a text string  to this control file that will
+ * either lock or unlock the segment associated with handle.
+ * Write a "0\n" (character zero, ascii 48) to unlock, or a "1\n" to lock.
+ * Anything else will be rejected. 
+ *
+ * Returns the number of bytes written on success (2) and -errno on 
+ * error.
+ */
+static ssize_t pmemfs_write_seg_lock(pmem_handle_t handle, const char *buf,
+                size_t count, const loff_t offset)
+{
+	struct pmem_handle	*hdl = (struct pmem_handle*)handle;
+	ssize_t			rc = -EINVAL;
+	unsigned char		input[3];
+
+	if (PMEM_HANDLE_TYPE_BLK != hdl->type) {
+		PMEM_DPRINT("ERROR: Invalid handle type %d for seg_lock\n",
+		            hdl->type);
+		return rc;
+	}
+
+	/* only accept one char of input, plus one trailing \n */
+	if (count != sizeof(unsigned char) + 1) {
+		PMEM_DPRINT("ERROR: Invalid size %d for lock segment\n",
+		            (int)count);
+		return rc;
+	}
+	/* and only accept write calls starting at offset 0 */
+	if (0 != offset) {
+		PMEM_DPRINT("ERROR: Invalid offset %d for write segment lock\n",
+		            (int)offset);
+		return rc;
+	}
+
+	/* pull in the input string */
+	memset(input, '\0', 3);
+	memcpy(&input, buf, count);
+
+	/* test input against the unlock constant */
+	if (0 == strncmp("1\n", input, 3)) {
+		//PMEM_DPRINT("INFO: User asked to lock segment.\n");
+		rc = pmem_lock_block(handle);
+	} else if (0 == strncmp("0\n", input, 3)){
+		//PMEM_DPRINT("INFO: User asked to unlock segment.\n");
+		rc = pmem_unlock_block(handle);
+	} else {
+		PMEM_DPRINT("ERROR: Unrecognized input for segment lock [%s]\n",
+		            input);
+		return rc;
+	}
+
+	if (rc < 0) {
+		PMEM_DPRINT("ERROR: lock/unlock segment failed\n");
+		return rc;
+	}
+	
+	return count;	
+} 
+
+/********************************************************************
+ *
+ * pmemfs internal 'worker' functions
+ *
+ *******************************************************************/
+/* static void *pmemfs_alloc_mem 
+ * Allocate memory with kmalloc if possible. This speeds up 
+ * driver operations immensely */
+static void *pmemfs_alloc_mem(ssize_t size)
+{
+	void *p;
+
+	if (size > PAGE_SIZE)
+		p = vmalloc(size);
+	else
+		p = kmalloc(size, GFP_KERNEL);
+	//PMEM_DPRINT("ALLOC_MEM: alloc %p with size %d\n", p, size);
+	return p;
+} 
+
+/* static void pmemfs_free_mem 
+ * Free the memory with the appropriate function */
+static void pmemfs_free_mem(void *mem, ssize_t size)
+{
+	//PMEM_DPRINT("ALLOC_MEM: Freeing %p with size %d\n", mem, size);
+	if (size > PAGE_SIZE)
+		vfree(mem);
+	else
+		kfree(mem);
+} 
+
+/* static void *pmemfs_alloc_finfo 
+ *
+ * Allocate and fill a pmemfs_file_info structure given the values for
+ * handle, type, read and write.  Arguments may be NULL.  The caller has
+ * the responsibility to kfree() the returned structure.
+ * Returns a pointer to the allocated structure on sucess and NULL on 
+ * error. */
+static struct pmemfs_file_info *pmemfs_alloc_finfo(pmem_handle_t handle,
+                pmemfs_file_t type, pmem_read_fn read, pmem_write_fn write)
+{
+	struct pmemfs_file_info	*f_info;
+	
+	f_info = (struct pmemfs_file_info*)kmalloc(
+	                sizeof(struct pmemfs_file_info), GFP_KERNEL);
+	if (!f_info) {
+		PMEM_DPRINT("ERROR: Failed to kmalloc pmemfs_file_info\n");
+		return NULL;
+	}
+	f_info->handle = handle;
+	f_info->type = type;
+	f_info->read = read;
+	f_info->write = write;
+
+	/* Don't let anything else hold on to this handle */
+	handle = NULL;
+
+	return f_info;
+} 
+
+/* static void pmemfs_free_finfo 
+ *
+ * Destroy a pmemfs_file_info structure and release all resources
+ * associated with it.  */
+static void pmemfs_free_finfo(struct pmemfs_file_info *f_info)
+{
+	if (likely(f_info)) {
+		if (likely(f_info->handle)) {
+			pmem_release_handle((pmem_handle_t *) &f_info->handle);
+		}
+		kfree(f_info);
+		f_info = NULL;
+	}
+} 
+
+/*  static char *pmemfs_build_segment_name 
+ *
+ *  Build a string that will be the name of the directory for the given 
+ *  segment id.
+ *  The string that is returned is allocated by kmalloc() and it is the 
+ *  responsibility of the caller to the free the memory when finished.
+ */
+static char *pmemfs_build_segment_name(int segno)
+{
+	char *buf = NULL;
+	int len = 0;
+
+	if (segno >= 0) {
+		/* FIXME: WR - dont need to allocate this much ram - wastage */
+		buf = (char*)kmalloc(NAME_MAX + 1, GFP_KERNEL);
+		if (!buf) {
+			printk(KERN_ERR "pmemfs_build_segment_name: Unable to\
+					allocate memory\n");
+			return buf;
+		}
+		len = snprintf(buf, NAME_MAX, "%s%d", PMEMFS_SEGMENT_PREFIX,
+				segno);
+		buf[len] = '\0';   /* ensure NULL termination */
+	}
+	else {
+		printk(KERN_ERR "ERROR: pmemfs_build_segment_name invalid \
+				segment=%d\n", segno);
+	}
+	return buf;
+} 
+
+/* static int pmemfs_add_region_to_segment 
+ *
+ * Adds the given region to the given segment.  Registers the appropriate
+ * VFS directories and files for the region in the filesystem location that
+ * belongs to the given segment.
+ * Returns 0 upon successful setup, -ERRNO on error
+ *
+ * sb - superblock of the filesystem to create inodes on
+ * seg_dentry - directory of the parent segment this region lives in
+ * parent_handle - opaque pmem handle to the partition the segment lives in
+ *                 this handle is needed to call pmem_region_get() to get
+ *                 the opaque pmem_handle_t for the inode private data
+ * region_name - name of the region to create
+ * block_id - block_id of the segment that this region is being added to.
+ *
+ */
+static int pmemfs_add_region_to_segment(struct super_block *sb,
+                struct dentry *seg_dentry,
+                pmem_handle_t parent_handle,
+                const char *region_name,
+                int block_id)
+{
+	struct pmem_handle	*hdl = (struct pmem_handle*)parent_handle;
+	struct pmemfs_file_info	*f_info;
+	struct region_handle	*region_handle;
+	struct dentry		*file_dentry, *reg_dentry;
+	pmem_handle_t		handle = NULL;
+	int 			rc = -EINVAL;
+
+	if (PMEM_HANDLE_TYPE_PART != hdl->type) {
+		PMEM_DPRINT("ERROR: Invalid handle type %d\n", hdl->type);
+		return rc;
+	}
+
+	/* create the directory for the region inside this segment */
+	reg_dentry = pmemfs_create_dir(sb, seg_dentry, region_name);
+	if (!reg_dentry) {
+		PMEM_DPRINT("ERROR: Failed to create dir for [%s]\n",
+		            region_name);
+		return rc;
+	}
+	
+	/* create the raw data access file for this region */ 
+	if (pmem_region_get(parent_handle, (char*)region_name, block_id,
+	                    &handle)) {
+		PMEM_DPRINT("ERROR: Cant get region handle [%s]\n",region_name);
+		return rc;
+	}
+	f_info = pmemfs_alloc_finfo(handle, PMEMFS_REG_RAW_DATA,
+	                            pmemfs_read_region,
+				    pmemfs_write_raw_region);
+	if (!f_info) {
+		PMEM_DPRINT("ERROR: Failed to allocate file_info\n");
+		return rc;
+	}
+
+	file_dentry = pmemfs_create_file(sb, reg_dentry,
+	                                 PMEMFS_REGION_PREFIX PMEMFS_RAW_DATA_SUFFIX, f_info,
+					 PMEMFS_PERM_RW,
+					 ((struct pmem_handle*)(handle))->size);
+	if (!file_dentry) {
+		PMEM_DPRINT("ERROR: Failed to create raw access file for region [%s]\n", region_name);
+		kfree(f_info);
+		return rc;
+	}
+
+	handle = NULL; /* let VFS reclaim memory on umount */
+
+	/* need to know what kind of region this is.  Convert the opaque
+	 * handle to let us look at the region_info */
+	region_handle = (struct region_handle*)f_info->handle;
+
+	/* WARNING: percpu regions */
+	
+	/* call pmem_region_get() twice to get 2 copies of the 'same' handle
+	 * for the 2 inodes.  Can't use the same handle twice since inode
+	 * destroy will dispose of the handle */
+	if (pmem_region_get(parent_handle, (char*)region_name, block_id,
+	                    &handle)) {
+		PMEM_DPRINT("ERROR: Cant get region handle [%s]\n",region_name);
+		return rc;
+	}
+
+	/* create the access to the region data / log data  - defaults to byte
+	 * access. */
+	f_info = pmemfs_alloc_finfo(handle, PMEMFS_REG_BYTE, pmemfs_read_region,
+	                            pmemfs_write_region);
+	if (!f_info) {
+		PMEM_DPRINT("ERROR: Failed kmalloc for file_info\n");
+		return rc;
+	}
+
+	if (region_handle->elem->hdr->data.num_log_desc) {
+		/* override default type and name for a log descriptor region */
+		f_info->type = PMEMFS_REG_RECORD;
+	}
+	file_dentry = pmemfs_create_file(sb, reg_dentry, PMEMFS_REGION_PREFIX PMEMFS_DATA_SUFFIX,
+	                                 f_info, PMEMFS_PERM_RW,
+	                                 ((struct pmem_handle*)(handle))->size);
+	if (!file_dentry) {
+		PMEM_DPRINT("ERROR: Failed to create region data access file for region [%s]\n", region_name);
+		kfree(f_info);
+		return rc;
+	}
+
+	handle = NULL; /* let VFS reclaim memory on umount */
+
+	/* call pmem_region_get() a third time to get 3 copies of the 'same' handle
+	 * for the 3 inodes.  Can't use the same handle repeatedly since inode
+	 * destroy will dispose of the handle */
+	if (pmem_region_get(parent_handle, (char*)region_name, block_id,
+	                    &handle)) {
+		PMEM_DPRINT("ERROR: Cant get region handle [%s]\n",region_name);
+		return rc;
+	}
+
+	/* create the access to the region header */
+	f_info = pmemfs_alloc_finfo(handle, PMEMFS_REG_HEADER, pmemfs_read_header,
+	                            pmemfs_write_header);
+	if (!f_info) {
+		PMEM_DPRINT("ERROR: Failed kmalloc for file_info\n");
+		return rc;
+	}
+
+	file_dentry = pmemfs_create_file(sb, reg_dentry, PMEMFS_REGION_PREFIX PMEMFS_HEADER_SUFFIX,
+	                                 f_info, PMEMFS_PERM_RW,
+	                                 pmem_get_hdr_size((struct pmem_handle*) handle));
+	if (!file_dentry) {
+		PMEM_DPRINT("ERROR: Failed to create region header access file for region [%s]\n", region_name);
+		kfree(f_info);
+		return rc;
+	}
+
+	handle = NULL; /* let VFS reclaim memory on umount */
+
+	return 0;		/* if we get this far, its a green light */
+} 
+
+/* static int pmemfs_add_region_to_part 
+ * Install the subtree that represents a region into the proper place in 
+ * the filesystem tree.  The parent partition is given so that we know
+ * where the regions belong.
+ * Assumes that the directories for the segments have already been created
+ * for the parent partition.
+ * Returns 0 on success, and -errno on error.
+ *
+ * sb - superblock of the filesystem where inodes will be created
+ * parent_hdl - opaque pmem handle for the partition this region lives in
+ * region_hdl - opaque pmem handle that represents the region to represent
+ */
+static int pmemfs_add_region_to_part(struct super_block *sb,
+                pmem_handle_t parent_hdl,
+                pmem_handle_t region_hdl)
+{
+	struct pmem_handle	*p_hdl = (struct pmem_handle*)parent_hdl;
+	struct pmem_handle	*r_hdl = (struct pmem_handle*)region_hdl;
+	struct part_handle	*parent_handle;
+	struct region_handle	*region_handle;
+	struct dentry		*part_dentry, *seg_dentry;
+	int			i;
+	char			*seg_name;
+	char			desc[PMEM_DESC_MAX];
+	int			rc = -EINVAL;
+
+	if (PMEM_HANDLE_TYPE_PART != p_hdl->type) {
+		PMEM_DPRINT("ERROR: Invalid handle type %d\n", p_hdl->type);
+		return rc;
+	}
+	if (PMEM_HANDLE_TYPE_REG != r_hdl->type) {
+		PMEM_DPRINT("ERROR: Invalid handle type %d\n", r_hdl->type);
+		return rc;
+	}
+
+	parent_handle = (struct part_handle*)p_hdl;
+	region_handle = (struct region_handle*)r_hdl;
+
+	part_dentry = pmemfs_find_dentry(sb->s_root,
+	                  parent_handle->elem->hdr->data.desc);
+	if (!part_dentry) {
+		PMEM_DPRINT("ERROR: Failed to find dentry for part [%s]\n",
+		            parent_handle->elem->hdr->data.desc);
+		return rc;
+	}
+	if (parent_handle->elem->hdr->data.num_blocks <= 0) {
+		PMEM_DPRINT("ERROR: Can't add region to partition without blocks\n");
+		return rc;
+	}
+
+	/* the region goes in every segment in the parent partition, and
+	 * the active segment  - this is done by cycling over the block id's
+	 * starting at the PMEM_ACTIVE_BLOCK (-1).
+	 *
+	 * HACK: This is dirty since we are abusing the fact that 
+	 * PMEM_ACTIVE_BLOCK is set to -1.  If this changes in the future,
+	 * then this code will need to be updated */
+#if ( -1 != PMEM_ACTIVE_BLOCK )
+#error PMEM_ACTIVE_BLOCK changed value: code must be updated
+#endif
+	for (i = -1; i < parent_handle->elem->hdr->data.num_blocks; i++) {
+		/* generate the 'path' for this segment */
+		if (i < 0) {
+			/* this is 'active' segment */
+			seg_name = (char*)PMEMFS_ACTIVE_REGION_DIR;
+		} else {
+			seg_name = pmemfs_build_segment_name(i);
+			if (!seg_name) {
+				printk(KERN_ERR "ERROR: unable to get segment "
+				       "name\n");
+				dput(part_dentry);
+				return rc;
+			}
+		}
+	
+		seg_dentry = pmemfs_find_dentry(part_dentry, seg_name);
+		if (!seg_dentry) {
+			PMEM_DPRINT("ERROR: failed to find dentry for segment [%s]\n", seg_name);
+			if (i >= 0) {
+				kfree(seg_name);
+			}
+			dput(part_dentry);
+			return rc;
+		}
+		if (i >= 0) {
+			/* seg_name is kmallocd for non-active seg */
+			kfree(seg_name);
+		}
+
+		/* make a local copy from pmem of the name of this
+		 * directory - the dcache uses plain old memcpy() */
+		memset(desc, '\0', PMEM_DESC_MAX);
+		__pmem_memcpy_fromio(desc, region_handle->elem->hdr->data.desc, PMEM_DESC_MAX);
+
+		rc = pmemfs_add_region_to_segment(sb, seg_dentry, parent_hdl, desc, i);
+		dput(seg_dentry); 	/* dont need dentry anymore */
+		if (rc) {
+			PMEM_DPRINT("ERROR: Failed to add region to segment \
+			             [%s]\n", seg_name);
+			break;
+		}
+	}
+	dput(part_dentry);	/* done with part dentry, decrement refcount */
+	return rc;
+} 
+
+/* static int pmemfs_part_populate_segments 
+ *
+ * Popluate the VFS tree for the segments that belong under the given
+ * partition.  This is a helper routine used by pmemfs_add_partition
+ * Assumes that there is at least _one_ segment in the given partition.
+ * Returns 0 on success, -errno on error
+ *
+ * sb - superblock of the pmemfs filesystem (needed for inode creation)
+ * part_handle - opaque handle for parent partition
+ * root - dentry for the parent, where this subtree will be rooted in VFS
+ */
+static int pmemfs_part_populate_segments(struct super_block *sb,
+           pmem_handle_t handle, struct dentry *root)
+{
+	struct pmem_handle	*hdl = (struct pmem_handle*)handle;
+	struct part_handle	*part_handle = NULL;
+	struct pmem_region_hdr	*reg_hdr;
+	struct pmemfs_file_info	*f_info;
+	struct dentry		*seg_dir, *ctrl_dir;
+	pmem_handle_t		seg_hdl = NULL;
+	unsigned char		*seg_name;
+	char			desc[PMEM_DESC_MAX];
+	int			i, j, fsize;
+	int			rc = -EINVAL;
+
+	if (PMEM_HANDLE_TYPE_PART != hdl->type) {
+		PMEM_DPRINT("ERROR: Invalid handle type %d\n", hdl->type);
+		return rc;
+	}
+
+	part_handle = (struct part_handle*)hdl;
+	/* ensure that there is at least 1 segment on this part */
+	if (part_handle->elem->hdr->data.num_blocks <= 0) {
+		PMEM_DPRINT("ERROR: No segments in partition\n");
+		return rc;
+	}
+
+	/* create directories and files for the segments level inside 
+	 * this partition
+	 * NOTE: This loop does one extra iteration to generate the dir
+	 * for the 'active' segment.*/
+	for (i = -1; i < part_handle->elem->hdr->data.num_blocks; i++) {
+		if (i < 0) {
+			seg_name = (char*)PMEMFS_ACTIVE_REGION_DIR;
+		} else {
+			seg_name = pmemfs_build_segment_name(i);
+			if (!seg_name) {
+				PMEM_DPRINT("ERROR: unable to get segment name\n");
+				return rc;
+			}
+		}
+		/* create the directory for the segment on this part */
+		seg_dir = pmemfs_create_dir(sb, root, seg_name);
+		if (!seg_dir) {
+			PMEM_DPRINT("ERROR: failed to create dir [%s/%s]\n",
+			            part_handle->elem->hdr->data.desc,
+			            seg_name);
+			return rc;
+		}
+		
+		ctrl_dir = pmemfs_create_dir(sb, seg_dir, PMEMFS_CONTROL_DIR);
+		if (!ctrl_dir) {
+			PMEM_DPRINT("ERROR: Failed to create control dir for "
+			            "segment [%s]\n",
+			            part_handle->elem->hdr->data.desc);
+			return rc;
+		}
+		/* get a pmem handle for this segment - need it for the 
+		 * internal inode private data structure */
+		if (pmem_block_get(handle, i, &seg_hdl)) {
+			PMEM_DPRINT("ERROR: Unable to get handle for block"
+			            "[%s]\n", seg_name);
+			return rc;
+		}
+		
+		/* control file to lock/unlock this segment */
+		f_info = pmemfs_alloc_finfo(seg_hdl, PMEMFS_LOCK_SEG, NULL,
+		                            pmemfs_write_seg_lock);
+		if (!f_info) {
+			PMEM_DPRINT("ERROR: Cant allocate f_info\n");
+			pmem_release_handle(&seg_hdl);
+			return rc;
+		}
+		fsize = 0; //((struct pmem_handle*)(seg_hdl))->size;
+		if (!pmemfs_create_file(sb, ctrl_dir, CTRL_LOCK_FILE, f_info,
+		                        PMEMFS_PERM_RW, fsize )) {
+			PMEM_DPRINT("ERROR: Unable to create lock control file "
+			            "for segment [%s]\n", seg_name);
+			kfree(f_info);
+			return rc;
+		}
+
+		seg_hdl = NULL; /* let VFS reclaim memory on umount */
+
+		/* WARNING: only pass each pmem_handle_t to pmemfs_alloc_finfo
+		 * once.  It will be free()d when shutting down the fs */
+		if (pmem_block_get(handle, i, &seg_hdl)) {
+			PMEM_DPRINT("ERROR: Unable to get handle for block"
+			            "[%s]\n", seg_name);
+			return rc;
+		}
+		/* create the 'access to a segment header' file */
+		f_info = pmemfs_alloc_finfo(seg_hdl, PMEMFS_SEG_HEADER,
+		                            pmemfs_read_header,
+					    pmemfs_write_header);
+		if (!f_info) {
+			PMEM_DPRINT("ERROR: Cant allocate f_info\n");
+			pmem_release_handle(&seg_hdl);
+			return rc;
+		}
+	        fsize = pmem_get_hdr_size((struct pmem_handle*) seg_hdl);
+		if (!pmemfs_create_file(sb, seg_dir, 
+		        PMEMFS_SEGMENT_PREFIX PMEMFS_HEADER_SUFFIX,
+		        f_info, PMEMFS_PERM_RW, fsize)) {
+			PMEM_DPRINT("ERROR: Unable to create access to "
+			            "segment header [%s]\n", seg_name);
+			kfree(f_info);
+			return rc;
+		}
+		seg_hdl = NULL; /* let VFS reclaim memory on umount */
+
+		/* WARNING: only pass each pmem_handle_t to pmemfs_alloc_finfo
+		 * once.  It will be free()d when shutting down the fs */
+		if (pmem_block_get(handle, i, &seg_hdl)) {
+			PMEM_DPRINT("ERROR: Unable to get handle for block"
+			            "[%s]\n", seg_name);
+			return rc;
+		}
+		/* create the 'access to a segment data' file */
+		f_info = pmemfs_alloc_finfo(seg_hdl, PMEMFS_SEG_DATA,
+		                            pmemfs_read_data,
+					    pmemfs_write_data);
+		if (!f_info) {
+			PMEM_DPRINT("ERROR: Cant allocate f_info\n");
+			pmem_release_handle(&seg_hdl);
+			return rc;
+		}
+
+		fsize = ((struct pmem_handle*) (seg_hdl))->size;
+		if (!pmemfs_create_file(sb, seg_dir, 
+		        PMEMFS_SEGMENT_PREFIX PMEMFS_DATA_SUFFIX,
+		        f_info, PMEMFS_PERM_RW, fsize)) {
+			PMEM_DPRINT("ERROR: Unable to create access to "
+			            "segment data [%s]\n", seg_name);
+			kfree(f_info);
+			return rc;
+		}
+
+		if (i >= 0) {
+			/* seg_name is only kmallocd for non-active seg */
+			kfree(seg_name);
+		}
+
+		seg_hdl = NULL; /* let VFS reclaim memory on umount */
+
+		/* now we have the segment dirs, fill in the 
+		 * region dirs/files for this segment*/
+		for (j = 0; j < part_handle->elem->hdr->data.num_regions; j++) {
+			reg_hdr = PMEM_GET_REGION_HDR(part_handle->elem->hdr,
+			                              j);
+			/* make a local copy from pmem of the name of this
+			 * directory - the dcache uses plain old memcpy() */
+			memset(desc, '\0', PMEM_DESC_MAX);
+			__pmem_memcpy_fromio(desc, reg_hdr->data.desc, PMEM_DESC_MAX);
+			if (pmemfs_add_region_to_segment(sb, seg_dir, handle,
+			                                 desc, i)) {
+				PMEM_DPRINT("ERROR: Unable to add region [%s] to partition.\n", desc);
+				return rc;
+			}
+		}
+	}
+
+	/* if we get to here, everything is fine */
+	return 0;
+} 
+
+/* static int pmemfs_add_partition 
+ *
+ * Install the 'subtree' that represent a partition into the VFS tree.
+ * This routine will recursively add all the files and directories that 
+ * belong under the parition - current segments and regions will be added
+ * to the tree.
+ * Returns 0 on success, -errno on error.
+ *
+ * sb - superblock for the pmemfs filesystem
+ * handle - opaque pmem handle representing the partition to be added
+ */
+static int pmemfs_add_partition(struct super_block *sb,
+		pmem_handle_t handle)
+{
+	struct pmem_handle	*hdl = (struct pmem_handle *) handle;
+	struct part_handle	*part_handle;
+	struct pmem_part_hdr	*part_hdr;
+	struct dentry		*part_dir, *ctrl_dir;
+	struct pmemfs_file_info	*f_info;
+	pmem_handle_t		inode_hdl = NULL;
+	int			fsize;
+	static char		desc[PMEM_DESC_MAX];
+	int			rc = -EINVAL;
+
+	if (PMEM_HANDLE_TYPE_PART != hdl->type) {
+		PMEM_DPRINT("ERROR: Invalid handle type %d\n", hdl->type);
+		return rc;
+	}
+
+	/* convenience handles */
+	part_handle = (struct part_handle *) hdl;
+	part_hdr = part_handle->elem->hdr;
+
+	/* Need to copy the name of the partition into a local buffer
+	 * so that the memcpy() inside d_alloc() in pmemfs_create_dir()
+	 * will be able to read the string. */
+	memset(desc, '\0', PMEM_DESC_MAX);
+	__pmem_memcpy_fromio(desc, part_hdr->data.desc, PMEM_DESC_MAX);
+
+	/* create the top level partition dir */
+	part_dir = pmemfs_create_dir(sb, sb->s_root, desc);
+	if (!part_dir) {
+		PMEM_DPRINT("ERROR: failed to create directory [%s]\n",
+		            desc);
+		return rc;
+	}
+	
+	/* create the "control" dir and its contents for this part */
+	ctrl_dir = pmemfs_create_dir(sb, part_dir, PMEMFS_CONTROL_DIR);
+	if (!ctrl_dir) {
+		PMEM_DPRINT("ERROR: Cant create control dir on part [%s]\n",
+				part_hdr->data.desc);
+		return rc;
+	}
+
+	/* new region control file. */
+	if (pmem_partition_get(part_hdr->data.desc, &inode_hdl)) {
+		PMEM_DPRINT("ERROR: Failed to get handle for part [%s]\n",
+		             part_hdr->data.desc);
+		return rc;
+	}
+	f_info = pmemfs_alloc_finfo(inode_hdl, PMEMFS_PART_NEW_REG, NULL,
+	                            pmemfs_write_new_region);
+	if (!f_info) {
+		PMEM_DPRINT("ERROR: Failed to create new region file\n");
+		return rc;
+	}
+	if (!pmemfs_create_file(sb, ctrl_dir, CTRL_NEW_REGION_FILE,
+	               f_info, PMEMFS_PERM_RW, 0)) {
+		PMEM_DPRINT("ERROR: new region control file failure\n");
+		kfree(f_info);
+		return rc;
+	}
+
+	inode_hdl = NULL; /* let VFS reclaim memory on umount */
+
+	/* WARNING: only pass each pmem_handle_t to pmemfs_alloc_finfo
+	 * once.  It will be free()d when shutting down the fs */
+
+	/* lock file for this partition */
+	if (pmem_partition_get(part_hdr->data.desc, &inode_hdl)) {
+		PMEM_DPRINT("ERROR: Failed to get handle for part [%s]\n",
+		            part_hdr->data.desc);
+		return rc;
+	}
+
+	f_info = pmemfs_alloc_finfo(inode_hdl, PMEMFS_LOCK_PART, NULL,
+	                            pmemfs_write_part_lock);
+	if (!f_info) {
+		PMEM_DPRINT("ERROR: failed to allocate inode data\n");
+		return rc;
+	}
+	if (!pmemfs_create_file(sb, ctrl_dir, CTRL_LOCK_FILE, f_info,
+	                        PMEMFS_PERM_RW, 0)) {
+		PMEM_DPRINT("ERROR: Unable to create lock file for part [%s]\n",
+		             part_hdr->data.desc);
+		kfree(f_info);
+		return rc;
+	}
+
+	inode_hdl = NULL; /* let VFS reclaim memory on umount */
+
+	/* rotate file for this partition */
+	if (pmem_partition_get(part_hdr->data.desc, &inode_hdl)) {
+		PMEM_DPRINT("ERROR: Failed to get handle for part [%s]\n",
+		             part_hdr->data.desc);
+		return rc;
+	}
+
+	f_info = pmemfs_alloc_finfo(inode_hdl, PMEMFS_PART_ROTATE, NULL,
+	                            pmemfs_write_part_rotate);
+	if (!f_info) {
+		PMEM_DPRINT("ERROR: failed to allocate inode data\n");
+		return rc;
+	}
+	if (!pmemfs_create_file(sb, ctrl_dir, CTRL_ROTATE_FILE, f_info,
+	                        PMEMFS_PERM_RW, 0)) {
+		PMEM_DPRINT("ERROR: Unable to create rotate file for"
+		            "part [%s]\n", part_hdr->data.desc);
+		kfree(f_info);
+		return rc;
+	}
+	
+	inode_hdl = NULL; /* let VFS reclaim memory on umount */
+
+	/* finish the control dir with the active_segment file  */
+	if (pmem_partition_get(part_hdr->data.desc, &inode_hdl)) {
+		PMEM_DPRINT("ERROR: Failed to get handle for part [%s]\n",
+		             part_hdr->data.desc);
+		return rc;
+	}
+
+	f_info = pmemfs_alloc_finfo(inode_hdl, PMEMFS_ACTIVE_SEGMENT,
+	                            pmemfs_read_active_segment, NULL);
+	if (!f_info) {
+		PMEM_DPRINT("ERROR: failed to allocate inode data\n");
+		return rc;
+	}
+	if (!pmemfs_create_file(sb, ctrl_dir, CTRL_ACTIVE_SEGMENT_FILE,
+	                        f_info, PMEMFS_PERM_RO, 0)) {
+		PMEM_DPRINT("ERROR: Unable to create active segment file for"
+		            "part [%s]\n", part_hdr->data.desc);
+		kfree(f_info);
+		return rc;
+	}
+	
+	inode_hdl = NULL; /* let VFS reclaim memory on umount */
+
+	/* create the raw partition access file */
+	if (pmem_partition_get(part_hdr->data.desc, &inode_hdl)) {
+		PMEM_DPRINT("ERROR: Failed to get handle for part [%s]\n",
+		            part_hdr->data.desc);
+		return rc;
+	}
+	f_info = pmemfs_alloc_finfo(inode_hdl, PMEMFS_RAW_PART,
+	                            pmemfs_read_raw_partition,
+				    pmemfs_write_raw_partition);
+	if (!f_info) {
+		PMEM_DPRINT("ERROR: failed to allocate inode data\n");
+		return rc;
+	}
+
+	fsize = part_hdr->data.size + PMEM_PART_HDR_MAX_SIZE;
+	if (!pmemfs_create_file(sb, part_dir, PMEMFS_RAW_ACCESS_FILE, f_info,
+	                        PMEMFS_PERM_RW, fsize)) {
+		PMEM_DPRINT("ERROR: Unable to create raw access to part [%s]\n",
+		            part_hdr->data.desc);
+		kfree(f_info);
+		return rc;
+	}
+
+	inode_hdl = NULL; /* let VFS reclaim memory on umount */
+
+	/* create the partiton header access file */
+	if (pmem_partition_get(part_hdr->data.desc, &inode_hdl)) {
+		PMEM_DPRINT("ERROR: Failed to get handle for part [%s]\n",
+		            part_hdr->data.desc);
+		return rc;
+	}
+	f_info = pmemfs_alloc_finfo(inode_hdl, PMEMFS_PART_HEADER,
+	                            pmemfs_read_header,
+				    pmemfs_write_header);
+	if (!f_info) {
+		PMEM_DPRINT("ERROR: failed to allocate inode data\n");
+		return rc;
+	}
+	fsize = pmem_get_hdr_size(inode_hdl);
+	if (!pmemfs_create_file(sb, part_dir, PMEMFS_PARTITION_PREFIX PMEMFS_HEADER_SUFFIX, f_info,
+	                        PMEMFS_PERM_RW, fsize)) {
+		PMEM_DPRINT("ERROR: Unable to create header access to part [%s]\n",
+		            part_hdr->data.desc);
+		kfree(f_info);
+		return rc;
+	}
+
+	inode_hdl = NULL; /* let VFS reclaim memory on umount */
+
+	/* create the partiton data access file */
+	if (pmem_partition_get(part_hdr->data.desc, &inode_hdl)) {
+		PMEM_DPRINT("ERROR: Failed to get handle for part [%s]\n",
+		            part_hdr->data.desc);
+		return rc;
+	}
+	f_info = pmemfs_alloc_finfo(inode_hdl, PMEMFS_PART_DATA,
+	                            pmemfs_read_data,
+				    pmemfs_write_data);
+	if (!f_info) {
+		PMEM_DPRINT("ERROR: failed to allocate inode data\n");
+		return rc;
+	}
+
+	fsize = ((struct pmem_handle*)(inode_hdl))->size;
+	if (!pmemfs_create_file(sb, part_dir, PMEMFS_PARTITION_PREFIX PMEMFS_DATA_SUFFIX, f_info,
+	                        PMEMFS_PERM_RW, fsize)) {
+		PMEM_DPRINT("ERROR: Unable to create data access to part [%s]\n",
+		            part_hdr->data.desc);
+		kfree(f_info);
+		return rc;
+	}
+
+	inode_hdl = NULL; /* let VFS reclaim memory on umount */
+
+	/* if there are no segments, then we are done.  Dont want to present
+	 * an active segment since there isnt any */
+	if (part_hdr->data.num_blocks <= 0) {
+		return 0;
+	}
+	/* populate the segments under this partition */
+	rc = pmemfs_part_populate_segments(sb, f_info->handle, part_dir);
+	
+	return rc;
+} 
+
+/* static int pmemfs_create_top_control_dir 
+*
+* Create the top level directory that holds the files that are used to 
+* configure/control pmem from userspace.  Creates the directory and
+* populates it with the appropriate files.
+* Returns 0 on success, -errno on error.
+*/
+static int pmemfs_create_top_control_dir(struct super_block *sb)
+{
+	int 			rc = -EINVAL;
+	struct dentry		*control_dir, *dentry;
+	struct pmemfs_file_info	*f_info;
+
+
+	/* create the top level "control" directory */
+	control_dir = pmemfs_create_dir(sb, sb->s_root, PMEMFS_CONTROL_DIR);
+	if (!control_dir) {
+		PMEM_DPRINT("ERROR: failed to create directory [%s]\n",
+		            PMEMFS_CONTROL_DIR);
+		return rc;
+	}
+
+	/* new partition control file. */
+	f_info = pmemfs_alloc_finfo(NULL, PMEMFS_NEW_PART, NULL,
+	                            pmemfs_write_new_part);
+	if (!f_info) {
+		PMEM_DPRINT("ERROR: Failed to create new partition file\n");
+		return rc;
+	}
+	dentry = pmemfs_create_file(sb, control_dir, CTRL_NEW_PART_FILE,
+	               f_info, PMEMFS_PERM_RW, 0);
+	if (!dentry) {
+		PMEM_DPRINT("ERROR: new part control file failure\n");
+		kfree(f_info);
+		return rc;
+	}
+
+	/* global pmem lock control file - locks all of pmem */
+	f_info = pmemfs_alloc_finfo(NULL, PMEMFS_LOCK_PMEM, NULL,
+	                            pmemfs_write_lock_pmem);
+	if (!f_info) {
+		PMEM_DPRINT("ERROR: Failed to create global lock file\n");
+		return rc;
+	}
+	dentry = pmemfs_create_file(sb, control_dir, CTRL_LOCK_FILE,
+	                            f_info, PMEMFS_PERM_RW, 0);
+	if (!dentry) {
+		PMEM_DPRINT("ERROR: global lock control file failure\n");
+		kfree(f_info);
+		return rc;
+	}
+	
+	/* active segments lock control file - locks all active segments */
+	f_info = pmemfs_alloc_finfo(NULL, PMEMFS_LOCK_ALL_ACTIVE, NULL,
+	                            pmemfs_write_lock_all_active);
+	if (!f_info) {
+		PMEM_DPRINT("ERROR: Failed to create global lock file\n");
+		return rc;
+	}
+	dentry = pmemfs_create_file(sb, control_dir, CTRL_LOCK_ACTIVE_SEG_FILE,
+	                            f_info, PMEMFS_PERM_RW, 0);
+	if (!dentry) {
+		PMEM_DPRINT("ERROR: global lock control file failure\n");
+		kfree(f_info);
+		return rc;
+	}
+
+	/* global rotate control file. */
+	f_info = pmemfs_alloc_finfo(NULL, PMEMFS_PMEM_ROTATE, NULL,
+	                            pmemfs_write_rotate_pmem);
+	if (!f_info) {
+		PMEM_DPRINT("ERROR: Failed to create global rotate node\n");
+		return rc;
+	}
+	dentry = pmemfs_create_file(sb, control_dir, CTRL_ROTATE_FILE,
+	                            f_info, PMEMFS_PERM_RW, 0);
+	if (!dentry) {
+		PMEM_DPRINT("ERROR: global rotate control file failure\n");
+		kfree(f_info);
+		return rc;
+	}
+	
+	/* reset flag manipulation control file. */
+	f_info = pmemfs_alloc_finfo(NULL, PMEMFS_RESET_PMEM, NULL,
+	                            pmemfs_write_reset_pmem);
+	if (!f_info) {
+		PMEM_DPRINT("ERROR: Failed to create reset node\n");
+		return rc;
+	}
+	dentry = pmemfs_create_file(sb, control_dir, CTRL_RESET_FILE, f_info,
+	                            PMEMFS_PERM_RW, 0);
+	if (!dentry) {
+		PMEM_DPRINT("ERROR: reset control file failure\n");
+		kfree(f_info);
+		return rc;
+	}
+
+	/* success if we get all the way through */
+	return 0;
+}
+
+/* static int pmemfs_create_initial_files 
+ * Create the initial file tree structure that we export  */
+static int pmemfs_create_initial_files(struct super_block *sb)
+{
+	struct part_list_elem	*part_elem;
+	struct list_head	*elem, *temp;
+	pmem_handle_t		handle = NULL;
+	struct pmemfs_file_info	*f_info;
+	struct dentry		*dentry;
+
+	/* NOTE:  There is some handle conversion magic happening here
+	 * that seems a waste, but is required by the pmemfs_add_partition()
+	 * subroutine to properly setup the internal private data ptr on the
+	 * files
+	 */
+
+	f_info = pmemfs_alloc_finfo(NULL, PMEMFS_RAW_PMEM,
+	                            pmemfs_read_raw_pmem,
+	                            pmemfs_write_raw_pmem);
+	if (!f_info) {
+		PMEM_DPRINT("ERROR: Failed to kmalloc file info node\n");
+		return -EINVAL;
+	}
+
+	/* create the global top level, access to all of pmem raw file */
+	dentry = pmemfs_create_file(sb, sb->s_root, PMEMFS_RAW_ACCESS_FILE,
+	               f_info, PMEMFS_PERM_RW, pmem.size);
+	if (!dentry) {
+		PMEM_DPRINT("ERROR: Failed to create pmem raw access file\n");
+		kfree(f_info);
+		return -EINVAL;
+	}
+	/* add the top level pmem control directory and files */
+	if (0 != pmemfs_create_top_control_dir(sb)) {
+		PMEM_DPRINT("ERROR: Failed to create top level control dir\n");
+		return -EINVAL;
+	}
+	/* show the shadow copy if it exists */
+	if (pmem.shadow) {
+		if (0 != pmem_shadow_get(&handle)) {
+			PMEM_DPRINT("ERROR: Cant get shadow handle\n");
+			return -EINVAL;
+		}
+		f_info = pmemfs_alloc_finfo(handle, PMEMFS_BACKUP,
+		                            pmemfs_read_raw_shadow, NULL);
+		if (!f_info) {
+			PMEM_DPRINT("ERROR: failed to create shadow finfo\n");
+			return -EINVAL;
+		}
+		handle = NULL; /* let VFS reclaim memory on umount */
+
+		dentry = pmemfs_create_file(sb, sb->s_root,
+				            PMEMFS_BACKUP_FILE, f_info,
+					    PMEMFS_PERM_RW, pmem.size);
+		if (!dentry) {
+			PMEM_DPRINT("ERROR: Cant add shadow file to VFS\n");
+			return -EINVAL;
+		}
+	}
+	/* If pmem is stored in hardware, create a file to indicate this */
+	if (pmem.using_hardware) {
+		f_info = pmemfs_alloc_finfo(NULL, PMEMFS_HW_INDICATOR,
+		                            NULL, NULL);
+		if (!f_info) {
+			PMEM_DPRINT("ERROR: failed to create hw indicator file\n");
+			return -EINVAL;
+		}
+		dentry = pmemfs_create_file(sb, sb->s_root,
+		                            PMEMFS_HW_INDICATOR_FILE, f_info,
+					    PMEMFS_PERM_NONE, 0);
+		if (!dentry) {
+			PMEM_DPRINT("ERROR: Cant add backup indicator file to VFS\n");
+			return -EINVAL;
+		}
+	}
+
+	/* for each pmem partition */
+	list_for_each_safe(elem, temp, &pmem.part_list) {
+		part_elem = list_entry(elem, struct part_list_elem, list_elem);
+		handle = NULL; /* make pmem_partition_get kmalloc new memory */
+		if (pmem_partition_get(part_elem->hdr->data.desc, &handle)) {
+			PMEM_DPRINT("ERROR: Cant get handle for part [%s]\n",
+			            part_elem->hdr->data.desc);
+			pmem_release_handle(&handle);
+			continue;
+		}
+		/* recursively registers the VFS tree for the partition. */
+		pmemfs_add_partition(sb, (struct part_handle*) handle);
+		pmem_release_handle(&handle);
+	}
+
+	return 0;
+} 
+
+
+/********************************************************************
+ *
+ * Pmem callbacks and registration functions
+ *
+ *******************************************************************/
+/* static void pmemfs_on_new_partition 
+ * Add a new partition to the pmemfs tree layout.
+ * Callback that is installed to receive notifications from the main 
+ * pmem code about added partitions */
+static void pmemfs_on_new_partition(pmem_handle_t handle)
+{
+	struct pmem_handle	*hdl = (struct pmem_handle*)handle;
+	struct part_handle	*part_handle;
+	int			rc;
+
+	if (PMEM_HANDLE_TYPE_PART != hdl->type) {
+		PMEM_DPRINT("ERROR: Invalid handle type %d\n",
+		            hdl->type);
+		return;
+	}
+	if (!pmemfs_super) {
+		PMEM_DPRINT("ERROR: Callback called before full pmemfs init\n");
+		return;
+	}
+
+	part_handle = (struct part_handle*)hdl;
+	rc = pmemfs_add_partition(pmemfs_super, part_handle);
+} 
+
+/* static void pmemfs_on_new_region 
+ * Add a new region to the given partition's pmemfs tree layout.
+ * This code is called when a new region is added to pmem */
+static void pmemfs_on_new_region(pmem_handle_t part_hdl,
+                pmem_handle_t region_hdl)
+{
+	struct pmem_handle	*p_hdl = (struct pmem_handle*)part_hdl;
+	struct pmem_handle	*r_hdl = (struct pmem_handle*)region_hdl;
+	int			rc;
+
+	PMEM_DPRINT("pmemfs_on_new_region callback\n");
+
+	if (PMEM_HANDLE_TYPE_PART != p_hdl->type) {
+		PMEM_DPRINT("ERROR: Invalid handle type %d\n", p_hdl->type);
+		return;
+	}
+	if (PMEM_HANDLE_TYPE_REG != r_hdl->type) {
+		PMEM_DPRINT("ERROR: Invalid handle type %d\n", r_hdl->type);
+		return;
+	}
+	if (!pmemfs_super) {
+		PMEM_DPRINT("ERROR: Callback called before full pmemfs init\n");
+		return;
+	}
+
+	rc = pmemfs_add_region_to_part(pmemfs_super, part_hdl, region_hdl);
+
+} 
+
+/* static void pmemfs_install_pmem_hooks 
+ * Install the notification callbacks into the main pmem code */
+static void pmemfs_install_pmem_hooks(void)
+{
+	pmem_events.create_partition = &pmemfs_on_new_partition;
+	pmem_events.create_region = &pmemfs_on_new_region;
+
+	PMEM_DPRINT("INFO: Installed callbacks part=%p, reg=%p\n",
+			pmem_events.create_partition,
+			pmem_events.create_region);
+} 
+
+/* static void pmemfs_uninstall_pmem_hooks {{{ 
+ * Remove the notification callbacks from pmem */
+static void pmemfs_uninstall_pmem_hooks(void)
+{
+	pmem_events.create_partition = &pmem_default_create_partition;
+	pmem_events.create_region = &pmem_default_create_region;
+
+	PMEM_DPRINT("INFO: Removed callbacks part=%p, reg=%p\n",
+			pmem_events.create_partition,
+			pmem_events.create_region);
+} 
+
+/********************************************************************
+ *
+ * Filesystem operations functions - see pmemfs_file_ops struct
+ *
+ *******************************************************************/
+/* static int pmemfs_open {{{
+ * Open a "file".  This basically provides a handle to the desired piece
+ * of pmem.
+ */
+static int pmemfs_open(struct inode *inode, struct file *filp)
+{
+	filp->f_ra.ra_pages = 0; /* No read-ahead */
+	filp->private_data = inode->i_private;
+	//PMEM_DPRINT("INFO: open flags are %d\n", (int)filp->f_mode);
+	return generic_file_open(inode, filp);
+	// return 0;
+} 
+
+/* static ssize_t pmemfs_read_file {{{
+ * Read a file. This is how a user can access the various parts of pmem.
+ */
+static ssize_t pmemfs_read_file(struct file *filp, char __user *buf,
+                size_t count, loff_t *offset)
+{
+	struct pmemfs_file_info	*f_info;
+	struct pmem_handle	*hdl;
+	char			*local_buf;
+	int			rc;
+	ssize_t			bytes_read = -EINVAL;
+
+	if (NULL == filp->private_data) {
+		PMEM_DPRINT("ERROR: Attempt to read from bad file handle %p\n",
+				filp);
+		return bytes_read;
+	}
+
+	f_info = (struct pmemfs_file_info*)filp->private_data;
+	if (!f_info) {
+		PMEM_DPRINT("ERROR: Invalid private data\n");
+		return bytes_read;
+	}
+	if (!f_info->read) {
+		/* no read capabiltites on this file */
+		return bytes_read;
+	}
+
+	hdl = (struct pmem_handle*)f_info->handle;
+	
+	local_buf = pmemfs_alloc_mem(count);
+	if (!local_buf) {
+		PMEM_DPRINT("ERROR: Failed to alloc %d bytes\n", (int)count);
+		return -ENOMEM;
+	}
+	
+	bytes_read = f_info->read(hdl, local_buf, count, *offset);
+
+
+	if (bytes_read > 0) {
+		rc = copy_to_user(buf, local_buf, bytes_read);
+		if (rc)	{
+			PMEM_DPRINT("ERROR: copy_to_user of %d failed rc=%d\n",
+		                    (int)bytes_read, rc);
+			bytes_read = -EFAULT;
+		} else {
+			/* update current filep offset */
+			*offset += bytes_read;
+		}
+	}
+	pmemfs_free_mem(local_buf, count);
+	
+	return bytes_read;
+} 
+
+/* static ssize_t pmemfs_write_file {{{
+ * Write a file.  Put data into pmem at the appropriate spot ..
+ */
+static ssize_t pmemfs_write_file(struct file *filp, const char __user *buf,
+		size_t count, loff_t *offset)
+{
+	struct inode		*inode = filp->f_dentry->d_inode;
+	struct pmemfs_file_info	*f_info;
+	struct pmem_handle	*hdl;
+	char			*local_buf;
+	int 			rc;
+	ssize_t			bytes_written = -EINVAL;
+
+
+	if (unlikely(!inode)) {
+		printk(KERN_ERR "%s: inode is NULL", __FUNCTION__);
+		dump_stack();
+		return bytes_written;
+	}
+
+	if (NULL == filp->private_data) {
+		PMEM_DPRINT("ERROR: Attempt to write to bad file handle %p\n",
+				filp);
+		return bytes_written;
+	}
+
+	f_info = (struct pmemfs_file_info*)filp->private_data;
+	if (!f_info) {
+		PMEM_DPRINT("ERROR: Invalid private data\n");
+		return bytes_written;
+	}
+	if (!f_info->write) {	/* no write capabilities */
+		PMEM_DPRINT("ERROR: Attempt to write to r/o file\n");
+		return bytes_written;
+	}
+
+	hdl = (struct pmem_handle*)f_info->handle;
+
+	local_buf = pmemfs_alloc_mem(count);
+	if (!local_buf) {
+		PMEM_DPRINT("ERROR: Failed to alloc %d bytes\n", (int)count);
+		return -ENOMEM;
+	}
+	rc = copy_from_user(local_buf, buf, count);
+	if (rc) {
+		PMEM_DPRINT("ERROR: copy_from_user of %d failed rc=%d\n",
+		            (int)count, rc);
+		pmemfs_free_mem(local_buf, count);
+		return -EFAULT;
+	}
+		
+	//PMEM_DPRINT("INFO: Got request to write to %p, offset=%d, len=%d\n",
+	//		filp, (int)*offset, (int)count);
+	
+	mutex_lock(&inode->i_mutex);
+	bytes_written = f_info->write(hdl, local_buf, count, *offset);
+	/* update modification times in pmemfs for mmap() to see the change */
+	inode->i_mtime = CURRENT_TIME;
+	mutex_unlock(&inode->i_mutex);	/* done writing to pmem, release lock */
+
+	pmemfs_free_mem(local_buf, count);
+
+	if (bytes_written > 0) {
+		*offset += bytes_written;
+	} else {
+		PMEM_DPRINT("ERROR: write failed with rc=%d\n",
+		            (int)bytes_written);
+	}
+	return bytes_written;
+} 
+
+/* static struct page* pmemfs_filemap_fault {{{
+ *
+ * Access a page for mmap() processes.
+ */
+int pmemfs_filemap_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
+{
+	struct page *page = NULL;
+	struct file		*filp = vma->vm_file;
+	struct pmemfs_file_info	*f_info;
+	struct pmem_handle	*hdl;
+	unsigned long		 pmem_addr;
+	unsigned long		 offset;
+	int ret = 0;
+
+	if (NULL == filp->private_data) {
+		PMEM_DPRINT("ERROR: Attempt to read from bad file handle %p\n",
+				filp);
+		return VM_FAULT_SIGBUS;
+	}
+
+	f_info = (struct pmemfs_file_info*)filp->private_data;
+	if (!f_info) {
+		PMEM_DPRINT("ERROR: Invalid private data\n");
+		return VM_FAULT_SIGBUS;
+	}
+	hdl = (struct pmem_handle*)f_info->handle;
+
+	if (!hdl) {
+		char *fn = "?";
+		char *path;
+
+		path = (char*)kmalloc(PATH_MAX, GFP_KERNEL);
+		if (path) {
+			fn = d_path(&filp->f_path, path, PATH_MAX);
+			if (IS_ERR(fn))
+				fn = "?";
+		}
+
+		/* This is a valid condition for files like the raw partition file and control files */
+		PMEM_DPRINT("Requested file has no PMEM handle: %s, using filemap_fault\n", fn);
+		kfree(path);
+
+		return filemap_fault(vma, vmf);
+	}
+
+	pmem_addr = (unsigned long)pmem_get_data_ptr(hdl);
+	offset    =  vmf->virtual_address - vma->vm_start;
+
+	/* Check if we are still inside the data structure */
+	if ( (offset + PAGE_SIZE) > hdl->size) {
+		PMEM_DPRINT("ERROR: offset %p and size %lu out of range\n",
+		            (void *)offset,
+		            (unsigned long)(vma->vm_end - vma->vm_start));
+		return VM_FAULT_SIGBUS;
+	}
+
+	/* Return the requested page within PMEM */
+	page = vmalloc_to_page((void *)(pmem_addr + offset));
+	get_page(page);
+
+	vmf->page = page;
+	return ret;
+} 
+
+/* static ssize_t pmemfs_file_mmap {{{
+ *
+ * Dont allow mmap() access to files on the active partition
+ */
+static int pmemfs_file_mmap(struct file *filp, struct vm_area_struct *vma)
+{
+	struct pmemfs_file_info	*f_info;
+	struct pmem_handle	*hdl;
+	struct block_handle	*seg_hdl;
+	struct region_handle	*region_hdl;
+	int			block_id = 0;
+	int 			rc = -EINVAL;
+#ifdef CONFIG_PMEM_PCI_DRIVER
+	struct pci_dev 		*pmem_pci_dev = NULL;
+	enum			pci_mmap_state mmap_state = pci_mmap_mem;
+	unsigned long		pmem_pci_start = 0;
+	int			write_combine = 1;
+#endif
+	unsigned long		pmem_offset = 0;
+
+	if (NULL == filp->private_data) {
+		PMEM_DPRINT("ERROR: Attempt to read from bad file handle %p\n",
+				filp);
+		return rc;
+	}
+
+	f_info = (struct pmemfs_file_info*)filp->private_data;
+	if (!f_info) {
+		PMEM_DPRINT("ERROR: Invalid private data\n");
+		return rc;
+	}
+	hdl = (struct pmem_handle*)f_info->handle;
+	
+	/* no mmap access to elements on active segment */
+	switch (f_info->type) {
+		/* always allow mmap here */
+		case PMEMFS_RAW_PMEM:
+			pmem_offset = 0;
+			break;
+		case PMEMFS_RAW_PART:
+		case PMEMFS_PART_HEADER:
+			pmem_offset = (unsigned long)pmem_get_hdr_ptr(hdl) - 
+			              (unsigned long)pmem.pmem;
+			break;
+		case PMEMFS_BACKUP:
+			/* vmalloc'd pages */
+			break;
+		case PMEMFS_PART_DATA:
+			pmem_offset = (unsigned long)pmem_get_data_ptr(hdl) - 
+			              (unsigned long)pmem.pmem;
+			break;
+		case PMEMFS_SEG_DATA:
+			pmem_offset = (unsigned long)pmem_get_data_ptr(hdl) - 
+			              (unsigned long)pmem.pmem;
+			seg_hdl = (struct block_handle*)hdl;
+			block_id = seg_hdl->block_id;
+			break;
+		case PMEMFS_REG_RAW_DATA:
+		case PMEMFS_REG_DATA:
+		case PMEMFS_REG_RECORD:
+		case PMEMFS_REG_BYTE:
+			pmem_offset = (unsigned long)pmem_get_data_ptr(hdl) - 
+			              (unsigned long)pmem.pmem;
+			region_hdl = (struct region_handle*)hdl;
+			block_id = region_hdl->block_id;
+			break;
+		case PMEMFS_SEG_HEADER:
+		case PMEMFS_REG_HEADER:
+			/* Segment and region headers are not page aligned.
+			 * libpmem uses raw pmem to obtain these headers. */
+		default:
+			PMEM_DPRINT("ERROR: mmap not supported on type %d\n",
+			            f_info->type);
+			return -EINVAL;
+			break;
+	}
+
+	/* deny mmap access to elements on the active segment */
+	if (PMEM_ACTIVE_BLOCK != block_id) {
+#ifdef pgprot_noncached
+		/* Treat all access to PMEM as O_SYNC, if supported by the
+		   hardware. */
+		if (pmem_uncached_access(vma->vm_pgoff << PAGE_SHIFT)) {
+			vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
+		}
+#endif
+
+#if 0
+		if (offset >= __pa(high_memory) || (filp->f_flags & O_SYNC))
+			vma->vm_flags |= VM_IO;
+#endif
+
+		file_accessed(filp);
+
+		vma->vm_flags |= VM_RESERVED;
+
+		if ((pmem.using_hardware) && (f_info->type != PMEMFS_BACKUP)) {
+			/* Shadow PMEM is in VMALLOC space */
+			if (pmem.using_io_mem) {
+#if defined(CONFIG_PMEM_PCI_DRIVER)
+				PMEM_DPRINT("Using pci_mmap_page_range\n");
+				/* vma->vm_flags |= VM_IO; */
+
+				pmem_pci_dev = pmem_arch_get_pci_dev(pmem_pci_dev);
+				pmem_pci_start = (unsigned long) pmem_arch_get_pci_start_ptr();
+
+				/* Add the physical address of the PCI device to the requested offset */
+				vma->vm_pgoff += (pmem_pci_start + PERSISTENT_CONTROL_SIZE + pmem_offset) >> PAGE_SHIFT;
+
+				PMEM_DPRINT("vma->vm_pgoff %lx, pmem_pci_start %lx, pmem_offset %lx\n",
+				            vma->vm_pgoff, pmem_pci_start, pmem_offset);
+
+				rc = pci_mmap_page_range(pmem_pci_dev,
+				                         vma,
+				                         mmap_state,
+				                         write_combine);
+#else
+				/* using_io_mem should never be true without an ioremap driver enabled */
+				printk(KERN_CRIT "ERROR: PMEM HW using_io_mem set without built-in support\n"); 
+#endif /* CONFIG_PCI_DRIVER */
+			} else {
+				/* Not all HW PMEM uses io_mem */
+				/* using_hardware should never be true without HW PMEM support enabled */
+				printk(KERN_CRIT "ERROR: PMEM HW using_hardware set without built-in support\n"); 
+			}
+		} else {
+			PMEM_DPRINT("Using pmemfs_filemap_fault\n");
+			/* Simulated PMEM, Shadow PMEM, and non-ioremap HW PMEM uses
+			 * vmalloc pages.  Use the nopage method to fault them in. */
+			vma->vm_ops = &pmemfs_file_vm_ops;
+			rc = 0;
+		}
+	} else {
+		/* FIXME: Remove printk here */
+		PMEM_DPRINT("ERROR: Cant mmap on the active segment\n");
+	}
+	return rc;
+} 
+
+/* static int pmemfs_readpage_sync {{{
+ *
+ * Read a single page syncronously.
+ */
+static int pmemfs_readpage_sync(struct pmemfs_file_info *f_info,
+                                struct page *page)
+{
+	struct pmem_handle	*hdl;
+	char			*buf = kmap(page);
+	int			count = PAGE_SIZE;
+	int			rc = - EINVAL;
+	loff_t			offset;
+
+	offset = (loff_t)page->index << PAGE_CACHE_SHIFT;
+	hdl = (struct pmem_handle*)f_info->handle;
+
+	rc = f_info->read(hdl, buf, count, offset);
+	if (rc < 0) {	/* read error */
+		PMEM_DPRINT("ERROR: readpage cant read count. rc=%d\n", rc);
+		kunmap(page);
+		unlock_page(page);
+		return rc;
+	} else if (rc != count) {
+		PMEM_DPRINT("WARNING: short read in readpage.rc=%d,count=%d\n",
+		            rc, count);
+	}
+	
+	/* if we didnt read enough data, fill in the rest of the page with
+	 * NULLs */
+	memset(buf + rc, 0, count - rc);
+
+	kunmap(page);
+	flush_dcache_page(page);
+	SetPageUptodate(page);
+	unlock_page(page);
+	
+	/* success */
+	return 0;
+} 
+
+/* static int pmemfs_readpage {{{
+ *
+ * read a page of data from a file to fill a page for mmap
+ */
+static int pmemfs_readpage(struct file *filp, struct page *page)
+{
+	int			rc = -EINVAL;
+	struct pmemfs_file_info	*f_info;
+
+	if (NULL == filp->private_data) {
+		PMEM_DPRINT("ERROR: readpage from bad file handle %p\n", filp);
+		return rc;
+	}
+
+	f_info = (struct pmemfs_file_info*)filp->private_data;
+	if (!f_info) {
+		PMEM_DPRINT("ERROR: Invalid readpage private data\n");
+		return rc;
+	}
+
+	if (NULL == f_info->read) {
+		PMEM_DPRINT("ERROR: Attempt to readpage w/o perms\n");
+		return rc;
+	}
+
+	page_cache_get(page);
+	rc = pmemfs_readpage_sync(f_info, page);
+	page_cache_release(page);
+
+	return rc;
+} 
+
+/* static int pmemfs_writepage_sync {{{
+ *
+ * Write a page synchronously.
+ * Offset is the data offset within the page.
+ */
+static int pmemfs_writepage_sync(struct pmemfs_file_info *f_info,
+                struct page *page, unsigned long pgoffset, unsigned int count)
+{
+	struct pmem_handle	*hdl;
+	loff_t			offset;
+	char			*buf = kmap(page) + pgoffset;
+	int			rc = -EFAULT;
+
+	offset = ((loff_t)page->index << PAGE_CACHE_SHIFT) + pgoffset;
+
+	hdl = (struct pmem_handle*)f_info->handle;
+
+	rc = f_info->write(hdl, buf, count, offset);
+	if (rc < 0) {	/* read error */
+		PMEM_DPRINT("ERROR: readpage cant read count. rc=%d\n", rc);
+		kunmap(page);
+		unlock_page(page);
+		return rc;
+	} else if (rc != count) {
+		PMEM_DPRINT("WARNING: short write on writepage_sync, count=%d"
+		            " rc=%d\n", count, rc);
+	}
+	
+	kunmap(page);
+	return 0;	/* success */
+} 
+
+/* static int pmemfs_writepage {{{
+ *
+ * Write a mmap()d page with changes back to its appropriate file.
+ */
+static int pmemfs_writepage(struct page *page, struct writeback_control *wbc)
+{
+	struct address_space 	*mapping = page->mapping;
+	struct inode 		*inode;
+	unsigned long 		end_index;
+	unsigned 		offset = PAGE_CACHE_SIZE;
+	int 			rc = -EFAULT;
+	struct pmemfs_file_info	*f_info;
+
+
+	if (!mapping)
+		BUG();
+	inode = mapping->host;
+
+	if (!inode)
+		BUG();
+
+	if (NULL == inode->i_private) {
+		PMEM_DPRINT("ERROR: writepage from bad file handle\n");
+		return rc;
+	}
+
+	f_info = (struct pmemfs_file_info*)inode->i_private;
+	if (!f_info) {
+		PMEM_DPRINT("ERROR: Invalid writepage private data\n");
+		return rc;
+	}
+
+	if (NULL == f_info->write) {
+		PMEM_DPRINT("ERROR: Attempt to writepage w/o perms\n");
+		return rc;
+	}
+
+	end_index = inode->i_size >> PAGE_CACHE_SHIFT;
+
+	/* check if the page-index is kosher */
+	if (page->index >= end_index) {
+		offset = inode->i_size & (PAGE_CACHE_SIZE-1);
+		/* are we completely out? */
+		if (page->index >= end_index+1 || !offset)
+			return 0; /* truncated - don't care */
+	}
+
+	page_cache_get(page);
+	
+	rc = pmemfs_writepage_sync(f_info, page, 0, offset);
+
+	flush_dcache_page(page);
+	SetPageUptodate(page);
+	unlock_page(page);
+	page_cache_release(page);
+	
+	return rc;
+} 
+
+/* static int pmemfs_updatepage 
+ *
+ * update a single page bak to its backing store.
+ */
+static int pmemfs_updatepage(struct file *filp, struct page *page,
+                unsigned long offset, unsigned int count)
+{
+	struct pmemfs_file_info	*f_info;
+	int			rc = -EFAULT;
+
+	if (NULL == filp->private_data) {
+		PMEM_DPRINT("ERROR: updatepage from bad file handle %p\n", filp);
+		return rc;
+	}
+
+	f_info = (struct pmemfs_file_info*)filp->private_data;
+	if (!f_info) {
+		PMEM_DPRINT("ERROR: Invalid updatepage private data\n");
+		return rc;
+	}
+
+	if (NULL == f_info->write) {
+		PMEM_DPRINT("ERROR: Attempt to updatepage w/o perms\n");
+		return rc;
+	}
+
+	return pmemfs_writepage_sync(f_info, page, offset, count);
+} 
+
+/* static int pmemfs_prepare_write 
+ *
+ * Dummy method - we dont need to do anythying to prepare for a write.
+ */
+static int pmemfs_prepare_write(struct file *filp, struct page *page, 
+                unsigned offset, unsigned to)
+{
+	return 0;
+} 
+
+/* static int pmemfs_commit_write 
+ *
+ * Flush a page back to the file it belongs to
+ */
+static int pmemfs_commit_write(struct file *filp, struct page *page,
+                unsigned offset, unsigned to)
+{
+	return pmemfs_updatepage(filp, page, offset, to-offset);
+} 
+
+/* static int pmemfs_unlink 
+ *
+ * Delete a "file".  The only file that can be deleted is the shadow file.
+ * Attempt to delete any other file will result in an error.  */
+static int pmemfs_unlink(struct inode *dir, struct dentry *dentry)
+{
+	struct inode		*inode = dentry->d_inode;
+	struct pmemfs_file_info *f_info = inode->i_private;
+	struct pmem_handle*	hdl;
+	int			rc;
+
+	//PMEM_DPRINT("INFO: Calling unlink on pmemfs with f_info = %p\n", f_info);
+	if ((!f_info) || (!f_info->handle)) {
+		return -EPERM;
+	}
+	hdl = (struct pmem_handle*)f_info->handle;
+
+	if (PMEM_HANDLE_TYPE_SHADOW != hdl->type) {
+		//PMEM_DPRINT("INFO: Attempt to delete file of type %d\n",
+		 //           hdl->type);
+		return -EPERM;
+	}
+
+	/* clean up the inode in the VFS for the pmem shadow copy */	
+	inode->i_ctime = dir->i_ctime = dir->i_mtime = CURRENT_TIME;
+	inode->i_nlink--;
+	dput(dentry);
+	
+	/* and actually dispose of the pmem_shadow copy */
+	rc = pmem_clear_shadow();
+	if (0 != rc) {
+		PMEM_DPRINT("ERROR: Clear shadow failed with rc=%d\n", rc);
+		return rc;
+	}
+
+	return 0;
+} 
+
+/* static int pmemfs_fill_super 
+ * "Fill" a superblock with mundane stuff.
+ */
+static int pmemfs_fill_super(struct super_block *sb, void *data, int silent)
+{
+	struct inode *root;
+	struct dentry *root_dentry;
+	
+	/* make sure that pmem is availiable */
+	if (!pmem.enabled) {
+		printk(KERN_ERR "FATAL ERROR: pmem not enabled - pmemfs cant "
+		                "mount.\n");
+		return -ENODEV;
+	}
+
+	/*  Basic parameters.  */
+	sb->s_blocksize = PAGE_CACHE_SIZE;
+	sb->s_blocksize_bits = PAGE_CACHE_SHIFT;
+	sb->s_magic = PMEMFS_MAGIC;
+	sb->s_op = &pmemfs_sb_ops;
+	/*
+	 * We need to conjure up an inode to represent the root directory
+	 * of this filesystem.  Its operations all come from libfs, so we
+	 * don't have to mess with actually *doing* things inside this
+	 * directory.
+	 */
+	root = pmemfs_create_inode(sb, S_IFDIR | 0755, 0);
+	if (!root) {
+		PMEM_DPRINT("ERROR: Failed to create root inode\n");
+		return -ENOMEM;
+	}
+	root->i_op = &pmemfs_inode_ops;
+	root->i_fop = &simple_dir_operations;
+	/* Get a root dentry to represent root of the fs */
+	root_dentry = d_alloc_root(root);
+	if (!root_dentry) {
+		PMEM_DPRINT("ERROR: Failed to alloc root dentry\n");
+		iput(root);
+		return -ENOMEM;
+	}
+	sb->s_root = root_dentry;
+	/*  Make up the initial "files" which are in pmemfs */
+	pmemfs_create_initial_files(sb);
+	
+	return 0;
+} 
+
+/* static void pmemfs_destroy_inode 
+ *
+ * Called when the filesystem is done with the given inode.
+ * Use this routine to clean up for the fact that this inode is 
+ * about to tossed into the brink.
+ */
+static void pmemfs_destroy_inode(struct inode *inode)
+{
+	//PMEM_DPRINT("INFO: Running pmemfs_destroy_inode\n");
+	/* destroy the pmem opaque handle attached for the inode i_private */
+	if (inode->i_private) {
+		pmemfs_free_finfo((struct pmemfs_file_info*)inode->i_private);
+		inode->i_private = NULL;
+	}
+} 
+
+/********************************************************************
+ *
+ * Filesystem registration and instantiation
+ *
+ *******************************************************************/
+/* static struct super_block *pmemfs_get_super {{{ 
+ * Called by the VFS layer to fetch the superblock for pmemfs
+ * After the superblock is populated, this routine installs the 
+ * pmem event handlers.  Also caches the location of the superblock 
+ * for events that are triggered from kernel space (which wont have
+ * the needed context)
+ */
+static int pmemfs_get_super(struct file_system_type *fst,
+		int flags, const char *devname, void *data, struct vfsmount *mnt)
+{
+	int ret;
+	/* cache the superblock that we return to the VFS layer
+	 * for use during the pmem event calls */
+	ret = get_sb_single(fst, flags, data, pmemfs_fill_super, mnt);
+	if (ret)
+		return ret;
+	pmemfs_super = (struct super_block *)mnt->mnt_sb;
+	return ret;
+} 
+
+/* pmemfs VFS callback structure defintions */
+static struct file_system_type pmemfs_type = {
+	.owner 		= THIS_MODULE,
+	.name		= "pmemfs",
+	.get_sb		= pmemfs_get_super,
+	.kill_sb	= kill_litter_super,
+};
+
+static struct super_operations pmemfs_sb_ops = {
+	.destroy_inode	= pmemfs_destroy_inode,
+	.statfs		= simple_statfs,
+	.drop_inode	= generic_delete_inode,
+};
+
+/* pmemfs file operations structure  */
+static struct file_operations pmemfs_file_ops = {
+	.open		= pmemfs_open,
+	.read 		= pmemfs_read_file,
+	.write		= pmemfs_write_file,
+	.mmap		= pmemfs_file_mmap,
+};
+
+static struct inode_operations pmemfs_inode_ops = {
+	.lookup		= simple_lookup,
+	.unlink		= pmemfs_unlink,
+};
+
+/* this structure def lifted from ramfs */
+static struct backing_dev_info pmemfs_backing_dev_info = {
+	.ra_pages	= 0,  /* no read-ahead */
+};
+	//.memory_backed	= 1,  /* doesnt contribute to dirty memory
+	//				 and wont be written out ! */
+
+static struct address_space_operations pmemfs_aops = {
+	.readpage	= pmemfs_readpage,
+	.writepage 	= pmemfs_writepage,
+	.prepare_write	= pmemfs_prepare_write,
+	.commit_write	= pmemfs_commit_write,
+};
+
+/* used to support mmap() */
+static struct vm_operations_struct pmemfs_file_vm_ops = {
+	.fault		= pmemfs_filemap_fault,
+};
+
+
+
+/********************************************************************
+ *
+ * Module initialization section
+ *
+ *******************************************************************/
+
+int __init pmemfs_init(void)
+{
+	int rc;
+
+	rc = register_filesystem(&pmemfs_type);
+	pmemfs_install_pmem_hooks();
+	printk("PMEMFS with MEL changes.\n");
+	return rc;
+}
+
+void __exit pmemfs_exit(void)
+{
+	pmemfs_uninstall_pmem_hooks();
+	unregister_filesystem(&pmemfs_type);
+}
+
+module_init(pmemfs_init);
+module_exit(pmemfs_exit)
+
+/* 
+ vim:ft=c:fdm=marker:ff=unix:ts=8
+ */
diff --git a/fs/proc/proc_misc.c b/fs/proc/proc_misc.c
index 49962c2..3056ef4 100644
--- a/fs/proc/proc_misc.c
+++ b/fs/proc/proc_misc.c
@@ -937,6 +937,9 @@ void __init proc_misc_init(void)
 #ifdef CONFIG_SCHEDSTATS
 	proc_create("schedstat", 0, NULL, &proc_schedstat_operations);
 #endif
+#ifdef CONFIG_PERSISTENT_PANIC
+	proc_create("kernel-core-dump", 0,NULL, &proc_kcore_dump_operations);
+#endif	
 #if defined(CONFIG_PROC_KCORE) && !defined(CONFIG_GRKERNSEC_PROC_ADD)
 	proc_root_kcore = proc_create("kcore", S_IRUSR, NULL, &proc_kcore_operations);
 	if (proc_root_kcore)
diff --git a/include/linux/pmem.h b/include/linux/pmem.h
new file mode 100644
index 0000000..88b5adb
--- /dev/null
+++ b/include/linux/pmem.h
@@ -0,0 +1,778 @@
+#ifndef PMEM_H
+#define PMEM_H
+
+#include <linux/version.h>
+#include <linux/types.h>
+#include <asm/page.h>
+#include <asm/byteorder.h>
+
+/*
+ * Persistent Memory
+ */
+ 
+#ifndef CONFIG_PMEM_SIZE
+#define CONFIG_PMEM_SIZE 0
+#endif
+ 
+/* Persistent memory diagnostic wrapper */
+#ifdef CONFIG_PMEM
+#define PERSISTENT_CONTROL_SIZE PAGE_SIZE	/*Reserve a whole page*/
+#define PERSISTENT_CONTROL_BOOTUP		0
+#define PERSISTENT_CONTROL_RESTART		0x11111111
+#define PERSISTENT_CONTROL_HALT			0x22222222
+#define PERSISTENT_CONTROL_PWR_OFF		0x33333333
+#endif/* CONFIG_PMEM */
+
+/********************************************************************
+ *
+ * Persistent memory definitions (includes filesystem layout)
+ *
+ *******************************************************************/
+#define PMEM_VERSION_CURRENT		1
+#define PMEM_ALIGN_BYTES		PAGE_SIZE
+/* Used to align the data to a page */
+#define PMEM_PART_HDR_MAX_SIZE		PAGE_SIZE
+
+#define PMEM_DESC_MAX			32
+#define PMEM_ACTIVE_BLOCK		-1
+#define PMEM_INVALID_BLOCK		-2
+/* At any point in time, each partition must have at least 2 unlocked 
+ * blocks so that locking doesnt interfere with rotation */
+#define PMEM_MIN_UNLOCKED_BLOCKS 	2
+
+/* Control block flags */
+#define PMEM_CB_FLAG_RESET		1
+#define PMEM_CB_NUM_FLAGS		1
+
+/* Partition flags */
+#define PMEM_PART_NUM_FLAGS		0
+
+/* Block flags */
+#define PMEM_BLOCK_FLAG_ACTIVE		1
+#define PMEM_BLOCK_FLAG_LOCK		2
+#define PMEM_BLOCK_NUM_FLAGS		2
+
+/* Region flags */
+#define PMEM_REGION_FLAG_PERPROC	1
+#define PMEM_REGION_FLAG_STOPFULL	2
+#define PMEM_REGION_NUM_FLAGS		2
+
+/* Log partition information */
+#define PMEM_PART_LOG_DESC		"logs"
+#define PMEM_PART_LOG_VERSION		1
+
+/* Log region information */
+#define PMEM_REG_GENERAL_DESC		"general"
+#define PMEM_REG_GENERAL_VERSION	1
+#define PMEM_REG_GENERAL_FIXED_SIZE	0
+#define PMEM_REG_GENERAL_FLAGS		0
+
+/* filesystem path constants */
+#define PMEMFS_PARTITION_PREFIX		"partition"
+#define PMEMFS_SEGMENT_PREFIX		"segment"
+#define PMEMFS_REGION_PREFIX		"region"
+#define PMEMFS_ACTIVE_REGION_DIR	"active"
+#define PMEMFS_RAW_ACCESS_FILE		"raw"
+#define PMEMFS_BACKUP_FILE		"backup"
+#define PMEMFS_HW_INDICATOR_FILE	"using_hardware"
+//#define PMEMFS_REGION_FILE		"region_data"
+#define PMEMFS_RAW_DATA_SUFFIX		"_raw_data"
+#define PMEMFS_DATA_SUFFIX		"_data"
+#define PMEMFS_HEADER_SUFFIX		"_header"
+#define PMEMFS_CONTROL_DIR		"control"
+
+/* control filenames in pmemfs */
+#define CTRL_NEW_PART_FILE		"new_partition"
+#define CTRL_NEW_REGION_FILE		"new_region"
+#define CTRL_LOCK_ACTIVE_SEG_FILE	"lock_active_segments"
+#define CTRL_LOCK_FILE			"lock"
+#define CTRL_ROTATE_FILE		"rotate"
+#define CTRL_RESET_FILE			"reset"
+#define CTRL_ACTIVE_SEGMENT_FILE	"active_segment"
+
+/* Persistent memory architecture definitions */
+#define PMEM_ARCH_PPC32		1
+#define PMEM_ARCH_X86		2
+#define PMEM_ARCH_ARM		3
+#define PMEM_ARCH_PPC64		4
+#define PMEM_ARCH_MIPS64	5
+
+
+/* Persistent memory endianess definitions */
+#if defined (__LITTLE_ENDIAN_BITFIELD)
+#define PMEM_ARCH_LITTLE_ENDIAN	1	
+#elif defined (__BIG_ENDIAN_BITFIELD)
+#define PMEM_ARCH_LITTLE_ENDIAN	0	
+#else
+#error "Unable to determine endianess of target"
+#endif
+
+/* Get architecture value */
+#define PMEM_GET_ARCH(cb_hdr) \
+	(__u8)((__u8)((cb_hdr)->data.arch_info<<1)>>1)
+
+/* Get endianess of pmem header  - 0 = big endian, 1 = little */
+#define PMEM_GET_ENDIANESS(cb_hdr) \
+	(__u8)((cb_hdr)->data.arch_info>>7)
+
+/* combine ARCH and endianess to get the arch_info */
+#define PMEM_ARCH_INFO	((PMEM_ARCH_LITTLE_ENDIAN<<7) + PMEM_ARCH_LOCAL)
+
+/* Allocation types */
+#define PMEM_ALLOC_TYPE_LOG    1
+
+
+/* Log data definitions */
+struct pmem_region_data_hdr
+{
+   __u32 start_offset;        /* Offset for the start of the region */
+   __u32 end_offset;          /* Offset for the end of the region */
+   __u32 current_offset;      /* Offset for the current location */
+   __u32 lost_logs;           /* Number of logs that were tossed */
+} __attribute__ ((aligned (8)));
+
+struct pmem_log_desc_index_data
+{
+   char desc[PMEM_DESC_MAX]; /* Region description */
+   __u16 oldest_index;       /* Index of oldest log in descriptor array */
+   __u16 curr_index;         /* Index of last written log in descriptor array */
+} __attribute__ ((aligned (8)));
+
+struct pmem_log_desc_index
+{
+   struct pmem_log_desc_index_data data;
+   __u32 checksum;
+} __attribute__ ((aligned (8)));
+
+struct pmem_log_desc_data
+{
+   __u64 hrtime;              /* High resolution time stamp of log */
+   __u32 sec;                 /* timeval seconds */
+   __u32 usec;                /* timeval useconds */
+   __u32 size;                /* Size of log data */
+   __u32 offset;              /* Offset of log data */
+   __u32 log_checksum;        /* Checksum of log data */
+} __attribute__ ((aligned (8)));
+
+struct pmem_log_desc
+{
+   struct pmem_log_desc_data data;
+   __u32 checksum;
+} __attribute__ ((aligned (8)));
+
+
+/* Checksum function for use in the kernel and in user space.
+ * - Must be architecture independant
+ * - Since it deals with 32bit chunks, the area should be 32bit aligned
+ */
+static inline __u32 pmem_checksum( __u32 *data, int len )
+{
+	__u32 sum = 0xffff;
+	int i = 0;
+
+	for (i=0; i<len/4; i++) {
+		sum += *((__u32*)data + i);
+	}
+
+	return(sum);
+}
+
+/* Validate the checksum in the data structure and return result */
+#define PMEM_VALIDATE_CHECKSUM(element) \
+   ({ ( pmem_checksum((__u32*)&((element)->data), sizeof((element)->data)) != \
+        (element)->checksum ) ? -1 : 0; })
+
+#define PMEM_VALIDATE_CB_CHECKSUM(cb) \
+   ({ ( pmem_checksum((__u32*)&((cb)->data), sizeof((cb)->data)) != \
+       ntohl((cb)->checksum) ) ? -1 : 0; })
+
+
+/* Driver structures */
+struct pmem_set_part
+{
+    char desc[PMEM_DESC_MAX]; /* Partition string id */
+};
+
+struct pmem_set_block
+{
+    __s8 block_id;            /* Block id */
+};
+
+struct pmem_set_region
+{
+    char desc[PMEM_DESC_MAX]; /* Region string id */
+    __s8 block_id;            /* Block id */
+};
+
+struct pmem_reg_part
+{
+    char desc[PMEM_DESC_MAX]; /* Partition string id */
+    __u32 size;               /* Size of partition data */
+    __u16 num_blocks;         /* Number of blocks */
+    __u8  version;            /* Version of partition */
+};
+
+struct pmem_reg_region
+{
+    char desc[PMEM_DESC_MAX]; /* Region string id */
+    __u32 size;           /* Size of region data */
+    __u32 flags;          /* See below for flags */
+    __u32 fixed_size;     /* Size of fixed size log. 0 for variable size logs */
+    __u16 num_log_desc;   /* Number of variable size log descriptors */
+    __u8  version;        /* Version of region */
+    __s8  block_id;       /* Block id for resulting region handle */
+};
+
+struct pmem_cb_info
+{
+    __u32 size;           /* Size of all of persistent memory */
+    __u16 num_part;       /* Number of partitions registered */
+    __u32 avail_mem;      /* Available memory */
+    __u8  num_cpus;	  /* Number of CPUs supported by pmem */
+};
+
+struct pmem_part_info
+{
+    __u32 size;           /* Size of partition data */
+    __u16 num_blocks;     /* Number of blocks in partition */
+    __u16 num_regions;    /* Number of regions provisioned */
+    __u32 avail_mem;      /* Available memory */
+    __s8  active_block_id; /* Block id of active block */
+};
+
+struct pmem_block_info
+{
+    __u32 size;           /* Size of block data */
+    __u32 sec;            /* timeval seconds when block became active */
+    __u32 usec;           /* timeval useconds when block became active */
+    __u8 active;          /* Activity boolean */
+    __u8 locked;          /* Locked boolean */
+};
+
+struct pmem_region_info
+{
+    __u32 size;           /* Size of region data */
+    __u32 fixed_size;     /* Size of fixed size logs */
+    __u16 num_logs;       /* Number of log descriptors */
+    __u16 num_cpu_areas;  /* Number of cpu areas in the region */
+};
+
+/* Persistent memory header definitions
+ *
+ * NOTE: All headers in persistent memory must be 64-bit aligned 
+ */
+
+struct pmem_cb_hdr_data {
+	__u32 size;		/* Size of all of persistent memory in bytes */
+	__u32 flags;		/* Flags for CB status */
+	__u8 version;		/* Version of the persistent memory data structures */
+	__u8 num_cpus;		/* Total number of cpus in the product */
+	__u8 arch_info;		/* Architecture enum (lower 7 bits) and endianess (upper bit) */
+	__u8 num_alloc;		/* Number of allocation table entries */
+} __attribute__ ((aligned(8)));
+
+struct pmem_cb_hdr {
+	struct pmem_cb_hdr_data data;
+	__u32 checksum;
+} __attribute__ ((aligned(8)));
+
+struct pmem_alloc_desc_data {
+	__u32 offset;		/* Offset from beginning of persistent memory to partition */
+	__u32 size;		/* Size of partition, including header */
+	__u8 type;		/* Type of partition */
+	char desc[PMEM_DESC_MAX];	/* Partition identifier */
+} __attribute__ ((aligned(8)));
+
+struct pmem_alloc_desc {
+	struct pmem_alloc_desc_data data;
+	__u32 checksum;
+} __attribute__ ((aligned(8)));
+
+struct pmem_part_hdr_data {
+	__u32 size;		/* Size of partition data */
+	__u32 flags;		/* Status flags */
+	__u8 num_blocks;	/* Number of blocks in partition */
+	__u8 num_regions;	/* Number of regions in partition */
+	__u8 version;		/* Version for partition */
+	char desc[PMEM_DESC_MAX];	/* String description of partition */
+} __attribute__ ((aligned(8)));
+
+struct pmem_part_hdr {
+	struct pmem_part_hdr_data data;
+	__u32 checksum;
+} __attribute__ ((aligned(8)));
+
+struct pmem_block_hdr_data {
+	__u32 offset;		/* Offset of block from beginning of partition header */
+	__u32 size;		/* Size of the block data */
+	__u32 sec;		/* timeval seconds of when block became active */
+	__u32 usec;		/* timeval useconds of when block became active */
+	__u32 flags;		/* Status flags */
+} __attribute__ ((aligned(8)));
+
+struct pmem_block_hdr {
+	struct pmem_block_hdr_data data;
+	__u32 checksum;
+} __attribute__ ((aligned(8)));
+
+struct pmem_region_hdr_data {
+	__u32 size;		/* Size of the region data */
+	__u32 offset;		/* Offset of region from beginning of block data */
+	__u32 flags;		/* Behaviour flags */
+	__u32 fixed_size;	/* Size of fixed logs */
+	__u16 num_log_desc;	/* Number of log descriptors */
+	__u8 version;		/* Version for region */
+	char desc[PMEM_DESC_MAX];	/* String description of region */
+} __attribute__ ((aligned(8)));
+
+struct pmem_region_hdr {
+	struct pmem_region_hdr_data data;
+	__u32 checksum;
+} __attribute__ ((aligned(8)));
+
+/********************************************************************
+ *
+ * Kernel exclusive definitions start here
+ *
+ *******************************************************************/
+
+#ifdef __KERNEL__
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <asm/io.h>
+
+#ifdef CONFIG_PMEM_DEBUG
+#define PMEM_DPRINT(...) do{printk("%s:%d, ", __FILE__, __LINE__); printk(__VA_ARGS__);} while(0)
+#else
+#define PMEM_DPRINT(...)
+#endif
+
+#ifndef CONFIG_SMP
+#define PMEM_NUM_SUPPORT_CPUS 1
+#else
+/* CONFIG_NR_CPUS must be set to the number of CPUs visible to the system.
+ * Failure to set this properly may cause crashes or wasted memory in percpu
+ * regions */
+#define PMEM_NUM_SUPPORT_CPUS CONFIG_NR_CPUS
+#endif
+
+#if defined(CONFIG_PPC32)
+#define PMEM_ARCH_LOCAL  PMEM_ARCH_PPC32
+#elif defined(CONFIG_X86)
+#define PMEM_ARCH_LOCAL  PMEM_ARCH_X86
+#elif defined(CONFIG_ARM)
+#define PMEM_ARCH_LOCAL  PMEM_ARCH_ARM
+#elif defined(CONFIG_MIPS64)
+#define PMEM_ARCH_LOCAL  PMEM_ARCH_MIPS64
+#elif defined(CONFIG_PPC64)
+#define PMEM_ARCH_LOCAL  PMEM_ARCH_PPC64
+#else
+#error "Unknown host architecture"
+#endif
+
+/* Macros */
+
+/* Get header structures */
+#define PMEM_GET_ALLOC_DESC(index) \
+   (struct pmem_alloc_desc*)((void*)pmem.pmem + sizeof(struct pmem_cb_hdr)) + index
+
+#define PMEM_GET_PART_HDR(alloc_desc) \
+   (struct pmem_part_hdr*)((void*)pmem.pmem + (alloc_desc)->data.offset)
+
+#define PMEM_GET_BLOCK_HDR(part_hdr, index) \
+   ((index) < (part_hdr)->data.num_blocks ? \
+   ((struct pmem_block_hdr*)((void*)(part_hdr) + \
+   sizeof(struct pmem_part_hdr)) + (index)) : NULL)
+
+#define PMEM_GET_REGION_HDR(part_hdr, index) \
+   ((index) < (part_hdr)->data.num_regions ? \
+   (struct pmem_region_hdr*)((void*)(part_hdr) + \
+   sizeof(struct pmem_part_hdr) + (part_hdr)->data.num_blocks * \
+   sizeof(struct pmem_block_hdr)) + (index) : NULL)
+
+/* Update the checksum in the data structure */
+#define PMEM_UPDATE_CHECKSUM(element) \
+   do { (element)->checksum = pmem_checksum((__u32*)&(element)->data, \
+        sizeof((element)->data)); } while (0)
+
+#define PMEM_UPDATE_CB_CHECKSUM(cb) \
+   do { (cb)->checksum = htonl( pmem_checksum((__u32*)&(cb)->data, \
+        sizeof((cb)->data)) ); } while (0)
+
+/* This macro checks if the offsets in the desired element overlap with any other
+ * elements in the array */
+#define PMEM_CHECK_FOR_OVERLAP(array_ptr, element_index, num_elements) \
+({  \
+   typeof (element_index) index_ = 0; \
+   \
+   while ( index_ < (num_elements) ) { \
+   \
+      if ( (index_ == (element_index)) || ((array_ptr)[index_].data.size == 0)) {\
+         index_++; \
+         continue; \
+      } \
+   \
+      if ( pmem_is_overlapping ( (array_ptr)[(element_index)].data.offset, \
+	                        (array_ptr)[(element_index)].data.size, \
+                            (array_ptr)[index_].data.offset, \
+							(array_ptr)[index_].data.size) ) {\
+         break; \
+      } \
+   \
+      index_++; \
+   } \
+   \
+   (index_ < (num_elements)) ? -1 : 0; \
+})
+
+/* On some architectures it may be necessary to force the system to
+ * write the data to main memory. Otherwise, if the system goes down
+ * before the cache is cleared, only some of the data will have been
+ * written.
+ */
+#ifdef CONFIG_PPC32
+static inline void pmem_arch_flush_cache(void *ptr, int size)
+{
+	void *tmp_ptr = ptr;
+	int tmp_size = size;
+	do {
+		asm volatile("dcbf 0,%0" : : "r" (tmp_ptr) : "memory");
+		tmp_ptr += 32;
+		tmp_size -= 32;
+	} while (tmp_size > 0);
+}
+#define PMEM_FLUSH_CACHE(ptr, size) pmem_arch_flush_cache((ptr),(size))
+#else
+#define PMEM_FLUSH_CACHE(ptr, size)
+#endif
+
+
+/* Dynamic structure definitions */
+struct pmem_cb_data {
+	struct pmem_cb_hdr *pmem;	/* Pointer to beginning of persistent memory */
+	__u32 size;			/* Size of all of persistent memory */
+	spinlock_t lock;		/* Lock for header access */
+	struct list_head part_list;	/* Pointer to linked list of partition data */
+	void *shadow;			/* Pointer to shadow copy of persistent memory */
+	__u8 enabled;			/* Indicates if persistent memory api is enabled */
+	__u8 using_hardware;		/* Indicates if pmem is using hardware */
+	__u8 using_io_mem;		/* Does pmem need to use the _io() family of functions to read/write to the hardware */
+};
+
+struct part_list_elem {
+	struct list_head list_elem;	/* Partition linked list element */
+	struct pmem_part_hdr *hdr;	/* Pointer to partition header in pmem */
+	__u32 active_block_offset;	/* Offset of active block in pmem */
+	struct list_head region_list;	/* Linked list of regions in partition */
+};
+
+struct region_list_elem {
+	struct list_head list_elem;	/* Region linked list element */
+	struct pmem_region_hdr *hdr;	/* Pointer to region header in pmem */
+	struct pmem_ptr_block *ptr_block;/* Registered pointer block */
+	spinlock_t data_lock;		/* Lock for data access */
+};
+
+/* Handle definitions */
+#define PMEM_HANDLE_TYPE_CB      1
+#define PMEM_HANDLE_TYPE_PART    2
+#define PMEM_HANDLE_TYPE_BLK     3
+#define PMEM_HANDLE_TYPE_REG     4
+#define PMEM_HANDLE_TYPE_SHADOW  5
+
+struct pmem_handle {
+	int type;
+	int offset;
+	int size;
+};
+
+struct part_handle {
+	struct pmem_handle hdl;
+	struct part_list_elem *elem;
+};
+
+struct block_handle {
+	struct pmem_handle hdl;
+	struct part_handle *parent;
+	int block_id;
+};
+
+struct region_handle {
+	struct pmem_handle hdl;
+	struct part_handle *parent;
+	struct region_list_elem *elem;
+	int block_id;
+};
+
+struct cb_handle {
+	struct pmem_handle hdl;
+};
+
+struct shadow_handle {
+	struct pmem_handle hdl;
+};
+
+/* Kernel API data structures */
+
+typedef void* pmem_handle_t;
+
+struct pmem_cpu_ptrs
+{
+   char *start;
+   char *end;
+   char *curr;
+   __u32 *lost_logs;
+   spinlock_t lock;
+};
+
+struct pmem_ptr_block_per_cpu
+{
+   struct pmem_cpu_ptrs *ptrs;
+   struct pmem_cpu_ptrs storeA;
+   struct pmem_cpu_ptrs storeB;
+};
+
+struct pmem_ptr_block
+{
+   struct pmem_ptr_block_per_cpu cpu[PMEM_NUM_SUPPORT_CPUS];
+};
+
+/* event callbacks - function ptrs that are executed on pmem events
+ * Used to keep the pmemfs VFS tree in sync with changes made by kernel
+ * space code */
+struct pmem_event_block {
+	void (*create_partition)(pmem_handle_t part_hdl);
+	void (*create_region)(pmem_handle_t part_hdl, pmem_handle_t region_hdl);
+};
+
+/* Global variables */
+extern struct pmem_cb_data pmem;
+extern struct pmem_event_block pmem_events;
+
+/* PMEM specific memory manipulation functions that know whether to use
+ * the commands for ioremapped memory or use the standard memcpy() family */
+/* memset() memcpy_fromio() memcpy_toio() */
+
+static inline void __pmem_memset(void *s, int c, size_t count)
+{
+	if (pmem.using_io_mem)
+		memset_io(s, c, count);
+	else
+		memset(s, c, count);
+}
+
+static inline void pmem_memset(void *s, int c, size_t count)
+{
+	int n, num_blocks, last_block_size;
+
+	num_blocks = count / PAGE_SIZE;
+	last_block_size = count % PAGE_SIZE;
+
+	for (n = 0; n < num_blocks; n++) {
+		__pmem_memset(s + (PAGE_SIZE * n), c, PAGE_SIZE);
+		schedule();
+	}
+
+	__pmem_memset(s + (PAGE_SIZE * n), c, last_block_size);
+}
+
+static inline void __pmem_memcpy_fromio(void *dst, const void *src, int count)
+{
+	if (pmem.using_io_mem)
+		memcpy_fromio(dst, src, count);
+	else
+		memcpy(dst, src, count);
+}
+
+static inline void pmem_memcpy_fromio(void *dst, const void *src, int count)
+{
+	int n, num_blocks, last_block_size;
+
+	num_blocks = count / PAGE_SIZE;
+	last_block_size = count % PAGE_SIZE;
+
+	for (n = 0; n < num_blocks; n++) {
+		__pmem_memcpy_fromio(dst + (PAGE_SIZE * n), src + (PAGE_SIZE * n), PAGE_SIZE);
+		schedule();
+	}
+
+	__pmem_memcpy_fromio(dst + (PAGE_SIZE * n), src + (PAGE_SIZE * n), last_block_size);
+}
+
+static inline void __pmem_memcpy_toio(void *dst, const void *src, int count)
+{
+	if (pmem.using_io_mem)
+		memcpy_toio(dst, src, count);
+	else
+		memcpy(dst, src, count);
+}
+
+static inline void pmem_memcpy_toio(void *dst, const void *src, int count)
+{
+	int n, num_blocks, last_block_size;
+
+	num_blocks = count / PAGE_SIZE;
+	last_block_size = count % PAGE_SIZE;
+
+	for (n = 0; n < num_blocks; n++) {
+		__pmem_memcpy_toio(dst + (PAGE_SIZE * n), src + (PAGE_SIZE * n), PAGE_SIZE);
+		schedule();
+	}
+
+	__pmem_memcpy_toio(dst + (PAGE_SIZE * n), src + (PAGE_SIZE * n), last_block_size);
+
+}
+
+/* Arch specific functions.  Implement each of these to add pmem support
+ * for a new target
+ */
+#ifdef CONFIG_PMEM_HARDWARE
+/* Architecture defined function to retreive information about where
+ * persistent memory is */
+extern int pmem_arch_get_persistent_memory(void **ptr, __u32 *size);
+/* Architecture defined function to setup cache information */
+extern unsigned long pmem_arch_pgprot_noncached(unsigned long prot);
+/* Architecture defined routine that retuns the size of the pmem segment
+ * This is used by persistent_mem_crc */
+extern int pmem_arch_get_size(void);
+
+/* Architecture specific pointers used to get the base pmem pointer
+ * and in the CRC check.  There is no cleanup function for the returned
+ * pointer and they may be called multiple times - so it is recommended 
+ * that they are implemented by caching the value of the pointers inside 
+ * the hardware specific implementation. Not all functions are 
+ * implemented for all architectures. */
+extern void *pmem_arch_get_start_ptr(void);
+extern void *pmem_arch_get_phys_start_ptr(void);
+extern void *pmem_arch_get_raw_start_ptr(void);
+extern void *pmem_arch_get_pci_start_ptr(void);
+extern void *pmem_arch_get_checksum_start_ptr(void);
+extern int pmem_arch_get_pci_resource_num(void);
+extern struct pci_dev *pmem_arch_get_pci_dev(struct pci_dev *dev);
+
+/* Are the returned pmem pointers created with ioremap() ?  This is 
+ * imporatant for the proper memory access functions to be used. */
+extern int pmem_arch_uses_ioremap(void);
+
+#endif /* CONFIG_PMEM_HARDWARE */
+
+
+
+/* API Function declarations */
+/* Registration functions */
+
+/* Register a partition. Updates handle with a partition handle if successful */
+extern int pmem_partition_reg(struct pmem_reg_part *part,
+                pmem_handle_t *handle);
+/* Register a region.Updates handle with a region handle if successful */
+extern int pmem_region_reg(pmem_handle_t part_handle,
+                struct pmem_reg_region *region, pmem_handle_t *handle);
+/* Register a pointer block with a region. */
+extern int pmem_register_ptr_block(pmem_handle_t region_handle,
+                struct pmem_ptr_block *ptr_block);
+
+/* Handle functions */
+
+/* Get a control block handle */
+extern int pmem_control_block_get(pmem_handle_t *handle);
+/* Get a partition handle */
+extern int pmem_partition_get(char *desc, pmem_handle_t *handle);
+/* Get a block handle */
+extern int pmem_block_get(pmem_handle_t part_handle, __s8 block_id,
+                pmem_handle_t *handle);
+/* Get a region handle */
+extern int pmem_region_get(pmem_handle_t part_handle,
+                char *desc, __s8 block_id, pmem_handle_t *handle);
+/* Get a shadow handle */
+extern int pmem_shadow_get(pmem_handle_t *handle);
+/* Release a handle - free its storage memory */
+extern void pmem_release_handle(pmem_handle_t *handle);
+/* Get the parent handle */
+extern pmem_handle_t pmem_get_handle_parent(pmem_handle_t handle);
+
+/* IO functions */
+
+/* Write to persistent memory from a kernel space buffer */
+extern int pmem_write_data(pmem_handle_t handle, const char *buffer, int size);
+/* Write to a handle's header area from a kernel space buffer */
+extern int pmem_write_header(pmem_handle_t handle, const char *buffer,
+                int size, loff_t offset);
+/* Read from persistent memory data segment into kernel space buffer */
+extern int pmem_read_data(pmem_handle_t handle, char *buffer, int size);
+/* Read from a handles header area for a kernel space buffer */
+extern int pmem_read_header(pmem_handle_t handle, char *buffer, int size,
+                loff_t offset);
+/* Adjust handle current offset using fseek() semantics */
+extern int pmem_seek(pmem_handle_t handle, int offset, int whence);
+/* Commands */
+
+/* Set the reset flag */
+extern void pmem_set_cb_reset_flag(void);
+/* Clear the reset flag */
+extern void pmem_clear_cb_reset_flag(void);
+/* Rotate blocks in a partition */
+extern int pmem_part_rotate(pmem_handle_t handle, int lock);
+/* Lock a block for rotation */
+extern int pmem_lock_block(pmem_handle_t handle);
+/* Unlock a block for rotation */
+extern int pmem_unlock_block(pmem_handle_t handle);
+/* Clear the shadow copy of persistent memory */
+extern int pmem_clear_shadow(void);
+/* Request control block information */
+extern int pmem_get_cb_info(pmem_handle_t handle, struct pmem_cb_info *info);
+/* Request partition information */
+extern int pmem_get_part_info(pmem_handle_t handle,
+		struct pmem_part_info *info);
+/* Request block information */
+extern int pmem_get_block_info(pmem_handle_t handle,
+		struct pmem_block_info *info);
+/* Request region information */
+extern int pmem_get_region_info(pmem_handle_t handle,
+		struct pmem_region_info *info);
+/* Disable all pointer blocks in a partition */
+extern int pmem_disable_ptr_blocks(pmem_handle_t part_handle);
+/* Destroy the registered pointer block for a given region */
+extern int pmem_destroy_ptr_block(pmem_handle_t reg_handle);
+/* Lock an entire partition */
+extern int pmem_lock_part(pmem_handle_t part_handle);
+/* Unlock a partition */
+extern int pmem_unlock_part(pmem_handle_t part_handle);
+/* Lock all of pmem  */
+extern int pmem_lock(void);
+/* Unlock all of pmem */
+extern int pmem_unlock(void);
+/* Lock the active segments in all paritions - lock all active segments */
+extern int pmem_lock_all_active_segments(void);
+/* Unlock the active segments in all paritions - unlock all active segments */
+extern int pmem_unlock_all_active_segments(void);
+/* Rotate the active segment on all partitions in pmem */
+extern int pmem_rotate(int lock);
+/* Clear a region back to its original state.  
+ * Intended for use by the vMC module */
+extern int pmem_clear_region(pmem_handle_t region_handle);
+
+/* Misc helper routines */
+/* get the size of a handles header */
+extern ssize_t pmem_get_hdr_size(pmem_handle_t handle);
+/* Get the block index of the active block */
+extern int pmem_get_active_block_index(struct pmem_part_hdr *part_hdr);
+
+extern void* pmem_get_data_ptr(struct pmem_handle *hdl);
+extern void* pmem_get_hdr_ptr(struct pmem_handle *hdl);
+
+extern int pmem_rotate_block_data(struct part_list_elem *part_elem, int lock);
+extern void pmem_update_ptr_block(struct part_list_elem *part_elem,
+                                  struct region_list_elem *region_elem,
+                                  int lock);
+extern int pmem_is_overlapping(__u32 offset_a, __u32 size_a, __u32 offset_b,
+		               __u32 size_b);
+extern void pmem_prepare_region(struct pmem_part_hdr *part_hdr,
+                __u32 block_offset, struct pmem_region_hdr *region_hdr);
+
+/* Default notification callback for partitions/regions - do nothing */
+extern void pmem_default_create_partition(pmem_handle_t handle);
+extern void pmem_default_create_region(pmem_handle_t parent, pmem_handle_t region);
+
+#endif /* __KERNEL__ */
+
+#endif
diff --git a/include/linux/schedhist.h b/include/linux/schedhist.h
new file mode 100644
index 0000000..9e3a0fc
--- /dev/null
+++ b/include/linux/schedhist.h
@@ -0,0 +1,95 @@
+#ifndef _SCHED_HIST_H
+#define _SCHED_HIST_H
+
+#include <linux/types.h>
+
+/* 
+ * scheduler process history stuff - userspace section
+ */
+
+#define PROC_NAME_SIZE 7            /* length of proc names to store */
+
+typedef __u64 __attribute__((aligned(8))) aligned__u64;
+
+/* For 32-bit userspace, and 64-bit kernels, we have to store the
+ * the timestamp data in something the userspace can understand.
+ * Always use a fixed size timestamp in stored sched_hist_entries
+ *  - 64-bit tv_sec and a 32-bit tv_usec
+ */
+typedef struct sched_timeval {
+	aligned__u64 tv_sec;
+	__u32 tv_usec;
+} sched_timeval __attribute__((aligned(8)));
+
+/* Note:
+ * if pid2 == -1, this is an event from a process
+ * being added to the runqueue, otherwise its a scheduler
+ * event.
+ * These fields are sized according to architecture
+ */
+#define SCHED_HIST_ENTRY_COMMON \
+	struct sched_timeval  tv;        /* normal timestamp */ \
+	aligned__u64          hrts;    /* high res timestamp */ \
+	__s32                 pid1;    /* duh */ \
+	aligned__u64          nip1;    /* next instruction pointer */ \
+	char                  pname1[PROC_NAME_SIZE + 1]; /* process name */ \
+	__s32                 pid2;    /* duh */ \
+	aligned__u64          nip2;    /* next instruction pointer */ \
+	char                  pname2[PROC_NAME_SIZE + 1]; /* process name */
+
+/* Define this to make it easier to work with the common part of
+ * the structure. This is what is stored in the scheduler history buffer
+ */
+typedef struct sched_hist_entry_common
+{
+	SCHED_HIST_ENTRY_COMMON
+} sched_hist_entry_common __attribute__((aligned(8)));
+
+/* User-land schedule history entry. */
+typedef struct sched_hist_entry
+{
+        SCHED_HIST_ENTRY_COMMON
+        int cpu;                    /* CPU doing the scheduling. */
+} sched_hist_entry __attribute__((aligned(8)));
+
+/* 
+ * scheduler process history stuff - kernel section 
+ * userspace applications wont know about these definitions
+ */
+#ifdef __KERNEL__
+
+#if 0
+#define SCHED_HIST_ENTRIES 6000     /* number of entries */
+
+/* For SMP support, we increase the scheduler history buffer to 12000
+ * entries, and then divide by the number of CPUs. In otherwords, each
+ * CPU will have 12000 / smp_num_cpus entries to work with.
+ */
+#define SCHED_HIST_NUMENTRIES(numCpus) \
+	(numCpus > 1 ? SCHED_HIST_ENTRIES * 2 : SCHED_HIST_ENTRIES)
+#define SCHED_HIST_ENTRIESPERCPU(numCpus) \
+	(SCHED_HIST_NUMENTRIES(numCpus) / numCpus);
+
+#endif
+
+/* The kernel doesn't need to track which CPU is scheduling, but it
+ * on SMP systems it does need to track whether the entry is being
+ * updated. 
+ */
+typedef struct sched_hist_kernentry
+{
+	SCHED_HIST_ENTRY_COMMON
+#ifdef CONFIG_SMP
+	int update;                 /* This entry is being updated. */
+#endif
+} sched_hist_kernentry __attribute__((aligned(8)));
+
+#endif /* __KERNEL__ */
+/* Persistent memory definitions */
+#define PMEM_REG_SCHED_DESC "sched_hist"
+#define PMEM_REG_SCHED_FIXED_SIZE sizeof(struct sched_hist_entry_common)
+#define PMEM_REG_SCHED_LOG_DESC 0
+#define PMEM_REG_SCHED_FLAGS PMEM_REGION_FLAG_PERPROC
+#define PMEM_REG_SCHED_VERSION 1
+
+#endif /* _SCHED_HIST_H */
diff --git a/init/Kconfig b/init/Kconfig
index 77966a1..eb538c0 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -633,6 +633,20 @@ config CC_OPTIMIZE_FOR_SIZE
 
 	  If unsure, say Y.
 
+config SCHED_HIST_BUF
+	bool "Scheduler history buffer support"
+	depends on PMEM
+	default n
+	help
+	  When enabled this keeps a history buffer of scheduler events.
+	  Events are logged into the persistent memory framework and
+	  can be read throught the pmem interface.
+
+config SCHED_HIST_SIZE
+	int "Scheduler history buffer size"
+	depends on SCHED_HIST_BUF
+	default 262144
+
 config SYSCTL
 	bool
 
@@ -981,6 +995,8 @@ config BASE_SMALL
 	default 0 if BASE_FULL
 	default 1 if !BASE_FULL
 
+source mm/pmem/Kconfig
+
 menuconfig MODULES
 	bool "Enable loadable module support"
 	help
diff --git a/init/main.c b/init/main.c
index 0c7e00d..911da54 100644
--- a/init/main.c
+++ b/init/main.c
@@ -82,6 +82,12 @@
 #warning gcc-4.1.0 is known to miscompile the kernel.  A different compiler version is recommended.
 #endif
 
+#ifdef CONFIG_PANIC_LOGS
+#include <linux/pmem.h>
+extern pmem_handle_t kcore_part_hdl;
+extern pmem_handle_t kcore_reg_hdl;
+#endif
+ 
 static int kernel_init(void *);
 
 extern void init_IRQ(void);
diff --git a/kernel/sched.c b/kernel/sched.c
index bd4d65a..7bc481b 100644
--- a/kernel/sched.c
+++ b/kernel/sched.c
@@ -118,6 +118,84 @@
  */
 #define RUNTIME_INF	((u64)~0ULL)
 
+#ifdef CONFIG_SCHED_HIST_BUF
+#include <linux/pmem.h>
+#include <linux/schedhist.h>
+#include <linux/time.h>
+
+/* Use pmem api to store scheduler history */
+struct pmem_ptr_block sched_hist_pmem_block = { };
+
+/**
+ * schedhist_addentry - add an entry to the scheduler history buffer.
+ * @prev: task that we are coming from (must not be NULL).
+ * @next: task that we are going to (or NULL).
+ *
+ * Adds an entry to the scheduler history buffer
+ * For 2.4.x, this call should only be made when the runqueue_lock
+ * is locked with a spin_lock.
+ */
+#define offset_of(_s, _m) \
+	&((_s *)0)->_m
+
+static inline void sched_hist_addentry(struct task_struct *prev,
+                                       struct task_struct *next)
+{
+
+	struct sched_hist_entry_common *entry;
+	struct pmem_cpu_ptrs *ptrs;
+
+	ptrs = sched_hist_pmem_block.cpu[smp_processor_id()].ptrs;
+
+	/* Verify ptr block has been registered */
+	if ((!ptrs) || (!prev))
+		return;
+
+	/* Check for wrap around. We can assume that the end pointer
+	 * is a multiple of the entry size since the API ensures this. */
+	ptrs->curr += sizeof(struct sched_hist_entry_common);
+	if (unlikely(ptrs->curr == ptrs->end)) {
+		ptrs->curr = ptrs->start;
+	}
+	entry = (struct sched_hist_entry_common*)ptrs->curr;
+
+	/* get timestamp and TBR values */
+	entry->tv.tv_sec = (__u64)xtime.tv_sec;
+	entry->tv.tv_usec = (__u32)xtime.tv_nsec/1000;
+#ifdef CONFIG_HIGH_RES_TIMERS
+	entry->hrts = (__u64)gethrtime();
+#else
+	entry->hrts = (__u64)get_cycles();
+#endif
+
+	/* store info on process being scheduled out
+	 * Trying to call thread_saved_pc() on a RUNNING task on i386
+	 * will panic.  */
+	if (prev->state == TASK_RUNNING) {
+		entry->nip1 = (__u64)0;
+	} else {
+		entry->nip1 = (__u64)thread_saved_pc(prev);
+	}
+	//entry->nip1 = (__u64)get_wchan(prev);
+	entry->pid1 = (__s32)prev->pid;
+	strncpy(entry->pname1, prev->comm, PROC_NAME_SIZE);
+
+	/* store info on process being scheduled in */
+	if (next) {
+		entry->nip2 = (__u64)thread_saved_pc(next);
+		entry->pid2 = (__s32)next->pid;
+		strncpy(entry->pname2, next->comm, PROC_NAME_SIZE);
+	} else {
+		entry->pid2 = (__s32)-1;
+		entry->nip2 = (__u64)0;
+		memset(entry->pname2, '\0', PROC_NAME_SIZE);
+	}
+	PMEM_FLUSH_CACHE(entry, 0);
+}
+#else
+#define sched_hist_addentry(prev,next) do { } while(0)
+#endif /* CONFIG_SCHED_HIST_BUF */
+
 #ifdef CONFIG_SMP
 /*
  * Divide a load by a sched group cpu_power : (load / sg->__cpu_power)
@@ -1642,6 +1720,7 @@ static void update_avg(u64 *avg, u64 sample)
 
 static void enqueue_task(struct rq *rq, struct task_struct *p, int wakeup)
 {
+ 	sched_hist_addentry(p, NULL);
 	sched_info_queued(p);
 	p->sched_class->enqueue_task(rq, p, wakeup);
 	p->se.on_rq = 1;
@@ -4460,6 +4539,7 @@ need_resched_nonpreemptible:
 
 	if (likely(prev != next)) {
 		sched_info_switch(prev, next);
+		sched_hist_addentry(prev, next);
 
 		rq->nr_switches++;
 		rq->curr = next;
diff --git a/mm/Makefile b/mm/Makefile
index f8105e0..22bbf9f 100644
--- a/mm/Makefile
+++ b/mm/Makefile
@@ -25,6 +25,7 @@ obj-$(CONFIG_MEMMON) += memmon.o
 obj-$(CONFIG_SHMEM) += shmem.o
 obj-$(CONFIG_TMPFS_POSIX_ACL) += shmem_acl.o
 obj-$(CONFIG_TINY_SHMEM) += tiny-shmem.o
+obj-$(CONFIG_PMEM) += pmem/
 obj-$(CONFIG_SLOB) += slob.o
 obj-$(CONFIG_MMU_NOTIFIER) += mmu_notifier.o
 obj-$(CONFIG_SLAB) += slab.o
diff --git a/mm/pmem/Kconfig b/mm/pmem/Kconfig
new file mode 100644
index 0000000..9ab32af
--- /dev/null
+++ b/mm/pmem/Kconfig
@@ -0,0 +1,129 @@
+# -*- shell-script -*-
+
+menu "Persistent Memory Support"
+
+config PMEM
+	bool "Enable Persistent Memory Support"
+	help
+	  Provide a framework for storing information in non-volatile memory.
+	  A simulation mode uses RAM instead of non-volatile memory.
+
+	  There is a true persistent mode for certain boards that have hardware
+	  support for it.  The current board list is xscale, 6101, f101, 7101, 717.
+
+	  For user space, a file system interface is provided by the PMEMFS pseudo-filesystem.
+	  This filesystem will not be visible in menuconfig or xconfig unless PMEM is enabled.
+	  When PMEM is enabled, PMEMFS is included as a module by default.
+
+
+config PMEM_SIZE
+	int "Total persistent memory size"
+	depends on PMEM
+	default 8388608
+	help
+	  This amount can be adjusted downward to be a multiple of PAGE_SIZE.
+	  There is also a maximum size allowed for each architecture which
+	  may override PMEM_SIZE or cause the kernel compilation to fail.
+
+config PMEM_HARDWARE
+	bool "Enable hardware persistent memory"
+	depends on PMEM && (ARCH_IXP2000 || PPC32 || PPC64 || INTEL_ATCA7101 || INTEL_ATCA717)
+	default n
+
+choice
+	prompt "Hardware Type"
+	depends on PMEM_HARDWARE
+	help
+	  Hardware support for persistent memory.  Choose the appropriate
+	  board type.
+
+config PMEM_HW_ATCAF101
+	bool "ATCA F101"
+	depends on PPC32
+	help
+	  Motorola ATCA F101 with persistent memory on a PCI 9030 Device.
+
+config PMEM_HW_ATCA6101
+	bool "ATCA 6101"
+	depends on PPC64
+	help
+	  Motorola ATCA 6101 with persistent memory on a PMC815 device.
+
+config PMEM_HW_ATCA7101
+	bool "ATCA 7101"
+	depends on INTEL_ATCA7101
+	help
+	  Motorola ATCA 7101 with persistent memory on a PCI 9030 device.
+
+config PMEM_HW_ATCA717
+	bool "ATCA 717"
+	depends on INTEL_ATCA717
+	help
+	  Motorola ATCA 717 with persistent memory on 280 PCI device.
+
+
+endchoice
+
+config FIRMWARE_SIZE
+	int "Firmware Size"
+	depends on PMEM_HW_PPMC280 || PMEM_HW_ATCA6101
+	default 8388608
+	help
+	  How much memory to reserve for the PMC firmware scratch area.
+
+config PMEM_PCI_DRIVER
+	bool
+	default y
+	depends on PMEM_HW_ATCA7101 || PMEM_HW_ATCA717 || PMEM_HW_ATCA6101 || PMEM_HW_ATCAF101
+
+config PMEM_DEBUG
+	bool "Debug persistent memory"
+	depends on PMEM
+	help
+	  Enable debugging printk()'s.
+
+config PMEM_SHADOW
+	bool "Shadow copy support"
+	depends on PMEM
+	help
+	  When re-initializing pmem, copy the contents to a "shadow" area.
+	  This only happens if pmem has hardware support.
+
+config PMEM_LOG_PART_SIZE
+	int "Log partition size"
+	depends on PMEM
+	default 2097152
+	help
+	  A partition in pmem for logging information is always created.  A
+	  general purpose region can be created in this partition
+	  by selecting PMEM_LOG_REG.  Whenever data is written to a pmem
+	  area with an associated log region, the time of the write and the amount
+	  of data written is logged.
+
+config PMEM_LOG_PART_SEGMENTS
+	int "Log partition segments"
+	depends on PMEM
+	default 2
+	help
+	  Divide the logging partition into this many segments.
+
+config PMEM_LOG_REG
+	bool "General log region"
+	depends on PMEM
+	help
+	  Create a general purpose log region in the log partition.
+
+config PMEM_LOG_REG_SIZE
+	int "General log region size"
+	depends on PMEM_LOG_REG
+	default 262144
+
+config PMEM_LOG_REG_LOGS
+	int "General log region number of logs"
+	depends on PMEM_LOG_REG
+	default 500
+	help
+	  The number of logs in the general log region.  Each log is 32 bytes
+	  of data.
+
+endmenu
diff --git a/mm/pmem/Makefile b/mm/pmem/Makefile
new file mode 100644
index 0000000..43d9608
--- /dev/null
+++ b/mm/pmem/Makefile
@@ -0,0 +1,5 @@
+pmem-objs := reg.o handle.o io.o cmds.o
+
+obj-$(CONFIG_PMEM) += pmem.o
+
+
diff --git a/mm/pmem/cmds.c b/mm/pmem/cmds.c
new file mode 100644
index 0000000..4d18cf4
--- /dev/null
+++ b/mm/pmem/cmds.c
@@ -0,0 +1,1077 @@
+#include <linux/slab.h>
+#include <linux/vmalloc.h>
+#include <linux/kernel.h>
+#include <linux/spinlock.h>
+#include <linux/time.h>
+#include <linux/errno.h>
+#include <linux/module.h>
+#include <linux/pmem.h>
+#include <linux/mm.h>
+#include <asm/types.h>
+
+/* constants used for readability */
+static const int PMEM_LOCK =	1;
+static const int PMEM_UNLOCK =	0;
+
+
+/* Get the number of blocks in a partition which are unlocked */
+static int get_num_unlocked_blocks(struct pmem_part_hdr *part_hdr)
+{
+	struct pmem_block_hdr *block_hdr;
+	int count = 0;
+	int i;
+
+	for (i = 0; i < part_hdr->data.num_blocks; i++) {
+		block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, i);
+		if (!(block_hdr->data.flags & PMEM_BLOCK_FLAG_LOCK))
+			count++;
+	}
+
+	return count;
+}
+
+/* Get the amount of free memory for a partition */
+static __u32 get_avail_alloc_mem(void)
+{
+	struct pmem_alloc_desc *alloc_desc;
+	__u32 alloc_table_size;
+	__u32 avail_size;
+
+	alloc_table_size = sizeof (struct pmem_cb_hdr) +
+	    (pmem.pmem->data.num_alloc + 1) * sizeof (struct pmem_alloc_desc);
+
+	if (pmem.pmem->data.num_alloc > 0) {
+		alloc_desc = PMEM_GET_ALLOC_DESC(pmem.pmem->data.num_alloc - 1);
+		avail_size = alloc_desc->data.offset - alloc_table_size;
+	} else {
+		avail_size = pmem.size - alloc_table_size;
+	}
+
+	avail_size -= avail_size % PAGE_SIZE;
+
+	/* Subtract the overhead for adding another partition. This
+	 * shows the number of bytes that the user can allocate */
+	if (avail_size >= PMEM_PART_HDR_MAX_SIZE)
+		return (avail_size - PMEM_PART_HDR_MAX_SIZE);
+	else
+		return (0);
+}
+
+/* Get the amount of free memory for a region */
+static __u32 get_avail_block_mem(struct pmem_part_hdr *part_hdr)
+{
+	struct pmem_block_hdr *block_hdr;
+	struct pmem_region_hdr *region_hdr;
+
+	if (part_hdr->data.num_blocks == 0)
+		return 0;
+
+	/* Just get any block since they all have the same size information */
+	block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, 0);
+
+	if (part_hdr->data.num_regions > 0) {
+		region_hdr =
+		    PMEM_GET_REGION_HDR(part_hdr,
+					part_hdr->data.num_regions - 1);
+		return (block_hdr->data.size -
+			(region_hdr->data.offset + region_hdr->data.size));
+	}
+
+	return (block_hdr->data.size);
+}
+
+/* "lock" the given ptr block or "unlock" depending on the value 
+ * of lock (0 = unlock, !0 = lock)
+ * Always returns success (0)  */
+static inline int __pmem_lock_ptr_block(struct pmem_ptr_block *ptr_block,
+                const int lock)
+{
+	unsigned int i;
+
+	if (!ptr_block) {
+		/* no */
+		PMEM_DPRINT("WARNING: Tried to lock=%d null ptr block\n",
+		            lock);
+		return 0;
+	}
+
+	for (i = 0; i < PMEM_NUM_SUPPORT_CPUS; i++) {
+		if (lock) {
+			/* know that ptrs is referencing one of storeA or
+			 * storeB, and one of storeA.start or storeB.start
+			 * is NULL, so we can zero out the reference w/o loss
+			 * since on the unlock, we can find the correct
+			 * backing store to point ptrs to.  Locking is easy,
+			 * unlocking is where the hairyness comes out. */
+			ptr_block->cpu[i].ptrs = NULL;
+
+		} else {	/* unlock */
+			if (ptr_block->cpu[i].ptrs) {
+				PMEM_DPRINT("ERROR: Refusing to unlock a "
+				            "unlocked ptrblock\n");
+				continue;
+			}
+			if ((ptr_block->cpu[i].storeA.start) &&
+			    (ptr_block->cpu[i].storeB.start)) {
+				PMEM_DPRINT("ERROR: Cant unlock inconsistant "
+				            "ptrblock !\n");
+				continue;
+			}
+
+			if (ptr_block->cpu[i].storeA.start) {
+				ptr_block->cpu[i].ptrs =
+				        &ptr_block->cpu[i].storeA;
+			} else if (ptr_block->cpu[i].storeB.start) {
+				ptr_block->cpu[i].ptrs =
+				        &ptr_block->cpu[i].storeB;
+			} else  {
+				PMEM_DPRINT("ERROR: Cant unlock ptr block - "
+				            "unable to find real ptr block\n");
+			}
+		}
+	}
+	return 0;
+}
+
+/* 
+ * "lock" or "unlock" any ptr blocks for the given partition.
+ * returns 0 on success, -errno on error. */
+static int pmem_lock_ptr_blocks(struct part_handle *part_hdl, const int lock)
+{
+	struct region_list_elem *region_elem;
+	struct list_head *elem, *temp;
+	int rc = 0;
+
+	list_for_each_safe(elem, temp, &(part_hdl->elem->region_list)) {
+		region_elem = list_entry(elem, struct region_list_elem, list_elem);
+		if (region_elem->ptr_block) {
+			(void)__pmem_lock_ptr_block(region_elem->ptr_block, lock);
+		}
+	}
+
+	return rc;
+}
+
+/* Do the locking on a given block header structure. 
+ *
+ * This function does no locking, and doesnt do anything about any
+ * ptr blocks that may point to regions inside the block 
+ *
+ * Always returns 0 since the given block structure will be 
+ * locked upon completion of this code  */
+static inline int pmem_lock_block_hdr(struct pmem_block_hdr *block_hdr)
+{
+	/* Check if the block is already locked */
+	if (block_hdr->data.flags & PMEM_BLOCK_FLAG_LOCK) {
+#ifdef CONFIG_PMEM_DEBUG
+		/* harmless error, but may be interesting for debugging */
+		PMEM_DPRINT("failed to lock block - already locked.\n");
+#endif
+		return 0;
+	}
+
+	block_hdr->data.flags |= PMEM_BLOCK_FLAG_LOCK;
+	PMEM_UPDATE_CHECKSUM(block_hdr);
+	PMEM_FLUSH_CACHE(block_hdr, sizeof(struct pmem_block_hdr));
+
+	return 0;
+}
+
+/* Unlock a given block header.  This function is the companion routine
+ * to pmem_lock_block_hdr(). */
+static inline int pmem_unlock_block_hdr(struct pmem_block_hdr *block_hdr)
+{
+	/* if the block is not locked, then we have nothing to do */
+	if (!block_hdr->data.flags & PMEM_BLOCK_FLAG_LOCK) {
+#ifdef CONFIG_PMEM_DEBUG
+		PMEM_DPRINT("no need to unlock block - not locked.\n");
+#endif
+		return 0;
+	}
+
+	block_hdr->data.flags &= ~PMEM_BLOCK_FLAG_LOCK;
+	PMEM_UPDATE_CHECKSUM(block_hdr);
+	PMEM_FLUSH_CACHE(block_hdr, sizeof(struct pmem_block_hdr));
+
+	return 0;
+}
+
+/* 
+ * Lock a given segment in pmem.  This routine is the internal implementation
+ * which will rotate the block if necessary */
+static int pmem_internal_lock_block(struct block_handle *block_hdl)
+{
+	struct part_handle *part_hdl;
+	struct pmem_part_hdr *part_hdr;
+	struct pmem_block_hdr *block_hdr;
+	int active_block_index, block_index;
+	unsigned long flags;
+	int rc = -EINVAL;
+
+	part_hdl = block_hdl->parent;
+	part_hdr = part_hdl->elem->hdr;
+
+	spin_lock_irqsave(&pmem.lock, flags);
+
+	active_block_index = pmem_get_active_block_index(part_hdr);
+
+	block_index = block_hdl->block_id;
+	if (block_hdl->block_id == PMEM_ACTIVE_BLOCK)
+		block_index = active_block_index;
+
+	/* Check if we can lock the block */
+	if (get_num_unlocked_blocks(part_hdr) <= PMEM_MIN_UNLOCKED_BLOCKS) {
+		/* We cannot lock any more blocks */
+		PMEM_DPRINT("failed to lock block - not enough unlocked " 
+		            "blocks in partition.\n");
+		goto release_done;
+	}
+
+	block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, block_index);
+
+	/* lock this block */	
+	if (pmem_lock_block_hdr(block_hdr)) {
+		PMEM_DPRINT("error locking block header\n");
+		goto release_done;
+	}
+
+	/* Check if we need to rotate the partition */
+	if (block_index == active_block_index) {
+		/* The partition must be rotated. Do not rely on the rotation
+	 	* function to lock the block since we cannot do this 
+		 * atomically. 
+		 */
+		spin_unlock_irqrestore(&pmem.lock, flags);
+		rc = pmem_rotate_block_data(part_hdl->elem, 0);
+		goto done;
+	}
+
+      release_done:
+	spin_unlock_irqrestore(&pmem.lock, flags);
+
+      done:
+	return (rc);
+}
+
+/* Update all of the pointer blocks in a partition with the 
+ * current region information */
+static void pmem_update_part_ptr_blocks(struct part_list_elem *part_elem)
+{
+	struct region_list_elem *region_elem;
+	struct list_head *elem, *temp;
+
+	list_for_each_safe(elem, temp, &(part_elem->region_list)) {
+		region_elem =
+		    list_entry(elem, struct region_list_elem, list_elem);
+		pmem_update_ptr_block(part_elem, region_elem, 0);
+	}
+}
+
+/* Manipulate the lock status of a partition.  This function knows how to 
+ * lock and unlock partitions.  Specify the desired action in lock (unlock = 0,
+ * lock = !0 ).
+ * Returns the number of blocks manipulated inside the partition on success, 
+ * or -errno on error */
+static int pmem_alter_part_lock(pmem_handle_t part_handle, const int lock)
+{
+	int rc = - EINVAL;
+	struct part_handle *hdl = (struct part_handle *) part_handle;
+	struct pmem_part_hdr *part_hdr;
+	struct pmem_block_hdr *block_hdr;
+	unsigned long flags;
+	int i;
+
+	if (!hdl) {
+		PMEM_DPRINT("NULL arguments\n");
+		return rc;
+	}
+
+	/* Ensure that it is a partition handle */
+	if (hdl->hdl.type != PMEM_HANDLE_TYPE_PART) {
+		PMEM_DPRINT("Invalid handle type\n");
+		return rc;
+	}
+	
+	part_hdr = hdl->elem->hdr;
+
+	spin_lock_irqsave(&pmem.lock, flags);
+
+	rc = 0;	
+	/* lock/unlock each block in this partition */
+	for(i = 0; i < part_hdr->data.num_blocks; i++) {
+		block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, i);
+		if (lock) {
+			pmem_lock_block_hdr(block_hdr);
+		} else {
+			pmem_unlock_block_hdr(block_hdr);
+		}
+		rc += 1; /* processed one segment */
+	}
+	/* manipulate the locks on the ptr blocks for this part as well */
+	if (0 != pmem_lock_ptr_blocks(hdl, lock)) {
+		PMEM_DPRINT("WARNING: unable to change locks on ptrblock\n");
+	}
+
+	spin_unlock_irqrestore(&pmem.lock, flags);
+	return rc;
+}
+
+/* Set the lock state on all partitions.  This routine can lock or unlock 
+ * depending on the value of lock.
+ * Returns the number of partitions locked on success, -errno on error */
+static int pmem_alter_global_lock(const int lock)
+{
+	unsigned int i, j;
+	unsigned long flags;
+	struct pmem_part_hdr* part_hdr;
+	struct pmem_block_hdr* block_hdr;
+	pmem_handle_t part_hdl = NULL;
+	int segments = 0;
+
+	part_hdl = (pmem_handle_t) kmalloc(sizeof (struct part_handle), GFP_KERNEL);
+	if (!part_hdl) {
+		PMEM_DPRINT("ERROR: Can't allocate memory for part_hdl for "
+		            "[%s]\n", part_hdr->data.desc);
+		segments = -1;
+		goto release_done;
+	}
+
+
+	spin_lock_irqsave(&pmem.lock, flags);
+
+	/* for each partition */
+	for (i = 0; i < pmem.pmem->data.num_alloc; i++) {
+		part_hdr = PMEM_GET_PART_HDR(PMEM_GET_ALLOC_DESC(i));
+		/* lock/unlock each block in the partition */
+		for (j = 0; j < part_hdr->data.num_blocks; j++) {
+			block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, j);
+			/* lock/unlock the header */
+			if (lock) {
+				if (!pmem_lock_block_hdr(block_hdr)) {
+					segments += 1; /* success */
+				}
+			} else {
+				if (!pmem_unlock_block_hdr(block_hdr)) {
+					segments += 1;
+				}
+			}
+		}
+
+		/* lock / unlock any ptr_blocks in this partition */
+		if (0 != pmem_partition_get(part_hdr->data.desc, &part_hdl)) {
+			PMEM_DPRINT("ERROR: Cant get part hdl for [%s]\n",
+			            part_hdr->data.desc);
+			segments = -1;
+			goto release_done;
+		}
+		if (0 != pmem_lock_ptr_blocks((struct part_handle*)part_hdl,
+		                               lock)) {
+			PMEM_DPRINT("WARNING:Unable to change ptrblock lock\n");
+		}
+	}
+
+release_done:
+	spin_unlock_irqrestore(&pmem.lock, flags);
+	pmem_release_handle(&part_hdl);
+	return segments;
+}
+
+/* Lock or unlock all active segments in pmem depending on the value
+ * of lock (unlock on 0. lock on non-zero).  This function cares about
+ * the number of blocks locked in a partition - if it fails, check to make
+ * sure you meet the minimum number of free unlocked blocks
+ *
+ * Returns the number of active segments that were locked.  */
+static int pmem_alter_active_segment_lock(const int lock)
+{
+	unsigned int i, j, unlocked_blocks, segments = 0;
+	unsigned long flags;
+	struct pmem_part_hdr* part_hdr;
+	struct pmem_block_hdr* block_hdr;
+	pmem_handle_t part_hdl = NULL;
+
+	part_hdl = (pmem_handle_t) kmalloc(sizeof (struct part_handle), GFP_KERNEL);
+	if (!part_hdl) {
+		PMEM_DPRINT("ERROR: Can't allocate memory for part_hdl for "
+		            "[%s]\n", part_hdr->data.desc);
+		segments = -1;
+		goto release_done;
+	}
+
+	spin_lock_irqsave(&pmem.lock, flags);
+	
+	/* for each partition */
+	for (i = 0; i < pmem.pmem->data.num_alloc; i++) {
+		part_hdr = PMEM_GET_PART_HDR(PMEM_GET_ALLOC_DESC(i));
+		/* if there is the min free unlocked blocks is not met on a 
+		 * lock request, then leave this partition alone */
+		if (lock) {
+	   		unlocked_blocks = get_num_unlocked_blocks(part_hdr);
+			if (unlocked_blocks <= PMEM_MIN_UNLOCKED_BLOCKS) {
+				continue;
+			}
+		}
+		/* find the active segment and lock/unlock it */
+		for (j = 0; j < part_hdr->data.num_blocks; j++) {
+			block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, j);
+			if (!(block_hdr->data.flags & PMEM_BLOCK_FLAG_ACTIVE)) 
+				continue;
+			/* lock/unlock the header */
+			if (lock) {
+				if (!pmem_lock_block_hdr(block_hdr)) {
+					segments += 1;
+				}
+			} else  {
+				if (!pmem_unlock_block_hdr(block_hdr)) {
+					segments += 1;
+				}
+				break;  /* skip the rest of this partition */
+			}
+		} /* for each segment */
+		/* and set the lock state for the ptr blocks on the regions
+		 * for this partition */
+		/* lock / unlock any ptr_blocks in this partition */
+		if (0 != pmem_partition_get(part_hdr->data.desc, &part_hdl)) {
+			PMEM_DPRINT("ERROR: Cant get part hdl for [%s]\n",
+			            part_hdr->data.desc);
+			segments = -1;
+			goto release_done;
+		}
+		if (0 != pmem_lock_ptr_blocks((struct part_handle*)part_hdl,
+		                               lock)) {
+			PMEM_DPRINT("WARNING:Unable to change ptrblock lock\n");
+		}
+	}
+
+release_done:
+	spin_unlock_irqrestore(&pmem.lock, flags);
+	pmem_release_handle(&part_hdl);
+	return segments;
+}
+
+
+/* Get the block index of the active block */
+int pmem_get_active_block_index(struct pmem_part_hdr *part_hdr)
+{
+	struct pmem_block_hdr *block_hdr;
+	int i;
+
+	for (i = 0; i < part_hdr->data.num_blocks; i++) {
+		block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, i);
+		if (block_hdr->data.flags & PMEM_BLOCK_FLAG_ACTIVE)
+			return i;
+	}
+
+	return -1;
+}
+EXPORT_SYMBOL(pmem_get_active_block_index);
+
+
+/* Rotate blocks in a partition */
+int pmem_part_rotate(pmem_handle_t handle, int lock)
+{
+	struct part_handle *hdl = (struct part_handle *) handle;
+	struct part_list_elem *part_elem;
+	int rc;
+
+	if (!handle) {
+		PMEM_DPRINT("NULL arguments\n");
+		return -EINVAL;
+	}
+
+	/* Ensure that it is a partition handle */
+	if (hdl->hdl.type != PMEM_HANDLE_TYPE_PART) {
+		PMEM_DPRINT("Invalid handle type\n");
+		return -EINVAL;
+	}
+
+	part_elem = hdl->elem;
+
+	/* Only rotate if the partition has blocks */
+	if (!part_elem->hdr->data.num_blocks) {
+		PMEM_DPRINT("No blocks to rotate\n");
+		return -EINVAL;
+	}
+
+	/* Rotate the block data in persistent memory */
+	rc = pmem_rotate_block_data(part_elem, lock);
+
+	return (rc);
+}
+EXPORT_SYMBOL(pmem_part_rotate);
+
+
+/* Do all the work necessary to rotate a log partition. 
+ * Returns the block id that just became inactive */
+int pmem_rotate_block_data(struct part_list_elem *part_elem, int lock)
+{
+	struct pmem_part_hdr *part_hdr;
+	struct pmem_block_hdr *active_block_hdr;
+	struct pmem_block_hdr *new_active_block_hdr = NULL;
+	struct pmem_region_hdr *region_hdr;
+	int active_index, new_active_index, index;
+	struct timeval tv;
+	unsigned long flags;
+
+	part_hdr = part_elem->hdr;
+
+	spin_lock_irqsave(&pmem.lock, flags);
+
+	active_index = pmem_get_active_block_index(part_hdr);
+	if (active_index < 0) {
+		spin_unlock_irqrestore(&pmem.lock, flags);
+		printk(KERN_ERR "Unable to get active block");
+		return -EINVAL;
+	}
+
+	active_block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, active_index);
+
+	/* Get the first unlocked block after the active block */
+	new_active_index = active_index + 1;
+	if (new_active_index > (part_hdr->data.num_blocks - 1))
+		new_active_index = 0;
+
+	while (new_active_index != active_index) {
+		new_active_block_hdr =
+		    PMEM_GET_BLOCK_HDR(part_hdr, new_active_index);
+
+		if (!(new_active_block_hdr->data.flags & PMEM_BLOCK_FLAG_LOCK))
+			break;
+
+		new_active_index++;
+		if (new_active_index > (part_hdr->data.num_blocks - 1))
+			new_active_index = 0;			
+	}
+
+	if (!new_active_block_hdr) {
+		spin_unlock_irqrestore(&pmem.lock, flags);
+		PMEM_DPRINT("ERROR: No new_active_block_hdr\n");
+		return -EFAULT;
+	}
+	if (new_active_index >= part_hdr->data.num_blocks) {
+		spin_unlock_irqrestore(&pmem.lock, flags);
+		printk(KERN_ERR "Invalid rotation index=%d\n", new_active_index); 
+		return (-EFAULT);
+	}
+
+	if (new_active_index == active_index) {
+		spin_unlock_irqrestore(&pmem.lock, flags);
+
+		/* Nothing to do. Return failure if the user wanted us to
+		 * lock the block but we were unable to */
+		if (lock)
+			return (-EINVAL);
+		else
+			return (active_index);
+	}
+
+	//PMEM_DPRINT("DEBUG: old activeid=%d, new activeid=%d\n", active_index, new_active_index);
+	/* Update the block activity header information */
+
+	/* Only allow the lock to occur if there are sufficient unlocked blocks */
+	if ((lock)
+	   && (get_num_unlocked_blocks(part_hdr) > PMEM_MIN_UNLOCKED_BLOCKS))
+		active_block_hdr->data.flags |= PMEM_BLOCK_FLAG_LOCK;
+
+	active_block_hdr->data.flags &= ~PMEM_BLOCK_FLAG_ACTIVE;
+	PMEM_UPDATE_CHECKSUM(active_block_hdr);
+	PMEM_FLUSH_CACHE(active_block_hdr, sizeof(struct pmem_block_hdr));
+
+	new_active_block_hdr->data.flags |= PMEM_BLOCK_FLAG_ACTIVE;
+
+	do_gettimeofday(&tv);
+	new_active_block_hdr->data.sec = tv.tv_sec;
+	new_active_block_hdr->data.usec = tv.tv_usec;
+
+	PMEM_UPDATE_CHECKSUM(new_active_block_hdr);
+	PMEM_FLUSH_CACHE(new_active_block_hdr, sizeof(struct pmem_block_hdr));
+
+	/* Prepare the new block */
+	for (index = 0; index < part_hdr->data.num_regions; index++) {
+		region_hdr = PMEM_GET_REGION_HDR(part_hdr, index);
+		pmem_prepare_region(part_hdr, new_active_block_hdr->data.offset,
+				    region_hdr);
+	}
+
+	/*PMEM_DPRINT("DEBUG: old active offset= %d, new active offset= %d\n", 
+			part_elem->active_block_offset, new_active_block_hdr->data.offset);
+			*/
+	part_elem->active_block_offset = new_active_block_hdr->data.offset;
+
+	/* We have done enough so that no one else will rotate the same 
+	 * block so unlock the headers */
+	spin_unlock_irqrestore(&pmem.lock, flags);
+
+	/* Make any updates to the pointer blocks that are necessary */
+	pmem_update_part_ptr_blocks(part_elem);
+
+	return (active_index);
+}
+
+/* Set the reset flag in the control block */
+void pmem_set_cb_reset_flag()
+{
+	unsigned long flags;
+
+	if (!pmem.enabled)
+		return;
+
+	spin_lock_irqsave(&pmem.lock, flags);
+
+	pmem.pmem->data.flags =
+	    htonl(ntohl(pmem.pmem->data.flags) | PMEM_CB_FLAG_RESET);
+	PMEM_UPDATE_CB_CHECKSUM(pmem.pmem);
+	PMEM_FLUSH_CACHE(pmem.pmem, sizeof(struct pmem_cb_hdr));
+
+	spin_unlock_irqrestore(&pmem.lock, flags);
+}
+EXPORT_SYMBOL(pmem_set_cb_reset_flag);
+
+/* Clear the reset flag in the control block */
+void pmem_clear_cb_reset_flag()
+{
+	unsigned long flags;
+
+	if (!pmem.enabled)
+		return;
+
+	spin_lock_irqsave(&pmem.lock, flags);
+
+	pmem.pmem->data.flags = htonl(ntohl(pmem.pmem->data.flags) & ~PMEM_CB_FLAG_RESET);
+	PMEM_UPDATE_CB_CHECKSUM(pmem.pmem);
+	PMEM_FLUSH_CACHE(pmem.pmem, sizeof(struct pmem_cb_hdr));
+
+	spin_unlock_irqrestore(&pmem.lock, flags);
+}
+EXPORT_SYMBOL(pmem_clear_cb_reset_flag);
+
+/* Get some of the dynamic pmem control block information that is not stored
+ * in pmem itself.  */
+int pmem_get_cb_info(pmem_handle_t handle, struct pmem_cb_info *info)
+{
+	struct cb_handle *hdl = (struct cb_handle *) handle;
+
+	if (!handle) {
+		PMEM_DPRINT("NULL arguments\n");
+		return -EINVAL;
+	}
+
+	/* Ensure that it is a cb handle */
+	if (hdl->hdl.type != PMEM_HANDLE_TYPE_CB) {
+		PMEM_DPRINT("Invalid handle type\n");
+		return -EINVAL;
+	}
+
+	info->size = pmem.size;
+	info->num_part = pmem.pmem->data.num_alloc;
+	info->avail_mem = get_avail_alloc_mem();
+	info->num_cpus = pmem.pmem->data.num_cpus;
+
+	return 0;
+}
+EXPORT_SYMBOL(pmem_get_cb_info);
+
+/* Get some partition information. This is used so that API users do not have to
+ * decode the header structures */
+int pmem_get_part_info(pmem_handle_t handle, struct pmem_part_info *info)
+{
+	struct part_handle *hdl = (struct part_handle *) handle;
+	struct part_list_elem *part_elem;
+
+	if (!handle) {
+		PMEM_DPRINT("NULL arguments\n");
+		return -EINVAL;
+	}
+
+	/* Ensure that it is a partition handle */
+	if (hdl->hdl.type != PMEM_HANDLE_TYPE_PART) {
+		PMEM_DPRINT("Invalid handle type\n");
+		return -EINVAL;
+	}
+
+	part_elem = hdl->elem;
+
+	info->size = part_elem->hdr->data.size;
+	info->num_blocks = part_elem->hdr->data.num_blocks;
+	info->num_regions = part_elem->hdr->data.num_regions;
+	info->avail_mem = get_avail_block_mem(part_elem->hdr);
+	info->active_block_id = pmem_get_active_block_index(part_elem->hdr);
+
+	return (0);
+}
+
+/* Get some block information. This is used so that API users do not have to
+ * decode the header structures */
+int pmem_get_block_info(pmem_handle_t handle, struct pmem_block_info *info)
+{
+	struct block_handle *block_hdl = (struct block_handle *) handle;
+	struct part_handle *part_hdl;
+	struct pmem_block_hdr *block_hdr;
+	struct pmem_part_hdr *part_hdr;
+
+	if (!handle) {
+		PMEM_DPRINT("NULL arguments\n");
+		return -EINVAL;
+	}
+
+	/* Ensure that it is a partition handle */
+	if (block_hdl->hdl.type != PMEM_HANDLE_TYPE_BLK) {
+		PMEM_DPRINT("Invalid handle type\n");
+		return -EINVAL;
+	}
+
+	part_hdl = block_hdl->parent;
+	part_hdr = part_hdl->elem->hdr;
+
+	if (block_hdl->block_id == PMEM_ACTIVE_BLOCK)
+		block_hdr = PMEM_GET_BLOCK_HDR(part_hdr,
+				pmem_get_active_block_index(part_hdr));
+	else
+		block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, block_hdl->block_id);
+
+	info->size = block_hdr->data.size;
+	info->active = (block_hdr->data.flags & PMEM_BLOCK_FLAG_ACTIVE) ? 1 : 0;
+	info->locked = (block_hdr->data.flags & PMEM_BLOCK_FLAG_LOCK) ? 1 : 0;
+	info->sec = block_hdr->data.sec;
+	info->usec = block_hdr->data.usec;
+
+	return 0;
+}
+
+/* Get some region information. This is used so that API users do not have to
+ * decode the header structures */
+int pmem_get_region_info(pmem_handle_t handle, struct pmem_region_info *info)
+{
+	struct region_handle *region_hdl = (struct region_handle *) handle;
+	struct pmem_region_hdr *region_hdr;
+
+	if (!handle) {
+		PMEM_DPRINT("NULL arguments\n");
+		return -EINVAL;
+	}
+
+	/* Ensure that it is a partition handle */
+	if (region_hdl->hdl.type != PMEM_HANDLE_TYPE_REG) {
+		PMEM_DPRINT("Invalid handle type\n");
+		return -EINVAL;
+	}
+
+	region_hdr = region_hdl->elem->hdr;
+
+	info->size = region_hdr->data.size;
+	info->num_cpu_areas =
+	    (region_hdr->data.flags & PMEM_REGION_FLAG_PERPROC)
+	    ? PMEM_NUM_SUPPORT_CPUS : 1;
+	info->fixed_size = region_hdr->data.fixed_size;
+	info->num_logs = region_hdr->data.num_log_desc;
+
+	return 0;
+}
+
+/* Lock the block defined by handle */
+int pmem_lock_block(pmem_handle_t handle)
+{
+	struct block_handle *block_hdl = (struct block_handle *) handle;
+	
+	if (!handle) {
+		PMEM_DPRINT("NULL arguments\n");
+		return -EINVAL;
+	}
+
+	/* Ensure that it is a block handle */
+	if (block_hdl->hdl.type != PMEM_HANDLE_TYPE_BLK) {
+		PMEM_DPRINT("Invalid handle type\n");
+		return -EINVAL;
+	}
+
+	return pmem_internal_lock_block(block_hdl);
+}
+EXPORT_SYMBOL(pmem_lock_block);
+
+/* Clear the lock flag for the block */
+int pmem_unlock_block(pmem_handle_t handle)
+{
+	struct block_handle *block_hdl = (struct block_handle *) handle;
+	struct part_handle *part_hdl;
+	struct pmem_part_hdr *part_hdr;
+	struct pmem_block_hdr *block_hdr;
+	int block_index;
+	unsigned long flags;
+
+	if (!handle) {
+		PMEM_DPRINT("NULL arguments\n");
+		return -EINVAL;
+	}
+
+	/* Ensure that it is a block handle */
+	if (block_hdl->hdl.type != PMEM_HANDLE_TYPE_BLK) {
+		PMEM_DPRINT("Invalid handle type\n");
+		return -EINVAL;
+	}
+
+	part_hdl = block_hdl->parent;
+	part_hdr = part_hdl->elem->hdr;
+
+	spin_lock_irqsave(&pmem.lock, flags);
+
+	block_index = block_hdl->block_id;
+	if (block_hdl->block_id == PMEM_ACTIVE_BLOCK)
+		block_index = pmem_get_active_block_index(part_hdr);
+
+	block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, block_index);
+	
+	/* unlock the block */
+	(void)pmem_unlock_block_hdr(block_hdr);
+
+	spin_unlock_irqrestore(&pmem.lock, flags);
+	return (0);
+}
+EXPORT_SYMBOL(pmem_unlock_block);
+
+/* Release the shadow copy memory */
+int pmem_clear_shadow()
+{
+	unsigned long flags;
+	void *ptr;
+
+	if (!pmem.enabled)
+		return -EAGAIN;
+
+	spin_lock_irqsave(&pmem.lock, flags);
+
+	if (!pmem.shadow) {
+		spin_unlock_irqrestore(&pmem.lock, flags);
+		PMEM_DPRINT("shadow is null\n");
+		return -EINVAL;
+	}
+
+	ptr = pmem.shadow;
+	pmem.shadow = NULL;
+
+	spin_unlock_irqrestore(&pmem.lock, flags);
+
+	vfree(ptr);
+
+	return 0;
+}
+EXPORT_SYMBOL(pmem_clear_shadow);
+
+/* Disable all pointer blocks in a partition .
+ * This function really destroys all the pointer blocks for the partition -
+ * they cant be recovered */
+int pmem_disable_ptr_blocks(pmem_handle_t part_handle)
+{
+	struct part_handle *hdl = (struct part_handle *) part_handle;
+	struct part_list_elem *part_elem;
+	struct region_list_elem *region_elem;
+	struct list_head *elem, *temp;
+	int i;
+
+	if (!hdl) {
+		PMEM_DPRINT("NULL arguments\n");
+		return -EINVAL;
+	}
+
+	/* Ensure that it is a partition handle */
+	if (hdl->hdl.type != PMEM_HANDLE_TYPE_PART) {
+		PMEM_DPRINT("Invalid handle type\n");
+		return -EINVAL;
+	}
+
+	part_elem = hdl->elem;
+
+	/* Loop over all of the regions */
+	list_for_each_safe(elem, temp, &(part_elem->region_list)) {
+		region_elem = list_entry(elem, struct region_list_elem, list_elem);
+		if (!region_elem->ptr_block)
+			continue;
+
+		/* Loop over each cpus pointers */
+		for (i = 0; i < PMEM_NUM_SUPPORT_CPUS; i++)
+			region_elem->ptr_block->cpu[i].ptrs = NULL;
+	}
+
+	return 0;
+}
+
+/* Destroy the registered pointer block for the given region */
+int pmem_destroy_ptr_block(pmem_handle_t reg_handle)
+{
+	struct region_handle *hdl = (struct region_handle *)reg_handle;
+	struct region_list_elem *region_elem;
+	unsigned long flags;
+	unsigned int i;
+
+	if (!hdl) {
+		PMEM_DPRINT("NULL region handle\n");
+		return -EINVAL;
+	}
+
+	if (PMEM_HANDLE_TYPE_REG != hdl->hdl.type) {
+		PMEM_DPRINT("Invalid handle type %d\n", hdl->hdl.type);
+		return -EINVAL;
+	}
+
+	region_elem = hdl->elem;
+
+	if (region_elem->ptr_block) {
+		spin_lock_irqsave(&pmem.lock, flags);
+		
+		/* disable each cpu block */
+		for (i=0; i<PMEM_NUM_SUPPORT_CPUS; i++) {
+			region_elem->ptr_block->cpu[i].ptrs = NULL;
+		}
+
+		/* and destroy the ptr block handle stored in the region */
+		region_elem->ptr_block = NULL;
+		spin_unlock_irqrestore(&pmem.lock, flags);
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(pmem_destroy_ptr_block);
+
+/* Lock an entire partition.  This routine overrides the minimum number of 
+ * unlocked blocks in a partition check, and does not rotate the active 
+ * block.  
+ * Returns the number of blocks locked on success, negative errno on error */
+int pmem_lock_part(pmem_handle_t part_handle)
+{
+	return pmem_alter_part_lock(part_handle, PMEM_LOCK);
+}
+EXPORT_SYMBOL(pmem_lock_part);
+
+/* Unlock a partition 
+ * Returns the number of blocks unlocked on success, negative errno on error */
+int pmem_unlock_part(pmem_handle_t part_handle)
+{
+	return pmem_alter_part_lock(part_handle, PMEM_UNLOCK);
+}
+EXPORT_SYMBOL(pmem_unlock_part);
+
+
+/* Lock all of pmem 
+ * Returns the number of segments that were locked */
+int pmem_lock(void)
+{
+	return pmem_alter_global_lock(PMEM_LOCK);
+}
+EXPORT_SYMBOL(pmem_lock);
+
+
+/* Unlock all of pmem 
+ * Returns the number of segments that were unlocked */
+int pmem_unlock(void)
+{
+	return pmem_alter_global_lock(PMEM_UNLOCK);
+}
+EXPORT_SYMBOL(pmem_unlock);
+
+
+/* Lock the active segments in all paritions - lock all active segments 
+ * Returns the number of active segments that were locked.  */
+int pmem_lock_all_active_segments(void)
+{
+	return pmem_alter_active_segment_lock(PMEM_LOCK);
+}
+EXPORT_SYMBOL(pmem_lock_all_active_segments);
+
+
+/* Unlock the active segments in all paritions 
+ * Returns the number of segments that were unlocked */
+int pmem_unlock_all_active_segments(void)
+{
+	return pmem_alter_active_segment_lock(PMEM_UNLOCK);
+}
+EXPORT_SYMBOL(pmem_unlock_all_active_segments);
+
+
+/* Rotate the active segment on all partitions in pmem.
+ * lock specifies whether to lock the rotated blocks or not
+ * Returns the number of active segments that were rotated */
+int pmem_rotate(int lock)
+{
+	struct part_list_elem *part_elem;
+	struct list_head *elem, *temp;
+	int rotated = 0;
+
+	/* for each partition */
+	list_for_each_safe(elem, temp, &pmem.part_list) {
+		part_elem = list_entry(elem, struct part_list_elem, list_elem);
+		/* Only rotate if the partition has blocks */
+		if (part_elem->hdr->data.num_blocks > 0) {
+			/* Rotate the block data in persistent memory */
+			if (pmem_rotate_block_data(part_elem, lock) > 0) {
+				rotated += 1;
+			}
+		}
+	}
+
+	return rotated;
+}
+EXPORT_SYMBOL(pmem_rotate);
+
+/* Clear a region back to its original state. Doesnt delete data, but sets
+ * the next active record to be at the start of the buffer 
+ *
+ * Intended for use by the vMC module
+ */
+int pmem_clear_region(pmem_handle_t region_handle)
+{
+	struct pmem_handle	*handle = (struct pmem_handle*)region_handle;
+	struct region_handle	*reg_hdl;
+	struct pmem_part_hdr	*part_hdr;
+	struct pmem_region_hdr	*reg_hdr;
+	struct pmem_block_hdr	*block_hdr;
+	int 			rc = -EINVAL;
+
+	if (PMEM_HANDLE_TYPE_REG != handle->type) {
+		PMEM_DPRINT("ERROR: Cant clear_region on type %d\n",
+		            handle->type);
+		return rc;
+	}
+
+	reg_hdl = (struct region_handle*)handle;
+	part_hdr = reg_hdl->parent->elem->hdr;
+	reg_hdr = reg_hdl->elem->hdr;
+	block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, reg_hdl->block_id);
+	
+	/* Reset the region data area in the block */
+	pmem_prepare_region(part_hdr, block_hdr->data.offset, reg_hdr);
+	
+	return 0; /* success */
+}
+EXPORT_SYMBOL(pmem_clear_region);
+
+
+
+/*
+ * Routine used by pmemfs
+ * Architectures vary in how they handle caching for addresses
+ * outside of main memory.
+ * Adapted from drivers/char/mem.c
+ */
+#ifdef pgprot_noncached
+int pmem_uncached_access(unsigned long addr)
+{
+#if defined(__i386__)
+ 	return !( test_bit(X86_FEATURE_MTRR, boot_cpu_data.x86_capability) ||
+		  test_bit(X86_FEATURE_K6_MTRR, boot_cpu_data.x86_capability) ||
+		  test_bit(X86_FEATURE_CYRIX_ARR, boot_cpu_data.x86_capability) ||
+		  test_bit(X86_FEATURE_CENTAUR_MCR, boot_cpu_data.x86_capability) )
+	  && addr >= __pa(high_memory);
+#elif defined(__x86_64__)
+	return 0;
+#elif defined(CONFIG_IA64)
+	return !(efi_mem_attributes(addr) & EFI_MEMORY_WB);
+#elif defined(CONFIG_PPC)
+	return !page_is_ram(addr >> PAGE_SHIFT);
+#else
+	return addr >= __pa(high_memory);
+#endif
+}
+
+EXPORT_SYMBOL(pmem_uncached_access);
+
+#endif /* pgprot_noncached */
+
diff --git a/mm/pmem/handle.c b/mm/pmem/handle.c
new file mode 100644
index 0000000..cce985e
--- /dev/null
+++ b/mm/pmem/handle.c
@@ -0,0 +1,473 @@
+#include <linux/string.h>
+#include <linux/errno.h>
+#include <linux/spinlock.h>
+#include <linux/kernel.h>
+#include <linux/slab.h>
+#include <linux/module.h>
+#include <linux/pmem.h>
+
+#include <asm/types.h>
+
+
+/* Return a pointer to the data for the element */
+void* pmem_get_data_ptr(struct pmem_handle *hdl)
+{
+	switch (hdl->type) {
+	case PMEM_HANDLE_TYPE_CB:
+		return (pmem.pmem);
+		break;
+
+	case PMEM_HANDLE_TYPE_PART:
+		{
+			struct part_handle *part_hdl =
+			    (struct part_handle *) hdl;
+			return ((void *) part_hdl->elem->hdr +
+				PMEM_PART_HDR_MAX_SIZE);
+			break;
+		}
+
+	case PMEM_HANDLE_TYPE_BLK:
+		{
+			struct block_handle *block_hdl =
+			    (struct block_handle *) hdl;
+			struct part_handle *part_hdl = block_hdl->parent;
+			struct pmem_block_hdr *block_hdr;
+
+			if (block_hdl->block_id == PMEM_ACTIVE_BLOCK) {
+				return ((void *) part_hdl->elem->hdr +
+					part_hdl->elem->active_block_offset);
+			} else {
+				block_hdr =
+				    PMEM_GET_BLOCK_HDR(part_hdl->elem->hdr,
+						       block_hdl->block_id);
+				return ((void *) part_hdl->elem->hdr +
+					block_hdr->data.offset);
+			}
+			break;
+		}
+
+	case PMEM_HANDLE_TYPE_REG:
+		{
+			struct region_handle *region_hdl =
+			    (struct region_handle *) hdl;
+			struct part_handle *part_hdl = region_hdl->parent;
+			struct pmem_block_hdr *block_hdr;
+
+			if (region_hdl->block_id == PMEM_ACTIVE_BLOCK) {
+				return ((void *) part_hdl->elem->hdr +
+					part_hdl->elem->active_block_offset +
+					region_hdl->elem->hdr->data.offset);
+			} else {
+				block_hdr =
+				    PMEM_GET_BLOCK_HDR(part_hdl->elem->hdr,
+						       region_hdl->block_id);
+				return ((void *) part_hdl->elem->hdr +
+					block_hdr->data.offset +
+					region_hdl->elem->hdr->data.offset);
+			}
+			break;
+		}
+
+	case PMEM_HANDLE_TYPE_SHADOW:
+		return (pmem.shadow);
+		break;
+	}
+
+	return (NULL);
+}
+EXPORT_SYMBOL(pmem_get_data_ptr);
+
+/* Get a pointer to the start of the header block for an element. */
+void* pmem_get_hdr_ptr(struct pmem_handle *hdl)
+{
+	struct block_handle	*block_hdl;
+	struct part_handle	*part_hdl;
+	struct region_handle	*region_hdl;
+	struct pmem_block_hdr	*block_hdr;
+	int			block_id;
+	void			*rp = NULL;
+
+	switch (hdl->type) {
+		case PMEM_HANDLE_TYPE_PART:
+			part_hdl = (struct part_handle*)hdl;
+			rp = (void*)part_hdl->elem->hdr;
+			break;
+		case PMEM_HANDLE_TYPE_BLK:
+			block_hdl = (struct block_handle*)hdl;
+			part_hdl = block_hdl->parent;
+			block_id = block_hdl->block_id;
+
+			if (PMEM_ACTIVE_BLOCK == block_id) {
+				block_id = pmem_get_active_block_index(
+				               part_hdl->elem->hdr);
+			}
+			block_hdr = PMEM_GET_BLOCK_HDR(part_hdl->elem->hdr,
+			                               block_id);
+			rp = (void*)block_hdr;
+			break;
+		case PMEM_HANDLE_TYPE_REG:
+			region_hdl = (struct region_handle*)hdl;
+			rp = (void*)region_hdl->elem->hdr;
+			break;
+		case PMEM_HANDLE_TYPE_CB:
+		case PMEM_HANDLE_TYPE_SHADOW:
+		default:
+			PMEM_DPRINT("ERROR: Cant get hdr ptr for type %d\n",
+			            hdl->type);
+			break;
+	}
+
+	return rp;
+}
+EXPORT_SYMBOL(pmem_get_hdr_ptr);
+
+/* Get the size of a handle's header */
+ssize_t pmem_get_hdr_size(pmem_handle_t handle)
+{
+	struct pmem_handle 	*hdl = (struct pmem_handle*)handle;
+	ssize_t 		rc = -EINVAL;
+	
+	switch (hdl->type) {
+		case PMEM_HANDLE_TYPE_PART:
+			/* how does PMEM_PART_HDR_MAX_SIZE fit into these
+			 * size discussions ? */
+			rc = sizeof(struct pmem_part_hdr);
+			break;
+		case PMEM_HANDLE_TYPE_BLK:
+			rc = sizeof(struct pmem_block_hdr);
+			break;
+		case PMEM_HANDLE_TYPE_REG:
+			rc = sizeof(struct pmem_region_hdr);
+			break;
+		case PMEM_HANDLE_TYPE_CB:
+			rc = sizeof(struct pmem_cb_hdr);
+			break;
+		case PMEM_HANDLE_TYPE_SHADOW:   
+			/* not sure if shadow header has a size ? */
+			PMEM_DPRINT("WARNING: shadow header size unknown\n");
+			rc = 0;
+			break;
+		default:
+			PMEM_DPRINT("ERROR: Invalid type %d\n", hdl->type);
+			break;
+	}
+
+	return rc;
+}
+EXPORT_SYMBOL(pmem_get_hdr_size);
+
+/* Get the partition element with this string description */
+static struct part_list_elem * get_part_elem(char *desc)
+{
+	struct part_list_elem *part_elem;
+	struct list_head *elem, *temp;
+
+	list_for_each_safe(elem, temp, &pmem.part_list) {
+		part_elem = list_entry(elem, struct part_list_elem, list_elem);
+		if (!strncmp(part_elem->hdr->data.desc, desc, PMEM_DESC_MAX))
+			return (part_elem);
+	}
+
+	return NULL;
+}
+
+/* Get the region element with this string description */
+static struct region_list_elem *
+get_region_elem(struct part_list_elem *part_elem, char *desc)
+{
+	struct region_list_elem *region_elem;
+	struct list_head *elem, *temp;
+
+	list_for_each_safe(elem, temp, &(part_elem->region_list)) {
+		region_elem =
+		    list_entry(elem, struct region_list_elem, list_elem);
+		if (!strncmp(region_elem->hdr->data.desc, desc, PMEM_DESC_MAX))
+			return (region_elem);
+	}
+
+	return NULL;
+}
+
+/* Get a control block handle
+ *    If the pmem_handle_t pointer that is passed does not point to NULL then 
+ *    the memory it points to will be used.  Otherwise kmalloc will be called 
+ *    to allocate new memory.
+ */
+int pmem_control_block_get(pmem_handle_t *handle)
+{
+	struct cb_handle *hdl = NULL;
+
+	if (!pmem.enabled)
+		return -EINVAL;
+
+	if (!handle)
+		return -EINVAL;
+
+#ifdef CONFIG_PMEM_DEBUG
+	if (PMEM_VALIDATE_CB_CHECKSUM(pmem.pmem) < 0)
+		printk(KERN_ERR "CB checksum invalid\n");
+#endif
+
+	if (!*handle) {
+		hdl = kmalloc(sizeof (struct cb_handle), GFP_KERNEL);
+
+		if (!hdl)
+			return -ENOMEM;
+
+		*handle = hdl;
+	} else {
+		/* PMEM_DPRINT("re-using *handle %p\n", *handle); */
+		hdl = (struct cb_handle *) *handle;
+	}
+
+	hdl->hdl.type = PMEM_HANDLE_TYPE_CB;
+	hdl->hdl.offset = 0;
+	hdl->hdl.size = pmem.size;
+
+	return 0;
+}
+EXPORT_SYMBOL(pmem_control_block_get);
+
+/* Get a partition handle
+ *    If the pmem_handle_t pointer that is passed does not point to NULL then 
+ *    the memory it points to will be used.  Otherwise kmalloc will be called 
+ *    to allocate new memory.
+ */
+int pmem_partition_get(char *desc, pmem_handle_t *handle)
+{
+	struct part_handle *hdl = NULL;
+	struct part_list_elem *elem = NULL;
+
+	if (!pmem.enabled)
+		return (-EINVAL);
+
+	elem = get_part_elem(desc);
+	if (!elem)
+		return (-EINVAL);
+
+	if (!handle)
+		return -EINVAL;
+
+#ifdef CONFIG_PMEM_DEBUG
+	if (PMEM_VALIDATE_CHECKSUM(elem->hdr) < 0)
+		printk(KERN_ERR "Partition checksum invalid\n");
+#endif
+
+	if (!*handle) {
+		hdl = kmalloc(sizeof (struct part_handle), GFP_KERNEL);
+
+		if (!hdl)
+			return -ENOMEM;
+
+		*handle = hdl;
+	} else {
+		/* PMEM_DPRINT("re-using *handle %p\n", *handle); */
+		hdl = (struct part_handle *) *handle;
+	}
+
+	hdl->hdl.type = PMEM_HANDLE_TYPE_PART;
+	hdl->hdl.offset = 0;
+	hdl->hdl.size = elem->hdr->data.size;
+	hdl->elem = elem;
+
+	return 0;
+}
+EXPORT_SYMBOL(pmem_partition_get);
+
+/* Get a block handle 
+ *    If the pmem_handle_t pointer that is passed does not point to NULL then 
+ *    the memory it points to will be used.  Otherwise kmalloc will be called 
+ *    to allocate new memory.
+ */
+int pmem_block_get(pmem_handle_t part_handle, __s8 block_id, pmem_handle_t *handle)
+{
+	struct part_handle *part_hdl = (struct part_handle *) part_handle;
+	struct pmem_block_hdr *block_hdr = NULL;
+	struct block_handle *hdl = NULL;
+
+	if (!pmem.enabled)
+		return -EINVAL;
+
+	if (!handle)
+		return -EINVAL;
+
+	if (!part_hdl || (part_hdl->hdl.type != PMEM_HANDLE_TYPE_PART))
+		return -EINVAL;
+
+	if (part_hdl->elem->hdr->data.num_blocks == 0)
+		return -EINVAL;
+
+	if (((block_id >= part_hdl->elem->hdr->data.num_blocks) ||
+	     (block_id < 0)) && (block_id != PMEM_ACTIVE_BLOCK))
+		return -EINVAL;
+
+	/* grab the block details from the first block */
+	block_hdr = PMEM_GET_BLOCK_HDR(part_hdl->elem->hdr, 0);
+
+#ifdef CONFIG_PMEM_DEBUG
+	if (PMEM_VALIDATE_CHECKSUM(block_hdr) < 0)
+		printk(KERN_ERR "Block checksum invalid\n");
+#endif
+
+	if (!*handle) {
+		hdl = kmalloc(sizeof (struct block_handle), GFP_KERNEL);
+
+		if (!hdl) 
+			return -ENOMEM;
+
+		*handle = hdl;
+	} else {
+		/* PMEM_DPRINT("re-using *handle %p\n", *handle); */
+		hdl = (struct block_handle *) *handle;
+	}
+
+	hdl->hdl.type = PMEM_HANDLE_TYPE_BLK;
+	hdl->hdl.offset = 0;
+	hdl->hdl.size = block_hdr->data.size;
+	hdl->parent = part_handle;
+	hdl->block_id = block_id;
+
+	return 0;
+}
+EXPORT_SYMBOL(pmem_block_get);
+
+/* Get a region handle
+ *    If the pmem_handle_t pointer that is passed does not point to NULL then 
+ *    the memory it points to will be used.  Otherwise kmalloc will be called 
+ *    to allocate new memory.
+ */
+int pmem_region_get(pmem_handle_t part_handle, char *desc, __s8 block_id,
+		pmem_handle_t *handle)
+{
+	struct part_handle *part_hdl = (struct part_handle *) part_handle;
+	struct region_handle *hdl = NULL;
+	struct region_list_elem *elem;
+
+	if (!pmem.enabled)
+		return -EINVAL;
+
+	if (!handle)
+		return -EINVAL;
+
+	if (!part_hdl || (part_hdl->hdl.type != PMEM_HANDLE_TYPE_PART))
+		return -EINVAL;
+
+	if (part_hdl->elem->hdr->data.num_blocks == 0)
+		return -EINVAL;
+
+	if ((block_id >= part_hdl->elem->hdr->data.num_blocks) ||
+	    (block_id < PMEM_ACTIVE_BLOCK))
+		return -EINVAL;
+
+	elem = get_region_elem(part_hdl->elem, desc);
+	if (!elem)
+		return -EINVAL;
+
+#ifdef CONFIG_PMEM_DEBUG
+	if (PMEM_VALIDATE_CHECKSUM(elem->hdr) < 0)
+		printk(KERN_ERR "Region checksum invalid\n");
+#endif
+
+	if (!*handle) {
+		hdl = kmalloc(sizeof (struct region_handle), GFP_KERNEL);
+
+		if (!hdl) 
+			return -ENOMEM;
+
+		*handle = hdl;
+	} else {
+		/* PMEM_DPRINT("re-using *handle %p\n", *handle); */
+		hdl = (struct region_handle *) *handle;
+	}
+
+	hdl->hdl.type = PMEM_HANDLE_TYPE_REG;
+	hdl->hdl.offset = 0;
+	hdl->hdl.size = elem->hdr->data.size;
+	hdl->parent = part_handle;
+	hdl->elem = elem;
+	hdl->block_id = block_id;
+
+	return 0;
+}
+EXPORT_SYMBOL(pmem_region_get);
+
+/* Get a shadow handle */
+int pmem_shadow_get(pmem_handle_t *handle)
+{
+	struct shadow_handle *hdl = NULL;
+
+	if (!pmem.enabled)
+		return (-EINVAL);
+
+	if (!pmem.shadow)
+		return (-EINVAL);
+
+	if (!handle)
+		return -EINVAL;
+
+	if (!*handle) {
+		hdl = kmalloc(sizeof (struct shadow_handle), GFP_KERNEL);
+
+		if (!hdl)
+			return -ENOMEM;
+
+		*handle = hdl;
+	} else {
+		/* PMEM_DPRINT("re-using *handle %p\n", *handle); */
+		hdl = (struct shadow_handle *) *handle;
+	}
+
+	hdl->hdl.type = PMEM_HANDLE_TYPE_SHADOW;
+	hdl->hdl.offset = 0;
+	hdl->hdl.size = pmem.size;
+
+	return 0;
+}
+EXPORT_SYMBOL(pmem_shadow_get);
+
+/* Free a handle
+ *   For future instrumentation */
+void inline pmem_free_handle(pmem_handle_t handle)
+{
+	kfree(handle);
+}
+
+/* Release a handle
+ *   For future instrumentation */
+void inline pmem_release_handle(pmem_handle_t *handle)
+{
+	pmem_free_handle(*handle);
+	*handle = NULL;
+	
+}
+EXPORT_SYMBOL(pmem_release_handle);
+
+/* Get the parent handle */
+pmem_handle_t pmem_get_handle_parent(pmem_handle_t handle)
+{
+	struct pmem_handle *hdl = (struct pmem_handle *) handle;
+
+	switch (hdl->type) {
+	case PMEM_HANDLE_TYPE_CB:
+	case PMEM_HANDLE_TYPE_PART:
+	case PMEM_HANDLE_TYPE_SHADOW:
+		return (NULL);
+		break;
+	case PMEM_HANDLE_TYPE_BLK:
+		{
+			struct block_handle *block =
+			    (struct block_handle *) handle;
+			return (block->parent);
+			break;
+		}
+	case PMEM_HANDLE_TYPE_REG:
+		{
+			struct region_handle *region =
+			    (struct region_handle *) handle;
+			return (region->parent);
+			break;
+		}
+	}
+
+	return (NULL);
+}
diff --git a/mm/pmem/io.c b/mm/pmem/io.c
new file mode 100644
index 0000000..8d63a56
--- /dev/null
+++ b/mm/pmem/io.c
@@ -0,0 +1,605 @@
+#include <linux/string.h>
+#include <linux/errno.h>
+#include <linux/kernel.h>
+#include <linux/spinlock.h>
+#include <linux/slab.h>
+#include <linux/mm.h>
+#include <linux/module.h>
+#include <linux/pmem.h>
+
+#include <asm/types.h>
+#include <asm/uaccess.h>	/* copy_to_user */
+
+
+
+/* static int is_locked {{{
+ *
+ * Check to see if a given handle is currently locked
+ * Returns 0 if not locked, non-zero if locked
+ */
+static int is_locked(pmem_handle_t handle)
+{
+	int			rc = 0;
+	struct pmem_handle	*hdl = (struct pmem_handle *) handle;
+	struct block_handle	*block_hdl;
+	struct region_handle	*reg_hdl;
+	struct pmem_part_hdr	*part_hdr;
+	struct pmem_block_hdr	*block_hdr;
+	int			block_id;
+
+	/* only care about segment handles and region handles
+	 * others are not affected by locks */
+	switch (hdl->type) {
+		case PMEM_HANDLE_TYPE_SHADOW:
+		case PMEM_HANDLE_TYPE_PART:
+		case PMEM_HANDLE_TYPE_CB:
+			/* cant be locked */
+			return rc;
+			break;
+		case PMEM_HANDLE_TYPE_BLK:
+			block_hdl = handle;
+			part_hdr = block_hdl->parent->elem->hdr;
+			block_id = block_hdl->block_id;
+			break;
+		case PMEM_HANDLE_TYPE_REG:
+			reg_hdl = handle;
+			part_hdr = reg_hdl->parent->elem->hdr; 
+			block_id = reg_hdl->block_id;
+			break;
+		default:
+			printk(KERN_ERR "ERROR: Unknown pmem handle type %d\n",
+			       hdl->type);
+			return rc;
+			break;
+	}
+
+	/* common code for checking locks on region/segment here */
+	if (PMEM_ACTIVE_BLOCK == block_id)
+		block_id = pmem_get_active_block_index(part_hdr);
+	block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, block_id);
+
+	if (PMEM_BLOCK_FLAG_LOCK & block_hdr->data.flags) {
+		/* locked */
+		rc = 1;
+	}
+	return rc;
+} 
+
+/* int pmem_read_data {{{
+ * Read data from persistent memory data segment */
+int pmem_read_data(pmem_handle_t handle, char *buffer, int size)
+{
+	struct pmem_handle *hdl = (struct pmem_handle *) handle;
+	int bytes_read = -EINVAL;
+	void *data_start;
+	spinlock_t *lock = NULL;
+	unsigned long flags = 0;
+
+#ifdef CONFIG_PMEM_DEBUG
+	if (!handle || !buffer || (size < 0)) {
+		PMEM_DPRINT("read: invalid arguments\n");
+		goto done;
+	}
+#endif
+
+	if (hdl->offset > hdl->size) {
+		PMEM_DPRINT("read: invalid offset %d with size %d\n",
+		              hdl->offset, hdl->size);
+		goto done;
+	}
+
+	data_start = pmem_get_data_ptr(hdl);
+	if (!data_start) {
+		PMEM_DPRINT("read: invalid handle\n");
+		goto done;
+	}
+
+	if (hdl->offset + size > hdl->size) {
+		bytes_read = hdl->size - hdl->offset;
+	} else {
+		bytes_read = size;
+	}
+
+	if (hdl->type == PMEM_HANDLE_TYPE_REG) {
+		/* Only acquire spinlock on reads of single CPU data */
+		struct region_handle *region_hdl =
+		    (struct region_handle *) handle;
+		if (!
+		    (region_hdl->elem->hdr->data.
+		     flags & PMEM_REGION_FLAG_PERPROC)) {
+			lock = &region_hdl->elem->data_lock;
+			spin_lock_irqsave(lock, flags);
+		}
+	}
+
+	//PMEM_DPRINT("INFO: data_read, start=%p, off=%d, size=%d\n", data_start, hdl->offset, bytes_read);
+	/* FIXME: Read from percpu region doesnt read the current cpu data */
+
+	__pmem_memcpy_fromio(buffer, data_start + hdl->offset, bytes_read);
+
+	if (lock) {
+		spin_unlock_irqrestore(lock, flags);
+	}
+	hdl->offset += bytes_read;
+
+      done:
+	return (bytes_read);
+} 
+EXPORT_SYMBOL(pmem_read_data);
+
+/* int pmem_read_header 
+ * Read data from the header for the given handle 
+ * Returns the number of bytes read from the header on success
+ * and -errno on error.*/
+int pmem_read_header(pmem_handle_t handle, char *buffer, int size, 
+               loff_t offset)
+{
+	struct pmem_handle	*hdl = (struct pmem_handle*)handle;
+	void			*header_start;
+	int			header_size;
+	ssize_t			bytes_read = -EINVAL;
+
+	header_size = pmem_get_hdr_size(handle);
+	if (header_size <= 0) {
+		PMEM_DPRINT("ERROR: Can't read a unsized header\n");
+		return bytes_read;
+	}
+	if (size > header_size) {
+		size = header_size;
+	}
+
+	header_start = pmem_get_hdr_ptr(hdl);
+	if (!header_start) {
+		PMEM_DPRINT("ERROR: Can't read from a NULL header\n");
+		return bytes_read;
+	}
+	
+	if (size + offset > header_size) {
+		bytes_read = header_size - offset;
+	} else {
+		bytes_read = size;
+	}
+
+	__pmem_memcpy_fromio(buffer, header_start + offset, bytes_read);
+	return bytes_read;
+} 
+EXPORT_SYMBOL(pmem_read_header);
+
+/* static int pmem_write_region_data 
+ * Write to a region without log descriptors. */
+static int pmem_write_region_data(struct region_handle *hdl, void *region_data,
+                 const char *buffer, int size)
+{
+	struct pmem_region_data_hdr *data_hdr;
+	struct pmem_region_hdr *region_hdr = hdl->elem->hdr;
+	int log_size, bytes_written;
+	char *log_ptr;
+	spinlock_t *lock;
+	unsigned long flags = 0;
+	int cpu;
+
+	if (region_hdr->data.flags & PMEM_REGION_FLAG_PERPROC) {
+		/* get current processor and disable kernel pre-emption */
+		cpu = get_cpu();
+		data_hdr = (struct pmem_region_data_hdr *) (region_data +
+						     PMEM_DESC_MAX) + cpu;
+		/* re-enable kernel preemption */
+		put_cpu();
+		lock = NULL;
+	} else {
+		data_hdr =
+		    (struct pmem_region_data_hdr *) (region_data +
+						     PMEM_DESC_MAX);
+		lock = &hdl->elem->data_lock;
+	}
+
+	log_size = region_hdr->data.fixed_size;
+
+	/* Check if the log needs to be truncated */
+	if (size > region_hdr->data.size) {
+		/* do we need to do more strict size checking here ? */
+		size = region_hdr->data.size;
+	}
+	if ((size > log_size) && (log_size != 0)) {
+		bytes_written = log_size;
+	} else {
+		bytes_written = size;
+		if (log_size == 0)
+			log_size = size;
+	}
+
+	if (lock)
+		spin_lock_irqsave(lock, flags);
+
+	/* Get the new offset */
+	if (data_hdr->current_offset + log_size > data_hdr->end_offset) {
+		/* Wrap around. Check if we should stop here */
+		if (region_hdr->data.flags & PMEM_REGION_FLAG_STOPFULL) {
+			if (lock)
+				spin_unlock_irqrestore(lock, flags);
+
+			return 0;
+		} else
+			data_hdr->current_offset =
+			    data_hdr->start_offset + log_size;
+	} else {
+		data_hdr->current_offset += log_size;
+	}
+	PMEM_FLUSH_CACHE(data_hdr, sizeof(struct pmem_region_data_hdr));
+
+	log_ptr = (char *) region_data + data_hdr->current_offset - log_size;
+
+	/* Done with the headers so release the lock */
+	if (lock)
+		spin_unlock_irqrestore(lock, flags);
+
+	/* Write the log */
+	/*
+	PMEM_DPRINT("INFO: write() reg_start=%p, cur_off=%p, end=%p,\
+		       	end_write=%p\n",
+		       	(void*)(data_hdr->start_offset+region_data), log_ptr,
+		       	(void*)(data_hdr->end_offset+region_data),
+		       	(void*)(log_ptr+bytes_written)); */
+	__pmem_memcpy_toio(log_ptr, buffer, bytes_written);
+	PMEM_FLUSH_CACHE(log_ptr, bytes_written);
+
+	return (bytes_written);
+}
+
+/* static void print_desc  */
+#ifdef EXTRA_PMEM_DEBUGGING
+/* Nobody seems to call this routine - leave it here in case it is needed
+ * for debugging purposes later. */
+#ifdef CONFIG_PMEM_DEBUG
+static void print_desc(struct pmem_log_desc *log_desc)
+{
+	int valid = 1;
+
+	if (PMEM_VALIDATE_CHECKSUM(log_desc) < 0)
+		valid = 0;
+
+	printk(KERN_ERR
+	       "Valid=%d, time=%LX, sec=%d, usec=%d, size=%d, offset=%d, checksum=%d\n",
+	       valid, log_desc->data.hrtime, log_desc->data.sec,
+	       log_desc->data.usec, log_desc->data.size, log_desc->data.offset,
+	       log_desc->data.log_checksum);
+	return;		/* do nothing */
+} 
+#else
+#define print_desc(log_desc) do { } while(0)
+#endif
+#endif
+
+
+/* static int pmem_write_log_desc 
+ * Write to a region with log descriptors */
+static int pmem_write_log_desc(struct region_handle *hdl, void *region_data,
+		const char *buffer, int size)
+{
+	struct pmem_log_desc_index *desc_index;
+	struct pmem_log_desc *log_desc;
+	struct pmem_region_hdr *region_hdr = hdl->elem->hdr;
+	struct timeval tv;
+	int bytes_written = 0;
+	int reuse_desc = 0;
+	int desc_arr_size, log_offset, i;
+	unsigned long flags;
+
+	desc_arr_size = sizeof (struct pmem_log_desc_index) +
+	    region_hdr->data.num_log_desc * sizeof (struct pmem_log_desc);
+	desc_index = region_data;
+
+	/* Enforce a max log size */
+	if (size > PAGE_SIZE)
+		size = PAGE_SIZE;
+
+	if (size > (region_hdr->data.size - desc_arr_size))
+		bytes_written = region_hdr->data.size - desc_arr_size;
+	else
+		bytes_written = size;
+
+	spin_lock_irqsave(&hdl->elem->data_lock, flags);
+
+#ifdef CONFIG_PMEM_DEBUG
+	if (PMEM_VALIDATE_CHECKSUM(desc_index) < 0) {
+		PMEM_DPRINT
+		    ("region data checksum failed. Reinitializing index\n");
+		PMEM_DPRINT("Data: last=%d, curr=%d, checksum=0x%x\n",
+			    desc_index->data.oldest_index,
+			    desc_index->data.curr_index, desc_index->checksum);
+
+		desc_index->data.oldest_index = 0;
+		desc_index->data.curr_index = 0;
+		PMEM_UPDATE_CHECKSUM(desc_index);
+	}
+#endif
+
+	/* Get the current descriptor. This points to the last written log */
+	log_desc = region_data + sizeof (struct pmem_log_desc_index) +
+	    desc_index->data.curr_index * sizeof (struct pmem_log_desc);
+
+#ifdef CONFIG_PMEM_DEBUG
+	if (PMEM_VALIDATE_CHECKSUM(log_desc) < 0) {
+		PMEM_DPRINT
+		    ("region data checksum failed. Reinitializing descriptor\n");
+
+		log_desc->data.size = 0;
+		log_desc->data.offset = 0;
+		PMEM_UPDATE_CHECKSUM(log_desc);
+	}
+#endif
+
+	/* Get the new offset */
+	if ((log_desc->data.offset + log_desc->data.size + bytes_written >=
+	     region_hdr->data.size) ||
+	    (log_desc->data.offset < desc_arr_size)) {
+		log_offset = desc_arr_size;
+	} else {
+		log_offset = log_desc->data.offset + log_desc->data.size;
+	}
+
+	/* Get the new descriptor */
+	if (desc_index->data.curr_index >= region_hdr->data.num_log_desc - 1)
+		desc_index->data.curr_index = 0;
+	else
+		desc_index->data.curr_index++;
+
+	if (desc_index->data.curr_index == desc_index->data.oldest_index) 
+		reuse_desc = 1;
+	
+	/* Update any overwritten descriptors */
+	i = desc_index->data.oldest_index;
+	do {
+		log_desc = region_data + sizeof (struct pmem_log_desc_index) +
+		    i * sizeof (struct pmem_log_desc);
+
+		/* Update i to point to the next log incase we need it */
+		i++;
+		if (i >= region_hdr->data.num_log_desc)
+			i = 0;
+
+		/* Check if we need to update the oldest index. This is needed when:
+		 * - The descriptor is invalid
+		 * - The descriptor needs to be re-used
+		 * - The data described by the descriptor will be overwritten
+		 */
+		if ((PMEM_VALIDATE_CHECKSUM(log_desc) < 0) ||
+		    (log_desc->data.size == 0) ||
+		    (reuse_desc) ||
+		    (pmem_is_overlapping(log_offset, bytes_written,
+					 log_desc->data.offset,
+					 log_desc->data.size))) {
+#ifdef CONFIG_PMEM_DEBUG
+			if (PMEM_VALIDATE_CHECKSUM(log_desc) < 0)
+				printk(KERN_ERR "desc invalid\n");
+			if (log_desc->data.size == 0)
+				printk(KERN_ERR "desc is zero\n");
+#endif
+			/* This descriptor is affected so zero it out */
+			log_desc->data.size = 0;
+			PMEM_UPDATE_CHECKSUM(log_desc);
+
+			desc_index->data.oldest_index = i;
+			reuse_desc = 0;
+		} else
+			break;
+	} while (i != desc_index->data.curr_index);
+
+	/* Done updating the indexes so update the checksum */
+	PMEM_UPDATE_CHECKSUM(desc_index);
+	PMEM_FLUSH_CACHE(desc_index, sizeof(struct pmem_log_desc_index));
+
+	log_desc = region_data + sizeof (struct pmem_log_desc_index) +
+	    desc_index->data.curr_index * sizeof (struct pmem_log_desc);
+
+	spin_unlock_irqrestore(&hdl->elem->data_lock, flags);
+
+	/* Update the new descriptor */
+	do_gettimeofday(&tv);
+#ifdef	CONFIG_HIGH_RES_TIMERS
+	log_desc->data.hrtime = gethrtime();
+#else
+	log_desc->data.hrtime = get_cycles();
+#endif
+	log_desc->data.sec = tv.tv_sec;
+	log_desc->data.usec = tv.tv_usec;
+	log_desc->data.size = bytes_written;
+	log_desc->data.offset = log_offset;
+	log_desc->data.log_checksum =
+	    pmem_checksum((__u32 *) buffer, bytes_written);
+	PMEM_UPDATE_CHECKSUM(log_desc);
+
+	/* Write the log */
+	__pmem_memcpy_toio((char *) region_data + log_desc->data.offset, buffer,
+	                  bytes_written);
+	PMEM_FLUSH_CACHE(log_desc, sizeof(struct pmem_log_desc));
+	PMEM_FLUSH_CACHE((char *) region_data + log_desc->data.offset,
+	                bytes_written);
+
+	return (bytes_written);
+}
+
+/* int pmem_write_data 
+ *
+ * Write data to persistent memory. 
+ * Returns the number of bytes written on success, and -errno on error */
+int pmem_write_data(pmem_handle_t handle, const char *buffer, int size)
+{
+	struct pmem_handle *hdl = (struct pmem_handle *) handle;
+	int bytes_written = -EINVAL;
+	void *data_start;
+
+	/* WARNING: The write_data function should be a totally quiet function
+	 * causing a printk here could end up being a recursive printk if
+	 * the PANIC_LOGS feature is on - PANIC_LOGS logs printk strings
+	 * to pmem and if we printk here on error, then that will put another
+	 * string in the kcore input buffer.  Bad. */
+	if (!pmem.enabled)
+		goto done;
+#ifdef CONFIG_PMEM_DEBUG
+	if (!handle || !buffer || (size < 0))
+		goto done;
+#endif
+
+	data_start = pmem_get_data_ptr(hdl);
+	if (!data_start)
+		goto done;
+
+	switch (hdl->type) {
+	case PMEM_HANDLE_TYPE_CB:
+	case PMEM_HANDLE_TYPE_SHADOW:
+		goto done;
+	case PMEM_HANDLE_TYPE_BLK:
+		/* determine if this is a locked segment */
+		if (0 != is_locked(handle)) {
+			bytes_written = -EPERM;
+			goto done;
+		}
+		/* fall through to the raw-write code */
+		break;
+	case PMEM_HANDLE_TYPE_PART:
+		/* fall through to the raw-write code */
+		break;
+	case PMEM_HANDLE_TYPE_REG:
+		{
+			struct region_handle *reg_hdl = handle;
+
+			/* determine if the region lives in a locked segment */
+			if (0 != is_locked(handle)) {
+				bytes_written = -EPERM;
+				goto done;
+			}
+
+			if (reg_hdl->elem->hdr->data.num_log_desc)
+				bytes_written = pmem_write_log_desc(reg_hdl,
+						  data_start, buffer, size);
+			else
+				bytes_written = pmem_write_region_data(reg_hdl,
+			                          data_start, buffer, size);
+
+			goto done;
+		}
+		break;
+	default:
+		PMEM_DPRINT("ERROR: Writing to unknown handle type %d\n", hdl->type);
+		goto done;
+		break;
+	}
+	
+	if (hdl->offset >= hdl->size)
+		goto done;
+
+	if (hdl->offset + size > hdl->size)
+		bytes_written = hdl->size - hdl->offset;
+	else
+		bytes_written = size;
+
+	__pmem_memcpy_toio(data_start + hdl->offset, buffer, bytes_written);
+
+	PMEM_FLUSH_CACHE(data_start + hdl->offset, bytes_written);
+	hdl->offset += bytes_written;
+
+      done:
+	return (bytes_written);
+}
+EXPORT_SYMBOL(pmem_write_data);
+
+/* int pmem_write_header 
+ *
+ * Write data to the header for the given handle 
+ * Returns the number of bytes written to the header on success
+ * and -errno on error.*/
+int pmem_write_header(pmem_handle_t handle, const char *buffer, int size,
+               loff_t offset)
+{
+	struct pmem_handle	*hdl = (struct pmem_handle*)handle;
+	void			*header_start;
+	int			header_size;
+	ssize_t			bytes_written = -EINVAL;
+
+	header_size = pmem_get_hdr_size(handle);
+	if (header_size <= 0) {
+		PMEM_DPRINT("ERROR: Can't write to an unsized header\n");
+		return bytes_written;
+	}
+	/* check to make sure that this header doesnt belong to a locked
+	 * segment/region */
+	if (0 != is_locked(handle)) {
+		PMEM_DPRINT("ERROR: Cant write to locked header\n");
+		return -EPERM;
+	}
+
+	if (size > header_size) {
+		size = header_size;
+	}
+
+	header_start = pmem_get_hdr_ptr(hdl);
+	if (!header_start) {
+		PMEM_DPRINT("ERROR: Can't write to a NULL header\n");
+		return bytes_written;
+	}
+	
+	if (size + offset > header_size) {
+		bytes_written = header_size - offset;
+	} else {
+		bytes_written = size;
+	}
+
+	__pmem_memcpy_toio(header_start + offset, buffer, bytes_written);
+	return bytes_written;
+}
+EXPORT_SYMBOL(pmem_write_header);
+
+/* int pmem_seek
+ * Update the given pmem handle current position according to the semantics
+ * of fseek()
+ */
+int pmem_seek(pmem_handle_t handle, int offset, int whence)
+{
+	struct pmem_handle *hdl = (struct pmem_handle *) handle;
+	int new_offset = -EINVAL;
+	void *data_start;
+
+#ifdef CONFIG_PMEM_DEBUG
+	if (!handle) {
+		PMEM_DPRINT("seek: invalid arguments\n");
+		goto done;
+	}
+#endif
+
+	data_start = pmem_get_data_ptr(hdl);
+	if (!data_start) {
+		PMEM_DPRINT("seek: invalid handle\n");
+		goto done;
+	}
+
+	switch (whence) {
+	case 0: /* SEEK_SET */
+		new_offset = offset;
+		break;
+	case 1: /* SEEK_CUR */
+		new_offset = hdl->offset + offset;
+		break;
+	case 2: /* SEEK_END */
+		new_offset = hdl->size - 1 + offset;
+		break;
+	default:
+		PMEM_DPRINT("seek: invalid whence, %d\n", whence);
+		goto done;
+		break;
+	}
+
+	if ((new_offset >= hdl->size) || (new_offset < 0)) {
+		new_offset = -EINVAL;
+		PMEM_DPRINT("invalid offset, offset=%d, size=%d\n", new_offset,
+			    hdl->size);
+		goto done;
+	}
+
+	hdl->offset = new_offset;
+
+      done:
+	return (new_offset);
+} 
+EXPORT_SYMBOL(pmem_seek);
+
diff --git a/mm/pmem/reg.c b/mm/pmem/reg.c
new file mode 100644
index 0000000..15cb112
--- /dev/null
+++ b/mm/pmem/reg.c
@@ -0,0 +1,725 @@
+#include <linux/string.h>
+#include <linux/errno.h>
+#include <linux/spinlock.h>
+#include <linux/slab.h>
+#include <linux/kernel.h>
+#include <linux/time.h>
+#include <linux/module.h>
+#include <linux/pmem.h>
+
+#include <asm/types.h>
+
+
+/* Search through the allocation table and find a matching
+ * description string */
+static struct pmem_alloc_desc * pmem_get_alloc_desc(char *desc)
+{
+	struct pmem_alloc_desc *alloc_desc;
+	int i;
+
+	for (i = 0; i < pmem.pmem->data.num_alloc; i++) {
+		alloc_desc = PMEM_GET_ALLOC_DESC(i);
+
+		if (!strncmp(alloc_desc->data.desc, desc, PMEM_DESC_MAX))
+			return (alloc_desc);
+	}
+
+	return NULL;
+}
+
+/* Search through the region headers and find a matching
+ * description string */
+static struct pmem_region_hdr *
+pmem_get_region_hdr(struct pmem_part_hdr *part_hdr, char *desc)
+{
+	struct pmem_region_hdr *region_hdr;
+	int i;
+
+	for (i = 0; i < part_hdr->data.num_regions; i++) {
+		region_hdr = PMEM_GET_REGION_HDR(part_hdr, i);
+
+		if (!strncmp(region_hdr->data.desc, desc, PMEM_DESC_MAX))
+			return (region_hdr);
+	}
+
+	return NULL;
+}
+
+/* Search through the allocation table to find unallocated memory */
+static int pmem_find_free_alloc_mem(__u32 size, __u32 * offset)
+{
+	struct pmem_alloc_desc *alloc_desc;
+	__u32 alloc_table_size;
+	__u32 end_offset;
+
+	alloc_table_size = sizeof (struct pmem_cb_hdr) +
+	    (pmem.pmem->data.num_alloc + 1) * sizeof (struct pmem_alloc_desc);
+
+	/* Allocate before the last partition */
+	if (pmem.pmem->data.num_alloc > 0) {
+		alloc_desc = PMEM_GET_ALLOC_DESC(pmem.pmem->data.num_alloc - 1);
+		end_offset = alloc_desc->data.offset;
+	} else {
+		end_offset = pmem.size;
+	}
+
+	/* Check if there is space between the last partition and 
+	 * the allocation table */
+	if (end_offset < (size + alloc_table_size))
+	{
+		return -1;
+	}
+
+	*offset = end_offset - size;
+	return 0;
+}
+
+/* Search through the block to find unallocated memory */
+static int pmem_find_free_block_mem(struct pmem_part_hdr *part_hdr,
+		__u32 size, __u32 * offset)
+{
+	struct pmem_block_hdr *block_hdr;
+	struct pmem_region_hdr *region_hdr;
+
+	/* Just get any block since they all have the same size information */
+	block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, 0);
+
+	/* Allocate after the last region */
+	if (part_hdr->data.num_regions > 0) {
+		region_hdr =
+		    PMEM_GET_REGION_HDR(part_hdr,
+					part_hdr->data.num_regions - 1);
+		*offset = region_hdr->data.offset + region_hdr->data.size;
+	} else {
+		*offset = 0;
+	}
+
+	if ((block_hdr->data.size - *offset) >= size)
+		return 0;
+
+	PMEM_DPRINT("No space: size=%d, block size=%d, offset=%d\n", size,
+		    block_hdr->data.size, *offset);
+	PMEM_DPRINT("Max size that could be allocated is %d\n",
+			(block_hdr->data.size - *offset));
+
+	/* Could not find enough memory */
+	return -1;
+}
+
+/* Prepare a region with log descriptors in a block for writing */
+static void pmem_prepare_log_desc(struct pmem_part_hdr *part_hdr,
+		__u32 block_offset, struct pmem_region_hdr *region_hdr)
+{
+	struct pmem_log_desc_index *log_desc_index;
+	struct pmem_log_desc *log_desc;
+
+	/* Create the current index structure */
+	log_desc_index =
+	    (void *) part_hdr + block_offset + region_hdr->data.offset;
+
+	strncpy(log_desc_index->data.desc, region_hdr->data.desc,
+		PMEM_DESC_MAX);
+	log_desc_index->data.oldest_index = 0;
+	log_desc_index->data.curr_index = 0;
+	PMEM_UPDATE_CHECKSUM(log_desc_index);
+
+	/* Create the first log descriptor. The rest will be created as 
+	 * they are needed */
+	log_desc =
+	    (void *) log_desc_index + sizeof (struct pmem_log_desc_index);
+	log_desc->data.hrtime = 0;
+	log_desc->data.sec = 0;
+	log_desc->data.usec = 0;
+	log_desc->data.size = 0;
+	log_desc->data.offset = 0;
+	log_desc->data.log_checksum = 0;
+	PMEM_UPDATE_CHECKSUM(log_desc);
+}
+
+/* Prepare a region without log descriptors in a block for writing */
+static void pmem_prepare_region_data(struct pmem_part_hdr *part_hdr,
+		__u32 block_offset, struct pmem_region_hdr *region_hdr)
+{
+	struct pmem_region_data_hdr *region_data_hdr;
+	__u32 data_start;
+	int num_cpus, per_proc_size, index;
+	void *data_ptr;
+
+	/* Check for per processor logs */
+	if (region_hdr->data.flags & PMEM_REGION_FLAG_PERPROC)
+		num_cpus = PMEM_NUM_SUPPORT_CPUS;
+	else
+		num_cpus = 1;
+
+	data_ptr = (void *) part_hdr + block_offset + region_hdr->data.offset;
+	strncpy(data_ptr, region_hdr->data.desc, PMEM_DESC_MAX);
+
+	data_start =
+	    PMEM_DESC_MAX + num_cpus * sizeof (struct pmem_region_data_hdr);
+	per_proc_size = (region_hdr->data.size - data_start) / num_cpus;
+	region_data_hdr = data_ptr + PMEM_DESC_MAX;
+
+	/* Create the offset structures */
+	for (index = 0; index < num_cpus; index++) {
+		region_data_hdr->start_offset =
+		    data_start + index * per_proc_size;
+		region_data_hdr->end_offset =
+		    data_start + (index + 1) * per_proc_size;
+		region_data_hdr->current_offset = region_data_hdr->start_offset;
+		region_data_hdr->lost_logs = 0;
+
+		PMEM_DPRINT("REGION: start=%d, end=%d, curr=%d, lost=%d "
+		            "desc=%s\n", region_data_hdr->start_offset,
+		            region_data_hdr->end_offset,
+		            region_data_hdr->current_offset,
+		            region_data_hdr->lost_logs, region_hdr->data.desc);
+
+		region_data_hdr++;
+	}
+}
+
+/* The following functions are the default callbacks that are called when
+ * the pmemfs module is not loaded.  This is to avoid the need for locking
+ * when executing the callbacks or when setting the callbacks.
+ */
+void pmem_default_create_partition(pmem_handle_t hdl)
+{
+	return;
+}
+EXPORT_SYMBOL(pmem_default_create_partition);
+
+void pmem_default_create_region(pmem_handle_t parent, pmem_handle_t region)
+{
+	return;
+}
+EXPORT_SYMBOL(pmem_default_create_region);
+
+
+/* Prepare a region in a block for writing */
+void pmem_prepare_region(struct pmem_part_hdr *part_hdr, __u32 block_offset,
+		struct pmem_region_hdr *region_hdr)
+{
+	if (region_hdr->data.num_log_desc != 0)
+		pmem_prepare_log_desc(part_hdr, block_offset, region_hdr);
+	else
+		pmem_prepare_region_data(part_hdr, block_offset, region_hdr);
+}
+
+/* Register a new partition */
+int pmem_partition_reg(struct pmem_reg_part *partition, pmem_handle_t *handle)
+{
+	struct pmem_alloc_desc *alloc_desc;
+	struct pmem_part_hdr *part_hdr;
+	struct pmem_block_hdr *block_hdr;
+	struct part_list_elem *part_elem;
+	__u32 block_offset=0, new_offset, block_size=0, part_size;
+	unsigned int index, len;
+	unsigned long flags;
+	int rc = 0;
+
+	if (unlikely(!partition)) {
+		PMEM_DPRINT("ERROR: Invalid partition handle %p\n", partition);
+		rc = -EINVAL;
+		goto done;
+	}
+
+	if (!pmem.enabled) {
+		PMEM_DPRINT("Not enabled\n");
+		rc = -EINVAL;
+		goto done;
+	}
+
+	part_size = partition->size;
+
+	/* Ensure proper alignment. Pad the size if necessary */
+	if (part_size % PMEM_ALIGN_BYTES)
+		part_size += PMEM_ALIGN_BYTES - (part_size % PMEM_ALIGN_BYTES);
+
+	/* Calculate the block locations */
+	if (partition->num_blocks > 0) {
+		block_offset = PMEM_PART_HDR_MAX_SIZE;
+		block_size = part_size / partition->num_blocks;
+		block_size -= block_size % PMEM_ALIGN_BYTES;
+
+		if (block_size == 0) {
+			PMEM_DPRINT("Invalid block size\n");
+			rc = -EINVAL;
+			goto done;
+		}
+		/* check to make sure that the number of blocks will not 
+		 * cause a header overflow in the pmem partition header */
+		if ((sizeof(struct pmem_part_hdr) + partition->num_blocks * 
+		     sizeof(struct pmem_block_hdr)) > PMEM_PART_HDR_MAX_SIZE) {
+			PMEM_DPRINT("Invalid number of blocks\n");
+			rc = -EINVAL;
+			goto done;
+		}         
+		PMEM_DPRINT("block_offset=%d, block_size=%d\n", block_offset,
+			    block_size);
+	}
+
+	/* Validate string. Must not be null and must have a null character */
+	len = strnlen(partition->desc, PMEM_DESC_MAX);
+	if ((0 == len) || (PMEM_DESC_MAX == len)) {
+		PMEM_DPRINT("Invalid partition string\n");
+		rc = -EINVAL;
+		goto done;
+	}
+
+	/* Allocate the dynamic structures */
+	part_elem = kmalloc(sizeof (struct part_list_elem), GFP_KERNEL);
+	if (!part_elem) {
+		printk(KERN_ERR "PMEM: Unable to allocate memory\n");
+		rc = -ENOMEM;
+		goto done;
+	}
+
+	/* Everything looks good so lets try to register this partition */
+	spin_lock_irqsave(&pmem.lock, flags);
+
+	/* Check if the partition already exists */
+	alloc_desc = pmem_get_alloc_desc(partition->desc);
+	if (alloc_desc) {
+		/* Partition already exists. Validate the arguments */
+		part_hdr = PMEM_GET_PART_HDR(alloc_desc);
+
+		if (alloc_desc->data.type != PMEM_ALLOC_TYPE_LOG) {
+			PMEM_DPRINT("Already exists\n");
+			rc = -EEXIST;
+			goto release_done;
+		}
+
+		if ((part_hdr->data.size != part_size) ||
+		    (part_hdr->data.num_blocks != partition->num_blocks) ||
+		    (part_hdr->data.version != partition->version)) {
+			PMEM_DPRINT("Already exists\n");
+			rc = -EEXIST;
+			goto release_done;
+		}
+
+		spin_unlock_irqrestore(&pmem.lock, flags);
+		kfree(part_elem);
+
+		/* Arguments are good so return success */
+		rc = pmem_partition_get(partition->desc, handle);
+		goto done;
+	}
+
+	/* The partition does not exist. Try to allocate a new partition */
+
+	/* Get an offset to a suitable memory location */
+	if (pmem_find_free_alloc_mem(part_size + PMEM_PART_HDR_MAX_SIZE,
+				     &new_offset) < 0) {
+		PMEM_DPRINT("Unable to find free memory\n");
+		rc = -ENOMEM;
+		goto release_done;
+	}
+
+	PMEM_DPRINT("size=%d, new_offset=%d\n", part_size, new_offset);
+
+	part_hdr = (void *) pmem.pmem + new_offset;
+
+	/* Fill in the header information */
+	strncpy(part_hdr->data.desc, partition->desc, PMEM_DESC_MAX);
+	if (len < PMEM_DESC_MAX) {
+		/* use the __pmem_memset command here since the data lives in 
+		 * pmem not a dynamic DS */
+		__pmem_memset(&part_hdr->data.desc[len], 0, PMEM_DESC_MAX - len);
+	}
+
+	part_hdr->data.size = part_size;
+	part_hdr->data.num_blocks = partition->num_blocks;
+	part_hdr->data.num_regions = 0;
+	part_hdr->data.flags = 0;
+	part_hdr->data.version = partition->version;
+
+	PMEM_UPDATE_CHECKSUM(part_hdr);
+
+	/* Update the allocation descriptor */
+	index = pmem.pmem->data.num_alloc;
+	alloc_desc = PMEM_GET_ALLOC_DESC(index);
+	strncpy(alloc_desc->data.desc, partition->desc, PMEM_DESC_MAX);
+	if (len < PMEM_DESC_MAX)
+		__pmem_memset(&alloc_desc->data.desc[len], 0, PMEM_DESC_MAX - len);
+
+	alloc_desc->data.offset = new_offset;
+	alloc_desc->data.size = part_size + PMEM_PART_HDR_MAX_SIZE;
+	alloc_desc->data.type = PMEM_ALLOC_TYPE_LOG;
+
+	PMEM_UPDATE_CHECKSUM(alloc_desc);
+
+	/* Update the control block header */
+	pmem.pmem->data.num_alloc = index + 1;
+	PMEM_UPDATE_CB_CHECKSUM(pmem.pmem);
+
+	/* We have done enough so that no one else will register the same 
+	 * partition so unlock the headers */
+	spin_unlock_irqrestore(&pmem.lock, flags);
+
+	part_elem->hdr = part_hdr;
+	part_elem->active_block_offset = 0;
+	INIT_LIST_HEAD(&part_elem->region_list);
+
+	/* Update the block headers */
+	for (index = 0; index < partition->num_blocks; index++) {
+		block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, index);
+
+		block_hdr->data.offset = block_offset + block_size * index;
+		block_hdr->data.size = block_size;
+		block_hdr->data.sec = 0;
+		block_hdr->data.usec = 0;
+		block_hdr->data.flags = 0;
+
+		/* Make one of the blocks active */
+		if (index == 0) {
+			struct timeval tv;
+
+			block_hdr->data.flags = PMEM_BLOCK_FLAG_ACTIVE;
+			part_elem->active_block_offset = block_hdr->data.offset;
+			do_gettimeofday(&tv);
+			block_hdr->data.sec = tv.tv_sec;
+			block_hdr->data.usec = tv.tv_usec;
+		}
+
+		PMEM_UPDATE_CHECKSUM(block_hdr);
+	}
+
+	/* Clear the partition memory area in persistent memory */
+	pmem_memset((void *) part_hdr + PMEM_PART_HDR_MAX_SIZE, 0, part_size);
+
+	spin_lock_irqsave(&pmem.lock, flags);
+
+	/* Partition is ready for use so add it to the list */
+	list_add(&(part_elem->list_elem), &pmem.part_list);
+
+	spin_unlock_irqrestore(&pmem.lock, flags);
+
+	rc = pmem_partition_get(partition->desc, handle);
+	
+	/* Notify any listeners that there is a new partition */
+	pmem_events.create_partition(*handle);
+	goto done;
+
+      release_done:
+	spin_unlock_irqrestore(&pmem.lock, flags);
+	kfree(part_elem);
+
+      done:
+	return (rc);
+}
+EXPORT_SYMBOL(pmem_partition_reg);
+
+/* Register a region. Returns a region handle if successful */
+int pmem_region_reg(pmem_handle_t part_handle,
+		struct pmem_reg_region *region, pmem_handle_t * handle)
+{
+	struct part_handle *hdl = (struct part_handle *) part_handle;
+	struct part_list_elem *part_elem;
+	struct region_list_elem *region_elem;
+	struct pmem_region_hdr *region_hdr;
+	__u32 new_offset, region_size;
+	unsigned long flags;
+	int rc = -EINVAL;
+	unsigned int len, next;
+
+	if (!part_handle || !region) {
+		PMEM_DPRINT("NULL arguments\n");
+		goto done;
+	}
+
+	/* Ensure that it is a partition handle */
+	if (hdl->hdl.type != PMEM_HANDLE_TYPE_PART) {
+		PMEM_DPRINT("Invalid handle at %p\n", part_handle);
+		goto done;
+	}
+
+	part_elem = hdl->elem;
+
+	/* Validate the arguments */
+	if (region->flags >> PMEM_REGION_NUM_FLAGS) {
+		PMEM_DPRINT("Invalid flags\n");
+		goto done;
+	}
+
+	if (region->size == 0) {
+		PMEM_DPRINT("Invalid size %d\n", region->size);
+		goto done;
+	}
+
+	region_size = region->size;
+
+	/* Validate string. Must not be null and must have a null character */
+	len = strnlen(region->desc, PMEM_DESC_MAX);
+	if ((0 == len) || (PMEM_DESC_MAX == len)) {
+		PMEM_DPRINT("Invalid region string\n");
+		goto done;
+	}
+
+	/* Ensure proper alignment. Pad the size if necessary */
+	if (region_size % PMEM_ALIGN_BYTES)
+		region_size += PMEM_ALIGN_BYTES -
+		    (region_size % PMEM_ALIGN_BYTES);
+
+	if ((region->num_log_desc != 0)
+	    && ((region->flags & PMEM_REGION_FLAG_PERPROC) ||
+		(region->fixed_size != 0))) {
+		PMEM_DPRINT("Invalid flags\n");
+		goto done;
+	}
+
+	if ((region->num_log_desc * sizeof (struct pmem_log_desc)
+	     + sizeof (struct pmem_log_desc_index))
+	    > region_size) {
+		PMEM_DPRINT("Invalid number of descriptors %x\n",
+			    region->num_log_desc);
+		goto done;
+	}
+
+	if ((region->fixed_size + sizeof (struct pmem_region_data_hdr)) >
+	    region_size) {
+		PMEM_DPRINT("Invalid fixed log size %x\n", region->fixed_size);
+		goto done;
+	}
+
+	if (part_elem->hdr->data.num_blocks == 0) {
+		PMEM_DPRINT("No blocks for region\n");
+		goto done;
+	}
+
+	region_elem = kmalloc(sizeof (struct region_list_elem), GFP_KERNEL);
+	if (!region_elem) {
+		PMEM_DPRINT("Unable to allocate memory\n");
+		rc = -ENOMEM;
+		goto done;
+	}
+
+#ifdef CONFIG_PMEM_DEBUG
+	if (PMEM_VALIDATE_CHECKSUM(part_elem->hdr) < 0) {
+		PMEM_DPRINT("Partition header invalid\n");
+		goto done;
+	}
+#endif
+	/* Everything looks good so lets try to register this region */
+	spin_lock_irqsave(&pmem.lock, flags);
+
+	/* Check if the region is already registered */
+	region_hdr = pmem_get_region_hdr(part_elem->hdr, region->desc);
+	if (region_hdr) {
+		/* Region is registered. Check if the arguments match */
+		if ((region_hdr->data.size != region_size) ||
+		    (region_hdr->data.fixed_size != region->fixed_size) ||
+		    (region_hdr->data.flags != region->flags) ||
+		    (region_hdr->data.num_log_desc != region->num_log_desc) ||
+		    (region_hdr->data.version != region->version)) {
+			PMEM_DPRINT("Region already exists\n");
+			rc = -EEXIST;
+			goto release_done;
+		}
+
+		spin_unlock_irqrestore(&pmem.lock, flags);
+		kfree(region_elem);
+
+		/* Arguments are good so return success */
+		rc = pmem_region_get(hdl, region->desc, region->block_id,
+				     handle);
+		goto done;
+	}
+
+	/* Region is not registered. Try to allocate a new one */
+
+	/* Get an area in the block */
+	if (pmem_find_free_block_mem(part_elem->hdr, region_size, &new_offset)
+	    < 0) {
+		PMEM_DPRINT("Unable to allocate space for part [%s]\n",
+		            region->desc);
+		rc = -ENOSPC;
+		goto release_done;
+	}
+
+	/* determine the id of the next region to be allocated */
+	next = part_elem->hdr->data.num_regions;
+	/* need to increment the number of regions here before success
+	 * so that the GET_REGION_HDR command will not return NULL.  The 
+	 * older non-safe version of GET_REGION_HDR didnt care, but the new
+	 * version does */
+	part_elem->hdr->data.num_regions++;
+	region_hdr = PMEM_GET_REGION_HDR(part_elem->hdr, next);
+	/* Check the region header does not overflow the first page of 
+	 * the partition */
+	if ((((long)region_hdr + 1) - (long)part_elem->hdr) > PMEM_PART_HDR_MAX_SIZE) {
+		PMEM_DPRINT("Unable to allocate space for region's header [%s]\n", region->desc);
+		/* not successful */
+		part_elem->hdr->data.num_regions--;
+		PMEM_UPDATE_CHECKSUM(part_elem->hdr);
+		rc = -ENOSPC;
+		goto release_done;
+	}               
+
+	region_elem->hdr = region_hdr;
+	region_elem->data_lock = SPIN_LOCK_UNLOCKED;
+	region_elem->ptr_block = NULL;
+
+	region_hdr->data.size = region_size;
+	region_hdr->data.offset = new_offset;
+	region_hdr->data.flags = region->flags;
+	region_hdr->data.fixed_size = region->fixed_size;
+	region_hdr->data.num_log_desc = region->num_log_desc;
+	region_hdr->data.version = region->version;
+	strncpy(region_hdr->data.desc, region->desc, PMEM_DESC_MAX);
+	if (len < PMEM_DESC_MAX)
+		pmem_memset(&region_hdr->data.desc[len], 0, PMEM_DESC_MAX - len);
+
+	PMEM_UPDATE_CHECKSUM(region_hdr);
+	PMEM_UPDATE_CHECKSUM(part_elem->hdr);
+
+	/* We have done enough so that no one else will register the same 
+	 * partition so unlock the headers */
+	spin_unlock_irqrestore(&pmem.lock, flags);
+
+	/* Update the region data area in the block */
+	pmem_prepare_region(part_elem->hdr, part_elem->active_block_offset,
+			region_hdr);
+
+	spin_lock_irqsave(&pmem.lock, flags);
+
+	/* Region is ready for use so add it to the list */
+	list_add(&(region_elem->list_elem), &(part_elem->region_list));
+
+	spin_unlock_irqrestore(&pmem.lock, flags);
+
+	rc = pmem_region_get(hdl, region->desc, region->block_id, handle);
+	
+	/* Notify any listeners that there is a new region in this partition */
+	pmem_events.create_region(hdl, *handle);
+	goto done;
+
+      release_done:
+	spin_unlock_irqrestore(&pmem.lock, flags);
+	kfree(region_elem);
+
+      done:
+	return (rc);
+}
+EXPORT_SYMBOL(pmem_region_reg);
+
+/* Register a pointer block with a region. */
+int pmem_register_ptr_block(pmem_handle_t region_handle,
+		struct pmem_ptr_block *ptr_block)
+{
+	struct region_handle *hdl = (struct region_handle *)region_handle;
+	struct pmem_block_hdr *block_hdr;
+	struct pmem_region_hdr *region_hdr;
+	int rc = 0;
+	int locked;
+	unsigned long flags;
+
+	if (hdl->hdl.type != PMEM_HANDLE_TYPE_REG) {
+		PMEM_DPRINT("Invalid handle type\n");
+		rc = -EINVAL;
+		goto done;
+	}
+
+	region_hdr = hdl->elem->hdr;
+
+	/* Special handles are only valid for fixed per-processor logs */
+	if ((region_hdr->data.num_log_desc != 0) ||
+	    !(region_hdr->data.flags & PMEM_REGION_FLAG_PERPROC)) {
+		PMEM_DPRINT("Invalid region for handle\n");
+		rc = -EINVAL;
+		goto done;
+	}
+
+	spin_lock_irqsave(&pmem.lock, flags);
+
+	/* check to see if this block is locked */
+	block_hdr = PMEM_GET_BLOCK_HDR(hdl->parent->elem->hdr,
+	                               hdl->block_id);
+	locked = (block_hdr->data.flags & PMEM_BLOCK_FLAG_LOCK);
+	
+	/* Disallow multiple registrations */
+	if (hdl->elem->ptr_block) {
+		spin_unlock_irqrestore(&pmem.lock, flags);
+		PMEM_DPRINT("block already registered\n");
+		rc = -EINVAL;
+		goto done;
+	}
+
+	hdl->elem->ptr_block = ptr_block;
+
+	spin_unlock_irqrestore(&pmem.lock, flags);
+
+	/* Fill in the handle data */
+	pmem_update_ptr_block(hdl->parent->elem, hdl->elem, locked);
+
+      done:
+	return (rc);
+}
+EXPORT_SYMBOL(pmem_register_ptr_block);
+
+/* Update the pointer block with the location of the region in the active 
+ * block.  If lock is set, then this routine will immediately lock the
+ * ptr block to prevent logging. */
+void pmem_update_ptr_block(struct part_list_elem *part_elem,
+                           struct region_list_elem *region_elem, int lock)
+{
+	struct pmem_region_data_hdr *region_data_hdr;
+	struct pmem_cpu_ptrs *ptrs = NULL;
+	void *region_data;
+	__u32 end_offset;
+	int num_cpus;
+	int i = 0;
+
+	if (!region_elem->ptr_block)
+		return;
+
+	if (region_elem->hdr->data.flags & PMEM_REGION_FLAG_PERPROC)
+		num_cpus = PMEM_NUM_SUPPORT_CPUS;
+	else
+		num_cpus = 1;
+
+	region_data = (void *) part_elem->hdr + part_elem->active_block_offset +
+	    region_elem->hdr->data.offset;
+
+	/* Setup the array pointers for each cpu */
+	for (i = 0; i < num_cpus; i++) {
+		region_data_hdr =
+		    (struct pmem_region_data_hdr *) (region_data +
+						     PMEM_DESC_MAX) + i;
+
+		/* Ensure that the end offset is a multiple of the logs */
+		end_offset = region_data_hdr->end_offset -
+		    ((region_data_hdr->end_offset -
+		      region_data_hdr->start_offset) %
+		     region_elem->hdr->data.fixed_size);
+
+		/* Update the storage that is not currently in use, mark the
+		 * unused storage area by setting its start ptr to NULL */
+		if (region_elem->ptr_block->cpu[i].ptrs ==
+		    &region_elem->ptr_block->cpu[i].storeA) {
+			ptrs = &region_elem->ptr_block->cpu[i].storeB;
+			region_elem->ptr_block->cpu[i].storeA.start = NULL;
+		} else {
+			ptrs = &region_elem->ptr_block->cpu[i].storeA;
+			region_elem->ptr_block->cpu[i].storeB.start = NULL;
+		}
+
+		ptrs->start = region_data + region_data_hdr->start_offset;
+		ptrs->end = region_data + end_offset;
+		ptrs->curr = ptrs->start;
+		ptrs->lost_logs = &region_data_hdr->lost_logs;
+
+#ifdef CONFIG_PMEM_DEBUG
+		if ((ptrs->start < (char *) pmem.pmem) ||
+		    (ptrs->end > ((char *) pmem.pmem + pmem.size))) {
+			PMEM_DPRINT("Invalid pointers\n");
+			return;
+		}
+#endif
+		if (0 == lock) {
+			/* Make the new pointer block active only if the 
+			 * block is not locked, otherwise they stay in
+			 * storeA/storeB */
+			region_elem->ptr_block->cpu[i].ptrs = ptrs;
+		}
+	}
+}
-- 
1.6.0.3

