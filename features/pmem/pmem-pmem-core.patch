From 3ea1621c0eedc7bdd41f0a4da71485818e6c355b Mon Sep 17 00:00:00 2001
From: Jason HU <yongqi.hu@windriver.com>
Date: Fri, 30 Apr 2010 02:45:58 -0700
Subject: [PATCH 1/4] pmem: pmem core

Provide a framework for storing information in non-volatile memory.
A simulation mode uses RAM instead of non-volatile memory.

There is a true persistent mode for certain boards that have hardware
support for it.

Signed-off-by: Jason HU <yongqi.hu@windriver.com>
---
 drivers/Makefile          |    1 +
 drivers/pmem/Makefile     |    4 +
 drivers/pmem/pmem_init.c  | 1350 +++++++++++++++++++++++++++++++++++++++++++++
 drivers/pmem/pmem_pci.c   |  263 +++++++++
 include/linux/kernel.h    |    5 +
 include/linux/module.h    |    5 +
 include/linux/pmem.h      |  848 ++++++++++++++++++++++++++++
 include/linux/schedhist.h |   95 ++++
 include/linux/sysctl.h    |    1 +
 init/Kconfig              |   38 ++
 kernel/module.c           |   85 +++
 kernel/panic.c            |   10 +
 kernel/printk.c           |   85 +++
 kernel/sched.c            |   95 ++++
 kernel/sysctl.c           |   14 +
 mm/Makefile               |    1 +
 mm/pmem/Kconfig           |   88 +++
 mm/pmem/Makefile          |    5 +
 mm/pmem/cmds.c            | 1100 ++++++++++++++++++++++++++++++++++++
 mm/pmem/handle.c          |  498 +++++++++++++++++
 mm/pmem/io.c              |  636 +++++++++++++++++++++
 mm/pmem/reg.c             |  750 +++++++++++++++++++++++++
 22 files changed, 5977 insertions(+), 0 deletions(-)
 create mode 100644 drivers/pmem/Makefile
 create mode 100644 drivers/pmem/pmem_init.c
 create mode 100644 drivers/pmem/pmem_pci.c
 create mode 100644 include/linux/pmem.h
 create mode 100644 include/linux/schedhist.h
 create mode 100644 mm/pmem/Kconfig
 create mode 100644 mm/pmem/Makefile
 create mode 100644 mm/pmem/cmds.c
 create mode 100644 mm/pmem/handle.c
 create mode 100644 mm/pmem/io.c
 create mode 100644 mm/pmem/reg.c

diff --git a/drivers/Makefile b/drivers/Makefile
index 2c4f277..1fd48a4 100644
--- a/drivers/Makefile
+++ b/drivers/Makefile
@@ -98,6 +98,7 @@ obj-$(CONFIG_INFINIBAND)	+= infiniband/
 obj-$(CONFIG_SGI_SN)		+= sn/
 obj-y				+= firmware/
 obj-$(CONFIG_CRYPTO)		+= crypto/
+obj-$(CONFIG_PMEM)		+= pmem/
 obj-$(CONFIG_SUPERH)		+= sh/
 obj-$(CONFIG_ARCH_SHMOBILE)	+= sh/
 obj-$(CONFIG_GENERIC_TIME)	+= clocksource/
diff --git a/drivers/pmem/Makefile b/drivers/pmem/Makefile
new file mode 100644
index 0000000..8df36a9
--- /dev/null
+++ b/drivers/pmem/Makefile
@@ -0,0 +1,4 @@
+ifeq ($(CONFIG_PMEM_PCI_DRIVER),y)
+obj-y	+= pmem_pci.o
+endif
+obj-y	+= pmem_init.o
diff --git a/drivers/pmem/pmem_init.c b/drivers/pmem/pmem_init.c
new file mode 100644
index 0000000..8f0ee7d
--- /dev/null
+++ b/drivers/pmem/pmem_init.c
@@ -0,0 +1,1350 @@
+/*
+ *  Pmem initialization routines.
+ *
+ *  These are broken out into the drivers/pmem tree to allow them to be
+ *  linked into the kernel after the PCI drivers.  The PCI drivers need to 
+ *  be able to find the memory to be used by pmem and then the initialization
+ *  can occur.
+ *
+ *  This initialization code used to be mm/pmem/init.c and init/main.c
+ *
+ *  Copyright 2010 Wind River Systems
+ *
+ *  This program is free software; you can redistribute it and/or modify it
+ *  under the terms of the GNU General Public License as published by the
+ *  Free Software Foundation; either version 2 of the License, or (at your
+ *  option) any later version.
+ *
+ *
+ *  THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ *  WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ *  IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+ *  BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS
+ *  OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ *  ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR
+ *  TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
+ *  USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ *  You should have received a copy of the GNU General Public License along
+ *  with this program; if not, write to the Free Software Foundation, Inc.,
+ *  675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#include <linux/string.h>
+#include <linux/errno.h>
+#include <linux/slab.h>
+#include <linux/vmalloc.h>
+#include <linux/spinlock.h>
+#include <linux/kernel.h>
+#include <linux/time.h>
+#include <linux/module.h>
+#include <linux/pmem.h>
+#include <linux/zlib.h>
+#include <linux/zutil.h>
+#include <linux/bootmem.h>	/* alloc_bootmem_pages() */
+#include <linux/init.h>		/* __setup */
+#include <linux/notifier.h>
+#include <linux/reboot.h>	/* register_reboot_notifier() */
+#include <linux/mm.h>
+#include <linux/sched.h>
+
+#include <asm/types.h>
+
+#ifdef CONFIG_SCHED_HIST_BUF
+#include <linux/schedhist.h>
+extern struct pmem_ptr_block sched_hist_pmem_block;
+#endif /* CONFIG_SCHED_HIST_BUF */
+
+#ifdef CONFIG_PANIC_LOGS
+extern pmem_handle_t kcore_part_hdl;
+extern pmem_handle_t kcore_reg_hdl;
+#endif /* CONFIG_PANIC_LOGS */
+
+#ifdef CONFIG_LHB
+extern struct pmem_ptr_block lhb_ptrs;
+#endif	/* CONFIG_LHB */
+
+#ifdef CONFIG_LHB_TESTMODE
+int lhb_mask = 7;
+
+#include <linux/proc_fs.h>
+
+#include <linux/uaccess.h>
+static struct proc_dir_entry *lhb_mask_dir, *lhb_mask_file;
+void lhb_mask_init(void);
+#else
+#define lhb_mask_init() do { } while(0)
+#endif  /* CONFIG_LHB_TESTMODE */
+
+#ifdef CONFIG_PMEM_LOG_REG
+extern pmem_handle_t general_part_hdl;
+extern pmem_handle_t general_reg_hdl;
+#endif /* CONFIG_PMEM_LOG_REG */
+
+/* Allow the kernel command line to force a pmem format */
+static int __initdata pmem_force_clear = 0;
+
+static int __init pmem_force_clear_setup(char *str)
+{
+	pmem_force_clear = 1;
+	return 1;
+}
+__setup("pmem_force_clear", pmem_force_clear_setup);
+
+/* the Global pmem structure */
+struct pmem_cb_data pmem = {
+	.pmem 		= NULL,
+	.size 		= 0,
+	.lock	 	= SPIN_LOCK_UNLOCKED,
+	.shadow		= NULL,
+	.enabled	= 0,
+	.using_hardware	= 0,
+	.using_io_mem   = 0,
+};
+EXPORT_SYMBOL(pmem);
+
+/* Reboot notification block for pmem */
+#ifdef CONFIG_BOARD_HAS_PMEM_HARDWARE
+static int pmem_reboot_handler(struct notifier_block *this, 
+                               unsigned long code, void *unused);
+static struct notifier_block pmem_reboot_notifier = {
+	pmem_reboot_handler,
+	NULL,
+	99
+};
+#endif
+
+
+/* initialize the callbacks to empty */
+struct pmem_event_block pmem_events = {
+	.create_partition	= &pmem_default_create_partition,
+	.create_region		= &pmem_default_create_region,
+};
+EXPORT_SYMBOL(pmem_events);
+
+struct persistent_control{
+	unsigned long magic;
+	unsigned long reboot_chksum;
+	unsigned long reboot_reason;
+	unsigned long reboot_counter;
+	unsigned long test;
+	unsigned long trigger0;
+	unsigned long trigger1;
+};
+
+#define PERSISTENT_CONTROL_MAGIC 		0xdeedbeef
+#define PERSISTENT_CONTROL_COUNTER_MASK		0xffff0000
+#define PERSISTENT_CONTROL_COUNTER_VALUE	0x12340000
+
+#define MR_CLEAN_CONTROL(persistent_control_st)	do{\
+	persistent_control_st->reboot_counter=PERSISTENT_CONTROL_COUNTER_VALUE;\
+	persistent_control_st->test=0;\
+	persistent_control_st->magic=0;\
+	persistent_control_st->reboot_reason=0;\
+	persistent_control_st->reboot_chksum=0;\
+}while(0)
+
+#define CLEAN_CONTROL(persistent_control_st)	do{\
+	persistent_control_st->reboot_reason=0;\
+	persistent_control_st->reboot_chksum=0;\
+}while(0)
+
+#define INIT_CONTROL(persistent_control_st)	do{\
+	persistent_control_st->reboot_counter=PERSISTENT_CONTROL_COUNTER_VALUE;\
+	persistent_control_st->magic=PERSISTENT_CONTROL_MAGIC;\
+}while(0)
+
+/* Returns true if the two areas overlap */
+int pmem_is_overlapping(__u32 offset_a, __u32 size_a, __u32 offset_b,
+		__u32 size_b)
+{
+	/* Check if above */
+	if (offset_a + size_a <= offset_b)
+		return 0;
+	/* Check if below */
+	if (offset_b + size_b <= offset_a)
+		return 0;
+
+	/* Above are false therefore it must overlap */
+	return 1;
+}
+
+/* Validate the data in the allocation table entry */
+static int pmem_validate_alloc_entry(int alloc_index)
+{
+	struct pmem_alloc_desc *alloc_desc;
+	int rc = 0;
+
+	alloc_desc = PMEM_GET_ALLOC_DESC(alloc_index);
+
+	if (PMEM_VALIDATE_CHECKSUM(alloc_desc) < 0) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	/* Range check the offset. Ensure that it is in persistent memory */
+	if ((alloc_desc->data.offset + alloc_desc->data.size) > pmem.size) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	/* Ensure that it is not inside the allocation table */
+	if (alloc_desc->data.offset <
+	    (sizeof (struct pmem_cb_hdr) +
+	     (pmem.pmem->data.num_alloc * sizeof (struct pmem_alloc_desc)))) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	switch (alloc_desc->data.type) {
+	case PMEM_ALLOC_TYPE_LOG:
+		break;
+	default:
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	PMEM_DPRINT("Partition: desc=%s, offset=%d, size=%d, type=%d\n",
+		    alloc_desc->data.desc, alloc_desc->data.offset,
+		    alloc_desc->data.size, alloc_desc->data.type);
+
+      done:
+	return (rc);
+}
+
+
+static int pmem_validate_block(struct pmem_part_hdr *part_hdr,
+		    struct pmem_block_hdr *block_hdr)
+{
+	int rc = 0;
+
+	if (PMEM_VALIDATE_CHECKSUM(block_hdr) < 0) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	if (block_hdr->data.size == 0) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	if (block_hdr->data.flags >> PMEM_BLOCK_NUM_FLAGS) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	/* Range check the offset. Ensure it is inside the partition data */
+	if (block_hdr->data.offset < PMEM_PART_HDR_MAX_SIZE) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	if ((block_hdr->data.offset + block_hdr->data.size) >
+	    (part_hdr->data.size + PMEM_PART_HDR_MAX_SIZE)) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	PMEM_DPRINT("Log block: hdr_offset=%ld, offset=%d, size=%d, flags=%d\n",
+		    (unsigned long) block_hdr - (unsigned long) pmem.pmem,
+		    block_hdr->data.offset, block_hdr->data.size,
+		    block_hdr->data.flags);
+
+      done:
+	return (rc);
+}
+
+static int pmem_validate_region(struct pmem_part_hdr *part_hdr,
+		struct pmem_region_hdr *region_hdr)
+{
+	struct pmem_block_hdr *block_hdr;
+	int rc = 0;
+
+	if (PMEM_VALIDATE_CHECKSUM(region_hdr) < 0) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	if (region_hdr->data.size == 0) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	if (region_hdr->data.flags >> PMEM_REGION_NUM_FLAGS) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	/* Get any block since they all have the same size information */
+	block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, 0);
+
+	/* Range check the offset. Ensure it is inside the block */
+	if ((region_hdr->data.offset + region_hdr->data.size) >
+	    block_hdr->data.size) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	PMEM_DPRINT
+	    ("Log region: desc=%s, offset=%d, size=%d, flags=%d, fixed_size=%d, num_log_desc=%d\n",
+	     region_hdr->data.desc, region_hdr->data.offset,
+	     region_hdr->data.size, region_hdr->data.flags,
+	     region_hdr->data.fixed_size, region_hdr->data.num_log_desc);
+
+      done:
+	return (rc);
+}
+
+#ifdef CONFIG_BOARD_HAS_PMEM_HARDWARE
+/*
+ * There is 3 way to take the system down. Power down, Spurious reset and Controlled 
+ * reboot.
+ * - Power down wipe off all persistent memory.
+ * - Spurious reset can be WatchDog or Blade reset initiated by another processor 
+ *   (Ex 405 reboot...).  The Spurious reset conserve persistent memory, in the case
+ *   of the 280, some special trick need to be used to tell the Firmware todo so.
+ * - Controlled reboot can be a reboot cmd (system call), panic, bug, oops...
+ *
+ * The following code try to determine why the system went down and if the reason is
+ * a Controlled reboot then a Chksum is performed and match against the control 
+ * information.
+ * That functionality will detect a possible Firmware memory trampler over the pmem...
+ * On a Controlled reboot, machine_restart() or machine_power_off() or machine_halt() 
+ * are used.  So here the idea is simple, in those above function simply update the 
+ * control information to know at the next boot time where we come from.
+ *
+ * On Neptune platform the Persistent memory is not  entirely validated. With the help
+ * of this code {reboot counter, magic value and trigger} there should be enough to 
+ * move forward...
+ *
+ * The Check Sum is the whole persistent memory except over the LOGBUFFER as it may 
+ * change at boot/reset time i.e. modified before this code run from a Reset/Reboot
+ *
+ * On the 280 the only way to add persistent storage is at the bottom.
+ * On 3PC the Firmware size is persistent, let use some of it.
+ * If the bottom of the Firmware size is corrupted, chance are the pmem is also 
+ * corrupted!  mem_pieces_remove add PERSISTENT_CONTROL_SIZE bytes to
+ * the bottom and the top of the Persistent storage to handle 280 and TPC and the 
+ * same time
+ * -etiennem
+ */
+
+static int persistent_mem_crc(int reason)
+{
+	static int init=0;
+	static char *persistent_mem = NULL ;
+	static struct persistent_control *persistent_control_st =NULL;
+	uLong adler = zlib_adler32(0L, NULL, 0);
+	static int pmem_size = 0;
+	struct part_list_elem	*part_elem;
+	struct list_head	*elem, *temp;
+	pmem_handle_t part_handle = NULL;
+	int i;
+
+	if(!init){
+		/* Acquire architecture specific pointers */
+		persistent_mem = (char*)pmem_arch_get_start_ptr();
+		if(!persistent_mem){
+			printk(KERN_ERR "Cannot get persistent_mem pointer !\n");
+			return -1;
+		}
+		persistent_control_st = (struct persistent_control*)pmem_arch_get_checksum_start_ptr();
+		if(!persistent_control_st){
+			printk(KERN_ERR "Cannot get persistent_control_st !\n");
+			return -1;
+		}
+
+		pmem_size = pmem_arch_get_size();
+		if (pmem_size <= 0) {
+			printk(KERN_ERR "Cant get pmem size from arch driver\n");
+			return -1;
+		}
+		init=1;
+
+	} else {
+		/* Stop the kernel users from writing to persistent memory
+		 * so that the checksum is valid */
+		list_for_each_safe(elem, temp, &pmem.part_list) {
+			part_elem = list_entry(elem, struct part_list_elem, 
+			                       list_elem);
+			if (pmem_partition_get(part_elem->hdr->data.desc, &part_handle) != 0) {
+				PMEM_DPRINT("ERROR: Cant get handle for part [%s]\n", part_elem->hdr->data.desc);
+				continue;
+			}
+			if (pmem_disable_ptr_blocks(part_handle) < 0) {
+				printk(KERN_ERR "Failed to disable pointer block for partition [%s]\n", part_elem->hdr->data.desc);
+			} else {
+#ifdef CONFIG_PMEM_DEBUG
+				printk(KERN_INFO "Kernel pmem pointer blocks disabled for [%s]\n", part_elem->hdr->data.desc);
+#endif
+			}
+			pmem_release_handle(&part_handle);
+		} /* for each pmem partition */
+		pmem.enabled = 0;
+	}
+	if (pmem_size % PAGE_SIZE != 0) {
+		printk(KERN_WARNING "PMEM: Warning - size not multiple of PAGE_SIZE\n");
+	}
+        /*Compute checksum -- do it in blocks so that the soft lockup
+	 detection doesn't trip and belch complaints... */
+	printk(KERN_INFO "PMEM CRC starting (this may take a while)\n");
+	for (i = 0 ; i < pmem_size/PAGE_SIZE ; i++) {
+		adler = zlib_adler32(adler, persistent_mem + (i*PAGE_SIZE), PAGE_SIZE);
+		schedule();
+	}
+	printk(KERN_INFO "Persistent Memory chkSum %lx\n",adler);
+
+	switch(reason){
+	/*****BOOT-UP time*****/
+	case PERSISTENT_CONTROL_BOOTUP:/*Boot-up, Let's check against the Last reset value*/
+#if 0
+		/*Visual Check*/
+		printk("Persistent Memory test %lx\n",persistent_control_st->test);
+		persistent_control_st->test=0x11223344;
+		printk("After Persistent Memory test %lx\n",persistent_control_st->test);
+#endif
+		/*Trigger use by UserLand dd if=/dev/mem bs=1M | od -t x 4 |grep 0x40414243
+		 * or the ASCII string "ABCDEFGH"*/
+		persistent_control_st->trigger0=0x41424344;
+		persistent_control_st->trigger1=0x45464748;
+
+		if(persistent_control_st->magic != PERSISTENT_CONTROL_MAGIC){/*Nothing to do here*/
+			printk(KERN_INFO "Persistent Memory Power Down\n");
+			/*Should also clear Pmem as well, later...*/
+			MR_CLEAN_CONTROL(persistent_control_st);
+			INIT_CONTROL(persistent_control_st);
+			return 0;
+		}
+		switch(persistent_control_st->reboot_reason){
+		case PERSISTENT_CONTROL_RESTART:
+		case PERSISTENT_CONTROL_HALT:
+		case PERSISTENT_CONTROL_PWR_OFF:
+			printk(KERN_INFO "Persistent Memory Controlled reboot\n");
+			if((persistent_control_st->reboot_counter & PERSISTENT_CONTROL_COUNTER_MASK)==PERSISTENT_CONTROL_COUNTER_VALUE){
+				printk(KERN_INFO "Persistent Memory reboot_counter=%ld\n",persistent_control_st->reboot_counter &~PERSISTENT_CONTROL_COUNTER_MASK);
+				persistent_control_st->reboot_counter++;
+			}
+			if(persistent_control_st->reboot_chksum != adler){
+				printk(KERN_INFO "WARNING Persistent Memory corrupted : new=%lx, old=%lx\n", persistent_control_st->reboot_chksum, adler);
+			}
+			CLEAN_CONTROL(persistent_control_st);
+			break;
+		default:
+			printk(KERN_INFO "Persistent Memory Spurious reset\n");
+			if((persistent_control_st->reboot_counter & PERSISTENT_CONTROL_COUNTER_MASK)==PERSISTENT_CONTROL_COUNTER_VALUE){
+				printk(KERN_INFO "Persistent Memory reboot_counter=%ld\n",persistent_control_st->reboot_counter &~PERSISTENT_CONTROL_COUNTER_MASK);
+				persistent_control_st->reboot_counter++;
+			}
+			printk(KERN_INFO "WARNING Persistent Memory NO chkSum validation\n");
+			/*This is the case for Neptune. 405 reset the 280*/
+			CLEAN_CONTROL(persistent_control_st);
+			break;
+		}
+		break;
+
+	/*****Controlled reboot time*****/
+	case PERSISTENT_CONTROL_RESTART:
+	case PERSISTENT_CONTROL_HALT:
+	case PERSISTENT_CONTROL_PWR_OFF:
+		persistent_control_st->reboot_reason = reason;
+		persistent_control_st->reboot_chksum = adler;
+		break;
+	default:
+		BUG();
+	}
+	PMEM_FLUSH_CACHE(persistent_control_st, sizeof(struct persistent_control));
+
+	return 0;
+}
+#endif /* CONFIG_BOARD_HAS_PMEM_HARDWARE */
+
+static int pmem_validate_partition(struct pmem_alloc_desc *alloc_desc)
+{
+	struct pmem_part_hdr *part_hdr;
+	struct pmem_block_hdr *block_hdr;
+	struct pmem_region_hdr *region_hdr;
+	__u32 block_size = 0;
+	__u32 newest_sec = 0;
+	__u32 newest_usec = 0;
+	int index = 0;
+	int block_active = 0;
+	int newest_block_index = 0;
+	int rc = 0;
+
+	if (alloc_desc->data.size < sizeof (struct pmem_part_hdr)) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	part_hdr = PMEM_GET_PART_HDR(alloc_desc);
+
+	if (PMEM_VALIDATE_CHECKSUM(part_hdr) < 0) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	if (part_hdr->data.size !=
+			alloc_desc->data.size - PMEM_PART_HDR_MAX_SIZE) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	if (strncmp(part_hdr->data.desc, alloc_desc->data.desc,
+	            PMEM_DESC_MAX)) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	if (part_hdr->data.flags != 0) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	PMEM_DPRINT("Log partition: desc=%s, size=%d, blocks=%d, regions=%d\n",
+		    part_hdr->data.desc, part_hdr->data.size,
+		    part_hdr->data.num_blocks, part_hdr->data.num_regions);
+
+	/* Check that each individual block is valid */
+	for (index = 0; index < part_hdr->data.num_blocks; index++) {
+		block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, index);
+
+		if (pmem_validate_block(part_hdr, block_hdr) < 0) {
+			rc = -1;
+			PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+			goto done;
+		}
+	}
+
+	/* Check that the blocks do not conflict with each other */
+	for (index = 0; index < part_hdr->data.num_blocks; index++) {
+		/* Get the first element for the overlap macro */
+		block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, 0);
+
+		if (PMEM_CHECK_FOR_OVERLAP
+		    (block_hdr, index, part_hdr->data.num_blocks) < 0) {
+			rc = -1;
+			PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+			goto done;
+		}
+
+		block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, index);
+
+		/* Ensure that all block sizes are the same */
+		if (block_size == 0)
+			block_size = block_hdr->data.size;
+		else if (block_hdr->data.size != block_size) {
+			rc = -1;
+			PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+			goto done;
+		}
+
+		/* Ensure that only one block was active */
+		if (block_hdr->data.flags & PMEM_BLOCK_FLAG_ACTIVE) {
+			if (block_active == 0)
+				block_active = 1;
+			else {
+				rc = -1;
+				PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+				goto done;
+			}
+		}
+
+		/* Check for the newest block index */
+		if ((block_hdr->data.sec > newest_sec) &&
+				(block_hdr->data.usec > newest_usec)) {
+			newest_sec = block_hdr->data.sec;
+			newest_usec = block_hdr->data.usec;
+			newest_block_index = index;
+		}
+	}
+
+	/* Check that one block is active */
+	if ((part_hdr->data.num_blocks > 0) && (block_active == 0)) {
+	/* Nothing was active. Just make the newest block active
+	   so that it will get rotated out */
+		PMEM_DPRINT("PMEM: Nothing active. Setting block %d active\n",
+				newest_block_index);
+
+		block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, newest_block_index);
+		block_hdr->data.flags |= PMEM_BLOCK_FLAG_ACTIVE;
+		PMEM_UPDATE_CHECKSUM(block_hdr);
+	}
+
+	/* Check that each individual region is valid */
+	for (index = 0; index < part_hdr->data.num_regions; index++) {
+		region_hdr = PMEM_GET_REGION_HDR(part_hdr, index);
+
+		if (pmem_validate_region(part_hdr, region_hdr) < 0) {
+			rc = -1;
+			PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+			goto done;
+		}
+	}
+
+	/* Check that the regions do not conflict with each other */
+	for (index = 0; index < part_hdr->data.num_regions; index++) {
+		/* Get the first element for the overlap macro */
+		region_hdr = PMEM_GET_REGION_HDR(part_hdr, 0);
+
+		if (PMEM_CHECK_FOR_OVERLAP
+		    (region_hdr, index, part_hdr->data.num_regions) < 0) {
+			rc = -1;
+			PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+			goto done;
+		}
+	}
+
+      done:
+	return (rc);
+}
+
+static int pmem_validate_control_block(void)
+{
+	struct pmem_cb_hdr *pmem_cbh;
+	int desc_index;
+	int rc = 0;
+
+	pmem_cbh = pmem.pmem;
+
+	if (!pmem_cbh) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		goto done;
+	}
+
+	if (PMEM_VALIDATE_CB_CHECKSUM(pmem_cbh) < 0) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+		PMEM_DPRINT("Chksum should be %d\n", (int)pmem_cbh->checksum);
+		goto done;
+	}
+
+	if (ntohl(pmem_cbh->data.size) != pmem.size) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, size=%d\n", ntohl(pmem_cbh->data.size));
+		goto done;
+	}
+
+	if (pmem_cbh->data.version != PMEM_VERSION_CURRENT) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, version=%d\n", pmem_cbh->data.version);
+		goto done;
+	}
+
+	if (pmem_cbh->data.num_cpus != PMEM_NUM_SUPPORT_CPUS) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, num_cpus=%d\n", pmem_cbh->data.num_cpus);
+		goto done;
+	}
+
+	//PMEM_DPRINT("INFO: Loading pmem with arch=%d, endian=%d\n", PMEM_GET_ARCH(pmem_cbh), PMEM_GET_ENDIANESS(pmem_cbh));
+	if (pmem_cbh->data.arch_info != PMEM_ARCH_INFO) {
+		rc = -1;
+		/* FIXME: Some cases may not be fatal here */
+		PMEM_DPRINT("Validation failed, arch=%d, endianess=%d\n",
+		            PMEM_GET_ARCH(pmem_cbh),
+		            PMEM_GET_ENDIANESS(pmem_cbh));
+		goto done;
+	}
+
+	if (ntohl(pmem_cbh->data.flags) >> PMEM_CB_NUM_FLAGS) {
+		rc = -1;
+		PMEM_DPRINT("Validation failed, flags=%d\n", ntohl(pmem_cbh->data.flags));
+		goto done;
+	}
+
+	/* Validate the allocation table */
+
+	/* Check that each individual entry is valid */
+	for (desc_index = 0; desc_index < pmem.pmem->data.num_alloc;
+	     desc_index++) {
+		if (pmem_validate_alloc_entry(desc_index) < 0) {
+			rc = -1;
+			PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+			goto done;
+		}
+	}
+
+	/* Check that the entries do not conflict with each other */
+	for (desc_index = 0; desc_index < pmem.pmem->data.num_alloc;
+	     desc_index++) {
+		/* Get the first element for the overlap macro */
+		struct pmem_alloc_desc *alloc_desc = PMEM_GET_ALLOC_DESC(0);
+
+		/* Ensure that no one else is using the same offsets */
+		if (PMEM_CHECK_FOR_OVERLAP
+		    (alloc_desc, desc_index, pmem.pmem->data.num_alloc) < 0) {
+			rc = -1;
+			PMEM_DPRINT("Validation failed, rc=%d\n", rc);
+			goto done;
+		}
+	}
+
+	PMEM_DPRINT("Control block is valid, num_alloc = %d\n",
+		    pmem.pmem->data.num_alloc);
+
+      done:
+	return (rc);
+}
+
+static int pmem_validate_pmem(void)
+{
+	struct pmem_alloc_desc *alloc_desc;
+	int desc_index;
+
+	if (pmem_validate_control_block() < 0)
+		return -1;
+
+	/* Loop through each partition and check its headers */
+	for (desc_index = 0; desc_index < pmem.pmem->data.num_alloc;
+	     desc_index++) {
+		alloc_desc = PMEM_GET_ALLOC_DESC(desc_index);
+
+		switch (alloc_desc->data.type) {
+		case PMEM_ALLOC_TYPE_LOG:
+			if (pmem_validate_partition(alloc_desc) < 0)
+				return -1;
+			break;
+		default:
+			return -1;
+		}
+
+		PMEM_DPRINT("Done validating partition %d\n", desc_index);
+	}
+
+	return 0;
+}
+
+
+static void pmem_shadowcopy(void)
+{
+#ifdef CONFIG_PMEM_SHADOW
+	/* Copy the memory to a shadow area */
+	if (pmem.shadow) {
+		PMEM_DPRINT("shadow not null\n");
+		return;
+	}
+
+	pmem.shadow = vmalloc(pmem.size);
+	if (!pmem.shadow) {
+		printk(KERN_ERR "PMEM: could not allocate %d shadow bytes\n",
+		       pmem.size);
+		return;
+	}
+
+	pmem_memcpy_fromio(pmem.shadow, (void*)pmem.pmem, pmem.size);
+	PMEM_DPRINT("pmem shadow copy at %p\n", pmem.shadow);
+#endif
+}
+
+/* Clears persistent memory and sets up a fresh control block */
+static int pmem_reinit(void)
+{
+	int rc = 0;
+	struct pmem_cb_hdr_data hdr_data;
+
+#ifdef CONFIG_PMEM_DEBUG
+	PMEM_DPRINT("pmem_reinit() running on %p with hw=%d:%d\n", pmem.pmem, pmem.using_hardware, pmem.using_io_mem);
+#endif
+	/* Only save a shadow copy if we're using hardware pmem.*/
+	if (pmem.using_hardware)
+		pmem_shadowcopy();
+
+	/* Clear all of persistent memory */
+	pmem_memset((void *) pmem.pmem, 0, pmem.size);
+	
+	
+	/* Create a new control block from scratch */
+	pmem.pmem->data.size = htonl(pmem.size);
+	pmem.pmem->data.flags = htonl(0);
+	pmem.pmem->data.version = PMEM_VERSION_CURRENT;
+	pmem.pmem->data.num_cpus = PMEM_NUM_SUPPORT_CPUS;
+	pmem.pmem->data.arch_info = PMEM_ARCH_INFO;
+	pmem.pmem->data.num_alloc = 0;
+
+	PMEM_UPDATE_CB_CHECKSUM(pmem.pmem);
+
+	/* If we are using hardware - make sure that the values just written
+	 * can be read back.  Sometimes if the hardware is misconfigured or
+	 * has errors, we can read back garbage values. */
+	if (pmem.using_hardware) {
+		pmem_memset(&hdr_data, 0, sizeof(struct pmem_cb_hdr_data));
+		hdr_data.size = htonl(pmem.size);
+		hdr_data.flags = htonl(0);
+		hdr_data.version = PMEM_VERSION_CURRENT;
+		hdr_data.num_cpus = PMEM_NUM_SUPPORT_CPUS;
+		hdr_data.arch_info = PMEM_ARCH_INFO;
+		hdr_data.num_alloc = 0;
+
+		rc = memcmp(&(pmem.pmem->data), &hdr_data,
+		            sizeof(struct pmem_cb_hdr_data));
+		if (0 != rc) {
+			printk(KERN_ERR "ERROR: Hardware pmem is inconsitent rc=%d\n", rc);
+			return -EIO;
+		}
+	}
+	
+	return 0;
+}
+
+/* Create the elements needed for operation */
+static int pmem_volatile_init(void)
+{
+	struct pmem_alloc_desc *alloc_desc;
+	int desc_index;
+
+	INIT_LIST_HEAD(&pmem.part_list);
+
+	for (desc_index = 0; desc_index < pmem.pmem->data.num_alloc;
+	     desc_index++) {
+		alloc_desc = PMEM_GET_ALLOC_DESC(desc_index);
+
+		switch (alloc_desc->data.type) {
+		case PMEM_ALLOC_TYPE_LOG:
+			{
+				struct part_list_elem *part_elem;
+				struct region_list_elem *region_elem;
+				struct pmem_block_hdr *block_hdr = NULL;
+				int index;
+
+				part_elem =
+				    kmalloc(sizeof (struct part_list_elem),
+					    GFP_KERNEL);
+				if (!part_elem) {
+					printk(KERN_ERR
+					       "PMEM: Unable to allocate memory\n");
+					return (-ENOMEM);
+				}
+
+				part_elem->hdr = PMEM_GET_PART_HDR(alloc_desc);
+				part_elem->active_block_offset = 0;
+				INIT_LIST_HEAD(&part_elem->region_list);
+
+				/* Figure out which block was active. */
+				for (index = 0;
+				     index < part_elem->hdr->data.num_blocks;
+				     index++) {
+					block_hdr =
+					    PMEM_GET_BLOCK_HDR(part_elem->hdr,
+							       index);
+
+					if (block_hdr->data.
+					    flags & PMEM_BLOCK_FLAG_ACTIVE)
+						part_elem->active_block_offset =
+						    block_hdr->data.offset;
+				}
+
+				/* Ensure that one block is active if there are blocks */
+				if ((part_elem->active_block_offset == 0)
+				    && (block_hdr)) {
+					part_elem->active_block_offset =
+					    block_hdr->data.offset;
+					block_hdr->data.flags &=
+					    PMEM_BLOCK_FLAG_ACTIVE;
+					PMEM_UPDATE_CHECKSUM(block_hdr);
+				}
+
+				for (index = 0;
+				     index < part_elem->hdr->data.num_regions;
+				     index++) {
+					region_elem =
+					    kmalloc(sizeof
+						    (struct region_list_elem),
+						    GFP_KERNEL);
+					if (!region_elem) {
+						printk(KERN_ERR
+						       "PMEM: Unable to allocate memory\n");
+						return (-ENOMEM);
+					}
+
+					region_elem->hdr =
+					    PMEM_GET_REGION_HDR(part_elem->hdr,
+								index);
+					region_elem->data_lock =
+					    SPIN_LOCK_UNLOCKED;
+					region_elem->ptr_block = NULL;
+
+					list_add(&(region_elem->list_elem),
+						 &(part_elem->region_list));
+				}
+
+				list_add(&(part_elem->list_elem),
+					 &pmem.part_list);
+				break;
+			}
+		}
+
+		PMEM_DPRINT("Created elements for partition %d\n", desc_index);
+	}
+
+	return 0;
+}
+
+/* Performs extra partition initialization such as rotating blocks */
+static void pmem_rotate_partitions(void)
+{
+	struct part_list_elem *part_elem;
+	struct list_head *elem, *temp;
+
+	list_for_each_safe(elem, temp, &pmem.part_list) {
+		part_elem = list_entry(elem, struct part_list_elem, list_elem);
+
+		if (part_elem->hdr->data.num_blocks > 1)
+			if (pmem_rotate_block_data(part_elem, 0) < 0)
+				printk(KERN_ERR
+				       "PMEM: unable to rotate partition\n");
+	}
+	PMEM_DPRINT("INFO: pmem_rotate_partitions() done.\n");
+}
+
+static int pmem_reg_users(void)
+{
+	struct pmem_reg_part part;
+	struct pmem_reg_region region;
+	pmem_handle_t log_part_hdl = NULL;
+#if defined(CONFIG_SCHED_HIST_BUF) || defined(CONFIG_LHB)
+	pmem_handle_t region_hdl = NULL;
+#endif
+	int rc;
+
+	/* Initialize the various pointer blocks */
+#ifdef CONFIG_SCHED_HIST_BUF
+	memset(&sched_hist_pmem_block, '\0', sizeof(struct pmem_ptr_block));
+#endif
+#ifdef CONFIG_PANIC_LOGS
+	kcore_part_hdl = NULL;
+	kcore_reg_hdl = NULL;
+#endif
+#ifdef CONFIG_LHB
+	memset(&lhb_ptrs, '\0', sizeof(struct pmem_ptr_block));
+#endif
+#ifdef CONFIG_PMEM_LOG_REG
+	general_part_hdl = NULL;
+	general_reg_hdl = NULL;
+#endif
+
+	/* Register the log partition */
+	strncpy(part.desc, PMEM_PART_LOG_DESC, PMEM_DESC_MAX);
+	part.size = CONFIG_PMEM_LOG_PART_SIZE;
+	part.num_blocks = CONFIG_PMEM_LOG_PART_SEGMENTS;
+	part.version = PMEM_PART_LOG_VERSION;
+
+	rc = pmem_partition_reg(&part, &log_part_hdl);
+	if (rc < 0) {
+		printk(KERN_ERR "ERROR: **********************************\n");
+		printk(KERN_ERR "ERROR: Unable to register log partition\n");
+		printk(KERN_ERR "ERROR: Kernel regions are DISABLED (LHB, etc)!\n");
+		printk(KERN_ERR "ERROR: **********************************\n");
+		return rc;
+	}
+
+#ifdef CONFIG_PANIC_LOGS
+	/* Register the printk region and keep the handles */
+	if (pmem_partition_reg(&part, &kcore_part_hdl) < 0) {
+		printk(KERN_ERR "ERROR: **********************************\n");
+		printk(KERN_ERR "ERROR: Unable to get log partition\n");
+		printk(KERN_ERR "ERROR: Persistent panic feature DISABLED\n");
+		printk(KERN_ERR "ERROR: **********************************\n");
+	}
+	else {
+		strncpy(region.desc, "kcore", PMEM_DESC_MAX);
+		region.size = CONFIG_PANIC_LOGS_SIZE;
+		region.flags = PMEM_REGION_FLAG_PERPROC | PMEM_REGION_FLAG_STOPFULL;
+		region.fixed_size = 0;
+		region.num_log_desc = 0;
+		region.version = 1;
+		region.block_id = PMEM_ACTIVE_BLOCK;
+
+		if (pmem_region_reg(kcore_part_hdl, &region, &kcore_reg_hdl) < 0) {
+			printk(KERN_ERR "ERROR: **********************************\n");
+			printk(KERN_ERR "ERROR: Unable to get kcore logs region\n");
+			printk(KERN_ERR "ERROR: Persistent panic feature disabled\n");
+			printk(KERN_ERR "ERROR: **********************************\n");
+			
+			pmem_release_handle(&kcore_part_hdl);
+		}
+	}
+#endif
+
+#ifdef CONFIG_PMEM_LOG_REG
+	/* Register the general region and keep the handles */
+	if (pmem_partition_reg(&part, &general_part_hdl) < 0) {
+		printk(KERN_ERR "ERROR: **********************************\n");
+		printk(KERN_ERR "ERROR: Unable to get general partition\n");
+		printk(KERN_ERR "ERROR: **********************************\n");
+	}
+	else {
+		/* Register the log region and release the handle */
+		strncpy(region.desc, PMEM_REG_GENERAL_DESC, PMEM_DESC_MAX);
+		region.size = CONFIG_PMEM_LOG_REG_SIZE;
+		region.flags = PMEM_REG_GENERAL_FLAGS;
+		region.fixed_size = PMEM_REG_GENERAL_FIXED_SIZE;
+		region.num_log_desc = CONFIG_PMEM_LOG_REG_LOGS;
+		region.version = PMEM_REG_GENERAL_VERSION;
+		region.block_id = PMEM_ACTIVE_BLOCK;
+
+		if (pmem_region_reg(general_part_hdl, &region, &general_reg_hdl) < 0) {
+			printk(KERN_ERR "ERROR: **********************************\n");
+			printk(KERN_ERR "ERROR: Unable to get general region\n");
+			printk(KERN_ERR "ERROR: **********************************\n");
+			pmem_release_handle(&general_part_hdl);
+		}
+	}
+#endif
+
+#ifdef CONFIG_SCHED_HIST_BUF 
+	strncpy(region.desc, PMEM_REG_SCHED_DESC, PMEM_DESC_MAX);
+	region.size = CONFIG_SCHED_HIST_SIZE;
+	region.flags = PMEM_REG_SCHED_FLAGS;
+	region.fixed_size = PMEM_REG_SCHED_FIXED_SIZE;
+	region.num_log_desc = PMEM_REG_SCHED_LOG_DESC;
+	region.version = PMEM_REG_SCHED_VERSION;
+	region.block_id = PMEM_ACTIVE_BLOCK;
+
+	if (pmem_region_reg(log_part_hdl, &region, &region_hdl) < 0) {
+		printk(KERN_ERR "ERROR: **********************************\n");
+		printk(KERN_ERR "ERROR: Unable to get sched history region\n");
+		printk(KERN_ERR "ERROR: Scheduler History is DISABLED.\n");
+		printk(KERN_ERR "ERROR: **********************************\n");
+	}
+	else {
+		if (pmem_register_ptr_block(region_hdl, &sched_hist_pmem_block) < 0) {
+			printk(KERN_ERR "ERROR: *****************************\n");
+			printk(KERN_ERR "ERORR: Unable to get sched history ptr block\n");
+			printk(KERN_ERR "ERROR: Scheduler History is DISABLED.\n");
+			printk(KERN_ERR "ERROR: *****************************\n");
+		}
+
+		pmem_release_handle(&region_hdl);
+	}
+	
+#endif /* CONFIG_SCHED_HIST_BUF */
+#ifdef CONFIG_LHB
+	/* Register the exception region and the pointer block and
+	 * release the handles */
+	strncpy(region.desc, PMEM_REG_EXCEPT_DESC, PMEM_DESC_MAX);
+	region.size = CONFIG_LHB_SIZE;
+	region.flags = PMEM_REG_EXCEPT_FLAGS;
+	region.fixed_size = PMEM_REG_EXCEPT_FIXED_SIZE;
+	region.num_log_desc = PMEM_REG_EXCEPT_LOG_DESC;
+	region.version = PMEM_REG_EXCEPT_VERSION;
+	region.block_id = PMEM_ACTIVE_BLOCK;
+
+	if (pmem_region_reg(log_part_hdl, &region, &region_hdl) < 0) {
+		printk(KERN_ERR "ERROR: *************************************\n");
+		printk(KERN_ERR "ERROR: Unable to get exception history region\n");
+		printk(KERN_ERR "ERROR: Linux History Buffer is DISABLED.\n");
+		printk(KERN_ERR "ERROR: *************************************\n");
+
+	}
+	else {
+		if (pmem_register_ptr_block(region_hdl, &lhb_ptrs) < 0) {
+			printk(KERN_ERR "ERROR: *************************************\n");
+			printk(KERN_ERR "ERROR: Unable to get exception history  ptr block\n");
+			printk(KERN_ERR "ERROR: Linux History Buffer is DISABLED.\n");
+			printk(KERN_ERR "ERROR: *************************************\n");
+		}
+		pmem_release_handle(&region_hdl);
+	}
+#endif
+
+	/* Release the log partition */
+	pmem_release_handle(&log_part_hdl);
+
+	printk(KERN_INFO "PMEM: pmem_reg_users() done.\n");
+	return 0;
+}
+
+/* Setup pmem to use simulated memory.  A chunk of vmalloc()d ram.
+ * Returns 0 on successful allocated of the simulated pmem
+ */
+static int alloc_simulated_pmem(void)
+{
+	unsigned long len;
+	unsigned long end;
+	int rc = 0;
+	int i;
+
+	/* Round off required pmem size to PAGE_SIZE */
+	len = CONFIG_PMEM_SIZE + (PAGE_SIZE -1);
+	len &= ~(PAGE_SIZE - 1);
+	if ((pmem.pmem = vmalloc(len))) {
+		pmem.size = CONFIG_PMEM_SIZE;
+		pmem.using_hardware = 0;
+		pmem.using_io_mem = 0;
+
+		end = (unsigned long)pmem.pmem + len;
+		for (i = 0; i < len ; i += PAGE_SIZE) {
+			SetPageReserved(vmalloc_to_page((void *)((unsigned long)pmem.pmem + i)));
+		}
+
+		printk(KERN_ERR "PMEM: Using simulated pmem\n");
+	} else {
+		printk(KERN_ERR "PMEM: No simulated pmem available\n");
+		rc = -ENOMEM;
+	}
+
+	return rc;
+}
+
+/* Generic initialization function for persistent memory.
+ * When this function has completed, users can start using
+ * persistent memory. */
+static int pmem_init(void)
+{
+	int rc = -EINVAL;
+	int do_reinit=1;
+
+	if (pmem.enabled) {
+		/* Already initialized */
+		printk(KERN_ERR "WARNING: Calling pmem_init with pmem already"
+		                " initialized\n");
+		return rc;
+	}
+
+#ifdef CONFIG_BOARD_HAS_PMEM_HARDWARE
+	/* Get the memory.  If this passes, it is assumed that
+	 * hardware pmem is available. */
+	if (pmem_arch_get_persistent_memory((void **) &pmem.pmem, &pmem.size) >= 0) {
+		pmem.using_hardware = 1;
+		pmem.using_io_mem = (__u8)pmem_arch_uses_ioremap();
+
+		/* notify the pmem crc checker that the system is coming up.
+		 * On PPC32 this used to be in the setup_arch call, but 
+		 * on Xscale, the system is not yet ready to execute this
+		 * code, so its moved to here */
+		persistent_mem_crc(PERSISTENT_CONTROL_BOOTUP);
+	} else {
+		printk(KERN_ERR "PMEM: Unable to get hardware persistent memory\n");
+		pmem.using_hardware = 0;
+		pmem.using_io_mem = 0;
+	}
+#endif
+
+alloc_simulated:
+	if (!pmem.using_hardware) {
+		if (0 != alloc_simulated_pmem()) {
+			printk(KERN_ERR "PMEM: Cant get simulated pmem !\n");
+			return rc;
+		}
+		goto reinit;
+	}
+
+	if (!pmem.pmem) {
+		printk(KERN_ERR "ERROR: pmem memory hasnt been setup!\n");
+		return rc;
+	}
+
+	PMEM_DPRINT("PMEM: phys=%p, virt=%p, size=%d hardware=%d:%d\n",
+		    (void *) __pa(pmem.pmem), pmem.pmem, pmem.size,
+		    pmem.using_hardware, pmem.using_io_mem);
+
+	/* check if the kernel asked to forcibly format pmem */
+	if (pmem_force_clear) {
+		printk(KERN_INFO "PMEM: Asked to forcibly format pmem\n");
+		goto reinit;
+	}
+	/* Validate the headers in persistent memory
+	 * If any corruption exists, or the memory is uninitialized
+	 * then clear the memory and start over */
+	if (pmem_validate_pmem() < 0) {
+		printk(KERN_ERR "PMEM: Structure is corrupt\n");
+		goto reinit;
+	}
+
+	/* Check if the user requested to clear all persistent memory */
+	if (ntohl(pmem.pmem->data.flags) & PMEM_CB_FLAG_RESET) {
+		printk(KERN_NOTICE "PMEM: Clearing persistent memory based on user flags\n");
+		goto reinit;
+	}
+
+	do_reinit=0;
+reinit:
+	if (do_reinit) {
+		if (0 != pmem_reinit()) {
+			if (pmem.using_hardware) {
+				/* fall back to simulated pmem */
+				pmem.using_hardware = 0;
+				pmem.using_io_mem = 0;
+				goto alloc_simulated;
+			} else {
+				printk(KERN_ERR "ERROR: Cant init pmem !\n");
+				return rc;
+			}
+		}
+	}
+
+	/* Create the dynamic structures for existing data */
+	if (pmem_volatile_init() < 0) {
+		printk(KERN_ERR "PMEM: Unable to create dynamic structures\n");
+		return rc;
+	}
+
+	/* Initialize the existing partitions if we didn't wipe the memory. */
+	if (!do_reinit) {
+		/* ensure that the active partition is locked from the previous
+		 * boot */
+		if (pmem.using_hardware) {
+			pmem_lock_all_active_segments();
+		}
+		pmem_rotate_partitions();
+	}
+
+#ifdef CONFIG_PMEM_DEBUG
+	if (pmem_validate_pmem() < 0) {
+		printk(KERN_ERR
+		       "PMEM: Partition initialization caused corruption\n");
+	}
+#endif
+
+	printk(KERN_NOTICE "PMEM: initialization completed\n");
+	pmem.enabled = 1;
+
+
+	/* install shutdown/reboot CRC routine only if using hardware pmem */
+#ifdef CONFIG_BOARD_HAS_PMEM_HARDWARE
+	if (pmem.using_hardware) {
+		register_reboot_notifier(&pmem_reboot_notifier);
+	}
+#endif
+	/* Now call the function that will setup common kernel areas */
+	rc = pmem_reg_users();
+
+	lhb_mask_init();
+
+	return rc;
+}
+__initcall(pmem_init);
+
+#ifdef CONFIG_LHB_TESTMODE
+static int lhb_mask_write(struct file *file, const char *buffer,
+                          unsigned long count, void *data)
+{
+	int val;
+	char debug_string[12] = { '\0' };
+
+	if (count > sizeof(debug_string) - 1)
+		return -EINVAL;
+
+	if (copy_from_user(debug_string, buffer, count))
+		return -EFAULT;
+
+	debug_string[count] = '\0';
+
+	val = (int)simple_strtoul(debug_string, NULL, 0);
+	if ((val < 0) || (val >= 8)) 
+		return -EINVAL;
+	else 
+		lhb_mask = val;
+
+	return count;
+}
+
+static int lhb_mask_read(char *page, char **start, off_t off, int count,
+                         int *eof, void *data)
+{
+	int len;
+
+	len = sprintf(page, "Current LHB Mask = %d\n", lhb_mask);
+
+	return len;
+}
+
+void lhb_mask_init(void)
+{
+	lhb_mask_dir = proc_mkdir("lhb", NULL);
+	if (!lhb_mask_dir) {
+		printk(KERN_ERR "Failed to create LHB Mask dir in proc\n");
+		return;
+	}
+
+	lhb_mask_dir->owner = THIS_MODULE;
+
+	lhb_mask_file = create_proc_entry("mask", 0600, lhb_mask_dir);
+	if (!lhb_mask_file) {
+		remove_proc_entry("lhb", NULL);
+		printk(KERN_ERR "Failed to create LHB Mask entry in proc\n");
+		return;
+	}
+	lhb_mask_file->data = NULL;
+	lhb_mask_file->read_proc = lhb_mask_read;
+	lhb_mask_file->write_proc = lhb_mask_write;
+	lhb_mask_file->owner = THIS_MODULE;
+
+	printk(KERN_INFO "Loaded LHB Event mask support with value %d\n", lhb_mask);
+}
+
+#endif
+#ifdef CONFIG_BOARD_HAS_PMEM_HARDWARE
+/* Hardware pmem wants to update the CRC on pmem on reboot */
+static int pmem_reboot_handler(struct notifier_block *this,
+			       unsigned long code, void *unused)
+{
+	int reason;
+	int rc = -EFAULT;
+#ifdef CONFIG_RESET_LOGS
+	struct timeval tv;
+
+	kcore_logon(1);
+	do_gettimeofday(&tv);
+	printk(KERN_EMERG "pmem reboot handler at %9d:%06d\n", (int) tv.tv_sec, (int) tv.tv_usec);
+	dump_stack();
+	printk("Current process %d: '%s'\n", current->pid, current->comm);
+	kcore_logoff_quiet();
+#endif
+	printk(KERN_INFO "INFO: Starting pmem_reboot_handler\n");
+	
+	/* lock the current active segment to prevent accidental 
+	 * overwrite in the case of a reboot loop */
+	pmem_lock_all_active_segments();
+
+	switch (code) {
+		case SYS_RESTART:
+			/* also handles SYS_DOWN  */
+			reason = PERSISTENT_CONTROL_RESTART;
+			break;
+		case SYS_HALT:
+			reason = PERSISTENT_CONTROL_HALT;
+			break;
+		case SYS_POWER_OFF:
+			reason = PERSISTENT_CONTROL_PWR_OFF;
+			break;
+		default:
+			reason = 0xffffffff;
+			printk(KERN_ERR "WARN: Unknown reboot code %ld\n", code);
+			break;
+	}
+	
+	/* always going to be a reboot if CONFIG_ALWAYS_RESTART is set */
+	rc = persistent_mem_crc(reason);
+	if (rc) {
+		printk(KERN_ERR "ERROR: Failed to calculate pmem CRC err=%d\n", rc);
+	}
+
+	printk(KERN_INFO "INFO: Finishing pmem_reboot_handler\n");
+	return rc;
+}
+#endif 
+
+
+
diff --git a/drivers/pmem/pmem_pci.c b/drivers/pmem/pmem_pci.c
new file mode 100644
index 0000000..3d9f5b8
--- /dev/null
+++ b/drivers/pmem/pmem_pci.c
@@ -0,0 +1,263 @@
+ /*
+ *  Copyright 2010 Wind River Systems
+ *
+ *  This program is free software; you can redistribute it and/or modify it
+ *  under the terms of the GNU General Public License as published by the
+ *  Free Software Foundation; either version 2 of the License, or (at your
+ *  option) any later version.
+ *
+ *
+ *  THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ *  WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ *  IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+ *  BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS
+ *  OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ *  ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR
+ *  TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
+ *  USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ *  You should have received a copy of the GNU General Public License along
+ *  with this program; if not, write to the Free Software Foundation, Inc.,
+ *  675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/pci.h>
+#include <linux/init.h>
+
+#include <linux/pmem.h>
+
+#define PCI_DEVICE_ID_MOTOROLA_9030	0x4825
+#define PCI_9030_PMEM_RESOURCE		2
+
+#define DEV_TYPE_9030	1
+
+/* 'global' values that the pmem pci driver will set.
+ *
+ * These will be read by the pmem_arch_get_persistent_memory() routine 
+ */
+static unsigned long pmem_pci_start = 0;
+static unsigned long pmem_pci_length = 0;
+static unsigned short pmem_pci_vendor_id = 0;
+static unsigned short pmem_pci_device_id = 0;
+static unsigned short pmem_pci_resource_num = 0;
+
+/* single access point to the ioremaped pmem pointers.  Removes the need for
+ * the crc code to iounmap its pointers */
+static void *pmem_start_ptr = (void *)0;
+static void *pmem_checksum_ptr = (void *)0;
+
+/* PCI based pmem is always ioremapped in to the kernels address space */
+int pmem_arch_uses_ioremap(void)
+{
+	return 1;
+}
+
+int pmem_arch_get_size(void)
+{
+	unsigned long size;
+
+	/* Start with user requested size, and round down required to
+	   PAGE_SIZE.  This allows partitions to be page aligned for MMAP to
+	   work. */
+	size = (CONFIG_PMEM_SIZE >> PAGE_SHIFT) << PAGE_SHIFT;
+
+	if (!pmem_pci_start) {
+		printk(KERN_ERR
+		       "Error: Cant determine size before PCI mapping\n");
+		return -1;
+	}
+
+	if (size + PERSISTENT_CONTROL_SIZE > pmem_pci_length) {
+#ifdef CONFIG_PMEM_DEBUG
+		PMEM_DPRINT("INFO: reducing pmem size to meet hw limits\n");
+#endif
+		size = ((pmem_pci_length - PERSISTENT_CONTROL_SIZE)
+		        >> PAGE_SHIFT) << PAGE_SHIFT;
+	}
+
+	if (size % PAGE_SIZE) {
+		PMEM_DPRINT("WARNING: PMEM size (%lu) is not a multiple of "
+		            "PAGE_SIZE\n", size);
+	}
+	return size;
+}
+
+struct pci_dev *pmem_arch_get_pci_dev(struct pci_dev *dev)
+{
+	if (!pmem_pci_start) {
+		PMEM_DPRINT("ERROR: Asked for pmem pci_dev before PCI init done\n");
+	}
+	
+	return pci_get_device(pmem_pci_vendor_id, pmem_pci_device_id, NULL);
+}
+EXPORT_SYMBOL(pmem_arch_get_pci_dev);
+
+void *pmem_arch_get_pci_start_ptr(void)
+{
+	if (!pmem_pci_start) {
+		PMEM_DPRINT("ERROR: Asked for pmem pci ptr before PCI init done\n");
+	}
+
+	return (void *)pmem_pci_start;
+}
+EXPORT_SYMBOL(pmem_arch_get_pci_start_ptr);
+
+int pmem_arch_get_pci_resource_num(void)
+{
+	if (!pmem_pci_start) {
+		PMEM_DPRINT("ERROR: Asked for pmem pci resource num before PCI init done\n");
+	}
+
+	return pmem_pci_resource_num;
+}
+EXPORT_SYMBOL(pmem_arch_get_pci_resource_num);
+
+void *pmem_arch_get_start_ptr(void)
+{
+	int pmem_size = pmem_arch_get_size();
+
+	if (!pmem_pci_start) {
+		PMEM_DPRINT("ERROR: Asked for pmem ptr before PCI init done\n");
+		return 0;
+	}
+
+	if (!pmem_start_ptr) {
+
+#ifdef CONFIG_PMEM_DEBUG
+		printk(KERN_INFO "PCI Persistent Memory located at %lx "
+		       "size %x\n", pmem_pci_start + PERSISTENT_CONTROL_SIZE,
+		       pmem_size);
+#endif
+		pmem_start_ptr = (void *)ioremap_nocache(pmem_pci_start +
+							 PERSISTENT_CONTROL_SIZE,
+							 pmem_size);
+		if (!pmem_pci_start) {
+			printk(KERN_ERR "ERROR: Cant get pci pmem \n");
+		}
+	}
+
+	return pmem_start_ptr;
+}
+EXPORT_SYMBOL(pmem_arch_get_start_ptr);
+
+void *pmem_arch_get_checksum_start_ptr(void)
+{
+	if (!pmem_pci_start)
+		return 0;
+
+	if (!pmem_checksum_ptr) {
+#ifdef CONFIG_PMEM_DEBUG
+		printk("PCI Persistent chkSum located at %lx size %x\n",
+		       pmem_pci_start, (int)PERSISTENT_CONTROL_SIZE);
+#endif
+		pmem_checksum_ptr = ioremap_nocache(pmem_pci_start,
+						    PERSISTENT_CONTROL_SIZE);
+		if (!pmem_checksum_ptr) {
+			printk(KERN_ERR "ERROR: Cant get pci pmem CRC mem\n");
+		}
+	}
+
+	return pmem_checksum_ptr;
+}
+
+int pmem_arch_get_persistent_memory(void **ptr, __u32 * size)
+{
+	unsigned long pmem_ptr;
+
+	pmem_ptr = (unsigned long)pmem_arch_get_start_ptr();
+	if (!pmem_ptr) {
+		printk(KERN_ERR "ERROR: Cant find PCI pmem segment\n");
+		return -EFAULT;
+	}
+
+	*ptr = (void *)pmem_ptr;
+	*size = pmem_arch_get_size();
+
+#ifdef CONFIG_PMEM_DEBUG
+	PMEM_DPRINT("INFO: raw size = %ld, crc_size=%d, pmem.size=%d\n",
+		    pmem_pci_length, (int)PERSISTENT_CONTROL_SIZE, pmem.size);
+#endif
+	return 0;
+}
+EXPORT_SYMBOL(pmem_arch_get_persistent_memory);
+
+/* Initialize the 9030 device to be used for Pmem */
+static int probe_9030(struct pci_dev *dev, const struct pci_device_id *id)
+{
+	pmem_pci_start = pci_resource_start(dev, PCI_9030_PMEM_RESOURCE);
+	pmem_pci_length =
+	    pci_resource_end(dev, PCI_9030_PMEM_RESOURCE) - pmem_pci_start;
+	pmem_pci_resource_num = PCI_9030_PMEM_RESOURCE;
+
+#ifdef CONFIG_PMEM_DEBUG
+	printk(KERN_INFO
+	       "INFO: Found PCI memory start=%lx, len %ld w/ devid=[%s]\n",
+	       pmem_pci_start, pmem_pci_length, pci_name(dev));
+#endif
+	return 0;
+}
+
+static int probe(struct pci_dev *dev, const struct pci_device_id *id)
+{
+	pci_enable_device(dev);
+
+	pmem_pci_vendor_id = dev->vendor;
+	pmem_pci_device_id = dev->device;
+#ifdef CONFIG_PMEM_DEBUG
+	printk(KERN_INFO "INFO: PCI vendorid=%d, deviceid=%d\n", pmem_pci_vendor_id, pmem_pci_device_id);
+#endif
+	
+	/* pass off init to the function stored in driver_data */
+	if (!id->driver_data) {
+		printk(KERN_ERR
+		       "ERROR: NULL data field in pmem_pci device table\n");
+		return -1;
+	}
+
+	return ((int (*)(struct pci_dev *, const struct pci_device_id *))
+		(id->driver_data)) (dev, id);
+}
+
+static void remove(struct pci_dev *dev)
+{
+	/* clean up any allocated resources and stuff here.
+	 * like call release_region();
+	 */
+}
+
+/* PCI driver boiler plate */
+static struct pci_device_id ids[] = {
+	{PCI_DEVICE(PCI_VENDOR_ID_MOTOROLA, PCI_DEVICE_ID_MOTOROLA_9030),
+	 0, 0, (kernel_ulong_t) & probe_9030},
+	{0,}
+};
+
+MODULE_DEVICE_TABLE(pci, ids);
+
+static struct pci_driver pci_driver = {
+	.name = "pci_pmem",
+	.id_table = ids,
+	.probe = probe,
+	.remove = remove,
+};
+
+static int __init pmem_pci_init(void)
+{
+	return pci_register_driver(&pci_driver);
+}
+
+/* should never be called */
+static void __exit pmem_pci_exit(void)
+{
+	pci_unregister_driver(&pci_driver);
+}
+
+MODULE_LICENSE("GPL");
+
+module_init(pmem_pci_init);
+module_exit(pmem_pci_exit);
+
diff --git a/include/linux/kernel.h b/include/linux/kernel.h
index 9365227..33b4baf 100644
--- a/include/linux/kernel.h
+++ b/include/linux/kernel.h
@@ -297,6 +297,11 @@ extern void printk_tick(void);
 extern void asmlinkage __attribute__((format(printf, 1, 2)))
 	early_printk(const char *fmt, ...);
 
+#ifdef CONFIG_RESET_LOGS
+extern void kcore_logon(int reboot);
+extern void kcore_logoff_quiet(void);
+#endif
+
 unsigned long int_sqrt(unsigned long);
 
 static inline void console_silent(void)
diff --git a/include/linux/module.h b/include/linux/module.h
index 097c89e..1bbf3a6 100644
--- a/include/linux/module.h
+++ b/include/linux/module.h
@@ -540,6 +540,11 @@ extern void print_modules(void);
 extern void module_update_tracepoints(void);
 extern int module_get_iter_tracepoints(struct tracepoint_iter *iter);
 
+#ifdef CONFIG_PANIC_LOGS
+extern void pmem_dump_module_list(void);
+extern void pmem_dump_ksyms(void);
+#endif /* CONFIG_PANIC_LOGS */
+
 #else /* !CONFIG_MODULES... */
 #define EXPORT_SYMBOL(sym)
 #define EXPORT_SYMBOL_GPL(sym)
diff --git a/include/linux/pmem.h b/include/linux/pmem.h
new file mode 100644
index 0000000..4eaa1ab
--- /dev/null
+++ b/include/linux/pmem.h
@@ -0,0 +1,848 @@
+#ifndef PMEM_H
+#define PMEM_H
+
+#include <linux/version.h>
+#include <linux/types.h>
+#include <linux/sched.h>
+#include <asm/page.h>
+#include <asm/byteorder.h>
+
+/*
+ * Persistent Memory
+ */
+ 
+#ifndef CONFIG_PMEM_SIZE
+#define CONFIG_PMEM_SIZE 0
+#endif
+ 
+/* Persistent memory diagnostic wrapper */
+#ifdef CONFIG_PMEM
+#define PERSISTENT_CONTROL_SIZE PAGE_SIZE	/*Reserve a whole page*/
+#define PERSISTENT_CONTROL_BOOTUP		0
+#define PERSISTENT_CONTROL_RESTART		0x11111111
+#define PERSISTENT_CONTROL_HALT			0x22222222
+#define PERSISTENT_CONTROL_PWR_OFF		0x33333333
+#endif/* CONFIG_PMEM */
+
+/********************************************************************
+ *
+ * Persistent memory definitions (includes filesystem layout)
+ *
+ *******************************************************************/
+#define PMEM_VERSION_CURRENT		1
+#define PMEM_ALIGN_BYTES		PAGE_SIZE
+/* Used to align the data to a page */
+#define PMEM_PART_HDR_MAX_SIZE		PAGE_SIZE
+
+#define PMEM_DESC_MAX			32
+#define PMEM_ACTIVE_BLOCK		-1
+#define PMEM_INVALID_BLOCK		-2
+/* At any point in time, each partition must have at least 2 unlocked 
+ * blocks so that locking doesnt interfere with rotation */
+#define PMEM_MIN_UNLOCKED_BLOCKS 	2
+
+/* Control block flags */
+#define PMEM_CB_FLAG_RESET		1
+#define PMEM_CB_NUM_FLAGS		1
+
+/* Partition flags */
+#define PMEM_PART_NUM_FLAGS		0
+
+/* Block flags */
+#define PMEM_BLOCK_FLAG_ACTIVE		1
+#define PMEM_BLOCK_FLAG_LOCK		2
+#define PMEM_BLOCK_NUM_FLAGS		2
+
+/* Region flags */
+#define PMEM_REGION_FLAG_CIRCBUF        0
+#define PMEM_REGION_FLAG_PERPROC	1
+#define PMEM_REGION_FLAG_STOPFULL	2
+#define PMEM_REGION_NUM_FLAGS		2
+
+/* Log partition information */
+#define PMEM_PART_LOG_DESC		"logs"
+#define PMEM_PART_LOG_VERSION		1
+
+/* Log region information */
+#define PMEM_REG_GENERAL_DESC		"general"
+#define PMEM_REG_GENERAL_VERSION	1
+#define PMEM_REG_GENERAL_FIXED_SIZE	0
+#define PMEM_REG_GENERAL_FLAGS		0
+
+/* filesystem path constants */
+#define PMEMFS_PARTITION_PREFIX		"partition"
+#define PMEMFS_SEGMENT_PREFIX		"segment"
+#define PMEMFS_REGION_PREFIX		"region"
+#define PMEMFS_ACTIVE_REGION_DIR	"active"
+#define PMEMFS_RAW_ACCESS_FILE		"raw"
+#define PMEMFS_BACKUP_FILE		"backup"
+#define PMEMFS_HW_INDICATOR_FILE	"using_hardware"
+//#define PMEMFS_REGION_FILE		"region_data"
+#define PMEMFS_RAW_DATA_SUFFIX		"_raw_data"
+#define PMEMFS_DATA_SUFFIX		"_data"
+#define PMEMFS_HEADER_SUFFIX		"_header"
+#define PMEMFS_CONTROL_DIR		"control"
+
+/* control filenames in pmemfs */
+#define CTRL_NEW_PART_FILE		"new_partition"
+#define CTRL_NEW_REGION_FILE		"new_region"
+#define CTRL_LOCK_ACTIVE_SEG_FILE	"lock_active_segments"
+#define CTRL_LOCK_FILE			"lock"
+#define CTRL_ROTATE_FILE		"rotate"
+#define CTRL_RESET_FILE			"reset"
+#define CTRL_ACTIVE_SEGMENT_FILE	"active_segment"
+
+/* Persistent memory architecture definitions */
+#define PMEM_ARCH_PPC32		1
+#define PMEM_ARCH_X86		2
+#define PMEM_ARCH_ARM		3
+#define PMEM_ARCH_PPC64		4
+#define PMEM_ARCH_MIPS64	5
+
+
+/* Persistent memory endianess definitions */
+#if defined (__LITTLE_ENDIAN_BITFIELD)
+#define PMEM_ARCH_LITTLE_ENDIAN	1	
+#elif defined (__BIG_ENDIAN_BITFIELD)
+#define PMEM_ARCH_LITTLE_ENDIAN	0	
+#else
+#error "Unable to determine endianess of target"
+#endif
+
+/* Get architecture value */
+#define PMEM_GET_ARCH(cb_hdr) \
+	(__u8)((__u8)((cb_hdr)->data.arch_info<<1)>>1)
+
+/* Get endianess of pmem header  - 0 = big endian, 1 = little */
+#define PMEM_GET_ENDIANESS(cb_hdr) \
+	(__u8)((cb_hdr)->data.arch_info>>7)
+
+/* combine ARCH and endianess to get the arch_info */
+#define PMEM_ARCH_INFO	((PMEM_ARCH_LITTLE_ENDIAN<<7) + PMEM_ARCH_LOCAL)
+
+/* Allocation types */
+#define PMEM_ALLOC_TYPE_LOG    1
+
+
+/* Log data definitions */
+struct pmem_region_data_hdr
+{
+   __u32 start_offset;        /* Offset for the start of the region */
+   __u32 end_offset;          /* Offset for the end of the region */
+   __u32 current_offset;      /* Offset for the current location */
+   __u32 lost_logs;           /* Number of logs that were tossed */
+} __attribute__ ((aligned (8)));
+
+struct pmem_log_desc_index_data
+{
+   char desc[PMEM_DESC_MAX]; /* Region description */
+   __u16 oldest_index;       /* Index of oldest log in descriptor array */
+   __u16 curr_index;         /* Index of last written log in descriptor array */
+} __attribute__ ((aligned (8)));
+
+struct pmem_log_desc_index
+{
+   struct pmem_log_desc_index_data data;
+   __u32 checksum;
+} __attribute__ ((aligned (8)));
+
+struct pmem_log_desc_data
+{
+   __u64 hrtime;              /* High resolution time stamp of log */
+   __u32 sec;                 /* timeval seconds */
+   __u32 usec;                /* timeval useconds */
+   __u32 size;                /* Size of log data */
+   __u32 offset;              /* Offset of log data */
+   __u32 log_checksum;        /* Checksum of log data */
+} __attribute__ ((aligned (8)));
+
+struct pmem_log_desc
+{
+   struct pmem_log_desc_data data;
+   __u32 checksum;
+} __attribute__ ((aligned (8)));
+
+
+/* Checksum function for use in the kernel and in user space.
+ * - Must be architecture independant
+ * - Since it deals with 32bit chunks, the area should be 32bit aligned
+ */
+static inline __u32 pmem_checksum( __u32 *data, int len )
+{
+	__u32 sum = 0xffff;
+	int i = 0;
+
+	for (i=0; i<len/4; i++) {
+		sum += *((__u32*)data + i);
+	}
+
+	return(sum);
+}
+
+/* Validate the checksum in the data structure and return result */
+#define PMEM_VALIDATE_CHECKSUM(element) \
+   ({ ( pmem_checksum((__u32*)&((element)->data), sizeof((element)->data)) != \
+        (element)->checksum ) ? -1 : 0; })
+
+#define PMEM_VALIDATE_CB_CHECKSUM(cb) \
+   ({ ( pmem_checksum((__u32*)&((cb)->data), sizeof((cb)->data)) != \
+       ntohl((cb)->checksum) ) ? -1 : 0; })
+
+
+/* Driver structures */
+struct pmem_set_part
+{
+    char desc[PMEM_DESC_MAX]; /* Partition string id */
+};
+
+struct pmem_set_block
+{
+    __s8 block_id;            /* Block id */
+};
+
+struct pmem_set_region
+{
+    char desc[PMEM_DESC_MAX]; /* Region string id */
+    __s8 block_id;            /* Block id */
+};
+
+struct pmem_reg_part
+{
+    char desc[PMEM_DESC_MAX]; /* Partition string id */
+    __u32 size;               /* Size of partition data */
+    __u16 num_blocks;         /* Number of blocks */
+    __u8  version;            /* Version of partition */
+};
+
+struct pmem_reg_region
+{
+    char desc[PMEM_DESC_MAX]; /* Region string id */
+    __u32 size;           /* Size of region data */
+    __u32 flags;          /* See below for flags */
+    __u32 fixed_size;     /* Size of fixed size log. 0 for variable size logs */
+    __u16 num_log_desc;   /* Number of variable size log descriptors */
+    __u8  version;        /* Version of region */
+    __s8  block_id;       /* Block id for resulting region handle */
+};
+
+struct pmem_cb_info
+{
+    __u32 size;           /* Size of all of persistent memory */
+    __u16 num_part;       /* Number of partitions registered */
+    __u32 avail_mem;      /* Available memory */
+    __u8  num_cpus;	  /* Number of CPUs supported by pmem */
+};
+
+struct pmem_part_info
+{
+    __u32 size;           /* Size of partition data */
+    __u16 num_blocks;     /* Number of blocks in partition */
+    __u16 num_regions;    /* Number of regions provisioned */
+    __u32 avail_mem;      /* Available memory */
+    __s8  active_block_id; /* Block id of active block */
+};
+
+struct pmem_block_info
+{
+    __u32 size;           /* Size of block data */
+    __u32 sec;            /* timeval seconds when block became active */
+    __u32 usec;           /* timeval useconds when block became active */
+    __u8 active;          /* Activity boolean */
+    __u8 locked;          /* Locked boolean */
+};
+
+struct pmem_region_info
+{
+    __u32 size;           /* Size of region data */
+    __u32 fixed_size;     /* Size of fixed size logs */
+    __u16 num_logs;       /* Number of log descriptors */
+    __u16 num_cpu_areas;  /* Number of cpu areas in the region */
+};
+
+/* Persistent memory header definitions
+ *
+ * NOTE: All headers in persistent memory must be 64-bit aligned 
+ */
+
+struct pmem_cb_hdr_data {
+	__u32 size;		/* Size of all of persistent memory in bytes */
+	__u32 flags;		/* Flags for CB status */
+	__u8 version;		/* Version of the persistent memory data structures */
+	__u8 num_cpus;		/* Total number of cpus in the product */
+	__u8 arch_info;		/* Architecture enum (lower 7 bits) and endianess (upper bit) */
+	__u8 num_alloc;		/* Number of allocation table entries */
+} __attribute__ ((aligned(8)));
+
+struct pmem_cb_hdr {
+	struct pmem_cb_hdr_data data;
+	__u32 checksum;
+} __attribute__ ((aligned(8)));
+
+struct pmem_alloc_desc_data {
+	__u32 offset;		/* Offset from beginning of persistent memory to partition */
+	__u32 size;		/* Size of partition, including header */
+	__u8 type;		/* Type of partition */
+	char desc[PMEM_DESC_MAX];	/* Partition identifier */
+} __attribute__ ((aligned(8)));
+
+struct pmem_alloc_desc {
+	struct pmem_alloc_desc_data data;
+	__u32 checksum;
+} __attribute__ ((aligned(8)));
+
+struct pmem_part_hdr_data {
+	__u32 size;		/* Size of partition data */
+	__u32 flags;		/* Status flags */
+	__u8 num_blocks;	/* Number of blocks in partition */
+	__u8 num_regions;	/* Number of regions in partition */
+	__u8 version;		/* Version for partition */
+	char desc[PMEM_DESC_MAX];	/* String description of partition */
+} __attribute__ ((aligned(8)));
+
+struct pmem_part_hdr {
+	struct pmem_part_hdr_data data;
+	__u32 checksum;
+} __attribute__ ((aligned(8)));
+
+struct pmem_block_hdr_data {
+	__u32 offset;		/* Offset of block from beginning of partition header */
+	__u32 size;		/* Size of the block data */
+	__u32 sec;		/* timeval seconds of when block became active */
+	__u32 usec;		/* timeval useconds of when block became active */
+	__u32 flags;		/* Status flags */
+} __attribute__ ((aligned(8)));
+
+struct pmem_block_hdr {
+	struct pmem_block_hdr_data data;
+	__u32 checksum;
+} __attribute__ ((aligned(8)));
+
+struct pmem_region_hdr_data {
+	__u32 size;		/* Size of the region data */
+	__u32 offset;		/* Offset of region from beginning of block data */
+	__u32 flags;		/* Behaviour flags */
+	__u32 fixed_size;	/* Size of fixed logs */
+	__u16 num_log_desc;	/* Number of log descriptors */
+	__u8 version;		/* Version for region */
+	char desc[PMEM_DESC_MAX];	/* String description of region */
+} __attribute__ ((aligned(8)));
+
+struct pmem_region_hdr {
+	struct pmem_region_hdr_data data;
+	__u32 checksum;
+} __attribute__ ((aligned(8)));
+
+/********************************************************************
+ *
+ * Kernel exclusive definitions start here
+ *
+ *******************************************************************/
+
+#ifdef __KERNEL__
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <asm/io.h>
+
+#ifdef CONFIG_PMEM_DEBUG
+#define PMEM_DPRINT(...) do{printk("%s:%d, ", __FILE__, __LINE__); printk(__VA_ARGS__);} while(0)
+#else
+#define PMEM_DPRINT(...)
+#endif
+
+#ifdef CONFIG_LHB
+extern volatile int disable_history_buffer;
+#if defined(CONFIG_PPC)
+/* exception history buffer */
+typedef struct hist_buf_struct
+{
+        unsigned int    hbnip;  /* NIP */
+        unsigned int    hblink; /* LINK */
+        unsigned short  hbtrap; /* TRAP */
+        unsigned short  hbsc;   /* last system call */
+        unsigned int    hbtl;   /* Low order Timebase */
+} hist_buf_struct;
+#elif defined(CONFIG_X86)
+/* 'TrapNo' numbers used for LHB events that dont run through do_trap
+ * Start these at 32 since it appears that all standard signal numbers
+ * are < 31 */
+#define LHB_SYSCALL             32
+#define LHB_VSYSCALL            LHB_SYSCALL
+#define LHB_PAGEFAULT           33
+#define LHB_DEBUG_EXCEPT        34
+#define LHB_NMI_EXCEPT          35
+#define LHB_COPROC_ERROR        36
+#define LHB_DOUBLEFAULT         37
+
+#define LHB_IRQ                 40
+#define LHB_RESTART             99
+/* exception history buffer structure
+ * Designed to hold less, detailed exception info
+ * vs more sparse details. */
+typedef struct hist_buf_struct
+{
+        int trapno;             /* trap type */
+        int signo;              /* signal */
+        int errcode;            /* error code passed to do_trap() */
+        /* potential x86 registers of interest */
+        long eax;
+        long ebx;
+        long ecx;
+        long edx;
+        long esi;
+        long edi;
+        long ebp;
+        long esp;
+        long eip;
+        unsigned int sc;        /* system call id */
+        int irq;                /* irq number */
+
+        __u64 timestamp;        /* High-res timestamp */
+} hist_buf_struct;
+#endif
+
+/* Persistent memory definitions */
+#define PMEM_REG_EXCEPT_DESC "except_hist"
+#define PMEM_REG_EXCEPT_FIXED_SIZE sizeof(struct hist_buf_struct)
+#define PMEM_REG_EXCEPT_LOG_DESC 0
+#define PMEM_REG_EXCEPT_FLAGS PMEM_REGION_FLAG_PERPROC
+#define PMEM_REG_EXCEPT_VERSION 1
+#endif /* CONFIG_LHB */
+
+#ifdef CONFIG_LHB_TESTMODE
+extern int lhb_mask;
+/* define the mask bits for event logging selection.  This allows the
+ * common events to be masked out for testing purposes */
+#define LHB_MASK_PAGEFAULT      4
+#define LHB_MASK_IRQ            2
+#define LHB_MASK_SYSCALL        1
+#endif /* CONFIG_LHB_TESTMODE */
+
+#ifndef CONFIG_SMP
+#define PMEM_NUM_SUPPORT_CPUS 1
+#else
+/* CONFIG_NR_CPUS must be set to the number of CPUs visible to the system.
+ * Failure to set this properly may cause crashes or wasted memory in percpu
+ * regions */
+#define PMEM_NUM_SUPPORT_CPUS CONFIG_NR_CPUS
+#endif
+
+#if defined(CONFIG_PPC32)
+#define PMEM_ARCH_LOCAL  PMEM_ARCH_PPC32
+#elif defined(CONFIG_X86)
+#define PMEM_ARCH_LOCAL  PMEM_ARCH_X86
+#elif defined(CONFIG_ARM)
+#define PMEM_ARCH_LOCAL  PMEM_ARCH_ARM
+#elif defined(CONFIG_CPU_MIPS64)
+#define PMEM_ARCH_LOCAL  PMEM_ARCH_MIPS64
+#elif defined(CONFIG_PPC64)
+#define PMEM_ARCH_LOCAL  PMEM_ARCH_PPC64
+#else
+#error "Unknown host architecture"
+#endif
+
+/* Macros */
+
+/* Get header structures */
+#define PMEM_GET_ALLOC_DESC(index) \
+   (struct pmem_alloc_desc*)((void*)pmem.pmem + sizeof(struct pmem_cb_hdr)) + index
+
+#define PMEM_GET_PART_HDR(alloc_desc) \
+   (struct pmem_part_hdr*)((void*)pmem.pmem + (alloc_desc)->data.offset)
+
+#define PMEM_GET_BLOCK_HDR(part_hdr, index) \
+   ((index) < (part_hdr)->data.num_blocks ? \
+   ((struct pmem_block_hdr*)((void*)(part_hdr) + \
+   sizeof(struct pmem_part_hdr)) + (index)) : NULL)
+
+#define PMEM_GET_REGION_HDR(part_hdr, index) \
+   ((index) < (part_hdr)->data.num_regions ? \
+   (struct pmem_region_hdr*)((void*)(part_hdr) + \
+   sizeof(struct pmem_part_hdr) + (part_hdr)->data.num_blocks * \
+   sizeof(struct pmem_block_hdr)) + (index) : NULL)
+
+/* Update the checksum in the data structure */
+#define PMEM_UPDATE_CHECKSUM(element) \
+   do { (element)->checksum = pmem_checksum((__u32*)&(element)->data, \
+        sizeof((element)->data)); } while (0)
+
+#define PMEM_UPDATE_CB_CHECKSUM(cb) \
+   do { (cb)->checksum = htonl( pmem_checksum((__u32*)&(cb)->data, \
+        sizeof((cb)->data)) ); } while (0)
+
+/* This macro checks if the offsets in the desired element overlap with any other
+ * elements in the array */
+#define PMEM_CHECK_FOR_OVERLAP(array_ptr, element_index, num_elements) \
+({  \
+   typeof (element_index) index_ = 0; \
+   \
+   while ( index_ < (num_elements) ) { \
+   \
+      if ( (index_ == (element_index)) || ((array_ptr)[index_].data.size == 0)) {\
+         index_++; \
+         continue; \
+      } \
+   \
+      if ( pmem_is_overlapping ( (array_ptr)[(element_index)].data.offset, \
+	                        (array_ptr)[(element_index)].data.size, \
+                            (array_ptr)[index_].data.offset, \
+							(array_ptr)[index_].data.size) ) {\
+         break; \
+      } \
+   \
+      index_++; \
+   } \
+   \
+   (index_ < (num_elements)) ? -1 : 0; \
+})
+
+/* On some architectures it may be necessary to force the system to
+ * write the data to main memory. Otherwise, if the system goes down
+ * before the cache is cleared, only some of the data will have been
+ * written.
+ */
+#ifdef CONFIG_PPC32
+static inline void pmem_arch_flush_cache(void *ptr, int size)
+{
+	void *tmp_ptr = ptr;
+	int tmp_size = size;
+	do {
+		asm volatile("dcbf 0,%0" : : "r" (tmp_ptr) : "memory");
+		tmp_ptr += 32;
+		tmp_size -= 32;
+	} while (tmp_size > 0);
+}
+#define PMEM_FLUSH_CACHE(ptr, size) pmem_arch_flush_cache((ptr),(size))
+#else
+#define PMEM_FLUSH_CACHE(ptr, size)
+#endif
+
+
+/* Dynamic structure definitions */
+struct pmem_cb_data {
+	struct pmem_cb_hdr *pmem;	/* Pointer to beginning of persistent memory */
+	__u32 size;			/* Size of all of persistent memory */
+	spinlock_t lock;		/* Lock for header access */
+	struct list_head part_list;	/* Pointer to linked list of partition data */
+	void *shadow;			/* Pointer to shadow copy of persistent memory */
+	__u8 enabled;			/* Indicates if persistent memory api is enabled */
+	__u8 using_hardware;		/* Indicates if pmem is using hardware */
+	__u8 using_io_mem;		/* Does pmem need to use the _io() family of functions to read/write to the hardware */
+};
+
+struct part_list_elem {
+	struct list_head list_elem;	/* Partition linked list element */
+	struct pmem_part_hdr *hdr;	/* Pointer to partition header in pmem */
+	__u32 active_block_offset;	/* Offset of active block in pmem */
+	struct list_head region_list;	/* Linked list of regions in partition */
+};
+
+struct region_list_elem {
+	struct list_head list_elem;	/* Region linked list element */
+	struct pmem_region_hdr *hdr;	/* Pointer to region header in pmem */
+	struct pmem_ptr_block *ptr_block;/* Registered pointer block */
+	spinlock_t data_lock;		/* Lock for data access */
+};
+
+/* Handle definitions */
+#define PMEM_HANDLE_TYPE_CB      1
+#define PMEM_HANDLE_TYPE_PART    2
+#define PMEM_HANDLE_TYPE_BLK     3
+#define PMEM_HANDLE_TYPE_REG     4
+#define PMEM_HANDLE_TYPE_SHADOW  5
+
+struct pmem_handle {
+	int type;
+	int offset;
+	int size;
+};
+
+struct part_handle {
+	struct pmem_handle hdl;
+	struct part_list_elem *elem;
+};
+
+struct block_handle {
+	struct pmem_handle hdl;
+	struct part_handle *parent;
+	int block_id;
+};
+
+struct region_handle {
+	struct pmem_handle hdl;
+	struct part_handle *parent;
+	struct region_list_elem *elem;
+	int block_id;
+};
+
+struct cb_handle {
+	struct pmem_handle hdl;
+};
+
+struct shadow_handle {
+	struct pmem_handle hdl;
+};
+
+/* Kernel API data structures */
+
+typedef void* pmem_handle_t;
+
+struct pmem_cpu_ptrs
+{
+   char *start;
+   char *end;
+   char *curr;
+   __u32 *lost_logs;
+   spinlock_t lock;
+};
+
+struct pmem_ptr_block_per_cpu
+{
+   struct pmem_cpu_ptrs *ptrs;
+   struct pmem_cpu_ptrs storeA;
+   struct pmem_cpu_ptrs storeB;
+};
+
+struct pmem_ptr_block
+{
+   struct pmem_ptr_block_per_cpu cpu[PMEM_NUM_SUPPORT_CPUS];
+};
+
+/* event callbacks - function ptrs that are executed on pmem events
+ * Used to keep the pmemfs VFS tree in sync with changes made by kernel
+ * space code */
+struct pmem_event_block {
+	void (*create_partition)(pmem_handle_t part_hdl);
+	void (*create_region)(pmem_handle_t part_hdl, pmem_handle_t region_hdl);
+};
+
+/* Global variables */
+extern struct pmem_cb_data pmem;
+extern struct pmem_event_block pmem_events;
+
+/* PMEM specific memory manipulation functions that know whether to use
+ * the commands for ioremapped memory or use the standard memcpy() family */
+/* memset() memcpy_fromio() memcpy_toio() */
+
+static inline void __pmem_memset(void *s, int c, size_t count)
+{
+	if (pmem.using_io_mem)
+		memset_io(s, c, count);
+	else
+		memset(s, c, count);
+}
+
+static inline void pmem_memset(void *s, int c, size_t count)
+{
+	int n, num_blocks, last_block_size;
+
+	num_blocks = count / PAGE_SIZE;
+	last_block_size = count % PAGE_SIZE;
+
+	for (n = 0; n < num_blocks; n++) {
+		__pmem_memset(s + (PAGE_SIZE * n), c, PAGE_SIZE);
+		schedule();
+	}
+
+	__pmem_memset(s + (PAGE_SIZE * n), c, last_block_size);
+}
+
+static inline void __pmem_memcpy_fromio(void *dst, const void *src, int count)
+{
+	if (pmem.using_io_mem)
+		memcpy_fromio(dst, src, count);
+	else
+		memcpy(dst, src, count);
+}
+
+static inline void pmem_memcpy_fromio(void *dst, const void *src, int count)
+{
+	int n, num_blocks, last_block_size;
+
+	num_blocks = count / PAGE_SIZE;
+	last_block_size = count % PAGE_SIZE;
+
+	for (n = 0; n < num_blocks; n++) {
+		__pmem_memcpy_fromio(dst + (PAGE_SIZE * n), src + (PAGE_SIZE * n), PAGE_SIZE);
+		schedule();
+	}
+
+	__pmem_memcpy_fromio(dst + (PAGE_SIZE * n), src + (PAGE_SIZE * n), last_block_size);
+}
+
+static inline void __pmem_memcpy_toio(void *dst, const void *src, int count)
+{
+	if (pmem.using_io_mem)
+		memcpy_toio(dst, src, count);
+	else
+		memcpy(dst, src, count);
+}
+
+static inline void pmem_memcpy_toio(void *dst, const void *src, int count)
+{
+	int n, num_blocks, last_block_size;
+
+	num_blocks = count / PAGE_SIZE;
+	last_block_size = count % PAGE_SIZE;
+
+	for (n = 0; n < num_blocks; n++) {
+		__pmem_memcpy_toio(dst + (PAGE_SIZE * n), src + (PAGE_SIZE * n), PAGE_SIZE);
+		schedule();
+	}
+
+	__pmem_memcpy_toio(dst + (PAGE_SIZE * n), src + (PAGE_SIZE * n), last_block_size);
+
+}
+
+/* Arch specific functions.  Implement each of these to add pmem support
+ * for a new target
+ */
+#ifdef CONFIG_BOARD_HAS_PMEM_HARDWARE
+/* Architecture defined function to retreive information about where
+ * persistent memory is */
+extern int pmem_arch_get_persistent_memory(void **ptr, __u32 *size);
+/* Architecture defined function to setup cache information */
+extern unsigned long pmem_arch_pgprot_noncached(unsigned long prot);
+/* Architecture defined routine that retuns the size of the pmem segment
+ * This is used by persistent_mem_crc */
+extern int pmem_arch_get_size(void);
+
+/* Architecture specific pointers used to get the base pmem pointer
+ * and in the CRC check.  There is no cleanup function for the returned
+ * pointer and they may be called multiple times - so it is recommended 
+ * that they are implemented by caching the value of the pointers inside 
+ * the hardware specific implementation. Not all functions are 
+ * implemented for all architectures. */
+extern void *pmem_arch_get_start_ptr(void);
+extern void *pmem_arch_get_phys_start_ptr(void);
+extern void *pmem_arch_get_raw_start_ptr(void);
+extern void *pmem_arch_get_pci_start_ptr(void);
+extern void *pmem_arch_get_checksum_start_ptr(void);
+extern int pmem_arch_get_pci_resource_num(void);
+extern struct pci_dev *pmem_arch_get_pci_dev(struct pci_dev *dev);
+
+/* Are the returned pmem pointers created with ioremap() ?  This is 
+ * imporatant for the proper memory access functions to be used. */
+extern int pmem_arch_uses_ioremap(void);
+
+#endif /* CONFIG_BOARD_HAS_PMEM_HARDWARE */
+
+
+
+/* API Function declarations */
+/* Registration functions */
+
+/* Register a partition. Updates handle with a partition handle if successful */
+extern int pmem_partition_reg(struct pmem_reg_part *part,
+                pmem_handle_t *handle);
+/* Register a region.Updates handle with a region handle if successful */
+extern int pmem_region_reg(pmem_handle_t part_handle,
+                struct pmem_reg_region *region, pmem_handle_t *handle);
+/* Register a pointer block with a region. */
+extern int pmem_register_ptr_block(pmem_handle_t region_handle,
+                struct pmem_ptr_block *ptr_block);
+
+/* Handle functions */
+
+/* Get a control block handle */
+extern int pmem_control_block_get(pmem_handle_t *handle);
+/* Get a partition handle */
+extern int pmem_partition_get(char *desc, pmem_handle_t *handle);
+/* Get a block handle */
+extern int pmem_block_get(pmem_handle_t part_handle, __s8 block_id,
+                pmem_handle_t *handle);
+/* Get a region handle */
+extern int pmem_region_get(pmem_handle_t part_handle,
+                char *desc, __s8 block_id, pmem_handle_t *handle);
+/* Get a shadow handle */
+extern int pmem_shadow_get(pmem_handle_t *handle);
+/* Release a handle - free its storage memory */
+extern void pmem_release_handle(pmem_handle_t *handle);
+/* Get the parent handle */
+extern pmem_handle_t pmem_get_handle_parent(pmem_handle_t handle);
+
+/* IO functions */
+
+/* Write to persistent memory from a kernel space buffer */
+extern int pmem_write_data(pmem_handle_t handle, const char *buffer, int size);
+/* Write to a handle's header area from a kernel space buffer */
+extern int pmem_write_header(pmem_handle_t handle, const char *buffer,
+                int size, loff_t offset);
+/* Read from persistent memory data segment into kernel space buffer */
+extern int pmem_read_data(pmem_handle_t handle, char *buffer, int size);
+/* Read from a handles header area for a kernel space buffer */
+extern int pmem_read_header(pmem_handle_t handle, char *buffer, int size,
+                loff_t offset);
+/* Adjust handle current offset using fseek() semantics */
+extern int pmem_seek(pmem_handle_t handle, int offset, int whence);
+/* Commands */
+
+/* Set the reset flag */
+extern void pmem_set_cb_reset_flag(void);
+/* Clear the reset flag */
+extern void pmem_clear_cb_reset_flag(void);
+/* Rotate blocks in a partition */
+extern int pmem_part_rotate(pmem_handle_t handle, int lock);
+/* Lock a block for rotation */
+extern int pmem_lock_block(pmem_handle_t handle);
+/* Unlock a block for rotation */
+extern int pmem_unlock_block(pmem_handle_t handle);
+/* Clear the shadow copy of persistent memory */
+extern int pmem_clear_shadow(void);
+/* Request control block information */
+extern int pmem_get_cb_info(pmem_handle_t handle, struct pmem_cb_info *info);
+/* Request partition information */
+extern int pmem_get_part_info(pmem_handle_t handle,
+		struct pmem_part_info *info);
+/* Request block information */
+extern int pmem_get_block_info(pmem_handle_t handle,
+		struct pmem_block_info *info);
+/* Request region information */
+extern int pmem_get_region_info(pmem_handle_t handle,
+		struct pmem_region_info *info);
+/* Disable all pointer blocks in a partition */
+extern int pmem_disable_ptr_blocks(pmem_handle_t part_handle);
+/* Destroy the registered pointer block for a given region */
+extern int pmem_destroy_ptr_block(pmem_handle_t reg_handle);
+/* Lock an entire partition */
+extern int pmem_lock_part(pmem_handle_t part_handle);
+/* Unlock a partition */
+extern int pmem_unlock_part(pmem_handle_t part_handle);
+/* Lock all of pmem  */
+extern int pmem_lock(void);
+/* Unlock all of pmem */
+extern int pmem_unlock(void);
+/* Lock the active segments in all paritions - lock all active segments */
+extern int pmem_lock_all_active_segments(void);
+/* Unlock the active segments in all paritions - unlock all active segments */
+extern int pmem_unlock_all_active_segments(void);
+/* Rotate the active segment on all partitions in pmem */
+extern int pmem_rotate(int lock);
+/* Clear a region back to its original state.  
+ * Intended for use by the vMC module */
+extern int pmem_clear_region(pmem_handle_t region_handle);
+
+/* Misc helper routines */
+/* get the size of a handles header */
+extern ssize_t pmem_get_hdr_size(pmem_handle_t handle);
+/* Get the block index of the active block */
+extern int pmem_get_active_block_index(struct pmem_part_hdr *part_hdr);
+
+extern void* pmem_get_data_ptr(struct pmem_handle *hdl);
+extern void* pmem_get_hdr_ptr(struct pmem_handle *hdl);
+
+extern int pmem_rotate_block_data(struct part_list_elem *part_elem, int lock);
+extern void pmem_update_ptr_block(struct part_list_elem *part_elem,
+                                  struct region_list_elem *region_elem,
+                                  int lock);
+extern int pmem_is_overlapping(__u32 offset_a, __u32 size_a, __u32 offset_b,
+		               __u32 size_b);
+extern void pmem_prepare_region(struct pmem_part_hdr *part_hdr,
+                __u32 block_offset, struct pmem_region_hdr *region_hdr);
+
+/* Default notification callback for partitions/regions - do nothing */
+extern void pmem_default_create_partition(pmem_handle_t handle);
+extern void pmem_default_create_region(pmem_handle_t parent, pmem_handle_t region);
+
+#endif /* __KERNEL__ */
+
+#endif
diff --git a/include/linux/schedhist.h b/include/linux/schedhist.h
new file mode 100644
index 0000000..9e3a0fc
--- /dev/null
+++ b/include/linux/schedhist.h
@@ -0,0 +1,95 @@
+#ifndef _SCHED_HIST_H
+#define _SCHED_HIST_H
+
+#include <linux/types.h>
+
+/* 
+ * scheduler process history stuff - userspace section
+ */
+
+#define PROC_NAME_SIZE 7            /* length of proc names to store */
+
+typedef __u64 __attribute__((aligned(8))) aligned__u64;
+
+/* For 32-bit userspace, and 64-bit kernels, we have to store the
+ * the timestamp data in something the userspace can understand.
+ * Always use a fixed size timestamp in stored sched_hist_entries
+ *  - 64-bit tv_sec and a 32-bit tv_usec
+ */
+typedef struct sched_timeval {
+	aligned__u64 tv_sec;
+	__u32 tv_usec;
+} sched_timeval __attribute__((aligned(8)));
+
+/* Note:
+ * if pid2 == -1, this is an event from a process
+ * being added to the runqueue, otherwise its a scheduler
+ * event.
+ * These fields are sized according to architecture
+ */
+#define SCHED_HIST_ENTRY_COMMON \
+	struct sched_timeval  tv;        /* normal timestamp */ \
+	aligned__u64          hrts;    /* high res timestamp */ \
+	__s32                 pid1;    /* duh */ \
+	aligned__u64          nip1;    /* next instruction pointer */ \
+	char                  pname1[PROC_NAME_SIZE + 1]; /* process name */ \
+	__s32                 pid2;    /* duh */ \
+	aligned__u64          nip2;    /* next instruction pointer */ \
+	char                  pname2[PROC_NAME_SIZE + 1]; /* process name */
+
+/* Define this to make it easier to work with the common part of
+ * the structure. This is what is stored in the scheduler history buffer
+ */
+typedef struct sched_hist_entry_common
+{
+	SCHED_HIST_ENTRY_COMMON
+} sched_hist_entry_common __attribute__((aligned(8)));
+
+/* User-land schedule history entry. */
+typedef struct sched_hist_entry
+{
+        SCHED_HIST_ENTRY_COMMON
+        int cpu;                    /* CPU doing the scheduling. */
+} sched_hist_entry __attribute__((aligned(8)));
+
+/* 
+ * scheduler process history stuff - kernel section 
+ * userspace applications wont know about these definitions
+ */
+#ifdef __KERNEL__
+
+#if 0
+#define SCHED_HIST_ENTRIES 6000     /* number of entries */
+
+/* For SMP support, we increase the scheduler history buffer to 12000
+ * entries, and then divide by the number of CPUs. In otherwords, each
+ * CPU will have 12000 / smp_num_cpus entries to work with.
+ */
+#define SCHED_HIST_NUMENTRIES(numCpus) \
+	(numCpus > 1 ? SCHED_HIST_ENTRIES * 2 : SCHED_HIST_ENTRIES)
+#define SCHED_HIST_ENTRIESPERCPU(numCpus) \
+	(SCHED_HIST_NUMENTRIES(numCpus) / numCpus);
+
+#endif
+
+/* The kernel doesn't need to track which CPU is scheduling, but it
+ * on SMP systems it does need to track whether the entry is being
+ * updated. 
+ */
+typedef struct sched_hist_kernentry
+{
+	SCHED_HIST_ENTRY_COMMON
+#ifdef CONFIG_SMP
+	int update;                 /* This entry is being updated. */
+#endif
+} sched_hist_kernentry __attribute__((aligned(8)));
+
+#endif /* __KERNEL__ */
+/* Persistent memory definitions */
+#define PMEM_REG_SCHED_DESC "sched_hist"
+#define PMEM_REG_SCHED_FIXED_SIZE sizeof(struct sched_hist_entry_common)
+#define PMEM_REG_SCHED_LOG_DESC 0
+#define PMEM_REG_SCHED_FLAGS PMEM_REGION_FLAG_PERPROC
+#define PMEM_REG_SCHED_VERSION 1
+
+#endif /* _SCHED_HIST_H */
diff --git a/include/linux/sysctl.h b/include/linux/sysctl.h
index 1535d7d..3f40db0 100644
--- a/include/linux/sysctl.h
+++ b/include/linux/sysctl.h
@@ -156,6 +156,7 @@ enum
 	KERN_NMI_TOGGLE=77, /* int: NMI on/off */
 	KERN_NMI_WATCHDOG_THRESH=78, /* int: nmi watchdog timedout threshold */
 	KERN_HRT_KHZ=79, /* int: high res time frequency in khz */
+	KERN_DISABLE_HISTORY_BUFFER=80, /* int: disable the logging to exception and scheduler history buffers */
 };
 
 
diff --git a/init/Kconfig b/init/Kconfig
index d62531a..183eb08 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -746,6 +746,42 @@ config CC_OPTIMIZE_FOR_SIZE
 
 	  If unsure, say Y.
 
+config SCHED_HIST_BUF
+	bool "Scheduler history buffer support"
+	depends on PMEM
+	default n
+	help
+	  When enabled this keeps a history buffer of scheduler events.
+	  Events are logged into the persistent memory framework and
+	  can be read throught the pmem interface.
+
+config SCHED_HIST_SIZE
+	int "Scheduler history buffer size"
+	depends on SCHED_HIST_BUF
+	default 262144
+
+config PANIC_LOGS
+	bool "Panic logging support"
+	depends on PMEM
+	default n
+	help
+	  When enabled this keeps a history buffer of panic events.
+
+config PANIC_LOGS_SIZE
+	int "Panic log size"
+	depends on PANIC_LOGS
+	default 262144
+	help
+	  Specifies the size (in bytes) of the panic history buffer in
+	  persistent memory.
+
+config RESET_LOGS 
+        bool "Reset logging support"
+	depends on PANIC_LOGS&&PMEM
+        default n
+        help
+          When enabled this logs reset info to kcore.
+
 config SYSCTL
 	bool
 
@@ -1198,6 +1234,8 @@ config BASE_SMALL
 	default 0 if BASE_FULL
 	default 1 if !BASE_FULL
 
+source mm/pmem/Kconfig
+
 menuconfig MODULES
 	bool "Enable loadable module support"
 	help
diff --git a/kernel/module.c b/kernel/module.c
index 496d793..40f4650 100644
--- a/kernel/module.c
+++ b/kernel/module.c
@@ -3051,6 +3051,91 @@ void module_layout(struct module *mod,
 EXPORT_SYMBOL(module_layout);
 #endif
 
+#ifdef CONFIG_PANIC_LOGS
+
+#include <linux/pmem.h>
+extern pmem_handle_t kcore_reg_hdl;    /* instantiated in printk.c  */
+
+#define MODLIST_STRING_SIZE	4096
+#define KSYMS_STRING_SIZE	8192
+/*  A buffer used to hold the string representation of print_modules()
+ *  in order to dump it to pmem.  Allocated here as a static array 
+ *  so that we dont have to rely on kmalloc when called from kcore_logoff - 
+ *  a panic/oops may have happened. */
+static char modtextbuf[MODLIST_STRING_SIZE];
+/* Similar buffer for module symbols */
+static char ksymstextbuf[KSYMS_STRING_SIZE+1];
+
+
+/*
+ * Same function as module.c void print_modules(void) EXCEPT that
+ * redirection of printk is used
+ */
+void pmem_dump_module_list(void)
+{
+	struct module *mod;
+	unsigned int pos = 0;
+
+	if (!kcore_reg_hdl)
+		return;
+
+	pos += snprintf(modtextbuf+pos, MODLIST_STRING_SIZE-pos, "\n");
+	list_for_each_entry(mod, &modules, list)
+		pos += snprintf(modtextbuf+pos, MODLIST_STRING_SIZE-pos, "%s ", mod->name);
+	pos += snprintf(modtextbuf+pos, MODLIST_STRING_SIZE-pos-1, "\n");
+
+	pmem_write_data(kcore_reg_hdl, modtextbuf, pos);
+}
+
+void pmem_dump_ksyms(void)
+{
+	struct module *mod;
+	const struct kernel_symbol *sym;
+	unsigned int len;
+	unsigned int i;
+	
+	if (!kcore_reg_hdl)
+		return;
+
+	pmem_write_data(kcore_reg_hdl, "\n", 1);
+
+	list_for_each_entry(mod, &modules, list) {
+		for (i = 0; i < mod->num_syms; i++) {
+			sym = &mod->syms[i];
+			if (mod->name)
+				len = snprintf(ksymstextbuf, KSYMS_STRING_SIZE,
+						"%0*lx %s\t[%s]\n",
+						(int)(2*sizeof(void*)),
+						sym->value, sym->name,
+						mod->name);
+			else
+				len = snprintf(ksymstextbuf, KSYMS_STRING_SIZE,
+						"%0*lx %s\n",
+						(int)(2*sizeof(void*)),
+						sym->value, sym->name);
+			pmem_write_data(kcore_reg_hdl, ksymstextbuf, len);
+		}
+		/* and gpl symbols as well */
+		for (i = 0; i < mod->num_gpl_syms; i++) {
+			sym = &mod->gpl_syms[i];
+			if (mod->name)
+				len = snprintf(ksymstextbuf, KSYMS_STRING_SIZE,
+						"%0*lx %s\t[%s]\n",
+						(int)(2*sizeof(void*)),
+						sym->value, sym->name,
+						mod->name);
+			else
+				len = snprintf(ksymstextbuf, KSYMS_STRING_SIZE,
+						"%0*lx %s\n",
+						(int)(2*sizeof(void*)),
+						sym->value, sym->name);
+			pmem_write_data(kcore_reg_hdl, ksymstextbuf, len);
+		}
+	}
+}
+
+#endif /* CONFIG_PANIC_LOGS */
+
 #ifdef CONFIG_TRACEPOINTS
 void module_update_tracepoints(void)
 {
diff --git a/kernel/panic.c b/kernel/panic.c
index 13d966b..da6f1f9 100644
--- a/kernel/panic.c
+++ b/kernel/panic.c
@@ -30,6 +30,13 @@ static int pause_on_oops;
 static int pause_on_oops_flag;
 static DEFINE_SPINLOCK(pause_on_oops_lock);
 
+#ifdef CONFIG_PANIC_LOGS
+extern void kcore_logon(int reboot);
+extern void kcore_logoff(void);
+#else
+#define kcore_logon(reboot) do { } while(0)
+#define kcore_logoff() do { } while(0)
+#endif /* CONFIG_PANIC_LOGS */
 int panic_timeout;
 
 ATOMIC_NOTIFIER_HEAD(panic_notifier_list);
@@ -91,6 +98,7 @@ NORET_TYPE void panic(const char * fmt, ...)
 	va_start(args, fmt);
 	vsnprintf(buf, sizeof(buf), fmt, args);
 	va_end(args);
+	kcore_logon(1);
 	printk(KERN_EMERG "Kernel panic - not syncing: %s\n",buf);
 #ifdef CONFIG_DEBUG_BUGVERBOSE
 	dump_stack();
@@ -123,6 +131,7 @@ NORET_TYPE void panic(const char * fmt, ...)
 		 */
 		printk(KERN_EMERG "Rebooting in %d seconds..", panic_timeout);
 
+		kcore_logoff();
 		for (i = 0; i < panic_timeout; i++) {
 			touch_nmi_watchdog();
 			panic_blink_one_second();
@@ -150,6 +159,7 @@ NORET_TYPE void panic(const char * fmt, ...)
 		disabled_wait(caller);
 	}
 #endif
+	kcore_logoff();
 	local_irq_enable();
 	while (1) {
 		touch_softlockup_watchdog();
diff --git a/kernel/printk.c b/kernel/printk.c
index 444b770..4c912cf 100644
--- a/kernel/printk.c
+++ b/kernel/printk.c
@@ -107,6 +107,68 @@ static DEFINE_SPINLOCK(logbuf_lock);
 #define LOG_BUF_MASK (log_buf_len-1)
 #define LOG_BUF(idx) (log_buf[(idx) & LOG_BUF_MASK])
 
+#if defined (CONFIG_PANIC_LOGS) || defined (CONFIG_PMEM_LOG_REG) || defined (CONFIG_RESET_LOGS)
+#include <linux/pmem.h>
+#include <linux/time.h>
+
+pmem_handle_t kcore_part_hdl = 0;
+pmem_handle_t kcore_reg_hdl = 0;
+atomic_t log_printks = ATOMIC_INIT(0);
+
+void kcore_logon(int reboot)
+{
+	if (!kcore_reg_hdl)
+		return;
+
+	/*
+	if (atomic_read(&log_printks))
+		return;
+		*/
+
+	if (reboot)
+		pmem_disable_ptr_blocks(kcore_part_hdl);
+
+	atomic_inc(&log_printks);
+	printk(KERN_INFO "Kcore timestamp : %lu.%06lu\n", xtime.tv_sec, (xtime.tv_nsec/1000));
+#ifdef	SUPPORT_HRTIME
+	printk(KERN_INFO "Kcore HighResolution timestamp : %LX\n", (__u64)gethrtime());
+#else
+	printk(KERN_INFO "Kcore HighResolution timestamp : %LX\n", (__u64)get_cycles());
+#endif
+}
+
+void kcore_logoff(void)
+{
+	if (!kcore_reg_hdl)
+		return;
+
+	if (!atomic_dec_and_test(&log_printks))
+		return;
+
+#ifdef CONFIG_PANIC_LOGS
+	pmem_dump_module_list();
+	pmem_dump_ksyms();
+#endif /* CONFIG_PANIC_LOGS */
+}
+
+void kcore_logoff_quiet(void)
+{
+	if (!kcore_reg_hdl)
+		return;
+
+	if (!atomic_dec_and_test(&log_printks))
+		return;
+}
+
+EXPORT_SYMBOL(kcore_logon);
+EXPORT_SYMBOL(kcore_logoff_quiet);
+#endif
+
+#ifdef CONFIG_PMEM_LOG_REG
+pmem_handle_t general_part_hdl = 0;
+pmem_handle_t general_reg_hdl = 0;
+#endif
+
 /*
  * The indices into log_buf are not constrained to log_buf_len - they
  * must be masked before subscripting
@@ -474,6 +536,29 @@ static void _call_console_drivers(unsigned start,
 			__call_console_drivers(start, end);
 		}
 	}
+#if defined (CONFIG_PANIC_LOGS) || defined (CONFIG_RESET_LOGS)
+	if ((start != end) && atomic_read(&log_printks)) {/*Dont care about log level*/
+		if ((start & LOG_BUF_MASK) > (end & LOG_BUF_MASK)) {
+			/* wrapped write */
+			pmem_write_data(kcore_reg_hdl, &LOG_BUF(start & LOG_BUF_MASK), __LOG_BUF_LEN-(start & LOG_BUF_MASK));
+			pmem_write_data(kcore_reg_hdl, &LOG_BUF(0), (end & LOG_BUF_MASK) -0);
+		} else {
+			pmem_write_data(kcore_reg_hdl, &LOG_BUF(start), end-start);
+		}
+	}
+#endif
+
+#ifdef CONFIG_PMEM_LOG_REG
+	if (start != end) {/*Dont care about log level*/
+		if ((start & LOG_BUF_MASK) > (end & LOG_BUF_MASK)) {
+			/* wrapped write */
+			pmem_write_data(general_reg_hdl, &LOG_BUF(start & LOG_BUF_MASK), log_buf_len-(start & LOG_BUF_MASK));
+			pmem_write_data(general_reg_hdl, &LOG_BUF(0), (end & LOG_BUF_MASK) -0);
+		} else {
+       			pmem_write_data(general_reg_hdl, &LOG_BUF(start), end-start);
+		}
+	}
+#endif
 }
 
 /*
diff --git a/kernel/sched.c b/kernel/sched.c
index eadb6ab..3e70d52 100644
--- a/kernel/sched.c
+++ b/kernel/sched.c
@@ -120,6 +120,98 @@
  */
 #define RUNTIME_INF	((u64)~0ULL)
 
+#if defined(CONFIG_LHB) || defined(CONFIG_SCHED_HIST_BUF)
+volatile int disable_history_buffer = 0;
+#endif
+
+#ifdef CONFIG_SCHED_HIST_BUF
+#include <linux/pmem.h>
+#include <linux/schedhist.h>
+#include <linux/time.h>
+
+/* Use pmem api to store scheduler history */
+struct pmem_ptr_block sched_hist_pmem_block = { };
+
+/**
+ * schedhist_addentry - add an entry to the scheduler history buffer.
+ * @prev: task that we are coming from (must not be NULL).
+ * @next: task that we are going to (or NULL).
+ *
+ * Adds an entry to the scheduler history buffer
+ * For 2.4.x, this call should only be made when the runqueue_lock
+ * is locked with a spin_lock.
+ */
+#define offset_of(_s, _m) \
+	&((_s *)0)->_m
+
+#define PMEM_SCHED_ADD_RUNQ 0x8000000
+
+static inline void sched_hist_addentry(struct task_struct *prev,
+                                       struct task_struct *next)
+{
+
+	struct sched_hist_entry_common *entry;
+	struct pmem_cpu_ptrs *ptrs;
+
+	ptrs = sched_hist_pmem_block.cpu[smp_processor_id()].ptrs;
+
+	/* Verify ptr block has been registered */
+	if ((!ptrs) || (!prev))
+		return;
+
+	if (disable_history_buffer)
+		return;
+
+	/* Check for wrap around. We can assume that the end pointer
+	 * is a multiple of the entry size since the API ensures this. */
+	ptrs->curr += sizeof(struct sched_hist_entry_common);
+	if (unlikely(ptrs->curr == ptrs->end)) {
+		ptrs->curr = ptrs->start;
+	}
+	entry = (struct sched_hist_entry_common*)ptrs->curr;
+
+	/* get timestamp and TBR values */
+	entry->tv.tv_sec = (__u64)xtime.tv_sec;
+	entry->tv.tv_usec = (__u32)xtime.tv_nsec/1000;
+#ifdef SUPPORT_HRTIME
+	entry->hrts = (__u64)gethrtime();
+#else
+	entry->hrts = (__u64)get_cycles();
+#endif
+
+	/* store info on process being scheduled out
+	 * Trying to call thread_saved_pc() on a RUNNING task on i386
+	 * will panic.  */
+	if (prev->state == TASK_RUNNING) {
+		entry->nip1 = (__u64)0;
+	} else {
+		entry->nip1 = (__u64)thread_saved_pc(prev);
+	}
+	//entry->nip1 = (__u64)get_wchan(prev);
+	entry->pid1 = (__s32)prev->pid;
+	strncpy(entry->pname1, prev->comm, PROC_NAME_SIZE);
+
+	/* store info on process being scheduled in */
+	if (next) {
+		entry->nip2 = (__u64)thread_saved_pc(next);
+		entry->pid2 = (__s32)next->pid;
+		strncpy(entry->pname2, next->comm, PROC_NAME_SIZE);
+	} else {
+		/* setting the high-order bit of pid2 indicates
+		 * that we're adding an entry to the runqueue
+		 * rather than switching tasks
+		 */
+		entry->pid2 = (__s32)(PMEM_SCHED_ADD_RUNQ | current->pid);
+		entry->nip2 = (current->state == TASK_RUNNING) ?
+		                0:(__u64)thread_saved_pc(current);
+		strncpy(entry->pname2, current->comm, PROC_NAME_SIZE);
+	}
+	PMEM_FLUSH_CACHE(entry, 0);
+}
+#else
+#define sched_hist_addentry(prev,next) do { } while(0)
+#endif /* CONFIG_SCHED_HIST_BUF */
+
 static inline int rt_policy(int policy)
 {
 	if (unlikely(policy == SCHED_FIFO || policy == SCHED_RR))
@@ -1869,6 +1961,8 @@ static void update_avg(u64 *avg, u64 sample)
 static void
 enqueue_task(struct rq *rq, struct task_struct *p, int wakeup, bool head)
 {
+ 	sched_hist_addentry(p, NULL);
+
 	if (wakeup)
 		p->se.start_runtime = p->se.sum_exec_runtime;
 
@@ -3730,6 +3824,7 @@ need_resched_nonpreemptible:
 
 	if (likely(prev != next)) {
 		sched_info_switch(prev, next);
+		sched_hist_addentry(prev, next);
 		perf_event_task_sched_out(prev, next);
 
 		rq->nr_switches++;
diff --git a/kernel/sysctl.c b/kernel/sysctl.c
index 477b212..a8a7420 100644
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -156,6 +156,10 @@ extern int unaligned_dump_stack;
 
 extern struct ratelimit_state printk_ratelimit_state;
 
+#if defined(CONFIG_LHB) || defined(CONFIG_SCHED_HIST_BUF)
+extern volatile int disable_history_buffer ;
+#endif
+
 #ifdef CONFIG_PROC_SYSCTL
 static int proc_do_cad_pid(struct ctl_table *table, int write,
 		  void __user *buffer, size_t *lenp, loff_t *ppos);
@@ -222,6 +226,16 @@ static struct ctl_table root_table[] = {
 		.mode		= 0555,
 		.child		= dev_table,
 	},
+#if defined(CONFIG_LHB) || defined(CONFIG_SCHED_HIST_BUF)
+	{
+		.ctl_name       = KERN_DISABLE_HISTORY_BUFFER,
+		.procname       = "disable_history_buffer",
+		.data           = &disable_history_buffer,
+		.maxlen         = sizeof(int),
+		.mode           = 0644,
+		.proc_handler   = &proc_dointvec,
+	},
+#endif
 /*
  * NOTE: do not add new entries to this table unless you have read
  * Documentation/sysctl/ctl_unnumbered.txt
diff --git a/mm/Makefile b/mm/Makefile
index 6c2a73a..5ba2c95 100644
--- a/mm/Makefile
+++ b/mm/Makefile
@@ -22,6 +22,7 @@ obj-$(CONFIG_HUGETLBFS)	+= hugetlb.o
 obj-$(CONFIG_NUMA) 	+= mempolicy.o
 obj-$(CONFIG_SPARSEMEM)	+= sparse.o
 obj-$(CONFIG_SPARSEMEM_VMEMMAP) += sparse-vmemmap.o
+obj-$(CONFIG_PMEM) += pmem/
 obj-$(CONFIG_SLOB) += slob.o
 obj-$(CONFIG_MMU_NOTIFIER) += mmu_notifier.o
 obj-$(CONFIG_KSM) += ksm.o
diff --git a/mm/pmem/Kconfig b/mm/pmem/Kconfig
new file mode 100644
index 0000000..5f59564
--- /dev/null
+++ b/mm/pmem/Kconfig
@@ -0,0 +1,88 @@
+# -*- shell-script -*-
+
+menu "Persistent Memory Support"
+
+config PMEM
+	bool "Enable Persistent Memory Support"
+	help
+	  Provide a framework for storing information in non-volatile memory.
+	  A simulation mode uses RAM instead of non-volatile memory.
+
+	  There is a true persistent mode for certain boards that have hardware
+	  support for it.  The current board list is xscale, 7101.
+
+	  For user space, a file system interface is provided by the PMEMFS pseudo-filesystem.
+	  This filesystem will not be visible in menuconfig or xconfig unless PMEM is enabled.
+	  When PMEM is enabled, PMEMFS is included as a module by default.
+
+
+config PMEM_SIZE
+	int "Total persistent memory size"
+	depends on PMEM
+	default 8388608
+	help
+	  This amount can be adjusted downward to be a multiple of PAGE_SIZE.
+	  There is also a maximum size allowed for each architecture which
+	  may override PMEM_SIZE or cause the kernel compilation to fail.
+
+config BOARD_HAS_PMEM_HARDWARE
+	bool "Enable hardware persistent memory"
+	depends on PMEM
+	default n
+
+config PMEM_PCI_DRIVER
+	bool
+	default y
+	depends on BOARD_HAS_PMEM_HARDWARE
+
+config PMEM_DEBUG
+	bool "Debug persistent memory"
+	depends on PMEM
+	help
+	  Enable debugging printk()'s.
+
+config PMEM_SHADOW
+	bool "Shadow copy support"
+	depends on PMEM
+	help
+	  When re-initializing pmem, copy the contents to a "shadow" area.
+	  This only happens if pmem has hardware support.
+
+config PMEM_LOG_PART_SIZE
+	int "Log partition size"
+	depends on PMEM
+	default 2097152
+	help
+	  A partition in pmem for logging information is always created.  A
+	  general purpose region can be created in this partition
+	  by selecting PMEM_LOG_REG.  Whenever data is written to a pmem
+	  area with an associated log region, the time of the write and the amount
+	  of data written is logged.
+
+config PMEM_LOG_PART_SEGMENTS
+	int "Log partition segments"
+	depends on PMEM
+	default 2
+	help
+	  Divide the logging partition into this many segments.
+
+config PMEM_LOG_REG
+	bool "General log region"
+	depends on PMEM
+	help
+	  Create a general purpose log region in the log partition.
+
+config PMEM_LOG_REG_SIZE
+	int "General log region size"
+	depends on PMEM_LOG_REG
+	default 262144
+
+config PMEM_LOG_REG_LOGS
+	int "General log region number of logs"
+	depends on PMEM_LOG_REG
+	default 500
+	help
+	  The number of logs in the general log region.  Each log is 32 bytes
+	  of data.
+
+endmenu
diff --git a/mm/pmem/Makefile b/mm/pmem/Makefile
new file mode 100644
index 0000000..43d9608
--- /dev/null
+++ b/mm/pmem/Makefile
@@ -0,0 +1,5 @@
+pmem-objs := reg.o handle.o io.o cmds.o
+
+obj-$(CONFIG_PMEM) += pmem.o
+
+
diff --git a/mm/pmem/cmds.c b/mm/pmem/cmds.c
new file mode 100644
index 0000000..f62696c
--- /dev/null
+++ b/mm/pmem/cmds.c
@@ -0,0 +1,1100 @@
+ /*
+ *  Copyright 2010 Wind River Systems
+ *
+ *  This program is free software; you can redistribute it and/or modify it
+ *  under the terms of the GNU General Public License as published by the
+ *  Free Software Foundation; either version 2 of the License, or (at your
+ *  option) any later version.
+ *
+ *
+ *  THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ *  WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ *  IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+ *  BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS
+ *  OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ *  ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR
+ *  TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
+ *  USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ *  You should have received a copy of the GNU General Public License along
+ *  with this program; if not, write to the Free Software Foundation, Inc.,
+ *  675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#include <linux/slab.h>
+#include <linux/vmalloc.h>
+#include <linux/kernel.h>
+#include <linux/spinlock.h>
+#include <linux/time.h>
+#include <linux/errno.h>
+#include <linux/module.h>
+#include <linux/pmem.h>
+#include <linux/mm.h>
+#include <asm/types.h>
+
+/* constants used for readability */
+static const int PMEM_LOCK =	1;
+static const int PMEM_UNLOCK =	0;
+
+
+/* Get the number of blocks in a partition which are unlocked */
+static int get_num_unlocked_blocks(struct pmem_part_hdr *part_hdr)
+{
+	struct pmem_block_hdr *block_hdr;
+	int count = 0;
+	int i;
+
+	for (i = 0; i < part_hdr->data.num_blocks; i++) {
+		block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, i);
+		if (!(block_hdr->data.flags & PMEM_BLOCK_FLAG_LOCK))
+			count++;
+	}
+
+	return count;
+}
+
+/* Get the amount of free memory for a partition */
+static __u32 get_avail_alloc_mem(void)
+{
+	struct pmem_alloc_desc *alloc_desc;
+	__u32 alloc_table_size;
+	__u32 avail_size;
+
+	alloc_table_size = sizeof (struct pmem_cb_hdr) +
+	    (pmem.pmem->data.num_alloc + 1) * sizeof (struct pmem_alloc_desc);
+
+	if (pmem.pmem->data.num_alloc > 0) {
+		alloc_desc = PMEM_GET_ALLOC_DESC(pmem.pmem->data.num_alloc - 1);
+		avail_size = alloc_desc->data.offset - alloc_table_size;
+	} else {
+		avail_size = pmem.size - alloc_table_size;
+	}
+
+	avail_size -= avail_size % PAGE_SIZE;
+
+	/* Subtract the overhead for adding another partition. This
+	 * shows the number of bytes that the user can allocate */
+	if (avail_size >= PMEM_PART_HDR_MAX_SIZE)
+		return (avail_size - PMEM_PART_HDR_MAX_SIZE);
+	else
+		return (0);
+}
+
+/* Get the amount of free memory for a region */
+static __u32 get_avail_block_mem(struct pmem_part_hdr *part_hdr)
+{
+	struct pmem_block_hdr *block_hdr;
+	struct pmem_region_hdr *region_hdr;
+
+	if (part_hdr->data.num_blocks == 0)
+		return 0;
+
+	/* Just get any block since they all have the same size information */
+	block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, 0);
+
+	if (part_hdr->data.num_regions > 0) {
+		region_hdr =
+		    PMEM_GET_REGION_HDR(part_hdr,
+					part_hdr->data.num_regions - 1);
+		return (block_hdr->data.size -
+			(region_hdr->data.offset + region_hdr->data.size));
+	}
+
+	return (block_hdr->data.size);
+}
+
+/* "lock" the given ptr block or "unlock" depending on the value 
+ * of lock (0 = unlock, !0 = lock)
+ * Always returns success (0)  */
+static inline int __pmem_lock_ptr_block(struct pmem_ptr_block *ptr_block,
+                const int lock)
+{
+	unsigned int i;
+
+	if (!ptr_block) {
+		/* no */
+		PMEM_DPRINT("WARNING: Tried to lock=%d null ptr block\n",
+		            lock);
+		return 0;
+	}
+
+	for (i = 0; i < PMEM_NUM_SUPPORT_CPUS; i++) {
+		if (lock) {
+			/* know that ptrs is referencing one of storeA or
+			 * storeB, and one of storeA.start or storeB.start
+			 * is NULL, so we can zero out the reference w/o loss
+			 * since on the unlock, we can find the correct
+			 * backing store to point ptrs to.  Locking is easy,
+			 * unlocking is where the hairyness comes out. */
+			ptr_block->cpu[i].ptrs = NULL;
+
+		} else {	/* unlock */
+			if (ptr_block->cpu[i].ptrs) {
+				PMEM_DPRINT("ERROR: Refusing to unlock a "
+				            "unlocked ptrblock\n");
+				continue;
+			}
+			if ((ptr_block->cpu[i].storeA.start) &&
+			    (ptr_block->cpu[i].storeB.start)) {
+				PMEM_DPRINT("ERROR: Cant unlock inconsistant "
+				            "ptrblock !\n");
+				continue;
+			}
+
+			if (ptr_block->cpu[i].storeA.start) {
+				ptr_block->cpu[i].ptrs =
+				        &ptr_block->cpu[i].storeA;
+			} else if (ptr_block->cpu[i].storeB.start) {
+				ptr_block->cpu[i].ptrs =
+				        &ptr_block->cpu[i].storeB;
+			} else  {
+				PMEM_DPRINT("ERROR: Cant unlock ptr block - "
+				            "unable to find real ptr block\n");
+			}
+		}
+	}
+	return 0;
+}
+
+/* 
+ * "lock" or "unlock" any ptr blocks for the given partition.
+ * returns 0 on success, -errno on error. */
+static int pmem_lock_ptr_blocks(struct part_handle *part_hdl, const int lock)
+{
+	struct region_list_elem *region_elem;
+	struct list_head *elem, *temp;
+	int rc = 0;
+
+	list_for_each_safe(elem, temp, &(part_hdl->elem->region_list)) {
+		region_elem = list_entry(elem, struct region_list_elem, list_elem);
+		if (region_elem->ptr_block) {
+			(void)__pmem_lock_ptr_block(region_elem->ptr_block, lock);
+		}
+	}
+
+	return rc;
+}
+
+/* Do the locking on a given block header structure. 
+ *
+ * This function does no locking, and doesnt do anything about any
+ * ptr blocks that may point to regions inside the block 
+ *
+ * Always returns 0 since the given block structure will be 
+ * locked upon completion of this code  */
+static inline int pmem_lock_block_hdr(struct pmem_block_hdr *block_hdr)
+{
+	/* Check if the block is already locked */
+	if (block_hdr->data.flags & PMEM_BLOCK_FLAG_LOCK) {
+#ifdef CONFIG_PMEM_DEBUG
+		/* harmless error, but may be interesting for debugging */
+		PMEM_DPRINT("failed to lock block - already locked.\n");
+#endif
+		return 0;
+	}
+
+	block_hdr->data.flags |= PMEM_BLOCK_FLAG_LOCK;
+	PMEM_UPDATE_CHECKSUM(block_hdr);
+	PMEM_FLUSH_CACHE(block_hdr, sizeof(struct pmem_block_hdr));
+
+	return 0;
+}
+
+/* Unlock a given block header.  This function is the companion routine
+ * to pmem_lock_block_hdr(). */
+static inline int pmem_unlock_block_hdr(struct pmem_block_hdr *block_hdr)
+{
+	/* if the block is not locked, then we have nothing to do */
+	if (!block_hdr->data.flags & PMEM_BLOCK_FLAG_LOCK) {
+#ifdef CONFIG_PMEM_DEBUG
+		PMEM_DPRINT("no need to unlock block - not locked.\n");
+#endif
+		return 0;
+	}
+
+	block_hdr->data.flags &= ~PMEM_BLOCK_FLAG_LOCK;
+	PMEM_UPDATE_CHECKSUM(block_hdr);
+	PMEM_FLUSH_CACHE(block_hdr, sizeof(struct pmem_block_hdr));
+
+	return 0;
+}
+
+/* 
+ * Lock a given segment in pmem.  This routine is the internal implementation
+ * which will rotate the block if necessary */
+static int pmem_internal_lock_block(struct block_handle *block_hdl)
+{
+	struct part_handle *part_hdl;
+	struct pmem_part_hdr *part_hdr;
+	struct pmem_block_hdr *block_hdr;
+	int active_block_index, block_index;
+	unsigned long flags;
+	int rc = -EINVAL;
+
+	part_hdl = block_hdl->parent;
+	part_hdr = part_hdl->elem->hdr;
+
+	spin_lock_irqsave(&pmem.lock, flags);
+
+	active_block_index = pmem_get_active_block_index(part_hdr);
+
+	block_index = block_hdl->block_id;
+	if (block_hdl->block_id == PMEM_ACTIVE_BLOCK)
+		block_index = active_block_index;
+
+	/* Check if we can lock the block */
+	if (get_num_unlocked_blocks(part_hdr) <= PMEM_MIN_UNLOCKED_BLOCKS) {
+		/* We cannot lock any more blocks */
+		PMEM_DPRINT("failed to lock block - not enough unlocked " 
+		            "blocks in partition.\n");
+		goto release_done;
+	}
+
+	block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, block_index);
+
+	/* lock this block */	
+	if (pmem_lock_block_hdr(block_hdr)) {
+		PMEM_DPRINT("error locking block header\n");
+		goto release_done;
+	}
+
+	/* Check if we need to rotate the partition */
+	if (block_index == active_block_index) {
+		/* The partition must be rotated. Do not rely on the rotation
+	 	* function to lock the block since we cannot do this 
+		 * atomically. 
+		 */
+		spin_unlock_irqrestore(&pmem.lock, flags);
+		rc = pmem_rotate_block_data(part_hdl->elem, 0);
+		goto done;
+	}
+
+      release_done:
+	spin_unlock_irqrestore(&pmem.lock, flags);
+
+      done:
+	return (rc);
+}
+
+/* Update all of the pointer blocks in a partition with the 
+ * current region information */
+static void pmem_update_part_ptr_blocks(struct part_list_elem *part_elem)
+{
+	struct region_list_elem *region_elem;
+	struct list_head *elem, *temp;
+
+	list_for_each_safe(elem, temp, &(part_elem->region_list)) {
+		region_elem =
+		    list_entry(elem, struct region_list_elem, list_elem);
+		pmem_update_ptr_block(part_elem, region_elem, 0);
+	}
+}
+
+/* Manipulate the lock status of a partition.  This function knows how to 
+ * lock and unlock partitions.  Specify the desired action in lock (unlock = 0,
+ * lock = !0 ).
+ * Returns the number of blocks manipulated inside the partition on success, 
+ * or -errno on error */
+static int pmem_alter_part_lock(pmem_handle_t part_handle, const int lock)
+{
+	int rc = - EINVAL;
+	struct part_handle *hdl = (struct part_handle *) part_handle;
+	struct pmem_part_hdr *part_hdr;
+	struct pmem_block_hdr *block_hdr;
+	unsigned long flags;
+	int i;
+
+	if (!hdl) {
+		PMEM_DPRINT("NULL arguments\n");
+		return rc;
+	}
+
+	/* Ensure that it is a partition handle */
+	if (hdl->hdl.type != PMEM_HANDLE_TYPE_PART) {
+		PMEM_DPRINT("Invalid handle type\n");
+		return rc;
+	}
+	
+	part_hdr = hdl->elem->hdr;
+
+	spin_lock_irqsave(&pmem.lock, flags);
+
+	rc = 0;	
+	/* lock/unlock each block in this partition */
+	for(i = 0; i < part_hdr->data.num_blocks; i++) {
+		block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, i);
+		if (lock) {
+			pmem_lock_block_hdr(block_hdr);
+		} else {
+			pmem_unlock_block_hdr(block_hdr);
+		}
+		rc += 1; /* processed one segment */
+	}
+	/* manipulate the locks on the ptr blocks for this part as well */
+	if (0 != pmem_lock_ptr_blocks(hdl, lock)) {
+		PMEM_DPRINT("WARNING: unable to change locks on ptrblock\n");
+	}
+
+	spin_unlock_irqrestore(&pmem.lock, flags);
+	return rc;
+}
+
+/* Set the lock state on all partitions.  This routine can lock or unlock 
+ * depending on the value of lock.
+ * Returns the number of partitions locked on success, -errno on error */
+static int pmem_alter_global_lock(const int lock)
+{
+	unsigned int i, j;
+	unsigned long flags;
+	struct pmem_part_hdr* part_hdr;
+	struct pmem_block_hdr* block_hdr;
+	pmem_handle_t part_hdl = NULL;
+	int segments = 0;
+
+	part_hdl = (pmem_handle_t) kmalloc(sizeof (struct part_handle), GFP_KERNEL);
+	if (!part_hdl) {
+		PMEM_DPRINT("ERROR: Can't allocate memory for part_hdl\n");
+		segments = -1;
+		goto release_done;
+	}
+
+
+	spin_lock_irqsave(&pmem.lock, flags);
+
+	/* for each partition */
+	for (i = 0; i < pmem.pmem->data.num_alloc; i++) {
+		part_hdr = PMEM_GET_PART_HDR(PMEM_GET_ALLOC_DESC(i));
+		/* lock/unlock each block in the partition */
+		for (j = 0; j < part_hdr->data.num_blocks; j++) {
+			block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, j);
+			/* lock/unlock the header */
+			if (lock) {
+				if (!pmem_lock_block_hdr(block_hdr)) {
+					segments += 1; /* success */
+				}
+			} else {
+				if (!pmem_unlock_block_hdr(block_hdr)) {
+					segments += 1;
+				}
+			}
+		}
+
+		/* lock / unlock any ptr_blocks in this partition */
+		if (0 != pmem_partition_get(part_hdr->data.desc, &part_hdl)) {
+			PMEM_DPRINT("ERROR: Cant get part hdl for [%s]\n",
+			            part_hdr->data.desc);
+			segments = -1;
+			goto release_done;
+		}
+		if (0 != pmem_lock_ptr_blocks((struct part_handle*)part_hdl,
+		                               lock)) {
+			PMEM_DPRINT("WARNING:Unable to change ptrblock lock\n");
+		}
+	}
+
+release_done:
+	spin_unlock_irqrestore(&pmem.lock, flags);
+	pmem_release_handle(&part_hdl);
+	return segments;
+}
+
+/* Lock or unlock all active segments in pmem depending on the value
+ * of lock (unlock on 0. lock on non-zero).  This function cares about
+ * the number of blocks locked in a partition - if it fails, check to make
+ * sure you meet the minimum number of free unlocked blocks
+ *
+ * Returns the number of active segments that were locked.  */
+static int pmem_alter_active_segment_lock(const int lock)
+{
+	unsigned int i, j, unlocked_blocks, segments = 0;
+	unsigned long flags;
+	struct pmem_part_hdr* part_hdr;
+	struct pmem_block_hdr* block_hdr;
+	pmem_handle_t part_hdl = NULL;
+
+	part_hdl = (pmem_handle_t) kmalloc(sizeof (struct part_handle), GFP_KERNEL);
+	if (!part_hdl) {
+		PMEM_DPRINT("ERROR: Can't allocate memory for part_hdl\n");
+		segments = -1;
+		goto release_done;
+	}
+
+	spin_lock_irqsave(&pmem.lock, flags);
+	
+	/* for each partition */
+	for (i = 0; i < pmem.pmem->data.num_alloc; i++) {
+		part_hdr = PMEM_GET_PART_HDR(PMEM_GET_ALLOC_DESC(i));
+		/* if there is the min free unlocked blocks is not met on a 
+		 * lock request, then leave this partition alone */
+		if (lock) {
+	   		unlocked_blocks = get_num_unlocked_blocks(part_hdr);
+			if (unlocked_blocks <= PMEM_MIN_UNLOCKED_BLOCKS) {
+				continue;
+			}
+		}
+		/* find the active segment and lock/unlock it */
+		for (j = 0; j < part_hdr->data.num_blocks; j++) {
+			block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, j);
+			if (!(block_hdr->data.flags & PMEM_BLOCK_FLAG_ACTIVE)) 
+				continue;
+			/* lock/unlock the header */
+			if (lock) {
+				if (!pmem_lock_block_hdr(block_hdr)) {
+					segments += 1;
+				}
+			} else  {
+				if (!pmem_unlock_block_hdr(block_hdr)) {
+					segments += 1;
+				}
+				break;  /* skip the rest of this partition */
+			}
+		} /* for each segment */
+		/* and set the lock state for the ptr blocks on the regions
+		 * for this partition */
+		/* lock / unlock any ptr_blocks in this partition */
+		if (0 != pmem_partition_get(part_hdr->data.desc, &part_hdl)) {
+			PMEM_DPRINT("ERROR: Cant get part hdl for [%s]\n",
+			            part_hdr->data.desc);
+			segments = -1;
+			goto release_done;
+		}
+		if (0 != pmem_lock_ptr_blocks((struct part_handle*)part_hdl,
+		                               lock)) {
+			PMEM_DPRINT("WARNING:Unable to change ptrblock lock\n");
+		}
+	}
+
+release_done:
+	spin_unlock_irqrestore(&pmem.lock, flags);
+	pmem_release_handle(&part_hdl);
+	return segments;
+}
+
+
+/* Get the block index of the active block */
+int pmem_get_active_block_index(struct pmem_part_hdr *part_hdr)
+{
+	struct pmem_block_hdr *block_hdr;
+	int i;
+
+	for (i = 0; i < part_hdr->data.num_blocks; i++) {
+		block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, i);
+		if (block_hdr->data.flags & PMEM_BLOCK_FLAG_ACTIVE)
+			return i;
+	}
+
+	return -1;
+}
+EXPORT_SYMBOL(pmem_get_active_block_index);
+
+
+/* Rotate blocks in a partition */
+int pmem_part_rotate(pmem_handle_t handle, int lock)
+{
+	struct part_handle *hdl = (struct part_handle *) handle;
+	struct part_list_elem *part_elem;
+	int rc;
+
+	if (!handle) {
+		PMEM_DPRINT("NULL arguments\n");
+		return -EINVAL;
+	}
+
+	/* Ensure that it is a partition handle */
+	if (hdl->hdl.type != PMEM_HANDLE_TYPE_PART) {
+		PMEM_DPRINT("Invalid handle type\n");
+		return -EINVAL;
+	}
+
+	part_elem = hdl->elem;
+
+	/* Only rotate if the partition has blocks */
+	if (!part_elem->hdr->data.num_blocks) {
+		PMEM_DPRINT("No blocks to rotate\n");
+		return -EINVAL;
+	}
+
+	/* Rotate the block data in persistent memory */
+	rc = pmem_rotate_block_data(part_elem, lock);
+
+	return (rc);
+}
+EXPORT_SYMBOL(pmem_part_rotate);
+
+
+/* Do all the work necessary to rotate a log partition. 
+ * Returns the block id that just became inactive */
+int pmem_rotate_block_data(struct part_list_elem *part_elem, int lock)
+{
+	struct pmem_part_hdr *part_hdr;
+	struct pmem_block_hdr *active_block_hdr;
+	struct pmem_block_hdr *new_active_block_hdr = NULL;
+	struct pmem_region_hdr *region_hdr;
+	int active_index, new_active_index, index;
+	struct timeval tv;
+	unsigned long flags;
+
+	part_hdr = part_elem->hdr;
+
+	spin_lock_irqsave(&pmem.lock, flags);
+
+	active_index = pmem_get_active_block_index(part_hdr);
+	if (active_index < 0) {
+		spin_unlock_irqrestore(&pmem.lock, flags);
+		printk(KERN_ERR "Unable to get active block");
+		return -EINVAL;
+	}
+
+	active_block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, active_index);
+
+	/* Get the first unlocked block after the active block */
+	new_active_index = active_index + 1;
+	if (new_active_index > (part_hdr->data.num_blocks - 1))
+		new_active_index = 0;
+
+	while (new_active_index != active_index) {
+		new_active_block_hdr =
+		    PMEM_GET_BLOCK_HDR(part_hdr, new_active_index);
+
+		if (!(new_active_block_hdr->data.flags & PMEM_BLOCK_FLAG_LOCK))
+			break;
+
+		new_active_index++;
+		if (new_active_index > (part_hdr->data.num_blocks - 1))
+			new_active_index = 0;			
+	}
+
+	if (!new_active_block_hdr) {
+		spin_unlock_irqrestore(&pmem.lock, flags);
+		PMEM_DPRINT("ERROR: No new_active_block_hdr\n");
+		return -EFAULT;
+	}
+	if (new_active_index >= part_hdr->data.num_blocks) {
+		spin_unlock_irqrestore(&pmem.lock, flags);
+		printk(KERN_ERR "Invalid rotation index=%d\n", new_active_index); 
+		return (-EFAULT);
+	}
+
+	if (new_active_index == active_index) {
+		spin_unlock_irqrestore(&pmem.lock, flags);
+
+		/* Nothing to do. Return failure if the user wanted us to
+		 * lock the block but we were unable to */
+		if (lock)
+			return (-EINVAL);
+		else
+			return (active_index);
+	}
+
+	//PMEM_DPRINT("DEBUG: old activeid=%d, new activeid=%d\n", active_index, new_active_index);
+	/* Update the block activity header information */
+
+	/* Only allow the lock to occur if there are sufficient unlocked blocks */
+	if ((lock)
+	   && (get_num_unlocked_blocks(part_hdr) > PMEM_MIN_UNLOCKED_BLOCKS))
+		active_block_hdr->data.flags |= PMEM_BLOCK_FLAG_LOCK;
+
+	active_block_hdr->data.flags &= ~PMEM_BLOCK_FLAG_ACTIVE;
+	PMEM_UPDATE_CHECKSUM(active_block_hdr);
+	PMEM_FLUSH_CACHE(active_block_hdr, sizeof(struct pmem_block_hdr));
+
+	new_active_block_hdr->data.flags |= PMEM_BLOCK_FLAG_ACTIVE;
+
+	do_gettimeofday(&tv);
+	new_active_block_hdr->data.sec = tv.tv_sec;
+	new_active_block_hdr->data.usec = tv.tv_usec;
+
+	PMEM_UPDATE_CHECKSUM(new_active_block_hdr);
+	PMEM_FLUSH_CACHE(new_active_block_hdr, sizeof(struct pmem_block_hdr));
+
+	/* Prepare the new block */
+	for (index = 0; index < part_hdr->data.num_regions; index++) {
+		region_hdr = PMEM_GET_REGION_HDR(part_hdr, index);
+		pmem_prepare_region(part_hdr, new_active_block_hdr->data.offset,
+				    region_hdr);
+	}
+
+	/*PMEM_DPRINT("DEBUG: old active offset= %d, new active offset= %d\n", 
+			part_elem->active_block_offset, new_active_block_hdr->data.offset);
+			*/
+	part_elem->active_block_offset = new_active_block_hdr->data.offset;
+
+	/* We have done enough so that no one else will rotate the same 
+	 * block so unlock the headers */
+	spin_unlock_irqrestore(&pmem.lock, flags);
+
+	/* Make any updates to the pointer blocks that are necessary */
+	pmem_update_part_ptr_blocks(part_elem);
+
+	return (active_index);
+}
+
+/* Set the reset flag in the control block */
+void pmem_set_cb_reset_flag()
+{
+	unsigned long flags;
+
+	if (!pmem.enabled)
+		return;
+
+	spin_lock_irqsave(&pmem.lock, flags);
+
+	pmem.pmem->data.flags =
+	    htonl(ntohl(pmem.pmem->data.flags) | PMEM_CB_FLAG_RESET);
+	PMEM_UPDATE_CB_CHECKSUM(pmem.pmem);
+	PMEM_FLUSH_CACHE(pmem.pmem, sizeof(struct pmem_cb_hdr));
+
+	spin_unlock_irqrestore(&pmem.lock, flags);
+}
+EXPORT_SYMBOL(pmem_set_cb_reset_flag);
+
+/* Clear the reset flag in the control block */
+void pmem_clear_cb_reset_flag()
+{
+	unsigned long flags;
+
+	if (!pmem.enabled)
+		return;
+
+	spin_lock_irqsave(&pmem.lock, flags);
+
+	pmem.pmem->data.flags = htonl(ntohl(pmem.pmem->data.flags) & ~PMEM_CB_FLAG_RESET);
+	PMEM_UPDATE_CB_CHECKSUM(pmem.pmem);
+	PMEM_FLUSH_CACHE(pmem.pmem, sizeof(struct pmem_cb_hdr));
+
+	spin_unlock_irqrestore(&pmem.lock, flags);
+}
+EXPORT_SYMBOL(pmem_clear_cb_reset_flag);
+
+/* Get some of the dynamic pmem control block information that is not stored
+ * in pmem itself.  */
+int pmem_get_cb_info(pmem_handle_t handle, struct pmem_cb_info *info)
+{
+	struct cb_handle *hdl = (struct cb_handle *) handle;
+
+	if (!handle) {
+		PMEM_DPRINT("NULL arguments\n");
+		return -EINVAL;
+	}
+
+	/* Ensure that it is a cb handle */
+	if (hdl->hdl.type != PMEM_HANDLE_TYPE_CB) {
+		PMEM_DPRINT("Invalid handle type\n");
+		return -EINVAL;
+	}
+
+	info->size = pmem.size;
+	info->num_part = pmem.pmem->data.num_alloc;
+	info->avail_mem = get_avail_alloc_mem();
+	info->num_cpus = pmem.pmem->data.num_cpus;
+
+	return 0;
+}
+EXPORT_SYMBOL(pmem_get_cb_info);
+
+/* Get some partition information. This is used so that API users do not have to
+ * decode the header structures */
+int pmem_get_part_info(pmem_handle_t handle, struct pmem_part_info *info)
+{
+	struct part_handle *hdl = (struct part_handle *) handle;
+	struct part_list_elem *part_elem;
+
+	if (!handle) {
+		PMEM_DPRINT("NULL arguments\n");
+		return -EINVAL;
+	}
+
+	/* Ensure that it is a partition handle */
+	if (hdl->hdl.type != PMEM_HANDLE_TYPE_PART) {
+		PMEM_DPRINT("Invalid handle type\n");
+		return -EINVAL;
+	}
+
+	part_elem = hdl->elem;
+
+	info->size = part_elem->hdr->data.size;
+	info->num_blocks = part_elem->hdr->data.num_blocks;
+	info->num_regions = part_elem->hdr->data.num_regions;
+	info->avail_mem = get_avail_block_mem(part_elem->hdr);
+	info->active_block_id = pmem_get_active_block_index(part_elem->hdr);
+
+	return (0);
+}
+
+/* Get some block information. This is used so that API users do not have to
+ * decode the header structures */
+int pmem_get_block_info(pmem_handle_t handle, struct pmem_block_info *info)
+{
+	struct block_handle *block_hdl = (struct block_handle *) handle;
+	struct part_handle *part_hdl;
+	struct pmem_block_hdr *block_hdr;
+	struct pmem_part_hdr *part_hdr;
+
+	if (!handle) {
+		PMEM_DPRINT("NULL arguments\n");
+		return -EINVAL;
+	}
+
+	/* Ensure that it is a partition handle */
+	if (block_hdl->hdl.type != PMEM_HANDLE_TYPE_BLK) {
+		PMEM_DPRINT("Invalid handle type\n");
+		return -EINVAL;
+	}
+
+	part_hdl = block_hdl->parent;
+	part_hdr = part_hdl->elem->hdr;
+
+	if (block_hdl->block_id == PMEM_ACTIVE_BLOCK)
+		block_hdr = PMEM_GET_BLOCK_HDR(part_hdr,
+				pmem_get_active_block_index(part_hdr));
+	else
+		block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, block_hdl->block_id);
+
+	info->size = block_hdr->data.size;
+	info->active = (block_hdr->data.flags & PMEM_BLOCK_FLAG_ACTIVE) ? 1 : 0;
+	info->locked = (block_hdr->data.flags & PMEM_BLOCK_FLAG_LOCK) ? 1 : 0;
+	info->sec = block_hdr->data.sec;
+	info->usec = block_hdr->data.usec;
+
+	return 0;
+}
+
+/* Get some region information. This is used so that API users do not have to
+ * decode the header structures */
+int pmem_get_region_info(pmem_handle_t handle, struct pmem_region_info *info)
+{
+	struct region_handle *region_hdl = (struct region_handle *) handle;
+	struct pmem_region_hdr *region_hdr;
+
+	if (!handle) {
+		PMEM_DPRINT("NULL arguments\n");
+		return -EINVAL;
+	}
+
+	/* Ensure that it is a partition handle */
+	if (region_hdl->hdl.type != PMEM_HANDLE_TYPE_REG) {
+		PMEM_DPRINT("Invalid handle type\n");
+		return -EINVAL;
+	}
+
+	region_hdr = region_hdl->elem->hdr;
+
+	info->size = region_hdr->data.size;
+	info->num_cpu_areas =
+	    (region_hdr->data.flags & PMEM_REGION_FLAG_PERPROC)
+	    ? PMEM_NUM_SUPPORT_CPUS : 1;
+	info->fixed_size = region_hdr->data.fixed_size;
+	info->num_logs = region_hdr->data.num_log_desc;
+
+	return 0;
+}
+
+/* Lock the block defined by handle */
+int pmem_lock_block(pmem_handle_t handle)
+{
+	struct block_handle *block_hdl = (struct block_handle *) handle;
+	
+	if (!handle) {
+		PMEM_DPRINT("NULL arguments\n");
+		return -EINVAL;
+	}
+
+	/* Ensure that it is a block handle */
+	if (block_hdl->hdl.type != PMEM_HANDLE_TYPE_BLK) {
+		PMEM_DPRINT("Invalid handle type\n");
+		return -EINVAL;
+	}
+
+	return pmem_internal_lock_block(block_hdl);
+}
+EXPORT_SYMBOL(pmem_lock_block);
+
+/* Clear the lock flag for the block */
+int pmem_unlock_block(pmem_handle_t handle)
+{
+	struct block_handle *block_hdl = (struct block_handle *) handle;
+	struct part_handle *part_hdl;
+	struct pmem_part_hdr *part_hdr;
+	struct pmem_block_hdr *block_hdr;
+	int block_index;
+	unsigned long flags;
+
+	if (!handle) {
+		PMEM_DPRINT("NULL arguments\n");
+		return -EINVAL;
+	}
+
+	/* Ensure that it is a block handle */
+	if (block_hdl->hdl.type != PMEM_HANDLE_TYPE_BLK) {
+		PMEM_DPRINT("Invalid handle type\n");
+		return -EINVAL;
+	}
+
+	part_hdl = block_hdl->parent;
+	part_hdr = part_hdl->elem->hdr;
+
+	spin_lock_irqsave(&pmem.lock, flags);
+
+	block_index = block_hdl->block_id;
+	if (block_hdl->block_id == PMEM_ACTIVE_BLOCK)
+		block_index = pmem_get_active_block_index(part_hdr);
+
+	block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, block_index);
+	
+	/* unlock the block */
+	(void)pmem_unlock_block_hdr(block_hdr);
+
+	spin_unlock_irqrestore(&pmem.lock, flags);
+	return (0);
+}
+EXPORT_SYMBOL(pmem_unlock_block);
+
+/* Release the shadow copy memory */
+int pmem_clear_shadow()
+{
+	unsigned long flags;
+	void *ptr;
+
+	if (!pmem.enabled)
+		return -EAGAIN;
+
+	spin_lock_irqsave(&pmem.lock, flags);
+
+	if (!pmem.shadow) {
+		spin_unlock_irqrestore(&pmem.lock, flags);
+		PMEM_DPRINT("shadow is null\n");
+		return -EINVAL;
+	}
+
+	ptr = pmem.shadow;
+	pmem.shadow = NULL;
+
+	spin_unlock_irqrestore(&pmem.lock, flags);
+
+	vfree(ptr);
+
+	return 0;
+}
+EXPORT_SYMBOL(pmem_clear_shadow);
+
+/* Disable all pointer blocks in a partition .
+ * This function really destroys all the pointer blocks for the partition -
+ * they cant be recovered */
+int pmem_disable_ptr_blocks(pmem_handle_t part_handle)
+{
+	struct part_handle *hdl = (struct part_handle *) part_handle;
+	struct part_list_elem *part_elem;
+	struct region_list_elem *region_elem;
+	struct list_head *elem, *temp;
+	int i;
+
+	if (!hdl) {
+		PMEM_DPRINT("NULL arguments\n");
+		return -EINVAL;
+	}
+
+	/* Ensure that it is a partition handle */
+	if (hdl->hdl.type != PMEM_HANDLE_TYPE_PART) {
+		PMEM_DPRINT("Invalid handle type\n");
+		return -EINVAL;
+	}
+
+	part_elem = hdl->elem;
+
+	/* Loop over all of the regions */
+	list_for_each_safe(elem, temp, &(part_elem->region_list)) {
+		region_elem = list_entry(elem, struct region_list_elem, list_elem);
+		if (!region_elem->ptr_block)
+			continue;
+
+		/* Loop over each cpus pointers */
+		for (i = 0; i < PMEM_NUM_SUPPORT_CPUS; i++)
+			region_elem->ptr_block->cpu[i].ptrs = NULL;
+	}
+
+	return 0;
+}
+
+/* Destroy the registered pointer block for the given region */
+int pmem_destroy_ptr_block(pmem_handle_t reg_handle)
+{
+	struct region_handle *hdl = (struct region_handle *)reg_handle;
+	struct region_list_elem *region_elem;
+	unsigned long flags;
+	unsigned int i;
+
+	if (!hdl) {
+		PMEM_DPRINT("NULL region handle\n");
+		return -EINVAL;
+	}
+
+	if (PMEM_HANDLE_TYPE_REG != hdl->hdl.type) {
+		PMEM_DPRINT("Invalid handle type %d\n", hdl->hdl.type);
+		return -EINVAL;
+	}
+
+	region_elem = hdl->elem;
+
+	if (region_elem->ptr_block) {
+		spin_lock_irqsave(&pmem.lock, flags);
+		
+		/* disable each cpu block */
+		for (i=0; i<PMEM_NUM_SUPPORT_CPUS; i++) {
+			region_elem->ptr_block->cpu[i].ptrs = NULL;
+		}
+
+		/* and destroy the ptr block handle stored in the region */
+		region_elem->ptr_block = NULL;
+		spin_unlock_irqrestore(&pmem.lock, flags);
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(pmem_destroy_ptr_block);
+
+/* Lock an entire partition.  This routine overrides the minimum number of 
+ * unlocked blocks in a partition check, and does not rotate the active 
+ * block.  
+ * Returns the number of blocks locked on success, negative errno on error */
+int pmem_lock_part(pmem_handle_t part_handle)
+{
+	return pmem_alter_part_lock(part_handle, PMEM_LOCK);
+}
+EXPORT_SYMBOL(pmem_lock_part);
+
+/* Unlock a partition 
+ * Returns the number of blocks unlocked on success, negative errno on error */
+int pmem_unlock_part(pmem_handle_t part_handle)
+{
+	return pmem_alter_part_lock(part_handle, PMEM_UNLOCK);
+}
+EXPORT_SYMBOL(pmem_unlock_part);
+
+
+/* Lock all of pmem 
+ * Returns the number of segments that were locked */
+int pmem_lock(void)
+{
+	return pmem_alter_global_lock(PMEM_LOCK);
+}
+EXPORT_SYMBOL(pmem_lock);
+
+
+/* Unlock all of pmem 
+ * Returns the number of segments that were unlocked */
+int pmem_unlock(void)
+{
+	return pmem_alter_global_lock(PMEM_UNLOCK);
+}
+EXPORT_SYMBOL(pmem_unlock);
+
+
+/* Lock the active segments in all paritions - lock all active segments 
+ * Returns the number of active segments that were locked.  */
+int pmem_lock_all_active_segments(void)
+{
+	return pmem_alter_active_segment_lock(PMEM_LOCK);
+}
+EXPORT_SYMBOL(pmem_lock_all_active_segments);
+
+
+/* Unlock the active segments in all paritions 
+ * Returns the number of segments that were unlocked */
+int pmem_unlock_all_active_segments(void)
+{
+	return pmem_alter_active_segment_lock(PMEM_UNLOCK);
+}
+EXPORT_SYMBOL(pmem_unlock_all_active_segments);
+
+
+/* Rotate the active segment on all partitions in pmem.
+ * lock specifies whether to lock the rotated blocks or not
+ * Returns the number of active segments that were rotated */
+int pmem_rotate(int lock)
+{
+	struct part_list_elem *part_elem;
+	struct list_head *elem, *temp;
+	int rotated = 0;
+
+	/* for each partition */
+	list_for_each_safe(elem, temp, &pmem.part_list) {
+		part_elem = list_entry(elem, struct part_list_elem, list_elem);
+		/* Only rotate if the partition has blocks */
+		if (part_elem->hdr->data.num_blocks > 0) {
+			/* Rotate the block data in persistent memory */
+			if (pmem_rotate_block_data(part_elem, lock) > 0) {
+				rotated += 1;
+			}
+		}
+	}
+
+	return rotated;
+}
+EXPORT_SYMBOL(pmem_rotate);
+
+/* Clear a region back to its original state. Doesnt delete data, but sets
+ * the next active record to be at the start of the buffer 
+ *
+ * Intended for use by the vMC module
+ */
+int pmem_clear_region(pmem_handle_t region_handle)
+{
+	struct pmem_handle	*handle = (struct pmem_handle*)region_handle;
+	struct region_handle	*reg_hdl;
+	struct pmem_part_hdr	*part_hdr;
+	struct pmem_region_hdr	*reg_hdr;
+	struct pmem_block_hdr	*block_hdr;
+	int 			rc = -EINVAL;
+
+	if (PMEM_HANDLE_TYPE_REG != handle->type) {
+		PMEM_DPRINT("ERROR: Cant clear_region on type %d\n",
+		            handle->type);
+		return rc;
+	}
+
+	reg_hdl = (struct region_handle*)handle;
+	part_hdr = reg_hdl->parent->elem->hdr;
+	reg_hdr = reg_hdl->elem->hdr;
+	block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, reg_hdl->block_id);
+	
+	/* Reset the region data area in the block */
+	pmem_prepare_region(part_hdr, block_hdr->data.offset, reg_hdr);
+	
+	return 0; /* success */
+}
+EXPORT_SYMBOL(pmem_clear_region);
+
+
+
+/*
+ * Routine used by pmemfs
+ * Architectures vary in how they handle caching for addresses
+ * outside of main memory.
+ * Adapted from drivers/char/mem.c
+ */
+#ifdef pgprot_noncached
+int pmem_uncached_access(unsigned long addr)
+{
+#if defined(__i386__)
+ 	return !( test_bit(X86_FEATURE_MTRR, boot_cpu_data.x86_capability) ||
+		  test_bit(X86_FEATURE_K6_MTRR, boot_cpu_data.x86_capability) ||
+		  test_bit(X86_FEATURE_CYRIX_ARR, boot_cpu_data.x86_capability) ||
+		  test_bit(X86_FEATURE_CENTAUR_MCR, boot_cpu_data.x86_capability) )
+	  && addr >= __pa(high_memory);
+#elif defined(__x86_64__)
+	return 0;
+#elif defined(CONFIG_IA64)
+	return !(efi_mem_attributes(addr) & EFI_MEMORY_WB);
+#elif defined(CONFIG_PPC)
+	return !page_is_ram(addr >> PAGE_SHIFT);
+#else
+	return addr >= __pa(high_memory);
+#endif
+}
+
+EXPORT_SYMBOL(pmem_uncached_access);
+
+#endif /* pgprot_noncached */
+
diff --git a/mm/pmem/handle.c b/mm/pmem/handle.c
new file mode 100644
index 0000000..6436498
--- /dev/null
+++ b/mm/pmem/handle.c
@@ -0,0 +1,498 @@
+ /*
+ *  Copyright 2010 Wind River Systems
+ *
+ *  This program is free software; you can redistribute it and/or modify it
+ *  under the terms of the GNU General Public License as published by the
+ *  Free Software Foundation; either version 2 of the License, or (at your
+ *  option) any later version.
+ *
+ *
+ *  THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ *  WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ *  IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+ *  BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS
+ *  OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ *  ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR
+ *  TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
+ *  USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ *  You should have received a copy of the GNU General Public License along
+ *  with this program; if not, write to the Free Software Foundation, Inc.,
+ *  675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#include <linux/string.h>
+#include <linux/errno.h>
+#include <linux/spinlock.h>
+#include <linux/kernel.h>
+#include <linux/slab.h>
+#include <linux/module.h>
+#include <linux/pmem.h>
+
+#include <asm/types.h>
+
+
+/* Return a pointer to the data for the element */
+void* pmem_get_data_ptr(struct pmem_handle *hdl)
+{
+	switch (hdl->type) {
+	case PMEM_HANDLE_TYPE_CB:
+		return (pmem.pmem);
+		break;
+
+	case PMEM_HANDLE_TYPE_PART:
+		{
+			struct part_handle *part_hdl =
+			    (struct part_handle *) hdl;
+			return ((void *) part_hdl->elem->hdr +
+				PMEM_PART_HDR_MAX_SIZE);
+			break;
+		}
+
+	case PMEM_HANDLE_TYPE_BLK:
+		{
+			struct block_handle *block_hdl =
+			    (struct block_handle *) hdl;
+			struct part_handle *part_hdl = block_hdl->parent;
+			struct pmem_block_hdr *block_hdr;
+
+			if (block_hdl->block_id == PMEM_ACTIVE_BLOCK) {
+				return ((void *) part_hdl->elem->hdr +
+					part_hdl->elem->active_block_offset);
+			} else {
+				block_hdr =
+				    PMEM_GET_BLOCK_HDR(part_hdl->elem->hdr,
+						       block_hdl->block_id);
+				return ((void *) part_hdl->elem->hdr +
+					block_hdr->data.offset);
+			}
+			break;
+		}
+
+	case PMEM_HANDLE_TYPE_REG:
+		{
+			struct region_handle *region_hdl =
+			    (struct region_handle *) hdl;
+			struct part_handle *part_hdl = region_hdl->parent;
+			struct pmem_block_hdr *block_hdr;
+
+			if (region_hdl->block_id == PMEM_ACTIVE_BLOCK) {
+				return ((void *) part_hdl->elem->hdr +
+					part_hdl->elem->active_block_offset +
+					region_hdl->elem->hdr->data.offset);
+			} else {
+				block_hdr =
+				    PMEM_GET_BLOCK_HDR(part_hdl->elem->hdr,
+						       region_hdl->block_id);
+				return ((void *) part_hdl->elem->hdr +
+					block_hdr->data.offset +
+					region_hdl->elem->hdr->data.offset);
+			}
+			break;
+		}
+
+	case PMEM_HANDLE_TYPE_SHADOW:
+		return (pmem.shadow);
+		break;
+	}
+
+	return (NULL);
+}
+EXPORT_SYMBOL(pmem_get_data_ptr);
+
+/* Get a pointer to the start of the header block for an element. */
+void* pmem_get_hdr_ptr(struct pmem_handle *hdl)
+{
+	struct block_handle	*block_hdl;
+	struct part_handle	*part_hdl;
+	struct region_handle	*region_hdl;
+	struct pmem_block_hdr	*block_hdr;
+	int			block_id;
+	void			*rp = NULL;
+
+	switch (hdl->type) {
+		case PMEM_HANDLE_TYPE_PART:
+			part_hdl = (struct part_handle*)hdl;
+			rp = (void*)part_hdl->elem->hdr;
+			break;
+		case PMEM_HANDLE_TYPE_BLK:
+			block_hdl = (struct block_handle*)hdl;
+			part_hdl = block_hdl->parent;
+			block_id = block_hdl->block_id;
+
+			if (PMEM_ACTIVE_BLOCK == block_id) {
+				block_id = pmem_get_active_block_index(
+				               part_hdl->elem->hdr);
+			}
+			block_hdr = PMEM_GET_BLOCK_HDR(part_hdl->elem->hdr,
+			                               block_id);
+			rp = (void*)block_hdr;
+			break;
+		case PMEM_HANDLE_TYPE_REG:
+			region_hdl = (struct region_handle*)hdl;
+			rp = (void*)region_hdl->elem->hdr;
+			break;
+		case PMEM_HANDLE_TYPE_CB:
+		case PMEM_HANDLE_TYPE_SHADOW:
+		default:
+			PMEM_DPRINT("ERROR: Cant get hdr ptr for type %d\n",
+			            hdl->type);
+			break;
+	}
+
+	return rp;
+}
+EXPORT_SYMBOL(pmem_get_hdr_ptr);
+
+/* Get the size of a handle's header */
+ssize_t pmem_get_hdr_size(pmem_handle_t handle)
+{
+	struct pmem_handle 	*hdl = (struct pmem_handle*)handle;
+	ssize_t 		rc = -EINVAL;
+	
+	switch (hdl->type) {
+		case PMEM_HANDLE_TYPE_PART:
+			/* how does PMEM_PART_HDR_MAX_SIZE fit into these
+			 * size discussions ? */
+			rc = sizeof(struct pmem_part_hdr);
+			break;
+		case PMEM_HANDLE_TYPE_BLK:
+			rc = sizeof(struct pmem_block_hdr);
+			break;
+		case PMEM_HANDLE_TYPE_REG:
+			rc = sizeof(struct pmem_region_hdr);
+			break;
+		case PMEM_HANDLE_TYPE_CB:
+			rc = sizeof(struct pmem_cb_hdr);
+			break;
+		case PMEM_HANDLE_TYPE_SHADOW:   
+			/* not sure if shadow header has a size ? */
+			PMEM_DPRINT("WARNING: shadow header size unknown\n");
+			rc = 0;
+			break;
+		default:
+			PMEM_DPRINT("ERROR: Invalid type %d\n", hdl->type);
+			break;
+	}
+
+	return rc;
+}
+EXPORT_SYMBOL(pmem_get_hdr_size);
+
+/* Get the partition element with this string description */
+static struct part_list_elem * get_part_elem(char *desc)
+{
+	struct part_list_elem *part_elem;
+	struct list_head *elem, *temp;
+
+	list_for_each_safe(elem, temp, &pmem.part_list) {
+		part_elem = list_entry(elem, struct part_list_elem, list_elem);
+		if (!strncmp(part_elem->hdr->data.desc, desc, PMEM_DESC_MAX))
+			return (part_elem);
+	}
+
+	return NULL;
+}
+
+/* Get the region element with this string description */
+static struct region_list_elem *
+get_region_elem(struct part_list_elem *part_elem, char *desc)
+{
+	struct region_list_elem *region_elem;
+	struct list_head *elem, *temp;
+
+	list_for_each_safe(elem, temp, &(part_elem->region_list)) {
+		region_elem =
+		    list_entry(elem, struct region_list_elem, list_elem);
+		if (!strncmp(region_elem->hdr->data.desc, desc, PMEM_DESC_MAX))
+			return (region_elem);
+	}
+
+	return NULL;
+}
+
+/* Get a control block handle
+ *    If the pmem_handle_t pointer that is passed does not point to NULL then 
+ *    the memory it points to will be used.  Otherwise kmalloc will be called 
+ *    to allocate new memory.
+ */
+int pmem_control_block_get(pmem_handle_t *handle)
+{
+	struct cb_handle *hdl = NULL;
+
+	if (!pmem.enabled)
+		return -EINVAL;
+
+	if (!handle)
+		return -EINVAL;
+
+#ifdef CONFIG_PMEM_DEBUG
+	if (PMEM_VALIDATE_CB_CHECKSUM(pmem.pmem) < 0)
+		printk(KERN_ERR "CB checksum invalid\n");
+#endif
+
+	if (!*handle) {
+		hdl = kmalloc(sizeof (struct cb_handle), GFP_KERNEL);
+
+		if (!hdl)
+			return -ENOMEM;
+
+		*handle = hdl;
+	} else {
+		/* PMEM_DPRINT("re-using *handle %p\n", *handle); */
+		hdl = (struct cb_handle *) *handle;
+	}
+
+	hdl->hdl.type = PMEM_HANDLE_TYPE_CB;
+	hdl->hdl.offset = 0;
+	hdl->hdl.size = pmem.size;
+
+	return 0;
+}
+EXPORT_SYMBOL(pmem_control_block_get);
+
+/* Get a partition handle
+ *    If the pmem_handle_t pointer that is passed does not point to NULL then 
+ *    the memory it points to will be used.  Otherwise kmalloc will be called 
+ *    to allocate new memory.
+ */
+int pmem_partition_get(char *desc, pmem_handle_t *handle)
+{
+	struct part_handle *hdl = NULL;
+	struct part_list_elem *elem = NULL;
+
+	if (!pmem.enabled)
+		return (-EINVAL);
+
+	elem = get_part_elem(desc);
+	if (!elem)
+		return (-EINVAL);
+
+	if (!handle)
+		return -EINVAL;
+
+#ifdef CONFIG_PMEM_DEBUG
+	if (PMEM_VALIDATE_CHECKSUM(elem->hdr) < 0)
+		printk(KERN_ERR "Partition checksum invalid\n");
+#endif
+
+	if (!*handle) {
+		hdl = kmalloc(sizeof (struct part_handle), GFP_KERNEL);
+
+		if (!hdl)
+			return -ENOMEM;
+
+		*handle = hdl;
+	} else {
+		/* PMEM_DPRINT("re-using *handle %p\n", *handle); */
+		hdl = (struct part_handle *) *handle;
+	}
+
+	hdl->hdl.type = PMEM_HANDLE_TYPE_PART;
+	hdl->hdl.offset = 0;
+	hdl->hdl.size = elem->hdr->data.size;
+	hdl->elem = elem;
+
+	return 0;
+}
+EXPORT_SYMBOL(pmem_partition_get);
+
+/* Get a block handle 
+ *    If the pmem_handle_t pointer that is passed does not point to NULL then 
+ *    the memory it points to will be used.  Otherwise kmalloc will be called 
+ *    to allocate new memory.
+ */
+int pmem_block_get(pmem_handle_t part_handle, __s8 block_id, pmem_handle_t *handle)
+{
+	struct part_handle *part_hdl = (struct part_handle *) part_handle;
+	struct pmem_block_hdr *block_hdr = NULL;
+	struct block_handle *hdl = NULL;
+
+	if (!pmem.enabled)
+		return -EINVAL;
+
+	if (!handle)
+		return -EINVAL;
+
+	if (!part_hdl || (part_hdl->hdl.type != PMEM_HANDLE_TYPE_PART))
+		return -EINVAL;
+
+	if (part_hdl->elem->hdr->data.num_blocks == 0)
+		return -EINVAL;
+
+	if (((block_id >= part_hdl->elem->hdr->data.num_blocks) ||
+	     (block_id < 0)) && (block_id != PMEM_ACTIVE_BLOCK))
+		return -EINVAL;
+
+	/* grab the block details from the first block */
+	block_hdr = PMEM_GET_BLOCK_HDR(part_hdl->elem->hdr, 0);
+
+#ifdef CONFIG_PMEM_DEBUG
+	if (PMEM_VALIDATE_CHECKSUM(block_hdr) < 0)
+		printk(KERN_ERR "Block checksum invalid\n");
+#endif
+
+	if (!*handle) {
+		hdl = kmalloc(sizeof (struct block_handle), GFP_KERNEL);
+
+		if (!hdl) 
+			return -ENOMEM;
+
+		*handle = hdl;
+	} else {
+		/* PMEM_DPRINT("re-using *handle %p\n", *handle); */
+		hdl = (struct block_handle *) *handle;
+	}
+
+	hdl->hdl.type = PMEM_HANDLE_TYPE_BLK;
+	hdl->hdl.offset = 0;
+	hdl->hdl.size = block_hdr->data.size;
+	hdl->parent = part_handle;
+	hdl->block_id = block_id;
+
+	return 0;
+}
+EXPORT_SYMBOL(pmem_block_get);
+
+/* Get a region handle
+ *    If the pmem_handle_t pointer that is passed does not point to NULL then 
+ *    the memory it points to will be used.  Otherwise kmalloc will be called 
+ *    to allocate new memory.
+ */
+int pmem_region_get(pmem_handle_t part_handle, char *desc, __s8 block_id,
+		pmem_handle_t *handle)
+{
+	struct part_handle *part_hdl = (struct part_handle *) part_handle;
+	struct region_handle *hdl = NULL;
+	struct region_list_elem *elem;
+
+	if (!pmem.enabled)
+		return -EINVAL;
+
+	if (!handle)
+		return -EINVAL;
+
+	if (!part_hdl || (part_hdl->hdl.type != PMEM_HANDLE_TYPE_PART))
+		return -EINVAL;
+
+	if (part_hdl->elem->hdr->data.num_blocks == 0)
+		return -EINVAL;
+
+	if ((block_id >= part_hdl->elem->hdr->data.num_blocks) ||
+	    (block_id < PMEM_ACTIVE_BLOCK))
+		return -EINVAL;
+
+	elem = get_region_elem(part_hdl->elem, desc);
+	if (!elem)
+		return -EINVAL;
+
+#ifdef CONFIG_PMEM_DEBUG
+	if (PMEM_VALIDATE_CHECKSUM(elem->hdr) < 0)
+		printk(KERN_ERR "Region checksum invalid\n");
+#endif
+
+	if (!*handle) {
+		hdl = kmalloc(sizeof (struct region_handle), GFP_KERNEL);
+
+		if (!hdl) 
+			return -ENOMEM;
+
+		*handle = hdl;
+	} else {
+		/* PMEM_DPRINT("re-using *handle %p\n", *handle); */
+		hdl = (struct region_handle *) *handle;
+	}
+
+	hdl->hdl.type = PMEM_HANDLE_TYPE_REG;
+	hdl->hdl.offset = 0;
+	hdl->hdl.size = elem->hdr->data.size;
+	hdl->parent = part_handle;
+	hdl->elem = elem;
+	hdl->block_id = block_id;
+
+	return 0;
+}
+EXPORT_SYMBOL(pmem_region_get);
+
+/* Get a shadow handle */
+int pmem_shadow_get(pmem_handle_t *handle)
+{
+	struct shadow_handle *hdl = NULL;
+
+	if (!pmem.enabled)
+		return (-EINVAL);
+
+	if (!pmem.shadow)
+		return (-EINVAL);
+
+	if (!handle)
+		return -EINVAL;
+
+	if (!*handle) {
+		hdl = kmalloc(sizeof (struct shadow_handle), GFP_KERNEL);
+
+		if (!hdl)
+			return -ENOMEM;
+
+		*handle = hdl;
+	} else {
+		/* PMEM_DPRINT("re-using *handle %p\n", *handle); */
+		hdl = (struct shadow_handle *) *handle;
+	}
+
+	hdl->hdl.type = PMEM_HANDLE_TYPE_SHADOW;
+	hdl->hdl.offset = 0;
+	hdl->hdl.size = pmem.size;
+
+	return 0;
+}
+EXPORT_SYMBOL(pmem_shadow_get);
+
+/* Free a handle
+ *   For future instrumentation */
+void inline pmem_free_handle(pmem_handle_t handle)
+{
+	kfree(handle);
+}
+
+/* Release a handle
+ *   For future instrumentation */
+void inline pmem_release_handle(pmem_handle_t *handle)
+{
+	pmem_free_handle(*handle);
+	*handle = NULL;
+	
+}
+EXPORT_SYMBOL(pmem_release_handle);
+
+/* Get the parent handle */
+pmem_handle_t pmem_get_handle_parent(pmem_handle_t handle)
+{
+	struct pmem_handle *hdl = (struct pmem_handle *) handle;
+
+	switch (hdl->type) {
+	case PMEM_HANDLE_TYPE_CB:
+	case PMEM_HANDLE_TYPE_PART:
+	case PMEM_HANDLE_TYPE_SHADOW:
+		return (NULL);
+		break;
+	case PMEM_HANDLE_TYPE_BLK:
+		{
+			struct block_handle *block =
+			    (struct block_handle *) handle;
+			return (block->parent);
+			break;
+		}
+	case PMEM_HANDLE_TYPE_REG:
+		{
+			struct region_handle *region =
+			    (struct region_handle *) handle;
+			return (region->parent);
+			break;
+		}
+	}
+
+	return (NULL);
+}
diff --git a/mm/pmem/io.c b/mm/pmem/io.c
new file mode 100644
index 0000000..294534c
--- /dev/null
+++ b/mm/pmem/io.c
@@ -0,0 +1,636 @@
+ /*
+ *  Copyright 2010 Wind River Systems
+ *
+ *  This program is free software; you can redistribute it and/or modify it
+ *  under the terms of the GNU General Public License as published by the
+ *  Free Software Foundation; either version 2 of the License, or (at your
+ *  option) any later version.
+ *
+ *
+ *  THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ *  WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ *  IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+ *  BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS
+ *  OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ *  ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR
+ *  TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
+ *  USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ *  You should have received a copy of the GNU General Public License along
+ *  with this program; if not, write to the Free Software Foundation, Inc.,
+ *  675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#include <linux/string.h>
+#include <linux/errno.h>
+#include <linux/kernel.h>
+#include <linux/spinlock.h>
+#include <linux/slab.h>
+#include <linux/mm.h>
+#include <linux/module.h>
+#include <linux/pmem.h>
+
+#include <asm/types.h>
+#include <asm/uaccess.h>	/* copy_to_user */
+
+
+
+/* static int is_locked {{{
+ *
+ * Check to see if a given handle is currently locked
+ * Returns 0 if not locked, non-zero if locked
+ */
+static int is_locked(pmem_handle_t handle)
+{
+	int			rc = 0;
+	struct pmem_handle	*hdl = (struct pmem_handle *) handle;
+	struct block_handle	*block_hdl;
+	struct region_handle	*reg_hdl;
+	struct pmem_part_hdr	*part_hdr;
+	struct pmem_block_hdr	*block_hdr;
+	int			block_id;
+
+	/* only care about segment handles and region handles
+	 * others are not affected by locks */
+	switch (hdl->type) {
+		case PMEM_HANDLE_TYPE_SHADOW:
+		case PMEM_HANDLE_TYPE_PART:
+		case PMEM_HANDLE_TYPE_CB:
+			/* cant be locked */
+			return rc;
+			break;
+		case PMEM_HANDLE_TYPE_BLK:
+			block_hdl = handle;
+			part_hdr = block_hdl->parent->elem->hdr;
+			block_id = block_hdl->block_id;
+			break;
+		case PMEM_HANDLE_TYPE_REG:
+			reg_hdl = handle;
+			part_hdr = reg_hdl->parent->elem->hdr; 
+			block_id = reg_hdl->block_id;
+			break;
+		default:
+			printk(KERN_ERR "ERROR: Unknown pmem handle type %d\n",
+			       hdl->type);
+			return rc;
+			break;
+	}
+
+	/* common code for checking locks on region/segment here */
+	if (PMEM_ACTIVE_BLOCK == block_id)
+		block_id = pmem_get_active_block_index(part_hdr);
+	block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, block_id);
+
+	if (PMEM_BLOCK_FLAG_LOCK & block_hdr->data.flags) {
+		/* locked */
+		rc = 1;
+	}
+	return rc;
+} 
+
+/* int pmem_read_data {{{
+ * Read data from persistent memory data segment */
+int pmem_read_data(pmem_handle_t handle, char *buffer, int size)
+{
+	struct pmem_handle *hdl = (struct pmem_handle *) handle;
+	int bytes_read = -EINVAL;
+	void *data_start;
+	spinlock_t *lock = NULL;
+	unsigned long flags = 0;
+
+	/* There is a window where pmem is enabled but the handles have not yet
+	 * been registered.
+	 */
+	if (!handle)
+		goto done;
+
+#ifdef CONFIG_PMEM_DEBUG
+	if (!buffer || (size < 0)) {
+		PMEM_DPRINT("read: invalid arguments\n");
+		goto done;
+	}
+#endif
+
+	if (hdl->offset > hdl->size) {
+		PMEM_DPRINT("read: invalid offset %d with size %d\n",
+		              hdl->offset, hdl->size);
+		goto done;
+	}
+
+	data_start = pmem_get_data_ptr(hdl);
+	if (!data_start) {
+		PMEM_DPRINT("read: invalid handle\n");
+		goto done;
+	}
+
+	if (hdl->offset + size > hdl->size) {
+		bytes_read = hdl->size - hdl->offset;
+	} else {
+		bytes_read = size;
+	}
+
+	if (hdl->type == PMEM_HANDLE_TYPE_REG) {
+		/* Only acquire spinlock on reads of single CPU data */
+		struct region_handle *region_hdl =
+		    (struct region_handle *) handle;
+		if (!
+		    (region_hdl->elem->hdr->data.
+		     flags & PMEM_REGION_FLAG_PERPROC)) {
+			lock = &region_hdl->elem->data_lock;
+			spin_lock_irqsave(lock, flags);
+		}
+	}
+
+	//PMEM_DPRINT("INFO: data_read, start=%p, off=%d, size=%d\n", data_start, hdl->offset, bytes_read);
+	/* FIXME: Read from percpu region doesnt read the current cpu data */
+
+	__pmem_memcpy_fromio(buffer, data_start + hdl->offset, bytes_read);
+
+	if (lock) {
+		spin_unlock_irqrestore(lock, flags);
+	}
+	hdl->offset += bytes_read;
+
+      done:
+	return (bytes_read);
+} 
+EXPORT_SYMBOL(pmem_read_data);
+
+/* int pmem_read_header 
+ * Read data from the header for the given handle 
+ * Returns the number of bytes read from the header on success
+ * and -errno on error.*/
+int pmem_read_header(pmem_handle_t handle, char *buffer, int size, 
+               loff_t offset)
+{
+	struct pmem_handle	*hdl = (struct pmem_handle*)handle;
+	void			*header_start;
+	int			header_size;
+	ssize_t			bytes_read = -EINVAL;
+
+	header_size = pmem_get_hdr_size(handle);
+	if (header_size <= 0) {
+		PMEM_DPRINT("ERROR: Can't read a unsized header\n");
+		return bytes_read;
+	}
+	if (size > header_size) {
+		size = header_size;
+	}
+
+	header_start = pmem_get_hdr_ptr(hdl);
+	if (!header_start) {
+		PMEM_DPRINT("ERROR: Can't read from a NULL header\n");
+		return bytes_read;
+	}
+	
+	if (size + offset > header_size) {
+		bytes_read = header_size - offset;
+	} else {
+		bytes_read = size;
+	}
+
+	__pmem_memcpy_fromio(buffer, header_start + offset, bytes_read);
+	return bytes_read;
+} 
+EXPORT_SYMBOL(pmem_read_header);
+
+/* static int pmem_write_region_data 
+ * Write to a region without log descriptors. */
+static int pmem_write_region_data(struct region_handle *hdl, void *region_data,
+                 const char *buffer, int size)
+{
+	struct pmem_region_data_hdr *data_hdr;
+	struct pmem_region_hdr *region_hdr = hdl->elem->hdr;
+	int log_size, bytes_written;
+	char *log_ptr;
+	spinlock_t *lock;
+	unsigned long flags = 0;
+	int cpu;
+
+	if (region_hdr->data.flags & PMEM_REGION_FLAG_PERPROC) {
+		/* get current processor and disable kernel pre-emption */
+		cpu = get_cpu();
+		data_hdr = (struct pmem_region_data_hdr *) (region_data +
+						     PMEM_DESC_MAX) + cpu;
+		/* re-enable kernel preemption */
+		put_cpu();
+		lock = NULL;
+	} else {
+		data_hdr =
+		    (struct pmem_region_data_hdr *) (region_data +
+						     PMEM_DESC_MAX);
+		lock = &hdl->elem->data_lock;
+	}
+
+	log_size = region_hdr->data.fixed_size;
+
+	/* Check if the log needs to be truncated */
+	if (size > region_hdr->data.size) {
+		/* do we need to do more strict size checking here ? */
+		size = region_hdr->data.size;
+	}
+	if ((size > log_size) && (log_size != 0)) {
+		bytes_written = log_size;
+	} else {
+		bytes_written = size;
+		if (log_size == 0)
+			log_size = size;
+	}
+
+	if (lock)
+		spin_lock_irqsave(lock, flags);
+
+	/* Get the new offset */
+	if (data_hdr->current_offset + log_size > data_hdr->end_offset) {
+		/* Wrap around. Check if we should stop here */
+		if (region_hdr->data.flags & PMEM_REGION_FLAG_STOPFULL) {
+			if (lock)
+				spin_unlock_irqrestore(lock, flags);
+
+			return 0;
+		} else
+			data_hdr->current_offset =
+			    data_hdr->start_offset + log_size;
+	} else {
+		data_hdr->current_offset += log_size;
+	}
+	PMEM_FLUSH_CACHE(data_hdr, sizeof(struct pmem_region_data_hdr));
+
+	log_ptr = (char *) region_data + data_hdr->current_offset - log_size;
+
+	/* Done with the headers so release the lock */
+	if (lock)
+		spin_unlock_irqrestore(lock, flags);
+
+	/* Write the log */
+	/*
+	PMEM_DPRINT("INFO: write() reg_start=%p, cur_off=%p, end=%p,\
+		       	end_write=%p\n",
+		       	(void*)(data_hdr->start_offset+region_data), log_ptr,
+		       	(void*)(data_hdr->end_offset+region_data),
+		       	(void*)(log_ptr+bytes_written)); */
+	__pmem_memcpy_toio(log_ptr, buffer, bytes_written);
+	PMEM_FLUSH_CACHE(log_ptr, bytes_written);
+
+	return (bytes_written);
+}
+
+/* static void print_desc  */
+#ifdef EXTRA_PMEM_DEBUGGING
+/* Nobody seems to call this routine - leave it here in case it is needed
+ * for debugging purposes later. */
+#ifdef CONFIG_PMEM_DEBUG
+static void print_desc(struct pmem_log_desc *log_desc)
+{
+	int valid = 1;
+
+	if (PMEM_VALIDATE_CHECKSUM(log_desc) < 0)
+		valid = 0;
+
+	printk(KERN_ERR
+	       "Valid=%d, time=%LX, sec=%d, usec=%d, size=%d, offset=%d, checksum=%d\n",
+	       valid, log_desc->data.hrtime, log_desc->data.sec,
+	       log_desc->data.usec, log_desc->data.size, log_desc->data.offset,
+	       log_desc->data.log_checksum);
+	return;		/* do nothing */
+} 
+#else
+#define print_desc(log_desc) do { } while(0)
+#endif
+#endif
+
+
+/* static int pmem_write_log_desc 
+ * Write to a region with log descriptors */
+static int pmem_write_log_desc(struct region_handle *hdl, void *region_data,
+		const char *buffer, int size)
+{
+	struct pmem_log_desc_index *desc_index;
+	struct pmem_log_desc *log_desc;
+	struct pmem_region_hdr *region_hdr = hdl->elem->hdr;
+	struct timeval tv;
+	int bytes_written = 0;
+	int reuse_desc = 0;
+	int desc_arr_size, log_offset, i;
+	unsigned long flags;
+
+	desc_arr_size = sizeof (struct pmem_log_desc_index) +
+	    region_hdr->data.num_log_desc * sizeof (struct pmem_log_desc);
+	desc_index = region_data;
+
+	/* Enforce a max log size */
+	if (size > PAGE_SIZE)
+		size = PAGE_SIZE;
+
+	if (size > (region_hdr->data.size - desc_arr_size))
+		bytes_written = region_hdr->data.size - desc_arr_size;
+	else
+		bytes_written = size;
+
+	spin_lock_irqsave(&hdl->elem->data_lock, flags);
+
+#ifdef CONFIG_PMEM_DEBUG
+	if (PMEM_VALIDATE_CHECKSUM(desc_index) < 0) {
+		PMEM_DPRINT
+		    ("region data checksum failed. Reinitializing index\n");
+		PMEM_DPRINT("Data: last=%d, curr=%d, checksum=0x%x\n",
+			    desc_index->data.oldest_index,
+			    desc_index->data.curr_index, desc_index->checksum);
+
+		desc_index->data.oldest_index = 0;
+		desc_index->data.curr_index = 0;
+		PMEM_UPDATE_CHECKSUM(desc_index);
+	}
+#endif
+
+	/* Get the current descriptor. This points to the last written log */
+	log_desc = region_data + sizeof (struct pmem_log_desc_index) +
+	    desc_index->data.curr_index * sizeof (struct pmem_log_desc);
+
+#ifdef CONFIG_PMEM_DEBUG
+	if (PMEM_VALIDATE_CHECKSUM(log_desc) < 0) {
+		PMEM_DPRINT
+		    ("region data checksum failed. Reinitializing descriptor\n");
+
+		log_desc->data.size = 0;
+		log_desc->data.offset = 0;
+		PMEM_UPDATE_CHECKSUM(log_desc);
+	}
+#endif
+
+	/* Get the new offset */
+	if ((log_desc->data.offset + log_desc->data.size + bytes_written >=
+	     region_hdr->data.size) ||
+	    (log_desc->data.offset < desc_arr_size)) {
+		log_offset = desc_arr_size;
+	} else {
+		log_offset = log_desc->data.offset + log_desc->data.size;
+	}
+
+	/* Get the new descriptor */
+	if (desc_index->data.curr_index >= region_hdr->data.num_log_desc - 1)
+		desc_index->data.curr_index = 0;
+	else
+		desc_index->data.curr_index++;
+
+	if (desc_index->data.curr_index == desc_index->data.oldest_index) 
+		reuse_desc = 1;
+	
+	/* Update any overwritten descriptors */
+	i = desc_index->data.oldest_index;
+	do {
+		log_desc = region_data + sizeof (struct pmem_log_desc_index) +
+		    i * sizeof (struct pmem_log_desc);
+
+		/* Update i to point to the next log incase we need it */
+		i++;
+		if (i >= region_hdr->data.num_log_desc)
+			i = 0;
+
+		/* Check if we need to update the oldest index. This is needed when:
+		 * - The descriptor is invalid
+		 * - The descriptor needs to be re-used
+		 * - The data described by the descriptor will be overwritten
+		 */
+		if ((PMEM_VALIDATE_CHECKSUM(log_desc) < 0) ||
+		    (log_desc->data.size == 0) ||
+		    (reuse_desc) ||
+		    (pmem_is_overlapping(log_offset, bytes_written,
+					 log_desc->data.offset,
+					 log_desc->data.size))) {
+#ifdef CONFIG_PMEM_DEBUG
+			if (PMEM_VALIDATE_CHECKSUM(log_desc) < 0)
+				printk(KERN_ERR "desc invalid\n");
+			if (log_desc->data.size == 0)
+				printk(KERN_ERR "desc is zero\n");
+#endif
+			/* This descriptor is affected so zero it out */
+			log_desc->data.size = 0;
+			PMEM_UPDATE_CHECKSUM(log_desc);
+
+			desc_index->data.oldest_index = i;
+			reuse_desc = 0;
+		} else
+			break;
+	} while (i != desc_index->data.curr_index);
+
+	/* Done updating the indexes so update the checksum */
+	PMEM_UPDATE_CHECKSUM(desc_index);
+	PMEM_FLUSH_CACHE(desc_index, sizeof(struct pmem_log_desc_index));
+
+	log_desc = region_data + sizeof (struct pmem_log_desc_index) +
+	    desc_index->data.curr_index * sizeof (struct pmem_log_desc);
+
+	spin_unlock_irqrestore(&hdl->elem->data_lock, flags);
+
+	/* Update the new descriptor */
+	do_gettimeofday(&tv);
+#ifdef	SUPPORT_HRTIME
+	log_desc->data.hrtime = gethrtime();
+#else
+	log_desc->data.hrtime = get_cycles();
+#endif
+	log_desc->data.sec = tv.tv_sec;
+	log_desc->data.usec = tv.tv_usec;
+	log_desc->data.size = bytes_written;
+	log_desc->data.offset = log_offset;
+	log_desc->data.log_checksum =
+	    pmem_checksum((__u32 *) buffer, bytes_written);
+	PMEM_UPDATE_CHECKSUM(log_desc);
+
+	/* Write the log */
+	__pmem_memcpy_toio((char *) region_data + log_desc->data.offset, buffer,
+	                  bytes_written);
+	PMEM_FLUSH_CACHE(log_desc, sizeof(struct pmem_log_desc));
+	PMEM_FLUSH_CACHE((char *) region_data + log_desc->data.offset,
+	                bytes_written);
+
+	return (bytes_written);
+}
+
+/* int pmem_write_data 
+ *
+ * Write data to persistent memory. 
+ * Returns the number of bytes written on success, and -errno on error */
+int pmem_write_data(pmem_handle_t handle, const char *buffer, int size)
+{
+	struct pmem_handle *hdl = (struct pmem_handle *) handle;
+	int bytes_written = -EINVAL;
+	void *data_start;
+
+	/* WARNING: The write_data function should be a totally quiet function
+	 * causing a printk here could end up being a recursive printk if
+	 * the PANIC_LOGS feature is on - PANIC_LOGS logs printk strings
+	 * to pmem and if we printk here on error, then that will put another
+	 * string in the kcore input buffer.  Bad. */
+	if (!pmem.enabled)
+		goto done;
+#ifdef CONFIG_PMEM_DEBUG
+	if (!handle || !buffer || (size < 0))
+		goto done;
+#endif
+
+	data_start = pmem_get_data_ptr(hdl);
+	if (!data_start)
+		goto done;
+
+	switch (hdl->type) {
+	case PMEM_HANDLE_TYPE_CB:
+	case PMEM_HANDLE_TYPE_SHADOW:
+		goto done;
+	case PMEM_HANDLE_TYPE_BLK:
+		/* determine if this is a locked segment */
+		if (0 != is_locked(handle)) {
+			bytes_written = -EPERM;
+			goto done;
+		}
+		/* fall through to the raw-write code */
+		break;
+	case PMEM_HANDLE_TYPE_PART:
+		/* fall through to the raw-write code */
+		break;
+	case PMEM_HANDLE_TYPE_REG:
+		{
+			struct region_handle *reg_hdl = handle;
+
+			/* determine if the region lives in a locked segment */
+			if (0 != is_locked(handle)) {
+				bytes_written = -EPERM;
+				goto done;
+			}
+
+			if (reg_hdl->elem->hdr->data.num_log_desc)
+				bytes_written = pmem_write_log_desc(reg_hdl,
+						  data_start, buffer, size);
+			else
+				bytes_written = pmem_write_region_data(reg_hdl,
+			                          data_start, buffer, size);
+
+			goto done;
+		}
+		break;
+	default:
+		PMEM_DPRINT("ERROR: Writing to unknown handle type %d\n", hdl->type);
+		goto done;
+		break;
+	}
+	
+	if (hdl->offset >= hdl->size)
+		goto done;
+
+	if (hdl->offset + size > hdl->size)
+		bytes_written = hdl->size - hdl->offset;
+	else
+		bytes_written = size;
+
+	__pmem_memcpy_toio(data_start + hdl->offset, buffer, bytes_written);
+
+	PMEM_FLUSH_CACHE(data_start + hdl->offset, bytes_written);
+	hdl->offset += bytes_written;
+
+      done:
+	return (bytes_written);
+}
+EXPORT_SYMBOL(pmem_write_data);
+
+/* int pmem_write_header 
+ *
+ * Write data to the header for the given handle 
+ * Returns the number of bytes written to the header on success
+ * and -errno on error.*/
+int pmem_write_header(pmem_handle_t handle, const char *buffer, int size,
+               loff_t offset)
+{
+	struct pmem_handle	*hdl = (struct pmem_handle*)handle;
+	void			*header_start;
+	int			header_size;
+	ssize_t			bytes_written = -EINVAL;
+
+	header_size = pmem_get_hdr_size(handle);
+	if (header_size <= 0) {
+		PMEM_DPRINT("ERROR: Can't write to an unsized header\n");
+		return bytes_written;
+	}
+	/* check to make sure that this header doesnt belong to a locked
+	 * segment/region */
+	if (0 != is_locked(handle)) {
+		PMEM_DPRINT("ERROR: Cant write to locked header\n");
+		return -EPERM;
+	}
+
+	if (size > header_size) {
+		size = header_size;
+	}
+
+	header_start = pmem_get_hdr_ptr(hdl);
+	if (!header_start) {
+		PMEM_DPRINT("ERROR: Can't write to a NULL header\n");
+		return bytes_written;
+	}
+	
+	if (size + offset > header_size) {
+		bytes_written = header_size - offset;
+	} else {
+		bytes_written = size;
+	}
+
+	__pmem_memcpy_toio(header_start + offset, buffer, bytes_written);
+	return bytes_written;
+}
+EXPORT_SYMBOL(pmem_write_header);
+
+/* int pmem_seek
+ * Update the given pmem handle current position according to the semantics
+ * of fseek()
+ */
+int pmem_seek(pmem_handle_t handle, int offset, int whence)
+{
+	struct pmem_handle *hdl = (struct pmem_handle *) handle;
+	int new_offset = -EINVAL;
+	void *data_start;
+
+#ifdef CONFIG_PMEM_DEBUG
+	if (!handle) {
+		PMEM_DPRINT("seek: invalid arguments\n");
+		goto done;
+	}
+#endif
+
+	data_start = pmem_get_data_ptr(hdl);
+	if (!data_start) {
+		PMEM_DPRINT("seek: invalid handle\n");
+		goto done;
+	}
+
+	switch (whence) {
+	case 0: /* SEEK_SET */
+		new_offset = offset;
+		break;
+	case 1: /* SEEK_CUR */
+		new_offset = hdl->offset + offset;
+		break;
+	case 2: /* SEEK_END */
+		new_offset = hdl->size - 1 + offset;
+		break;
+	default:
+		PMEM_DPRINT("seek: invalid whence, %d\n", whence);
+		goto done;
+		break;
+	}
+
+	if ((new_offset >= hdl->size) || (new_offset < 0)) {
+		new_offset = -EINVAL;
+		PMEM_DPRINT("invalid offset, offset=%d, size=%d\n", new_offset,
+			    hdl->size);
+		goto done;
+	}
+
+	hdl->offset = new_offset;
+
+      done:
+	return (new_offset);
+} 
+EXPORT_SYMBOL(pmem_seek);
+
diff --git a/mm/pmem/reg.c b/mm/pmem/reg.c
new file mode 100644
index 0000000..5fe2f6e
--- /dev/null
+++ b/mm/pmem/reg.c
@@ -0,0 +1,750 @@
+ /*
+ *  Copyright 2010 Wind River Systems
+ *
+ *  This program is free software; you can redistribute it and/or modify it
+ *  under the terms of the GNU General Public License as published by the
+ *  Free Software Foundation; either version 2 of the License, or (at your
+ *  option) any later version.
+ *
+ *
+ *  THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ *  WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ *  IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+ *  BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS
+ *  OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ *  ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR
+ *  TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
+ *  USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ *  You should have received a copy of the GNU General Public License along
+ *  with this program; if not, write to the Free Software Foundation, Inc.,
+ *  675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#include <linux/string.h>
+#include <linux/errno.h>
+#include <linux/spinlock.h>
+#include <linux/slab.h>
+#include <linux/kernel.h>
+#include <linux/time.h>
+#include <linux/module.h>
+#include <linux/pmem.h>
+
+#include <asm/types.h>
+
+
+/* Search through the allocation table and find a matching
+ * description string */
+static struct pmem_alloc_desc * pmem_get_alloc_desc(char *desc)
+{
+	struct pmem_alloc_desc *alloc_desc;
+	int i;
+
+	for (i = 0; i < pmem.pmem->data.num_alloc; i++) {
+		alloc_desc = PMEM_GET_ALLOC_DESC(i);
+
+		if (!strncmp(alloc_desc->data.desc, desc, PMEM_DESC_MAX))
+			return (alloc_desc);
+	}
+
+	return NULL;
+}
+
+/* Search through the region headers and find a matching
+ * description string */
+static struct pmem_region_hdr *
+pmem_get_region_hdr(struct pmem_part_hdr *part_hdr, char *desc)
+{
+	struct pmem_region_hdr *region_hdr;
+	int i;
+
+	for (i = 0; i < part_hdr->data.num_regions; i++) {
+		region_hdr = PMEM_GET_REGION_HDR(part_hdr, i);
+
+		if (!strncmp(region_hdr->data.desc, desc, PMEM_DESC_MAX))
+			return (region_hdr);
+	}
+
+	return NULL;
+}
+
+/* Search through the allocation table to find unallocated memory */
+static int pmem_find_free_alloc_mem(__u32 size, __u32 * offset)
+{
+	struct pmem_alloc_desc *alloc_desc;
+	__u32 alloc_table_size;
+	__u32 end_offset;
+
+	alloc_table_size = sizeof (struct pmem_cb_hdr) +
+	    (pmem.pmem->data.num_alloc + 1) * sizeof (struct pmem_alloc_desc);
+
+	/* Allocate before the last partition */
+	if (pmem.pmem->data.num_alloc > 0) {
+		alloc_desc = PMEM_GET_ALLOC_DESC(pmem.pmem->data.num_alloc - 1);
+		end_offset = alloc_desc->data.offset;
+	} else {
+		end_offset = pmem.size;
+	}
+
+	/* Check if there is space between the last partition and 
+	 * the allocation table */
+	if (end_offset < (size + alloc_table_size))
+	{
+		return -1;
+	}
+
+	*offset = end_offset - size;
+	return 0;
+}
+
+/* Search through the block to find unallocated memory */
+static int pmem_find_free_block_mem(struct pmem_part_hdr *part_hdr,
+		__u32 size, __u32 * offset)
+{
+	struct pmem_block_hdr *block_hdr;
+	struct pmem_region_hdr *region_hdr;
+
+	/* Just get any block since they all have the same size information */
+	block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, 0);
+
+	/* Allocate after the last region */
+	if (part_hdr->data.num_regions > 0) {
+		region_hdr =
+		    PMEM_GET_REGION_HDR(part_hdr,
+					part_hdr->data.num_regions - 1);
+		*offset = region_hdr->data.offset + region_hdr->data.size;
+	} else {
+		*offset = 0;
+	}
+
+	if ((block_hdr->data.size - *offset) >= size)
+		return 0;
+
+	PMEM_DPRINT("No space: size=%d, block size=%d, offset=%d\n", size,
+		    block_hdr->data.size, *offset);
+	PMEM_DPRINT("Max size that could be allocated is %d\n",
+			(block_hdr->data.size - *offset));
+
+	/* Could not find enough memory */
+	return -1;
+}
+
+/* Prepare a region with log descriptors in a block for writing */
+static void pmem_prepare_log_desc(struct pmem_part_hdr *part_hdr,
+		__u32 block_offset, struct pmem_region_hdr *region_hdr)
+{
+	struct pmem_log_desc_index *log_desc_index;
+	struct pmem_log_desc *log_desc;
+
+	/* Create the current index structure */
+	log_desc_index =
+	    (void *) part_hdr + block_offset + region_hdr->data.offset;
+
+	strncpy(log_desc_index->data.desc, region_hdr->data.desc,
+		PMEM_DESC_MAX);
+	log_desc_index->data.oldest_index = 0;
+	log_desc_index->data.curr_index = 0;
+	PMEM_UPDATE_CHECKSUM(log_desc_index);
+
+	/* Create the first log descriptor. The rest will be created as 
+	 * they are needed */
+	log_desc =
+	    (void *) log_desc_index + sizeof (struct pmem_log_desc_index);
+	log_desc->data.hrtime = 0;
+	log_desc->data.sec = 0;
+	log_desc->data.usec = 0;
+	log_desc->data.size = 0;
+	log_desc->data.offset = 0;
+	log_desc->data.log_checksum = 0;
+	PMEM_UPDATE_CHECKSUM(log_desc);
+}
+
+/* Prepare a region without log descriptors in a block for writing */
+static void pmem_prepare_region_data(struct pmem_part_hdr *part_hdr,
+		__u32 block_offset, struct pmem_region_hdr *region_hdr)
+{
+	struct pmem_region_data_hdr *region_data_hdr;
+	__u32 data_start;
+	int num_cpus, per_proc_size, index;
+	void *data_ptr;
+
+	/* Check for per processor logs */
+	if (region_hdr->data.flags & PMEM_REGION_FLAG_PERPROC)
+		num_cpus = PMEM_NUM_SUPPORT_CPUS;
+	else
+		num_cpus = 1;
+
+	data_ptr = (void *) part_hdr + block_offset + region_hdr->data.offset;
+	strncpy(data_ptr, region_hdr->data.desc, PMEM_DESC_MAX);
+
+	data_start =
+	    PMEM_DESC_MAX + num_cpus * sizeof (struct pmem_region_data_hdr);
+	per_proc_size = (region_hdr->data.size - data_start) / num_cpus;
+	region_data_hdr = data_ptr + PMEM_DESC_MAX;
+
+	/* Create the offset structures */
+	for (index = 0; index < num_cpus; index++) {
+		region_data_hdr->start_offset =
+		    data_start + index * per_proc_size;
+		region_data_hdr->end_offset =
+		    data_start + (index + 1) * per_proc_size;
+		region_data_hdr->current_offset = region_data_hdr->start_offset;
+		region_data_hdr->lost_logs = 0;
+
+		PMEM_DPRINT("REGION: start=%d, end=%d, curr=%d, lost=%d "
+		            "desc=%s\n", region_data_hdr->start_offset,
+		            region_data_hdr->end_offset,
+		            region_data_hdr->current_offset,
+		            region_data_hdr->lost_logs, region_hdr->data.desc);
+
+		region_data_hdr++;
+	}
+}
+
+/* The following functions are the default callbacks that are called when
+ * the pmemfs module is not loaded.  This is to avoid the need for locking
+ * when executing the callbacks or when setting the callbacks.
+ */
+void pmem_default_create_partition(pmem_handle_t hdl)
+{
+	return;
+}
+EXPORT_SYMBOL(pmem_default_create_partition);
+
+void pmem_default_create_region(pmem_handle_t parent, pmem_handle_t region)
+{
+	return;
+}
+EXPORT_SYMBOL(pmem_default_create_region);
+
+
+/* Prepare a region in a block for writing */
+void pmem_prepare_region(struct pmem_part_hdr *part_hdr, __u32 block_offset,
+		struct pmem_region_hdr *region_hdr)
+{
+	if (region_hdr->data.num_log_desc != 0)
+		pmem_prepare_log_desc(part_hdr, block_offset, region_hdr);
+	else
+		pmem_prepare_region_data(part_hdr, block_offset, region_hdr);
+}
+
+/* Register a new partition */
+int pmem_partition_reg(struct pmem_reg_part *partition, pmem_handle_t *handle)
+{
+	struct pmem_alloc_desc *alloc_desc;
+	struct pmem_part_hdr *part_hdr;
+	struct pmem_block_hdr *block_hdr;
+	struct part_list_elem *part_elem;
+	__u32 block_offset=0, new_offset, block_size=0, part_size;
+	unsigned int index, len;
+	unsigned long flags;
+	int rc = 0;
+
+	if (unlikely(!partition)) {
+		PMEM_DPRINT("ERROR: Invalid partition handle %p\n", partition);
+		rc = -EINVAL;
+		goto done;
+	}
+
+	if (!pmem.enabled) {
+		PMEM_DPRINT("Not enabled\n");
+		rc = -EINVAL;
+		goto done;
+	}
+
+	part_size = partition->size;
+
+	/* Ensure proper alignment. Pad the size if necessary */
+	if (part_size % PMEM_ALIGN_BYTES)
+		part_size += PMEM_ALIGN_BYTES - (part_size % PMEM_ALIGN_BYTES);
+
+	/* Calculate the block locations */
+	if (partition->num_blocks > 0) {
+		block_offset = PMEM_PART_HDR_MAX_SIZE;
+		block_size = part_size / partition->num_blocks;
+		block_size -= block_size % PMEM_ALIGN_BYTES;
+
+		if (block_size == 0) {
+			PMEM_DPRINT("Invalid block size\n");
+			rc = -EINVAL;
+			goto done;
+		}
+		/* check to make sure that the number of blocks will not 
+		 * cause a header overflow in the pmem partition header */
+		if ((sizeof(struct pmem_part_hdr) + partition->num_blocks * 
+		     sizeof(struct pmem_block_hdr)) > PMEM_PART_HDR_MAX_SIZE) {
+			PMEM_DPRINT("Invalid number of blocks\n");
+			rc = -EINVAL;
+			goto done;
+		}         
+		PMEM_DPRINT("block_offset=%d, block_size=%d\n", block_offset,
+			    block_size);
+	}
+
+	/* Validate string. Must not be null and must have a null character */
+	len = strnlen(partition->desc, PMEM_DESC_MAX);
+	if ((0 == len) || (PMEM_DESC_MAX == len)) {
+		PMEM_DPRINT("Invalid partition string\n");
+		rc = -EINVAL;
+		goto done;
+	}
+
+	/* Allocate the dynamic structures */
+	part_elem = kmalloc(sizeof (struct part_list_elem), GFP_KERNEL);
+	if (!part_elem) {
+		printk(KERN_ERR "PMEM: Unable to allocate memory\n");
+		rc = -ENOMEM;
+		goto done;
+	}
+
+	/* Everything looks good so lets try to register this partition */
+	spin_lock_irqsave(&pmem.lock, flags);
+
+	/* Check if the partition already exists */
+	alloc_desc = pmem_get_alloc_desc(partition->desc);
+	if (alloc_desc) {
+		/* Partition already exists. Validate the arguments */
+		part_hdr = PMEM_GET_PART_HDR(alloc_desc);
+
+		if (alloc_desc->data.type != PMEM_ALLOC_TYPE_LOG) {
+			PMEM_DPRINT("Already exists\n");
+			rc = -EEXIST;
+			goto release_done;
+		}
+
+		if ((part_hdr->data.size != part_size) ||
+		    (part_hdr->data.num_blocks != partition->num_blocks) ||
+		    (part_hdr->data.version != partition->version)) {
+			PMEM_DPRINT("Already exists\n");
+			rc = -EEXIST;
+			goto release_done;
+		}
+
+		spin_unlock_irqrestore(&pmem.lock, flags);
+		kfree(part_elem);
+
+		/* Arguments are good so return success */
+		rc = pmem_partition_get(partition->desc, handle);
+		goto done;
+	}
+
+	/* The partition does not exist. Try to allocate a new partition */
+
+	/* Get an offset to a suitable memory location */
+	if (pmem_find_free_alloc_mem(part_size + PMEM_PART_HDR_MAX_SIZE,
+				     &new_offset) < 0) {
+		PMEM_DPRINT("Unable to find free memory\n");
+		rc = -ENOMEM;
+		goto release_done;
+	}
+
+	PMEM_DPRINT("size=%d, new_offset=%d\n", part_size, new_offset);
+
+	part_hdr = (void *) pmem.pmem + new_offset;
+
+	/* Fill in the header information */
+	strncpy(part_hdr->data.desc, partition->desc, PMEM_DESC_MAX);
+	if (len < PMEM_DESC_MAX) {
+		/* use the __pmem_memset command here since the data lives in 
+		 * pmem not a dynamic DS */
+		__pmem_memset(&part_hdr->data.desc[len], 0, PMEM_DESC_MAX - len);
+	}
+
+	part_hdr->data.size = part_size;
+	part_hdr->data.num_blocks = partition->num_blocks;
+	part_hdr->data.num_regions = 0;
+	part_hdr->data.flags = 0;
+	part_hdr->data.version = partition->version;
+
+	PMEM_UPDATE_CHECKSUM(part_hdr);
+
+	/* Update the allocation descriptor */
+	index = pmem.pmem->data.num_alloc;
+	alloc_desc = PMEM_GET_ALLOC_DESC(index);
+	strncpy(alloc_desc->data.desc, partition->desc, PMEM_DESC_MAX);
+	if (len < PMEM_DESC_MAX)
+		__pmem_memset(&alloc_desc->data.desc[len], 0, PMEM_DESC_MAX - len);
+
+	alloc_desc->data.offset = new_offset;
+	alloc_desc->data.size = part_size + PMEM_PART_HDR_MAX_SIZE;
+	alloc_desc->data.type = PMEM_ALLOC_TYPE_LOG;
+
+	PMEM_UPDATE_CHECKSUM(alloc_desc);
+
+	/* Update the control block header */
+	pmem.pmem->data.num_alloc = index + 1;
+	PMEM_UPDATE_CB_CHECKSUM(pmem.pmem);
+
+	/* We have done enough so that no one else will register the same 
+	 * partition so unlock the headers */
+	spin_unlock_irqrestore(&pmem.lock, flags);
+
+	part_elem->hdr = part_hdr;
+	part_elem->active_block_offset = 0;
+	INIT_LIST_HEAD(&part_elem->region_list);
+
+	/* Update the block headers */
+	for (index = 0; index < partition->num_blocks; index++) {
+		block_hdr = PMEM_GET_BLOCK_HDR(part_hdr, index);
+
+		block_hdr->data.offset = block_offset + block_size * index;
+		block_hdr->data.size = block_size;
+		block_hdr->data.sec = 0;
+		block_hdr->data.usec = 0;
+		block_hdr->data.flags = 0;
+
+		/* Make one of the blocks active */
+		if (index == 0) {
+			struct timeval tv;
+
+			block_hdr->data.flags = PMEM_BLOCK_FLAG_ACTIVE;
+			part_elem->active_block_offset = block_hdr->data.offset;
+			do_gettimeofday(&tv);
+			block_hdr->data.sec = tv.tv_sec;
+			block_hdr->data.usec = tv.tv_usec;
+		}
+
+		PMEM_UPDATE_CHECKSUM(block_hdr);
+	}
+
+	/* Clear the partition memory area in persistent memory */
+	pmem_memset((void *) part_hdr + PMEM_PART_HDR_MAX_SIZE, 0, part_size);
+
+	spin_lock_irqsave(&pmem.lock, flags);
+
+	/* Partition is ready for use so add it to the list */
+	list_add(&(part_elem->list_elem), &pmem.part_list);
+
+	spin_unlock_irqrestore(&pmem.lock, flags);
+
+	rc = pmem_partition_get(partition->desc, handle);
+	
+	/* Notify any listeners that there is a new partition */
+	pmem_events.create_partition(*handle);
+	goto done;
+
+      release_done:
+	spin_unlock_irqrestore(&pmem.lock, flags);
+	kfree(part_elem);
+
+      done:
+	return (rc);
+}
+EXPORT_SYMBOL(pmem_partition_reg);
+
+/* Register a region. Returns a region handle if successful */
+int pmem_region_reg(pmem_handle_t part_handle,
+		struct pmem_reg_region *region, pmem_handle_t * handle)
+{
+	struct part_handle *hdl = (struct part_handle *) part_handle;
+	struct part_list_elem *part_elem;
+	struct region_list_elem *region_elem;
+	struct pmem_region_hdr *region_hdr;
+	__u32 new_offset, region_size;
+	unsigned long flags;
+	int rc = -EINVAL;
+	unsigned int len, next;
+
+	if (!part_handle || !region) {
+		PMEM_DPRINT("NULL arguments\n");
+		goto done;
+	}
+
+	/* Ensure that it is a partition handle */
+	if (hdl->hdl.type != PMEM_HANDLE_TYPE_PART) {
+		PMEM_DPRINT("Invalid handle at %p\n", part_handle);
+		goto done;
+	}
+
+	part_elem = hdl->elem;
+
+	/* Validate the arguments */
+	if (region->flags >> PMEM_REGION_NUM_FLAGS) {
+		PMEM_DPRINT("Invalid flags\n");
+		goto done;
+	}
+
+	if (region->size == 0) {
+		PMEM_DPRINT("Invalid size %d\n", region->size);
+		goto done;
+	}
+
+	region_size = region->size;
+
+	/* Validate string. Must not be null and must have a null character */
+	len = strnlen(region->desc, PMEM_DESC_MAX);
+	if ((0 == len) || (PMEM_DESC_MAX == len)) {
+		PMEM_DPRINT("Invalid region string\n");
+		goto done;
+	}
+
+	/* Ensure proper alignment. Pad the size if necessary */
+	if (region_size % PMEM_ALIGN_BYTES)
+		region_size += PMEM_ALIGN_BYTES -
+		    (region_size % PMEM_ALIGN_BYTES);
+
+	if ((region->num_log_desc != 0)
+	    && ((region->flags & PMEM_REGION_FLAG_PERPROC) ||
+		(region->fixed_size != 0))) {
+		PMEM_DPRINT("Invalid flags\n");
+		goto done;
+	}
+
+	if ((region->num_log_desc * sizeof (struct pmem_log_desc)
+	     + sizeof (struct pmem_log_desc_index))
+	    > region_size) {
+		PMEM_DPRINT("Invalid number of descriptors %x\n",
+			    region->num_log_desc);
+		goto done;
+	}
+
+	if ((region->fixed_size + sizeof (struct pmem_region_data_hdr)) >
+	    region_size) {
+		PMEM_DPRINT("Invalid fixed log size %x\n", region->fixed_size);
+		goto done;
+	}
+
+	if (part_elem->hdr->data.num_blocks == 0) {
+		PMEM_DPRINT("No blocks for region\n");
+		goto done;
+	}
+
+	region_elem = kmalloc(sizeof (struct region_list_elem), GFP_KERNEL);
+	if (!region_elem) {
+		PMEM_DPRINT("Unable to allocate memory\n");
+		rc = -ENOMEM;
+		goto done;
+	}
+
+#ifdef CONFIG_PMEM_DEBUG
+	if (PMEM_VALIDATE_CHECKSUM(part_elem->hdr) < 0) {
+		PMEM_DPRINT("Partition header invalid\n");
+		goto done;
+	}
+#endif
+	/* Everything looks good so lets try to register this region */
+	spin_lock_irqsave(&pmem.lock, flags);
+
+	/* Check if the region is already registered */
+	region_hdr = pmem_get_region_hdr(part_elem->hdr, region->desc);
+	if (region_hdr) {
+		/* Region is registered. Check if the arguments match */
+		if ((region_hdr->data.size != region_size) ||
+		    (region_hdr->data.fixed_size != region->fixed_size) ||
+		    (region_hdr->data.flags != region->flags) ||
+		    (region_hdr->data.num_log_desc != region->num_log_desc) ||
+		    (region_hdr->data.version != region->version)) {
+			PMEM_DPRINT("Region already exists\n");
+			rc = -EEXIST;
+			goto release_done;
+		}
+
+		spin_unlock_irqrestore(&pmem.lock, flags);
+		kfree(region_elem);
+
+		/* Arguments are good so return success */
+		rc = pmem_region_get(hdl, region->desc, region->block_id,
+				     handle);
+		goto done;
+	}
+
+	/* Region is not registered. Try to allocate a new one */
+
+	/* Get an area in the block */
+	if (pmem_find_free_block_mem(part_elem->hdr, region_size, &new_offset)
+	    < 0) {
+		PMEM_DPRINT("Unable to allocate space for part [%s]\n",
+		            region->desc);
+		rc = -ENOSPC;
+		goto release_done;
+	}
+
+	/* determine the id of the next region to be allocated */
+	next = part_elem->hdr->data.num_regions;
+	/* need to increment the number of regions here before success
+	 * so that the GET_REGION_HDR command will not return NULL.  The 
+	 * older non-safe version of GET_REGION_HDR didnt care, but the new
+	 * version does */
+	part_elem->hdr->data.num_regions++;
+	region_hdr = PMEM_GET_REGION_HDR(part_elem->hdr, next);
+	/* Check the region header does not overflow the first page of 
+	 * the partition */
+	if ((((long)region_hdr + 1) - (long)part_elem->hdr) > PMEM_PART_HDR_MAX_SIZE) {
+		PMEM_DPRINT("Unable to allocate space for region's header [%s]\n", region->desc);
+		/* not successful */
+		part_elem->hdr->data.num_regions--;
+		PMEM_UPDATE_CHECKSUM(part_elem->hdr);
+		rc = -ENOSPC;
+		goto release_done;
+	}               
+
+	region_elem->hdr = region_hdr;
+	region_elem->data_lock = SPIN_LOCK_UNLOCKED;
+	region_elem->ptr_block = NULL;
+
+	region_hdr->data.size = region_size;
+	region_hdr->data.offset = new_offset;
+	region_hdr->data.flags = region->flags;
+	region_hdr->data.fixed_size = region->fixed_size;
+	region_hdr->data.num_log_desc = region->num_log_desc;
+	region_hdr->data.version = region->version;
+	strncpy(region_hdr->data.desc, region->desc, PMEM_DESC_MAX);
+	if (len < PMEM_DESC_MAX)
+		pmem_memset(&region_hdr->data.desc[len], 0, PMEM_DESC_MAX - len);
+
+	PMEM_UPDATE_CHECKSUM(region_hdr);
+	PMEM_UPDATE_CHECKSUM(part_elem->hdr);
+
+	/* We have done enough so that no one else will register the same 
+	 * partition so unlock the headers */
+	spin_unlock_irqrestore(&pmem.lock, flags);
+
+	/* Update the region data area in the block */
+	pmem_prepare_region(part_elem->hdr, part_elem->active_block_offset,
+			region_hdr);
+
+	spin_lock_irqsave(&pmem.lock, flags);
+
+	/* Region is ready for use so add it to the list */
+	list_add(&(region_elem->list_elem), &(part_elem->region_list));
+
+	spin_unlock_irqrestore(&pmem.lock, flags);
+
+	rc = pmem_region_get(hdl, region->desc, region->block_id, handle);
+	
+	/* Notify any listeners that there is a new region in this partition */
+	pmem_events.create_region(hdl, *handle);
+	goto done;
+
+      release_done:
+	spin_unlock_irqrestore(&pmem.lock, flags);
+	kfree(region_elem);
+
+      done:
+	return (rc);
+}
+EXPORT_SYMBOL(pmem_region_reg);
+
+/* Register a pointer block with a region. */
+int pmem_register_ptr_block(pmem_handle_t region_handle,
+		struct pmem_ptr_block *ptr_block)
+{
+	struct region_handle *hdl = (struct region_handle *)region_handle;
+	struct pmem_block_hdr *block_hdr;
+	struct pmem_region_hdr *region_hdr;
+	int rc = 0;
+	int locked;
+	unsigned long flags;
+
+	if (hdl->hdl.type != PMEM_HANDLE_TYPE_REG) {
+		PMEM_DPRINT("Invalid handle type\n");
+		rc = -EINVAL;
+		goto done;
+	}
+
+	region_hdr = hdl->elem->hdr;
+
+	/* Special handles are only valid for fixed per-processor logs */
+	if ((region_hdr->data.num_log_desc != 0) ||
+	    !(region_hdr->data.flags & PMEM_REGION_FLAG_PERPROC)) {
+		PMEM_DPRINT("Invalid region for handle\n");
+		rc = -EINVAL;
+		goto done;
+	}
+
+	spin_lock_irqsave(&pmem.lock, flags);
+
+	/* check to see if this block is locked */
+	block_hdr = PMEM_GET_BLOCK_HDR(hdl->parent->elem->hdr,
+	                               hdl->block_id);
+	locked = (block_hdr->data.flags & PMEM_BLOCK_FLAG_LOCK);
+	
+	/* Disallow multiple registrations */
+	if (hdl->elem->ptr_block) {
+		spin_unlock_irqrestore(&pmem.lock, flags);
+		PMEM_DPRINT("block already registered\n");
+		rc = -EINVAL;
+		goto done;
+	}
+
+	hdl->elem->ptr_block = ptr_block;
+
+	spin_unlock_irqrestore(&pmem.lock, flags);
+
+	/* Fill in the handle data */
+	pmem_update_ptr_block(hdl->parent->elem, hdl->elem, locked);
+
+      done:
+	return (rc);
+}
+EXPORT_SYMBOL(pmem_register_ptr_block);
+
+/* Update the pointer block with the location of the region in the active 
+ * block.  If lock is set, then this routine will immediately lock the
+ * ptr block to prevent logging. */
+void pmem_update_ptr_block(struct part_list_elem *part_elem,
+                           struct region_list_elem *region_elem, int lock)
+{
+	struct pmem_region_data_hdr *region_data_hdr;
+	struct pmem_cpu_ptrs *ptrs = NULL;
+	void *region_data;
+	__u32 end_offset;
+	int num_cpus;
+	int i = 0;
+
+	if (!region_elem->ptr_block)
+		return;
+
+	if (region_elem->hdr->data.flags & PMEM_REGION_FLAG_PERPROC)
+		num_cpus = PMEM_NUM_SUPPORT_CPUS;
+	else
+		num_cpus = 1;
+
+	region_data = (void *) part_elem->hdr + part_elem->active_block_offset +
+	    region_elem->hdr->data.offset;
+
+	/* Setup the array pointers for each cpu */
+	for (i = 0; i < num_cpus; i++) {
+		region_data_hdr =
+		    (struct pmem_region_data_hdr *) (region_data +
+						     PMEM_DESC_MAX) + i;
+
+		/* Ensure that the end offset is a multiple of the logs */
+		end_offset = region_data_hdr->end_offset -
+		    ((region_data_hdr->end_offset -
+		      region_data_hdr->start_offset) %
+		     region_elem->hdr->data.fixed_size);
+
+		/* Update the storage that is not currently in use, mark the
+		 * unused storage area by setting its start ptr to NULL */
+		if (region_elem->ptr_block->cpu[i].ptrs ==
+		    &region_elem->ptr_block->cpu[i].storeA) {
+			ptrs = &region_elem->ptr_block->cpu[i].storeB;
+			region_elem->ptr_block->cpu[i].storeA.start = NULL;
+		} else {
+			ptrs = &region_elem->ptr_block->cpu[i].storeA;
+			region_elem->ptr_block->cpu[i].storeB.start = NULL;
+		}
+
+		ptrs->start = region_data + region_data_hdr->start_offset;
+		ptrs->end = region_data + end_offset;
+		ptrs->curr = ptrs->start;
+		ptrs->lost_logs = &region_data_hdr->lost_logs;
+
+#ifdef CONFIG_PMEM_DEBUG
+		if ((ptrs->start < (char *) pmem.pmem) ||
+		    (ptrs->end > ((char *) pmem.pmem + pmem.size))) {
+			PMEM_DPRINT("Invalid pointers\n");
+			return;
+		}
+#endif
+		if (0 == lock) {
+			/* Make the new pointer block active only if the 
+			 * block is not locked, otherwise they stay in
+			 * storeA/storeB */
+			region_elem->ptr_block->cpu[i].ptrs = ptrs;
+		}
+	}
+}
-- 
1.6.5.2

