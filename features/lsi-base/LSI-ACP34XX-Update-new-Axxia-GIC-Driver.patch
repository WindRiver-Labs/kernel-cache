From 148191bde625aeff7549d53c5be0cee595f3125b Mon Sep 17 00:00:00 2001
From: David Mercado <david.mercado@windriver.com>
Date: Fri, 22 Nov 2013 08:23:36 -0500
Subject: [PATCH 155/159] LSI ACP34XX: Update new Axxia GIC Driver

git.yoctoproject.org/git/linux-yocto-3.10
commit ba251fcf654cd1f17e72a17f7e0ea903f33f92e0 standard/axxia/base.

Update driver with various improvements (e.g., removed more magic
numbers, improved IPI multiplex code, fixed set_affinity to be
CPU hotplug compatible, etc.).

Signed-off-by: David Mercado <david.mercado@windriver.com>
---
 arch/arm/mach-axxia/axxia-gic.c              |  431 +++++++++++++++-----------
 arch/arm/mach-axxia/axxia.c                  |    3 +-
 arch/arm/mach-axxia/include/mach/axxia-gic.h |    4 +-
 arch/arm/mach-axxia/platsmp.c                |    4 +-
 4 files changed, 252 insertions(+), 190 deletions(-)

diff --git a/arch/arm/mach-axxia/axxia-gic.c b/arch/arm/mach-axxia/axxia-gic.c
index 291e90a..d2edcef 100644
--- a/arch/arm/mach-axxia/axxia-gic.c
+++ b/arch/arm/mach-axxia/axxia-gic.c
@@ -59,6 +59,8 @@
 #include <mach/axxia-gic.h>
 
 #define MAX_GIC_INTERRUPTS  1020
+#define MAX_NUM_CLUSTERS    4
+#define CORES_PER_CLUSTER   4
 
 static u32 irq_cpuid[MAX_GIC_INTERRUPTS];
 static void __iomem *ipi_mask_reg_base;
@@ -85,9 +87,13 @@ enum axxia_ext_ipi_num {
 	MAX_AXM_IPI_NUM
 };
 
-#define AXXIA_RPC 0xff /* Some big arbritary number */
+/*
+ * Some big arbritary number that won't ever conflict with
+ * the IPI numbers defined in arch/arm/kernel/smp.c
+ */
+#define AXXIA_RPC 0xff
 
-/* We pack these into an integer, so four is the max! */
+/* RPC Message types. */
 enum axxia_mux_msg_type {
 	MUX_MSG_CALL_FUNC = 0,
 	MUX_MSG_CALL_FUNC_SINGLE,
@@ -105,7 +111,6 @@ static void muxed_ipi_message_pass(const struct cpumask *mask,
 				   enum axxia_mux_msg_type ipi_num)
 {
 	struct axxia_mux_msg *info;
-	char *message;
 	int cpu;
 
 	/*
@@ -115,8 +120,7 @@ static void muxed_ipi_message_pass(const struct cpumask *mask,
 
 	for_each_cpu(cpu, mask) {
 		info = &per_cpu(ipi_mux_msg, cpu_logical_map(cpu));
-		message = (char *)&info->msg;
-		message[ipi_num] = 1;
+		info->msg |= 1 << ipi_num;
 	}
 }
 
@@ -129,25 +133,14 @@ static void axxia_ipi_demux(struct pt_regs *regs)
 
 	do {
 		all = xchg(&info->msg, 0);
-#ifdef __LITTLE_ENDIAN
-		if (all & (1 << (8 * MUX_MSG_CALL_FUNC)))
-			handle_IPI(3, regs); /* 3 = ARM IPI_CALL_FUNC */
-		if (all & (1 << (8 * MUX_MSG_CALL_FUNC_SINGLE)))
-			handle_IPI(4, regs); /* 4 = ARM IPI_CALL_FUNC_SINGLE */
-		if (all & (1 << (8 * MUX_MSG_CPU_STOP)))
-			handle_IPI(5, regs); /* 5 = ARM IPI_CPU_STOP */
-		if (all & (1 << (8 * MUX_MSG_CPU_WAKEUP)))
-			handle_IPI(0, regs); /* 0 = ARM IPI_WAKEUP */
-#else
-		if (all & (1 << (24 - 8 * MUX_MSG_CALL_FUNC)))
+		if (all & (1 << MUX_MSG_CALL_FUNC))
 			handle_IPI(3, regs); /* 3 = ARM IPI_CALL_FUNC */
-		if (all & (1 << (24 - 8 * MUX_MSG_CALL_FUNC_SINGLE)))
+		if (all & (1 << MUX_MSG_CALL_FUNC_SINGLE))
 			handle_IPI(4, regs); /* 4 = ARM IPI_CALL_FUNC_SINGLE */
-		if (all & (1 << (24 - 8 * MUX_MSG_CPU_STOP)))
+		if (all & (1 << MUX_MSG_CPU_STOP))
 			handle_IPI(5, regs); /* 5 = ARM IPI_CPU_STOP */
-		if (all & (1 << (24 - 8 * MUX_MSG_CPU_WAKEUP)))
+		if (all & (1 << MUX_MSG_CPU_WAKEUP))
 			handle_IPI(0, regs); /* 0 = ARM IPI_WAKEUP */
-#endif
 	} while (info->msg);
 }
 
@@ -172,7 +165,7 @@ struct gic_chip_data {
 
 static DEFINE_RAW_SPINLOCK(irq_controller_lock);
 
-static struct gic_chip_data gic_data __read_mostly;
+static struct gic_chip_data gic_data[MAX_NUM_CLUSTERS] __read_mostly;
 
 #define gic_data_dist_base(d)	((d)->dist_base.common_base)
 #define gic_data_cpu_base(d)	((d)->cpu_base.common_base)
@@ -233,6 +226,10 @@ static void axxia_gic_run_gic_rpc(int cpu, axxia_call_func_t *func, void *info)
 	struct axxia_gic_rpc *slot = &__get_cpu_var(axxia_gic_rpc);
 	int timeout;
 
+	/* If the target CPU isn't online, don't bother. */
+	if (!cpu_online(cpu))
+		return;
+
 	slot->cpu = cpu;
 	slot->info = info;
 	dsb();
@@ -298,7 +295,8 @@ static void gic_mask_irq(struct irq_data *d)
 	 * the IRQ masking directly. Otherwise, use the IPI mechanism
 	 * to remotely do the masking.
 	 */
-	if ((irq_cpuid[irqid] / 4) == (pcpu / 4))
+	if ((irq_cpuid[irqid] / CORES_PER_CLUSTER) ==
+		(pcpu / CORES_PER_CLUSTER))
 		_gic_mask_irq(d);
 	else
 		axxia_gic_run_gic_rpc(irq_cpuid[irqid], _gic_mask_irq, d);
@@ -341,7 +339,8 @@ static void gic_unmask_irq(struct irq_data *d)
 	 * the IRQ masking directly. Otherwise, use the IPI mechanism
 	 * to remotely do the masking.
 	 */
-	if ((irq_cpuid[irqid] / 4) == (pcpu / 4))
+	if ((irq_cpuid[irqid] / CORES_PER_CLUSTER) ==
+		(pcpu / CORES_PER_CLUSTER))
 		_gic_unmask_irq(d);
 	else
 		axxia_gic_run_gic_rpc(irq_cpuid[irqid], _gic_unmask_irq, d);
@@ -407,17 +406,18 @@ struct gic_set_type_wrapper_struct {
 
 static void gic_set_type_wrapper(void *data)
 {
-	struct gic_set_type_wrapper_struct *pArgs =
+	struct gic_set_type_wrapper_struct *args =
 		(struct gic_set_type_wrapper_struct *)data;
 
-	pArgs->status = _gic_set_type(pArgs->d, pArgs->type);
+	args->status = _gic_set_type(args->d, args->type);
 	dmb();
 }
 #endif
 
 static int gic_set_type(struct irq_data *d, unsigned int type)
 {
-	int i, cpu, nr_cluster_ids = ((nr_cpu_ids-1) / 4) + 1;
+	int i, j, cpu;
+	int nr_cluster_ids = ((nr_cpu_ids - 1) / CORES_PER_CLUSTER) + 1;
 	unsigned int gicirq = gic_irq(d);
 	u32 pcpu = cpu_logical_map(smp_processor_id());
 	struct gic_set_type_wrapper_struct data;
@@ -445,16 +445,23 @@ static int gic_set_type(struct irq_data *d, unsigned int type)
 	data.d = d;
 	data.type = type;
 	for (i = 0; i < nr_cluster_ids; i++) {
-		if (i == (pcpu / 4))
+		if (i == (pcpu / CORES_PER_CLUSTER))
 			continue;
 
-		/* Have the first cpu in each cluster execute this. */
-		cpu = i * 4;
-		if (cpu_online(cpu)) {
-			axxia_gic_run_gic_rpc(cpu, gic_set_type_wrapper, &data);
-			if (data.status != 0)
-				pr_err("Failed to set IRQ type for cpu%d\n",
-				       cpu);
+		/*
+		 * Have some core in each cluster execute this,
+		 * Start with the first core on that cluster.
+		 */
+		cpu = i * CORES_PER_CLUSTER;
+		for (j = cpu; j < cpu + CORES_PER_CLUSTER; j++) {
+			if (cpu_online(j)) {
+				axxia_gic_run_gic_rpc(j, gic_set_type_wrapper,
+						      &data);
+				if (data.status != 0)
+					pr_err("IRQ set type error for cpu%d\n",
+					       j);
+				break;
+			}
 		}
 	}
 	return ret;
@@ -476,12 +483,12 @@ struct gic_set_affinity_wrapper_struct {
 
 static void _gic_set_affinity(void *data)
 {
-	struct gic_set_affinity_wrapper_struct *pArgs =
+	struct gic_set_affinity_wrapper_struct *args =
 		(struct gic_set_affinity_wrapper_struct *)data;
-	void __iomem *reg  = gic_dist_base(pArgs->d) +
-			     GIC_DIST_TARGET + (gic_irq(pArgs->d) & ~3);
-	unsigned int shift = (gic_irq(pArgs->d) % 4) * 8;
-	unsigned int cpu = cpumask_any_and(pArgs->mask_val, cpu_online_mask);
+	void __iomem *reg  = gic_dist_base(args->d) +
+			     GIC_DIST_TARGET + (gic_irq(args->d) & ~3);
+	unsigned int shift = (gic_irq(args->d) % 4) * 8;
+	unsigned int cpu = cpumask_any_and(args->mask_val, cpu_online_mask);
 	u32 val, affinity_mask, affinity_bit;
 	u32 enable_mask, enable_offset;
 
@@ -489,21 +496,22 @@ static void _gic_set_affinity(void *data)
 	 * Normalize the cpu number as seen by Linux (0-15) to a
 	 * number as seen by a cluster (0-3).
 	 */
-	affinity_bit = 1 << ((cpu_logical_map(cpu) % 4) + shift);
+	affinity_bit = 1 << ((cpu_logical_map(cpu) % CORES_PER_CLUSTER) +
+				shift);
 	affinity_mask = 0xff << shift;
 
-	enable_mask = 1 << (gic_irq(pArgs->d) % 32);
-	enable_offset = 4 * (gic_irq(pArgs->d) / 32);
+	enable_mask = 1 << (gic_irq(args->d) % 32);
+	enable_offset = 4 * (gic_irq(args->d) / 32);
 
 	raw_spin_lock(&irq_controller_lock);
 	val = readl_relaxed(reg) & ~affinity_mask;
-	if (pArgs->disable == true) {
+	if (args->disable == true) {
 		writel_relaxed(val, reg);
-		writel_relaxed(enable_mask, gic_data_dist_base(&gic_data)
+		writel_relaxed(enable_mask, gic_data_dist_base(&gic_data[0])
 				+ GIC_DIST_ENABLE_CLEAR + enable_offset);
 	} else {
 		writel_relaxed(val | affinity_bit, reg);
-		writel_relaxed(enable_mask, gic_data_dist_base(&gic_data)
+		writel_relaxed(enable_mask, gic_data_dist_base(&gic_data[0])
 				+ GIC_DIST_ENABLE_SET + enable_offset);
 	}
 	raw_spin_unlock(&irq_controller_lock);
@@ -546,7 +554,8 @@ static int gic_set_affinity(struct irq_data *d,
 	data.mask_val = mask_val;
 	data.disable = false;
 
-	if ((cpu_logical_map(cpu) / 4) == (pcpu / 4))
+	if ((cpu_logical_map(cpu) / CORES_PER_CLUSTER) ==
+		(pcpu / CORES_PER_CLUSTER))
 		_gic_set_affinity(&data);
 	else
 		axxia_gic_run_gic_rpc(cpu, _gic_set_affinity, &data);
@@ -556,14 +565,16 @@ static int gic_set_affinity(struct irq_data *d,
 	 * different than the prior cluster, remove the IRQ affinity
 	 * on the old cluster.
 	 */
-	if ((cpu_logical_map(cpu) / 4) != (irq_cpuid[irqid] / 4)) {
+	if ((cpu_logical_map(cpu) / CORES_PER_CLUSTER) !=
+		(irq_cpuid[irqid] / CORES_PER_CLUSTER)) {
 		/*
 		 * If old cpu assignment falls within the same cluster as
 		 * the cpu we're currently running on, set the IRQ affinity
 		 * directly. Otherwise, use IPI mechanism.
 		 */
 		data.disable = true;
-		if ((irq_cpuid[irqid] / 4) == (pcpu / 4))
+		if ((irq_cpuid[irqid] / CORES_PER_CLUSTER) ==
+			(pcpu / CORES_PER_CLUSTER))
 			_gic_set_affinity(&data);
 		else
 			axxia_gic_run_gic_rpc(irq_cpuid[irqid],
@@ -590,10 +601,11 @@ static int gic_set_wake(struct irq_data *d, unsigned int on)
 #define gic_set_wake	NULL
 #endif
 
+static
 asmlinkage void __exception_irq_entry axxia_gic_handle_irq(struct pt_regs *regs)
 {
 	u32 irqstat, irqnr;
-	struct gic_chip_data *gic = &gic_data;
+	struct gic_chip_data *gic = &gic_data[0]; /* OK to always use 0 */
 	void __iomem *cpu_base = gic_data_cpu_base(gic);
 
 	do {
@@ -814,6 +826,21 @@ static void __cpuinit gic_cpu_init(struct gic_chip_data *gic)
 }
 
 #ifdef CONFIG_CPU_PM
+
+static u32 get_cluster_id(void)
+{
+	u32 mpidr, cluster;
+
+	mpidr = read_cpuid_mpidr();
+	cluster = MPIDR_AFFINITY_LEVEL(mpidr, 1);
+
+	/* Cluster ID should always be between 0 and 3. */
+	if (cluster >= MAX_NUM_CLUSTERS)
+		cluster = 0;
+
+	return cluster;
+}
+
 /*
  * Saves the GIC distributor registers during suspend or idle.  Must be called
  * with interrupts disabled but before powering down the GIC.  After calling
@@ -825,23 +852,26 @@ static void gic_dist_save(void)
 	unsigned int gic_irqs;
 	void __iomem *dist_base;
 	int i;
+	u32 this_cluster;
 
-	gic_irqs = gic_data.gic_irqs;
-	dist_base = gic_data_dist_base(&gic_data);
+	this_cluster = get_cluster_id();
+
+	gic_irqs = gic_data[this_cluster].gic_irqs;
+	dist_base = gic_data_dist_base(&gic_data[this_cluster]);
 
 	if (!dist_base)
 		return;
 
 	for (i = 0; i < DIV_ROUND_UP(gic_irqs, 16); i++)
-		gic_data.saved_spi_conf[i] =
+		gic_data[this_cluster].saved_spi_conf[i] =
 			readl_relaxed(dist_base + GIC_DIST_CONFIG + i * 4);
 
 	for (i = 0; i < DIV_ROUND_UP(gic_irqs, 4); i++)
-		gic_data.saved_spi_target[i] =
+		gic_data[this_cluster].saved_spi_target[i] =
 			readl_relaxed(dist_base + GIC_DIST_TARGET + i * 4);
 
 	for (i = 0; i < DIV_ROUND_UP(gic_irqs, 32); i++)
-		gic_data.saved_spi_enable[i] =
+		gic_data[this_cluster].saved_spi_enable[i] =
 			readl_relaxed(dist_base + GIC_DIST_ENABLE_SET + i * 4);
 }
 
@@ -857,9 +887,12 @@ static void gic_dist_restore(void)
 	unsigned int gic_irqs;
 	unsigned int i;
 	void __iomem *dist_base;
+	u32 this_cluster;
+
+	this_cluster = get_cluster_id();
 
-	gic_irqs = gic_data.gic_irqs;
-	dist_base = gic_data_dist_base(&gic_data);
+	gic_irqs = gic_data[this_cluster].gic_irqs;
+	dist_base = gic_data_dist_base(&gic_data[this_cluster]);
 
 	if (!dist_base)
 		return;
@@ -867,7 +900,7 @@ static void gic_dist_restore(void)
 	writel_relaxed(0, dist_base + GIC_DIST_CTRL);
 
 	for (i = 0; i < DIV_ROUND_UP(gic_irqs, 16); i++)
-		writel_relaxed(gic_data.saved_spi_conf[i],
+		writel_relaxed(gic_data[this_cluster].saved_spi_conf[i],
 			dist_base + GIC_DIST_CONFIG + i * 4);
 
 	for (i = 0; i < DIV_ROUND_UP(gic_irqs, 4); i++)
@@ -875,11 +908,11 @@ static void gic_dist_restore(void)
 			dist_base + GIC_DIST_PRI + i * 4);
 
 	for (i = 0; i < DIV_ROUND_UP(gic_irqs, 4); i++)
-		writel_relaxed(gic_data.saved_spi_target[i],
+		writel_relaxed(gic_data[this_cluster].saved_spi_target[i],
 			dist_base + GIC_DIST_TARGET + i * 4);
 
 	for (i = 0; i < DIV_ROUND_UP(gic_irqs, 32); i++)
-		writel_relaxed(gic_data.saved_spi_enable[i],
+		writel_relaxed(gic_data[this_cluster].saved_spi_enable[i],
 			dist_base + GIC_DIST_ENABLE_SET + i * 4);
 
 	writel_relaxed(1, dist_base + GIC_DIST_CTRL);
@@ -891,18 +924,21 @@ static void gic_cpu_save(void)
 	u32 *ptr;
 	void __iomem *dist_base;
 	void __iomem *cpu_base;
+	u32 this_cluster;
 
-	dist_base = gic_data_dist_base(&gic_data);
-	cpu_base = gic_data_cpu_base(&gic_data);
+	this_cluster = get_cluster_id();
+
+	dist_base = gic_data_dist_base(&gic_data[this_cluster]);
+	cpu_base = gic_data_cpu_base(&gic_data[this_cluster]);
 
 	if (!dist_base || !cpu_base)
 		return;
 
-	ptr = __this_cpu_ptr(gic_data.saved_ppi_enable);
+	ptr = __this_cpu_ptr(gic_data[this_cluster].saved_ppi_enable);
 	for (i = 0; i < DIV_ROUND_UP(32, 32); i++)
 		ptr[i] = readl_relaxed(dist_base + GIC_DIST_ENABLE_SET + i * 4);
 
-	ptr = __this_cpu_ptr(gic_data.saved_ppi_conf);
+	ptr = __this_cpu_ptr(gic_data[this_cluster].saved_ppi_conf);
 	for (i = 0; i < DIV_ROUND_UP(32, 16); i++)
 		ptr[i] = readl_relaxed(dist_base + GIC_DIST_CONFIG + i * 4);
 
@@ -914,18 +950,21 @@ static void gic_cpu_restore(void)
 	u32 *ptr;
 	void __iomem *dist_base;
 	void __iomem *cpu_base;
+	u32 this_cluster;
+
+	this_cluster = get_cluster_id();
 
-	dist_base = gic_data_dist_base(&gic_data);
-	cpu_base = gic_data_cpu_base(&gic_data);
+	dist_base = gic_data_dist_base(&gic_data[this_cluster]);
+	cpu_base = gic_data_cpu_base(&gic_data[this_cluster]);
 
 	if (!dist_base || !cpu_base)
 		return;
 
-	ptr = __this_cpu_ptr(gic_data.saved_ppi_enable);
+	ptr = __this_cpu_ptr(gic_data[this_cluster].saved_ppi_enable);
 	for (i = 0; i < DIV_ROUND_UP(32, 32); i++)
 		writel_relaxed(ptr[i], dist_base + GIC_DIST_ENABLE_SET + i * 4);
 
-	ptr = __this_cpu_ptr(gic_data.saved_ppi_conf);
+	ptr = __this_cpu_ptr(gic_data[this_cluster].saved_ppi_conf);
 	for (i = 0; i < DIV_ROUND_UP(32, 16); i++)
 		writel_relaxed(ptr[i], dist_base + GIC_DIST_CONFIG + i * 4);
 
@@ -939,7 +978,6 @@ static void gic_cpu_restore(void)
 static int _gic_notifier(struct notifier_block *self,
 			 unsigned long cmd, void *v)
 {
-	int i;
 	switch (cmd) {
 	case CPU_PM_ENTER:
 		gic_cpu_save();
@@ -969,17 +1007,17 @@ struct gic_notifier_wrapper_struct {
 
 static void gic_notifier_wrapper(void *data)
 {
-	struct gic_notifier_wrapper_struct *pArgs =
+	struct gic_notifier_wrapper_struct *args =
 		(struct gic_notifier_wrapper_struct *)data;
 
-	_gic_notifier(pArgs->self, pArgs->cmd, pArgs->v);
+	_gic_notifier(args->self, args->cmd, args->v);
 }
 
 static int gic_notifier(struct notifier_block *self, unsigned long cmd,	void *v)
 {
-	int i, cpu;
+	int i, j, cpu;
 	struct gic_notifier_wrapper_struct data;
-	int nr_cluster_ids = ((nr_cpu_ids-1) / 4) + 1;
+	int nr_cluster_ids = ((nr_cpu_ids-1) / CORES_PER_CLUSTER) + 1;
 	u32 pcpu = cpu_logical_map(smp_processor_id());
 
 	/* Use IPI mechanism to execute this at other clusters. */
@@ -988,13 +1026,21 @@ static int gic_notifier(struct notifier_block *self, unsigned long cmd,	void *v)
 	data.v = v;
 	for (i = 0; i < nr_cluster_ids; i++) {
 		/* Skip the cluster we're already executing on - do last. */
-		if ((pcpu / 4) == i)
+		if ((pcpu / CORES_PER_CLUSTER) == i)
 			continue;
 
-		/* Have the first cpu in each cluster execute this. */
-		cpu = i * 4;
-		if (cpu_online(cpu))
-			axxia_gic_run_gic_rpc(cpu, gic_notifier_wrapper, &data);
+		/*
+		 * Have some core in each cluster execute this,
+		 * Start with the first core on that cluster.
+		 */
+		cpu = i * CORES_PER_CLUSTER;
+		for (j = cpu; j < cpu + CORES_PER_CLUSTER; j++) {
+			if (cpu_online(j)) {
+				axxia_gic_run_gic_rpc(j, gic_notifier_wrapper,
+						      &data);
+				break;
+			}
+		}
 	}
 
 	/* Execute on this cluster. */
@@ -1017,7 +1063,7 @@ static void __init gic_pm_init(struct gic_chip_data *gic)
 		sizeof(u32));
 	BUG_ON(!gic->saved_ppi_conf);
 
-	if (gic == &gic_data)
+	if (gic == &gic_data[0])
 		cpu_pm_register_notifier(&gic_notifier_block);
 }
 #else
@@ -1026,6 +1072,94 @@ static void __init gic_pm_init(struct gic_chip_data *gic)
 }
 #endif /* CONFIG_CPU_PM */
 
+#ifdef CONFIG_SMP
+void axxia_gic_raise_softirq(const struct cpumask *mask, unsigned int irq)
+{
+	int cpu;
+	unsigned long map = 0;
+	unsigned int regoffset;
+	u32 phys_cpu = cpu_logical_map(smp_processor_id());
+
+	/* Sanity check the physical cpu number */
+	if (phys_cpu >= nr_cpu_ids) {
+		pr_err("Invalid cpu num (%d) >= max (%d)\n",
+			phys_cpu, nr_cpu_ids);
+		return;
+	}
+
+	/* Convert our logical CPU mask into a physical one. */
+	for_each_cpu(cpu, mask)
+		map |= 1 << cpu_logical_map(cpu);
+
+	/*
+	 * Convert the standard ARM IPI number (as defined in
+	 * arch/arm/kernel/smp.c) to an Axxia IPI interrupt.
+	 * The Axxia sends IPI interrupts to other cores via
+	 * the use of "IPI send" registers. Each register is
+	 * specific to a sending CPU and IPI number. For example:
+	 * regoffset 0x0 = CPU0 uses to send IPI0 to other CPUs
+	 * regoffset 0x4 = CPU0 uses to send IPI1 to other CPUs
+	 * ...
+	 * regoffset 0x1000 = CPU1 uses to send IPI0 to other CPUs
+	 * regoffset 0x1004 = CPU1 uses to send IPI1 to other CPUs
+	 * ...
+	 */
+
+	if (phys_cpu < 8)
+		regoffset = phys_cpu * 0x1000;
+	else
+		regoffset = (phys_cpu - 8) * 0x1000 + 0x10000;
+
+	switch (irq) {
+	case 0: /* IPI_WAKEUP */
+		regoffset += 0x8; /* Axxia IPI2 */
+		muxed_ipi_message_pass(mask, MUX_MSG_CPU_WAKEUP);
+		break;
+
+	case 1: /* IPI_TIMER */
+		regoffset += 0x0; /* Axxia IPI0 */
+		break;
+
+	case 2: /* IPI_RESCHEDULE */
+		regoffset += 0x4; /* Axxia IPI1 */
+		break;
+
+	case 3: /* IPI_CALL_FUNC */
+		regoffset += 0x8; /* Axxia IPI2 */
+		muxed_ipi_message_pass(mask, MUX_MSG_CALL_FUNC);
+		break;
+
+	case 4: /* IPI_CALL_FUNC_SINGLE */
+		regoffset += 0x8; /* Axxia IPI2 */
+		muxed_ipi_message_pass(mask, MUX_MSG_CALL_FUNC_SINGLE);
+		break;
+
+	case 5: /* IPI_CPU_STOP */
+		regoffset += 0x8; /* Axxia IPI2 */
+		muxed_ipi_message_pass(mask, MUX_MSG_CPU_STOP);
+		break;
+
+	case AXXIA_RPC:
+		regoffset += 0xC; /* Axxia IPI3 */
+		break;
+
+	default:
+		/* Unknown ARM IPI */
+		pr_err("Unknown ARM IPI num (%d)!\n", irq);
+		return;
+	}
+
+	/*
+	 * Ensure that stores to Normal memory are visible to the
+	 * other CPUs before issuing the IPI.
+	 */
+	dsb();
+
+	/* Axxia chip uses external SPI interrupts for IPI functionality. */
+	writel_relaxed(map, ipi_send_reg_base + regoffset);
+}
+#endif /* SMP */
+
 static int gic_irq_domain_map(struct irq_domain *d, unsigned int irq,
 				irq_hw_number_t hw)
 {
@@ -1071,20 +1205,24 @@ const struct irq_domain_ops gic_irq_domain_ops = {
 	.xlate = gic_irq_domain_xlate,
 };
 
-void __init gic_init_bases(unsigned int gic_nr, int irq_start,
-			   void __iomem *dist_base, void __iomem *cpu_base,
-			   u32 percpu_offset, struct device_node *node)
+void __init axxia_gic_init_bases(int irq_start,
+				 void __iomem *dist_base,
+				 void __iomem *cpu_base,
+				 struct device_node *node)
 {
 	irq_hw_number_t hwirq_base;
 	struct gic_chip_data *gic;
 	int gic_irqs, irq_base;
+	int i;
 
-	gic = &gic_data;
+	for (i = 0; i < MAX_NUM_CLUSTERS; i++) {
+		gic = &gic_data[i];
 
-	/* Normal, sane GIC... */
-	gic->dist_base.common_base = dist_base;
-	gic->cpu_base.common_base = cpu_base;
-	gic_set_base_accessor(gic, gic_get_common_base);
+		/* Normal, sane GIC... */
+		gic->dist_base.common_base = dist_base;
+		gic->cpu_base.common_base = cpu_base;
+		gic_set_base_accessor(gic, gic_get_common_base);
+	}
 
 	/*
 	 * For primary GICs, skip over SGIs.
@@ -1102,11 +1240,15 @@ void __init gic_init_bases(unsigned int gic_nr, int irq_start,
 	 * Find out how many interrupts are supported.
 	 * The GIC only supports up to 1020 interrupt sources.
 	 */
+	gic = &gic_data[0];
 	gic_irqs = readl_relaxed(gic_data_dist_base(gic) + GIC_DIST_CTR) & 0x1f;
 	gic_irqs = (gic_irqs + 1) * 32;
 	if (gic_irqs > MAX_GIC_INTERRUPTS)
 		gic_irqs = MAX_GIC_INTERRUPTS;
-	gic->gic_irqs = gic_irqs;
+	for (i = 0; i < MAX_NUM_CLUSTERS; i++) {
+		gic = &gic_data[i];
+		gic->gic_irqs = gic_irqs;
+	}
 
 	gic_irqs -= hwirq_base; /* calculate # of irqs to allocate */
 	irq_base = irq_alloc_descs(irq_start, 16, gic_irqs, numa_node_id());
@@ -1116,6 +1258,7 @@ void __init gic_init_bases(unsigned int gic_nr, int irq_start,
 		     irq_start);
 		irq_base = irq_start;
 	}
+	gic = &gic_data[0];
 	gic->domain = irq_domain_add_legacy(node, gic_irqs, irq_base,
 				    hwirq_base, &gic_irq_domain_ops, gic);
 	if (WARN_ON(!gic->domain))
@@ -1124,6 +1267,7 @@ void __init gic_init_bases(unsigned int gic_nr, int irq_start,
 #ifdef CONFIG_SMP
 	set_smp_cross_call(axxia_gic_raise_softirq);
 #endif
+	set_handle_irq(axxia_gic_handle_irq);
 
 	gic_axxia_init(gic);
 	gic_dist_init(gic);
@@ -1133,118 +1277,42 @@ void __init gic_init_bases(unsigned int gic_nr, int irq_start,
 
 void __cpuinit axxia_gic_secondary_init(void)
 {
-	gic_cpu_init(&gic_data);
-}
-
-
-void __cpuinit axxia_gic_secondary_cluster_init(void)
-{
-	struct gic_chip_data *gic = &gic_data;
-
 	/*
-	 * Initialize the GIC distributor and cpu interfaces
-	 * for secondary clusters in the Axxia SoC.
+	 * OK to always use the gic_data associated with
+	 * the first cluster. All clusters use the same
+	 * dist and cpu base addresses.
 	 */
 
-	gic_dist_init(gic);
-	gic_cpu_init(gic);
+	gic_cpu_init(&gic_data[0]);
 }
 
-#ifdef CONFIG_SMP
-void axxia_gic_raise_softirq(const struct cpumask *mask, unsigned int irq)
-{
-	int cpu;
-	unsigned long map = 0;
-	unsigned int regoffset;
-	u32 phys_cpu = cpu_logical_map(smp_processor_id());
-
-	/* Sanity check the physical cpu number */
-	if (phys_cpu >= nr_cpu_ids) {
-		pr_err("Invalid cpu num (%d) >= max (%d)\n",
-			phys_cpu, nr_cpu_ids);
-		return;
-	}
-
-	/* Convert our logical CPU mask into a physical one. */
-	for_each_cpu(cpu, mask)
-		map |= 1 << cpu_logical_map(cpu);
 
+void __cpuinit axxia_gic_secondary_cluster_init(void)
+{
 	/*
-	 * Convert the standard ARM IPI number (as defined in
-	 * arch/arm/kernel/smp.c) to an Axxia IPI interrupt.
-	 * The Axxia sends IPI interrupts to other cores via
-	 * the use of "IPI send" registers. Each register is
-	 * specific to a sending CPU and IPI number. For example:
-	 * regoffset 0x0 = CPU0 uses to send IPI0 to other CPUs
-	 * regoffset 0x4 = CPU0 uses to send IPI1 to other CPUs
-	 * ...
-	 * regoffset 0x1000 = CPU1 uses to send IPI0 to other CPUs
-	 * regoffset 0x1004 = CPU1 uses to send IPI1 to other CPUs
-	 * ...
+	 * OK to always use the gic_data associated with
+	 * the first cluster. All clusters use the same
+	 * dist and cpu base addresses.
 	 */
 
-	if (phys_cpu < 8)
-		regoffset = phys_cpu * 0x1000;
-	else
-		regoffset = (phys_cpu - 8) * 0x1000 + 0x10000;
-
-	switch (irq) {
-	case 0: /* IPI_WAKEUP */
-		regoffset += 0x8; /* Axxia IPI2 */
-		muxed_ipi_message_pass(mask, MUX_MSG_CPU_WAKEUP);
-		break;
-
-	case 1: /* IPI_TIMER */
-		regoffset += 0x0; /* Axxia IPI0 */
-		break;
-
-	case 2: /* IPI_RESCHEDULE */
-		regoffset += 0x4; /* Axxia IPI1 */
-		break;
-
-	case 3: /* IPI_CALL_FUNC */
-		regoffset += 0x8; /* Axxia IPI2 */
-		muxed_ipi_message_pass(mask, MUX_MSG_CALL_FUNC);
-		break;
-
-	case 4: /* IPI_CALL_FUNC_SINGLE */
-		regoffset += 0x8; /* Axxia IPI2 */
-		muxed_ipi_message_pass(mask, MUX_MSG_CALL_FUNC_SINGLE);
-		break;
-
-	case 5: /* IPI_CPU_STOP */
-		regoffset += 0x8; /* Axxia IPI2 */
-		muxed_ipi_message_pass(mask, MUX_MSG_CPU_STOP);
-		break;
-
-	case AXXIA_RPC:
-		regoffset += 0xC; /* Axxia IPI3 */
-		break;
-
-	default:
-		/* Unknown ARM IPI */
-		pr_err("Unknown ARM IPI num (%d)!\n", irq);
-		return;
-	}
+	struct gic_chip_data *gic = &gic_data[0];
 
 	/*
-	 * Ensure that stores to Normal memory are visible to the
-	 * other CPUs before issuing the IPI.
+	 * Initialize the GIC distributor and cpu interfaces
+	 * for secondary clusters in the Axxia SoC.
 	 */
-	dsb();
 
-	/* Axxia chip uses external SPI interrupts for IPI functionality. */
-	writel_relaxed(map, ipi_send_reg_base + regoffset);
+	gic_dist_init(gic);
+	gic_cpu_init(gic);
 }
-#endif /* SMP */
 
 #ifdef CONFIG_OF
 
-int __init gic_of_init(struct device_node *node, struct device_node *parent)
+int __init axxia_gic_of_init(struct device_node *node,
+			     struct device_node *parent)
 {
 	void __iomem *cpu_base;
 	void __iomem *dist_base;
-	u32 percpu_offset;
 
 	if (WARN_ON(!node))
 		return -ENODEV;
@@ -1261,10 +1329,7 @@ int __init gic_of_init(struct device_node *node, struct device_node *parent)
 	ipi_send_reg_base = of_iomap(node, 5);
 	WARN(!ipi_send_reg_base, "unable to map Axxia IPI send registers\n");
 
-	if (of_property_read_u32(node, "cpu-offset", &percpu_offset))
-		percpu_offset = 0;
-
-	gic_init_bases(0, -1, dist_base, cpu_base, percpu_offset, node);
+	axxia_gic_init_bases(-1, dist_base, cpu_base, node);
 
 	return 0;
 }
diff --git a/arch/arm/mach-axxia/axxia.c b/arch/arm/mach-axxia/axxia.c
index ba5574d..ef87fb2 100644
--- a/arch/arm/mach-axxia/axxia.c
+++ b/arch/arm/mach-axxia/axxia.c
@@ -73,7 +73,7 @@ void __init axxia_dt_init_early(void)
 static struct of_device_id axxia_irq_match[] __initdata = {
 	{
 		.compatible = "arm,cortex-a15-gic",
-		.data = gic_of_init,
+		.data = axxia_gic_of_init,
 	},
 	{ }
 };
@@ -360,7 +360,6 @@ DT_MACHINE_START(AXXIA_DT, "LSI Axxia")
 	.init_irq	= axxia_dt_init_irq,
 	.init_time	= axxia_dt_timer_init,
 	.init_machine	= axxia_dt_init,
-	.handle_irq	= axxia_gic_handle_irq,
 	.restart	= axxia_restart,
 #if defined(CONFIG_ZONE_DMA) && defined(CONFIG_ARM_LPAE)
 	.dma_zone_size	= (4ULL * SZ_1G) - 1,
diff --git a/arch/arm/mach-axxia/include/mach/axxia-gic.h b/arch/arm/mach-axxia/include/mach/axxia-gic.h
index cc36a51..9ca0609 100644
--- a/arch/arm/mach-axxia/include/mach/axxia-gic.h
+++ b/arch/arm/mach-axxia/include/mach/axxia-gic.h
@@ -8,10 +8,10 @@
 #ifndef __AXXIA_GIC_H
 #define __AXXIA_GIC_H
 
-void axxia_gic_handle_irq(struct pt_regs *regs);
 void axxia_gic_raise_softirq(const struct cpumask *mask, unsigned int irq);
 void axxia_gic_secondary_cluster_init(void);
 void axxia_gic_secondary_init(void);
-int __init gic_of_init(struct device_node *node, struct device_node *parent);
+int __init axxia_gic_of_init(struct device_node *node,
+			     struct device_node *parent);
 
 #endif
diff --git a/arch/arm/mach-axxia/platsmp.c b/arch/arm/mach-axxia/platsmp.c
index 33bdfd0..cfd18d9 100644
--- a/arch/arm/mach-axxia/platsmp.c
+++ b/arch/arm/mach-axxia/platsmp.c
@@ -105,10 +105,8 @@ int __cpuinit axxia_boot_secondary(unsigned int cpu, struct task_struct *idle)
 	/* Release the specified core */
 	write_pen_release(phys_cpu);
 
-#ifdef CONFIG_HOTPLUG_CPU
 	/* Send a wakeup IPI to get the idled cpu out of WFI state */
-	axxia_gic_raise_softirq(cpumask_of(cpu), 1);
-#endif
+	arch_send_wakeup_ipi_mask(cpumask_of(cpu));
 
 	/* Wait for so long, then give up if nothing happens ... */
 #ifdef CONFIG_ARCH_AXXIA_SIM
-- 
1.7.5.4

