From 885864a8d2a80c1093594459cccefcabed1956e0 Mon Sep 17 00:00:00 2001
From: Jiang Lu <lu.jiang@windriver.com>
Date: Wed, 28 May 2014 03:12:31 +0000
Subject: [PATCH] Axxia:resolve a dead-lock issue with axxia gic

Fixed following calltrace on lsi-axm55xx platform:

BUG: spinlock lockup suspected on CPU#0, swapper/0/0
BUG: spinlock lockup suspected on CPU#8, clock_settime_4/12914
 lock: irq_desc+0x2450/0x10000, .magic: dead4ead, .owner: irqbalance/436, .owner_cpu: 2
CPU: 8 PID: 12914 Comm: clock_settime_4 Not tainted 3.10.39-grsec-WR6.0.0.6_secure #1
[<c0416bd8>] (unwind_backtrace+0x0/0xec) from [<c041221c>] (show_stack+0x20/0x24)
[<c041221c>] (show_stack+0x20/0x24) from [<c0a217a4>] (dump_stack+0x20/0x28)
[<c0a217a4>] (dump_stack+0x20/0x28) from [<c0a21f2c>] (spin_dump+0x80/0x94)
[<c0a21f2c>] (spin_dump+0x80/0x94) from [<c0748858>] (do_raw_spin_lock+0x124/0x170)
[<c0748858>] (do_raw_spin_lock+0x124/0x170) from [<c0a273e0>] (_raw_spin_lock+0x50/0x58)
[<c0a273e0>] (_raw_spin_lock+0x50/0x58) from [<c04a46e4>] (handle_fasteoi_irq+0x24/0x10c)
[<c04a46e4>] (handle_fasteoi_irq+0x24/0x10c) from [<c04a0cac>] (generic_handle_irq+0x30/0x40)
[<c04a0cac>] (generic_handle_irq+0x30/0x40) from [<c040ee84>] (handle_IRQ+0x78/0xa0)
[<c040ee84>] (handle_IRQ+0x78/0xa0) from [<c04087fc>] (axxia_gic_handle_irq+0x174/0x19c)
[<c04087fc>] (axxia_gic_handle_irq+0x174/0x19c) from [<c0a282c4>] (__irq_svc+0x44/0x78)

On axxia platform, there are 16 cores on SOC. The 16 cores are divided
into 4 clusters. Each irq must be assigned to a cluster, the irq
operations must be performed on core within same cluster. IRQ
operations include mask/unmask an irq, change type for an irq, and
update affinity for an irq. So when received an irq operation on cpu
didn't belong to the cluster, kernel need send an IPI to corresponding
cpu to complete the operation, then let the issuer wait.

In kernel, irq operation routine and ISR need acquire the lock of the
irq desc. Since the irq operation routine and ISR may be served on
different cores at the same time. This would introduce a dead lock in
kernel.

This patch force the IPI issuer release the lock before waiting, which
could allow the receiver to acquire the lock to complete the operation.

Signed-off-by: Jiang Lu <lu.jiang@windriver.com>
---
 arch/arm/mach-axxia/axxia-gic.c | 26 +++++++++++++++++++-------
 1 file changed, 19 insertions(+), 7 deletions(-)

diff --git a/arch/arm/mach-axxia/axxia-gic.c b/arch/arm/mach-axxia/axxia-gic.c
index 1071a74..e1538df 100644
--- a/arch/arm/mach-axxia/axxia-gic.c
+++ b/arch/arm/mach-axxia/axxia-gic.c
@@ -221,10 +221,16 @@ static void axxia_gic_handle_gic_rpc_ipi(void)
 	irq_exit();
 }
 
-static void axxia_gic_run_gic_rpc(int cpu, axxia_call_func_t *func, void *info)
+static void axxia_gic_run_gic_rpc(int cpu, axxia_call_func_t *func, void *info,
+	struct irq_data *d)
 {
 	struct axxia_gic_rpc *slot = &__get_cpu_var(axxia_gic_rpc);
 
+	struct irq_desc *desc = NULL;
+
+	if (d)
+		desc = irq_to_desc(d->irq);
+
 	/* If the target CPU isn't online, don't bother. */
 	if (!cpu_online(cpu))
 		return;
@@ -240,10 +246,16 @@ static void axxia_gic_run_gic_rpc(int cpu, axxia_call_func_t *func, void *info)
 	/* Send the IPI. */
 	axxia_gic_raise_softirq(cpumask_of(cpu), AXXIA_RPC);
 
+	if (desc)
+		raw_spin_unlock(&desc->lock);
+
 	while (slot->func) {
 		axxia_gic_handle_gic_rpc(); /* Execute other CPU requests */
 		cpu_relax();
 	}
+
+	if (desc)
+		raw_spin_lock(&desc->lock);
 }
 
 /*
@@ -294,7 +306,7 @@ static void gic_mask_irq(struct irq_data *d)
 		(pcpu / CORES_PER_CLUSTER))
 		_gic_mask_irq(d);
 	else
-		axxia_gic_run_gic_rpc(irq_cpuid[irqid], _gic_mask_irq, d);
+		axxia_gic_run_gic_rpc(irq_cpuid[irqid], _gic_mask_irq, d, d);
 }
 
 static void _gic_unmask_irq(void *arg)
@@ -338,7 +350,7 @@ static void gic_unmask_irq(struct irq_data *d)
 		(pcpu / CORES_PER_CLUSTER))
 		_gic_unmask_irq(d);
 	else
-		axxia_gic_run_gic_rpc(irq_cpuid[irqid], _gic_unmask_irq, d);
+		axxia_gic_run_gic_rpc(irq_cpuid[irqid], _gic_unmask_irq, d, d);
 }
 
 static void gic_eoi_irq(struct irq_data *d)
@@ -451,7 +463,7 @@ static int gic_set_type(struct irq_data *d, unsigned int type)
 		for (j = cpu; j < cpu + CORES_PER_CLUSTER; j++) {
 			if (cpu_online(j)) {
 				axxia_gic_run_gic_rpc(j, gic_set_type_wrapper,
-						      &data);
+						      &data, d);
 				if (data.status != 0)
 					pr_err("IRQ set type error for cpu%d\n",
 					       j);
@@ -553,7 +565,7 @@ static int gic_set_affinity(struct irq_data *d,
 		(pcpu / CORES_PER_CLUSTER))
 		_gic_set_affinity(&data);
 	else
-		axxia_gic_run_gic_rpc(cpu, _gic_set_affinity, &data);
+		axxia_gic_run_gic_rpc(cpu, _gic_set_affinity, &data, d);
 
 	/*
 	 * If the new physical cpu assignment is on a cluster that's
@@ -574,7 +586,7 @@ static int gic_set_affinity(struct irq_data *d,
 		else
 			axxia_gic_run_gic_rpc(irq_cpuid[irqid],
 					      _gic_set_affinity,
-					      &data);
+					      &data, d);
 	}
 
 	/* Update Axxia IRQ affinity table with the new physical CPU number. */
@@ -1032,7 +1044,7 @@ static int gic_notifier(struct notifier_block *self, unsigned long cmd,	void *v)
 		for (j = cpu; j < cpu + CORES_PER_CLUSTER; j++) {
 			if (cpu_online(j)) {
 				axxia_gic_run_gic_rpc(j, gic_notifier_wrapper,
-						      &data);
+						      &data, NULL);
 				break;
 			}
 		}
-- 
1.9.2

