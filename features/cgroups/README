This README introduces cgroup (controller group) from concept, some 
basic information, and basic test usages.

There are five parts as follows in this README:

1. Control Groups

   Control Groups provide a mechanism for aggregating/partitioning sets of
   tasks, and all their future children, into hierarchical groups with
   specialized behaviour. The use of VFS in Groups is the key technical
   characteristic, which is the main reason that there is only a minimum of
   additional kernel code to add cgroup into current kernel, accompanying
   with using all of merits of VFS.

   Please refer to Documentation/cgroups.txt for more detailed information.
   And the following are the current directory tree of DOCs for controllers:

	Documentation/cgroups.txt
	    |
	    |--device-mapper
	    |         `-ioband.txt		// dm-ioband controller
	    `--controllers	
		  |--------memory.txt		// memory controller
		  |--------devices.txt		// device controller
		  `--------resource_counter.txt // resource controller

2. Some basic test usage for a few controllers:

   1) memory controller
      	
      Steps to validate memory controller as follows:
 
	# mkdir -p /cgroups/memcg
	# mount -t cgroup none /cgroups/memcg -o memory

	# mkdir /cgroups/0  (create 0 memory controller for current bash)
	# echo $$ >  /cgroups/0/tasks

	# echo 4M > /cgroups/0/memory.limit_in_bytes(set limit)
	# cat /cgroups/0/memory.limit_in_bytes
	4194304
	
	# cat /cgroups/0/memory.usage_in_bytes(Check the usage)
 	106496

	Then scp a great file to user@ip_address
	# cat /cgroups/0/memory.usage_in_bytes	
	4010200(4194304 or so)

        Please refer to Documentation/controllers/memory.txt 
	for more detailed information.
	
   2) net traffic controller(uplink only)
     
	*) Set QoS for cgroup
	
	   tc qdisc add dev eth0 root handle 1: htb
	   # For cgroup(500KB/s) 
	   tc class add dev eth0 parent 1: classid 1:10 htb rate 4mbit ceil 4mbit
	   # For others(5MB/s)
	   tc class add dev eth0 parent 1: classid 1:20 htb rate 40mbit ceil 40mbit

	   # cls_groups filter for cgroup process 
	   tc filter add dev eth0 parent 1: handle 800 protocol ip prio 10 cgroup value 0x1234 classid 1:10
	   # Basic filter for others 
	   tc filter add dev eth0 parent 1: protocol ip prio 20 basic classid 1:20

	*) In another shell do the following is high recommended
	   mkdir -p /cgroups/tc
	   mount -t cgroup tc -otc /cgroups/tc
	   mkdir /cgroups/tc/file_transfer
	   echo 0x1234 > /cgroups/tc/file_transfer/tc.classid
	   echo $$ > /cgroups/tc/file_transfer/tasks

	*) scp document to user@ip_addr in that another shell,(uplink only)
           It will be found that the speed of file transferring is 500KB/s or so
           in that another shell, and 5MB/s in others.

   3) dm-ioband and bio_tracking

      There are an example of script to do a test on them,which control
      bandwidth bases on the number of I/O sectors.

	#!/bin/sh
	DEV1=/dev/sda1
	DEV2=/dev/sda2
	DEV3=/dev/sda3
	DEVSIZE1=$(blockdev --getsize $DEV1)
	DEVSIZE2=$(blockdev --getsize $DEV2)
	DEVSIZE3=$(blockdev --getsize $DEV3)

	RANGE=10240 # Mbytes
	
	XDDOPT="-op write -queuedepth 32 -blocksize 512
	-seek random -datapattern random -dio -timelimit 60
	-mbytes $RANGE -seek range $((RANGE * 1048576 / 512))"
	
	echo "0 $DEVSIZE1 ioband $DEV1 1 0 0" \
	   "cgroup weight-iosize 0 :100 1:40 2:20 3:10" | dmsetup create ioband1
	echo "0 $DEVSIZE2 ioband $DEV2 1 0 0" \
	   "cgroup weight-iosize 0 :100 1:40 2:20 3:10" | dmsetup create ioband2
	echo "0 $DEVSIZE3 ioband $DEV3 1 0 0" \
	   "cgroup weight-iosize 0 :100 1:40 2:20 3:10" | dmsetup create ioband3

	dmsetup table
	echo $$ > /cgroups/bio_tracking/1/tasks
	xdd.linux -targets 1 /dev/mapper/ioband1 $XDDOPT -reqsize 32 \
	    -output cgroup1.txt &
	echo $$ > /cgroups/bio_tracking/2/tasks
	xdd.linux -targets 1 /dev/mapper/ioband2 $XDDOPT -reqsize 64 \
	    -output cgroup2.txt &
	echo $$ > /cgroups/bio_tracking/3/tasks
	xdd.linux -targets 1 /dev/mapper/ioband3 $XDDOPT -reqsize 128 \
	    -output cgroup3.txt &
	wait
	
	dmsetup remove ioband1
	dmsetup remove ioband2
	dmsetup remove ioband3
	
      This scritpt require xdd disk I/O testing tool which can be downloaded
      from here:
	http://www.ioperformance.com/products.htm

      It is high recommended that your test environment is constructed based
      on your application, and these kind of tests on bio_tracking are finished
      on a pure work harddsik, which is not your system harddisk(OS).

      Refer to the following links of both for more detailed information.

3. Add/remove a new controller named foo into wrlinux

   1) create foo.scc, the content seems as follows:

	# include kernel config segment for foo controller here.
    	kconf non-hardware foo.cfg 

	# patch foo_xxxx.patch into wrlinux
	patch 0001-foo-merge-into-3.0.patch

   2) To add a foo.scc of this new controller into cgroups.scc.

4. Notes: 

   These controllers are in active development by their respsective communities.
   They are stable as tested by Wind River and form a basis for future 
   extension and deployment. Each configuration is unique, so care must be
   taken to ensure applicability and stability of the controllers.

5. References:

   * dm-ioband: I/O bandwidth controller: (1.8)
    	http://people.valinux.co.jp/~ryov/dm-ioband/

   * BIO tracking
        http://people.valinux.co.jp/~ryov/bio-cgroup/ 

   * memrlimit
        http://kerneltrap.org/mailarchive/linux-kernel/2008/5/21/1898334

   * Traffic control cgroups subsystem
	http://kerneltrap.org/mailarchive/linux-netdev/2008/8/5/2822344
	https://kerneltrap.org/mailarchive/linux-kernel/2008/9/10/3260004
	http://marc.info/?l=linux-netdev&m=121641636011223&w=2
