From e8766dae192852c6fdfd2fd968b51409aab041bf Mon Sep 17 00:00:00 2001
From: Paul Gortmaker <paul.gortmaker@windriver.com>
Date: Wed, 13 Nov 2013 14:33:57 -0500
Subject: [PATCH] x86/kvm: convert async page fault to simple wait queue

We currently see:

 BUG: sleeping function called from invalid context at linux/kernel/rtmutex.c:659
  in_atomic(): 1, irqs_disabled(): 1, pid: 0, name: swapper/0
  Preemption disabled at:[<ffffffff81635df7>] rest_init+0x7b/0x84

  Pid: 0, comm: swapper/0 Not tainted 3.4.66-preempt-rt #1
  Call Trace:
   [<ffffffff81068767>] __might_sleep+0x107/0x180
   [<ffffffff81659444>] rt_spin_lock+0x24/0x50
   [<ffffffff81025bd6>] apf_task_wake_all+0x46/0xb0
   [<ffffffff81025d50>] kvm_async_pf_task_wake+0x110/0x120
   [<ffffffff8165c758>] do_async_page_fault+0x78/0x90
   [<ffffffff81659f35>] async_page_fault+0x25/0x30
   [<ffffffff8102628b>] ? native_safe_halt+0xb/0x10
   [<ffffffff81009f15>] default_idle+0xc5/0x320
   [<ffffffff8100b0b6>] cpu_idle+0xe6/0x130
   [<ffffffff81635df7>] rest_init+0x7b/0x84
   [<ffffffff81cc4bde>] start_kernel+0x39f/0x3ac
   [<ffffffff81cc463d>] ? repair_env_string+0x5a/0x5a
   [<ffffffff81cc432a>] x86_64_start_reservations+0x131/0x135
   [<ffffffff81cc4140>] ? early_idt_handlers+0x140/0x140
   [<ffffffff81cc4430>] x86_64_start_kernel+0x102/0x111

This happens because we can't be using normal wait queues in
atomic context, since they don't (and can't) use a raw lock.

Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c
index cd6d9a5a42f6..2494b0105987 100644
--- a/arch/x86/kernel/kvm.c
+++ b/arch/x86/kernel/kvm.c
@@ -21,6 +21,7 @@
  */
 
 #include <linux/context_tracking.h>
+#include <linux/wait-simple.h>
 #include <linux/module.h>
 #include <linux/kernel.h>
 #include <linux/kvm_para.h>
@@ -89,7 +90,7 @@ static void kvm_io_delay(void)
 
 struct kvm_task_sleep_node {
 	struct hlist_node link;
-	wait_queue_head_t wq;
+	struct swait_head wq;
 	u32 token;
 	int cpu;
 	bool halted;
@@ -120,7 +121,7 @@ void kvm_async_pf_task_wait(u32 token)
 	u32 key = hash_32(token, KVM_TASK_SLEEP_HASHBITS);
 	struct kvm_task_sleep_head *b = &async_pf_sleepers[key];
 	struct kvm_task_sleep_node n, *e;
-	DEFINE_WAIT(wait);
+	DEFINE_SWAITER(wait);
 
 	rcu_irq_enter();
 
@@ -139,13 +140,13 @@ void kvm_async_pf_task_wait(u32 token)
 	n.token = token;
 	n.cpu = smp_processor_id();
 	n.halted = is_idle_task(current) || preempt_count() > 1;
-	init_waitqueue_head(&n.wq);
+	init_swait_head(&n.wq);
 	hlist_add_head(&n.link, &b->list);
 	spin_unlock(&b->lock);
 
 	for (;;) {
 		if (!n.halted)
-			prepare_to_wait(&n.wq, &wait, TASK_UNINTERRUPTIBLE);
+			swait_prepare(&n.wq, &wait, TASK_UNINTERRUPTIBLE);
 		if (hlist_unhashed(&n.link))
 			break;
 
@@ -164,7 +165,7 @@ void kvm_async_pf_task_wait(u32 token)
 		}
 	}
 	if (!n.halted)
-		finish_wait(&n.wq, &wait);
+		swait_finish(&n.wq, &wait);
 
 	rcu_irq_exit();
 	return;
@@ -176,8 +177,8 @@ static void apf_task_wake_one(struct kvm_task_sleep_node *n)
 	hlist_del_init(&n->link);
 	if (n->halted)
 		smp_send_reschedule(n->cpu);
-	else if (waitqueue_active(&n->wq))
-		wake_up(&n->wq);
+	else if (swaitqueue_active(&n->wq))
+		swait_wake(&n->wq);
 }
 
 static void apf_task_wake_all(void)
@@ -229,7 +230,7 @@ again:
 		}
 		n->token = token;
 		n->cpu = smp_processor_id();
-		init_waitqueue_head(&n->wq);
+		init_swait_head(&n->wq);
 		hlist_add_head(&n->link, &b->list);
 	} else
 		apf_task_wake_one(n);
-- 
1.9.0

