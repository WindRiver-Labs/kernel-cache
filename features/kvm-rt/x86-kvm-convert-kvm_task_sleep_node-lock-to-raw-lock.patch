From ca14c231b3d0c35a96e78672c8c477d1701a0422 Mon Sep 17 00:00:00 2001
From: Paul Gortmaker <paul.gortmaker@windriver.com>
Date: Wed, 13 Nov 2013 14:51:26 -0500
Subject: [PATCH] x86/kvm: convert kvm_task_sleep_node lock to raw lock

This trace:

  BUG: sleeping function called from invalid context at kernel/rtmutex.c:659
  in_atomic(): 1, irqs_disabled(): 1, pid: 0, name: swapper/0
  Preemption disabled at:[<ffffffff81635df7>] rest_init+0x7b/0x84

  Pid: 0, comm: swapper/0 Not tainted 3.4.66-preempt-rt #1
  Call Trace:
   [<ffffffff81068767>] __might_sleep+0x107/0x180
   [<ffffffff81659444>] rt_spin_lock+0x24/0x50
   [<ffffffff81025bd6>] apf_task_wake_all+0x46/0xb0
   [<ffffffff81025d50>] kvm_async_pf_task_wake+0x110/0x120
   [<ffffffff8165c758>] do_async_page_fault+0x78/0x90
   [<ffffffff81659f35>] async_page_fault+0x25/0x30
   [<ffffffff8102628b>] ? native_safe_halt+0xb/0x10
   [<ffffffff81009f15>] default_idle+0xc5/0x320
   [<ffffffff8100b0b6>] cpu_idle+0xe6/0x130
   [<ffffffff81635df7>] rest_init+0x7b/0x84
   [<ffffffff81cc4bde>] start_kernel+0x39f/0x3ac
   [<ffffffff81cc463d>] ? repair_env_string+0x5a/0x5a
   [<ffffffff81cc432a>] x86_64_start_reservations+0x131/0x135
   [<ffffffff81cc4140>] ? early_idt_handlers+0x140/0x140
   [<ffffffff81cc4430>] x86_64_start_kernel+0x102/0x111

shows that apf_task_wake_all() is called in atomic context, and hence
the lock used in that function can't be left as non-raw.

Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c
index dfedc40..d63b108 100644
--- a/arch/x86/kernel/kvm.c
+++ b/arch/x86/kernel/kvm.c
@@ -87,7 +87,7 @@ struct kvm_task_sleep_node {
 };
 
 static struct kvm_task_sleep_head {
-	spinlock_t lock;
+	raw_spinlock_t lock;
 	struct hlist_head list;
 } async_pf_sleepers[KVM_TASK_SLEEP_HASHSIZE];
 
@@ -120,13 +120,13 @@ void kvm_async_pf_task_wait(u32 token)
 
 	rcu_irq_enter();
 
-	spin_lock(&b->lock);
+	raw_spin_lock(&b->lock);
 	e = _find_apf_task(b, token);
 	if (e) {
 		/* dummy entry exist -> wake up was delivered ahead of PF */
 		hlist_del(&e->link);
 		kfree(e);
-		spin_unlock(&b->lock);
+		raw_spin_unlock(&b->lock);
 
 		rcu_irq_exit();
 		return;
@@ -137,7 +137,7 @@ void kvm_async_pf_task_wait(u32 token)
 	n.halted = idle || preempt_count() > 1;
 	init_swait_head(&n.wq);
 	hlist_add_head(&n.link, &b->list);
-	spin_unlock(&b->lock);
+	raw_spin_unlock(&b->lock);
 
 	for (;;) {
 		if (!n.halted)
@@ -183,14 +183,14 @@ static void apf_task_wake_all(void)
 	for (i = 0; i < KVM_TASK_SLEEP_HASHSIZE; i++) {
 		struct hlist_node *p, *next;
 		struct kvm_task_sleep_head *b = &async_pf_sleepers[i];
-		spin_lock(&b->lock);
+		raw_spin_lock(&b->lock);
 		hlist_for_each_safe(p, next, &b->list) {
 			struct kvm_task_sleep_node *n =
 				hlist_entry(p, typeof(*n), link);
 			if (n->cpu == smp_processor_id())
 				apf_task_wake_one(n);
 		}
-		spin_unlock(&b->lock);
+		raw_spin_unlock(&b->lock);
 	}
 }
 
@@ -206,7 +206,7 @@ void kvm_async_pf_task_wake(u32 token)
 	}
 
 again:
-	spin_lock(&b->lock);
+	raw_spin_lock(&b->lock);
 	n = _find_apf_task(b, token);
 	if (!n) {
 		/*
@@ -219,7 +219,7 @@ again:
 			 * Allocation failed! Busy wait while other cpu
 			 * handles async PF.
 			 */
-			spin_unlock(&b->lock);
+			raw_spin_unlock(&b->lock);
 			cpu_relax();
 			goto again;
 		}
@@ -229,7 +229,7 @@ again:
 		hlist_add_head(&n->link, &b->list);
 	} else
 		apf_task_wake_one(n);
-	spin_unlock(&b->lock);
+	raw_spin_unlock(&b->lock);
 	return;
 }
 EXPORT_SYMBOL_GPL(kvm_async_pf_task_wake);
@@ -501,7 +501,7 @@ void __init kvm_guest_init(void)
 	paravirt_ops_setup();
 	register_reboot_notifier(&kvm_pv_reboot_nb);
 	for (i = 0; i < KVM_TASK_SLEEP_HASHSIZE; i++)
-		spin_lock_init(&async_pf_sleepers[i].lock);
+		raw_spin_lock_init(&async_pf_sleepers[i].lock);
 	if (kvm_para_has_feature(KVM_FEATURE_ASYNC_PF))
 		x86_init.irqs.trap_init = kvm_apf_trap_init;
 
-- 
1.8.4.1

