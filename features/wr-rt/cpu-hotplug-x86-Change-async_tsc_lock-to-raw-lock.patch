From 5d6cf77ce28a40d44313f4ca76d596ff9322c96b Mon Sep 17 00:00:00 2001
From: Yang Shi <yang.shi@windriver.com>
Date: Tue, 22 Jan 2013 09:55:33 -0800
Subject: [PATCH] cpu hotplug: x86: Change async_tsc_lock to raw lock

When taking a CPU offline on x86 via
echo 0 >/sys/devices/system/cpu/cpu1/online
Below call trace is triggered:
BUG: sleeping function called from invalid context at linux/kernel/rtmutex.c:646
in_atomic(): 1, irqs_disabled(): 1, pid: 18, name: migration/3
Pid: 18, comm: migration/3 Not tainted 3.4.20-rt29-WR5.0.1.0_preempt-rt #3
Call Trace:
[<ffffffff810687a0>] __might_sleep+0xe0/0x110
[<ffffffff81074c2e>] ? update_max_interval+0x1e/0x40
[<ffffffff816f8ff5>] rt_spin_lock+0x25/0x50
[<ffffffff816e5bce>] hotcpu_callback+0x22/0x154
[<ffffffff816fd2fa>] notifier_call_chain+0x5a/0x80
[<ffffffff810a3c01>] ? wait_for_stop_done+0x1/0xb0
[<ffffffff81060d2e>] __raw_notifier_call_chain+0xe/0x10
[<ffffffff81036751>] __cpu_notify+0x21/0x40
[<ffffffff816d6303>] take_cpu_down+0x33/0x40
[<ffffffff810a3d7b>] stop_machine_cpu_stop+0x9b/0x100
[<ffffffff810a3ce0>] ? cpu_stop_init_done+0x30/0x30
[<ffffffff810a3ce0>] ? cpu_stop_init_done+0x30/0x30
[<ffffffff810a3f14>] cpu_stopper_thread+0xe4/0x1d0
[<ffffffff816fd489>] ? sub_preempt_count+0xa9/0xe0
[<ffffffff816fd489>] ? sub_preempt_count+0xa9/0xe0
[<ffffffff810a3e30>] ? cpu_stop_signal_done+0x50/0x50
[<ffffffff81059fd5>] kthread+0x95/0xa0
[<ffffffff81702394>] kernel_thread_helper+0x4/0x10
[<ffffffff810675dd>] ? finish_task_switch+0x5d/0xe0
[<ffffffff816f93bc>] ? _raw_spin_unlock_irq+0x1c/0x40
[<ffffffff816f984a>] ? retint_restore_args+0xe/0xe
[<ffffffff81059f40>] ? __init_kthread_worker+0x60/0x60
[<ffffffff81702390>] ? gs_change+0xb/0xb

So, change async_tsc_lock to raw lock to prevent from the call trace.

LTTng 1.x is the only consumer of get_trace_clock, so the raw lock won't
have significant impact on the hot path.

Signed-off-by: Yang Shi <yang.shi@windriver.com>
---
 arch/x86/kernel/trace-clock.c |   14 +++++++-------
 1 file changed, 7 insertions(+), 7 deletions(-)

diff --git a/arch/x86/kernel/trace-clock.c b/arch/x86/kernel/trace-clock.c
index 05412b2..a7eb441 100644
--- a/arch/x86/kernel/trace-clock.c
+++ b/arch/x86/kernel/trace-clock.c
@@ -16,7 +16,7 @@
 
 static cycles_t trace_clock_last_tsc;
 static DEFINE_PER_CPU(struct timer_list, update_timer);
-static DEFINE_SPINLOCK(async_tsc_lock);
+static DEFINE_RAW_SPINLOCK(async_tsc_lock);
 static int async_tsc_refcount;	/* Number of readers */
 static int async_tsc_enabled;	/* Async TSC enabled on all online CPUs */
 
@@ -145,7 +145,7 @@ static int __cpuinit hotcpu_callback(struct notifier_block *nb,
 	unsigned int hotcpu = (unsigned long)hcpu;
 	int cpu;
 
-	spin_lock(&async_tsc_lock);
+	raw_spin_lock(&async_tsc_lock);
 	switch (action) {
 	case CPU_UP_PREPARE:
 	case CPU_UP_PREPARE_FROZEN:
@@ -189,7 +189,7 @@ static int __cpuinit hotcpu_callback(struct notifier_block *nb,
 		break;
 #endif /* CONFIG_HOTPLUG_CPU */
 	}
-	spin_unlock(&async_tsc_lock);
+	raw_spin_unlock(&async_tsc_lock);
 
 	return NOTIFY_OK;
 }
@@ -210,7 +210,7 @@ int get_trace_clock(void)
 	}
 
 	get_online_cpus();
-	spin_lock(&async_tsc_lock);
+	raw_spin_lock(&async_tsc_lock);
 	if (async_tsc_refcount++ || trace_clock_is_sync())
 		goto end;
 
@@ -218,7 +218,7 @@ int get_trace_clock(void)
 	for_each_online_cpu(cpu)
 		enable_trace_clock(cpu);
 end:
-	spin_unlock(&async_tsc_lock);
+	raw_spin_unlock(&async_tsc_lock);
 	put_online_cpus();
 	return 0;
 }
@@ -229,7 +229,7 @@ void put_trace_clock(void)
 	int cpu;
 
 	get_online_cpus();
-	spin_lock(&async_tsc_lock);
+	raw_spin_lock(&async_tsc_lock);
 	WARN_ON(async_tsc_refcount <= 0);
 	if (async_tsc_refcount != 1 || !async_tsc_enabled)
 		goto end;
@@ -241,7 +241,7 @@ end:
 	async_tsc_refcount--;
 	if (!async_tsc_refcount && num_online_cpus() == 1)
 		set_trace_clock_is_sync(1);
-	spin_unlock(&async_tsc_lock);
+	raw_spin_unlock(&async_tsc_lock);
 	put_online_cpus();
 }
 EXPORT_SYMBOL_GPL(put_trace_clock);
-- 
1.7.9.7

