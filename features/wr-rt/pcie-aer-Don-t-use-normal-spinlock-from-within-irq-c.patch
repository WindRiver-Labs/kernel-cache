From 3cf58b1ae91cac170c06c7d42bbcce38df43563d Mon Sep 17 00:00:00 2001
From: Paul Gortmaker <paul.gortmaker@windriver.com>
Date: Tue, 13 Oct 2015 17:58:41 -0400
Subject: [PATCH 2/2] pcie/aer: Don't use normal spinlock from within irq
 context

Currently we will see the following trace:

 ------------------
BUG: sleeping function called from invalid context at kernel/rtmutex.c:796
in_atomic(): 1, irqs_disabled(): 1, pid: 0, name: swapper/0
Preemption disabled at:[<ffffffff81093a0b>] cpu_startup_entry+0x13b/0x2f0

CPU: 0 PID: 0 Comm: swapper/0 Tainted: G O 3.10.79-rt74_preempt-rt #1
Hardware name: Juniper Networks, Inc. 0512/HSW RE MX , BIOS NGRE_v0.43 02/17/2015
00000000000000fb ffff88107fc03dd8 ffffffff819620da ffff88107fc03df0
ffffffff81073aaf ffffffff81e0cf40 ffff88107fc03e08 ffffffff81968420
0000000000000004 ffff88107fc03e48 ffffffff8141f2e6 ffff88107fc03e5c
Call Trace:
<IRQ> [<ffffffff819620da>] dump_stack+0x19/0x1b
[<ffffffff81073aaf>] __might_sleep+0xef/0x160
[<ffffffff81968420>] rt_spin_lock+0x20/0x50
[<ffffffff8141f2e6>] pci_read_aer+0x36/0x140
[<ffffffff8140db8f>] pci_check_and_set_intx_mask.isra.29+0x4f/0xd0
[<ffffffff8140dc2b>] pci_check_and_mask_intx+0x1b/0x20
[<ffffffffa04d4ea7>] kvm_assigned_dev_intx+0x27/0x80 [kvm]
[<ffffffff810d8597>] handle_irq_event_percpu+0x87/0x290
[<ffffffff8173698c>] ? cpuidle_enter_state+0x4c/0xc0
[<ffffffff810d8808>] handle_irq_event+0x68/0x90
[<ffffffff810dbcf9>] handle_fasteoi_irq+0xc9/0x140
[<ffffffff81004efe>] handle_irq+0x1e/0x30
[<ffffffff8196b14d>] do_IRQ+0x4d/0xc0
[<ffffffff81968caa>] common_interrupt+0x6a/0x6a
<EOI> [<ffffffff8173698c>] ? cpuidle_enter_state+0x4c/0xc0
[<ffffffff81736ad8>] cpuidle_idle_call+0xd8/0x2c0
[<ffffffff8100cace>] arch_cpu_idle+0xe/0x30
[<ffffffff81093a4e>] cpu_startup_entry+0x17e/0x2f0
[<ffffffff8194cc14>] rest_init+0x84/0x90
[<ffffffff81f07e1e>] start_kernel+0x3b3/0x3bf
[<ffffffff81f07874>] ? repair_env_string+0x5c/0x5c
[<ffffffff81f075ad>] x86_64_start_reservations+0x2a/0x2c
[<ffffffff81f0767b>] x86_64_start_kernel+0xcc/0xcf
 ------------------

Since this is debugging code for software injections of errors, we
can convert to raw lock w/o any drastic concern about long latencies.
Nobody should be running error injection debugging tests on a system
already deployed to handle low latency requirements.

Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
---
 drivers/pci/pcie/aer/aer_inject.c | 34 +++++++++++++++++-----------------
 1 file changed, 17 insertions(+), 17 deletions(-)

diff --git a/drivers/pci/pcie/aer/aer_inject.c b/drivers/pci/pcie/aer/aer_inject.c
index 67cb91aad95b..d8756210efed 100644
--- a/drivers/pci/pcie/aer/aer_inject.c
+++ b/drivers/pci/pcie/aer/aer_inject.c
@@ -72,7 +72,7 @@ static LIST_HEAD(einjected);
 static LIST_HEAD(pci_bus_ops_list);
 
 /* Protect einjected and pci_bus_ops_list */
-static DEFINE_SPINLOCK(inject_lock);
+static DEFINE_RAW_SPINLOCK(inject_lock);
 
 static void aer_error_init(struct aer_error *err, u16 domain,
 			   unsigned int bus, unsigned int devfn,
@@ -126,7 +126,7 @@ static struct pci_bus_ops *pci_bus_ops_pop(void)
 	unsigned long flags;
 	struct pci_bus_ops *bus_ops = NULL;
 
-	spin_lock_irqsave(&inject_lock, flags);
+	raw_spin_lock_irqsave(&inject_lock, flags);
 	if (list_empty(&pci_bus_ops_list))
 		bus_ops = NULL;
 	else {
@@ -134,7 +134,7 @@ static struct pci_bus_ops *pci_bus_ops_pop(void)
 		list_del(lh);
 		bus_ops = list_entry(lh, struct pci_bus_ops, list);
 	}
-	spin_unlock_irqrestore(&inject_lock, flags);
+	raw_spin_unlock_irqrestore(&inject_lock, flags);
 	return bus_ops;
 }
 
@@ -190,7 +190,7 @@ static int pci_read_aer(struct pci_bus *bus, unsigned int devfn, int where,
 	struct pci_ops *ops;
 	int domain;
 
-	spin_lock_irqsave(&inject_lock, flags);
+	raw_spin_lock_irqsave(&inject_lock, flags);
 	if (size != sizeof(u32))
 		goto out;
 	domain = pci_domain_nr(bus);
@@ -203,12 +203,12 @@ static int pci_read_aer(struct pci_bus *bus, unsigned int devfn, int where,
 	sim = find_pci_config_dword(err, where, NULL);
 	if (sim) {
 		*val = *sim;
-		spin_unlock_irqrestore(&inject_lock, flags);
+		raw_spin_unlock_irqrestore(&inject_lock, flags);
 		return 0;
 	}
 out:
 	ops = __find_pci_bus_ops(bus);
-	spin_unlock_irqrestore(&inject_lock, flags);
+	raw_spin_unlock_irqrestore(&inject_lock, flags);
 	return ops->read(bus, devfn, where, size, val);
 }
 
@@ -222,7 +222,7 @@ static int pci_write_aer(struct pci_bus *bus, unsigned int devfn, int where,
 	struct pci_ops *ops;
 	int domain;
 
-	spin_lock_irqsave(&inject_lock, flags);
+	raw_spin_lock_irqsave(&inject_lock, flags);
 	if (size != sizeof(u32))
 		goto out;
 	domain = pci_domain_nr(bus);
@@ -238,12 +238,12 @@ static int pci_write_aer(struct pci_bus *bus, unsigned int devfn, int where,
 			*sim ^= val;
 		else
 			*sim = val;
-		spin_unlock_irqrestore(&inject_lock, flags);
+		raw_spin_unlock_irqrestore(&inject_lock, flags);
 		return 0;
 	}
 out:
 	ops = __find_pci_bus_ops(bus);
-	spin_unlock_irqrestore(&inject_lock, flags);
+	raw_spin_unlock_irqrestore(&inject_lock, flags);
 	return ops->write(bus, devfn, where, size, val);
 }
 
@@ -271,14 +271,14 @@ static int pci_bus_set_aer_ops(struct pci_bus *bus)
 	if (!bus_ops)
 		return -ENOMEM;
 	ops = pci_bus_set_ops(bus, &pci_ops_aer);
-	spin_lock_irqsave(&inject_lock, flags);
+	raw_spin_lock_irqsave(&inject_lock, flags);
 	if (ops == &pci_ops_aer)
 		goto out;
 	pci_bus_ops_init(bus_ops, bus, ops);
 	list_add(&bus_ops->list, &pci_bus_ops_list);
 	bus_ops = NULL;
 out:
-	spin_unlock_irqrestore(&inject_lock, flags);
+	raw_spin_unlock_irqrestore(&inject_lock, flags);
 	kfree(bus_ops);
 	return 0;
 }
@@ -377,7 +377,7 @@ static int aer_inject(struct aer_error_inj *einj)
 				       uncor_mask);
 	}
 
-	spin_lock_irqsave(&inject_lock, flags);
+	raw_spin_lock_irqsave(&inject_lock, flags);
 
 	err = __find_aer_error_by_dev(dev);
 	if (!err) {
@@ -397,7 +397,7 @@ static int aer_inject(struct aer_error_inj *einj)
 	if (!aer_mask_override && einj->cor_status &&
 	    !(einj->cor_status & ~cor_mask)) {
 		ret = -EINVAL;
-		spin_unlock_irqrestore(&inject_lock, flags);
+		raw_spin_unlock_irqrestore(&inject_lock, flags);
 		printk(KERN_WARNING "The correctable error(s) is masked "
 				"by device\n");
 		goto out_put;
@@ -405,7 +405,7 @@ static int aer_inject(struct aer_error_inj *einj)
 	if (!aer_mask_override && einj->uncor_status &&
 	    !(einj->uncor_status & ~uncor_mask)) {
 		ret = -EINVAL;
-		spin_unlock_irqrestore(&inject_lock, flags);
+		raw_spin_unlock_irqrestore(&inject_lock, flags);
 		printk(KERN_WARNING "The uncorrectable error(s) is masked "
 				"by device\n");
 		goto out_put;
@@ -441,7 +441,7 @@ static int aer_inject(struct aer_error_inj *einj)
 		rperr->source_id &= 0x0000ffff;
 		rperr->source_id |= ((einj->bus << 8) | devfn) << 16;
 	}
-	spin_unlock_irqrestore(&inject_lock, flags);
+	raw_spin_unlock_irqrestore(&inject_lock, flags);
 
 	if (aer_mask_override) {
 		pci_write_config_dword(dev, pos_cap_err + PCI_ERR_COR_MASK,
@@ -524,12 +524,12 @@ static void __exit aer_inject_exit(void)
 		kfree(bus_ops);
 	}
 
-	spin_lock_irqsave(&inject_lock, flags);
+	raw_spin_lock_irqsave(&inject_lock, flags);
 	list_for_each_entry_safe(err, err_next, &einjected, list) {
 		list_del(&err->list);
 		kfree(err);
 	}
-	spin_unlock_irqrestore(&inject_lock, flags);
+	raw_spin_unlock_irqrestore(&inject_lock, flags);
 }
 
 module_init(aer_inject_init);
-- 
2.4.3

