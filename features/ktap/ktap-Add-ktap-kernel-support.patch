From 2c3bfdc290de1c8379c43be9182365365e653c4b Mon Sep 17 00:00:00 2001
From: Yang Shi <yang.shi@windriver.com>
Date: Mon, 15 Sep 2014 15:38:55 -0700
Subject: [PATCH 1/4] ktap: Add ktap kernel support

Original patches taken from:
http://github.com/ktap/ktap.git
Up to commit 5b16b61deb6c8a99d71e5167706a5f5566135828
("disable ktap raw tracepoint interface")

Add ktap headers into uapi/ to avoid conflicting with other kernel headers.
Correct headers include path accordingly and create Makefile for ktap kernel
module build.

Signed-off-by: Yang Shi <yang.shi@windriver.com>
---
 include/uapi/ktap/ktap_arch.h       |   33 +
 include/uapi/ktap/ktap_bc.h         |  369 ++++++++
 include/uapi/ktap/ktap_err.h        |   11 +
 include/uapi/ktap/ktap_errmsg.h     |  135 +++
 include/uapi/ktap/ktap_ffi.h        |  198 ++++
 include/uapi/ktap/ktap_types.h      |  517 ++++++++++
 kernel/trace/Makefile               |    2 +
 kernel/trace/ktap/Kconfig           |   32 +
 kernel/trace/ktap/Makefile          |   16 +
 kernel/trace/ktap/amalg.c           |   45 +
 kernel/trace/ktap/ffi/call_x86_64.S |  143 +++
 kernel/trace/ktap/ffi/cdata.c       |  433 +++++++++
 kernel/trace/ktap/ffi/ffi_call.c    |  355 +++++++
 kernel/trace/ktap/ffi/ffi_symbol.c  |  182 ++++
 kernel/trace/ktap/ffi/ffi_type.c    |   52 ++
 kernel/trace/ktap/ffi/ffi_util.c    |  200 ++++
 kernel/trace/ktap/kp_bcread.c       |  429 +++++++++
 kernel/trace/ktap/kp_bcread.h       |    6 +
 kernel/trace/ktap/kp_events.c       |  843 +++++++++++++++++
 kernel/trace/ktap/kp_events.h       |   71 ++
 kernel/trace/ktap/kp_mempool.c      |   94 ++
 kernel/trace/ktap/kp_mempool.h      |    8 +
 kernel/trace/ktap/kp_obj.c          |  293 ++++++
 kernel/trace/ktap/kp_obj.h          |   20 +
 kernel/trace/ktap/kp_str.c          |  393 ++++++++
 kernel/trace/ktap/kp_str.h          |   14 +
 kernel/trace/ktap/kp_tab.c          |  845 +++++++++++++++++
 kernel/trace/ktap/kp_tab.h          |   59 ++
 kernel/trace/ktap/kp_transport.c    |  649 +++++++++++++
 kernel/trace/ktap/kp_transport.h    |   13 +
 kernel/trace/ktap/kp_vm.c           | 1767 +++++++++++++++++++++++++++++++++++
 kernel/trace/ktap/kp_vm.h           |   43 +
 kernel/trace/ktap/ktap.c            |  276 ++++++
 kernel/trace/ktap/ktap.h            |  184 ++++
 kernel/trace/ktap/lib_ansi.c        |  142 +++
 kernel/trace/ktap/lib_base.c        |  409 ++++++++
 kernel/trace/ktap/lib_ffi.c         |  100 ++
 kernel/trace/ktap/lib_kdebug.c      |  202 ++++
 kernel/trace/ktap/lib_net.c         |  107 +++
 kernel/trace/ktap/lib_table.c       |   58 ++
 kernel/trace/ktap/lib_timer.c       |  210 +++++
 lib/Kconfig.debug                   |    1 +
 42 files changed, 9959 insertions(+)
 create mode 100644 include/uapi/ktap/ktap_arch.h
 create mode 100644 include/uapi/ktap/ktap_bc.h
 create mode 100644 include/uapi/ktap/ktap_err.h
 create mode 100644 include/uapi/ktap/ktap_errmsg.h
 create mode 100644 include/uapi/ktap/ktap_ffi.h
 create mode 100644 include/uapi/ktap/ktap_types.h
 create mode 100644 kernel/trace/ktap/Kconfig
 create mode 100644 kernel/trace/ktap/Makefile
 create mode 100644 kernel/trace/ktap/amalg.c
 create mode 100644 kernel/trace/ktap/ffi/call_x86_64.S
 create mode 100644 kernel/trace/ktap/ffi/cdata.c
 create mode 100644 kernel/trace/ktap/ffi/ffi_call.c
 create mode 100644 kernel/trace/ktap/ffi/ffi_symbol.c
 create mode 100644 kernel/trace/ktap/ffi/ffi_type.c
 create mode 100644 kernel/trace/ktap/ffi/ffi_util.c
 create mode 100644 kernel/trace/ktap/kp_bcread.c
 create mode 100644 kernel/trace/ktap/kp_bcread.h
 create mode 100644 kernel/trace/ktap/kp_events.c
 create mode 100644 kernel/trace/ktap/kp_events.h
 create mode 100644 kernel/trace/ktap/kp_mempool.c
 create mode 100644 kernel/trace/ktap/kp_mempool.h
 create mode 100644 kernel/trace/ktap/kp_obj.c
 create mode 100644 kernel/trace/ktap/kp_obj.h
 create mode 100644 kernel/trace/ktap/kp_str.c
 create mode 100644 kernel/trace/ktap/kp_str.h
 create mode 100644 kernel/trace/ktap/kp_tab.c
 create mode 100644 kernel/trace/ktap/kp_tab.h
 create mode 100644 kernel/trace/ktap/kp_transport.c
 create mode 100644 kernel/trace/ktap/kp_transport.h
 create mode 100644 kernel/trace/ktap/kp_vm.c
 create mode 100644 kernel/trace/ktap/kp_vm.h
 create mode 100644 kernel/trace/ktap/ktap.c
 create mode 100644 kernel/trace/ktap/ktap.h
 create mode 100644 kernel/trace/ktap/lib_ansi.c
 create mode 100644 kernel/trace/ktap/lib_base.c
 create mode 100644 kernel/trace/ktap/lib_ffi.c
 create mode 100644 kernel/trace/ktap/lib_kdebug.c
 create mode 100644 kernel/trace/ktap/lib_net.c
 create mode 100644 kernel/trace/ktap/lib_table.c
 create mode 100644 kernel/trace/ktap/lib_timer.c

diff --git a/include/uapi/ktap/ktap_arch.h b/include/uapi/ktap/ktap_arch.h
new file mode 100644
index 0000000..aeb7036
--- /dev/null
+++ b/include/uapi/ktap/ktap_arch.h
@@ -0,0 +1,33 @@
+#ifndef __KTAP_ARCH__
+#define __KTAP_ARCH__
+
+#ifdef __KERNEL__
+#include <linux/types.h>
+#include <asm/byteorder.h>
+
+#if defined(__LITTLE_ENDIAN)
+#define KP_LE				1
+#define KP_BE				0
+#define KP_ENDIAN_SELECT(le, be)        le
+#elif defined(__BIG_ENDIAN)
+#define KP_LE				0
+#define KP_BE				1
+#define KP_ENDIAN_SELECT(le, be)        be
+#endif
+
+#else /* __KERNEL__ */
+
+#if __BYTE_ORDER == __LITTLE_ENDIAN
+#define KP_LE				1
+#define KP_BE				0
+#define KP_ENDIAN_SELECT(le, be)        le
+#elif __BYTE_ORDER == __BIG_ENDIAN
+#define KP_LE				0
+#define KP_BE				1
+#define KP_ENDIAN_SELECT(le, be)        be
+#else
+#error "could not determine byte order"
+#endif
+
+#endif
+#endif
diff --git a/include/uapi/ktap/ktap_bc.h b/include/uapi/ktap/ktap_bc.h
new file mode 100644
index 0000000..fcebf9e
--- /dev/null
+++ b/include/uapi/ktap/ktap_bc.h
@@ -0,0 +1,369 @@
+/*
+ * Bytecode instruction format.
+ *
+ * Copyright (C) 2012-2014 Jovi Zhangwei <jovi.zhangwei@gmail.com>.
+ * Copyright (C) 2005-2014 Mike Pall.
+ */
+
+#ifndef __KTAP_BC_H__
+#define __KTAP_BC_H__
+
+#include <ktap/ktap_arch.h>
+
+/*TODO*/
+#define KP_STATIC_ASSERT(cond)
+#define kp_assert(cond)
+
+/* Types for handling bytecodes. */
+typedef uint32_t BCIns;  /* Bytecode instruction. */
+typedef uint32_t BCPos;  /* Bytecode position. */
+typedef uint32_t BCReg;  /* Bytecode register. */
+typedef int32_t BCLine;  /* Bytecode line number. */
+
+/*
+ * Bytecode instruction format, 32 bit wide, fields of 8 or 16 bit:
+ *
+ * +----+----+----+----+
+ * | B  | C  | A  | OP | Format ABC
+ * +----+----+----+----+
+ * |    D    | A  | OP | Format AD
+ * +--------------------
+ * MSB               LSB
+ *
+ * In-memory instructions are always stored in host byte order.
+ */
+
+/* Operand ranges and related constants. */
+#define BCMAX_A		0xff
+#define BCMAX_B		0xff
+#define BCMAX_C		0xff
+#define BCMAX_D		0xffff
+#define BCBIAS_J	0x8000
+#define NO_REG		BCMAX_A
+#define NO_JMP		(~(BCPos)0)
+
+/* Macros to get instruction fields. */
+#define bc_op(i)	((BCOp)((i)&0xff))
+#define bc_a(i)		((BCReg)(((i)>>8)&0xff))
+#define bc_b(i)		((BCReg)((i)>>24))
+#define bc_c(i)		((BCReg)(((i)>>16)&0xff))
+#define bc_d(i)		((BCReg)((i)>>16))
+#define bc_j(i)		((ptrdiff_t)bc_d(i)-BCBIAS_J)
+
+/* Macros to set instruction fields. */
+#define setbc_byte(p, x, ofs) \
+	((uint8_t *)(p))[KP_ENDIAN_SELECT(ofs, 3 - ofs)] = (uint8_t)(x)
+#define setbc_op(p, x)	setbc_byte(p, (x), 0)
+#define setbc_a(p, x)	setbc_byte(p, (x), 1)
+#define setbc_b(p, x)	setbc_byte(p, (x), 3)
+#define setbc_c(p, x)	setbc_byte(p, (x), 2)
+#define setbc_d(p, x) \
+	((uint16_t *)(p))[KP_ENDIAN_SELECT(1, 0)] = (uint16_t)(x)
+#define setbc_j(p, x)	setbc_d(p, (BCPos)((int32_t)(x)+BCBIAS_J))
+
+/* Macros to compose instructions. */
+#define BCINS_ABC(o, a, b, c) \
+	(((BCIns)(o))|((BCIns)(a)<<8)|((BCIns)(b)<<24)|((BCIns)(c)<<16))
+#define BCINS_AD(o, a, d) \
+	(((BCIns)(o))|((BCIns)(a)<<8)|((BCIns)(d)<<16))
+#define BCINS_AJ(o, a, j)	BCINS_AD(o, a, (BCPos)((int32_t)(j)+BCBIAS_J))
+
+/*
+ * Bytecode instruction definition. Order matters, see below.
+ *
+ * (name, filler, Amode, Bmode, Cmode or Dmode, metamethod)
+ *
+ * The opcode name suffixes specify the type for RB/RC or RD:
+ * V = variable slot
+ * S = string const
+ * N = number const
+ * P = primitive type (~itype)
+ * B = unsigned byte literal
+ * M = multiple args/results
+ */
+#define BCDEF(_) \
+	/* Comparison ops. ORDER OPR. */ \
+	_(ISLT,	var,	___,	var,	lt) \
+	_(ISGE,	var,	___,	var,	lt) \
+	_(ISLE,	var,	___,	var,	le) \
+	_(ISGT,	var,	___,	var,	le) \
+	\
+	_(ISEQV,	var,	___,	var,	eq) \
+	_(ISNEV,	var,	___,	var,	eq) \
+	_(ISEQS,	var,	___,	str,	eq) \
+	_(ISNES,	var,	___,	str,	eq) \
+	_(ISEQN,	var,	___,	num,	eq) \
+	_(ISNEN,	var,	___,	num,	eq) \
+	_(ISEQP,	var,	___,	pri,	eq) \
+	_(ISNEP,	var,	___,	pri,	eq) \
+	\
+	/* Unary test and copy ops. */ \
+	_(ISTC,	dst,	___,	var,	___) \
+	_(ISFC,	dst,	___,	var,	___) \
+	_(IST,	___,	___,	var,	___) \
+	_(ISF,	___,	___,	var,	___) \
+	_(ISTYPE,	var,	___,	lit,	___) \
+	_(ISNUM,	var,	___,	lit,	___) \
+	\
+	/* Unary ops. */ \
+	_(MOV,	dst,	___,	var,	___) \
+	_(NOT,	dst,	___,	var,	___) \
+	_(UNM,	dst,	___,	var,	unm) \
+	\
+	/* Binary ops. ORDER OPR. VV last, POW must be next. */ \
+	_(ADDVN,	dst,	var,	num,	add) \
+	_(SUBVN,	dst,	var,	num,	sub) \
+	_(MULVN,	dst,	var,	num,	mul) \
+	_(DIVVN,	dst,	var,	num,	div) \
+	_(MODVN,	dst,	var,	num,	mod) \
+	\
+	_(ADDNV,	dst,	var,	num,	add) \
+	_(SUBNV,	dst,	var,	num,	sub) \
+	_(MULNV,	dst,	var,	num,	mul) \
+	_(DIVNV,	dst,	var,	num,	div) \
+	_(MODNV,	dst,	var,	num,	mod) \
+	\
+	_(ADDVV,	dst,	var,	var,	add) \
+	_(SUBVV,	dst,	var,	var,	sub) \
+	_(MULVV,	dst,	var,	var,	mul) \
+	_(DIVVV,	dst,	var,	var,	div) \
+	_(MODVV,	dst,	var,	var,	mod) \
+	\
+	_(POW,	dst,	var,	var,	pow) \
+	_(CAT,	dst,	rbase,	rbase,	concat) \
+	\
+	/* Constant ops. */ \
+	_(KSTR,	dst,	___,	str,	___) \
+	_(KCDATA,	dst,	___,	cdata,	___) \
+	_(KSHORT,	dst,	___,	lits,	___) \
+	_(KNUM,	dst,	___,	num,	___) \
+	_(KPRI,	dst,	___,	pri,	___) \
+	_(KNIL,	base,	___,	base,	___) \
+	\
+	/* Upvalue and function ops. */ \
+	_(UGET,	dst,	___,	uv,	___) \
+	_(USETV,	uv,	___,	var,	___) \
+	_(UINCV,	uv,	___,	var,	___) \
+	_(USETS,	uv,	___,	str,	___) \
+	_(USETN,	uv,	___,	num,	___) \
+	_(UINCN,	uv,	___,	num,	___) \
+	_(USETP,	uv,	___,	pri,	___) \
+	_(UCLO,	rbase,	___,	jump,	___) \
+	_(FNEW,	dst,	___,	func,	gc) \
+	\
+	/* Table ops. */ \
+	_(TNEW,	dst,	___,	lit,	gc) \
+	_(TDUP,	dst,	___,	tab,	gc) \
+	_(GGET,	dst,	___,	str,	index) \
+	_(GSET,	var,	___,	str,	newindex) \
+	_(GINC,	var,	___,	str,	newindex) \
+	_(TGETV,	dst,	var,	var,	index) \
+	_(TGETS,	dst,	var,	str,	index) \
+	_(TGETB,	dst,	var,	lit,	index) \
+	_(TGETR,	dst,	var,	var,	index) \
+	_(TSETV,	var,	var,	var,	newindex) \
+	_(TINCV,	var,	var,	var,	newindex) \
+	_(TSETS,	var,	var,	str,	newindex) \
+	_(TINCS,	var,	var,	str,	newindex) \
+	_(TSETB,	var,	var,	lit,	newindex) \
+	_(TINCB,	var,	var,	lit,	newindex) \
+	_(TSETM,	base,	___,	num,	newindex) \
+	_(TSETR,	var,	var,	var,	newindex) \
+	\
+	/* Calls and vararg handling. T = tail call. */ \
+	_(CALLM,	base,	lit,	lit,	call) \
+	_(CALL,	base,	lit,	lit,	call) \
+	_(CALLMT,	base,	___,	lit,	call) \
+	_(CALLT,	base,	___,	lit,	call) \
+	_(ITERC,	base,	lit,	lit,	call) \
+	_(ITERN,	base,	lit,	lit,	call) \
+	_(VARG,	base,	lit,	lit,	___) \
+	_(ISNEXT,	base,	___,	jump,	___) \
+	\
+	/* Returns. */ \
+	_(RETM,	base,	___,	lit,	___) \
+	_(RET,	rbase,	___,	lit,	___) \
+	_(RET0,	rbase,	___,	lit,	___) \
+	_(RET1,	rbase,	___,	lit,	___) \
+	\
+	/* Loops and branches. I/J = interp/JIT, I/C/L = init/call/loop. */ \
+	_(FORI,	base,	___,	jump,	___) \
+	_(JFORI,	base,	___,	jump,	___) \
+	\
+	_(FORL,	base,	___,	jump,	___) \
+	_(IFORL,	base,	___,	jump,	___) \
+	_(JFORL,	base,	___,	lit,	___) \
+	\
+	_(ITERL,	base,	___,	jump,	___) \
+	_(IITERL,	base,	___,	jump,	___) \
+	_(JITERL,	base,	___,	lit,	___) \
+	\
+	_(LOOP,	rbase,	___,	jump,	___) \
+	_(ILOOP,	rbase,	___,	jump,	___) \
+	_(JLOOP,	rbase,	___,	lit,	___) \
+	\
+	_(JMP,	rbase,	___,	jump,	___) \
+	\
+	/*Function headers. I/J = interp/JIT, F/V/C = fixarg/vararg/C func.*/ \
+	_(FUNCF,	rbase,	___,	___,	___) \
+	_(IFUNCF,	rbase,	___,	___,	___) \
+	_(JFUNCF,	rbase,	___,	lit,	___) \
+	_(FUNCV,	rbase,	___,	___,	___) \
+	_(IFUNCV,	rbase,	___,	___,	___) \
+	_(JFUNCV,	rbase,	___,	lit,	___) \
+	_(FUNCC,	rbase,	___,	___,	___) \
+	_(FUNCCW,	rbase,	___,	___,	___) \
+	\
+	/* specific purpose bc. */	\
+	_(VARGN,	dst, ___,	lit,	___) \
+	_(VARGSTR,	dst, ___,	lit,	___) \
+	_(VPROBENAME,	dst, ___,	lit,	___) \
+	_(VPID,		dst, ___,	lit,	___) \
+	_(VTID,		dst, ___,	lit,	___) \
+	_(VUID,		dst, ___,	lit,	___) \
+	_(VCPU,		dst, ___,	lit,	___) \
+	_(VEXECNAME,	dst, ___,	lit,	___) \
+	\
+	_(GFUNC,	dst, ___,	___,	___) /*load global C function*/
+
+/* Bytecode opcode numbers. */
+typedef enum {
+#define BCENUM(name, ma, mb, mc, mt)	BC_##name,
+	BCDEF(BCENUM)
+#undef BCENUM
+	BC__MAX
+} BCOp;
+
+KP_STATIC_ASSERT((int)BC_ISEQV+1 == (int)BC_ISNEV);
+KP_STATIC_ASSERT(((int)BC_ISEQV^1) == (int)BC_ISNEV);
+KP_STATIC_ASSERT(((int)BC_ISEQS^1) == (int)BC_ISNES);
+KP_STATIC_ASSERT(((int)BC_ISEQN^1) == (int)BC_ISNEN);
+KP_STATIC_ASSERT(((int)BC_ISEQP^1) == (int)BC_ISNEP);
+KP_STATIC_ASSERT(((int)BC_ISLT^1) == (int)BC_ISGE);
+KP_STATIC_ASSERT(((int)BC_ISLE^1) == (int)BC_ISGT);
+KP_STATIC_ASSERT(((int)BC_ISLT^3) == (int)BC_ISGT);
+KP_STATIC_ASSERT((int)BC_IST-(int)BC_ISTC == (int)BC_ISF-(int)BC_ISFC);
+KP_STATIC_ASSERT((int)BC_CALLT-(int)BC_CALL == (int)BC_CALLMT-(int)BC_CALLM);
+KP_STATIC_ASSERT((int)BC_CALLMT + 1 == (int)BC_CALLT);
+KP_STATIC_ASSERT((int)BC_RETM + 1 == (int)BC_RET);
+KP_STATIC_ASSERT((int)BC_FORL + 1 == (int)BC_IFORL);
+KP_STATIC_ASSERT((int)BC_FORL + 2 == (int)BC_JFORL);
+KP_STATIC_ASSERT((int)BC_ITERL + 1 == (int)BC_IITERL);
+KP_STATIC_ASSERT((int)BC_ITERL + 2 == (int)BC_JITERL);
+KP_STATIC_ASSERT((int)BC_LOOP + 1 == (int)BC_ILOOP);
+KP_STATIC_ASSERT((int)BC_LOOP + 2 == (int)BC_JLOOP);
+KP_STATIC_ASSERT((int)BC_FUNCF + 1 == (int)BC_IFUNCF);
+KP_STATIC_ASSERT((int)BC_FUNCF + 2 == (int)BC_JFUNCF);
+KP_STATIC_ASSERT((int)BC_FUNCV + 1 == (int)BC_IFUNCV);
+KP_STATIC_ASSERT((int)BC_FUNCV + 2 == (int)BC_JFUNCV);
+
+/* This solves a circular dependency problem, change as needed. */
+#define FF_next_N	4
+
+/* Stack slots used by FORI/FORL, relative to operand A. */
+enum {
+	FORL_IDX, FORL_STOP, FORL_STEP, FORL_EXT
+};
+
+/* Bytecode operand modes. ORDER BCMode */
+typedef enum {
+	/* Mode A must be <= 7 */
+	BCMnone, BCMdst, BCMbase, BCMvar, BCMrbase, BCMuv,
+	BCMlit, BCMlits, BCMpri, BCMnum, BCMstr, BCMtab, BCMfunc,
+	BCMjump, BCMcdata,
+	BCM_max
+} BCMode;
+
+#define BCM___	BCMnone
+
+#define bcmode_a(op)	((BCMode)(bc_mode[op] & 7))
+#define bcmode_b(op)	((BCMode)((bc_mode[op] >> 3) & 15))
+#define bcmode_c(op)	((BCMode)((bc_mode[op] >> 7) & 15))
+#define bcmode_d(op)	bcmode_c(op)
+#define bcmode_hasd(op)	((bc_mode[op] & (15 << 3)) == (BCMnone << 3))
+#define bcmode_mm(op)	((MMS)(bc_mode[op] >> 11))
+
+#define BCMODE(name, ma, mb, mc, mm) \
+	(BCM##ma | (BCM##mb << 3) | (BCM##mc << 7)|(MM_##mm << 11)),
+#define BCMODE_FF	0
+
+static inline int bc_isret(BCOp op)
+{
+	return (op == BC_RETM || op == BC_RET || op == BC_RET0 ||
+		op == BC_RET1);
+}
+
+/* 
+ * Metamethod definition
+ * Note ktap don't use any lua methmethod currently.
+ */
+typedef enum {
+	MM_lt,
+	MM_le,
+	MM_eq,
+	MM_unm,
+	MM_add,
+	MM_sub,
+	MM_mul,
+	MM_div,
+	MM_mod,
+	MM_pow,
+	MM_concat,
+	MM_gc,
+	MM_index,
+	MM_newindex,
+	MM_call,
+	MM__MAX,
+	MM____ = MM__MAX
+} MMS;
+
+
+/* -- Bytecode dump format ------------------------------------------------ */
+
+/*
+** dump   = header proto+ 0U
+** header = ESC 'L' 'J' versionB flagsU [namelenU nameB*]
+** proto  = lengthU pdata
+** pdata  = phead bcinsW* uvdataH* kgc* knum* [debugB*]
+** phead  = flagsB numparamsB framesizeB numuvB numkgcU numknU numbcU
+**          [debuglenU [firstlineU numlineU]]
+** kgc    = kgctypeU { ktab | (loU hiU) | (rloU rhiU iloU ihiU) | strB* }
+** knum   = intU0 | (loU1 hiU)
+** ktab   = narrayU nhashU karray* khash*
+** karray = ktabk
+** khash  = ktabk ktabk
+** ktabk  = ktabtypeU { intU | (loU hiU) | strB* }
+**
+** B = 8 bit, H = 16 bit, W = 32 bit, U = ULEB128 of W, U0/U1 = ULEB128 of W+1
+*/
+
+/* Bytecode dump header. */
+#define BCDUMP_HEAD1		0x15
+#define BCDUMP_HEAD2		0x22
+#define BCDUMP_HEAD3		0x06
+
+/* If you perform *any* kind of private modifications to the bytecode itself
+** or to the dump format, you *must* set BCDUMP_VERSION to 0x80 or higher.
+*/
+#define BCDUMP_VERSION		1
+
+/* Compatibility flags. */
+#define BCDUMP_F_BE		0x01
+#define BCDUMP_F_STRIP		0x02
+#define BCDUMP_F_FFI		0x04
+
+#define BCDUMP_F_KNOWN		(BCDUMP_F_FFI*2-1)
+
+/* Type codes for the GC constants of a prototype. Plus length for strings. */
+enum {
+	BCDUMP_KGC_CHILD, BCDUMP_KGC_TAB, BCDUMP_KGC_I64, BCDUMP_KGC_U64,
+	BCDUMP_KGC_COMPLEX, BCDUMP_KGC_STR
+};
+
+/* Type codes for the keys/values of a constant table. */
+enum {
+	BCDUMP_KTAB_NIL, BCDUMP_KTAB_FALSE, BCDUMP_KTAB_TRUE,
+	BCDUMP_KTAB_INT, BCDUMP_KTAB_NUM, BCDUMP_KTAB_STR
+};
+
+#endif /* __KTAP_BC_H__ */
diff --git a/include/uapi/ktap/ktap_err.h b/include/uapi/ktap/ktap_err.h
new file mode 100644
index 0000000..b7e1a31
--- /dev/null
+++ b/include/uapi/ktap/ktap_err.h
@@ -0,0 +1,11 @@
+#ifndef __KTAP_ERR_H__
+#define __KTAP_ERR_H__
+
+typedef enum {
+#define ERRDEF(name, msg) \
+	KP_ERR_##name, KP_ERR_##name##_ = KP_ERR_##name + sizeof(msg)-1,
+#include "ktap_errmsg.h"
+	KP_ERR__MAX
+} ErrMsg;
+
+#endif
diff --git a/include/uapi/ktap/ktap_errmsg.h b/include/uapi/ktap/ktap_errmsg.h
new file mode 100644
index 0000000..fd7a081
--- /dev/null
+++ b/include/uapi/ktap/ktap_errmsg.h
@@ -0,0 +1,135 @@
+/*
+ * VM error messages.
+ * Copyright (C) 2012-2014 Jovi Zhangwei <jovi.zhangwei@gmail.com>.
+ * Copyright (C) 2005-2014 Mike Pall.
+ */
+
+/* Basic error handling. */
+ERRDEF(ERRMEM,	"not enough memory")
+ERRDEF(ERRERR,	"error in error handling")
+
+/* Allocations. */
+ERRDEF(STROV,	"string length overflow")
+ERRDEF(UDATAOV,	"userdata length overflow")
+ERRDEF(STKOV,	"stack overflow")
+ERRDEF(STKOVM,	"stack overflow (%s)")
+ERRDEF(TABOV,	"table overflow")
+
+/* Table indexing. */
+ERRDEF(NANIDX,	"table index is NaN")
+ERRDEF(NILIDX,	"table index is nil")
+ERRDEF(NEXTIDX,	"invalid key to " KTAP_QL("next"))
+
+/* Metamethod resolving. */
+ERRDEF(BADCALL,	"attempt to call a %s value")
+ERRDEF(BADOPRT,	"attempt to %s %s " KTAP_QS " (a %s value)")
+ERRDEF(BADOPRV,	"attempt to %s a %s value")
+ERRDEF(BADCMPT,	"attempt to compare %s with %s")
+ERRDEF(BADCMPV,	"attempt to compare two %s values")
+ERRDEF(GETLOOP,	"loop in gettable")
+ERRDEF(SETLOOP,	"loop in settable")
+ERRDEF(OPCALL,	"call")
+ERRDEF(OPINDEX,	"index")
+ERRDEF(OPARITH,	"perform arithmetic on")
+ERRDEF(OPCAT,	"concatenate")
+ERRDEF(OPLEN,	"get length of")
+
+/* Type checks. */
+ERRDEF(BADSELF,	"calling " KTAP_QS " on bad self (%s)")
+ERRDEF(BADARG,	"bad argument #%d to " KTAP_QS " (%s)")
+ERRDEF(BADTYPE,	"%s expected, got %s")
+ERRDEF(BADVAL,	"invalid value")
+ERRDEF(NOVAL,	"value expected")
+ERRDEF(NOCORO,	"coroutine expected")
+ERRDEF(NOTABN,	"nil or table expected")
+ERRDEF(NOLFUNC,	"ktap function expected")
+ERRDEF(NOFUNCL,	"function or level expected")
+ERRDEF(NOSFT,	"string/function/table expected")
+ERRDEF(NOPROXY,	"boolean or proxy expected")
+ERRDEF(FORINIT,	KTAP_QL("for") " initial value must be a number")
+ERRDEF(FORLIM,	KTAP_QL("for") " limit must be a number")
+ERRDEF(FORSTEP,	KTAP_QL("for") " step must be a number")
+
+/* C API checks. */
+ERRDEF(NOENV,	"no calling environment")
+ERRDEF(CYIELD,	"attempt to yield across C-call boundary")
+ERRDEF(BADLU,	"bad light userdata pointer")
+
+/* Standard library function errors. */
+ERRDEF(ASSERT,	"assertion failed!")
+ERRDEF(PROTMT,	"cannot change a protected metatable")
+ERRDEF(UNPACK,	"too many results to unpack")
+ERRDEF(RDRSTR,	"reader function must return a string")
+ERRDEF(PRTOSTR,	KTAP_QL("tostring") " must return a string to " KTAP_QL("print"))
+ERRDEF(IDXRNG,	"index out of range")
+ERRDEF(BASERNG,	"base out of range")
+ERRDEF(LVLRNG,	"level out of range")
+ERRDEF(INVLVL,	"invalid level")
+ERRDEF(INVOPT,	"invalid option")
+ERRDEF(INVOPTM,	"invalid option " KTAP_QS)
+ERRDEF(INVFMT,	"invalid format")
+ERRDEF(SETFENV,	KTAP_QL("setfenv") " cannot change environment of given object")
+ERRDEF(CORUN,	"cannot resume running coroutine")
+ERRDEF(CODEAD,	"cannot resume dead coroutine")
+ERRDEF(COSUSP,	"cannot resume non-suspended coroutine")
+ERRDEF(TABINS,	"wrong number of arguments to " KTAP_QL("insert"))
+ERRDEF(TABCAT,	"invalid value (%s) at index %d in table for " KTAP_QL("concat"))
+ERRDEF(TABSORT,	"invalid order function for sorting")
+ERRDEF(IOCLFL,	"attempt to use a closed file")
+ERRDEF(IOSTDCL,	"standard file is closed")
+ERRDEF(OSUNIQF,	"unable to generate a unique filename")
+ERRDEF(OSDATEF,	"field " KTAP_QS " missing in date table")
+ERRDEF(STRDUMP,	"unable to dump given function")
+ERRDEF(STRSLC,	"string slice too long")
+ERRDEF(STRPATB,	"missing " KTAP_QL("[") " after " KTAP_QL("%f") " in pattern")
+ERRDEF(STRPATC,	"invalid pattern capture")
+ERRDEF(STRPATE,	"malformed pattern (ends with " KTAP_QL("%") ")")
+ERRDEF(STRPATM,	"malformed pattern (missing " KTAP_QL("]") ")")
+ERRDEF(STRPATU,	"unbalanced pattern")
+ERRDEF(STRPATX,	"pattern too complex")
+ERRDEF(STRCAPI,	"invalid capture index")
+ERRDEF(STRCAPN,	"too many captures")
+ERRDEF(STRCAPU,	"unfinished capture")
+ERRDEF(STRFMT,	"invalid option " KTAP_QS " to " KTAP_QL("format"))
+ERRDEF(STRGSRV,	"invalid replacement value (a %s)")
+ERRDEF(BADMODN,	"name conflict for module " KTAP_QS)
+ERRDEF(JITOPT,	"unknown or malformed optimization flag " KTAP_QS)
+
+/* Lexer/parser errors. */
+ERRDEF(XMODE,	"attempt to load chunk with wrong mode")
+ERRDEF(XNEAR,	"%s near " KTAP_QS)
+ERRDEF(XLINES,	"chunk has too many lines")
+ERRDEF(XLEVELS,	"chunk has too many syntax levels")
+ERRDEF(XNUMBER,	"malformed number")
+ERRDEF(XLSTR,	"unfinished long string")
+ERRDEF(XLCOM,	"unfinished long comment")
+ERRDEF(XSTR,	"unfinished string")
+ERRDEF(XESC,	"invalid escape sequence")
+ERRDEF(XLDELIM,	"invalid long string delimiter")
+ERRDEF(XTOKEN,	KTAP_QS " expected")
+ERRDEF(XJUMP,	"control structure too long")
+ERRDEF(XSLOTS,	"function or expression too complex")
+ERRDEF(XLIMC,	"chunk has more than %d local variables")
+ERRDEF(XLIMM,	"main function has more than %d %s")
+ERRDEF(XLIMF,	"function at line %d has more than %d %s")
+ERRDEF(XMATCH,	KTAP_QS " expected (to close " KTAP_QS " at line %d)")
+ERRDEF(XFIXUP,	"function too long for return fixup")
+ERRDEF(XPARAM,	"<name> or " KTAP_QL("...") " expected")
+ERRDEF(XAMBIG,	"ambiguous syntax (function call x new statement)")
+ERRDEF(XFUNARG,	"function arguments expected")
+ERRDEF(XSYMBOL,	"unexpected symbol")
+ERRDEF(XDOTS,	"cannot use " KTAP_QL("...") " outside a vararg function")
+ERRDEF(XSYNTAX,	"syntax error")
+ERRDEF(XFOR,	KTAP_QL("=") " or " KTAP_QL("in") " expected")
+ERRDEF(XBREAK,	"no loop to break")
+ERRDEF(XLUNDEF,	"undefined label " KTAP_QS)
+ERRDEF(XLDUP,	"duplicate label " KTAP_QS)
+ERRDEF(XGSCOPE,	"<goto %s> jumps into the scope of local " KTAP_QS)
+ERRDEF(XEVENTDEF,"cannot parse eventdef " KTAP_QS)
+
+/* Bytecode reader errors. */
+ERRDEF(BCFMT,	"cannot load incompatible bytecode")
+ERRDEF(BCBAD,	"cannot load malformed bytecode")
+
+#undef ERRDEF
+
diff --git a/include/uapi/ktap/ktap_ffi.h b/include/uapi/ktap/ktap_ffi.h
new file mode 100644
index 0000000..4200cab
--- /dev/null
+++ b/include/uapi/ktap/ktap_ffi.h
@@ -0,0 +1,198 @@
+#ifndef __KTAP_FFI_H__
+#define __KTAP_FFI_H__
+
+#ifdef CONFIG_KTAP_FFI
+
+#include "../include/ktap_types.h"
+
+/*
+ * Types design in FFI module
+ *
+ * ktap_cdata_t is an instance of csymbol, so it's a combination of csymbol
+ * and it's actual data in memory.
+ *
+ * csymbol structs are globally unique and readonly type that represent C
+ * types.  For non scalar C types like struct and function, helper structs are
+ * used to store detailed information. See csymbol_func and csymbol_struct for
+ * more information.
+ */
+
+typedef enum {
+	/* 0 - 4 */
+	FFI_VOID,
+	FFI_UINT8,
+	FFI_INT8,
+	FFI_UINT16,
+	FFI_INT16,
+	/* 5 - 9 */
+	FFI_UINT32,
+	FFI_INT32,
+	FFI_UINT64,
+	FFI_INT64,
+	FFI_PTR,
+	/* 10 - 12 */
+	FFI_FUNC,
+	FFI_STRUCT,
+	FFI_UNION,
+	FFI_UNKNOWN,
+} ffi_type;
+#define NUM_FFI_TYPE ((int)FFI_UNKNOWN)
+
+
+/* following struct and macros are added for C typedef
+ * size and alignment calculation */
+typedef struct {
+	size_t size;
+	size_t align;
+	const char *name;
+} ffi_mode;
+extern const ffi_mode const ffi_type_modes[];
+
+#define ffi_type_size(t) (ffi_type_modes[t].size)
+#define ffi_type_align(t) (ffi_type_modes[t].align)
+#define ffi_type_name(t) (ffi_type_modes[t].name)
+
+
+/* start of csymbol definition */
+#define CSYM_NAME_MAX_LEN 64
+
+typedef struct csymbol_func {
+	void *addr;		/* function address */
+	csymbol_id ret_id;	/* function return type */
+	int arg_nr;		/* number of arguments */
+	csymbol_id *arg_ids;	/* function argument types */
+	unsigned has_var_arg;	/* is this a var arg function? */
+} csymbol_func;
+
+typedef struct struct_member {
+	char name[CSYM_NAME_MAX_LEN];	/* name for this struct member */
+	csymbol_id id;			/* type for this struct member */
+	int len;				/* length of the array, -1 for non-array */
+} struct_member;
+
+typedef struct csymbol_struct {
+	int memb_nr;			/* number of members */
+	struct_member *members;		/* array for each member definition */
+	size_t size;			/* bytes used to store struct */
+	/* alignment of the struct, 0 indicates uninitialization */
+	size_t align;
+} csymbol_struct;
+
+
+/* wrapper struct for all C symbols */
+typedef struct csymbol {
+	char name[CSYM_NAME_MAX_LEN];	/* name for this symbol */
+	ffi_type type;			/* type for this symbol  */
+	/* following members are used only for non scalar C types */
+	union {
+		csymbol_id p;		/* pointer type */
+		csymbol_func f;		/* C function type */
+		csymbol_struct st;	/* struct/union type */
+		csymbol_id td;		/* typedef type */
+	} u;
+} csymbol;
+
+
+/* helper macors for c symbols */
+#define max_csym_id(ks) (G(ks)->ffis.csym_nr)
+
+/* lookup csymbol address by its id */
+inline csymbol *ffi_get_csym_by_id(ktap_state_t *ks, csymbol_id id);
+#define id_to_csym(ks, id) (ffi_get_csym_by_id(ks, id))
+
+/* helper macros for struct csymbol */
+#define csym_type(cs) ((cs)->type)
+#define csym_name(cs) ((cs)->name)
+
+/*
+ * helper macros for pointer symbol
+ */
+#define csym_ptr_deref_id(cs) ((cs)->u.p)
+#define csym_set_ptr_deref_id(cs, id) ((cs)->u.p = (id))
+/* following macro gets csymbol address */
+#define csym_ptr_deref(ks, cs) (id_to_csym(ks, csym_ptr_deref_id(cs)))
+
+/*
+ * helper macros for function symbol
+ * csym_* accepts csymbol type
+ * csymf_* accepts csymbol_func type
+ */
+#define csymf_addr(csf) ((csf)->addr)
+#define csymf_ret_id(csf) ((csf)->ret_id)
+#define csymf_arg_nr(csf) ((csf)->arg_nr)
+#define csymf_arg_ids(csf) ((csf)->arg_ids)
+/* get csymbol id for the nth argument */
+#define csymf_arg_id(csf, n) ((csf)->arg_ids[n])
+#define csym_func(cs) (&((cs)->u.f))
+#define csym_func_addr(cs) (csymf_addr(csym_func(cs)))
+#define csym_func_arg_ids(cs) (csymf_arg_ids(csym_func(cs)))
+/* following macros get csymbol address */
+#define csymf_ret(ks, csf) (id_to_csym(ks, csymf_ret_id(csf)))
+/* get csymbol address for the nth argument */
+#define csymf_arg(ks, csf, n) (id_to_csym(ks, csymf_arg_id(csf, n)))
+#define csym_func_arg(ks, cs, n) (csymf_arg(ks, csym_func(cs), n))
+
+/*
+ * helper macors for struct symbol
+ * csym_* accepts csymbol type
+ * csymst_* accepts csymbol_struct type
+ */
+#define csymst_mb_nr(csst) ((csst)->memb_nr)
+#define csym_struct(cs) (&(cs)->u.st)
+#define csym_struct_mb(cs, n) (csymst_mb(csym_struct(cs), n))
+#define csym_struct_mb_nr(cs) (csymst_mb_nr(csym_struct(cs)))
+/* following macro gets csymbol address for the nth struct member */
+#define csymst_mb(csst, n) (&(csst)->members[n])
+#define csymst_mb_name(csst, n) ((csst)->members[n].name)
+#define csymst_mb_id(csst, n) ((csst)->members[n].id)
+#define csymst_mb_len(csst, n) ((csst)->members[n].len)
+#define csymst_mb_csym(ks, csst, n) (id_to_csym(ks, (csst)->members[n].id))
+
+
+/*
+ * helper macros for ktap_cdata_t type
+ */
+#define cd_csym_id(cd) ((cd)->id)
+#define cd_set_csym_id(cd, id) (cd_csym_id(cd) = (id))
+#define cd_csym(ks, cd) (id_to_csym(ks, cd_csym_id(cd)))
+#define cd_type(ks, cd) (csym_type(cd_csym(ks, cd)))
+
+#define cd_int(cd) ((cd)->u.i)
+#define cd_ptr(cd) ((cd)->u.p.addr)
+#define cd_ptr_nmemb(cd) ((cd)->u.p.nmemb)
+#define cd_record(cd) ((cd)->u.rec)
+#define cd_struct(cd) cd_record(cd)
+#define cd_union(cd) cd_record(cd)
+
+#ifdef __KERNEL__
+size_t csym_size(ktap_state_t *ks, csymbol *sym);
+size_t csym_align(ktap_state_t *ks, csymbol *sym);
+size_t csym_record_mb_offset_by_name(ktap_state_t *ks,
+		csymbol *cs, const char *name);
+struct_member *csymst_mb_by_name(ktap_state_t *ks,
+		csymbol_struct *csst, const char *name);
+
+void ffi_free_symbols(ktap_state_t *ks);
+csymbol_id ffi_get_csym_id(ktap_state_t *ks, char *name);
+
+ktap_cdata_t *kp_cdata_new(ktap_state_t *ks, csymbol_id id);
+ktap_cdata_t *kp_cdata_new_ptr(ktap_state_t *ks, void *addr,
+		int nmemb, csymbol_id id, int to_allocate);
+ktap_cdata_t *kp_cdata_new_record(ktap_state_t *ks, void *val, csymbol_id id);
+void kp_cdata_dump(ktap_state_t *ks, ktap_cdata_t *cd);
+int kp_cdata_type_match(ktap_state_t *ks, csymbol *cs, ktap_val_t *val);
+void kp_cdata_unpack(ktap_state_t *ks, char *dst, csymbol *cs, ktap_val_t *val);
+void kp_cdata_ptr_set(ktap_state_t *ks, ktap_cdata_t *cd,
+		ktap_val_t *key, ktap_val_t *val);
+void kp_cdata_ptr_get(ktap_state_t *ks, ktap_cdata_t *cd,
+		ktap_val_t *key, ktap_val_t *val);
+void kp_cdata_record_set(ktap_state_t *ks, ktap_cdata_t *cd,
+		 ktap_val_t *key, ktap_val_t *val);
+void kp_cdata_record_get(ktap_state_t *ks, ktap_cdata_t *cd,
+		 ktap_val_t *key, ktap_val_t *val);
+
+int ffi_call(ktap_state_t *ks, csymbol_func *cf);
+
+#endif /* for CONFIG_FFI_KTAP */
+#endif /* for __KERNEL__ */
+#endif /* __KTAP_FFI_H__ */
diff --git a/include/uapi/ktap/ktap_types.h b/include/uapi/ktap/ktap_types.h
new file mode 100644
index 0000000..f477ac7
--- /dev/null
+++ b/include/uapi/ktap/ktap_types.h
@@ -0,0 +1,517 @@
+/*
+ * ktap types definition.
+ *
+ * Copyright (C) 2012-2014 Jovi Zhangwei <jovi.zhangwei@gmail.com>.
+ * Copyright (C) 2005-2014 Mike Pall.
+ * Copyright (C) 1994-2008 Lua.org, PUC-Rio.
+ */
+
+#ifndef __KTAP_TYPES_H__
+#define __KTAP_TYPES_H__
+
+#ifdef __KERNEL__
+#include <linux/perf_event.h>
+#else
+typedef char u8;
+#include <stdlib.h>
+#include <stdio.h>
+#include <string.h>
+#include <stdint.h>
+typedef int ptrdiff_t;
+#endif
+
+#include <ktap/ktap_bc.h>
+
+/* Various VM limits. */
+#define KP_MAX_MEMPOOL_SIZE	10000	/* Max. mempool size(Kbytes). */
+#define KP_MAX_STR	512		/* Max. string length. */
+#define KP_MAX_STRNUM	9999		/* Max. string number. */
+
+#define KP_MAX_STRTAB	(1<<26)		/* Max. string table size. */
+#define KP_MAX_HBITS	26		/* Max. hash bits. */
+#define KP_MAX_ABITS	28		/* Max. bits of array key. */
+#define KP_MAX_ASIZE	((1<<(KP_MAX_ABITS-1))+1)  /* Max. array part size. */
+#define KP_MAX_COLOSIZE	16		/* Max. elems for colocated array. */
+
+#define KP_MAX_LINE	1000		/* Max. source code line number. */
+#define KP_MAX_XLEVEL	200		/* Max. syntactic nesting level. */
+#define KP_MAX_BCINS	(1<<26)		/* Max. # of bytecode instructions. */
+#define KP_MAX_SLOTS	250		/* Max. # of slots in a ktap func. */
+#define KP_MAX_LOCVAR	200		/* Max. # of local variables. */
+#define KP_MAX_UPVAL	60		/* Max. # of upvalues. */
+
+#define KP_MAX_CACHED_CFUNCTION	128	/* Max. cached global cfunction */
+
+#define KP_MAX_STACK_DEPTH	50	/* Max. stack depth */
+
+/*
+ * The first argument type of kdebug.trace_by_id()
+ * The value is a userspace memory pointer.
+ * Maybe embed it into the trunk file in future.
+ */
+typedef struct ktap_eventdesc {
+	int nr;  /* the number of events id */
+	int *id_arr; /* events id array */
+	char *filter;
+} ktap_eventdesc_t;
+
+
+/* ktap option for each script */
+typedef struct ktap_option {
+	char *trunk; /* __user */
+	int trunk_len;
+	int argc;
+	char **argv; /* __user */
+	int verbose;
+	int trace_pid;
+	int workload;
+	int trace_cpu;
+	int print_timestamp;
+	int quiet;
+	int dry_run;
+} ktap_option_t;
+
+/*
+ * Ioctls that can be done on a ktap fd:
+ * todo: use _IO macro in include/uapi/asm-generic/ioctl.h
+ */
+#define KTAP_CMD_IOC_VERSION		('$' + 0)
+#define KTAP_CMD_IOC_RUN		('$' + 1)
+#define KTAP_CMD_IOC_EXIT		('$' + 3)
+
+#define KTAP_VERSION_MAJOR       "0"
+#define KTAP_VERSION_MINOR       "4"
+
+#define KTAP_VERSION    "ktap " KTAP_VERSION_MAJOR "." KTAP_VERSION_MINOR
+#define KTAP_AUTHOR    "Jovi Zhangwei <jovi.zhangwei@gmail.com>"
+#define KTAP_COPYRIGHT  KTAP_VERSION "  Copyright (C) 2012-2014, " KTAP_AUTHOR
+
+#define MYINT(s)        (s[0] - '0')
+#define VERSION         (MYINT(KTAP_VERSION_MAJOR) * 16 + MYINT(KTAP_VERSION_MINOR))
+
+typedef long ktap_number;
+typedef int ktap_instr_t;
+typedef union ktap_obj ktap_obj_t;
+
+struct ktap_state;
+typedef int (*ktap_cfunction) (struct ktap_state *ks);
+
+/* ktap_val_t is basic value type in ktap stack, for reference all objects */
+typedef struct ktap_val {
+	union {
+		ktap_obj_t *gc;		/* collectable objects, str/tab/... */
+		void *p;		/* light userdata */
+		ktap_cfunction f;	/* light C functions */
+		ktap_number n;		/* numbers */
+		struct {
+			uint16_t depth;	/* stack depth */
+			uint16_t skip;	/* skip stack entries */
+		} stack;
+	} val;
+	union {
+		int type;		/* type for above val */
+		const unsigned int *pcr;/* Overlaps PC for ktap frames.*/
+	};
+} ktap_val_t;
+
+typedef ktap_val_t *StkId;
+
+#define GCHeader ktap_obj_t *nextgc; u8 gct;
+
+typedef struct ktap_str {
+	GCHeader;
+	u8 reserved;  /* Used by lexer for fast lookup of reserved words. */
+	u8 extra;
+	unsigned int hash;
+	int len;  /* number of characters in string */
+} ktap_str_t;
+
+typedef struct ktap_upval {
+	GCHeader;
+	uint8_t closed;	/* Set if closed (i.e. uv->v == &uv->u.value). */
+	uint8_t immutable;	/* Immutable value. */
+	union {
+		ktap_val_t tv; /* If closed: the value itself. */
+		struct { /* If open: double linked list, anchored at thread. */
+			struct ktap_upval *prev;
+			struct ktap_upval *next;
+		};
+	};
+	ktap_val_t *v;  /* Points to stack slot (open) or above (closed). */
+} ktap_upval_t;
+
+
+typedef struct ktap_func {
+	GCHeader;
+	u8 nupvalues;
+	BCIns *pc;
+	struct ktap_proto *p;
+	struct ktap_upval *upvals[1];  /* list of upvalues */
+} ktap_func_t;
+
+typedef struct ktap_proto {
+	GCHeader;
+	uint8_t numparams;	/* Number of parameters. */
+	uint8_t framesize;	/* Fixed frame size. */
+	int sizebc;		/* Number of bytecode instructions. */
+	ktap_obj_t *gclist;
+	void *k;	/* Split constant array (points to the middle). */
+	void *uv;	/* Upvalue list. local slot|0x8000 or parent uv idx. */
+	int sizekgc;	/* Number of collectable constants. */
+	int sizekn;	/* Number of lua_Number constants. */
+	int sizept;	/* Total size including colocated arrays. */
+	uint8_t sizeuv;	/* Number of upvalues. */
+	uint8_t flags;	/* Miscellaneous flags (see below). */
+
+	/* --- The following fields are for debugging/tracebacks only --- */
+	ktap_str_t *chunkname;	/* Chunk name this function was defined in. */
+	BCLine firstline;	/* First line of the function definition. */
+	BCLine numline;	/* Number of lines for the function definition. */
+	void *lineinfo;	/* Compressed map from bytecode ins. to source line. */
+	void *uvinfo;	/* Upvalue names. */
+	void *varinfo;	/* Names and compressed extents of local variables. */
+} ktap_proto_t;
+
+/* Flags for prototype. */
+#define PROTO_CHILD		0x01	/* Has child prototypes. */
+#define PROTO_VARARG		0x02	/* Vararg function. */
+#define PROTO_FFI		0x04	/* Uses BC_KCDATA for FFI datatypes. */
+#define PROTO_NOJIT		0x08	/* JIT disabled for this function. */
+#define PROTO_ILOOP		0x10	/* Patched bytecode with ILOOP etc. */
+/* Only used during parsing. */
+#define PROTO_HAS_RETURN	0x20	/* Already emitted a return. */
+#define PROTO_FIXUP_RETURN	0x40	/* Need to fixup emitted returns. */
+/* Top bits used for counting created closures. */
+#define PROTO_CLCOUNT		0x20	/* Base of saturating 3 bit counter. */
+#define PROTO_CLC_BITS		3
+#define PROTO_CLC_POLY		(3*PROTO_CLCOUNT)  /* Polymorphic threshold. */
+
+#define PROTO_UV_LOCAL		0x8000	/* Upvalue for local slot. */
+#define PROTO_UV_IMMUTABLE	0x4000	/* Immutable upvalue. */
+
+#define proto_kgc(pt, idx)	(((ktap_obj_t *)(pt)->k)[idx])
+#define proto_bc(pt)		((BCIns *)((char *)(pt) + sizeof(ktap_proto_t)))
+#define proto_bcpos(pt, pc)	((BCPos)((pc) - proto_bc(pt)))
+#define proto_uv(pt)		((uint16_t *)(pt)->uv)
+
+#define proto_chunkname(pt)	((pt)->chunkname)
+#define proto_lineinfo(pt)	((const void *)(pt)->lineinfo)
+#define proto_uvinfo(pt)	((const uint8_t *)(pt)->uvinfo)
+#define proto_varinfo(pt)	((const uint8_t *)(pt)->varinfo)
+
+
+typedef struct ktap_node_t {
+	ktap_val_t val; /* Value object. Must be first field. */
+	ktap_val_t key; /* Key object. */
+	struct ktap_node_t *next;  /* hash chain */
+} ktap_node_t;
+
+/* ktap_tab */
+typedef struct ktap_tab {
+	GCHeader;
+#ifdef __KERNEL__
+	arch_spinlock_t lock;
+#endif
+	ktap_val_t *array;    /* Array part. */
+	ktap_node_t *node;    /* Hash part. */
+	ktap_node_t *freetop; /* any free position is before this position */
+
+	uint32_t asize;		/* Size of array part (keys [0, asize-1]). */
+	uint32_t hmask;		/* log2 of size of `node' array */
+
+	uint32_t hnum;		/* number of all nodes */
+
+	ktap_obj_t *gclist;
+} ktap_tab_t;
+
+#ifdef CONFIG_KTAP_FFI
+typedef int csymbol_id;
+typedef uint64_t cdata_number;
+typedef struct csymbol csymbol;
+
+/* global ffi state maintained in each ktap vm instance */
+typedef struct ffi_state {
+	ktap_tab_t *ctable;
+	int csym_nr;
+	csymbol *csym_arr;
+} ffi_state;
+
+/* instance of csymbol */
+typedef struct ktap_cdata {
+	GCHeader;
+	csymbol_id id;
+	union {
+		cdata_number i;
+		struct {
+			void *addr;
+			int nmemb;	/* number of memory block */
+		} p;			/* pointer data */
+		void *rec;		/* struct member or union data */
+	} u;
+} ktap_cdata_t;
+#endif
+
+typedef struct ktap_stats {
+	int mem_allocated;
+	int nr_mem_allocate;
+	int events_hits;
+	int events_missed;
+} ktap_stats_t;
+
+#define KTAP_STATS(ks)	this_cpu_ptr(G(ks)->stats)
+
+
+#define KTAP_RUNNING	0 /* normal running state */
+#define KTAP_TRACE_END	1 /* running in trace_end function */
+#define KTAP_EXIT	2 /* normal exit, set when call exit() */
+#define KTAP_ERROR	3 /* error state, called by kp_error */
+
+typedef struct ktap_global_state {
+	void *mempool;		/* string memory pool */
+	void *mp_freepos;	/* free position in memory pool */
+	int mp_size;		/* memory pool size */
+#ifdef __KERNEL__
+	arch_spinlock_t mp_lock;/* mempool lock */
+#endif
+
+	ktap_str_t **strhash;	/* String hash table (hash chain anchors). */
+	int strmask;		/* String hash mask (size of hash table-1). */
+	int strnum;		/* Number of strings in hash table. */
+#ifdef __KERNEL__
+	arch_spinlock_t str_lock; /* string operation lock */
+#endif
+
+	ktap_val_t registry;
+	ktap_tab_t *gtab;	/* global table contains cfunction and args */
+	ktap_obj_t *allgc; /* list of all collectable objects */
+	ktap_upval_t uvhead; /* head of list of all open upvalues */
+
+	struct ktap_state *mainthread; /*main state */
+	int state; /* status of ktapvm, KTAP_RUNNING, KTAP_TRACE_END, etc */
+#ifdef __KERNEL__
+	/* reserved global percpu data */
+	void __percpu *percpu_state[PERF_NR_CONTEXTS];
+	void __percpu *percpu_print_buffer[PERF_NR_CONTEXTS];
+	void __percpu *percpu_temp_buffer[PERF_NR_CONTEXTS];
+
+	/* for recursion tracing check */
+	int __percpu *recursion_context[PERF_NR_CONTEXTS];
+
+	ktap_option_t *parm; /* ktap options */
+	pid_t trace_pid;
+	struct task_struct *trace_task;
+	cpumask_var_t cpumask;
+	struct ring_buffer *buffer;
+	struct dentry *trace_pipe_dentry;
+	struct task_struct *task;
+	int trace_enabled;
+	int wait_user; /* flag to indicat waiting user consume content */
+
+	struct list_head timers; /* timer list */
+	struct ktap_stats __percpu *stats; /* memory allocation stats */
+	struct list_head events_head; /* probe event list */
+
+	ktap_func_t *trace_end_closure; /* trace_end closure */
+#ifdef CONFIG_KTAP_FFI
+	ffi_state  ffis;
+#endif
+
+	/* C function table for fast call */
+	int nr_builtin_cfunction;
+	ktap_cfunction gfunc_tbl[KP_MAX_CACHED_CFUNCTION];
+#endif
+} ktap_global_state_t;
+
+
+typedef struct ktap_state {
+	ktap_global_state_t *g;	/* global state */
+	int stop;		/* don't enter tracing handler if stop is 1 */
+	StkId top;		/* stack top */
+	StkId func;		/* execute light C function */
+	StkId stack_last;	/* last stack pointer */
+	StkId stack;		/* ktap stack, percpu pre-reserved */
+	ktap_upval_t *openupval;/* opened upvals list */
+
+#ifdef __KERNEL__
+	/* current fired event which allocated on stack */
+	struct ktap_event_data *current_event;
+#endif
+} ktap_state_t;
+
+#define G(ks)   (ks->g)
+
+typedef struct ktap_rawobj {
+	GCHeader;
+	void *v;
+} ktap_rawobj;
+
+/*
+ * Union of all collectable objects
+ */
+union ktap_obj {
+	struct { GCHeader } gch;
+	struct ktap_str ts;
+	struct ktap_func fn;
+	struct ktap_tab h;
+	struct ktap_proto pt;
+	struct ktap_upval uv;
+	struct ktap_state th;  /* thread */
+	struct ktap_rawobj rawobj;
+#ifdef CONFIG_KTAP_FFI
+	struct ktap_cdata cd;
+#endif
+};
+
+#define gch(o)			(&(o)->gch)
+
+/* macros to convert a ktap_obj_t into a specific value */
+#define gco2ts(o)		(&((o)->ts))
+#define gco2uv(o)		(&((o)->uv))
+#define obj2gco(v)		((ktap_obj_t *)(v))
+
+/* predefined values in the registry */
+#define KTAP_RIDX_GLOBALS	1
+#define KTAP_RIDX_LAST		KTAP_RIDX_GLOBALS
+
+/* ktap object types */
+#define KTAP_TNIL		(~0u)
+#define KTAP_TFALSE		(~1u)
+#define KTAP_TTRUE		(~2u)
+#define KTAP_TNUM		(~3u)
+#define KTAP_TLIGHTUD		(~4u)
+#define KTAP_TSTR		(~5u)
+#define KTAP_TUPVAL		(~6u)
+#define KTAP_TTHREAD		(~7u)
+#define KTAP_TPROTO		(~8u)
+#define KTAP_TFUNC		(~9u)
+#define KTAP_TCFUNC		(~10u)
+#define KTAP_TCDATA		(~11u)
+#define KTAP_TTAB		(~12u)
+#define KTAP_TUDATA		(~13u)
+
+/* Specfic types */
+#define KTAP_TEVENTSTR		(~14u) /* argstr */
+#define KTAP_TKSTACK		(~15u) /* stack(), not intern to string yet */
+#define KTAP_TKIP		(~16u) /* kernel function ip addres */
+#define KTAP_TUIP		(~17u) /* userspace function ip addres */
+
+/* This is just the canonical number type used in some places. */
+#define KTAP_TNUMX		(~18u)
+
+
+#define itype(o)		((o)->type)
+#define setitype(o, t)		((o)->type = (t))
+
+#define val_(o)			((o)->val)
+#define gcvalue(o)		(val_(o).gc)
+
+#define nvalue(o)		(val_(o).n)
+#define boolvalue(o)		(KTAP_TFALSE - (o)->type)
+#define hvalue(o)		(&val_(o).gc->h)
+#define phvalue(o)		(&val_(o).gc->ph)
+#define clvalue(o)		(&val_(o).gc->fn)
+#define ptvalue(o)		(&val_(o).gc->pt)
+
+#define getstr(ts)		(const char *)((ts) + 1)
+#define rawtsvalue(o)		(&val_(o).gc->ts)
+#define svalue(o)		getstr(rawtsvalue(o))
+
+#define pvalue(o)		(&val_(o).p)
+#define fvalue(o)		(val_(o).f)
+#ifdef CONFIG_KTAP_FFI
+#define cdvalue(o)		(&val_(o).gc->cd)
+#endif
+
+#define is_nil(o)		(itype(o) == KTAP_TNIL)
+#define is_false(o)		(itype(o) == KTAP_TFALSE)
+#define is_true(o)		(itype(o) == KTAP_TTRUE)
+#define is_bool(o)		(is_false(o) || is_true(o))
+#define is_string(o)		(itype(o) == KTAP_TSTR)
+#define is_number(o)		(itype(o) == KTAP_TNUM)
+#define is_table(o)		(itype(o) == KTAP_TTAB)
+#define is_proto(o)		(itype(o) == KTAP_TPROTO)
+#define is_function(o)		(itype(o) == KTAP_TFUNC)
+#define is_cfunc(o)		(itype(o) == KTAP_TCFUNC)
+#define is_eventstr(o)		(itype(o) == KTAP_TEVENTSTR)
+#define is_kip(o)		(itype(o) == KTAP_TKIP)
+#define is_btrace(o)		(itype(o) == KTAP_TBTRACE)
+#ifdef CONFIG_KTAP_FFI
+#define is_cdata(o)		(itype(o) == KTAP_TCDATA)
+#endif
+
+
+#define set_nil(o)		((o)->type = KTAP_TNIL)
+#define set_bool(o, x)		((o)->type = KTAP_TFALSE-(uint32_t)(x))
+
+static inline void set_number(ktap_val_t *o, ktap_number n)
+{
+	setitype(o, KTAP_TNUM);
+	o->val.n = n;
+}
+
+static inline void set_string(ktap_val_t *o, const ktap_str_t *str)
+{
+	setitype(o, KTAP_TSTR);
+	o->val.gc = (ktap_obj_t *)str;
+}
+
+static inline void set_table(ktap_val_t *o, ktap_tab_t *tab)
+{
+	setitype(o, KTAP_TTAB);
+	o->val.gc = (ktap_obj_t *)tab;
+}
+
+static inline void set_proto(ktap_val_t *o, ktap_proto_t *pt)
+{
+	setitype(o, KTAP_TPROTO);
+	o->val.gc = (ktap_obj_t *)pt;
+}
+
+static inline void set_kstack(ktap_val_t *o, uint16_t depth, uint16_t skip)
+{
+	setitype(o, KTAP_TKSTACK);
+	o->val.stack.depth = depth;
+	o->val.stack.skip = skip;
+}
+
+static inline void set_func(ktap_val_t *o, ktap_func_t *fn)
+{
+	setitype(o, KTAP_TFUNC);
+	o->val.gc = (ktap_obj_t *)fn;
+}
+
+static inline void set_cfunc(ktap_val_t *o, ktap_cfunction fn)
+{
+	setitype(o, KTAP_TCFUNC);
+	o->val.f = fn;
+}
+
+static inline void set_eventstr(ktap_val_t *o)
+{
+	setitype(o, KTAP_TEVENTSTR);
+}
+
+static inline void set_ip(ktap_val_t *o, unsigned long addr)
+{
+	setitype(o, KTAP_TKIP);
+	o->val.n = addr;
+}
+
+
+#ifdef CONFIG_KTAP_FFI
+#define set_cdata(o, x)		{ setitype(o, KTAP_TCDATA); (o)->val.gc = x; }
+#endif
+
+#define set_obj(o1, o2)		{ *(o1) = *(o2); }
+
+#define incr_top(ks)		{ks->top++;}
+
+/*
+ * KTAP_QL describes how error messages quote program elements.
+ * CHANGE it if you want a different appearance.
+ */
+#define KTAP_QL(x)      "'" x "'"
+#define KTAP_QS         KTAP_QL("%s")
+
+#endif /* __KTAP_TYPES_H__ */
+
diff --git a/kernel/trace/Makefile b/kernel/trace/Makefile
index 1378e84..9ff511c 100644
--- a/kernel/trace/Makefile
+++ b/kernel/trace/Makefile
@@ -62,4 +62,6 @@ endif
 obj-$(CONFIG_PROBE_EVENTS) += trace_probe.o
 obj-$(CONFIG_UPROBE_EVENT) += trace_uprobe.o
 
+obj-$(CONFIG_KTAP) += ktap/
+
 libftrace-y := ftrace.o
diff --git a/kernel/trace/ktap/Kconfig b/kernel/trace/ktap/Kconfig
new file mode 100644
index 0000000..6c6cf63
--- /dev/null
+++ b/kernel/trace/ktap/Kconfig
@@ -0,0 +1,32 @@
+config KTAP
+	tristate "a programable dynamic tracing tool for Linux"
+	depends on PERF_EVENTS && EVENT_TRACING
+	default n
+	help
+	  ktap is a new script-based dynamic tracing tool for Linux,
+	  it uses a scripting language and lets users trace the
+	  Linux kernel dynamically. ktap is designed to give
+	  operational insights with interoperability that allow
+	  users to tune, troubleshoot and extend kernel and application.
+	  It's similar with Linux Systemtap and Solaris Dtrace.
+
+	  ktap have different design principles from Linux mainstream
+	  dynamic tracing language in that it's based on bytecode,
+	  so it doesn't depend upon GCC, doesn't require compiling
+	  kernel module for each script, safe to use in production
+	  environment, fulfilling the embedded ecosystem's tracing needs.
+
+	  See ktap tutorial for more information:
+	      http://www.ktap.org/doc/tutorial.html
+
+config KTAP_FFI
+	tristate "FFI support for ktap"
+	depends on KTAP
+	depends on X86_64
+	default n
+	help
+	  This option brings FFI support to ktap. With FFI enabled ktap,
+	  users can call into native kernel C function directly in ktap
+	  script. Except for a new cdef keyword, this option also adds
+	  a ffi module which exports helper functions like ffi.new and
+	  ffi.sizeof.
diff --git a/kernel/trace/ktap/Makefile b/kernel/trace/ktap/Makefile
new file mode 100644
index 0000000..4e57614
--- /dev/null
+++ b/kernel/trace/ktap/Makefile
@@ -0,0 +1,16 @@
+# Do not instrument the tracer itself:
+ifdef CONFIG_FUNCTION_TRACER
+ORIG_CFLAGS := $(KBUILD_CFLAGS)
+KBUILD_CFLAGS = $(subst -pg,,$(ORIG_CFLAGS))
+endif
+
+LIB_OBJS += lib_base.o lib_kdebug.o \
+	    lib_timer.o lib_ansi.o \
+	    lib_table.o lib_net.o
+
+ktapvm-y += ktap.o kp_bcread.o kp_obj.o \
+	    kp_str.o kp_mempool.o \
+	    kp_tab.o kp_vm.o \
+	    kp_transport.o kp_events.o $(LIB_OBJS)
+
+obj-$(CONFIG_KTAP) += ktapvm.o
diff --git a/kernel/trace/ktap/amalg.c b/kernel/trace/ktap/amalg.c
new file mode 100644
index 0000000..ae49af1
--- /dev/null
+++ b/kernel/trace/ktap/amalg.c
@@ -0,0 +1,45 @@
+/*
+ * kp_amalg.c - ktapvm kernel module amalgamation.
+ *
+ * This file is part of ktap by Jovi Zhangwei.
+ *
+ * Copyright (C) 2012-2013 Jovi Zhangwei <jovi.zhangwei@gmail.com>.
+ *
+ * ktap is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * ktap is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+#include "ktap.c"
+#include "kp_obj.c"
+#include "kp_bcread.c"
+#include "kp_str.c"
+#include "kp_mempool.c"
+#include "kp_tab.c"
+#include "kp_transport.c"
+#include "kp_vm.c"
+#include "kp_events.c"
+#include "lib_base.c"
+#include "lib_ansi.c"
+#include "lib_kdebug.c"
+#include "lib_timer.c"
+#include "lib_table.c"
+#include "lib_net.c"
+
+#ifdef CONFIG_KTAP_FFI
+#include "ffi/ffi_call.c"
+#include "ffi/ffi_type.c"
+#include "ffi/ffi_symbol.c"
+#include "ffi/cdata.c"
+#include "ffi/ffi_util.c"
+#include "lib_ffi.c"
+#endif
diff --git a/kernel/trace/ktap/ffi/call_x86_64.S b/kernel/trace/ktap/ffi/call_x86_64.S
new file mode 100644
index 0000000..29b2915
--- /dev/null
+++ b/kernel/trace/ktap/ffi/call_x86_64.S
@@ -0,0 +1,143 @@
+/*
+ * call_x86_64.S - assembly code to call C function and handle return value
+ *
+ * This file is part of ktap by Jovi Zhangwei
+ *
+ * Copyright (C) 2012-2013 Jovi Zhangwei <jovi.zhangwei@gmail.com>.
+ *
+ * ktap is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * ktap is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+
+#ifdef __x86_64
+
+	.file "call_x86_64.S"
+	.text
+
+/*	ffi_call_assem_x86_64(void *stack, void *temp_stack,
+ *		void *rvalue, void *func_addr, ffi_type rftype)
+ *	@stack: base address of register values and new stack
+ *	@temp_stack: stack to store temporary values
+ *	@func_addr: Function address
+ *	@rvalue: where to put return value
+ *	@rftype: FFI type of return value
+ */
+	.align 2
+	.globl	ffi_call_assem_x86_64
+	.type	ffi_call_assem_x86_64,@function
+
+ffi_call_assem_x86_64:
+	movq	(%rsp), %rax	/* save return address */
+	/* move stuffs to temp memory region(void *temp_stack) */
+	movq	%rcx, (%rsi)	/* save pointer to return value */
+	movq	%r8, 8(%rsi)	/* save return_ffi_type */
+	movq	%rbp, 16(%rsi)	/* save %rbp */
+	movq	%rax, 24(%rsi)	/* save return address */
+	movq	%rsp, 32(%rsi)	/* save %rsp */
+	movq	%rsi, %rbp	/* point %rbp to temp memory region */
+
+	movq	%rdx, %r11	/* move function address to %r11 */
+
+	movq	%rdi, %r10	/* set %r10 point to register region */
+	movq	(%r10), %rdi	/* load registers */
+	movq	8(%r10), %rsi
+	movq	16(%r10), %rdx
+	movq	24(%r10), %rcx
+	movq	32(%r10), %r8
+	movq	40(%r10), %r9
+	xorq	%rax, %rax
+
+	leaq	48(%r10), %rsp
+
+	callq	*%r11
+
+	movq	32(%rbp), %rsp	/* restore %rsp */
+	movq	24(%rbp), %rcx	/* restore return address */
+	movq	%rcx, (%rsp)
+
+	movq	(%rbp), %rcx	/* get pointer to return value */
+	movq	8(%rbp), %r8	/* get return_ffi_type */
+	movq	16(%rbp), %rbp	/* restore rbp */
+
+	leaq	.Lreturn_table(%rip), %r11	/* start address of return_table */
+	movslq	(%r11, %r8, 8), %r11	/* fetch target address from table */
+	jmpq	*%r11			/* jump according to value in table */
+
+	.align 8
+.Lreturn_table:
+	.quad	.Lreturn_void		/* FFI_VOID */
+	.quad	.Lreturn_uint8		/* FFI_UINT8 */
+	.quad	.Lreturn_int8		/* FFI_INT8 */
+	.quad	.Lreturn_uint16		/* FFI_UINT16 */
+	.quad	.Lreturn_int16		/* FFI_INT16 */
+	.quad	.Lreturn_uint32		/* FFI_UINT32 */
+	.quad	.Lreturn_int32		/* FFI_INT32 */
+	.quad	.Lreturn_uint64		/* FFI_UINT64 */
+	.quad	.Lreturn_int64		/* FFI_INT64 */
+	.quad	.Lreturn_ptr		/* FFI_PTR */
+	.quad	.Lreturn_func		/* FFI_FUNC */
+	.quad	.Lreturn_struct		/* FFI_STRUCT */
+	.quad	.Lreturn_unknown	/* FFI_UNKNOWN */
+
+	.align 8
+.Lreturn_void:
+.Lreturn_func:
+.Lreturn_unknown:
+	retq
+	.align 8
+.Lreturn_uint8:
+	movzbq	%al, %rax
+	movq	%rax, (%rcx)
+	retq
+	.align 8
+.Lreturn_int8:
+	movsbq	%al, %rax
+	movq	%rax, (%rcx)
+	retq
+	.align 8
+.Lreturn_uint16:
+	movzwq	%ax, %rax
+	movq	%rax, (%rcx)
+	retq
+	.align 8
+.Lreturn_int16:
+	movswq	%ax, %rax
+	movq	%rax, (%rcx)
+	retq
+	.align 8
+.Lreturn_uint32:
+	movl	%eax, %eax
+	movq	%rax, (%rcx)
+	retq
+	.align 8
+.Lreturn_int32:
+	movslq	%eax, %rax
+	movq	%rax, (%rcx)
+	retq
+	.align 8
+.Lreturn_uint64:
+.Lreturn_int64:
+.Lreturn_ptr:
+	movq	%rax, (%rcx)
+	retq
+/* Struct type indicates that struct is put into at most two registers,
+ * and 16 bytes space is always available
+ */
+	.align 8
+.Lreturn_struct:
+	movq	%rax, (%rcx)
+	movq	%rdx, 8(%rcx)
+	retq
+
+#endif /* end for __x86_64 */
diff --git a/kernel/trace/ktap/ffi/cdata.c b/kernel/trace/ktap/ffi/cdata.c
new file mode 100644
index 0000000..f355458
--- /dev/null
+++ b/kernel/trace/ktap/ffi/cdata.c
@@ -0,0 +1,433 @@
+/*
+ * cdata.c - support functions for ktap_cdata_t
+ *
+ * This file is part of ktap by Jovi Zhangwei
+ *
+ * Copyright (C) 2012-2013 Jovi Zhangwei <jovi.zhangwei@gmail.com>.
+ *
+ * ktap is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * ktap is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+
+#include "../../include/ktap_types.h"
+#include "../../include/ktap_ffi.h"
+#include "../kp_obj.h"
+
+ktap_cdata_t *kp_cdata_new(ktap_state_t *ks, csymbol_id id)
+{
+	ktap_cdata_t *cd;
+
+	cd = &kp_obj_newobject(ks, KTAP_TYPE_CDATA, sizeof(ktap_cdata_t), NULL)->cd;
+	cd_set_csym_id(cd, id);
+
+	return cd;
+}
+
+ktap_cdata_t *kp_cdata_new_number(ktap_state_t *ks, void *val, csymbol_id id)
+{
+	ktap_cdata_t *cd;
+
+	cd = kp_cdata_new(ks, id);
+	cd_int(cd) = (cdata_number)val;
+
+	return cd;
+}
+
+/* argument nmemb here indicates the length of array that is pointed to,
+ * -1 for unknown */
+ktap_cdata_t *kp_cdata_new_ptr(ktap_state_t *ks, void *addr,
+			     int nmemb, csymbol_id id, int to_allocate)
+{
+	ktap_cdata_t *cd;
+	size_t memb_size;
+	csymbol_id deref_id;
+
+	cd = kp_cdata_new(ks, id);
+
+	if (to_allocate) {
+		/* allocate new empty space */
+		deref_id = csym_ptr_deref_id(id_to_csym(ks, id));
+		memb_size = csym_size(ks, id_to_csym(ks, deref_id));
+		cd_ptr(cd) = kp_rawobj_alloc(ks, memb_size * nmemb);
+	} else {
+		cd_ptr(cd) = addr;
+	}
+
+	cd_ptr_nmemb(cd) = nmemb;
+
+	return cd;
+}
+
+ktap_cdata_t *kp_cdata_new_record(ktap_state_t *ks, void *val, csymbol_id id)
+{
+	ktap_cdata_t *cd;
+	size_t size;
+
+	cd = kp_cdata_new(ks, id);
+
+	/* if val == NULL, allocate new empty space */
+	if (val == NULL) {
+		size = csym_size(ks, id_to_csym(ks, id));
+		cd_record(cd) = kp_rawobj_alloc(ks, size);
+	} else
+		cd_record(cd) = val;
+
+	return cd;
+}
+
+ktap_cdata_t *kp_cdata_new_by_id(ktap_state_t *ks, void *val, csymbol_id id)
+{
+	csymbol *cs = id_to_csym(ks, id);
+
+	switch (csym_type(cs)) {
+	case FFI_VOID:
+		kp_error(ks, "Error: Cannot new a void type\n");
+		return NULL;
+	case FFI_UINT8:
+	case FFI_INT8:
+	case FFI_UINT16:
+	case FFI_INT16:
+	case FFI_UINT32:
+	case FFI_INT32:
+	case FFI_UINT64:
+	case FFI_INT64:
+		return kp_cdata_new_number(ks, val, id);
+	case FFI_PTR:
+		return kp_cdata_new_ptr(ks, val, 0, id, 0);
+	case FFI_STRUCT:
+	case FFI_UNION:
+		return kp_cdata_new_record(ks, val, id);
+	case FFI_FUNC:
+		kp_error(ks, "Error: Cannot new a function type\n");
+		return NULL;
+	case FFI_UNKNOWN:
+	default:
+		kp_error(ks, "Error: unknown csymbol type %s\n", csym_name(cs));
+		return NULL;
+	}
+}
+
+void kp_cdata_dump(ktap_state_t *ks, ktap_cdata_t *cd)
+{
+	switch (cd_type(ks, cd)) {
+	case FFI_UINT8:	case FFI_INT8:
+		kp_printf(ks, "c int(0x%01x)", cd_int(cd));
+		break;
+	case FFI_UINT16:	case FFI_INT16:
+		kp_printf(ks, "c int(0x%02x)", cd_int(cd));
+		break;
+	case FFI_UINT32:	case FFI_INT32:
+		kp_printf(ks, "c int(0x%04x)", cd_int(cd));
+		break;
+	case FFI_UINT64:	case FFI_INT64:
+		kp_printf(ks, "c int(0x%08x)", cd_int(cd));
+		break;
+	case FFI_PTR:
+		kp_printf(ks, "c pointer(0x%p)", cd_ptr(cd));
+		break;
+	case FFI_STRUCT:
+		kp_printf(ks, "c struct(0x%p)", cd_struct(cd));
+		break;
+	case FFI_UNION:
+		kp_printf(ks, "c union(0x%p)", cd_union(cd));
+		break;
+	default:
+		kp_printf(ks, "unsupported cdata type %d!\n", cd_type(ks, cd));
+	}
+}
+
+/* Notice: Even if the types are matched, there may exist the lost of
+ * data in the unpack process due to precision */
+int kp_cdata_type_match(ktap_state_t *ks, csymbol *cs, ktap_val_t *val)
+{
+	ffi_type type;
+
+	type = csym_type(cs);
+	if (type == FFI_FUNC)
+		goto error;
+
+	switch (ttypenv(val)) {
+	case KTAP_TYPE_LIGHTUSERDATA:
+		if (type != FFI_PTR) goto error;
+		break;
+	case KTAP_TYPE_BOOLEAN:
+	case KTAP_TYPE_NUMBER:
+		if (type != FFI_UINT8 && type != FFI_INT8
+		&& type != FFI_UINT16 && type != FFI_INT16
+		&& type != FFI_UINT32 && type != FFI_INT32
+		&& type != FFI_UINT64 && type != FFI_INT64)
+			goto error;
+		break;
+	case KTAP_TYPE_STRING:
+		if (type != FFI_PTR && type != FFI_UINT8 && type != FFI_INT8)
+			goto error;
+		break;
+	case KTAP_TYPE_CDATA:
+		if (cs != cd_csym(ks, cdvalue(val)))
+			goto error;
+		break;
+	default:
+		goto error;
+	}
+	return 0;
+
+error:
+	return -1;
+}
+
+static void kp_cdata_value(ktap_state_t *ks, ktap_val_t *val, void **out_addr,
+			   size_t *out_size, void **temp)
+{
+	ktap_cdata_t *cd;
+	csymbol *cs;
+	ffi_type type;
+
+	switch (ttypenv(val)) {
+	case KTAP_TYPE_BOOLEAN:
+		*out_addr = &bvalue(val);
+		*out_size = sizeof(int);
+		return;
+	case KTAP_TYPE_LIGHTUSERDATA:
+		*out_addr = pvalue(val);
+		*out_size = sizeof(void *);
+		return;
+	case KTAP_TYPE_NUMBER:
+		*out_addr = &nvalue(val);
+		*out_size = sizeof(ktap_number);
+		return;
+	case KTAP_TYPE_STRING:
+		*temp = (void *)svalue(val);
+		*out_addr = temp;
+		*out_size = sizeof(void *);
+		return;
+	}
+
+	cd = cdvalue(val);
+	cs = cd_csym(ks, cd);
+	type = csym_type(cs);
+	*out_size = csym_size(ks, cs);
+	switch (type) {
+	case FFI_VOID:
+		kp_error(ks, "Error: Cannot copy data from void type\n");
+		return;
+	case FFI_UINT8:
+	case FFI_INT8:
+	case FFI_UINT16:
+	case FFI_INT16:
+	case FFI_UINT32:
+	case FFI_INT32:
+	case FFI_UINT64:
+	case FFI_INT64:
+		*out_addr = &cd_int(cd);
+		return;
+	case FFI_PTR:
+		*out_addr = &cd_ptr(cd);
+		return;
+	case FFI_STRUCT:
+	case FFI_UNION:
+		*out_addr = cd_record(cd);
+		return;
+	case FFI_FUNC:
+	case FFI_UNKNOWN:
+		kp_error(ks, "Error: internal error for csymbol %s\n",
+				csym_name(cs));
+		return;
+	}
+}
+
+/* Check whether or not type is matched before unpacking */
+void kp_cdata_unpack(ktap_state_t *ks, char *dst, csymbol *cs, ktap_val_t *val)
+{
+	size_t size = csym_size(ks, cs), val_size;
+	void *val_addr, *temp;
+
+	kp_cdata_value(ks, val, &val_addr, &val_size, &temp);
+	if (val_size > size)
+		val_size = size;
+	memmove(dst, val_addr, val_size);
+	memset(dst + val_size, 0, size - val_size);
+}
+
+/* Check whether or not type is matched before packing */
+void kp_cdata_pack(ktap_state_t *ks, ktap_val_t *val, char *src, csymbol *cs)
+{
+	size_t size = csym_size(ks, cs), val_size;
+	void *val_addr, *temp;
+
+	kp_cdata_value(ks, val, &val_addr, &val_size, &temp);
+	if (size > val_size)
+		size = val_size;
+	memmove(val_addr, src, size);
+	memset(val_addr + size, 0, val_size - size);
+}
+
+/* Init its cdata type, but not its actual value */
+static void kp_cdata_init(ktap_state_t *ks, ktap_val_t *val, void *addr, int len,
+			  csymbol_id id)
+{
+	ffi_type type = csym_type(id_to_csym(ks, id));
+
+	switch (type) {
+	case FFI_PTR:
+		set_cdata(val, kp_cdata_new_ptr(ks, addr, len, id, 0));
+		break;
+	case FFI_STRUCT:
+	case FFI_UNION:
+		set_cdata(val, kp_cdata_new_record(ks, addr, id));
+		break;
+	case FFI_UINT8:
+	case FFI_INT8:
+	case FFI_UINT16:
+	case FFI_INT16:
+	case FFI_UINT32:
+	case FFI_INT32:
+	case FFI_UINT64:
+	case FFI_INT64:
+		/* set all these value into ktap_number(long) */
+		set_number(val, 0);
+		break;
+	default:
+		set_cdata(val, kp_cdata_new(ks, id));
+		break;
+	}
+}
+
+void kp_cdata_ptr_set(ktap_state_t *ks, ktap_cdata_t *cd,
+		      ktap_val_t *key, ktap_val_t *val)
+{
+	ktap_number idx;
+	csymbol *cs;
+	size_t size;
+	char *addr;
+
+	if (!is_number(key)) {
+		kp_error(ks, "array index should be number\n");
+		return;
+	}
+	idx = nvalue(key);
+	if (unlikely(idx < 0 || (cd_ptr_nmemb(cd) >= 0
+					&& idx >= cd_ptr_nmemb(cd)))) {
+		kp_error(ks, "array index out of bound\n");
+		return;
+	}
+
+	cs = csym_ptr_deref(ks, cd_csym(ks, cd));
+	if (kp_cdata_type_match(ks, cs, val)) {
+		kp_error(ks, "array member should be %s type\n", csym_name(cs));
+		return;
+	}
+	size = csym_size(ks, cs);
+	addr = cd_ptr(cd);
+	addr += size * idx;
+	kp_cdata_unpack(ks, addr, cs, val);
+}
+
+void kp_cdata_ptr_get(ktap_state_t *ks, ktap_cdata_t *cd,
+		      ktap_val_t *key, ktap_val_t *val)
+{
+	ktap_number idx;
+	csymbol *cs;
+	size_t size;
+	char *addr;
+	csymbol_id cs_id;
+
+	if (!is_number(key)) {
+		kp_error(ks, "array index should be number\n");
+		return;
+	}
+	idx = nvalue(key);
+	if (unlikely(idx < 0 || (cd_ptr_nmemb(cd) >= 0
+					&& idx >= cd_ptr_nmemb(cd)))) {
+		kp_error(ks, "array index out of bound\n");
+		return;
+	}
+
+	cs_id = csym_ptr_deref_id(cd_csym(ks, cd));
+	cs = id_to_csym(ks, cs_id);
+	size = csym_size(ks, cs);
+	addr = cd_ptr(cd);
+	addr += size * idx;
+
+	kp_cdata_init(ks, val, addr, -1, cs_id);
+	kp_cdata_pack(ks, val, addr, cs);
+}
+
+void kp_cdata_record_set(ktap_state_t *ks, ktap_cdata_t *cd,
+			 ktap_val_t *key, ktap_val_t *val)
+{
+	const char *mb_name;
+	csymbol *cs, *mb_cs;
+	csymbol_struct *csst;
+	struct_member *mb;
+	char *addr;
+
+	if (!is_shrstring(key)) {
+		kp_error(ks, "struct member name should be string\n");
+		return;
+	}
+	mb_name = svalue(key);
+	cs = cd_csym(ks, cd);
+	csst = csym_struct(cs);
+	mb = csymst_mb_by_name(ks, csst, mb_name);
+	if (mb == NULL) {
+		kp_error(ks, "struct member %s doesn't exist\n", mb_name);
+		return;
+	}
+
+	mb_cs = id_to_csym(ks, mb->id);
+	if (kp_cdata_type_match(ks, mb_cs, val)) {
+		kp_error(ks, "struct member should be %s type\n",
+			     csym_name(mb_cs));
+		return;
+	}
+
+	addr = cd_record(cd);
+	addr += csym_record_mb_offset_by_name(ks, cs, mb_name);
+	kp_cdata_unpack(ks, addr, mb_cs, val);
+}
+
+void kp_cdata_record_get(ktap_state_t *ks, ktap_cdata_t *cd,
+			 ktap_val_t *key, ktap_val_t *val)
+{
+	const char *mb_name;
+	csymbol *cs, *mb_cs;
+	csymbol_struct *csst;
+	struct_member *mb;
+	char *addr;
+	csymbol_id mb_cs_id;
+
+	if (!is_shrstring(key)) {
+		kp_error(ks, "struct member name should be string\n");
+		return;
+	}
+
+	mb_name = svalue(key);
+	cs = cd_csym(ks, cd);
+	csst = csym_struct(cs);
+	mb = csymst_mb_by_name(ks, csst, mb_name);
+	if (mb == NULL) {
+		kp_error(ks, "struct member %s doesn't exist\n", mb_name);
+		return;
+	}
+
+	mb_cs_id = mb->id;
+	mb_cs = id_to_csym(ks, mb_cs_id);
+	addr = cd_record(cd);
+	addr += csym_record_mb_offset_by_name(ks, cs, mb_name);
+
+	kp_cdata_init(ks, val, addr, mb->len, mb_cs_id);
+	if (mb->len < 0)
+		kp_cdata_pack(ks, val, addr, mb_cs);
+}
+
diff --git a/kernel/trace/ktap/ffi/ffi_call.c b/kernel/trace/ktap/ffi/ffi_call.c
new file mode 100644
index 0000000..7a43f2b
--- /dev/null
+++ b/kernel/trace/ktap/ffi/ffi_call.c
@@ -0,0 +1,355 @@
+/*
+ * ffi_call.c - foreign function calling library support for ktap
+ *
+ * This file is part of ktap by Jovi Zhangwei.
+ *
+ * Copyright (C) 2012-2013 Jovi Zhangwei <jovi.zhangwei@gmail.com>.
+ *
+ * ktap is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * ktap is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+#include <linux/ctype.h>
+#include <linux/slab.h>
+#include "../../include/ktap_types.h"
+#include "../../include/ktap_ffi.h"
+#include "../ktap.h"
+#include "../kp_vm.h"
+#include "../kp_obj.h"
+
+static int ffi_type_check(ktap_state_t *ks, csymbol_func *csf, int idx)
+{
+	StkId arg;
+	csymbol *cs;
+
+	if (idx >= csymf_arg_nr(csf))
+		return 0;
+	arg = kp_arg(ks, idx + 1);
+	cs = csymf_arg(ks, csf, idx);
+
+	if (!kp_cdata_type_match(ks, cs, arg))
+		return 0;
+	else {
+		kp_error(ks, "Cannot convert to csymbol %s for arg %d\n",
+				csym_name(cs), idx);
+		return -1;
+	}
+}
+
+static csymbol *ffi_get_arg_csym(ktap_state_t *ks, csymbol_func *csf, int idx)
+{
+	StkId arg;
+	csymbol *cs;
+
+	if (idx < csymf_arg_nr(csf))
+		return csymf_arg(ks, csf, idx);
+
+	arg = kp_arg(ks, idx + 1);
+	cs = id_to_csym(ks, ffi_get_csym_id(ks, "void *"));
+	switch (ttypenv(arg)) {
+	case KTAP_TYPE_LIGHTUSERDATA:
+	case KTAP_TYPE_BOOLEAN:
+	case KTAP_TYPE_NUMBER:
+	case KTAP_TYPE_STRING:
+		return cs;
+	case KTAP_TYPE_CDATA:
+		return cd_csym(ks, cdvalue(arg));
+	default:
+		kp_error(ks, "Error: Cannot get type for arg %d\n", idx);
+		return cs;
+	}
+}
+
+static void ffi_unpack(ktap_state_t *ks, char *dst, csymbol_func *csf,
+		int idx, int align)
+{
+	StkId arg = kp_arg(ks, idx + 1);
+	csymbol *cs = ffi_get_arg_csym(ks, csf, idx);
+	size_t size = csym_size(ks, cs);
+
+	/* initialize the destination section */
+	memset(dst, 0, ALIGN(size, align));
+
+	kp_cdata_unpack(ks, dst, cs, arg);
+}
+
+#ifdef __x86_64
+
+enum arg_status {
+	IN_REGISTER,
+	IN_MEMORY,
+	IN_STACK,
+};
+
+#define ALIGN_STACK(v, a) ((void *)(ALIGN(((uint64_t)v), a)))
+#define STACK_ALIGNMENT 8
+#define REDZONE_SIZE 128
+#define GPR_SIZE (sizeof(void *))
+#define MAX_GPR 6
+#define MAX_GPR_SIZE (MAX_GPR * GPR_SIZE)
+#define NEWSTACK_SIZE 512
+
+#define ffi_call_arch(ks, cf, rvalue) ffi_call_x86_64(ks, cf, rvalue)
+
+extern void ffi_call_assem_x86_64(void *stack, void *temp_stack,
+					void *func_addr, void *rvalue, ffi_type rtype);
+
+static void ffi_call_x86_64(ktap_state_t *ks, csymbol_func *csf, void *rvalue)
+{
+	int i;
+	int gpr_nr;
+	int arg_bytes; /* total bytes needed for exceeded args in stack */
+	int mem_bytes; /* total bytes needed for memory storage */
+	char *stack, *stack_p, *gpr_p, *arg_p, *mem_p, *tmp_p;
+	int arg_nr;
+	csymbol *rsym;
+	ffi_type rtype;
+	size_t rsize;
+	bool ret_in_memory;
+	/* New stack to call C function */
+	char space[NEWSTACK_SIZE];
+
+	arg_nr = kp_arg_nr(ks);
+	rsym = csymf_ret(ks, csf);
+	rtype = csym_type(rsym);
+	rsize = csym_size(ks, rsym);
+	ret_in_memory = false;
+	if (rtype == FFI_STRUCT || rtype == FFI_UNION) {
+		if (rsize > 16) {
+			rvalue = kp_malloc(ks, rsize);
+			rtype = FFI_VOID;
+			ret_in_memory = true;
+		} else {
+			/* much easier to always copy 16 bytes from registers */
+			rvalue = kp_malloc(ks, 16);
+		}
+	}
+
+	gpr_nr = 0;
+	arg_bytes = mem_bytes = 0;
+	if (ret_in_memory)
+		gpr_nr++;
+	/* calculate bytes needed for stack */
+	for (i = 0; i < arg_nr; i++) {
+		csymbol *cs = ffi_get_arg_csym(ks, csf, i);
+		size_t size = csym_size(ks, cs);
+		size_t align = csym_align(ks, cs);
+		enum arg_status st = IN_REGISTER;
+		int n_gpr_nr = 0;
+		if (size > 32) {
+			st = IN_MEMORY;
+			n_gpr_nr = 1;
+		} else if (size > 16)
+			st = IN_STACK;
+		else
+			n_gpr_nr = ALIGN(size, GPR_SIZE) / GPR_SIZE;
+
+		if (gpr_nr + n_gpr_nr > MAX_GPR) {
+			if (st == IN_MEMORY)
+				arg_bytes += GPR_SIZE;
+			else
+				st = IN_STACK;
+		} else
+			gpr_nr += n_gpr_nr;
+		if (st == IN_STACK) {
+			arg_bytes = ALIGN(arg_bytes, align);
+			arg_bytes += size;
+			arg_bytes = ALIGN(arg_bytes, STACK_ALIGNMENT);
+		}
+		if (st == IN_MEMORY) {
+			mem_bytes = ALIGN(mem_bytes, align);
+			mem_bytes += size;
+			mem_bytes = ALIGN(mem_bytes, STACK_ALIGNMENT);
+		}
+	}
+
+	/* apply space to fake stack for C function call */
+	if (16 + REDZONE_SIZE + MAX_GPR_SIZE + arg_bytes +
+			mem_bytes + 6 * 8 >= NEWSTACK_SIZE) {
+		kp_error(ks, "Unable to handle that many arguments by now\n");
+		return;
+	}
+	stack = space;
+	/* 128 bytes below %rsp is red zone */
+	/* stack should be 16-bytes aligned */
+	stack_p = ALIGN_STACK(stack + REDZONE_SIZE, 16);
+	/* save general purpose registers here */
+	gpr_p = stack_p;
+	memset(gpr_p, 0, MAX_GPR_SIZE);
+	/* save arguments in stack here */
+	arg_p = gpr_p + MAX_GPR_SIZE;
+	/* save arguments in memory here */
+	mem_p = arg_p + arg_bytes;
+	/* set additional space as temporary space */
+	tmp_p = mem_p + mem_bytes;
+
+	/* copy arguments here */
+	gpr_nr = 0;
+	if (ret_in_memory) {
+		memcpy(gpr_p, &rvalue, GPR_SIZE);
+		gpr_p += GPR_SIZE;
+		gpr_nr++;
+	}
+	for (i = 0; i < arg_nr; i++) {
+		csymbol *cs = ffi_get_arg_csym(ks, csf, i);
+		size_t size = csym_size(ks, cs);
+		size_t align = csym_align(ks, cs);
+		enum arg_status st = IN_REGISTER;
+		int n_gpr_nr = 0;
+		if (size > 32) {
+			st = IN_MEMORY;
+			n_gpr_nr = 1;
+		} else if (size > 16)
+			st = IN_STACK;
+		else
+			n_gpr_nr = ALIGN(size, GPR_SIZE) / GPR_SIZE;
+
+		if (st == IN_MEMORY)
+			mem_p = ALIGN_STACK(mem_p, align);
+		/* Tricky way about storing it above mem_p. It won't overflow
+		 * because temp region can be temporarily used if necesseary. */
+		ffi_unpack(ks, mem_p, csf, i, GPR_SIZE);
+		if (gpr_nr + n_gpr_nr > MAX_GPR) {
+			if (st == IN_MEMORY) {
+				memcpy(arg_p, &mem_p, GPR_SIZE);
+				arg_p += GPR_SIZE;
+			} else
+				st = IN_STACK;
+		} else {
+			memcpy(gpr_p, mem_p, n_gpr_nr * GPR_SIZE);
+			gpr_p += n_gpr_nr * GPR_SIZE;
+			gpr_nr += n_gpr_nr;
+		}
+		if (st == IN_STACK) {
+			arg_p = ALIGN_STACK(arg_p, align);
+			memcpy(arg_p, mem_p, size);
+			arg_p += size;
+			arg_p = ALIGN_STACK(arg_p, STACK_ALIGNMENT);
+		}
+		if (st == IN_MEMORY) {
+			mem_p += size;
+			mem_p = ALIGN_STACK(mem_p, STACK_ALIGNMENT);
+		}
+	}
+
+	kp_verbose_printf(ks, "Stack location: %p -redzone- %p -general purpose "
+			"register used- %p -zero- %p -stack for argument- %p"
+			" -memory for argument- %p -temp stack-\n",
+			stack, stack_p, gpr_p, stack_p + MAX_GPR_SIZE,
+			arg_p, mem_p);
+	kp_verbose_printf(ks, "GPR number: %d; arg in stack: %d; "
+			"arg in mem: %d\n",
+			gpr_nr, arg_bytes, mem_bytes);
+	kp_verbose_printf(ks, "Return: address %p type %d\n", rvalue, rtype);
+	kp_verbose_printf(ks, "Number of register used: %d\n", gpr_nr);
+	kp_verbose_printf(ks, "Start FFI call on %p\n", csf->addr);
+	ffi_call_assem_x86_64(stack_p, tmp_p, csf->addr, rvalue, rtype);
+}
+
+#else /* non-supported platform */
+
+#define ffi_call(ks, cf, rvalue) ffi_call_unsupported(ks, cf, rvalue)
+
+static void ffi_call_unsupported(ktap_state_t *ks,
+		csymbol_func *csf, void *rvalue)
+{
+	kp_error(ks, "unsupported architecture.\n");
+}
+
+#endif /* end for platform-specific setting */
+
+
+static int ffi_set_return(ktap_state_t *ks, void *rvalue, csymbol_id ret_id)
+{
+	ktap_cdata_t *cd;
+	ffi_type type = csym_type(id_to_csym(ks, ret_id));
+
+	/* push return value to ktap stack */
+	switch (type) {
+	case FFI_VOID:
+		return 0;
+	case FFI_UINT8:
+	case FFI_INT8:
+	case FFI_UINT16:
+	case FFI_INT16:
+	case FFI_UINT32:
+	case FFI_INT32:
+	case FFI_UINT64:
+	case FFI_INT64:
+		set_number(ks->top, (ktap_number)rvalue);
+		break;
+	case FFI_PTR:
+		cd = kp_cdata_new_ptr(ks, rvalue, -1, ret_id, 0);
+		set_cdata(ks->top, cd);
+		break;
+	case FFI_STRUCT:
+	case FFI_UNION:
+		cd = kp_cdata_new_record(ks, rvalue, ret_id);
+		set_cdata(ks->top, cd);
+		break;
+	case FFI_FUNC:
+	case FFI_UNKNOWN:
+		kp_error(ks, "Error: Have not support ffi_type %s\n",
+				ffi_type_name(type));
+		return 0;
+	}
+	incr_top(ks);
+	return 1;
+}
+
+/*
+ * Call C into function
+ * First argument should be function symbol address, argument types
+ * and return type.
+ * Left arguments should be arguments for calling the C function.
+ * Types between Ktap and C are converted automatically.
+ * Only support x86_64 function call by now
+ */
+int ffi_call(ktap_state_t *ks, csymbol_func *csf)
+{
+	int i;
+	int expected_arg_nr, arg_nr;
+	ktap_closure_t *cl;
+	void *rvalue;
+
+	expected_arg_nr = csymf_arg_nr(csf);
+	arg_nr = kp_arg_nr(ks);
+
+	/* check stack status for C call */
+	if (!csf->has_var_arg && expected_arg_nr != arg_nr) {
+		kp_error(ks, "wrong argument number %d, which should be %d\n",
+				arg_nr, expected_arg_nr);
+		goto out;
+	}
+	if (csf->has_var_arg && expected_arg_nr > arg_nr) {
+		kp_error(ks, "argument number %d, which should be bigger than %d\n",
+				arg_nr, expected_arg_nr);
+		goto out;
+	}
+
+	/* maybe useful later, leave it here first */
+	cl = clvalue(kp_arg(ks, arg_nr + 1));
+
+	/* check the argument types */
+	for (i = 0; i < arg_nr; i++) {
+		if (ffi_type_check(ks, csf, i) < 0)
+			goto out;
+	}
+
+	/* platform-specific calling workflow */
+	ffi_call_arch(ks, csf, &rvalue);
+	kp_verbose_printf(ks, "Finish FFI call\n");
+
+out:
+	return ffi_set_return(ks, rvalue, csymf_ret_id(csf));
+}
diff --git a/kernel/trace/ktap/ffi/ffi_symbol.c b/kernel/trace/ktap/ffi/ffi_symbol.c
new file mode 100644
index 0000000..e851a0f
--- /dev/null
+++ b/kernel/trace/ktap/ffi/ffi_symbol.c
@@ -0,0 +1,182 @@
+/*
+ * ffi_symbol.c - ktapvm kernel module ffi symbol submodule
+ *
+ * This file is part of ktap by Jovi Zhangwei.
+ *
+ * Copyright (C) 2012-2013 Jovi Zhangwei <jovi.zhangwei@gmail.com>.
+ *
+ * ktap is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * ktap is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+
+#include "../../include/ktap_types.h"
+#include "../../include/ktap_ffi.h"
+#include "../ktap.h"
+#include "../kp_vm.h"
+#include "../kp_obj.h"
+#include "../kp_str.h"
+#include "../kp_tab.h"
+
+static inline csymbol *get_csym_arr(ktap_state_t *ks)
+{
+	return G(ks)->ffis.csym_arr;
+}
+
+static inline int get_csym_nr(ktap_state_t *ks)
+{
+	return G(ks)->ffis.csym_nr;
+}
+
+static inline void set_csym_arr(ktap_state_t *ks, csymbol *csym)
+{
+	G(ks)->ffis.csym_arr = csym;
+}
+
+static inline void set_csym_nr(ktap_state_t *ks, int nr)
+{
+	G(ks)->ffis.csym_nr = nr;
+}
+
+
+static inline ktap_tab_t *get_ffi_ctable(ktap_state_t *ks)
+{
+	return G(ks)->ffis.ctable;
+}
+
+static int setup_ffi_ctable(ktap_state_t *ks)
+{
+	ktap_val_t ffi_name, ffi_lib_name, ffi_mt;
+	const ktap_val_t *gt, *ffit;
+	ktap_tab_t *registry;
+
+	gt = kp_tab_getint(hvalue(&G(ks)->registry), KTAP_RIDX_GLOBALS);
+
+	G(ks)->ffis.ctable = kp_tab_new(ks, 0, 512);
+	if (!G(ks)->ffis.ctable)
+		return -1;
+
+	/* get global["ffi"] */
+	set_string(&ffi_name, kp_str_new(ks, "ffi"));
+	registry = hvalue(gt);
+	ffit = kp_tab_get(ks, registry, &ffi_name);
+	/* insert ffi C table to ffi table */
+	set_table(&ffi_mt, get_ffi_ctable(ks));
+	set_string(&ffi_lib_name, kp_str_new(ks, "C"));
+	registry = hvalue(ffit);
+	kp_tab_setvalue(ks, registry, &ffi_lib_name, &ffi_mt);
+
+	return 0;
+}
+
+inline csymbol *ffi_get_csym_by_id(ktap_state_t *ks, int id)
+{
+	return &(get_csym_arr(ks)[id]);
+}
+
+csymbol_id ffi_get_csym_id(ktap_state_t *ks, char *name)
+{
+	int i;
+
+	for (i = 0; i < get_csym_nr(ks); i++) {
+		if (!strcmp(name, csym_name(ffi_get_csym_by_id(ks, i)))) {
+			return i;
+		}
+	}
+
+	kp_error(ks, "Cannot find csymbol with name %s\n", name);
+	return 0;
+}
+
+static void add_ffi_func_to_ctable(ktap_state_t *ks, csymbol_id id)
+{
+	ktap_val_t func_name, fv;
+	ktap_cdata_t *cd;
+	csymbol *cs;
+
+	/* push cdata to ctable */
+	set_cdata(&fv, kp_obj_newobject(ks, KTAP_TYPE_CDATA, sizeof(ktap_cdata_t),
+					NULL));
+	cd = cdvalue(&fv);
+	cd_set_csym_id(cd, id);
+
+	cs = id_to_csym(ks, id);
+	set_string(&func_name, kp_str_new(ks, csym_name(cs)));
+	kp_tab_setvalue(ks, get_ffi_ctable(ks), &func_name, &fv);
+}
+
+static int setup_ffi_symbol_table(ktap_state_t *ks)
+{
+	int i;
+	csymbol *cs;
+
+	if (setup_ffi_ctable(ks))
+		return -1;
+
+	/* push all functions to ctable */
+	for (i = 0; i < get_csym_nr(ks); i++) {
+		cs = &get_csym_arr(ks)[i];
+		switch (cs->type) {
+		case FFI_FUNC:
+			kp_verbose_printf(ks, "[%d] loading C function %s\n",
+					i, csym_name(cs));
+			add_ffi_func_to_ctable(ks, i);
+			kp_verbose_printf(ks, "%s loaded\n", csym_name(cs));
+			break;
+		case FFI_STRUCT:
+		case FFI_UNION:
+			break;
+		default:
+			break;
+		}
+	}
+
+	return 0;
+}
+
+void ffi_free_symbols(ktap_state_t *ks)
+{
+	int i;
+	csymbol_id *arg_ids;
+	csymbol *cs;
+
+	if (!get_csym_arr(ks))
+		return;
+
+	for (i = 0; i < get_csym_nr(ks); i++) {
+		cs = &get_csym_arr(ks)[i];
+		switch (csym_type(cs)) {
+		case FFI_FUNC:
+			arg_ids = csym_func_arg_ids(cs);
+			if (arg_ids)
+				kp_free(ks, arg_ids);
+			break;
+		case FFI_STRUCT:
+		case FFI_UNION:
+			/*@TODO finish this  20.11 2013 (houqp)*/
+			break;
+		default:
+			break;
+		}
+	}
+
+	kp_free(ks, get_csym_arr(ks));
+}
+
+int ffi_set_csym_arr(ktap_state_t *ks, int cs_nr, csymbol *new_arr)
+{
+	set_csym_nr(ks, cs_nr);
+	set_csym_arr(ks, new_arr);
+	return setup_ffi_symbol_table(ks);
+}
+
diff --git a/kernel/trace/ktap/ffi/ffi_type.c b/kernel/trace/ktap/ffi/ffi_type.c
new file mode 100644
index 0000000..6ebeb31
--- /dev/null
+++ b/kernel/trace/ktap/ffi/ffi_type.c
@@ -0,0 +1,52 @@
+#include "../../include/ktap_ffi.h"
+#ifdef __KERNEL__
+#include <linux/types.h>
+#else
+#include <stdint.h>
+#include <stddef.h>
+#endif
+
+#define CTYPE_MODE_HELPER(name, type)	\
+struct _##name##_align {		\
+	type t1;			\
+	char c;				\
+	type t2;			\
+};
+
+#define CTYPE_MODE(name)				\
+{							\
+	offsetof(struct _##name##_align, c),		\
+	offsetof(struct _##name##_align, t2) -		\
+		offsetof(struct _##name##_align, c),	\
+	#name					\
+}
+
+#define CTYPE_MODE_NAME(name) _##name##_mode
+
+/* ffi_ctype_mode should be corresponded to ffi_ctype */
+CTYPE_MODE_HELPER(uint8, uint8_t);
+CTYPE_MODE_HELPER(int8, int8_t);
+CTYPE_MODE_HELPER(uint16, uint16_t);
+CTYPE_MODE_HELPER(int16, int16_t);
+CTYPE_MODE_HELPER(uint32, uint32_t);
+CTYPE_MODE_HELPER(int32, int32_t);
+CTYPE_MODE_HELPER(uint64, uint64_t);
+CTYPE_MODE_HELPER(int64, int64_t);
+CTYPE_MODE_HELPER(pointer, void*);
+
+const ffi_mode ffi_type_modes[NUM_FFI_TYPE+1] = {
+	{0, 1, "void"},
+	CTYPE_MODE(uint8),
+	CTYPE_MODE(int8),
+	CTYPE_MODE(uint16),
+	CTYPE_MODE(int16),
+	CTYPE_MODE(uint32),
+	CTYPE_MODE(int32),
+	CTYPE_MODE(uint64),
+	CTYPE_MODE(int64),
+	CTYPE_MODE(pointer),
+	{0, 1, "function"},
+	{0, 1, "struct"},
+	{0, 1, "union"},
+	{0, 1, "unknown"},
+};
diff --git a/kernel/trace/ktap/ffi/ffi_util.c b/kernel/trace/ktap/ffi/ffi_util.c
new file mode 100644
index 0000000..1379ef9
--- /dev/null
+++ b/kernel/trace/ktap/ffi/ffi_util.c
@@ -0,0 +1,200 @@
+/*
+ * ffi_util.c - utility function for ffi module
+ *
+ * This file is part of ktap by Jovi Zhangwei.
+ *
+ * Copyright (C) 2012-2013 Jovi Zhangwei <jovi.zhangwei@gmail.com>.
+ *
+ * ktap is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * ktap is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+
+#include "../../include/ktap_types.h"
+#include "../../include/ktap_ffi.h"
+#include "../ktap.h"
+
+static void init_csym_struct(ktap_state_t *ks, csymbol_struct *csst)
+{
+	int nr = csymst_mb_nr(csst);
+	size_t size = 0;
+	size_t align = 1;
+	int i, var_len;
+	size_t var_size, var_align;
+
+	if (csymst_mb_nr(csst) < 0) {
+		kp_error(ks, "Find size of undefined struct");
+		return;
+	}
+
+	for (i = 0; i < nr; i++) {
+		csymbol *var_cs = csymst_mb_csym(ks, csst, i);
+		var_len = csymst_mb_len(csst, i);
+		if (var_len < 0) {
+			var_len = 1;
+		} else {
+			var_cs = csym_ptr_deref(ks, var_cs);
+		}
+		var_size = csym_size(ks, var_cs) * var_len;
+		var_align = csym_align(ks, var_cs);
+		size = ALIGN(size, var_align);
+		size += var_size;
+		align = align > var_align ? align : var_align;
+	}
+	size = ALIGN(size, align);
+	csst->size = size;
+	csst->align = align;
+}
+
+static void init_csym_union(ktap_state_t *ks, csymbol_struct *csst)
+{
+	int nr = csymst_mb_nr(csst);
+	size_t size = 0;
+	size_t align = 1;
+	int i, var_len;
+	size_t var_size, var_align;
+
+	if (csymst_mb_nr(csst) < 0) {
+		kp_error(ks, "Find size of undefined struct");
+		return;
+	}
+
+	for (i = 0; i < nr; i++) {
+		csymbol *var_cs = csymst_mb_csym(ks, csst, i);
+		var_len = csymst_mb_len(csst, i);
+		if (var_len < 0) {
+			var_len = 1;
+		} else {
+			var_cs = csym_ptr_deref(ks, var_cs);
+		}
+		var_size = csym_size(ks, var_cs) * var_len;
+		var_align = csym_align(ks, var_cs);
+		size = size > var_size ? size : var_size;
+		align = align > var_align ? align : var_align;
+	}
+	csst->size = size;
+	csst->align = align;
+}
+
+
+size_t csym_size(ktap_state_t *ks, csymbol *cs)
+{
+	ffi_type type = csym_type(cs);
+	switch(type) {
+	case FFI_STRUCT:
+		if (csym_struct(cs)->align == 0)
+			init_csym_struct(ks, csym_struct(cs));
+		return csym_struct(cs)->size;
+	case FFI_UNION:
+		if (csym_struct(cs)->align == 0)
+			init_csym_union(ks, csym_struct(cs));
+		return csym_struct(cs)->size;
+	default:
+		return ffi_type_size(type);
+	}
+}
+
+size_t csym_align(ktap_state_t *ks, csymbol *cs)
+{
+	ffi_type type = csym_type(cs);
+	switch(type) {
+	case FFI_STRUCT:
+		if (csym_struct(cs)->align == 0)
+			init_csym_struct(ks, csym_struct(cs));
+		return csym_struct(cs)->align;
+	case FFI_UNION:
+		if (csym_struct(cs)->align == 0)
+			init_csym_union(ks, csym_struct(cs));
+		return csym_struct(cs)->align;
+	default:
+		return ffi_type_align(type);
+	}
+}
+
+size_t csym_record_mb_offset_by_name(ktap_state_t *ks,
+		csymbol *cs, const char *name)
+{
+	csymbol_struct *csst = csym_struct(cs);
+	int nr = csymst_mb_nr(csst);
+	size_t off = 0, sub_off;
+	size_t align = 1;
+	int i, var_len;
+	size_t var_size, var_align;
+
+	if (nr < 0) {
+		kp_error(ks, "Find size of undefined struct");
+		return 0;
+	}
+
+	for (i = 0; i < nr; i++) {
+		csymbol *var_cs = csymst_mb_csym(ks, csst, i);
+		var_len = csymst_mb_len(csst, i);
+		if (var_len < 0) {
+			var_len = 1;
+		} else {
+			var_cs = csym_ptr_deref(ks, var_cs);
+		}
+		var_size = csym_size(ks, var_cs) * var_len;
+		var_align = csym_align(ks, var_cs);
+		off = ALIGN(off, var_align);
+		if (!strcmp(name, csymst_mb_name(csst, i)))
+			return off;
+		if (!strcmp("", csymst_mb_name(csst, i))) {
+			if (csym_type(var_cs) != FFI_STRUCT &&
+					csym_type(var_cs) != FFI_UNION) {
+				kp_error(ks, "Parse error: non-record type without name");
+				return -1;
+			}
+			sub_off = csym_record_mb_offset_by_name(ks,
+					var_cs, name);
+			if (sub_off >= 0)
+				return off + sub_off;
+		}
+		if (csym_type(cs) == FFI_STRUCT)
+			off += var_size;
+		else
+			off = 0;
+		align = align > var_align ? align : var_align;
+	}
+	return -1;
+}
+
+struct_member *csymst_mb_by_name(ktap_state_t *ks,
+		csymbol_struct *csst, const char *name)
+{
+	int nr = csymst_mb_nr(csst);
+	int i;
+	struct_member *memb;
+	csymbol *cs = NULL;
+
+	if (nr < 0) {
+		kp_error(ks, "Find size of undefined struct");
+		return NULL;
+	}
+
+	for (i = 0; i < nr; i++) {
+		if (!strcmp(name, csymst_mb_name(csst, i)))
+			return csymst_mb(csst, i);
+		if (!strcmp("", csymst_mb_name(csst, i))) {
+			cs = csymst_mb_csym(ks, csst, i);
+			if (csym_type(cs) != FFI_STRUCT && csym_type(cs) != FFI_UNION) {
+				kp_error(ks, "Parse error: non-record type without name");
+				return NULL;
+			}
+			memb = csymst_mb_by_name(ks, csym_struct(cs), name);
+			if (memb != NULL)
+				return memb;
+		}
+	}
+	return NULL;
+}
diff --git a/kernel/trace/ktap/kp_bcread.c b/kernel/trace/ktap/kp_bcread.c
new file mode 100644
index 0000000..4425837
--- /dev/null
+++ b/kernel/trace/ktap/kp_bcread.c
@@ -0,0 +1,429 @@
+/*
+ * Bytecode reader.
+ *
+ * This file is part of ktap by Jovi Zhangwei.
+ *
+ * Copyright (C) 2012-2014 Jovi Zhangwei <jovi.zhangwei@gmail.com>.
+ *
+ * Adapted from luajit and lua interpreter.
+ * Copyright (C) 2005-2014 Mike Pall.
+ * Copyright (C) 1994-2008 Lua.org, PUC-Rio.
+ *
+ * ktap is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * ktap is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+#include <uapi/ktap/ktap_types.h>
+#include <uapi/ktap/ktap_bc.h>
+#include <uapi/ktap/ktap_err.h>
+#include "ktap.h"
+#include "kp_obj.h"
+#include "kp_str.h"
+#include "kp_tab.h"
+#include "kp_vm.h"
+
+
+/* Context for bytecode reader. */
+typedef struct BCReadCtx {
+	ktap_state_t *ks;
+	int flags;
+	char *start;
+	char *p;
+	char *pe;
+	ktap_str_t *chunkname;
+	ktap_val_t *savetop;
+} BCReadCtx;
+
+
+#define bcread_flags(ctx)	(ctx)->flags
+#define bcread_swap(ctx) \
+	((bcread_flags(ctx) & BCDUMP_F_BE) != KP_BE*BCDUMP_F_BE)
+#define bcread_oldtop(ctx)		(ctx)->savetop
+#define bcread_savetop(ctx)	(ctx)->savetop = (ctx)->ks->top;
+
+static inline uint32_t bswap(uint32_t x)
+{
+	return (uint32_t)__builtin_bswap32((int32_t)x);
+}
+
+/* -- Input buffer handling ----------------------------------------------- */
+
+/* Throw reader error. */
+static void bcread_error(BCReadCtx *ctx, ErrMsg em)
+{
+	kp_error(ctx->ks, "%s\n", err2msg(em));
+}
+
+/* Return memory block from buffer. */
+static inline uint8_t *bcread_mem(BCReadCtx *ctx, int len)
+{
+	uint8_t *p = (uint8_t *)ctx->p;
+	ctx->p += len;
+	kp_assert(ctx->p <= ctx->pe);
+	return p;
+}
+
+/* Copy memory block from buffer. */
+static void bcread_block(BCReadCtx *ctx, void *q, int len)
+{
+	memcpy(q, bcread_mem(ctx, len), len);
+}
+
+/* Read byte from buffer. */
+static inline uint32_t bcread_byte(BCReadCtx *ctx)
+{
+	kp_assert(ctx->p < ctx->pe);
+	return (uint32_t)(uint8_t)*ctx->p++;
+}
+
+/* Read ULEB128 value from buffer. */
+static inline uint32_t bcread_uint32(BCReadCtx *ctx)
+{
+	uint32_t v;
+	bcread_block(ctx, &v, sizeof(uint32_t));
+	kp_assert(ctx->p <= ctx->pe);
+	return v;
+}
+
+/* -- Bytecode reader ----------------------------------------------------- */
+
+/* Read debug info of a prototype. */
+static void bcread_dbg(BCReadCtx *ctx, ktap_proto_t *pt, int sizedbg)
+{
+	void *lineinfo = (void *)proto_lineinfo(pt);
+
+	bcread_block(ctx, lineinfo, sizedbg);
+	/* Swap lineinfo if the endianess differs. */
+	if (bcread_swap(ctx) && pt->numline >= 256) {
+		int i, n = pt->sizebc-1;
+		if (pt->numline < 65536) {
+			uint16_t *p = (uint16_t *)lineinfo;
+			for (i = 0; i < n; i++)
+				p[i] = (uint16_t)((p[i] >> 8)|(p[i] << 8));
+		} else {
+			uint32_t *p = (uint32_t *)lineinfo;
+			for (i = 0; i < n; i++)
+				p[i] = bswap(p[i]);
+		}
+	}
+}
+
+/* Find pointer to varinfo. */
+static const void *bcread_varinfo(ktap_proto_t *pt)
+{
+	const uint8_t *p = proto_uvinfo(pt);
+	int n = pt->sizeuv;
+	if (n)
+		while (*p++ || --n) ;
+	return p;
+}
+
+/* Read a single constant key/value of a template table. */
+static int bcread_ktabk(BCReadCtx *ctx, ktap_val_t *o)
+{
+	int tp = bcread_uint32(ctx);
+	if (tp >= BCDUMP_KTAB_STR) {
+		int len = tp - BCDUMP_KTAB_STR;
+		const char *p = (const char *)bcread_mem(ctx, len);
+		ktap_str_t *ts = kp_str_new(ctx->ks, p, len);
+		if (unlikely(!ts))
+			return -ENOMEM;
+
+		set_string(o, ts);
+	} else if (tp == BCDUMP_KTAB_NUM) {
+		set_number(o, *(ktap_number *)bcread_mem(ctx,
+					sizeof(ktap_number)));
+	} else {
+ 		kp_assert(tp <= BCDUMP_KTAB_TRUE);
+		setitype(o, ~tp);
+	}
+	return 0;
+}
+
+/* Read a template table. */
+static ktap_tab_t *bcread_ktab(BCReadCtx *ctx)
+{
+	int narray = bcread_uint32(ctx);
+	int nhash = bcread_uint32(ctx);
+
+	ktap_tab_t *t = kp_tab_new(ctx->ks, narray, hsize2hbits(nhash));
+	if (!t)
+		return NULL;
+
+	if (narray) {  /* Read array entries. */
+		int i;
+		ktap_val_t *o = t->array;
+		for (i = 0; i < narray; i++, o++) 
+			if (bcread_ktabk(ctx, o))
+				return NULL;
+	}
+	if (nhash) {  /* Read hash entries. */
+		int i;
+		for (i = 0; i < nhash; i++) {
+			ktap_val_t key;
+			ktap_val_t val;
+			if (bcread_ktabk(ctx, &key))
+				return NULL;
+			kp_assert(!is_nil(&key));
+			if (bcread_ktabk(ctx, &val))
+				return NULL;
+			kp_tab_set(ctx->ks, t, &key, &val);
+		}
+	}
+	return t;
+}
+
+/* Read GC constants(string, table, child proto) of a prototype. */
+static int bcread_kgc(BCReadCtx *ctx, ktap_proto_t *pt, int sizekgc)
+{
+	ktap_obj_t **kr = (ktap_obj_t **)pt->k - (ptrdiff_t)sizekgc;
+	int i;
+
+	for (i = 0; i < sizekgc; i++, kr++) {
+		int tp = bcread_uint32(ctx);
+		if (tp >= BCDUMP_KGC_STR) {
+			int len = tp - BCDUMP_KGC_STR;
+			const char *p = (const char *)bcread_mem(ctx, len);
+			*kr =(ktap_obj_t *)kp_str_new(ctx->ks, p, len);
+			if (unlikely(!*kr))
+				return -1;
+		} else if (tp == BCDUMP_KGC_TAB) {
+			*kr = (ktap_obj_t *)bcread_ktab(ctx);
+			if (unlikely(!*kr))
+				return -1;
+		} else if (tp == BCDUMP_KGC_CHILD){
+			ktap_state_t *ks = ctx->ks;
+			if (ks->top <= bcread_oldtop(ctx)) {
+				bcread_error(ctx, KP_ERR_BCBAD);
+				return -1;
+			}
+			ks->top--;
+			*kr = (ktap_obj_t *)ptvalue(ks->top);
+		} else {
+			bcread_error(ctx, KP_ERR_BCBAD);
+			return -1;
+		}
+	}
+
+	return 0;
+}
+
+/* Read number constants of a prototype. */
+static void bcread_knum(BCReadCtx *ctx, ktap_proto_t *pt, int sizekn)
+{
+	int i;
+	ktap_val_t *o = pt->k;
+
+	for (i = 0; i < sizekn; i++, o++) {
+		set_number(o, *(ktap_number *)bcread_mem(ctx,
+					sizeof(ktap_number)));
+	}
+}
+
+/* Read bytecode instructions. */
+static void bcread_bytecode(BCReadCtx *ctx, ktap_proto_t *pt, int sizebc)
+{
+	BCIns *bc = proto_bc(pt);
+	bc[0] = BCINS_AD((pt->flags & PROTO_VARARG) ? BC_FUNCV : BC_FUNCF,
+			  pt->framesize, 0);
+	bcread_block(ctx, bc+1, (sizebc-1)*(int)sizeof(BCIns));
+	/* Swap bytecode instructions if the endianess differs. */
+	if (bcread_swap(ctx)) {
+		int i;
+		for (i = 1; i < sizebc; i++) bc[i] = bswap(bc[i]);
+	}
+}
+
+/* Read upvalue refs. */
+static void bcread_uv(BCReadCtx *ctx, ktap_proto_t *pt, int sizeuv)
+{
+	if (sizeuv) {
+		uint16_t *uv = proto_uv(pt);
+		bcread_block(ctx, uv, sizeuv*2);
+		/* Swap upvalue refs if the endianess differs. */
+		if (bcread_swap(ctx)) {
+			int i;
+			for (i = 0; i < sizeuv; i++)
+				uv[i] = (uint16_t)((uv[i] >> 8)|(uv[i] << 8));
+		}
+	}
+}
+
+/* Read a prototype. */
+static ktap_proto_t *bcread_proto(BCReadCtx *ctx)
+{
+	ktap_proto_t *pt;
+	int framesize, numparams, flags;
+	int sizeuv, sizekgc, sizekn, sizebc, sizept;
+	int ofsk, ofsuv, ofsdbg;
+	int sizedbg = 0;
+	BCLine firstline = 0, numline = 0;
+
+	/* Read prototype header. */
+	flags = bcread_byte(ctx);
+	numparams = bcread_byte(ctx);
+	framesize = bcread_byte(ctx);
+	sizeuv = bcread_byte(ctx);
+	sizekgc = bcread_uint32(ctx);
+	sizekn = bcread_uint32(ctx);
+	sizebc = bcread_uint32(ctx) + 1;
+	if (!(bcread_flags(ctx) & BCDUMP_F_STRIP)) {
+		sizedbg = bcread_uint32(ctx);
+		if (sizedbg) {
+			firstline = bcread_uint32(ctx);
+			numline = bcread_uint32(ctx);
+		}
+	}
+
+	/* Calculate total size of prototype including all colocated arrays. */
+	sizept = (int)sizeof(ktap_proto_t) + sizebc * (int)sizeof(BCIns) +
+			sizekgc * (int)sizeof(ktap_obj_t *);
+	sizept = (sizept + (int)sizeof(ktap_val_t)-1) &
+			~((int)sizeof(ktap_val_t)-1);
+	ofsk = sizept; sizept += sizekn*(int)sizeof(ktap_val_t);
+	ofsuv = sizept; sizept += ((sizeuv+1)&~1)*2;
+	ofsdbg = sizept; sizept += sizedbg;
+
+	/* Allocate prototype object and initialize its fields. */
+	pt = (ktap_proto_t *)kp_obj_new(ctx->ks, (int)sizept);
+	pt->gct = ~KTAP_TPROTO;
+	pt->numparams = (uint8_t)numparams;
+	pt->framesize = (uint8_t)framesize;
+	pt->sizebc = sizebc;
+	pt->k = (char *)pt + ofsk;
+	pt->uv = (char *)pt + ofsuv;
+	pt->sizekgc = 0;  /* Set to zero until fully initialized. */
+	pt->sizekn = sizekn;
+	pt->sizept = sizept;
+	pt->sizeuv = (uint8_t)sizeuv;
+	pt->flags = (uint8_t)flags;
+	pt->chunkname = ctx->chunkname;
+
+	/* Close potentially uninitialized gap between bc and kgc. */
+	*(uint32_t *)((char *)pt + ofsk - sizeof(ktap_obj_t *)*(sizekgc+1))
+									= 0;
+
+	/* Read bytecode instructions and upvalue refs. */
+	bcread_bytecode(ctx, pt, sizebc);
+	bcread_uv(ctx, pt, sizeuv);
+
+	/* Read constants. */
+	if (bcread_kgc(ctx, pt, sizekgc))
+		return NULL;
+	pt->sizekgc = sizekgc;
+	bcread_knum(ctx, pt, sizekn);
+
+	/* Read and initialize debug info. */
+	pt->firstline = firstline;
+	pt->numline = numline;
+	if (sizedbg) {
+		int sizeli = (sizebc-1) << (numline < 256 ? 0 :
+					numline < 65536 ? 1 : 2);
+		pt->lineinfo = (char *)pt + ofsdbg;
+		pt->uvinfo = (char *)pt + ofsdbg + sizeli;
+		bcread_dbg(ctx, pt, sizedbg);
+		pt->varinfo = (void *)bcread_varinfo(pt);
+	} else {
+		pt->lineinfo = NULL;
+		pt->uvinfo = NULL;
+		pt->varinfo = NULL;
+	}
+	return pt;
+}
+
+/* Read and check header of bytecode dump. */
+static int bcread_header(BCReadCtx *ctx)
+{
+	uint32_t flags;
+
+	if (bcread_byte(ctx) != BCDUMP_HEAD1 ||
+		bcread_byte(ctx) != BCDUMP_HEAD2 ||
+		bcread_byte(ctx) != BCDUMP_HEAD3 ||
+		bcread_byte(ctx) != BCDUMP_VERSION)
+		return -1;
+
+	bcread_flags(ctx) = flags = bcread_byte(ctx);
+
+	if ((flags & ~(BCDUMP_F_KNOWN)) != 0)
+		return -1;
+
+	if ((flags & BCDUMP_F_FFI)) {
+		return -1;
+	}
+
+	if ((flags & BCDUMP_F_STRIP)) {
+		ctx->chunkname = kp_str_newz(ctx->ks, "striped");
+	} else {
+		int len = bcread_uint32(ctx);
+		ctx->chunkname = kp_str_new(ctx->ks,
+				(const char *)bcread_mem(ctx, len), len);
+	}
+
+	if (unlikely(!ctx->chunkname))
+		return -1;
+
+	return 0;
+}
+
+/* Read a bytecode dump. */
+ktap_proto_t *kp_bcread(ktap_state_t *ks, unsigned char *buff, int len)
+{
+	BCReadCtx ctx;
+
+	ctx.ks = ks;
+	ctx.p = buff;
+	ctx.pe = buff + len;
+
+	ctx.start = buff;
+
+	bcread_savetop(&ctx);
+	/* Check for a valid bytecode dump header. */
+	if (bcread_header(&ctx)) {
+		bcread_error(&ctx, KP_ERR_BCFMT);
+		return NULL;
+	}
+
+	for (;;) {  /* Process all prototypes in the bytecode dump. */
+		ktap_proto_t *pt;
+		int len;
+		const char *startp;
+		/* Read length. */
+		if (ctx.p < ctx.pe && ctx.p[0] == 0) {  /* Shortcut EOF. */
+			ctx.p++;
+			break;
+		}
+		len = bcread_uint32(&ctx);
+		if (!len)
+			break;  /* EOF */
+		startp = ctx.p;
+		pt = bcread_proto(&ctx);
+		if (!pt)
+			return NULL;
+		if (ctx.p != startp + len) {
+			bcread_error(&ctx, KP_ERR_BCBAD);
+			return NULL;
+		}
+		set_proto(ks->top, pt);
+		incr_top(ks);
+	}
+	if ((int32_t)(2*(uint32_t)(ctx.pe - ctx.p)) > 0 ||
+			ks->top-1 != bcread_oldtop(&ctx)) {
+		bcread_error(&ctx, KP_ERR_BCBAD);
+		return NULL;
+	}
+
+	/* Pop off last prototype. */
+	ks->top--;
+	return ptvalue(ks->top);
+}
+
diff --git a/kernel/trace/ktap/kp_bcread.h b/kernel/trace/ktap/kp_bcread.h
new file mode 100644
index 0000000..ea2dde2
--- /dev/null
+++ b/kernel/trace/ktap/kp_bcread.h
@@ -0,0 +1,6 @@
+#ifndef __KTAP_BCREAD_H__
+#define __KTAP_BCREAD_H__
+
+ktap_proto_t *kp_bcread(ktap_state_t *ks, unsigned char *buff, int len);
+
+#endif /* __KTAP_BCREAD_H__ */
diff --git a/kernel/trace/ktap/kp_events.c b/kernel/trace/ktap/kp_events.c
new file mode 100644
index 0000000..1efbda3
--- /dev/null
+++ b/kernel/trace/ktap/kp_events.c
@@ -0,0 +1,843 @@
+/*
+ * kp_events.c - ktap events management (registry, destroy, event callback)
+ *
+ * This file is part of ktap by Jovi Zhangwei.
+ *
+ * Copyright (C) 2012-2013 Jovi Zhangwei <jovi.zhangwei@gmail.com>.
+ *
+ * ktap is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * ktap is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+#include <linux/module.h>
+#include <linux/ctype.h>
+#include <linux/slab.h>
+#include <linux/version.h>
+#include <asm/syscall.h>
+#include <uapi/ktap/ktap_types.h>
+#include "ktap.h"
+#include "kp_obj.h"
+#include "kp_str.h"
+#include "kp_transport.h"
+#include "kp_vm.h"
+#include "kp_events.h"
+
+const char *kp_event_tostr(ktap_state_t *ks)
+{
+	struct ktap_event_data *e = ks->current_event;
+	struct ftrace_event_call *call;
+	struct trace_iterator *iter;
+	struct trace_event *ev;
+	enum print_line_t ret = TRACE_TYPE_NO_CONSUME;
+	static const char *dummy_msg = "argstr_not_available";
+
+	/* need to check current context is vaild tracing context */
+	if (!ks->current_event) {
+		kp_error(ks, "cannot stringify event str in invalid context\n");
+		return NULL;
+	}
+
+	/*check if stringified before */
+	if (ks->current_event->argstr)
+		return getstr(ks->current_event->argstr);
+
+	/* timer event and raw tracepoint don't have associated argstr */
+	if (e->event->type == KTAP_EVENT_TYPE_PERF && e->event->perf->tp_event)
+		call = e->event->perf->tp_event;
+	else
+		return dummy_msg;
+
+	/* Simulate the iterator */
+
+	/*
+	 * use temp percpu buffer as trace_iterator
+	 * we cannot use same print_buffer because we may called from printf.
+	 */
+	iter = kp_this_cpu_temp_buffer(ks);
+
+	trace_seq_init(&iter->seq);
+	iter->ent = e->data->raw->data;
+
+	ev = &(call->event);
+	if (ev)
+		ret = ev->funcs->trace(iter, 0, ev);
+
+	if (ret != TRACE_TYPE_NO_CONSUME) {
+		struct trace_seq *s = &iter->seq;
+		int len = s->len >= PAGE_SIZE ? PAGE_SIZE - 1 : s->len;
+
+		s->buffer[len] = '\0';
+		return &s->buffer[0];
+	}
+
+	return dummy_msg;
+}
+
+/* return string repr of 'argstr' */
+const ktap_str_t *kp_event_stringify(ktap_state_t *ks)
+{
+	const char *str;
+	ktap_str_t *ts;
+
+	/*check if stringified before */
+	if (ks->current_event->argstr)
+		return ks->current_event->argstr;
+
+	str = kp_event_tostr(ks);
+	if (!str)
+		return NULL;
+
+	ts = kp_str_newz(ks, str);
+	ks->current_event->argstr = ts;
+	return ts;
+}
+
+/*
+ * This definition should keep update with kernel/trace/trace.h
+ * TODO: export this struct in kernel 
+ */
+struct ftrace_event_field {
+	struct list_head        link;
+	const char              *name;
+	const char              *type;
+	int                     filter_type;
+	int                     offset;
+	int                     size;
+	int                     is_signed;
+};
+
+static struct list_head *get_fields(struct ftrace_event_call *event_call)
+{
+	if (!event_call->class->get_fields)
+		return &event_call->class->fields;
+	return event_call->class->get_fields(event_call);
+}
+
+void kp_event_getarg(ktap_state_t *ks, ktap_val_t *ra, int idx)
+{
+	struct ktap_event_data *e = ks->current_event;
+	struct ktap_event *event = e->event;
+	struct ktap_event_field *event_fields = &event->fields[idx];
+
+	switch (event_fields->type)  {
+	case KTAP_EVENT_FIELD_TYPE_INT: {
+		struct trace_entry *entry = e->data->raw->data;
+		void *value = (unsigned char *)entry + event_fields->offset;
+		int n = *(int *)value;
+		set_number(ra, n);
+		return;
+		}
+	case KTAP_EVENT_FIELD_TYPE_LONG: {
+		struct trace_entry *entry = e->data->raw->data;
+		void *value = (unsigned char *)entry + event_fields->offset;
+		long n = *(long *)value;
+		set_number(ra, n);
+		return;
+		}
+	case KTAP_EVENT_FIELD_TYPE_STRING: {
+		struct trace_entry *entry = e->data->raw->data;
+		ktap_str_t *ts;
+		void *value = (unsigned char *)entry + event_fields->offset;
+		ts = kp_str_newz(ks, (char *)value);
+		if (ts)
+			set_string(ra, ts);
+		else
+			set_nil(ra);
+		return;
+		}
+	case KTAP_EVENT_FIELD_TYPE_CONST: {
+		set_number(ra, (ktap_number)event_fields->offset);
+		return;
+		}
+	case KTAP_EVENT_FIELD_TYPE_REGESTER: {
+		unsigned long *reg = (unsigned long *)((u8 *)e->regs +
+					event_fields->offset);
+		set_number(ra, *reg);
+		return;
+		}
+	case KTAP_EVENT_FIELD_TYPE_NIL:
+		set_nil(ra);
+		return;
+	case KTAP_EVENT_FIELD_TYPE_INVALID:
+		kp_error(ks, "the field type is not supported yet\n");
+		set_nil(ra);
+		return;
+	}
+}
+
+/* init all fields of event, for quick arg1..arg9 access */
+static int init_event_fields(ktap_state_t *ks, struct ktap_event *event)
+{
+	struct ftrace_event_call *event_call = event->perf->tp_event; 
+	struct ktap_event_field *event_fields = &event->fields[0];
+	struct ftrace_event_field *field;
+	struct list_head *head;
+	int idx = 0, n = 0;
+
+	/* only init fields for tracepoint, not timer event */
+	if (!event_call)
+		return 0;
+
+	/* intern probename */
+	event->name = kp_str_newz(ks, event_call->name);
+	if (unlikely(!event->name))
+		return -ENOMEM;
+
+	head = get_fields(event_call);
+	list_for_each_entry_reverse(field, head, link) {
+		if (n++ == 9) {
+			/*
+			 * For some events have fields more than 9, just ignore
+			 * those rest fields at present.
+			 *
+			 * TODO: support access all fields in tracepoint event
+			 *
+			 * Examples: mce:mce_record, ext4:ext4_writepages, ...
+			 */
+			return 0;
+		}
+
+		event_fields[idx].offset = field->offset;
+
+		if (field->size == 4) {
+			event_fields[idx].type = KTAP_EVENT_FIELD_TYPE_INT;
+			idx++;
+			continue;
+		} else if (field->size == 8) {
+			event_fields[idx].type = KTAP_EVENT_FIELD_TYPE_LONG;
+			idx++;
+			continue;
+		}
+		if (!strncmp(field->type, "char", 4)) {
+			event_fields[idx].type = KTAP_EVENT_FIELD_TYPE_STRING;
+			idx++;
+			continue;
+		}
+
+		/* TODO: add more type check */
+		event_fields[idx++].type = KTAP_EVENT_FIELD_TYPE_INVALID;
+	}
+
+	/* init all rest fields as NIL */
+	while (idx < 9)
+		event_fields[idx++].type = KTAP_EVENT_FIELD_TYPE_NIL;
+
+	return 0;
+}
+
+static inline void call_probe_closure(ktap_state_t *mainthread,
+				      ktap_func_t *fn,
+				      struct ktap_event_data *e, int rctx)
+{
+	ktap_state_t *ks;
+	ktap_val_t *func;
+
+	ks = kp_vm_new_thread(mainthread, rctx);
+	set_func(ks->top, fn);
+	func = ks->top;
+	incr_top(ks);
+
+	ks->current_event = e;
+
+	kp_vm_call(ks, func, 0);
+
+	ks->current_event = NULL;
+	kp_vm_exit_thread(ks);
+}
+
+/*
+ * Callback tracing function for perf event subsystem.
+ *
+ * make ktap reentrant, don't disable irq in callback function,
+ * same as perf and ftrace. to make reentrant, we need some
+ * percpu data to be context isolation(irq/sirq/nmi/process)
+ *
+ * The recursion checking in here is mainly purpose for avoiding
+ * corrupt ktap_state_t with timer closure callback. For tracepoint
+ * recusion, perf core already handle it.
+ *
+ * Note tracepoint handler is calling with rcu_read_lock.
+ */
+static void perf_callback(struct perf_event *perf_event,
+			   struct perf_sample_data *data,
+			   struct pt_regs *regs)
+{
+	struct ktap_event *event;
+	struct ktap_event_data e;
+	ktap_state_t *ks;
+	int rctx;
+
+	event = perf_event->overflow_handler_context;
+	ks = event->ks;
+
+	if (unlikely(ks->stop))
+		return;
+
+	rctx = get_recursion_context(ks);
+	if (unlikely(rctx < 0))
+		return;
+
+	e.event = event;
+	e.data = data;
+	e.regs = regs;
+	e.argstr = NULL;
+
+	call_probe_closure(ks, event->fn, &e, rctx);
+
+	put_recursion_context(ks, rctx);
+}
+
+/*
+ * Generic ktap event creation function (based on perf callback)
+ * purpose for tracepoints/kprobe/uprobe/profile-timer/hw_breakpoint/pmu.
+ */
+int kp_event_create(ktap_state_t *ks, struct perf_event_attr *attr,
+		    struct task_struct *task, const char *filter,
+		    ktap_func_t *fn)
+{
+	struct ktap_event *event;
+	struct perf_event *perf_event;
+	void *callback = perf_callback;
+	int cpu, ret;
+
+	if (G(ks)->parm->dry_run)
+		callback = NULL;
+
+	/*
+	 * don't tracing until ktap_wait, the reason is:
+	 * 1). some event may hit before apply filter
+	 * 2). more simple to manage tracing thread
+	 * 3). avoid race with mainthread.
+	 *
+	 * Another way to do this is make attr.disabled as 1, then use
+	 * perf_event_enable after filter apply, however, perf_event_enable
+	 * was not exported in kernel older than 3.3, so we drop this method.
+	 */
+	ks->stop = 1;
+
+	for_each_cpu(cpu, G(ks)->cpumask) {
+		event = kzalloc(sizeof(struct ktap_event), GFP_KERNEL);
+		if (!event)
+			return -ENOMEM;
+
+		event->type = KTAP_EVENT_TYPE_PERF;
+		event->ks = ks;
+		event->fn = fn;
+		perf_event = perf_event_create_kernel_counter(attr, cpu, task,
+							      callback, event);
+		if (IS_ERR(perf_event)) {
+			int err = PTR_ERR(perf_event);
+			kp_error(ks, "unable register perf event: "
+				     "[cpu: %d; id: %d; err: %d]\n",
+				     cpu, attr->config, err);
+			kfree(event);
+			return err;
+		}
+
+		if (attr->type == PERF_TYPE_TRACEPOINT) {
+			const char *name = perf_event->tp_event->name;
+			kp_verbose_printf(ks, "enable perf event: "
+					      "[cpu: %d; id: %d; name: %s; "
+					      "filter: %s; pid: %d]\n",
+					      cpu, attr->config, name, filter,
+					      task ? task_tgid_vnr(task) : -1);
+		} else if (attr->type == PERF_TYPE_SOFTWARE &&
+			 attr->config == PERF_COUNT_SW_CPU_CLOCK) {
+			kp_verbose_printf(ks, "enable profile event: "
+					      "[cpu: %d; sample_period: %d]\n",
+					      cpu, attr->sample_period);
+		} else {
+			kp_verbose_printf(ks, "unknown perf event type\n");
+		}
+
+		event->perf = perf_event;
+		INIT_LIST_HEAD(&event->list);
+		list_add_tail(&event->list, &G(ks)->events_head);
+
+		if (init_event_fields(ks, event)) {
+			kp_error(ks, "unable init event fields id %d\n",
+					attr->config);
+			perf_event_release_kernel(event->perf);
+			list_del(&event->list);
+			kfree(event);
+			return ret;
+		}
+
+		if (!filter)
+			continue;
+
+		ret = kp_ftrace_profile_set_filter(perf_event, attr->config,
+						   filter);
+		if (ret) {
+			kp_error(ks, "unable set event filter: "
+				     "[id: %d; filter: %s; ret: %d]\n",
+				     attr->config, filter, ret);
+			perf_event_release_kernel(event->perf);
+			list_del(&event->list);
+			kfree(event);
+			return ret;
+		}
+	}
+
+	return 0;
+}
+
+/*
+ * tracepoint_probe_register functions changed prototype by introduce
+ * 'struct tracepoint', this cause hard to refer tracepoint by name.
+ * And these ktap raw tracepoint interface is not courage to use, so disable
+ * it now.
+ */
+#if 0
+/*
+ * Ignore function proto in here, just use first argument.
+ */
+static void probe_callback(void *__data)
+{
+	struct ktap_event *event = __data;
+	ktap_state_t *ks = event->ks;
+	struct ktap_event_data e;
+	struct pt_regs regs; /* pt_regs maybe is large for stack */
+	int rctx;
+
+	if (unlikely(ks->stop))
+		return;
+
+	rctx = get_recursion_context(ks);
+	if (unlikely(rctx < 0))
+		return;
+
+	perf_fetch_caller_regs(&regs);
+
+	e.event = event;
+	e.regs = &regs;
+	e.argstr = NULL;
+
+	call_probe_closure(ks, event->fn, &e, rctx);
+
+	put_recursion_context(ks, rctx);
+}
+
+/*
+ * syscall events handling
+ */
+
+static DEFINE_MUTEX(syscall_trace_lock);
+static DECLARE_BITMAP(enabled_enter_syscalls, NR_syscalls);
+static DECLARE_BITMAP(enabled_exit_syscalls, NR_syscalls);
+static int sys_refcount_enter;
+static int sys_refcount_exit;
+
+static int get_syscall_num(const char *name)
+{
+	int i;
+
+	for (i = 0; i < NR_syscalls; i++) {
+		if (syscalls_metadata[i] &&
+		    !strcmp(name, syscalls_metadata[i]->name + 4))
+			return i;
+	}
+	return -1;
+}
+
+static void trace_syscall_enter(void *data, struct pt_regs *regs, long id)
+{
+	struct ktap_event *event = data;
+	ktap_state_t *ks = event->ks;
+	struct ktap_event_data e;
+	int syscall_nr;
+	int rctx;
+
+	if (unlikely(ks->stop))
+		return;
+
+	syscall_nr = syscall_get_nr(current, regs);
+	if (unlikely(syscall_nr < 0))
+		return;
+	if (!test_bit(syscall_nr, enabled_enter_syscalls))
+		return;
+
+	rctx = get_recursion_context(ks);
+	if (unlikely(rctx < 0))
+		return;
+
+	e.event = event;
+	e.regs = regs;
+	e.argstr = NULL;
+
+	call_probe_closure(ks, event->fn, &e, rctx);
+
+	put_recursion_context(ks, rctx);
+}
+
+static void trace_syscall_exit(void *data, struct pt_regs *regs, long id)
+{
+	struct ktap_event *event = data;
+	ktap_state_t *ks = event->ks;
+	struct ktap_event_data e;
+	int syscall_nr;
+	int rctx;
+
+	syscall_nr = syscall_get_nr(current, regs);
+	if (unlikely(syscall_nr < 0))
+		return;
+	if (!test_bit(syscall_nr, enabled_exit_syscalls))
+		return;
+
+	if (unlikely(ks->stop))
+		return;
+
+	rctx = get_recursion_context(ks);
+	if (unlikely(rctx < 0))
+		return;
+
+	e.event = event;
+	e.regs = regs;
+	e.argstr = NULL;
+
+	call_probe_closure(ks, event->fn, &e, rctx);
+
+	put_recursion_context(ks, rctx);
+}
+
+/* called in dry-run mode, purpose for compare overhead with normal vm call */
+static void dry_run_callback(void *data, struct pt_regs *regs, long id)
+{
+
+}
+
+static void init_syscall_event_fields(struct ktap_event *event, int is_enter)
+{
+	struct ftrace_event_call *event_call;
+	struct ktap_event_field *event_fields = &event->fields[0];
+	struct syscall_metadata *meta = syscalls_metadata[event->syscall_nr];
+	int idx = 0;
+
+	event_call = is_enter ? meta->enter_event : meta->exit_event;
+
+	event_fields[0].type = KTAP_EVENT_FIELD_TYPE_CONST;
+	event_fields[0].offset = event->syscall_nr;
+
+	if (!is_enter) {
+#ifdef CONFIG_X86_64
+		event_fields[1].type = KTAP_EVENT_FIELD_TYPE_REGESTER;
+		event_fields[1].offset = offsetof(struct pt_regs, ax);
+#endif
+		return;
+	}
+
+	while (idx++ < meta->nb_args) {
+		event_fields[idx].type = KTAP_EVENT_FIELD_TYPE_REGESTER;
+#ifdef CONFIG_X86_64
+		switch (idx) {
+		case 1:
+			event_fields[idx].offset = offsetof(struct pt_regs, di);
+			break;
+		case 2:
+			event_fields[idx].offset = offsetof(struct pt_regs, si);
+			break;
+		case 3:
+			event_fields[idx].offset = offsetof(struct pt_regs, dx);
+			break;
+		case 4:
+			event_fields[idx].offset =
+						offsetof(struct pt_regs, r10);
+			break;
+		case 5:
+			event_fields[idx].offset = offsetof(struct pt_regs, r8);
+			break;
+		case 6:
+			event_fields[idx].offset = offsetof(struct pt_regs, r9);
+			break;
+		}
+#else
+#warning "don't support syscall tracepoint event register access in this arch, use 'trace syscalls:* {}' instead"
+		break;
+#endif
+	}
+
+	/* init all rest fields as NIL */
+	while (idx < 9)
+		event_fields[idx++].type = KTAP_EVENT_FIELD_TYPE_NIL;
+}
+
+static int syscall_event_register(ktap_state_t *ks, const char *event_name,
+				  struct ktap_event *event)
+{
+	int syscall_nr = 0, is_enter = 0;
+	void *callback = NULL;
+	int ret = 0;
+
+	if (!strncmp(event_name, "sys_enter_", 10)) {
+		is_enter = 1;
+		event->type = KTAP_EVENT_TYPE_SYSCALL_ENTER;
+		syscall_nr = get_syscall_num(event_name + 10);
+		callback = trace_syscall_enter;
+	} else if (!strncmp(event_name, "sys_exit_", 9)) {
+		is_enter = 0;
+		event->type = KTAP_EVENT_TYPE_SYSCALL_EXIT;
+		syscall_nr = get_syscall_num(event_name + 9);
+		callback = trace_syscall_exit;
+	}
+	
+	if (G(ks)->parm->dry_run)
+		callback = dry_run_callback;
+
+	if (syscall_nr < 0)
+		return -1;
+
+	event->syscall_nr = syscall_nr;
+
+	init_syscall_event_fields(event, is_enter);
+
+	mutex_lock(&syscall_trace_lock);
+	if (is_enter) {
+		if (!sys_refcount_enter)
+			ret = register_trace_sys_enter(callback, event);
+		if (!ret) {
+			set_bit(syscall_nr, enabled_enter_syscalls);
+			sys_refcount_enter++;
+		}
+	} else {
+		if (!sys_refcount_exit)
+			ret = register_trace_sys_exit(callback, event);
+		if (!ret) {
+			set_bit(syscall_nr, enabled_exit_syscalls);
+			sys_refcount_exit++;
+		}
+	}
+	mutex_unlock(&syscall_trace_lock);
+
+	return ret;
+}
+
+static int syscall_event_unregister(ktap_state_t *ks, struct ktap_event *event)
+{
+	int ret = 0;
+	void *callback;
+	
+	if (event->type == KTAP_EVENT_TYPE_SYSCALL_ENTER)
+		callback = trace_syscall_enter;
+	else
+		callback = trace_syscall_exit;
+
+	if (G(ks)->parm->dry_run)
+		callback = dry_run_callback;
+
+	mutex_lock(&syscall_trace_lock);
+	if (event->type == KTAP_EVENT_TYPE_SYSCALL_ENTER) {
+		sys_refcount_enter--;
+        	clear_bit(event->syscall_nr, enabled_enter_syscalls);
+        	if (!sys_refcount_enter)
+                	unregister_trace_sys_enter(callback, event);
+	} else {
+		sys_refcount_exit--;
+        	clear_bit(event->syscall_nr, enabled_exit_syscalls);
+        	if (!sys_refcount_exit)
+                	unregister_trace_sys_exit(callback, event);
+	}
+	mutex_unlock(&syscall_trace_lock);
+
+	return ret;
+}
+
+/*
+ * Register tracepoint event directly, not based on perf callback
+ *
+ * This tracing method would be more faster than perf callback,
+ * because it won't need to write trace data into any temp buffer,
+ * and code path is much shorter than perf callback.
+ */
+int kp_event_create_tracepoint(ktap_state_t *ks, const char *event_name,
+			       ktap_func_t *fn)
+{
+	struct ktap_event *event;
+	void *callback = probe_callback;
+	int is_syscall = 0;
+	int ret;
+
+	if (G(ks)->parm->dry_run)
+		callback = NULL;
+
+	if (!strncmp(event_name, "sys_enter_", 10) ||
+	    !strncmp(event_name, "sys_exit_", 9))
+		is_syscall = 1;
+
+	event = kzalloc(sizeof(struct ktap_event), GFP_KERNEL);
+	if (!event)
+		return -ENOMEM;
+
+	event->ks = ks;
+	event->fn = fn;
+	event->name = kp_str_newz(ks, event_name);
+	if (unlikely(!event->name)) {
+		kfree(event);
+		return -ENOMEM;
+	}
+
+	INIT_LIST_HEAD(&event->list);
+	list_add_tail(&event->list, &G(ks)->events_head);
+
+	if (is_syscall) {
+		ret = syscall_event_register(ks, event_name, event);
+	} else {
+		event->type = KTAP_EVENT_TYPE_TRACEPOINT;
+		ret = tracepoint_probe_register(event_name, callback, event);
+	}
+
+	if (ret) {
+		kp_error(ks, "register tracepoint %s failed, ret: %d\n",
+				event_name, ret);
+		list_del(&event->list);
+		kfree(event);
+		return ret;
+	}
+	return 0;
+}
+
+#endif
+
+/* kprobe handler */
+static int __kprobes pre_handler_kprobe(struct kprobe *p, struct pt_regs *regs)
+{
+	struct ktap_event *event = container_of(p, struct ktap_event, kp);
+	ktap_state_t *ks = event->ks;
+	struct ktap_event_data e;
+	int rctx;
+
+	if (unlikely(ks->stop))
+		return 0;
+
+	rctx = get_recursion_context(ks);
+	if (unlikely(rctx < 0))
+		return 0;
+
+	e.event = event;
+	e.regs = regs;
+	e.argstr = NULL;
+
+	call_probe_closure(ks, event->fn, &e, rctx);
+
+	put_recursion_context(ks, rctx);
+	return 0;
+}
+
+/*
+ * Register kprobe event directly, not based on perf callback
+ *
+ * This tracing method would be more faster than perf callback,
+ * because it won't need to write trace data into any temp buffer,
+ * and code path is much shorter than perf callback.
+ */
+int kp_event_create_kprobe(ktap_state_t *ks, const char *event_name,
+			   ktap_func_t *fn)
+{
+	struct ktap_event *event;
+	void *callback = pre_handler_kprobe;
+	int ret;
+
+	if (G(ks)->parm->dry_run)
+		callback = NULL;
+
+	event = kzalloc(sizeof(struct ktap_event), GFP_KERNEL);
+	if (!event)
+		return -ENOMEM;
+
+	event->ks = ks;
+	event->fn = fn;
+	event->name = kp_str_newz(ks, event_name);
+	if (unlikely(!event->name)) {
+		kfree(event);
+		return -ENOMEM;
+	}
+
+	INIT_LIST_HEAD(&event->list);
+	list_add_tail(&event->list, &G(ks)->events_head);
+
+	event->type = KTAP_EVENT_TYPE_KPROBE;
+
+	event->kp.symbol_name = event_name;
+	event->kp.pre_handler = callback;
+	ret = register_kprobe(&event->kp);
+	if (ret) {
+		kp_error(ks, "register kprobe event %s failed, ret: %d\n",
+				event_name, ret);
+		list_del(&event->list);
+		kfree(event);
+		return ret;
+	}
+	return 0;
+}
+
+
+static void events_destroy(ktap_state_t *ks)
+{
+	struct ktap_event *event;
+	struct list_head *tmp, *pos;
+	struct list_head *head = &G(ks)->events_head;
+
+	list_for_each(pos, head) {
+		event = container_of(pos, struct ktap_event,
+					   list);
+		if (event->type == KTAP_EVENT_TYPE_PERF)
+			perf_event_release_kernel(event->perf);
+#if 0
+		else if (event->type == KTAP_EVENT_TYPE_TRACEPOINT)
+			tracepoint_probe_unregister(getstr(event->name),
+						    probe_callback, event);
+		else if (event->type == KTAP_EVENT_TYPE_SYSCALL_ENTER ||
+			 event->type == KTAP_EVENT_TYPE_SYSCALL_EXIT )
+			syscall_event_unregister(ks, event);
+#endif
+		else if (event->type == KTAP_EVENT_TYPE_KPROBE)
+			unregister_kprobe(&event->kp);
+        }
+       	/*
+	 * Ensure our callback won't be called anymore. The buffers
+	 * will be freed after that.
+	 */
+	tracepoint_synchronize_unregister();
+
+	list_for_each_safe(pos, tmp, head) {
+		event = container_of(pos, struct ktap_event,
+					   list);
+		list_del(&event->list);
+		kfree(event);
+	}
+}
+
+void kp_events_exit(ktap_state_t *ks)
+{
+	if (!G(ks)->trace_enabled)
+		return;
+
+	events_destroy(ks);
+
+	/* call trace_end_closure after all event unregistered */
+	if ((G(ks)->state != KTAP_ERROR) && G(ks)->trace_end_closure) {
+		G(ks)->state = KTAP_TRACE_END;
+		set_func(ks->top, G(ks)->trace_end_closure);
+		incr_top(ks);
+		kp_vm_call(ks, ks->top - 1, 0);
+		G(ks)->trace_end_closure = NULL;
+	}
+
+	G(ks)->trace_enabled = 0;
+}
+
+int kp_events_init(ktap_state_t *ks)
+{
+	G(ks)->trace_enabled = 1;
+	return 0;
+}
+
diff --git a/kernel/trace/ktap/kp_events.h b/kernel/trace/ktap/kp_events.h
new file mode 100644
index 0000000..b24f723
--- /dev/null
+++ b/kernel/trace/ktap/kp_events.h
@@ -0,0 +1,71 @@
+#ifndef __KTAP_EVENTS_H__
+#define __KTAP_EVENTS_H__
+
+#include <linux/ftrace_event.h>
+#include <trace/syscall.h>
+#include <trace/events/syscalls.h>
+#include <linux/syscalls.h>
+#include <linux/kprobes.h>
+
+enum KTAP_EVENT_FIELD_TYPE {
+	KTAP_EVENT_FIELD_TYPE_INVALID = 0, /* arg type not support yet */
+
+	KTAP_EVENT_FIELD_TYPE_INT,
+	KTAP_EVENT_FIELD_TYPE_LONG,
+	KTAP_EVENT_FIELD_TYPE_STRING,
+
+	KTAP_EVENT_FIELD_TYPE_REGESTER,
+	KTAP_EVENT_FIELD_TYPE_CONST,
+	KTAP_EVENT_FIELD_TYPE_NIL /* arg not exist */
+};
+
+struct ktap_event_field {
+	enum KTAP_EVENT_FIELD_TYPE type;
+	int offset;
+};
+
+enum KTAP_EVENT_TYPE {
+	KTAP_EVENT_TYPE_PERF,
+	KTAP_EVENT_TYPE_TRACEPOINT,
+	KTAP_EVENT_TYPE_SYSCALL_ENTER,
+	KTAP_EVENT_TYPE_SYSCALL_EXIT,
+	KTAP_EVENT_TYPE_KPROBE,
+};
+
+struct ktap_event {
+	struct list_head list;
+	int type;
+	ktap_state_t *ks;
+	ktap_func_t *fn;
+	struct perf_event *perf;
+	int syscall_nr; /* for syscall event */
+	struct ktap_event_field fields[9]; /* arg1..arg9 */
+	ktap_str_t *name; /* intern probename string */
+
+	struct kprobe kp; /* kprobe event */
+};
+
+/* this structure allocate on stack */
+struct ktap_event_data {
+	struct ktap_event *event;
+	struct perf_sample_data *data;
+	struct pt_regs *regs;
+	ktap_str_t *argstr; /* for cache argstr intern string */
+};
+
+int kp_events_init(ktap_state_t *ks);
+void kp_events_exit(ktap_state_t *ks);
+
+int kp_event_create(ktap_state_t *ks, struct perf_event_attr *attr,
+		    struct task_struct *task, const char *filter,
+		    ktap_func_t *fn);
+int kp_event_create_tracepoint(ktap_state_t *ks, const char *event_name,
+			       ktap_func_t *fn);
+
+int kp_event_create_kprobe(ktap_state_t *ks, const char *event_name,
+			   ktap_func_t *fn);
+void kp_event_getarg(ktap_state_t *ks, ktap_val_t *ra, int idx);
+const char *kp_event_tostr(ktap_state_t *ks);
+const ktap_str_t *kp_event_stringify(ktap_state_t *ks);
+
+#endif /* __KTAP_EVENTS_H__ */
diff --git a/kernel/trace/ktap/kp_mempool.c b/kernel/trace/ktap/kp_mempool.c
new file mode 100644
index 0000000..2d40a0c
--- /dev/null
+++ b/kernel/trace/ktap/kp_mempool.c
@@ -0,0 +1,94 @@
+/*
+ * kp_mempool.c - ktap memory pool, service for string allocation
+ *
+ * This file is part of ktap by Jovi Zhangwei.
+ *
+ * Copyright (C) 2012-2013 Jovi Zhangwei <jovi.zhangwei@gmail.com>.
+ *
+ * ktap is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * ktap is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+#include <uapi/ktap/ktap_types.h>
+#include "kp_obj.h"
+#include "kp_str.h"
+
+#include <linux/ctype.h>
+#include <linux/module.h>
+#include "ktap.h"
+
+
+/*
+ * allocate memory from mempool, the allocated memory will be free
+ * util ktap exit.
+ * TODO: lock-free allocation
+ */
+void *kp_mempool_alloc(ktap_state_t *ks, int size)
+{
+	ktap_global_state_t *g = G(ks);
+	void *mempool = g->mempool;
+	void *freepos = g->mp_freepos;
+	void *addr;
+	unsigned long flags;
+
+	local_irq_save(flags);
+	arch_spin_lock(&g->mp_lock);
+
+	if (unlikely((unsigned long)((char *)freepos + size)) >
+		     (unsigned long)((char *)mempool + g->mp_size)) {
+		addr = NULL;
+		goto out;
+	}
+
+	addr = freepos;
+	g->mp_freepos = (char *)freepos + size;
+ out:
+
+	arch_spin_unlock(&g->mp_lock);
+	local_irq_restore(flags);
+	return addr;
+}
+
+/*
+ * destroy mempool.
+ */
+void kp_mempool_destroy(ktap_state_t *ks)
+{
+	ktap_global_state_t *g = G(ks);
+
+	if (!g->mempool)
+		return;
+
+	vfree(g->mempool);
+	g->mempool = NULL;
+	g->mp_freepos = NULL;
+	g->mp_size = 0;
+}
+
+/*
+ * pre-allocate size Kbytes memory pool.
+ */
+int kp_mempool_init(ktap_state_t *ks, int size)
+{
+	ktap_global_state_t *g = G(ks);
+
+	g->mempool = vmalloc(size * 1024);
+	if (!g->mempool)
+		return -ENOMEM;
+
+	g->mp_freepos = g->mempool;
+	g->mp_size = size * 1024;
+	g->mp_lock = (arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED;
+	return 0;
+}
+
diff --git a/kernel/trace/ktap/kp_mempool.h b/kernel/trace/ktap/kp_mempool.h
new file mode 100644
index 0000000..3eabf5e
--- /dev/null
+++ b/kernel/trace/ktap/kp_mempool.h
@@ -0,0 +1,8 @@
+#ifndef __KTAP_MEMPOOL_H__
+#define __KTAP_MEMPOOL_H__
+
+void *kp_mempool_alloc(ktap_state_t *ks, int size);
+void kp_mempool_destroy(ktap_state_t *ks);
+int kp_mempool_init(ktap_state_t *ks, int size);
+
+#endif /* __KTAP_MEMPOOL_H__ */
diff --git a/kernel/trace/ktap/kp_obj.c b/kernel/trace/ktap/kp_obj.c
new file mode 100644
index 0000000..2011cf6
--- /dev/null
+++ b/kernel/trace/ktap/kp_obj.c
@@ -0,0 +1,293 @@
+/*
+ * kp_obj.c - ktap object generic operation
+ *
+ * This file is part of ktap by Jovi Zhangwei.
+ *
+ * Copyright (C) 2012-2013 Jovi Zhangwei <jovi.zhangwei@gmail.com>.
+ *
+ * Adapted from luajit and lua interpreter.
+ * Copyright (C) 2005-2014 Mike Pall.
+ * Copyright (C) 1994-2008 Lua.org, PUC-Rio.
+ *
+ * ktap is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * ktap is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+#include <linux/stacktrace.h>
+#include <linux/module.h>
+#include <linux/kallsyms.h>
+#include <linux/slab.h>
+#include <uapi/ktap/ktap_types.h>
+#include <uapi/ktap/ktap_ffi.h>
+#include "kp_obj.h"
+#include "kp_str.h"
+#include "kp_tab.h"
+#include "ktap.h"
+#include "kp_vm.h"
+#include "kp_transport.h"
+
+/* Error message strings. */
+const char *kp_err_allmsg =
+#define ERRDEF(name, msg)       msg "\0"
+#include <uapi/ktap/ktap_errmsg.h>
+;
+
+/* memory allocation flag */
+#define KTAP_ALLOC_FLAGS ((GFP_KERNEL | __GFP_NORETRY | __GFP_NOWARN) \
+			 & ~__GFP_WAIT)
+
+/*
+ * TODO: It's not safe to call into facilities in the kernel at-large,
+ * so we may need to use ktap own memory pool, not kmalloc.
+ */
+
+
+void *kp_malloc(ktap_state_t *ks, int size)
+{
+	void *addr;
+
+	addr = kmalloc(size, KTAP_ALLOC_FLAGS);
+	if (unlikely(!addr)) {
+		kp_error(ks, "kmalloc failed\n");
+	}
+	return addr;
+}
+
+void *kp_zalloc(ktap_state_t *ks, int size)
+{
+	void *addr;
+
+	addr = kzalloc(size, KTAP_ALLOC_FLAGS);
+	if (unlikely(!addr))
+		kp_error(ks, "kzalloc failed\n");
+	return addr;
+}
+
+void kp_free(ktap_state_t *ks, void *addr)
+{
+	kfree(addr);
+}
+
+
+void kp_obj_dump(ktap_state_t *ks, const ktap_val_t *v)
+{
+	switch (itype(v)) {
+	case KTAP_TNIL:
+		kp_puts(ks, "NIL");
+		break;
+	case KTAP_TTRUE:
+		kp_printf(ks, "true");
+		break;
+	case KTAP_TFALSE:
+		kp_printf(ks, "false");
+		break;
+	case KTAP_TNUM:
+		kp_printf(ks, "NUM %ld", nvalue(v));
+		break;
+	case KTAP_TLIGHTUD:
+		kp_printf(ks, "LIGHTUD 0x%lx", (unsigned long)pvalue(v));
+		break;
+	case KTAP_TFUNC:
+		kp_printf(ks, "FUNCTION 0x%lx", (unsigned long)fvalue(v));
+		break;
+	case KTAP_TSTR:
+		kp_printf(ks, "STR #%s", svalue(v));
+		break;
+	case KTAP_TTAB:
+		kp_printf(ks, "TABLE 0x%lx", (unsigned long)hvalue(v));
+		break;
+        default:
+		kp_printf(ks, "GCVALUE 0x%lx", (unsigned long)gcvalue(v));
+		break;
+	}
+}
+
+void kp_obj_show(ktap_state_t *ks, const ktap_val_t *v)
+{
+	switch (itype(v)) {
+	case KTAP_TNIL:
+		kp_puts(ks, "nil");
+		break;
+	case KTAP_TTRUE:
+		kp_puts(ks, "true");
+		break;
+	case KTAP_TFALSE:
+		kp_puts(ks, "false");
+		break;
+	case KTAP_TNUM:
+		kp_printf(ks, "%ld", nvalue(v));
+		break;
+	case KTAP_TLIGHTUD:
+		kp_printf(ks, "lightud 0x%lx", (unsigned long)pvalue(v));
+		break;
+	case KTAP_TCFUNC:
+		kp_printf(ks, "cfunction 0x%lx", (unsigned long)fvalue(v));
+		break;
+	case KTAP_TFUNC:
+		kp_printf(ks, "function 0x%lx", (unsigned long)gcvalue(v));
+		break;
+	case KTAP_TSTR:
+		kp_puts(ks, svalue(v));
+		break;
+	case KTAP_TTAB:
+		kp_printf(ks, "table 0x%lx", (unsigned long)hvalue(v));
+		break;
+#ifdef CONFIG_KTAP_FFI
+	case KTAP_TCDATA:
+		kp_cdata_dump(ks, cdvalue(v));
+		break;
+#endif
+	case KTAP_TEVENTSTR:
+		/* check event context */
+		if (!ks->current_event) {
+			kp_error(ks,
+			"cannot stringify event str in invalid context\n");
+			return;
+		}
+
+		kp_transport_event_write(ks, ks->current_event);
+		break;
+	case KTAP_TKSTACK:
+		kp_transport_print_kstack(ks, v->val.stack.depth,
+					      v->val.stack.skip);
+		break;
+        default:
+		kp_error(ks, "print unknown value type: %d\n", itype(v));
+		break;
+	}
+}
+
+
+/*
+ * equality of ktap values.
+ */
+int kp_obj_rawequal(const ktap_val_t *t1, const ktap_val_t *t2)
+{
+	switch (itype(t1)) {
+	case KTAP_TNIL:
+	case KTAP_TTRUE:
+	case KTAP_TFALSE:
+		return 1;
+	case KTAP_TNUM:
+		return nvalue(t1) == nvalue(t2);
+	case KTAP_TLIGHTUD:
+		return pvalue(t1) == pvalue(t2);
+	case KTAP_TFUNC:
+		return fvalue(t1) == fvalue(t2);
+	case KTAP_TSTR:
+		return rawtsvalue(t1) == rawtsvalue(t2);
+	case KTAP_TTAB:
+		return hvalue(t1) == hvalue(t2);
+	default:
+		return gcvalue(t1) == gcvalue(t2);
+	}
+
+	return 0;
+}
+
+/*
+ * ktap will not use lua's length operator for table,
+ * also # is not for length operator any more in ktap.
+ */
+int kp_obj_len(ktap_state_t *ks, const ktap_val_t *v)
+{
+	switch(itype(v)) {
+	case KTAP_TTAB:
+		return kp_tab_len(ks, hvalue(v));
+	case KTAP_TSTR:
+		return rawtsvalue(v)->len;
+	default:
+		kp_printf(ks, "cannot get length of type %d\n", v->type);
+		return -1;
+	}
+	return 0;
+}
+
+/* need to protect allgc field? */
+ktap_obj_t *kp_obj_new(ktap_state_t *ks, size_t size)
+{
+	ktap_obj_t *o, **list;
+
+	if (ks != G(ks)->mainthread) {
+		kp_error(ks, "kp_obj_new only can be called in mainthread\n");
+		return NULL;
+	}
+
+	o = kp_malloc(ks, size);
+	if (unlikely(!o))
+		return NULL;
+
+	list = &G(ks)->allgc;
+	gch(o)->nextgc = *list;
+	*list = o;
+
+	return o;
+}
+
+
+/* this function may be time consuming, move out from table set/get? */
+ktap_str_t *kp_obj_kstack2str(ktap_state_t *ks, uint16_t depth, uint16_t skip)
+{
+	struct stack_trace trace;
+	unsigned long *bt;
+	char *btstr, *p;
+	int i;
+
+	bt = kp_this_cpu_print_buffer(ks); /* use print percpu buffer */
+	trace.nr_entries = 0;
+	trace.skip = skip;
+	trace.max_entries = depth;
+	trace.entries = (unsigned long *)(bt + 1);
+	save_stack_trace(&trace);
+
+	/* convert backtrace to string */
+	p = btstr = kp_this_cpu_temp_buffer(ks);
+	for (i = 0; i < trace.nr_entries; i++) {
+		unsigned long addr = trace.entries[i];
+
+		if (addr == ULONG_MAX)
+			break;
+
+		p += sprint_symbol(p, addr);
+		*p++ = '\n';
+        }
+
+	return kp_str_new(ks, btstr, p - btstr);
+}
+
+void kp_obj_free_gclist(ktap_state_t *ks, ktap_obj_t *o)
+{
+	while (o) {
+		ktap_obj_t *next;
+
+		next = gch(o)->nextgc;
+		switch (gch(o)->gct) {
+		case ~KTAP_TTAB:
+			kp_tab_free(ks, (ktap_tab_t *)o);
+			break;
+		case ~KTAP_TUPVAL:
+			kp_freeupval(ks, (ktap_upval_t *)o);
+			break;
+		default:
+			kp_free(ks, o);
+		}
+		o = next;
+	}
+}
+
+void kp_obj_freeall(ktap_state_t *ks)
+{
+	kp_obj_free_gclist(ks, G(ks)->allgc);
+	G(ks)->allgc = NULL;
+}
+
diff --git a/kernel/trace/ktap/kp_obj.h b/kernel/trace/ktap/kp_obj.h
new file mode 100644
index 0000000..2655ee6
--- /dev/null
+++ b/kernel/trace/ktap/kp_obj.h
@@ -0,0 +1,20 @@
+#ifndef __KTAP_OBJ_H__
+#define __KTAP_OBJ_H__
+
+void *kp_malloc(ktap_state_t *ks, int size);
+void *kp_zalloc(ktap_state_t *ks, int size);
+void kp_free(ktap_state_t *ks, void *addr);
+
+void kp_obj_dump(ktap_state_t *ks, const ktap_val_t *v);
+void kp_obj_show(ktap_state_t *ks, const ktap_val_t *v);
+int kp_obj_len(ktap_state_t *ks, const ktap_val_t *rb);
+ktap_obj_t *kp_obj_new(ktap_state_t *ks, size_t size);
+int kp_obj_rawequal(const ktap_val_t *t1, const ktap_val_t *t2);
+ktap_str_t *kp_obj_kstack2str(ktap_state_t *ks, uint16_t depth, uint16_t skip);
+void kp_obj_free_gclist(ktap_state_t *ks, ktap_obj_t *o);
+void kp_obj_freeall(ktap_state_t *ks);
+
+#define kp_obj_equal(o1, o2) \
+	(((o1)->type == (o2)->type) && kp_obj_rawequal(o1, o2))
+
+#endif /* __KTAP_OBJ_H__ */
diff --git a/kernel/trace/ktap/kp_str.c b/kernel/trace/ktap/kp_str.c
new file mode 100644
index 0000000..fdfb82b
--- /dev/null
+++ b/kernel/trace/ktap/kp_str.c
@@ -0,0 +1,393 @@
+/*
+ * kp_str.c - ktap string data struction manipulation
+ *
+ * This file is part of ktap by Jovi Zhangwei.
+ *
+ * Copyright (C) 2012-2013 Jovi Zhangwei <jovi.zhangwei@gmail.com>.
+ *
+ * Adapted from luajit and lua interpreter.
+ * Copyright (C) 2005-2014 Mike Pall.
+ * Copyright (C) 1994-2008 Lua.org, PUC-Rio.
+ *
+ * ktap is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * ktap is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+#include <uapi/ktap/ktap_types.h>
+#include "kp_obj.h"
+#include "kp_str.h"
+#include "kp_mempool.h"
+
+#include <linux/ctype.h>
+#include <linux/module.h>
+#include <linux/kallsyms.h>
+#include "ktap.h"
+#include "kp_transport.h"
+#include "kp_vm.h"
+#include "kp_events.h"
+
+int kp_str_cmp(const ktap_str_t *ls, const ktap_str_t *rs)
+{
+	const char *l = getstr(ls);
+	size_t ll = ls->len;
+	const char *r = getstr(rs);
+	size_t lr = rs->len;
+
+	for (;;) {
+		int temp = strcmp(l, r);
+		if (temp != 0)
+			return temp;
+		else {
+			/* strings are equal up to a `\0' */
+
+			/* index of first `\0' in both strings */
+			size_t len = strlen(l);
+
+			/* r is finished? */
+			if (len == lr)
+				return (len == ll) ? 0 : 1;
+			else if (len == ll)  /* l is finished? */
+				return -1;
+
+			/*
+			 * both strings longer than `len';
+			 * go on comparing (after the `\0')
+			 */
+			len++;
+			l += len; ll -= len; r += len; lr -= len;
+		}
+	}
+}
+
+/* Fast string data comparison. Caveat: unaligned access to 1st string! */
+static __always_inline int str_fastcmp(const char *a, const char *b, int len)
+{
+	int i = 0;
+
+	kp_assert(len > 0);
+	kp_assert((((uintptr_t)a + len - 1)&(PAGE_SIZE - 1)) <= PAGE_SIZE - 4);
+
+	do {  /* Note: innocuous access up to end of string + 3. */
+		uint32_t v = *(uint32_t *)(a + i) ^ *(const uint32_t *)(b + i);
+		if (v) {
+			i -= len;
+#if KP_LE
+			return (int32_t)i >= -3 ? (v << (32 + (i << 3))) : 1;
+#else
+			return (int32_t)i >= -3 ? (v >> (32 + (i << 3))) : 1;
+#endif
+		}
+		i += 4;
+	} while (i < len);
+	return 0;
+}
+
+
+//TODO: change hash algo
+
+#define STRING_HASHLIMIT	5
+static __always_inline unsigned int kp_str_hash(const char *str, size_t len)
+{
+	unsigned int h = 201236 ^ len;
+	size_t step = (len >> STRING_HASHLIMIT) + 1;
+	size_t l1;
+
+	for (l1 = len; l1 >= step; l1 -= step)
+		h = h ^ ((h<<5) + (h>>2) + (u8)(str[l1 - 1]));
+
+	return h;
+}
+
+
+/*
+ * resizes the string table
+ */
+int kp_str_resize(ktap_state_t *ks, int newmask)
+{
+	ktap_global_state_t *g = G(ks);
+	ktap_str_t **newhash;
+
+	newhash = kp_zalloc(ks, (newmask + 1) * sizeof(ktap_str_t *));
+	if (!newhash)
+		return -ENOMEM;
+
+	g->strmask = newmask;
+	g->strhash = newhash;
+	return 0;
+}
+
+/*
+ * Intern a string and return string object.
+ */
+ktap_str_t * kp_str_new(ktap_state_t *ks, const char *str, size_t len)
+{
+	ktap_global_state_t *g = G(ks);
+	ktap_str_t *s;
+	ktap_obj_t *o;
+	unsigned int h = kp_str_hash(str, len);
+	unsigned long flags;
+
+	if (len >= KP_MAX_STR)
+		return NULL;
+
+	local_irq_save(flags);
+	arch_spin_lock(&g->str_lock);
+
+	o = (ktap_obj_t *)g->strhash[h & g->strmask];
+	if (likely((((uintptr_t)str+len-1) & (PAGE_SIZE-1)) <= PAGE_SIZE-4)) {
+		while (o != NULL) {
+			ktap_str_t *sx = (ktap_str_t *)o;
+			if (sx->len == len &&
+			    !str_fastcmp(str, getstr(sx), len)) {
+				arch_spin_unlock(&g->str_lock);
+				local_irq_restore(flags);
+				return sx; /* Return existing string. */
+			}
+			o = gch(o)->nextgc;
+		}
+	} else { /* Slow path: end of string is too close to a page boundary */
+		while (o != NULL) {
+			ktap_str_t *sx = (ktap_str_t *)o;
+			if (sx->len == len &&
+			    !memcmp(str, getstr(sx), len)) {
+				arch_spin_unlock(&g->str_lock);
+				local_irq_restore(flags);
+				return sx; /* Return existing string. */
+			}
+			o = gch(o)->nextgc;
+		}
+	}
+
+	/* create a new string, allocate it from mempool, not use kmalloc. */
+	s = kp_mempool_alloc(ks, sizeof(ktap_str_t) + len + 1);
+	if (unlikely(!s))
+		goto out;
+	s->gct = ~KTAP_TSTR;
+	s->len = len;
+	s->hash = h;
+	s->reserved = 0;
+	memcpy(s + 1, str, len);
+	((char *)(s + 1))[len] = '\0';  /* ending 0 */
+
+	/* Add it to string hash table */
+	h &= g->strmask;
+	s->nextgc = (ktap_obj_t *)g->strhash[h];
+	g->strhash[h] = s;
+	if (g->strnum++ > KP_MAX_STRNUM) {
+		kp_error(ks, "exceed max string number %d\n", KP_MAX_STRNUM);
+		s = NULL;
+	}
+
+ out:
+	arch_spin_unlock(&g->str_lock);
+	local_irq_restore(flags);
+	return s; /* Return newly interned string. */
+}
+
+void kp_str_freeall(ktap_state_t *ks)
+{
+	/* don't need to free string in here, it will handled by mempool */
+	kp_free(ks, G(ks)->strhash);
+}
+
+/* kp_str_fmt - printf implementation */
+
+/* macro to `unsign' a character */
+#define uchar(c)	((unsigned char)(c))
+
+#define L_ESC		'%'
+
+/* valid flags in a format specification */
+#define FLAGS	"-+ #0"
+
+#define INTFRMLEN	"ll"
+#define INTFRM_T	long long
+
+/*
+ * maximum size of each format specification (such as '%-099.99d')
+ * (+10 accounts for %99.99x plus margin of error)
+ */
+#define MAX_FORMAT	(sizeof(FLAGS) + sizeof(INTFRMLEN) + 10)
+
+static const char *scanformat(ktap_state_t *ks, const char *strfrmt, char *form)
+{
+	const char *p = strfrmt;
+	while (*p != '\0' && strchr(FLAGS, *p) != NULL)
+		p++;  /* skip flags */
+
+	if ((size_t)(p - strfrmt) >= sizeof(FLAGS)/sizeof(char)) {
+		kp_error(ks, "invalid format (repeated flags)\n");
+		return NULL;
+	}
+
+	if (isdigit(uchar(*p)))
+		p++;  /* skip width */
+
+	if (isdigit(uchar(*p)))
+		p++;  /* (2 digits at most) */
+
+	if (*p == '.') {
+		p++;
+		if (isdigit(uchar(*p)))
+			p++;  /* skip precision */
+		if (isdigit(uchar(*p)))
+			p++;  /* (2 digits at most) */
+	}
+
+	if (isdigit(uchar(*p))) {
+		kp_error(ks, "invalid format (width or precision too long)\n");
+		return NULL;
+	}
+
+	*(form++) = '%';
+	memcpy(form, strfrmt, (p - strfrmt + 1) * sizeof(char));
+	form += p - strfrmt + 1;
+	*form = '\0';
+	return p;
+}
+
+
+/*
+ * add length modifier into formats
+ */
+static void addlenmod(char *form, const char *lenmod)
+{
+	size_t l = strlen(form);
+	size_t lm = strlen(lenmod);
+	char spec = form[l - 1];
+
+	strcpy(form + l - 1, lenmod);
+	form[l + lm - 1] = spec;
+	form[l + lm] = '\0';
+}
+
+
+static void arg_error(ktap_state_t *ks, int narg, const char *extramsg)
+{
+	kp_error(ks, "bad argument #%d: (%s)\n", narg, extramsg);
+}
+
+int kp_str_fmt(ktap_state_t *ks, struct trace_seq *seq)
+{
+	int arg = 1;
+	size_t sfl;
+	ktap_val_t *arg_fmt = kp_arg(ks, 1);
+	int argnum = kp_arg_nr(ks);
+	const char *strfrmt, *strfrmt_end;
+
+	strfrmt = svalue(arg_fmt);
+	sfl = rawtsvalue(arg_fmt)->len;
+	strfrmt_end = strfrmt + sfl;
+
+	while (strfrmt < strfrmt_end) {
+		if (*strfrmt != L_ESC)
+			trace_seq_putc(seq, *strfrmt++);
+		else if (*++strfrmt == L_ESC)
+			trace_seq_putc(seq, *strfrmt++);
+		else { /* format item */
+			char form[MAX_FORMAT];
+
+			if (++arg > argnum) {
+				arg_error(ks, arg, "no value");
+				return -1;
+			}
+
+			strfrmt = scanformat(ks, strfrmt, form);
+			switch (*strfrmt++) {
+			case 'c':
+				kp_arg_checknumber(ks, arg);
+
+				trace_seq_printf(seq, form,
+						 nvalue(kp_arg(ks, arg)));
+				break;
+			case 'd':  case 'i': {
+				ktap_number n;
+				INTFRM_T ni;
+
+				kp_arg_checknumber(ks, arg);
+
+				n = nvalue(kp_arg(ks, arg));
+				ni = (INTFRM_T)n;
+				addlenmod(form, INTFRMLEN);
+				trace_seq_printf(seq, form, ni);
+				break;
+			}
+			case 'p': {
+				char str[KSYM_SYMBOL_LEN];
+
+				kp_arg_checknumber(ks, arg);
+
+				SPRINT_SYMBOL(str, nvalue(kp_arg(ks, arg)));
+				_trace_seq_puts(seq, str);
+				break;
+			}
+			case 'o':  case 'u':  case 'x':  case 'X': {
+				ktap_number n;
+				unsigned INTFRM_T ni;
+
+				kp_arg_checknumber(ks, arg);
+
+				n = nvalue(kp_arg(ks, arg));
+				ni = (unsigned INTFRM_T)n;
+				addlenmod(form, INTFRMLEN);
+				trace_seq_printf(seq, form, ni);
+				break;
+			}
+			case 's': {
+				ktap_val_t *v = kp_arg(ks, arg);
+				const char *s;
+				size_t l;
+
+				if (is_nil(v)) {
+					_trace_seq_puts(seq, "nil");
+					return 0;
+				}
+
+				if (is_eventstr(v)) {
+					const char *str = kp_event_tostr(ks);
+					if (!str)
+						return  -1;
+					_trace_seq_puts(seq,
+						kp_event_tostr(ks));
+					return 0;
+				}
+
+				kp_arg_checkstring(ks, arg);
+
+				s = svalue(v);
+				l = rawtsvalue(v)->len;
+				if (!strchr(form, '.') && l >= 100) {
+					/*
+					 * no precision and string is too long
+					 * to be formatted;
+					 * keep original string
+					 */
+					_trace_seq_puts(seq, s);
+					break;
+				} else {
+					trace_seq_printf(seq, form, s);
+					break;
+				}
+			}
+			default: /* also treat cases `pnLlh' */
+				kp_error(ks, "invalid option " KTAP_QL("%%%c")
+					     " to " KTAP_QL("format"),
+					     *(strfrmt - 1));
+				return -1;
+			}
+		}
+	}
+
+	return 0;
+}
+
diff --git a/kernel/trace/ktap/kp_str.h b/kernel/trace/ktap/kp_str.h
new file mode 100644
index 0000000..c6a52a6
--- /dev/null
+++ b/kernel/trace/ktap/kp_str.h
@@ -0,0 +1,14 @@
+#ifndef __KTAP_STR_H__
+#define __KTAP_STR_H__
+
+int kp_str_cmp(const ktap_str_t *ls, const ktap_str_t *rs);
+int kp_str_resize(ktap_state_t *ks, int newmask);
+void kp_str_freeall(ktap_state_t *ks);
+ktap_str_t * kp_str_new(ktap_state_t *ks, const char *str, size_t len);
+
+#define kp_str_newz(ks, s)	(kp_str_new(ks, s, strlen(s)))
+
+#include <linux/trace_seq.h>
+int kp_str_fmt(ktap_state_t *ks, struct trace_seq *seq);
+
+#endif /* __KTAP_STR_H__ */
diff --git a/kernel/trace/ktap/kp_tab.c b/kernel/trace/ktap/kp_tab.c
new file mode 100644
index 0000000..1cbacde
--- /dev/null
+++ b/kernel/trace/ktap/kp_tab.c
@@ -0,0 +1,845 @@
+/*
+ * kp_tab.c - Table handling.
+ *
+ * This file is part of ktap by Jovi Zhangwei.
+ *
+ * Copyright (C) 2012-2013 Jovi Zhangwei <jovi.zhangwei@gmail.com>.
+ *
+ * Adapted from luajit and lua interpreter.
+ * Copyright (C) 2005-2014 Mike Pall.
+ * Copyright (C) 1994-2008 Lua.org, PUC-Rio.
+ *
+ * ktap is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * ktap is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+#include <linux/spinlock.h>
+#include <linux/module.h>
+#include <linux/kallsyms.h>
+#include <linux/slab.h>
+#include <linux/sort.h>
+#include <uapi/ktap/ktap_types.h>
+#include "ktap.h"
+#include "kp_vm.h"
+#include "kp_obj.h"
+#include "kp_str.h"
+#include "kp_events.h"
+#include "kp_tab.h"
+
+#define tab_lock_init(t)						\
+	do {								\
+		(t)->lock = (arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED;	\
+	} while (0)
+#define tab_lock(t)						\
+	do {								\
+		local_irq_save(flags);					\
+		arch_spin_lock(&(t)->lock);				\
+	} while (0)
+#define tab_unlock(t)						\
+	do {								\
+		arch_spin_unlock(&(t)->lock);				\
+		local_irq_restore(flags);				\
+	} while (0)
+
+
+const ktap_val_t kp_niltv = { {NULL}, {KTAP_TNIL} } ;
+#define niltv  (&kp_niltv)
+
+/* -- Object hashing ------------------------------------------------------ */
+
+/* Hash values are masked with the table hash mask and used as an index. */
+static __always_inline
+ktap_node_t *hashmask(const ktap_tab_t *t, uint32_t hash)
+{
+	ktap_node_t *n = t->node;
+	return &n[hash & t->hmask];
+}
+
+/* String hashes are precomputed when they are interned. */
+#define hashstr(t, s)		hashmask(t, (s)->hash)
+
+#define hashlohi(t, lo, hi)	hashmask((t), hashrot((lo), (hi)))
+#define hashnum(t, o)		hashlohi((t), (o)->val.n & 0xffffffff, 0)
+#define hashgcref(t, o)		hashlohi((t),	\
+				((unsigned long)(o)->val.gc & 0xffffffff), \
+				((unsigned long)(o)->val.gc & 0xffffffff) + HASH_BIAS)
+
+
+/* Hash an arbitrary key and return its anchor position in the hash table. */
+static ktap_node_t *hashkey(const ktap_tab_t *t, const ktap_val_t *key)
+{
+	kp_assert(!tvisint(key));
+	if (is_string(key))
+		return hashstr(t, rawtsvalue(key));
+	else if (is_number(key))
+		return hashnum(t, key);
+	else if (is_bool(key))
+		return hashmask(t, boolvalue(key));
+	else
+		return hashgcref(t, key);
+}
+
+/* -- Table creation and destruction -------------------------------------- */
+
+/* Create new hash part for table. */
+static __always_inline
+int newhpart(ktap_state_t *ks, ktap_tab_t *t, uint32_t hbits)
+{
+	uint32_t hsize;
+	ktap_node_t *node;
+	kp_assert(hbits != 0);
+
+	if (hbits > KP_MAX_HBITS) {
+		//kp_error(ks, LJ_ERR_TABOV);
+		kp_error(ks, "table overflow\n");
+		return -1;
+	}
+	hsize = 1u << hbits;
+	node = vmalloc(hsize * sizeof(ktap_node_t));
+	if (!node)
+		return -ENOMEM;
+	t->freetop = &node[hsize];
+	t->node = node;
+	t->hmask = hsize-1;
+
+	return 0;
+}
+
+/*
+ * Q: Why all of these copies of t->hmask, t->node etc. to local variables?
+ * A: Because alias analysis for C is _really_ tough.
+ *    Even state-of-the-art C compilers won't produce good code without this.
+ */
+
+/* Clear hash part of table. */
+static __always_inline void clearhpart(ktap_tab_t *t)
+{
+	uint32_t i, hmask = t->hmask;
+	ktap_node_t *node = t->node;
+	kp_assert(t->hmask != 0);
+
+	for (i = 0; i <= hmask; i++) {
+		ktap_node_t *n = &node[i];
+		n->next = NULL;
+		set_nil(&n->key);
+		set_nil(&n->val);
+	}
+
+	t->hnum = 0;
+}
+
+/* Clear array part of table. */
+static __always_inline void clearapart(ktap_tab_t *t)
+{
+	uint32_t i, asize = t->asize;
+	ktap_val_t *array = t->array;
+	for (i = 0; i < asize; i++)
+		set_nil(&array[i]);
+}
+
+/* Create a new table. Note: the slots are not initialized (yet). */
+static ktap_tab_t *newtab(ktap_state_t *ks, uint32_t asize, uint32_t hbits)
+{
+	ktap_tab_t *t;
+ 
+	t = (ktap_tab_t *)kp_obj_new(ks, sizeof(ktap_tab_t));
+	t->gct = ~KTAP_TTAB;
+	t->array = NULL;
+	t->asize = 0;  /* In case the array allocation fails. */
+	t->hmask = 0;
+
+	tab_lock_init(t);
+
+	if (asize > 0) {
+		if (asize > KP_MAX_ASIZE) {
+			kp_error(ks, "table overflow\n");
+			return NULL;
+		}
+
+		t->array = vmalloc(asize * sizeof(ktap_val_t));
+		if (!t->array)
+			return NULL;
+		t->asize = asize;
+	}
+	if (hbits)
+		if (newhpart(ks, t, hbits)) {
+			vfree(t->array);
+			return NULL;		
+		}
+	return t;
+}
+
+/* Create a new table.
+ *
+ * The array size is non-inclusive. E.g. asize=128 creates array slots
+ * for 0..127, but not for 128. If you need slots 1..128, pass asize=129
+ * (slot 0 is wasted in this case).
+ *
+ * The hash size is given in hash bits. hbits=0 means no hash part.
+ * hbits=1 creates 2 hash slots, hbits=2 creates 4 hash slots and so on.
+ */
+ktap_tab_t *kp_tab_new(ktap_state_t *ks, uint32_t asize, uint32_t hbits)
+{
+	ktap_tab_t *t = newtab(ks, asize, hbits);
+	if (!t)
+		return NULL;
+
+	clearapart(t);
+	if (t->hmask > 0)
+		clearhpart(t);
+	return t;
+}
+
+#define TABLE_NARR_ENTRIES	255 /* PAGE_SIZE / sizeof(ktap_value) - 1 */
+#define TABLE_NREC_ENTRIES	2048 /* (PAGE_SIZE * 20) / sizeof(ktap_tnode)*/
+
+ktap_tab_t *kp_tab_new_ah(ktap_state_t *ks, int32_t a, int32_t h)
+{
+	if (a == 0 && h == 0) {
+		a = TABLE_NARR_ENTRIES;
+		h = TABLE_NREC_ENTRIES;
+	}
+
+	return kp_tab_new(ks, (uint32_t)(a > 0 ? a+1 : 0), hsize2hbits(h));
+}
+
+/* Duplicate a table. */
+ktap_tab_t *kp_tab_dup(ktap_state_t *ks, const ktap_tab_t *kt)
+{
+	ktap_tab_t *t;
+	uint32_t asize, hmask;
+	int i;
+
+	/* allocate default table size */
+	t = kp_tab_new_ah(ks, 0, 0);
+	if (!t)
+		return NULL;
+
+	asize = kt->asize;
+	if (asize > 0) {
+		ktap_val_t *array = t->array;
+		ktap_val_t *karray = kt->array;
+		if (asize < 64) {
+			/* An inlined loop beats memcpy for < 512 bytes. */
+			uint32_t i;
+			for (i = 0; i < asize; i++)
+				set_obj(&array[i], &karray[i]);
+		} else {
+			memcpy(array, karray, asize*sizeof(ktap_val_t));
+		}
+	}
+
+	hmask = kt->hmask;
+	for (i = 0; i <= hmask; i++) {
+		ktap_node_t *knode = &kt->node[i];
+		if (is_nil(&knode->key))
+			continue;
+		kp_tab_set(ks, t, &knode->key, &knode->val);
+	}
+	return t;
+}
+
+/* Clear a table. */
+void kp_tab_clear(ktap_tab_t *t)
+{
+	clearapart(t);
+	if (t->hmask > 0) {
+		ktap_node_t *node = t->node;
+		t->freetop = &node[t->hmask+1];
+		clearhpart(t);
+	}
+}
+
+/* Free a table. */
+void kp_tab_free(ktap_state_t *ks, ktap_tab_t *t)
+{
+	if (t->hmask > 0)
+		vfree(t->node);
+	if (t->asize > 0)
+		vfree(t->array);
+	kp_free(ks, t);
+}
+
+/* -- Table getters ------------------------------------------------------- */
+
+static const ktap_val_t *tab_getinth(ktap_tab_t *t, uint32_t key)
+{
+	ktap_val_t k;
+	ktap_node_t *n;
+
+	set_number(&k, (ktap_number)key);
+	n = hashnum(t, &k);
+	do {
+		if (is_number(&n->key) && nvalue(&n->key) == key) {
+			return &n->val;
+		}
+	} while ((n = n->next));
+	return niltv;
+}
+
+static __always_inline
+const ktap_val_t *tab_getint(ktap_tab_t *t, uint32_t key)
+{
+	return ((key < t->asize) ? arrayslot(t, key) :
+				   tab_getinth(t, key));
+}
+
+void kp_tab_getint(ktap_tab_t *t, uint32_t key, ktap_val_t *val)
+{
+	unsigned long flags;
+
+	tab_lock(t);
+	set_obj(val, tab_getint(t, key));
+	tab_unlock(t);
+}
+
+static const ktap_val_t *tab_getstr(ktap_tab_t *t, ktap_str_t *key)
+{
+	ktap_node_t *n = hashstr(t, key);
+	do {
+		if (is_string(&n->key) && rawtsvalue(&n->key) == key)
+			return &n->val;
+	} while ((n = n->next));
+	return niltv;
+}
+
+void kp_tab_getstr(ktap_tab_t *t, ktap_str_t *key, ktap_val_t *val)
+{
+	unsigned long flags;
+
+	tab_lock(t);
+	set_obj(val,  tab_getstr(t, key));
+	tab_unlock(t);
+}
+
+static const ktap_val_t *tab_get(ktap_state_t *ks, ktap_tab_t *t,
+				 const ktap_val_t *key)
+{
+	if (is_string(key)) {
+		return tab_getstr(t, rawtsvalue(key));
+	} else if (is_number(key)) {
+		ktap_number nk = nvalue(key);
+		uint32_t k = (uint32_t)nk;
+		if (nk == (ktap_number)k) {
+			return tab_getint(t, k);
+		} else {
+			goto genlookup;	/* Else use the generic lookup. */
+		}
+	} else if (is_eventstr(key)) {
+		const ktap_str_t *ts;
+
+		if (!ks->current_event) {
+			kp_error(ks,
+			"cannot stringify event str in invalid context\n");
+			return niltv;
+		}
+
+		ts = kp_event_stringify(ks);
+		if (!ts)
+			return niltv;
+
+		return tab_getstr(t, rawtsvalue(key));
+	} else if (!is_nil(key)) {
+		ktap_node_t *n;
+ genlookup:
+		n = hashkey(t, key);
+		do {
+			if (kp_obj_equal(&n->key, key))
+				return &n->val;
+		} while ((n = n->next));
+	}
+	return niltv;
+}
+
+void kp_tab_get(ktap_state_t *ks, ktap_tab_t *t, const ktap_val_t *key,
+		ktap_val_t *val)
+{
+	unsigned long flags;
+
+	tab_lock(t);
+	set_obj(val, tab_get(ks, t, key));
+	tab_unlock(t);
+}
+
+/* -- Table setters ------------------------------------------------------- */
+
+/* Insert new key. Use Brent's variation to optimize the chain length. */
+static ktap_val_t *kp_tab_newkey(ktap_state_t *ks, ktap_tab_t *t,
+				 const ktap_val_t *key)
+{
+	ktap_node_t *n = hashkey(t, key);
+
+	if (!is_nil(&n->val) || t->hmask == 0) {
+		ktap_node_t *nodebase = t->node;
+		ktap_node_t *collide, *freenode = t->freetop;
+
+		kp_assert(freenode >= nodebase &&
+			  freenode <= nodebase+t->hmask+1);
+		do {
+			if (freenode == nodebase) {  /* No free node found? */
+				//kp_error(ks, LJ_ERR_TABOV);
+				kp_error(ks, "table overflow\n");
+				return NULL;
+			}
+		} while (!is_nil(&(--freenode)->key));
+
+		t->freetop = freenode;
+		collide = hashkey(t, &n->key);
+		if (collide != n) {  /* Colliding node not the main node? */
+			while (collide->next != n)
+				/* Find predecessor. */
+				collide = collide->next;
+			collide->next = freenode;  /* Relink chain. */
+ 			/* Copy colliding node into free node and
+			 * free main node. */
+			freenode->val = n->val;
+			freenode->key = n->key;
+			freenode->next = n->next;
+			n->next = NULL;
+			set_nil(&n->val);
+			/* Rechain pseudo-resurrected string keys with
+			 * colliding hashes. */
+			while (freenode->next) {
+				ktap_node_t *nn = freenode->next;
+				if (is_string(&nn->key) && !is_nil(&nn->val) &&
+					hashstr(t, rawtsvalue(&nn->key)) == n) {
+					freenode->next = nn->next;
+					nn->next = n->next;
+					n->next = nn;
+				} else {
+					freenode = nn;
+				}
+			}
+		} else {  /* Otherwise use free node. */
+			freenode->next = n->next;  /* Insert into chain. */
+			n->next = freenode;
+			n = freenode;
+		}
+	}
+	set_obj(&n->key, key);
+	t->hnum++;
+	return &n->val;
+}
+
+static ktap_val_t *tab_setinth(ktap_state_t *ks, ktap_tab_t *t, uint32_t key)
+{
+	ktap_val_t k;
+	ktap_node_t *n;
+
+	set_number(&k, (ktap_number)key);
+	n = hashnum(t, &k);
+	do {
+		if (is_number(&n->key) && nvalue(&n->key) == key)
+			return &n->val;
+	} while ((n = n->next));
+	return kp_tab_newkey(ks, t, &k);
+}
+
+static __always_inline
+ktap_val_t *tab_setint(ktap_state_t *ks, ktap_tab_t *t, uint32_t key)
+{
+	return ((key < t->asize) ? arrayslot(t, key) :
+				   tab_setinth(ks, t, key));
+}
+
+void kp_tab_setint(ktap_state_t *ks, ktap_tab_t *t,
+		   uint32_t key, const ktap_val_t *val)
+{
+	ktap_val_t *v;
+	unsigned long flags;
+
+	tab_lock(t);
+	v = tab_setint(ks, t, key);
+	if (likely(v))
+		set_obj(v, val);
+	tab_unlock(t);
+}
+
+void kp_tab_incrint(ktap_state_t *ks, ktap_tab_t *t, uint32_t key,
+		    ktap_number n)
+{
+	ktap_val_t *v;
+	unsigned long flags;
+
+	tab_lock(t);
+	v = tab_setint(ks, t, key);
+	if (unlikely(!v))
+		goto out;
+
+	if (likely(is_number(v)))
+		set_number(v, nvalue(v) + n);
+	else if (is_nil(v))
+		set_number(v, n);
+	else
+		kp_error(ks, "use '+=' operator on non-number value\n");
+
+ out:
+	tab_unlock(t);
+}
+
+static ktap_val_t *tab_setstr(ktap_state_t *ks, ktap_tab_t *t,
+			      const ktap_str_t *key)
+{
+	ktap_val_t k;
+	ktap_node_t *n = hashstr(t, key);
+	do {
+		if (is_string(&n->key) && rawtsvalue(&n->key) == key)
+			return &n->val;
+	} while ((n = n->next));
+	set_string(&k, key);
+	return kp_tab_newkey(ks, t, &k);
+}
+
+void kp_tab_setstr(ktap_state_t *ks, ktap_tab_t *t, const ktap_str_t *key,
+		   const ktap_val_t *val)
+{
+	ktap_val_t *v;
+	unsigned long flags;
+
+	tab_lock(t);
+	v = tab_setstr(ks, t, key);
+	if (likely(v))
+		set_obj(v, val);
+	tab_unlock(t);
+}
+
+void kp_tab_incrstr(ktap_state_t *ks, ktap_tab_t *t, const ktap_str_t *key,
+		    ktap_number n)
+{
+	ktap_val_t *v;
+	unsigned long flags;
+
+	tab_lock(t);
+	v = tab_setstr(ks, t, key);
+	if (unlikely(!v))
+		goto out;
+
+	if (likely(is_number(v)))
+		set_number(v, nvalue(v) + n);
+	else if (is_nil(v))
+		set_number(v, n);
+	else
+		kp_error(ks, "use '+=' operator on non-number value\n");
+ out:
+	tab_unlock(t);
+}
+
+static ktap_val_t *tab_set(ktap_state_t *ks, ktap_tab_t *t,
+			   const ktap_val_t *key)
+{
+	ktap_node_t *n;
+
+	if (is_string(key)) {
+		return tab_setstr(ks, t, rawtsvalue(key));
+	} else if (is_number(key)) {
+		ktap_number nk = nvalue(key);
+		uint32_t k = (ktap_number)nk;
+		if (nk == (ktap_number)k)
+			return tab_setint(ks, t, k);
+	} else if (itype(key) == KTAP_TKSTACK) {
+		/* change stack into string */
+		ktap_str_t *bt = kp_obj_kstack2str(ks, key->val.stack.depth,
+						       key->val.stack.skip);
+		if (!bt)
+			return NULL;
+		return tab_setstr(ks, t, bt);
+	} else if (is_eventstr(key)) {
+		const ktap_str_t *ts;
+
+		if (!ks->current_event) {
+			kp_error(ks,
+			"cannot stringify event str in invalid context\n");
+			return NULL;
+		}
+
+		ts = kp_event_stringify(ks);
+		if (!ts)
+			return NULL;
+
+		return tab_setstr(ks, t, ts);
+		/* Else use the generic lookup. */
+	} else if (is_nil(key)) {
+		//kp_error(ks, LJ_ERR_NILIDX);
+		kp_error(ks, "table nil index\n");
+		return NULL;
+	}
+	n = hashkey(t, key);
+	do {
+		if (kp_obj_equal(&n->key, key))
+			return &n->val;
+	} while ((n = n->next));
+	return kp_tab_newkey(ks, t, key);
+}
+
+void kp_tab_set(ktap_state_t *ks, ktap_tab_t *t,
+		const ktap_val_t *key, const ktap_val_t *val)
+{
+	ktap_val_t *v;
+	unsigned long flags;
+
+	tab_lock(t);
+	v = tab_set(ks, t, key);
+	if (likely(v))
+		set_obj(v, val);
+	tab_unlock(t);
+}
+
+void kp_tab_incr(ktap_state_t *ks, ktap_tab_t *t, ktap_val_t *key,
+		 ktap_number n)
+{
+	ktap_val_t *v;
+	unsigned long flags;
+
+	tab_lock(t);
+	v = tab_set(ks, t, key);
+	if (unlikely(!v))
+		goto out;
+
+	if (likely(is_number(v)))
+		set_number(v, nvalue(v) + n);
+	else if (is_nil(v))
+		set_number(v, n);
+	else
+		kp_error(ks, "use '+=' operator on non-number value\n");
+ out:
+	tab_unlock(t);
+}
+
+
+/* -- Table traversal ----------------------------------------------------- */
+
+/* Get the traversal index of a key. */
+static uint32_t keyindex(ktap_state_t *ks, ktap_tab_t *t,
+			 const ktap_val_t *key)
+{
+	if (is_number(key)) {
+		ktap_number nk = nvalue(key);
+		uint32_t k = (uint32_t)nk;
+		/* Array key indexes: [0..t->asize-1] */
+		if ((uint32_t)k < t->asize && nk == (ktap_number)k)
+			return (uint32_t)k;
+	}
+
+	if (!is_nil(key)) {
+		ktap_node_t *n = hashkey(t, key);
+		do {
+			if (kp_obj_equal(&n->key, key))
+				return t->asize + (uint32_t)(n - (t->node));
+			/* Hash key indexes: [t->asize..t->asize+t->nmask] */
+		} while ((n = n->next));
+		//kp_err_msg(ks, LJ_ERR_NEXTIDX);
+		kp_error(ks, "table next index\n");
+		return 0;  /* unreachable */
+	}
+	return ~0u;  /* A nil key starts the traversal. */
+}
+
+/* Advance to the next step in a table traversal. */
+int kp_tab_next(ktap_state_t *ks, ktap_tab_t *t, ktap_val_t *key)
+{
+	unsigned long flags;
+	uint32_t i;
+
+	tab_lock(t);
+	i = keyindex(ks, t, key);  /* Find predecessor key index. */
+
+	/* First traverse the array keys. */
+	for (i++; i < t->asize; i++)
+ 		if (!is_nil(arrayslot(t, i))) {
+			set_number(key, i);
+			set_obj(key + 1, arrayslot(t, i));
+			tab_unlock(t);
+			return 1;
+		}
+	/* Then traverse the hash keys. */
+	for (i -= t->asize; i <= t->hmask; i++) {
+		ktap_node_t *n = &t->node[i];
+		if (!is_nil(&n->val)) {
+			set_obj(key, &n->key);
+			set_obj(key + 1, &n->val);
+			tab_unlock(t);
+			return 1;
+		}
+	}
+	tab_unlock(t);
+	return 0;  /* End of traversal. */
+}
+
+/* -- Table length calculation -------------------------------------------- */
+
+int kp_tab_len(ktap_state_t *ks, ktap_tab_t *t)
+{
+	unsigned long flags;
+	int i, len = 0;
+
+	tab_lock(t);
+	for (i = 0; i < t->asize; i++) {
+		ktap_val_t *v = &t->array[i];
+
+		if (is_nil(v))
+			continue;
+		len++;
+	}
+
+	for (i = 0; i <= t->hmask; i++) {
+		ktap_node_t *n = &t->node[i];
+
+		if (is_nil(&n->key))
+			continue;
+
+		len++;
+	}
+	tab_unlock(t);
+	return len;
+}
+
+static void string_convert(char *output, const char *input)
+{
+	if (strlen(input) > 32) {
+		strncpy(output, input, 32-4);
+		memset(output + 32-4, '.', 3);
+	} else
+		memcpy(output, input, strlen(input));
+}
+
+typedef struct ktap_node2 {
+	ktap_val_t key;
+	ktap_val_t val;
+} ktap_node2_t;
+
+static int hist_record_cmp(const void *i, const void *j)
+{
+	ktap_number n1 = nvalue(&((const ktap_node2_t *)i)->val);
+	ktap_number n2 = nvalue(&((const ktap_node2_t *)j)->val);
+
+	if (n1 == n2)
+		return 0;
+	else if (n1 < n2)
+		return 1;
+	else
+		return -1;
+}
+
+/* todo: make histdump to be faster, just need to sort n entries, not all */
+
+/* print_hist: key should be number/string/ip, value must be number */
+static void tab_histdump(ktap_state_t *ks, ktap_tab_t *t, int shownums)
+{
+	long start_time, delta_time;
+	uint32_t i, asize = t->asize;
+	ktap_val_t *array = t->array;
+	uint32_t hmask = t->hmask;
+	ktap_node_t *node = t->node;
+	ktap_node2_t *sort_mem;
+	char dist_str[39];
+	int total = 0, sum = 0;
+
+	start_time = gettimeofday_ns();
+
+	sort_mem = kmalloc((t->asize + t->hnum) * sizeof(ktap_node2_t),
+				GFP_KERNEL);
+	if (!sort_mem)
+		return;
+
+	/* copy all values in table into sort_mem. */
+	for (i = 0; i < asize; i++) {
+		ktap_val_t *val = &array[i];
+		if (is_nil(val))
+			continue;
+
+		if (!is_number(val)) {
+			kp_error(ks, "print_hist only can print number\n");
+			goto out;
+		}
+
+		set_number(&sort_mem[total].key, i);
+		set_obj(&sort_mem[total].val, val);
+		sum += nvalue(val);
+		total++;
+	}
+
+	for (i = 0; i <= hmask; i++) {
+		ktap_node_t *n = &node[i];
+		ktap_val_t *val = &n->val;
+
+		if (is_nil(val))
+			continue;
+
+		if (!is_number(val)) {
+			kp_error(ks, "print_hist only can print number\n");
+			goto out;
+		}
+
+		set_obj(&sort_mem[total].key, &n->key);
+		set_obj(&sort_mem[total].val, val);
+		sum += nvalue(val);
+		total++;
+	}
+
+	/* sort */
+	sort(sort_mem, total, sizeof(ktap_node2_t), hist_record_cmp, NULL);
+
+	dist_str[sizeof(dist_str) - 1] = '\0';
+
+	for (i = 0; i < total; i++) {
+		ktap_val_t *key = &sort_mem[i].key;
+		ktap_number num = nvalue(&sort_mem[i].val);
+		int ratio;
+
+		if (!--shownums)
+			break;
+
+		memset(dist_str, ' ', sizeof(dist_str) - 1);
+		ratio = (num * (sizeof(dist_str) - 1)) / sum;
+		memset(dist_str, '@', ratio);
+
+		if (is_string(key)) {
+			//char buf[32] = {0};
+
+			//string_convert(buf, svalue(key));
+			if (rawtsvalue(key)->len > 32) {
+				kp_puts(ks, svalue(key));
+				kp_printf(ks, "%s\n%d\n\n", dist_str, num);
+			} else {
+				kp_printf(ks, "%31s |%s%-7d\n", svalue(key),
+								dist_str, num);
+			}
+		} else if (is_number(key)) {
+			kp_printf(ks, "%31d |%s%-7d\n", nvalue(key),
+						dist_str, num);
+		} else if (is_kip(key)) {
+			char str[KSYM_SYMBOL_LEN];
+			char buf[32] = {0};
+
+			SPRINT_SYMBOL(str, nvalue(key));
+			string_convert(buf, str);
+			kp_printf(ks, "%31s |%s%-7d\n", buf, dist_str, num);
+		}
+	}
+
+	if (!shownums && total)
+		kp_printf(ks, "%31s |\n", "...");
+
+ out:
+	kfree(sort_mem);
+
+	delta_time = (gettimeofday_ns() - start_time) / NSEC_PER_USEC;
+	kp_verbose_printf(ks, "tab_histdump time: %d (us)\n", delta_time);
+}
+
+#define DISTRIBUTION_STR "------------- Distribution -------------"
+void kp_tab_print_hist(ktap_state_t *ks, ktap_tab_t *t, int n)
+{
+	kp_printf(ks, "%31s%s%s\n", "value ", DISTRIBUTION_STR, " count");
+	tab_histdump(ks, t, n);
+}
diff --git a/kernel/trace/ktap/kp_tab.h b/kernel/trace/ktap/kp_tab.h
new file mode 100644
index 0000000..2a7576d
--- /dev/null
+++ b/kernel/trace/ktap/kp_tab.h
@@ -0,0 +1,59 @@
+#ifndef __KTAP_TAB_H__
+#define __KTAP_TAB_H__
+
+/* Hash constants. Tuned using a brute force search. */
+#define HASH_BIAS       (-0x04c11db7)
+#define HASH_ROT1       14
+#define HASH_ROT2       5
+#define HASH_ROT3       13
+
+/* Every half-decent C compiler transforms this into a rotate instruction. */
+#define kp_rol(x, n)    (((x)<<(n)) | ((x)>>(-(int)(n)&(8*sizeof(x)-1))))
+#define kp_ror(x, n)    (((x)<<(-(int)(n)&(8*sizeof(x)-1))) | ((x)>>(n)))
+
+/* Scramble the bits of numbers and pointers. */
+static __always_inline uint32_t hashrot(uint32_t lo, uint32_t hi)
+{
+	/* Prefer variant that compiles well for a 2-operand CPU. */
+	lo ^= hi; hi = kp_rol(hi, HASH_ROT1);
+	lo -= hi; hi = kp_rol(hi, HASH_ROT2);
+	hi ^= lo; hi -= kp_rol(lo, HASH_ROT3);
+	return hi;
+}
+
+
+#define FLS(x)       ((uint32_t)(__builtin_clz(x)^31))
+#define hsize2hbits(s)  ((s) ? ((s)==1 ? 1 : 1+FLS((uint32_t)((s)-1))) : 0)
+
+#define arrayslot(t, i)         (&(t)->array[(i)])
+
+void kp_tab_set(ktap_state_t *ks, ktap_tab_t *t, const ktap_val_t *key,
+		const ktap_val_t *val);
+void kp_tab_setstr(ktap_state_t *ks, ktap_tab_t *t,
+		   const ktap_str_t *key, const ktap_val_t *val);
+void kp_tab_incrstr(ktap_state_t *ks, ktap_tab_t *t, const ktap_str_t *key,
+		    ktap_number n);
+void kp_tab_get(ktap_state_t *ks, ktap_tab_t *t, const ktap_val_t *key,
+		ktap_val_t *val);
+void kp_tab_getstr(ktap_tab_t *t, ktap_str_t *key, ktap_val_t *val);
+
+void kp_tab_getint(ktap_tab_t *t, uint32_t key, ktap_val_t *val);
+void kp_tab_setint(ktap_state_t *ks, ktap_tab_t *t,
+		   uint32_t key, const ktap_val_t *val);
+void kp_tab_incrint(ktap_state_t *ks, ktap_tab_t *t, uint32_t key,
+		    ktap_number n);
+ktap_tab_t *kp_tab_new(ktap_state_t *ks, uint32_t asize, uint32_t hbits);
+ktap_tab_t *kp_tab_new_ah(ktap_state_t *ks, int32_t a, int32_t h);
+ktap_tab_t *kp_tab_dup(ktap_state_t *ks, const ktap_tab_t *kt);
+
+void kp_tab_free(ktap_state_t *ks, ktap_tab_t *t);
+int kp_tab_len(ktap_state_t *ks, ktap_tab_t *t);
+void kp_tab_dump(ktap_state_t *ks, ktap_tab_t *t);
+void kp_tab_clear(ktap_tab_t *t);
+void kp_tab_print_hist(ktap_state_t *ks, ktap_tab_t *t, int n);
+int kp_tab_next(ktap_state_t *ks, ktap_tab_t *t, StkId key);
+int kp_tab_sort_next(ktap_state_t *ks, ktap_tab_t *t, StkId key);
+void kp_tab_sort(ktap_state_t *ks, ktap_tab_t *t, ktap_func_t *cmp_func);
+void kp_tab_incr(ktap_state_t *ks, ktap_tab_t *t, ktap_val_t *key,
+		ktap_number n);
+#endif /* __KTAP_TAB_H__ */
diff --git a/kernel/trace/ktap/kp_transport.c b/kernel/trace/ktap/kp_transport.c
new file mode 100644
index 0000000..521aa76
--- /dev/null
+++ b/kernel/trace/ktap/kp_transport.c
@@ -0,0 +1,649 @@
+/*
+ * kp_transport.c - ktap transport functionality
+ *
+ * This file is part of ktap by Jovi Zhangwei.
+ *
+ * Copyright (C) 2012-2013 Jovi Zhangwei <jovi.zhangwei@gmail.com>.
+ *
+ * ktap is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * ktap is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+#include <linux/debugfs.h>
+#include <linux/ftrace_event.h>
+#include <linux/stacktrace.h>
+#include <linux/clocksource.h>
+#include <asm/uaccess.h>
+#include <linux/slab.h>
+#include <linux/module.h>
+#include <linux/kallsyms.h>
+#include <uapi/ktap/ktap_types.h>
+#include "ktap.h"
+#include "kp_events.h"
+#include "kp_transport.h"
+
+struct ktap_trace_iterator {
+	struct ring_buffer	*buffer;
+	int			print_timestamp;
+	void			*private;
+
+	struct trace_iterator	iter;
+};
+
+enum ktap_trace_type {
+	__TRACE_FIRST_TYPE = 0,
+
+	TRACE_FN = 1, /* must be same as ftrace definition in kernel */
+	TRACE_PRINT,
+	TRACE_BPUTS,
+	TRACE_STACK,
+	TRACE_USER_STACK,
+
+	__TRACE_LAST_TYPE,
+};
+
+#define KTAP_TRACE_ITER(iter)	\
+	container_of(iter, struct ktap_trace_iterator, iter)
+
+static
+ssize_t _trace_seq_to_user(struct trace_seq *s, char __user *ubuf, size_t cnt)
+{
+	int len;
+	int ret;
+
+	if (!cnt)
+		return 0;
+
+	if (s->len <= s->readpos)
+		return -EBUSY;
+
+	len = s->len - s->readpos;
+	if (cnt > len)
+		cnt = len;
+	ret = copy_to_user(ubuf, s->buffer + s->readpos, cnt);
+	if (ret == cnt)
+		return -EFAULT;
+
+	cnt -= ret;
+
+	s->readpos += cnt;
+	return cnt;
+}
+
+int _trace_seq_puts(struct trace_seq *s, const char *str)
+{
+	int len = strlen(str);
+
+	if (s->full)
+		return 0;
+
+	if (len > ((PAGE_SIZE - 1) - s->len)) {
+		s->full = 1;
+		return 0;
+	}
+
+	memcpy(s->buffer + s->len, str, len);
+	s->len += len;
+
+	return len;
+}
+
+static int trace_empty(struct trace_iterator *iter)
+{
+	struct ktap_trace_iterator *ktap_iter = KTAP_TRACE_ITER(iter);
+	int cpu;
+
+	for_each_online_cpu(cpu) {
+		if (!ring_buffer_empty_cpu(ktap_iter->buffer, cpu))
+			return 0;
+	}
+
+	return 1;
+}
+
+static void trace_consume(struct trace_iterator *iter)
+{
+	struct ktap_trace_iterator *ktap_iter = KTAP_TRACE_ITER(iter);
+
+	ring_buffer_consume(ktap_iter->buffer, iter->cpu, &iter->ts,
+			    &iter->lost_events);
+}
+
+unsigned long long ns2usecs(cycle_t nsec)
+{
+	nsec += 500;
+	do_div(nsec, 1000);
+	return nsec;
+}
+
+static int trace_print_timestamp(struct trace_iterator *iter)
+{
+	struct trace_seq *s = &iter->seq;
+	unsigned long long t;
+	unsigned long secs, usec_rem;
+
+	t = ns2usecs(iter->ts);
+	usec_rem = do_div(t, USEC_PER_SEC);
+	secs = (unsigned long)t;
+
+	return trace_seq_printf(s, "%5lu.%06lu: ", secs, usec_rem);
+}
+
+/* todo: export kernel function ftrace_find_event in future, and make faster */
+static struct trace_event *(*ftrace_find_event)(int type);
+
+static enum print_line_t print_trace_fmt(struct trace_iterator *iter)
+{
+	struct ktap_trace_iterator *ktap_iter = KTAP_TRACE_ITER(iter);
+	struct trace_entry *entry = iter->ent;
+	struct trace_event *ev;
+
+	ev = ftrace_find_event(entry->type);
+
+	if (ktap_iter->print_timestamp && !trace_print_timestamp(iter))
+		return TRACE_TYPE_PARTIAL_LINE;
+
+	if (ev) {
+		int ret = ev->funcs->trace(iter, 0, ev);
+
+		/* overwrite '\n' at the ending */
+		iter->seq.buffer[iter->seq.len - 1] = '\0';
+		iter->seq.len--;
+		return ret;
+	}
+
+	return TRACE_TYPE_PARTIAL_LINE;
+}
+
+static enum print_line_t print_trace_stack(struct trace_iterator *iter)
+{
+	struct trace_entry *entry = iter->ent;
+	struct stack_trace trace;
+	char str[KSYM_SYMBOL_LEN];
+	int i;
+
+	trace.entries = (unsigned long *)(entry + 1);
+	trace.nr_entries = (iter->ent_size - sizeof(*entry)) /
+			   sizeof(unsigned long);
+
+	if (!_trace_seq_puts(&iter->seq, "<stack trace>\n"))
+		return TRACE_TYPE_PARTIAL_LINE;
+
+	for (i = 0; i < trace.nr_entries; i++) {
+		unsigned long p = trace.entries[i];
+
+		if (p == ULONG_MAX)
+			break;
+
+		sprint_symbol(str, p);
+		if (!trace_seq_printf(&iter->seq, " => %s\n", str))
+			return TRACE_TYPE_PARTIAL_LINE;
+	}
+
+	return TRACE_TYPE_HANDLED;
+}
+
+struct ktap_ftrace_entry {
+	struct trace_entry entry;
+	unsigned long ip;
+	unsigned long parent_ip;
+};
+
+static enum print_line_t print_trace_fn(struct trace_iterator *iter)
+{
+	struct ktap_trace_iterator *ktap_iter = KTAP_TRACE_ITER(iter);
+	struct ktap_ftrace_entry *field = (struct ktap_ftrace_entry *)iter->ent;
+	char str[KSYM_SYMBOL_LEN];
+
+	if (ktap_iter->print_timestamp && !trace_print_timestamp(iter))
+		return TRACE_TYPE_PARTIAL_LINE;
+
+	sprint_symbol(str, field->ip);
+	if (!_trace_seq_puts(&iter->seq, str))
+		return TRACE_TYPE_PARTIAL_LINE;
+
+	if (!_trace_seq_puts(&iter->seq, " <- "))
+		return TRACE_TYPE_PARTIAL_LINE;
+
+	sprint_symbol(str, field->parent_ip);
+	if (!_trace_seq_puts(&iter->seq, str))
+		return TRACE_TYPE_PARTIAL_LINE;
+
+	return TRACE_TYPE_HANDLED;
+}
+
+static enum print_line_t print_trace_bputs(struct trace_iterator *iter)
+{
+	if (!_trace_seq_puts(&iter->seq,
+			    (const char *)(*(unsigned long *)(iter->ent + 1))))
+		return TRACE_TYPE_PARTIAL_LINE;
+
+	return TRACE_TYPE_HANDLED;
+}
+
+static enum print_line_t print_trace_line(struct trace_iterator *iter)
+{
+	struct trace_entry *entry = iter->ent;
+	char *str = (char *)(entry + 1);
+
+	if (entry->type == TRACE_PRINT) {
+		if (!trace_seq_printf(&iter->seq, "%s", str))
+			return TRACE_TYPE_PARTIAL_LINE;
+
+		return TRACE_TYPE_HANDLED;
+	}
+
+	if (entry->type == TRACE_BPUTS)
+		return print_trace_bputs(iter);
+
+	if (entry->type == TRACE_STACK)
+		return print_trace_stack(iter);
+
+	if (entry->type == TRACE_FN)
+		return print_trace_fn(iter);
+
+	return print_trace_fmt(iter);
+}
+
+static struct trace_entry *
+peek_next_entry(struct trace_iterator *iter, int cpu, u64 *ts,
+		unsigned long *lost_events)
+{
+	struct ktap_trace_iterator *ktap_iter = KTAP_TRACE_ITER(iter);
+	struct ring_buffer_event *event;
+
+	event = ring_buffer_peek(ktap_iter->buffer, cpu, ts, lost_events);
+	if (event) {
+		iter->ent_size = ring_buffer_event_length(event);
+		return ring_buffer_event_data(event);
+	}
+
+	return NULL;
+}
+
+static struct trace_entry *
+__find_next_entry(struct trace_iterator *iter, int *ent_cpu,
+		  unsigned long *missing_events, u64 *ent_ts)
+{
+	struct ktap_trace_iterator *ktap_iter = KTAP_TRACE_ITER(iter);
+	struct ring_buffer *buffer = ktap_iter->buffer;
+	struct trace_entry *ent, *next = NULL;
+	unsigned long lost_events = 0, next_lost = 0;
+	u64 next_ts = 0, ts;
+	int next_cpu = -1;
+	int next_size = 0;
+	int cpu;
+
+	for_each_online_cpu(cpu) {
+		if (ring_buffer_empty_cpu(buffer, cpu))
+			continue;
+
+		ent = peek_next_entry(iter, cpu, &ts, &lost_events);
+		/*
+		 * Pick the entry with the smallest timestamp:
+		 */
+		if (ent && (!next || ts < next_ts)) {
+			next = ent;
+			next_cpu = cpu;
+			next_ts = ts;
+			next_lost = lost_events;
+			next_size = iter->ent_size;
+		}
+	}
+
+	iter->ent_size = next_size;
+
+	if (ent_cpu)
+		*ent_cpu = next_cpu;
+
+	if (ent_ts)
+		*ent_ts = next_ts;
+
+	if (missing_events)
+		*missing_events = next_lost;
+
+	return next;
+}
+
+/* Find the next real entry, and increment the iterator to the next entry */
+static void *trace_find_next_entry_inc(struct trace_iterator *iter)
+{
+	iter->ent = __find_next_entry(iter, &iter->cpu,
+				      &iter->lost_events, &iter->ts);
+	if (iter->ent)
+		iter->idx++;
+
+	return iter->ent ? iter : NULL;
+}
+
+static void poll_wait_pipe(void)
+{
+	set_current_state(TASK_INTERRUPTIBLE);
+	/* sleep for 100 msecs, and try again. */
+	schedule_timeout(HZ / 10);
+}
+
+static int tracing_wait_pipe(struct file *filp)
+{
+	struct trace_iterator *iter = filp->private_data;
+	struct ktap_trace_iterator *ktap_iter = KTAP_TRACE_ITER(iter);
+	ktap_state_t *ks = ktap_iter->private;
+
+	while (trace_empty(iter)) {
+
+		if ((filp->f_flags & O_NONBLOCK)) {
+			return -EAGAIN;
+		}
+
+		mutex_unlock(&iter->mutex);
+
+		poll_wait_pipe();
+
+		mutex_lock(&iter->mutex);
+
+		if (G(ks)->wait_user && trace_empty(iter))
+			return -EINTR;
+	}
+
+	return 1;
+}
+
+static ssize_t
+tracing_read_pipe(struct file *filp, char __user *ubuf, size_t cnt,
+		  loff_t *ppos)
+{
+	struct trace_iterator *iter = filp->private_data;
+	ssize_t sret;
+
+	/* return any leftover data */
+	sret = _trace_seq_to_user(&iter->seq, ubuf, cnt);
+	if (sret != -EBUSY)
+		return sret;
+	/*
+	 * Avoid more than one consumer on a single file descriptor
+	 * This is just a matter of traces coherency, the ring buffer itself
+	 * is protected.
+	 */
+	mutex_lock(&iter->mutex);
+
+waitagain:
+	sret = tracing_wait_pipe(filp);
+	if (sret <= 0)
+		goto out;
+
+	/* stop when tracing is finished */
+	if (trace_empty(iter)) {
+		sret = 0;
+		goto out;
+	}
+
+	if (cnt >= PAGE_SIZE)
+		cnt = PAGE_SIZE - 1;
+
+	/* reset all but tr, trace, and overruns */
+	memset(&iter->seq, 0,
+	       sizeof(struct trace_iterator) -
+	       offsetof(struct trace_iterator, seq));
+	iter->pos = -1;
+
+	while (trace_find_next_entry_inc(iter) != NULL) {
+		enum print_line_t ret;
+		int len = iter->seq.len;
+
+		ret = print_trace_line(iter);
+		if (ret == TRACE_TYPE_PARTIAL_LINE) {
+			/* don't print partial lines */
+			iter->seq.len = len;
+			break;
+		}
+		if (ret != TRACE_TYPE_NO_CONSUME)
+			trace_consume(iter);
+
+		if (iter->seq.len >= cnt)
+			break;
+
+		/*
+		 * Setting the full flag means we reached the trace_seq buffer
+		 * size and we should leave by partial output condition above.
+		 * One of the trace_seq_* functions is not used properly.
+		 */
+		WARN_ONCE(iter->seq.full, "full flag set for trace type %d",
+			  iter->ent->type);
+	}
+
+	/* Now copy what we have to the user */
+	sret = _trace_seq_to_user(&iter->seq, ubuf, cnt);
+	if (iter->seq.readpos >= iter->seq.len)
+		trace_seq_init(&iter->seq);
+
+	/*
+	 * If there was nothing to send to user, in spite of consuming trace
+	 * entries, go back to wait for more entries.
+	 */
+	if (sret == -EBUSY)
+		goto waitagain;
+
+out:
+	mutex_unlock(&iter->mutex);
+
+	return sret;
+}
+
+static int tracing_open_pipe(struct inode *inode, struct file *filp)
+{
+	struct ktap_trace_iterator *ktap_iter;
+	ktap_state_t *ks = inode->i_private;
+
+	/* create a buffer to store the information to pass to userspace */
+	ktap_iter = kzalloc(sizeof(*ktap_iter), GFP_KERNEL);
+	if (!ktap_iter)
+		return -ENOMEM;
+
+	ktap_iter->private = ks;
+	ktap_iter->buffer = G(ks)->buffer;
+	ktap_iter->print_timestamp = G(ks)->parm->print_timestamp;
+	mutex_init(&ktap_iter->iter.mutex);
+	filp->private_data = &ktap_iter->iter;
+
+	nonseekable_open(inode, filp);
+
+	return 0;
+}
+
+static int tracing_release_pipe(struct inode *inode, struct file *file)
+{
+	struct trace_iterator *iter = file->private_data;
+	struct ktap_trace_iterator *ktap_iter = KTAP_TRACE_ITER(iter);
+
+	mutex_destroy(&iter->mutex);
+	kfree(ktap_iter);
+	return 0;
+}
+
+static const struct file_operations tracing_pipe_fops = {
+	.open		= tracing_open_pipe,
+	.read		= tracing_read_pipe,
+	.splice_read	= NULL,
+	.release	= tracing_release_pipe,
+	.llseek		= no_llseek,
+};
+
+/*
+ * preempt disabled in ring_buffer_lock_reserve
+ *
+ * The implementation is similar with funtion __ftrace_trace_stack.
+ */
+void kp_transport_print_kstack(ktap_state_t *ks, uint16_t depth, uint16_t skip)
+{
+	struct ring_buffer *buffer = G(ks)->buffer;
+	struct ring_buffer_event *event;
+	struct trace_entry *entry;
+	int size;
+
+	size = depth * sizeof(unsigned long);
+	event = ring_buffer_lock_reserve(buffer, sizeof(*entry) + size);
+	if (!event) {
+		KTAP_STATS(ks)->events_missed += 1;
+		return;
+	} else {
+		struct stack_trace trace;
+
+		entry = ring_buffer_event_data(event);
+		tracing_generic_entry_update(entry, 0, 0);
+		entry->type = TRACE_STACK;
+
+		trace.nr_entries = 0;
+		trace.skip = skip;
+		trace.max_entries = depth;
+		trace.entries = (unsigned long *)(entry + 1);
+		save_stack_trace(&trace);
+
+		ring_buffer_unlock_commit(buffer, event);
+	}
+}
+
+void kp_transport_event_write(ktap_state_t *ks, struct ktap_event_data *e)
+{
+	struct ring_buffer *buffer = G(ks)->buffer;
+	struct ring_buffer_event *event;
+	struct trace_entry *ev_entry = e->data->raw->data;
+	struct trace_entry *entry;
+	int entry_size = e->data->raw->size;
+
+	event = ring_buffer_lock_reserve(buffer, entry_size +
+					 sizeof(struct ftrace_event_call *));
+	if (!event) {
+		KTAP_STATS(ks)->events_missed += 1;
+		return;
+	} else {
+		entry = ring_buffer_event_data(event);
+
+		memcpy(entry, ev_entry, entry_size);
+
+		ring_buffer_unlock_commit(buffer, event);
+	}
+}
+
+void kp_transport_write(ktap_state_t *ks, const void *data, size_t length)
+{
+	struct ring_buffer *buffer = G(ks)->buffer;
+	struct ring_buffer_event *event;
+	struct trace_entry *entry;
+	int size;
+
+	size = sizeof(struct trace_entry) + length;
+
+	event = ring_buffer_lock_reserve(buffer, size);
+	if (!event) {
+		KTAP_STATS(ks)->events_missed += 1;
+		return;
+	} else {
+		entry = ring_buffer_event_data(event);
+
+		tracing_generic_entry_update(entry, 0, 0);
+		entry->type = TRACE_PRINT;
+		memcpy(entry + 1, data, length);
+
+		ring_buffer_unlock_commit(buffer, event);
+	}
+}
+
+/* general print function */
+void kp_printf(ktap_state_t *ks, const char *fmt, ...)
+{
+	char buff[1024];
+	va_list args;
+	int len;
+
+	va_start(args, fmt);
+	len = vscnprintf(buff, 1024, fmt, args);
+	va_end(args);
+
+	buff[len] = '\0';
+	kp_transport_write(ks, buff, len + 1);
+}
+
+void __kp_puts(ktap_state_t *ks, const char *str)
+{
+	kp_transport_write(ks, str, strlen(str) + 1);
+}
+
+void __kp_bputs(ktap_state_t *ks, const char *str)
+{
+	struct ring_buffer *buffer = G(ks)->buffer;
+	struct ring_buffer_event *event;
+	struct trace_entry *entry;
+	int size;
+
+	size = sizeof(struct trace_entry) + sizeof(unsigned long *);
+
+	event = ring_buffer_lock_reserve(buffer, size);
+	if (!event) {
+		KTAP_STATS(ks)->events_missed += 1;
+		return;
+	} else {
+		entry = ring_buffer_event_data(event);
+
+		tracing_generic_entry_update(entry, 0, 0);
+		entry->type = TRACE_BPUTS;
+		*(unsigned long *)(entry + 1) = (unsigned long)str;
+
+		ring_buffer_unlock_commit(buffer, event);
+	}
+}
+
+void kp_transport_exit(ktap_state_t *ks)
+{
+	if (G(ks)->buffer)
+		ring_buffer_free(G(ks)->buffer);
+	debugfs_remove(G(ks)->trace_pipe_dentry);
+}
+
+#define TRACE_BUF_SIZE_DEFAULT	1441792UL /* 16384 * 88 (sizeof(entry)) */
+
+int kp_transport_init(ktap_state_t *ks, struct dentry *dir)
+{
+	struct ring_buffer *buffer;
+	struct dentry *dentry;
+	char filename[32] = {0};
+
+#ifdef CONFIG_PPC64
+	ftrace_find_event = (void *)kallsyms_lookup_name(".ftrace_find_event");
+#else
+	ftrace_find_event = (void *)kallsyms_lookup_name("ftrace_find_event");
+#endif
+	if (!ftrace_find_event) {
+		printk("ktap: cannot lookup ftrace_find_event in kallsyms\n");
+		return -EINVAL;
+	}
+
+	buffer = ring_buffer_alloc(TRACE_BUF_SIZE_DEFAULT, RB_FL_OVERWRITE);
+	if (!buffer)
+		return -ENOMEM;
+
+	sprintf(filename, "trace_pipe_%d", (int)task_tgid_vnr(current));
+
+	dentry = debugfs_create_file(filename, 0444, dir,
+				     ks, &tracing_pipe_fops);
+	if (!dentry) {
+		pr_err("ktapvm: cannot create trace_pipe file in debugfs\n");
+		ring_buffer_free(buffer);
+		return -1;
+	}
+
+	G(ks)->buffer = buffer;
+	G(ks)->trace_pipe_dentry = dentry;
+
+	return 0;
+}
+
diff --git a/kernel/trace/ktap/kp_transport.h b/kernel/trace/ktap/kp_transport.h
new file mode 100644
index 0000000..bc7c892
--- /dev/null
+++ b/kernel/trace/ktap/kp_transport.h
@@ -0,0 +1,13 @@
+#ifndef __KTAP_TRANSPORT_H__
+#define __KTAP_TRANSPORT_H__
+
+void kp_transport_write(ktap_state_t *ks, const void *data, size_t length);
+void kp_transport_event_write(ktap_state_t *ks, struct ktap_event_data *e);
+void kp_transport_print_kstack(ktap_state_t *ks, uint16_t depth, uint16_t skip);
+void *kp_transport_reserve(ktap_state_t *ks, size_t length);
+void kp_transport_exit(ktap_state_t *ks);
+int kp_transport_init(ktap_state_t *ks, struct dentry *dir);
+
+int _trace_seq_puts(struct trace_seq *s, const char *str);
+
+#endif /* __KTAP_TRANSPORT_H__ */
diff --git a/kernel/trace/ktap/kp_vm.c b/kernel/trace/ktap/kp_vm.c
new file mode 100644
index 0000000..4dc8554
--- /dev/null
+++ b/kernel/trace/ktap/kp_vm.c
@@ -0,0 +1,1767 @@
+/*
+ * kp_vm.c - ktap script virtual machine in Linux kernel
+ *
+ * This file is part of ktap by Jovi Zhangwei.
+ *
+ * Copyright (C) 2012-2013 Jovi Zhangwei <jovi.zhangwei@gmail.com>.
+ *
+ * Adapted from luajit and lua interpreter.
+ * Copyright (C) 2005-2014 Mike Pall.
+ * Copyright (C) 1994-2008 Lua.org, PUC-Rio.
+ *
+ * ktap is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * ktap is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+#include <linux/slab.h>
+#include <linux/ftrace_event.h>
+#include <linux/signal.h>
+#include <linux/sched.h>
+#include <linux/uaccess.h>
+#include <uapi/ktap/ktap_types.h>
+#include <uapi/ktap/ktap_bc.h>
+#include <uapi/ktap/ktap_ffi.h>
+#include "ktap.h"
+#include "kp_obj.h"
+#include "kp_str.h"
+#include "kp_mempool.h"
+#include "kp_tab.h"
+#include "kp_transport.h"
+#include "kp_vm.h"
+#include "kp_events.h"
+
+#define KTAP_MIN_RESERVED_STACK_SIZE 20
+#define KTAP_STACK_SIZE		120 /* enlarge this value for big stack */
+#define KTAP_STACK_SIZE_BYTES	(KTAP_STACK_SIZE * sizeof(ktap_val_t))
+
+#define KTAP_PERCPU_BUFFER_SIZE	(3 * PAGE_SIZE)
+
+static ktap_cfunction gfunc_get(ktap_state_t *ks, int idx);
+static int gfunc_getidx(ktap_global_state_t *g, ktap_cfunction cfunc);
+
+static ktap_str_t *str_concat(ktap_state_t *ks, StkId top, int start, int end)
+{
+	int i, len = 0;
+	ktap_str_t *ts;
+	char *ptr, *buffer;
+
+	for (i = start; i <= end; i++) {
+		if (!is_string(top + i)) {
+			kp_error(ks, "cannot concat non-string\n");
+			return NULL;
+		}
+
+		len += rawtsvalue(top + i)->len;
+	}
+
+	if (len >= KTAP_PERCPU_BUFFER_SIZE) {
+		kp_error(ks, "Error: too long string concatenation\n");
+		return NULL;
+	}
+
+	preempt_disable_notrace();
+
+	buffer = kp_this_cpu_print_buffer(ks);
+	ptr = buffer;
+
+	for (i = start; i <= end; i++) {
+		int len = rawtsvalue(top + i)->len;
+		strncpy(ptr, svalue(top + i), len);
+		ptr += len;
+	}
+	ts = kp_str_new(ks, buffer, len);
+
+	preempt_enable_notrace();
+
+	return ts;
+}
+
+static ktap_upval_t *findupval(ktap_state_t *ks, StkId slot)
+{
+	ktap_global_state_t *g = G(ks);
+	ktap_upval_t **pp = &ks->openupval;
+	ktap_upval_t *p;
+	ktap_upval_t *uv;
+
+	while (*pp != NULL && (p = *pp)->v >= slot) {
+		if (p->v == slot) {  /* found a corresponding upvalue? */
+			return p;
+		}
+		pp = (ktap_upval_t **)&p->nextgc;
+	}
+
+	/* not found: create a new one */
+	uv = (ktap_upval_t *)kp_malloc(ks, sizeof(ktap_upval_t));
+	if (!uv)
+		return NULL;
+	uv->gct = ~KTAP_TUPVAL;
+	uv->closed = 0; /* still open */
+	uv->v = slot;  /* current value lives in the stack */
+	/* Insert into sorted list of open upvalues. */
+	uv->nextgc = (ktap_obj_t *)*pp;
+	*pp = uv;
+	uv->prev = &g->uvhead;  /* double link it in `uvhead' list */
+	uv->next = g->uvhead.next;
+	uv->next->prev = uv;
+	g->uvhead.next = uv;
+	return uv;
+}
+
+static void unlinkupval(ktap_upval_t *uv)
+{
+	uv->next->prev = uv->prev;  /* remove from `uvhead' list */
+	uv->prev->next = uv->next;
+}
+
+void kp_freeupval(ktap_state_t *ks, ktap_upval_t *uv)
+{
+	if (!uv->closed)  /* is it open? */
+		unlinkupval(uv);  /* remove from open list */
+	kp_free(ks, uv);  /* free upvalue */
+}
+
+/* close upvals */
+static void func_closeuv(ktap_state_t *ks, StkId level)
+{
+	ktap_upval_t *uv;
+	ktap_global_state_t *g = G(ks);
+	while (ks->openupval != NULL &&
+		(uv = ks->openupval)->v >= level) {
+		ktap_obj_t *o = obj2gco(uv);
+		/* remove from `open' list */
+		ks->openupval = (ktap_upval_t *)uv->nextgc;
+		unlinkupval(uv);  /* remove upvalue from 'uvhead' list */
+		set_obj(&uv->tv, uv->v);  /* move value to upvalue slot */
+		uv->v = &uv->tv;  /* now current value lives here */
+		uv->closed = 1;
+		gch(o)->nextgc = g->allgc; /* link upvalue into 'allgc' list */
+		g->allgc = o;
+	}
+}
+
+#define SIZE_KTAP_FUNC(n) (sizeof(ktap_func_t) - sizeof(ktap_obj_t *) + \
+			   sizeof(ktap_obj_t *) * (n))
+static ktap_func_t *func_new_empty(ktap_state_t *ks, ktap_proto_t *pt)
+{
+	ktap_func_t *fn;
+
+	/* only mainthread can create new function */
+	if (ks != G(ks)->mainthread) {
+		kp_error(ks, "only mainthread can create function\n");
+		return NULL;
+	}
+
+	fn = (ktap_func_t *)kp_obj_new(ks, SIZE_KTAP_FUNC(pt->sizeuv));
+	if (!fn)
+		return NULL;
+	fn->gct = ~KTAP_TFUNC;
+	fn->nupvalues = 0; /* Set to zero until upvalues are initialized. */
+	fn->pc = proto_bc(pt);
+	fn->p = pt;
+
+	return fn;
+}
+
+static ktap_func_t *func_new(ktap_state_t *ks, ktap_proto_t *pt,
+			     ktap_func_t *parent, ktap_val_t *base)
+{
+	ktap_func_t *fn;
+	int nuv = pt->sizeuv, i;
+
+	fn = func_new_empty(ks, pt);
+	if (!fn)
+		return NULL;
+
+	fn->nupvalues = nuv;
+	for (i = 0; i < nuv; i++) {
+		uint32_t v = proto_uv(pt)[i];
+		ktap_upval_t *uv;
+
+		if (v & PROTO_UV_LOCAL) {
+			uv = findupval(ks, base + (v & 0xff));
+			if (!uv)
+				return NULL;
+			uv->immutable = ((v /PROTO_UV_IMMUTABLE) & 1);
+		} else {
+			uv = parent->upvals[v];
+		}
+		fn->upvals[i] = uv;
+	}
+	return fn;
+}
+
+static inline int checkstack(ktap_state_t *ks, int n)
+{
+	if (unlikely(ks->stack_last - ks->top <= n)) {
+		kp_error(ks, "stack overflow, please enlarge stack size\n");
+		return -1;
+	}
+	return 0;
+}
+
+static StkId adjust_varargs(ktap_state_t *ks, ktap_proto_t *p, int actual)
+{
+	int i;
+	int nfixargs = p->numparams;
+	StkId base, fixed;
+
+	/* move fixed parameters to final position */
+	fixed = ks->top - actual;  /* first fixed argument */
+	base = ks->top;  /* final position of first argument */
+
+	for (i=0; i < nfixargs; i++) {
+		set_obj(ks->top++, fixed + i);
+		set_nil(fixed + i);
+	}
+
+	return base;
+}
+
+static void poscall(ktap_state_t *ks, StkId func, StkId first_result,
+		   int wanted)
+{
+	int i;
+
+	for (i = wanted; i != 0 && first_result < ks->top; i--)
+		set_obj(func++, first_result++);
+
+	while(i-- > 0)
+		set_nil(func++);
+}
+
+void kp_vm_call_proto(ktap_state_t *ks, ktap_proto_t *pt)
+{
+	ktap_func_t *fn;
+
+	fn = func_new_empty(ks, pt);
+	if (!fn)
+		return;
+	set_func(ks->top++, fn);
+	kp_vm_call(ks, ks->top - 1, 0);
+}
+
+/*
+ * Hot loop detaction
+ *
+ * Check hot loop detaction in three cases:
+ * 1. jmp -x: this happens in 'while (expr) { ... }'
+ * 2. FORPREP-FORLOOP
+ * 3. TFORCALL-TFORLOOP
+ */ 
+static __always_inline int check_hot_loop(ktap_state_t *ks, int loop_count)
+{
+	if (unlikely(loop_count == kp_max_loop_count)) {
+		kp_error(ks, "loop execute count exceed max limit(%d)\n",
+			     kp_max_loop_count);
+		return -1;
+	}
+
+	return 0;
+}
+
+#define dojump(i, e) { pc += (int)bc_d(i) - BCBIAS_J + e; }
+#define donextjump  { instr = *pc; dojump(instr, 1); }
+
+#define NUMADD(a, b)    ((a) + (b))
+#define NUMSUB(a, b)    ((a) - (b))
+#define NUMMUL(a, b)    ((a) * (b))
+#define NUMDIV(a, b)    ((a) / (b))
+#define NUMUNM(a)       (-(a))
+#define NUMEQ(a, b)     ((a) == (b))
+#define NUMLT(a, b)     ((a) < (b))
+#define NUMLE(a, b)     ((a) <= (b))
+#define NUMMOD(a, b)    ((a) % (b))
+
+#define arith_VV(ks, op) { \
+	ktap_val_t *rb = RB; \
+	ktap_val_t *rc = RC; \
+	if (is_number(rb) && is_number(rc)) { \
+		ktap_number nb = nvalue(rb), nc = nvalue(rc); \
+		set_number(RA, op(nb, nc)); \
+	} else {	\
+		kp_puts(ks, "Error: Cannot make arith operation\n");	\
+		return;	\
+	} }
+
+#define arith_VN(ks, op) { \
+	ktap_val_t *rb = RB; \
+	if (is_number(rb)) { \
+		ktap_number nb = nvalue(rb);\
+		ktap_number nc = nvalue((ktap_val_t *)kbase + bc_c(instr));\
+		set_number(RA, op(nb, nc)); \
+	} else {	\
+		kp_puts(ks, "Error: Cannot make arith operation\n");	\
+		return;	\
+	} }
+
+#define arith_NV(ks, op) { \
+	ktap_val_t *rb = RB; \
+	if (is_number(rb)) { \
+		ktap_number nb = nvalue(rb);\
+		ktap_number nc = nvalue((ktap_val_t *)kbase + bc_c(instr));\
+		set_number(RA, op(nc, nb)); \
+	} else {	\
+		kp_puts(ks, "Error: Cannot make arith operation\n");	\
+		return;	\
+	} }
+
+
+static const char * const bc_names[] = {
+#define BCNAME(name, ma, mb, mc, mt)       #name,
+	BCDEF(BCNAME)
+#undef BCNAME
+	NULL
+};
+
+
+/*
+ * ktap bytecode interpreter routine
+ *
+ *
+ * kp_vm_call only can be used for:
+ * 1). call ktap function, not light C function
+ * 2). accept fixed argument function
+ */
+void kp_vm_call(ktap_state_t *ks, StkId func, int nresults)
+{
+	int loop_count = 0;
+	ktap_func_t *fn;
+	ktap_proto_t *pt;
+	ktap_obj_t **kbase;
+	unsigned int instr, op;
+	const unsigned int *pc;
+	StkId base; /* stack pointer */
+	int multres = 0; /* temp varible */
+	ktap_tab_t *gtab = G(ks)->gtab;
+
+	/* use computed goto for opcode dispatch */
+
+	static void *dispatch_table[] = {
+#define BCNAME(name, ma, mb, mc, mt)       &&DO_BC_##name,
+		BCDEF(BCNAME)
+#undef BCNAME
+	};
+
+#define DISPATCH()				\
+	do {					\
+		instr = *(pc++);		\
+		op = bc_op(instr);		\
+		goto *dispatch_table[op];	\
+	} while (0)
+
+#define RA	(base + bc_a(instr))
+#define RB	(base + bc_b(instr))
+#define RC	(base + bc_c(instr))
+#define RD	(base + bc_d(instr))
+#define RKD	((ktap_val_t *)kbase + bc_d(instr))
+
+	/*TODO: fix argument number mismatch, example: sort cmp closure */
+
+	fn = clvalue(func);
+	pt = fn->p;
+	kbase = fn->p->k;
+	base = func + 1;
+	pc = proto_bc(pt) + 1;
+	ks->top = base + pt->framesize;
+	func->pcr = 0; /* no previous frame */
+
+	/* main loop of interpreter */
+	DISPATCH();
+
+	while (1) {
+	DO_BC_ISLT: /* Jump if A < D */
+		if (!is_number(RA) || !is_number(RD)) {
+			kp_error(ks, "compare with non-number\n");
+			return;
+		}
+
+		if (nvalue(RA) >= nvalue(RD))
+			pc++;
+		else
+			donextjump;
+		DISPATCH();
+	DO_BC_ISGE: /* Jump if A >= D */
+		if (!is_number(RA) || !is_number(RD)) {
+			kp_error(ks, "compare with non-number\n");
+			return;
+		}
+
+		if (nvalue(RA) < nvalue(RD))
+			pc++;
+		else
+			donextjump;
+		DISPATCH();
+	DO_BC_ISLE: /* Jump if A <= D */
+		if (!is_number(RA) || !is_number(RD)) {
+			kp_error(ks, "compare with non-number\n");
+			return;
+		}
+
+		if (nvalue(RA) > nvalue(RD))
+			pc++;
+		else
+			donextjump;
+		DISPATCH();
+	DO_BC_ISGT: /* Jump if A > D */
+		if (!is_number(RA) || !is_number(RD)) {
+			kp_error(ks, "compare with non-number\n");
+			return;
+		}
+
+		if (nvalue(RA) <= nvalue(RD))
+			pc++;
+		else
+			donextjump;
+		DISPATCH();
+	DO_BC_ISEQV: /* Jump if A = D */
+		if (!kp_obj_equal(RA, RD))
+			pc++;
+		else
+			donextjump;
+		DISPATCH();
+	DO_BC_ISNEV: /* Jump if A != D */
+		if (kp_obj_equal(RA, RD))
+			pc++;
+		else
+			donextjump;
+		DISPATCH();
+	DO_BC_ISEQS: { /* Jump if A = D */
+		int idx = ~bc_d(instr);
+
+		if (!is_string(RA) ||
+				rawtsvalue(RA) != (ktap_str_t *)kbase[idx])
+			pc++;
+		else
+			donextjump;
+		DISPATCH();
+		}
+	DO_BC_ISNES: { /* Jump if A != D */
+		int idx = ~bc_d(instr);
+
+		if (is_string(RA) &&
+			rawtsvalue(RA) == (ktap_str_t *)kbase[idx])
+			pc++;
+		else
+			donextjump;
+		DISPATCH();
+		}
+	DO_BC_ISEQN: /* Jump if A = D */
+		if (!is_number(RA) || nvalue(RA) !=  nvalue(RKD))
+			pc++;
+		else
+			donextjump;
+		DISPATCH();
+	DO_BC_ISNEN: /* Jump if A != D */
+		if (is_number(RA) && nvalue(RA) ==  nvalue(RKD))
+			pc++;
+		else
+			donextjump;
+		DISPATCH();
+	DO_BC_ISEQP: /* Jump if A = D */
+		if (itype(RA) != ~bc_d(instr))
+			pc++;
+		else
+			donextjump;
+		DISPATCH();
+	DO_BC_ISNEP: /* Jump if A != D */
+		if (itype(RA) == ~bc_d(instr))
+			pc++;
+		else
+			donextjump;
+		DISPATCH();
+	DO_BC_ISTC: /* Copy D to A and jump, if D is true */
+		if (itype(RD) == KTAP_TNIL || itype(RD) == KTAP_TFALSE)
+			pc++;
+		else {
+			set_obj(RA, RD);
+			donextjump;
+		}
+		DISPATCH();
+	DO_BC_ISFC: /* Copy D to A and jump, if D is false */
+		if (itype(RD) != KTAP_TNIL && itype(RD) != KTAP_TFALSE)
+			pc++;
+		else {
+			set_obj(RA, RD);
+			donextjump;
+		}
+		DISPATCH();
+	DO_BC_IST: /* Jump if D is true */
+		if (itype(RD) == KTAP_TNIL || itype(RD) == KTAP_TFALSE)
+			pc++;
+		else
+			donextjump;
+		DISPATCH();
+	DO_BC_ISF: /* Jump if D is false */
+		/* only nil and false are considered false,
+		 * all other values are true */
+		if (itype(RD) != KTAP_TNIL && itype(RD) != KTAP_TFALSE)
+			pc++;
+		else
+			donextjump;
+		DISPATCH();
+	DO_BC_ISTYPE: /* generated by genlibbc, not compiler; not used now */
+	DO_BC_ISNUM:
+		return;
+	DO_BC_MOV: /* Copy D to A */
+		set_obj(RA, RD);
+		DISPATCH();
+	DO_BC_NOT: /* Set A to boolean not of D */
+		if (itype(RD) == KTAP_TNIL || itype(RD) == KTAP_TFALSE)
+			setitype(RA, KTAP_TTRUE);
+		else
+			setitype(RA, KTAP_TFALSE);
+
+		DISPATCH();
+	DO_BC_UNM: /* Set A to -D (unary minus) */
+		if (!is_number(RD)) {
+			kp_error(ks, "use '-' operator on non-number\n");
+			return;
+		}
+
+		set_number(RA, -nvalue(RD));
+		DISPATCH();
+	DO_BC_ADDVN: /* A = B + C */
+		arith_VN(ks, NUMADD);
+		DISPATCH();
+	DO_BC_SUBVN: /* A = B - C */
+		arith_VN(ks, NUMSUB);
+		DISPATCH();
+	DO_BC_MULVN: /* A = B * C */
+		arith_VN(ks, NUMMUL);
+		DISPATCH();
+	DO_BC_DIVVN: /* A = B / C */
+		/* divide 0 checking */
+		if (!nvalue((ktap_val_t *)kbase + bc_c(instr))) {
+			kp_error(ks, "divide 0 arith operation\n");
+			return;
+		}
+		arith_VN(ks, NUMDIV);
+		DISPATCH();
+	DO_BC_MODVN: /* A = B % C */
+		/* divide 0 checking */
+		if (!nvalue((ktap_val_t *)kbase + bc_c(instr))) {
+			kp_error(ks, "mod 0 arith operation\n");
+			return;
+		}
+		arith_VN(ks, NUMMOD);
+		DISPATCH();
+	DO_BC_ADDNV: /* A = C + B */
+		arith_NV(ks, NUMADD);
+		DISPATCH();
+	DO_BC_SUBNV: /* A = C - B */
+		arith_NV(ks, NUMSUB);
+		DISPATCH();
+	DO_BC_MULNV: /* A = C * B */
+		arith_NV(ks, NUMMUL);
+		DISPATCH();
+	DO_BC_DIVNV: /* A = C / B */
+		/* divide 0 checking */
+		if (!nvalue(RB)){
+			kp_error(ks, "divide 0 arith operation\n");
+			return;
+		}
+		arith_NV(ks, NUMDIV);
+		DISPATCH();
+	DO_BC_MODNV: /* A = C % B */
+		/* divide 0 checking */
+		if (!nvalue(RB)){
+			kp_error(ks, "mod 0 arith operation\n");
+			return;
+		}
+		arith_NV(ks, NUMMOD);
+		DISPATCH();
+	DO_BC_ADDVV: /* A = B + C */
+		arith_VV(ks, NUMADD);
+		DISPATCH();
+	DO_BC_SUBVV: /* A = B - C */
+		arith_VV(ks, NUMSUB);
+		DISPATCH();
+	DO_BC_MULVV: /* A = B * C */
+		arith_VV(ks, NUMMUL);
+		DISPATCH();
+	DO_BC_DIVVV: /* A = B / C */
+		arith_VV(ks, NUMDIV);
+		DISPATCH();
+	DO_BC_MODVV: /* A = B % C */
+		arith_VV(ks, NUMMOD);
+		DISPATCH();
+	DO_BC_POW: /* A = B ^ C, rejected */
+		return;
+	DO_BC_CAT: { /* A = B .. ~ .. C */
+		/* The CAT instruction concatenates all values in
+		 * variable slots B to C inclusive. */
+		ktap_str_t *ts = str_concat(ks, base, bc_b(instr),
+					    bc_c(instr));
+		if (!ts)
+			return;
+		
+		set_string(RA, ts);
+		DISPATCH();
+		}
+	DO_BC_KSTR: { /* Set A to string constant D */
+		int idx = ~bc_d(instr);
+		set_string(RA, (ktap_str_t *)kbase[idx]);
+		DISPATCH();
+		}
+	DO_BC_KCDATA: /* not used now */
+		DISPATCH();
+	DO_BC_KSHORT: /* Set A to 16 bit signed integer D */
+		set_number(RA, bc_d(instr));
+		DISPATCH();
+	DO_BC_KNUM: /* Set A to number constant D */
+		set_number(RA, nvalue(RKD));
+		DISPATCH();
+	DO_BC_KPRI: /* Set A to primitive D */
+		setitype(RA, ~bc_d(instr));
+		DISPATCH();
+	DO_BC_KNIL: { /* Set slots A to D to nil */
+		int i;
+		for (i = 0; i <= bc_d(instr) - bc_a(instr); i++) {
+			set_nil(RA + i);
+		}
+		DISPATCH();
+		}
+	DO_BC_UGET: /* Set A to upvalue D */
+		set_obj(RA, fn->upvals[bc_d(instr)]->v);
+		DISPATCH();
+	DO_BC_USETV: /* Set upvalue A to D */
+		set_obj(fn->upvals[bc_a(instr)]->v, RD);
+		DISPATCH();
+	DO_BC_UINCV: { /* upvalus[A] += D */
+		ktap_val_t *v = fn->upvals[bc_a(instr)]->v;
+		if (unlikely(!is_number(RD) || !is_number(v))) {
+			kp_error(ks, "use '+=' on non-number\n");
+			return;
+		}
+		set_number(v, nvalue(v) + nvalue(RD));
+		DISPATCH();
+		}
+	DO_BC_USETS: { /* Set upvalue A to string constant D */
+		int idx = ~bc_d(instr);
+		set_string(fn->upvals[bc_a(instr)]->v,
+				(ktap_str_t *)kbase[idx]);
+		DISPATCH();
+		}
+	DO_BC_USETN: /* Set upvalue A to number constant D */
+		set_number(fn->upvals[bc_a(instr)]->v, nvalue(RKD));
+		DISPATCH();
+	DO_BC_UINCN: { /* upvalus[A] += D */
+		ktap_val_t *v = fn->upvals[bc_a(instr)]->v;
+		if (unlikely(!is_number(v))) {
+			kp_error(ks, "use '+=' on non-number\n");
+			return;
+		}
+		set_number(v, nvalue(v) + nvalue(RKD));
+		DISPATCH();
+		}
+	DO_BC_USETP: /* Set upvalue A to primitive D */
+		setitype(fn->upvals[bc_a(instr)]->v, ~bc_d(instr));
+		DISPATCH();
+	DO_BC_UCLO: /* Close upvalues for slots . rbase and jump to target D */
+		if (ks->openupval != NULL)
+			func_closeuv(ks, RA);
+		dojump(instr, 0);
+		DISPATCH();
+	DO_BC_FNEW: {
+		/* Create new closure from prototype D and store it in A */
+		int idx = ~bc_d(instr);
+		ktap_func_t *subfn = func_new(ks, (ktap_proto_t *)kbase[idx],
+					      fn, base);
+		if (unlikely(!subfn))
+			return;
+		set_func(RA, subfn);
+		DISPATCH();
+		}
+	DO_BC_TNEW: { /* Set A to new table with size D */
+		/* 
+		 * preallocate default narr and nrec,
+		 * op_b and op_c is not used
+		 * This would allocate more memory for some static table.
+		 */
+		ktap_tab_t *t = kp_tab_new_ah(ks, 0, 0);
+		if (unlikely(!t))
+			return;
+		set_table(RA, t);
+		DISPATCH();
+		}
+	DO_BC_TDUP: { /* Set A to duplicated template table D */
+		int idx = ~bc_d(instr);
+		ktap_tab_t *t = kp_tab_dup(ks, (ktap_tab_t *)kbase[idx]);
+		if (!t)
+			return;
+		set_table(RA, t);
+		DISPATCH();
+		}
+	DO_BC_GGET: { /* A = _G[D] */
+		int idx = ~bc_d(instr);
+		kp_tab_getstr(gtab, (ktap_str_t *)kbase[idx], RA);
+		DISPATCH();
+		}
+	DO_BC_GSET: /* _G[D] = A, rejected. */
+	DO_BC_GINC: /* _G[D] += A, rejected. */
+		return;
+	DO_BC_TGETV: /* A = B[C] */
+		if (unlikely(!is_table(RB))) {
+			kp_error(ks, "get key from non-table\n");
+			return;
+		}
+
+		kp_tab_get(ks, hvalue(RB), RC, RA);
+		DISPATCH();
+	DO_BC_TGETS: { /* A = B[C] */
+		int idx = ~bc_c(instr);
+
+		if (unlikely(!is_table(RB))) {
+			kp_error(ks, "get key from non-table\n");
+			return;
+		}
+		kp_tab_getstr(hvalue(RB), (ktap_str_t *)kbase[idx], RA);
+		DISPATCH();
+		}
+	DO_BC_TGETB: { /* A = B[C] */
+		/* 8 bit literal C operand as an unsigned integer
+		 * index (0..255)) */
+		uint8_t idx = bc_c(instr);
+
+		if (unlikely(!is_table(RB))) {
+			kp_error(ks, "set key to non-table\n");
+			return;
+		}
+		kp_tab_getint(hvalue(RB), idx, RA);
+		DISPATCH();
+		}
+	DO_BC_TGETR: /* generated by genlibbc, not compiler, not used */
+		return;
+	DO_BC_TSETV: /* B[C] = A */
+		if (unlikely(!is_table(RB))) {
+			kp_error(ks, "set key to non-table\n");
+			return;
+		}
+		kp_tab_set(ks, hvalue(RB), RC, RA);
+		DISPATCH();
+	DO_BC_TINCV: /* B[C] += A */
+		if (unlikely(!is_table(RB))) {
+			kp_error(ks, "set key to non-table\n");
+			return;
+		}
+		if (unlikely(!is_number(RA))) {
+			kp_error(ks, "use '+=' on non-number\n");
+			return;
+		}
+		kp_tab_incr(ks, hvalue(RB), RC, nvalue(RA));
+		DISPATCH();
+	DO_BC_TSETS: { /* B[C] = A */
+		int idx = ~bc_c(instr);
+
+		if (unlikely(!is_table(RB))) {
+			kp_error(ks, "set key to non-table\n");
+			return;
+		}
+		kp_tab_setstr(ks, hvalue(RB), (ktap_str_t *)kbase[idx], RA);
+		DISPATCH();
+		}
+	DO_BC_TINCS: { /* B[C] += A */
+		int idx = ~bc_c(instr);
+
+		if (unlikely(!is_table(RB))) {
+			kp_error(ks, "set key to non-table\n");
+			return;
+		}
+		if (unlikely(!is_number(RA))) {
+			kp_error(ks, "use '+=' on non-number\n");
+			return;
+		}
+		kp_tab_incrstr(ks, hvalue(RB), (ktap_str_t *)kbase[idx],
+				nvalue(RA));
+		DISPATCH();
+		}
+	DO_BC_TSETB: { /* B[C] = A */
+		/* 8 bit literal C operand as an unsigned integer
+		 * index (0..255)) */
+		uint8_t idx = bc_c(instr);
+
+		if (unlikely(!is_table(RB))) {
+			kp_error(ks, "set key to non-table\n");
+			return;
+		}
+		kp_tab_setint(ks, hvalue(RB), idx, RA);
+		DISPATCH();
+		}
+	DO_BC_TINCB: { /* B[C] = A */
+		uint8_t idx = bc_c(instr);
+
+		if (unlikely(!is_table(RB))) {
+			kp_error(ks, "set key to non-table\n");
+			return;
+		}
+		if (unlikely(!is_number(RA))) {
+			kp_error(ks, "use '+=' on non-number\n");
+			return;
+		}
+		kp_tab_incrint(ks, hvalue(RB), idx, nvalue(RA));
+		DISPATCH();
+		}
+	DO_BC_TSETM: /* don't support */
+		return;
+	DO_BC_TSETR: /* generated by genlibbc, not compiler, not used */
+		return;
+	DO_BC_CALLM:
+	DO_BC_CALL: { /* b: return_number + 1; c: argument + 1 */
+		int c = bc_c(instr);
+		int nresults = bc_b(instr) - 1;
+		StkId oldtop = ks->top;
+		StkId newfunc = RA;
+
+		if (op == BC_CALL && c != 0)
+			ks->top = RA + c;
+		else if (op == BC_CALLM)
+			ks->top = RA + c + multres;
+
+		if (itype(newfunc) == KTAP_TCFUNC) { /* light C function */
+			ktap_cfunction f = fvalue(newfunc);
+			int n;
+
+			if (unlikely(checkstack(ks,
+					KTAP_MIN_RESERVED_STACK_SIZE)))
+				return;
+
+			ks->func = newfunc;
+			n = (*f)(ks);
+			if (unlikely(n < 0)) /* error occured */
+				return;
+			poscall(ks, newfunc, ks->top - n, nresults);
+
+			ks->top = oldtop;
+			multres = n + 1; /* set to multres */
+			DISPATCH();
+		} else if (itype(newfunc) == KTAP_TFUNC) { /* ktap function */
+			int n;
+
+			func = newfunc;
+			pt = clvalue(func)->p;
+
+			if (unlikely(checkstack(ks, pt->framesize)))
+				return;
+
+			/* get number of real arguments */
+			n = (int)(ks->top - func) - 1;
+
+			/* complete missing arguments */
+			for (; n < pt->numparams; n++)
+				set_nil(ks->top++);
+
+			base = (!(pt->flags & PROTO_VARARG)) ? func + 1 :
+						adjust_varargs(ks, pt, n);
+
+			fn = clvalue(func);
+			pt = fn->p;
+			kbase = pt->k;
+			func->pcr = pc - 1; /* save pc */
+			ks->top = base + pt->framesize;
+			pc = proto_bc(pt) + 1; /* starting point */
+			DISPATCH();
+		} else {
+			kp_error(ks, "attempt to call nil function\n");
+			return;
+		}
+		}
+	DO_BC_CALLMT: /* don't support */
+		return;
+	DO_BC_CALLT: { /* Tailcall: return A(A+1, ..., A+D-1) */
+		StkId nfunc = RA;
+
+		if (itype(nfunc) == KTAP_TCFUNC) { /* light C function */
+			kp_error(ks, "don't support callt for C function");
+			return;
+		} else if (itype(nfunc) == KTAP_TFUNC) { /* ktap function */
+			int aux;
+
+			/*
+			 * tail call: put called frame (n) in place of
+			 * caller one (o)
+			 */
+			StkId ofunc = func; /* caller function */
+			/* last stack slot filled by 'precall' */
+			StkId lim = nfunc + 1 + clvalue(nfunc)->p->numparams;
+
+			fn = clvalue(nfunc);
+			ofunc->val = nfunc->val;
+
+			/* move new frame into old one */
+			for (aux = 1; nfunc + aux < lim; aux++)
+				set_obj(ofunc + aux, nfunc + aux);
+
+			pt = fn->p;
+			kbase = pt->k;
+			ks->top = base + pt->framesize;
+			pc = proto_bc(pt) + 1; /* starting point */
+			DISPATCH();
+		} else {
+			kp_error(ks, "attempt to call nil function\n");
+			return;
+		}
+		}
+	DO_BC_ITERC: /* don't support it now */
+		return;
+	DO_BC_ITERN: /* Specialized ITERC, if iterator function A-3 is next()*/
+		/* detect hot loop */
+		if (unlikely(check_hot_loop(ks, loop_count++) < 0))
+			return;
+
+		if (kp_tab_next(ks, hvalue(RA - 2), RA)) {
+			donextjump; /* Get jump target from ITERL */
+		} else {
+			pc++; /* jump to ITERL + 1 */
+		}
+		DISPATCH();
+	DO_BC_VARG: /* don't support */
+		return;
+	DO_BC_ISNEXT: /* Verify ITERN specialization and jump */
+		if (!is_cfunc(RA - 3) || !is_table(RA - 2) || !is_nil(RA - 1)
+			|| fvalue(RA - 3) != (ktap_cfunction)kp_tab_next) {
+			/* Despecialize bytecode if any of the checks fail. */
+			setbc_op(pc - 1, BC_JMP);
+			dojump(instr, 0);
+			setbc_op(pc, BC_ITERC);
+		} else {
+			dojump(instr, 0);
+			set_nil(RA); /* init control variable */
+		}
+		DISPATCH();
+	DO_BC_RETM: /* don't support return multiple values */
+	DO_BC_RET:
+		return;
+	DO_BC_RET0:
+		/* if it's called from external invocation, just return */
+		if (!func->pcr)
+			return;
+
+		pc = func->pcr; /* restore PC */
+
+		multres = bc_d(instr);
+		set_nil(func);
+
+		base = func - bc_a(*pc);
+		func = base - 1;
+		fn = clvalue(func);
+		kbase = fn->p->k;
+		ks->top = base + pt->framesize;
+		pc++;
+
+		DISPATCH();
+	DO_BC_RET1:
+		/* if it's called from external invocation, just return */
+		if (!func->pcr)
+			return;
+
+		pc = func->pcr; /* restore PC */
+
+		multres = bc_d(instr);
+		set_obj(base - 1, RA); /* move result */
+
+		base = func - bc_a(*pc);
+		func = base - 1;
+		fn = clvalue(func);
+		kbase = fn->p->k;
+		ks->top = base + pt->framesize;
+		pc++;
+
+		DISPATCH();
+	DO_BC_FORI: { /* Numeric 'for' loop init */
+		ktap_number idx;
+		ktap_number limit;
+		ktap_number step;
+
+		if (unlikely(!is_number(RA) || !is_number(RA + 1) ||
+				!is_number(RA + 2))) {
+			kp_error(ks, KTAP_QL("for")
+				 " init/limit/step value must be a number\n");
+			return;
+		}
+
+		idx = nvalue(RA);
+		limit = nvalue(RA + 1);
+		step = nvalue(RA + 2);
+
+		if (NUMLT(0, step) ? NUMLE(idx, limit) : NUMLE(limit, idx)) {
+			set_number(RA + 3, nvalue(RA));
+		} else {
+			dojump(instr, 0);
+		}
+		DISPATCH();
+		}
+	DO_BC_JFORI: /* not used */
+		return;
+	DO_BC_FORL: { /* Numeric 'for' loop */
+		ktap_number step = nvalue(RA + 2);
+		/* increment index */
+		ktap_number idx = NUMADD(nvalue(RA), step);
+		ktap_number limit = nvalue(RA + 1);
+		if (NUMLT(0, step) ? NUMLE(idx, limit) : NUMLE(limit, idx)) {
+			dojump(instr, 0); /* jump back */
+			set_number(RA, idx);  /* update internal index... */
+			set_number(RA + 3, idx);  /* ...and external index */
+		}
+
+		if (unlikely(check_hot_loop(ks, loop_count++) < 0))
+			return;
+
+		DISPATCH();
+		}
+	DO_BC_IFORL: /* not used */
+	DO_BC_JFORL:
+	DO_BC_ITERL:
+	DO_BC_IITERL:
+	DO_BC_JITERL:
+		return;
+	DO_BC_LOOP: /* Generic loop */
+		/* ktap use this bc to detect hot loop */
+		if (unlikely(check_hot_loop(ks, loop_count++) < 0))
+			return;
+		DISPATCH();
+	DO_BC_ILOOP: /* not used */
+	DO_BC_JLOOP:
+		return;
+	DO_BC_JMP: /* Jump */
+		dojump(instr, 0);
+		DISPATCH();
+	DO_BC_FUNCF: /* function header, not used */
+	DO_BC_IFUNCF:
+	DO_BC_JFUNCF:
+	DO_BC_FUNCV:
+	DO_BC_IFUNCV:
+	DO_BC_JFUNCV:
+	DO_BC_FUNCC:
+	DO_BC_FUNCCW:	
+		return;
+	DO_BC_VARGN: /* arg0 .. arg9*/
+		if (unlikely(!ks->current_event)) {
+			kp_error(ks, "invalid event context\n");
+			return;
+		}
+
+		kp_event_getarg(ks, RA, bc_d(instr));
+		DISPATCH();
+	DO_BC_VARGSTR: { /* argstr */
+		/*
+		 * If you pass argstr to print/printf function directly,
+		 * then no extra string generated, so don't worry string
+		 * poll size for below case:
+		 *     print(argstr)
+		 *
+		 * If you use argstr as table key like below, then it may
+		 * overflow your string pool size, so be care of on it.
+		 *     table[argstr] = V
+		 *
+		 * If you assign argstr to upval or table value like below,
+		 * it don't really write string, just write type KTAP_TEVENTSTR,
+		 * the value will be interpreted when value print out in valid
+		 * event context, if context mismatch, error will report.
+		 *     table[V] = argstr
+		 *     upval = argstr
+		 *
+		 * If you want to save real string of argstr, then use it like
+		 * below, again, be care of string pool size in this case.
+		 *     table[V] = stringof(argstr)
+		 *     upval = stringof(argstr)
+		 */
+		struct ktap_event_data *e = ks->current_event;
+
+		if (unlikely(!e)) {
+			kp_error(ks, "invalid event context\n");
+			return;
+		}
+
+		if (e->argstr) /* argstr been stringified */
+			set_string(RA, e->argstr);
+		else
+			set_eventstr(RA);
+		DISPATCH();
+		}
+	DO_BC_VPROBENAME: { /* probename */
+		struct ktap_event_data *e = ks->current_event;
+
+		if (unlikely(!e)) {
+			kp_error(ks, "invalid event context\n");
+			return;
+		}
+		set_string(RA, e->event->name);
+		DISPATCH();
+		}
+	DO_BC_VPID: /* pid */
+		set_number(RA, (int)current->pid);
+		DISPATCH();
+	DO_BC_VTID: /* tid */
+		set_number(RA, (int)task_pid_vnr(current));
+		DISPATCH();
+	DO_BC_VUID: { /* uid */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 5, 0)
+		uid_t uid = from_kuid_munged(current_user_ns(), current_uid());
+#else
+		uid_t uid = current_uid();
+#endif
+		set_number(RA, (int)uid);
+		DISPATCH();
+		}
+	DO_BC_VCPU: /* cpu */
+		set_number(RA, smp_processor_id());
+		DISPATCH();
+	DO_BC_VEXECNAME: { /* execname */
+		ktap_str_t *ts = kp_str_newz(ks, current->comm);
+		if (unlikely(!ts))
+			return;
+		set_string(RA, ts);
+		DISPATCH();
+		}
+	DO_BC_GFUNC: { /* Call built-in C function, patched by BC_GGET */
+		ktap_cfunction cfunc = gfunc_get(ks, bc_d(instr));
+		set_cfunc(RA, cfunc);
+		DISPATCH();
+		}
+	}
+}
+
+/*
+ * Validate byte code and static analysis.
+ *
+ * TODO: more type checking before real running.
+ */
+int kp_vm_validate_code(ktap_state_t *ks, ktap_proto_t *pt, ktap_val_t *base)
+{
+	const unsigned int *pc = proto_bc(pt) + 1;
+	unsigned int instr, op;
+	ktap_obj_t **kbase = pt->k;
+	ktap_tab_t *gtab = G(ks)->gtab;
+	int i;
+
+#define RA	(base + bc_a(instr))
+#define RB	(base + bc_b(instr))
+#define RC	(base + bc_c(instr))
+#define RD	(base + bc_d(instr))
+
+	if (pt->framesize > KP_MAX_SLOTS) {
+		kp_error(ks, "exceed max frame size %d\n", pt->framesize);
+		return -1;
+	}
+
+	if (base + pt->framesize > ks->stack_last) {
+		kp_error(ks, "stack overflow\n");
+		return -1;
+	}
+
+	for (i = 0; i < pt->sizebc - 1; i++) {
+		instr = *pc++;
+		op = bc_op(instr);
+
+
+		if (op >= BC__MAX) {
+			kp_error(ks, "unknown byte code %d\n", op);
+			return -1;
+		}
+
+		switch (op) {
+		case BC_FNEW: {
+			int idx = ~bc_d(instr);
+			ktap_proto_t *newpt = (ktap_proto_t *)kbase[idx];
+			if (kp_vm_validate_code(ks, newpt, RA + 1))
+				return -1;
+
+			break;
+			}
+		case BC_RETM: case BC_RET:
+			kp_error(ks, "don't support return multiple values\n");
+			return -1;
+		case BC_GSET: case BC_GINC: { /* _G[D] = A, _G[D] += A */
+			int idx = ~bc_d(instr);
+			ktap_str_t *ts = (ktap_str_t *)kbase[idx];
+			kp_error(ks, "cannot set global variable '%s'\n",
+					getstr(ts));
+			return -1;
+			}
+		case BC_GGET: {
+			int idx = ~bc_d(instr);
+			ktap_str_t *ts = (ktap_str_t *)kbase[idx];
+			ktap_val_t val;
+			kp_tab_getstr(gtab, ts, &val);
+			if (is_nil(&val)) {
+				kp_error(ks, "undefined global variable"
+						" '%s'\n", getstr(ts));
+				return -1;
+			} else if (is_cfunc(&val)) {
+				int idx = gfunc_getidx(G(ks), fvalue(&val));
+				if (idx >= 0) {
+					/* patch BC_GGET bytecode to BC_GFUNC */
+					setbc_op(pc - 1, BC_GFUNC);
+					setbc_d(pc - 1, idx);
+				}
+			}
+			break;
+			}
+		case BC_ITERC:
+			kp_error(ks, "ktap only support pairs iteraor\n");
+			return -1;
+		case BC_POW:
+			kp_error(ks, "ktap don't support pow arith\n");
+			return -1;
+		}
+	}
+
+	return 0;
+}
+
+/* return cfunction by idx */
+static ktap_cfunction gfunc_get(ktap_state_t *ks, int idx)
+{
+	return G(ks)->gfunc_tbl[idx];
+}
+
+/* get cfunction index, the index is for fast get cfunction in runtime */
+static int gfunc_getidx(ktap_global_state_t *g, ktap_cfunction cfunc)
+{
+	int nr = g->nr_builtin_cfunction;
+	ktap_cfunction *gfunc_tbl = g->gfunc_tbl;
+	int i;
+
+	for (i = 0; i < nr; i++) {
+		if (gfunc_tbl[i] == cfunc)
+			return i;
+	}
+
+	return -1;
+}
+
+static void gfunc_add(ktap_state_t *ks, ktap_cfunction cfunc)
+{
+	int nr = G(ks)->nr_builtin_cfunction;
+
+	if (nr == KP_MAX_CACHED_CFUNCTION) {
+		kp_error(ks, "please enlarge KP_MAX_CACHED_CFUNCTION %d\n",
+				KP_MAX_CACHED_CFUNCTION);
+		return;
+	}
+	G(ks)->gfunc_tbl[nr] = cfunc;
+	G(ks)->nr_builtin_cfunction++;
+}
+
+/* function for register library */
+int kp_vm_register_lib(ktap_state_t *ks, const char *libname,
+		       const ktap_libfunc_t *funcs)
+{
+	ktap_tab_t *gtab = G(ks)->gtab;
+	ktap_tab_t *target_tbl;
+	int i;
+
+	/* lib is null when register baselib function */
+	if (libname == NULL)
+		target_tbl = gtab;
+	else {
+		ktap_val_t key, val;
+		ktap_str_t *ts = kp_str_newz(ks, libname);
+		if (!ts)
+			return -ENOMEM;
+
+		/* calculate the function number contained by this library */
+		for (i = 0; funcs[i].name != NULL; i++) {
+		}
+
+		target_tbl = kp_tab_new_ah(ks, 0, i + 1);
+		if (!target_tbl)
+			return -ENOMEM;
+
+		set_string(&key, ts);
+		set_table(&val, target_tbl);
+		kp_tab_set(ks, gtab, &key, &val);
+	}
+
+	/* TODO: be care of same function name issue, foo() and tbl.foo() */
+	for (i = 0; funcs[i].name != NULL; i++) {
+		ktap_str_t *func_name = kp_str_newz(ks, funcs[i].name);
+		ktap_val_t fn;
+
+		if (unlikely(!func_name))
+			return -ENOMEM;
+
+		set_cfunc(&fn, funcs[i].func);
+		kp_tab_setstr(ks, target_tbl, func_name, &fn);
+
+		gfunc_add(ks, funcs[i].func);
+	}
+
+	return 0;
+}
+
+static int init_registry(ktap_state_t *ks)
+{
+	ktap_tab_t *registry = kp_tab_new_ah(ks, 2, 0);
+	ktap_val_t gtbl;
+	ktap_tab_t *t;
+
+	if (!registry)
+		return -1;
+
+	set_table(&G(ks)->registry, registry);
+
+	/* assume there will have max 1024 global variables */
+	t = kp_tab_new_ah(ks, 0, 1024);
+	if (!t)
+		return -1;
+
+	set_table(&gtbl, t);
+	kp_tab_setint(ks, registry, KTAP_RIDX_GLOBALS, &gtbl);
+	G(ks)->gtab = t;
+
+	return 0;
+}
+
+static int init_arguments(ktap_state_t *ks, int argc, char __user **user_argv)
+{
+	ktap_tab_t *gtbl = G(ks)->gtab;
+	ktap_tab_t *arg_tbl = kp_tab_new_ah(ks, argc, 1);
+	ktap_val_t arg_tblval;
+	ktap_val_t arg_tsval;
+	ktap_str_t *argts = kp_str_newz(ks, "arg");
+	char **argv;
+	int i, ret;
+
+	if (!arg_tbl)
+		return -1;
+
+	if (unlikely(!argts))
+		return -ENOMEM;
+
+	set_string(&arg_tsval, argts);
+	set_table(&arg_tblval, arg_tbl);
+	kp_tab_set(ks, gtbl, &arg_tsval, &arg_tblval);
+
+	if (!argc)
+		return 0;
+
+	if (argc > 1024)
+		return -EINVAL;
+
+	argv = kzalloc(argc * sizeof(char *), GFP_KERNEL);
+	if (!argv)
+		return -ENOMEM;
+
+	ret = copy_from_user(argv, user_argv, argc * sizeof(char *));
+	if (ret < 0) {
+		kfree(argv);
+		return -EFAULT;
+	}
+
+	ret = 0;
+	for (i = 0; i < argc; i++) {
+		ktap_val_t val;
+		char __user *ustr = argv[i];
+		char *kstr;
+		int len;
+		int res;
+
+		len = strlen_user(ustr);
+		if (len > 0x1000) {
+			ret = -EINVAL;
+			break;
+		}
+
+		kstr = kmalloc(len + 1, GFP_KERNEL);
+		if (!kstr) {
+			ret = -ENOMEM;
+			break;
+		}
+
+		if (strncpy_from_user(kstr, ustr, len) < 0) {
+			kfree(kstr);
+			ret = -EFAULT;
+			break;
+		}
+
+		kstr[len] = '\0';
+
+		if (!kstrtoint(kstr, 10, &res)) {
+			set_number(&val, res);
+		} else {
+			ktap_str_t *ts = kp_str_newz(ks, kstr);
+			if (unlikely(!ts)) {
+				kfree(kstr);
+				ret = -ENOMEM;
+				break;
+			}
+				
+			set_string(&val, ts);
+		}
+
+		kp_tab_setint(ks, arg_tbl, i, &val);
+
+		kfree(kstr);
+	}
+
+	kfree(argv);
+	return ret;
+}
+
+static void free_preserved_data(ktap_state_t *ks)
+{
+	int cpu, i, j;
+
+	/* free stack for each allocated ktap_state */
+	for_each_possible_cpu(cpu) {
+		for (j = 0; j < PERF_NR_CONTEXTS; j++) {
+			void *percpu_state = G(ks)->percpu_state[j];
+			ktap_state_t *pks;
+
+			if (!percpu_state)
+				break;
+			pks = per_cpu_ptr(percpu_state, cpu);
+			if (!ks)
+				break;
+			kfree(pks->stack);
+		}
+	}
+
+	/* free percpu ktap_state */
+	for (i = 0; i < PERF_NR_CONTEXTS; i++) {
+		if (G(ks)->percpu_state[i])
+			free_percpu(G(ks)->percpu_state[i]);
+	}
+
+	/* free percpu ktap print buffer */
+	for (i = 0; i < PERF_NR_CONTEXTS; i++) {
+		if (G(ks)->percpu_print_buffer[i])
+			free_percpu(G(ks)->percpu_print_buffer[i]);
+	}
+
+	/* free percpu ktap temp buffer */
+	for (i = 0; i < PERF_NR_CONTEXTS; i++) {
+		if (G(ks)->percpu_temp_buffer[i])
+			free_percpu(G(ks)->percpu_temp_buffer[i]);
+	}
+
+	/* free percpu ktap recursion context flag */
+	for (i = 0; i < PERF_NR_CONTEXTS; i++)
+		if (G(ks)->recursion_context[i])
+			free_percpu(G(ks)->recursion_context[i]);
+}
+
+#define ALLOC_PERCPU(size)  __alloc_percpu(size, __alignof__(char))
+static int init_preserved_data(ktap_state_t *ks)
+{
+	void __percpu *data;
+	int cpu, i, j;
+
+	/* init percpu ktap_state */
+	for (i = 0; i < PERF_NR_CONTEXTS; i++) {
+		data = ALLOC_PERCPU(sizeof(ktap_state_t));
+		if (!data)
+			goto fail;
+		G(ks)->percpu_state[i] = data;
+	}
+
+	/* init stack for each allocated ktap_state */
+	for_each_possible_cpu(cpu) {
+		for (j = 0; j < PERF_NR_CONTEXTS; j++) {
+			void *percpu_state = G(ks)->percpu_state[j];
+			ktap_state_t *pks;
+
+			if (!percpu_state)
+				break;
+			pks = per_cpu_ptr(percpu_state, cpu);
+			if (!ks)
+				break;
+			pks->stack = kzalloc(KTAP_STACK_SIZE_BYTES, GFP_KERNEL);
+			if (!pks->stack)
+				goto fail;
+
+			pks->stack_last = pks->stack + KTAP_STACK_SIZE;
+			G(pks) = G(ks);
+		}
+	}
+
+	/* init percpu ktap print buffer */
+	for (i = 0; i < PERF_NR_CONTEXTS; i++) {
+		data = ALLOC_PERCPU(KTAP_PERCPU_BUFFER_SIZE);
+		if (!data)
+			goto fail;
+		G(ks)->percpu_print_buffer[i] = data;
+	}
+
+	/* init percpu ktap temp buffer */
+	for (i = 0; i < PERF_NR_CONTEXTS; i++) {
+		data = ALLOC_PERCPU(KTAP_PERCPU_BUFFER_SIZE);
+		if (!data)
+			goto fail;
+		G(ks)->percpu_temp_buffer[i] = data;
+	}
+
+	/* init percpu ktap recursion context flag */
+	for (i = 0; i < PERF_NR_CONTEXTS; i++) {
+		data = alloc_percpu(int);
+		if (!data)
+			goto fail;
+		G(ks)->recursion_context[i] = data;
+	}
+
+	return 0;
+
+ fail:
+	free_preserved_data(ks);
+	return -ENOMEM;
+}
+
+/*
+ * wait ktapio thread read all content in ring buffer.
+ *
+ * Here we use stupid approach to sync with ktapio thread,
+ * note that we cannot use semaphore/completion/other sync method,
+ * because ktapio thread could be killed by SIG_KILL in anytime, there
+ * have no safe way to up semaphore or wake waitqueue before thread exit.
+ *
+ * we also cannot use waitqueue of current->signal->wait_chldexit to sync
+ * exit, becasue mainthread and ktapio thread are in same thread group.
+ *
+ * Also ktap mainthread must wait ktapio thread exit, otherwise ktapio
+ * thread will oops when access ktap structure.
+ */
+static void wait_user_completion(ktap_state_t *ks)
+{
+	struct task_struct *tsk = G(ks)->task;
+	G(ks)->wait_user = 1;
+
+	while (1) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		/* sleep for 100 msecs, and try again. */
+		schedule_timeout(HZ / 10);
+
+		if (get_nr_threads(tsk) == 1)
+			break;
+	}
+}
+
+static void sleep_loop(ktap_state_t *ks,
+			int (*actor)(ktap_state_t *ks, void *arg), void *arg)
+{
+	while (!ks->stop) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		/* sleep for 100 msecs, and try again. */
+		schedule_timeout(HZ / 10);
+
+		if (actor(ks, arg))
+			return;
+	}
+}
+
+static int sl_wait_task_pause_actor(ktap_state_t *ks, void *arg)
+{
+	struct task_struct *task = (struct task_struct *)arg;
+
+	if (task->state)
+		return 1;
+	else
+		return 0;
+}
+
+static int sl_wait_task_exit_actor(ktap_state_t *ks, void *arg)
+{
+	struct task_struct *task = (struct task_struct *)arg;
+
+	if (signal_pending(current)) {
+		flush_signals(current);
+
+		/* newline for handle CTRL+C display as ^C */
+		kp_puts(ks, "\n");
+		return 1;
+	}
+
+	/* stop waiting if target pid is exited */
+	if (task && task->state == TASK_DEAD)
+			return 1;
+
+	return 0;
+}
+
+/* wait user interrupt, signal killed */
+static void wait_user_interrupt(ktap_state_t *ks)
+{
+	struct task_struct *task = G(ks)->trace_task;
+
+	if (G(ks)->state == KTAP_EXIT || G(ks)->state == KTAP_ERROR)
+		return;
+
+	/* let tracing goes now. */
+	ks->stop = 0;
+
+	if (G(ks)->parm->workload) {
+		/* make sure workload is in pause state
+		 * so it won't miss the signal */
+		sleep_loop(ks, sl_wait_task_pause_actor, task);
+		/* tell workload process to start executing */
+		send_sig(SIGINT, G(ks)->trace_task, 0);
+	}
+
+	if (!G(ks)->parm->quiet)
+		kp_printf(ks, "Tracing... Hit Ctrl-C to end.\n");
+
+	sleep_loop(ks, sl_wait_task_exit_actor, task);
+}
+
+/*
+ * ktap exit, free all resources.
+ */
+void kp_vm_exit(ktap_state_t *ks)
+{
+	if (!list_empty(&G(ks)->events_head) ||
+	    !list_empty(&G(ks)->timers))
+		wait_user_interrupt(ks);
+
+	kp_exit_timers(ks);
+	kp_events_exit(ks);
+
+	/* free all resources got by ktap */
+#ifdef CONFIG_KTAP_FFI
+	ffi_free_symbols(ks);
+#endif
+	kp_str_freeall(ks);
+	kp_mempool_destroy(ks);
+
+	func_closeuv(ks, 0); /* close all open upvals, let below call free it */
+	kp_obj_freeall(ks);
+
+	kp_vm_exit_thread(ks);
+	kp_free(ks, ks->stack);
+
+	free_preserved_data(ks);
+	free_cpumask_var(G(ks)->cpumask);
+
+	wait_user_completion(ks);
+
+	/* should invoke after wait_user_completion */
+	if (G(ks)->trace_task)
+		put_task_struct(G(ks)->trace_task);
+
+	kp_transport_exit(ks);
+	kp_free(ks, ks); /* free self */
+}
+
+/*
+ * ktap mainthread initization
+ */
+ktap_state_t *kp_vm_new_state(ktap_option_t *parm, struct dentry *dir)
+{
+	ktap_state_t *ks;
+	ktap_global_state_t *g;
+	pid_t pid;
+	int cpu;
+
+	ks = kzalloc(sizeof(ktap_state_t) + sizeof(ktap_global_state_t),
+		     GFP_KERNEL);
+	if (!ks)
+		return NULL;
+
+	G(ks) = (ktap_global_state_t *)(ks + 1);
+	g = G(ks);
+	g->mainthread = ks;
+	g->task = current;
+	g->parm = parm;
+	g->str_lock = (arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED;
+	g->strmask = ~(int)0;
+	g->uvhead.prev = &g->uvhead;
+	g->uvhead.next = &g->uvhead;
+	g->state = KTAP_RUNNING;
+	INIT_LIST_HEAD(&(g->timers));
+	INIT_LIST_HEAD(&(g->events_head));
+
+	if (kp_transport_init(ks, dir))
+		goto out;
+
+	ks->stack = kp_malloc(ks, KTAP_STACK_SIZE_BYTES);
+	if (!ks->stack)
+		goto out;
+
+	ks->stack_last = ks->stack + KTAP_STACK_SIZE;
+	ks->top = ks->stack;
+
+	pid = (pid_t)parm->trace_pid;
+	if (pid != -1) {
+		struct task_struct *task;
+
+		rcu_read_lock();
+		task = pid_task(find_vpid(pid), PIDTYPE_PID);
+		if (!task) {
+			kp_error(ks, "cannot find pid %d\n", pid);
+			rcu_read_unlock();
+			goto out;
+		}
+		g->trace_task = task;
+		get_task_struct(task);
+		rcu_read_unlock();
+	}
+
+	if( !alloc_cpumask_var(&g->cpumask, GFP_KERNEL))
+		goto out;
+
+	cpumask_copy(g->cpumask, cpu_online_mask);
+
+	cpu = parm->trace_cpu;
+	if (cpu != -1) {
+		if (!cpu_online(cpu)) {
+			kp_error(ks, "ktap: cpu %d is not online\n", cpu);
+			goto out;
+		}
+
+		cpumask_clear(g->cpumask);
+		cpumask_set_cpu(cpu, g->cpumask);
+	}
+
+	if (kp_mempool_init(ks, KP_MAX_MEMPOOL_SIZE))
+		goto out;
+
+	if (kp_str_resize(ks, 1024 - 1)) /* set string hashtable size */
+		goto out;
+
+	if (init_registry(ks))
+		goto out;
+	if (init_arguments(ks, parm->argc, parm->argv))
+		goto out;
+
+	/* init librarys */
+	if (kp_lib_init_base(ks))
+		goto out;
+	if (kp_lib_init_kdebug(ks))
+		goto out;
+	if (kp_lib_init_timer(ks))
+		goto out;
+	if (kp_lib_init_ansi(ks))
+		goto out;
+#ifdef CONFIG_KTAP_FFI
+	if (kp_lib_init_ffi(ks))
+		goto out;
+#endif
+	if (kp_lib_init_table(ks))
+		goto out;
+
+	if (kp_lib_init_net(ks))
+		goto out;
+
+	if (init_preserved_data(ks))
+		goto out;
+
+	if (kp_events_init(ks))
+		goto out;
+
+	return ks;
+
+ out:
+	g->state = KTAP_ERROR;
+	kp_vm_exit(ks);
+	return NULL;
+}
+
diff --git a/kernel/trace/ktap/kp_vm.h b/kernel/trace/ktap/kp_vm.h
new file mode 100644
index 0000000..a01e969
--- /dev/null
+++ b/kernel/trace/ktap/kp_vm.h
@@ -0,0 +1,43 @@
+#ifndef __KTAP_VM_H__
+#define __KTAP_VM_H__
+
+#include "kp_obj.h"
+
+void kp_vm_call_proto(ktap_state_t *ks, ktap_proto_t *pt);
+void kp_vm_call(ktap_state_t *ks, StkId func, int nresults);
+int kp_vm_validate_code(ktap_state_t *ks, ktap_proto_t *pt, ktap_val_t *base);
+void kp_vm_exit(ktap_state_t *ks);
+ktap_state_t *kp_vm_new_state(ktap_option_t *parm, struct dentry *dir);
+void kp_optimize_code(ktap_state_t *ks, int level, ktap_proto_t *f);
+int kp_vm_register_lib(ktap_state_t *ks, const char *libname,
+		       const ktap_libfunc_t *funcs);
+
+
+static __always_inline
+ktap_state_t *kp_vm_new_thread(ktap_state_t *mainthread, int rctx)
+{
+	ktap_state_t *ks;
+
+	ks = kp_this_cpu_state(mainthread, rctx);
+	ks->top = ks->stack;
+	return ks;
+}
+
+static __always_inline
+void kp_vm_exit_thread(ktap_state_t *ks)
+{
+}
+
+/*
+ * This function only tell ktapvm this thread want to exit,
+ * let mainthread handle real exit work later.
+ */
+static __always_inline
+void kp_vm_try_to_exit(ktap_state_t *ks)
+{
+	G(ks)->mainthread->stop = 1;
+	G(ks)->state = KTAP_EXIT;
+}
+
+
+#endif /* __KTAP_VM_H__ */
diff --git a/kernel/trace/ktap/ktap.c b/kernel/trace/ktap/ktap.c
new file mode 100644
index 0000000..c46701b
--- /dev/null
+++ b/kernel/trace/ktap/ktap.c
@@ -0,0 +1,276 @@
+/*
+ * ktap.c - ktapvm kernel module main entry
+ *
+ * This file is part of ktap by Jovi Zhangwei.
+ *
+ * Copyright (C) 2012-2013 Jovi Zhangwei <jovi.zhangwei@gmail.com>.
+ *
+ * ktap is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * ktap is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+/*
+ * this file is the first file to be compile, add CONFIG_ checking in here.
+ * See Requirements in doc/tutorial.md
+ */
+
+#include <linux/version.h>
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3, 1, 0)
+#error "Currently ktap don't support kernel older than 3.1"
+#endif
+
+#if !CONFIG_EVENT_TRACING
+#error "Please enable CONFIG_EVENT_TRACING before compile ktap"
+#endif
+
+#if !CONFIG_PERF_EVENTS
+#error "Please enable CONFIG_PERF_EVENTS before compile ktap"
+#endif
+
+#include <linux/module.h>
+#include <linux/errno.h>
+#include <linux/file.h>
+#include <linux/slab.h>
+#include <linux/fcntl.h>
+#include <linux/sched.h>
+#include <linux/poll.h>
+#include <linux/anon_inodes.h>
+#include <linux/debugfs.h>
+#include <linux/vmalloc.h>
+#include <uapi/ktap/ktap_types.h>
+#include "ktap.h"
+#include "kp_bcread.h"
+#include "kp_vm.h"
+
+/* common helper function */
+long gettimeofday_ns(void)
+{
+	struct timespec now;
+
+	getnstimeofday(&now);
+	return now.tv_sec * NSEC_PER_SEC + now.tv_nsec;
+}
+
+static int load_trunk(ktap_option_t *parm, unsigned long **buff)
+{
+	int ret;
+	unsigned long *vmstart;
+
+	vmstart = vmalloc(parm->trunk_len);
+	if (!vmstart)
+		return -ENOMEM;
+
+	ret = copy_from_user(vmstart, (void __user *)parm->trunk,
+			     parm->trunk_len);
+	if (ret < 0) {
+		vfree(vmstart);
+		return -EFAULT;
+	}
+
+	*buff = vmstart;
+	return 0;
+}
+
+static struct dentry *kp_dir_dentry;
+
+/* Ktap Main Entry */
+static int ktap_main(struct file *file, ktap_option_t *parm)
+{
+	unsigned long *buff = NULL;
+	ktap_state_t *ks;
+	ktap_proto_t *pt;
+	long start_time, delta_time;
+	int ret;
+
+	start_time = gettimeofday_ns();
+
+	ks = kp_vm_new_state(parm, kp_dir_dentry);
+	if (unlikely(!ks))
+		return -ENOEXEC;
+
+	file->private_data = ks;
+
+	ret = load_trunk(parm, &buff);
+	if (ret) {
+		kp_error(ks, "cannot load file\n");
+		goto out;
+	}
+
+	pt = kp_bcread(ks, (unsigned char *)buff, parm->trunk_len);
+
+	vfree(buff);
+
+	if (pt) {
+		/* validate byte code */
+		if (kp_vm_validate_code(ks, pt, ks->stack))
+			goto out;
+
+		delta_time = (gettimeofday_ns() - start_time) / NSEC_PER_USEC;
+		kp_verbose_printf(ks, "booting time: %d (us)\n", delta_time);
+
+		/* enter vm */
+		kp_vm_call_proto(ks, pt);
+	}
+
+ out:
+	kp_vm_exit(ks);
+	return ret;
+}
+
+
+static void print_version(void)
+{
+}
+
+static long ktap_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	ktap_option_t parm;
+	int ret;
+
+	switch (cmd) {
+	case KTAP_CMD_IOC_VERSION:
+		print_version();
+		return 0;
+	case KTAP_CMD_IOC_RUN:
+		/*
+		 * must be root to run ktap script (at least for now)
+		 *
+		 * TODO: check perf_paranoid sysctl and allow non-root user
+		 * to use ktap for tracing process(like uprobe) ?
+		 */
+		if (!capable(CAP_SYS_ADMIN))
+			return -EACCES;
+
+		ret = copy_from_user(&parm, (void __user *)arg,
+				     sizeof(ktap_option_t));
+		if (ret < 0)
+			return -EFAULT;
+
+		return ktap_main(file, &parm);
+	default:
+		return -EINVAL;
+	};
+
+        return 0;
+}
+
+static const struct file_operations ktap_fops = {
+	.llseek                 = no_llseek,
+	.unlocked_ioctl         = ktap_ioctl,
+};
+
+static long ktapvm_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	int new_fd, err;
+	struct file *new_file;
+
+	new_fd = get_unused_fd();
+	if (new_fd < 0)
+		return new_fd;
+
+	new_file = anon_inode_getfile("[ktap]", &ktap_fops, NULL, O_RDWR);
+	if (IS_ERR(new_file)) {
+		err = PTR_ERR(new_file);
+		put_unused_fd(new_fd);
+		return err;
+	}
+
+	file->private_data = NULL;
+	fd_install(new_fd, new_file);
+	return new_fd;
+}
+
+static const struct file_operations ktapvm_fops = {
+	.owner  = THIS_MODULE,
+	.unlocked_ioctl         = ktapvm_ioctl,
+};
+
+int (*kp_ftrace_profile_set_filter)(struct perf_event *event, int event_id,
+				    const char *filter_str);
+
+struct syscall_metadata **syscalls_metadata;
+
+/*TODO: kill this function in future */
+static int __init init_dummy_kernel_functions(void)
+{
+	unsigned long *addr;
+
+	/*
+	 * ktap need symbol ftrace_profile_set_filter to set event filter, 
+	 * export it in future. 
+	 */
+#ifdef CONFIG_PPC64
+	kp_ftrace_profile_set_filter =
+		(void *)kallsyms_lookup_name(".ftrace_profile_set_filter");
+#else
+	kp_ftrace_profile_set_filter =
+		(void *)kallsyms_lookup_name("ftrace_profile_set_filter");
+#endif
+	if (!kp_ftrace_profile_set_filter) {
+		pr_err("ktap: cannot lookup ftrace_profile_set_filter "
+			"in kallsyms\n");
+		return -1;
+	}
+
+	/* use syscalls_metadata for syscall event handling */
+	addr = (void *)kallsyms_lookup_name("syscalls_metadata");
+	if (!addr) {
+		pr_err("ktap: cannot lookup syscalls_metadata in kallsyms\n");
+		return -1;
+	}
+
+	syscalls_metadata = (struct syscall_metadata **)*addr;
+	return 0;
+}
+
+static int __init init_ktap(void)
+{
+	struct dentry *ktapvm_dentry;
+
+	if (init_dummy_kernel_functions())
+		return -1;
+
+	kp_dir_dentry = debugfs_create_dir("ktap", NULL);
+	if (!kp_dir_dentry) {
+		pr_err("ktap: debugfs_create_dir failed\n");
+		return -1;
+	}
+
+	ktapvm_dentry = debugfs_create_file("ktapvm", 0444, kp_dir_dentry, NULL,
+					    &ktapvm_fops);
+
+	if (!ktapvm_dentry) {
+		pr_err("ktapvm: cannot create ktapvm file\n");
+		debugfs_remove_recursive(kp_dir_dentry);
+		return -1;
+	}
+
+	return 0;
+}
+
+static void __exit exit_ktap(void)
+{
+	debugfs_remove_recursive(kp_dir_dentry);
+}
+
+module_init(init_ktap);
+module_exit(exit_ktap);
+
+MODULE_AUTHOR("Jovi Zhangwei <jovi.zhangwei@gmail.com>");
+MODULE_DESCRIPTION("ktap");
+MODULE_LICENSE("GPL");
+
+int kp_max_loop_count = 100000;
+module_param_named(max_loop_count, kp_max_loop_count, int, S_IRUGO | S_IWUSR);
+MODULE_PARM_DESC(max_loop_count, "max loop execution count");
+
diff --git a/kernel/trace/ktap/ktap.h b/kernel/trace/ktap/ktap.h
new file mode 100644
index 0000000..4efd901
--- /dev/null
+++ b/kernel/trace/ktap/ktap.h
@@ -0,0 +1,184 @@
+#ifndef __KTAP_H__
+#define __KTAP_H__
+
+#include <linux/version.h>
+#include <linux/hardirq.h>
+#include <linux/trace_seq.h>
+
+/* for built-in library C function register */
+typedef struct ktap_libfunc {
+        const char *name; /* function name */
+        ktap_cfunction func; /* function pointer */
+} ktap_libfunc_t;
+
+long gettimeofday_ns(void); /* common helper function */
+int kp_lib_init_base(ktap_state_t *ks);
+int kp_lib_init_kdebug(ktap_state_t *ks);
+int kp_lib_init_timer(ktap_state_t *ks);
+int kp_lib_init_table(ktap_state_t *ks);
+int kp_lib_init_ansi(ktap_state_t *ks);
+int kp_lib_init_net(ktap_state_t *ks);
+#ifdef CONFIG_KTAP_FFI
+int kp_lib_init_ffi(ktap_state_t *ks);
+#endif
+
+void kp_exit_timers(ktap_state_t *ks);
+void kp_freeupval(ktap_state_t *ks, ktap_upval_t *uv);
+
+extern int (*kp_ftrace_profile_set_filter)(struct perf_event *event,
+					   int event_id,
+					   const char *filter_str);
+
+extern struct syscall_metadata **syscalls_metadata;
+
+/* get from kernel/trace/trace.h */
+static __always_inline int trace_get_context_bit(void)
+{
+	int bit;
+
+	if (in_interrupt()) {
+		if (in_nmi())
+			bit = 0;
+		else if (in_irq())
+			bit = 1;
+		else
+			bit = 2;
+	} else
+		bit = 3;
+
+	return bit;
+}
+
+static __always_inline int get_recursion_context(ktap_state_t *ks)
+{
+	int rctx = trace_get_context_bit();
+	int *val = __this_cpu_ptr(G(ks)->recursion_context[rctx]);
+
+	if (*val)
+		return -1;
+
+	*val = true;
+	return rctx;
+}
+
+static inline void put_recursion_context(ktap_state_t *ks, int rctx)
+{
+	int *val = __this_cpu_ptr(G(ks)->recursion_context[rctx]);
+	*val = false;
+}
+
+static inline void *kp_this_cpu_state(ktap_state_t *ks, int rctx)
+{
+	return this_cpu_ptr(G(ks)->percpu_state[rctx]);
+}
+
+static inline void *kp_this_cpu_print_buffer(ktap_state_t *ks)
+{
+	return this_cpu_ptr(G(ks)->percpu_print_buffer[trace_get_context_bit()]);
+}
+
+static inline void *kp_this_cpu_temp_buffer(ktap_state_t *ks)
+{
+	return this_cpu_ptr(G(ks)->percpu_temp_buffer[trace_get_context_bit()]);
+}
+
+#define kp_verbose_printf(ks, ...) \
+	if (G(ks)->parm->verbose)	\
+		kp_printf(ks, "[verbose] "__VA_ARGS__);
+
+/* argument operation macro */
+#define kp_arg(ks, idx)	((ks)->func + (idx))
+#define kp_arg_nr(ks)	((int)(ks->top - (ks->func + 1)))
+
+#define kp_arg_check(ks, idx, type)				\
+	do {							\
+		if (unlikely(itype(kp_arg(ks, idx)) != type)) {	\
+			kp_error(ks, "wrong type of argument %d\n", idx);\
+			return -1;				\
+		}						\
+	} while (0)
+
+#define kp_arg_checkstring(ks, idx)				\
+	({							\
+		ktap_val_t *o = kp_arg(ks, idx);		\
+		if (unlikely(!is_string(o))) {			\
+			kp_error(ks, "wrong type of argument %d\n", idx); \
+			return -1;				\
+		}						\
+		svalue(o);					\
+	})
+
+#define kp_arg_checkfunction(ks, idx)				\
+	({							\
+		ktap_val_t *o = kp_arg(ks, idx);		\
+		if (unlikely(!is_function(o))) {			\
+			kp_error(ks, "wrong type of argument %d\n", idx); \
+			return -1;				\
+		}						\
+		clvalue(o);					\
+	})
+
+#define kp_arg_checknumber(ks, idx)				\
+	({							\
+		ktap_val_t *o = kp_arg(ks, idx);		\
+		if (unlikely(!is_number(o))) {			\
+			kp_error(ks, "wrong type of argument %d\n", idx); \
+			return -1;				\
+		}						\
+		nvalue(o);					\
+	})
+
+#define kp_arg_checkoptnumber(ks, idx, def)			\
+	({							\
+		ktap_number n;					\
+		if (idx > kp_arg_nr(ks)) {				\
+			n = def;				\
+		} else {					\
+			ktap_val_t *o = kp_arg(ks, idx);	\
+			if (unlikely(!is_number(o))) {		\
+				kp_error(ks, "wrong type of argument %d\n", \
+					     idx);		\
+				return -1;			\
+			}					\
+			n = nvalue(o);				\
+		}						\
+		n;						\
+	})
+
+#define kp_error(ks, args...)			\
+	do {					\
+		kp_printf(ks, "error: "args);	\
+		kp_vm_try_to_exit(ks);		\
+		G(ks)->state = KTAP_ERROR;	\
+	} while(0)
+
+
+/* TODO: this code need to cleanup */
+#if LINUX_VERSION_CODE > KERNEL_VERSION(3, 5, 0)
+#define SPRINT_SYMBOL	sprint_symbol_no_offset
+#else
+#define SPRINT_SYMBOL	sprint_symbol
+#endif
+
+extern int kp_max_loop_count;
+
+void kp_printf(ktap_state_t *ks, const char *fmt, ...);
+void __kp_puts(ktap_state_t *ks, const char *str);
+void __kp_bputs(ktap_state_t *ks, const char *str);
+
+#define kp_puts(ks, str) ({						\
+	static const char *trace_printk_fmt				\
+		__attribute__((section("__trace_printk_fmt"))) =	\
+		__builtin_constant_p(str) ? str : NULL;			\
+									\
+	if (__builtin_constant_p(str))					\
+		__kp_bputs(ks, trace_printk_fmt);		\
+	else								\
+		__kp_puts(ks, str);		\
+})
+
+#define err2msg(em)     (kp_err_allmsg+(int)(em))
+extern const char *kp_err_allmsg;
+
+#endif /* __KTAP_H__ */
+
diff --git a/kernel/trace/ktap/lib_ansi.c b/kernel/trace/ktap/lib_ansi.c
new file mode 100644
index 0000000..fe261da
--- /dev/null
+++ b/kernel/trace/ktap/lib_ansi.c
@@ -0,0 +1,142 @@
+/*
+ * lib_ansi.c - ANSI escape sequences library
+ *
+ * http://en.wikipedia.org/wiki/ANSI_escape_code
+ *
+ * This file is part of ktap by Jovi Zhangwei.
+ *
+ * Copyright (C) 2012-2013 Jovi Zhangwei <jovi.zhangwei@gmail.com>.
+ *
+ * ktap is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * ktap is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+#include <uapi/ktap/ktap_types.h>
+#include "ktap.h"
+#include "kp_vm.h"
+
+/**
+ * function ansi.clear_screen - Move cursor to top left and clear screen.
+ *
+ * Description: Sends ansi code for moving cursor to top left and then the
+ * ansi code for clearing the screen from the cursor position to the end.
+ */
+
+static int kplib_ansi_clear_screen(ktap_state_t *ks)
+{
+	kp_printf(ks, "\033[1;1H\033[J");
+	return 0;
+}
+
+/**
+ * function ansi.set_color - Set the ansi Select Graphic Rendition mode.
+ * @fg: Foreground color to set.
+ *
+ * Description: Sends ansi code for Select Graphic Rendition mode for the
+ * given forground color. Black (30), Blue (34), Green (32), Cyan (36),
+ * Red (31), Purple (35), Brown (33), Light Gray (37).
+ */
+
+static int kplib_ansi_set_color(ktap_state_t *ks)
+{
+	int fg = kp_arg_checknumber(ks, 1);
+
+	kp_printf(ks, "\033[%dm", fg);
+	return 0;
+}
+
+/**
+ * function ansi.set_color2 - Set the ansi Select Graphic Rendition mode.
+ * @fg: Foreground color to set.
+ * @bg: Background color to set.
+ *
+ * Description: Sends ansi code for Select Graphic Rendition mode for the
+ * given forground color, Black (30), Blue (34), Green (32), Cyan (36),
+ * Red (31), Purple (35), Brown (33), Light Gray (37) and the given
+ * background color, Black (40), Red (41), Green (42), Yellow (43),
+ * Blue (44), Magenta (45), Cyan (46), White (47).
+ */
+static int kplib_ansi_set_color2(ktap_state_t *ks)
+{
+	int fg = kp_arg_checknumber(ks, 1);
+	int bg = kp_arg_checknumber(ks, 2);
+	
+	kp_printf(ks, "\033[%d;%dm", fg, bg);
+	return 0;
+}
+
+/**
+ * function ansi.set_color3 - Set the ansi Select Graphic Rendition mode.
+ * @fg: Foreground color to set.
+ * @bg: Background color to set.
+ * @attr: Color attribute to set.
+ *
+ * Description: Sends ansi code for Select Graphic Rendition mode for the
+ * given forground color, Black (30), Blue (34), Green (32), Cyan (36),
+ * Red (31), Purple (35), Brown (33), Light Gray (37), the given
+ * background color, Black (40), Red (41), Green (42), Yellow (43),
+ * Blue (44), Magenta (45), Cyan (46), White (47) and the color attribute
+ * All attributes off (0), Intensity Bold (1), Underline Single (4),
+ * Blink Slow (5), Blink Rapid (6), Image Negative (7).
+ */
+static int kplib_ansi_set_color3(ktap_state_t *ks)
+{
+	int fg = kp_arg_checknumber(ks, 1);
+	int bg = kp_arg_checknumber(ks, 2);
+	int attr = kp_arg_checknumber(ks, 3);
+
+	if (attr)
+		kp_printf(ks, "\033[%d;%d;%dm", fg, bg, attr);
+	else
+		kp_printf(ks, "\033[%d;%dm", fg, bg);
+	
+	return 0;
+}
+
+/**
+ * function ansi.reset_color - Resets Select Graphic Rendition mode.
+ *
+ * Description: Sends ansi code to reset foreground, background and color
+ * attribute to default values.
+ */
+static int kplib_ansi_reset_color(ktap_state_t *ks)
+{
+	kp_printf(ks, "\033[0;0m");
+	return 0;
+}
+
+/**
+ * function ansi.new_line - Move cursor to new line.
+ *
+ * Description: Sends ansi code new line.
+ */
+static int kplib_ansi_new_line (ktap_state_t *ks)
+{
+	kp_printf(ks, "\12");
+	return 0;
+}
+
+static const ktap_libfunc_t ansi_lib_funcs[] = {
+	{"clear_screen", kplib_ansi_clear_screen},
+	{"set_color", kplib_ansi_set_color},
+	{"set_color2", kplib_ansi_set_color2},
+	{"set_color3", kplib_ansi_set_color3},
+	{"reset_color", kplib_ansi_reset_color},
+	{"new_line", kplib_ansi_new_line},
+	{NULL}
+};
+
+int kp_lib_init_ansi(ktap_state_t *ks)
+{
+	return kp_vm_register_lib(ks, "ansi", ansi_lib_funcs); 
+}
diff --git a/kernel/trace/ktap/lib_base.c b/kernel/trace/ktap/lib_base.c
new file mode 100644
index 0000000..b776af6
--- /dev/null
+++ b/kernel/trace/ktap/lib_base.c
@@ -0,0 +1,409 @@
+/*
+ * lib_base.c - base library
+ *
+ * Caveat: all kernel funtion called by ktap library have to be lock free,
+ * otherwise system will deadlock.
+ *
+ * This file is part of ktap by Jovi Zhangwei.
+ *
+ * Copyright (C) 2012-2013 Jovi Zhangwei <jovi.zhangwei@gmail.com>.
+ *
+ * ktap is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * ktap is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+#include <linux/version.h>
+#include <linux/hardirq.h>
+#include <linux/module.h>
+#include <linux/kallsyms.h>
+#include <linux/sched.h>
+#include <linux/uaccess.h>
+#include <linux/utsname.h>
+#include <linux/time.h>
+#include <linux/clocksource.h>
+#include <linux/ring_buffer.h>
+#include <linux/stacktrace.h>
+#include <linux/cred.h>
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 5, 0)
+#include <linux/uidgid.h>
+#endif
+#include <uapi/ktap/ktap_types.h>
+#include "ktap.h"
+#include "kp_obj.h"
+#include "kp_str.h"
+#include "kp_tab.h"
+#include "kp_transport.h"
+#include "kp_events.h"
+#include "kp_vm.h"
+
+static int kplib_print(ktap_state_t *ks)
+{
+	int i;
+	int n = kp_arg_nr(ks);
+
+	for (i = 1; i <= n; i++) {
+		ktap_val_t *arg = kp_arg(ks, i);
+		if (i > 1)
+			kp_puts(ks, "\t");
+		kp_obj_show(ks, arg);
+	}
+
+	kp_puts(ks, "\n");
+	return 0;
+}
+
+/* don't engage with intern string in printf, use buffer directly */
+static int kplib_printf(ktap_state_t *ks)
+{
+	struct trace_seq *seq;
+
+	preempt_disable_notrace();
+
+	seq = kp_this_cpu_print_buffer(ks);
+	trace_seq_init(seq);
+
+	if (kp_str_fmt(ks, seq))
+		goto out;
+
+	seq->buffer[seq->len] = '\0';
+	kp_transport_write(ks, seq->buffer, seq->len + 1);
+
+ out:
+	preempt_enable_notrace();
+	return 0;
+}
+
+#define HISTOGRAM_DEFAULT_TOP_NUM	20
+
+static int kplib_print_hist(ktap_state_t *ks)
+{
+	int n ;
+
+	kp_arg_check(ks, 1, KTAP_TTAB);
+	n = kp_arg_checkoptnumber(ks, 2, HISTOGRAM_DEFAULT_TOP_NUM);
+
+	n = min(n, 1000);
+	n = max(n, HISTOGRAM_DEFAULT_TOP_NUM);
+
+	kp_tab_print_hist(ks, hvalue(kp_arg(ks, 1)), n);
+
+	return 0;
+}
+
+static int kplib_pairs(ktap_state_t *ks)
+{
+	kp_arg_check(ks, 1, KTAP_TTAB);
+
+	set_cfunc(ks->top++, (ktap_cfunction)kp_tab_next);
+	set_table(ks->top++, hvalue(kp_arg(ks, 1)));
+	set_nil(ks->top++);
+	return 3;
+}
+
+static int kplib_len(ktap_state_t *ks)
+{
+	int len = kp_obj_len(ks, kp_arg(ks, 1));
+
+	if (len < 0)
+		return -1;
+
+	set_number(ks->top, len);
+	incr_top(ks);
+	return 1;
+}
+
+static int kplib_delete(ktap_state_t *ks)
+{
+	kp_arg_check(ks, 1, KTAP_TTAB);
+	kp_tab_clear(hvalue(kp_arg(ks, 1)));
+	return 0;
+}
+
+#ifdef CONFIG_STACKTRACE
+static int kplib_stack(ktap_state_t *ks)
+{
+	uint16_t skip, depth = 10;
+
+	depth = kp_arg_checkoptnumber(ks, 1, 10); /* default as 10 */
+	depth = min_t(uint16_t, depth, KP_MAX_STACK_DEPTH);
+	skip = kp_arg_checkoptnumber(ks, 2, 10); /* default as 10 */
+
+	set_kstack(ks->top, depth, skip);
+	incr_top(ks);
+	return 1;
+}
+#else
+static int kplib_stack(ktap_state_t *ks)
+{
+	kp_error(ks, "Please enable CONFIG_STACKTRACE before call stack()\n");
+	return -1;
+}
+#endif
+
+
+extern unsigned long long ns2usecs(cycle_t nsec);
+static int kplib_print_trace_clock(ktap_state_t *ks)
+{
+	unsigned long long t;
+	unsigned long secs, usec_rem;
+	u64 timestamp;
+
+	/* use ring buffer's timestamp */
+	timestamp = ring_buffer_time_stamp(G(ks)->buffer, smp_processor_id());
+
+	t = ns2usecs(timestamp);
+	usec_rem = do_div(t, USEC_PER_SEC);
+	secs = (unsigned long)t;
+
+	kp_printf(ks, "%5lu.%06lu\n", secs, usec_rem);
+	return 0;
+}
+
+static int kplib_num_cpus(ktap_state_t *ks)
+{
+	set_number(ks->top, num_online_cpus());
+	incr_top(ks);
+	return 1;
+}
+
+/* TODO: intern string firstly */
+static int kplib_arch(ktap_state_t *ks)
+{
+	ktap_str_t *ts = kp_str_newz(ks, utsname()->machine);
+	if (unlikely(!ts))
+		return -1;
+
+	set_string(ks->top, ts);
+	incr_top(ks);
+	return 1;
+}
+
+/* TODO: intern string firstly */
+static int kplib_kernel_v(ktap_state_t *ks)
+{
+	ktap_str_t *ts = kp_str_newz(ks, utsname()->release);
+	if (unlikely(!ts))
+		return -1;
+
+	set_string(ks->top, ts);
+	incr_top(ks);
+	return 1;
+}
+
+static int kplib_kernel_string(ktap_state_t *ks)
+{
+	unsigned long addr = kp_arg_checknumber(ks, 1);
+	char str[256] = {0};
+	ktap_str_t *ts;
+	char *ret;
+
+	ret = strncpy((void *)str, (const void *)addr, 256);
+	(void) &ret;  /* Silence compiler warning. */
+	str[255] = '\0';
+
+	ts = kp_str_newz(ks, str);
+	if (unlikely(!ts))
+		return -1;
+
+	set_string(ks->top, ts);
+	incr_top(ks);
+	return 1;
+}
+
+static int kplib_user_string(ktap_state_t *ks)
+{
+	unsigned long addr = kp_arg_checknumber(ks, 1);
+	char str[256] = {0};
+	ktap_str_t *ts;
+	int ret;
+
+	pagefault_disable();
+	ret = __copy_from_user_inatomic((void *)str, (const void *)addr, 256);
+	(void) &ret;  /* Silence compiler warning. */
+	pagefault_enable();
+	str[255] = '\0';
+
+	ts = kp_str_newz(ks, str);
+	if (unlikely(!ts))
+		return -1;
+
+	set_string(ks->top, ts);
+	incr_top(ks);
+	return 1;
+}
+
+static int kplib_stringof(ktap_state_t *ks)
+{
+	ktap_val_t *v = kp_arg(ks, 1);
+	const ktap_str_t *ts = NULL;
+
+	if (itype(v) == KTAP_TEVENTSTR) {
+		ts = kp_event_stringify(ks);
+	} else if (itype(v) == KTAP_TKIP) {
+		char str[KSYM_SYMBOL_LEN];
+
+		SPRINT_SYMBOL(str, nvalue(v));
+		ts = kp_str_newz(ks, str);
+	}
+
+	if (unlikely(!ts))
+		return -1;
+
+	set_string(ks->top++, ts);
+	return 1;
+}
+
+static int kplib_ipof(ktap_state_t *ks)
+{
+	unsigned long addr = kp_arg_checknumber(ks, 1);
+
+	set_ip(ks->top++, addr);
+	return 1;
+}
+
+static int kplib_gettimeofday_ns(ktap_state_t *ks)
+{
+	set_number(ks->top, gettimeofday_ns());
+	incr_top(ks);
+
+	return 1;
+}
+
+static int kplib_gettimeofday_us(ktap_state_t *ks)
+{
+	set_number(ks->top, gettimeofday_ns() / NSEC_PER_USEC);
+	incr_top(ks);
+
+	return 1;
+}
+
+static int kplib_gettimeofday_ms(ktap_state_t *ks)
+{
+	set_number(ks->top, gettimeofday_ns() / NSEC_PER_MSEC);
+	incr_top(ks);
+
+	return 1;
+}
+
+static int kplib_gettimeofday_s(ktap_state_t *ks)
+{
+	set_number(ks->top, gettimeofday_ns() / NSEC_PER_SEC);
+	incr_top(ks);
+
+	return 1;
+}
+
+/*
+ * use gdb to get field offset of struct task_struct, for example:
+ *
+ * gdb vmlinux
+ * (gdb)p &(((struct task_struct *)0).prio)
+ */
+static int kplib_curr_taskinfo(ktap_state_t *ks)
+{
+	int offset = kp_arg_checknumber(ks, 1);
+	int fetch_bytes  = kp_arg_checkoptnumber(ks, 2, 4); /* fetch 4 bytes */
+
+	if (offset >= sizeof(struct task_struct)) {
+		set_nil(ks->top++);
+		kp_error(ks, "access out of bound value of task_struct\n");
+		return 1;
+	}
+
+#define RET_VALUE ((unsigned long)current + offset)
+
+	switch (fetch_bytes) {
+	case 4:
+		set_number(ks->top, *(unsigned int *)RET_VALUE);
+		break;
+	case 8:
+		set_number(ks->top, *(unsigned long *)RET_VALUE);
+		break;
+	default:
+		kp_error(ks, "unsupported fetch bytes in curr_task_info\n");
+		set_nil(ks->top);
+		break;
+	}
+
+#undef RET_VALUE
+
+	incr_top(ks);
+	return 1;
+}
+
+/*
+ * This built-in function mainly purpose scripts/schedule/schedtimes.kp
+ */
+static int kplib_in_iowait(ktap_state_t *ks)
+{
+	set_number(ks->top, current->in_iowait);
+	incr_top(ks);
+
+	return 1;
+}
+
+static int kplib_in_interrupt(ktap_state_t *ks)
+{
+	int ret = in_interrupt();
+
+	set_number(ks->top, ret);
+	incr_top(ks);
+	return 1;
+}
+
+static int kplib_exit(ktap_state_t *ks)
+{
+	kp_vm_try_to_exit(ks);
+
+	/* do not execute bytecode any more in this thread */
+	return -1;
+}
+
+static const ktap_libfunc_t base_lib_funcs[] = {
+	{"print", kplib_print},
+	{"printf", kplib_printf},
+	{"print_hist", kplib_print_hist},
+
+	{"pairs", kplib_pairs},
+	{"len", kplib_len},
+	{"delete", kplib_delete},
+
+	{"stack", kplib_stack},
+	{"print_trace_clock", kplib_print_trace_clock},
+
+	{"num_cpus", kplib_num_cpus},
+	{"arch", kplib_arch},
+	{"kernel_v", kplib_kernel_v},
+	{"kernel_string", kplib_kernel_string},
+	{"user_string", kplib_user_string},
+	{"stringof", kplib_stringof},
+	{"ipof", kplib_ipof},
+
+	{"gettimeofday_ns", kplib_gettimeofday_ns},
+	{"gettimeofday_us", kplib_gettimeofday_us},
+	{"gettimeofday_ms", kplib_gettimeofday_ms},
+	{"gettimeofday_s", kplib_gettimeofday_s},
+
+	{"curr_taskinfo", kplib_curr_taskinfo},
+
+	{"in_iowait", kplib_in_iowait},
+	{"in_interrupt", kplib_in_interrupt},
+
+	{"exit", kplib_exit},
+	{NULL}
+};
+
+int kp_lib_init_base(ktap_state_t *ks)
+{
+	return kp_vm_register_lib(ks, NULL, base_lib_funcs); 
+}
diff --git a/kernel/trace/ktap/lib_ffi.c b/kernel/trace/ktap/lib_ffi.c
new file mode 100644
index 0000000..5d90e30
--- /dev/null
+++ b/kernel/trace/ktap/lib_ffi.c
@@ -0,0 +1,100 @@
+/*
+ * lib_ffi.c - FFI library
+ *
+ * This file is part of ktap by Jovi Zhangwei.
+ *
+ * Copyright (C) 2012-2013 Jovi Zhangwei <jovi.zhangwei@gmail.com>.
+ *
+ * ktap is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * ktap is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+#include <uapi/ktap/ktap_types.h>
+#include <uapi/ktap/ktap_ffi.h>
+#include "ktap.h"
+#include "kp_vm.h"
+
+
+static int kplib_ffi_new(ktap_state_t *ks)
+{
+	int n = kp_arg_nr(ks);
+	csymbol_id cs_id = kp_arg_checknumber(ks, 1);
+	int array_size = kp_arg_checknumber(ks, 2);
+	int is_array = kp_arg_checknumber(ks, 3);
+	ktap_cdata_t *cd;
+
+	if (unlikely(n != 3)) {
+		/* this is not likely to happen since ffi.new arguments are
+		 * generated by compiler */
+		set_nil(ks->top++);
+		kp_error(ks, "wrong number of arguments\n");
+		return 1;
+	}
+
+	if (unlikely(cs_id > max_csym_id(ks)))
+		kp_error(ks, "invalid csymbol id\n");
+
+	kp_verbose_printf(ks, "ffi.new symbol %s with length %d\n",
+			id_to_csym(ks, cs_id)->name, array_size);
+
+	if (is_array)
+		cd = kp_cdata_new_ptr(ks, NULL, array_size, cs_id, 1);
+	else
+		cd = kp_cdata_new_by_id(ks, NULL, cs_id);
+	set_cdata(ks->top, cd);
+	incr_top(ks);
+
+	return 1;
+}
+
+static int kplib_ffi_cast(ktap_state_t *ks)
+{
+	int n = kp_arg_nr(ks);
+	csymbol_id cs_id = kp_arg_checknumber(ks, 1);
+	unsigned long addr = kp_arg_checknumber(ks, 2);
+	ktap_cdata_t *cd;
+
+	if (unlikely(n != 2)) {
+		/* this is not likely to happen since ffi.cast arguments are
+		 * generated by compiler */
+		set_nil(ks->top++);
+		kp_error(ks, "wrong number of arguments\n");
+		return 1;
+	}
+
+	if (unlikely(cs_id > max_csym_id(ks)))
+		kp_error(ks, "invalid csymbol id\n");
+
+	cd = kp_cdata_new_record(ks, (void *)addr, cs_id);
+	set_cdata(ks->top, cd);
+	incr_top(ks);
+	return 1;
+}
+
+static int kplib_ffi_sizeof(ktap_state_t *ks)
+{
+	/*@TODO finish this  08.11 2013 (houqp)*/
+	return 0;
+}
+
+static const ktap_libfunc_t ffi_lib_funcs[] = {
+	{"sizeof", kplib_ffi_sizeof},
+	{"new", kplib_ffi_new},
+	{"cast", kplib_ffi_cast},
+	{NULL}
+};
+
+int kp_lib_init_ffi(ktap_state_t *ks)
+{
+	return kp_vm_register_lib(ks, "ffi", ffi_lib_funcs);
+}
diff --git a/kernel/trace/ktap/lib_kdebug.c b/kernel/trace/ktap/lib_kdebug.c
new file mode 100644
index 0000000..7917e10
--- /dev/null
+++ b/kernel/trace/ktap/lib_kdebug.c
@@ -0,0 +1,202 @@
+/*
+ * lib_kdebug.c - kdebug library support for ktap
+ *
+ * This file is part of ktap by Jovi Zhangwei.
+ *
+ * Copyright (C) 2012-2013 Jovi Zhangwei <jovi.zhangwei@gmail.com>.
+ *
+ * ktap is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * ktap is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+#include <linux/module.h>
+#include <linux/ctype.h>
+#include <linux/slab.h>
+#include <linux/version.h>
+#include <linux/ftrace_event.h>
+#include <uapi/ktap/ktap_types.h>
+#include "ktap.h"
+#include "kp_obj.h"
+#include "kp_str.h"
+#include "kp_transport.h"
+#include "kp_vm.h"
+#include "kp_events.h"
+
+/**
+ * function kdebug.trace_by_id
+ *
+ * @uaddr: userspace address refer to ktap_eventdesc_t
+ * @closure
+ */
+static int kplib_kdebug_trace_by_id(ktap_state_t *ks)
+{
+	unsigned long uaddr = kp_arg_checknumber(ks, 1);
+	ktap_func_t *fn = kp_arg_checkfunction(ks, 2);
+	struct task_struct *task = G(ks)->trace_task;
+	ktap_eventdesc_t eventsdesc;
+	char *filter = NULL;
+	int *id_arr;
+	int ret, i;
+
+	if (G(ks)->mainthread != ks) {
+		kp_error(ks,
+		    "kdebug.trace_by_id only can be called in mainthread\n");
+		return -1;
+	}
+
+	/* kdebug.trace_by_id cannot be called in trace_end state */
+	if (G(ks)->state != KTAP_RUNNING) {
+		kp_error(ks,
+		    "kdebug.trace_by_id only can be called in RUNNING state\n");
+		return -1;
+	}
+
+	/* copy ktap_eventdesc_t from userspace */
+	ret = copy_from_user(&eventsdesc, (void *)uaddr,
+			     sizeof(ktap_eventdesc_t));
+	if (ret < 0)
+		return -1;
+
+	if (eventsdesc.filter) {
+		int len;
+
+		len = strlen_user(eventsdesc.filter);
+		if (len > 0x1000)
+			return -1;
+
+		filter = kmalloc(len + 1, GFP_KERNEL);
+		if (!filter)
+			return -1;
+
+		/* copy filter string from userspace */
+		if (strncpy_from_user(filter, eventsdesc.filter, len) < 0) {
+			kfree(filter);
+			return -1;
+		}
+	}
+
+	id_arr = kmalloc(eventsdesc.nr * sizeof(int), GFP_KERNEL);
+	if (!id_arr) {
+		kfree(filter);
+		return -1;
+	}
+
+	/* copy all event id from userspace */
+	ret = copy_from_user(id_arr, eventsdesc.id_arr,
+			     eventsdesc.nr * sizeof(int));
+	if (ret < 0) {
+		kfree(filter);
+		kfree(id_arr);
+		return -1;
+	}
+
+	fn = clvalue(kp_arg(ks, 2));
+
+	for (i = 0; i < eventsdesc.nr; i++) {
+		struct perf_event_attr attr;
+
+		cond_resched();
+
+		if (signal_pending(current)) {
+			flush_signals(current);
+			kfree(filter);
+			kfree(id_arr);
+			return -1;
+		}
+
+		memset(&attr, 0, sizeof(attr));
+		attr.type = PERF_TYPE_TRACEPOINT;	
+		attr.config = id_arr[i];
+		attr.sample_type = PERF_SAMPLE_RAW | PERF_SAMPLE_TIME |
+				   PERF_SAMPLE_CPU | PERF_SAMPLE_PERIOD;
+		attr.sample_period = 1;
+		attr.size = sizeof(attr);
+		attr.disabled = 0;
+
+		/* register event one by one */
+		ret = kp_event_create(ks, &attr, task, filter, fn);
+		if (ret < 0)
+			break;
+	}
+
+	kfree(filter);
+	kfree(id_arr);
+	return 0;
+}
+
+static int kplib_kdebug_trace_end(ktap_state_t *ks)
+{
+	/* trace_end_closure will be called when ktap main thread exit */
+	G(ks)->trace_end_closure = kp_arg_checkfunction(ks, 1);
+	return 0;
+}
+
+#if 0
+static int kplib_kdebug_tracepoint(ktap_state_t *ks)
+{
+	const char *event_name = kp_arg_checkstring(ks, 1);
+	ktap_func_t *fn = kp_arg_checkfunction(ks, 2);
+
+	if (G(ks)->mainthread != ks) {
+		kp_error(ks,
+		    "kdebug.tracepoint only can be called in mainthread\n");
+		return -1;
+	}
+
+	/* kdebug.tracepoint cannot be called in trace_end state */
+	if (G(ks)->state != KTAP_RUNNING) {
+		kp_error(ks,
+		    "kdebug.tracepoint only can be called in RUNNING state\n");
+		return -1;
+	}
+
+	return kp_event_create_tracepoint(ks, event_name, fn);
+}
+#endif
+
+static int kplib_kdebug_kprobe(ktap_state_t *ks)
+{
+	const char *event_name = kp_arg_checkstring(ks, 1);
+	ktap_func_t *fn = kp_arg_checkfunction(ks, 2);
+
+	if (G(ks)->mainthread != ks) {
+		kp_error(ks,
+		    "kdebug.kprobe only can be called in mainthread\n");
+		return -1;
+	}
+
+	/* kdebug.kprobe cannot be called in trace_end state */
+	if (G(ks)->state != KTAP_RUNNING) {
+		kp_error(ks,
+		    "kdebug.kprobe only can be called in RUNNING state\n");
+		return -1;
+	}
+
+	return kp_event_create_kprobe(ks, event_name, fn);
+}
+static const ktap_libfunc_t kdebug_lib_funcs[] = {
+	{"trace_by_id", kplib_kdebug_trace_by_id},
+	{"trace_end", kplib_kdebug_trace_end},
+
+#if 0
+	{"tracepoint", kplib_kdebug_tracepoint},
+#endif
+	{"kprobe", kplib_kdebug_kprobe},
+	{NULL}
+};
+
+int kp_lib_init_kdebug(ktap_state_t *ks)
+{
+	return kp_vm_register_lib(ks, "kdebug", kdebug_lib_funcs);
+}
+
diff --git a/kernel/trace/ktap/lib_net.c b/kernel/trace/ktap/lib_net.c
new file mode 100644
index 0000000..3a764ce
--- /dev/null
+++ b/kernel/trace/ktap/lib_net.c
@@ -0,0 +1,107 @@
+/*
+ * lib_base.c - base library
+ *
+ * This file is part of ktap by Jovi Zhangwei.
+ *
+ * Copyright (C) 2012-2013 Jovi Zhangwei <jovi.zhangwei@gmail.com>.
+ *
+ * ktap is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * ktap is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+#include <net/inet_sock.h>
+#include <uapi/ktap/ktap_types.h>
+#include "ktap.h"
+#include "kp_obj.h"
+#include "kp_str.h"
+#include "kp_vm.h"
+
+/**
+ * Return the source IP address for a given sock
+ */
+static int kplib_net_ip_sock_saddr(ktap_state_t *ks)
+{
+	struct inet_sock *isk;
+	int family;
+
+	/* TODO: need to validate the address firstly */	
+
+	isk = (struct inet_sock *)kp_arg_checknumber(ks, 1);
+	family = isk->sk.__sk_common.skc_family;
+
+	if (family == AF_INET) {
+		set_number(ks->top, isk->inet_rcv_saddr);
+	} else {
+		kp_error(ks, "ip_sock_saddr only support ipv4 now\n");
+		set_nil(ks->top);
+	}
+
+	incr_top(ks);
+	return 1;
+}
+
+/**
+ * Return the destination IP address for a given sock
+ */
+static int kplib_net_ip_sock_daddr(ktap_state_t *ks)
+{
+	struct inet_sock *isk;
+	int family;
+
+	/* TODO: need to validate the address firstly */	
+
+	isk = (struct inet_sock *)kp_arg_checknumber(ks, 1);
+	family = isk->sk.__sk_common.skc_family;
+
+	if (family == AF_INET) {
+		set_number(ks->top, isk->inet_daddr);
+	} else {
+		kp_error(ks, "ip_sock_daddr only support ipv4 now\n");
+		set_nil(ks->top);
+	}
+
+	incr_top(ks);
+	return 1;
+
+}
+
+/**
+ * Returns a string representation for an IP address
+ */
+static int kplib_net_format_ip_addr(ktap_state_t *ks)
+{
+	__be32 ip = (__be32)kp_arg_checknumber(ks, 1);
+	ktap_str_t *ts;
+	char ipstr[32];
+
+	snprintf(ipstr, 32, "%pI4", &ip);
+	ts = kp_str_newz(ks, ipstr);
+	if (ts) {
+		set_string(ks->top, kp_str_newz(ks, ipstr));
+		incr_top(ks);
+		return 1;
+	} else
+		return -1;
+}
+
+static const ktap_libfunc_t net_lib_funcs[] = {
+	{"ip_sock_saddr", kplib_net_ip_sock_saddr},
+	{"ip_sock_daddr", kplib_net_ip_sock_daddr},
+	{"format_ip_addr", kplib_net_format_ip_addr},
+	{NULL}
+};
+
+int kp_lib_init_net(ktap_state_t *ks)
+{
+	return kp_vm_register_lib(ks, "net", net_lib_funcs); 
+}
diff --git a/kernel/trace/ktap/lib_table.c b/kernel/trace/ktap/lib_table.c
new file mode 100644
index 0000000..27bdf2f
--- /dev/null
+++ b/kernel/trace/ktap/lib_table.c
@@ -0,0 +1,58 @@
+/*
+ * lib_table.c - Table library
+ *
+ * This file is part of ktap by Jovi Zhangwei.
+ *
+ * Copyright (C) 2012-2013 Jovi Zhangwei <jovi.zhangwei@gmail.com>.
+ *
+ * ktap is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * ktap is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+#include <linux/ctype.h>
+#include <linux/slab.h>
+#include <linux/delay.h>
+#include <linux/sched.h>
+#include <uapi/ktap/ktap_types.h>
+#include "ktap.h"
+#include "kp_obj.h"
+#include "kp_vm.h"
+#include "kp_tab.h"
+
+static int kplib_table_new(ktap_state_t *ks)
+{
+	int narr = kp_arg_checkoptnumber(ks, 1, 0);
+	int nrec = kp_arg_checkoptnumber(ks, 2, 0);
+	ktap_tab_t *h;
+
+	h = kp_tab_new_ah(ks, narr, nrec);
+	if (!h) {
+		set_nil(ks->top);
+	} else {
+		set_table(ks->top, h);
+	}
+
+	incr_top(ks);
+	return 1;
+}
+
+static const ktap_libfunc_t table_lib_funcs[] = {
+	{"new",	kplib_table_new},
+	{NULL}
+};
+
+int kp_lib_init_table(ktap_state_t *ks)
+{
+	return kp_vm_register_lib(ks, "table", table_lib_funcs);
+}
+
diff --git a/kernel/trace/ktap/lib_timer.c b/kernel/trace/ktap/lib_timer.c
new file mode 100644
index 0000000..d7d6814
--- /dev/null
+++ b/kernel/trace/ktap/lib_timer.c
@@ -0,0 +1,210 @@
+/*
+ * lib_timer.c - timer library support for ktap
+ *
+ * This file is part of ktap by Jovi Zhangwei.
+ *
+ * Copyright (C) 2012-2013 Jovi Zhangwei <jovi.zhangwei@gmail.com>.
+ *
+ * ktap is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * ktap is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+#include <linux/ctype.h>
+#include <linux/slab.h>
+#include <linux/delay.h>
+#include <linux/sched.h>
+#include <uapi/ktap/ktap_types.h>
+#include "ktap.h"
+#include "kp_obj.h"
+#include "kp_vm.h"
+#include "kp_events.h"
+
+struct ktap_hrtimer {
+	struct hrtimer timer;
+	ktap_state_t *ks;
+	ktap_func_t *fn;
+	u64 ns;
+	struct list_head list;
+};
+
+/*
+ * Currently ktap disallow tracing event in timer callback closure,
+ * that will corrupt ktap_state_t and ktap stack, because timer closure
+ * and event closure use same irq percpu ktap_state_t and stack.
+ * We can use a different percpu ktap_state_t and stack for timer purpuse,
+ * but that's don't bring any big value with cost on memory consuming.
+ *
+ * So just simply disable tracing in timer closure,
+ * get_recursion_context()/put_recursion_context() is used for this purpose.
+ */
+static enum hrtimer_restart hrtimer_ktap_fn(struct hrtimer *timer)
+{
+	struct ktap_hrtimer *t;
+	ktap_state_t *ks;
+	int rctx;
+
+	rcu_read_lock_sched_notrace();
+
+	t = container_of(timer, struct ktap_hrtimer, timer);
+	rctx = get_recursion_context(t->ks);
+
+	ks = kp_vm_new_thread(t->ks, rctx);
+	set_func(ks->top, t->fn);
+	incr_top(ks);
+	kp_vm_call(ks, ks->top - 1, 0);
+	kp_vm_exit_thread(ks);
+
+	hrtimer_add_expires_ns(timer, t->ns);
+
+	put_recursion_context(ks, rctx);
+	rcu_read_unlock_sched_notrace();
+
+	return HRTIMER_RESTART;
+}
+
+static int set_tick_timer(ktap_state_t *ks, u64 period, ktap_func_t *fn)
+{
+	struct ktap_hrtimer *t;
+
+	t = kp_malloc(ks, sizeof(*t));
+	if (unlikely(!t))
+		return -ENOMEM;
+	t->ks = ks;
+	t->fn = fn;
+	t->ns = period;
+
+	INIT_LIST_HEAD(&t->list);
+	list_add(&t->list, &(G(ks)->timers));
+
+	hrtimer_init(&t->timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	t->timer.function = hrtimer_ktap_fn;
+	hrtimer_start(&t->timer, ns_to_ktime(period), HRTIMER_MODE_REL);
+
+	return 0;
+}
+
+static int set_profile_timer(ktap_state_t *ks, u64 period, ktap_func_t *fn)
+{
+	struct perf_event_attr attr;
+
+	memset(&attr, 0, sizeof(attr));
+	attr.type = PERF_TYPE_SOFTWARE;
+	attr.config = PERF_COUNT_SW_CPU_CLOCK;
+	attr.sample_type = PERF_SAMPLE_RAW | PERF_SAMPLE_TIME |
+			   PERF_SAMPLE_CPU | PERF_SAMPLE_PERIOD;
+	attr.sample_period = period;
+	attr.size = sizeof(attr);
+	attr.disabled = 0;
+
+	return kp_event_create(ks, &attr, NULL, NULL, fn);
+}
+
+static int do_tick_profile(ktap_state_t *ks, int is_tick)
+{
+	const char *str = kp_arg_checkstring(ks, 1);
+	ktap_func_t *fn = kp_arg_checkfunction(ks, 2);
+	const char *tmp;
+	char interval_str[32] = {0};
+	char suffix[10] = {0};
+	int i = 0, ret, n;
+	int factor;
+
+	tmp = str;
+	while (isdigit(*tmp))
+		tmp++;
+
+	strncpy(interval_str, str, tmp - str);
+	if (kstrtoint(interval_str, 10, &n))
+		goto error;
+
+	strncpy(suffix, tmp, 9);
+	while (suffix[i] != ' ' && suffix[i] != '\0')
+		i++;
+
+	suffix[i] = '\0';
+
+	if (!strcmp(suffix, "s") || !strcmp(suffix, "sec"))
+		factor = NSEC_PER_SEC;
+	else if (!strcmp(suffix, "ms") || !strcmp(suffix, "msec"))
+		factor = NSEC_PER_MSEC;
+	else if (!strcmp(suffix, "us") || !strcmp(suffix, "usec"))
+		factor = NSEC_PER_USEC;
+	else
+		goto error;
+
+	if (is_tick)
+		ret = set_tick_timer(ks, (u64)factor * n, fn);
+	else
+		ret = set_profile_timer(ks, (u64)factor * n, fn);
+
+	return ret;
+
+ error:
+	kp_error(ks, "cannot parse timer interval: %s\n", str);
+	return -1;
+}
+
+/*
+ * tick-n probes fire on only one CPU per interval.
+ * valid time suffixes: sec/s, msec/ms, usec/us
+ */
+static int kplib_timer_tick(ktap_state_t *ks)
+{
+	/* timer.tick cannot be called in trace_end state */
+	if (G(ks)->state != KTAP_RUNNING) {
+		kp_error(ks,
+			 "timer.tick only can be called in RUNNING state\n");
+		return -1;
+	}
+
+	return do_tick_profile(ks, 1);
+}
+
+/*
+ * A profile-n probe fires every fixed interval on every CPU
+ * valid time suffixes: sec/s, msec/ms, usec/us
+ */
+static int kplib_timer_profile(ktap_state_t *ks)
+{
+	/* timer.profile cannot be called in trace_end state */
+	if (G(ks)->state != KTAP_RUNNING) {
+		kp_error(ks,
+			 "timer.profile only can be called in RUNNING state\n");
+		return -1;
+	}
+
+	return do_tick_profile(ks, 0);
+}
+
+void kp_exit_timers(ktap_state_t *ks)
+{
+	struct ktap_hrtimer *t, *tmp;
+	struct list_head *timers_list = &(G(ks)->timers);
+
+	list_for_each_entry_safe(t, tmp, timers_list, list) {
+		hrtimer_cancel(&t->timer);
+		kp_free(ks, t);
+	}
+}
+
+static const ktap_libfunc_t timer_lib_funcs[] = {
+	{"profile",	kplib_timer_profile},
+	{"tick",	kplib_timer_tick},
+	{NULL}
+};
+
+int kp_lib_init_timer(ktap_state_t *ks)
+{
+	return kp_vm_register_lib(ks, "timer", timer_lib_funcs);
+}
+
diff --git a/lib/Kconfig.debug b/lib/Kconfig.debug
index 5f8fff1..2fb341b 100644
--- a/lib/Kconfig.debug
+++ b/lib/Kconfig.debug
@@ -1423,6 +1423,7 @@ config DEBUG_STRICT_USER_COPY_CHECKS
 	  If unsure, say N.
 
 source kernel/trace/Kconfig
+source kernel/trace/ktap/Kconfig
 
 menu "Runtime Testing"
 
-- 
2.0.2

