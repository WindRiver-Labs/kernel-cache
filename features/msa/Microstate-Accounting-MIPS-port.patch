From 2ff4bb7f006cea7b99f5e8077d631cd7466e7381 Mon Sep 17 00:00:00 2001
From: Philby John <pjohn@mvista.com>
Date: Mon, 28 Jun 2010 17:00:22 +0530
Subject: [PATCH 09/18] Microstate Accounting MIPS port

From the upstream project:
git://microstate.git.sourceforge.net/gitroot/microstate/linux-msa

This patch covers common MIPS code to make MSA work properly on
specific MIPS platform/board, MSA callbacks should be added to platform
specific code to cover board specific IRQs and exceptions.
To test that MSA works correctly, counters were added to msa_user/msa_kernel.
Counter delta was constant after system fully boot - so every kernel enter/exit
was tracked by MSA.

Signed-off-by: Philby John <pjohn@mvista.com>
Signed-off-by: Konstantin Baydarov <kbaidarov@ru.mvista.com>
Signed-off-by: Srikanth Krishnakar <skrishna@in.mvista.com>
Signed-off-by: Corey Minyard <cminyard@cavium.com>
---
 arch/mips/Kconfig                   |   23 ++++++++
 arch/mips/include/asm/irq.h         |   41 +++++++++++++++
 arch/mips/include/asm/msa.h         |   64 +++++++++++++++++++++++
 arch/mips/include/asm/msa_calling.h |   67 ++++++++++++++++++++++++
 arch/mips/include/asm/smp.h         |    4 ++
 arch/mips/include/asm/unistd.h      |   15 +++---
 arch/mips/kernel/binfmt_elfn32.c    |   22 ++++++++
 arch/mips/kernel/binfmt_elfo32.c    |   22 ++++++++
 arch/mips/kernel/entry.S            |    8 +++
 arch/mips/kernel/irq.c              |    6 ++-
 arch/mips/kernel/scall32-o32.S      |    8 +++
 arch/mips/kernel/scall64-64.S       |    4 ++
 arch/mips/kernel/scall64-n32.S      |    4 ++
 arch/mips/kernel/scall64-o32.S      |    4 ++
 arch/mips/kernel/smp.c              |   20 ++++++-
 arch/mips/kernel/smtc.c             |    3 +-
 arch/mips/kernel/traps.c            |    3 ++
 arch/mips/mm/fault.c                |    6 +++
 include/linux/msa.h                 |    5 ++
 kernel/msa.c                        |   98 ++++++++++++++++++++++++++++-------
 20 files changed, 398 insertions(+), 29 deletions(-)
 create mode 100644 arch/mips/include/asm/msa.h
 create mode 100644 arch/mips/include/asm/msa_calling.h

diff --git a/arch/mips/Kconfig b/arch/mips/Kconfig
index ce30e2f..b55f9f3 100644
--- a/arch/mips/Kconfig
+++ b/arch/mips/Kconfig
@@ -2331,6 +2331,29 @@ config USE_OF
 	help
 	  Include support for flattened device tree machine descriptions.
 
+config MICROSTATE_ACCT
+	bool "Microstate accounting"
+	depends on MIPS_MT_DISABLED
+	help
+	  This option causes the kernel to keep very accurate track of
+	  how long your threads spend on the runqueues, running, or asleep or
+	  stopped.  It will slow down your kernel.
+	  Times are reported in /proc/pid/msa and through a new msa()
+	  system call.
+
+choice
+	depends on MICROSTATE_ACCT
+	prompt "Microstate timing source"
+	default MICROSTATE_C0_COUNT_REGISTER
+
+config MICROSTATE_C0_COUNT_REGISTER
+	bool "Use 32-bit c0 cout register"
+	depends on HIGH_RES_TIMERS
+	help
+	  Use MIPS c0 count register for MSA timekeeping.
+
+endchoice
+
 endmenu
 
 config LOCKDEP_SUPPORT
diff --git a/arch/mips/include/asm/irq.h b/arch/mips/include/asm/irq.h
index fb698dc..5db1f8c 100644
--- a/arch/mips/include/asm/irq.h
+++ b/arch/mips/include/asm/irq.h
@@ -14,6 +14,7 @@
 #include <linux/irqdomain.h>
 
 #include <asm/mipsmtregs.h>
+#include <asm/ptrace.h>
 
 #include <irq.h>
 
@@ -116,6 +117,46 @@ static inline int smtc_handle_on_other_cpu(unsigned int irq)
 
 #endif
 
+/*
+ * This struct defines the way the registers are stored on the stack during a
+ * system call/exception. As usual the registers k0/k1 aren't being saved.
+ */
+static __always_inline bool msa_get_reg(void)
+{
+	struct pt_regs regs;
+
+#ifndef CONFIG_KALLSYMS
+	/*
+	 * Remove any garbage that may be in regs (specially func
+	 * addresses) to avoid show_raw_backtrace() to report them
+	 */
+	memset(regs, 0, sizeof(regs));
+#endif
+	__asm__ __volatile__(
+		".set push\n\t"
+		".set noat\n\t"
+#ifdef CONFIG_64BIT
+		"1: dla $1, 1b\n\t"
+		"sd $1, %0\n\t"
+		"sd $29, %1\n\t"
+		"sd $31, %2\n\t"
+#else
+		"1: la $1, 1b\n\t"
+		"sw $1, %0\n\t"
+		"sw $29, %1\n\t"
+		"sw $31, %2\n\t"
+#endif
+		".set pop\n\t"
+		: "=m" (regs.cp0_epc),
+		"=m" (regs.regs[29]), "=m" (regs.regs[31])
+		: : "memory");
+
+	if ((regs.cp0_status & ST0_KSU) == KSU_USER)
+		return true;
+	else
+		return false;
+}
+
 extern void do_IRQ(unsigned int irq);
 
 #ifdef CONFIG_MIPS_MT_SMTC_IRQAFF
diff --git a/arch/mips/include/asm/msa.h b/arch/mips/include/asm/msa.h
new file mode 100644
index 0000000..35703fb
--- /dev/null
+++ b/arch/mips/include/asm/msa.h
@@ -0,0 +1,64 @@
+/************************************************************************
+ * asm-mips/msa.h
+ *
+ * Provide an architecture-specific clock.
+ */
+
+#include <linux/timex.h>
+#include <asm/time.h>
+
+#ifndef _ASM_MIPS_MSA_H
+#define _ASM_MIPS_MSA_H
+
+# if defined(CONFIG_MICROSTATE_C0_COUNT_REGISTER)
+/*
+ * MSA uses MIPS 32 bit C0 counter register.
+ */
+extern msa_time_t msa_cycles_last;
+extern u32 msa_last_count;
+extern seqlock_t msa_seqlock;
+
+static inline msa_time_t msa_now(void)
+{
+	u32 count;
+	unsigned long seq, flags;
+	msa_time_t ret;
+
+	do {
+		seq = read_seqbegin(&msa_seqlock);
+		count = read_c0_count();
+		/* Udate if delta > (0xffffffff/4) */
+		if (count - msa_last_count > 0x3fffffffUL) {
+			write_seqlock_irqsave(&msa_seqlock, flags);
+			msa_cycles_last += (u32) (count - msa_last_count);
+			msa_last_count = count;
+			ret = msa_cycles_last;
+			write_sequnlock_irqrestore(&msa_seqlock, flags);
+			break;
+		}
+		ret = msa_cycles_last + (u32) (count - msa_last_count);
+	} while (read_seqretry(&msa_seqlock, seq));
+
+	return ret;
+}
+
+static inline u64 msa_to_nsec(msa_time_t cycles)
+{
+	msa_time_t sec, nsec;
+
+	sec = cycles;
+	/* To prevent overflow, first, extract seconds  */
+	nsec = do_div(sec, mips_hpt_frequency);
+	/* Then multiply reminder cycles value to get nsecs */
+	nsec *= 1000000000ULL;
+	do_div(nsec, mips_hpt_frequency);
+	return sec * 1000000000ULL + nsec;
+}
+
+#  define MSA_NOW(now)  do { (now) = msa_now(); } while (0)
+#  define MSA_TO_NSEC(clk) msa_to_nsec(clk)
+# else
+#  error "No clocksource defined for Microstate Accounting"
+# endif
+
+#endif
diff --git a/arch/mips/include/asm/msa_calling.h b/arch/mips/include/asm/msa_calling.h
new file mode 100644
index 0000000..17a5f17
--- /dev/null
+++ b/arch/mips/include/asm/msa_calling.h
@@ -0,0 +1,67 @@
+/* Reload some registers clobbered by trace_hardirqs_on */
+#ifdef CONFIG_MICROSTATE_ACCT
+#ifdef CONFIG_64BIT
+# define MSA_SAVE_REGS					\
+	LONG_SUB	sp, sp, 64;			\
+	LONG_S	$11, 0(sp);				\
+	LONG_S	$10, 8(sp);				\
+	LONG_S	$9, 16(sp);				\
+	LONG_S	$8, 24(sp);				\
+	LONG_S	$7, 32(sp);				\
+	LONG_S	$6, 40(sp);				\
+	LONG_S	$5, 48(sp);				\
+	LONG_S	$4, 56(sp);				\
+	LONG_S	$2, 64(sp)
+
+# define MSA_RESTORE_REGS				\
+	LONG_L	$11, 0(sp);				\
+	LONG_L	$10, 8(sp);				\
+	LONG_L	$9, 16(sp);				\
+	LONG_L	$8, 24(sp);				\
+	LONG_L	$7, 32(sp);				\
+	LONG_L	$6, 40(sp);				\
+	LONG_L	$5, 48(sp);				\
+	LONG_L	$4, 56(sp);				\
+	LONG_L	$2, 64(sp);				\
+	LONG_ADD	sp, sp, 64
+
+#else
+# define MSA_SAVE_REGS					\
+	LONG_SUB	sp, sp, 32;			\
+	LONG_S	$7, 0(sp);				\
+	LONG_S	$6, 8(sp);				\
+	LONG_S	$5, 16(sp);				\
+	LONG_S	$4, 24(sp);				\
+	LONG_S	$2, 32(sp)
+
+# define MSA_RESTORE_REGS				\
+	LONG_L	$7, 0(sp);				\
+	LONG_L	$6, 8(sp);				\
+	LONG_L	$5, 16(sp);				\
+	LONG_L	$4, 24(sp);				\
+	LONG_L	$2, 32(sp);				\
+	LONG_ADD	sp, sp, 32
+#endif
+#endif
+
+	.macro MSA_USER
+#ifdef CONFIG_MICROSTATE_ACCT
+	nop
+	MSA_SAVE_REGS
+	jal	msa_user
+	nop
+	MSA_RESTORE_REGS
+	nop
+#endif
+	.endm
+
+	.macro MSA_KERNEL
+#ifdef CONFIG_MICROSTATE_ACCT
+	nop
+	MSA_SAVE_REGS
+	jal	msa_kernel
+	nop
+	MSA_RESTORE_REGS
+	nop
+#endif
+	.endm
diff --git a/arch/mips/include/asm/smp.h b/arch/mips/include/asm/smp.h
index d4fb4d8..78baa00 100644
--- a/arch/mips/include/asm/smp.h
+++ b/arch/mips/include/asm/smp.h
@@ -75,7 +75,11 @@ static inline void __cpu_die(unsigned int cpu)
 extern void play_dead(void);
 #endif
 
+#ifdef CONFIG_MICROSTATE_ACCT
+extern asmlinkage void smp_call_function_interrupt(int irq);
+#else
 extern asmlinkage void smp_call_function_interrupt(void);
+#endif
 
 static inline void arch_send_call_function_single_ipi(int cpu)
 {
diff --git a/arch/mips/include/asm/unistd.h b/arch/mips/include/asm/unistd.h
index d8dad53..ba58a32 100644
--- a/arch/mips/include/asm/unistd.h
+++ b/arch/mips/include/asm/unistd.h
@@ -367,16 +367,17 @@
 #define __NR_setns			(__NR_Linux + 344)
 #define __NR_process_vm_readv		(__NR_Linux + 345)
 #define __NR_process_vm_writev		(__NR_Linux + 346)
+#define __NR_msa			(__NR_Linux + 347)
 
 /*
  * Offset of the last Linux o32 flavoured syscall
  */
-#define __NR_Linux_syscalls		346
+#define __NR_Linux_syscalls		347
 
 #endif /* _MIPS_SIM == _MIPS_SIM_ABI32 */
 
 #define __NR_O32_Linux			4000
-#define __NR_O32_Linux_syscalls		346
+#define __NR_O32_Linux_syscalls		347
 
 #if _MIPS_SIM == _MIPS_SIM_ABI64
 
@@ -690,16 +691,17 @@
 #define __NR_setns			(__NR_Linux + 303)
 #define __NR_process_vm_readv		(__NR_Linux + 304)
 #define __NR_process_vm_writev		(__NR_Linux + 305)
+#define __NR_msa			(__NR_Linux + 306)
 
 /*
  * Offset of the last Linux 64-bit flavoured syscall
  */
-#define __NR_Linux_syscalls		305
+#define __NR_Linux_syscalls		306
 
 #endif /* _MIPS_SIM == _MIPS_SIM_ABI64 */
 
 #define __NR_64_Linux			5000
-#define __NR_64_Linux_syscalls		305
+#define __NR_64_Linux_syscalls		306
 
 #if _MIPS_SIM == _MIPS_SIM_NABI32
 
@@ -1018,16 +1020,17 @@
 #define __NR_setns			(__NR_Linux + 308)
 #define __NR_process_vm_readv		(__NR_Linux + 309)
 #define __NR_process_vm_writev		(__NR_Linux + 310)
+#define __NR_msa			(__NR_Linux + 311)
 
 /*
  * Offset of the last N32 flavoured syscall
  */
-#define __NR_Linux_syscalls		310
+#define __NR_Linux_syscalls		311
 
 #endif /* _MIPS_SIM == _MIPS_SIM_NABI32 */
 
 #define __NR_N32_Linux			6000
-#define __NR_N32_Linux_syscalls		310
+#define __NR_N32_Linux_syscalls		311
 
 #ifdef __KERNEL__
 
diff --git a/arch/mips/kernel/binfmt_elfn32.c b/arch/mips/kernel/binfmt_elfn32.c
index 9fdd8bc..7497361 100644
--- a/arch/mips/kernel/binfmt_elfn32.c
+++ b/arch/mips/kernel/binfmt_elfn32.c
@@ -108,6 +108,28 @@ jiffies_to_compat_timeval(unsigned long jiffies, struct compat_timeval *value)
 	value->tv_usec = rem / NSEC_PER_USEC;
 }
 
+#ifdef CONFIG_MICROSTATE_ACCT
+#ifdef cputime_to_timeval
+#undef cputime_to_timeval
+#endif
+#define cputime_to_timeval(__ct, __val) (*(__val) = ns_to_timeval_compat(__ct))
+
+/*
+ * ns_to_timeval_compat - same as ns_to_timeval, except
+ * it returns struct compat_timeval.
+ */
+static struct compat_timeval ns_to_timeval_compat(const s64 nsec)
+{
+       struct timespec ts = ns_to_timespec(nsec);
+       struct compat_timeval tv;
+
+       tv.tv_sec = ts.tv_sec;
+       tv.tv_usec = (suseconds_t) ts.tv_nsec / 1000;
+
+       return tv;
+}
+#endif
+
 #define ELF_CORE_EFLAGS EF_MIPS_ABI2
 
 MODULE_DESCRIPTION("Binary format loader for compatibility with n32 Linux/MIPS binaries");
diff --git a/arch/mips/kernel/binfmt_elfo32.c b/arch/mips/kernel/binfmt_elfo32.c
index ff44823..a60a5ff 100644
--- a/arch/mips/kernel/binfmt_elfo32.c
+++ b/arch/mips/kernel/binfmt_elfo32.c
@@ -127,6 +127,28 @@ jiffies_to_compat_timeval(unsigned long jiffies, struct compat_timeval *value)
 	value->tv_usec = rem / NSEC_PER_USEC;
 }
 
+#ifdef CONFIG_MICROSTATE_ACCT
+#ifdef cputime_to_timeval
+#undef cputime_to_timeval
+#endif
+#define cputime_to_timeval(__ct, __val) (*(__val) = ns_to_timeval_compat(__ct))
+
+/*
+ * ns_to_timeval_compat - same as ns_to_timeval, except
+ * it returns struct compat_timeval.
+ */
+static struct compat_timeval ns_to_timeval_compat(const s64 nsec)
+{
+	struct timespec ts = ns_to_timespec(nsec);
+	struct compat_timeval tv;
+
+	tv.tv_sec = ts.tv_sec;
+	tv.tv_usec = (suseconds_t) ts.tv_nsec / 1000;
+
+	return tv;
+}
+#endif
+
 void elf32_core_copy_regs(elf_gregset_t grp, struct pt_regs *regs)
 {
 	int i;
diff --git a/arch/mips/kernel/entry.S b/arch/mips/kernel/entry.S
index 37acfa0..0482434 100644
--- a/arch/mips/kernel/entry.S
+++ b/arch/mips/kernel/entry.S
@@ -19,6 +19,7 @@
 #ifdef CONFIG_MIPS_MT_SMTC
 #include <asm/mipsmtregs.h>
 #endif
+#include <asm/msa_calling.h>
 
 #ifndef CONFIG_PREEMPT
 #define resume_kernel	restore_all
@@ -47,6 +48,7 @@ resume_userspace:
 	LONG_L	a2, TI_FLAGS($28)	# current->work
 	andi	t0, a2, _TIF_WORK_MASK	# (ignoring syscall_trace)
 	bnez	t0, work_pending
+	MSA_USER
 	j	restore_all
 
 #ifdef CONFIG_PREEMPT
@@ -76,6 +78,7 @@ FEXPORT(syscall_exit)
 	li	t0, _TIF_ALLWORK_MASK
 	and	t0, a2, t0
 	bnez	t0, syscall_exit_work
+	MSA_USER
 
 FEXPORT(restore_all)			# restore full frame
 #ifdef CONFIG_MIPS_MT_SMTC
@@ -153,6 +156,11 @@ work_resched:
 	LONG_L	a2, TI_FLAGS($28)
 	andi	t0, a2, _TIF_WORK_MASK	# is there any work to be done
 					# other than syscall tracing?
+#ifdef CONFIG_MICROSTATE_ACCT
+	bnez    t0, msa_user_skip
+	MSA_USER
+msa_user_skip:
+#endif
 	beqz	t0, restore_all
 	andi	t0, a2, _TIF_NEED_RESCHED
 	bnez	t0, work_resched
diff --git a/arch/mips/kernel/irq.c b/arch/mips/kernel/irq.c
index a5aa43d..f240ba1 100644
--- a/arch/mips/kernel/irq.c
+++ b/arch/mips/kernel/irq.c
@@ -141,10 +141,11 @@ static inline void check_stack_overflow(void) {}
 void __irq_entry do_IRQ(unsigned int irq)
 {
 	irq_enter();
+	msa_start_irq(irq);
 	check_stack_overflow();
 	if (!smtc_handle_on_other_cpu(irq))
 		generic_handle_irq(irq);
-	irq_exit();
+	msa_irq_exit(irq, msa_get_reg());
 }
 
 #ifdef CONFIG_MIPS_MT_SMTC_IRQAFF
@@ -156,9 +157,10 @@ void __irq_entry do_IRQ(unsigned int irq)
 void __irq_entry do_IRQ_no_affinity(unsigned int irq)
 {
 	irq_enter();
+	msa_start_irq(irq);
 	smtc_im_backstop(irq);
 	generic_handle_irq(irq);
-	irq_exit();
+	msa_irq_exit(irq, msa_get_reg());
 }
 
 #endif /* CONFIG_MIPS_MT_SMTC_IRQAFF */
diff --git a/arch/mips/kernel/scall32-o32.S b/arch/mips/kernel/scall32-o32.S
index a632bc1..bd4b9fb 100644
--- a/arch/mips/kernel/scall32-o32.S
+++ b/arch/mips/kernel/scall32-o32.S
@@ -20,6 +20,7 @@
 #include <asm/unistd.h>
 #include <asm/war.h>
 #include <asm/asm-offsets.h>
+#include <asm/msa_calling.h>
 
 /* Highest syscall used of any syscall flavour */
 #define MAX_SYSCALL_NO	__NR_O32_Linux + __NR_O32_Linux_syscalls
@@ -32,6 +33,7 @@ NESTED(handle_sys, PT_SIZE, sp)
 	STI
 	.set	at
 
+	MSA_KERNEL
 	lw	t1, PT_EPC(sp)		# skip syscall on return
 
 	subu	v0, v0, __NR_O32_Linux	# check syscall number
@@ -77,6 +79,7 @@ o32_syscall_exit:
 	and	t0, a2
 	bnez	t0, o32_syscall_exit_work
 
+	MSA_USER
 	j	restore_partial
 
 o32_syscall_exit_work:
@@ -593,6 +596,11 @@ einval:	li	v0, -ENOSYS
 	sys	sys_setns		2
 	sys	sys_process_vm_readv	6	/* 4345 */
 	sys	sys_process_vm_writev	6
+#ifdef CONFIG_MICROSTATE_ACCT
+	sys	sys_msa			3
+#else
+	sys	sys_msa			2
+#endif
 	.endm
 
 	/* We pre-compute the number of _instruction_ bytes needed to
diff --git a/arch/mips/kernel/scall64-64.S b/arch/mips/kernel/scall64-64.S
index 3b5a5e9..a22b6d4 100644
--- a/arch/mips/kernel/scall64-64.S
+++ b/arch/mips/kernel/scall64-64.S
@@ -19,6 +19,7 @@
 #include <asm/thread_info.h>
 #include <asm/unistd.h>
 #include <asm/war.h>
+#include <asm/msa_calling.h>
 
 #ifndef CONFIG_BINFMT_ELF32
 /* Neither O32 nor N32, so define handle_sys here */
@@ -37,6 +38,7 @@ NESTED(handle_sys64, PT_SIZE, sp)
 	TRACE_IRQS_ON_RELOAD
 	STI
 	.set	at
+	MSA_KERNEL
 #endif
 
 	dsubu	t0, v0, __NR_64_Linux	# check syscall number
@@ -80,6 +82,7 @@ n64_syscall_exit:
 	and	t0, a2, t0
 	bnez	t0, n64_syscall_exit_work
 
+	MSA_USER
 	j	restore_partial
 
 n64_syscall_exit_work:
@@ -432,4 +435,5 @@ sys_call_table:
 	PTR	sys_setns
 	PTR	sys_process_vm_readv
 	PTR	sys_process_vm_writev		/* 5305 */
+	PTR	sys_msa
 	.size	sys_call_table,.-sys_call_table
diff --git a/arch/mips/kernel/scall64-n32.S b/arch/mips/kernel/scall64-n32.S
index 6be6f70..22fb015 100644
--- a/arch/mips/kernel/scall64-n32.S
+++ b/arch/mips/kernel/scall64-n32.S
@@ -16,6 +16,7 @@
 #include <asm/stackframe.h>
 #include <asm/thread_info.h>
 #include <asm/unistd.h>
+#include <asm/msa_calling.h>
 
 /* This duplicates the definition from <linux/sched.h> */
 #define PT_TRACESYS	0x00000002	/* tracing system calls */
@@ -36,6 +37,7 @@ NESTED(handle_sysn32, PT_SIZE, sp)
 	TRACE_IRQS_ON_RELOAD
 	STI
 	.set	at
+	MSA_KERNEL
 #endif
 
 	dsubu	t0, v0, __NR_N32_Linux	# check syscall number
@@ -78,6 +80,7 @@ NESTED(handle_sysn32, PT_SIZE, sp)
 	and	t0, a2, t0
 	bnez	t0, n32_syscall_exit_work
 
+	MSA_USER
 	j	restore_partial
 
 n32_syscall_exit_work:
@@ -432,4 +435,5 @@ EXPORT(sysn32_call_table)
 	PTR	sys_setns
 	PTR	compat_sys_process_vm_readv
 	PTR	compat_sys_process_vm_writev	/* 6310 */
+	PTR	sys_msa
 	.size	sysn32_call_table,.-sysn32_call_table
diff --git a/arch/mips/kernel/scall64-o32.S b/arch/mips/kernel/scall64-o32.S
index 5422855..0824893 100644
--- a/arch/mips/kernel/scall64-o32.S
+++ b/arch/mips/kernel/scall64-o32.S
@@ -23,6 +23,7 @@
 #include <asm/thread_info.h>
 #include <asm/unistd.h>
 #include <asm/sysmips.h>
+#include <asm/msa_calling.h>
 
 	.align  5
 NESTED(handle_sys, PT_SIZE, sp)
@@ -31,6 +32,7 @@ NESTED(handle_sys, PT_SIZE, sp)
 	TRACE_IRQS_ON_RELOAD
 	STI
 	.set	at
+	MSA_KERNEL
 	ld	t1, PT_EPC(sp)		# skip syscall on return
 
 	dsubu	t0, v0, __NR_O32_Linux	# check syscall number
@@ -107,6 +109,7 @@ o32_syscall_exit:
 	and	t0, a2, t0
 	bnez	t0, o32_syscall_exit_work
 
+	MSA_USER
 	j	restore_partial
 
 o32_syscall_exit_work:
@@ -550,4 +553,5 @@ sys_call_table:
 	PTR	sys_setns
 	PTR	compat_sys_process_vm_readv	/* 4345 */
 	PTR	compat_sys_process_vm_writev
+	PTR	sys_msa
 	.size	sys_call_table,.-sys_call_table
diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index ba9376b..2c9505c 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -132,15 +132,33 @@ asmlinkage __cpuinit void start_secondary(void)
 	cpu_idle();
 }
 
+#ifdef CONFIG_MICROSTATE_ACCT
+/*
+ * Only caller of smp_call_function_interrupt() knows irq number.
+ * So smp_call_function_interrupt() should have irq argument.
+ */
+void __irq_entry smp_call_function_interrupt(int irq)
+#else
 /*
  * Call into both interrupt handlers, as we share the IPI for them
  */
 void __irq_entry smp_call_function_interrupt(void)
+#endif
 {
+#ifndef CONFIG_MICROSTATE_ACCT
+	/*
+	* When MSA isn't selected msa_start_irq() is empty and
+	* and we don't care what irq number would be passed
+	* to msa_start_irq()/msa_irq_exit().
+	*/
+       int irq = 0;
+#endif
+
 	irq_enter();
+	msa_start_irq(irq);
 	generic_smp_call_function_single_interrupt();
 	generic_smp_call_function_interrupt();
-	irq_exit();
+	msa_irq_exit(irq, msa_get_reg());
 }
 
 static void stop_this_cpu(void *dummy)
diff --git a/arch/mips/kernel/smtc.c b/arch/mips/kernel/smtc.c
index f5dd38f..9c077d1 100644
--- a/arch/mips/kernel/smtc.c
+++ b/arch/mips/kernel/smtc.c
@@ -946,10 +946,11 @@ static void __irq_entry smtc_clock_tick_interrupt(void)
 	int irq = MIPS_CPU_IRQ_BASE + 1;
 
 	irq_enter();
+	msa_start_irq(irq);
 	kstat_incr_irqs_this_cpu(irq, irq_to_desc(irq));
 	cd = &per_cpu(mips_clockevent_device, cpu);
 	cd->event_handler(cd);
-	irq_exit();
+	msa_irq_exit(irq, msa_get_reg());
 }
 
 void ipi_decode(struct smtc_ipi *pipi)
diff --git a/arch/mips/kernel/traps.c b/arch/mips/kernel/traps.c
index cfdaaa4..16b2ba3 100644
--- a/arch/mips/kernel/traps.c
+++ b/arch/mips/kernel/traps.c
@@ -888,6 +888,7 @@ asmlinkage void do_ri(struct pt_regs *regs)
 		return;
 
 	die_if_kernel("Reserved instruction in kernel code", regs);
+	msa_kernel();
 
 	if (unlikely(compute_return_epc(regs) < 0))
 		return;
@@ -986,6 +987,8 @@ asmlinkage void do_cpu(struct pt_regs *regs)
 
 	die_if_kernel("do_cpu invoked from kernel context!", regs);
 
+	msa_kernel();
+
 	cpid = (regs->cp0_cause >> CAUSEB_CE) & 3;
 
 	switch (cpid) {
diff --git a/arch/mips/mm/fault.c b/arch/mips/mm/fault.c
index c14f6df..a0706c4 100644
--- a/arch/mips/mm/fault.c
+++ b/arch/mips/mm/fault.c
@@ -44,6 +44,12 @@ asmlinkage void __kprobes do_page_fault(struct pt_regs *regs, unsigned long writ
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE |
 						 (write ? FAULT_FLAG_WRITE : 0);
 
+#ifdef CONFIG_MICROSTATE_ACCT
+	if (user_mode(regs))
+		msa_kernel();
+#endif
+
+
 #if 0
 	printk("Cpu%d[%s:%d:%0*lx:%ld:%0*lx]\n", raw_smp_processor_id(),
 	       current->comm, current->pid, field, address, write,
diff --git a/include/linux/msa.h b/include/linux/msa.h
index 2e840ac..ebcad4d 100644
--- a/include/linux/msa.h
+++ b/include/linux/msa.h
@@ -75,8 +75,13 @@ void msa_update_parent(struct task_struct *parent, struct task_struct *this);
 void msa_init(struct task_struct *p);
 void msa_set_timer(struct task_struct *p, int state);
 void msa_start_irq(int irq);
+void msa_start_irq_raw(int irq);
 void msa_continue_irq(int oldirq, int newirq);
 void msa_irq_exit(int irq, int is_going_to_user);
+void msa_irq_exit_raw(int irq);
+#ifdef CONFIG_MIPS
+void msa_irq_exit_raw(int irq);
+#endif
 asmlinkage void msa_kernel(void);
 asmlinkage void msa_user(void);
 
diff --git a/kernel/msa.c b/kernel/msa.c
index f207931..c67a7fd 100644
--- a/kernel/msa.c
+++ b/kernel/msa.c
@@ -39,6 +39,11 @@ struct msa_irq {
 	msa_time_t last_entered;
 };
 
+#ifdef CONFIG_MICROSTATE_C0_COUNT_REGISTER
+msa_time_t msa_cycles_last;
+u32 msa_last_count;
+__cacheline_aligned_in_smp DEFINE_SEQLOCK(msa_seqlock);
+#endif
 /*
  * Dummy this out for the moment.
  */
@@ -249,34 +254,18 @@ asmlinkage void msa_user(void)
 		msa_system_time(p, msa_to_cputime(delta));
 }
 
-/**
- * msa_start_irq: mark the start of an interrupt handler.
- * @irq: irq number being handled.
- *
- * Update the current task state to MSA_INTERRUPTED, and start
- * accumulating time to the interrupt handler for irq.
- *
- * Note that the irq_id does not have to be the actual irq, just some way
- * to uniquely identify the interrupt source that is less than NR_IRQ.
- * x86 uses the vector, for instance, since the IRQ numbers don't map
- * to all the relevant interrupt sources.
- */
-void msa_start_irq(int irq_id)
+static inline void _msa_start_irq(int irq, int nested)
 {
 	struct task_struct *p = current;
 	struct microstates *msp = &p->microstates;
 	msa_time_t now;
-	int nested;
 
-	BUG_ON(irq_id > NR_IRQS);
+	BUG_ON(irq > NR_IRQS);
 
 	/* we're in an interrupt handler... no possibility of preemption */
 	MSA_NOW(now);
 
-	nested = hardirq_count() - HARDIRQ_OFFSET;
-	BUG_ON(nested < 0);
-
-	__get_cpu_var(msa_irq)[irq_id].last_entered = now;
+	__get_cpu_var(msa_irq)[irq].last_entered = now;
 
 	if (!nested) {
 		msa_time_t delta = now - msp->last_change;
@@ -294,6 +283,37 @@ void msa_start_irq(int irq_id)
 	}
 }
 
+/*
+ * msa_start_irq: mark the start of an interrupt handler.
+ * @irq: irq number being handled.
+ *
+ * Update the current task state to MSA_INTERRUPTED, and start
+ * accumulating time to the interrupt handler for irq.
+ */
+void msa_start_irq(int irq)
+{
+	int nested;
+
+	/* we're in an interrupt handler... no possibility of preemption */
+	nested = hardirq_count() - HARDIRQ_OFFSET;
+	BUG_ON(nested < 0);
+
+	_msa_start_irq(irq, nested);
+}
+
+/*
+ * Same as msa_start_irq() except it's called from irq handler that
+ * don't call irq_enter/irq_exit.
+ */
+void msa_start_irq_raw(int irq)
+{
+	int nested;
+
+	/* we're in an interrupt handler... no possibility of preemption */
+	nested = hardirq_count();
+	_msa_start_irq(irq, nested);
+}
+
 /**
  * msa_continue_irq: While remaining in MSA_INTERRUPTED state, switch
  * to a new IRQ.
@@ -481,6 +501,46 @@ SYSCALL_DEFINE3(msa, int, ntimers, int, which, msa_time_t __user *, timers)
 	return 0;
 }
 
+/*
+ * Same as msa_irq_exit() except it's called from irq handler that
+ * don't call irq_enter/irq_exit. So don't call irq_exit() here and
+ * don't account softirqs time.
+ */
+void msa_irq_exit_raw(int irq_id)
+{
+	struct task_struct *p = current;
+	struct microstates *mp = &p->microstates;
+	u64 *cpustat = kcpustat_this_cpu->cpustat;
+	msa_time_t now, delta;
+	struct msa_irq *mip;
+	int nested;
+
+	BUG_ON(irq_id > NR_IRQS);
+
+	mip = get_cpu_var(msa_irq);
+	nested = hardirq_count();
+	BUG_ON(nested < 0);
+
+	MSA_NOW(now);
+	delta = now - mip[irq_id].last_entered;
+	mip[irq_id].times += delta;
+	if (!nested) {
+		msa_time_t before = now;
+
+		cpustat[CPUTIME_IRQ] +=	msa_to_cputime64(delta);
+
+		MSA_NOW(now);
+		delta = now - before;
+		if (mp->cur_state == MSA_INTERRUPTED) {
+			mp->timers[mp->cur_state] += now - mp->last_change;
+			mp->last_change = now;
+			mp->cur_state = mp->next_state;
+		}
+	}
+
+	put_cpu_var(msa_irq);
+}
+
 #ifdef CONFIG_PROC_FS
 
 /*
-- 
1.7.9.7

