From 4d0192f546720da77c4c507668ba43a16f59cbb8 Mon Sep 17 00:00:00 2001
From: Corey Minyard <cminyard@mvista.com>
Date: Thu, 6 May 2010 14:26:37 -0500
Subject: [PATCH 01/18] msa-base

Source: git://microstate.git.sourceforge.net/gitroot/microstate/microstate
MR: 37518
Type: Integration
Disposition: Local
ChangeID: 4c40b23656d6684c120e7e2190a97d4a36caf83f
Description:

Microstate accounting is the ability to track CPU usage for processes
and interrupts accurately and at a fine grain.  This gives the ability
to know exactly how much CPU each process and interrupt used.  It also
tracks how much time processes spend waiting on various things, like
sitting on the run queue, paging, sleeping, waiting on futexes, and other
things.

This is important for a number of reasons.  Some task loads,
especially ones driven by timers, can be far off reality with a
timer-driven statistical measurement.  Short-term measurements are
impossible as they can be very inaccurate.

In addition, a useful profiler can be written that very accurately
profiles a task's CPU usage, and how much time the task spends paging,
sleeping, waiting on futexes, etc.

Signed-off-by: Corey Minyard <cminyard@mvista.com>
Signed-off-by: Dale Farnsworth <dfarnsworth@mvista.com>
---
 Documentation/MicrostateAccounting |  152 ++++++++++++
 fs/select.c                        |    4 +-
 include/asm-generic/msa.h          |   19 ++
 include/linux/msa.h                |  105 +++++++++
 include/linux/sched.h              |    4 +
 init/main.c                        |    1 +
 kernel/Makefile                    |    2 +-
 kernel/exit.c                      |    2 +
 kernel/fork.c                      |    2 +
 kernel/futex.c                     |    2 +
 kernel/msa.c                       |  458 ++++++++++++++++++++++++++++++++++++
 kernel/sched/core.c                |    2 +
 mm/memory.c                        |    7 +
 13 files changed, 758 insertions(+), 2 deletions(-)
 create mode 100644 Documentation/MicrostateAccounting
 create mode 100644 include/asm-generic/msa.h
 create mode 100644 include/linux/msa.h
 create mode 100644 kernel/msa.c

diff --git a/Documentation/MicrostateAccounting b/Documentation/MicrostateAccounting
new file mode 100644
index 0000000..63a8b53
--- /dev/null
+++ b/Documentation/MicrostateAccounting
@@ -0,0 +1,152 @@
+			Microstate Accounting
+			---------------------
+
+Microstate accounting provides a way to have accurate CPU and wait
+time for tasks and interrupts.  This provides a number of advantages
+over standard accounting:
+
+* Standard accounting is inaccurate.  Normal accounting is only done
+  at timer ticks.  The task running when the tick occurs is assigned
+  the entire timeslice worth of CPU, no matter how much CPU it
+  actually used.  This means that the reported values can be very
+  inaccurate, especially for operations driven by timers, since they
+  will run right after a timer tick.
+
+* Standard accounting provides no information about where the task
+  has spent time besides running.
+
+* Standard accounting provides no information about time spend in
+  interrupts
+
+Microstate accounting provides accurate information on time used by
+the task by tying into the places where task switches and interrupts
+occur.  It counts time the task spent running in userland and in the
+kernel, and it counts the time the task spends waiting on various
+things.  It also counts time spent running in interrupts on a
+per-interrupt basis.
+
+Note that all times reported by Microstate Accounting are in nanoseconds.
+
+
+Enabling
+--------
+
+To enable Microstate Accouting, enable the MICROSTATE_ACCT config,
+generally in the debug menu of the config.  You will then be prompted
+to choose a clocksource.  The generic sched_clock clocksource is a
+fairly safe choice.  You may have a per-architecture clock (like the
+TSC), only enable that if you know it is a stable clocksource on all
+systems on which the kernel will be used.
+
+
+Problems
+--------
+
+The big issue with Microstate Accouting is the overhead.  Every
+syscall, every interrupt, every task switch, etc. has a little
+overhead added to account for the time.  So it is less efficient than
+normal accouting.  This has been measured on some systems and is
+generally substantially less than a percent, but it is a lot more than
+just accouting in the timer tick.
+
+
+Uses
+----
+
+So why would you want to use this feature?  A couple of things have
+been done with this feature already:
+
+ Accurate, detailed profiling.  With Microstate Accouting, profiling
+ can be done on a time basis and it will be very accurate.  In
+ addition, information about tie spent waiting, paging, etc. can be
+ collected to look for issues besides just CPU hotspots.  And a single
+ call to a function is enough to do a reasonable profile on it.
+
+ CPU usage watching.  Along with setitimer() and SIGVTALRM, or an
+ external thread watching, a framework can be built to start an
+ operation and make sure it doesn't use more CPU than expected, and
+ abort the operation if it does.  This is simply not possible with
+ standard accouting, too many false positives happen where a thread has
+ used very little CPU but since it happened to be running during a
+ couple of timer tick, it gets a lot of CPU reported.
+
+In addition to this, because information about waiting and paging and
+such are collected, and because interrupt time is collected, this is
+useful for generally watching a system to help find problem and
+eliminate bottlenecks.
+
+
+Task Information
+----------------
+The following time values are kept for tasks:
+
+ONCPU_USER      Time spent running in userland
+ONCPU_SYS       Time spent running in the kernel on behalf of the task
+INTERRUPTIBLE   Time spent in an interruptible wait
+UNINTERRUPTIBLE Time spent in an uninterruptible wait
+INTERRUPTED     Time the task was interrupted by an interrupt
+RUNQUEUE        Time spent on the run queue
+STOPPED         Time in stopped state
+ZOMBIE          Time in zombie state
+SLP_POLL        Time spent in select/poll/etc.
+SLP_PAGING      Time spent waiting for data to be paged in/out.
+SLP_FUTEX       Time spent waiting on a futex.
+
+This information can be fetched in two different ways: via a syscall
+and via the proc filesystem.
+
+The msa syscall has the following format:
+
+	msa(int ntimers, int which, msa_time_t *timers)
+
+ntimers is the number of timers to fetch, and the more common timers are
+first in the array.
+
+which can be MSA_SELF for the all threads in the running process, MSA_THREAD
+for the running thread, and MSA_CHILDREN for all child processes.
+
+timers is an array of time values, in nanoseconds, indexed by the
+msa_thread_state enum.  This is returned.
+
+Each process has an "msa" file in /proc/<pid>/... directory, and each
+thread has one, too.  This contains information in the format:
+
+ State:      Poll
+ Now:              232598740315
+ ONCPU_USER              1261115
+ ONCPU_SYS           10385737584
+ INTERRUPTIBLE          70269326
+ UNINTERRUPTIBLE     26479449181
+ INTERRUPTED            33478535
+ RUNQUEUE                  31424
+ STOPPED                       0
+ ZOMBIE                        0
+ SLP_POLL            93200925229
+ SLP_PAGING            245287043
+ SLP_FUTEX                     0
+
+Where "State" shows the current state and "Now" is the current MSA
+time value.
+
+
+Interrupt Information
+---------------------
+
+/proc/msa_irq_time contains information about time spent in each
+interrupt on each cpu, in the format:
+
+Now:   3464616756951
+                 CPU0            CPU1
+ 48:           263050               0
+ 49:            13799               0
+ 51:          6957282               0
+ 60:            23524               0
+ 62:           155218               0
+ 73:         33442572               0
+ 81:         65786233               0
+ 89:             5586               0
+239:        928126476       817359081
+251:         28414698        79626107
+
+"Now" is the current MSA time value, and the values are the time in
+nanoseconds spent in each interrupt.
diff --git a/fs/select.c b/fs/select.c
index 17d33d0..0b45f15 100644
--- a/fs/select.c
+++ b/fs/select.c
@@ -235,8 +235,10 @@ int poll_schedule_timeout(struct poll_wqueues *pwq, int state,
 	int rc = -EINTR;
 
 	set_current_state(state);
-	if (!pwq->triggered)
+	if (!pwq->triggered) {
+		msa_next_state(current, MSA_POLL_SLEEP);
 		rc = schedule_hrtimeout_range(expires, slack, HRTIMER_MODE_ABS);
+	}
 	__set_current_state(TASK_RUNNING);
 
 	/*
diff --git a/include/asm-generic/msa.h b/include/asm-generic/msa.h
new file mode 100644
index 0000000..ded8090
--- /dev/null
+++ b/include/asm-generic/msa.h
@@ -0,0 +1,19 @@
+/*
+ * asm-generic/msa.h
+ * Provide a generic time-of-day clock for
+ * microstate accounting.
+ */
+
+#ifndef _ASM_GENERIC_MSA_H
+#define _ASM_GENERIC_MSA_H
+
+# ifdef __KERNEL__
+/*
+ * Every architecture is supposed to provide sched_clock, a free-running,
+ * non-wrapping, per-cpu clock in nanoseconds.
+ */
+#  define MSA_NOW(now)  do { (now) = sched_clock(); } while (0)
+#  define MSA_TO_NSEC(clk) (clk)
+# endif
+
+#endif /* _ASM_GENERIC_MSA_H */
diff --git a/include/linux/msa.h b/include/linux/msa.h
new file mode 100644
index 0000000..2e840ac
--- /dev/null
+++ b/include/linux/msa.h
@@ -0,0 +1,105 @@
+/*
+ * msa.h
+ *   microstate accounting.  See Documentation/MicrostateAccounting for info.
+ *
+ * Copyright (c) Peter Chubb 2005
+ *  UNSW and National ICT Australia
+ * Copyright (c) 2010 MontaVista Software, LLC
+ *  Corey Minyard <minyard@mvista.com>, <minyard@acm.org>, <source@mvista.com>
+ */
+
+#ifndef _LINUX_MSA_H
+#define _LINUX_MSA_H
+
+#include <linux/types.h>
+
+typedef uint64_t msa_time_t;
+
+/*
+ * Tracked states
+ */
+enum msa_thread_state {
+	MSA_UNKNOWN = -1,
+	MSA_ONCPU_USER,
+	MSA_ONCPU_SYS,
+	MSA_INTERRUPTIBLE_SLEEP,
+	MSA_UNINTERRUPTIBLE_SLEEP,
+	MSA_ONRUNQUEUE,
+	MSA_ZOMBIE,
+	MSA_STOPPED,
+	MSA_INTERRUPTED,
+	MSA_PAGING_SLEEP,
+	MSA_FUTEX_SLEEP,
+	MSA_POLL_SLEEP,
+
+	MSA_NR_STATES /* Must be last */
+};
+
+/* Values for "which" in the msa syscall */
+#define MSA_THREAD	0	/* Just the current thread */
+#define MSA_CHILDREN	1	/* All dead and waited-for threads */
+#define MSA_SELF	2	/* All threads in current process */
+#define MSA_GET_NOW	3	/* Current MSA timer in the first value */
+
+#ifdef __KERNEL__
+
+#include <linux/compiler.h> /* For __user */
+
+extern long asmlinkage sys_msa(int ntimers, int which,
+			       msa_time_t __user *timers);
+
+/* Forward definition... */
+struct task_struct;
+
+#ifdef CONFIG_MICROSTATE_ACCT
+
+#include <asm/msa.h>
+
+/*
+ * Times are tracked for the current task in timers[], and for the
+ * current task's children in child_timers[] (accumulated at wait()
+ * time).  One of these structures is added to every struct task_struct.
+ */
+struct microstates {
+	enum msa_thread_state cur_state;
+	enum msa_thread_state next_state;
+	msa_time_t last_change;	/* When the last change happened */
+	msa_time_t timers[MSA_NR_STATES];
+	msa_time_t child_timers[MSA_NR_STATES];
+};
+
+/* Has to be a macro because microstates is part of task_struct */
+#define msa_next_state(p, s) do { (p)->microstates.next_state = s; } while(0)
+void msa_switch(struct task_struct *prev, struct task_struct *next);
+void msa_update_parent(struct task_struct *parent, struct task_struct *this);
+void msa_init(struct task_struct *p);
+void msa_set_timer(struct task_struct *p, int state);
+void msa_start_irq(int irq);
+void msa_continue_irq(int oldirq, int newirq);
+void msa_irq_exit(int irq, int is_going_to_user);
+asmlinkage void msa_kernel(void);
+asmlinkage void msa_user(void);
+
+#else /* CONFIG_MICROSTATE_ACCT */
+
+/*
+ * Dummy functions to do nothing, for when MICROSTATE_ACCT is configured off.
+ */
+#define msa_next_state(p, s) do { } while (0)
+static inline void msa_switch(struct task_struct *prev,
+			      struct task_struct *next) { }
+static inline void msa_update_parent(struct task_struct *parent,
+				     struct task_struct *this) { }
+
+static inline void msa_init(struct task_struct *p) { }
+static inline void msa_set_timer(struct task_struct *p, int state) { }
+static inline void msa_start_irq(int irq) { }
+static inline void msa_continue_irq(int oldirq, int newirq) { }
+#define msa_irq_exit(irq, is_going_to_user) irq_exit()
+static inline void msa_kernel(void) { }
+static inline void msa_user(void) { }
+
+#endif /* CONFIG_MICROSTATE_ACCT */
+#endif /* __KERNEL__ */
+
+#endif /* _LINUX_MSA_H */
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 81a173c..107f02e 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -60,6 +60,7 @@ struct sched_param {
 #include <linux/thread_info.h>
 #include <linux/cpumask.h>
 #include <linux/errno.h>
+#include <linux/msa.h>
 #include <linux/nodemask.h>
 #include <linux/mm_types.h>
 
@@ -1397,6 +1398,9 @@ struct task_struct {
 	unsigned long nvcsw, nivcsw; /* context switch counts */
 	struct timespec start_time; 		/* monotonic time */
 	struct timespec real_start_time;	/* boot based time */
+#ifdef CONFIG_MICROSTATE_ACCT
+	struct microstates microstates;
+#endif
 /* mm fault and swap info: this can arguably be seen as either mm-specific or thread-specific */
 	unsigned long min_flt, maj_flt;
 
diff --git a/init/main.c b/init/main.c
index 44b2433..3a694e8 100644
--- a/init/main.c
+++ b/init/main.c
@@ -554,6 +554,7 @@ asmlinkage void __init start_kernel(void)
 	time_init();
 	profile_init();
 	call_function_init();
+	msa_init(&init_task);
 	if (!irqs_disabled())
 		printk(KERN_CRIT "start_kernel(): bug: interrupts were "
 				 "enabled early\n");
diff --git a/kernel/Makefile b/kernel/Makefile
index cb41b95..8e3da1c 100644
--- a/kernel/Makefile
+++ b/kernel/Makefile
@@ -10,7 +10,7 @@ obj-y     = fork.o exec_domain.o panic.o printk.o \
 	    kthread.o wait.o kfifo.o sys_ni.o posix-cpu-timers.o mutex.o \
 	    hrtimer.o rwsem.o nsproxy.o srcu.o semaphore.o \
 	    notifier.o ksysfs.o cred.o \
-	    async.o range.o groups.o
+	    async.o range.o groups.o msa.o
 
 ifdef CONFIG_FUNCTION_TRACER
 # Do not trace debug files and internal ftrace files
diff --git a/kernel/exit.c b/kernel/exit.c
index d8bd3b42..5bf7505 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -200,6 +200,8 @@ repeat:
 			leader->exit_state = EXIT_DEAD;
 	}
 
+	msa_update_parent(p->parent, p);
+
 	write_unlock_irq(&tasklist_lock);
 	release_thread(p);
 	call_rcu(&p->rcu, delayed_put_task_struct);
diff --git a/kernel/fork.c b/kernel/fork.c
index 687a15d..5977caa 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1204,6 +1204,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->vfork_done = NULL;
 	spin_lock_init(&p->alloc_lock);
 
+	msa_init(p);
+
 	init_sigpending(&p->pending);
 
 	p->utime = p->stime = p->gtime = 0;
diff --git a/kernel/futex.c b/kernel/futex.c
index e2b0fb9..ea014d2 100644
--- a/kernel/futex.c
+++ b/kernel/futex.c
@@ -1765,6 +1765,7 @@ static void futex_wait_queue_me(struct futex_hash_bucket *hb, struct futex_q *q,
 	 * queue_me() calls spin_unlock() upon completion, both serializing
 	 * access to the hash list and forcing another memory barrier.
 	 */
+	msa_next_state(current, MSA_FUTEX_SLEEP);
 	set_current_state(TASK_INTERRUPTIBLE);
 	queue_me(q, hb);
 
@@ -2023,6 +2024,7 @@ retry_private:
 	/*
 	 * Block on the PI mutex:
 	 */
+	msa_next_state(current, MSA_FUTEX_SLEEP);
 	if (!trylock)
 		ret = rt_mutex_timed_lock(&q.pi_state->pi_mutex, to, 1);
 	else {
diff --git a/kernel/msa.c b/kernel/msa.c
new file mode 100644
index 0000000..039e9ab
--- /dev/null
+++ b/kernel/msa.c
@@ -0,0 +1,458 @@
+/*
+ * Microstate accounting.
+ * Try to account for various states much more accurately than
+ * the normal code does.
+ *
+ * Copyright (c) Peter Chubb 2005
+ *  UNSW and National ICT Australia
+ * Copyright (c) 2010 MontaVista Software, LLC
+ *  Corey Minyard <minyard@mvista.com>, <minyard@acm.org>, <source@mvista.com>
+ * This code is released under the Gnu Public Licence, version 2.
+ */
+
+
+#include <linux/types.h>
+#include <linux/errno.h>
+#include <linux/linkage.h>
+#include <linux/msa.h>
+#include <linux/syscalls.h>
+#ifdef CONFIG_MICROSTATE_ACCT
+#include <linux/irq.h>
+#include <linux/hardirq.h>
+#include <linux/sched.h>
+#include <linux/jiffies.h>
+#include <linux/acct.h>
+#include <linux/tsacct_kern.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+#include <linux/init.h>
+#include <linux/bug.h>
+
+#include <asm/uaccess.h>
+
+/*
+ * Track time spend in interrupt handlers.
+ */
+struct msa_irq {
+	msa_time_t times;
+	msa_time_t last_entered;
+};
+
+/*
+ * Time spent in interrupt handlers
+ */
+static DEFINE_PER_CPU(struct msa_irq[NR_IRQS + 1], msa_irq);
+
+
+/**
+ * msa_switch: Update microstate timers when switching from one task to
+ * another.
+ *
+ * @prev, @next:  The prev task is coming off the processor;
+ *                the new task is about to run on the processor.
+ *
+ * Update the times in both prev and next.  It may be necessary to infer the
+ * next state for each task.
+ *
+ */
+void msa_switch(struct task_struct *prev, struct task_struct *next)
+{
+	struct microstates *prev_msp = &prev->microstates;
+	struct microstates *next_msp = &next->microstates;
+	msa_time_t now;
+	enum msa_thread_state next_state;
+	unsigned long flags;
+
+	local_irq_save(flags);
+
+	MSA_NOW(now);
+
+	next_msp->timers[next_msp->cur_state] += now - next_msp->last_change;
+	prev_msp->timers[prev_msp->cur_state] += now - prev_msp->last_change;
+
+	/*
+	 * Update states, state is sort of a bitmask, except that
+	 * TASK_RUNNING is 0.
+	 */
+	if (prev->state == TASK_RUNNING)
+		next_state = MSA_ONRUNQUEUE;
+	else if (prev->state & TASK_INTERRUPTIBLE)
+		next_state = MSA_INTERRUPTIBLE_SLEEP;
+	else if (prev->state & TASK_UNINTERRUPTIBLE)
+		next_state = MSA_UNINTERRUPTIBLE_SLEEP;
+	else if (prev->state & (TASK_STOPPED | TASK_TRACED))
+		next_state = MSA_STOPPED;
+	else if (prev->state & (TASK_DEAD | EXIT_DEAD | EXIT_ZOMBIE))
+		next_state = MSA_ZOMBIE;
+	else {
+		printk(KERN_WARNING "msa: Setting UNKNOWN state from %ld\n",
+		       prev->state);
+		WARN_ON(1);
+		next_state = MSA_UNKNOWN;
+	}
+
+	/* special states */
+	switch (prev_msp->next_state) {
+	case MSA_PAGING_SLEEP:
+	case MSA_FUTEX_SLEEP:
+	case MSA_POLL_SLEEP:
+		if (prev->state & TASK_INTERRUPTIBLE ||
+		    prev->state & TASK_UNINTERRUPTIBLE)
+			next_state = prev_msp->next_state;
+	default:
+		break;
+	}
+
+	prev_msp->next_state = prev_msp->cur_state;
+	prev_msp->cur_state = next_state;
+	prev_msp->last_change = now;
+
+	next_msp->last_change = now;
+
+	WARN_ON(next_msp->next_state == MSA_UNKNOWN);
+	next_msp->cur_state = next_msp->next_state;
+	if (next_msp->cur_state != MSA_ONCPU_USER)
+		next_msp->cur_state = MSA_ONCPU_SYS;
+	next_msp->next_state = MSA_UNKNOWN;
+
+	local_irq_restore(flags);
+}
+
+/**
+ * msa_init:  Initialise the struct microstates in a new task
+ * @p: pointer to the struct task_struct to be initialised
+ *
+ * This function is called from copy_process().
+ * It initialises the microstate timers to zero, and sets the
+ * current state to MSA_UNINTERRUPTIBLE_SLEEP.
+ */
+void msa_init(struct task_struct *p)
+{
+	struct microstates *msp = &p->microstates;
+
+	memset(msp, 0, sizeof *msp);
+	MSA_NOW(msp->last_change);
+	msp->cur_state = MSA_UNINTERRUPTIBLE_SLEEP;
+	msp->next_state = MSA_ONCPU_SYS;
+}
+
+/**
+ * __msa_set_timer: Helper function to update microstate times.
+ * &msp:  Pointer to the struct microstates to update
+ * next_state: the state being changed to.
+ *
+ * The time spent in the current state is updated, and the time of
+ * last state change set to MSA_NOW().  Then the current state is updated
+ * to next_state.
+ */
+static inline msa_time_t __msa_set_timer(struct microstates *msp, int next_state)
+{
+	unsigned long flags;
+	msa_time_t now, delta;
+
+	local_irq_save(flags);
+	MSA_NOW(now);
+	delta = now - msp->last_change;
+	msp->timers[msp->cur_state] += delta;
+	msp->last_change = now;
+	msp->cur_state = next_state;
+	local_irq_restore(flags);
+
+	return delta;
+}
+
+/**
+ * __msa_set_timer_onswitch: same as __msa_set_timer, but it do timing
+ * stuff only when state changes.
+ */
+static inline msa_time_t __msa_set_timer_onswitch(struct microstates *msp,
+						  int next_state)
+{
+	unsigned long flags;
+	msa_time_t now, delta;
+
+	local_irq_save(flags);
+	delta = 0;
+	if (msp->cur_state == next_state)
+		goto out;
+	MSA_NOW(now);
+	delta = now - msp->last_change;
+	msp->timers[msp->cur_state] += delta;
+	msp->last_change = now;
+	msp->cur_state = next_state;
+out:
+	local_irq_restore(flags);
+	return delta;
+}
+
+/**
+ * msa_set_timer:  Time stamp an explicit state change.
+ * @p: pointer to the task that has just changed state.
+ * @next_state: the state being changed to.
+ *
+ * This function is called, e.g., from __activate_task(), when an
+ * immediate state change happens.
+ */
+void msa_set_timer(struct task_struct *p, int next_state)
+{
+	struct microstates *msp = &current->microstates;
+	__msa_set_timer(msp, MSA_ONCPU_SYS);
+}
+
+/*
+ * Helper routines, to be called from assembly language stubs
+ */
+
+/**
+ * msa_kernel: change state to MSA_ONCPU_SYS.
+ *
+ * Should be called upon every entry in the kernel from user space.
+ */
+asmlinkage void msa_kernel(void)
+{
+	struct microstates *msp = &current->microstates;
+	__msa_set_timer(msp, MSA_ONCPU_SYS);
+}
+
+/**
+ * msa_user: change state out of MSA_ONCPU_SYS
+ *
+ * Called when about to leave the kernel.
+ */
+asmlinkage void msa_user(void)
+{
+	struct microstates *msp = &current->microstates;
+	__msa_set_timer(msp, MSA_ONCPU_USER);
+}
+
+/**
+ * msa_start_irq: mark the start of an interrupt handler.
+ * @irq: irq number being handled.
+ *
+ * Update the current task state to MSA_INTERRUPTED, and start
+ * accumulating time to the interrupt handler for irq.
+ *
+ * Note that the irq_id does not have to be the actual irq, just some way
+ * to uniquely identify the interrupt source that is less than NR_IRQ.
+ * x86 uses the vector, for instance, since the IRQ numbers don't map
+ * to all the relevant interrupt sources.
+ */
+void msa_start_irq(int irq_id)
+{
+	struct task_struct *p = current;
+	struct microstates *msp = &p->microstates;
+	msa_time_t now;
+	int nested;
+
+	BUG_ON(irq_id > NR_IRQS);
+
+	/* we're in an interrupt handler... no possibility of preemption */
+	MSA_NOW(now);
+
+	nested = hardirq_count() - HARDIRQ_OFFSET;
+	BUG_ON(nested < 0);
+
+	__get_cpu_var(msa_irq)[irq_id].last_entered = now;
+
+	if (!nested) {
+		msa_time_t delta = now - msp->last_change;
+		msp->timers[msp->cur_state] += delta;
+		msp->last_change = now;
+		if (msp->cur_state == MSA_ONCPU_USER
+				|| msp->cur_state == MSA_ONCPU_SYS) {
+			msp->next_state = msp->cur_state;
+			msp->cur_state = MSA_INTERRUPTED;
+		}
+	}
+}
+
+/**
+ * msa_continue_irq: While remaining in MSA_INTERRUPTED state, switch
+ * to a new IRQ.
+ *
+ * @oldirq: the irq that was just serviced
+ * @newirq: the irq that is about to be serviced.
+ *
+ * Architectures such as IA64 can handle more than one interrupt
+ * without allowing the interrupted process to continue.  This function
+ * is called when switching to a new interrupt.
+ */
+void msa_continue_irq(int oldirq_id, int newirq_id)
+{
+	msa_time_t now;
+	struct msa_irq *mip;
+
+	BUG_ON(oldirq_id > NR_IRQS);
+	BUG_ON(newirq_id > NR_IRQS);
+
+	MSA_NOW(now);
+	/* we're in an interrupt handler... no possibility of preemption */
+	BUG_ON(!in_interrupt());
+	mip = __get_cpu_var(msa_irq);
+
+	mip[oldirq_id].times +=  now - mip[oldirq_id].last_entered;
+	mip[newirq_id].last_entered = now;
+}
+
+/**
+ * msa_finish_irq: end processing for an interrupt.
+ * @irq: the interrupt that was just serviced.
+ *
+ * Update the time spent handling irq, then update the current task's
+ * state to MSA_ONCPU_USER or MSA_ONCPU_SYS.
+ *
+ * This MUST be called instead of irq_exit() whenever msa_start_irq()
+ * was called for a given irq.  irq_exit() is implied by this function.
+ *
+ * See the notes in msa_start_irq() for info about irq_id
+ */
+void msa_irq_exit(int irq_id, int is_going_to_user)
+{
+	struct task_struct *p = current;
+	struct microstates *msp = &p->microstates;
+	msa_time_t now, delta;
+	struct msa_irq *mip;
+	int nested;
+
+	BUG_ON(irq_id > NR_IRQS);
+
+	mip = get_cpu_var(msa_irq);
+	nested = hardirq_count() - HARDIRQ_OFFSET;
+	BUG_ON(nested < 0);
+
+	MSA_NOW(now);
+	delta = now - mip[irq_id].last_entered;
+	mip[irq_id].times += delta;
+
+	irq_exit();
+
+	if (!nested) {
+		msa_time_t before = now;
+		MSA_NOW(now);
+		delta = now - before;
+		msp->timers[msp->cur_state] += now - msp->last_change;
+		msp->last_change = now;
+		if (is_going_to_user)
+			msp->cur_state = MSA_ONCPU_USER;
+		else
+			msp->cur_state = MSA_ONCPU_SYS;
+	}
+
+	put_cpu_var(msa_irq);
+}
+
+/**
+ * msa_update_parent:  Accumulate child times into parent, after zombie is over.
+ * @parent: pointer to parent task
+ * @this: pointer to task that is now a zombie
+ *
+ * Called from release_task(). (Note: it may be better to call this
+ * from wait_zombie())
+ */
+void msa_update_parent(struct task_struct *parent, struct task_struct *this)
+{
+	enum msa_thread_state s;
+	msa_time_t *pmsp = parent->microstates.child_timers;
+	struct microstates *msp = &this->microstates;
+	msa_time_t *msc = msp->timers;
+	msa_time_t *msgc = msp->child_timers;
+
+	/*
+	 * State could be MSA_ZOMBIE (if parent is interested)
+	 * or something else (if the parent isn't interested)
+	 */
+	__msa_set_timer(msp, msp->cur_state);
+
+	for (s = 0; s < MSA_NR_STATES; s++)
+		*pmsp++ += *msc++ + *msgc++;
+}
+
+/**
+ * sys_msa: get microstate data for self or waited-for children.
+ * @ntimers: the number of timers requested
+ * @which: which set of timers is wanted.
+ * @timers: pointer in user space to an array of timers.
+ *
+ * 'which' can take the values
+ *   MSA_THREAD: return times for current thread only
+ *   MSA_SELF:  return times for current process,
+ *		summing over all live threads
+ *   MSA_CHILDREN: return sum of times for all dead children.
+ *   MSA_GET_NOW: return the current msa timer value in the first value.
+ *
+ * The timers are ordered so that the most interesting ones are first.
+ * Thus a user program can ask for only the  few most interesting ones
+ * if it wishes.  Also, we can add more in the kernel as we need
+ * to without invalidating user code.
+ */
+SYSCALL_DEFINE3(msa, int, ntimers, int, which, msa_time_t __user *, timers)
+{
+	msa_time_t *tp;
+	int i;
+	struct microstates *msp = &current->microstates;
+	struct microstates out;
+
+	WARN_ON(current->microstates.cur_state != MSA_ONCPU_SYS);
+
+	if (ntimers <= 0 || ntimers > MSA_NR_STATES)
+		return -EINVAL;
+
+	switch (which) {
+	case MSA_SELF:
+	case MSA_THREAD:
+		preempt_disable();
+		__msa_set_timer(msp, MSA_ONCPU_SYS);
+
+		if (which == MSA_SELF) {
+			struct task_struct *task;
+			struct task_struct *leader_task;
+			msa_time_t *tp1;
+
+			memset(out.timers, 0, sizeof(out.timers));
+			read_lock(&tasklist_lock);
+			leader_task = task = current->group_leader;
+			do {
+				tp = task->microstates.timers;
+				tp1 = out.timers;
+				for (i = 0; i < ntimers; i++)
+					*tp1++ += *tp++;
+			} while ((task = next_thread(task)) != leader_task);
+			read_unlock(&tasklist_lock);
+			tp = out.timers;
+		} else
+			tp = msp->timers;
+		preempt_enable();
+		break;
+
+	case MSA_CHILDREN:
+		tp =  msp->child_timers;
+		break;
+
+	case MSA_GET_NOW:
+		ntimers = 1;
+		tp = out.timers;
+		MSA_NOW(*tp);
+		break;
+
+	default:
+		return -EINVAL;
+	}
+
+	for (i = 0; i < ntimers; i++) {
+		__u64 x = MSA_TO_NSEC(*tp++);
+		if (copy_to_user(timers++, &x, sizeof x))
+			return -EFAULT;
+	}
+
+	return 0;
+}
+
+#else
+/*
+ * Stub for sys_msa when CONFIG_MICROSTATE_ACCT is off.
+ */
+SYSCALL_DEFINE3(msa, int, ntimers, int, which, msa_time_t __user *, timers)
+{
+	return -ENOSYS;
+}
+#endif /* CONFIG_MICROSTATE_ACCT */
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index e5212ae..14b6acb 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -715,6 +715,7 @@ static void set_load_weight(struct task_struct *p)
 static void enqueue_task(struct rq *rq, struct task_struct *p, int flags)
 {
 	update_rq_clock(rq);
+	msa_next_state(p, MSA_ONRUNQUEUE);
 	sched_info_queued(p);
 	p->sched_class->enqueue_task(rq, p, flags);
 }
@@ -1912,6 +1913,7 @@ static inline void
 prepare_task_switch(struct rq *rq, struct task_struct *prev,
 		    struct task_struct *next)
 {
+	msa_switch(prev, next);
 	sched_info_switch(prev, next);
 	perf_event_task_sched_out(prev, next);
 	fire_sched_out_preempt_notifiers(prev, next);
diff --git a/mm/memory.c b/mm/memory.c
index 6105f47..e51ebef 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -616,6 +616,8 @@ int __pte_alloc_kernel(pmd_t *pmd, unsigned long address)
 
 	smp_wmb(); /* See comment in __pte_alloc */
 
+	msa_next_state(current, MSA_PAGING_SLEEP);
+
 	spin_lock(&init_mm.page_table_lock);
 	if (likely(pmd_none(*pmd))) {	/* Has another populated it ? */
 		pmd_populate_kernel(&init_mm, pmd, new);
@@ -623,6 +625,7 @@ int __pte_alloc_kernel(pmd_t *pmd, unsigned long address)
 	} else
 		VM_BUG_ON(pmd_trans_splitting(*pmd));
 	spin_unlock(&init_mm.page_table_lock);
+	msa_next_state(current, MSA_UNKNOWN);
 	if (new)
 		pte_free_kernel(&init_mm, new);
 	return 0;
@@ -2912,8 +2915,10 @@ static int do_swap_page(struct mm_struct *mm, struct vm_area_struct *vma,
 	page = lookup_swap_cache(entry);
 	if (!page) {
 		grab_swap_token(mm); /* Contend for token _before_ read-in */
+		msa_next_state(current, MSA_PAGING_SLEEP);
 		page = swapin_readahead(entry,
 					GFP_HIGHUSER_MOVABLE, vma, address);
+		msa_next_state(current, MSA_UNKNOWN);
 		if (!page) {
 			/*
 			 * Back out if somebody else faulted in this pte
@@ -3213,7 +3218,9 @@ static int __do_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 	vmf.flags = flags;
 	vmf.page = NULL;
 
+	msa_next_state(current, MSA_PAGING_SLEEP);
 	ret = vma->vm_ops->fault(vma, &vmf);
+	msa_next_state(current, MSA_UNKNOWN);
 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE |
 			    VM_FAULT_RETRY)))
 		goto uncharge_out;
-- 
1.7.9.7

