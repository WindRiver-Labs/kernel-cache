From ef45fd4d5550f0c8de4e830f07a8d95c8f8ee686 Mon Sep 17 00:00:00 2001
From: Jason HU <yongqi.hu@windriver.com>
Date: Tue, 25 May 2010 15:11:56 +0800
Subject: [PATCH] memmon: detailed monitoring of process memory pages

2 modules were introduced: a connector to transfer data to user
space and a memmon module to tracking the dirty memory of process.
This allows more detailed tracking of the memory dirtied by a process.

Signed-off-by: Jason HU <yongqi.hu@windriver.com>
---
 drivers/connector/Kconfig     |    7 +
 drivers/connector/Makefile    |    1 +
 drivers/connector/cn_memmon.c |  186 +++++++++++
 include/asm-generic/pgtable.h |   33 ++
 include/linux/cn_memmon.h     |   73 +++++
 include/linux/connector.h     |    8 +-
 include/linux/mm_types.h      |    5 +
 init/Kconfig                  |   16 +
 kernel/fork.c                 |   34 ++
 mm/Makefile                   |    1 +
 mm/memmon.c                   |  688 +++++++++++++++++++++++++++++++++++++++++
 11 files changed, 1051 insertions(+), 1 deletions(-)
 create mode 100644 drivers/connector/cn_memmon.c
 create mode 100644 include/linux/cn_memmon.h
 create mode 100644 mm/memmon.c

diff --git a/drivers/connector/Kconfig b/drivers/connector/Kconfig
index 6e6730f..aa614a8 100644
--- a/drivers/connector/Kconfig
+++ b/drivers/connector/Kconfig
@@ -19,4 +19,11 @@ config PROC_EVENTS
 	  Provide a connector that reports process events to userspace. Send
 	  events such as fork, exec, id change (uid, gid, suid, etc), and exit.
 
+config CN_MEMMON
+	boolean "Provides connector interface to memmon"
+	depends on MEMMON && CONNECTOR = y
+	default y
+	---help---
+	  Provide a connector that provides the userspace interface to memmon.
+
 endif # CONNECTOR
diff --git a/drivers/connector/Makefile b/drivers/connector/Makefile
index 1f255e4..0163ee8 100644
--- a/drivers/connector/Makefile
+++ b/drivers/connector/Makefile
@@ -1,4 +1,5 @@
 obj-$(CONFIG_CONNECTOR)		+= cn.o
+obj-$(CONFIG_CN_MEMMON)	+= cn_memmon.o
 obj-$(CONFIG_PROC_EVENTS)	+= cn_proc.o
 
 cn-y				+= cn_queue.o connector.o
diff --git a/drivers/connector/cn_memmon.c b/drivers/connector/cn_memmon.c
new file mode 100644
index 0000000..b26bec3
--- /dev/null
+++ b/drivers/connector/cn_memmon.c
@@ -0,0 +1,186 @@
+/*
+ * drivers/connector/cn_memmon.c, memory monitor surpport
+ *
+ * Copyright (c) 2006-2008 Wind River Systems, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ * See the GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ */
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/skbuff.h>
+#include <linux/compat.h>
+#include <asm/unaligned.h>
+
+#include <linux/cn_memmon.h>
+
+static struct cb_id cn_memmon_id = { CN_IDX_MEMMON, CN_VAL_MEMMON };
+static char cn_memmon_name[] = "cn_memmon";
+
+#define CN_MEMMON_MSG_SIZE (sizeof(struct cn_msg) + sizeof(struct memmon_info))
+
+void cn_memmon_callback(struct cn_msg *msg, struct netlink_skb_parms *nsp)
+{
+	struct memmon_info *args = NULL;
+	struct mm_struct *mm;
+	struct cn_msg *reply;
+	struct memmon_info *replymsg = NULL;
+	int ret;
+	char replybuf[CN_MEMMON_MSG_SIZE];
+	struct task_struct *p;
+	int  args_options;
+	char args_cmd;
+#if (BITS_PER_LONG == 64)
+	compat_pid_t args_pid;
+#else
+	pid_t args_pid;
+#endif
+
+	reply = (struct cn_msg *) replybuf;
+	memset(replybuf, 0x00, sizeof(replybuf));
+
+	args = (struct memmon_info *)msg->data;
+
+	args_cmd     = get_unaligned(&args->cmd);
+	args_options = get_unaligned(&args->options);
+	args_pid     = get_unaligned(&args->pid);
+#ifdef DEBUG
+	printk("cn_memmon_callback:\n");
+	printk("\tpid: %d\n", args_pid);
+	printk("\tseq: %d\n", get_unaligned(&msg->seq));
+	printk("\tack: %d\n", get_unaligned(&msg->ack));
+
+	switch (args_cmd) {
+	case MONITOR_SET:
+		printk("\tcmd: MONITOR_SET\n");
+		break;
+	case MONITOR_CLR:
+		printk("\tcmd: MONITOR_CLR\n");
+		break;
+	case MONITOR_COLLECT:
+		printk("\tcmd: MONITOR_COLLECT\n");
+		break;
+	default:
+		printk("\tcmd: UNKNOWN\n");
+	}
+
+	printk("\tflags: %u\n", get_unaligned(&args->flags));
+	printk("\tbufsize: %u\n", get_unaligned(&args->bufsize));
+	printk("\tbuf: 0x%08x\n", get_unaligned(&args->buf));
+	printk("\tstart: 0x%08x\n", get_unaligned(&args->start));
+	printk("\tend: 0x%08x\n", get_unaligned(&args->end));
+	printk("\toptions: 0x%04x\n", args_options);
+
+	if (args_options & CHECK_DIRTY_STATE)
+		printk("\t\tCHECK_DIRTY_STATE\n");
+	if (args_options & CHECK_FLAGS)
+		printk("\t\tCHECK_FLAGS\n");
+	if (args_options & STORE_MATCHING_PAGES)
+		printk("\t\tSTORE_MATCHING_PAGES\n");
+	if (args_options & CLEAN_STORED_PAGES)
+		printk("\t\tCLEAN_STORED_PAGES\n");
+	if (args_options & CLEAN_MATCHING_PAGES)
+		printk("\t\tCLEAN_MATCHING_PAGES\n");
+	if (args_options & STOP_WHEN_BUF_FULL)
+		printk("\t\tSTOP_WHEN_BUF_FULL\n");
+	if (args_options & CHECK_SWAPPED_STATE)
+		printk("\t\tCHECK_SWAPPED_STATE\n");
+	if (args_options & VALID_PAGES_ONLY)
+		printk("\t\tVALID_PAGES_ONLY\n");
+#endif
+
+	ret = -EINVAL;
+	if (get_unaligned(&args->start) > get_unaligned(&args->end))
+		goto out;
+
+	p = find_task_by_pid_ns(args_pid, &init_pid_ns);
+	if (!p) {
+		printk(KERN_ERR "memmon: invalid pid %d\n", args_pid);
+		goto out;
+	}
+
+#if (BITS_PER_LONG == 64)
+	/* Memmon only works on 32-bit user space tasks. */
+	if (!test_tsk_thread_flag(p, TIF_32BIT)) {
+		printk(KERN_ERR
+		       "memmon: memmon is not available for 64 bit tasks "
+		       "(pid=%i, %s)\n",
+		       args_pid, p->comm);
+		ret = -ENOSYS; /* ENOTSUP is not defined in the kernel */
+		goto out;
+	}
+#endif
+
+	mm = get_task_mm(p);
+	if (!mm) {
+		printk(KERN_ERR "memmon: no mm context\n");
+		ret = -EFAULT;
+		goto out;
+	}
+
+	switch (args_cmd) {
+	case MONITOR_SET:
+		ret = memmon_set_address_flags(mm, args);
+		break;
+	case MONITOR_CLR:
+		ret = 0;
+		memmon_clr_address_flags(mm, args);
+		break;
+	case MONITOR_COLLECT:
+		ret = memmon_collect_data(mm, args);
+		break;
+	}
+	mmput(mm);
+out:
+#ifdef DEBUG
+	printk("\tretval: %d\n", ret);
+#endif
+	memcpy(&reply->id, &cn_memmon_id, sizeof(reply->id));
+
+	put_unaligned(get_unaligned(&msg->seq), &reply->seq);
+	put_unaligned(get_unaligned(&msg->ack) + 1, &reply->ack);
+	put_unaligned(sizeof(struct memmon_info), &reply->len);
+
+	replymsg = (struct memmon_info *) reply->data;
+	put_unaligned(KERNEL_REPLY, &replymsg->cmd);
+	put_unaligned(ret, &replymsg->count);
+
+	cn_netlink_send(reply, CN_IDX_MEMMON, GFP_KERNEL);
+#ifdef DEBUG
+	printk("\tcn_netlink_send done\n");
+#endif
+	return;
+}
+
+static int cn_memmon_init(void)
+{
+	int err;
+
+	err = cn_add_callback(&cn_memmon_id, cn_memmon_name,
+						cn_memmon_callback);
+	if (err) {
+		printk(KERN_WARNING "cn_memmon failed to register\n");
+		return err;
+	}
+	return 0;
+}
+
+static void cn_memmon_exit(void)
+{
+	cn_del_callback(&cn_memmon_id);
+}
+
+module_init(cn_memmon_init);
+module_exit(cn_memmon_exit);
diff --git a/include/asm-generic/pgtable.h b/include/asm-generic/pgtable.h
index fea8ed3..3f7e971 100644
--- a/include/asm-generic/pgtable.h
+++ b/include/asm-generic/pgtable.h
@@ -50,6 +50,39 @@
 })
 #endif
 
+
+#ifdef CONFIG_MEMMON
+/*
+ * Bring back the ptep_clear_flush_dirty for memmon module,
+ * refer to mm/memmon.c
+ */
+#ifndef __HAVE_ARCH_PTEP_TEST_AND_CLEAR_DIRTY
+#define ptep_test_and_clear_dirty(__vma, __address, __ptep)		\
+({									\
+	pte_t __pte = *__ptep;						\
+	int r = 1;							\
+	if (!pte_dirty(__pte))						\
+		r = 0;							\
+	else								\
+		set_pte_at((__vma)->vm_mm, (__address), (__ptep),	\
+			   pte_mkclean(__pte));				\
+	r;								\
+})
+#endif
+
+#ifndef __HAVE_ARCH_PTEP_CLEAR_DIRTY_FLUSH
+#define ptep_clear_flush_dirty(__vma, __address, __ptep)		\
+({									\
+	int __dirty;							\
+	__dirty = ptep_test_and_clear_dirty(__vma, __address, __ptep);	\
+	if (__dirty)							\
+		flush_tlb_page(__vma, __address);			\
+	__dirty;							\
+})
+#endif
+
+#endif
+
 #ifndef __HAVE_ARCH_PTEP_GET_AND_CLEAR
 #define ptep_get_and_clear(__mm, __address, __ptep)			\
 ({									\
diff --git a/include/linux/cn_memmon.h b/include/linux/cn_memmon.h
new file mode 100644
index 0000000..8ec737f
--- /dev/null
+++ b/include/linux/cn_memmon.h
@@ -0,0 +1,73 @@
+/*
+ * include/linux/cn_memmon.h, memory monitor surpport
+ *
+ * Copyright (c) 2006-2008 Wind River Systems, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ * See the GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ */
+#ifndef CN_MEMMON_H
+#define CN_MEMMON_H
+
+#include <linux/types.h>
+#include <linux/connector.h>
+
+/* The following commands are valid for the memmon operations */
+#define MONITOR_SET            1
+#define MONITOR_CLR            2
+#define MONITOR_COLLECT        3
+#define KERNEL_REPLY           4
+
+/* The default for memmon collect operations is to return the number of dirty
+* user-readable pages.  The following  options may be OR'd together to
+* specify additional behaviour.
+*/
+#define CHECK_DIRTY_STATE              0x001
+#define CHECK_FLAGS                    0x002
+#define STORE_MATCHING_PAGES           0x004
+#define CLEAN_STORED_PAGES             0x008
+#define CLEAN_MATCHING_PAGES           0x010
+#define STOP_WHEN_BUF_FULL             0x020
+#define CHECK_SWAPPED_STATE            0x040
+#define VALID_PAGES_ONLY               0x080
+
+/* This is the data structure for memory monitor operations.
+ * "cmd" is the requested operation
+ * "start" and "end" give the address range of interest
+ * "flags" is set/matched against the flags set for the memory pages
+ *
+ * When specifying the collect operation, there are more fields that
+ * must be filled in.
+ * "buf" is a userspace buffer where we will dump the information
+ * "bufsize" is the size of the buffer in bytes.
+ * "count" is used by the kernel reply message
+ * "options" specify variants of the data collection
+ */
+struct memmon_info {
+	unsigned char flags;
+	unsigned char cmd;
+	unsigned short options;
+	int bufsize;
+	unsigned int buf;
+	unsigned int start;
+	unsigned int end;
+	unsigned int count;
+	pid_t pid;
+};
+
+int memmon_set_address_flags(struct mm_struct *mm, struct memmon_info *args);
+void memmon_clr_address_flags(struct mm_struct *mm, struct memmon_info *args);
+int memmon_collect_data(struct mm_struct *mm, struct memmon_info *args);
+
+#endif /* CN_MEMMON_H */
diff --git a/include/linux/connector.h b/include/linux/connector.h
index 3a779ff..3d2744b 100644
--- a/include/linux/connector.h
+++ b/include/linux/connector.h
@@ -43,7 +43,13 @@
 #define CN_IDX_DRBD			0x8
 #define CN_VAL_DRBD			0x1
 
-#define CN_NETLINK_USERS		8
+/*
+ * memmon connector unique ids -- used for message routing
+ */
+#define CN_IDX_MEMMON                     0x9
+#define CN_VAL_MEMMON                     0x1
+
+#define CN_NETLINK_USERS		9
 
 /*
  * Maximum connector's message size.
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 30c18b5..042377a 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -333,6 +333,11 @@ struct mm_struct {
 	unsigned long oom_protect_bytes;	/* exempt from oom death if
 						 * total vm < this */
 #endif /* CONFIG_OOM_PROTECT */
+
+#ifdef CONFIG_MEMMON
+	/* used to track per-page memmon flags */
+	unsigned long memmon_flag_dir;
+#endif
 };
 
 /* Future-safe accessor for struct mm_struct's cpu_vm_mask. */
diff --git a/init/Kconfig b/init/Kconfig
index b5f433f..0cdef63 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -1144,6 +1144,22 @@ config OOM_PROTECT
 	  request protection from the out-of-memory killer as long as
 	  the process is using less than the specified amount of virtual
 	  memory.  This is accomplished through a call to setrlimit().
+config MEMMON
+	bool "Enable detailed monitoring of process memory pages"
+	default n
+	help
+	  When enabled, this allows more detailed tracking of the memory dirtied by
+	  a process.
+
+config MEMMON_SWAP_SUPPORT
+	bool "Enable memory monitoring for swappable pages"
+	depends on MEMMON
+	default n
+	help
+	  When enabled, this allows memmon tracking of pages that are swappable.
+	  This slows down dirty page scanning significantly, so only enable if
+	  actually needed.
+
 endmenu		# General setup
 
 config HAVE_GENERIC_DMA_COHERENT
diff --git a/kernel/fork.c b/kernel/fork.c
index 02508bb..72bbfb6 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -482,9 +482,40 @@ static void mm_init_aio(struct mm_struct *mm)
 #endif
 }
 
+#ifdef CONFIG_MEMMON
+
+/* We want to mimimize external dependencies, so we will not
+ * use any nice typedefs or anything in here.
+ * mm->memmon_flag_dir (if nonzero) points to a page of memory.  That
+ * page consists of an array of 256 pointers, each of which (if nonzero)
+ * points to another page of memory.  We need to free all that memory.
+ */
+static void exit_memmon(struct mm_struct *mm)
+{
+	void **pagepp = (void **) mm->memmon_flag_dir;
+	void **end;
+
+	if (!pagepp)
+		return;
+
+	end = pagepp + 256;
+	while (pagepp < end) {
+	if (*pagepp)
+		free_page((unsigned long)*pagepp);
+		pagepp++;
+	}
+	free_page(mm->memmon_flag_dir);
+	mm->memmon_flag_dir = 0;
+}
+#endif
+
+
 static struct mm_struct * mm_init(struct mm_struct * mm, struct task_struct *p)
 {
 	atomic_set(&mm->mm_users, 1);
+#ifdef CONFIG_MEMMON
+	mm->memmon_flag_dir = 0;
+#endif
 	atomic_set(&mm->mm_count, 1);
 	init_rwsem(&mm->mmap_sem);
 	INIT_LIST_HEAD(&mm->mmlist);
@@ -555,6 +586,9 @@ void mmput(struct mm_struct *mm)
 		ksm_exit(mm);
 		exit_mmap(mm);
 		set_mm_exe_file(mm, NULL);
+#ifdef CONFIG_MEMMON
+		exit_memmon(mm);
+#endif
 		if (!list_empty(&mm->mmlist)) {
 			spin_lock(&mmlist_lock);
 			list_del(&mm->mmlist);
diff --git a/mm/Makefile b/mm/Makefile
index 6c2a73a..415ead8 100644
--- a/mm/Makefile
+++ b/mm/Makefile
@@ -44,3 +44,4 @@ obj-$(CONFIG_MEMORY_FAILURE) += memory-failure.o
 obj-$(CONFIG_HWPOISON_INJECT) += hwpoison-inject.o
 obj-$(CONFIG_DEBUG_KMEMLEAK) += kmemleak.o
 obj-$(CONFIG_DEBUG_KMEMLEAK_TEST) += kmemleak-test.o
+obj-$(CONFIG_MEMMON) += memmon.o
diff --git a/mm/memmon.c b/mm/memmon.c
new file mode 100644
index 0000000..ddb1104
--- /dev/null
+++ b/mm/memmon.c
@@ -0,0 +1,688 @@
+/*
+ * mm/memmon.c, memory monitor surpport
+ *
+ * Copyright (c) 2006-2008 Wind River Systems, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ * See the GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ */
+#include <linux/errno.h>
+#include <linux/cn_memmon.h>
+#include <linux/sched.h>
+#include <linux/gfp.h>
+#include <linux/uaccess.h>
+#include <asm/unaligned.h>
+#include <asm/pgtable.h>
+#include <asm/cacheflush.h>
+#include <asm/tlbflush.h>
+#include <linux/highmem.h>
+
+#include <linux/timex.h>
+#include <asm/mmu_context.h>
+
+
+#ifdef CONFIG_PPC64
+#define BATCH_FLUSH
+static inline void flush_tlb_pending()
+{
+	struct ppc64_tlb_batch *tlbbatch = &get_cpu_var(ppc64_tlb_batch);
+
+	/* If there's a TLB batch pending, then we must flush it because the
+	 * pages are going to be freed and we really don't want to have a CPU
+	 * access a freed page because it has a stale TLB
+	 */
+	if (tlbbatch->index)
+		__flush_tlb_pending(tlbbatch);
+
+	put_cpu_var(ppc64_tlb_batch);
+}
+#endif
+
+/* The memmon "sister" page flags works like this:
+ * At the lowest level, each page of memory has a corresponding
+ * byte of flags called the "memmon flag entry".  These are grouped
+ * into a page worth of flags, and together they make up a "memmon flag
+ * table".  There are 4096 mfe's in an mft.  Each mft covers 4096^2 or
+ * 16777216 bytes of address space.
+ *
+ * Each mft is pointed to by a "memmon flag directory entry", which is
+ * stored on a separate page.  It only takes 256 mfde's to cover the entire
+ * 32-bit address space.  These mfde's are grouped into the "memmon flag
+ * directory".
+ *
+ * Finally, a pointer to the "memmon flag directory" (if it exists) is
+ * stored in the mm_struct for the memory map in question.
+ *
+ * To get from a memory address to the flags for that address, we do the
+ * following:
+ * 1) Get the location the memmon flag directory.  (Note: it may not exist
+ *    in which case there were no flags for that address.)
+ * 2) Use the high 8 bits of the memory address as an offset within the
+ *    memmon flag directory.
+ * 3) Get the location of the memmon flag table.  (Note: it may not exist
+ *    in which case there were no flags for that address.)
+ * 4) Use the next 12 bits of the memory address as an offset within the
+ *    memmon flag table.
+ * 5) The byte at that location are the flag bits for the address in question.
+ */
+
+#define MFT_SHIFT         PAGE_SHIFT
+#define ENTRIES_PER_MFT   PAGE_SIZE
+#define MFT_MASK          (ENTRIES_PER_MFT - 1)
+#define mfte_offset(addr) (((addr) >> (MFT_SHIFT)) & MFT_MASK)
+
+#define MFD_SHIFT         (PAGE_SHIFT + PAGE_SHIFT)
+#define PTRS_PER_MFD      (2 << (32 - MFD_SHIFT))
+#define MFD_MASK          (PTRS_PER_MFD - 1)
+#define mfde_offset(addr) (((addr) >> (MFD_SHIFT)) & MFD_MASK)
+
+#define LAST_FLAG_TABLE_ADDR(addr) ((addr)|0xFFFFFF)
+
+#define memmon_flag_dir(mm) ((mfdep_t *) &mm->memmon_flag_dir)
+#define memmon_flag_dir_entry(mfdp, addr) (*mfdp + mfde_offset(addr))
+#define memmon_flag_table_entry(mfdep, addr) (*mfdep + mfte_offset(addr))
+
+typedef unsigned char mfte_t;
+typedef mfte_t *mfde_t;
+typedef mfde_t *mfdep_t;
+
+static inline void memmon_set_flags(unsigned char *flagptr, unsigned char flags)
+{
+	*flagptr |= flags;
+}
+
+static inline void memmon_clr_flags(unsigned char *flagptr, unsigned char flags)
+{
+	*flagptr &= ~flags;
+}
+
+static inline int memmon_test_flags(unsigned char *flagptr, unsigned char flags)
+{
+	return *flagptr & flags;
+}
+
+
+/*
+ * Copied from fs/aio.c
+ * The calling kernel thread is the netlink connector queue (cqueue).
+ *
+ * use_mm
+ *      Makes the calling kernel thread take on the specified
+ *      mm context.
+ *      Called by the retry thread execute retries within the
+ *      iocb issuer's mm context, so that copy_from/to_user
+ *      operations work seamlessly for aio.
+ *      (Note: this routine is intended to be called only
+ *      from a kernel thread context)
+ */
+static void use_mm(struct mm_struct *mm)
+{
+	struct mm_struct *active_mm;
+	struct task_struct *tsk = current;
+
+	task_lock(tsk);
+	active_mm = tsk->active_mm;
+	atomic_inc(&mm->mm_count);
+	tsk->mm = mm;
+	tsk->active_mm = mm;
+	switch_mm(active_mm, mm, tsk);
+	task_unlock(tsk);
+
+	mmdrop(active_mm);
+}
+
+/*
+ * unuse_mm
+ *      Reverses the effect of use_mm, i.e. releases the
+ *      specified mm context which was earlier taken on
+ *      by the calling kernel thread
+ *      (Note: this routine is intended to be called only
+ *      from a kernel thread context)
+ */
+static void unuse_mm(struct mm_struct *mm)
+{
+	struct task_struct *tsk = current;
+
+	task_lock(tsk);
+	tsk->mm = NULL;
+	/* active_mm is still 'mm' */
+	enter_lazy_tlb(mm, tsk);
+	task_unlock(tsk);
+}
+
+/* This routine allows us to set flag bits in the byte reserved for memmon
+ * flags for the specified address range.
+ *
+ * "start" and "end" are pointers to userspace addresses, "ppage" is a pointer
+ * to the address of a zeroed page of memory, and "flags" is a byte.
+ *
+ * We may need memory if it has not yet been allocated.  In this case there are
+ * two possibilities: 1) If *ppage is 0, we return -ENOBUFS.  2) If *ppage is
+ * not NULL, we use the memory and set *ppage to 0.
+ *
+ * Once we locate the current flags for the given address range, we OR "flags"
+ * with the current flags and update "start" to reflect the range of addresses
+ * affected.
+ *
+ * We will only affect flags within a single flag table at a time.  If we
+ * updated the whole desired range, then we return 0.  If there were no errors
+ * but we did not apply the flag to the whole range, we return 1.  If this
+ * occurs, the caller should call us again.
+ */
+
+static int __set_address_flags(struct mm_struct *mm, unsigned int *start,
+	unsigned int *end, unsigned long *ppage, unsigned char flags)
+{
+	int ret;
+	mfdep_t *mfdp;
+	mfdep_t mfdep;
+	mfte_t *tmp_mftep, *end_mftep;
+	unsigned int last_addr, tempend;
+
+
+	ret = -ENOBUFS;
+	down_write(&mm->mmap_sem);
+
+	mfdp = memmon_flag_dir(mm);
+#ifdef DEBUG
+	printk("*mfdp: %p\n", *mfdp);
+#endif
+	if (!*mfdp) {
+		if (!*ppage)
+			goto up_out;
+
+		/* caller passed in a page, how nice of them */
+		*mfdp = (mfdep_t) *ppage;
+		*ppage = 0;
+	}
+
+	mfdep = memmon_flag_dir_entry(mfdp, *start);
+#ifdef DEBUG
+	printk("*mfdp: %p\n", *mfdp);
+	printk("mfdep: %p\n", mfdep);
+	printk("*mfdep: %p\n", *mfdep);
+#endif
+	if (!*mfdep) {
+		if (!*ppage)
+			goto up_out;
+
+		/* caller passed in a page, how nice of them */
+		*mfdep = (mfde_t) *ppage;
+		*ppage = 0;
+	}
+#ifdef DEBUG
+	printk("*mfdep: %p\n", *mfdep);
+#endif
+
+	/* Each time we call this function, we can only handle address ranges
+	 * that map to a single flag table.
+	 *
+	 * To ensure this, we set "tempend" to whichever is smaller, either
+	 * "*end", or the last address that will map to the same flag table
+	 * as "*start".
+	 */
+	last_addr = LAST_FLAG_TABLE_ADDR(*start);
+	tempend = min(last_addr, *end);
+
+	tmp_mftep = memmon_flag_table_entry(mfdep, *start);
+	end_mftep = memmon_flag_table_entry(mfdep, tempend);
+
+#ifdef DEBUG
+	printk("tmp_mftep: %p\n", tmp_mftep);
+	printk("end_mftep: %p\n", end_mftep);
+#endif
+
+	while (tmp_mftep <= end_mftep) {
+		memmon_set_flags(tmp_mftep, flags);
+		tmp_mftep++;
+	}
+
+	/* Success on this call.  If we need to get called again,
+	 * return 1.
+	 */
+
+	*start = tempend + 1;
+	ret = (tempend == *end) ? 0 : 1;
+
+up_out:
+	up_write(&mm->mmap_sem);
+	return ret;
+}
+
+/* This is a helper routine primarily to handle memory use.
+ *
+ * __set_address_flag() will return -ENOBUFS if it needs a zeroed page of
+ * memory.  If this occurs, we attempt to allocate such a page and call it
+ * again.  If the function uses it then it will zero out the address of the
+ * page.
+ *
+ * If we are racing with another cpu, it is possible that the memory may not
+ * be used, in which case we must free it ourselves.
+ *
+ * Note that we may have to call the function up to three times in the worst
+ * possible case.
+ *
+ * On failure we return -ENOMEM if we could not allocate enough memory.
+ */
+
+static int _set_address_flags(struct mm_struct *mm, unsigned int *start,
+		unsigned int *end, unsigned char flags)
+{
+	int ret;
+	unsigned long page = 0;
+#ifdef DEBUG
+	printk("start: %x  end: %x\n", *start, *end);
+#endif
+
+	do {
+		ret = __set_address_flags(mm, start, end, &page, flags);
+		if (ret == -ENOBUFS) {
+			page = get_zeroed_page(GFP_KERNEL);
+			if (!page)
+				ret = -ENOMEM;
+		}
+	} while (ret && (ret != -ENOMEM));
+
+	if (page)
+		free_page(page);
+	return ret;
+}
+
+
+
+
+/* This routine allows us to set flag(s) in the byte reserved for memmon flags
+ * for the specified address range. "args->start" and "args->end" are
+ * value/result pointers to userspace addresses, and "args->flags" is the
+ * flag pattern that is to be set.
+ *
+ * The values of the "args->start" and "args->end" parameters may be
+ * overwritten, so the caller should not rely on them after calling this.
+ *
+ * On success we return 0, on failure we return -ENOMEM if we could not
+ * allocate enough memory.
+ *
+ * On any call, "args->start" will be updated such that the range from
+ * "args->start" to "args->end" is the range that was *not* affected.  On
+ * successful completion, "args->start" will be one past "args->end".
+ */
+
+int memmon_set_address_flags(struct mm_struct *mm, struct memmon_info *args)
+{
+	int ret = 0;
+
+	/* On each loop through, the function will update the values of
+	 * start and end to reflect the range of addresses that have not
+	 * been affected.  We keep looping until done or we get an error.
+	 */
+	do {
+		ret = _set_address_flags(mm, &args->start, &args->end,
+			args->flags);
+	} while (ret > 0);
+	return ret;
+}
+
+
+
+
+/* This routine allows us to clear a flag bit in the byte reserved for memmon
+ * flags for the specified address range.
+ *
+ * "start" and "end" are pointers to userspace addresses, and "flagbit" is
+ * the bit number of one of the flags listed at the top of this file. "ppage"
+ * is a pointer to the address of a zeroed page of memory.
+ *
+ * On completion we set the bit flag on the given address range and update
+ * "start" to reflect the range of addresses affected.
+ *
+ * Normally we will only affect a page worth of flags at a time.  (The
+ * exception to this is if nothing was being monitored.)
+ * If we updated the whole desired range, then we return 0.  If there were no
+ * errors but we did not apply the flag to the whole range, we return 1.  If
+ * this occurs, the caller should call us again.
+ */
+
+static int _clr_address_flags(struct mm_struct *mm, unsigned int *start,
+	unsigned int *end, unsigned char flags)
+{
+	int ret;
+	mfdep_t *mfdp;
+	mfdep_t mfdep;
+	mfte_t *tmp_mftep, *end_mftep;
+	unsigned int tempend;
+
+	down_write(&mm->mmap_sem);
+
+	mfdp = memmon_flag_dir(mm);
+	if (!*mfdp) {
+		/* Nothing is monitored, so shortcut any possible additional
+		 * ranges that have to be cleared.
+		 */
+		tempend = *end;
+		goto success_out;
+	}
+
+	/* Each time we call this function, we can only handle address ranges
+	 * that map to a single L2 table.
+	 *
+	 * "tempend" is set to the last address that will map to the same
+	 * L2 table as "*start".
+	 *
+	 * If "tempend" is after "*end", it means that
+	 * "*end" maps to the same L2 table as "*start", so we then set
+	 * "tempend" to "*end".
+	 */
+	tempend = LAST_FLAG_TABLE_ADDR(*start);
+	if (tempend > *end)
+		tempend = *end;
+
+	/* We want to get a pointer to the appropriate entry
+	 * in the L1 table.  This could be 0 if nothing has ever been
+	 * monitored in that address range--which just means less work for us.
+	 */
+	mfdep = memmon_flag_dir_entry(mfdp, *start);
+	if (!*mfdep) {
+		/* Nothing in this particular range is being monitored, so no
+		 * work to clear it.  "tempend" is already set, so just call
+		 * it a success.
+		 */
+		 goto success_out;
+	}
+
+	/* Now we want to find the right entry range in the L2 table */
+	tmp_mftep = memmon_flag_table_entry(mfdep, *start);
+	end_mftep = memmon_flag_table_entry(mfdep, tempend);
+
+	while (tmp_mftep <= end_mftep) {
+		memmon_clr_flags(tmp_mftep, flags);
+		tmp_mftep++;
+	}
+
+success_out:
+	/* Success on this call.  If we need to get called again,
+	 * return 1.
+	 */
+
+	*start = tempend + 1;
+	ret = (tempend == *end) ? 0 : 1;
+
+	up_write(&mm->mmap_sem);
+	return ret;
+}
+
+/* This routine allows us to clear one or more bits in the byte reserved for
+ * memmon flags for the specified address.  "args->start" and "args->end" give
+ * the range of addresses to be affected, and "args->flags" is the bits which
+ * are to be cleared.
+ *
+ * There is no return code, since there is no reason for this to fail.
+ *
+ * It is *not* considered an error to try and clear a bit on an address
+ * that is not currently being monitored.
+ *
+ * The caller should ensure that args->start is smaller than args->end.
+ */
+
+void memmon_clr_address_flags(struct mm_struct *mm, struct memmon_info *args)
+{
+	int rc;
+
+	/* On each loop through, the function will update the values of
+	 * start and end to reflect the range of addresses that have not
+	 * been affected.  We keep looping until done.
+	 */
+	do {
+		rc = _clr_address_flags(mm, &args->start, &args->end,
+			args->flags);
+	} while (rc > 0);
+}
+
+
+/* it is expected that the caller holds mm->mmap_sem */
+
+static mfte_t *va_to_mftep(struct mm_struct *mm, unsigned int addr)
+{
+	mfdep_t *mfdp;
+	mfdep_t mfdep;
+	mfte_t *mftep = 0;
+
+	mfdp = memmon_flag_dir(mm);
+	if (!*mfdp)
+		goto out;
+
+	/* We want to get a pointer to the appropriate entry
+	 * in the L1 table.
+	 */
+	mfdep = memmon_flag_dir_entry(mfdp, addr);
+	if (!*mfdep)
+		goto out;
+
+	/* Now we want to find the right entry in the L2 table */
+	mftep = memmon_flag_table_entry(mfdep, addr);
+
+out:
+	return mftep;
+}
+
+/* Page table walking code stolen from follow_page() except
+ * that this version does not support huge tlbs.
+ */
+static  pte_t *va_to_ptep_map(struct mm_struct *mm, unsigned long addr)
+{
+	pgd_t *pgd;
+	pud_t *pud;
+	pmd_t *pmd;
+	pte_t *ptep = 0;
+
+	pgd = pgd_offset(mm, addr);
+	if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))
+		goto out;
+
+	pud = pud_offset(pgd, addr);
+	if (pud_none(*pud) || unlikely(pud_bad(*pud)))
+		goto out;
+
+	pmd = pmd_offset(pud, addr);
+	if (pmd_none(*pmd) || unlikely(pmd_bad(*pmd)))
+		goto out;
+
+	ptep = pte_offset_map(pmd, addr);
+	if (!ptep)
+		goto out;
+
+out:
+	return ptep;
+}
+
+int memmon_collect_data(struct mm_struct *mm, struct memmon_info *args)
+{
+	int ret;
+	mm_segment_t old_fs;
+	unsigned int start = get_unaligned(&args->start);
+	unsigned int end = get_unaligned(&args->end);
+	unsigned long bufaddr = get_unaligned(&args->buf);
+	unsigned int __user *buf = (unsigned int __user *) bufaddr;
+	int options = get_unaligned(&args->options);
+	unsigned int entries = 0;
+
+	unsigned int addr;
+	unsigned int page_count = 0;
+	unsigned int loop_count = 0;
+	int args_bufsize;
+#ifdef BATCH_FLUSH
+	int need_flush = 0;
+#endif
+	ret = -EFAULT;
+
+	args_bufsize = get_unaligned(&args->bufsize);
+	old_fs = get_fs();
+	if (options & STORE_MATCHING_PAGES) {
+		/*
+		 * Take on the memmon user's mm context so that put_user()
+		 * operates in the proper address space.
+		 */
+		set_fs(USER_DS);
+		use_mm(mm);
+
+		ret = !access_ok(VERIFY_WRITE, buf, args_bufsize);
+		if (ret) {
+			printk("verification failed\n");
+			ret = -EFAULT;
+			goto out;
+		}
+		entries = args_bufsize / sizeof(*buf);
+	}
+
+	down_write(&mm->mmap_sem);
+
+	/* scan through the entire address space given */
+	page_count = 0;
+
+	/* Must hold the page table spinlock while walking */
+	spin_lock(&mm->page_table_lock);
+
+	for (addr = start&PAGE_MASK; addr <= end; addr += PAGE_SIZE) {
+		/* Don't remove the initialization of ptep. It's
+		 * there to handle unmap_continue, below.
+		 */
+		pte_t *ptep = 0;
+		int need_clean = (options & CLEAN_MATCHING_PAGES);
+
+		/* Periodically drop the lock to allow preemption.  Testing
+		 * on the G5 gives lock-hold times of about 500usec with
+		 * negligible effects on overall performance.
+		 */
+		if (++loop_count == 1000) {
+#ifdef BATCH_FLUSH
+			if (need_flush) {
+				need_flush = 0;
+				flush_tlb_pending();
+			}
+#endif
+
+			spin_unlock(&mm->page_table_lock);
+			spin_lock(&mm->page_table_lock);
+			loop_count = 0;
+		}
+
+		/* If the option is specified, test the specified flags
+		 * against the flags stored for this page.
+		 */
+		if (options & CHECK_FLAGS) {
+			mfte_t *flagp = va_to_mftep(mm, addr);
+			if (!flagp)
+				goto unmap_continue;
+
+			if (!memmon_test_flags(flagp, args->flags))
+				goto unmap_continue;
+		}
+
+#ifdef CONFIG_MEMMON_SWAP_SUPPORT
+		/* Check if the page has been dirtied or swapped out.  These two
+		 * states are always mutually exclusive.  When both options
+		 * are specified we count the page if either the page is dirty
+		 * or swapped.
+		 */
+		if (options & (CHECK_SWAPPED_STATE | CHECK_DIRTY_STATE)) {
+			int match = 0;
+
+			ptep = va_to_ptep_map(mm, addr);
+			if (!ptep)
+				goto unmap_continue;
+
+			if (pte_present(*ptep) && (options & CHECK_DIRTY_STATE)) {
+				match = pte_dirty(*ptep);
+				if (!match)
+					match |= PageDirty(pte_page(*ptep));
+				if (!match)
+					match |= PageSwapCache(pte_page(*ptep));
+			} else if (!pte_present(*ptep) &&
+					(options & CHECK_SWAPPED_STATE)) {
+				match = !pte_none(*ptep);
+			}
+
+			if (!match)
+				goto unmap_continue;
+		}
+#else
+		/* Check if the page has been dirtied. */
+		if (options & CHECK_DIRTY_STATE) {
+			ptep = va_to_ptep_map(mm, addr);
+
+			if (!ptep || !pte_present(*ptep) || !pte_dirty(*ptep))
+				goto unmap_continue;
+		}
+#endif
+
+		/* The page passed all requested checks. Count it. This must
+		 * happen before the comparison against "entries" below.
+		 */
+		page_count++;
+
+		/* See if we want to store the page address. */
+		if (page_count <= entries) {
+			__put_user(addr, buf);
+			buf++;
+
+			/* Handle option to clean the page. */
+			if (options & CLEAN_STORED_PAGES)
+				need_clean = 1;
+		}
+
+		/* WARNING: A call to this function does not cause the contents
+		 * of page to be swapped to disk.  It simply clears the "dirty"
+		 * flag and invalidates the TLB.  Calling this function is a
+		 * great way to lose data during swapping.
+		 */
+		if (need_clean) {
+#ifdef BATCH_FLUSH
+			/* On some architectures (currently only ppc64) we can
+			 * do all of the test/clear individually here, then
+			 * flush them all at once.  This saves about 20% of the
+			 * cost of a 10000 dirty-page scan over 1GB of memory.
+			 */
+			ptep_test_and_clear_dirty(find_vma(mm, addr),
+						addr, ptep);
+			need_flush = 1;
+#else
+			ptep_clear_flush_dirty(find_vma(mm, addr), addr, ptep);
+#endif
+		}
+
+		/* Handle option to stop early. */
+		if ((page_count == entries) && (options & STOP_WHEN_BUF_FULL))
+			addr = end+1;
+
+unmap_continue:
+		if (ptep)
+			pte_unmap(ptep);
+	}
+	spin_unlock(&mm->page_table_lock);
+	ret = page_count;
+
+	up_write(&mm->mmap_sem);
+out:
+#ifdef BATCH_FLUSH
+	if (need_flush)
+		flush_tlb_pending();
+#endif
+	if (options & STORE_MATCHING_PAGES) {
+		unuse_mm(mm);
+		set_fs(old_fs);
+	}
+
+	return ret;
+}
+
+
-- 
1.6.0.3

