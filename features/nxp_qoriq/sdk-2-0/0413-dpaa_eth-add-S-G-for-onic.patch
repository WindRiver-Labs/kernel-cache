From ca8e1accda6756c0627e15e053a90a477e0e6185 Mon Sep 17 00:00:00 2001
From: Marian-Cristian Rotariu <marian.rotariu@freescale.com>
Date: Fri, 22 Jan 2016 19:38:21 +0200
Subject: [PATCH 413/752] dpaa_eth: add S/G for onic

This patch adds Tx S/G for onic Ethernet driver. In order to achieve that, a
new specialiazed drainig buffer pool was created.

By design, onic does not have confirmation capabilities. To release the
allocated space for transmission, a draining buffer pool is used. A periodic
routine flushes the draining buffer pool at each kernel tick.

To accomodate different frame descriptors, S/G frames and contiguous frames, an
auxiliary dedicated draining buffer pool is filled with S/G fragments. These
buffer pools are transparent for the user/admin of the network interface.

Signed-off-by: Marian-Cristian Rotariu <marian.rotariu@freescale.com>
[Original patch taken from QorIQ-SDK-V2.0-20160527-yocto]
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 .../ethernet/freescale/sdk_dpaa/dpaa_eth_generic.c | 294 +++++++++++++++++----
 .../ethernet/freescale/sdk_dpaa/dpaa_eth_generic.h |   1 +
 2 files changed, 249 insertions(+), 46 deletions(-)

diff --git a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_generic.c b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_generic.c
index dc7a619..2ab5539 100644
--- a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_generic.c
+++ b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_generic.c
@@ -51,6 +51,7 @@
 #include <linux/if_vlan.h>
 #include <linux/ip.h>
 #include <linux/ipv6.h>
+#include <linux/percpu.h>
 
 #include "dpaa_eth.h"
 #include "dpaa_eth_common.h"
@@ -90,6 +91,7 @@ static void dpa_generic_ern(struct qman_portal *portal,
 static int __hot dpa_generic_tx(struct sk_buff *skb,
 				struct net_device *netdev);
 static void dpa_generic_drain_bp(struct dpa_bp *bp, u8 nbuf);
+static void dpa_generic_drain_sg_bp(struct dpa_bp *sg_bp, u8 nbuf);
 
 static const struct net_device_ops dpa_generic_ops = {
 	.ndo_open = dpa_generic_start,
@@ -110,6 +112,7 @@ static void dpa_generic_draining_timer(unsigned long arg)
 
 	/* drain in pairs of 4 buffers */
 	dpa_generic_drain_bp(priv->draining_tx_bp, 4);
+	dpa_generic_drain_sg_bp(priv->draining_tx_sg_bp, 4);
 
 	if (atomic_read(&priv->ifup))
 		mod_timer(&(priv->timer), jiffies + 1);
@@ -371,6 +374,7 @@ dpa_generic_rx_dqrr(struct qman_portal *portal,
 	 * enough
 	 */
 	dpa_generic_drain_bp(priv->draining_tx_bp, 1);
+	dpa_generic_drain_sg_bp(priv->draining_tx_sg_bp, 1);
 
 	if (unlikely(dpaa_eth_napi_schedule(percpu_priv, portal)))
 		return qman_cb_dqrr_stop;
@@ -450,6 +454,45 @@ qman_consume:
 	return qman_cb_dqrr_consume;
 }
 
+static void dpa_generic_drain_sg_bp(struct dpa_bp *sgbp, u8 nbuf)
+{
+	int ret;
+	struct bm_buffer bmb[8];
+
+	do {
+		ret = bman_acquire(sgbp->pool, bmb, nbuf, 0);
+	} while (ret >= 0);
+}
+
+inline void dpa_release_sg(struct sk_buff *skb, dma_addr_t addr,
+		struct dpa_bp *bp)
+{
+	struct qm_sg_entry *sgt = phys_to_virt(addr + DPA_DEFAULT_TX_HEADROOM);
+	int nr_frags = skb_shinfo(skb)->nr_frags;
+	dma_addr_t sg_addr;
+	int j;
+
+	dma_unmap_single(bp->dev, addr, DPA_DEFAULT_TX_HEADROOM +
+			sizeof(struct qm_sg_entry) * (1 + nr_frags),
+			DMA_BIDIRECTIONAL);
+
+	for (j = 0; j <= nr_frags; j++) {
+		DPA_BUG_ON(sgt[j].extension);
+		sg_addr = qm_sg_addr(&sgt[j]);
+		dma_unmap_page(bp->dev, sg_addr,
+				sgt[j].length, DMA_BIDIRECTIONAL);
+	}
+
+	dev_kfree_skb_any(skb);
+}
+
+inline void dpa_release_contig(struct sk_buff *skb, dma_addr_t addr,
+		struct dpa_bp *bp)
+{
+	dma_unmap_single(bp->dev, addr, bp->size, DMA_BIDIRECTIONAL);
+	dev_kfree_skb_any(skb);
+}
+
 static void dpa_generic_drain_bp(struct dpa_bp *bp, u8 nbuf)
 {
 	int ret, i;
@@ -464,11 +507,15 @@ static void dpa_generic_drain_bp(struct dpa_bp *bp, u8 nbuf)
 		ret = bman_acquire(bp->pool, bmb, nbuf, 0);
 		if (ret > 0) {
 			for (i = 0; i < nbuf; i++) {
-				addr = bm_buf_addr(&bmb[i]);
+				addr = bm_buffer_get64(&bmb[i]);
 				skbh = (struct sk_buff **)phys_to_virt(addr);
 				dma_unmap_single(bp->dev, addr, bp->size,
 						DMA_TO_DEVICE);
-				dev_kfree_skb_any(*skbh);
+
+				if (skb_is_nonlinear(*skbh))
+					dpa_release_sg(*skbh, addr, bp);
+				else
+					dpa_release_contig(*skbh, addr, bp);
 			}
 			count -= i;
 		}
@@ -582,6 +629,128 @@ return_error:
 	return retval;
 }
 
+static inline int generic_skb_to_sg_fd(struct dpa_generic_priv_s *priv,
+		struct sk_buff *skb, struct qm_fd *fd)
+{
+	struct dpa_bp *dpa_bp = priv->draining_tx_bp;
+	struct dpa_bp *dpa_sg_bp = priv->draining_tx_sg_bp;
+	dma_addr_t addr;
+	struct sk_buff **skbh;
+	struct net_device *net_dev = priv->net_dev;
+	int err;
+
+	struct qm_sg_entry *sgt;
+	void *sgt_buf;
+	void *buffer_start;
+	skb_frag_t *frag;
+	int i, j;
+	const enum dma_data_direction dma_dir = DMA_BIDIRECTIONAL;
+	const int nr_frags = skb_shinfo(skb)->nr_frags;
+
+	memset(fd, 0, sizeof(*fd));
+	fd->format = qm_fd_sg;
+
+	/* get a page frag to store the SGTable */
+	sgt_buf = netdev_alloc_frag(priv->tx_headroom +
+			sizeof(struct qm_sg_entry) * (1 + nr_frags));
+	if (unlikely(!sgt_buf)) {
+		dev_err(dpa_bp->dev, "netdev_alloc_frag() failed\n");
+		return -ENOMEM;
+	}
+
+	memset(sgt_buf, 0, priv->tx_headroom +
+			sizeof(struct qm_sg_entry) * (1 + nr_frags));
+
+	/* do this before dma_map_single(DMA_TO_DEVICE), because we may need to
+	 * write into the skb.
+	 */
+	err = dpa_generic_tx_csum(priv, skb, fd,
+			sgt_buf + DPA_TX_PRIV_DATA_SIZE);
+	if (unlikely(err < 0)) {
+		if (netif_msg_tx_err(priv) && net_ratelimit())
+			netdev_err(net_dev, "HW csum error: %d\n", err);
+		goto csum_failed;
+	}
+
+	sgt = (struct qm_sg_entry *)(sgt_buf + priv->tx_headroom);
+	sgt[0].bpid = dpa_sg_bp->bpid;
+	sgt[0].offset = 0;
+	sgt[0].length = skb_headlen(skb);
+	sgt[0].extension = 0;
+	sgt[0].final = 0;
+
+	addr = dma_map_single(dpa_sg_bp->dev, skb->data, sgt[0].length,
+			dma_dir);
+	if (unlikely(dma_mapping_error(dpa_sg_bp->dev, addr))) {
+		dev_err(dpa_sg_bp->dev, "DMA mapping failed");
+		err = -EINVAL;
+		goto sg0_map_failed;
+	}
+
+	sgt[0].addr_hi = (uint8_t)upper_32_bits(addr);
+	sgt[0].addr_lo = cpu_to_be32(lower_32_bits(addr));
+
+	/* populate the rest of SGT entries */
+	for (i = 1; i <= nr_frags; i++) {
+		frag = &skb_shinfo(skb)->frags[i - 1];
+		sgt[i].bpid = dpa_sg_bp->bpid;
+		sgt[i].offset = 0;
+		sgt[i].length = frag->size;
+		sgt[i].extension = 0;
+		sgt[i].final = 0;
+
+		DPA_BUG_ON(!skb_frag_page(frag));
+		addr = skb_frag_dma_map(dpa_bp->dev, frag, 0, sgt[i].length,
+				dma_dir);
+		if (unlikely(dma_mapping_error(dpa_sg_bp->dev, addr))) {
+			dev_err(dpa_sg_bp->dev, "DMA mapping failed");
+			err = -EINVAL;
+			goto sg_map_failed;
+		}
+
+		/* keep the offset in the address */
+		sgt[i].addr_hi = (uint8_t)upper_32_bits(addr);
+		sgt[i].addr_lo = cpu_to_be32(lower_32_bits(addr));
+	}
+	sgt[i - 1].final = 1;
+
+	fd->length20 = skb->len;
+	fd->offset = priv->tx_headroom;
+
+	/* DMA map the SGT page */
+	buffer_start = (void *)sgt - dpa_fd_offset(fd);
+	/* Can't write at "negative" offset in buffer_start, because this skb
+	 * may not have been allocated by us.
+	 */
+	DPA_WRITE_SKB_PTR(skb, skbh, buffer_start, 0);
+
+	addr = dma_map_single(dpa_bp->dev, buffer_start,
+			priv->tx_headroom + sizeof(struct qm_sg_entry) * (1 + nr_frags),
+			dma_dir);
+	if (unlikely(dma_mapping_error(dpa_bp->dev, addr))) {
+		dev_err(dpa_bp->dev, "DMA mapping failed");
+		err = -EINVAL;
+		goto sgt_map_failed;
+	}
+
+	fd->bpid = dpa_bp->bpid;
+	fd->addr_hi = (uint8_t)upper_32_bits(addr);
+	fd->addr_lo = lower_32_bits(addr);
+
+	return 0;
+
+sgt_map_failed:
+sg_map_failed:
+	for (j = 0; j < i; j++)
+		dma_unmap_page(dpa_sg_bp->dev, qm_sg_addr(&sgt[j]),
+				be32_to_cpu(sgt[j].length), dma_dir);
+sg0_map_failed:
+csum_failed:
+	put_page(virt_to_head_page(sgt_buf));
+
+	return err;
+}
+
 static int __hot dpa_generic_tx(struct sk_buff *skb, struct net_device *netdev)
 {
 	struct dpa_generic_priv_s *priv = netdev_priv(netdev);
@@ -589,65 +758,72 @@ static int __hot dpa_generic_tx(struct sk_buff *skb, struct net_device *netdev)
 		raw_cpu_ptr(priv->percpu_priv);
 	struct rtnl_link_stats64 *percpu_stats = &percpu_priv->stats;
 	struct dpa_bp *bp = priv->draining_tx_bp;
+	struct dpa_bp *sg_bp = priv->draining_tx_sg_bp;
 	struct sk_buff **skbh = NULL;
 	dma_addr_t addr;
 	struct qm_fd fd;
 	int queue_mapping;
 	struct qman_fq *egress_fq;
+	const bool nonlinear = skb_is_nonlinear(skb);
 	int i = 0, err = 0;
 	int *countptr;
 
-	if (unlikely(skb_headroom(skb) < priv->tx_headroom)) {
-		struct sk_buff *skb_new;
+	if (nonlinear && skb_shinfo(skb)->nr_frags < DPA_SGT_MAX_ENTRIES) {
+		err = generic_skb_to_sg_fd(priv, skb, &fd);
+		if (unlikely(err < 0))
+			goto sg_failed;
+		percpu_priv->tx_frag_skbuffs++;
+	} else {
+		if (unlikely(skb_headroom(skb) < priv->tx_headroom)) {
+			struct sk_buff *skb_new;
+
+			skb_new = skb_realloc_headroom(skb, priv->tx_headroom);
+			if (unlikely(!skb_new)) {
+				percpu_stats->tx_errors++;
+				kfree_skb(skb);
+				goto done;
+			}
 
-		skb_new = skb_realloc_headroom(skb, priv->tx_headroom);
-		if (unlikely(!skb_new)) {
-			percpu_stats->tx_errors++;
 			kfree_skb(skb);
-			goto done;
+			skb = skb_new;
 		}
-		kfree_skb(skb);
-		skb = skb_new;
-	}
 
-	clear_fd(&fd);
+		clear_fd(&fd);
 
-	/* store skb backpointer to release the skb later */
-	skbh = (struct sk_buff **)(skb->data - priv->tx_headroom);
-	*skbh = skb;
+		/* store skb backpointer to release the skb later */
+		skbh = (struct sk_buff **)(skb->data - priv->tx_headroom);
+		*skbh = skb;
 
-	/* TODO check if skb->len + priv->tx_headroom < bp->size */
+		/* do this before dma_map_single(), because we may need to write
+		 * into the skb.
+		 */
+		err = dpa_generic_tx_csum(priv, skb, &fd,
+				((char *)skbh) + DPA_TX_PRIV_DATA_SIZE);
+		if (unlikely(err < 0)) {
+			if (netif_msg_tx_err(priv) && net_ratelimit())
+				netdev_err(netdev, "HW csum error: %d\n", err);
+			return err;
+		}
 
-	/* Enable L3/L4 hardware checksum computation.
-	 *
-	 * We must do this before dma_map_single(), because we may
-	 * need to write into the skb.
-	 */
-	err = dpa_generic_tx_csum(priv, skb, &fd,
-				 ((char *)skbh) + DPA_TX_PRIV_DATA_SIZE);
-	if (unlikely(err < 0)) {
-		if (netif_msg_tx_err(priv) && net_ratelimit())
-			netdev_err(netdev, "HW csum error: %d\n", err);
-		return err;
-	}
+		addr = dma_map_single(bp->dev, skbh,
+				skb->len + priv->tx_headroom, DMA_TO_DEVICE);
+		if (unlikely(dma_mapping_error(bp->dev, addr))) {
+			if (netif_msg_tx_err(priv)  && net_ratelimit())
+				netdev_err(netdev, "dma_map_single() failed\n");
+			goto dma_mapping_failed;
+		}
 
-	addr = dma_map_single(bp->dev, skbh,
-			skb->len + priv->tx_headroom, DMA_TO_DEVICE);
-	if (unlikely(dma_mapping_error(bp->dev, addr))) {
-		if (netif_msg_tx_err(priv)  && net_ratelimit())
-			netdev_err(netdev, "dma_map_single() failed\n");
-		goto dma_mapping_failed;
+		fd.format = qm_fd_contig;
+		fd.length20 = skb->len;
+		fd.offset = priv->tx_headroom;
+		fd.addr_hi = (uint8_t)upper_32_bits(addr);
+		fd.addr_lo = lower_32_bits(addr);
+		/* fd.cmd |= FM_FD_CMD_FCO; */
+		fd.bpid = bp->bpid;
 	}
 
-	fd.format = qm_fd_contig;
-	fd.length20 = skb->len;
-	fd.offset = priv->tx_headroom;
-	fd.addr_hi = (uint8_t)upper_32_bits(addr);
-	fd.addr_lo = lower_32_bits(addr);
-	/* fd.cmd |= FM_FD_CMD_FCO; */
-	fd.bpid = bp->bpid;
-
 	dpa_generic_drain_bp(bp, 1);
+	dpa_generic_drain_sg_bp(sg_bp, 1);
 
 	queue_mapping = dpa_get_queue_mapping(skb);
 	egress_fq = priv->egress_fqs[queue_mapping];
@@ -674,6 +850,7 @@ static int __hot dpa_generic_tx(struct sk_buff *skb, struct net_device *netdev)
 
 xmit_failed:
 	dma_unmap_single(bp->dev, addr, fd.offset + fd.length20, DMA_TO_DEVICE);
+sg_failed:
 dma_mapping_failed:
 	percpu_stats->tx_errors++;
 	dev_kfree_skb(skb);
@@ -743,7 +920,7 @@ static int dpa_generic_netdev_init(struct device_node *dpa_node,
 		return -EINVAL;
 	}
 
-	netdev->hw_features |= NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM;
+	netdev->hw_features |= NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM | NETIF_F_SG;
 	netdev->priv_flags |= IFF_LIVE_ADDR_CHANGE;
 	netdev->features |= netdev->hw_features;
 	netdev->vlan_features = netdev->features;
@@ -986,7 +1163,7 @@ static int dpa_generic_rx_bp_probe(struct platform_device *_of_dev,
 		}
 
 		bp[i].percpu_count = devm_alloc_percpu(dev,
-						       *bp[i].percpu_count);
+				*bp[i].percpu_count);
 	}
 
 	of_node_put(oh_node);
@@ -1025,11 +1202,13 @@ _return_of_node_put:
 static int dpa_generic_tx_bp_probe(struct platform_device *_of_dev,
 				   struct fm_port *tx_port,
 				   struct dpa_bp **draining_tx_bp,
+				   struct dpa_bp **draining_tx_sg_bp,
 				   struct dpa_buffer_layout_s **tx_buf_layout)
 {
 	struct device *dev = &_of_dev->dev;
 	struct fm_port_params params;
 	struct dpa_bp *bp = NULL;
+	struct dpa_bp *bp_sg = NULL;
 	struct dpa_buffer_layout_s *buf_layout = NULL;
 
 	buf_layout = devm_kzalloc(dev, sizeof(*buf_layout), GFP_KERNEL);
@@ -1058,6 +1237,19 @@ static int dpa_generic_tx_bp_probe(struct platform_device *_of_dev,
 	bp->target_count = CONFIG_FSL_DPAA_ETH_MAX_BUF_COUNT;
 
 	*draining_tx_bp = bp;
+
+	bp_sg = devm_kzalloc(dev, sizeof(*bp_sg), GFP_KERNEL);
+	if (unlikely(bp_sg == NULL)) {
+		dev_err(dev, "devm_kzalloc() failed\n");
+		return -ENOMEM;
+	}
+
+	bp_sg->size = dpa_bp_size(buf_layout);
+	bp_sg->percpu_count = alloc_percpu(*bp_sg->percpu_count);
+	bp_sg->target_count = CONFIG_FSL_DPAA_ETH_MAX_BUF_COUNT;
+
+	*draining_tx_sg_bp = bp_sg;
+
 	*tx_buf_layout = buf_layout;
 
 	return 0;
@@ -1318,6 +1510,7 @@ static int dpa_generic_bp_create(struct net_device *net_dev,
 				 struct dpa_bp *rx_bp,
 				 struct dpa_buffer_layout_s *rx_buf_layout,
 				 struct dpa_bp *draining_tx_bp,
+				 struct dpa_bp *draining_tx_sg_bp,
 				 struct dpa_buffer_layout_s *tx_buf_layout)
 {
 	struct dpa_generic_priv_s *priv = netdev_priv(net_dev);
@@ -1328,6 +1521,7 @@ static int dpa_generic_bp_create(struct net_device *net_dev,
 	priv->rx_bp = rx_bp;
 	priv->rx_buf_layout = rx_buf_layout;
 	priv->draining_tx_bp = draining_tx_bp;
+	priv->draining_tx_sg_bp = draining_tx_sg_bp;
 	priv->tx_buf_layout = tx_buf_layout;
 
 	err = dpa_bp_alloc(priv->rx_bp);
@@ -1344,6 +1538,13 @@ static int dpa_generic_bp_create(struct net_device *net_dev,
 		return err;
 	}
 
+	err = dpa_bp_alloc(priv->draining_tx_sg_bp);
+	if (err < 0) {
+		/* _dpa_bp_free(priv->draining_tx_bp); */
+		priv->draining_tx_sg_bp = NULL;
+		return err;
+	}
+
 	return 0;
 }
 
@@ -1429,6 +1630,7 @@ static int dpa_generic_eth_probe(struct platform_device *_of_dev)
 	int rx_bp_count = 0;
 	int disable_buff_dealloc = 0;
 	struct dpa_bp *rx_bp = NULL, *draining_tx_bp = NULL;
+	struct dpa_bp *draining_tx_sg_bp = NULL;
 	struct dpa_buffer_layout_s *rx_buf_layout = NULL, *tx_buf_layout = NULL;
 	struct list_head *dpa_fq_list;
 	static u8 generic_idx;
@@ -1448,7 +1650,7 @@ static int dpa_generic_eth_probe(struct platform_device *_of_dev)
 		return err;
 
 	err = dpa_generic_tx_bp_probe(_of_dev, tx_port, &draining_tx_bp,
-			&tx_buf_layout);
+			&draining_tx_sg_bp, &tx_buf_layout);
 	if (err < 0)
 		return err;
 
@@ -1481,7 +1683,7 @@ static int dpa_generic_eth_probe(struct platform_device *_of_dev)
 	priv->timer.function = dpa_generic_draining_timer;
 
 	err = dpa_generic_bp_create(netdev, rx_bp_count, rx_bp, rx_buf_layout,
-			draining_tx_bp, tx_buf_layout);
+			draining_tx_bp, draining_tx_sg_bp, tx_buf_layout);
 	if (err < 0)
 		goto bp_create_failed;
 
diff --git a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_generic.h b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_generic.h
index aa74edf..98f7073 100644
--- a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_generic.h
+++ b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_generic.h
@@ -46,6 +46,7 @@ struct dpa_generic_priv_s {
 	struct dpa_buffer_layout_s *rx_buf_layout;
 
 	struct dpa_bp *draining_tx_bp;
+	struct dpa_bp *draining_tx_sg_bp;
 	struct dpa_buffer_layout_s *tx_buf_layout;
 
 	/* Store here the needed Tx headroom for convenience and speed
-- 
2.9.3

