From da88811acea98c7e74b8b7b1ef14fd1f626924d2 Mon Sep 17 00:00:00 2001
From: Cristian Sovaiala <cristian.sovaiala@freescale.com>
Date: Fri, 17 Jul 2015 09:50:16 +0300
Subject: [PATCH 539/752] dpaa_eth: Use accessors to update and query the
 qm_sg_entry structure

Signed-off-by: Cristian Sovaiala <cristian.sovaiala@freescale.com>
Change-Id: Ie8546ed04a7e23eda4c5fa8a4cea041ddc40fccb
[Original patch taken from QorIQ-SDK-V2.0-20160527-yocto]
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 .../ethernet/freescale/sdk_dpaa/dpaa_eth_common.c  | 17 ++---
 .../net/ethernet/freescale/sdk_dpaa/dpaa_eth_sg.c  | 82 +++++++++-------------
 2 files changed, 40 insertions(+), 59 deletions(-)

diff --git a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_common.c b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_common.c
index 48b2663..f80a343 100644
--- a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_common.c
+++ b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_common.c
@@ -1543,26 +1543,23 @@ void dpa_release_sgt(struct qm_sg_entry *sgt)
 	memset(bmb, 0, DPA_BUFF_RELEASE_MAX * sizeof(struct bm_buffer));
 
 	do {
-		dpa_bp = dpa_bpid2pool(sgt[i].bpid);
+		dpa_bp = dpa_bpid2pool(qm_sg_entry_get_bpid(&sgt[i]));
 		DPA_BUG_ON(!dpa_bp);
 
 		j = 0;
 		do {
-			be32_to_cpus(&sgt[i].sgt_efl);
-			DPA_BUG_ON(sgt[i].extension);
-
-			bmb[j].hi       = sgt[i].addr_hi;
-			bmb[j].lo       = be32_to_cpu(sgt[i].addr_lo);
-
+			DPA_BUG_ON(qm_sg_entry_get_ext(&sgt[i]));
+			bm_buffer_set64(&bmb[j], qm_sg_addr(&sgt[i]));
 
 			j++; i++;
 		} while (j < ARRAY_SIZE(bmb) &&
-				!sgt[i-1].final &&
-				sgt[i-1].bpid == sgt[i].bpid);
+			!qm_sg_entry_get_final(&sgt[i-1]) &&
+			qm_sg_entry_get_bpid(&sgt[i-1]) ==
+			qm_sg_entry_get_bpid(&sgt[i]));
 
 		while (bman_release(dpa_bp->pool, bmb, j, 0))
 			cpu_relax();
-	} while (!sgt[i-1].final);
+	} while (!qm_sg_entry_get_final(&sgt[i-1]));
 }
 EXPORT_SYMBOL(dpa_release_sgt);
 
diff --git a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_sg.c b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_sg.c
index 823a499..b9532d6 100644
--- a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_sg.c
+++ b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_sg.c
@@ -250,21 +250,16 @@ struct sk_buff *_dpa_cleanup_tx_fd(const struct dpa_priv_s *priv,
 		}
 #endif /* CONFIG_FSL_DPAA_TS */
 
-		be32_to_cpus(&sgt[0].sgt_efl);
-
 		/* sgt[0] is from lowmem, was dma_map_single()-ed */
-		sg_addr = sgt[0].addr_hi << 32;
-		sg_addr += be32_to_cpu(sgt[0].addr_lo);
-		sg_len = sgt[0].length;
+		sg_addr = qm_sg_addr(&sgt[0]);
+		sg_len = qm_sg_entry_get_len(&sgt[0]);
 		dma_unmap_single(dpa_bp->dev, sg_addr, sg_len, dma_dir);
 
 		/* remaining pages were mapped with dma_map_page() */
 		for (i = 1; i <= nr_frags; i++) {
-			be32_to_cpus(&sgt[i].sgt_efl);
-			DPA_BUG_ON(sgt[i].extension);
-			sg_addr = sgt[i].addr_hi << 32;
-			sg_addr += be32_to_cpu(sgt[i].addr_lo);
-			sg_len = sgt[i].length;
+			DPA_BUG_ON(qm_sg_entry_get_ext(&sgt[i]));
+			sg_addr = qm_sg_addr(&sgt[i]);
+			sg_len = qm_sg_entry_get_len(&sgt[i]);
 			dma_unmap_page(dpa_bp->dev, sg_addr, sg_len, dma_dir);
 		}
 
@@ -429,14 +424,13 @@ static struct sk_buff *__hot sg_fd_to_skb(const struct dpa_priv_s *priv,
 	/* Iterate through the SGT entries and add data buffers to the skb */
 	sgt = vaddr + fd_off;
 	for (i = 0; i < DPA_SGT_MAX_ENTRIES; i++) {
-		be32_to_cpus(&sgt[i].sgt_efl);
 		/* Extension bit is not supported */
-		DPA_BUG_ON(sgt[i].extension);
+		DPA_BUG_ON(qm_sg_entry_get_ext(&sgt[i]));
 
 		/* We use a single global Rx pool */
-		DPA_BUG_ON(dpa_bp != dpa_bpid2pool(sgt[i].bpid));
+		DPA_BUG_ON(dpa_bp !=
+			   dpa_bpid2pool(qm_sg_entry_get_bpid(&sgt[i])));
 
-		be64_to_cpus(sgt + i);
 		sg_addr = qm_sg_addr(&sgt[i]);
 		sg_vaddr = phys_to_virt(sg_addr);
 		DPA_BUG_ON(!IS_ALIGNED((unsigned long)sg_vaddr,
@@ -471,7 +465,7 @@ static struct sk_buff *__hot sg_fd_to_skb(const struct dpa_priv_s *priv,
 			 */
 			DPA_BUG_ON(fd_off != priv->rx_headroom);
 			skb_reserve(skb, fd_off);
-			skb_put(skb, sgt[i].length);
+			skb_put(skb, qm_sg_entry_get_len(&sgt[i]));
 		} else {
 			/* Not the first S/G entry; all data from buffer will
 			 * be added in an skb fragment; fragment index is offset
@@ -497,8 +491,9 @@ static struct sk_buff *__hot sg_fd_to_skb(const struct dpa_priv_s *priv,
 			/* page_offset only refers to the beginning of sgt[i];
 			 * but the buffer itself may have an internal offset.
 			 */
-			frag_offset = sgt[i].offset + page_offset;
-			frag_len = sgt[i].length;
+			frag_offset = qm_sg_entry_get_offset(&sgt[i]) +
+					page_offset;
+			frag_len = qm_sg_entry_get_len(&sgt[i]);
 			/* skb_add_rx_frag() does no checking on the page; if
 			 * we pass it a tail page, we'll end up with
 			 * bad page accounting and eventually with segafults.
@@ -509,7 +504,7 @@ static struct sk_buff *__hot sg_fd_to_skb(const struct dpa_priv_s *priv,
 		/* Update the pool count for the current {cpu x bpool} */
 		(*count_ptr)--;
 
-		if (sgt[i].final)
+		if (qm_sg_entry_get_final(&sgt[i]))
 			break;
 	}
 	WARN_ONCE(i == DPA_SGT_MAX_ENTRIES, "No final bit on SGT\n");
@@ -772,14 +767,12 @@ int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
 	}
 
 	sgt = (struct qm_sg_entry *)(sgt_buf + priv->tx_headroom);
-	sgt[0].bpid = 0xff;
-	sgt[0].offset = 0;
-	sgt[0].length = skb_headlen(skb);
-	sgt[0].extension = 0;
-	sgt[0].final = 0;
-	sg_len = sgt[0].length;
-
-	cpu_to_be32s(&sgt[0].sgt_efl);
+	sg_len = skb_headlen(skb);
+	qm_sg_entry_set_bpid(&sgt[0], 0xff);
+	qm_sg_entry_set_offset(&sgt[0], 0);
+	qm_sg_entry_set_len(&sgt[0], sg_len);
+	qm_sg_entry_set_ext(&sgt[0], 0);
+	qm_sg_entry_set_final(&sgt[0], 0);
 
 	addr = dma_map_single(dpa_bp->dev, skb->data, sg_len, dma_dir);
 	if (unlikely(dma_mapping_error(dpa_bp->dev, addr))) {
@@ -789,38 +782,32 @@ int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
 
 	}
 
-	sgt[0].addr_hi = (uint8_t)upper_32_bits(addr);
-	sgt[0].addr_lo = cpu_to_be32(lower_32_bits(addr));
+	qm_sg_entry_set64(&sgt[0], addr);
 
 	/* populate the rest of SGT entries */
 	for (i = 1; i <= nr_frags; i++) {
 		frag = &skb_shinfo(skb)->frags[i - 1];
-		sgt[i].bpid = 0xff;
-		sgt[i].offset = 0;
-		sgt[i].length = frag->size;
-		sgt[i].extension = 0;
-		sg_len = sgt[i].length;
+		qm_sg_entry_set_bpid(&sgt[i], 0xff);
+		qm_sg_entry_set_offset(&sgt[i], 0);
+		qm_sg_entry_set_len(&sgt[i], frag->size);
+		qm_sg_entry_set_ext(&sgt[i], 0);
+
 		if (i == nr_frags)
-			sgt[i].final = 1;
+			qm_sg_entry_set_final(&sgt[i], 1);
 		else
-			sgt[i].final = 0;
-
+			qm_sg_entry_set_final(&sgt[i], 0);
 
 		DPA_BUG_ON(!skb_frag_page(frag));
-
-		cpu_to_be32s(&sgt[i].sgt_efl);
-
-		addr = skb_frag_dma_map(dpa_bp->dev, frag, 0, sg_len, dma_dir);
+		addr = skb_frag_dma_map(dpa_bp->dev, frag, 0, frag->size,
+					dma_dir);
 		if (unlikely(dma_mapping_error(dpa_bp->dev, addr))) {
 			dev_err(dpa_bp->dev, "DMA mapping failed");
 			err = -EINVAL;
 			goto sg_map_failed;
 		}
 
-
 		/* keep the offset in the address */
-		sgt[i].addr_hi = (uint8_t)upper_32_bits(addr);
-		sgt[i].addr_lo = cpu_to_be32(lower_32_bits(addr));
+		qm_sg_entry_set64(&sgt[i], addr);
 	}
 
 	fd->length20 = skb->len;
@@ -829,7 +816,6 @@ int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
 	/* DMA map the SGT page */
 	buffer_start = (void *)sgt - priv->tx_headroom;
 	DPA_WRITE_SKB_PTR(skb, skbh, buffer_start, 0);
-
 	addr = dma_map_single(dpa_bp->dev, buffer_start, priv->tx_headroom +
 			      sizeof(struct qm_sg_entry) * (1 + nr_frags),
 			      dma_dir);
@@ -848,11 +834,9 @@ int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
 sgt_map_failed:
 sg_map_failed:
 	for (j = 0; j < i; j++) {
-		be32_to_cpus(&sgt[j].sgt_efl);
-		sg_addr = sgt[j].addr_hi << 32;
-		sg_addr += be32_to_cpu(sgt[j].addr_lo);
-		dma_unmap_page(dpa_bp->dev, sg_addr, sgt[j].length,
-				dma_dir);
+		sg_addr = qm_sg_addr(&sgt[j]);
+		dma_unmap_page(dpa_bp->dev, sg_addr,
+			       qm_sg_entry_get_len(&sgt[j]), dma_dir);
 	}
 sg0_map_failed:
 csum_failed:
-- 
2.9.3

