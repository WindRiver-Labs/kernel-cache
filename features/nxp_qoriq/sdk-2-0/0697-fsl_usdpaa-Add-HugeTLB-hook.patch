From c196e8b3a0d4a45adf05b0bf22c3a93224c8ecfd Mon Sep 17 00:00:00 2001
From: Roy Pledge <roy.pledge@nxp.com>
Date: Mon, 11 Apr 2016 08:19:20 -0400
Subject: [PATCH 697/752] fsl_usdpaa: Add HugeTLB hook

Use a custom page fault handling routine to ensure huge pages are
used on PPC platforms

Signed-off-by: Roy Pledge <roy.pledge@nxp.com>
[Original patch taken from QorIQ-SDK-V2.0-20160527-yocto]
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 arch/powerpc/mm/mem.c | 33 ++++++++++++++++++++++++++++++++-
 1 file changed, 32 insertions(+), 1 deletion(-)

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 5f84433..d129d61 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -37,6 +37,9 @@
 #include <linux/slab.h>
 #include <linux/vmalloc.h>
 
+/* See hook_usdpaa_tlb1() */
+#include <linux/fsl_usdpaa.h>
+
 #include <asm/pgalloc.h>
 #include <asm/prom.h>
 #include <asm/io.h>
@@ -491,12 +494,38 @@ void flush_icache_user_range(struct vm_area_struct *vma, struct page *page,
 }
 EXPORT_SYMBOL(flush_icache_user_range);
 
+#ifdef CONFIG_FSL_USDPAA
+/*
+ * NB: this 'usdpaa' check+hack is to create TLB1 entries to cover the buffer
+ * memory used by run-to-completion UIO-based apps ("User-Space DataPath
+ * Acceleration Architecture"). It is expected to be phased out once HugeTLB
+ * support is hooked up with support for physical address conversion. The other
+ * half of this hack is in drivers/misc/fsl_usdpaa.c.
+ */
+static inline void hook_usdpaa_tlb1(struct vm_area_struct *vma,
+				    unsigned long address, pte_t *ptep)
+{
+	unsigned long pfn = pte_pfn(*ptep);
+	u64 phys_addr;
+	u64 size;
+	int tlb_idx = usdpaa_test_fault(pfn, &phys_addr, &size);
+	if (tlb_idx != -1) {
+		unsigned long va = address & ~(size - 1);
+		flush_tlb_mm(vma->vm_mm);
+		settlbcam(tlb_idx, va, phys_addr, size, pte_val(*ptep),
+			  mfspr(SPRN_PID));
+	}
+}
+#else
+#define hook_usdpaa_tlb1(a, b, c) do { } while (0)
+#endif
+
 /*
  * This is called at the end of handling a user page fault, when the
  * fault has been handled by updating a PTE in the linux page tables.
  * We use it to preload an HPTE into the hash table corresponding to
  * the updated linux PTE.
- * 
+ *
  * This must always be called with the pte lock held.
  */
 void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
@@ -537,6 +566,8 @@ void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
 	}
 
 	hash_preload(vma->vm_mm, address, access, trap);
+#elif defined(CONFIG_FSL_USDPAA)
+	hook_usdpaa_tlb1(vma, address, ptep);
 #endif /* CONFIG_PPC_STD_MMU */
 #if (defined(CONFIG_PPC_BOOK3E_64) || defined(CONFIG_PPC_FSL_BOOK3E)) \
 	&& defined(CONFIG_HUGETLB_PAGE)
-- 
2.9.3

