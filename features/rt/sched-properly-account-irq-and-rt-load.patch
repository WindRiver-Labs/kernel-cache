Subject: sched: properly account IRQ and RT load in SCHED_OTHER load balancing
Date: 	Thu, 14 Aug 2008 12:10:58 +0200
From: 	Peter Zijlstra <pzijlstr@redhat.com>


Seems to build and run - will have to wait for -rt2 to do some real
testing as I'm too lazy to pick up all the fixes and this kernel seems
to like to fall apart.

Will now see if I can get something together for the wake_idle() bit of
the original patch - but I guess we can do something similar..

---
 include/linux/hardirq.h |   10 ++++
 include/linux/sched.h   |    1 +
 kernel/sched.c          |  114 +++++++++++++++++++++++++++++++++++++++++++----
 kernel/sched_debug.c    |    2 +
 kernel/sched_rt.c       |    8 +++
 kernel/softirq.c        |    1 +
 kernel/sysctl.c         |    8 +++
 7 files changed, 135 insertions(+), 9 deletions(-)

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index 9c7cbbc..2849785 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -126,6 +126,14 @@ static inline void account_system_vtime(struct task_struct *tsk)
 }
 #endif
 
+#ifdef CONFIG_SMP
+extern void sched_irq_enter(void);
+extern void sched_irq_exit(void);
+#else
+# define sched_irq_enter() do { } while (0)
+# define sched_irq_exit() do { } while (0)
+#endif
+
 #if defined(CONFIG_PREEMPT_RCU) && defined(CONFIG_NO_HZ)
 extern void rcu_irq_enter(void);
 extern void rcu_irq_exit(void);
@@ -142,6 +150,7 @@ extern void rcu_irq_exit(void);
  */
 #define __irq_enter()					\
 	do {						\
+		sched_irq_enter();			\
 		rcu_irq_enter();			\
 		account_system_vtime(current);		\
 		add_preempt_count(HARDIRQ_OFFSET);	\
@@ -162,6 +171,7 @@ extern void irq_enter(void);
 		account_system_vtime(current);		\
 		sub_preempt_count(HARDIRQ_OFFSET);	\
 		rcu_irq_exit();				\
+		sched_irq_exit();			\
 	} while (0)
 
 /*
diff --git a/include/linux/sched.h b/include/linux/sched.h
index c979242..c926169 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1761,6 +1761,7 @@ extern unsigned int sysctl_sched_features;
 extern unsigned int sysctl_sched_migration_cost;
 extern unsigned int sysctl_sched_nr_migrate;
 extern unsigned int sysctl_sched_shares_ratelimit;
+extern unsigned int sysctl_sched_time_avg;
 
 int sched_nr_latency_handler(struct ctl_table *table, int write,
 		struct file *file, void __user *buffer, size_t *length,
diff --git a/kernel/sched.c b/kernel/sched.c
index 7631b58..e69cd5d 100644
--- a/kernel/sched.c
+++ b/kernel/sched.c
@@ -619,6 +619,12 @@ struct rq {
 
 	struct task_struct *migration_thread;
 	struct list_head migration_queue;
+
+	u64 irq_stamp;
+	unsigned long irq_time;
+	unsigned long rt_time;
+	u64 age_stamp;
+
 #endif
 
 #ifdef CONFIG_SCHED_HRTICK
@@ -887,6 +893,11 @@ unsigned int sysctl_sched_shares_ratelimit = 250000;
  */
 unsigned int sysctl_sched_rt_period = 1000000;
 
+/*
+ * period/2 over which we average the the IRQ and RT cpu consumption
+ */
+const_debug unsigned int sysctl_sched_time_avg = NSEC_PER_SEC / 2;
+
 static __read_mostly int scheduler_running;
 
 /*
@@ -1229,6 +1240,76 @@ static inline void init_hrtick(void)
 }
 #endif
 
+#ifdef CONFIG_SMP
+/*
+ * Measure IRQ time, we start when we first enter IRQ state
+ * end stop when we last leave IRQ state (nested IRQs).
+ */
+void sched_irq_enter(void)
+{
+	if (!in_irq()) {
+		struct rq *rq = this_rq();
+
+		update_rq_clock(rq);
+		rq->irq_stamp = rq->clock;
+	}
+}
+
+void sched_irq_exit(void)
+{
+	if (!in_irq()) {
+		struct rq *rq = this_rq();
+
+		update_rq_clock(rq);
+		rq->irq_time += rq->clock - rq->irq_stamp;
+	}
+}
+
+/*
+ * Every period/2 we half the accumulated time. See lib/proportions.c
+ */
+static void sched_age_time(struct rq *rq)
+{
+	if (rq->clock - rq->age_stamp >= sysctl_sched_time_avg) {
+		rq->irq_time /= 2;
+		rq->rt_time /= 2;
+		rq->age_stamp = rq->clock;
+	}
+}
+
+/*
+ * Scale the SCHED_OTHER load on this rq up to compensate for the pressure
+ * of IRQ and RT usage of this CPU.
+ *
+ * See lib/proportions.c
+ */
+static unsigned long sched_scale_load(struct rq *rq, u64 load)
+{
+	u32 period = sysctl_sched_time_avg + (rq->clock - rq->age_stamp);
+
+	load *= period;
+	period -= rq->irq_time + rq->rt_time;
+	if ((s64)period <= 0)
+		period = 1;
+	load = div_u64(load, period);
+
+	/*
+	 * Clip the maximal load value to something plenty high - at these
+	 * load values we're lost anyway.
+	 */
+	return min_t(unsigned long, load, 1UL << 22);
+}
+#else
+static inline void sched_age_time(struct rq *rq)
+{
+}
+
+static inline unsigned long sched_scale_load(struct rq *rq, unsigned long load)
+{
+	return load;
+}
+#endif
+
 /*
  * resched_task - mark a task 'to be rescheduled now'.
  *
@@ -1708,8 +1789,12 @@ static void dec_nr_running(struct rq *rq)
 static void set_load_weight(struct task_struct *p)
 {
 	if (task_has_rt_policy(p)) {
-		p->se.load.weight = prio_to_weight[0] * 2;
-		p->se.load.inv_weight = prio_to_wmult[0] >> 1;
+		/*
+		 * Real-time tasks do not contribute to SCHED_OTHER load
+		 * this is compensated by sched_scale_load() usage.
+		 */
+		p->se.load.weight = 0;
+		p->se.load.inv_weight = 0;
 		return;
 	}
 
@@ -2104,10 +2189,10 @@ static unsigned long source_load(int cpu, int type)
 	struct rq *rq = cpu_rq(cpu);
 	unsigned long total = weighted_cpuload(cpu);
 
-	if (type == 0 || !sched_feat(LB_BIAS))
-		return total;
+	if (type)
+		total = min(rq->cpu_load[type-1], total);
 
-	return min(rq->cpu_load[type-1], total);
+	return sched_scale_load(rq, total);
 }
 
 /*
@@ -2119,10 +2204,10 @@ static unsigned long target_load(int cpu, int type)
 	struct rq *rq = cpu_rq(cpu);
 	unsigned long total = weighted_cpuload(cpu);
 
-	if (type == 0 || !sched_feat(LB_BIAS))
-		return total;
+	if (type)
+		total = max(rq->cpu_load[type-1], total);
 
-	return max(rq->cpu_load[type-1], total);
+	return sched_scale_load(rq, total);
 }
 
 /*
@@ -3121,10 +3206,20 @@ balance_tasks(struct rq *this_rq, int this_cpu, struct rq *busiest,
 	int loops = 0, pulled = 0, pinned = 0;
 	struct task_struct *p;
 	long rem_load_move = max_load_move;
+	unsigned long busy_weight, this_weight, weight_scale;
 
 	if (max_load_move == 0)
 		goto out;
 
+	/*
+	 * Compute a weight scale to properly account for the varying
+	 * load inflation between these CPUs.
+	 */
+	busy_weight = sched_scale_load(busiest, NICE_0_LOAD);
+	this_weight = sched_scale_load(busiest, NICE_0_LOAD);
+
+	weight_scale = div_u64((u64)this_weight * NICE_0_LOAD, busy_weight);
+
 	pinned = 1;
 
 	/*
@@ -3143,7 +3238,7 @@ next:
 
 	pull_task(busiest, p, this_rq, this_cpu);
 	pulled++;
-	rem_load_move -= p->se.load.weight;
+	rem_load_move -= (weight_scale * p->se.load.weight) >> NICE_0_SHIFT;
 
 	/*
 	 * We only want to steal up to the prescribed amount of weighted load.
@@ -4441,6 +4536,7 @@ void scheduler_tick(void)
 	spin_lock(&rq->lock);
 	update_rq_clock(rq);
 	update_cpu_load(rq);
+	sched_age_time(rq);
 	if (curr != rq->idle && curr->se.on_rq)
 		curr->sched_class->task_tick(rq, curr, 0);
 	spin_unlock(&rq->lock);
diff --git a/kernel/sched_debug.c b/kernel/sched_debug.c
index 512a5d9..53ea144 100644
--- a/kernel/sched_debug.c
+++ b/kernel/sched_debug.c
@@ -161,6 +161,8 @@ void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 			SPLIT_NS(spread0));
 	SEQ_printf(m, "  .%-30s: %ld\n", "nr_running", cfs_rq->nr_running);
 	SEQ_printf(m, "  .%-30s: %ld\n", "load", cfs_rq->load.weight);
+	SEQ_printf(m, "  .%-30s: %ld\n", "scaled_load",
+			sched_scale_load(rq, cfs_rq->load.weight));
 #ifdef CONFIG_SCHEDSTATS
 #define P(n) SEQ_printf(m, "  .%-30s: %d\n", #n, rq->n);
 
diff --git a/kernel/sched_rt.c b/kernel/sched_rt.c
index 1c3e9f6..5091fd1 100644
--- a/kernel/sched_rt.c
+++ b/kernel/sched_rt.c
@@ -484,6 +484,14 @@ static void update_curr_rt(struct rq *rq)
 	if (unlikely((s64)delta_exec < 0))
 		delta_exec = 0;
 
+#ifdef CONFIG_SMP
+	/*
+	 * Account the time spend running RT tasks on this rq. Used to inflate
+	 * this rq's load values.
+	 */
+	rq->rt_time += delta_exec;
+#endif
+
 	schedstat_set(curr->se.exec_max, max(curr->se.exec_max, delta_exec));
 
 	curr->se.sum_exec_runtime += delta_exec;
diff --git a/kernel/softirq.c b/kernel/softirq.c
index d16dd9a..158d775 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -410,6 +410,7 @@ void irq_exit(void)
 	account_system_vtime(current);
 	trace_hardirq_exit();
 	sub_preempt_count(IRQ_EXIT_OFFSET);
+	sched_irq_exit();
 	if (!in_interrupt() && local_softirq_pending())
 		invoke_softirq();
 
diff --git a/kernel/sysctl.c b/kernel/sysctl.c
index bc07737..984ea4e 100644
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -317,6 +317,14 @@ static struct ctl_table kern_table[] = {
 		.mode		= 0644,
 		.proc_handler	= &proc_dointvec,
 	},
+	{
+		.ctl_name	= CTL_UNNUMBERED,
+		.procname	= "sched_time_avg",
+		.data		= &sysctl_sched_time_avg,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec,
+	},
 #endif
 	{
 		.ctl_name	= CTL_UNNUMBERED,
