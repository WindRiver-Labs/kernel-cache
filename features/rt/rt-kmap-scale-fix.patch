Hi Ingo,

Apply on top of what is still in -rt.

This seems to survive a kbuild -j64 & -j512 (although with that latter the
machine goes off for a while, but does return with a kernel).

If you can spare a cycle between hacking syslets and -rt, could you have a
look at the logic this patch adds?

---
 include/linux/sched.h |    1 +
 mm/highmem.c          |   97 ++++++++++++++++++++++++++++++++++++++++++++-----
 2 files changed, 88 insertions(+), 10 deletions(-)

diff --git a/include/linux/sched.h b/include/linux/sched.h
index c24ba62..49f3e9f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1619,6 +1619,7 @@ extern cputime_t task_gtime(struct task_struct *p);
 #define PF_MEMALLOC	0x00000800	/* Allocating memory */
 #define PF_FLUSHER	0x00001000	/* responsible for disk writeback */
 #define PF_USED_MATH	0x00002000	/* if unset the fpu must be initialized before use */
+#define PF_KMAP		0x00004000	/* this context has a kmap */
 #define PF_NOFREEZE	0x00008000	/* this thread should not be frozen */
 #define PF_FROZEN	0x00010000	/* frozen for system suspend */
 #define PF_FSTRANS	0x00020000	/* inside a filesystem transaction */
diff --git a/mm/highmem.c b/mm/highmem.c
index d4e8870..81c34e1 100644
--- a/mm/highmem.c
+++ b/mm/highmem.c
@@ -32,6 +32,7 @@
 #include <linux/hash.h>
 #include <linux/highmem.h>
 #include <linux/blktrace_api.h>
+#include <linux/hardirq.h>
 
 #include <asm/tlbflush.h>
 #include <asm/pgtable.h>
@@ -68,10 +69,13 @@ unsigned int nr_free_highpages (void)
  */
 static atomic_t pkmap_count[LAST_PKMAP];
 static atomic_t pkmap_hand;
+static atomic_t pkmap_free;
+static atomic_t pkmap_users;
 
 pte_t * pkmap_page_table;
 
-static DECLARE_WAIT_QUEUE_HEAD(pkmap_map_wait);
+static DECLARE_WAIT_QUEUE_HEAD(pkmap_wait);
+
 /*
  * Try to free a given kmap slot.
  *
@@ -85,6 +89,7 @@ static int pkmap_try_free(int pos)
 	if (atomic_cmpxchg(&pkmap_count[pos], 1, 0) != 1)
 		return -1;
 
+	atomic_dec(&pkmap_free);
 	/*
 	 * TODO: add a young bit to make it CLOCK
 	 */
@@ -113,7 +118,8 @@ static inline void pkmap_put(atomic_t *counter)
 		BUG();
 
 	case 1:
-		wake_up(&pkmap_map_wait);
+		atomic_inc(&pkmap_free);
+		wake_up(&pkmap_wait);
 	}
 }
 
@@ -122,11 +128,10 @@ static inline void pkmap_put(atomic_t *counter)
 static int pkmap_get_free(void)
 {
 	int i, pos, flush;
-	DECLARE_WAITQUEUE(wait, current);
 
 restart:
 	for (i = 0; i < LAST_PKMAP; i++) {
-		pos = atomic_inc_return(&pkmap_hand) % LAST_PKMAP;
+		pos = atomic_inc_return(&pkmap_hand) & LAST_PKMAP_MASK;
 		flush = pkmap_try_free(pos);
 		if (flush >= 0)
 			goto got_one;
@@ -135,10 +140,8 @@ restart:
 	/*
 	 * wait for somebody else to unmap their entries
 	 */
-	__set_current_state(TASK_UNINTERRUPTIBLE);
-	add_wait_queue(&pkmap_map_wait, &wait);
-	schedule();
-	remove_wait_queue(&pkmap_map_wait, &wait);
+	if (likely(!in_interrupt()))
+		wait_event(pkmap_wait, atomic_read(&pkmap_free) != 0);
 
 	goto restart;
 
@@ -147,7 +150,7 @@ got_one:
 #if 0
 		flush_tlb_kernel_range(PKMAP_ADDR(pos), PKMAP_ADDR(pos+1));
 #else
-		int pos2 = (pos + 1) % LAST_PKMAP;
+		int pos2 = (pos + 1) & LAST_PKMAP_MASK;
 		int nr;
 		int entries[TLB_BATCH];
 
@@ -157,7 +160,7 @@ got_one:
 		 * Scan ahead of the hand to minimise search distances.
 		 */
 		for (i = 0, nr = 0; i < LAST_PKMAP && nr < TLB_BATCH;
-				i++, pos2 = (pos2 + 1) % LAST_PKMAP) {
+				i++, pos2 = (pos2 + 1) & LAST_PKMAP_MASK) {
 
 			flush = pkmap_try_free(pos2);
 			if (flush < 0)
@@ -222,9 +225,79 @@ void kmap_flush_unused(void)
 	WARN_ON_ONCE(1);
 }
 
+/*
+ * Avoid starvation deadlock by limiting the number of tasks that can obtain a
+ * kmap to (LAST_PKMAP - KM_TYPE_NR*NR_CPUS)/2.
+ */
+static void kmap_account(void)
+{
+	int weight;
+
+#ifndef CONFIG_PREEMPT_RT
+	if (in_interrupt()) {
+		/* irqs can always get them */
+		weight = -1;
+	} else
+#endif
+	if (current->flags & PF_KMAP) {
+		current->flags &= ~PF_KMAP;
+		/* we already accounted the second */
+		weight = 0;
+	} else {
+		/* mark 1, account 2 */
+		current->flags |= PF_KMAP;
+		weight = 2;
+	}
+
+	if (weight > 0) {
+		/*
+		 * reserve KM_TYPE_NR maps per CPU for interrupt context
+		 */
+		const int target = LAST_PKMAP
+#ifndef CONFIG_PREEMPT_RT
+				- KM_TYPE_NR*NR_CPUS
+#endif
+			;
+
+again:
+		wait_event(pkmap_wait,
+			atomic_read(&pkmap_users) + weight <= target);
+
+		if (atomic_add_return(weight, &pkmap_users) > target) {
+			atomic_sub(weight, &pkmap_users);
+			goto again;
+		}
+	}
+}
+
+static void kunmap_account(void)
+{
+	int weight;
+
+#ifndef CONFIG_PREEMPT_RT
+	if (in_irq()) {
+		weight = -1;
+	} else
+#endif
+	if (current->flags & PF_KMAP) {
+		/* there was only 1 kmap, un-account both */
+		current->flags &= ~PF_KMAP;
+		weight = 2;
+	} else {
+		/* there were two kmaps, un-account per kunmap */
+		weight = 1;
+	}
+
+	if (weight > 0)
+		atomic_sub(weight, &pkmap_users);
+	wake_up(&pkmap_wait);
+}
+
  void *kmap_high(struct page *page)
 {
 	unsigned long vaddr;
+
+	kmap_account();
 again:
 	vaddr = (unsigned long)page_address(page);
 	if (vaddr) {
@@ -265,6 +338,7 @@ EXPORT_SYMBOL(kmap_high);
 	unsigned long vaddr = (unsigned long)page_address(page);
 	BUG_ON(!vaddr);
 	pkmap_put(&pkmap_count[PKMAP_NR(vaddr)]);
+	kunmap_account();
 }
 
 EXPORT_SYMBOL(kunmap_high);
@@ -420,6 +494,9 @@ void __init page_address_init(void)
 
 	for (i = 0; i < ARRAY_SIZE(pkmap_count); i++)
 		atomic_set(&pkmap_count[i], 1);
+	atomic_set(&pkmap_hand, 0);
+	atomic_set(&pkmap_free, LAST_PKMAP);
+	atomic_set(&pkmap_users, 0);
 #endif
 
 #ifdef HASHED_PAGE_VIRTUAL
