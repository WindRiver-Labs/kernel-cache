From: Steven Rostedt <srostedt@redhat.com>
Add event tracer.

This patch adds a event trace that hooks into various events
in the kernel. Although it can be used separately, it is mainly
to help other traces (wakeup and preempt off) with seeing various
events in the traces without having to enable the heavy mcount
hooks.

Signed-off-by: Steven Rostedt <srostedt@redhat.com>
---
 kernel/trace/Kconfig          |    8 +
 kernel/trace/Makefile         |    1 +
 kernel/trace/trace.c          |  220 ++++++++++++++++++
 kernel/trace/trace.h          |   98 ++++++++
 kernel/trace/trace_events.c   |  489 +++++++++++++++++++++++++++++++++++++++++
 kernel/trace/trace_selftest.c |    7 +
 6 files changed, 823 insertions(+), 0 deletions(-)

diff --git a/kernel/trace/Kconfig b/kernel/trace/Kconfig
index aa53fdd..f9f6901 100644
--- a/kernel/trace/Kconfig
+++ b/kernel/trace/Kconfig
@@ -93,6 +93,14 @@ config SCHED_TRACER
 	  This tracer tracks the latency of the highest priority task
 	  to be scheduled in, starting from the point it has woken up.
 
+config EVENT_TRACER
+	bool "trace kernel events"
+	select CONTEXT_SWITCH_TRACER
+	help
+	  This option activates the event tracer of the latency_tracer.
+	  It activates markers through out the kernel for tracing.
+	  This option has a fairly low overhead when enabled.
+
 config CONTEXT_SWITCH_TRACER
 	bool "Trace process context switches"
 	depends on HAVE_FTRACE
diff --git a/kernel/trace/Makefile b/kernel/trace/Makefile
index 71d17de..1036d42 100644
--- a/kernel/trace/Makefile
+++ b/kernel/trace/Makefile
@@ -20,5 +20,6 @@ obj-$(CONFIG_IRQSOFF_TRACER) += trace_irqsoff.o
 obj-$(CONFIG_PREEMPT_TRACER) += trace_irqsoff.o
 obj-$(CONFIG_SCHED_TRACER) += trace_sched_wakeup.o
 obj-$(CONFIG_MMIOTRACE) += trace_mmiotrace.o
+obj-$(CONFIG_EVENT_TRACER) += trace_events.o
 
 libftrace-y := ftrace.o
diff --git a/kernel/trace/trace.c b/kernel/trace/trace.c
index f88e031..0dcef22 100644
--- a/kernel/trace/trace.c
+++ b/kernel/trace/trace.c
@@ -1021,6 +1021,126 @@ ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3)
 	local_irq_restore(flags);
 }
 
+void tracing_event_irq(struct trace_array *tr,
+		       struct trace_array_cpu *data,
+		       unsigned long flags,
+		       unsigned long ip,
+		       int irq, int usermode,
+		       unsigned long retip)
+{
+	struct trace_entry *entry;
+
+	entry = tracing_get_trace_entry(tr, data);
+	tracing_generic_entry_update(entry, flags);
+	entry->type			= TRACE_IRQ;
+	entry->irq.ip		= ip;
+	entry->irq.irq		= irq;
+	entry->irq.ret_ip		= retip;
+	entry->irq.usermode	= usermode;
+}
+
+void tracing_event_fault(struct trace_array *tr,
+			 struct trace_array_cpu *data,
+			 unsigned long flags,
+			 unsigned long ip,
+			 unsigned long retip,
+			 unsigned long error_code,
+			 unsigned long address)
+{
+	struct trace_entry *entry;
+
+	entry = tracing_get_trace_entry(tr, data);
+	tracing_generic_entry_update(entry, flags);
+	entry->type			= TRACE_FAULT;
+	entry->fault.ip		= ip;
+	entry->fault.ret_ip	= retip;
+	entry->fault.errorcode	= error_code;
+	entry->fault.address	= address;
+}
+
+void tracing_event_timer_set(struct trace_array *tr,
+			     struct trace_array_cpu *data,
+			     unsigned long flags,
+			     unsigned long ip,
+			     ktime_t *expires, void *timer)
+{
+	struct trace_entry *entry;
+
+	entry = tracing_get_trace_entry(tr, data);
+	tracing_generic_entry_update(entry, flags);
+	entry->type			= TRACE_TIMER_SET;
+	entry->timer.ip		= ip;
+	entry->timer.expire	= *expires;
+	entry->timer.timer	= timer;
+}
+
+void tracing_event_timer_triggered(struct trace_array *tr,
+				   struct trace_array_cpu *data,
+				   unsigned long flags,
+				   unsigned long ip,
+				   ktime_t *expired, void *timer)
+{
+	struct trace_entry *entry;
+
+	entry = tracing_get_trace_entry(tr, data);
+	tracing_generic_entry_update(entry, flags);
+	entry->type			= TRACE_TIMER_TRIG;
+	entry->timer.ip		= ip;
+	entry->timer.expire	= *expired;
+	entry->timer.timer	= timer;
+}
+
+void tracing_event_timestamp(struct trace_array *tr,
+			     struct trace_array_cpu *data,
+			     unsigned long flags,
+			     unsigned long ip,
+			     ktime_t *now)
+{
+	struct trace_entry *entry;
+
+	entry = tracing_get_trace_entry(tr, data);
+	tracing_generic_entry_update(entry, flags);
+	entry->type			= TRACE_TIMESTAMP;
+	entry->timestamp.ip		= ip;
+	entry->timestamp.now		= *now;
+}
+
+void tracing_event_task_activate(struct trace_array *tr,
+				 struct trace_array_cpu *data,
+				 unsigned long flags,
+				 unsigned long ip,
+				 struct task_struct *p,
+				 int task_cpu)
+{
+	struct trace_entry *entry;
+
+	entry = tracing_get_trace_entry(tr, data);
+	tracing_generic_entry_update(entry, flags);
+	entry->type			= TRACE_TASK_ACT;
+	entry->task.ip		= ip;
+	entry->task.pid		= p->pid;
+	entry->task.prio		= p->prio;
+	entry->task.cpu		= task_cpu;
+}
+
+void tracing_event_task_deactivate(struct trace_array *tr,
+				   struct trace_array_cpu *data,
+				   unsigned long flags,
+				   unsigned long ip,
+				   struct task_struct *p,
+				   int task_cpu)
+{
+	struct trace_entry *entry;
+
+	entry = tracing_get_trace_entry(tr, data);
+	tracing_generic_entry_update(entry, flags);
+	entry->type			= TRACE_TASK_DEACT;
+	entry->task.ip		= ip;
+	entry->task.pid		= p->pid;
+	entry->task.prio		= p->prio;
+	entry->task.cpu		= task_cpu;
+}
+
 #ifdef CONFIG_FTRACE
 static void
 function_trace_call(unsigned long ip, unsigned long parent_ip)
@@ -1560,6 +1680,55 @@ print_lat_fmt(struct trace_iterator *iter, unsigned int trace_idx, int cpu)
 		}
 		trace_seq_puts(s, "\n");
 		break;
+	case TRACE_IRQ:
+		seq_print_ip_sym(s, entry->irq.ip, sym_flags);
+		if (entry->irq.irq >= 0)
+			trace_seq_printf(s, " %d ", entry->irq.irq);
+		if (entry->irq.usermode)
+			trace_seq_puts(s, " (usermode)\n ");
+		else {
+			trace_seq_puts(s, " (");
+			seq_print_ip_sym(s, entry->irq.ret_ip, sym_flags);
+			trace_seq_puts(s, ")\n");
+		}
+		break;
+	case TRACE_FAULT:
+		seq_print_ip_sym(s, entry->fault.ip, sym_flags);
+		trace_seq_printf(s, " %lx ", entry->fault.errorcode);
+		trace_seq_puts(s, " (");
+		seq_print_ip_sym(s, entry->fault.ret_ip, sym_flags);
+		trace_seq_puts(s, ")");
+		trace_seq_printf(s, " [%lx]\n", entry->fault.address);
+		break;
+	case TRACE_TIMER_SET:
+		seq_print_ip_sym(s, entry->timer.ip, sym_flags);
+		trace_seq_printf(s, " (%Ld) (%p)\n",
+			   entry->timer.expire, entry->timer.timer);
+		break;
+	case TRACE_TIMER_TRIG:
+		seq_print_ip_sym(s, entry->timer.ip, sym_flags);
+		trace_seq_printf(s, " (%Ld) (%p)\n",
+			   entry->timer.expire, entry->timer.timer);
+		break;
+	case TRACE_TIMESTAMP:
+		seq_print_ip_sym(s, entry->timestamp.ip, sym_flags);
+		trace_seq_printf(s, " (%Ld)\n",
+			   entry->timestamp.now.tv64);
+		break;
+	case TRACE_TASK_ACT:
+		seq_print_ip_sym(s, entry->task.ip, sym_flags);
+		comm = trace_find_cmdline(entry->task.pid);
+		trace_seq_printf(s, " %s %d %d [%d]\n",
+			   comm, entry->task.pid,
+			   entry->task.prio, entry->task.cpu);
+		break;
+	case TRACE_TASK_DEACT:
+		seq_print_ip_sym(s, entry->task.ip, sym_flags);
+		comm = trace_find_cmdline(entry->task.pid);
+		trace_seq_printf(s, " %s %d %d [%d]\n",
+			   comm, entry->task.pid,
+			   entry->task.prio, entry->task.cpu);
+		break;
 	default:
 		trace_seq_printf(s, "Unknown type %d\n", entry->type);
 	}
@@ -1660,6 +1829,55 @@ static int print_trace_fmt(struct trace_iterator *iter)
 		if (!ret)
 			return 0;
 		break;
+	case TRACE_IRQ:
+		seq_print_ip_sym(s, entry->irq.ip, sym_flags);
+		if (entry->irq.irq >= 0)
+			trace_seq_printf(s, " %d ", entry->irq.irq);
+		if (entry->irq.usermode)
+			trace_seq_puts(s, " (usermode)\n ");
+		else {
+			trace_seq_puts(s, " (");
+			seq_print_ip_sym(s, entry->irq.ret_ip, sym_flags);
+			trace_seq_puts(s, ")\n");
+		}
+		break;
+	case TRACE_FAULT:
+		seq_print_ip_sym(s, entry->fault.ip, sym_flags);
+		trace_seq_printf(s, " %lx ", entry->fault.errorcode);
+		trace_seq_puts(s, " (");
+		seq_print_ip_sym(s, entry->fault.ret_ip, sym_flags);
+		trace_seq_puts(s, ")");
+		trace_seq_printf(s, " [%lx]\n", entry->fault.address);
+		break;
+	case TRACE_TIMER_SET:
+		seq_print_ip_sym(s, entry->timer.ip, sym_flags);
+		trace_seq_printf(s, " (%Ld) (%p)\n",
+			   entry->timer.expire, entry->timer.timer);
+		break;
+	case TRACE_TIMER_TRIG:
+		seq_print_ip_sym(s, entry->timer.ip, sym_flags);
+		trace_seq_printf(s, " (%Ld) (%p)\n",
+			   entry->timer.expire, entry->timer.timer);
+		break;
+	case TRACE_TIMESTAMP:
+		seq_print_ip_sym(s, entry->timestamp.ip, sym_flags);
+		trace_seq_printf(s, " (%Ld)\n",
+			   entry->timestamp.now.tv64);
+		break;
+	case TRACE_TASK_ACT:
+		seq_print_ip_sym(s, entry->task.ip, sym_flags);
+		comm = trace_find_cmdline(entry->task.pid);
+		trace_seq_printf(s, " %s %d %d [%d]\n",
+			   comm, entry->task.pid,
+			   entry->task.prio, entry->task.cpu);
+		break;
+	case TRACE_TASK_DEACT:
+		seq_print_ip_sym(s, entry->task.ip, sym_flags);
+		comm = trace_find_cmdline(entry->task.pid);
+		trace_seq_printf(s, " %s %d %d [%d]\n",
+			   comm, entry->task.pid,
+			   entry->task.prio, entry->task.cpu);
+		break;
 	}
 	return 1;
 }
@@ -1804,6 +2022,8 @@ static int print_bin_fmt(struct trace_iterator *iter)
 		SEQ_PUT_FIELD_RET(s, entry->special.arg2);
 		SEQ_PUT_FIELD_RET(s, entry->special.arg3);
 		break;
+	default:
+		trace_seq_printf(s, "Unknown type %d\n", entry->type);
 	}
 	return 1;
 }
diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index f69f867..3f3660b 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -17,6 +17,13 @@ enum trace_type {
 	TRACE_SPECIAL,
 	TRACE_MMIO_RW,
 	TRACE_MMIO_MAP,
+	TRACE_IRQ,
+	TRACE_FAULT,
+	TRACE_TIMER_SET,
+	TRACE_TIMER_TRIG,
+	TRACE_TIMESTAMP,
+	TRACE_TASK_ACT,
+	TRACE_TASK_DEACT,
 
 	__TRACE_LAST_TYPE
 };
@@ -50,6 +57,45 @@ struct special_entry {
 	unsigned long		arg3;
 };
 
+struct irq_entry {
+	unsigned long		ip;
+	unsigned long		ret_ip;
+	unsigned		irq;
+	unsigned		usermode;
+};
+
+struct fault_entry {
+	unsigned long		ip;
+	unsigned long		ret_ip;
+	unsigned long		errorcode;
+	unsigned long		address;
+};
+
+struct timer_entry {
+	unsigned long		ip;
+	ktime_t			expire;
+	void			*timer;
+};
+
+struct timestamp_entry {
+	unsigned long		ip;
+	ktime_t			now;
+};
+
+struct task_entry {
+	unsigned long		ip;
+	pid_t			pid;
+	unsigned		prio;
+	int			cpu;
+};
+
+struct wakeup_entry {
+	unsigned long		ip;
+	pid_t			pid;
+	unsigned		prio;
+	unsigned		curr_prio;
+};
+
 /*
  * Stack-trace entry:
  */
@@ -80,6 +126,12 @@ struct trace_entry {
 		struct stack_entry		stack;
 		struct mmiotrace_rw		mmiorw;
 		struct mmiotrace_map		mmiomap;
+		struct irq_entry		irq;
+		struct fault_entry		fault;
+		struct timer_entry		timer;
+		struct timestamp_entry		timestamp;
+		struct task_entry		task;
+		struct wakeup_entry		wakeup;
 	};
 };
 
@@ -222,6 +274,52 @@ void trace_function(struct trace_array *tr,
 		    unsigned long ip,
 		    unsigned long parent_ip,
 		    unsigned long flags);
+void tracing_event_irq(struct trace_array *tr,
+		       struct trace_array_cpu *data,
+		       unsigned long flags,
+		       unsigned long ip,
+		       int irq, int usermode,
+		       unsigned long retip);
+void tracing_event_fault(struct trace_array *tr,
+			 struct trace_array_cpu *data,
+			 unsigned long flags,
+			 unsigned long ip,
+			 unsigned long retip,
+			 unsigned long error_code,
+			 unsigned long address);
+void tracing_event_timer_set(struct trace_array *tr,
+			     struct trace_array_cpu *data,
+			     unsigned long flags,
+			     unsigned long ip,
+			     ktime_t *expires, void *timer);
+void tracing_event_timer_triggered(struct trace_array *tr,
+				   struct trace_array_cpu *data,
+				   unsigned long flags,
+				   unsigned long ip,
+				   ktime_t *expired, void *timer);
+void tracing_event_timestamp(struct trace_array *tr,
+			     struct trace_array_cpu *data,
+			     unsigned long flags,
+			     unsigned long ip,
+			     ktime_t *now);
+void tracing_event_task_activate(struct trace_array *tr,
+				 struct trace_array_cpu *data,
+				 unsigned long flags,
+				 unsigned long ip,
+				 struct task_struct *p,
+				 int cpu);
+void tracing_event_task_deactivate(struct trace_array *tr,
+				   struct trace_array_cpu *data,
+				   unsigned long flags,
+				   unsigned long ip,
+				   struct task_struct *p,
+				   int cpu);
+void tracing_event_wakeup(struct trace_array *tr,
+			  struct trace_array_cpu *data,
+			  unsigned long flags,
+			  unsigned long ip,
+			  pid_t pid, int prio,
+			  int curr_prio);
 
 void tracing_start_cmdline_record(void);
 void tracing_stop_cmdline_record(void);
diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
new file mode 100644
index 0000000..55801c2
--- /dev/null
+++ b/kernel/trace/trace_events.c
@@ -0,0 +1,489 @@
+/*
+ * trace task events
+ *
+ * Copyright (C) 2007 Steven Rostedt <srostedt@redhat.com>
+ *
+ * Based on code from the latency_tracer, that is:
+ *
+ *  Copyright (C) 2004-2006 Ingo Molnar
+ *  Copyright (C) 2004 William Lee Irwin III
+ */
+#include <linux/module.h>
+#include <linux/fs.h>
+#include <linux/debugfs.h>
+#include <linux/kallsyms.h>
+#include <linux/uaccess.h>
+#include <linux/ftrace.h>
+
+#include <trace/sched.h>
+
+#include "trace.h"
+
+static struct trace_array __read_mostly	*events_trace;
+static int __read_mostly	tracer_enabled;
+static atomic_t			event_ref;
+
+static void event_reset(struct trace_array *tr)
+{
+	struct trace_array_cpu *data;
+	int cpu;
+
+	for_each_possible_cpu(cpu) {
+		data = tr->data[cpu];
+		tracing_reset(data);
+	}
+
+	tr->time_start = ftrace_now(raw_smp_processor_id());
+}
+
+static void
+event_irq_callback(int irq, int user, unsigned long ip)
+{
+	struct trace_array *tr = events_trace;
+	struct trace_array_cpu *data;
+	unsigned long flags;
+	int cpu;
+	long disable;
+
+	if (!tracer_enabled)
+		return;
+
+	/* interrupts should be off, we are in an interrupt */
+	cpu = smp_processor_id();
+	data = tr->data[cpu];
+
+	disable = atomic_inc_return(&data->disabled);
+	if (disable != 1)
+		goto out;
+
+	local_save_flags(flags);
+	tracing_event_irq(tr, data, flags, CALLER_ADDR1, irq, user, ip);
+
+ out:
+	atomic_dec(&data->disabled);
+}
+
+static void
+event_fault_callback(unsigned long ip, unsigned long error, unsigned long addr)
+{
+	struct trace_array *tr = events_trace;
+	struct trace_array_cpu *data;
+	unsigned long flags;
+	long disable;
+	int cpu;
+
+	if (!tracer_enabled)
+		return;
+
+	preempt_disable_notrace();
+	cpu = smp_processor_id();
+	data = tr->data[cpu];
+
+	disable = atomic_inc_return(&data->disabled);
+	if (disable != 1)
+		goto out;
+
+	local_save_flags(flags);
+	tracing_event_fault(tr, data, flags, CALLER_ADDR1, ip, error, addr);
+
+ out:
+	atomic_dec(&data->disabled);
+	preempt_enable_notrace();
+}
+
+static void
+event_timer_set_callback(ktime_t *expires, struct hrtimer *timer)
+{
+	struct trace_array *tr = events_trace;
+	struct trace_array_cpu *data;
+	unsigned long flags;
+	long disable;
+	int cpu;
+
+	if (!tracer_enabled)
+		return;
+
+	/* interrupts should be off, we are in an interrupt */
+	cpu = smp_processor_id();
+	data = tr->data[cpu];
+
+	disable = atomic_inc_return(&data->disabled);
+	if (disable != 1)
+		goto out;
+
+	local_save_flags(flags);
+	tracing_event_timer_set(tr, data, flags, CALLER_ADDR1, expires, timer);
+
+ out:
+	atomic_dec(&data->disabled);
+}
+
+static void
+event_timer_triggered_callback(ktime_t *expires, struct hrtimer *timer)
+{
+	struct trace_array *tr = events_trace;
+	struct trace_array_cpu *data;
+	unsigned long flags;
+	long disable;
+	int cpu;
+
+	if (!tracer_enabled)
+		return;
+
+	/* interrupts should be off, we are in an interrupt */
+	cpu = smp_processor_id();
+	data = tr->data[cpu];
+
+	disable = atomic_inc_return(&data->disabled);
+	if (disable != 1)
+		goto out;
+
+	local_save_flags(flags);
+	tracing_event_timer_triggered(tr, data, flags, CALLER_ADDR1, expires, timer);
+
+ out:
+	atomic_dec(&data->disabled);
+}
+
+static void
+event_hrtimer_callback(ktime_t *time)
+{
+	struct trace_array *tr = events_trace;
+	struct trace_array_cpu *data;
+	unsigned long flags;
+	long disable;
+	int cpu;
+
+	if (!tracer_enabled)
+		return;
+
+	/* interrupts should be off, we are in an interrupt */
+	cpu = smp_processor_id();
+	data = tr->data[cpu];
+
+	disable = atomic_inc_return(&data->disabled);
+	if (disable != 1)
+		goto out;
+
+	local_save_flags(flags);
+	tracing_event_timestamp(tr, data, flags, CALLER_ADDR1, time);
+
+ out:
+	atomic_dec(&data->disabled);
+}
+
+static void
+event_task_activate_callback(struct task_struct *p, int rqcpu)
+{
+	struct trace_array *tr = events_trace;
+	struct trace_array_cpu *data;
+	unsigned long flags;
+	long disable;
+	int cpu;
+
+	if (!tracer_enabled)
+		return;
+
+	/* interrupts should be off, we are in an interrupt */
+	cpu = smp_processor_id();
+	data = tr->data[cpu];
+
+	disable = atomic_inc_return(&data->disabled);
+	if (disable != 1)
+		goto out;
+
+	local_save_flags(flags);
+	tracing_event_task_activate(tr, data, flags, CALLER_ADDR1, p, rqcpu);
+
+ out:
+	atomic_dec(&data->disabled);
+}
+
+static void
+event_task_deactivate_callback(struct task_struct *p, int rqcpu)
+{
+	struct trace_array *tr = events_trace;
+	struct trace_array_cpu *data;
+	unsigned long flags;
+	long disable;
+	int cpu;
+
+	if (!tracer_enabled)
+		return;
+
+	/* interrupts should be off, we are in an interrupt */
+	cpu = smp_processor_id();
+	data = tr->data[cpu];
+
+	disable = atomic_inc_return(&data->disabled);
+	if (disable != 1)
+		goto out;
+
+	local_save_flags(flags);
+	tracing_event_task_deactivate(tr, data, flags, CALLER_ADDR1, p, rqcpu);
+
+ out:
+	atomic_dec(&data->disabled);
+}
+
+static void
+event_wakeup_callback(struct rq *rq, struct task_struct *wakee)
+{
+	struct trace_array *tr = events_trace;
+	struct trace_array_cpu *data;
+	unsigned long flags;
+	long disable;
+	int cpu;
+
+	if (!tracer_enabled)
+		return;
+
+	/* interrupts should be disabled */
+	cpu = smp_processor_id();
+	data = tr->data[cpu];
+
+	disable = atomic_inc_return(&data->disabled);
+	if (unlikely(disable != 1))
+		goto out;
+
+	local_save_flags(flags);
+	/* record process's command line */
+	tracing_record_cmdline(wakee);
+	tracing_record_cmdline(current);
+
+	tracing_sched_wakeup_trace(tr, data, wakee, current, flags);
+
+ out:
+	atomic_dec(&data->disabled);
+}
+
+static void
+event_ctx_callback(struct rq *rq, struct task_struct *prev,
+		   struct task_struct *next)
+{
+	struct trace_array *tr = events_trace;
+	struct trace_array_cpu *data;
+	unsigned long flags;
+	long disable;
+	int cpu;
+
+	if (!tracer_enabled)
+		return;
+
+	tracing_record_cmdline(prev);
+	tracing_record_cmdline(next);
+
+	/* interrupts should be disabled */
+	cpu = smp_processor_id();
+	data = tr->data[cpu];
+	disable = atomic_inc_return(&data->disabled);
+
+	if (likely(disable != 1))
+		goto out;
+
+	local_save_flags(flags);
+	tracing_sched_switch_trace(tr, data, prev, next, flags);
+ out:
+	atomic_dec(&data->disabled);
+}
+
+static void event_tracer_register(struct trace_array *tr)
+{
+	int ret;
+
+	ret = register_trace_event_irq(event_irq_callback);
+	if (ret) {
+		pr_info("event trace: Couldn't activate tracepoint"
+			" probe to event_irq\n");
+		return;
+	}
+
+	ret = register_trace_event_fault(event_fault_callback);
+	if (ret) {
+		pr_info("event trace: Couldn't activate tracepoint"
+			" probe to event_fault\n");
+		goto out1;
+	}
+
+	ret = register_trace_event_timer_set(event_timer_set_callback);
+	if (ret) {
+		pr_info("event trace: Couldn't activate tracepoint"
+			" probe to event_timer_set\n");
+		goto out2;
+	}
+
+	ret = register_trace_event_timer_triggered(event_timer_triggered_callback);
+	if (ret) {
+		pr_info("event trace: Couldn't activate tracepoint"
+			" probe to event_timer_triggered\n");
+		goto out3;
+	}
+
+	ret = register_trace_event_timestamp(event_hrtimer_callback);
+	if (ret) {
+		pr_info("event trace: Couldn't activate tracepoint"
+			" probe to event_timestamp\n");
+		goto out4;
+	}
+
+	ret = register_trace_event_task_activate(event_task_activate_callback);
+	if (ret) {
+		pr_info("event trace: Couldn't activate tracepoint"
+			" probe to event_task_activate\n");
+		goto out5;
+	}
+
+	ret = register_trace_event_task_deactivate(event_task_deactivate_callback);
+	if (ret) {
+		pr_info("event trace: Couldn't activate tracepoint"
+			" probe to event_task_deactivate\n");
+		goto out6;
+	}
+
+	ret = register_trace_sched_wakeup(event_wakeup_callback);
+	if (ret) {
+		pr_info("event trace: Couldn't activate tracepoint"
+			" probe to kernel_sched_wakeup\n");
+		goto out7;
+	}
+
+	ret = register_trace_sched_wakeup_new(event_wakeup_callback);
+	if (ret) {
+		pr_info("event trace: Couldn't activate tracepoint"
+			" probe to kernel_sched_wakeup_new\n");
+		goto out8;
+	}
+
+	ret = register_trace_sched_switch(event_ctx_callback);
+	if (ret) {
+		pr_info("event trace: Couldn't activate tracepoint"
+			" probe to kernel_sched_schedule\n");
+		goto out9;
+	}
+
+	return;
+
+ out9:
+	unregister_trace_sched_wakeup_new(event_wakeup_callback);
+ out8:
+	unregister_trace_sched_wakeup(event_wakeup_callback);
+ out7:
+	unregister_trace_event_task_deactivate(event_task_deactivate_callback);
+ out6:
+	unregister_trace_event_task_activate(event_task_activate_callback);
+ out5:
+	unregister_trace_event_timestamp(event_hrtimer_callback);
+ out4:
+	unregister_trace_event_timer_triggered(event_timer_triggered_callback);
+ out3:
+	unregister_trace_event_timer_set(event_timer_set_callback);
+ out2:
+	unregister_trace_event_fault(event_fault_callback);
+ out1:
+	unregister_trace_event_irq(event_irq_callback);
+}
+
+static void event_tracer_unregister(struct trace_array *tr)
+{
+	unregister_trace_sched_switch(event_ctx_callback);
+	unregister_trace_sched_wakeup_new(event_wakeup_callback);
+	unregister_trace_sched_wakeup(event_wakeup_callback);
+	unregister_trace_event_task_deactivate(event_task_deactivate_callback);
+	unregister_trace_event_task_activate(event_task_activate_callback);
+	unregister_trace_event_timestamp(event_hrtimer_callback);
+	unregister_trace_event_timer_triggered(event_timer_triggered_callback);
+	unregister_trace_event_timer_set(event_timer_set_callback);
+	unregister_trace_event_fault(event_fault_callback);
+	unregister_trace_event_irq(event_irq_callback);
+}
+
+void trace_event_register(struct trace_array *tr)
+{
+	long ref;
+
+	ref = atomic_inc_return(&event_ref);
+	if (ref == 1)
+		event_tracer_register(tr);
+}
+
+void trace_event_unregister(struct trace_array *tr)
+{
+	long ref;
+
+	ref = atomic_dec_and_test(&event_ref);
+	if (ref)
+		event_tracer_unregister(tr);
+}
+
+static void start_event_trace(struct trace_array *tr)
+{
+	event_reset(tr);
+	trace_event_register(tr);
+	tracing_start_function_trace();
+	tracer_enabled = 1;
+}
+
+static void stop_event_trace(struct trace_array *tr)
+{
+	tracer_enabled = 0;
+	tracing_stop_function_trace();
+	trace_event_unregister(tr);
+}
+
+static void event_trace_init(struct trace_array *tr)
+{
+	events_trace = tr;
+
+	if (tr->ctrl)
+		start_event_trace(tr);
+}
+
+static void event_trace_reset(struct trace_array *tr)
+{
+	if (tr->ctrl)
+		stop_event_trace(tr);
+}
+
+static void event_trace_ctrl_update(struct trace_array *tr)
+{
+	if (tr->ctrl)
+		start_event_trace(tr);
+	else
+		stop_event_trace(tr);
+}
+
+static void event_trace_open(struct trace_iterator *iter)
+{
+	/* stop the trace while dumping */
+	if (iter->tr->ctrl)
+		tracer_enabled = 0;
+}
+
+static void event_trace_close(struct trace_iterator *iter)
+{
+	if (iter->tr->ctrl)
+		tracer_enabled = 1;
+}
+
+static struct tracer event_trace __read_mostly =
+{
+	.name = "events",
+	.init = event_trace_init,
+	.reset = event_trace_reset,
+	.open = event_trace_open,
+	.close = event_trace_close,
+	.ctrl_update = event_trace_ctrl_update,
+};
+
+__init static int init_event_trace(void)
+{
+	int ret;
+
+	ret = register_tracer(&event_trace);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+device_initcall(init_event_trace);
diff --git a/kernel/trace/trace_selftest.c b/kernel/trace/trace_selftest.c
index 0911b7e..bcdc35d 100644
--- a/kernel/trace/trace_selftest.c
+++ b/kernel/trace/trace_selftest.c
@@ -11,6 +11,13 @@ static inline int trace_valid_entry(struct trace_entry *entry)
 	case TRACE_WAKE:
 	case TRACE_STACK:
 	case TRACE_SPECIAL:
+	case TRACE_IRQ:
+	case TRACE_FAULT:
+	case TRACE_TIMER_SET:
+	case TRACE_TIMER_TRIG:
+	case TRACE_TIMESTAMP:
+	case TRACE_TASK_ACT:
+	case TRACE_TASK_DEACT:
 		return 1;
 	}
 	return 0;
