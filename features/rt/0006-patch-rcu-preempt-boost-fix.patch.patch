From 7a2cc56ad63e67ad079af42b998050fcd37a4ec6 Mon Sep 17 00:00:00 2001
From: Paul Gortmaker <paul.gortmaker@windriver.com>
Date: Fri, 5 Sep 2008 00:22:20 -0400
Subject: [PATCH] patch rcu-preempt-boost-fix.patch

---
 kernel/rcupreempt-boost.c |   39 ++++++++++++++++++++++++++++++++++++---
 1 files changed, 36 insertions(+), 3 deletions(-)

diff --git a/kernel/rcupreempt-boost.c b/kernel/rcupreempt-boost.c
index 7ba6e07..61cf2a5 100644
--- a/kernel/rcupreempt-boost.c
+++ b/kernel/rcupreempt-boost.c
@@ -221,6 +221,11 @@ RCU_BOOST_TRACE_FUNC_DECL(over_taken)
 # define rcu_trace_boost_over_taken(rbd) do { } while (0)
 #endif /* CONFIG_RCU_TRACE */
 
+static inline int rcu_is_boosted(struct task_struct *task)
+{
+	return !list_empty(&task->rcub_entry);
+}
+
 /*
  * Helper function to boost a task's prio.
  */
@@ -259,7 +264,7 @@ void __rcu_preempt_boost(void)
 	rcu_trace_boost_boost_called(RCU_BOOST_ME);
 
 	/* check to see if we are already boosted */
-	if (unlikely(curr->rcub_rbdp))
+	if (unlikely(rcu_is_boosted(curr)))
 		return;
 
 	/*
@@ -311,15 +316,42 @@ void __rcu_preempt_unboost(void)
 	rcu_trace_boost_unboost_called(RCU_BOOST_ME);
 
 	/* if not boosted, then ignore */
-	if (likely(!curr->rcub_rbdp))
+	if (likely(!rcu_is_boosted(curr)))
 		return;
 
+	/*
+	 * Need to be very careful with NMIs.
+	 * If we take the lock and an NMI comes in
+	 * and it may try to unboost us if curr->rcub_rbdp
+	 * is still set. So we zero it before grabbing the lock.
+	 * But this also means that we might be boosted again
+	 * so the boosting code needs to be aware of this.
+	 */
 	rbd = curr->rcub_rbdp;
+	curr->rcub_rbdp = NULL;
+
+	/*
+	 * Now an NMI might have came in after we grab
+	 * the below lock. This check makes sure that
+	 * the NMI doesn't try grabbing the lock
+	 * while we already have it.
+	 */
+	if (unlikely(!rbd))
+		return;
 
 	spin_lock_irqsave(&rbd->rbs_lock, flags);
+	/*
+	 * It is still possible that an NMI came in
+	 * between the "is_boosted" check and setting
+	 * the rcu_rbdp to NULL. This would mean that
+	 * the NMI already dequeued us.
+	 */
+	if (unlikely(!rcu_is_boosted(curr)))
+		goto out;
+
 	list_del_init(&curr->rcub_entry);
 
-	rcu_trace_boost_unboosted(curr->rcub_rbdp);
+	rcu_trace_boost_unboosted(rbd);
 
 	set_rcu_prio(curr, MAX_PRIO);
 
@@ -330,6 +362,7 @@ void __rcu_preempt_unboost(void)
 	curr->rcub_rbdp = NULL;
 
 	spin_unlock(&curr->pi_lock);
+ out:
 	spin_unlock_irqrestore(&rbd->rbs_lock, flags);
 }
 
-- 
1.6.0.90.g436ed

