From 17df4a32cf5753f809220174636118628873e1cf Mon Sep 17 00:00:00 2001
From: Steven Rostedt <srostedt@redhat.com>
Date: Fri, 5 Sep 2008 00:23:55 -0400
Subject: [PATCH] Subject: rtmutex: break out early on first run
 Lock stealing and non cmpxchg will always go into the slow path.
 This patch detects the fact that we didn't go through the work of
 blocking and will exit early.

Signed-off-by: Steven Rostedt <srostedt@redhat.com>
---
 kernel/rtmutex.c |   10 +++++++++-
 1 files changed, 9 insertions(+), 1 deletions(-)

diff --git a/kernel/rtmutex.c b/kernel/rtmutex.c
index f2fc4e7..d2f99cb 100644
--- a/kernel/rtmutex.c
+++ b/kernel/rtmutex.c
@@ -835,6 +835,7 @@ rt_spin_lock_slowlock(struct rt_mutex *lock)
 	struct rt_mutex_waiter waiter;
 	unsigned long saved_state, state, flags;
 	struct task_struct *orig_owner;
+	int missed = 0;
 
 	debug_rt_mutex_init_waiter(&waiter);
 	waiter.task = NULL;
@@ -861,8 +862,14 @@ rt_spin_lock_slowlock(struct rt_mutex *lock)
 		int saved_lock_depth = current->lock_depth;
 
 		/* Try to acquire the lock */
-		if (do_try_to_take_rt_mutex(lock, STEAL_LATERAL))
+		if (do_try_to_take_rt_mutex(lock, STEAL_LATERAL)) {
+			/* If we never blocked break out now */
+			if (!missed)
+				goto unlock;
 			break;
+		}
+		missed = 1;
+
 		/*
 		 * waiter.task is NULL the first time we come here and
 		 * when we have been woken up by the previous owner
@@ -921,6 +928,7 @@ rt_spin_lock_slowlock(struct rt_mutex *lock)
 	 */
 	fixup_rt_mutex_waiters(lock);
 
+ unlock:
 	spin_unlock_irqrestore(&lock->wait_lock, flags);
 
 	debug_rt_mutex_free_waiter(&waiter);
-- 
1.6.0.90.g436ed

