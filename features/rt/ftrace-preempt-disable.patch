From: Steven Rostedt <srostedt@redhat.com>
Subject: ftrace: create function ftrace_preempt_disable

Signed-off-by: Steven Rostedt <srostedt@redhat.com>
diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
index 6039714..a77301b 100644
--- a/include/linux/ftrace.h
+++ b/include/linux/ftrace.h
@@ -7,6 +7,7 @@
 #ifdef CONFIG_FTRACE
 
 #include <linux/linkage.h>
+#include <linux/sched.h>
 #include <linux/fs.h>
 
 extern int ftrace_enabled;
@@ -71,6 +72,44 @@ void ftrace_kill(void);
 void __ftrace_kill(void);
 void ftrace_kill_atomic(void);
 
+/**
+ * ftrace_preempt_disable - preempt disable used by function tracers
+ *
+ * The function tracer needs to be careful in disabling preemption.
+ * If it calls preempt_enable() inside the scheduler, we may cause
+ * a recursive inifinite loop.
+ *
+ * Returns flag to be used by ftrace_preempt_enable()
+ */
+static inline int ftrace_preempt_disable(void)
+{
+	int resched;
+
+	resched = need_resched();
+	barrier();
+	preempt_disable_notrace();
+
+	return resched;
+}
+
+/**
+ * ftrace_preempt_enable - preempt enable used by function tracers
+ * @resched: variable returned by corresponding ftrace_preempt_disable
+ *
+ * If NEED_RESCHED was set before we disabed preemption, since we
+ * have not preempted yet, we either have interrupts off, preemption
+ * off, or we are inside the scheduler.  The first two are OK,
+ * but if resched is set and we are in the scheduler, calling
+ * preempt_enable that reschedules will cause a recursion back into
+ * the scheduler that will crash the box.
+ */
+static inline void ftrace_preempt_enable(int resched)
+{
+	if (resched)
+		preempt_enable_no_resched_notrace();
+	else
+		preempt_enable_notrace();
+}
 
 #else /* !CONFIG_FTRACE */
 # define register_ftrace_function(ops) do { } while (0)
diff --git a/kernel/trace/ftrace.c b/kernel/trace/ftrace.c
index 8725c20..7b56b0f 100644
--- a/kernel/trace/ftrace.c
+++ b/kernel/trace/ftrace.c
@@ -370,8 +370,7 @@ ftrace_record_ip(unsigned long ip)
 	if (!ftrace_enabled || ftrace_disabled)
 		return;
 
-	resched = need_resched();
-	preempt_disable_notrace();
+	resched = ftrace_preempt_disable();
 
 	/*
 	 * We simply need to protect against recursion.
@@ -417,11 +416,7 @@ ftrace_record_ip(unsigned long ip)
  out:
 	per_cpu(ftrace_shutdown_disable_cpu, cpu)--;
 
-	/* prevent recursion with scheduler */
-	if (resched)
-		preempt_enable_no_resched_notrace();
-	else
-		preempt_enable_notrace();
+	ftrace_preempt_enable(resched);
 }
 
 #define FTRACE_ADDR ((long)(ftrace_caller))
diff --git a/kernel/trace/trace.c b/kernel/trace/trace.c
index 2033201..c14faaa 100644
--- a/kernel/trace/trace.c
+++ b/kernel/trace/trace.c
@@ -1223,8 +1223,7 @@ function_trace_call(unsigned long ip, unsigned long parent_ip)
 	if (skip_trace(ip))
 		return;
 
-	resched = need_resched();
-	preempt_disable_notrace();
+	resched = ftrace_preempt_disable();
 	cpu = raw_smp_processor_id();
 	data = tr->data[cpu];
 	disabled = atomic_inc_return(&data->disabled);
@@ -1236,17 +1235,7 @@ function_trace_call(unsigned long ip, unsigned long parent_ip)
 
 	atomic_dec(&data->disabled);
 
-	/*
-	 * To prevent recursion with schedule(), we look at the
-	 * resched flag before disabling preemption. If it is already
-	 * set, then we may be just calling schedule, and we don't
-	 * want to call schedule again. But if it was not set, then
-	 * it is fine to call schedule() if we need to.
-	 */
-	if (resched)
-		preempt_enable_no_resched_notrace();
-	else
-		preempt_enable_notrace();
+	ftrace_preempt_enable(resched);
 }
 
 static struct ftrace_ops trace_ops __read_mostly =
diff --git a/kernel/trace/trace_sched_wakeup.c b/kernel/trace/trace_sched_wakeup.c
index 227eabc..8bd2ee3 100644
--- a/kernel/trace/trace_sched_wakeup.c
+++ b/kernel/trace/trace_sched_wakeup.c
@@ -48,8 +48,7 @@ wakeup_tracer_call(unsigned long ip, unsigned long parent_ip)
 	if (likely(!wakeup_task) || !ftrace_enabled)
 		return;
 
-	resched = need_resched();
-	preempt_disable_notrace();
+	resched = ftrace_preempt_disable();
 
 	cpu = raw_smp_processor_id();
 	data = tr->data[cpu];
@@ -70,6 +69,7 @@ wakeup_tracer_call(unsigned long ip, unsigned long parent_ip)
 	if (task_cpu(wakeup_task) != cpu)
 		goto unlock;
 
+	/* interrupts are disabled, no worry about scheduling */
 	preempt_enable_no_resched_notrace();
 	trace_function(tr, data, ip, parent_ip, flags);
 	preempt_disable_notrace();
@@ -81,15 +81,7 @@ wakeup_tracer_call(unsigned long ip, unsigned long parent_ip)
  out:
 	atomic_dec(&data->disabled);
 
-	/*
-	 * To prevent recursion from the scheduler, if the
-	 * resched flag was set before we entered, then
-	 * don't reschedule.
-	 */
-	if (resched)
-		preempt_enable_no_resched_notrace();
-	else
-		preempt_enable_notrace();
+	ftrace_preempt_enable(resched);
 }
 
 static struct ftrace_ops trace_ops __read_mostly =
