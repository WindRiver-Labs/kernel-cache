From d02e5866cddf4ed591730642d82cd77e837e59e8 Mon Sep 17 00:00:00 2001
From: Michal Schmidt <mschmidt@redhat.com>
Date: Tue, 30 Sep 2008 17:02:26 -0400
Subject: [PATCH] Michal Schmidt's fix for load average calculation

Subject: Re: [PATCH] idle load == # of CPUs

This is an attempt to fix https://bugzilla.redhat.com/show_bug.cgi?id=253099

The bug is caused by the fact that the local timer interrupts happen usually
at the same time on all CPUs. All CPUs then raise their timer softirqs. When
calc_load() runs, it sees not only itself but all the other softirq-timer
per-cpu threads running too. And softirq-sched too, which is woken up from
scheduler_tick().

In a BZ comment I speculated about three possible solutions:
(a) Somehow make sure the timer interrupts are synchronized between CPUs, but with a per-CPU offset, so that they don't fire at the same time.
(b) Make the timer softirq threads (and softirq-sched?) special and not take them into loadavg calculation.
(c) Don't calculate loadavg from the timer softirq. It should be possible to run calc_load() periodically from a hrtimer.

This patch implements (b).
Comments welcome.

Michal
---
 fs/proc/proc_misc.c |   11 ++++++++---
 kernel/timer.c      |   49 ++++++++++++++++++++++++++++++++++++++++++++++++-
 2 files changed, 56 insertions(+), 4 deletions(-)

diff --git a/fs/proc/proc_misc.c b/fs/proc/proc_misc.c
index 5d3778f..3d53b3b 100644
--- a/fs/proc/proc_misc.c
+++ b/fs/proc/proc_misc.c
@@ -112,10 +112,15 @@ static int loadavg_rt_read_proc(char *page, char **start, off_t off,
 	extern unsigned long rt_nr_running(void);
 	int a, b, c;
 	int len;
+	unsigned long seq;
+
+	do {
+		seq = read_seqbegin(&xtime_lock);
+		a = avenrun_rt[0] + (FIXED_1/200);
+		b = avenrun_rt[1] + (FIXED_1/200);
+		c = avenrun_rt[2] + (FIXED_1/200);
+	} while (read_seqretry(&xtime_lock, seq));
 
-	a = avenrun_rt[0] + (FIXED_1/200);
-	b = avenrun_rt[1] + (FIXED_1/200);
-	c = avenrun_rt[2] + (FIXED_1/200);
 	len = sprintf(page,"%d.%02d %d.%02d %d.%02d %ld/%d %d\n",
 		LOAD_INT(a), LOAD_FRAC(a),
 		LOAD_INT(b), LOAD_FRAC(b),
diff --git a/kernel/timer.c b/kernel/timer.c
index 24079c1..cccef47 100644
--- a/kernel/timer.c
+++ b/kernel/timer.c
@@ -38,6 +38,7 @@
 #include <linux/delay.h>
 #include <linux/tick.h>
 #include <linux/kallsyms.h>
+#include <linux/kthread.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
@@ -1055,7 +1056,7 @@ void update_process_times(int user_tick)
 static unsigned long count_active_tasks(void)
 {
 	/*
-	 * On PREEMPT_RT, we are running in the timer softirq thread,
+	 * On PREEMPT_RT, we are running in the loadavg thread,
 	 * so consider 1 less running tasks:
 	 */
 #ifdef CONFIG_PREEMPT_RT
@@ -1125,6 +1126,50 @@ static inline void calc_load(unsigned long ticks)
 	}
 }
 
+#ifdef CONFIG_PREEMPT_RT
+static int loadavg_calculator(void *data)
+{
+	unsigned long now, last;
+
+	last = jiffies;
+	while (!kthread_should_stop()) {
+		struct timespec delay = {
+			.tv_sec = LOAD_FREQ / HZ,
+			.tv_nsec = 0
+		};
+
+		hrtimer_nanosleep(&delay, NULL, HRTIMER_MODE_REL,
+			CLOCK_MONOTONIC);
+		now = jiffies;
+		write_seqlock_irq(&xtime_lock);
+		calc_load(now - last);
+		write_sequnlock_irq(&xtime_lock);
+		last = now;
+	}
+
+	return 0;
+}
+
+static int __init start_loadavg_calculator(void)
+{
+	struct task_struct *p;
+	struct sched_param param = { .sched_priority = MAX_USER_RT_PRIO/2 };
+
+	p = kthread_create(loadavg_calculator, NULL, "loadavg");
+	if (IS_ERR(p)) {
+		printk(KERN_ERR "Could not create the loadavg thread.\n");
+		return 1;
+	}
+
+	sched_setscheduler(p, SCHED_FIFO, &param);
+	wake_up_process(p);
+
+	return 0;
+}
+
+late_initcall(start_loadavg_calculator);
+#endif
+
 /*
  * Called by the local, per-CPU timer interrupt on SMP.
  */
@@ -1154,7 +1199,9 @@ static inline void update_times(void)
 	ticks = jiffies - last_tick;
 	if (ticks) {
 		last_tick += ticks;
+#ifndef CONFIG_PREEMPT_RT
 		calc_load(ticks);
+#endif
 	}
 	write_sequnlock_irqrestore(&xtime_lock, flags);
 }
-- 
1.6.0.90.g436ed

