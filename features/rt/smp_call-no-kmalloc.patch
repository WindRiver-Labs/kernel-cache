From: Steven Rostedt <srostedt@redhat.com>
Date: Tue, 27 Jan 2009 01:43:56 -0500
Subject: [PATCH] preempt-rt: back port change for smp_call_function_mask

The smp_call_function_mask calls kmalloc, but we can not do this
in PREEMPT_RT because kmalloc can schedule, and this is a call
with preempt_disabled.  The slow path unfortunately has a bug
where it can wait for all CPUS to go through a quiescent state.
But there are cases that call this and the IPI handlers wait
for the caller. This will deadlock.

Luckily, mainline has rewritten this code and does it a bit
more simpler. This patch takes bits and pieces of that code
to make it play nice with PREEMPT_RT.

Signed-off-by: Steven Rostedt <srostedt@redhat.com>
(PG: plucked from RT git 680f9b74705a6b57cdf768ccb7c2ce0d51ac7d33)
---
 kernel/smp.c |   66 +++++++++++++--------------------------------------------
 1 files changed, 15 insertions(+), 51 deletions(-)

diff --git a/kernel/smp.c b/kernel/smp.c
index 64295d0..0f32131 100644
--- a/kernel/smp.c
+++ b/kernel/smp.c
@@ -266,42 +266,6 @@ void __smp_call_function_single(int cpu, struct call_single_data *data)
 	generic_exec_single(cpu, data);
 }
 
-/* Dummy function */
-static void quiesce_dummy(void *unused)
-{
-}
-
-/*
- * Ensure stack based data used in call function mask is safe to free.
- *
- * This is needed by smp_call_function_mask when using on-stack data, because
- * a single call function queue is shared by all CPUs, and any CPU may pick up
- * the data item on the queue at any time before it is deleted. So we need to
- * ensure that all CPUs have transitioned through a quiescent state after
- * this call.
- *
- * This is a very slow function, implemented by sending synchronous IPIs to
- * all possible CPUs. For this reason, we have to alloc data rather than use
- * stack based data even in the case of synchronous calls. The stack based
- * data is then just used for deadlock/oom fallback which will be very rare.
- *
- * If a faster scheme can be made, we could go back to preferring stack based
- * data -- the data allocation/free is non-zero cost.
- */
-static void smp_call_function_mask_quiesce_stack(cpumask_t mask)
-{
-	struct call_single_data data;
-	int cpu;
-
-	data.func = quiesce_dummy;
-	data.info = NULL;
-
-	for_each_cpu_mask(cpu, mask) {
-		data.flags = CSD_FLAG_WAIT;
-		generic_exec_single(cpu, &data);
-	}
-}
-
 /**
  * smp_call_function_mask(): Run a function on a set of other CPUs.
  * @mask: The set of cpus to run on.
@@ -322,12 +286,10 @@ static void smp_call_function_mask_quiesce_stack(cpumask_t mask)
 int smp_call_function_mask(cpumask_t mask, void (*func)(void *), void *info,
 			   int wait)
 {
-	struct call_function_data d;
 	struct call_function_data *data = NULL;
 	cpumask_t allbutself;
 	unsigned long flags;
 	int cpu, num_cpus;
-	int slowpath = 0;
 
 	/* Can deadlock when called with interrupts disabled */
 	WARN_ON(irqs_disabled());
@@ -351,19 +313,24 @@ int smp_call_function_mask(cpumask_t mask, void (*func)(void *), void *info,
 
 #ifndef CONFIG_PREEMPT_RT
 	data = kmalloc(sizeof(*data), GFP_ATOMIC);
-	if (data) {
-		data->csd.flags = CSD_FLAG_ALLOC;
-		if (wait)
-			data->csd.flags |= CSD_FLAG_WAIT;
-	} else
+	if (unlikely(!data))
 #endif
 	{
-		data = &d;
-		data->csd.flags = CSD_FLAG_WAIT;
-		wait = 1;
-		slowpath = 1;
+		/* Slow path. */
+		for_each_online_cpu(cpu) {
+			if (cpu == smp_processor_id())
+				continue;
+			if (cpumask_test_cpu(cpu, &mask))
+				smp_call_function_single(cpu, func, info, wait);
+		}
+		return 0;
 	}
 
+	data->csd.flags = CSD_FLAG_ALLOC;
+
+	if (wait)
+		data->csd.flags |= CSD_FLAG_WAIT;
+
 	spin_lock_init(&data->lock);
 	data->csd.func = func;
 	data->csd.info = info;
@@ -383,11 +350,8 @@ int smp_call_function_mask(cpumask_t mask, void (*func)(void *), void *info,
 	arch_send_call_function_ipi(mask);
 
 	/* optionally wait for the CPUs to complete */
-	if (wait) {
+	if (wait)
 		csd_flag_wait(&data->csd);
-		if (unlikely(slowpath))
-			smp_call_function_mask_quiesce_stack(mask);
-	}
 
 	return 0;
 }
