From: Steven Rostedt <srostedt@redhat.com>
Subject: rt: rwlock fix non nested unlocking

The RW PI locks are sensitive to locking and unlocking order. This is
a bug, since it is not always the case that unlocking will be in the
reverse order of locking.

This patch lets there be "holes" in the rwlock array for a task. The holes
are caused when a lock is released out of order.

Signed-off-by: Steven Rostedt <srostedt@redhat.com>
---
 kernel/rtmutex.c |   56 +++++++++++++++++++++++++++++++++++++++++++++++--------
 1 file changed, 48 insertions(+), 8 deletions(-)

Index: linux-2.6.26.6-rt10/kernel/rtmutex.c
===================================================================
--- linux-2.6.26.6-rt10.orig/kernel/rtmutex.c	2008-10-12 00:14:16.000000000 -0400
+++ linux-2.6.26.6-rt10/kernel/rtmutex.c	2008-10-12 00:14:17.000000000 -0400
@@ -1738,10 +1738,21 @@ rt_read_slowunlock(struct rw_mutex *rwm,
 			spin_lock(&current->pi_lock);
 			current->owned_read_locks[i].count--;
 			if (!current->owned_read_locks[i].count) {
-				current->reader_lock_count--;
-				WARN_ON_ONCE(i != current->reader_lock_count);
-				atomic_dec(&rwm->owners);
+				if (likely(i == current->reader_lock_count - 1)) {
+					int count = --current->reader_lock_count;
+
+					/* check for holes */
+					for (count--; count >= 0; count--) {
+						rls = &current->owned_read_locks[count];
+						if (likely(rls->lock))
+							break;
+						current->reader_lock_count--;
+					}
+					WARN_ON(current->reader_lock_count < 0);
+				} /* else ... Bah! bad locking practice */
 				rls = &current->owned_read_locks[i];
+
+				atomic_dec(&rwm->owners);
 				WARN_ON(!rls->list.prev || list_empty(&rls->list));
 				list_del_init(&rls->list);
 				rls->lock = NULL;
@@ -1877,20 +1888,49 @@ rt_read_fastunlock(struct rw_mutex *rwm,
 		int owners;
 
 		spin_lock_irqsave(&current->pi_lock, flags);
-		reader_count = --current->reader_lock_count;
-		spin_unlock_irqrestore(&current->pi_lock, flags);
-
-		rt_mutex_deadlock_account_unlock(current);
+		reader_count = current->reader_lock_count - 1;
 		if (unlikely(reader_count < 0)) {
 			    reader_count = 0;
 			    WARN_ON_ONCE(1);
 		}
+		rls = &current->owned_read_locks[reader_count];
+		if (unlikely(rls->lock != rwm)) {
+			int i;
+
+			/* Bah! Bad locking practice! */
+			for (i = reader_count - 1; i >= 0; i--) {
+				rls = &current->owned_read_locks[i];
+				if (rls->lock == rwm)
+					break;
+			}
+			if (unlikely(i < 0)) {
+				/* We don't own the lock?? */
+				WARN_ON_ONCE(1);
+				spin_unlock_irqrestore(&current->pi_lock, flags);
+				return;
+			}
+			/* We will have a hole */
+		} else {
+			struct reader_lock_struct *rs;
+			int count = --current->reader_lock_count;
+
+			/* check for holes */
+			for (count--; count >= 0; count--) {
+				rs = &current->owned_read_locks[count];
+				if (likely(rs->lock))
+					break;
+				current->reader_lock_count--;
+			}
+			WARN_ON(current->reader_lock_count < 0);
+		}
+		spin_unlock_irqrestore(&current->pi_lock, flags);
+
+		rt_mutex_deadlock_account_unlock(current);
 		owners = atomic_dec_return(&rwm->owners);
 		if (unlikely(owners < 0)) {
 			atomic_set(&rwm->owners, 0);
 			WARN_ON_ONCE(1);
 		}
-		rls = &current->owned_read_locks[reader_count];
 		WARN_ON_ONCE(rls->lock != rwm);
 		WARN_ON(rls->list.prev && !list_empty(&rls->list));
 		WARN_ON(rls->count != 1);
