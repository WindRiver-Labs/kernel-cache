From: Carsten Emde <Carsten.Emde@osadl.org>
Subject: ftrace:  display real preempt_count in ftracer
Date: Wed, 28 Jan 2009 14:39:51 +0100

Ftrace determined the preempt_count after preemption was disabled
instead of the original preemption count.

Signed-off-by: Carsten Emde <C.Emde@osadl.org>
Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

---
 kernel/trace/trace.c              |   52 +++++++++++++++++++-----------------
 kernel/trace/trace.h              |    3 +-
 kernel/trace/trace_hist.c         |    2 +-
 kernel/trace/trace_irqsoff.c      |   13 +++++---
 kernel/trace/trace_sched_wakeup.c |    9 ++++--
 5 files changed, 44 insertions(+), 35 deletions(-)

diff --git a/features/rt/kernel/trace/trace.c b/kernel/trace/trace.c
index 92716ce..5e745a3 100644
--- a/kernel/trace/trace.c
+++ b/kernel/trace/trace.c
@@ -821,12 +821,10 @@ tracing_get_trace_entry(struct trace_array *tr, struct trace_array_cpu *data)
 }
 
 static inline void
-tracing_generic_entry_update(struct trace_entry *entry, unsigned long flags)
+tracing_generic_entry_update(struct trace_entry *entry, unsigned long flags,
+			     unsigned long pc)
 {
 	struct task_struct *tsk = current;
-	unsigned long pc;
-
-	pc = preempt_count();
 
 	entry->preempt_count	= pc & 0xff;
 	entry->pid		= (tsk) ? tsk->pid : 0;
@@ -839,7 +837,8 @@ tracing_generic_entry_update(struct trace_entry *entry, unsigned long flags)
 
 void
 trace_function(struct trace_array *tr, struct trace_array_cpu *data,
-	       unsigned long ip, unsigned long parent_ip, unsigned long flags)
+	       unsigned long ip, unsigned long parent_ip, unsigned long flags,
+	       unsigned long pc)
 {
 	struct trace_entry *entry;
 	unsigned long irq_flags;
@@ -847,7 +846,7 @@ trace_function(struct trace_array *tr, struct trace_array_cpu *data,
 	raw_local_irq_save(irq_flags);
 	__raw_spin_lock(&data->lock);
 	entry			= tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, pc);
 	entry->type		= TRACE_FN;
 	entry->fn.ip		= ip;
 	entry->fn.parent_ip	= parent_ip;
@@ -860,7 +859,8 @@ ftrace(struct trace_array *tr, struct trace_array_cpu *data,
        unsigned long ip, unsigned long parent_ip, unsigned long flags)
 {
 	if (likely(!atomic_read(&data->disabled)))
-		trace_function(tr, data, ip, parent_ip, flags);
+		trace_function(tr, data, ip, parent_ip, flags,
+			       preempt_count());
 }
 
 #ifdef CONFIG_MMIOTRACE
@@ -874,7 +874,7 @@ void __trace_mmiotrace_rw(struct trace_array *tr, struct trace_array_cpu *data,
 	__raw_spin_lock(&data->lock);
 
 	entry			= tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, 0);
+	tracing_generic_entry_update(entry, 0, preempt_count());
 	entry->type		= TRACE_MMIO_RW;
 	entry->mmiorw		= *rw;
 
@@ -894,7 +894,7 @@ void __trace_mmiotrace_map(struct trace_array *tr, struct trace_array_cpu *data,
 	__raw_spin_lock(&data->lock);
 
 	entry			= tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, 0);
+	tracing_generic_entry_update(entry, 0, preempt_count());
 	entry->type		= TRACE_MMIO_MAP;
 	entry->mmiomap		= *map;
 
@@ -917,7 +917,7 @@ void __trace_stack(struct trace_array *tr,
 		return;
 
 	entry			= tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type		= TRACE_STACK;
 
 	memset(&entry->stack, 0, sizeof(entry->stack));
@@ -942,7 +942,7 @@ __trace_special(void *__tr, void *__data,
 	raw_local_irq_save(irq_flags);
 	__raw_spin_lock(&data->lock);
 	entry			= tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, 0);
+	tracing_generic_entry_update(entry, 0, preempt_count());
 	entry->type		= TRACE_SPECIAL;
 	entry->special.arg1	= arg1;
 	entry->special.arg2	= arg2;
@@ -967,7 +967,7 @@ tracing_sched_switch_trace(struct trace_array *tr,
 	raw_local_irq_save(irq_flags);
 	__raw_spin_lock(&data->lock);
 	entry			= tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type		= TRACE_CTX;
 	entry->ctx.prev_pid	= prev->pid;
 	entry->ctx.prev_prio	= prev->prio;
@@ -993,7 +993,7 @@ tracing_sched_wakeup_trace(struct trace_array *tr,
 	raw_local_irq_save(irq_flags);
 	__raw_spin_lock(&data->lock);
 	entry			= tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type		= TRACE_WAKE;
 	entry->ctx.prev_pid	= curr->pid;
 	entry->ctx.prev_prio	= curr->prio;
@@ -1042,7 +1042,7 @@ void tracing_event_irq(struct trace_array *tr,
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type			= TRACE_IRQ;
 	entry->irq.ip		= ip;
 	entry->irq.irq		= irq;
@@ -1061,7 +1061,7 @@ void tracing_event_fault(struct trace_array *tr,
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type			= TRACE_FAULT;
 	entry->fault.ip		= ip;
 	entry->fault.ret_ip	= retip;
@@ -1078,7 +1078,7 @@ void tracing_event_timer_set(struct trace_array *tr,
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type			= TRACE_TIMER_SET;
 	entry->timer.ip		= ip;
 	entry->timer.expire	= *expires;
@@ -1094,7 +1094,7 @@ void tracing_event_program_event(struct trace_array *tr,
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type		= TRACE_PROGRAM_EVENT;
 	entry->program.ip	= ip;
 	entry->program.expire	= *expires;
@@ -1111,7 +1111,7 @@ void tracing_event_resched_task(struct trace_array *tr,
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type		= TRACE_RESCHED_TASK;
 	entry->task.ip	= ip;
 	entry->task.prio	= p->prio;
@@ -1128,7 +1128,7 @@ void tracing_event_timer_triggered(struct trace_array *tr,
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type			= TRACE_TIMER_TRIG;
 	entry->timer.ip		= ip;
 	entry->timer.expire	= *expired;
@@ -1144,7 +1144,7 @@ void tracing_event_timestamp(struct trace_array *tr,
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type			= TRACE_TIMESTAMP;
 	entry->timestamp.ip		= ip;
 	entry->timestamp.now		= *now;
@@ -1160,7 +1160,7 @@ void tracing_event_task_activate(struct trace_array *tr,
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type			= TRACE_TASK_ACT;
 	entry->task.ip		= ip;
 	entry->task.pid		= p->pid;
@@ -1178,7 +1178,7 @@ void tracing_event_task_deactivate(struct trace_array *tr,
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type			= TRACE_TASK_DEACT;
 	entry->task.ip		= ip;
 	entry->task.pid		= p->pid;
@@ -1198,7 +1198,7 @@ void tracing_event_syscall(struct trace_array *tr,
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type			= TRACE_SYSCALL;
 	entry->syscall.ip		= ip;
 	entry->syscall.nr		= nr;
@@ -1216,7 +1216,7 @@ void tracing_event_sysret(struct trace_array *tr,
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type			= TRACE_SYSRET;
 	entry->sysret.ip		= ip;
 	entry->sysret.ret		= ret;
@@ -1231,6 +1231,7 @@ function_trace_call(unsigned long ip, unsigned long parent_ip)
 	unsigned long flags;
 	long disabled;
 	int cpu, resched;
+	unsigned long pc;
 
 	if (unlikely(!ftrace_function_enabled))
 		return;
@@ -1238,6 +1239,7 @@ function_trace_call(unsigned long ip, unsigned long parent_ip)
 	if (skip_trace(ip))
 		return;
 
+	pc = preempt_count();
 	resched = ftrace_preempt_disable();
 	cpu = raw_smp_processor_id();
 	data = tr->data[cpu];
@@ -1245,7 +1247,7 @@ function_trace_call(unsigned long ip, unsigned long parent_ip)
 
 	if (likely(disabled == 1)) {
 		local_save_flags(flags);
-		trace_function(tr, data, ip, parent_ip, flags);
+		trace_function(tr, data, ip, parent_ip, flags, pc);
 	}
 
 	atomic_dec(&data->disabled);
diff --git a/features/rt/kernel/trace/trace.h b/kernel/trace/trace.h
index 013c43c..535d70f 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -299,7 +299,8 @@ void trace_function(struct trace_array *tr,
 		    struct trace_array_cpu *data,
 		    unsigned long ip,
 		    unsigned long parent_ip,
-		    unsigned long flags);
+		    unsigned long flags,
+		    unsigned long pc);
 void tracing_event_irq(struct trace_array *tr,
 		       struct trace_array_cpu *data,
 		       unsigned long flags,
diff --git a/features/rt/kernel/trace/trace_hist.c b/kernel/trace/trace_hist.c
index f3f49eb..8be4719 100644
--- a/kernel/trace/trace_hist.c
+++ b/kernel/trace/trace_hist.c
@@ -354,7 +354,7 @@ notrace void tracing_hist_preempt_stop(int irqs_on)
 	cpu = raw_smp_processor_id();
 
 #ifdef CONFIG_INTERRUPT_OFF_HIST
-	if (irqs_on  &&
+	if (irqs_on &&
 	    per_cpu(hist_irqsoff_tracing, cpu)) {
 		stop = ftrace_now(cpu);
 		stop_set++;
diff --git a/features/rt/kernel/trace/trace_irqsoff.c b/kernel/trace/trace_irqsoff.c
index 1638d1f..cb78bef 100644
--- a/kernel/trace/trace_irqsoff.c
+++ b/kernel/trace/trace_irqsoff.c
@@ -99,7 +99,8 @@ irqsoff_tracer_call(unsigned long ip, unsigned long parent_ip)
 	disabled = atomic_inc_return(&data->disabled);
 
 	if (likely(disabled == 1))
-		trace_function(tr, data, ip, parent_ip, flags);
+		trace_function(tr, data, ip, parent_ip, flags,
+			       preempt_count());
 
 	atomic_dec(&data->disabled);
 }
@@ -154,7 +155,8 @@ check_critical_timing(struct trace_array *tr,
 	if (!report_latency(delta))
 		goto out_unlock;
 
-	trace_function(tr, data, CALLER_ADDR0, parent_ip, flags);
+	trace_function(tr, data, CALLER_ADDR0, parent_ip, flags,
+		       preempt_count());
 
 	latency = nsecs_to_usecs(delta);
 
@@ -178,7 +180,8 @@ out:
 	data->critical_sequence = max_sequence;
 	data->preempt_timestamp = ftrace_now(cpu);
 	tracing_reset(data);
-	trace_function(tr, data, CALLER_ADDR0, parent_ip, flags);
+	trace_function(tr, data, CALLER_ADDR0, parent_ip, flags,
+		       preempt_count());
 }
 
 static inline void
@@ -211,7 +214,7 @@ start_critical_timing(unsigned long ip, unsigned long parent_ip)
 
 	local_save_flags(flags);
 
-	trace_function(tr, data, ip, parent_ip, flags);
+	trace_function(tr, data, ip, parent_ip, flags, preempt_count());
 
 	per_cpu(tracing_cpu, cpu) = 1;
 
@@ -245,7 +248,7 @@ stop_critical_timing(unsigned long ip, unsigned long parent_ip)
 	atomic_inc(&data->disabled);
 
 	local_save_flags(flags);
-	trace_function(tr, data, ip, parent_ip, flags);
+	trace_function(tr, data, ip, parent_ip, flags, preempt_count());
 	check_critical_timing(tr, data, parent_ip ? : ip, cpu);
 	data->critical_start = 0;
 	atomic_dec(&data->disabled);
diff --git a/features/rt/kernel/trace/trace_sched_wakeup.c b/kernel/trace/trace_sched_wakeup.c
index 8bd2ee3..0c62757 100644
--- a/kernel/trace/trace_sched_wakeup.c
+++ b/kernel/trace/trace_sched_wakeup.c
@@ -44,10 +44,12 @@ wakeup_tracer_call(unsigned long ip, unsigned long parent_ip)
 	long disabled;
 	int resched;
 	int cpu;
+	unsigned long pc;
 
 	if (likely(!wakeup_task) || !ftrace_enabled)
 		return;
 
+	pc = preempt_count();
 	resched = ftrace_preempt_disable();
 
 	cpu = raw_smp_processor_id();
@@ -71,7 +73,7 @@ wakeup_tracer_call(unsigned long ip, unsigned long parent_ip)
 
 	/* interrupts are disabled, no worry about scheduling */
 	preempt_enable_no_resched_notrace();
-	trace_function(tr, data, ip, parent_ip, flags);
+	trace_function(tr, data, ip, parent_ip, flags, pc);
 	preempt_disable_notrace();
 
  unlock:
@@ -149,7 +151,8 @@ wakeup_sched_switch(void *private, void *rq, struct task_struct *prev,
 	if (unlikely(!tracer_enabled || next != wakeup_task))
 		goto out_unlock;
 
-	trace_function(tr, data, CALLER_ADDR1, CALLER_ADDR2, flags);
+	trace_function(tr, data, CALLER_ADDR1, CALLER_ADDR2, flags,
+		       preempt_count());
 
 	/*
 	 * usecs conversion is slow so we try to delay the conversion
@@ -270,7 +273,7 @@ wakeup_check_start(struct trace_array *tr, struct task_struct *p,
 
 	tr->data[wakeup_cpu]->preempt_timestamp = ftrace_now(cpu);
 	trace_function(tr, tr->data[wakeup_cpu],
-		       CALLER_ADDR1, CALLER_ADDR2, flags);
+		       CALLER_ADDR1, CALLER_ADDR2, flags, preempt_count());
 
 out_locked:
 	__raw_spin_unlock(&wakeup_lock);
