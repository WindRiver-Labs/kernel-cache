From: Steven Rostedt <srostedt@redhat.com>
Subject: rtmutex: break out early on first run

Lock stealing and non cmpxchg will always go into the slow path.
This patch detects the fact that we didn't go through the work of
blocking and will exit early.

Signed-off-by: Steven Rostedt <srostedt@redhat.com>
---
 kernel/rtmutex.c |   10 +++++++++-
 1 file changed, 9 insertions(+), 1 deletion(-)

Index: linux-2.6.26.3-rt6/kernel/rtmutex.c
===================================================================
--- linux-2.6.26.3-rt6.orig/kernel/rtmutex.c	2008-09-05 00:23:55.000000000 -0400
+++ linux-2.6.26.3-rt6/kernel/rtmutex.c	2008-09-05 00:23:55.000000000 -0400
@@ -835,6 +835,7 @@ rt_spin_lock_slowlock(struct rt_mutex *l
 	struct rt_mutex_waiter waiter;
 	unsigned long saved_state, state, flags;
 	struct task_struct *orig_owner;
+	int missed = 0;
 
 	debug_rt_mutex_init_waiter(&waiter);
 	waiter.task = NULL;
@@ -861,8 +862,14 @@ rt_spin_lock_slowlock(struct rt_mutex *l
 		int saved_lock_depth = current->lock_depth;
 
 		/* Try to acquire the lock */
-		if (do_try_to_take_rt_mutex(lock, STEAL_LATERAL))
+		if (do_try_to_take_rt_mutex(lock, STEAL_LATERAL)) {
+			/* If we never blocked break out now */
+			if (!missed)
+				goto unlock;
 			break;
+		}
+		missed = 1;
+
 		/*
 		 * waiter.task is NULL the first time we come here and
 		 * when we have been woken up by the previous owner
@@ -921,6 +928,7 @@ rt_spin_lock_slowlock(struct rt_mutex *l
 	 */
 	fixup_rt_mutex_waiters(lock);
 
+ unlock:
 	spin_unlock_irqrestore(&lock->wait_lock, flags);
 
 	debug_rt_mutex_free_waiter(&waiter);
