From 7fe1bb700452016635bbb4613aa04ecf58de9242 Mon Sep 17 00:00:00 2001
From: Andi Kleen <ak@suse.de>
Date: Tue, 30 Sep 2008 17:00:32 -0400
Subject: [PATCH] From ak@suse.de Tue Oct 23 16:24:16 2007
 Date: Tue, 23 Oct 2007 19:13:03 +0200
 To: linux-rt-users@vger.kernel.org
 Subject: [PATCH] Fix rt preempt slab NUMA freeing

When this_cpu changes in the free path node needs to change too.
Otherwise the slab can end up in the wrong node's list and this
eventually leads to WARN_ONs and of course worse NUMA performace.

This patch is likely not complete (the NUMA slab code is *very* hairy),
but seems to make the make -j128 test survive for at least two hours.

But at least it fixes one case that regularly triggered during
testing, resulting in slabs in the wrong node lists and triggering
WARN_ONs in slab_put/get_obj

I tried a complete audit of keeping this_cpu/node/slabp in sync when needed, but
it is very hairy code and I likely missed some cases. This so far
fixes only the simple free path; but it seems to be good enough
to not trigger easily anymore on a NUMA system with memory pressure.

Longer term the only good fix is probably to migrate to slub.
Or disable NUMA slab for PREEMPT_RT (its value has been disputed
in some benchmarks anyways)

Signed-off-by: Andi Kleen <ak@suse.de>
---
 mm/slab.c |    4 +++-
 1 files changed, 3 insertions(+), 1 deletions(-)

diff --git a/mm/slab.c b/mm/slab.c
index acdfd18..9502dac 100644
--- a/mm/slab.c
+++ b/mm/slab.c
@@ -1193,7 +1193,7 @@ cache_free_alien(struct kmem_cache *cachep, void *objp, int *this_cpu)
 	struct array_cache *alien = NULL;
 	int node;
 
-	node = numa_node_id();
+	node = cpu_to_node(*this_cpu);
 
 	/*
 	 * Make sure we are not freeing a object from another node to the array
@@ -4211,6 +4211,8 @@ static void cache_reap(struct work_struct *w)
 
 		work_done += reap_alien(searchp, l3, &this_cpu);
 
+		node = cpu_to_node(this_cpu);
+
 		work_done += drain_array(searchp, l3,
 			    cpu_cache_get(searchp, this_cpu), 0, node);
 
-- 
1.6.0.90.g436ed

