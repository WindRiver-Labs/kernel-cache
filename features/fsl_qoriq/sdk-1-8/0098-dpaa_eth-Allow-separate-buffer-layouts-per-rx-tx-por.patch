From 79605c7c01bd4fce628f4ee29027154e9fc0f4ae Mon Sep 17 00:00:00 2001
From: Ioana Radulescu <ruxandra.radulescu@freescale.com>
Date: Wed, 17 Apr 2013 19:42:35 +0000
Subject: [PATCH 098/987] dpaa_eth: Allow separate buffer layouts per rx/tx
 port

[Original patch taken from QorIQ-SDK-V1.6-SOURCE-20140619-yocto.iso]

Until now all ports expected the same (hardcoded) buffer layout,
where buffer layout means: private data size, presence of parse results,
hash results, timestamp. Buffer layout also influences the size of the
buffers for private ports.

Create a framework for configuring the buffer layout on a per rx/tx
port basis. There is no change in functionality at the moment (the same
values for buffer layout are kept).

Signed-off-by: Ioana Radulescu <ruxandra.radulescu@freescale.com>
Change-Id: I3792b095330575d199f4cbe071de718ae6d45ab1
Reviewed-on: http://git.am.freescale.net:8181/1467
Reviewed-by: Bucur Madalin-Cristian-B32716 <madalin.bucur@freescale.com>
Reviewed-by: Fleming Andrew-AFLEMING <AFLEMING@freescale.com>
Tested-by: Fleming Andrew-AFLEMING <AFLEMING@freescale.com>
---
 .../net/ethernet/freescale/dpa/dpaa_eth-common.h   |  24 ++--
 drivers/net/ethernet/freescale/dpa/dpaa_eth.c      | 157 ++++++++++++++-------
 drivers/net/ethernet/freescale/dpa/dpaa_eth.h      |  63 ++++++---
 drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c   |  26 ++--
 drivers/net/ethernet/freescale/dpa/offline_port.c  |  13 +-
 5 files changed, 189 insertions(+), 94 deletions(-)

diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth-common.h b/drivers/net/ethernet/freescale/dpa/dpaa_eth-common.h
index d5f3be1..ac34130 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth-common.h
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth-common.h
@@ -55,24 +55,28 @@ enum dpa_fq_type {
 #endif
 };
 
+struct dpa_buffer_layout_s {
+	uint16_t	priv_data_size;
+	bool		parse_results;
+	bool		time_stamp;
+	bool		hash_results;
+};
 
+#define DPA_TX_PRIV_DATA_SIZE	16
 #define DPA_PARSE_RESULTS_SIZE sizeof(t_FmPrsResult)
-#define DPA_HASH_RESULTS_SIZE 16
+#define DPA_TIME_STAMP_SIZE 8
+#define DPA_HASH_RESULTS_SIZE 8
 
-#define DPA_TX_PRIV_DATA_SIZE	16
 
-#define dpaa_eth_init_port(type, port, param, errq_id, defq_id, priv_size, \
-			   has_timer) \
+#define dpaa_eth_init_port(type, port, param, errq_id, defq_id, buf_layout) \
 { \
 	param.errq = errq_id; \
 	param.defq = defq_id; \
-	param.priv_data_size = priv_size; \
-	param.parse_results = true; \
-	param.hash_results = true; \
+	param.priv_data_size = buf_layout->priv_data_size; \
+	param.parse_results = buf_layout->parse_results; \
+	param.hash_results = buf_layout->hash_results; \
 	param.frag_enable = false; \
-	param.time_stamp = has_timer; \
-	param.data_align = 0;	\
-	param.manip_extra_space = 0; \
+	param.time_stamp = buf_layout->time_stamp; \
 	fm_set_##type##_port_params(port, &param); \
 }
 
diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth.c b/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
index 4b54ce4..bf8d1c9 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
@@ -108,7 +108,8 @@
 #define DPA_FQ_TD		0x200000
 
 /* S/G table requires at least 256 bytes */
-#define SGT_BUFFER_SIZE		DPA_BP_SIZE(256)
+#define sgt_buffer_size(priv) \
+	dpa_get_buffer_size(&priv->buf_layout[TX], 256)
 
 /* Maximum frame size on Tx for which skb copying is preferrable to
  * creating a S/G frame */
@@ -121,12 +122,19 @@
 #define DPA_MAX_FD_OFFSET	((1 << 9) - 1)
 
 /*
- * Maximum size of a buffer that is to be recycled back to the buffer pool.
+ * Extra size of a buffer (beyond the size of the buffers that are seeded into
+ * the global pool) for which recycling is allowed.
  * The value is arbitrary, but tries to reach a balance such that originating
  * frames may get recycled, while forwarded skbs that get reallocated on Tx
  * aren't allowed to grow unboundedly.
  */
-#define DPA_BP_MAX_BUF_SIZE	(DEFAULT_BUF_SIZE + 256)
+#define DPA_RECYCLE_EXTRA_SIZE	256
+
+/* For MAC-based interfaces, we compute the tx needed headroom from the
+ * associated Tx port's buffer layout settings.
+ * For MACless interfaces just use a default value.
+ */
+#define DPA_DEFAULT_TX_HEADROOM	64
 
 #define DPA_DESCRIPTION "FSL DPAA Ethernet driver"
 
@@ -383,7 +391,8 @@ static void dpaa_eth_seed_pool(struct dpa_bp *bp)
  * Add buffers/pages/skbuffs for Rx processing whenever bpool count falls below
  * REFILL_THRESHOLD.
  */
-static void dpaa_eth_refill_bpools(struct dpa_percpu_priv_s *percpu_priv)
+static void dpaa_eth_refill_bpools(struct dpa_priv_s *priv,
+				   struct dpa_percpu_priv_s *percpu_priv)
 {
 	int *countptr = percpu_priv->dpa_bp_count;
 	int count = *countptr;
@@ -404,9 +413,12 @@ static void dpaa_eth_refill_bpools(struct dpa_percpu_priv_s *percpu_priv)
 
 	/* Add skbs to the percpu skb list, reuse var count */
 	count = percpu_priv->skb_count;
-	if (unlikely(count < DEFAULT_SKB_COUNT / 4))
-		dpa_list_add_skbs(percpu_priv,
-				  DEFAULT_SKB_COUNT - count);
+	if (unlikely(count < DEFAULT_SKB_COUNT / 4)) {
+		int skb_size = priv->tx_headroom + dpa_get_rx_extra_headroom() +
+				DPA_COPIED_HEADERS_SIZE;
+		dpa_list_add_skbs(percpu_priv, DEFAULT_SKB_COUNT - count,
+				  skb_size);
+	}
 #endif
 }
 
@@ -645,7 +657,7 @@ _dpa_fq_alloc(struct list_head *list, struct dpa_fq *dpa_fq)
 			initfq.we_mask |= QM_INITFQ_WE_OAC;
 			initfq.fqd.oac_init.oac = QM_OAC_CG;
 			initfq.fqd.oac_init.oal = min(sizeof(struct sk_buff) +
-				DPA_BP_HEAD, (size_t)FSL_QMAN_MAX_OAL);
+				priv->tx_headroom, (size_t)FSL_QMAN_MAX_OAL);
 		}
 
 		/*
@@ -926,7 +938,7 @@ struct sk_buff *_dpa_cleanup_tx_fd(const struct dpa_priv_s *priv,
 		void *vaddr = phys_to_virt(addr);
 
 		/* Unmap first buffer (contains S/G table) */
-		dma_unmap_single(bp->dev, addr, SGT_BUFFER_SIZE,
+		dma_unmap_single(bp->dev, addr, sgt_buffer_size(priv),
 				 DMA_TO_DEVICE);
 
 		/* Unmap data buffer */
@@ -1467,14 +1479,20 @@ static struct dpa_bp *dpa_size2pool(struct dpa_priv_s *priv, size_t size)
 	int i;
 
 	for (i = 0; i < priv->bp_count; i++)
-		if (DPA_BP_SIZE(size) <= priv->dpa_bp[i].size)
+		if ((size + priv->tx_headroom) <= priv->dpa_bp[i].size)
 			return dpa_bpid2pool(priv->dpa_bp[i].bpid);
 	return ERR_PTR(-ENODEV);
 }
 
-static inline uint32_t dpa_bp_size(struct fm_port *rx_port)
+static void dpa_set_buffer_layout(struct dpa_priv_s *priv, struct fm_port *port,
+				  struct dpa_buffer_layout_s *layout, int type)
 {
-	return DEFAULT_BUF_SIZE;
+	layout->priv_data_size = (type == RX ?
+			DPA_RX_PRIV_DATA_SIZE : DPA_TX_PRIV_DATA_SIZE);
+	layout->parse_results = true;
+	layout->hash_results = true;
+	if (priv && priv->tsu && priv->tsu->valid)
+		layout->time_stamp = true;
 }
 
 /**
@@ -1614,7 +1632,7 @@ static int __hot dpa_shared_tx(struct sk_buff *skb, struct net_device *net_dev)
 	fd.length20 = skb_headlen(skb);
 	fd.addr_hi = bmb.hi;
 	fd.addr_lo = bmb.lo;
-	fd.offset = DPA_BP_HEAD;
+	fd.offset = priv->tx_headroom;
 
 	/*
 	 * The virtual address of the buffer pool is expected to be NULL
@@ -1676,7 +1694,7 @@ static int skb_to_sg_fd(struct dpa_priv_s *priv,
 	int err;
 
 	/* Allocate the first buffer in the FD (used for storing S/G table) */
-	vaddr = kmalloc(SGT_BUFFER_SIZE, GFP_ATOMIC);
+	vaddr = kmalloc(sgt_buffer_size(priv), GFP_ATOMIC);
 	if (unlikely(vaddr == NULL)) {
 		if (netif_msg_tx_err(priv) && net_ratelimit())
 			netdev_err(net_dev, "Memory allocation failed\n");
@@ -1688,7 +1706,7 @@ static int skb_to_sg_fd(struct dpa_priv_s *priv,
 
 	/* Fill in FD */
 	fd->format = qm_fd_sg;
-	fd->offset = DPA_BP_HEAD;
+	fd->offset = priv->tx_headroom;
 	fd->length20 = skb->len;
 
 	/* Enable hardware checksum computation */
@@ -1702,7 +1720,7 @@ static int skb_to_sg_fd(struct dpa_priv_s *priv,
 	}
 
 	/* Map the buffer and store its address in the FD */
-	paddr = dma_map_single(dpa_bp->dev, vaddr, SGT_BUFFER_SIZE,
+	paddr = dma_map_single(dpa_bp->dev, vaddr, sgt_buffer_size(priv),
 			       DMA_TO_DEVICE);
 	if (unlikely(dma_mapping_error(dpa_bp->dev, paddr))) {
 		if (netif_msg_tx_err(priv) && net_ratelimit())
@@ -1724,10 +1742,10 @@ static int skb_to_sg_fd(struct dpa_priv_s *priv,
 	 * Put the same offset in the data buffer as in the SGT (first) buffer.
 	 * This is the format for S/G frames generated by FMan; the manual is
 	 * not clear if same is required of Tx S/G frames, but since we know
-	 * for sure we have at least DPA_BP_HEAD bytes of skb headroom, lets not
-	 * take any chances.
+	 * for sure we have at least tx_headroom bytes of skb headroom,
+	 * lets not take any chances.
 	 */
-	sg_entry->offset = DPA_BP_HEAD;
+	sg_entry->offset = priv->tx_headroom;
 
 	paddr = dma_map_single(dpa_bp->dev, skb->data - sg_entry->offset,
 			       dpa_bp->size, DMA_TO_DEVICE);
@@ -1756,11 +1774,11 @@ static int skb_to_contig_fd(struct dpa_priv_s *priv,
 	int err;
 
 	/*
-	 * We are guaranteed that we have at least DPA_BP_HEAD of headroom.
+	 * We are guaranteed that we have at least tx_headroom bytes.
 	 * Buffers we allocated are padded to improve cache usage. In order
 	 * to increase buffer re-use, we aim to keep any such buffers the
-	 * same. This means the address passed to the FM should be DPA_BP_HEAD
-	 * before the data for forwarded frames.
+	 * same. This means the address passed to the FM should be
+	 * tx_headroom bytes before the data for forwarded frames.
 	 *
 	 * However, offer some flexibility in fd layout, to allow originating
 	 * (termination) buffers to be also recycled when possible.
@@ -1775,23 +1793,24 @@ static int skb_to_contig_fd(struct dpa_priv_s *priv,
 	 * - there's enough room in the buffer pool
 	 */
 	if (likely(skb_is_recycleable(skb, dpa_bp->size) &&
-		   (skb_end_pointer(skb) - skb->head <= DPA_BP_MAX_BUF_SIZE) &&
+		   (skb_end_pointer(skb) - skb->head <=
+			dpa_bp->size + DPA_RECYCLE_EXTRA_SIZE) &&
 		   (*percpu_priv->dpa_bp_count < dpa_bp->target_count))) {
 		/* Compute the minimum necessary fd offset */
 		offset = dpa_bp->size - skb->len - skb_tailroom(skb);
 
 		/*
-		 * And make sure the offset is no lower than DPA_BP_HEAD,
-		 * as required by FMan
+		 * And make sure the offset is no lower than the offset
+		 * required by FMan
 		 */
-		offset = max(offset, (int)DPA_BP_HEAD);
+		offset = max_t(int, offset, priv->tx_headroom);
 
 		/*
 		 * We also need to align the buffer address to 16, such that
 		 * Fman will be able to reuse it on Rx.
 		 * Since the buffer going to FMan starts at (skb->data - offset)
 		 * this is what we'll try to align. We already know that
-		 * headroom is at least DPA_BP_HEAD bytes long, but with
+		 * headroom is at least tx_headroom bytes long, but with
 		 * the extra offset needed for alignment we may go beyond
 		 * the beginning of the buffer.
 		 *
@@ -1821,7 +1840,7 @@ static int skb_to_contig_fd(struct dpa_priv_s *priv,
 		 * No recycling here, so we don't care about address alignment.
 		 * Just use the smallest offset required by FMan
 		 */
-		offset = DPA_BP_HEAD;
+		offset = priv->tx_headroom;
 	}
 
 	skbh = (struct sk_buff **)(skb->data - offset);
@@ -1879,10 +1898,10 @@ int __hot dpa_tx(struct sk_buff *skb, struct net_device *net_dev)
 	clear_fd(&fd);
 	queue_mapping = dpa_get_queue_mapping(skb);
 
-	if (unlikely(skb_headroom(skb) < DPA_BP_HEAD)) {
+	if (unlikely(skb_headroom(skb) < priv->tx_headroom)) {
 		struct sk_buff *skb_new;
 
-		skb_new = skb_realloc_headroom(skb, DPA_BP_HEAD);
+		skb_new = skb_realloc_headroom(skb, priv->tx_headroom);
 		if (unlikely(!skb_new)) {
 			percpu_stats->tx_errors++;
 			kfree_skb(skb);
@@ -2003,7 +2022,7 @@ ingress_rx_error_dqrr(struct qman_portal		*portal,
 		return qman_cb_dqrr_stop;
 	}
 
-	dpaa_eth_refill_bpools(percpu_priv);
+	dpaa_eth_refill_bpools(priv, percpu_priv);
 	_dpa_rx_error(net_dev, priv, percpu_priv, &dq->fd, fq->fqid);
 
 	return qman_cb_dqrr_consume;
@@ -2041,7 +2060,7 @@ shared_rx_dqrr(struct qman_portal *portal, struct qman_fq *fq,
 	}
 
 	skb = __netdev_alloc_skb(net_dev,
-				 DPA_BP_HEAD + dpa_fd_length(fd),
+				 priv->tx_headroom + dpa_fd_length(fd),
 				 GFP_ATOMIC);
 	if (unlikely(skb == NULL)) {
 		if (netif_msg_rx_err(priv) && net_ratelimit())
@@ -2052,7 +2071,7 @@ shared_rx_dqrr(struct qman_portal *portal, struct qman_fq *fq,
 		goto out;
 	}
 
-	skb_reserve(skb, DPA_BP_HEAD);
+	skb_reserve(skb, priv->tx_headroom);
 
 	if (fd->format == qm_fd_sg) {
 		if (dpa_bp->vaddr) {
@@ -2164,7 +2183,7 @@ ingress_rx_default_dqrr(struct qman_portal		*portal,
 	}
 
 	/* Vale of plenty: make sure we didn't run out of buffers */
-	dpaa_eth_refill_bpools(percpu_priv);
+	dpaa_eth_refill_bpools(priv, percpu_priv);
 	_dpa_rx(net_dev, priv, percpu_priv, &dq->fd, fq->fqid);
 
 	return qman_cb_dqrr_consume;
@@ -2600,7 +2619,8 @@ static int dpa_tx_unit_test(struct net_device *net_dev)
 
 	/* Try packet sizes from 64-bytes to just above the maximum */
 	for (size = 64; size <= 9600 + 128; size += 64) {
-		for (headroom = DPA_BP_HEAD; headroom < 0x800; headroom += 16) {
+		for (headroom = priv->tx_headroom; headroom < 0x800;
+		     headroom += 16) {
 			int ret;
 			struct sk_buff *skb;
 
@@ -2731,10 +2751,11 @@ static int __cold dpa_start(struct net_device *net_dev)
 	}
 	for_each_online_cpu(i) {
 		percpu_priv = per_cpu_ptr(priv->percpu_priv, i);
-		if (!priv->shared && !percpu_priv->dpa_bp)
+		if (!priv->shared && !percpu_priv->dpa_bp) {
 			percpu_priv->dpa_bp = priv->dpa_bp;
 			percpu_priv->dpa_bp_count =
 				per_cpu_ptr(priv->dpa_bp->percpu_count, i);
+		}
 	}
 
 	dpaa_eth_napi_enable(priv);
@@ -3081,12 +3102,15 @@ static int __cold dpa_debugfs_show(struct seq_file *file, void *offset)
 		"CPU           irqs        rx        tx   recycle" \
 		"   confirm     tx sg    tx err    rx err   bp count\n",
 		priv->net_dev->name);
+
 	for_each_online_cpu(i) {
 		percpu_priv = per_cpu_ptr(priv->percpu_priv, i);
 
 		/* Only private interfaces have an associated counter for bp
-		 * buffers */
-		if (!priv->shared)
+		 * buffers. Also the counter isn't initialized before the first
+		 * ifconfig up
+		 */
+		if (!priv->shared && percpu_priv->dpa_bp_count)
 			dpa_bp_count = *percpu_priv->dpa_bp_count;
 
 		total.in_interrupt += percpu_priv->in_interrupt;
@@ -3607,17 +3631,18 @@ static void dpa_setup_ingress_queues(struct dpa_priv_s *priv,
 
 static void
 dpaa_eth_init_tx_port(struct fm_port *port, struct dpa_fq *errq,
-		struct dpa_fq *defq, bool has_timer)
+		struct dpa_fq *defq, struct dpa_buffer_layout_s *buf_layout)
 {
 	struct fm_port_params tx_port_param;
 
 	dpaa_eth_init_port(tx, port, tx_port_param, errq->fqid, defq->fqid,
-			DPA_TX_PRIV_DATA_SIZE, has_timer);
+			   buf_layout);
 }
 
 static void
 dpaa_eth_init_rx_port(struct fm_port *port, struct dpa_bp *bp, size_t count,
-		struct dpa_fq *errq, struct dpa_fq *defq, bool has_timer)
+		struct dpa_fq *errq, struct dpa_fq *defq,
+		struct dpa_buffer_layout_s *buf_layout)
 {
 	struct fm_port_params rx_port_param;
 	int i;
@@ -3632,7 +3657,7 @@ dpaa_eth_init_rx_port(struct fm_port *port, struct dpa_bp *bp, size_t count,
 	}
 
 	dpaa_eth_init_port(rx, port, rx_port_param, errq->fqid, defq->fqid,
-			DPA_RX_PRIV_DATA_SIZE, has_timer);
+			   buf_layout);
 }
 
 static void dpa_rx_fq_init(struct dpa_priv_s *priv, struct list_head *head,
@@ -3741,7 +3766,8 @@ static int dpa_netdev_init(struct device_node *dpa_node,
 	memcpy(net_dev->dev_addr, mac_addr, net_dev->addr_len);
 
 	SET_ETHTOOL_OPS(net_dev, &dpa_ethtool_ops);
-	net_dev->needed_headroom = DPA_BP_HEAD;
+
+	net_dev->needed_headroom = priv->tx_headroom;
 	net_dev->watchdog_timeo = msecs_to_jiffies(tx_timeout);
 
 	err = register_netdev(net_dev);
@@ -3795,7 +3821,12 @@ static int dpa_private_netdev_init(struct device_node *dpa_node,
 		/* init the percpu list and add some skbs */
 		skb_queue_head_init(&percpu_priv->skb_list);
 
-		dpa_list_add_skbs(percpu_priv, DEFAULT_SKB_COUNT);
+		/* Skbs must accomodate the headroom and enough space
+		 * for the frame headers.
+		 */
+		dpa_list_add_skbs(percpu_priv, DEFAULT_SKB_COUNT,
+				  priv->tx_headroom + DPA_COPIED_HEADERS_SIZE +
+				  dpa_get_rx_extra_headroom());
 #endif
 		netif_napi_add(net_dev, &percpu_priv->napi, dpaa_eth_poll,
 			       DPA_NAPI_WEIGHT);
@@ -3888,7 +3919,7 @@ static const struct of_device_id dpa_match[];
 static int
 dpaa_eth_probe(struct platform_device *_of_dev)
 {
-	int err, i;
+	int err = 0, i;
 	struct device *dev;
 	struct device_node *dpa_node;
 	struct dpa_bp *dpa_bp;
@@ -3898,6 +3929,7 @@ dpaa_eth_probe(struct platform_device *_of_dev)
 	size_t count;
 	struct net_device *net_dev = NULL;
 	struct dpa_priv_s *priv = NULL;
+	struct dpa_percpu_priv_s *percpu_priv;
 	struct dpa_fq *rxdefault = NULL;
 	struct dpa_fq *txdefault = NULL;
 	struct dpa_fq *rxerror = NULL;
@@ -3908,7 +3940,7 @@ dpaa_eth_probe(struct platform_device *_of_dev)
 	struct dpa_fq *txrecycle = NULL;
 	struct fm_port *rxport = NULL;
 	struct fm_port *txport = NULL;
-	bool has_timer = FALSE;
+	struct dpa_buffer_layout_s *buf_layout = NULL;
 	bool is_shared = false;
 	struct mac_device *mac_dev;
 	int proxy_enet;
@@ -3966,6 +3998,18 @@ dpaa_eth_probe(struct platform_device *_of_dev)
 	} else if (mac_dev) {
 		rxport = mac_dev->port_dev[RX];
 		txport = mac_dev->port_dev[TX];
+
+		/* We have physical ports, so we need to establish
+		 * the buffer layout.
+		 */
+		buf_layout = devm_kzalloc(dev, 2 * sizeof(*buf_layout),
+					  GFP_KERNEL);
+		if (!buf_layout) {
+			dev_err(dev, "devm_kzalloc() failed\n");
+			goto alloc_failed;
+		}
+		dpa_set_buffer_layout(priv, rxport, &buf_layout[RX], RX);
+		dpa_set_buffer_layout(priv, txport, &buf_layout[TX], TX);
 	}
 
 	if (!dpa_bp->kernel_pool) {
@@ -3975,7 +4019,7 @@ dpaa_eth_probe(struct platform_device *_of_dev)
 		 * buffer pool, based on FMan port buffer layout;also update
 		 * the maximum buffer size for private ports if necessary
 		 */
-		dpa_bp->size = dpa_bp_size(rxport);
+		dpa_bp->size = dpa_bp_size(&buf_layout[RX]);
 		if (dpa_bp->size > default_buf_size)
 			default_buf_size = dpa_bp->size;
 	}
@@ -4082,8 +4126,13 @@ dpaa_eth_probe(struct platform_device *_of_dev)
 				goto fq_alloc_failed;
 		}
 
-		if (priv->tsu && priv->tsu->valid)
-			has_timer = TRUE;
+		if (mac_dev) {
+			priv->buf_layout = buf_layout;
+			priv->tx_headroom =
+				dpa_get_headroom(&priv->buf_layout[TX]);
+		} else {
+			priv->tx_headroom = DPA_DEFAULT_TX_HEADROOM;
+		}
 	}
 
 	/* All real interfaces need their ports initialized */
@@ -4091,8 +4140,9 @@ dpaa_eth_probe(struct platform_device *_of_dev)
 		struct fm_port_pcd_param rx_port_pcd_param;
 
 		dpaa_eth_init_rx_port(rxport, dpa_bp, count, rxerror,
-				rxdefault, has_timer);
-		dpaa_eth_init_tx_port(txport, txerror, txdefault, has_timer);
+				      rxdefault, &buf_layout[RX]);
+		dpaa_eth_init_tx_port(txport, txerror, txdefault,
+				      &buf_layout[TX]);
 
 		rx_port_pcd_param.cba = dpa_alloc_pcd_fqids;
 		rx_port_pcd_param.cbf = dpa_free_pcd_fqids;
@@ -4126,6 +4176,10 @@ dpaa_eth_probe(struct platform_device *_of_dev)
 		err = -ENOMEM;
 		goto alloc_percpu_failed;
 	}
+	for_each_online_cpu(i) {
+		percpu_priv = per_cpu_ptr(priv->percpu_priv, i);
+		memset(percpu_priv, 0, sizeof(*percpu_priv));
+	}
 
 	if (priv->shared)
 		err = dpa_shared_netdev_init(dpa_node, net_dev);
@@ -4166,6 +4220,7 @@ get_channel_failed:
 bp_create_failed:
 tx_fq_probe_failed:
 rx_fq_probe_failed:
+alloc_failed:
 mac_probe_failed:
 bp_probe_failed:
 	dev_set_drvdata(dev, NULL);
diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth.h b/drivers/net/ethernet/freescale/dpa/dpaa_eth.h
index d931274..60e165f 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth.h
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth.h
@@ -71,6 +71,22 @@
 #define dpa_get_max_mtu()	\
 	(dpa_get_max_frm() - (VLAN_ETH_HLEN + ETH_FCS_LEN))
 
+
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+/* We may want this value configurable. Must be <= PAGE_SIZE
+ * A lower value may help with recycling rates, at least on forwarding
+ */
+#define dpa_bp_size(buffer_layout)	PAGE_SIZE
+#else
+
+/* Default buffer size is based on L2 MAX_FRM value, minus the FCS which
+ * is stripped down by hardware.
+ */
+#define dpa_bp_size(buffer_layout) \
+	dpa_get_buffer_size(buffer_layout, (dpa_get_max_frm() - ETH_FCS_LEN))
+#endif
+
+
 #define DPA_RX_PRIV_DATA_SIZE   (DPA_TX_PRIV_DATA_SIZE + \
 					dpa_get_rx_extra_headroom())
 /* number of Tx queues to FMan */
@@ -146,32 +162,16 @@ struct dpaa_eth_hooks_s {
 
 void fsl_dpaa_eth_set_hooks(struct dpaa_eth_hooks_s *hooks);
 
-/* The netdevice's needed_headroom */
-#define DPA_BP_HEAD (DPA_TX_PRIV_DATA_SIZE + DPA_PARSE_RESULTS_SIZE + \
-			DPA_HASH_RESULTS_SIZE)
-#define DPA_BP_SIZE(s)	(DPA_BP_HEAD + dpa_get_rx_extra_headroom() + (s))
-
 #define DPA_SGT_MAX_ENTRIES 16 /* maximum number of entries in SG Table */
 
 #ifdef CONFIG_DPAA_ETH_SG_SUPPORT
 #define DEFAULT_SKB_COUNT 64 /* maximum number of SKBs in each percpu list */
 /*
- * We may want this value configurable. Must be <= PAGE_SIZE
- * A lower value may help with recycling rates, at least on forwarding
- */
-#define DEFAULT_BUF_SIZE	PAGE_SIZE
-/*
  * Default amount data to be copied from the beginning of a frame into the
  * linear part of the skb, in case we aren't using the hardware parser.
  */
 #define DPA_COPIED_HEADERS_SIZE 128
 
-#else
-/*
- * Default buffer size is based on L2 MAX_FRM value, minus the FCS which is
- * stripped down by hardware.
- */
-#define DEFAULT_BUF_SIZE DPA_BP_SIZE(dpa_get_max_frm() - ETH_FCS_LEN)
 #endif /* CONFIG_DPAA_ETH_SG_SUPPORT */
 
 /*
@@ -390,6 +390,12 @@ struct dpa_priv_s {
 		 */
 		u32 cgr_congested_count;
 	} cgr_data;
+	/*
+	 * Store here the needed Tx headroom for convenience and speed
+	 * (even though it can be computed based on the fields of buf_layout)
+	 */
+	uint16_t tx_headroom;
+	struct dpa_buffer_layout_s *buf_layout;
 };
 
 extern const struct ethtool_ops dpa_ethtool_ops;
@@ -422,7 +428,8 @@ void __hot _dpa_process_parse_results(const t_FmPrsResult *parse_results,
 void dpa_bp_add_8_pages(const struct dpa_bp *dpa_bp, int cpu_id);
 int _dpa_bp_add_8_pages(const struct dpa_bp *dpa_bp);
 
-void dpa_list_add_skbs(struct dpa_percpu_priv_s *cpu_priv, int count);
+void dpa_list_add_skbs(struct dpa_percpu_priv_s *cpu_priv, int count,
+		int skb_size);
 #endif
 
 /*
@@ -479,6 +486,28 @@ static inline int dpa_check_rx_mtu(struct sk_buff *skb, int mtu)
 	return 0;
 }
 
+static inline uint16_t dpa_get_headroom(struct dpa_buffer_layout_s *bl)
+{
+	/* The frame headroom must accomodate:
+	 * - the driver private data area
+	 * - parse results, hash results, timestamp if selected
+	 * If either hash results or time stamp are selected, both will
+	 * be copied to/from the frame headroom, as TS is located between PR and
+	 * HR in the IC and IC copy size has a granularity of 16bytes
+	 * (see description of FMBM_RICP and FMBM_TICP registers in DPAARM)
+	 */
+	return bl->priv_data_size +
+		(bl->parse_results ? DPA_PARSE_RESULTS_SIZE : 0) +
+		(bl->hash_results || bl->time_stamp ?
+		 DPA_TIME_STAMP_SIZE + DPA_HASH_RESULTS_SIZE : 0);
+}
+
+static inline uint16_t dpa_get_buffer_size(struct dpa_buffer_layout_s *bl,
+					   uint16_t data_size)
+{
+	return dpa_get_headroom(bl) + data_size;
+}
+
 void fm_mac_dump_regs(struct mac_device *mac_dev);
 
 void dpaa_eth_sysfs_remove(struct device *dev);
diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c b/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c
index 1b8a64a..9aec590 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c
@@ -179,15 +179,13 @@ static struct sk_buff *dpa_list_get_skb(struct dpa_percpu_priv_s *cpu_priv)
 	return new_skb;
 }
 
-void dpa_list_add_skbs(struct dpa_percpu_priv_s *cpu_priv, int count)
+void dpa_list_add_skbs(struct dpa_percpu_priv_s *cpu_priv, int count, int size)
 {
 	struct sk_buff *new_skb;
 	int i;
 
 	for (i = 0; i < count; i++) {
-		new_skb = dev_alloc_skb(DPA_BP_HEAD +
-				dpa_get_rx_extra_headroom() +
-				DPA_COPIED_HEADERS_SIZE);
+		new_skb = dev_alloc_skb(size);
 		if (unlikely(!new_skb)) {
 			pr_err("dev_alloc_skb() failed\n");
 			break;
@@ -459,8 +457,8 @@ void __hot _dpa_rx(struct net_device *net_dev,
 
 	if (unlikely(skb == NULL)) {
 		/* List is empty, so allocate a new skb */
-		skb = dev_alloc_skb(DPA_BP_HEAD + dpa_get_rx_extra_headroom() +
-			DPA_COPIED_HEADERS_SIZE);
+		skb = dev_alloc_skb(priv->tx_headroom +
+			dpa_get_rx_extra_headroom() + DPA_COPIED_HEADERS_SIZE);
 		if (unlikely(skb == NULL)) {
 			if (netif_msg_rx_err(priv) && net_ratelimit())
 				netdev_err(net_dev,
@@ -476,7 +474,7 @@ void __hot _dpa_rx(struct net_device *net_dev,
 	 * Make sure forwarded skbs will have enough space on Tx,
 	 * if extra headers are added.
 	 */
-	skb_reserve(skb, DPA_BP_HEAD + dpa_get_rx_extra_headroom());
+	skb_reserve(skb, priv->tx_headroom + dpa_get_rx_extra_headroom());
 
 	dpa_bp_removed_one_page(dpa_bp, addr);
 
@@ -538,8 +536,8 @@ static int __hot skb_to_contig_fd(struct dpa_priv_s *priv,
 	struct net_device *net_dev = priv->net_dev;
 	int err;
 
-	/* We are guaranteed that we have at least DPA_BP_HEAD of headroom. */
-	skbh = (struct sk_buff **)(skb->data - DPA_BP_HEAD);
+	/* We are guaranteed that we have at least tx_headroom bytes */
+	skbh = (struct sk_buff **)(skb->data - priv->tx_headroom);
 
 	*skbh = skb;
 
@@ -562,7 +560,7 @@ static int __hot skb_to_contig_fd(struct dpa_priv_s *priv,
 	/* Fill in the FD */
 	fd->format = qm_fd_contig;
 	fd->length20 = skb->len;
-	fd->offset = DPA_BP_HEAD; /* This is now guaranteed */
+	fd->offset = priv->tx_headroom; /* This is now guaranteed */
 
 	addr = dma_map_single(dpa_bp->dev, skbh, dpa_bp->size, DMA_TO_DEVICE);
 	if (unlikely(dma_mapping_error(dpa_bp->dev, addr))) {
@@ -615,7 +613,7 @@ static int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
 		goto csum_failed;
 	}
 
-	sgt = (struct qm_sg_entry *)(sgt_page + DPA_BP_HEAD);
+	sgt = (struct qm_sg_entry *)(sgt_page + priv->tx_headroom);
 	sgt[0].bpid = dpa_bp->bpid;
 	sgt[0].offset = 0;
 	sgt[0].length = skb_headlen(skb);
@@ -659,7 +657,7 @@ static int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
 	sgt[i - 1].final = 1;
 
 	fd->length20 = skb->len;
-	fd->offset = DPA_BP_HEAD;
+	fd->offset = priv->tx_headroom;
 
 	/* DMA map the SGT page */
 	buffer_start = (void *)sgt - dpa_fd_offset(fd);
@@ -728,10 +726,10 @@ int __hot dpa_tx(struct sk_buff *skb, struct net_device *net_dev)
 		 * data, parse results, etc. Normally this shouldn't happen if
 		 * we're here via the standard kernel stack.
 		 */
-		if (unlikely(skb_headroom(skb) < DPA_BP_HEAD)) {
+		if (unlikely(skb_headroom(skb) < priv->tx_headroom)) {
 			struct sk_buff *skb_new;
 
-			skb_new = skb_realloc_headroom(skb, DPA_BP_HEAD);
+			skb_new = skb_realloc_headroom(skb, priv->tx_headroom);
 			if (unlikely(!skb_new)) {
 				dev_kfree_skb(skb);
 				percpu_stats->tx_errors++;
diff --git a/drivers/net/ethernet/freescale/dpa/offline_port.c b/drivers/net/ethernet/freescale/dpa/offline_port.c
index 434d9cf..bdde1b29 100644
--- a/drivers/net/ethernet/freescale/dpa/offline_port.c
+++ b/drivers/net/ethernet/freescale/dpa/offline_port.c
@@ -97,6 +97,14 @@ static int __cold oh_free_pcd_fqids(struct device *dev, uint32_t base_fqid)
 	return 0;
 }
 
+static void oh_set_buffer_layout(struct dpa_buffer_layout_s *layout)
+{
+	layout->priv_data_size = DPA_TX_PRIV_DATA_SIZE;
+	layout->parse_results = true;
+	layout->hash_results = true;
+	layout->time_stamp = false;
+}
+
 static int
 oh_port_probe(struct platform_device *_of_dev)
 {
@@ -114,6 +122,7 @@ oh_port_probe(struct platform_device *_of_dev)
 	uint32_t		 crt_fq_count;
 	struct fm_port_params	 oh_port_tx_params;
 	struct fm_port_pcd_param	oh_port_pcd_params;
+	struct dpa_buffer_layout_s buf_layout;
 	/* True if the current partition owns the OH port. */
 	bool init_oh_port;
 	const struct of_device_id *match;
@@ -263,10 +272,10 @@ oh_port_probe(struct platform_device *_of_dev)
 		goto return_kfree;
 	}
 
+	oh_set_buffer_layout(&buf_layout);
 	/* Set Tx params */
 	dpaa_eth_init_port(tx, oh_config->oh_port, oh_port_tx_params,
-		oh_config->error_fqid, oh_config->default_fqid,
-		DPA_TX_PRIV_DATA_SIZE, FALSE);
+		oh_config->error_fqid, oh_config->default_fqid, (&buf_layout));
 	/* Set PCD params */
 	oh_port_pcd_params.cba = oh_alloc_pcd_fqids;
 	oh_port_pcd_params.cbf = oh_free_pcd_fqids;
-- 
1.9.1

