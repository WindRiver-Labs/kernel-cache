From d57be58e873861c3051b1b0c0f18cab813ee501c Mon Sep 17 00:00:00 2001
From: Ioana Radulescu <ruxandra.radulescu@freescale.com>
Date: Tue, 29 Jan 2013 21:16:55 +0000
Subject: [PATCH 040/987] dpaa_eth: Use per-cpu Tx confirmation queues

[Original patch taken from QorIQ-SDK-V1.6-SOURCE-20140619-yocto.iso]

Up until now, each Ethernet private interface used to have a number of
Tx queues equal to the number of cpus but a single (default) confirmation
queue. Switch to using a per-core confirmation queue for each Tx queue,
such that for frames sent by a core their respective confirmation will be
processed by the same core.

This, on one hand, improves cache utilization, and on the other hand avoids
dequeueing from a pool channel queue, which incurs a performance penalty
due to a QMan erratum.

This change does not affect shared, MAC-less or proxy interfaces.

Change-Id: I4effd0728b1df94d4adb1cbc507f706303f043ff
Signed-off-by: Ioana Radulescu <ruxandra.radulescu@freescale.com>
(cherry picked from commit 864059e4716e178b98c5d8f90c72a76080dd3ec1)
Reviewed-on: http://git.am.freescale.net:8181/1038
Reviewed-by: Fleming Andrew-AFLEMING <AFLEMING@freescale.com>
Tested-by: Fleming Andrew-AFLEMING <AFLEMING@freescale.com>
---
 drivers/net/ethernet/freescale/dpa/dpaa_eth.c    | 99 +++++++++++++++++++++---
 drivers/net/ethernet/freescale/dpa/dpaa_eth.h    |  1 +
 drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c |  2 +-
 3 files changed, 92 insertions(+), 10 deletions(-)

diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth.c b/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
index 6b38ce2..009ddde 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
@@ -565,6 +565,18 @@ dpa_bp_free(struct dpa_priv_s *priv, struct dpa_bp *dpa_bp)
 
 /* QM */
 
+static struct qman_fq *_dpa_get_tx_conf_queue(const struct dpa_priv_s *priv,
+					       struct qman_fq *tx_fq)
+{
+	int i;
+
+	for (i = 0; i < DPAA_ETH_TX_QUEUES; i++)
+		if (priv->egress_fqs[i] == tx_fq)
+			return priv->conf_fqs[i];
+
+	return NULL;
+}
+
 static int __devinit __must_check __attribute__((nonnull))
 _dpa_fq_alloc(struct list_head *list, struct dpa_fq *dpa_fq)
 {
@@ -573,6 +585,7 @@ _dpa_fq_alloc(struct list_head *list, struct dpa_fq *dpa_fq)
 	struct device		*dev;
 	struct qman_fq		*fq;
 	struct qm_mcc_initfq	 initfq;
+	struct qman_fq		*confq;
 
 	priv = netdev_priv(dpa_fq->net_dev);
 	dev = dpa_fq->net_dev->dev.parent;
@@ -605,6 +618,17 @@ _dpa_fq_alloc(struct list_head *list, struct dpa_fq *dpa_fq)
 		if (dpa_fq->fq_type == FQ_TYPE_TX) {
 			initfq.fqd.fq_ctrl |= QM_FQCTRL_CGE;
 			initfq.fqd.cgid = priv->cgr_data.cgr.cgrid;
+
+			/* Configure per-cpu Tx confirmation queue */
+			confq = _dpa_get_tx_conf_queue(priv, &dpa_fq->fq_base);
+			if (confq) {
+				initfq.we_mask |= QM_INITFQ_WE_CONTEXTA |
+						  QM_INITFQ_WE_CONTEXTB;
+				/* CTXA[OVFQ] = 1 */
+				initfq.fqd.context_a.hi = 0x80000000;
+				initfq.fqd.context_a.lo = 0x0;
+				initfq.fqd.context_b = qman_fq_fqid(confq);
+			}
 		}
 
 		/* Initialization common to all ingress queues */
@@ -1508,7 +1532,7 @@ static int __hot dpa_shared_tx(struct sk_buff *skb, struct net_device *net_dev)
 	memset(&fd, 0, sizeof(fd));
 	fd.format = qm_fd_contig;
 
-	queue_mapping = skb_get_queue_mapping(skb);
+	queue_mapping = smp_processor_id();
 
 	dpa_bp = dpa_size2pool(priv, skb_headlen(skb));
 	if (unlikely(IS_ERR(dpa_bp))) {
@@ -1790,7 +1814,8 @@ int __hot dpa_tx(struct sk_buff *skb, struct net_device *net_dev)
 	percpu_priv = per_cpu_ptr(priv->percpu_priv, smp_processor_id());
 
 	clear_fd(&fd);
-	queue_mapping = skb_get_queue_mapping(skb);
+	/* Use the Tx queue of the current cpu */
+	queue_mapping = smp_processor_id();
 
 	if (skb_headroom(skb) < DPA_BP_HEAD) {
 		struct sk_buff *skb_new;
@@ -3176,10 +3201,14 @@ static const struct fqid_cell default_fqids[][3] __devinitconst = {
 	[TX] = { {0, 1}, {0, 1}, {0, DPAA_ETH_TX_QUEUES} }
 };
 
+static const struct fqid_cell tx_confirm_fqids[] __devinitconst = {
+	{0, DPAA_ETH_TX_QUEUES}
+};
+
 static int __devinit
 dpa_fq_probe(struct platform_device *_of_dev, struct list_head *list,
 		struct dpa_fq **defq, struct dpa_fq **errq,
-		struct dpa_fq **fqs, int ptype)
+		struct dpa_fq **fqs, struct dpa_fq **txconfq, int ptype)
 {
 	struct device *dev = &_of_dev->dev;
 	struct device_node *np = dev->of_node;
@@ -3189,6 +3218,27 @@ dpa_fq_probe(struct platform_device *_of_dev, struct list_head *list,
 	struct dpa_fq *dpa_fq;
 	int err = 0;
 
+	/* per-core tx confirmation queues */
+	if (txconfq) {
+		fqids = tx_confirm_fqids;
+		dpa_fq = devm_kzalloc(dev, sizeof(*dpa_fq) * fqids[0].count,
+					GFP_KERNEL);
+		if (dpa_fq == NULL) {
+			dpaa_eth_err(dev, "devm_kzalloc() failed\n");
+			return -ENOMEM;
+		}
+		*txconfq = dpa_fq;
+		for (j = 0; j < fqids[0].count; j++)
+			dpa_fq[j].fq_type = FQ_TYPE_TX_CONFIRM;
+
+		for (j = 0; j < fqids[0].count; j++) {
+			dpa_fq[j].fqid = fqids[0].start ?
+				fqids[0].start + j : 0;
+			_dpa_assign_wq(dpa_fq + j);
+			list_add_tail(&dpa_fq[j].list, list);
+		}
+	}
+
 	fqids = of_get_property(np, fsl_qman_frame_queues[ptype], &lenp);
 	if (fqids == NULL) {
 		fqids = default_fqids[ptype];
@@ -3300,6 +3350,29 @@ static void dpa_setup_egress(struct dpa_priv_s *priv,
 	}
 }
 
+static void dpa_setup_conf_queues(struct dpa_priv_s *priv, struct dpa_fq *fq)
+{
+	const cpumask_t *affine_cpus = qman_affine_cpus();
+	struct list_head *ptr = &fq->list;
+	int i;
+
+	/*
+	 * Configure the queues to be core affine.
+	 * The implicit assumption here is that each cpu has its own Tx queue
+	 */
+	for (i = 0; i < DPAA_ETH_TX_QUEUES; i++) {
+		struct dpa_fq *iter = list_entry(ptr, struct dpa_fq, list);
+
+		dpa_setup_ingress(priv, iter, &tx_private_defq);
+		/* If cpu not online, just leave the default pool channel */
+		if (cpumask_test_cpu(i, affine_cpus))
+			iter->channel = qman_affine_channel(i);
+		priv->conf_fqs[i] = &iter->fq_base;
+
+		ptr = ptr->next;
+	}
+}
+
 static void dpa_setup_ingress_queues(struct dpa_priv_s *priv,
 		struct list_head *head, struct dpa_fq *fq)
 {
@@ -3400,7 +3473,8 @@ static void dpa_rx_fq_init(struct dpa_priv_s *priv, struct list_head *head,
 
 static void dpa_tx_fq_init(struct dpa_priv_s *priv, struct list_head *head,
 			struct dpa_fq *defq, struct dpa_fq *errq,
-			struct dpa_fq *fqs, struct fm_port *port)
+			struct dpa_fq *fqs, struct dpa_fq *confqs,
+			struct fm_port *port)
 {
 	if (fqs)
 		dpa_setup_egress(priv, head, fqs, port);
@@ -3419,6 +3493,8 @@ static void dpa_tx_fq_init(struct dpa_priv_s *priv, struct list_head *head,
 	} else {
 		dpa_setup_ingress(priv, defq, &tx_private_defq);
 		dpa_setup_ingress(priv, errq, &tx_private_errq);
+		if (confqs)
+			dpa_setup_conf_queues(priv, confqs);
 	}
 }
 
@@ -3643,9 +3719,11 @@ dpaa_eth_probe(struct platform_device *_of_dev)
 	struct dpa_fq *txerror = NULL;
 	struct dpa_fq *rxextra = NULL;
 	struct dpa_fq *txfqs = NULL;
+	struct dpa_fq *txconf = NULL;
 	struct fm_port *rxport = NULL;
 	struct fm_port *txport = NULL;
 	bool has_timer = FALSE;
+	bool is_shared = false;
 	struct mac_device *mac_dev;
 	int proxy_enet;
 	const struct of_device_id *match;
@@ -3694,6 +3772,8 @@ dpaa_eth_probe(struct platform_device *_of_dev)
 		err = PTR_ERR(dpa_bp);
 		goto bp_probe_failed;
 	}
+	if (!dpa_bp->kernel_pool)
+		is_shared = true;
 
 	mac_dev = dpa_mac_probe(_of_dev);
 	if (IS_ERR(mac_dev)) {
@@ -3709,19 +3789,20 @@ dpaa_eth_probe(struct platform_device *_of_dev)
 
 	if (rxport)
 		err = dpa_fq_probe(_of_dev, &rxfqlist, &rxdefault, &rxerror,
-				&rxextra, RX);
+				&rxextra, NULL, RX);
 	else
 		err = dpa_fq_probe(_of_dev, &rxfqlist, NULL, NULL,
-				&rxextra, RX);
+				&rxextra, NULL, RX);
 
 	if (err < 0)
 		goto rx_fq_probe_failed;
 
 	if (txport)
 		err = dpa_fq_probe(_of_dev, &txfqlist, &txdefault, &txerror,
-				&txfqs, TX);
+				&txfqs, (is_shared ? NULL : &txconf), TX);
 	else
-		err = dpa_fq_probe(_of_dev, &txfqlist, NULL, NULL, &txfqs, TX);
+		err = dpa_fq_probe(_of_dev, &txfqlist, NULL, NULL, &txfqs,
+				NULL, TX);
 
 	if (err < 0)
 		goto tx_fq_probe_failed;
@@ -3767,7 +3848,7 @@ dpaa_eth_probe(struct platform_device *_of_dev)
 
 		dpa_rx_fq_init(priv, &rxfqlist, rxdefault, rxerror, rxextra);
 		dpa_tx_fq_init(priv, &txfqlist, txdefault, txerror, txfqs,
-				txport);
+				txconf, txport);
 
 		/*
 		 * Create a congestion group for this netdev, with
diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth.h b/drivers/net/ethernet/freescale/dpa/dpaa_eth.h
index 0bbfd0b..9cbe9d7 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth.h
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth.h
@@ -354,6 +354,7 @@ struct dpa_priv_s {
 	uint16_t		 channel;	/* "fsl,qman-channel-id" */
 	struct list_head	 dpa_fq_list;
 	struct qman_fq		*egress_fqs[DPAA_ETH_TX_QUEUES];
+	struct qman_fq		*conf_fqs[DPAA_ETH_TX_QUEUES];
 
 	struct mac_device	*mac_dev;
 
diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c b/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c
index f636d6b..97b31fb 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c
@@ -787,7 +787,7 @@ int __hot dpa_tx(struct sk_buff *skb, struct net_device *net_dev)
 
 	clear_fd(&fd);
 
-	queue_mapping = skb_get_queue_mapping(skb);
+	queue_mapping = smp_processor_id();
 
 
 #ifdef CONFIG_FSL_DPA_1588
-- 
1.9.1

