From 3bf8b666613d0cb1c7c325dc2306cfea13d77346 Mon Sep 17 00:00:00 2001
From: Marian Chereji <marian.chereji@freescale.com>
Date: Mon, 21 Jul 2014 17:33:10 +0300
Subject: [PATCH 804/987] dpa_offload: Aggregate dynamically created HM
 operations

The DPA classifier driver allows multiple HM operations to be aggregated
in the same HM node only when importing the HM node resources. When
creating the nodes dynamically, the driver does not search for candidates
for aggregation before creating the HM nodes.

This update improves the dpa_classifier driver to actively check for
HM operation aggregation possibilities to certain neighbor HM nodes even
when using the API in dynamic mode.

Signed-off-by: Marian Chereji <marian.chereji@freescale.com>
Change-Id: I96bb3eb62be81e3d1c3ac83947f32bee230ace38
Reviewed-on: http://git.am.freescale.net:8181/15220
Reviewed-by: Radu-Andrei Bulie <Radu.Bulie@freescale.com>
[Lu:Original patch taken from
QorIQ-SDK-V1.7-SOURCE-20141218-yocto.iso]
Signed-off-by: Jiang Lu <lu.jiang@windriver.com>
---
 drivers/staging/fsl_dpa_offload/dpa_classifier.c | 448 +++++++++++++----------
 drivers/staging/fsl_dpa_offload/dpa_classifier.h |   8 -
 2 files changed, 252 insertions(+), 204 deletions(-)

diff --git a/drivers/staging/fsl_dpa_offload/dpa_classifier.c b/drivers/staging/fsl_dpa_offload/dpa_classifier.c
index 49dad23..f3dcb51 100644
--- a/drivers/staging/fsl_dpa_offload/dpa_classifier.c
+++ b/drivers/staging/fsl_dpa_offload/dpa_classifier.c
@@ -3797,6 +3797,166 @@ static int import_hm_nodes_to_chain(void * const *node_array,
 	return 0;
 }
 
+static struct dpa_cls_hm_node *try_compatible_node(const struct dpa_cls_hm *hm)
+{
+	struct dpa_cls_hm_node *hm_node = NULL;
+	const int update_flags = DPA_CLS_HM_UPDATE_IPv4_UPDATE |
+					DPA_CLS_HM_UPDATE_IPv6_UPDATE |
+					DPA_CLS_HM_UPDATE_UDP_TCP_UPDATE;
+	const int replace_flags = DPA_CLS_HM_REPLACE_IPv4_BY_IPv6 |
+					DPA_CLS_HM_REPLACE_IPv6_BY_IPv4;
+
+	dpa_cls_dbg(("DEBUG: dpa_classifier %s (%d) -->\n", __func__,
+		__LINE__));
+
+	if (list_empty(hm->hm_chain)) {
+		/*
+		 * There is nothing in the HM node chain. Don't bother any more
+		 * to look for anything:
+		 */
+		dpa_cls_dbg(("DEBUG: dpa_classifier %s (%d) <-- did not find a compatible node.\n",
+			__func__, __LINE__));
+		return NULL;
+	}
+
+	/* Get the last item in the chain */
+	hm_node = list_entry(hm->hm_chain->next,
+				struct dpa_cls_hm_node, list_node);
+	/*
+	 * If the previous HM node is not a HDR_MANIP, then it can't be
+	 * compatible for aggregation:
+	 */
+	if (hm_node->params.type != e_FM_PCD_MANIP_HDR) {
+		dpa_cls_dbg(("DEBUG: dpa_classifier %s (%d) <-- did not find a compatible node.\n",
+			__func__, __LINE__));
+		return NULL;
+	}
+
+	switch (hm->type) {
+	case DPA_CLS_HM_TYPE_REMOVE:
+		dpa_cls_dbg(("DEBUG: dpa_classifier %s (%d): Looking for REMOVE HM compatible nodes...\n",
+			__func__, __LINE__));
+		/*
+		 * If in the previous HM node the remove operation is already
+		 * used, then it is not compatible for aggregation:
+		 */
+		if (hm_node->params.u.hdr.rmv)
+			hm_node = NULL;
+		break;
+	case DPA_CLS_HM_TYPE_INSERT:
+		dpa_cls_dbg(("DEBUG: dpa_classifier %s (%d): Looking for INSERT HM compatible nodes...\n",
+			__func__, __LINE__));
+		/*
+		 * If in the previous HM node the insert operation is already
+		 * used, then it is not compatible for aggregation:
+		 */
+		if (hm_node->params.u.hdr.insrt)
+			hm_node = NULL;
+		break;
+	case DPA_CLS_HM_TYPE_UPDATE:
+		dpa_cls_dbg(("DEBUG: dpa_classifier %s (%d): Looking for UPDATE HM compatible nodes...\n",
+			__func__, __LINE__));
+		/*
+		 * If in the previous HM node the update operation is already
+		 * used and we also have to do header updates, then it is not
+		 * compatible for aggregation:
+		 */
+		if ((hm->update_params.op_flags & update_flags) &&
+			(hm_node->params.u.hdr.fieldUpdate))
+			hm_node = NULL;
+
+		/*
+		 * If in the previous HM node the custom header replace
+		 * operation is already used and we also have to do header
+		 * replacement, then it is not compatible for aggregation:
+		 */
+		if ((hm->update_params.op_flags & replace_flags) &&
+			(hm_node->params.u.hdr.custom))
+			hm_node = NULL;
+		break;
+	case DPA_CLS_HM_TYPE_VLAN:
+		dpa_cls_dbg(("DEBUG: dpa_classifier %s (%d): Looking for VLAN HM compatible nodes...\n",
+			__func__, __LINE__));
+		switch (hm->vlan_params.type) {
+		case DPA_CLS_HM_VLAN_INGRESS:
+			/*
+			 * If in the previous HM node the remove operation is
+			 * already used, then it is not compatible for
+			 * aggregation:
+			 */
+			if (hm_node->params.u.hdr.rmv)
+				hm_node = NULL;
+			break;
+		case DPA_CLS_HM_VLAN_EGRESS:
+			/*
+			 * If in the previous HM node the insert operation is
+			 * already used and we need to insert VLANs, then it is
+			 * not compatible for aggregation:
+			 */
+			if ((hm->vlan_params.egress.num_tags) &&
+				(hm_node->params.u.hdr.insrt))
+				hm_node = NULL;
+			/*
+			 * If in the previous HM node the update operation is
+			 * already used and we need to do VLAN update, then it
+			 * is not compatible for aggregation:
+			 */
+			if ((hm->vlan_params.egress.update_op) &&
+				(hm_node->params.u.hdr.fieldUpdate))
+				hm_node = NULL;
+			break;
+		default:
+			hm_node = NULL;
+			break;
+		}
+		break;
+	case DPA_CLS_HM_TYPE_MPLS:
+		dpa_cls_dbg(("DEBUG: dpa_classifier %s (%d): Looking for MPLS HM compatible nodes...\n",
+			__func__, __LINE__));
+		switch (hm->mpls_params.type) {
+		case DPA_CLS_HM_MPLS_INSERT_LABELS:
+			/*
+			 * If in the previous HM node the insert operation is
+			 * already used, then it is not compatible for
+			 * aggregation:
+			 */
+			if (hm_node->params.u.hdr.insrt)
+				hm_node = NULL;
+			break;
+		case DPA_CLS_HM_MPLS_REMOVE_ALL_LABELS:
+			/*
+			 * If in the previous HM node the remove operation is
+			 * already used, then it is not compatible for
+			 * aggregation:
+			 */
+			if (hm_node->params.u.hdr.rmv)
+				hm_node = NULL;
+			break;
+		default:
+			hm_node = NULL;
+			break;
+		}
+		break;
+	default:
+		hm_node = NULL;
+		break;
+	}
+
+#ifdef DPA_CLASSIFIER_DEBUG
+	if (hm_node)
+		dpa_cls_dbg(("DEBUG: dpa_classifier %s (%d): FOUND compatible hm_node = 0x%p.\n",
+			__func__, __LINE__, hm_node));
+	else
+		dpa_cls_dbg(("DEBUG: dpa_classifier %s (%d): Compatible hm_node NOT FOUND.\n",
+			__func__, __LINE__));
+#endif /* DPA_CLASSIFIER_DEBUG */
+
+	dpa_cls_dbg(("DEBUG: dpa_classifier %s (%d) <--\n", __func__,
+		__LINE__));
+
+	return hm_node;
+}
+
 static int add_local_hm_nodes_to_chain(struct dpa_cls_hm *phm)
 {
 	int i;
@@ -4090,41 +4250,6 @@ static void remove_hm_node(struct dpa_cls_hm_node *node)
 	kfree(node);
 }
 
-static struct dpa_cls_hm_node
-	*find_compatible_hm_node(enum dpa_cls_hm_node_type	type,
-				struct list_head		*list)
-{
-	struct dpa_cls_hm_node *phm_node;
-	e_FmPcdManipHdrFieldUpdateType val;
-
-	BUG_ON(!list);
-
-	switch (type) {
-	case DPA_CLS_HM_NODE_IPv4_HDR_UPDATE:
-		val = e_FM_PCD_MANIP_HDR_FIELD_UPDATE_IPV4;
-		break;
-	case DPA_CLS_HM_NODE_IPv6_HDR_UPDATE:
-		val = e_FM_PCD_MANIP_HDR_FIELD_UPDATE_IPV6;
-		break;
-	case DPA_CLS_HM_NODE_TCPUDP_HDR_UPDATE:
-		val = e_FM_PCD_MANIP_HDR_FIELD_UPDATE_TCP_UDP;
-		break;
-	default:
-		log_err("Don't know how to search for nodes compatible with "
-			"type=%d.\n", type);
-		return NULL;
-	}
-
-	list_for_each_entry(phm_node, list, list_node) {
-		if ((phm_node->params.type == e_FM_PCD_MANIP_HDR) &&
-			(phm_node->params.u.hdr.fieldUpdate) &&
-			(phm_node->params.u.hdr.fieldUpdateParams.type == val))
-				return phm_node;
-	}
-
-	return NULL;
-}
-
 static int create_new_hm_op(int *hmd, int next_hmd)
 {
 	int err;
@@ -4324,12 +4449,8 @@ static int nat_hm_prepare_nodes(struct dpa_cls_hm *pnat_hm,
 				const struct dpa_cls_hm_nat_resources *res)
 {
 	struct dpa_cls_hm_node *hm_node = NULL;
-	struct dpa_cls_hm *pnext_hm = NULL;
 	void * const *phm_nodes;
 	int err = 0;
-	enum dpa_cls_hm_node_type l3_update_node = DPA_CLS_HM_NODE_LAST_ENTRY;
-	enum dpa_cls_hm_node_type l4_update_node = DPA_CLS_HM_NODE_LAST_ENTRY;
-	unsigned int ip_ver = 0;
 
 	dpa_cls_dbg(("DEBUG: dpa_classifier %s (%d) -->\n", __func__,
 		__LINE__));
@@ -4346,78 +4467,31 @@ static int nat_hm_prepare_nodes(struct dpa_cls_hm *pnat_hm,
 					pnat_hm);
 	}
 
-	/* Create HM nodes */
-	if (pnat_hm->nat_params.type == DPA_CLS_HM_NAT_TYPE_TRADITIONAL) {
-		if (pnat_hm->nat_params.flags &
-			DPA_CLS_HM_NAT_UPDATE_SIP)
-			ip_ver = pnat_hm->nat_params.nat.sip.version;
-		if (pnat_hm->nat_params.flags &
-			DPA_CLS_HM_NAT_UPDATE_DIP)
-			ip_ver = pnat_hm->nat_params.nat.dip.version;
-		if ((pnat_hm->nat_params.flags &
-				DPA_CLS_HM_NAT_UPDATE_SPORT) ||
-				(pnat_hm->nat_params.flags &
-				DPA_CLS_HM_NAT_UPDATE_DPORT))
-			l4_update_node = DPA_CLS_HM_NODE_TCPUDP_HDR_UPDATE;
-		if (ip_ver) {
-			if (ip_ver == 4)
-				l3_update_node =
-					DPA_CLS_HM_NODE_IPv4_HDR_UPDATE;
-			else
-				l3_update_node =
-					DPA_CLS_HM_NODE_IPv6_HDR_UPDATE;
-		}
-	} else {
-		if (pnat_hm->nat_params.nat_pt.type ==
-				DPA_CLS_HM_NAT_PT_IPv6_TO_IPv4)
-			l3_update_node =
-				DPA_CLS_HM_NODE_HDR_REPLACE_IPv6_BY_IPv4;
-		else
-			l3_update_node =
-				DPA_CLS_HM_NODE_HDR_REPLACE_IPv4_BY_IPv6;
-	}
-
-	/* Check if we can attach to an existing update node */
-	if (!list_empty(&pnat_hm->list_node))
-		pnext_hm = list_entry(pnat_hm->list_node.next,
-			struct dpa_cls_hm,
-			list_node);
-
-	if (l3_update_node != DPA_CLS_HM_NODE_LAST_ENTRY) {
-		/* Check if we can attach to an existing L3 update node */
-		if (pnext_hm)
-			hm_node = find_compatible_hm_node(l3_update_node,
-							pnext_hm->hm_chain);
-		/* If not, create an L3 update node: */
-		if (!hm_node) {
-			hm_node = kzalloc(sizeof(*hm_node), GFP_KERNEL);
-			if (!hm_node) {
-				log_err("No more memory for header manip "
-					"nodes.\n");
-				return -ENOMEM;
-			}
-			INIT_LIST_HEAD(&hm_node->list_node);
-			pnat_hm->hm_node[0] = hm_node;
-		}
+	/* Create a header manip node for this update: */
+	hm_node = kzalloc(sizeof(*hm_node), GFP_KERNEL);
+
+	dpa_cls_dbg(("DEBUG: dpa_classifier %s (%d): Created new hm_node = 0x%p\n",
+		__func__, __LINE__, hm_node));
+	if (!hm_node) {
+		log_err("No more memory for header manip nodes.\n");
+		return -ENOMEM;
 	}
 
-	hm_node = NULL;
-	if (l4_update_node != DPA_CLS_HM_NODE_LAST_ENTRY) {
-		/* Check if we can attach to an existing L4 update node */
-		if (pnext_hm)
-			hm_node = find_compatible_hm_node(l4_update_node,
-							pnext_hm->hm_chain);
-		/* If not create an L4 update node: */
+	INIT_LIST_HEAD(&hm_node->list_node);
+	pnat_hm->hm_node[0] = hm_node;
+
+	if (pnat_hm->nat_params.flags &
+		(DPA_CLS_HM_NAT_UPDATE_SPORT | DPA_CLS_HM_NAT_UPDATE_DPORT)) {
+		hm_node = kzalloc(sizeof(*hm_node), GFP_KERNEL);
+		dpa_cls_dbg(("DEBUG: dpa_classifier %s (%d): Created new hm_node = 0x%p\n",
+			__func__, __LINE__, hm_node));
 		if (!hm_node) {
-			hm_node = kzalloc(sizeof(*hm_node), GFP_KERNEL);
-			if (!hm_node) {
-				log_err("No more memory for header manip "
-					"nodes.\n");
-				return -ENOMEM;
-			}
-			INIT_LIST_HEAD(&hm_node->list_node);
-			pnat_hm->hm_node[1] = hm_node;
+			log_err("No more memory for header manip nodes.\n");
+			return -ENOMEM;
 		}
+
+		INIT_LIST_HEAD(&hm_node->list_node);
+		pnat_hm->hm_node[1] = hm_node;
 	}
 
 	add_local_hm_nodes_to_chain(pnat_hm);
@@ -4887,14 +4961,15 @@ static int fwd_hm_prepare_nodes(struct dpa_cls_hm *pfwd_hm,
 					pfwd_hm);
 	}
 
+	/* Create a header manip node: */
 	hm_node = kzalloc(sizeof(*hm_node), GFP_KERNEL);
 	if (!hm_node) {
-		log_err("Not enough memory for header manip nodes.\n");
+		log_err("No more memory for header manip nodes.\n");
 		return -ENOMEM;
 	}
 
 	INIT_LIST_HEAD(&hm_node->list_node);
-	pfwd_hm->hm_node[0]	= hm_node;
+	pfwd_hm->hm_node[0] = hm_node;
 
 	if (pfwd_hm->update_params.ip_frag_params.mtu) {
 		/* IP fragmentation option is enabled */
@@ -5309,22 +5384,30 @@ static int remove_hm_prepare_nodes(struct dpa_cls_hm *premove_hm,
 	if (res) { /* Import HM nodes */
 		phm_nodes = &res->remove_node;
 
-		err = import_hm_nodes_to_chain(phm_nodes,
+		dpa_cls_dbg(("DEBUG: dpa_classifier %s (%d) <--\n", __func__,
+			__LINE__));
+
+		return import_hm_nodes_to_chain(phm_nodes,
 					premove_hm->num_nodes,
 					premove_hm);
-	} else { /* Create HM nodes */
+	}
+
+	hm_node = try_compatible_node(premove_hm);
+	if (hm_node == NULL) {
+		/* Create a header manip node for this remove: */
 		hm_node = kzalloc(sizeof(*hm_node), GFP_KERNEL);
 		if (!hm_node) {
-			log_err("Not enough memory for header manip nodes.\n");
+			log_err("No more memory for header manip nodes.\n");
 			return -ENOMEM;
 		}
 
 		INIT_LIST_HEAD(&hm_node->list_node);
-		premove_hm->hm_node[0]	= hm_node;
-
-		add_local_hm_nodes_to_chain(premove_hm);
 	}
 
+	premove_hm->hm_node[0] = hm_node;
+
+	add_local_hm_nodes_to_chain(premove_hm);
+
 	dpa_cls_dbg(("DEBUG: dpa_classifier %s (%d) <--\n", __func__,
 		__LINE__));
 
@@ -5594,22 +5677,30 @@ static int insert_hm_prepare_nodes(struct dpa_cls_hm *pinsert_hm,
 	if (res) { /* Import HM nodes */
 		phm_nodes = &res->insert_node;
 
-		err = import_hm_nodes_to_chain(phm_nodes,
+		dpa_cls_dbg(("DEBUG: dpa_classifier %s (%d) <--\n", __func__,
+			__LINE__));
+
+		return import_hm_nodes_to_chain(phm_nodes,
 					pinsert_hm->num_nodes,
 					pinsert_hm);
-	} else { /* Create HM nodes */
+	}
+
+	hm_node = try_compatible_node(pinsert_hm);
+	if (hm_node == NULL) {
+		/* Create a header manip node for this insert: */
 		hm_node = kzalloc(sizeof(*hm_node), GFP_KERNEL);
 		if (!hm_node) {
-			log_err("Not enough memory for header manip nodes.\n");
+			log_err("No more memory for header manip nodes.\n");
 			return -ENOMEM;
 		}
 
 		INIT_LIST_HEAD(&hm_node->list_node);
-		pinsert_hm->hm_node[0]	= hm_node;
-
-		add_local_hm_nodes_to_chain(pinsert_hm);
 	}
 
+	pinsert_hm->hm_node[0] = hm_node;
+
+	add_local_hm_nodes_to_chain(pinsert_hm);
+
 	dpa_cls_dbg(("DEBUG: dpa_classifier %s (%d) <--\n", __func__,
 		__LINE__));
 
@@ -5986,8 +6077,6 @@ static int update_hm_prepare_nodes(struct dpa_cls_hm *pupdate_hm,
 {
 	struct dpa_cls_hm_node *hm_node = NULL;
 	void * const *phm_nodes;
-	struct dpa_cls_hm *pnext_hm = NULL;
-	int update_ops, replace_ops;
 	int err = 0;
 
 	dpa_cls_dbg(("DEBUG: dpa_classifier %s (%d) -->\n", __func__,
@@ -6005,73 +6094,24 @@ static int update_hm_prepare_nodes(struct dpa_cls_hm *pupdate_hm,
 					pupdate_hm);
 	}
 
-	update_ops = DPA_CLS_HM_UPDATE_IPv4_UPDATE |
-			DPA_CLS_HM_UPDATE_IPv6_UPDATE |
-			DPA_CLS_HM_UPDATE_UDP_TCP_UPDATE;
-
-	replace_ops = DPA_CLS_HM_REPLACE_IPv4_BY_IPv6 |
-			DPA_CLS_HM_REPLACE_IPv6_BY_IPv4;
-
-	if ((pupdate_hm->update_params.op_flags & update_ops) ||
-		(pupdate_hm->update_params.op_flags & replace_ops)) {
-		/* Create HM nodes */
-		/* Check if we can attach to an existing update node */
-		if (!list_empty(&pupdate_hm->list_node)) {
-			pnext_hm = list_entry(pupdate_hm->list_node.next,
-					struct dpa_cls_hm,
-					list_node);
-
-			if (pupdate_hm->update_params.op_flags &
-				DPA_CLS_HM_UPDATE_IPv4_UPDATE)
-				/*
-				 * See if there is any other IPv4 update node
-				 * in this chain
-				 */
-				hm_node = find_compatible_hm_node(
-					DPA_CLS_HM_NODE_IPv4_HDR_UPDATE,
-					pnext_hm->hm_chain);
-
-			if (pupdate_hm->update_params.op_flags &
-				DPA_CLS_HM_UPDATE_IPv6_UPDATE)
-				/*
-				 * See if there is any other IPv6 update node
-				 * in this chain
-				 */
-				hm_node = find_compatible_hm_node(
-					DPA_CLS_HM_NODE_IPv6_HDR_UPDATE,
-					pnext_hm->hm_chain);
-
-			if (pupdate_hm->update_params.op_flags &
-				DPA_CLS_HM_UPDATE_UDP_TCP_UPDATE)
-				/*
-				 * See if there is any other TCP/UDP header
-				 * update node in this chain
-				 */
-				hm_node = find_compatible_hm_node(
-					DPA_CLS_HM_NODE_TCPUDP_HDR_UPDATE,
-					pnext_hm->hm_chain);
-		}
+	hm_node = try_compatible_node(pupdate_hm);
+	if ((pupdate_hm->update_params.ip_frag_params.mtu) ||
+			(hm_node == NULL)) {
+		/* Create a header manip node for this update: */
+		hm_node = kzalloc(sizeof(*hm_node), GFP_KERNEL);
 
-		/*
-		 * If no compatible HM node was found for the header update
-		 * operations...
-		 */
+		dpa_cls_dbg(("DEBUG: dpa_classifier %s (%d): Created new hm_node = 0x%p\n",
+			__func__, __LINE__, hm_node));
 		if (!hm_node) {
-			/* Create a header manip node for this update: */
-			hm_node = kzalloc(sizeof(*hm_node), GFP_KERNEL);
-
-			if (!hm_node) {
-				log_err("No more memory for header manip "
-					"nodes.\n");
-				return -ENOMEM;
-			}
-
-			INIT_LIST_HEAD(&hm_node->list_node);
+			log_err("No more memory for header manip nodes.\n");
+			return -ENOMEM;
 		}
 
-		pupdate_hm->hm_node[0] = hm_node;
+		INIT_LIST_HEAD(&hm_node->list_node);
 	}
 
+	pupdate_hm->hm_node[0] = hm_node;
+
 	if (pupdate_hm->update_params.ip_frag_params.mtu) {
 		/* IP fragmentation option is enabled */
 		/* Create a header manip node: */
@@ -6670,22 +6710,30 @@ static int vlan_hm_prepare_nodes(struct dpa_cls_hm *pvlan_hm,
 	if (res) { /* Import HM nodes */
 		phm_nodes = &res->vlan_node;
 
-		err = import_hm_nodes_to_chain(phm_nodes,
+		dpa_cls_dbg(("DEBUG: dpa_classifier %s (%d) <--\n", __func__,
+			__LINE__));
+
+		return import_hm_nodes_to_chain(phm_nodes,
 					pvlan_hm->num_nodes,
 					pvlan_hm);
-	} else { /* Create HM nodes */
+	}
+
+	hm_node = try_compatible_node(pvlan_hm);
+	if (hm_node == NULL) {
+		/* Create a header manip node for this insert: */
 		hm_node = kzalloc(sizeof(*hm_node), GFP_KERNEL);
 		if (!hm_node) {
-			log_err("Not enough memory for header manip nodes.\n");
+			log_err("No more memory for header manip nodes.\n");
 			return -ENOMEM;
 		}
 
 		INIT_LIST_HEAD(&hm_node->list_node);
-		pvlan_hm->hm_node[0]	= hm_node;
-
-		add_local_hm_nodes_to_chain(pvlan_hm);
 	}
 
+	pvlan_hm->hm_node[0] = hm_node;
+
+	add_local_hm_nodes_to_chain(pvlan_hm);
+
 	dpa_cls_dbg(("DEBUG: dpa_classifier %s (%d) <--\n", __func__,
 		__LINE__));
 
@@ -7057,22 +7105,30 @@ static int mpls_hm_prepare_nodes(struct dpa_cls_hm *pmpls_hm,
 	if (res) { /* Import HM nodes */
 		phm_nodes = &res->ins_rm_node;
 
-		err = import_hm_nodes_to_chain(phm_nodes,
+		dpa_cls_dbg(("DEBUG: dpa_classifier %s (%d) <--\n", __func__,
+			__LINE__));
+
+		return import_hm_nodes_to_chain(phm_nodes,
 					pmpls_hm->num_nodes,
 					pmpls_hm);
-	} else { /* Create HM nodes */
+	}
+
+	hm_node = try_compatible_node(pmpls_hm);
+	if (hm_node == NULL) {
+		/* Create a header manip node for this insert: */
 		hm_node = kzalloc(sizeof(*hm_node), GFP_KERNEL);
 		if (!hm_node) {
-			log_err("Not enough memory for header manip nodes.\n");
+			log_err("No more memory for header manip nodes.\n");
 			return -ENOMEM;
 		}
 
 		INIT_LIST_HEAD(&hm_node->list_node);
-		pmpls_hm->hm_node[0]	= hm_node;
-
-		add_local_hm_nodes_to_chain(pmpls_hm);
 	}
 
+	pmpls_hm->hm_node[0] = hm_node;
+
+	add_local_hm_nodes_to_chain(pmpls_hm);
+
 	dpa_cls_dbg(("DEBUG: dpa_classifier %s (%d) <--\n", __func__,
 		__LINE__));
 
diff --git a/drivers/staging/fsl_dpa_offload/dpa_classifier.h b/drivers/staging/fsl_dpa_offload/dpa_classifier.h
index 659e1b7..59ae335 100644
--- a/drivers/staging/fsl_dpa_offload/dpa_classifier.h
+++ b/drivers/staging/fsl_dpa_offload/dpa_classifier.h
@@ -508,14 +508,6 @@ static inline void key_apply_mask(const struct dpa_offload_lookup_key *key,
 	uint8_t *new_key);
 
 /*
- * Finds in a chain of low level header manipulation nodes a node which is
- * compatible with a specific operation, so that the node can be reused.
- */
-static struct dpa_cls_hm_node
-	*find_compatible_hm_node(enum dpa_cls_hm_node_type type,
-							struct list_head *list);
-
-/*
  * Import a set of low level header manipulation nodes into an existing
  * low level header manipulation nodes list (associated with a classifier
  * header manipulation op).
-- 
1.9.1

