From 46e5b9311cb1825fe50869914b8f31535d440186 Mon Sep 17 00:00:00 2001
From: Zhang Xiao <xiao.zhang@windriver.com>
Date: Thu, 13 Aug 2015 14:25:36 +0800
Subject: [PATCH 1203/1207] powerpc/e6500: hardware tablewalk bug fix on
 per-core spinlock

There is a per-core lock for TLB handlers but been mis-used thus
cause the hardware tablewalk failed.

The root cause of this issue comes from a mis-use of a pointer. That pointer
was used to point to a data structure. And this data structure was 4bytes aligned
so the lowest two bits of its address is always zero. That means the lowest two bits
of this point should always been zero. While in fact, these two bits are not
wasted. It was use as a lock for two threads of the same hard core. 1 is locked and
0 is un-locked. While when using it, some pieces of process ignored it. The pointer
was used directly to get the data structure and data in it. It is right when the lock
was not set but when the lock was set, it is not the address of the data structure.
Such issue will only effect on hardware tablewalk, and will overwrite an very very
important TLB entry: entry 0 of TLB1. This will cause the kernel hangs for ever.

Signed-off-by: Zhang Xiao <xiao.zhang@windriver.com>
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
---
 arch/powerpc/include/asm/mmu-book3e.h |  2 +-
 arch/powerpc/mm/tlb_low_64e.S         | 22 ++++++++++++++--------
 2 files changed, 15 insertions(+), 9 deletions(-)

diff --git a/arch/powerpc/include/asm/mmu-book3e.h b/arch/powerpc/include/asm/mmu-book3e.h
index b9c9912..d4d7b95 100644
--- a/arch/powerpc/include/asm/mmu-book3e.h
+++ b/arch/powerpc/include/asm/mmu-book3e.h
@@ -304,7 +304,7 @@ struct tlb_core_data {
 	/* For software way selection, as on Freescale TLB1 */
 	u8 esel_next, esel_max, esel_first;
 	u8 lrat_next, lrat_max;
-};
+} __attribute__((aligned(4)));
 
 #ifdef CONFIG_PPC64
 extern unsigned long linear_map_top;
diff --git a/arch/powerpc/mm/tlb_low_64e.S b/arch/powerpc/mm/tlb_low_64e.S
index f3ec653..727bf27 100644
--- a/arch/powerpc/mm/tlb_low_64e.S
+++ b/arch/powerpc/mm/tlb_low_64e.S
@@ -357,23 +357,27 @@ BEGIN_FTR_SECTION		/* CPU_FTR_SMT */
 	 *
 	 * MAS6:IND should be already set based on MAS4
 	 */
-1:	lbarx	r15,0,r11
+	mtocrf  0x01,r11
+	addi    r11,r11,TCD_LOCK-1 /* -1 to compensate for low bit set */
+	bf      31,1f           /* no lock if TLB_PER_CORE_HAS_LOCK clear */
+2:	lbarx	r15,0,r11
 	lhz	r10,PACAPACAINDEX(r13)
 	cmpdi	r15,0
 	cmpdi	cr1,r15,1	/* set cr1.eq = 0 for non-recursive */
 	addi	r10,r10,1
-	bne	2f
+	bne	3f
 	stbcx.	r10,0,r11
-	bne	1b
-3:
+	bne	2b
+4:
 	.subsection 1
-2:	cmpd	cr1,r15,r10	/* recursive lock due to mcheck/crit/etc? */
-	beq	cr1,3b		/* unlock will happen if cr1.eq = 0 */
+3:	cmpd	cr1,r15,r10	/* recursive lock due to mcheck/crit/etc? */
+	beq	cr1,4b		/* unlock will happen if cr1.eq = 0 */
 	lbz	r15,0(r11)
 	cmpdi	r15,0
-	bne	2b
-	b	1b
+	bne	3b
+	b	2b
 	.previous
+1:      subi    r11,r11,TCD_LOCK-1
 
 	/*
 	 * Erratum A-008139 says that we can't use tlbwe to change
@@ -383,6 +387,7 @@ BEGIN_FTR_SECTION		/* CPU_FTR_SMT */
 	 * with tlbilx before overwriting.
 	 */
 
+	rldicr  r11,r11,0,62
 	lbz	r15,TCD_ESEL_NEXT(r11)
 	rlwinm	r10,r15,16,0xff0000
 	oris	r10,r10,MAS0_TLBSEL(1)@h
@@ -495,6 +500,7 @@ BEGIN_FTR_SECTION
 	beq	cr1,1f		/* no unlock if lock was recursively grabbed */
 	li	r15,0
 	isync
+	rldicr  r11,r11,0,62
 	stb	r15,0(r11)
 1:
 END_FTR_SECTION_IFSET(CPU_FTR_SMT)
-- 
2.0.2

