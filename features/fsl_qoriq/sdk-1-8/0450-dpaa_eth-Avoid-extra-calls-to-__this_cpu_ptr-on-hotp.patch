From b6fa144fe1ee886c3c8a2cc3fd5cfae3f98cc46c Mon Sep 17 00:00:00 2001
From: "Lu.Jiang" <lu.jiang@windriver.com>
Date: Thu, 11 Sep 2014 17:09:18 +0800
Subject: [PATCH 450/987] dpaa_eth: Avoid extra calls to __this_cpu_ptr on
 hotpath

[Original patch taken from QorIQ-SDK-V1.6-SOURCE-20140619-yocto.iso]

A pointer to the percpu counter for buffers in the global Rx buffer
pool is acquired in several places along the driver's hotpath.
It's more efficient to obtain it just once and then pass it along
as an argument to subsequent functions.

Signed-off-by: Ioana Radulescu <ruxandra.radulescu@freescale.com>
Reviewed-on: http://git.am.freescale.net:8181/7091
Reviewed-by: Cristian-Constantin Sovaiala <Cristian.Sovaiala@freescale.com>
Reviewed-by: Thomas Trefny <Tom.Trefny@freescale.com>
Tested-by: Review Code-CDREVIEW <CDREVIEW@freescale.com>
Change-Id: I8aaff73892eead387f8c54b0593c5c58f5ab250a
Reviewed-on: http://git.am.freescale.net:8181/7353
Reviewed-by: Jose Rivera <German.Rivera@freescale.com>
Reviewed-on: http://git.am.freescale.net:8181/7686
Reviewed-by: Madalin-Cristian Bucur <madalin.bucur@freescale.com>
Tested-by: Madalin-Cristian Bucur <madalin.bucur@freescale.com>
Signed-off-by: Lu.Jiang <lu.jiang@windriver.com>
---
 drivers/net/ethernet/freescale/dpa/dpaa_eth.c    | 13 ++++++++---
 drivers/net/ethernet/freescale/dpa/dpaa_eth.h    |  5 +++--
 drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c | 28 ++++++++++--------------
 3 files changed, 25 insertions(+), 21 deletions(-)

diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth.c b/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
index 1abb677..6bccf37 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
@@ -321,16 +321,18 @@ priv_rx_error_dqrr(struct qman_portal		*portal,
 	struct net_device		*net_dev;
 	struct dpa_priv_s		*priv;
 	struct dpa_percpu_priv_s	*percpu_priv;
+	int				*count_ptr;
 
 	net_dev = ((struct dpa_fq *)fq)->net_dev;
 	priv = netdev_priv(net_dev);
 
 	percpu_priv = __this_cpu_ptr(priv->percpu_priv);
+	count_ptr = __this_cpu_ptr(priv->dpa_bp->percpu_count);
 
 	if (dpaa_eth_napi_schedule(percpu_priv, portal))
 		return qman_cb_dqrr_stop;
 
-	if (unlikely(dpaa_eth_refill_bpools(priv->dpa_bp)))
+	if (unlikely(dpaa_eth_refill_bpools(priv->dpa_bp, count_ptr)))
 		/* Unable to refill the buffer pool due to insufficient
 		 * system memory. Just release the frame back into the pool,
 		 * otherwise we'll soon end up with an empty buffer pool.
@@ -351,29 +353,34 @@ priv_rx_default_dqrr(struct qman_portal		*portal,
 	struct net_device		*net_dev;
 	struct dpa_priv_s		*priv;
 	struct dpa_percpu_priv_s	*percpu_priv;
+	int                             *count_ptr;
+	struct dpa_bp			*dpa_bp;
 
 	net_dev = ((struct dpa_fq *)fq)->net_dev;
 	priv = netdev_priv(net_dev);
+	dpa_bp = priv->dpa_bp;
 
 	/* Trace the Rx fd */
 	trace_dpa_rx_fd(net_dev, fq, &dq->fd);
 
 	/* IRQ handler, non-migratable; safe to use __this_cpu_ptr here */
 	percpu_priv = __this_cpu_ptr(priv->percpu_priv);
+	count_ptr = __this_cpu_ptr(dpa_bp->percpu_count);
 
 	if (unlikely(dpaa_eth_napi_schedule(percpu_priv, portal)))
 		return qman_cb_dqrr_stop;
 
 	/* Vale of plenty: make sure we didn't run out of buffers */
 
-	if (unlikely(dpaa_eth_refill_bpools(priv->dpa_bp)))
+	if (unlikely(dpaa_eth_refill_bpools(dpa_bp, count_ptr)))
 		/* Unable to refill the buffer pool due to insufficient
 		 * system memory. Just release the frame back into the pool,
 		 * otherwise we'll soon end up with an empty buffer pool.
 		 */
 		dpa_fd_release(net_dev, &dq->fd);
 	else
-		_dpa_rx(net_dev, portal, priv, percpu_priv, &dq->fd, fq->fqid);
+		_dpa_rx(net_dev, portal, priv, percpu_priv, &dq->fd, fq->fqid,
+			count_ptr);
 
 	return qman_cb_dqrr_consume;
 }
diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth.h b/drivers/net/ethernet/freescale/dpa/dpaa_eth.h
index 64cee7f..9d50347 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth.h
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth.h
@@ -417,13 +417,14 @@ struct fm_port_fqs {
 
 /* functions with different implementation for SG and non-SG: */
 int dpa_bp_priv_seed(struct dpa_bp *dpa_bp);
-int dpaa_eth_refill_bpools(struct dpa_bp *dpa_bp);
+int dpaa_eth_refill_bpools(struct dpa_bp *dpa_bp, int *count_ptr);
 void __hot _dpa_rx(struct net_device *net_dev,
 		struct qman_portal *portal,
 		const struct dpa_priv_s *priv,
 		struct dpa_percpu_priv_s *percpu_priv,
 		const struct qm_fd *fd,
-		u32 fqid);
+		u32 fqid,
+		int *count_ptr);
 int __hot dpa_tx(struct sk_buff *skb, struct net_device *net_dev);
 struct sk_buff *_dpa_cleanup_tx_fd(const struct dpa_priv_s *priv,
 				   const struct qm_fd *fd);
diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c b/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c
index 60bf3e0..b582937 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c
@@ -68,13 +68,12 @@
  * @vaddr fragment must have been allocated with netdev_alloc_frag(),
  * specifically for fitting into @dpa_bp.
  */
-static void dpa_bp_recycle_frag(struct dpa_bp *dpa_bp, unsigned long vaddr)
+static void dpa_bp_recycle_frag(struct dpa_bp *dpa_bp, unsigned long vaddr,
+				int *count_ptr)
 {
 	struct bm_buffer bmb;
-	int *count_ptr;
 	dma_addr_t addr;
 
-	count_ptr = __this_cpu_ptr(dpa_bp->percpu_count);
 	addr = dma_map_single(dpa_bp->dev, (void *)vaddr, dpa_bp->size,
 			      DMA_BIDIRECTIONAL);
 	if (unlikely(dma_mapping_error(dpa_bp->dev, addr))) {
@@ -185,9 +184,8 @@ int dpa_bp_priv_seed(struct dpa_bp *dpa_bp)
 /* Add buffers/(pages) for Rx processing whenever bpool count falls below
  * REFILL_THRESHOLD.
  */
-int dpaa_eth_refill_bpools(struct dpa_bp *dpa_bp)
+int dpaa_eth_refill_bpools(struct dpa_bp *dpa_bp, int *countptr)
 {
-	int *countptr = __this_cpu_ptr(dpa_bp->percpu_count);
 	int count = *countptr;
 	int new_bufs;
 
@@ -418,7 +416,8 @@ static struct sk_buff *__hot contig_fd_to_skb(const struct dpa_priv_s *priv,
  * The page fragment holding the S/G Table is recycled here.
  */
 static struct sk_buff *__hot sg_fd_to_skb(const struct dpa_priv_s *priv,
-			       const struct qm_fd *fd, int *use_gro)
+			       const struct qm_fd *fd, int *use_gro,
+			       int *count_ptr)
 {
 	const struct qm_sg_entry *sgt;
 	dma_addr_t addr = qm_fd_addr(fd);
@@ -432,7 +431,6 @@ static struct sk_buff *__hot sg_fd_to_skb(const struct dpa_priv_s *priv,
 	int i;
 	const fm_prs_result_t *parse_results;
 	struct sk_buff *skb = NULL, *skb_tmp, **skbh;
-	int *count_ptr;
 
 	vaddr = phys_to_virt(addr);
 	DPA_BUG_ON(!IS_ALIGNED((unsigned long)vaddr, SMP_CACHE_BYTES));
@@ -446,7 +444,6 @@ static struct sk_buff *__hot sg_fd_to_skb(const struct dpa_priv_s *priv,
 
 		/* We use a single global Rx pool */
 		DPA_BUG_ON(dpa_bp != dpa_bpid2pool(sgt[i].bpid));
-		count_ptr = __this_cpu_ptr(dpa_bp->percpu_count);
 
 		sg_addr = qm_sg_addr(&sgt[i]);
 		sg_vaddr = phys_to_virt(sg_addr);
@@ -528,7 +525,7 @@ static struct sk_buff *__hot sg_fd_to_skb(const struct dpa_priv_s *priv,
 
 	/* recycle the SGT fragment */
 	DPA_BUG_ON(dpa_bp != dpa_bpid2pool(fd->bpid));
-	dpa_bp_recycle_frag(dpa_bp, (unsigned long)vaddr);
+	dpa_bp_recycle_frag(dpa_bp, (unsigned long)vaddr, count_ptr);
 	return skb;
 }
 
@@ -537,7 +534,8 @@ void __hot _dpa_rx(struct net_device *net_dev,
 		const struct dpa_priv_s *priv,
 		struct dpa_percpu_priv_s *percpu_priv,
 		const struct qm_fd *fd,
-		u32 fqid)
+		u32 fqid,
+		int *count_ptr)
 {
 	struct dpa_bp *dpa_bp;
 	struct sk_buff *skb;
@@ -546,7 +544,6 @@ void __hot _dpa_rx(struct net_device *net_dev,
 	unsigned int skb_len;
 	struct rtnl_link_stats64 *percpu_stats = &percpu_priv->stats;
 	int use_gro = net_dev->features & NETIF_F_GRO;
-	int *count_ptr;
 
 	if (unlikely(fd_status & FM_FD_STAT_RX_ERRORS) != 0) {
 		if (netif_msg_hw(priv) && net_ratelimit())
@@ -559,7 +556,6 @@ void __hot _dpa_rx(struct net_device *net_dev,
 
 	dpa_bp = priv->dpa_bp;
 	DPA_BUG_ON(dpa_bp != dpa_bpid2pool(fd->bpid));
-	count_ptr = __this_cpu_ptr(dpa_bp->percpu_count);
 
 	/* prefetch the first 64 bytes of the frame or the SGT start */
 	dma_unmap_single(dpa_bp->dev, addr, dpa_bp->size, DMA_BIDIRECTIONAL);
@@ -578,7 +574,7 @@ void __hot _dpa_rx(struct net_device *net_dev,
 		}
 		skb = contig_fd_to_skb(priv, fd, &use_gro);
 	} else
-		skb = sg_fd_to_skb(priv, fd, &use_gro);
+		skb = sg_fd_to_skb(priv, fd, &use_gro, count_ptr);
 
 	/* Account for either the contig buffer or the SGT buffer (depending on
 	 * which case we were in) having been removed from the pool.
@@ -626,7 +622,8 @@ _release_frame:
 }
 
 static int __hot skb_to_contig_fd(struct dpa_priv_s *priv,
-				  struct sk_buff *skb, struct qm_fd *fd)
+				  struct sk_buff *skb, struct qm_fd *fd,
+				  int *count_ptr)
 {
 	struct sk_buff **skbh;
 	dma_addr_t addr;
@@ -637,7 +634,6 @@ static int __hot skb_to_contig_fd(struct dpa_priv_s *priv,
 	unsigned char *buffer_start;
 
 #if (!defined(CONFIG_FSL_DPAA_TS) && !defined(CONFIG_FSL_DPAA_1588))
-	int *count_ptr = __this_cpu_ptr(dpa_bp->percpu_count);
 
 	/* Check recycling conditions; only if timestamp support is not
 	 * enabled, otherwise we need the fd back on tx confirmation
@@ -912,7 +908,7 @@ int __hot dpa_tx(struct sk_buff *skb, struct net_device *net_dev)
 			goto enomem;
 
 		/* Finally, create a contig FD from this skb */
-		err = skb_to_contig_fd(priv, skb, &fd);
+		err = skb_to_contig_fd(priv, skb, &fd, countptr);
 	}
 	if (unlikely(err < 0))
 		goto skb_to_fd_failed;
-- 
1.9.1

