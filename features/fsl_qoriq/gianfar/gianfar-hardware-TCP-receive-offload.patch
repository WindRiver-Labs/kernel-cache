From d1725c7b3157d3c84d34f999299e32275c140d59 Mon Sep 17 00:00:00 2001
From: Claudiu Manoil <claudiu.manoil@freescale.com>
Date: Mon, 2 Apr 2012 16:47:28 +0000
Subject: [PATCH 08/27] gianfar: hardware TCP receive offload

Insert TCP stream rules into eTSEC filer table for offloading TCP receive.
eTSEC will accelerate TCP receive processing for a given stream as soon as
its data volume exceeds 1MB.

Signed-off-by: Jiajun Wu <b06378@freescale.com>
Signed-off-by: Claudiu Manoil <claudiu.manoil@freescale.com>
[Kevin: Original patch taken from FSL
QorIQ-SDK-V1.2-SOURCE-20120614-yocto.iso image. Just some minor context
mods in order to port to 3.4 kernel.]
Signed-off-by: Kevin Hao <kexin.hao@windriver.com>
Signed-off-by: Tiejun Chen <tiejun.chen@windriver.com>
---
 drivers/net/ethernet/freescale/Kconfig   |   10 ++
 drivers/net/ethernet/freescale/gianfar.c |  253 ++++++++++++++++++++++++++++++
 drivers/net/ethernet/freescale/gianfar.h |   23 +++
 include/linux/skbuff.h                   |    3 +
 include/net/sock.h                       |    4 +
 net/core/sock.c                          |   16 ++
 net/ipv4/tcp_ipv4.c                      |   25 +++
 7 files changed, 334 insertions(+)

diff --git a/drivers/net/ethernet/freescale/Kconfig b/drivers/net/ethernet/freescale/Kconfig
index d756a16..ef6518d 100644
--- a/drivers/net/ethernet/freescale/Kconfig
+++ b/drivers/net/ethernet/freescale/Kconfig
@@ -64,6 +64,16 @@ config FSL_PQ_MDIO
 
 source "drivers/net/ethernet/freescale/dpa/NetCommSw/Kconfig"
 
+config GFAR_HW_TCP_RECEIVE_OFFLOAD
+	default n
+	bool "Hardware TCP receive offload"
+	depends on GIANFAR && EXPERIMENTAL && !BONDING
+	help
+	  Hardware offload receive large TCP data.
+
+	  Inserts TCP stream rules into eTSEC filer table,
+	  to offload TCP receive.
+
 config UCC_GETH
 	tristate "Freescale QE Gigabit Ethernet"
 	depends on QUICC_ENGINE
diff --git a/drivers/net/ethernet/freescale/gianfar.c b/drivers/net/ethernet/freescale/gianfar.c
index 7c2f174..543c312 100644
--- a/drivers/net/ethernet/freescale/gianfar.c
+++ b/drivers/net/ethernet/freescale/gianfar.c
@@ -107,6 +107,9 @@
 #ifdef CONFIG_GIANFAR_L2SRAM
 #include <asm/fsl_85xx_cache_sram.h>
 #endif
+#ifdef CONFIG_GFAR_HW_TCP_RECEIVE_OFFLOAD
+#include <net/tcp.h>
+#endif
 
 #define TX_TIMEOUT      (1*HZ)
 
@@ -398,6 +401,11 @@ static void gfar_init_mac(struct net_device *ndev)
 	if (priv->rx_filer_enable) {
 		rctrl |= RCTRL_FILREN;
 		/* Program the RIR0 reg with the required distribution */
+#ifdef CONFIG_GFAR_HW_TCP_RECEIVE_OFFLOAD
+		if (priv->hw_tcp.en)
+			gfar_write(&regs->rir0, TWO_QUEUE_RIR0);
+		else
+#endif
 		gfar_write(&regs->rir0, DEFAULT_RIR0);
 	}
 
@@ -918,6 +926,155 @@ static unsigned int reverse_bitmap(unsigned int bit_map, unsigned int max_qs)
 	return new_bit_map;
 }
 
+#ifdef CONFIG_GFAR_HW_TCP_RECEIVE_OFFLOAD
+void gfar_setup_hwaccel_tcp4_receive(struct sock *sk, struct sk_buff *skb)
+{
+	u32 rqfcr, rqfpr, rqidx;
+	int i;
+	struct tcphdr *th;
+	struct iphdr *iph;
+	struct gfar_private *priv = netdev_priv(skb->gfar_dev);
+	struct gfar_hw_tcp_rcv_handle *hw_tcp = &priv->hw_tcp;
+
+	if (!hw_tcp->en)
+		return;
+
+	i = hw_tcp->empty_chan_idx;
+	hw_tcp->chan[i] = sk;
+	/* keep the reference to this "channel" for sk_free() */
+	sk->hw_tcp_chan_ref = &(hw_tcp->chan[i]);
+
+	/* convert channel index to filer table index (4 entries per chan) */
+	i = hw_tcp->filer_idx + (hw_tcp->empty_chan_idx * 4);
+
+	/* setup the hw tcp channel */
+	th = tcp_hdr(skb);
+	iph = ip_hdr(skb);
+	/* setup IPv4 source address */
+	rqfcr = RQFCR_CMP_EXACT | RQFCR_PID_SIA | RQFCR_AND;
+	rqfpr = ntohl(iph->saddr);
+	priv->ftp_rqfcr[i] = rqfcr;
+	priv->ftp_rqfpr[i] = rqfpr;
+	gfar_write_filer(priv, i, rqfcr, rqfpr);
+	i++;
+	/* setup IPv4 destination address */
+	rqfcr = RQFCR_CMP_EXACT | RQFCR_PID_DIA | RQFCR_AND;
+	rqfpr = ntohl(iph->daddr);
+	priv->ftp_rqfcr[i] = rqfcr;
+	priv->ftp_rqfpr[i] = rqfpr;
+	gfar_write_filer(priv, i, rqfcr, rqfpr);
+	i++;
+	/* setup TCP source port */
+	rqfcr = RQFCR_CMP_EXACT | RQFCR_PID_SPT | RQFCR_AND;
+	rqfpr = ntohs(th->source);
+	priv->ftp_rqfcr[i] = rqfcr;
+	priv->ftp_rqfpr[i] = rqfpr;
+	gfar_write_filer(priv, i, rqfcr, rqfpr);
+	i++;
+	/* setup TCP destination port */
+	rqidx = (GFAR_TCP_START_Q_IDX + hw_tcp->empty_chan_idx); /* set Q */
+	rqfcr = RQFCR_CMP_EXACT | RQFCR_PID_DPT | (rqidx << 10);
+	rqfpr = ntohs(th->dest);
+	priv->ftp_rqfcr[i] = rqfcr;
+	priv->ftp_rqfpr[i] = rqfpr;
+	gfar_write_filer(priv, i, rqfcr, rqfpr);
+
+	/* "round-robin" to the next empty hw tcp channel */
+	i = (hw_tcp->empty_chan_idx + 1) % hw_tcp->chan_cnt;
+	while (hw_tcp->chan[i] && (i != hw_tcp->empty_chan_idx))
+		i = (i + 1) % hw_tcp->chan_cnt;
+	/* if none found then take the next in line (and empty it) */
+	if (i == hw_tcp->empty_chan_idx)
+		i = (i + 1) % hw_tcp->chan_cnt;
+
+	/* update the empty chan idx for the next hwaccel setup call */
+	hw_tcp->empty_chan_idx = i;
+
+	/* clean up the next in line tcp channel, if necessary */
+	if (hw_tcp->chan[i]) {
+		/* remove referece from corresp. sk to this "channel" */
+		hw_tcp->chan[i]->hw_tcp_chan_ref = NULL;
+		hw_tcp->chan[i] = NULL;
+
+		/* convert channel index to filer table index */
+		i = hw_tcp->filer_idx + (i * 4);
+
+		/* clear the corresp. table entries */
+		rqfcr = RQFCR_CMP_NOMATCH;
+		rqfpr = FPR_FILER_MASK;
+		priv->ftp_rqfcr[i] = rqfcr;
+		priv->ftp_rqfpr[i] = rqfpr;
+		gfar_write_filer(priv, i, rqfcr, rqfpr);
+		i++;
+		priv->ftp_rqfcr[i] = rqfcr;
+		priv->ftp_rqfpr[i] = rqfpr;
+		gfar_write_filer(priv, i, rqfcr, rqfpr);
+		i++;
+		priv->ftp_rqfcr[i] = rqfcr;
+		priv->ftp_rqfpr[i] = rqfpr;
+		gfar_write_filer(priv, i, rqfcr, rqfpr);
+		i++;
+		priv->ftp_rqfcr[i] = rqfcr;
+		priv->ftp_rqfpr[i] = rqfpr;
+		gfar_write_filer(priv, i, rqfcr, rqfpr);
+	}
+}
+
+static u32 gfar_init_hw_tcp_cluster(struct gfar_private *priv, u32 rqfar)
+{
+	u32 rqfcr, rqfpr;
+	int i, j;
+
+	if (!priv->hw_tcp.en)
+		return rqfar;
+	/* 4 entries per channel, plus extra 4 for guard rule and clustering */
+	i = rqfar - 4 * (priv->hw_tcp.chan_cnt + 1);
+	if (i < 0)
+		BUG();
+
+	printk(KERN_INFO "%s: enabled hardware TCP receive offload\n",
+			priv->ndev->name);
+
+	rqfcr = RQFCR_CMP_EXACT | RQFCR_PID_MASK | RQFCR_AND;
+	rqfpr = RQFPR_IPV4 | RQFPR_TCP;
+	priv->ftp_rqfcr[i] = rqfcr;
+	priv->ftp_rqfpr[i] = rqfpr;
+	gfar_write_filer(priv, i, rqfcr, rqfpr);
+	i++;
+	rqfcr = RQFCR_CMP_EXACT | RQFCR_PID_PARSE | RQFCR_AND;
+	rqfpr = RQFPR_IPV4 | RQFPR_TCP;
+	priv->ftp_rqfcr[i] = rqfcr;
+	priv->ftp_rqfpr[i] = rqfpr;
+	gfar_write_filer(priv, i, rqfcr, rqfpr);
+	i++;
+	rqfcr = RQFCR_CMP_EXACT | RQFCR_PID_MASK | RQFCR_CLE | RQFCR_AND;
+	rqfpr = FPR_FILER_MASK;
+	priv->ftp_rqfcr[i] = rqfcr;
+	priv->ftp_rqfpr[i] = rqfpr;
+	gfar_write_filer(priv, i, rqfcr, rqfpr);
+	i++;
+	/* hold idx of the first channel's 1st entry */
+	priv->hw_tcp.filer_idx = i;
+
+	rqfcr = RQFCR_CMP_NOMATCH;
+	rqfpr = FPR_FILER_MASK;
+	for (j = 0; j < (priv->hw_tcp.chan_cnt * 4); j++) {
+		priv->ftp_rqfcr[i] = rqfcr;
+		priv->ftp_rqfpr[i] = rqfpr;
+		gfar_write_filer(priv, i, rqfcr, rqfpr);
+		i++;
+	}
+
+	rqfpr = FPR_FILER_MASK;
+	rqfcr = RQFCR_CMP_NOMATCH | RQFCR_CLE;
+	priv->ftp_rqfcr[i] = rqfcr;
+	priv->ftp_rqfpr[i] = rqfpr;
+	gfar_write_filer(priv, i, rqfcr, rqfpr);
+
+	return rqfar - 4 * (priv->hw_tcp.chan_cnt + 1);
+}
+#endif
+
 static u32 cluster_entry_per_class(struct gfar_private *priv, u32 rqfar,
 				   u32 class)
 {
@@ -973,6 +1130,9 @@ static void gfar_init_filer_table(struct gfar_private *priv)
 	rqfar = cluster_entry_per_class(priv, rqfar, RQFPR_IPV4 | RQFPR_UDP);
 	rqfar = cluster_entry_per_class(priv, rqfar, RQFPR_IPV4 | RQFPR_TCP);
 
+#ifdef CONFIG_GFAR_HW_TCP_RECEIVE_OFFLOAD
+	rqfar = gfar_init_hw_tcp_cluster(priv, rqfar);
+#endif
 	/* cur_filer_idx indicated the first non-masked rule */
 	priv->cur_filer_idx = rqfar;
 
@@ -1247,6 +1407,21 @@ static int gfar_probe(struct platform_device *ofdev)
 			strcpy(priv->gfargrp[i].int_name_tx, dev->name);
 	}
 
+#ifdef CONFIG_GFAR_HW_TCP_RECEIVE_OFFLOAD
+	/* set the number of hw_tcp channels */
+	priv->hw_tcp.chan_cnt = (priv->num_rx_queues > GFAR_TCP_START_Q_IDX) \
+				? (priv->num_rx_queues - GFAR_TCP_START_Q_IDX) \
+				: 0;
+	priv->hw_tcp.en = 1;
+	/* we need at least 2 hw tcp channels for this feature */
+	if (priv->hw_tcp.chan_cnt < 2 ||
+		!(priv->ndev->features & NETIF_F_RXCSUM))
+		priv->hw_tcp.en = 0;
+	/* not a good idea to activate this feature if this gfar instance
+	 * does not support it */
+	WARN_ON(!priv->hw_tcp.en);
+#endif
+
 	/* Initialize the filer table */
 	gfar_init_filer_table(priv);
 
@@ -3291,6 +3466,78 @@ static int gfar_process_frame(struct net_device *dev, struct sk_buff *skb,
 	return 0;
 }
 
+#ifdef CONFIG_GFAR_HW_TCP_RECEIVE_OFFLOAD
+static inline void gfar_hwaccel_tcp4_receive(struct gfar_private *priv,
+					     struct gfar_priv_rx_q *rx_queue,
+					     struct sk_buff *skb)
+{
+	const struct tcphdr *th;
+	const struct iphdr *iph;
+	int p_len;
+	int ph_len;
+	struct rxfcb *fcb;
+	struct sock *gfar_sk;
+	int tcp_chan_idx = rx_queue->qindex - GFAR_TCP_START_Q_IDX;
+
+	/*
+	 * mark this skb to be checked by the gfar hw tcp rcv setup code
+	 * "hooked" inside tcp_v4_do_rcv()
+	 */
+	skb->gfar_dev = priv->ndev;
+	if ((tcp_chan_idx < 0) || !priv->hw_tcp.chan[tcp_chan_idx]) {
+		gfar_process_frame(priv->ndev, skb, GMAC_FCB_LEN);
+		return;
+	}
+
+	gfar_sk = priv->hw_tcp.chan[tcp_chan_idx];
+
+	fcb = (struct rxfcb *)skb->data;
+	gfar_rx_checksum(skb, fcb);
+
+	skb->pkt_type = PACKET_HOST;
+	/* set IPv4 header */
+	skb->network_header = skb->data + GMAC_FCB_LEN \
+				+ ETH_HLEN + priv->padding;
+	iph = ip_hdr(skb);
+
+	if (iph->ihl > 5 || (iph->frag_off & htons(IP_MF | IP_OFFSET)) ||
+		(gfar_sk->sk_state != TCP_ESTABLISHED)) {
+		gfar_process_frame(priv->ndev, skb, GMAC_FCB_LEN);
+		return;
+	}
+
+	ph_len = iph->ihl * 4; /* IPv4 header length, in bytes */
+	p_len = ntohs(iph->tot_len); /* total length, in bytes */
+
+	if (p_len <  (skb->len - GMAC_FCB_LEN - ETH_HLEN)) {
+		skb->tail -= (skb->len - GMAC_FCB_LEN - ETH_HLEN - p_len);
+		skb->len = p_len - ph_len;
+	} else
+		skb->len = skb->len - (GMAC_FCB_LEN + ETH_HLEN + ph_len);
+
+	/*set TCP header*/
+	skb->transport_header = skb->network_header + ph_len;
+	skb->data = skb->transport_header;
+	th = tcp_hdr(skb);
+	TCP_SKB_CB(skb)->seq = ntohl(th->seq);
+	TCP_SKB_CB(skb)->end_seq = (TCP_SKB_CB(skb)->seq + th->syn + th->fin +
+					skb->len - (th->doff * 4));
+	TCP_SKB_CB(skb)->ack_seq = ntohl(th->ack_seq);
+	TCP_SKB_CB(skb)->when	 = 0;
+	TCP_SKB_CB(skb)->sacked	 = 0;
+
+	bh_lock_sock(gfar_sk);
+	if (!sock_owned_by_user(gfar_sk)) {
+		if (tcp_rcv_established(gfar_sk, skb, tcp_hdr(skb), skb->len)) {
+			tcp_v4_send_reset(gfar_sk, skb);
+			kfree_skb(skb);
+		}
+	} else
+		sk_add_backlog(gfar_sk, skb);
+	bh_unlock_sock(gfar_sk);
+}
+#endif
+
 /* gfar_clean_rx_ring() -- Processes each frame in the rx ring
  *   until the budget/quota has been reached. Returns the number
  *   of frames handled
@@ -3349,6 +3596,12 @@ int gfar_clean_rx_ring(struct gfar_priv_rx_q *rx_queue, int rx_work_limit)
 				skb_put(skb, pkt_len);
 				rx_queue->stats.rx_bytes += pkt_len;
 				skb_record_rx_queue(skb, rx_queue->qindex);
+#ifdef CONFIG_GFAR_HW_TCP_RECEIVE_OFFLOAD
+				if (likely(priv->hw_tcp.en))
+					gfar_hwaccel_tcp4_receive
+							(priv, rx_queue, skb);
+				else
+#endif
 				gfar_process_frame(dev, skb, amount_pull);
 
 			} else {
diff --git a/drivers/net/ethernet/freescale/gianfar.h b/drivers/net/ethernet/freescale/gianfar.h
index 8ca4df4..371cd80 100644
--- a/drivers/net/ethernet/freescale/gianfar.h
+++ b/drivers/net/ethernet/freescale/gianfar.h
@@ -431,6 +431,8 @@ extern const char gfar_driver_version[];
 /* This default RIR value directly corresponds
  * to the 3-bit hash value generated */
 #define DEFAULT_RIR0	0x05397700
+/* Map even hash values to Q0, and odd ones to Q1 */
+#define TWO_QUEUE_RIR0	0x04104100
 
 /* RQFCR register bits */
 #define RQFCR_GPI		0x80000000
@@ -553,6 +555,12 @@ extern const char gfar_driver_version[];
 
 #define GFAR_INT_NAME_MAX	(IFNAMSIZ + 6)	/* '_g#_xx' */
 
+#ifdef CONFIG_GFAR_HW_TCP_RECEIVE_OFFLOAD
+#define GFAR_TCP_START_Q_IDX	2 /* First RXQ to host a HW TCP channel */
+/* MAX #of HW TCP channels */
+#define GFAR_TCP_CHAN_MAXCNT	(MAX_RX_QS - GFAR_TCP_START_Q_IDX)
+#endif
+
 struct txbd8
 {
 	union {
@@ -1107,6 +1115,18 @@ struct gfar_recycle_cntxt_percpu {
 	unsigned int free_count;
 };
 
+#ifdef CONFIG_GFAR_HW_TCP_RECEIVE_OFFLOAD
+struct gfar_hw_tcp_rcv_handle {
+	bool en; /* enable feature on the current interface or not */
+	u8 chan_cnt; /* num_rx_qs-TCP_START_Q_IDX or 0 */
+	u8 filer_idx; /* filer table index to the 1st entry/1st tcp channel */
+	u8 empty_chan_idx; /* index of the next empty channel */
+	struct sock *chan[GFAR_TCP_CHAN_MAXCNT]; /* channel <-> sk mapping */
+};
+
+extern void tcp_v4_send_reset(struct sock *sk, struct sk_buff *skb);
+#endif
+
 /* Struct stolen almost completely (and shamelessly) from the FCC enet source
  * (Ok, that's not so true anymore, but there is a family resemblance)
  * The GFAR buffer descriptors track the ring buffers.  The rx_bd_base
@@ -1196,6 +1216,9 @@ struct gfar_private {
 	int hwts_rx_en;
 	int hwts_tx_en;
 
+#ifdef CONFIG_GFAR_HW_TCP_RECEIVE_OFFLOAD
+	struct gfar_hw_tcp_rcv_handle hw_tcp;
+#endif
 	/*Filer table*/
 	unsigned int ftp_rqfpr[MAX_FILER_IDX + 1];
 	unsigned int ftp_rqfcr[MAX_FILER_IDX + 1];
diff --git a/include/linux/skbuff.h b/include/linux/skbuff.h
index c1bae8d..303b972 100644
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@ -394,6 +394,9 @@ struct sk_buff {
 
 	struct sock		*sk;
 	struct net_device	*dev;
+#ifdef CONFIG_GFAR_HW_TCP_RECEIVE_OFFLOAD
+	struct net_device	*gfar_dev;
+#endif
 
 	/*
 	 * This is the control buffer. It is free to use for every
diff --git a/include/net/sock.h b/include/net/sock.h
index 5a0a58a..7d744f9 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -315,6 +315,10 @@ struct sock {
 #ifdef CONFIG_XFRM
 	struct xfrm_policy	*sk_policy[2];
 #endif
+#ifdef CONFIG_GFAR_HW_TCP_RECEIVE_OFFLOAD
+	struct sock		**hw_tcp_chan_ref;
+	u32			init_seq;
+#endif
 	unsigned long 		sk_flags;
 	struct dst_entry	*sk_dst_cache;
 	spinlock_t		sk_dst_lock;
diff --git a/net/core/sock.c b/net/core/sock.c
index 0f8402e..ba18092 100644
--- a/net/core/sock.c
+++ b/net/core/sock.c
@@ -1250,6 +1250,14 @@ static void __sk_free(struct sock *sk)
 		printk(KERN_DEBUG "%s: optmem leakage (%d bytes) detected.\n",
 		       __func__, atomic_read(&sk->sk_omem_alloc));
 
+#ifdef CONFIG_GFAR_HW_TCP_RECEIVE_OFFLOAD
+	if (sk->hw_tcp_chan_ref) {
+		/* clear the reference held by gfar driver to this sk */
+		*sk->hw_tcp_chan_ref = NULL;
+		sk->hw_tcp_chan_ref = NULL;
+	}
+#endif
+
 	if (sk->sk_peer_cred)
 		put_cred(sk->sk_peer_cred);
 	put_pid(sk->sk_peer_pid);
@@ -1343,6 +1351,10 @@ struct sock *sk_clone_lock(const struct sock *sk, const gfp_t priority)
 		newsk->sk_forward_alloc = 0;
 		newsk->sk_send_head	= NULL;
 		newsk->sk_userlocks	= sk->sk_userlocks & ~SOCK_BINDPORT_LOCK;
+#ifdef CONFIG_GFAR_HW_TCP_RECEIVE_OFFLOAD
+		newsk->init_seq		= sk->init_seq;
+		newsk->hw_tcp_chan_ref	= NULL;
+#endif
 
 		sock_reset_flag(newsk, SOCK_DONE);
 		skb_queue_head_init(&newsk->sk_error_queue);
@@ -2122,6 +2134,10 @@ void sock_init_data(struct socket *sock, struct sock *sk)
 	sk->sk_rcvlowat		=	1;
 	sk->sk_rcvtimeo		=	MAX_SCHEDULE_TIMEOUT;
 	sk->sk_sndtimeo		=	MAX_SCHEDULE_TIMEOUT;
+#ifdef CONFIG_GFAR_HW_TCP_RECEIVE_OFFLOAD
+	sk->hw_tcp_chan_ref	=	NULL;
+	sk->init_seq		=	0;
+#endif
 
 	sk->sk_stamp = ktime_set(-1L, 0);
 
diff --git a/net/ipv4/tcp_ipv4.c b/net/ipv4/tcp_ipv4.c
index c1c937f..be75a24 100644
--- a/net/ipv4/tcp_ipv4.c
+++ b/net/ipv4/tcp_ipv4.c
@@ -585,7 +585,11 @@ int tcp_v4_gso_send_check(struct sk_buff *skb)
  *	Exception: precedence violation. We do not implement it in any case.
  */
 
+#ifdef CONFIG_GFAR_HW_TCP_RECEIVE_OFFLOAD
+void tcp_v4_send_reset(struct sock *sk, struct sk_buff *skb)
+#else
 static void tcp_v4_send_reset(struct sock *sk, struct sk_buff *skb)
+#endif
 {
 	const struct tcphdr *th = tcp_hdr(skb);
 	struct {
@@ -1587,6 +1591,16 @@ static __sum16 tcp_v4_checksum_init(struct sk_buff *skb)
 }
 
 
+#ifdef CONFIG_GFAR_HW_TCP_RECEIVE_OFFLOAD
+#define GFAR_TCP_HWACCEL_THR	(1024*1024) /*1M*/
+extern void gfar_setup_hwaccel_tcp4_receive(struct sock *sk,
+					    struct sk_buff *skb);
+static inline bool gfar_tcp_hwaccel_thr(struct sock *sk, struct sk_buff *skb)
+{
+	return (TCP_SKB_CB(skb)->seq > (sk->init_seq + GFAR_TCP_HWACCEL_THR));
+}
+EXPORT_SYMBOL(tcp_v4_send_reset);
+#endif
 /* The socket must have it's spinlock held when we get
  * here.
  *
@@ -1614,6 +1628,12 @@ int tcp_v4_do_rcv(struct sock *sk, struct sk_buff *skb)
 
 	if (sk->sk_state == TCP_ESTABLISHED) { /* Fast path */
 		sock_rps_save_rxhash(sk, skb);
+
+#ifdef CONFIG_GFAR_HW_TCP_RECEIVE_OFFLOAD
+		if (skb->gfar_dev && !sk->hw_tcp_chan_ref && !sk->sk_filter &&
+			gfar_tcp_hwaccel_thr(sk, skb))
+			gfar_setup_hwaccel_tcp4_receive(sk, skb);
+#endif
 		if (tcp_rcv_established(sk, skb, tcp_hdr(skb), skb->len)) {
 			rsk = sk;
 			goto reset;
@@ -1712,6 +1732,11 @@ int tcp_v4_rcv(struct sk_buff *skb)
 	if (!sk)
 		goto no_tcp_socket;
 
+#ifdef CONFIG_GFAR_HW_TCP_RECEIVE_OFFLOAD
+	if (th->syn)
+		sk->init_seq = ntohl(th->seq);
+#endif
+
 process:
 	if (sk->sk_state == TCP_TIME_WAIT)
 		goto do_time_wait;
-- 
1.7.9.7

