From 770fd90eea4a31a6b4ba83cf72080351fddc23e8 Mon Sep 17 00:00:00 2001
From: Tiejun Chen <tiejun.chen@windriver.com>
Date: Wed, 12 Sep 2012 15:24:00 +0800
Subject: [PATCH 11/27] gianfar: Optimization for routed packets by exchanging
 rx/tx buffers

Patch enhances the Gianfar driver performance for the packets that are
received by the driver and transmitted by the driver in the same context.
Performance is enhanced by exchanging the skbs between the Tx and Rx rings,
rather than allocating new skbs from kernel or taking it from recycle queue.
The cleanup of BDs is done by the xmit function, so there is no need of the
Tx interrupts, or Tx cleanup functionality in the receive path, avoiding the
locks for cleanups

Patch gives best performance with DEFAULT_RX_RING_SIZE 32, DEFAULT_TX_RING_SIZE 32,
and GFAR_DEV_WEIGHT 32

Signed-off-by: Rajan Gupta <rajan.gupta@freescale.com>
[Kevin: Original patch taken from FSL
QorIQ-SDK-V1.2-SOURCE-20120614-yocto.iso image]
Signed-off-by: Kevin Hao <kexin.hao@windriver.com>
Signed-off-by: Tiejun Chen <tiejun.chen@windriver.com>
---
 drivers/net/ethernet/freescale/Kconfig   |    8 +++
 drivers/net/ethernet/freescale/gianfar.c |  108 ++++++++++++++++++++++++++++++
 include/linux/skbuff.h                   |    4 ++
 3 files changed, 120 insertions(+)

diff --git a/drivers/net/ethernet/freescale/Kconfig b/drivers/net/ethernet/freescale/Kconfig
index ef6518d..6bb8f63 100644
--- a/drivers/net/ethernet/freescale/Kconfig
+++ b/drivers/net/ethernet/freescale/Kconfig
@@ -64,6 +64,14 @@ config FSL_PQ_MDIO
 
 source "drivers/net/ethernet/freescale/dpa/NetCommSw/Kconfig"
 
+config RX_TX_BUFF_XCHG
+        default n
+	bool "RX and TX ring buffer exchange for Routed packets"
+	depends on GIANFAR && EXPERIMENTAL
+	---help---
+	  Enable this flag to get better throughput for the routing functionality.
+	  Enhances the performance for IPv4 Routing, NAT forwarding.
+ 
 config GFAR_HW_TCP_RECEIVE_OFFLOAD
 	default n
 	bool "Hardware TCP receive offload"
diff --git a/drivers/net/ethernet/freescale/gianfar.c b/drivers/net/ethernet/freescale/gianfar.c
index 21690c3..14f3259 100644
--- a/drivers/net/ethernet/freescale/gianfar.c
+++ b/drivers/net/ethernet/freescale/gianfar.c
@@ -111,7 +111,13 @@
 #include <net/tcp.h>
 #endif
 
+#ifdef CONFIG_RX_TX_BUFF_XCHG
+#define RT_PKT_ID 0xff
+#define KER_PKT_ID 0xfe
+#define TX_TIMEOUT      (5*HZ)
+#else
 #define TX_TIMEOUT      (1*HZ)
+#endif
 
 const char gfar_driver_version[] = "1.3";
 static struct gfar_recycle_cntxt *gfar_global_recycle_cntxt;
@@ -695,6 +701,11 @@ static int gfar_of_init(struct platform_device *ofdev, struct net_device **pdev)
 		return -EINVAL;
 	}
 
+#ifdef CONFIG_RX_TX_BUFF_XCHG
+	/* Creating multilple queues for avoiding lock in xmit function.*/
+	num_tx_qs = (num_tx_qs < 3) ? 3 : num_tx_qs;
+#endif
+
 	*pdev = alloc_etherdev_mq(sizeof(*priv), num_tx_qs);
 	dev = *pdev;
 	if (NULL == dev)
@@ -2083,7 +2094,9 @@ void gfar_halt(struct net_device *dev)
 static void free_grp_irqs(struct gfar_priv_grp *grp)
 {
 	free_irq(grp->interruptError, grp);
+#ifndef CONFIG_RX_TX_BUFF_XCHG
 	free_irq(grp->interruptTransmit, grp);
+#endif
 	free_irq(grp->interruptReceive, grp);
 }
 
@@ -2364,6 +2377,7 @@ static int register_grp_irqs(struct gfar_priv_grp *grp)
 			goto err_irq_fail;
 		}
 
+#ifndef CONFIG_RX_TX_BUFF_XCHG
 		if (likely(tx_napi_enabled)) {
 			err = request_irq(grp->interruptTransmit,
 					gfar_transmit, 0,
@@ -2373,6 +2387,7 @@ static int register_grp_irqs(struct gfar_priv_grp *grp)
 					gfar_transmit_no_napi, 0,
 					grp->int_name_tx, grp);
 		}
+#endif
 
 		if (err < 0) {
 			netif_err(priv, intr, dev, "Can't get IRQ %d\n",
@@ -2400,7 +2415,9 @@ static int register_grp_irqs(struct gfar_priv_grp *grp)
 	return 0;
 
 rx_irq_fail:
+#ifndef CONFIG_RX_TX_BUFF_XCHG
 	free_irq(grp->interruptTransmit, grp);
+#endif
 tx_irq_fail:
 	free_irq(grp->interruptError, grp);
 err_irq_fail:
@@ -2621,6 +2638,10 @@ static int gfar_start_xmit(struct sk_buff *skb, struct net_device *dev)
 	u32 bufaddr;
 	unsigned long flags;
 	unsigned int nr_frags, nr_txbds, length, fcb_length = GMAC_FCB_LEN;
+#ifdef CONFIG_RX_TX_BUFF_XCHG
+	struct sk_buff *new_skb;
+	int skb_curtx = 0;
+#endif
 
 	/*
 	 * TOE=1 frames larger than 2500 bytes may see excess delays
@@ -2636,7 +2657,11 @@ static int gfar_start_xmit(struct sk_buff *skb, struct net_device *dev)
 			return ret;
 	}
 
+#ifdef CONFIG_RX_TX_BUFF_XCHG
+	rq = smp_processor_id() + 1;
+#else
 	rq = skb->queue_mapping;
+#endif
 	tx_queue = priv->tx_queue[rq];
 	txq = netdev_get_tx_queue(dev, rq);
 	base = tx_queue->tx_bd_base;
@@ -2678,6 +2703,7 @@ static int gfar_start_xmit(struct sk_buff *skb, struct net_device *dev)
 	else
 		nr_txbds = nr_frags + 1;
 
+#ifndef CONFIG_RX_TX_BUFF_XCHG
 	/* check if there is space to queue this packet */
 	if (nr_txbds > tx_queue->num_txbdfree) {
 		/* no space, stop the queue */
@@ -2685,7 +2711,36 @@ static int gfar_start_xmit(struct sk_buff *skb, struct net_device *dev)
 		dev->stats.tx_fifo_errors++;
 		return NETDEV_TX_BUSY;
 	}
+#else
+	txbdp = tx_queue->cur_tx;
+	skb_curtx = tx_queue->skb_curtx;
+	do {
+		lstatus = txbdp->lstatus;
+		if ((lstatus & BD_LFLAG(TXBD_READY))) {
+			/* BD not free for tx */
+			netif_tx_stop_queue(txq);
+			dev->stats.tx_fifo_errors++;
+			return NETDEV_TX_BUSY;
+		}
+
+		/* BD is free to be used by s/w */
+		/* Free skb for this BD if not recycled */
+		if (tx_queue->tx_skbuff[skb_curtx] &&
+		    tx_queue->tx_skbuff[skb_curtx]->owner == KER_PKT_ID) {
+			dev_kfree_skb_any(tx_queue->tx_skbuff[skb_curtx]);
+			tx_queue->tx_skbuff[skb_curtx] = NULL;
+		}
+
+		txbdp->lstatus &= BD_LFLAG(TXBD_WRAP);
+		skb_curtx = (skb_curtx + 1) & TX_RING_MOD_MASK(tx_queue->tx_ring_size);
+		nr_txbds--;
 
+		if (!nr_txbds)
+			break;
+
+		txbdp = next_txbd(txbdp, base, tx_queue->tx_ring_size);
+	} while (1);
+#endif
 	/* Update transmit stats */
 	tx_queue->stats.tx_bytes += skb->len;
 	tx_queue->stats.tx_packets++;
@@ -2781,6 +2836,17 @@ static int gfar_start_xmit(struct sk_buff *skb, struct net_device *dev)
 		gfar_write(&regs->dfvlan, vlan_ctrl);
 	}
 
+#ifdef CONFIG_RX_TX_BUFF_XCHG
+	new_skb = tx_queue->tx_skbuff[tx_queue->skb_curtx];
+	skb_curtx = tx_queue->skb_curtx;
+	if (new_skb && skb->owner != RT_PKT_ID) {
+		/* Packet from Kernel free the skb to recycle poll */
+		new_skb->dev = dev;
+		gfar_free_skb(new_skb);
+		new_skb = NULL;
+	}
+#endif
+ 
 	txbdp_start->bufPtr = dma_map_single(&priv->ofdev->dev, skb->data,
 			skb_headlen(skb), DMA_TO_DEVICE);
 
@@ -2813,7 +2879,9 @@ static int gfar_start_xmit(struct sk_buff *skb, struct net_device *dev)
 	 * also must grab the lock before setting ready bit for the first
 	 * to be transmitted BD.
 	 */
+#ifndef CONFIG_RX_TX_BUFF_XCHG
 	spin_lock_irqsave(&tx_queue->txlock, flags);
+#endif
 
 	/*
 	 * The powerpc-specific eieio() is used, as wmb() has too strong
@@ -2838,6 +2906,7 @@ static int gfar_start_xmit(struct sk_buff *skb, struct net_device *dev)
 
 	tx_queue->cur_tx = next_txbd(txbdp, base, tx_queue->tx_ring_size);
 
+#ifndef CONFIG_RX_TX_BUFF_XCHG
 	/* reduce TxBD free count */
 	tx_queue->num_txbdfree -= (nr_txbds);
 
@@ -2848,12 +2917,25 @@ static int gfar_start_xmit(struct sk_buff *skb, struct net_device *dev)
 
 		dev->stats.tx_fifo_errors++;
 	}
+#endif
 
 	/* Tell the DMA to go go go */
 	gfar_write(&regs->tstat, TSTAT_CLEAR_THALT >> tx_queue->qindex);
 
+#ifndef CONFIG_RX_TX_BUFF_XCHG
 	/* Unlock priv */
 	spin_unlock_irqrestore(&tx_queue->txlock, flags);
+#endif
+#ifdef CONFIG_RX_TX_BUFF_XCHG
+{
+	struct net_device *dev = skb->dev;
+	struct gfar_private *priv = netdev_priv(dev);
+
+	if (!skb_recycle_check(skb, priv->rx_buffer_size + RXBUF_ALIGNMENT))
+		skb->owner = KER_PKT_ID;
+}
+	skb->new_skb = new_skb;
+#endif
 
 	return NETDEV_TX_OK;
 }
@@ -3642,8 +3724,10 @@ int gfar_clean_rx_ring(struct gfar_priv_rx_q *rx_queue, int rx_work_limit)
 		struct sk_buff *newskb;
 		rmb();
 
+#ifndef CONFIG_RX_TX_BUFF_XCHG
 		/* Add another skb for the future */
 		newskb = gfar_new_skb(dev);
+#endif
 
 		skb = rx_queue->rx_skbuff[rx_queue->skb_currx];
 
@@ -3654,6 +3738,7 @@ int gfar_clean_rx_ring(struct gfar_priv_rx_q *rx_queue, int rx_work_limit)
 				bdp->length > priv->rx_buffer_size))
 			bdp->status = RXBD_LARGE;
 
+#ifndef CONFIG_RX_TX_BUFF_XCHG
 		/* We drop the frame if we failed to allocate a new buffer */
 		if (unlikely(!newskb || !(bdp->status & RXBD_LAST) ||
 				 bdp->status & RXBD_ERR)) {
@@ -3665,6 +3750,12 @@ int gfar_clean_rx_ring(struct gfar_priv_rx_q *rx_queue, int rx_work_limit)
 				skb->dev = dev;
 				gfar_free_skb(skb);
 			}
+#else
+		if (unlikely(!(bdp->status & RXBD_LAST) ||
+				bdp->status & RXBD_ERR)) {
+			count_errors(bdp->status, dev);
+			newskb = skb;
+#endif
 		} else {
 			/* Increment the number of packets */
 			rx_queue->stats.rx_packets++;
@@ -3676,6 +3767,9 @@ int gfar_clean_rx_ring(struct gfar_priv_rx_q *rx_queue, int rx_work_limit)
 				skb_put(skb, pkt_len);
 				rx_queue->stats.rx_bytes += pkt_len;
 				skb_record_rx_queue(skb, rx_queue->qindex);
+#ifdef CONFIG_RX_TX_BUFF_XCHG
+				skb->owner = RT_PKT_ID;
+#endif
 #ifdef CONFIG_GFAR_HW_TCP_RECEIVE_OFFLOAD
 				if (likely(priv->hw_tcp.en))
 					gfar_hwaccel_tcp4_receive
@@ -3683,15 +3777,29 @@ int gfar_clean_rx_ring(struct gfar_priv_rx_q *rx_queue, int rx_work_limit)
 				else
 #endif
 				gfar_process_frame(dev, skb, amount_pull);
+#ifdef CONFIG_RX_TX_BUFF_XCHG
+				newskb = skb->new_skb;
+				skb->owner = 0;
+				skb->new_skb = NULL;
+#endif
 
 			} else {
 				netif_warn(priv, rx_err, dev, "Missing skb!\n");
 				rx_queue->stats.rx_dropped++;
 				priv->extra_stats.rx_skbmissing++;
 			}
+		}
 
+#ifdef CONFIG_RX_TX_BUFF_XCHG
+		if (!newskb) {
+			/* Allocate new skb for Rx ring */
+			newskb = gfar_new_skb(dev);
 		}
 
+		if (!newskb)
+			/* All memory Exhausted,a BUG */
+			BUG();
+#endif
 		rx_queue->rx_skbuff[rx_queue->skb_currx] = newskb;
 
 		/* Setup the new bdp */
diff --git a/include/linux/skbuff.h b/include/linux/skbuff.h
index 303b972..d213b85 100644
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@ -451,6 +451,10 @@ struct sk_buff {
 
 	__u32			rxhash;
 
+#ifdef CONFIG_RX_TX_BUFF_XCHG
+	__u8			owner;
+	struct sk_buff		*new_skb;
+#endif
 	__u16			vlan_tci;
 
 #ifdef CONFIG_NET_SCHED
-- 
1.7.9.7

