From 81bae28057e991e51627e69a6a75571e918ab367 Mon Sep 17 00:00:00 2001
From: Geoff Thorpe <Geoff.Thorpe@freescale.com>
Date: Wed, 14 Mar 2012 04:49:49 +0000
Subject: [PATCH 078/121] qbman: check resources when they're deallocated

When FQIDs are deallocated we check that they are in the OOS state first
(and when they aren't a "leak" warning is logged without completing the
deallocation). Likewise when BPIDs are deallocated we confirm that the
pools are empty.

Signed-off-by: Geoff Thorpe <Geoff.Thorpe@freescale.com>
[Kevin: Original patch taken from FSL
QorIQ-SDK-V1.2-SOURCE-20120614-yocto.iso image]
Signed-off-by: Kevin Hao <kexin.hao@windriver.com>
---
 drivers/staging/fsl_qbman/bman_driver.c |   12 +--
 drivers/staging/fsl_qbman/dpa_alloc.c   |  148 +++++++++++++++++++++++++++++--
 drivers/staging/fsl_qbman/dpa_sys.h     |   13 ++-
 drivers/staging/fsl_qbman/qman_driver.c |   13 +--
 4 files changed, 168 insertions(+), 18 deletions(-)

diff --git a/drivers/staging/fsl_qbman/bman_driver.c b/drivers/staging/fsl_qbman/bman_driver.c
index ff6c9e6..0d3734e 100644
--- a/drivers/staging/fsl_qbman/bman_driver.c
+++ b/drivers/staging/fsl_qbman/bman_driver.c
@@ -333,12 +333,6 @@ static __init int bman_init(void)
 		else
 			pr_err("Bman err interrupt handler missing\n");
 	}
-	/* Initialise BPID allocation ranges */
-	for_each_compatible_node(dn, NULL, "fsl,bpid-range") {
-		ret = fsl_bpid_range_init(dn);
-		if (ret)
-			return ret;
-	}
 	/* Initialise any declared buffer pools */
 	for_each_compatible_node(dn, NULL, "fsl,bpool") {
 		ret = fsl_bpool_init(dn);
@@ -438,6 +432,12 @@ static __init int bman_init(void)
 		}
 	} while (1);
 #endif
+	/* Initialise BPID allocation ranges */
+	for_each_compatible_node(dn, NULL, "fsl,bpid-range") {
+		ret = fsl_bpid_range_init(dn);
+		if (ret)
+			return ret;
+	}
 	return 0;
 }
 subsys_initcall(bman_init);
diff --git a/drivers/staging/fsl_qbman/dpa_alloc.c b/drivers/staging/fsl_qbman/dpa_alloc.c
index d26007b..4d54064 100644
--- a/drivers/staging/fsl_qbman/dpa_alloc.c
+++ b/drivers/staging/fsl_qbman/dpa_alloc.c
@@ -30,21 +30,79 @@
  */
 
 #include "dpa_sys.h"
+#include <linux/fsl_qman.h>
+#include <linux/fsl_bman.h>
 
 /* Qman and Bman APIs are front-ends to the common code; */
 
 static DECLARE_DPA_ALLOC(bpalloc);
 static DECLARE_DPA_ALLOC(fqalloc);
 
+/* This is a sort-of-conditional dpa_alloc_free() routine. Eg. when releasing
+ * FQIDs (probably from user-space), it can filter out those that aren't in the
+ * OOS state (better to leak a h/w resource than to crash). This function
+ * returns the number of invalid IDs that were not released. */
+static u32 release_id_range(struct dpa_alloc *alloc, u32 id, u32 count,
+			     int (*is_valid)(u32 id))
+{
+	int valid_mode = 0;
+	u32 loop = id, total_invalid = 0;
+	while (loop < (id + count)) {
+		int isvalid = is_valid(loop);
+		if (!valid_mode) {
+			/* We're looking for a valid ID to terminate an invalid
+			 * range */
+			if (isvalid) {
+				/* We finished a range of invalid IDs, a valid
+				 * range is now underway */
+				valid_mode = 1;
+				count -= (loop - id);
+				id = loop;
+			} else
+				total_invalid++;
+		} else {
+			/* We're looking for an invalid ID to terminate a
+			 * valid range */
+			if (!isvalid) {
+				/* Release the range of valid IDs, an unvalid
+				 * range is now underway */
+				if (loop > id)
+					dpa_alloc_free(alloc, id, loop - id);
+				valid_mode = 0;
+			}
+		}
+		loop++;
+	}
+	/* Release any unterminated range of valid IDs */
+	if (valid_mode && count)
+		dpa_alloc_free(alloc, id, count);
+	return total_invalid;
+}
+
 int bman_alloc_bpid_range(u32 *result, u32 count, u32 align, int partial)
 {
 	return dpa_alloc_new(&bpalloc, result, count, align, partial);
 }
 EXPORT_SYMBOL(bman_alloc_bpid_range);
 
+static int bp_valid(u32 bpid)
+{
+	struct bm_pool_state state;
+	int ret = bman_query_pools(&state);
+	BUG_ON(ret);
+	if (bman_depletion_get(&state.as.state, bpid))
+		/* "Available==1" means unavailable, go figure. Ie. it has no
+		 * buffers, which is means it is valid for deallocation. (So
+		 * true means false, which means true...) */
+		return 1;
+	return 0;
+}
 void bman_release_bpid_range(u32 bpid, u32 count)
 {
-	dpa_alloc_free(&bpalloc, bpid, count);
+	u32 total_invalid = release_id_range(&bpalloc, bpid, count, bp_valid);
+	if (total_invalid)
+		pr_err("BPID range [%d..%d] (%d) had %d leaks\n",
+			bpid, bpid + count - 1, count, total_invalid);
 }
 EXPORT_SYMBOL(bman_release_bpid_range);
 
@@ -54,9 +112,22 @@ int qman_alloc_fqid_range(u32 *result, u32 count, u32 align, int partial)
 }
 EXPORT_SYMBOL(qman_alloc_fqid_range);
 
+static int fq_valid(u32 fqid)
+{
+	struct qman_fq fq = {
+		.fqid = fqid
+	};
+	struct qm_mcr_queryfq_np np;
+	int err = qman_query_fq_np(&fq, &np);
+	BUG_ON(err);
+	return ((np.state & QM_MCR_NP_STATE_MASK) == QM_MCR_NP_STATE_OOS);
+}
 void qman_release_fqid_range(u32 fqid, u32 count)
 {
-	dpa_alloc_free(&fqalloc, fqid, count);
+	u32 total_invalid = release_id_range(&fqalloc, fqid, count, fq_valid);
+	if (total_invalid)
+		pr_err("FQID range [%d..%d] (%d) had %d leaks\n",
+			fqid, fqid + count - 1, count, total_invalid);
 }
 EXPORT_SYMBOL(qman_release_fqid_range);
 
@@ -164,15 +235,15 @@ err:
 
 /* Allocate the list node using GFP_ATOMIC, because we *really* want to avoid
  * forcing error-handling on to users in the deallocation path. */
-void dpa_alloc_free(struct dpa_alloc *alloc, u32 fqid, u32 count)
+void dpa_alloc_free(struct dpa_alloc *alloc, u32 base_id, u32 count)
 {
 	struct alloc_node *i, *node = kmalloc(sizeof(*node), GFP_ATOMIC);
 	BUG_ON(!node);
-	DPRINT("release_range(%d,%d)\n", fqid, count);
+	DPRINT("release_range(%d,%d)\n", base_id, count);
 	DUMP(alloc);
 	BUG_ON(!count);
 	spin_lock_irq(&alloc->lock);
-	node->base = fqid;
+	node->base = base_id;
 	node->num = count;
 	list_for_each_entry(i, &alloc->list, list) {
 		if (i->base >= node->base) {
@@ -208,3 +279,70 @@ done:
 	spin_unlock_irq(&alloc->lock);
 	DUMP(alloc);
 }
+
+int dpa_alloc_reserve(struct dpa_alloc *alloc, u32 base, u32 num)
+{
+	struct alloc_node *i = NULL;
+	struct alloc_node *margin_left, *margin_right;
+
+	DPRINT("alloc_reserve(%d,%d)\n", base_id, count);
+	DUMP(alloc);
+	margin_left = kmalloc(sizeof(*margin_left), GFP_KERNEL);
+	if (!margin_left)
+		goto err;
+	margin_right = kmalloc(sizeof(*margin_right), GFP_KERNEL);
+	if (!margin_right) {
+		kfree(margin_left);
+		goto err;
+	}
+	spin_lock_irq(&alloc->lock);
+	list_for_each_entry(i, &alloc->list, list)
+		if ((i->base <= base) && ((i->base + i->num) >= (base + num)))
+			/* yep, the reservation is within this node */
+			goto done;
+	i = NULL;
+done:
+	if (i) {
+		if (base != i->base) {
+			margin_left->base = i->base;
+			margin_left->num = base - i->base;
+			list_add_tail(&margin_left->list, &i->list);
+		} else
+			kfree(margin_left);
+		if ((base + num) < (i->base + i->num)) {
+			margin_right->base = base + num;
+			margin_right->num = (i->base + i->num) -
+						(base + num);
+			list_add(&margin_right->list, &i->list);
+		} else
+			kfree(margin_right);
+		list_del(&i->list);
+		kfree(i);
+	}
+	spin_unlock_irq(&alloc->lock);
+err:
+	DPRINT("returning %d\n", i ? 0 : -ENOMEM);
+	DUMP(alloc);
+	return i ? 0 : -ENOMEM;
+}
+
+int dpa_alloc_pop(struct dpa_alloc *alloc, u32 *result, u32 *count)
+{
+	struct alloc_node *i = NULL;
+	DPRINT("alloc_pop()\n");
+	DUMP(alloc);
+	spin_lock_irq(&alloc->lock);
+	if (!list_empty(&alloc->list)) {
+		i = list_entry(alloc->list.next, struct alloc_node, list);
+		list_del(&i->list);
+	}
+	spin_unlock_irq(&alloc->lock);
+	DPRINT("returning %d\n", i ? 0 : -ENOMEM);
+	DUMP(alloc);
+	if (!i)
+		return -ENOMEM;
+	*result = i->base;
+	*count = i->num;
+	kfree(i);
+	return 0;
+}
diff --git a/drivers/staging/fsl_qbman/dpa_sys.h b/drivers/staging/fsl_qbman/dpa_sys.h
index e802f11..a11439c 100644
--- a/drivers/staging/fsl_qbman/dpa_sys.h
+++ b/drivers/staging/fsl_qbman/dpa_sys.h
@@ -75,9 +75,20 @@ struct dpa_alloc {
 		}, \
 		.lock = __SPIN_LOCK_UNLOCKED(name.lock) \
 	}
+static inline void dpa_alloc_init(struct dpa_alloc *alloc)
+{
+	INIT_LIST_HEAD(&alloc->list);
+	spin_lock_init(&alloc->lock);
+}
 int dpa_alloc_new(struct dpa_alloc *alloc, u32 *result, u32 count, u32 align,
 		  int partial);
-void dpa_alloc_free(struct dpa_alloc *alloc, u32 fqid, u32 count);
+void dpa_alloc_free(struct dpa_alloc *alloc, u32 base_id, u32 count);
+/* Like 'new' but specifies the desired range, returns -ENOMEM if the entire
+ * desired range is not available, or 0 for success. */
+int dpa_alloc_reserve(struct dpa_alloc *alloc, u32 base_id, u32 count);
+/* Pops and returns contiguous ranges from the allocator. Returns -ENOMEM when
+ * 'alloc' is empty. */
+int dpa_alloc_pop(struct dpa_alloc *alloc, u32 *result, u32 *count);
 
 /* When copying aligned words or shorts, try to avoid memcpy() */
 #define CONFIG_TRY_BETTER_MEMCPY
diff --git a/drivers/staging/fsl_qbman/qman_driver.c b/drivers/staging/fsl_qbman/qman_driver.c
index 1fb8ae6..47fb86c 100644
--- a/drivers/staging/fsl_qbman/qman_driver.c
+++ b/drivers/staging/fsl_qbman/qman_driver.c
@@ -341,12 +341,6 @@ static __init int qman_init(void)
 		else
 			pr_err("Qman err interrupt handler missing\n");
 	}
-	/* Initialise FQID allocation ranges */
-	for_each_compatible_node(dn, NULL, "fsl,fqid-range") {
-		ret = fsl_fqid_range_init(dn);
-		if (ret)
-			return ret;
-	}
 	/* Parse pool channels */
 	for_each_compatible_node(dn, NULL, "fsl,qman-pool-channel") {
 		const u32 *index = of_get_property(dn, "cell-index", NULL);
@@ -443,6 +437,13 @@ static __init int qman_init(void)
 		}
 	} while (1);
 #endif
+	/* Initialise FQID allocation ranges */
+	for_each_compatible_node(dn, NULL, "fsl,fqid-range") {
+		ret = fsl_fqid_range_init(dn);
+		if (ret)
+			return ret;
+	}
+
 	/* This is to ensure h/w-internal CGR memory is zeroed out. Note that we
 	 * do this for all conceivable CGRIDs, not all of which are necessarily
 	 * available on the underlying hardware version. We ignore any errors
-- 
1.7.9.7

