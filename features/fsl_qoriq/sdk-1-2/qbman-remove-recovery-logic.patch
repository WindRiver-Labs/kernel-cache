From b3acb12bb50e3e53d2597e8767ac9583ed0e9973 Mon Sep 17 00:00:00 2001
From: Geoff Thorpe <Geoff.Thorpe@freescale.com>
Date: Wed, 14 Mar 2012 04:49:37 +0000
Subject: [PATCH 071/121] qbman: remove recovery logic

The qbman drivers had an ad-hoc "recovery" scheme that needs to be
rethought. In particular it is assumes the use of a system-global
recovery of resources when the kernel boots, which provides no help to
implementing recovery in smaller-scale, more-dynamic use-cases, and
gets in the way of reworking portal initialisation for precisely the
same dynamic use-cases.

Signed-off-by: Geoff Thorpe <Geoff.Thorpe@freescale.com>
[Kevin: Original patch taken from FSL
QorIQ-SDK-V1.2-SOURCE-20120614-yocto.iso image]
Signed-off-by: Kevin Hao <kexin.hao@windriver.com>
---
 drivers/staging/fsl_qbman/bman_driver.c  |   62 +------
 drivers/staging/fsl_qbman/bman_high.c    |   46 +----
 drivers/staging/fsl_qbman/bman_private.h |    4 +-
 drivers/staging/fsl_qbman/dpa_sys.h      |    4 -
 drivers/staging/fsl_qbman/qman_driver.c  |   62 +------
 drivers/staging/fsl_qbman/qman_high.c    |  270 +-----------------------------
 drivers/staging/fsl_qbman/qman_private.h |    4 +-
 include/linux/fsl_bman.h                 |   10 --
 include/linux/fsl_qman.h                 |   10 --
 9 files changed, 31 insertions(+), 441 deletions(-)

diff --git a/drivers/staging/fsl_qbman/bman_driver.c b/drivers/staging/fsl_qbman/bman_driver.c
index 18bed5b..7116a53 100644
--- a/drivers/staging/fsl_qbman/bman_driver.c
+++ b/drivers/staging/fsl_qbman/bman_driver.c
@@ -151,8 +151,7 @@ EXPORT_SYMBOL(bm_pool_free);
 #ifdef CONFIG_FSL_BMAN_PORTAL
 static __init struct bman_portal *init_affine_portal(
 					struct bm_portal_config *pconfig,
-					int cpu, struct bman_portal *redirect,
-					int recovery_mode)
+					int cpu, struct bman_portal *redirect)
 {
 	struct bman_portal *portal;
 	struct cpumask oldmask = *tsk_cpus_allowed(current);
@@ -163,7 +162,7 @@ static __init struct bman_portal *init_affine_portal(
 	if (redirect)
 		portal = bman_create_affine_slave(redirect);
 	else {
-		portal = bman_create_affine_portal(pconfig, recovery_mode);
+		portal = bman_create_affine_portal(pconfig);
 #ifdef CONFIG_FSL_DPA_PIRQ_SLOW
 		if (portal)
 			bman_irqsource_add(BM_PIRQ_RCRI | BM_PIRQ_BSCN);
@@ -331,8 +330,7 @@ static int __init fsl_bpool_init(struct device_node *node)
 	return ret;
 }
 
-static int __init fsl_bpool_range_init(struct device_node *node,
-					int recovery_mode)
+static int __init fsl_bpool_range_init(struct device_node *node)
 {
 	int ret, warned = 0;
 	u32 bpid;
@@ -364,44 +362,11 @@ static int __init fsl_bpool_range_init(struct device_node *node,
 			num_pools--;
 		}
 	}
-#ifdef CONFIG_FSL_BMAN_PORTAL
-	/* If in recovery mode *and* we are using a private BPID allocation
-	 * range, then automatically clean up all BPIDs in that range so we can
-	 * automatically exit recovery mode too. */
-	if (recovery_mode) {
-		for (bpid = range[0]; bpid < (range[0] + range[1]); bpid++) {
-			ret = bman_recovery_cleanup_bpid(bpid);
-			if (ret) {
-				pr_err("Failed to recovery BPID %d\n", bpid);
-				return ret;
-			}
-		}
-	}
-#else
-	BUG_ON(recovery_mode);
-#endif
-	pr_info("Bman: BPID allocator includes range %d:%d%s\n",
-		range[0], range[1], recovery_mode ? " (recovered)" : "");
+	pr_info("Bman: BPID allocator includes range %d:%d\n",
+		range[0], range[1]);
 	return 0;
 }
 
-#ifdef CONFIG_FSL_BMAN_PORTAL
-void bman_recovery_exit(void)
-{
-	unsigned int cpu;
-
-	for_each_cpu(cpu, bman_affine_cpus()) {
-		struct cpumask oldmask = *tsk_cpus_allowed(current);
-		const struct cpumask *newmask = get_cpu_mask(cpu);
-		set_cpus_allowed_ptr(current, newmask);
-		bman_recovery_exit_local();
-		set_cpus_allowed_ptr(current, &oldmask);
-		pr_info("Bman portal exited recovery, cpu %d\n", cpu);
-	}
-}
-EXPORT_SYMBOL(bman_recovery_exit);
-#endif
-
 static __init int bman_init(void)
 {
 #ifdef CONFIG_FSL_BMAN_PORTAL
@@ -412,7 +377,7 @@ static __init int bman_init(void)
 #endif
 	struct device_node *dn;
 	struct bm_portal_config *pcfg;
-	int ret, recovery_mode = 0;
+	int ret;
 	LIST_HEAD(cfg_list);
 
 	for_each_compatible_node(dn, NULL, "fsl,bman") {
@@ -428,8 +393,6 @@ static __init int bman_init(void)
 		num_pools = bman_pool_max;
 	}
 #ifdef CONFIG_FSL_BMAN_PORTAL
-	if (fsl_dpa_should_recover())
-		recovery_mode = 1;
 	for_each_compatible_node(dn, NULL, "fsl,bman-portal") {
 		if (!of_device_is_available(dn))
 			continue;
@@ -470,8 +433,7 @@ static __init int bman_init(void)
 		if (pcfg->public_cfg.cpu < 0 || !cpumask_test_cpu(
 					pcfg->public_cfg.cpu, &slave_cpus))
 			continue;
-		p = init_affine_portal(pcfg, pcfg->public_cfg.cpu, NULL,
-					recovery_mode);
+		p = init_affine_portal(pcfg, pcfg->public_cfg.cpu, NULL);
 		if (p) {
 			if (is_shared)
 				sharing_portal = p;
@@ -483,7 +445,7 @@ static __init int bman_init(void)
 		int loop;
 		for_each_cpu(loop, &slave_cpus) {
 			struct bman_portal *p = init_affine_portal(NULL, loop,
-					sharing_portal, recovery_mode);
+					sharing_portal);
 			if (!p)
 				pr_err("Failed slave Bman portal for cpu %d\n",
 					loop);
@@ -506,16 +468,10 @@ static __init int bman_init(void)
 			bman_depletion_fill(&pools);
 			num_pools = 64;
 		}
-		ret = fsl_bpool_range_init(dn, recovery_mode);
+		ret = fsl_bpool_range_init(dn);
 		if (ret)
 			return ret;
 	}
-#ifdef CONFIG_FSL_BMAN_PORTAL
-	/* If using private BPID allocation, exit recovery mode automatically
-	 * (ie. after automatic recovery) */
-	if (recovery_mode && explicit_allocator)
-		bman_recovery_exit();
-#endif
 	for_each_compatible_node(dn, NULL, "fsl,bpool") {
 		ret = fsl_bpool_init(dn);
 		if (ret)
diff --git a/drivers/staging/fsl_qbman/bman_high.c b/drivers/staging/fsl_qbman/bman_high.c
index 334cde7..8bce299 100644
--- a/drivers/staging/fsl_qbman/bman_high.c
+++ b/drivers/staging/fsl_qbman/bman_high.c
@@ -191,8 +191,7 @@ static irqreturn_t portal_isr(__always_unused int irq, void *ptr)
 }
 
 struct bman_portal *bman_create_affine_portal(
-			const struct bm_portal_config *config,
-			int recovery_mode __maybe_unused)
+			const struct bm_portal_config *config)
 {
 	struct bman_portal *portal = get_raw_affine_portal();
 	struct bm_portal *__p = &portal->p;
@@ -260,14 +259,10 @@ struct bman_portal *bman_create_affine_portal(
 		pr_err("request_irq() failed\n");
 		goto fail_irq;
 	}
-	/* Enable the bits that make sense */
-	if (!recovery_mode)
-		bm_isr_uninhibit(__p);
 	/* Need RCR to be empty before continuing */
-	bm_isr_disable_write(__p, ~BM_PIRQ_RCRI);
 	ret = bm_rcr_get_fill(__p);
 	if (ret) {
-		pr_err("Bman RCR unclean, need recovery\n");
+		pr_err("Bman RCR unclean\n");
 		goto fail_rcr_empty;
 	}
 	/* Success */
@@ -276,6 +271,7 @@ struct bman_portal *bman_create_affine_portal(
 	cpumask_set_cpu(config->public_cfg.cpu, &affine_mask);
 	spin_unlock(&affine_mask_lock);
 	bm_isr_disable_write(__p, 0);
+	bm_isr_uninhibit(__p);
 	put_affine_portal();
 	return portal;
 fail_rcr_empty:
@@ -547,42 +543,6 @@ done:
 }
 EXPORT_SYMBOL(bman_poll);
 
-int bman_recovery_cleanup_bpid(u32 bpid)
-{
-	struct bman_pool pool = {
-		.params = {
-			.bpid = bpid
-		}
-	};
-	struct bm_buffer bufs[8];
-	int ret = 0;
-	unsigned int num_bufs = 0;
-	do {
-		/* Acquire is all-or-nothing, so we drain in 8s, then in
-		 * 1s for the remainder. */
-		if (ret != 1)
-			ret = bman_acquire(&pool, bufs, 8, 0);
-		if (ret < 8)
-			ret = bman_acquire(&pool, bufs, 1, 0);
-		if (ret > 0)
-			num_bufs += ret;
-	} while (ret > 0);
-	if (num_bufs)
-		pr_info("Bman: BPID %d recovered (%d bufs)\n", bpid, num_bufs);
-	return 0;
-}
-EXPORT_SYMBOL(bman_recovery_cleanup_bpid);
-
-/* called from bman_driver.c::bman_recovery_exit() only (if exporting, use
- * get_raw_affine_portal() and check for the "SLAVE" bit). */
-void bman_recovery_exit_local(void)
-{
-	struct bman_portal *p = get_affine_portal();
-	bm_isr_status_clear(&p->p, 0xffffffff);
-	bm_isr_uninhibit(&p->p);
-	put_affine_portal();
-}
-
 static const u32 zero_thresholds[4] = {0, 0, 0, 0};
 
 struct bman_pool *bman_new_pool(const struct bman_pool_params *params)
diff --git a/drivers/staging/fsl_qbman/bman_private.h b/drivers/staging/fsl_qbman/bman_private.h
index 3d4d8ed..5f2e31d 100644
--- a/drivers/staging/fsl_qbman/bman_private.h
+++ b/drivers/staging/fsl_qbman/bman_private.h
@@ -68,11 +68,9 @@ int bman_init_error_int(struct device_node *node);
 
 /* Hooks from bman_driver.c in to bman_high.c */
 struct bman_portal *bman_create_affine_portal(
-			const struct bm_portal_config *config,
-			int recovery_mode);
+			const struct bm_portal_config *config);
 struct bman_portal *bman_create_affine_slave(struct bman_portal *redirect);
 const struct bm_portal_config *bman_destroy_affine_portal(void);
-void bman_recovery_exit_local(void);
 
 /* Pool logic in the portal driver, during initialisation, needs to know if
  * there's access to CCSR or not (if not, it'll cripple the pool allocator). */
diff --git a/drivers/staging/fsl_qbman/dpa_sys.h b/drivers/staging/fsl_qbman/dpa_sys.h
index e5982d9..73cb7b1 100644
--- a/drivers/staging/fsl_qbman/dpa_sys.h
+++ b/drivers/staging/fsl_qbman/dpa_sys.h
@@ -122,10 +122,6 @@ const struct dpa_uio_class *dpa_uio_qman(void);
 
 /* These stubs are re-mapped to hypervisor+failover features in kernel trees
  * that contain that support. */
-static inline int fsl_dpa_should_recover(void)
-{
-	return 0;
-}
 static inline int pamu_enable_liodn(struct device_node *n, int i)
 {
 	return 0;
diff --git a/drivers/staging/fsl_qbman/qman_driver.c b/drivers/staging/fsl_qbman/qman_driver.c
index 86b005b..9119e2d 100644
--- a/drivers/staging/fsl_qbman/qman_driver.c
+++ b/drivers/staging/fsl_qbman/qman_driver.c
@@ -54,8 +54,7 @@ EXPORT_SYMBOL(dpa_uio_qman);
 #ifdef CONFIG_FSL_QMAN_PORTAL
 static __init struct qman_portal *init_affine_portal(
 					struct qm_portal_config *pconfig,
-					int cpu, struct qman_portal *redirect,
-					int recovery_mode)
+					int cpu, struct qman_portal *redirect)
 {
 	struct qman_portal *portal;
 	struct cpumask oldmask = *tsk_cpus_allowed(current);
@@ -66,8 +65,7 @@ static __init struct qman_portal *init_affine_portal(
 	if (redirect)
 		portal = qman_create_affine_slave(redirect);
 	else {
-		portal = qman_create_affine_portal(pconfig, NULL,
-				recovery_mode);
+		portal = qman_create_affine_portal(pconfig, NULL);
 		if (portal) {
 			u32 irq_sources = 0;
 			/* default: enable all (available) pool channels */
@@ -253,8 +251,7 @@ static void __init fsl_qman_portal_destroy(struct qm_portal_config *pcfg)
 	kfree(pcfg);
 }
 
-static __init int fsl_fqid_range_init(struct device_node *node,
-					int recovery_mode)
+static __init int fsl_fqid_range_init(struct device_node *node)
 {
 	int ret;
 	u32 *range = (u32 *)of_get_property(node, "fsl,fqid-range", &ret);
@@ -269,45 +266,11 @@ static __init int fsl_fqid_range_init(struct device_node *node,
 		return -EINVAL;
 	}
 	qman_release_fqid_range(range[0], range[1]);
-#ifdef CONFIG_FSL_QMAN_PORTAL
-	/* If in recovery mode *and* we are using a private FQ allocation range,
-	 * then automatically clean up all FQs in that range so we can
-	 * automatically exit recovery mode too. */
-	if (recovery_mode) {
-		u32 fqid;
-		for (fqid = range[0]; fqid < (range[0] + range[1]); fqid++) {
-			ret = qman_recovery_cleanup_fq(fqid);
-			if (ret) {
-				pr_err("Failed to recovery FQID %d\n", fqid);
-				return ret;
-			}
-		}
-	}
-#else
-	BUG_ON(recovery_mode);
-#endif
-	pr_info("Qman: FQID allocator includes range %d:%d%s\n",
-		range[0], range[1], recovery_mode ? " (recovered)" : "");
+	pr_info("Qman: FQID allocator includes range %d:%d\n",
+		range[0], range[1]);
 	return 0;
 }
 
-#ifdef CONFIG_FSL_QMAN_PORTAL
-void qman_recovery_exit(void)
-{
-	unsigned int cpu;
-
-	for_each_cpu(cpu, qman_affine_cpus()) {
-		struct cpumask oldmask = *tsk_cpus_allowed(current);
-		const struct cpumask *newmask = get_cpu_mask(cpu);
-		set_cpus_allowed_ptr(current, newmask);
-		qman_recovery_exit_local();
-		set_cpus_allowed_ptr(current, &oldmask);
-		pr_info("Qman portal exited recovery, cpu %d\n", cpu);
-	}
-}
-EXPORT_SYMBOL(qman_recovery_exit);
-#endif
-
 /***************/
 /* Driver load */
 /***************/
@@ -323,7 +286,7 @@ static __init int qman_init(void)
 #endif
 	struct device_node *dn;
 	struct qm_portal_config *pcfg;
-	int ret, use_bpid0 = 1, recovery_mode = 0;
+	int ret, use_bpid0 = 1;
 	LIST_HEAD(cfg_list);
 
 	for_each_compatible_node(dn, NULL, "fsl,qman") {
@@ -338,8 +301,6 @@ static __init int qman_init(void)
 		return ret;
 #endif
 #ifdef CONFIG_FSL_QMAN_PORTAL
-	if (fsl_dpa_should_recover())
-		recovery_mode = 1;
 	for_each_compatible_node(dn, NULL, "fsl,qman-portal") {
 		if (!of_device_is_available(dn))
 			continue;
@@ -380,8 +341,7 @@ static __init int qman_init(void)
 		if (pcfg->public_cfg.cpu < 0 || !cpumask_test_cpu(
 					pcfg->public_cfg.cpu, &slave_cpus))
 			continue;
-		p = init_affine_portal(pcfg, pcfg->public_cfg.cpu, NULL,
-					recovery_mode);
+		p = init_affine_portal(pcfg, pcfg->public_cfg.cpu, NULL);
 		if (p) {
 			if (is_shared)
 				sharing_portal = p;
@@ -392,7 +352,7 @@ static __init int qman_init(void)
 		int loop;
 		for_each_cpu(loop, &slave_cpus) {
 			struct qman_portal *p = init_affine_portal(NULL, loop,
-					sharing_portal, recovery_mode);
+					sharing_portal);
 			if (!p)
 				pr_err("Failed slave Qman portal for cpu %d\n",
 					loop);
@@ -411,15 +371,11 @@ static __init int qman_init(void)
 #endif
 	for_each_compatible_node(dn, NULL, "fsl,fqid-range") {
 		use_bpid0 = 0;
-		ret = fsl_fqid_range_init(dn, recovery_mode);
+		ret = fsl_fqid_range_init(dn);
 		if (ret)
 			return ret;
 	}
 #ifdef CONFIG_FSL_QMAN_PORTAL
-	/* If using private FQ allocation, exit recovery mode automatically (ie.
-	 * after automatic recovery) */
-	if (recovery_mode && !use_bpid0)
-		qman_recovery_exit();
 	for (cgr.cgrid = 0; cgr.cgrid < __CGR_NUM; cgr.cgrid++) {
 		/* This is to ensure h/w-internal CGR memory is zeroed out. Note
 		 * that we do this for all conceivable CGRIDs, not all of which
diff --git a/drivers/staging/fsl_qbman/qman_high.c b/drivers/staging/fsl_qbman/qman_high.c
index f1a791a..c8580fa 100644
--- a/drivers/staging/fsl_qbman/qman_high.c
+++ b/drivers/staging/fsl_qbman/qman_high.c
@@ -72,8 +72,6 @@ static inline int fq_isclear(struct qman_fq *fq, u32 mask)
 	return !(fq->flags & mask);
 }
 
-#define PORTAL_BITS_RECOVERY	0x00040000	/* recovery mode */
-
 struct qman_portal {
 	struct qm_portal p;
 	unsigned long bits; /* PORTAL_BITS_*** - dynamic, strictly internal */
@@ -332,32 +330,9 @@ loop:
 	goto loop;
 }
 
-/* this is called from qman_create_affine_portal() if not initialising in
- * recovery mode, otherwise from qman_recovery_exit_local() after recovery is
- * done. */
-static void post_recovery(struct qman_portal *p __always_unused,
-			const struct qm_portal_config *config)
-{
-	struct device_node *tmp_node, *node = config->node;
-	/* Enable DMA on portal LIODNs (stashing) and those of its sub-nodes
-	 * (Fman TX and SEC/PME accelerators, where available). */
-	if (pamu_enable_liodn(node, -1))
-		/* If there's a PAMU problem, best to continue anyway and let
-		 * the corresponding traffic hit whatever problems it will hit,
-		 * than to fail portal initialisation and trigger a crash in
-		 * dependent code that has no relationship to the PAMU issue. */
-		pr_err("Failed to enable portal LIODN %s\n",
-			node->full_name);
-	for_each_child_of_node(node, tmp_node)
-		if (pamu_enable_liodn(tmp_node, -1))
-			pr_err("Failed to enable portal LIODN %s\n",
-				tmp_node->full_name);
-}
-
 struct qman_portal *qman_create_affine_portal(
 			const struct qm_portal_config *config,
-			const struct qman_cgrs *cgrs,
-			int recovery_mode)
+			const struct qman_cgrs *cgrs)
 {
 	struct qman_portal *portal = get_raw_affine_portal();
 	struct qm_portal *__p = &portal->p;
@@ -385,8 +360,7 @@ struct qman_portal *qman_create_affine_portal(
 #define QM_DQRR_CMODE qm_dqrr_cdc
 #endif
 	if (qm_dqrr_init(__p, config, qm_dqrr_dpush, qm_dqrr_pvb,
-			recovery_mode ?  qm_dqrr_cci : QM_DQRR_CMODE,
-			DQRR_MAXFILL)) {
+			QM_DQRR_CMODE, DQRR_MAXFILL)) {
 		pr_err("Qman DQRR initialisation failed\n");
 		goto fail_dqrr;
 	}
@@ -394,45 +368,6 @@ struct qman_portal *qman_create_affine_portal(
 		pr_err("Qman MR initialisation failed\n");
 		goto fail_mr;
 	}
-	/* for recovery mode, quiesce SDQCR/VDQCR and drain DQRR+MR until h/w
-	 * wraps up anything it was doing (5ms is ample idle time). */
-	if (recovery_mode) {
-		const struct qm_dqrr_entry *dq;
-		const struct qm_mr_entry *msg;
-		int idle = 0;
-		/* quiesce SDQCR/VDQCR, then drain till h/w wraps up anything it
-		 * was doing (5ms is more than enough to ensure it's done). */
-		qm_dqrr_sdqcr_set(__p, 0);
-		qm_dqrr_vdqcr_set(__p, 0);
-drain_loop:
-		qm_dqrr_pvb_update(__p);
-		dq = qm_dqrr_current(__p);
-		qm_mr_pvb_update(__p);
-		msg = qm_mr_current(__p);
-		if (dq) {
-			pr_warning("DQRR recovery: dumping dqrr %02x:%02x for "
-				"FQID %d\n", dq->verb & QM_DQRR_VERB_MASK,
-				dq->stat, dq->fqid);
-			qm_dqrr_next(__p);
-			qm_dqrr_cci_consume(__p, 1);
-		}
-		if (msg) {
-			pr_warning("MR recovery: dumping msg 0x%02x for "
-				"FQID %d\n", msg->verb & QM_MR_VERB_TYPE_MASK,
-				msg->fq.fqid);
-			qm_mr_next(__p);
-			qm_mr_cci_consume(__p, 1);
-		}
-		if (!dq && !msg) {
-			if (++idle < 5) {
-				msleep(1);
-				goto drain_loop;
-			}
-		} else {
-			idle = 0;
-			goto drain_loop;
-		}
-	}
 	if (qm_mc_init(__p)) {
 		pr_err("Qman MC initialisation failed\n");
 		goto fail_mc;
@@ -458,7 +393,7 @@ drain_loop:
 	for (ret = 0; ret < __CGR_NUM; ret++)
 		INIT_LIST_HEAD(&portal->cgr_cbs[ret]);
 	spin_lock_init(&portal->cgr_lock);
-	portal->bits = recovery_mode ? PORTAL_BITS_RECOVERY : 0;
+	portal->bits = 0;
 	portal->slowpoll = 0;
 #ifdef CONFIG_FSL_DPA_CAN_WAIT_SYNC
 	portal->eqci_owned = NULL;
@@ -501,30 +436,24 @@ drain_loop:
 		pr_err("request_irq() failed\n");
 		goto fail_irq;
 	}
-	if (recovery_mode) {
-		qm_isr_inhibit(__p);
-	} else {
-		post_recovery(portal, config);
-		qm_isr_uninhibit(__p);
-	}
 	/* Need EQCR to be empty before continuing */
 	isdr ^= QM_PIRQ_EQCI;
 	qm_isr_disable_write(__p, isdr);
 	ret = qm_eqcr_get_fill(__p);
 	if (ret) {
-		pr_err("Qman EQCR unclean, need recovery\n");
+		pr_err("Qman EQCR unclean\n");
 		goto fail_eqcr_empty;
 	}
 	isdr ^= (QM_PIRQ_DQRI | QM_PIRQ_MRI);
 	qm_isr_disable_write(__p, isdr);
 	if (qm_dqrr_current(__p) != NULL) {
-		pr_err("Qman DQRR unclean, need recovery\n");
+		pr_err("Qman DQRR unclean\n");
 		goto fail_dqrr_mr_empty;
 	}
 	if (qm_mr_current(__p) != NULL) {
 		/* special handling, drain just in case it's a few FQRNIs */
 		if (drain_mr_fqrni(__p)) {
-			pr_err("Qman MR unclean, need recovery\n");
+			pr_err("Qman MR unclean\n");
 			goto fail_dqrr_mr_empty;
 		}
 	}
@@ -535,8 +464,9 @@ drain_loop:
 	affine_channels[config->public_cfg.cpu] = config->public_cfg.channel;
 	spin_unlock(&affine_mask_lock);
 	qm_isr_disable_write(__p, 0);
+	qm_isr_uninhibit(__p);
 	/* Write a sane SDQCR */
-	qm_dqrr_sdqcr_set(__p, recovery_mode ? 0 : portal->sdqcr);
+	qm_dqrr_sdqcr_set(__p, portal->sdqcr);
 	return portal;
 fail_dqrr_mr_empty:
 fail_eqcr_empty:
@@ -560,7 +490,6 @@ fail_mr:
 fail_dqrr:
 	qm_eqcr_finish(__p);
 fail_eqcr:
-	put_affine_portal();
 	return NULL;
 }
 
@@ -672,8 +601,6 @@ static u32 __poll_portal_slow(struct qman_portal *p, u32 is)
 {
 	const struct qm_mr_entry *msg;
 
-	BUG_ON(p->bits & PORTAL_BITS_RECOVERY);
-
 	if (is & QM_PIRQ_CSCI) {
 		struct qman_cgrs rr, c;
 		struct qm_mc_result *mcr;
@@ -838,7 +765,6 @@ static inline unsigned int __poll_portal_fast(struct qman_portal *p,
 	enum qman_cb_dqrr_result res;
 	unsigned int limit = 0;
 
-	BUG_ON(p->bits & PORTAL_BITS_RECOVERY);
 loop:
 	qm_dqrr_pvb_update(&p->p);
 	dq = qm_dqrr_current(&p->p);
@@ -1056,186 +982,6 @@ done:
 }
 EXPORT_SYMBOL(qman_poll);
 
-/* Recovery processing. */
-static int recovery_poll_mr(struct qman_portal *p, u32 fqid)
-{
-	const struct qm_mr_entry *msg;
-	enum {
-		wait_for_fqrn,
-		wait_for_fqrl,
-		done
-	} state = wait_for_fqrn;
-	u8 v, fqs = 0;
-
-loop:
-	qm_mr_pvb_update(&p->p);
-	msg = qm_mr_current(&p->p);
-	if (!msg) {
-		cpu_relax();
-		goto loop;
-	}
-	v = msg->verb & QM_MR_VERB_TYPE_MASK;
-	/* all MR messages have "fqid" in the same place */
-	if (msg->fq.fqid != fqid) {
-ignore_msg:
-		pr_warning("recovery_poll_mr(), ignoring msg 0x%02x for "
-			"FQID %d\n", v, msg->fq.fqid);
-		goto next_msg;
-	}
-	if (state == wait_for_fqrn) {
-		if ((v != QM_MR_VERB_FQRN) && (v != QM_MR_VERB_FQRNI))
-			goto ignore_msg;
-		fqs = msg->fq.fqs;
-		if (!(fqs & QM_MR_FQS_ORLPRESENT))
-			state = done;
-		else
-			state = wait_for_fqrl;
-	} else {
-		if (v != QM_MR_VERB_FQRL)
-			goto ignore_msg;
-		state = done;
-	}
-next_msg:
-	qm_mr_next(&p->p);
-	qm_mr_cci_consume(&p->p, 1);
-	if (state != done)
-		goto loop;
-	return (fqs & QM_MR_FQS_NOTEMPTY) ? 1 : 0;
-}
-static unsigned int recovery_poll_dqrr(struct qman_portal *p, u32 fqid)
-{
-	const struct qm_dqrr_entry *dq;
-	u8 empty = 0, num_fds = 0;
-
-loop:
-	qm_dqrr_pvb_update(&p->p);
-	dq = qm_dqrr_current(&p->p);
-	if (!dq) {
-		cpu_relax();
-		goto loop;
-	}
-	if (!(dq->stat & QM_DQRR_STAT_UNSCHEDULED)) {
-ignore_dqrr:
-		pr_warning("recovery_poll_dqrr(), ignoring dqrr %02x:%02x "
-			"for FQID %d\n",
-			dq->verb & QM_DQRR_VERB_MASK, dq->stat, dq->fqid);
-		goto next_dqrr;
-	}
-	if (dq->fqid != fqid)
-		goto ignore_dqrr;
-	if (dq->stat & QM_DQRR_STAT_FD_VALID)
-		num_fds++;
-	if (dq->stat & QM_DQRR_STAT_FQ_EMPTY)
-		empty = 1;
-next_dqrr:
-	qm_dqrr_next(&p->p);
-	qm_dqrr_cci_consume(&p->p, 1);
-	if (!empty)
-		goto loop;
-	return num_fds;
-}
-int qman_recovery_cleanup_fq(u32 fqid)
-{
-	struct qm_mc_command *mcc;
-	struct qm_mc_result *mcr;
-	struct qman_portal *p = get_affine_portal();
-	unsigned long irqflags __maybe_unused;
-	int ret = 0;
-	unsigned int num_fds = 0;
-	const char *s;
-	u8 state;
-
-	/* Lock this whole flow down via the portal's "vdqcr" */
-	PORTAL_IRQ_LOCK(p, irqflags);
-	BUG_ON(!(p->bits & PORTAL_BITS_RECOVERY));
-	if (p->vdqcr_owned)
-		ret = -EBUSY;
-	else
-		p->vdqcr_owned = (void *)1;
-	PORTAL_IRQ_UNLOCK(p, irqflags);
-	if (ret)
-		goto out;
-
-	/* Query the FQ's state */
-	mcc = qm_mc_start(&p->p);
-	mcc->queryfq.fqid = fqid;
-	qm_mc_commit(&p->p, QM_MCC_VERB_QUERYFQ_NP);
-	while (!(mcr = qm_mc_result(&p->p)))
-		cpu_relax();
-	DPA_ASSERT((mcr->verb & QM_MCR_VERB_MASK) == QM_MCR_VERB_QUERYFQ_NP);
-	if (mcr->result != QM_MCR_RESULT_OK) {
-		ret = -EIO;
-		goto out;
-	}
-	state = mcr->queryfq_np.state & QM_MCR_NP_STATE_MASK;
-
-	/* OOS: nothing to do */
-	if (state == QM_MCR_NP_STATE_OOS)
-		goto out;
-	/* Otherwise: must be retired */
-	if (state != QM_MCR_NP_STATE_RETIRED) {
-		mcc = qm_mc_start(&p->p);
-		mcc->alterfq.fqid = fqid;
-		qm_mc_commit(&p->p, QM_MCC_VERB_ALTER_RETIRE);
-		while (!(mcr = qm_mc_result(&p->p)))
-			cpu_relax();
-		DPA_ASSERT((mcr->verb & QM_MCR_VERB_MASK) ==
-				QM_MCR_VERB_ALTER_RETIRE);
-		if ((mcr->result != QM_MCR_RESULT_OK) &&
-				(mcr->result != QM_MCR_RESULT_PENDING)) {
-			ret = -EIO;
-			goto out;
-		}
-		ret = recovery_poll_mr(p, fqid);
-		if (!ret)
-			/* FQ empty */
-			goto oos;
-	}
-	/* Drain till empty */
-	qm_dqrr_vdqcr_set(&p->p, fqid & 0x00ffffff);
-	num_fds = recovery_poll_dqrr(p, fqid);
-
-oos:
-	mcc = qm_mc_start(&p->p);
-	mcc->alterfq.fqid = fqid;
-	qm_mc_commit(&p->p, QM_MCC_VERB_ALTER_OOS);
-	while (!(mcr = qm_mc_result(&p->p)))
-		cpu_relax();
-	DPA_ASSERT((mcr->verb & QM_MCR_VERB_MASK) == QM_MCR_VERB_ALTER_OOS);
-	if (mcr->result != QM_MCR_RESULT_OK)
-		ret = -EIO;
-	/* done */
-	s = (state == QM_MCR_NP_STATE_RETIRED) ? "retired" :
-		(state == QM_MCR_NP_STATE_PARKED) ? "parked" : "scheduled";
-	pr_info("Qman: %s FQID %d recovered (%d frames)\n", s, fqid, num_fds);
-out:
-	PORTAL_IRQ_LOCK(p, irqflags);
-	p->vdqcr_owned = NULL;
-	PORTAL_IRQ_UNLOCK(p, irqflags);
-	put_affine_portal();
-	return ret;
-}
-EXPORT_SYMBOL(qman_recovery_cleanup_fq);
-
-/* called from qman_driver.c::qman_recovery_exit() only (if exporting, use
- * get_raw_affine_portal() and check for the "SLAVE" bit). */
-void qman_recovery_exit_local(void)
-{
-	struct qman_portal *p = get_affine_portal();
-	BUG_ON(!(p->bits & PORTAL_BITS_RECOVERY));
-	/* Reinitialise DQRR using expected settings */
-	qm_dqrr_finish(&p->p);
-	post_recovery(p, p->config);
-	clear_bits(PORTAL_BITS_RECOVERY, &p->bits);
-	if (qm_dqrr_init(&p->p, p->config, qm_dqrr_dpush, qm_dqrr_pvb,
-			QM_DQRR_CMODE, DQRR_MAXFILL))
-		panic("Qman DQRR initialisation failed, recovery broken");
-	qm_dqrr_sdqcr_set(&p->p, p->sdqcr);
-	qm_isr_status_clear(&p->p, 0xffffffff);
-	qm_isr_uninhibit(&p->p);
-	put_affine_portal();
-}
-
 void qman_stop_dequeues(void)
 {
 	struct qman_portal *p = get_affine_portal();
diff --git a/drivers/staging/fsl_qbman/qman_private.h b/drivers/staging/fsl_qbman/qman_private.h
index 6f5004b..84e4062 100644
--- a/drivers/staging/fsl_qbman/qman_private.h
+++ b/drivers/staging/fsl_qbman/qman_private.h
@@ -141,11 +141,9 @@ void qman_liodn_fixup(enum qm_channel channel);
 /* Hooks from qman_driver.c in to qman_high.c */
 struct qman_portal *qman_create_affine_portal(
 			const struct qm_portal_config *config,
-			const struct qman_cgrs *cgrs,
-			int recovery_mode);
+			const struct qman_cgrs *cgrs);
 struct qman_portal *qman_create_affine_slave(struct qman_portal *redirect);
 const struct qm_portal_config *qman_destroy_affine_portal(void);
-void qman_recovery_exit_local(void);
 
 /* This CGR feature is supported by h/w and required by unit-tests and the
  * debugfs hooks, so is implemented in the driver. However it allows an explicit
diff --git a/include/linux/fsl_bman.h b/include/linux/fsl_bman.h
index 1d312ea..cf4cbae 100644
--- a/include/linux/fsl_bman.h
+++ b/include/linux/fsl_bman.h
@@ -358,16 +358,6 @@ u32 bman_poll_slow(void);
 void bman_poll(void);
 
 /**
- * bman_recovery_cleanup_bpid  - in recovery mode, cleanup a buffer pool
- */
-int bman_recovery_cleanup_bpid(u32 bpid);
-
-/**
- * bman_recovery_exit - leave recovery mode
- */
-void bman_recovery_exit(void);
-
-/**
  * bman_rcr_is_empty - Determine if portal's RCR is empty
  *
  * For use in situations where a cpu-affine caller needs to determine when all
diff --git a/include/linux/fsl_qman.h b/include/linux/fsl_qman.h
index 9014c42..223067b 100644
--- a/include/linux/fsl_qman.h
+++ b/include/linux/fsl_qman.h
@@ -1285,16 +1285,6 @@ u32 qman_poll_slow(void);
 void qman_poll(void);
 
 /**
- * qman_recovery_cleanup_fq - in recovery mode, cleanup a FQ of unknown state
- */
-int qman_recovery_cleanup_fq(u32 fqid);
-
-/**
- * qman_recovery_exit - leave recovery mode
- */
-void qman_recovery_exit(void);
-
-/**
  * qman_stop_dequeues - Stop h/w dequeuing to the s/w portal
  *
  * Disables DQRR processing of the portal. This is reference-counted, so
-- 
1.7.9.7

