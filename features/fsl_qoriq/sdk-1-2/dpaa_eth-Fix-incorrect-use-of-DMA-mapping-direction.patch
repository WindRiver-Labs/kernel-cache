From c579ce4d8fa459ac111dfa0518f5e5245325e51c Mon Sep 17 00:00:00 2001
From: Ioana Radulescu <ruxandra.radulescu@freescale.com>
Date: Wed, 28 Mar 2012 15:56:30 +0000
Subject: [PATCH 104/128] dpaa_eth: Fix incorrect use of DMA mapping direction

Tx buffers were being mapped with DMA_TO_DEVICE flag, which was
incorrect in the case of recycling, when those buffers ended up
back in the buffer pool and were re-used on Rx.

This patch introduces a bidirectional mapping for recycled buffers.
Newly allocated buffers which are used to populate the pool are now
also mapped this way, to avoid any inconsistencies when doing the
unmapping on Rx.

Slight performance decrease in forwarding scenarios (<2%).

Signed-off-by: Ioana Radulescu <ruxandra.radulescu@freescale.com>
[Kevin: Original patch taken from FSL
QorIQ-SDK-V1.2-SOURCE-20120614-yocto.iso image]
Signed-off-by: Kevin Hao <kexin.hao@windriver.com>
---
 drivers/net/ethernet/freescale/dpa/dpaa_eth.c |   34 +++++++++++++++++++------
 1 file changed, 26 insertions(+), 8 deletions(-)

diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth.c b/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
index 7448656..66b4d6a 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
@@ -222,8 +222,15 @@ static void dpa_bp_add_8(struct dpa_bp *dpa_bp)
 		skbh = (struct sk_buff **)(skb->head + pad);
 		*skbh = skb;
 
+		/*
+		 * Here we need to map only for device write (DMA_FROM_DEVICE),
+		 * but on Tx recycling we may also get buffers in the pool that
+		 * are mapped bidirectionally.
+		 * Use DMA_BIDIRECTIONAL here as well to avoid any
+		 * inconsistencies when unmapping.
+		 */
 		addr = dma_map_single(dpa_bp->dev, skb->head + pad,
-				dpa_bp->size, DMA_FROM_DEVICE);
+				dpa_bp->size, DMA_BIDIRECTIONAL);
 		if (unlikely(dma_mapping_error(dpa_bp->dev, addr))) {
 			dpaa_eth_err(dpa_bp->dev, "DMA mapping failed");
 			break;
@@ -415,7 +422,7 @@ _dpa_bp_free(struct dpa_bp *dpa_bp)
 				struct sk_buff *skb;
 
 				dma_unmap_single(bp->dev, addr, bp->size,
-						DMA_FROM_DEVICE);
+						DMA_BIDIRECTIONAL);
 
 				skb = *skbh;
 				dev_kfree_skb_any(skb);
@@ -629,8 +636,15 @@ static void _dpa_cleanup_tx_fd(const struct dpa_priv_s *priv,
 		return;
 
 	if (fd->format == qm_fd_contig)
-		/* For contiguous frames, just unmap data buffer */
-		dma_unmap_single(bp->dev, addr, bp->size, DMA_TO_DEVICE);
+		/* For contiguous frames, just unmap data buffer;
+		 * mapping direction depends on whether the frame was
+		 * meant to be recycled or not */
+		if (fd->cmd & FM_FD_CMD_FCO)
+			dma_unmap_single(bp->dev, addr, bp->size,
+					 DMA_BIDIRECTIONAL);
+		else
+			dma_unmap_single(bp->dev, addr, bp->size,
+					 DMA_TO_DEVICE);
 	else {
 		/* For s/g, we need to unmap both the SGT buffer and the
 		 * data buffer, and also free the SGT buffer */
@@ -822,7 +836,7 @@ dpa_csum_validation(const struct dpa_priv_s	*priv,
 	if (unlikely(!frm))
 		return;
 
-	dma_unmap_single(dpa_bp->dev, addr, dpa_bp->size, DMA_FROM_DEVICE);
+	dma_unmap_single(dpa_bp->dev, addr, dpa_bp->size, DMA_BIDIRECTIONAL);
 
 	parse_result = (t_FmPrsResult *)(frm + DPA_PRIV_DATA_SIZE);
 
@@ -909,7 +923,7 @@ static void __hot _dpa_rx(struct net_device *net_dev,
 
 	dpa_bp = dpa_bpid2pool(fd->bpid);
 
-	dma_unmap_single(dpa_bp->dev, addr, dpa_bp->size, DMA_FROM_DEVICE);
+	dma_unmap_single(dpa_bp->dev, addr, dpa_bp->size, DMA_BIDIRECTIONAL);
 
 	skb = *skbh;
 	prefetch(skb);
@@ -1349,6 +1363,7 @@ static int skb_to_contig_fd(struct dpa_priv_s *priv,
 	struct dpa_bp *dpa_bp;
 	unsigned int pad;
 	struct net_device *net_dev = priv->net_dev;
+	enum dma_data_direction dma_dir = DMA_TO_DEVICE;
 	int err;
 
 	/*
@@ -1368,7 +1383,7 @@ static int skb_to_contig_fd(struct dpa_priv_s *priv,
 
 	/* Enable L3/L4 hardware checksum computation.
 	 *
-	 * We must do this before dma_map_single(DMA_TO_DEVICE), because we may
+	 * We must do this before dma_map_single(), because we may
 	 * need to write into the skb. */
 	err = dpa_enable_tx_csum(priv, skb, fd,
 				 ((char *)skbh) + DPA_PRIV_DATA_SIZE);
@@ -1395,9 +1410,12 @@ static int skb_to_contig_fd(struct dpa_priv_s *priv,
 			&& (IS_ALIGNED((unsigned long)skbh, 16)))) {
 		fd->cmd |= FM_FD_CMD_FCO;
 		fd->bpid = dpa_bp->bpid;
+		/* Since the buffer will get back to the Bman pool
+		 * and be re-used on Rx, map it for both read and write */
+		dma_dir = DMA_BIDIRECTIONAL;
 	}
 
-	addr = dma_map_single(dpa_bp->dev, skbh, dpa_bp->size, DMA_TO_DEVICE);
+	addr = dma_map_single(dpa_bp->dev, skbh, dpa_bp->size, dma_dir);
 	if (unlikely(dma_mapping_error(dpa_bp->dev, addr))) {
 		if (netif_msg_tx_err(priv)  && net_ratelimit())
 			cpu_netdev_err(net_dev, "dma_map_single() failed\n");
-- 
1.7.9.7

