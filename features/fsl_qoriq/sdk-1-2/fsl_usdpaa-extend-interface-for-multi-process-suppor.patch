From 546c7de20f306d01a5b86863da2e6fe873fdf719 Mon Sep 17 00:00:00 2001
From: Geoff Thorpe <Geoff.Thorpe@freescale.com>
Date: Wed, 14 Mar 2012 04:49:52 +0000
Subject: [PATCH 086/121] fsl_usdpaa: extend interface for multi-process
 support

This change enhances the ioctl interface of the fsl_usdpaa device to
support multi-process USDPAA scenarios. Multiple DMA regions can be
mapped, with support for process-private (and unnamed) maps and
cross-process (and named) maps. The latter also supports cross-process,
per-region locks to allow user-space processes to synchronise their
manipulations of shared regions (the user-space drivers use this
facility for implementing buffer allocators within such regions).

This change also updates the "hugetlb-like" TLB1 hack to support such
configurations. In particular, if the boot-arg has a comma-separated
second parameter, this will instruct the driver to reserve more than one
TLB1 entry for its fault-handling. Eg. with "usdpaa_mem=64M,4" a 64M
region will be reserved and 4 TLB1 entries will be available to map it
or subsets of it, potentially from multiple processes. More than 4
distinct mappings could be made simultaneously (from one process or
many), at the risk of significant performance degradation unless no more
than 4 mappings were being regularly accessed by software.

Signed-off-by: Geoff Thorpe <Geoff.Thorpe@freescale.com>
[Kevin: Original patch taken from FSL
QorIQ-SDK-V1.2-SOURCE-20120614-yocto.iso image]
Signed-off-by: Kevin Hao <kexin.hao@windriver.com>
---
 arch/powerpc/mm/mem.c      |   28 ++-
 drivers/misc/fsl_usdpaa.c  |  484 ++++++++++++++++++++++++++++++++++++++------
 include/linux/fsl_usdpaa.h |   76 +++++--
 3 files changed, 492 insertions(+), 96 deletions(-)

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 5d54c4e..b5fdd5f 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -525,27 +525,26 @@ EXPORT_SYMBOL(flush_icache_user_range);
 
 #ifdef CONFIG_FSL_USDPAA
 /*
- * NB: this 'usdpaa' check+hack is to create a single TLB1 entry to cover the
- * buffer memory used by run-to-completion UIO-based apps ("User-Space DataPath
+ * NB: this 'usdpaa' check+hack is to create TLB1 entries to cover the buffer
+ * memory used by run-to-completion UIO-based apps ("User-Space DataPath
  * Acceleration Architecture"). It is expected to be phased out once HugeTLB
- * support is hooked up. The other half of this hack is in
- * drivers/misc/fsl_shmem.c.
+ * support is hooked up with support for physical address conversion. The other
+ * half of this hack is in drivers/misc/fsl_usdpaa.c.
  */
 static inline void hook_usdpaa_tlb1(struct vm_area_struct *vma,
-				unsigned long address, pte_t *ptep)
+				    unsigned long address, pte_t *ptep)
 {
 	unsigned long pfn = pte_pfn(*ptep);
-	if ((pfn < (usdpaa_pfn_start + usdpaa_pfn_len)) &&
-	    (pfn >= usdpaa_pfn_start)) {
-		unsigned long va = address & ~(usdpaa_phys_size - 1);
+	u64 phys_addr;
+	u64 size;
+	int tlb_idx = usdpaa_test_fault(pfn, &phys_addr, &size);
+	if (tlb_idx != -1) {
+		unsigned long va = address & ~(size - 1);
 		flush_tlb_mm(vma->vm_mm);
-		settlbcam(usdpaa_tlbcam_index, va,
-			usdpaa_phys_start, usdpaa_phys_size,
-			pte_val(*ptep), mfspr(SPRN_PID));
+		settlbcam(tlb_idx, va, phys_addr, size, pte_val(*ptep),
+			  mfspr(SPRN_PID));
 	}
 }
-#else
-#define hook_usdpaa_tlb1(a, b, c) do { ; } while (0)
 #endif
 
 /*
@@ -581,8 +580,7 @@ void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
 	else if (trap != 0x300)
 		return;
 	hash_preload(vma->vm_mm, address, access, trap);
-#else
-
+#elif defined(CONFIG_FSL_USDPAA)
 	hook_usdpaa_tlb1(vma, address, ptep);
 
 #endif /* CONFIG_PPC_STD_MMU */
diff --git a/drivers/misc/fsl_usdpaa.c b/drivers/misc/fsl_usdpaa.c
index 1ab8254..95b0d06 100644
--- a/drivers/misc/fsl_usdpaa.c
+++ b/drivers/misc/fsl_usdpaa.c
@@ -21,20 +21,58 @@
 #include <linux/module.h>
 #include <linux/slab.h>
 
-/* Physical address range */
-u64 usdpaa_phys_start;
-u64 usdpaa_phys_size;
+/* Physical address range of the memory reservation, exported for mm/mem.c */
+static u64 phys_start;
+static u64 phys_size;
+/* PFN versions of the above */
+static unsigned long pfn_start;
+static unsigned long pfn_size;
 
-/* PFN versions */
-unsigned long usdpaa_pfn_start;
-unsigned long usdpaa_pfn_len;
+/* Memory reservations are manipulated under this spinlock (which is why 'refs'
+ * isn't atomic_t). */
+static DEFINE_SPINLOCK(mem_lock);
 
-/* TLB1 index */
-unsigned int usdpaa_tlbcam_index;
+/* The range of TLB1 indices */
+static unsigned int first_tlb;
+static unsigned int num_tlb;
+static unsigned int current_tlb; /* loops around for fault handling */
+
+/* Memory reservation is represented as a list of 'mem_fragment's, some of which
+ * may be mapped. Unmapped fragments are always merged where possible. */
+static LIST_HEAD(mem_list);
+
+struct mem_mapping;
+
+/* Memory fragments are in 'mem_list'. */
+struct mem_fragment {
+	u64 base;
+	u64 len;
+	unsigned long pfn_base; /* PFN version of 'base' */
+	unsigned long pfn_len; /* PFN version of 'len' */
+	unsigned int refs; /* zero if unmapped */
+	struct list_head list;
+	/* if mapped, flags+name captured at creation time */
+	u32 flags;
+	char name[USDPAA_DMA_NAME_MAX];
+	/* support multi-process locks per-memory-fragment. */
+	int has_locking;
+	wait_queue_head_t wq;
+	struct mem_mapping *owner;
+};
+
+/* Mappings of memory fragments in 'struct ctx'. These are created from
+ * ioctl(USDPAA_IOCTL_DMA_MAP), though the actual mapping then happens via a
+ * mmap(). */
+struct mem_mapping {
+	struct mem_fragment *frag;
+	struct list_head list;
+};
 
 /* Per-FD state (which should also be per-process but we don't enforce that) */
 struct ctx {
+	/* Allocated resources get put here for accounting */
 	struct dpa_alloc ids[usdpaa_id_max];
+	struct list_head maps;
 };
 
 /* Different resource classes */
@@ -74,6 +112,121 @@ static const struct alloc_backend {
 	}
 };
 
+/* Helper for ioctl_dma_map() when we have a larger fragment than we need. This
+ * splits the fragment into 4 and returns the upper-most. (The caller can loop
+ * until it has a suitable fragment size.) */
+static struct mem_fragment *split_frag(struct mem_fragment *frag)
+{
+	struct mem_fragment *x[3];
+	x[0] = kmalloc(sizeof(struct mem_fragment), GFP_KERNEL);
+	x[1] = kmalloc(sizeof(struct mem_fragment), GFP_KERNEL);
+	x[2] = kmalloc(sizeof(struct mem_fragment), GFP_KERNEL);
+	if (!x[0] || !x[1] || !x[2]) {
+		kfree(x[0]);
+		kfree(x[1]);
+		kfree(x[2]);
+		return NULL;
+	}
+	BUG_ON(frag->refs);
+	frag->len >>= 2;
+	frag->pfn_len >>= 2;
+	x[0]->base = frag->base + frag->len;
+	x[1]->base = x[0]->base + frag->len;
+	x[2]->base = x[1]->base + frag->len;
+	x[0]->len = x[1]->len = x[2]->len = frag->len;
+	x[0]->pfn_base = frag->pfn_base + frag->pfn_len;
+	x[1]->pfn_base = x[0]->pfn_base + frag->pfn_len;
+	x[2]->pfn_base = x[1]->pfn_base + frag->pfn_len;
+	x[0]->pfn_len = x[1]->pfn_len = x[2]->pfn_len = frag->pfn_len;
+	x[0]->refs = x[1]->refs = x[2]->refs = 0;
+	list_add(&x[0]->list, &frag->list);
+	list_add(&x[1]->list, &x[0]->list);
+	list_add(&x[2]->list, &x[1]->list);
+	return x[2];
+}
+
+/* Conversely, when a fragment is released we look to see whether its
+ * similarly-split siblings are free to be reassembled. */
+static struct mem_fragment *merge_frag(struct mem_fragment *frag)
+{
+	/* If this fragment can be merged with its siblings, it will have
+	 * newbase and newlen as its geometry. */
+	uint64_t newlen = frag->len << 2;
+	uint64_t newbase = frag->base & ~(newlen - 1);
+	struct mem_fragment *tmp, *leftmost = frag, *rightmost = frag;
+	/* Scan left until we find the start */
+	tmp = list_entry(frag->list.prev, struct mem_fragment, list);
+	while ((&tmp->list != &mem_list) && (tmp->base >= newbase)) {
+		if (tmp->refs)
+			return NULL;
+		if (tmp->len != tmp->len)
+			return NULL;
+		leftmost = tmp;
+		tmp = list_entry(tmp->list.prev, struct mem_fragment, list);
+	}
+	/* Scan right until we find the end */
+	tmp = list_entry(frag->list.next, struct mem_fragment, list);
+	while ((&tmp->list != &mem_list) && (tmp->base < (newbase + newlen))) {
+		if (tmp->refs)
+			return NULL;
+		if (tmp->len != tmp->len)
+			return NULL;
+		rightmost = tmp;
+		tmp = list_entry(tmp->list.next, struct mem_fragment, list);
+	}
+	if (leftmost == rightmost)
+		return NULL;
+	/* OK, we can merge */
+	frag = leftmost;
+	frag->len = newlen;
+	frag->pfn_len = newlen >> PAGE_SHIFT;
+	while (1) {
+		int lastone;
+		tmp = list_entry(frag->list.next, struct mem_fragment, list);
+		lastone = (tmp == rightmost);
+		if (&tmp->list == &mem_list)
+			break;
+		list_del(&tmp->list);
+		kfree(tmp);
+		if (lastone)
+			break;
+	}
+	return frag;
+}
+
+/* Helper to verify that 'sz' is (4096 * 4^x) for some x. */
+static int is_good_size(u64 sz)
+{
+	int log = ilog2(phys_size);
+	if ((phys_size & (phys_size - 1)) || (log < 12) || (log & 1))
+		return 0;
+	return 1;
+}
+
+/* Hook from arch/powerpc/mm/mem.c */
+int usdpaa_test_fault(unsigned long pfn, u64 *phys_addr, u64 *size)
+{
+	struct mem_fragment *frag;
+	int idx = -1;
+	if ((pfn < pfn_start) || (pfn >= (pfn_start + pfn_size)))
+		return -1;
+	/* It's in-range, we need to find the fragment */
+	spin_lock(&mem_lock);
+	list_for_each_entry(frag, &mem_list, list) {
+		if ((pfn >= frag->pfn_base) && (pfn < (frag->pfn_base +
+						       frag->pfn_len))) {
+			*phys_addr = frag->base;
+			*size = frag->len;
+			idx = current_tlb++;
+			if (current_tlb >= (first_tlb + num_tlb))
+				current_tlb = first_tlb;
+			break;
+		}
+	}
+	spin_unlock(&mem_lock);
+	return idx;
+}
+
 static int usdpaa_open(struct inode *inode, struct file *filp)
 {
 	const struct alloc_backend *backend = &alloc_backends[0];
@@ -87,6 +240,8 @@ static int usdpaa_open(struct inode *inode, struct file *filp)
 		backend++;
 	}
 
+	INIT_LIST_HEAD(&ctx->maps);
+
 	filp->f_mapping->backing_dev_info = &directly_mappable_cdev_bdi;
 
 	return 0;
@@ -95,6 +250,7 @@ static int usdpaa_open(struct inode *inode, struct file *filp)
 static int usdpaa_release(struct inode *inode, struct file *filp)
 {
 	struct ctx *ctx = filp->private_data;
+	struct mem_mapping *map, *tmp;
 	const struct alloc_backend *backend = &alloc_backends[0];
 	while (backend->id_type != usdpaa_id_max) {
 		int ret, leaks = 0;
@@ -112,16 +268,49 @@ static int usdpaa_release(struct inode *inode, struct file *filp)
 				backend->acronym, (leaks > 1) ? "s" : "");
 		backend++;
 	}
+	spin_lock(&mem_lock);
+	list_for_each_entry_safe(map, tmp, &ctx->maps, list) {
+		if (map->frag->has_locking && (map->frag->owner == map)) {
+			map->frag->owner = NULL;
+			wake_up(&map->frag->wq);
+		}
+		if (!--map->frag->refs) {
+			struct mem_fragment *frag = map->frag;
+			do {
+				frag = merge_frag(frag);
+			} while (frag);
+		}
+		list_del(&map->list);
+		kfree(map);
+	}
+	spin_unlock(&mem_lock);
 	kfree(ctx);
 	return 0;
 }
 
-static int usdpaa_mmap(struct file *file, struct vm_area_struct *vma)
+static int usdpaa_mmap(struct file *filp, struct vm_area_struct *vma)
 {
-	if (remap_pfn_range(vma, vma->vm_start,	usdpaa_pfn_start,
-			    vma->vm_end - vma->vm_start, vma->vm_page_prot))
-		return -EAGAIN;
-	return 0;
+	struct ctx *ctx = filp->private_data;
+	struct mem_mapping *map;
+	int ret = 0;
+
+	spin_lock(&mem_lock);
+	list_for_each_entry(map, &ctx->maps, list) {
+		if (map->frag->pfn_base == vma->vm_pgoff)
+			goto map_match;
+	}
+	spin_unlock(&mem_lock);
+	return -ENOMEM;
+
+map_match:
+	if (map->frag->len != (vma->vm_end - vma->vm_start))
+		ret = -EINVAL;
+	spin_unlock(&mem_lock);
+	if (!ret)
+		ret = remap_pfn_range(vma, vma->vm_start, map->frag->pfn_base,
+				      vma->vm_end - vma->vm_start,
+				      vma->vm_page_prot);
+	return ret;
 }
 
 /* Return the nearest rounded-up address >= 'addr' that is 'sz'-aligned. 'sz'
@@ -138,22 +327,13 @@ static unsigned long usdpaa_get_unmapped_area(struct file *file,
 					      unsigned long pgoff,
 					      unsigned long flags)
 {
-	struct mm_struct *mm = current->mm;
 	struct vm_area_struct *vma;
 
-	if (pgoff) {
-		pr_err("%s: non-zero mmap page-offset 0x%lx is invalid\n",
-			__func__, pgoff);
-		return -EINVAL;
-	}
-	/* Only support mappings of the right size */
-	if (len != usdpaa_phys_size) {
-		pr_err("%s: mmap size 0x%lx doesn't match region (0x%llx)\n",
-			__func__, len, usdpaa_phys_size);
+	if (!is_good_size(len))
 		return -EINVAL;
-	}
+
 	addr = USDPAA_MEM_ROUNDUP(addr, len);
-	vma = find_vma(mm, addr);
+	vma = find_vma(current->mm, addr);
 	/* Keep searching until we reach the end of currently-used virtual
 	 * address-space or we find a big enough gap. */
 	while (vma) {
@@ -167,19 +347,9 @@ static unsigned long usdpaa_get_unmapped_area(struct file *file,
 	return addr;
 }
 
-static long ioctl_get_region(void __user *arg)
-{
-	struct usdpaa_ioctl_get_region i = {
-		.phys_start = usdpaa_phys_start,
-		.phys_len = usdpaa_phys_size
-	};
-	return copy_to_user(arg, &i, sizeof(i));
-}
-
-static long ioctl_id_alloc(struct file *fp, void __user *arg)
+static long ioctl_id_alloc(struct ctx *ctx, void __user *arg)
 {
 	struct usdpaa_ioctl_id_alloc i;
-	struct ctx *ctx = fp->private_data;
 	const struct alloc_backend *backend;
 	int ret = copy_from_user(&i, arg, sizeof(i));
 	if (ret)
@@ -203,10 +373,9 @@ static long ioctl_id_alloc(struct file *fp, void __user *arg)
 	return 0;
 }
 
-static long ioctl_id_release(struct file *fp, void __user *arg)
+static long ioctl_id_release(struct ctx *ctx, void __user *arg)
 {
 	struct usdpaa_ioctl_id_release i;
-	struct ctx *ctx = fp->private_data;
 	const struct alloc_backend *backend;
 	int ret = copy_from_user(&i, arg, sizeof(i));
 	if (ret)
@@ -223,16 +392,183 @@ static long ioctl_id_release(struct file *fp, void __user *arg)
 	backend->release(i.base, i.num);
 	return 0;
 }
+
+static long ioctl_dma_map(struct ctx *ctx, void __user *arg)
+{
+	struct usdpaa_ioctl_dma_map i;
+	struct mem_fragment *frag;
+	struct mem_mapping *map, *tmp;
+	u64 search_size;
+	int ret = copy_from_user(&i, arg, sizeof(i));
+	if (ret)
+		return ret;
+	if (i.len && !is_good_size(i.len))
+		return -EINVAL;
+	map = kmalloc(sizeof(*map), GFP_KERNEL);
+	if (!map)
+		return -ENOMEM;
+	spin_lock(&mem_lock);
+	if (i.flags & USDPAA_DMA_FLAG_SHARE) {
+		list_for_each_entry(frag, &mem_list, list) {
+			if (frag->refs && (frag->flags &
+					   USDPAA_DMA_FLAG_SHARE) &&
+					!strncmp(i.name, frag->name,
+						 USDPAA_DMA_NAME_MAX)) {
+				/* Matching entry */
+				if ((i.flags & USDPAA_DMA_FLAG_CREATE) &&
+				    !(i.flags & USDPAA_DMA_FLAG_LAZY)) {
+					ret = -EBUSY;
+					goto out;
+				}
+				list_for_each_entry(tmp, &ctx->maps, list)
+					if (tmp->frag == frag) {
+						ret = -EBUSY;
+						goto out;
+					}
+				i.has_locking = frag->has_locking;
+				i.did_create = 0;
+				i.len = frag->len;
+				goto do_map;
+			}
+		}
+		/* No matching entry */
+		if (!(i.flags & USDPAA_DMA_FLAG_CREATE)) {
+			ret = -ENOMEM;
+			goto out;
+		}
+	}
+	/* New fragment required, size must be provided. */
+	if (!i.len) {
+		ret = -EINVAL;
+		goto out;
+	}
+	/* We search for the required size and if that fails, for the next
+	 * biggest size, etc. */
+	for (search_size = i.len; search_size <= phys_size; search_size <<= 2) {
+		list_for_each_entry(frag, &mem_list, list) {
+			if (!frag->refs && (frag->len == search_size)) {
+				while (frag->len > i.len) {
+					frag = split_frag(frag);
+					if (!frag) {
+						ret = -ENOMEM;
+						goto out;
+					}
+				}
+				frag->flags = i.flags;
+				strncpy(frag->name, i.name,
+					USDPAA_DMA_NAME_MAX);
+				frag->has_locking = i.has_locking;
+				init_waitqueue_head(&frag->wq);
+				frag->owner = NULL;
+				i.did_create = 1;
+				goto do_map;
+			}
+		}
+	}
+	ret = -ENOMEM;
+	goto out;
+
+do_map:
+	map->frag = frag;
+	frag->refs++;
+	list_add(&map->list, &ctx->maps);
+	i.pa_offset = frag->base;
+
+out:
+	spin_unlock(&mem_lock);
+	if (!ret)
+		ret = copy_to_user(arg, &i, sizeof(i));
+	else
+		kfree(map);
+	return ret;
+}
+
+static int test_lock(struct mem_mapping *map)
+{
+	int ret = 0;
+	spin_lock(&mem_lock);
+	if (!map->frag->owner) {
+		map->frag->owner = map;
+		ret = 1;
+	}
+	spin_unlock(&mem_lock);
+	return ret;
+}
+
+static long ioctl_dma_lock(struct ctx *ctx, void __user *arg)
+{
+	struct mem_mapping *map;
+	struct vm_area_struct *vma;
+
+	down_read(&current->mm->mmap_sem);
+	vma = find_vma(current->mm, (unsigned long)arg);
+	if (!vma || (vma->vm_start > (unsigned long)arg)) {
+		up_read(&current->mm->mmap_sem);
+		return -EFAULT;
+	}
+	spin_lock(&mem_lock);
+	list_for_each_entry(map, &ctx->maps, list) {
+		if (map->frag->pfn_base == vma->vm_pgoff)
+			goto map_match;
+	}
+	map = NULL;
+map_match:
+	spin_unlock(&mem_lock);
+	up_read(&current->mm->mmap_sem);
+
+	if (!map->frag->has_locking)
+		return -ENODEV;
+	return wait_event_interruptible(map->frag->wq, test_lock(map));
+}
+
+static long ioctl_dma_unlock(struct ctx *ctx, void __user *arg)
+{
+	struct mem_mapping *map;
+	struct vm_area_struct *vma;
+	int ret;
+
+	down_read(&current->mm->mmap_sem);
+	vma = find_vma(current->mm, (unsigned long)arg);
+	if (!vma || (vma->vm_start > (unsigned long)arg))
+		ret = -EFAULT;
+	else {
+		spin_lock(&mem_lock);
+		list_for_each_entry(map, &ctx->maps, list) {
+			if (map->frag->pfn_base == vma->vm_pgoff) {
+				if (!map->frag->has_locking)
+					ret = -ENODEV;
+				else if (map->frag->owner == map) {
+					map->frag->owner = NULL;
+					wake_up(&map->frag->wq);
+					ret = 0;
+				} else
+					ret = -EBUSY;
+				goto map_match;
+			}
+		}
+		ret = -EINVAL;
+map_match:
+		spin_unlock(&mem_lock);
+	}
+	up_read(&current->mm->mmap_sem);
+	return ret;
+}
+
 static long usdpaa_ioctl(struct file *fp, unsigned int cmd, unsigned long arg)
 {
+	struct ctx *ctx = fp->private_data;
 	void __user *a = (void __user *)arg;
 	switch (cmd) {
-	case USDPAA_IOCTL_GET_PHYS_BASE:
-		return ioctl_get_region(a);
 	case USDPAA_IOCTL_ID_ALLOC:
-		return ioctl_id_alloc(fp, a);
+		return ioctl_id_alloc(ctx, a);
 	case USDPAA_IOCTL_ID_RELEASE:
-		return ioctl_id_release(fp, a);
+		return ioctl_id_release(ctx, a);
+	case USDPAA_IOCTL_DMA_MAP:
+		return ioctl_dma_map(ctx, a);
+	case USDPAA_IOCTL_DMA_LOCK:
+		return ioctl_dma_lock(ctx, a);
+	case USDPAA_IOCTL_DMA_UNLOCK:
+		return ioctl_dma_unlock(ctx, a);
 	}
 	return -EINVAL;
 }
@@ -253,51 +589,75 @@ static struct miscdevice usdpaa_miscdev = {
 };
 
 /* Early-boot memory allocation. The boot-arg "usdpaa_mem=<x>" is used to
- * indicate how much memory (if any) to allocate during early boot. */
+ * indicate how much memory (if any) to allocate during early boot. If the
+ * format "usdpaa_mem=<x>,<y>" is used, then <y> will be interpreted as the
+ * number of TLB1 entries to reserve (default is 1). If there are more mappings
+ * than there are TLB1 entries, fault-handling will occur. */
 static __init int usdpaa_mem(char *arg)
 {
-	usdpaa_phys_size = memparse(arg, &arg);
+	phys_size = memparse(arg, &arg);
+	num_tlb = 1;
+	if (*arg == ',') {
+		unsigned long ul;
+		int err = kstrtoul(arg + 1, 0, &ul);
+		if (err < 0) {
+			num_tlb = 1;
+			pr_warning("ERROR, usdpaa_mem arg is invalid\n");
+		} else
+			num_tlb = (unsigned int)ul;
+	}
 	return 0;
 }
 early_param("usdpaa_mem", usdpaa_mem);
 
 __init void fsl_usdpaa_init_early(void)
 {
-	int log;
-	if (!usdpaa_phys_size) {
+	if (!phys_size) {
 		pr_info("No USDPAA memory, no 'usdpaa_mem' bootarg\n");
 		return;
 	}
-	/* Size must be 4^x * 4096, for some x */
-	log = ilog2(usdpaa_phys_size);
-	if ((usdpaa_phys_size & (usdpaa_phys_size - 1)) || (log < 12) ||
-			(log & 1)) {
+	if (!is_good_size(phys_size)) {
 		pr_err("'usdpaa_mem' bootarg must be 4096*4^x\n");
-		usdpaa_phys_size = 0;
+		phys_size = 0;
 		return;
 	}
-	usdpaa_phys_start = memblock_alloc(usdpaa_phys_size, usdpaa_phys_size);
-	if (usdpaa_phys_start) {
-		usdpaa_pfn_start = (usdpaa_phys_start >> PAGE_SHIFT);
-		usdpaa_pfn_len = (usdpaa_phys_size >> PAGE_SHIFT);
-		usdpaa_tlbcam_index = tlbcam_index++;
-		pr_info("USDPAA region at %llx:%llx\n",
-			usdpaa_phys_start, usdpaa_phys_size);
-	} else
+	phys_start = memblock_alloc(phys_size, phys_size);
+	if (!phys_start) {
 		pr_err("Failed to reserve USDPAA region (sz:%llx)\n",
-		       usdpaa_phys_size);
+		       phys_size);
+		return;
+	}
+	pfn_start = phys_start >> PAGE_SHIFT;
+	pfn_size = phys_size >> PAGE_SHIFT;
+	first_tlb = current_tlb = tlbcam_index;
+	tlbcam_index += num_tlb;
+	pr_info("USDPAA region at %llx:%llx(%lx:%lx), %d TLB1 entries)\n",
+		phys_start, phys_size, pfn_start, pfn_size, num_tlb);
 }
 
 static int __init usdpaa_init(void)
 {
+	struct mem_fragment *frag;
 	int ret;
 
 	pr_info("Freescale USDPAA process driver\n");
-
-	if (!usdpaa_phys_size) {
+	if (!phys_start) {
 		pr_warning("fsl-usdpaa: no region found\n");
 		return 0;
 	}
+	frag = kmalloc(sizeof(*frag), GFP_KERNEL);
+	if (!frag) {
+		pr_err("Failed to setup USDPAA memory accounting\n");
+		return -ENOMEM;
+	}
+	frag->base = phys_start;
+	frag->len = phys_size;
+	frag->pfn_base = pfn_start;
+	frag->pfn_len = pfn_size;
+	frag->refs = 0;
+	init_waitqueue_head(&frag->wq);
+	frag->owner = NULL;
+	list_add(&frag->list, &mem_list);
 	ret = misc_register(&usdpaa_miscdev);
 	if (ret)
 		pr_err("fsl-usdpaa: failed to register misc device\n");
diff --git a/include/linux/fsl_usdpaa.h b/include/linux/fsl_usdpaa.h
index 3b38a89..c5fed6f 100644
--- a/include/linux/fsl_usdpaa.h
+++ b/include/linux/fsl_usdpaa.h
@@ -27,10 +27,6 @@ enum usdpaa_id_type {
 	usdpaa_id_max /* <-- not a valid type, represents the number of types */
 };
 #define USDPAA_IOCTL_MAGIC 'u'
-struct usdpaa_ioctl_get_region {
-	uint64_t phys_start;
-	uint64_t phys_len;
-};
 struct usdpaa_ioctl_id_alloc {
 	uint32_t base; /* Return value, the start of the allocated range */
 	enum usdpaa_id_type id_type; /* what kind of resource(s) to allocate */
@@ -44,29 +40,71 @@ struct usdpaa_ioctl_id_release {
 	uint32_t base;
 	uint32_t num;
 };
-#define USDPAA_IOCTL_GET_PHYS_BASE \
-	_IOR(USDPAA_IOCTL_MAGIC, 0x01, struct usdpaa_ioctl_get_region)
+/* Maximum length for a map name, including NULL-terminator */
+#define USDPAA_DMA_NAME_MAX 16
+/* Flags for requesting DMA maps. Maps are private+unnamed or sharable+named.
+ * For a sharable and named map, specify _SHARED (whether creating one or
+ * binding to an existing one). If _SHARED is specified and _CREATE is not, then
+ * the mapping must already exist. If _SHARED and _CREATE are specified and the
+ * mapping doesn't already exist, it will be created. If _SHARED and _CREATE are
+ * specified and the mapping already exists, the mapping will fail unless _LAZY
+ * is specified. When mapping to a pre-existing sharable map, the length must be
+ * an exact match. Lengths must be a power-of-4 multiple of page size.
+ *
+ * Note that this does not actually map the memory to user-space, that is done
+ * by a subsequent mmap() using the page offset returned from this ioctl(). The
+ * ioctl() is what gives the process permission to do this, and a page-offset
+ * with which to do so.
+ */
+#define USDPAA_DMA_FLAG_SHARE    0x01
+#define USDPAA_DMA_FLAG_CREATE   0x02
+#define USDPAA_DMA_FLAG_LAZY     0x04
+struct usdpaa_ioctl_dma_map {
+	/* If the map succeeds, pa_offset is returned and can be used in a
+	 * subsequent call to mmap(). */
+	uint64_t pa_offset;
+	/* Input parameter, the length of the region to be created (or if
+	 * mapping an existing region, this must match it). Must be a power-of-4
+	 * multiple of page size. */
+	uint64_t len;
+	/* Input parameter, the USDPAA_DMA_FLAG_* settings. */
+	uint32_t flags;
+	/* If _FLAG_SHARE is specified, the name of the region to be created (or
+	 * of the existing mapping to use). */
+	char name[USDPAA_DMA_NAME_MAX];
+	/* If this ioctl() creates the mapping, this is an input parameter
+	 * stating whether the region supports locking. If mapping an existing
+	 * region, this is a return value indicating the same thing. */
+	int has_locking;
+	/* In the case of a successful map with _CREATE and _LAZY, this return
+	 * value indicates whether we created the mapped region or whether it
+	 * already existed. */
+	int did_create;
+};
 #define USDPAA_IOCTL_ID_ALLOC \
-	_IOWR(USDPAA_IOCTL_MAGIC, 0x02, struct usdpaa_ioctl_id_alloc)
+	_IOWR(USDPAA_IOCTL_MAGIC, 0x01, struct usdpaa_ioctl_id_alloc)
 #define USDPAA_IOCTL_ID_RELEASE \
-	_IOW(USDPAA_IOCTL_MAGIC, 0x03, struct usdpaa_ioctl_id_release)
+	_IOW(USDPAA_IOCTL_MAGIC, 0x02, struct usdpaa_ioctl_id_release)
+#define USDPAA_IOCTL_DMA_MAP \
+	_IOWR(USDPAA_IOCTL_MAGIC, 0x03, struct usdpaa_ioctl_dma_map)
+/* We implement a cross-process locking scheme per DMA map. Call this ioctl()
+ * with a mmap()'d address, and the process will (interruptible) sleep if the
+ * lock is already held by another process. Process destruction will
+ * automatically clean up any held locks. */
+#define USDPAA_IOCTL_DMA_LOCK \
+	_IOW(USDPAA_IOCTL_MAGIC, 0x04, unsigned char)
+#define USDPAA_IOCTL_DMA_UNLOCK \
+	_IOW(USDPAA_IOCTL_MAGIC, 0x05, unsigned char)
 
 #ifdef __KERNEL__
 
-/* Physical address range */
-extern u64 usdpaa_phys_start;
-extern u64 usdpaa_phys_size;
-
-/* PFN versions */
-extern unsigned long usdpaa_pfn_start;
-extern unsigned long usdpaa_pfn_len;
-
-/* TLB1 index */
-extern unsigned int usdpaa_tlbcam_index;
-
 /* Early-boot hook */
 void __init fsl_usdpaa_init_early(void);
 
+/* Fault-handling in arch/powerpc/mm/mem.c gives USDPAA an opportunity to detect
+ * faults within its ranges via this hook. */
+int usdpaa_test_fault(unsigned long pfn, u64 *phys_addr, u64 *size);
+
 #endif /* __KERNEL__ */
 
 #endif /* CONFIG_FSL_USDPAA */
-- 
1.7.9.7

