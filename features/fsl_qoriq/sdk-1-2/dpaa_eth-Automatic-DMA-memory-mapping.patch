From c42a62e84f4d76fcaa2c9a5c2bb0cf7b3af39dee Mon Sep 17 00:00:00 2001
From: Cristian Sovaiala <cristian.sovaiala@freescale.com>
Date: Wed, 4 Apr 2012 16:00:50 +0000
Subject: [PATCH 112/128] dpaa_eth: Automatic DMA memory mapping

In USDPAA scenario, DMA memory is mapped on the fly with
kmap_atomic on each incoming/outgoing frame. The number of
pages to map is calculated given the size of the frame

On TX side we rely on the fact that parse_results structure
always resides in the first page, thus csum calculation is only
made when the first page is mapped

The physical address of the buffer pools is checked and if it
equals zero then skip buffers mapping and seeding as USDPAA will
handle this

Given that in USDPAA scenario the buffers are allocated and
seeded from user space, the virtual address of the buffer pool
is null unless the memory is mapped by the driver so we can rely
on this to split the path of execution into USDPAA and HV

Signed-off-by: Cristian Sovaiala <cristian.sovaiala@freescale.com>
[Kevin: Original patch taken from FSL
QorIQ-SDK-V1.2-SOURCE-20120614-yocto.iso image]
Signed-off-by: Kevin Hao <kexin.hao@windriver.com>
---
 drivers/net/ethernet/freescale/dpa/dpaa_eth.c |  128 ++++++++++++++++++++++---
 1 file changed, 115 insertions(+), 13 deletions(-)

diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth.c b/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
index a24a180..866f85c 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
@@ -319,6 +319,15 @@ static void dpaa_eth_seed_pool(struct dpa_bp *bp)
 
 static int dpa_make_shared_pool(struct dpa_bp *bp)
 {
+	/*
+	 * In MAC-less and Shared-MAC scenarios the physical
+	 * address of the buffer pool in device tree is set
+	 * to 0 to specify that another entity (USDPAA) will
+	 * allocate and seed the buffers
+	 */
+	if (!bp->paddr)
+		return 0;
+
 	devm_request_mem_region(bp->dev, bp->paddr, bp->size * bp->count,
 			KBUILD_MODNAME);
 	bp->vaddr = devm_ioremap_prot(bp->dev, bp->paddr,
@@ -392,7 +401,7 @@ dpa_bp_alloc(struct dpa_bp *dpa_bp)
 		if (!default_pool)
 			default_pool = dpa_bp;
 	} else {
-		err = dpa_make_shared_pool(dpa_bp);
+		err = dpa_make_shared_port_pool(dpa_bp);
 		if (err)
 			goto make_shared_pool_failed;
 	}
@@ -1252,7 +1261,11 @@ static int __hot dpa_shared_tx(struct sk_buff *skb, struct net_device *net_dev)
 	struct qm_fd fd;
 	int queue_mapping;
 	int err;
-	void *dpa_bp_vaddr;
+	void *dpa_bp_vaddr, *page_vaddr;
+	dma_addr_t addr_offset;
+	struct page *page;
+	unsigned int i, offset, no_pages;
+	size_t remain, size, total_size;
 
 	priv = netdev_priv(net_dev);
 	percpu_priv = per_cpu_ptr(priv->percpu_priv, smp_processor_id());
@@ -1283,15 +1296,62 @@ static int __hot dpa_shared_tx(struct sk_buff *skb, struct net_device *net_dev)
 	fd.addr_lo = bmb.lo;
 	fd.offset = DPA_BP_HEAD;
 
-	dpa_bp_vaddr = dpa_phys2virt(dpa_bp, bm_buf_addr(&bmb));
+	/*
+	 * The virtual address of the buffer pool is expected to be NULL
+	 * in scenarios like MAC-less or Shared-MAC between Linux and
+	 * USDPAA. In this case the buffers are dynamically mapped/unmapped.
+	 */
+	if (dpa_bp->vaddr) {
+		dpa_bp_vaddr = dpa_phys2virt(dpa_bp, bm_buf_addr(&bmb));
+
+		/* Copy the packet payload */
+		skb_copy_from_linear_data(skb,
+		                          dpa_bp_vaddr + dpa_fd_offset(&fd),
+		                          dpa_fd_length(&fd));
+
+		/* Enable L3/L4 hardware checksum computation, if applicable */
+		err = dpa_enable_tx_csum(priv, skb, &fd,
+		                         dpa_bp_vaddr + DPA_PRIV_DATA_SIZE);
 
-	/* Copy the packet payload */
-	skb_copy_from_linear_data(skb, dpa_bp_vaddr + dpa_fd_offset(&fd),
-		dpa_fd_length(&fd));
+		goto static_map;
+	}
+
+	remain = dpa_fd_offset(&fd) + dpa_fd_length(&fd);
+	no_pages = ((remain - 1) >> PAGE_SHIFT) + 1;
+	total_size = remain;
+	addr_offset = bm_buf_addr(&bmb);
+
+	for (i = 0; i < no_pages; i++) {
+		page = pfn_to_page(addr_offset >> PAGE_SHIFT);
+		offset = offset_in_page(addr_offset);
+		size = remain;
+
+		if (remain + offset > PAGE_SIZE) {
+			size = PAGE_SIZE - offset;
+			addr_offset += size;
+		}
+
+		page_vaddr = kmap_atomic(page);
+
+		if (i)
+			memcpy(page_vaddr + offset,
+			       skb->data + total_size - remain, size);
+		else {
+			memcpy(page_vaddr + dpa_fd_offset(&fd) + offset,
+			       skb->data + total_size - remain,
+			       size - dpa_fd_offset(&fd));
+
+			err = dpa_enable_tx_csum(priv, skb, &fd,
+			                         page_vaddr + offset +
+			                         DPA_PRIV_DATA_SIZE);
+		}
 
-	/* Enable L3/L4 hardware checksum computation, if applicable */
-	err = dpa_enable_tx_csum(priv, skb, &fd,
-		dpa_bp_vaddr + DPA_PRIV_DATA_SIZE);
+		kunmap_atomic(page_vaddr);
+
+		remain -= size;
+	}
+
+static_map:
 	if (unlikely(err < 0)) {
 		if (netif_msg_tx_err(priv) && net_ratelimit())
 			cpu_netdev_err(net_dev, "Tx HW csum error: %d\n", err);
@@ -1604,6 +1664,12 @@ shared_rx_dqrr(struct qman_portal *portal, struct qman_fq *fq,
 	struct dpa_bp *dpa_bp;
 	size_t size;
 	struct sk_buff *skb;
+	void *dpa_bp_vaddr;
+	dma_addr_t addr_offset;
+	struct page *page;
+	unsigned int i, offset, no_pages;
+	size_t remain, total_size;
+
 
 	net_dev = ((struct dpa_fq *)fq)->net_dev;
 	priv = netdev_priv(net_dev);
@@ -1647,11 +1713,47 @@ shared_rx_dqrr(struct qman_portal *portal, struct qman_fq *fq,
 
 	skb_reserve(skb, DPA_BP_HEAD);
 
-	/* Fill the SKB */
-	memcpy(skb_put(skb, dpa_fd_length(fd)),
-			dpa_phys2virt(dpa_bp, qm_fd_addr(fd)) +
-			dpa_fd_offset(fd), dpa_fd_length(fd));
+	if (dpa_bp->vaddr) {
+		/* Fill the SKB */
+		memcpy(skb_put(skb, dpa_fd_length(fd)),
+		       dpa_phys2virt(dpa_bp, qm_fd_addr(fd)) +
+		       dpa_fd_offset(fd), dpa_fd_length(fd));
+		goto static_map;
+	}
+
+	remain = dpa_fd_offset(fd) + dpa_fd_length(fd);
+	no_pages = ((remain - 1) >> PAGE_SHIFT) + 1;
+	total_size = remain;
+	addr_offset = qm_fd_addr(fd);
+
+	for (i = 0; i < no_pages; i++) {
+		page = pfn_to_page(addr_offset >> PAGE_SHIFT);
+		offset = offset_in_page(addr_offset);
+		size = remain;
+
+		if (remain + offset > PAGE_SIZE) {
+			size = PAGE_SIZE - offset;
+			addr_offset += size;
+		}
+
+		dpa_bp_vaddr = kmap_atomic(page);
+
+		if (i)
+			memcpy(skb_put(skb, size) + total_size - remain,
+			       dpa_bp_vaddr + offset, size);
+		else
+			memcpy(skb_put(skb, size - dpa_fd_offset(fd)) +
+			       total_size - remain,
+			       dpa_bp_vaddr + dpa_fd_offset(fd) +
+			       offset,
+			       size - dpa_fd_offset(fd));
+
+		kunmap_atomic(dpa_bp_vaddr);
+
+		remain -= size;
+	}
 
+static_map:
 	skb->protocol = eth_type_trans(skb, net_dev);
 
 	if (unlikely(skb->len > net_dev->mtu)) {
-- 
1.7.9.7

