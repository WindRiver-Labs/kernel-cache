From 915b3cb42df34e67e9d1d45191ca4868646a514f Mon Sep 17 00:00:00 2001
From: Ioana Radulescu <ruxandra.radulescu@freescale.com>
Date: Tue, 25 Oct 2011 11:28:46 +0300
Subject: [PATCH 008/128] dpaa_eth: Use blocking bman_release() function

Call bman_release with a WAIT flag, which guarantees the function
will eventually succeed (and in a reasonable timeframe).

Due to this change, dpa_fd_release() function cannot fail anymore,
so remove any checks to its return value.

Signed-off-by: Ioana Radulescu <ruxandra.radulescu@freescale.com>
[Kevin: Original patch taken from FSL
QorIQ-SDK-V1.2-SOURCE-20120614-yocto.iso image]
Signed-off-by: Kevin Hao <kexin.hao@windriver.com>
---
 drivers/net/ethernet/freescale/dpa/dpaa_eth.c |  101 +++++--------------------
 1 file changed, 18 insertions(+), 83 deletions(-)

diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth.c b/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
index 73e220c..da5ea81 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
@@ -187,26 +187,6 @@ static void dpa_bp_depletion(struct bman_portal	*portal,
 		pr_err("Invalid Pool depleted notification!\n");
 }
 
-static void bmb_free(struct dpa_bp *bp, struct bm_buffer *bmb)
-{
-	int i;
-	struct sk_buff **skbh;
-	struct sk_buff *skb;
-
-	for (i = 0; i < 8; i++) {
-		dma_addr_t addr = bm_buf_addr(&bmb[i]);
-		if (!addr)
-			break;
-
-		skbh = (struct sk_buff **)phys_to_virt(addr);
-		skb = *skbh;
-
-		dma_unmap_single(bp->dev, addr, bp->size, DMA_FROM_DEVICE);
-
-		dev_kfree_skb(skb);
-	}
-}
-
 static void dpa_bp_add_8(struct dpa_bp *dpa_bp)
 {
 	struct bm_buffer bmb[8];
@@ -214,7 +194,6 @@ static void dpa_bp_add_8(struct dpa_bp *dpa_bp)
 	dma_addr_t addr;
 	int i;
 	struct sk_buff *skb;
-	int err;
 	int *count_ptr;
 
 	count_ptr = per_cpu_ptr(dpa_bp->percpu_count, smp_processor_id());
@@ -251,12 +230,11 @@ static void dpa_bp_add_8(struct dpa_bp *dpa_bp)
 	/* Avoid releasing a completely null buffer; bman_release() requires
 	 * at least one buf. */
 	if (likely(i)) {
-		err = bman_release(dpa_bp->pool, bmb, i, 0);
+		/* Using the WAIT flag, bman_release() is guaranteed to succeed
+		 * in a reasonable amount of time*/
+		bman_release(dpa_bp->pool, bmb, i, BMAN_RELEASE_FLAG_WAIT);
 
-		if (unlikely(err < 0))
-			bmb_free(dpa_bp, bmb);
-		else
-			*count_ptr += i;
+		*count_ptr += i;
 	}
 }
 
@@ -305,8 +283,7 @@ static void dpaa_eth_seed_pool(struct dpa_bp *bp)
 
 		} while (--count && (num_bufs < 8));
 
-		while (bman_release(bp->pool, bufs, num_bufs, 0))
-			cpu_relax();
+		bman_release(bp->pool, bufs, num_bufs, BMAN_RELEASE_FLAG_WAIT);
 	}
 }
 
@@ -574,13 +551,13 @@ dpa_fd_offset(const struct qm_fd *fd)
 	return fd->offset;
 }
 
-static int __must_check __attribute__((nonnull))
+static void __attribute__((nonnull))
 dpa_fd_release(const struct net_device *net_dev, const struct qm_fd *fd)
 {
-	int				 _errno, __errno, i, j;
+	int				 i, j;
 	const struct dpa_priv_s		*priv;
 	const struct qm_sg_entry	*sgt;
-	struct dpa_bp		*_dpa_bp, *dpa_bp;
+	struct dpa_bp			*_dpa_bp, *dpa_bp;
 	struct bm_buffer		 _bmb, bmb[8];
 
 	priv = netdev_priv(net_dev);
@@ -591,7 +568,6 @@ dpa_fd_release(const struct net_device *net_dev, const struct qm_fd *fd)
 	_dpa_bp = dpa_bpid2pool(fd->bpid);
 	BUG_ON(IS_ERR(_dpa_bp));
 
-	_errno = 0;
 	if (fd->format == qm_fd_sg) {
 		sgt = (phys_to_virt(bm_buf_addr(&_bmb)) + dpa_fd_offset(fd));
 
@@ -611,28 +587,12 @@ dpa_fd_release(const struct net_device *net_dev, const struct qm_fd *fd)
 					!sgt[i-1].final &&
 					sgt[i-1].bpid == sgt[i].bpid);
 
-			__errno = bman_release(dpa_bp->pool, bmb, j, 0);
-			if (unlikely(__errno < 0)) {
-				if (netif_msg_drv(priv) && net_ratelimit())
-					cpu_netdev_err(net_dev,
-						"bman_release(%hu) = %d\n",
-						dpa_bp->bpid, _errno);
-				if (_errno >= 0)
-					_errno = __errno;
-			}
+			bman_release(dpa_bp->pool, bmb, j,
+				     BMAN_RELEASE_FLAG_WAIT);
 		} while (!sgt[i-1].final);
 	}
 
-	__errno = bman_release(_dpa_bp->pool, &_bmb, 1, 0);
-	if (unlikely(__errno < 0)) {
-		if (netif_msg_drv(priv) && net_ratelimit())
-			cpu_netdev_err(net_dev, "bman_release(%hu) = %d\n",
-					_dpa_bp->bpid, __errno);
-		if (_errno >= 0)
-			_errno = __errno;
-	}
-
-	return _errno;
+	bman_release(_dpa_bp->pool, &_bmb, 1, BMAN_RELEASE_FLAG_WAIT);
 }
 
 /* net_device */
@@ -821,19 +781,13 @@ static void _dpa_rx_error(struct net_device *net_dev,
 		struct dpa_percpu_priv_s *percpu_priv,
 		const struct qm_fd *fd)
 {
-	int _errno;
-
 	if (netif_msg_hw(priv) && net_ratelimit())
 		cpu_netdev_warn(net_dev, "FD status = 0x%08x\n",
 				fd->status & FM_FD_STAT_ERRORS);
 
 	percpu_priv->stats.rx_errors++;
 
-	_errno = dpa_fd_release(net_dev, fd);
-	if (unlikely(_errno < 0)) {
-		dump_stack();
-		panic("Can't release buffer to the BM during RX\n");
-	}
+	dpa_fd_release(net_dev, fd);
 }
 
 static void _dpa_tx_error(struct net_device		*net_dev,
@@ -865,7 +819,6 @@ static void __hot _dpa_rx(struct net_device *net_dev,
 		struct dpa_percpu_priv_s *percpu_priv,
 		const struct qm_fd *fd)
 {
-	int _errno;
 	struct dpa_bp *dpa_bp;
 	struct sk_buff *skb;
 	struct sk_buff **skbh;
@@ -942,11 +895,7 @@ drop_large_frame:
 	(*percpu_priv->dpa_bp_count)++;
 	skb_recycle(skb);
 _return_dpa_fd_release:
-	_errno = dpa_fd_release(net_dev, fd);
-	if (unlikely(_errno < 0)) {
-		dump_stack();
-		panic("Can't release buffer to the BM during RX\n");
-	}
+	dpa_fd_release(net_dev, fd);
 }
 
 static void dpaa_eth_napi_disable(struct dpa_priv_s *priv)
@@ -1399,7 +1348,6 @@ shared_rx_dqrr(struct qman_portal *portal, struct qman_fq *fq,
 	struct net_device		*net_dev;
 	struct dpa_priv_s		*priv;
 	struct dpa_percpu_priv_s	*percpu_priv;
-	int err;
 	const struct qm_fd *fd = &dq->fd;
 	struct dpa_bp *dpa_bp;
 	size_t size;
@@ -1473,11 +1421,7 @@ shared_rx_dqrr(struct qman_portal *portal, struct qman_fq *fq,
 	net_dev->last_rx = jiffies;
 
 out:
-	err = dpa_fd_release(net_dev, fd);
-	if (unlikely(err < 0)) {
-		dump_stack();
-		panic("Can't release buffer to the BM during RX\n");
-	}
+	dpa_fd_release(net_dev, fd);
 
 	return qman_cb_dqrr_consume;
 }
@@ -1563,7 +1507,6 @@ static void shared_ern(struct qman_portal	*portal,
 {
 	struct net_device *net_dev;
 	const struct dpa_priv_s	*priv;
-	int err;
 	struct dpa_percpu_priv_s *percpu_priv;
 	struct dpa_fq *dpa_fq = (struct dpa_fq *)fq;
 
@@ -1571,11 +1514,7 @@ static void shared_ern(struct qman_portal	*portal,
 	priv = netdev_priv(net_dev);
 	percpu_priv = per_cpu_ptr(priv->percpu_priv, smp_processor_id());
 
-	err = dpa_fd_release(net_dev, &msg->ern.fd);
-	if (unlikely(err < 0)) {
-		dump_stack();
-		panic("Can't release buffer to the BM during a TX\n");
-	}
+	dpa_fd_release(net_dev, &msg->ern.fd);
 
 	percpu_priv->stats.tx_dropped++;
 	percpu_priv->stats.tx_fifo_errors++;
@@ -1590,7 +1529,8 @@ static void egress_ern(struct qman_portal	*portal,
 	struct sk_buff *skb;
 	struct sk_buff **skbh;
 	struct dpa_percpu_priv_s	*percpu_priv;
-	dma_addr_t addr = qm_fd_addr(&msg->ern.fd);
+	struct qm_fd fd = msg->ern.fd;
+	dma_addr_t addr = qm_fd_addr(&fd);
 	struct dpa_bp *bp;
 
 	net_dev = ((struct dpa_fq *)fq)->net_dev;
@@ -1607,12 +1547,7 @@ static void egress_ern(struct qman_portal	*portal,
 	 * manually.
 	 */
 	if (msg->ern.fd.cmd & FM_FD_CMD_FCO) {
-		struct bm_buffer bmb;
-
-		bm_buffer_set64(&bmb, addr);
-		while (bman_release(bp->pool, &bmb, 1, 0))
-			cpu_relax();
-
+		dpa_fd_release(net_dev, &fd);
 		return;
 	}
 
-- 
1.7.9.7

