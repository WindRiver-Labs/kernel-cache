From 3631c9dca7368865bb93e93d9fdd972be7e90813 Mon Sep 17 00:00:00 2001
From: Madalin Bucur <madalin.bucur@freescale.com>
Date: Thu, 5 Apr 2012 14:23:06 +0000
Subject: [PATCH 114/128] dpaa_eth: Add Scatter-Gather support

* Summary *
This patch introduces Scatter-Gather capability in the DPAA-Ethernet
driver. Support is available on both Rx and Tx, forwarding and
termination scenarios.

* Features *
Support is available as a compile-time option, by default turned OFF.
The SG code is mostly all in a new file, conditionally-compiled.
Definitions common to both SG and non-SG code are moved to headers.

This code supports both SG and linear frames/skbuffs, but it is not
optimized for the linear case (if SG support is activated, the linear
frames are processed by code other than the non-SG driver).

On the Rx path, the first 128 bytes of the received frame are memcopied
into the linear part of a new skb. The remaining ingress buffers are
directly used as skb frags.

On the Tx path, the linear part of a nonlinear skb is memcopied to the
first egress SG buffer, while skb frags are used as SG buffers (no copy).
The linear frames are sent without involving a memcopy.

The dynamic FMan resources allocation algorithm can reduce the size
of the Rx FIFOs when Scatter Gather support is enabled. Added the
required modifications to take SG support into account.

This feature is still work in progress, especially in what concerns the
performance improvement. As such, TODOs are still present in the code.

* Known Limitations *
 - No support for NETIF_F_HIGHDMA. This means that non-linear egress
   skbuffs allocated from highmem will be automatically linearized by
   the stack.
 - Only one ingress fragment per page is supported. Buffer pools are
   populated with PAGE_SIZE buffers. An unintended consequence is that
   recycling is not possible with SG code.
 - No integration with ethtool.
 - Performance was not a stated goal in this initial version of the
   feature.

Signed-off-by: Madalin Bucur <madalin.bucur@freescale.com>
Signed-off-by: Ioana Radulescu <ruxandra.radulescu@freescale.com>
Reviewed-by: Bogdan Hamciuc <bogdan.hamciuc@freescale.com>
[Kevin: Original patch taken from FSL
QorIQ-SDK-V1.2-SOURCE-20120614-yocto.iso image]
Signed-off-by: Kevin Hao <kexin.hao@windriver.com>
---
 drivers/net/ethernet/freescale/Kconfig             |    8 +
 drivers/net/ethernet/freescale/dpa/Makefile        |    2 +
 .../dpa/NetCommSw/src/wrapper/lnxwrp_resources.c   |    6 +
 drivers/net/ethernet/freescale/dpa/dpaa_1588.c     |   48 ++
 drivers/net/ethernet/freescale/dpa/dpaa_eth.c      |  192 +++--
 drivers/net/ethernet/freescale/dpa/dpaa_eth.h      |  150 ++++
 drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c   |  805 ++++++++++++++++++++
 7 files changed, 1105 insertions(+), 106 deletions(-)
 create mode 100644 drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c

diff --git a/drivers/net/ethernet/freescale/Kconfig b/drivers/net/ethernet/freescale/Kconfig
index 2698970..cf7661d 100644
--- a/drivers/net/ethernet/freescale/Kconfig
+++ b/drivers/net/ethernet/freescale/Kconfig
@@ -128,6 +128,14 @@ config DPA_MAX_FRM_SIZE
               "fsl_fman_phy_max_frm=<YourValue>";
 	    * in non-Hypervisor-based scenarios, via u-boot's env, by modifying the "bootargs" env variable.
 
+config DPAA_ETH_SG_SUPPORT
+	bool "Add support for S/G frames in the DPAA Ethernet driver"
+	depends on DPA
+	default n
+	help
+	  Configure this to add S/G frames support in the DPAA Ethernet driver.
+	  Please note this replaces the optimal processing code also for non S/G frames.
+
 config FSL_DPA_1588
 	tristate "IEEE 1588-compliant timestamping"
 	depends on DPA
diff --git a/drivers/net/ethernet/freescale/dpa/Makefile b/drivers/net/ethernet/freescale/dpa/Makefile
index 2c970f3..764d02c4d 100644
--- a/drivers/net/ethernet/freescale/dpa/Makefile
+++ b/drivers/net/ethernet/freescale/dpa/Makefile
@@ -11,9 +11,11 @@ EXTRA_CFLAGS += -I$(NET_DPA)
 #Netcomm SW tree
 obj-$(CONFIG_FSL_FMAN) += NetCommSw/
 obj-$(CONFIG_FSL_DPA_1588) += dpaa_1588.o
+obj-$(CONFIG_DPAA_ETH_SG_SUPPORT) += fsl-dpa-sg.o
 obj-$(CONFIG_DPA) += fsl-mac.o fsl-dpa.o
 obj-$(CONFIG_DPA_OFFLINE_PORTS) += fsl-oh.o
 
 fsl-dpa-objs	:= dpa-ethtool.o dpaa_eth.o xgmac_mdio.o
+fsl-dpa-sg-objs	:= dpaa_eth_sg.o
 fsl-mac-objs	:= mac.o mac-api.o
 fsl-oh-objs	:= offline_port.o
diff --git a/drivers/net/ethernet/freescale/dpa/NetCommSw/src/wrapper/lnxwrp_resources.c b/drivers/net/ethernet/freescale/dpa/NetCommSw/src/wrapper/lnxwrp_resources.c
index 1aab8e3..afd89c1 100644
--- a/drivers/net/ethernet/freescale/dpa/NetCommSw/src/wrapper/lnxwrp_resources.c
+++ b/drivers/net/ethernet/freescale/dpa/NetCommSw/src/wrapper/lnxwrp_resources.c
@@ -247,7 +247,13 @@ static uint32_t get_largest_buf_size(uint32_t max_rx_frame_size, uint32_t buf_si
 	uint32_t bp_size = bp_head + max_rx_frame_size
 		+ NET_IP_ALIGN;			/* DPA_BP_SIZE */
 
+#ifndef CONFIG_DPAA_ETH_SG_SUPPORT
 	return CEIL_DIV(bp_size, buf_size);
+#else
+	bp_size = CEIL_DIV(bp_size, 16);	/* frame split in 16 frags */
+
+	return max((uint32_t)16, CEIL_DIV(bp_size, buf_size));
+#endif /* CONFIG_DPAA_ETH_SG_SUPPORT */
 }
 #endif
 
diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_1588.c b/drivers/net/ethernet/freescale/dpa/dpaa_1588.c
index 5c117d5..e836ab2 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_1588.c
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_1588.c
@@ -186,14 +186,31 @@ static u8 *dpa_ptp_parse_packet(struct sk_buff *skb, u16 *eth_type)
 	u8 *pos = skb->data + ETH_ALEN + ETH_ALEN;
 	u8 *ptp_loc = NULL;
 	u8 msg_type;
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+	u32 access_len = ETH_ALEN + ETH_ALEN + DPA_ETYPE_LEN;
+#endif
 	struct iphdr *iph;
 	struct udphdr *udph;
 	struct ipv6hdr *ipv6h;
 
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+	/* when we can receive S/G frames we need to check the data we want to
+	 * access is in the linear skb buffer */
+	if (!pskb_may_pull(skb, access_len))
+		return NULL;
+#endif
+
 	*eth_type = *((u16 *)pos);
 
 	/* Check if inner tag is here */
 	if (*eth_type == ETH_P_8021Q) {
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+		access_len += DPA_VLAN_TAG_LEN;
+
+		if (!pskb_may_pull(skb, access_len))
+			return NULL;
+#endif
+
 		pos += DPA_VLAN_TAG_LEN;
 		*eth_type = *((u16 *)pos);
 	}
@@ -204,6 +221,12 @@ static u8 *dpa_ptp_parse_packet(struct sk_buff *skb, u16 *eth_type)
 	/* Transport of PTP over Ethernet */
 	case ETH_P_1588:
 		ptp_loc = pos;
+
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+		if (!pskb_may_pull(skb, access_len + PTP_OFFS_MSG_TYPE + 1))
+			return NULL;
+#endif
+
 		msg_type = *((u8 *)(ptp_loc + PTP_OFFS_MSG_TYPE)) & 0xf;
 		if ((msg_type == PTP_MSGTYPE_SYNC)
 			|| (msg_type == PTP_MSGTYPE_DELREQ)
@@ -214,9 +237,24 @@ static u8 *dpa_ptp_parse_packet(struct sk_buff *skb, u16 *eth_type)
 	/* Transport of PTP over IPv4 */
 	case ETH_P_IP:
 		iph = (struct iphdr *)pos;
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+		access_len += sizeof(struct iphdr);
+
+		if (!pskb_may_pull(skb, access_len))
+			return NULL;
+#endif
+
 		if (ntohs(iph->protocol) != IPPROTO_UDP)
 			return NULL;
 
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+		access_len += iph->ihl * 4 - sizeof(struct iphdr) +
+				sizeof(struct udphdr);
+
+		if (!pskb_may_pull(skb, access_len))
+			return NULL;
+#endif
+
 		pos += iph->ihl * 4;
 		udph = (struct udphdr *)pos;
 		if (ntohs(udph->dest) != 319)
@@ -226,6 +264,11 @@ static u8 *dpa_ptp_parse_packet(struct sk_buff *skb, u16 *eth_type)
 	/* Transport of PTP over IPv6 */
 	case ETH_P_IPV6:
 		ipv6h = (struct ipv6hdr *)pos;
+
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+		access_len += sizeof(struct ipv6hdr) + sizeof(struct udphdr);
+#endif
+
 		if (ntohs(ipv6h->nexthdr) != IPPROTO_UDP)
 			return NULL;
 
@@ -267,6 +310,11 @@ static int dpa_ptp_store_stamp(struct net_device *dev, struct sk_buff *skb,
 		return -EINVAL;
 	}
 
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+	if (!pskb_may_pull(skb, ptp_loc - skb->data + PTP_OFFS_SEQ_ID + 2))
+		return -EINVAL;
+#endif
+
 	ptp_data->ident.version = *(ptp_loc + PTP_OFFS_VER_PTP) & 0xf;
 	ptp_data->ident.msg_type = *(ptp_loc + PTP_OFFS_MSG_TYPE) & 0xf;
 	ptp_data->ident.seq_id = *((u16 *)(ptp_loc + PTP_OFFS_SEQ_ID));
diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth.c b/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
index 6913331..bed6627 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
@@ -68,7 +68,7 @@
 #define ARRAY2_SIZE(arr)	(ARRAY_SIZE(arr) * ARRAY_SIZE((arr)[0]))
 
 #define DEFAULT_COUNT		64
-#define DEFAULT_BUF_SIZE DPA_BP_SIZE(fsl_fman_phy_maxfrm);
+
 #define DPA_NAPI_WEIGHT		64
 
 /* S/G table requires at least 256 bytes */
@@ -81,30 +81,6 @@
 /* Bootarg used to override the Kconfig DPA_MAX_FRM_SIZE value */
 #define FSL_FMAN_PHY_MAXFRM_BOOTARG	"fsl_fman_phy_max_frm"
 
-/*
- * Values for the L3R field of the FM Parse Results
- */
-/* L3 Type field: First IP Present IPv4 */
-#define FM_L3_PARSE_RESULT_IPV4	0x8000
-/* L3 Type field: First IP Present IPv6 */
-#define FM_L3_PARSE_RESULT_IPV6	0x4000
-
-/*
- * Values for the L4R field of the FM Parse Results
- */
-/* L4 Type field: UDP */
-#define FM_L4_PARSE_RESULT_UDP	0x40
-/* L4 Type field: TCP */
-#define FM_L4_PARSE_RESULT_TCP	0x20
-
-/*
- * FD status field indicating whether the FM Parser has attempted to validate
- * the L4 csum of the frame.
- * Note that having this bit set doesn't necessarily imply that the checksum
- * is valid. One would have to check the parse results to find that out.
- */
-#define FM_FD_STAT_L4CV		0x00000004
-
 /* Valid checksum indication */
 #define DPA_CSUM_VALID		0xFFFF
 
@@ -156,19 +132,8 @@ int dpa_free_pcd_fqids(struct device *, uint32_t) __attribute__((weak));
 
 /* BM */
 
-#define DPA_BP_HEAD (DPA_PRIV_DATA_SIZE + DPA_PARSE_RESULTS_SIZE + \
-			DPA_HASH_RESULTS_SIZE)
-#define DPA_BP_SIZE(s)	(DPA_BP_HEAD + (s))
-
 #define DPAA_ETH_MAX_PAD (L1_CACHE_BYTES * 8)
 
-#define FM_FD_STAT_ERRORS						\
-	(FM_PORT_FRM_ERR_DMA | FM_PORT_FRM_ERR_PHYSICAL	| \
-	 FM_PORT_FRM_ERR_SIZE | FM_PORT_FRM_ERR_CLS_DISCARD | \
-	 FM_PORT_FRM_ERR_EXTRACTION | FM_PORT_FRM_ERR_NO_SCHEME	| \
-	 FM_PORT_FRM_ERR_ILL_PLCR | FM_PORT_FRM_ERR_PRS_TIMEOUT	| \
-	 FM_PORT_FRM_ERR_PRS_ILL_INSTRUCT | FM_PORT_FRM_ERR_PRS_HDR_ERR)
-
 static struct dpa_bp *dpa_bp_array[64];
 
 static struct dpa_bp *default_pool;
@@ -189,7 +154,7 @@ void fsl_dpaa_eth_set_hooks(struct dpaa_eth_hooks_s *hooks)
 EXPORT_SYMBOL(fsl_dpaa_eth_set_hooks);
 
 
-static struct dpa_bp *dpa_bpid2pool(int bpid)
+struct dpa_bp *dpa_bpid2pool(int bpid)
 {
 	return dpa_bp_array[bpid];
 }
@@ -201,6 +166,7 @@ static void dpa_bp_depletion(struct bman_portal	*portal,
 		pr_err("Invalid Pool depleted notification!\n");
 }
 
+#ifndef CONFIG_DPAA_ETH_SG_SUPPORT
 static void dpa_bp_add_8(struct dpa_bp *dpa_bp)
 {
 	struct bm_buffer bmb[8];
@@ -267,7 +233,7 @@ static void dpa_bp_add_8(struct dpa_bp *dpa_bp)
 	}
 }
 
-static void dpa_make_private_pool(struct dpa_bp *dpa_bp)
+void dpa_make_private_pool(struct dpa_bp *dpa_bp)
 {
 	int i;
 
@@ -293,7 +259,7 @@ static void dpa_make_private_pool(struct dpa_bp *dpa_bp)
 			*thiscount = *thiscount - j;
 	}
 }
-
+#endif /* CONFIG_DPAA_ETH_SG_SUPPORT */
 
 static void dpaa_eth_seed_pool(struct dpa_bp *bp)
 {
@@ -421,6 +387,22 @@ pdev_register_failed:
 	return err;
 }
 
+#ifndef CONFIG_DPAA_ETH_SG_SUPPORT
+static inline void _dpa_bp_free_buf(void *addr)
+{
+	struct sk_buff **skbh = addr;
+	struct sk_buff *skb;
+
+	skb = *skbh;
+	dev_kfree_skb_any(skb);
+}
+#else
+static inline void _dpa_bp_free_buf(void *addr)
+{
+	free_page((unsigned long)addr);
+}
+#endif
+
 static void __cold __attribute__((nonnull))
 _dpa_bp_free(struct dpa_bp *dpa_bp)
 {
@@ -440,14 +422,11 @@ _dpa_bp_free(struct dpa_bp *dpa_bp)
 
 			for (i = 0; i < num; i++) {
 				dma_addr_t addr = bm_buf_addr(&bmb[i]);
-				struct sk_buff **skbh = phys_to_virt(addr);
-				struct sk_buff *skb;
 
 				dma_unmap_single(bp->dev, addr, bp->size,
 						DMA_BIDIRECTIONAL);
 
-				skb = *skbh;
-				dev_kfree_skb_any(skb);
+				_dpa_bp_free_buf(phys_to_virt(addr));
 			}
 		} while (num == 8);
 	}
@@ -580,19 +559,7 @@ dpa_fq_free(struct device *dev, struct list_head *list)
 }
 
 
-static inline ssize_t __const __must_check __attribute__((nonnull))
-dpa_fd_length(const struct qm_fd *fd)
-{
-	return fd->length20;
-}
-
-static inline ssize_t __const __must_check __attribute__((nonnull))
-dpa_fd_offset(const struct qm_fd *fd)
-{
-	return fd->offset;
-}
-
-static void __attribute__((nonnull))
+void __attribute__((nonnull))
 dpa_fd_release(const struct net_device *net_dev, const struct qm_fd *fd)
 {
 	int				 i, j;
@@ -638,6 +605,7 @@ dpa_fd_release(const struct net_device *net_dev, const struct qm_fd *fd)
 		cpu_relax();
 }
 
+#ifndef CONFIG_DPAA_ETH_SG_SUPPORT
 /*
  * Cleanup function for outgoing frame descriptors that were built on Tx path,
  * either contiguous frames or scatter/gather ones with a single data buffer.
@@ -649,7 +617,7 @@ dpa_fd_release(const struct net_device *net_dev, const struct qm_fd *fd)
  * Return the skb backpointer, since for S/G frames the buffer containing it
  * gets freed here.
  */
-static struct sk_buff *_dpa_cleanup_tx_fd(const struct dpa_priv_s *priv,
+struct sk_buff *_dpa_cleanup_tx_fd(const struct dpa_priv_s *priv,
 			       const struct qm_fd *fd)
 {
 	dma_addr_t addr = qm_fd_addr(fd);
@@ -702,6 +670,7 @@ static struct sk_buff *_dpa_cleanup_tx_fd(const struct dpa_priv_s *priv,
 
 	return skb;
 }
+#endif /* CONFIG_DPAA_ETH_SG_SUPPORT */
 
 /* net_device */
 
@@ -826,6 +795,7 @@ static int dpa_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)
 }
 #endif
 
+#ifndef CONFIG_DPAA_ETH_SG_SUPPORT
 /*
  * When we put the buffer into the pool, we purposefully added
  * some padding to the address so that the buffers wouldn't all
@@ -855,6 +825,7 @@ static int dpa_process_one(struct dpa_percpu_priv_s *percpu_priv,
 
 	return 0;
 }
+#endif
 
 /*
  * Checks whether the checksum field in Parse Results array is valid
@@ -935,7 +906,8 @@ static void _dpa_tx_error(struct net_device		*net_dev,
 	dev_kfree_skb(skb);
 }
 
-static void __hot _dpa_rx(struct net_device *net_dev,
+#ifndef CONFIG_DPAA_ETH_SG_SUPPORT
+void __hot _dpa_rx(struct net_device *net_dev,
 		const struct dpa_priv_s *priv,
 		struct dpa_percpu_priv_s *percpu_priv,
 		const struct qm_fd *fd,
@@ -985,12 +957,9 @@ static void __hot _dpa_rx(struct net_device *net_dev,
 
 	skb->protocol = eth_type_trans(skb, net_dev);
 
-	if (unlikely(skb->len > net_dev->mtu)) {
-		if ((skb->protocol != ETH_P_8021Q) ||
-				(skb->len > net_dev->mtu + 4)) {
-			percpu_priv->stats.rx_dropped++;
-			goto drop_large_frame;
-		}
+	if (unlikely(dpa_check_rx_mtu(skb, net_dev->mtu))) {
+		percpu_priv->stats.rx_dropped++;
+		goto drop_large_frame;
 	}
 
 	/* Check if the FMan Parser has already validated the L4 csum. */
@@ -1026,6 +995,7 @@ drop_large_frame:
 _return_dpa_fd_release:
 	dpa_fd_release(net_dev, fd);
 }
+#endif /* CONFIG_DPAA_ETH_SG_SUPPORT */
 
 static void dpaa_eth_napi_disable(struct dpa_priv_s *priv)
 {
@@ -1065,12 +1035,31 @@ static int dpaa_eth_poll(struct napi_struct *napi, int budget)
 
 	count = *percpu_priv->dpa_bp_count;
 
+#ifndef CONFIG_DPAA_ETH_SG_SUPPORT
 	if (count < DEFAULT_COUNT / 4) {
 		int i;
 
 		for (i = count; i < DEFAULT_COUNT; i += 8)
 			dpa_bp_add_8(percpu_priv->dpa_bp);
 	}
+#else
+	if (count < DEFAULT_COUNT / 4) {
+		int i;
+
+		/* Add pages to the buffer pool */
+		for (i = count; i < DEFAULT_COUNT; i += 8)
+			dpa_bp_add_8_pages(percpu_priv->dpa_bp,
+					   smp_processor_id());
+	}
+
+	/* Add skbs to the percpu skb list, reuse var count */
+	count = percpu_priv->skb_count;
+
+	if (count < DEFAULT_SKB_COUNT / 4)
+		dpa_list_add_skbs(percpu_priv,
+				  DEFAULT_SKB_COUNT - count);
+#endif
+
 
 	if (cleaned < budget) {
 		int tmp;
@@ -1144,7 +1133,7 @@ dpa_phys2virt(const struct dpa_bp *dpa_bp, dma_addr_t addr)
  * Note that this function may modify the fd->cmd field and the skb data buffer
  * (the Parse Results area).
  */
-static inline int dpa_enable_tx_csum(struct dpa_priv_s *priv,
+int dpa_enable_tx_csum(struct dpa_priv_s *priv,
 	struct sk_buff *skb, struct qm_fd *fd, char *parse_results)
 {
 	t_FmPrsResult *parse_result;
@@ -1231,29 +1220,6 @@ return_error:
 	return retval;
 }
 
-static inline int __hot dpa_xmit(struct dpa_priv_s *priv,
-			struct dpa_percpu_priv_s *percpu, int queue,
-			struct qm_fd *fd)
-{
-	int err;
-
-	prefetchw(&percpu->start_tx);
-	err = qman_enqueue(priv->egress_fqs[queue], fd, 0);
-	if (unlikely(err < 0)) {
-		if (netif_msg_tx_err(priv) && net_ratelimit())
-			cpu_netdev_err(priv->net_dev, "qman_enqueue() = %d\n",
-					err);
-		percpu->stats.tx_errors++;
-		percpu->stats.tx_fifo_errors++;
-		return err;
-	}
-
-	percpu->stats.tx_packets++;
-	percpu->stats.tx_bytes += dpa_fd_length(fd);
-
-	return NETDEV_TX_OK;
-}
-
 static int __hot dpa_shared_tx(struct sk_buff *skb, struct net_device *net_dev)
 {
 	struct dpa_bp *dpa_bp;
@@ -1372,14 +1338,7 @@ buf_acquire_failed:
 	return NETDEV_TX_OK;
 }
 
-/* Equivalent to a memset(0), but works faster */
-static inline void clear_fd(struct qm_fd *fd)
-{
-	fd->opaque_addr = 0;
-	fd->opaque = 0;
-	fd->cmd = 0;
-}
-
+#ifndef CONFIG_DPAA_ETH_SG_SUPPORT
 static int skb_to_sg_fd(struct dpa_priv_s *priv,
 		struct sk_buff *skb, struct qm_fd *fd)
 {
@@ -1533,7 +1492,7 @@ static int skb_to_contig_fd(struct dpa_priv_s *priv,
 	return 0;
 }
 
-static int __hot dpa_tx(struct sk_buff *skb, struct net_device *net_dev)
+int __hot dpa_tx(struct sk_buff *skb, struct net_device *net_dev)
 {
 	struct dpa_priv_s	*priv;
 	struct qm_fd		 fd;
@@ -1613,6 +1572,7 @@ static int __hot dpa_tx(struct sk_buff *skb, struct net_device *net_dev)
 		(*percpu_priv->dpa_bp_count)++;
 		percpu_priv->tx_returned++;
 	}
+
 	if (unlikely(dpa_xmit(priv, percpu_priv, queue_mapping, &fd) < 0))
 		goto xmit_failed;
 
@@ -1631,6 +1591,7 @@ fd_create_failed:
 
 	return NETDEV_TX_OK;
 }
+#endif /* CONFIG_DPAA_ETH_SG_SUPPORT */
 
 static enum qman_cb_dqrr_result
 ingress_rx_error_dqrr(struct qman_portal		*portal,
@@ -1759,13 +1720,10 @@ shared_rx_dqrr(struct qman_portal *portal, struct qman_fq *fq,
 static_map:
 	skb->protocol = eth_type_trans(skb, net_dev);
 
-	if (unlikely(skb->len > net_dev->mtu)) {
-		if ((skb->protocol != ETH_P_8021Q) ||
-				(skb->len > net_dev->mtu + 4)) {
-			percpu_priv->stats.rx_dropped++;
-			dev_kfree_skb_any(skb);
-			goto out;
-		}
+	if (unlikely(dpa_check_rx_mtu(skb, net_dev->mtu))) {
+		percpu_priv->stats.rx_dropped++;
+		dev_kfree_skb_any(skb);
+		goto out;
 	}
 
 	if (unlikely(netif_rx(skb) != NET_RX_SUCCESS))
@@ -2392,6 +2350,13 @@ dpa_bp_probe(struct platform_device *_of_dev, size_t *count)
 	} else if (has_kernel_pool) {
 		dpa_bp->target_count = DEFAULT_COUNT;
 		dpa_bp->size = DEFAULT_BUF_SIZE;
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+		if (dpa_bp->size > PAGE_SIZE) {
+			dpaa_eth_warning(dev, "Default buffer size too large. "
+				     "Round down to PAGE_SIZE\n");
+			dpa_bp->size = PAGE_SIZE;
+		}
+#endif
 		dpa_bp->kernel_pool = 1;
 	}
 
@@ -2970,6 +2935,13 @@ static int dpa_netdev_init(struct device_node *dpa_node,
 		mac_addr = priv->mac_dev->addr;
 		net_dev->features |= (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM);
 		net_dev->vlan_features |= (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM);
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+		/* Advertise S/G support for MAC-ful, private interfaces */
+		if (!priv->shared) {
+			net_dev->features |= NETIF_F_SG;
+			net_dev->vlan_features |= NETIF_F_SG;
+		}
+#endif
 	}
 
 	memcpy(net_dev->perm_addr, mac_addr, net_dev->addr_len);
@@ -3018,8 +2990,10 @@ static int dpa_private_netdev_init(struct device_node *dpa_node,
 	struct dpa_priv_s *priv = netdev_priv(net_dev);
 	struct dpa_percpu_priv_s *percpu_priv;
 
-	/* although we access another CPU's private data here
-	 * we do it at initialization so it is safe */
+	/*
+	 * Although we access another CPU's private data here
+	 * we do it at initialization so it is safe
+	 */
 	for_each_online_cpu(i) {
 		percpu_priv = per_cpu_ptr(priv->percpu_priv, i);
 		percpu_priv->net_dev = net_dev;
@@ -3027,6 +3001,12 @@ static int dpa_private_netdev_init(struct device_node *dpa_node,
 		percpu_priv->dpa_bp = priv->dpa_bp;
 		percpu_priv->dpa_bp_count =
 			per_cpu_ptr(priv->dpa_bp->percpu_count, i);
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+		/* init the percpu list and add some skbs */
+		skb_queue_head_init(&percpu_priv->skb_list);
+
+		dpa_list_add_skbs(percpu_priv, DEFAULT_SKB_COUNT);
+#endif
 		netif_napi_add(net_dev, &percpu_priv->napi, dpaa_eth_poll,
 			       DPA_NAPI_WEIGHT);
 	}
diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth.h b/drivers/net/ethernet/freescale/dpa/dpaa_eth.h
index 54b0dc2..9180667 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth.h
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth.h
@@ -39,6 +39,9 @@
 #include <linux/workqueue.h>	/* struct work_struct */
 #include <linux/skbuff.h>
 #include <linux/hardirq.h>
+#include <linux/if_vlan.h>	/* vlan_eth_hdr */
+#include <linux/ip.h>		/* ip_hdr */
+#include <linux/ipv6.h>		/* ipv6_hdr */
 #ifdef CONFIG_DEBUG_FS
 #include <linux/dcache.h>	/* struct dentry */
 #endif
@@ -47,6 +50,10 @@
 
 #include "dpaa_eth-common.h"
 
+#include "fsl_fman.h"
+#include "fm_ext.h"
+#include "fm_port_ext.h" /* FM_PORT_FRM_ERR_* */
+
 #include "mac.h"		/* struct mac_device */
 
 
@@ -123,6 +130,51 @@ struct dpaa_eth_hooks_s {
 
 void fsl_dpaa_eth_set_hooks(struct dpaa_eth_hooks_s *hooks);
 
+#define DPA_BP_HEAD (DPA_PRIV_DATA_SIZE + DPA_PARSE_RESULTS_SIZE + \
+			DPA_HASH_RESULTS_SIZE)
+#define DPA_BP_SIZE(s)	(DPA_BP_HEAD + (s))
+
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+#define DEFAULT_SKB_COUNT 64 /* maximum number of SKBs in each percpu list */
+/*
+ * We may want this value configurable. Must be <= PAGE_SIZE
+ * A lower value may help with recycling rates, at least on forwarding
+ */
+#define DEFAULT_BUF_SIZE	PAGE_SIZE
+#else
+#define DEFAULT_BUF_SIZE DPA_BP_SIZE(fsl_fman_phy_maxfrm);
+#endif /* CONFIG_DPAA_ETH_SG_SUPPORT */
+
+/*
+ * Values for the L3R field of the FM Parse Results
+ */
+/* L3 Type field: First IP Present IPv4 */
+#define FM_L3_PARSE_RESULT_IPV4	0x8000
+/* L3 Type field: First IP Present IPv6 */
+#define FM_L3_PARSE_RESULT_IPV6	0x4000
+
+/*
+ * Values for the L4R field of the FM Parse Results
+ */
+/* L4 Type field: UDP */
+#define FM_L4_PARSE_RESULT_UDP	0x40
+/* L4 Type field: TCP */
+#define FM_L4_PARSE_RESULT_TCP	0x20
+
+/*
+ * FD status field indicating whether the FM Parser has attempted to validate
+ * the L4 csum of the frame.
+ * Note that having this bit set doesn't necessarily imply that the checksum
+ * is valid. One would have to check the parse results to find that out.
+ */
+#define FM_FD_STAT_L4CV		0x00000004
+
+#define FM_FD_STAT_ERRORS						\
+	(FM_PORT_FRM_ERR_DMA | FM_PORT_FRM_ERR_PHYSICAL	| \
+	 FM_PORT_FRM_ERR_SIZE | FM_PORT_FRM_ERR_CLS_DISCARD | \
+	 FM_PORT_FRM_ERR_EXTRACTION | FM_PORT_FRM_ERR_NO_SCHEME	| \
+	 FM_PORT_FRM_ERR_ILL_PLCR | FM_PORT_FRM_ERR_PRS_TIMEOUT	| \
+	 FM_PORT_FRM_ERR_PRS_ILL_INSTRUCT | FM_PORT_FRM_ERR_PRS_HDR_ERR)
 
 struct pcd_range {
 	uint32_t			 base;
@@ -187,6 +239,12 @@ struct dpa_percpu_priv_s {
 	int *dpa_bp_count;
 	struct dpa_bp *dpa_bp;
 	struct napi_struct napi;
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+	/* a list of preallocated SKBs for this CPU */
+	struct sk_buff_head skb_list;
+	/* current number of skbs in the CPU's list */
+	int skb_count;
+#endif
 	u32 start_tx;
 	u32 in_interrupt;
 	u32 ingress_calls;
@@ -226,6 +284,44 @@ struct dpa_priv_s {
 extern const struct ethtool_ops dpa_ethtool_ops;
 extern int fsl_fman_phy_maxfrm;
 
+void __attribute__((nonnull))
+dpa_fd_release(const struct net_device *net_dev, const struct qm_fd *fd);
+
+void dpa_make_private_pool(struct dpa_bp *dpa_bp);
+
+struct dpa_bp *dpa_bpid2pool(int bpid);
+
+void __hot _dpa_rx(struct net_device *net_dev,
+		const struct dpa_priv_s *priv,
+		struct dpa_percpu_priv_s *percpu_priv,
+		const struct qm_fd *fd,
+		u32 fqid);
+
+int __hot dpa_tx(struct sk_buff *skb, struct net_device *net_dev);
+
+struct sk_buff *_dpa_cleanup_tx_fd(const struct dpa_priv_s *priv,
+				   const struct qm_fd *fd);
+
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+void dpa_bp_add_8_pages(struct dpa_bp *dpa_bp, int cpu_id);
+
+void dpa_list_add_skbs(struct dpa_percpu_priv_s *cpu_priv, int count);
+#endif
+
+/*
+ * Turn on HW checksum computation for this outgoing frame.
+ * If the current protocol is not something we support in this regard
+ * (or if the stack has already computed the SW checksum), we do nothing.
+ *
+ * Returns 0 if all goes well (or HW csum doesn't apply), and a negative value
+ * otherwise.
+ *
+ * Note that this function may modify the fd->cmd field and the skb data buffer
+ * (the Parse Results area).
+ */
+int dpa_enable_tx_csum(struct dpa_priv_s *priv,
+	struct sk_buff *skb, struct qm_fd *fd, char *parse_results);
+
 static inline int dpaa_eth_napi_schedule(struct dpa_percpu_priv_s *percpu_priv)
 {
 	if (unlikely(in_irq())) {
@@ -239,6 +335,60 @@ static inline int dpaa_eth_napi_schedule(struct dpa_percpu_priv_s *percpu_priv)
 	return 0;
 }
 
+static inline ssize_t __const __must_check __attribute__((nonnull))
+dpa_fd_length(const struct qm_fd *fd)
+{
+	return fd->length20;
+}
+
+static inline ssize_t __const __must_check __attribute__((nonnull))
+dpa_fd_offset(const struct qm_fd *fd)
+{
+	return fd->offset;
+}
+
+/* Verifies if the skb length is below the interface MTU */
+static inline int dpa_check_rx_mtu(struct sk_buff *skb, int mtu)
+{
+	if (unlikely(skb->len > mtu))
+		if ((skb->protocol != ETH_P_8021Q) || (skb->len > mtu + 4))
+			return -1;
+
+	return 0;
+}
+
+/* Equivalent to a memset(0), but works faster */
+static inline void clear_fd(struct qm_fd *fd)
+{
+	fd->opaque_addr = 0;
+	fd->opaque = 0;
+	fd->cmd = 0;
+}
+
+static inline int __hot dpa_xmit(struct dpa_priv_s *priv,
+			struct dpa_percpu_priv_s *percpu, int queue,
+			struct qm_fd *fd)
+{
+	int err, i;
+
+	prefetchw(&percpu->start_tx);
+	for (i = 0; i < 100000; i++) {
+		err = qman_enqueue(priv->egress_fqs[queue], fd, 0);
+		if (err != -EBUSY)
+			break;
+	}
+	if (unlikely(err < 0)) {
+		percpu->stats.tx_errors++;
+		percpu->stats.tx_fifo_errors++;
+		return err;
+	}
+
+	percpu->stats.tx_packets++;
+	percpu->stats.tx_bytes += dpa_fd_length(fd);
+
+	return NETDEV_TX_OK;
+}
+
 #if defined CONFIG_DPA_ETH_WQ_LEGACY
 #define DPA_NUM_WQS 8
 /*
diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c b/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c
new file mode 100644
index 0000000..ff2677b
--- /dev/null
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c
@@ -0,0 +1,805 @@
+/*
+ * Copyright 2012 Freescale Semiconductor Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *	 notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *	 notice, this list of conditions and the following disclaimer in the
+ *	 documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *	 names of its contributors may be used to endorse or promote products
+ *	 derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/init.h>
+#include <linux/skbuff.h>
+#include <linux/fsl_bman.h>
+
+#include "dpaa_eth.h"
+#include "dpaa_1588.h"
+
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+
+#define DPA_COPIED_HEADERS_SIZE 128 /* TODO: determine the required value */
+#define DPA_SGT_MAX_ENTRIES 16 /* maximum number of entries in SG Table */
+
+/*
+ * It does not return a page as you get the page from the fd,
+ * this is only for refcounting and DMA unmapping
+ */
+static inline void dpa_bp_removed_one_page(struct dpa_bp *dpa_bp,
+					   dma_addr_t dma_addr)
+{
+	int *count_ptr;
+
+	count_ptr = per_cpu_ptr(dpa_bp->percpu_count, smp_processor_id());
+	(*count_ptr)--;
+
+	dma_unmap_single(dpa_bp->dev, dma_addr, dpa_bp->size,
+		DMA_BIDIRECTIONAL);
+}
+
+/* DMA map and add a page into the bpool */
+static void dpa_bp_add_page(struct dpa_bp *dpa_bp, unsigned long vaddr)
+{
+	struct bm_buffer bmb;
+	int *count_ptr;
+	dma_addr_t addr;
+	int offset;
+
+	count_ptr = per_cpu_ptr(dpa_bp->percpu_count, smp_processor_id());
+
+	/* Make sure we don't map beyond end of page */
+	offset = vaddr & (PAGE_SIZE - 1);
+	if (unlikely(dpa_bp->size + offset > PAGE_SIZE)) {
+		free_page(vaddr);
+		return;
+	}
+	addr = dma_map_single(dpa_bp->dev, (void *)vaddr, dpa_bp->size,
+			      DMA_BIDIRECTIONAL);
+	if (unlikely(dma_mapping_error(dpa_bp->dev, addr))) {
+		dpaa_eth_err(dpa_bp->dev, "DMA mapping failed");
+		return;
+	}
+
+	bm_buffer_set64(&bmb, addr);
+
+	while (bman_release(dpa_bp->pool, &bmb, 1, 0))
+		cpu_relax();
+
+	(*count_ptr)++;
+}
+
+void dpa_bp_add_8_pages(struct dpa_bp *dpa_bp, int cpu_id)
+{
+	struct bm_buffer bmb[8];
+	unsigned long new_page;
+	int *count_ptr;
+	dma_addr_t addr;
+	int i;
+
+	count_ptr = per_cpu_ptr(dpa_bp->percpu_count, cpu_id);
+
+	for (i = 0; i < 8; i++) {
+		new_page = __get_free_page(GFP_ATOMIC);
+		if (unlikely(!new_page)) {
+			dpaa_eth_err(dpa_bp->dev, "__get_free_page() failed\n");
+			bm_buffer_set64(&bmb[i], 0);
+			break;
+		}
+
+		addr = dma_map_single(dpa_bp->dev, (void *)new_page,
+				dpa_bp->size, DMA_BIDIRECTIONAL);
+		if (unlikely(dma_mapping_error(dpa_bp->dev, addr))) {
+			dpaa_eth_err(dpa_bp->dev, "DMA mapping failed");
+			free_page(new_page);
+			break;
+		}
+
+		bm_buffer_set64(&bmb[i], addr);
+	}
+
+	/*
+	 * Avoid releasing a completely null buffer; bman_release() requires
+	 * at least one buffer.
+	 */
+	if (likely(i)) {
+		/*
+		 * Release the buffers. In case bman is busy, keep trying
+		 * until successful. bman_release() is guaranteed to succeed
+		 * in a reasonable amount of time
+		 */
+		while (bman_release(dpa_bp->pool, bmb, i, 0))
+			cpu_relax();
+
+		*count_ptr += i;
+	}
+}
+
+void dpa_list_add_skb(struct dpa_percpu_priv_s *cpu_priv,
+		      struct sk_buff *new_skb)
+{
+	struct sk_buff_head *list_ptr;
+
+	if (cpu_priv->skb_count > DEFAULT_SKB_COUNT) {
+		dev_kfree_skb(new_skb);
+		return;
+	}
+
+	list_ptr = &cpu_priv->skb_list;
+	skb_queue_head(list_ptr, new_skb);
+
+	cpu_priv->skb_count += 1;
+}
+
+static struct sk_buff *dpa_list_get_skb(struct dpa_percpu_priv_s *cpu_priv)
+{
+	struct sk_buff_head *list_ptr;
+	struct sk_buff *new_skb;
+
+	list_ptr = &cpu_priv->skb_list;
+
+	new_skb = skb_dequeue(list_ptr);
+	if (new_skb)
+		cpu_priv->skb_count -= 1;
+
+	return new_skb;
+}
+
+void dpa_list_add_skbs(struct dpa_percpu_priv_s *cpu_priv, int count)
+{
+	struct sk_buff *new_skb;
+	int i;
+
+	for (i = 0; i < count; i++) {
+		/*
+		 * new skb of (NET_SKB_PAD + DPA_BP_HEAD +
+		 * DPA_COPIED_HEADERS_SIZE) bytes
+		 */
+		new_skb = dev_alloc_skb(DPA_BP_SIZE(DPA_COPIED_HEADERS_SIZE));
+		if (unlikely(!new_skb)) {
+			pr_err("dev_alloc_skb() failed\n");
+			break;
+		}
+
+		dpa_list_add_skb(cpu_priv, new_skb);
+	}
+}
+
+void dpa_make_private_pool(struct dpa_bp *dpa_bp)
+{
+	int i;
+
+	dpa_bp->percpu_count = __alloc_percpu(sizeof(*dpa_bp->percpu_count),
+					__alignof__(*dpa_bp->percpu_count));
+
+	/* Give each CPU an allotment of "page_count" buffers */
+	for_each_online_cpu(i) {
+		int j;
+
+		/*
+		 * Although we access another CPU's counters here
+		 * we do it at boot time so it is safe
+		 */
+		for (j = 0; j < dpa_bp->config_count; j += 8)
+			dpa_bp_add_8_pages(dpa_bp, i);
+	}
+}
+
+/*
+ * Cleanup function for outgoing frame descriptors that were built on Tx path,
+ * either contiguous frames or scatter/gather ones.
+ * Skb freeing is not handled here.
+ *
+ * This function may be called on error paths in the Tx function, so guard
+ * against cases when not all fd relevant fields were filled in.
+ *
+ * Return the skb backpointer, since for S/G frames the buffer containing it
+ * gets freed here.
+ */
+struct sk_buff *_dpa_cleanup_tx_fd(const struct dpa_priv_s *priv,
+					  const struct qm_fd *fd)
+{
+	const struct qm_sg_entry *sgt;
+	int i;
+	struct dpa_bp *dpa_bp = priv->dpa_bp;
+	dma_addr_t addr = qm_fd_addr(fd);
+	struct sk_buff **skbh;
+	struct sk_buff *skb = NULL;
+	enum dma_data_direction dma_dir;
+
+	dma_dir = (fd->cmd & FM_FD_CMD_FCO) ? DMA_BIDIRECTIONAL : DMA_TO_DEVICE;
+	dma_unmap_single(dpa_bp->dev, addr, dpa_bp->size, dma_dir);
+
+	/* retrieve skb back pointer */
+	skbh = (struct sk_buff **)phys_to_virt(addr);
+	skb = *skbh;
+
+	if (fd->format == qm_fd_sg) {
+		/* all storage items used are pages */
+		sgt = phys_to_virt(addr + dpa_fd_offset(fd));
+
+		for (i = 0; i < DPA_SGT_MAX_ENTRIES; i++) {
+			BUG_ON(sgt[i].extension);
+
+			dma_unmap_single(dpa_bp->dev, sgt[i].addr,
+					 dpa_bp->size, dma_dir);
+
+			if (sgt[i].final)
+				break;
+		}
+
+		/*
+		 * TODO: dpa_bp_add_page() ?
+		 * We could put these in the pool, since we allocated them
+		 * and we know they're not used by anyone else
+		 */
+
+		/* Free separately the pages that we allocated on Tx */
+		free_page((unsigned long)phys_to_virt(addr));
+		free_page((unsigned long)phys_to_virt(sgt[0].addr));
+	}
+
+	return skb;
+}
+
+/*
+ * Move the first DPA_COPIED_HEADERS_SIZE bytes to the skb linear buffer to
+ * provide the networking stack the headers it requires in the linear buffer.
+ *
+ * If the entire frame fits in the skb linear buffer, the page holding the
+ * received data is recycled as it is no longer required.
+ */
+static void __hot contig_fd_to_skb(const struct dpa_priv_s *priv,
+				   const struct qm_fd *fd, struct sk_buff *skb)
+{
+	unsigned int copy_size;
+	dma_addr_t addr = qm_fd_addr(fd);
+	void *vaddr;
+	struct page *page;
+	int frag_offset, page_offset;
+	struct dpa_bp *dpa_bp = priv->dpa_bp;
+
+	vaddr = phys_to_virt(addr);
+
+	/* copy the headers in the linear portion */
+	/*
+	 * TODO: maybe adjust to actual headers length from
+	 * parse results
+	 */
+	copy_size = min(DPA_COPIED_HEADERS_SIZE, dpa_fd_length(fd));
+	memcpy(skb_put(skb, copy_size), vaddr + dpa_fd_offset(fd), copy_size);
+
+#ifdef CONFIG_FSL_DPA_1588
+	if (priv->tsu && priv->tsu->valid && priv->tsu->hwts_rx_en_ioctl)
+		dpa_ptp_store_rxstamp(priv->net_dev, skb, fd);
+#endif
+
+	/*
+	 * If frame is longer than the amount we copy in the linear
+	 * buffer, add the page as fragment,
+	 * otherwise recycle the page
+	 */
+	page = virt_to_page(vaddr);
+	if (copy_size < dpa_fd_length(fd)) {
+		/* add the page as a fragment in the skb */
+		page_offset = (unsigned long)vaddr & (PAGE_SIZE - 1);
+		frag_offset = page_offset + dpa_fd_offset(fd) + copy_size;
+		skb_add_rx_frag(skb, 0, page, frag_offset,
+				dpa_fd_length(fd) - copy_size);
+	} else {
+		/* recycle the page */
+		dpa_bp_add_page(dpa_bp, (unsigned long)vaddr);
+	}
+}
+
+
+/*
+ * Move the first DPA_COPIED_HEADERS_SIZE bytes to the skb linear buffer to
+ * provide the networking stack the headers it requires in the linear buffer
+ * and add the rest of the frame as skb fragments.
+ *
+ * The page holding the S/G Table is recycled here.
+ */
+static void __hot sg_fd_to_skb(const struct dpa_priv_s *priv,
+			       const struct qm_fd *fd, struct sk_buff *skb)
+{
+	const struct qm_sg_entry *sgt;
+	dma_addr_t addr = qm_fd_addr(fd);
+	dma_addr_t sg_addr;
+	void *vaddr, *sg_vaddr;
+	struct dpa_bp *dpa_bp;
+	struct page *page;
+	int frag_offset, frag_len;
+	int page_offset;
+	int i;
+
+	vaddr = phys_to_virt(addr);
+
+	/*
+	 * Iterate through the SGT entries and add the data buffers as
+	 * skb fragments
+	 */
+	sgt = vaddr + dpa_fd_offset(fd);
+	for (i = 0; i < DPA_SGT_MAX_ENTRIES; i++) {
+		/* Extension bit is not supported */
+		BUG_ON(sgt[i].extension);
+
+		dpa_bp = dpa_bpid2pool(sgt[i].bpid);
+		BUG_ON(IS_ERR(dpa_bp));
+
+		sg_addr = qm_sg_addr(&sgt[i]);
+		sg_vaddr = phys_to_virt(sg_addr);
+
+		dpa_bp_removed_one_page(dpa_bp, sg_addr);
+		page = virt_to_page(sg_vaddr);
+
+		/*
+		 * Padding at the beginning of the page
+		 * (offset in page from where BMan buffer begins)
+		 */
+		page_offset = (unsigned long)sg_vaddr & (PAGE_SIZE - 1);
+
+		if (i == 0) {
+			/* This is the first fragment */
+			/* Move the network headers in the skb linear portion */
+			memcpy(skb_put(skb, DPA_COPIED_HEADERS_SIZE),
+				sg_vaddr + sgt[i].offset,
+				DPA_COPIED_HEADERS_SIZE);
+
+			/* Adjust offset/length for the remaining data */
+			frag_offset = sgt[i].offset + page_offset +
+				      DPA_COPIED_HEADERS_SIZE;
+			frag_len = sgt[i].length - DPA_COPIED_HEADERS_SIZE;
+		} else {
+			/*
+			 * Not the first fragment; all data from buferr will
+			 * be added in an skb fragment
+			 */
+			frag_offset = sgt[i].offset + page_offset;
+			frag_len = sgt[i].length;
+		}
+		/* Add data buffer to the skb */
+		skb_add_rx_frag(skb, i, page, frag_offset, frag_len);
+
+		if (sgt[i].final)
+			break;
+	}
+
+#ifdef CONFIG_FSL_DPA_1588
+	if (priv->tsu && priv->tsu->valid && priv->tsu->hwts_rx_en_ioctl)
+		dpa_ptp_store_rxstamp(priv->net_dev, skb, fd);
+#endif
+
+	/* recycle the SGT page */
+	dpa_bp = dpa_bpid2pool(fd->bpid);
+	BUG_ON(IS_ERR(dpa_bp));
+	dpa_bp_add_page(dpa_bp, (unsigned long)vaddr);
+}
+
+void __hot _dpa_rx(struct net_device *net_dev,
+		const struct dpa_priv_s *priv,
+		struct dpa_percpu_priv_s *percpu_priv,
+		const struct qm_fd *fd,
+		u32 fqid)
+{
+	struct dpa_bp *dpa_bp;
+	struct sk_buff *skb;
+	dma_addr_t addr = qm_fd_addr(fd);
+	u32 fd_status = fd->status;
+
+	if (unlikely(fd_status & FM_FD_STAT_ERRORS) != 0) {
+		if (netif_msg_hw(priv) && net_ratelimit())
+			cpu_netdev_warn(net_dev, "FD status = 0x%08x\n",
+					fd_status & FM_FD_STAT_ERRORS);
+
+		percpu_priv->stats.rx_errors++;
+		goto _release_frame;
+	}
+
+	dpa_bp = dpa_bpid2pool(fd->bpid);
+	skb = dpa_list_get_skb(percpu_priv);
+
+	if (unlikely(skb == NULL)) {
+		/* List is empty, so allocate a new skb */
+		skb = dev_alloc_skb(DPA_BP_SIZE(DPA_COPIED_HEADERS_SIZE));
+		if (unlikely(skb == NULL)) {
+			if (netif_msg_rx_err(priv) && net_ratelimit())
+				cpu_netdev_err(net_dev,
+						"Could not alloc skb\n");
+			percpu_priv->stats.rx_dropped++;
+			goto _release_frame;
+		}
+	}
+
+	/* TODO We might want to do some prefetches here (skb, shinfo, data) */
+
+	/*
+	 * Reserve DPA_BP_HEAD bytes of headroom, such that forwarded skbs
+	 * will have enough headroom space on Tx
+	 */
+	skb_reserve(skb, DPA_BP_HEAD);
+
+	dpa_bp_removed_one_page(dpa_bp, addr);
+
+	/* prefetch the first 64 bytes of the frame or the SGT start */
+	prefetch(phys_to_virt(addr) + dpa_fd_offset(fd));
+
+	if (likely(fd->format == qm_fd_contig))
+		contig_fd_to_skb(priv, fd, skb);
+	else if (fd->format == qm_fd_sg)
+		sg_fd_to_skb(priv, fd, skb);
+	else
+		/* The only FD types that we may receive are contig and S/G */
+		BUG();
+
+	skb->protocol = eth_type_trans(skb, net_dev);
+
+	if (unlikely(dpa_check_rx_mtu(skb, net_dev->mtu))) {
+		percpu_priv->stats.rx_dropped++;
+		dev_kfree_skb(skb);
+		return;
+	}
+
+	/* Check if the FMan Parser has already validated the L4 csum. */
+	if (fd_status & FM_FD_STAT_L4CV) {
+		/*
+		 * If we're here, the csum must be valid (if it hadn't,
+		 * the frame would have been received on the Error FQ,
+		 * respectively on the _dpa_rx_error() path).
+		 */
+		skb->ip_summed = CHECKSUM_UNNECESSARY;
+	} else
+		skb->ip_summed = CHECKSUM_NONE;
+
+	if (unlikely(netif_receive_skb(skb) == NET_RX_DROP))
+		percpu_priv->stats.rx_dropped++;
+	else {
+		percpu_priv->stats.rx_packets++;
+		percpu_priv->stats.rx_bytes += skb->len;
+	}
+
+	net_dev->last_rx = jiffies;
+
+	return;
+
+_release_frame:
+	dpa_fd_release(net_dev, fd);
+}
+
+static int __hot skb_to_contig_fd(struct dpa_priv_s *priv,
+				  struct sk_buff *skb, struct qm_fd *fd)
+{
+	struct sk_buff **skbh;
+	dma_addr_t addr;
+	struct dpa_bp *dpa_bp;
+	struct net_device *net_dev = priv->net_dev;
+	int err;
+
+	/* We are guaranteed that we have at least DPA_BP_HEAD of headroom. */
+	skbh = (struct sk_buff **)(skb->data - DPA_BP_HEAD);
+
+	*skbh = skb;
+
+	dpa_bp = priv->dpa_bp;
+
+	/*
+	 * Enable L3/L4 hardware checksum computation.
+	 *
+	 * We must do this before dma_map_single(DMA_TO_DEVICE), because we may
+	 * need to write into the skb.
+	 */
+	err = dpa_enable_tx_csum(priv, skb, fd,
+				 ((char *)skbh) + DPA_PRIV_DATA_SIZE);
+	if (unlikely(err < 0)) {
+		if (netif_msg_tx_err(priv) && net_ratelimit())
+			cpu_netdev_err(net_dev, "HW csum error: %d\n", err);
+		return err;
+	}
+
+	/* Fill in the FD */
+	fd->format = qm_fd_contig;
+	fd->length20 = skb->len;
+	fd->offset = DPA_BP_HEAD; /* This is now guaranteed */
+
+	addr = dma_map_single(dpa_bp->dev, skbh, dpa_bp->size, DMA_TO_DEVICE);
+	if (unlikely(dma_mapping_error(dpa_bp->dev, addr))) {
+		if (netif_msg_tx_err(priv) && net_ratelimit())
+			cpu_netdev_err(net_dev, "dma_map_single() failed\n");
+		return -EINVAL;
+	}
+	fd->addr_hi = upper_32_bits(addr);
+	fd->addr_lo = lower_32_bits(addr);
+
+	return 0;
+}
+
+static int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
+			      struct dpa_percpu_priv_s *percpu_priv,
+			      struct sk_buff *skb, struct qm_fd *fd)
+{
+	struct dpa_bp *dpa_bp = priv->dpa_bp;
+	dma_addr_t addr;
+	struct sk_buff **skbh;
+	struct net_device *net_dev = priv->net_dev;
+	int err;
+
+	struct qm_sg_entry *sgt;
+	unsigned long sgt_page, sg0_page;
+	void *buffer_start;
+	skb_frag_t *frag;
+	int i, j, nr_frags;
+	enum dma_data_direction dma_dir;
+	bool can_recycle = false;
+
+	fd->format = qm_fd_sg;
+
+	/* get a new page to store the SGTable */
+	sgt_page = __get_free_page(GFP_ATOMIC);
+	if (unlikely(!sgt_page)) {
+		dpaa_eth_err(dpa_bp->dev, "__get_free_page() failed\n");
+		return -ENOMEM;
+	}
+
+	/*
+	 * Enable L3/L4 hardware checksum computation.
+	 *
+	 * We must do this before dma_map_single(DMA_TO_DEVICE), because we may
+	 * need to write into the skb.
+	 */
+	err = dpa_enable_tx_csum(priv, skb, fd,
+				 (void *)sgt_page + DPA_PRIV_DATA_SIZE);
+	if (unlikely(err < 0)) {
+		if (netif_msg_tx_err(priv) && net_ratelimit())
+			cpu_netdev_err(net_dev, "HW csum error: %d\n", err);
+		goto csum_failed;
+	}
+
+	sgt = (struct qm_sg_entry *)(sgt_page + DPA_BP_HEAD);
+	/*
+	 * TODO: do we need to memset all entries or just the number of entries
+	 * we really use? Might improve perf...
+	 */
+	memset(sgt, 0, DPA_SGT_MAX_ENTRIES * sizeof(*sgt));
+
+	/*
+	 * Decide whether the skb is recycleable. We will need this information
+	 * upfront to decide what DMA mapping direction we want to use.
+	 */
+	nr_frags =  skb_shinfo(skb)->nr_frags;
+	if (!skb_cloned(skb) && !skb_shared(skb) &&
+	   (*percpu_priv->dpa_bp_count + nr_frags + 2 < dpa_bp->target_count)) {
+		can_recycle = true;
+		/*
+		 * We want each fragment to have at least dpa_bp->size bytes.
+		 * If even one fragment is smaller, the entire FD becomes
+		 * unrecycleable.
+		 */
+		for (i = 0; i < nr_frags; i++)
+			if (skb_shinfo(skb)->frags[i].size < dpa_bp->size) {
+				can_recycle = false;
+				break;
+		}
+	}
+	dma_dir = can_recycle ? DMA_BIDIRECTIONAL : DMA_TO_DEVICE;
+
+	/*
+	 * Populate the first SGT entry
+	 * get a new page to store the skb linear buffer content
+	 * in the first SGT entry
+	 *
+	 * TODO: See if we can use the original page that contains
+	 * the linear buffer
+	 */
+	sg0_page = __get_free_page(GFP_ATOMIC);
+	if (unlikely(!sg0_page)) {
+		dpaa_eth_err(dpa_bp->dev, "__get_free_page() failed\n");
+		err = -ENOMEM;
+		goto sg0_page_alloc_failed;
+	}
+
+	sgt[0].bpid = dpa_bp->bpid;
+	sgt[0].offset = 0;
+	sgt[0].length = skb_headlen(skb);
+
+	if (sgt[0].offset + skb_headlen(skb) > dpa_bp->size) {
+		dpaa_eth_err(dpa_bp->dev, "linear buff is too big\n");
+		err = -EINVAL;
+		goto skb_linear_too_large;
+	}
+
+	buffer_start = (void *)sg0_page;
+	memcpy(buffer_start + sgt[0].offset, skb->data, skb_headlen(skb));
+	addr = dma_map_single(dpa_bp->dev, buffer_start, dpa_bp->size, dma_dir);
+	if (unlikely(dma_mapping_error(dpa_bp->dev, addr))) {
+		dpaa_eth_err(dpa_bp->dev, "DMA mapping failed");
+		err = -EINVAL;
+		goto sg0_map_failed;
+
+	}
+	sgt[0].addr_hi = upper_32_bits(addr);
+	sgt[0].addr_lo = lower_32_bits(addr);
+
+	/* populate the rest of SGT entries */
+	for (i = 1; i <= skb_shinfo(skb)->nr_frags; i++) {
+		frag = &skb_shinfo(skb)->frags[i - 1];
+		sgt[i].bpid = dpa_bp->bpid;
+		sgt[i].offset = 0;
+		sgt[i].length = frag->size;
+
+		/* This shouldn't happen */
+		BUG_ON(!frag->page);
+
+		buffer_start = page_address(frag->page) + frag->page_offset;
+		addr = dma_map_single(dpa_bp->dev, buffer_start, dpa_bp->size,
+				      dma_dir);
+		if (unlikely(dma_mapping_error(dpa_bp->dev, addr))) {
+			dpaa_eth_err(dpa_bp->dev, "DMA mapping failed");
+			err = -EINVAL;
+			goto sg_map_failed;
+		}
+		/* keep the offset in the address */
+		sgt[i].addr_hi = upper_32_bits(addr);
+		sgt[i].addr_lo = lower_32_bits(addr);
+	}
+	sgt[i - 1].final = 1;
+
+	fd->length20 = skb->len;
+	fd->offset = DPA_BP_HEAD;
+
+	buffer_start = (void *)sgt - dpa_fd_offset(fd);
+	skbh = (struct sk_buff **)buffer_start;
+	*skbh = skb;
+
+	/* DMA map the SGT page */
+	addr = dma_map_single(dpa_bp->dev, buffer_start, dpa_bp->size, dma_dir);
+	if (unlikely(dma_mapping_error(dpa_bp->dev, addr))) {
+		dpaa_eth_err(dpa_bp->dev, "DMA mapping failed");
+		err = -EINVAL;
+		goto sgt_map_failed;
+	}
+	fd->addr_hi = upper_32_bits(addr);
+	fd->addr_lo = lower_32_bits(addr);
+
+	if (can_recycle) {
+		/* all pages are going to be recycled */
+		fd->cmd |= FM_FD_CMD_FCO;
+		fd->bpid = dpa_bp->bpid;
+	}
+
+	return 0;
+
+sgt_map_failed:
+sg_map_failed:
+	for (j = 0; j < i; j++)
+		dma_unmap_single(dpa_bp->dev, qm_sg_addr(&sgt[j]), dpa_bp->size,
+				 dma_dir);
+sg0_map_failed:
+skb_linear_too_large:
+	free_page(sg0_page);
+sg0_page_alloc_failed:
+csum_failed:
+	free_page(sgt_page);
+
+	return err;
+}
+
+int __hot dpa_tx(struct sk_buff *skb, struct net_device *net_dev)
+{
+	struct dpa_priv_s	*priv;
+	struct qm_fd		 fd;
+	struct dpa_percpu_priv_s *percpu_priv;
+	int queue_mapping;
+	int err;
+
+	priv = netdev_priv(net_dev);
+	percpu_priv = per_cpu_ptr(priv->percpu_priv, smp_processor_id());
+
+	clear_fd(&fd);
+
+	queue_mapping = skb_get_queue_mapping(skb);
+
+
+#ifdef CONFIG_FSL_DPA_1588
+	if (priv->tsu && priv->tsu->valid && priv->tsu->hwts_tx_en_ioctl)
+		fd.cmd |= FM_FD_CMD_UPD;
+#endif
+
+	if (skb_is_nonlinear(skb)) {
+		/* Just create a S/G fd based on the skb */
+		err = skb_to_sg_fd(priv, percpu_priv, skb, &fd);
+	} else {
+		/*
+		 * Make sure we have enough headroom to accomodate private
+		 * data, parse results, etc
+		 */
+		if (skb_headroom(skb) < DPA_BP_HEAD) {
+			struct sk_buff *skb_new;
+
+			skb_new = skb_realloc_headroom(skb, DPA_BP_HEAD);
+			if (unlikely(!skb_new)) {
+				dev_kfree_skb(skb);
+				percpu_priv->stats.tx_errors++;
+				return NETDEV_TX_OK;
+			}
+			dev_kfree_skb(skb);
+			skb = skb_new;
+		}
+
+		/*
+		 * We're going to store the skb backpointer at the beginning
+		 * of the data buffer, so we need a privately owned skb
+		 */
+		skb = skb_unshare(skb, GFP_ATOMIC);
+		if (unlikely(!skb)) {
+			percpu_priv->stats.tx_errors++;
+			return NETDEV_TX_OK;
+		}
+
+		/* Finally, create a contig FD from this skb */
+		err = skb_to_contig_fd(priv, skb, &fd);
+	}
+	if (unlikely(err < 0)) {
+		percpu_priv->stats.tx_errors++;
+		dev_kfree_skb(skb);
+		return NETDEV_TX_OK;
+	}
+	if (fd.cmd & FM_FD_CMD_FCO) {
+		/*
+		 * Need to free the skb, but without releasing
+		 * the page fragments, so increment the pages usage count
+		 */
+		int i;
+
+		for (i = 0; i < skb_shinfo(skb)->nr_frags; i++)
+			get_page(skb_shinfo(skb)->frags[i].page);
+
+		/*
+		 * We release back to the pool a number of pages equal to
+		 * the number of skb fragments + one page for the linear
+		 * portion of the skb + one page for the S/G table
+		 */
+		*percpu_priv->dpa_bp_count += skb_shinfo(skb)->nr_frags + 2;
+		percpu_priv->tx_returned++;
+		dev_kfree_skb(skb);
+		skb = NULL;
+	}
+
+	if (unlikely(dpa_xmit(priv, percpu_priv, queue_mapping, &fd) < 0))
+		goto xmit_failed;
+
+	net_dev->trans_start = jiffies;
+
+	return NETDEV_TX_OK;
+
+xmit_failed:
+	if (fd.cmd & FM_FD_CMD_FCO) {
+		dpa_fd_release(net_dev, &fd);
+		return NETDEV_TX_OK;
+	}
+	_dpa_cleanup_tx_fd(priv, &fd);
+	dev_kfree_skb(skb);
+
+	return NETDEV_TX_OK;
+}
+
+#endif /* CONFIG_DPAA_ETH_SG_SUPPORT */
-- 
1.7.9.7

