From e462e8f42b8821b804866868950d9355643bcc39 Mon Sep 17 00:00:00 2001
From: Camelia Groza <camelia.groza@nxp.com>
Date: Tue, 25 Oct 2016 20:09:25 +0300
Subject: [PATCH 140/388] dpaa_eth: enable the 4K workaround only for the
 LS1043 SoC

For ARM platforms add runtime checks to verify whether the FMan 4K
errata workaround should be applied or not. These checks might reduce
performance for ARM SoCs.

Signed-off-by: Camelia Groza <camelia.groza@nxp.com>
[Original patch taken from SDK-V2.0-1703]
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 drivers/net/ethernet/freescale/sdk_dpaa/Kconfig    |   1 -
 drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth.c |  20 ++-
 drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth.h |  36 +++-
 .../ethernet/freescale/sdk_dpaa/dpaa_eth_common.h  |   9 +-
 .../ethernet/freescale/sdk_dpaa/dpaa_eth_generic.c |  12 ++
 .../net/ethernet/freescale/sdk_dpaa/dpaa_eth_sg.c  | 182 ++++++++++++++++-----
 6 files changed, 203 insertions(+), 57 deletions(-)

diff --git a/drivers/net/ethernet/freescale/sdk_dpaa/Kconfig b/drivers/net/ethernet/freescale/sdk_dpaa/Kconfig
index ec1115d..89e3165 100644
--- a/drivers/net/ethernet/freescale/sdk_dpaa/Kconfig
+++ b/drivers/net/ethernet/freescale/sdk_dpaa/Kconfig
@@ -63,7 +63,6 @@ config FSL_DPAA_GENERIC_DRIVER
 
 config FSL_DPAA_ETH_JUMBO_FRAME
 	bool "Optimize for jumbo frames"
-	depends on !ARM64
 	default n
 	---help---
 	  Optimize the DPAA Ethernet driver throughput for large frames
diff --git a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth.c b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth.c
index fbe4718..50b1676 100644
--- a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth.c
+++ b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth.c
@@ -102,6 +102,11 @@ static const char rtx[][3] = {
 	[TX] = "TX"
 };
 
+#ifdef CONFIG_ARM64
+bool dpa_4k_issue;
+EXPORT_SYMBOL(dpa_4k_issue);
+#endif
+
 /* BM */
 
 #define DPAA_ETH_MAX_PAD (L1_CACHE_BYTES * 8)
@@ -954,13 +959,19 @@ dpaa_eth_priv_probe(struct platform_device *_of_dev)
 	 * buffer pool, based on FMan port buffer layout;also update
 	 * the maximum buffer size for private ports if necessary
 	 */
-	dpa_bp->size = dpa_bp_size(&buf_layout[RX]);
+#ifdef CONFIG_FSL_DPAA_ETH_JUMBO_FRAME
+	/* On LS1043 we do not allow large buffers due to the 4K errata. */
+	if (unlikely(dpa_4k_errata))
+		dpa_bp->size = dpa_4k_bp_size(&buf_layout[RX]);
+	else
+#endif
+		dpa_bp->size = dpa_bp_size(&buf_layout[RX]);
 
 #ifdef CONFIG_FSL_DPAA_ETH_JUMBO_FRAME
 	/* We only want to use jumbo frame optimization if we actually have
 	 * L2 MAX FRM set for jumbo frames as well.
 	 */
-	if (fm_get_max_frm() < 9600)
+	if (likely(!dpa_4k_errata) && fm_get_max_frm() < 9600)
 		dev_warn(dev,
 			"Invalid configuration: if jumbo frames support is on, FSL_FM_MAX_FRAME_SIZE should be set to 9600\n");
 #endif
@@ -1149,6 +1160,11 @@ static int __init __cold dpa_load(void)
 	dpa_max_frm = fm_get_max_frm();
 	dpa_num_cpus = num_possible_cpus();
 
+#ifdef CONFIG_ARM64
+	/* Detect if the current SoC requires the 4K alignment workaround */
+	dpa_4k_issue = check_4k_issue_soc();
+#endif
+
 #ifdef CONFIG_FSL_DPAA_DBG_LOOP
 	memset(dpa_loop_netdevs, 0, sizeof(dpa_loop_netdevs));
 #endif
diff --git a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth.h b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth.h
index 677b66c..4366358 100644
--- a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth.h
+++ b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth.h
@@ -34,6 +34,7 @@
 
 #include <linux/netdevice.h>
 #include <linux/fsl_qman.h>	/* struct qman_fq */
+#include <linux/sys_soc.h>	/* soc_device_match */
 
 #include "fm_ext.h"
 #include "dpaa_eth_trace.h"
@@ -101,7 +102,13 @@ struct dpa_buffer_layout_s {
 #define DPA_BP_RAW_SIZE \
 	((DPA_MAX_FRM_SIZE + DPA_MAX_FD_OFFSET + \
 	  sizeof(struct skb_shared_info) + 128) & ~(SMP_CACHE_BYTES - 1))
-#endif
+#ifdef CONFIG_ARM64
+/* On LS1043 we do not use Jumbo and large buffers due to the 4K errata */
+#define DPA_BP_RAW_SIZE_4K	2048
+#define dpa_4k_bp_size(buffer_layout)	(SKB_WITH_OVERHEAD(DPA_BP_RAW_SIZE_4K) - \
+						SMP_CACHE_BYTES)
+#endif /* CONFIG_ARM64 */
+#endif /* CONFIG_FSL_DPAA_ETH_JUMBO_FRAME */
 
 /* This is what FMan is ever allowed to use.
  * FMan-DMA requires 16-byte alignment for Rx buffers, but SKB_DATA_ALIGN is
@@ -676,18 +683,33 @@ static inline void _dpa_bp_free_pf(void *addr)
 /* TODO: LS1043A SoC has a HW issue regarding FMan DMA transactions; The issue
  * manifests itself at high traffic rates when frames exceed 4K memory
  * boundaries; For the moment, we use a SW workaround to avoid frames larger
- * than 4K or that exceed 4K alignements.
+ * than 4K or that exceed 4K alignments.
  */
 
 #ifdef CONFIG_ARM64
-#define DPAA_LS1043A_DMA_4K_ISSUE	1
-#endif
+/* Detect the SoC at runtime */
+static inline bool check_4k_issue_soc(void)
+{
+	const struct soc_device_attribute soc_msi_matches[] = {
+		{ .family = "QorIQ LS1043A",
+		  .data = NULL },
+		{ },
+	};
 
-#ifdef DPAA_LS1043A_DMA_4K_ISSUE
+	if (soc_device_match(soc_msi_matches))
+		return true;
+
+	return false;
+}
+
+extern bool dpa_4k_issue;
+#define dpa_4k_errata	dpa_4k_issue
 #define HAS_DMA_ISSUE(start, size) \
 	(((u64)(start) ^ ((u64)(start) + (u64)(size))) & ~0xFFF)
-
 #define BOUNDARY_4K(start, size) (((u64)(start) + (u64)(size)) & ~0xFFF)
-#endif  /* DPAA_LS1043A_DMA_4K_ISSUE  */
+
+#else
+#define dpa_4k_errata	false
+#endif  /* CONFIG_ARM64 */
 
 #endif	/* __DPA_H */
diff --git a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_common.h b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_common.h
index 54d039c..8a41c16 100644
--- a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_common.h
+++ b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_common.h
@@ -57,12 +57,11 @@
 
 #define DPA_SGT_MAX_ENTRIES 16 /* maximum number of entries in SG Table */
 
-#ifdef DPAA_LS1043A_DMA_4K_ISSUE
-/* each S/G entry can be divided into two S/G entries */
-#define DPA_SGT_ENTRIES_THRESHOLD 	7
-#else
 #define DPA_SGT_ENTRIES_THRESHOLD	DPA_SGT_MAX_ENTRIES
-#endif /* DPAA_LS1043A_DMA_4K_ISSUE */
+#ifdef CONFIG_ARM64
+/* each S/G entry can be divided into two S/G entries */
+#define DPA_SGT_4K_ENTRIES_THRESHOLD	7
+#endif /* CONFIG_ARM64 */
 
 
 #define DPA_BUFF_RELEASE_MAX 8 /* maximum number of buffers released at once */
diff --git a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_generic.c b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_generic.c
index 2876244..f0c375f 100644
--- a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_generic.c
+++ b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_generic.c
@@ -1216,6 +1216,12 @@ static int dpa_generic_tx_bp_probe(struct platform_device *_of_dev,
 		return -ENOMEM;
 	}
 
+#ifdef CONFIG_FSL_DPAA_ETH_JUMBO_FRAME
+	/* On LS1043 we do not allow large buffers due to the 4K errata. */
+	if (unlikely(dpa_4k_errata))
+		bp->size = dpa_4k_bp_size(buf_layout);
+	else
+#endif
 	bp->size = dpa_bp_size(buf_layout);
 	bp->percpu_count = devm_alloc_percpu(dev, *bp->percpu_count);
 	bp->target_count = CONFIG_FSL_DPAA_ETH_MAX_BUF_COUNT;
@@ -1228,6 +1234,12 @@ static int dpa_generic_tx_bp_probe(struct platform_device *_of_dev,
 		return -ENOMEM;
 	}
 
+#ifdef CONFIG_FSL_DPAA_ETH_JUMBO_FRAME
+	/* On LS1043 we do not allow large buffers due to the 4K errata. */
+	if (unlikely(dpa_4k_errata))
+		bp_sg->size = dpa_4k_bp_size(buf_layout);
+	else
+#endif
 	bp_sg->size = dpa_bp_size(buf_layout);
 	bp_sg->percpu_count = alloc_percpu(*bp_sg->percpu_count);
 	bp_sg->target_count = CONFIG_FSL_DPAA_ETH_MAX_BUF_COUNT;
diff --git a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_sg.c b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_sg.c
index a66b337..4164100 100644
--- a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_sg.c
+++ b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_sg.c
@@ -99,8 +99,12 @@ static int _dpa_bp_add_8_bufs(const struct dpa_bp *dpa_bp)
 		 * We only need enough space to store a pointer, but allocate
 		 * an entire cacheline for performance reasons.
 		 */
-#ifdef DPAA_LS1043A_DMA_4K_ISSUE
-		new_buf	= page_address(alloc_page(GFP_ATOMIC));
+#ifdef CONFIG_ARM64
+		if (unlikely(dpa_4k_errata))
+			new_buf = page_address(alloc_page(GFP_ATOMIC));
+		else
+			new_buf = netdev_alloc_frag(SMP_CACHE_BYTES +
+						    DPA_BP_RAW_SIZE);
 #else
 		new_buf = netdev_alloc_frag(SMP_CACHE_BYTES + DPA_BP_RAW_SIZE);
 #endif
@@ -235,13 +239,21 @@ struct sk_buff *_dpa_cleanup_tx_fd(const struct dpa_priv_s *priv,
 
 	if (unlikely(fd->format == qm_fd_sg)) {
 		nr_frags = skb_shinfo(skb)->nr_frags;
-#ifdef DPAA_LS1043A_DMA_4K_ISSUE
+
+#ifdef CONFIG_ARM64
 /* addressing the 4k DMA issue can yield a larger number of fragments than
  * the skb had
  */
-		dma_unmap_single(dpa_bp->dev, addr, dpa_fd_offset(fd) +
-				 sizeof(struct qm_sg_entry) * DPA_SGT_MAX_ENTRIES,
-				 dma_dir);
+		if (unlikely(dpa_4k_errata))
+			dma_unmap_single(dpa_bp->dev, addr, dpa_fd_offset(fd) +
+					 sizeof(struct qm_sg_entry) *
+					 DPA_SGT_MAX_ENTRIES,
+					 dma_dir);
+		else
+			dma_unmap_single(dpa_bp->dev, addr, dpa_fd_offset(fd) +
+					 sizeof(struct qm_sg_entry) *
+					 (1 + nr_frags),
+					 dma_dir);
 #else
 		dma_unmap_single(dpa_bp->dev, addr, dpa_fd_offset(fd) +
 				 sizeof(struct qm_sg_entry) * (1 + nr_frags),
@@ -270,14 +282,26 @@ struct sk_buff *_dpa_cleanup_tx_fd(const struct dpa_priv_s *priv,
 		sg_addr = qm_sg_addr(&sgt[0]);
 		sg_len = qm_sg_entry_get_len(&sgt[0]);
 		dma_unmap_single(dpa_bp->dev, sg_addr, sg_len, dma_dir);
-#ifdef DPAA_LS1043A_DMA_4K_ISSUE
-		i = 1;
-		do {
-			DPA_BUG_ON(qm_sg_entry_get_ext(&sgt[i]));
-			sg_addr = qm_sg_addr(&sgt[i]);
-			sg_len = qm_sg_entry_get_len(&sgt[i]);
-			dma_unmap_page(dpa_bp->dev, sg_addr, sg_len, dma_dir);
-		} while (!qm_sg_entry_get_final(&sgt[i++]));
+#ifdef CONFIG_ARM64
+		if (unlikely(dpa_4k_errata)) {
+			i = 1;
+			do {
+				DPA_BUG_ON(qm_sg_entry_get_ext(&sgt[i]));
+				sg_addr = qm_sg_addr(&sgt[i]);
+				sg_len = qm_sg_entry_get_len(&sgt[i]);
+				dma_unmap_page(dpa_bp->dev, sg_addr, sg_len,
+					       dma_dir);
+			} while (!qm_sg_entry_get_final(&sgt[i++]));
+		} else {
+			/* remaining pages were mapped with dma_map_page() */
+			for (i = 1; i <= nr_frags; i++) {
+				DPA_BUG_ON(qm_sg_entry_get_ext(&sgt[i]));
+				sg_addr = qm_sg_addr(&sgt[i]);
+				sg_len = qm_sg_entry_get_len(&sgt[i]);
+				dma_unmap_page(dpa_bp->dev, sg_addr, sg_len,
+					       dma_dir);
+			}
+		}
 #else
 		/* remaining pages were mapped with dma_map_page() */
 		for (i = 1; i <= nr_frags; i++) {
@@ -747,7 +771,7 @@ int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
 	struct net_device *net_dev = priv->net_dev;
 	int sg_len;
 	int err;
-#ifdef DPAA_LS1043A_DMA_4K_ISSUE
+#ifdef CONFIG_ARM64
 	dma_addr_t boundary;
 	int k;
 #endif
@@ -756,23 +780,35 @@ int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
 	void *sgt_buf;
 	void *buffer_start;
 	skb_frag_t *frag;
-	int i, j;
+	int i, j = 0;
 	const enum dma_data_direction dma_dir = DMA_TO_DEVICE;
 	const int nr_frags = skb_shinfo(skb)->nr_frags;
 
 	fd->format = qm_fd_sg;
-#ifdef DPAA_LS1043A_DMA_4K_ISSUE
-	/* get a page frag to store the SGTable */
-	sgt_buf = netdev_alloc_frag(priv->tx_headroom +
-		sizeof(struct qm_sg_entry) * DPA_SGT_MAX_ENTRIES);
-	if (unlikely(!sgt_buf)) {
-		dev_err(dpa_bp->dev, "netdev_alloc_frag() failed\n");
-		return -ENOMEM;
-	}
+#ifdef CONFIG_ARM64
+	if (unlikely(dpa_4k_errata)) {
+		/* get a page frag to store the SGTable */
+		sgt_buf = netdev_alloc_frag(priv->tx_headroom +
+			sizeof(struct qm_sg_entry) * DPA_SGT_MAX_ENTRIES);
+		if (unlikely(!sgt_buf)) {
+			dev_err(dpa_bp->dev, "netdev_alloc_frag() failed\n");
+			return -ENOMEM;
+		}
 
-	/* it seems that the memory allocator does not zero the allocated mem */
-	memset(sgt_buf, 0, priv->tx_headroom +
-		sizeof(struct qm_sg_entry) * DPA_SGT_MAX_ENTRIES);
+		memset(sgt_buf, 0, priv->tx_headroom +
+			sizeof(struct qm_sg_entry) * DPA_SGT_MAX_ENTRIES);
+	} else {
+		/* get a page frag to store the SGTable */
+		sgt_buf = netdev_alloc_frag(priv->tx_headroom +
+			sizeof(struct qm_sg_entry) * (1 + nr_frags));
+		if (unlikely(!sgt_buf)) {
+			dev_err(dpa_bp->dev, "netdev_alloc_frag() failed\n");
+			return -ENOMEM;
+		}
+
+		memset(sgt_buf, 0, priv->tx_headroom +
+			sizeof(struct qm_sg_entry) * (1 + nr_frags));
+	}
 #else
 	/* get a page frag to store the SGTable */
 	sgt_buf = netdev_alloc_frag(priv->tx_headroom +
@@ -782,6 +818,7 @@ int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
 		return -ENOMEM;
 	}
 
+	/* it seems that the memory allocator does not zero the allocated mem */
 	memset(sgt_buf, 0, priv->tx_headroom +
 		sizeof(struct qm_sg_entry) * (1 + nr_frags));
 #endif
@@ -817,8 +854,38 @@ int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
 
 	qm_sg_entry_set64(&sgt[0], addr);
 
-#ifdef DPAA_LS1043A_DMA_4K_ISSUE
-	j = 0;
+#ifdef CONFIG_ARM64
+	if (unlikely(dpa_4k_errata))
+		goto workaround;
+
+	/* populate the rest of SGT entries */
+	for (i = 1; i <= nr_frags; i++) {
+		frag = &skb_shinfo(skb)->frags[i - 1];
+		qm_sg_entry_set_bpid(&sgt[i], 0xff);
+		qm_sg_entry_set_offset(&sgt[i], 0);
+		qm_sg_entry_set_len(&sgt[i], frag->size);
+		qm_sg_entry_set_ext(&sgt[i], 0);
+
+		if (i == nr_frags)
+			qm_sg_entry_set_final(&sgt[i], 1);
+		else
+			qm_sg_entry_set_final(&sgt[i], 0);
+
+		DPA_BUG_ON(!skb_frag_page(frag));
+		addr = skb_frag_dma_map(dpa_bp->dev, frag, 0, frag->size,
+					dma_dir);
+		if (unlikely(dma_mapping_error(dpa_bp->dev, addr))) {
+			dev_err(dpa_bp->dev, "DMA mapping failed");
+			err = -EINVAL;
+			goto sg_map_failed;
+		}
+
+		/* keep the offset in the address */
+		qm_sg_entry_set64(&sgt[i], addr);
+	}
+	goto bypass_workaround;
+
+workaround:
 	if (unlikely(HAS_DMA_ISSUE(skb->data, sg_len))) {
 		boundary = BOUNDARY_4K(skb->data, sg_len);
 		qm_sg_entry_set_len(&sgt[j], (u64)boundary - (u64)addr);
@@ -877,6 +944,9 @@ int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
 			qm_sg_entry_set_final(&sgt[j], 1);
 		else
 			qm_sg_entry_set_final(&sgt[j], 0);
+	}
+
+bypass_workaround:
 #else
 
 	/* populate the rest of SGT entries */
@@ -903,8 +973,8 @@ int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
 
 		/* keep the offset in the address */
 		qm_sg_entry_set64(&sgt[i], addr);
-#endif
 	}
+#endif
 
 	fd->length20 = skb->len;
 	fd->offset = priv->tx_headroom;
@@ -912,10 +982,19 @@ int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
 	/* DMA map the SGT page */
 	buffer_start = (void *)sgt - priv->tx_headroom;
 	DPA_WRITE_SKB_PTR(skb, skbh, buffer_start, 0);
-#ifdef DPAA_LS1043A_DMA_4K_ISSUE
-	addr = dma_map_single(dpa_bp->dev, buffer_start, priv->tx_headroom +
-			      sizeof(struct qm_sg_entry) * DPA_SGT_MAX_ENTRIES,
-			      dma_dir);
+#ifdef CONFIG_ARM64
+	if (unlikely(dpa_4k_errata))
+		addr = dma_map_single(dpa_bp->dev, buffer_start,
+				      priv->tx_headroom +
+				      sizeof(struct qm_sg_entry) *
+				      DPA_SGT_MAX_ENTRIES,
+				      dma_dir);
+	else
+		addr = dma_map_single(dpa_bp->dev, buffer_start,
+				      priv->tx_headroom +
+				      sizeof(struct qm_sg_entry) *
+				      (1 + nr_frags),
+				      dma_dir);
 #else
 	addr = dma_map_single(dpa_bp->dev, buffer_start, priv->tx_headroom +
 			      sizeof(struct qm_sg_entry) * (1 + nr_frags),
@@ -935,12 +1014,19 @@ int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
 
 sgt_map_failed:
 sg_map_failed:
-#ifdef DPAA_LS1043A_DMA_4K_ISSUE
-	for (k = 0; k < j; k++) {
-		sg_addr = qm_sg_addr(&sgt[k]);
-		dma_unmap_page(dpa_bp->dev, sg_addr,
-			       qm_sg_entry_get_len(&sgt[k]), dma_dir);
-	}
+#ifdef CONFIG_ARM64
+	if (unlikely(dpa_4k_errata))
+		for (k = 0; k < j; k++) {
+			sg_addr = qm_sg_addr(&sgt[k]);
+			dma_unmap_page(dpa_bp->dev, sg_addr,
+				       qm_sg_entry_get_len(&sgt[k]), dma_dir);
+		}
+	else
+		for (j = 0; j < i; j++) {
+			sg_addr = qm_sg_addr(&sgt[j]);
+			dma_unmap_page(dpa_bp->dev, sg_addr,
+				       qm_sg_entry_get_len(&sgt[j]), dma_dir);
+		}
 #else
 	for (j = 0; j < i; j++) {
 		sg_addr = qm_sg_addr(&sgt[j]);
@@ -1018,11 +1104,22 @@ int __hot dpa_tx_extended(struct sk_buff *skb, struct net_device *net_dev,
 	 * Btw, we're using the first sgt entry to store the linear part of
 	 * the skb, so we're one extra frag short.
 	 */
+#ifdef CONFIG_ARM64
+	if (nonlinear &&
+	    ((unlikely(dpa_4k_errata) &&
+	      likely(skb_shinfo(skb)->nr_frags < DPA_SGT_4K_ENTRIES_THRESHOLD)) ||
+	    (likely(!dpa_4k_errata) &&
+	     likely(skb_shinfo(skb)->nr_frags < DPA_SGT_ENTRIES_THRESHOLD)))) {
+		/* Just create a S/G fd based on the skb */
+		err = skb_to_sg_fd(priv, skb, &fd);
+		percpu_priv->tx_frag_skbuffs++;
+#else
 	if (nonlinear &&
 		likely(skb_shinfo(skb)->nr_frags < DPA_SGT_ENTRIES_THRESHOLD)) {
 		/* Just create a S/G fd based on the skb */
 		err = skb_to_sg_fd(priv, skb, &fd);
 		percpu_priv->tx_frag_skbuffs++;
+#endif
 	} else {
 		/* Make sure we have enough headroom to accommodate private
 		 * data, parse results, etc. Normally this shouldn't happen if
@@ -1062,8 +1159,9 @@ int __hot dpa_tx_extended(struct sk_buff *skb, struct net_device *net_dev,
 			/* Common out-of-memory error path */
 			goto enomem;
 
-#ifdef DPAA_LS1043A_DMA_4K_ISSUE
-		if (unlikely(HAS_DMA_ISSUE(skb->data, skb->len))) {
+#ifdef CONFIG_ARM64
+		if (unlikely(dpa_4k_errata) &&
+		    unlikely(HAS_DMA_ISSUE(skb->data, skb->len))) {
 			err = skb_to_sg_fd(priv, skb, &fd);
 			percpu_priv->tx_frag_skbuffs++;
 		} else {
-- 
2.9.3

