From 46cbd075510ab51936a8f51794db5c3568ef583e Mon Sep 17 00:00:00 2001
From: Tudor Ambarus <tudor-dan.ambarus@nxp.com>
Date: Tue, 28 Jun 2016 20:01:47 +0300
Subject: [PATCH 031/388] crypto: caampkc - handle core endianness != caam
 endianness

There are SoCs like LS1043A where CAAM endianness (BE) does not match
the default endianness of the core (LE). Update the caampkc driver
so that it works in these cases.

Testing was done using openssl speed algorithms on LS1043A and LS1012A.

Signed-off-by: Tudor Ambarus <tudor-dan.ambarus@nxp.com>
[Original patch taken from SDK-V2.0-1703]
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 drivers/crypto/caam/caampkc.c     | 156 ++++++-------
 drivers/crypto/caam/desc_constr.h |   7 +
 drivers/crypto/caam/pkc_desc.c    | 469 ++++++++++++++++----------------------
 3 files changed, 274 insertions(+), 358 deletions(-)

diff --git a/drivers/crypto/caam/caampkc.c b/drivers/crypto/caam/caampkc.c
index 1a8656a..1ae96eb 100644
--- a/drivers/crypto/caam/caampkc.c
+++ b/drivers/crypto/caam/caampkc.c
@@ -43,33 +43,33 @@ static void rsa_unmap(struct device *dev,
 	case RSA_PUB:
 		{
 			struct rsa_pub_req_s *pub_req = &req->req_u.rsa_pub_req;
-			struct rsa_pub_desc_s *rsa_pub_desc =
-			    (struct rsa_pub_desc_s *)edesc->hw_desc;
-
-			dma_unmap_single(dev, rsa_pub_desc->n_dma,
-					 pub_req->n_len, DMA_TO_DEVICE);
-			dma_unmap_single(dev, rsa_pub_desc->e_dma,
-					 pub_req->e_len, DMA_TO_DEVICE);
-			dma_unmap_single(dev, rsa_pub_desc->g_dma,
-					 pub_req->g_len, DMA_FROM_DEVICE);
-			dma_unmap_single(dev, rsa_pub_desc->f_dma,
-					 pub_req->f_len, DMA_TO_DEVICE);
+			struct rsa_pub_edesc_s *pub_edesc =
+					&edesc->dma_u.rsa_pub_edesc;
+
+			dma_unmap_single(dev, pub_edesc->n_dma, pub_req->n_len,
+					 DMA_TO_DEVICE);
+			dma_unmap_single(dev, pub_edesc->e_dma, pub_req->e_len,
+					 DMA_TO_DEVICE);
+			dma_unmap_single(dev, pub_edesc->g_dma, pub_req->g_len,
+					 DMA_FROM_DEVICE);
+			dma_unmap_single(dev, pub_edesc->f_dma, pub_req->f_len,
+					 DMA_TO_DEVICE);
 			break;
 		}
 	case RSA_PRIV_FORM1:
 		{
 			struct rsa_priv_frm1_req_s *priv_req =
 			    &req->req_u.rsa_priv_f1;
-			struct rsa_priv_frm1_desc_s *rsa_priv_desc =
-			    (struct rsa_priv_frm1_desc_s *)edesc->hw_desc;
+			struct rsa_priv_frm1_edesc_s *priv_edesc =
+					&edesc->dma_u.rsa_priv_f1_edesc;
 
-			dma_unmap_single(dev, rsa_priv_desc->n_dma,
+			dma_unmap_single(dev, priv_edesc->n_dma,
 					 priv_req->n_len, DMA_TO_DEVICE);
-			dma_unmap_single(dev, rsa_priv_desc->d_dma,
+			dma_unmap_single(dev, priv_edesc->d_dma,
 					 priv_req->d_len, DMA_TO_DEVICE);
-			dma_unmap_single(dev, rsa_priv_desc->f_dma,
+			dma_unmap_single(dev, priv_edesc->f_dma,
 					 priv_req->f_len, DMA_FROM_DEVICE);
-			dma_unmap_single(dev, rsa_priv_desc->g_dma,
+			dma_unmap_single(dev, priv_edesc->g_dma,
 					 priv_req->g_len, DMA_TO_DEVICE);
 			break;
 		}
@@ -77,27 +77,23 @@ static void rsa_unmap(struct device *dev,
 		{
 			struct rsa_priv_frm2_req_s *priv_req =
 			    &req->req_u.rsa_priv_f2;
-			struct rsa_priv_frm2_desc_s *rsa_priv_desc =
-			    (struct rsa_priv_frm2_desc_s *)edesc->hw_desc;
+			struct rsa_priv_frm2_edesc_s *priv_edesc =
+					&edesc->dma_u.rsa_priv_f2_edesc;
 
-			dma_unmap_single(dev, rsa_priv_desc->p_dma,
+			dma_unmap_single(dev, priv_edesc->p_dma,
 					 priv_req->p_len, DMA_TO_DEVICE);
-			dma_unmap_single(dev, rsa_priv_desc->q_dma,
+			dma_unmap_single(dev, priv_edesc->q_dma,
 					 priv_req->q_len, DMA_TO_DEVICE);
-			dma_unmap_single(dev, rsa_priv_desc->d_dma,
+			dma_unmap_single(dev, priv_edesc->d_dma,
 					 priv_req->d_len, DMA_TO_DEVICE);
-			dma_unmap_single(dev, rsa_priv_desc->g_dma,
+			dma_unmap_single(dev, priv_edesc->g_dma,
 					 priv_req->g_len, DMA_TO_DEVICE);
-			dma_unmap_single(dev, rsa_priv_desc->f_dma,
+			dma_unmap_single(dev, priv_edesc->f_dma,
 					 priv_req->f_len, DMA_FROM_DEVICE);
-			dma_unmap_single(dev,
-					 edesc->dma_u.rsa_priv_f2_edesc.
-					 tmp1_dma, priv_req->p_len,
-					 DMA_BIDIRECTIONAL);
-			dma_unmap_single(dev,
-					 edesc->dma_u.rsa_priv_f2_edesc.
-					 tmp2_dma, priv_req->q_len,
-					 DMA_BIDIRECTIONAL);
+			dma_unmap_single(dev, priv_edesc->tmp1_dma,
+					 priv_req->p_len, DMA_BIDIRECTIONAL);
+			dma_unmap_single(dev, priv_edesc->tmp2_dma,
+					 priv_req->q_len, DMA_BIDIRECTIONAL);
 			kfree(edesc->dma_u.rsa_priv_f2_edesc.tmp1);
 			kfree(edesc->dma_u.rsa_priv_f2_edesc.tmp2);
 			break;
@@ -106,31 +102,27 @@ static void rsa_unmap(struct device *dev,
 		{
 			struct rsa_priv_frm3_req_s *priv_req =
 			    &req->req_u.rsa_priv_f3;
-			struct rsa_priv_frm3_desc_s *rsa_priv_desc =
-			    (struct rsa_priv_frm3_desc_s *)edesc->hw_desc;
+			struct rsa_priv_frm3_edesc_s *priv_edesc =
+					&edesc->dma_u.rsa_priv_f3_edesc;
 
-			dma_unmap_single(dev, rsa_priv_desc->p_dma,
+			dma_unmap_single(dev, priv_edesc->p_dma,
 					 priv_req->p_len, DMA_TO_DEVICE);
-			dma_unmap_single(dev, rsa_priv_desc->q_dma,
+			dma_unmap_single(dev, priv_edesc->q_dma,
 					 priv_req->q_len, DMA_TO_DEVICE);
-			dma_unmap_single(dev, rsa_priv_desc->dq_dma,
+			dma_unmap_single(dev, priv_edesc->dq_dma,
 					 priv_req->dq_len, DMA_TO_DEVICE);
-			dma_unmap_single(dev, rsa_priv_desc->dp_dma,
+			dma_unmap_single(dev, priv_edesc->dp_dma,
 					 priv_req->dp_len, DMA_TO_DEVICE);
-			dma_unmap_single(dev, rsa_priv_desc->c_dma,
+			dma_unmap_single(dev, priv_edesc->c_dma,
 					 priv_req->c_len, DMA_TO_DEVICE);
-			dma_unmap_single(dev, rsa_priv_desc->g_dma,
+			dma_unmap_single(dev, priv_edesc->g_dma,
 					 priv_req->g_len, DMA_TO_DEVICE);
-			dma_unmap_single(dev, rsa_priv_desc->f_dma,
+			dma_unmap_single(dev, priv_edesc->f_dma,
 					 priv_req->f_len, DMA_FROM_DEVICE);
-			dma_unmap_single(dev,
-					 edesc->dma_u.rsa_priv_f3_edesc.
-					 tmp1_dma, priv_req->p_len,
-					 DMA_BIDIRECTIONAL);
-			dma_unmap_single(dev,
-					 edesc->dma_u.rsa_priv_f3_edesc.
-					 tmp2_dma, priv_req->q_len,
-					 DMA_BIDIRECTIONAL);
+			dma_unmap_single(dev, priv_edesc->tmp1_dma,
+					 priv_req->p_len, DMA_BIDIRECTIONAL);
+			dma_unmap_single(dev, priv_edesc->tmp2_dma,
+					 priv_req->q_len, DMA_BIDIRECTIONAL);
 			kfree(edesc->dma_u.rsa_priv_f3_edesc.tmp1);
 			kfree(edesc->dma_u.rsa_priv_f3_edesc.tmp2);
 			break;
@@ -162,18 +154,17 @@ static void dh_unmap(struct device *dev,
 		      struct dh_edesc_s *edesc, struct pkc_request *req)
 {
 	struct dh_key_req_s *dh_req = &req->req_u.dh_req;
-	struct dh_key_desc_s *dh_desc =
-	    (struct dh_key_desc_s *)edesc->hw_desc;
-	dma_unmap_single(dev, dh_desc->q_dma,
+
+	dma_unmap_single(dev, edesc->q_dma,
 			 dh_req->q_len, DMA_TO_DEVICE);
-	dma_unmap_single(dev, dh_desc->w_dma,
+	dma_unmap_single(dev, edesc->w_dma,
 			 dh_req->pub_key_len, DMA_TO_DEVICE);
-	dma_unmap_single(dev, dh_desc->s_dma,
+	dma_unmap_single(dev, edesc->s_dma,
 			 dh_req->s_len, DMA_TO_DEVICE);
-	dma_unmap_single(dev, dh_desc->z_dma,
+	dma_unmap_single(dev, edesc->z_dma,
 			 dh_req->z_len, DMA_FROM_DEVICE);
 	if (edesc->req_type == ECDH_COMPUTE_KEY)
-		dma_unmap_single(dev, dh_desc->ab_dma,
+		dma_unmap_single(dev, edesc->ab_dma,
 				 dh_req->ab_len, DMA_TO_DEVICE);
 }
 
@@ -185,21 +176,20 @@ static void dsa_unmap(struct device *dev,
 	case ECDSA_SIGN:
 	{
 		struct dsa_sign_req_s *dsa_req = &req->req_u.dsa_sign;
-		struct dsa_sign_desc_s *dsa_desc =
-		    (struct dsa_sign_desc_s *)edesc->hw_desc;
-		dma_unmap_single(dev, dsa_desc->q_dma,
+
+		dma_unmap_single(dev, edesc->q_dma,
 				 dsa_req->q_len, DMA_TO_DEVICE);
-		dma_unmap_single(dev, dsa_desc->r_dma,
+		dma_unmap_single(dev, edesc->r_dma,
 				 dsa_req->r_len, DMA_TO_DEVICE);
-		dma_unmap_single(dev, dsa_desc->g_dma,
+		dma_unmap_single(dev, edesc->g_dma,
 				 dsa_req->g_len, DMA_TO_DEVICE);
-		dma_unmap_single(dev, dsa_desc->s_dma,
+		dma_unmap_single(dev, edesc->s_dma,
 				 dsa_req->priv_key_len, DMA_TO_DEVICE);
-		dma_unmap_single(dev, dsa_desc->f_dma,
+		dma_unmap_single(dev, edesc->f_dma,
 				 dsa_req->m_len, DMA_TO_DEVICE);
-		dma_unmap_single(dev, dsa_desc->c_dma,
+		dma_unmap_single(dev, edesc->c_dma,
 				 dsa_req->d_len, DMA_FROM_DEVICE);
-		dma_unmap_single(dev, dsa_desc->d_dma,
+		dma_unmap_single(dev, edesc->d_dma,
 				 dsa_req->d_len, DMA_FROM_DEVICE);
 		if (req->type == ECDSA_SIGN)
 			dma_unmap_single(dev, edesc->ab_dma,
@@ -210,30 +200,29 @@ static void dsa_unmap(struct device *dev,
 	case ECDSA_VERIFY:
 	{
 		struct dsa_verify_req_s *dsa_req = &req->req_u.dsa_verify;
-		struct dsa_verify_desc_s *dsa_desc =
-		    (struct dsa_verify_desc_s *)edesc->hw_desc;
-		dma_unmap_single(dev, dsa_desc->q_dma,
+
+		dma_unmap_single(dev, edesc->q_dma,
 				 dsa_req->q_len, DMA_TO_DEVICE);
-		dma_unmap_single(dev, dsa_desc->r_dma,
+		dma_unmap_single(dev, edesc->r_dma,
 				 dsa_req->r_len, DMA_TO_DEVICE);
-		dma_unmap_single(dev, dsa_desc->g_dma,
+		dma_unmap_single(dev, edesc->g_dma,
 				 dsa_req->g_len, DMA_TO_DEVICE);
-		dma_unmap_single(dev, dsa_desc->w_dma,
+		dma_unmap_single(dev, edesc->key_dma,
 				 dsa_req->pub_key_len, DMA_TO_DEVICE);
-		dma_unmap_single(dev, dsa_desc->f_dma,
+		dma_unmap_single(dev, edesc->f_dma,
 				 dsa_req->m_len, DMA_TO_DEVICE);
-		dma_unmap_single(dev, dsa_desc->c_dma,
+		dma_unmap_single(dev, edesc->c_dma,
 				 dsa_req->d_len, DMA_TO_DEVICE);
-		dma_unmap_single(dev, dsa_desc->d_dma,
+		dma_unmap_single(dev, edesc->d_dma,
 				 dsa_req->d_len, DMA_TO_DEVICE);
 		if (req->type == ECDSA_VERIFY) {
-			dma_unmap_single(dev, dsa_desc->tmp_dma,
+			dma_unmap_single(dev, edesc->tmp_dma,
 					 2*edesc->l_len, DMA_BIDIRECTIONAL);
 			dma_unmap_single(dev, edesc->ab_dma,
 					 dsa_req->ab_len, DMA_TO_DEVICE);
 		} else {
-			dma_unmap_single(dev, dsa_desc->tmp_dma,
-				 edesc->l_len, DMA_BIDIRECTIONAL);
+			dma_unmap_single(dev, edesc->tmp_dma,
+					 edesc->l_len, DMA_BIDIRECTIONAL);
 		}
 		kfree(edesc->tmp);
 	}
@@ -242,17 +231,16 @@ static void dsa_unmap(struct device *dev,
 	case ECC_KEYGEN:
 	{
 		struct keygen_req_s *key_req = &req->req_u.keygen;
-		struct dlc_keygen_desc_s *key_desc =
-		    (struct dlc_keygen_desc_s *)edesc->hw_desc;
-		dma_unmap_single(dev, key_desc->q_dma,
+
+		dma_unmap_single(dev, edesc->q_dma,
 				 key_req->q_len, DMA_TO_DEVICE);
-		dma_unmap_single(dev, key_desc->r_dma,
+		dma_unmap_single(dev, edesc->r_dma,
 				 key_req->r_len, DMA_TO_DEVICE);
-		dma_unmap_single(dev, key_desc->g_dma,
+		dma_unmap_single(dev, edesc->g_dma,
 				 key_req->g_len, DMA_TO_DEVICE);
-		dma_unmap_single(dev, key_desc->s_dma,
+		dma_unmap_single(dev, edesc->s_dma,
 				 key_req->priv_key_len, DMA_FROM_DEVICE);
-		dma_unmap_single(dev, key_desc->w_dma,
+		dma_unmap_single(dev, edesc->key_dma,
 				 key_req->pub_key_len, DMA_FROM_DEVICE);
 		if (req->type == ECC_KEYGEN)
 			dma_unmap_single(dev, edesc->ab_dma,
diff --git a/drivers/crypto/caam/desc_constr.h b/drivers/crypto/caam/desc_constr.h
index 560a197..5c18c38 100644
--- a/drivers/crypto/caam/desc_constr.h
+++ b/drivers/crypto/caam/desc_constr.h
@@ -77,6 +77,13 @@ static inline void init_job_desc(u32 *desc, u32 options)
 	init_desc(desc, CMD_DESC_HDR | options);
 }
 
+static inline void init_job_desc_pdb(u32 *desc, u32 options, size_t pdb_bytes)
+{
+	u32 pdb_len = (pdb_bytes + CAAM_CMD_SZ - 1) / CAAM_CMD_SZ;
+
+	init_job_desc(desc, (((pdb_len + 1) << HDR_START_IDX_SHIFT)) | options);
+}
+
 static inline void append_ptr(u32 *desc, dma_addr_t ptr)
 {
 	dma_addr_t *offset = (dma_addr_t *)desc_end(desc);
diff --git a/drivers/crypto/caam/pkc_desc.c b/drivers/crypto/caam/pkc_desc.c
index 2e81519..722bfe8 100644
--- a/drivers/crypto/caam/pkc_desc.c
+++ b/drivers/crypto/caam/pkc_desc.c
@@ -16,169 +16,140 @@
 /* Descriptor for RSA Public operation */
 void *caam_rsa_pub_desc(struct rsa_edesc *edesc)
 {
-	u32 start_idx, desc_size;
-	struct rsa_pub_desc_s *rsa_pub_desc =
-	    (struct rsa_pub_desc_s *)edesc->hw_desc;
 	struct rsa_pub_edesc_s *pub_edesc = &edesc->dma_u.rsa_pub_edesc;
+	u32 *desc = edesc->hw_desc;
 #ifdef CAAM_DEBUG
-	uint32_t i;
-	uint32_t *buf = (uint32_t *)rsa_pub_desc;
+	u32 i;
 #endif
 
-	desc_size = sizeof(struct rsa_pub_desc_s) / sizeof(u32);
-	start_idx = desc_size - 1;
-	start_idx &= HDR_START_IDX_MASK;
-	init_job_desc(edesc->hw_desc, (start_idx << HDR_START_IDX_SHIFT) |
-		      (start_idx & HDR_DESCLEN_MASK) | HDR_ONE);
-	rsa_pub_desc->n_dma = pub_edesc->n_dma;
-	rsa_pub_desc->e_dma = pub_edesc->e_dma;
-	rsa_pub_desc->f_dma = pub_edesc->f_dma;
-	rsa_pub_desc->g_dma = pub_edesc->g_dma;
-	rsa_pub_desc->sgf_flg = (pub_edesc->sg_flgs.e_len << RSA_PDB_E_SHIFT)
-	    | pub_edesc->sg_flgs.n_len;
-	rsa_pub_desc->msg_len = pub_edesc->f_len;
-	rsa_pub_desc->op = CMD_OPERATION | OP_TYPE_UNI_PROTOCOL |
-	    OP_PCLID_RSAENC_PUBKEY;
+	init_job_desc_pdb(desc, 0, sizeof(struct rsa_pub_desc_s) -
+			  2 * CAAM_CMD_SZ);
+	append_cmd(desc, (pub_edesc->sg_flgs.e_len << RSA_PDB_E_SHIFT) |
+		   pub_edesc->sg_flgs.n_len);
+	append_ptr(desc, pub_edesc->f_dma);
+	append_ptr(desc, pub_edesc->g_dma);
+	append_ptr(desc, pub_edesc->n_dma);
+	append_ptr(desc, pub_edesc->e_dma);
+	append_cmd(desc, pub_edesc->f_len);
+	append_operation(desc, OP_TYPE_UNI_PROTOCOL | OP_PCLID_RSAENC_PUBKEY);
+
 #ifdef CAAM_DEBUG
-	for (i = 0; i < desc_size; i++)
-		pr_debug("[%d] %x\n", i, buf[i]);
+	for (i = 0; i < desc_len(desc); i++)
+		pr_debug("[%d] %x\n", i, desc[i]);
 #endif
-	return rsa_pub_desc;
+	return desc;
 }
 
 /* Descriptor for RSA Private operation Form1 */
 void *caam_rsa_priv_f1_desc(struct rsa_edesc *edesc)
 {
-	u32 start_idx, desc_size;
-	struct rsa_priv_frm1_desc_s *rsa_priv_desc =
-	    (struct rsa_priv_frm1_desc_s *)edesc->hw_desc;
+	u32 *desc = edesc->hw_desc;
 	struct rsa_priv_frm1_edesc_s *priv_edesc =
-	    &edesc->dma_u.rsa_priv_f1_edesc;
+			&edesc->dma_u.rsa_priv_f1_edesc;
 
-	desc_size = sizeof(struct rsa_priv_frm1_desc_s) / sizeof(u32);
-	start_idx = desc_size - 1;
-	start_idx &= HDR_START_IDX_MASK;
-	init_job_desc(edesc->hw_desc, (start_idx << HDR_START_IDX_SHIFT) |
-		      (start_idx & HDR_DESCLEN_MASK) | HDR_ONE);
-	rsa_priv_desc->n_dma = priv_edesc->n_dma;
-	rsa_priv_desc->d_dma = priv_edesc->d_dma;
-	rsa_priv_desc->f_dma = priv_edesc->f_dma;
-	rsa_priv_desc->g_dma = priv_edesc->g_dma;
-	/* TBD. Support SG flags */
-	rsa_priv_desc->sgf_flg = (priv_edesc->sg_flgs.d_len << RSA_PDB_D_SHIFT)
-	    | priv_edesc->sg_flgs.n_len;
-	rsa_priv_desc->op = CMD_OPERATION | OP_TYPE_UNI_PROTOCOL |
-	    OP_PCLID_RSADEC_PRVKEY | RSA_PRIV_KEY_FRM_1;
-	return rsa_priv_desc;
+	init_job_desc_pdb(desc, 0, sizeof(struct rsa_priv_frm1_desc_s) -
+			  2 * CAAM_CMD_SZ);
+	append_cmd(desc, (priv_edesc->sg_flgs.d_len << RSA_PDB_D_SHIFT) |
+		   priv_edesc->sg_flgs.n_len);
+	append_ptr(desc, priv_edesc->g_dma);
+	append_ptr(desc, priv_edesc->f_dma);
+	append_ptr(desc, priv_edesc->n_dma);
+	append_ptr(desc, priv_edesc->d_dma);
+	append_operation(desc, OP_TYPE_UNI_PROTOCOL | OP_PCLID_RSADEC_PRVKEY |
+			 RSA_PRIV_KEY_FRM_1);
+	return desc;
 }
 
 /* Descriptor for RSA Private operation Form2 */
 void *caam_rsa_priv_f2_desc(struct rsa_edesc *edesc)
 {
-	u32 start_idx, desc_size;
-	struct rsa_priv_frm2_desc_s *rsa_priv_desc =
-	    (struct rsa_priv_frm2_desc_s *)edesc->hw_desc;
+	u32 *desc = edesc->hw_desc;
 	struct rsa_priv_frm2_edesc_s *priv_edesc =
-	    &edesc->dma_u.rsa_priv_f2_edesc;
+			&edesc->dma_u.rsa_priv_f2_edesc;
 
-	desc_size = sizeof(struct rsa_priv_frm2_desc_s) / sizeof(u32);
-	start_idx = desc_size - 1;
-	start_idx &= HDR_START_IDX_MASK;
-	init_job_desc(edesc->hw_desc, (start_idx << HDR_START_IDX_SHIFT) |
-		      (start_idx & HDR_DESCLEN_MASK) | HDR_ONE);
-	rsa_priv_desc->p_dma = priv_edesc->p_dma;
-	rsa_priv_desc->q_dma = priv_edesc->q_dma;
-	rsa_priv_desc->d_dma = priv_edesc->d_dma;
-	rsa_priv_desc->f_dma = priv_edesc->f_dma;
-	rsa_priv_desc->g_dma = priv_edesc->g_dma;
-	rsa_priv_desc->tmp1_dma = priv_edesc->tmp1_dma;
-	rsa_priv_desc->tmp2_dma = priv_edesc->tmp2_dma;
-	rsa_priv_desc->sgf_flg = (priv_edesc->sg_flgs.d_len << RSA_PDB_D_SHIFT)
-	    | priv_edesc->sg_flgs.n_len;
-	rsa_priv_desc->p_q_len = (priv_edesc->q_len << RSA_PDB_Q_SHIFT)
-	    | priv_edesc->p_len;
-	rsa_priv_desc->op = CMD_OPERATION | OP_TYPE_UNI_PROTOCOL |
-	    OP_PCLID_RSADEC_PRVKEY | RSA_PRIV_KEY_FRM_2;
-	return rsa_priv_desc;
+	init_job_desc_pdb(desc, 0, sizeof(struct rsa_priv_frm2_desc_s) -
+			  2 * CAAM_CMD_SZ);
+	append_cmd(desc, (priv_edesc->sg_flgs.d_len << RSA_PDB_D_SHIFT) |
+			 priv_edesc->sg_flgs.n_len);
+	append_ptr(desc, priv_edesc->g_dma);
+	append_ptr(desc, priv_edesc->f_dma);
+	append_ptr(desc, priv_edesc->d_dma);
+	append_ptr(desc, priv_edesc->p_dma);
+	append_ptr(desc, priv_edesc->q_dma);
+	append_ptr(desc, priv_edesc->tmp1_dma);
+	append_ptr(desc, priv_edesc->tmp2_dma);
+	append_cmd(desc, (priv_edesc->q_len << RSA_PDB_Q_SHIFT) |
+			 priv_edesc->p_len);
+	append_operation(desc, OP_TYPE_UNI_PROTOCOL | OP_PCLID_RSADEC_PRVKEY |
+			 RSA_PRIV_KEY_FRM_2);
+	return desc;
 }
 
 /* Descriptor for RSA Private operation Form3 */
 void *caam_rsa_priv_f3_desc(struct rsa_edesc *edesc)
 {
-	u32 start_idx, desc_size;
-	struct rsa_priv_frm3_desc_s *rsa_priv_desc =
-	    (struct rsa_priv_frm3_desc_s *)edesc->hw_desc;
+	u32 *desc = edesc->hw_desc;
 	struct rsa_priv_frm3_edesc_s *priv_edesc =
-	    &edesc->dma_u.rsa_priv_f3_edesc;
+			&edesc->dma_u.rsa_priv_f3_edesc;
 #ifdef CAAM_DEBUG
-	uint32_t *buf = (uint32_t *)rsa_priv_desc;
-	uint32_t i;
+	u32 i;
 #endif
 
-	desc_size = sizeof(struct rsa_priv_frm3_desc_s) / sizeof(u32);
-	start_idx = desc_size - 1;
-	start_idx &= HDR_START_IDX_MASK;
-	init_job_desc(edesc->hw_desc, (start_idx << HDR_START_IDX_SHIFT) |
-		      (start_idx & HDR_DESCLEN_MASK) | HDR_ONE);
-	rsa_priv_desc->p_dma = priv_edesc->p_dma;
-	rsa_priv_desc->q_dma = priv_edesc->q_dma;
-	rsa_priv_desc->dp_dma = priv_edesc->dp_dma;
-	rsa_priv_desc->dq_dma = priv_edesc->dq_dma;
-	rsa_priv_desc->c_dma = priv_edesc->c_dma;
-	rsa_priv_desc->f_dma = priv_edesc->f_dma;
-	rsa_priv_desc->g_dma = priv_edesc->g_dma;
-	rsa_priv_desc->tmp1_dma = priv_edesc->tmp1_dma;
-	rsa_priv_desc->tmp2_dma = priv_edesc->tmp2_dma;
-	rsa_priv_desc->p_q_len = (priv_edesc->q_len << RSA_PDB_Q_SHIFT)
-	    | priv_edesc->p_len;
-	/* TBD: SG Flags to be filled */
-	rsa_priv_desc->sgf_flg = priv_edesc->sg_flgs.n_len;
-	rsa_priv_desc->op = CMD_OPERATION | OP_TYPE_UNI_PROTOCOL |
-	    OP_PCLID_RSADEC_PRVKEY | RSA_PRIV_KEY_FRM_3;
+	init_job_desc_pdb(desc, 0, sizeof(struct rsa_priv_frm3_desc_s) -
+			  2 * CAAM_CMD_SZ);
+	append_cmd(desc, priv_edesc->sg_flgs.n_len);
+	append_ptr(desc, priv_edesc->g_dma);
+	append_ptr(desc, priv_edesc->f_dma);
+	append_ptr(desc, priv_edesc->c_dma);
+	append_ptr(desc, priv_edesc->p_dma);
+	append_ptr(desc, priv_edesc->q_dma);
+	append_ptr(desc, priv_edesc->dp_dma);
+	append_ptr(desc, priv_edesc->dq_dma);
+	append_ptr(desc, priv_edesc->tmp1_dma);
+	append_ptr(desc, priv_edesc->tmp2_dma);
+	append_cmd(desc, (priv_edesc->q_len << RSA_PDB_Q_SHIFT) |
+		   priv_edesc->p_len);
+	append_operation(desc, OP_TYPE_UNI_PROTOCOL | OP_PCLID_RSADEC_PRVKEY |
+			 RSA_PRIV_KEY_FRM_3);
+
 #ifdef CAAM_DEBUG
-	for (i = 0; i < desc_size; i++)
-		pr_debug("[%d] %x\n", i, buf[i]);
+	for (i = 0; i < desc_len(desc); i++)
+		pr_debug("[%d] %x\n", i, desc[i]);
 #endif
-	return rsa_priv_desc;
+	return desc;
 }
 
 /* DH sign CAAM descriptor */
 void *caam_dh_key_desc(struct dh_edesc_s *edesc)
 {
-	u32 start_idx, desc_size;
-	void *desc;
+	u32 *desc = edesc->hw_desc;
+	u32 op = OP_TYPE_UNI_PROTOCOL | OP_PCLID_DH;
 #ifdef CAAM_DEBUG
-	uint32_t i;
-	uint32_t *buf;
+	u32 i;
 #endif
-	struct dh_key_desc_s *dh_desc =
-	    (struct dh_key_desc_s *)edesc->hw_desc;
-	desc_size = sizeof(struct dh_key_desc_s) / sizeof(u32);
-	start_idx = desc_size - 1;
-	start_idx &= HDR_START_IDX_MASK;
-	init_job_desc(edesc->hw_desc, (start_idx << HDR_START_IDX_SHIFT) |
-		      (start_idx & HDR_DESCLEN_MASK) | HDR_ONE);
-	dh_desc->sgf_ln = (edesc->l_len << DH_PDB_L_SHIFT) |
-		((edesc->n_len & DH_PDB_N_MASK));
-	dh_desc->q_dma = edesc->q_dma;
-	dh_desc->w_dma = edesc->w_dma;
-	dh_desc->s_dma = edesc->s_dma;
-	dh_desc->z_dma = edesc->z_dma;
-	dh_desc->op = CMD_OPERATION | OP_TYPE_UNI_PROTOCOL |
-	    OP_PCLID_DH;
+
+	init_job_desc_pdb(desc, 0, sizeof(struct dh_key_desc_s) -
+			  2 * CAAM_CMD_SZ);
+	append_cmd(desc, (edesc->l_len << DH_PDB_L_SHIFT) |
+			 (edesc->n_len & DH_PDB_N_MASK));
+	append_ptr(desc, edesc->q_dma);
+	/* pointer to r (unused) */
+	append_ptr(desc, 0);
+	append_ptr(desc, edesc->w_dma);
+	append_ptr(desc, edesc->s_dma);
+	append_ptr(desc, edesc->z_dma);
 	if (edesc->req_type == ECDH_COMPUTE_KEY) {
-		dh_desc->ab_dma = edesc->ab_dma;
-		dh_desc->op |= OP_PCL_PKPROT_ECC;
+		append_ptr(desc, edesc->ab_dma);
+		op |= OP_PCL_PKPROT_ECC;
 		if (edesc->curve_type == ECC_BINARY)
-			dh_desc->op |= OP_PCL_PKPROT_F2M;
+			op |= OP_PCL_PKPROT_F2M;
 	}
+	append_operation(desc, op);
 
-	desc = dh_desc;
 #ifdef CAAM_DEBUG
-	buf = desc;
-	pr_debug("%d DH Descriptor is:\n", desc_size);
-	for (i = 0; i < desc_size; i++)
-		pr_debug("[%d] %x\n", i, buf[i]);
+	pr_debug("DH Descriptor:\n");
+	for (i = 0; i < desc_len(desc); i++)
+		pr_debug("[%d] %x\n", i, desc[i]);
 #endif
 	return desc;
 }
@@ -186,203 +157,153 @@ void *caam_dh_key_desc(struct dh_edesc_s *edesc)
 /* DSA sign CAAM descriptor */
 void *caam_dsa_sign_desc(struct dsa_edesc_s *edesc)
 {
-	u32 start_idx, desc_size;
-	void *desc;
+	u32 *desc = edesc->hw_desc;
+	u32 op = OP_TYPE_UNI_PROTOCOL | OP_PCLID_DSASIGN;
 #ifdef CAAM_DEBUG
 	uint32_t i;
-	uint32_t *buf;
 #endif
 
 	if (edesc->req_type == ECDSA_SIGN) {
-		struct ecdsa_sign_desc_s *ecdsa_desc =
-		    (struct ecdsa_sign_desc_s *)edesc->hw_desc;
-		desc_size = sizeof(struct ecdsa_sign_desc_s) / sizeof(u32);
-		start_idx = desc_size - 1;
-		start_idx &= HDR_START_IDX_MASK;
-		init_job_desc(edesc->hw_desc,
-			      (start_idx << HDR_START_IDX_SHIFT) |
-			      (start_idx & HDR_DESCLEN_MASK) | HDR_ONE);
-		ecdsa_desc->sgf_ln = (edesc->l_len << DSA_PDB_L_SHIFT) |
-			((edesc->n_len & DSA_PDB_N_MASK));
-		ecdsa_desc->q_dma = edesc->q_dma;
-		ecdsa_desc->r_dma = edesc->r_dma;
-		ecdsa_desc->g_dma = edesc->g_dma;
-		ecdsa_desc->s_dma = edesc->key_dma;
-		ecdsa_desc->f_dma = edesc->f_dma;
-		ecdsa_desc->c_dma = edesc->c_dma;
-		ecdsa_desc->d_dma = edesc->d_dma;
-		ecdsa_desc->ab_dma = edesc->ab_dma;
-		ecdsa_desc->op = CMD_OPERATION | OP_TYPE_UNI_PROTOCOL |
-		    OP_PCLID_DSASIGN | OP_PCL_PKPROT_ECC;
+		op |= OP_PCL_PKPROT_ECC;
 		if (edesc->curve_type == ECC_BINARY)
-			ecdsa_desc->op |= OP_PCL_PKPROT_F2M;
+			op |= OP_PCL_PKPROT_F2M;
 
-		desc = ecdsa_desc;
+		init_job_desc_pdb(desc, 0, sizeof(struct ecdsa_sign_desc_s) -
+				  2 * CAAM_CMD_SZ);
+		append_cmd(desc, (edesc->l_len << DSA_PDB_L_SHIFT) |
+				 (edesc->n_len & DSA_PDB_N_MASK));
+		append_ptr(desc, edesc->q_dma);
+		append_ptr(desc, edesc->r_dma);
+		append_ptr(desc, edesc->g_dma);
+		append_ptr(desc, edesc->key_dma);
+		append_ptr(desc, edesc->f_dma);
+		append_ptr(desc, edesc->c_dma);
+		append_ptr(desc, edesc->d_dma);
+		append_ptr(desc, edesc->ab_dma);
+		append_operation(desc, op);
 	} else {
-		struct dsa_sign_desc_s *dsa_desc =
-		    (struct dsa_sign_desc_s *)edesc->hw_desc;
-		desc_size = sizeof(struct dsa_sign_desc_s) / sizeof(u32);
-		start_idx = desc_size - 1;
-		start_idx &= HDR_START_IDX_MASK;
-		init_job_desc(edesc->hw_desc,
-			      (start_idx << HDR_START_IDX_SHIFT) |
-			      (start_idx & HDR_DESCLEN_MASK) | HDR_ONE);
-		dsa_desc->sgf_ln = (edesc->l_len << DSA_PDB_L_SHIFT) |
-			((edesc->n_len & DSA_PDB_N_MASK));
-		dsa_desc->q_dma = edesc->q_dma;
-		dsa_desc->r_dma = edesc->r_dma;
-		dsa_desc->g_dma = edesc->g_dma;
-		dsa_desc->s_dma = edesc->key_dma;
-		dsa_desc->f_dma = edesc->f_dma;
-		dsa_desc->c_dma = edesc->c_dma;
-		dsa_desc->d_dma = edesc->d_dma;
-		dsa_desc->op = CMD_OPERATION | OP_TYPE_UNI_PROTOCOL |
-		    OP_PCLID_DSASIGN;
-		desc = dsa_desc;
+		init_job_desc_pdb(desc, 0, sizeof(struct dsa_sign_desc_s) -
+				  2 * CAAM_CMD_SZ);
+		append_cmd(desc, (edesc->l_len << DSA_PDB_L_SHIFT) |
+				 (edesc->n_len & DSA_PDB_N_MASK));
+		append_ptr(desc, edesc->q_dma);
+		append_ptr(desc, edesc->r_dma);
+		append_ptr(desc, edesc->g_dma);
+		append_ptr(desc, edesc->key_dma);
+		append_ptr(desc, edesc->f_dma);
+		append_ptr(desc, edesc->c_dma);
+		append_ptr(desc, edesc->d_dma);
+		append_operation(desc, op);
 	}
+
 #ifdef CAAM_DEBUG
-	buf = desc;
-	pr_debug("DSA Descriptor is:");
-	for (i = 0; i < desc_size; i++)
-		pr_debug("[%d] %x ", i, buf[i]);
-	pr_debug("\n");
+	pr_debug("DSA Descriptor:\n");
+	for (i = 0; i < desc_len(desc); i++)
+		pr_debug("[%d] %x\n", i, desc[i]);
 #endif
-
 	return desc;
 }
 
 /* DSA/ECDSA/DH/ECDH keygen CAAM descriptor */
 void *caam_keygen_desc(struct dsa_edesc_s *edesc)
 {
-	u32 start_idx, desc_size;
-	void *desc;
+	u32 *desc = edesc->hw_desc;
+	u32 sgf_len = (edesc->l_len << DSA_PDB_L_SHIFT) |
+		      (edesc->n_len & DSA_PDB_N_MASK);
+	u32 op = OP_TYPE_UNI_PROTOCOL | OP_PCLID_PUBLICKEYPAIR;
+	dma_addr_t g_dma = edesc->g_dma;
 #ifdef CAAM_DEBUG
-	uint32_t i;
-	uint32_t *buf;
+	u32 i;
 #endif
 
 	if (edesc->req_type == ECC_KEYGEN) {
-		struct ecc_keygen_desc_s *ecc_desc =
-		    (struct ecc_keygen_desc_s *)edesc->hw_desc;
-		desc_size = sizeof(struct ecc_keygen_desc_s) / sizeof(u32);
-		start_idx = desc_size - 1;
-		start_idx &= HDR_START_IDX_MASK;
-		init_job_desc(edesc->hw_desc,
-			      (start_idx << HDR_START_IDX_SHIFT) |
-			      (start_idx & HDR_DESCLEN_MASK) | HDR_ONE);
-		ecc_desc->sgf_ln = (edesc->l_len << DSA_PDB_L_SHIFT) |
-				   (edesc->n_len & DSA_PDB_N_MASK);
 		if (edesc->erratum_A_006899) {
-			ecc_desc->sgf_ln |= DSA_PDB_SGF_G;
-			ecc_desc->g_dma = edesc->g_sg_dma;
-		} else {
-			ecc_desc->g_dma = edesc->g_dma;
+			sgf_len |= DSA_PDB_SGF_G;
+			g_dma = edesc->g_sg_dma;
 		}
-		ecc_desc->q_dma = edesc->q_dma;
-		ecc_desc->r_dma = edesc->r_dma;
-		ecc_desc->s_dma = edesc->s_dma;
-		ecc_desc->w_dma = edesc->key_dma;
-		ecc_desc->ab_dma = edesc->ab_dma;
-		ecc_desc->op = CMD_OPERATION | OP_TYPE_UNI_PROTOCOL |
-		    OP_PCLID_PUBLICKEYPAIR | OP_PCL_PKPROT_ECC;
+
+		op |= OP_PCL_PKPROT_ECC;
 		if (edesc->curve_type == ECC_BINARY)
-			ecc_desc->op |= OP_PCL_PKPROT_F2M;
+			op |= OP_PCL_PKPROT_F2M;
 
-		desc = ecc_desc;
+		init_job_desc_pdb(desc, 0, sizeof(struct ecc_keygen_desc_s) -
+				  2 * CAAM_CMD_SZ);
+		append_cmd(desc, sgf_len);
+		append_ptr(desc, edesc->q_dma);
+		append_ptr(desc, edesc->r_dma);
+		append_ptr(desc, g_dma);
+		append_ptr(desc, edesc->s_dma);
+		append_ptr(desc, edesc->key_dma);
+		append_ptr(desc, edesc->ab_dma);
+		append_operation(desc, op);
 	} else {
-		struct dlc_keygen_desc_s *key_desc =
-		    (struct dlc_keygen_desc_s *)edesc->hw_desc;
-		desc_size = sizeof(struct dlc_keygen_desc_s) / sizeof(u32);
-		start_idx = desc_size - 1;
-		start_idx &= HDR_START_IDX_MASK;
-		init_job_desc(edesc->hw_desc,
-			      (start_idx << HDR_START_IDX_SHIFT) |
-			      (start_idx & HDR_DESCLEN_MASK) | HDR_ONE);
-		key_desc->sgf_ln = (edesc->l_len << DSA_PDB_L_SHIFT) |
-			((edesc->n_len & DSA_PDB_N_MASK));
-		key_desc->q_dma = edesc->q_dma;
-		key_desc->r_dma = edesc->r_dma;
-		key_desc->g_dma = edesc->g_dma;
-		key_desc->s_dma = edesc->s_dma;
-		key_desc->w_dma = edesc->key_dma;
-		key_desc->op = CMD_OPERATION | OP_TYPE_UNI_PROTOCOL |
-		    OP_PCLID_PUBLICKEYPAIR;
-		desc = key_desc;
+		init_job_desc_pdb(desc, 0, sizeof(struct dlc_keygen_desc_s) -
+				  2 * CAAM_CMD_SZ);
+		append_cmd(desc, sgf_len);
+		append_ptr(desc, edesc->q_dma);
+		append_ptr(desc, edesc->r_dma);
+		append_ptr(desc, g_dma);
+		append_ptr(desc, edesc->s_dma);
+		append_ptr(desc, edesc->key_dma);
+		append_operation(desc, op);
 	}
+
 #ifdef CAAM_DEBUG
-	buf = desc;
-	pr_debug("DSA Keygen Descriptor is:");
-	for (i = 0; i < desc_size; i++)
-		pr_debug("[%d] %x ", i, buf[i]);
-	pr_debug("\n");
+	pr_debug("DSA Keygen Descriptor:\n");
+	for (i = 0; i < desc_len(desc); i++)
+		pr_debug("[%d] %x ", i, desc[i]);
 #endif
-
 	return desc;
 }
 
 /* DSA verify CAAM descriptor */
 void *caam_dsa_verify_desc(struct dsa_edesc_s *edesc)
 {
-	u32 start_idx, desc_size;
-	void *desc;
+	u32 *desc = edesc->hw_desc;
+	u32 op = OP_TYPE_UNI_PROTOCOL | OP_PCLID_DSAVERIFY;
 #ifdef CAAM_DEBUG
-	uint32_t i;
-	uint32_t *buf;
+	u32 i;
 #endif
 
 	if (edesc->req_type == ECDSA_VERIFY) {
-		struct ecdsa_verify_desc_s *ecdsa_desc =
-		    (struct ecdsa_verify_desc_s *)edesc->hw_desc;
-		desc_size = sizeof(struct ecdsa_verify_desc_s) / sizeof(u32);
-		start_idx = desc_size - 1;
-		start_idx &= HDR_START_IDX_MASK;
-		init_job_desc(edesc->hw_desc,
-			      (start_idx << HDR_START_IDX_SHIFT) |
-			      (start_idx & HDR_DESCLEN_MASK) | HDR_ONE);
-		ecdsa_desc->sgf_ln = (edesc->l_len << DSA_PDB_L_SHIFT) |
-			((edesc->n_len & DSA_PDB_N_MASK));
-		ecdsa_desc->q_dma = edesc->q_dma;
-		ecdsa_desc->r_dma = edesc->r_dma;
-		ecdsa_desc->g_dma = edesc->g_dma;
-		ecdsa_desc->w_dma = edesc->key_dma;
-		ecdsa_desc->f_dma = edesc->f_dma;
-		ecdsa_desc->c_dma = edesc->c_dma;
-		ecdsa_desc->d_dma = edesc->d_dma;
-		ecdsa_desc->tmp_dma = edesc->tmp_dma;
-		ecdsa_desc->ab_dma = edesc->ab_dma;
-		ecdsa_desc->op = CMD_OPERATION | OP_TYPE_UNI_PROTOCOL |
-		    OP_PCLID_DSAVERIFY | OP_PCL_PKPROT_ECC;
+		op |= OP_PCL_PKPROT_ECC;
 		if (edesc->curve_type == ECC_BINARY)
-			ecdsa_desc->op |= OP_PCL_PKPROT_F2M;
-		desc = ecdsa_desc;
+			op |= OP_PCL_PKPROT_F2M;
+
+		init_job_desc_pdb(desc, 0, sizeof(struct ecdsa_verify_desc_s) -
+				  2 * CAAM_CMD_SZ);
+		append_cmd(desc, (edesc->l_len << DSA_PDB_L_SHIFT) |
+				 (edesc->n_len & DSA_PDB_N_MASK));
+		append_ptr(desc, edesc->q_dma);
+		append_ptr(desc, edesc->r_dma);
+		append_ptr(desc, edesc->g_dma);
+		append_ptr(desc, edesc->key_dma);
+		append_ptr(desc, edesc->f_dma);
+		append_ptr(desc, edesc->c_dma);
+		append_ptr(desc, edesc->d_dma);
+		append_ptr(desc, edesc->tmp_dma);
+		append_ptr(desc, edesc->ab_dma);
+		append_operation(desc, op);
 	} else {
-		struct dsa_verify_desc_s *dsa_desc =
-		    (struct dsa_verify_desc_s *)edesc->hw_desc;
-		desc_size = sizeof(struct dsa_verify_desc_s) / sizeof(u32);
-		start_idx = desc_size - 1;
-		start_idx &= HDR_START_IDX_MASK;
-		init_job_desc(edesc->hw_desc,
-			      (start_idx << HDR_START_IDX_SHIFT) |
-			      (start_idx & HDR_DESCLEN_MASK) | HDR_ONE);
-		dsa_desc->sgf_ln = (edesc->l_len << DSA_PDB_L_SHIFT) |
-			((edesc->n_len & DSA_PDB_N_MASK));
-		dsa_desc->q_dma = edesc->q_dma;
-		dsa_desc->r_dma = edesc->r_dma;
-		dsa_desc->g_dma = edesc->g_dma;
-		dsa_desc->w_dma = edesc->key_dma;
-		dsa_desc->f_dma = edesc->f_dma;
-		dsa_desc->c_dma = edesc->c_dma;
-		dsa_desc->d_dma = edesc->d_dma;
-		dsa_desc->tmp_dma = edesc->tmp_dma;
-		dsa_desc->op = CMD_OPERATION | OP_TYPE_UNI_PROTOCOL |
-		    OP_PCLID_DSAVERIFY;
-		desc = dsa_desc;
+		init_job_desc_pdb(desc, 0, sizeof(struct dsa_verify_desc_s) -
+				  2 * CAAM_CMD_SZ);
+		append_cmd(desc, (edesc->l_len << DSA_PDB_L_SHIFT) |
+				 (edesc->n_len & DSA_PDB_N_MASK));
+		append_ptr(desc, edesc->q_dma);
+		append_ptr(desc, edesc->r_dma);
+		append_ptr(desc, edesc->g_dma);
+		append_ptr(desc, edesc->key_dma);
+		append_ptr(desc, edesc->f_dma);
+		append_ptr(desc, edesc->c_dma);
+		append_ptr(desc, edesc->d_dma);
+		append_ptr(desc, edesc->tmp_dma);
+		append_operation(desc, op);
 	}
+
 #ifdef CAAM_DEBUG
-	buf = desc;
-	pr_debug("DSA Descriptor is:\n");
-	for (i = 0; i < desc_size; i++)
-		pr_debug("[%d] %x\n", i, buf[i]);
+	pr_debug("DSA Descriptor:\n");
+	for (i = 0; i < desc_len(desc); i++)
+		pr_debug("[%d] %x\n", i, desc[i]);
 #endif
 	return desc;
 }
-- 
2.9.3

