From ff9d4afbd949bf047655177582a8d4d4d3d94618 Mon Sep 17 00:00:00 2001
From: Ahmed Mansour <Ahmed.Mansour@freescale.com>
Date: Fri, 13 Nov 2015 14:33:13 -0500
Subject: [PATCH 0927/1383] dpaa2-dce: Add DCE Driver

Add the DCE driver API and update the Kconfigs accordingly. Added
initial documentation in README. The driver uses the dce flib to
implement an easy to use interface to DCE

ENGR00369242

Signed-off-by: Ahmed Mansour <Ahmed.Mansour@freescale.com>
[Original patch taken from QorIQ-SDK-V2.0-20160527-yocto]
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 drivers/staging/fsl-dpaa2/Kconfig            |   1 +
 drivers/staging/fsl-dpaa2/Makefile           |   1 +
 drivers/staging/fsl-dpaa2/dce/Kconfig        |  24 +
 drivers/staging/fsl-dpaa2/dce/Makefile       |  10 +
 drivers/staging/fsl-dpaa2/dce/README         |   8 +
 drivers/staging/fsl-dpaa2/dce/dce-sys-decl.h |  53 ++
 drivers/staging/fsl-dpaa2/dce/dce.c          | 474 ++++++++++++++++++
 drivers/staging/fsl-dpaa2/dce/dce.h          | 369 ++++++++++++++
 drivers/staging/fsl-dpaa2/dce/dpdcei-cmd.h   |   2 +-
 drivers/staging/fsl-dpaa2/dce/dpdcei-drv.c   | 724 +++++++++++++++++++++++++++
 drivers/staging/fsl-dpaa2/dce/dpdcei-drv.h   | 101 ++++
 11 files changed, 1766 insertions(+), 1 deletion(-)
 create mode 100644 drivers/staging/fsl-dpaa2/dce/Kconfig
 create mode 100644 drivers/staging/fsl-dpaa2/dce/Makefile
 create mode 100644 drivers/staging/fsl-dpaa2/dce/README
 create mode 100644 drivers/staging/fsl-dpaa2/dce/dce-sys-decl.h
 create mode 100644 drivers/staging/fsl-dpaa2/dce/dce.c
 create mode 100644 drivers/staging/fsl-dpaa2/dce/dce.h
 create mode 100644 drivers/staging/fsl-dpaa2/dce/dpdcei-drv.c
 create mode 100644 drivers/staging/fsl-dpaa2/dce/dpdcei-drv.h

diff --git a/drivers/staging/fsl-dpaa2/Kconfig b/drivers/staging/fsl-dpaa2/Kconfig
index 79295233..a5dfdab 100644
--- a/drivers/staging/fsl-dpaa2/Kconfig
+++ b/drivers/staging/fsl-dpaa2/Kconfig
@@ -11,3 +11,4 @@ config FSL_DPAA2
 source "drivers/staging/fsl-dpaa2/ethernet/Kconfig"
 source "drivers/staging/fsl-dpaa2/mac/Kconfig"
 source "drivers/staging/fsl-dpaa2/evb/Kconfig"
+source "drivers/staging/fsl-dpaa2/dce/Kconfig"
diff --git a/drivers/staging/fsl-dpaa2/Makefile b/drivers/staging/fsl-dpaa2/Makefile
index 67d9d0b..ce01f1a 100644
--- a/drivers/staging/fsl-dpaa2/Makefile
+++ b/drivers/staging/fsl-dpaa2/Makefile
@@ -4,4 +4,5 @@
 
 obj-$(CONFIG_FSL_DPAA2_ETH)	+= ethernet/
 obj-$(CONFIG_FSL_DPAA2_MAC)	+= mac/
+obj-$(CONFIG_FSL_DPAA2_DCE)	+= dce/
 obj-$(CONFIG_FSL_DPAA2_EVB)	+= evb/
diff --git a/drivers/staging/fsl-dpaa2/dce/Kconfig b/drivers/staging/fsl-dpaa2/dce/Kconfig
new file mode 100644
index 0000000..793f11f
--- /dev/null
+++ b/drivers/staging/fsl-dpaa2/dce/Kconfig
@@ -0,0 +1,24 @@
+#
+# Freescale DPAA2 Decompression Compression (DCE) driver
+#
+# Copyright (C) 2015 Freescale Semiconductor, Inc.
+#
+# This file is released under the GPLv2
+#
+
+config FSL_DPAA2_DCE
+	tristate "DPAA2 Decompression Compression Engine"
+	depends on FSL_DPAA2 && FSL_MC_BUS && FSL_MC_DPIO
+	---help---
+	  A simplified interface to DCE that allows for asynchronous
+	  use of DCE without requiring complex setup
+
+config FSL_DCE_FLOW_LIMIT
+        int "Number of flows supported per compression or decompression device"
+        depends on FSL_DPAA2_DCE
+        default 65536
+        ---help---
+          This is the number of flows per compression or decompression device.
+	  A hashtable is allocated to locate corresponding flow objects
+	  based on a hash value.
+	  This is used for both stateful and stateless flows.
diff --git a/drivers/staging/fsl-dpaa2/dce/Makefile b/drivers/staging/fsl-dpaa2/dce/Makefile
new file mode 100644
index 0000000..339dedd
--- /dev/null
+++ b/drivers/staging/fsl-dpaa2/dce/Makefile
@@ -0,0 +1,10 @@
+#
+# Makefile for the Freescale DPAA2 DCE driver
+#
+# Copyright (C) 2015 Freescale Semiconductor, Inc.
+#
+# This file is released under the GPLv2
+#
+
+obj-$(CONFIG_FSL_DPAA2_DCE) += fsl-dce-api.o
+fsl-dce-api-objs	:= dpdcei-drv.o dpdcei.o dce-fd.o dce-fcr.o dce-scf-compression.o dce-scf-decompression.o dce-fd-frc.o dce.o
diff --git a/drivers/staging/fsl-dpaa2/dce/README b/drivers/staging/fsl-dpaa2/dce/README
new file mode 100644
index 0000000..6760479
--- /dev/null
+++ b/drivers/staging/fsl-dpaa2/dce/README
@@ -0,0 +1,8 @@
+Decompression Compression Engine (DCE)
+	The DCE is a freescale hardware acceelrator that can be used to offload
+	compression and decompression operations that use the DEFLATE algorithm
+
+	The DCE is represented by objects called DPDCEI. All modules that do
+	compression or decompression must get a handle to a dpdcei. All work
+	necessary to get access and share DPDCEI objects is simplified in the
+	DCE API docuemented in dce.h
diff --git a/drivers/staging/fsl-dpaa2/dce/dce-sys-decl.h b/drivers/staging/fsl-dpaa2/dce/dce-sys-decl.h
new file mode 100644
index 0000000..cde2235
--- /dev/null
+++ b/drivers/staging/fsl-dpaa2/dce/dce-sys-decl.h
@@ -0,0 +1,53 @@
+/* Copyright (C) 2014 Freescale Semiconductor, Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/io.h>
+#include <linux/dma-mapping.h>
+#include <linux/bootmem.h>
+#include <linux/slab.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/delay.h>
+#include <linux/memblock.h>
+#include <linux/completion.h>
+#include <linux/log2.h>
+#include <linux/types.h>
+#include <linux/ioctl.h>
+#include <linux/device.h>
+#include <linux/smp.h>
+#include <linux/vmalloc.h>
+
+/* Similarly-named functions */
+#define upper32(a) upper_32_bits(a)
+#define lower32(a) lower_32_bits(a)
+
diff --git a/drivers/staging/fsl-dpaa2/dce/dce.c b/drivers/staging/fsl-dpaa2/dce/dce.c
new file mode 100644
index 0000000..52efa1a
--- /dev/null
+++ b/drivers/staging/fsl-dpaa2/dce/dce.c
@@ -0,0 +1,474 @@
+/* Copyright (C) 2015 Freescale Semiconductor, Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "dce-scf-compression.h"
+#include "dce.h"
+/* #define debug */
+
+MODULE_AUTHOR("Freescale Semicondictor, Inc");
+MODULE_DESCRIPTION("DCE API");
+MODULE_LICENSE("Dual BSD/GPL");
+
+/* dma memories that need to be allocated
+ *	memory		size			alignment_req
+ *
+ *	pending_out_ptr	comp: 8202B		none (64B optimal)
+ *	pending_out_ptr	decomp: 28k (1024 * 28)	none (64B optimal)
+ *	history_ptr	comp: 4096		64B
+ *	history_ptr	decomp: 32768		64B
+ *	decomp_ctx_ptr	decomp only 256B	none
+ *	extra_ptr	extra_limit defines the length for decompression.
+ *			no alignment requirements.
+ */
+
+
+/* an internal structure that contains information per DCE interaction, this
+ * structure is necessary because if the API is used asynchronously the response
+ * comes back on the same frame that was sent. If the same frame struct is used
+ * for different transactions with DCE then there is a chance that the second
+ * response will overwrite the information written by the first */
+struct work_unit {
+	union store {
+		/* faster if aligned */
+		struct dpaa2_fd fd_list_store[3] __aligned(64);
+		struct {
+			struct dpaa2_fd output_fd;
+			struct dpaa2_fd input_fd;
+			struct dpaa2_fd scf_fd;
+			void *context;
+		};
+	} store;
+	struct scf_c_cfg scf_result __aligned(64); /* must 64 byte align */
+	struct dpaa2_fd fd_list;
+};
+
+/* trigger_user_callback - takes the information from the callbacks and
+ * `massages' the data into user friendly format */
+/* TODO: make sure to mention that fd must be completely clean*/
+static void trigger_user_callback(struct dce_session *session,
+				  struct dpaa2_fd const *fd)
+{
+	dma_addr_t store_phys_addr = dpaa2_fd_get_addr(fd);
+	union store *store = phys_to_virt(store_phys_addr);
+	struct work_unit *work_unit = container_of(store, struct work_unit,
+						   store);
+	uint8_t status = fd_frc_get_status((struct fd_attr *)fd);
+	size_t input_consumed;
+
+	dma_unmap_single(&session->device->dev, store_phys_addr,
+			 sizeof(store->fd_list_store), DMA_BIDIRECTIONAL);
+#ifdef debug
+	pr_info("dce: callback phys_to_virt work_unit %p", work_unit);
+	pretty_print_fd((struct fd_attr *)fd);
+	pretty_print_fle_n((struct fle_attr *)store->fd_list_store, 3);
+#endif
+	switch (status) {
+	case OUTPUT_BLOCKED_SUSPEND:
+	case ACQUIRE_DATA_BUFFER_DENIED_SUSPEND:
+	case ACQUIRE_TABLE_BUFFER_DENIED_SUSPEND:
+	case MEMBER_END_SUSPENDED:
+	case Z_BLOCK_SUSPENDED:
+	case OLL_REACHED_DISCARD:
+		input_consumed = scf_c_result_get_bytes_processed(
+				(struct scf_c_result *)
+				&work_unit->scf_result);
+		break;
+	case FULLY_PROCESSED:
+	case STREAM_END:
+		input_consumed = dpaa2_fd_get_len(&store->input_fd);
+		break;
+	default:
+		/* some other unexpected type of suspend, no input
+		 * processed */
+		input_consumed = 0;
+	}
+	dma_unmap_single(&session->device->dev,
+			 dpaa2_fd_get_addr(&store->scf_fd),
+			 sizeof(struct scf_c_result),
+			 DMA_BIDIRECTIONAL);
+	if (session->callback_frame) {
+		session->callback_frame(session, status, &store->input_fd,
+					&store->output_fd, input_consumed,
+					store->context);
+	} else {
+		size_t output_produced = dpaa2_fd_get_len(&store->output_fd);
+		dma_addr_t output = dpaa2_fd_get_addr(&store->output_fd);
+		dma_addr_t input = dpaa2_fd_get_addr(&store->input_fd);
+
+		session->callback_data(session, status, input, output,
+				       input_consumed, output_produced,
+				       store->context);
+	}
+	kmem_cache_free(session->work_cache,
+		       container_of(store, struct work_unit, store));
+}
+
+/* internal_callback - this is the callback that gets triggered by the DCE flow.
+ *
+ * This simple callback does simple checking the calls a function to trigger the
+ * user callback if all checks were passed */
+static void internal_callback(struct dce_flow *flow, u32 cmd,
+			    const struct dpaa2_fd *fd)
+{
+	struct dce_session *session = container_of(flow,
+						   struct dce_session,
+						   flow);
+	switch ((enum dce_cmd)cmd) {
+	case DCE_CMD_NOP:
+		pr_info("Received unexpected NOP response in DCE API\n");
+		WARN_ON(true); /* it is unexpected that the DCE API will send
+				* a NOP command, so we should never be here */
+		break;
+	case DCE_CMD_CTX_INVALIDATE:
+		pr_info("Received unexpected context invalidate in DCE API\n");
+		WARN_ON(true); /* we should never be here */
+		break;
+	case DCE_CMD_PROCESS:
+#ifdef debug
+		pr_info("Received callback for DCE process command\n");
+#endif
+		trigger_user_callback(session, fd);
+		break;
+	default:
+		pr_info("Unknown cmd %d\n", cmd);
+		break;
+	}
+}
+
+static void cleanup_caches(struct dce_session *session)
+{
+	if (session->pending_cache)
+		kmem_cache_destroy(session->pending_cache);
+	if (session->history_cache)
+		kmem_cache_destroy(session->history_cache);
+	if (session->context_cache)
+		kmem_cache_destroy(session->context_cache);
+	if (session->work_cache)
+		kmem_cache_destroy(session->work_cache);
+}
+
+static int setup_caches(struct dce_session *session)
+{
+	if (session->engine == DCE_COMPRESSION) {
+		session->pending_cache =
+			kmem_cache_create("pending_compression_output",
+				8202, 64, 0, NULL);
+		session->history_cache =
+			kmem_cache_create("compression_history",
+				4096, 64, 0, NULL);
+		session->context_cache = NULL;
+
+	} else if (session->engine == DCE_DECOMPRESSION) {
+		session->pending_cache =
+			kmem_cache_create("pending_decompression_output",
+				1024 * 28, 64, 0, NULL);
+		session->history_cache =
+			kmem_cache_create("decompression_history",
+				1024 * 32, 64, 0, NULL);
+		session->context_cache =
+			kmem_cache_create("decompression_context",
+				256, 0, 0, NULL);
+	} else {
+		return -EINVAL;
+	}
+
+	/* Alignment not necessary for work cache? Specified in struct */
+	session->work_cache = kmem_cache_create("work_units",
+					sizeof(struct work_unit), 64, 0, NULL);
+
+	if (!session->pending_cache || !session->history_cache ||
+		!session->work_cache || (!session->context_cache &&
+				session->engine == DCE_DECOMPRESSION)) {
+		cleanup_caches(session);
+		return -ENOMEM;
+	}
+	return 0;
+}
+
+static void free_dce_internals(struct dce_session *session)
+{
+	if (session->pending_output.vaddr)
+		kmem_cache_free(session->pending_cache,
+				session->pending_output.vaddr);
+	if (session->history.vaddr)
+		kmem_cache_free(session->history_cache, session->history.vaddr);
+	if (session->decomp_context.vaddr)
+		kmem_cache_free(session->context_cache,
+				session->decomp_context.vaddr);
+	session->pending_output.vaddr = session->history.vaddr =
+		session->decomp_context.vaddr = NULL;
+
+}
+
+static int alloc_dce_internals(struct dce_session *session)
+{
+	session->pending_output.vaddr = kmem_cache_zalloc(
+					session->pending_cache, GFP_KERNEL);
+	session->history.vaddr = kmem_cache_zalloc(session->history_cache,
+			GFP_KERNEL);
+	if (session->context_cache)
+		session->decomp_context.vaddr =
+			kmem_cache_zalloc(session->context_cache, GFP_KERNEL);
+	if (!session->pending_output.vaddr || !session->history.vaddr ||
+			(!session->decomp_context.vaddr &&
+			 session->context_cache)) {
+		free_dce_internals(session);
+		return -ENOMEM;
+	}
+	return 0;
+}
+
+int dce_session_create(struct dce_session *session,
+		       struct dce_session_params *params)
+{
+	/* We do not create the session struct here to allow our user to nest
+	 * the session struct in their own structures and recover the container
+	 * of the session using container_of() */
+
+	int ret;
+
+	/* We must make clear the session struct here. The session has many
+	 * pointers, other functions will assume they are valid if they are not
+	 * cleared and attempt to use them */
+	*session = (struct dce_session){0};
+
+	/* get (de)compression device */
+	if (params->engine == DCE_COMPRESSION)
+		session->device = get_compression_device();
+	else if (params->engine == DCE_DECOMPRESSION)
+		session->device = get_decompression_device();
+	else
+		return -EINVAL;
+
+	if (!session->device)
+		return -EBUSY;
+
+	ret = dce_flow_create(session->device, &session->flow);
+	if (ret)
+		return -EBUSY;
+	/* No need to configure the flow context record, because the first frame
+	 * will carry an SCR with the correct configuration and DCE will update
+	 * the FCR to match */
+
+	/* FIXME: We should use a stockpile for work units to make allocation
+	 * quick and remove the need for dma_(un)mapping per work item */
+	ret = setup_caches(session);
+	if (ret)
+		goto fail_setup_caches;
+
+	ret = alloc_dce_internals(session);
+	if (ret)
+		goto fail_dce_internals;
+
+	/* FIXME: Must handle gz_header if it is present here */
+	session->flow.cb = internal_callback;
+	session->engine = params->engine;
+	session->paradigm = params->paradigm;
+	session->compression_format = params->compression_format;
+	session->compression_effort = params->compression_effort;
+	session->buffer_pool_id = params->buffer_pool_id;
+	session->buffer_pool_id2 = params->buffer_pool_id2;
+	session->release_buffers = params->release_buffers;
+	session->encode_base_64 = params->encode_base_64;
+	session->callback_frame = params->callback_frame;
+	session->callback_data = params->callback_data;
+
+	/* Handle gzip header */
+	if (session->compression_format == DCE_SESSION_CF_GZIP) {
+		if (params->gz_header)
+			session->gz_header = params->gz_header;
+	}
+	return 0;
+
+fail_dce_internals:
+	cleanup_caches(session);
+
+fail_setup_caches:
+	dce_flow_destroy(&session->flow);
+	return ret;
+}
+EXPORT_SYMBOL(dce_session_create);
+
+struct fsl_mc_device *dce_session_device(struct dce_session *session)
+{
+	return session->device;
+}
+EXPORT_SYMBOL(dce_session_device);
+
+int dce_session_destroy(struct dce_session *session)
+{
+	/* Attempt to destroy the session while frames in flight */
+	if (atomic_read(&session->flow.frames_in_flight))
+		return -EACCES;
+	free_dce_internals(session);
+	cleanup_caches(session);
+	dce_flow_destroy(&session->flow);
+	return 0;
+}
+EXPORT_SYMBOL(dce_session_destroy);
+
+int dce_process_frame(struct dce_session *session,
+		      struct dpaa2_fd *input_fd,
+		      struct dpaa2_fd *output_fd,
+		      enum dce_flush_parameter flush,
+		      bool initial_frame,
+		      bool recycled_frame,
+		      void *context)
+{
+	struct dce_flow *flow = &session->flow;
+	struct work_unit *work_unit = kmem_cache_zalloc(session->work_cache,
+							GFP_KERNEL);
+	struct dpaa2_fd *fd_list;
+	struct dpaa2_fd *scf_fd;
+	dma_addr_t t_paddr;
+	int ret;
+
+#ifdef debug
+	pr_info("dce: work_unit %p\n", work_unit);
+#endif
+
+	/* if BMan support is enabled and this is the first frame then we need
+	 * to do some setup of the SCF. Currently BMan does not function */
+	/* dma_condition(session, work_unit); */
+
+	/* Must copy the frames over. No way around it because the frames have
+	 * to be stored in a contiguous frame list */
+	work_unit->store.input_fd = *input_fd;
+	work_unit->store.output_fd = *output_fd;
+
+	/* reorient the pointers in my stack to point to the copy for
+	 * convenience in later usage */
+	input_fd = &work_unit->store.input_fd;
+	output_fd = &work_unit->store.output_fd;
+
+	/* do the same for our scf_fd and the fd_list */
+	fd_list = &work_unit->fd_list;
+	scf_fd = &work_unit->store.scf_fd;
+
+	/* we only need to do setup work for the SCF because the input and
+	 * output were passed in with correct setup by our caller */
+
+	/* SCF */
+	t_paddr = dma_map_single(&session->device->dev,
+			&work_unit->scf_result, sizeof(struct scf_c_cfg),
+			DMA_BIDIRECTIONAL);
+	if (!t_paddr) {
+		ret = -EIO;
+		goto fail;
+	}
+	dpaa2_sg_set_final((struct dpaa2_sg_entry *)scf_fd, true);
+	dpaa2_fd_set_addr(scf_fd, t_paddr);
+	dpaa2_fd_set_len(scf_fd, sizeof(struct scf_c_cfg));
+	/* Set to recycle or truncate mode, don't need to do this every time for
+	 * statefull sessions. dont need to do it at all for stateless sessions.
+	 * Doing it every time for now. pmode 1 = truncate, 0 = recycle */
+	if (session->paradigm == DCE_SESSION_STATEFUL_RECYCLE)
+		scf_c_cfg_set_pmode((struct scf_c_cfg *)&work_unit->scf_result,
+				false);
+	else
+		scf_c_cfg_set_pmode((struct scf_c_cfg *)&work_unit->scf_result,
+				true);
+
+	/* FD */
+	dpaa2_fd_set_len(fd_list, dpaa2_fd_get_len(input_fd));
+	dpaa2_fd_set_format(fd_list, dpaa2_fd_list);
+	t_paddr =  dma_map_single(&session->device->dev,
+			work_unit->store.fd_list_store,
+			sizeof(work_unit->store.fd_list_store),
+			DMA_BIDIRECTIONAL);
+	if (!t_paddr) {
+		ret = -EIO;
+		goto fail;
+	}
+	dpaa2_fd_set_addr(fd_list, t_paddr);
+	fd_frc_set_ce((struct fd_attr *)fd_list, session->compression_effort);
+	/* hardware bug requires the SCR flush to occur every time */
+	fd_frc_set_scrf((struct fd_attr *)fd_list, true);
+	fd_frc_set_sf((struct fd_attr *)fd_list, !!session->paradigm);
+	fd_frc_set_recycle((struct fd_attr *)fd_list, recycled_frame);
+	fd_frc_set_initial((struct fd_attr *)fd_list, initial_frame);
+	fd_frc_set_z_flush((struct fd_attr *)fd_list, flush);
+
+	/* Set caller context */
+	work_unit->store.context = context;
+
+#ifdef debug
+	pr_info("dce: Before enqueue\n");
+	pretty_print_fd((struct fd_attr *)fd_list);
+	pretty_print_fle_n(
+		(struct fle_attr *)&work_unit->store.fd_list_store[0], 3);
+
+	hexdump(fd_list, sizeof(*fd_list));
+	hexdump(work_unit->store.fd_list_store,
+			sizeof(work_unit->store.fd_list_store[0])*3);
+#endif
+
+	/* enqueue request */
+	ret = enqueue_fd(flow, fd_list);
+	if (ret)
+		goto fail;
+
+	return 0;
+
+fail:
+	kmem_cache_free(session->work_cache, work_unit);
+	return ret;
+}
+EXPORT_SYMBOL(dce_process_frame);
+
+#define EMPTY_DPAA_FD {.words = {0, 0, 0, 0, 0, 0, 0, 0} }
+
+int dce_process_data(struct dce_session *session,
+		     dma_addr_t input,
+		     dma_addr_t output,
+		     size_t input_len,
+		     size_t output_len,
+		     enum dce_flush_parameter flush,
+		     bool initial_frame,
+		     bool recycled_frame,
+		     void *context)
+{
+	struct dpaa2_fd input_fd = EMPTY_DPAA_FD,
+		output_fd = EMPTY_DPAA_FD;
+
+	/* Input fd setup */
+	dpaa2_fd_set_addr(&input_fd, input);
+	dpaa2_fd_set_len(&input_fd, input_len);
+
+	/* Output fd setup */
+	dpaa2_fd_set_addr(&output_fd, output);
+	dpaa2_fd_set_len(&output_fd, output_len);
+
+	return dce_process_frame(session, &input_fd, &output_fd, flush,
+				 initial_frame, recycled_frame, context);
+}
+EXPORT_SYMBOL(dce_process_data);
+
diff --git a/drivers/staging/fsl-dpaa2/dce/dce.h b/drivers/staging/fsl-dpaa2/dce/dce.h
new file mode 100644
index 0000000..48c6dda
--- /dev/null
+++ b/drivers/staging/fsl-dpaa2/dce/dce.h
@@ -0,0 +1,369 @@
+/* Copyright (C) 2015 Freescale Semiconductor, Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+ * Assumptions:
+ * - Currently we allocate one SCF per frame list to DCE
+ *   FIXME: this allocation is slow and should not be on the data path
+ * - There is no need for supporting BMan in the convenience functions *_data
+ *   like process_data() and its associated callback, because if contiguous
+ *   memory is given, contiguous memory should be returned. BMan buffers will be
+ *   tough to glue into one contiguous map
+ * - A DCE API session will only need to serve one callback form. There are
+ *   currently two callback forms, process_frame, and process_data
+ */
+
+
+#include "dpdcei-drv.h"
+#include "dce-fd.h"
+#include "dce-fd-frc.h"
+
+
+/*
+ * The DCE API - A reentrant simplified interface to DCE
+ *
+ * Goal:
+ *  This API was designed to simplify interaction with DCE as much as possible
+ *  without loss of flexibility and acceleration offered by DCE hardware
+ *
+ * Theory of operation:
+ *  A user creates a session object to process multiple pieces of similar data
+ *  on DCE.  All subsequent interaction is done through this session. One
+ *  session can be used concurrently, if order is not necessary. Multiple
+ *  sessions can be used simultaneously
+ *
+ * Expected user knowledge:
+ *  Users of the API must have a basic understanding of DCE interface to
+ *  be able to select the correct flags and supply the right input/output based
+ *  on DCE specific response codes. In addition to this header file, the user
+ *  should have read the `System Interface' section of the DCE block guide.
+ *  Special attention should be given to the following sections. `PROCESS
+ *  command', `Frame Processing Modes', `Multiple Members Input
+ *  Frames', `Status Code Enumeration', `zlib Flush Semantics'
+ */
+
+
+/* enum dce_engine - The engine to use for session operations */
+enum dce_engine {
+	DCE_COMPRESSION,
+	DCE_DECOMPRESSION
+};
+
+enum dce_paradigm {
+	DCE_SESSION_STATELESS = 0,
+	DCE_SESSION_STATEFUL_TRUNCATION = 1,
+	DCE_SESSION_STATEFUL_RECYCLE = 2
+};
+
+enum dce_compression_format {
+	DCE_SESSION_CF_DEFLATE = 0,
+	DCE_SESSION_CF_ZLIB = 1,
+	DCE_SESSION_CF_GZIP = 2
+};
+
+enum dce_compression_effort {
+	DCE_SESSION_CE_NONE = 0,
+	DCE_SESSION_CE_STATIC_HUFF_STRMATCH = 1,
+	DCE_SESSION_CE_HUFF_ONLY = 2,
+	DCE_SESSION_CE_BEST_POSSIBLE = 3,
+};
+
+/* These align with the ZLIB standard */
+enum dce_flush_parameter {
+	DCE_Z_NO_FLUSH = 0x0,
+	DCE_Z_PARTIAL_FLUSH = 0x1,
+	DCE_Z_SYNC_FLUSH = 0x2,
+	DCE_Z_FULL_FLUSH = 0x3,
+	DCE_Z_FINISH = 0x4,
+	DCE_Z_BLOCK = 0x5,
+	DCE_Z_TREES = 0x6
+};
+
+struct dce_gz_header {
+	int text; /* True if compressed data believed to be text */
+	unsigned long time; /* Modification time */
+	int xflags; /* Extra flags indicating compression level (not used when
+		       writing a gzip file) */
+	int os; /* operating system */
+	dma_addr_t meta_data; /* Compression: dma to `extra' field, `name'
+				 field, and `comment' field. `name' and
+				 `comment' fields must be zero terminated.
+				 meta_data must be set to NULL if none of the
+				 fields are present
+				 Decompression: dma to `extra' field, `name'
+				 field, and comment field. meta_data must be
+				 set to NULL if fields are not needed. Fields
+				 will be discarded */
+	unsigned int extra_len; /* Compression: `extra' field length in
+				   meta_data
+				   Decompression: Length of the `extra' field
+				   (valid if meta_data != NULL) */
+	unsigned int name_len; /* Compression: `name' field length in meta_data
+				  Decompression: Length of the `name' field
+				  (valid if meta_data != NULL) */
+	unsigned int comment_len; /* Compression: `comment' field length in
+				     meta_daata
+				     Decompression: Length of the `comment'
+				     field
+				     (valid if meta_data != NULL) */
+	unsigned int meta_max; /* Space at meta_data (when reading header) */
+	int hcrc; /* true if there was or will be a header crc */
+	int done; /* true when done reading gzip header (not used when writing a
+		     gzip file) */
+};
+
+struct dce_session;
+
+/**
+ * \typedef dce_callback_frame
+ * \brief Return result of a (de)compress process_frame() call
+ * @session:	Pointer to session struct for which response was received from
+ *		DCE
+ * @status:	The status returned by DCE
+ * @input_fd:	Pointer to the input frame. NB: The FD pointed to is no
+ *		persistent. A copy should be made by the callback if the
+ *		preservation of the FD is needed
+ * @output_fd:	Pointer to output FD. Same consideration as @input_fd
+ * @input_consumed:	Number of bytes used in creating output
+ * @output_produced:	Number of bytes produced
+ * @context:	Pointer to user defined object received in process_frame() call
+ */
+typedef void (*dce_callback_frame)(struct dce_session *session,
+		uint8_t status,
+		struct dpaa2_fd *input_fd,
+		struct dpaa2_fd *output_fd,
+		size_t input_consumed,
+		void *context);
+
+/**
+ * \typedef dce_callback_data
+ * \brief Return result of a (de)compress process_data() call
+ * @session:	Pointer to session struct for which response was received from
+ *		DCE
+ * @status:	The status returned by DCE
+ * @input:	Input pointer as received by the API in process_data call
+ * @output:	Output pointer to resulting data
+ * @input_consumed:	Number of bytes used in creating output
+ * @output_produced:	Number of bytes produced
+ * @context:	Pointer to user defined object received in process_data() call
+ */
+typedef void (*dce_callback_data)(struct dce_session *session,
+		uint8_t status,
+		dma_addr_t input,
+		dma_addr_t output,
+		size_t input_consumed,
+		size_t output_produced,
+		void *context);
+
+/* dce_params - parameters used in initialisation of dce_session
+ * TODO: must figure out how buffer pool support works. Who populates it
+ *  who frees buffers? Who knows the buffer size ... could cause a change in
+ *  API */
+struct dce_session_params {
+	enum dce_engine engine; /* compression or decompression */
+	enum dce_paradigm paradigm; /* statefull_recycle, statefull_truncate,
+				     * or stateless */
+	/* gzip, zlib, deflate */
+	enum dce_compression_format compression_format;
+	enum dce_compression_effort compression_effort; /* compression effort */
+	struct dce_gz_header *gz_header; /* Valid in gzip mode. Should be NULL
+					  * in all other modes
+					  * Compression: Pointer to gzip header
+					  * with appropriate values to use for
+					  * setting up gzip member headers
+					  * Decompression: Pointer to gzip
+					  * struct in which to place read
+					  * headers
+					  * NB: Header must be persistent until
+					  * session_destroy() */
+	unsigned buffer_pool_id; /* Not supported in current hardware */
+	unsigned buffer_pool_id2; /* Not supported in current hardware */
+	bool release_buffers; /* Not supported in current hardware */
+	bool encode_base_64; /* session will handle 64 bit encoded data */
+	/* User defined callback function for process_frame() result */
+	dce_callback_frame callback_frame;
+	/* User defined callback function for process_data() result */
+	dce_callback_data callback_data;
+};
+
+/* FIXME: these two structs were originally in the dce.c moved here because I
+ * needed to declare the struct in my application that uses dce. Not sure if
+ * there is a better way that allows the application to struct the objects */
+struct dma_hw_mem {
+	void *vaddr;
+	size_t len;
+	dma_addr_t paddr;
+};
+/* dce_session - struct used to keep track of session state. This struct is not
+ * visible to the user */
+struct dce_session {
+	enum dce_engine engine;
+	enum dce_paradigm paradigm;
+	enum dce_compression_format compression_format;
+	enum dce_compression_effort compression_effort;
+	struct dce_gz_header *gz_header;
+	unsigned buffer_pool_id;
+	unsigned buffer_pool_id2;
+	bool release_buffers;
+	bool encode_base_64;
+	dce_callback_frame callback_frame;
+	dce_callback_data callback_data;
+	struct fsl_mc_device *device;
+	struct dce_flow flow;
+	struct kmem_cache *pending_cache;
+	struct kmem_cache *history_cache;
+	struct kmem_cache *context_cache;
+	struct kmem_cache *work_cache;
+	struct dma_hw_mem pending_output;
+	struct dma_hw_mem history;
+	struct dma_hw_mem decomp_context;
+};
+
+/**
+ * dce_session_create() - Initialise a session for compression or decompression
+ * @session:	Pointer to a session struct to be initialised
+ * @params:	Pointer to a params struct to be used in configuring the session
+ *
+ * Contextual information is stored in the session object, such as the buffer
+ * pool id to use for getting buffers, the gzip header pointer to info such as
+ * the ID1 ID2 CM FLG MTIME XFL OS fields. A session is setup then used to send
+ * many requests to DCE
+ *
+ * Return:	0 on success, error otherwise
+ */
+int dce_session_create(struct dce_session *session,
+		       struct dce_session_params *params);
+
+/** dce_session_device - gets the (de)compression device used in the session
+ * @session:	Pointer to a session struct from which to get a device
+ *
+ * Can be used by the DCE caller to dma map memory to the device before passing
+ * it to the process functions
+ *
+ * Return:	Pointer to device. NULL pointer if error
+ */
+struct fsl_mc_device *dce_session_device(struct dce_session *session);
+
+
+/**
+ * dce_session_destroy() - cleanup and release resources held by session
+ * @session:	Pointer to a session to be retired
+ *
+ * This function checks for work units in flight and make sure that there is no
+ * attempt to cleanup a session while WIP
+ *
+ * Return:	0 on success, -EACCES if there is still work in progress
+ */
+int dce_session_destroy(struct dce_session *session);
+
+
+/**
+ * dce_process_frame() - Compress or decompress a frame asynchronously
+ * @session:	Pointer to session struct on which to send (de)compress requests
+ * @input_fd:	Pointer to a FD that contains the input data
+ * @output_fd:	Pointer to a FD that has the output buffer. If this parameter is
+ *		NULL then the buffer pool associated with the session to acquire
+ *		buffers as necessary
+ * @flush:	Flush behaviour for the request using zLib semantics
+ * @initial_frame:	Indicates that this is the first frame in a flow
+ * @recycled_frame:	Indicates this frame is a response to a session suspend
+ * @context:	Pointer to a caller defined object that is returned in dequeue
+ *
+ * More on @context
+ * The caller can point context at a meaningful object to allow the user defined
+ * callback to take some useful action. e.g. Wakeup a sleeping thread, pass on
+ * some information about the destination for the data
+ *
+ * Return:	0 on success,
+ *		-EBUSY if the device is busy and call must be reattempted
+ */
+int dce_process_frame(struct dce_session *session,
+		      struct dpaa2_fd *input_fd,
+		      struct dpaa2_fd *output_fd,
+		      enum dce_flush_parameter flush,
+		      bool initial_frame,
+		      bool recycled_frame,
+		      void *context);
+
+
+/**
+ * dce_process_data() - Compress or decompress arbitrary data asynchronously
+ * @session:	Pointer to a session struct on which to send (de)compress
+ *		requests
+ * @input:	DMA address to input data, can be NULL if final input was
+ *		passed in the previous process calls
+ * @output:	DMA address to output buffer, must not be NULL
+ * @input_len:	Size of the data for input
+ * @output_len:	Size of the output buffer available. BMan output is not
+ *		supported in rev 1 silicon. The size currently must be greater
+ *		than 0
+ * @flush:	Flush behaviour for the request using zLib semantics
+ * @initial_request:	Indicates that this is the first frame in a flow
+ * @recycled_request:	Indicates this frame is a response to a session suspend
+ * @context:	Pointer to a caller defined object that is returned in dequeue
+ *
+ * More on @context
+ * The caller can point context at a meaningful object to allow the user defined
+ * callback to take some useful action. e.g. Wakeup a sleeping thread, pass on
+ * some information about where is the destination for the data.
+ *
+ * Return:	0 on success,
+ *		-EBUSY if the device is busy and call must be reattempted
+ */
+int dce_process_data(struct dce_session *session,
+		     dma_addr_t input,
+		     dma_addr_t output,
+		     size_t input_len,
+		     size_t output_len,
+		     enum dce_flush_parameter flush,
+		     bool initial_request,
+		     bool recycled_request,
+		     void *context);
+
+
+/**
+ * dce_gz_header_update() - Notify session of a gzip header update
+ * @session: Pointer to a session struct that must be notified of the header
+ *	     update
+ *
+ * This function is only valid for Compression sessions
+ * Return: 0 on success,
+ *	   -EBUSY if the device is busy and call must be reattempted
+ *	   -EINVAL if the session is not in gzip mode, is a decompression
+ *	   session, or a stateless compression session. For stateless
+ *	   compression sessions the gzip header will be updated automatically
+ *	   with every call to process_frame or process_data
+ */
+int dce_gz_header_update(struct dce_session *session);
+
+
+/* Maybe add a scatter gather version of process to handle kernel scatter
+ * gather */
diff --git a/drivers/staging/fsl-dpaa2/dce/dpdcei-cmd.h b/drivers/staging/fsl-dpaa2/dce/dpdcei-cmd.h
index f4d6ac1..828c8dc 100644
--- a/drivers/staging/fsl-dpaa2/dce/dpdcei-cmd.h
+++ b/drivers/staging/fsl-dpaa2/dce/dpdcei-cmd.h
@@ -34,7 +34,7 @@
 
 /* DPDCEI Version */
 #define DPDCEI_VER_MAJOR				1
-#define DPDCEI_VER_MINOR				0
+#define DPDCEI_VER_MINOR				1
 
 /* Command IDs */
 #define DPDCEI_CMDID_CLOSE				0x800
diff --git a/drivers/staging/fsl-dpaa2/dce/dpdcei-drv.c b/drivers/staging/fsl-dpaa2/dce/dpdcei-drv.c
new file mode 100644
index 0000000..1fd49be
--- /dev/null
+++ b/drivers/staging/fsl-dpaa2/dce/dpdcei-drv.c
@@ -0,0 +1,724 @@
+/* Copyright 2014 Freescale Semiconductor Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *	 notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *	 notice, this list of conditions and the following disclaimer in the
+ *	 documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *	 names of its contributors may be used to endorse or promote products
+ *	 derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/types.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/interrupt.h>
+#include <linux/dma-mapping.h>
+#include <linux/circ_buf.h>
+
+#include "../../fsl-mc/include/mc.h"
+#include "../../fsl-mc/include/fsl_dpaa2_io.h"
+#include "dpdcei.h"
+#include "dpdcei-cmd.h"
+#include "dpdcei-drv.h"
+#include "dce-private.h"
+
+#include "dce-fd-frc.h"
+
+#define LDPAA_DCE_DESCRIPTION "Freescale LDPAA DCE Driver"
+
+#define DQ_STORE_SIZE	16
+
+MODULE_LICENSE("Dual BSD/GPL");
+MODULE_AUTHOR("Freescale Semiconductor, Inc");
+MODULE_DESCRIPTION(LDPAA_DCE_DESCRIPTION);
+
+static int setup_flow_lookup_table(struct fsl_mc_device *ls_dev,
+					struct dpdcei_priv *priv)
+{
+	priv->flow_table_size = CONFIG_FSL_DCE_FLOW_LIMIT;
+	spin_lock_init(&priv->table_lock);
+
+	priv->flow_lookup_table = vzalloc((priv->flow_table_size *
+				sizeof(void *)));
+	if (!priv->flow_lookup_table)
+		return -ENOMEM;
+	return 0;
+}
+
+static int find_empty_flow_table_entry(u32 *entry, struct dce_flow *flow)
+{
+	u32 i;
+	struct device *dev = &flow->ls_dev->dev;
+	struct dpdcei_priv *priv;
+
+	priv = dev_get_drvdata(dev);
+
+	spin_lock(&priv->table_lock);
+	for (i = 1; i < priv->flow_table_size; i++) {
+		if (priv->flow_lookup_table[i] == NULL) {
+			*entry = i;
+			priv->flow_lookup_table[i] = flow;
+			spin_unlock(&priv->table_lock);
+			return 0;
+		}
+	}
+	spin_unlock(&priv->table_lock);
+	return -ENOMEM;
+}
+
+static void clear_flow_table_entry(struct dce_flow *flow, u32 entry)
+{
+	struct device *dev = &flow->ls_dev->dev;
+	struct dpdcei_priv *priv;
+
+	priv = dev_get_drvdata(dev);
+	spin_lock(&priv->table_lock);
+	BUG_ON(entry >= priv->flow_table_size);
+	priv->flow_lookup_table[entry] = NULL;
+	spin_unlock(&priv->table_lock);
+}
+
+/* hack to get handle */
+static struct fsl_mc_device *compression;
+static struct fsl_mc_device *decompression;
+
+struct fsl_mc_device *get_compression_device(void)
+{
+	return compression;
+}
+EXPORT_SYMBOL(get_compression_device);
+
+struct fsl_mc_device *get_decompression_device(void)
+{
+	return decompression;
+}
+EXPORT_SYMBOL(get_decompression_device);
+
+int dce_flow_create(struct fsl_mc_device *ls_dev, struct dce_flow *flow)
+{
+	int err;
+	struct device *dev = &ls_dev->dev;
+	struct dpdcei_priv *priv;
+
+	priv = dev_get_drvdata(dev);
+
+	/* associate flow to device */
+	flow->ls_dev = ls_dev;
+
+	flow->flc.len = sizeof(struct fcr);
+	flow->flc.virt = kmem_cache_zalloc(priv->slab_fcr, GFP_KERNEL);
+	if (!flow->flc.virt) {
+		err = -ENOMEM;
+		goto err_fcr_alloc;
+	}
+
+	err = find_empty_flow_table_entry(&flow->key, flow);
+	if (err) {
+		dev_err(dev, "Hash table full\n");
+		goto err_get_table_entry;
+	}
+	/* set the next_flc to myself, but virtual address */
+	fcr_set_next_flc(flow->flc.virt, (uint64_t)flow);
+	flow->flc.phys = dma_map_single(dev, flow->flc.virt, flow->flc.len,
+				DMA_BIDIRECTIONAL);
+	if (dma_mapping_error(dev, flow->flc.phys)) {
+		err = -EIO;
+		goto err_dma_map;
+	}
+	atomic_set(&flow->frames_in_flight, 0);
+	return 0;
+
+err_dma_map:
+	clear_flow_table_entry(flow, flow->key);
+err_get_table_entry:
+	kmem_cache_free(priv->slab_fcr, flow->flc.virt);
+err_fcr_alloc:
+	return err;
+}
+EXPORT_SYMBOL(dce_flow_create);
+
+int dce_flow_destroy(struct dce_flow *flow)
+{
+	struct device *dev = &flow->ls_dev->dev;
+	struct dpdcei_priv *priv;
+
+	priv = dev_get_drvdata(dev);
+
+	dma_unmap_single(dev, flow->flc.phys, flow->flc.len, DMA_BIDIRECTIONAL);
+	flow->flc.phys = 0;
+	flow->flc.len = 0;
+
+	clear_flow_table_entry(flow, flow->key);
+	kmem_cache_free(priv->slab_fcr, flow->flc.virt);
+	flow->flc.virt = NULL;
+	return 0;
+}
+EXPORT_SYMBOL(dce_flow_destroy);
+
+int enqueue_fd(struct dce_flow *flow, struct dpaa2_fd *fd)
+{
+	struct device *dev = &flow->ls_dev->dev;
+	struct dpdcei_priv *priv;
+	enum dce_cmd cmd = fd_frc_get_cmd((struct fd_attr *)fd);
+	int err = 0;
+
+	priv = dev_get_drvdata(dev);
+
+	/* set the FD[FLC] "flow context pointer" to input flow physical */
+
+	/* TODO: update what stashing control is added */
+	fd_attr_set_flc_64((struct fd_attr *)fd, flow->flc.phys);
+
+	switch (cmd) {
+	case DCE_CMD_NOP:
+		fd_frc_set_nop_token((struct fd_attr *)fd, flow->key);
+		break;
+	case DCE_CMD_CTX_INVALIDATE:
+		fd_frc_set_cic_token((struct fd_attr *)fd, flow->key);
+		break;
+	case DCE_CMD_PROCESS:
+		/* ensure that SCRF is set, hw bug */
+		fd->simple.frc |= (1 << 22);
+		break;
+	default:
+		dev_err(dev, "Unsupported dce command %d\n", cmd);
+		BUG();
+		return -EINVAL;
+	}
+
+	/* advance head now since consumer can be called during enqueue */
+	atomic_inc(&priv->frames_in_flight);
+	atomic_inc(&flow->frames_in_flight);
+
+	err = dpaa2_io_service_enqueue_fq(priv->dpio_p, priv->tx_fqid, fd);
+	if (err < 0) {
+		dev_err(dev, "error enqueueing Tx frame\n");
+		atomic_dec(&priv->frames_in_flight);
+		atomic_dec(&flow->frames_in_flight);
+	}
+	return err;
+}
+EXPORT_SYMBOL(enqueue_fd);
+
+int enqueue_nop(struct dce_flow *flow)
+{
+	struct dpaa2_fd fd;
+
+	memset(&fd, 0, sizeof(fd));
+	/* setup FD as a NOP command */
+	fd_frc_set_cmd((struct fd_attr *)&fd, DCE_CMD_NOP);
+
+	return enqueue_fd(flow, &fd);
+}
+EXPORT_SYMBOL(enqueue_nop);
+
+int enqueue_cic(struct dce_flow *flow)
+{
+	struct dpaa2_fd fd;
+
+	memset(&fd, 0, sizeof(fd));
+	/* setup FD as a CIC command */
+	fd_frc_set_cmd((struct fd_attr *)&fd, DCE_CMD_CTX_INVALIDATE);
+
+	return enqueue_fd(flow, &fd);
+}
+EXPORT_SYMBOL(enqueue_cic);
+
+static const char *engine_to_str(enum dpdcei_engine engine)
+{
+	if (engine == DPDCEI_ENGINE_COMPRESSION)
+		return "COMPRESSION";
+	else if (engine == DPDCEI_ENGINE_DECOMPRESSION)
+		return "DECOMPRESSION";
+	else
+		return "UNKNOWN";
+}
+
+/*****************************************************************************/
+/* all input to be enqueued is stored in a circular ring */
+
+/* DCE responses control block */
+struct dce_response_cb {
+	struct work_struct async_response_work; /* Asynchronous resposne work */
+	struct dpaa2_io_notification_ctx *ctx;
+};
+
+static int dpaa2_dce_pull_dequeue_rx(struct dpdcei_priv *priv)
+{
+	struct device *dev = &priv->dpdcei_dev->dev;
+	int err = 0;
+	int is_last = 0;
+	struct dpaa2_dq *dq;
+	const struct dpaa2_fd *fd;
+	struct dce_flow *flow;
+	u32 key;
+
+	do {
+		err = dpaa2_io_service_pull_fq(priv->dpio_p, priv->rx_fqid,
+					      priv->rx_store);
+	} while (err);
+
+	while (!is_last) {
+		enum dce_cmd cmd;
+
+		do {
+			dq = dpaa2_io_store_next(priv->rx_store, &is_last);
+		} while (!is_last && !dq);
+		if (!dq) {
+			dev_err(dev, "FQID returned no valid frames!\n");
+			break;
+		}
+
+		/* Obtain FD and process it */
+		fd = dpaa2_dq_fd(dq);
+		/* We are already CPU-affine, and since we aren't going
+		 * to start more than one Rx thread per CPU, we're
+		 * good enough for now.
+		 */
+		cmd = fd_frc_get_cmd((struct fd_attr *)fd);
+		flow = (struct dce_flow *)fd_attr_get_flc_64(
+						(struct fd_attr *)fd);
+
+		switch (cmd) {
+		case DCE_CMD_NOP:
+			key = fd_frc_get_nop_token((struct fd_attr *)fd);
+			flow = priv->flow_lookup_table[key];
+			flow->cb(flow, cmd, fd);
+			break;
+		case DCE_CMD_CTX_INVALIDATE:
+			key = fd_frc_get_cic_token((struct fd_attr *)fd);
+			flow = priv->flow_lookup_table[key];
+			flow->cb(flow, cmd, fd);
+			break;
+		case DCE_CMD_PROCESS:
+			flow->cb(flow, cmd, fd);
+			break;
+
+		default:
+			dev_err(dev, "Unsupported DCE CMD %d\n", cmd);
+		}
+
+		atomic_dec(&priv->frames_in_flight);
+		atomic_dec(&flow->frames_in_flight);
+	}
+	return 0;
+}
+
+static void dequeue_rx_work_func(struct work_struct *work)
+{
+	struct dce_response_cb *ent;
+	struct dpdcei_priv *priv;
+
+	ent = container_of(work, struct dce_response_cb, async_response_work);
+	priv = container_of(ent->ctx, struct dpdcei_priv, notif_ctx_rx);
+	dpaa2_dce_pull_dequeue_rx(priv);
+	dpaa2_io_service_rearm(priv->dpio_p, ent->ctx);
+}
+
+static void fqdan_cb_rx(struct dpaa2_io_notification_ctx *ctx)
+{
+	struct dpdcei_priv *priv = container_of(ctx, struct dpdcei_priv,
+						   notif_ctx_rx);
+	struct dce_response_cb *work;
+
+	work = kmalloc(sizeof(*work), GFP_KERNEL);
+	INIT_WORK(&work->async_response_work, dequeue_rx_work_func);
+	work->ctx = ctx;
+	queue_work(priv->async_resp_wq, &work->async_response_work);
+}
+
+static int __cold dpdcei_dpio_service_setup(struct dpdcei_priv *priv)
+{
+	int err;
+	struct device *dev = &priv->dpdcei_dev->dev;
+
+	/* Register notification callbacks */
+	priv->notif_ctx_rx.is_cdan = 0;
+	priv->notif_ctx_rx.desired_cpu = -1;
+	priv->notif_ctx_rx.cb = fqdan_cb_rx;
+	priv->notif_ctx_rx.id = priv->rx_fqid;
+	err = dpaa2_io_service_register(priv->dpio_p, &priv->notif_ctx_rx);
+	if (err) {
+		dev_err(dev, "Rx notif register failed 0x%x\n", err);
+		return err;
+	}
+	return 0;
+}
+
+static int dpdcei_dpio_service_teardown(struct dpdcei_priv *priv)
+{
+	int err;
+	struct device *dev = &priv->dpdcei_dev->dev;
+
+	/* Deregister notification callbacks */
+	err = dpaa2_io_service_deregister(priv->dpio_p, &priv->notif_ctx_rx);
+	if (err) {
+		dev_err(dev, "dpdcei_dpio_service_teardown failed 0x%x\n", err);
+		return err;
+	}
+	return 0;
+}
+
+static int __cold dpdcei_bind_dpio(struct dpdcei_priv *priv,
+				struct fsl_mc_io *mc_io, uint16_t dpdcei_handle)
+{
+	int err;
+	struct dpdcei_rx_queue_cfg rx_queue_cfg;
+
+	/* Configure the Tx queue to generate FQDANs */
+	rx_queue_cfg.options = DPDCEI_QUEUE_OPT_USER_CTX |
+				DPDCEI_QUEUE_OPT_DEST;
+	rx_queue_cfg.user_ctx = priv->notif_ctx_rx.qman64;
+	rx_queue_cfg.dest_cfg.dest_type = DPDCEI_DEST_DPIO;
+	rx_queue_cfg.dest_cfg.dest_id = priv->notif_ctx_rx.dpio_id;
+	/* TODO: dpio could have 2 or 8 WQ need to query dpio perhaps
+	 *	hard code it to 1 for now */
+	rx_queue_cfg.dest_cfg.priority = 0;
+	err = dpdcei_set_rx_queue(mc_io, dpdcei_handle, &rx_queue_cfg);
+	if (err) {
+		dev_err(&priv->dpdcei_dev->dev,
+			"dpdcei_set_rx_flow() failed\n");
+		return err;
+	}
+
+	return 0;
+}
+
+static int __cold dpdcei_unbind_dpio(struct dpdcei_priv *priv,
+				struct fsl_mc_io *mc_io,
+				uint16_t dpdcei_handle)
+{
+	int err;
+
+	err = dpdcei_reset(mc_io, dpdcei_handle);
+	if (err) {
+		dev_err(&priv->dpdcei_dev->dev,
+			"dpdcei_reset failed\n");
+		return err;
+	}
+	priv->notif_ctx_rx.qman64 = 0;
+	priv->notif_ctx_rx.dpio_id = 0;
+
+	return 0;
+}
+
+static int dpaa2_dce_alloc_store(struct dpdcei_priv *priv)
+{
+	struct device *dev = &priv->dpdcei_dev->dev;
+
+	priv->rx_store = dpaa2_io_store_create(DQ_STORE_SIZE, dev);
+	if (!priv->rx_store) {
+		dev_err(dev, "dpaa2_io_store_create() failed\n");
+		return -ENOMEM;
+	}
+	return 0;
+}
+
+static void dpaa2_dce_free_store(struct dpdcei_priv *priv)
+{
+	dpaa2_io_store_destroy(priv->rx_store);
+}
+
+static int __cold dpaa2_dpdcei_probe(struct fsl_mc_device *ls_dev)
+{
+	struct dpdcei_priv *priv = NULL;
+	struct device *dev = &ls_dev->dev;
+	struct dpdcei_rx_queue_attr rx_attr;
+	struct dpdcei_tx_queue_attr tx_attr;
+	struct dpaa2_io *dpio_s;
+	int err = 0;
+
+	dev_info(dev, "dpdcei probe\n");
+
+	memset(&rx_attr, 0, sizeof(rx_attr));
+	memset(&tx_attr, 0, sizeof(tx_attr));
+
+	priv = devm_kzalloc(dev, sizeof(*priv), GFP_KERNEL);
+	if (!priv) {
+		err = -ENOMEM;
+		goto err_priv_alloc;
+	}
+	dev_set_drvdata(dev, priv);
+	priv->dpdcei_dev = ls_dev;
+
+	/* initialize lookup table */
+	setup_flow_lookup_table(ls_dev, priv);
+
+	/* Get dpio default service */
+	dpio_s = dpaa2_io_default_service();
+	if (!dpio_s) {
+		dev_err(dev, "Cannot get dpio service\n");
+		goto err_get_dpio_service;
+	}
+
+	err = dpaa2_io_service_get_persistent(dpio_s, -1, &priv->dpio_p);
+	if (err) {
+		dev_err(dev, "Cannot get dpio object\n");
+		goto err_get_dpio;
+	}
+
+	/* done with service */
+	dpaa2_io_down(dpio_s);
+
+	/* in flight ring initialization */
+	atomic_set(&priv->frames_in_flight, 0);
+
+	/*
+	 * Create work queue to defer work when asynchronous reponses are
+	 * received
+	 */
+
+	/* TODO: confirm valueis of wq flags being used */
+	priv->async_resp_wq = alloc_workqueue("dce_async_resp_wq",
+			WQ_UNBOUND | WQ_MEM_RECLAIM, WQ_MAX_ACTIVE);
+	if (!priv->async_resp_wq) {
+		dev_err(dev, "Cannot allocate response work queue\n");
+		err = -ENOSPC;
+		goto err_alloc_wq;
+	}
+
+	/* setup memory for flow dma stuctures */
+	priv->slab_fcr = kmem_cache_create("dce_frc", sizeof(struct fcr),
+			FCR_ALIGN, SLAB_HWCACHE_ALIGN, NULL);
+	if (!priv->slab_fcr) {
+		dev_err(dev, "Can't allocate fcr slab\n");
+		err = -ENOMEM;
+		goto err_fcr_slab_alloc;
+	}
+
+	err = fsl_mc_portal_allocate(ls_dev, 0, &ls_dev->mc_io);
+	if (err) {
+		dev_err(dev, "MC portal allocation failed\n");
+		goto err_mcportal;
+	}
+
+	/* get a handle for the DPDCEI this interface is associate with */
+	err = dpdcei_open(ls_dev->mc_io, ls_dev->obj_desc.id,
+			&ls_dev->mc_handle);
+	if (err) {
+		dev_err(dev, "dpdcei_open() failed\n");
+		goto err_open;
+	}
+
+	err = dpdcei_get_attributes(ls_dev->mc_io, ls_dev->mc_handle,
+				&priv->dpdcei_attrs);
+	if (err) {
+		dev_err(dev, "dpdcei_get_attributes() failed %d\n", err);
+		goto err_get_attr;
+	}
+
+	if (priv->dpdcei_attrs.version.major > DPDCEI_VER_MAJOR) {
+		dev_err(dev, "DPDCEI major version mismatch\n"
+			     " found %u.%u, supported version is %u.%u\n",
+				priv->dpdcei_attrs.version.major,
+				priv->dpdcei_attrs.version.minor,
+				DPDCEI_VER_MAJOR,
+				DPDCEI_VER_MINOR);
+	} else if (priv->dpdcei_attrs.version.minor > DPDCEI_VER_MINOR) {
+		dev_err(dev, "DPDCEI minor version mismatch\n"
+			     " found %u.%u, supported version is %u.%u\n",
+				priv->dpdcei_attrs.version.major,
+				priv->dpdcei_attrs.version.minor,
+				DPDCEI_VER_MAJOR,
+				DPDCEI_VER_MINOR);
+	}
+
+	dev_info(dev, "DPDCEI: id=%d, engine=%s\n", priv->dpdcei_attrs.id,
+		engine_to_str(priv->dpdcei_attrs.engine));
+
+	/* Only support one compression and decompression device */
+	if ((priv->dpdcei_attrs.engine == DPDCEI_ENGINE_COMPRESSION) &&
+			(compression != NULL)) {
+		dev_err(dev, "Compression device already present\n");
+		goto err_get_attr;
+	} else if ((priv->dpdcei_attrs.engine == DPDCEI_ENGINE_DECOMPRESSION)
+			 && (decompression != NULL)) {
+		dev_err(dev, "Decompression device already present\n");
+		goto err_get_attr;
+	}
+
+	err = dpdcei_get_rx_queue(ls_dev->mc_io, ls_dev->mc_handle, &rx_attr);
+	if (err) {
+		dev_err(dev, "dpdcei_get_rx_queue() failed %d\n", err);
+		goto err_get_attr;
+	}
+
+	priv->rx_fqid = rx_attr.fqid;
+
+	err = dpdcei_get_tx_queue(ls_dev->mc_io, ls_dev->mc_handle, &tx_attr);
+	if (err) {
+		dev_err(dev, "dpdcei_get_rx_queue() failed %d\n", err);
+		goto err_get_attr;
+	}
+	priv->tx_fqid = tx_attr.fqid;
+
+	/* dpio store */
+	err = dpaa2_dce_alloc_store(priv);
+	if (err)
+		goto err_get_attr;
+
+	/* dpio services */
+	err = dpdcei_dpio_service_setup(priv);
+	if (err)
+		goto err_dpio_setup;
+
+	/* DPDCEI binding to DPIO */
+	err = dpdcei_bind_dpio(priv, ls_dev->mc_io, ls_dev->mc_handle);
+	if (err) {
+		dev_err(dev, "Error dpdcei bind %d\n", err);
+		goto err_bind;
+	}
+
+	/* Enable the device */
+	err = dpdcei_enable(ls_dev->mc_io, ls_dev->mc_handle);
+	if (err) {
+		dev_err(dev, "dpdcei_enable failed %d\n", err);
+		goto err_enable;
+	}
+
+	if (priv->dpdcei_attrs.engine == DPDCEI_ENGINE_COMPRESSION)
+		compression = ls_dev;
+	else
+		decompression = ls_dev;
+
+	dev_info(dev, "dpdcei: probed object %d\n", ls_dev->obj_desc.id);
+	return 0;
+err_enable:
+	dpdcei_unbind_dpio(priv, ls_dev->mc_io, ls_dev->mc_handle);
+err_bind:
+	dpdcei_dpio_service_teardown(priv);
+err_dpio_setup:
+	dpaa2_dce_free_store(priv);
+err_get_attr:
+	dpdcei_close(ls_dev->mc_io, ls_dev->mc_handle);
+err_open:
+	fsl_mc_portal_free(ls_dev->mc_io);
+err_mcportal:
+	kmem_cache_destroy(priv->slab_fcr);
+err_fcr_slab_alloc:
+	destroy_workqueue(priv->async_resp_wq);
+err_alloc_wq:
+	dpaa2_io_down(priv->dpio_p);
+err_get_dpio:
+	dpaa2_io_down(dpio_s);
+err_get_dpio_service:
+	dev_set_drvdata(dev, NULL);
+	devm_kfree(dev, priv);
+err_priv_alloc:
+	return err;
+}
+
+static int __cold dpaa2_dpdcei_remove(struct fsl_mc_device *ls_dev)
+{
+	struct device *dev;
+	struct dpdcei_priv *priv;
+	int err;
+
+	dev = &ls_dev->dev;
+	priv = dev_get_drvdata(dev);
+
+	/* TODO: need to quiesce the device */
+	if (atomic_read(&priv->frames_in_flight)) {
+		dev_info(dev, "Frames still in flight\n");
+		return -EBUSY;
+	}
+
+	/* disable the device */
+	err = dpdcei_disable(ls_dev->mc_io, ls_dev->mc_handle);
+	if (err) {
+		dev_err(dev, "dpdcei_disable failed %d\n", err);
+		goto err_disable;
+	}
+
+	/* DPDCEI unbinding to DPIO */
+	err = dpdcei_unbind_dpio(priv, ls_dev->mc_io, ls_dev->mc_handle);
+	if (err) {
+		dev_err(dev, "Error dpdcei unbind 0x%x\n", err);
+		goto err_unbind;
+	}
+
+	/* dpio service teardown */
+	err = dpdcei_dpio_service_teardown(priv);
+	if (err) {
+		dev_err(dev, "Error dpdcei service teardown 0x%x\n", err);
+		goto err_service_teardown;
+	}
+
+	dpaa2_dce_free_store(priv);
+
+	err = dpdcei_close(ls_dev->mc_io, ls_dev->mc_handle);
+	if (err) {
+		dev_err(dev, "Error dpdcei close 0x%x\n", err);
+		goto err_dpdcei_close;
+	}
+
+	fsl_mc_portal_free(ls_dev->mc_io);
+
+	ls_dev->mc_io = NULL;
+
+	kmem_cache_destroy(priv->slab_fcr);
+
+	destroy_workqueue(priv->async_resp_wq);
+
+	dpaa2_io_down(priv->dpio_p);
+
+	/* Only support one compression and decompression device */
+	if (priv->dpdcei_attrs.engine == DPDCEI_ENGINE_COMPRESSION)
+		compression = NULL;
+	else if (priv->dpdcei_attrs.engine == DPDCEI_ENGINE_DECOMPRESSION)
+		decompression = NULL;
+	dev_set_drvdata(dev, NULL);
+	devm_kfree(dev, priv);
+	return 0;
+
+err_dpdcei_close:
+	/* todo: allocate store */
+err_service_teardown:
+	/* todo: rebind dpio */
+err_unbind:
+	dpdcei_enable(ls_dev->mc_io, ls_dev->mc_handle);
+err_disable:
+	return err;
+}
+
+static const struct fsl_mc_device_match_id dpaa2_dpdcei_match_id_table[] = {
+	{
+		.vendor = FSL_MC_VENDOR_FREESCALE,
+		.obj_type = "dpdcei",
+	},
+	{ .vendor = 0x0 }
+};
+
+static struct fsl_mc_driver dpaa2_dpdcei_driver = {
+	.driver = {
+		.name		= KBUILD_MODNAME,
+		.owner		= THIS_MODULE,
+	},
+	.probe		= dpaa2_dpdcei_probe,
+	.remove		= dpaa2_dpdcei_remove,
+	.match_id_table = dpaa2_dpdcei_match_id_table
+};
+
+module_fsl_mc_driver(dpaa2_dpdcei_driver);
diff --git a/drivers/staging/fsl-dpaa2/dce/dpdcei-drv.h b/drivers/staging/fsl-dpaa2/dce/dpdcei-drv.h
new file mode 100644
index 0000000..ccb0f33
--- /dev/null
+++ b/drivers/staging/fsl-dpaa2/dce/dpdcei-drv.h
@@ -0,0 +1,101 @@
+/* Copyright 2014 Freescale Semiconductor Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *	 notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *	 notice, this list of conditions and the following disclaimer in the
+ *	 documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *	 names of its contributors may be used to endorse or promote products
+ *	 derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef __DPDCEI_DRV_H
+#define __DPDCEI_DRV_H
+
+#include "../../fsl-mc/include/mc.h"
+#include "../../fsl-mc/include/fsl_dpaa2_io.h"
+#include <linux/dma-mapping.h>
+#include <linux/slab.h>
+#include <linux/workqueue.h>
+#include "dpdcei.h"
+#include "dce-fcr.h"
+
+struct dpdcei_priv {
+	struct fsl_mc_device *dpdcei_dev;
+	struct dpaa2_io *dpio_service;
+	struct dpdcei_attr dpdcei_attrs;
+	u32 rx_fqid;
+	u32 tx_fqid;
+
+	/* dpio services */
+	struct dpaa2_io *dpio_p;
+	struct dpaa2_io_notification_ctx notif_ctx_rx;
+	struct dpaa2_io_store *rx_store;
+
+	/* dma memory for flow */
+	struct kmem_cache *slab_fcr;
+
+	atomic_t frames_in_flight;
+
+	/* hash index to flow */
+	spinlock_t table_lock;
+	size_t flow_table_size;
+	void **flow_lookup_table;
+
+	/*
+	 * Multi threaded work queue used to defer the work to be
+	 * done when an asynchronous responses are received
+	 */
+	struct workqueue_struct *async_resp_wq;
+};
+
+/* Hack to get access to device */
+struct fsl_mc_device *get_compression_device(void);
+struct fsl_mc_device *get_decompression_device(void);
+
+struct flc_dma {
+	void *virt;
+	dma_addr_t phys;
+	size_t len;
+};
+
+struct dce_flow {
+	/* the callback to be invoked when the respose arrives */
+	void (*cb)(struct dce_flow *, u32 cmd, const struct dpaa2_fd *fd);
+	struct fsl_mc_device *ls_dev;
+
+	/* flow memory: both virtual and dma memory */
+	struct flc_dma flc;
+	atomic_t frames_in_flight;
+	/* key used to lookup flow in flow table */
+	u32 key;
+};
+
+int dce_flow_create(struct fsl_mc_device *dev, struct dce_flow *flow);
+int dce_flow_destroy(struct dce_flow *flow);
+
+int enqueue_fd(struct dce_flow *flow, struct dpaa2_fd *fd);
+int enqueue_nop(struct dce_flow *flow);
+int enqueue_cic(struct dce_flow *flow);
+
+#endif
-- 
2.8.1

