From dfeb3e72f6df06a21bba646761a61ea0fe8b75f4 Mon Sep 17 00:00:00 2001
From: Bogdan Hamciuc <bogdan.hamciuc@freescale.com>
Date: Thu, 16 Apr 2015 12:49:57 +0300
Subject: [PATCH 0675/1383] flib,dpaa2-eth: updated Eth (was: Rebasing onto
 kernel 3.19, MC 0.6)

updated Ethernet driver from 4.0 branch

Signed-off-by: Bogdan Hamciuc <bogdan.hamciuc@freescale.com>
[Stuart: cherry-picked patch from 4.0 and split it up]
Signed-off-by: Stuart Yoder <stuart.yoder@freescale.com>
[Original patch taken from QorIQ-SDK-V2.0-20160527-yocto]
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 MAINTAINERS                                        |   15 +
 drivers/net/ethernet/Kconfig                       |    1 -
 drivers/staging/Kconfig                            |    2 +
 drivers/staging/Makefile                           |    1 +
 drivers/staging/fsl-dpaa2/Kconfig                  |   11 +
 drivers/staging/fsl-dpaa2/Makefile                 |    5 +
 drivers/staging/fsl-dpaa2/ethernet/Kconfig         |   25 +
 drivers/staging/fsl-dpaa2/ethernet/Makefile        |   20 +
 drivers/staging/fsl-dpaa2/ethernet/dpni.c          | 1467 ++++++++++++++
 drivers/staging/fsl-dpaa2/ethernet/ldpaa_eth.c     | 2124 ++++++++++++++++++++
 drivers/staging/fsl-dpaa2/ethernet/ldpaa_eth.h     |  262 +++
 .../staging/fsl-dpaa2/ethernet/ldpaa_eth_trace.h   |  130 ++
 drivers/staging/fsl-dpaa2/ethernet/ldpaa_ethtool.c |  258 +++
 13 files changed, 4320 insertions(+), 1 deletion(-)
 create mode 100644 drivers/staging/fsl-dpaa2/Kconfig
 create mode 100644 drivers/staging/fsl-dpaa2/Makefile
 create mode 100644 drivers/staging/fsl-dpaa2/ethernet/Kconfig
 create mode 100644 drivers/staging/fsl-dpaa2/ethernet/Makefile
 create mode 100644 drivers/staging/fsl-dpaa2/ethernet/dpni.c
 create mode 100644 drivers/staging/fsl-dpaa2/ethernet/ldpaa_eth.c
 create mode 100644 drivers/staging/fsl-dpaa2/ethernet/ldpaa_eth.h
 create mode 100644 drivers/staging/fsl-dpaa2/ethernet/ldpaa_eth_trace.h
 create mode 100644 drivers/staging/fsl-dpaa2/ethernet/ldpaa_ethtool.c

diff --git a/MAINTAINERS b/MAINTAINERS
index 17afe24..f507ad8 100644
--- a/MAINTAINERS
+++ b/MAINTAINERS
@@ -4233,6 +4233,21 @@ L:	linux-kernel@vger.kernel.org
 S:	Maintained
 F:	drivers/staging/fsl-mc/
 
+FREESCALE DPAA2 ETH DRIVER
+M:	Ioana Radulescu <ruxandra.radulescu@freescale.com>
+M:	Bogdan Hamciuc <bogdan.hamciuc@freescale.com>
+M:	Cristian Sovaiala <cristian.sovaiala@freescale.com>
+L:	linux-kernel@vger.kernel.org
+S:	Maintained
+F:	drivers/staging/fsl-dpaa2/ethernet/
+
+FREESCALE QORIQ MANAGEMENT COMPLEX RESTOOL DRIVER
+M:	Lijun Pan <Lijun.Pan@freescale.com>
+L:	linux-kernel@vger.kernel.org
+S:	Maintained
+F:	drivers/staging/fsl-mc/bus/mc-ioctl.h
+F:	drivers/staging/fsl-mc/bus/mc-restool.c
+
 FREEVXFS FILESYSTEM
 M:	Christoph Hellwig <hch@infradead.org>
 W:	ftp://ftp.openlinux.org/pub/people/hch/vxfs
diff --git a/drivers/net/ethernet/Kconfig b/drivers/net/ethernet/Kconfig
index eadcb05..3393e45 100644
--- a/drivers/net/ethernet/Kconfig
+++ b/drivers/net/ethernet/Kconfig
@@ -68,7 +68,6 @@ source "drivers/net/ethernet/dlink/Kconfig"
 source "drivers/net/ethernet/emulex/Kconfig"
 source "drivers/net/ethernet/neterion/Kconfig"
 source "drivers/net/ethernet/faraday/Kconfig"
-source "drivers/net/ethernet/freescale/Kconfig"
 source "drivers/net/ethernet/fujitsu/Kconfig"
 source "drivers/net/ethernet/hisilicon/Kconfig"
 source "drivers/net/ethernet/hp/Kconfig"
diff --git a/drivers/staging/Kconfig b/drivers/staging/Kconfig
index fb38798..4714e26 100644
--- a/drivers/staging/Kconfig
+++ b/drivers/staging/Kconfig
@@ -118,6 +118,8 @@ source "drivers/staging/fsl_rman/Kconfig"
 
 source "drivers/staging/fsl_qbman/Kconfig"
 
+source "drivers/staging/fsl-dpaa2/Kconfig"
+
 source "drivers/staging/lttng/Kconfig"
 
 endif # STAGING
diff --git a/drivers/staging/Makefile b/drivers/staging/Makefile
index f159e88..8dbae7f 100644
--- a/drivers/staging/Makefile
+++ b/drivers/staging/Makefile
@@ -51,4 +51,5 @@ obj-$(CONFIG_I2O)		+= i2o/
 obj-$(CONFIG_FSL_MC_BUS)	+= fsl-mc/
 obj-$(CONFIG_FSL_RMAN_UIO)	+= fsl_rman/
 obj-$(CONFIG_FSL_DPA)           += fsl_qbman/
+obj-$(CONFIG_FSL_DPAA2)		+= fsl-dpaa2/
 obj-$(CONFIG_LTTNG)	+= lttng/
diff --git a/drivers/staging/fsl-dpaa2/Kconfig b/drivers/staging/fsl-dpaa2/Kconfig
new file mode 100644
index 0000000..24dc933
--- /dev/null
+++ b/drivers/staging/fsl-dpaa2/Kconfig
@@ -0,0 +1,11 @@
+#
+# Freescale device configuration
+#
+
+config FSL_DPAA2
+	bool "Freescale DPAA2 devices"
+	depends on FSL_MC_BUS
+	---help---
+	  Build drivers for Freescale DataPath Acceleration Architecture (DPAA2) family of SoCs.
+# TODO move DPIO driver in-here?
+source "drivers/staging/fsl-dpaa2/ethernet/Kconfig"
diff --git a/drivers/staging/fsl-dpaa2/Makefile b/drivers/staging/fsl-dpaa2/Makefile
new file mode 100644
index 0000000..dab1c21
--- /dev/null
+++ b/drivers/staging/fsl-dpaa2/Makefile
@@ -0,0 +1,5 @@
+#
+# Makefile for the Freescale network device drivers.
+#
+
+obj-$(CONFIG_FSL_DPAA2_ETH)	+= ethernet/
diff --git a/drivers/staging/fsl-dpaa2/ethernet/Kconfig b/drivers/staging/fsl-dpaa2/ethernet/Kconfig
new file mode 100644
index 0000000..43f0172
--- /dev/null
+++ b/drivers/staging/fsl-dpaa2/ethernet/Kconfig
@@ -0,0 +1,25 @@
+#
+# Freescale DPAA Ethernet driver configuration
+#
+# Copyright (C) 2014-2015 Freescale Semiconductor, Inc.
+#
+# This file is released under the GPLv2
+#
+
+menuconfig FSL_DPAA2_ETH
+	tristate "Freescale DPAA Ethernet"
+	depends on FSL_DPAA2 && FSL_MC_BUS && FSL_MC_DPIO
+	select FSL_DPAA2_MAC
+	default y
+	---help---
+	  Freescale Data Path Acceleration Architecture Ethernet
+	  driver, using the Freescale MC bus driver.
+
+if FSL_DPAA2_ETH
+config FSL_DPAA2_ETH_GCOV
+	bool "Gcov support in the FSL LDPAA Ethernet driver"
+	default n
+	depends on GCOV_KERNEL
+	---help---
+	  Compile the driver source with GCOV_PROFILE := y
+endif
diff --git a/drivers/staging/fsl-dpaa2/ethernet/Makefile b/drivers/staging/fsl-dpaa2/ethernet/Makefile
new file mode 100644
index 0000000..e856789
--- /dev/null
+++ b/drivers/staging/fsl-dpaa2/ethernet/Makefile
@@ -0,0 +1,20 @@
+#
+# Makefile for the Freescale DPAA Ethernet controllers
+#
+# Copyright (C) 2014 Freescale Semiconductor, Inc.
+#
+# This file is released under the GPLv2
+#
+
+ccflags-y += -DVERSION=\"\"
+
+obj-$(CONFIG_FSL_DPAA2_ETH) += fsl-ldpaa-eth.o
+
+fsl-ldpaa-eth-objs    := ldpaa_eth.o ldpaa_ethtool.o dpni.o
+
+#Needed by the tracing framework
+CFLAGS_ldpaa_eth.o := -I$(src)
+
+ifeq ($(CONFIG_FSL_DPAA2_ETH_GCOV),y)
+	GCOV_PROFILE := y
+endif
diff --git a/drivers/staging/fsl-dpaa2/ethernet/dpni.c b/drivers/staging/fsl-dpaa2/ethernet/dpni.c
new file mode 100644
index 0000000..b7714c4
--- /dev/null
+++ b/drivers/staging/fsl-dpaa2/ethernet/dpni.c
@@ -0,0 +1,1467 @@
+/* Copyright 2013-2015 Freescale Semiconductor Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ * * Redistributions of source code must retain the above copyright
+ * notice, this list of conditions and the following disclaimer.
+ * * Redistributions in binary form must reproduce the above copyright
+ * notice, this list of conditions and the following disclaimer in the
+ * documentation and/or other materials provided with the distribution.
+ * * Neither the name of the above-listed copyright holders nor the
+ * names of any contributors may be used to endorse or promote products
+ * derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE
+ * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+#include "../../fsl-mc/include/mc-sys.h"
+#include "../../fsl-mc/include/mc-cmd.h"
+#include "../../fsl-mc/include/dpni.h"
+#include "../../fsl-mc/include/dpni-cmd.h"
+
+int dpni_prepare_key_cfg(struct dpkg_profile_cfg *cfg,
+			 uint8_t *key_cfg_buf)
+{
+	int i, j;
+	int offset = 0;
+	int param = 1;
+	uint64_t *params = (uint64_t *)key_cfg_buf;
+	struct {
+		uint32_t field;
+		uint8_t size;
+		uint8_t offset;
+		uint8_t hdr_index;
+		uint8_t constant;
+		uint8_t num_of_repeats;
+		enum net_prot prot;
+		enum dpkg_extract_from_hdr_type type;
+	} u_cfg[DPKG_MAX_NUM_OF_EXTRACTS] = { 0 };
+
+	if (!key_cfg_buf || !cfg)
+			return -EINVAL;
+
+	for (i = 0; i < cfg->num_extracts; i++) {
+		switch (cfg->extracts[i].type) {
+		case DPKG_EXTRACT_FROM_HDR:
+			u_cfg[i].prot = cfg->extracts[i].extract.from_hdr.prot;
+			u_cfg[i].type = cfg->extracts[i].extract.from_hdr.type;
+			u_cfg[i].field =
+				cfg->extracts[i].extract.from_hdr.field;
+			u_cfg[i].size = cfg->extracts[i].extract.from_hdr.size;
+			u_cfg[i].offset =
+				cfg->extracts[i].extract.from_hdr.offset;
+			u_cfg[i].hdr_index =
+				cfg->extracts[i].extract.from_hdr.hdr_index;
+			break;
+		case DPKG_EXTRACT_FROM_DATA:
+			u_cfg[i].size = cfg->extracts[i].extract.from_data.size;
+			u_cfg[i].offset =
+				cfg->extracts[i].extract.from_data.offset;
+			break;
+		case DPKG_EXTRACT_CONSTANT:
+			u_cfg[i].constant =
+				cfg->extracts[i].extract.constant.constant;
+			u_cfg[i].num_of_repeats =
+			       cfg->extracts[i].extract.constant.num_of_repeats;
+			break;
+		default:
+			return -EINVAL;
+		}
+	}
+	params[0] |= mc_enc(0, 8, cfg->num_extracts);
+	params[0] = cpu_to_le64(params[0]);
+
+	for (i = 0; i < DPKG_MAX_NUM_OF_EXTRACTS; i++) {
+		params[param] |= mc_enc(0, 8, u_cfg[i].prot);
+		params[param] |= mc_enc(8, 4, u_cfg[i].type);
+		params[param] |= mc_enc(16, 8, u_cfg[i].size);
+		params[param] |= mc_enc(24, 8, u_cfg[i].offset);
+		params[param] |= mc_enc(32, 32, u_cfg[i].field);
+		params[param] = cpu_to_le64(params[param]);
+		param++;
+		params[param] |= mc_enc(0, 8, u_cfg[i].hdr_index);
+		params[param] |= mc_enc(8, 8, u_cfg[i].constant);
+		params[param] |= mc_enc(16, 8, u_cfg[i].num_of_repeats);
+		params[param] |= mc_enc(
+			24, 8, cfg->extracts[i].num_of_byte_masks);
+		params[param] |= mc_enc(32, 4, cfg->extracts[i].type);
+		params[param] = cpu_to_le64(params[param]);
+		param++;
+		for (j = 0; j < 4; j++) {
+			params[param] |= mc_enc(
+				(offset), 8, cfg->extracts[i].masks[j].mask);
+			params[param] |= mc_enc(
+				(offset + 8), 8,
+				cfg->extracts[i].masks[j].offset);
+			offset += 16;
+		}
+		params[param] = cpu_to_le64(params[param]);
+		param++;
+	}
+	return 0;
+}
+
+int dpni_open(struct fsl_mc_io *mc_io, int dpni_id, uint16_t *token)
+{
+	struct mc_command cmd = { 0 };
+	int err;
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_OPEN,
+					  MC_CMD_PRI_LOW, 0);
+	DPNI_CMD_OPEN(cmd, dpni_id);
+
+	/* send command to mc*/
+	err = mc_send_command(mc_io, &cmd);
+	if (err)
+		return err;
+
+	/* retrieve response parameters */
+	*token = MC_CMD_HDR_READ_TOKEN(cmd.header);
+
+	return 0;
+}
+
+int dpni_close(struct fsl_mc_io *mc_io, uint16_t token)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_CLOSE,
+					  MC_CMD_PRI_HIGH, token);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_create(struct fsl_mc_io *mc_io,
+		const struct dpni_cfg *cfg,
+		uint16_t *token)
+{
+	struct mc_command cmd = { 0 };
+	int err;
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_CREATE,
+					  MC_CMD_PRI_LOW,
+					  0);
+	DPNI_CMD_CREATE(cmd, cfg);
+
+	/* send command to mc*/
+	err = mc_send_command(mc_io, &cmd);
+	if (err)
+		return err;
+
+	/* retrieve response parameters */
+	*token = MC_CMD_HDR_READ_TOKEN(cmd.header);
+
+	return 0;
+}
+
+int dpni_destroy(struct fsl_mc_io *mc_io, uint16_t token)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_DESTROY,
+					  MC_CMD_PRI_LOW,
+					  token);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_set_pools(struct fsl_mc_io *mc_io,
+		   uint16_t token,
+		   const struct dpni_pools_cfg *cfg)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_POOLS,
+					  MC_CMD_PRI_LOW,
+					  token);
+	DPNI_CMD_SET_POOLS(cmd, cfg);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_enable(struct fsl_mc_io *mc_io, uint16_t token)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_ENABLE,
+					  MC_CMD_PRI_LOW, token);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_disable(struct fsl_mc_io *mc_io, uint16_t token)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_DISABLE,
+					  MC_CMD_PRI_LOW,
+					  token);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_is_enabled(struct fsl_mc_io *mc_io, uint16_t token, int *en)
+{
+	struct mc_command cmd = { 0 };
+	int err;
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_IS_ENABLED, MC_CMD_PRI_LOW,
+					  token);
+
+	/* send command to mc*/
+	err = mc_send_command(mc_io, &cmd);
+	if (err)
+		return err;
+
+	/* retrieve response parameters */
+	DPNI_RSP_IS_ENABLED(cmd, *en);
+
+	return 0;
+}
+
+int dpni_reset(struct fsl_mc_io *mc_io, uint16_t token)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_RESET,
+					  MC_CMD_PRI_LOW, token);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+
+
+int dpni_set_irq(struct fsl_mc_io *mc_io,
+		 uint16_t token,
+		 uint8_t irq_index,
+		 uint64_t irq_addr,
+		 uint32_t irq_val,
+		 int user_irq_id)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_IRQ,
+					  MC_CMD_PRI_LOW,
+					  token);
+	DPNI_CMD_SET_IRQ(cmd, irq_index, irq_addr, irq_val, user_irq_id);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+
+int dpni_get_irq(struct fsl_mc_io *mc_io,
+		 uint16_t token,
+		 uint8_t irq_index,
+		 int *type,
+		 uint64_t *irq_addr,
+		 uint32_t *irq_val,
+		 int *user_irq_id)
+{
+	struct mc_command cmd = { 0 };
+	int err;
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_GET_IRQ,
+					  MC_CMD_PRI_LOW,
+					  token);
+	DPNI_CMD_GET_IRQ(cmd, irq_index);
+
+	/* send command to mc*/
+	err = mc_send_command(mc_io, &cmd);
+	if (err)
+		return err;
+
+	/* retrieve response parameters */
+	DPNI_RSP_GET_IRQ(cmd, *type, *irq_addr, *irq_val, *user_irq_id);
+
+	return 0;
+}
+
+int dpni_set_irq_enable(struct fsl_mc_io *mc_io,
+			uint16_t token,
+			uint8_t irq_index,
+			uint8_t en)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_IRQ_ENABLE,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_SET_IRQ_ENABLE(cmd, irq_index, en);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_get_irq_enable(struct fsl_mc_io *mc_io,
+			uint16_t token,
+			uint8_t irq_index,
+			uint8_t *en)
+{
+	struct mc_command cmd = { 0 };
+	int err;
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_GET_IRQ_ENABLE,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_GET_IRQ_ENABLE(cmd, irq_index);
+
+	/* send command to mc*/
+	err = mc_send_command(mc_io, &cmd);
+	if (err)
+		return err;
+
+	/* retrieve response parameters */
+	DPNI_RSP_GET_IRQ_ENABLE(cmd, *en);
+
+	return 0;
+}
+
+int dpni_set_irq_mask(struct fsl_mc_io *mc_io,
+		      uint16_t token,
+		      uint8_t irq_index,
+		      uint32_t mask)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_IRQ_MASK,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_SET_IRQ_MASK(cmd, irq_index, mask);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_get_irq_mask(struct fsl_mc_io *mc_io,
+		      uint16_t token,
+		      uint8_t irq_index,
+		      uint32_t *mask)
+{
+	struct mc_command cmd = { 0 };
+	int err;
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_GET_IRQ_MASK,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_GET_IRQ_MASK(cmd, irq_index);
+
+	/* send command to mc*/
+	err = mc_send_command(mc_io, &cmd);
+	if (err)
+		return err;
+
+	/* retrieve response parameters */
+	DPNI_RSP_GET_IRQ_MASK(cmd, *mask);
+
+	return 0;
+}
+
+int dpni_get_irq_status(struct fsl_mc_io *mc_io,
+			uint16_t token,
+			uint8_t irq_index,
+			uint32_t *status)
+{
+	struct mc_command cmd = { 0 };
+	int err;
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_GET_IRQ_STATUS,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_GET_IRQ_STATUS(cmd, irq_index);
+
+	/* send command to mc*/
+	err = mc_send_command(mc_io, &cmd);
+	if (err)
+		return err;
+
+	/* retrieve response parameters */
+	DPNI_RSP_GET_IRQ_STATUS(cmd, *status);
+
+	return 0;
+}
+
+int dpni_clear_irq_status(struct fsl_mc_io *mc_io,
+			  uint16_t token,
+			  uint8_t irq_index,
+			  uint32_t status)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_CLEAR_IRQ_STATUS,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_CLEAR_IRQ_STATUS(cmd, irq_index, status);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_get_attributes(struct fsl_mc_io *mc_io,
+			uint16_t token,
+			struct dpni_attr *attr)
+{
+	struct mc_command cmd = { 0 };
+	int err;
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_GET_ATTR,
+					  MC_CMD_PRI_LOW,
+					  token);
+
+	/* send command to mc*/
+	err = mc_send_command(mc_io, &cmd);
+	if (err)
+		return err;
+
+	/* retrieve response parameters */
+	DPNI_RSP_GET_ATTR(cmd, attr);
+
+	return 0;
+}
+
+int dpni_set_errors_behavior(struct fsl_mc_io *mc_io,
+			     uint16_t token,
+			      struct dpni_error_cfg *cfg)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_ERRORS_BEHAVIOR,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_SET_ERRORS_BEHAVIOR(cmd, cfg);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_get_rx_buffer_layout(struct fsl_mc_io *mc_io,
+			      uint16_t token,
+			      struct dpni_buffer_layout *layout)
+{
+	struct mc_command cmd = { 0 };
+	int err;
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_GET_RX_BUFFER_LAYOUT,
+					  MC_CMD_PRI_LOW, token);
+
+	/* send command to mc*/
+	err = mc_send_command(mc_io, &cmd);
+	if (err)
+		return err;
+
+	/* retrieve response parameters */
+	DPNI_RSP_GET_RX_BUFFER_LAYOUT(cmd, layout);
+
+	return 0;
+}
+
+int dpni_set_rx_buffer_layout(struct fsl_mc_io *mc_io,
+			      uint16_t token,
+			      const struct dpni_buffer_layout *layout)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_RX_BUFFER_LAYOUT,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_SET_RX_BUFFER_LAYOUT(cmd, layout);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_get_tx_buffer_layout(struct fsl_mc_io *mc_io,
+			      uint16_t token,
+			      struct dpni_buffer_layout *layout)
+{
+	struct mc_command cmd = { 0 };
+	int err;
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_GET_TX_BUFFER_LAYOUT,
+					  MC_CMD_PRI_LOW, token);
+
+	/* send command to mc*/
+	err = mc_send_command(mc_io, &cmd);
+	if (err)
+		return err;
+
+	/* retrieve response parameters */
+	DPNI_RSP_GET_TX_BUFFER_LAYOUT(cmd, layout);
+
+	return 0;
+}
+
+int dpni_set_tx_buffer_layout(struct fsl_mc_io *mc_io,
+			      uint16_t token,
+			      const struct dpni_buffer_layout *layout)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_TX_BUFFER_LAYOUT,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_SET_TX_BUFFER_LAYOUT(cmd, layout);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_get_tx_conf_buffer_layout(struct fsl_mc_io *mc_io,
+				   uint16_t token,
+				   struct dpni_buffer_layout *layout)
+{
+	struct mc_command cmd = { 0 };
+	int err;
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_GET_TX_CONF_BUFFER_LAYOUT,
+					  MC_CMD_PRI_LOW, token);
+
+	/* send command to mc*/
+	err = mc_send_command(mc_io, &cmd);
+	if (err)
+		return err;
+
+	/* retrieve response parameters */
+	DPNI_RSP_GET_TX_CONF_BUFFER_LAYOUT(cmd, layout);
+
+	return 0;
+}
+
+int dpni_set_tx_conf_buffer_layout(struct fsl_mc_io *mc_io,
+				   uint16_t token,
+				   const struct dpni_buffer_layout *layout)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_TX_CONF_BUFFER_LAYOUT,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_SET_TX_CONF_BUFFER_LAYOUT(cmd, layout);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_get_l3_chksum_validation(struct fsl_mc_io *mc_io,
+				  uint16_t token,
+				  int *en)
+{
+	struct mc_command cmd = { 0 };
+	int err;
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_GET_L3_CHKSUM_VALIDATION,
+					  MC_CMD_PRI_LOW,
+					  token);
+
+	/* send command to mc*/
+	err = mc_send_command(mc_io, &cmd);
+	if (err)
+		return err;
+
+	/* retrieve response parameters */
+	DPNI_RSP_GET_L3_CHKSUM_VALIDATION(cmd, *en);
+
+	return 0;
+}
+
+int dpni_set_l3_chksum_validation(struct fsl_mc_io *mc_io,
+				  uint16_t token,
+				  int en)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_L3_CHKSUM_VALIDATION,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_SET_L3_CHKSUM_VALIDATION(cmd, en);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_get_l4_chksum_validation(struct fsl_mc_io *mc_io,
+				  uint16_t token,
+				  int *en)
+{
+	struct mc_command cmd = { 0 };
+	int err;
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_GET_L4_CHKSUM_VALIDATION,
+					  MC_CMD_PRI_LOW, token);
+
+	/* send command to mc*/
+	err = mc_send_command(mc_io, &cmd);
+	if (err)
+		return err;
+
+	/* retrieve response parameters */
+	DPNI_RSP_GET_L4_CHKSUM_VALIDATION(cmd, *en);
+
+	return 0;
+}
+
+int dpni_set_l4_chksum_validation(struct fsl_mc_io *mc_io,
+				  uint16_t token,
+				  int en)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_L4_CHKSUM_VALIDATION,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_SET_L4_CHKSUM_VALIDATION(cmd, en);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_get_qdid(struct fsl_mc_io *mc_io, uint16_t token, uint16_t *qdid)
+{
+	struct mc_command cmd = { 0 };
+	int err;
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_GET_QDID,
+					  MC_CMD_PRI_LOW,
+					  token);
+
+	/* send command to mc*/
+	err = mc_send_command(mc_io, &cmd);
+	if (err)
+		return err;
+
+	/* retrieve response parameters */
+	DPNI_RSP_GET_QDID(cmd, *qdid);
+
+	return 0;
+}
+
+int dpni_get_spid(struct fsl_mc_io *mc_io, uint16_t token, uint16_t *spid)
+{
+	struct mc_command cmd = { 0 };
+	int err;
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_GET_SPID,
+					  MC_CMD_PRI_LOW,
+					  token);
+
+	/* send command to mc*/
+	err = mc_send_command(mc_io, &cmd);
+	if (err)
+		return err;
+
+	/* retrieve response parameters */
+	DPNI_RSP_GET_SPID(cmd, *spid);
+
+	return 0;
+}
+
+int dpni_get_tx_data_offset(struct fsl_mc_io *mc_io,
+			    uint16_t token,
+			    uint16_t *data_offset)
+{
+	struct mc_command cmd = { 0 };
+	int err;
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_GET_TX_DATA_OFFSET,
+					  MC_CMD_PRI_LOW, token);
+
+	/* send command to mc*/
+	err = mc_send_command(mc_io, &cmd);
+	if (err)
+		return err;
+
+	/* retrieve response parameters */
+	DPNI_RSP_GET_TX_DATA_OFFSET(cmd, *data_offset);
+
+	return 0;
+}
+
+int dpni_get_counter(struct fsl_mc_io *mc_io,
+		     uint16_t token,
+		     enum dpni_counter counter,
+		     uint64_t *value)
+{
+	struct mc_command cmd = { 0 };
+	int err;
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_GET_COUNTER,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_GET_COUNTER(cmd, counter);
+
+	/* send command to mc*/
+	err = mc_send_command(mc_io, &cmd);
+	if (err)
+		return err;
+
+	/* retrieve response parameters */
+	DPNI_RSP_GET_COUNTER(cmd, *value);
+
+	return 0;
+}
+
+int dpni_set_counter(struct fsl_mc_io *mc_io,
+		     uint16_t token,
+		     enum dpni_counter counter,
+		     uint64_t value)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_COUNTER,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_SET_COUNTER(cmd, counter, value);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_set_link_cfg(struct fsl_mc_io *mc_io,
+		      uint16_t token,
+		     const struct dpni_link_cfg *cfg)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_LINK_CFG,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_SET_LINK_CFG(cmd, cfg);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_get_link_state(struct fsl_mc_io *mc_io,
+			uint16_t token,
+			struct dpni_link_state *state)
+{
+	struct mc_command cmd = { 0 };
+	int err;
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_GET_LINK_STATE,
+					  MC_CMD_PRI_LOW, token);
+
+	/* send command to mc*/
+	err = mc_send_command(mc_io, &cmd);
+	if (err)
+		return err;
+
+	/* retrieve response parameters */
+	DPNI_RSP_GET_LINK_STATE(cmd, state);
+
+	return 0;
+}
+
+int dpni_set_max_frame_length(struct fsl_mc_io *mc_io, uint16_t token,
+			      uint16_t max_frame_length)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_MAX_FRAME_LENGTH,
+					  MC_CMD_PRI_LOW,
+					  token);
+	DPNI_CMD_SET_MAX_FRAME_LENGTH(cmd, max_frame_length);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_get_max_frame_length(struct fsl_mc_io *mc_io, uint16_t token,
+			      uint16_t *max_frame_length)
+{
+	struct mc_command cmd = { 0 };
+	int err;
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_GET_MAX_FRAME_LENGTH,
+					  MC_CMD_PRI_LOW,
+					  token);
+
+	/* send command to mc*/
+	err = mc_send_command(mc_io, &cmd);
+	if (err)
+		return err;
+
+	/* retrieve response parameters */
+	DPNI_RSP_GET_MAX_FRAME_LENGTH(cmd, *max_frame_length);
+
+	return 0;
+}
+
+int dpni_set_mtu(struct fsl_mc_io *mc_io, uint16_t token, uint16_t mtu)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_MTU,
+					  MC_CMD_PRI_LOW,
+					  token);
+	DPNI_CMD_SET_MTU(cmd, mtu);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_get_mtu(struct fsl_mc_io *mc_io, uint16_t token, uint16_t *mtu)
+{
+	struct mc_command cmd = { 0 };
+	int err;
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_GET_MTU,
+					  MC_CMD_PRI_LOW,
+					  token);
+
+	/* send command to mc*/
+	err = mc_send_command(mc_io, &cmd);
+	if (err)
+		return err;
+
+	/* retrieve response parameters */
+	DPNI_RSP_GET_MTU(cmd, *mtu);
+
+	return 0;
+}
+
+int dpni_set_multicast_promisc(struct fsl_mc_io *mc_io, uint16_t token, int en)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_MCAST_PROMISC,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_SET_MULTICAST_PROMISC(cmd, en);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_get_multicast_promisc(struct fsl_mc_io *mc_io, uint16_t token, int *en)
+{
+	struct mc_command cmd = { 0 };
+	int err;
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_GET_MCAST_PROMISC,
+					  MC_CMD_PRI_LOW, token);
+
+	/* send command to mc*/
+	err = mc_send_command(mc_io, &cmd);
+	if (err)
+		return err;
+
+	/* retrieve response parameters */
+	DPNI_RSP_GET_MULTICAST_PROMISC(cmd, *en);
+
+	return 0;
+}
+
+int dpni_set_unicast_promisc(struct fsl_mc_io *mc_io, uint16_t token, int en)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_UNICAST_PROMISC,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_SET_UNICAST_PROMISC(cmd, en);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_get_unicast_promisc(struct fsl_mc_io *mc_io, uint16_t token, int *en)
+{
+	struct mc_command cmd = { 0 };
+	int err;
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_GET_UNICAST_PROMISC,
+					  MC_CMD_PRI_LOW, token);
+
+	/* send command to mc*/
+	err = mc_send_command(mc_io, &cmd);
+	if (err)
+		return err;
+
+	/* retrieve response parameters */
+	DPNI_RSP_GET_UNICAST_PROMISC(cmd, *en);
+
+	return 0;
+}
+
+int dpni_set_primary_mac_addr(struct fsl_mc_io *mc_io,
+			      uint16_t token,
+			      const uint8_t mac_addr[6])
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_PRIM_MAC,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_SET_PRIMARY_MAC_ADDR(cmd, mac_addr);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_get_primary_mac_addr(struct fsl_mc_io *mc_io,
+			      uint16_t token,
+			      uint8_t mac_addr[6])
+{
+	struct mc_command cmd = { 0 };
+	int err;
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_GET_PRIM_MAC,
+					  MC_CMD_PRI_LOW, token);
+
+	/* send command to mc*/
+	err = mc_send_command(mc_io, &cmd);
+	if (err)
+		return err;
+
+	/* retrieve response parameters */
+	DPNI_RSP_GET_PRIMARY_MAC_ADDR(cmd, mac_addr);
+
+	return 0;
+}
+
+int dpni_add_mac_addr(struct fsl_mc_io *mc_io,
+		      uint16_t token,
+		      const uint8_t mac_addr[6])
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_ADD_MAC_ADDR,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_ADD_MAC_ADDR(cmd, mac_addr);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_remove_mac_addr(struct fsl_mc_io *mc_io,
+			 uint16_t token,
+			 const uint8_t mac_addr[6])
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_REMOVE_MAC_ADDR,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_REMOVE_MAC_ADDR(cmd, mac_addr);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_clear_mac_filters(struct fsl_mc_io *mc_io, uint16_t token, int unicast,
+			   int multicast)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_CLR_MAC_FILTERS,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_CLEAR_MAC_FILTERS(cmd, unicast, multicast);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_set_vlan_filters(struct fsl_mc_io *mc_io, uint16_t token, int en)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_VLAN_FILTERS,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_SET_VLAN_FILTERS(cmd, en);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_add_vlan_id(struct fsl_mc_io *mc_io, uint16_t token, uint16_t vlan_id)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_ADD_VLAN_ID,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_ADD_VLAN_ID(cmd, vlan_id);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_remove_vlan_id(struct fsl_mc_io *mc_io,
+			uint16_t token,
+			uint16_t vlan_id)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_REMOVE_VLAN_ID,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_REMOVE_VLAN_ID(cmd, vlan_id);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_clear_vlan_filters(struct fsl_mc_io *mc_io, uint16_t token)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_CLR_VLAN_FILTERS,
+					  MC_CMD_PRI_LOW, token);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_set_tx_tc(struct fsl_mc_io *mc_io,
+		   uint16_t token,
+		   uint8_t tc_id,
+		   const struct dpni_tx_tc_cfg *cfg)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_TX_TC,
+					  MC_CMD_PRI_LOW,
+					  token);
+	DPNI_CMD_SET_TX_TC(cmd, tc_id, cfg);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_set_rx_tc_dist(struct fsl_mc_io *mc_io,
+			uint16_t token,
+			uint8_t tc_id,
+			const struct dpni_rx_tc_dist_cfg *cfg)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_RX_TC_DIST,
+					  MC_CMD_PRI_LOW,
+					  token);
+	DPNI_CMD_SET_RX_TC_DIST(cmd, tc_id, cfg);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_set_tx_flow(struct fsl_mc_io *mc_io,
+		     uint16_t token,
+		     uint16_t *flow_id,
+		     const struct dpni_tx_flow_cfg *cfg)
+{
+	struct mc_command cmd = { 0 };
+	int err;
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_TX_FLOW,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_SET_TX_FLOW(cmd, *flow_id, cfg);
+
+	/* send command to mc*/
+	err = mc_send_command(mc_io, &cmd);
+	if (err)
+		return err;
+
+	/* retrieve response parameters */
+	DPNI_RSP_SET_TX_FLOW(cmd, *flow_id);
+
+	return 0;
+}
+
+int dpni_get_tx_flow(struct fsl_mc_io *mc_io,
+		     uint16_t token,
+		     uint16_t flow_id,
+		     struct dpni_tx_flow_attr *attr)
+{
+	struct mc_command cmd = { 0 };
+	int err;
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_GET_TX_FLOW,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_GET_TX_FLOW(cmd, flow_id);
+
+	/* send command to mc*/
+	err = mc_send_command(mc_io, &cmd);
+	if (err)
+		return err;
+
+	/* retrieve response parameters */
+	DPNI_RSP_GET_TX_FLOW(cmd, attr);
+
+	return 0;
+}
+
+int dpni_set_rx_flow(struct fsl_mc_io *mc_io,
+		     uint16_t token,
+		     uint8_t tc_id,
+		     uint16_t flow_id,
+		     const struct dpni_queue_cfg *cfg)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_RX_FLOW,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_SET_RX_FLOW(cmd, tc_id, flow_id, cfg);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_get_rx_flow(struct fsl_mc_io *mc_io,
+		     uint16_t token,
+		     uint8_t tc_id,
+		     uint16_t flow_id,
+		     struct dpni_queue_attr *attr)
+{
+	struct mc_command cmd = { 0 };
+	int err;
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_GET_RX_FLOW,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_GET_RX_FLOW(cmd, tc_id, flow_id);
+
+	/* send command to mc*/
+	err = mc_send_command(mc_io, &cmd);
+	if (err)
+		return err;
+
+	/* retrieve response parameters */
+	DPNI_RSP_GET_RX_FLOW(cmd, attr);
+
+	return 0;
+}
+
+
+int dpni_set_rx_err_queue(struct fsl_mc_io *mc_io, uint16_t token,
+			  const struct dpni_queue_cfg *cfg)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_RX_ERR_QUEUE,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_SET_RX_ERR_QUEUE(cmd, cfg);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_get_rx_err_queue(struct fsl_mc_io *mc_io, uint16_t token,
+			  struct dpni_queue_attr *attr)
+{
+	struct mc_command cmd = { 0 };
+	int err;
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_GET_RX_ERR_QUEUE,
+					  MC_CMD_PRI_LOW,
+					  token);
+
+	/* send command to mc*/
+	err = mc_send_command(mc_io, &cmd);
+	if (err)
+		return err;
+
+	/* retrieve response parameters */
+	DPNI_RSP_GET_RX_ERR_QUEUE(cmd, attr);
+
+	return 0;
+}
+
+int dpni_set_tx_conf_err_queue(struct fsl_mc_io *mc_io, uint16_t token,
+			       const struct dpni_queue_cfg *cfg)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_TX_CONF_ERR_QUEUE,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_SET_TX_CONF_ERR_QUEUE(cmd, cfg);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_get_tx_conf_err_queue(struct fsl_mc_io *mc_io, uint16_t token,
+			       struct dpni_queue_attr *attr)
+{
+	struct mc_command cmd = { 0 };
+	int err;
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_GET_TX_CONF_ERR_QUEUE,
+					  MC_CMD_PRI_LOW,
+					  token);
+
+	/* send command to mc*/
+	err = mc_send_command(mc_io, &cmd);
+	if (err)
+		return err;
+
+	/* retrieve response parameters */
+	DPNI_RSP_GET_TX_CONF_ERR_QUEUE(cmd, attr);
+
+	return 0;
+}
+
+
+int dpni_set_qos_table(struct fsl_mc_io *mc_io,
+		       uint16_t token,
+		       const struct dpni_qos_tbl_cfg *cfg)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_QOS_TBL,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_SET_QOS_TABLE(cmd, cfg);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_add_qos_entry(struct fsl_mc_io *mc_io,
+		       uint16_t token,
+		       const struct dpni_rule_cfg *cfg,
+		       uint8_t tc_id)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_ADD_QOS_ENT,
+					  MC_CMD_PRI_LOW, token);
+
+	DPNI_CMD_ADD_QOS_ENTRY(cmd, cfg, tc_id);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_remove_qos_entry(struct fsl_mc_io *mc_io,
+			  uint16_t token,
+			  const struct dpni_rule_cfg *cfg)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_REMOVE_QOS_ENT,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_REMOVE_QOS_ENTRY(cmd, cfg);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_clear_qos_table(struct fsl_mc_io *mc_io, uint16_t token)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_CLR_QOS_TBL,
+					  MC_CMD_PRI_LOW, token);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_add_fs_entry(struct fsl_mc_io *mc_io,
+		      uint16_t token,
+		      uint8_t tc_id,
+		      const struct dpni_rule_cfg *cfg,
+		      uint16_t flow_id)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_ADD_FS_ENT,
+					  MC_CMD_PRI_LOW,
+					  token);
+	DPNI_CMD_ADD_FS_ENTRY(cmd, tc_id, cfg, flow_id);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_remove_fs_entry(struct fsl_mc_io *mc_io,
+			 uint16_t token,
+			 uint8_t tc_id,
+			 const struct dpni_rule_cfg *cfg)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_REMOVE_FS_ENT,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_REMOVE_FS_ENTRY(cmd, tc_id, cfg);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_clear_fs_entries(struct fsl_mc_io *mc_io, uint16_t token,
+			  uint8_t tc_id)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_CLR_FS_ENT,
+					  MC_CMD_PRI_LOW,
+					  token);
+	DPNI_CMD_CLEAR_FS_ENTRIES(cmd, tc_id);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_set_vlan_insertion(struct fsl_mc_io *mc_io, uint16_t token, int en)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_VLAN_INSERTION,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_SET_VLAN_INSERTION(cmd, en);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_set_vlan_removal(struct fsl_mc_io *mc_io, uint16_t token, int en)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_VLAN_REMOVAL,
+					  MC_CMD_PRI_LOW, token);
+	DPNI_CMD_SET_VLAN_REMOVAL(cmd, en);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_set_ipr(struct fsl_mc_io *mc_io, uint16_t token, int en)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_IPR,
+					  MC_CMD_PRI_LOW,
+					  token);
+	DPNI_CMD_SET_IPR(cmd, en);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_set_ipf(struct fsl_mc_io *mc_io, uint16_t token, int en)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_IPF,
+					  MC_CMD_PRI_LOW,
+					  token);
+	DPNI_CMD_SET_IPF(cmd, en);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+int dpni_set_rx_tc_policing(struct fsl_mc_io	*mc_io,
+			    uint16_t		token,
+			    uint8_t		tc_id,
+			    const struct dpni_rx_tc_policing_cfg *cfg)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_RX_TC_POLICING,
+					  MC_CMD_PRI_LOW,
+					  token);
+	DPNI_CMD_SET_RX_TC_POLICING(cmd, tc_id, cfg);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
+
+void dpni_prepare_rx_tc_early_drop(const struct dpni_rx_tc_early_drop_cfg *cfg,
+				   uint8_t *early_drop_buf)
+{
+	uint64_t *ext_params = (uint64_t *)early_drop_buf;
+
+	DPNI_EXT_SET_RX_TC_EARLY_DROP(ext_params, cfg);
+}
+
+int dpni_set_rx_tc_early_drop(struct fsl_mc_io	*mc_io,
+			      uint16_t		token,
+			    uint8_t		tc_id,
+			    uint64_t		early_drop_iova)
+{
+	struct mc_command cmd = { 0 };
+
+	/* prepare command */
+	cmd.header = mc_encode_cmd_header(DPNI_CMDID_SET_RX_TC_EARLY_DROP,
+					  MC_CMD_PRI_LOW,
+					  token);
+	DPNI_CMD_SET_RX_TC_EARLY_DROP(cmd, tc_id, early_drop_iova);
+
+	/* send command to mc*/
+	return mc_send_command(mc_io, &cmd);
+}
diff --git a/drivers/staging/fsl-dpaa2/ethernet/ldpaa_eth.c b/drivers/staging/fsl-dpaa2/ethernet/ldpaa_eth.c
new file mode 100644
index 0000000..a0e48b8
--- /dev/null
+++ b/drivers/staging/fsl-dpaa2/ethernet/ldpaa_eth.c
@@ -0,0 +1,2124 @@
+/* Copyright 2014 Freescale Semiconductor Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *	 notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *	 notice, this list of conditions and the following disclaimer in the
+ *	 documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *	 names of its contributors may be used to endorse or promote products
+ *	 derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/etherdevice.h>
+#include <linux/of_net.h>
+#include <linux/interrupt.h>
+#include <linux/debugfs.h>
+
+#include "../../fsl-mc/include/mc.h"
+#include "../../fsl-mc/include/mc-sys.h" /* FSL_MC_IO_ATOMIC_CONTEXT_PORTAL */
+#include "ldpaa_eth.h"
+
+/* CREATE_TRACE_POINTS only needs to be defined once. Other dpa files
+ * using trace events only need to #include <trace/events/sched.h>
+ */
+#define CREATE_TRACE_POINTS
+#include "ldpaa_eth_trace.h"
+
+#define LDPAA_ETH_DESCRIPTION "Freescale DPAA Ethernet Driver"
+
+MODULE_LICENSE("Dual BSD/GPL");
+MODULE_AUTHOR("Freescale Semiconductor, Inc");
+MODULE_DESCRIPTION(LDPAA_ETH_DESCRIPTION);
+
+static uint8_t debug = -1;
+module_param(debug, byte, S_IRUGO);
+MODULE_PARM_DESC(debug, "Module/Driver verbosity level");
+
+static int ldpaa_dpbp_refill(struct ldpaa_eth_priv *priv, uint16_t bpid);
+
+
+static void ldpaa_eth_rx_csum(struct ldpaa_eth_priv *priv,
+			      uint32_t fd_status,
+			      struct sk_buff *skb)
+{
+	skb_checksum_none_assert(skb);
+
+	/* HW checksum validation is disabled, nothing to do here */
+	if (!(priv->net_dev->features & NETIF_F_RXCSUM))
+		return;
+
+	/* Read checksum validation bits */
+	if (!((fd_status & LDPAA_ETH_FAS_L3CV) &&
+	      (fd_status & LDPAA_ETH_FAS_L4CV)))
+		return;
+
+	/* Inform the stack there's no need to compute L3/L4 csum anymore */
+	skb->ip_summed = CHECKSUM_UNNECESSARY;
+}
+
+/* Free a received FD.
+ * Not to be used for Tx conf FDs or on any other paths.
+ */
+static void ldpaa_eth_free_rx_fd(struct ldpaa_eth_priv *priv,
+				 const struct dpaa_fd *fd)
+{
+	struct device *dev = priv->net_dev->dev.parent;
+	dma_addr_t addr = ldpaa_fd_get_addr(fd);
+	void *vaddr;
+	uint8_t fd_format = ldpaa_fd_get_format(fd);
+
+	dma_unmap_single(dev, addr, LDPAA_ETH_RX_BUFFER_SIZE, DMA_FROM_DEVICE);
+	vaddr = phys_to_virt(addr);
+
+	if (fd_format == dpaa_fd_sg) {
+		struct dpaa_sg_entry *sgt = vaddr + ldpaa_fd_get_offset(fd);
+		void *sg_vaddr;
+		int i;
+
+		for (i = 0; i < LDPAA_ETH_MAX_SG_ENTRIES; i++) {
+			addr = ldpaa_sg_get_addr(&sgt[i]);
+			dma_unmap_single(dev, addr, LDPAA_ETH_RX_BUFFER_SIZE,
+					 DMA_FROM_DEVICE);
+
+			sg_vaddr = phys_to_virt(addr);
+			put_page(virt_to_head_page(sg_vaddr));
+
+			if (ldpaa_sg_is_final(&sgt[i]))
+				break;
+		}
+	}
+
+	put_page(virt_to_head_page(vaddr));
+}
+
+/* Build a linear skb based on a single-buffer frame descriptor */
+static struct sk_buff *ldpaa_eth_build_linear_skb(struct ldpaa_eth_priv *priv,
+						  const struct dpaa_fd *fd,
+						  void *fd_vaddr)
+{
+	struct sk_buff *skb = NULL;
+	uint16_t fd_offset = ldpaa_fd_get_offset(fd);
+	uint32_t fd_length = ldpaa_fd_get_len(fd);
+	int *count;
+
+	skb = build_skb(fd_vaddr, LDPAA_ETH_RX_BUFFER_SIZE +
+			SKB_DATA_ALIGN(sizeof(struct skb_shared_info)));
+	if (unlikely(!skb)) {
+		netdev_err(priv->net_dev, "build_skb() failed\n");
+		return NULL;
+	}
+
+	skb_reserve(skb, fd_offset);
+	skb_put(skb, fd_length);
+
+	count = this_cpu_ptr(priv->buf_count);
+	(*count)--;
+
+	return skb;
+}
+
+
+/* Build a non linear (fragmented) skb based on a S/G table */
+static struct sk_buff *ldpaa_eth_build_frag_skb(struct ldpaa_eth_priv *priv,
+						const struct dpaa_sg_entry *sgt)
+{
+	struct sk_buff *skb = NULL;
+	struct device *dev = priv->net_dev->dev.parent;
+	void *sg_vaddr;
+	dma_addr_t sg_addr;
+	uint16_t sg_offset;
+	uint32_t sg_length;
+	struct page *page, *head_page;
+	int page_offset;
+	int *count;
+	int i;
+
+	for (i = 0; i < LDPAA_ETH_MAX_SG_ENTRIES; i++) {
+		const struct dpaa_sg_entry *sge = &sgt[i];
+
+		/* We don't support anything else yet! */
+		BUG_ON(ldpaa_sg_get_format(sge) != dpaa_sg_single);
+
+		/* Get the address, offset and length from the S/G entry */
+		sg_addr = ldpaa_sg_get_addr(sge);
+		dma_unmap_single(dev, sg_addr, LDPAA_ETH_RX_BUFFER_SIZE,
+				 DMA_FROM_DEVICE);
+		if (unlikely(dma_mapping_error(dev, sg_addr))) {
+			netdev_err(priv->net_dev, "DMA unmap failed\n");
+			return NULL;
+		}
+		sg_vaddr = phys_to_virt(sg_addr);
+		sg_length = ldpaa_sg_get_len(sge);
+
+		if (i == 0) {
+			/* We build the skb around the first data buffer */
+			skb = build_skb(sg_vaddr, LDPAA_ETH_RX_BUFFER_SIZE +
+				SKB_DATA_ALIGN(sizeof(struct skb_shared_info)));
+			if (unlikely(!skb)) {
+				netdev_err(priv->net_dev, "build_skb failed\n");
+				return NULL;
+			}
+			sg_offset = ldpaa_sg_get_offset(sge);
+			skb_reserve(skb, sg_offset);
+			skb_put(skb, sg_length);
+		} else {
+			/* Subsequent data in SGEntries are stored at
+			 * offset 0 in their buffers, we don't need to
+			 * compute sg_offset.
+			 */
+			WARN_ONCE(ldpaa_sg_get_offset(sge) != 0,
+				  "Non-zero offset in SGE[%d]!\n", i);
+
+			/* Rest of the data buffers are stored as skb frags */
+			page = virt_to_page(sg_vaddr);
+			head_page = virt_to_head_page(sg_vaddr);
+
+			/* Offset in page (which may be compound) */
+			page_offset = ((unsigned long)sg_vaddr &
+				(PAGE_SIZE - 1)) +
+				(page_address(page) - page_address(head_page));
+
+			skb_add_rx_frag(skb, i - 1, head_page, page_offset,
+					sg_length, LDPAA_ETH_RX_BUFFER_SIZE);
+		}
+
+		if (ldpaa_sg_is_final(sge))
+			break;
+	}
+
+	/* Count all data buffers + sgt buffer */
+	count = this_cpu_ptr(priv->buf_count);
+	*count -= i + 2;
+
+	return skb;
+}
+
+static void ldpaa_eth_rx(struct ldpaa_eth_priv *priv,
+			 const struct dpaa_fd *fd)
+{
+	dma_addr_t addr = ldpaa_fd_get_addr(fd);
+	uint8_t fd_format = ldpaa_fd_get_format(fd);
+	void *vaddr;
+	struct sk_buff *skb;
+	struct rtnl_link_stats64 *percpu_stats;
+	struct ldpaa_eth_stats *percpu_extras;
+	struct device *dev = priv->net_dev->dev.parent;
+	struct ldpaa_fas *fas;
+	uint32_t status = 0;
+
+	/* Tracing point */
+	trace_ldpaa_rx_fd(priv->net_dev, fd);
+
+	/* Refill pool if appropriate */
+	ldpaa_dpbp_refill(priv, priv->dpbp_attrs.bpid);
+
+	dma_unmap_single(dev, addr, LDPAA_ETH_RX_BUFFER_SIZE, DMA_FROM_DEVICE);
+	vaddr = phys_to_virt(addr);
+
+	percpu_stats = this_cpu_ptr(priv->percpu_stats);
+	percpu_extras = this_cpu_ptr(priv->percpu_extras);
+
+	if (fd->simple.frc & LDPAA_FD_FRC_FASV) {
+		/* Read the frame annotation status word and check for errors */
+		/* TODO ideally, we'd have a struct describing the HW FA */
+		fas = (struct ldpaa_fas *)
+				(vaddr + priv->buf_layout.private_data_size);
+		status = le32_to_cpu(fas->status);
+		if (status & LDPAA_ETH_RX_ERR_MASK) {
+			dev_err(dev, "Rx frame error(s): 0x%08x\n",
+				status & LDPAA_ETH_RX_ERR_MASK);
+			/* TODO when we grow up and get to run in Rx softirq,
+			* we won't need this. Besides, on RT we'd only need
+			* migrate_disable().
+			*/
+			percpu_stats->rx_errors++;
+			ldpaa_eth_free_rx_fd(priv, fd);
+			return;
+		} else if (status & LDPAA_ETH_RX_UNSUPP_MASK) {
+			/* TODO safety net; to be removed as we support more and
+			* more of these, e.g. rx multicast
+			*/
+			netdev_info(priv->net_dev,
+				    "Unsupported feature in bitmask: 0x%08x\n",
+				    status & LDPAA_ETH_RX_UNSUPP_MASK);
+		}
+	}
+
+	if (fd_format == dpaa_fd_single) {
+		skb = ldpaa_eth_build_linear_skb(priv, fd, vaddr);
+	} else if (fd_format == dpaa_fd_sg) {
+		const struct dpaa_sg_entry *sgt =
+				vaddr + ldpaa_fd_get_offset(fd);
+		skb = ldpaa_eth_build_frag_skb(priv, sgt);
+		put_page(virt_to_head_page(vaddr));
+		percpu_extras->rx_sg_frames++;
+		percpu_extras->rx_sg_bytes += skb->len;
+	} else {
+		/* We don't support any other format */
+		netdev_err(priv->net_dev, "Received invalid frame format\n");
+		BUG();
+	}
+
+	if (unlikely(!skb)) {
+		netdev_err(priv->net_dev, "error building skb\n");
+		goto err_build_skb;
+	}
+
+	skb->protocol = eth_type_trans(skb, priv->net_dev);
+
+	/* Check if we need to validate the L4 csum */
+	if (fd->simple.frc & LDPAA_FD_FRC_FASV)
+		ldpaa_eth_rx_csum(priv, status, skb);
+
+	if (unlikely(netif_rx(skb) == NET_RX_DROP))
+		/* Nothing to do here, the stack updates the dropped counter */
+		return;
+
+	percpu_stats->rx_packets++;
+	percpu_stats->rx_bytes += skb->len;
+	return;
+
+err_build_skb:
+	ldpaa_eth_free_rx_fd(priv, fd);
+	percpu_stats->rx_dropped++;
+}
+
+/* Consume all frames pull-dequeued into the store. This is the simplest way to
+ * make sure we don't accidentally issue another volatile dequeue which would
+ * overwrite (leak) frames already in the store.
+ *
+ * Observance of NAPI budget is not our concern, leaving that to the caller.
+ */
+static int ldpaa_eth_store_consume(struct ldpaa_eth_fq *fq)
+{
+	struct ldpaa_eth_priv *priv = fq->netdev_priv;
+	struct ldpaa_dq *dq;
+	const struct dpaa_fd *fd;
+	int cleaned = 0;
+	int is_last;
+
+	do {
+		dq = dpaa_io_store_next(fq->ring.store, &is_last);
+		if (unlikely(!dq)) {
+			if (unlikely(!is_last)) {
+				netdev_err(priv->net_dev,
+					   "FQID %d returned no valid frames!\n",
+					   fq->fqid);
+			}
+			fq->has_frames = false;
+			/* TODO add a ethtool counter for empty dequeues */
+			break;
+		}
+
+		/* Obtain FD and process it */
+		fd = ldpaa_dq_fd(dq);
+		fq->consume(priv, fd);
+		cleaned++;
+	} while (!is_last);
+
+	return cleaned;
+}
+
+static int ldpaa_eth_build_sg_fd(struct ldpaa_eth_priv *priv,
+				 struct sk_buff *skb,
+				 struct dpaa_fd *fd)
+{
+	struct device *dev = priv->net_dev->dev.parent;
+	void *sgt_buf = NULL;
+	dma_addr_t addr;
+	skb_frag_t *frag;
+	int nr_frags = skb_shinfo(skb)->nr_frags;
+	struct dpaa_sg_entry *sgt;
+	int i = 0, j, err;
+	int sgt_buf_size;
+	struct sk_buff **skbh;
+
+	sgt_buf_size = priv->tx_data_offset +
+		       sizeof(struct dpaa_sg_entry) * (1 + nr_frags);
+	sgt_buf = kzalloc(sgt_buf_size + LDPAA_ETH_BUF_ALIGN, GFP_ATOMIC);
+	if (unlikely(!sgt_buf)) {
+		netdev_err(priv->net_dev, "failed to allocate SGT buffer\n");
+		return -ENOMEM;
+	}
+
+	sgt_buf = PTR_ALIGN(sgt_buf, LDPAA_ETH_BUF_ALIGN);
+
+	/* PTA from egress side is passed as is to the confirmation side so
+	 * we need to clear some fields here in order to find consistent values
+	 * on TX confirmation. We are clearing FAS (Frame Annotation Status)
+	 * field here.
+	 */
+	memset(sgt_buf + priv->buf_layout.private_data_size, 0, 8);
+
+	/* Store the skb backpointer in the SGT buffer */
+	skbh = (struct sk_buff **)sgt_buf;
+	*skbh = skb;
+
+	sgt = (struct dpaa_sg_entry *)(sgt_buf + priv->tx_data_offset);
+
+	/* First S/G buffer built from linear part of skb */
+	ldpaa_sg_set_len(&sgt[0], skb_headlen(skb));
+	ldpaa_sg_set_offset(&sgt[0], (u16)skb_headroom(skb));
+	ldpaa_sg_set_bpid(&sgt[0], priv->dpbp_attrs.bpid);
+	ldpaa_sg_set_format(&sgt[0], dpaa_sg_single);
+
+	addr = dma_map_single(dev, skb->head, skb_tail_pointer(skb) - skb->head,
+			      DMA_TO_DEVICE);
+	if (unlikely(dma_mapping_error(dev, addr))) {
+		netdev_err(priv->net_dev, "dma_map_single() failed\n");
+		err = -EINVAL;
+		goto map0_failed;
+	}
+	ldpaa_sg_set_addr(&sgt[0], addr);
+
+	/* The rest of the S/G buffers built from skb frags */
+	for (i = 1; i <= nr_frags; i++) {
+		frag = &skb_shinfo(skb)->frags[i-1];
+
+		ldpaa_sg_set_bpid(&sgt[i], priv->dpbp_attrs.bpid);
+		ldpaa_sg_set_format(&sgt[0], dpaa_sg_single);
+		ldpaa_sg_set_offset(&sgt[i], 0);
+		ldpaa_sg_set_len(&sgt[i], frag->size);
+
+		addr = skb_frag_dma_map(dev, frag, 0, frag->size,
+					DMA_TO_DEVICE);
+		if (unlikely(dma_mapping_error(dev, addr))) {
+			netdev_err(priv->net_dev, "dma_map_single() failed\n");
+			err = -EINVAL;
+			goto map_failed;
+		}
+		ldpaa_sg_set_addr(&sgt[i], addr);
+	}
+
+	ldpaa_sg_set_final(&sgt[i-1], true);
+
+	addr = dma_map_single(dev, sgt_buf, sgt_buf_size, DMA_TO_DEVICE);
+	if (unlikely(dma_mapping_error(dev, addr))) {
+		netdev_err(priv->net_dev, "dma_map_single() failed\n");
+		err = -EINVAL;
+		goto map_failed;
+	}
+	ldpaa_fd_set_addr(fd, addr);
+	ldpaa_fd_set_offset(fd, priv->tx_data_offset);
+	ldpaa_fd_set_len(fd, skb->len);
+	ldpaa_fd_set_bpid(fd, priv->dpbp_attrs.bpid);
+	ldpaa_fd_set_format(fd, dpaa_fd_sg);
+
+	fd->simple.ctrl = LDPAA_FD_CTRL_ASAL | LDPAA_FD_CTRL_PTA |
+			 LDPAA_FD_CTRL_PTV1;
+
+	return 0;
+
+map_failed:
+	dma_unmap_single(dev, ldpaa_sg_get_addr(&sgt[0]),
+			 ldpaa_sg_get_len(&sgt[0]), DMA_TO_DEVICE);
+	for (j = 1; j < i; j++)
+		dma_unmap_page(dev, ldpaa_sg_get_addr(&sgt[j]),
+			       ldpaa_sg_get_len(&sgt[j]),
+			       DMA_TO_DEVICE);
+map0_failed:
+	kfree(sgt_buf);
+	return err;
+}
+
+static int ldpaa_eth_build_single_fd(struct ldpaa_eth_priv *priv,
+				     struct sk_buff *skb,
+				     struct dpaa_fd *fd)
+{
+	struct device *dev = priv->net_dev->dev.parent;
+	uint8_t *buffer_start;
+	struct sk_buff **skbh;
+	dma_addr_t addr;
+
+	buffer_start = PTR_ALIGN(skb->data - priv->tx_data_offset -
+				 LDPAA_ETH_BUF_ALIGN,
+				 LDPAA_ETH_BUF_ALIGN);
+
+	/* PTA from egress side is passed as is to the confirmation side so
+	 * we need to clear some fields here in order to find consistent values
+	 * on TX confirmation. We are clearing FAS (Frame Annotation Status)
+	 * field here.
+	 */
+	memset(buffer_start + priv->buf_layout.private_data_size, 0, 8);
+
+	/* Store a backpointer to the skb at the beginning of the buffer
+	 * (in the private data area) such that we can release it
+	 * on Tx confirm
+	 */
+	skbh = (struct sk_buff **)buffer_start;
+	*skbh = skb;
+
+	addr = dma_map_single(dev,
+			      buffer_start,
+			      skb_end_pointer(skb) - buffer_start,
+			      DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, addr)) {
+		dev_err(dev, "dma_map_single() failed\n");
+		return -EINVAL;
+	}
+
+	ldpaa_fd_set_addr(fd, addr);
+	ldpaa_fd_set_offset(fd, (uint16_t)(skb->data - buffer_start));
+	ldpaa_fd_set_bpid(fd, priv->dpbp_attrs.bpid);
+	ldpaa_fd_set_len(fd, skb->len);
+	ldpaa_fd_set_format(fd, dpaa_fd_single);
+
+	fd->simple.ctrl = LDPAA_FD_CTRL_ASAL | LDPAA_FD_CTRL_PTA |
+			 LDPAA_FD_CTRL_PTV1;
+
+	return 0;
+}
+
+static int ldpaa_eth_tx(struct sk_buff *skb, struct net_device *net_dev)
+{
+	struct ldpaa_eth_priv *priv = netdev_priv(net_dev);
+	struct device *dev = net_dev->dev.parent;
+	struct dpaa_fd fd;
+	struct rtnl_link_stats64 *percpu_stats;
+	struct ldpaa_eth_stats *percpu_extras;
+	int err, i;
+
+	percpu_stats = this_cpu_ptr(priv->percpu_stats);
+	percpu_extras = this_cpu_ptr(priv->percpu_extras);
+
+	/* Setup the FD fields */
+	memset(&fd, 0, sizeof(fd));
+
+	if (unlikely(skb_headroom(skb) < LDPAA_ETH_NEEDED_HEADROOM(priv))) {
+		struct sk_buff *ns;
+		/* ...Empty line to appease checkpatch... */
+		ns = skb_realloc_headroom(skb, LDPAA_ETH_NEEDED_HEADROOM(priv));
+		if (unlikely(!ns)) {
+			percpu_stats->tx_dropped++;
+			goto err_alloc_headroom;
+		}
+		dev_kfree_skb(skb);
+		skb = ns;
+	}
+
+	/* We'll be holding a back-reference to the skb until Tx Confirmation;
+	 * we don't want that overwritten by a concurrent Tx with a cloned skb.
+	 */
+	skb = skb_unshare(skb, GFP_ATOMIC);
+	if (unlikely(!skb)) {
+		netdev_err(net_dev, "Out of memory for skb_unshare()");
+		/* skb_unshare() has already freed the skb */
+		percpu_stats->tx_dropped++;
+		return NETDEV_TX_OK;
+	}
+
+	if (skb_is_nonlinear(skb)) {
+		err = ldpaa_eth_build_sg_fd(priv, skb, &fd);
+		percpu_extras->tx_sg_frames++;
+		percpu_extras->tx_sg_bytes += skb->len;
+	} else
+		err = ldpaa_eth_build_single_fd(priv, skb, &fd);
+	if (unlikely(err)) {
+		percpu_stats->tx_dropped++;
+		goto err_build_fd;
+	}
+
+	/* Tracing point */
+	trace_ldpaa_tx_fd(net_dev, &fd);
+
+	/* FIXME Ugly hack, and not even cpu hotplug-friendly */
+	for (i = 0; i < 100000; i++) {
+		err = dpaa_io_service_enqueue_qd(NULL, priv->tx_qdid,
+						 0, priv->fq[0].flowid, &fd);
+		if (err != -EBUSY)
+			break;
+	}
+	if (unlikely(err < 0)) {
+		dev_dbg(dev, "error enqueueing Tx frame\n");
+		percpu_stats->tx_errors++;
+		goto err_enqueue;
+	}
+	percpu_extras->tx_portal_busy += i;
+	percpu_stats->tx_packets++;
+	percpu_stats->tx_bytes += skb->len;
+
+	return NETDEV_TX_OK;
+
+err_enqueue:
+err_build_fd:
+err_alloc_headroom:
+	dev_kfree_skb(skb);
+
+	return NETDEV_TX_OK;
+}
+
+static void ldpaa_eth_tx_conf(struct ldpaa_eth_priv *priv,
+			      const struct dpaa_fd *fd)
+{
+	struct device *dev = priv->net_dev->dev.parent;
+	dma_addr_t fd_addr, sg_addr;
+	struct sk_buff **skbh, *skb;
+	struct ldpaa_fas *fas;
+	uint32_t status;
+	struct rtnl_link_stats64 *percpu_stats;
+	struct ldpaa_eth_stats *percpu_extras;
+	unsigned char *buffer_start;
+	int i, nr_frags, unmap_size;
+	struct dpaa_sg_entry *sgt;
+
+	/* Tracing point */
+	trace_ldpaa_tx_conf_fd(priv->net_dev, fd);
+
+	fd_addr = ldpaa_fd_get_addr(fd);
+
+	skbh = phys_to_virt(fd_addr);
+	skb = *skbh;
+
+	percpu_extras = this_cpu_ptr(priv->percpu_extras);
+	percpu_extras->tx_conf_frames++;
+	percpu_extras->tx_conf_bytes += skb->len;
+
+	if (ldpaa_fd_get_format(fd) == dpaa_fd_single) {
+		buffer_start = (unsigned char *)skbh;
+		/* Accessing the skb buffer is safe before dma unmap, because
+		 * we didn't map the actual skb shell.
+		 */
+		dma_unmap_single(dev, fd_addr,
+				 skb_end_pointer(skb) - buffer_start,
+				 DMA_TO_DEVICE);
+	} else {
+		/* Unmap the SGT buffer first. We didn't map the skb shell. */
+		nr_frags = skb_shinfo(skb)->nr_frags;
+		unmap_size = priv->tx_data_offset +
+		       sizeof(struct dpaa_sg_entry) * (1 + nr_frags);
+		dma_unmap_single(dev, fd_addr, unmap_size, DMA_TO_DEVICE);
+	}
+
+	/* Check the status from the Frame Annotation after we unmap the first
+	 * buffer but before we free it.
+	 */
+	if (fd->simple.frc & LDPAA_FD_FRC_FASV) {
+		fas = (struct ldpaa_fas *)
+			((void *)skbh + priv->buf_layout.private_data_size);
+		status = le32_to_cpu(fas->status);
+		if (status & LDPAA_ETH_TXCONF_ERR_MASK) {
+			dev_err(dev, "TxConf frame error(s): 0x%08x\n",
+				status & LDPAA_ETH_TXCONF_ERR_MASK);
+			percpu_stats = this_cpu_ptr(priv->percpu_stats);
+			/* Tx-conf logically pertains to the egress path.
+			 * TODO add some specific counters for tx-conf also.
+			 */
+			percpu_stats->tx_errors++;
+		}
+	}
+
+	if (ldpaa_fd_get_format(fd) == dpaa_fd_sg) {
+		/* First sg entry was dma_map_single'd, the rest were
+		 * dma_map_page'd.
+		 */
+		sgt = (void *)skbh + ldpaa_fd_get_offset(fd);
+		sg_addr = ldpaa_sg_get_addr(&sgt[0]);
+		unmap_size = ldpaa_sg_get_len(&sgt[0]) +
+			     ldpaa_sg_get_offset(&sgt[0]);
+		dma_unmap_single(dev, sg_addr, unmap_size,
+				 DMA_TO_DEVICE);
+		nr_frags = skb_shinfo(skb)->nr_frags;
+		for (i = 1; i <= nr_frags; i++) {
+			sg_addr = ldpaa_sg_get_addr(&sgt[i]);
+			unmap_size = ldpaa_sg_get_len(&sgt[i]) +
+				     ldpaa_sg_get_offset(&sgt[i]);
+			dma_unmap_page(dev, sg_addr, unmap_size, DMA_TO_DEVICE);
+		}
+		/* SGT buffer was kmalloc'ed on tx */
+		kfree(skbh);
+	}
+
+	/* Move on with skb release */
+	dev_kfree_skb(skb);
+}
+
+static int ldpaa_eth_set_rx_csum(struct ldpaa_eth_priv *priv, bool enable)
+{
+	int err;
+
+	err = dpni_set_l3_chksum_validation(priv->mc_io, priv->mc_token,
+					    enable);
+	if (unlikely(err)) {
+		netdev_err(priv->net_dev,
+			   "dpni_set_l3_chksum_validation() failed\n");
+		return err;
+	}
+
+	err = dpni_set_l4_chksum_validation(priv->mc_io, priv->mc_token,
+					    enable);
+	if (unlikely(err)) {
+		netdev_err(priv->net_dev,
+			   "dpni_set_l4_chksum_validation failed\n");
+		return err;
+	}
+
+	return 0;
+}
+
+static int ldpaa_eth_set_tx_csum(struct ldpaa_eth_priv *priv, bool enable)
+{
+	struct ldpaa_eth_fq *fq;
+	struct dpni_tx_flow_cfg tx_flow_cfg;
+	int err;
+	int i;
+
+	memset(&tx_flow_cfg, 0, sizeof(tx_flow_cfg));
+	tx_flow_cfg.options = DPNI_TX_FLOW_OPT_L3_CHKSUM_GEN |
+			      DPNI_TX_FLOW_OPT_L4_CHKSUM_GEN;
+	tx_flow_cfg.l3_chksum_gen = enable;
+	tx_flow_cfg.l4_chksum_gen = enable;
+
+	for (i = 0; i < priv->num_fqs; i++) {
+		fq = &priv->fq[i];
+		if (fq->type != LDPAA_TX_CONF_FQ)
+			continue;
+
+		/* The Tx flowid is kept in the corresponding TxConf FQ. */
+		err = dpni_set_tx_flow(priv->mc_io, priv->mc_token,
+				       &fq->flowid, &tx_flow_cfg);
+		if (unlikely(err)) {
+			netdev_err(priv->net_dev, "dpni_set_tx_flow failed\n");
+			return err;
+		}
+	}
+
+	return 0;
+}
+
+static inline int __ldpaa_eth_pull_fq(struct ldpaa_eth_fq *fq)
+{
+	int err;
+	int dequeues = -1;
+	struct ldpaa_eth_priv *priv = fq->netdev_priv;
+
+	/* Retry while portal is busy */
+	do {
+		err = dpaa_io_service_pull_fq(NULL, fq->fqid, fq->ring.store);
+		dequeues++;
+	} while (err == -EBUSY);
+	if (unlikely(err))
+		netdev_err(priv->net_dev, "dpaa_io_service_pull err %d", err);
+
+	fq->stats.rx_portal_busy += dequeues;
+	return err;
+}
+
+static int ldpaa_eth_poll(struct napi_struct *napi, int budget)
+{
+	struct ldpaa_eth_fq *fq;
+	int cleaned = 0, store_cleaned;
+	int err;
+
+	fq = container_of(napi, struct ldpaa_eth_fq, napi);
+	/* TODO Must prioritize TxConf over Rx NAPIs */
+
+	do {
+		store_cleaned = ldpaa_eth_store_consume(fq);
+		cleaned += store_cleaned;
+
+		if (store_cleaned < LDPAA_ETH_STORE_SIZE ||
+		    cleaned >= budget - LDPAA_ETH_STORE_SIZE)
+			break;
+
+		/* Try to dequeue some more */
+		err = __ldpaa_eth_pull_fq(fq);
+		if (unlikely(err))
+			break;
+		/* FIXME Must be able to safely query the store
+		 * before the DMA finishes the first transfer
+		 */
+		ndelay(1000);
+	} while (1);
+
+	if (cleaned < budget)
+		napi_complete(napi);
+
+	err = dpaa_io_service_rearm(NULL, &fq->nctx);
+	if (unlikely(err))
+		netdev_err(fq->netdev_priv->net_dev, "Rx notif rearm failed\n");
+
+	return cleaned;
+}
+
+static void ldpaa_eth_napi_enable(struct ldpaa_eth_priv *priv)
+{
+	struct ldpaa_eth_fq *fq;
+	int i;
+
+	for (i = 0; i < priv->num_fqs; i++) {
+		fq = &priv->fq[i];
+		napi_enable(&fq->napi);
+	}
+}
+
+static void ldpaa_eth_napi_disable(struct ldpaa_eth_priv *priv)
+{
+	struct ldpaa_eth_fq *fq;
+	int i;
+
+	for (i = 0; i < priv->num_fqs; i++) {
+		fq = &priv->fq[i];
+		napi_disable(&fq->napi);
+	}
+}
+
+static int __cold ldpaa_eth_open(struct net_device *net_dev)
+{
+	struct ldpaa_eth_priv *priv = netdev_priv(net_dev);
+	int err;
+
+	/* We'll only start the txqs when the link is actually ready; make sure
+	 * we don't race against the link up notification, which may come
+	 * immediately after dpni_enable();
+	 *
+	 * FIXME beware of race conditions
+	 */
+	netif_tx_stop_all_queues(net_dev);
+
+	err = dpni_enable(priv->mc_io, priv->mc_token);
+	if (err < 0) {
+		dev_err(net_dev->dev.parent, "dpni_enable() failed\n");
+		return err;
+	}
+
+	ldpaa_eth_napi_enable(priv);
+
+	return 0;
+}
+
+static int __cold ldpaa_eth_stop(struct net_device *net_dev)
+{
+	struct ldpaa_eth_priv *priv = netdev_priv(net_dev);
+
+	/* Stop Tx and Rx traffic */
+	netif_tx_stop_all_queues(net_dev);
+	dpni_disable(priv->mc_io, priv->mc_token);
+
+	/* TODO: Make sure queues are drained before if down is complete! */
+	msleep(100);
+
+	ldpaa_eth_napi_disable(priv);
+	msleep(100);
+
+	return 0;
+}
+
+static int ldpaa_eth_init(struct net_device *net_dev)
+{
+	uint64_t supported = 0;
+	uint64_t not_supported = 0;
+	const struct ldpaa_eth_priv *priv = netdev_priv(net_dev);
+	uint64_t options = priv->dpni_attrs.options;
+
+	/* Capabilities listing */
+	supported |= IFF_LIVE_ADDR_CHANGE | IFF_PROMISC | IFF_ALLMULTI;
+
+	if (options & DPNI_OPT_UNICAST_FILTER)
+		supported |= IFF_UNICAST_FLT;
+	else
+		not_supported |= IFF_UNICAST_FLT;
+
+	if (options & DPNI_OPT_MULTICAST_FILTER)
+		supported |= IFF_MULTICAST;
+	else
+		not_supported |= IFF_MULTICAST;
+
+	net_dev->priv_flags |= supported;
+	net_dev->priv_flags &= ~not_supported;
+
+	/* Features */
+	net_dev->features = NETIF_F_RXCSUM |
+			    NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |
+			    NETIF_F_SG | NETIF_F_HIGHDMA;
+	net_dev->hw_features = net_dev->features;
+
+	return 0;
+}
+
+static int ldpaa_eth_set_addr(struct net_device *net_dev, void *addr)
+{
+	struct ldpaa_eth_priv *priv = netdev_priv(net_dev);
+	struct device *dev = net_dev->dev.parent;
+	int err;
+
+	err = eth_mac_addr(net_dev, addr);
+	if (err < 0) {
+		dev_err(dev, "eth_mac_addr() failed with error %d\n", err);
+		return err;
+	}
+
+	err = dpni_set_primary_mac_addr(priv->mc_io, priv->mc_token,
+					net_dev->dev_addr);
+	if (err) {
+		dev_err(dev, "dpni_set_primary_mac_addr() failed (%d)\n", err);
+		return err;
+	}
+
+	return 0;
+}
+
+/** Fill in counters maintained by the GPP driver. These may be different from
+ * the hardware counters obtained by ethtool.
+ */
+static struct rtnl_link_stats64
+*ldpaa_eth_get_stats(struct net_device *net_dev,
+		     struct rtnl_link_stats64 *stats)
+{
+	struct ldpaa_eth_priv *priv = netdev_priv(net_dev);
+	struct rtnl_link_stats64 *percpu_stats;
+	u64 *cpustats;
+	u64 *netstats = (u64 *)stats;
+	int i, j;
+	int num = sizeof(struct rtnl_link_stats64) / sizeof(u64);
+
+	for_each_possible_cpu(i) {
+		percpu_stats = per_cpu_ptr(priv->percpu_stats, i);
+		cpustats = (u64 *)percpu_stats;
+		for (j = 0; j < num; j++)
+			netstats[j] += cpustats[j];
+	}
+
+	return stats;
+}
+
+static int ldpaa_eth_change_mtu(struct net_device *net_dev, int mtu)
+{
+	struct ldpaa_eth_priv *priv = netdev_priv(net_dev);
+	int err;
+
+	if (mtu < 68 || mtu > LDPAA_ETH_MAX_MTU) {
+		netdev_err(net_dev, "Invalid MTU %d. Valid range is: 68..%d\n",
+			   mtu, LDPAA_ETH_MAX_MTU);
+		return -EINVAL;
+	}
+
+	/* Set the maximum Rx frame length to match the transmit side;
+	 * account for L2 headers when computing the MFL
+	 */
+	err = dpni_set_max_frame_length(priv->mc_io, priv->mc_token,
+					(uint16_t)LDPAA_ETH_L2_MAX_FRM(mtu));
+	if (err) {
+		netdev_err(net_dev, "dpni_set_mfl() failed\n");
+		return err;
+	}
+
+	net_dev->mtu = mtu;
+	return 0;
+}
+
+/* Convenience macro to make code littered with error checking more readable */
+#define LDPAA_ETH_WARN_IF_ERR(err, netdevp, format, ...) \
+do { \
+	if (unlikely(err)) \
+		netdev_warn(netdevp, format, ##__VA_ARGS__); \
+} while (0)
+
+/* Copy mac unicast addresses from @net_dev to @priv.
+ * Its sole purpose is to make ldpaa_eth_set_rx_mode() more readable.
+ */
+static inline void _ldpaa_eth_hw_add_uc_addr(const struct net_device *net_dev,
+					     struct ldpaa_eth_priv *priv)
+{
+	struct netdev_hw_addr *ha;
+	int err;
+
+	netdev_for_each_uc_addr(ha, net_dev) {
+		err = dpni_add_mac_addr(priv->mc_io, priv->mc_token, ha->addr);
+		LDPAA_ETH_WARN_IF_ERR(err, priv->net_dev,
+				      "Could not add ucast MAC %pM to the filtering table (err %d)\n",
+				      ha->addr, err);
+	}
+}
+
+/* Copy mac multicast addresses from @net_dev to @priv
+ * Its sole purpose is to make ldpaa_eth_set_rx_mode() more readable.
+ */
+static inline void _ldpaa_eth_hw_add_mc_addr(const struct net_device *net_dev,
+					     struct ldpaa_eth_priv *priv)
+{
+	struct netdev_hw_addr *ha;
+	int err;
+
+	netdev_for_each_mc_addr(ha, net_dev) {
+		err = dpni_add_mac_addr(priv->mc_io, priv->mc_token, ha->addr);
+		LDPAA_ETH_WARN_IF_ERR(err, priv->net_dev,
+				      "Could not add mcast MAC %pM to the filtering table (err %d)\n",
+				      ha->addr, err);
+	}
+}
+
+static void ldpaa_eth_set_rx_mode(struct net_device *net_dev)
+{
+	struct ldpaa_eth_priv *priv = netdev_priv(net_dev);
+	int uc_count = netdev_uc_count(net_dev);
+	int mc_count = netdev_mc_count(net_dev);
+	uint8_t max_uc = priv->dpni_attrs.max_unicast_filters;
+	uint8_t max_mc = priv->dpni_attrs.max_multicast_filters;
+	uint64_t options = priv->dpni_attrs.options;
+	uint16_t mc_token = priv->mc_token;
+	struct fsl_mc_io *mc_io = priv->mc_io;
+	int err;
+
+	/* Basic sanity checks; these probably indicate a misconfiguration */
+	if (!(options & DPNI_OPT_UNICAST_FILTER) && max_uc != 0)
+		netdev_info(net_dev,
+			    "max_unicast_filters=%d, you must have DPNI_OPT_UNICAST_FILTER in the DPL\n",
+			    max_uc);
+	if (!(options & DPNI_OPT_MULTICAST_FILTER) && max_mc != 0)
+		netdev_info(net_dev,
+			    "max_multicast_filters=%d, you must have DPNI_OPT_MULTICAST_FILTER in the DPL\n",
+			    max_mc);
+
+	/* Force promiscuous if the uc or mc counts exceed our capabilities. */
+	if (uc_count > max_uc) {
+		netdev_info(net_dev,
+			    "Unicast addr count reached %d, max allowed is %d; forcing promisc\n",
+			    uc_count, max_uc);
+		goto force_promisc;
+	}
+	if (mc_count > max_mc) {
+		netdev_info(net_dev,
+			    "Multicast addr count reached %d, max allowed is %d; forcing promisc\n",
+			    mc_count, max_mc);
+		goto force_mc_promisc;
+	}
+
+	/* Adjust promisc settings due to flag combinations */
+	if (net_dev->flags & IFF_PROMISC) {
+		goto force_promisc;
+	} else if (net_dev->flags & IFF_ALLMULTI) {
+		/* First, rebuild unicast filtering table. This should be done
+		 * in promisc mode, in order to avoid frame loss while we
+		 * progressively add entries to the table.
+		 * We don't know whether we had been in promisc already, and
+		 * making an MC call to find it is expensive; so set uc promisc
+		 * nonetheless.
+		 */
+		err = dpni_set_unicast_promisc(mc_io, mc_token, 1);
+		LDPAA_ETH_WARN_IF_ERR(err, net_dev, "Can't set uc promisc\n");
+
+		/* Actual uc table reconstruction. */
+		err = dpni_clear_mac_filters(mc_io, mc_token, 1, 0);
+		LDPAA_ETH_WARN_IF_ERR(err, net_dev, "Can't clear uc filters\n");
+		_ldpaa_eth_hw_add_uc_addr(net_dev, priv);
+
+		/* Finally, clear uc promisc and set mc promisc as requested. */
+		err = dpni_set_unicast_promisc(mc_io, mc_token, 0);
+		LDPAA_ETH_WARN_IF_ERR(err, net_dev, "Can't clear uc promisc\n");
+		goto force_mc_promisc;
+	}
+
+	/* Neither unicast, nor multicast promisc will be on... eventually.
+	 * For now, rebuild mac filtering tables while forcing both of them on.
+	 */
+	err = dpni_set_unicast_promisc(mc_io, mc_token, 1);
+	LDPAA_ETH_WARN_IF_ERR(err, net_dev, "Can't set uc promisc (%d)\n", err);
+	err = dpni_set_multicast_promisc(mc_io, mc_token, 1);
+	LDPAA_ETH_WARN_IF_ERR(err, net_dev, "Can't set mc promisc (%d)\n", err);
+
+	/* Actual mac filtering tables reconstruction */
+	err = dpni_clear_mac_filters(mc_io, mc_token, 1, 1);
+	LDPAA_ETH_WARN_IF_ERR(err, net_dev, "Can't clear mac filters\n");
+	_ldpaa_eth_hw_add_mc_addr(net_dev, priv);
+	_ldpaa_eth_hw_add_uc_addr(net_dev, priv);
+
+	/* Now we can clear both ucast and mcast promisc, without risking
+	 * to drop legitimate frames anymore.
+	 */
+	err = dpni_set_unicast_promisc(mc_io, mc_token, 0);
+	LDPAA_ETH_WARN_IF_ERR(err, net_dev, "Can't clear ucast promisc\n");
+	err = dpni_set_multicast_promisc(mc_io, mc_token, 0);
+	LDPAA_ETH_WARN_IF_ERR(err, net_dev, "Can't clear mcast promisc\n");
+
+	return;
+
+force_promisc:
+	err = dpni_set_unicast_promisc(mc_io, mc_token, 1);
+	LDPAA_ETH_WARN_IF_ERR(err, net_dev, "Can't set ucast promisc\n");
+force_mc_promisc:
+	err = dpni_set_multicast_promisc(mc_io, mc_token, 1);
+	LDPAA_ETH_WARN_IF_ERR(err, net_dev, "Can't set mcast promisc\n");
+}
+
+static int ldpaa_eth_set_features(struct net_device *net_dev,
+				  netdev_features_t features)
+{
+	struct ldpaa_eth_priv *priv = netdev_priv(net_dev);
+	netdev_features_t changed = features ^ net_dev->features;
+	int err;
+
+	if (changed & NETIF_F_RXCSUM) {
+		bool enable = !!(features & NETIF_F_RXCSUM);
+
+		err = ldpaa_eth_set_rx_csum(priv, enable);
+		if (unlikely(err))
+			return err;
+	}
+
+	if (changed & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM)) {
+		bool enable = !!(features &
+				 (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM));
+		err = ldpaa_eth_set_tx_csum(priv, enable);
+		if (unlikely(err))
+			return err;
+	}
+
+	return 0;
+}
+
+static const struct net_device_ops ldpaa_eth_ops = {
+	.ndo_open = ldpaa_eth_open,
+	.ndo_start_xmit = ldpaa_eth_tx,
+	.ndo_stop = ldpaa_eth_stop,
+	.ndo_init = ldpaa_eth_init,
+	.ndo_set_mac_address = ldpaa_eth_set_addr,
+	.ndo_get_stats64 = ldpaa_eth_get_stats,
+	.ndo_change_mtu = ldpaa_eth_change_mtu,
+	.ndo_set_rx_mode = ldpaa_eth_set_rx_mode,
+	.ndo_set_features = ldpaa_eth_set_features,
+};
+
+static void ldpaa_eth_fqdan_cb(struct dpaa_io_notification_ctx *ctx)
+{
+	struct ldpaa_eth_fq *fq = container_of(ctx, struct ldpaa_eth_fq, nctx);
+
+	/* TODO check return value */
+	__ldpaa_eth_pull_fq(fq);
+
+	/* Update NAPI statistics */
+	switch (fq->type) {
+	case LDPAA_RX_FQ:
+		fq->stats.rx_fqdan++;
+		break;
+	case LDPAA_TX_CONF_FQ:
+		fq->stats.tx_conf_fqdan++;
+		break;
+	default:
+		WARN_ONCE(1, "Unknown FQ type: %d!", fq->type);
+	}
+
+	fq->has_frames = true;
+	napi_schedule(&fq->napi);
+}
+
+static void ldpaa_eth_setup_fqs(struct ldpaa_eth_priv *priv)
+{
+	int i;
+
+	/* We have one TxConf FQ per target CPU, although at the moment
+	 * we can't guarantee affinity.
+	 */
+	for_each_online_cpu(i) {
+		priv->fq[priv->num_fqs].netdev_priv = priv;
+		priv->fq[priv->num_fqs].type = LDPAA_TX_CONF_FQ;
+		priv->fq[priv->num_fqs++].consume = ldpaa_eth_tx_conf;
+	}
+
+	/* The number of Rx queues (Rx distribution width) may be different from
+	 * the number of cores.
+	 *
+	 * TODO: We still only have one traffic class for now,
+	 * but for multiple TCs may need an array of dist sizes.
+	 */
+	priv->rx_dist_size = roundup_pow_of_two(num_possible_cpus());
+	for (i = 0; i < priv->rx_dist_size; i++) {
+		priv->fq[priv->num_fqs].netdev_priv = priv;
+		priv->fq[priv->num_fqs].type = LDPAA_RX_FQ;
+		priv->fq[priv->num_fqs].consume = ldpaa_eth_rx;
+		priv->fq[priv->num_fqs++].flowid = i;
+	}
+}
+
+static int __cold ldpaa_dpio_setup(struct ldpaa_eth_priv *priv)
+{
+	struct dpaa_io_notification_ctx *nctx;
+	int err, i, j;
+	int cpu;
+
+	/* For each FQ, pick one CPU to deliver FQDANs to.
+	 * This may well change at runtime, either through irqbalance or
+	 * through direct user intervention.
+	 */
+	cpu = cpumask_first(cpu_online_mask);
+	for (i = 0; i < priv->num_fqs; i++) {
+		nctx = &priv->fq[i].nctx;
+		nctx->is_cdan = 0;
+		nctx->desired_cpu = cpu;
+		nctx->cb = ldpaa_eth_fqdan_cb;
+		/* Register the new context */
+		err = dpaa_io_service_register(NULL, nctx);
+		if (unlikely(err)) {
+			netdev_err(priv->net_dev,
+				   "Rx notifications register failed\n");
+			nctx->cb = NULL;
+			goto err_service_reg;
+		}
+
+		cpu = cpumask_next(cpu, cpu_online_mask);
+		if (cpu >= nr_cpu_ids)
+			cpu = cpumask_first(cpu_online_mask);
+	}
+
+	return 0;
+
+err_service_reg:
+	for (j = 0; j < i; j++) {
+		nctx = &priv->fq[j].nctx;
+		dpaa_io_service_deregister(NULL, nctx);
+	}
+
+	return err;
+}
+
+static void __cold ldpaa_dpio_free(struct ldpaa_eth_priv *priv)
+{
+	int i;
+
+	/* deregister FQDAN notifications */
+	for (i = 0; i < priv->num_fqs; i++)
+		dpaa_io_service_deregister(NULL, &priv->fq[i].nctx);
+}
+
+static void ldpaa_dpbp_drain_cnt(struct ldpaa_eth_priv *priv, int count)
+{
+	struct device *dev = priv->net_dev->dev.parent;
+	uint64_t buf_array[7];
+	void *vaddr;
+	int ret, i;
+
+	BUG_ON(count > 7);
+
+	do {
+		ret = dpaa_io_service_acquire(NULL, priv->dpbp_attrs.bpid,
+					      buf_array, count);
+		if (ret < 0) {
+			pr_err("dpaa_io_service_acquire() failed\n");
+			return;
+		}
+		for (i = 0; i < ret; i++) {
+			/* Same logic as on regular Rx path */
+			dma_unmap_single(dev, buf_array[i],
+					 LDPAA_ETH_RX_BUFFER_SIZE,
+					 DMA_FROM_DEVICE);
+			vaddr = phys_to_virt(buf_array[i]);
+			put_page(virt_to_head_page(vaddr));
+		}
+	} while (ret);
+}
+
+static void ldpaa_dpbp_drain(struct ldpaa_eth_priv *priv)
+{
+	ldpaa_dpbp_drain_cnt(priv, 7);
+	ldpaa_dpbp_drain_cnt(priv, 1);
+}
+
+static int ldpaa_bp_add_7(struct ldpaa_eth_priv *priv, uint16_t bpid)
+{
+	struct device *dev = priv->net_dev->dev.parent;
+	uint64_t buf_array[7];
+	void *buf;
+	dma_addr_t addr;
+	int i;
+
+	for (i = 0; i < 7; i++) {
+		/* Allocate buffer visible to WRIOP + skb shared info +
+		 * alignment padding
+		 */
+		buf = netdev_alloc_frag(LDPAA_ETH_RX_BUFFER_SIZE +
+					sizeof(struct skb_shared_info) +
+					2 * SMP_CACHE_BYTES);
+		if (unlikely(!buf)) {
+			dev_err(dev, "buffer allocation failed\n");
+			goto err_alloc;
+		}
+		buf = PTR_ALIGN(buf, SMP_CACHE_BYTES);
+
+		addr = dma_map_single(dev, buf, LDPAA_ETH_RX_BUFFER_SIZE,
+				      DMA_FROM_DEVICE);
+		if (dma_mapping_error(dev, addr)) {
+			dev_err(dev, "dma_map_single() failed\n");
+			goto err_map;
+		}
+		buf_array[i] = addr;
+	}
+
+release_bufs:
+	/* In case the portal is busy, retry until successful.
+	 * This function is guaranteed to succeed in a reasonable amount
+	 * of time.
+	 */
+	while (dpaa_io_service_release(NULL, bpid, buf_array, i))
+		cpu_relax();
+	return i;
+
+err_map:
+	put_page(virt_to_head_page(buf));
+err_alloc:
+	if (i)
+		goto release_bufs;
+
+	return 0;
+}
+
+static int ldpaa_dpbp_seed(struct ldpaa_eth_priv *priv, uint16_t bpid)
+{
+	int i, j;
+	int new_count;
+	int *count;
+
+	for_each_possible_cpu(j) {
+		for (i = 0; i < LDPAA_ETH_NUM_BUFS; i += 7) {
+			new_count = ldpaa_bp_add_7(priv, bpid);
+			count = per_cpu_ptr(priv->buf_count, j);
+			*count += new_count;
+		}
+	}
+
+	return 0;
+}
+
+/* Function is called from softirq context only, so we don't need to guard
+ * the access to percpu count
+ */
+static int ldpaa_dpbp_refill(struct ldpaa_eth_priv *priv, uint16_t bpid)
+{
+	int new_count;
+	int err = 0;
+	int *count = this_cpu_ptr(priv->buf_count);
+
+	if (unlikely(*count < LDPAA_ETH_REFILL_THRESH)) {
+		do {
+			new_count = ldpaa_bp_add_7(priv, bpid);
+			if (unlikely(!new_count)) {
+				/* Out of memory; abort for now, we'll
+				 * try later on
+				 */
+				break;
+			}
+			*count += new_count;
+		} while (*count < LDPAA_ETH_NUM_BUFS);
+
+		if (unlikely(*count < LDPAA_ETH_NUM_BUFS))
+			err = -ENOMEM;
+	}
+
+	return err;
+}
+
+static int __cold ldpaa_dpbp_setup(struct ldpaa_eth_priv *priv)
+{
+	int err;
+	struct fsl_mc_device *dpbp_dev;
+	struct device *dev = priv->net_dev->dev.parent;
+
+	err = fsl_mc_object_allocate(to_fsl_mc_device(dev), FSL_MC_POOL_DPBP,
+				     &dpbp_dev);
+	if (err) {
+		dev_err(dev, "DPBP device allocation failed\n");
+		return err;
+	}
+
+	priv->dpbp_dev = dpbp_dev;
+
+	err = dpbp_open(priv->mc_io, priv->dpbp_dev->obj_desc.id,
+			&dpbp_dev->mc_handle);
+	if (err) {
+		dev_err(dev, "dpbp_open() failed\n");
+		goto err_open;
+	}
+
+	err = dpbp_enable(priv->mc_io, dpbp_dev->mc_handle);
+	if (err) {
+		dev_err(dev, "dpbp_enable() failed\n");
+		goto err_enable;
+	}
+
+	err = dpbp_get_attributes(priv->mc_io, dpbp_dev->mc_handle,
+				  &priv->dpbp_attrs);
+	if (err) {
+		dev_err(dev, "dpbp_get_attributes() failed\n");
+		goto err_get_attr;
+	}
+
+	err = ldpaa_dpbp_seed(priv, priv->dpbp_attrs.bpid);
+	if (err) {
+		dev_err(dev, "Buffer seeding failed for DPBP %d (bpid=%d)\n",
+			priv->dpbp_dev->obj_desc.id, priv->dpbp_attrs.bpid);
+		goto err_seed;
+	}
+
+	return 0;
+
+err_seed:
+err_get_attr:
+	dpbp_disable(priv->mc_io, dpbp_dev->mc_handle);
+err_enable:
+	dpbp_close(priv->mc_io, dpbp_dev->mc_handle);
+err_open:
+	fsl_mc_object_free(dpbp_dev);
+
+	return err;
+}
+
+static void __cold ldpaa_dpbp_free(struct ldpaa_eth_priv *priv)
+{
+	ldpaa_dpbp_drain(priv);
+	dpbp_disable(priv->mc_io, priv->dpbp_dev->mc_handle);
+	dpbp_close(priv->mc_io, priv->dpbp_dev->mc_handle);
+	fsl_mc_object_free(priv->dpbp_dev);
+}
+
+static int __cold ldpaa_dpni_setup(struct fsl_mc_device *ls_dev)
+{
+	struct device *dev = &ls_dev->dev;
+	struct ldpaa_eth_priv *priv;
+	struct net_device *net_dev;
+	int err;
+
+	net_dev = dev_get_drvdata(dev);
+	priv = netdev_priv(net_dev);
+
+	priv->dpni_id = ls_dev->obj_desc.id;
+
+	/* and get a handle for the DPNI this interface is associate with */
+	err = dpni_open(priv->mc_io, priv->dpni_id, &priv->mc_token);
+	if (err) {
+		dev_err(dev, "dpni_open() failed\n");
+		goto err_open;
+	}
+
+	/* FIXME Alex's moral compass says this must be done */
+	ls_dev->mc_io = priv->mc_io;
+	ls_dev->mc_handle = priv->mc_token;
+	err = dpni_get_attributes(priv->mc_io, priv->mc_token,
+				  &priv->dpni_attrs);
+	if (err) {
+		dev_err(dev, "dpni_get_attributes() failed (err=%d)\n", err);
+		goto err_get_attr;
+	}
+
+	/* Configure our buffers' layout */
+	priv->buf_layout.options = DPNI_BUF_LAYOUT_OPT_PARSER_RESULT |
+				   DPNI_BUF_LAYOUT_OPT_FRAME_STATUS |
+				   DPNI_BUF_LAYOUT_OPT_PRIVATE_DATA_SIZE;
+	priv->buf_layout.pass_parser_result = true;
+	priv->buf_layout.pass_frame_status = true;
+	priv->buf_layout.private_data_size = LDPAA_ETH_SWA_SIZE;
+	/* ...rx, ... */
+	err = dpni_set_rx_buffer_layout(priv->mc_io, priv->mc_token,
+					&priv->buf_layout);
+	if (err) {
+		dev_err(dev, "dpni_set_rx_buffer_layout() failed");
+		goto err_buf_layout;
+	}
+	/* ... tx, ... */
+	priv->buf_layout.options &= ~DPNI_BUF_LAYOUT_OPT_PARSER_RESULT;
+	err = dpni_set_tx_buffer_layout(priv->mc_io, priv->mc_token,
+					&priv->buf_layout);
+	if (err) {
+		dev_err(dev, "dpni_set_tx_buffer_layout() failed");
+		goto err_buf_layout;
+	}
+	/* ... tx-confirm. */
+	priv->buf_layout.options &= ~DPNI_BUF_LAYOUT_OPT_PRIVATE_DATA_SIZE;
+	err = dpni_set_tx_conf_buffer_layout(priv->mc_io, priv->mc_token,
+					     &priv->buf_layout);
+	if (err) {
+		dev_err(dev, "dpni_set_tx_conf_buffer_layout() failed");
+		goto err_buf_layout;
+	}
+	/* Now that we've set our tx buffer layout, retrieve the minimum
+	 * required tx data offset.
+	 */
+	err = dpni_get_tx_data_offset(priv->mc_io, priv->mc_token,
+				      &priv->tx_data_offset);
+	if (err) {
+		dev_err(dev, "dpni_get_tx_data_offset() failed\n");
+		goto err_data_offset;
+	}
+
+	/* Warn in case TX data offset is not multiple of 64 bytes. */
+	WARN_ON(priv->tx_data_offset % 64);
+
+	/* Accommodate SWA space. */
+	priv->tx_data_offset += LDPAA_ETH_SWA_SIZE;
+
+	return 0;
+
+err_data_offset:
+err_buf_layout:
+err_get_attr:
+	dpni_close(priv->mc_io, priv->mc_token);
+err_open:
+	return err;
+}
+
+static void ldpaa_dpni_free(struct ldpaa_eth_priv *priv)
+{
+	int err;
+
+	err = dpni_reset(priv->mc_io, priv->mc_token);
+	if (unlikely(err))
+		netdev_warn(priv->net_dev, "dpni_reset() failed (err %d)\n",
+			    err);
+
+	dpni_close(priv->mc_io, priv->mc_token);
+}
+
+static int ldpaa_rx_flow_setup(struct ldpaa_eth_priv *priv,
+			       struct ldpaa_eth_fq *fq)
+{
+	struct dpni_queue_attr rx_queue_attr;
+	struct dpni_queue_cfg queue_cfg;
+	int err;
+
+	queue_cfg.options = DPNI_QUEUE_OPT_USER_CTX | DPNI_QUEUE_OPT_DEST;
+	queue_cfg.dest_cfg.dest_type = DPNI_DEST_DPIO;
+	queue_cfg.dest_cfg.priority = 3;
+	queue_cfg.user_ctx = fq->nctx.qman64;
+	queue_cfg.dest_cfg.dest_id = fq->nctx.dpio_id;
+	err = dpni_set_rx_flow(priv->mc_io, priv->mc_token, 0, fq->flowid,
+			       &queue_cfg);
+	if (unlikely(err)) {
+		netdev_err(priv->net_dev, "dpni_set_rx_flow() failed\n");
+		return err;
+	}
+
+	/* Get the actual FQID that was assigned by MC */
+	err = dpni_get_rx_flow(priv->mc_io, priv->mc_token, 0, fq->flowid,
+			       &rx_queue_attr);
+	if (unlikely(err)) {
+		netdev_err(priv->net_dev, "dpni_get_rx_flow() failed\n");
+		return err;
+	}
+	fq->fqid = rx_queue_attr.fqid;
+	fq->nctx.id = fq->fqid;
+
+	return 0;
+}
+
+static int ldpaa_tx_flow_setup(struct ldpaa_eth_priv *priv,
+			       struct ldpaa_eth_fq *fq)
+{
+	struct dpni_tx_flow_cfg tx_flow_cfg;
+	struct dpni_queue_cfg queue_cfg;
+	struct dpni_tx_flow_attr tx_flow_attr;
+	int err;
+
+	fq->flowid = DPNI_NEW_FLOW_ID;
+	memset(&tx_flow_cfg, 0, sizeof(tx_flow_cfg));
+	tx_flow_cfg.options = DPNI_TX_FLOW_OPT_QUEUE;
+	queue_cfg.options = DPNI_QUEUE_OPT_USER_CTX |
+			    DPNI_QUEUE_OPT_DEST;
+	queue_cfg.user_ctx = fq->nctx.qman64;
+	queue_cfg.dest_cfg.dest_type = DPNI_DEST_DPIO;
+	queue_cfg.dest_cfg.dest_id = fq->nctx.dpio_id;
+	queue_cfg.dest_cfg.priority = 3;
+	tx_flow_cfg.conf_err_cfg.queue_cfg = queue_cfg;
+	err = dpni_set_tx_flow(priv->mc_io, priv->mc_token,
+			       &fq->flowid, &tx_flow_cfg);
+	if (unlikely(err)) {
+		netdev_err(priv->net_dev, "dpni_set_tx_flow() failed\n");
+		return err;
+	}
+
+	err = dpni_get_tx_flow(priv->mc_io, priv->mc_token,
+			       fq->flowid, &tx_flow_attr);
+	if (unlikely(err)) {
+		netdev_err(priv->net_dev, "dpni_get_tx_flow() failed\n");
+		return err;
+	}
+	fq->fqid = tx_flow_attr.conf_err_attr.queue_attr.fqid;
+	fq->nctx.id = fq->fqid;
+
+	return 0;
+}
+
+
+static int ldpaa_dpni_bind(struct ldpaa_eth_priv *priv)
+{
+	struct net_device *net_dev = priv->net_dev;
+	struct device *dev = net_dev->dev.parent;
+	struct dpni_rx_tc_dist_cfg dist_cfg;
+	struct dpkg_profile_cfg key_cfg;
+	struct dpni_pools_cfg pools_params;
+	void *dist_mem;
+	dma_addr_t dist_dma_mem;
+	int err = 0;
+	int i;
+
+	pools_params.num_dpbp = 1;
+	pools_params.pools[0].dpbp_id = priv->dpbp_dev->obj_desc.id;
+	pools_params.pools[0].buffer_size = LDPAA_ETH_RX_BUFFER_SIZE;
+	err = dpni_set_pools(priv->mc_io, priv->mc_token, &pools_params);
+	if (unlikely(err)) {
+		dev_err(dev, "dpni_set_pools() failed\n");
+		return err;
+	}
+
+	memset(&dist_cfg, 0, sizeof(dist_cfg));
+
+	/* MC does nasty things to the dist_size value that we provide, but
+	 * doesn't offer any getter function for the value they compute and
+	 * subsequently use.
+	 * So we basically must provide the desired value minus one, and account
+	 * for the roundup to the next power of two that's done inside MC.
+	 */
+	dist_cfg.dist_size = num_possible_cpus() - 1;
+	dist_cfg.dist_mode = DPNI_DIST_MODE_HASH;
+
+	memset(&key_cfg, 0, sizeof(key_cfg));
+	key_cfg.num_extracts = 4;
+	/* IP source address */
+	key_cfg.extracts[0].type = DPKG_EXTRACT_FROM_HDR;
+	key_cfg.extracts[0].extract.from_hdr.prot = NET_PROT_IP;
+	key_cfg.extracts[0].extract.from_hdr.type = DPKG_FULL_FIELD;
+	key_cfg.extracts[0].extract.from_hdr.field = NH_FLD_IP_SRC;
+	key_cfg.extracts[0].num_of_byte_masks = 0;
+	/* IP destination address */
+	key_cfg.extracts[1].type = DPKG_EXTRACT_FROM_HDR;
+	key_cfg.extracts[1].extract.from_hdr.prot = NET_PROT_IP;
+	key_cfg.extracts[1].extract.from_hdr.type = DPKG_FULL_FIELD;
+	key_cfg.extracts[1].extract.from_hdr.field = NH_FLD_IP_DST;
+	key_cfg.extracts[1].num_of_byte_masks = 0;
+	/* UDP source port */
+	key_cfg.extracts[2].type = DPKG_EXTRACT_FROM_HDR;
+	key_cfg.extracts[2].extract.from_hdr.prot = NET_PROT_UDP;
+	key_cfg.extracts[2].extract.from_hdr.type = DPKG_FULL_FIELD;
+	key_cfg.extracts[2].extract.from_hdr.field = NH_FLD_UDP_PORT_SRC;
+	key_cfg.extracts[2].num_of_byte_masks = 0;
+	/* UDP destination port */
+	key_cfg.extracts[3].type = DPKG_EXTRACT_FROM_HDR;
+	key_cfg.extracts[3].extract.from_hdr.prot = NET_PROT_UDP;
+	key_cfg.extracts[3].extract.from_hdr.type = DPKG_FULL_FIELD;
+	key_cfg.extracts[3].extract.from_hdr.field = NH_FLD_UDP_PORT_DST;
+	key_cfg.extracts[3].num_of_byte_masks = 0;
+	/* Note: The above key works well for TCP also, as MC translates
+	 * the UDP extract field values to generic L4 source/destination ports
+	 */
+
+	dist_mem = kzalloc(256, GFP_KERNEL);
+	if (unlikely(!dist_mem)) {
+		netdev_err(priv->net_dev, "kzalloc() failed\n");
+		return -ENOMEM;
+	}
+
+	/* The function writes into dist_mem, so we must call it before
+	 * dma-mapping the buffer.
+	 */
+	err = dpni_prepare_key_cfg(&key_cfg, dist_mem);
+	if (unlikely(err)) {
+		dev_err(dev, "dpni_prepare_key_cfg error %d", err);
+		goto err_key_cfg;
+	}
+
+	/* Prepare for setting the rx dist */
+	dist_dma_mem = dma_map_single(dev, dist_mem, 256, DMA_BIDIRECTIONAL);
+	if (unlikely(dma_mapping_error(dev, dist_dma_mem))) {
+		netdev_err(priv->net_dev, "DMA mapping failed\n");
+		err = -ENOMEM;
+		goto err_map;
+	}
+	dist_cfg.key_cfg_iova = dist_dma_mem;
+
+	err = dpni_set_rx_tc_dist(priv->mc_io, priv->mc_token, 0, &dist_cfg);
+	if (unlikely(err)) {
+		netdev_err(priv->net_dev, "dpni_set_rx_tc_dist() failed\n");
+		goto err_rx_tc;
+	}
+
+	/* Configure Rx and Tx conf queues to generate FQDANs */
+	for (i = 0; i < priv->num_fqs; i++) {
+		if (priv->fq[i].type == LDPAA_RX_FQ)
+			err = ldpaa_rx_flow_setup(priv, &priv->fq[i]);
+		else
+			err = ldpaa_tx_flow_setup(priv, &priv->fq[i]);
+		if (unlikely(err))
+			goto err_flow_setup;
+	}
+
+	err = dpni_get_qdid(priv->mc_io, priv->mc_token, &priv->tx_qdid);
+	if (unlikely(err)) {
+		netdev_err(net_dev, "dpni_get_qdid() failed\n");
+		goto err_qdid;
+	}
+
+	return 0;
+
+err_qdid:
+err_flow_setup:
+err_rx_tc:
+	dma_unmap_single(dev, dist_dma_mem, 256, DMA_BIDIRECTIONAL);
+err_map:
+err_key_cfg:
+	kfree(dist_mem);
+	return err;
+}
+
+static int ldpaa_eth_alloc_rings(struct ldpaa_eth_priv *priv)
+{
+	struct net_device *net_dev = priv->net_dev;
+	struct device *dev = net_dev->dev.parent;
+	int i, j;
+
+	for (i = 0; i < priv->num_fqs; i++) {
+		priv->fq[i].ring.store =
+			dpaa_io_store_create(LDPAA_ETH_STORE_SIZE, dev);
+		if (unlikely(!priv->fq[i].ring.store)) {
+			netdev_err(net_dev, "dpaa_io_store_create() failed\n");
+			goto err_ring;
+		}
+	}
+
+	return 0;
+
+err_ring:
+	for (j = 0; j < i; j++)
+		dpaa_io_store_destroy(priv->fq[j].ring.store);
+
+	return -ENOMEM;
+}
+
+static void ldpaa_eth_free_rings(struct ldpaa_eth_priv *priv)
+{
+	int i;
+
+	for (i = 0; i < priv->num_fqs; i++)
+		dpaa_io_store_destroy(priv->fq[i].ring.store);
+}
+
+static int ldpaa_eth_netdev_init(struct net_device *net_dev)
+{
+	int err;
+	struct device *dev = net_dev->dev.parent;
+	struct ldpaa_eth_priv *priv = netdev_priv(net_dev);
+	uint8_t mac_addr[ETH_ALEN];
+
+	net_dev->netdev_ops = &ldpaa_eth_ops;
+
+	/* If the DPL contains all-0 mac_addr, set a random hardware address */
+	err = dpni_get_primary_mac_addr(priv->mc_io, priv->mc_token, mac_addr);
+	if (unlikely(err)) {
+		netdev_err(net_dev, "dpni_get_primary_mac_addr() failed (%d)",
+			   err);
+		return err;
+	}
+	if (is_zero_ether_addr(mac_addr)) {
+		/* Fills in net_dev->dev_addr, as required by
+		 * register_netdevice()
+		 */
+		eth_hw_addr_random(net_dev);
+		netdev_info(net_dev, "Replacing all-zero hwaddr with %pM",
+			    net_dev->dev_addr);
+		err = dpni_set_primary_mac_addr(priv->mc_io, priv->mc_token,
+						net_dev->dev_addr);
+		if (unlikely(err)) {
+			netdev_err(net_dev,
+				   "dpni_set_primary_mac_addr() failed (%d)\n",
+				   err);
+			return err;
+		}
+		/* Override NET_ADDR_RANDOM set by eth_hw_addr_random(); for all
+		 * practical purposes, this will be our "permanent" mac address,
+		 * at least until the next reboot. This move will also permit
+		 * register_netdevice() to properly fill up net_dev->perm_addr.
+		 */
+		net_dev->addr_assign_type = NET_ADDR_PERM;
+	} else {
+		/* NET_ADDR_PERM is default, all we have to do is
+		 * fill in the device addr.
+		 */
+		memcpy(net_dev->dev_addr, mac_addr, net_dev->addr_len);
+	}
+
+	/* Reserve enough space to align buffer as per hardware requirement;
+	 * NOTE: priv->tx_data_offset MUST be initialized at this point.
+	 */
+	net_dev->needed_headroom = LDPAA_ETH_NEEDED_HEADROOM(priv);
+
+	/* Our .ndo_init will be called herein */
+	err = register_netdev(net_dev);
+	if (err < 0) {
+		dev_err(dev, "register_netdev() = %d\n", err);
+		return err;
+	}
+
+	return 0;
+}
+
+static irqreturn_t dpni_irq0_handler(int irq_num, void *arg)
+{
+	return IRQ_WAKE_THREAD;
+}
+
+static irqreturn_t dpni_irq0_handler_thread(int irq_num, void *arg)
+{
+	int irq_index = DPNI_IRQ_INDEX;
+	uint32_t status, clear = 0;
+	struct device *dev = (struct device *)arg;
+	struct fsl_mc_device *dpni_dev = to_fsl_mc_device(dev);
+	struct fsl_mc_io *io = dpni_dev->mc_io;
+	uint16_t token = dpni_dev->mc_handle;
+	struct net_device *net_dev = dev_get_drvdata(dev);
+	struct dpni_link_state link_state;
+	int err;
+
+	/* Sanity check; TODO a bit of cleanup here */
+	if (WARN_ON(!dpni_dev || !dpni_dev->irqs || !dpni_dev->irqs[irq_index]))
+		goto out;
+	if (WARN_ON(dpni_dev->irqs[irq_index]->irq_number != irq_num))
+		goto out;
+
+	err = dpni_get_irq_status(io, token, irq_index, &status);
+	if (unlikely(err)) {
+		netdev_err(net_dev, "Can't get irq status (err %d)", err);
+		clear = 0xffffffff;
+		goto out;
+	}
+
+	if (status & DPNI_IRQ_EVENT_LINK_CHANGED) {
+		clear |= DPNI_IRQ_EVENT_LINK_CHANGED;
+
+		err = dpni_get_link_state(io, token, &link_state);
+		if (unlikely(err)) {
+			netdev_err(net_dev, "dpni_get_link_state err: %d", err);
+			goto out;
+		}
+		netdev_info(net_dev, "Link Event: state: %d", link_state.up);
+		WARN_ONCE(link_state.up > 1, "Garbage read into link_state");
+
+		if (link_state.up) {
+			netif_carrier_on(net_dev);
+			netif_tx_start_all_queues(net_dev);
+		} else {
+			netif_tx_stop_all_queues(net_dev);
+			netif_carrier_off(net_dev);
+		}
+	}
+
+out:
+	dpni_clear_irq_status(io, token, irq_index, clear);
+	return IRQ_HANDLED;
+}
+
+static int ldpaa_eth_setup_irqs(struct fsl_mc_device *ls_dev)
+{
+	int err = 0;
+	struct fsl_mc_device_irq *irq;
+	int irq_count = ls_dev->obj_desc.irq_count;
+	int irq_index = 0;
+	uint32_t mask = ~0x0u;
+
+	/* The only interrupt supported now is the link state notification. */
+	if (WARN_ON(irq_count != 1))
+		return -EINVAL;
+
+	irq = ls_dev->irqs[0];
+	err = devm_request_threaded_irq(&ls_dev->dev, irq->irq_number,
+					dpni_irq0_handler,
+					dpni_irq0_handler_thread,
+					IRQF_NO_SUSPEND | IRQF_ONESHOT,
+					dev_name(&ls_dev->dev), &ls_dev->dev);
+	if (err < 0) {
+		dev_err(&ls_dev->dev, "devm_request_threaded_irq(): %d", err);
+		return err;
+	}
+
+	err = dpni_set_irq(ls_dev->mc_io, ls_dev->mc_handle,
+			   irq_index, irq->msi_paddr,
+			   irq->msi_value, irq->irq_number);
+	if (err < 0) {
+		dev_err(&ls_dev->dev, "dpni_set_irq(): %d", err);
+		goto dpni_set_irq_err;
+	}
+
+	err = dpni_set_irq_mask(ls_dev->mc_io, ls_dev->mc_handle,
+				irq_index, mask);
+	if (err < 0) {
+		dev_err(&ls_dev->dev, "dpni_set_irq_mask(): %d", err);
+		goto dpni_set_irq_mask_err;
+	}
+
+	err = dpni_set_irq_enable(ls_dev->mc_io, ls_dev->mc_handle,
+				  irq_index, 1);
+	if (err < 0) {
+		dev_err(&ls_dev->dev, "dpni_set_irq_enable(): %d", err);
+		goto dpni_set_irq_enable_err;
+	}
+
+
+	return 0;
+
+dpni_set_irq_enable_err:
+dpni_set_irq_mask_err:
+dpni_set_irq_err:
+	devm_free_irq(&ls_dev->dev, irq->irq_number, &ls_dev->dev);
+	return err;
+}
+
+static void ldpaa_eth_napi_add(struct ldpaa_eth_priv *priv)
+{
+	int i, w;
+	struct ldpaa_eth_fq *fq;
+
+	for (i = 0; i < priv->num_fqs; i++) {
+		fq = &priv->fq[i];
+		/* TxConf must have precedence over Rx; this is one way of
+		 * doing so.
+		 * TODO this needs more testing & fine-tuning
+		 */
+		if (fq->type == LDPAA_TX_CONF_FQ)
+			w = LDPAA_ETH_TX_CONF_NAPI_WEIGHT;
+		else
+			w = LDPAA_ETH_RX_NAPI_WEIGHT;
+
+		netif_napi_add(priv->net_dev, &fq->napi, ldpaa_eth_poll, w);
+	}
+}
+
+static void ldpaa_eth_napi_del(struct ldpaa_eth_priv *priv)
+{
+	int i;
+	struct ldpaa_eth_fq *fq;
+
+	for (i = 0; i < priv->num_fqs; i++) {
+		fq = &priv->fq[i];
+		netif_napi_del(&fq->napi);
+	}
+}
+
+static int __cold
+ldpaa_eth_probe(struct fsl_mc_device *dpni_dev)
+{
+	struct device			*dev;
+	struct net_device		*net_dev = NULL;
+	struct ldpaa_eth_priv		*priv = NULL;
+	int				err = 0;
+	u8				bcast_addr[ETH_ALEN];
+
+	dev = &dpni_dev->dev;
+
+	/* Net device */
+	net_dev = alloc_etherdev_mq(sizeof(*priv), LDPAA_ETH_TX_QUEUES);
+	if (!net_dev) {
+		dev_err(dev, "alloc_etherdev_mq() failed\n");
+		return -ENOMEM;
+	}
+
+	SET_NETDEV_DEV(net_dev, dev);
+	dev_set_drvdata(dev, net_dev);
+
+	priv = netdev_priv(net_dev);
+	priv->net_dev = net_dev;
+	priv->msg_enable = netif_msg_init(debug, -1);
+
+	/* Obtain a MC portal */
+	err = fsl_mc_portal_allocate(dpni_dev, FSL_MC_IO_ATOMIC_CONTEXT_PORTAL,
+				     &priv->mc_io);
+	if (err) {
+		dev_err(dev, "MC portal allocation failed\n");
+		goto err_portal_alloc;
+	}
+
+	err = fsl_mc_allocate_irqs(dpni_dev);
+	if (err < 0)
+		/* FIXME: add error label */
+		return -EINVAL;
+
+	/* DPNI initialization */
+	err = ldpaa_dpni_setup(dpni_dev);
+	if (err < 0)
+		goto err_dpni_setup;
+
+	/* FQs and NAPI */
+	ldpaa_eth_setup_fqs(priv);
+	ldpaa_eth_napi_add(priv);
+
+	/* DPIO */
+	err = ldpaa_dpio_setup(priv);
+	if (err)
+		goto err_dpio_setup;
+
+	/* DPBP */
+	priv->buf_count = alloc_percpu(*priv->buf_count);
+	if (!priv->buf_count) {
+		dev_err(dev, "alloc_percpu() failed\n");
+		err = -ENOMEM;
+		goto err_alloc_bp_count;
+	}
+	err = ldpaa_dpbp_setup(priv);
+	if (err)
+		goto err_dpbp_setup;
+
+	/* DPNI binding to DPIO and DPBPs */
+	err = ldpaa_dpni_bind(priv);
+	if (err)
+		goto err_bind;
+
+	/* Percpu statistics */
+	priv->percpu_stats = alloc_percpu(*priv->percpu_stats);
+	if (!priv->percpu_stats) {
+		dev_err(dev, "alloc_percpu(percpu_stats) failed\n");
+		err = -ENOMEM;
+		goto err_alloc_percpu_stats;
+	}
+	priv->percpu_extras = alloc_percpu(*priv->percpu_extras);
+	if (!priv->percpu_extras) {
+		dev_err(dev, "alloc_percpu(percpu_extras) failed\n");
+		err = -ENOMEM;
+		goto err_alloc_percpu_extras;
+	}
+
+	snprintf(net_dev->name, IFNAMSIZ, "ni%d", dpni_dev->obj_desc.id);
+	if (!dev_valid_name(net_dev->name)) {
+		dev_warn(&net_dev->dev,
+			 "netdevice name \"%s\" cannot be used, reverting to default..\n",
+			 net_dev->name);
+		dev_alloc_name(net_dev, "eth%d");
+		dev_warn(&net_dev->dev, "using name \"%s\"\n", net_dev->name);
+	}
+
+	err = ldpaa_eth_netdev_init(net_dev);
+	if (err)
+		goto err_netdev_init;
+
+	/* Explicitly add the broadcast address to the MAC filtering table;
+	 * the MC won't do that for us.
+	 */
+	eth_broadcast_addr(bcast_addr);
+	err = dpni_add_mac_addr(priv->mc_io, priv->mc_token, bcast_addr);
+	if (err) {
+		netdev_warn(net_dev,
+			    "dpni_add_mac_addr() failed with code %d\n", err);
+		/* Won't return an error; at least, we'd have egress traffic */
+	}
+
+	/* Configure checksum offload based on current interface flags */
+	err = ldpaa_eth_set_rx_csum(priv,
+				    !!(net_dev->features & NETIF_F_RXCSUM));
+	if (unlikely(err))
+		goto err_csum;
+
+	err = ldpaa_eth_set_tx_csum(priv,
+				    !!(net_dev->features &
+				    (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM)));
+	if (unlikely(err))
+		goto err_csum;
+
+	err = ldpaa_eth_alloc_rings(priv);
+	if (unlikely(err))
+		goto err_alloc_rings;
+
+	net_dev->ethtool_ops = &ldpaa_ethtool_ops;
+
+	err = ldpaa_eth_setup_irqs(dpni_dev);
+	if (unlikely(err)) {
+		netdev_err(net_dev, "ERROR %d setting up interrupts", err);
+		/* fsl_mc_teardown_irqs() was already called, nothing to undo */
+		goto err_setup_irqs;
+	}
+
+	dev_info(dev, "ldpaa ethernet: Probed interface %s\n", net_dev->name);
+	return 0;
+
+err_setup_irqs:
+	ldpaa_eth_free_rings(priv);
+err_alloc_rings:
+err_csum:
+	unregister_netdev(net_dev);
+err_netdev_init:
+	free_percpu(priv->percpu_extras);
+err_alloc_percpu_extras:
+	free_percpu(priv->percpu_stats);
+err_alloc_percpu_stats:
+err_bind:
+	ldpaa_dpbp_free(priv);
+err_dpbp_setup:
+	free_percpu(priv->buf_count);
+err_alloc_bp_count:
+	ldpaa_dpio_free(priv);
+err_dpio_setup:
+	ldpaa_eth_napi_del(priv);
+	dpni_close(priv->mc_io, priv->mc_token);
+err_dpni_setup:
+	fsl_mc_portal_free(priv->mc_io);
+err_portal_alloc:
+	dev_set_drvdata(dev, NULL);
+	free_netdev(net_dev);
+
+	return err;
+}
+
+static int __cold
+ldpaa_eth_remove(struct fsl_mc_device *ls_dev)
+{
+	struct device		*dev;
+	struct net_device	*net_dev;
+	struct ldpaa_eth_priv *priv;
+
+	dev = &ls_dev->dev;
+	net_dev = dev_get_drvdata(dev);
+	priv = netdev_priv(net_dev);
+	ldpaa_dpio_free(priv);
+
+	unregister_netdev(net_dev);
+
+	ldpaa_eth_free_rings(priv);
+	ldpaa_eth_napi_del(priv);
+	ldpaa_dpbp_free(priv);
+	ldpaa_dpni_free(priv);
+
+	fsl_mc_portal_free(priv->mc_io);
+
+	free_percpu(priv->percpu_stats);
+	free_percpu(priv->percpu_extras);
+
+	dev_set_drvdata(dev, NULL);
+	free_netdev(net_dev);
+
+	return 0;
+}
+
+static const struct fsl_mc_device_match_id ldpaa_eth_match_id_table[] = {
+	{
+		.vendor = FSL_MC_VENDOR_FREESCALE,
+		.obj_type = "dpni",
+		.ver_major = DPNI_VER_MAJOR,
+		.ver_minor = DPNI_VER_MINOR
+	},
+	{ .vendor = 0x0 }
+};
+
+static struct fsl_mc_driver ldpaa_eth_driver = {
+	.driver = {
+		.name		= KBUILD_MODNAME,
+		.owner		= THIS_MODULE,
+	},
+	.probe		= ldpaa_eth_probe,
+	.remove		= ldpaa_eth_remove,
+	.match_id_table = ldpaa_eth_match_id_table
+};
+
+module_fsl_mc_driver(ldpaa_eth_driver);
diff --git a/drivers/staging/fsl-dpaa2/ethernet/ldpaa_eth.h b/drivers/staging/fsl-dpaa2/ethernet/ldpaa_eth.h
new file mode 100644
index 0000000..ebf96fb
--- /dev/null
+++ b/drivers/staging/fsl-dpaa2/ethernet/ldpaa_eth.h
@@ -0,0 +1,262 @@
+/* Copyright 2014 Freescale Semiconductor Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *	 notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *	 notice, this list of conditions and the following disclaimer in the
+ *	 documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *	 names of its contributors may be used to endorse or promote products
+ *	 derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef __LDPAA_ETH_H
+#define __LDPAA_ETH_H
+
+#include <linux/netdevice.h>
+#include <linux/if_vlan.h>
+#include "../../fsl-mc/include/fsl_dpaa_io.h"
+#include "../../fsl-mc/include/fsl_dpaa_fd.h"
+#include "../../fsl-mc/include/dpbp.h"
+#include "../../fsl-mc/include/dpni.h"
+#include "../../fsl-mc/include/dpni-cmd.h"
+
+#include "ldpaa_eth_trace.h"
+
+/* TODO : how many queues here? NR_CPUS? */
+#define LDPAA_ETH_TX_QUEUES		1
+#define LDPAA_ETH_STORE_SIZE		16
+/* NAPI weights *must* be a multiple of 16, i.e. the store size. */
+#define LDPAA_ETH_RX_NAPI_WEIGHT	64
+#define LDPAA_ETH_TX_CONF_NAPI_WEIGHT   256
+
+/* TODO: Sort of arbitrary values for bpools, but we'll need to tune.
+ * Supply enough buffers to reassembly several fragmented datagrams. Making it a
+ * multiple of 7, because we're doing ldpaa_bp_add_7(). This is a per-CPU
+ * counter.
+ */
+#define LDPAA_ETH_NUM_BUFS		(300 * 7)
+#define LDPAA_ETH_REFILL_THRESH		(LDPAA_ETH_NUM_BUFS * 5 / 6)
+#define LDPAA_ETH_RX_BUFFER_SIZE	2048
+
+/* Maximum receive frame size is 64K */
+#define LDPAA_ETH_MAX_SG_ENTRIES	((64 * 1024) / LDPAA_ETH_RX_BUFFER_SIZE)
+
+/* Maximum acceptable MTU value. It is in direct relation with the MC-enforced
+ * Max Frame Length (currently 10k).
+ */
+#define LDPAA_ETH_MAX_MTU	(10000 - VLAN_ETH_HLEN)
+/* Convert L3 MTU to L2 MFL */
+#define LDPAA_ETH_L2_MAX_FRM(mtu)	(mtu + VLAN_ETH_HLEN)
+
+/* Hardware requires alignment for ingress/egress buffer addresses
+ * and ingress buffer lengths.
+ */
+#define LDPAA_ETH_BUF_ALIGN		64
+#define LDPAA_ETH_NEEDED_HEADROOM(p_priv) \
+	((p_priv)->tx_data_offset + LDPAA_ETH_BUF_ALIGN)
+
+/* So far we're only accomodating a skb backpointer in the frame's
+ * software annotation, but the hardware options are either 0 or 64.
+ */
+#define LDPAA_ETH_SWA_SIZE		64
+
+/* Annotation valid bits in FD FRC */
+#define LDPAA_FD_FRC_FASV		0x8000
+#define LDPAA_FD_FRC_FAEADV		0x4000
+#define LDPAA_FD_FRC_FAPRV		0x2000
+#define LDPAA_FD_FRC_FAIADV		0x1000
+#define LDPAA_FD_FRC_FASWOV		0x0800
+#define LDPAA_FD_FRC_FAICFDV		0x0400
+
+/* Annotation bits in FD CTRL */
+#define LDPAA_FD_CTRL_ASAL		0x00020000	/* ASAL = 128 */
+#define LDPAA_FD_CTRL_PTA		0x00800000
+#define LDPAA_FD_CTRL_PTV1		0x00400000
+
+/* TODO: we may want to move this and other WRIOP related defines
+ * to a separate header
+ */
+/* Frame annotation status */
+struct ldpaa_fas {
+	u8 reserved;
+	u8 ppid;
+	__le16 ifpid;
+	__le32 status;
+} __packed;
+
+/* Debug frame, otherwise supposed to be discarded */
+#define LDPAA_ETH_FAS_DISC		0x80000000
+/* MACSEC frame */
+#define LDPAA_ETH_FAS_MS		0x40000000
+#define LDPAA_ETH_FAS_PTP		0x08000000
+/* Ethernet multicast frame */
+#define LDPAA_ETH_FAS_MC		0x04000000
+/* Ethernet broadcast frame */
+#define LDPAA_ETH_FAS_BC		0x02000000
+#define LDPAA_ETH_FAS_KSE		0x00040000
+#define LDPAA_ETH_FAS_EOFHE		0x00020000
+#define LDPAA_ETH_FAS_MNLE		0x00010000
+#define LDPAA_ETH_FAS_TIDE		0x00008000
+#define LDPAA_ETH_FAS_PIEE		0x00004000
+/* Frame length error */
+#define LDPAA_ETH_FAS_FLE		0x00002000
+/* Frame physical error; our favourite pastime */
+#define LDPAA_ETH_FAS_FPE		0x00001000
+#define LDPAA_ETH_FAS_PTE		0x00000080
+#define LDPAA_ETH_FAS_ISP		0x00000040
+#define LDPAA_ETH_FAS_PHE		0x00000020
+#define LDPAA_ETH_FAS_BLE		0x00000010
+/* L3 csum validation performed */
+#define LDPAA_ETH_FAS_L3CV		0x00000008
+/* L3 csum error */
+#define LDPAA_ETH_FAS_L3CE		0x00000004
+/* L4 csum validation performed */
+#define LDPAA_ETH_FAS_L4CV		0x00000002
+/* L4 csum error */
+#define LDPAA_ETH_FAS_L4CE		0x00000001
+/* These bits always signal errors */
+#define LDPAA_ETH_RX_ERR_MASK		(LDPAA_ETH_FAS_DISC	| \
+					 LDPAA_ETH_FAS_KSE	| \
+					 LDPAA_ETH_FAS_EOFHE	| \
+					 LDPAA_ETH_FAS_MNLE	| \
+					 LDPAA_ETH_FAS_TIDE	| \
+					 LDPAA_ETH_FAS_PIEE	| \
+					 LDPAA_ETH_FAS_FLE	| \
+					 LDPAA_ETH_FAS_FPE	| \
+					 LDPAA_ETH_FAS_PTE	| \
+					 LDPAA_ETH_FAS_ISP	| \
+					 LDPAA_ETH_FAS_PHE	| \
+					 LDPAA_ETH_FAS_BLE	| \
+					 LDPAA_ETH_FAS_L3CE	| \
+					 LDPAA_ETH_FAS_L4CE)
+/* Unsupported features in the ingress */
+#define LDPAA_ETH_RX_UNSUPP_MASK	LDPAA_ETH_FAS_MS
+/* TODO trim down the bitmask; not all of them apply to Tx-confirm */
+#define LDPAA_ETH_TXCONF_ERR_MASK	(LDPAA_ETH_FAS_KSE	| \
+					 LDPAA_ETH_FAS_EOFHE	| \
+					 LDPAA_ETH_FAS_MNLE	| \
+					 LDPAA_ETH_FAS_TIDE)
+
+/* TODO Temporarily, until dpni_clear_mac_table() is implemented */
+struct ldpaa_eth_mac_list {
+	u8 addr[ETH_ALEN];
+	struct list_head list;
+};
+
+/* Driver statistics, other than those in struct rtnl_link_stats64.
+ * These are usually collected per-CPU and aggregated by ethtool.
+ */
+struct ldpaa_eth_stats {
+	__u64	tx_conf_frames;
+	__u64	tx_conf_bytes;
+	__u64	tx_sg_frames;
+	__u64	tx_sg_bytes;
+	__u64	rx_sg_frames;
+	__u64	rx_sg_bytes;
+	/* Enqueues retried due to portal busy */
+	__u64	tx_portal_busy;
+};
+/* Per-FQ statistics */
+struct ldpaa_eth_fq_stats {
+	/* Volatile dequeues retried due to portal busy */
+	__u64	rx_portal_busy;
+	/* Number of FQDANs from Rx queues; useful to estimate avg NAPI len */
+	__u64	rx_fqdan;
+	/* Number of FQDANs from Tx Conf queues */
+	__u64	tx_conf_fqdan;
+};
+
+struct ldpaa_eth_ring {
+	struct dpaa_io_store *store;
+};
+
+/* Maximum number of Rx queues serviced by a CPU */
+#define LDPAA_ETH_MAX_RX_QUEUES		16
+#define LDPAA_ETH_MAX_TX_QUEUES		1
+#define LDPAA_ETH_MAX_QUEUES	\
+	(LDPAA_ETH_MAX_RX_QUEUES + LDPAA_ETH_MAX_TX_QUEUES)
+
+enum ldpaa_eth_fq_type {
+	LDPAA_RX_FQ = 0,
+	LDPAA_TX_CONF_FQ
+};
+
+struct ldpaa_eth_priv;
+
+struct ldpaa_eth_fq {
+	uint32_t fqid;
+	uint16_t flowid;
+	struct dpaa_io_notification_ctx nctx;
+	/* FQs are the current source of interrupts (notifications), so it
+	 * makes sense to have napi per FQ.
+	 */
+	struct napi_struct napi;
+	bool has_frames;
+	struct ldpaa_eth_ring ring;
+	enum ldpaa_eth_fq_type type;
+	/* Empty line to appease checkpatch */
+	void (*consume)(struct ldpaa_eth_priv *, const struct dpaa_fd *);
+	struct ldpaa_eth_priv *netdev_priv;	/* backpointer */
+	struct ldpaa_eth_fq_stats stats;
+};
+
+struct ldpaa_eth_priv {
+	struct net_device *net_dev;
+
+	uint8_t num_fqs;
+	/* First queue is tx conf, the rest are rx */
+	struct ldpaa_eth_fq fq[LDPAA_ETH_MAX_QUEUES];
+
+	int dpni_id;
+	struct dpni_attr dpni_attrs;
+	/* Insofar as the MC is concerned, we're using one layout on all 3 types
+	 * of buffers (Rx, Tx, Tx-Conf).
+	 */
+	struct dpni_buffer_layout buf_layout;
+	uint16_t tx_data_offset;
+
+	/* TODO: Support multiple BPs */
+	struct fsl_mc_device *dpbp_dev;
+	struct dpbp_attr dpbp_attrs;
+
+	int __percpu *buf_count;
+
+	uint16_t tx_qdid;
+	struct fsl_mc_io *mc_io;
+	struct dentry *debugfs_file;
+
+	/* Standard statistics */
+	struct rtnl_link_stats64 __percpu *percpu_stats;
+	/* Extra stats, in addition to the ones known by the kernel */
+	struct ldpaa_eth_stats __percpu *percpu_extras;
+	uint32_t msg_enable;	/* net_device message level */
+
+	uint16_t mc_token;
+
+	struct ldpaa_eth_percpu __percpu *ppriv;
+	uint8_t rx_dist_size;
+};
+
+extern const struct ethtool_ops ldpaa_ethtool_ops;
+
+#endif	/* __LDPAA_H */
diff --git a/drivers/staging/fsl-dpaa2/ethernet/ldpaa_eth_trace.h b/drivers/staging/fsl-dpaa2/ethernet/ldpaa_eth_trace.h
new file mode 100644
index 0000000..658812e
--- /dev/null
+++ b/drivers/staging/fsl-dpaa2/ethernet/ldpaa_eth_trace.h
@@ -0,0 +1,130 @@
+/* Copyright 2014 Freescale Semiconductor Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *	 notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *	 notice, this list of conditions and the following disclaimer in the
+ *	 documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *	 names of its contributors may be used to endorse or promote products
+ *	 derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM	ldpaa_eth
+
+#if !defined(_LDPAA_ETH_TRACE_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _LDPAA_ETH_TRACE_H
+
+#include <linux/skbuff.h>
+#include <linux/netdevice.h>
+#include "ldpaa_eth.h"
+#include <linux/tracepoint.h>
+
+#define TR_FMT "[%s] fd: addr=0x%llx, len=%u, off=%u"
+
+/* This is used to declare a class of events.
+ * individual events of this type will be defined below.
+ */
+
+/* Store details about a frame descriptor */
+DECLARE_EVENT_CLASS(ldpaa_eth_fd,
+		    /* Trace function prototype */
+		    TP_PROTO(struct net_device *netdev,
+			     const struct dpaa_fd *fd),
+
+		    /* Repeat argument list here */
+		    TP_ARGS(netdev, fd),
+
+		    /* A structure containing the relevant information we want
+		     * to record. Declare name and type for each normal element,
+		     * name, type and size for arrays. Use __string for variable
+		     * length strings.
+		     */
+		    TP_STRUCT__entry(
+				     __field(u64, fd_addr)
+				     __field(u32, fd_len)
+				     __field(u16, fd_offset)
+				     __string(name, netdev->name)
+		    ),
+
+		    /* The function that assigns values to the above declared
+		     * fields
+		     */
+		    TP_fast_assign(
+				   __entry->fd_addr = ldpaa_fd_get_addr(fd);
+				   __entry->fd_len = ldpaa_fd_get_len(fd);
+				   __entry->fd_offset = ldpaa_fd_get_offset(fd);
+				   __assign_str(name, netdev->name);
+		    ),
+
+		    /* This is what gets printed when the trace event is
+		     * triggered.
+		     */
+		    /* TODO: print the status using __print_flags() */
+		    TP_printk(TR_FMT,
+			      __get_str(name),
+			      __entry->fd_addr,
+			      __entry->fd_len,
+			      __entry->fd_offset)
+);
+
+/* Now declare events of the above type. Format is:
+ * DEFINE_EVENT(class, name, proto, args), with proto and args same as for class
+ */
+
+/* Tx (egress) fd */
+DEFINE_EVENT(ldpaa_eth_fd, ldpaa_tx_fd,
+	     TP_PROTO(struct net_device *netdev,
+		      const struct dpaa_fd *fd),
+
+	     TP_ARGS(netdev, fd)
+);
+
+/* Rx fd */
+DEFINE_EVENT(ldpaa_eth_fd, ldpaa_rx_fd,
+	     TP_PROTO(struct net_device *netdev,
+		      const struct dpaa_fd *fd),
+
+	     TP_ARGS(netdev, fd)
+);
+
+/* Tx confirmation fd */
+DEFINE_EVENT(ldpaa_eth_fd, ldpaa_tx_conf_fd,
+	     TP_PROTO(struct net_device *netdev,
+		      const struct dpaa_fd *fd),
+
+	     TP_ARGS(netdev, fd)
+);
+
+/* If only one event of a certain type needs to be declared, use TRACE_EVENT().
+ * The syntax is the same as for DECLARE_EVENT_CLASS().
+ */
+
+#endif /* _LDPAA_ETH_TRACE_H */
+
+/* This must be outside ifdef _LDPAA_ETH_TRACE_H */
+#undef TRACE_INCLUDE_PATH
+#define TRACE_INCLUDE_PATH .
+#undef TRACE_INCLUDE_FILE
+#define TRACE_INCLUDE_FILE	ldpaa_eth_trace
+#include <trace/define_trace.h>
diff --git a/drivers/staging/fsl-dpaa2/ethernet/ldpaa_ethtool.c b/drivers/staging/fsl-dpaa2/ethernet/ldpaa_ethtool.c
new file mode 100644
index 0000000..ac1941a
--- /dev/null
+++ b/drivers/staging/fsl-dpaa2/ethernet/ldpaa_ethtool.c
@@ -0,0 +1,258 @@
+/* Copyright 2014 Freescale Semiconductor Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *	 notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *	 notice, this list of conditions and the following disclaimer in the
+ *	 documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *	 names of its contributors may be used to endorse or promote products
+ *	 derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "../../fsl-mc/include/dpni.h"	/* DPNI_LINK_OPT_* */
+#include "ldpaa_eth.h"
+
+/* To be kept in sync with 'enum dpni_counter' */
+char ldpaa_ethtool_stats[][ETH_GSTRING_LEN] = {
+	"rx frames",
+	"rx bytes",
+	"rx frames dropped",
+	"rx err frames",
+	"rx mcast frames",
+	"rx mcast bytes",
+	"rx bcast frames",
+	"rx bcast bytes",
+	"tx frames",
+	"tx bytes",
+	"tx err frames",
+};
+/* To be kept in sync with 'struct ldpaa_eth_stats' */
+char ldpaa_ethtool_extras[][ETH_GSTRING_LEN] = {
+	/* per-cpu stats */
+
+	"tx conf frames",
+	"tx conf bytes",
+	"tx sg frames",
+	"tx sg bytes",
+	"rx sg frames",
+	"rx sg bytes",
+	/* how many times we had to retry the enqueue command */
+	"tx portal busy",
+
+	/* per-FQ stats */
+
+	/* How many times we had to retry the volatile dequeue command */
+	"rx portal busy",
+	"rx fqdan",
+	"tx conf fqdan",
+#ifdef CONFIG_FSL_QBMAN_DEBUG
+	"rx pending frames",
+	"rx pending bytes",
+	"tx conf pending frames",
+	"tx conf pending bytes",
+#endif
+};
+#define LDPAA_ETH_NUM_EXTRA_STATS	ARRAY_SIZE(ldpaa_ethtool_extras)
+
+static void __cold ldpaa_get_drvinfo(struct net_device *net_dev,
+				     struct ethtool_drvinfo *drvinfo)
+{
+	strlcpy(drvinfo->driver, KBUILD_MODNAME, sizeof(drvinfo->driver));
+	strlcpy(drvinfo->version, VERSION, sizeof(drvinfo->version));
+	strlcpy(drvinfo->fw_version, "N/A", sizeof(drvinfo->fw_version));
+	strlcpy(drvinfo->bus_info, dev_name(net_dev->dev.parent->parent),
+		sizeof(drvinfo->bus_info));
+}
+
+static uint32_t __cold ldpaa_get_msglevel(struct net_device *net_dev)
+{
+	return ((struct ldpaa_eth_priv *)netdev_priv(net_dev))->msg_enable;
+}
+
+static void __cold ldpaa_set_msglevel(struct net_device *net_dev,
+				      uint32_t msg_enable)
+{
+	((struct ldpaa_eth_priv *)netdev_priv(net_dev))->msg_enable =
+					msg_enable;
+}
+
+static int __cold ldpaa_get_settings(struct net_device *net_dev,
+				     struct ethtool_cmd *cmd)
+{
+	struct dpni_link_state state = {0};
+	int err = 0;
+	struct ldpaa_eth_priv *priv = netdev_priv(net_dev);
+
+	err = dpni_get_link_state(priv->mc_io, priv->mc_token, &state);
+	if (unlikely(err)) {
+		netdev_err(net_dev, "ERROR %d getting link state", err);
+		goto out;
+	}
+
+	if (state.options & DPNI_LINK_OPT_AUTONEG)
+		cmd->autoneg = AUTONEG_ENABLE;
+	if (!(state.options & DPNI_LINK_OPT_HALF_DUPLEX))
+		cmd->duplex = DUPLEX_FULL;
+	ethtool_cmd_speed_set(cmd, state.rate);
+
+out:
+	return err;
+}
+
+static int __cold ldpaa_set_settings(struct net_device *net_dev,
+				     struct ethtool_cmd *cmd)
+{
+	struct dpni_link_cfg cfg = {0};
+	struct ldpaa_eth_priv *priv = netdev_priv(net_dev);
+	int err = 0;
+
+	netdev_info(net_dev, "Setting link parameters...");
+
+	cfg.rate = ethtool_cmd_speed(cmd);
+	if (cmd->autoneg == AUTONEG_ENABLE)
+		cfg.options |= DPNI_LINK_OPT_AUTONEG;
+	else
+		cfg.options &= ~DPNI_LINK_OPT_AUTONEG;
+	if (cmd->duplex  == DUPLEX_HALF)
+		cfg.options |= DPNI_LINK_OPT_HALF_DUPLEX;
+	else
+		cfg.options &= ~DPNI_LINK_OPT_HALF_DUPLEX;
+
+	err = dpni_set_link_cfg(priv->mc_io, priv->mc_token, &cfg);
+	if (unlikely(err))
+		netdev_err(net_dev, "ERROR %d setting link cfg", err);
+
+	return err;
+}
+
+static void ldpaa_get_strings(struct net_device *netdev, u32 stringset,
+			      u8 *data)
+{
+	u8 *p = data;
+	int i;
+
+	switch (stringset) {
+	case ETH_SS_STATS:
+		for (i = 0; i < DPNI_CNT_NUM_STATS; i++) {
+			strlcpy(p, ldpaa_ethtool_stats[i], ETH_GSTRING_LEN);
+			p += ETH_GSTRING_LEN;
+		}
+		for (i = 0; i < LDPAA_ETH_NUM_EXTRA_STATS; i++) {
+			strlcpy(p, ldpaa_ethtool_extras[i], ETH_GSTRING_LEN);
+			p += ETH_GSTRING_LEN;
+		}
+		break;
+	}
+}
+
+static int ldpaa_get_sset_count(struct net_device *net_dev, int sset)
+{
+	switch (sset) {
+	case ETH_SS_STATS: /* ethtool_get_stats(), ethtool_get_drvinfo() */
+		return DPNI_CNT_NUM_STATS + LDPAA_ETH_NUM_EXTRA_STATS;
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+/** Fill in hardware counters, as returned by the MC firmware.
+ */
+static void ldpaa_get_ethtool_stats(struct net_device *net_dev,
+				    struct ethtool_stats *stats,
+				    u64 *data)
+{
+	int i; /* Current index in the data array */
+	int j, k, err;
+
+#ifdef CONFIG_FSL_QBMAN_DEBUG
+	uint32_t fcnt, bcnt;
+	uint32_t fcnt_rx_total = 0, fcnt_tx_total = 0;
+	uint32_t bcnt_rx_total = 0, bcnt_tx_total = 0;
+#endif
+	struct ldpaa_eth_priv *priv = netdev_priv(net_dev);
+	struct ldpaa_eth_stats *extras;
+	struct ldpaa_eth_fq_stats *fq_stats;
+
+	memset(data, 0,
+	       sizeof(u64) * (DPNI_CNT_NUM_STATS + LDPAA_ETH_NUM_EXTRA_STATS));
+
+	/* Print standard counters, from DPNI statistics */
+	for (i = 0; i < DPNI_CNT_NUM_STATS; i++) {
+		err = dpni_get_counter(priv->mc_io, priv->mc_token, i,
+				       data + i);
+		if (err != 0)
+			netdev_warn(net_dev, "Err %d getting DPNI counter %d",
+				    err, i);
+	}
+
+	/* Print per-cpu extra stats */
+	for_each_online_cpu(k) {
+		extras = per_cpu_ptr(priv->percpu_extras, k);
+		for (j = 0; j < sizeof(*extras) / sizeof(__u64); j++)
+			*((__u64 *)data + i + j) += *((__u64 *)extras + j);
+	}
+	i += j;
+
+	for (j = 0; j < priv->num_fqs; j++) {
+		fq_stats = &priv->fq[j].stats;
+		for (k = 0; k < sizeof(*fq_stats) / sizeof(__u64); k++)
+			*((__u64 *)data + i + k) += *((__u64 *)fq_stats + k);
+	}
+	i += k;
+
+#ifdef CONFIG_FSL_QBMAN_DEBUG
+	for (j = 0; j < priv->num_fqs; j++) {
+		/* Print FQ instantaneous counts */
+		err = dpaa_io_query_fq_count(NULL, priv->fq[j].fqid,
+					     &fcnt, &bcnt);
+		if (unlikely(err)) {
+			netdev_warn(net_dev, "FQ query error %d", err);
+			return;
+		}
+
+		if (priv->fq[j].type == LDPAA_TX_CONF_FQ) {
+			fcnt_tx_total += fcnt;
+			bcnt_tx_total += bcnt;
+		} else {
+			fcnt_rx_total += fcnt;
+			bcnt_rx_total += bcnt;
+		}
+	}
+	*(data + i++ + DPNI_CNT_NUM_STATS) = fcnt_rx_total;
+	*(data + i++ + DPNI_CNT_NUM_STATS) = bcnt_rx_total;
+	*(data + i++ + DPNI_CNT_NUM_STATS) = fcnt_tx_total;
+	*(data + i++ + DPNI_CNT_NUM_STATS) = bcnt_tx_total;
+#endif
+}
+
+const struct ethtool_ops ldpaa_ethtool_ops = {
+	.get_drvinfo = ldpaa_get_drvinfo,
+	.get_msglevel = ldpaa_get_msglevel,
+	.set_msglevel = ldpaa_set_msglevel,
+	.get_link = ethtool_op_get_link,
+	.get_settings = ldpaa_get_settings,
+	.set_settings = ldpaa_set_settings,
+	.get_sset_count = ldpaa_get_sset_count,
+	.get_ethtool_stats = ldpaa_get_ethtool_stats,
+	.get_strings = ldpaa_get_strings,
+};
-- 
2.8.1

