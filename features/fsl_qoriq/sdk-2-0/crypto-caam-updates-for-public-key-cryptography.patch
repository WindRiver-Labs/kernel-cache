From 47d86eb7c9b7e2c51dccac9f59bee0a7f4a8272f Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Horia=20Geant=C4=83?= <horia.geanta@nxp.com>
Date: Wed, 20 Jan 2016 12:15:03 +0200
Subject: [PATCH 0555/1429] crypto: caam - updates for public key cryptography
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

This is a squash of the following commits:
67659d841becfc172216df76f058db25588f87cf "caam driver updates for public key cryptography"
3975ac785ff22dd1e15a29eb28885002a1bd00f2 "crypto: caam - Fix crash in caampkc module_init"
followed by merge conflicts resolution.

"CAAM driver updates as per public key infrastructure changes in cryptoAPI
RSA, DSA, ECDSA are support as part of Public Key Crypto Operations."

Signed-off-by: Yashpal Dutta <yashpal.dutta@freescale.com>
Signed-off-by: Nitesh Lal <nitesh.lal@nxp.com>
Signed-off-by: Horia GeantÄƒ <horia.geanta@nxp.com>
[Original patch taken from QorIQ-SDK-V2.0-20160527-yocto]
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 drivers/crypto/caam/Kconfig       |   12 +
 drivers/crypto/caam/Makefile      |    3 +
 drivers/crypto/caam/caampkc.c     | 1191 +++++++++++++++++++++++++++++++++++++
 drivers/crypto/caam/desc.h        |    5 +-
 drivers/crypto/caam/desc_constr.h |    3 +
 drivers/crypto/caam/intern.h      |    3 +-
 drivers/crypto/caam/key_gen.h     |    6 +-
 drivers/crypto/caam/pdb.h         |  175 +++++-
 drivers/crypto/caam/pkc_desc.c    |  253 ++++++++
 drivers/crypto/caam/pkc_desc.h    |  201 +++++++
 10 files changed, 1824 insertions(+), 28 deletions(-)
 create mode 100644 drivers/crypto/caam/caampkc.c
 create mode 100644 drivers/crypto/caam/pkc_desc.c
 create mode 100644 drivers/crypto/caam/pkc_desc.h

diff --git a/drivers/crypto/caam/Kconfig b/drivers/crypto/caam/Kconfig
index 7fee41b..d186e2f 100644
--- a/drivers/crypto/caam/Kconfig
+++ b/drivers/crypto/caam/Kconfig
@@ -87,6 +87,18 @@ config CRYPTO_DEV_FSL_CAAM_CRYPTO_API
 	  To compile this as a module, choose M here: the module
 	  will be called caamalg.
 
+config FSL_CAAM_PKC_SUPPORT
+	tristate "Public Key Cryptography Support in CAAM driver"
+	depends on CRYPTO_DEV_FSL_CAAM_CRYPTO_API
+	default y
+	help
+	  Selecting this will allow SEC Public key support for
+	  RSA, DSA, DH, ECDH, ECDSA. Supported operations includes
+	  Keygen, Sign and Verify.
+
+	  To compile this as a module, choose M here: the module
+	  will be called caam_pkc.
+
 config CRYPTO_DEV_FSL_CAAM_AHASH_API
 	tristate "Register hash algorithm implementations with Crypto API"
 	depends on CRYPTO_DEV_FSL_CAAM && CRYPTO_DEV_FSL_CAAM_JR
diff --git a/drivers/crypto/caam/Makefile b/drivers/crypto/caam/Makefile
index 550758a..6cf38b4 100644
--- a/drivers/crypto/caam/Makefile
+++ b/drivers/crypto/caam/Makefile
@@ -13,3 +13,6 @@ obj-$(CONFIG_CRYPTO_DEV_FSL_CAAM_RNG_API) += caamrng.o
 
 caam-objs := ctrl.o
 caam_jr-objs := jr.o key_gen.o error.o
+
+obj-$(CONFIG_FSL_CAAM_PKC_SUPPORT) += caam_pkc.o
+caam_pkc-objs := caampkc.o pkc_desc.o
diff --git a/drivers/crypto/caam/caampkc.c b/drivers/crypto/caam/caampkc.c
new file mode 100644
index 0000000..c827f55
--- /dev/null
+++ b/drivers/crypto/caam/caampkc.c
@@ -0,0 +1,1191 @@
+/*
+ * \file - caampkc.c
+ * \brief - Freescale FSL CAAM support for Public Key Cryptography
+ *
+ * Author: Yashpal Dutta <yashpal.dutta@freescale.com>
+ *
+ * Copyright 2012 Freescale Semiconductor, Inc.
+ *
+ * There is no shared descriptor for PKC but Job descriptor must carry
+ * all the desired key parameters, input and output pointers
+ *
+ */
+
+#include "pkc_desc.h"
+
+/* PKC Priority */
+#define CAAM_PKC_PRIORITY 3000
+
+#ifdef DEBUG
+/* for print_hex_dumps with line references */
+#define debug(format, arg...) pr_debug(format, arg)
+#else
+#define debug(format, arg...)
+#endif
+
+/* Internal context of CAAM driver. May carry session specific
+     PKC information like key */
+struct caam_pkc_context_s {
+	/* Job Ring Device pointer for current request */
+	struct device *dev;
+};
+
+static void rsa_unmap(struct device *dev,
+		      struct rsa_edesc *edesc, struct pkc_request *req)
+{
+	switch (req->type) {
+	case RSA_PUB:
+		{
+			struct rsa_pub_req_s *pub_req = &req->req_u.rsa_pub_req;
+			struct rsa_pub_desc_s *rsa_pub_desc =
+			    (struct rsa_pub_desc_s *)edesc->hw_desc;
+
+			dma_unmap_single(dev, rsa_pub_desc->n_dma,
+					 pub_req->n_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, rsa_pub_desc->e_dma,
+					 pub_req->e_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, rsa_pub_desc->g_dma,
+					 pub_req->g_len, DMA_FROM_DEVICE);
+			dma_unmap_single(dev, rsa_pub_desc->f_dma,
+					 pub_req->f_len, DMA_TO_DEVICE);
+			break;
+		}
+	case RSA_PRIV_FORM1:
+		{
+			struct rsa_priv_frm1_req_s *priv_req =
+			    &req->req_u.rsa_priv_f1;
+			struct rsa_priv_frm1_desc_s *rsa_priv_desc =
+			    (struct rsa_priv_frm1_desc_s *)edesc->hw_desc;
+
+			dma_unmap_single(dev, rsa_priv_desc->n_dma,
+					 priv_req->n_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, rsa_priv_desc->d_dma,
+					 priv_req->d_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, rsa_priv_desc->f_dma,
+					 priv_req->f_len, DMA_FROM_DEVICE);
+			dma_unmap_single(dev, rsa_priv_desc->g_dma,
+					 priv_req->g_len, DMA_TO_DEVICE);
+			break;
+		}
+	case RSA_PRIV_FORM2:
+		{
+			struct rsa_priv_frm2_req_s *priv_req =
+			    &req->req_u.rsa_priv_f2;
+			struct rsa_priv_frm2_desc_s *rsa_priv_desc =
+			    (struct rsa_priv_frm2_desc_s *)edesc->hw_desc;
+
+			dma_unmap_single(dev, rsa_priv_desc->p_dma,
+					 priv_req->p_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, rsa_priv_desc->q_dma,
+					 priv_req->q_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, rsa_priv_desc->d_dma,
+					 priv_req->d_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, rsa_priv_desc->g_dma,
+					 priv_req->g_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, rsa_priv_desc->f_dma,
+					 priv_req->f_len, DMA_FROM_DEVICE);
+			dma_unmap_single(dev,
+					 edesc->dma_u.rsa_priv_f2_edesc.
+					 tmp1_dma, priv_req->p_len,
+					 DMA_BIDIRECTIONAL);
+			dma_unmap_single(dev,
+					 edesc->dma_u.rsa_priv_f2_edesc.
+					 tmp2_dma, priv_req->q_len,
+					 DMA_BIDIRECTIONAL);
+			kfree(edesc->dma_u.rsa_priv_f2_edesc.tmp1);
+			kfree(edesc->dma_u.rsa_priv_f2_edesc.tmp2);
+			break;
+		}
+	case RSA_PRIV_FORM3:
+		{
+			struct rsa_priv_frm3_req_s *priv_req =
+			    &req->req_u.rsa_priv_f3;
+			struct rsa_priv_frm3_desc_s *rsa_priv_desc =
+			    (struct rsa_priv_frm3_desc_s *)edesc->hw_desc;
+
+			dma_unmap_single(dev, rsa_priv_desc->p_dma,
+					 priv_req->p_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, rsa_priv_desc->q_dma,
+					 priv_req->q_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, rsa_priv_desc->dq_dma,
+					 priv_req->dq_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, rsa_priv_desc->dp_dma,
+					 priv_req->dp_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, rsa_priv_desc->c_dma,
+					 priv_req->c_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, rsa_priv_desc->g_dma,
+					 priv_req->g_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, rsa_priv_desc->f_dma,
+					 priv_req->f_len, DMA_FROM_DEVICE);
+			dma_unmap_single(dev,
+					 edesc->dma_u.rsa_priv_f3_edesc.
+					 tmp1_dma, priv_req->p_len,
+					 DMA_BIDIRECTIONAL);
+			dma_unmap_single(dev,
+					 edesc->dma_u.rsa_priv_f3_edesc.
+					 tmp2_dma, priv_req->q_len,
+					 DMA_BIDIRECTIONAL);
+			kfree(edesc->dma_u.rsa_priv_f3_edesc.tmp1);
+			kfree(edesc->dma_u.rsa_priv_f3_edesc.tmp2);
+			break;
+		}
+	default:
+		dev_err(dev, "Unable to find request type\n");
+	}
+}
+
+/* RSA Job Completion handler */
+static void rsa_op_done(struct device *dev, u32 *desc, u32 err, void *context)
+{
+	struct pkc_request *req = context;
+	struct rsa_edesc *edesc;
+
+	edesc = (struct rsa_edesc *)((char *)desc -
+				     offsetof(struct rsa_edesc, hw_desc));
+
+	if (err)
+		caam_jr_strstatus(dev, err);
+
+	rsa_unmap(dev, edesc, req);
+	kfree(edesc);
+
+	pkc_request_complete(req, err);
+}
+
+static void dsa_unmap(struct device *dev,
+		       struct dsa_edesc_s *edesc, struct pkc_request *req)
+{
+	switch (req->type) {
+	case DSA_SIGN:
+		{
+			struct dsa_sign_req_s *dsa_req = &req->req_u.dsa_sign;
+			struct dsa_sign_desc_s *dsa_desc =
+			    (struct dsa_sign_desc_s *)edesc->hw_desc;
+			dma_unmap_single(dev, dsa_desc->q_dma,
+					 dsa_req->q_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, dsa_desc->r_dma,
+					 dsa_req->r_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, dsa_desc->g_dma,
+					 dsa_req->g_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, dsa_desc->s_dma,
+					 dsa_req->priv_key_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, dsa_desc->f_dma,
+					 dsa_req->m_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, dsa_desc->c_dma,
+					 dsa_req->d_len, DMA_FROM_DEVICE);
+			dma_unmap_single(dev, dsa_desc->d_dma,
+					 dsa_req->d_len, DMA_FROM_DEVICE);
+		}
+	break;
+	case DSA_VERIFY:
+		{
+			struct dsa_verify_req_s *dsa_req =
+				 &req->req_u.dsa_verify;
+			struct dsa_verify_desc_s *dsa_desc =
+			    (struct dsa_verify_desc_s *)edesc->hw_desc;
+			dma_unmap_single(dev, dsa_desc->q_dma,
+					 dsa_req->q_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, dsa_desc->r_dma,
+					 dsa_req->r_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, dsa_desc->g_dma,
+					 dsa_req->g_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, dsa_desc->w_dma,
+					 dsa_req->pub_key_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, dsa_desc->f_dma,
+					 dsa_req->m_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, dsa_desc->c_dma,
+					 dsa_req->d_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, dsa_desc->d_dma,
+					 dsa_req->d_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, dsa_desc->tmp_dma,
+					 edesc->l_len, DMA_BIDIRECTIONAL);
+			kfree(edesc->tmp);
+		}
+	break;
+	default:
+		dev_err(dev, "Unable to find request type\n");
+	}
+}
+
+/* DSA Job Completion handler */
+static void dsa_op_done(struct device *dev, u32 *desc, u32 err, void *context)
+{
+	struct pkc_request *req = context;
+	struct dsa_edesc_s *edesc;
+
+	edesc = (struct dsa_edesc_s *)((char *)desc -
+				     offsetof(struct dsa_edesc_s, hw_desc));
+
+	if (err)
+		caam_jr_strstatus(dev, err);
+
+	dsa_unmap(dev, edesc, req);
+	kfree(edesc);
+
+	pkc_request_complete(req, err);
+}
+static int caam_dsa_sign_edesc(struct pkc_request *req,
+				struct dsa_edesc_s *edesc)
+{
+	struct crypto_pkc *tfm = crypto_pkc_reqtfm(req);
+	struct caam_pkc_context_s *ctxt = crypto_pkc_ctx(tfm);
+	struct device *dev = ctxt->dev;
+	struct dsa_sign_req_s *dsa_req = &req->req_u.dsa_sign;
+
+	edesc->l_len = dsa_req->q_len;
+	edesc->n_len = dsa_req->r_len;
+	edesc->req_type = req->type;
+	edesc->q_dma = dma_map_single(dev, dsa_req->q, dsa_req->q_len,
+					  DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, edesc->q_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto q_map_fail;
+	}
+
+	edesc->r_dma = dma_map_single(dev, dsa_req->r, dsa_req->r_len,
+					  DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, edesc->r_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto r_map_fail;
+	}
+
+	edesc->g_dma = dma_map_single(dev, dsa_req->g, dsa_req->g_len,
+					  DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, edesc->g_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto g_map_fail;
+	}
+
+	edesc->f_dma = dma_map_single(dev, dsa_req->m, dsa_req->m_len,
+				      DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, edesc->f_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto m_map_fail;
+	}
+
+	edesc->key_dma = dma_map_single(dev, dsa_req->priv_key,
+					dsa_req->priv_key_len, DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, edesc->key_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto key_map_fail;
+	}
+
+	edesc->c_dma = dma_map_single(dev, dsa_req->c, dsa_req->d_len,
+					  DMA_FROM_DEVICE);
+	if (dma_mapping_error(dev, edesc->c_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto c_map_fail;
+	}
+
+	edesc->d_dma = dma_map_single(dev, dsa_req->d, dsa_req->d_len,
+					  DMA_FROM_DEVICE);
+	if (dma_mapping_error(dev, edesc->d_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto d_map_fail;
+	}
+	if (edesc->req_type == ECDSA_SIGN) {
+		edesc->ab_dma = dma_map_single(dev, dsa_req->ab,
+					       dsa_req->ab_len, DMA_TO_DEVICE);
+		if (dma_mapping_error(dev, edesc->ab_dma)) {
+			dev_err(dev, "Unable to map  memory\n");
+			goto ab_map_fail;
+		}
+	}
+	return 0;
+ab_map_fail:
+	if (edesc->req_type == ECDSA_SIGN)
+		dma_unmap_single(dev, edesc->d_dma, dsa_req->d_len,
+				 DMA_TO_DEVICE);
+d_map_fail:
+	dma_unmap_single(dev, edesc->c_dma, dsa_req->d_len, DMA_FROM_DEVICE);
+c_map_fail:
+	dma_unmap_single(dev, edesc->key_dma, dsa_req->priv_key_len,
+			 DMA_TO_DEVICE);
+key_map_fail:
+	dma_unmap_single(dev, edesc->f_dma, dsa_req->m_len, DMA_FROM_DEVICE);
+m_map_fail:
+	dma_unmap_single(dev, edesc->g_dma, dsa_req->g_len, DMA_TO_DEVICE);
+g_map_fail:
+	dma_unmap_single(dev, edesc->r_dma, dsa_req->r_len, DMA_TO_DEVICE);
+r_map_fail:
+	dma_unmap_single(dev, edesc->q_dma, dsa_req->q_len, DMA_TO_DEVICE);
+q_map_fail:
+	return -EINVAL;
+}
+static int caam_dsa_verify_edesc(struct pkc_request *req,
+				  struct dsa_edesc_s *edesc)
+{
+	struct crypto_pkc *tfm = crypto_pkc_reqtfm(req);
+	struct caam_pkc_context_s *ctxt = crypto_pkc_ctx(tfm);
+	struct device *dev = ctxt->dev;
+	struct dsa_verify_req_s *dsa_req = &req->req_u.dsa_verify;
+
+	edesc->l_len = dsa_req->q_len;
+	edesc->n_len = dsa_req->r_len;
+	edesc->tmp = kzalloc(dsa_req->q_len, GFP_DMA);
+	edesc->req_type = req->type;
+	if (!edesc->tmp) {
+		pr_debug("Failed to allocate temp buffer for DSA Verify\n");
+		return -ENOMEM;
+	}
+
+	edesc->tmp_dma = dma_map_single(dev, edesc->tmp, dsa_req->q_len,
+					  DMA_BIDIRECTIONAL);
+	if (dma_mapping_error(dev, edesc->tmp_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto tmp_map_fail;
+	}
+
+	edesc->q_dma = dma_map_single(dev, dsa_req->q, dsa_req->q_len,
+					  DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, edesc->q_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto q_map_fail;
+	}
+
+	edesc->r_dma = dma_map_single(dev, dsa_req->r, dsa_req->r_len,
+					  DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, edesc->r_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto r_map_fail;
+	}
+
+	edesc->g_dma = dma_map_single(dev, dsa_req->g, dsa_req->g_len,
+					  DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, edesc->g_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto g_map_fail;
+	}
+
+	edesc->f_dma = dma_map_single(dev, dsa_req->m, dsa_req->m_len,
+					  DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, edesc->f_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto m_map_fail;
+	}
+
+	edesc->key_dma = dma_map_single(dev, dsa_req->pub_key,
+					dsa_req->pub_key_len, DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, edesc->key_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto key_map_fail;
+	}
+	edesc->c_dma = dma_map_single(dev, dsa_req->c, dsa_req->d_len,
+					  DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, edesc->c_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto c_map_fail;
+	}
+
+	edesc->d_dma = dma_map_single(dev, dsa_req->d, dsa_req->d_len,
+					  DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, edesc->d_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto d_map_fail;
+	}
+	if (edesc->req_type == ECDSA_VERIFY) {
+		edesc->ab_dma = dma_map_single(dev, dsa_req->ab,
+					       dsa_req->ab_len, DMA_TO_DEVICE);
+		if (dma_mapping_error(dev, edesc->ab_dma)) {
+			dev_err(dev, "Unable to map  memory\n");
+			goto ab_map_fail;
+		}
+	}
+	return 0;
+ab_map_fail:
+	if (edesc->req_type == ECDSA_VERIFY)
+		dma_unmap_single(dev, edesc->d_dma, dsa_req->d_len,
+				 DMA_TO_DEVICE);
+d_map_fail:
+	dma_unmap_single(dev, edesc->c_dma, dsa_req->d_len, DMA_TO_DEVICE);
+c_map_fail:
+	dma_unmap_single(dev, edesc->key_dma, dsa_req->pub_key_len,
+			 DMA_TO_DEVICE);
+key_map_fail:
+	dma_unmap_single(dev, edesc->f_dma, dsa_req->m_len, DMA_TO_DEVICE);
+m_map_fail:
+	dma_unmap_single(dev, edesc->g_dma, dsa_req->g_len, DMA_TO_DEVICE);
+g_map_fail:
+	dma_unmap_single(dev, edesc->r_dma, dsa_req->r_len, DMA_TO_DEVICE);
+r_map_fail:
+	dma_unmap_single(dev, edesc->q_dma, dsa_req->q_len, DMA_TO_DEVICE);
+q_map_fail:
+	dma_unmap_single(dev, edesc->tmp_dma, dsa_req->q_len,
+			 DMA_BIDIRECTIONAL);
+tmp_map_fail:
+	kfree(edesc->tmp);
+	return -EINVAL;
+}
+
+static int caam_rsa_pub_edesc(struct pkc_request *req, struct rsa_edesc *edesc)
+{
+	struct crypto_pkc *tfm = crypto_pkc_reqtfm(req);
+	struct caam_pkc_context_s *ctxt = crypto_pkc_ctx(tfm);
+	struct device *dev = ctxt->dev;
+	struct rsa_pub_req_s *pub_req = &req->req_u.rsa_pub_req;
+	struct rsa_pub_edesc_s *pub_edesc = &edesc->dma_u.rsa_pub_edesc;
+
+	if (pub_req->n_len > pub_req->g_len) {
+		pr_err("Output buffer length less than parameter n\n");
+		return -EINVAL;
+	}
+
+	pub_edesc->n_dma = dma_map_single(dev, pub_req->n, pub_req->n_len,
+					  DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, pub_edesc->n_dma)) {
+		dev_err(dev, "Unable to map  modulus memory\n");
+		goto n_pub_fail;
+	}
+
+	pub_edesc->e_dma = dma_map_single(dev, pub_req->e, pub_req->e_len,
+					  DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, pub_edesc->e_dma)) {
+		dev_err(dev, "Unable to map exponent memory\n");
+		goto e_pub_fail;
+	}
+
+	pub_edesc->f_dma = dma_map_single(dev, pub_req->f, pub_req->f_len,
+					  DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, pub_edesc->f_dma)) {
+		dev_err(dev, "Unable to map input buffer memory\n");
+		goto f_pub_fail;
+	}
+
+	pub_edesc->g_dma = dma_map_single(dev, pub_req->g, pub_req->g_len,
+					  DMA_FROM_DEVICE);
+	if (dma_mapping_error(dev, pub_edesc->g_dma)) {
+		dev_err(dev, "Unable to map output memory\n");
+		goto g_pub_fail;
+	}
+
+	/* TBD: Set SG flags in case input is SG */
+	pub_edesc->sg_flgs.e_len = pub_req->e_len;
+	pub_edesc->sg_flgs.n_len = pub_req->n_len;
+	pub_edesc->f_len = pub_req->f_len;
+/* Enable once we check SG */
+#ifdef SG_ENABLED
+	pub_edesc->sg_flgs.sg_f = 1;
+	pub_edesc->sg_flgs.sg_g = 1;
+	pub_edesc->sg_flgs.sg_e = 1;
+	pub_edesc->sg_flgs.sg_n = 1;
+#endif
+
+	return 0;
+g_pub_fail:
+	dma_unmap_single(dev, pub_edesc->f_dma, pub_req->f_len, DMA_TO_DEVICE);
+f_pub_fail:
+	dma_unmap_single(dev, pub_edesc->e_dma, pub_req->e_len, DMA_TO_DEVICE);
+e_pub_fail:
+	dma_unmap_single(dev, pub_edesc->n_dma, pub_req->n_len, DMA_TO_DEVICE);
+n_pub_fail:
+	return -EINVAL;
+}
+
+static int caam_rsa_priv_f1_edesc(struct pkc_request *req,
+				  struct rsa_edesc *edesc)
+{
+	struct crypto_pkc *tfm = crypto_pkc_reqtfm(req);
+	struct caam_pkc_context_s *ctxt = crypto_pkc_ctx(tfm);
+	struct device *dev = ctxt->dev;
+	struct rsa_priv_frm1_req_s *priv_req = &req->req_u.rsa_priv_f1;
+	struct rsa_priv_frm1_edesc_s *priv_edesc =
+	    &edesc->dma_u.rsa_priv_f1_edesc;
+
+	priv_edesc->n_dma = dma_map_single(dev, priv_req->n, priv_req->n_len,
+					   DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->n_dma)) {
+		dev_err(dev, "Unable to map  modulus memory\n");
+		goto n_f1_fail;
+	}
+
+	priv_edesc->d_dma = dma_map_single(dev, priv_req->d, priv_req->d_len,
+					   DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->d_dma)) {
+		dev_err(dev, "Unable to map exponent memory\n");
+		goto d_f1_fail;
+	}
+
+	priv_edesc->f_dma = dma_map_single(dev, priv_req->f, priv_req->f_len,
+					   DMA_FROM_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->f_dma)) {
+		dev_err(dev, "Unable to map output buffer memory\n");
+		goto f_f1_fail;
+	}
+
+	priv_edesc->g_dma = dma_map_single(dev, priv_req->g, priv_req->g_len,
+					   DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->g_dma)) {
+		dev_err(dev, "Unable to map input memory\n");
+		goto g_f1_fail;
+	}
+
+/* Enable once we check SG */
+#ifdef SG_ENABLED
+	priv_edesc->sg_flgs.sg_f = 1;
+	priv_edesc->sg_flgs.sg_g = 1;
+	priv_edesc->sg_flgs.sg_d = 1;
+	priv_edesc->sg_flgs.sg_n = 1;
+#endif
+	priv_edesc->sg_flgs.d_len =  priv_req->d_len;
+	priv_edesc->sg_flgs.n_len = priv_req->n_len;
+
+	return 0;
+g_f1_fail:
+	dma_unmap_single(dev, priv_edesc->f_dma, priv_req->f_len,
+			 DMA_FROM_DEVICE);
+f_f1_fail:
+	dma_unmap_single(dev, priv_edesc->d_dma, priv_req->d_len,
+			 DMA_TO_DEVICE);
+d_f1_fail:
+	dma_unmap_single(dev, priv_edesc->n_dma, priv_req->n_len,
+			 DMA_TO_DEVICE);
+n_f1_fail:
+	return -EINVAL;
+}
+
+static int caam_rsa_priv_f2_edesc(struct pkc_request *req,
+				  struct rsa_edesc *edesc)
+{
+	struct crypto_pkc *tfm = crypto_pkc_reqtfm(req);
+	struct caam_pkc_context_s *ctxt = crypto_pkc_ctx(tfm);
+	struct device *dev = ctxt->dev;
+	struct rsa_priv_frm2_req_s *priv_req = &req->req_u.rsa_priv_f2;
+	struct rsa_priv_frm2_edesc_s *priv_edesc =
+	    &edesc->dma_u.rsa_priv_f2_edesc;
+
+	/* tmp1 must be as long as p */
+	priv_edesc->tmp1 = kzalloc(priv_req->p_len, GFP_DMA);
+
+	if (!priv_edesc->tmp1)
+		return -ENOMEM;
+
+	/* tmp2 must be as long as q */
+	priv_edesc->tmp2 = kzalloc(priv_req->q_len, GFP_DMA);
+	if (!priv_edesc->tmp2) {
+		kfree(priv_edesc->tmp1);
+		return -ENOMEM;
+	}
+
+	priv_edesc->tmp1_dma =
+	    dma_map_single(dev, priv_edesc->tmp1, priv_req->p_len,
+			   DMA_BIDIRECTIONAL);
+	if (dma_mapping_error(dev, priv_edesc->tmp1_dma)) {
+		dev_err(dev, "Unable to map  modulus memory\n");
+		goto tmp1_f2_fail;
+	}
+
+	priv_edesc->tmp2_dma =
+	    dma_map_single(dev, priv_edesc->tmp2, priv_req->q_len,
+			   DMA_BIDIRECTIONAL);
+	if (dma_mapping_error(dev, priv_edesc->tmp2_dma)) {
+		dev_err(dev, "Unable to map  modulus memory\n");
+		goto tmp2_f2_fail;
+	}
+
+	priv_edesc->p_dma = dma_map_single(dev, priv_req->p, priv_req->p_len,
+					   DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->p_dma)) {
+		dev_err(dev, "Unable to map  modulus memory\n");
+		goto p_f2_fail;
+	}
+
+	priv_edesc->q_dma = dma_map_single(dev, priv_req->q, priv_req->q_len,
+					   DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->q_dma)) {
+		dev_err(dev, "Unable to map exponent memory\n");
+		goto q_f2_fail;
+	}
+
+	priv_edesc->d_dma = dma_map_single(dev, priv_req->d, priv_req->d_len,
+					   DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->d_dma)) {
+		dev_err(dev, "Unable to map exponent memory\n");
+		goto d_f2_fail;
+	}
+
+	priv_edesc->f_dma = dma_map_single(dev, priv_req->f, priv_req->f_len,
+					   DMA_FROM_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->f_dma)) {
+		dev_err(dev, "Unable to map output buffer memory\n");
+		goto f_f2_fail;
+	}
+
+	priv_edesc->g_dma = dma_map_single(dev, priv_req->g, priv_req->g_len,
+					   DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->g_dma)) {
+		dev_err(dev, "Unable to map input memory\n");
+		goto g_f2_fail;
+	}
+	priv_edesc->sg_flgs.d_len = priv_req->d_len;
+	priv_edesc->sg_flgs.n_len = priv_req->n_len;
+	priv_edesc->q_len = priv_req->q_len;
+	priv_edesc->p_len = priv_req->p_len;
+
+	/* TBD: Set SG flags in case input is SG */
+	return 0;
+g_f2_fail:
+	dma_unmap_single(dev, priv_edesc->f_dma, priv_req->f_len,
+			 DMA_FROM_DEVICE);
+f_f2_fail:
+	dma_unmap_single(dev, priv_edesc->d_dma, priv_req->d_len,
+			 DMA_TO_DEVICE);
+d_f2_fail:
+	dma_unmap_single(dev, priv_edesc->q_dma, priv_req->q_len,
+			 DMA_TO_DEVICE);
+q_f2_fail:
+	dma_unmap_single(dev, priv_edesc->p_dma, priv_req->p_len,
+			 DMA_TO_DEVICE);
+p_f2_fail:
+	dma_unmap_single(dev, priv_edesc->tmp2_dma, priv_req->q_len,
+			 DMA_TO_DEVICE);
+tmp2_f2_fail:
+	dma_unmap_single(dev, priv_edesc->tmp1_dma, priv_req->p_len,
+			 DMA_BIDIRECTIONAL);
+	kfree(priv_edesc->tmp2);
+tmp1_f2_fail:
+	kfree(priv_edesc->tmp1);
+	return -EINVAL;
+}
+
+static int caam_rsa_priv_f3_edesc(struct pkc_request *req,
+				  struct rsa_edesc *edesc)
+{
+	struct crypto_pkc *tfm = crypto_pkc_reqtfm(req);
+	struct caam_pkc_context_s *ctxt = crypto_pkc_ctx(tfm);
+	struct device *dev = ctxt->dev;
+	struct rsa_priv_frm3_req_s *priv_req = &req->req_u.rsa_priv_f3;
+	struct rsa_priv_frm3_edesc_s *priv_edesc =
+	    &edesc->dma_u.rsa_priv_f3_edesc;
+
+	priv_edesc->tmp1 = kzalloc(priv_req->p_len, GFP_DMA);
+
+	if (!priv_edesc->tmp1)
+		return -ENOMEM;
+
+	priv_edesc->tmp2 = kzalloc(priv_req->q_len, GFP_DMA);
+	if (!priv_edesc->tmp2) {
+		kfree(priv_edesc->tmp1);
+		return -ENOMEM;
+	}
+
+	priv_edesc->tmp1_dma =
+	    dma_map_single(dev, priv_edesc->tmp1, priv_req->p_len,
+			   DMA_BIDIRECTIONAL);
+	if (dma_mapping_error(dev, priv_edesc->tmp1_dma)) {
+		dev_err(dev, "Unable to map  modulus memory\n");
+		goto tmp1_f2_fail;
+	}
+
+	priv_edesc->tmp2_dma =
+	    dma_map_single(dev, priv_edesc->tmp2, priv_req->q_len,
+			   DMA_BIDIRECTIONAL);
+	if (dma_mapping_error(dev, priv_edesc->tmp2_dma)) {
+		dev_err(dev, "Unable to map  modulus memory\n");
+		goto tmp2_f2_fail;
+	}
+
+	priv_edesc->p_dma = dma_map_single(dev, priv_req->p, priv_req->p_len,
+					   DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->p_dma)) {
+		dev_err(dev, "Unable to map  modulus memory\n");
+		goto p_f3_fail;
+	}
+
+	priv_edesc->q_dma = dma_map_single(dev, priv_req->q, priv_req->q_len,
+					   DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->q_dma)) {
+		dev_err(dev, "Unable to map exponent memory\n");
+		goto q_f3_fail;
+	}
+
+	priv_edesc->dp_dma =
+	    dma_map_single(dev, priv_req->dp, priv_req->dp_len, DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->dp_dma)) {
+		dev_err(dev, "Unable to map dp memory\n");
+		goto dp_f3_fail;
+	}
+
+	priv_edesc->dq_dma =
+	    dma_map_single(dev, priv_req->dq, priv_req->dq_len, DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->dq_dma)) {
+		dev_err(dev, "Unable to map dq memory\n");
+		goto dq_f3_fail;
+	}
+
+	priv_edesc->c_dma = dma_map_single(dev, priv_req->c, priv_req->c_len,
+					   DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->c_dma)) {
+		dev_err(dev, "Unable to map Coefficient memory\n");
+		goto c_f3_fail;
+	}
+
+	priv_edesc->f_dma = dma_map_single(dev, priv_req->f, priv_req->f_len,
+					   DMA_FROM_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->f_dma)) {
+		dev_err(dev, "Unable to map output buffer memory\n");
+		goto f_f3_fail;
+	}
+
+	priv_edesc->g_dma = dma_map_single(dev, priv_req->g, priv_req->g_len,
+					   DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->g_dma)) {
+		dev_err(dev, "Unable to map input memory\n");
+		goto g_f3_fail;
+	}
+
+	priv_edesc->sg_flgs.n_len = priv_req->f_len;
+	priv_edesc->q_len = priv_req->q_len;
+	priv_edesc->p_len = priv_req->p_len;
+
+	return 0;
+g_f3_fail:
+	dma_unmap_single(dev, priv_edesc->f_dma, priv_req->f_len,
+			 DMA_FROM_DEVICE);
+f_f3_fail:
+	dma_unmap_single(dev, priv_edesc->c_dma, priv_req->c_len,
+			 DMA_TO_DEVICE);
+c_f3_fail:
+	dma_unmap_single(dev, priv_edesc->dq_dma, priv_req->dq_len,
+			 DMA_TO_DEVICE);
+dq_f3_fail:
+	dma_unmap_single(dev, priv_edesc->dp_dma, priv_req->dp_len,
+			 DMA_TO_DEVICE);
+dp_f3_fail:
+	dma_unmap_single(dev, priv_edesc->q_dma, priv_req->q_len,
+			 DMA_TO_DEVICE);
+q_f3_fail:
+	dma_unmap_single(dev, priv_edesc->p_dma, priv_req->p_len,
+			 DMA_TO_DEVICE);
+p_f3_fail:
+	dma_unmap_single(dev, priv_edesc->tmp2_dma, priv_req->q_len,
+			 DMA_TO_DEVICE);
+tmp2_f2_fail:
+	dma_unmap_single(dev, priv_edesc->tmp1_dma, priv_req->p_len,
+			 DMA_BIDIRECTIONAL);
+	kfree(priv_edesc->tmp2);
+tmp1_f2_fail:
+	kfree(priv_edesc->tmp1);
+
+	return -EINVAL;
+}
+
+/* CAAM Descriptor creator for RSA Public Key operations */
+static void *caam_rsa_desc_init(struct pkc_request *req)
+{
+	void *desc = NULL;
+	struct rsa_edesc *edesc = NULL;
+
+	switch (req->type) {
+	case RSA_PUB:
+		{
+			edesc =
+			    kzalloc(sizeof(*edesc) +
+				    sizeof(struct rsa_pub_desc_s), GFP_DMA);
+
+			if (!edesc)
+				return NULL;
+
+			if (caam_rsa_pub_edesc(req, edesc)) {
+				kfree(edesc);
+				return NULL;
+			}
+
+			desc = caam_rsa_pub_desc(edesc);
+			break;
+		}
+	case RSA_PRIV_FORM1:
+		{
+			edesc =
+			    kzalloc(sizeof(*edesc) +
+				    sizeof(struct rsa_priv_frm1_desc_s),
+				    GFP_DMA);
+			if (!edesc)
+				return NULL;
+
+			if (caam_rsa_priv_f1_edesc(req, edesc)) {
+				kfree(edesc);
+				return NULL;
+			}
+
+			desc = caam_rsa_priv_f1_desc(edesc);
+			break;
+		}
+	case RSA_PRIV_FORM2:
+		{
+			edesc =
+			    kzalloc(sizeof(*edesc) +
+				    sizeof(struct rsa_priv_frm2_desc_s),
+				    GFP_DMA);
+			if (!edesc)
+				return NULL;
+
+			if (caam_rsa_priv_f2_edesc(req, edesc)) {
+				kfree(edesc);
+				return NULL;
+			}
+
+			desc = caam_rsa_priv_f2_desc(edesc);
+			break;
+		}
+	case RSA_PRIV_FORM3:
+		{
+			edesc = kzalloc(sizeof(*edesc) +
+					sizeof(struct rsa_priv_frm3_desc_s),
+					GFP_DMA);
+			if (!edesc)
+				return NULL;
+
+			if (caam_rsa_priv_f3_edesc(req, edesc)) {
+				kfree(edesc);
+				return NULL;
+			}
+
+			desc = caam_rsa_priv_f3_desc(edesc);
+			break;
+		}
+	default:
+		pr_debug("Unknown request type\n");
+		return NULL;
+	}
+
+	edesc->req_type = req->type;
+	return desc;
+}
+
+/* CAAM Descriptor creator for RSA Public Key operations */
+static void *caam_dsa_desc_init(struct pkc_request *req)
+{
+	void *desc = NULL;
+	struct dsa_edesc_s *edesc = NULL;
+
+	switch (req->type) {
+	case DSA_SIGN:
+		{
+			edesc = kzalloc(sizeof(*edesc) +
+					sizeof(struct dsa_sign_desc_s),
+					GFP_DMA);
+			if (!edesc)
+				return NULL;
+
+			if (caam_dsa_sign_edesc(req, edesc)) {
+				kfree(edesc);
+				return NULL;
+			}
+
+			desc = caam_dsa_sign_desc(edesc);
+			break;
+		}
+		break;
+	case DSA_VERIFY:
+		{
+			edesc = kzalloc(sizeof(*edesc) +
+					sizeof(struct dsa_verify_desc_s),
+					GFP_DMA);
+			if (!edesc)
+				return NULL;
+
+			if (caam_dsa_verify_edesc(req, edesc)) {
+				kfree(edesc);
+				return NULL;
+			}
+
+			desc = caam_dsa_verify_desc(edesc);
+			break;
+		}
+		break;
+	case ECDSA_SIGN:
+		{
+			edesc = kzalloc(sizeof(*edesc) +
+					sizeof(struct ecdsa_sign_desc_s),
+					GFP_DMA);
+			if (!edesc)
+				return NULL;
+
+			if (caam_dsa_sign_edesc(req, edesc)) {
+				kfree(edesc);
+				return NULL;
+			}
+
+			desc = caam_dsa_sign_desc(edesc);
+			break;
+		}
+
+		break;
+	case ECDSA_VERIFY:
+		{
+			edesc = kzalloc(sizeof(*edesc) +
+					sizeof(struct ecdsa_verify_desc_s),
+					GFP_DMA);
+			if (!edesc)
+				return NULL;
+
+			if (caam_dsa_verify_edesc(req, edesc)) {
+				kfree(edesc);
+				return NULL;
+			}
+
+			desc = caam_dsa_verify_desc(edesc);
+			break;
+		}
+	default:
+		pr_debug("Unknown DSA Desc init request\n");
+		return NULL;
+	}
+
+	edesc->req_type = req->type;
+	return desc;
+}
+
+/* DSA operation Handler */
+static int dsa_op(struct pkc_request *req)
+{
+	struct crypto_pkc *pkc_tfm = crypto_pkc_reqtfm(req);
+	struct caam_pkc_context_s *ctxt = crypto_pkc_ctx(pkc_tfm);
+	struct device *dev = ctxt->dev;
+	int ret = 0;
+	void *desc = NULL;
+
+	desc = caam_dsa_desc_init(req);
+	if (!desc) {
+		dev_err(dev, "Unable to allocate descriptor\n");
+		return -ENOMEM;
+	}
+
+	ret = caam_jr_enqueue(dev, desc, dsa_op_done, req);
+	if (!ret)
+		ret = -EINPROGRESS;
+
+	return ret;
+}
+
+/* RSA operation Handler */
+static int rsa_op(struct pkc_request *req)
+{
+	struct crypto_pkc *pkc_tfm = crypto_pkc_reqtfm(req);
+	struct caam_pkc_context_s *ctxt = crypto_pkc_ctx(pkc_tfm);
+	struct device *dev = ctxt->dev;
+	int ret = 0;
+	void *desc = NULL;
+
+	desc = caam_rsa_desc_init(req);
+	if (!desc) {
+		dev_err(dev, "Unable to allocate descriptor\n");
+		return -ENOMEM;
+	}
+
+	ret = caam_jr_enqueue(dev, desc, rsa_op_done, req);
+	if (!ret)
+		ret = -EINPROGRESS;
+
+	return ret;
+}
+
+/* PKC Descriptor Template */
+struct caam_pkc_template {
+	char name[CRYPTO_MAX_ALG_NAME];
+	char driver_name[CRYPTO_MAX_ALG_NAME];
+	char pkc_name[CRYPTO_MAX_ALG_NAME];
+	char pkc_driver_name[CRYPTO_MAX_ALG_NAME];
+	u32 type;
+	struct pkc_alg template_pkc;
+};
+
+static struct caam_pkc_template driver_pkc[] = {
+	/* RSA driver registeration hooks */
+	{
+	 .name = "rsa",
+	 .driver_name = "rsa-caam",
+	 .pkc_name = "pkc(rsa)",
+	 .pkc_driver_name = "pkc-rsa-caam",
+	 .type = CRYPTO_ALG_TYPE_PKC_RSA,
+	 .template_pkc = {
+			  .pkc_op = rsa_op,
+			  .min_keysize = 512,
+			  .max_keysize = 4096,
+			  },
+	 },
+	/* DSA driver registeration hooks */
+	{
+	 .name = "dsa",
+	 .driver_name = "dsa-caam",
+	 .pkc_name = "pkc(dsa)",
+	 .pkc_driver_name = "pkc-dsa-caam",
+	 .type = CRYPTO_ALG_TYPE_PKC_DSA,
+	 .template_pkc = {
+			  .pkc_op = dsa_op,
+			  .min_keysize = 512,
+			  .max_keysize = 4096,
+			  },
+	 }
+};
+
+struct caam_pkc_alg {
+	struct list_head entry;
+	struct device *ctrldev;
+	struct crypto_alg crypto_alg;
+};
+
+/* Per session pkc's driver context creation function */
+static int caam_pkc_cra_init(struct crypto_tfm *tfm)
+{
+	struct caam_pkc_context_s *ctx = crypto_tfm_ctx(tfm);
+
+	ctx->dev = caam_jr_alloc();
+
+	if (IS_ERR(ctx->dev)) {
+		pr_err("Job Ring Device allocation for transform failed\n");
+		return PTR_ERR(ctx->dev);
+	}
+	return 0;
+}
+
+/* Per session pkc's driver context cleanup function */
+static void caam_pkc_cra_exit(struct crypto_tfm *tfm)
+{
+	/* Nothing to cleanup in private context */
+}
+
+static struct caam_pkc_alg *caam_pkc_alloc(struct device *ctrldev,
+					   struct caam_pkc_template *template,
+					   bool keyed)
+{
+	struct caam_pkc_alg *t_alg;
+	struct crypto_alg *alg;
+
+	t_alg = kzalloc(sizeof(*t_alg), GFP_KERNEL);
+	if (!t_alg) {
+		dev_err(ctrldev, "failed to allocate t_alg\n");
+		return NULL;
+	}
+	alg = &t_alg->crypto_alg;
+	alg->cra_pkc = template->template_pkc;
+
+	if (keyed) {
+		snprintf(alg->cra_name, CRYPTO_MAX_ALG_NAME, "%s",
+			 template->pkc_name);
+		snprintf(alg->cra_driver_name, CRYPTO_MAX_ALG_NAME, "%s",
+			 template->pkc_driver_name);
+	} else {
+		snprintf(alg->cra_name, CRYPTO_MAX_ALG_NAME, "%s",
+			 template->name);
+		snprintf(alg->cra_driver_name, CRYPTO_MAX_ALG_NAME, "%s",
+			 template->driver_name);
+	}
+	alg->cra_module = THIS_MODULE;
+	alg->cra_init = caam_pkc_cra_init;
+	alg->cra_exit = caam_pkc_cra_exit;
+	alg->cra_ctxsize = sizeof(struct caam_pkc_context_s);
+	alg->cra_priority = CAAM_PKC_PRIORITY;
+	alg->cra_alignmask = 0;
+	alg->cra_flags = CRYPTO_ALG_ASYNC | template->type;
+	alg->cra_type = &crypto_pkc_type;
+	t_alg->ctrldev = ctrldev;
+
+	return t_alg;
+}
+
+/* Public Key Cryptography module initialization handler */
+static int __init caam_pkc_init(void)
+{
+	struct device_node *dev_node;
+	struct platform_device *pdev;
+	struct device *ctrldev;
+	struct caam_drv_private *priv;
+	int i = 0, err = 0;
+
+	dev_node = of_find_compatible_node(NULL, NULL, "fsl,sec-v4.0");
+	if (!dev_node) {
+		dev_node = of_find_compatible_node(NULL, NULL, "fsl,sec4.0");
+		if (!dev_node)
+			return -ENODEV;
+	}
+
+	pdev = of_find_device_by_node(dev_node);
+	if (!pdev)
+		return -ENODEV;
+
+	ctrldev = &pdev->dev;
+	priv = dev_get_drvdata(ctrldev);
+	of_node_put(dev_node);
+
+	/*
+	 * If priv is NULL, it's probably because the caam driver wasn't
+	 * properly initialized (e.g. RNG4 init failed). Thus, bail out here.
+	 */
+	if (!priv)
+		return -ENODEV;
+
+	INIT_LIST_HEAD(&priv->pkc_list);
+
+	/* register crypto algorithms the device supports */
+	for (i = 0; i < ARRAY_SIZE(driver_pkc); i++) {
+		/* TODO: check if h/w supports alg */
+		struct caam_pkc_alg *t_alg;
+
+		/* register pkc algorithm */
+		t_alg = caam_pkc_alloc(ctrldev, &driver_pkc[i], true);
+		if (IS_ERR(t_alg)) {
+			err = PTR_ERR(t_alg);
+			dev_warn(ctrldev, "%s alg allocation failed\n",
+				 driver_pkc[i].driver_name);
+			continue;
+		}
+
+		err = crypto_register_alg(&t_alg->crypto_alg);
+		if (err) {
+			dev_warn(ctrldev, "%s alg registration failed\n",
+				 t_alg->crypto_alg.cra_driver_name);
+			kfree(t_alg);
+		} else {
+			list_add_tail(&t_alg->entry, &priv->pkc_list);
+		}
+	}
+
+	if (!list_empty(&priv->pkc_list))
+		dev_info(ctrldev, "%s algorithms registered in /proc/crypto\n",
+			 (char *)of_get_property(dev_node, "compatible", NULL));
+
+	return err;
+}
+
+static void __exit caam_pkc_exit(void)
+{
+	struct device_node *dev_node;
+	struct platform_device *pdev;
+	struct device *ctrldev;
+	struct caam_drv_private *priv;
+	struct caam_pkc_alg *t_alg, *n;
+
+	dev_node = of_find_compatible_node(NULL, NULL, "fsl,sec-v4.0");
+
+	if (!dev_node) {
+		dev_node = of_find_compatible_node(NULL, NULL, "fsl,sec4.0");
+		if (!dev_node)
+			return;
+	}
+
+	pdev = of_find_device_by_node(dev_node);
+
+	if (!pdev)
+		return;
+
+	ctrldev = &pdev->dev;
+	of_node_put(dev_node);
+	priv = dev_get_drvdata(ctrldev);
+
+	if (!priv->pkc_list.next)
+		return;
+
+	list_for_each_entry_safe(t_alg, n, &priv->pkc_list, entry) {
+		crypto_unregister_alg(&t_alg->crypto_alg);
+		list_del(&t_alg->entry);
+		kfree(t_alg);
+	}
+}
+
+module_init(caam_pkc_init);
+module_exit(caam_pkc_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("FSL CAAM support for PKC functions of crypto API");
+MODULE_AUTHOR("Yashpal Dutta <yashpal.dutta@freescale.com>");
diff --git a/drivers/crypto/caam/desc.h b/drivers/crypto/caam/desc.h
index 1e93c6a..60aa660 100644
--- a/drivers/crypto/caam/desc.h
+++ b/drivers/crypto/caam/desc.h
@@ -2,7 +2,7 @@
  * CAAM descriptor composition header
  * Definitions to support CAAM descriptor instruction generation
  *
- * Copyright 2008-2011 Freescale Semiconductor, Inc.
+ * Copyright 2008-2012 Freescale Semiconductor, Inc.
  */
 
 #ifndef DESC_H
@@ -454,6 +454,9 @@ struct sec4_sg_entry {
 #define OP_PCLID_PUBLICKEYPAIR	(0x14 << OP_PCLID_SHIFT)
 #define OP_PCLID_DSASIGN	(0x15 << OP_PCLID_SHIFT)
 #define OP_PCLID_DSAVERIFY	(0x16 << OP_PCLID_SHIFT)
+#define OP_PCLID_DH             (0x17 << OP_PCLID_SHIFT)
+#define OP_PCLID_RSAENC_PUBKEY  (0x18 << OP_PCLID_SHIFT)
+#define OP_PCLID_RSADEC_PRVKEY  (0x19 << OP_PCLID_SHIFT)
 
 /* Assuming OP_TYPE = OP_TYPE_DECAP_PROTOCOL/ENCAP_PROTOCOL */
 #define OP_PCLID_IPSEC		(0x01 << OP_PCLID_SHIFT)
diff --git a/drivers/crypto/caam/desc_constr.h b/drivers/crypto/caam/desc_constr.h
index 98d07de..f0df377 100644
--- a/drivers/crypto/caam/desc_constr.h
+++ b/drivers/crypto/caam/desc_constr.h
@@ -4,6 +4,8 @@
  * Copyright 2008-2012 Freescale Semiconductor, Inc.
  */
 
+#ifndef _DESC_CONSTR_H_
+#define _DESC_CONSTR_H_
 #include "desc.h"
 
 #define IMMEDIATE (1 << 23)
@@ -388,3 +390,4 @@ do { \
 	APPEND_MATH_IMM_u64(LSHIFT, desc, dest, src0, src1, data)
 #define append_math_rshift_imm_u64(desc, dest, src0, src1, data) \
 	APPEND_MATH_IMM_u64(RSHIFT, desc, dest, src0, src1, data)
+#endif
diff --git a/drivers/crypto/caam/intern.h b/drivers/crypto/caam/intern.h
index 66495a5..b2a0109 100644
--- a/drivers/crypto/caam/intern.h
+++ b/drivers/crypto/caam/intern.h
@@ -2,7 +2,7 @@
  * CAAM/SEC 4.x driver backend
  * Private/internal definitions between modules
  *
- * Copyright 2008-2011 Freescale Semiconductor, Inc.
+ * Copyright 2008-2012 Freescale Semiconductor, Inc.
  *
  */
 
@@ -89,6 +89,7 @@ struct caam_drv_private {
 	u8 qi_present;		/* Nonzero if QI present in device */
 	int secvio_irq;		/* Security violation interrupt number */
 	int virt_en;		/* Virtualization enabled in CAAM */
+	struct list_head pkc_list; /* list of registered pkc algorithms */
 
 #define	RNG4_MAX_HANDLES 2
 	/* RNG4 block */
diff --git a/drivers/crypto/caam/key_gen.h b/drivers/crypto/caam/key_gen.h
index c5588f6..8ce751b 100644
--- a/drivers/crypto/caam/key_gen.h
+++ b/drivers/crypto/caam/key_gen.h
@@ -1,10 +1,11 @@
 /*
  * CAAM/SEC 4.x definitions for handling key-generation jobs
  *
- * Copyright 2008-2011 Freescale Semiconductor, Inc.
+ * Copyright 2008-2012 Freescale Semiconductor, Inc.
  *
  */
-
+#ifndef _KEY_GEN_H_
+#define _KEY_GEN_H_
 struct split_key_result {
 	struct completion completion;
 	int err;
@@ -15,3 +16,4 @@ void split_key_done(struct device *dev, u32 *desc, u32 err, void *context);
 int gen_split_key(struct device *jrdev, u8 *key_out, int split_key_len,
 		    int split_key_pad_len, const u8 *key_in, u32 keylen,
 		    u32 alg_op);
+#endif
diff --git a/drivers/crypto/caam/pdb.h b/drivers/crypto/caam/pdb.h
index 3a87c0c..7cf2662 100644
--- a/drivers/crypto/caam/pdb.h
+++ b/drivers/crypto/caam/pdb.h
@@ -373,30 +373,157 @@ struct srtp_decap_pdb {
 
 #define DSA_PDB_N_MASK		0x7f
 
-struct dsa_sign_pdb {
-	u32 sgf_ln; /* Use DSA_PDB_ defintions per above */
-	u8 *q;
-	u8 *r;
-	u8 *g;	/* or Gx,y */
-	u8 *s;
-	u8 *f;
-	u8 *c;
-	u8 *d;
-	u8 *ab; /* ECC only */
-	u8 *u;
-};
+struct dsa_sign_desc_s {
+	uint32_t	desc_hdr;
+	uint32_t sgf_ln; /* Use DSA_PDB_ definitions per above */
+	dma_addr_t q_dma;
+	dma_addr_t r_dma;
+	dma_addr_t g_dma;	/* or Gx,y */
+	dma_addr_t s_dma;
+	dma_addr_t f_dma;
+	dma_addr_t c_dma;
+	dma_addr_t d_dma;
+	uint32_t	op;
+} __packed;
+
+struct dsa_verify_desc_s {
+	uint32_t	desc_hdr;
+	uint32_t sgf_ln;
+	dma_addr_t q_dma;
+	dma_addr_t r_dma;
+	dma_addr_t g_dma;	/* or Gx,y */
+	dma_addr_t w_dma; /* or Wx,y */
+	dma_addr_t f_dma;
+	dma_addr_t c_dma;
+	dma_addr_t d_dma;
+	dma_addr_t tmp_dma; /* temporary data block */
+	uint32_t	op;
+} __packed;
+
+struct ecdsa_sign_desc_s {
+	uint32_t	desc_hdr;
+	uint32_t sgf_ln; /* Use ECDSA_PDB_ definitions per above */
+	dma_addr_t q_dma;
+	dma_addr_t r_dma;
+	dma_addr_t g_dma;	/* or Gx,y */
+	dma_addr_t s_dma;
+	dma_addr_t f_dma;
+	dma_addr_t c_dma;
+	dma_addr_t d_dma;
+	dma_addr_t ab_dma;
+	uint32_t	op;
+} __packed;
+
+struct ecdsa_verify_desc_s {
+	uint32_t	desc_hdr;
+	uint32_t sgf_ln;
+	dma_addr_t q_dma;
+	dma_addr_t r_dma;
+	dma_addr_t g_dma;	/* or Gx,y */
+	dma_addr_t w_dma; /* or Wx,y */
+	dma_addr_t f_dma;
+	dma_addr_t c_dma;
+	dma_addr_t d_dma;
+	dma_addr_t tmp_dma; /* temporary data block */
+	dma_addr_t ab_dma;
+	uint32_t	op;
+} __packed;
+
+/* DSA/ECDSA Protocol Data Blocks */
+#define RSA_PDB_SGF_SHIFT       28
+#define RSA_PDB_MSG_FMT_SHIFT   12
+#define RSA_PDB_E_SHIFT       12
+#define RSA_PDB_E_MASK        (0xFFF << RSA_PDB_E_SHIFT)
+#define RSA_PDB_D_SHIFT       12
+#define RSA_PDB_D_MASK        (0xFFF << RSA_PDB_D_SHIFT)
+#define RSA_PDB_Q_SHIFT       12
+#define RSA_PDB_Q_MASK        (0xFFF << RSA_PDB_Q_SHIFT)
+
+
+#define RSA_PDB_SGF_F		(0x8 << RSA_PDB_SGF_SHIFT)
+#define RSA_PDB_SGF_G		(0x4 << RSA_PDB_SGF_SHIFT)
+#define RSA_PDB_SGF_N		(0x2 << RSA_PDB_SGF_SHIFT)
+#define RSA_PDB_SGF_E		(0x1 << RSA_PDB_SGF_SHIFT)
+
+#define RSA_PRIV_KEY_FRM_1	0
+#define RSA_PRIV_KEY_FRM_2	1
+#define RSA_PRIV_KEY_FRM_3	2
+
+#define RSA_PDB_PVT_FRM1_SGF_SHIFT  28
+#define RSA_PDB_PVT_FRM1_D_SHIFT 10
+
+#define RSA_PDB_PVT_MSG_FMT_SHIFT   12
+#define RSA_PDB_PVT_FRM3_SGF_SHIFT  23
+#define RSA_PDB_PVT_FRM3_Q_SHIFT  12
+
+/* RSA Pub_Key Descriptor */
+struct rsa_pub_desc_s {
+	uint32_t	desc_hdr;
+	uint32_t	sgf_flg;
+	dma_addr_t	f_dma;
+	dma_addr_t	g_dma;
+	dma_addr_t	n_dma;
+	dma_addr_t	e_dma;
+	uint32_t	msg_len;
+	uint32_t	op;
+} __packed;
 
-struct dsa_verify_pdb {
-	u32 sgf_ln;
-	u8 *q;
-	u8 *r;
-	u8 *g;	/* or Gx,y */
-	u8 *w; /* or Wx,y */
-	u8 *f;
-	u8 *c;
-	u8 *d;
-	u8 *tmp; /* temporary data block */
-	u8 *ab; /* only used if ECC processing */
-};
+/*
+ * Form1 Priv_key Decryption Descriptor
+ * Private key is represented by (n,d)
+ * f is decrypted data pointer while g is pointer
+ * of encrypted data
+ */
+struct rsa_priv_frm1_desc_s {
+	uint32_t	desc_hdr;
+	uint32_t	sgf_flg;
+	dma_addr_t	g_dma;
+	dma_addr_t	f_dma;
+	dma_addr_t	n_dma;
+	dma_addr_t	d_dma;
+	uint32_t	op;
+} __packed;
+
+/*
+ * Form2 Priv_key Decryption Descriptor
+ * Private key is represented by (p,q,d)
+ * f is decrypted data pointer while g is pointer
+ * of encrypted data
+ */
+struct rsa_priv_frm2_desc_s {
+	uint32_t	desc_hdr;
+	uint32_t	sgf_flg;
+	dma_addr_t	g_dma;
+	dma_addr_t	f_dma;
+	dma_addr_t	d_dma;
+	dma_addr_t	p_dma;
+	dma_addr_t	q_dma;
+	dma_addr_t	tmp1_dma;
+	dma_addr_t	tmp2_dma;
+	uint32_t	p_q_len;
+	uint32_t	op;
+} __packed;
+
+/*
+ * Form3 Priv_key Decryption Descriptor
+ * Private key is represented by (p,q,dp,dq,c)
+ * f is decrypted data pointer while g is input pointer
+ * of encrypted data
+ */
+struct rsa_priv_frm3_desc_s {
+	uint32_t	desc_hdr;
+	uint32_t	sgf_flg;
+	dma_addr_t	g_dma;
+	dma_addr_t	f_dma;
+	dma_addr_t	c_dma;
+	dma_addr_t	p_dma;
+	dma_addr_t	q_dma;
+	dma_addr_t	dp_dma;
+	dma_addr_t	dq_dma;
+	dma_addr_t	tmp1_dma;
+	dma_addr_t	tmp2_dma;
+	uint32_t	p_q_len;
+	uint32_t	op;
+} __packed;
 
 #endif
diff --git a/drivers/crypto/caam/pkc_desc.c b/drivers/crypto/caam/pkc_desc.c
new file mode 100644
index 0000000..a607dd4
--- /dev/null
+++ b/drivers/crypto/caam/pkc_desc.c
@@ -0,0 +1,253 @@
+/*
+ * \file - pkc_desc.c
+ * \brief - Freescale FSL CAAM support for Public Key Cryptography descriptor
+ *
+ * Author: Yashpal Dutta <yashpal.dutta@freescale.com>
+ *
+ * Copyright 2012 Freescale Semiconductor, Inc.
+ *
+ * There is no shared descriptor for PKC but Job descriptor must carry
+ * all the desired key parameters, input and output pointers
+ *
+ */
+#include "pkc_desc.h"
+
+/*#define CAAM_DEBUG */
+/* Descriptor for RSA Public operation */
+void *caam_rsa_pub_desc(struct rsa_edesc *edesc)
+{
+	u32 start_idx, desc_size;
+	struct rsa_pub_desc_s *rsa_pub_desc =
+	    (struct rsa_pub_desc_s *)edesc->hw_desc;
+	struct rsa_pub_edesc_s *pub_edesc = &edesc->dma_u.rsa_pub_edesc;
+#ifdef CAAM_DEBUG
+	uint32_t i;
+	uint32_t *buf = (uint32_t *)rsa_pub_desc;
+#endif
+
+	desc_size = sizeof(struct rsa_pub_desc_s) / sizeof(u32);
+	start_idx = desc_size - 1;
+	start_idx &= HDR_START_IDX_MASK;
+	init_job_desc(edesc->hw_desc, (start_idx << HDR_START_IDX_SHIFT) |
+		      (start_idx & HDR_DESCLEN_MASK) | HDR_ONE);
+	rsa_pub_desc->n_dma = pub_edesc->n_dma;
+	rsa_pub_desc->e_dma = pub_edesc->e_dma;
+	rsa_pub_desc->f_dma = pub_edesc->f_dma;
+	rsa_pub_desc->g_dma = pub_edesc->g_dma;
+	rsa_pub_desc->sgf_flg = (pub_edesc->sg_flgs.e_len << RSA_PDB_E_SHIFT)
+	    | pub_edesc->sg_flgs.n_len;
+	rsa_pub_desc->msg_len = pub_edesc->f_len;
+	rsa_pub_desc->op = CMD_OPERATION | OP_TYPE_UNI_PROTOCOL |
+	    OP_PCLID_RSAENC_PUBKEY;
+#ifdef CAAM_DEBUG
+	for (i = 0; i < desc_size; i++)
+		pr_debug("[%d] %x\n", i, buf[i]);
+#endif
+	return rsa_pub_desc;
+}
+
+/* Descriptor for RSA Private operation Form1 */
+void *caam_rsa_priv_f1_desc(struct rsa_edesc *edesc)
+{
+	u32 start_idx, desc_size;
+	struct rsa_priv_frm1_desc_s *rsa_priv_desc =
+	    (struct rsa_priv_frm1_desc_s *)edesc->hw_desc;
+	struct rsa_priv_frm1_edesc_s *priv_edesc =
+	    &edesc->dma_u.rsa_priv_f1_edesc;
+
+	desc_size = sizeof(struct rsa_priv_frm1_desc_s) / sizeof(u32);
+	start_idx = desc_size - 1;
+	start_idx &= HDR_START_IDX_MASK;
+	init_job_desc(edesc->hw_desc, (start_idx << HDR_START_IDX_SHIFT) |
+		      (start_idx & HDR_DESCLEN_MASK) | HDR_ONE);
+	rsa_priv_desc->n_dma = priv_edesc->n_dma;
+	rsa_priv_desc->d_dma = priv_edesc->d_dma;
+	rsa_priv_desc->f_dma = priv_edesc->f_dma;
+	rsa_priv_desc->g_dma = priv_edesc->g_dma;
+	/* TBD. Support SG flags */
+	rsa_priv_desc->sgf_flg = (priv_edesc->sg_flgs.d_len << RSA_PDB_D_SHIFT)
+	    | priv_edesc->sg_flgs.n_len;
+	rsa_priv_desc->op = CMD_OPERATION | OP_TYPE_UNI_PROTOCOL |
+	    OP_PCLID_RSADEC_PRVKEY | RSA_PRIV_KEY_FRM_1;
+	return rsa_priv_desc;
+}
+
+/* Descriptor for RSA Private operation Form2 */
+void *caam_rsa_priv_f2_desc(struct rsa_edesc *edesc)
+{
+	u32 start_idx, desc_size;
+	struct rsa_priv_frm2_desc_s *rsa_priv_desc =
+	    (struct rsa_priv_frm2_desc_s *)edesc->hw_desc;
+	struct rsa_priv_frm2_edesc_s *priv_edesc =
+	    &edesc->dma_u.rsa_priv_f2_edesc;
+
+	desc_size = sizeof(struct rsa_priv_frm2_desc_s) / sizeof(u32);
+	start_idx = desc_size - 1;
+	start_idx &= HDR_START_IDX_MASK;
+	init_job_desc(edesc->hw_desc, (start_idx << HDR_START_IDX_SHIFT) |
+		      (start_idx & HDR_DESCLEN_MASK) | HDR_ONE);
+	rsa_priv_desc->p_dma = priv_edesc->p_dma;
+	rsa_priv_desc->q_dma = priv_edesc->q_dma;
+	rsa_priv_desc->d_dma = priv_edesc->d_dma;
+	rsa_priv_desc->f_dma = priv_edesc->f_dma;
+	rsa_priv_desc->g_dma = priv_edesc->g_dma;
+	rsa_priv_desc->tmp1_dma = priv_edesc->tmp1_dma;
+	rsa_priv_desc->tmp2_dma = priv_edesc->tmp2_dma;
+	rsa_priv_desc->sgf_flg = (priv_edesc->sg_flgs.d_len << RSA_PDB_D_SHIFT)
+	    | priv_edesc->sg_flgs.n_len;
+	rsa_priv_desc->p_q_len = (priv_edesc->q_len << RSA_PDB_Q_SHIFT)
+	    | priv_edesc->p_len;
+	rsa_priv_desc->op = CMD_OPERATION | OP_TYPE_UNI_PROTOCOL |
+	    OP_PCLID_RSADEC_PRVKEY | RSA_PRIV_KEY_FRM_2;
+	return rsa_priv_desc;
+}
+
+/* Descriptor for RSA Private operation Form3 */
+void *caam_rsa_priv_f3_desc(struct rsa_edesc *edesc)
+{
+	u32 start_idx, desc_size;
+	struct rsa_priv_frm3_desc_s *rsa_priv_desc =
+	    (struct rsa_priv_frm3_desc_s *)edesc->hw_desc;
+	struct rsa_priv_frm3_edesc_s *priv_edesc =
+	    &edesc->dma_u.rsa_priv_f3_edesc;
+#ifdef CAAM_DEBUG
+	uint32_t *buf = (uint32_t *)rsa_priv_desc;
+	uint32_t i;
+#endif
+
+	desc_size = sizeof(struct rsa_priv_frm3_desc_s) / sizeof(u32);
+	start_idx = desc_size - 1;
+	start_idx &= HDR_START_IDX_MASK;
+	init_job_desc(edesc->hw_desc, (start_idx << HDR_START_IDX_SHIFT) |
+		      (start_idx & HDR_DESCLEN_MASK) | HDR_ONE);
+	rsa_priv_desc->p_dma = priv_edesc->p_dma;
+	rsa_priv_desc->q_dma = priv_edesc->q_dma;
+	rsa_priv_desc->dp_dma = priv_edesc->dp_dma;
+	rsa_priv_desc->dq_dma = priv_edesc->dq_dma;
+	rsa_priv_desc->c_dma = priv_edesc->c_dma;
+	rsa_priv_desc->f_dma = priv_edesc->f_dma;
+	rsa_priv_desc->g_dma = priv_edesc->g_dma;
+	rsa_priv_desc->tmp1_dma = priv_edesc->tmp1_dma;
+	rsa_priv_desc->tmp2_dma = priv_edesc->tmp2_dma;
+	rsa_priv_desc->p_q_len = (priv_edesc->q_len << RSA_PDB_Q_SHIFT)
+	    | priv_edesc->p_len;
+	/* TBD: SG Flags to be filled */
+	rsa_priv_desc->sgf_flg = priv_edesc->sg_flgs.n_len;
+	rsa_priv_desc->op = CMD_OPERATION | OP_TYPE_UNI_PROTOCOL |
+	    OP_PCLID_RSADEC_PRVKEY | RSA_PRIV_KEY_FRM_3;
+#ifdef CAAM_DEBUG
+	for (i = 0; i < desc_size; i++)
+		pr_debug("[%d] %x\n", i, buf[i]);
+#endif
+	return rsa_priv_desc;
+}
+
+/* DSA sign CAAM descriptor */
+void *caam_dsa_sign_desc(struct dsa_edesc_s *edesc)
+{
+	u32 start_idx, desc_size;
+	void *desc;
+
+	if (edesc->req_type == ECDSA_SIGN) {
+		struct ecdsa_sign_desc_s *ecdsa_desc =
+		    (struct ecdsa_sign_desc_s *)edesc->hw_desc;
+		desc_size = sizeof(struct ecdsa_sign_desc_s) / sizeof(u32);
+		start_idx = desc_size - 1;
+		start_idx &= HDR_START_IDX_MASK;
+		init_job_desc(edesc->hw_desc,
+			      (start_idx << HDR_START_IDX_SHIFT) |
+			      (start_idx & HDR_DESCLEN_MASK) | HDR_ONE);
+		ecdsa_desc->sgf_ln = (edesc->l_len << DSA_PDB_L_SHIFT) |
+			((edesc->n_len & DSA_PDB_N_MASK));
+		ecdsa_desc->q_dma = edesc->q_dma;
+		ecdsa_desc->r_dma = edesc->r_dma;
+		ecdsa_desc->g_dma = edesc->g_dma;
+		ecdsa_desc->s_dma = edesc->key_dma;
+		ecdsa_desc->f_dma = edesc->f_dma;
+		ecdsa_desc->c_dma = edesc->c_dma;
+		ecdsa_desc->d_dma = edesc->d_dma;
+		ecdsa_desc->ab_dma = edesc->ab_dma;
+		ecdsa_desc->op = CMD_OPERATION | OP_TYPE_UNI_PROTOCOL |
+		    OP_PCLID_DSASIGN | OP_PCL_PKPROT_ECC;
+		desc = ecdsa_desc;
+	} else {
+		struct dsa_sign_desc_s *dsa_desc =
+		    (struct dsa_sign_desc_s *)edesc->hw_desc;
+		desc_size = sizeof(struct dsa_sign_desc_s) / sizeof(u32);
+		start_idx = desc_size - 1;
+		start_idx &= HDR_START_IDX_MASK;
+		init_job_desc(edesc->hw_desc,
+			      (start_idx << HDR_START_IDX_SHIFT) |
+			      (start_idx & HDR_DESCLEN_MASK) | HDR_ONE);
+		dsa_desc->sgf_ln = (edesc->l_len << DSA_PDB_L_SHIFT) |
+			((edesc->n_len & DSA_PDB_N_MASK));
+		dsa_desc->q_dma = edesc->q_dma;
+		dsa_desc->r_dma = edesc->r_dma;
+		dsa_desc->g_dma = edesc->g_dma;
+		dsa_desc->s_dma = edesc->key_dma;
+		dsa_desc->f_dma = edesc->f_dma;
+		dsa_desc->c_dma = edesc->c_dma;
+		dsa_desc->d_dma = edesc->d_dma;
+		dsa_desc->op = CMD_OPERATION | OP_TYPE_UNI_PROTOCOL |
+		    OP_PCLID_DSASIGN;
+		desc = dsa_desc;
+	}
+
+	return desc;
+}
+
+/* DSA verify CAAM descriptor */
+void *caam_dsa_verify_desc(struct dsa_edesc_s *edesc)
+{
+	u32 start_idx, desc_size;
+	void *desc;
+
+	if (edesc->req_type == ECDSA_VERIFY) {
+		struct ecdsa_verify_desc_s *ecdsa_desc =
+		    (struct ecdsa_verify_desc_s *)edesc->hw_desc;
+		desc_size = sizeof(struct ecdsa_verify_desc_s) / sizeof(u32);
+		start_idx = desc_size - 1;
+		start_idx &= HDR_START_IDX_MASK;
+		init_job_desc(edesc->hw_desc,
+			      (start_idx << HDR_START_IDX_SHIFT) |
+			      (start_idx & HDR_DESCLEN_MASK) | HDR_ONE);
+		ecdsa_desc->sgf_ln = (edesc->l_len << DSA_PDB_L_SHIFT) |
+			((edesc->n_len & DSA_PDB_N_MASK));
+		ecdsa_desc->q_dma = edesc->q_dma;
+		ecdsa_desc->r_dma = edesc->r_dma;
+		ecdsa_desc->g_dma = edesc->g_dma;
+		ecdsa_desc->w_dma = edesc->key_dma;
+		ecdsa_desc->f_dma = edesc->f_dma;
+		ecdsa_desc->c_dma = edesc->c_dma;
+		ecdsa_desc->d_dma = edesc->d_dma;
+		ecdsa_desc->tmp_dma = edesc->tmp_dma;
+		ecdsa_desc->ab_dma = edesc->ab_dma;
+		ecdsa_desc->op = CMD_OPERATION | OP_TYPE_UNI_PROTOCOL |
+		    OP_PCLID_DSAVERIFY | OP_PCL_PKPROT_ECC;
+		desc = ecdsa_desc;
+
+	} else {
+		struct dsa_verify_desc_s *dsa_desc =
+		    (struct dsa_verify_desc_s *)edesc->hw_desc;
+		desc_size = sizeof(struct dsa_verify_desc_s) / sizeof(u32);
+		start_idx = desc_size - 1;
+		start_idx &= HDR_START_IDX_MASK;
+		init_job_desc(edesc->hw_desc,
+			      (start_idx << HDR_START_IDX_SHIFT) |
+			      (start_idx & HDR_DESCLEN_MASK) | HDR_ONE);
+		dsa_desc->sgf_ln = (edesc->l_len << DSA_PDB_L_SHIFT) |
+			((edesc->n_len & DSA_PDB_N_MASK));
+		dsa_desc->q_dma = edesc->q_dma;
+		dsa_desc->r_dma = edesc->r_dma;
+		dsa_desc->g_dma = edesc->g_dma;
+		dsa_desc->w_dma = edesc->key_dma;
+		dsa_desc->f_dma = edesc->f_dma;
+		dsa_desc->c_dma = edesc->c_dma;
+		dsa_desc->d_dma = edesc->d_dma;
+		dsa_desc->tmp_dma = edesc->tmp_dma;
+		dsa_desc->op = CMD_OPERATION | OP_TYPE_UNI_PROTOCOL |
+		    OP_PCLID_DSAVERIFY;
+		desc = dsa_desc;
+	}
+	return desc;
+}
diff --git a/drivers/crypto/caam/pkc_desc.h b/drivers/crypto/caam/pkc_desc.h
new file mode 100644
index 0000000..56fc2ee
--- /dev/null
+++ b/drivers/crypto/caam/pkc_desc.h
@@ -0,0 +1,201 @@
+/*
+ * \file - pkc_desc.h
+ * \brief - Freescale FSL CAAM support for Public Key Cryptography
+ *
+ * Author: Yashpal Dutta <yashpal.dutta@freescale.com>
+ *
+ * Copyright 2012 Freescale Semiconductor, Inc.
+ *
+ * There is no shared descriptor for PKC but Job descriptor must carry
+ * all the desired key parameters, input and output pointers
+ *
+ */
+#ifndef _PKC_DESC_H_
+#define _PKC_DESC_H_
+
+#include "compat.h"
+
+#include "regs.h"
+#include "intern.h"
+#include "desc_constr.h"
+#include "jr.h"
+#include "error.h"
+#include "sg_sw_sec4.h"
+#include <linux/crypto.h>
+#include "pdb.h"
+
+/* Public key SG Flags and e, n lengths */
+struct sg_public_flg_s {
+	uint32_t sg_f:1;
+	uint32_t sg_g:1;
+	uint32_t sg_n:1;
+	uint32_t sg_e:1;
+	uint32_t reserved:4;
+	uint32_t e_len:12;
+	uint32_t n_len:12;
+};
+
+/* RSA public Extended Descriptor fields
+ @n_dma - n, e represents the public key
+ @e_dma - Public key exponent,  n is modulus
+ @g_dma - Output RSA-encrypted value
+ @f_dma - Input RSA-Plain value
+ @n_len, e_len - Public key param length
+ @ f_len - input data len
+ */
+struct rsa_pub_edesc_s {
+	dma_addr_t n_dma;
+	dma_addr_t e_dma;
+	dma_addr_t g_dma;
+	dma_addr_t f_dma;
+	struct sg_public_flg_s sg_flgs;
+	uint32_t reserved:20;
+	uint32_t f_len:12;
+};
+
+/* Private key Form1 SG Flags and d, n lengths */
+struct sg_priv_f1_flg_s {
+	uint32_t sg_g:1;
+	uint32_t sg_f:1;
+	uint32_t sg_n:1;
+	uint32_t sg_d:1;
+	uint32_t reserved:4;
+	uint32_t d_len:12;
+	uint32_t n_len:12;
+};
+
+/* RSA PrivKey Form1 Extended Descriptor fields
+ @n_dma - n, d represents the private key form1 representation
+ @d_dma - d is the private exponent, n is the modules
+ @g_dma - Input RSA-encrypted value
+ @f_dma - Output RSA-Plain value
+ */
+struct rsa_priv_frm1_edesc_s {
+	dma_addr_t n_dma;
+	dma_addr_t d_dma;
+	dma_addr_t f_dma;
+	dma_addr_t g_dma;
+	struct sg_priv_f1_flg_s sg_flgs;
+};
+
+/* Private key Form2 SG Flags and d, n lengths */
+struct sg_priv_f2_flg_s {
+	uint32_t sg_g:1;
+	uint32_t sg_f:1;
+	uint32_t sg_d:1;
+	uint32_t sg_p:1;
+	uint32_t sg_q:1;
+	uint32_t sg_tmp1:1;
+	uint32_t sg_tmp2:1;
+	uint32_t reserved:1;
+	uint32_t d_len:12;
+	uint32_t n_len:12;
+};
+
+/* RSA PrivKey Form2
+ @p, q, d represents the private key form2 representation
+ @d - d is private exponent, p and q are the two primes
+ @f - output pointer
+ @g - input pointer
+ */
+struct rsa_priv_frm2_edesc_s {
+	dma_addr_t p_dma;
+	dma_addr_t q_dma;
+	dma_addr_t d_dma;
+	dma_addr_t f_dma;
+	dma_addr_t g_dma;
+	int8_t *tmp1;
+	int8_t *tmp2;
+	dma_addr_t tmp1_dma;
+	dma_addr_t tmp2_dma;
+	struct sg_priv_f2_flg_s sg_flgs;
+	uint32_t reserved:8;
+	uint32_t q_len:12;
+	uint32_t p_len:12;
+};
+
+/* Private key Form3 SG Flags and d, n lengths */
+struct sg_priv_f3_flg_s {
+	uint32_t sg_g:1;
+	uint32_t sg_f:1;
+	uint32_t sg_c:1;
+	uint32_t sg_p:1;
+	uint32_t sg_q:1;
+	uint32_t sg_dp:1;
+	uint32_t sg_dq:1;
+	uint32_t sg_tmp1:1;
+	uint32_t sg_tmp2:1;
+	uint32_t reserved:11;
+	uint32_t n_len:12;
+};
+
+/* RSA PrivKey Form3
+ @n - p, q, dp, dq, c represents the private key form3 representation
+ @dp - First CRT exponent factor
+ @dq - Second CRT exponent factor
+ @c - CRT Coefficient
+ @f - output pointer
+ @g - input pointer
+ */
+struct rsa_priv_frm3_edesc_s {
+	dma_addr_t p_dma;
+	dma_addr_t q_dma;
+	dma_addr_t dp_dma;
+	dma_addr_t dq_dma;
+	dma_addr_t c_dma;
+	dma_addr_t f_dma;
+	dma_addr_t g_dma;
+	int8_t *tmp1;
+	int8_t *tmp2;
+	dma_addr_t tmp1_dma;
+	dma_addr_t tmp2_dma;
+	struct sg_priv_f3_flg_s sg_flgs;
+	uint32_t reserved:8;
+	uint32_t q_len:12;
+	uint32_t p_len:12;
+};
+
+/*
+ * rsa_edesc - s/w-extended rsa descriptor
+ * @hw_desc: the h/w job descriptor
+ */
+struct rsa_edesc {
+	enum pkc_req_type req_type;
+	union {
+		struct rsa_pub_edesc_s rsa_pub_edesc;
+		struct rsa_priv_frm1_edesc_s rsa_priv_f1_edesc;
+		struct rsa_priv_frm2_edesc_s rsa_priv_f2_edesc;
+		struct rsa_priv_frm3_edesc_s rsa_priv_f3_edesc;
+	} dma_u;
+	u32 hw_desc[];
+};
+
+/*
+ * dsa_edesc - s/w-extended for dsa and ecdsa descriptors
+ * @hw_desc: the h/w job descriptor
+ */
+struct dsa_edesc_s {
+	enum pkc_req_type req_type;
+	uint32_t l_len;
+	uint32_t n_len;
+	dma_addr_t key_dma;
+	dma_addr_t f_dma;
+	dma_addr_t q_dma;
+	dma_addr_t r_dma;
+	dma_addr_t c_dma;
+	dma_addr_t d_dma;
+	dma_addr_t ab_dma;
+	dma_addr_t g_dma;
+	dma_addr_t tmp_dma;
+	uint8_t *tmp; /* Allocate locally for dsa_verify */
+	u32 hw_desc[];
+};
+
+void *caam_rsa_pub_desc(struct rsa_edesc *);
+void *caam_rsa_priv_f1_desc(struct rsa_edesc *);
+void *caam_rsa_priv_f2_desc(struct rsa_edesc *);
+void *caam_rsa_priv_f3_desc(struct rsa_edesc *);
+void *caam_dsa_sign_desc(struct dsa_edesc_s *);
+void *caam_dsa_verify_desc(struct dsa_edesc_s *);
+
+#endif
-- 
1.7.5.4

