From 66ca73d8458bf6737f05c07fbca056f091c9249c Mon Sep 17 00:00:00 2001
From: Haiying Wang <Haiying.Wang@freescale.com>
Date: Thu, 23 Apr 2015 14:28:56 -0400
Subject: [PATCH 0634/1429] dpio/qbman: rename qbman_dq_entry to qbman_result

Currently qbman_dq_entry is used for both dq result in dqrr
and memory, and notifications in dqrr and memory. It doesn't
make sense to have dq_entry in name for those notifications
which have nothing to do with dq. So we rename this as
qbman_result which is meaningful for both cases.

Signed-off-by: Haiying Wang <Haiying.Wang@freescale.com>
Change-Id: I62b3e729c571a1195e8802a9fab3fca97a14eae4
Reviewed-on: http://git.am.freescale.net:8181/35535
Tested-by: Review Code-CDREVIEW <CDREVIEW@freescale.com>
Reviewed-by: Roy Pledge <roy.pledge@freescale.com>
Reviewed-by: Stuart Yoder <stuart.yoder@freescale.com>
[Original patch taken from QorIQ-SDK-V2.0-20160527-yocto]
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 drivers/staging/fsl-mc/bus/dpio/dpio_service.c     |    8 +-
 drivers/staging/fsl-mc/bus/dpio/fsl_qbman_portal.h |   54 ++++++------
 drivers/staging/fsl-mc/bus/dpio/qbman_portal.c     |   98 ++++++++++----------
 drivers/staging/fsl-mc/bus/dpio/qbman_sys_decl.h   |    2 +-
 drivers/staging/fsl-mc/bus/dpio/qbman_test.c       |    2 +-
 5 files changed, 82 insertions(+), 82 deletions(-)

diff --git a/drivers/staging/fsl-mc/bus/dpio/dpio_service.c b/drivers/staging/fsl-mc/bus/dpio/dpio_service.c
index be460a0..38f1699 100644
--- a/drivers/staging/fsl-mc/bus/dpio/dpio_service.c
+++ b/drivers/staging/fsl-mc/bus/dpio/dpio_service.c
@@ -336,11 +336,11 @@ int dpaa_io_poll(struct dpaa_io *obj)
 	swp = obj->object.swp;
 	dq = qbman_swp_dqrr_next(swp);
 	if (dq) {
-		if (qbman_dq_entry_is_FQDAN(dq)) {
+		if (qbman_result_is_FQDAN(dq)) {
 			struct dpaa_io_notification_ctx *ctx;
 			uint64_t q64;
 
-			q64 = qbman_dq_entry_SCN_ctx(dq);
+			q64 = qbman_result_SCN_ctx(dq);
 			ctx = (void *)q64;
 			ctx->cb(ctx);
 		} else
@@ -657,12 +657,12 @@ struct ldpaa_dq *dpaa_io_store_next(struct dpaa_io_store *s, int *is_last)
 	int match;
 	struct ldpaa_dq *ret = &s->vaddr[s->idx];
 
-	match = qbman_dq_entry_has_new_result(s->swp, ret);
+	match = qbman_result_has_new_result(s->swp, ret);
 	if (!match) {
 		*is_last = 0;
 		return NULL;
 	}
-	BUG_ON(!qbman_dq_entry_is_DQ(ret));
+	BUG_ON(!qbman_result_is_DQ(ret));
 	s->idx++;
 	if (ldpaa_dq_is_pull_complete(ret)) {
 		*is_last = 1;
diff --git a/drivers/staging/fsl-mc/bus/dpio/fsl_qbman_portal.h b/drivers/staging/fsl-mc/bus/dpio/fsl_qbman_portal.h
index 09121b9..c52527f 100644
--- a/drivers/staging/fsl-mc/bus/dpio/fsl_qbman_portal.h
+++ b/drivers/staging/fsl-mc/bus/dpio/fsl_qbman_portal.h
@@ -161,12 +161,12 @@ void qbman_swp_dqrr_consume(struct qbman_swp *, const struct ldpaa_dq *);
  * conversion to ensure that the user's dequeue result storage is in host-endian
  * format (whether or not that is the same as the little-endian format that
  * hardware DMA'd to the user's storage). As such, once the user has called
- * qbman_dq_entry_has_new_result() and been returned a valid dequeue result,
+ * qbman_result_has_new_result() and been returned a valid dequeue result,
  * they should not call it again on the same memory location (except of course
  * if another dequeue command has been executed to produce a new result to that
  * location).
  */
-int qbman_dq_entry_has_new_result(struct qbman_swp *,
+int qbman_result_has_new_result(struct qbman_swp *,
 				  const struct ldpaa_dq *);
 
 /* -------------------------------------------------------- */
@@ -174,50 +174,50 @@ int qbman_dq_entry_has_new_result(struct qbman_swp *,
 /* -------------------------------------------------------- */
 
 /* DQRR entries may contain non-dequeue results, ie. notifications */
-int qbman_dq_entry_is_DQ(const struct ldpaa_dq *);
+int qbman_result_is_DQ(const struct ldpaa_dq *);
 /* All the non-dequeue results (FQDAN/CDAN/CSCN/...) are "state change
  * notifications" of one type or another. Some APIs apply to all of them, of the
- * form qbman_dq_entry_SCN_***(). */
-static inline int qbman_dq_entry_is_SCN(const struct ldpaa_dq *dq)
+ * form qbman_result_SCN_***(). */
+static inline int qbman_result_is_SCN(const struct ldpaa_dq *dq)
 {
-	return !qbman_dq_entry_is_DQ(dq);
+	return !qbman_result_is_DQ(dq);
 }
 /* Recognise different notification types, only required if the user allows for
  * these to occur, and cares about them when they do. */
-int qbman_dq_entry_is_FQDAN(const struct ldpaa_dq *);
+int qbman_result_is_FQDAN(const struct ldpaa_dq *);
 				/* FQ Data Availability */
-int qbman_dq_entry_is_CDAN(const struct ldpaa_dq *);
+int qbman_result_is_CDAN(const struct ldpaa_dq *);
 				/* Channel Data Availability */
-int qbman_dq_entry_is_CSCN(const struct ldpaa_dq *);
+int qbman_result_is_CSCN(const struct ldpaa_dq *);
 				/* Congestion State Change */
-int qbman_dq_entry_is_BPSCN(const struct ldpaa_dq *);
+int qbman_result_is_BPSCN(const struct ldpaa_dq *);
 				/* Buffer Pool State Change */
-int qbman_dq_entry_is_CGCU(const struct ldpaa_dq *);
+int qbman_result_is_CGCU(const struct ldpaa_dq *);
 				/* Congestion Group Count Update */
 /* Frame queue state change notifications; (FQDAN in theory counts too as it
  * leaves a FQ parked, but it is primarily a data availability notification) */
-int qbman_dq_entry_is_FQRN(const struct ldpaa_dq *); /* Retirement */
-int qbman_dq_entry_is_FQRNI(const struct ldpaa_dq *);
+int qbman_result_is_FQRN(const struct ldpaa_dq *); /* Retirement */
+int qbman_result_is_FQRNI(const struct ldpaa_dq *);
 				/* Retirement Immediate */
-int qbman_dq_entry_is_FQPN(const struct ldpaa_dq *); /* Park */
+int qbman_result_is_FQPN(const struct ldpaa_dq *); /* Park */
 
 /* NB: for parsing dequeue results (when "is_DQ" is TRUE), use the higher-layer
  * ldpaa_dq_*() functions. */
 
 /* State-change notifications (FQDAN/CDAN/CSCN/...). */
-uint8_t qbman_dq_entry_SCN_state(const struct ldpaa_dq *);
-uint32_t qbman_dq_entry_SCN_rid(const struct ldpaa_dq *);
-uint64_t qbman_dq_entry_SCN_ctx(const struct ldpaa_dq *);
+uint8_t qbman_result_SCN_state(const struct ldpaa_dq *);
+uint32_t qbman_result_SCN_rid(const struct ldpaa_dq *);
+uint64_t qbman_result_SCN_ctx(const struct ldpaa_dq *);
 /* Type-specific "resource IDs". Mainly for illustration purposes, though it
  * also gives the appropriate type widths. */
-#define qbman_dq_entry_FQDAN_fqid(dq) qbman_dq_entry_SCN_rid(dq)
-#define qbman_dq_entry_FQRN_fqid(dq) qbman_dq_entry_SCN_rid(dq)
-#define qbman_dq_entry_FQRNI_fqid(dq) qbman_dq_entry_SCN_rid(dq)
-#define qbman_dq_entry_FQPN_fqid(dq) qbman_dq_entry_SCN_rid(dq)
-#define qbman_dq_entry_CDAN_cid(dq) ((uint16_t)qbman_dq_entry_SCN_rid(dq))
-#define qbman_dq_entry_CSCN_cgid(dq) ((uint16_t)qbman_dq_entry_SCN_rid(dq))
-#define qbman_dq_entry_CGCU_cgid(dq) ((uint16_t)qbman_dq_entry_SCN_rid(dq))
-#define qbman_dq_entry_BPSCN_bpid(dq) ((uint16_t)qbman_dq_entry_SCN_rid(dq))
+#define qbman_result_FQDAN_fqid(dq) qbman_result_SCN_rid(dq)
+#define qbman_result_FQRN_fqid(dq) qbman_result_SCN_rid(dq)
+#define qbman_result_FQRNI_fqid(dq) qbman_result_SCN_rid(dq)
+#define qbman_result_FQPN_fqid(dq) qbman_result_SCN_rid(dq)
+#define qbman_result_CDAN_cid(dq) ((uint16_t)qbman_result_SCN_rid(dq))
+#define qbman_result_CSCN_cgid(dq) ((uint16_t)qbman_result_SCN_rid(dq))
+#define qbman_result_CGCU_cgid(dq) ((uint16_t)qbman_result_SCN_rid(dq))
+#define qbman_result_BPSCN_bpid(dq) ((uint16_t)qbman_result_SCN_rid(dq))
 
 	/************/
 	/* Enqueues */
@@ -336,7 +336,7 @@ int qbman_swp_fq_schedule(struct qbman_swp *, uint32_t fqid);
  * and thus be available for selection by any channel-dequeuing behaviour (push
  * or pull). If the FQ is subsequently "dequeued" from the channel and is still
  * empty at the time this happens, the resulting dq_entry will have no FD.
- * (qbman_dq_entry_DQ_fd() will return NULL.) */
+ * (qbman_result_DQ_fd() will return NULL.) */
 int qbman_swp_fq_force(struct qbman_swp *, uint32_t fqid);
 
 /* These functions change the FQ flow-control stuff between XON/XOFF. (The
@@ -345,7 +345,7 @@ int qbman_swp_fq_force(struct qbman_swp *, uint32_t fqid);
  * non-empty, meaning they won't be selected for scheduled dequeuing. If a FQ is
  * changed to XOFF after it had already become truly-scheduled to a channel, and
  * a pull dequeue of that channel occurs that selects that FQ for dequeuing,
- * then the resulting dq_entry will have no FD. (qbman_dq_entry_DQ_fd() will
+ * then the resulting dq_entry will have no FD. (qbman_result_DQ_fd() will
  * return NULL.) */
 int qbman_swp_fq_xon(struct qbman_swp *, uint32_t fqid);
 int qbman_swp_fq_xoff(struct qbman_swp *, uint32_t fqid);
diff --git a/drivers/staging/fsl-mc/bus/dpio/qbman_portal.c b/drivers/staging/fsl-mc/bus/dpio/qbman_portal.c
index daf8bcc..349b3dc 100644
--- a/drivers/staging/fsl-mc/bus/dpio/qbman_portal.c
+++ b/drivers/staging/fsl-mc/bus/dpio/qbman_portal.c
@@ -612,16 +612,16 @@ static struct qb_attr_code code_dqrr_byte_count = QB_CODE(4, 0, 32);
 static struct qb_attr_code code_dqrr_frame_count = QB_CODE(5, 0, 24);
 static struct qb_attr_code code_dqrr_ctx_lo = QB_CODE(6, 0, 32);
 
-#define QBMAN_DQRR_RESPONSE_DQ        0x60
-#define QBMAN_DQRR_RESPONSE_FQRN      0x21
-#define QBMAN_DQRR_RESPONSE_FQRNI     0x22
-#define QBMAN_DQRR_RESPONSE_FQPN      0x24
-#define QBMAN_DQRR_RESPONSE_FQDAN     0x25
-#define QBMAN_DQRR_RESPONSE_CDAN      0x26
-#define QBMAN_DQRR_RESPONSE_CSCN_MEM  0x27
-#define QBMAN_DQRR_RESPONSE_CGCU      0x28
-#define QBMAN_DQRR_RESPONSE_BPSCN     0x29
-#define QBMAN_DQRR_RESPONSE_CSCN_WQ   0x2a
+#define QBMAN_RESULT_DQ        0x60
+#define QBMAN_RESULT_FQRN      0x21
+#define QBMAN_RESULT_FQRNI     0x22
+#define QBMAN_RESULT_FQPN      0x24
+#define QBMAN_RESULT_FQDAN     0x25
+#define QBMAN_RESULT_CDAN      0x26
+#define QBMAN_RESULT_CSCN_MEM  0x27
+#define QBMAN_RESULT_CGCU      0x28
+#define QBMAN_RESULT_BPSCN     0x29
+#define QBMAN_RESULT_CSCN_WQ   0x2a
 
 static struct qb_attr_code code_dqpi_pi = QB_CODE(0, 0, 4);
 
@@ -698,7 +698,7 @@ const struct ldpaa_dq *qbman_swp_dqrr_next(struct qbman_swp *s)
 	   indicate that the vdq is no longer busy */
 	flags = ldpaa_dq_flags(dq);
 	response_verb = qb_attr_code_decode(&code_dqrr_response, &verb);
-	if ((response_verb == QBMAN_DQRR_RESPONSE_DQ) &&
+	if ((response_verb == QBMAN_RESULT_DQ) &&
 	    (flags & LDPAA_DQ_STAT_VOLATILE) &&
 	    (flags & LDPAA_DQ_STAT_EXPIRED))
 		atomic_inc(&s->vdq.busy);
@@ -720,7 +720,7 @@ EXPORT_SYMBOL(qbman_swp_dqrr_consume);
 /* Polling user-provided storage */
 /*********************************/
 
-int qbman_dq_entry_has_new_result(struct qbman_swp *s,
+int qbman_result_has_new_result(struct qbman_swp *s,
 				  const struct ldpaa_dq *dq)
 {
 	/* To avoid converting the little-endian DQ entry to host-endian prior
@@ -763,13 +763,13 @@ int qbman_dq_entry_has_new_result(struct qbman_swp *s,
 	}
 	return 1;
 }
-EXPORT_SYMBOL(qbman_dq_entry_has_new_result);
+EXPORT_SYMBOL(qbman_result_has_new_result);
 
 /********************************/
 /* Categorising dequeue entries */
 /********************************/
 
-static inline int __qbman_dq_entry_is_x(const struct ldpaa_dq *dq, uint32_t x)
+static inline int __qbman_result_is_x(const struct ldpaa_dq *dq, uint32_t x)
 {
 	const uint32_t *p = qb_cl(dq);
 	uint32_t response_verb = qb_attr_code_decode(&code_dqrr_response, p);
@@ -777,66 +777,66 @@ static inline int __qbman_dq_entry_is_x(const struct ldpaa_dq *dq, uint32_t x)
 	return response_verb == x;
 }
 
-int qbman_dq_entry_is_DQ(const struct ldpaa_dq *dq)
+int qbman_result_is_DQ(const struct ldpaa_dq *dq)
 {
-	return __qbman_dq_entry_is_x(dq, QBMAN_DQRR_RESPONSE_DQ);
+	return __qbman_result_is_x(dq, QBMAN_RESULT_DQ);
 }
-EXPORT_SYMBOL(qbman_dq_entry_is_DQ);
+EXPORT_SYMBOL(qbman_result_is_DQ);
 
-int qbman_dq_entry_is_FQDAN(const struct ldpaa_dq *dq)
+int qbman_result_is_FQDAN(const struct ldpaa_dq *dq)
 {
-	return __qbman_dq_entry_is_x(dq, QBMAN_DQRR_RESPONSE_FQDAN);
+	return __qbman_result_is_x(dq, QBMAN_RESULT_FQDAN);
 }
-EXPORT_SYMBOL(qbman_dq_entry_is_FQDAN);
+EXPORT_SYMBOL(qbman_result_is_FQDAN);
 
-int qbman_dq_entry_is_CDAN(const struct ldpaa_dq *dq)
+int qbman_result_is_CDAN(const struct ldpaa_dq *dq)
 {
-	return __qbman_dq_entry_is_x(dq, QBMAN_DQRR_RESPONSE_CDAN);
+	return __qbman_result_is_x(dq, QBMAN_RESULT_CDAN);
 }
-EXPORT_SYMBOL(qbman_dq_entry_is_CDAN);
+EXPORT_SYMBOL(qbman_result_is_CDAN);
 
-int qbman_dq_entry_is_CSCN(const struct ldpaa_dq *dq)
+int qbman_result_is_CSCN(const struct ldpaa_dq *dq)
 {
-	return __qbman_dq_entry_is_x(dq, QBMAN_DQRR_RESPONSE_CSCN_MEM) ||
-		__qbman_dq_entry_is_x(dq, QBMAN_DQRR_RESPONSE_CSCN_WQ);
+	return __qbman_result_is_x(dq, QBMAN_RESULT_CSCN_MEM) ||
+		__qbman_result_is_x(dq, QBMAN_RESULT_CSCN_WQ);
 }
-EXPORT_SYMBOL(qbman_dq_entry_is_CSCN);
+EXPORT_SYMBOL(qbman_result_is_CSCN);
 
-int qbman_dq_entry_is_BPSCN(const struct ldpaa_dq *dq)
+int qbman_result_is_BPSCN(const struct ldpaa_dq *dq)
 {
-	return __qbman_dq_entry_is_x(dq, QBMAN_DQRR_RESPONSE_BPSCN);
+	return __qbman_result_is_x(dq, QBMAN_RESULT_BPSCN);
 }
-EXPORT_SYMBOL(qbman_dq_entry_is_BPSCN);
+EXPORT_SYMBOL(qbman_result_is_BPSCN);
 
-int qbman_dq_entry_is_CGCU(const struct ldpaa_dq *dq)
+int qbman_result_is_CGCU(const struct ldpaa_dq *dq)
 {
-	return __qbman_dq_entry_is_x(dq, QBMAN_DQRR_RESPONSE_CGCU);
+	return __qbman_result_is_x(dq, QBMAN_RESULT_CGCU);
 }
-EXPORT_SYMBOL(qbman_dq_entry_is_CGCU);
+EXPORT_SYMBOL(qbman_result_is_CGCU);
 
-int qbman_dq_entry_is_FQRN(const struct ldpaa_dq *dq)
+int qbman_result_is_FQRN(const struct ldpaa_dq *dq)
 {
-	return __qbman_dq_entry_is_x(dq, QBMAN_DQRR_RESPONSE_FQRN);
+	return __qbman_result_is_x(dq, QBMAN_RESULT_FQRN);
 }
-EXPORT_SYMBOL(qbman_dq_entry_is_FQRN);
+EXPORT_SYMBOL(qbman_result_is_FQRN);
 
-int qbman_dq_entry_is_FQRNI(const struct ldpaa_dq *dq)
+int qbman_result_is_FQRNI(const struct ldpaa_dq *dq)
 {
-	return __qbman_dq_entry_is_x(dq, QBMAN_DQRR_RESPONSE_FQRNI);
+	return __qbman_result_is_x(dq, QBMAN_RESULT_FQRNI);
 }
-EXPORT_SYMBOL(qbman_dq_entry_is_FQRNI);
+EXPORT_SYMBOL(qbman_result_is_FQRNI);
 
-int qbman_dq_entry_is_FQPN(const struct ldpaa_dq *dq)
+int qbman_result_is_FQPN(const struct ldpaa_dq *dq)
 {
-	return __qbman_dq_entry_is_x(dq, QBMAN_DQRR_RESPONSE_FQPN);
+	return __qbman_result_is_x(dq, QBMAN_RESULT_FQPN);
 }
-EXPORT_SYMBOL(qbman_dq_entry_is_FQPN);
+EXPORT_SYMBOL(qbman_result_is_FQPN);
 
 /*********************************/
 /* Parsing frame dequeue results */
 /*********************************/
 
-/* These APIs assume qbman_dq_entry_is_DQ() is TRUE */
+/* These APIs assume qbman_result_is_DQ() is TRUE */
 
 uint32_t ldpaa_dq_flags(const struct ldpaa_dq *dq)
 {
@@ -910,29 +910,29 @@ static struct qb_attr_code code_scn_state = QB_CODE(0, 16, 8);
 static struct qb_attr_code code_scn_rid = QB_CODE(1, 0, 24);
 static struct qb_attr_code code_scn_ctx_lo = QB_CODE(2, 0, 32);
 
-uint8_t qbman_dq_entry_SCN_state(const struct ldpaa_dq *dq)
+uint8_t qbman_result_SCN_state(const struct ldpaa_dq *dq)
 {
 	const uint32_t *p = qb_cl(dq);
 
 	return (uint8_t)qb_attr_code_decode(&code_scn_state, p);
 }
-EXPORT_SYMBOL(qbman_dq_entry_SCN_state);
+EXPORT_SYMBOL(qbman_result_SCN_state);
 
-uint32_t qbman_dq_entry_SCN_rid(const struct ldpaa_dq *dq)
+uint32_t qbman_result_SCN_rid(const struct ldpaa_dq *dq)
 {
 	const uint32_t *p = qb_cl(dq);
 
 	return qb_attr_code_decode(&code_scn_rid, p);
 }
-EXPORT_SYMBOL(qbman_dq_entry_SCN_rid);
+EXPORT_SYMBOL(qbman_result_SCN_rid);
 
-uint64_t qbman_dq_entry_SCN_ctx(const struct ldpaa_dq *dq)
+uint64_t qbman_result_SCN_ctx(const struct ldpaa_dq *dq)
 {
 	const uint64_t *p = (uint64_t *)qb_cl(dq);
 
 	return qb_attr_code_decode_64(&code_scn_ctx_lo, p);
 }
-EXPORT_SYMBOL(qbman_dq_entry_SCN_ctx);
+EXPORT_SYMBOL(qbman_result_SCN_ctx);
 
 /******************/
 /* Buffer release */
diff --git a/drivers/staging/fsl-mc/bus/dpio/qbman_sys_decl.h b/drivers/staging/fsl-mc/bus/dpio/qbman_sys_decl.h
index 3dc6065..4cef852 100644
--- a/drivers/staging/fsl-mc/bus/dpio/qbman_sys_decl.h
+++ b/drivers/staging/fsl-mc/bus/dpio/qbman_sys_decl.h
@@ -49,7 +49,7 @@
 #include "fsl_qbman_base.h"
 
 /* The platform-independent code shouldn't need endianness, except for
- * weird/fast-path cases like qbman_dq_entry_has_token(), which needs to
+ * weird/fast-path cases like qbman_result_has_token(), which needs to
  * perform a passive and endianness-specific test on a read-only data structure
  * very quickly. It's an exception, and this symbol is used for that case. */
 #if defined(__BIG_ENDIAN)
diff --git a/drivers/staging/fsl-mc/bus/dpio/qbman_test.c b/drivers/staging/fsl-mc/bus/dpio/qbman_test.c
index 2e4910a..1bb22b4 100644
--- a/drivers/staging/fsl-mc/bus/dpio/qbman_test.c
+++ b/drivers/staging/fsl-mc/bus/dpio/qbman_test.c
@@ -231,7 +231,7 @@ static void do_pull_dequeue(struct qbman_swp *swp)
 		DBG_POLL_START(loopvar);
 		do {
 			DBG_POLL_CHECK(loopvar);
-			ret = qbman_dq_entry_has_new_result(swp,
+			ret = qbman_result_has_new_result(swp,
 							    &dq_storage[i]);
 		} while (!ret);
 
-- 
1.7.5.4

