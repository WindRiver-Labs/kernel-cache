From 76a1581377d4dfd61cf16af2ed63b904406606d9 Mon Sep 17 00:00:00 2001
From: Roy Pledge <roy.pledge@freescale.com>
Date: Wed, 6 Mar 2013 14:55:29 +0200
Subject: [PATCH 251/547] Cleanup USDPAA resources when a process exits

Add an API to query the usage levels of DMA memory.
Elminates UIO and converts USDPAA to interact with the kernel via a
character device.
Removed misc/devices fsl_usdpaa file.

Remove unneeded file.

Change-Id: I797548e2c79f7098e6378484bcae2b455f022cd4
Reviewed-on: http://git.am.freescale.net:8181/1546
Reviewed-by: Ladouceur Jeffrey-R11498 <Jeffrey.Ladouceur@freescale.com>
Reviewed-by: Wang Haiying-R54964 <Haiying.Wang@freescale.com>
Reviewed-by: Fleming Andrew-AFLEMING <AFLEMING@freescale.com>
Tested-by: Fleming Andrew-AFLEMING <AFLEMING@freescale.com>
[Original patch taken from QorIQ-SDK-V1.4-SOURCE-20130625-yocto.iso
 Just a minor modification for do_mmap_pgoff to port 3.10 kernel]
Signed-off-by: Bin Jiang <bin.jiang@windriver.com>
---
 drivers/misc/Makefile                      |    1 -
 drivers/misc/fsl_usdpaa.c                  |  701 --------------
 drivers/staging/fsl_qbman/Kconfig          |    9 -
 drivers/staging/fsl_qbman/Makefile         |    2 +-
 drivers/staging/fsl_qbman/bman_driver.c    |   86 +-
 drivers/staging/fsl_qbman/bman_high.c      |  124 ++-
 drivers/staging/fsl_qbman/bman_low.h       |   87 +-
 drivers/staging/fsl_qbman/bman_private.h   |   10 +
 drivers/staging/fsl_qbman/dpa_alloc.c      |  339 +++++--
 drivers/staging/fsl_qbman/dpa_sys.h        |   27 -
 drivers/staging/fsl_qbman/fsl_usdpaa.c     | 1399 ++++++++++++++++++++++++++++
 drivers/staging/fsl_qbman/fsl_usdpaa_irq.c |  194 ++++
 drivers/staging/fsl_qbman/qman_driver.c    |  106 +--
 drivers/staging/fsl_qbman/qman_high.c      |  132 ++-
 drivers/staging/fsl_qbman/qman_low.h       |  334 +++++--
 drivers/staging/fsl_qbman/qman_private.h   |   11 +
 include/linux/fsl_bman.h                   |    4 +
 include/linux/fsl_qman.h                   |   76 ++
 include/linux/fsl_usdpaa.h                 |  162 +++-
 19 files changed, 2643 insertions(+), 1161 deletions(-)
 delete mode 100644 drivers/misc/fsl_usdpaa.c
 create mode 100644 drivers/staging/fsl_qbman/fsl_usdpaa.c
 create mode 100644 drivers/staging/fsl_qbman/fsl_usdpaa_irq.c

diff --git a/drivers/misc/Makefile b/drivers/misc/Makefile
index ee51470..c235d5b 100644
--- a/drivers/misc/Makefile
+++ b/drivers/misc/Makefile
@@ -24,7 +24,6 @@ obj-$(CONFIG_SENSORS_BH1770)	+= bh1770glc.o
 obj-$(CONFIG_SENSORS_APDS990X)	+= apds990x.o
 obj-$(CONFIG_SGI_IOC4)		+= ioc4.o
 obj-$(CONFIG_ENCLOSURE_SERVICES) += enclosure.o
-obj-$(CONFIG_FSL_USDPAA)	+= fsl_usdpaa.o
 obj-$(CONFIG_KGDB_TESTS)	+= kgdbts.o
 obj-$(CONFIG_SGI_XP)		+= sgi-xp/
 obj-$(CONFIG_SGI_GRU)		+= sgi-gru/
diff --git a/drivers/misc/fsl_usdpaa.c b/drivers/misc/fsl_usdpaa.c
deleted file mode 100644
index 206d3ee..0000000
--- a/drivers/misc/fsl_usdpaa.c
+++ /dev/null
@@ -1,701 +0,0 @@
-/* Copyright (C) 2008-2012 Freescale Semiconductor, Inc.
- * Authors: Andy Fleming <afleming@freescale.com>
- *	    Timur Tabi <timur@freescale.com>
- *	    Geoff Thorpe <Geoff.Thorpe@freescale.com>
- *
- * This file is licensed under the terms of the GNU General Public License
- * version 2.  This program is licensed "as is" without any warranty of any
- * kind, whether express or implied.
- */
-
-#include <linux/fsl_usdpaa.h>
-#include <linux/fsl_qman.h>
-#include <linux/fsl_bman.h>
-
-#include <linux/module.h>
-#include <linux/miscdevice.h>
-#include <linux/fs.h>
-#include <linux/cdev.h>
-#include <linux/mm.h>
-#include <linux/of.h>
-#include <linux/memblock.h>
-#include <linux/slab.h>
-
-/* Physical address range of the memory reservation, exported for mm/mem.c */
-static u64 phys_start;
-static u64 phys_size;
-/* PFN versions of the above */
-static unsigned long pfn_start;
-static unsigned long pfn_size;
-
-/* Memory reservations are manipulated under this spinlock (which is why 'refs'
- * isn't atomic_t). */
-static DEFINE_SPINLOCK(mem_lock);
-
-/* The range of TLB1 indices */
-static unsigned int first_tlb;
-static unsigned int num_tlb;
-static unsigned int current_tlb; /* loops around for fault handling */
-
-/* Memory reservation is represented as a list of 'mem_fragment's, some of which
- * may be mapped. Unmapped fragments are always merged where possible. */
-static LIST_HEAD(mem_list);
-
-struct mem_mapping;
-
-/* Memory fragments are in 'mem_list'. */
-struct mem_fragment {
-	u64 base;
-	u64 len;
-	unsigned long pfn_base; /* PFN version of 'base' */
-	unsigned long pfn_len; /* PFN version of 'len' */
-	unsigned int refs; /* zero if unmapped */
-	struct list_head list;
-	/* if mapped, flags+name captured at creation time */
-	u32 flags;
-	char name[USDPAA_DMA_NAME_MAX];
-	/* support multi-process locks per-memory-fragment. */
-	int has_locking;
-	wait_queue_head_t wq;
-	struct mem_mapping *owner;
-};
-
-/* Mappings of memory fragments in 'struct ctx'. These are created from
- * ioctl(USDPAA_IOCTL_DMA_MAP), though the actual mapping then happens via a
- * mmap(). */
-struct mem_mapping {
-	struct mem_fragment *frag;
-	struct list_head list;
-};
-
-/* Per-FD state (which should also be per-process but we don't enforce that) */
-struct ctx {
-	/* Allocated resources get put here for accounting */
-	struct dpa_alloc ids[usdpaa_id_max];
-	struct list_head maps;
-};
-
-/* Different resource classes */
-static const struct alloc_backend {
-	enum usdpaa_id_type id_type;
-	int (*alloc)(u32 *, u32, u32, int);
-	void (*release)(u32 base, unsigned int count);
-	const char *acronym;
-} alloc_backends[] = {
-	{
-		.id_type = usdpaa_id_fqid,
-		.alloc = qman_alloc_fqid_range,
-		.release = qman_release_fqid_range,
-		.acronym = "FQID"
-	},
-	{
-		.id_type = usdpaa_id_bpid,
-		.alloc = bman_alloc_bpid_range,
-		.release = bman_release_bpid_range,
-		.acronym = "BPID"
-	},
-	{
-		.id_type = usdpaa_id_qpool,
-		.alloc = qman_alloc_pool_range,
-		.release = qman_release_pool_range,
-		.acronym = "QPOOL"
-	},
-	{
-		.id_type = usdpaa_id_cgrid,
-		.alloc = qman_alloc_cgrid_range,
-		.release = qman_release_cgrid_range,
-		.acronym = "CGRID"
-	},
-	{
-		.id_type = usdpaa_id_ceetm0_lfqid,
-		.alloc = qman_alloc_ceetm0_lfqid_range,
-		.release = qman_release_ceetm0_lfqid_range,
-		.acronym = "CEETM0_LFQID"
-	},
-	{
-		.id_type = usdpaa_id_ceetm0_channelid,
-		.alloc = qman_alloc_ceetm0_channel_range,
-		.release = qman_release_ceetm0_channel_range,
-		.acronym = "CEETM0_CHANNELID"
-	},
-	{
-		.id_type = usdpaa_id_ceetm1_lfqid,
-		.alloc = qman_alloc_ceetm1_lfqid_range,
-		.release = qman_release_ceetm1_lfqid_range,
-		.acronym = "CEETM1_LFQID"
-	},
-	{
-		.id_type = usdpaa_id_ceetm1_channelid,
-		.alloc = qman_alloc_ceetm1_channel_range,
-		.release = qman_release_ceetm1_channel_range,
-		.acronym = "CEETM1_CHANNELID"
-	},
-	{
-		/* This terminates the array */
-		.id_type = usdpaa_id_max
-	}
-};
-
-/* Helper for ioctl_dma_map() when we have a larger fragment than we need. This
- * splits the fragment into 4 and returns the upper-most. (The caller can loop
- * until it has a suitable fragment size.) */
-static struct mem_fragment *split_frag(struct mem_fragment *frag)
-{
-	struct mem_fragment *x[3];
-	x[0] = kmalloc(sizeof(struct mem_fragment), GFP_KERNEL);
-	x[1] = kmalloc(sizeof(struct mem_fragment), GFP_KERNEL);
-	x[2] = kmalloc(sizeof(struct mem_fragment), GFP_KERNEL);
-	if (!x[0] || !x[1] || !x[2]) {
-		kfree(x[0]);
-		kfree(x[1]);
-		kfree(x[2]);
-		return NULL;
-	}
-	BUG_ON(frag->refs);
-	frag->len >>= 2;
-	frag->pfn_len >>= 2;
-	x[0]->base = frag->base + frag->len;
-	x[1]->base = x[0]->base + frag->len;
-	x[2]->base = x[1]->base + frag->len;
-	x[0]->len = x[1]->len = x[2]->len = frag->len;
-	x[0]->pfn_base = frag->pfn_base + frag->pfn_len;
-	x[1]->pfn_base = x[0]->pfn_base + frag->pfn_len;
-	x[2]->pfn_base = x[1]->pfn_base + frag->pfn_len;
-	x[0]->pfn_len = x[1]->pfn_len = x[2]->pfn_len = frag->pfn_len;
-	x[0]->refs = x[1]->refs = x[2]->refs = 0;
-	list_add(&x[0]->list, &frag->list);
-	list_add(&x[1]->list, &x[0]->list);
-	list_add(&x[2]->list, &x[1]->list);
-	return x[2];
-}
-
-/* Conversely, when a fragment is released we look to see whether its
- * similarly-split siblings are free to be reassembled. */
-static struct mem_fragment *merge_frag(struct mem_fragment *frag)
-{
-	/* If this fragment can be merged with its siblings, it will have
-	 * newbase and newlen as its geometry. */
-	uint64_t newlen = frag->len << 2;
-	uint64_t newbase = frag->base & ~(newlen - 1);
-	struct mem_fragment *tmp, *leftmost = frag, *rightmost = frag;
-	/* Scan left until we find the start */
-	tmp = list_entry(frag->list.prev, struct mem_fragment, list);
-	while ((&tmp->list != &mem_list) && (tmp->base >= newbase)) {
-		if (tmp->refs)
-			return NULL;
-		if (tmp->len != tmp->len)
-			return NULL;
-		leftmost = tmp;
-		tmp = list_entry(tmp->list.prev, struct mem_fragment, list);
-	}
-	/* Scan right until we find the end */
-	tmp = list_entry(frag->list.next, struct mem_fragment, list);
-	while ((&tmp->list != &mem_list) && (tmp->base < (newbase + newlen))) {
-		if (tmp->refs)
-			return NULL;
-		if (tmp->len != tmp->len)
-			return NULL;
-		rightmost = tmp;
-		tmp = list_entry(tmp->list.next, struct mem_fragment, list);
-	}
-	if (leftmost == rightmost)
-		return NULL;
-	/* OK, we can merge */
-	frag = leftmost;
-	frag->len = newlen;
-	frag->pfn_len = newlen >> PAGE_SHIFT;
-	while (1) {
-		int lastone;
-		tmp = list_entry(frag->list.next, struct mem_fragment, list);
-		lastone = (tmp == rightmost);
-		if (&tmp->list == &mem_list)
-			break;
-		list_del(&tmp->list);
-		kfree(tmp);
-		if (lastone)
-			break;
-	}
-	return frag;
-}
-
-/* Helper to verify that 'sz' is (4096 * 4^x) for some x. */
-static int is_good_size(u64 sz)
-{
-	int log = ilog2(phys_size);
-	if ((phys_size & (phys_size - 1)) || (log < 12) || (log & 1))
-		return 0;
-	return 1;
-}
-
-/* Hook from arch/powerpc/mm/mem.c */
-int usdpaa_test_fault(unsigned long pfn, u64 *phys_addr, u64 *size)
-{
-	struct mem_fragment *frag;
-	int idx = -1;
-	if ((pfn < pfn_start) || (pfn >= (pfn_start + pfn_size)))
-		return -1;
-	/* It's in-range, we need to find the fragment */
-	spin_lock(&mem_lock);
-	list_for_each_entry(frag, &mem_list, list) {
-		if ((pfn >= frag->pfn_base) && (pfn < (frag->pfn_base +
-						       frag->pfn_len))) {
-			*phys_addr = frag->base;
-			*size = frag->len;
-			idx = current_tlb++;
-			if (current_tlb >= (first_tlb + num_tlb))
-				current_tlb = first_tlb;
-			break;
-		}
-	}
-	spin_unlock(&mem_lock);
-	return idx;
-}
-
-static int usdpaa_open(struct inode *inode, struct file *filp)
-{
-	const struct alloc_backend *backend = &alloc_backends[0];
-	struct ctx *ctx = kmalloc(sizeof(struct ctx), GFP_KERNEL);
-	if (!ctx)
-		return -ENOMEM;
-	filp->private_data = ctx;
-
-	while (backend->id_type != usdpaa_id_max) {
-		dpa_alloc_init(&ctx->ids[backend->id_type]);
-		backend++;
-	}
-
-	INIT_LIST_HEAD(&ctx->maps);
-
-	filp->f_mapping->backing_dev_info = &directly_mappable_cdev_bdi;
-
-	return 0;
-}
-
-static int usdpaa_release(struct inode *inode, struct file *filp)
-{
-	struct ctx *ctx = filp->private_data;
-	struct mem_mapping *map, *tmp;
-	const struct alloc_backend *backend = &alloc_backends[0];
-	while (backend->id_type != usdpaa_id_max) {
-		int ret, leaks = 0;
-		do {
-			u32 id, num;
-			ret = dpa_alloc_pop(&ctx->ids[backend->id_type],
-					    &id, &num);
-			if (!ret) {
-				leaks += num;
-				backend->release(id, num);
-			}
-		} while (ret == 1);
-		if (leaks)
-			pr_crit("USDPAA process leaking %d %s%s\n", leaks,
-				backend->acronym, (leaks > 1) ? "s" : "");
-		backend++;
-	}
-	spin_lock(&mem_lock);
-	list_for_each_entry_safe(map, tmp, &ctx->maps, list) {
-		if (map->frag->has_locking && (map->frag->owner == map)) {
-			map->frag->owner = NULL;
-			wake_up(&map->frag->wq);
-		}
-		if (!--map->frag->refs) {
-			struct mem_fragment *frag = map->frag;
-			do {
-				frag = merge_frag(frag);
-			} while (frag);
-		}
-		list_del(&map->list);
-		kfree(map);
-	}
-	spin_unlock(&mem_lock);
-	kfree(ctx);
-	return 0;
-}
-
-static int usdpaa_mmap(struct file *filp, struct vm_area_struct *vma)
-{
-	struct ctx *ctx = filp->private_data;
-	struct mem_mapping *map;
-	int ret = 0;
-
-	spin_lock(&mem_lock);
-	list_for_each_entry(map, &ctx->maps, list) {
-		if (map->frag->pfn_base == vma->vm_pgoff)
-			goto map_match;
-	}
-	spin_unlock(&mem_lock);
-	return -ENOMEM;
-
-map_match:
-	if (map->frag->len != (vma->vm_end - vma->vm_start))
-		ret = -EINVAL;
-	spin_unlock(&mem_lock);
-	if (!ret)
-		ret = remap_pfn_range(vma, vma->vm_start, map->frag->pfn_base,
-				      vma->vm_end - vma->vm_start,
-				      vma->vm_page_prot);
-	return ret;
-}
-
-/* Return the nearest rounded-up address >= 'addr' that is 'sz'-aligned. 'sz'
- * must be a power of 2, but both 'addr' and 'sz' can be expressions. */
-#define USDPAA_MEM_ROUNDUP(addr, sz) \
-	({ \
-		unsigned long foo_align = (sz) - 1; \
-		((addr) + foo_align) & ~foo_align; \
-	})
-/* Searching for a size-aligned virtual address range starting from 'addr' */
-static unsigned long usdpaa_get_unmapped_area(struct file *file,
-					      unsigned long addr,
-					      unsigned long len,
-					      unsigned long pgoff,
-					      unsigned long flags)
-{
-	struct vm_area_struct *vma;
-
-	if (!is_good_size(len))
-		return -EINVAL;
-
-	addr = USDPAA_MEM_ROUNDUP(addr, len);
-	vma = find_vma(current->mm, addr);
-	/* Keep searching until we reach the end of currently-used virtual
-	 * address-space or we find a big enough gap. */
-	while (vma) {
-		if ((addr + len) < vma->vm_start)
-			return addr;
-		addr = USDPAA_MEM_ROUNDUP(vma->vm_end, len);
-		vma = vma->vm_next;
-	}
-	if ((TASK_SIZE - len) < addr)
-		return -ENOMEM;
-	return addr;
-}
-
-static long ioctl_id_alloc(struct ctx *ctx, void __user *arg)
-{
-	struct usdpaa_ioctl_id_alloc i;
-	const struct alloc_backend *backend;
-	int ret = copy_from_user(&i, arg, sizeof(i));
-	if (ret)
-		return ret;
-	if ((i.id_type >= usdpaa_id_max) || !i.num)
-		return -EINVAL;
-	backend = &alloc_backends[i.id_type];
-	/* Allocate the required resource type */
-	ret = backend->alloc(&i.base, i.num, i.align, i.partial);
-	if (ret < 0)
-		return ret;
-	i.num = ret;
-	/* Copy the result to user-space */
-	ret = copy_to_user(arg, &i, sizeof(i));
-	if (ret) {
-		backend->release(i.base, i.num);
-		return ret;
-	}
-	/* Assign the allocated range to the FD accounting */
-	dpa_alloc_free(&ctx->ids[i.id_type], i.base, i.num);
-	return 0;
-}
-
-static long ioctl_id_release(struct ctx *ctx, void __user *arg)
-{
-	struct usdpaa_ioctl_id_release i;
-	const struct alloc_backend *backend;
-	int ret = copy_from_user(&i, arg, sizeof(i));
-	if (ret)
-		return ret;
-	if ((i.id_type >= usdpaa_id_max) || !i.num)
-		return -EINVAL;
-	backend = &alloc_backends[i.id_type];
-	/* Pull the range out of the FD accounting - the range is valid iff this
-	 * succeeds. */
-	ret = dpa_alloc_reserve(&ctx->ids[i.id_type], i.base, i.num);
-	if (ret)
-		return ret;
-	/* Release the resource to the backend */
-	backend->release(i.base, i.num);
-	return 0;
-}
-
-static long ioctl_dma_map(struct ctx *ctx, void __user *arg)
-{
-	struct usdpaa_ioctl_dma_map i;
-	struct mem_fragment *frag;
-	struct mem_mapping *map, *tmp;
-	u64 search_size;
-	int ret = copy_from_user(&i, arg, sizeof(i));
-	if (ret)
-		return ret;
-	if (i.len && !is_good_size(i.len))
-		return -EINVAL;
-	map = kmalloc(sizeof(*map), GFP_KERNEL);
-	if (!map)
-		return -ENOMEM;
-	spin_lock(&mem_lock);
-	if (i.flags & USDPAA_DMA_FLAG_SHARE) {
-		list_for_each_entry(frag, &mem_list, list) {
-			if (frag->refs && (frag->flags &
-					   USDPAA_DMA_FLAG_SHARE) &&
-					!strncmp(i.name, frag->name,
-						 USDPAA_DMA_NAME_MAX)) {
-				/* Matching entry */
-				if ((i.flags & USDPAA_DMA_FLAG_CREATE) &&
-				    !(i.flags & USDPAA_DMA_FLAG_LAZY)) {
-					ret = -EBUSY;
-					goto out;
-				}
-				list_for_each_entry(tmp, &ctx->maps, list)
-					if (tmp->frag == frag) {
-						ret = -EBUSY;
-						goto out;
-					}
-				i.has_locking = frag->has_locking;
-				i.did_create = 0;
-				i.len = frag->len;
-				goto do_map;
-			}
-		}
-		/* No matching entry */
-		if (!(i.flags & USDPAA_DMA_FLAG_CREATE)) {
-			ret = -ENOMEM;
-			goto out;
-		}
-	}
-	/* New fragment required, size must be provided. */
-	if (!i.len) {
-		ret = -EINVAL;
-		goto out;
-	}
-	/* We search for the required size and if that fails, for the next
-	 * biggest size, etc. */
-	for (search_size = i.len; search_size <= phys_size; search_size <<= 2) {
-		list_for_each_entry(frag, &mem_list, list) {
-			if (!frag->refs && (frag->len == search_size)) {
-				while (frag->len > i.len) {
-					frag = split_frag(frag);
-					if (!frag) {
-						ret = -ENOMEM;
-						goto out;
-					}
-				}
-				frag->flags = i.flags;
-				strncpy(frag->name, i.name,
-					USDPAA_DMA_NAME_MAX);
-				frag->has_locking = i.has_locking;
-				init_waitqueue_head(&frag->wq);
-				frag->owner = NULL;
-				i.did_create = 1;
-				goto do_map;
-			}
-		}
-	}
-	ret = -ENOMEM;
-	goto out;
-
-do_map:
-	map->frag = frag;
-	frag->refs++;
-	list_add(&map->list, &ctx->maps);
-	i.pa_offset = frag->base;
-
-out:
-	spin_unlock(&mem_lock);
-	if (!ret)
-		ret = copy_to_user(arg, &i, sizeof(i));
-	else
-		kfree(map);
-	return ret;
-}
-
-static int test_lock(struct mem_mapping *map)
-{
-	int ret = 0;
-	spin_lock(&mem_lock);
-	if (!map->frag->owner) {
-		map->frag->owner = map;
-		ret = 1;
-	}
-	spin_unlock(&mem_lock);
-	return ret;
-}
-
-static long ioctl_dma_lock(struct ctx *ctx, void __user *arg)
-{
-	struct mem_mapping *map;
-	struct vm_area_struct *vma;
-
-	down_read(&current->mm->mmap_sem);
-	vma = find_vma(current->mm, (unsigned long)arg);
-	if (!vma || (vma->vm_start > (unsigned long)arg)) {
-		up_read(&current->mm->mmap_sem);
-		return -EFAULT;
-	}
-	spin_lock(&mem_lock);
-	list_for_each_entry(map, &ctx->maps, list) {
-		if (map->frag->pfn_base == vma->vm_pgoff)
-			goto map_match;
-	}
-	map = NULL;
-map_match:
-	spin_unlock(&mem_lock);
-	up_read(&current->mm->mmap_sem);
-
-	if (!map->frag->has_locking)
-		return -ENODEV;
-	return wait_event_interruptible(map->frag->wq, test_lock(map));
-}
-
-static long ioctl_dma_unlock(struct ctx *ctx, void __user *arg)
-{
-	struct mem_mapping *map;
-	struct vm_area_struct *vma;
-	int ret;
-
-	down_read(&current->mm->mmap_sem);
-	vma = find_vma(current->mm, (unsigned long)arg);
-	if (!vma || (vma->vm_start > (unsigned long)arg))
-		ret = -EFAULT;
-	else {
-		spin_lock(&mem_lock);
-		list_for_each_entry(map, &ctx->maps, list) {
-			if (map->frag->pfn_base == vma->vm_pgoff) {
-				if (!map->frag->has_locking)
-					ret = -ENODEV;
-				else if (map->frag->owner == map) {
-					map->frag->owner = NULL;
-					wake_up(&map->frag->wq);
-					ret = 0;
-				} else
-					ret = -EBUSY;
-				goto map_match;
-			}
-		}
-		ret = -EINVAL;
-map_match:
-		spin_unlock(&mem_lock);
-	}
-	up_read(&current->mm->mmap_sem);
-	return ret;
-}
-
-static long usdpaa_ioctl(struct file *fp, unsigned int cmd, unsigned long arg)
-{
-	struct ctx *ctx = fp->private_data;
-	void __user *a = (void __user *)arg;
-	switch (cmd) {
-	case USDPAA_IOCTL_ID_ALLOC:
-		return ioctl_id_alloc(ctx, a);
-	case USDPAA_IOCTL_ID_RELEASE:
-		return ioctl_id_release(ctx, a);
-	case USDPAA_IOCTL_DMA_MAP:
-		return ioctl_dma_map(ctx, a);
-	case USDPAA_IOCTL_DMA_LOCK:
-		return ioctl_dma_lock(ctx, a);
-	case USDPAA_IOCTL_DMA_UNLOCK:
-		return ioctl_dma_unlock(ctx, a);
-	}
-	return -EINVAL;
-}
-
-static const struct file_operations usdpaa_fops = {
-	.open		   = usdpaa_open,
-	.release	   = usdpaa_release,
-	.mmap		   = usdpaa_mmap,
-	.get_unmapped_area = usdpaa_get_unmapped_area,
-	.unlocked_ioctl	   = usdpaa_ioctl,
-	.compat_ioctl	   = usdpaa_ioctl
-};
-
-static struct miscdevice usdpaa_miscdev = {
-	.name = "fsl-usdpaa",
-	.fops = &usdpaa_fops,
-	.minor = MISC_DYNAMIC_MINOR,
-};
-
-/* Early-boot memory allocation. The boot-arg "usdpaa_mem=<x>" is used to
- * indicate how much memory (if any) to allocate during early boot. If the
- * format "usdpaa_mem=<x>,<y>" is used, then <y> will be interpreted as the
- * number of TLB1 entries to reserve (default is 1). If there are more mappings
- * than there are TLB1 entries, fault-handling will occur. */
-static __init int usdpaa_mem(char *arg)
-{
-	phys_size = memparse(arg, &arg);
-	num_tlb = 1;
-	if (*arg == ',') {
-		unsigned long ul;
-		int err = kstrtoul(arg + 1, 0, &ul);
-		if (err < 0) {
-			num_tlb = 1;
-			pr_warning("ERROR, usdpaa_mem arg is invalid\n");
-		} else
-			num_tlb = (unsigned int)ul;
-	}
-	return 0;
-}
-early_param("usdpaa_mem", usdpaa_mem);
-
-__init void fsl_usdpaa_init_early(void)
-{
-	if (!phys_size) {
-		pr_info("No USDPAA memory, no 'usdpaa_mem' bootarg\n");
-		return;
-	}
-	if (!is_good_size(phys_size)) {
-		pr_err("'usdpaa_mem' bootarg must be 4096*4^x\n");
-		phys_size = 0;
-		return;
-	}
-	phys_start = memblock_alloc(phys_size, phys_size);
-	if (!phys_start) {
-		pr_err("Failed to reserve USDPAA region (sz:%llx)\n",
-		       phys_size);
-		return;
-	}
-	pfn_start = phys_start >> PAGE_SHIFT;
-	pfn_size = phys_size >> PAGE_SHIFT;
-	first_tlb = current_tlb = tlbcam_index;
-	tlbcam_index += num_tlb;
-	pr_info("USDPAA region at %llx:%llx(%lx:%lx), %d TLB1 entries)\n",
-		phys_start, phys_size, pfn_start, pfn_size, num_tlb);
-}
-
-static int __init usdpaa_init(void)
-{
-	struct mem_fragment *frag;
-	int ret;
-
-	pr_info("Freescale USDPAA process driver\n");
-	if (!phys_start) {
-		pr_warning("fsl-usdpaa: no region found\n");
-		return 0;
-	}
-	frag = kmalloc(sizeof(*frag), GFP_KERNEL);
-	if (!frag) {
-		pr_err("Failed to setup USDPAA memory accounting\n");
-		return -ENOMEM;
-	}
-	frag->base = phys_start;
-	frag->len = phys_size;
-	frag->pfn_base = pfn_start;
-	frag->pfn_len = pfn_size;
-	frag->refs = 0;
-	init_waitqueue_head(&frag->wq);
-	frag->owner = NULL;
-	list_add(&frag->list, &mem_list);
-	ret = misc_register(&usdpaa_miscdev);
-	if (ret)
-		pr_err("fsl-usdpaa: failed to register misc device\n");
-	return ret;
-}
-
-static void __exit usdpaa_exit(void)
-{
-	misc_deregister(&usdpaa_miscdev);
-}
-
-module_init(usdpaa_init);
-module_exit(usdpaa_exit);
-
-MODULE_LICENSE("GPL");
-MODULE_AUTHOR("Freescale Semiconductor");
-MODULE_DESCRIPTION("Freescale USDPAA process driver");
diff --git a/drivers/staging/fsl_qbman/Kconfig b/drivers/staging/fsl_qbman/Kconfig
index 1c3906b..d5adc5a 100644
--- a/drivers/staging/fsl_qbman/Kconfig
+++ b/drivers/staging/fsl_qbman/Kconfig
@@ -34,15 +34,6 @@ config FSL_DPA_PORTAL_SHARE
 	bool
 	default y
 
-config FSL_DPA_UIO
-	tristate "export USDPAA portals via UIO"
-	depends on UIO
-	default y
-	---help---
-	  Any portals unused by the kernel are exported as UIO devices for
-	  use by USDPAA (User Space DataPath Acceleration Architecture)
-	  applications.
-
 config FSL_BMAN
 	bool "Freescale Buffer Manager (BMan) support"
 	default y
diff --git a/drivers/staging/fsl_qbman/Makefile b/drivers/staging/fsl_qbman/Makefile
index 330cdfb..2975e28 100644
--- a/drivers/staging/fsl_qbman/Makefile
+++ b/drivers/staging/fsl_qbman/Makefile
@@ -22,4 +22,4 @@ obj-$(CONFIG_FSL_QMAN_DEBUGFS)	+= qman_debugfs_interface.o
 qman_debugfs_interface-y	 = qman_debugfs.o
 
 # USDPAA
-obj-$(CONFIG_FSL_DPA_UIO)	+= dpa_uio.o
+obj-$(CONFIG_FSL_USDPAA)	+= fsl_usdpaa.o fsl_usdpaa_irq.o
diff --git a/drivers/staging/fsl_qbman/bman_driver.c b/drivers/staging/fsl_qbman/bman_driver.c
index 55297607..06b77ed 100644
--- a/drivers/staging/fsl_qbman/bman_driver.c
+++ b/drivers/staging/fsl_qbman/bman_driver.c
@@ -47,6 +47,8 @@ u16 bman_portal_max;
 static struct bman_portal *shared_portals[NR_CPUS];
 static int num_shared_portals;
 static int shared_portals_idx;
+static LIST_HEAD(unused_pcfgs);
+static DEFINE_SPINLOCK(unused_pcfgs_lock);
 
 static int __init fsl_bpool_init(struct device_node *node)
 {
@@ -93,7 +95,7 @@ static int __init fsl_bpid_range_init(struct device_node *node)
 			node->full_name);
 		return -EINVAL;
 	}
-	bman_release_bpid_range(range[0], range[1]);
+	bman_seed_bpid_range(range[0], range[1]);
 	pr_info("Bman: BPID allocator includes range %d:%d\n",
 		range[0], range[1]);
 	return 0;
@@ -186,13 +188,6 @@ err:
 	return NULL;
 }
 
-static void destroy_pcfg(struct bm_portal_config *pcfg)
-{
-	iounmap(pcfg->addr_virt[DPA_PORTAL_CI]);
-	iounmap(pcfg->addr_virt[DPA_PORTAL_CE]);
-	kfree(pcfg);
-}
-
 static struct bm_portal_config *get_pcfg(struct list_head *list)
 {
 	struct bm_portal_config *pcfg;
@@ -203,58 +198,21 @@ static struct bm_portal_config *get_pcfg(struct list_head *list)
 	return pcfg;
 }
 
-/* UIO handling callbacks */
-#define BMAN_UIO_PREAMBLE() \
-	const struct bm_portal_config *pcfg = \
-		container_of(__p, struct bm_portal_config, list)
-static int bman_uio_cb_init(const struct list_head *__p, struct uio_info *info)
-{
-	BMAN_UIO_PREAMBLE();
-	/* big enough for "bman-uio-xx" */
-	char *name = kzalloc(16, GFP_KERNEL);
-	if (!name)
-		return -ENOMEM;
-	sprintf(name, "bman-uio-%x", pcfg->public_cfg.index);
-	info->name = name;
-	info->mem[DPA_PORTAL_CE].name = "cena";
-	info->mem[DPA_PORTAL_CE].addr = pcfg->addr_phys[DPA_PORTAL_CE].start;
-	info->mem[DPA_PORTAL_CE].size =
-		resource_size(&pcfg->addr_phys[DPA_PORTAL_CE]);
-	info->mem[DPA_PORTAL_CE].memtype = UIO_MEM_PHYS;
-	info->mem[DPA_PORTAL_CI].name = "cinh";
-	info->mem[DPA_PORTAL_CI].addr = pcfg->addr_phys[DPA_PORTAL_CI].start;
-	info->mem[DPA_PORTAL_CI].size =
-		resource_size(&pcfg->addr_phys[DPA_PORTAL_CI]);
-	info->mem[DPA_PORTAL_CI].memtype = UIO_MEM_PHYS;
-	info->irq = pcfg->public_cfg.irq;
-	return 0;
-}
-static void bman_uio_cb_destroy(const struct list_head *__p,
-				struct uio_info *info)
+struct bm_portal_config *bm_get_unused_portal(void)
 {
-	BMAN_UIO_PREAMBLE();
-	kfree(info->name);
-	/* We own this struct but had passed it to the dpa_uio layer as a const
-	 * so that we don't accidentally meddle with it in the dpa_uio code.
-	 * Here it's passed back to us for final clean it up, so de-constify. */
-	destroy_pcfg((struct bm_portal_config *)pcfg);
+	struct bm_portal_config *ret;
+	spin_lock(&unused_pcfgs_lock);
+	ret = get_pcfg(&unused_pcfgs);
+	spin_unlock(&unused_pcfgs_lock);
+	return ret;
 }
-static void bman_uio_cb_interrupt(const struct list_head *__p)
+
+void bm_put_unused_portal(struct bm_portal_config *pcfg)
 {
-	BMAN_UIO_PREAMBLE();
-	/* This is the only manipulation of a portal register that isn't in the
-	 * regular kernel portal driver (_high.c/_low.h). It is also the only
-	 * time the kernel touches a register on a portal that is otherwise
-	 * being driven by a user-space driver. So rather than messing up
-	 * encapsulation for one trivial call, I am hard-coding the offset to
-	 * the inhibit register and writing it directly from here. */
-	out_be32(pcfg->addr_virt[DPA_PORTAL_CI] + 0xe0c, ~(u32)0);
+	spin_lock(&unused_pcfgs_lock);
+	list_add(&pcfg->list, &unused_pcfgs);
+	spin_unlock(&unused_pcfgs_lock);
 }
-static const struct dpa_uio_vtable bman_uio = {
-	.init_uio = bman_uio_cb_init,
-	.destroy = bman_uio_cb_destroy,
-	.on_interrupt = bman_uio_cb_interrupt
-};
 
 static struct bman_portal *init_pcfg(struct bm_portal_config *pcfg)
 {
@@ -337,14 +295,13 @@ __setup("bportals=", parse_bportals);
  * 5. Shared portals are initialised on their respective cpus.
  * 6. Each remaining cpu is initialised to slave to one of the shared portals,
  *    which are selected in a round-robin fashion.
- * Any portal configs left unused are exported as UIO devices.
+ * Any portal configs left unused are available for USDPAA allocation.
  */
 static __init int bman_init(void)
 {
 	struct cpumask slave_cpus;
 	struct cpumask unshared_cpus = *cpu_none_mask;
 	struct cpumask shared_cpus = *cpu_none_mask;
-	LIST_HEAD(unused_pcfgs);
 	LIST_HEAD(unshared_pcfgs);
 	LIST_HEAD(shared_pcfgs);
 	struct device_node *dn;
@@ -447,19 +404,6 @@ static __init int bman_init(void)
 		for_each_cpu(cpu, &slave_cpus)
 			init_slave(cpu);
 	pr_info("Bman portals initialised\n");
-#ifdef CONFIG_FSL_DPA_UIO
-	/* Export any left over portals as UIO devices */
-	do {
-		pcfg = get_pcfg(&unused_pcfgs);
-		if (!pcfg)
-			break;
-		ret = dpa_uio_register(&pcfg->list, &bman_uio);
-		if (ret) {
-			pr_err("Failure registering BMan UIO portal\n");
-			destroy_pcfg(pcfg);
-		}
-	} while (1);
-#endif
 	/* Initialise BPID allocation ranges */
 	for_each_compatible_node(dn, NULL, "fsl,bpid-range") {
 		ret = fsl_bpid_range_init(dn);
diff --git a/drivers/staging/fsl_qbman/bman_high.c b/drivers/staging/fsl_qbman/bman_high.c
index ecdfae7..c6e4096 100644
--- a/drivers/staging/fsl_qbman/bman_high.c
+++ b/drivers/staging/fsl_qbman/bman_high.c
@@ -61,6 +61,8 @@ struct bman_portal {
 	 * you'll never guess the hash-function ... */
 	struct bman_pool *cb[64];
 	char irqname[MAX_IRQNAME];
+	/* Track if the portal was alloced by the driver */
+	u8 alloced;
 };
 
 /* For an explanation of the locking, redirection, or affine-portal logic,
@@ -191,20 +193,23 @@ static irqreturn_t portal_isr(__always_unused int irq, void *ptr)
 	return IRQ_HANDLED;
 }
 
-struct bman_portal *bman_create_affine_portal(
-			const struct bm_portal_config *config)
+
+struct bman_portal *bman_create_portal(
+				       struct bman_portal *portal,
+				       const struct bm_portal_config *config)
 {
-	struct bman_portal *portal = get_raw_affine_portal();
-	struct bm_portal *__p = &portal->p;
+	struct bm_portal *__p;
 	const struct bman_depletion *pools = &config->public_cfg.mask;
 	int ret;
 	u8 bpid = 0;
 
-	/* A criteria for calling this function (from bman_driver.c) is that
-	 * we're already affine to the cpu and won't schedule onto another cpu.
-	 * This means we can put_affine_portal() and yet continue to use
-	 * "portal", which in turn means aspects of this routine can sleep. */
-	put_affine_portal();
+	if (!portal) {
+		portal = kmalloc(sizeof(*portal), GFP_KERNEL);
+		portal->alloced = 1;
+	} else
+		portal->alloced = 0;
+
+	__p = &portal->p;
 
 	/* prep the low-level portal struct with the mapped addresses from the
 	 * config, everything that follows depends on it and "config" is more
@@ -262,6 +267,7 @@ struct bman_portal *bman_create_affine_portal(
 		pr_err("irq_set_affinity() failed\n");
 		goto fail_affinity;
 	}
+
 	/* Need RCR to be empty before continuing */
 	ret = bm_rcr_get_fill(__p);
 	if (ret) {
@@ -270,9 +276,7 @@ struct bman_portal *bman_create_affine_portal(
 	}
 	/* Success */
 	portal->config = config;
-	spin_lock(&affine_mask_lock);
-	cpumask_set_cpu(config->public_cfg.cpu, &affine_mask);
-	spin_unlock(&affine_mask_lock);
+
 	bm_isr_disable_write(__p, 0);
 	bm_isr_uninhibit(__p);
 	return portal;
@@ -289,9 +293,26 @@ fail_isr:
 fail_mc:
 	bm_rcr_finish(__p);
 fail_rcr:
+	if (portal->alloced)
+		kfree(portal);
 	return NULL;
 }
 
+struct bman_portal *bman_create_affine_portal(
+			const struct bm_portal_config *config)
+{
+	struct bman_portal *portal = get_raw_affine_portal();
+	portal = bman_create_portal(portal, config);
+	if (portal) {
+		spin_lock(&affine_mask_lock);
+		cpumask_set_cpu(config->public_cfg.cpu, &affine_mask);
+		spin_unlock(&affine_mask_lock);
+	}
+	put_affine_portal();
+	return portal;
+}
+
+
 struct bman_portal *bman_create_affine_slave(struct bman_portal *redirect)
 {
 #ifdef CONFIG_FSL_DPA_PORTAL_SHARE
@@ -309,6 +330,24 @@ struct bman_portal *bman_create_affine_slave(struct bman_portal *redirect)
 #endif
 }
 
+void bman_destroy_portal(struct bman_portal *bm)
+{
+	const struct bm_portal_config *pcfg;
+	pcfg = bm->config;
+	bm_rcr_cce_update(&bm->p);
+	bm_rcr_cce_update(&bm->p);
+
+	free_irq(pcfg->public_cfg.irq, bm);
+
+	kfree(bm->pools);
+	bm_isr_finish(&bm->p);
+	bm_mc_finish(&bm->p);
+	bm_rcr_finish(&bm->p);
+	bm->config = NULL;
+	if (bm->alloced)
+		kfree(bm);
+}
+
 const struct bm_portal_config *bman_destroy_affine_portal(void)
 {
 	struct bman_portal *bm = get_raw_affine_portal();
@@ -322,14 +361,7 @@ const struct bm_portal_config *bman_destroy_affine_portal(void)
 	bm->is_shared = 0;
 #endif
 	pcfg = bm->config;
-	bm_rcr_cce_update(&bm->p);
-	bm_rcr_cce_update(&bm->p);
-	free_irq(pcfg->public_cfg.irq, bm);
-	kfree(bm->pools);
-	bm_isr_finish(&bm->p);
-	bm_mc_finish(&bm->p);
-	bm_rcr_finish(&bm->p);
-	bm->config = NULL;
+	bman_destroy_portal(bm);
 	spin_lock(&affine_mask_lock);
 	cpumask_clear_cpu(pcfg->public_cfg.cpu, &affine_mask);
 	spin_unlock(&affine_mask_lock);
@@ -629,24 +661,8 @@ void bman_free_pool(struct bman_pool *pool)
 		pool->sp = NULL;
 		pool->params.flags ^= BMAN_POOL_FLAG_STOCKPILE;
 	}
-	if (pool->params.flags & BMAN_POOL_FLAG_DYNAMIC_BPID) {
-		/* When releasing a BPID to the dynamic allocator, that pool
-		 * must be *empty*. This code makes it so by dropping everything
-		 * into the bit-bucket. This ignores whether or not it was a
-		 * mistake (or a leak) on the caller's part not to drain the
-		 * pool beforehand. */
-		struct bm_buffer bufs[8];
-		int ret = 0;
-		do {
-			/* Acquire is all-or-nothing, so we drain in 8s, then in
-			 * 1s for the remainder. */
-			if (ret != 1)
-				ret = bman_acquire(pool, bufs, 8, 0);
-			if (ret < 8)
-				ret = bman_acquire(pool, bufs, 1, 0);
-		} while (ret > 0);
+	if (pool->params.flags & BMAN_POOL_FLAG_DYNAMIC_BPID)
 		bman_release_bpid(pool->params.bpid);
-	}
 	kfree(pool);
 }
 EXPORT_SYMBOL(bman_free_pool);
@@ -822,16 +838,16 @@ int bman_release(struct bman_pool *pool, const struct bm_buffer *bufs, u8 num,
 		return -EINVAL;
 	if (pool->params.flags & BMAN_POOL_FLAG_NO_RELEASE)
 		return -EINVAL;
-#endif
-	/* Without stockpile, this API is a pass-through to the h/w operation */
-	if (!(pool->params.flags & BMAN_POOL_FLAG_STOCKPILE))
-		return __bman_release(pool, bufs, num, flags);
-#ifdef CONFIG_FSL_DPA_CHECKING
 	if (!atomic_dec_and_test(&pool->in_use)) {
 		pr_crit("Parallel attempts to enter bman_released() detected.");
 		panic("only one instance of bman_released/acquired allowed");
 	}
 #endif
+	/* Without stockpile, this API is a pass-through to the h/w operation */
+	if (!(pool->params.flags & BMAN_POOL_FLAG_STOCKPILE)) {
+		ret = __bman_release(pool, bufs, num, flags);
+		goto release_done;
+	}
 	/* This needs some explanation. Adding the given buffers may take the
 	 * stockpile over the threshold, but in fact the stockpile may already
 	 * *be* over the threshold if a previous release-to-hw attempt had
@@ -908,16 +924,16 @@ int bman_acquire(struct bman_pool *pool, struct bm_buffer *bufs, u8 num,
 		return -EINVAL;
 	if (pool->params.flags & BMAN_POOL_FLAG_ONLY_RELEASE)
 		return -EINVAL;
-#endif
-	/* Without stockpile, this API is a pass-through to the h/w operation */
-	if (!(pool->params.flags & BMAN_POOL_FLAG_STOCKPILE))
-		return __bman_acquire(pool, bufs, num);
-#ifdef CONFIG_FSL_DPA_CHECKING
 	if (!atomic_dec_and_test(&pool->in_use)) {
 		pr_crit("Parallel attempts to enter bman_acquire() detected.");
 		panic("only one instance of bman_released/acquired allowed");
 	}
 #endif
+	/* Without stockpile, this API is a pass-through to the h/w operation */
+	if (!(pool->params.flags & BMAN_POOL_FLAG_STOCKPILE)) {
+		ret = __bman_acquire(pool, bufs, num);
+		goto acquire_done;
+	}
 	/* Only need a h/w op if we'll hit the low-water thresh */
 	if (!(flags & BMAN_ACQUIRE_FLAG_STOCKPILE) &&
 			(pool->sp_fill <= (BMAN_STOCKPILE_LOW + num))) {
@@ -1007,3 +1023,17 @@ int bman_update_pool_thresholds(struct bman_pool *pool, const u32 *thresholds)
 }
 EXPORT_SYMBOL(bman_update_pool_thresholds);
 #endif
+
+int bman_shutdown_pool(u32 bpid)
+{
+	struct bman_portal *p = get_affine_portal();
+	__maybe_unused unsigned long irqflags;
+	int ret;
+
+	PORTAL_IRQ_LOCK(p, irqflags);
+	ret = bm_shutdown_pool(&p->p, bpid);
+	PORTAL_IRQ_UNLOCK(p, irqflags);
+	put_affine_portal();
+	return ret;
+}
+EXPORT_SYMBOL(bman_shutdown_pool);
diff --git a/drivers/staging/fsl_qbman/bman_low.h b/drivers/staging/fsl_qbman/bman_low.h
index 262bae7..23dcba0 100644
--- a/drivers/staging/fsl_qbman/bman_low.h
+++ b/drivers/staging/fsl_qbman/bman_low.h
@@ -36,20 +36,20 @@
 /***************************/
 
 /* Cache-inhibited register offsets */
-#define REG_RCR_PI_CINH		0x0000
-#define REG_RCR_CI_CINH		0x0004
-#define REG_RCR_ITR		0x0008
-#define REG_CFG			0x0100
-#define REG_SCN(n)		(0x0200 + ((n) << 2))
-#define REG_ISR			0x0e00
+#define BM_REG_RCR_PI_CINH	0x0000
+#define BM_REG_RCR_CI_CINH	0x0004
+#define BM_REG_RCR_ITR		0x0008
+#define BM_REG_CFG		0x0100
+#define BM_REG_SCN(n)		(0x0200 + ((n) << 2))
+#define BM_REG_ISR		0x0e00
 
 /* Cache-enabled register offsets */
-#define CL_CR			0x0000
-#define CL_RR0			0x0100
-#define CL_RR1			0x0140
-#define CL_RCR			0x1000
-#define CL_RCR_PI_CENA		0x3000
-#define CL_RCR_CI_CENA		0x3100
+#define BM_CL_CR		0x0000
+#define BM_CL_RR0		0x0100
+#define BM_CL_RR1		0x0140
+#define BM_CL_RCR		0x1000
+#define BM_CL_RCR_PI_CENA	0x3000
+#define BM_CL_RCR_CI_CENA	0x3100
 
 /* BTW, the drivers (and h/w programming model) already obtain the required
  * synchronisation for portal accesses via lwsync(), hwsync(), and
@@ -61,8 +61,8 @@
 /* Cache-inhibited register access. */
 #define __bm_in(bm, o)		__raw_readl((bm)->addr_ci + (o))
 #define __bm_out(bm, o, val)	__raw_writel((val), (bm)->addr_ci + (o))
-#define bm_in(reg)		__bm_in(&portal->addr, REG_##reg)
-#define bm_out(reg, val)	__bm_out(&portal->addr, REG_##reg, val)
+#define bm_in(reg)		__bm_in(&portal->addr, BM_REG_##reg)
+#define bm_out(reg, val)	__bm_out(&portal->addr, BM_REG_##reg, val)
 
 /* Cache-enabled (index) register access */
 #define __bm_cl_touch_ro(bm, o) dcbt_ro((bm)->addr_ce + (o))
@@ -75,16 +75,17 @@
 		dcbf(__tmpclout); \
 	} while (0)
 #define __bm_cl_invalidate(bm, o) dcbi((bm)->addr_ce + (o))
-#define bm_cl_touch_ro(reg)	__bm_cl_touch_ro(&portal->addr, CL_##reg##_CENA)
-#define bm_cl_touch_rw(reg)	__bm_cl_touch_rw(&portal->addr, CL_##reg##_CENA)
-#define bm_cl_in(reg)		__bm_cl_in(&portal->addr, CL_##reg##_CENA)
-#define bm_cl_out(reg, val)	__bm_cl_out(&portal->addr, CL_##reg##_CENA, val)
-#define bm_cl_invalidate(reg) __bm_cl_invalidate(&portal->addr, CL_##reg##_CENA)
+#define bm_cl_touch_ro(reg) __bm_cl_touch_ro(&portal->addr, BM_CL_##reg##_CENA)
+#define bm_cl_touch_rw(reg) __bm_cl_touch_rw(&portal->addr, BM_CL_##reg##_CENA)
+#define bm_cl_in(reg)	    __bm_cl_in(&portal->addr, BM_CL_##reg##_CENA)
+#define bm_cl_out(reg, val) __bm_cl_out(&portal->addr, BM_CL_##reg##_CENA, val)
+#define bm_cl_invalidate(reg)\
+	__bm_cl_invalidate(&portal->addr, BM_CL_##reg##_CENA)
 
 /* Cyclic helper for rings. FIXME: once we are able to do fine-grain perf
  * analysis, look at using the "extra" bit in the ring index registers to avoid
  * cyclic issues. */
-static inline u8 cyc_diff(u8 ringsize, u8 first, u8 last)
+static inline u8 bm_cyc_diff(u8 ringsize, u8 first, u8 last)
 {
 	/* 'first' is included, 'last' is excluded */
 	if (first <= last)
@@ -192,12 +193,13 @@ static inline int bm_rcr_init(struct bm_portal *portal, enum bm_rcr_pmode pmode,
 	u32 cfg;
 	u8 pi;
 
-	rcr->ring = portal->addr.addr_ce + CL_RCR;
+	rcr->ring = portal->addr.addr_ce + BM_CL_RCR;
 	rcr->ci = bm_in(RCR_CI_CINH) & (BM_RCR_SIZE - 1);
 	pi = bm_in(RCR_PI_CINH) & (BM_RCR_SIZE - 1);
 	rcr->cursor = rcr->ring + pi;
 	rcr->vbit = (bm_in(RCR_PI_CINH) & BM_RCR_SIZE) ?  BM_RCR_VERB_VBIT : 0;
-	rcr->available = BM_RCR_SIZE - 1 - cyc_diff(BM_RCR_SIZE, rcr->ci, pi);
+	rcr->available = BM_RCR_SIZE - 1
+		- bm_cyc_diff(BM_RCR_SIZE, rcr->ci, pi);
 	rcr->ithresh = bm_in(RCR_ITR);
 #ifdef CONFIG_FSL_DPA_CHECKING
 	rcr->busy = 0;
@@ -322,7 +324,7 @@ static inline u8 bm_rcr_cci_update(struct bm_portal *portal)
 	u8 diff, old_ci = rcr->ci;
 	DPA_ASSERT(rcr->cmode == bm_rcr_cci);
 	rcr->ci = bm_in(RCR_CI_CINH) & (BM_RCR_SIZE - 1);
-	diff = cyc_diff(BM_RCR_SIZE, old_ci, rcr->ci);
+	diff = bm_cyc_diff(BM_RCR_SIZE, old_ci, rcr->ci);
 	rcr->available += diff;
 	return diff;
 }
@@ -341,7 +343,7 @@ static inline u8 bm_rcr_cce_update(struct bm_portal *portal)
 	DPA_ASSERT(rcr->cmode == bm_rcr_cce);
 	rcr->ci = bm_cl_in(RCR_CI) & (BM_RCR_SIZE - 1);
 	bm_cl_invalidate(RCR_CI);
-	diff = cyc_diff(BM_RCR_SIZE, old_ci, rcr->ci);
+	diff = bm_cyc_diff(BM_RCR_SIZE, old_ci, rcr->ci);
 	rcr->available += diff;
 	return diff;
 }
@@ -378,8 +380,8 @@ static inline u8 bm_rcr_get_fill(struct bm_portal *portal)
 static inline int bm_mc_init(struct bm_portal *portal)
 {
 	register struct bm_mc *mc = &portal->mc;
-	mc->cr = portal->addr.addr_ce + CL_CR;
-	mc->rr = portal->addr.addr_ce + CL_RR0;
+	mc->cr = portal->addr.addr_ce + BM_CL_CR;
+	mc->rr = portal->addr.addr_ce + BM_CL_RR0;
 	mc->rridx = (__raw_readb(&mc->cr->__dont_write_directly__verb) &
 			BM_MCC_VERB_VBIT) ?  0 : 1;
 	mc->vbit = mc->rridx ? BM_MCC_VERB_VBIT : 0;
@@ -466,7 +468,7 @@ static inline void bm_isr_finish(__always_unused struct bm_portal *portal)
 {
 }
 
-#define SCN_REG(bpid) REG_SCN((bpid) / 32)
+#define SCN_REG(bpid) BM_REG_SCN((bpid) / 32)
 #define SCN_BIT(bpid) (0x80000000 >> (bpid & 31))
 static inline void bm_isr_bscn_mask(struct bm_portal *portal, u8 bpid,
 					int enable)
@@ -484,11 +486,38 @@ static inline void bm_isr_bscn_mask(struct bm_portal *portal, u8 bpid,
 
 static inline u32 __bm_isr_read(struct bm_portal *portal, enum bm_isr_reg n)
 {
-	return __bm_in(&portal->addr, REG_ISR + (n << 2));
+	return __bm_in(&portal->addr, BM_REG_ISR + (n << 2));
 }
 
 static inline void __bm_isr_write(struct bm_portal *portal, enum bm_isr_reg n,
 					u32 val)
 {
-	__bm_out(&portal->addr, REG_ISR + (n << 2), val);
+	__bm_out(&portal->addr, BM_REG_ISR + (n << 2), val);
+}
+
+/* Buffer Pool Cleanup */
+static inline int bm_shutdown_pool(struct bm_portal *p, u32 bpid)
+{
+	struct bm_mc_command *bm_cmd;
+	struct bm_mc_result *bm_res;
+
+	int aq_count = 0;
+	bool stop = false;
+	while (!stop) {
+		/* Aquire buffers until empty */
+		bm_cmd = bm_mc_start(p);
+		bm_cmd->acquire.bpid = bpid;
+		bm_mc_commit(p, BM_MCC_VERB_CMD_ACQUIRE |  1);
+		while (!(bm_res = bm_mc_result(p)))
+			cpu_relax();
+		if (!(bm_res->verb & BM_MCR_VERB_ACQUIRE_BUFCOUNT)) {
+			/* Pool is empty */
+			/* TBD : Should we do a few extra iterations in
+			   case some other some blocks keep buffers 'on deck',
+			   which may also be problematic */
+			stop = true;
+		} else
+			++aq_count;
+	};
+	return 0;
 }
diff --git a/drivers/staging/fsl_qbman/bman_private.h b/drivers/staging/fsl_qbman/bman_private.h
index c567314..e57ca68 100644
--- a/drivers/staging/fsl_qbman/bman_private.h
+++ b/drivers/staging/fsl_qbman/bman_private.h
@@ -68,11 +68,21 @@ int bman_init_ccsr(struct device_node *node);
 #endif
 
 /* Hooks from bman_driver.c in to bman_high.c */
+struct bman_portal *bman_create_portal(
+				       struct bman_portal *portal,
+				       const struct bm_portal_config *config);
 struct bman_portal *bman_create_affine_portal(
 			const struct bm_portal_config *config);
 struct bman_portal *bman_create_affine_slave(struct bman_portal *redirect);
+void bman_destroy_portal(struct bman_portal *bm);
+
 const struct bm_portal_config *bman_destroy_affine_portal(void);
 
+/* Hooks from fsl_usdpaa.c to bman_driver.c */
+struct bm_portal_config *bm_get_unused_portal(void);
+void bm_put_unused_portal(struct bm_portal_config *pcfg);
+void bm_set_liodns(struct bm_portal_config *pcfg);
+
 /* Pool logic in the portal driver, during initialisation, needs to know if
  * there's access to CCSR or not (if not, it'll cripple the pool allocator). */
 #ifdef CONFIG_FSL_BMAN_CONFIG
diff --git a/drivers/staging/fsl_qbman/dpa_alloc.c b/drivers/staging/fsl_qbman/dpa_alloc.c
index 03a9d28..3fb158e 100644
--- a/drivers/staging/fsl_qbman/dpa_alloc.c
+++ b/drivers/staging/fsl_qbman/dpa_alloc.c
@@ -93,27 +93,27 @@ int bman_alloc_bpid_range(u32 *result, u32 count, u32 align, int partial)
 }
 EXPORT_SYMBOL(bman_alloc_bpid_range);
 
-static int bp_valid(u32 bpid)
-{
-	struct bm_pool_state state;
-	int ret = bman_query_pools(&state);
-	BUG_ON(ret);
-	if (bman_depletion_get(&state.as.state, bpid))
-		/* "Available==1" means unavailable, go figure. Ie. it has no
-		 * buffers, which is means it is valid for deallocation. (So
-		 * true means false, which means true...) */
-		return 1;
-	return 0;
+static int bp_cleanup(u32 bpid)
+{
+	return (bman_shutdown_pool(bpid) == 0);
 }
 void bman_release_bpid_range(u32 bpid, u32 count)
 {
-	u32 total_invalid = release_id_range(&bpalloc, bpid, count, bp_valid);
+	u32 total_invalid = release_id_range(&bpalloc, bpid, count, bp_cleanup);
 	if (total_invalid)
 		pr_err("BPID range [%d..%d] (%d) had %d leaks\n",
 			bpid, bpid + count - 1, count, total_invalid);
 }
 EXPORT_SYMBOL(bman_release_bpid_range);
 
+void bman_seed_bpid_range(u32 bpid, u32 count)
+{
+	dpa_alloc_seed(&bpalloc, bpid, count);
+}
+EXPORT_SYMBOL(bman_seed_bpid_range);
+
+
+
 /* FQID allocator front-end */
 
 int qman_alloc_fqid_range(u32 *result, u32 count, u32 align, int partial)
@@ -122,25 +122,31 @@ int qman_alloc_fqid_range(u32 *result, u32 count, u32 align, int partial)
 }
 EXPORT_SYMBOL(qman_alloc_fqid_range);
 
-static int fq_valid(u32 fqid)
+static int fq_cleanup(u32 fqid)
 {
-	struct qman_fq fq = {
-		.fqid = fqid
-	};
-	struct qm_mcr_queryfq_np np;
-	int err = qman_query_fq_np(&fq, &np);
-	BUG_ON(err);
-	return ((np.state & QM_MCR_NP_STATE_MASK) == QM_MCR_NP_STATE_OOS);
+	return (qman_shutdown_fq(fqid) == 0);
 }
 void qman_release_fqid_range(u32 fqid, u32 count)
 {
-	u32 total_invalid = release_id_range(&fqalloc, fqid, count, fq_valid);
+	u32 total_invalid = release_id_range(&fqalloc, fqid, count, fq_cleanup);
 	if (total_invalid)
 		pr_err("FQID range [%d..%d] (%d) had %d leaks\n",
 			fqid, fqid + count - 1, count, total_invalid);
 }
 EXPORT_SYMBOL(qman_release_fqid_range);
 
+int qman_reserve_fqid_range(u32 fqid, u32 count)
+{
+	return dpa_alloc_reserve(&fqalloc, fqid, count);
+}
+EXPORT_SYMBOL(qman_reserve_fqid_range);
+
+void qman_seed_fqid_range(u32 fqid, u32 count)
+{
+	dpa_alloc_seed(&fqalloc, fqid, count);
+}
+EXPORT_SYMBOL(qman_seed_fqid_range);
+
 /* Pool-channel allocator front-end */
 
 int qman_alloc_pool_range(u32 *result, u32 count, u32 align, int partial)
@@ -149,12 +155,12 @@ int qman_alloc_pool_range(u32 *result, u32 count, u32 align, int partial)
 }
 EXPORT_SYMBOL(qman_alloc_pool_range);
 
-static int qp_valid(u32 qp)
+static int qpool_cleanup(u32 qp)
 {
-	/* TBD: when resource-management improves, we may be able to find
-	 * something better than this. Currently we query all FQDs starting from
+	/* We query all FQDs starting from
 	 * FQID 1 until we get an "invalid FQID" error, looking for non-OOS FQDs
-	 * whose destination channel is the pool-channel being released. */
+	 * whose destination channel is the pool-channel being released.
+	 * When a non-OOS FQD is found we attempt to clean it up */
 	struct qman_fq fq = {
 		.fqid = 1
 	};
@@ -169,9 +175,13 @@ static int qp_valid(u32 qp)
 			struct qm_fqd fqd;
 			err = qman_query_fq(&fq, &fqd);
 			BUG_ON(err);
-			if (fqd.dest.channel == qp)
-				/* The channel is the FQ's target, can't free */
-				return 0;
+			if (fqd.dest.channel == qp) {
+				/* The channel is the FQ's target, clean it */
+				if (qman_shutdown_fq(fq.fqid) != 0)
+					/* Couldn't shut down the FQ
+					   so the pool must be leaked */
+					return 0;
+			}
 		}
 		/* Move to the next FQID */
 		fq.fqid++;
@@ -179,7 +189,8 @@ static int qp_valid(u32 qp)
 }
 void qman_release_pool_range(u32 qp, u32 count)
 {
-	u32 total_invalid = release_id_range(&qpalloc, qp, count, qp_valid);
+	u32 total_invalid = release_id_range(&qpalloc, qp,
+					     count, qpool_cleanup);
 	if (total_invalid) {
 		/* Pool channels are almost always used individually */
 		if (count == 1)
@@ -192,6 +203,15 @@ void qman_release_pool_range(u32 qp, u32 count)
 }
 EXPORT_SYMBOL(qman_release_pool_range);
 
+
+void qman_seed_pool_range(u32 poolid, u32 count)
+{
+	dpa_alloc_seed(&qpalloc, poolid, count);
+
+}
+EXPORT_SYMBOL(qman_seed_pool_range);
+
+
 /* CGR ID allocator front-end */
 
 int qman_alloc_cgrid_range(u32 *result, u32 count, u32 align, int partial)
@@ -200,15 +220,56 @@ int qman_alloc_cgrid_range(u32 *result, u32 count, u32 align, int partial)
 }
 EXPORT_SYMBOL(qman_alloc_cgrid_range);
 
+static int cqr_cleanup(u32 cgrid)
+{
+	/* We query all FQDs starting from
+	 * FQID 1 until we get an "invalid FQID" error, looking for non-OOS FQDs
+	 * whose CGR is the CGR being released.
+	 */
+	struct qman_fq fq = {
+		.fqid = 1
+	};
+	int err;
+	do {
+		struct qm_mcr_queryfq_np np;
+		err = qman_query_fq_np(&fq, &np);
+		if (err)
+			/* FQID range exceeded, found no problems */
+			return 1;
+		if ((np.state & QM_MCR_NP_STATE_MASK) != QM_MCR_NP_STATE_OOS) {
+			struct qm_fqd fqd;
+			err = qman_query_fq(&fq, &fqd);
+			BUG_ON(err);
+			if ((fqd.fq_ctrl & QM_FQCTRL_CGE) &&
+			    (fqd.cgid == cgrid)) {
+				pr_err("CRGID 0x%x is being used by FQID 0x%x,"
+				       " CGR will be leaked\n",
+				       cgrid, fq.fqid);
+				return 1;
+			}
+		}
+		/* Move to the next FQID */
+		fq.fqid++;
+	} while (1);
+}
+
 void qman_release_cgrid_range(u32 cgrid, u32 count)
 {
-	u32 total_invalid = release_id_range(&cgralloc, cgrid, count, NULL);
+	u32 total_invalid = release_id_range(&cgralloc, cgrid,
+					     count, cqr_cleanup);
 	if (total_invalid)
 		pr_err("CGRID range [%d..%d] (%d) had %d leaks\n",
 			cgrid, cgrid + count - 1, count, total_invalid);
 }
 EXPORT_SYMBOL(qman_release_cgrid_range);
 
+void qman_seed_cgrid_range(u32 cgrid, u32 count)
+{
+	dpa_alloc_seed(&cgralloc, cgrid, count);
+
+}
+EXPORT_SYMBOL(qman_seed_cgrid_range);
+
 /* CEETM CHANNEL ID allocator front-end */
 int qman_alloc_ceetm0_channel_range(u32 *result, u32 count, u32 align,
 								 int partial)
@@ -236,6 +297,13 @@ void qman_release_ceetm0_channel_range(u32 channelid, u32 count)
 }
 EXPORT_SYMBOL(qman_release_ceetm0_channel_range);
 
+void qman_seed_ceetm0_channel_range(u32 channelid, u32 count)
+{
+	dpa_alloc_seed(&ceetm0_challoc, channelid, count);
+
+}
+EXPORT_SYMBOL(qman_seed_ceetm0_channel_range);
+
 void qman_release_ceetm1_channel_range(u32 channelid, u32 count)
 {
 	u32 total_invalid;
@@ -247,6 +315,13 @@ void qman_release_ceetm1_channel_range(u32 channelid, u32 count)
 }
 EXPORT_SYMBOL(qman_release_ceetm1_channel_range);
 
+void qman_seed_ceetm1_channel_range(u32 channelid, u32 count)
+{
+	dpa_alloc_seed(&ceetm1_challoc, channelid, count);
+
+}
+EXPORT_SYMBOL(qman_seed_ceetm1_channel_range);
+
 /* CEETM LFQID allocator front-end */
 int qman_alloc_ceetm0_lfqid_range(u32 *result, u32 count, u32 align,
 								 int partial)
@@ -274,6 +349,13 @@ void qman_release_ceetm0_lfqid_range(u32 lfqid, u32 count)
 }
 EXPORT_SYMBOL(qman_release_ceetm0_lfqid_range);
 
+void qman_seed_ceetm0_lfqid_range(u32 lfqid, u32 count)
+{
+	dpa_alloc_seed(&ceetm0_lfqidalloc, lfqid, count);
+
+}
+EXPORT_SYMBOL(qman_seed_ceetm0_lfqid_range);
+
 void qman_release_ceetm1_lfqid_range(u32 lfqid, u32 count)
 {
 	u32 total_invalid;
@@ -286,6 +368,14 @@ void qman_release_ceetm1_lfqid_range(u32 lfqid, u32 count)
 }
 EXPORT_SYMBOL(qman_release_ceetm1_lfqid_range);
 
+void qman_seed_ceetm1_lfqid_range(u32 lfqid, u32 count)
+{
+	dpa_alloc_seed(&ceetm1_lfqidalloc, lfqid, count);
+
+}
+EXPORT_SYMBOL(qman_seed_ceetm1_lfqid_range);
+
+
 /* Everything else is the common backend to all the allocators */
 
 /* The allocator is a (possibly-empty) list of these; */
@@ -293,6 +383,10 @@ struct alloc_node {
 	struct list_head list;
 	u32 base;
 	u32 num;
+	/* refcount and is_alloced are only set
+	   when the node is in the used list */
+	unsigned int refcount;
+	int is_alloced;
 };
 
 /* #define DPA_ALLOC_DEBUG */
@@ -304,12 +398,25 @@ static void DUMP(struct dpa_alloc *alloc)
 	int off = 0;
 	char buf[256];
 	struct alloc_node *p;
-	list_for_each_entry(p, &alloc->list, list) {
+	pr_info("Free Nodes\n");
+	list_for_each_entry(p, &alloc->free, list) {
 		if (off < 255)
 			off += snprintf(buf + off, 255-off, "{%d,%d}",
 				p->base, p->base + p->num - 1);
 	}
 	pr_info("%s\n", buf);
+
+	off = 0;
+	pr_info("Used Nodes\n");
+	list_for_each_entry(p, &alloc->used, list) {
+		if (off < 255)
+			off += snprintf(buf + off, 255-off, "{%d,%d}",
+				p->base, p->base + p->num - 1);
+	}
+	pr_info("%s\n", buf);
+
+
+
 }
 #else
 #define DPRINT(x...)	do { ; } while (0)
@@ -319,7 +426,7 @@ static void DUMP(struct dpa_alloc *alloc)
 int dpa_alloc_new(struct dpa_alloc *alloc, u32 *result, u32 count, u32 align,
 		  int partial)
 {
-	struct alloc_node *i = NULL, *next_best = NULL;
+	struct alloc_node *i = NULL, *next_best = NULL, *used_node = NULL;
 	u32 base, next_best_base = 0, num = 0, next_best_num = 0;
 	struct alloc_node *margin_left, *margin_right;
 
@@ -338,7 +445,7 @@ int dpa_alloc_new(struct dpa_alloc *alloc, u32 *result, u32 count, u32 align,
 		goto err;
 	}
 	spin_lock_irq(&alloc->lock);
-	list_for_each_entry(i, &alloc->list, list) {
+	list_for_each_entry(i, &alloc->free, list) {
 		base = (i->base + align - 1) / align;
 		base *= align;
 		if ((base - i->base) >= i->num)
@@ -385,12 +492,24 @@ done:
 err:
 	DPRINT("returning %d\n", i ? num : -ENOMEM);
 	DUMP(alloc);
-	return i ? (int)num : -ENOMEM;
+	if (!i)
+		return -ENOMEM;
+
+	/* Add the allocation to the used list with a refcount of 1 */
+	used_node = kmalloc(sizeof(*used_node), GFP_KERNEL);
+	if (!used_node)
+		return -ENOMEM;
+	used_node->base = *result;
+	used_node->num = num;
+	used_node->refcount = 1;
+	used_node->is_alloced = 1;
+	list_add_tail(&used_node->list, &alloc->used);
+	return (int)num;
 }
 
 /* Allocate the list node using GFP_ATOMIC, because we *really* want to avoid
  * forcing error-handling on to users in the deallocation path. */
-void dpa_alloc_free(struct dpa_alloc *alloc, u32 base_id, u32 count)
+static void _dpa_alloc_free(struct dpa_alloc *alloc, u32 base_id, u32 count)
 {
 	struct alloc_node *i, *node = kmalloc(sizeof(*node), GFP_ATOMIC);
 	BUG_ON(!node);
@@ -398,9 +517,11 @@ void dpa_alloc_free(struct dpa_alloc *alloc, u32 base_id, u32 count)
 	DUMP(alloc);
 	BUG_ON(!count);
 	spin_lock_irq(&alloc->lock);
+
+
 	node->base = base_id;
 	node->num = count;
-	list_for_each_entry(i, &alloc->list, list) {
+	list_for_each_entry(i, &alloc->free, list) {
 		if (i->base >= node->base) {
 			/* BUG_ON(any overlapping) */
 			BUG_ON(i->base < (node->base + node->num));
@@ -408,11 +529,11 @@ void dpa_alloc_free(struct dpa_alloc *alloc, u32 base_id, u32 count)
 			goto done;
 		}
 	}
-	list_add_tail(&node->list, &alloc->list);
+	list_add_tail(&node->list, &alloc->free);
 done:
 	/* Merge to the left */
 	i = list_entry(node->list.prev, struct alloc_node, list);
-	if (node->list.prev != &alloc->list) {
+	if (node->list.prev != &alloc->free) {
 		BUG_ON((i->base + i->num) > node->base);
 		if ((i->base + i->num) == node->base) {
 			node->base = i->base;
@@ -423,7 +544,7 @@ done:
 	}
 	/* Merge to the right */
 	i = list_entry(node->list.next, struct alloc_node, list);
-	if (node->list.next != &alloc->list) {
+	if (node->list.next != &alloc->free) {
 		BUG_ON((node->base + node->num) > i->base);
 		if ((node->base + node->num) == i->base) {
 			node->num += i->num;
@@ -435,60 +556,107 @@ done:
 	DUMP(alloc);
 }
 
-int dpa_alloc_reserve(struct dpa_alloc *alloc, u32 base, u32 num)
+
+void dpa_alloc_free(struct dpa_alloc *alloc, u32 base_id, u32 count)
 {
 	struct alloc_node *i = NULL;
-	struct alloc_node *margin_left, *margin_right;
+	spin_lock_irq(&alloc->lock);
 
-	DPRINT("alloc_reserve(%d,%d)\n", base_id, count);
-	DUMP(alloc);
-	margin_left = kmalloc(sizeof(*margin_left), GFP_KERNEL);
-	if (!margin_left)
-		goto err;
-	margin_right = kmalloc(sizeof(*margin_right), GFP_KERNEL);
-	if (!margin_right) {
-		kfree(margin_left);
-		goto err;
+	/* First find the node in the used list and decrement its ref count */
+	list_for_each_entry(i, &alloc->used, list) {
+		if (i->base == base_id && i->num == count) {
+			--i->refcount;
+			if (i->refcount == 0) {
+				list_del(&i->list);
+				spin_unlock_irq(&alloc->lock);
+				if (i->is_alloced)
+					_dpa_alloc_free(alloc, base_id, count);
+				kfree(i);
+				return;
+			}
+			spin_unlock_irq(&alloc->lock);
+			return;
+		}
 	}
+	/* Couldn't find the allocation */
+	pr_err("Attempt to free ID 0x%x COUNT %d that wasn't alloc'd or reserved\n",
+	       base_id, count);
+	spin_unlock_irq(&alloc->lock);
+}
+
+void dpa_alloc_seed(struct dpa_alloc *alloc, u32 base_id, u32 count)
+{
+	/* Same as free but no previous allocation checking is needed */
+	_dpa_alloc_free(alloc, base_id, count);
+}
+
+
+int dpa_alloc_reserve(struct dpa_alloc *alloc, u32 base, u32 num)
+{
+	struct alloc_node *i = NULL, *used_node;
+
+	DPRINT("alloc_reserve(%d,%d)\n", base, num);
+	DUMP(alloc);
+
 	spin_lock_irq(&alloc->lock);
-	list_for_each_entry(i, &alloc->list, list)
-		if ((i->base <= base) && ((i->base + i->num) >= (base + num)))
+
+	/* Check for the node in the used list.
+	   If found, increase it's refcount */
+	list_for_each_entry(i, &alloc->used, list) {
+		if ((i->base == base) && (i->num == num)) {
+			++i->refcount;
+			spin_unlock_irq(&alloc->lock);
+			return 0;
+		}
+		if ((base >= i->base) && (base < (i->base + i->num))) {
+			/* This is an attempt to reserve a region that was
+			   already reserved or alloced with a different
+			   base or num */
+			pr_err("Cannot reserve %d - %d, it overlaps with"
+			       " existing reservation from %d - %d\n",
+			       base, base + num - 1, i->base,
+			       i->base + i->num - 1);
+			spin_unlock_irq(&alloc->lock);
+			return -1;
+		}
+	}
+	/* Check to make sure this ID isn't in the free list */
+	list_for_each_entry(i, &alloc->free, list) {
+		if ((base >= i->base) && (base < (i->base + i->num))) {
 			/* yep, the reservation is within this node */
-			goto done;
-	i = NULL;
-done:
-	if (i) {
-		if (base != i->base) {
-			margin_left->base = i->base;
-			margin_left->num = base - i->base;
-			list_add_tail(&margin_left->list, &i->list);
-		} else
-			kfree(margin_left);
-		if ((base + num) < (i->base + i->num)) {
-			margin_right->base = base + num;
-			margin_right->num = (i->base + i->num) -
-						(base + num);
-			list_add(&margin_right->list, &i->list);
-		} else
-			kfree(margin_right);
-		list_del(&i->list);
-		kfree(i);
+			pr_err("Cannot reserve %d - %d, it overlaps with"
+			       " free range %d - %d and must be alloced\n",
+			       base, base + num - 1,
+			       i->base, i->base + i->num - 1);
+			spin_unlock_irq(&alloc->lock);
+			return -1;
+		}
 	}
+	/* Add the allocation to the used list with a refcount of 1 */
+	used_node = kmalloc(sizeof(*used_node), GFP_KERNEL);
+	if (!used_node) {
+		spin_unlock_irq(&alloc->lock);
+		return -ENOMEM;
+
+	}
+	used_node->base = base;
+	used_node->num = num;
+	used_node->refcount = 1;
+	used_node->is_alloced = 0;
+	list_add_tail(&used_node->list, &alloc->used);
 	spin_unlock_irq(&alloc->lock);
-err:
-	DPRINT("returning %d\n", i ? 0 : -ENOMEM);
-	DUMP(alloc);
-	return i ? 0 : -ENOMEM;
+	return 0;
 }
 
+
 int dpa_alloc_pop(struct dpa_alloc *alloc, u32 *result, u32 *count)
 {
 	struct alloc_node *i = NULL;
 	DPRINT("alloc_pop()\n");
 	DUMP(alloc);
 	spin_lock_irq(&alloc->lock);
-	if (!list_empty(&alloc->list)) {
-		i = list_entry(alloc->list.next, struct alloc_node, list);
+	if (!list_empty(&alloc->free)) {
+		i = list_entry(alloc->free.next, struct alloc_node, list);
 		list_del(&i->list);
 	}
 	spin_unlock_irq(&alloc->lock);
@@ -501,3 +669,20 @@ int dpa_alloc_pop(struct dpa_alloc *alloc, u32 *result, u32 *count)
 	kfree(i);
 	return 0;
 }
+
+int dpa_alloc_check(struct dpa_alloc *list_head, u32 item)
+{
+	struct alloc_node *i = NULL;
+	int res = 0;
+	DPRINT("alloc_check()\n");
+	spin_lock_irq(&list_head->lock);
+
+	list_for_each_entry(i, &list_head->free, list) {
+		if ((item >= i->base) && (item < (i->base + i->num))) {
+			res = 1;
+			break;
+		}
+	}
+	spin_unlock_irq(&list_head->lock);
+	return res;
+}
diff --git a/drivers/staging/fsl_qbman/dpa_sys.h b/drivers/staging/fsl_qbman/dpa_sys.h
index 4aa61ae..235706b 100644
--- a/drivers/staging/fsl_qbman/dpa_sys.h
+++ b/drivers/staging/fsl_qbman/dpa_sys.h
@@ -69,33 +69,6 @@
 /* When copying aligned words or shorts, try to avoid memcpy() */
 #define CONFIG_TRY_BETTER_MEMCPY
 
-/* Handle portals destined for USDPAA (user-space).
- *
- * The UIO handling is mostly in dpa_uio.c which is common to qman and bman, but
- * there are some specifics to each case, and they have independent data
- * structures. The "pcfg"s for qman and bman portals are maintained in lists in
- * their respective drivers, and they're detached from those lists when they are
- * to be registered as UIO devices, so we have dpa_uio.c store them in a
- * mixed-type list, and use this vtable of callbacks to let the qman+bman
- * drivers container_of() the list item to their respective object wrappers and
- * implement whatever logic distinguishes them.
- */
-struct dpa_uio_vtable {
-	/* This callback should fill in 'name', 'mem', and 'irq'. The rest will
-	 * be filled in by dpa_uio.c */
-	int (*init_uio)(const struct list_head *pcfg, struct uio_info *info);
-	/* Free up whatever object contains 'pcfg' */
-	void (*destroy)(const struct list_head *pcfg, struct uio_info *info);
-	/* Called when the portal is opened (Qman uses this for rerouting
-	 * stashing to the current cpu) */
-	int (*on_open)(struct list_head *pcfg);
-	void (*on_close)(const struct list_head *pcfg);
-	/* Called when an interrupt fires - must disable interrupts */
-	void (*on_interrupt)(const struct list_head *pcfg);
-};
-int __init dpa_uio_register(struct list_head *new_pcfg,
-			    const struct dpa_uio_vtable *vtable);
-
 /* For 2-element tables related to cache-inhibited and cache-enabled mappings */
 #define DPA_PORTAL_CE 0
 #define DPA_PORTAL_CI 1
diff --git a/drivers/staging/fsl_qbman/fsl_usdpaa.c b/drivers/staging/fsl_qbman/fsl_usdpaa.c
new file mode 100644
index 0000000..d7a6ee1
--- /dev/null
+++ b/drivers/staging/fsl_qbman/fsl_usdpaa.c
@@ -0,0 +1,1399 @@
+/* Copyright (C) 2008-2012 Freescale Semiconductor, Inc.
+ * Authors: Andy Fleming <afleming@freescale.com>
+ *	    Timur Tabi <timur@freescale.com>
+ *	    Geoff Thorpe <Geoff.Thorpe@freescale.com>
+ *
+ * This file is licensed under the terms of the GNU General Public License
+ * version 2.  This program is licensed "as is" without any warranty of any
+ * kind, whether express or implied.
+ */
+
+#include <linux/fsl_usdpaa.h>
+#include "bman_low.h"
+#include "qman_low.h"
+
+#include <linux/miscdevice.h>
+#include <linux/fs.h>
+#include <linux/cdev.h>
+#include <linux/mm.h>
+#include <linux/of.h>
+#include <linux/memblock.h>
+#include <linux/slab.h>
+#include <linux/mman.h>
+
+/* Physical address range of the memory reservation, exported for mm/mem.c */
+static u64 phys_start;
+static u64 phys_size;
+/* PFN versions of the above */
+static unsigned long pfn_start;
+static unsigned long pfn_size;
+
+/* Memory reservations are manipulated under this spinlock (which is why 'refs'
+ * isn't atomic_t). */
+static DEFINE_SPINLOCK(mem_lock);
+
+/* The range of TLB1 indices */
+static unsigned int first_tlb;
+static unsigned int num_tlb;
+static unsigned int current_tlb; /* loops around for fault handling */
+
+/* Memory reservation is represented as a list of 'mem_fragment's, some of which
+ * may be mapped. Unmapped fragments are always merged where possible. */
+static LIST_HEAD(mem_list);
+
+struct mem_mapping;
+
+/* Memory fragments are in 'mem_list'. */
+struct mem_fragment {
+	u64 base;
+	u64 len;
+	unsigned long pfn_base; /* PFN version of 'base' */
+	unsigned long pfn_len; /* PFN version of 'len' */
+	unsigned int refs; /* zero if unmapped */
+	struct list_head list;
+	/* if mapped, flags+name captured at creation time */
+	u32 flags;
+	char name[USDPAA_DMA_NAME_MAX];
+	/* support multi-process locks per-memory-fragment. */
+	int has_locking;
+	wait_queue_head_t wq;
+	struct mem_mapping *owner;
+};
+
+/* Mappings of memory fragments in 'struct ctx'. These are created from
+ * ioctl(USDPAA_IOCTL_DMA_MAP), though the actual mapping then happens via a
+ * mmap(). */
+struct mem_mapping {
+	struct mem_fragment *frag;
+	struct list_head list;
+};
+
+struct portal_mapping {
+	struct usdpaa_ioctl_portal_map user;
+	union {
+		struct qm_portal_config *qportal;
+		struct bm_portal_config *bportal;
+	};
+	/* Declare space for the portals in case the process
+	   exits unexpectedly and needs to be cleaned by the kernel */
+	union {
+		struct qm_portal qman_portal_low;
+		struct bm_portal bman_portal_low;
+	};
+	struct list_head list;
+	struct resource *phys;
+};
+
+/* Track the DPAA resources the process is using */
+struct active_resource {
+	struct list_head list;
+	u32 id;
+	u32 num;
+	unsigned int refcount;
+};
+
+/* Per-FD state (which should also be per-process but we don't enforce that) */
+struct ctx {
+	/* Lock to protect the context */
+	spinlock_t lock;
+	/* Allocated resources get put here for accounting */
+	struct list_head resources[usdpaa_id_max];
+	/* list of DMA maps */
+	struct list_head maps;
+	/* list of portal maps */
+	struct list_head portals;
+};
+
+/* Different resource classes */
+static const struct alloc_backend {
+	enum usdpaa_id_type id_type;
+	int (*alloc)(u32 *, u32, u32, int);
+	void (*release)(u32 base, unsigned int count);
+	int (*reserve)(u32 base, unsigned int count);
+	const char *acronym;
+} alloc_backends[] = {
+	{
+		.id_type = usdpaa_id_fqid,
+		.alloc = qman_alloc_fqid_range,
+		.release = qman_release_fqid_range,
+		.reserve = qman_reserve_fqid_range,
+		.acronym = "FQID"
+	},
+	{
+		.id_type = usdpaa_id_bpid,
+		.alloc = bman_alloc_bpid_range,
+		.release = bman_release_bpid_range,
+		.acronym = "BPID"
+	},
+	{
+		.id_type = usdpaa_id_qpool,
+		.alloc = qman_alloc_pool_range,
+		.release = qman_release_pool_range,
+		.acronym = "QPOOL"
+	},
+	{
+		.id_type = usdpaa_id_cgrid,
+		.alloc = qman_alloc_cgrid_range,
+		.release = qman_release_cgrid_range,
+		.acronym = "CGRID"
+	},
+	{
+		.id_type = usdpaa_id_ceetm0_lfqid,
+		.alloc = qman_alloc_ceetm0_lfqid_range,
+		.release = qman_release_ceetm0_lfqid_range,
+		.acronym = "CEETM0_LFQID"
+	},
+	{
+		.id_type = usdpaa_id_ceetm0_channelid,
+		.alloc = qman_alloc_ceetm0_channel_range,
+		.release = qman_release_ceetm0_channel_range,
+		.acronym = "CEETM0_LFQID"
+	},
+	{
+		.id_type = usdpaa_id_ceetm1_lfqid,
+		.alloc = qman_alloc_ceetm1_lfqid_range,
+		.release = qman_release_ceetm1_lfqid_range,
+		.acronym = "CEETM1_LFQID"
+	},
+	{
+		.id_type = usdpaa_id_ceetm1_channelid,
+		.alloc = qman_alloc_ceetm1_channel_range,
+		.release = qman_release_ceetm1_channel_range,
+		.acronym = "CEETM1_LFQID"
+	},
+	{
+		/* This terminates the array */
+		.id_type = usdpaa_id_max
+	}
+};
+
+/* Helper for ioctl_dma_map() when we have a larger fragment than we need. This
+ * splits the fragment into 4 and returns the upper-most. (The caller can loop
+ * until it has a suitable fragment size.) */
+static struct mem_fragment *split_frag(struct mem_fragment *frag)
+{
+	struct mem_fragment *x[3];
+	x[0] = kmalloc(sizeof(struct mem_fragment), GFP_KERNEL);
+	x[1] = kmalloc(sizeof(struct mem_fragment), GFP_KERNEL);
+	x[2] = kmalloc(sizeof(struct mem_fragment), GFP_KERNEL);
+	if (!x[0] || !x[1] || !x[2]) {
+		kfree(x[0]);
+		kfree(x[1]);
+		kfree(x[2]);
+		return NULL;
+	}
+	BUG_ON(frag->refs);
+	frag->len >>= 2;
+	frag->pfn_len >>= 2;
+	x[0]->base = frag->base + frag->len;
+	x[1]->base = x[0]->base + frag->len;
+	x[2]->base = x[1]->base + frag->len;
+	x[0]->len = x[1]->len = x[2]->len = frag->len;
+	x[0]->pfn_base = frag->pfn_base + frag->pfn_len;
+	x[1]->pfn_base = x[0]->pfn_base + frag->pfn_len;
+	x[2]->pfn_base = x[1]->pfn_base + frag->pfn_len;
+	x[0]->pfn_len = x[1]->pfn_len = x[2]->pfn_len = frag->pfn_len;
+	x[0]->refs = x[1]->refs = x[2]->refs = 0;
+	list_add(&x[0]->list, &frag->list);
+	list_add(&x[1]->list, &x[0]->list);
+	list_add(&x[2]->list, &x[1]->list);
+	return x[2];
+}
+
+/* Conversely, when a fragment is released we look to see whether its
+ * similarly-split siblings are free to be reassembled. */
+static struct mem_fragment *merge_frag(struct mem_fragment *frag)
+{
+	/* If this fragment can be merged with its siblings, it will have
+	 * newbase and newlen as its geometry. */
+	uint64_t newlen = frag->len << 2;
+	uint64_t newbase = frag->base & ~(newlen - 1);
+	struct mem_fragment *tmp, *leftmost = frag, *rightmost = frag;
+	/* Scan left until we find the start */
+	tmp = list_entry(frag->list.prev, struct mem_fragment, list);
+	while ((&tmp->list != &mem_list) && (tmp->base >= newbase)) {
+		if (tmp->refs)
+			return NULL;
+		if (tmp->len != tmp->len)
+			return NULL;
+		leftmost = tmp;
+		tmp = list_entry(tmp->list.prev, struct mem_fragment, list);
+	}
+	/* Scan right until we find the end */
+	tmp = list_entry(frag->list.next, struct mem_fragment, list);
+	while ((&tmp->list != &mem_list) && (tmp->base < (newbase + newlen))) {
+		if (tmp->refs)
+			return NULL;
+		if (tmp->len != tmp->len)
+			return NULL;
+		rightmost = tmp;
+		tmp = list_entry(tmp->list.next, struct mem_fragment, list);
+	}
+	if (leftmost == rightmost)
+		return NULL;
+	/* OK, we can merge */
+	frag = leftmost;
+	frag->len = newlen;
+	frag->pfn_len = newlen >> PAGE_SHIFT;
+	while (1) {
+		int lastone;
+		tmp = list_entry(frag->list.next, struct mem_fragment, list);
+		lastone = (tmp == rightmost);
+		if (&tmp->list == &mem_list)
+			break;
+		list_del(&tmp->list);
+		kfree(tmp);
+		if (lastone)
+			break;
+	}
+	return frag;
+}
+
+/* Helper to verify that 'sz' is (4096 * 4^x) for some x. */
+static int is_good_size(u64 sz)
+{
+	int log = ilog2(phys_size);
+	if ((phys_size & (phys_size - 1)) || (log < 12) || (log & 1))
+		return 0;
+	return 1;
+}
+
+/* Hook from arch/powerpc/mm/mem.c */
+int usdpaa_test_fault(unsigned long pfn, u64 *phys_addr, u64 *size)
+{
+	struct mem_fragment *frag;
+	int idx = -1;
+	if ((pfn < pfn_start) || (pfn >= (pfn_start + pfn_size)))
+		return -1;
+	/* It's in-range, we need to find the fragment */
+	spin_lock(&mem_lock);
+	list_for_each_entry(frag, &mem_list, list) {
+		if ((pfn >= frag->pfn_base) && (pfn < (frag->pfn_base +
+						       frag->pfn_len))) {
+			*phys_addr = frag->base;
+			*size = frag->len;
+			idx = current_tlb++;
+			if (current_tlb >= (first_tlb + num_tlb))
+				current_tlb = first_tlb;
+			break;
+		}
+	}
+	spin_unlock(&mem_lock);
+	return idx;
+}
+
+static int usdpaa_open(struct inode *inode, struct file *filp)
+{
+	const struct alloc_backend *backend = &alloc_backends[0];
+	struct ctx *ctx = kmalloc(sizeof(struct ctx), GFP_KERNEL);
+	if (!ctx)
+		return -ENOMEM;
+	filp->private_data = ctx;
+
+	while (backend->id_type != usdpaa_id_max) {
+		INIT_LIST_HEAD(&ctx->resources[backend->id_type]);
+		backend++;
+	}
+
+	INIT_LIST_HEAD(&ctx->maps);
+	INIT_LIST_HEAD(&ctx->portals);
+	spin_lock_init(&ctx->lock);
+
+	filp->f_mapping->backing_dev_info = &directly_mappable_cdev_bdi;
+
+	return 0;
+}
+
+
+#define DQRR_MAXFILL 15
+
+/* Reset a QMan portal to its default state */
+static int init_qm_portal(struct qm_portal_config *config,
+			  struct qm_portal *portal)
+{
+	portal->addr.addr_ce = config->addr_virt[DPA_PORTAL_CE];
+	portal->addr.addr_ci = config->addr_virt[DPA_PORTAL_CI];
+
+	/* Initialize the DQRR.  This will stop any dequeue
+	   commands that are in progress */
+	if (qm_dqrr_init(portal, config, qm_dqrr_dpush, qm_dqrr_pvb,
+			 qm_dqrr_cdc, DQRR_MAXFILL)) {
+		pr_err("qm_dqrr_init() failed when trying to"
+		       " recover portal, portal will be leaked\n");
+		return 1;
+	}
+	/* Consume any items in the dequeue ring */
+	qm_dqrr_cdc_consume_n(portal, 0xffff);
+
+	/* Initialize the EQCR */
+	if (qm_eqcr_init(portal, qm_eqcr_pvb, qm_eqcr_cce)) {
+		pr_err("Qman EQCR initialisation failed\n");
+		return 1;
+	}
+	/* initialize the MR */
+	if (qm_mr_init(portal, qm_mr_pvb, qm_mr_cci)) {
+		pr_err("Qman MR initialisation failed\n");
+		return 1;
+	}
+	qm_mr_pvb_update(portal);
+	while (qm_mr_current(portal)) {
+		qm_mr_next(portal);
+		qm_mr_cci_consume_to_current(portal);
+		qm_mr_pvb_update(portal);
+	}
+
+	if (qm_mc_init(portal)) {
+		pr_err("Qman MC initialisation failed\n");
+		return 1;
+	}
+	return 0;
+}
+
+static int init_bm_portal(struct bm_portal_config *config,
+			  struct bm_portal *portal)
+{
+	portal->addr.addr_ce = config->addr_virt[DPA_PORTAL_CE];
+	portal->addr.addr_ci = config->addr_virt[DPA_PORTAL_CI];
+
+	if (bm_rcr_init(portal, bm_rcr_pvb, bm_rcr_cce)) {
+		pr_err("Bman RCR initialisation failed\n");
+	return 1;
+	}
+	if (bm_mc_init(portal)) {
+		pr_err("Bman MC initialisation failed\n");
+		return 1;
+	}
+	return 0;
+}
+
+/* Function that will scan all FQ's in the system.  For each FQ that is not
+   OOS it will call the check_channel helper to determine if the FQ should
+   be torn down.  If the check_channel helper returns true the FQ will be
+   transitioned to the OOS state */
+static int qm_check_and_destroy_fqs(struct qm_portal *portal, void *ctx,
+				    bool (*check_channel)
+				    (void *ctx, u32 channel))
+{
+	u32 fq_id = 0;
+	while (1) {
+		struct qm_mc_command *mcc;
+		struct qm_mc_result *mcr;
+		u8 state;
+		u32 channel;
+
+		/* Determine the channel for the FQID */
+		mcc = qm_mc_start(portal);
+		mcc->queryfq.fqid = fq_id;
+		qm_mc_commit(portal, QM_MCC_VERB_QUERYFQ);
+		while (!(mcr = qm_mc_result(portal)))
+			cpu_relax();
+		DPA_ASSERT((mcr->verb & QM_MCR_VERB_MASK)
+			   == QM_MCR_VERB_QUERYFQ);
+		if (mcr->result != QM_MCR_RESULT_OK)
+			break; /* End of valid FQIDs */
+
+		channel = mcr->queryfq.fqd.dest.channel;
+		/* Determine the state of the FQID */
+		mcc = qm_mc_start(portal);
+		mcc->queryfq_np.fqid = fq_id;
+		qm_mc_commit(portal, QM_MCC_VERB_QUERYFQ_NP);
+		while (!(mcr = qm_mc_result(portal)))
+			cpu_relax();
+		DPA_ASSERT((mcr->verb & QM_MCR_VERB_MASK)
+			   == QM_MCR_VERB_QUERYFQ_NP);
+		state = mcr->queryfq_np.state & QM_MCR_NP_STATE_MASK;
+		if (state == QM_MCR_NP_STATE_OOS)
+			/* Already OOS, no need to do anymore checks */
+			goto next;
+
+		if (check_channel(ctx, channel))
+			qm_shutdown_fq(portal, fq_id);
+ next:
+		++fq_id;
+	}
+	return 0;
+
+}
+
+static bool check_channel_device(void *_ctx, u32 channel)
+{
+	struct ctx *ctx = _ctx;
+	struct portal_mapping *portal, *tmpportal;
+	struct active_resource *res;
+
+	/* See if the FQ is destined for one of the portals we're cleaning up */
+	list_for_each_entry_safe(portal, tmpportal, &ctx->portals, list) {
+		if (portal->user.type == usdpaa_portal_qman) {
+			if (portal->qportal->public_cfg.channel == channel) {
+				/* This FQs destination is a portal
+				   we're cleaning, send a retire */
+				return true;
+			}
+		}
+	}
+
+	/* Check the pool channels that will be released as well */
+	list_for_each_entry(res, &ctx->resources[usdpaa_id_qpool], list) {
+		if ((res->id >= channel) &&
+		    ((res->id + res->num - 1) <= channel))
+			return true;
+	}
+	return false;
+}
+
+
+
+static int usdpaa_release(struct inode *inode, struct file *filp)
+{
+	struct ctx *ctx = filp->private_data;
+	struct mem_mapping *map, *tmpmap;
+	struct portal_mapping *portal, *tmpportal;
+	const struct alloc_backend *backend = &alloc_backends[0];
+	struct active_resource *res;
+	struct qm_portal *qm_cleanup_portal = NULL;
+	struct bm_portal *bm_cleanup_portal = NULL;
+	struct qm_portal_config *qm_alloced_portal = NULL;
+	struct bm_portal_config *bm_alloced_portal = NULL;
+
+	/* The following logic is used to recover resources that were not
+	   correctly released by the process that is closing the FD.
+	   Step 1: syncronize the HW with the qm_portal/bm_portal structures
+	   in the kernel
+	*/
+
+	list_for_each_entry_safe(portal, tmpportal, &ctx->portals, list) {
+		/* Try to recover any portals that weren't shut down */
+		if (portal->user.type == usdpaa_portal_qman) {
+			init_qm_portal(portal->qportal,
+				       &portal->qman_portal_low);
+			if (!qm_cleanup_portal)
+				qm_cleanup_portal = &portal->qman_portal_low;
+		} else {
+			/* BMAN */
+			init_bm_portal(portal->bportal,
+				       &portal->bman_portal_low);
+			if (!bm_cleanup_portal)
+				bm_cleanup_portal = &portal->bman_portal_low;
+		}
+	}
+	/* If no portal was found, allocate one for cleanup */
+	if (!qm_cleanup_portal) {
+		qm_alloced_portal = qm_get_unused_portal();
+		if (!qm_alloced_portal) {
+			pr_crit("No QMan portal avalaible for cleanup\n");
+			return -1;
+		}
+		qm_cleanup_portal = kmalloc(sizeof(struct qm_portal),
+					    GFP_KERNEL);
+		if (!qm_cleanup_portal)
+			return -ENOMEM;
+		init_qm_portal(qm_alloced_portal, qm_cleanup_portal);
+
+	}
+	if (!bm_cleanup_portal) {
+		bm_alloced_portal = bm_get_unused_portal();
+		if (!bm_alloced_portal) {
+			pr_crit("No BMan portal avalaible for cleanup\n");
+			return -1;
+		}
+		bm_cleanup_portal = kmalloc(sizeof(struct bm_portal),
+					    GFP_KERNEL);
+		if (!bm_cleanup_portal)
+			return -ENOMEM;
+		init_bm_portal(bm_alloced_portal, bm_cleanup_portal);
+	}
+
+	/* OOS the FQs associated with this process */
+	qm_check_and_destroy_fqs(qm_cleanup_portal, ctx, check_channel_device);
+
+	while (backend->id_type != usdpaa_id_max) {
+		int leaks = 0;
+		list_for_each_entry(res, &ctx->resources[backend->id_type],
+				    list) {
+			leaks += res->num;
+			backend->release(res->id, res->num);
+		}
+		if (leaks)
+			pr_crit("USDPAA process leaking %d %s%s\n", leaks,
+				backend->acronym, (leaks > 1) ? "s" : "");
+		backend++;
+	}
+	/* Release any DMA regions */
+	spin_lock(&mem_lock);
+	list_for_each_entry_safe(map, tmpmap, &ctx->maps, list) {
+		if (map->frag->has_locking && (map->frag->owner == map)) {
+			map->frag->owner = NULL;
+			wake_up(&map->frag->wq);
+		}
+		if (!--map->frag->refs) {
+			struct mem_fragment *frag = map->frag;
+			do {
+				frag = merge_frag(frag);
+			} while (frag);
+		}
+		list_del(&map->list);
+		kfree(map);
+	}
+	spin_unlock(&mem_lock);
+
+	/* Return portals */
+	list_for_each_entry_safe(portal, tmpportal, &ctx->portals, list) {
+		if (portal->user.type == usdpaa_portal_qman) {
+			/* Give the portal back to the allocator */
+			qm_put_unused_portal(portal->qportal);
+		} else {
+			bm_put_unused_portal(portal->bportal);
+		}
+		list_del(&portal->list);
+		kfree(portal);
+	}
+	if (qm_alloced_portal) {
+		qm_put_unused_portal(qm_alloced_portal);
+		kfree(qm_cleanup_portal);
+	}
+	if (bm_alloced_portal) {
+		bm_put_unused_portal(bm_alloced_portal);
+		kfree(bm_cleanup_portal);
+	}
+
+	kfree(ctx);
+	return 0;
+}
+
+static int check_mmap_dma(struct ctx *ctx, struct vm_area_struct *vma,
+			  int *match, unsigned long *pfn)
+{
+	struct mem_mapping *map;
+
+	list_for_each_entry(map, &ctx->maps, list) {
+		if (map->frag->pfn_base == vma->vm_pgoff) {
+			*match = 1;
+			if (map->frag->len != (vma->vm_end - vma->vm_start))
+				return -EINVAL;
+			*pfn = map->frag->pfn_base;
+			return 0;
+		}
+	}
+	*match = 0;
+	return 0;
+}
+
+static int check_mmap_resource(struct resource *res, struct vm_area_struct *vma,
+			       int *match, unsigned long *pfn)
+{
+	*pfn = res->start >> PAGE_SHIFT;
+	if (*pfn == vma->vm_pgoff) {
+		*match = 1;
+		if ((vma->vm_end - vma->vm_start) != resource_size(res))
+			return -EINVAL;
+	} else
+		*match = 0;
+	return 0;
+}
+
+static int check_mmap_portal(struct ctx *ctx, struct vm_area_struct *vma,
+			      int *match, unsigned long *pfn)
+{
+	struct portal_mapping *portal;
+	int ret;
+
+	list_for_each_entry(portal, &ctx->portals, list) {
+		ret = check_mmap_resource(&portal->phys[DPA_PORTAL_CE], vma,
+					  match, pfn);
+		if (*match) {
+			vma->vm_page_prot =
+				pgprot_cached_noncoherent(vma->vm_page_prot);
+			return ret;
+		}
+		ret = check_mmap_resource(&portal->phys[DPA_PORTAL_CI], vma,
+					  match, pfn);
+		if (*match) {
+			vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
+			return ret;
+		}
+	}
+	*match = 0;
+	return 0;
+}
+
+static int usdpaa_mmap(struct file *filp, struct vm_area_struct *vma)
+{
+	struct ctx *ctx = filp->private_data;
+	unsigned long pfn;
+	int match, ret;
+
+	spin_lock(&mem_lock);
+	ret = check_mmap_dma(ctx, vma, &match, &pfn);
+	if (!match)
+		ret = check_mmap_portal(ctx, vma, &match, &pfn);
+	spin_unlock(&mem_lock);
+	if (!match)
+		return -EINVAL;
+	if (!ret)
+		ret = remap_pfn_range(vma, vma->vm_start, pfn,
+				      vma->vm_end - vma->vm_start,
+				      vma->vm_page_prot);
+	return ret;
+}
+
+/* Return the nearest rounded-up address >= 'addr' that is 'sz'-aligned. 'sz'
+ * must be a power of 2, but both 'addr' and 'sz' can be expressions. */
+#define USDPAA_MEM_ROUNDUP(addr, sz) \
+	({ \
+		unsigned long foo_align = (sz) - 1; \
+		((addr) + foo_align) & ~foo_align; \
+	})
+/* Searching for a size-aligned virtual address range starting from 'addr' */
+static unsigned long usdpaa_get_unmapped_area(struct file *file,
+					      unsigned long addr,
+					      unsigned long len,
+					      unsigned long pgoff,
+					      unsigned long flags)
+{
+	struct vm_area_struct *vma;
+
+	if (!is_good_size(len))
+		return -EINVAL;
+
+	addr = USDPAA_MEM_ROUNDUP(addr, len);
+	vma = find_vma(current->mm, addr);
+	/* Keep searching until we reach the end of currently-used virtual
+	 * address-space or we find a big enough gap. */
+	while (vma) {
+		if ((addr + len) < vma->vm_start)
+			return addr;
+		addr = USDPAA_MEM_ROUNDUP(vma->vm_end, len);
+		vma = vma->vm_next;
+	}
+	if ((TASK_SIZE - len) < addr)
+		return -ENOMEM;
+	return addr;
+}
+
+static long ioctl_id_alloc(struct ctx *ctx, void __user *arg)
+{
+	struct usdpaa_ioctl_id_alloc i;
+	const struct alloc_backend *backend;
+	struct active_resource *res;
+	int ret = copy_from_user(&i, arg, sizeof(i));
+	if (ret)
+		return ret;
+	if ((i.id_type >= usdpaa_id_max) || !i.num)
+		return -EINVAL;
+	backend = &alloc_backends[i.id_type];
+	/* Allocate the required resource type */
+	ret = backend->alloc(&i.base, i.num, i.align, i.partial);
+	if (ret < 0)
+		return ret;
+	i.num = ret;
+	/* Copy the result to user-space */
+	ret = copy_to_user(arg, &i, sizeof(i));
+	if (ret) {
+		backend->release(i.base, i.num);
+		return ret;
+	}
+	/* Assign the allocated range to the FD accounting */
+	res = kmalloc(sizeof(*res), GFP_KERNEL);
+	if (!res) {
+		backend->release(i.base, i.num);
+		return -ENOMEM;
+	}
+	spin_lock(&ctx->lock);
+	res->id = i.base;
+	res->num = i.num;
+	res->refcount = 1;
+	list_add(&res->list, &ctx->resources[i.id_type]);
+	spin_unlock(&ctx->lock);
+	return 0;
+}
+
+static long ioctl_id_release(struct ctx *ctx, void __user *arg)
+{
+	struct usdpaa_ioctl_id_release i;
+	const struct alloc_backend *backend;
+	struct active_resource *tmp, *pos;
+
+	int ret = copy_from_user(&i, arg, sizeof(i));
+	if (ret)
+		return ret;
+	if ((i.id_type >= usdpaa_id_max) || !i.num)
+		return -EINVAL;
+	backend = &alloc_backends[i.id_type];
+	/* Pull the range out of the FD accounting - the range is valid iff this
+	 * succeeds. */
+	spin_lock(&ctx->lock);
+	list_for_each_entry_safe(pos, tmp, &ctx->resources[i.id_type], list) {
+		if (pos->id == i.base && pos->num == i.num) {
+			pos->refcount--;
+			if (pos->refcount) {
+				spin_unlock(&ctx->lock);
+				return 0; /* Still being used */
+			}
+			list_del(&pos->list);
+			kfree(pos);
+			spin_unlock(&ctx->lock);
+			goto found;
+		}
+	}
+	/* Failed to find the resource */
+	spin_unlock(&ctx->lock);
+	return -EINVAL;
+found:
+	/* Release the resource to the backend */
+	backend->release(i.base, i.num);
+	return 0;
+}
+
+static long ioctl_id_reserve(struct ctx *ctx, void __user *arg)
+{
+	struct usdpaa_ioctl_id_reserve i;
+	const struct alloc_backend *backend;
+	struct active_resource *tmp, *pos;
+
+	int ret = copy_from_user(&i, arg, sizeof(i));
+	if (ret)
+		return ret;
+	if ((i.id_type >= usdpaa_id_max) || !i.num)
+		return -EINVAL;
+	backend = &alloc_backends[i.id_type];
+	if (!backend->reserve)
+		return -EINVAL;
+	/* Pull the range out of the FD accounting - the range is valid iff this
+	 * succeeds. */
+	spin_lock(&ctx->lock);
+	list_for_each_entry_safe(pos, tmp, &ctx->resources[i.id_type], list) {
+		if (pos->id == i.base && pos->num == i.num) {
+			pos->refcount++;
+			spin_unlock(&ctx->lock);
+			return 0;
+		}
+	}
+
+	/* Failed to find the resource */
+	spin_unlock(&ctx->lock);
+
+	/* Reserve the resource in the backend */
+	ret = backend->reserve(i.base, i.num);
+	if (ret)
+		return ret;
+	/* Assign the reserved range to the FD accounting */
+	pos = kmalloc(sizeof(*pos), GFP_KERNEL);
+	if (!pos) {
+		backend->release(i.base, i.num);
+		return -ENOMEM;
+	}
+	spin_lock(&ctx->lock);
+	pos->id = i.base;
+	pos->num = i.num;
+	pos->refcount = 1;
+	list_add(&pos->list, &ctx->resources[i.id_type]);
+	spin_unlock(&ctx->lock);
+	return 0;
+}
+
+static long ioctl_dma_map(struct file *fp, struct ctx *ctx,
+			  struct usdpaa_ioctl_dma_map *i)
+{
+	struct mem_fragment *frag;
+	struct mem_mapping *map, *tmp;
+	u64 search_size;
+	int ret = 0;
+	if (i->len && !is_good_size(i->len))
+		return -EINVAL;
+	map = kmalloc(sizeof(*map), GFP_KERNEL);
+	if (!map)
+		return -ENOMEM;
+	spin_lock(&mem_lock);
+	if (i->flags & USDPAA_DMA_FLAG_SHARE) {
+		list_for_each_entry(frag, &mem_list, list) {
+			if (frag->refs && (frag->flags &
+					   USDPAA_DMA_FLAG_SHARE) &&
+					!strncmp(i->name, frag->name,
+						 USDPAA_DMA_NAME_MAX)) {
+				/* Matching entry */
+				if ((i->flags & USDPAA_DMA_FLAG_CREATE) &&
+				    !(i->flags & USDPAA_DMA_FLAG_LAZY)) {
+					ret = -EBUSY;
+					goto out;
+				}
+				list_for_each_entry(tmp, &ctx->maps, list)
+					if (tmp->frag == frag) {
+						ret = -EBUSY;
+						goto out;
+					}
+				i->has_locking = frag->has_locking;
+				i->did_create = 0;
+				i->len = frag->len;
+				goto do_map;
+			}
+		}
+		/* No matching entry */
+		if (!(i->flags & USDPAA_DMA_FLAG_CREATE)) {
+			ret = -ENOMEM;
+			goto out;
+		}
+	}
+	/* New fragment required, size must be provided. */
+	if (!i->len) {
+		ret = -EINVAL;
+		goto out;
+	}
+	/* We search for the required size and if that fails, for the next
+	 * biggest size, etc. */
+	for (search_size = i->len; search_size <= phys_size;
+	     search_size <<= 2) {
+		list_for_each_entry(frag, &mem_list, list) {
+			if (!frag->refs && (frag->len == search_size)) {
+				while (frag->len > i->len) {
+					frag = split_frag(frag);
+					if (!frag) {
+						ret = -ENOMEM;
+						goto out;
+					}
+				}
+				frag->flags = i->flags;
+				strncpy(frag->name, i->name,
+					USDPAA_DMA_NAME_MAX);
+				frag->has_locking = i->has_locking;
+				init_waitqueue_head(&frag->wq);
+				frag->owner = NULL;
+				i->did_create = 1;
+				goto do_map;
+			}
+		}
+	}
+	ret = -ENOMEM;
+	goto out;
+
+do_map:
+	map->frag = frag;
+	frag->refs++;
+	list_add(&map->list, &ctx->maps);
+	i->phys_addr = frag->base;
+
+out:
+	spin_unlock(&mem_lock);
+	if (!ret) {
+		unsigned long longret;
+		unsigned long populate;
+		down_write(&current->mm->mmap_sem);
+		longret = do_mmap_pgoff(fp, 0, map->frag->len, PROT_READ |
+			(i->flags & USDPAA_DMA_FLAG_RDONLY ? 0 : PROT_WRITE),
+			MAP_SHARED, map->frag->pfn_base, &populate);
+		up_write(&current->mm->mmap_sem);
+
+		if (populate)
+			mm_populate(longret, populate);
+
+		if (longret & ~PAGE_MASK)
+			ret = (int)longret;
+		else
+			i->ptr = (void *)longret;
+	} else
+		kfree(map);
+	return ret;
+}
+
+static long ioctl_dma_unmap(struct ctx *ctx, void __user *arg)
+{
+	struct mem_mapping *map;
+	struct vm_area_struct *vma;
+	int ret;
+
+	down_write(&current->mm->mmap_sem);
+	vma = find_vma(current->mm, (unsigned long)arg);
+	if (!vma || (vma->vm_start > (unsigned long)arg)) {
+		up_write(&current->mm->mmap_sem);
+		return -EFAULT;
+	}
+	spin_lock(&mem_lock);
+	list_for_each_entry(map, &ctx->maps, list) {
+		if (map->frag->pfn_base == vma->vm_pgoff) {
+			/* Drop the map lock if we hold it */
+			if (map->frag->has_locking &&
+					(map->frag->owner == map)) {
+				map->frag->owner = NULL;
+				wake_up(&map->frag->wq);
+			}
+			goto map_match;
+		}
+	}
+	map = NULL;
+map_match:
+	spin_unlock(&mem_lock);
+	if (map) {
+		unsigned long base = vma->vm_start;
+		size_t sz = vma->vm_end - vma->vm_start;
+		do_munmap(current->mm, base, sz);
+		ret = 0;
+	} else
+		ret = -EFAULT;
+	up_write(&current->mm->mmap_sem);
+	return ret;
+}
+
+static long ioctl_dma_stats(struct ctx *ctx, void __user *arg)
+{
+	struct mem_fragment *frag;
+	struct usdpaa_ioctl_dma_used result;
+
+	result.free_bytes = 0;
+	result.total_bytes = phys_size;
+
+	list_for_each_entry(frag, &mem_list, list) {
+		result.free_bytes += frag->len;
+	}
+
+	return copy_to_user(arg, &result, sizeof(result)); }
+
+static int test_lock(struct mem_mapping *map)
+{
+	int ret = 0;
+	spin_lock(&mem_lock);
+	if (!map->frag->owner) {
+		map->frag->owner = map;
+		ret = 1;
+	}
+	spin_unlock(&mem_lock);
+	return ret;
+}
+
+static long ioctl_dma_lock(struct ctx *ctx, void __user *arg)
+{
+	struct mem_mapping *map;
+	struct vm_area_struct *vma;
+
+	down_read(&current->mm->mmap_sem);
+	vma = find_vma(current->mm, (unsigned long)arg);
+	if (!vma || (vma->vm_start > (unsigned long)arg)) {
+		up_read(&current->mm->mmap_sem);
+		return -EFAULT;
+	}
+	spin_lock(&mem_lock);
+	list_for_each_entry(map, &ctx->maps, list) {
+		if (map->frag->pfn_base == vma->vm_pgoff)
+			goto map_match;
+	}
+	map = NULL;
+map_match:
+	spin_unlock(&mem_lock);
+	up_read(&current->mm->mmap_sem);
+
+	if (!map->frag->has_locking)
+		return -ENODEV;
+	return wait_event_interruptible(map->frag->wq, test_lock(map));
+}
+
+static long ioctl_dma_unlock(struct ctx *ctx, void __user *arg)
+{
+	struct mem_mapping *map;
+	struct vm_area_struct *vma;
+	int ret;
+
+	down_read(&current->mm->mmap_sem);
+	vma = find_vma(current->mm, (unsigned long)arg);
+	if (!vma || (vma->vm_start > (unsigned long)arg))
+		ret = -EFAULT;
+	else {
+		spin_lock(&mem_lock);
+		list_for_each_entry(map, &ctx->maps, list) {
+			if (map->frag->pfn_base == vma->vm_pgoff) {
+				if (!map->frag->has_locking)
+					ret = -ENODEV;
+				else if (map->frag->owner == map) {
+					map->frag->owner = NULL;
+					wake_up(&map->frag->wq);
+					ret = 0;
+				} else
+					ret = -EBUSY;
+				goto map_match;
+			}
+		}
+		ret = -EINVAL;
+map_match:
+		spin_unlock(&mem_lock);
+	}
+	up_read(&current->mm->mmap_sem);
+	return ret;
+}
+
+static int portal_mmap(struct file *fp, struct resource *res, void **ptr)
+{
+	unsigned long longret = 0;
+	unsigned long populate;
+
+	down_write(&current->mm->mmap_sem);
+	do {
+		longret = do_mmap_pgoff(fp, 0, resource_size(res),
+					PROT_READ | PROT_WRITE, MAP_SHARED,
+					res->start >> PAGE_SHIFT, &populate);
+	} while (longret == 0);
+	up_write(&current->mm->mmap_sem);
+
+	if (populate)
+		mm_populate(longret, populate);
+
+	if (longret & ~PAGE_MASK)
+		return (int)longret;
+
+	*ptr = (void *) longret;
+	return 0;
+}
+
+static void portal_munmap(struct resource *res, void  *ptr)
+{
+	down_write(&current->mm->mmap_sem);
+	do_munmap(current->mm, (unsigned long)ptr, resource_size(res));
+	up_write(&current->mm->mmap_sem);
+}
+
+static long ioctl_portal_map(struct file *fp, struct ctx *ctx,
+			     struct usdpaa_ioctl_portal_map  *arg)
+{
+	struct portal_mapping *mapping = kmalloc(sizeof(*mapping), GFP_KERNEL);
+	int ret;
+
+	if (!mapping)
+		return -ENOMEM;
+	memcpy(&mapping->user, arg, sizeof(mapping->user));
+	if (mapping->user.type == usdpaa_portal_qman) {
+		mapping->qportal = qm_get_unused_portal();
+		if (!mapping->qportal) {
+			ret = -ENODEV;
+			goto err_get_portal;
+		}
+		mapping->phys = &mapping->qportal->addr_phys[0];
+		mapping->user.channel = mapping->qportal->public_cfg.channel;
+		mapping->user.pools = mapping->qportal->public_cfg.pools;
+		mapping->user.irq = mapping->qportal->public_cfg.irq;
+	} else if (mapping->user.type == usdpaa_portal_bman) {
+		mapping->bportal = bm_get_unused_portal();
+		if (!mapping->bportal) {
+			ret = -ENODEV;
+			goto err_get_portal;
+		}
+		mapping->phys = &mapping->bportal->addr_phys[0];
+		mapping->user.irq = mapping->bportal->public_cfg.irq;
+	} else {
+		ret = -EINVAL;
+		goto err_copy_from_user;
+	}
+	/* Need to put pcfg in ctx's list before the mmaps because the mmap
+	 * handlers look it up. */
+	spin_lock(&mem_lock);
+	list_add(&mapping->list, &ctx->portals);
+	spin_unlock(&mem_lock);
+	ret = portal_mmap(fp, &mapping->phys[DPA_PORTAL_CE],
+			  &mapping->user.addr.cena);
+	if (ret)
+		goto err_mmap_cena;
+	ret = portal_mmap(fp, &mapping->phys[DPA_PORTAL_CI],
+			  &mapping->user.addr.cinh);
+	if (ret)
+		goto err_mmap_cinh;
+	memcpy(arg, &mapping->user, sizeof(mapping->user));
+	return ret;
+
+err_mmap_cinh:
+	portal_munmap(&mapping->phys[DPA_PORTAL_CE], mapping->user.addr.cena);
+err_mmap_cena:
+	if ((mapping->user.type == usdpaa_portal_qman) && mapping->qportal)
+		qm_put_unused_portal(mapping->qportal);
+	else if ((mapping->user.type == usdpaa_portal_bman) && mapping->bportal)
+		bm_put_unused_portal(mapping->bportal);
+	spin_lock(&mem_lock);
+	list_del(&mapping->list);
+	spin_unlock(&mem_lock);
+err_get_portal:
+err_copy_from_user:
+	kfree(mapping);
+	return ret;
+}
+
+static bool check_portal_channel(void *ctx, u32 channel)
+{
+	u32 portal_channel = *(u32 *)ctx;
+	if (portal_channel == channel) {
+		/* This FQs destination is a portal
+		   we're cleaning, send a retire */
+		return true;
+	}
+	return false;
+}
+
+static long ioctl_portal_unmap(struct ctx *ctx, struct usdpaa_portal_map *i)
+{
+	struct portal_mapping *mapping;
+	struct vm_area_struct *vma;
+	unsigned long pfn;
+	u32 channel;
+
+	/* Get the PFN corresponding to one of the virt addresses */
+	down_read(&current->mm->mmap_sem);
+	vma = find_vma(current->mm, (unsigned long)i->cinh);
+	if (!vma || (vma->vm_start > (unsigned long)i->cinh)) {
+		up_read(&current->mm->mmap_sem);
+		return -EFAULT;
+	}
+	pfn = vma->vm_pgoff;
+	up_read(&current->mm->mmap_sem);
+
+	/* Find the corresponding portal */
+	spin_lock(&mem_lock);
+	list_for_each_entry(mapping, &ctx->portals, list) {
+		if (pfn == (mapping->phys[DPA_PORTAL_CI].start >> PAGE_SHIFT))
+			goto found;
+	}
+	mapping = NULL;
+found:
+	if (mapping)
+		list_del(&mapping->list);
+	spin_unlock(&mem_lock);
+	if (!mapping)
+		return -ENODEV;
+	portal_munmap(&mapping->phys[DPA_PORTAL_CI], mapping->user.addr.cinh);
+	portal_munmap(&mapping->phys[DPA_PORTAL_CE], mapping->user.addr.cena);
+	if (mapping->user.type == usdpaa_portal_qman) {
+		init_qm_portal(mapping->qportal,
+				       &mapping->qman_portal_low);
+
+		/* Tear down any FQs this portal is referencing */
+		channel = mapping->qportal->public_cfg.channel;
+		qm_check_and_destroy_fqs(&mapping->qman_portal_low, &channel,
+					 check_portal_channel);
+		qm_put_unused_portal(mapping->qportal);
+	} else if (mapping->user.type == usdpaa_portal_bman) {
+		init_bm_portal(mapping->bportal,
+			       &mapping->bman_portal_low);
+		bm_put_unused_portal(mapping->bportal);
+	}
+	kfree(mapping);
+	return 0;
+}
+
+static long usdpaa_ioctl(struct file *fp, unsigned int cmd, unsigned long arg)
+{
+	struct ctx *ctx = fp->private_data;
+	void __user *a = (void __user *)arg;
+	switch (cmd) {
+	case USDPAA_IOCTL_ID_ALLOC:
+		return ioctl_id_alloc(ctx, a);
+	case USDPAA_IOCTL_ID_RELEASE:
+		return ioctl_id_release(ctx, a);
+	case USDPAA_IOCTL_ID_RESERVE:
+		return ioctl_id_reserve(ctx, a);
+	case USDPAA_IOCTL_DMA_MAP:
+	{
+		struct usdpaa_ioctl_dma_map input;
+		int ret;
+		if (copy_from_user(&input, a, sizeof(input)))
+			return -EFAULT;
+		ret = ioctl_dma_map(fp, ctx, &input);
+		if (copy_to_user(a, &input, sizeof(input)))
+			return -EFAULT;
+		return ret;
+	}
+	case USDPAA_IOCTL_DMA_UNMAP:
+		return ioctl_dma_unmap(ctx, a);
+	case USDPAA_IOCTL_DMA_LOCK:
+		return ioctl_dma_lock(ctx, a);
+	case USDPAA_IOCTL_DMA_UNLOCK:
+		return ioctl_dma_unlock(ctx, a);
+	case USDPAA_IOCTL_PORTAL_MAP:
+	{
+		struct usdpaa_ioctl_portal_map input;
+		int ret;
+		if (copy_from_user(&input, a, sizeof(input)))
+			return -EFAULT;
+		ret =  ioctl_portal_map(fp, ctx, &input);
+		if (copy_to_user(a, &input, sizeof(input)))
+			return -EFAULT;
+		return ret;
+	}
+	case USDPAA_IOCTL_PORTAL_UNMAP:
+	{
+		struct usdpaa_portal_map input;
+		if (copy_from_user(&input, a, sizeof(input)))
+			return -EFAULT;
+		return ioctl_portal_unmap(ctx, &input);
+	}
+	case USDPAA_IOCTL_DMA_USED:
+		return ioctl_dma_stats(ctx, a);
+	}
+	return -EINVAL;
+}
+
+
+
+static long usdpaa_ioctl_compat(struct file *fp, unsigned int cmd,
+				unsigned long arg)
+{
+	struct ctx *ctx = fp->private_data;
+	void __user *a = (void __user *)arg;
+	switch (cmd) {
+#ifdef CONFIG_COMPAT
+	case USDPAA_IOCTL_DMA_MAP_COMPAT:
+	{
+		int ret;
+		struct usdpaa_ioctl_dma_map_compat input;
+		struct usdpaa_ioctl_dma_map converted;
+
+		if (copy_from_user(&input, a, sizeof(input)))
+			return -EFAULT;
+
+		converted.ptr = compat_ptr(input.ptr);
+		converted.phys_addr = input.phys_addr;
+		converted.len = input.len;
+		converted.flags = input.flags;
+		strncpy(converted.name, input.name, USDPAA_DMA_NAME_MAX);
+		converted.has_locking = input.has_locking;
+		converted.did_create = input.did_create;
+
+		ret = ioctl_dma_map(fp, ctx, &converted);
+		input.ptr = ptr_to_compat(converted.ptr);
+		input.phys_addr = converted.phys_addr;
+		strncpy(input.name, converted.name, USDPAA_DMA_NAME_MAX);
+		input.has_locking = converted.has_locking;
+		input.did_create = converted.did_create;
+		if (copy_to_user(a, &input, sizeof(input)))
+			return -EFAULT;
+		return ret;
+	}
+	case USDPAA_IOCTL_PORTAL_MAP_COMPAT:
+	{
+		int ret;
+		struct compat_usdpaa_ioctl_portal_map input;
+		struct usdpaa_ioctl_portal_map converted;
+		if (copy_from_user(&input, a, sizeof(input)))
+			return -EFAULT;
+		converted.type = input.type;
+		ret = ioctl_portal_map(fp, ctx, &converted);
+		input.addr.cinh = ptr_to_compat(converted.addr.cinh);
+		input.addr.cena = ptr_to_compat(converted.addr.cena);
+		input.channel = converted.channel;
+		input.pools = converted.pools;
+		input.irq = converted.irq;
+		if (copy_to_user(a, &input, sizeof(input)))
+			return -EFAULT;
+		return ret;
+	}
+	case USDPAA_IOCTL_PORTAL_UNMAP_COMPAT:
+	{
+		struct usdpaa_portal_map_compat input;
+		struct usdpaa_portal_map converted;
+
+		if (copy_from_user(&input, a, sizeof(input)))
+			return -EFAULT;
+		converted.cinh = compat_ptr(input.cinh);
+		converted.cena = compat_ptr(input.cena);
+		return ioctl_portal_unmap(ctx, &converted);
+	}
+#endif
+	default:
+		return usdpaa_ioctl(fp, cmd, arg);
+	}
+	return -EINVAL;
+}
+
+static const struct file_operations usdpaa_fops = {
+	.open		   = usdpaa_open,
+	.release	   = usdpaa_release,
+	.mmap		   = usdpaa_mmap,
+	.get_unmapped_area = usdpaa_get_unmapped_area,
+	.unlocked_ioctl	   = usdpaa_ioctl,
+	.compat_ioctl	   = usdpaa_ioctl_compat
+};
+
+static struct miscdevice usdpaa_miscdev = {
+	.name = "fsl-usdpaa",
+	.fops = &usdpaa_fops,
+	.minor = MISC_DYNAMIC_MINOR,
+};
+
+/* Early-boot memory allocation. The boot-arg "usdpaa_mem=<x>" is used to
+ * indicate how much memory (if any) to allocate during early boot. If the
+ * format "usdpaa_mem=<x>,<y>" is used, then <y> will be interpreted as the
+ * number of TLB1 entries to reserve (default is 1). If there are more mappings
+ * than there are TLB1 entries, fault-handling will occur. */
+static __init int usdpaa_mem(char *arg)
+{
+	phys_size = memparse(arg, &arg);
+	num_tlb = 1;
+	if (*arg == ',') {
+		unsigned long ul;
+		int err = kstrtoul(arg + 1, 0, &ul);
+		if (err < 0) {
+			num_tlb = 1;
+			pr_warn("ERROR, usdpaa_mem arg is invalid\n");
+		} else
+			num_tlb = (unsigned int)ul;
+	}
+	return 0;
+}
+early_param("usdpaa_mem", usdpaa_mem);
+
+__init void fsl_usdpaa_init_early(void)
+{
+	if (!phys_size) {
+		pr_info("No USDPAA memory, no 'usdpaa_mem' bootarg\n");
+		return;
+	}
+	if (!is_good_size(phys_size)) {
+		pr_err("'usdpaa_mem' bootarg must be 4096*4^x\n");
+		phys_size = 0;
+		return;
+	}
+	phys_start = memblock_alloc(phys_size, phys_size);
+	if (!phys_start) {
+		pr_err("Failed to reserve USDPAA region (sz:%llx)\n",
+		       phys_size);
+		return;
+	}
+	pfn_start = phys_start >> PAGE_SHIFT;
+	pfn_size = phys_size >> PAGE_SHIFT;
+	first_tlb = current_tlb = tlbcam_index;
+	tlbcam_index += num_tlb;
+	pr_info("USDPAA region at %llx:%llx(%lx:%lx), %d TLB1 entries)\n",
+		phys_start, phys_size, pfn_start, pfn_size, num_tlb);
+}
+
+static int __init usdpaa_init(void)
+{
+	struct mem_fragment *frag;
+	int ret;
+
+	pr_info("Freescale USDPAA process driver\n");
+	if (!phys_start) {
+		pr_warn("fsl-usdpaa: no region found\n");
+		return 0;
+	}
+	frag = kmalloc(sizeof(*frag), GFP_KERNEL);
+	if (!frag) {
+		pr_err("Failed to setup USDPAA memory accounting\n");
+		return -ENOMEM;
+	}
+	frag->base = phys_start;
+	frag->len = phys_size;
+	frag->pfn_base = pfn_start;
+	frag->pfn_len = pfn_size;
+	frag->refs = 0;
+	init_waitqueue_head(&frag->wq);
+	frag->owner = NULL;
+	list_add(&frag->list, &mem_list);
+	ret = misc_register(&usdpaa_miscdev);
+	if (ret)
+		pr_err("fsl-usdpaa: failed to register misc device\n");
+	return ret;
+}
+
+static void __exit usdpaa_exit(void)
+{
+	misc_deregister(&usdpaa_miscdev);
+}
+
+module_init(usdpaa_init);
+module_exit(usdpaa_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Freescale Semiconductor");
+MODULE_DESCRIPTION("Freescale USDPAA process driver");
diff --git a/drivers/staging/fsl_qbman/fsl_usdpaa_irq.c b/drivers/staging/fsl_qbman/fsl_usdpaa_irq.c
new file mode 100644
index 0000000..faf3e5d
--- /dev/null
+++ b/drivers/staging/fsl_qbman/fsl_usdpaa_irq.c
@@ -0,0 +1,194 @@
+/* Copyright (c) 2013 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *	 notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *	 notice, this list of conditions and the following disclaimer in the
+ *	 documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *	 names of its contributors may be used to endorse or promote products
+ *	 derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/* define a device that allows USPDAA processes to open a file
+   decriptor and specify which IRQ it wants to montior using an ioctl()
+   When an IRQ is received, the device becomes readable so that a process
+   can use read() or select() type calls to monitor for IRQs */
+
+#include <linux/miscdevice.h>
+#include <linux/fs.h>
+#include <linux/cdev.h>
+#include <linux/slab.h>
+#include <linux/interrupt.h>
+#include <linux/poll.h>
+#include <linux/uaccess.h>
+#include <linux/fsl_usdpaa.h>
+#include <linux/module.h>
+
+struct usdpaa_irq_ctx {
+	int irq_set; /* Set to true once the irq is set via ioctl */
+	unsigned int irq_num;
+	unsigned int last_irq_count; /* Last value returned from read */
+	unsigned int irq_count; /* Number of irqs since last read */
+	wait_queue_head_t wait_queue; /* Waiting processes */
+	spinlock_t lock;
+};
+
+
+static int usdpaa_irq_open(struct inode *inode, struct file *filp)
+{
+	struct usdpaa_irq_ctx *ctx = kmalloc(sizeof(struct usdpaa_irq_ctx),
+					     GFP_KERNEL);
+	if (!ctx)
+		return -ENOMEM;
+
+	ctx->irq_set = 0;
+	ctx->irq_count = 0;
+	ctx->last_irq_count = 0;
+	init_waitqueue_head(&ctx->wait_queue);
+	spin_lock_init(&ctx->lock);
+	filp->private_data = ctx;
+	return 0;
+}
+
+static int usdpaa_irq_release(struct inode *inode, struct file *filp)
+{
+	struct usdpaa_irq_ctx *ctx = filp->private_data;
+	if (ctx->irq_set)
+		free_irq(ctx->irq_num, ctx);
+	kfree(filp->private_data);
+	return 0;
+}
+
+
+irqreturn_t usdpaa_irq_handler(int irq, void *_ctx)
+{
+	struct usdpaa_irq_ctx *ctx = _ctx;
+	spin_lock(&ctx->lock);
+	++ctx->irq_count;
+	wake_up_all(&ctx->wait_queue);
+	spin_unlock(&ctx->lock);
+	return IRQ_HANDLED;
+}
+
+static long usdpaa_irq_ioctl(struct file *fp, unsigned int cmd,
+			     unsigned long arg)
+{
+	int ret;
+	struct usdpaa_irq_ctx *ctx = fp->private_data;
+
+	if (cmd != USDPAA_IOCTL_PORTAL_IRQ_MAP)
+		return -EINVAL;
+
+	/* Lookup IRQ number for portal */
+	if (ctx->irq_set) {
+		free_irq(ctx->irq_num, ctx);
+		ctx->irq_set = 0;
+	}
+	ret = request_irq(arg, usdpaa_irq_handler, 0, "usdpaa_irq", ctx);
+	if (ret) {
+		pr_err("request_irq for irq %lu failed, ret= %d\n", arg, ret);
+		return ret;
+	}
+	ctx->irq_set = 1;
+	ctx->irq_num = arg;
+	return 0;
+};
+
+static ssize_t usdpaa_irq_read(struct file *filp, char __user *buff,
+			       size_t count, loff_t *offp)
+{
+	struct usdpaa_irq_ctx *ctx = filp->private_data;
+	int ret;
+
+	if (!ctx->irq_set) {
+		pr_err("Reading USDPAA IRQ before it was set\n");
+		return -EINVAL;
+	}
+
+	ret = wait_event_interruptible(ctx->wait_queue,
+				       ctx->irq_count != ctx->last_irq_count);
+	if (ret == -ERESTARTSYS)
+		return ret;
+
+	if (copy_to_user(buff, &ctx->irq_count, sizeof(unsigned int)))
+		return -EFAULT;
+	ctx->last_irq_count = ctx->irq_count;
+	return sizeof(unsigned int);
+
+}
+
+static unsigned int usdpaa_irq_poll(struct file *filp, poll_table *wait)
+{
+	struct usdpaa_irq_ctx *ctx = filp->private_data;
+	unsigned int ret = 0;
+
+	if (!ctx->irq_set)
+		return POLLHUP;
+
+	poll_wait(filp, &ctx->wait_queue, wait);
+
+	spin_lock_irq(&ctx->lock);
+	if (ctx->irq_count != ctx->last_irq_count)
+		ret = POLLIN | POLLRDNORM;
+	spin_unlock_irq(&ctx->lock);
+	return ret;
+}
+
+static const struct file_operations usdpaa_irq_fops = {
+	.open		   = usdpaa_irq_open,
+	.release	   = usdpaa_irq_release,
+	.unlocked_ioctl	   = usdpaa_irq_ioctl,
+	.compat_ioctl	   = usdpaa_irq_ioctl,
+	.read              = usdpaa_irq_read,
+	.poll              = usdpaa_irq_poll
+};
+
+static struct miscdevice usdpaa_miscdev = {
+	.name = "fsl-usdpaa-irq",
+	.fops = &usdpaa_irq_fops,
+	.minor = MISC_DYNAMIC_MINOR,
+};
+
+static int __init usdpaa_irq_init(void)
+{
+	int ret;
+
+	pr_info("Freescale USDPAA process IRQ driver\n");
+	ret = misc_register(&usdpaa_miscdev);
+	if (ret)
+		pr_err("fsl-usdpaa-irq: failed to register misc device\n");
+	return ret;
+}
+
+static void __exit usdpaa_irq_exit(void)
+{
+	misc_deregister(&usdpaa_miscdev);
+}
+
+module_init(usdpaa_irq_init);
+module_exit(usdpaa_irq_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Freescale Semiconductor");
+MODULE_DESCRIPTION("Freescale USDPAA process IRQ driver");
diff --git a/drivers/staging/fsl_qbman/qman_driver.c b/drivers/staging/fsl_qbman/qman_driver.c
index 4e85d4f..c163ddd 100644
--- a/drivers/staging/fsl_qbman/qman_driver.c
+++ b/drivers/staging/fsl_qbman/qman_driver.c
@@ -60,6 +60,8 @@ static u32 fqd_size = (PAGE_SIZE << CONFIG_FSL_QMAN_FQD_SZ);
 static struct qman_portal *shared_portals[NR_CPUS];
 static int num_shared_portals;
 static int shared_portals_idx;
+static LIST_HEAD(unused_pcfgs);
+static DEFINE_SPINLOCK(unused_pcfgs_lock);
 
 /* A SDQCR mask comprising all the available/visible pool channels */
 static u32 pools_sdqcr;
@@ -83,7 +85,7 @@ static __init int fsl_fqid_range_init(struct device_node *node)
 		pr_err(STR_ERR_CELL, STR_FQID_RANGE, 2, node->full_name);
 		return -EINVAL;
 	}
-	qman_release_fqid_range(range[0], range[1]);
+	qman_seed_fqid_range(range[0], range[1]);
 	pr_info("Qman: FQID allocator includes range %d:%d\n",
 		range[0], range[1]);
 	return 0;
@@ -120,7 +122,7 @@ static __init int fsl_pool_channel_range_init(struct device_node *node)
 		pr_err(STR_ERR_CELL, STR_POOL_CHAN_RANGE, 1, node->full_name);
 		return -EINVAL;
 	}
-	qman_release_pool_range(chanid[0], chanid[1]);
+	qman_seed_pool_range(chanid[0], chanid[1]);
 	pr_info("Qman: pool channel allocator includes range %d:%d\n",
 		chanid[0], chanid[1]);
 	return 0;
@@ -140,7 +142,7 @@ static __init int fsl_cgrid_range_init(struct device_node *node)
 		pr_err(STR_ERR_CELL, STR_CGRID_RANGE, 2, node->full_name);
 		return -EINVAL;
 	}
-	qman_release_cgrid_range(range[0], range[1]);
+	qman_seed_cgrid_range(range[0], range[1]);
 	pr_info("Qman: CGRID allocator includes range %d:%d\n",
 		range[0], range[1]);
 	for (cgr.cgrid = 0; cgr.cgrid < __CGR_NUM; cgr.cgrid++) {
@@ -182,9 +184,9 @@ static __init int fsl_ceetm_init(struct device_node *node)
 	}
 
 	if (dcp_portal == qm_dc_portal_fman0)
-		qman_release_ceetm0_lfqid_range(range[0], range[1]);
+		qman_seed_ceetm0_lfqid_range(range[0], range[1]);
 	if (dcp_portal == qm_dc_portal_fman1)
-		qman_release_ceetm1_lfqid_range(range[0], range[1]);
+		qman_seed_ceetm1_lfqid_range(range[0], range[1]);
 	pr_debug("Qman: The lfqid allocator of CEETM %d includes range"
 			" 0x%x:0x%x\n", dcp_portal, range[0], range[1]);
 
@@ -267,9 +269,9 @@ static __init int fsl_ceetm_init(struct device_node *node)
 	}
 
 	if (dcp_portal == qm_dc_portal_fman0)
-		qman_release_ceetm0_channel_range(range[0], range[1]);
+		qman_seed_ceetm0_channel_range(range[0], range[1]);
 	if (dcp_portal == qm_dc_portal_fman1)
-		qman_release_ceetm1_channel_range(range[0], range[1]);
+		qman_seed_ceetm1_channel_range(range[0], range[1]);
 	pr_debug("Qman: The channel allocator of CEETM %d includes"
 			" range %d:%d\n", dcp_portal, range[0], range[1]);
 
@@ -439,14 +441,6 @@ err:
 	return NULL;
 }
 
-/* Destroy a previously-parsed portal config. */
-static void destroy_pcfg(struct qm_portal_config *pcfg)
-{
-	iounmap(pcfg->addr_virt[DPA_PORTAL_CI]);
-	iounmap(pcfg->addr_virt[DPA_PORTAL_CE]);
-	kfree(pcfg);
-}
-
 static struct qm_portal_config *get_pcfg(struct list_head *list)
 {
 	struct qm_portal_config *pcfg;
@@ -530,45 +524,12 @@ _iommu_domain_free:
 	iommu_domain_free(pcfg->iommu_domain);
 }
 
-/* UIO handling callbacks */
-#define QMAN_UIO_PREAMBLE() \
-	struct qm_portal_config *pcfg = \
-		container_of(__p, struct qm_portal_config, list)
-static int qman_uio_cb_init(const struct list_head *__p, struct uio_info *info)
-{
-	QMAN_UIO_PREAMBLE();
-	/* big enough for "qman-uio-xx" */
-	char *name = kzalloc(16, GFP_KERNEL);
-	if (!name)
-		return -ENOMEM;
-	sprintf(name, "qman-uio-%x", pcfg->public_cfg.index);
-	info->name = name;
-	info->mem[DPA_PORTAL_CE].name = "cena";
-	info->mem[DPA_PORTAL_CE].addr = pcfg->addr_phys[DPA_PORTAL_CE].start;
-	info->mem[DPA_PORTAL_CE].size =
-		resource_size(&pcfg->addr_phys[DPA_PORTAL_CE]);
-	info->mem[DPA_PORTAL_CE].memtype = UIO_MEM_PHYS;
-	info->mem[DPA_PORTAL_CI].name = "cinh";
-	info->mem[DPA_PORTAL_CI].addr = pcfg->addr_phys[DPA_PORTAL_CI].start;
-	info->mem[DPA_PORTAL_CI].size =
-		resource_size(&pcfg->addr_phys[DPA_PORTAL_CI]);
-	info->mem[DPA_PORTAL_CI].memtype = UIO_MEM_PHYS;
-	info->irq = pcfg->public_cfg.irq;
-	return 0;
-}
-static void qman_uio_cb_destroy(const struct list_head *__p,
-				struct uio_info *info)
-{
-	QMAN_UIO_PREAMBLE();
-	kfree(info->name);
-	/* We own this struct but had passed it to the dpa_uio layer as a const
-	 * so that we don't accidentally meddle with it in the dpa_uio code.
-	 * Here it's passed back to us for final clean it up, so de-constify. */
-	destroy_pcfg((struct qm_portal_config *)pcfg);
-}
-static int qman_uio_cb_open(struct list_head *__p)
+struct qm_portal_config *qm_get_unused_portal(void)
 {
-	QMAN_UIO_PREAMBLE();
+	struct qm_portal_config *ret;
+	spin_lock(&unused_pcfgs_lock);
+	ret = get_pcfg(&unused_pcfgs);
+	spin_unlock(&unused_pcfgs_lock);
 	/* Bind stashing LIODNs to the CPU we are currently executing on, and
 	 * set the portal to use the stashing request queue corresonding to the
 	 * cpu as well. The user-space driver assumption is that the pthread has
@@ -577,26 +538,17 @@ static int qman_uio_cb_open(struct list_head *__p)
 	 * stashing will go to whatever cpu they happened to be running on when
 	 * opening the device file, and if that isn't the cpu they subsequently
 	 * bind to and do their polling on, tough. */
-	portal_set_cpu(pcfg, hard_smp_processor_id());
-	return 0;
+	if (ret)
+		portal_set_cpu(ret, hard_smp_processor_id());
+	return ret;
 }
-static void qman_uio_cb_interrupt(const struct list_head *__p)
+
+void qm_put_unused_portal(struct qm_portal_config *pcfg)
 {
-	QMAN_UIO_PREAMBLE();
-	/* This is the only manipulation of a portal register that isn't in the
-	 * regular kernel portal driver (_high.c/_low.h). It is also the only
-	 * time the kernel touches a register on a portal that is otherwise
-	 * being driven by a user-space driver. So rather than messing up
-	 * encapsulation for one trivial call, I am hard-coding the offset to
-	 * the inhibit register and writing it directly from here. */
-	out_be32(pcfg->addr_virt[DPA_PORTAL_CI] + 0xe0c, ~(u32)0);
+	spin_lock(&unused_pcfgs_lock);
+	list_add(&pcfg->list, &unused_pcfgs);
+	spin_unlock(&unused_pcfgs_lock);
 }
-static const struct dpa_uio_vtable qman_uio = {
-	.init_uio = qman_uio_cb_init,
-	.destroy = qman_uio_cb_destroy,
-	.on_open = qman_uio_cb_open,
-	.on_interrupt = qman_uio_cb_interrupt
-};
 
 static struct qman_portal *init_pcfg(struct qm_portal_config *pcfg)
 {
@@ -657,7 +609,6 @@ static __init int qman_init(void)
 	struct cpumask slave_cpus;
 	struct cpumask unshared_cpus = *cpu_none_mask;
 	struct cpumask shared_cpus = *cpu_none_mask;
-	LIST_HEAD(unused_pcfgs);
 	LIST_HEAD(unshared_pcfgs);
 	LIST_HEAD(shared_pcfgs);
 	struct device_node *dn;
@@ -776,19 +727,6 @@ static __init int qman_init(void)
 		for_each_cpu(cpu, &slave_cpus)
 			init_slave(cpu);
 	pr_info("Qman portals initialised\n");
-#ifdef CONFIG_FSL_DPA_UIO
-	/* Export any left over portals as UIO devices */
-	do {
-		pcfg = get_pcfg(&unused_pcfgs);
-		if (!pcfg)
-			break;
-		ret = dpa_uio_register(&pcfg->list, &qman_uio);
-		if (ret) {
-			pr_err("Failure registering QMan UIO portal\n");
-			destroy_pcfg(pcfg);
-		}
-	} while (1);
-#endif
 	/* Initialise FQID allocation ranges */
 	for_each_compatible_node(dn, NULL, "fsl,fqid-range") {
 		ret = fsl_fqid_range_init(dn);
diff --git a/drivers/staging/fsl_qbman/qman_high.c b/drivers/staging/fsl_qbman/qman_high.c
index ca1b62c..20a1be7 100644
--- a/drivers/staging/fsl_qbman/qman_high.c
+++ b/drivers/staging/fsl_qbman/qman_high.c
@@ -103,12 +103,16 @@ struct qman_portal {
 	struct list_head cgr_cbs;
 	/* list lock */
 	spinlock_t cgr_lock;
+
 	/* 2-element array. ccgrs[0] is mask, ccgrs[1] is snapshot. */
 	struct qman_ccgrs *ccgrs[QMAN_CEETM_MAX];
 	/* 256-element array, each is a linked-list of CCSCN handlers. */
 	struct list_head ccgr_cbs[QMAN_CEETM_MAX];
 	/* list lock */
 	spinlock_t ccgr_lock;
+
+	/* track if memory was allocated by the driver */
+	u8 alloced;
 };
 
 #ifdef CONFIG_FSL_DPA_PORTAL_SHARE
@@ -336,29 +340,35 @@ loop:
 		if (!msg)
 			return 0;
 	}
-	if ((msg->verb & QM_MR_VERB_TYPE_MASK) != QM_MR_VERB_FQRNI)
+	if ((msg->verb & QM_MR_VERB_TYPE_MASK) != QM_MR_VERB_FQRNI) {
 		/* We aren't draining anything but FQRNIs */
+		pr_err("QMan found verb 0x%x in MR\n", msg->verb);
 		return -1;
+	}
 	qm_mr_next(p);
 	qm_mr_cci_consume(p, 1);
 	goto loop;
 }
 
-struct qman_portal *qman_create_affine_portal(
+
+struct qman_portal *qman_create_portal(
+			struct qman_portal *portal,
 			const struct qm_portal_config *config,
 			const struct qman_cgrs *cgrs)
 {
-	struct qman_portal *portal = get_raw_affine_portal();
-	struct qm_portal *__p = &portal->p;
+	struct qm_portal *__p;
 	char buf[16];
 	int ret;
 	u32 isdr;
 
-	/* A criteria for calling this function (from qman_driver.c) is that
-	 * we're already affine to the cpu and won't schedule onto another cpu.
-	 * This means we can put_affine_portal() and yet continue to use
-	 * "portal", which in turn means aspects of this routine can sleep. */
-	put_affine_portal();
+	if (!portal) {
+		portal = kmalloc(sizeof(*portal), GFP_KERNEL);
+		portal->alloced = 1;
+	} else
+		portal->alloced = 0;
+
+	__p = &portal->p;
+
 	/* prep the low-level portal struct with the mapped addresses from the
 	 * config, everything that follows depends on it and "config" is more
 	 * for (de)reference... */
@@ -456,6 +466,7 @@ struct qman_portal *qman_create_affine_portal(
 		pr_err("irq_set_affinity() failed\n");
 		goto fail_affinity;
 	}
+
 	/* Need EQCR to be empty before continuing */
 	isdr ^= QM_PIRQ_EQCI;
 	qm_isr_disable_write(__p, isdr);
@@ -468,21 +479,20 @@ struct qman_portal *qman_create_affine_portal(
 	qm_isr_disable_write(__p, isdr);
 	if (qm_dqrr_current(__p) != NULL) {
 		pr_err("Qman DQRR unclean\n");
-		goto fail_dqrr_mr_empty;
+		qm_dqrr_cdc_consume_n(__p, 0xffff);
 	}
 	if (qm_mr_current(__p) != NULL) {
 		/* special handling, drain just in case it's a few FQRNIs */
 		if (drain_mr_fqrni(__p)) {
-			pr_err("Qman MR unclean\n");
+			const struct qm_mr_entry *e = qm_mr_current(__p);
+			pr_err("Qman MR unclean, MR VERB 0x%x, "
+			       "rc 0x%x\n, addr 0x%x",
+			       e->verb, e->ern.rc, e->ern.fd.addr_lo);
 			goto fail_dqrr_mr_empty;
 		}
 	}
 	/* Success */
 	portal->config = config;
-	spin_lock(&affine_mask_lock);
-	cpumask_set_cpu(config->public_cfg.cpu, &affine_mask);
-	affine_channels[config->public_cfg.cpu] = config->public_cfg.channel;
-	spin_unlock(&affine_mask_lock);
 	qm_isr_disable_write(__p, 0);
 	qm_isr_uninhibit(__p);
 	/* Write a sane SDQCR */
@@ -517,6 +527,32 @@ fail_eqcr:
 	return NULL;
 }
 
+
+
+struct qman_portal *qman_create_affine_portal(
+			const struct qm_portal_config *config,
+			const struct qman_cgrs *cgrs)
+{
+	struct qman_portal *res;
+	struct qman_portal *portal = get_raw_affine_portal();
+	/* A criteria for calling this function (from qman_driver.c) is that
+	 * we're already affine to the cpu and won't schedule onto another cpu.
+	 * This means we can put_affine_portal() and yet continue to use
+	 * "portal", which in turn means aspects of this routine can sleep. */
+	put_affine_portal();
+
+	res = qman_create_portal(portal, config, cgrs);
+	if (res) {
+		spin_lock(&affine_mask_lock);
+		cpumask_set_cpu(config->public_cfg.cpu, &affine_mask);
+		affine_channels[config->public_cfg.cpu] =
+			config->public_cfg.channel;
+		spin_unlock(&affine_mask_lock);
+	}
+	return res;
+}
+
+
 /* These checks are BUG_ON()s because the driver is already supposed to avoid
  * these cases. */
 struct qman_portal *qman_create_affine_slave(struct qman_portal *redirect)
@@ -540,23 +576,17 @@ struct qman_portal *qman_create_affine_slave(struct qman_portal *redirect)
 #endif
 }
 
-const struct qm_portal_config *qman_destroy_affine_portal(void)
+
+
+void qman_destroy_portal(struct qman_portal *qm)
 {
-	/* We don't want to redirect if we're a slave, use "raw" */
-	struct qman_portal *qm = get_raw_affine_portal();
 	const struct qm_portal_config *pcfg;
-	int cpu;
 	int i;
-#ifdef CONFIG_FSL_DPA_PORTAL_SHARE
-	if (qm->sharing_redirect) {
-		qm->sharing_redirect = NULL;
-		put_affine_portal();
-		return NULL;
-	}
-	qm->is_shared = 0;
-#endif
-	pcfg = qm->config;
-	cpu = pcfg->public_cfg.cpu;
+
+	/* Stop dequeues on the portal */
+	qm_dqrr_sdqcr_set(&qm->p, 0);
+
+
 	/* NB we do this to "quiesce" EQCR. If we add enqueue-completions or
 	 * something related to QM_PIRQ_EQCI, this may need fixing.
 	 * Also, due to the prefetching model used for CI updates in the enqueue
@@ -566,7 +596,10 @@ const struct qm_portal_config *qman_destroy_affine_portal(void)
 	 * began. */
 	qm_eqcr_cce_update(&qm->p);
 	qm_eqcr_cce_update(&qm->p);
+	pcfg = qm->config;
+
 	free_irq(pcfg->public_cfg.irq, qm);
+
 	kfree(qm->cgrs);
 	if (num_ceetms)
 		for (i = 0; i < num_ceetms; i++)
@@ -576,7 +609,34 @@ const struct qm_portal_config *qman_destroy_affine_portal(void)
 	qm_mr_finish(&qm->p);
 	qm_dqrr_finish(&qm->p);
 	qm_eqcr_finish(&qm->p);
+
+	platform_device_del(qm->pdev);
+	platform_device_put(qm->pdev);
+
 	qm->config = NULL;
+	if (qm->alloced)
+		kfree(qm);
+}
+
+const struct qm_portal_config *qman_destroy_affine_portal(void)
+{
+	/* We don't want to redirect if we're a slave, use "raw" */
+	struct qman_portal *qm = get_raw_affine_portal();
+	const struct qm_portal_config *pcfg;
+	int cpu;
+#ifdef CONFIG_FSL_DPA_PORTAL_SHARE
+	if (qm->sharing_redirect) {
+		qm->sharing_redirect = NULL;
+		put_affine_portal();
+		return NULL;
+	}
+	qm->is_shared = 0;
+#endif
+	pcfg = qm->config;
+	cpu = pcfg->public_cfg.cpu;
+
+	qman_destroy_portal(qm);
+
 	spin_lock(&affine_mask_lock);
 	cpumask_clear_cpu(cpu, &affine_mask);
 	spin_unlock(&affine_mask_lock);
@@ -1243,6 +1303,7 @@ EXPORT_SYMBOL(qman_create_fq);
 
 void qman_destroy_fq(struct qman_fq *fq, u32 flags __maybe_unused)
 {
+
 	/* We don't need to lock the FQ as it is a pre-condition that the FQ be
 	 * quiesced. Instead, run some checks. */
 	switch (fq->state) {
@@ -4429,3 +4490,16 @@ int qman_get_wpm(int *wpm_enable)
 	return qm_get_wpm(wpm_enable);
 }
 EXPORT_SYMBOL(qman_get_wpm);
+
+int qman_shutdown_fq(u32 fqid)
+{
+	struct qman_portal *p;
+	unsigned long irqflags __maybe_unused;
+	int ret;
+	p = get_affine_portal();
+	PORTAL_IRQ_LOCK(p, irqflags);
+	ret = qm_shutdown_fq(&p->p, fqid);
+	PORTAL_IRQ_UNLOCK(p, irqflags);
+	put_affine_portal();
+	return ret;
+}
diff --git a/drivers/staging/fsl_qbman/qman_low.h b/drivers/staging/fsl_qbman/qman_low.h
index d517af5..26641b6 100644
--- a/drivers/staging/fsl_qbman/qman_low.h
+++ b/drivers/staging/fsl_qbman/qman_low.h
@@ -36,36 +36,36 @@
 /***************************/
 
 /* Cache-inhibited register offsets */
-#define REG_EQCR_PI_CINH	0x0000
-#define REG_EQCR_CI_CINH	0x0004
-#define REG_EQCR_ITR		0x0008
-#define REG_DQRR_PI_CINH	0x0040
-#define REG_DQRR_CI_CINH	0x0044
-#define REG_DQRR_ITR		0x0048
-#define REG_DQRR_DCAP		0x0050
-#define REG_DQRR_SDQCR		0x0054
-#define REG_DQRR_VDQCR		0x0058
-#define REG_DQRR_PDQCR		0x005c
-#define REG_MR_PI_CINH		0x0080
-#define REG_MR_CI_CINH		0x0084
-#define REG_MR_ITR		0x0088
-#define REG_CFG			0x0100
-#define REG_ISR			0x0e00
-#define REG_ITPR		0x0e14
+#define QM_REG_EQCR_PI_CINH	0x0000
+#define QM_REG_EQCR_CI_CINH	0x0004
+#define QM_REG_EQCR_ITR		0x0008
+#define QM_REG_DQRR_PI_CINH	0x0040
+#define QM_REG_DQRR_CI_CINH	0x0044
+#define QM_REG_DQRR_ITR		0x0048
+#define QM_REG_DQRR_DCAP	0x0050
+#define QM_REG_DQRR_SDQCR	0x0054
+#define QM_REG_DQRR_VDQCR	0x0058
+#define QM_REG_DQRR_PDQCR	0x005c
+#define QM_REG_MR_PI_CINH	0x0080
+#define QM_REG_MR_CI_CINH	0x0084
+#define QM_REG_MR_ITR		0x0088
+#define QM_REG_CFG		0x0100
+#define QM_REG_ISR		0x0e00
+#define QM_REG_ITPR		0x0e14
 
 /* Cache-enabled register offsets */
-#define CL_EQCR			0x0000
-#define CL_DQRR			0x1000
-#define CL_MR			0x2000
-#define CL_EQCR_PI_CENA		0x3000
-#define CL_EQCR_CI_CENA		0x3100
-#define CL_DQRR_PI_CENA		0x3200
-#define CL_DQRR_CI_CENA		0x3300
-#define CL_MR_PI_CENA		0x3400
-#define CL_MR_CI_CENA		0x3500
-#define CL_CR			0x3800
-#define CL_RR0			0x3900
-#define CL_RR1			0x3940
+#define QM_CL_EQCR		0x0000
+#define QM_CL_DQRR		0x1000
+#define QM_CL_MR		0x2000
+#define QM_CL_EQCR_PI_CENA	0x3000
+#define QM_CL_EQCR_CI_CENA	0x3100
+#define QM_CL_DQRR_PI_CENA	0x3200
+#define QM_CL_DQRR_CI_CENA	0x3300
+#define QM_CL_MR_PI_CENA	0x3400
+#define QM_CL_MR_CI_CENA	0x3500
+#define QM_CL_CR		0x3800
+#define QM_CL_RR0		0x3900
+#define QM_CL_RR1		0x3940
 
 /* BTW, the drivers (and h/w programming model) already obtain the required
  * synchronisation for portal accesses via lwsync(), hwsync(), and
@@ -77,8 +77,8 @@
 /* Cache-inhibited register access. */
 #define __qm_in(qm, o)		__raw_readl((qm)->addr_ci + (o))
 #define __qm_out(qm, o, val)	__raw_writel((val), (qm)->addr_ci + (o))
-#define qm_in(reg)		__qm_in(&portal->addr, REG_##reg)
-#define qm_out(reg, val)	__qm_out(&portal->addr, REG_##reg, val)
+#define qm_in(reg)		__qm_in(&portal->addr, QM_REG_##reg)
+#define qm_out(reg, val)	__qm_out(&portal->addr, QM_REG_##reg, val)
 
 /* Cache-enabled (index) register access */
 #define __qm_cl_touch_ro(qm, o) dcbt_ro((qm)->addr_ce + (o))
@@ -91,11 +91,12 @@
 		dcbf(__tmpclout); \
 	} while (0)
 #define __qm_cl_invalidate(qm, o) dcbi((qm)->addr_ce + (o))
-#define qm_cl_touch_ro(reg)	__qm_cl_touch_ro(&portal->addr, CL_##reg##_CENA)
-#define qm_cl_touch_rw(reg)	__qm_cl_touch_rw(&portal->addr, CL_##reg##_CENA)
-#define qm_cl_in(reg)		__qm_cl_in(&portal->addr, CL_##reg##_CENA)
-#define qm_cl_out(reg, val)	__qm_cl_out(&portal->addr, CL_##reg##_CENA, val)
-#define qm_cl_invalidate(reg) __qm_cl_invalidate(&portal->addr, CL_##reg##_CENA)
+#define qm_cl_touch_ro(reg) __qm_cl_touch_ro(&portal->addr, QM_CL_##reg##_CENA)
+#define qm_cl_touch_rw(reg) __qm_cl_touch_rw(&portal->addr, QM_CL_##reg##_CENA)
+#define qm_cl_in(reg)	    __qm_cl_in(&portal->addr, QM_CL_##reg##_CENA)
+#define qm_cl_out(reg, val) __qm_cl_out(&portal->addr, QM_CL_##reg##_CENA, val)
+#define qm_cl_invalidate(reg)\
+	__qm_cl_invalidate(&portal->addr, QM_CL_##reg##_CENA)
 
 /* Cache-enabled ring access */
 #define qm_cl(base, idx)	((void *)base + ((idx) << 6))
@@ -103,7 +104,7 @@
 /* Cyclic helper for rings. FIXME: once we are able to do fine-grain perf
  * analysis, look at using the "extra" bit in the ring index registers to avoid
  * cyclic issues. */
-static inline u8 cyc_diff(u8 ringsize, u8 first, u8 last)
+static inline u8 qm_cyc_diff(u8 ringsize, u8 first, u8 last)
 {
 	/* 'first' is included, 'last' is excluded */
 	if (first <= last)
@@ -201,11 +202,11 @@ struct qm_mc {
 #ifdef CONFIG_FSL_DPA_CHECKING
 	enum {
 		/* Can be _mc_start()ed */
-		mc_idle,
+		qman_mc_idle,
 		/* Can be _mc_commit()ed or _mc_abort()ed */
-		mc_user,
+		qman_mc_user,
 		/* Can only be _mc_retry()ed */
-		mc_hw
+		qman_mc_hw
 	} state;
 #endif
 };
@@ -290,7 +291,7 @@ static inline int qm_eqcr_init(struct qm_portal *portal,
 	u32 cfg;
 	u8 pi;
 
-	eqcr->ring = portal->addr.addr_ce + CL_EQCR;
+	eqcr->ring = portal->addr.addr_ce + QM_CL_EQCR;
 	eqcr->ci = qm_in(EQCR_CI_CINH) & (QM_EQCR_SIZE - 1);
 	qm_cl_invalidate(EQCR_CI);
 	pi = qm_in(EQCR_PI_CINH) & (QM_EQCR_SIZE - 1);
@@ -298,7 +299,7 @@ static inline int qm_eqcr_init(struct qm_portal *portal,
 	eqcr->vbit = (qm_in(EQCR_PI_CINH) & QM_EQCR_SIZE) ?
 			QM_EQCR_VERB_VBIT : 0;
 	eqcr->available = QM_EQCR_SIZE - 1 -
-			cyc_diff(QM_EQCR_SIZE, eqcr->ci, pi);
+			qm_cyc_diff(QM_EQCR_SIZE, eqcr->ci, pi);
 	eqcr->ithresh = qm_in(EQCR_ITR);
 #ifdef CONFIG_FSL_DPA_CHECKING
 	eqcr->busy = 0;
@@ -332,6 +333,8 @@ static inline struct qm_eqcr_entry *qm_eqcr_start(struct qm_portal *portal)
 	DPA_ASSERT(!eqcr->busy);
 	if (!eqcr->available)
 		return NULL;
+
+
 #ifdef CONFIG_FSL_DPA_CHECKING
 	eqcr->busy = 1;
 #endif
@@ -434,7 +437,7 @@ static inline u8 qm_eqcr_cci_update(struct qm_portal *portal)
 	u8 diff, old_ci = eqcr->ci;
 	DPA_ASSERT(eqcr->cmode == qm_eqcr_cci);
 	eqcr->ci = qm_in(EQCR_CI_CINH) & (QM_EQCR_SIZE - 1);
-	diff = cyc_diff(QM_EQCR_SIZE, old_ci, eqcr->ci);
+	diff = qm_cyc_diff(QM_EQCR_SIZE, old_ci, eqcr->ci);
 	eqcr->available += diff;
 	return diff;
 }
@@ -453,7 +456,7 @@ static inline u8 qm_eqcr_cce_update(struct qm_portal *portal)
 	DPA_ASSERT(eqcr->cmode == qm_eqcr_cce);
 	eqcr->ci = qm_cl_in(EQCR_CI) & (QM_EQCR_SIZE - 1);
 	qm_cl_invalidate(EQCR_CI);
-	diff = cyc_diff(QM_EQCR_SIZE, old_ci, eqcr->ci);
+	diff = qm_cyc_diff(QM_EQCR_SIZE, old_ci, eqcr->ci);
 	eqcr->available += diff;
 	return diff;
 }
@@ -526,11 +529,11 @@ static inline int qm_dqrr_init(struct qm_portal *portal,
 	qm_out(DQRR_SDQCR, 0);
 	qm_out(DQRR_VDQCR, 0);
 	qm_out(DQRR_PDQCR, 0);
-	dqrr->ring = portal->addr.addr_ce + CL_DQRR;
+	dqrr->ring = portal->addr.addr_ce + QM_CL_DQRR;
 	dqrr->pi = qm_in(DQRR_PI_CINH) & (QM_DQRR_SIZE - 1);
 	dqrr->ci = qm_in(DQRR_CI_CINH) & (QM_DQRR_SIZE - 1);
 	dqrr->cursor = dqrr->ring + dqrr->ci;
-	dqrr->fill = cyc_diff(QM_DQRR_SIZE, dqrr->ci, dqrr->pi);
+	dqrr->fill = qm_cyc_diff(QM_DQRR_SIZE, dqrr->ci, dqrr->pi);
 	dqrr->vbit = (qm_in(DQRR_PI_CINH) & QM_DQRR_SIZE) ?
 			QM_DQRR_VERB_VBIT : 0;
 	dqrr->ithresh = qm_in(DQRR_ITR);
@@ -593,7 +596,7 @@ static inline u8 qm_dqrr_pci_update(struct qm_portal *portal)
 	u8 diff, old_pi = dqrr->pi;
 	DPA_ASSERT(dqrr->pmode == qm_dqrr_pci);
 	dqrr->pi = qm_in(DQRR_PI_CINH) & (QM_DQRR_SIZE - 1);
-	diff = cyc_diff(QM_DQRR_SIZE, old_pi, dqrr->pi);
+	diff = qm_cyc_diff(QM_DQRR_SIZE, old_pi, dqrr->pi);
 	dqrr->fill += diff;
 	return diff;
 }
@@ -612,7 +615,7 @@ static inline u8 qm_dqrr_pce_update(struct qm_portal *portal)
 	u8 diff, old_pi = dqrr->pi;
 	DPA_ASSERT(dqrr->pmode == qm_dqrr_pce);
 	dqrr->pi = qm_cl_in(DQRR_PI) & (QM_DQRR_SIZE - 1);
-	diff = cyc_diff(QM_DQRR_SIZE, old_pi, dqrr->pi);
+	diff = qm_cyc_diff(QM_DQRR_SIZE, old_pi, dqrr->pi);
 	dqrr->fill += diff;
 	return diff;
 }
@@ -845,7 +848,7 @@ static inline int qm_mr_init(struct qm_portal *portal, enum qm_mr_pmode pmode,
 		return -EINVAL;
 	}
 #endif
-	mr->ring = portal->addr.addr_ce + CL_MR;
+	mr->ring = portal->addr.addr_ce + QM_CL_MR;
 	mr->pi = qm_in(MR_PI_CINH) & (QM_MR_SIZE - 1);
 	mr->ci = qm_in(MR_CI_CINH) & (QM_MR_SIZE - 1);
 #ifdef CONFIG_FSL_QMAN_BUG_AND_FEATURE_REV1
@@ -855,7 +858,7 @@ static inline int qm_mr_init(struct qm_portal *portal, enum qm_mr_pmode pmode,
 	else
 #endif
 	mr->cursor = mr->ring + mr->ci;
-	mr->fill = cyc_diff(QM_MR_SIZE, mr->ci, mr->pi);
+	mr->fill = qm_cyc_diff(QM_MR_SIZE, mr->ci, mr->pi);
 	mr->vbit = (qm_in(MR_PI_CINH) & QM_MR_SIZE) ? QM_MR_VERB_VBIT : 0;
 	mr->ithresh = qm_in(MR_ITR);
 #ifdef CONFIG_FSL_DPA_CHECKING
@@ -907,7 +910,7 @@ static inline u8 qm_mr_pci_update(struct qm_portal *portal)
 	u8 diff, old_pi = mr->pi;
 	DPA_ASSERT(mr->pmode == qm_mr_pci);
 	mr->pi = qm_in(MR_PI_CINH);
-	diff = cyc_diff(QM_MR_SIZE, old_pi, mr->pi);
+	diff = qm_cyc_diff(QM_MR_SIZE, old_pi, mr->pi);
 	mr->fill += diff;
 	return diff;
 }
@@ -926,7 +929,7 @@ static inline u8 qm_mr_pce_update(struct qm_portal *portal)
 	u8 diff, old_pi = mr->pi;
 	DPA_ASSERT(mr->pmode == qm_mr_pce);
 	mr->pi = qm_cl_in(MR_PI) & (QM_MR_SIZE - 1);
-	diff = cyc_diff(QM_MR_SIZE, old_pi, mr->pi);
+	diff = qm_cyc_diff(QM_MR_SIZE, old_pi, mr->pi);
 	mr->fill += diff;
 	return diff;
 }
@@ -1013,13 +1016,13 @@ static inline void qm_mr_set_ithresh(struct qm_portal *portal, u8 ithresh)
 static inline int qm_mc_init(struct qm_portal *portal)
 {
 	register struct qm_mc *mc = &portal->mc;
-	mc->cr = portal->addr.addr_ce + CL_CR;
-	mc->rr = portal->addr.addr_ce + CL_RR0;
+	mc->cr = portal->addr.addr_ce + QM_CL_CR;
+	mc->rr = portal->addr.addr_ce + QM_CL_RR0;
 	mc->rridx = (__raw_readb(&mc->cr->__dont_write_directly__verb) &
 			QM_MCC_VERB_VBIT) ?  0 : 1;
 	mc->vbit = mc->rridx ? QM_MCC_VERB_VBIT : 0;
 #ifdef CONFIG_FSL_DPA_CHECKING
-	mc->state = mc_idle;
+	mc->state = qman_mc_idle;
 #endif
 	return 0;
 }
@@ -1027,9 +1030,9 @@ static inline int qm_mc_init(struct qm_portal *portal)
 static inline void qm_mc_finish(struct qm_portal *portal)
 {
 	__maybe_unused register struct qm_mc *mc = &portal->mc;
-	DPA_ASSERT(mc->state == mc_idle);
+	DPA_ASSERT(mc->state == qman_mc_idle);
 #ifdef CONFIG_FSL_DPA_CHECKING
-	if (mc->state != mc_idle)
+	if (mc->state != qman_mc_idle)
 		pr_crit("Losing incomplete MC command\n");
 #endif
 }
@@ -1037,9 +1040,9 @@ static inline void qm_mc_finish(struct qm_portal *portal)
 static inline struct qm_mc_command *qm_mc_start(struct qm_portal *portal)
 {
 	register struct qm_mc *mc = &portal->mc;
-	DPA_ASSERT(mc->state == mc_idle);
+	DPA_ASSERT(mc->state == qman_mc_idle);
 #ifdef CONFIG_FSL_DPA_CHECKING
-	mc->state = mc_user;
+	mc->state = qman_mc_user;
 #endif
 	dcbz_64(mc->cr);
 	return mc->cr;
@@ -1048,9 +1051,9 @@ static inline struct qm_mc_command *qm_mc_start(struct qm_portal *portal)
 static inline void qm_mc_abort(struct qm_portal *portal)
 {
 	__maybe_unused register struct qm_mc *mc = &portal->mc;
-	DPA_ASSERT(mc->state == mc_user);
+	DPA_ASSERT(mc->state == qman_mc_user);
 #ifdef CONFIG_FSL_DPA_CHECKING
-	mc->state = mc_idle;
+	mc->state = qman_mc_idle;
 #endif
 }
 
@@ -1058,7 +1061,7 @@ static inline void qm_mc_commit(struct qm_portal *portal, u8 myverb)
 {
 	register struct qm_mc *mc = &portal->mc;
 	struct qm_mc_result *rr = mc->rr + mc->rridx;
-	DPA_ASSERT(mc->state == mc_user);
+	DPA_ASSERT(mc->state == qman_mc_user);
 	lwsync();
 #ifdef CONFIG_FSL_QMAN_BUG_AND_FEATURE_REV1
 	if ((qman_ip_rev == QMAN_REV10) && ((myverb & QM_MCC_VERB_MASK) ==
@@ -1076,11 +1079,11 @@ static inline void qm_mc_commit(struct qm_portal *portal, u8 myverb)
 			dcbit_ro(rr);
 		} while (!__raw_readb(&rr->verb));
 #ifdef CONFIG_FSL_DPA_CHECKING
-		mc->state = mc_idle;
+		mc->state = qman_mc_idle;
 #endif
 		if (rr->result != QM_MCR_RESULT_OK) {
 #ifdef CONFIG_FSL_DPA_CHECKING
-			mc->state = mc_hw;
+			mc->state = qman_mc_hw;
 #endif
 			return;
 		}
@@ -1098,7 +1101,7 @@ static inline void qm_mc_commit(struct qm_portal *portal, u8 myverb)
 	dcbf(mc->cr);
 	dcbit_ro(rr);
 #ifdef CONFIG_FSL_DPA_CHECKING
-	mc->state = mc_hw;
+	mc->state = qman_mc_hw;
 #endif
 }
 
@@ -1106,7 +1109,7 @@ static inline struct qm_mc_result *qm_mc_result(struct qm_portal *portal)
 {
 	register struct qm_mc *mc = &portal->mc;
 	struct qm_mc_result *rr = mc->rr + mc->rridx;
-	DPA_ASSERT(mc->state == mc_hw);
+	DPA_ASSERT(mc->state == qman_mc_hw);
 	/* The inactive response register's verb byte always returns zero until
 	 * its command is submitted and completed. This includes the valid-bit,
 	 * in case you were wondering... */
@@ -1136,7 +1139,7 @@ static inline struct qm_mc_result *qm_mc_result(struct qm_portal *portal)
 	mc->rridx ^= 1;
 	mc->vbit ^= QM_MCC_VERB_VBIT;
 #ifdef CONFIG_FSL_DPA_CHECKING
-	mc->state = mc_idle;
+	mc->state = qman_mc_idle;
 #endif
 	return rr;
 }
@@ -1161,11 +1164,206 @@ static inline void qm_isr_set_iperiod(struct qm_portal *portal, u16 iperiod)
 
 static inline u32 __qm_isr_read(struct qm_portal *portal, enum qm_isr_reg n)
 {
-	return __qm_in(&portal->addr, REG_ISR + (n << 2));
+	return __qm_in(&portal->addr, QM_REG_ISR + (n << 2));
 }
 
 static inline void __qm_isr_write(struct qm_portal *portal, enum qm_isr_reg n,
 					u32 val)
 {
-	__qm_out(&portal->addr, REG_ISR + (n << 2), val);
+	__qm_out(&portal->addr, QM_REG_ISR + (n << 2), val);
+}
+
+/* Cleanup FQs */
+static inline int qm_shutdown_fq(struct qm_portal *portal, u32 fqid)
+{
+
+	struct qm_mc_command *mcc;
+	struct qm_mc_result *mcr;
+	u8 state;
+	int orl_empty, fq_empty, count, drain = 0;
+	u32 result;
+	u32 channel, wq;
+
+	/* Determine the state of the FQID */
+	mcc = qm_mc_start(portal);
+	mcc->queryfq_np.fqid = fqid;
+	qm_mc_commit(portal, QM_MCC_VERB_QUERYFQ_NP);
+	while (!(mcr = qm_mc_result(portal)))
+		cpu_relax();
+	DPA_ASSERT((mcr->verb & QM_MCR_VERB_MASK) == QM_MCR_VERB_QUERYFQ_NP);
+	state = mcr->queryfq_np.state & QM_MCR_NP_STATE_MASK;
+	if (state == QM_MCR_NP_STATE_OOS)
+		return 0; /* Already OOS, no need to do anymore checks */
+
+	/* Query which channel the FQ is using */
+	mcc = qm_mc_start(portal);
+	mcc->queryfq.fqid = fqid;
+	qm_mc_commit(portal, QM_MCC_VERB_QUERYFQ);
+	while (!(mcr = qm_mc_result(portal)))
+		cpu_relax();
+	DPA_ASSERT((mcr->verb & QM_MCR_VERB_MASK) == QM_MCR_VERB_QUERYFQ);
+
+	/* Need to store these since the MCR gets reused */
+	channel = mcr->queryfq.fqd.dest.channel;
+	wq = mcr->queryfq.fqd.dest.wq;
+
+	switch (state) {
+	case QM_MCR_NP_STATE_TEN_SCHED:
+	case QM_MCR_NP_STATE_TRU_SCHED:
+	case QM_MCR_NP_STATE_ACTIVE:
+	case QM_MCR_NP_STATE_PARKED:
+		orl_empty = 0;
+		mcc = qm_mc_start(portal);
+		mcc->alterfq.fqid = fqid;
+		qm_mc_commit(portal, QM_MCC_VERB_ALTER_RETIRE);
+		while (!(mcr = qm_mc_result(portal)))
+			cpu_relax();
+		DPA_ASSERT((mcr->verb & QM_MCR_VERB_MASK) ==
+			   QM_MCR_VERB_ALTER_RETIRE);
+		result = mcr->result; /* Make a copy as we reuse MCR below */
+
+		if (result == QM_MCR_RESULT_PENDING) {
+			/* Need to wait for the FQRN in the message ring, which
+			   will only occur once the FQ has been drained.  In
+			   order for the FQ to drain the portal needs to be set
+			   to dequeue from the channel the FQ is scheduled on */
+			const struct qm_mr_entry *msg;
+			const struct qm_dqrr_entry *dqrr = NULL;
+			int found_fqrn = 0;
+			u16 dequeue_wq = 0;
+
+			/* Flag that we need to drain FQ */
+			drain = 1;
+
+			if (channel >= 0x400 && channel < 0x410) {
+				/* Pool channel, enable the bit in the portal */
+				dequeue_wq = (channel-0x400)<<4 | wq;
+			} else if (channel < 0x400) {
+				/* Dedicated channel */
+				dequeue_wq = wq;
+			}
+			while (!found_fqrn) {
+				/* Keep draining DQRR while checking the MR*/
+				qm_dqrr_sdqcr_set(portal,
+						  0x41000000 | dequeue_wq);
+				qm_dqrr_pvb_update(portal);
+				dqrr = qm_dqrr_current(portal);
+				while (dqrr) {
+					qm_dqrr_cdc_consume_1ptr(portal,
+								 dqrr, 0);
+					qm_dqrr_pvb_update(portal);
+					qm_dqrr_next(portal);
+					dqrr = qm_dqrr_current(portal);
+				}
+
+				/* Process message ring too */
+				qm_mr_pvb_update(portal);
+				msg = qm_mr_current(portal);
+				while (msg) {
+					if ((msg->verb & QM_MR_VERB_TYPE_MASK)
+					    == QM_MR_VERB_FQRN)
+						found_fqrn = 1;
+					qm_mr_next(portal);
+					qm_mr_cci_consume_to_current(portal);
+					qm_mr_pvb_update(portal);
+					msg = qm_mr_current(portal);
+				}
+				cpu_relax();
+			}
+		}
+		if (result != QM_MCR_RESULT_OK &&
+		    result !=  QM_MCR_RESULT_PENDING) {
+			/* error */
+			pr_err("qman_retire_fq failed on FQ 0x%x, result=0x%x\n",
+			       fqid, result);
+			return -1;
+		}
+		if (!(mcr->alterfq.fqs & QM_MCR_FQS_ORLPRESENT)) {
+			/* ORL had no entries, no need to wait until the
+			   ERNs come in */
+			orl_empty = 1;
+		}
+		/* Retirement succeeded, check to see if FQ needs
+		   to be drained */
+		if (drain || mcr->alterfq.fqs & QM_MCR_FQS_NOTEMPTY) {
+			/* FQ is Not Empty, drain using volatile DQ commands */
+			fq_empty = 0;
+			do {
+				const struct qm_dqrr_entry *dqrr = NULL;
+				u32 vdqcr = fqid | QM_VDQCR_NUMFRAMES_SET(3);
+				qm_dqrr_vdqcr_set(portal, vdqcr);
+
+				/* Wait for a dequeue to occur */
+				while (dqrr == NULL) {
+					qm_dqrr_pvb_update(portal);
+					dqrr = qm_dqrr_current(portal);
+					if (!dqrr)
+						cpu_relax();
+				}
+				/* Process the dequeues, making sure to
+				   empty the ring completely */
+				while (dqrr) {
+					if (dqrr->stat & QM_DQRR_STAT_FQ_EMPTY)
+						fq_empty = 1;
+					qm_dqrr_cdc_consume_1ptr(portal,
+								 dqrr, 0);
+					qm_dqrr_pvb_update(portal);
+					qm_dqrr_next(portal);
+					dqrr = qm_dqrr_current(portal);
+				}
+			} while (fq_empty == 0);
+		}
+		/* Wait for the ORL to have been completely drained */
+		count = 0;
+		while (orl_empty == 0) {
+			const struct qm_mr_entry *msg;
+			qm_mr_pvb_update(portal);
+			msg = qm_mr_current(portal);
+			while (msg) {
+				++count;
+				if ((msg->verb & QM_MR_VERB_TYPE_MASK) ==
+				    QM_MR_VERB_FQRL)
+					orl_empty = 1;
+				qm_mr_next(portal);
+				qm_mr_cci_consume_to_current(portal);
+				qm_mr_pvb_update(portal);
+				msg = qm_mr_current(portal);
+			}
+			cpu_relax();
+		}
+		mcc = qm_mc_start(portal);
+		mcc->alterfq.fqid = fqid;
+		qm_mc_commit(portal, QM_MCC_VERB_ALTER_OOS);
+		while (!(mcr = qm_mc_result(portal)))
+			cpu_relax();
+		DPA_ASSERT((mcr->verb & QM_MCR_VERB_MASK) ==
+			   QM_MCR_VERB_ALTER_OOS);
+		if (mcr->result != QM_MCR_RESULT_OK) {
+			pr_err("OOS Failed on FQID 0x%x, result 0x%x\n",
+			       fqid, mcr->result);
+			return -1;
+		}
+		return 0;
+		break;
+	case QM_MCR_NP_STATE_RETIRED:
+		/* Send OOS Command */
+		mcc = qm_mc_start(portal);
+		mcc->alterfq.fqid = fqid;
+		qm_mc_commit(portal, QM_MCC_VERB_ALTER_OOS);
+		while (!(mcr = qm_mc_result(portal)))
+			cpu_relax();
+		DPA_ASSERT((mcr->verb & QM_MCR_VERB_MASK) ==
+			   QM_MCR_VERB_ALTER_OOS);
+		if (mcr->result) {
+			pr_err("OOS Failed on FQID 0x%x\n", fqid);
+			return -1;
+		}
+		return 0;
+		break;
+	case QM_MCR_NP_STATE_OOS:
+		/*  Done */
+		return 0;
+		break;
+	}
+	return -1;
 }
diff --git a/drivers/staging/fsl_qbman/qman_private.h b/drivers/staging/fsl_qbman/qman_private.h
index 32f31e4..ff18462 100644
--- a/drivers/staging/fsl_qbman/qman_private.h
+++ b/drivers/staging/fsl_qbman/qman_private.h
@@ -204,11 +204,22 @@ int qm_set_wpm(int wpm);
 int qm_get_wpm(int *wpm);
 
 /* Hooks from qman_driver.c in to qman_high.c */
+struct qman_portal *qman_create_portal(
+			struct qman_portal *portal,
+			const struct qm_portal_config *config,
+			const struct qman_cgrs *cgrs);
+
 struct qman_portal *qman_create_affine_portal(
 			const struct qm_portal_config *config,
 			const struct qman_cgrs *cgrs);
 struct qman_portal *qman_create_affine_slave(struct qman_portal *redirect);
 const struct qm_portal_config *qman_destroy_affine_portal(void);
+void qman_destroy_portal(struct qman_portal *qm);
+
+/* Hooks from fsl_usdpaa.c to qman_driver.c */
+struct qm_portal_config *qm_get_unused_portal(void);
+void qm_put_unused_portal(struct qm_portal_config *pcfg);
+void qm_set_liodns(struct qm_portal_config *pcfg);
 
 /* This CGR feature is supported by h/w and required by unit-tests and the
  * debugfs hooks, so is implemented in the driver. However it allows an explicit
diff --git a/include/linux/fsl_bman.h b/include/linux/fsl_bman.h
index 072f326..48663d7 100644
--- a/include/linux/fsl_bman.h
+++ b/include/linux/fsl_bman.h
@@ -403,6 +403,10 @@ static inline void bman_release_bpid(u32 bpid)
 	bman_release_bpid_range(bpid, 1);
 }
 
+void bman_seed_bpid_range(u32 bpid, unsigned int count);
+
+
+int bman_shutdown_pool(u32 bpid);
 
 	/* Pool management */
 	/* --------------- */
diff --git a/include/linux/fsl_qman.h b/include/linux/fsl_qman.h
index 7c83027..da2c67d3 100644
--- a/include/linux/fsl_qman.h
+++ b/include/linux/fsl_qman.h
@@ -2089,6 +2089,22 @@ static inline void qman_release_fqid(u32 fqid)
 	qman_release_fqid_range(fqid, 1);
 }
 
+void qman_seed_fqid_range(u32 fqid, unsigned int count);
+
+
+int qman_shutdown_fq(u32 fqid);
+
+/**
+ * qman_reserve_fqid_range - Reserve the specified range of frame queue IDs
+ * @fqid: the base FQID of the range to deallocate
+ * @count: the number of FQIDs in the range
+ */
+int qman_reserve_fqid_range(u32 fqid, unsigned int count);
+static inline int qman_reserve_fqid(u32 fqid)
+{
+	return qman_reserve_fqid_range(fqid, 1);
+}
+
 	/* Pool-channel management */
 	/* ----------------------- */
 /**
@@ -2121,6 +2137,18 @@ static inline void qman_release_pool(u32 id)
 	qman_release_pool_range(id, 1);
 }
 
+/**
+ * qman_reserve_pool_range - Reserve the specified range of pool-channel IDs
+ * @id: the base pool-channel ID of the range to reserve
+ * @count: the number of pool-channel IDs in the range
+ */
+int qman_reserve_pool_range(u32 id, unsigned int count);
+static inline int qman_reserve_pool(u32 id)
+{
+	return qman_reserve_pool_range(id, 1);
+}
+
+void qman_seed_pool_range(u32 id, unsigned int count);
 
 	/* CGR management */
 	/* -------------- */
@@ -2221,6 +2249,20 @@ static inline void qman_release_cgrid(u32 id)
 	qman_release_cgrid_range(id, 1);
 }
 
+/**
+ * qman_reserve_cgrid_range - Reserve the specified range of CGR ID
+ * @id: the base CGR ID of the range to reserve
+ * @count: the number of CGR IDs in the range
+ */
+int qman_reserve_cgrid_range(u32 id, unsigned int count);
+static inline int qman_reserve_cgrid(u32 id)
+{
+	return qman_reserve_cgrid_range(id, 1);
+}
+
+void qman_seed_cgrid_range(u32 id, unsigned int count);
+
+
 	/* Helpers */
 	/* ------- */
 /**
@@ -2431,6 +2473,15 @@ static inline void qman_release_ceetm0_channelid(u32 channelid)
 	qman_release_ceetm0_channel_range(channelid, 1);
 }
 
+int qman_reserve_ceetm0_channel_range(u32 channelid, u32 count);
+static inline int qman_reserve_ceetm0_channelid(u32 channelid)
+{
+	return qman_reserve_ceetm0_channel_range(channelid, 1);
+}
+
+void qman_seed_ceetm0_channel_range(u32 channelid, u32 count);
+
+
 int qman_alloc_ceetm1_channel_range(u32 *result, u32 count, u32 align,
 								int partial);
 static inline int qman_alloc_ceetm1_channel(u32 *result)
@@ -2443,6 +2494,14 @@ static inline void qman_release_ceetm1_channelid(u32 channelid)
 {
 	qman_release_ceetm1_channel_range(channelid, 1);
 }
+int qman_reserve_ceetm1_channel_range(u32 channelid, u32 count);
+static inline int qman_reserve_ceetm1_channelid(u32 channelid)
+{
+	return qman_reserve_ceetm1_channel_range(channelid, 1);
+}
+
+void qman_seed_ceetm1_channel_range(u32 channelid, u32 count);
+
 
 int qman_alloc_ceetm0_lfqid_range(u32 *result, u32 count, u32 align,
 								int partial);
@@ -2456,6 +2515,14 @@ static inline void qman_release_ceetm0_lfqid(u32 lfqid)
 {
 	qman_release_ceetm0_lfqid_range(lfqid, 1);
 }
+int qman_reserve_ceetm0_lfqid_range(u32 lfqid, u32 count);
+static inline int qman_reserve_ceetm0_lfqid(u32 lfqid)
+{
+	return qman_reserve_ceetm0_lfqid_range(lfqid, 1);
+}
+
+void qman_seed_ceetm0_lfqid_range(u32 lfqid, u32 count);
+
 
 int qman_alloc_ceetm1_lfqid_range(u32 *result, u32 count, u32 align,
 								int partial);
@@ -2469,6 +2536,15 @@ static inline void qman_release_ceetm1_lfqid(u32 lfqid)
 {
 	qman_release_ceetm1_lfqid_range(lfqid, 1);
 }
+int qman_reserve_ceetm1_lfqid_range(u32 lfqid, u32 count);
+static inline int qman_reserve_ceetm1_lfqid(u32 lfqid)
+{
+	return qman_reserve_ceetm1_lfqid_range(lfqid, 1);
+}
+
+void qman_seed_ceetm1_lfqid_range(u32 lfqid, u32 count);
+
+
 	/* ----------------------------- */
 	/* CEETM :: sub-portals          */
 	/* ----------------------------- */
diff --git a/include/linux/fsl_usdpaa.h b/include/linux/fsl_usdpaa.h
index 1f9e1dc..dc2a063 100644
--- a/include/linux/fsl_usdpaa.h
+++ b/include/linux/fsl_usdpaa.h
@@ -14,11 +14,17 @@ extern "C" {
 
 #include <linux/uaccess.h>
 #include <linux/ioctl.h>
+#include <linux/fsl_qman.h> /* For "enum qm_channel" */
+#include <linux/compat.h>
 
 #ifdef CONFIG_FSL_USDPAA
 
-/* Allocation of resource IDs uses a generic interface. This enum is used to
- * distinguish between the type of underlying object being manipulated. */
+/******************************/
+/* Allocation of resource IDs */
+/******************************/
+
+/* This enum is used to distinguish between the type of underlying object being
+ * manipulated. */
 enum usdpaa_id_type {
 	usdpaa_id_fqid,
 	usdpaa_id_bpid,
@@ -44,6 +50,25 @@ struct usdpaa_ioctl_id_release {
 	uint32_t base;
 	uint32_t num;
 };
+struct usdpaa_ioctl_id_reserve {
+	enum usdpaa_id_type id_type;
+	uint32_t base;
+	uint32_t num;
+};
+
+
+/* ioctl() commands */
+#define USDPAA_IOCTL_ID_ALLOC \
+	_IOWR(USDPAA_IOCTL_MAGIC, 0x01, struct usdpaa_ioctl_id_alloc)
+#define USDPAA_IOCTL_ID_RELEASE \
+	_IOW(USDPAA_IOCTL_MAGIC, 0x02, struct usdpaa_ioctl_id_release)
+#define USDPAA_IOCTL_ID_RESERVE \
+	_IOW(USDPAA_IOCTL_MAGIC, 0x0A, struct usdpaa_ioctl_id_reserve)
+
+/**********************/
+/* Mapping DMA memory */
+/**********************/
+
 /* Maximum length for a map name, including NULL-terminator */
 #define USDPAA_DMA_NAME_MAX 16
 /* Flags for requesting DMA maps. Maps are private+unnamed or sharable+named.
@@ -63,10 +88,11 @@ struct usdpaa_ioctl_id_release {
 #define USDPAA_DMA_FLAG_SHARE    0x01
 #define USDPAA_DMA_FLAG_CREATE   0x02
 #define USDPAA_DMA_FLAG_LAZY     0x04
+#define USDPAA_DMA_FLAG_RDONLY   0x08
 struct usdpaa_ioctl_dma_map {
-	/* If the map succeeds, pa_offset is returned and can be used in a
-	 * subsequent call to mmap(). */
-	uint64_t pa_offset;
+	/* Output parameters - virtual and physical addresses */
+	void *ptr;
+	uint64_t phys_addr;
 	/* Input parameter, the length of the region to be created (or if
 	 * mapping an existing region, this must match it). Must be a power-of-4
 	 * multiple of page size. */
@@ -85,20 +111,112 @@ struct usdpaa_ioctl_dma_map {
 	 * already existed. */
 	int did_create;
 };
-#define USDPAA_IOCTL_ID_ALLOC \
-	_IOWR(USDPAA_IOCTL_MAGIC, 0x01, struct usdpaa_ioctl_id_alloc)
-#define USDPAA_IOCTL_ID_RELEASE \
-	_IOW(USDPAA_IOCTL_MAGIC, 0x02, struct usdpaa_ioctl_id_release)
+
+#ifdef CONFIG_COMPAT
+struct usdpaa_ioctl_dma_map_compat {
+	/* Output parameters - virtual and physical addresses */
+	compat_uptr_t ptr;
+	uint64_t phys_addr;
+	/* Input parameter, the length of the region to be created (or if
+	 * mapping an existing region, this must match it). Must be a power-of-4
+	 * multiple of page size. */
+	uint64_t len;
+	/* Input parameter, the USDPAA_DMA_FLAG_* settings. */
+	uint32_t flags;
+	/* If _FLAG_SHARE is specified, the name of the region to be created (or
+	 * of the existing mapping to use). */
+	char name[USDPAA_DMA_NAME_MAX];
+	/* If this ioctl() creates the mapping, this is an input parameter
+	 * stating whether the region supports locking. If mapping an existing
+	 * region, this is a return value indicating the same thing. */
+	int has_locking;
+	/* In the case of a successful map with _CREATE and _LAZY, this return
+	 * value indicates whether we created the mapped region or whether it
+	 * already existed. */
+	int did_create;
+};
+
+#define USDPAA_IOCTL_DMA_MAP_COMPAT \
+	_IOWR(USDPAA_IOCTL_MAGIC, 0x03, struct usdpaa_ioctl_dma_map_compat)
+#endif
+
+
 #define USDPAA_IOCTL_DMA_MAP \
 	_IOWR(USDPAA_IOCTL_MAGIC, 0x03, struct usdpaa_ioctl_dma_map)
+/* munmap() does not remove the DMA map, just the user-space mapping to it.
+ * This ioctl will do both (though you can munmap() before calling the ioctl
+ * too). */
+#define USDPAA_IOCTL_DMA_UNMAP \
+	_IOW(USDPAA_IOCTL_MAGIC, 0x04, unsigned char)
 /* We implement a cross-process locking scheme per DMA map. Call this ioctl()
  * with a mmap()'d address, and the process will (interruptible) sleep if the
  * lock is already held by another process. Process destruction will
  * automatically clean up any held locks. */
 #define USDPAA_IOCTL_DMA_LOCK \
-	_IOW(USDPAA_IOCTL_MAGIC, 0x04, unsigned char)
-#define USDPAA_IOCTL_DMA_UNLOCK \
 	_IOW(USDPAA_IOCTL_MAGIC, 0x05, unsigned char)
+#define USDPAA_IOCTL_DMA_UNLOCK \
+	_IOW(USDPAA_IOCTL_MAGIC, 0x06, unsigned char)
+
+/***************************************/
+/* Mapping and using QMan/BMan portals */
+/***************************************/
+enum usdpaa_portal_type {
+	 usdpaa_portal_qman,
+	 usdpaa_portal_bman,
+};
+
+struct usdpaa_ioctl_portal_map {
+	/* Input parameter, is a qman or bman portal required. */
+	enum usdpaa_portal_type type;
+	/* Return value if the map succeeds, this gives the mapped
+	 * cache-inhibited (cinh) and cache-enabled (cena) addresses. */
+	struct usdpaa_portal_map {
+		void *cinh;
+		void *cena;
+	} addr;
+	/* Qman-specific return values */
+	uint16_t channel;
+	uint32_t pools;
+	uint32_t irq;
+};
+
+#ifdef CONFIG_COMPAT
+struct compat_usdpaa_ioctl_portal_map {
+	/* Input parameter, is a qman or bman portal required. */
+	enum usdpaa_portal_type type;
+	/* Return value if the map succeeds, this gives the mapped
+	 * cache-inhibited (cinh) and cache-enabled (cena) addresses. */
+	struct usdpaa_portal_map_compat {
+		compat_uptr_t cinh;
+		compat_uptr_t cena;
+	} addr;
+	/* Qman-specific return values */
+	uint16_t channel;
+	uint32_t pools;
+	uint32_t irq;
+};
+#define USDPAA_IOCTL_PORTAL_MAP_COMPAT \
+	_IOWR(USDPAA_IOCTL_MAGIC, 0x07, struct compat_usdpaa_ioctl_portal_map)
+#define USDPAA_IOCTL_PORTAL_UNMAP_COMPAT \
+	_IOW(USDPAA_IOCTL_MAGIC, 0x08, struct usdpaa_portal_map_compat)
+#endif
+
+#define USDPAA_IOCTL_PORTAL_MAP \
+	_IOWR(USDPAA_IOCTL_MAGIC, 0x07, struct usdpaa_ioctl_portal_map)
+#define USDPAA_IOCTL_PORTAL_UNMAP \
+	_IOW(USDPAA_IOCTL_MAGIC, 0x08, struct usdpaa_portal_map)
+
+#define USDPAA_IOCTL_PORTAL_IRQ_MAP \
+	_IOW(USDPAA_IOCTL_MAGIC, 0x09, uint32_t)
+
+/* ioctl to query the amount of DMA memory used in the system */
+struct usdpaa_ioctl_dma_used {
+	uint64_t free_bytes;
+	uint64_t total_bytes;
+};
+#define USDPAA_IOCTL_DMA_USED \
+	_IOR(USDPAA_IOCTL_MAGIC, 0x0B, struct usdpaa_ioctl_dma_used)
+
 
 #ifdef __KERNEL__
 
@@ -120,31 +238,41 @@ int usdpaa_test_fault(unsigned long pfn, u64 *phys_addr, u64 *size);
  * both the FQID and BPID allocators. The fsl_usdpaa driver also uses this
  * interface for tracking per-process allocations handed out to user-space. */
 struct dpa_alloc {
-	struct list_head list;
+	struct list_head free;
 	spinlock_t lock;
+	struct list_head used;
 };
 #define DECLARE_DPA_ALLOC(name) \
 	struct dpa_alloc name = { \
-		.list = { \
-			.prev = &name.list, \
-			.next = &name.list \
+		.free = { \
+			.prev = &name.free, \
+			.next = &name.free \
 		}, \
-		.lock = __SPIN_LOCK_UNLOCKED(name.lock) \
+		.lock = __SPIN_LOCK_UNLOCKED(name.lock), \
+		.used = { \
+			 .prev = &name.used, \
+			 .next = &name.used \
+		 } \
 	}
 static inline void dpa_alloc_init(struct dpa_alloc *alloc)
 {
-	INIT_LIST_HEAD(&alloc->list);
+	INIT_LIST_HEAD(&alloc->free);
+	INIT_LIST_HEAD(&alloc->used);
 	spin_lock_init(&alloc->lock);
 }
 int dpa_alloc_new(struct dpa_alloc *alloc, u32 *result, u32 count, u32 align,
 		  int partial);
 void dpa_alloc_free(struct dpa_alloc *alloc, u32 base_id, u32 count);
+void dpa_alloc_seed(struct dpa_alloc *alloc, u32 fqid, u32 count);
+
 /* Like 'new' but specifies the desired range, returns -ENOMEM if the entire
  * desired range is not available, or 0 for success. */
 int dpa_alloc_reserve(struct dpa_alloc *alloc, u32 base_id, u32 count);
 /* Pops and returns contiguous ranges from the allocator. Returns -ENOMEM when
  * 'alloc' is empty. */
 int dpa_alloc_pop(struct dpa_alloc *alloc, u32 *result, u32 *count);
+/* Returns 1 if the specified id is alloced, 0 otherwise */
+int dpa_alloc_check(struct dpa_alloc *list, u32 id);
 #endif /* __KERNEL__ */
 
 #ifdef __cplusplus
-- 
1.8.4.93.g57e4c17

