From 23f041a5f318635ed418eafa29652e757b94ec74 Mon Sep 17 00:00:00 2001
From: Andrei Varvara <andrei.varvara@freescale.com>
Date: Fri, 15 Mar 2013 09:47:24 +0200
Subject: [PATCH 400/547] dpa_offload: Add DPA IPsec component

The DPA IPSec component exports a set of functions used to:
    - initialize the DPA IPSec module internal data structures
    - create and configure full inbound IPSec hardware accelerated paths
    - create and configure full outbound IPSec hardware accelerated paths
    - replace expired SAs (after rekeying) without packet loss

During the initialization phase the DPA IPSec implementation
performs a series of actions meant to remove the need to perform
memory allocations and hardware/software object initializations
during the runtime phases.

    INBOUND PATH:
    _____________      ______________       _______________________________       __________
   || SA lookup ||--> || Decryption || --> || Inbound Policy Verification || --> || App Rx ||
   ||___________||    ||____________||     ||_____________________________||     ||________||

    The inbound processing of an encrypted packet begins by determining the SA
    that will be used for decryption and authentication. In accordance to the
    RFC the packets are classified based on a 3-tuple that uniquely identifies
    the SA. This 3-tuple is formed from:
        - the destination IP address in the IP header of the encrypted packet
        - the value of the IP protocol field in the IP header of the encrypted packet
        - the value in the SPI field in the ESP header

    A special case is that were the encrypted packets are encapsulated in an
    UDP header in order to support NAT traversal. In this case the
    classification key should contain the following fields:
        - the destination IP address in the IP header of the encrypted packet
        - the IP protocol field in the IP header of the encrypted packet
        - the SPI field in the ESP header
        - the source UDP port in the UDP header
        - the destination UDP port in the UDP header

    This lookup is offloaded to FMAN by means of classifier API and FMAN API.

    When an encrypted packet matches an offloaded key, it is directed by the
    hardware into the decryption process by enquing the packet to a SEC frame
    queue (FQ). A shared descriptor (representing the decryption SA) is set on
    this FQ and the SEC will begin the decryption process and then place the
    clear text packet to a FQ that is input for an offline port (OH). Processing
    continues with inbound policy verification done on OH using the FMAN hardware.
    After this step the packet is enqueued to a FQ created by the application
    which benefits of IPSec security

   OUTBOUND PATH:
    __________       _________________       ______________       __________________
   || App Tx || --> || Policy Lookup || --> || Encryption || --> || Error Checking ||
   ||________||     ||_______________||     ||____________||     ||________________||

    The primary function of the policy lookup block is to classify frames and
    determine the correct SA on which they should be processed.

    The DPA IPSec can be configured to build a policy key using any subset of the following fields:
        - masked source IP address
        - masked destination IP address
        - optionally masked IP protocol field  value
        - masked source port value / ICMP type field value
        - masked destination port value / ICMP code field value

    IP fragmentation can be configured per policy and is performed, if required,
    on the packets before being sent to the Encryption Block. A fragmentation header
    manipulation identifier has to be passed when offloading the policy. If a
    clear text packet hits an offloaded policy the packet will be directed by FMAN
    hardware into the proper FQ for SEC processing.

    After the SEC has completed all the required operations, a new frame is created
    containing the ESP encapsulated packet. This frame will be sent to
    the next block for further processing i.e input to offline port where error checking
    is done prior to forwarding the packet to an application desired FQ based on the
    SA that processed that packet.

Signed-off-by: Andrei Varvara <andrei.varvara@freescale.com>
Signed-off-by: Mihai Serb
Change-Id: Id8a4afa1cfda42dd2ba1408614a5900cb7b80cee
Reviewed-on: http://git.am.freescale.net:8181/2235
Reviewed-by: Fleming Andrew-AFLEMING <AFLEMING@freescale.com>
Tested-by: Fleming Andrew-AFLEMING <AFLEMING@freescale.com>
[Original patch taken from QorIQ-SDK-V1.4-SOURCE-20130625-yocto.iso]
Signed-off-by: Bin Jiang <bin.jiang@windriver.com>
---
 drivers/staging/fsl_dpa_offload/dpa_ipsec.c      | 4794 ++++++++++++++++++++++
 drivers/staging/fsl_dpa_offload/dpa_ipsec.h      |  459 +++
 drivers/staging/fsl_dpa_offload/dpa_ipsec_desc.c | 1821 ++++++++
 drivers/staging/fsl_dpa_offload/dpa_ipsec_desc.h |   95 +
 include/linux/fsl_dpa_ipsec.h                    |  516 +++
 5 files changed, 7685 insertions(+)
 create mode 100644 drivers/staging/fsl_dpa_offload/dpa_ipsec.c
 create mode 100644 drivers/staging/fsl_dpa_offload/dpa_ipsec.h
 create mode 100644 drivers/staging/fsl_dpa_offload/dpa_ipsec_desc.c
 create mode 100644 drivers/staging/fsl_dpa_offload/dpa_ipsec_desc.h
 create mode 100644 include/linux/fsl_dpa_ipsec.h

diff --git a/drivers/staging/fsl_dpa_offload/dpa_ipsec.c b/drivers/staging/fsl_dpa_offload/dpa_ipsec.c
new file mode 100644
index 0000000..82cc2f2
--- /dev/null
+++ b/drivers/staging/fsl_dpa_offload/dpa_ipsec.c
@@ -0,0 +1,4794 @@
+/* Copyright 2008-2013 Freescale Semiconductor, Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/netdevice.h>
+#include <linux/delay.h>
+#include <linux/in.h>
+#include <linux/in6.h>
+
+#include "dpa_ipsec.h"
+#include "dpa_ipsec_desc.h"
+#include "dpa_classifier.h"
+
+#include "fm_common.h"
+#include "fm_pcd.h"
+
+/* DPA IPsec Mapping between API algorithm suites and SEC algorithm IDs */
+struct ipsec_alg_suite ipsec_algs[] = IPSEC_ALGS;
+
+/* globally allocated because of performance constraints */
+static t_FmPcdCcNodeParams cc_node_prms;
+
+/* Global dpa_ipsec component */
+struct dpa_ipsec *gbl_dpa_ipsec;
+
+static int sa_flush_policies(struct dpa_ipsec_sa *sa);
+static int sa_rekeying_outbound(struct dpa_ipsec_sa *new_sa);
+static void *alloc_ipsec_manip(struct dpa_ipsec *dpa_ipsec);
+
+/* Debug support functions */
+
+#ifdef DEBUG_PARAM
+int print_sa_sec_param(struct dpa_ipsec_sa *sa)
+{
+	int i;
+	struct dpa_ipsec_policy_entry *policy_entry, *tmp_policy_entry;
+	struct dpa_ipsec_policy_selectors *policy_selectors;
+
+	BUG_ON(!sa);
+
+	pr_info("\n Printing SA SEC PARAM for sa %p\n", sa);
+	pr_info("\n sa_dir = %d\n", sa->sa_dir);
+	pr_info("\n id = %d\n", sa->id);
+	pr_info(" dpa_ipsec addr = %p\n", sa->dpa_ipsec);
+	pr_info(" from_sec_fq addr = %p\n", sa->from_sec_fq);
+
+	pr_info("\n auth_data.auth_type = %d\n", sa->auth_data.auth_type);
+	pr_info("auth_data.auth_key_len = %d\n",
+		sa->auth_data.auth_key_len);
+	pr_info("auth_data.auth_key is\n");
+	for (i = 0; i < sa->auth_data.auth_key_len; i++)
+		pr_info("%x, ", sa->auth_data.auth_key[i]);
+
+	pr_info("\n cipher_data.cipher_type = %d\n",
+		sa->cipher_data.cipher_type);
+	pr_info("cipher_data.cipher_key_len = %d\n",
+		sa->cipher_data.cipher_key_len);
+	pr_info("cipher_data.cipher_key is\n");
+	for (i = 0; i < sa->cipher_data.cipher_key_len; i++)
+		pr_info("%x, ", sa->cipher_data.cipher_key[i]);
+
+	pr_info("\n sa_bpid = %d\n", sa->sa_bpid);
+	pr_info(" spi = %d\n", sa->spi);
+	pr_info(" sa_wqid = %d\n", sa->sa_wqid);
+	pr_info(" outbound_flowid = %d\n", sa->outbound_flowid);
+
+	pr_info("dest_addr.version = %d\n", sa->dest_addr.version);
+	pr_info("dest_addr = %x.%x.%x.%x\n",
+		sa->dest_addr.ipv4.byte[0],
+		sa->dest_addr.ipv4.byte[1],
+		sa->dest_addr.ipv4.byte[2], sa->dest_addr.ipv4.byte[3]);
+	pr_info("src_addr.version = %d\n", sa->src_addr.version);
+	pr_info("src_addr = %x.%x.%x.%x\n",
+		sa->src_addr.ipv4.byte[0],
+		sa->src_addr.ipv4.byte[1],
+		sa->src_addr.ipv4.byte[2], sa->src_addr.ipv4.byte[3]);
+
+	if (sa_is_outbound(sa)) {
+		uint8_t *out_hdr;
+		out_hdr = &sa->sec_desc->pdb_en.ip_hdr[0];
+		pr_info("Outer Header length  %d\n",
+			sa->sec_desc->pdb_en.ip_hdr_len);
+		pr_info("Outer Header is:\n");
+		for (i = 0; i < sa->sec_desc->pdb_en.ip_hdr_len; i++)
+			pr_info("%x, ", *(out_hdr + i));
+
+		pr_info("pdb_en.ip_hdr_len %d\n",
+			sa->sec_desc->pdb_en.ip_hdr_len);
+		pr_info("pdb_en.spi = %d\n", sa->sec_desc->pdb_en.spi);
+		pr_info("pdb_en.seq_num = %d\n",
+			sa->sec_desc->pdb_en.seq_num);
+		pr_info("pdb_en.options = 0x%x\n",
+			sa->sec_desc->pdb_en.options);
+		pr_info("pdb_en.desc_hdr = 0x%x\n",
+			sa->sec_desc->pdb_en.desc_hdr);
+		pr_info("pdb_en.ip_nh = 0x%x\n",
+			sa->sec_desc->pdb_en.ip_nh);
+	} else {
+		pr_info("pdb_dec.hmo_ip_hdr_len %d\n",
+			sa->sec_desc->pdb_dec.hmo_ip_hdr_len);
+		pr_info("pdb_dec.options %d\n",
+			sa->sec_desc->pdb_dec.options);
+		pr_info("pdb_dec.seq_num %d\n",
+			sa->sec_desc->pdb_dec.seq_num);
+	}
+
+	pr_info("\n Printing all policies from this SA policy_list\n");
+	list_for_each_entry_safe(policy_entry, tmp_policy_entry,
+				 &sa->policy_headlist, node) {
+		policy_selectors = &policy_entry->policy_selectors;
+		pr_info("policy_selectors src_addr.version = %d\n",
+			policy_selectors->src_addr.version);
+		pr_info("policy_selectors src_addr = %x.%x.%x.%x\n",
+			policy_selectors->src_addr.ipv4.byte[0],
+			policy_selectors->src_addr.ipv4.byte[1],
+			policy_selectors->src_addr.ipv4.byte[2],
+			policy_selectors->src_addr.ipv4.byte[3]);
+		pr_info("\n policy_selectors dest_addr.version = %d\n",
+			policy_selectors->dest_addr.version);
+		pr_info("policy_selectors dest_addr = %x.%x.%x.%x\n",
+			policy_selectors->dest_addr.ipv4.byte[0],
+			policy_selectors->dest_addr.ipv4.byte[1],
+			policy_selectors->dest_addr.ipv4.byte[2],
+			policy_selectors->dest_addr.ipv4.byte[3]);
+
+		pr_info("\n policy_selectors dest_port = %d\n",
+			policy_selectors->dest_port);
+		pr_info(" policy_selectors src_port = %d\n",
+			policy_selectors->src_port);
+		pr_info(" policy_selectors dest_port = %d\n",
+			policy_selectors->dest_port_mask);
+		pr_info(" policy_selectors dest_port = %d\n",
+			policy_selectors->src_port_mask);
+		pr_info(" policy_selectors proto = %d\n",
+			policy_selectors->protocol);
+		pr_info(" policy_selectors dest_prefix_len = %d\n",
+			policy_selectors->dest_prefix_len);
+		pr_info(" policy_selectors src_prefix_len = %d\n",
+			policy_selectors->src_prefix_len);
+	}
+	pr_info("\n Done printing SA SEC PARAM for sa %p\n", sa);
+
+	return 0;
+}
+#endif
+
+/* Initialization functions */
+
+/* store params needed during runtime or free */
+static inline void store_ipsec_params(struct dpa_ipsec *dpa_ipsec,
+				      const struct dpa_ipsec_params *params)
+{
+	struct dpa_ipsec_pre_sec_out_params *pre_sec_out_params;
+	struct dpa_ipsec_pol_table *any_ipv4_table, *any_ipv6_table;
+	int i;
+
+	/* copy config params */
+	dpa_ipsec->config = *params;
+
+	/*
+	 * reconfigure the array of outbound policy table parameters, in order
+	 * to simplify the process of choosing the correct table during runtime
+	 * add / remove policies operations
+	 */
+
+	pre_sec_out_params = &dpa_ipsec->config.pre_sec_out_params;
+	/* get the desc for the ANY tables */
+	any_ipv4_table = &pre_sec_out_params->table[DPA_IPSEC_PROTO_ANY_IPV4];
+	any_ipv6_table = &pre_sec_out_params->table[DPA_IPSEC_PROTO_ANY_IPV6];
+
+	/*
+	 * replace the parameters of a table for a specific protocol, if an
+	 * invalid table desc was provided, with those of the corresponding ANY
+	 * table for that IP version
+	 */
+	for (i = 0; i < DPA_IPSEC_MAX_SUPPORTED_PROTOS - 2; i++) {
+		if (pre_sec_out_params->table[i].dpa_cls_td ==
+							DPA_OFFLD_DESC_NONE) {
+			/* IPV4 table desc are at even indexes (IPV6 at odd) */
+			if (i & 0x01)
+				pre_sec_out_params->table[i] = *any_ipv6_table;
+			else
+				pre_sec_out_params->table[i] = *any_ipv4_table;
+		}
+	}
+}
+
+/* check that the provided params are valid */
+static int check_ipsec_params(const struct dpa_ipsec_params *prms)
+{
+	const struct dpa_ipsec_pre_sec_out_params *pre_sec_out_prms;
+	const struct dpa_ipsec_pre_sec_in_params *pre_sec_in_prms;
+	struct dpa_cls_tbl_params table_params;
+	int i, err, valid_tables = 0, fqid_range_size, min_fqid_num;
+
+	if (!prms) {
+		pr_err("Invalid DPA IPsec parameters handle\n");
+		return -EINVAL;
+	}
+
+	if ((prms->post_sec_in_params.do_pol_check) && (!prms->fm_pcd)) {
+		pr_err("Provide a valid PCD handle to enable inbound policy check!\n");
+		return -EINVAL;
+	}
+
+	/*
+	 * check that all required table descriptors were provided:
+	 * - at least one table for outbound policy lookup
+	 * - one table for index lookup after decryption
+	 * - one table for SA lookup
+	 */
+
+	/* check outbound policy tables */
+	pre_sec_out_prms = &prms->pre_sec_out_params;
+	for (i = 0; i < DPA_IPSEC_MAX_SUPPORTED_PROTOS; i++)
+		if (pre_sec_out_prms->table[i].dpa_cls_td !=
+						DPA_OFFLD_DESC_NONE) {
+			/* verify that a valid key structure was configured */
+			if (!pre_sec_out_prms->table[i].key_fields) {
+				pr_err("Invalid key struct. for out table %d\n",
+				       i);
+				return -EINVAL;
+			}
+
+			/* verify that it is not an indexed table */
+			err = dpa_classif_table_get_params(
+					pre_sec_out_prms->table[i].dpa_cls_td,
+				       &table_params);
+			if (err < 0) {
+				pr_err("Couldn't check type of outbound policy lookup table\n");
+				return -EINVAL;
+			}
+
+			if (table_params.type == DPA_CLS_TBL_INDEXED) {
+				pr_err("Outbound policy lookup table cannot be of type INDEXED\n");
+				return -EINVAL;
+			}
+			valid_tables++;
+		}
+
+	if (!valid_tables) {
+		pr_err("Specify at least one table for outbound policy lookup\n");
+		return -EINVAL;
+	}
+
+	/* post decryption SA classification table */
+	if (prms->post_sec_in_params.dpa_cls_td == DPA_OFFLD_DESC_NONE) {
+		pr_err("Specify a valid table for post decryption classification\n");
+		return -EINVAL;
+	}
+
+	/* get post decryption table parameters */
+	err = dpa_classif_table_get_params(prms->post_sec_in_params.dpa_cls_td,
+					   &table_params);
+	if (err < 0) {
+		pr_err("Could not check type of post decryption table\n");
+		return -EINVAL;
+	}
+
+	/* verify that it is an indexed table */
+	if (table_params.type != DPA_CLS_TBL_INDEXED) {
+		pr_err("Post decryption table must be of type INDEXED\n");
+		return -EINVAL;
+	}
+
+	/*
+	 * verify that it can hold a flow ID value for each possible IN SA plus
+	 * the reserved number of flow ID values (base_flow_id)
+	 */
+	if (table_params.indexed_params.entries_cnt <
+	    (prms->max_sa_pairs + prms->post_sec_in_params.base_flow_id)) {
+		pr_err("The post decryption table size is to small!\n");
+		return -EINVAL;
+	}
+
+	/* check pre decryption SA lookup tables */
+	valid_tables = 0;
+	pre_sec_in_prms = &prms->pre_sec_in_params;
+	for (i = 0; i < DPA_IPSEC_MAX_SA_TYPE; i++)
+		if (pre_sec_in_prms->dpa_cls_td[i] != DPA_OFFLD_DESC_NONE) {
+			/* verify that it is not an indexed table */
+			err = dpa_classif_table_get_params(
+						pre_sec_in_prms->dpa_cls_td[i],
+						&table_params);
+			if (err < 0) {
+				pr_err("Couldn't check type of SA table\n");
+				return -EINVAL;
+			}
+
+			if (table_params.type == DPA_CLS_TBL_INDEXED) {
+				pr_err("SA tables mustn't be of type index\n");
+				return -EINVAL;
+			}
+			valid_tables++;
+		}
+	if (!valid_tables) {
+		pr_err("Specify at least one valid table for SA lookup\n");
+		return -EINVAL;
+	}
+
+	/*
+	 * verify that at least one field was selected for building inbound
+	 * policy keys
+	 */
+	if (prms->post_sec_in_params.do_pol_check &&
+	    prms->post_sec_in_params.key_fields == 0) {
+		pr_err("At least one field must be specified IN policy keys\n");
+		return -EINVAL;
+	}
+
+	/*
+	 * verify that the instance is configured
+	 * for offloading at least one SA pair
+	 */
+	if (prms->max_sa_pairs == 0) {
+		pr_err("The instance must be configured for offloading at least one SA pair\n");
+		return -EINVAL;
+	}
+
+	/* Verify the parameters of the FQID range - if one was provided */
+	if (prms->fqid_range) {
+		fqid_range_size = prms->fqid_range->end_fqid -
+						prms->fqid_range->start_fqid;
+		min_fqid_num = prms->max_sa_pairs * 2 * NUM_FQS_PER_SA;
+		if (fqid_range_size <= 0 || fqid_range_size <  min_fqid_num) {
+			pr_err("Insufficient number of FQIDs in range!\n");
+			return -EINVAL;
+		}
+	}
+
+	return 0;
+}
+
+static void calc_in_pol_key_size(struct dpa_ipsec *dpa_ipsec, uint8_t *key_size)
+{
+	uint8_t key_fields, field_mask = 0;
+	int i;
+
+	BUG_ON(!dpa_ipsec);
+	BUG_ON(!key_size);
+
+	/* default value for key_size (set now in case a failure occurs later)*/
+	*key_size = 0;
+
+	key_fields = dpa_ipsec->config.post_sec_in_params.key_fields;
+
+	for (i = 0; i < DPA_IPSEC_MAX_KEY_FIELDS; i++) {
+		field_mask = (uint8_t)(1 << i);
+		switch (key_fields & field_mask) {
+		case DPA_IPSEC_KEY_FIELD_SIP:
+			if (dpa_ipsec->config.post_sec_in_params.use_ipv6_pol)
+				*key_size += DPA_OFFLD_IPv6_ADDR_LEN_BYTES;
+			else
+				*key_size += DPA_OFFLD_IPv4_ADDR_LEN_BYTES;
+			break;
+
+		case DPA_IPSEC_KEY_FIELD_DIP:
+			if (dpa_ipsec->config.post_sec_in_params.use_ipv6_pol)
+				*key_size += DPA_OFFLD_IPv6_ADDR_LEN_BYTES;
+			else
+				*key_size += DPA_OFFLD_IPv4_ADDR_LEN_BYTES;
+			break;
+
+		case DPA_IPSEC_KEY_FIELD_PROTO:
+			*key_size += IP_PROTO_FIELD_LEN;
+			break;
+
+		case DPA_IPSEC_KEY_FIELD_SPORT:
+			*key_size += PORT_FIELD_LEN;
+			break;
+
+		case DPA_IPSEC_KEY_FIELD_DPORT:
+			*key_size += PORT_FIELD_LEN;
+			break;
+		}
+	}
+}
+
+static int create_inpol_node(struct dpa_ipsec *dpa_ipsec, void **cc_node)
+{
+	t_FmPcdCcNextEngineParams *next_engine_miss_action;
+
+	BUG_ON(!dpa_ipsec);
+	BUG_ON(!cc_node);
+
+	/* default value for cc_node (set now in case a failure occurs later) */
+	*cc_node = NULL;
+
+	memset(&cc_node_prms, 0, sizeof(cc_node_prms));
+	cc_node_prms.extractCcParams.type = e_FM_PCD_EXTRACT_NON_HDR;
+	cc_node_prms.extractCcParams.extractNonHdr.src =
+						e_FM_PCD_EXTRACT_FROM_KEY;
+	cc_node_prms.extractCcParams.extractNonHdr.action =
+						e_FM_PCD_ACTION_EXACT_MATCH;
+	cc_node_prms.extractCcParams.extractNonHdr.offset = 0;
+	cc_node_prms.extractCcParams.extractNonHdr.size =
+					      dpa_ipsec->sa_mng.inpol_key_size;
+
+	cc_node_prms.keysParams.numOfKeys = 0;
+	cc_node_prms.keysParams.keySize = dpa_ipsec->sa_mng.inpol_key_size;
+
+	next_engine_miss_action =
+			&cc_node_prms.keysParams.ccNextEngineParamsForMiss;
+	next_engine_miss_action->nextEngine = e_FM_PCD_DONE;
+
+	*cc_node = FM_PCD_MatchTableSet(dpa_ipsec->config.fm_pcd,
+					&cc_node_prms);
+	if (!*cc_node) {
+		pr_err("%s: FM_PCD_MatchTableSet failed!\n", __func__);
+		return -EBUSY;
+	}
+
+	return 0;
+}
+
+static inline void destroy_inpol_node(struct dpa_ipsec *dpa_ipsec,
+				      void *cc_node)
+{
+	t_Error fmd_err;
+
+	BUG_ON(!dpa_ipsec);
+	BUG_ON(!cc_node);
+
+	fmd_err = FM_PCD_MatchTableDelete(cc_node);
+	if (fmd_err != E_OK) {
+		pr_err("%s: FM_PCD_MatchTableDelete failed!\n", __func__);
+		pr_err("Could not free policy check CC Node\n");
+	}
+}
+
+static int create_inpol_cls_tbl(struct dpa_ipsec *dpa_ipsec,
+				void *cc_node,
+				int *td)
+{
+	struct dpa_cls_tbl_params params;
+	int err;
+
+	BUG_ON(!dpa_ipsec);
+	BUG_ON(!cc_node);
+	BUG_ON(!td);
+
+	*td = DPA_OFFLD_DESC_NONE;
+
+	memset(&params, 0, sizeof(params));
+	params.entry_mgmt = DPA_CLS_TBL_MANAGE_BY_REF;
+	params.type = DPA_CLS_TBL_EXACT_MATCH;
+	params.exact_match_params.entries_cnt = DPA_IPSEC_MAX_IN_POL_PER_SA;
+	params.exact_match_params.key_size = dpa_ipsec->sa_mng.inpol_key_size;
+	params.exact_match_params.use_priorities = true;
+	params.cc_node = cc_node;
+	err = dpa_classif_table_create(&params, td);
+	if (err < 0) {
+		pr_err("Could not create exact match tbl");
+		return err;
+	}
+
+	return 0;
+}
+
+static inline void destroy_inpol_cls_tbl(int td)
+{
+	int err;
+
+	if (td != DPA_OFFLD_DESC_NONE) {
+		err = dpa_classif_table_free(td);
+		if (err < 0)
+			pr_err("Could not free EM table\n");
+	}
+}
+
+static int get_inbound_flowid(struct dpa_ipsec *dpa_ipsec, uint16_t *flowid)
+{
+	BUG_ON(!dpa_ipsec);
+	BUG_ON(!dpa_ipsec->sa_mng.inbound_flowid_cq);
+	BUG_ON(!flowid);
+
+	if (cq_get_2bytes(dpa_ipsec->sa_mng.inbound_flowid_cq, flowid) < 0) {
+		pr_err("Could not retrieve a valid inbound flow ID\n");
+		return -EDOM;
+	}
+
+	return 0;
+}
+
+static int put_inbound_flowid(struct dpa_ipsec *dpa_ipsec, uint16_t flowid)
+{
+	BUG_ON(!dpa_ipsec);
+	BUG_ON(!dpa_ipsec->sa_mng.inbound_flowid_cq);
+
+	if (cq_put_2bytes(dpa_ipsec->sa_mng.inbound_flowid_cq, flowid) < 0) {
+		pr_err("Could not release inbound flow id\n");
+		return -EDOM;
+	}
+
+	return 0;
+}
+
+static int create_inbound_flowid_cq(struct dpa_ipsec *dpa_ipsec)
+{
+	void *cq;
+	uint16_t base_flow_id;
+	int i, err;
+
+	BUG_ON(!dpa_ipsec);
+
+	cq = cq_new(dpa_ipsec->sa_mng.max_num_sa / 2, sizeof(uint16_t));
+	if (!cq) {
+		pr_err("Could not create inbound flow ID management CQ\n");
+		return -ENOMEM;
+	}
+
+	dpa_ipsec->sa_mng.inbound_flowid_cq = cq;
+
+	/* Populate the created CQ with flow ids */
+	base_flow_id = dpa_ipsec->config.post_sec_in_params.base_flow_id;
+	for (i = base_flow_id;
+	     i < dpa_ipsec->sa_mng.max_num_sa / 2 + base_flow_id; i++) {
+		err = put_inbound_flowid(dpa_ipsec, (uint16_t) i);
+		if (err < 0) {
+			pr_err("Couldn't fill flow id management queue\n");
+			cq_delete(cq);
+			dpa_ipsec->sa_mng.inbound_flowid_cq = NULL;
+			return err;
+		}
+	}
+
+	return 0;
+}
+
+static inline void destroy_inbound_flowid_cq(struct cq *inbound_flowid_cq)
+{
+	/* sanity checks */
+	if (inbound_flowid_cq)
+		cq_delete(inbound_flowid_cq);
+}
+
+static int get_free_inbpol_tbl(struct dpa_ipsec *dpa_ipsec, int *table_desc)
+{
+	struct inpol_tbl *inpol_tbl;
+	struct list_head *head;
+	int ret = 0;
+
+	BUG_ON(!dpa_ipsec);
+	BUG_ON(!table_desc);
+
+	/* Lock inbound policy list */
+	mutex_lock(&dpa_ipsec->sa_mng.inpol_tables_lock);
+
+	head = &dpa_ipsec->sa_mng.inpol_tables;
+
+	list_for_each_entry(inpol_tbl, head, table_list)
+		if (!inpol_tbl->used)
+			break;
+
+	if (!inpol_tbl->used) {
+		BUG_ON(inpol_tbl->td < 0);
+		inpol_tbl->used = true;
+		*table_desc = inpol_tbl->td;
+	} else {
+		pr_err("No more free EM tables for inbound policy verification\n");
+		ret = -ENOMEM;
+	}
+
+	/* Unlock inbound policy list */
+	mutex_unlock(&dpa_ipsec->sa_mng.inpol_tables_lock);
+
+	return ret;
+}
+
+static void put_free_inbpol_tbl(struct dpa_ipsec *dpa_ipsec, int table_desc)
+{
+	struct inpol_tbl *inpol_tbl;
+	struct list_head *head;
+
+	BUG_ON(!dpa_ipsec);
+	BUG_ON(table_desc < 0);
+
+	/* Lock inbound policy list */
+	mutex_lock(&dpa_ipsec->sa_mng.inpol_tables_lock);
+
+	head = &dpa_ipsec->sa_mng.inpol_tables;
+
+	list_for_each_entry(inpol_tbl, head, table_list)
+	    if (inpol_tbl->td == table_desc)
+		break;
+
+	if (inpol_tbl->used)
+		inpol_tbl->used = FALSE;
+	else
+		pr_warn("Exact match table %d is not used\n", table_desc);
+
+	/* Unlock inbound policy list */
+	mutex_unlock(&dpa_ipsec->sa_mng.inpol_tables_lock);
+}
+
+static int get_free_ipsec_manip_node(struct dpa_ipsec *dpa_ipsec, void **hm)
+{
+	struct ipsec_manip_node *ipsec_manip_node;
+	struct list_head *head;
+	int ret = 0;
+
+	BUG_ON(!dpa_ipsec);
+	BUG_ON(!hm);
+
+	/*
+	 * Lock IPSec manip node list
+	 */
+	mutex_lock(&dpa_ipsec->sa_mng.ipsec_manip_node_lock);
+
+	head = &dpa_ipsec->sa_mng.ipsec_manip_node_list;
+
+	list_for_each_entry(ipsec_manip_node, head, ipsec_manip_node_list)
+		if (!ipsec_manip_node->used)
+			break;
+
+	if (!ipsec_manip_node->used) {
+		BUG_ON(!ipsec_manip_node->hm);
+		ipsec_manip_node->used = true;
+		*hm = ipsec_manip_node->hm;
+	} else {
+		pr_err("No more free IPSec manip nodes for special operations\n");
+		ret = -ENOMEM;
+	}
+
+	/*
+	 * Unlock IPSec manip node list
+	 */
+	mutex_unlock(&dpa_ipsec->sa_mng.ipsec_manip_node_lock);
+
+	return ret;
+}
+
+static void put_free_ipsec_manip_node(struct dpa_ipsec *dpa_ipsec, void *hm)
+{
+	struct ipsec_manip_node *ipsec_manip_node;
+	struct list_head *head;
+	bool found = false;
+
+	BUG_ON(!dpa_ipsec);
+	BUG_ON(!hm);
+
+	/*
+	 * Lock IPSec manip node list
+	 */
+	mutex_lock(&dpa_ipsec->sa_mng.ipsec_manip_node_lock);
+
+	head = &dpa_ipsec->sa_mng.ipsec_manip_node_list;
+
+	list_for_each_entry(ipsec_manip_node, head, ipsec_manip_node_list)
+		if (ipsec_manip_node->hm == hm) {
+			found = true;
+			break;
+		}
+
+	BUG_ON(!found);
+
+	if (ipsec_manip_node->used)
+		ipsec_manip_node->used = false;
+	else
+		pr_warn("IPSec manip node %p is not used\n", hm);
+
+	/*
+	 * Unlock IPSec manip node list
+	 */
+	mutex_unlock(&dpa_ipsec->sa_mng.ipsec_manip_node_lock);
+}
+
+static void replace_ipsec_manip_node(struct dpa_ipsec *dpa_ipsec, void *hm_old,
+				     void *hm_new)
+{
+	struct ipsec_manip_node *ipsec_manip_node;
+	struct list_head *head;
+	bool found = false;
+
+	BUG_ON(!dpa_ipsec);
+	BUG_ON(!hm_old);
+	BUG_ON(!hm_new);
+
+	/*
+	 * Lock IPSec manip node list
+	 */
+	mutex_lock(&dpa_ipsec->sa_mng.ipsec_manip_node_lock);
+
+	head = &dpa_ipsec->sa_mng.ipsec_manip_node_list;
+
+	list_for_each_entry(ipsec_manip_node, head, ipsec_manip_node_list)
+		if (ipsec_manip_node->hm == hm_old) {
+			found = true;
+			break;
+		}
+
+	BUG_ON(!found);
+
+	if (ipsec_manip_node->used)
+		ipsec_manip_node->hm = hm_new;
+	else
+		pr_warn("IPSec manip node %p is not used\n", hm_old);
+
+	/*
+	 * Unlock IPSec manip node list
+	 */
+	mutex_unlock(&dpa_ipsec->sa_mng.ipsec_manip_node_lock);
+}
+
+/* initialize fqid management CQ */
+static int create_fqid_cq(struct dpa_ipsec *dpa_ipsec)
+{
+	struct dpa_ipsec_fqid_range *fqid_range;
+	struct cq *fqid_cq;
+	int i;
+
+	BUG_ON(!dpa_ipsec);
+
+	if (dpa_ipsec->config.fqid_range) {
+		fqid_range = dpa_ipsec->config.fqid_range;
+		fqid_cq = cq_new(fqid_range->end_fqid - fqid_range->start_fqid,
+				 sizeof(uint32_t));
+		if (!fqid_cq) {
+			pr_err("Could not create CQ for FQID management!\n");
+			return -ENOMEM;
+		}
+
+		dpa_ipsec->sa_mng.fqid_cq = fqid_cq;
+
+		/* fill the CQ */
+		for (i = fqid_range->start_fqid; i < fqid_range->end_fqid; i++)
+			if (cq_put_4bytes(fqid_cq, (uint16_t)i) < 0) {
+				pr_err("Could not fill fqid management CQ!\n");
+				return -EDOM;
+			}
+	}
+
+	return 0;
+}
+
+/* destroy the FQID management CQ - if one was initialized */
+static inline void destroy_fqid_cq(struct dpa_ipsec *dpa_ipsec)
+{
+	if (dpa_ipsec->sa_mng.fqid_cq) {
+		cq_delete(dpa_ipsec->sa_mng.fqid_cq);
+		dpa_ipsec->sa_mng.fqid_cq = NULL;
+	}
+}
+
+/*
+ * Create a circular queue with id's for aquiring SA's handles
+ * Allocate a maximum number of SA internal structures to be used at runtime.
+ * Param[in]	dpa_ipsec - Instance for which SaMng is initialized
+ * Return value	0 on success. Error code otherwise.
+ * Cleanup provided by free_sa_mng().
+ */
+static int init_sa_manager(struct dpa_ipsec *dpa_ipsec)
+{
+	struct dpa_ipsec_sa_mng *sa_mng;
+	struct dpa_ipsec_sa *sa;
+	int i = 0, err;
+
+	BUG_ON(!dpa_ipsec);
+
+	sa_mng = &dpa_ipsec->sa_mng;
+	sa_mng->max_num_sa = dpa_ipsec->config.max_sa_pairs * 2;
+
+	/* Initialize the SA IPSec manip node list and its protective lock */
+	INIT_LIST_HEAD(&dpa_ipsec->sa_mng.ipsec_manip_node_list);
+	mutex_init(&sa_mng->ipsec_manip_node_lock);
+
+	INIT_LIST_HEAD(&sa_mng->inpol_tables);
+
+	/* create queue that holds free SA IDs */
+	sa_mng->sa_id_cq = cq_new(sa_mng->max_num_sa, sizeof(int));
+	if (!sa_mng->sa_id_cq) {
+		pr_err("Could not create SA IDs circular queue\n");
+		return -ENOMEM;
+	}
+
+	/* fill with ids */
+	for (i = 0; i < sa_mng->max_num_sa; i++)
+		if (cq_put_4bytes(sa_mng->sa_id_cq, i) < 0) {
+			pr_err("Could not fill SA ID management CQ\n");
+			return -EDOM;
+		}
+
+	/* initialize the circular queue for FQIDs management */
+	err = create_fqid_cq(dpa_ipsec);
+	if (err < 0) {
+		pr_err("Could not initialize FQID management mechanism!\n");
+		return err;
+	}
+
+	/* alloc SA array */
+	sa = kzalloc(sa_mng->max_num_sa * sizeof(*sa_mng->sa), GFP_KERNEL);
+	if (!sa) {
+		pr_err("Could not allocate memory for SAs\n");
+		return -ENOMEM;
+	}
+	sa_mng->sa = sa;
+
+	/* alloc cipher/auth stuff */
+	for (i = 0; i < sa_mng->max_num_sa; i++) {
+		mutex_init(&sa_mng->sa[i].lock);
+		sa[i].cipher_data.cipher_key =
+					kzalloc(MAX_CIPHER_KEY_LEN, GFP_KERNEL);
+		if (!sa[i].cipher_data.cipher_key) {
+			pr_err("Could not allocate memory for cipher key\n");
+			return -ENOMEM;
+		}
+		sa[i].auth_data.auth_key =
+					kzalloc(MAX_AUTH_KEY_LEN, GFP_KERNEL);
+		if (!sa[i].auth_data.auth_key) {
+			pr_err("Could not allocate memory for authentication key\n");
+			return -ENOMEM;
+		}
+
+		sa[i].auth_data.split_key =
+					kzalloc(MAX_AUTH_KEY_LEN, GFP_KERNEL);
+		if (!sa[i].auth_data.split_key) {
+			pr_err("Could not allocate memory for authentication split key\n");
+			return -ENOMEM;
+		}
+
+		sa[i].from_sec_fq = kzalloc(sizeof(struct qman_fq), GFP_KERNEL);
+		if (!sa[i].from_sec_fq) {
+			pr_err("Can't allocate space for 'from SEC FQ'\n");
+			return -ENOMEM;
+		}
+
+		sa[i].to_sec_fq = kzalloc(sizeof(struct qman_fq), GFP_KERNEL);
+		if (!sa[i].to_sec_fq) {
+			pr_err("Can't allocate space for 'to SEC FQ'\n");
+			return -ENOMEM;
+		}
+
+		/* Allocate space for the SEC descriptor which is holding the
+		 * preheader information and the share descriptor.
+		 * Required 64 byte align.
+		 */
+		sa[i].sec_desc_unaligned =
+			kzalloc(sizeof(struct sec_descriptor) + 64, GFP_KERNEL);
+		if (!sa[i].sec_desc_unaligned) {
+			pr_err("Could not allocate memory for SEC descriptor\n");
+			return -ENOMEM;
+		}
+		sa[i].sec_desc = PTR_ALIGN(sa[i].sec_desc_unaligned, 64);
+
+		/* Allocate space for extra material space in case when the
+		 * descriptor is greater than 64 words */
+		sa[i].sec_desc_extra_cmds_unaligned =
+			kzalloc(2 * MAX_EXTRA_DESC_COMMANDS + L1_CACHE_BYTES,
+				GFP_KERNEL);
+		if (!sa[i].sec_desc_extra_cmds_unaligned) {
+			pr_err("Allocation failed for CAAM extra commands\n");
+			return -ENOMEM;
+		}
+		sa[i].sec_desc_extra_cmds =
+				PTR_ALIGN(sa[i].sec_desc_extra_cmds_unaligned,
+					  L1_CACHE_BYTES);
+		if (sa[i].sec_desc_extra_cmds_unaligned ==
+		    sa[i].sec_desc_extra_cmds)
+			sa[i].sec_desc_extra_cmds += L1_CACHE_BYTES;
+
+		/*
+		 * Allocate space for the SEC replacement job descriptor
+		 * Required 64 byte alignment
+		 */
+		sa[i].rjob_desc_unaligned =
+			kzalloc(MAX_CAAM_DESCSIZE * sizeof(uint32_t) + 64,
+				GFP_KERNEL);
+		if (!sa[i].rjob_desc_unaligned) {
+			pr_err("No memory for replacement job descriptor\n");
+			return -ENOMEM;
+		}
+		sa[i].rjob_desc = PTR_ALIGN(sa[i].rjob_desc_unaligned, 64);
+
+		/*
+		 * Initialize the policy parameter list which will hold all
+		 * inbound or outbound policy parameters which were use to
+		 * generate PCD entries
+		 */
+		INIT_LIST_HEAD(&sa[i].policy_headlist);
+
+		/* init the inbound SA lookup table desc with an invalid value*/
+		sa[i].inbound_sa_td = DPA_OFFLD_DESC_NONE;
+	}
+
+	err = create_inbound_flowid_cq(dpa_ipsec);
+	if (err < 0) {
+		pr_err("Could not create inbound policy flow id cq\n");
+		return err;
+	}
+
+	/*
+	 * If policy check is enabled than for every possible inbound SA create
+	 * an Exact Match Table and link it to the Inbound Index Table
+	 */
+	if (dpa_ipsec->config.post_sec_in_params.do_pol_check == true) {
+		struct inpol_tbl *pol_table;
+		void *cc_node;
+
+		/* calculate key size for policy verification tables */
+		calc_in_pol_key_size(dpa_ipsec,
+				     &dpa_ipsec->sa_mng.inpol_key_size);
+
+		if (dpa_ipsec->sa_mng.inpol_key_size == 0) {
+			pr_err("Invalid argument: in policy table key size\n");
+			return -EFAULT;
+		}
+
+		mutex_init(&sa_mng->inpol_tables_lock);
+
+		mutex_lock(&sa_mng->inpol_tables_lock);
+		for (i = 0; i < dpa_ipsec->config.max_sa_pairs; i++) {
+			pol_table = kzalloc(sizeof(*pol_table), GFP_KERNEL);
+			if (!pol_table) {
+				pr_err("Could not allocate memory for policy table");
+				mutex_unlock(&sa_mng->inpol_tables_lock);
+				return -ENOMEM;
+			}
+
+			/* create cc node for inbound policy */
+			err = create_inpol_node(dpa_ipsec, &cc_node);
+			if (err < 0) {
+				pr_err("Could not create cc node for EM table\n");
+				kfree(pol_table);
+				mutex_unlock(&sa_mng->inpol_tables_lock);
+				return err;
+			}
+			pol_table->cc_node = cc_node;
+			err = create_inpol_cls_tbl(dpa_ipsec,
+						   cc_node,
+						   &pol_table->td);
+			if (err < 0) {
+				pr_err("Failed create in policy table\n");
+				destroy_inpol_node(dpa_ipsec, cc_node);
+				kfree(pol_table);
+				mutex_unlock(&sa_mng->inpol_tables_lock);
+				return err;
+			}
+
+			list_add(&pol_table->table_list,
+				 &dpa_ipsec->sa_mng.inpol_tables);
+		}
+		mutex_unlock(&sa_mng->inpol_tables_lock);
+	}
+
+	/* Populate the list of IPSec manip node */
+	mutex_lock(&sa_mng->ipsec_manip_node_lock);
+	for (i = 0; i < dpa_ipsec->config.max_sa_manip_ops; i++) {
+		struct ipsec_manip_node *node;
+		node = kzalloc(sizeof(*node), GFP_KERNEL);
+		if (!node) {
+			pr_err("Could not allocate memory for IPSec manip node\n");
+			mutex_unlock(&sa_mng->ipsec_manip_node_lock);
+			return -ENOMEM;
+		}
+
+		node->hm = alloc_ipsec_manip(dpa_ipsec);
+		if (!node->hm) {
+			pr_err("Could not create IPSec manip node\n");
+			kfree(node);
+			mutex_unlock(&sa_mng->ipsec_manip_node_lock);
+			return -ENOMEM;
+		}
+
+		node->used = false;
+		list_add(&node->ipsec_manip_node_list,
+			 &dpa_ipsec->sa_mng.ipsec_manip_node_list);
+
+	}
+	mutex_unlock(&sa_mng->ipsec_manip_node_lock);
+
+	/* Initialize the SA rekeying list and its protective lock */
+	INIT_LIST_HEAD(&dpa_ipsec->sa_mng.sa_rekeying_headlist);
+	mutex_init(&sa_mng->sa_rekeying_headlist_lock);
+
+	/* Creating a single thread work queue used to defer work when there are
+	 * inbound SA's in rekeying process */
+	dpa_ipsec->sa_mng.sa_rekeying_wq =
+		create_singlethread_workqueue("sa_rekeying_wq");
+	if (!dpa_ipsec->sa_mng.sa_rekeying_wq) {
+		pr_err("Creating SA rekeying work queue failed\n");
+		return -ENOSPC;
+	}
+
+	/* Initialize the work needed to be done rekeying inbound process */
+	INIT_DELAYED_WORK(&dpa_ipsec->sa_mng.sa_rekeying_work,
+			  sa_rekeying_work_func);
+
+	return 0;
+}
+
+/* cleanup SA manager */
+static void free_sa_mng(struct dpa_ipsec *dpa_ipsec)
+{
+	struct dpa_ipsec_sa_mng *sa_mng;
+	struct inpol_tbl *pol_tbl, *tmp;
+	struct list_head *head;
+	struct list_head *pos, *n;
+	struct ipsec_manip_node *node;
+	int i = 0;
+
+	/* sanity checks */
+	if (!dpa_ipsec) {
+		pr_err("Invalid argument: NULL DPA IPSec instance\n");
+		return;
+	}
+
+	sa_mng = (struct dpa_ipsec_sa_mng *)&dpa_ipsec->sa_mng;
+	/* Remove the DPA IPsec created tables for policy verification */
+	if (dpa_ipsec->config.post_sec_in_params.do_pol_check) {
+		head = &sa_mng->inpol_tables;
+		list_for_each_entry_safe(pol_tbl, tmp, head, table_list) {
+			destroy_inpol_cls_tbl(pol_tbl->td);
+			list_del(&pol_tbl->table_list);
+			destroy_inpol_node(dpa_ipsec, pol_tbl->cc_node);
+			kfree(pol_tbl);
+		}
+	}
+
+	/* dealloc cipher/auth stuff */
+	if (sa_mng->sa) {
+		for (i = 0; i < sa_mng->max_num_sa; i++) {
+			kfree(sa_mng->sa[i].cipher_data.cipher_key);
+			sa_mng->sa[i].cipher_data.cipher_key = NULL;
+
+			kfree(sa_mng->sa[i].auth_data.auth_key);
+			sa_mng->sa[i].auth_data.auth_key = NULL;
+
+			kfree(sa_mng->sa[i].auth_data.split_key);
+			sa_mng->sa[i].auth_data.split_key = NULL;
+
+			kfree(sa_mng->sa[i].from_sec_fq);
+			sa_mng->sa[i].from_sec_fq = NULL;
+
+			kfree(sa_mng->sa[i].to_sec_fq);
+			sa_mng->sa[i].to_sec_fq = NULL;
+
+			kfree(sa_mng->sa[i].sec_desc_unaligned);
+			sa_mng->sa[i].sec_desc_unaligned = NULL;
+			sa_mng->sa[i].sec_desc = NULL;
+
+			kfree(sa_mng->sa[i].sec_desc_extra_cmds_unaligned);
+			sa_mng->sa[i].sec_desc_extra_cmds_unaligned = NULL;
+
+			kfree(sa_mng->sa[i].rjob_desc_unaligned);
+			sa_mng->sa[i].rjob_desc_unaligned = NULL;
+		}
+
+		kfree(sa_mng->sa);
+		sa_mng->sa = NULL;
+	}
+
+	/* release SA ID management CQ */
+	if (sa_mng->sa_id_cq) {
+		cq_delete(sa_mng->sa_id_cq);
+		sa_mng->sa_id_cq = NULL;
+	}
+
+	/* destroy fqid management CQ */
+	destroy_fqid_cq(dpa_ipsec);
+
+	/* release inbound flow ID management CQ */
+	destroy_inbound_flowid_cq(dpa_ipsec->sa_mng.inbound_flowid_cq);
+	dpa_ipsec->sa_mng.inbound_flowid_cq = NULL;
+
+	/* destroy rekeying workqueue */
+	if (sa_mng->sa_rekeying_wq) {
+		destroy_workqueue(sa_mng->sa_rekeying_wq);
+		sa_mng->sa_rekeying_wq = NULL;
+	}
+
+	/* cleanup hmanips */
+	list_for_each_safe(pos, n, &dpa_ipsec->sa_mng.ipsec_manip_node_list) {
+		node = container_of(pos, struct ipsec_manip_node,
+				    ipsec_manip_node_list);
+		list_del(&node->ipsec_manip_node_list);
+		kfree(node->hm);
+		kfree(node);
+	}
+}
+
+/* cleanup Ipsec */
+static void free_resources(void)
+{
+	struct dpa_ipsec *dpa_ipsec;
+
+	/* sanity checks */
+	if (!gbl_dpa_ipsec) {
+		pr_err("There is no DPA IPSec instance initialized\n");
+		return;
+	}
+	dpa_ipsec = gbl_dpa_ipsec;
+
+	/* free all SA related stuff */
+	free_sa_mng(dpa_ipsec);
+
+	kfree(dpa_ipsec->used_sa_ids);
+	kfree(dpa_ipsec);
+	gbl_dpa_ipsec = NULL;
+}
+
+/* Runtime functions */
+
+/* Convert prefixLen into IP address's netmask. */
+static int set_ip_addr_mask(uint8_t *mask, uint8_t prefix_len,
+			     uint8_t mask_len)
+{
+	static const uint8_t mask_bits[] = {0x00, 0x80, 0xc0, 0xe0, 0xf0, 0xf8,
+					    0xfc, 0xfe, 0xff};
+	uint8_t bit, off;
+
+	BUG_ON(!mask);
+
+	off = prefix_len / 8;
+	bit = prefix_len % 8;
+	while (off--)
+		*mask++ = 0xff;
+	if (bit)
+		*mask = mask_bits[bit];
+
+	return 0;
+}
+
+static int set_flow_id_action(struct dpa_ipsec_sa *sa,
+			      struct dpa_cls_tbl_action *action)
+{
+	struct dpa_ipsec *dpa_ipsec;
+	struct dpa_offload_lookup_key tbl_key;
+	struct dpa_cls_tbl_entry_mod_params mod_params;
+	int table, err;
+	uint8_t key_data;
+
+	BUG_ON(!sa);
+	BUG_ON(!action);
+
+	dpa_ipsec = sa->dpa_ipsec;
+
+	/* Currently the flowid cannot be greater than 255 */
+	key_data	= (uint8_t)sa->inbound_flowid;
+
+	memset(&tbl_key, 0, sizeof(tbl_key));
+	tbl_key.byte	= &key_data;
+	tbl_key.mask	= NULL;
+	tbl_key.size	= sizeof(uint8_t);
+
+	memset(&mod_params, 0, sizeof(mod_params));
+	mod_params.action = action;
+	mod_params.type = DPA_CLS_TBL_MODIFY_ACTION;
+	table = dpa_ipsec->config.post_sec_in_params.dpa_cls_td;
+	err = dpa_classif_table_modify_entry_by_key(table, &tbl_key,
+						    &mod_params);
+	if (err < 0) {
+		pr_err("Couldn't set flowID action for SA id %d\n", sa->id);
+		return err;
+	}
+	sa->valid_flowid_entry = true;
+
+	return 0;
+}
+
+static int fill_policy_key(int td,
+			   struct dpa_ipsec_policy_params *pol_params,
+			   uint8_t key_fields,
+			   uint8_t *key, uint8_t *mask, uint8_t *key_len)
+{
+	struct dpa_cls_tbl_params tbl_params;
+	uint8_t offset = 0, field_mask = 0, tbl_key_size = 0;
+	int err = 0, i;
+
+	BUG_ON(!pol_params);
+	BUG_ON(!key);
+	BUG_ON(!mask);
+	BUG_ON(!key_len);
+
+	/* Fill in the key components */
+	for (i = 0; i < DPA_IPSEC_MAX_KEY_FIELDS; i++) {
+		field_mask = (uint8_t) (1 << i);
+		switch (key_fields & field_mask) {
+		case DPA_IPSEC_KEY_FIELD_SIP:
+			memcpy(key + offset,
+			       IP_ADDR(pol_params->src_addr),
+			       IP_ADDR_LEN(pol_params->src_addr));
+			err = set_ip_addr_mask(mask + offset,
+					     pol_params->src_prefix_len,
+					     IP_ADDR_LEN(pol_params->src_addr));
+			if (err < 0)
+				return err;
+			offset += IP_ADDR_LEN(pol_params->src_addr);
+			break;
+
+		case DPA_IPSEC_KEY_FIELD_DIP:
+			memcpy(key + offset,
+			       IP_ADDR(pol_params->dest_addr),
+			       IP_ADDR_LEN(pol_params->dest_addr));
+			err = set_ip_addr_mask(mask + offset,
+					    pol_params->dest_prefix_len,
+					    IP_ADDR_LEN(pol_params->dest_addr));
+			if (err < 0)
+				return err;
+			offset += IP_ADDR_LEN(pol_params->dest_addr);
+			break;
+
+		case DPA_IPSEC_KEY_FIELD_PROTO:
+		      SET_BYTE_VAL_IN_ARRAY(key, offset, pol_params->protocol);
+		      SET_IP_PROTO_MASK(mask, offset, pol_params->masked_proto);
+			offset += IP_PROTO_FIELD_LEN;
+			break;
+
+		/* case DPA_IPSEC_KEY_FIELD_ICMP_TYPE: */
+		case DPA_IPSEC_KEY_FIELD_SPORT:
+			if ((pol_params->protocol == IPPROTO_ICMP) ||
+			   (pol_params->protocol == IPPROTO_ICMPV6)) {
+				SET_BYTE_VAL_IN_ARRAY(key, offset,
+						    pol_params->icmp.icmp_type);
+				SET_BYTE_VAL_IN_ARRAY(mask, offset,
+					       pol_params->icmp.icmp_type_mask);
+				offset += ICMP_HDR_FIELD_LEN;
+			} else {
+				memcpy(key + offset,
+				       (uint8_t *) &(pol_params->l4.src_port),
+				       PORT_FIELD_LEN);
+				SET_L4_PORT_MASK(mask, offset,
+						 pol_params->l4.src_port_mask);
+				offset += PORT_FIELD_LEN;
+			}
+			break;
+
+		/* case DPA_IPSEC_KEY_FIELD_ICMP_CODE: */
+		case DPA_IPSEC_KEY_FIELD_DPORT:
+			if ((pol_params->protocol == IPPROTO_ICMP) ||
+			   (pol_params->protocol == IPPROTO_ICMPV6)) {
+				SET_BYTE_VAL_IN_ARRAY(key, offset,
+						    pol_params->icmp.icmp_code);
+				SET_BYTE_VAL_IN_ARRAY(mask, offset,
+					       pol_params->icmp.icmp_code_mask);
+				offset += ICMP_HDR_FIELD_LEN;
+			} else {
+				memcpy(key + offset,
+				       (uint8_t *) &(pol_params->l4.dest_port),
+				       PORT_FIELD_LEN);
+				SET_L4_PORT_MASK(mask, offset,
+						 pol_params->l4.dest_port_mask);
+				offset += PORT_FIELD_LEN;
+			}
+			break;
+		}
+	}
+
+	/*
+	 * Add padding to compensate difference in size between table maximum
+	 * key size and computed key size.
+	 */
+
+	/* get table params (including maximum key size) */
+	err = dpa_classif_table_get_params(td, &tbl_params);
+	if (err < 0) {
+		pr_err("Could not retrieve table maximum key size\n");
+		return -EINVAL;
+	}
+	tbl_key_size = TABLE_KEY_SIZE(tbl_params);
+
+	if (tbl_key_size < offset) {
+		pr_err("Policy key is greater than maximum table key size\n");
+		return -EINVAL;
+	}
+
+	if (tbl_key_size > offset) {
+		for (i = 0; i < tbl_key_size - offset; i++) {
+			*(key + offset + i) = DPA_IPSEC_DEF_PAD_VAL;
+			/* ignore padding during classification (mask it) */
+			*(mask + offset + i) = 0x00;
+		}
+		offset = tbl_key_size;
+	}
+
+	/* Store key length */
+	*key_len = offset;
+
+	return 0;
+}
+
+/*
+ * fill dpa_cls_action structure with common values
+ * if new_fqid = 0, the FQID will not be overridden
+ */
+static inline void fill_cls_action_enq(struct dpa_cls_tbl_action *action_prm,
+				       int en_stats, uint32_t new_fqid,
+				       int hmd)
+{
+	action_prm->type = DPA_CLS_TBL_ACTION_ENQ;
+	action_prm->enable_statistics = en_stats;
+	if (new_fqid != 0) {
+		action_prm->enq_params.new_fqid = new_fqid;
+		action_prm->enq_params.override_fqid = true;
+	} else
+		action_prm->enq_params.override_fqid = FALSE;
+	action_prm->enq_params.policer_params = NULL;
+	action_prm->enq_params.hmd = hmd;
+}
+
+static inline void fill_cls_action_drop(struct dpa_cls_tbl_action *action,
+					int en_stats)
+{
+	memset(action, 0, sizeof(struct dpa_cls_tbl_action));
+	action->type = DPA_CLS_TBL_ACTION_DROP;
+	action->enable_statistics = en_stats;
+}
+
+/* Used at runtime when preallocation of IPSec manip node is not enabled */
+static int create_ipsec_manip(struct dpa_ipsec_sa *sa, int next_hmd, int *hmd)
+{
+	t_FmPcdManipParams pcd_manip_params;
+	t_FmPcdManipSpecialOffloadParams *offld_params;
+	t_Handle hm;
+	int err;
+
+	BUG_ON(!sa);
+	BUG_ON(!hmd);
+
+	if (!sa->use_var_iphdr_len && !sa->dscp_copy && !sa->ecn_copy &&
+	    !(sa_is_outbound(sa) && sa->enable_dpovrd)) {
+		/* no need to create a new manipulation objects chain */
+		*hmd = next_hmd;
+		return 0;
+	}
+
+	memset(&pcd_manip_params, 0, sizeof(struct t_FmPcdManipParams));
+	pcd_manip_params.type = e_FM_PCD_MANIP_SPECIAL_OFFLOAD;
+	offld_params = &pcd_manip_params.u.specialOffload;
+	offld_params->type = e_FM_PCD_MANIP_SPECIAL_OFFLOAD_IPSEC;
+	if (sa_is_inbound(sa)) {
+		offld_params->u.ipsec.decryption = true;
+		offld_params->u.ipsec.variableIpHdrLen = sa->use_var_iphdr_len;
+	} else {
+		offld_params->u.ipsec.variableIpVersion = true;
+		offld_params->u.ipsec.outerIPHdrLen = (uint8_t)
+						sa->sec_desc->pdb_en.ip_hdr_len;
+	}
+	offld_params->u.ipsec.ecnCopy = sa->ecn_copy;
+	offld_params->u.ipsec.dscpCopy = sa->dscp_copy;
+
+	pcd_manip_params.h_NextManip = dpa_classif_hm_lock_chain(next_hmd);
+	dpa_classif_hm_release_chain(next_hmd);
+
+	hm = FM_PCD_ManipNodeSet(sa->dpa_ipsec->config.fm_pcd,
+				 &pcd_manip_params);
+	if (!hm) {
+		pr_err("%s: FM_PCD_ManipNodeSet failed!\n", __func__);
+		return -EBUSY;
+	}
+
+	err = dpa_classif_import_static_hm(hm, next_hmd, hmd);
+	if (err < 0)
+		pr_err("%s: Failed to import header manipulation into DPA "
+			"Classifier.\n", __func__);
+
+	return err;
+}
+
+/*
+ * If preallocation of IPSec manip node(s) was specified the code is using this
+ * function to allocate and populate the list of IPSec manip objects
+ */
+static void *alloc_ipsec_manip(struct dpa_ipsec *dpa_ipsec)
+{
+	t_FmPcdManipParams pcd_manip_params;
+	t_FmPcdManipSpecialOffloadParams *offld_params;
+	t_Handle hm;
+
+	BUG_ON(!dpa_ipsec);
+
+	memset(&pcd_manip_params, 0, sizeof(struct t_FmPcdManipParams));
+	pcd_manip_params.type = e_FM_PCD_MANIP_SPECIAL_OFFLOAD;
+	offld_params = &pcd_manip_params.u.specialOffload;
+	offld_params->type = e_FM_PCD_MANIP_SPECIAL_OFFLOAD_IPSEC;
+	offld_params->u.ipsec.decryption = true;
+	offld_params->u.ipsec.variableIpHdrLen = false;
+	offld_params->u.ipsec.ecnCopy = false;
+	offld_params->u.ipsec.dscpCopy = false;
+	offld_params->u.ipsec.variableIpVersion = false;
+	offld_params->u.ipsec.outerIPHdrLen = 0;
+	pcd_manip_params.h_NextManip = NULL;
+
+	hm = FM_PCD_ManipNodeSet(dpa_ipsec->config.fm_pcd, &pcd_manip_params);
+	if (!hm) {
+		pr_err("%s: FM_PCD_ManipSetNode failed!\n", __func__);
+		return NULL;
+	}
+
+	return hm;
+}
+
+/* Used at runtime when preallocation of IPSec manip nodes was enabled */
+static int update_ipsec_manip(struct dpa_ipsec_sa *sa, int next_hmd, int *hmd)
+{
+	t_FmPcdManipParams pcd_manip_params;
+	t_FmPcdManipSpecialOffloadParams *offld_params;
+	t_Handle ipsec_hm = NULL, new_hm = NULL;
+	t_Error err;
+	int ret;
+
+	BUG_ON(!sa);
+	BUG_ON(!hmd);
+
+	if (!sa->use_var_iphdr_len && !sa->dscp_copy && !sa->ecn_copy &&
+	    !(sa_is_outbound(sa) && sa->enable_dpovrd)) {
+		/* no need to create a new manipulation objects chain */
+		*hmd = next_hmd;
+		return 0;
+	}
+
+	memset(&pcd_manip_params, 0, sizeof(struct t_FmPcdManipParams));
+	pcd_manip_params.type = e_FM_PCD_MANIP_SPECIAL_OFFLOAD;
+	offld_params = &pcd_manip_params.u.specialOffload;
+	offld_params->type = e_FM_PCD_MANIP_SPECIAL_OFFLOAD_IPSEC;
+	if (sa_is_inbound(sa)) {
+		offld_params->u.ipsec.decryption = true;
+		offld_params->u.ipsec.variableIpHdrLen = sa->use_var_iphdr_len;
+	} else {
+		offld_params->u.ipsec.variableIpVersion = true;
+		offld_params->u.ipsec.outerIPHdrLen = (uint8_t)
+						sa->sec_desc->pdb_en.ip_hdr_len;
+	}
+	offld_params->u.ipsec.ecnCopy = sa->ecn_copy;
+	offld_params->u.ipsec.dscpCopy = sa->dscp_copy;
+
+	pcd_manip_params.h_NextManip = dpa_classif_hm_lock_chain(next_hmd);
+	dpa_classif_hm_release_chain(next_hmd);
+
+	ret = get_free_ipsec_manip_node(sa->dpa_ipsec, &ipsec_hm);
+	if (ret < 0) {
+		pr_err("%s: get_free_ipsec_manip_node failed for %s SA %d!\n",
+			__func__, sa_is_inbound(sa) ?
+			"inbound" : "outbound", sa->id);
+		return ret;
+	}
+
+	/* Should not be NULL */
+	BUG_ON(!ipsec_hm);
+
+	new_hm = FM_PCD_ManipNodeSet(sa->dpa_ipsec->config.fm_pcd,
+				     &pcd_manip_params);
+	if (!new_hm) {
+		pr_err("%s: FM_PCD_ManipSetNode failed!\n", __func__);
+		put_free_ipsec_manip_node(sa->dpa_ipsec, ipsec_hm);
+		return -EBUSY;
+	}
+
+	replace_ipsec_manip_node(sa->dpa_ipsec, ipsec_hm, new_hm);
+
+	err = FM_PCD_ManipNodeDelete(ipsec_hm);
+	if (err != E_OK) {
+		pr_err("%s: FM_PCD_ManipNodeDelete failed for %s SA %d!\n",
+			__func__, sa_is_inbound(sa) ?
+			"inbound" : "outbound", sa->id);
+		put_free_ipsec_manip_node(sa->dpa_ipsec, new_hm);
+		return -EBUSY;
+	}
+
+	ret = dpa_classif_import_static_hm(new_hm, next_hmd, hmd);
+	if (ret < 0) {
+		pr_err("%s: Failed to import header manipulation into DPA "
+			"Classifier.\n", __func__);
+		put_free_ipsec_manip_node(sa->dpa_ipsec, new_hm);
+	}
+
+	return ret;
+}
+
+/* Destroy the DPA IPSec Special header manip or put it in the pool */
+static int destroy_recycle_manip(struct dpa_ipsec_sa *sa,
+				 struct hmd_entry *entry)
+{
+	t_Handle hm;
+	int hmd, err = 0;
+
+	BUG_ON(!sa);
+	BUG_ON(!entry);
+
+	hmd = entry->hmd;
+	BUG_ON(hmd == DPA_OFFLD_DESC_NONE);
+
+	hm = dpa_classif_get_static_hm_handle(hmd);
+	BUG_ON(!hm);
+
+	if (sa->dpa_ipsec->config.max_sa_manip_ops > 0) {
+		/* return to pool */
+		put_free_ipsec_manip_node(sa->dpa_ipsec, hm);
+		goto remove_hm;
+	}
+
+	if (entry->hmd_special_op) {
+		/*
+		 * Destroy only the IPSec special operation that was created
+		 * inside IPSec
+		 */
+		err = FM_PCD_ManipNodeDelete(hm);
+		if (err != E_OK) {
+			pr_err("%s: FM_PCD_ManipNodeDelete failed for SA %d!\n",
+				__func__, sa->id);
+			return -EBUSY;
+		}
+	}
+
+	/* Removed from classifier but not from memory. HM is still usable */
+remove_hm:
+	err = dpa_classif_free_hm(hmd);
+	if (err < 0) {
+		pr_err("%s: Failed to remove header manip!\n", __func__);
+		return err;
+	}
+
+	return 0;
+}
+
+static int update_inbound_policy(struct dpa_ipsec_sa *sa,
+				 struct dpa_ipsec_policy_entry *policy_entry,
+				 enum mng_op_type op_type)
+{
+	struct dpa_ipsec *dpa_ipsec;
+	struct dpa_ipsec_policy_params *pol_params;
+	uint8_t key_len;
+	struct dpa_cls_tbl_action *action;
+	struct dpa_offload_lookup_key tbl_key;
+	uint8_t key_data[DPA_OFFLD_MAXENTRYKEYSIZE];
+	uint8_t mask_data[DPA_OFFLD_MAXENTRYKEYSIZE];
+	int entry_id, err;
+
+	BUG_ON(!sa);
+	BUG_ON(!policy_entry);
+
+	memset(key_data, 0, DPA_OFFLD_MAXENTRYKEYSIZE);
+	memset(mask_data, 0, DPA_OFFLD_MAXENTRYKEYSIZE);
+	if (sa->em_inpol_td < 0) {
+		pr_err("Invalid exact match table for SA %d.\n", sa->id);
+		return -EINVAL;
+	}
+
+	dpa_ipsec = sa->dpa_ipsec;
+	BUG_ON(!dpa_ipsec);
+	pol_params = &policy_entry->pol_params;
+
+	switch (op_type) {
+	case MNG_OP_ADD:
+		tbl_key.byte = key_data;
+		tbl_key.mask = mask_data;
+
+		/*
+		 * Key contains:
+		 * IP SRC ADDR	- from Policy handle
+		 * IP DST ADDR	- from Policy handle
+		 * IP_PROTO	- from Policy handle
+		 * SRC_PORT	- from Policy handle (for UDP & TCP)
+		 * DST_PORT	- from Policy handle (for UDP & TCP)
+		 */
+		err = fill_policy_key(sa->em_inpol_td,
+				      pol_params,
+				      dpa_ipsec->config.post_sec_in_params.
+				      key_fields, tbl_key.byte, tbl_key.mask,
+				      &key_len);
+		if (err < 0)
+			return err;
+
+		tbl_key.size = key_len;
+
+		if (pol_params->dir_params.type == DPA_IPSEC_POL_DIR_PARAMS_ACT)
+			action = &pol_params->dir_params.in_action;
+		else
+			action = &sa->def_sa_action;
+		err = dpa_classif_table_insert_entry(sa->em_inpol_td, &tbl_key,
+					      action,
+					      policy_entry->pol_params.priority,
+					      &entry_id);
+		if (err < 0) {
+			pr_err("Could not insert key in EM table\n");
+			return err;
+		}
+		policy_entry->entry_id = entry_id;
+		break;
+	case MNG_OP_REMOVE:
+		entry_id = policy_entry->entry_id;
+		err = dpa_classif_table_delete_entry_by_ref(sa->em_inpol_td,
+							    entry_id);
+		if (err < 0) {
+			pr_err("Could not remove key in EM table\n");
+			return err;
+		}
+		break;
+	case MNG_OP_MODIFY:
+		pr_err("Modify operation unsupported for IN Policy PCD\n");
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int update_outbound_policy(struct dpa_ipsec_sa *sa,
+				  struct dpa_ipsec_policy_entry *policy_entry,
+				  enum mng_op_type op_type)
+{
+	struct dpa_ipsec *dpa_ipsec;
+	struct dpa_ipsec_pre_sec_out_params *pre_sec_out_params;
+	struct dpa_ipsec_policy_params *pol_params;
+	uint8_t key_len, table_idx, key_fields;
+	struct dpa_offload_lookup_key tbl_key;
+	struct dpa_cls_tbl_action action;
+	struct dpa_cls_tbl_entry_mod_params params;
+	int table, err;
+	int manip_hmd = DPA_OFFLD_DESC_NONE, pol_hmd = DPA_OFFLD_DESC_NONE;
+	uint8_t key_data[DPA_OFFLD_MAXENTRYKEYSIZE];
+	uint8_t mask_data[DPA_OFFLD_MAXENTRYKEYSIZE];
+
+	BUG_ON(!sa);
+	BUG_ON(!policy_entry);
+
+	memset(key_data, 0, DPA_OFFLD_MAXENTRYKEYSIZE);
+	memset(mask_data, 0, DPA_OFFLD_MAXENTRYKEYSIZE);
+	dpa_ipsec = sa->dpa_ipsec;
+	BUG_ON(!dpa_ipsec);
+	pre_sec_out_params = &dpa_ipsec->config.pre_sec_out_params;
+
+	pol_params = &policy_entry->pol_params;
+	if (IP_ADDR_TYPE_IPV4(pol_params->dest_addr))
+		table_idx = GET_POL_TABLE_IDX(pol_params->protocol, IPV4);
+	else
+		table_idx = GET_POL_TABLE_IDX(pol_params->protocol, IPV6);
+	table = pre_sec_out_params->table[table_idx].dpa_cls_td;
+	key_fields = pre_sec_out_params->table[table_idx].key_fields;
+
+	/*
+	 * check if a valid desc for a proto specific table or an ANY table was
+	 * provided
+	 */
+	if (table == DPA_OFFLD_DESC_NONE) {
+		pr_err("No suitable table found for this policy type!\n");
+		return -EBADF;
+	}
+
+	switch (op_type) {
+	case MNG_OP_ADD:
+		tbl_key.byte = key_data;
+		tbl_key.mask = mask_data;
+
+		/*
+		 * Key may contain:
+		 * IP SRC ADDR  - from Policy handle
+		 * IP DST ADDR  - from Policy handle
+		 * IP_PROTO     - from Policy handle
+		 * SRC_PORT     - from Policy handle (for UDP & TCP & SCTP)
+		 * DST_PORT     - from Policy handle (for UDP & TCP & SCTP)
+		 */
+		err = fill_policy_key(table, pol_params, key_fields,
+				tbl_key.byte, tbl_key.mask, &key_len);
+		if (err < 0)
+			return err;
+
+		tbl_key.size = key_len;
+
+		/* Configure fragmentation */
+		if (pol_params->dir_params.type ==
+					DPA_IPSEC_POL_DIR_PARAMS_MANIP) {
+			manip_hmd = pol_params->dir_params.manip_desc;
+			/*
+			 * check_policy_params validated manip descriptor
+			 */
+			BUG_ON(manip_hmd < 0);
+		}
+
+		/* Init IPSec Manip. object (if required) for outbound policy */
+		if (manip_hmd == DPA_OFFLD_DESC_NONE)
+			goto no_frag_or_manip;
+
+		/* need to chain the IPSec Manip and Frag/Manip */
+		if (sa->dpa_ipsec->config.max_sa_manip_ops == 0)
+			err = create_ipsec_manip(sa, manip_hmd,
+						 &policy_entry->hmd);
+		else
+			err = update_ipsec_manip(sa, manip_hmd,
+						 &policy_entry->hmd);
+		if (err < 0) {
+			pr_err("Couldn't create policy manip chain!\n");
+			return err;
+		}
+
+		pol_hmd = policy_entry->hmd;
+		if (pol_hmd == manip_hmd)
+			policy_entry->hmd_special_op = false;
+		else
+			policy_entry->hmd_special_op = true;
+
+		goto set_manipulation;
+
+no_frag_or_manip:
+		if (sa->ipsec_hmd == DPA_OFFLD_DESC_NONE) {
+			/*
+			 * need to create the IPSec Manip (per SA),
+			 * if it was not created earlier
+			 */
+			if (sa->dpa_ipsec->config.max_sa_manip_ops == 0)
+				err = create_ipsec_manip(sa,
+							 DPA_OFFLD_DESC_NONE,
+							 &sa->ipsec_hmd);
+			else
+				err = update_ipsec_manip(sa,
+							 DPA_OFFLD_DESC_NONE,
+							 &sa->ipsec_hmd);
+			if (err < 0) {
+				pr_err("Couldn't create SA manip!\n");
+				return err;
+			}
+		}
+		pol_hmd = sa->ipsec_hmd;
+
+set_manipulation:
+		memset(&action, 0, sizeof(action));
+		fill_cls_action_enq(&action, FALSE,
+				    qman_fq_fqid(sa->to_sec_fq), pol_hmd);
+
+		err = dpa_classif_table_insert_entry(table, &tbl_key, &action,
+					      policy_entry->pol_params.priority,
+					      &policy_entry->entry_id);
+		if (err < 0) {
+			pr_err("Could not add key in exact match table\n");
+			return err;
+		}
+		break;
+	case MNG_OP_REMOVE:
+		err = dpa_classif_table_delete_entry_by_ref(table,
+							policy_entry->entry_id);
+		if (err < 0) {
+			pr_err("Could not remove key from EM table\n");
+			return err;
+		}
+
+		if (policy_entry->hmd != DPA_OFFLD_DESC_NONE) {
+			struct hmd_entry hmd_entry;
+			hmd_entry.hmd = policy_entry->hmd;
+			hmd_entry.hmd_special_op = policy_entry->hmd_special_op;
+			err = destroy_recycle_manip(sa, &hmd_entry);
+			if (err < 0) {
+				pr_err("Couldn't delete frag & ipsec manip\n");
+				return err;
+			}
+			policy_entry->hmd = DPA_OFFLD_DESC_NONE;
+		}
+
+		break;
+	case MNG_OP_MODIFY:
+		if (policy_entry->hmd != DPA_OFFLD_DESC_NONE)
+			pol_hmd = policy_entry->hmd;
+		else
+			pol_hmd = sa->ipsec_hmd;
+
+		memset(&action, 0, sizeof(action));
+		fill_cls_action_enq(&action, FALSE,
+				    qman_fq_fqid((sa->to_sec_fq)), pol_hmd);
+
+		memset(&params, 0, sizeof(params));
+		params.type = DPA_CLS_TBL_MODIFY_ACTION;
+		params.key = NULL;
+		params.action = &action;
+
+		err = dpa_classif_table_modify_entry_by_ref(table,
+							 policy_entry->entry_id,
+							 &params);
+		if (err < 0) {
+			pr_err("Could not modify key in EM table\n");
+			return err;
+		}
+		break;
+	}
+
+	return 0;
+}
+
+static int update_pre_sec_inbound_table(struct dpa_ipsec_sa *sa,
+					enum mng_op_type op_type)
+{
+	struct dpa_ipsec *dpa_ipsec;
+	int table, table_idx, entry_id, offset, err = 0, tbl_key_size = 0, i;
+	struct dpa_offload_lookup_key tbl_key;
+	struct dpa_cls_tbl_action action;
+	struct dpa_cls_tbl_params tbl_params;
+	struct dpa_cls_tbl_entry_mod_params mod_params;
+	uint8_t key[DPA_OFFLD_MAXENTRYKEYSIZE];
+
+	BUG_ON(!sa);
+
+	dpa_ipsec = sa->dpa_ipsec;
+	BUG_ON(!dpa_ipsec);
+
+	switch (op_type) {
+	case MNG_OP_ADD:
+		/* Determine the correct table to be used for this type of SA */
+		table_idx = GET_SA_TABLE_IDX(sa->dest_addr, sa->use_udp_encap);
+		table =
+		      dpa_ipsec->config.pre_sec_in_params.dpa_cls_td[table_idx];
+		if (table == DPA_OFFLD_DESC_NONE) {
+			pr_err("No SA table defined for this type of SA\n");
+			return -EBADF;
+		}
+
+		/* Store the table descriptor to be used in subsequent ops */
+		sa->inbound_sa_td = table;
+
+		/*
+		 * Mark classifier entry id as invalid until it's properly
+		 * inserted
+		 */
+		sa->inbound_hash_entry = DPA_OFFLD_INVALID_OBJECT_ID;
+		sa->valid_flowid_entry = false;
+
+		tbl_key.byte = key;
+		/* Key masks are not supported by HASH tables*/
+		tbl_key.mask = NULL;
+
+		/*
+		 * Key contains:
+		 * IP DST ADDR  - from SA handle
+		 * IP_PROTO     - always ESP (SEC limitation)
+		 * UDP_SPORT    - in case of UDP encapsulated ESP
+		 * UDP_DPORT    - in case of UDP encapsulated ESP
+		 * SPI          - from SA handle
+		 */
+
+		/* Fill in the key components */
+		memcpy(key, IP_ADDR(sa->dest_addr), IP_ADDR_LEN(sa->dest_addr));
+
+		offset = IP_ADDR_LEN(sa->dest_addr);
+		if (sa->use_udp_encap) {
+			SET_BYTE_VAL_IN_ARRAY(key, offset, IPPROTO_UDP);
+			offset += IP_PROTO_FIELD_LEN;
+			memcpy(key + offset, (uint8_t *) &(sa->udp_src_port),
+			       PORT_FIELD_LEN);
+			offset += PORT_FIELD_LEN;
+			memcpy(key + offset, (uint8_t *) &(sa->udp_dest_port),
+			       PORT_FIELD_LEN);
+			offset += PORT_FIELD_LEN;
+		} else {
+			SET_BYTE_VAL_IN_ARRAY(key, offset, IPPROTO_ESP);
+			offset += IP_PROTO_FIELD_LEN;
+		}
+
+		memcpy(key + offset, (uint8_t *) &sa->spi, ESP_SPI_FIELD_LEN);
+		offset += ESP_SPI_FIELD_LEN;
+
+		/* determine padding length based on the table params */
+		err = dpa_classif_table_get_params(table, &tbl_params);
+		if (err < 0) {
+			pr_err("Could not get table maximum key size\n");
+			return err;
+		}
+		tbl_key_size = TABLE_KEY_SIZE(tbl_params);
+
+		if (tbl_key_size < offset) {
+			pr_err("SA lookup key is greater than maximum table key size\n");
+			return -EINVAL;
+		}
+
+		if (tbl_key_size > offset) {
+			for (i = 0; i < tbl_key_size - offset; i++)
+				*(key + offset + i) = DPA_IPSEC_DEF_PAD_VAL;
+			offset = tbl_key_size;
+		}
+
+		/* Key size cannot be greater than 56 bytes */
+		tbl_key.size = (uint8_t)offset;
+
+		/* Complete the parameters for table insert function */
+		memset(&action, 0, sizeof(action));
+		fill_cls_action_enq(&action, FALSE,
+			qman_fq_fqid((sa->to_sec_fq)), sa->ipsec_hmd);
+
+		err = dpa_classif_table_insert_entry(table, &tbl_key, &action,
+						     0, &entry_id);
+		if (err < 0) {
+			pr_err("Could not add key for inbound SA!\n");
+			return err;
+		}
+		sa->inbound_hash_entry = entry_id;
+		break;
+
+	case MNG_OP_REMOVE:
+		entry_id = sa->inbound_hash_entry;
+		err = dpa_classif_table_delete_entry_by_ref(sa->inbound_sa_td,
+							    entry_id);
+		if (err < 0) {
+			pr_err("Could not remove key for inbound SA!\n");
+			return err;
+		}
+		sa->inbound_hash_entry = DPA_OFFLD_INVALID_OBJECT_ID;
+		break;
+
+	case MNG_OP_MODIFY:
+		fill_cls_action_drop(&action, FALSE);
+
+		memset(&mod_params, 0, sizeof(mod_params));
+		mod_params.type = DPA_CLS_TBL_MODIFY_ACTION;
+		mod_params.key = NULL;
+		mod_params.action = &action;
+
+		entry_id = sa->inbound_hash_entry;
+		err = dpa_classif_table_modify_entry_by_ref(sa->inbound_sa_td,
+							    entry_id,
+							    &mod_params);
+		if (err < 0) {
+			pr_err("Failed set drop action for inbound SA %d\n",
+				  sa->id);
+			return err;
+		}
+		sa->inbound_hash_entry = DPA_OFFLD_INVALID_OBJECT_ID;
+		break;
+	}
+
+	return err;
+}
+
+static int remove_inbound_hash_entry(struct dpa_ipsec_sa *sa)
+{
+	int err;
+
+	BUG_ON(!sa);
+
+	err = update_pre_sec_inbound_table(sa, MNG_OP_REMOVE);
+	if (unlikely(err < 0)) {
+		pr_crit("Failed to remove inbound key for SA %d\n", sa->id);
+		return -ENOTRECOVERABLE;
+	}
+
+	return 0;
+}
+
+static inline int remove_inbound_flow_id_classif(struct dpa_ipsec_sa *sa)
+{
+	struct dpa_ipsec *dpa_ipsec;
+	struct dpa_cls_tbl_action action;
+	int err;
+
+	dpa_ipsec = sa->dpa_ipsec;
+
+	memset(&action, 0, sizeof(action));
+	action.type = DPA_CLS_TBL_ACTION_DROP;
+
+	err = set_flow_id_action(sa, &action);
+	if (err < 0) {
+		pr_err("Could not remove SA entry in indexed table\n");
+		return err;
+	}
+
+	if (dpa_ipsec->config.post_sec_in_params.do_pol_check)
+		put_free_inbpol_tbl(dpa_ipsec, sa->em_inpol_td);
+
+	err = put_inbound_flowid(dpa_ipsec, sa->inbound_flowid);
+	if (err < 0)
+		return err;
+	sa->valid_flowid_entry = false;
+
+	return 0;
+}
+
+static int get_new_fqid(struct dpa_ipsec *dpa_ipsec, uint32_t *fqid)
+{
+	int err = 0;
+
+	BUG_ON(!dpa_ipsec);
+	BUG_ON(!fqid);
+
+	if (dpa_ipsec->sa_mng.fqid_cq != NULL) {
+		err = cq_get_4bytes(dpa_ipsec->sa_mng.fqid_cq, fqid);
+		if (err < 0)
+			pr_err("FQID allocation (from range) failure."
+				   "QMan error code %d\n", err);
+		return err;
+	}
+
+	/* No pool defined. Get FQID from default allocator. */
+	err = qman_alloc_fqid(fqid);
+	if (err < 0) {
+		pr_err("FQID allocation (no pool) failure.\n");
+		return -ERANGE;
+	}
+
+	return 0;
+}
+
+static void put_free_fqid(uint32_t fqid)
+{
+	struct dpa_ipsec *dpa_ipsec;
+	int err;
+
+	if (!gbl_dpa_ipsec) {
+		pr_err("There is no DPA IPSec instance initialized\n");
+		return;
+	}
+	dpa_ipsec = gbl_dpa_ipsec;
+
+	/* recycle the FQID */
+	if (dpa_ipsec->sa_mng.fqid_cq != NULL) {
+		err = cq_put_4bytes(dpa_ipsec->sa_mng.fqid_cq, fqid);
+		BUG_ON(err < 0);
+	} else
+		qman_release_fqid(fqid);
+}
+
+static int wait_until_fq_empty(struct qman_fq *fq, int timeout)
+{
+	struct qm_mcr_queryfq_np queryfq_np;
+
+	BUG_ON(!fq);
+
+	do {
+		qman_query_fq_np(fq, &queryfq_np);
+		cpu_relax();
+		udelay(1);
+		timeout = timeout - 1;
+	} while (queryfq_np.frm_cnt && timeout);
+
+	if (timeout == 0) {
+		pr_err("Timeout. Fq with id %d not empty.\n", fq->fqid);
+		return -EBUSY;
+	}
+
+	return 0;
+}
+
+static int remove_sa_sec_fq(struct qman_fq *sec_fq)
+{
+	int err, flags, timeout = WAIT4_FQ_EMPTY_TIMEOUT;
+
+	BUG_ON(!sec_fq);
+
+	/* Check if already removed, and return success if so. */
+	if (sec_fq->fqid == 0)
+		return 0;
+
+	err = wait_until_fq_empty(sec_fq, timeout);
+	if (err < 0)
+		return err;
+
+	err = qman_retire_fq(sec_fq, &flags);
+	if (err < 0) {
+		pr_err("Failed to retire FQ %d\n", sec_fq->fqid);
+		return err;
+	}
+
+	err = qman_oos_fq(sec_fq);
+	if (err < 0) {
+		pr_err("Failed to OOS FQ %d\n", sec_fq->fqid);
+		return err;
+	}
+
+	qman_destroy_fq(sec_fq, 0);
+
+	/* release FQID */
+	put_free_fqid(sec_fq->fqid);
+
+	/* Clean the FQ structure for reuse */
+	memset(sec_fq, 0, sizeof(struct qman_fq));
+
+	return 0;
+}
+
+static int remove_sa_fq_pair(struct dpa_ipsec_sa *sa)
+{
+	int err;
+
+	BUG_ON(!sa);
+
+	err = remove_sa_sec_fq(sa->to_sec_fq);
+	if (err < 0)
+		return err;
+
+	if (sa_is_single(sa)) {
+		err = remove_sa_sec_fq(sa->from_sec_fq);
+		if (err < 0)
+			return err;
+	}
+
+	return 0;
+}
+
+static int create_sec_frame_queue(uint32_t fq_id, uint16_t channel,
+				  uint16_t wq_id, uint32_t ctx_a_hi,
+				  uint32_t ctx_a_lo, uint32_t ctxB,
+				  uint32_t sp_op, void *fm_pcd, bool parked,
+				  struct qman_fq *fq)
+{
+	struct qm_mcc_initfq fq_opts;
+	uint32_t flags;
+	int err = 0;
+
+	BUG_ON(!fq);
+
+	memset(fq, 0, sizeof(struct qman_fq));
+
+	flags = QMAN_FQ_FLAG_LOCKED | QMAN_FQ_FLAG_TO_DCPORTAL;
+
+	err = qman_create_fq(fq_id, flags, fq);
+	if (unlikely(err < 0)) {
+		pr_err("Could not create FQ with ID: %u\n", fq_id);
+		goto create_sec_fq_err;
+	}
+
+	/*
+	 * generate a parked queue or a scheduled one depending on the function
+	 * input parameters.
+	 */
+	flags = (parked == true) ? 0 : QMAN_INITFQ_FLAG_SCHED;
+	memset(&fq_opts, 0, sizeof(fq_opts));
+	fq_opts.we_mask = QM_INITFQ_WE_DESTWQ | QM_INITFQ_WE_CONTEXTA |
+			  QM_INITFQ_WE_CONTEXTB;
+	if (ctx_a_lo) {
+		fq_opts.fqd.context_a.hi = ctx_a_hi;
+		fq_opts.fqd.context_a.lo = ctx_a_lo;
+		fq_opts.fqd.context_b = ctxB;
+	} else {
+		uint8_t sp_op_code = 0;
+		t_Error error;
+
+		/*
+		 * configure uCode commands for handling flowID and other update
+		 * operations:
+		 * - retrieve special operation code (for IPSec and possibly
+		 *   other updates on UDP header fields)
+		 * - enable ctxB;
+		 * - set ctxA override;
+		 * - set in ctxB the FlowID and special operation code
+		 */
+		if (sp_op) {
+			void *fm = NULL;
+
+			/* get FMan handle from the PCD handle*/
+			fm = ((t_FmPcd *)fm_pcd)->h_Fm;
+
+			error = FM_GetSpecialOperationCoding(fm, sp_op,
+							     &sp_op_code);
+			if (error != E_OK) {
+				pr_err("FM_GetSpecialOperationCoding failed\n");
+				pr_err("Could not retrieve special op code");
+				goto create_sec_fq_err;
+			}
+			/* the opcode is a 4-bit value */
+			sp_op_code &= NIA_OPCODE_MASK;
+		}
+#if (DPAA_VERSION == 10)
+		/* FMAN v2 devices: opcode and flow id are stored in contextB */
+		FM_CONTEXTA_SET_OVERRIDE(&fq_opts.fqd.context_a, true);
+		FM_CONTEXTB_SET_FQID(&(fq_opts.fqd.context_b), ctxB |
+					(sp_op_code << 20));
+#elif (DPAA_VERSION == 11)
+		/* FMAN v3 devices: opcode and flow id are stored in contextA */
+		FM_CONTEXTA_SET_A1_VALID(&fq_opts.fqd.context_a, true);
+		FM_CONTEXTA_SET_A1(&fq_opts.fqd.context_a,
+				((ctxB << 4) | sp_op_code));
+#endif
+	}
+
+	fq_opts.fqd.dest.wq = wq_id;
+	fq_opts.fqd.dest.channel = channel;
+
+	err = qman_init_fq(fq, flags, &fq_opts);
+	if (unlikely(err < 0)) {
+		pr_err("Could not init FQ with ID: %u\n", fq->fqid);
+		goto create_sec_fq_err;
+	}
+
+	return 0;
+
+ create_sec_fq_err:
+	/*Reset all fields of FQ structure (including FQID) to mark it invalid*/
+	memset(fq, 0, sizeof(struct qman_fq));
+
+	return err;
+}
+
+static int create_sa_fq_pair(struct dpa_ipsec_sa *sa,
+			     bool reuse_from_secfq, bool parked_to_secfq)
+{
+	void *ctxtA;
+	uint32_t ctxtA_hi, ctxtA_lo;
+	phys_addr_t addr;
+	struct dpa_ipsec *dpa_ipsec;
+	uint32_t fqid_from_sec = 0, fqid_to_sec = 0;
+	int err;
+
+	BUG_ON(!sa);
+
+	dpa_ipsec = sa->dpa_ipsec;
+	BUG_ON(!dpa_ipsec);
+
+	err = create_sec_descriptor(sa);
+	if (err < 0) {
+		pr_err("Could not create sec descriptor\n");
+		return err;
+	}
+
+	ctxtA = sa->sec_desc;
+	addr = virt_to_phys(ctxtA);
+	ctxtA_hi = (uint32_t) (addr >> 32);
+	ctxtA_lo = (uint32_t) (addr);
+
+	/*
+	 * If reuse FROM SEC FQ is false than create other FROM SEC FQ
+	 * and set it as output frame queue for this SA. Otherwise
+	 * profit that you poses a valid FROM SEC FQ from the OLD SA
+	 * and use it accordingly.
+	 */
+	if (!reuse_from_secfq) {
+		uint16_t chan, flow_id;
+		uint32_t sp_op = 0;
+
+		sp_op = FM_SP_OP_IPSEC;
+		if (sa_is_outbound(sa) && sa->use_udp_encap)
+			sp_op |= FM_SP_OP_IPSEC_UPDATE_UDP_LEN;
+		if (sa->dscp_copy || sa->ecn_copy)
+			sp_op |= FM_SP_OP_IPSEC_MANIP | FM_SP_OP_RPD;
+
+		/* acquire fqid for 'FROM SEC' fq */
+		err = get_new_fqid(dpa_ipsec, &fqid_from_sec);
+		if (err < 0)
+			return err;
+
+		if (sa_is_outbound(sa)) {
+			chan = dpa_ipsec->config.post_sec_out_params.qm_tx_ch;
+			flow_id = sa->outbound_flowid;
+		} else {
+			chan = dpa_ipsec->config.post_sec_in_params.qm_tx_ch;
+			flow_id = sa->inbound_flowid;
+		}
+
+		err = create_sec_frame_queue(fqid_from_sec,
+					     chan, sa->sa_wqid, 0, 0, /* ctxA */
+					     flow_id, /*ctxB forwarding info*/
+					     sp_op,
+					     sa->dpa_ipsec->config.fm_pcd,
+					     FALSE, sa->from_sec_fq);
+		if (err < 0) {
+			pr_err("From SEC FQ couldn't be created\n");
+			goto create_fq_pair_err;
+		}
+	}
+
+	/* acquire fqid for 'TO SEC' fq */
+	err = get_new_fqid(dpa_ipsec, &fqid_to_sec);
+	if (err < 0)
+		goto create_fq_pair_err;
+
+	err = create_sec_frame_queue(fqid_to_sec,
+			dpa_ipsec->config.qm_sec_ch,
+			sa->sa_wqid, ctxtA_hi, ctxtA_lo, /* ctxA */
+			qman_fq_fqid(sa->from_sec_fq), /*ctxB - output SEC fq*/
+			0, NULL, parked_to_secfq, sa->to_sec_fq);
+	if (err < 0) {
+		pr_err("%s FQ (to SEC) couldn't be created\n",
+			sa_is_outbound(sa) ? "Encrypt" : "Decrypt");
+		goto create_fq_pair_err;
+	}
+
+	return 0;
+
+ create_fq_pair_err:
+	if (qman_fq_fqid(sa->from_sec_fq) != 0)
+		remove_sa_sec_fq(sa->from_sec_fq);
+	else
+		put_free_fqid(fqid_from_sec);	/*just recycle the FQID*/
+
+	if (fqid_to_sec != 0)
+		put_free_fqid(fqid_to_sec); /*a FQID was allocated;recycle it*/
+
+	return err;
+}
+
+static inline int set_cipher_auth_alg(enum dpa_ipsec_cipher_alg alg_suite,
+			       uint16_t *cipher, uint16_t *auth)
+{
+	*cipher = ipsec_algs[alg_suite].enc_alg;
+	*auth = ipsec_algs[alg_suite].auth_alg;
+
+	if (*cipher == OP_PCL_IPSEC_INVALID_ALG_ID ||
+	    *auth == OP_PCL_IPSEC_INVALID_ALG_ID) {
+		pr_err("Invalid algorithm suite selected\n");
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int copy_sa_params_to_out_sa(struct dpa_ipsec_sa *sa,
+				    struct dpa_ipsec_sa_params *sa_params)
+{
+	struct iphdr *outer_ip_hdr;
+	unsigned int ip_addr_type;
+	int err;
+
+	BUG_ON(!sa);
+	BUG_ON(!sa_params);
+
+	sa->sa_dir = DPA_IPSEC_OUTBOUND;
+	sa->sa_bpid = sa_params->sa_bpid;
+	sa->sa_wqid = sa_params->sa_wqid;
+	ip_addr_type = sa_params->sa_out_params.ip_ver;
+
+	sa->alg_suite = sa_params->crypto_params.alg_suite;
+	err = set_cipher_auth_alg(sa_params->crypto_params.alg_suite,
+				  &sa->cipher_data.cipher_type,
+				  &sa->auth_data.auth_type);
+	if (err < 0)
+		return err;
+
+	sa->auth_data.auth_key_len = sa_params->crypto_params.auth_key_len;
+	memcpy(sa->auth_data.auth_key,
+	       sa_params->crypto_params.auth_key,
+	       sa_params->crypto_params.auth_key_len);
+
+	sa->cipher_data.cipher_key_len =
+		sa_params->crypto_params.cipher_key_len;
+	memcpy(sa->cipher_data.cipher_key,
+	       sa_params->crypto_params.cipher_key,
+	       sa_params->crypto_params.cipher_key_len);
+	sa->sec_desc->pdb_en.spi = sa_params->spi;
+	sa->sec_desc->pdb_en.options = PDBOPTS_ESP_TUNNEL |
+				       PDBOPTS_ESP_INCIPHDR |
+				       PDBOPTS_ESP_IPHDRSRC;
+	if (sa_params->hdr_upd_flags) {
+		if (sa_params->hdr_upd_flags & DPA_IPSEC_HDR_COPY_TOS)
+			sa->sec_desc->pdb_en.options |= PDBOPTS_ESP_DIFFSERV;
+		if (sa_params->hdr_upd_flags & DPA_IPSEC_HDR_COPY_DF) {
+			if (ip_addr_type == DPA_IPSEC_ADDR_T_IPv4)
+				sa->sec_desc->pdb_en.hmo_rsvd |=
+							PDBHMO_ESP_DFBIT;
+			else
+				pr_warn("Copy DF not supported for IPv6 SA");
+		}
+		if (sa_params->hdr_upd_flags & DPA_IPSEC_HDR_DEC_TTL)
+			sa->sec_desc->pdb_en.hmo_rsvd |=
+					PDBHMO_ESP_ENCAP_DEC_TTL;
+
+		sa->dscp_copy =
+			sa_params->hdr_upd_flags & DPA_IPSEC_HDR_COPY_DSCP;
+		sa->ecn_copy =
+			sa_params->hdr_upd_flags & DPA_IPSEC_HDR_COPY_ECN;
+	}
+
+	sa->enable_dpovrd = true;
+
+	if (sa_params->use_ext_seq_num) {
+		sa->sec_desc->pdb_en.seq_num_ext_hi =
+			(sa_params->start_seq_num & SEQ_NUM_HI_MASK) >> 32;
+		sa->sec_desc->pdb_en.options |= PDBOPTS_ESP_ESN;
+	}
+	sa->sec_desc->pdb_en.seq_num =
+				sa_params->start_seq_num & SEQ_NUM_LOW_MASK;
+
+	if (ip_addr_type == DPA_IPSEC_ADDR_T_IPv6)
+		sa->sec_desc->pdb_en.options |= PDBOPTS_ESP_IPV6;
+	else
+		sa->sec_desc->pdb_en.options |= PDBOPTS_ESP_UPDATE_CSUM;
+
+	if (!sa_params->sa_out_params.init_vector)
+		sa->sec_desc->pdb_en.options |= PDBOPTS_ESP_IVSRC;
+	else
+		memcpy(&sa->sec_desc->pdb_en.cbc,
+		       sa_params->sa_out_params.init_vector->init_vector,
+		       sa_params->sa_out_params.init_vector->length);
+
+	sa->outbound_flowid = sa_params->sa_out_params.post_sec_flow_id;
+
+	/* Copy the outer header and generate the original header checksum */
+	memcpy(&sa->sec_desc->pdb_en.ip_hdr[0],
+	       sa_params->sa_out_params.outer_ip_header,
+	       sa_params->sa_out_params.ip_hdr_size);
+
+	if (sa_params->sa_out_params.outer_udp_header) {
+		uint8_t *tmp;
+		struct udphdr *udp_hdr;
+
+		tmp = (uint8_t *) &sa->sec_desc->pdb_en.ip_hdr[0];
+		memcpy(tmp + sa_params->sa_out_params.ip_hdr_size,
+		       sa_params->sa_out_params.outer_udp_header,
+		       UDP_HEADER_LEN);
+		sa->sec_desc->pdb_en.ip_hdr_len =
+			sa_params->sa_out_params.ip_hdr_size + UDP_HEADER_LEN;
+		sa->use_udp_encap = true;
+
+		/* disable UDP checksum calculation, because for now there is
+		 * no mechanism for UDP checksum update */
+		udp_hdr = (struct udphdr *) (tmp +
+				sa_params->sa_out_params.ip_hdr_size);
+		udp_hdr->check = 0x0000;
+
+		if (ip_addr_type == DPA_IPSEC_ADDR_T_IPv4) {
+			outer_ip_hdr = (struct iphdr *)
+						&sa->sec_desc->pdb_en.ip_hdr[0];
+			outer_ip_hdr->protocol = IPPROTO_UDP;
+		} else {
+			/*
+			 * this should never be reached - it should be checked
+			 * before in check SA params function
+			 */
+			pr_err("NAT-T is not supported for IPv6 SAs\n");
+			return -EINVAL;
+		}
+	} else {
+		sa->sec_desc->pdb_en.ip_hdr_len =
+				sa_params->sa_out_params.ip_hdr_size;
+	}
+
+	if (ip_addr_type == DPA_IPSEC_ADDR_T_IPv4) {
+		outer_ip_hdr = (struct iphdr *) &sa->sec_desc->pdb_en.ip_hdr[0];
+		outer_ip_hdr->check =
+			ip_fast_csum((unsigned char *)outer_ip_hdr,
+				     outer_ip_hdr->ihl);
+	}
+
+	/* Only IPv4 inner packets are currently supported */
+	sa->sec_desc->pdb_en.ip_nh = 0x04;
+
+	sa->l2_hdr_size = sa_params->l2_hdr_size;
+	sa->enable_stats = sa_params->enable_stats;
+#ifdef DEBUG_PARAM
+	/* Printing all the parameters */
+	print_sa_sec_param(sa);
+#endif
+
+	return 0;
+}
+
+static int copy_sa_params_to_in_sa(struct dpa_ipsec_sa *sa,
+				   struct dpa_ipsec_sa_params *sa_params,
+				   bool rekeying)
+{
+	struct dpa_ipsec *dpa_ipsec;
+	int err;
+
+	BUG_ON(!sa);
+	BUG_ON(!sa_params);
+
+	dpa_ipsec = sa->dpa_ipsec;
+	BUG_ON(!dpa_ipsec);
+
+	/* reserve a FlowID for this SA only if we are not rekeying */
+	if (!rekeying) {
+		err = get_inbound_flowid(dpa_ipsec, &sa->inbound_flowid);
+		if (err < 0) {
+			pr_err("Can't get valid inbound flow id\n");
+			sa->inbound_flowid = INVALID_INB_FLOW_ID;
+			return -EINVAL;
+		}
+	}
+
+	sa->sa_dir = DPA_IPSEC_INBOUND;
+	sa->sa_bpid = sa_params->sa_bpid;
+	sa->sa_wqid = sa_params->sa_wqid;
+	sa->spi = sa_params->spi;
+
+	sa->alg_suite = sa_params->crypto_params.alg_suite;
+	err = set_cipher_auth_alg(sa_params->crypto_params.alg_suite,
+				  &sa->cipher_data.cipher_type,
+				  &sa->auth_data.auth_type);
+	if (err < 0)
+		return err;
+
+	sa->auth_data.auth_key_len = sa_params->crypto_params.auth_key_len;
+	memcpy(sa->auth_data.auth_key,
+	       sa_params->crypto_params.auth_key,
+	       sa_params->crypto_params.auth_key_len);
+
+	sa->cipher_data.cipher_key_len =
+			sa_params->crypto_params.cipher_key_len;
+	memcpy(sa->cipher_data.cipher_key,
+	       sa_params->crypto_params.cipher_key,
+	       sa_params->crypto_params.cipher_key_len);
+
+	sa->use_udp_encap = sa_params->sa_in_params.use_udp_encap;
+	sa->udp_src_port  = sa_params->sa_in_params.src_port;
+	sa->udp_dest_port = sa_params->sa_in_params.dest_port;
+	sa->use_var_iphdr_len = sa_params->sa_in_params.use_var_iphdr_len;
+
+	memcpy(&sa->def_sa_action,
+	       &sa_params->sa_in_params.post_ipsec_action,
+	       sizeof(struct dpa_cls_tbl_action));
+
+	sa->sec_desc->pdb_dec.seq_num =
+			sa_params->start_seq_num & SEQ_NUM_LOW_MASK;
+	sa->sec_desc->pdb_dec.options = PDBOPTS_ESP_TUNNEL |
+					PDBOPTS_ESP_OUTFMT;
+
+	if (dpa_ipsec->sec_ver >= SEC_VER_5_3)
+		sa->sec_desc->pdb_dec.options |= PDBOPTS_ESP_AOFL;
+
+	if (sa_params->use_ext_seq_num) {
+		sa->sec_desc->pdb_dec.seq_num_ext_hi =
+			(sa_params->start_seq_num & SEQ_NUM_HI_MASK) >> 32;
+		sa->sec_desc->pdb_dec.options |= PDBOPTS_ESP_ESN;
+	}
+
+	if (sa_params->sa_in_params.dest_addr.version ==
+							DPA_IPSEC_ADDR_T_IPv6)
+		sa->sec_desc->pdb_dec.options |= PDBOPTS_ESP_IPVSN;
+	else
+		sa->sec_desc->pdb_dec.options |= PDBOPTS_ESP_VERIFY_CSUM;
+
+	switch (sa_params->sa_in_params.arw) {
+	case DPA_IPSEC_ARSNONE:
+		sa->sec_desc->pdb_dec.options |= PDBOPTS_ESP_ARSNONE;
+		break;
+	case DPA_IPSEC_ARS32:
+		sa->sec_desc->pdb_dec.options |= PDBOPTS_ESP_ARS32;
+		break;
+	case DPA_IPSEC_ARS64:
+		sa->sec_desc->pdb_dec.options |= PDBOPTS_ESP_ARS64;
+		break;
+	default:
+		pr_err("Invalid ARS mode specified\n");
+		return -EINVAL;
+	}
+
+	/*
+	 * Updated the offset to the point in frame were the encrypted
+	 * stuff starts.
+	 */
+	if (sa_params->sa_in_params.dest_addr.version ==
+							DPA_IPSEC_ADDR_T_IPv6)
+		sa->sec_desc->pdb_dec.hmo_ip_hdr_len =
+					(uint16_t) sizeof(struct ipv6hdr);
+	else
+		sa->sec_desc->pdb_dec.hmo_ip_hdr_len =
+					(uint16_t) sizeof(struct iphdr);
+	if (sa->use_udp_encap)
+		sa->sec_desc->pdb_dec.hmo_ip_hdr_len += UDP_HEADER_LEN;
+
+	if (sa_params->hdr_upd_flags) {
+		if (sa_params->hdr_upd_flags & DPA_IPSEC_HDR_COPY_TOS)
+			sa->sec_desc->pdb_dec.hmo_ip_hdr_len |=
+					PDBHMO_ESP_DIFFSERV;
+		if (sa_params->hdr_upd_flags & DPA_IPSEC_HDR_DEC_TTL)
+			sa->sec_desc->pdb_dec.hmo_ip_hdr_len |=
+					PDBHMO_ESP_DECAP_DEC_TTL;
+		if (sa_params->hdr_upd_flags & DPA_IPSEC_HDR_COPY_DF)
+			pr_info("Copy DF bit not supported for inbound SAs");
+
+		sa->dscp_copy =
+			sa_params->hdr_upd_flags & DPA_IPSEC_HDR_COPY_DSCP;
+		sa->ecn_copy =
+			sa_params->hdr_upd_flags & DPA_IPSEC_HDR_COPY_ECN;
+	}
+
+	/* Only for outbound */
+	sa->enable_dpovrd = false;
+
+	memcpy(&sa->src_addr,
+	       &sa_params->sa_in_params.src_addr,
+	       sizeof(struct dpa_offload_ip_address));
+
+	memcpy(&sa->dest_addr,
+	       &sa_params->sa_in_params.dest_addr,
+	       sizeof(struct dpa_offload_ip_address));
+
+	sa->policy_miss_action = sa_params->sa_in_params.policy_miss_action;
+	sa->l2_hdr_size = sa_params->l2_hdr_size;
+	sa->enable_stats = sa_params->enable_stats;
+#ifdef DEBUG_PARAM
+	/* Printing all the parameters */
+	print_sa_sec_param(sa);
+#endif
+
+	return 0;
+}
+
+static int check_policy_params(struct dpa_ipsec_sa *sa,
+			       struct dpa_ipsec_policy_params *pol_params)
+{
+	BUG_ON(!sa);
+	BUG_ON(!pol_params);
+
+	/* check if both IP address are of the same type */
+	if (pol_params->src_addr.version != pol_params->dest_addr.version) {
+		pr_err("Src and dest IP address types must be the same!\n");
+		return -EINVAL;
+	}
+
+	/* check if IP address version is valid */
+	if (pol_params->src_addr.version != DPA_IPSEC_ADDR_T_IPv4 &&
+	    pol_params->src_addr.version != DPA_IPSEC_ADDR_T_IPv6) {
+		pr_err("Src and dest IP address types either 4 or 6!\n");
+		return -EINVAL;
+	}
+
+	/* check if fragmentation is enabled for inbound SAs */
+	if (pol_params->dir_params.type == DPA_IPSEC_POL_DIR_PARAMS_MANIP &&
+	    sa_is_inbound(sa)) {
+		pr_err("Fragmentation or header manipulation can't be enabled for inbound policy!\n");
+		return -EINVAL;
+	}
+
+	if (pol_params->dir_params.type == DPA_IPSEC_POL_DIR_PARAMS_MANIP &&
+	    pol_params->dir_params.manip_desc < 0) {
+		pr_err("Invalid manip descriptor for SA id %d\n", sa->id);
+		return -EINVAL;
+	}
+
+	/*
+	 * check if post inbound policy verification action was configured for
+	 * outbound policies
+	 */
+	if (pol_params->dir_params.type == DPA_IPSEC_POL_DIR_PARAMS_ACT &&
+	    sa_is_outbound(sa)) {
+		pr_err("Action cannot be configured for outbound policy!\n");
+		return -EINVAL;
+	}
+
+	/* check if DF bit was set and an IPv6 policy is being offloaded */
+	if (sa_is_outbound(sa) &&
+	    sa->sec_desc->pdb_en.hmo_rsvd == PDBHMO_ESP_DFBIT &&
+	    pol_params->src_addr.version == DPA_IPSEC_ADDR_T_IPv6) {
+		pr_err("Can't add IPv6 policy to IPv4 SA w/ DF bit copy set\n");
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int store_policy_param_to_sa_pol_list(struct dpa_ipsec_sa *sa,
+				struct dpa_ipsec_policy_params *policy_params,
+				struct dpa_ipsec_policy_entry **policy_entry)
+{
+	struct dpa_ipsec_policy_entry *pol_entry;
+
+	BUG_ON(!sa);
+	BUG_ON(!policy_params);
+	BUG_ON(!policy_entry);
+
+	pol_entry = kzalloc(sizeof(*pol_entry), GFP_KERNEL);
+	if (!pol_entry) {
+		pr_err("Could not allocate memory for policy\n");
+		return -ENOMEM;
+	}
+
+	/* Initialize the policy Manip handle to an invalid value */
+	pol_entry->hmd = DPA_OFFLD_DESC_NONE;
+
+	/* copy policy parameters */
+	pol_entry->pol_params = *policy_params;
+
+	/* add policy to the SA's policy list */
+	list_add(&pol_entry->node, &sa->policy_headlist);
+
+	*policy_entry = pol_entry;
+
+	return 0;
+}
+
+static inline int addr_match(struct dpa_offload_ip_address *addr1,
+			     struct dpa_offload_ip_address *addr2)
+{
+	if (addr1->version != addr2->version)
+		return false;
+
+	switch (addr1->version) {
+	case DPA_IPSEC_ADDR_T_IPv4:
+		if (addr1->addr.ipv4.word != addr2->addr.ipv4.word)
+			return false;
+		break;
+	case DPA_IPSEC_ADDR_T_IPv6:
+		if (memcmp(&addr1->addr.ipv6.byte, &addr2->addr.ipv6.byte,
+			   DPA_OFFLD_IPv6_ADDR_LEN_BYTES))
+			return false;
+		break;
+	default:
+		/*
+		 * IP's version was checked for validity when policy was
+		 * off-loaded so it can be invalid only if DPA IPsec component
+		 * messed it up.
+		 */
+		pr_err("Invalid IP version\n");
+		BUG();
+	}
+
+	return true;
+}
+
+
+static int find_policy(struct dpa_ipsec_sa *sa,
+		       struct dpa_ipsec_policy_params *pol,
+		       struct dpa_ipsec_policy_entry **policy_entry)
+{
+	struct dpa_ipsec_policy_entry *pol_entry, *tmp;
+
+	BUG_ON(!sa);
+	BUG_ON(!pol);
+	BUG_ON(!policy_entry);
+
+	if (list_empty(&sa->policy_headlist)) {
+		pr_err("Policy list is empty\n");
+		return -EDOM;
+	}
+
+	list_for_each_entry_safe(pol_entry, tmp, &sa->policy_headlist, node) {
+		struct dpa_ipsec_policy_params *cpol;
+		uint8_t cproto;
+
+		cpol = &pol_entry->pol_params;
+		cproto = cpol->protocol;
+
+		if (cpol->dest_prefix_len != pol->dest_prefix_len ||
+		    cpol->src_prefix_len != pol->src_prefix_len ||
+		    !addr_match(&cpol->dest_addr, &pol->dest_addr) ||
+		    !addr_match(&cpol->src_addr, &pol->src_addr) ||
+		    cpol->protocol != pol->protocol ||
+		    cpol->masked_proto != pol->masked_proto ||
+		    cpol->priority != pol->priority)
+			continue;
+
+		if (cproto == IPPROTO_UDP || cproto == IPPROTO_TCP ||
+		    cproto == IPPROTO_SCTP)
+			if (cpol->l4.dest_port != pol->l4.dest_port ||
+			    cpol->l4.dest_port_mask != pol->l4.dest_port_mask ||
+			    cpol->l4.src_port != pol->l4.src_port ||
+			    cpol->l4.src_port_mask != pol->l4.src_port_mask)
+				continue;
+
+		if (cproto == IPPROTO_ICMP || cproto == IPPROTO_ICMPV6) {
+			struct dpa_ipsec_icmp_params *c;
+			c = &cpol->icmp;
+			if (c->icmp_code != pol->icmp.icmp_code ||
+			    c->icmp_code_mask != pol->icmp.icmp_code_mask ||
+			    c->icmp_type != pol->icmp.icmp_type ||
+			    c->icmp_type_mask != pol->icmp.icmp_type_mask)
+				continue;
+		}
+
+		/* found entry matching the input policy parameters */
+		*policy_entry = pol_entry;
+		return 0;
+	}
+
+	/* did not find the entry that matches the input policy parameters */
+	return -EDOM;
+}
+
+static inline int get_policy_count_for_sa(struct dpa_ipsec_sa *sa)
+{
+	struct dpa_ipsec_policy_entry *policy_entry, *tmp_policy_entry;
+	int pol_count = 0;
+
+	if (list_empty(&sa->policy_headlist)) {
+		pr_debug("Policy parameter list is empty\n");
+		return 0;
+	}
+
+
+	list_for_each_entry_safe(policy_entry, tmp_policy_entry,
+				 &sa->policy_headlist, node)
+		pol_count++;
+
+	return pol_count;
+}
+
+static int copy_all_policies(struct dpa_ipsec_sa *sa,
+			     struct dpa_ipsec_policy_params *policy_params,
+			     int num_pol)
+{
+	struct dpa_ipsec_policy_entry *policy_entry, *tmp_policy_entry;
+	int pol_count = 0;
+
+	BUG_ON(!sa);
+	BUG_ON(!policy_params);
+
+	if (list_empty(&sa->policy_headlist)) {
+		pr_err("Policy parameter list is empty\n");
+		return 0;
+	}
+
+	list_for_each_entry_safe(policy_entry, tmp_policy_entry,
+				 &sa->policy_headlist, node) {
+		pol_count++;
+		if (pol_count > num_pol) {
+			pr_err("Num policies in this SA greater than %d",
+				num_pol);
+			return -EAGAIN;
+		}
+
+		policy_params[pol_count - 1] = policy_entry->pol_params;
+	}
+
+	return 0;
+}
+
+static int remove_policy_from_sa_policy_list(struct dpa_ipsec_sa *sa,
+					     struct dpa_ipsec_policy_entry
+					     *policy_entry)
+{
+	BUG_ON(!sa);
+	BUG_ON(!policy_entry);
+
+	if (list_empty(&sa->policy_headlist)) {
+		pr_err("Policy parameter list is empty\n");
+		return -EINVAL;
+	}
+
+	/* unlink this policy from SA's list */
+	list_del(&policy_entry->node);
+
+	/* release memory used for holding policy general parameters */
+	kfree(policy_entry);
+
+	return 0;
+}
+
+static int remove_policy(struct dpa_ipsec_sa *sa,
+			 struct dpa_ipsec_policy_entry *policy_entry)
+{
+	int err;
+
+	BUG_ON(!sa);
+	BUG_ON(!policy_entry);
+
+	if (sa_is_inbound(sa)) {
+		err = update_inbound_policy(sa, policy_entry, MNG_OP_REMOVE);
+		if (err < 0) {
+			pr_err("Could not remove the inbound policy\n");
+			return err;
+		}
+
+		err = remove_policy_from_sa_policy_list(sa, policy_entry);
+		if (err < 0) {
+			pr_err("Couldn't remove inbound policy from SA policy list\n");
+			return err;
+		}
+	} else {  /* DPA_IPSEC_OUTBOUND */
+		err = update_outbound_policy(sa, policy_entry, MNG_OP_REMOVE);
+		if (err < 0) {
+			pr_err("Could not remove the outbound policy\n");
+			return err;
+		}
+
+		err = remove_policy_from_sa_policy_list(sa, policy_entry);
+		if (err < 0) {
+			pr_err("Could not remove outbound policy from SA policy list\n");
+			return err;
+		}
+	}
+
+	return 0;
+}
+
+static struct dpa_ipsec_sa *get_sa_from_sa_id(int sa_id)
+{
+	struct dpa_ipsec *dpa_ipsec;
+	struct dpa_ipsec_sa_mng *sa_mng;
+	struct dpa_ipsec_sa *sa = NULL;
+
+	if (!gbl_dpa_ipsec) {
+		pr_err("There is no dpa_ipsec component initialized\n");
+		return NULL;
+	}
+
+	dpa_ipsec = gbl_dpa_ipsec;
+	sa_mng = &dpa_ipsec->sa_mng;
+
+	if (sa_id < 0 || sa_id > sa_mng->max_num_sa) {
+		pr_err("Invalid SA id %d provided\n", sa_id);
+		return NULL;
+	}
+	sa = &sa_mng->sa[sa_id];
+
+	/* Validity check for this SA */
+	if (sa->used_sa_index == -1) {
+		pr_err("SA with id %d is not valid\n", sa_id);
+		return NULL;
+	}
+
+	return sa;
+}
+
+static int check_sa_params(struct dpa_ipsec_sa_params *sa_params)
+{
+	uint16_t cipher_alg, auth_alg;
+	int err = 0;
+
+	/* sanity checks */
+	if (!sa_params) {
+		pr_err("Invalid SA parameters handle\n");
+		return -EINVAL;
+	}
+
+	/*
+	 * check crypto params:
+	 * - an authentication key must always be provided
+	 * - a cipher key must be provided if alg != NULL encryption
+	 */
+
+	err = set_cipher_auth_alg(sa_params->crypto_params.alg_suite,
+				  &cipher_alg, &auth_alg);
+	if (err < 0)
+		return err;
+
+	if (sa_params->crypto_params.auth_key == NULL) {
+		pr_err("A valid authentication key must be provided\n");
+		return -EINVAL;
+	}
+
+	/* TODO: check cipher_key ONLY if alg != null encryption */
+	if (sa_params->crypto_params.cipher_key == NULL) {
+		pr_err("A valid cipher key must be provided\n");
+		return -EINVAL;
+	}
+
+	if (sa_params->sa_dir == DPA_IPSEC_OUTBOUND) {
+		if ((sa_params->sa_out_params.ip_hdr_size == 0) ||
+		    (sa_params->sa_out_params.outer_ip_header == NULL)) {
+			pr_err("Transport mode is not currently supported."
+				   "Specify a valid encapsulation header\n");
+			return -EINVAL;
+		}
+
+		if (sa_params->sa_out_params.outer_udp_header &&
+			sa_params->sa_out_params.ip_ver ==
+				DPA_IPSEC_ADDR_T_IPv6) {
+			pr_err("NAT-T is not supported for IPV6 SAs\n");
+			return -EINVAL;
+		}
+	} else {
+		/* Inbound SA */
+		if (sa_params->sa_in_params.src_addr.version !=
+		    sa_params->sa_in_params.dest_addr.version) {
+			pr_err("Source and destination IP address must be of same type\n");
+			return -EINVAL;
+		}
+
+		if (sa_params->sa_in_params.use_udp_encap &&
+			sa_params->sa_in_params.src_addr.version ==
+				DPA_IPSEC_ADDR_T_IPv6) {
+			pr_err("NAT-T is not supported for IPV6 SAs\n");
+			return -EINVAL;
+		}
+	}
+
+	/* check buffer pool ID validity */
+	if (sa_params->sa_bpid > MAX_BUFFER_POOL_ID) {
+		pr_err("Invalid SA buffer pool ID.\n");
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int get_new_sa(struct dpa_ipsec *dpa_ipsec,
+		      struct dpa_ipsec_sa **new_sa,
+		      uint32_t *sa_id)
+{
+	struct dpa_ipsec_sa *sa;
+	uint32_t id;
+	int i;
+
+	BUG_ON(!dpa_ipsec);
+
+	BUG_ON(!new_sa);
+	*new_sa = NULL;
+
+	BUG_ON(!sa_id);
+
+	/* Acquire DPA IPSec instance lock */
+	mutex_lock(&dpa_ipsec->lock);
+
+	/* Get an id for new SA */
+	if (cq_get_4bytes(dpa_ipsec->sa_mng.sa_id_cq, &id) < 0) {
+		pr_err("No more unused SA handles\n");
+		/* Release DPA IPSec instance lock */
+		mutex_unlock(&dpa_ipsec->lock);
+		return -EDOM;
+	}
+
+	for (i = 0; i < dpa_ipsec->sa_mng.max_num_sa; i++)
+		if (dpa_ipsec->used_sa_ids[i] == DPA_OFFLD_INVALID_OBJECT_ID)
+			break;
+	if (i == dpa_ipsec->sa_mng.max_num_sa) {
+		pr_err("No more unused SAs ID holders");
+		cq_put_4bytes(dpa_ipsec->sa_mng.sa_id_cq, id);
+		/* Release DPA IPSec instance lock */
+		mutex_unlock(&dpa_ipsec->lock);
+		return -EDOM;
+	}
+
+	/* Acquire a preallocated SA structure */
+	sa = &dpa_ipsec->sa_mng.sa[id];
+	sa->id = id;
+	sa->used_sa_index = i;
+	dpa_ipsec->used_sa_ids[sa->used_sa_index] = sa->id;
+	dpa_ipsec->num_used_sas++;
+
+	/* Release DPA IPSec instance lock */
+	mutex_unlock(&dpa_ipsec->lock);
+
+	*sa_id = id;
+	*new_sa = sa;
+
+	return 0;
+}
+
+/*
+ * Expects that SA lock is acquired for this SA structure and parent/child SA.
+ * Always acquire parent SA lock before child SA lock.
+ */
+static int put_sa(struct dpa_ipsec_sa *sa)
+{
+	struct dpa_ipsec *dpa_ipsec;
+	struct dpa_ipsec_sa_mng *sa_mng;
+	int err;
+
+	BUG_ON(!sa);
+
+	dpa_ipsec = sa->dpa_ipsec;
+	BUG_ON(!dpa_ipsec);
+
+	if (sa->used_sa_index < 0) {
+		pr_crit("Invalid used_sa_index for SA with id %d\n", sa->id);
+		return -EFAULT;
+	}
+
+	/* Acquire DPA IPSec instance lock */
+	mutex_lock(&dpa_ipsec->lock);
+	sa_mng = &dpa_ipsec->sa_mng;
+
+	/*
+	 * AV's TODO: create a cleaning function for preallocated SA structure
+	 * and call here that function
+	 */
+
+	/* Release the SA id in the SA IDs circular queue */
+	err = cq_put_4bytes(sa_mng->sa_id_cq, sa->id);
+	if (err < 0) {
+		pr_err("Could not release the sa id %d\n", sa->id);
+		/* Release DPA IPSec instance lock */
+		mutex_unlock(&dpa_ipsec->lock);
+		return -EDOM;
+	}
+
+	/* Mark as free index in used SA IDs vector of this DPA IPSEC instance*/
+	dpa_ipsec->used_sa_ids[sa->used_sa_index] = DPA_OFFLD_INVALID_OBJECT_ID;
+	dpa_ipsec->num_used_sas--;
+	sa->used_sa_index = -1;
+
+	/* Release DPA IPSec instance lock */
+	mutex_unlock(&dpa_ipsec->lock);
+
+	return 0;
+}
+
+static int rollback_create_sa(struct dpa_ipsec_sa *sa)
+{
+	struct dpa_ipsec *dpa_ipsec;
+	int err_rb;
+
+	BUG_ON(!sa);
+	dpa_ipsec = sa->dpa_ipsec;
+	BUG_ON(!dpa_ipsec);
+
+	if (sa_is_outbound(sa))
+		goto remove_fq_pair;
+
+	/* Inbound SA */
+	if (sa->inbound_hash_entry != DPA_OFFLD_INVALID_OBJECT_ID) {
+		err_rb = update_pre_sec_inbound_table(sa, MNG_OP_REMOVE);
+		if (err_rb < 0) {
+			pr_err("Couln't remove SA lookup table entry\n");
+			return err_rb;
+		}
+	}
+
+	if (sa->ipsec_hmd != DPA_OFFLD_DESC_NONE) {
+		struct hmd_entry hmd_entry;
+		hmd_entry.hmd = sa->ipsec_hmd;
+		hmd_entry.hmd_special_op = true;
+		err_rb = destroy_recycle_manip(sa, &hmd_entry);
+		if (err_rb < 0) {
+			pr_err("Could not delete manip object!\n");
+			return err_rb;
+		}
+		sa->ipsec_hmd = DPA_OFFLD_DESC_NONE;
+	}
+
+	if (dpa_ipsec->config.post_sec_in_params.do_pol_check == true &&
+	    sa->valid_flowid_entry) {
+		err_rb = remove_inbound_flow_id_classif(sa);
+		if (err_rb < 0) {
+			pr_err("Couldn't remove post decrypt tbl entry\n");
+			return err_rb;
+		}
+	}
+
+remove_fq_pair:
+	err_rb = remove_sa_fq_pair(sa);
+	if (err_rb < 0) {
+		pr_err("Could not remove SA FQs.\n");
+		return err_rb;
+	}
+
+	/* Free the SA id and FlowID (for inbound SAs only).*/
+	if (sa_is_inbound(sa) &&
+	    sa->inbound_flowid != INVALID_INB_FLOW_ID)
+		put_inbound_flowid(dpa_ipsec, sa->inbound_flowid);
+
+	err_rb = put_sa(sa);
+
+	return err_rb;
+}
+
+static int rollback_rekeying_sa(struct dpa_ipsec_sa *sa)
+{
+	struct dpa_ipsec *dpa_ipsec;
+	int err_rb;
+
+	BUG_ON(!sa);
+	dpa_ipsec = sa->dpa_ipsec;
+	BUG_ON(!dpa_ipsec);
+
+	if (sa_is_outbound(sa))
+		goto remove_fq_pair;
+
+	/* Inbound SA */
+	if (sa->inbound_hash_entry != DPA_OFFLD_INVALID_OBJECT_ID) {
+		err_rb = update_pre_sec_inbound_table(sa, MNG_OP_REMOVE);
+		if (err_rb < 0) {
+			pr_err("Couln't remove SA lookup table entry\n");
+			return err_rb;
+		}
+	}
+
+	if (sa->ipsec_hmd != DPA_OFFLD_DESC_NONE) {
+		struct hmd_entry hmd_entry;
+		hmd_entry.hmd = sa->ipsec_hmd;
+		hmd_entry.hmd_special_op = true;
+		err_rb = destroy_recycle_manip(sa, &hmd_entry);
+		if (err_rb < 0) {
+			pr_err("Could not delete manip object!\n");
+			return err_rb;
+		}
+		sa->ipsec_hmd = DPA_OFFLD_DESC_NONE;
+	}
+
+remove_fq_pair:
+	err_rb = remove_sa_fq_pair(sa);
+	if (err_rb < 0) {
+		pr_err("Could not remove SA FQs.\n");
+		return err_rb;
+	}
+
+	err_rb = put_sa(sa);
+
+	return err_rb;
+}
+
+int dpa_ipsec_init(const struct dpa_ipsec_params *params, int *dpa_ipsec_id)
+{
+	struct dpa_ipsec *dpa_ipsec = NULL;
+	uint32_t max_num_sa;
+	int err = 0;
+
+	/* multiple DPA IPSec instances are not currently supported */
+	unused(dpa_ipsec_id);
+
+	/* sanity checks */
+	if (gbl_dpa_ipsec) {
+		pr_err("There is already an initialized dpa_ipsec component.\n");
+		pr_err("Multiple DPA IPSec Instances aren't currently supported.\n");
+		return -EPERM;
+	}
+
+	/* make sure all user params are ok and init can start */
+	err = check_ipsec_params(params);
+	if (err < 0)
+		return err;
+
+	/* alloc control block */
+	dpa_ipsec = kzalloc(sizeof(*dpa_ipsec), GFP_KERNEL);
+	if (!dpa_ipsec) {
+		pr_err("Could not allocate memory for control block.\n");
+		return -ENOMEM;
+	}
+
+	/* store the control block so rollback can occur in case of error */
+	gbl_dpa_ipsec = dpa_ipsec;
+
+	/* Initialize DPA IPSec instance lock */
+	mutex_init(&dpa_ipsec->lock);
+
+	/* store parameters */
+	store_ipsec_params(dpa_ipsec, params);
+
+	/* init SA manager */
+	err = init_sa_manager(dpa_ipsec);
+	if (err < 0) {
+		free_resources();
+		return err;
+	}
+
+	/* Init used sa vector */
+	max_num_sa = dpa_ipsec->sa_mng.max_num_sa;
+	dpa_ipsec->used_sa_ids = kmalloc(max_num_sa * sizeof(u32), GFP_KERNEL);
+	if (!dpa_ipsec->used_sa_ids) {
+		pr_err("No more memory for used sa id's vector ");
+		free_resources();
+		return -ENOMEM;
+	}
+	memset(dpa_ipsec->used_sa_ids, DPA_OFFLD_INVALID_OBJECT_ID,
+	       max_num_sa * sizeof(uint32_t));
+	dpa_ipsec->num_used_sas = 0;
+
+	/* retrieve and store SEC ERA information */
+	err = get_sec_info(dpa_ipsec);
+	if (err < 0) {
+		free_resources();
+		return err;
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(dpa_ipsec_init);
+
+int dpa_ipsec_free(int dpa_ipsec_id)
+{
+	int err = 0;
+
+	/* multiple DPA IPSec instances are not currently supported */
+	unused(dpa_ipsec_id);
+
+	/* destroy all SAs offloaded in this DPA IPSec instance */
+	err = dpa_ipsec_flush_all_sa(0);
+	if (err < 0) {
+		pr_err("Could not remove all SAs from this instance!\n");
+		return err;
+	}
+
+	free_resources();
+
+	return 0;
+}
+EXPORT_SYMBOL(dpa_ipsec_free);
+
+int dpa_ipsec_create_sa(int dpa_ipsec_id,
+			struct dpa_ipsec_sa_params *sa_params, int *sa_id)
+{
+	struct dpa_ipsec *dpa_ipsec;
+	struct dpa_ipsec_sa *sa;
+	uint32_t id;
+	int err = 0, err_rb = 0;
+
+	/* multiple DPA IPSec instances are not currently supported */
+	unused(dpa_ipsec_id);
+
+	if (!gbl_dpa_ipsec) {
+		pr_err("There is no dpa_ipsec component initialized\n");
+		return -EPERM;
+	}
+
+	if (!sa_id) {
+		pr_err("Invalid SA ID holder\n");
+		return -EINVAL;
+	}
+	*sa_id = DPA_OFFLD_INVALID_OBJECT_ID;
+
+	err = check_sa_params(sa_params);
+	if (err < 0)
+		return err;
+
+	dpa_ipsec = gbl_dpa_ipsec;
+
+	err = get_new_sa(dpa_ipsec, &sa, &id);
+	if (err < 0) {
+		pr_err("Failed retrieving a preallocated SA\n");
+		return err;
+	}
+
+	/* Update internal SA structure. First acquire its lock */
+	mutex_lock(&sa->lock);
+	sa->sa_dir = sa_params->sa_dir;
+	sa->dpa_ipsec = dpa_ipsec;
+	sa->parent_sa = NULL;
+	sa->child_sa = NULL;
+	sa->sa_rekeying_node.next = LIST_POISON1;
+	sa->sa_rekeying_node.prev = LIST_POISON2;
+
+	/* Copy SA params into the internal SA structure */
+	if (sa_is_outbound(sa))
+		err = copy_sa_params_to_out_sa(sa, sa_params);
+	else /* DPA_IPSEC_INBOUND */
+		err = copy_sa_params_to_in_sa(sa, sa_params, FALSE);
+
+	if (err < 0) {
+		pr_err("Could not copy SA parameters into SA\n");
+		goto create_sa_err;
+	}
+
+	/* Initialize the SA Manip handle to an invalid value */
+	sa->ipsec_hmd = DPA_OFFLD_DESC_NONE;
+
+	/* Initialize the IPSec Manip. object (if required) for inbound SAs */
+	if (sa_is_inbound(sa)) {
+		if (sa->dpa_ipsec->config.max_sa_manip_ops == 0)
+			err = create_ipsec_manip(sa, DPA_OFFLD_DESC_NONE,
+						 &sa->ipsec_hmd);
+		else
+			err = update_ipsec_manip(sa, DPA_OFFLD_DESC_NONE,
+						 &sa->ipsec_hmd);
+		if (err < 0) {
+			pr_err("Could not create Manip object for in SA!\n");
+			goto create_sa_err;
+		}
+	}
+
+	/* Generate the split key from the normal auth key */
+	err = generate_split_key(&sa->auth_data);
+	if (err < 0)
+		goto create_sa_err;
+
+	/* Call internal function to create SEC FQ according to SA parameters */
+	err = create_sa_fq_pair(sa, FALSE, FALSE);
+	if (err < 0) {
+		pr_err("Could not create SEC frame queues\n");
+		goto create_sa_err;
+	}
+
+	if (sa_is_inbound(sa)) {
+		err = update_pre_sec_inbound_table(sa, MNG_OP_ADD);
+		if (err < 0) {
+			pr_err("Could not update PCD entry\n");
+			goto create_sa_err;
+		}
+
+		if (dpa_ipsec->config.post_sec_in_params.do_pol_check) {
+			int inbpol_td;
+			struct dpa_cls_tbl_action action;
+
+			err = get_free_inbpol_tbl(dpa_ipsec, &inbpol_td);
+			if (err < 0) {
+				pr_err("Could not get a free EM table\n");
+				goto create_sa_err;
+			}
+			sa->em_inpol_td = inbpol_td;
+
+			/*
+			 * Link Exact Match table with the index table on
+			 * inbound_flowid
+			 */
+			memset(&action, 0, sizeof(action));
+			action.type = DPA_CLS_TBL_ACTION_NEXT_TABLE;
+			action.next_table_params.next_td = inbpol_td;
+			action.enable_statistics = FALSE;
+			err = set_flow_id_action(sa, &action);
+			if (err < 0) {
+				pr_err("Can't link EM table with index table\n");
+				goto create_sa_err;
+			}
+
+			err = dpa_classif_table_modify_miss_action(inbpol_td,
+						       &sa->policy_miss_action);
+			if (err < 0) {
+				pr_err("Can't set policy miss action\n");
+				goto create_sa_err;
+			}
+		} else {
+			/* Set the post decryption default action */
+			err = set_flow_id_action(sa, &sa->def_sa_action);
+			if (err < 0) {
+				pr_err("Could not set default action for post decryption\n");
+				goto create_sa_err;
+			}
+		}
+	}
+
+	/* SA done OK. Return the SA id */
+	*sa_id = id;
+
+	/* Unlock the SA structure */
+	mutex_unlock(&sa->lock);
+
+	return 0;
+
+	/* Something went wrong. Begin roll-back */
+ create_sa_err:
+
+	/* A invalid SA ID is returned if roll-back succeeds and the actual
+	 * reserved SA ID if it fails. The SA ID can be used to try again to
+	 * free resources by calling dpa_ipsec_remove_sa
+	 */
+	err_rb = rollback_create_sa(sa);
+	if (err_rb < 0)
+		*sa_id = id;
+
+	/* Unlock the SA structure */
+	mutex_unlock(&sa->lock);
+
+	return err;
+}
+EXPORT_SYMBOL(dpa_ipsec_create_sa);
+
+/*
+ * Expects that locks are acquired for this SA and its child if any.
+ * Expects that no child SA is passed to it
+ *
+ * Function description:
+ * Steps done for a parent SA:
+ * 1. Remove the PCD entry that makes traffic to go to SEC
+ * 2. Wait until SEC consumes the frames in the TO_SEC queue of this SA
+ * 3. Schedule the TO SEC FQ of the child SA even if no traffic arrived on it
+ * 4. Inherit parent SA's inbound post decryption classification
+ * 5. If policy verification is enabled inherit parent SA's policies.
+ * 6. Remove the child SA for the rekeying list
+ * 7. Remove the parent SA's TO_SEC FQ
+ * 8. Free all memory used for this SA i.e recycle this SA
+ *
+ * Steps done for a single SA:
+ * 1. Remove the PCD entries that make traffic to go to SEC
+ * 2. Remove TO_SEC FQ and FROM_SEC FQ
+ *	2.1. Wait until SEC consumes the frames in the TO_SEC queue of this SA
+ *	2.2. Wait until FROM_SEC queue gets empty, frames are distributed by the
+ *	     post SEC offline port according to its PCD entries
+ * 3. If policy verification is enabled, flush SA policies.
+ * 4. Remove the index entry from the post SEC offline port index table
+ * 5. Free all memory used for this SA i.e recycle this SA
+ */
+static int remove_inbound_sa(struct dpa_ipsec_sa *sa)
+{
+	struct dpa_ipsec_sa *child_sa;
+	int err, timeout = WAIT4_FQ_EMPTY_TIMEOUT;
+
+	if (sa_is_parent(sa)) {
+		child_sa = sa->child_sa;
+		/* Remove PCD entry that makes traffic go to SEC */
+		if (sa->inbound_hash_entry != DPA_OFFLD_INVALID_OBJECT_ID) {
+			err = remove_inbound_hash_entry(sa);
+			if (err == -ENOTRECOVERABLE)
+				return err;
+		}
+
+		/* destroy SA manip */
+		if (sa->ipsec_hmd != DPA_OFFLD_DESC_NONE) {
+			struct hmd_entry hmd_entry;
+			hmd_entry.hmd = sa->ipsec_hmd;
+			hmd_entry.hmd_special_op = true;
+			err = destroy_recycle_manip(sa, &hmd_entry);
+			if (err < 0) {
+				pr_err("Could not delete manip object!\n");
+				return err;
+			}
+			sa->ipsec_hmd = DPA_OFFLD_DESC_NONE;
+		}
+
+		err = wait_until_fq_empty(sa->to_sec_fq, timeout);
+		if (err < 0) {
+			pr_err("Waiting old SA's TO SEC FQ to get empty\n");
+			return -ETIME;
+		}
+
+		/* schedule child SA */
+		err = schedule_sa(child_sa);
+		if (unlikely(err < 0)) {
+			pr_err("Scheduling child SA %d failed\n", child_sa->id);
+			return -EIO;
+		}
+
+		/* Update the child SA with parent SA's inbound indx entry */
+		child_sa->valid_flowid_entry = sa->valid_flowid_entry;
+
+		/* Inherit parent SA's policy list and then set it empty */
+		if (sa->dpa_ipsec->config.post_sec_in_params.do_pol_check)
+			list_splice_init(&sa->policy_headlist,
+					 &child_sa->policy_headlist);
+
+		/* Acquire protective lock for the SA rekeying list */
+		mutex_lock(&sa->dpa_ipsec->sa_mng.sa_rekeying_headlist_lock);
+
+		child_sa->parent_sa = NULL;
+		sa->child_sa = NULL;
+
+		/* Remove the child SA from rekeying list */
+		list_del(&child_sa->sa_rekeying_node);
+
+		/* Invalidate the FROM SEC FQ of parent SA */
+		memset(sa->from_sec_fq, 0, sizeof(struct qman_fq));
+		sa->from_sec_fq->fqid = 0;
+
+		/* Release the list lock so other threads may use it */
+		mutex_unlock(&sa->dpa_ipsec->sa_mng.sa_rekeying_headlist_lock);
+
+		/* Call rekeying callback to inform upper layer that rekeying
+		 * process was finished for this SA and is ready for use */
+		if (child_sa->rekey_event_cb)
+			child_sa->rekey_event_cb(0, child_sa->id, 0);
+
+		/* Now free the parent SA structure and all its resources */
+		err = remove_sa_sec_fq(sa->to_sec_fq);
+		if (err < 0) {
+			pr_err("Couln't remove SA %d TO SEC FQ\n", sa->id);
+			return -EUCLEAN;
+		}
+
+		/* Recycle parent SA memory */
+		err = put_sa(sa);
+		if (unlikely(err < 0)) {
+			pr_err("Could not recycle parent SA.\n");
+			return -EDQUOT;
+		}
+
+		return 0;
+	}
+
+	BUG_ON(sa_is_child(sa));
+
+	/* SA is single i.e has no child and can't be child for other SA */
+
+	/* Remove PCD entry that makes traffic go to SEC entry is valid */
+	if (sa->inbound_hash_entry != DPA_OFFLD_INVALID_OBJECT_ID) {
+		err = remove_inbound_hash_entry(sa);
+		if (err == -ENOTRECOVERABLE)
+			return err;
+	}
+
+	/* destroy SA manip */
+	if (sa->ipsec_hmd != DPA_OFFLD_DESC_NONE) {
+		struct hmd_entry hmd_entry;
+		hmd_entry.hmd = sa->ipsec_hmd;
+		hmd_entry.hmd_special_op = true;
+		err = destroy_recycle_manip(sa, &hmd_entry);
+		if (err < 0) {
+			pr_err("Could not delete manip object!\n");
+			return err;
+		}
+		sa->ipsec_hmd = DPA_OFFLD_DESC_NONE;
+	}
+
+	/* Destroy the TO_SEC and FROM_SEC queues */
+	err = remove_sa_fq_pair(sa);
+	if (err != 0) {
+		pr_err("Could not remove the SEC frame queues\n");
+		return err;
+	}
+
+	/* Flush policy if policy check is enabled */
+	if (sa->dpa_ipsec->config.post_sec_in_params.do_pol_check) {
+		err = sa_flush_policies(sa);
+		if (err < 0) {
+			pr_err("Could not flush inbound policies");
+			return err;
+		}
+	}
+
+	/* Remove the flow id classification after decryption */
+	if (sa->valid_flowid_entry) {
+		err = remove_inbound_flow_id_classif(sa);
+		if (err < 0)
+			return err;
+	}
+
+	/* Mark SA as free */
+	err = put_sa(sa);
+	if (err < 0) {
+		pr_err("Could not recycle the sa with id %d\n", sa->id);
+		return err;
+	}
+
+	return 0;
+}
+
+/*
+ * Expects that locks are acquired for this SA and its child if any.
+ * Expects that no child SA is passed to it
+ *
+ * Function description:
+ * Steps done for a parent SA:
+ * 1. Call the sa_rekeying_outbound function which is going to:
+ *	a. wait until TO SEC FQ is empty or timeout
+ *	b. schedule the child TO SEC FQ
+ *	c. remove the parent TO SEC FQ
+ *	d. free all memory used for this SA i.e recycle this SA
+ * 2. In case error code is telling that child SA is ready to use, i.e
+ *    sa_rekeying_outbound returned 0, -EUCLEAN, -EDQUOT:
+ *	a. lock SA rekeying list
+ *	b. set as single the parent SA and child SA i.e parent SA has no child
+ *	   and child SA has no parent
+ *	c. remove the child SA from the rekeying list, rekeying was complete
+ *	d. invalidate parent SA's TO SEC FQ
+ *	e. unlock SA rekeying list
+ *	f. if child SA has a valid callback trigger this call to inform upper
+ *	   layer that this SA was rekeyed successfully.
+ *
+ * Steps done for a single SA:
+ * 1. Flush SA policies i.e remove the PCD entries that direct traffic to SEC
+ * 2. Remove TO_SEC FQ and FROM_SEC FQ
+ *	2.1. Wait until SEC consumes the frames in the TO_SEC queue of this SA
+ *	2.2. Wait until FROM_SEC queue gets empty, frames are distributed by the
+ *	     post SEC offline port according to its PCD entries
+ * 3. Free all memory used for this SA i.e recycle this SA
+ */
+static int remove_outbound_sa(struct dpa_ipsec_sa *sa)
+{
+	struct dpa_ipsec_sa *child_sa;
+	int err;
+
+	if (sa_is_parent(sa)) {
+		struct dpa_ipsec_sa_mng *sa_mng;
+
+		BUG_ON(!sa->dpa_ipsec);
+		sa_mng = &sa->dpa_ipsec->sa_mng;
+
+		child_sa = sa->child_sa;
+
+		err = sa_rekeying_outbound(child_sa);
+
+		/* Remove child SA from rekeying list if processing was OK */
+		if (err == 0 || err == -EUCLEAN || err == -EDQUOT) {
+			/* Acquire protective lock for the SA rekeying list */
+			mutex_lock(&sa_mng->sa_rekeying_headlist_lock);
+
+			sa->child_sa = NULL;
+			child_sa->parent_sa = NULL;
+
+			/* Remove the child SA from rekeying list */
+			list_del(&child_sa->sa_rekeying_node);
+
+			/* Invalidate the FROM SEC FQ of parent SA */
+			memset(sa->from_sec_fq, 0, sizeof(struct qman_fq));
+			sa->from_sec_fq->fqid = 0;
+
+			/* Release the list lock so other threads may use it */
+			mutex_unlock(&sa_mng->sa_rekeying_headlist_lock);
+
+			/*
+			 * Call rekeying callback to inform upper layer that
+			 * rekeying process was finished for this SA and is
+			 * ready for used
+			 */
+			if (child_sa->rekey_event_cb)
+				child_sa->rekey_event_cb(0, child_sa->id, 0);
+		}
+
+		return err;
+	}
+
+	BUG_ON(sa_is_child(sa));
+
+	/* SA is single i.e has no child and can't be child for other SA */
+
+	/* Flush policies i.e remove PCD entries that direct traffic to SEC */
+	err = sa_flush_policies(sa);
+	if (err < 0) {
+		pr_err("Could not flush outbound policies\n");
+		return err;
+	}
+
+	/* destroy SA manip, if one was initialized */
+	if (sa->ipsec_hmd != DPA_OFFLD_DESC_NONE) {
+		struct hmd_entry hmd_entry;
+		hmd_entry.hmd = sa->ipsec_hmd;
+		hmd_entry.hmd_special_op = true;
+		err = destroy_recycle_manip(sa, &hmd_entry);
+		if (err < 0) {
+			pr_err("Couldn't delete SA manip\n");
+			return err;
+		}
+		sa->ipsec_hmd = DPA_OFFLD_DESC_NONE;
+	}
+
+	/* Destroy the TO_SEC and FROM_SEC queues */
+	err = remove_sa_fq_pair(sa);
+	if (err < 0) {
+		pr_err("Could not remove the SEC frame queues\n");
+		return err;
+	}
+
+	/* Mark SA as free */
+	err = put_sa(sa);
+	if (err < 0) {
+		pr_err("Could not recycle the SA id %d\n", sa->id);
+		return err;
+	}
+
+	return 0;
+}
+
+/*
+ * Function description:
+ *
+ * SA is single (has no child SA and its not a child for other SA):
+ *	- acquire lock for SA, return -EAGAIN if lock is contended.
+ * SA is child:
+ *	- return error code -EINPROGRESS since this SA is in rekeying process
+ *	- to remove this SA first must be removed its parent SA using API or the
+ *	  rekeying process finished successfully for this SA
+ * SA is parent:
+ *	- always acquire parent SA lock before child SA lock
+ *	- acquire lock for SA, return -EAGAIN if lock is contended
+ *	- acquire lock for SA's child, return -EAGAIN if lock is contended
+ *	- call remove_inbound_sa or remove_outbound_sa depending on the SA
+ *	  direction, which will do the work required and call the rekeying
+ *	  callback to inform upper layer about the child SA success.
+ *	  Returned code:
+ *		a. -ENOTRECOVERABLE if failed to removed the
+ *		    PCD entry of the inbound SA that makes traffic go to SEC.
+ *		    recommended action: recall this function for several times
+ *		    and if the returned code is the same, then reboot the system
+ *		b. -ETIME if the parent SA's TO SEC FQ is not yet empty
+ *		c. -EIO if failed to schedule the child's TO SEC FQ. Unlikely.
+ *		d. -EUCLEAN if parent SA needs cleaning (its TO SEC FQ couldn't
+ *		    be removed)
+ *		e. -EDQUOT if failed to recycle the parent SA.
+ *	  In case of -EUCLEAN and -EDQUOT the recommended action is to call
+ *	  dpa_ipsec_remove with the parent SA id. Child SA id is ready to work.
+ *	- release SA's child lock
+ *	- release SA lock
+ */
+int dpa_ipsec_remove_sa(int sa_id)
+{
+	struct dpa_ipsec_sa *sa, *child_sa = NULL;
+	int ret = 0;
+
+	sa = get_sa_from_sa_id(sa_id);
+	if (!sa) {
+		pr_err("Invalid SA handle for SA id %d\n", sa_id);
+		return -EINVAL;
+	}
+
+	/* Always acquire parent lock before child's lock */
+	ret = mutex_trylock(&sa->lock);
+	if (ret == 0)
+		return -EAGAIN;
+
+	if (sa_is_child(sa)) {
+		pr_err("This SA %d is a child in rekeying process\n", sa_id);
+		mutex_unlock(&sa->lock);
+		return -EINPROGRESS;
+	}
+
+	/* SA is parent? If so acquire its child's lock */
+	if (sa_is_parent(sa)) {
+		child_sa = sa->child_sa;
+		ret = mutex_trylock(&child_sa->lock);
+		if (ret == 0) {
+			mutex_unlock(&sa->lock);
+			return -EAGAIN;
+		}
+	}
+
+	if (sa_is_inbound(sa))
+		ret = remove_inbound_sa(sa);
+	else
+		ret = remove_outbound_sa(sa);
+
+	/* Release child's lock first */
+	if (child_sa)
+		mutex_unlock(&child_sa->lock);
+
+	/* Release parent lock */
+	mutex_unlock(&sa->lock);
+
+	return ret;
+}
+EXPORT_SYMBOL(dpa_ipsec_remove_sa);
+
+int dpa_ipsec_sa_add_policy(int sa_id,
+			    struct dpa_ipsec_policy_params *policy_params)
+{
+	struct dpa_ipsec_policy_entry *policy_entry;
+	struct dpa_ipsec_sa *sa;
+	int ret;
+
+	if (!policy_params) {
+		pr_err("Invalid policy params handle\n");
+		return -EINVAL;
+	}
+
+	sa = get_sa_from_sa_id(sa_id);
+	if (!sa) {
+		pr_err("Invalid SA handle\n");
+		return -EINVAL;
+	}
+
+	ret = mutex_trylock(&sa->lock);
+	if (ret == 0) {
+		pr_err("Failed to acquire lock for SA %d\n", sa->id);
+		return -EBUSY;
+	}
+
+	ret = check_policy_params(sa, policy_params);
+	if (ret < 0) {
+		mutex_unlock(&sa->lock);
+		return ret;
+	}
+
+	if (sa_is_parent(sa) && sa_is_outbound(sa)) {
+		pr_err("Illegal to set out policy - parent SA %d\n", sa->id);
+		mutex_unlock(&sa->lock);
+		return -EPERM;
+	}
+
+	if (sa_is_child(sa) && sa_is_inbound(sa)) {
+		pr_err("Illegal to set in policy on child SA %d\n", sa->id);
+		mutex_unlock(&sa->lock);
+		return -EPERM;
+	}
+
+	/*
+	 * One SA could have more in/out policies
+	 * Store all the in/out policies into the SA policy param list in order
+	 * to know what to remove when SA expires.
+	 */
+	ret = store_policy_param_to_sa_pol_list(sa, policy_params,
+						&policy_entry);
+	if (ret < 0) {
+		pr_err("Could not store the policy in the SA\n");
+		mutex_unlock(&sa->lock);
+		return ret;
+	}
+
+	/*Insert inbound or outbound policy for this SA depending on it's type*/
+	if (sa_is_inbound(sa)) {
+		ret = update_inbound_policy(sa, policy_entry, MNG_OP_ADD);
+		if (ret < 0) {
+			remove_policy_from_sa_policy_list(sa, policy_entry);
+			pr_err("Could not add the inbound policy\n");
+			kfree(policy_entry);
+		}
+	} else {  /* DPA_IPSEC_OUTBOUND */
+		ret = update_outbound_policy(sa, policy_entry, MNG_OP_ADD);
+		if (ret < 0) {
+			remove_policy_from_sa_policy_list(sa, policy_entry);
+			pr_err("Could not add the outbound policy\n");
+			kfree(policy_entry);
+		}
+	}
+
+	mutex_unlock(&sa->lock);
+
+	return ret;
+}
+EXPORT_SYMBOL(dpa_ipsec_sa_add_policy);
+
+int dpa_ipsec_sa_remove_policy(int sa_id,
+			       struct dpa_ipsec_policy_params *policy_params)
+{
+	struct dpa_ipsec_policy_entry *policy_entry;
+	struct dpa_ipsec_sa *sa;
+	int ret = 0;
+
+	if (!policy_params) {
+		pr_err("Invalid policy parameters handle\n");
+		return -EINVAL;
+	}
+
+	sa = get_sa_from_sa_id(sa_id);
+	if (!sa) {
+		pr_err("Invalid SA handle provided\n");
+		return -EINVAL;
+	}
+
+	ret = mutex_trylock(&sa->lock);
+	if (ret == 0) {
+		pr_err("Failed to acquire lock for SA %d\n", sa->id);
+		return -EBUSY;
+	}
+
+	if (sa_is_parent(sa) && sa_is_outbound(sa)) {
+		pr_err("Illegal removing out policy parent SA %d\n", sa->id);
+		mutex_unlock(&sa->lock);
+		return -EPERM;
+	}
+
+	if (sa_is_child(sa) && sa_is_inbound(sa)) {
+		pr_err("Illegal removing in policy, child SA %d\n", sa->id);
+		mutex_unlock(&sa->lock);
+		return -EPERM;
+	}
+
+	ret = find_policy(sa, policy_params, &policy_entry);
+	if (ret < 0) {
+		pr_err("Could not find policy entry in SA policy list\n");
+		mutex_unlock(&sa->lock);
+		return ret;
+	}
+
+	/*
+	 * found the policy entry in SA policy parameter list;
+	 * depending on the type of the SA remove the PCD entry for this policy
+	 * and afterwards remove the policy param from SA policy param list
+	 */
+	ret = remove_policy(sa, policy_entry);
+
+	mutex_unlock(&sa->lock);
+
+	return ret;
+}
+EXPORT_SYMBOL(dpa_ipsec_sa_remove_policy);
+
+/*
+ * Returned error code: -EUSERS
+ *	- if both parent SA and child SA are in invalid state, some or none of
+ *	  the old's policies were safely transfered to the child SA but some
+ *	  policies remained offloaded through parent SA.
+ */
+int dpa_ipsec_sa_rekeying(int sa_id,
+			  struct dpa_ipsec_sa_params *sa_params,
+			  dpa_ipsec_rekey_event_cb rekey_event_cb,
+			  bool auto_rmv_old_sa,
+			  int *new_sa_id)
+{
+	struct dpa_ipsec *dpa_ipsec = NULL;
+	struct dpa_ipsec_sa_mng *sa_mng = NULL;
+	struct dpa_ipsec_sa *old_sa, *new_sa;
+	struct dpa_ipsec_policy_entry *policy_entry, *tmp_policy_entry;
+	struct timeval timeval;
+	unsigned long jiffies_to_wait;
+	uint32_t id;
+	int err = 0, err_rb;
+
+	if (!gbl_dpa_ipsec) {
+		pr_err("There is no dpa_ipsec instance initialized\n");
+		return -EPERM;
+	}
+	dpa_ipsec = gbl_dpa_ipsec;
+
+	if (!new_sa_id) {
+		pr_err("Invalid SA ID holder\n");
+		return -EINVAL;
+	}
+	*new_sa_id = DPA_OFFLD_INVALID_OBJECT_ID;
+
+	err = check_sa_params(sa_params);
+	if (err < 0)
+		return err;
+
+	old_sa = get_sa_from_sa_id(sa_id);
+	if (!old_sa) {
+		pr_err("Invalid SA handle provided\n");
+		return -EINVAL;
+	}
+
+	/* Acquire parent SA's lock */
+	err = mutex_trylock(&old_sa->lock);
+	if (err == 0) {
+		pr_err("SA %d is being used\n", old_sa->id);
+		return -EBUSY;
+	}
+
+	/* Check if SA is currently in rekeying process */
+	if (sa_currently_in_rekeying(old_sa)) {
+		pr_err("SA with id %d is already in rekeying process\n",
+			  old_sa->id);
+		mutex_unlock(&old_sa->lock);
+		return -EEXIST;
+	}
+
+	/* Check if new SA parameters are matching the rekeyed SA */
+	if (old_sa->sa_dir != sa_params->sa_dir) {
+		pr_err("New SA parameters don't match the parent SA %d\n",
+			  old_sa->sa_dir);
+		mutex_unlock(&old_sa->lock);
+		return -EINVAL;
+	}
+
+	err = get_new_sa(dpa_ipsec, &new_sa, &id);
+	if (err < 0) {
+		pr_err("Failed retrieving a preallocated SA\n");
+		mutex_unlock(&old_sa->lock);
+		return err;
+	}
+
+	/* Update the new SA structure */
+	mutex_lock(&new_sa->lock);
+	new_sa->dpa_ipsec = old_sa->dpa_ipsec;
+	new_sa->inbound_flowid = old_sa->inbound_flowid;
+	new_sa->ipsec_hmd = old_sa->ipsec_hmd;
+	new_sa->valid_flowid_entry = false;
+	new_sa->rekey_event_cb = rekey_event_cb;
+	if (auto_rmv_old_sa) {
+		new_sa->parent_sa = old_sa;
+		new_sa->child_sa  = NULL;
+			new_sa->sa_rekeying_node.next = LIST_POISON1;
+			new_sa->sa_rekeying_node.prev = LIST_POISON2;
+		old_sa->child_sa = new_sa;
+			old_sa->parent_sa = NULL;
+	} else {
+		new_sa->parent_sa = NULL;
+		new_sa->child_sa  = NULL;
+		old_sa->child_sa  = NULL;
+		old_sa->parent_sa = NULL;
+	}
+
+	/* Copy SA params into the internal SA structure */
+	if (sa_is_outbound(old_sa))
+		err = copy_sa_params_to_out_sa(new_sa, sa_params);
+	else
+		err = copy_sa_params_to_in_sa(new_sa, sa_params, true);
+
+	if (err < 0) {
+		pr_err("Could not copy SA parameters into SA\n");
+		goto rekey_sa_err;
+	}
+
+	/* Initialize the IPSec Manip. object (if required) for inbound SAs */
+	if (sa_is_inbound(new_sa)) {
+		if (new_sa->dpa_ipsec->config.max_sa_manip_ops == 0)
+			err = create_ipsec_manip(new_sa, DPA_OFFLD_DESC_NONE,
+						 &new_sa->ipsec_hmd);
+		else
+			err = update_ipsec_manip(new_sa, DPA_OFFLD_DESC_NONE,
+						 &new_sa->ipsec_hmd);
+		if (err < 0) {
+			pr_err("Could not create Manip object for in SA!\n");
+			goto rekey_sa_err;
+		}
+	}
+
+	/* Generate the split key from the normal auth key */
+	err = generate_split_key(&new_sa->auth_data);
+	if (err < 0)
+		goto rekey_sa_err;
+
+	/*
+	 * Update the new SA with information from the old SA
+	 * The from SEC frame queue of the old SA will be used by the new SA
+	 */
+	memcpy(new_sa->from_sec_fq, old_sa->from_sec_fq,
+	       sizeof(struct qman_fq));
+
+	/* Exact match table will be reused by the new SA. */
+	new_sa->em_inpol_td = old_sa->em_inpol_td;
+
+	/* Create SEC queues according to SA parameters */
+	err = create_sa_fq_pair(new_sa, true, true);
+	if (err < 0) {
+		pr_err("Could not create SEC frame queues\n");
+		goto rekey_sa_err;
+	}
+
+	timeval.tv_sec = 0;
+	timeval.tv_usec = REKEY_SCHED_DELAY;
+	jiffies_to_wait = timeval_to_jiffies(&timeval);
+
+	/*
+	 * AV's note: Since we have reused the FROM SEC FQ it is not needed to
+	 * make another entry in the table of the post SEC OH PORT.
+	 */
+	if (sa_is_outbound(new_sa)) {
+		INIT_LIST_HEAD(&new_sa->policy_headlist);
+
+		/* Update child's SA policies if its parent SA has policies */
+		list_for_each_entry_safe(policy_entry, tmp_policy_entry,
+					 &old_sa->policy_headlist,
+					 node) {
+			err = update_outbound_policy(new_sa,
+						     policy_entry,
+						     MNG_OP_MODIFY);
+			if (err < 0) {
+				/*
+				 * Keep both SAs. Both must be removed
+				 * using remove_sa
+				 */
+				*new_sa_id = new_sa->id;
+				pr_err("Could't modify outbound policy\n");
+				new_sa->parent_sa = NULL;
+				new_sa->child_sa  = NULL;
+				old_sa->child_sa  = NULL;
+				old_sa->parent_sa = NULL;
+				/*
+				 * AV's note TODO: investigate the removal of FQ
+				 * to SEC even is it has frames in it and is in
+				 * parked state
+				 */
+				mutex_unlock(&new_sa->lock);
+				mutex_unlock(&old_sa->lock);
+				return -EUSERS;
+			}
+			list_del(&policy_entry->node);
+			list_add(&policy_entry->node, &new_sa->policy_headlist);
+		}
+
+		/*
+		 * Need to destroy the old SA. Have to wail until its TO SEC
+		 * FQ is empty. This is done in work queue, schedule it.
+		 */
+		sa_mng = &dpa_ipsec->sa_mng;
+
+		mutex_lock(&sa_mng->sa_rekeying_headlist_lock);
+		list_add_tail(&new_sa->sa_rekeying_node,
+			 &sa_mng->sa_rekeying_headlist);
+		mutex_unlock(&sa_mng->sa_rekeying_headlist_lock);
+
+		queue_delayed_work(sa_mng->sa_rekeying_wq,
+				   &sa_mng->sa_rekeying_work,
+				   jiffies_to_wait);
+	} else {		/* DPA_IPSEC_INBOUND */
+		/* Need to update the IN SA PCD entry */
+		err = update_pre_sec_inbound_table(new_sa, MNG_OP_ADD);
+		if (err < 0) {
+			pr_err("Could not add PCD entry for new SA\n");
+			goto rekey_sa_err;
+		}
+
+		if (auto_rmv_old_sa) {
+			sa_mng = &dpa_ipsec->sa_mng;
+			/* Add new SA into the sa_rekeying_headlist */
+			mutex_lock(&sa_mng->sa_rekeying_headlist_lock);
+			list_add_tail(&new_sa->sa_rekeying_node,
+				 &sa_mng->sa_rekeying_headlist);
+			mutex_unlock(&sa_mng->sa_rekeying_headlist_lock);
+
+			/* schedule inbound SA's rekeying */
+			queue_delayed_work(sa_mng->sa_rekeying_wq,
+					   &sa_mng->sa_rekeying_work,
+					   jiffies_to_wait);
+		} else {
+			/*
+			 * The old SA has to be removed using the
+			 * dpa_ipsec_remove_sa function when the hard SA
+			 * expiration time limit is reached.
+			 *
+			 * Since the difference between soft and hard limit
+			 * can be several seconds it is required to schedule the
+			 * TO SEC FQ of the new SA.
+			 */
+			err = qman_schedule_fq(new_sa->to_sec_fq);
+			if (err < 0) {
+				mutex_unlock(&new_sa->lock);
+				mutex_unlock(&old_sa->lock);
+				return err;
+			}
+		}
+	}
+
+	/* Rekeying done ok. */
+	*new_sa_id = new_sa->id;
+	mutex_unlock(&new_sa->lock);
+	mutex_unlock(&old_sa->lock);
+
+	return 0;
+
+/*
+ * Rekeying failed before updating/adding any table entry.
+ * It is safe to remove new SA
+ */
+rekey_sa_err:
+	/*
+	 * If rollback was successful return an invalid ID for new SA,
+	 * otherwise return the acquired SA id so that upper layer could use it
+	 * in subsequent attempts of removing it by calling dpa_ipsec_remove_sa
+	 */
+
+	err_rb = rollback_rekeying_sa(new_sa);
+	if (err_rb < 0)
+		*new_sa_id = new_sa->id;
+
+	mutex_unlock(&new_sa->lock);
+	mutex_unlock(&old_sa->lock);
+
+	return err;
+}
+EXPORT_SYMBOL(dpa_ipsec_sa_rekeying);
+
+/*
+ * Expects that SA's parent and SA's lock are acquired in this order.
+ *
+ * Function description:
+ *	1. wait until TO SEC FQ is empty or timeout
+ *	2. schedule the child TO SEC FQ
+ *	3. remove the parent TO SEC FQ
+ *	4. free all memory used for this SA i.e recycle this SA
+ *
+ * Rekeying process successful if the returned error was: 0, -EUCLEAN, -EDQUOT
+ *	- error code 0 for perfect rekeying
+ *	- error code -EUCLEAN if during rekeying process the removal of the
+ *	  TO SEC FQ of old SA failed. Upper layer has to call the
+ *	  dpa_ipsec_remove_sa at a later time (not from callback) to try again
+ *	  freeing old SA resources. New SA is working perfectly.
+ *	- error code -EDQUOT if failed to recycle old SA memory. Upper layer
+ *	  has to call the dpa_ipsec_remove_sa at a later time (not from
+ *	  callback)to try again recycling old SA.
+ *
+ * Rekeying process in progress if the returned error is: -ETIME or -EIO.
+ *	- error code -ETIME if timeout occurred when waiting for old SA TO SEC
+ *	  FQ to get empty. Upper layer has nothing to do since the old SA TO
+ *	  SEC FQ will get empty eventually.
+ *	- error code -EIO if rekeying failed to schedule the new SA. Upper layer
+ *	  has nothing to do since the new SA TO SEC FQ will get scheduled
+ *	  eventually.
+ */
+static int sa_rekeying_outbound(struct dpa_ipsec_sa *new_sa)
+{
+	struct dpa_ipsec_sa *old_sa;
+	int err, timeout = WAIT4_FQ_EMPTY_TIMEOUT; /* microseconds */
+
+	old_sa = new_sa->parent_sa;
+	BUG_ON(!old_sa);
+
+	err = wait_until_fq_empty(old_sa->to_sec_fq, timeout);
+	if (err < 0) {
+		pr_err("Waiting old SA's TO SEC FQ to get empty. Timeout\n");
+		return -ETIME;
+	}
+
+	/* Schedule the new SA */
+	err = qman_schedule_fq(new_sa->to_sec_fq);
+	if (unlikely(err < 0)) {
+		pr_err("Scheduling the new SA %d failed\n", new_sa->id);
+		return -EIO;
+	}
+
+	/* Now free the old SA structure and all its resources */
+	err = remove_sa_sec_fq(old_sa->to_sec_fq);
+	if (err < 0) {
+		pr_err("Couln't remove old SA's %d TO SEC FQ\n", old_sa->id);
+		rekey_err_report(new_sa->rekey_event_cb, 0, new_sa->id,
+				 -EUCLEAN);
+		return -EUCLEAN;
+	}
+
+	/* Recycle SA memory */
+	err = put_sa(old_sa);
+	if (unlikely(err < 0)) {
+		pr_err("Could not recycle parent SA.\n");
+		rekey_err_report(new_sa->rekey_event_cb, 0, new_sa->id,
+				 -EDQUOT);
+		return -EDQUOT;
+	}
+
+	return 0;
+}
+
+/*
+ * Expects that SA's parent and SA's lock are acquired in this order.
+ *
+ * Function description:
+ * Rekeying process successful if the returned error was: 0, -EUCLEAN, -EDQUOT
+ *	- error code 0 for perfect rekeying
+ *	- error code -EUCLEAN if during rekeying process the removal of the
+ *	  TO SEC FQ of old SA failed. Upper layer has to call the
+ *	  dpa_ipsec_remove_sa at a later time (not from callback) to try again
+ *	  freeing old SA resources. New SA is working perfectly.
+ *	- error code -EDQUOT if failed to recycle old SA memory. Upper layer
+ *	  has to call the dpa_ipsec_remove_sa at a later time (not from
+ *	  callback)to try again recycling old SA.
+ *
+ * Rekeying process in progress if the returned error is: -EINPROGRESS, -ETIME
+ *	  or -EIO.
+ *	- error code -EINPROGRESS if no frame arrived on the new SA TO SEC FQ.
+ *	  If HARD expiration event occurs on the old SA and rekeying is still
+ *	  in progress the upper layer should call the dpa_ipsec_remove_sa with
+ *	  old SA id which will remove old SA and will automatically schedule
+ *	  new SA even if no frames have arrived on the new SA TO SEC FQ.
+ *	- error code -ETIME if timeout occurred when waiting for old SA TO SEC
+ *	  FQ to get empty. Upper layer has nothing to do since the old SA TO
+ *	  SEC FQ will get empty eventually.
+ *	- error code -EIO if rekeying failed to schedule the new SA. Upper layer
+ *	  has nothing to do since the old SA TO SEC FQ will get scheduled
+ *	  eventually.
+ *
+ * Rekeying in critical state: -ENOTRECOVERABLE
+ *	- Failed to delete the hash entry formed by old SA (SPI, ...)
+ *	- If an attacker would sent frames matching the old SA (SPI, ...) than
+ *	  FMAN will direct those frames to old SA FQ. In this case the wait
+ *	  until old SA FQ is empty is not valid, since being attacked this FQ
+ *	  might not get empty.
+ *	  There is a tiny probability that above scenario to happen, but if it
+ *	  does for several times on the same SA the recommended action would
+ *	  be to call the dpa_ipsec_remove_sa with the parent SA id. In case this
+ *	  function also fails several times then we recommend to reboot the
+ *	  system.
+ */
+static int sa_rekeying_inbound(struct dpa_ipsec_sa *new_sa)
+{
+	struct dpa_ipsec_sa *old_sa;
+	struct qm_mcr_queryfq_np queryfq_np;
+	int err = 0, timeout = WAIT4_FQ_EMPTY_TIMEOUT; /* microseconds */
+
+	/* Check if the new SA TO SEC FQ has frame descriptors enqueued in it */
+	qman_query_fq_np(new_sa->to_sec_fq, &queryfq_np);
+	if (queryfq_np.frm_cnt == 0)
+		return -EINPROGRESS;
+
+	/*
+	 * Received at least one packet encrypted with the new SA.
+	 * Remove PCD entry that makes traffic go to SEC if the entry is valid.
+	 */
+	old_sa = new_sa->parent_sa;
+	BUG_ON(!old_sa);
+
+	if (old_sa->inbound_hash_entry != DPA_OFFLD_INVALID_OBJECT_ID) {
+		err = remove_inbound_hash_entry(old_sa);
+		if (err < 0) {
+			rekey_err_report(new_sa->rekey_event_cb, 0, new_sa->id,
+					 err);
+			if (err == -ENOTRECOVERABLE)
+				return err;
+		}
+
+		/* destroy SA manip */
+		if (old_sa->ipsec_hmd != DPA_OFFLD_DESC_NONE) {
+			struct hmd_entry hmd_entry;
+			hmd_entry.hmd = old_sa->ipsec_hmd;
+			hmd_entry.hmd_special_op = true;
+			err = destroy_recycle_manip(old_sa, &hmd_entry);
+			if (err < 0) {
+				pr_err("Could not delete manip object!\n");
+				return err;
+			}
+			old_sa->ipsec_hmd = DPA_OFFLD_DESC_NONE;
+		}
+	}
+
+	err = wait_until_fq_empty(old_sa->to_sec_fq, timeout);
+	if (err < 0) {
+		pr_err("Waiting old SA's TO SEC FQ to get empty. Timeout\n");
+		return -ETIME;
+	}
+
+	/* schedule new inbound SA */
+	err = qman_schedule_fq(new_sa->to_sec_fq);
+	if (unlikely(err < 0)) {
+		pr_err("Scheduling the new SA %d failed\n", new_sa->id);
+		return -EIO;
+	}
+
+	/* Update the new SA with old SA's inbound indx entry */
+	new_sa->valid_flowid_entry = old_sa->valid_flowid_entry;
+
+	/* Inherit old SA policy list and then set it empty */
+	if (old_sa->dpa_ipsec->config.post_sec_in_params.do_pol_check)
+		list_splice_init(&old_sa->policy_headlist,
+				 &new_sa->policy_headlist);
+
+	/* Now free the old SA structure and all its resources */
+	err = remove_sa_sec_fq(old_sa->to_sec_fq);
+	if (err < 0) {
+		pr_err("Couln't remove old SA's %d TO SEC FQ\n", old_sa->id);
+		rekey_err_report(new_sa->rekey_event_cb, 0, new_sa->id,
+				 -EUCLEAN);
+		return -EUCLEAN;
+	}
+
+	/* Recycle SA memory */
+	err = put_sa(old_sa);
+	if (unlikely(err < 0)) {
+		pr_err("Could not recycle parent SA.\n");
+		rekey_err_report(new_sa->rekey_event_cb, 0, new_sa->id,
+				 -EDQUOT);
+		return -EDQUOT;
+	}
+
+	return 0;
+}
+
+static inline struct dpa_ipsec_sa *find_and_lock_sa_to_work_on(
+					struct dpa_ipsec_sa *child_sa,
+					struct dpa_ipsec_sa_mng *sa_mng)
+{
+	struct dpa_ipsec_sa *parent_sa;
+	struct list_head *head;
+	int err;
+
+	head = &sa_mng->sa_rekeying_headlist;
+
+	list_for_each_entry_continue(child_sa, head, sa_rekeying_node) {
+		parent_sa = child_sa->parent_sa;
+		BUG_ON(!parent_sa);
+
+		/* Always acquire parent SA lock before child SA lock */
+		err = mutex_trylock(&parent_sa->lock);
+		if (err == 0)
+			continue;
+
+		/* Acquire child SA lock */
+		err = mutex_trylock(&child_sa->lock);
+		if (err == 0) {
+			mutex_unlock(&parent_sa->lock);
+			continue;
+		}
+
+		return child_sa;
+	}
+
+	return NULL;
+}
+
+void sa_rekeying_work_func(struct work_struct *work)
+{
+	struct dpa_ipsec_sa_mng *sa_mng;
+	struct dpa_ipsec_sa *child_sa, *parent_sa, *next_child_sa, *pos;
+	struct list_head *head;
+	int err;
+
+	sa_mng = container_of((struct delayed_work *)work,
+			      struct dpa_ipsec_sa_mng, sa_rekeying_work);
+
+	/* Acquire protective lock for the SA rekeying list */
+	mutex_lock(&sa_mng->sa_rekeying_headlist_lock);
+
+	head = &sa_mng->sa_rekeying_headlist;
+	pos = container_of(head, struct dpa_ipsec_sa, sa_rekeying_node);
+
+	child_sa = find_and_lock_sa_to_work_on(pos, sa_mng);
+
+	/* Release the list lock so other threads may use it */
+	mutex_unlock(&sa_mng->sa_rekeying_headlist_lock);
+
+	while (child_sa) {
+		parent_sa = child_sa->parent_sa;
+		BUG_ON(!parent_sa);
+
+		/* Process this child SA accordingly */
+		if (sa_is_outbound(child_sa))
+			err = sa_rekeying_outbound(child_sa);
+		else /* DPA_IPSEC_INBOUND */
+			err = sa_rekeying_inbound(child_sa);
+
+		/* Acquire protective lock for the SA rekeying list */
+		mutex_lock(&sa_mng->sa_rekeying_headlist_lock);
+
+		next_child_sa = find_and_lock_sa_to_work_on(child_sa, sa_mng);
+
+		/* Remove child SA from rekeying list if processing was OK */
+		if (err == 0 || err == -EUCLEAN || err == -EDQUOT) {
+			parent_sa->child_sa = NULL;
+			child_sa->parent_sa = NULL;
+			list_del(&child_sa->sa_rekeying_node);
+		}
+
+		/* Release the list lock so other threads may use it */
+		mutex_unlock(&sa_mng->sa_rekeying_headlist_lock);
+
+		if (err == 0 && child_sa->rekey_event_cb)
+			child_sa->rekey_event_cb(0, child_sa->id, err);
+
+		/*
+		 * Parent SA lock is always acquired before child SA lock so
+		 * unlocking them is done backwards
+		 */
+		mutex_unlock(&child_sa->lock);
+		mutex_unlock(&parent_sa->lock);
+
+		child_sa = next_child_sa;
+	}
+
+	/* Acquire protective lock for the sa rekeying list */
+	mutex_lock(&sa_mng->sa_rekeying_headlist_lock);
+
+	/* Reschedule work if there is at least one SA in rekeying process */
+	if (!list_empty(head)) {
+		struct timeval timeval;
+		unsigned long jiffies_to_wait;
+
+		timeval.tv_sec = 0;
+		timeval.tv_usec = REKEY_SCHED_DELAY;
+		jiffies_to_wait = timeval_to_jiffies(&timeval);
+
+		queue_delayed_work(sa_mng->sa_rekeying_wq,
+				   &sa_mng->sa_rekeying_work,
+				   jiffies_to_wait);
+	}
+
+	/* Release protective lock for the SA rekeying list */
+	mutex_unlock(&sa_mng->sa_rekeying_headlist_lock);
+
+	return;
+}
+
+/*
+ * Functional description:
+ *
+ * Removes the PCD entries that make traffic go to SEC for the SA given as input
+ *
+ * Returns:
+ *	- operation successful, returned code 0
+ *	- resource currently busy try again, returned code -EAGAIN
+ *	- remove inbound entry failed, returned code -ENOTRECOVERABLE
+ *	- remove outbound policies failed, at least one policy for this SA is
+ *	  still in the system, returned code -EBADSLT
+ */
+int dpa_ipsec_disable_sa(int sa_id)
+{
+	struct dpa_ipsec_sa *sa;
+	int ret = 0, err = 0;
+
+	sa = get_sa_from_sa_id(sa_id);
+	if (!sa) {
+		pr_err("Invalid SA handle for SA id %d\n", sa_id);
+		return -EINVAL;
+	}
+
+	/* Acquire protective lock for this SA */
+	ret = mutex_trylock(&sa->lock);
+	if (ret == 0)
+		return -EAGAIN;
+
+	if (!sa_is_single(sa)) {
+		pr_err("SA %d is a parent or child in rekeying\n", sa_id);
+		mutex_unlock(&sa->lock);
+		return -EINPROGRESS;
+	}
+
+	if (sa_is_inbound(sa) &&
+	    sa->inbound_hash_entry != DPA_OFFLD_INVALID_OBJECT_ID)
+		ret = remove_inbound_hash_entry(sa);
+	else { /* DPA_IPSEC_OUTBOUND */
+		err = sa_flush_policies(sa);
+		if (err < 0)
+			ret = -EBADSLT;
+	}
+
+	/* Release protective lock for this SA */
+	mutex_unlock(&sa->lock);
+
+	return ret;
+}
+EXPORT_SYMBOL(dpa_ipsec_disable_sa);
+
+/*
+ * Flush all SAs. If an error occurs while removing an SA, the flush process
+ * will continue with the next SAs and the return value will be -EAGAIN,
+ * which informs the upper layer that there is still at least one SA left
+ */
+int dpa_ipsec_flush_all_sa(int dpa_ipsec_id)
+{
+	struct dpa_ipsec *dpa_ipsec;
+	uint32_t i, sa_id;
+	int err = 0, ret;
+
+	/* multiple DPA IPSec instances are not currently supported */
+	unused(dpa_ipsec_id);
+
+	if (!gbl_dpa_ipsec) {
+		pr_err("There is no dpa_ipsec component initialized\n");
+		return -EPERM;
+	}
+	dpa_ipsec = gbl_dpa_ipsec;
+
+	flush_delayed_work(&dpa_ipsec->sa_mng.sa_rekeying_work);
+
+	for (i = 0; i < dpa_ipsec->sa_mng.max_num_sa; i++) {
+		mutex_lock(&dpa_ipsec->lock);
+		sa_id = dpa_ipsec->used_sa_ids[i];
+		mutex_unlock(&dpa_ipsec->lock);
+
+		if (sa_id != DPA_OFFLD_INVALID_OBJECT_ID) {
+			ret = dpa_ipsec_remove_sa(sa_id);
+			if (ret < 0)
+				err = -EAGAIN;
+		}
+	}
+
+	return err;
+}
+EXPORT_SYMBOL(dpa_ipsec_flush_all_sa);
+
+int dpa_ipsec_sa_get_policies(int sa_id,
+			      struct dpa_ipsec_policy_params *policy_params,
+			      int *num_pol)
+{
+	struct dpa_ipsec_sa *sa;
+	int ret;
+
+	if (sa_id < 0) {
+		pr_err("Invalid SA id");
+		return -EINVAL;
+	}
+
+	if (!num_pol) {
+		pr_err("Invalid num_pol parameter handle\n");
+		return -EINVAL;
+	}
+
+	sa = get_sa_from_sa_id(sa_id);
+	if (!sa) {
+		pr_err("Invalid SA handle\n");
+		return -EINVAL;
+	}
+
+	ret = mutex_trylock(&sa->lock);
+	if (ret == 0) {
+		pr_err("Failed to acquire lock for SA %d\n", sa->id);
+		return -EBUSY;
+	}
+
+	if (!policy_params) {
+		/* get the number of policies for SA with id sa_id */
+		*num_pol = get_policy_count_for_sa(sa);
+		mutex_unlock(&sa->lock);
+		return 0;
+	}
+
+	ret = copy_all_policies(sa, policy_params, *num_pol);
+
+	mutex_unlock(&sa->lock);
+
+	return ret;
+}
+EXPORT_SYMBOL(dpa_ipsec_sa_get_policies);
+
+/* Expects that SA structure is locked */
+static int sa_flush_policies(struct dpa_ipsec_sa *sa)
+{
+	struct dpa_ipsec_policy_entry *pol_entry, *tmp;
+	int err = 0, ret = 0;
+
+	BUG_ON(!sa);
+
+	list_for_each_entry_safe(pol_entry, tmp, &sa->policy_headlist, node) {
+		err = remove_policy(sa, pol_entry);
+		if (err < 0) {
+			pr_err("Failed remove policy entry SA %d\n", sa->id);
+			ret = -EAGAIN;
+			/*
+			 * continue with the other policies even if error
+			 * occured for this policy
+			 */
+		}
+	}
+
+	return ret;
+}
+
+int dpa_ipsec_sa_flush_policies(int sa_id)
+{
+	struct dpa_ipsec_sa *sa;
+	int ret = 0;
+
+	sa = get_sa_from_sa_id(sa_id);
+	if (!sa) {
+		pr_err("Invalid SA handle for SA id %d\n", sa_id);
+		return -EINVAL;
+	}
+
+	ret = mutex_trylock(&sa->lock);
+	if (ret == 0) {
+		pr_err("Failed to acquire lock for SA %d\n", sa->id);
+		return -EBUSY;
+	}
+
+	ret = sa_flush_policies(sa);
+
+	mutex_unlock(&sa->lock);
+
+	return ret;
+}
+EXPORT_SYMBOL(dpa_ipsec_sa_flush_policies);
+
+int dpa_ipsec_sa_get_stats(int sa_id, struct dpa_ipsec_sa_stats *sa_stats)
+{
+	struct dpa_ipsec_sa *sa;
+	int ret = 0;
+	uint32_t *desc;
+
+	if (!sa_stats) {
+		pr_err("Invalid SA statistics storage pointer\n");
+		return -EINVAL;
+	}
+
+	sa = get_sa_from_sa_id(sa_id);
+	if (!sa) {
+		pr_err("Invalid SA handle for SA id %d\n", sa_id);
+		return -EINVAL;
+	}
+
+	ret = mutex_trylock(&sa->lock);
+	if (ret == 0) {
+		pr_err("Failed to acquire lock for SA %d\n", sa->id);
+		return -EBUSY;
+	}
+
+	if (!sa->enable_stats) {
+		pr_err("Statistics are not enabled for SA id %d\n", sa_id);
+		mutex_unlock(&sa->lock);
+		return -EPERM;
+	}
+
+	desc = (uint32_t *)sa->sec_desc->desc;
+	if (!sa->sec_desc_extended) {
+		sa_stats->packets_count = *(desc + sa->stats_offset / 4);
+		sa_stats->bytes_count = *(desc + sa->stats_offset / 4 + 1);
+	} else {
+		sa_stats->bytes_count = *(desc + sa->stats_offset / 4);
+		sa_stats->packets_count = *(desc + sa->stats_offset / 4 + 1);
+	}
+
+	mutex_unlock(&sa->lock);
+
+	return 0;
+}
+EXPORT_SYMBOL(dpa_ipsec_sa_get_stats);
+
+int dpa_ipsec_sa_modify(int sa_id, struct dpa_ipsec_sa_modify_prm *modify_prm)
+{
+	struct dpa_ipsec_sa *sa;
+	dma_addr_t dma_rjobd;
+	uint32_t *rjobd;
+	struct qm_fd fd;
+	int ret;
+
+	if (!modify_prm) {
+		pr_err("Invalid modify SA parameter\n");
+		return -EINVAL;
+	}
+
+	sa = get_sa_from_sa_id(sa_id);
+	if (!sa) {
+		pr_err("Invalid SA id provided\n");
+		return -EINVAL;
+	}
+
+	ret = mutex_trylock(&sa->lock);
+	if (ret == 0) {
+		pr_err("SA %d is being used\n", sa->id);
+		return -EBUSY;
+	}
+
+	BUG_ON(!sa->dpa_ipsec);
+
+	switch (modify_prm->type) {
+	case DPA_IPSEC_SA_MODIFY_ARS:
+		ret = build_rjob_desc_ars_update(sa, modify_prm->arw);
+		if (ret < 0)
+			return ret;
+		break;
+	case DPA_IPSEC_SA_MODIFY_SEQ_NUM:
+		pr_err("Modifying SEQ number is unsupported\n");
+		return -EOPNOTSUPP;
+	case DPA_IPSEC_SA_MODIFY_EXT_SEQ_NUM:
+		pr_err("Modifying extended SEQ number is unsupported\n");
+		return -EOPNOTSUPP;
+	case DPA_IPSEC_SA_MODIFY_CRYPTO:
+		pr_err("Modifying cryptographic parameters is unsupported\n");
+		return -EOPNOTSUPP;
+	default:
+		pr_err("Invalid type for modify parameters\n");
+		mutex_unlock(&sa->lock);
+		return -EINVAL;
+	}
+
+	rjobd = sa->rjob_desc;
+	dma_rjobd = dma_map_single(sa->dpa_ipsec->jrdev, rjobd,
+				   desc_len(rjobd) * CAAM_CMD_SZ,
+				   DMA_BIDIRECTIONAL);
+	if (!dma_rjobd) {
+		pr_err("Failed DMA mapping the RJD for SA %d\n", sa->id);
+		return -ENXIO;
+	}
+
+	memset(&fd, 0x00, sizeof(struct qm_fd));
+	/* fill frame descriptor parameters */
+	fd.format = qm_fd_contig;
+	qm_fd_addr_set64(&fd, dma_rjobd);
+	fd.length20 = desc_len(rjobd) * sizeof(uint32_t);
+	fd.offset = 0;
+	fd.bpid = 0;
+	fd.cmd = FD_CMD_REPLACE_JOB_DESC;
+	ret = qman_enqueue(sa->to_sec_fq, &fd, 0);
+	if (ret != 0) {
+		pr_err("Could not enqueue frame with RJAD for SA %d\n", sa->id);
+		ret = -ETXTBSY;
+	}
+
+	dma_unmap_single(sa->dpa_ipsec->jrdev, dma_rjobd,
+			 desc_len(rjobd) * CAAM_CMD_SZ, DMA_BIDIRECTIONAL);
+
+	mutex_unlock(&sa->lock);
+
+	return ret;
+}
+EXPORT_SYMBOL(dpa_ipsec_sa_modify);
diff --git a/drivers/staging/fsl_dpa_offload/dpa_ipsec.h b/drivers/staging/fsl_dpa_offload/dpa_ipsec.h
new file mode 100644
index 0000000..f55871b
--- /dev/null
+++ b/drivers/staging/fsl_dpa_offload/dpa_ipsec.h
@@ -0,0 +1,459 @@
+/* Copyright 2008-2013 Freescale Semiconductor, Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef __DPA_IPSEC_H__
+#define __DPA_IPSEC_H__
+
+#include <linux/fsl_qman.h>
+#include <linux/fsl_bman.h>
+#include <linux/fsl_dpa_ipsec.h>
+
+/* From Linux for Shared Descriptor auxiliary structures */
+#include <linux/bitops.h>
+#include <linux/compiler.h>
+#include <linux/workqueue.h>
+
+/*For IP header structure definition */
+#include <linux/ip.h>
+#include <linux/ipv6.h>
+
+/*For UDP header structure definition */
+#include <linux/udp.h>
+
+#include "desc.h"
+
+#include "fm_pcd_ext.h"
+#include "cq.h"
+
+#define OP_PCL_IPSEC_INVALID_ALG_ID	0xFFFF
+
+#define IPSEC_ALGS_ENTRY(enc, auth)	{		\
+		.enc_alg = OP_PCL_IPSEC_ ## enc,	\
+		.auth_alg = OP_PCL_IPSEC_ ## auth	\
+	}
+
+#define IPSEC_ALGS	{					\
+	/* DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_96_MD5_128 */	\
+	IPSEC_ALGS_ENTRY(3DES, HMAC_MD5_96),			\
+	/* DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_96_SHA_160 */	\
+	IPSEC_ALGS_ENTRY(3DES, HMAC_SHA1_96),			\
+	/* DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_MD5_128 */	\
+	IPSEC_ALGS_ENTRY(3DES, HMAC_MD5_128),			\
+	/* DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_SHA_160 */	\
+	IPSEC_ALGS_ENTRY(3DES, HMAC_SHA1_160),			\
+	/* DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_SHA_256_128 */	\
+	IPSEC_ALGS_ENTRY(3DES, HMAC_SHA2_256_128),		\
+	/* DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_SHA_384_192 */	\
+	IPSEC_ALGS_ENTRY(3DES, HMAC_SHA2_384_192),		\
+	/* DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_SHA_512_256 */	\
+	IPSEC_ALGS_ENTRY(3DES, HMAC_SHA2_512_256),		\
+	/* DPA_IPSEC_CIPHER_ALG_NULL_ENC_HMAC_96_MD5_128 */	\
+	IPSEC_ALGS_ENTRY(INVALID_ALG_ID, HMAC_MD5_96),		\
+	/* DPA_IPSEC_CIPHER_ALG_NULL_ENC_HMAC_96_SHA_160 */	\
+	IPSEC_ALGS_ENTRY(INVALID_ALG_ID, HMAC_SHA1_96),		\
+	/* DPA_IPSEC_CIPHER_ALG_NULL_ENC_AES_XCBC_MAC_96 */	\
+	IPSEC_ALGS_ENTRY(INVALID_ALG_ID, AES_XCBC_MAC_96),	\
+	/* DPA_IPSEC_CIPHER_ALG_NULL_ENC_HMAC_MD5_128 */	\
+	IPSEC_ALGS_ENTRY(INVALID_ALG_ID, HMAC_MD5_128),		\
+	/* DPA_IPSEC_CIPHER_ALG_NULL_ENC_HMAC_SHA_160 */	\
+	IPSEC_ALGS_ENTRY(INVALID_ALG_ID, HMAC_SHA1_160),	\
+	/* DPA_IPSEC_CIPHER_ALG_NULL_ENC_HMAC_SHA_256_128 */	\
+	IPSEC_ALGS_ENTRY(INVALID_ALG_ID, HMAC_SHA2_256_128),	\
+	/* DPA_IPSEC_CIPHER_ALG_NULL_ENC_HMAC_SHA_384_192 */	\
+	IPSEC_ALGS_ENTRY(INVALID_ALG_ID, HMAC_SHA2_384_192),	\
+	/* DPA_IPSEC_CIPHER_ALG_NULL_ENC_HMAC_SHA_512_256 */	\
+	IPSEC_ALGS_ENTRY(INVALID_ALG_ID, HMAC_SHA2_512_256),	\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_96_MD5_128 */	\
+	IPSEC_ALGS_ENTRY(AES_CBC, HMAC_MD5_96),			\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_96_SHA_160 */	\
+	IPSEC_ALGS_ENTRY(AES_CBC, HMAC_SHA1_96),		\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CBC_AES_XCBC_MAC_96 */	\
+	IPSEC_ALGS_ENTRY(AES_CBC, AES_XCBC_MAC_96),		\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_MD5_128 */		\
+	IPSEC_ALGS_ENTRY(AES_CBC, HMAC_MD5_128),		\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_SHA_160 */		\
+	IPSEC_ALGS_ENTRY(AES_CBC, HMAC_SHA1_160),		\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_SHA_256_128 */	\
+	IPSEC_ALGS_ENTRY(AES_CBC, HMAC_SHA2_256_128),		\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_SHA_384_192 */	\
+	IPSEC_ALGS_ENTRY(AES_CBC, HMAC_SHA2_384_192),		\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_SHA_512_256 */	\
+	IPSEC_ALGS_ENTRY(AES_CBC, HMAC_SHA2_512_256),		\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_96_MD5_128 */	\
+	IPSEC_ALGS_ENTRY(AES_CTR, HMAC_MD5_96),			\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_96_SHA_160 */	\
+	IPSEC_ALGS_ENTRY(AES_CTR, HMAC_SHA1_96),		\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CTR_AES_XCBC_MAC_96 */	\
+	IPSEC_ALGS_ENTRY(AES_CTR, AES_XCBC_MAC_96),		\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_MD5_128 */		\
+	IPSEC_ALGS_ENTRY(AES_CTR, HMAC_MD5_128),		\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_SHA_160 */		\
+	IPSEC_ALGS_ENTRY(AES_CTR, HMAC_SHA1_160),		\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_SHA_256_128 */	\
+	IPSEC_ALGS_ENTRY(AES_CTR, HMAC_SHA2_256_128),		\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_SHA_384_192 */	\
+	IPSEC_ALGS_ENTRY(AES_CTR, HMAC_SHA2_384_192),		\
+	/* DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_SHA_512_256 */	\
+	IPSEC_ALGS_ENTRY(AES_CTR, HMAC_SHA2_512_256)		\
+}
+
+#define GET_POL_TABLE_IDX(_proto, _ip_ver)				\
+	((_proto == IPPROTO_TCP)  ? DPA_IPSEC_PROTO_TCP_##_ip_ver :	\
+	 (_proto == IPPROTO_UDP)  ? DPA_IPSEC_PROTO_UDP_##_ip_ver :	\
+	((_proto == IPPROTO_ICMP) ||					\
+	 (_proto == IPPROTO_ICMPV6)) ? DPA_IPSEC_PROTO_ICMP_##_ip_ver :	\
+	 (_proto == IPPROTO_SCTP) ? DPA_IPSEC_PROTO_SCTP_##_ip_ver :	\
+	  DPA_IPSEC_PROTO_ANY_##_ip_ver)
+
+#define DPA_IPSEC_ADDR_T_IPv4	4
+#define DPA_IPSEC_ADDR_T_IPv6	6
+
+#define IP_ADDR_TYPE_IPV4(_ipAddr) (_ipAddr.version == DPA_IPSEC_ADDR_T_IPv4)
+#define IP_ADDR(_ipAddr) \
+	(IP_ADDR_TYPE_IPV4(_ipAddr) ? \
+	(_ipAddr.addr.ipv4.byte) : (_ipAddr.addr.ipv6.byte))
+#define IP_ADDR_LEN(_ipAddr) \
+	(IP_ADDR_TYPE_IPV4(_ipAddr) ? \
+	(DPA_OFFLD_IPv4_ADDR_LEN_BYTES) : (DPA_OFFLD_IPv6_ADDR_LEN_BYTES))
+
+#define SET_BYTE_VAL_IN_ARRAY(_key, _off, _val) (_key[_off] = _val)
+#define SET_IP_PROTO_MASK(_mask, _off, _is_masked) \
+		(_mask[_off] = _is_masked ? 0x00 : 0xFF)
+#define SET_L4_PORT_MASK(_mask, _off, _val) \
+		(*(uint16_t *) &(_mask[_off]) = _val)
+
+#define TABLE_KEY_SIZE(_tbl_params) \
+	((_tbl_params.type == DPA_CLS_TBL_HASH) ? \
+		tbl_params.hash_params.key_size : \
+		(_tbl_params.type == DPA_CLS_TBL_EXACT_MATCH) ?	\
+			 tbl_params.exact_match_params.key_size : 0)
+
+#define GET_SA_TABLE_IDX(_dest_addr, _use_udp_encap) \
+	(!IP_ADDR_TYPE_IPV4(_dest_addr) ? DPA_IPSEC_SA_IPV6 :  \
+	 _use_udp_encap ? DPA_IPSEC_SA_IPV4_NATT :  DPA_IPSEC_SA_IPV4)
+
+#define SEQ_NUM_HI_MASK		0xFFFFFFFF00000000
+#define SEQ_NUM_LOW_MASK	0x00000000FFFFFFFF
+
+#define MAX_NUM_OF_SA       2000
+#define MAX_CIPHER_KEY_LEN  100
+#define MAX_AUTH_KEY_LEN    256
+#define MAX_BUFFER_POOL_ID  63
+
+/* number of FQs that will be created internally for each SA */
+#define NUM_FQS_PER_SA		2
+
+#define UDP_HEADER_LEN		8
+#define NEXT_HEADER_IS_IPv4	0x04
+
+#define WAIT4_FQ_EMPTY_TIMEOUT	10000 /* Time in microseconds */
+#define REKEY_SCHED_DELAY	100   /* Time in microseconds */
+
+#define INVALID_INB_FLOW_ID	0xFFFF
+
+/* The maximum length (in bytes) for the CAAM extra commands */
+#define MAX_EXTRA_DESC_COMMANDS		(64 * sizeof(uint32_t))
+
+#define NIA_OPCODE_MASK		0x0F
+
+#define SEC_DEF_VER 40 /* like in P4080 */
+#define SEC_VER_5_3 53
+
+/* DPA IPSec Encryption & authentication algorithm identifiers */
+struct ipsec_alg_suite {
+	uint16_t	enc_alg;
+	uint16_t	auth_alg;
+};
+
+/* DPA IPsec PCD management operation types */
+enum mng_op_type {
+	MNG_OP_ADD = 0,
+	MNG_OP_REMOVE,
+	MNG_OP_MODIFY
+};
+
+/* DPA IPsec Cipher Parameters */
+struct cipher_params {
+	uint16_t cipher_type;	 /* Algorithm type as defined by SEC driver   */
+	uint8_t *cipher_key;	 /* Address to the encryption key	      */
+	uint32_t cipher_key_len; /* Length in bytes of the normal key         */
+};
+
+/* DPA IPsec Authentication Parameters */
+struct auth_params {
+	uint16_t auth_type;	/* Algorithm type as defined by SEC driver    */
+	uint8_t *auth_key;	/* Address to the normal key		      */
+	uint32_t auth_key_len;	/* Length in bytes of the normal key          */
+	uint8_t *split_key;	/* Address to the generated split key         */
+	uint32_t split_key_len;	/* Length in bytes of the split key           */
+	uint32_t split_key_pad_len;/* Length in bytes of the padded split key */
+};
+
+/*
+ * DPA IPsec Security Association
+ * This structure will represent a SA. All SA structures will be allocated
+ * in the initialization part for performance reasons.
+ */
+struct dpa_ipsec_sa {
+	struct dpa_ipsec *dpa_ipsec;	    /* Pointer to DPA_IPSEC           */
+	enum dpa_ipsec_direction sa_dir;    /* SA direction		      */
+	uint32_t id;			    /* Used to index in circular queue*/
+	enum dpa_ipsec_cipher_alg alg_suite; /* DPA IPSEC algorithm suite     */
+	struct cipher_params cipher_data;   /* Encryption parameters	      */
+	struct auth_params auth_data;	    /* Authentication key parameters  */
+	struct sec_descriptor *sec_desc_unaligned; /* Allocated at init time.
+					  * When releasing memory only free
+					  * this pointer and do not act on
+					  * sec_desc address		      */
+	struct sec_descriptor *sec_desc; /* 64 byte aligned address where is
+					  * computed the SEC 4.x descriptor
+					  * according to the SA information.
+					  * do not free this pointer!	      */
+	uint32_t *sec_desc_extra_cmds_unaligned;
+	uint32_t *sec_desc_extra_cmds; /* aligned to CORE cache line size     */
+	bool	 sec_desc_extended; /* true if SEC descriptor is extended     */
+	uint32_t *rjob_desc_unaligned;
+	uint32_t *rjob_desc; /* replacement job descriptor address	      */
+	uint32_t stats_offset; /* Offset of the statistics (in bytes)	      */
+	uint32_t stats_indx; /* Index of the lifetime counter in descriptor   */
+	uint32_t next_cmd_indx; /* Next command index after SHD header	      */
+	uint8_t  job_desc_len; /* Number of words CAAM Job Descriptor occupies
+				* form the CAAM Descriptor length
+				* MAX_CAAM_DESCSIZE			      */
+	bool enable_stats; /* Enable counting packets and bytes processed     */
+	bool dscp_copy; /* Enable DSCP propagation support		      */
+	bool ecn_copy; /* Enable DSCP propagation support		      */
+	bool enable_dpovrd; /* Enable DECO Protocol Override Register	      */
+	struct qman_fq *to_sec_fq; /*From this Frame Queue SEC consumes frames*/
+	struct qman_fq *from_sec_fq; /*In this Frame Queue SEC will enqueue the
+				encryption/decryption result (FD).            */
+	uint16_t sa_wqid; /* Work queue id in which the TO SEC FQ will be put */
+	uint8_t sa_bpid;  /* Buffer pool id used by SEC for acquiring buffers,
+			     comes from user. Default buffer pool 63	      */
+	uint32_t spi;	/* IPsec Security parameter index		      */
+	struct dpa_offload_ip_address src_addr;  /* Source IP address	      */
+	struct dpa_offload_ip_address dest_addr; /* Destination IP address    */
+	uint16_t outbound_flowid; /* Value used to classify frames encrypted
+				 with this SA				      */
+	bool use_udp_encap;   /* NAT-T is activated for this SA.	      */
+	uint16_t udp_src_port;	/* Source UDP port (for UDP encapsulated ESP)
+				   Only for inbound  SAs.		      */
+	uint16_t udp_dest_port;	/* Destination UDP port (for UDP encap ESP)
+				   Only for inbound  SAs.                     */
+	uint16_t inbound_flowid; /* Value used for identifying an inbound SA. */
+	bool valid_flowid_entry; /* Valid entry in the flowID table	      */
+	int inbound_hash_entry;	/* Entry in the hash table
+				   corresponding to SPI extended key	      */
+	int inbound_sa_td; /* Descriptor for the SA lookup table in which this
+			    * SA's key will be placed */
+	struct dpa_cls_tbl_action def_sa_action;
+	struct list_head policy_headlist; /* Head of the policy param list
+			 used to store all the in/out policy parameters in order
+			 to know how to remove the corresponding PCD entries  */
+	struct dpa_cls_tbl_action policy_miss_action; /* Action for frames that
+						       * fail inbound policy
+						       * verification	      */
+	int em_inpol_td; /* Exact match table descriptor for inbound policy
+			    check					      */
+	struct dpa_ipsec_sa *parent_sa;	/* Address of the parent SA or NULL   */
+	struct dpa_ipsec_sa *child_sa;	/* Address of the child SA or NULL    */
+	struct list_head sa_rekeying_node; /* For linking in SA rekeying list */
+	int used_sa_index; /* Index in the used_sa_ids vector of the dpa ipsec
+			      instance this SA is part of.		      */
+	bool use_var_iphdr_len; /* Enable variable IP header length support   */
+	int ipsec_hmd;		/* Manip object for special IPSec functions   */
+	dpa_ipsec_rekey_event_cb rekey_event_cb;
+	uint32_t l2_hdr_size; /* Size of the Ethernet header, including any
+			      * VLAN information.			      */
+	struct mutex lock; /* Lock for this SA structure */
+};
+
+/*
+ * Parameters for inbound policy verification tables
+ * Global list lock - inpol_tables_lock from SA manager
+ */
+struct inpol_tbl {
+	void *cc_node; /* Cc node handle on top of which the table is created */
+	int td;	 /* Exact match table used for inbound policy verification    */
+	bool used;
+	struct list_head table_list;
+};
+
+/*
+ * Parameters for IPsec special manipulations eg. DSCP/ECN update
+ * Global list lock - ipsec_manip_node_lock from SA manager
+ */
+struct ipsec_manip_node {
+	/*
+	 * IPsec manip node handle; returned by FM_PCD_ManipNodeSet
+	 */
+	void *hm;
+	bool used;
+	struct list_head ipsec_manip_node_list;
+};
+
+/* DPA IPSEC - Security Associations Management */
+struct dpa_ipsec_sa_mng {
+	struct dpa_ipsec_sa *sa; /* Array of SAs. Use indexes from sa_id_cq   */
+	struct cq *sa_id_cq;	/* Circular Queue with id's for SAs           */
+	uint32_t max_num_sa;	/* Maximum number of SAs                      */
+
+	/* Circular queue with flow IDs for identifying an inbound SA */
+	struct cq *inbound_flowid_cq;
+
+	/* Inbound policy verification tables key size.*/
+	uint8_t inpol_key_size;
+
+	/*
+	 * Head list of tables used for inbound
+	 * policy verification. List of inpol_tbl structures.
+	 * Populated only if inbound policy verification is enabled
+	 */
+	struct list_head inpol_tables;
+	struct mutex inpol_tables_lock; /* List lock inbound policy table */
+
+	/* ipsec_manip_node head list */
+	struct list_head ipsec_manip_node_list;
+
+	/* Lock for IPsec manip node list */
+	struct mutex ipsec_manip_node_lock;
+	struct delayed_work sa_rekeying_work;
+
+	/* Work queue used to defer the work to be done during rekeying */
+	struct workqueue_struct *sa_rekeying_wq;
+
+	/* Head list with inbound SA's currently in the rekeying process */
+	struct list_head sa_rekeying_headlist;
+	struct mutex sa_rekeying_headlist_lock; /* Lock for the rekeying list */
+	struct cq *fqid_cq; /* Circular queue with FQIDs for internal FQs     */
+};
+
+/* DPA IPSEC - Control Block */
+struct dpa_ipsec {
+	/* Configuration parameters as provided in dap_ipsec_config_and_init */
+	struct dpa_ipsec_params config;
+	struct dpa_ipsec_sa_mng sa_mng;	/* Internal DPA IPsec SA manager      */
+	int *used_sa_ids;	/* Sa ids used by this dpa ipsec instance     */
+	int num_used_sas;  /* The current number of sa's used by this instance*/
+	int sec_era; /* SEC ERA information */
+	int sec_ver; /* SEC version information */
+	struct device *jrdev; /* Job ring device */
+	struct mutex lock; /* Lock for this dpa_ipsec instance */
+};
+
+struct hmd_entry {
+	int hmd;
+	bool hmd_special_op;
+};
+
+/* DPA IPSEC - Security Policy Parameter Entry */
+struct dpa_ipsec_policy_entry {
+	struct dpa_ipsec_policy_params pol_params; /* Policy parameters       */
+	int entry_id;		/* Set by dpa_classif_table_insert_entry      */
+
+	/*
+	 * Header manip for IPSec special operation or
+	 * if none Header manip for fragmentation or
+	 * manipulation
+	 */
+	int hmd;
+
+	/*
+	 * true is hmd is IPSec special operation, false
+	 * is hmd refers to an outside manip object
+	 */
+	bool hmd_special_op;
+	struct list_head node;	/* Node in linked list			      */
+};
+
+void sa_rekeying_work_func(struct work_struct *work);
+
+static inline int sa_currently_on_rekeying_list(struct dpa_ipsec_sa *sa)
+{
+	return (sa->sa_rekeying_node.next == LIST_POISON1 &&
+		sa->sa_rekeying_node.prev == LIST_POISON2) ? FALSE : TRUE;
+}
+
+static inline int sa_currently_in_rekeying(struct dpa_ipsec_sa *sa)
+{
+	return (sa->parent_sa || sa->child_sa) ? TRUE : FALSE;
+}
+
+static inline int sa_is_parent(struct dpa_ipsec_sa *sa)
+{
+	return sa->child_sa ? TRUE : FALSE;
+}
+
+static inline int sa_is_child(struct dpa_ipsec_sa *sa)
+{
+	return sa->parent_sa ? TRUE : FALSE;
+}
+
+static inline int sa_is_single(struct dpa_ipsec_sa *sa)
+{
+	return (!sa_is_parent(sa) && !sa_is_child(sa)) ? TRUE : FALSE;
+}
+
+static inline int sa_is_outbound(struct dpa_ipsec_sa *sa)
+{
+	return sa->sa_dir == DPA_IPSEC_OUTBOUND ? TRUE : FALSE;
+}
+
+static inline int sa_is_inbound(struct dpa_ipsec_sa *sa)
+{
+	return sa->sa_dir == DPA_IPSEC_INBOUND ? TRUE : FALSE;
+}
+
+static inline int schedule_sa(struct dpa_ipsec_sa *sa)
+{
+	enum qman_fq_state state;
+	u32 flags;
+	int err;
+
+	qman_fq_state(sa->to_sec_fq, &state, &flags);
+	if (state == qman_fq_state_parked) {
+		err = qman_schedule_fq(sa->to_sec_fq);
+		if (unlikely(err < 0))
+			return -EIO;
+		return 0;
+	}
+
+	return state == qman_fq_state_sched ? 0 : -EPERM;
+}
+
+static inline void rekey_err_report(dpa_ipsec_rekey_event_cb rekey_event_cb,
+				int dpa_ipsec_id, uint32_t sa_id, int err)
+{
+	if (rekey_event_cb)
+		rekey_event_cb(dpa_ipsec_id, sa_id, err);
+}
+
+#endif	/* __DPA_IPSEC_H__ */
diff --git a/drivers/staging/fsl_dpa_offload/dpa_ipsec_desc.c b/drivers/staging/fsl_dpa_offload/dpa_ipsec_desc.c
new file mode 100644
index 0000000..0fbc547
--- /dev/null
+++ b/drivers/staging/fsl_dpa_offload/dpa_ipsec_desc.c
@@ -0,0 +1,1821 @@
+/* Copyright 2008-2012 Freescale Semiconductor, Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/version.h>
+#include <linux/platform_device.h>
+
+#include "compat.h"
+#include "desc.h"
+#include "error.h"
+#include "jr.h"
+#include "ctrl.h"
+
+#include "dpa_ipsec_desc.h"
+
+static struct device *get_jrdev(struct dpa_ipsec *dpa_ipsec);
+
+/* If SEC ERA is unknown default to this value */
+#define SEC_DEF_ERA	2 /* like in P4080 */
+
+/*
+ * to retrieve a 256 byte aligned buffer address from an address
+ * we need to copy only the first 7 bytes
+ */
+#define ALIGNED_PTR_ADDRESS_SZ	(CAAM_PTR_SZ - 1)
+
+#define JOB_DESC_HDR_LEN	CAAM_CMD_SZ
+#define SEQ_OUT_PTR_SGF_MASK	0x01000000;
+/* relative offset where the input pointer should be updated in the descriptor*/
+#define IN_PTR_REL_OFF		4 /* words from current location */
+/* dummy pointer value */
+#define DUMMY_PTR_VAL		0x00000000
+#define PTR_LEN			2	/* Descriptor is created only for 8 byte
+					 * pointer. PTR_LEN is in words. */
+
+static const struct of_device_id sec_jr_match[] = {
+	{
+	 .compatible = "fsl,sec-v4.0-job-ring"
+	}
+};
+
+/* retrieve and store SEC information */
+int get_sec_info(struct dpa_ipsec *dpa_ipsec)
+{
+	struct device_node *sec_node;
+	struct platform_device *sec_of_dev;
+	int sec_era = -EINVAL;
+
+	sec_node = of_find_compatible_node(NULL, NULL, "fsl,sec-v5.3");
+	if (sec_node)
+		dpa_ipsec->sec_ver = SEC_VER_5_3;
+	else {
+		dpa_ipsec->sec_ver = SEC_DEF_VER;
+
+		sec_node = of_find_compatible_node(NULL, NULL, "fsl,sec-v4.0");
+		if (sec_node == NULL) {
+			pr_err("Can't find device_node for SEC! Check device tree!\n");
+			goto def_sec_era;
+		}
+	}
+
+	sec_of_dev = of_find_device_by_node(sec_node);
+	if (sec_of_dev == NULL) {
+		pr_err(KERN_ERR "SEC platform_device null!\n");
+		goto def_sec_era;
+	}
+
+	sec_era = caam_get_era(sec_of_dev);
+
+def_sec_era:
+	if (sec_era < 0)
+		/* Unknown ERA - use default */
+		sec_era = SEC_DEF_ERA;
+
+	dpa_ipsec->sec_era = sec_era;
+	dpa_ipsec->jrdev = get_jrdev(dpa_ipsec);
+	if (!dpa_ipsec->jrdev)
+		return -EINVAL;
+
+	return 0;
+}
+
+
+static struct device *get_jrdev(struct dpa_ipsec *dpa_ipsec)
+{
+	struct device_node *sec_jr_node;
+	struct platform_device *sec_of_jr_dev;
+	struct device *sec_jr_dev;
+
+	if (dpa_ipsec->jrdev)
+		return dpa_ipsec->jrdev;
+
+	sec_jr_node = of_find_matching_node(NULL, &sec_jr_match[0]);
+	if (sec_jr_node == NULL) {
+		pr_err("Couln't find the device_node SEC job-ring, check the device tree\n");
+		return NULL;
+	}
+
+	sec_jr_node = of_find_matching_node(sec_jr_node, &sec_jr_match[0]);
+	if (sec_jr_node == NULL) {
+		pr_err("Couln't find the device_node SEC job-ring, check the device tree\n");
+		return NULL;
+	}
+
+	sec_of_jr_dev = of_find_device_by_node(sec_jr_node);
+	if (sec_of_jr_dev == NULL) {
+		pr_err(KERN_ERR "SEC job-ring of_device null\n");
+		return NULL;
+	}
+
+	sec_jr_dev = &sec_of_jr_dev->dev;
+
+	return sec_jr_dev;
+}
+
+static inline u32 get_ipsec_op_type(enum dpa_ipsec_direction sa_dir)
+{
+	return sa_dir == DPA_IPSEC_INBOUND ?  OP_TYPE_DECAP_PROTOCOL :
+					      OP_TYPE_ENCAP_PROTOCOL;
+}
+
+static inline int get_cipher_params(enum dpa_ipsec_cipher_alg cipher_alg,
+				    uint32_t *iv_length, uint32_t *icv_length,
+				    uint32_t *max_pad_length)
+{
+	switch (cipher_alg) {
+	case DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_96_MD5_128:
+	case DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_96_SHA_160:
+		*iv_length = 8;
+		*max_pad_length = 8;
+		*icv_length = 12;
+		break;
+	case DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_MD5_128:
+		*iv_length = 8;
+		*max_pad_length = 8;
+		*icv_length = 16;
+		break;
+	case DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_SHA_160:
+	case DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_SHA_256_128:
+		*iv_length = 8;
+		*max_pad_length = 8;
+		*icv_length = 20;
+		break;
+	case DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_SHA_384_192:
+		*iv_length = 8;
+		*max_pad_length = 8;
+		*icv_length = 24;
+		break;
+	case DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_SHA_512_256:
+		*iv_length = 8;
+		*max_pad_length = 8;
+		*icv_length = 32;
+		break;
+	case DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_96_MD5_128:
+	case DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_96_SHA_160:
+	case DPA_IPSEC_CIPHER_ALG_AES_CBC_AES_XCBC_MAC_96:
+		*iv_length = 16;
+		*max_pad_length = 16;
+		*icv_length = 12;
+		break;
+	case DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_MD5_128:
+	case DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_SHA_256_128:
+		*iv_length = 16;
+		*max_pad_length = 16;
+		*icv_length = 16;
+		break;
+	case DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_SHA_160:
+		*iv_length = 16;
+		*max_pad_length = 16;
+		*icv_length = 20;
+		break;
+	case DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_SHA_384_192:
+		*iv_length = 16;
+		*max_pad_length = 16;
+		*icv_length = 24;
+		break;
+	case DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_SHA_512_256:
+		*iv_length = 16;
+		*max_pad_length = 16;
+		*icv_length = 32;
+		break;
+	case DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_96_MD5_128:
+	case DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_96_SHA_160:
+	case DPA_IPSEC_CIPHER_ALG_AES_CTR_AES_XCBC_MAC_96:
+		*iv_length = 16;
+		*max_pad_length = 16;
+		*icv_length = 12;
+		break;
+	case DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_MD5_128:
+	case DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_SHA_256_128:
+		*iv_length = 8;
+		*max_pad_length = 4;
+		*icv_length = 16;
+		break;
+	case DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_SHA_160:
+		*iv_length = 8;
+		*max_pad_length = 4;
+		*icv_length = 20;
+		break;
+	case DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_SHA_384_192:
+		*iv_length = 8;
+		*max_pad_length = 4;
+		*icv_length = 24;
+		break;
+	case DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_SHA_512_256:
+		*iv_length = 8;
+		*max_pad_length = 4;
+		*icv_length = 32;
+		break;
+	default:
+		*iv_length = 0;
+		*icv_length = 0;
+		*max_pad_length = 0;
+		pr_err("Unsupported cipher suite %d\n", cipher_alg);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static inline void build_stats_descriptor_part(struct dpa_ipsec_sa *sa,
+					       size_t pdb_len)
+{
+	u32 *desc, *padding_jump;
+	u32 block_size, stats_offset, offset;
+
+	BUG_ON(!sa);
+
+	desc = (u32 *) sa->sec_desc->desc;
+
+	stats_offset = sizeof(sa->sec_desc->hdr_word) + pdb_len -
+		       DPA_IPSEC_STATS_LEN * sizeof(u32);
+	sa->stats_offset = stats_offset;
+	memset((u8 *)desc + stats_offset, 0, DPA_IPSEC_STATS_LEN * sizeof(u32));
+
+	/* Copy from descriptor to MATH REG 0 the current statistics */
+	append_move(desc, MOVE_SRC_DESCBUF | MOVE_DEST_MATH0 | MOVE_WAITCOMP |
+		    (stats_offset << MOVE_OFFSET_SHIFT) | sizeof(u64));
+
+	/* Load 1 in MATH REG 1 */
+	append_math_add(desc, REG1, ZERO, ONE, MATH_LEN_8BYTE);
+
+	/*
+	 * Perform 32-bit left shift of DEST and concatenate with left 32 bits
+	 * of SRC1 i.e MATH REG 1 = 0x00000001_00000000
+	 */
+	append_math_ldshift(desc, REG1, REG0, REG1, MATH_LEN_8BYTE);
+
+	if (sa->sa_dir == DPA_IPSEC_INBOUND) {
+		/* MATH REG 2 = Sequence in length */
+		append_math_add_imm_u32(desc, REG2, SEQINLEN, IMM, 0);
+		goto after_padding;
+	} else {
+		/* MATH REG 2 = Sequence in length + 2 */
+		append_math_add_imm_u32(desc, REG2, SEQINLEN, IMM, 2);
+	}
+
+	switch (sa->cipher_data.cipher_type) {
+	case OP_PCL_IPSEC_3DES:
+		block_size = 8; /* block size in bytes */
+		break;
+	case OP_PCL_IPSEC_AES_CBC:
+	case OP_PCL_IPSEC_AES_CTR:
+	case OP_PCL_IPSEC_AES_XTS:
+	case OP_PCL_IPSEC_AES_CCM8:
+	case OP_PCL_IPSEC_AES_CCM12:
+	case OP_PCL_IPSEC_AES_CCM16:
+	case OP_PCL_IPSEC_AES_GCM8:
+	case OP_PCL_IPSEC_AES_GCM12:
+	case OP_PCL_IPSEC_AES_GCM16:
+		block_size = 16; /* block size in bytes */
+		break;
+	default:
+		pr_crit("Invalid cipher algorithm for SA %d\n", sa->id);
+		return;
+	}
+
+	/* Adding padding to byte counter */
+	append_math_and_imm_u32(desc, REG3, REG2, IMM, block_size - 1);
+
+	/* Previous operation result is 0 i.e padding added to bytes count */
+	padding_jump = append_jump(desc, CLASS_BOTH | JUMP_TEST_ALL |
+				   JUMP_COND_MATH_Z);
+
+	/* MATH REG 2 = MATH REG 2 + 1 */
+	append_math_add(desc, REG2, REG2, ONE, MATH_LEN_8BYTE);
+
+	/* jump back to adding padding i.e jump back 4 words */
+	offset = (-4) & 0x000000FF;
+	append_jump(desc, (offset << JUMP_OFFSET_SHIFT));
+
+	set_jump_tgt_here(desc, padding_jump);
+	/* Done adding padding to byte counter */
+
+after_padding:
+	/* MATH REG 1  = MATH REG 1 + MATH REG 2 */
+	append_math_add(desc, REG1, REG1, REG2, MATH_LEN_8BYTE);
+
+	/* MATH REG0 = MATH REG 0 + MATH REG1 */
+	append_math_add(desc, REG0, REG0, REG1, MATH_LEN_8BYTE);
+
+	/* Store in the descriptor but not in external memory */
+	append_move(desc, MOVE_SRC_MATH0 | MOVE_DEST_DESCBUF | MOVE_WAITCOMP |
+		    (stats_offset << MOVE_OFFSET_SHIFT) | sizeof(u64));
+}
+
+static inline void save_stats_in_external_mem(struct dpa_ipsec_sa *sa)
+{
+	u32 *desc;
+	u32 stats_offset;
+
+	desc = (u32 *) sa->sec_desc->desc;
+
+	/* statistics offset = predetermined offset */
+	stats_offset = sa->stats_offset;
+
+	/* Store command: in the case of the Descriptor Buffer the length
+	 * is specified in 4-byte words, but in all other cases the length
+	 * is specified in bytes. Offset in 4 byte words */
+	append_store(desc, 0, DPA_IPSEC_STATS_LEN, LDST_CLASS_DECO |
+		     ((stats_offset / 4) << LDST_OFFSET_SHIFT) |
+		     LDST_SRCDST_WORD_DESCBUF_SHARED);
+
+	/* Jump with CALM to be sure previous operation was finished */
+	append_jump(desc, JUMP_COND_CALM | (1 << JUMP_OFFSET_SHIFT));
+}
+
+/* insert a cmd in the desc at a given index and optionally update desc len */
+static void insert_sec_cmd(uint32_t *desc, uint32_t index, uint32_t cmd,
+			  bool update_len)
+{
+	uint32_t *desc_cmd;
+
+	desc_cmd = desc + index;
+	*desc_cmd = cmd;
+
+	if (update_len)
+		(*desc)++;
+}
+
+/* insert cmds for SEQ_IN/OUT_PTR copy with specified offset (shr_desc_len) */
+static void insert_ptr_copy_cmds(uint32_t *desc, uint32_t index,
+				 uint32_t shr_desc_len, bool update_desc_len)
+{
+	uint32_t cmd, off, len;
+
+	/*
+	 * insert the commands at the specified index
+	 * if index == 0 insert at next position in desc
+	 */
+	if (!index)
+		index = desc_len(desc);
+
+	/*
+	 * move out ptr (from job desc) to math reg 1 & 2, except the last byte;
+	 * assuming all buffers are 256 bits aligned, setting the last address
+	 * byte to 0x00  will give the buffer address;
+	 */
+	off = CAAM_PTR_SZ;
+	off = (shr_desc_len * CAAM_CMD_SZ + off) << MOVE_OFFSET_SHIFT;
+	len = CAAM_CMD_SZ + JOB_DESC_HDR_LEN + ALIGNED_PTR_ADDRESS_SZ;
+	cmd = CMD_MOVE | MOVE_SRC_DESCBUF | MOVE_DEST_MATH1 | off | len;
+	insert_sec_cmd(desc, index, cmd, update_desc_len);
+
+	/*
+	 * move in ptr (from job desc) to math reg 0, except the last byte;
+	 * assuming all buffers are 256 bits aligned, setting the last address
+	 * byte to 0x00  will give the buffer address;
+	 */
+	off = JOB_DESC_HDR_LEN + 3 * CAAM_CMD_SZ + 2 * CAAM_PTR_SZ;
+	off = (shr_desc_len * CAAM_CMD_SZ + off) << MOVE_OFFSET_SHIFT;
+	len = ALIGNED_PTR_ADDRESS_SZ;
+	cmd = CMD_MOVE | MOVE_SRC_DESCBUF | MOVE_DEST_MATH0 | off | len;
+	insert_sec_cmd(desc, ++index, cmd, update_desc_len);
+}
+
+/* build the command set for copying the frame meta data */
+static void build_meta_data_desc_cmds(struct dpa_ipsec_sa *sa,
+				      unsigned int sec_era,
+				      unsigned int move_size)
+{
+	uint32_t *desc, off, len, opt, *no_sg_jump;
+	uint32_t sg_mask = SEQ_OUT_PTR_SGF_MASK;
+
+	BUG_ON(!sa);
+
+	desc = (uint32_t *) sa->sec_desc->desc;
+
+	/* insert cmds to copy SEQ_IN/OUT_PTR - offset will be updated later */
+	insert_ptr_copy_cmds(desc, 0, 0, true);
+
+	/* detect & handle scatter / gather frames */
+
+	/*
+	 * the SEQ OUT PTR command is now in math reg 1, so the SGF bit can be
+	 * checked using a math command;
+	 */
+	append_math_and_imm_u32(desc, NONE, REG1, IMM, sg_mask);
+
+	opt = CLASS_NONE | JUMP_TYPE_LOCAL | JUMP_COND_MATH_Z | JUMP_TEST_ALL;
+	no_sg_jump = append_jump(desc, opt);
+
+	if (sec_era == 2) {
+		/* disable iNFO FIFO entries for p4080rev2 & ??? */
+		len = 0x10 << LDST_LEN_SHIFT;
+		append_cmd(desc, CMD_LOAD | DISABLE_AUTO_INFO_FIFO | len);
+
+		/*
+		 * load in IN FIFO the S/G Entry located in the 5th reg after
+		 * MATH3 -> offset = sizeof(GT_REG) * 4 + offset_math3_to_GT_REG
+		 * len = sizeof(S/G entry)
+		 */
+		opt = MOVE_SRC_MATH3 | MOVE_DEST_CLASS1INFIFO;
+		off = 127 << MOVE_OFFSET_SHIFT;
+		len = 49 << MOVE_LEN_SHIFT;
+		append_move(desc, opt | off | len);
+
+		/* enable iNFO FIFO entries */
+		append_cmd(desc, CMD_LOAD | ENABLE_AUTO_INFO_FIFO);
+	} else {
+		/* ????? */
+		opt = LDST_IMM | LDST_CLASS_DECO | LDST_SRCDST_WORD_DECOCTRL;
+		len = 0x10 << LDST_LEN_SHIFT;
+		append_cmd(desc, CMD_LOAD | opt | len);
+
+		/*
+		 * load in IN FIFO the S/G Entry located in the 5th reg after
+		 * MATH3 -> offset = sizeof(GT_REG) * 4 + offset_math3_to_GT_REG
+		 * len = sizeof(S/G entry)
+		 */
+		opt = MOVE_SRC_MATH3 | MOVE_DEST_INFIFO_NOINFO;
+		off = 127 << MOVE_OFFSET_SHIFT;
+		len = 49 << MOVE_LEN_SHIFT;
+		append_move(desc, opt | off | len);
+	}
+
+	/*
+	 * throw away the first part of the S/G table and keep only the buffer
+	 * address;
+	 * offset = undefined memory after MATH3;
+	 * len =
+	 */
+	opt = MOVE_SRC_INFIFO | MOVE_DEST_MATH3;
+	off = 8 << MOVE_OFFSET_SHIFT;
+	len = 41 << MOVE_LEN_SHIFT;
+	append_move(desc, opt | off | len);
+
+	/* put the buffer address (still in the IN FIFO) in MATH2 */
+	opt = MOVE_SRC_INFIFO | MOVE_DEST_MATH2;
+	off = 0 << MOVE_OFFSET_SHIFT;
+	len = 8 << MOVE_LEN_SHIFT;
+	append_move(desc, opt | off | len);
+
+	/* update no S/G jump location */
+	set_jump_tgt_here(desc, no_sg_jump);
+
+	/* save input pointer to predefined location in descriptor */
+	opt = MOVE_SRC_MATH0 | MOVE_DEST_DESCBUF;
+	off = ((desc_len(desc) + IN_PTR_REL_OFF) << 2) << MOVE_OFFSET_SHIFT;
+	len = ALIGNED_PTR_ADDRESS_SZ << MOVE_LEN_SHIFT;
+	append_move(desc, opt | off | len);
+
+	/* save output pointer to predefined location in descriptor */
+	opt = MOVE_WAITCOMP | MOVE_SRC_MATH2 | MOVE_DEST_DESCBUF;
+	off += (CAAM_PTR_SZ +  2 * CAAM_CMD_SZ) << MOVE_OFFSET_SHIFT;
+	len = ALIGNED_PTR_ADDRESS_SZ << MOVE_LEN_SHIFT;
+	append_move(desc, opt | off | len);
+
+	/* fix LIODN */
+	opt = LDST_IMM | LDST_CLASS_DECO | LDST_SRCDST_WORD_DECOCTRL;
+	off = 0x40 << LDST_OFFSET_SHIFT; /* SEQ LIODN */
+	append_cmd(desc, CMD_LOAD | opt | off);
+
+	/* actual move commands - pointers will be updated at runtime */
+
+	/* load the data to be moved - insert dummy pointer */
+	opt = LDST_CLASS_2_CCB | LDST_SRCDST_WORD_CLASS_CTX;
+	off = 0 << LDST_OFFSET_SHIFT;
+	len = move_size << LDST_LEN_SHIFT;
+	append_load(desc, DUMMY_PTR_VAL, len, opt | off);
+
+	/* wait for completion */
+	opt = JUMP_COND_CALM | (1 << JUMP_OFFSET_SHIFT);
+	append_jump(desc, opt);
+
+	/* store the data to the output fifo - insert dummy pointer */
+	opt = LDST_CLASS_2_CCB | LDST_SRCDST_WORD_CLASS_CTX;
+	off = 0 << LDST_OFFSET_SHIFT;
+	len = move_size << LDST_LEN_SHIFT;
+	append_store(desc, DUMMY_PTR_VAL, len, opt | off);
+
+	/* fix LIODN */
+	opt = LDST_IMM | LDST_CLASS_DECO | LDST_SRCDST_WORD_DECOCTRL;
+	off = 0x80 << LDST_OFFSET_SHIFT; /* NON_SEQ LIODN */
+	append_cmd(desc, CMD_LOAD | opt | off);
+}
+
+int build_shared_descriptor(struct dpa_ipsec_sa *sa,
+			    dma_addr_t auth_key_dma,
+			    dma_addr_t crypto_key_dma, u32 bytes_to_copy)
+{
+	uint32_t *desc, *key_jump_cmd, copy_ptr_index = 0;
+	int opthdrsz;
+	size_t pdb_len = 0;
+
+	desc = (u32 *) sa->sec_desc->desc;
+
+	/* Reserve 2 words for statistics */
+	if (sa->enable_stats)
+		pdb_len = DPA_IPSEC_STATS_LEN * sizeof(u32);
+
+	if (sa->sa_dir == DPA_IPSEC_OUTBOUND) {
+		/* Compute optional header size, rounded up to descriptor
+		 * word size */
+		opthdrsz = (sa->sec_desc->pdb_en.ip_hdr_len + 3) & ~3;
+		pdb_len += sizeof(struct ipsec_encap_pdb) + opthdrsz;
+		init_sh_desc_pdb(desc, HDR_SAVECTX | HDR_SHARE_SERIAL, pdb_len);
+	} else {
+		pdb_len += sizeof(struct ipsec_decap_pdb);
+		init_sh_desc_pdb(desc, HDR_SAVECTX | HDR_SHARE_SERIAL, pdb_len);
+	}
+
+	/* Key jump */
+	key_jump_cmd = append_jump(desc, CLASS_BOTH | JUMP_TEST_ALL |
+				   JUMP_COND_SHRD | JUMP_COND_SELF);
+
+	/* check whether a split of a normal key is used */
+	if (sa->auth_data.split_key_len)
+		/* Append split authentication key */
+		append_key(desc, auth_key_dma, sa->auth_data.split_key_len,
+			   CLASS_2 | KEY_ENC | KEY_DEST_MDHA_SPLIT);
+	else
+		/* Append normal authentication key */
+		append_key(desc, auth_key_dma, sa->auth_data.auth_key_len,
+			   CLASS_2 | KEY_DEST_CLASS_REG);
+
+	/* Append cipher key */
+	append_key(desc, crypto_key_dma, sa->cipher_data.cipher_key_len,
+		   CLASS_1 | KEY_DEST_CLASS_REG);
+
+	set_jump_tgt_here(desc, key_jump_cmd);
+
+	/* copy frame meta data (IC) to enable DSCP / ECN propagation */
+	if (sa->dscp_copy || sa->ecn_copy) {
+		/* save location of ptr copy commands to update offset later */
+		copy_ptr_index = desc_len(desc);
+		build_meta_data_desc_cmds(sa, sa->dpa_ipsec->sec_era, 64);
+	}
+
+	if (bytes_to_copy == 0)
+		goto skip_byte_copy;
+
+	/* Copy L2 header from the original packet to the outer packet */
+
+	/* ld: deco-deco-ctrl len=0 offs=8 imm -auto-nfifo-entries */
+	append_cmd(desc, CMD_LOAD | DISABLE_AUTO_INFO_FIFO);
+
+	/* seqfifold: both msgdata-last2-last1-flush1 len=4 */
+	append_seq_fifo_load(desc, bytes_to_copy, FIFOLD_TYPE_MSG |
+			     FIFOLD_CLASS_BOTH | FIFOLD_TYPE_LAST1 |
+			     FIFOLD_TYPE_LAST2 | FIFOLD_TYPE_FLUSH1);
+
+	/* ld: deco-deco-ctrl len=0 offs=4 imm +auto-nfifo-entries */
+	append_cmd(desc, CMD_LOAD | ENABLE_AUTO_INFO_FIFO);
+
+	/* move: ififo->deco-alnblk -> ofifo, len=4 */
+	append_move(desc, MOVE_SRC_INFIFO | MOVE_DEST_OUTFIFO | bytes_to_copy);
+
+	/* seqfifostr: msgdata len=4 */
+	append_seq_fifo_store(desc, FIFOST_TYPE_MESSAGE_DATA, bytes_to_copy);
+
+	/* Done coping L2 header from the original packet to the outer packet */
+
+skip_byte_copy:
+
+	if (sa->enable_stats)
+		build_stats_descriptor_part(sa, pdb_len);
+
+	/* Protocol specific operation */
+	append_operation(desc, OP_PCLID_IPSEC |
+			 get_ipsec_op_type(sa->sa_dir) |
+			 sa->cipher_data.cipher_type | sa->auth_data.auth_type);
+
+	if (sa->enable_stats)
+		save_stats_in_external_mem(sa);
+
+	if (sa->dscp_copy || sa->ecn_copy)
+		/* insert cmds to copy SEQ_IN/OUT_PTR - with updated offset */
+		insert_ptr_copy_cmds(desc, copy_ptr_index,
+				     desc_len(desc), false);
+
+	if (desc_len(desc) >= MAX_CAAM_SHARED_DESCSIZE) {
+		if (sa->enable_stats)
+			memset((uint8_t *)desc + sa->stats_offset, 0,
+				MAX_CAAM_DESCSIZE * sizeof(u32) -
+				sa->stats_offset);
+		return -EPERM;
+	}
+
+	return 0;
+}
+
+/* Move size should be set to 64 bytes */
+int built_encap_extra_material(struct dpa_ipsec_sa *sa,
+			       dma_addr_t auth_key_dma,
+			       dma_addr_t crypto_key_dma,
+			       unsigned int move_size)
+{
+	uint32_t *extra_cmds, *padding_jump, *key_jump_cmd;
+	uint32_t len, off_b, off_w, off, opt;
+	unsigned char job_desc_len, block_size;
+
+	/* sec_desc_extra_cmds is the address were the first SEC extra command
+	 * is located, from here SEC will overwrite Job descriptor part. Need
+	 * to insert a dummy command because the LINUX CAAM API uses first word
+	 * for storing the length of the descriptor. */
+	extra_cmds = sa->sec_desc_extra_cmds - 1;
+
+	/* Dummy command - will not be executed at all. Only for setting to 1
+	 * the length of the extra_cmds descriptor so that first extra material
+	 * command will be located exactly at sec_desc_extra_cmds address. */
+	append_cmd(extra_cmds, 0xdead0000);
+
+	/* Start Extra Material Group 1 */
+	/* Load from the input address 64 bytes into internal register */
+	/* load the data to be moved - insert dummy pointer */
+	opt = LDST_CLASS_2_CCB | LDST_SRCDST_WORD_CLASS_CTX;
+	off = 0 << LDST_OFFSET_SHIFT;
+	len = move_size << LDST_LEN_SHIFT;
+	append_load(extra_cmds, DUMMY_PTR_VAL, len, opt | off);
+
+	/* Wait to finish previous operation */
+	opt = JUMP_COND_CALM | (1 << JUMP_OFFSET_SHIFT);
+	append_jump(extra_cmds, opt);
+
+	/* Store the data to the output FIFO - insert dummy pointer */
+	opt = LDST_CLASS_2_CCB | LDST_SRCDST_WORD_CLASS_CTX;
+	off = 0 << LDST_OFFSET_SHIFT;
+	len = move_size << LDST_LEN_SHIFT;
+	append_store(extra_cmds, DUMMY_PTR_VAL, len, opt | off);
+
+	/* Fix LIODN */
+	opt = LDST_IMM | LDST_CLASS_DECO | LDST_SRCDST_WORD_DECOCTRL;
+	off = 0x80 << LDST_OFFSET_SHIFT; /* NON_SEQ LIODN */
+	append_cmd(extra_cmds, CMD_LOAD | opt | off);
+
+	/* MATH0 += 1 (packet counter) */
+	append_math_add(extra_cmds, REG0, REG0, ONE, MATH_LEN_8BYTE);
+
+	/* Overwrite the job-desc location (word 51 or 53) with the second
+	 * group (10 words) */
+	job_desc_len = sa->job_desc_len;
+	opt   = MOVE_SRC_INFIFO | MOVE_DEST_DESCBUF | MOVE_WAITCOMP;
+	off_w = MAX_CAAM_DESCSIZE - job_desc_len;
+	off_b = off_w * sizeof(uint32_t); /* calculate off in bytes */
+	len   = (10 * sizeof(uint32_t)) << MOVE_LEN_SHIFT;
+	append_move(extra_cmds, opt | (off_b << MOVE_OFFSET_SHIFT) | len);
+
+	/* Jump to the beginning of the JOB Descriptor to start executing
+	 * the extra material group 2 */
+	append_cmd(extra_cmds, 0xa00000f6);
+
+	/* End of Extra Material Group 1 */
+
+	/* Start Extra Material Group 2 */
+	/* MATH REG 2 = Sequence in length + 2; 2 for pad-len and NH field */
+	append_math_add_imm_u32(extra_cmds, REG2, SEQINLEN, IMM, 2);
+
+	switch (sa->cipher_data.cipher_type) {
+	case OP_PCL_IPSEC_3DES:
+		block_size = 8; /* block size in bytes */
+		break;
+	case OP_PCL_IPSEC_AES_CBC:
+	case OP_PCL_IPSEC_AES_CTR:
+	case OP_PCL_IPSEC_AES_XTS:
+	case OP_PCL_IPSEC_AES_CCM8:
+	case OP_PCL_IPSEC_AES_CCM12:
+	case OP_PCL_IPSEC_AES_CCM16:
+	case OP_PCL_IPSEC_AES_GCM8:
+	case OP_PCL_IPSEC_AES_GCM12:
+	case OP_PCL_IPSEC_AES_GCM16:
+		block_size = 16; /* block size in bytes */
+		break;
+	default:
+		pr_crit("Invalid cipher algorithm for SA %d\n", sa->id);
+		return -EINVAL;
+	}
+
+	/* Adding padding to byte counter */
+	append_math_and_imm_u32(extra_cmds, REG3, REG2, IMM, block_size - 1);
+
+	/* Previous operation result is 0 i.e padding added to bytes count */
+	padding_jump = append_jump(extra_cmds, CLASS_BOTH | JUMP_TEST_ALL |
+				   JUMP_COND_MATH_Z);
+
+	/* MATH REG 2 = MATH REG 2 + 1 */
+	append_math_add(extra_cmds, REG2, REG2, ONE, MATH_LEN_4BYTE);
+
+	/* jump back to adding padding i.e jump back 4 words */
+	off = (-4) & 0x000000FF;
+	append_jump(extra_cmds, (off << JUMP_OFFSET_SHIFT));
+
+	set_jump_tgt_here(extra_cmds, padding_jump);
+	/* Done adding padding to byte counter */
+
+	/*
+	 * Perform 32-bit left shift of DEST and concatenate with left 32 bits
+	 * of SRC1 i.e MATH REG 2 = 0x00bytecount_00000000
+	 */
+	append_math_ldshift(extra_cmds, REG2, REG0, REG2, MATH_LEN_8BYTE);
+
+	/* MATH REG 0  = MATH REG 0 + MATH REG 2 */
+	append_math_add(extra_cmds, REG0, REG0, REG2, MATH_LEN_8BYTE);
+
+	/* Overwrite the job-desc location (word 51 or 53) with the third
+	 * group (11 words) */
+	opt   = MOVE_SRC_INFIFO | MOVE_DEST_DESCBUF | MOVE_WAITCOMP;
+	off_w = MAX_CAAM_DESCSIZE - job_desc_len;
+	off_b = off_w * sizeof(uint32_t); /* calculate off in bytes */
+	len   = (11 * sizeof(uint32_t)) << MOVE_LEN_SHIFT;
+	append_move(extra_cmds, opt | (off_b << MOVE_OFFSET_SHIFT) | len);
+
+	/* Jump to the beginning of the JOB Descriptor to start executing
+	 * the extra material group 3. The command for jumping back is already
+	 * here from extra material group 1 */
+
+	/* End of Extra Material Group 2 */
+
+	/* Start Extra Material Group 3 */
+
+	if (sa->enable_stats) {
+		/* Store statistics in the CAAM internal descriptor */
+		off_b = sa->stats_indx * CAAM_CMD_SZ;
+		append_move(extra_cmds, MOVE_SRC_MATH0 | MOVE_DEST_DESCBUF |
+			    (off_b << MOVE_OFFSET_SHIFT) |
+			    sizeof(uint64_t));
+	} else {
+		/* Statistics are disabled. Do not update descriptor counter */
+		append_cmd(extra_cmds, 0xA0000001); /* NOP for SEC */
+	}
+
+	/* Key jump */
+	key_jump_cmd = append_jump(extra_cmds, CLASS_BOTH | JUMP_TEST_ALL |
+				   JUMP_COND_SHRD);
+
+	/* check whether a split of a normal key is used */
+	if (sa->auth_data.split_key_len)
+		/* Append split authentication key */
+		append_key(extra_cmds, auth_key_dma,
+			   sa->auth_data.split_key_len,
+			   CLASS_2 | KEY_ENC | KEY_DEST_MDHA_SPLIT);
+	else
+		/* Append normal authentication key */
+		append_key(extra_cmds, auth_key_dma, sa->auth_data.auth_key_len,
+			   CLASS_2 | KEY_DEST_CLASS_REG);
+
+	/* Append cipher key */
+	append_key(extra_cmds, crypto_key_dma, sa->cipher_data.cipher_key_len,
+		   CLASS_1 | KEY_DEST_CLASS_REG);
+
+	set_jump_tgt_here(extra_cmds, key_jump_cmd);
+
+	/* Protocol specific operation */
+	append_operation(extra_cmds, OP_PCLID_IPSEC | OP_TYPE_ENCAP_PROTOCOL |
+			 sa->cipher_data.cipher_type | sa->auth_data.auth_type);
+
+	if (sa->enable_stats) {
+		/* Store command: in the case of the Descriptor Buffer the
+		 * length is specified in 4-byte words, but in all other cases
+		 * the length is specified in bytes. Offset in 4 byte words */
+		off_w = sa->stats_indx;
+		append_store(extra_cmds, 0, DPA_IPSEC_STATS_LEN,
+			     LDST_CLASS_DECO | (off_w << LDST_OFFSET_SHIFT) |
+			     LDST_SRCDST_WORD_DESCBUF_SHARED);
+	} else {
+		/* Do not store lifetime counter in external memory */
+		append_cmd(extra_cmds, 0xA0000001); /* NOP for SEC */
+	}
+
+	/* Jump with CALM to be sure previous operation was finished */
+	append_jump(extra_cmds, JUMP_TYPE_HALT_USER | JUMP_COND_CALM);
+
+	/* End of Extra Material Group 3 */
+
+	return 0;
+}
+
+/* Move size should be set to 64 bytes */
+void built_decap_extra_material(struct dpa_ipsec_sa *sa,
+			       dma_addr_t auth_key_dma,
+			       dma_addr_t crypto_key_dma)
+{
+	uint32_t *extra_cmds;
+	uint32_t off_b, off_w, data;
+
+	/* sec_desc_extra_cmds is the address were the first SEC extra command
+	 * is located, from here SEC will overwrite Job descriptor part. Need
+	 * to insert a dummy command because the LINUX CAAM API uses first word
+	 * for storing the length of the descriptor. */
+	extra_cmds = sa->sec_desc_extra_cmds - 1;
+
+	/* Dummy command - will not be executed at all. Only for setting to 1
+	 * the length of the extra_cmds descriptor so that first extra material
+	 * command will be located exactly at sec_desc_extra_cmds address. */
+	append_cmd(extra_cmds, 0xdead0000);
+
+	data = 16;
+	append_math_rshift_imm_u64(extra_cmds, REG2, REG2, IMM, data);
+
+	/* math: (math1 - math2)->math1 len=8 */
+	append_math_sub(extra_cmds, REG1, REG1, REG2, MATH_LEN_8BYTE);
+
+	/* math: (math0 + 1)->math0 len=8 */
+	append_math_add(extra_cmds, REG0, REG0, ONE, MATH_LEN_8BYTE);
+
+	append_math_ldshift(extra_cmds, REG1, REG0, REG1, MATH_LEN_8BYTE);
+
+	append_math_add(extra_cmds, REG0, REG0, REG1, MATH_LEN_8BYTE);
+
+	append_cmd(extra_cmds, 0x7883c824);
+
+	/* Store in the descriptor but not in external memory */
+	off_b = sa->stats_offset;
+	append_move(extra_cmds, MOVE_SRC_MATH0 | MOVE_DEST_DESCBUF |
+		    MOVE_WAITCOMP | (off_b << MOVE_OFFSET_SHIFT) | sizeof(u64));
+
+	append_cmd(extra_cmds, 0xa70040fe);
+
+	append_cmd(extra_cmds, 0xa00000f7);
+
+	/* check whether a split of a normal key is used */
+	if (sa->auth_data.split_key_len)
+		/* Append split authentication key */
+		append_key(extra_cmds, auth_key_dma,
+			   sa->auth_data.split_key_len,
+			   CLASS_2 | KEY_ENC | KEY_DEST_MDHA_SPLIT);
+	else
+		/* Append normal authentication key */
+		append_key(extra_cmds, auth_key_dma, sa->auth_data.auth_key_len,
+			   CLASS_2 | KEY_DEST_CLASS_REG);
+
+	/* Append cipher key */
+	append_key(extra_cmds, crypto_key_dma, sa->cipher_data.cipher_key_len,
+		   CLASS_1 | KEY_DEST_CLASS_REG);
+
+	/* Protocol specific operation */
+	append_operation(extra_cmds, OP_PCLID_IPSEC | OP_TYPE_DECAP_PROTOCOL |
+			 sa->cipher_data.cipher_type | sa->auth_data.auth_type);
+
+	/* Store command: in the case of the Descriptor Buffer the length
+	 * is specified in 4-byte words, but in all other cases the length
+	 * is specified in bytes. Offset in 4 byte words */
+	off_w = sa->stats_indx;
+	append_store(extra_cmds, 0, DPA_IPSEC_STATS_LEN,
+		     LDST_CLASS_DECO | (off_w << LDST_OFFSET_SHIFT) |
+		     LDST_SRCDST_WORD_DESCBUF_SHARED);
+
+	append_jump(extra_cmds, JUMP_TYPE_HALT_USER | JUMP_COND_CALM);
+}
+
+int build_extended_encap_shared_descriptor(struct dpa_ipsec_sa *sa,
+				     dma_addr_t auth_key_dma,
+				     dma_addr_t crypto_key_dma,
+				     uint8_t bytes_to_copy,
+				     int sec_era)
+{
+	uint32_t *desc, *no_sg_jump, *extra_cmds;
+	uint32_t len, off_b, off_w, opt, stats_off_b, sg_mask;
+	struct device *jrdev;
+	unsigned int extra_cmds_len;
+	unsigned char job_desc_len;
+	dma_addr_t dma_extra_cmds;
+	int ret;
+
+	desc = (uint32_t *)sa->sec_desc->desc;
+
+	if (sec_era == 2) {
+		if (sa->enable_stats)
+			sa->stats_indx = 27;
+		sa->next_cmd_indx = 29;
+	} else {
+		if (sa->enable_stats)
+			sa->stats_indx = 28;
+		sa->next_cmd_indx = 30;
+	}
+
+	/* This code only works when SEC is configured to use PTR on 64 bit
+	 * so the Job Descriptor length is 13 words long when DPOWRD is set */
+	job_desc_len = 13;
+
+	/* Set CAAM Job Descriptor length */
+	sa->job_desc_len = job_desc_len;
+
+	/* Set lifetime counter stats offset */
+	sa->stats_offset = sa->stats_indx * sizeof(uint32_t);
+
+	ret = built_encap_extra_material(sa, auth_key_dma, crypto_key_dma, 64);
+	if (ret < 0) {
+		pr_err("Failed to create extra CAAM commands\n");
+		return -EAGAIN;
+	}
+
+	extra_cmds = sa->sec_desc_extra_cmds - 1;
+	extra_cmds_len = desc_len(extra_cmds) - 1;
+
+	/* get the jr device  */
+	jrdev = get_jrdev(sa->dpa_ipsec);
+	if (!jrdev) {
+		pr_err("Failed to get the job ring device, check the dts\n");
+		return -EINVAL;
+	}
+
+	dma_extra_cmds = dma_map_single(jrdev, sa->sec_desc_extra_cmds,
+					extra_cmds_len * sizeof(uint32_t),
+					DMA_TO_DEVICE);
+	if (!dma_extra_cmds) {
+		pr_err("Could not DMA map extra CAAM commands\n");
+		return -ENXIO;
+	}
+
+	init_sh_desc_pdb(desc, HDR_SAVECTX | HDR_SHARE_SERIAL,
+			 (sa->next_cmd_indx - 1) * sizeof(uint32_t));
+
+	if (sec_era == 2) {
+		/* disable iNFO FIFO entries for p4080rev2 & ??? */
+		len = 0x10 << LDST_LEN_SHIFT;
+		append_cmd(desc, CMD_LOAD | DISABLE_AUTO_INFO_FIFO | len);
+
+		/*
+		 * load in IN FIFO the S/G Entry located in the 5th reg after
+		 * MATH3 -> offset = sizeof(GT_REG) * 4 + offset_math3_to_GT_REG
+		 * len = sizeof(S/G entry)
+		 * Offset refers to SRC
+		 */
+		opt   = MOVE_SRC_MATH3 | MOVE_DEST_CLASS1INFIFO;
+		off_b = 127 << MOVE_OFFSET_SHIFT;
+		len   = 49 << MOVE_LEN_SHIFT;
+		append_move(desc, opt | off_b | len);
+
+		/*
+		 * L2 part 1
+		 * Load from input packet to INPUT DATA FIFO first bytes_to_copy
+		 * bytes.
+		 */
+		append_seq_fifo_load(desc, bytes_to_copy, FIFOLD_TYPE_MSG |
+				     FIFOLD_CLASS_BOTH | FIFOLD_TYPE_LAST1 |
+				     FIFOLD_TYPE_LAST2 | FIFOLD_TYPE_FLUSH1);
+
+		/*
+		 * Extra word part 1
+		 * Load extra words for this descriptor into the INPUT DATA FIFO
+		 */
+		append_fifo_load(desc, dma_extra_cmds,
+				 extra_cmds_len * sizeof(uint32_t),
+				 FIFOLD_TYPE_MSG | FIFOLD_CLASS_BOTH |
+				 FIFOLD_TYPE_LAST1 | FIFOLD_TYPE_LAST2 |
+				 FIFOLD_TYPE_FLUSH1);
+
+		/* enable iNFO FIFO entries */
+		append_cmd(desc, CMD_LOAD | ENABLE_AUTO_INFO_FIFO);
+	} else {
+		/* ????? */
+		opt = LDST_IMM | LDST_CLASS_DECO | LDST_SRCDST_WORD_DECOCTRL;
+		len = 0x10 << LDST_LEN_SHIFT;
+		append_cmd(desc, CMD_LOAD | opt | len);
+
+		/*
+		 * load in IN FIFO the S/G Entry located in the 5th reg after
+		 * MATH3 -> offset = sizeof(GT_REG) * 4 + offset_math3_to_GT_REG
+		 * len = sizeof(S/G entry)
+		 */
+		opt   = MOVE_SRC_MATH3 | MOVE_DEST_INFIFO_NOINFO;
+		off_b = 127 << MOVE_OFFSET_SHIFT;
+		len   = 49 << MOVE_LEN_SHIFT;
+		append_move(desc, opt | off_b | len);
+
+		/*
+		 * L2 part 1
+		 * Load from input packet to INPUT DATA FIFO first bytes_to_copy
+		 * bytes. No information FIFO entry even if automatic
+		 * iNformation FIFO entries are enabled.
+		 */
+		append_seq_fifo_load(desc, bytes_to_copy, FIFOLD_CLASS_BOTH |
+				     FIFOLD_TYPE_NOINFOFIFO);
+
+		/*
+		 * Extra word part 1
+		 * Load extra words for this descriptor into the INPUT DATA FIFO
+		 */
+		append_fifo_load(desc, dma_extra_cmds,
+				 extra_cmds_len * sizeof(uint32_t),
+				 FIFOLD_CLASS_BOTH | FIFOLD_TYPE_NOINFOFIFO);
+	}
+
+	/*
+	 * throw away the first part of the S/G table and keep only the buffer
+	 * address;
+	 * offset = undefined memory after MATH3; Refers to the destination.
+	 * len = 41 bytes to discard
+	 */
+	opt   = MOVE_SRC_INFIFO | MOVE_DEST_MATH3;
+	off_b = 8 << MOVE_OFFSET_SHIFT;
+	len   = 41 << MOVE_LEN_SHIFT;
+	append_move(desc, opt | off_b | len);
+
+	/* put the buffer address (still in the IN FIFO) in MATH2 */
+	opt   = MOVE_SRC_INFIFO | MOVE_DEST_MATH3;
+	off_b = 0 << MOVE_OFFSET_SHIFT;
+	len   = 8 << MOVE_LEN_SHIFT;
+	append_move(desc, opt | off_b | len);
+
+	/* copy 15 bytes starting at 4 bytes before the OUT-PTR-CMD in
+	 * the job-desc into math1
+	 * i.e. in the low-part of math1 we have the out-ptr-cmd and
+	 * in the math2 we will have the address of the out-ptr
+	 */
+	opt = MOVE_SRC_DESCBUF | MOVE_DEST_MATH1;
+	off_b = (MAX_CAAM_DESCSIZE - job_desc_len + PTR_LEN) * sizeof(uint32_t);
+	len = (8 + 4 * PTR_LEN - 1) << MOVE_LEN_SHIFT;
+	append_move(desc, opt | (off_b << MOVE_OFFSET_SHIFT) | len);
+
+	/* Copy 7 bytes of the in-ptr into math0 */
+	opt   = MOVE_SRC_DESCBUF | MOVE_DEST_MATH0;
+	off_w = MAX_CAAM_DESCSIZE - job_desc_len + 1 + 3 + 2 * PTR_LEN;
+	off_b = off_w * sizeof(uint32_t); /* calculate off in bytes */
+	len   = ALIGNED_PTR_ADDRESS_SZ << MOVE_LEN_SHIFT;
+	append_move(desc, opt | (off_b << MOVE_OFFSET_SHIFT) | len);
+
+	/*
+	 * the SEQ OUT PTR command is now in math reg 1, so the SGF bit can be
+	 * checked using a math command;
+	 */
+	sg_mask = SEQ_OUT_PTR_SGF_MASK;
+	append_math_and_imm_u32(desc, NONE, REG1, IMM, sg_mask);
+
+	opt = CLASS_NONE | JUMP_TYPE_LOCAL | JUMP_COND_MATH_Z | JUMP_TEST_ALL;
+	no_sg_jump = append_jump(desc, opt);
+
+	append_math_add(desc, REG2, ZERO, REG3, MATH_LEN_8BYTE);
+
+	/* update no S/G jump location */
+	set_jump_tgt_here(desc, no_sg_jump);
+
+	/* seqfifostr: msgdata len=4 */
+	append_seq_fifo_store(desc, FIFOST_TYPE_MESSAGE_DATA, bytes_to_copy);
+
+	/* move: ififo->deco-alnblk -> ofifo, len=4 */
+	append_move(desc, MOVE_SRC_INFIFO | MOVE_DEST_OUTFIFO | bytes_to_copy);
+
+	/* Overwrite the job-desc location (word 51 or 53) with the first
+	 * group (11 words)*/
+	opt   = MOVE_SRC_INFIFO | MOVE_DEST_DESCBUF;
+	off_w = MAX_CAAM_DESCSIZE - job_desc_len;
+	off_b = off_w * sizeof(uint32_t); /* calculate off in bytes */
+	len   = (11 * sizeof(uint32_t)) << MOVE_LEN_SHIFT;
+	append_move(desc, opt | (off_b << MOVE_OFFSET_SHIFT) | len);
+
+	/*
+	 * Copy the context of math0 (input address) to words 52+53 or 54+56
+	 * depending where the Job Descriptor starts.
+	 * They will be used later by the load command.
+	 */
+	opt = MOVE_SRC_MATH0 | MOVE_DEST_DESCBUF;
+	off_w = MAX_CAAM_DESCSIZE - job_desc_len + 1; /* 52 + 53 or 54 + 55 */
+	off_b = off_w * sizeof(uint32_t);
+	len = ALIGNED_PTR_ADDRESS_SZ << MOVE_LEN_SHIFT;
+	append_move(desc, opt | (off_b << MOVE_OFFSET_SHIFT) | len);
+
+	/*
+	 * Copy the context of math2 (output address) to words 56+57 or 58+59
+	 * depending where the Job Descriptor starts.
+	 * They will be used later by the store command.
+	 */
+	opt = MOVE_SRC_MATH2 | MOVE_DEST_DESCBUF;
+	off_w = MAX_CAAM_DESCSIZE - job_desc_len + 5; /* 56 + 57 or 58 + 59 */
+	off_b = off_w * sizeof(uint32_t);
+	len = ALIGNED_PTR_ADDRESS_SZ << MOVE_LEN_SHIFT;
+	append_move(desc, opt | (off_b << MOVE_OFFSET_SHIFT) | len);
+
+	/* Fix LIODN - OFFSET[0:1] - 01 = SEQ LIODN */
+	opt = LDST_IMM | LDST_CLASS_DECO | LDST_SRCDST_WORD_DECOCTRL;
+	off_b = 0x40; /* SEQ LIODN */
+	append_cmd(desc, CMD_LOAD | opt | (off_b << LDST_OFFSET_SHIFT));
+
+	/* Copy the context of the counters from word 29 into math0 */
+	/* Copy from descriptor to MATH REG 0 the current statistics */
+	stats_off_b = sa->stats_indx * CAAM_CMD_SZ;
+	append_move(desc, MOVE_SRC_DESCBUF | MOVE_DEST_MATH0 |
+		    (stats_off_b << MOVE_OFFSET_SHIFT) | sizeof(uint64_t));
+
+	dma_unmap_single(sa->dpa_ipsec->jrdev, dma_extra_cmds,
+			 extra_cmds_len * sizeof(uint32_t), DMA_TO_DEVICE);
+
+	return 0;
+}
+
+int build_extended_decap_shared_descriptor(struct dpa_ipsec_sa *sa,
+					   dma_addr_t auth_key_dma,
+					   dma_addr_t crypto_key_dma,
+					   uint32_t bytes_to_copy,
+					   uint8_t move_size,
+					   int sec_era)
+{
+	uint32_t *desc, *no_sg_jump, *extra_cmds;
+	uint32_t len, off_b, off_w, opt, stats_off_b, sg_mask, extra_cmds_len,
+		 esp_length, iv_length, icv_length, max_pad, data;
+	dma_addr_t dma_extra_cmds;
+	struct device *jrdev;
+
+	desc = (uint32_t *)sa->sec_desc->desc;
+
+	/* CAAM hdr cmd + PDB size in words */
+	sa->next_cmd_indx =
+		sizeof(struct ipsec_decap_pdb) / sizeof(uint32_t) + 1;
+	if (sa->enable_stats) {
+		sa->stats_indx = sa->next_cmd_indx;
+		sa->next_cmd_indx += 2;
+		if (sec_era != 2) {
+			sa->stats_indx += 1;
+			sa->next_cmd_indx += 1;
+		}
+	}
+
+	/* Set lifetime counter stats offset */
+	sa->stats_offset = sa->stats_indx * sizeof(uint32_t);
+
+	built_decap_extra_material(sa, auth_key_dma, crypto_key_dma);
+
+	extra_cmds = sa->sec_desc_extra_cmds - 1;
+	extra_cmds_len = desc_len(extra_cmds) - 1;
+
+	/* get the jr device  */
+	jrdev = get_jrdev(sa->dpa_ipsec);
+	if (!jrdev) {
+		pr_err("Failed to get the job ring device, check the dts\n");
+		return -EINVAL;
+	}
+
+	dma_extra_cmds = dma_map_single(jrdev, sa->sec_desc_extra_cmds,
+					extra_cmds_len * sizeof(uint32_t),
+					DMA_TO_DEVICE);
+	if (!dma_extra_cmds) {
+		pr_err("Could not DMA map extra CAAM commands\n");
+		return -ENXIO;
+	}
+
+	init_sh_desc_pdb(desc, HDR_SAVECTX | HDR_SHARE_SERIAL,
+			 (sa->next_cmd_indx - 1) * sizeof(uint32_t));
+
+	if (sec_era == 2) {
+		/* disable iNFO FIFO entries for p4080rev2 & ??? */
+		len = 0x10 << LDST_LEN_SHIFT;
+		append_cmd(desc, CMD_LOAD | DISABLE_AUTO_INFO_FIFO | len);
+
+		/*
+		 * load in IN FIFO the S/G Entry located in the 5th reg after
+		 * MATH3 -> offset = sizeof(GT_REG) * 4 + offset_math3_to_GT_REG
+		 * len = sizeof(S/G entry)
+		 * Offset refers to SRC
+		 */
+		opt   = MOVE_SRC_MATH3 | MOVE_DEST_CLASS1INFIFO;
+		off_b = 127 << MOVE_OFFSET_SHIFT;
+		len   = 49 << MOVE_LEN_SHIFT;
+		append_move(desc, opt | off_b | len);
+
+		/*
+		 * L2 part 1
+		 * Load from input packet to INPUT DATA FIFO first bytes_to_copy
+		 * bytes.
+		 */
+		append_seq_fifo_load(desc, bytes_to_copy, FIFOLD_TYPE_MSG |
+				     FIFOLD_CLASS_BOTH | FIFOLD_TYPE_LAST1 |
+				     FIFOLD_TYPE_LAST2 | FIFOLD_TYPE_FLUSH1);
+
+		/*
+		 * Extra word part 1
+		 * Load extra words for this descriptor into the INPUT DATA FIFO
+		 */
+		append_fifo_load(desc, dma_extra_cmds,
+				 extra_cmds_len * sizeof(uint32_t),
+				 FIFOLD_TYPE_MSG | FIFOLD_CLASS_BOTH |
+				 FIFOLD_TYPE_LAST1 | FIFOLD_TYPE_LAST2 |
+				 FIFOLD_TYPE_FLUSH1);
+
+		/* enable iNFO FIFO entries */
+		append_cmd(desc, CMD_LOAD | ENABLE_AUTO_INFO_FIFO);
+	} else {
+		/* ????? */
+		opt = LDST_IMM | LDST_CLASS_DECO | LDST_SRCDST_WORD_DECOCTRL;
+		len = 0x10 << LDST_LEN_SHIFT;
+		append_cmd(desc, CMD_LOAD | opt | len);
+
+		/*
+		 * load in IN FIFO the S/G Entry located in the 5th reg after
+		 * MATH3 -> offset = sizeof(GT_REG) * 4 + offset_math3_to_GT_REG
+		 * len = sizeof(S/G entry)
+		 */
+		opt   = MOVE_SRC_MATH3 | MOVE_DEST_INFIFO_NOINFO;
+		off_b = 127 << MOVE_OFFSET_SHIFT;
+		len   = 49 << MOVE_LEN_SHIFT;
+		append_move(desc, opt | off_b | len);
+
+		/*
+		 * L2 part 1
+		 * Load from input packet to INPUT DATA FIFO first bytes_to_copy
+		 * bytes. No information FIFO entry even if automatic
+		 * iNformation FIFO entries are enabled.
+		 */
+		append_seq_fifo_load(desc, bytes_to_copy, FIFOLD_CLASS_BOTH |
+				     FIFOLD_TYPE_NOINFOFIFO);
+
+		/*
+		 * Extra word part 1
+		 * Load extra words for this descriptor into the INPUT DATA FIFO
+		 */
+		append_fifo_load(desc, dma_extra_cmds,
+				 extra_cmds_len * sizeof(uint32_t),
+				 FIFOLD_CLASS_BOTH | FIFOLD_TYPE_NOINFOFIFO);
+	}
+
+	/*
+	 * throw away the first part of the S/G table and keep only the buffer
+	 * address;
+	 * offset = undefined memory after MATH3; Refers to the destination.
+	 * len = 41 bytes to discard
+	 */
+	opt   = MOVE_SRC_INFIFO | MOVE_DEST_MATH3;
+	off_b = 8 << MOVE_OFFSET_SHIFT;
+	len   = 41 << MOVE_LEN_SHIFT;
+	append_move(desc, opt | off_b | len);
+
+	/* put the buffer address (still in the IN FIFO) in MATH2 */
+	opt   = MOVE_SRC_INFIFO | MOVE_DEST_MATH3;
+	off_b = 0 << MOVE_OFFSET_SHIFT;
+	len   = 8 << MOVE_LEN_SHIFT;
+	append_move(desc, opt | off_b | len);
+
+	/*
+	 * Copy 15 bytes starting at 4 bytes before the OUT-PTR-CMD in
+	 * the job-desc into math1
+	 * i.e. in the low-part of math1 we have the out-ptr-cmd and
+	 * in the math2 we will have the address of the out-ptr
+	 */
+	opt = MOVE_SRC_DESCBUF | MOVE_DEST_MATH1;
+	off_b = (50 + 1 * PTR_LEN) * sizeof(uint32_t);
+	len = (8 + 4 * PTR_LEN - 1) << MOVE_LEN_SHIFT;
+	append_move(desc, opt | (off_b << MOVE_OFFSET_SHIFT) | len);
+
+	/* Copy 7 bytes of the in-ptr into math0 */
+	opt   = MOVE_SRC_DESCBUF | MOVE_DEST_MATH0;
+	off_w = 50 + 1 + 3 + 2 * PTR_LEN;
+	off_b = off_w * sizeof(uint32_t); /* calculate off in bytes */
+	len   = ALIGNED_PTR_ADDRESS_SZ << MOVE_LEN_SHIFT;
+	append_move(desc, opt | (off_b << MOVE_OFFSET_SHIFT) | len);
+
+	/*
+	 * the SEQ OUT PTR command is now in math reg 1, so the SGF bit can be
+	 * checked using a math command;
+	 */
+	sg_mask = SEQ_OUT_PTR_SGF_MASK;
+	append_math_and_imm_u32(desc, NONE, REG1, IMM, sg_mask);
+
+	opt = CLASS_NONE | JUMP_TYPE_LOCAL | JUMP_COND_MATH_Z | JUMP_TEST_ALL;
+	no_sg_jump = append_jump(desc, opt);
+
+	append_math_add(desc, REG2, ZERO, REG3, MATH_LEN_8BYTE);
+
+	/* update no S/G jump location */
+	set_jump_tgt_here(desc, no_sg_jump);
+
+	/* seqfifostr: msgdata len=4 */
+	append_seq_fifo_store(desc, FIFOST_TYPE_MESSAGE_DATA, bytes_to_copy);
+
+	/* move: ififo->deco-alnblk -> ofifo, len */
+	append_move(desc, MOVE_SRC_INFIFO | MOVE_DEST_OUTFIFO | bytes_to_copy);
+
+	/* Overwrite the job-desc location (word 50) with the first
+	 * group (10 words)*/
+	opt   = MOVE_SRC_INFIFO | MOVE_DEST_DESCBUF;
+	off_w = 50;
+	off_b = off_w * sizeof(uint32_t); /* calculate off in bytes */
+	len   = (10 * sizeof(uint32_t)) << MOVE_LEN_SHIFT;
+	append_move(desc, opt | (off_b << MOVE_OFFSET_SHIFT) | len);
+
+	/*
+	 * Copy the context of math0 (input address) to words 32+33
+	 * They will be used later by the load command.
+	 */
+	opt = MOVE_SRC_MATH0 | MOVE_DEST_DESCBUF;
+	off_w = 32;
+	off_b = off_w * sizeof(uint32_t);
+	len = ALIGNED_PTR_ADDRESS_SZ << MOVE_LEN_SHIFT;
+	append_move(desc, opt | (off_b << MOVE_OFFSET_SHIFT) | len);
+
+	/*
+	 * Copy the context of math2 (output address) to words 56+57 or 58+59
+	 * depending where the Job Descriptor starts.
+	 * They will be used later by the store command.
+	 */
+	opt = MOVE_SRC_MATH2 | MOVE_DEST_DESCBUF | MOVE_WAITCOMP;
+	off_w = 36;
+	off_b = off_w * sizeof(uint32_t);
+	len = ALIGNED_PTR_ADDRESS_SZ << MOVE_LEN_SHIFT;
+	append_move(desc, opt | (off_b << MOVE_OFFSET_SHIFT) | len);
+
+	/* Fix LIODN - OFFSET[0:1] - 01 = SEQ LIODN */
+	opt = LDST_IMM | LDST_CLASS_DECO | LDST_SRCDST_WORD_DECOCTRL;
+	off_b = 0x40; /* SEQ LIODN */
+	append_cmd(desc, CMD_LOAD | opt | (off_b << LDST_OFFSET_SHIFT));
+
+	/* Load from the input address 64 bytes into internal register */
+	/* load the data to be moved - insert dummy pointer */
+	opt = LDST_CLASS_2_CCB | LDST_SRCDST_WORD_CLASS_CTX;
+	off_b = 0 << LDST_OFFSET_SHIFT;
+	len = move_size << LDST_LEN_SHIFT;
+	append_load(desc, DUMMY_PTR_VAL, len, opt | off_b);
+
+	/* Wait to finish previous operation */
+	opt = JUMP_COND_CALM | (1 << JUMP_OFFSET_SHIFT);
+	append_jump(desc, opt);
+
+	/* Store the data to the output FIFO - insert dummy pointer */
+	opt = LDST_CLASS_2_CCB | LDST_SRCDST_WORD_CLASS_CTX;
+	off_b = 0 << LDST_OFFSET_SHIFT;
+	len = move_size << LDST_LEN_SHIFT;
+	append_store(desc, DUMMY_PTR_VAL, len, opt | off_b);
+
+	/* Fix LIODN */
+	opt = LDST_IMM | LDST_CLASS_DECO | LDST_SRCDST_WORD_DECOCTRL;
+	off_b = 0x80 << LDST_OFFSET_SHIFT; /* NON_SEQ LIODN */
+	append_cmd(desc, CMD_LOAD | opt | off_b);
+
+	/* Copy from descriptor to MATH REG 0 the current statistics */
+	stats_off_b = sa->stats_indx * CAAM_CMD_SZ;
+	append_move(desc, MOVE_SRC_DESCBUF | MOVE_DEST_MATH0 | MOVE_WAITCOMP |
+		    (stats_off_b << MOVE_OFFSET_SHIFT) | sizeof(uint64_t));
+
+	/* Remove unnecessary headers
+	 * MATH1 = 0 - (esp_length + iv_length + icv_length) */
+	esp_length = 8; /* SPI + SEQ NUM */
+	get_cipher_params(sa->alg_suite, &iv_length, &icv_length, &max_pad);
+	data = (uint32_t) (esp_length + iv_length + icv_length);
+	append_math_sub_imm_u64(desc, REG1, ZERO, IMM, data);
+
+	/* MATH1 += SIL (bytes counter) */
+	append_math_add(desc, REG1, SEQINLEN, REG1, MATH_LEN_8BYTE);
+
+	/* data = outer IP header - should be read from DPOVRD register
+	 * MATH 2 = outer IP header length */
+	data = 20;
+	opt = LDST_CLASS_DECO | LDST_SRCDST_WORD_DECO_MATH2;
+	len = sizeof(data) << LDST_LEN_SHIFT;
+	append_load_as_imm(desc, &data, len, opt);
+
+	off_w = 7;
+	append_jump(desc, (off_w << JUMP_OFFSET_SHIFT));
+
+	/* jump: all-match[] always-jump offset=0 local->[00] */
+	append_jump(desc, (0 << JUMP_OFFSET_SHIFT));
+
+	/* jump: all-match[] always-jump offset=0 local->[00] */
+	append_jump(desc, (0 << JUMP_OFFSET_SHIFT));
+
+	data = 0x00ff0000;
+	append_math_and_imm_u64(desc, REG2, DPOVRD, IMM, data);
+
+	dma_unmap_single(sa->dpa_ipsec->jrdev, dma_extra_cmds,
+			 extra_cmds_len * sizeof(uint32_t), DMA_TO_DEVICE);
+
+	return 0;
+}
+
+
+int create_sec_descriptor(struct dpa_ipsec_sa *sa)
+{
+	struct sec_descriptor *sec_desc;
+	struct device *jrdev;
+	dma_addr_t auth_key_dma;
+	dma_addr_t crypto_key_dma;
+	int ret = 0;
+
+	/* get the jr device  */
+	jrdev = get_jrdev(sa->dpa_ipsec);
+	if (!jrdev) {
+		pr_err("Failed to get the job ring device, check the dts\n");
+		return -EINVAL;
+	}
+
+	/* check whether a split of a normal key is used */
+	if (sa->auth_data.split_key_len)
+		auth_key_dma = dma_map_single(jrdev, sa->auth_data.split_key,
+					      sa->auth_data.split_key_pad_len,
+					      DMA_TO_DEVICE);
+	else
+		auth_key_dma = dma_map_single(jrdev, sa->auth_data.auth_key,
+					      sa->auth_data.auth_key_len,
+					      DMA_TO_DEVICE);
+	if (!auth_key_dma) {
+		pr_err("Could not DMA map authentication key\n");
+		return -EINVAL;
+	}
+
+	crypto_key_dma = dma_map_single(jrdev, sa->cipher_data.cipher_key,
+					sa->cipher_data.cipher_key_len,
+					DMA_TO_DEVICE);
+	if (!crypto_key_dma) {
+		pr_err("Could not DMA map cipher key\n");
+		return -EINVAL;
+	}
+
+	/*
+	 * Build the shared descriptor and see if its length is less than
+	 * 64 words. If build_shared_descriptor returns -EPERM than it is
+	 * required to build the extended shared descriptor in order to have
+	 * all the SA features that were required.
+	 */
+	ret = build_shared_descriptor(sa, auth_key_dma, crypto_key_dma,
+				      sa->l2_hdr_size);
+	switch (ret) {
+	case 0:
+		sa->sec_desc_extended = false;
+		goto done_shared_desc;
+	case -EPERM:
+		sa->sec_desc_extended = true;
+		goto build_extended_shared_desc;
+	default:
+		pr_err("Failed to create SEC descriptor for SA %d\n", sa->id);
+		return -EFAULT;
+	}
+
+build_extended_shared_desc:
+	/* Build the extended shared descriptor */
+	if (sa->sa_dir == DPA_IPSEC_INBOUND)
+		ret = build_extended_decap_shared_descriptor(sa, auth_key_dma,
+				crypto_key_dma, sa->l2_hdr_size, 64,
+				sa->dpa_ipsec->sec_era);
+	else
+		ret = build_extended_encap_shared_descriptor(sa, auth_key_dma,
+				crypto_key_dma, sa->l2_hdr_size,
+				sa->dpa_ipsec->sec_era);
+	if (ret < 0) {
+		pr_err("Failed to create SEC descriptor for SA %d\n", sa->id);
+		return -EFAULT;
+	}
+
+done_shared_desc:
+	sec_desc = sa->sec_desc;
+	/* setup preheader */
+	sec_desc->preheader.hi.field.idlen = desc_len((u32 *) sec_desc->desc);
+	sec_desc->preheader.lo.field.pool_id = sa->sa_bpid;
+	sec_desc->preheader.lo.field.pool_buffer_size = 0;
+	sec_desc->preheader.lo.field.offset =
+		(sa->sa_dir == DPA_IPSEC_INBOUND) ?
+			sa->dpa_ipsec->config.post_sec_in_params.data_off :
+			sa->dpa_ipsec->config.post_sec_out_params.data_off;
+
+	dma_unmap_single(jrdev, auth_key_dma,
+			 sa->auth_data.split_key_pad_len, DMA_TO_DEVICE);
+	dma_unmap_single(jrdev, crypto_key_dma,
+			 sa->cipher_data.cipher_key_len, DMA_TO_DEVICE);
+	return 0;
+}
+
+/*
+ * Create descriptor for updating the anti replay window size
+ * [21] B0951A17       jobhdr: shrsz=21 shr share=serial reo len=23
+ * [22] 00000000               sharedesc->@0x02abb5008
+ * [23] 2ABB5008
+ * [24] 79340008         move: descbuf+0[00] -> math0, len=8 wait
+ * [25] A82CC108         math: (0 - 1)->math1 len=8
+ * [26] AC214108         math: (math1 - imm1)->math1 len=8 ifb
+ * [27] 000000C0               imm1=192
+ * [28] A8501008         math: (math0 & math1)->math0 len=8
+ * [29] 1640180A           ld: deco-descbuf len=10 offs=24
+ * [30] 00000000               ptr->@0x02abb5434
+ * [31] 2ABB5434
+ * [32] A1001001         jump: jsl1 all-match[calm] offset=1 local->[33]
+ * [33] A00000F7         jump: all-match[] always-jump offset=-9 local->[24]
+ * [34] AC404008         math: (math0 | imm1)->math0 len=8 ifb
+ * [35] 000000C0               imm1=192
+ * [36] 79430008         move: math0 -> descbuf+0[00], len=8 wait
+ * [37] 79631804         move: math2 -> descbuf+24[06], len=4 wait
+ * [38] 56420107          str: deco-shrdesc+1 len=7
+ * [39] A8034304         math: (math3 + imm1)->math3 len=4
+ * [40] 00000000               imm1=0
+ * [41] 78720008         move: math3+0 -> ofifo, len=8
+ * [42] 68300008	 seqfifostr: msgdata len=8
+ * [43] A1C01002         jump: jsl1 all-match[calm] halt-user status=2
+ */
+int build_rjob_desc_ars_update(struct dpa_ipsec_sa *sa, enum dpa_ipsec_arw arw)
+{
+	uint32_t *desc, *rjobd, off;
+	uint8_t options;
+	enum dpa_ipsec_arw c_arw;
+	size_t ars_off;
+	dma_addr_t dma_shdesc;
+
+	/* Check input parameters */
+	BUG_ON(!sa);
+	if (sa->sa_dir != DPA_IPSEC_INBOUND) {
+		pr_err("ARS update not supported for outbound SA %d\n", sa->id);
+		return -EINVAL;
+	}
+
+	BUG_ON(!sa->sec_desc);
+	desc = (uint32_t *)sa->sec_desc->desc;
+	options = (uint8_t)(*(desc + 1) & 0x000000FF);
+	c_arw = options >> 6;
+	if (c_arw == arw) {
+		pr_err("SA %d has already set this ARS %d\n", sa->id, arw);
+		return -EALREADY;
+	}
+
+	/* Get DMA address for this SA shared descriptor */
+	dma_shdesc = dma_map_single(sa->dpa_ipsec->jrdev, sa->sec_desc->desc,
+				    desc_len(sa->sec_desc->desc) * sizeof(u32),
+				    DMA_BIDIRECTIONAL);
+	if (!dma_shdesc) {
+		pr_err("Failed DMA map shared descriptor for SA %d\n", sa->id);
+		return -ENXIO;
+	}
+
+	/* Create replacement job descriptor for ARS update */
+	BUG_ON(!sa->rjob_desc);
+	rjobd = sa->rjob_desc;
+
+	init_job_desc(rjobd, HDR_SHARE_SERIAL | HDR_SHARED | HDR_REVERSE |
+		      (desc_len(sa->sec_desc->desc) << HDR_START_IDX_SHIFT));
+
+	/* Set DMA address of the shared descriptor */
+	append_ptr(rjobd, dma_shdesc);
+
+	/* Retrieve header and options from PDB in MATH 0 */
+	append_move(rjobd, MOVE_SRC_DESCBUF | MOVE_DEST_MATH0 | MOVE_WAITCOMP |
+		    (0 << MOVE_OFFSET_SHIFT) | sizeof(u64));
+
+	/* MATH_REG1 = 0xFFFFFFFF_FFFFFFFF */
+	append_math_sub(rjobd, REG1, ZERO, ONE, MATH_LEN_8BYTE);
+
+	/* MATH_REG1 = 0xFFFFFFFF_FFFFFF3F */
+	append_math_sub_imm_u64(rjobd, REG1, REG1, IMM, 0xC0);
+
+	/* Reset ARS bits */
+	append_math_and(rjobd, REG0, REG0, REG1, MATH_LEN_8BYTE);
+
+	/*
+	 * Overwrite RJD immediately after the SHD pointer i.e shared descriptor
+	 * length plus 1 plus another 3 words
+	 * Offset and length are expressed in words
+	 * 3w - RJD header + SHD pointer
+	 * 5w - five instructions for doing some part of ARS modification
+	 * 3w - load instruction + pointer
+	 * 1w - jump calm
+	 * 1w - jump back to the remaining descriptor
+	 */
+	append_load(rjobd, virt_to_phys((void *)(rjobd + 3 + 5 + 3 + 1 + 1)),
+		    10, LDST_CLASS_DECO | LDST_SRCDST_WORD_DESCBUF |
+		    ((desc_len(sa->sec_desc->desc) + 3) << LDST_OFFSET_SHIFT));
+
+	/* wait for completion o previous operation */
+	append_jump(rjobd, JUMP_COND_CALM | (1 << JUMP_OFFSET_SHIFT));
+
+	/* jump back to remaining descriptor i.e jump back 9 words */
+	off = (-9) & 0x000000FF;
+	append_jump(rjobd, (off << JUMP_OFFSET_SHIFT));
+
+	/* Convert PDB ARS to new size */
+	switch (arw) {
+	case DPA_IPSEC_ARSNONE:
+		/*
+		 * nothing to do because previous command reseted ARS bits
+		 * add 2 NOPs to conserve descriptor size
+		 */
+		append_cmd(rjobd, 0xA0000001); /* NOP for SEC */
+		append_cmd(rjobd, 0xA0000001); /* NOP for SEC */
+		break;
+	case DPA_IPSEC_ARS32:
+		append_math_or_imm_u64(rjobd, REG0, REG0, IMM,
+				       PDBOPTS_ESP_ARS32);
+		break;
+	case DPA_IPSEC_ARS64:
+		append_math_or_imm_u64(rjobd, REG0, REG0, IMM,
+				       PDBOPTS_ESP_ARS64);
+		break;
+	default:
+		pr_err("Invalid ARS\n");
+		BUG();
+	}
+
+	/* Put header and options back to PDB */
+	append_move(rjobd, MOVE_SRC_MATH0 | MOVE_DEST_DESCBUF | MOVE_WAITCOMP |
+		    (0 << MOVE_OFFSET_SHIFT) | sizeof(u64));
+
+	/*
+	 * anti_replay[0] - used for 32ARS - LS bit represent the frame with
+	 * highest SEQ number that has been successfully authenticated so far
+	 * i.e the frame that had SEQ/ESN from PDB seq_num/seq_num_ext_hi
+	 *
+	 * anti_replay[1] - used when 64ARS is configured - LS bit represent
+	 * a frame with a immediate older SEQ number than the MS bit of the
+	 * anti_replay[0] i.e
+	 * SEQ(LS bit of anti_replay[1]) = SEQ(MS bit of anti_replay[0]) - 1;
+	 *
+	 * always reset to 0 all bits from anti_replay[1]
+	 * reset to 0 all bits from anti_replay[0] only if updating from ARS to
+	 * no ARS.
+	 * MOVE_SRC_MATH2 was not used until now i.e has value 0
+	 */
+	ars_off = offsetof(struct ipsec_decap_pdb, anti_replay);
+	if (arw == DPA_IPSEC_ARSNONE)
+		append_move(rjobd, MOVE_SRC_MATH2 | MOVE_DEST_DESCBUF |
+			    MOVE_WAITCOMP | (ars_off << MOVE_OFFSET_SHIFT) |
+			    sizeof(u64));
+	else
+		append_move(rjobd, MOVE_SRC_MATH2 | MOVE_DEST_DESCBUF |
+			    MOVE_WAITCOMP |
+			    ((ars_off + 4) << MOVE_OFFSET_SHIFT) | sizeof(u32));
+
+	/*
+	 * Update shared descriptor in memory - only PDB
+	 * special case - offset and length are in words
+	 */
+	append_store(rjobd, 0, sizeof(struct ipsec_decap_pdb) / sizeof(u32),
+		     LDST_CLASS_DECO | (1 << LDST_OFFSET_SHIFT) |
+		     LDST_SRCDST_WORD_DESCBUF_SHARED);
+
+	append_math_add_imm_u32(rjobd, REG3, REG3, IMM, sa->id);
+
+	/* move: ififo->deco-alnblk -> ofifo, len=4 */
+	append_move(rjobd, MOVE_SRC_MATH3 | MOVE_DEST_OUTFIFO | 8);
+
+	/* seqfifostr: msgdata len=4 */
+	append_seq_fifo_store(rjobd, FIFOST_TYPE_MESSAGE_DATA, 8);
+
+	/*
+	 * Exit replacement job descriptor, halt with user error
+	 * FD status will be a special user error, generated only on request by
+	 * a descriptor command (not by any other circumstance) i.e no confusing
+	 * this frame for any other error. Jump with CALM to be sure previous
+	 * operation was finished
+	 */
+	append_cmd(rjobd, 0xA1C01002);
+
+	dma_unmap_single(sa->dpa_ipsec->jrdev, dma_shdesc,
+			 desc_len(sa->sec_desc->desc) * sizeof(u32),
+			 DMA_BIDIRECTIONAL);
+
+	return 0;
+}
+
+static void split_key_done(struct device *dev, u32 * desc, u32 err,
+			   void *context)
+{
+	register atomic_t *done = context;
+
+	if (err) {
+		char tmp[256];
+		dev_err(dev, "%s\n", caam_jr_strstatus(tmp, err));
+	}
+	atomic_set(done, 1);
+}
+
+/* determine the HASH algorithm and the coresponding split key length */
+int get_split_key_info(struct auth_params *auth_param, u32 *hmac_alg)
+{
+	/*
+	 * Sizes for MDHA pads (*not* keys): MD5, SHA1, 224, 256, 384, 512
+	 * Running digest size
+	 */
+	const u8 mdpadlen[] = {16, 20, 32, 32, 64, 64};
+
+	switch (auth_param->auth_type) {
+	case OP_PCL_IPSEC_HMAC_MD5_96:
+	case OP_PCL_IPSEC_HMAC_MD5_128:
+		*hmac_alg = OP_ALG_ALGSEL_MD5;
+		break;
+	case OP_PCL_IPSEC_HMAC_SHA1_96:
+	case OP_PCL_IPSEC_HMAC_SHA1_160:
+		*hmac_alg = OP_ALG_ALGSEL_SHA1;
+		break;
+	case OP_PCL_IPSEC_HMAC_SHA2_256_128:
+		*hmac_alg = OP_ALG_ALGSEL_SHA256;
+		break;
+	case OP_PCL_IPSEC_HMAC_SHA2_384_192:
+		*hmac_alg = OP_ALG_ALGSEL_SHA384;
+		break;
+	case OP_PCL_IPSEC_HMAC_SHA2_512_256:
+		*hmac_alg = OP_ALG_ALGSEL_SHA512;
+		break;
+	case OP_PCL_IPSEC_AES_XCBC_MAC_96:
+		*hmac_alg = 0;
+		auth_param->split_key_len = 0;
+		break;
+	default:
+		pr_err("Unsupported authentication algorithm\n");
+		return -EINVAL;
+	}
+
+	if (*hmac_alg)
+		auth_param->split_key_len =
+				mdpadlen[(*hmac_alg & OP_ALG_ALGSEL_SUBMASK) >>
+					 OP_ALG_ALGSEL_SHIFT] * 2;
+
+	return 0;
+}
+
+int generate_split_key(struct auth_params *auth_param)
+{
+	struct device *jrdev;
+	dma_addr_t dma_addr_in, dma_addr_out;
+	u32 *desc, timeout = 1000000, alg_sel = 0;
+	struct dpa_ipsec_sa *sa;
+	atomic_t done;
+	int ret = 0;
+
+	sa = container_of(auth_param, struct dpa_ipsec_sa, auth_data);
+	BUG_ON(!sa->dpa_ipsec);
+
+	ret = get_split_key_info(auth_param, &alg_sel);
+	/* exit if error or there is no need to compute a split key */
+	if (ret < 0 || alg_sel == 0)
+		return ret;
+
+	jrdev = get_jrdev(sa->dpa_ipsec);
+	if (!jrdev) {
+		pr_err("Could not get job ring device, please check dts\n");
+		return -ENODEV;
+	}
+
+	desc = kmalloc(CAAM_CMD_SZ * 6 + CAAM_PTR_SZ * 2, GFP_KERNEL | GFP_DMA);
+	if (!desc) {
+		pr_err("Allocate memory failed for split key desc\n");
+		return -ENOMEM;
+	}
+
+	auth_param->split_key_pad_len = ALIGN(auth_param->split_key_len, 16);
+
+	dma_addr_in = dma_map_single(jrdev, auth_param->auth_key,
+				     auth_param->auth_key_len, DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, dma_addr_in)) {
+		dev_err(jrdev, "Unable to DMA map the input key address\n");
+		kfree(desc);
+		return -ENOMEM;
+	}
+
+	dma_addr_out = dma_map_single(jrdev, auth_param->split_key,
+				      auth_param->split_key_pad_len,
+				      DMA_FROM_DEVICE);
+	if (dma_mapping_error(jrdev, dma_addr_out)) {
+		dev_err(jrdev, "Unable to DMA map the output key address\n");
+		dma_unmap_single(jrdev, dma_addr_in, auth_param->auth_key_len,
+				 DMA_TO_DEVICE);
+		kfree(desc);
+		return -ENOMEM;
+	}
+
+	init_job_desc(desc, 0);
+
+	append_key(desc, dma_addr_in, auth_param->auth_key_len,
+		   CLASS_2 | KEY_DEST_CLASS_REG);
+
+	/* Sets MDHA up into an HMAC-INIT */
+	append_operation(desc, (OP_ALG_TYPE_CLASS2 << OP_ALG_TYPE_SHIFT) |
+			 alg_sel | OP_ALG_AAI_HMAC |
+			OP_ALG_DECRYPT | OP_ALG_AS_INIT);
+
+	/* Do a FIFO_LOAD of zero, this will trigger the internal key expansion
+	   into both pads inside MDHA */
+	append_fifo_load_as_imm(desc, NULL, 0, LDST_CLASS_2_CCB |
+				FIFOLD_TYPE_MSG | FIFOLD_TYPE_LAST2);
+
+	/* FIFO_STORE with the explicit split-key content store
+	 * (0x26 output type) */
+	append_fifo_store(desc, dma_addr_out, auth_param->split_key_len,
+			  LDST_CLASS_2_CCB | FIFOST_TYPE_SPLIT_KEK);
+
+	atomic_set(&done, 0);
+	ret = caam_jr_enqueue(jrdev, desc, split_key_done, &done);
+
+	while (!atomic_read(&done) && --timeout) {
+		udelay(1);
+		cpu_relax();
+	}
+
+	if (timeout == 0)
+		pr_err("Timeout waiting for job ring to complete\n");
+
+	dma_unmap_single(jrdev, dma_addr_out, auth_param->split_key_pad_len,
+			 DMA_FROM_DEVICE);
+	dma_unmap_single(jrdev, dma_addr_in, auth_param->auth_key_len,
+			 DMA_TO_DEVICE);
+	kfree(desc);
+	return ret;
+}
diff --git a/drivers/staging/fsl_dpa_offload/dpa_ipsec_desc.h b/drivers/staging/fsl_dpa_offload/dpa_ipsec_desc.h
new file mode 100644
index 0000000..16edff4
--- /dev/null
+++ b/drivers/staging/fsl_dpa_offload/dpa_ipsec_desc.h
@@ -0,0 +1,95 @@
+/* Copyright 2008-2012 Freescale Semiconductor, Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _DPA_IPSEC_DESC_H_
+#define _DPA_IPSEC_DESC_H_
+
+#include "pdb.h"
+#include "dpa_ipsec.h"
+#include "desc_constr.h"
+
+#define DPA_IPSEC_STATS_LEN	2	/* length in words */
+#define MAX_CAAM_SHARED_DESCSIZE 50	/* If CAAM used with QI the maximum
+					 * shared descriptor length is 50 words
+					 */
+
+/* preheader */
+struct preheader {
+	union {
+		uint32_t word;
+		struct {
+			uint16_t rsvd63_48;
+			unsigned int rsvd47_39:9;
+			unsigned int idlen:7;
+		} field;
+	} __packed hi;
+
+	union {
+		uint32_t word;
+		struct {
+			unsigned int rsvd31_30:2;
+			unsigned int fsgt:1;
+			unsigned int lng:1;
+			unsigned int offset:2;
+			unsigned int abs:1;
+			unsigned int add_buf:1;
+			uint8_t pool_id;
+			uint16_t pool_buffer_size;
+		} field;
+	} __packed lo;
+} __packed;
+
+struct desc_hdr {
+	uint32_t hdr_word;
+	union {
+		struct ipsec_encap_pdb pdb_en;
+		struct ipsec_decap_pdb pdb_dec;
+	};
+};
+
+struct sec_descriptor {
+	struct preheader preheader;	/* SEC preheader */
+	/* SEC Shared Descriptor */
+	union {
+		uint32_t desc[MAX_CAAM_DESCSIZE];
+		struct desc_hdr desc_hdr;
+#define hdr_word	desc_hdr.hdr_word
+#define pdb_en		desc_hdr.pdb_en
+#define pdb_dec		desc_hdr.pdb_dec
+	};
+};
+
+int get_sec_info(struct dpa_ipsec *dpa_ipsec);
+int create_sec_descriptor(struct dpa_ipsec_sa *sa);
+int generate_split_key(struct auth_params *auth_param);
+int build_rjob_desc_ars_update(struct dpa_ipsec_sa *sa, enum dpa_ipsec_arw arw);
+
+#endif	/* _DPA_IPSEC_DESC_H_ */
diff --git a/include/linux/fsl_dpa_ipsec.h b/include/linux/fsl_dpa_ipsec.h
new file mode 100644
index 0000000..e5ac6aa
--- /dev/null
+++ b/include/linux/fsl_dpa_ipsec.h
@@ -0,0 +1,516 @@
+/* Copyright 2008-2012 Freescale Semiconductor, Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+ * DPA-IPSec Application Programming Interface.
+ */
+
+#ifndef __FSL_DPA_IPSEC_H
+#define __FSL_DPA_IPSEC_H
+
+#include "fsl_dpa_classifier.h"
+
+/* General DPA-IPSec defines */
+#define IP_PROTO_FIELD_LEN		1
+#define ESP_SPI_FIELD_LEN		4
+#define PORT_FIELD_LEN			2
+#define ICMP_HDR_FIELD_LEN		1
+
+#define MAX_SIZE_IP_UDP_SPI_KEY	\
+		(1 * DPA_OFFLD_IPv6_ADDR_LEN_BYTES + \
+		IP_PROTO_FIELD_LEN + \
+		2 * PORT_FIELD_LEN + \
+		ESP_SPI_FIELD_LEN)
+
+#define MAX_SIZE_IP_UDP_SPI_KEY_IPV4 \
+		(1 * DPA_OFFLD_IPv4_ADDR_LEN_BYTES + \
+		IP_PROTO_FIELD_LEN + \
+		2 * PORT_FIELD_LEN + \
+		ESP_SPI_FIELD_LEN)
+
+#define MAX_SIZE_POLICY_KEY \
+		(2 * DPA_OFFLD_IPv6_ADDR_LEN_BYTES + \
+		IP_PROTO_FIELD_LEN + \
+		2 * PORT_FIELD_LEN)
+
+#define MAX_SIZE_POLICY_KEY_IPV4 \
+		(2 * DPA_OFFLD_IPv4_ADDR_LEN_BYTES + \
+		IP_PROTO_FIELD_LEN + \
+		2 * PORT_FIELD_LEN)
+
+#define DPA_IPSEC_MAX_IN_POL_PER_SA  255  /* Maximum supported number of
+					   * inbound policies per SA	      */
+
+/*
+ * IPSec Special Operations
+ */
+#define DPA_IPSEC_HDR_COPY_TOS		0x01 /* Copy TOS / DiffServ byte from
+					      * inner / outer header to outer /
+					      * inner header		      */
+#define	DPA_IPSEC_HDR_COPY_DF		0x02 /* Copy DF bit from outer header
+					      * to outer / inner header	      */
+#define DPA_IPSEC_HDR_DEC_TTL		0x04 /* Automatically decrment the TTL
+					      * value in the inner / outer hdr*/
+#define DPA_IPSEC_HDR_COPY_DSCP		0x08 /* Copy DSCP bits from inner /
+					      * outer header to outer / inner
+					      * header			      */
+#define DPA_IPSEC_HDR_COPY_ECN		0x10 /* Copy ECN bits from inner /
+					      * outer header to outer / inner
+					      * header			      */
+
+#define DPA_IPSEC_KEY_FIELD_SIP		0x01 /* Use source IP address in key  */
+#define DPA_IPSEC_KEY_FIELD_DIP		0x02 /* Use destination IP in key     */
+#define	DPA_IPSEC_KEY_FIELD_PROTO	0x04 /* Use IP protocol field in key  */
+#define DPA_IPSEC_KEY_FIELD_SPORT	0x08 /* Use source port in key        */
+#define DPA_IPSEC_KEY_FIELD_ICMP_TYPE	0x08 /* Use ICMP type field in key    */
+#define DPA_IPSEC_KEY_FIELD_DPORT	0x10 /* Use destination port in key   */
+#define DPA_IPSEC_KEY_FIELD_ICMP_CODE	0x10 /* Use ICMP code field in key    */
+#define	DPA_IPSEC_MAX_KEY_FIELDS	5    /* Maximum key components        */
+
+#define DPA_IPSEC_DEF_PAD_VAL		0xAA /* Value to be used as padding in
+					      * classification keys           */
+
+/* DPA-IPSec Supported Protocols (for policy offloading) */
+enum dpa_ipsec_proto {
+	DPA_IPSEC_PROTO_TCP_IPV4 = 0,
+	DPA_IPSEC_PROTO_TCP_IPV6,
+	DPA_IPSEC_PROTO_UDP_IPV4,
+	DPA_IPSEC_PROTO_UDP_IPV6,
+	DPA_IPSEC_PROTO_ICMP_IPV4,
+	DPA_IPSEC_PROTO_ICMP_IPV6,
+	DPA_IPSEC_PROTO_SCTP_IPV4,
+	DPA_IPSEC_PROTO_SCTP_IPV6,
+	DPA_IPSEC_PROTO_ANY_IPV4,
+	DPA_IPSEC_PROTO_ANY_IPV6,
+	DPA_IPSEC_MAX_SUPPORTED_PROTOS
+};
+
+/* DPA IPSec supported types of SAs */
+enum dpa_ipsec_sa_type {
+	DPA_IPSEC_SA_IPV4 = 0,
+	DPA_IPSEC_SA_IPV4_NATT,
+	DPA_IPSEC_SA_IPV6,
+	DPA_IPSEC_MAX_SA_TYPE
+};
+
+/*
+ * DPA-IPSec Post SEC Data Offsets. 1 BURST = 32 or 64 bytes
+ * depending on SEC configuration. Default BURST size = 64 bytes
+ */
+enum dpa_ipsec_data_off {
+	DPA_IPSEC_DATA_OFF_NONE = 0,
+	DPA_IPSEC_DATA_OFF_1_BURST,
+	DPA_IPSEC_DATA_OFF_2_BURST,
+	DPA_IPSEC_DATA_OFF_3_BURST
+};
+
+/* DPA IPSec outbound policy lookup table parameters */
+struct dpa_ipsec_pol_table {
+	int	dpa_cls_td; /* DPA Classifier table descriptor		      */
+	uint8_t	key_fields; /* Flags indicating policy key components.
+			     * (use DPA_IPSEC_KEY_FIELD* macros to configure) */
+};
+
+/* DPA-IPSec Pre-Sec Inbound Parameters */
+struct dpa_ipsec_pre_sec_in_params {
+	int dpa_cls_td[DPA_IPSEC_MAX_SA_TYPE]; /* SA lookup tables descriptors*/
+};
+
+/* DPA-IPSec Pre-Sec Outbound Parameters */
+struct dpa_ipsec_pre_sec_out_params {
+	/* Oubound policy lookup tables parameters */
+	struct dpa_ipsec_pol_table table[DPA_IPSEC_MAX_SUPPORTED_PROTOS];
+};
+
+/* DPA-IPSec Post-Sec-Inbound Parameters */
+struct dpa_ipsec_post_sec_in_params {
+	enum dpa_ipsec_data_off data_off;/*Data offset in the decrypted buffer*/
+	uint16_t qm_tx_ch;   /* QMan channel of the post decryption OH port   */
+	int dpa_cls_td;	     /* Index table descriptor			      */
+	bool do_pol_check;   /* Enable inbound policy verification	      */
+	uint8_t key_fields;  /* Flags indicating policy key components.
+			      * (use DPA_IPSEC_KEY_FIELD* macros to configure)
+			      *  Relevant only if do_pol_check = TRUE	      */
+	bool use_ipv6_pol;   /* Activate support for IPv6 policies. Allows
+			      * better MURAM management. Relevant only if
+			      * do_pol_check = TRUE			      */
+	uint16_t base_flow_id; /* The start value of the range of flow ID values
+				* used by this instance in post decryption    */
+};
+
+/* DPA-IPSec Post-Sec-Inbound Parameters */
+struct dpa_ipsec_post_sec_out_params {
+	enum dpa_ipsec_data_off data_off;/*Data offset in the decrypted buffer*/
+	uint16_t qm_tx_ch; /* QMan channel of the post encrytion OH port      */
+};
+
+/* DPA IPSec FQID range parameters */
+struct dpa_ipsec_fqid_range {
+	uint32_t	start_fqid;
+	uint32_t	end_fqid;
+};
+
+/* IPsec parameters used to configure the DPA IPsec instance */
+struct dpa_ipsec_params {
+	struct dpa_ipsec_pre_sec_in_params pre_sec_in_params;
+	struct dpa_ipsec_post_sec_in_params post_sec_in_params;
+	struct dpa_ipsec_pre_sec_out_params pre_sec_out_params;
+	struct dpa_ipsec_post_sec_out_params post_sec_out_params;
+	void *fm_pcd;		/* Handle of the PCD object		      */
+	uint16_t qm_sec_ch;	/* QMan channel# for the SEC		      */
+	uint16_t max_sa_pairs;	/* Maximum number of SA pairs
+				 * (1 SA Pair = 1 In SA + 1 Out SA)	      */
+
+	/*
+	 * Maximum number of special IPSec
+	 * manipulation operations that can be
+	 * enabled. eg DSCP/ECN update, IP variable
+	 * length. The max_sa_manip_ops
+	 * should be incremented with the number
+	 * of manipulations per every outbound
+	 * policy
+	 */
+	uint32_t max_sa_manip_ops;
+	struct dpa_ipsec_fqid_range *fqid_range; /* FQID range to be used by
+						  * DPA IPSec for allocating
+						  * FQIDs for internal FQs    */
+	uint8_t ipf_bpid;	/* Scratch buffer pool for IP Frag.	      */
+};
+
+/* Initialize a DPA-IPSec instance. */
+int dpa_ipsec_init(const struct dpa_ipsec_params *params, int *dpa_ipsec_id);
+
+/* Free a DPA-IPSec instance */
+int dpa_ipsec_free(int dpa_ipsec_id);
+
+/* DPA-IPSec data flow source specification */
+enum dpa_ipsec_direction {
+	DPA_IPSEC_INBOUND = 0,	/* Inbound				      */
+	DPA_IPSEC_OUTBOUND	/* Outbound				      */
+};
+
+/* DPA-IPSec Supported Cipher Suites */
+enum dpa_ipsec_cipher_alg {
+	DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_96_MD5_128,
+	DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_96_SHA_160,
+	DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_MD5_128,
+	DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_SHA_160,
+	DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_SHA_256_128,
+	DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_SHA_384_192,
+	DPA_IPSEC_CIPHER_ALG_3DES_CBC_HMAC_SHA_512_256,
+	DPA_IPSEC_CIPHER_ALG_NULL_ENC_HMAC_96_MD5_128,
+	DPA_IPSEC_CIPHER_ALG_NULL_ENC_HMAC_96_SHA_160,
+	DPA_IPSEC_CIPHER_ALG_NULL_ENC_AES_XCBC_MAC_96,
+	DPA_IPSEC_CIPHER_ALG_NULL_ENC_HMAC_MD5_128,
+	DPA_IPSEC_CIPHER_ALG_NULL_ENC_HMAC_SHA_160,
+	DPA_IPSEC_CIPHER_ALG_NULL_ENC_HMAC_SHA_256_128,
+	DPA_IPSEC_CIPHER_ALG_NULL_ENC_HMAC_SHA_384_192,
+	DPA_IPSEC_CIPHER_ALG_NULL_ENC_HMAC_SHA_512_256,
+	DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_96_MD5_128,
+	DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_96_SHA_160,
+	DPA_IPSEC_CIPHER_ALG_AES_CBC_AES_XCBC_MAC_96,
+	DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_MD5_128,
+	DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_SHA_160,
+	DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_SHA_256_128,
+	DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_SHA_384_192,
+	DPA_IPSEC_CIPHER_ALG_AES_CBC_HMAC_SHA_512_256,
+	DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_96_MD5_128,
+	DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_96_SHA_160,
+	DPA_IPSEC_CIPHER_ALG_AES_CTR_AES_XCBC_MAC_96,
+	DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_MD5_128,
+	DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_SHA_160,
+	DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_SHA_256_128,
+	DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_SHA_384_192,
+	DPA_IPSEC_CIPHER_ALG_AES_CTR_HMAC_SHA_512_256
+};
+
+/* DPA-IPSec Initialization Vector */
+struct dpa_ipsec_init_vector {
+	uint8_t *init_vector;	/* Pointer to the initialization vector	      */
+	uint8_t length;		/* Length in bytes. May be 8 or 16 bytes      */
+};
+
+/* DPA IPSEC Anti Replay Window Size */
+enum dpa_ipsec_arw {
+	DPA_IPSEC_ARSNONE = 0,	/* No Anti Replay Protection		      */
+	DPA_IPSEC_ARS32   = 1,	/* 32 bit Anti Replay Window size	      */
+	DPA_IPSEC_ARS64   = 3,	/* 64 bit Anti Replay Window size	      */
+};
+
+/* DPA-IPSec Security Association Cryptographic Parameters */
+struct dpa_ipsec_sa_crypto_params {
+	enum dpa_ipsec_cipher_alg alg_suite;	/* Algorithm suite specifying
+						 * encryption and authentication
+						 * algorithms to be used      */
+	uint8_t *cipher_key;	/* Address of the encryption key	      */
+	uint8_t cipher_key_len;	/* Length of the encryption key in bytes      */
+	uint8_t *auth_key;	/* Address of the authentication key	      */
+	uint8_t auth_key_len;	/* Length of the authentication key in bytes  */
+};
+
+/* DPA-IPSec SA Modes */
+enum dpa_ipsec_sa_mode {
+	DPA_IPSEC_SA_MODE_TUNNEL = 0,
+	DPA_IPSEC_SA_MODE_TRANSPORT
+};
+
+/* DPA-IPSec SA Protocols */
+enum dpa_ipsec_sa_proto {
+	DPA_IPSEC_SA_PROTO_ESP = 0,
+	DPA_IPSEC_SA_PROTO_AH
+};
+
+/* DPA-IPSec Security Association Out Parameters */
+struct dpa_ipsec_sa_out_params {
+	struct dpa_ipsec_init_vector *init_vector; /* Initialization vector
+						    * (IV). Null for using the
+						    * internal random number
+						    * generator               */
+	unsigned int ip_ver;	/* IPv4 or IPv6 address type		      */
+	uint16_t ip_hdr_size;	/* IP header size including any IP options    */
+	void *outer_ip_header;	/* IP encapsulation header		      */
+	void *outer_udp_header;	/* UDP encapsulation header
+				 * (for SAs using NAT-T)		      */
+	uint16_t post_sec_flow_id;  /* Flow ID used to mark frames encrypted
+				     * using this SA                          */
+};
+
+/* DPA-IPSec Security Association In Parameters */
+struct dpa_ipsec_sa_in_params {
+	enum dpa_ipsec_arw arw;	/* Anti replay window			      */
+	bool use_var_iphdr_len; /* Enable variable IP header length support   */
+	struct dpa_offload_ip_address src_addr;	/* Source IP address	      */
+	struct dpa_offload_ip_address dest_addr; /* Destination IP address    */
+	bool use_udp_encap;	/* NAT-T is activated (UDP encapsulated ESP)  */
+	uint16_t src_port;	/* Source UDP port (UDP encapsulated ESP)     */
+	uint16_t dest_port;	/* Destination UDP port (UDP encapsulated ESP)*/
+	struct dpa_cls_tbl_action policy_miss_action; /* Action for frames that
+						       * fail inbound policy
+						       * verification         */
+	struct dpa_cls_tbl_action post_ipsec_action; /* Action to be performed
+						      * on the frames after
+						      * inbound IPSec processing
+						      * is completed          */
+};
+
+/* DPA-IPSec Security Association Parameters */
+struct dpa_ipsec_sa_params {
+	uint32_t spi;		/* IPSec Security parameter index	      */
+	bool use_ext_seq_num;	/* Enable extended sequence number	      */
+	uint64_t start_seq_num;	/* Sequence number to start with	      */
+	uint32_t l2_hdr_size;	/* Size of the Ethernet header, including any
+				 * VLAN information.			      */
+	enum dpa_ipsec_sa_mode sa_mode;	/* Tunnel or transport mode selection */
+	enum dpa_ipsec_sa_proto sa_proto; /* Protocol to be used (AH or ESP)-
+					   * Only ESP supported currently     */
+	uint8_t hdr_upd_flags;	/* Flags for propagating information from inner
+				 * to outer header and vice versa	      */
+	uint8_t sa_wqid;	/* Work queue Id for all the queues in this SA*/
+	uint8_t sa_bpid;	/* Buffer Pool ID to be used with this SA     */
+	bool	enable_stats;	/* Enable counting packets and bytes processed*/
+	struct dpa_ipsec_sa_crypto_params crypto_params;/* IPSec crypto params*/
+	enum dpa_ipsec_direction sa_dir;  /* SA direction: Outbound/Inbound   */
+	union {
+		struct dpa_ipsec_sa_in_params sa_in_params; /* Inb SA params  */
+		struct dpa_ipsec_sa_out_params sa_out_params; /* Out SA params*/
+	};
+};
+
+/* DPA-IPSEC Rekeying error callback */
+typedef int (*dpa_ipsec_rekey_event_cb) (int dpa_ipsec_id, int sa_id,
+					 int error);
+
+/* Offload an SA. */
+int dpa_ipsec_create_sa(int dpa_ipsec_id,
+			struct dpa_ipsec_sa_params *sa_params, int *sa_id);
+
+/* This function will be used when rekeying a SA.
+ *	- The new SA will inherit the old SA's policies.
+ *	- To SEC FQ of the new SA will be created in parked mode and
+ *	  will be scheduled after the to SEC FQ of the old SA is empty,
+ *	  exception only when auto_rmv_old_sa if false.
+ *	  This will ensure the preservation of the frame order.
+ *	- To SEC FQ of the old SA will be retired and destroyed when it
+ *	  has no purpose.
+ *	- Memory allocated for old SA will be returned to the SA memory pool
+ *	- auto_rmv_old_sa
+ *		- relevant only for an inbound SA.
+ *		- if true:
+ *			- the old SA will be removed automatically when
+ *			  encrypted traffic starts flowing on the new SA
+ *			- the new SA is not scheduled until traffic arrives on
+ *			  its TO SEC FQ.
+ *		- if false:
+ *			- the old and new SA will be active in the same time.
+ *			- the old SA has to be removed using the
+ *			  dpa_ipsec_remove_sa function when the hard SA
+ *			  expiration time limit is reached
+ *			- Since the difference between soft and hard limit
+ *			  can be several seconds it is required to schedule the
+ *			  TO SEC FQ of the new SA.
+ *
+ *	- rekey_event_cb (UNUSED parameter)
+ */
+int dpa_ipsec_sa_rekeying(int sa_id,
+			  struct dpa_ipsec_sa_params *sa_params,
+			  dpa_ipsec_rekey_event_cb rekey_event_cb,
+			  bool auto_rmv_old_sa, int *new_sa_id);
+
+/*
+ * Disables a SA before removal (no more packets will be processed
+ * using this SA). The resource associated with this SA are not
+ * freed until dpa_ipsec_remove_sa is called.
+ */
+int dpa_ipsec_disable_sa(int sa_id);
+
+/* Unregister a SA and destroys the accelerated path. */
+int dpa_ipsec_remove_sa(int sa_id);
+
+/*
+ * This function will remove all SAs (in a specified DPA IPSec
+ * instance)that were offloaded using the DPA IPsec API
+ */
+int dpa_ipsec_flush_all_sa(int dpa_ipsec_id);
+
+struct dpa_ipsec_l4_params {
+	uint16_t src_port;	/* Source port				      */
+	uint16_t src_port_mask;	/* Source port mask			      */
+	uint16_t dest_port;	/* Destination port			      */
+	uint16_t dest_port_mask;/* Destination port mask		      */
+};
+
+struct dpa_ipsec_icmp_params {
+	uint8_t	icmp_type;	/* Type of ICMP message			      */
+	uint8_t	icmp_type_mask; /* Mask for ICMP type field		      */
+	uint8_t	icmp_code;	/* ICMP message code			      */
+	uint8_t	icmp_code_mask; /* Mask for ICMP code field		      */
+};
+
+/* DPA IPSec direction specific policy params types */
+enum dpa_ipsec_pol_dir_params_type {
+	/*
+	 * No direction specific params
+	 */
+	DPA_IPSEC_POL_DIR_PARAMS_NONE = 0,
+
+	 /*
+	  * Fragmentation or header manipulation
+	  * params (outbound policies only)
+	  */
+	DPA_IPSEC_POL_DIR_PARAMS_MANIP,
+
+	 /*
+	  * Action params (inbound policies only)
+	  */
+	DPA_IPSEC_POL_DIR_PARAMS_ACT
+};
+
+/* DPA IPSec direction specific parameters for Security Policies */
+struct dpa_ipsec_pol_dir_params {
+	enum dpa_ipsec_pol_dir_params_type type;
+	union {
+		 /*
+		  * Manipulation descriptor for fragmentation or
+		  * header manipulation
+		  */
+		int manip_desc;
+		struct dpa_cls_tbl_action in_action; /* Action to be performed
+						      * for frames matching
+						      * the policy selectors  */
+	};
+};
+
+/* DPA-IPSec Security Policy Parameters */
+struct dpa_ipsec_policy_params {
+	struct dpa_offload_ip_address src_addr;	/* Source IP address	      */
+	uint8_t src_prefix_len;	/* Source network prefix		      */
+	struct dpa_offload_ip_address dest_addr; /**< Destination IP address  */
+	uint8_t dest_prefix_len; /* Destination network prefix		      */
+	uint8_t protocol;	/* Protocol				      */
+	bool masked_proto;	/* Mask the entire protocol field	      */
+	union {
+		struct dpa_ipsec_l4_params	l4;	/* L4 protos params   */
+		struct dpa_ipsec_icmp_params	icmp;	/* ICMP proto params  */
+	};
+	struct dpa_ipsec_pol_dir_params dir_params;
+	int priority;		/* Policy priority			      */
+};
+
+/* Add a new rule for policy verification / lookup. */
+int dpa_ipsec_sa_add_policy(int sa_id,
+			    struct dpa_ipsec_policy_params *policy_params);
+
+/* Removes a rule for policy verification / lookup. */
+int dpa_ipsec_sa_remove_policy(int sa_id,
+			       struct dpa_ipsec_policy_params *policy_params);
+
+/*
+ * Retrieves all the policies linked to the specified SA. In order
+ * to determine the size of the policy_params array, the function
+ * must first be called with policy_params = NULL. In this case it
+ * will only return the number of policy entries linked to the SA.
+ */
+int dpa_ipsec_sa_get_policies(int sa_id,
+			      struct dpa_ipsec_policy_params *policy_params,
+			      int *num_pol);
+
+/* This function will remove all policies associated with the specified SA */
+int dpa_ipsec_sa_flush_policies(int sa_id);
+
+/* DPA-IPSec Statistics */
+struct dpa_ipsec_sa_stats {
+	uint32_t packets_count;
+	uint32_t bytes_count;
+};
+
+/* This function will populate sa_stats with SEC statistics for SA with sa_id */
+int dpa_ipsec_sa_get_stats(int sa_id, struct dpa_ipsec_sa_stats *sa_stats);
+
+enum dpa_ipsec_sa_modify_type {
+	DPA_IPSEC_SA_MODIFY_ARS = 0, /* Set the anti replay window size	      */
+	DPA_IPSEC_SA_MODIFY_SEQ_NUM, /* Set the sequence number for this SA   */
+	DPA_IPSEC_SA_MODIFY_EXT_SEQ_NUM, /* Set the extended sequence number  */
+	DPA_IPSEC_SA_MODIFY_CRYPTO /* Reset the crypto algorithms for this SA */
+};
+
+struct dpa_ipsec_sa_modify_prm {
+	enum dpa_ipsec_sa_modify_type type;
+	union {
+		enum dpa_ipsec_arw arw;
+		uint32_t seq;
+		uint64_t ext_seq;
+		struct dpa_ipsec_sa_crypto_params crypto_params;
+	};
+};
+
+int dpa_ipsec_sa_modify(int sa_id, struct dpa_ipsec_sa_modify_prm *modify_prm);
+
+#endif	/* __FSL_DPA_IPSEC_H */
-- 
1.8.4.93.g57e4c17

