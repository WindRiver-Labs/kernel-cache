From ba5275145f8d60b98b9c163c667f9b5708e3ed51 Mon Sep 17 00:00:00 2001
From: Bogdan Hamciuc <bogdan.hamciuc@freescale.com>
Date: Fri, 1 Mar 2013 02:25:52 +0200
Subject: [PATCH 184/547] dpaa_eth: Optimize buffer allocation code

Reduce the code size and number of jumps generated for
dpa_bp_add_8_pages(), which is one of the hottest functions on the Rx
data path.

Remove dependency on per_cpu_ptr of dpaaa_eth_refill_bpools(), by
splitting it into a "this_cpu_ptr" hot function and a  "per_cpu_ptr"
cold function wrapper.

Change-Id: If1bf9a29e9c43bf2345a36488f15846b1481a095
Signed-off-by: Bogdan Hamciuc <bogdan.hamciuc@freescale.com>
Reviewed-on: http://git.am.freescale.net:8181/1058
Reviewed-by: Fleming Andrew-AFLEMING <AFLEMING@freescale.com>
Tested-by: Fleming Andrew-AFLEMING <AFLEMING@freescale.com>
[Original patch taken from QorIQ-SDK-V1.4-SOURCE-20130625-yocto.iso]
Signed-off-by: Bin Jiang <bin.jiang@windriver.com>
---
 drivers/net/ethernet/freescale/dpa/dpaa_eth.c    | 21 ++++----
 drivers/net/ethernet/freescale/dpa/dpaa_eth.h    |  3 +-
 drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c | 65 ++++++++++++++----------
 3 files changed, 49 insertions(+), 40 deletions(-)

diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth.c b/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
index 6619528..32fb628 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
@@ -253,7 +253,7 @@ copy_to_unmapped_area(dma_addr_t phys_start, void *src, size_t buf_size)
 }
 
 #ifndef CONFIG_DPAA_ETH_SG_SUPPORT
-static void dpa_bp_add_8(struct dpa_bp *dpa_bp)
+static void dpa_bp_add_8(const struct dpa_bp *dpa_bp)
 {
 	struct bm_buffer bmb[8];
 	struct sk_buff **skbh;
@@ -375,28 +375,25 @@ static void dpaa_eth_seed_pool(struct dpa_bp *bp)
  */
 static void dpaa_eth_refill_bpools(struct dpa_percpu_priv_s *percpu_priv)
 {
-	int count = *percpu_priv->dpa_bp_count;
+	int *countptr = percpu_priv->dpa_bp_count;
+	int count = *countptr;
+	const struct dpa_bp *dpa_bp = percpu_priv->dpa_bp;
 
 #ifndef CONFIG_DPAA_ETH_SG_SUPPORT
 	if (unlikely(count < REFILL_THRESHOLD)) {
 		int i;
 
 		for (i = count; i < DEFAULT_COUNT; i += 8)
-			dpa_bp_add_8(percpu_priv->dpa_bp);
+			dpa_bp_add_8(dpa_bp);
 	}
 #else
-	if (unlikely(count < REFILL_THRESHOLD)) {
-		int i, cpu;
-
-		/* Add pages to the buffer pool */
-		cpu = smp_processor_id();
-		for (i = count; i < DEFAULT_COUNT; i += 8)
-			dpa_bp_add_8_pages(percpu_priv->dpa_bp, cpu);
-	}
+	/* Add pages to the buffer pool */
+	while (count < DEFAULT_COUNT)
+		count += _dpa_bp_add_8_pages(dpa_bp);
+	*countptr = count;
 
 	/* Add skbs to the percpu skb list, reuse var count */
 	count = percpu_priv->skb_count;
-
 	if (unlikely(count < DEFAULT_SKB_COUNT / 4))
 		dpa_list_add_skbs(percpu_priv,
 				  DEFAULT_SKB_COUNT - count);
diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth.h b/drivers/net/ethernet/freescale/dpa/dpaa_eth.h
index 49a5c08..c12f90d 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth.h
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth.h
@@ -426,7 +426,8 @@ int __hot _dpa_process_parse_results(const t_FmPrsResult *parse_results,
 				     unsigned int *hdr_size __maybe_unused);
 
 #ifdef CONFIG_DPAA_ETH_SG_SUPPORT
-void dpa_bp_add_8_pages(struct dpa_bp *dpa_bp, int cpu_id);
+void dpa_bp_add_8_pages(const struct dpa_bp *dpa_bp, int cpu_id);
+int _dpa_bp_add_8_pages(const struct dpa_bp *dpa_bp);
 
 void dpa_list_add_skbs(struct dpa_percpu_priv_s *cpu_priv, int count);
 #endif
diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c b/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c
index cc269a3..1acf303 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c
@@ -88,50 +88,61 @@ static void dpa_bp_add_page(struct dpa_bp *dpa_bp, unsigned long vaddr)
 	(*count_ptr)++;
 }
 
-void dpa_bp_add_8_pages(struct dpa_bp *dpa_bp, int cpu_id)
+int _dpa_bp_add_8_pages(const struct dpa_bp *dpa_bp)
 {
 	struct bm_buffer bmb[8];
 	unsigned long new_page;
-	int *count_ptr;
 	dma_addr_t addr;
 	int i;
-
-	count_ptr = per_cpu_ptr(dpa_bp->percpu_count, cpu_id);
+	struct device *dev = dpa_bp->dev;
 
 	for (i = 0; i < 8; i++) {
 		new_page = __get_free_page(GFP_ATOMIC);
-		if (unlikely(!new_page)) {
-			dpaa_eth_err(dpa_bp->dev, "__get_free_page() failed\n");
-			bm_buffer_set64(&bmb[i], 0);
-			break;
+		if (likely(new_page)) {
+			addr = dma_map_single(dev, (void *)new_page,
+					dpa_bp->size, DMA_BIDIRECTIONAL);
+			if (likely(!dma_mapping_error(dev, addr))) {
+				bm_buffer_set64(&bmb[i], addr);
+				continue;
+			} else
+				free_page(new_page);
 		}
 
-		addr = dma_map_single(dpa_bp->dev, (void *)new_page,
-				dpa_bp->size, DMA_BIDIRECTIONAL);
-		if (unlikely(dma_mapping_error(dpa_bp->dev, addr))) {
-			dpaa_eth_err(dpa_bp->dev, "DMA mapping failed");
-			free_page(new_page);
-			break;
-		}
-
-		bm_buffer_set64(&bmb[i], addr);
+		/* Something went wrong */
+		goto bail_out;
 	}
 
+release_bufs:
+	/*
+	 * Release the buffers. In case bman is busy, keep trying
+	 * until successful. bman_release() is guaranteed to succeed
+	 * in a reasonable amount of time
+	 */
+	while (unlikely(bman_release(dpa_bp->pool, bmb, i, 0)))
+		cpu_relax();
+
+	return i;
+
+bail_out:
+	dev_err(dpa_bp->dev, "dpa_bp_add_8_pages() failed\n");
+	bm_buffer_set64(&bmb[i], 0);
 	/*
 	 * Avoid releasing a completely null buffer; bman_release() requires
 	 * at least one buffer.
 	 */
-	if (likely(i)) {
-		/*
-		 * Release the buffers. In case bman is busy, keep trying
-		 * until successful. bman_release() is guaranteed to succeed
-		 * in a reasonable amount of time
-		 */
-		while (bman_release(dpa_bp->pool, bmb, i, 0))
-			cpu_relax();
+	if (likely(i))
+		goto release_bufs;
 
-		*count_ptr += i;
-	}
+	return 0;
+}
+
+/*
+ * Cold path wrapper over _dpa_bp_add_8_pages().
+ */
+void dpa_bp_add_8_pages(const struct dpa_bp *dpa_bp, int cpu)
+{
+	int *count_ptr = per_cpu_ptr(dpa_bp->percpu_count, cpu);
+	*count_ptr += _dpa_bp_add_8_pages(dpa_bp);
 }
 
 void dpa_list_add_skb(struct dpa_percpu_priv_s *cpu_priv,
-- 
1.8.4.93.g57e4c17

