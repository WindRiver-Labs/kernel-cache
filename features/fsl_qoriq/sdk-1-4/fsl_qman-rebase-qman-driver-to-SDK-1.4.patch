From f391f4b02b68b4a1215e3396215c3e1b2401fa9d Mon Sep 17 00:00:00 2001
From: Jiang Lu <lu.jiang@windriver.com>
Date: Fri, 19 Jul 2013 11:04:12 +0800
Subject: [PATCH 164/430] fsl_qman:rebase qman driver to SDK 1.4

[Original patch is from QorIQ-SDK-V1.4-20130625-yocto,
fsl_qbman: Add drivers for the Freescale DPAA Q/BMan]

Signed-off-by: Jiang Lu <lu.jiang@windriver.com>
---
 drivers/staging/fsl_qbman/Kconfig        |    7 ++
 drivers/staging/fsl_qbman/qman_config.c  |    3 +-
 drivers/staging/fsl_qbman/qman_driver.c  |   16 ++--
 drivers/staging/fsl_qbman/qman_high.c    |  132 ++++++++++++++++--------------
 drivers/staging/fsl_qbman/qman_private.h |    2 +-
 include/linux/fsl_qman.h                 |    7 ++-
 6 files changed, 92 insertions(+), 75 deletions(-)

diff --git a/drivers/staging/fsl_qbman/Kconfig b/drivers/staging/fsl_qbman/Kconfig
index df5ea87..d5adc5a 100644
--- a/drivers/staging/fsl_qbman/Kconfig
+++ b/drivers/staging/fsl_qbman/Kconfig
@@ -162,6 +162,13 @@ config FSL_QMAN_FQD_SZ
 	  ex: 10 => PAGE_SIZE * (2^10)
 	  Note: Default device-trees now require minimum Kconfig setting of 10.
 
+config FSL_QMAN_PFDR_SZ
+	int "size of the PFDR pool"
+	default 13
+	---help---
+	  This is the size of the PFDR pool defined as: PAGE_SIZE * (2^value)
+	  ex: 13 => PAGE_SIZE * (2^13)
+
 # Corenet initiator settings. Stash request queues are 4-deep to match cores'
 # ability to snart. Stash priority is 3, other priorities are 2.
 config FSL_QMAN_CI_SCHED_CFG_SRCCIV
diff --git a/drivers/staging/fsl_qbman/qman_config.c b/drivers/staging/fsl_qbman/qman_config.c
index f126e21..ccef57b 100644
--- a/drivers/staging/fsl_qbman/qman_config.c
+++ b/drivers/staging/fsl_qbman/qman_config.c
@@ -572,9 +572,8 @@ static int qm_init_pfdr(struct qman *qm, u32 pfdr_start, u32 num)
 /* Config driver */
 /*****************/
 
-/* TODO: Kconfig these? */
 #define DEFAULT_FQD_SZ	(PAGE_SIZE << CONFIG_FSL_QMAN_FQD_SZ)
-#define DEFAULT_PFDR_SZ	(PAGE_SIZE << 12)
+#define DEFAULT_PFDR_SZ	(PAGE_SIZE << CONFIG_FSL_QMAN_PFDR_SZ)
 
 /* We support only one of these */
 static struct qman *qm;
diff --git a/drivers/staging/fsl_qbman/qman_driver.c b/drivers/staging/fsl_qbman/qman_driver.c
index 5ee41d3..cf81eae 100644
--- a/drivers/staging/fsl_qbman/qman_driver.c
+++ b/drivers/staging/fsl_qbman/qman_driver.c
@@ -37,7 +37,7 @@
  * where CCSR isn't available) */
 u16 qman_ip_rev;
 EXPORT_SYMBOL(qman_ip_rev);
-u16 qm_channel_pool1;
+u16 qm_channel_pool1 = QMAN_CHANNEL_POOL1;
 EXPORT_SYMBOL(qm_channel_pool1);
 u16 qm_channel_caam = QMAN_CHANNEL_CAAM;
 EXPORT_SYMBOL(qm_channel_caam);
@@ -104,7 +104,6 @@ static __init int fsl_pool_channel_range_sdqcr(struct device_node *node)
 		pr_err(STR_ERR_CELL, STR_POOL_CHAN_RANGE, 1, node->full_name);
 		return -EINVAL;
 	}
-	qm_channel_pool1 = chanid[0];
 	for (ret = 0; ret < chanid[1]; ret++)
 		pools_sdqcr |= QM_SDQCR_CHANNELS_POOL_CONV(chanid[0] + ret);
 	return 0;
@@ -124,7 +123,6 @@ static __init int fsl_pool_channel_range_init(struct device_node *node)
 		return -EINVAL;
 	}
 	qman_seed_pool_range(chanid[0], chanid[1]);
-	qm_channel_pool1 = chanid[0];
 	pr_info("Qman: pool channel allocator includes range %d:%d\n",
 		chanid[0], chanid[1]);
 	return 0;
@@ -189,7 +187,7 @@ static __init int fsl_ceetm_init(struct device_node *node)
 		qman_seed_ceetm0_lfqid_range(range[0], range[1]);
 	if (dcp_portal == qm_dc_portal_fman1)
 		qman_seed_ceetm1_lfqid_range(range[0], range[1]);
-	pr_info("Qman: The lfqid allocator of CEETM %d includes range"
+	pr_debug("Qman: The lfqid allocator of CEETM %d includes range"
 			" 0x%x:0x%x\n", dcp_portal, range[0], range[1]);
 
 	qman_ceetms[dcp_portal].idx = dcp_portal;
@@ -209,7 +207,7 @@ static __init int fsl_ceetm_init(struct device_node *node)
 	}
 
 	for (i = 0; i < range[1]; i++) {
-		sp = kmalloc(sizeof(*sp), GFP_KERNEL);
+		sp = kzalloc(sizeof(*sp), GFP_KERNEL);
 		if (!sp) {
 			pr_err("Can't alloc memory for sub-portal %d\n",
 							range[0] + i);
@@ -221,7 +219,7 @@ static __init int fsl_ceetm_init(struct device_node *node)
 		list_add_tail(&sp->node, &qman_ceetms[dcp_portal].sub_portals);
 		sp++;
 	}
-	pr_info("Qman: Reserve sub-portal %d:%d for CEETM %d\n",
+	pr_debug("Qman: Reserve sub-portal %d:%d for CEETM %d\n",
 					range[0], range[1], dcp_portal);
 	qman_ceetms[dcp_portal].sp_range[0] = range[0];
 	qman_ceetms[dcp_portal].sp_range[1] = range[1];
@@ -239,7 +237,7 @@ static __init int fsl_ceetm_init(struct device_node *node)
 	}
 
 	for (i = 0; i < range[1]; i++) {
-		lni = kmalloc(sizeof(*lni), GFP_KERNEL);
+		lni = kzalloc(sizeof(*lni), GFP_KERNEL);
 		if (!lni) {
 			pr_err("Can't alloc memory for LNI %d\n",
 							range[0] + i);
@@ -252,7 +250,7 @@ static __init int fsl_ceetm_init(struct device_node *node)
 		list_add_tail(&lni->node, &qman_ceetms[dcp_portal].lnis);
 		lni++;
 	}
-	pr_info("Qman: Reserve LNI %d:%d for CEETM %d\n",
+	pr_debug("Qman: Reserve LNI %d:%d for CEETM %d\n",
 					range[0], range[1], dcp_portal);
 	qman_ceetms[dcp_portal].lni_range[0] = range[0];
 	qman_ceetms[dcp_portal].lni_range[1] = range[1];
@@ -274,7 +272,7 @@ static __init int fsl_ceetm_init(struct device_node *node)
 		qman_seed_ceetm0_channel_range(range[0], range[1]);
 	if (dcp_portal == qm_dc_portal_fman1)
 		qman_seed_ceetm1_channel_range(range[0], range[1]);
-	pr_info("Qman: The channel allocator of CEETM %d includes"
+	pr_debug("Qman: The channel allocator of CEETM %d includes"
 			" range %d:%d\n", dcp_portal, range[0], range[1]);
 
 	/* Set CEETM PRES register */
diff --git a/drivers/staging/fsl_qbman/qman_high.c b/drivers/staging/fsl_qbman/qman_high.c
index b1ebc3a..d57871c 100644
--- a/drivers/staging/fsl_qbman/qman_high.c
+++ b/drivers/staging/fsl_qbman/qman_high.c
@@ -1095,6 +1095,7 @@ u16 qman_affine_channel(int cpu)
 	BUG_ON(!cpumask_test_cpu(cpu, &affine_mask));
 	return affine_channels[cpu];
 }
+EXPORT_SYMBOL(qman_affine_channel);
 
 int qman_poll_dqrr(unsigned int limit)
 {
@@ -2501,7 +2502,7 @@ int qman_ceetm_configure_cq(struct qm_mcc_ceetm_cq_config *opts)
 	return 0;
 }
 
-int qman_ceetm_query_cq(struct qm_ceetm_cq *cq, u32 dcp_idx,
+int qman_ceetm_query_cq(unsigned int cqid, unsigned int dcpid,
 				struct qm_mcr_ceetm_cq_query *cq_query)
 {
 	struct qm_mc_command *mcc;
@@ -2514,8 +2515,8 @@ int qman_ceetm_query_cq(struct qm_ceetm_cq *cq, u32 dcp_idx,
 	PORTAL_IRQ_LOCK(p, irqflags);
 
 	mcc = qm_mc_start(&p->p);
-	mcc->cq_query.cqid = cq->idx;
-	mcc->cq_query.dcpid = cq->parent->dcp_idx;
+	mcc->cq_query.cqid = cqid;
+	mcc->cq_query.dcpid = dcpid;
 	qm_mc_commit(&p->p, QM_CEETM_VERB_CQ_QUERY);
 	while (!(mcr = qm_mc_result(&p->p)))
 		cpu_relax();
@@ -2802,7 +2803,7 @@ int qman_ceetm_cq_peek_pop_xsfdrread(struct qm_ceetm_cq *cq,
 	switch (command_type) {
 	case 0:
 	case 1:
-		mcc->cq_ppxr.cqid = cq->idx;
+		mcc->cq_ppxr.cqid = (cq->parent->idx << 4) | cq->idx;
 		break;
 	case 2:
 		mcc->cq_ppxr.xsfdr = xsfdr;
@@ -2994,7 +2995,7 @@ int qman_ceetm_sp_claim(struct qm_ceetm_sp **sp, enum qm_dc_portal dcp_idx,
 {
 	struct qm_ceetm_sp *p;
 
-	DPA_ASSERT((dcp_id ==  qm_dc_portal_fman0) ||
+	DPA_ASSERT((dcp_idx ==  qm_dc_portal_fman0) ||
 			(dcp_idx == qm_dc_portal_fman1));
 
 	if ((sp_idx < qman_ceetms[dcp_idx].sp_range[0]) ||
@@ -3026,9 +3027,10 @@ int qman_ceetm_sp_release(struct qm_ceetm_sp *sp)
 	}
 
 	list_for_each_entry(p, &qman_ceetms[sp->dcp_idx].sub_portals, node) {
-		if (p->idx == sp->idx)
+		if (p->idx == sp->idx) {
 			p->is_claimed = 0;
 			p->lni = NULL;
+		}
 	}
 	/* Disable CEETM mode of this sub-portal */
 	qman_sp_disable_ceetm_mode(sp->dcp_idx, sp->idx);
@@ -3065,26 +3067,31 @@ EXPORT_SYMBOL(qman_ceetm_lni_claim);
 int qman_ceetm_lni_release(struct qm_ceetm_lni *lni)
 {
 	struct qm_ceetm_lni *p;
+	struct qm_mcc_ceetm_mapping_shaper_tcfc_config config_opts;
 
 	if (!list_empty(&lni->channels)) {
 		pr_err("The LNI dependencies are not released!\n");
 		return -EBUSY;
 	}
 
-	lni->shaper_enable = 0;
-	lni->shaper_couple = 0;
-	lni->cr_token_rate.whole = 0;
-	lni->cr_token_rate.fraction = 0;
-	lni->er_token_rate.whole = 0;
-	lni->er_token_rate.fraction = 0;
-	lni->cr_token_bucket_limit = 0;
-	lni->er_token_bucket_limit = 0;
-	lni->is_claimed = 0;
 	list_for_each_entry(p, &qman_ceetms[lni->dcp_idx].lnis, node) {
-		if (p->idx == lni->idx)
+		if (p->idx == lni->idx) {
+			p->shaper_enable = 0;
+			p->shaper_couple = 0;
+			p->cr_token_rate.whole = 0;
+			p->cr_token_rate.fraction = 0;
+			p->er_token_rate.whole = 0;
+			p->er_token_rate.fraction = 0;
+			p->cr_token_bucket_limit = 0;
+			p->er_token_bucket_limit = 0;
 			p->is_claimed = 0;
+		}
 	}
-	return 0;
+	config_opts.cid = CEETM_COMMAND_LNI_SHAPER | lni->idx;
+	config_opts.dcpid = lni->dcp_idx;
+	memset(&config_opts.shaper_config, 0,
+				sizeof(config_opts.shaper_config));
+	return	qman_ceetm_configure_mapping_shaper_tcfc(&config_opts);
 }
 EXPORT_SYMBOL(qman_ceetm_lni_release);
 
@@ -3092,11 +3099,6 @@ int qman_ceetm_sp_set_lni(struct qm_ceetm_sp *sp, struct qm_ceetm_lni *lni)
 {
 	struct qm_mcc_ceetm_mapping_shaper_tcfc_config config_opts;
 
-	/*if (sp->lni->idx == lni->idx) {
-		pr_err("This SP <-> LNI mapping has been set\n");
-		return -EINVAL;
-	}
-	*/
 	config_opts.cid = CEETM_COMMAND_SP_MAPPING | sp->idx;
 	config_opts.dcpid = sp->dcp_idx;
 	config_opts.sp_mapping.map_lni_id = lni->idx;
@@ -3106,8 +3108,7 @@ int qman_ceetm_sp_set_lni(struct qm_ceetm_sp *sp, struct qm_ceetm_lni *lni)
 		return -EINVAL;
 
 	/* Enable CEETM mode for this sub-portal */
-	qman_sp_enable_ceetm_mode(sp->dcp_idx, sp->idx);
-	return 0;
+	return qman_sp_enable_ceetm_mode(sp->dcp_idx, sp->idx);
 }
 EXPORT_SYMBOL(qman_ceetm_sp_set_lni);
 
@@ -3128,7 +3129,8 @@ int qman_ceetm_sp_get_lni(struct qm_ceetm_sp *sp, unsigned int *lni_idx)
 }
 EXPORT_SYMBOL(qman_ceetm_sp_get_lni);
 
-int qman_ceetm_lni_enable_shaper(struct qm_ceetm_lni *lni, int coupled)
+int qman_ceetm_lni_enable_shaper(struct qm_ceetm_lni *lni, int coupled,
+								int oal)
 {
 	struct qm_mcc_ceetm_mapping_shaper_tcfc_config config_opts;
 
@@ -3142,12 +3144,11 @@ int qman_ceetm_lni_enable_shaper(struct qm_ceetm_lni *lni, int coupled)
 
 	config_opts.cid = CEETM_COMMAND_LNI_SHAPER | lni->idx;
 	config_opts.dcpid = lni->dcp_idx;
-	config_opts.shaper_config.cpl = (coupled << 7);
-				 /* | oal_value;  TBD - oal_value */
+	config_opts.shaper_config.cpl = (coupled << 7) | oal;
 	config_opts.shaper_config.crtcr = (lni->cr_token_rate.whole << 13) |
 			 lni->cr_token_rate.fraction;
 	config_opts.shaper_config.ertcr = (lni->er_token_rate.whole << 13) |
-			 lni->cr_token_rate.fraction;
+			 lni->er_token_rate.fraction;
 	config_opts.shaper_config.crtbl = lni->cr_token_bucket_limit;
 	config_opts.shaper_config.ertbl = lni->er_token_bucket_limit;
 	return	qman_ceetm_configure_mapping_shaper_tcfc(&config_opts);
@@ -3156,24 +3157,13 @@ EXPORT_SYMBOL(qman_ceetm_lni_enable_shaper);
 
 int qman_ceetm_lni_disable_shaper(struct qm_ceetm_lni *lni)
 {
-	struct qm_mcc_ceetm_mapping_shaper_tcfc_config config_opts;
-
 	if (!lni->shaper_enable) {
 		pr_err("The shaper has been disabled\n");
 		return -EINVAL;
 	}
 
 	lni->shaper_enable = 0;
-	lni->shaper_couple = 0;
-
-	config_opts.cid = CEETM_COMMAND_LNI_SHAPER | lni->idx;
-	config_opts.dcpid = lni->dcp_idx;
-	config_opts.shaper_config.cpl = 0; /* | oal_value;  TBD - oal_value */
-	config_opts.shaper_config.crtcr = 0;
-	config_opts.shaper_config.ertcr = 0;
-	config_opts.shaper_config.ertbl = 0;
-	config_opts.shaper_config.crtbl = 0;
-	return	qman_ceetm_configure_mapping_shaper_tcfc(&config_opts);
+	return 0;
 }
 EXPORT_SYMBOL(qman_ceetm_lni_disable_shaper);
 
@@ -3310,7 +3300,6 @@ int qman_ceetm_lni_set_tcfcc(struct qm_ceetm_lni *lni,
 	struct qm_mcc_ceetm_mapping_shaper_tcfc_query query_opts;
 	struct qm_mcr_ceetm_mapping_shaper_tcfc_query query_result;
 	u64 lnitcfcc;
-	int ret;
 
 	if ((cq_level > 15) | (traffic_class > 7)) {
 		pr_err("The CQ or traffic class id is out of range\n");
@@ -3319,15 +3308,23 @@ int qman_ceetm_lni_set_tcfcc(struct qm_ceetm_lni *lni,
 
 	query_opts.cid = CEETM_COMMAND_TCFC | lni->idx;
 	query_opts.dcpid = lni->dcp_idx;
-	ret = qman_ceetm_query_mapping_shaper_tcfc(&query_opts, &query_result);
+	if (qman_ceetm_query_mapping_shaper_tcfc(&query_opts, &query_result)) {
+		pr_err("Fail to query tcfcc\n");
+		return -EINVAL;
+	}
 
 	lnitcfcc = query_result.tcfc_query.lnitcfcc;
-	if (traffic_class == -1) /* disable tcfc for this CQ */
-		lnitcfcc &= ~(1 >> (cq_level * 4));
-	else
-		lnitcfcc |=
-			((QMAN_CEETM_LNITCFCC_ENABLE | traffic_class) & 0xF) <<
-			QMAN_CEETM_LNITCFCC_CQ_LEVEL_SHIFT(cq_level);
+	if (traffic_class == -1) {
+		/* disable tcfc for this CQ */
+		lnitcfcc &= ~((u64)QMAN_CEETM_LNITCFCC_ENABLE <<
+				QMAN_CEETM_LNITCFCC_CQ_LEVEL_SHIFT(cq_level));
+	} else {
+		lnitcfcc &= ~((u64)0xF <<
+				QMAN_CEETM_LNITCFCC_CQ_LEVEL_SHIFT(cq_level));
+		lnitcfcc |= ((u64)(QMAN_CEETM_LNITCFCC_ENABLE |
+				traffic_class)) <<
+				QMAN_CEETM_LNITCFCC_CQ_LEVEL_SHIFT(cq_level);
+	}
 	config_opts.tcfc_config.lnitcfcc = lnitcfcc;
 	config_opts.cid = CEETM_COMMAND_TCFC | lni->idx;
 	config_opts.dcpid = lni->dcp_idx;
@@ -3335,14 +3332,14 @@ int qman_ceetm_lni_set_tcfcc(struct qm_ceetm_lni *lni,
 }
 EXPORT_SYMBOL(qman_ceetm_lni_set_tcfcc);
 
-#define QMAN_CEETM_LNITCFCC_TC_MASK 0x00000007
+#define QMAN_CEETM_LNITCFCC_TC_MASK 0x7
 int qman_ceetm_lni_get_tcfcc(struct qm_ceetm_lni *lni, unsigned int cq_level,
 						int *traffic_class)
 {
 	struct qm_mcc_ceetm_mapping_shaper_tcfc_query query_opts;
 	struct qm_mcr_ceetm_mapping_shaper_tcfc_query query_result;
 	int ret;
-	int lnitcfcc;
+	u8 lnitcfcc;
 
 	if (cq_level > 15) {
 		pr_err("the CQ level is out of range\n");
@@ -3354,9 +3351,9 @@ int qman_ceetm_lni_get_tcfcc(struct qm_ceetm_lni *lni, unsigned int cq_level,
 	ret = qman_ceetm_query_mapping_shaper_tcfc(&query_opts, &query_result);
 	if (ret)
 		return ret;
-	lnitcfcc = query_result.tcfc_query.lnitcfcc >>
-		QMAN_CEETM_LNITCFCC_CQ_LEVEL_SHIFT(cq_level);
-	if ((lnitcfcc & QMAN_CEETM_LNITCFCC_ENABLE) > 1)
+	lnitcfcc = (u8)(query_result.tcfc_query.lnitcfcc >>
+				QMAN_CEETM_LNITCFCC_CQ_LEVEL_SHIFT(cq_level));
+	if (lnitcfcc & QMAN_CEETM_LNITCFCC_ENABLE)
 		*traffic_class = lnitcfcc & QMAN_CEETM_LNITCFCC_TC_MASK;
 	else
 		*traffic_class = -1;
@@ -3383,11 +3380,9 @@ int qman_ceetm_channel_claim(struct qm_ceetm_channel **channel,
 		return -ENODEV;
 	}
 
-	p = kmalloc(sizeof(*p), GFP_KERNEL);
+	p = kzalloc(sizeof(*p), GFP_KERNEL);
 	p->idx = channel_idx;
 	p->dcp_idx = lni->dcp_idx;
-	p->shaper_enable = 0;
-	p->shaper_couple = 0;
 	list_add_tail(&p->node, &lni->channels);
 	INIT_LIST_HEAD(&p->class_queues);
 	INIT_LIST_HEAD(&p->ccgs);
@@ -3408,6 +3403,7 @@ EXPORT_SYMBOL(qman_ceetm_channel_claim);
 
 int qman_ceetm_channel_release(struct qm_ceetm_channel *channel)
 {
+	struct qm_mcc_ceetm_mapping_shaper_tcfc_config config_opts;
 	if (!list_empty(&channel->class_queues)) {
 		pr_err("CEETM channel#%d has class queue unreleased!\n",
 						channel->idx);
@@ -3418,6 +3414,16 @@ int qman_ceetm_channel_release(struct qm_ceetm_channel *channel)
 						channel->idx);
 		return -EBUSY;
 	}
+
+	config_opts.cid = CEETM_COMMAND_CHANNEL_SHAPER | channel->idx;
+	config_opts.dcpid = channel->dcp_idx;
+	memset(&config_opts.shaper_config, 0,
+				sizeof(config_opts.shaper_config));
+	if (qman_ceetm_configure_mapping_shaper_tcfc(&config_opts)) {
+		pr_err("Can't reset channel shapping parameters\n");
+		return -EINVAL;
+	}
+
 	if (channel->dcp_idx == qm_dc_portal_fman0)
 		qman_release_ceetm0_channelid(channel->idx);
 	if (channel->dcp_idx == qm_dc_portal_fman1)
@@ -3466,11 +3472,13 @@ int qman_ceetm_channel_enable_shaper(struct qm_ceetm_channel *channel,
 
 	config_opts.cid = CEETM_COMMAND_CHANNEL_SHAPER | channel->idx;
 	config_opts.shaper_config.cpl = coupled << 7;
-	if (qman_ceetm_configure_mapping_shaper_tcfc(&config_opts)) {
-		pr_err("Can't set coupled for channel #%d\n", channel->idx);
-		return -EINVAL;
-	}
-	return 0;
+	config_opts.shaper_config.crtcr = (channel->cr_token_rate.whole << 13) |
+					channel->cr_token_rate.fraction;
+	config_opts.shaper_config.ertcr = (channel->er_token_rate.whole << 13) |
+					channel->er_token_rate.fraction;
+	config_opts.shaper_config.crtbl = channel->cr_token_bucket_limit;
+	config_opts.shaper_config.ertbl = channel->er_token_bucket_limit;
+	return qman_ceetm_configure_mapping_shaper_tcfc(&config_opts);
 }
 EXPORT_SYMBOL(qman_ceetm_channel_enable_shaper);
 
@@ -4146,7 +4154,7 @@ int qman_ceetm_lfq_release(struct qm_ceetm_lfq *lfq)
 {
 	if (lfq->parent->dcp_idx == qm_dc_portal_fman0)
 		qman_release_ceetm0_lfqid(lfq->idx);
-	if (lfq->parent->dcp_idx == qm_dc_portal_fman0)
+	if (lfq->parent->dcp_idx == qm_dc_portal_fman1)
 		qman_release_ceetm1_lfqid(lfq->idx);
 	list_del(&lfq->node);
 	kfree(lfq);
diff --git a/drivers/staging/fsl_qbman/qman_private.h b/drivers/staging/fsl_qbman/qman_private.h
index eb59035..8bead6b 100644
--- a/drivers/staging/fsl_qbman/qman_private.h
+++ b/drivers/staging/fsl_qbman/qman_private.h
@@ -375,7 +375,7 @@ int qman_sp_enable_ceetm_mode(enum qm_dc_portal portal, u16 sub_portal);
 int qman_sp_disable_ceetm_mode(enum qm_dc_portal portal, u16 sub_portal);
 int qman_ceetm_set_prescaler(enum qm_dc_portal portal);
 int qman_ceetm_get_prescaler(u16 *pres);
-int qman_ceetm_query_cq(struct qm_ceetm_cq *cq, u32 dcp_idx,
+int qman_ceetm_query_cq(unsigned int cqid, unsigned int dcpid,
                                 struct qm_mcr_ceetm_cq_query *cq_query);
 int qman_ceetm_query_ccgr(struct qm_mcc_ceetm_ccgr_query *ccgr_query,
 				struct qm_mcr_ceetm_ccgr_query *response);
diff --git a/include/linux/fsl_qman.h b/include/linux/fsl_qman.h
index 122d925..8124f95 100644
--- a/include/linux/fsl_qman.h
+++ b/include/linux/fsl_qman.h
@@ -40,8 +40,10 @@ extern "C" {
 
 /* Hardware constants */
 #define QM_CHANNEL_SWPORTAL0 0
+#define QMAN_CHANNEL_POOL1 0x21
 #define QMAN_CHANNEL_CAAM 0x80
 #define QMAN_CHANNEL_PME 0xa0
+#define QMAN_CHANNEL_POOL1_REV3 0x401
 #define QMAN_CHANNEL_CAAM_REV3 0x840
 #define QMAN_CHANNEL_PME_REV3 0x860
 extern u16 qm_channel_pool1;
@@ -2652,6 +2654,8 @@ int qman_ceetm_sp_get_lni(struct qm_ceetm_sp *sp,
  * qman_ceetm_lni_disable_shaper - Enables/disables shaping on the LNI.
  * @lni: the given LNI.
  * @coupled: indicates whether CR and ER are coupled.
+ * @oal: the overhead accounting length which is added to the actual length of
+ * each frame when performing shaper calculations.
  *
  * When the number of (unused) committed-rate tokens reach the committed-rate
  * token limit, 'coupled' indicates whether surplus tokens should be added to
@@ -2671,7 +2675,8 @@ int qman_ceetm_sp_get_lni(struct qm_ceetm_sp *sp,
  * a) -EINVAL if the shaper is has already disabled.
  * b) -EIO if calling configure shaper command returns error.
  */
-int qman_ceetm_lni_enable_shaper(struct qm_ceetm_lni *lni, int coupled);
+int qman_ceetm_lni_enable_shaper(struct qm_ceetm_lni *lni, int coupled,
+								int oal);
 int qman_ceetm_lni_disable_shaper(struct qm_ceetm_lni *lni);
 
 /**
-- 
1.7.5.4

