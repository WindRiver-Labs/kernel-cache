From f852ee027b97f159bca998797387ea26a0f69f5e Mon Sep 17 00:00:00 2001
From: Bogdan Hamciuc <bogdan.hamciuc@freescale.com>
Date: Fri, 8 Feb 2013 18:45:07 +0200
Subject: [PATCH 159/547] dpaa_eth: DPAA Ethernet driver from SDK-v1.3.1

Raw move of the DPAA-Ethernet driver as of SDK-v1.3.1.
Also squashed compile fixes on the 3.8 codebase and a number of newer
commits necessary for a clean compilation.

Squashed commits:
 - fmd,dpaa_eth: Reorganize port param structures in wrapper API
 - dpaa_eth: Cleanup sysfs interface
 - dpaa_eth: Minor sysfs change
 - dpaa_eth: Move all sysfs code to a separate file
 - dpaa_eth: Port FSL-specific netdev features

Change-Id: I51b65ae719b154f55302016f4e8bd943d8a062cb
Signed-off-by: Bogdan Hamciuc <bogdan.hamciuc@freescale.com>
Signed-off-by: Ioana Radulescu <ruxandra.radulescu@freescale.com>
Reviewed-on: http://git.am.freescale.net:8181/1031
Reviewed-by: Fleming Andrew-AFLEMING <AFLEMING@freescale.com>
Tested-by: Fleming Andrew-AFLEMING <AFLEMING@freescale.com>
[Original patch taken from QorIQ-SDK-V1.4-SOURCE-20130625-yocto.iso
 Just a minor modification for phy_connect and phy_attach to port
 3.10 kernel; Remove unnecessary code for hacking net core]
Signed-off-by: Bin Jiang <bin.jiang@windriver.com>
---
 drivers/net/ethernet/freescale/Makefile            |    1 +
 drivers/net/ethernet/freescale/dpa/Makefile        |    4 +-
 drivers/net/ethernet/freescale/dpa/dpa-ethtool.c   |  202 +
 drivers/net/ethernet/freescale/dpa/dpaa_1588.c     |  616 +++
 drivers/net/ethernet/freescale/dpa/dpaa_1588.h     |  141 +
 .../net/ethernet/freescale/dpa/dpaa_eth-common.h   |   20 +
 drivers/net/ethernet/freescale/dpa/dpaa_eth.c      | 4002 ++++++++++++++++++++
 drivers/net/ethernet/freescale/dpa/dpaa_eth.h      |    8 +-
 drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c   |  883 +++++
 .../net/ethernet/freescale/dpa/dpaa_eth_sysfs.c    |  181 +
 drivers/net/ethernet/freescale/dpa/mac-api.c       |  839 ++++
 drivers/net/ethernet/freescale/dpa/mac.c           |  432 +++
 drivers/net/ethernet/freescale/dpa/offline_port.c  |  342 ++
 drivers/net/ethernet/freescale/dpa/offline_port.h  |   45 +
 14 files changed, 7708 insertions(+), 8 deletions(-)
 create mode 100644 drivers/net/ethernet/freescale/dpa/dpa-ethtool.c
 create mode 100644 drivers/net/ethernet/freescale/dpa/dpaa_1588.c
 create mode 100644 drivers/net/ethernet/freescale/dpa/dpaa_1588.h
 create mode 100644 drivers/net/ethernet/freescale/dpa/dpaa_eth.c
 create mode 100644 drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c
 create mode 100644 drivers/net/ethernet/freescale/dpa/dpaa_eth_sysfs.c
 create mode 100644 drivers/net/ethernet/freescale/dpa/mac-api.c
 create mode 100644 drivers/net/ethernet/freescale/dpa/mac.c
 create mode 100644 drivers/net/ethernet/freescale/dpa/offline_port.c
 create mode 100644 drivers/net/ethernet/freescale/dpa/offline_port.h

diff --git a/drivers/net/ethernet/freescale/Makefile b/drivers/net/ethernet/freescale/Makefile
index 3d9f255..9aa8c00 100644
--- a/drivers/net/ethernet/freescale/Makefile
+++ b/drivers/net/ethernet/freescale/Makefile
@@ -13,6 +13,7 @@ obj-$(CONFIG_FSL_PQ_MDIO) += fsl_pq_mdio.o
 obj-$(CONFIG_FSL_XGMAC_MDIO) += xgmac_mdio.o
 obj-$(CONFIG_GIANFAR) += gianfar_driver.o
 obj-$(if $(CONFIG_FSL_FMAN),y) += fman/
+obj-$(if $(CONFIG_DPA_ETH),y) += dpa/
 obj-$(CONFIG_PTP_1588_CLOCK_GIANFAR) += gianfar_ptp.o
 gianfar_driver-objs := gianfar.o \
 		gianfar_ethtool.o \
diff --git a/drivers/net/ethernet/freescale/dpa/Makefile b/drivers/net/ethernet/freescale/dpa/Makefile
index 54b3a96..83ec698 100644
--- a/drivers/net/ethernet/freescale/dpa/Makefile
+++ b/drivers/net/ethernet/freescale/dpa/Makefile
@@ -8,14 +8,12 @@ include $(srctree)/drivers/net/ethernet/freescale/fman/ncsw_config.mk
 
 EXTRA_CFLAGS += -I$(NET_DPA)
 
-#Netcomm SW tree
-obj-$(CONFIG_FSL_FMAN) += ../fman/
 obj-$(CONFIG_FSL_DPA_1588) += dpaa_1588.o
 obj-$(CONFIG_DPAA_ETH_SG_SUPPORT) += fsl-dpa-sg.o
 obj-$(CONFIG_DPA_ETH) += fsl-mac.o fsl-dpa.o
 obj-$(CONFIG_DPA_OFFLINE_PORTS) += fsl-oh.o
 
-fsl-dpa-objs	:= dpa-ethtool.o dpaa_eth.o dpaa_eth_sysfs.o xgmac_mdio.o
+fsl-dpa-objs	:= dpa-ethtool.o dpaa_eth.o dpaa_eth_sysfs.o
 fsl-dpa-sg-objs	:= dpaa_eth_sg.o
 fsl-mac-objs	:= mac.o mac-api.o
 fsl-oh-objs	:= offline_port.o
diff --git a/drivers/net/ethernet/freescale/dpa/dpa-ethtool.c b/drivers/net/ethernet/freescale/dpa/dpa-ethtool.c
new file mode 100644
index 0000000..458d3ab
--- /dev/null
+++ b/drivers/net/ethernet/freescale/dpa/dpa-ethtool.c
@@ -0,0 +1,202 @@
+/* Copyright 2008-2012 Freescale Semiconductor, Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *	 notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *	 notice, this list of conditions and the following disclaimer in the
+ *	 documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *	 names of its contributors may be used to endorse or promote products
+ *	 derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/string.h>
+
+#include "dpaa_eth.h"
+
+static int __cold dpa_get_settings(struct net_device *net_dev, struct ethtool_cmd *et_cmd)
+{
+	int			 _errno;
+	struct dpa_priv_s	*priv;
+
+	priv = netdev_priv(net_dev);
+
+	if (priv->mac_dev == NULL) {
+		cpu_netdev_info(net_dev, "This is a MAC-less interface\n");
+		return -ENODEV;
+	}
+	if (unlikely(priv->mac_dev->phy_dev == NULL)) {
+		cpu_netdev_err(net_dev, "phy device not initialized\n");
+		return -ENODEV;
+	}
+
+	_errno = phy_ethtool_gset(priv->mac_dev->phy_dev, et_cmd);
+	if (unlikely(_errno < 0))
+		cpu_netdev_err(net_dev, "phy_ethtool_gset() = %d\n", _errno);
+
+	return _errno;
+}
+
+static int __cold dpa_set_settings(struct net_device *net_dev, struct ethtool_cmd *et_cmd)
+{
+	int			 _errno;
+	struct dpa_priv_s	*priv;
+
+	priv = netdev_priv(net_dev);
+
+	if (priv->mac_dev == NULL) {
+		cpu_netdev_info(net_dev, "This is a MAC-less interface\n");
+		return -ENODEV;
+	}
+	if (unlikely(priv->mac_dev->phy_dev == NULL)) {
+		cpu_netdev_err(net_dev, "phy device not initialized\n");
+		return -ENODEV;
+	}
+
+	_errno = phy_ethtool_sset(priv->mac_dev->phy_dev, et_cmd);
+	if (unlikely(_errno < 0))
+		cpu_netdev_err(net_dev, "phy_ethtool_sset() = %d\n", _errno);
+
+	return _errno;
+}
+
+static void __cold dpa_get_drvinfo(struct net_device *net_dev, struct ethtool_drvinfo *drvinfo)
+{
+	int		 _errno;
+
+	strncpy(drvinfo->driver, KBUILD_MODNAME,
+		sizeof(drvinfo->driver) - 1)[sizeof(drvinfo->driver)-1] = 0;
+	strncpy(drvinfo->version, VERSION,
+		sizeof(drvinfo->driver) - 1)[sizeof(drvinfo->version)-1] = 0;
+	_errno = snprintf(drvinfo->fw_version, sizeof(drvinfo->fw_version), "%X", 0);
+
+	if (unlikely(_errno >= sizeof(drvinfo->fw_version))) {	/* Truncated output */
+		cpu_netdev_notice(net_dev, "snprintf() = %d\n", _errno);
+	} else if (unlikely(_errno < 0)) {
+		cpu_netdev_warn(net_dev, "snprintf() = %d\n", _errno);
+		memset(drvinfo->fw_version, 0, sizeof(drvinfo->fw_version));
+	}
+	strncpy(drvinfo->bus_info, dev_name(net_dev->dev.parent->parent),
+		sizeof(drvinfo->bus_info) - 1)[sizeof(drvinfo->bus_info)-1] = 0;
+}
+
+uint32_t __cold dpa_get_msglevel(struct net_device *net_dev)
+{
+	return ((struct dpa_priv_s *)netdev_priv(net_dev))->msg_enable;
+}
+
+void __cold dpa_set_msglevel(struct net_device *net_dev, uint32_t msg_enable)
+{
+	((struct dpa_priv_s *)netdev_priv(net_dev))->msg_enable = msg_enable;
+}
+
+int __cold dpa_nway_reset(struct net_device *net_dev)
+{
+	int			 _errno;
+	struct dpa_priv_s	*priv;
+
+	priv = netdev_priv(net_dev);
+
+	if (priv->mac_dev == NULL) {
+		cpu_netdev_info(net_dev, "This is a MAC-less interface\n");
+		return -ENODEV;
+	}
+	if (unlikely(priv->mac_dev->phy_dev == NULL)) {
+		cpu_netdev_err(net_dev, "phy device not initialized\n");
+		return -ENODEV;
+	}
+
+	_errno = 0;
+	if (priv->mac_dev->phy_dev->autoneg) {
+		_errno = phy_start_aneg(priv->mac_dev->phy_dev);
+		if (unlikely(_errno < 0))
+			cpu_netdev_err(net_dev, "phy_start_aneg() = %d\n",
+					_errno);
+	}
+
+	return _errno;
+}
+
+void __cold dpa_get_ringparam(struct net_device *net_dev, struct ethtool_ringparam *et_ringparam)
+{
+	et_ringparam->rx_max_pending	   = 0;
+	et_ringparam->rx_mini_max_pending  = 0;
+	et_ringparam->rx_jumbo_max_pending = 0;
+	et_ringparam->tx_max_pending	   = 0;
+
+	et_ringparam->rx_pending	   = 0;
+	et_ringparam->rx_mini_pending	   = 0;
+	et_ringparam->rx_jumbo_pending	   = 0;
+	et_ringparam->tx_pending	   = 0;
+}
+
+void __cold dpa_get_pauseparam(struct net_device *net_dev, struct ethtool_pauseparam *et_pauseparam)
+{
+	struct dpa_priv_s	*priv;
+
+	priv = netdev_priv(net_dev);
+
+	if (priv->mac_dev == NULL) {
+		cpu_netdev_info(net_dev, "This is a MAC-less interface\n");
+		return;
+	}
+	if (unlikely(priv->mac_dev->phy_dev == NULL)) {
+		cpu_netdev_err(net_dev, "phy device not initialized\n");
+		return;
+	}
+
+	et_pauseparam->autoneg	= priv->mac_dev->phy_dev->autoneg;
+}
+
+int __cold dpa_set_pauseparam(struct net_device *net_dev, struct ethtool_pauseparam *et_pauseparam)
+{
+	struct dpa_priv_s	*priv;
+
+	priv = netdev_priv(net_dev);
+
+	if (priv->mac_dev == NULL) {
+		cpu_netdev_info(net_dev, "This is a MAC-less interface\n");
+		return -ENODEV;
+	}
+	if (unlikely(priv->mac_dev->phy_dev == NULL)) {
+		cpu_netdev_err(net_dev, "phy device not initialized\n");
+		return -ENODEV;
+	}
+
+	priv->mac_dev->phy_dev->autoneg = et_pauseparam->autoneg;
+
+	return 0;
+}
+
+const struct ethtool_ops dpa_ethtool_ops = {
+	.get_settings = dpa_get_settings,
+	.set_settings = dpa_set_settings,
+	.get_drvinfo = dpa_get_drvinfo,
+	.get_msglevel = dpa_get_msglevel,
+	.set_msglevel = dpa_set_msglevel,
+	.nway_reset = dpa_nway_reset,
+	.get_ringparam = dpa_get_ringparam,
+	.get_pauseparam = dpa_get_pauseparam,
+	.set_pauseparam = dpa_set_pauseparam,
+	.self_test = NULL, /* TODO invoke the cold-boot unit-test? */
+	.get_ethtool_stats = NULL, /* TODO other stats, currently in debugfs */
+};
diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_1588.c b/drivers/net/ethernet/freescale/dpa/dpaa_1588.c
new file mode 100644
index 0000000..2ac2a01
--- /dev/null
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_1588.c
@@ -0,0 +1,616 @@
+/*
+ * drivers/net/dpa/dpaa_1588.c
+ *
+ * Copyright (C) 2011 Freescale Semiconductor, Inc.
+ * Copyright (C) 2009 IXXAT Automation, GmbH
+ *
+ * DPAA Ethernet Driver -- IEEE 1588 interface functionality
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ */
+#include <linux/io.h>
+#include <linux/device.h>
+#include <linux/fs.h>
+#include <linux/vmalloc.h>
+#include <linux/spinlock.h>
+#include <linux/ip.h>
+#include <linux/ipv6.h>
+#include <linux/udp.h>
+#include <asm/div64.h>
+#include "dpaa_eth.h"
+#include "dpaa_1588.h"
+
+static int dpa_ptp_init_circ(struct dpa_ptp_circ_buf *ptp_buf, u32 size)
+{
+	struct circ_buf *circ_buf = &ptp_buf->circ_buf;
+
+	circ_buf->buf = vmalloc(sizeof(struct dpa_ptp_data) * size);
+	if (!circ_buf->buf)
+		return 1;
+
+	circ_buf->head = 0;
+	circ_buf->tail = 0;
+	ptp_buf->size = size;
+	spin_lock_init(&ptp_buf->ptp_lock);
+
+	return 0;
+}
+
+static void dpa_ptp_reset_circ(struct dpa_ptp_circ_buf *ptp_buf, u32 size)
+{
+	struct circ_buf *circ_buf = &ptp_buf->circ_buf;
+
+	circ_buf->head = 0;
+	circ_buf->tail = 0;
+	ptp_buf->size = size;
+}
+
+static int dpa_ptp_insert(struct dpa_ptp_circ_buf *ptp_buf,
+			  struct dpa_ptp_data *data)
+{
+	struct circ_buf *circ_buf = &ptp_buf->circ_buf;
+	int size = ptp_buf->size;
+	struct dpa_ptp_data *tmp;
+	unsigned long flags;
+	int head, tail;
+
+	spin_lock_irqsave(&ptp_buf->ptp_lock, flags);
+
+	head = circ_buf->head;
+	tail = circ_buf->tail;
+
+	if (CIRC_SPACE(head, tail, size) <= 0) {
+		spin_unlock_irqrestore(&ptp_buf->ptp_lock, flags);
+		return 1;
+	}
+
+	tmp = (struct dpa_ptp_data *)(circ_buf->buf) + head;
+	memcpy(tmp, data, sizeof(struct dpa_ptp_data));
+
+	circ_buf->head = (head + 1) & (size - 1);
+
+	spin_unlock_irqrestore(&ptp_buf->ptp_lock, flags);
+
+	return 0;
+}
+
+static int dpa_ptp_is_ident_match(struct dpa_ptp_ident *dst,
+				  struct dpa_ptp_ident *src)
+{
+	int ret;
+
+	if ((dst->version != src->version) || (dst->msg_type != src->msg_type))
+		return 0;
+
+	if ((dst->netw_prot == src->netw_prot)
+			|| src->netw_prot == DPA_PTP_PROT_DONTCARE) {
+		if (dst->seq_id != src->seq_id)
+			return 0;
+
+		ret = memcmp(dst->snd_port_id, src->snd_port_id,
+				DPA_PTP_SOURCE_PORT_LENGTH);
+		if (ret)
+			return 0;
+		else
+			return 1;
+	}
+
+	return 0;
+}
+
+static int dpa_ptp_find_and_remove(struct dpa_ptp_circ_buf *ptp_buf,
+				   struct dpa_ptp_ident *ident,
+				   struct dpa_ptp_time *ts)
+{
+	struct circ_buf *circ_buf = &ptp_buf->circ_buf;
+	int size = ptp_buf->size;
+	int head, tail, idx;
+	unsigned long flags;
+	struct dpa_ptp_data *tmp;
+	struct dpa_ptp_ident *tmp_ident;
+
+	spin_lock_irqsave(&ptp_buf->ptp_lock, flags);
+
+	head = circ_buf->head;
+	tail = idx = circ_buf->tail;
+
+	if (CIRC_CNT_TO_END(head, tail, size) == 0) {
+		spin_unlock_irqrestore(&ptp_buf->ptp_lock, flags);
+		return 1;
+	}
+
+	while (idx != head) {
+		tmp = (struct dpa_ptp_data *)(circ_buf->buf) + idx;
+		tmp_ident = &tmp->ident;
+		if (dpa_ptp_is_ident_match(tmp_ident, ident))
+			break;
+		idx = (idx + 1) & (size - 1);
+	}
+
+	if (idx == head) {
+		circ_buf->tail = head;
+		spin_unlock_irqrestore(&ptp_buf->ptp_lock, flags);
+		return 1;
+	}
+
+	ts->sec = tmp->ts.sec;
+	ts->nsec = tmp->ts.nsec;
+
+	circ_buf->tail = (idx + 1) & (size - 1);
+
+	spin_unlock_irqrestore(&ptp_buf->ptp_lock, flags);
+
+	return 0;
+}
+
+static int dpa_ptp_get_time(dma_addr_t fd_addr, u32 *high, u32 *low)
+{
+	u8 *ts_addr = (u8 *)phys_to_virt(fd_addr);
+	u32 sec, nsec, mod;
+	u64 tmp;
+
+	ts_addr += DPA_PTP_TIMESTAMP_OFFSET;
+	sec = *((u32 *)ts_addr);
+	nsec = *(((u32 *)ts_addr) + 1);
+	tmp = ((u64)sec << 32 | nsec) * DPA_PTP_NOMINAL_FREQ_PERIOD;
+
+	mod = do_div(tmp, NANOSEC_PER_SECOND);
+	*high = (u32)tmp;
+	*low = mod;
+
+	return 0;
+}
+
+/*
+ * Parse the PTP packets
+ *
+ * The PTP header can be found in an IPv4 packet, IPv6 patcket or in
+ * an IEEE802.3 ethernet frame. This function returns the position of
+ * the PTP packet or NULL if no PTP found
+ */
+static u8 *dpa_ptp_parse_packet(struct sk_buff *skb, u16 *eth_type)
+{
+	u8 *pos = skb->data + ETH_ALEN + ETH_ALEN;
+	u8 *ptp_loc = NULL;
+	u8 msg_type;
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+	u32 access_len = ETH_ALEN + ETH_ALEN + DPA_ETYPE_LEN;
+#endif
+	struct iphdr *iph;
+	struct udphdr *udph;
+	struct ipv6hdr *ipv6h;
+
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+	/* when we can receive S/G frames we need to check the data we want to
+	 * access is in the linear skb buffer */
+	if (!pskb_may_pull(skb, access_len))
+		return NULL;
+#endif
+
+	*eth_type = *((u16 *)pos);
+
+	/* Check if inner tag is here */
+	if (*eth_type == ETH_P_8021Q) {
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+		access_len += DPA_VLAN_TAG_LEN;
+
+		if (!pskb_may_pull(skb, access_len))
+			return NULL;
+#endif
+
+		pos += DPA_VLAN_TAG_LEN;
+		*eth_type = *((u16 *)pos);
+	}
+
+	pos += DPA_ETYPE_LEN;
+
+	switch (*eth_type) {
+	/* Transport of PTP over Ethernet */
+	case ETH_P_1588:
+		ptp_loc = pos;
+
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+		if (!pskb_may_pull(skb, access_len + PTP_OFFS_MSG_TYPE + 1))
+			return NULL;
+#endif
+
+		msg_type = *((u8 *)(ptp_loc + PTP_OFFS_MSG_TYPE)) & 0xf;
+		if ((msg_type == PTP_MSGTYPE_SYNC)
+			|| (msg_type == PTP_MSGTYPE_DELREQ)
+			|| (msg_type == PTP_MSGTYPE_PDELREQ)
+			|| (msg_type == PTP_MSGTYPE_PDELRESP))
+				return ptp_loc;
+		break;
+	/* Transport of PTP over IPv4 */
+	case ETH_P_IP:
+		iph = (struct iphdr *)pos;
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+		access_len += sizeof(struct iphdr);
+
+		if (!pskb_may_pull(skb, access_len))
+			return NULL;
+#endif
+
+		if (ntohs(iph->protocol) != IPPROTO_UDP)
+			return NULL;
+
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+		access_len += iph->ihl * 4 - sizeof(struct iphdr) +
+				sizeof(struct udphdr);
+
+		if (!pskb_may_pull(skb, access_len))
+			return NULL;
+#endif
+
+		pos += iph->ihl * 4;
+		udph = (struct udphdr *)pos;
+		if (ntohs(udph->dest) != 319)
+			return NULL;
+		ptp_loc = pos + sizeof(struct udphdr);
+		break;
+	/* Transport of PTP over IPv6 */
+	case ETH_P_IPV6:
+		ipv6h = (struct ipv6hdr *)pos;
+
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+		access_len += sizeof(struct ipv6hdr) + sizeof(struct udphdr);
+#endif
+
+		if (ntohs(ipv6h->nexthdr) != IPPROTO_UDP)
+			return NULL;
+
+		pos += sizeof(struct ipv6hdr);
+		udph = (struct udphdr *)pos;
+		if (ntohs(udph->dest) != 319)
+			return NULL;
+		ptp_loc = pos + sizeof(struct udphdr);
+		break;
+	default:
+		break;
+	}
+
+	return ptp_loc;
+}
+
+static int dpa_ptp_store_stamp(struct net_device *dev, struct sk_buff *skb,
+		dma_addr_t fd_addr, struct dpa_ptp_data *ptp_data)
+{
+	u32 sec, nsec;
+	u8 *ptp_loc;
+	u16 eth_type;
+
+	ptp_loc = dpa_ptp_parse_packet(skb, &eth_type);
+	if (!ptp_loc)
+		return -EINVAL;
+
+	switch (eth_type) {
+	case ETH_P_IP:
+		ptp_data->ident.netw_prot = DPA_PTP_PROT_IPV4;
+		break;
+	case ETH_P_IPV6:
+		ptp_data->ident.netw_prot = DPA_PTP_PROT_IPV6;
+		break;
+	case ETH_P_1588:
+		ptp_data->ident.netw_prot = DPA_PTP_PROT_802_3;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+	if (!pskb_may_pull(skb, ptp_loc - skb->data + PTP_OFFS_SEQ_ID + 2))
+		return -EINVAL;
+#endif
+
+	ptp_data->ident.version = *(ptp_loc + PTP_OFFS_VER_PTP) & 0xf;
+	ptp_data->ident.msg_type = *(ptp_loc + PTP_OFFS_MSG_TYPE) & 0xf;
+	ptp_data->ident.seq_id = *((u16 *)(ptp_loc + PTP_OFFS_SEQ_ID));
+	memcpy(ptp_data->ident.snd_port_id, ptp_loc + PTP_OFFS_SRCPRTID,
+			DPA_PTP_SOURCE_PORT_LENGTH);
+
+	dpa_ptp_get_time(fd_addr, &sec, &nsec);
+	ptp_data->ts.sec = (u64)sec;
+	ptp_data->ts.nsec = nsec;
+
+	return 0;
+}
+
+void dpa_ptp_store_txstamp(struct net_device *dev, struct sk_buff *skb,
+			   const struct qm_fd *fd)
+{
+	struct dpa_priv_s *priv = netdev_priv(dev);
+	struct dpa_ptp_tsu *tsu = priv->tsu;
+	struct dpa_ptp_data ptp_tx_data;
+	dma_addr_t fd_addr = qm_fd_addr(fd);
+	int ret;
+
+	ret = dpa_ptp_store_stamp(dev, skb, fd_addr, &ptp_tx_data);
+	if (ret)
+		return;
+	dpa_ptp_insert(&tsu->tx_timestamps, &ptp_tx_data);
+}
+
+void dpa_ptp_store_rxstamp(struct net_device *dev, struct sk_buff *skb,
+			   const struct qm_fd *fd)
+{
+	struct dpa_priv_s *priv = netdev_priv(dev);
+	struct dpa_ptp_tsu *tsu = priv->tsu;
+	struct dpa_ptp_data ptp_rx_data;
+	dma_addr_t fd_addr = qm_fd_addr(fd);
+	int ret;
+
+	ret = dpa_ptp_store_stamp(dev, skb,
+					fd_addr + fm_get_rx_extra_headroom(),
+					&ptp_rx_data);
+	if (ret)
+		return;
+	dpa_ptp_insert(&tsu->rx_timestamps, &ptp_rx_data);
+}
+
+static uint8_t dpa_get_tx_timestamp(struct dpa_ptp_tsu *ptp_tsu,
+				    struct dpa_ptp_ident *ident,
+				    struct dpa_ptp_time *ts)
+{
+	struct dpa_ptp_tsu *tsu = ptp_tsu;
+	struct dpa_ptp_time tmp;
+	int flag;
+
+	flag = dpa_ptp_find_and_remove(&tsu->tx_timestamps, ident, &tmp);
+	if (!flag) {
+		ts->sec = tmp.sec;
+		ts->nsec = tmp.nsec;
+		return 0;
+	}
+
+	return -1;
+}
+
+static uint8_t dpa_get_rx_timestamp(struct dpa_ptp_tsu *ptp_tsu,
+				    struct dpa_ptp_ident *ident,
+				    struct dpa_ptp_time *ts)
+{
+	struct dpa_ptp_tsu *tsu = ptp_tsu;
+	struct dpa_ptp_time tmp;
+	int flag;
+
+	flag = dpa_ptp_find_and_remove(&tsu->rx_timestamps, ident, &tmp);
+	if (!flag) {
+		ts->sec = tmp.sec;
+		ts->nsec = tmp.nsec;
+		return 0;
+	}
+
+	return -1;
+}
+
+static void dpa_set_fiper_alarm(struct dpa_ptp_tsu *tsu,
+		struct dpa_ptp_time *cnt_time)
+{
+	struct mac_device *mac_dev = tsu->dpa_priv->mac_dev;
+	u64 tmp, fiper;
+
+	if (mac_dev->fm_rtc_disable)
+		mac_dev->fm_rtc_disable(tsu->dpa_priv->net_dev);
+
+	/* TMR_FIPER1 will pulse every second after ALARM1 expired */
+	tmp = (u64)cnt_time->sec * NANOSEC_PER_SECOND + (u64)cnt_time->nsec;
+	fiper = NANOSEC_PER_SECOND - DPA_PTP_NOMINAL_FREQ_PERIOD;
+	if (mac_dev->fm_rtc_set_alarm)
+		mac_dev->fm_rtc_set_alarm(tsu->dpa_priv->net_dev, 0, tmp);
+	if (mac_dev->fm_rtc_set_fiper)
+		mac_dev->fm_rtc_set_fiper(tsu->dpa_priv->net_dev, 0, fiper);
+
+	if (mac_dev->fm_rtc_enable)
+		mac_dev->fm_rtc_enable(tsu->dpa_priv->net_dev);
+}
+
+static void dpa_get_curr_cnt(struct dpa_ptp_tsu *tsu,
+		struct dpa_ptp_time *curr_time)
+{
+	struct mac_device *mac_dev = tsu->dpa_priv->mac_dev;
+	u64 tmp;
+	u32 mod;
+
+	if (mac_dev->fm_rtc_get_cnt)
+		mac_dev->fm_rtc_get_cnt(tsu->dpa_priv->net_dev, &tmp);
+
+	mod = do_div(tmp, NANOSEC_PER_SECOND);
+	curr_time->sec = (u32)tmp;
+	curr_time->nsec = mod;
+}
+
+static void dpa_set_1588cnt(struct dpa_ptp_tsu *tsu,
+		struct dpa_ptp_time *cnt_time)
+{
+	struct mac_device *mac_dev = tsu->dpa_priv->mac_dev;
+	u64 tmp;
+
+	tmp = (u64)cnt_time->sec * NANOSEC_PER_SECOND + (u64)cnt_time->nsec;
+
+	if (mac_dev->fm_rtc_set_cnt)
+		mac_dev->fm_rtc_set_cnt(tsu->dpa_priv->net_dev, tmp);
+
+	/* Restart fiper two seconds later */
+	cnt_time->sec += 2;
+	cnt_time->nsec = 0;
+	dpa_set_fiper_alarm(tsu, cnt_time);
+}
+
+static void dpa_get_drift(struct dpa_ptp_tsu *tsu, u32 *addend)
+{
+	struct mac_device *mac_dev = tsu->dpa_priv->mac_dev;
+	u32 drift;
+
+	if (mac_dev->fm_rtc_get_drift)
+		mac_dev->fm_rtc_get_drift(tsu->dpa_priv->net_dev, &drift);
+
+	*addend = drift;
+}
+
+static void dpa_set_drift(struct dpa_ptp_tsu *tsu, u32 addend)
+{
+	struct mac_device *mac_dev = tsu->dpa_priv->mac_dev;
+
+	if (mac_dev->fm_rtc_set_drift)
+		mac_dev->fm_rtc_set_drift(tsu->dpa_priv->net_dev, addend);
+}
+
+static void dpa_flush_timestamp(struct dpa_ptp_tsu *tsu)
+{
+	dpa_ptp_reset_circ(&tsu->rx_timestamps, DEFAULT_PTP_RX_BUF_SZ);
+	dpa_ptp_reset_circ(&tsu->tx_timestamps, DEFAULT_PTP_TX_BUF_SZ);
+}
+
+int dpa_ioctl_1588(struct net_device *dev, struct ifreq *ifr, int cmd)
+{
+	struct dpa_priv_s *priv = netdev_priv(dev);
+	struct dpa_ptp_tsu *tsu = priv->tsu;
+	struct mac_device *mac_dev = priv->mac_dev;
+	struct dpa_ptp_data ptp_data;
+	struct dpa_ptp_data *ptp_data_user;
+	struct dpa_ptp_time act_time;
+	u32 addend;
+	int retval = 0;
+
+	if (!tsu || !tsu->valid)
+		return -ENODEV;
+
+	switch (cmd) {
+	case PTP_ENBL_TXTS_IOCTL:
+		tsu->hwts_tx_en_ioctl = 1;
+		if (mac_dev->fm_rtc_enable)
+			mac_dev->fm_rtc_enable(dev);
+		if (mac_dev->ptp_enable)
+			mac_dev->ptp_enable(mac_dev);
+		break;
+	case PTP_DSBL_TXTS_IOCTL:
+		tsu->hwts_tx_en_ioctl = 0;
+		if (mac_dev->fm_rtc_disable)
+			mac_dev->fm_rtc_disable(dev);
+		if (mac_dev->ptp_disable)
+			mac_dev->ptp_disable(mac_dev);
+		break;
+	case PTP_ENBL_RXTS_IOCTL:
+		tsu->hwts_rx_en_ioctl = 1;
+		break;
+	case PTP_DSBL_RXTS_IOCTL:
+		tsu->hwts_rx_en_ioctl = 0;
+		break;
+	case PTP_GET_RX_TIMESTAMP:
+		ptp_data_user = (struct dpa_ptp_data *)ifr->ifr_data;
+		if (copy_from_user(&ptp_data.ident,
+				&ptp_data_user->ident, sizeof(ptp_data.ident)))
+			return -EINVAL;
+
+		if (dpa_get_rx_timestamp(tsu, &ptp_data.ident, &ptp_data.ts))
+			return -EAGAIN;
+
+		if (copy_to_user((void __user *)&ptp_data_user->ts,
+				&ptp_data.ts, sizeof(ptp_data.ts)))
+			return -EFAULT;
+		break;
+	case PTP_GET_TX_TIMESTAMP:
+		ptp_data_user = (struct dpa_ptp_data *)ifr->ifr_data;
+		if (copy_from_user(&ptp_data.ident,
+				&ptp_data_user->ident, sizeof(ptp_data.ident)))
+			return -EINVAL;
+
+		if (dpa_get_tx_timestamp(tsu, &ptp_data.ident, &ptp_data.ts))
+			return -EAGAIN;
+
+		if (copy_to_user((void __user *)&ptp_data_user->ts,
+				&ptp_data.ts, sizeof(ptp_data.ts)))
+			return -EFAULT;
+		break;
+	case PTP_GET_TIME:
+		dpa_get_curr_cnt(tsu, &act_time);
+		if (copy_to_user(ifr->ifr_data, &act_time, sizeof(act_time)))
+			return -EFAULT;
+		break;
+	case PTP_SET_TIME:
+		if (copy_from_user(&act_time, ifr->ifr_data, sizeof(act_time)))
+			return -EINVAL;
+		dpa_set_1588cnt(tsu, &act_time);
+		break;
+	case PTP_GET_ADJ:
+		dpa_get_drift(tsu, &addend);
+		if (copy_to_user(ifr->ifr_data, &addend, sizeof(addend)))
+			return -EFAULT;
+		break;
+	case PTP_SET_ADJ:
+		if (copy_from_user(&addend, ifr->ifr_data, sizeof(addend)))
+			return -EINVAL;
+		dpa_set_drift(tsu, addend);
+		break;
+	case PTP_SET_FIPER_ALARM:
+		if (copy_from_user(&act_time, ifr->ifr_data, sizeof(act_time)))
+			return -EINVAL;
+		dpa_set_fiper_alarm(tsu, &act_time);
+		break;
+	case PTP_CLEANUP_TS:
+		dpa_flush_timestamp(tsu);
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	return retval;
+}
+
+int dpa_ptp_init(struct dpa_priv_s *priv)
+{
+	struct dpa_ptp_tsu *tsu;
+
+	/* Allocate memory for PTP structure */
+	tsu = kzalloc(sizeof(struct dpa_ptp_tsu), GFP_KERNEL);
+	if (!tsu)
+		return -ENOMEM;
+
+	memset(tsu, 0, sizeof(*tsu));
+	tsu->valid = TRUE;
+	tsu->dpa_priv = priv;
+
+	dpa_ptp_init_circ(&tsu->rx_timestamps, DEFAULT_PTP_RX_BUF_SZ);
+	dpa_ptp_init_circ(&tsu->tx_timestamps, DEFAULT_PTP_TX_BUF_SZ);
+
+	priv->tsu = tsu;
+
+	return 0;
+}
+EXPORT_SYMBOL(dpa_ptp_init);
+
+void dpa_ptp_cleanup(struct dpa_priv_s *priv)
+{
+	struct dpa_ptp_tsu *tsu = priv->tsu;
+
+	tsu->valid = FALSE;
+	vfree(tsu->rx_timestamps.circ_buf.buf);
+	vfree(tsu->tx_timestamps.circ_buf.buf);
+
+	kfree(tsu);
+}
+EXPORT_SYMBOL(dpa_ptp_cleanup);
+
+static int __init __cold dpa_ptp_load(void)
+{
+	return 0;
+}
+module_init(dpa_ptp_load);
+
+static void __exit __cold dpa_ptp_unload(void)
+{
+}
+module_exit(dpa_ptp_unload);
diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_1588.h b/drivers/net/ethernet/freescale/dpa/dpaa_1588.h
new file mode 100644
index 0000000..ac2fc73
--- /dev/null
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_1588.h
@@ -0,0 +1,141 @@
+/*
+ * drivers/net/dpa/dpaa_1588.h
+ *
+ * Copyright (C) 2011 Freescale Semiconductor, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ */
+#ifndef __DPAA_1588_H__
+#define __DPAA_1588_H__
+
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/circ_buf.h>
+#include <linux/fsl_qman.h>
+
+#define DEFAULT_PTP_RX_BUF_SZ		2048
+#define DEFAULT_PTP_TX_BUF_SZ		512
+
+/* 1588 private ioctl calls */
+#define PTP_ENBL_TXTS_IOCTL	SIOCDEVPRIVATE
+#define PTP_DSBL_TXTS_IOCTL	(SIOCDEVPRIVATE + 1)
+#define PTP_ENBL_RXTS_IOCTL	(SIOCDEVPRIVATE + 2)
+#define PTP_DSBL_RXTS_IOCTL	(SIOCDEVPRIVATE + 3)
+#define PTP_GET_TX_TIMESTAMP	(SIOCDEVPRIVATE + 4)
+#define PTP_GET_RX_TIMESTAMP	(SIOCDEVPRIVATE + 5)
+#define PTP_SET_TIME		(SIOCDEVPRIVATE + 6)
+#define PTP_GET_TIME		(SIOCDEVPRIVATE + 7)
+#define PTP_SET_FIPER_ALARM	(SIOCDEVPRIVATE + 8)
+#define PTP_SET_ADJ		(SIOCDEVPRIVATE + 9)
+#define PTP_GET_ADJ		(SIOCDEVPRIVATE + 10)
+#define PTP_CLEANUP_TS		(SIOCDEVPRIVATE + 11)
+
+/* PTP V2 message type */
+enum {
+	PTP_MSGTYPE_SYNC		= 0x0,
+	PTP_MSGTYPE_DELREQ		= 0x1,
+	PTP_MSGTYPE_PDELREQ		= 0x2,
+	PTP_MSGTYPE_PDELRESP		= 0x3,
+	PTP_MSGTYPE_FLWUP		= 0x8,
+	PTP_MSGTYPE_DELRESP		= 0x9,
+	PTP_MSGTYPE_PDELRES_FLWUP	= 0xA,
+	PTP_MSGTYPE_ANNOUNCE		= 0xB,
+	PTP_MSGTYPE_SGNLNG		= 0xC,
+	PTP_MSGTYPE_MNGMNT		= 0xD,
+};
+
+/* Byte offset of data in the PTP V2 headers */
+#define PTP_OFFS_MSG_TYPE		0
+#define PTP_OFFS_VER_PTP		1
+#define PTP_OFFS_MSG_LEN		2
+#define PTP_OFFS_DOM_NMB		4
+#define PTP_OFFS_FLAGS			6
+#define PTP_OFFS_CORFIELD		8
+#define PTP_OFFS_SRCPRTID		20
+#define PTP_OFFS_SEQ_ID			30
+#define PTP_OFFS_CTRL			32
+#define PTP_OFFS_LOGMEAN		33
+
+#define PTP_IP_OFFS			14
+#define PTP_UDP_OFFS			34
+#define PTP_HEADER_OFFS			42
+#define PTP_MSG_TYPE_OFFS		(PTP_HEADER_OFFS + PTP_OFFS_MSG_TYPE)
+#define PTP_SPORT_ID_OFFS		(PTP_HEADER_OFFS + PTP_OFFS_SRCPRTID)
+#define PTP_SEQ_ID_OFFS			(PTP_HEADER_OFFS + PTP_OFFS_SEQ_ID)
+#define PTP_CTRL_OFFS			(PTP_HEADER_OFFS + PTP_OFFS_CTRL)
+
+/* 1588-2008 network protocol enumeration values */
+#define DPA_PTP_PROT_IPV4		1
+#define DPA_PTP_PROT_IPV6		2
+#define DPA_PTP_PROT_802_3		3
+#define DPA_PTP_PROT_DONTCARE		0xFFFF
+
+#define DPA_PTP_SOURCE_PORT_LENGTH	10
+#define DPA_PTP_HEADER_SZE		34
+#define DPA_ETYPE_LEN			2
+#define DPA_VLAN_TAG_LEN		4
+
+#define DPA_PTP_TIMESTAMP_OFFSET	0x30
+#define DPA_PTP_NOMINAL_FREQ_PERIOD	5  /* 5ns -> 200M */
+#define NANOSEC_PER_SECOND		1000000000
+
+/* Struct needed to identify a timestamp */
+struct dpa_ptp_ident {
+	u8	version;
+	u8	msg_type;
+	u16	netw_prot;
+	u16	seq_id;
+	u8	snd_port_id[DPA_PTP_SOURCE_PORT_LENGTH];
+};
+
+/* Timestamp format in 1588-2008 */
+struct dpa_ptp_time {
+	u64	sec;	/* just 48 bit used */
+	u32	nsec;
+};
+
+/* needed for timestamp data over ioctl */
+struct dpa_ptp_data {
+	struct dpa_ptp_ident	ident;
+	struct dpa_ptp_time	ts;
+};
+
+struct dpa_ptp_circ_buf {
+	struct circ_buf circ_buf;
+	u32 size;
+	spinlock_t ptp_lock;
+};
+
+/* PTP TSU control structure */
+struct dpa_ptp_tsu {
+	struct dpa_priv_s *dpa_priv;
+	bool valid;
+	struct dpa_ptp_circ_buf rx_timestamps;
+	struct dpa_ptp_circ_buf tx_timestamps;
+
+	/* HW timestamping over ioctl enabled flag */
+	int hwts_tx_en_ioctl;
+	int hwts_rx_en_ioctl;
+};
+
+extern int dpa_ptp_init(struct dpa_priv_s *priv);
+extern void dpa_ptp_cleanup(struct dpa_priv_s *priv);
+extern void dpa_ptp_store_txstamp(struct net_device *dev, struct sk_buff *skb,
+				  const struct qm_fd *fd);
+extern void dpa_ptp_store_rxstamp(struct net_device *dev, struct sk_buff *skb,
+				  const struct qm_fd *fd);
+extern int dpa_ioctl_1588(struct net_device *dev, struct ifreq *ifr, int cmd);
+#endif
diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth-common.h b/drivers/net/ethernet/freescale/dpa/dpaa_eth-common.h
index 5b63688..6808c73 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth-common.h
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth-common.h
@@ -42,6 +42,26 @@
 
 #define __hot
 
+/*
+ * TODO Remove these altogether. They have been removed form kernel 3.8
+ * and are still here for compatibility while we're rebasing the code.
+ */
+#ifndef __devinit
+#define __devinit
+#endif
+#ifndef __devexit_p
+#define __devexit_p
+#endif
+#ifndef __devinitdata
+#define __devinitdata
+#endif
+#ifndef __devinitconst
+#define __devinitconst
+#endif
+#ifndef __devexit
+#define __devexit
+#endif
+
 #define cpu_printk(level, format, arg...) \
 	pr_##level("cpu%d: " format, smp_processor_id(), ##arg)
 
diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth.c b/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
new file mode 100644
index 0000000..64ffe53
--- /dev/null
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
@@ -0,0 +1,4002 @@
+/*
+ * Copyright 2008-2012 Freescale Semiconductor Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *	 notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *	 notice, this list of conditions and the following disclaimer in the
+ *	 documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *	 names of its contributors may be used to endorse or promote products
+ *	 derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/sort.h>
+#include <linux/of_mdio.h>
+#include <linux/of_platform.h>
+#include <linux/of_net.h>
+#include <linux/kthread.h>
+#include <linux/io.h>
+#include <linux/etherdevice.h>
+#include <linux/if_arp.h>	/* arp_hdr_len() */
+#include <linux/if_vlan.h>	/* VLAN_HLEN */
+#include <linux/icmp.h>		/* struct icmphdr */
+#include <linux/ip.h>		/* struct iphdr */
+#include <linux/ipv6.h>		/* struct ipv6hdr */
+#include <linux/udp.h>		/* struct udphdr */
+#include <linux/tcp.h>		/* struct tcphdr */
+#include <linux/net.h>		/* net_ratelimit() */
+#include <linux/if_ether.h>	/* ETH_P_IP and ETH_P_IPV6 */
+#include <linux/highmem.h>
+#include <linux/percpu.h>
+#include <linux/dma-mapping.h>
+#include <asm/smp.h>		/* get_hard_smp_processor_id() */
+#ifdef CONFIG_DEBUG_FS
+#include <linux/debugfs.h>
+#include <asm/debug.h>
+#endif
+#include <linux/fsl_bman.h>
+
+#include "fsl_fman.h"
+#include "fm_ext.h"
+#include "fm_port_ext.h"
+
+#include "mac.h"
+#include "dpaa_eth.h"
+#include "dpaa_1588.h"
+
+#define ARRAY2_SIZE(arr)	(ARRAY_SIZE(arr) * ARRAY_SIZE((arr)[0]))
+
+/* DPAA platforms benefit from hardware-assisted queue management */
+#define DPA_NETIF_FEATURES	(0)
+
+#define DEFAULT_COUNT		128
+#define REFILL_THRESHOLD	80
+
+#define DPA_NAPI_WEIGHT		64
+
+/* Size in bytes of the Congestion State notification threshold on 10G ports */
+#define DPA_CS_THRESHOLD_10G	0x10000000
+/*
+ * Size in bytes of the Congestion State notification threshold on 1G ports.
+
+ * The 1G dTSECs can quite easily be flooded by cores doing Tx in a tight loop
+ * (e.g. by sending UDP datagrams at "while(1) speed"),
+ * and the larger the frame size, the more acute the problem.
+ *
+ * So we have to find a balance between these factors:
+ *	- avoiding the device staying congested for a prolonged time (risking
+ *	  the netdev watchdog to fire - see also the tx_timeout module param);
+ *	- affecting performance of protocols such as TCP, which otherwise
+ *	  behave well under the congestion notification mechanism;
+ *	- preventing the Tx cores from tightly-looping (as if the congestion
+ *	  threshold was too low to be effective);
+ *	- running out of memory if the CS threshold is set too high.
+ */
+#define DPA_CS_THRESHOLD_1G	0x10000000
+/* Set a congestion threshold for MAC-less devices, too. */
+#define DPA_CS_THRESHOLD_MACLESS	0x10000000
+
+/* S/G table requires at least 256 bytes */
+#define SGT_BUFFER_SIZE		DPA_BP_SIZE(256)
+
+/* Maximum frame size on Tx for which skb copying is preferrable to
+ * creating a S/G frame */
+#define DPA_SKB_COPY_MAX_SIZE	256
+
+/* Valid checksum indication */
+#define DPA_CSUM_VALID		0xFFFF
+
+/* Maximum offset value for a contig or sg FD (represented on 9bits) */
+#define DPA_MAX_FD_OFFSET	((1 << 9) - 1)
+
+/*
+ * Maximum size of a buffer that is to be recycled back to the buffer pool.
+ * The value is arbitrary, but tries to reach a balance such that originating
+ * frames may get recycled, while forwarded skbs that get reallocated on Tx
+ * aren't allowed to grow unboundedly.
+ */
+#define DPA_BP_MAX_BUF_SIZE	(DEFAULT_BUF_SIZE + 256)
+
+#define DPA_DESCRIPTION "FSL DPAA Ethernet driver"
+
+MODULE_LICENSE("Dual BSD/GPL");
+
+MODULE_AUTHOR("Andy Fleming <afleming@freescale.com>");
+
+MODULE_DESCRIPTION(DPA_DESCRIPTION);
+
+static uint8_t debug = -1;
+module_param(debug, byte, S_IRUGO);
+MODULE_PARM_DESC(debug, "Module/Driver verbosity level");
+
+/* This has to work in tandem with the DPA_CS_THRESHOLD_xxx values. */
+static uint16_t __devinitdata tx_timeout = 1000;
+module_param(tx_timeout, ushort, S_IRUGO);
+MODULE_PARM_DESC(tx_timeout, "The Tx timeout in ms");
+
+#ifdef CONFIG_DEBUG_FS
+static struct dentry *dpa_debugfs_root;
+#endif
+
+/* dpaa_eth mirror for the FMan values */
+static int dpa_rx_extra_headroom;
+static int dpa_max_frm;
+
+static const char rtx[][3] = {
+	[RX] = "RX",
+	[TX] = "TX"
+};
+
+#if defined(CONFIG_FSL_FMAN_TEST)
+/* Defined as weak, to be implemented by fman pcd tester. */
+int dpa_alloc_pcd_fqids(struct device *, uint32_t, uint8_t, uint32_t *)
+__attribute__((weak));
+
+int dpa_free_pcd_fqids(struct device *, uint32_t) __attribute__((weak));
+#endif /* CONFIG_DPAA_FMAN_UNIT_TESTS */
+
+/* BM */
+
+#define DPAA_ETH_MAX_PAD (L1_CACHE_BYTES * 8)
+
+static struct dpa_bp *dpa_bp_array[64];
+
+static struct dpa_bp *default_pool;
+
+/* A set of callbacks for hooking into the fastpath at different points. */
+static struct dpaa_eth_hooks_s dpaa_eth_hooks;
+/*
+ * This function should only be called on the probe paths, since it makes no
+ * effort to guarantee consistency of the destination hooks structure.
+ */
+void fsl_dpaa_eth_set_hooks(struct dpaa_eth_hooks_s *hooks)
+{
+	if (hooks)
+		dpaa_eth_hooks = *hooks;
+	else
+		pr_err("NULL pointer to hooks!\n");
+}
+EXPORT_SYMBOL(fsl_dpaa_eth_set_hooks);
+
+
+struct dpa_bp *dpa_bpid2pool(int bpid)
+{
+	return dpa_bp_array[bpid];
+}
+
+static void dpa_bp_depletion(struct bman_portal	*portal,
+		struct bman_pool *pool, void *cb_ctx, int depleted)
+{
+	if (net_ratelimit())
+		pr_err("Invalid Pool depleted notification!\n");
+}
+
+/*
+ * Copy from a memory region that requires kmapping to a linear buffer,
+ * taking into account page boundaries in the source
+ */
+static void
+copy_from_unmapped_area(void *dest, dma_addr_t phys_start, size_t buf_size)
+{
+	struct page *page;
+	size_t size, offset;
+	void *page_vaddr;
+
+	while (buf_size > 0) {
+		offset = offset_in_page(phys_start);
+		size = (offset + buf_size > PAGE_SIZE) ?
+			PAGE_SIZE - offset : buf_size;
+
+		page = pfn_to_page(phys_start >> PAGE_SHIFT);
+		page_vaddr = kmap_atomic(page);
+
+		memcpy(dest, page_vaddr + offset, size);
+
+		kunmap_atomic(page_vaddr);
+
+		phys_start += size;
+		dest += size;
+		buf_size -= size;
+	}
+}
+
+/*
+ * Copy to a memory region that requires kmapping from a linear buffer,
+ * taking into account page boundaries in the destination
+ */
+static void
+copy_to_unmapped_area(dma_addr_t phys_start, void *src, size_t buf_size)
+{
+	struct page *page;
+	size_t size, offset;
+	void *page_vaddr;
+
+	while (buf_size > 0) {
+		offset = offset_in_page(phys_start);
+		size = (offset + buf_size > PAGE_SIZE) ?
+				PAGE_SIZE - offset : buf_size;
+
+		page = pfn_to_page(phys_start >> PAGE_SHIFT);
+		page_vaddr = kmap_atomic(page);
+
+		memcpy(page_vaddr + offset, src, size);
+
+		kunmap_atomic(page_vaddr);
+
+		phys_start += size;
+		src += size;
+		buf_size -= size;
+	}
+}
+
+#ifndef CONFIG_DPAA_ETH_SG_SUPPORT
+static void dpa_bp_add_8(struct dpa_bp *dpa_bp)
+{
+	struct bm_buffer bmb[8];
+	struct sk_buff **skbh;
+	dma_addr_t addr;
+	int i;
+	struct sk_buff *skb;
+	int *count_ptr;
+
+	count_ptr = per_cpu_ptr(dpa_bp->percpu_count, smp_processor_id());
+
+	for (i = 0; i < 8; i++) {
+		/*
+		 * The buffers tend to be aligned all to the same cache
+		 * index.  A standard dequeue operation pulls in 15 packets.
+		 * This means that when it stashes, it evicts half of the
+		 * packets it's stashing. In order to prevent that, we pad
+		 * by a variable number of cache lines, to reduce collisions.
+		 * We always pad by at least 1 cache line, because we want
+		 * a little extra room at the beginning for IPSec and to
+		 * accommodate NET_IP_ALIGN.
+		 */
+		int pad = (i + 1) * L1_CACHE_BYTES;
+
+		skb = dev_alloc_skb(dpa_bp->size + pad);
+		if (unlikely(!skb)) {
+			printk(KERN_ERR "dev_alloc_skb() failed\n");
+			bm_buffer_set64(&bmb[i], 0);
+			break;
+		}
+
+		skbh = (struct sk_buff **)(skb->head + pad);
+		*skbh = skb;
+
+		/*
+		 * Here we need to map only for device write (DMA_FROM_DEVICE),
+		 * but on Tx recycling we may also get buffers in the pool that
+		 * are mapped bidirectionally.
+		 * Use DMA_BIDIRECTIONAL here as well to avoid any
+		 * inconsistencies when unmapping.
+		 */
+		addr = dma_map_single(dpa_bp->dev, skb->head + pad,
+				dpa_bp->size, DMA_BIDIRECTIONAL);
+		if (unlikely(dma_mapping_error(dpa_bp->dev, addr))) {
+			dpaa_eth_err(dpa_bp->dev, "DMA mapping failed");
+			break;
+		}
+
+		bm_buffer_set64(&bmb[i], addr);
+	}
+
+	/* Avoid releasing a completely null buffer; bman_release() requires
+	 * at least one buf. */
+	if (likely(i)) {
+		/*
+		 * Release the buffers. In case bman is busy, keep trying
+		 * until successful. bman_release() is guaranteed to succeed
+		 * in a reasonable amount of time
+		 */
+		while (bman_release(dpa_bp->pool, bmb, i, 0))
+			cpu_relax();
+
+		*count_ptr += i;
+	}
+}
+
+void dpa_make_private_pool(struct dpa_bp *dpa_bp)
+{
+	int i;
+
+	dpa_bp->percpu_count = __alloc_percpu(sizeof(*dpa_bp->percpu_count),
+			__alignof__(*dpa_bp->percpu_count));
+
+	/* Give each cpu an allotment of "count" buffers */
+	for_each_online_cpu(i) {
+		int *thiscount;
+		int *countptr;
+		int j;
+		thiscount = per_cpu_ptr(dpa_bp->percpu_count,
+				smp_processor_id());
+		countptr = per_cpu_ptr(dpa_bp->percpu_count, i);
+
+		for (j = 0; j < dpa_bp->target_count; j += 8)
+			dpa_bp_add_8(dpa_bp);
+
+		/* Adjust the counts */
+		*countptr = j;
+
+		if (countptr != thiscount)
+			*thiscount = *thiscount - j;
+	}
+}
+#endif /* CONFIG_DPAA_ETH_SG_SUPPORT */
+
+static void dpaa_eth_seed_pool(struct dpa_bp *bp)
+{
+	int count = bp->target_count;
+	size_t addr = bp->paddr;
+
+	while (count) {
+		struct bm_buffer bufs[8];
+		int num_bufs = 0;
+
+		do {
+			BUG_ON(addr > 0xffffffffffffull);
+			bufs[num_bufs].bpid = bp->bpid;
+			bm_buffer_set64(&bufs[num_bufs++], addr);
+			addr += bp->size;
+
+		} while (--count && (num_bufs < 8));
+
+		while (bman_release(bp->pool, bufs, num_bufs, 0))
+			cpu_relax();
+	}
+}
+
+/*
+ * Add buffers/pages/skbuffs for Rx processing whenever bpool count falls below
+ * REFILL_THRESHOLD.
+ */
+static void dpaa_eth_refill_bpools(struct dpa_percpu_priv_s *percpu_priv)
+{
+	int count = *percpu_priv->dpa_bp_count;
+
+#ifndef CONFIG_DPAA_ETH_SG_SUPPORT
+	if (unlikely(count < REFILL_THRESHOLD)) {
+		int i;
+
+		for (i = count; i < DEFAULT_COUNT; i += 8)
+			dpa_bp_add_8(percpu_priv->dpa_bp);
+	}
+#else
+	if (unlikely(count < REFILL_THRESHOLD)) {
+		int i, cpu;
+
+		/* Add pages to the buffer pool */
+		cpu = smp_processor_id();
+		for (i = count; i < DEFAULT_COUNT; i += 8)
+			dpa_bp_add_8_pages(percpu_priv->dpa_bp, cpu);
+	}
+
+	/* Add skbs to the percpu skb list, reuse var count */
+	count = percpu_priv->skb_count;
+
+	if (unlikely(count < DEFAULT_SKB_COUNT / 4))
+		dpa_list_add_skbs(percpu_priv,
+				  DEFAULT_SKB_COUNT - count);
+#endif
+}
+
+static int dpa_make_shared_port_pool(struct dpa_bp *bp)
+{
+	/*
+	 * In MAC-less and Shared-MAC scenarios the physical
+	 * address of the buffer pool in device tree is set
+	 * to 0 to specify that another entity (USDPAA) will
+	 * allocate and seed the buffers
+	 */
+	if (!bp->paddr)
+		return 0;
+
+	devm_request_mem_region(bp->dev, bp->paddr,
+			bp->size * bp->config_count, KBUILD_MODNAME);
+	bp->vaddr = devm_ioremap_prot(bp->dev, bp->paddr,
+			bp->size * bp->config_count, 0);
+	if (bp->vaddr == NULL) {
+		cpu_pr_err("Could not map memory for pool %d\n", bp->bpid);
+		return -EIO;
+	}
+
+	if (bp->seed_pool)
+		dpaa_eth_seed_pool(bp);
+
+	return 0;
+}
+
+static int __devinit __must_check __attribute__((nonnull))
+dpa_bp_alloc(struct dpa_bp *dpa_bp)
+{
+	int err;
+	struct bman_pool_params	 bp_params;
+	struct platform_device *pdev;
+
+	BUG_ON(dpa_bp->size == 0);
+	BUG_ON(dpa_bp->config_count == 0);
+
+	bp_params.flags = BMAN_POOL_FLAG_DEPLETION;
+	bp_params.cb = dpa_bp_depletion;
+	bp_params.cb_ctx = dpa_bp;
+
+	/* We support two options.  Either a global shared pool, or
+	 * a specified pool. If the pool is specified, we only
+	 * create one per bpid */
+	if (dpa_bp->kernel_pool && default_pool) {
+		atomic_inc(&default_pool->refs);
+		return 0;
+	}
+
+	if (dpa_bp_array[dpa_bp->bpid]) {
+		atomic_inc(&dpa_bp_array[dpa_bp->bpid]->refs);
+		return 0;
+	}
+
+	if (dpa_bp->bpid == 0)
+		bp_params.flags |= BMAN_POOL_FLAG_DYNAMIC_BPID;
+	else
+		bp_params.bpid = dpa_bp->bpid;
+
+	dpa_bp->pool = bman_new_pool(&bp_params);
+	if (unlikely(dpa_bp->pool == NULL)) {
+		cpu_pr_err("bman_new_pool() failed\n");
+		return -ENODEV;
+	}
+
+	dpa_bp->bpid = bman_get_params(dpa_bp->pool)->bpid;
+
+	pdev = platform_device_register_simple("dpaa_eth_bpool",
+			dpa_bp->bpid, NULL, 0);
+	if (IS_ERR(pdev)) {
+		err = PTR_ERR(pdev);
+		goto pdev_register_failed;
+	}
+
+	err = dma_set_mask(&pdev->dev, DMA_BIT_MASK(40));
+	if (err)
+		goto pdev_mask_failed;
+
+	dpa_bp->dev = &pdev->dev;
+
+	if (dpa_bp->kernel_pool) {
+		dpa_make_private_pool(dpa_bp);
+		if (!default_pool)
+			default_pool = dpa_bp;
+	} else {
+		err = dpa_make_shared_port_pool(dpa_bp);
+		if (err)
+			goto make_shared_pool_failed;
+	}
+
+	dpa_bp_array[dpa_bp->bpid] = dpa_bp;
+
+	atomic_set(&dpa_bp->refs, 1);
+
+	return 0;
+
+make_shared_pool_failed:
+pdev_mask_failed:
+	platform_device_unregister(pdev);
+pdev_register_failed:
+	bman_free_pool(dpa_bp->pool);
+
+	return err;
+}
+
+#ifndef CONFIG_DPAA_ETH_SG_SUPPORT
+static inline void _dpa_bp_free_buf(void *addr)
+{
+	struct sk_buff **skbh = addr;
+	struct sk_buff *skb;
+
+	skb = *skbh;
+	dev_kfree_skb_any(skb);
+}
+#else
+static inline void _dpa_bp_free_buf(void *addr)
+{
+	free_page((unsigned long)addr);
+}
+#endif
+
+static void __cold __attribute__((nonnull))
+_dpa_bp_free(struct dpa_bp *dpa_bp)
+{
+	struct dpa_bp *bp = dpa_bpid2pool(dpa_bp->bpid);
+
+	if (!atomic_dec_and_test(&bp->refs))
+		return;
+
+	if (bp->kernel_pool) {
+		int num;
+
+		do {
+			struct bm_buffer bmb[8];
+			int i;
+
+			num = bman_acquire(bp->pool, bmb, 8, 0);
+
+			for (i = 0; i < num; i++) {
+				dma_addr_t addr = bm_buf_addr(&bmb[i]);
+
+				dma_unmap_single(bp->dev, addr, bp->size,
+						DMA_BIDIRECTIONAL);
+
+				_dpa_bp_free_buf(phys_to_virt(addr));
+			}
+		} while (num == 8);
+	}
+
+	dpa_bp_array[bp->bpid] = 0;
+	bman_free_pool(bp->pool);
+}
+
+static void __cold __attribute__((nonnull))
+dpa_bp_free(struct dpa_priv_s *priv, struct dpa_bp *dpa_bp)
+{
+	int i;
+
+	for (i = 0; i < priv->bp_count; i++)
+		_dpa_bp_free(&priv->dpa_bp[i]);
+}
+
+/* QM */
+
+static int __devinit __must_check __attribute__((nonnull))
+_dpa_fq_alloc(struct list_head *list, struct dpa_fq *dpa_fq)
+{
+	int			 _errno;
+	const struct dpa_priv_s	*priv;
+	struct device		*dev;
+	struct qman_fq		*fq;
+	struct qm_mcc_initfq	 initfq;
+
+	priv = netdev_priv(dpa_fq->net_dev);
+	dev = dpa_fq->net_dev->dev.parent;
+
+	if (dpa_fq->fqid == 0)
+		dpa_fq->flags |= QMAN_FQ_FLAG_DYNAMIC_FQID;
+
+	dpa_fq->init = !(dpa_fq->flags & QMAN_FQ_FLAG_NO_MODIFY);
+
+	_errno = qman_create_fq(dpa_fq->fqid, dpa_fq->flags, &dpa_fq->fq_base);
+	if (_errno) {
+		dpaa_eth_err(dev, "qman_create_fq() failed\n");
+		return _errno;
+	}
+	fq = &dpa_fq->fq_base;
+
+	if (dpa_fq->init) {
+		initfq.we_mask = QM_INITFQ_WE_FQCTRL;
+		/* FIXME: why would we want to keep an empty FQ in cache? */
+		initfq.fqd.fq_ctrl = QM_FQCTRL_PREFERINCACHE;
+
+		/* FQ placement */
+		initfq.we_mask |= QM_INITFQ_WE_DESTWQ;
+
+		initfq.fqd.dest.channel	= dpa_fq->channel;
+		initfq.fqd.dest.wq = dpa_fq->wq;
+
+		/* Put all egress queues in a congestion group of their own */
+		initfq.we_mask |= QM_INITFQ_WE_CGID;
+		if (dpa_fq->fq_type == FQ_TYPE_TX) {
+			initfq.fqd.fq_ctrl |= QM_FQCTRL_CGE;
+			initfq.fqd.cgid = priv->cgr_data.cgr.cgrid;
+		}
+
+		/* Initialization common to all ingress queues */
+		if (dpa_fq->flags & QMAN_FQ_FLAG_NO_ENQUEUE) {
+			initfq.we_mask |= QM_INITFQ_WE_CONTEXTA;
+			initfq.fqd.fq_ctrl |=
+				QM_FQCTRL_CTXASTASHING | QM_FQCTRL_AVOIDBLOCK;
+			initfq.fqd.context_a.stashing.exclusive =
+				QM_STASHING_EXCL_DATA | QM_STASHING_EXCL_CTX |
+				QM_STASHING_EXCL_ANNOTATION;
+			initfq.fqd.context_a.stashing.data_cl = 2;
+			initfq.fqd.context_a.stashing.annotation_cl = 1;
+			initfq.fqd.context_a.stashing.context_cl =
+				DIV_ROUND_UP(sizeof(struct qman_fq), 64);
+		};
+
+		_errno = qman_init_fq(fq, QMAN_INITFQ_FLAG_SCHED, &initfq);
+		if (_errno < 0) {
+			dpaa_eth_err(dev, "qman_init_fq(%u) = %d\n",
+					qman_fq_fqid(fq), _errno);
+			qman_destroy_fq(fq, 0);
+			return _errno;
+		}
+	}
+
+	dpa_fq->fqid = qman_fq_fqid(fq);
+	list_add_tail(&dpa_fq->list, list);
+
+	return 0;
+}
+
+static int __cold __attribute__((nonnull))
+_dpa_fq_free(struct device *dev, struct qman_fq *fq)
+{
+	int			 _errno, __errno;
+	struct dpa_fq		*dpa_fq;
+	const struct dpa_priv_s	*priv;
+
+	_errno = 0;
+
+	dpa_fq = container_of(fq, struct dpa_fq, fq_base);
+	priv = netdev_priv(dpa_fq->net_dev);
+
+	if (dpa_fq->init) {
+		_errno = qman_retire_fq(fq, NULL);
+		if (unlikely(_errno < 0) && netif_msg_drv(priv))
+			dpaa_eth_err(dev, "qman_retire_fq(%u) = %d\n",
+					qman_fq_fqid(fq), _errno);
+
+		__errno = qman_oos_fq(fq);
+		if (unlikely(__errno < 0) && netif_msg_drv(priv)) {
+			dpaa_eth_err(dev, "qman_oos_fq(%u) = %d\n",
+					qman_fq_fqid(fq), __errno);
+			if (_errno >= 0)
+				_errno = __errno;
+		}
+	}
+
+	qman_destroy_fq(fq, 0);
+	list_del(&dpa_fq->list);
+
+	return _errno;
+}
+
+static int __cold __attribute__((nonnull))
+dpa_fq_free(struct device *dev, struct list_head *list)
+{
+	int		 _errno, __errno;
+	struct dpa_fq	*dpa_fq, *tmp;
+
+	_errno = 0;
+	list_for_each_entry_safe(dpa_fq, tmp, list, list) {
+		__errno = _dpa_fq_free(dev, (struct qman_fq *)dpa_fq);
+		if (unlikely(__errno < 0) && _errno >= 0)
+			_errno = __errno;
+	}
+
+	return _errno;
+}
+
+static inline void * __must_check __attribute__((nonnull))
+dpa_phys2virt(const struct dpa_bp *dpa_bp, dma_addr_t addr)
+{
+	return dpa_bp->vaddr + (addr - dpa_bp->paddr);
+}
+
+static void
+dpa_release_sgt(struct qm_sg_entry *sgt, struct dpa_bp *dpa_bp,
+		struct bm_buffer *bmb)
+{
+	int i = 0, j;
+
+	do {
+		dpa_bp = dpa_bpid2pool(sgt[i].bpid);
+		BUG_ON(IS_ERR(dpa_bp));
+
+		j = 0;
+		do {
+			BUG_ON(sgt[i].extension);
+
+			bmb[j].hi       = sgt[i].addr_hi;
+			bmb[j].lo       = sgt[i].addr_lo;
+
+			j++; i++;
+		} while (j < ARRAY_SIZE(bmb) &&
+				!sgt[i-1].final &&
+				sgt[i-1].bpid == sgt[i].bpid);
+
+		while (bman_release(dpa_bp->pool, bmb, j, 0))
+			cpu_relax();
+	} while (!sgt[i-1].final);
+}
+
+static void
+dpa_fd_release_sg(const struct net_device *net_dev,
+			const struct qm_fd *fd)
+{
+	const struct dpa_priv_s		*priv;
+	struct qm_sg_entry		*sgt;
+	struct dpa_bp			*_dpa_bp, *dpa_bp;
+	struct bm_buffer		 _bmb, bmb[8];
+
+	priv = netdev_priv(net_dev);
+
+	_bmb.hi	= fd->addr_hi;
+	_bmb.lo	= fd->addr_lo;
+
+	_dpa_bp = dpa_bpid2pool(fd->bpid);
+
+	if (_dpa_bp->vaddr) {
+		sgt = dpa_phys2virt(_dpa_bp, bm_buf_addr(&_bmb)) +
+					dpa_fd_offset(fd);
+		dpa_release_sgt(sgt, dpa_bp, bmb);
+	} else {
+		sgt = kmalloc(DPA_SGT_MAX_ENTRIES * sizeof(*sgt), GFP_ATOMIC);
+		if (sgt == NULL) {
+			if (netif_msg_tx_err(priv) && net_ratelimit())
+				cpu_netdev_err(net_dev,
+					"Memory allocation failed\n");
+			return;
+		}
+
+		copy_from_unmapped_area(sgt, bm_buf_addr(&_bmb) +
+						dpa_fd_offset(fd),
+					min(DPA_SGT_MAX_ENTRIES * sizeof(*sgt),
+						_dpa_bp->size));
+		dpa_release_sgt(sgt, dpa_bp, bmb);
+		kfree(sgt);
+	}
+
+	while (bman_release(_dpa_bp->pool, &_bmb, 1, 0))
+		cpu_relax();
+}
+
+void __attribute__((nonnull))
+dpa_fd_release(const struct net_device *net_dev, const struct qm_fd *fd)
+{
+	const struct dpa_priv_s		*priv;
+	struct qm_sg_entry		*sgt;
+	struct dpa_bp			*_dpa_bp, *dpa_bp;
+	struct bm_buffer		 _bmb, bmb[8];
+
+	priv = netdev_priv(net_dev);
+
+	_bmb.hi	= fd->addr_hi;
+	_bmb.lo	= fd->addr_lo;
+
+	_dpa_bp = dpa_bpid2pool(fd->bpid);
+	BUG_ON(IS_ERR(_dpa_bp));
+
+	if (fd->format == qm_fd_sg) {
+		sgt = (phys_to_virt(bm_buf_addr(&_bmb)) + dpa_fd_offset(fd));
+		dpa_release_sgt(sgt, dpa_bp, bmb);
+	}
+
+	while (bman_release(_dpa_bp->pool, &_bmb, 1, 0))
+		cpu_relax();
+}
+
+#ifndef CONFIG_DPAA_ETH_SG_SUPPORT
+/*
+ * Cleanup function for outgoing frame descriptors that were built on Tx path,
+ * either contiguous frames or scatter/gather ones with a single data buffer.
+ * Skb freeing is not handled here.
+ *
+ * This function may be called on error paths in the Tx function, so guard
+ * against cases when not all fd relevant fields were filled in.
+ *
+ * Return the skb backpointer, since for S/G frames the buffer containing it
+ * gets freed here.
+ */
+struct sk_buff *_dpa_cleanup_tx_fd(const struct dpa_priv_s *priv,
+			       const struct qm_fd *fd)
+{
+	dma_addr_t addr = qm_fd_addr(fd);
+	dma_addr_t sg_addr;
+	struct dpa_bp *bp = priv->dpa_bp;
+	struct sk_buff **skbh;
+	struct sk_buff *skb = NULL;
+
+	BUG_ON(!fd);
+
+	if (unlikely(!addr))
+		return skb;
+
+	skbh = (struct sk_buff **)phys_to_virt(addr);
+
+	if (fd->format == qm_fd_contig) {
+		/* For contiguous frames, just unmap data buffer;
+		 * mapping direction depends on whether the frame was
+		 * meant to be recycled or not */
+		if (fd->cmd & FM_FD_CMD_FCO)
+			dma_unmap_single(bp->dev, addr, bp->size,
+					 DMA_BIDIRECTIONAL);
+		else
+			dma_unmap_single(bp->dev, addr, bp->size,
+					 DMA_TO_DEVICE);
+		/* Retrieve the skb backpointer */
+		skb = *skbh;
+	} else {
+		/* For s/g, we need to unmap both the SGT buffer and the
+		 * data buffer, and also free the SGT buffer */
+		struct qm_sg_entry *sg_entry;
+		void *vaddr = phys_to_virt(addr);
+
+		/* Unmap first buffer (contains S/G table) */
+		dma_unmap_single(bp->dev, addr, SGT_BUFFER_SIZE,
+				 DMA_TO_DEVICE);
+
+		/* Unmap data buffer */
+		sg_entry = (struct qm_sg_entry *)(vaddr + fd->offset);
+		sg_addr = qm_sg_addr(sg_entry);
+		if (likely(sg_addr))
+			dma_unmap_single(bp->dev, sg_addr, bp->size,
+					 DMA_TO_DEVICE);
+		/* Retrieve the skb backpointer */
+		skb = *skbh;
+
+		/* Free first buffer (which was allocated on Tx) */
+		kfree(vaddr);
+	}
+
+	return skb;
+}
+#endif /* CONFIG_DPAA_ETH_SG_SUPPORT */
+
+/* net_device */
+
+static struct net_device_stats * __cold
+dpa_get_stats(struct net_device *net_dev)
+{
+	struct dpa_priv_s *priv = netdev_priv(net_dev);
+	unsigned long *netstats;
+	unsigned long *cpustats;
+	int i, j;
+	struct dpa_percpu_priv_s	*percpu_priv;
+	int numstats = sizeof(net_dev->stats) / sizeof(unsigned long);
+
+	netstats = (unsigned long *)&net_dev->stats;
+
+	memset(netstats, 0, sizeof(net_dev->stats));
+
+	for_each_online_cpu(i) {
+		percpu_priv = per_cpu_ptr(priv->percpu_priv, i);
+
+		cpustats = (unsigned long *)&percpu_priv->stats;
+
+		for (j = 0; j < numstats; j++)
+			netstats[j] += cpustats[j];
+	}
+
+	return &net_dev->stats;
+}
+
+static int dpa_change_mtu(struct net_device *net_dev, int new_mtu)
+{
+	const int max_mtu = dpa_get_max_mtu();
+	const int min_mtu = dpa_get_min_mtu();
+
+	/* Make sure we don't exceed the Ethernet controller's MAXFRM */
+	if (new_mtu < min_mtu || new_mtu > max_mtu) {
+		cpu_netdev_err(net_dev, "Invalid L3 mtu %d "
+				"(must be between %d and %d).\n",
+				new_mtu, min_mtu, max_mtu);
+		return -EINVAL;
+	}
+	net_dev->mtu = new_mtu;
+
+	return 0;
+}
+
+/* .ndo_init callback */
+static int dpa_ndo_init(struct net_device *net_dev)
+{
+	/*
+	 * If fsl_fm_max_frm is set to a higher value than the all-common 1500,
+	 * we choose conservatively and let the user explicitly set a higher
+	 * MTU via ifconfig. Otherwise, the user may end up with different MTUs
+	 * in the same LAN.
+	 * If on the other hand fsl_fm_max_frm has been chosen below 1500,
+	 * start with the maximum allowed.
+	 */
+	int init_mtu = min(dpa_get_max_mtu(), ETH_DATA_LEN);
+
+	pr_debug("Setting initial MTU on net device: %d\n", init_mtu);
+	net_dev->mtu = init_mtu;
+
+	return 0;
+}
+
+static int dpa_set_mac_address(struct net_device *net_dev, void *addr)
+{
+	const struct dpa_priv_s	*priv;
+	int			 _errno;
+
+	priv = netdev_priv(net_dev);
+
+	_errno = eth_mac_addr(net_dev, addr);
+	if (_errno < 0) {
+		if (netif_msg_drv(priv))
+			cpu_netdev_err(net_dev,
+				       "eth_mac_addr() = %d\n",
+				       _errno);
+		return _errno;
+	}
+
+	if (!priv->mac_dev)
+		/* MAC-less interface, so nothing more to do here */
+		return 0;
+
+	_errno = priv->mac_dev->change_addr(priv->mac_dev, net_dev->dev_addr);
+	if (_errno < 0) {
+		if (netif_msg_drv(priv))
+			cpu_netdev_err(net_dev,
+				       "mac_dev->change_addr() = %d\n",
+				       _errno);
+		return _errno;
+	}
+
+	return 0;
+}
+
+static void dpa_set_rx_mode(struct net_device *net_dev)
+{
+	int			 _errno;
+	const struct dpa_priv_s	*priv;
+
+	priv = netdev_priv(net_dev);
+
+	if (!priv->mac_dev)
+		return;
+
+	if (!!(net_dev->flags & IFF_PROMISC) != priv->mac_dev->promisc) {
+		_errno = priv->mac_dev->change_promisc(priv->mac_dev);
+		if (unlikely(_errno < 0) && netif_msg_drv(priv))
+			cpu_netdev_err(net_dev,
+					   "mac_dev->change_promisc() = %d\n",
+					   _errno);
+	}
+
+	_errno = priv->mac_dev->set_multi(net_dev);
+	if (unlikely(_errno < 0) && netif_msg_drv(priv))
+		cpu_netdev_err(net_dev, "mac_dev->set_multi() = %d\n", _errno);
+}
+
+#ifdef CONFIG_FSL_DPA_1588
+static int dpa_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)
+{
+	struct dpa_priv_s *priv = netdev_priv(dev);
+	int ret = 0;
+
+	if (!netif_running(dev))
+		return -EINVAL;
+
+	if ((cmd >= PTP_ENBL_TXTS_IOCTL) && (cmd <= PTP_CLEANUP_TS)) {
+		if (priv->tsu && priv->tsu->valid)
+			ret = dpa_ioctl_1588(dev, rq, cmd);
+		else
+			ret = -ENODEV;
+	}
+
+	return ret;
+}
+#endif
+
+#ifndef CONFIG_DPAA_ETH_SG_SUPPORT
+/*
+ * When we put the buffer into the pool, we purposefully added
+ * some padding to the address so that the buffers wouldn't all
+ * be page-aligned. But the skb has been reset to a default state,
+ * so it is pointing up to DPAA_ETH_MAX_PAD - L1_CACHE_BYTES bytes
+ * before the actual data. We subtract skb->head from the fd addr,
+ * and then mask off the translated part to get the actual distance.
+ */
+static int dpa_process_one(struct dpa_percpu_priv_s *percpu_priv,
+		struct sk_buff *skb, struct dpa_bp *bp, const struct qm_fd *fd)
+{
+	dma_addr_t addr = qm_fd_addr(fd);
+	u32 addrlo = lower_32_bits(addr);
+	u32 skblo = lower_32_bits((unsigned long)skb->head);
+	u32 pad = (addrlo - skblo) & (PAGE_SIZE - 1);
+	unsigned int data_start;
+
+	(*percpu_priv->dpa_bp_count)--;
+
+	/*
+	 * The skb is currently pointed at head + headroom. The packet
+	 * starts at skb->head + pad + fd offset.
+	 */
+	data_start = pad + dpa_fd_offset(fd) - skb_headroom(skb);
+	skb_put(skb, dpa_fd_length(fd) + data_start);
+	skb_pull(skb, data_start);
+
+	return 0;
+}
+#endif
+
+/*
+ * Checks whether the checksum field in Parse Results array is valid
+ * (equals 0xFFFF) and increments the .cse counter otherwise
+ */
+static inline void
+dpa_csum_validation(const struct dpa_priv_s	*priv,
+		struct dpa_percpu_priv_s *percpu_priv,
+		const struct qm_fd *fd)
+{
+	dma_addr_t addr = qm_fd_addr(fd);
+	struct dpa_bp *dpa_bp = priv->dpa_bp;
+	void *frm = phys_to_virt(addr);
+	t_FmPrsResult *parse_result;
+
+	if (unlikely(!frm))
+		return;
+
+	dma_unmap_single(dpa_bp->dev, addr, dpa_bp->size, DMA_BIDIRECTIONAL);
+
+	parse_result = (t_FmPrsResult *)(frm + DPA_RX_PRIV_DATA_SIZE);
+
+	if (parse_result->cksum != DPA_CSUM_VALID)
+		percpu_priv->rx_errors.cse++;
+}
+
+static void _dpa_rx_error(struct net_device *net_dev,
+		const struct dpa_priv_s	*priv,
+		struct dpa_percpu_priv_s *percpu_priv,
+		const struct qm_fd *fd,
+		u32 fqid)
+{
+	/*
+	 * limit common, possibly innocuous Rx FIFO Overflow errors'
+	 * interference with zero-loss convergence benchmark results.
+	 */
+	if (likely(fd->status & FM_FD_STAT_ERR_PHYSICAL))
+		pr_warn_once("fsl-dpa: non-zero error counters " \
+			"in fman statistics (sysfs)\n");
+	else
+		if (netif_msg_hw(priv) && net_ratelimit())
+			cpu_netdev_err(net_dev, "FD status = 0x%08x\n",
+					fd->status & FM_FD_STAT_ERRORS);
+
+	if (dpaa_eth_hooks.rx_error &&
+		dpaa_eth_hooks.rx_error(net_dev, fd, fqid) == DPAA_ETH_STOLEN)
+		/* it's up to the hook to perform resource cleanup */
+		return;
+
+	percpu_priv->stats.rx_errors++;
+
+	if (fd->status & FM_PORT_FRM_ERR_DMA)
+		percpu_priv->rx_errors.dme++;
+	if (fd->status & FM_PORT_FRM_ERR_PHYSICAL)
+		percpu_priv->rx_errors.fpe++;
+	if (fd->status & FM_PORT_FRM_ERR_SIZE)
+		percpu_priv->rx_errors.fse++;
+	if (fd->status & FM_PORT_FRM_ERR_PRS_HDR_ERR)
+		percpu_priv->rx_errors.phe++;
+	if (fd->status & FM_FD_STAT_L4CV)
+		dpa_csum_validation(priv, percpu_priv, fd);
+
+	dpa_fd_release(net_dev, fd);
+}
+
+static void _dpa_tx_error(struct net_device		*net_dev,
+			  const struct dpa_priv_s	*priv,
+			  struct dpa_percpu_priv_s	*percpu_priv,
+			  const struct qm_fd		*fd,
+			  u32				 fqid)
+{
+	struct sk_buff *skb;
+
+	if (netif_msg_hw(priv) && net_ratelimit())
+		cpu_netdev_warn(net_dev, "FD status = 0x%08x\n",
+				fd->status & FM_FD_STAT_ERRORS);
+
+	if (dpaa_eth_hooks.tx_error &&
+		dpaa_eth_hooks.tx_error(net_dev, fd, fqid) == DPAA_ETH_STOLEN)
+		/* now the hook must ensure proper cleanup */
+		return;
+
+	percpu_priv->stats.tx_errors++;
+
+	skb = _dpa_cleanup_tx_fd(priv, fd);
+	dev_kfree_skb(skb);
+}
+
+/*
+ * Helper function to factor out frame validation logic on all Rx paths. Its
+ * purpose is to extract from the Parse Results structure information about
+ * the integrity of the frame, its checksum, the length of the parsed headers
+ * and whether the frame is suitable for GRO.
+ *
+ * @skb		will have its ip_summed field overwritten;
+ * @use_gro	will only be written with 0, if the frame is definitely not
+ *		GRO-able; otherwise, it will be left unchanged;
+ * @hdr_size	will be written with a safe value, at least the size of the
+ *		headers' length.
+ *
+ * Returns 0 if the frame contained no detectable error (including if the FMan
+ * Parser has not in fact been running), and a non-zero value if the Parser
+ * has run but encountered an error.
+ */
+int __hot _dpa_process_parse_results(const t_FmPrsResult *parse_results,
+	const struct qm_fd *fd,
+	struct sk_buff *skb,
+	int *use_gro,
+	unsigned int *hdr_size __maybe_unused)
+{
+	if (likely(fm_l4_hxs_has_run(parse_results))) {
+		/*
+		 * Was there any parsing error? Note: this includes the check
+		 * for a valid L4 checksum.
+		 */
+		if (unlikely(fm_l4_hxs_error(parse_results)))
+			/* Leave it to the caller to handle the frame. */
+			return parse_results->l4r;
+
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+		/*
+		 * If the HXS Parser has successfully run, we can reduce the
+		 * number of bytes we'll memcopy into skb->data.
+		 */
+		*hdr_size = parse_results->nxthdr_off;
+#endif
+		/*
+		 * We know the frame is valid. But has the L4 checksum actually
+		 * been validated? (The L4CV bit is only set if the frame is
+		 * TCP or UDP-with-non-zero-csum.)
+		 */
+		if (fd->status & FM_FD_STAT_L4CV)
+			skb->ip_summed = CHECKSUM_UNNECESSARY;
+		else
+			/*
+			 * If it turns out to be a 0-csum UDP, the stack will
+			 * figure it out itself later, sparing us an extra
+			 * check here on the fastpath of every incoming frame.
+			 */
+			skb->ip_summed = CHECKSUM_NONE;
+
+		/*
+		 * Don't go through GRO for certain types of traffic that
+		 * we know are not GRO-able, such as dgram-based protocols.
+		 * In the worst-case scenarios, such as small-pkt terminating
+		 * UDP, the extra GRO processing would be overkill.
+		 *
+		 * The only protocol the Parser supports that is also GRO-able
+		 * is currently TCP.
+		 */
+		if (!fm_l4_frame_is_tcp(parse_results))
+			*use_gro = 0;
+	} else {
+		/* Inform the stack that we haven't done any csum validation. */
+		skb->ip_summed = CHECKSUM_NONE;
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+		/*
+		 * Also, since the Parser hasn't run, we don't know the size of
+		 * the headers, so we fall back to a safe default.
+		 */
+		*hdr_size = min((ssize_t)DPA_COPIED_HEADERS_SIZE,
+				dpa_fd_length(fd));
+#endif
+		/*
+		 * Bypass GRO for unknown traffic or if no PCDs are applied.
+		 * It's unlikely that a GRO handler is installed for this proto
+		 * or, if it is, user does not seem to care about performance
+		 * (otherwise, PCDs would have been in place).
+		 */
+		*use_gro = 0;
+	}
+
+	return 0;
+}
+
+#ifndef CONFIG_DPAA_ETH_SG_SUPPORT
+void __hot _dpa_rx(struct net_device *net_dev,
+		const struct dpa_priv_s *priv,
+		struct dpa_percpu_priv_s *percpu_priv,
+		const struct qm_fd *fd,
+		u32 fqid)
+{
+	struct dpa_bp *dpa_bp;
+	struct sk_buff *skb;
+	struct sk_buff **skbh;
+	dma_addr_t addr = qm_fd_addr(fd);
+	u32 fd_status = fd->status;
+	unsigned int skb_len;
+	t_FmPrsResult *parse_result;
+	int ret;
+	unsigned int hdr_size_unused;
+	int use_gro = net_dev->features & NETIF_F_GRO;
+
+	skbh = (struct sk_buff **)phys_to_virt(addr);
+
+	if (unlikely(fd_status & FM_FD_STAT_ERRORS) != 0) {
+		if (netif_msg_hw(priv) && net_ratelimit())
+			cpu_netdev_warn(net_dev, "FD status = 0x%08x\n",
+					fd->status & FM_FD_STAT_ERRORS);
+
+		percpu_priv->stats.rx_errors++;
+
+		goto _return_dpa_fd_release;
+	}
+
+	if (unlikely(fd->format != qm_fd_contig)) {
+		percpu_priv->stats.rx_dropped++;
+		if (netif_msg_rx_status(priv) && net_ratelimit())
+			cpu_netdev_warn(net_dev, "Dropping a SG frame\n");
+		goto _return_dpa_fd_release;
+	}
+
+	dpa_bp = dpa_bpid2pool(fd->bpid);
+
+	dma_unmap_single(dpa_bp->dev, addr, dpa_bp->size, DMA_BIDIRECTIONAL);
+
+	skb = *skbh;
+	prefetch(skb);
+
+	/* Fill the SKB */
+	dpa_process_one(percpu_priv, skb, dpa_bp, fd);
+
+	prefetch(skb_shinfo(skb));
+
+#ifdef CONFIG_FSL_DPA_1588
+	if (priv->tsu && priv->tsu->valid && priv->tsu->hwts_rx_en_ioctl)
+		dpa_ptp_store_rxstamp(net_dev, skb, fd);
+#endif
+
+	skb->protocol = eth_type_trans(skb, net_dev);
+
+	if (unlikely(dpa_check_rx_mtu(skb, net_dev->mtu))) {
+		percpu_priv->stats.rx_dropped++;
+		goto drop_large_frame;
+	}
+
+	/* Execute the Rx processing hook, if it exists. */
+	if (dpaa_eth_hooks.rx_default && dpaa_eth_hooks.rx_default(skb,
+		net_dev, fqid) == DPAA_ETH_STOLEN)
+		/* won't count the rx bytes in */
+		goto skb_stolen;
+
+	skb_len = skb->len;
+
+	/* Validate the skb csum and figure out whether GRO is appropriate */
+	parse_result = (t_FmPrsResult *)((u8 *)skbh + DPA_RX_PRIV_DATA_SIZE);
+	ret = _dpa_process_parse_results(parse_result, fd, skb, &use_gro,
+					 &hdr_size_unused);
+	if (unlikely(ret)) {
+		percpu_priv->l4_hxs_errors++;
+		percpu_priv->stats.rx_dropped++;
+		goto drop_invalid_frame;
+	}
+	if (use_gro) {
+		gro_result_t gro_result;
+
+		gro_result = napi_gro_receive(&percpu_priv->napi, skb);
+		if (unlikely(gro_result == GRO_DROP)) {
+			percpu_priv->stats.rx_dropped++;
+			goto packet_dropped;
+		}
+	} else if (unlikely(netif_receive_skb(skb) == NET_RX_DROP)) {
+		percpu_priv->stats.rx_dropped++;
+		goto packet_dropped;
+	}
+
+	percpu_priv->stats.rx_packets++;
+	percpu_priv->stats.rx_bytes += skb_len;
+
+packet_dropped:
+skb_stolen:
+	net_dev->last_rx = jiffies;
+	return;
+
+drop_invalid_frame:
+drop_large_frame:
+	dev_kfree_skb(skb);
+	return;
+
+_return_dpa_fd_release:
+	dpa_fd_release(net_dev, fd);
+}
+#endif /* CONFIG_DPAA_ETH_SG_SUPPORT */
+
+static void dpaa_eth_napi_disable(struct dpa_priv_s *priv)
+{
+	struct dpa_percpu_priv_s *percpu_priv;
+	int i;
+
+	if (priv->shared)
+		return;
+
+	for_each_online_cpu(i) {
+		percpu_priv = per_cpu_ptr(priv->percpu_priv, i);
+		napi_disable(&percpu_priv->napi);
+	}
+}
+
+static void dpaa_eth_napi_enable(struct dpa_priv_s *priv)
+{
+	struct dpa_percpu_priv_s *percpu_priv;
+	int i;
+
+	if (priv->shared)
+		return;
+
+	for_each_online_cpu(i) {
+		percpu_priv = per_cpu_ptr(priv->percpu_priv, i);
+		napi_enable(&percpu_priv->napi);
+	}
+}
+
+static int dpaa_eth_poll(struct napi_struct *napi, int budget)
+{
+	int cleaned = qman_poll_dqrr(budget);
+
+	if (cleaned < budget) {
+		int tmp;
+		napi_complete(napi);
+		tmp = qman_irqsource_add(QM_PIRQ_DQRI);
+		BUG_ON(tmp);
+	}
+
+	return cleaned;
+}
+
+static void __hot _dpa_tx_conf(struct net_device	*net_dev,
+			  const struct dpa_priv_s	*priv,
+			  struct dpa_percpu_priv_s	*percpu_priv,
+			  const struct qm_fd		*fd,
+			  u32				 fqid)
+{
+	struct sk_buff	*skb;
+
+	if (unlikely(fd->status & FM_FD_STAT_ERRORS) != 0) {
+		if (netif_msg_hw(priv) && net_ratelimit())
+			cpu_netdev_warn(net_dev, "FD status = 0x%08x\n",
+					fd->status & FM_FD_STAT_ERRORS);
+
+		percpu_priv->stats.tx_errors++;
+	}
+
+	if (dpaa_eth_hooks.tx_confirm && dpaa_eth_hooks.tx_confirm(net_dev,
+		fd, fqid) == DPAA_ETH_STOLEN)
+		/* it's the hook that must now perform cleanup */
+		return;
+
+	/* This might not perfectly reflect the reality, if the core dequeueing
+	 * the Tx confirmation is different from the one that did the enqueue,
+	 * but at least it'll show up in the total count. */
+	percpu_priv->tx_confirm++;
+
+	skb = _dpa_cleanup_tx_fd(priv, fd);
+
+#ifdef CONFIG_FSL_DPA_1588
+	if (priv->tsu && priv->tsu->valid && priv->tsu->hwts_tx_en_ioctl)
+		dpa_ptp_store_txstamp(net_dev, skb, fd);
+#endif
+	dev_kfree_skb(skb);
+}
+
+static struct dpa_bp *dpa_size2pool(struct dpa_priv_s *priv, size_t size)
+{
+	int i;
+
+	for (i = 0; i < priv->bp_count; i++)
+		if (DPA_BP_SIZE(size) <= priv->dpa_bp[i].size)
+			return dpa_bpid2pool(priv->dpa_bp[i].bpid);
+	return ERR_PTR(-ENODEV);
+}
+
+/**
+ * Turn on HW checksum computation for this outgoing frame.
+ * If the current protocol is not something we support in this regard
+ * (or if the stack has already computed the SW checksum), we do nothing.
+ *
+ * Returns 0 if all goes well (or HW csum doesn't apply), and a negative value
+ * otherwise.
+ *
+ * Note that this function may modify the fd->cmd field and the skb data buffer
+ * (the Parse Results area).
+ */
+int dpa_enable_tx_csum(struct dpa_priv_s *priv,
+	struct sk_buff *skb, struct qm_fd *fd, char *parse_results)
+{
+	t_FmPrsResult *parse_result;
+	struct iphdr *iph;
+	struct ipv6hdr *ipv6h = NULL;
+	int l4_proto;
+	int ethertype = ntohs(skb->protocol);
+	int retval = 0;
+
+	if (!priv->mac_dev || skb->ip_summed != CHECKSUM_PARTIAL)
+		return 0;
+
+	/* Note: L3 csum seems to be already computed in sw, but we can't choose
+	 * L4 alone from the FM configuration anyway. */
+
+	/* Fill in some fields of the Parse Results array, so the FMan
+	 * can find them as if they came from the FMan Parser. */
+	parse_result = (t_FmPrsResult *)parse_results;
+
+	/* If we're dealing with VLAN, get the real Ethernet type */
+	if (ethertype == ETH_P_8021Q) {
+		/* We can't always assume the MAC header is set correctly
+		 * by the stack, so reset to beginning of skb->data */
+		skb_reset_mac_header(skb);
+		ethertype = ntohs(vlan_eth_hdr(skb)->h_vlan_encapsulated_proto);
+	}
+
+	/* Fill in the relevant L3 parse result fields
+	 * and read the L4 protocol type */
+	switch (ethertype) {
+	case ETH_P_IP:
+		parse_result->l3r = FM_L3_PARSE_RESULT_IPV4;
+		iph = ip_hdr(skb);
+		BUG_ON(iph == NULL);
+		l4_proto = ntohs(iph->protocol);
+		break;
+	case ETH_P_IPV6:
+		parse_result->l3r = FM_L3_PARSE_RESULT_IPV6;
+		ipv6h = ipv6_hdr(skb);
+		BUG_ON(ipv6h == NULL);
+		l4_proto = ntohs(ipv6h->nexthdr);
+		break;
+	default:
+		/* We shouldn't even be here */
+		if (netif_msg_tx_err(priv) && net_ratelimit())
+			cpu_netdev_alert(priv->net_dev, "Can't compute HW csum "
+				"for L3 proto 0x%x\n", ntohs(skb->protocol));
+		retval = -EIO;
+		goto return_error;
+	}
+
+	/* Fill in the relevant L4 parse result fields */
+	switch (l4_proto) {
+	case IPPROTO_UDP:
+		parse_result->l4r = FM_L4_PARSE_RESULT_UDP;
+		break;
+	case IPPROTO_TCP:
+		parse_result->l4r = FM_L4_PARSE_RESULT_TCP;
+		break;
+	default:
+		/* This can as well be a BUG() */
+		if (netif_msg_tx_err(priv) && net_ratelimit())
+			cpu_netdev_alert(priv->net_dev, "Can't compute HW csum "
+				"for L4 proto 0x%x\n", l4_proto);
+		retval = -EIO;
+		goto return_error;
+	}
+
+	/* At index 0 is IPOffset_1 as defined in the Parse Results */
+	parse_result->ip_off[0] = skb_network_offset(skb);
+	parse_result->l4_off = skb_transport_offset(skb);
+
+	/* Enable L3 (and L4, if TCP or UDP) HW checksum. */
+	fd->cmd |= FM_FD_CMD_RPD | FM_FD_CMD_DTC;
+
+	/*
+	 * On P1023 and similar platforms fd->cmd interpretation could
+	 * be disabled by setting CONTEXT_A bit ICMD; currently this bit
+	 * is not set so we do not need to check; in the future, if/when
+	 * using context_a we need to check this bit
+	 */
+
+return_error:
+	return retval;
+}
+
+static int __hot dpa_shared_tx(struct sk_buff *skb, struct net_device *net_dev)
+{
+	struct dpa_bp *dpa_bp;
+	struct bm_buffer bmb;
+	struct dpa_percpu_priv_s *percpu_priv;
+	struct dpa_priv_s *priv;
+	struct qm_fd fd;
+	int queue_mapping;
+	int err;
+	void *dpa_bp_vaddr;
+	t_FmPrsResult parse_results;
+
+	priv = netdev_priv(net_dev);
+	percpu_priv = per_cpu_ptr(priv->percpu_priv, smp_processor_id());
+
+	memset(&fd, 0, sizeof(fd));
+	fd.format = qm_fd_contig;
+
+	queue_mapping = skb_get_queue_mapping(skb);
+
+	dpa_bp = dpa_size2pool(priv, skb_headlen(skb));
+	if (unlikely(IS_ERR(dpa_bp))) {
+		percpu_priv->stats.tx_errors++;
+		err = PTR_ERR(dpa_bp);
+		goto bpools_too_small_error;
+	}
+
+	err = bman_acquire(dpa_bp->pool, &bmb, 1, 0);
+	if (unlikely(err <= 0)) {
+		percpu_priv->stats.tx_errors++;
+		if (err == 0)
+			err = -ENOMEM;
+		goto buf_acquire_failed;
+	}
+	fd.bpid = dpa_bp->bpid;
+
+	fd.length20 = skb_headlen(skb);
+	fd.addr_hi = bmb.hi;
+	fd.addr_lo = bmb.lo;
+	fd.offset = DPA_BP_HEAD;
+
+	/*
+	 * The virtual address of the buffer pool is expected to be NULL
+	 * in scenarios like MAC-less or Shared-MAC between Linux and
+	 * USDPAA. In this case the buffers are dynamically mapped/unmapped.
+	 */
+	if (dpa_bp->vaddr) {
+		dpa_bp_vaddr = dpa_phys2virt(dpa_bp, bm_buf_addr(&bmb));
+
+		/* Copy the packet payload */
+		skb_copy_from_linear_data(skb,
+		                          dpa_bp_vaddr + dpa_fd_offset(&fd),
+		                          dpa_fd_length(&fd));
+
+		/* Enable L3/L4 hardware checksum computation, if applicable */
+		err = dpa_enable_tx_csum(priv, skb, &fd,
+					 dpa_bp_vaddr + DPA_TX_PRIV_DATA_SIZE);
+	} else {
+		err = dpa_enable_tx_csum(priv, skb, &fd,
+					 (char *)&parse_results);
+
+		copy_to_unmapped_area(bm_buf_addr(&bmb) + DPA_TX_PRIV_DATA_SIZE,
+				&parse_results,
+				DPA_PARSE_RESULTS_SIZE);
+
+		copy_to_unmapped_area(bm_buf_addr(&bmb) + dpa_fd_offset(&fd),
+				skb->data,
+				dpa_fd_length(&fd));
+	}
+
+	if (unlikely(err < 0)) {
+		if (netif_msg_tx_err(priv) && net_ratelimit())
+			cpu_netdev_err(net_dev, "Tx HW csum error: %d\n", err);
+		percpu_priv->stats.tx_errors++;
+		goto l3_l4_csum_failed;
+	}
+
+	err = dpa_xmit(priv, percpu_priv, queue_mapping, &fd);
+
+l3_l4_csum_failed:
+bpools_too_small_error:
+buf_acquire_failed:
+	/* We're done with the skb */
+	dev_kfree_skb(skb);
+
+	return NETDEV_TX_OK;
+}
+
+#ifndef CONFIG_DPAA_ETH_SG_SUPPORT
+static int skb_to_sg_fd(struct dpa_priv_s *priv,
+		struct sk_buff *skb, struct qm_fd *fd)
+{
+	struct dpa_bp *dpa_bp = priv->dpa_bp;
+	void *vaddr;
+	dma_addr_t paddr;
+	struct sk_buff **skbh;
+	struct qm_sg_entry *sg_entry;
+	struct net_device *net_dev = priv->net_dev;
+	int err;
+
+	/* Allocate the first buffer in the FD (used for storing S/G table) */
+	vaddr = kmalloc(SGT_BUFFER_SIZE, GFP_ATOMIC);
+	if (unlikely(vaddr == NULL)) {
+		if (netif_msg_tx_err(priv) && net_ratelimit())
+			cpu_netdev_err(net_dev, "Memory allocation failed\n");
+		return -ENOMEM;
+	}
+	/* Store skb backpointer at the beginning of the buffer */
+	skbh = (struct sk_buff **)vaddr;
+	*skbh = skb;
+
+	/* Fill in FD */
+	fd->format = qm_fd_sg;
+	fd->offset = DPA_BP_HEAD;
+	fd->length20 = skb->len;
+
+	/* Enable hardware checksum computation */
+	err = dpa_enable_tx_csum(priv, skb, fd,
+		(char *)vaddr + DPA_TX_PRIV_DATA_SIZE);
+	if (unlikely(err < 0)) {
+		if (netif_msg_tx_err(priv) && net_ratelimit())
+			cpu_netdev_err(net_dev, "HW csum error: %d\n", err);
+		kfree(vaddr);
+		return err;
+	}
+
+	/* Map the buffer and store its address in the FD */
+	paddr = dma_map_single(dpa_bp->dev, vaddr, SGT_BUFFER_SIZE,
+			       DMA_TO_DEVICE);
+	if (unlikely(dma_mapping_error(dpa_bp->dev, paddr))) {
+		if (netif_msg_tx_err(priv) && net_ratelimit())
+			cpu_netdev_err(net_dev, "DMA mapping failed\n");
+		kfree(vaddr);
+		return -EINVAL;
+	}
+
+	fd->addr_hi = upper_32_bits(paddr);
+	fd->addr_lo = lower_32_bits(paddr);
+
+	/* Fill in S/G entry */
+	sg_entry = (struct qm_sg_entry *)(vaddr + fd->offset);
+
+	sg_entry->extension = 0;
+	sg_entry->final = 1;
+	sg_entry->length = skb->len;
+	/*
+	 * Put the same offset in the data buffer as in the SGT (first) buffer.
+	 * This is the format for S/G frames generated by FMan; the manual is
+	 * not clear if same is required of Tx S/G frames, but since we know
+	 * for sure we have at least DPA_BP_HEAD bytes of skb headroom, lets not
+	 * take any chances.
+	 */
+	sg_entry->offset = DPA_BP_HEAD;
+
+	paddr = dma_map_single(dpa_bp->dev, skb->data - sg_entry->offset,
+			       dpa_bp->size, DMA_TO_DEVICE);
+	if (unlikely(dma_mapping_error(dpa_bp->dev, paddr))) {
+		if (netif_msg_tx_err(priv) && net_ratelimit())
+			cpu_netdev_err(net_dev, "DMA mapping failed\n");
+		return -EINVAL;
+	}
+	sg_entry->addr_hi = upper_32_bits(paddr);
+	sg_entry->addr_lo = lower_32_bits(paddr);
+
+	return 0;
+}
+
+static int skb_to_contig_fd(struct dpa_priv_s *priv,
+		struct dpa_percpu_priv_s *percpu_priv,
+		struct sk_buff *skb, struct qm_fd *fd)
+{
+	struct sk_buff **skbh;
+	dma_addr_t addr;
+	struct dpa_bp *dpa_bp = priv->dpa_bp;
+	struct net_device *net_dev = priv->net_dev;
+	enum dma_data_direction dma_dir = DMA_TO_DEVICE;
+	bool can_recycle = false;
+	int offset, extra_offset;
+	int err;
+
+	/*
+	 * We are guaranteed that we have at least DPA_BP_HEAD of headroom.
+	 * Buffers we allocated are padded to improve cache usage. In order
+	 * to increase buffer re-use, we aim to keep any such buffers the
+	 * same. This means the address passed to the FM should be DPA_BP_HEAD
+	 * before the data for forwarded frames.
+	 *
+	 * However, offer some flexibility in fd layout, to allow originating
+	 * (termination) buffers to be also recycled when possible.
+	 *
+	 * First, see if the conditions needed to recycle the skb are met:
+	 * - skb not cloned, not shared
+	 * - buffer size is large enough to accomodate a maximum size Rx frame
+	 * - buffer size does not exceed the maximum size allowed in the pool
+	 *   (to avoid unbounded increase of buffer size in certain forwarding
+	 *   conditions)
+	 * - buffer address is 16 byte aligned, as per DPAARM
+	 * - there's enough room in the buffer pool
+	 */
+	if (likely(skb_is_recycleable(skb, dpa_bp->size) &&
+		   (skb_end_pointer(skb) - skb->head <= DPA_BP_MAX_BUF_SIZE) &&
+		   (*percpu_priv->dpa_bp_count < dpa_bp->target_count))) {
+		/* Compute the minimum necessary fd offset */
+		offset = dpa_bp->size - skb->len - skb_tailroom(skb);
+
+		/*
+		 * And make sure the offset is no lower than DPA_BP_HEAD,
+		 * as required by FMan
+		 */
+		offset = max(offset, (int)DPA_BP_HEAD);
+
+		/*
+		 * We also need to align the buffer address to 16, such that
+		 * Fman will be able to reuse it on Rx.
+		 * Since the buffer going to FMan starts at (skb->data - offset)
+		 * this is what we'll try to align. We already know that
+		 * headroom is at least DPA_BP_HEAD bytes long, but with
+		 * the extra offset needed for alignment we may go beyond
+		 * the beginning of the buffer.
+		 *
+		 * Also need to check that we don't go beyond the maximum
+		 * offset that can be set for a contiguous FD.
+		 */
+		extra_offset = (unsigned long)(skb->data - offset) & 0xF;
+		if (likely((offset + extra_offset) <= skb_headroom(skb) &&
+			   (offset + extra_offset) <= DPA_MAX_FD_OFFSET)) {
+			/* We're good to go for recycling*/
+			offset += extra_offset;
+			can_recycle = true;
+		}
+	}
+
+	if (likely(can_recycle)) {
+		/* Buffer will get recycled, setup fd accordingly */
+		fd->cmd |= FM_FD_CMD_FCO;
+		fd->bpid = dpa_bp->bpid;
+		/*
+		 * Since the buffer will get back to the Bman pool
+		 * and be re-used on Rx, map it for both read and write
+		 */
+		dma_dir = DMA_BIDIRECTIONAL;
+	} else {
+		/*
+		 * No recycling here, so we don't care about address alignment.
+		 * Just use the smallest offset required by FMan
+		 */
+		offset = DPA_BP_HEAD;
+	}
+
+	skbh = (struct sk_buff **)(skb->data - offset);
+	*skbh = skb;
+
+
+	/* Enable L3/L4 hardware checksum computation.
+	 *
+	 * We must do this before dma_map_single(), because we may
+	 * need to write into the skb. */
+	err = dpa_enable_tx_csum(priv, skb, fd,
+				 ((char *)skbh) + DPA_TX_PRIV_DATA_SIZE);
+	if (unlikely(err < 0)) {
+		if (netif_msg_tx_err(priv) && net_ratelimit())
+			cpu_netdev_err(net_dev, "HW csum error: %d\n", err);
+		return err;
+	}
+
+	fd->format = qm_fd_contig;
+	fd->length20 = skb->len;
+	fd->offset = offset;
+
+	addr = dma_map_single(dpa_bp->dev, skbh, dpa_bp->size, dma_dir);
+	if (unlikely(dma_mapping_error(dpa_bp->dev, addr))) {
+		if (netif_msg_tx_err(priv)  && net_ratelimit())
+			cpu_netdev_err(net_dev, "dma_map_single() failed\n");
+		return -EINVAL;
+	}
+
+	fd->addr_hi = upper_32_bits(addr);
+	fd->addr_lo = lower_32_bits(addr);
+
+	return 0;
+}
+
+int __hot dpa_tx(struct sk_buff *skb, struct net_device *net_dev)
+{
+	struct dpa_priv_s	*priv;
+	struct qm_fd		 fd;
+	struct dpa_percpu_priv_s *percpu_priv;
+	int queue_mapping;
+	int err;
+
+	/* If there is a Tx hook, run it. */
+	if (dpaa_eth_hooks.tx &&
+		dpaa_eth_hooks.tx(skb, net_dev) == DPAA_ETH_STOLEN)
+		/* won't update any Tx stats */
+		goto done;
+
+	priv = netdev_priv(net_dev);
+	percpu_priv = per_cpu_ptr(priv->percpu_priv, smp_processor_id());
+
+	clear_fd(&fd);
+	queue_mapping = skb_get_queue_mapping(skb);
+
+	if (skb_headroom(skb) < DPA_BP_HEAD) {
+		struct sk_buff *skb_new;
+
+		skb_new = skb_realloc_headroom(skb, DPA_BP_HEAD);
+		if (unlikely(!skb_new)) {
+			percpu_priv->stats.tx_errors++;
+			kfree_skb(skb);
+			goto done;
+		}
+		kfree_skb(skb);
+		skb = skb_new;
+	}
+
+#ifdef CONFIG_FSL_DPA_1588
+	if (priv->tsu && priv->tsu->valid && priv->tsu->hwts_tx_en_ioctl)
+		fd.cmd |= FM_FD_CMD_UPD;
+#endif
+
+	/*
+	 * We have two paths here:
+	 *
+	 * 1.If the skb is cloned, create a S/G frame to avoid unsharing it.
+	 * The S/G table will contain only one entry, pointing to our skb
+	 * data buffer.
+	 * The private data area containing the skb backpointer will reside
+	 * inside the first buffer, such that it won't risk being overwritten
+	 * in case a second skb pointing to the same data buffer is being
+	 * processed concurently.
+	 * No recycling is possible in this case, as the data buffer is shared.
+	 *
+	 * 2.If skb is not cloned, then the private area inside it can be
+	 * safely used to store the skb backpointer. Simply create a contiguous
+	 * fd in this case.
+	 * Recycling can happen if the right conditions are met.
+	 */
+	if (skb_cloned(skb) && (skb->len > DPA_SKB_COPY_MAX_SIZE))
+		err = skb_to_sg_fd(priv, skb, &fd);
+	else {
+		/* If cloned skb, but length is below DPA_SKB_COPY_MAX_SIZE,
+		 * it's more efficient to unshare it and then use the new skb */
+		skb = skb_unshare(skb, GFP_ATOMIC);
+		if (unlikely(!skb)) {
+			percpu_priv->stats.tx_errors++;
+			goto done;
+		}
+		err = skb_to_contig_fd(priv, percpu_priv, skb, &fd);
+	}
+	if (unlikely(err < 0)) {
+		percpu_priv->stats.tx_errors++;
+		goto fd_create_failed;
+	}
+
+#if (DPAA_VERSION >= 11)
+	fd.cmd &= ~FM_FD_CMD_FCO;
+#endif
+
+	if (fd.cmd & FM_FD_CMD_FCO) {
+		/* This skb is recycleable, and the fd generated from it
+		 * has been filled in accordingly */
+		skb_recycle(skb);
+		skb = NULL;
+		(*percpu_priv->dpa_bp_count)++;
+		percpu_priv->tx_returned++;
+	}
+
+	if (unlikely(dpa_xmit(priv, percpu_priv, queue_mapping, &fd) < 0))
+		goto xmit_failed;
+
+	net_dev->trans_start = jiffies;
+	goto done;
+
+xmit_failed:
+	if (fd.cmd & FM_FD_CMD_FCO) {
+		(*percpu_priv->dpa_bp_count)--;
+		percpu_priv->tx_returned--;
+	}
+fd_create_failed:
+	_dpa_cleanup_tx_fd(priv, &fd);
+	dev_kfree_skb(skb);
+
+done:
+	return NETDEV_TX_OK;
+}
+#endif /* CONFIG_DPAA_ETH_SG_SUPPORT */
+
+/**
+ * Congestion group state change notification callback.
+ * Stops the device's egress queues while they are congested and
+ * wakes them upon exiting congested state.
+ * Also updates some CGR-related stats.
+ */
+static void dpaa_eth_cgscn(struct qman_portal *qm, struct qman_cgr *cgr,
+	int congested)
+{
+	struct dpa_priv_s *priv = (struct dpa_priv_s *)container_of(cgr,
+		struct dpa_priv_s, cgr_data.cgr);
+
+	if (congested) {
+		priv->cgr_data.congestion_start_jiffies = jiffies;
+		netif_tx_stop_all_queues(priv->net_dev);
+		priv->cgr_data.cgr_congested_count++;
+	} else {
+		priv->cgr_data.congested_jiffies +=
+			(jiffies - priv->cgr_data.congestion_start_jiffies);
+		netif_tx_wake_all_queues(priv->net_dev);
+	}
+}
+
+static enum qman_cb_dqrr_result
+ingress_rx_error_dqrr(struct qman_portal		*portal,
+		      struct qman_fq			*fq,
+		      const struct qm_dqrr_entry	*dq)
+{
+	struct net_device		*net_dev;
+	struct dpa_priv_s		*priv;
+	struct dpa_percpu_priv_s	*percpu_priv;
+
+	net_dev = ((struct dpa_fq *)fq)->net_dev;
+	priv = netdev_priv(net_dev);
+
+	percpu_priv = per_cpu_ptr(priv->percpu_priv, smp_processor_id());
+
+	if (dpaa_eth_napi_schedule(percpu_priv)) {
+		percpu_priv->in_interrupt++;
+		return qman_cb_dqrr_stop;
+	}
+
+	dpaa_eth_refill_bpools(percpu_priv);
+	_dpa_rx_error(net_dev, priv, percpu_priv, &dq->fd, fq->fqid);
+
+	return qman_cb_dqrr_consume;
+}
+
+static enum qman_cb_dqrr_result __hot
+shared_rx_dqrr(struct qman_portal *portal, struct qman_fq *fq,
+		const struct qm_dqrr_entry *dq)
+{
+	struct net_device		*net_dev;
+	struct dpa_priv_s		*priv;
+	struct dpa_percpu_priv_s	*percpu_priv;
+	const struct qm_fd *fd = &dq->fd;
+	struct dpa_bp *dpa_bp;
+	struct sk_buff *skb;
+	struct qm_sg_entry *sgt;
+	int i;
+
+	net_dev = ((struct dpa_fq *)fq)->net_dev;
+	priv = netdev_priv(net_dev);
+
+	percpu_priv = per_cpu_ptr(priv->percpu_priv, smp_processor_id());
+
+	dpa_bp = dpa_bpid2pool(fd->bpid);
+	BUG_ON(IS_ERR(dpa_bp));
+
+	if (unlikely(fd->status & FM_FD_STAT_ERRORS) != 0) {
+		if (netif_msg_hw(priv) && net_ratelimit())
+			cpu_netdev_warn(net_dev, "FD status = 0x%08x\n",
+					fd->status & FM_FD_STAT_ERRORS);
+
+		percpu_priv->stats.rx_errors++;
+
+		goto out;
+	}
+
+	skb = __netdev_alloc_skb(net_dev,
+				 DPA_BP_HEAD + dpa_fd_length(fd),
+				 GFP_ATOMIC);
+	if (unlikely(skb == NULL)) {
+		if (netif_msg_rx_err(priv) && net_ratelimit())
+			cpu_netdev_err(net_dev, "Could not alloc skb\n");
+
+		percpu_priv->stats.rx_dropped++;
+
+		goto out;
+	}
+
+	skb_reserve(skb, DPA_BP_HEAD);
+
+	if (fd->format == qm_fd_sg) {
+		if (dpa_bp->vaddr) {
+			sgt = dpa_phys2virt(dpa_bp,
+					    qm_fd_addr(fd)) + dpa_fd_offset(fd);
+
+			for (i = 0; i < DPA_SGT_MAX_ENTRIES; i++) {
+				BUG_ON(sgt[i].extension);
+
+				/* copy from sgt[i] */
+				memcpy(skb_put(skb, sgt[i].length),
+					dpa_phys2virt(dpa_bp,
+							qm_sg_addr(&sgt[i]) +
+							sgt[i].offset),
+					sgt[i].length);
+				if (sgt[i].final)
+					break;
+			}
+		} else {
+			sgt = kmalloc(DPA_SGT_MAX_ENTRIES * sizeof(*sgt),
+					GFP_ATOMIC);
+			if (unlikely(sgt == NULL)) {
+				if (netif_msg_tx_err(priv) && net_ratelimit())
+					cpu_netdev_err(net_dev,
+						"Memory allocation failed\n");
+				return -ENOMEM;
+			}
+
+			copy_from_unmapped_area(sgt,
+					qm_fd_addr(fd) + dpa_fd_offset(fd),
+					min(DPA_SGT_MAX_ENTRIES * sizeof(*sgt),
+							dpa_bp->size));
+
+			for (i = 0; i < DPA_SGT_MAX_ENTRIES; i++) {
+				BUG_ON(sgt[i].extension);
+
+				copy_from_unmapped_area(
+					skb_put(skb, sgt[i].length),
+					qm_sg_addr(&sgt[i]) + sgt[i].offset,
+					sgt[i].length);
+
+				if (sgt[i].final)
+					break;
+			}
+
+			kfree(sgt);
+		}
+		goto skb_copied;
+	}
+
+	/* otherwise fd->format == qm_fd_contig */
+	if (dpa_bp->vaddr) {
+		/* Fill the SKB */
+		memcpy(skb_put(skb, dpa_fd_length(fd)),
+		       dpa_phys2virt(dpa_bp, qm_fd_addr(fd)) +
+		       dpa_fd_offset(fd), dpa_fd_length(fd));
+	} else {
+		copy_from_unmapped_area(skb_put(skb, dpa_fd_length(fd)),
+					qm_fd_addr(fd) + dpa_fd_offset(fd),
+					dpa_fd_length(fd));
+	}
+
+skb_copied:
+	skb->protocol = eth_type_trans(skb, net_dev);
+
+	if (unlikely(dpa_check_rx_mtu(skb, net_dev->mtu))) {
+		percpu_priv->stats.rx_dropped++;
+		dev_kfree_skb_any(skb);
+		goto out;
+	}
+
+	if (unlikely(netif_rx(skb) != NET_RX_SUCCESS))
+		percpu_priv->stats.rx_dropped++;
+	else {
+		percpu_priv->stats.rx_packets++;
+		percpu_priv->stats.rx_bytes += dpa_fd_length(fd);
+	}
+
+	net_dev->last_rx = jiffies;
+
+out:
+	if (fd->format == qm_fd_sg)
+		dpa_fd_release_sg(net_dev, fd);
+	else
+		dpa_fd_release(net_dev, fd);
+
+	return qman_cb_dqrr_consume;
+}
+
+
+static enum qman_cb_dqrr_result __hot
+ingress_rx_default_dqrr(struct qman_portal		*portal,
+			struct qman_fq			*fq,
+			const struct qm_dqrr_entry	*dq)
+{
+	struct net_device		*net_dev;
+	struct dpa_priv_s		*priv;
+	struct dpa_percpu_priv_s	*percpu_priv;
+
+	net_dev = ((struct dpa_fq *)fq)->net_dev;
+	priv = netdev_priv(net_dev);
+
+	percpu_priv = per_cpu_ptr(priv->percpu_priv, smp_processor_id());
+
+	if (unlikely(dpaa_eth_napi_schedule(percpu_priv))) {
+		percpu_priv->in_interrupt++;
+		return qman_cb_dqrr_stop;
+	}
+
+	prefetchw(&percpu_priv->ingress_calls);
+
+	/* Vale of plenty: make sure we didn't run out of buffers */
+	dpaa_eth_refill_bpools(percpu_priv);
+	_dpa_rx(net_dev, priv, percpu_priv, &dq->fd, fq->fqid);
+
+	return qman_cb_dqrr_consume;
+}
+
+static enum qman_cb_dqrr_result
+ingress_tx_error_dqrr(struct qman_portal		*portal,
+		      struct qman_fq			*fq,
+		      const struct qm_dqrr_entry	*dq)
+{
+	struct net_device		*net_dev;
+	struct dpa_priv_s		*priv;
+	struct dpa_percpu_priv_s	*percpu_priv;
+
+	net_dev = ((struct dpa_fq *)fq)->net_dev;
+	priv = netdev_priv(net_dev);
+
+	percpu_priv = per_cpu_ptr(priv->percpu_priv, smp_processor_id());
+
+	if (dpaa_eth_napi_schedule(percpu_priv)) {
+		percpu_priv->in_interrupt++;
+		return qman_cb_dqrr_stop;
+	}
+
+	_dpa_tx_error(net_dev, priv, percpu_priv, &dq->fd, fq->fqid);
+
+	return qman_cb_dqrr_consume;
+}
+
+static enum qman_cb_dqrr_result __hot
+ingress_tx_default_dqrr(struct qman_portal		*portal,
+			struct qman_fq			*fq,
+			const struct qm_dqrr_entry	*dq)
+{
+	struct net_device		*net_dev;
+	struct dpa_priv_s		*priv;
+	struct dpa_percpu_priv_s	*percpu_priv;
+
+	net_dev = ((struct dpa_fq *)fq)->net_dev;
+	priv = netdev_priv(net_dev);
+
+	percpu_priv = per_cpu_ptr(priv->percpu_priv, smp_processor_id());
+
+	if (dpaa_eth_napi_schedule(percpu_priv)) {
+		percpu_priv->in_interrupt++;
+		return qman_cb_dqrr_stop;
+	}
+
+	_dpa_tx_conf(net_dev, priv, percpu_priv, &dq->fd, fq->fqid);
+
+	return qman_cb_dqrr_consume;
+}
+
+static enum qman_cb_dqrr_result
+shared_tx_error_dqrr(struct qman_portal                *portal,
+		     struct qman_fq                    *fq,
+		     const struct qm_dqrr_entry        *dq)
+{
+	struct net_device               *net_dev;
+	struct dpa_priv_s               *priv;
+	struct dpa_percpu_priv_s        *percpu_priv;
+	struct dpa_bp			*dpa_bp;
+	const struct qm_fd		*fd = &dq->fd;
+
+	net_dev = ((struct dpa_fq *)fq)->net_dev;
+	priv = netdev_priv(net_dev);
+
+	dpa_bp = dpa_bpid2pool(fd->bpid);
+	BUG_ON(IS_ERR(dpa_bp));
+
+	percpu_priv = per_cpu_ptr(priv->percpu_priv, smp_processor_id());
+
+	if (netif_msg_hw(priv) && net_ratelimit())
+		netdev_warn(net_dev, "FD status = 0x%08x\n",
+				fd->status & FM_FD_STAT_ERRORS);
+
+	if ((fd->format == qm_fd_sg) && (!dpa_bp->vaddr))
+		dpa_fd_release_sg(net_dev, fd);
+	else
+		dpa_fd_release(net_dev, fd);
+
+	percpu_priv->stats.tx_errors++;
+
+	return qman_cb_dqrr_consume;
+}
+
+static enum qman_cb_dqrr_result __hot
+shared_tx_default_dqrr(struct qman_portal              *portal,
+		       struct qman_fq                  *fq,
+		       const struct qm_dqrr_entry      *dq)
+{
+	struct net_device               *net_dev;
+	struct dpa_priv_s               *priv;
+	struct dpa_percpu_priv_s        *percpu_priv;
+	struct dpa_bp			*dpa_bp;
+	const struct qm_fd		*fd = &dq->fd;
+
+	net_dev = ((struct dpa_fq *)fq)->net_dev;
+	priv = netdev_priv(net_dev);
+
+	dpa_bp = dpa_bpid2pool(fd->bpid);
+	BUG_ON(IS_ERR(dpa_bp));
+
+	percpu_priv = per_cpu_ptr(priv->percpu_priv, smp_processor_id());
+
+	if (unlikely(fd->status & FM_FD_STAT_ERRORS) != 0) {
+		if (netif_msg_hw(priv) && net_ratelimit())
+			netdev_warn(net_dev, "FD status = 0x%08x\n",
+					fd->status & FM_FD_STAT_ERRORS);
+
+		percpu_priv->stats.tx_errors++;
+	}
+
+	if ((fd->format == qm_fd_sg) && (!dpa_bp->vaddr))
+		dpa_fd_release_sg(net_dev, fd);
+	else
+		dpa_fd_release(net_dev, fd);
+
+	percpu_priv->tx_confirm++;
+
+	return qman_cb_dqrr_consume;
+}
+
+static void count_ern(struct dpa_percpu_priv_s *percpu_priv,
+		      const struct qm_mr_entry *msg)
+{
+	switch (msg->ern.rc & QM_MR_RC_MASK) {
+	case QM_MR_RC_CGR_TAILDROP:
+		percpu_priv->ern_cnt.cg_tdrop++;
+		break;
+	case QM_MR_RC_WRED:
+		percpu_priv->ern_cnt.wred++;
+		break;
+	case QM_MR_RC_ERROR:
+		percpu_priv->ern_cnt.err_cond++;
+		break;
+	case QM_MR_RC_ORPWINDOW_EARLY:
+		percpu_priv->ern_cnt.early_window++;
+		break;
+	case QM_MR_RC_ORPWINDOW_LATE:
+		percpu_priv->ern_cnt.late_window++;
+		break;
+	case QM_MR_RC_FQ_TAILDROP:
+		percpu_priv->ern_cnt.fq_tdrop++;
+		break;
+	case QM_MR_RC_ORPWINDOW_RETIRED:
+		percpu_priv->ern_cnt.fq_retired++;
+		break;
+	case QM_MR_RC_ORP_ZERO:
+		percpu_priv->ern_cnt.orp_zero++;
+		break;
+	}
+}
+
+static void shared_ern(struct qman_portal	*portal,
+		       struct qman_fq		*fq,
+		       const struct qm_mr_entry	*msg)
+{
+	struct net_device *net_dev;
+	const struct dpa_priv_s	*priv;
+	struct dpa_percpu_priv_s *percpu_priv;
+	struct dpa_fq *dpa_fq = (struct dpa_fq *)fq;
+
+	net_dev = dpa_fq->net_dev;
+	priv = netdev_priv(net_dev);
+	percpu_priv = per_cpu_ptr(priv->percpu_priv, smp_processor_id());
+
+	dpa_fd_release(net_dev, &msg->ern.fd);
+
+	percpu_priv->stats.tx_dropped++;
+	percpu_priv->stats.tx_fifo_errors++;
+	count_ern(percpu_priv, msg);
+}
+
+static void egress_ern(struct qman_portal	*portal,
+		       struct qman_fq		*fq,
+		       const struct qm_mr_entry	*msg)
+{
+	struct net_device	*net_dev;
+	const struct dpa_priv_s	*priv;
+	struct sk_buff *skb;
+	struct dpa_percpu_priv_s	*percpu_priv;
+	struct qm_fd fd = msg->ern.fd;
+
+	net_dev = ((struct dpa_fq *)fq)->net_dev;
+	priv = netdev_priv(net_dev);
+	percpu_priv = per_cpu_ptr(priv->percpu_priv, smp_processor_id());
+
+	percpu_priv->stats.tx_dropped++;
+	percpu_priv->stats.tx_fifo_errors++;
+	count_ern(percpu_priv, msg);
+
+	/*
+	 * If we intended this buffer to go into the pool
+	 * when the FM was done, we need to put it in
+	 * manually.
+	 */
+	if (msg->ern.fd.cmd & FM_FD_CMD_FCO) {
+		dpa_fd_release(net_dev, &fd);
+		return;
+	}
+
+	skb = _dpa_cleanup_tx_fd(priv, &fd);
+	dev_kfree_skb_any(skb);
+}
+
+static const struct qman_fq rx_shared_fq __devinitconst = {
+	.cb = { .dqrr = shared_rx_dqrr }
+};
+static const struct qman_fq rx_private_defq __devinitconst = {
+	.cb = { .dqrr = ingress_rx_default_dqrr }
+};
+static const struct qman_fq rx_private_errq __devinitconst = {
+	.cb = { .dqrr = ingress_rx_error_dqrr }
+};
+static const struct qman_fq tx_private_defq __devinitconst = {
+	.cb = { .dqrr = ingress_tx_default_dqrr }
+};
+static const struct qman_fq tx_private_errq __devinitconst = {
+	.cb = { .dqrr = ingress_tx_error_dqrr }
+};
+static const struct qman_fq tx_shared_defq __devinitconst = {
+	.cb = { .dqrr = shared_tx_default_dqrr }
+};
+static const struct qman_fq tx_shared_errq __devinitconst = {
+	.cb = { .dqrr = shared_tx_error_dqrr }
+};
+static const struct qman_fq private_egress_fq __devinitconst = {
+	.cb = { .ern = egress_ern }
+};
+static const struct qman_fq shared_egress_fq __devinitconst = {
+	.cb = { .ern = shared_ern }
+};
+
+#ifdef CONFIG_DPAA_ETH_UNIT_TESTS
+static bool __devinitdata tx_unit_test_passed = true;
+
+static void __devinit tx_unit_test_ern(struct qman_portal	*portal,
+		       struct qman_fq		*fq,
+		       const struct qm_mr_entry	*msg)
+{
+	struct net_device *net_dev;
+	struct dpa_priv_s *priv;
+	struct sk_buff **skbh;
+	struct sk_buff *skb;
+	const struct qm_fd *fd;
+	dma_addr_t addr;
+
+	net_dev = ((struct dpa_fq *)fq)->net_dev;
+	priv = netdev_priv(net_dev);
+
+	tx_unit_test_passed = false;
+
+	fd = &msg->ern.fd;
+
+	addr = qm_fd_addr(fd);
+
+	skbh = (struct sk_buff **)phys_to_virt(addr);
+	skb = *skbh;
+
+	if (!skb || !is_kernel_addr((unsigned long)skb))
+		panic("Corrupt skb in ERN!\n");
+
+	kfree_skb(skb);
+}
+
+static unsigned char __devinitdata *tx_unit_skb_head;
+static unsigned char __devinitdata *tx_unit_skb_end;
+static int __devinitdata tx_unit_tested;
+
+static enum qman_cb_dqrr_result __devinit tx_unit_test_dqrr(
+		struct qman_portal *portal,
+		struct qman_fq *fq,
+		const struct qm_dqrr_entry *dq)
+{
+	struct net_device *net_dev;
+	struct dpa_priv_s *priv;
+	struct sk_buff **skbh;
+	struct sk_buff *skb;
+	const struct qm_fd *fd;
+	dma_addr_t addr;
+	unsigned char *startaddr;
+	struct dpa_percpu_priv_s *percpu_priv;
+
+	tx_unit_test_passed = false;
+
+	tx_unit_tested++;
+
+	net_dev = ((struct dpa_fq *)fq)->net_dev;
+	priv = netdev_priv(net_dev);
+
+	percpu_priv = per_cpu_ptr(priv->percpu_priv, smp_processor_id());
+
+	fd = &dq->fd;
+
+	addr = qm_fd_addr(fd);
+
+	skbh = (struct sk_buff **)phys_to_virt(addr);
+	startaddr = (unsigned char *)skbh;
+	skb = *skbh;
+
+	if (!skb || !is_kernel_addr((unsigned long)skb))
+		panic("Invalid skb address in TX Unit Test FD\n");
+
+	/* Make sure we're dealing with the same skb */
+	if (skb->head != tx_unit_skb_head
+			|| skb_end_pointer(skb) != tx_unit_skb_end)
+		goto out;
+
+	/*
+	 * If we recycled, then there must be enough room between fd.addr
+	 * and skb->end for a new RX buffer
+	 */
+	if (fd->cmd & FM_FD_CMD_FCO) {
+		size_t bufsize = skb_end_pointer(skb) - startaddr;
+
+		if (bufsize < dpa_get_max_frm())
+			goto out;
+	} else {
+		/*
+		 * If we didn't recycle, but the buffer was big enough,
+		 * increment the counter to put it back
+		 */
+		if (skb_end_pointer(skb) - skb->head >=
+			dpa_get_max_frm())
+			(*percpu_priv->dpa_bp_count)++;
+
+		/* If we didn't recycle, the data pointer should be good */
+		if (skb->data != startaddr + dpa_fd_offset(fd))
+			goto out;
+	}
+
+	tx_unit_test_passed = true;
+out:
+	/* The skb is no longer needed, and belongs to us */
+	kfree_skb(skb);
+
+	return qman_cb_dqrr_consume;
+}
+
+static const struct qman_fq tx_unit_test_fq __devinitconst = {
+	.cb = { .dqrr = tx_unit_test_dqrr, .ern = tx_unit_test_ern }
+};
+
+static struct __devinitdata dpa_fq unit_fq;
+
+static bool __devinitdata tx_unit_test_ran; /* Starts as false */
+
+static int __devinit dpa_tx_unit_test(struct net_device *net_dev)
+{
+	/* Create a new FQ */
+	struct dpa_priv_s *priv = netdev_priv(net_dev);
+	struct qman_fq *oldq;
+	int size, headroom;
+	struct dpa_percpu_priv_s *percpu_priv;
+	cpumask_var_t old_cpumask;
+	int test_count = 0;
+	int err = 0;
+	int tests_failed = 0;
+	const cpumask_t *cpus = qman_affine_cpus();
+
+	if (!alloc_cpumask_var(&old_cpumask, GFP_KERNEL)) {
+		pr_err("UNIT test cpumask allocation failed\n");
+		return -ENOMEM;
+	}
+
+	cpumask_copy(old_cpumask, tsk_cpus_allowed(current));
+	set_cpus_allowed_ptr(current, cpus);
+	/* disable bottom halves */
+	local_bh_disable();
+
+	percpu_priv = per_cpu_ptr(priv->percpu_priv, smp_processor_id());
+
+	qman_irqsource_remove(QM_PIRQ_DQRI);
+	unit_fq.net_dev = net_dev;
+	unit_fq.fq_base = tx_unit_test_fq;
+
+	/* Save old queue */
+	oldq = priv->egress_fqs[smp_processor_id()];
+
+	err = qman_create_fq(0, QMAN_FQ_FLAG_DYNAMIC_FQID, &unit_fq.fq_base);
+
+	if (err < 0) {
+		pr_err("UNIT test FQ create failed: %d\n", err);
+		goto fq_create_fail;
+	}
+
+	err = qman_init_fq(&unit_fq.fq_base,
+			QMAN_INITFQ_FLAG_SCHED | QMAN_INITFQ_FLAG_LOCAL, NULL);
+	if (err < 0) {
+		pr_err("UNIT test FQ init failed: %d\n", err);
+		goto fq_init_fail;
+	}
+
+	pr_err("TX Unit Test using FQ %d\n", qman_fq_fqid(&unit_fq.fq_base));
+
+	/* Replace queue 0 with this queue */
+	priv->egress_fqs[smp_processor_id()] = &unit_fq.fq_base;
+
+	/* Try packet sizes from 64-bytes to just above the maximum */
+	for (size = 64; size <= 9600 + 128; size += 64) {
+		for (headroom = DPA_BP_HEAD; headroom < 0x800; headroom += 16) {
+			int ret;
+			struct sk_buff *skb;
+
+			test_count++;
+
+			skb = dev_alloc_skb(size + headroom);
+
+			if (!skb) {
+				pr_err("Failed to allocate skb\n");
+				err = -ENOMEM;
+				goto end_test;
+			}
+
+			if (skb_end_pointer(skb) - skb->head >=
+					dpa_get_max_frm())
+				(*percpu_priv->dpa_bp_count)--;
+
+			skb_put(skb, size + headroom);
+			skb_pull(skb, headroom);
+
+			tx_unit_skb_head = skb->head;
+			tx_unit_skb_end = skb_end_pointer(skb);
+
+			skb_set_queue_mapping(skb, smp_processor_id());
+
+			/* tx */
+			ret = net_dev->netdev_ops->ndo_start_xmit(skb, net_dev);
+
+			if (ret != NETDEV_TX_OK) {
+				pr_err("Failed to TX with err %d\n", ret);
+				err = -EIO;
+				goto end_test;
+			}
+
+			/* Wait for it to arrive */
+			ret = spin_event_timeout(qman_poll_dqrr(1) != 0,
+					100000, 1);
+
+			if (!ret)
+				pr_err("TX Packet never arrived\n");
+
+			/* Was it good? */
+			if (tx_unit_test_passed == false) {
+				pr_err("Test failed:\n");
+				pr_err("size: %d pad: %d head: %p end: %p\n",
+					size, headroom, tx_unit_skb_head,
+					tx_unit_skb_end);
+				tests_failed++;
+			}
+		}
+	}
+
+end_test:
+	err = qman_retire_fq(&unit_fq.fq_base, NULL);
+	if (unlikely(err < 0))
+		pr_err("Could not retire TX Unit Test FQ (%d)\n", err);
+
+	err = qman_oos_fq(&unit_fq.fq_base);
+	if (unlikely(err < 0))
+		pr_err("Could not OOS TX Unit Test FQ (%d)\n", err);
+
+fq_init_fail:
+	qman_destroy_fq(&unit_fq.fq_base, 0);
+
+fq_create_fail:
+	priv->egress_fqs[smp_processor_id()] = oldq;
+	local_bh_enable();
+	qman_irqsource_add(QM_PIRQ_DQRI);
+	tx_unit_test_ran = true;
+	set_cpus_allowed_ptr(current, old_cpumask);
+	free_cpumask_var(old_cpumask);
+
+	pr_err("Tested %d/%d packets. %d failed\n", test_count, tx_unit_tested,
+		tests_failed);
+
+	if (tests_failed)
+		err = -EINVAL;
+
+	/* Reset counters */
+	memset(&percpu_priv->stats, 0, sizeof(percpu_priv->stats));
+
+	return err;
+}
+#endif
+
+static int __cold dpa_start(struct net_device *net_dev)
+{
+	int err, i;
+	struct dpa_priv_s *priv;
+	struct mac_device *mac_dev;
+
+	priv = netdev_priv(net_dev);
+	mac_dev = priv->mac_dev;
+
+	if (!mac_dev)
+		goto no_mac;
+
+	dpaa_eth_napi_enable(priv);
+
+	err = mac_dev->init_phy(net_dev);
+	if (err < 0) {
+		if (netif_msg_ifup(priv))
+			cpu_netdev_err(net_dev, "init_phy() = %d\n", err);
+		goto init_phy_failed;
+	}
+
+	for_each_port_device(i, mac_dev->port_dev)
+		fm_port_enable(mac_dev->port_dev[i]);
+
+	err = priv->mac_dev->start(mac_dev);
+	if (err < 0) {
+		if (netif_msg_ifup(priv))
+			cpu_netdev_err(net_dev, "mac_dev->start() = %d\n", err);
+		goto mac_start_failed;
+	}
+
+no_mac:
+	netif_tx_start_all_queues(net_dev);
+
+	return 0;
+
+mac_start_failed:
+	for_each_port_device(i, mac_dev->port_dev)
+		fm_port_disable(mac_dev->port_dev[i]);
+
+init_phy_failed:
+	dpaa_eth_napi_disable(priv);
+
+	return err;
+}
+
+static int __cold dpa_stop(struct net_device *net_dev)
+{
+	int _errno, i;
+	struct dpa_priv_s *priv;
+	struct mac_device *mac_dev;
+
+	priv = netdev_priv(net_dev);
+	mac_dev = priv->mac_dev;
+
+	netif_tx_stop_all_queues(net_dev);
+
+	if (!mac_dev)
+		return 0;
+
+	_errno = mac_dev->stop(mac_dev);
+	if (unlikely(_errno < 0))
+		if (netif_msg_ifdown(priv))
+			cpu_netdev_err(net_dev, "mac_dev->stop() = %d\n",
+					_errno);
+
+	for_each_port_device(i, mac_dev->port_dev)
+		fm_port_disable(mac_dev->port_dev[i]);
+
+	if (mac_dev->phy_dev)
+		phy_disconnect(mac_dev->phy_dev);
+	mac_dev->phy_dev = NULL;
+
+	dpaa_eth_napi_disable(priv);
+
+	return _errno;
+}
+
+static void __cold dpa_timeout(struct net_device *net_dev)
+{
+	const struct dpa_priv_s	*priv;
+	struct dpa_percpu_priv_s *percpu_priv;
+
+	priv = netdev_priv(net_dev);
+	percpu_priv = per_cpu_ptr(priv->percpu_priv, smp_processor_id());
+
+	if (netif_msg_timer(priv))
+		cpu_netdev_crit(net_dev, "Transmit timeout latency: %u ms\n",
+			jiffies_to_msecs(jiffies - net_dev->trans_start));
+
+	percpu_priv->stats.tx_errors++;
+}
+
+static int __devinit dpa_bp_cmp(const void *dpa_bp0, const void *dpa_bp1)
+{
+	return ((struct dpa_bp *)dpa_bp0)->size -
+			((struct dpa_bp *)dpa_bp1)->size;
+}
+
+static struct dpa_bp * __devinit __cold __must_check __attribute__((nonnull))
+dpa_bp_probe(struct platform_device *_of_dev, size_t *count)
+{
+	int			 i, lenp, na, ns;
+	struct device		*dev;
+	struct device_node	*dev_node;
+	const phandle		*phandle_prop;
+	const uint32_t		*bpid;
+	const uint32_t		*bpool_cfg;
+	struct dpa_bp		*dpa_bp;
+	int has_kernel_pool = 0;
+	int has_shared_pool = 0;
+
+	dev = &_of_dev->dev;
+
+	/* The default is one, if there's no property */
+	*count = 1;
+
+	/* There are three types of buffer pool configuration:
+	 * 1) No bp assignment
+	 * 2) A static assignment to an empty configuration
+	 * 3) A static assignment to one or more configured pools
+	 *
+	 * We don't support using multiple unconfigured pools.
+	 */
+
+	/* Get the buffer pools to be used */
+	phandle_prop = of_get_property(dev->of_node,
+					"fsl,bman-buffer-pools", &lenp);
+
+	if (phandle_prop)
+		*count = lenp / sizeof(phandle);
+	else {
+		if (default_pool)
+			return default_pool;
+
+		has_kernel_pool = 1;
+	}
+
+	dpa_bp = devm_kzalloc(dev, *count * sizeof(*dpa_bp), GFP_KERNEL);
+	if (unlikely(dpa_bp == NULL)) {
+		dpaa_eth_err(dev, "devm_kzalloc() failed\n");
+		return ERR_PTR(-ENOMEM);
+	}
+
+	dev_node = of_find_node_by_path("/");
+	if (unlikely(dev_node == NULL)) {
+		dpaa_eth_err(dev, "of_find_node_by_path(/) failed\n");
+		return ERR_PTR(-EINVAL);
+	}
+
+	na = of_n_addr_cells(dev_node);
+	ns = of_n_size_cells(dev_node);
+
+	for (i = 0; i < *count && phandle_prop; i++) {
+		of_node_put(dev_node);
+		dev_node = of_find_node_by_phandle(phandle_prop[i]);
+		if (unlikely(dev_node == NULL)) {
+			dpaa_eth_err(dev, "of_find_node_by_phandle() failed\n");
+			return ERR_PTR(-EFAULT);
+		}
+
+		if (unlikely(!of_device_is_compatible(dev_node, "fsl,bpool"))) {
+			dpaa_eth_err(dev,
+				"!of_device_is_compatible(%s, fsl,bpool)\n",
+				dev_node->full_name);
+			dpa_bp = ERR_PTR(-EINVAL);
+			goto _return_of_node_put;
+		}
+
+		bpid = of_get_property(dev_node, "fsl,bpid", &lenp);
+		if ((bpid == NULL) || (lenp != sizeof(*bpid))) {
+			dpaa_eth_err(dev, "fsl,bpid property not found.\n");
+			dpa_bp = ERR_PTR(-EINVAL);
+			goto _return_of_node_put;
+		}
+		dpa_bp[i].bpid = *bpid;
+
+		bpool_cfg = of_get_property(dev_node, "fsl,bpool-ethernet-cfg",
+					&lenp);
+		if (bpool_cfg && (lenp == (2 * ns + na) * sizeof(*bpool_cfg))) {
+			const uint32_t *seed_pool;
+
+			dpa_bp[i].config_count =
+				(int)of_read_number(bpool_cfg, ns);
+			dpa_bp[i].size	= of_read_number(bpool_cfg + ns, ns);
+			dpa_bp[i].paddr	=
+				of_read_number(bpool_cfg + 2 * ns, na);
+
+			seed_pool = of_get_property(dev_node,
+					"fsl,bpool-ethernet-seeds", &lenp);
+			dpa_bp[i].seed_pool = !!seed_pool;
+
+			has_shared_pool = 1;
+		} else {
+			has_kernel_pool = 1;
+		}
+
+		if (i > 0)
+			has_shared_pool = 1;
+	}
+
+	if (has_kernel_pool && has_shared_pool) {
+		dpaa_eth_err(dev, "Invalid buffer pool configuration "
+			"for node %s\n", dev_node->full_name);
+		dpa_bp = ERR_PTR(-EINVAL);
+		goto _return_of_node_put;
+	} else if (has_kernel_pool) {
+		dpa_bp->target_count = DEFAULT_COUNT;
+		dpa_bp->size = DEFAULT_BUF_SIZE;
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+		if (dpa_bp->size > PAGE_SIZE) {
+			dpaa_eth_warning(dev, "Default buffer size too large. "
+				     "Round down to PAGE_SIZE\n");
+			dpa_bp->size = PAGE_SIZE;
+		}
+#endif
+		dpa_bp->kernel_pool = 1;
+	}
+
+	sort(dpa_bp, *count, sizeof(*dpa_bp), dpa_bp_cmp, NULL);
+
+	return dpa_bp;
+
+_return_of_node_put:
+	if (dev_node)
+		of_node_put(dev_node);
+
+	return dpa_bp;
+}
+
+static int dpa_bp_create(struct net_device *net_dev, struct dpa_bp *dpa_bp,
+			size_t count)
+{
+	struct dpa_priv_s *priv = netdev_priv(net_dev);
+	int i;
+
+	if (dpa_bp->kernel_pool) {
+		priv->shared = 0;
+
+		if (netif_msg_probe(priv))
+			cpu_dev_info(net_dev->dev.parent,
+				"Using private BM buffer pools\n");
+	} else {
+		priv->shared = 1;
+	}
+
+	priv->dpa_bp = dpa_bp;
+	priv->bp_count = count;
+
+	for (i = 0; i < count; i++) {
+		int err;
+		err = dpa_bp_alloc(&dpa_bp[i]);
+		if (err < 0) {
+			dpa_bp_free(priv, dpa_bp);
+			priv->dpa_bp = NULL;
+			return err;
+		}
+
+		/* For now, just point to the default pool.
+		 * We can add support for more pools, later
+		 */
+		if (dpa_bp->kernel_pool)
+			priv->dpa_bp = default_pool;
+	}
+
+	return 0;
+}
+
+static struct mac_device * __devinit __cold __must_check
+__attribute__((nonnull))
+dpa_mac_probe(struct platform_device *_of_dev)
+{
+	struct device		*dpa_dev, *dev;
+	struct device_node	*mac_node;
+	int			 lenp;
+	const phandle		*phandle_prop;
+	struct platform_device	*of_dev;
+	struct mac_device	*mac_dev;
+#ifdef CONFIG_FSL_DPA_1588
+	struct net_device	*net_dev = NULL;
+	struct dpa_priv_s	*priv = NULL;
+	struct device_node	*timer_node;
+#endif
+
+	phandle_prop = of_get_property(_of_dev->dev.of_node,
+					"fsl,fman-mac", &lenp);
+	if (phandle_prop == NULL)
+		return NULL;
+
+	BUG_ON(lenp != sizeof(phandle));
+
+	dpa_dev = &_of_dev->dev;
+
+	mac_node = of_find_node_by_phandle(*phandle_prop);
+	if (unlikely(mac_node == NULL)) {
+		dpaa_eth_err(dpa_dev, "of_find_node_by_phandle() failed\n");
+		return ERR_PTR(-EFAULT);
+	}
+
+	of_dev = of_find_device_by_node(mac_node);
+	if (unlikely(of_dev == NULL)) {
+		dpaa_eth_err(dpa_dev, "of_find_device_by_node(%s) failed\n",
+				mac_node->full_name);
+		of_node_put(mac_node);
+		return ERR_PTR(-EINVAL);
+	}
+	of_node_put(mac_node);
+
+	dev = &of_dev->dev;
+
+	mac_dev = dev_get_drvdata(dev);
+	if (unlikely(mac_dev == NULL)) {
+		dpaa_eth_err(dpa_dev, "dev_get_drvdata(%s) failed\n",
+				dev_name(dev));
+		return ERR_PTR(-EINVAL);
+	}
+
+#ifdef CONFIG_FSL_DPA_1588
+	phandle_prop = of_get_property(mac_node, "ptimer-handle", &lenp);
+	if (phandle_prop && ((mac_dev->phy_if != PHY_INTERFACE_MODE_SGMII) ||
+			((mac_dev->phy_if == PHY_INTERFACE_MODE_SGMII) &&
+			 (mac_dev->speed == SPEED_1000)))) {
+		timer_node = of_find_node_by_phandle(*phandle_prop);
+		if (timer_node && (net_dev = dev_get_drvdata(dpa_dev))) {
+			priv = netdev_priv(net_dev);
+			if (!dpa_ptp_init(priv))
+				dpaa_eth_info(dev, "%s: ptp-timer enabled\n",
+						mac_node->full_name);
+		}
+	}
+#endif
+
+	return mac_dev;
+}
+
+static const char fsl_qman_frame_queues[][25] __devinitconst = {
+	[RX] = "fsl,qman-frame-queues-rx",
+	[TX] = "fsl,qman-frame-queues-tx"
+};
+
+#ifdef CONFIG_DEBUG_FS
+static int __cold dpa_debugfs_show(struct seq_file *file, void *offset)
+{
+	int				 i;
+	struct dpa_priv_s		*priv;
+	struct dpa_percpu_priv_s	*percpu_priv, total;
+	struct dpa_bp *dpa_bp;
+	unsigned int dpa_bp_count = 0;
+	unsigned int count_total = 0;
+	struct qm_mcr_querycgr query_cgr;
+
+	BUG_ON(offset == NULL);
+
+	priv = netdev_priv((struct net_device *)file->private);
+
+	dpa_bp = priv->dpa_bp;
+
+	memset(&total, 0, sizeof(total));
+
+	/* "Standard" counters */
+	seq_printf(file, "\nDPA counters for %s:\n"
+		"CPU           irqs        rx        tx   recycle" \
+		"   confirm     tx sg    tx err    rx err   l4 hxs drp    bp count\n",
+		priv->net_dev->name);
+	for_each_online_cpu(i) {
+		percpu_priv = per_cpu_ptr(priv->percpu_priv, i);
+
+		/* Only private interfaces have an associated counter for bp
+		 * buffers */
+		if (!priv->shared)
+			dpa_bp_count = *percpu_priv->dpa_bp_count;
+
+		total.in_interrupt += percpu_priv->in_interrupt;
+		total.ingress_calls += percpu_priv->stats.rx_packets;
+		total.stats.tx_packets += percpu_priv->stats.tx_packets;
+		total.tx_returned += percpu_priv->tx_returned;
+		total.tx_confirm += percpu_priv->tx_confirm;
+		total.tx_frag_skbuffs += percpu_priv->tx_frag_skbuffs;
+		total.stats.tx_errors += percpu_priv->stats.tx_errors;
+		total.stats.rx_errors += percpu_priv->stats.rx_errors;
+		total.l4_hxs_errors += percpu_priv->l4_hxs_errors;
+		count_total += dpa_bp_count;
+
+		seq_printf(file, "     %hu/%hu  %8u  %8lu  %8lu  %8u  %8u" \
+				"  %8u  %8lu  %8lu     %8u    %8d\n",
+				get_hard_smp_processor_id(i), i,
+				percpu_priv->in_interrupt,
+				percpu_priv->stats.rx_packets,
+				percpu_priv->stats.tx_packets,
+				percpu_priv->tx_returned,
+				percpu_priv->tx_confirm,
+				percpu_priv->tx_frag_skbuffs,
+				percpu_priv->stats.tx_errors,
+				percpu_priv->stats.rx_errors,
+				percpu_priv->l4_hxs_errors,
+				dpa_bp_count);
+	}
+	seq_printf(file, "Total     %8u  %8u  %8lu  %8u  %8u  %8u  %8lu  %8lu" \
+				"     %8u    %8d\n",
+			total.in_interrupt,
+			total.ingress_calls,
+			total.stats.tx_packets,
+			total.tx_returned,
+			total.tx_confirm,
+			total.tx_frag_skbuffs,
+			total.stats.tx_errors,
+			total.stats.rx_errors,
+			total.l4_hxs_errors,
+			count_total);
+
+	/* Congestion stats */
+	seq_printf(file, "\nDevice congestion stats:\n");
+	seq_printf(file, "Device has been congested for %d ms.\n",
+		jiffies_to_msecs(priv->cgr_data.congested_jiffies));
+
+	qman_query_cgr(&priv->cgr_data.cgr, &query_cgr);
+	seq_printf(file, "CGR id %d avg count: %llu\n",
+		priv->cgr_data.cgr.cgrid, qm_mcr_querycgr_a_get64(&query_cgr));
+	seq_printf(file, "Device entered congestion %u times. "
+		"Current congestion state is: %s.\n",
+		priv->cgr_data.cgr_congested_count,
+		query_cgr.cgr.cs ? "congested" : "not congested");
+
+	/* Rx Errors demultiplexing */
+	seq_printf(file, "\nDPA RX Errors:\nCPU        dma err  phys err" \
+				"  size err   hdr err  csum err\n");
+	for_each_online_cpu(i) {
+		percpu_priv = per_cpu_ptr(priv->percpu_priv, i);
+
+		total.rx_errors.dme += percpu_priv->rx_errors.dme;
+		total.rx_errors.fpe += percpu_priv->rx_errors.fpe;
+		total.rx_errors.fse += percpu_priv->rx_errors.fse;
+		total.rx_errors.phe += percpu_priv->rx_errors.phe;
+		total.rx_errors.cse += percpu_priv->rx_errors.cse;
+
+		seq_printf(file, "     %hu/%hu  %8u  %8u  %8u" \
+					"  %8u  %8u\n",
+				get_hard_smp_processor_id(i), i,
+				percpu_priv->rx_errors.dme,
+				percpu_priv->rx_errors.fpe,
+				percpu_priv->rx_errors.fse,
+				percpu_priv->rx_errors.phe,
+				percpu_priv->rx_errors.cse);
+	}
+	seq_printf(file, "Total     %8u  %8u  %8u  %8u  %8u\n",
+			total.rx_errors.dme,
+			total.rx_errors.fpe,
+			total.rx_errors.fse,
+			total.rx_errors.phe,
+			total.rx_errors.cse);
+
+	/* ERN demultiplexing */
+	seq_printf(file, "\nDPA ERN counters:\n  CPU     cg_td      wred  " \
+			"err_cond   early_w    late_w     fq_td    fq_ret" \
+			"     orp_z\n");
+	for_each_online_cpu(i) {
+		percpu_priv = per_cpu_ptr(priv->percpu_priv, i);
+
+		total.ern_cnt.cg_tdrop += percpu_priv->ern_cnt.cg_tdrop;
+		total.ern_cnt.wred += percpu_priv->ern_cnt.wred;
+		total.ern_cnt.err_cond += percpu_priv->ern_cnt.err_cond;
+		total.ern_cnt.early_window += percpu_priv->ern_cnt.early_window;
+		total.ern_cnt.late_window += percpu_priv->ern_cnt.late_window;
+		total.ern_cnt.fq_tdrop += percpu_priv->ern_cnt.fq_tdrop;
+		total.ern_cnt.fq_retired += percpu_priv->ern_cnt.fq_retired;
+		total.ern_cnt.orp_zero += percpu_priv->ern_cnt.orp_zero;
+
+		seq_printf(file, "  %hu/%hu  %8u  %8u  %8u  %8u  %8u  %8u" \
+			"  %8u  %8u\n",
+			get_hard_smp_processor_id(i), i,
+			percpu_priv->ern_cnt.cg_tdrop,
+			percpu_priv->ern_cnt.wred,
+			percpu_priv->ern_cnt.err_cond,
+			percpu_priv->ern_cnt.early_window,
+			percpu_priv->ern_cnt.late_window,
+			percpu_priv->ern_cnt.fq_tdrop,
+			percpu_priv->ern_cnt.fq_retired,
+			percpu_priv->ern_cnt.orp_zero);
+	}
+	seq_printf(file, "Total  %8u  %8u  %8u  %8u  %8u  %8u  %8u  %8u\n",
+		total.ern_cnt.cg_tdrop,
+		total.ern_cnt.wred,
+		total.ern_cnt.err_cond,
+		total.ern_cnt.early_window,
+		total.ern_cnt.late_window,
+		total.ern_cnt.fq_tdrop,
+		total.ern_cnt.fq_retired,
+		total.ern_cnt.orp_zero);
+
+	return 0;
+}
+
+static int __cold dpa_debugfs_open(struct inode *inode, struct file *file)
+{
+	int			 _errno;
+	const struct net_device	*net_dev;
+
+	_errno = single_open(file, dpa_debugfs_show, inode->i_private);
+	if (unlikely(_errno < 0)) {
+		net_dev = (struct net_device *)inode->i_private;
+
+		if (netif_msg_drv((struct dpa_priv_s *)netdev_priv(net_dev)))
+			cpu_netdev_err(net_dev, "single_open() = %d\n",
+					_errno);
+	}
+	return _errno;
+}
+
+static const struct file_operations dpa_debugfs_fops = {
+	.open		= dpa_debugfs_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+#endif
+
+static u16 dpa_select_queue(struct net_device *net_dev, struct sk_buff *skb)
+{
+	return smp_processor_id();
+}
+
+
+static int dpa_ndo_set_features(struct net_device *dev,
+	netdev_features_t features)
+{
+	/*
+	 * TODO either differentiate between MAC-less and MAC-ful (e.g. for
+	 * NETIF_F_RXCSUM), or use a different callback.
+	 *
+	 * Also, for things like NETIF_F_RXCSUM we ought to gain control
+	 * of the FMan port.
+	 */
+	pr_err(".ndo_set_features = %s() not yet implemented!\n", __func__);
+	return -EINVAL;
+}
+
+
+static const struct net_device_ops dpa_private_ops = {
+	.ndo_open = dpa_start,
+	.ndo_start_xmit = dpa_tx,
+	.ndo_stop = dpa_stop,
+	.ndo_tx_timeout = dpa_timeout,
+	.ndo_get_stats = dpa_get_stats,
+	.ndo_set_mac_address = dpa_set_mac_address,
+	.ndo_validate_addr = eth_validate_addr,
+	.ndo_select_queue = dpa_select_queue,
+	.ndo_change_mtu = dpa_change_mtu,
+	.ndo_set_rx_mode = dpa_set_rx_mode,
+	.ndo_init = dpa_ndo_init,
+	.ndo_set_features = dpa_ndo_set_features,
+#ifdef CONFIG_FSL_DPA_1588
+	.ndo_do_ioctl = dpa_ioctl,
+#endif
+};
+
+static const struct net_device_ops dpa_shared_ops = {
+        .ndo_open = dpa_start,
+        .ndo_start_xmit = dpa_shared_tx,
+        .ndo_stop = dpa_stop,
+        .ndo_tx_timeout = dpa_timeout,
+        .ndo_get_stats = dpa_get_stats,
+        .ndo_set_mac_address = dpa_set_mac_address,
+        .ndo_validate_addr = eth_validate_addr,
+#ifdef CONFIG_DPAA_ETH_USE_NDO_SELECT_QUEUE
+        .ndo_select_queue = dpa_select_queue,
+#endif
+        .ndo_change_mtu = dpa_change_mtu,
+        .ndo_set_rx_mode = dpa_set_rx_mode,
+        .ndo_init = dpa_ndo_init,
+        .ndo_set_features = dpa_ndo_set_features,
+#ifdef CONFIG_FSL_DPA_1588
+        .ndo_do_ioctl = dpa_ioctl,
+#endif
+};
+
+static u32 rx_pool_channel;
+static DEFINE_SPINLOCK(rx_pool_channel_init);
+
+static int __devinit dpa_get_channel(struct device *dev,
+					struct device_node *dpa_node)
+{
+	spin_lock(&rx_pool_channel_init);
+	if (!rx_pool_channel) {
+		u32 pool;
+		int ret = qman_alloc_pool(&pool);
+		if (!ret)
+			rx_pool_channel = pool;
+	}
+	spin_unlock(&rx_pool_channel_init);
+	if (!rx_pool_channel)
+		return -ENOMEM;
+	return rx_pool_channel;
+}
+
+struct fqid_cell {
+	uint32_t start;
+	uint32_t count;
+};
+
+static const struct fqid_cell default_fqids[][3] __devinitconst = {
+	[RX] = { {0, 1}, {0, 1}, {0, DPAA_ETH_RX_QUEUES} },
+	[TX] = { {0, 1}, {0, 1}, {0, DPAA_ETH_TX_QUEUES} }
+};
+
+static int __devinit
+dpa_fq_probe(struct platform_device *_of_dev, struct list_head *list,
+		struct dpa_fq **defq, struct dpa_fq **errq,
+		struct dpa_fq **fqs, int ptype)
+{
+	struct device *dev = &_of_dev->dev;
+	struct device_node *np = dev->of_node;
+	const struct fqid_cell *fqids;
+	int i, j, lenp;
+	int num_fqids;
+	struct dpa_fq *dpa_fq;
+	int err = 0;
+
+	fqids = of_get_property(np, fsl_qman_frame_queues[ptype], &lenp);
+	if (fqids == NULL) {
+		fqids = default_fqids[ptype];
+		num_fqids = 3;
+	} else
+		num_fqids = lenp / sizeof(*fqids);
+
+	for (i = 0; i < num_fqids; i++) {
+		dpa_fq = devm_kzalloc(dev, sizeof(*dpa_fq) * fqids[i].count,
+					GFP_KERNEL);
+		if (dpa_fq == NULL) {
+			dpaa_eth_err(dev, "devm_kzalloc() failed\n");
+			return -ENOMEM;
+		}
+
+		/* The first queue is the Error queue */
+		if (i == 0 && errq) {
+			*errq = dpa_fq;
+
+			if (fqids[i].count != 1) {
+				dpaa_eth_err(dev, "Too many error queues!\n");
+				err = -EINVAL;
+				goto invalid_error_queues;
+			}
+
+			dpa_fq[0].fq_type = (ptype == RX ?
+				FQ_TYPE_RX_ERROR : FQ_TYPE_TX_ERROR);
+		}
+
+		/* The second queue is the the Default queue */
+		if (i == 1 && defq) {
+			*defq = dpa_fq;
+
+			if (fqids[i].count != 1) {
+				dpaa_eth_err(dev, "Too many default queues!\n");
+				err = -EINVAL;
+				goto invalid_default_queues;
+			}
+
+			dpa_fq[0].fq_type = (ptype == RX ?
+				FQ_TYPE_RX_DEFAULT : FQ_TYPE_TX_CONFIRM);
+		}
+
+		/*
+		 * All subsequent queues are gathered together.
+		 * The first 8 will be used by the private linux interface
+		 * if these are TX queues
+		 */
+		if (i == 2 || (!errq && i == 0 && fqs)) {
+			*fqs = dpa_fq;
+
+			for (j = 0; j < fqids[i].count; j++)
+				dpa_fq[j].fq_type = (ptype == RX ?
+					FQ_TYPE_RX_PCD : FQ_TYPE_TX);
+		}
+
+#warning We lost the 8-queue enforcement
+
+		for (j = 0; j < fqids[i].count; j++) {
+			dpa_fq[j].fqid = fqids[i].start ?
+				fqids[i].start + j : 0;
+			_dpa_assign_wq(dpa_fq + j);
+			list_add_tail(&dpa_fq[j].list, list);
+		}
+	}
+
+invalid_default_queues:
+invalid_error_queues:
+	return err;
+}
+
+static void dpa_setup_ingress(struct dpa_priv_s *priv, struct dpa_fq *fq,
+			const struct qman_fq *template)
+{
+	fq->fq_base = *template;
+	fq->net_dev = priv->net_dev;
+
+	fq->flags = QMAN_FQ_FLAG_NO_ENQUEUE;
+	fq->channel = priv->channel;
+}
+
+static void dpa_setup_egress(struct dpa_priv_s *priv,
+				struct list_head *head, struct dpa_fq *fq,
+				struct fm_port *port)
+{
+	struct list_head *ptr = &fq->list;
+	int i = 0;
+
+	while (true) {
+		struct dpa_fq *iter = list_entry(ptr, struct dpa_fq, list);
+		if (priv->shared)
+			iter->fq_base = shared_egress_fq;
+		else
+			iter->fq_base = private_egress_fq;
+
+		iter->net_dev = priv->net_dev;
+		priv->egress_fqs[i++] = &iter->fq_base;
+
+		if (port) {
+			iter->flags = QMAN_FQ_FLAG_TO_DCPORTAL;
+			iter->channel = fm_get_tx_port_channel(port);
+		} else
+			iter->flags = QMAN_FQ_FLAG_NO_MODIFY;
+
+		if (list_is_last(ptr, head))
+			break;
+
+		ptr = ptr->next;
+	}
+}
+
+static void dpa_setup_ingress_queues(struct dpa_priv_s *priv,
+		struct list_head *head, struct dpa_fq *fq)
+{
+	struct list_head *ptr = &fq->list;
+	u32 fqid;
+	int portals[NR_CPUS];
+	int i, cpu, num_portals = 0;
+	const cpumask_t *affine_cpus = qman_affine_cpus();
+
+	for_each_cpu(cpu, affine_cpus)
+		portals[num_portals++] = qman_affine_channel(cpu);
+	if (num_portals == 0) {
+		dpaa_eth_err(fq->net_dev->dev.parent,
+			     "No Qman software (affine) channels found");
+		return;
+	}
+
+	i = 0;
+	fqid = 0;
+	if (priv->mac_dev)
+		fqid = (priv->mac_dev->res->start & 0x1fffff) >> 6;
+
+	while (true) {
+		struct dpa_fq *iter = list_entry(ptr, struct dpa_fq, list);
+
+		if (priv->shared)
+			dpa_setup_ingress(priv, iter, &rx_shared_fq);
+		else
+			dpa_setup_ingress(priv, iter, &rx_private_defq);
+
+		if (!iter->fqid)
+			iter->fqid = fqid++;
+
+		/* Assign the queues to a channel in a round-robin fashion */
+		iter->channel = portals[i];
+		i = (i + 1) % num_portals;
+
+		if (list_is_last(ptr, head))
+			break;
+
+		ptr = ptr->next;
+	}
+}
+
+static void __devinit
+dpaa_eth_init_tx_port(struct fm_port *port, struct dpa_fq *errq,
+		struct dpa_fq *defq, bool has_timer)
+{
+	struct fm_port_params tx_port_param;
+
+	dpaa_eth_init_port(tx, port, tx_port_param, errq->fqid, defq->fqid,
+			DPA_TX_PRIV_DATA_SIZE, has_timer);
+}
+
+static void __devinit
+dpaa_eth_init_rx_port(struct fm_port *port, struct dpa_bp *bp, size_t count,
+		struct dpa_fq *errq, struct dpa_fq *defq, bool has_timer)
+{
+	struct fm_port_params rx_port_param;
+	int i;
+
+	count = min(ARRAY_SIZE(rx_port_param.pool_param), count);
+	rx_port_param.num_pools = count;
+	for (i = 0; i < count; i++) {
+		if (i >= rx_port_param.num_pools)
+			break;
+
+		rx_port_param.pool_param[i].id = bp[i].bpid;
+		rx_port_param.pool_param[i].size = bp[i].size;
+	}
+
+	dpaa_eth_init_port(rx, port, rx_port_param, errq->fqid, defq->fqid,
+			DPA_RX_PRIV_DATA_SIZE, has_timer);
+}
+
+static void dpa_rx_fq_init(struct dpa_priv_s *priv, struct list_head *head,
+			struct dpa_fq *defq, struct dpa_fq *errq,
+			struct dpa_fq *fqs)
+{
+	if (fqs)
+		dpa_setup_ingress_queues(priv, head, fqs);
+
+	/* Only real devices need default/error queues set up */
+	if (!priv->mac_dev)
+		return;
+
+	if (defq->fqid == 0 && netif_msg_probe(priv))
+		cpu_pr_info("Using dynamic RX QM frame queues\n");
+
+	if (priv->shared) {
+		dpa_setup_ingress(priv, defq, &rx_shared_fq);
+		dpa_setup_ingress(priv, errq, &rx_shared_fq);
+	} else {
+		dpa_setup_ingress(priv, defq, &rx_private_defq);
+		dpa_setup_ingress(priv, errq, &rx_private_errq);
+	}
+}
+
+static void dpa_tx_fq_init(struct dpa_priv_s *priv, struct list_head *head,
+			struct dpa_fq *defq, struct dpa_fq *errq,
+			struct dpa_fq *fqs, struct fm_port *port)
+{
+	if (fqs)
+		dpa_setup_egress(priv, head, fqs, port);
+
+	/* Only real devices need default/error queues set up */
+	if (!priv->mac_dev)
+		return;
+
+	if (defq->fqid == 0 && netif_msg_probe(priv))
+		cpu_pr_info("Using dynamic TX QM frame queues\n");
+
+	/* The shared driver doesn't use tx confirmation */
+	if (priv->shared) {
+		dpa_setup_ingress(priv, defq, &tx_shared_defq);
+		dpa_setup_ingress(priv, errq, &tx_shared_errq);
+	} else {
+		dpa_setup_ingress(priv, defq, &tx_private_defq);
+		dpa_setup_ingress(priv, errq, &tx_private_errq);
+	}
+}
+
+static int dpa_netdev_init(struct device_node *dpa_node,
+		struct net_device *net_dev)
+{
+	int err;
+	const uint8_t *mac_addr;
+	struct dpa_priv_s *priv = netdev_priv(net_dev);
+	struct device *dev = net_dev->dev.parent;
+
+	net_dev->features |= DPA_NETIF_FEATURES;
+	net_dev->vlan_features |= DPA_NETIF_FEATURES;
+
+	if (!priv->mac_dev) {
+		/* Get the MAC address */
+		mac_addr = of_get_mac_address(dpa_node);
+		if (mac_addr == NULL) {
+			if (netif_msg_probe(priv))
+				dpaa_eth_err(dev, "No MAC address found!\n");
+			return -EINVAL;
+		}
+	} else {
+		net_dev->mem_start = priv->mac_dev->res->start;
+		net_dev->mem_end = priv->mac_dev->res->end;
+
+		mac_addr = priv->mac_dev->addr;
+		net_dev->features |= (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM);
+		net_dev->vlan_features |= (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM);
+
+		/*
+		 * Advertise S/G and HIGHDMA support for MAC-ful,
+		 * private interfaces
+		 */
+		if (!priv->shared) {
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+			net_dev->features |= NETIF_F_SG | NETIF_F_HIGHDMA
+				/*
+				 * Recent kernels enable GSO automatically, if
+				 * we declare NETIF_F_SG. For conformity, we'll
+				 * still declare GSO explicitly.
+				 */
+				| NETIF_F_GSO;
+			net_dev->vlan_features |= NETIF_F_SG | NETIF_F_HIGHDMA
+				| NETIF_F_GSO;
+#endif
+			/* Advertise GRO support */
+			net_dev->features |= NETIF_F_GRO;
+			net_dev->vlan_features |= NETIF_F_GRO;
+		}
+	}
+
+	memcpy(net_dev->perm_addr, mac_addr, net_dev->addr_len);
+	memcpy(net_dev->dev_addr, mac_addr, net_dev->addr_len);
+
+	SET_ETHTOOL_OPS(net_dev, &dpa_ethtool_ops);
+	net_dev->needed_headroom = DPA_BP_HEAD;
+	net_dev->watchdog_timeo = msecs_to_jiffies(tx_timeout);
+
+	err = register_netdev(net_dev);
+	if (err < 0) {
+		dpaa_eth_err(dev, "register_netdev() = %d\n", err);
+		return err;
+	}
+
+#ifdef CONFIG_DEBUG_FS
+	priv->debugfs_file = debugfs_create_file(net_dev->name, S_IRUGO,
+						 dpa_debugfs_root, net_dev,
+						 &dpa_debugfs_fops);
+	if (unlikely(priv->debugfs_file == NULL)) {
+		cpu_netdev_err(net_dev, "debugfs_create_file(%s/%s/%s) = %d\n",
+				powerpc_debugfs_root->d_iname,
+				dpa_debugfs_root->d_iname,
+				net_dev->name, err);
+
+		unregister_netdev(net_dev);
+		return -ENOMEM;
+	}
+#endif
+
+	return 0;
+}
+
+static int dpa_shared_netdev_init(struct device_node *dpa_node,
+				struct net_device *net_dev)
+{
+	net_dev->netdev_ops = &dpa_shared_ops;
+
+	return dpa_netdev_init(dpa_node, net_dev);
+}
+
+static int dpa_private_netdev_init(struct device_node *dpa_node,
+				struct net_device *net_dev)
+{
+	int i;
+	struct dpa_priv_s *priv = netdev_priv(net_dev);
+	struct dpa_percpu_priv_s *percpu_priv;
+
+	/*
+	 * Although we access another CPU's private data here
+	 * we do it at initialization so it is safe
+	 */
+	for_each_online_cpu(i) {
+		percpu_priv = per_cpu_ptr(priv->percpu_priv, i);
+		percpu_priv->net_dev = net_dev;
+
+		percpu_priv->dpa_bp = priv->dpa_bp;
+		percpu_priv->dpa_bp_count =
+			per_cpu_ptr(priv->dpa_bp->percpu_count, i);
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+		/* init the percpu list and add some skbs */
+		skb_queue_head_init(&percpu_priv->skb_list);
+
+		dpa_list_add_skbs(percpu_priv, DEFAULT_SKB_COUNT);
+#endif
+		netif_napi_add(net_dev, &percpu_priv->napi, dpaa_eth_poll,
+			       DPA_NAPI_WEIGHT);
+	}
+
+	net_dev->netdev_ops = &dpa_private_ops;
+
+	return dpa_netdev_init(dpa_node, net_dev);
+}
+
+int dpa_alloc_pcd_fqids(struct device *dev, uint32_t num,
+				uint8_t alignment, uint32_t *base_fqid)
+{
+	dpaa_eth_crit(dev, "callback not implemented!\n");
+	BUG();
+
+	return 0;
+}
+
+int dpa_free_pcd_fqids(struct device *dev, uint32_t base_fqid)
+{
+
+	dpaa_eth_crit(dev, "callback not implemented!\n");
+	BUG();
+
+	return 0;
+}
+
+static int dpaa_eth_add_channel(void *__arg)
+{
+	const cpumask_t *cpus = qman_affine_cpus();
+	u32 pool = QM_SDQCR_CHANNELS_POOL_CONV((u32)(unsigned long)__arg);
+	int cpu;
+
+	for_each_cpu(cpu, cpus) {
+		set_cpus_allowed_ptr(current, get_cpu_mask(cpu));
+		qman_static_dequeue_add(pool);
+	}
+	return 0;
+}
+
+static int dpaa_eth_cgr_init(struct dpa_priv_s *priv)
+{
+	struct qm_mcc_initcgr initcgr;
+	u32 cs_th;
+	int err;
+
+	err = qman_alloc_cgrid(&priv->cgr_data.cgr.cgrid);
+	if (err < 0) {
+		cpu_pr_err("Error %d allocating CGR ID\n", err);
+		goto out_error;
+	}
+	priv->cgr_data.cgr.cb = dpaa_eth_cgscn;
+
+	/* Enable Congestion State Change Notifications and CS taildrop */
+	initcgr.we_mask = QM_CGR_WE_CSCN_EN | QM_CGR_WE_CS_THRES;
+	initcgr.cgr.cscn_en = QM_CGR_EN;
+	/*
+	 * Set different thresholds based on the MAC speed.
+	 * TODO: this may turn suboptimal if the MAC is reconfigured at a speed
+	 * lower than its max, e.g. if a dTSEC later negotiates a 100Mbps link.
+	 * In such cases, we ought to reconfigure the threshold, too.
+	 */
+	if (!priv->mac_dev)
+		cs_th = DPA_CS_THRESHOLD_MACLESS;
+	else if (priv->mac_dev->if_support & SUPPORTED_10000baseT_Full)
+		cs_th = DPA_CS_THRESHOLD_10G;
+	else
+		cs_th = DPA_CS_THRESHOLD_1G;
+	qm_cgr_cs_thres_set64(&initcgr.cgr.cs_thres, cs_th, 1);
+
+	initcgr.we_mask |= QM_CGR_WE_CSTD_EN;
+	initcgr.cgr.cstd_en = QM_CGR_EN;
+
+	err = qman_create_cgr(&priv->cgr_data.cgr, QMAN_CGR_FLAG_USE_INIT,
+		&initcgr);
+	if (err < 0) {
+		cpu_pr_err("Error %d creating CGR with ID %d\n", err,
+			priv->cgr_data.cgr.cgrid);
+		qman_release_cgrid(priv->cgr_data.cgr.cgrid);
+		goto out_error;
+	}
+	cpu_pr_debug("Created CGR %d for netdev with hwaddr %pM on "
+		"QMan channel %d\n", priv->cgr_data.cgr.cgrid,
+		priv->mac_dev->addr, priv->cgr_data.cgr.chan);
+
+out_error:
+	return err;
+}
+
+static const struct of_device_id dpa_match[] __devinitconst ;
+static int __devinit
+dpaa_eth_probe(struct platform_device *_of_dev)
+{
+	int err, i;
+	struct device *dev;
+	struct device_node *dpa_node;
+	struct dpa_bp *dpa_bp;
+	struct dpa_fq *dpa_fq, *tmp;
+	struct list_head rxfqlist;
+	struct list_head txfqlist;
+	size_t count;
+	struct net_device *net_dev = NULL;
+	struct dpa_priv_s *priv = NULL;
+	struct dpa_fq *rxdefault = NULL;
+	struct dpa_fq *txdefault = NULL;
+	struct dpa_fq *rxerror = NULL;
+	struct dpa_fq *txerror = NULL;
+	struct dpa_fq *rxextra = NULL;
+	struct dpa_fq *txfqs = NULL;
+	struct fm_port *rxport = NULL;
+	struct fm_port *txport = NULL;
+	bool has_timer = FALSE;
+	struct mac_device *mac_dev;
+	int proxy_enet;
+	const struct of_device_id *match;
+
+	dev = &_of_dev->dev;
+
+	dpa_node = dev->of_node;
+
+	match = of_match_device(dpa_match, dev);
+	if (!match)
+		return -EINVAL;
+
+	if (!of_device_is_available(dpa_node))
+		return -ENODEV;
+
+	/*
+	 * If it's not an fsl,dpa-ethernet node, we just serve as a proxy
+	 * initializer driver, and don't do any linux device setup
+	 */
+	proxy_enet = strcmp(match->compatible, "fsl,dpa-ethernet");
+
+	/*
+	 * Allocate this early, so we can store relevant information in
+	 * the private area
+	 */
+	if (!proxy_enet) {
+		net_dev = alloc_etherdev_mq(sizeof(*priv), DPAA_ETH_TX_QUEUES);
+		if (!net_dev) {
+			dpaa_eth_err(dev, "alloc_etherdev_mq() failed\n");
+			return -ENOMEM;
+		}
+
+		/* Do this here, so we can be verbose early */
+		SET_NETDEV_DEV(net_dev, dev);
+		dev_set_drvdata(dev, net_dev);
+
+		priv = netdev_priv(net_dev);
+		priv->net_dev = net_dev;
+
+		priv->msg_enable = netif_msg_init(debug, -1);
+	}
+
+	/* Get the buffer pools assigned to this interface */
+	dpa_bp = dpa_bp_probe(_of_dev, &count);
+	if (IS_ERR(dpa_bp)) {
+		err = PTR_ERR(dpa_bp);
+		goto bp_probe_failed;
+	}
+
+	mac_dev = dpa_mac_probe(_of_dev);
+	if (IS_ERR(mac_dev)) {
+		err = PTR_ERR(mac_dev);
+		goto mac_probe_failed;
+	} else if (mac_dev) {
+		rxport = mac_dev->port_dev[RX];
+		txport = mac_dev->port_dev[TX];
+	}
+
+	INIT_LIST_HEAD(&rxfqlist);
+	INIT_LIST_HEAD(&txfqlist);
+
+	if (rxport)
+		err = dpa_fq_probe(_of_dev, &rxfqlist, &rxdefault, &rxerror,
+				&rxextra, RX);
+	else
+		err = dpa_fq_probe(_of_dev, &rxfqlist, NULL, NULL,
+				&rxextra, RX);
+
+	if (err < 0)
+		goto rx_fq_probe_failed;
+
+	if (txport)
+		err = dpa_fq_probe(_of_dev, &txfqlist, &txdefault, &txerror,
+				&txfqs, TX);
+	else
+		err = dpa_fq_probe(_of_dev, &txfqlist, NULL, NULL, &txfqs, TX);
+
+	if (err < 0)
+		goto tx_fq_probe_failed;
+
+	/*
+	 * Now we have all of the configuration information.
+	 * We support a number of configurations:
+	 * 1) Private interface - An optimized linux ethernet driver with
+	 *    a real network connection.
+	 * 2) Shared interface - A device intended for virtual connections
+	 *    or for a real interface that is shared between partitions
+	 * 3) Proxy initializer - Just configures the MAC on behalf of
+	 *    another partition
+	 */
+
+	/* bp init */
+	if (net_dev) {
+		struct task_struct *kth;
+
+		err = dpa_bp_create(net_dev, dpa_bp, count);
+
+		if (err < 0)
+			goto bp_create_failed;
+
+		priv->mac_dev = mac_dev;
+
+		priv->channel = dpa_get_channel(dev, dpa_node);
+
+		if (priv->channel < 0) {
+			err = priv->channel;
+			goto get_channel_failed;
+		}
+
+		/* Start a thread that will walk the cpus with affine portals
+		 * and add this pool channel to each's dequeue mask. */
+		kth = kthread_run(dpaa_eth_add_channel,
+				  (void *)(unsigned long)priv->channel,
+				  "dpaa_%p:%d", net_dev, priv->channel);
+		if (!kth) {
+			err = -ENOMEM;
+			goto add_channel_failed;
+		}
+
+		dpa_rx_fq_init(priv, &rxfqlist, rxdefault, rxerror, rxextra);
+		dpa_tx_fq_init(priv, &txfqlist, txdefault, txerror, txfqs,
+				txport);
+
+		/*
+		 * Create a congestion group for this netdev, with
+		 * dynamically-allocated CGR ID.
+		 * Must be executed after probing the MAC, but before
+		 * assigning the egress FQs to the CGRs.
+		 */
+		err = dpaa_eth_cgr_init(priv);
+		if (err < 0) {
+			dpaa_eth_err(dev, "Error initializing CGR\n");
+			goto cgr_init_failed;
+		}
+
+		/* Add the FQs to the interface, and make them active */
+		INIT_LIST_HEAD(&priv->dpa_fq_list);
+
+		list_for_each_entry_safe(dpa_fq, tmp, &rxfqlist, list) {
+			err = _dpa_fq_alloc(&priv->dpa_fq_list, dpa_fq);
+			if (err < 0)
+				goto fq_alloc_failed;
+		}
+
+		list_for_each_entry_safe(dpa_fq, tmp, &txfqlist, list) {
+			err = _dpa_fq_alloc(&priv->dpa_fq_list, dpa_fq);
+			if (err < 0)
+				goto fq_alloc_failed;
+		}
+
+		if (priv->tsu && priv->tsu->valid)
+			has_timer = TRUE;
+	}
+
+	/* All real interfaces need their ports initialized */
+	if (mac_dev) {
+		struct fm_port_pcd_param rx_port_pcd_param;
+
+		dpaa_eth_init_rx_port(rxport, dpa_bp, count, rxerror,
+				rxdefault, has_timer);
+		dpaa_eth_init_tx_port(txport, txerror, txdefault, has_timer);
+
+		rx_port_pcd_param.cba = dpa_alloc_pcd_fqids;
+		rx_port_pcd_param.cbf = dpa_free_pcd_fqids;
+		rx_port_pcd_param.dev = dev;
+		fm_port_pcd_bind(rxport, &rx_port_pcd_param);
+	}
+
+	/*
+	 * Proxy interfaces need to be started, and the allocated
+	 * memory freed
+	 */
+	if (!net_dev) {
+		devm_kfree(&_of_dev->dev, dpa_bp);
+		devm_kfree(&_of_dev->dev, rxdefault);
+		devm_kfree(&_of_dev->dev, rxerror);
+		devm_kfree(&_of_dev->dev, txdefault);
+		devm_kfree(&_of_dev->dev, txerror);
+
+		if (mac_dev)
+			for_each_port_device(i, mac_dev->port_dev)
+				fm_port_enable(mac_dev->port_dev[i]);
+
+		return 0;
+	}
+
+	/* Now we need to initialize either a private or shared interface */
+	priv->percpu_priv = __alloc_percpu(sizeof(*priv->percpu_priv),
+					   __alignof__(*priv->percpu_priv));
+	if (priv->percpu_priv == NULL) {
+		dpaa_eth_err(dev, "__alloc_percpu() failed\n");
+		err = -ENOMEM;
+		goto alloc_percpu_failed;
+	}
+
+	if (priv->shared)
+		err = dpa_shared_netdev_init(dpa_node, net_dev);
+	else
+		err = dpa_private_netdev_init(dpa_node, net_dev);
+
+	if (err < 0)
+		goto netdev_init_failed;
+
+	dpaa_eth_sysfs_init(&net_dev->dev);
+
+#ifdef CONFIG_DPAA_ETH_UNIT_TESTS
+	/* The unit test is designed to test private interfaces */
+	if (!priv->shared && !tx_unit_test_ran) {
+		err = dpa_tx_unit_test(net_dev);
+
+		WARN_ON(err);
+	}
+#endif
+
+	return 0;
+
+netdev_init_failed:
+	if (net_dev)
+		free_percpu(priv->percpu_priv);
+alloc_percpu_failed:
+fq_alloc_failed:
+	if (net_dev) {
+		dpa_fq_free(dev, &priv->dpa_fq_list);
+		qman_release_cgrid(priv->cgr_data.cgr.cgrid);
+		qman_delete_cgr(&priv->cgr_data.cgr);
+	}
+cgr_init_failed:
+add_channel_failed:
+get_channel_failed:
+	if (net_dev)
+		dpa_bp_free(priv, priv->dpa_bp);
+bp_create_failed:
+tx_fq_probe_failed:
+rx_fq_probe_failed:
+mac_probe_failed:
+bp_probe_failed:
+	dev_set_drvdata(dev, NULL);
+	if (net_dev)
+		free_netdev(net_dev);
+
+	return err;
+}
+
+static const struct of_device_id dpa_match[] __devinitconst = {
+	{
+		.compatible	= "fsl,dpa-ethernet"
+	},
+	{
+		.compatible	= "fsl,dpa-ethernet-init"
+	},
+	{}
+};
+MODULE_DEVICE_TABLE(of, dpa_match);
+
+static int __devexit __cold dpa_remove(struct platform_device *of_dev)
+{
+	int			err;
+	struct device		*dev;
+	struct net_device	*net_dev;
+	struct dpa_priv_s	*priv;
+
+	dev = &of_dev->dev;
+	net_dev = dev_get_drvdata(dev);
+	priv = netdev_priv(net_dev);
+
+	dpaa_eth_sysfs_remove(dev);
+
+	dev_set_drvdata(dev, NULL);
+	unregister_netdev(net_dev);
+
+	err = dpa_fq_free(dev, &priv->dpa_fq_list);
+
+	free_percpu(priv->percpu_priv);
+
+	dpa_bp_free(priv, priv->dpa_bp);
+
+#ifdef CONFIG_DEBUG_FS
+	debugfs_remove(priv->debugfs_file);
+#endif
+
+#ifdef CONFIG_FSL_DPA_1588
+	if (priv->tsu && priv->tsu->valid)
+		dpa_ptp_cleanup(priv);
+#endif
+
+	free_netdev(net_dev);
+
+	return err;
+}
+
+static struct platform_driver dpa_driver = {
+	.driver = {
+		.name		= KBUILD_MODNAME,
+		.of_match_table	= dpa_match,
+		.owner		= THIS_MODULE,
+	},
+	.probe		= dpaa_eth_probe,
+	.remove		= __devexit_p(dpa_remove)
+};
+
+static int __init __cold dpa_load(void)
+{
+	int	 _errno;
+
+	cpu_pr_info(KBUILD_MODNAME ": " DPA_DESCRIPTION " (" VERSION ")\n");
+
+	/* initialise dpaa_eth mirror values */
+	dpa_rx_extra_headroom = fm_get_rx_extra_headroom();
+	dpa_max_frm = fm_get_max_frm();
+
+#ifdef CONFIG_DEBUG_FS
+	dpa_debugfs_root = debugfs_create_dir(KBUILD_MODNAME,
+					      powerpc_debugfs_root);
+	if (unlikely(dpa_debugfs_root == NULL)) {
+		_errno = -ENOMEM;
+		cpu_pr_err(KBUILD_MODNAME ": %s:%hu:%s(): "
+			   "debugfs_create_dir(%s/"KBUILD_MODNAME") = %d\n",
+			   __file__, __LINE__, __func__,
+			   powerpc_debugfs_root->d_iname, _errno);
+		goto _return;
+	}
+#endif
+
+	_errno = platform_driver_register(&dpa_driver);
+	if (unlikely(_errno < 0)) {
+		cpu_pr_err(KBUILD_MODNAME
+			": %s:%hu:%s(): platform_driver_register() = %d\n",
+			__file__, __LINE__, __func__, _errno);
+		goto _return_debugfs_remove;
+	}
+
+	goto _return;
+
+_return_debugfs_remove:
+#ifdef CONFIG_DEBUG_FS
+	debugfs_remove(dpa_debugfs_root);
+#endif
+_return:
+	cpu_pr_debug(KBUILD_MODNAME ": %s:%s() ->\n", __file__, __func__);
+
+	return _errno;
+}
+module_init(dpa_load);
+
+static void __exit __cold dpa_unload(void)
+{
+	cpu_pr_debug(KBUILD_MODNAME ": -> %s:%s()\n", __file__, __func__);
+
+	platform_driver_unregister(&dpa_driver);
+
+#ifdef CONFIG_DEBUG_FS
+	debugfs_remove(dpa_debugfs_root);
+#endif
+
+	cpu_pr_debug(KBUILD_MODNAME ": %s:%s() ->\n", __file__, __func__);
+}
+module_exit(dpa_unload);
diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth.h b/drivers/net/ethernet/freescale/dpa/dpaa_eth.h
index 79a67d3..0bbfd0b 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth.h
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth.h
@@ -345,9 +345,6 @@ struct dpa_percpu_priv_s {
 	struct dpa_ern_cnt ern_cnt;
 };
 
-/* increment when adding new sysfs entries */
-#define DPA_MAX_NUM_OF_SYSFS_ATTRS 5
-
 struct dpa_priv_s {
 	struct dpa_bp *dpa_bp;
 	size_t bp_count;
@@ -390,8 +387,6 @@ struct dpa_priv_s {
 		 */
 		u32 cgr_congested_count;
 	} cgr_data;
-	/* sysfs entries, last value must be NULL */
-	struct device_attribute *sysfs_attrs[DPA_MAX_NUM_OF_SYSFS_ATTRS + 1];
 };
 
 extern const struct ethtool_ops dpa_ethtool_ops;
@@ -482,6 +477,9 @@ static inline int dpa_check_rx_mtu(struct sk_buff *skb, int mtu)
 
 void fm_mac_dump_regs(struct mac_device *mac_dev);
 
+void dpaa_eth_sysfs_remove(struct device *dev);
+void dpaa_eth_sysfs_init(struct device *dev);
+
 /* Equivalent to a memset(0), but works faster */
 static inline void clear_fd(struct qm_fd *fd)
 {
diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c b/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c
new file mode 100644
index 0000000..d93e358
--- /dev/null
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c
@@ -0,0 +1,883 @@
+/*
+ * Copyright 2012 Freescale Semiconductor Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *	 notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *	 notice, this list of conditions and the following disclaimer in the
+ *	 documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *	 names of its contributors may be used to endorse or promote products
+ *	 derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/init.h>
+#include <linux/skbuff.h>
+#include <linux/highmem.h>
+#include <linux/fsl_bman.h>
+
+#include "dpaa_eth.h"
+#include "dpaa_1588.h"
+
+#ifdef CONFIG_DPAA_ETH_SG_SUPPORT
+#define DPA_SGT_MAX_ENTRIES 16 /* maximum number of entries in SG Table */
+
+/*
+ * It does not return a page as you get the page from the fd,
+ * this is only for refcounting and DMA unmapping
+ */
+static inline void dpa_bp_removed_one_page(struct dpa_bp *dpa_bp,
+					   dma_addr_t dma_addr)
+{
+	int *count_ptr;
+
+	count_ptr = per_cpu_ptr(dpa_bp->percpu_count, smp_processor_id());
+	(*count_ptr)--;
+
+	dma_unmap_single(dpa_bp->dev, dma_addr, dpa_bp->size,
+		DMA_BIDIRECTIONAL);
+}
+
+/* DMA map and add a page into the bpool */
+static void dpa_bp_add_page(struct dpa_bp *dpa_bp, unsigned long vaddr)
+{
+	struct bm_buffer bmb;
+	int *count_ptr;
+	dma_addr_t addr;
+	int offset;
+
+	count_ptr = per_cpu_ptr(dpa_bp->percpu_count, smp_processor_id());
+
+	/* Make sure we don't map beyond end of page */
+	offset = vaddr & (PAGE_SIZE - 1);
+	if (unlikely(dpa_bp->size + offset > PAGE_SIZE)) {
+		free_page(vaddr);
+		return;
+	}
+	addr = dma_map_single(dpa_bp->dev, (void *)vaddr, dpa_bp->size,
+			      DMA_BIDIRECTIONAL);
+	if (unlikely(dma_mapping_error(dpa_bp->dev, addr))) {
+		dpaa_eth_err(dpa_bp->dev, "DMA mapping failed");
+		return;
+	}
+
+	bm_buffer_set64(&bmb, addr);
+
+	while (bman_release(dpa_bp->pool, &bmb, 1, 0))
+		cpu_relax();
+
+	(*count_ptr)++;
+}
+
+void dpa_bp_add_8_pages(struct dpa_bp *dpa_bp, int cpu_id)
+{
+	struct bm_buffer bmb[8];
+	unsigned long new_page;
+	int *count_ptr;
+	dma_addr_t addr;
+	int i;
+
+	count_ptr = per_cpu_ptr(dpa_bp->percpu_count, cpu_id);
+
+	for (i = 0; i < 8; i++) {
+		new_page = __get_free_page(GFP_ATOMIC);
+		if (unlikely(!new_page)) {
+			dpaa_eth_err(dpa_bp->dev, "__get_free_page() failed\n");
+			bm_buffer_set64(&bmb[i], 0);
+			break;
+		}
+
+		addr = dma_map_single(dpa_bp->dev, (void *)new_page,
+				dpa_bp->size, DMA_BIDIRECTIONAL);
+		if (unlikely(dma_mapping_error(dpa_bp->dev, addr))) {
+			dpaa_eth_err(dpa_bp->dev, "DMA mapping failed");
+			free_page(new_page);
+			break;
+		}
+
+		bm_buffer_set64(&bmb[i], addr);
+	}
+
+	/*
+	 * Avoid releasing a completely null buffer; bman_release() requires
+	 * at least one buffer.
+	 */
+	if (likely(i)) {
+		/*
+		 * Release the buffers. In case bman is busy, keep trying
+		 * until successful. bman_release() is guaranteed to succeed
+		 * in a reasonable amount of time
+		 */
+		while (bman_release(dpa_bp->pool, bmb, i, 0))
+			cpu_relax();
+
+		*count_ptr += i;
+	}
+}
+
+void dpa_list_add_skb(struct dpa_percpu_priv_s *cpu_priv,
+		      struct sk_buff *new_skb)
+{
+	struct sk_buff_head *list_ptr;
+
+	if (cpu_priv->skb_count > DEFAULT_SKB_COUNT) {
+		dev_kfree_skb(new_skb);
+		return;
+	}
+
+	list_ptr = &cpu_priv->skb_list;
+	skb_queue_head(list_ptr, new_skb);
+
+	cpu_priv->skb_count += 1;
+}
+
+static struct sk_buff *dpa_list_get_skb(struct dpa_percpu_priv_s *cpu_priv)
+{
+	struct sk_buff_head *list_ptr;
+	struct sk_buff *new_skb;
+
+	list_ptr = &cpu_priv->skb_list;
+
+	new_skb = skb_dequeue(list_ptr);
+	if (new_skb)
+		cpu_priv->skb_count -= 1;
+
+	return new_skb;
+}
+
+void dpa_list_add_skbs(struct dpa_percpu_priv_s *cpu_priv, int count)
+{
+	struct sk_buff *new_skb;
+	int i;
+
+	for (i = 0; i < count; i++) {
+		new_skb = dev_alloc_skb(DPA_BP_HEAD +
+				dpa_get_rx_extra_headroom() +
+				DPA_COPIED_HEADERS_SIZE);
+		if (unlikely(!new_skb)) {
+			pr_err("dev_alloc_skb() failed\n");
+			break;
+		}
+
+		dpa_list_add_skb(cpu_priv, new_skb);
+	}
+}
+
+void dpa_make_private_pool(struct dpa_bp *dpa_bp)
+{
+	int i;
+
+	dpa_bp->percpu_count = __alloc_percpu(sizeof(*dpa_bp->percpu_count),
+					__alignof__(*dpa_bp->percpu_count));
+
+	/* Give each CPU an allotment of "page_count" buffers */
+	for_each_online_cpu(i) {
+		int j;
+
+		/*
+		 * Although we access another CPU's counters here
+		 * we do it at boot time so it is safe
+		 */
+		for (j = 0; j < dpa_bp->config_count; j += 8)
+			dpa_bp_add_8_pages(dpa_bp, i);
+	}
+}
+
+/*
+ * Cleanup function for outgoing frame descriptors that were built on Tx path,
+ * either contiguous frames or scatter/gather ones.
+ * Skb freeing is not handled here.
+ *
+ * This function may be called on error paths in the Tx function, so guard
+ * against cases when not all fd relevant fields were filled in.
+ *
+ * Return the skb backpointer, since for S/G frames the buffer containing it
+ * gets freed here.
+ */
+struct sk_buff *_dpa_cleanup_tx_fd(const struct dpa_priv_s *priv,
+	const struct qm_fd *fd)
+{
+	const struct qm_sg_entry *sgt;
+	int i;
+	struct dpa_bp *dpa_bp = priv->dpa_bp;
+	dma_addr_t addr = qm_fd_addr(fd);
+	struct sk_buff **skbh;
+	struct sk_buff *skb = NULL;
+	enum dma_data_direction dma_dir;
+
+	dma_dir = (fd->cmd & FM_FD_CMD_FCO) ? DMA_BIDIRECTIONAL : DMA_TO_DEVICE;
+	dma_unmap_single(dpa_bp->dev, addr, dpa_bp->size, dma_dir);
+
+	/* retrieve skb back pointer */
+	skbh = (struct sk_buff **)phys_to_virt(addr);
+	skb = *skbh;
+
+	if (fd->format == qm_fd_sg) {
+		/*
+		 * All storage items used are pages, but only the sgt and
+		 * the first page are guaranteed to reside in lowmem.
+		 */
+		sgt = phys_to_virt(addr + dpa_fd_offset(fd));
+
+		/* page 0 is from lowmem, was dma_map_single()-ed */
+		dma_unmap_single(dpa_bp->dev, sgt[0].addr,
+				 dpa_bp->size, dma_dir);
+
+		/* remaining pages were mapped with dma_map_page() */
+		for (i = 1; i < skb_shinfo(skb)->nr_frags; i++) {
+			BUG_ON(sgt[i].extension);
+
+			dma_unmap_page(dpa_bp->dev, sgt[i].addr,
+					dpa_bp->size, dma_dir);
+		}
+
+		/*
+		 * TODO: dpa_bp_add_page() ?
+		 * We could put these in the pool, since we allocated them
+		 * and we know they're not used by anyone else
+		 */
+
+		/* Free separately the pages that we allocated on Tx */
+		free_page((unsigned long)phys_to_virt(addr));
+		free_page((unsigned long)phys_to_virt(sgt[0].addr));
+	}
+
+	return skb;
+}
+
+/*
+ * Move the first DPA_COPIED_HEADERS_SIZE bytes to the skb linear buffer to
+ * provide the networking stack the headers it requires in the linear buffer.
+ *
+ * If the entire frame fits in the skb linear buffer, the page holding the
+ * received data is recycled as it is no longer required.
+ *
+ * Return 0 if the ingress skb was properly constructed, non-zero if an error
+ * was encountered and the frame should be dropped.
+ */
+static int __hot contig_fd_to_skb(const struct dpa_priv_s *priv,
+	const struct qm_fd *fd, struct sk_buff *skb, int *use_gro)
+{
+	unsigned int copy_size = DPA_COPIED_HEADERS_SIZE;
+	dma_addr_t addr = qm_fd_addr(fd);
+	void *vaddr;
+	struct page *page;
+	int frag_offset, page_offset;
+	struct dpa_bp *dpa_bp = priv->dpa_bp;
+	unsigned char *tailptr;
+	const t_FmPrsResult *parse_results;
+	int ret;
+
+	vaddr = phys_to_virt(addr);
+
+#ifdef CONFIG_FSL_DPA_1588
+	if (priv->tsu && priv->tsu->valid && priv->tsu->hwts_rx_en_ioctl)
+		dpa_ptp_store_rxstamp(priv->net_dev, skb, fd);
+#endif
+
+	/* Peek at the parse results for frame validation. */
+	parse_results = (const t_FmPrsResult *)(vaddr + DPA_RX_PRIV_DATA_SIZE);
+	ret = _dpa_process_parse_results(parse_results, fd, skb, use_gro,
+		&copy_size);
+	if (unlikely(ret))
+		/* This is definitely a bad frame, don't go further. */
+		return ret;
+
+	tailptr = skb_put(skb, copy_size);
+
+	/* Copy (at least) the headers in the linear portion */
+	memcpy(tailptr, vaddr + dpa_fd_offset(fd), copy_size);
+
+	/*
+	 * If frame is longer than the amount we copy in the linear
+	 * buffer, add the page as fragment,
+	 * otherwise recycle the page
+	 */
+	page = pfn_to_page(addr >> PAGE_SHIFT);
+
+	if (copy_size < dpa_fd_length(fd)) {
+		/* add the page as a fragment in the skb */
+		page_offset = (unsigned long)vaddr & (PAGE_SIZE - 1);
+		frag_offset = page_offset + dpa_fd_offset(fd) + copy_size;
+		skb_add_rx_frag(skb, 0, page, frag_offset,
+		                dpa_fd_length(fd) - copy_size,
+		                /* TODO kernel 3.8 fixup; we might want
+		                 * to better account for the truesize */
+				dpa_fd_length(fd) - copy_size);
+	} else {
+		/* recycle the page */
+		dpa_bp_add_page(dpa_bp, (unsigned long)vaddr);
+	}
+
+	return 0;
+}
+
+
+/*
+ * Move the first bytes of the frame to the skb linear buffer to
+ * provide the networking stack the headers it requires in the linear buffer,
+ * and add the rest of the frame as skb fragments.
+ *
+ * The page holding the S/G Table is recycled here.
+ */
+static int __hot sg_fd_to_skb(const struct dpa_priv_s *priv,
+			       const struct qm_fd *fd, struct sk_buff *skb,
+			       int *use_gro)
+{
+	const struct qm_sg_entry *sgt;
+	dma_addr_t addr = qm_fd_addr(fd);
+	dma_addr_t sg_addr;
+	void *vaddr, *sg_vaddr;
+	struct dpa_bp *dpa_bp;
+	struct page *page;
+	int frag_offset, frag_len;
+	int page_offset;
+	int i, ret;
+	unsigned int copy_size = DPA_COPIED_HEADERS_SIZE;
+	const t_FmPrsResult *parse_results;
+
+	vaddr = phys_to_virt(addr);
+	/*
+	 * In the case of a SG frame, FMan stores the Internal Context
+	 * in the buffer containing the sgt.
+	 */
+	parse_results = (const t_FmPrsResult *)(vaddr + DPA_RX_PRIV_DATA_SIZE);
+	/* Validate the frame before anything else. */
+	ret = _dpa_process_parse_results(parse_results, fd, skb, use_gro,
+		&copy_size);
+	if (unlikely(ret))
+		/* Bad frame, stop processing now. */
+		return ret;
+
+	/*
+	 * Iterate through the SGT entries and add the data buffers as
+	 * skb fragments
+	 */
+	sgt = vaddr + dpa_fd_offset(fd);
+	for (i = 0; i < DPA_SGT_MAX_ENTRIES; i++) {
+		/* Extension bit is not supported */
+		BUG_ON(sgt[i].extension);
+
+		dpa_bp = dpa_bpid2pool(sgt[i].bpid);
+		BUG_ON(IS_ERR(dpa_bp));
+
+		sg_addr = qm_sg_addr(&sgt[i]);
+		sg_vaddr = phys_to_virt(sg_addr);
+
+		dpa_bp_removed_one_page(dpa_bp, sg_addr);
+		page = pfn_to_page(sg_addr >> PAGE_SHIFT);
+
+		/*
+		 * Padding at the beginning of the page
+		 * (offset in page from where BMan buffer begins)
+		 */
+		page_offset = (unsigned long)sg_vaddr & (PAGE_SIZE - 1);
+
+		if (i == 0) {
+			/* This is the first fragment */
+			/* Move the network headers in the skb linear portion */
+			memcpy(skb_put(skb, copy_size),
+				sg_vaddr + sgt[i].offset,
+				copy_size);
+
+			/* Adjust offset/length for the remaining data */
+			frag_offset = sgt[i].offset + page_offset + copy_size;
+			frag_len = sgt[i].length - copy_size;
+		} else {
+			/*
+			 * Not the first fragment; all data from buferr will
+			 * be added in an skb fragment
+			 */
+			frag_offset = sgt[i].offset + page_offset;
+			frag_len = sgt[i].length;
+		}
+		/*
+		 * Add data buffer to the skb
+		 *
+		 * TODO kernel 3.8 fixup; we might want to account for
+		 * the true-truesize.
+		 */
+		skb_add_rx_frag(skb, i, page, frag_offset, frag_len, frag_len);
+
+		if (sgt[i].final)
+			break;
+	}
+
+#ifdef CONFIG_FSL_DPA_1588
+	if (priv->tsu && priv->tsu->valid && priv->tsu->hwts_rx_en_ioctl)
+		dpa_ptp_store_rxstamp(priv->net_dev, skb, fd);
+#endif
+
+	/* recycle the SGT page */
+	dpa_bp = dpa_bpid2pool(fd->bpid);
+	BUG_ON(IS_ERR(dpa_bp));
+	dpa_bp_add_page(dpa_bp, (unsigned long)vaddr);
+
+	return 0;
+}
+
+void __hot _dpa_rx(struct net_device *net_dev,
+		const struct dpa_priv_s *priv,
+		struct dpa_percpu_priv_s *percpu_priv,
+		const struct qm_fd *fd,
+		u32 fqid)
+{
+	struct dpa_bp *dpa_bp;
+	struct sk_buff *skb;
+	dma_addr_t addr = qm_fd_addr(fd);
+	u32 fd_status = fd->status;
+	unsigned int skb_len;
+	int use_gro = net_dev->features & NETIF_F_GRO;
+
+	if (unlikely(fd_status & FM_FD_STAT_ERRORS) != 0) {
+		if (netif_msg_hw(priv) && net_ratelimit())
+			cpu_netdev_warn(net_dev, "FD status = 0x%08x\n",
+					fd_status & FM_FD_STAT_ERRORS);
+
+		percpu_priv->stats.rx_errors++;
+		goto _release_frame;
+	}
+
+	dpa_bp = dpa_bpid2pool(fd->bpid);
+	skb = dpa_list_get_skb(percpu_priv);
+
+	if (unlikely(skb == NULL)) {
+		/* List is empty, so allocate a new skb */
+		skb = dev_alloc_skb(DPA_BP_HEAD + dpa_get_rx_extra_headroom() +
+			DPA_COPIED_HEADERS_SIZE);
+		if (unlikely(skb == NULL)) {
+			if (netif_msg_rx_err(priv) && net_ratelimit())
+				cpu_netdev_err(net_dev,
+						"Could not alloc skb\n");
+			percpu_priv->stats.rx_dropped++;
+			goto _release_frame;
+		}
+	}
+
+	/* TODO We might want to do some prefetches here (skb, shinfo, data) */
+
+	/*
+	 * Make sure forwarded skbs will have enough space on Tx,
+	 * if extra headers are added.
+	 */
+	skb_reserve(skb, DPA_BP_HEAD + dpa_get_rx_extra_headroom());
+
+	dpa_bp_removed_one_page(dpa_bp, addr);
+
+	/* prefetch the first 64 bytes of the frame or the SGT start */
+	prefetch(phys_to_virt(addr) + dpa_fd_offset(fd));
+
+	if (likely(fd->format == qm_fd_contig)) {
+		if (unlikely(contig_fd_to_skb(priv, fd, skb, &use_gro))) {
+			/*
+			 * There was a L4 HXS error - e.g. the L4 csum was
+			 * invalid - so drop the frame early instead of passing
+			 * it on to the stack. We'll increment our private
+			 * counters to track this event.
+			 */
+			percpu_priv->l4_hxs_errors++;
+			percpu_priv->stats.rx_dropped++;
+			goto drop_bad_frame;
+		}
+	} else if (fd->format == qm_fd_sg) {
+		if (unlikely(sg_fd_to_skb(priv, fd, skb, &use_gro))) {
+			percpu_priv->l4_hxs_errors++;
+			percpu_priv->stats.rx_dropped++;
+			goto drop_bad_frame;
+		}
+	} else
+		/* The only FD types that we may receive are contig and S/G */
+		BUG();
+
+	skb->protocol = eth_type_trans(skb, net_dev);
+
+	if (unlikely(dpa_check_rx_mtu(skb, net_dev->mtu))) {
+		percpu_priv->stats.rx_dropped++;
+		goto drop_bad_frame;
+	}
+
+	skb_len = skb->len;
+
+	if (use_gro) {
+		gro_result_t gro_result;
+
+		gro_result = napi_gro_receive(&percpu_priv->napi, skb);
+		if (unlikely(gro_result == GRO_DROP)) {
+			percpu_priv->stats.rx_dropped++;
+			goto packet_dropped;
+		}
+	} else if (unlikely(netif_receive_skb(skb) == NET_RX_DROP)) {
+		percpu_priv->stats.rx_dropped++;
+		goto packet_dropped;
+	}
+
+	percpu_priv->stats.rx_packets++;
+	percpu_priv->stats.rx_bytes += skb_len;
+
+packet_dropped:
+	net_dev->last_rx = jiffies;
+	return;
+
+drop_bad_frame:
+	dev_kfree_skb(skb);
+	return;
+
+_release_frame:
+	dpa_fd_release(net_dev, fd);
+}
+
+static int __hot skb_to_contig_fd(struct dpa_priv_s *priv,
+				  struct sk_buff *skb, struct qm_fd *fd)
+{
+	struct sk_buff **skbh;
+	dma_addr_t addr;
+	struct dpa_bp *dpa_bp;
+	struct net_device *net_dev = priv->net_dev;
+	int err;
+
+	/* We are guaranteed that we have at least DPA_BP_HEAD of headroom. */
+	skbh = (struct sk_buff **)(skb->data - DPA_BP_HEAD);
+
+	*skbh = skb;
+
+	dpa_bp = priv->dpa_bp;
+
+	/*
+	 * Enable L3/L4 hardware checksum computation.
+	 *
+	 * We must do this before dma_map_single(DMA_TO_DEVICE), because we may
+	 * need to write into the skb.
+	 */
+	err = dpa_enable_tx_csum(priv, skb, fd,
+				 ((char *)skbh) + DPA_TX_PRIV_DATA_SIZE);
+	if (unlikely(err < 0)) {
+		if (netif_msg_tx_err(priv) && net_ratelimit())
+			cpu_netdev_err(net_dev, "HW csum error: %d\n", err);
+		return err;
+	}
+
+	/* Fill in the FD */
+	fd->format = qm_fd_contig;
+	fd->length20 = skb->len;
+	fd->offset = DPA_BP_HEAD; /* This is now guaranteed */
+
+	addr = dma_map_single(dpa_bp->dev, skbh, dpa_bp->size, DMA_TO_DEVICE);
+	if (unlikely(dma_mapping_error(dpa_bp->dev, addr))) {
+		if (netif_msg_tx_err(priv) && net_ratelimit())
+			cpu_netdev_err(net_dev, "dma_map_single() failed\n");
+		return -EINVAL;
+	}
+	fd->addr_hi = upper_32_bits(addr);
+	fd->addr_lo = lower_32_bits(addr);
+
+	return 0;
+}
+
+static int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
+			      struct dpa_percpu_priv_s *percpu_priv,
+			      struct sk_buff *skb, struct qm_fd *fd)
+{
+	struct dpa_bp *dpa_bp = priv->dpa_bp;
+	dma_addr_t addr;
+	struct sk_buff **skbh;
+	struct net_device *net_dev = priv->net_dev;
+	int err;
+
+	struct qm_sg_entry *sgt;
+	unsigned long sgt_page, sg0_page;
+	void *buffer_start;
+	skb_frag_t *frag;
+	int i, j, nr_frags;
+	enum dma_data_direction dma_dir;
+	bool can_recycle = false;
+
+	fd->format = qm_fd_sg;
+
+	/* get a new page to store the SGTable */
+	sgt_page = __get_free_page(GFP_ATOMIC);
+	if (unlikely(!sgt_page)) {
+		dpaa_eth_err(dpa_bp->dev, "__get_free_page() failed\n");
+		return -ENOMEM;
+	}
+
+	/*
+	 * Enable L3/L4 hardware checksum computation.
+	 *
+	 * We must do this before dma_map_single(DMA_TO_DEVICE), because we may
+	 * need to write into the skb.
+	 */
+	err = dpa_enable_tx_csum(priv, skb, fd,
+				 (void *)sgt_page + DPA_TX_PRIV_DATA_SIZE);
+	if (unlikely(err < 0)) {
+		if (netif_msg_tx_err(priv) && net_ratelimit())
+			cpu_netdev_err(net_dev, "HW csum error: %d\n", err);
+		goto csum_failed;
+	}
+
+	sgt = (struct qm_sg_entry *)(sgt_page + DPA_BP_HEAD);
+	/*
+	 * TODO: do we need to memset all entries or just the number of entries
+	 * we really use? Might improve perf...
+	 */
+	memset(sgt, 0, DPA_SGT_MAX_ENTRIES * sizeof(*sgt));
+
+	/*
+	 * Decide whether the skb is recycleable. We will need this information
+	 * upfront to decide what DMA mapping direction we want to use.
+	 */
+	nr_frags =  skb_shinfo(skb)->nr_frags;
+	if (!skb_cloned(skb) && !skb_shared(skb) &&
+	   (*percpu_priv->dpa_bp_count + nr_frags + 2 < dpa_bp->target_count)) {
+		can_recycle = true;
+		/*
+		 * We want each fragment to have at least dpa_bp->size bytes.
+		 * If even one fragment is smaller, the entire FD becomes
+		 * unrecycleable.
+		 * Same holds if the fragments are allocated from highmem.
+		 */
+		for (i = 0; i < nr_frags; i++) {
+			skb_frag_t *crt_frag = &skb_shinfo(skb)->frags[i];
+			if ((crt_frag->size < dpa_bp->size) ||
+			    PageHighMem(crt_frag->page.p)) {
+				can_recycle = false;
+				break;
+			}
+		}
+	}
+	dma_dir = can_recycle ? DMA_BIDIRECTIONAL : DMA_TO_DEVICE;
+
+	/*
+	 * Populate the first SGT entry
+	 * get a new page to store the skb linear buffer content
+	 * in the first SGT entry
+	 *
+	 * TODO: See if we can use the original page that contains
+	 * the linear buffer
+	 */
+	sg0_page = __get_free_page(GFP_ATOMIC);
+	if (unlikely(!sg0_page)) {
+		dpaa_eth_err(dpa_bp->dev, "__get_free_page() failed\n");
+		err = -ENOMEM;
+		goto sg0_page_alloc_failed;
+	}
+
+	sgt[0].bpid = dpa_bp->bpid;
+	sgt[0].offset = 0;
+	sgt[0].length = skb_headlen(skb);
+
+	/*
+	 * FIXME need more than one page if the linear part of the skb
+	 * is longer than PAGE_SIZE
+	 */
+	if (unlikely(sgt[0].offset + skb_headlen(skb) > dpa_bp->size)) {
+		pr_warn_once("tx headlen %d larger than available buffs %d\n",
+			skb_headlen(skb), dpa_bp->size);
+		err = -EINVAL;
+		goto skb_linear_too_large;
+	}
+
+	buffer_start = (void *)sg0_page;
+	memcpy(buffer_start + sgt[0].offset, skb->data, skb_headlen(skb));
+	addr = dma_map_single(dpa_bp->dev, buffer_start, dpa_bp->size, dma_dir);
+	if (unlikely(dma_mapping_error(dpa_bp->dev, addr))) {
+		dpaa_eth_err(dpa_bp->dev, "DMA mapping failed");
+		err = -EINVAL;
+		goto sg0_map_failed;
+
+	}
+	sgt[0].addr_hi = upper_32_bits(addr);
+	sgt[0].addr_lo = lower_32_bits(addr);
+
+	/* populate the rest of SGT entries */
+	for (i = 1; i <= skb_shinfo(skb)->nr_frags; i++) {
+		frag = &skb_shinfo(skb)->frags[i - 1];
+		sgt[i].bpid = dpa_bp->bpid;
+		sgt[i].offset = 0;
+		sgt[i].length = frag->size;
+
+		/* This shouldn't happen */
+		BUG_ON(!frag->page.p);
+
+		addr = dma_map_page(dpa_bp->dev, skb_frag_page(frag),
+			frag->page_offset, dpa_bp->size, dma_dir);
+
+		if (unlikely(dma_mapping_error(dpa_bp->dev, addr))) {
+			dpaa_eth_err(dpa_bp->dev, "DMA mapping failed");
+			err = -EINVAL;
+			goto sg_map_failed;
+		}
+
+		/* keep the offset in the address */
+		sgt[i].addr_hi = upper_32_bits(addr);
+		sgt[i].addr_lo = lower_32_bits(addr);
+	}
+	sgt[i - 1].final = 1;
+
+	fd->length20 = skb->len;
+	fd->offset = DPA_BP_HEAD;
+
+	/* DMA map the SGT page */
+	buffer_start = (void *)sgt - dpa_fd_offset(fd);
+	skbh = (struct sk_buff **)buffer_start;
+	*skbh = skb;
+
+	addr = dma_map_single(dpa_bp->dev, buffer_start, dpa_bp->size, dma_dir);
+	if (unlikely(dma_mapping_error(dpa_bp->dev, addr))) {
+		dpaa_eth_err(dpa_bp->dev, "DMA mapping failed");
+		err = -EINVAL;
+		goto sgt_map_failed;
+	}
+	fd->addr_hi = upper_32_bits(addr);
+	fd->addr_lo = lower_32_bits(addr);
+
+	if (can_recycle) {
+		/* all pages are going to be recycled */
+		fd->cmd |= FM_FD_CMD_FCO;
+		fd->bpid = dpa_bp->bpid;
+	}
+
+	return 0;
+
+sgt_map_failed:
+sg_map_failed:
+	for (j = 0; j < i; j++)
+		dma_unmap_page(dpa_bp->dev, qm_sg_addr(&sgt[j]),
+			dpa_bp->size, dma_dir);
+sg0_map_failed:
+	free_page(sg0_page);
+skb_linear_too_large:
+sg0_page_alloc_failed:
+csum_failed:
+	free_page(sgt_page);
+
+	return err;
+}
+
+int __hot dpa_tx(struct sk_buff *skb, struct net_device *net_dev)
+{
+	struct dpa_priv_s	*priv;
+	struct qm_fd		 fd;
+	struct dpa_percpu_priv_s *percpu_priv;
+	int queue_mapping;
+	int err;
+
+	priv = netdev_priv(net_dev);
+	percpu_priv = per_cpu_ptr(priv->percpu_priv, smp_processor_id());
+
+	clear_fd(&fd);
+
+	queue_mapping = skb_get_queue_mapping(skb);
+
+
+#ifdef CONFIG_FSL_DPA_1588
+	if (priv->tsu && priv->tsu->valid && priv->tsu->hwts_tx_en_ioctl)
+		fd.cmd |= FM_FD_CMD_UPD;
+#endif
+
+	if (skb_is_nonlinear(skb)) {
+		/* Just create a S/G fd based on the skb */
+		err = skb_to_sg_fd(priv, percpu_priv, skb, &fd);
+		percpu_priv->tx_frag_skbuffs++;
+	} else {
+		/*
+		 * Make sure we have enough headroom to accomodate private
+		 * data, parse results, etc
+		 */
+		if (skb_headroom(skb) < DPA_BP_HEAD) {
+			struct sk_buff *skb_new;
+
+			skb_new = skb_realloc_headroom(skb, DPA_BP_HEAD);
+			if (unlikely(!skb_new)) {
+				dev_kfree_skb(skb);
+				percpu_priv->stats.tx_errors++;
+				return NETDEV_TX_OK;
+			}
+			dev_kfree_skb(skb);
+			skb = skb_new;
+		}
+
+		/*
+		 * We're going to store the skb backpointer at the beginning
+		 * of the data buffer, so we need a privately owned skb
+		 */
+		skb = skb_unshare(skb, GFP_ATOMIC);
+		if (unlikely(!skb)) {
+			percpu_priv->stats.tx_errors++;
+			return NETDEV_TX_OK;
+		}
+
+		/* Finally, create a contig FD from this skb */
+		err = skb_to_contig_fd(priv, skb, &fd);
+	}
+	if (unlikely(err < 0)) {
+		percpu_priv->stats.tx_errors++;
+		dev_kfree_skb(skb);
+		return NETDEV_TX_OK;
+	}
+
+#if (DPAA_VERSION >= 11)
+	fd.cmd &= ~FM_FD_CMD_FCO;
+#endif
+
+	if (fd.cmd & FM_FD_CMD_FCO) {
+		/*
+		 * Need to free the skb, but without releasing
+		 * the page fragments, so increment the pages usage count
+		 */
+		int i;
+
+		for (i = 0; i < skb_shinfo(skb)->nr_frags; i++)
+			get_page(skb_shinfo(skb)->frags[i].page.p);
+
+		/*
+		 * We release back to the pool a number of pages equal to
+		 * the number of skb fragments + one page for the linear
+		 * portion of the skb + one page for the S/G table
+		 */
+		*percpu_priv->dpa_bp_count += skb_shinfo(skb)->nr_frags + 2;
+		percpu_priv->tx_returned++;
+		dev_kfree_skb(skb);
+		skb = NULL;
+	}
+
+	if (unlikely(dpa_xmit(priv, percpu_priv, queue_mapping, &fd) < 0))
+		goto xmit_failed;
+
+	net_dev->trans_start = jiffies;
+
+	return NETDEV_TX_OK;
+
+xmit_failed:
+	if (fd.cmd & FM_FD_CMD_FCO) {
+		*percpu_priv->dpa_bp_count -= skb_shinfo(skb)->nr_frags + 2;
+		percpu_priv->tx_returned--;
+
+		dpa_fd_release(net_dev, &fd);
+		return NETDEV_TX_OK;
+	}
+	_dpa_cleanup_tx_fd(priv, &fd);
+	dev_kfree_skb(skb);
+
+	return NETDEV_TX_OK;
+}
+
+#endif /* CONFIG_DPAA_ETH_SG_SUPPORT */
diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth_sysfs.c b/drivers/net/ethernet/freescale/dpa/dpaa_eth_sysfs.c
new file mode 100644
index 0000000..69aa8f9
--- /dev/null
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth_sysfs.c
@@ -0,0 +1,181 @@
+
+/*
+ * Copyright 2008-2012 Freescale Semiconductor Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *	 notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *	 notice, this list of conditions and the following disclaimer in the
+ *	 documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *	 names of its contributors may be used to endorse or promote products
+ *	 derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kthread.h>
+#include <linux/io.h>
+#include <linux/of_net.h>
+#include "dpaa_eth.h"
+
+static ssize_t dpaa_eth_show_addr(struct device *dev,
+		struct device_attribute *attr, char *buf)
+{
+	struct dpa_priv_s *priv = netdev_priv(to_net_dev(dev));
+	struct mac_device *mac_dev = priv->mac_dev;
+
+	if (mac_dev)
+		return sprintf(buf, "%llx",
+				(unsigned long long)mac_dev->res->start);
+	else
+		return sprintf(buf, "none");
+}
+
+static ssize_t dpaa_eth_show_fqids(struct device *dev,
+		struct device_attribute *attr, char *buf)
+{
+	struct dpa_priv_s *priv = netdev_priv(to_net_dev(dev));
+	ssize_t bytes = 0;
+	int i = 0;
+	char *str;
+	struct dpa_fq *fq;
+	struct dpa_fq *tmp;
+	struct dpa_fq *prev = NULL;
+	u32 first_fqid = 0;
+	u32 last_fqid = 0;
+	char *prevstr = NULL;
+
+	list_for_each_entry_safe(fq, tmp, &priv->dpa_fq_list, list) {
+		switch (fq->fq_type) {
+		case FQ_TYPE_RX_DEFAULT:
+			str = "Rx default";
+			break;
+		case FQ_TYPE_RX_ERROR:
+			str = "Rx error";
+			break;
+		case FQ_TYPE_RX_PCD:
+			str = "Rx PCD";
+			break;
+		case FQ_TYPE_TX_CONFIRM:
+			str = "Tx confirmation";
+			break;
+		case FQ_TYPE_TX_ERROR:
+			str = "Tx error";
+			break;
+		case FQ_TYPE_TX:
+			str = "Tx";
+			break;
+		default:
+			str = "Unknown";
+		}
+
+		if (prev && (abs(fq->fqid - prev->fqid) != 1 ||
+					str != prevstr)) {
+			if (last_fqid == first_fqid)
+				bytes += sprintf(buf + bytes,
+					"%s: %d\n", prevstr, prev->fqid);
+			else
+				bytes += sprintf(buf + bytes,
+					"%s: %d - %d\n", prevstr,
+					first_fqid, last_fqid);
+		}
+
+		if (prev && abs(fq->fqid - prev->fqid) == 1 && str == prevstr)
+			last_fqid = fq->fqid;
+		else
+			first_fqid = last_fqid = fq->fqid;
+
+		prev = fq;
+		prevstr = str;
+		i++;
+	}
+
+	if (prev) {
+		if (last_fqid == first_fqid)
+			bytes += sprintf(buf + bytes, "%s: %d\n", prevstr,
+					prev->fqid);
+		else
+			bytes += sprintf(buf + bytes, "%s: %d - %d\n", prevstr,
+					first_fqid, last_fqid);
+	}
+
+	return bytes;
+}
+
+static ssize_t dpaa_eth_show_dflt_bpid(struct device *dev,
+		struct device_attribute *attr, char *buf)
+{
+	ssize_t bytes = 0;
+	struct dpa_priv_s *priv = netdev_priv(to_net_dev(dev));
+	struct dpa_bp *dpa_bp = priv->dpa_bp;
+
+	if (priv->bp_count != 1)
+		bytes += snprintf(buf, PAGE_SIZE, "-1\n");
+	else
+		bytes += snprintf(buf, PAGE_SIZE, "%u\n", dpa_bp->bpid);
+
+	return bytes;
+}
+
+static ssize_t dpaa_eth_show_mac_regs(struct device *dev,
+		struct device_attribute *attr, char *buf)
+{
+	struct dpa_priv_s *priv = netdev_priv(to_net_dev(dev));
+	struct mac_device *mac_dev = priv->mac_dev;
+
+	fm_mac_dump_regs(mac_dev);
+
+	return 0;
+}
+
+static struct device_attribute dpaa_eth_attrs[] = {
+	__ATTR(device_addr, S_IRUGO, dpaa_eth_show_addr, NULL),
+	__ATTR(fqids, S_IRUGO, dpaa_eth_show_fqids, NULL),
+	__ATTR(dflt_bpid, S_IRUGO, dpaa_eth_show_dflt_bpid, NULL),
+	__ATTR(mac_regs, S_IRUGO, dpaa_eth_show_mac_regs, NULL)
+};
+
+void dpaa_eth_sysfs_init(struct device *dev)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(dpaa_eth_attrs); i++)
+		if (device_create_file(dev, &dpaa_eth_attrs[i])) {
+			dev_err(dev, "Error creating sysfs file\n");
+			goto device_create_file_failed;
+		}
+
+	return;
+
+device_create_file_failed:
+	while (i > 0)
+		device_remove_file(dev, &dpaa_eth_attrs[--i]);
+}
+
+void dpaa_eth_sysfs_remove(struct device *dev)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(dpaa_eth_attrs); i++)
+		device_remove_file(dev, &dpaa_eth_attrs[i]);
+}
diff --git a/drivers/net/ethernet/freescale/dpa/mac-api.c b/drivers/net/ethernet/freescale/dpa/mac-api.c
new file mode 100644
index 0000000..00bf944
--- /dev/null
+++ b/drivers/net/ethernet/freescale/dpa/mac-api.c
@@ -0,0 +1,839 @@
+/* Copyright 2008-2012 Freescale Semiconductor, Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *	 notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *	 notice, this list of conditions and the following disclaimer in the
+ *	 documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *	 names of its contributors may be used to endorse or promote products
+ *	 derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/of_platform.h>
+#include <linux/of_mdio.h>
+#include <linux/phy.h>
+#include <linux/netdevice.h>
+
+#include "dpaa_eth-common.h"
+#include "dpaa_eth.h"
+#include "mac.h"
+
+#include "error_ext.h"	/* GET_ERROR_TYPE, E_OK */
+#include "fm_mac_ext.h"
+#include "fm_rtc_ext.h"
+
+#define MAC_DESCRIPTION "FSL FMan MAC API based driver"
+
+MODULE_LICENSE("Dual BSD/GPL");
+
+MODULE_AUTHOR("Emil Medve <Emilian.Medve@Freescale.com>");
+
+MODULE_DESCRIPTION(MAC_DESCRIPTION);
+
+struct mac_priv_s {
+	t_Handle	mac;
+};
+
+const char	*mac_driver_description __initconst = MAC_DESCRIPTION;
+const size_t	 mac_sizeof_priv[] __devinitconst = {
+	[DTSEC] = sizeof(struct mac_priv_s),
+	[XGMAC] = sizeof(struct mac_priv_s),
+	[MEMAC] = sizeof(struct mac_priv_s)
+};
+
+static const e_EnetMode _100[] __devinitconst =
+{
+	[PHY_INTERFACE_MODE_MII]	= e_ENET_MODE_MII_100,
+	[PHY_INTERFACE_MODE_RMII]	= e_ENET_MODE_RMII_100
+};
+
+static const e_EnetMode _1000[] __devinitconst =
+{
+	[PHY_INTERFACE_MODE_GMII]	= e_ENET_MODE_GMII_1000,
+	[PHY_INTERFACE_MODE_SGMII]	= e_ENET_MODE_SGMII_1000,
+	[PHY_INTERFACE_MODE_TBI]	= e_ENET_MODE_TBI_1000,
+	[PHY_INTERFACE_MODE_RGMII]	= e_ENET_MODE_RGMII_1000,
+	[PHY_INTERFACE_MODE_RGMII_ID]	= e_ENET_MODE_RGMII_1000,
+	[PHY_INTERFACE_MODE_RGMII_RXID]	= e_ENET_MODE_RGMII_1000,
+	[PHY_INTERFACE_MODE_RGMII_TXID]	= e_ENET_MODE_RGMII_1000,
+	[PHY_INTERFACE_MODE_RTBI]	= e_ENET_MODE_RTBI_1000
+};
+
+static e_EnetMode __devinit __cold __attribute__((nonnull))
+macdev2enetinterface(const struct mac_device *mac_dev)
+{
+	switch (mac_dev->max_speed) {
+	case SPEED_100:
+		return _100[mac_dev->phy_if];
+	case SPEED_1000:
+		return _1000[mac_dev->phy_if];
+	case SPEED_10000:
+		return e_ENET_MODE_XGMII_10000;
+	default:
+		return e_ENET_MODE_MII_100;
+	}
+}
+
+static void mac_exception(t_Handle _mac_dev, e_FmMacExceptions exception)
+{
+	struct mac_device	*mac_dev;
+
+	mac_dev = (struct mac_device *)_mac_dev;
+
+	if (e_FM_MAC_EX_10G_RX_FIFO_OVFL == exception) {
+		/* don't flag RX FIFO after the first */
+		FM_MAC_SetException(
+		    ((struct mac_priv_s *)macdev_priv(_mac_dev))->mac,
+		    e_FM_MAC_EX_10G_RX_FIFO_OVFL, false);
+		printk(KERN_ERR "10G MAC got RX FIFO Error = %x\n", exception);
+	}
+
+	cpu_dev_dbg(mac_dev->dev, "%s:%s() -> %d\n", __file__, __func__,
+		exception);
+}
+
+static int __devinit __cold init(struct mac_device *mac_dev)
+{
+	int					_errno;
+	t_Error				err;
+	struct mac_priv_s	*priv;
+	t_FmMacParams		param;
+	uint32_t			version;
+
+	priv = macdev_priv(mac_dev);
+
+	param.baseAddr =  (typeof(param.baseAddr))(uintptr_t)devm_ioremap(
+		mac_dev->dev, mac_dev->res->start, 0x2000);
+	param.enetMode	= macdev2enetinterface(mac_dev);
+	memcpy(&param.addr, mac_dev->addr, min(sizeof(param.addr),
+		sizeof(mac_dev->addr)));
+	param.macId			= mac_dev->cell_index;
+	param.h_Fm 			= (t_Handle)mac_dev->fm;
+	param.mdioIrq		= NO_IRQ;
+	param.f_Exception	= mac_exception;
+	param.f_Event		= mac_exception;
+	param.h_App			= mac_dev;
+
+	priv->mac = FM_MAC_Config(&param);
+	if (unlikely(priv->mac == NULL)) {
+		dpaa_eth_err(mac_dev->dev, "FM_MAC_Config() failed\n");
+		_errno = -EINVAL;
+		goto _return;
+	}
+
+	fm_mac_set_handle(mac_dev->fm_dev, priv->mac,
+		(macdev2enetinterface(mac_dev) != e_ENET_MODE_XGMII_10000) ?
+			param.macId : param.macId + FM_MAX_NUM_OF_1G_MACS);
+
+	err = FM_MAC_ConfigMaxFrameLength(priv->mac,
+					  fm_get_max_frm());
+	_errno = -GET_ERROR_TYPE(err);
+	if (unlikely(_errno < 0)) {
+		dpaa_eth_err(mac_dev->dev,
+			"FM_MAC_ConfigMaxFrameLength() = 0x%08x\n", err);
+		goto _return_fm_mac_free;
+	}
+
+	if (macdev2enetinterface(mac_dev) != e_ENET_MODE_XGMII_10000) {
+		/* 10G always works with pad and CRC */
+		err = FM_MAC_ConfigPadAndCrc(priv->mac, true);
+		_errno = -GET_ERROR_TYPE(err);
+		if (unlikely(_errno < 0)) {
+			dpaa_eth_err(mac_dev->dev,
+				"FM_MAC_ConfigPadAndCrc() = 0x%08x\n", err);
+			goto _return_fm_mac_free;
+		}
+
+		err = FM_MAC_ConfigHalfDuplex(priv->mac, mac_dev->half_duplex);
+		_errno = -GET_ERROR_TYPE(err);
+		if (unlikely(_errno < 0)) {
+			dpaa_eth_err(mac_dev->dev,
+				"FM_MAC_ConfigHalfDuplex() = 0x%08x\n", err);
+			goto _return_fm_mac_free;
+		}
+	}
+	else  {
+		err = FM_MAC_ConfigResetOnInit(priv->mac, true);
+		_errno = -GET_ERROR_TYPE(err);
+		if (unlikely(_errno < 0)) {
+			dpaa_eth_err(mac_dev->dev,
+				"FM_MAC_ConfigResetOnInit() = 0x%08x\n", err);
+			goto _return_fm_mac_free;
+		}
+	}
+
+	err = FM_MAC_Init(priv->mac);
+	_errno = -GET_ERROR_TYPE(err);
+	if (unlikely(_errno < 0)) {
+		dpaa_eth_err(mac_dev->dev, "FM_MAC_Init() = 0x%08x\n", err);
+		goto _return_fm_mac_free;
+	}
+
+#ifndef CONFIG_FMAN_MIB_CNT_OVF_IRQ_EN
+	/* For 1G MAC, disable by default the MIB counters overflow interrupt */
+	if (macdev2enetinterface(mac_dev) != e_ENET_MODE_XGMII_10000) {
+		err = FM_MAC_SetException(priv->mac,
+				e_FM_MAC_EX_1G_RX_MIB_CNT_OVFL, FALSE);
+		_errno = -GET_ERROR_TYPE(err);
+		if (unlikely(_errno < 0)) {
+			dpaa_eth_err(mac_dev->dev,
+				"FM_MAC_SetException() = 0x%08x\n", err);
+			goto _return_fm_mac_free;
+		}
+	}
+#endif /* !CONFIG_FMAN_MIB_CNT_OVF_IRQ_EN */
+
+	/* For 10G MAC, disable Tx ECC exception */
+	if (macdev2enetinterface(mac_dev) == e_ENET_MODE_XGMII_10000) {
+		err = FM_MAC_SetException(priv->mac,
+					  e_FM_MAC_EX_10G_1TX_ECC_ER, FALSE);
+		_errno = -GET_ERROR_TYPE(err);
+		if (unlikely(_errno < 0)) {
+			dpaa_eth_err(mac_dev->dev,
+				"FM_MAC_SetException() = 0x%08x\n", err);
+			goto _return_fm_mac_free;
+		}
+	}
+
+	err = FM_MAC_GetVesrion(priv->mac, &version);
+	_errno = -GET_ERROR_TYPE(err);
+	if (unlikely(_errno < 0)) {
+		dpaa_eth_err(mac_dev->dev, "FM_MAC_GetVesrion() = 0x%08x\n",
+				err);
+		goto _return_fm_mac_free;
+	}
+	cpu_dev_info(mac_dev->dev, "FMan %s version: 0x%08x\n",
+		((macdev2enetinterface(mac_dev) != e_ENET_MODE_XGMII_10000) ?
+			"dTSEC" : "XGEC"), version);
+
+	goto _return;
+
+
+_return_fm_mac_free:
+	err = FM_MAC_Free(priv->mac);
+	if (unlikely(-GET_ERROR_TYPE(err) < 0))
+		dpaa_eth_err(mac_dev->dev, "FM_MAC_Free() = 0x%08x\n", err);
+_return:
+	return _errno;
+}
+
+static int __devinit __cold memac_init(struct mac_device *mac_dev)
+{
+	int			_errno;
+	t_Error			err;
+	struct mac_priv_s	*priv;
+	t_FmMacParams		param;
+
+	priv = macdev_priv(mac_dev);
+
+	param.baseAddr =  (typeof(param.baseAddr))(uintptr_t)devm_ioremap(
+		mac_dev->dev, mac_dev->res->start, 0x2000);
+	param.enetMode	= macdev2enetinterface(mac_dev);
+	memcpy(&param.addr, mac_dev->addr, sizeof(mac_dev->addr));
+	param.macId		= mac_dev->cell_index;
+	param.h_Fm		= (t_Handle)mac_dev->fm;
+	param.mdioIrq		= NO_IRQ;
+	param.f_Exception	= mac_exception;
+	param.f_Event		= mac_exception;
+	param.h_App		= mac_dev;
+
+	priv->mac = FM_MAC_Config(&param);
+	if (unlikely(priv->mac == NULL)) {
+		dev_err(mac_dev->dev, "FM_MAC_Config() failed\n");
+		_errno = -EINVAL;
+		goto _return;
+	}
+
+	err = FM_MAC_ConfigMaxFrameLength(priv->mac, fm_get_max_frm());
+	_errno = -GET_ERROR_TYPE(err);
+	if (unlikely(_errno < 0)) {
+		dev_err(mac_dev->dev,
+			"FM_MAC_ConfigMaxFrameLength() = 0x%08x\n", err);
+		goto _return_fm_mac_free;
+	}
+
+	err = FM_MAC_ConfigResetOnInit(priv->mac, true);
+	_errno = -GET_ERROR_TYPE(err);
+	if (unlikely(_errno < 0)) {
+		dev_err(mac_dev->dev,
+			"FM_MAC_ConfigResetOnInit() = 0x%08x\n", err);
+		goto _return_fm_mac_free;
+	}
+
+	err = FM_MAC_Init(priv->mac);
+	_errno = -GET_ERROR_TYPE(err);
+	if (unlikely(_errno < 0)) {
+		dev_err(mac_dev->dev, "FM_MAC_Init() = 0x%08x\n", err);
+		goto _return_fm_mac_free;
+	}
+
+	dev_info(mac_dev->dev, "FMan MEMAC\n");
+
+	goto _return;
+
+_return_fm_mac_free:
+	err = FM_MAC_Free(priv->mac);
+	if (unlikely(-GET_ERROR_TYPE(err) < 0))
+		dev_err(mac_dev->dev, "FM_MAC_Free() = 0x%08x\n", err);
+_return:
+	return _errno;
+}
+
+static int __cold start(struct mac_device *mac_dev)
+{
+	int	 _errno;
+	t_Error	 err;
+	struct phy_device *phy_dev = mac_dev->phy_dev;
+
+	err = FM_MAC_Enable(((struct mac_priv_s *)macdev_priv(mac_dev))->mac,
+			e_COMM_MODE_RX_AND_TX);
+	_errno = -GET_ERROR_TYPE(err);
+	if (unlikely(_errno < 0))
+		dpaa_eth_err(mac_dev->dev, "FM_MAC_Enable() = 0x%08x\n", err);
+
+	if (phy_dev) {
+		if (macdev2enetinterface(mac_dev) != e_ENET_MODE_XGMII_10000)
+			phy_start(phy_dev);
+		else if (phy_dev->drv->read_status)
+			phy_dev->drv->read_status(phy_dev);
+	}
+
+	return _errno;
+}
+
+static int __cold stop(struct mac_device *mac_dev)
+{
+	int	 _errno;
+	t_Error	 err;
+
+	if (mac_dev->phy_dev &&
+		(macdev2enetinterface(mac_dev) != e_ENET_MODE_XGMII_10000))
+		phy_stop(mac_dev->phy_dev);
+
+	err = FM_MAC_Disable(((struct mac_priv_s *)macdev_priv(mac_dev))->mac,
+				e_COMM_MODE_RX_AND_TX);
+	_errno = -GET_ERROR_TYPE(err);
+	if (unlikely(_errno < 0))
+		dpaa_eth_err(mac_dev->dev, "FM_MAC_Disable() = 0x%08x\n", err);
+
+	return _errno;
+}
+
+static int __cold change_promisc(struct mac_device *mac_dev)
+{
+	int	 _errno;
+	t_Error	 err;
+
+	err = FM_MAC_SetPromiscuous(
+			((struct mac_priv_s *)macdev_priv(mac_dev))->mac,
+			mac_dev->promisc = !mac_dev->promisc);
+	_errno = -GET_ERROR_TYPE(err);
+	if (unlikely(_errno < 0))
+		dpaa_eth_err(mac_dev->dev,
+				"FM_MAC_SetPromiscuous() = 0x%08x\n", err);
+
+	return _errno;
+}
+
+static int __cold set_multi(struct net_device *net_dev)
+{
+	struct dpa_priv_s       *priv;
+	struct mac_device       *mac_dev;
+	struct mac_priv_s 	*mac_priv;
+	struct mac_address	*old_addr, *tmp;
+	struct netdev_hw_addr	*ha;
+	int 			 _errno;
+	t_Error 		 err;
+
+	priv = netdev_priv(net_dev);
+	mac_dev = priv->mac_dev;
+	mac_priv = macdev_priv(mac_dev);
+
+	/* Clear previous address list */
+	list_for_each_entry_safe(old_addr, tmp, &mac_dev->mc_addr_list, list) {
+		err = FM_MAC_RemoveHashMacAddr(mac_priv->mac,
+					       (t_EnetAddr  *)old_addr->addr);
+		_errno = -GET_ERROR_TYPE(err);
+		if (_errno < 0) {
+			dpaa_eth_err(mac_dev->dev,
+				"FM_MAC_RemoveHashMacAddr() = 0x%08x\n", err);
+			return _errno;
+		}
+		list_del(&old_addr->list);
+		kfree(old_addr);
+	}
+
+	/* Add all the addresses from the new list */
+	netdev_for_each_mc_addr(ha, net_dev) {
+		err = FM_MAC_AddHashMacAddr(mac_priv->mac,
+				(t_EnetAddr *)ha->addr);
+		_errno = -GET_ERROR_TYPE(err);
+		if (_errno < 0) {
+			dpaa_eth_err(mac_dev->dev,
+				     "FM_MAC_AddHashMacAddr() = 0x%08x\n", err);
+			return _errno;
+		}
+		tmp = kmalloc(sizeof(struct mac_address), GFP_ATOMIC);
+		if (!tmp) {
+			dpaa_eth_err(mac_dev->dev, "Out of memory\n");
+			return -ENOMEM;
+		}
+		memcpy(tmp->addr, ha->addr, ETH_ALEN);
+		list_add(&tmp->list, &mac_dev->mc_addr_list);
+	}
+	return 0;
+}
+
+static int __cold change_addr(struct mac_device *mac_dev, uint8_t *addr)
+{
+	int	_errno;
+	t_Error err;
+
+	err = FM_MAC_ModifyMacAddr(
+			((struct mac_priv_s *)macdev_priv(mac_dev))->mac,
+			(t_EnetAddr *)addr);
+	_errno = -GET_ERROR_TYPE(err);
+	if (_errno < 0)
+		dpaa_eth_err(mac_dev->dev,
+			     "FM_MAC_ModifyMacAddr() = 0x%08x\n", err);
+
+	return _errno;
+}
+
+static void adjust_link(struct net_device *net_dev)
+{
+	struct dpa_priv_s *priv = netdev_priv(net_dev);
+	struct mac_device *mac_dev = priv->mac_dev;
+	struct phy_device *phy_dev = mac_dev->phy_dev;
+#if (DPAA_VERSION < 11)
+	struct mac_priv_s *mac_priv;
+#endif
+	int	 _errno;
+	t_Error	 err;
+
+	if (!phy_dev->link) {
+#if (DPAA_VERSION < 11)
+#warning fsl_pq_mdio_lock() no longer supported
+/*
+		fsl_pq_mdio_lock(NULL);
+*/
+
+		mac_priv = (struct mac_priv_s *)macdev_priv(mac_dev);
+		DtsecRestartTbiAN(mac_priv->mac);
+
+#warning fsl_pq_mdio_unlock() no longer supported
+/*
+		fsl_pq_mdio_unlock(NULL);
+*/
+#endif
+		return;
+	}
+
+	err = FM_MAC_AdjustLink(
+			((struct mac_priv_s *)macdev_priv(mac_dev))->mac,
+			phy_dev->speed, phy_dev->duplex);
+	_errno = -GET_ERROR_TYPE(err);
+	if (unlikely(_errno < 0))
+		dpaa_eth_err(mac_dev->dev, "FM_MAC_AdjustLink() = 0x%08x\n",
+				err);
+
+	return;
+}
+
+/* Initializes driver's PHY state, and attaches to the PHY.
+ * Returns 0 on success.
+ */
+static int dtsec_init_phy(struct net_device *net_dev)
+{
+	struct dpa_priv_s	*priv;
+	struct mac_device	*mac_dev;
+	struct phy_device	*phy_dev;
+
+	priv = netdev_priv(net_dev);
+	mac_dev = priv->mac_dev;
+
+	if (!mac_dev->phy_node)
+		phy_dev = phy_connect(net_dev, mac_dev->fixed_bus_id,
+				&adjust_link, mac_dev->phy_if);
+	else
+		phy_dev = of_phy_connect(net_dev, mac_dev->phy_node,
+				&adjust_link, 0, mac_dev->phy_if);
+	if (unlikely(phy_dev == NULL) || IS_ERR(phy_dev)) {
+		cpu_netdev_err(net_dev, "Could not connect to PHY %s\n",
+				mac_dev->phy_node ?
+					mac_dev->phy_node->full_name :
+					mac_dev->fixed_bus_id);
+		return phy_dev == NULL ? -ENODEV : PTR_ERR(phy_dev);
+	}
+
+	/* Remove any features not supported by the controller */
+	phy_dev->supported &= priv->mac_dev->if_support;
+	phy_dev->advertising = phy_dev->supported;
+
+	priv->mac_dev->phy_dev = phy_dev;
+
+	return 0;
+}
+
+static int xgmac_init_phy(struct net_device *net_dev)
+{
+	struct dpa_priv_s *priv = netdev_priv(net_dev);
+	struct mac_device *mac_dev = priv->mac_dev;
+	struct phy_device *phy_dev;
+
+	if (!mac_dev->phy_node)
+		phy_dev = phy_attach(net_dev, mac_dev->fixed_bus_id,
+				mac_dev->phy_if);
+	else
+		phy_dev = of_phy_attach(net_dev, mac_dev->phy_node, 0,
+				mac_dev->phy_if);
+	if (unlikely(phy_dev == NULL) || IS_ERR(phy_dev)) {
+		cpu_netdev_err(net_dev, "Could not attach to PHY %s\n",
+				mac_dev->phy_node ?
+					mac_dev->phy_node->full_name :
+					mac_dev->fixed_bus_id);
+		return phy_dev == NULL ? -ENODEV : PTR_ERR(phy_dev);
+	}
+
+	phy_dev->supported &= priv->mac_dev->if_support;
+	phy_dev->advertising = phy_dev->supported;
+
+	mac_dev->phy_dev = phy_dev;
+
+	return 0;
+}
+
+static int memac_init_phy(struct net_device *net_dev)
+{
+	struct dpa_priv_s       *priv;
+	struct mac_device       *mac_dev;
+	struct phy_device       *phy_dev;
+
+	priv = netdev_priv(net_dev);
+	mac_dev = priv->mac_dev;
+
+	if (macdev2enetinterface(mac_dev) == e_ENET_MODE_XGMII_10000) {
+		if (!mac_dev->phy_node)
+			phy_dev = phy_attach(net_dev, mac_dev->fixed_bus_id,
+				mac_dev->phy_if);
+		else
+			phy_dev = of_phy_attach(net_dev, mac_dev->phy_node, 0,
+				mac_dev->phy_if);
+	} else {
+		if (!mac_dev->phy_node)
+			phy_dev = phy_connect(net_dev, mac_dev->fixed_bus_id,
+				&adjust_link, mac_dev->phy_if);
+		else
+			phy_dev = of_phy_connect(net_dev, mac_dev->phy_node,
+				&adjust_link, 0, mac_dev->phy_if);
+	}
+
+	if (unlikely(phy_dev == NULL) || IS_ERR(phy_dev)) {
+		netdev_err(net_dev, "Could not connect to PHY %s\n",
+			mac_dev->phy_node ?
+				mac_dev->phy_node->full_name :
+				mac_dev->fixed_bus_id);
+		return phy_dev == NULL ? -ENODEV : PTR_ERR(phy_dev);
+	}
+
+	/* Remove any features not supported by the controller */
+	phy_dev->supported &= priv->mac_dev->if_support;
+	phy_dev->advertising = phy_dev->supported;
+
+	mac_dev->phy_dev = phy_dev;
+
+	return 0;
+}
+
+static int __cold uninit(struct mac_device *mac_dev)
+{
+	int			 _errno, __errno;
+	t_Error			 err;
+	const struct mac_priv_s	*priv;
+
+	priv = macdev_priv(mac_dev);
+
+	err = FM_MAC_Disable(priv->mac, e_COMM_MODE_RX_AND_TX);
+	_errno = -GET_ERROR_TYPE(err);
+	if (unlikely(_errno < 0))
+		dpaa_eth_err(mac_dev->dev, "FM_MAC_Disable() = 0x%08x\n", err);
+
+	err = FM_MAC_Free(priv->mac);
+	__errno = -GET_ERROR_TYPE(err);
+	if (unlikely(__errno < 0)) {
+		dpaa_eth_err(mac_dev->dev, "FM_MAC_Free() = 0x%08x\n", err);
+		if (_errno < 0)
+			_errno = __errno;
+	}
+
+	return _errno;
+}
+
+static int __cold ptp_enable(struct mac_device *mac_dev)
+{
+	int			 _errno;
+	t_Error			 err;
+	const struct mac_priv_s	*priv;
+
+	priv = macdev_priv(mac_dev);
+
+	err = FM_MAC_Enable1588TimeStamp(priv->mac);
+	_errno = -GET_ERROR_TYPE(err);
+	if (unlikely(_errno < 0))
+		dpaa_eth_err(mac_dev->dev, "FM_MAC_Enable1588TimeStamp()"
+				"= 0x%08x\n", err);
+	return _errno;
+}
+
+static int __cold ptp_disable(struct mac_device *mac_dev)
+{
+	int			 _errno;
+	t_Error			 err;
+	const struct mac_priv_s	*priv;
+
+	priv = macdev_priv(mac_dev);
+
+	err = FM_MAC_Disable1588TimeStamp(priv->mac);
+	_errno = -GET_ERROR_TYPE(err);
+	if (unlikely(_errno < 0))
+		dpaa_eth_err(mac_dev->dev, "FM_MAC_Disable1588TimeStamp()"
+				"= 0x%08x\n", err);
+	return _errno;
+}
+
+static void *get_mac_handle(struct mac_device *mac_dev)
+{
+	const struct mac_priv_s	*priv;
+	priv = macdev_priv(mac_dev);
+	return (void*)priv->mac;
+}
+
+static int __cold fm_rtc_enable(struct net_device *net_dev)
+{
+	struct dpa_priv_s *priv = netdev_priv(net_dev);
+	struct mac_device *mac_dev = priv->mac_dev;
+	int			 _errno;
+	t_Error			 err;
+
+	err = FM_RTC_Enable(fm_get_rtc_handle(mac_dev->fm_dev), 0);
+	_errno = -GET_ERROR_TYPE(err);
+	if (unlikely(_errno < 0))
+		dpaa_eth_err(mac_dev->dev, "FM_RTC_Enable = 0x%08x\n", err);
+
+	return _errno;
+}
+
+static int __cold fm_rtc_disable(struct net_device *net_dev)
+{
+	struct dpa_priv_s *priv = netdev_priv(net_dev);
+	struct mac_device *mac_dev = priv->mac_dev;
+	int			 _errno;
+	t_Error			 err;
+
+	err = FM_RTC_Disable(fm_get_rtc_handle(mac_dev->fm_dev));
+	_errno = -GET_ERROR_TYPE(err);
+	if (unlikely(_errno < 0))
+		dpaa_eth_err(mac_dev->dev, "FM_RTC_Disable = 0x%08x\n", err);
+
+	return _errno;
+}
+
+static int __cold fm_rtc_get_cnt(struct net_device *net_dev, uint64_t *ts)
+{
+	struct dpa_priv_s *priv = netdev_priv(net_dev);
+	struct mac_device *mac_dev = priv->mac_dev;
+	int _errno;
+	t_Error	err;
+
+	err = FM_RTC_GetCurrentTime(fm_get_rtc_handle(mac_dev->fm_dev), ts);
+	_errno = -GET_ERROR_TYPE(err);
+	if (unlikely(_errno < 0))
+		dpaa_eth_err(mac_dev->dev, "FM_RTC_GetCurrentTime = 0x%08x\n",
+				err);
+
+	return _errno;
+}
+
+static int __cold fm_rtc_set_cnt(struct net_device *net_dev, uint64_t ts)
+{
+	struct dpa_priv_s *priv = netdev_priv(net_dev);
+	struct mac_device *mac_dev = priv->mac_dev;
+	int _errno;
+	t_Error	err;
+
+	err = FM_RTC_SetCurrentTime(fm_get_rtc_handle(mac_dev->fm_dev), ts);
+	_errno = -GET_ERROR_TYPE(err);
+	if (unlikely(_errno < 0))
+		dpaa_eth_err(mac_dev->dev, "FM_RTC_SetCurrentTime = 0x%08x\n",
+				err);
+
+	return _errno;
+}
+
+static int __cold fm_rtc_get_drift(struct net_device *net_dev, uint32_t *drift)
+{
+	struct dpa_priv_s *priv = netdev_priv(net_dev);
+	struct mac_device *mac_dev = priv->mac_dev;
+	int _errno;
+	t_Error	err;
+
+	err = FM_RTC_GetFreqCompensation(fm_get_rtc_handle(mac_dev->fm_dev),
+			drift);
+	_errno = -GET_ERROR_TYPE(err);
+	if (unlikely(_errno < 0))
+		dpaa_eth_err(mac_dev->dev, "FM_RTC_GetFreqCompensation ="
+				"0x%08x\n", err);
+
+	return _errno;
+}
+
+static int __cold fm_rtc_set_drift(struct net_device *net_dev, uint32_t drift)
+{
+	struct dpa_priv_s *priv = netdev_priv(net_dev);
+	struct mac_device *mac_dev = priv->mac_dev;
+	int _errno;
+	t_Error	err;
+
+	err = FM_RTC_SetFreqCompensation(fm_get_rtc_handle(mac_dev->fm_dev),
+			drift);
+	_errno = -GET_ERROR_TYPE(err);
+	if (unlikely(_errno < 0))
+		dpaa_eth_err(mac_dev->dev, "FM_RTC_SetFreqCompensation ="
+				"0x%08x\n", err);
+
+	return _errno;
+}
+
+static int __cold fm_rtc_set_alarm(struct net_device *net_dev, uint32_t id,
+		uint64_t time)
+{
+	struct dpa_priv_s *priv = netdev_priv(net_dev);
+	struct mac_device *mac_dev = priv->mac_dev;
+	t_FmRtcAlarmParams alarm;
+	int _errno;
+	t_Error	err;
+
+	alarm.alarmId = id;
+	alarm.alarmTime = time;
+	alarm.f_AlarmCallback = NULL;
+	err = FM_RTC_SetAlarm(fm_get_rtc_handle(mac_dev->fm_dev),
+			&alarm);
+	_errno = -GET_ERROR_TYPE(err);
+	if (unlikely(_errno < 0))
+		dpaa_eth_err(mac_dev->dev, "FM_RTC_SetAlarm ="
+				"0x%08x\n", err);
+
+	return _errno;
+}
+
+static int __cold fm_rtc_set_fiper(struct net_device *net_dev, uint32_t id,
+		uint64_t fiper)
+{
+	struct dpa_priv_s *priv = netdev_priv(net_dev);
+	struct mac_device *mac_dev = priv->mac_dev;
+	t_FmRtcPeriodicPulseParams pp;
+	int _errno;
+	t_Error	err;
+
+	pp.periodicPulseId = id;
+	pp.periodicPulsePeriod = fiper;
+	pp.f_PeriodicPulseCallback = NULL;
+	err = FM_RTC_SetPeriodicPulse(fm_get_rtc_handle(mac_dev->fm_dev), &pp);
+	_errno = -GET_ERROR_TYPE(err);
+	if (unlikely(_errno < 0))
+		dpaa_eth_err(mac_dev->dev, "FM_RTC_SetPeriodicPulse ="
+				"0x%08x\n", err);
+
+	return _errno;
+}
+
+
+void fm_mac_dump_regs(struct mac_device *mac_dev)
+{
+	struct mac_priv_s *mac_priv = macdev_priv(mac_dev);
+
+	FM_MAC_DumpRegs(mac_priv->mac);
+}
+
+static void __devinit __cold setup_dtsec(struct mac_device *mac_dev)
+{
+	mac_dev->init_phy	= dtsec_init_phy;
+	mac_dev->init		= init;
+	mac_dev->start		= start;
+	mac_dev->stop		= stop;
+	mac_dev->change_promisc	= change_promisc;
+	mac_dev->change_addr    = change_addr;
+	mac_dev->set_multi      = set_multi;
+	mac_dev->uninit		= uninit;
+	mac_dev->ptp_enable		= ptp_enable;
+	mac_dev->ptp_disable		= ptp_disable;
+	mac_dev->get_mac_handle		= get_mac_handle;
+	mac_dev->fm_rtc_enable		= fm_rtc_enable;
+	mac_dev->fm_rtc_disable		= fm_rtc_disable;
+	mac_dev->fm_rtc_get_cnt		= fm_rtc_get_cnt;
+	mac_dev->fm_rtc_set_cnt		= fm_rtc_set_cnt;
+	mac_dev->fm_rtc_get_drift	= fm_rtc_get_drift;
+	mac_dev->fm_rtc_set_drift	= fm_rtc_set_drift;
+	mac_dev->fm_rtc_set_alarm	= fm_rtc_set_alarm;
+	mac_dev->fm_rtc_set_fiper	= fm_rtc_set_fiper;
+}
+
+static void __devinit __cold setup_xgmac(struct mac_device *mac_dev)
+{
+	mac_dev->init_phy	= xgmac_init_phy;
+	mac_dev->init		= init;
+	mac_dev->start		= start;
+	mac_dev->stop		= stop;
+	mac_dev->change_promisc	= change_promisc;
+	mac_dev->change_addr    = change_addr;
+	mac_dev->set_multi      = set_multi;
+	mac_dev->uninit		= uninit;
+}
+
+static void __devinit __cold setup_memac(struct mac_device *mac_dev)
+{
+	mac_dev->init_phy	= memac_init_phy;
+	mac_dev->init		= memac_init;
+	mac_dev->start		= start;
+	mac_dev->stop		= stop;
+	mac_dev->change_promisc	= change_promisc;
+	mac_dev->change_addr    = change_addr;
+	mac_dev->set_multi      = set_multi;
+	mac_dev->uninit		= uninit;
+	mac_dev->fm_rtc_enable		= fm_rtc_enable;
+	mac_dev->fm_rtc_disable		= fm_rtc_disable;
+	mac_dev->fm_rtc_get_cnt		= fm_rtc_get_cnt;
+	mac_dev->fm_rtc_set_cnt		= fm_rtc_set_cnt;
+	mac_dev->fm_rtc_get_drift	= fm_rtc_get_drift;
+	mac_dev->fm_rtc_set_drift	= fm_rtc_set_drift;
+	mac_dev->fm_rtc_set_alarm	= fm_rtc_set_alarm;
+	mac_dev->fm_rtc_set_fiper	= fm_rtc_set_fiper;
+}
+
+void (*const mac_setup[])(struct mac_device *mac_dev) __devinitconst = {
+	[DTSEC] = setup_dtsec,
+	[XGMAC] = setup_xgmac,
+	[MEMAC] = setup_memac
+};
diff --git a/drivers/net/ethernet/freescale/dpa/mac.c b/drivers/net/ethernet/freescale/dpa/mac.c
new file mode 100644
index 0000000..7594b56
--- /dev/null
+++ b/drivers/net/ethernet/freescale/dpa/mac.c
@@ -0,0 +1,432 @@
+/* Copyright 2008-2012 Freescale Semiconductor, Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *	 notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *	 notice, this list of conditions and the following disclaimer in the
+ *	 documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *	 names of its contributors may be used to endorse or promote products
+ *	 derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/of_platform.h>
+#include <linux/of_net.h>
+#include <linux/device.h>
+#include <linux/phy.h>
+
+#include "dpaa_eth-common.h"
+
+#include "lnxwrp_fm_ext.h"
+
+#include "mac.h"
+
+#define DTSEC_SUPPORTED \
+	(SUPPORTED_10baseT_Half \
+	| SUPPORTED_10baseT_Full \
+	| SUPPORTED_100baseT_Half \
+	| SUPPORTED_100baseT_Full \
+	| SUPPORTED_Autoneg \
+	| SUPPORTED_MII)
+
+static const char phy_str[][11] __devinitconst =
+{
+	[PHY_INTERFACE_MODE_MII]	= "mii",
+	[PHY_INTERFACE_MODE_GMII]	= "gmii",
+	[PHY_INTERFACE_MODE_SGMII]	= "sgmii",
+	[PHY_INTERFACE_MODE_TBI]	= "tbi",
+	[PHY_INTERFACE_MODE_RMII]	= "rmii",
+	[PHY_INTERFACE_MODE_RGMII]	= "rgmii",
+	[PHY_INTERFACE_MODE_RGMII_ID]	= "rgmii-id",
+	[PHY_INTERFACE_MODE_RGMII_RXID]	= "rgmii-rxid",
+	[PHY_INTERFACE_MODE_RGMII_TXID]	= "rgmii-txid",
+	[PHY_INTERFACE_MODE_RTBI]	= "rtbi",
+	[PHY_INTERFACE_MODE_XGMII]	= "xgmii"
+};
+
+static phy_interface_t __devinit __pure __attribute__((nonnull)) str2phy(const char *str)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(phy_str); i++)
+		if (strcmp(str, phy_str[i]) == 0)
+			return (phy_interface_t)i;
+
+	return PHY_INTERFACE_MODE_MII;
+}
+
+static const uint16_t phy2speed[] __devinitconst =
+{
+	[PHY_INTERFACE_MODE_MII]	= SPEED_100,
+	[PHY_INTERFACE_MODE_GMII]	= SPEED_1000,
+	[PHY_INTERFACE_MODE_SGMII]	= SPEED_1000,
+	[PHY_INTERFACE_MODE_TBI]	= SPEED_1000,
+	[PHY_INTERFACE_MODE_RMII]	= SPEED_100,
+	[PHY_INTERFACE_MODE_RGMII]	= SPEED_1000,
+	[PHY_INTERFACE_MODE_RGMII_ID]	= SPEED_1000,
+	[PHY_INTERFACE_MODE_RGMII_RXID]	= SPEED_1000,
+	[PHY_INTERFACE_MODE_RGMII_TXID]	= SPEED_1000,
+	[PHY_INTERFACE_MODE_RTBI]	= SPEED_1000,
+	[PHY_INTERFACE_MODE_XGMII]	= SPEED_10000
+};
+
+static struct mac_device * __devinit __cold
+alloc_macdev(struct device *dev, size_t sizeof_priv, void (*setup)(struct mac_device *mac_dev))
+{
+	struct mac_device	*mac_dev;
+
+	mac_dev = devm_kzalloc(dev, sizeof(*mac_dev) + sizeof_priv, GFP_KERNEL);
+	if (unlikely(mac_dev == NULL))
+		mac_dev = ERR_PTR(-ENOMEM);
+	else {
+		mac_dev->dev = dev;
+		dev_set_drvdata(dev, mac_dev);
+		setup(mac_dev);
+	}
+
+	return mac_dev;
+}
+
+static int __devexit __cold free_macdev(struct mac_device *mac_dev)
+{
+	dev_set_drvdata(mac_dev->dev, NULL);
+
+	return mac_dev->uninit(mac_dev);
+}
+
+static const struct of_device_id mac_match[] __devinitconst = {
+	[DTSEC] = {
+		.compatible	= "fsl,fman-1g-mac"
+	},
+	[XGMAC] = {
+		.compatible	= "fsl,fman-10g-mac"
+	},
+	[MEMAC] = {
+		.compatible	= "fsl,fman-memac"
+	},
+	{}
+};
+MODULE_DEVICE_TABLE(of, mac_match);
+
+static int __devinit __cold mac_probe(struct platform_device *_of_dev)
+{
+	int			 _errno, i, lenp;
+	struct device		*dev;
+	struct device_node	*mac_node, *dev_node;
+	struct mac_device	*mac_dev;
+	struct platform_device	*of_dev;
+	struct resource		 res;
+	const uint8_t		*mac_addr;
+	const char		*char_prop;
+	const phandle		*phandle_prop;
+	const uint32_t		*uint32_prop;
+        const struct of_device_id *match;
+
+	dev = &_of_dev->dev;
+	mac_node = dev->of_node;
+
+        match = of_match_device(mac_match, dev);
+        if (!match)
+                return -EINVAL;
+
+	for (i = 0; i < ARRAY_SIZE(mac_match) - 1 && match != mac_match + i; i++);
+	BUG_ON(i >= ARRAY_SIZE(mac_match) - 1);
+
+	mac_dev = alloc_macdev(dev, mac_sizeof_priv[i], mac_setup[i]);
+	if (IS_ERR(mac_dev)) {
+		_errno = PTR_ERR(mac_dev);
+		dpaa_eth_err(dev, "alloc_macdev() = %d\n", _errno);
+		goto _return;
+	}
+
+	INIT_LIST_HEAD(&mac_dev->mc_addr_list);
+
+	/* Get the FM node */
+	dev_node = of_get_parent(mac_node);
+	if (unlikely(dev_node == NULL)) {
+		dpaa_eth_err(dev, "of_get_parent(%s) failed\n",
+				mac_node->full_name);
+		_errno = -EINVAL;
+		goto _return_dev_set_drvdata;
+	}
+
+	of_dev = of_find_device_by_node(dev_node);
+	if (unlikely(of_dev == NULL)) {
+		dpaa_eth_err(dev, "of_find_device_by_node(%s) failed\n",
+				dev_node->full_name);
+		_errno = -EINVAL;
+		goto _return_of_node_put;
+	}
+
+	mac_dev->fm_dev = fm_bind(&of_dev->dev);
+	if (unlikely(mac_dev->fm_dev == NULL)) {
+		dpaa_eth_err(dev, "fm_bind(%s) failed\n", dev_node->full_name);
+		_errno = -ENODEV;
+		goto _return_of_node_put;
+	}
+
+    mac_dev->fm = (void *)fm_get_handle(mac_dev->fm_dev);
+	of_node_put(dev_node);
+
+	/* Get the address of the memory mapped registers */
+	_errno = of_address_to_resource(mac_node, 0, &res);
+	if (unlikely(_errno < 0)) {
+		dpaa_eth_err(dev, "of_address_to_resource(%s) = %d\n",
+				mac_node->full_name, _errno);
+		goto _return_dev_set_drvdata;
+	}
+
+	mac_dev->res = __devm_request_region(
+		dev,
+		fm_get_mem_region(mac_dev->fm_dev),
+		res.start, res.end + 1 - res.start, "mac");
+	if (unlikely(mac_dev->res == NULL)) {
+		dpaa_eth_err(dev, "__devm_request_mem_region(mac) failed\n");
+		_errno = -EBUSY;
+		goto _return_dev_set_drvdata;
+	}
+
+	mac_dev->vaddr = devm_ioremap(dev, mac_dev->res->start,
+				      mac_dev->res->end + 1 - mac_dev->res->start);
+	if (unlikely(mac_dev->vaddr == NULL)) {
+		dpaa_eth_err(dev, "devm_ioremap() failed\n");
+		_errno = -EIO;
+		goto _return_dev_set_drvdata;
+	}
+
+	/*
+	 * XXX: Warning, future versions of Linux will most likely not even
+	 * call the driver code to allow us to override the TBIPA value,
+	 * we'll need to address this when we move to newer kernel rev
+	 */
+#define TBIPA_OFFSET		0x1c
+#define TBIPA_DEFAULT_ADDR	5
+	mac_dev->tbi_node = of_parse_phandle(mac_node, "tbi-handle", 0);
+	if (mac_dev->tbi_node) {
+		u32 tbiaddr = TBIPA_DEFAULT_ADDR;
+
+		uint32_prop = of_get_property(mac_dev->tbi_node, "reg", NULL);
+		if (uint32_prop)
+			tbiaddr = *uint32_prop;
+		out_be32(mac_dev->vaddr + TBIPA_OFFSET, tbiaddr);
+	}
+
+	if (!of_device_is_available(mac_node)) {
+		devm_iounmap(dev, mac_dev->vaddr);
+		__devm_release_region(dev, fm_get_mem_region(mac_dev->fm_dev),
+			res.start, res.end + 1 - res.start);
+		fm_unbind(mac_dev->fm_dev);
+		devm_kfree(dev, mac_dev);
+		dev_set_drvdata(dev, NULL);
+		return -ENODEV;
+	}
+
+	/* Get the cell-index */
+	uint32_prop = of_get_property(mac_node, "cell-index", &lenp);
+	if (unlikely(uint32_prop == NULL)) {
+		dpaa_eth_err(dev, "of_get_property(%s, cell-index) failed\n",
+				mac_node->full_name);
+		_errno = -EINVAL;
+		goto _return_dev_set_drvdata;
+	}
+	BUG_ON(lenp != sizeof(uint32_t));
+	mac_dev->cell_index = *uint32_prop;
+
+	/* Get the MAC address */
+	mac_addr = of_get_mac_address(mac_node);
+	if (unlikely(mac_addr == NULL)) {
+		dpaa_eth_err(dev, "of_get_mac_address(%s) failed\n",
+				mac_node->full_name);
+		_errno = -EINVAL;
+		goto _return_dev_set_drvdata;
+	}
+	memcpy(mac_dev->addr, mac_addr, sizeof(mac_dev->addr));
+
+	/* Get the port handles */
+	phandle_prop = of_get_property(mac_node, "fsl,port-handles", &lenp);
+	if (unlikely(phandle_prop == NULL)) {
+		dpaa_eth_err(dev, "of_get_property(%s, port-handles) failed\n",
+				mac_node->full_name);
+		_errno = -EINVAL;
+		goto _return_dev_set_drvdata;
+	}
+	BUG_ON(lenp != sizeof(phandle) * ARRAY_SIZE(mac_dev->port_dev));
+
+	for_each_port_device(i, mac_dev->port_dev) {
+		/* Find the port node */
+		dev_node = of_find_node_by_phandle(phandle_prop[i]);
+		if (unlikely(dev_node == NULL)) {
+			dpaa_eth_err(dev, "of_find_node_by_phandle() failed\n");
+			_errno = -EINVAL;
+			goto _return_of_node_put;
+		}
+
+		of_dev = of_find_device_by_node(dev_node);
+		if (unlikely(of_dev == NULL)) {
+			dpaa_eth_err(dev, "of_find_device_by_node(%s) failed\n",
+					dev_node->full_name);
+			_errno = -EINVAL;
+			goto _return_of_node_put;
+		}
+
+		mac_dev->port_dev[i] = fm_port_bind(&of_dev->dev);
+		if (unlikely(mac_dev->port_dev[i] == NULL)) {
+			dpaa_eth_err(dev, "dev_get_drvdata(%s) failed\n",
+					dev_node->full_name);
+			_errno = -EINVAL;
+			goto _return_of_node_put;
+		}
+		of_node_put(dev_node);
+	}
+
+	/* Get the PHY connection type */
+	char_prop = (const char *)of_get_property(mac_node,
+						"phy-connection-type", NULL);
+	if (unlikely(char_prop == NULL)) {
+		dpaa_eth_warning(dev,
+				"of_get_property(%s, phy-connection-type) "
+				"failed. Defaulting to MII\n",
+				mac_node->full_name);
+		mac_dev->phy_if = PHY_INTERFACE_MODE_MII;
+	} else
+		mac_dev->phy_if = str2phy(char_prop);
+
+	mac_dev->link		= false;
+	mac_dev->half_duplex	= false;
+	mac_dev->speed		= phy2speed[mac_dev->phy_if];
+	mac_dev->max_speed	= mac_dev->speed;
+	mac_dev->if_support = DTSEC_SUPPORTED;
+	/* We don't support half-duplex in SGMII mode */
+	if (strstr(char_prop, "sgmii"))
+		mac_dev->if_support &= ~(SUPPORTED_10baseT_Half |
+					SUPPORTED_100baseT_Half);
+
+	/* Gigabit support (no half-duplex) */
+	if (mac_dev->max_speed == 1000)
+		mac_dev->if_support |= SUPPORTED_1000baseT_Full;
+
+	/* The 10G interface only supports one mode */
+	if (strstr(char_prop, "xgmii"))
+		mac_dev->if_support = SUPPORTED_10000baseT_Full;
+
+	/* Get the rest of the PHY information */
+	mac_dev->phy_node = of_parse_phandle(mac_node, "phy-handle", 0);
+	if (mac_dev->phy_node == NULL) {
+		int sz;
+		const u32 *phy_id = of_get_property(mac_node, "fixed-link",
+							&sz);
+		if (!phy_id || sz < sizeof(*phy_id)) {
+			cpu_dev_err(dev, "No PHY (or fixed link) found\n");
+			_errno = -EINVAL;
+			goto _return_dev_set_drvdata;
+		}
+
+		sprintf(mac_dev->fixed_bus_id, PHY_ID_FMT, "0", phy_id[0]);
+	}
+
+	_errno = mac_dev->init(mac_dev);
+	if (unlikely(_errno < 0)) {
+		dpaa_eth_err(dev, "mac_dev->init() = %d\n", _errno);
+		goto _return_dev_set_drvdata;
+	}
+
+	cpu_dev_info(dev,
+		"FMan MAC address: %02hx:%02hx:%02hx:%02hx:%02hx:%02hx\n",
+		     mac_dev->addr[0], mac_dev->addr[1], mac_dev->addr[2],
+		     mac_dev->addr[3], mac_dev->addr[4], mac_dev->addr[5]);
+
+	goto _return;
+
+_return_of_node_put:
+	of_node_put(dev_node);
+_return_dev_set_drvdata:
+	dev_set_drvdata(dev, NULL);
+_return:
+	return _errno;
+}
+
+static int __devexit __cold mac_remove(struct platform_device *of_dev)
+{
+	int			 i, _errno;
+	struct device		*dev;
+	struct mac_device	*mac_dev;
+
+	dev = &of_dev->dev;
+	mac_dev = (struct mac_device *)dev_get_drvdata(dev);
+
+	for_each_port_device(i, mac_dev->port_dev)
+		fm_port_unbind(mac_dev->port_dev[i]);
+
+	fm_unbind(mac_dev->fm_dev);
+
+	_errno = free_macdev(mac_dev);
+
+	return _errno;
+}
+
+static struct platform_driver mac_driver = {
+	.driver = {
+		.name		= KBUILD_MODNAME,
+		.of_match_table	= mac_match,
+		.owner		= THIS_MODULE,
+	},
+	.probe		= mac_probe,
+	.remove		= __devexit_p(mac_remove)
+};
+
+static int __init __cold mac_load(void)
+{
+	int	 _errno;
+
+	cpu_pr_debug(KBUILD_MODNAME ": -> %s:%s()\n", __file__, __func__);
+
+	cpu_pr_info(KBUILD_MODNAME ": %s (" VERSION ")\n",
+		mac_driver_description);
+
+	_errno = platform_driver_register(&mac_driver);
+	if (unlikely(_errno < 0)) {
+		cpu_pr_err(KBUILD_MODNAME ": %s:%hu:%s(): " \
+			"platform_driver_register() = %d\n",
+			   __file__, __LINE__, __func__, _errno);
+		goto _return;
+	}
+
+	goto _return;
+
+_return:
+	cpu_pr_debug(KBUILD_MODNAME ": %s:%s() ->\n", __file__, __func__);
+
+	return _errno;
+}
+module_init(mac_load);
+
+static void __exit __cold mac_unload(void)
+{
+	cpu_pr_debug(KBUILD_MODNAME ": -> %s:%s()\n", __file__, __func__);
+
+	platform_driver_unregister(&mac_driver);
+
+	cpu_pr_debug(KBUILD_MODNAME ": %s:%s() ->\n", __file__, __func__);
+}
+module_exit(mac_unload);
diff --git a/drivers/net/ethernet/freescale/dpa/offline_port.c b/drivers/net/ethernet/freescale/dpa/offline_port.c
new file mode 100644
index 0000000..c11dc3e
--- /dev/null
+++ b/drivers/net/ethernet/freescale/dpa/offline_port.c
@@ -0,0 +1,342 @@
+/*
+ * Copyright 2011-2012 Freescale Semiconductor Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *	 notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *	 notice, this list of conditions and the following disclaimer in the
+ *	 documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *	 names of its contributors may be used to endorse or promote products
+ *	 derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+ * Offline Parsing / Host Command port driver for FSL QorIQ FMan.
+ * Validates device-tree configuration and sets up the offline ports.
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/of_platform.h>
+
+#include "offline_port.h"
+#include "dpaa_eth-common.h"
+
+#define OH_MOD_DESCRIPTION	"FSL FMan Offline Parsing port driver"
+
+MODULE_LICENSE("Dual BSD/GPL");
+MODULE_AUTHOR("Bogdan Hamciuc <bogdan.hamciuc@freescale.com>");
+MODULE_DESCRIPTION(OH_MOD_DESCRIPTION);
+
+
+static const struct of_device_id oh_port_match_table[] __devinitconst = {
+	{
+		.compatible	= "fsl,dpa-oh"
+	},
+	{
+		.compatible	= "fsl,dpa-oh-shared"
+	},
+	{}
+};
+MODULE_DEVICE_TABLE(of, oh_port_match_table);
+
+static int oh_port_remove(struct platform_device *_of_dev);
+static int oh_port_probe(struct platform_device *_of_dev);
+
+static struct platform_driver oh_port_driver = {
+	.driver = {
+		.name		= KBUILD_MODNAME,
+		.of_match_table	= oh_port_match_table,
+		.owner		= THIS_MODULE,
+	},
+	.probe		= oh_port_probe,
+	.remove		= __devexit_p(oh_port_remove)
+};
+
+/* Allocation code for the OH port's PCD frame queues */
+static int __devinit __cold oh_alloc_pcd_fqids(struct device *dev,
+	uint32_t num,
+	uint8_t alignment,
+	uint32_t *base_fqid)
+{
+	cpu_dev_crit(dev, "callback not implemented!\n");
+	BUG();
+
+	return 0;
+}
+
+static int __devinit __cold oh_free_pcd_fqids(struct device *dev, uint32_t base_fqid)
+{
+	dpaa_eth_crit(dev, "callback not implemented!\n");
+	BUG();
+
+	return 0;
+}
+
+static int __devinit
+oh_port_probe(struct platform_device *_of_dev)
+{
+	struct device		*dpa_oh_dev;
+	struct device_node	*dpa_oh_node;
+	int			 lenp, _errno = 0, fq_idx;
+	const phandle		*oh_port_handle;
+	struct platform_device	*oh_of_dev;
+	struct device_node	*oh_node;
+	struct device		*oh_dev;
+	struct dpa_oh_config_s	*oh_config;
+	uint32_t		*oh_all_queues;
+	uint32_t		 queues_count;
+	uint32_t		 crt_fqid_base;
+	uint32_t		 crt_fq_count;
+	struct fm_port_params	 oh_port_tx_params;
+	struct fm_port_pcd_param	oh_port_pcd_params;
+	/* True if the current partition owns the OH port. */
+	bool init_oh_port;
+	const struct of_device_id *match;
+
+	dpa_oh_dev = &_of_dev->dev;
+	dpa_oh_node = dpa_oh_dev->of_node;
+	BUG_ON(dpa_oh_node == NULL);
+
+	match = of_match_device(oh_port_match_table, dpa_oh_dev);
+	if (!match)
+		return -EINVAL;
+
+	cpu_dev_dbg(dpa_oh_dev, "Probing OH port...\n");
+
+	/*
+	 * Find the referenced OH node
+	 */
+
+	oh_port_handle = of_get_property(dpa_oh_node,
+		"fsl,fman-oh-port", &lenp);
+	if (oh_port_handle == NULL) {
+		cpu_dev_err(dpa_oh_dev, "No OH port handle found in node %s\n",
+			dpa_oh_node->full_name);
+		return -EINVAL;
+	}
+
+	BUG_ON(lenp % sizeof(*oh_port_handle));
+	if (lenp != sizeof(*oh_port_handle)) {
+		cpu_dev_err(dpa_oh_dev, "Found %lu OH port bindings in node %s,"
+			" only 1 phandle is allowed.\n",
+			(unsigned long int)(lenp / sizeof(*oh_port_handle)),
+			dpa_oh_node->full_name);
+		return -EINVAL;
+	}
+
+	/* Read configuration for the OH port */
+	oh_node = of_find_node_by_phandle(*oh_port_handle);
+	if (oh_node == NULL) {
+		cpu_dev_err(dpa_oh_dev, "Can't find OH node referenced from "
+			"node %s\n", dpa_oh_node->full_name);
+		return -EINVAL;
+	}
+	cpu_dev_info(dpa_oh_dev, "Found OH node handle compatible with %s.\n",
+		match->compatible);
+
+	oh_of_dev = of_find_device_by_node(oh_node);
+	BUG_ON(oh_of_dev == NULL);
+	oh_dev = &oh_of_dev->dev;
+	of_node_put(oh_node);
+
+	/*
+	 * The OH port must be initialized exactly once.
+	 * The following scenarios are of interest:
+	 *	- the node is Linux-private (will always initialize it);
+	 *	- the node is shared between two Linux partitions
+	 *	  (only one of them will initialize it);
+	 *	- the node is shared between a Linux and a LWE partition
+	 *	  (Linux will initialize it) - "fsl,dpa-oh-shared"
+	 */
+
+	/* Check if the current partition owns the OH port
+	 * and ought to initialize it. It may be the case that we leave this
+	 * to another (also Linux) partition. */
+	init_oh_port = strcmp(match->compatible, "fsl,dpa-oh-shared");
+
+	/* If we aren't the "owner" of the OH node, we're done here. */
+	if (!init_oh_port) {
+		cpu_dev_dbg(dpa_oh_dev, "Not owning the shared OH port %s, "
+			"will not initialize it.\n", oh_node->full_name);
+		return 0;
+	}
+
+	/* Allocate OH dev private data */
+	oh_config = devm_kzalloc(dpa_oh_dev, sizeof(*oh_config), GFP_KERNEL);
+	if (oh_config == NULL) {
+		cpu_dev_err(dpa_oh_dev, "Can't allocate private data for "
+			"OH node %s referenced from node %s!\n",
+			oh_node->full_name, dpa_oh_node->full_name);
+		return -ENOMEM;
+	}
+
+	/*
+	 * Read FQ ids/nums for the DPA OH node
+	 */
+	oh_all_queues = (uint32_t *)of_get_property(dpa_oh_node,
+		"fsl,qman-frame-queues-oh", &lenp);
+	if (oh_all_queues == NULL) {
+		cpu_dev_err(dpa_oh_dev, "No frame queues have been "
+			"defined for OH node %s referenced from node %s\n",
+			oh_node->full_name, dpa_oh_node->full_name);
+		_errno = -EINVAL;
+		goto return_kfree;
+	}
+
+	/* Check that the OH error and default FQs are there */
+	BUG_ON(lenp % (2 * sizeof(*oh_all_queues)));
+	queues_count = lenp / (2 * sizeof(*oh_all_queues));
+	if (queues_count != 2) {
+		dpaa_eth_err(dpa_oh_dev, "Error and Default queues must be "
+			"defined for OH node %s referenced from node %s\n",
+			oh_node->full_name, dpa_oh_node->full_name);
+		_errno = -EINVAL;
+		goto return_kfree;
+	}
+
+	/* Read the FQIDs defined for this OH port */
+	cpu_dev_dbg(dpa_oh_dev, "Reading %d queues...\n", queues_count);
+	fq_idx = 0;
+
+	/* Error FQID - must be present */
+	crt_fqid_base = oh_all_queues[fq_idx++];
+	crt_fq_count = oh_all_queues[fq_idx++];
+	if (crt_fq_count != 1) {
+		cpu_dev_err(dpa_oh_dev, "Only 1 Error FQ allowed in OH node %s "
+			"referenced from node %s (read: %d FQIDs).\n",
+			oh_node->full_name, dpa_oh_node->full_name,
+			crt_fq_count);
+		_errno = -EINVAL;
+		goto return_kfree;
+	}
+	oh_config->error_fqid = crt_fqid_base;
+	cpu_dev_dbg(dpa_oh_dev, "Read Error FQID 0x%x for OH port %s.\n",
+		oh_config->error_fqid, oh_node->full_name);
+
+	/* Default FQID - must be present */
+	crt_fqid_base = oh_all_queues[fq_idx++];
+	crt_fq_count = oh_all_queues[fq_idx++];
+	if (crt_fq_count != 1) {
+		cpu_dev_err(dpa_oh_dev, "Only 1 Default FQ allowed "
+			"in OH node %s referenced from %s (read: %d FQIDs).\n",
+			oh_node->full_name, dpa_oh_node->full_name,
+			crt_fq_count);
+		_errno = -EINVAL;
+		goto return_kfree;
+	}
+	oh_config->default_fqid = crt_fqid_base;
+	cpu_dev_dbg(dpa_oh_dev, "Read Default FQID 0x%x for OH port %s.\n",
+		oh_config->default_fqid, oh_node->full_name);
+
+	/* Get a handle to the fm_port so we can set
+	 * its configuration params */
+	oh_config->oh_port = fm_port_bind(oh_dev);
+	if (oh_config->oh_port == NULL) {
+		cpu_dev_err(dpa_oh_dev, "NULL drvdata from fm port dev %s!\n",
+			oh_node->full_name);
+		_errno = -EINVAL;
+		goto return_kfree;
+	}
+
+	/* Set Tx params */
+	dpaa_eth_init_port(tx, oh_config->oh_port, oh_port_tx_params,
+		oh_config->error_fqid, oh_config->default_fqid,
+		DPA_TX_PRIV_DATA_SIZE, FALSE);
+	/* Set PCD params */
+	oh_port_pcd_params.cba = oh_alloc_pcd_fqids;
+	oh_port_pcd_params.cbf = oh_free_pcd_fqids;
+	oh_port_pcd_params.dev = dpa_oh_dev;
+	fm_port_pcd_bind(oh_config->oh_port, &oh_port_pcd_params);
+
+	dev_set_drvdata(dpa_oh_dev, oh_config);
+
+	/* Enable the OH port */
+	fm_port_enable(oh_config->oh_port);
+	cpu_dev_info(dpa_oh_dev, "OH port %s enabled.\n", oh_node->full_name);
+
+	return 0;
+
+return_kfree:
+	devm_kfree(dpa_oh_dev, oh_config);
+	return _errno;
+}
+
+static int __devexit __cold oh_port_remove(struct platform_device *_of_dev)
+{
+	int _errno = 0;
+	struct dpa_oh_config_s *oh_config;
+
+	cpu_pr_info("Removing OH port...\n");
+
+	oh_config = dev_get_drvdata(&_of_dev->dev);
+	if (oh_config == NULL) {
+		cpu_pr_err(KBUILD_MODNAME
+			": %s:%hu:%s(): No OH config in device private data!\n",
+			__file__, __LINE__, __func__);
+		_errno = -ENODEV;
+		goto return_error;
+	}
+	if (oh_config->oh_port == NULL) {
+		cpu_pr_err(KBUILD_MODNAME
+			": %s:%hu:%s(): No fm port in device private data!\n",
+			__file__, __LINE__, __func__);
+		_errno = -EINVAL;
+		goto return_error;
+	}
+
+	fm_port_disable(oh_config->oh_port);
+	devm_kfree(&_of_dev->dev, oh_config);
+	dev_set_drvdata(&_of_dev->dev, NULL);
+
+return_error:
+	return _errno;
+}
+
+static int __init __cold oh_port_load(void)
+{
+	int _errno;
+
+	cpu_pr_info(KBUILD_MODNAME ": " OH_MOD_DESCRIPTION " (" VERSION ")\n");
+
+	_errno = platform_driver_register(&oh_port_driver);
+	if (_errno < 0) {
+		cpu_pr_err(KBUILD_MODNAME
+			": %s:%hu:%s(): platform_driver_register() = %d\n",
+			__file__, __LINE__, __func__, _errno);
+	}
+
+	cpu_pr_debug(KBUILD_MODNAME ": %s:%s() ->\n", __file__, __func__);
+	return _errno;
+}
+module_init(oh_port_load);
+
+static void __exit __cold oh_port_unload(void)
+{
+	cpu_pr_debug(KBUILD_MODNAME ": -> %s:%s()\n", __file__, __func__);
+
+	platform_driver_unregister(&oh_port_driver);
+
+	cpu_pr_debug(KBUILD_MODNAME ": %s:%s() ->\n", __file__, __func__);
+}
+module_exit(oh_port_unload);
diff --git a/drivers/net/ethernet/freescale/dpa/offline_port.h b/drivers/net/ethernet/freescale/dpa/offline_port.h
new file mode 100644
index 0000000..1b1a63f
--- /dev/null
+++ b/drivers/net/ethernet/freescale/dpa/offline_port.h
@@ -0,0 +1,45 @@
+/*
+ * Copyright 2011 Freescale Semiconductor Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *	 notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *	 notice, this list of conditions and the following disclaimer in the
+ *	 documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *	 names of its contributors may be used to endorse or promote products
+ *	 derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef __OFFLINE_PORT_H
+#define __OFFLINE_PORT_H
+
+#include "fsl_fman.h"
+
+/* OH port configuration */
+struct dpa_oh_config_s {
+	uint32_t		error_fqid;
+	uint32_t		default_fqid;
+	struct fm_port		*oh_port;
+};
+
+#endif /* __OFFLINE_PORT_H */
-- 
1.8.4.93.g57e4c17

