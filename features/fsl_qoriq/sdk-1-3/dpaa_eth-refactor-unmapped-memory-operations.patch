From ba359ad26c25e098f2b84a74b20c32becf24a563 Mon Sep 17 00:00:00 2001
From: Madalin Bucur <madalin.bucur@freescale.com>
Date: Wed, 8 Aug 2012 15:43:51 +0000
Subject: [PATCH 033/162] dpaa_eth: refactor unmapped memory operations

Code copying to and from unmapped memory is refactored by adding
functions to allow code reuse.

Signed-off-by: Madalin Bucur <madalin.bucur@freescale.com>
[Kevin: Original patch taken from FSL
QorIQ-SDK-V1.3-SOURCE-20121114-yocto.iso tarball.]
Signed-off-by: Kevin Hao <kexin.hao@windriver.com>
---
 drivers/net/ethernet/freescale/dpa/dpaa_eth.c |  152 ++++++++++++-------------
 1 file changed, 75 insertions(+), 77 deletions(-)

diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth.c b/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
index ef099f6..05e2f37 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
@@ -177,6 +177,64 @@ static void dpa_bp_depletion(struct bman_portal	*portal,
 		pr_err("Invalid Pool depleted notification!\n");
 }
 
+/*
+ * Copy from a memory region that requires kmapping to a linear buffer,
+ * taking into account page boundaries in the source
+ */
+static void
+copy_from_unmapped_area(void *dest, dma_addr_t phys_start, size_t buf_size)
+{
+	struct page *page;
+	size_t size, offset;
+	void *page_vaddr;
+
+	while (buf_size > 0) {
+		offset = offset_in_page(phys_start);
+		size = (offset + buf_size > PAGE_SIZE) ?
+			PAGE_SIZE - offset : buf_size;
+
+		page = pfn_to_page(phys_start >> PAGE_SHIFT);
+		page_vaddr = kmap_atomic(page);
+
+		memcpy(dest, page_vaddr + offset, size);
+
+		kunmap_atomic(page_vaddr);
+
+		phys_start += size;
+		dest += size;
+		buf_size -= size;
+	}
+}
+
+/*
+ * Copy to a memory region that requires kmapping from a linear buffer,
+ * taking into account page boundaries in the destination
+ */
+static void
+copy_to_unmapped_area(dma_addr_t phys_start, void *src, size_t buf_size)
+{
+	struct page *page;
+	size_t size, offset;
+	void *page_vaddr;
+
+	while (buf_size > 0) {
+		offset = offset_in_page(phys_start);
+		size = (offset + buf_size > PAGE_SIZE) ?
+				PAGE_SIZE - offset : buf_size;
+
+		page = pfn_to_page(phys_start >> PAGE_SHIFT);
+		page_vaddr = kmap_atomic(page);
+
+		memcpy(page_vaddr + offset, src, size);
+
+		kunmap_atomic(page_vaddr);
+
+		phys_start += size;
+		src += size;
+		buf_size -= size;
+	}
+}
+
 #ifndef CONFIG_DPAA_ETH_SG_SUPPORT
 static void dpa_bp_add_8(struct dpa_bp *dpa_bp)
 {
@@ -1247,10 +1305,7 @@ static int __hot dpa_shared_tx(struct sk_buff *skb, struct net_device *net_dev)
 	struct qm_fd fd;
 	int queue_mapping;
 	int err;
-	void *dpa_bp_vaddr, *page_vaddr, *virt_start;
-	dma_addr_t phys_start;
-	struct page *page;
-	size_t remaining, size, offset;
+	void *dpa_bp_vaddr;
 	t_FmPrsResult parse_results;
 
 	priv = netdev_priv(net_dev);
@@ -1299,55 +1354,19 @@ static int __hot dpa_shared_tx(struct sk_buff *skb, struct net_device *net_dev)
 		/* Enable L3/L4 hardware checksum computation, if applicable */
 		err = dpa_enable_tx_csum(priv, skb, &fd,
 					 dpa_bp_vaddr + DPA_TX_PRIV_DATA_SIZE);
+	} else {
+		err = dpa_enable_tx_csum(priv, skb, &fd,
+					 (char *)&parse_results);
 
-		goto static_map;
-	}
-
-	err = dpa_enable_tx_csum(priv, skb, &fd, (char *)&parse_results);
-
-	phys_start = bm_buf_addr(&bmb) + DPA_TX_PRIV_DATA_SIZE;
-	virt_start = &parse_results;
-	remaining = DPA_PARSE_RESULTS_SIZE;
-
-	while (remaining > 0) {
-		offset = offset_in_page(phys_start);
-		size = (offset + remaining > PAGE_SIZE) ?
-				PAGE_SIZE - offset : remaining;
-
-		page = pfn_to_page(phys_start >> PAGE_SHIFT);
-		dpa_bp_vaddr = kmap_atomic(page);
-
-		memcpy(dpa_bp_vaddr + offset, virt_start, size);
-
-		kunmap_atomic(dpa_bp_vaddr);
-
-		phys_start += size;
-		virt_start += size;
-		remaining -= size;
-	}
-
-	phys_start = bm_buf_addr(&bmb) + dpa_fd_offset(&fd);
-	virt_start = skb->data;
-	remaining = dpa_fd_length(&fd);
-
-	while (remaining > 0) {
-		offset = offset_in_page(phys_start);
-		size = (offset + remaining > PAGE_SIZE) ?
-			PAGE_SIZE - offset : remaining;
-
-		page = pfn_to_page(phys_start >> PAGE_SHIFT);
-		page_vaddr = kmap_atomic(page);
-
-		memcpy(page_vaddr + offset, virt_start, size);
-
-		kunmap_atomic(page_vaddr);
+		copy_to_unmapped_area(bm_buf_addr(&bmb) + DPA_TX_PRIV_DATA_SIZE,
+				&parse_results,
+				DPA_PARSE_RESULTS_SIZE);
 
-		phys_start += size;
-		virt_start += size;
-		remaining -= size;
+		copy_to_unmapped_area(bm_buf_addr(&bmb) + dpa_fd_offset(&fd),
+				skb->data,
+				dpa_fd_length(&fd));
 	}
 
-static_map:
 	if (unlikely(err < 0)) {
 		if (netif_msg_tx_err(priv) && net_ratelimit())
 			cpu_netdev_err(net_dev, "Tx HW csum error: %d\n", err);
@@ -1695,11 +1714,7 @@ shared_rx_dqrr(struct qman_portal *portal, struct qman_fq *fq,
 	struct dpa_percpu_priv_s	*percpu_priv;
 	const struct qm_fd *fd = &dq->fd;
 	struct dpa_bp *dpa_bp;
-	size_t size, remaining, offset;
 	struct sk_buff *skb;
-	void *dpa_bp_vaddr;
-	dma_addr_t phys_start;
-	struct page *page;
 
 	net_dev = ((struct dpa_fq *)fq)->net_dev;
 	priv = netdev_priv(net_dev);
@@ -1728,9 +1743,9 @@ shared_rx_dqrr(struct qman_portal *portal, struct qman_fq *fq,
 		goto out;
 	}
 
-	size = dpa_fd_length(fd);
-
-	skb = __netdev_alloc_skb(net_dev, DPA_BP_HEAD + size, GFP_ATOMIC);
+	skb = __netdev_alloc_skb(net_dev,
+				 DPA_BP_HEAD + dpa_fd_length(fd),
+				 GFP_ATOMIC);
 	if (unlikely(skb == NULL)) {
 		if (netif_msg_rx_err(priv) && net_ratelimit())
 			cpu_netdev_err(net_dev, "Could not alloc skb\n");
@@ -1747,29 +1762,12 @@ shared_rx_dqrr(struct qman_portal *portal, struct qman_fq *fq,
 		memcpy(skb_put(skb, dpa_fd_length(fd)),
 		       dpa_phys2virt(dpa_bp, qm_fd_addr(fd)) +
 		       dpa_fd_offset(fd), dpa_fd_length(fd));
-		goto static_map;
-	}
-
-	phys_start = qm_fd_addr(fd) + dpa_fd_offset(fd);
-	remaining = dpa_fd_length(fd);
-
-	while (remaining > 0) {
-		offset = offset_in_page(phys_start);
-		size = (offset + remaining > PAGE_SIZE) ?
-			PAGE_SIZE - offset : remaining;
-
-		page = pfn_to_page(phys_start >> PAGE_SHIFT);
-		dpa_bp_vaddr = kmap_atomic(page);
-
-		memcpy(skb_put(skb, size), dpa_bp_vaddr + offset, size);
-
-		kunmap_atomic(dpa_bp_vaddr);
-
-		phys_start += size;
-		remaining -= size;
+	} else {
+		copy_from_unmapped_area(skb_put(skb, dpa_fd_length(fd)),
+					qm_fd_addr(fd) + dpa_fd_offset(fd),
+					dpa_fd_length(fd));
 	}
 
-static_map:
 	skb->protocol = eth_type_trans(skb, net_dev);
 
 	if (unlikely(dpa_check_rx_mtu(skb, net_dev->mtu))) {
-- 
1.7.9.7

