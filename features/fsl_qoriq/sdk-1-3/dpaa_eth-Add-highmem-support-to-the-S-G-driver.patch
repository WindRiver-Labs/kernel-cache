From be1933d4a728dc0f41db8e0b603b4ef01ef6f57b Mon Sep 17 00:00:00 2001
From: Bogdan Hamciuc <bogdan.hamciuc@freescale.com>
Date: Thu, 12 Jul 2012 17:56:16 +0300
Subject: [PATCH 036/162] dpaa_eth: Add highmem support to the S/G driver

Map/unmap egress pages potentially allocated from highmem. This is
especially the case of outgoing skbuff frags sent from userspace via the
sendfile() syscall.

Signed-off-by: Bogdan Hamciuc <bogdan.hamciuc@freescale.com>
[Kevin: Original patch taken from FSL
QorIQ-SDK-V1.3-SOURCE-20121114-yocto.iso tarball.]
Signed-off-by: Kevin Hao <kexin.hao@windriver.com>
---
 drivers/net/ethernet/freescale/dpa/dpaa_eth.c    |    9 ++--
 drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c |   53 ++++++++++++++--------
 2 files changed, 40 insertions(+), 22 deletions(-)

diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth.c b/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
index 8e492ab..7ab9609 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
@@ -3159,10 +3159,13 @@ static int dpa_netdev_init(struct device_node *dpa_node,
 		net_dev->features |= (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM);
 		net_dev->vlan_features |= (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM);
 #ifdef CONFIG_DPAA_ETH_SG_SUPPORT
-		/* Advertise S/G support for MAC-ful, private interfaces */
+		/*
+		 * Advertise S/G and HIGHDMA support for MAC-ful,
+		 * private interfaces
+		 */
 		if (!priv->shared) {
-			net_dev->features |= NETIF_F_SG;
-			net_dev->vlan_features |= NETIF_F_SG;
+			net_dev->features |= NETIF_F_SG | NETIF_F_HIGHDMA;
+			net_dev->vlan_features |= NETIF_F_SG | NETIF_F_HIGHDMA;
 		}
 #endif
 	}
diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c b/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c
index 3cbc544..2ccfd23 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c
@@ -32,6 +32,7 @@
 
 #include <linux/init.h>
 #include <linux/skbuff.h>
+#include <linux/highmem.h>
 #include <linux/fsl_bman.h>
 
 #include "dpaa_eth.h"
@@ -214,7 +215,7 @@ void dpa_make_private_pool(struct dpa_bp *dpa_bp)
  * gets freed here.
  */
 struct sk_buff *_dpa_cleanup_tx_fd(const struct dpa_priv_s *priv,
-					  const struct qm_fd *fd)
+	const struct qm_fd *fd)
 {
 	const struct qm_sg_entry *sgt;
 	int i;
@@ -232,17 +233,22 @@ struct sk_buff *_dpa_cleanup_tx_fd(const struct dpa_priv_s *priv,
 	skb = *skbh;
 
 	if (fd->format == qm_fd_sg) {
-		/* all storage items used are pages */
+		/*
+		 * All storage items used are pages, but only the sgt and
+		 * the first page are guaranteed to reside in lowmem.
+		 */
 		sgt = phys_to_virt(addr + dpa_fd_offset(fd));
 
-		for (i = 0; i < DPA_SGT_MAX_ENTRIES; i++) {
-			BUG_ON(sgt[i].extension);
+		/* page 0 is from lowmem, was dma_map_single()-ed */
+		dma_unmap_single(dpa_bp->dev, sgt[0].addr,
+				 dpa_bp->size, dma_dir);
 
-			dma_unmap_single(dpa_bp->dev, sgt[i].addr,
-					 dpa_bp->size, dma_dir);
+		/* remaining pages were mapped with dma_map_page() */
+		for (i = 1; i < skb_shinfo(skb)->nr_frags; i++) {
+			BUG_ON(sgt[i].extension);
 
-			if (sgt[i].final)
-				break;
+			dma_unmap_page(dpa_bp->dev, sgt[i].addr,
+					dpa_bp->size, dma_dir);
 		}
 
 		/*
@@ -594,11 +600,15 @@ static int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
 		 * We want each fragment to have at least dpa_bp->size bytes.
 		 * If even one fragment is smaller, the entire FD becomes
 		 * unrecycleable.
+		 * Same holds if the fragments are allocated from highmem.
 		 */
-		for (i = 0; i < nr_frags; i++)
-			if (skb_shinfo(skb)->frags[i].size < dpa_bp->size) {
+		for (i = 0; i < nr_frags; i++) {
+			skb_frag_t *crt_frag = &skb_shinfo(skb)->frags[i];
+			if ((crt_frag->size < dpa_bp->size) ||
+			    PageHighMem(crt_frag->page)) {
 				can_recycle = false;
 				break;
+			}
 		}
 	}
 	dma_dir = can_recycle ? DMA_BIDIRECTIONAL : DMA_TO_DEVICE;
@@ -622,8 +632,13 @@ static int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
 	sgt[0].offset = 0;
 	sgt[0].length = skb_headlen(skb);
 
-	if (sgt[0].offset + skb_headlen(skb) > dpa_bp->size) {
-		dpaa_eth_err(dpa_bp->dev, "linear buff is too big\n");
+	/*
+	 * FIXME need more than one page if the linear part of the skb
+	 * is longer than PAGE_SIZE
+	 */
+	if (unlikely(sgt[0].offset + skb_headlen(skb) > dpa_bp->size)) {
+		pr_warn_once("tx headlen %d larger than available buffs %d\n",
+			skb_headlen(skb), dpa_bp->size);
 		err = -EINVAL;
 		goto skb_linear_too_large;
 	}
@@ -650,14 +665,14 @@ static int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
 		/* This shouldn't happen */
 		BUG_ON(!frag->page);
 
-		buffer_start = page_address(frag->page) + frag->page_offset;
-		addr = dma_map_single(dpa_bp->dev, buffer_start, dpa_bp->size,
-				      dma_dir);
+		addr = dma_map_page(dpa_bp->dev, frag->page, frag->page_offset,
+			dpa_bp->size, dma_dir);
 		if (unlikely(dma_mapping_error(dpa_bp->dev, addr))) {
 			dpaa_eth_err(dpa_bp->dev, "DMA mapping failed");
 			err = -EINVAL;
 			goto sg_map_failed;
 		}
+
 		/* keep the offset in the address */
 		sgt[i].addr_hi = upper_32_bits(addr);
 		sgt[i].addr_lo = lower_32_bits(addr);
@@ -667,11 +682,11 @@ static int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
 	fd->length20 = skb->len;
 	fd->offset = DPA_BP_HEAD;
 
+	/* DMA map the SGT page */
 	buffer_start = (void *)sgt - dpa_fd_offset(fd);
 	skbh = (struct sk_buff **)buffer_start;
 	*skbh = skb;
 
-	/* DMA map the SGT page */
 	addr = dma_map_single(dpa_bp->dev, buffer_start, dpa_bp->size, dma_dir);
 	if (unlikely(dma_mapping_error(dpa_bp->dev, addr))) {
 		dpaa_eth_err(dpa_bp->dev, "DMA mapping failed");
@@ -692,11 +707,11 @@ static int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
 sgt_map_failed:
 sg_map_failed:
 	for (j = 0; j < i; j++)
-		dma_unmap_single(dpa_bp->dev, qm_sg_addr(&sgt[j]), dpa_bp->size,
-				 dma_dir);
+		dma_unmap_page(dpa_bp->dev, qm_sg_addr(&sgt[j]),
+			dpa_bp->size, dma_dir);
 sg0_map_failed:
-skb_linear_too_large:
 	free_page(sg0_page);
+skb_linear_too_large:
 sg0_page_alloc_failed:
 csum_failed:
 	free_page(sgt_page);
-- 
1.7.9.7

