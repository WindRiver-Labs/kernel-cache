From b155f300d42ffd99ca17c697de6bf403920c8c96 Mon Sep 17 00:00:00 2001
From: Bogdan Hamciuc <bogdan.hamciuc@freescale.com>
Date: Fri, 13 Jul 2012 12:04:01 +0300
Subject: [PATCH 037/162] dpaa_eth: Add CGR-based taildrop and congestion
 notification

Replace the FQTD mechanism with a more robust CGR-based congestion
detection. This allows us to notify the stack when the device is
congested, preventing it from pushing frames in a tight loop that would
waste CPU resources.
The actual congestion status-based taildrop mechanism, although enabled,
should be little-to-not used at runtime, if the kernel's egress queues
are timely stopped.

This change renounces the NETIF_F_HW_QDISC feature in the SG
driver. It remains still usable in the non-SG driver.

Signed-off-by: Bogdan Hamciuc <bogdan.hamciuc@freescale.com>
[Kevin: Original patch taken from FSL
QorIQ-SDK-V1.3-SOURCE-20121114-yocto.iso tarball. Remove the
part for net/core/dev.c, and always set DPA_NETIF_FEATURES to 0
since we don't support hw qdisc optimization.]
Signed-off-by: Kevin Hao <kexin.hao@windriver.com>
---
 drivers/net/ethernet/freescale/dpa/dpaa_eth.c    |  196 +++++++++++++++++-----
 drivers/net/ethernet/freescale/dpa/dpaa_eth.h    |   22 ++-
 drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c |    3 +
 3 files changed, 175 insertions(+), 46 deletions(-)

diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth.c b/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
index 7ab9609..2378192 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
@@ -67,20 +67,23 @@
 
 #define ARRAY2_SIZE(arr)	(ARRAY_SIZE(arr) * ARRAY_SIZE((arr)[0]))
 
+/* 
+ * We don't support hw qdisc optimization no matther SG is
+ * enabled or not.
+ */ 
+#define DPA_NETIF_FEATURES	0
+
 #define DEFAULT_COUNT		128
 #define REFILL_THRESHOLD	80
 
 #define DPA_NAPI_WEIGHT		64
 
-/*
- * Size in bytes of the QMan taildrop threshold.
- * If a FQ contains a number of bytes higher than this threshold,
- * it will drop any subsequently enqueued frame.
- *
- * In the future, we may want to have different values for queues
- * belonging to 1G vs 10G ports.
+/**
+ * Size in bytes of the Congestion State threshold.
+ * Used for CGR-based congestion control on the egress ports.
+ * Values for the 1G and 10G ports should be proportionately different.
  */
-#define DPA_TAILDROP_THRESHOLD	0x200000
+#define DPA_CS_THRESHOLD	0x10000000
 
 /* S/G table requires at least 256 bytes */
 #define SGT_BUFFER_SIZE		DPA_BP_SIZE(256)
@@ -540,12 +543,24 @@ _dpa_fq_alloc(struct list_head *list, struct dpa_fq *dpa_fq)
 	fq = &dpa_fq->fq_base;
 
 	if (dpa_fq->init) {
-		initfq.we_mask = QM_INITFQ_WE_DESTWQ;
+		initfq.we_mask = QM_INITFQ_WE_FQCTRL;
+		/* FIXME: why would we want to keep an empty FQ in cache? */
+		initfq.fqd.fq_ctrl = QM_FQCTRL_PREFERINCACHE;
+
+		/* FQ placement */
+		initfq.we_mask |= QM_INITFQ_WE_DESTWQ;
+
 		initfq.fqd.dest.channel	= dpa_fq->channel;
 		initfq.fqd.dest.wq = dpa_fq->wq;
-		initfq.we_mask |= QM_INITFQ_WE_TDTHRESH | QM_INITFQ_WE_FQCTRL;
-		qm_fqd_taildrop_set(&initfq.fqd.td, DPA_TAILDROP_THRESHOLD, 1);
-		initfq.fqd.fq_ctrl = QM_FQCTRL_TDE | QM_FQCTRL_PREFERINCACHE;
+
+		/* Put all egress queues in a congestion group of their own */
+		initfq.we_mask |= QM_INITFQ_WE_CGID;
+		if (dpa_fq->fq_type == FQ_TYPE_TX) {
+			initfq.fqd.fq_ctrl |= QM_FQCTRL_CGE;
+			initfq.fqd.cgid = priv->cgr_data.cgr.cgrid;
+		}
+
+		/* Initialization common to all ingress queues */
 		if (dpa_fq->flags & QMAN_FQ_FLAG_NO_ENQUEUE) {
 			initfq.we_mask |= QM_INITFQ_WE_CONTEXTA;
 			initfq.fqd.fq_ctrl |=
@@ -1649,7 +1664,7 @@ int __hot dpa_tx(struct sk_buff *skb, struct net_device *net_dev)
 	if (dpaa_eth_hooks.tx &&
 		dpaa_eth_hooks.tx(skb, net_dev) == DPAA_ETH_STOLEN)
 		/* won't update any Tx stats */
-		return NETDEV_TX_OK;
+		goto done;
 
 	priv = netdev_priv(net_dev);
 	percpu_priv = per_cpu_ptr(priv->percpu_priv, smp_processor_id());
@@ -1664,7 +1679,7 @@ int __hot dpa_tx(struct sk_buff *skb, struct net_device *net_dev)
 		if (unlikely(!skb_new)) {
 			percpu_priv->stats.tx_errors++;
 			kfree_skb(skb);
-			return NETDEV_TX_OK;
+			goto done;
 		}
 		kfree_skb(skb);
 		skb = skb_new;
@@ -1700,7 +1715,7 @@ int __hot dpa_tx(struct sk_buff *skb, struct net_device *net_dev)
 		skb = skb_unshare(skb, GFP_ATOMIC);
 		if (unlikely(!skb)) {
 			percpu_priv->stats.tx_errors++;
-			return NETDEV_TX_OK;
+			goto done;
 		}
 		err = skb_to_contig_fd(priv, percpu_priv, skb, &fd);
 	}
@@ -1722,8 +1737,7 @@ int __hot dpa_tx(struct sk_buff *skb, struct net_device *net_dev)
 		goto xmit_failed;
 
 	net_dev->trans_start = jiffies;
-
-	return NETDEV_TX_OK;
+	goto done;
 
 xmit_failed:
 	if (fd.cmd & FM_FD_CMD_FCO) {
@@ -1734,10 +1748,34 @@ fd_create_failed:
 	_dpa_cleanup_tx_fd(priv, &fd);
 	dev_kfree_skb(skb);
 
+done:
 	return NETDEV_TX_OK;
 }
 #endif /* CONFIG_DPAA_ETH_SG_SUPPORT */
 
+/**
+ * Congestion group state change notification callback.
+ * Stops the device's egress queues while they are congested and
+ * wakes them upon exiting congested state.
+ * Also updates some CGR-related stats.
+ */
+static void dpaa_eth_cgscn(struct qman_portal *qm, struct qman_cgr *cgr,
+	int congested)
+{
+	struct dpa_priv_s *priv = (struct dpa_priv_s *)container_of(cgr,
+		struct dpa_priv_s, cgr_data.cgr);
+
+	if (congested) {
+		priv->cgr_data.congestion_start_jiffies = jiffies;
+		netif_tx_stop_all_queues(priv->net_dev);
+		priv->cgr_data.cgr_congested_count++;
+	} else {
+		priv->cgr_data.congested_jiffies +=
+			(jiffies - priv->cgr_data.congestion_start_jiffies);
+		netif_tx_wake_all_queues(priv->net_dev);
+	}
+}
+
 static enum qman_cb_dqrr_result
 ingress_rx_error_dqrr(struct qman_portal		*portal,
 		      struct qman_fq			*fq,
@@ -2416,8 +2454,8 @@ static void __cold dpa_timeout(struct net_device *net_dev)
 	percpu_priv = per_cpu_ptr(priv->percpu_priv, smp_processor_id());
 
 	if (netif_msg_timer(priv))
-		cpu_netdev_crit(net_dev, "Transmit timeout latency: %lu ms\n",
-				(jiffies - net_dev->trans_start) * 1000 / HZ);
+		cpu_netdev_crit(net_dev, "Transmit timeout latency: %u ms\n",
+			jiffies_to_msecs(jiffies - net_dev->trans_start));
 
 	percpu_priv->stats.tx_errors++;
 }
@@ -2678,6 +2716,7 @@ static int __cold dpa_debugfs_show(struct seq_file *file, void *offset)
 	struct dpa_bp *dpa_bp;
 	unsigned int dpa_bp_count = 0;
 	unsigned int count_total = 0;
+	struct qm_mcr_querycgr query_cgr;
 
 	BUG_ON(offset == NULL);
 
@@ -2687,6 +2726,7 @@ static int __cold dpa_debugfs_show(struct seq_file *file, void *offset)
 
 	memset(&total, 0, sizeof(total));
 
+	/* "Standard" counters */
 	seq_printf(file, "\nDPA counters for %s:\n"
 		"CPU           irqs        rx        tx   recycle" \
 		"   confirm     tx sg    tx err    rx err  bp count\n",
@@ -2734,33 +2774,48 @@ static int __cold dpa_debugfs_show(struct seq_file *file, void *offset)
 			total.stats.rx_errors,
 			count_total);
 
+	/* Congestion stats */
+	seq_printf(file, "\nDevice congestion stats:\n");
+	seq_printf(file, "Device has been congested for %d ms.\n",
+		jiffies_to_msecs(priv->cgr_data.congested_jiffies));
+
+	qman_query_cgr(&priv->cgr_data.cgr, &query_cgr);
+	seq_printf(file, "CGR id %d avg count: %llu\n",
+		priv->cgr_data.cgr.cgrid, qm_mcr_querycgr_a_get64(&query_cgr));
+	seq_printf(file, "Device entered congestion %u times. "
+		"Current congestion state is: %s.\n",
+		priv->cgr_data.cgr_congested_count,
+		query_cgr.cgr.cs ? "congested" : "not congested");
+
+	/* Rx Errors demultiplexing */
 	seq_printf(file, "\nDPA RX Errors:\nCPU        dma err  phys err" \
 				"  size err   hdr err  csum err\n");
-		for_each_online_cpu(i) {
-			percpu_priv = per_cpu_ptr(priv->percpu_priv, i);
-
-			total.rx_errors.dme += percpu_priv->rx_errors.dme;
-			total.rx_errors.fpe += percpu_priv->rx_errors.fpe;
-			total.rx_errors.fse += percpu_priv->rx_errors.fse;
-			total.rx_errors.phe += percpu_priv->rx_errors.phe;
-			total.rx_errors.cse += percpu_priv->rx_errors.cse;
-
-			seq_printf(file, "     %hu/%hu  %8u  %8u  %8u" \
-						"  %8u  %8u\n",
-					get_hard_smp_processor_id(i), i,
-					percpu_priv->rx_errors.dme,
-					percpu_priv->rx_errors.fpe,
-					percpu_priv->rx_errors.fse,
-					percpu_priv->rx_errors.phe,
-					percpu_priv->rx_errors.cse);
-		}
-		seq_printf(file, "Total     %8u  %8u  %8u  %8u  %8u\n",
-				total.rx_errors.dme,
-				total.rx_errors.fpe,
-				total.rx_errors.fse,
-				total.rx_errors.phe,
-				total.rx_errors.cse);
+	for_each_online_cpu(i) {
+		percpu_priv = per_cpu_ptr(priv->percpu_priv, i);
+
+		total.rx_errors.dme += percpu_priv->rx_errors.dme;
+		total.rx_errors.fpe += percpu_priv->rx_errors.fpe;
+		total.rx_errors.fse += percpu_priv->rx_errors.fse;
+		total.rx_errors.phe += percpu_priv->rx_errors.phe;
+		total.rx_errors.cse += percpu_priv->rx_errors.cse;
 
+		seq_printf(file, "     %hu/%hu  %8u  %8u  %8u" \
+					"  %8u  %8u\n",
+				get_hard_smp_processor_id(i), i,
+				percpu_priv->rx_errors.dme,
+				percpu_priv->rx_errors.fpe,
+				percpu_priv->rx_errors.fse,
+				percpu_priv->rx_errors.phe,
+				percpu_priv->rx_errors.cse);
+	}
+	seq_printf(file, "Total     %8u  %8u  %8u  %8u  %8u\n",
+			total.rx_errors.dme,
+			total.rx_errors.fpe,
+			total.rx_errors.fse,
+			total.rx_errors.phe,
+			total.rx_errors.cse);
+
+	/* ERN demultiplexing */
 	seq_printf(file, "\nDPA ERN counters:\n  CPU     cg_td      wred  " \
 			"err_cond   early_w    late_w     fq_td    fq_ret" \
 			"     orp_z\n");
@@ -3381,6 +3436,43 @@ static int dpaa_eth_add_channel(void *__arg)
 	}
 	return 0;
 }
+
+static int dpaa_eth_cgr_init(struct dpa_priv_s *priv)
+{
+	struct qm_mcc_initcgr initcgr;
+	int err;
+
+	err = qman_alloc_cgrid(&priv->cgr_data.cgr.cgrid);
+	if (err < 0) {
+		cpu_pr_err("Error %d allocating CGR ID\n", err);
+		goto out_error;
+	}
+	priv->cgr_data.cgr.cb = dpaa_eth_cgscn;
+
+	/* Enable Congestion State Change Notifications and CS taildrop */
+	initcgr.we_mask = QM_CGR_WE_CSCN_EN | QM_CGR_WE_CS_THRES;
+	initcgr.cgr.cscn_en = QM_CGR_EN;
+	qm_cgr_cs_thres_set64(&initcgr.cgr.cs_thres, DPA_CS_THRESHOLD, 1);
+
+	initcgr.we_mask |= QM_CGR_WE_CSTD_EN;
+	initcgr.cgr.cstd_en = QM_CGR_EN;
+
+	err = qman_create_cgr(&priv->cgr_data.cgr, QMAN_CGR_FLAG_USE_INIT,
+		&initcgr);
+	if (err < 0) {
+		cpu_pr_err("Error %d creating CGR with ID %d\n", err,
+			priv->cgr_data.cgr.cgrid);
+		qman_release_cgrid(priv->cgr_data.cgr.cgrid);
+		goto out_error;
+	}
+	cpu_pr_debug("Created CGR %d for netdev with hwaddr %pM on "
+		"QMan channel %d\n", priv->cgr_data.cgr.cgrid,
+		priv->mac_dev->addr, priv->cgr_data.cgr.chan);
+
+out_error:
+	return err;
+}
+
 static const struct of_device_id dpa_match[] __devinitconst ;
 static int __devinit
 dpaa_eth_probe(struct platform_device *_of_dev)
@@ -3527,6 +3619,18 @@ dpaa_eth_probe(struct platform_device *_of_dev)
 		dpa_tx_fq_init(priv, &txfqlist, txdefault, txerror, txfqs,
 				txport);
 
+		/*
+		 * Create a congestion group for this netdev, with
+		 * dynamically-allocated CGR ID.
+		 * Must be executed after probing the MAC, but before
+		 * assigning the egress FQs to the CGRs.
+		 */
+		err = dpaa_eth_cgr_init(priv);
+		if (err < 0) {
+			dpaa_eth_err(dev, "Error initializing CGR\n");
+			goto cgr_init_failed;
+		}
+
 		/* Add the FQs to the interface, and make them active */
 		INIT_LIST_HEAD(&priv->dpa_fq_list);
 
@@ -3616,8 +3720,12 @@ netdev_init_failed:
 		free_percpu(priv->percpu_priv);
 alloc_percpu_failed:
 fq_alloc_failed:
-	if (net_dev)
+	if (net_dev) {
 		dpa_fq_free(dev, &priv->dpa_fq_list);
+		qman_release_cgrid(priv->cgr_data.cgr.cgrid);
+		qman_delete_cgr(&priv->cgr_data.cgr);
+	}
+cgr_init_failed:
 add_channel_failed:
 get_channel_failed:
 	if (net_dev)
diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth.h b/drivers/net/ethernet/freescale/dpa/dpaa_eth.h
index aee5ba8..7124b7d 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth.h
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth.h
@@ -61,7 +61,6 @@
 
 #define DPA_RX_PRIV_DATA_SIZE   (DPA_TX_PRIV_DATA_SIZE + \
 					dpa_get_rx_extra_headroom())
-
 /* number of Tx queues to FMan */
 #define DPAA_ETH_TX_QUEUES	8
 #define DPAA_ETH_RX_QUEUES	128
@@ -315,6 +314,23 @@ struct dpa_priv_s {
 	int			priv_pcd_num_ranges;
 	struct pcd_range	priv_pcd_ranges[FMAN_PCD_TESTS_MAX_NUM_RANGES];
 #endif
+
+	struct {
+		/**
+		 * All egress queues to a given net device belong to one
+		 * (and the same) congestion group.
+		 */
+		struct qman_cgr cgr;
+		/* If congested, when it began. Used for performance stats. */
+		u32 congestion_start_jiffies;
+		/* Number of jiffies the Tx port was congested. */
+		u32 congested_jiffies;
+		/**
+		 * Counter for the number of times the CGR
+		 * entered congestion state
+		 */
+		u32 cgr_congested_count;
+	} cgr_data;
 };
 
 extern const struct ethtool_ops dpa_ethtool_ops;
@@ -417,7 +433,9 @@ static inline int __hot dpa_xmit(struct dpa_priv_s *priv,
 		if (err != -EBUSY)
 			break;
 	}
+
 	if (unlikely(err < 0)) {
+		/* TODO differentiate b/w -EBUSY (EQCR full) and other codes? */
 		percpu->stats.tx_errors++;
 		percpu->stats.tx_fifo_errors++;
 		return err;
@@ -426,7 +444,7 @@ static inline int __hot dpa_xmit(struct dpa_priv_s *priv,
 	percpu->stats.tx_packets++;
 	percpu->stats.tx_bytes += dpa_fd_length(fd);
 
-	return NETDEV_TX_OK;
+	return 0;
 }
 
 #if defined CONFIG_DPA_ETH_WQ_LEGACY
diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c b/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c
index 2ccfd23..7db537c 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c
@@ -810,6 +810,9 @@ int __hot dpa_tx(struct sk_buff *skb, struct net_device *net_dev)
 
 xmit_failed:
 	if (fd.cmd & FM_FD_CMD_FCO) {
+		*percpu_priv->dpa_bp_count -= skb_shinfo(skb)->nr_frags + 2;
+		percpu_priv->tx_returned--;
+
 		dpa_fd_release(net_dev, &fd);
 		return NETDEV_TX_OK;
 	}
-- 
1.7.9.7

