From b94141a38144901dab232688bfda1c1285b9bbc4 Mon Sep 17 00:00:00 2001
From: Chen-Hui Zhao <chenhui.zhao@freescale.com>
Date: Tue, 20 Nov 2012 18:15:21 +0000
Subject: [PATCH 169/227] powerpc/smp: add cpu hotplug support for e6500

* Only if two threads of one core are offline, the core can
  enter PH20 state.
* Clear PH20 bits before core reset, or core will not restart.
* Introduced a variable l2cache_type in the struce cpu_spec to
  indentify the type of L2 cache.

Signed-off-by: Zhao Chenhui <chenhui.zhao@freescale.com>
Signed-off-by: Li Yang <leoli@freescale.com>
Signed-off-by: Andy Fleming <afleming@freescale.com>
[Kevin: Original patch taken from fsl sdk 1.3.1
QorIQ-SDK-V1.3.1-SOURCE-20121220-yocto.iso.]
Signed-off-by: Kevin Hao <kexin.hao@windriver.com>
---
 arch/powerpc/include/asm/cputable.h |   10 ++++
 arch/powerpc/kernel/cputable.c      |    5 ++
 arch/powerpc/platforms/85xx/smp.c   |   94 ++++++++++++++++++++++++++++-------
 3 files changed, 91 insertions(+), 18 deletions(-)

diff --git a/arch/powerpc/include/asm/cputable.h b/arch/powerpc/include/asm/cputable.h
index 0980867..6ac0dfb 100644
--- a/arch/powerpc/include/asm/cputable.h
+++ b/arch/powerpc/include/asm/cputable.h
@@ -65,6 +65,13 @@ enum powerpc_pmc_type {
 	PPC_PMC_G4 = 3,
 };
 
+enum powerpc_l2cache_type {
+	PPC_L2_CACHE_DEFAULT = 0,
+	PPC_L2_CACHE_CORE    = 1, /* L2 cache used exclusively by one core */
+	PPC_L2_CACHE_CLUSTER = 2, /* L2 cache shared by a core cluster */
+	PPC_L2_CACHE_SOC     = 3, /* L2 cache shared by all cores */
+};
+
 struct pt_regs;
 
 extern int machine_check_generic(struct pt_regs *regs);
@@ -90,6 +97,9 @@ struct cpu_spec {
 	unsigned int	icache_bsize;
 	unsigned int	dcache_bsize;
 
+	/* L2 cache type */
+	enum powerpc_l2cache_type l2cache_type;
+
 	/* number of performance monitor counters */
 	unsigned int	num_pmcs;
 	enum powerpc_pmc_type pmc_type;
diff --git a/arch/powerpc/kernel/cputable.c b/arch/powerpc/kernel/cputable.c
index fbb0dd9..0dd9a55 100644
--- a/arch/powerpc/kernel/cputable.c
+++ b/arch/powerpc/kernel/cputable.c
@@ -1994,6 +1994,7 @@ static struct cpu_spec __initdata cpu_specs[] = {
 		.cpu_setup		= __setup_cpu_e500v1,
 		.machine_check		= machine_check_e500,
 		.platform		= "ppc8540",
+		.l2cache_type		= PPC_L2_CACHE_SOC,
 	},
 	{	/* e500v2 */
 		.pvr_mask		= 0xffff0000,
@@ -2013,6 +2014,7 @@ static struct cpu_spec __initdata cpu_specs[] = {
 		.cpu_setup		= __setup_cpu_e500v2,
 		.machine_check		= machine_check_e500,
 		.platform		= "ppc8548",
+		.l2cache_type		= PPC_L2_CACHE_SOC,
 	},
 	{	/* e500mc */
 		.pvr_mask		= 0xffff0000,
@@ -2030,6 +2032,7 @@ static struct cpu_spec __initdata cpu_specs[] = {
 		.cpu_setup		= __setup_cpu_e500mc,
 		.machine_check		= machine_check_e500mc,
 		.platform		= "ppce500mc",
+		.l2cache_type		= PPC_L2_CACHE_CORE,
 	},
 #endif /* CONFIG_PPC32 */
 	{	/* e5500 */
@@ -2049,6 +2052,7 @@ static struct cpu_spec __initdata cpu_specs[] = {
 		.cpu_restore		= __restore_cpu_e5500,
 		.machine_check		= machine_check_e500mc,
 		.platform		= "ppce5500",
+		.l2cache_type		= PPC_L2_CACHE_CORE,
 	},
 #ifndef CONFIG_PPC32
 	{	/* e6500 */
@@ -2069,6 +2073,7 @@ static struct cpu_spec __initdata cpu_specs[] = {
 		.cpu_restore		= __restore_cpu_e6500,
 		.machine_check		= machine_check_e500mc,
 		.platform		= "ppce6500",
+		.l2cache_type		= PPC_L2_CACHE_CLUSTER,
 	},
 #endif
 #ifdef CONFIG_PPC32
diff --git a/arch/powerpc/platforms/85xx/smp.c b/arch/powerpc/platforms/85xx/smp.c
index 6199a72..28ce9c8 100644
--- a/arch/powerpc/platforms/85xx/smp.c
+++ b/arch/powerpc/platforms/85xx/smp.c
@@ -48,6 +48,8 @@ static int tb_valid;
 static u32 cur_booting_core;
 static bool rcpmv2;
 
+extern void fsl_enable_threads(void);
+
 #ifdef CONFIG_PPC_E500MC
 /* get a physical mask of online cores and booting core */
 static inline u32 get_phy_cpu_mask(void)
@@ -167,6 +169,19 @@ static void __cpuinit smp_85xx_setup_cpu(int cpu_nr);
 
 #ifdef CONFIG_HOTPLUG_CPU
 #ifdef CONFIG_PPC_E500MC
+static inline bool is_core_down(unsigned int thread)
+{
+	cpumask_t thd_mask;
+
+	if (!smt_capable())
+		return true;
+
+	cpumask_shift_left(&thd_mask, &threads_core_mask,
+			cpu_core_index_of_thread(thread) * threads_per_core);
+
+	return !cpumask_intersects(&thd_mask, cpu_online_mask);
+}
+
 static void __cpuinit smp_85xx_mach_cpu_die(void)
 {
 	unsigned int cpu = smp_processor_id();
@@ -177,8 +192,11 @@ static void __cpuinit smp_85xx_mach_cpu_die(void)
 
 	mtspr(SPRN_TCR, 0);
 
-	__flush_disable_L1();
-	disable_backside_L2_cache();
+	if (is_core_down(cpu))
+		__flush_disable_L1();
+
+	if (cur_cpu_spec->l2cache_type == PPC_L2_CACHE_CORE)
+		disable_backside_L2_cache();
 
 	generic_set_cpu_dead(cpu);
 
@@ -189,10 +207,17 @@ static void __cpuinit smp_85xx_mach_cpu_die(void)
 void platform_cpu_die(unsigned int cpu)
 {
 	unsigned int hw_cpu = get_hard_smp_processor_id(cpu);
-	struct ccsr_rcpm __iomem *rcpm = guts_regs;
-
-	/* Core Nap Operation */
-	setbits32(&rcpm->cnapcr, 1 << hw_cpu);
+	struct ccsr_rcpm __iomem *rcpm;
+
+	if (rcpmv2 && is_core_down(cpu)) {
+		/* enter PH20 status */
+		setbits32(&((struct ccsr_rcpm_v2 *)guts_regs)->pcph20setr,
+				1 << cpu_core_index_of_thread(hw_cpu));
+	} else if (!rcpmv2) {
+		rcpm = guts_regs;
+		/* Core Nap Operation */
+		setbits32(&rcpm->cnapcr, 1 << hw_cpu);
+	}
 }
 #else
 /* for e500v1 and e500v2 */
@@ -256,6 +281,7 @@ static int __cpuinit smp_85xx_kick_cpu(int nr)
 #endif
 #ifdef CONFIG_PPC_E500MC
 	struct ccsr_rcpm __iomem *rcpm = guts_regs;
+	struct ccsr_rcpm_v2 __iomem *rcpm_v2 = guts_regs;
 #endif
 
 	WARN_ON(nr < 0 || nr >= NR_CPUS);
@@ -265,14 +291,45 @@ static int __cpuinit smp_85xx_kick_cpu(int nr)
 
 #ifdef CONFIG_PPC64
 	/* If the cpu we're kicking is a thread, kick it and return */
-	if (cpu_thread_in_core(nr) != 0) {
-		local_irq_save(flags);
-
-		smp_generic_kick_cpu(nr);
-
-		local_irq_restore(flags);
+	if (smt_capable() && (cpu_thread_in_core(nr) != 0)) {
+		/*
+		 * Since Thread 1 can not start Thread 0 in the same core,
+		 * Thread 0 of each core must run first before starting
+		 * Thread 1.
+		 */
+		if (cpu_online(cpu_first_thread_sibling(nr))) {
+
+			local_irq_save(flags);
+			/*
+			 * In cpu hotplug case, Thread 1 of Core 0 must
+			 * start by calling fsl_enable_threads(). Thread 1
+			 * of other cores can be started by Thread 0
+			 * after reset.
+			 */
+			if (nr == 1 && system_state == SYSTEM_RUNNING)
+				fsl_enable_threads();
+
+			smp_generic_kick_cpu(nr);
+
+			generic_set_cpu_up(nr);
+			cur_booting_core = hw_cpu;
+
+			local_irq_restore(flags);
+
+			return 0;
+		} else {
+			pr_err("%s: Can not start CPU #%d. Start CPU #%d first.\n",
+				__func__, nr, cpu_first_thread_sibling(nr));
+			return -ENOENT;
+		}
+	}
 
-		return 0;
+	/* Starting Thread 0 will reset core, so put both threads down first */
+	if (smt_capable() && system_state == SYSTEM_RUNNING &&
+			cpu_thread_in_core(nr) == 0 && !is_core_down(nr)) {
+			pr_err("%s: Can not start CPU #%d. Put CPU #%d down first.",
+				__func__, nr, cpu_last_thread_sibling(nr));
+			return -ENOENT;
 	}
 #endif
 
@@ -314,11 +371,12 @@ static int __cpuinit smp_85xx_kick_cpu(int nr)
 		flush_spin_table(spin_table);
 
 #ifdef CONFIG_PPC_E500MC
-		/*
-		 * Due to an erratum of core warm reset, clear NAP bits
-		 * in the CNAPCR register by hand prior to reset.
-		 */
-		clrbits32(&rcpm->cnapcr, 1 << hw_cpu);
+		/* Due to an erratum, wake the core before reset. */
+		if (rcpmv2)
+			setbits32(&rcpm_v2->pcph20clrr,
+				1 << cpu_core_index_of_thread(hw_cpu));
+		else
+			clrbits32(&rcpm->cnapcr, 1 << hw_cpu);
 #endif
 
 		/*
-- 
1.7.9.7

