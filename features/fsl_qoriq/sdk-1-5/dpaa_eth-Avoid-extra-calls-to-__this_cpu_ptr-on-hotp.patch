From 43db97b7f6d066c3ac0c10478b9fb4aa70e0a2d0 Mon Sep 17 00:00:00 2001
From: Ioana Radulescu <ruxandra.radulescu@freescale.com>
Date: Wed, 4 Dec 2013 17:08:25 +0200
Subject: [PATCH 368/383] dpaa_eth: Avoid extra calls to __this_cpu_ptr on
 hotpath

A pointer to the percpu counter for buffers in the global Rx buffer
pool is acquired in several places along the driver's hotpath.
It's more efficient to obtain it just once and then pass it along
as an argument to subsequent functions.

Signed-off-by: Ioana Radulescu <ruxandra.radulescu@freescale.com>
Change-Id: Ic5fc750fa50795586fb5f049a252accd7b1af574
Reviewed-on: http://git.am.freescale.net:8181/7091
Reviewed-by: Cristian-Constantin Sovaiala <Cristian.Sovaiala@freescale.com>
Reviewed-by: Thomas Trefny <Tom.Trefny@freescale.com>
Tested-by: Review Code-CDREVIEW <CDREVIEW@freescale.com>
[Original patch taken from QorIQ-SDK-V1.5-SOURCE-20131219-yocto.iso]
Signed-off-by: Bin Jiang <bin.jiang@windriver.com>
---
 drivers/net/ethernet/freescale/dpa/dpaa_eth.c    |   13 ++++++++--
 drivers/net/ethernet/freescale/dpa/dpaa_eth.h    |    5 ++-
 drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c |   28 +++++++++------------
 3 files changed, 25 insertions(+), 21 deletions(-)

diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth.c b/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
index d5b1a33..33717ad 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth.c
@@ -321,16 +321,18 @@ priv_rx_error_dqrr(struct qman_portal		*portal,
 	struct net_device		*net_dev;
 	struct dpa_priv_s		*priv;
 	struct dpa_percpu_priv_s	*percpu_priv;
+	int				*count_ptr;
 
 	net_dev = ((struct dpa_fq *)fq)->net_dev;
 	priv = netdev_priv(net_dev);
 
 	percpu_priv = __this_cpu_ptr(priv->percpu_priv);
+	count_ptr = __this_cpu_ptr(priv->dpa_bp->percpu_count);
 
 	if (dpaa_eth_napi_schedule(percpu_priv, portal))
 		return qman_cb_dqrr_stop;
 
-	if (unlikely(dpaa_eth_refill_bpools(priv->dpa_bp)))
+	if (unlikely(dpaa_eth_refill_bpools(priv->dpa_bp, count_ptr)))
 		/* Unable to refill the buffer pool due to insufficient
 		 * system memory. Just release the frame back into the pool,
 		 * otherwise we'll soon end up with an empty buffer pool.
@@ -351,29 +353,34 @@ priv_rx_default_dqrr(struct qman_portal		*portal,
 	struct net_device		*net_dev;
 	struct dpa_priv_s		*priv;
 	struct dpa_percpu_priv_s	*percpu_priv;
+	int                             *count_ptr;
+	struct dpa_bp			*dpa_bp;
 
 	net_dev = ((struct dpa_fq *)fq)->net_dev;
 	priv = netdev_priv(net_dev);
+	dpa_bp = priv->dpa_bp;
 
 	/* Trace the Rx fd */
 	trace_dpa_rx_fd(net_dev, fq, &dq->fd);
 
 	/* IRQ handler, non-migratable; safe to use __this_cpu_ptr here */
 	percpu_priv = __this_cpu_ptr(priv->percpu_priv);
+	count_ptr = __this_cpu_ptr(dpa_bp->percpu_count);
 
 	if (unlikely(dpaa_eth_napi_schedule(percpu_priv, portal)))
 		return qman_cb_dqrr_stop;
 
 	/* Vale of plenty: make sure we didn't run out of buffers */
 
-	if (unlikely(dpaa_eth_refill_bpools(priv->dpa_bp)))
+	if (unlikely(dpaa_eth_refill_bpools(dpa_bp, count_ptr)))
 		/* Unable to refill the buffer pool due to insufficient
 		 * system memory. Just release the frame back into the pool,
 		 * otherwise we'll soon end up with an empty buffer pool.
 		 */
 		dpa_fd_release(net_dev, &dq->fd);
 	else
-		_dpa_rx(net_dev, portal, priv, percpu_priv, &dq->fd, fq->fqid);
+		_dpa_rx(net_dev, portal, priv, percpu_priv, &dq->fd, fq->fqid,
+			count_ptr);
 
 	return qman_cb_dqrr_consume;
 }
diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth.h b/drivers/net/ethernet/freescale/dpa/dpaa_eth.h
index 80e0d20..88aa4d8 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth.h
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth.h
@@ -419,13 +419,14 @@ struct fm_port_fqs {
 
 /* functions with different implementation for SG and non-SG: */
 int dpa_bp_priv_seed(struct dpa_bp *dpa_bp);
-int dpaa_eth_refill_bpools(struct dpa_bp *dpa_bp);
+int dpaa_eth_refill_bpools(struct dpa_bp *dpa_bp, int *count_ptr);
 void __hot _dpa_rx(struct net_device *net_dev,
 		struct qman_portal *portal,
 		const struct dpa_priv_s *priv,
 		struct dpa_percpu_priv_s *percpu_priv,
 		const struct qm_fd *fd,
-		u32 fqid);
+		u32 fqid,
+		int *count_ptr);
 int __hot dpa_tx(struct sk_buff *skb, struct net_device *net_dev);
 struct sk_buff *_dpa_cleanup_tx_fd(const struct dpa_priv_s *priv,
 				   const struct qm_fd *fd);
diff --git a/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c b/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c
index 9dd3e11..c8c3496 100644
--- a/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c
+++ b/drivers/net/ethernet/freescale/dpa/dpaa_eth_sg.c
@@ -68,13 +68,12 @@
  * @vaddr fragment must have been allocated with netdev_alloc_frag(),
  * specifically for fitting into @dpa_bp.
  */
-static void dpa_bp_recycle_frag(struct dpa_bp *dpa_bp, unsigned long vaddr)
+static void dpa_bp_recycle_frag(struct dpa_bp *dpa_bp, unsigned long vaddr,
+				int *count_ptr)
 {
 	struct bm_buffer bmb;
-	int *count_ptr;
 	dma_addr_t addr;
 
-	count_ptr = __this_cpu_ptr(dpa_bp->percpu_count);
 	addr = dma_map_single(dpa_bp->dev, (void *)vaddr, dpa_bp->size,
 			      DMA_BIDIRECTIONAL);
 	if (unlikely(dma_mapping_error(dpa_bp->dev, addr))) {
@@ -185,9 +184,8 @@ int dpa_bp_priv_seed(struct dpa_bp *dpa_bp)
 /* Add buffers/(pages) for Rx processing whenever bpool count falls below
  * REFILL_THRESHOLD.
  */
-int dpaa_eth_refill_bpools(struct dpa_bp *dpa_bp)
+int dpaa_eth_refill_bpools(struct dpa_bp *dpa_bp, int *countptr)
 {
-	int *countptr = __this_cpu_ptr(dpa_bp->percpu_count);
 	int count = *countptr;
 	int new_bufs;
 
@@ -410,7 +408,8 @@ static struct sk_buff *__hot contig_fd_to_skb(const struct dpa_priv_s *priv,
  * The page fragment holding the S/G Table is recycled here.
  */
 static struct sk_buff *__hot sg_fd_to_skb(const struct dpa_priv_s *priv,
-			       const struct qm_fd *fd, int *use_gro)
+			       const struct qm_fd *fd, int *use_gro,
+			       int *count_ptr)
 {
 	const struct qm_sg_entry *sgt;
 	dma_addr_t addr = qm_fd_addr(fd);
@@ -424,7 +423,6 @@ static struct sk_buff *__hot sg_fd_to_skb(const struct dpa_priv_s *priv,
 	int i;
 	const fm_prs_result_t *parse_results;
 	struct sk_buff *skb = NULL, *skb_tmp, **skbh;
-	int *count_ptr;
 
 	vaddr = phys_to_virt(addr);
 	DPA_BUG_ON(!IS_ALIGNED((unsigned long)vaddr, SMP_CACHE_BYTES));
@@ -438,7 +436,6 @@ static struct sk_buff *__hot sg_fd_to_skb(const struct dpa_priv_s *priv,
 
 		/* We use a single global Rx pool */
 		DPA_BUG_ON(dpa_bp != dpa_bpid2pool(sgt[i].bpid));
-		count_ptr = __this_cpu_ptr(dpa_bp->percpu_count);
 
 		sg_addr = qm_sg_addr(&sgt[i]);
 		sg_vaddr = phys_to_virt(sg_addr);
@@ -519,7 +516,7 @@ static struct sk_buff *__hot sg_fd_to_skb(const struct dpa_priv_s *priv,
 
 	/* recycle the SGT fragment */
 	DPA_BUG_ON(dpa_bp != dpa_bpid2pool(fd->bpid));
-	dpa_bp_recycle_frag(dpa_bp, (unsigned long)vaddr);
+	dpa_bp_recycle_frag(dpa_bp, (unsigned long)vaddr, count_ptr);
 	return skb;
 }
 
@@ -528,7 +525,8 @@ void __hot _dpa_rx(struct net_device *net_dev,
 		const struct dpa_priv_s *priv,
 		struct dpa_percpu_priv_s *percpu_priv,
 		const struct qm_fd *fd,
-		u32 fqid)
+		u32 fqid,
+		int *count_ptr)
 {
 	struct dpa_bp *dpa_bp;
 	struct sk_buff *skb;
@@ -537,7 +535,6 @@ void __hot _dpa_rx(struct net_device *net_dev,
 	unsigned int skb_len;
 	struct rtnl_link_stats64 *percpu_stats = &percpu_priv->stats;
 	int use_gro = net_dev->features & NETIF_F_GRO;
-	int *count_ptr;
 
 	if (unlikely(fd_status & FM_FD_STAT_RX_ERRORS) != 0) {
 		if (netif_msg_hw(priv) && net_ratelimit())
@@ -550,7 +547,6 @@ void __hot _dpa_rx(struct net_device *net_dev,
 
 	dpa_bp = priv->dpa_bp;
 	DPA_BUG_ON(dpa_bp != dpa_bpid2pool(fd->bpid));
-	count_ptr = __this_cpu_ptr(dpa_bp->percpu_count);
 
 	/* prefetch the first 64 bytes of the frame or the SGT start */
 	dma_unmap_single(dpa_bp->dev, addr, dpa_bp->size, DMA_BIDIRECTIONAL);
@@ -571,7 +567,7 @@ void __hot _dpa_rx(struct net_device *net_dev,
 #endif
 		skb = contig_fd_to_skb(priv, fd, &use_gro);
 	} else
-		skb = sg_fd_to_skb(priv, fd, &use_gro);
+		skb = sg_fd_to_skb(priv, fd, &use_gro, count_ptr);
 
 	/* Account for either the contig buffer or the SGT buffer (depending on
 	 * which case we were in) having been removed from the pool.
@@ -619,7 +615,8 @@ _release_frame:
 }
 
 static int __hot skb_to_contig_fd(struct dpa_priv_s *priv,
-				  struct sk_buff *skb, struct qm_fd *fd)
+				  struct sk_buff *skb, struct qm_fd *fd,
+				  int *count_ptr)
 {
 	struct sk_buff **skbh;
 	dma_addr_t addr;
@@ -630,7 +627,6 @@ static int __hot skb_to_contig_fd(struct dpa_priv_s *priv,
 	unsigned char *buffer_start;
 
 #if (!defined(CONFIG_FSL_DPAA_TS) && !defined(CONFIG_FSL_DPAA_1588))
-	int *count_ptr = __this_cpu_ptr(dpa_bp->percpu_count);
 
 	/* Check recycling conditions; only if timestamp support is not
 	 * enabled, otherwise we need the fd back on tx confirmation
@@ -905,7 +901,7 @@ int __hot dpa_tx(struct sk_buff *skb, struct net_device *net_dev)
 			goto enomem;
 
 		/* Finally, create a contig FD from this skb */
-		err = skb_to_contig_fd(priv, skb, &fd);
+		err = skb_to_contig_fd(priv, skb, &fd, countptr);
 	}
 	if (unlikely(err < 0))
 		goto skb_to_fd_failed;
-- 
1.7.5.4

