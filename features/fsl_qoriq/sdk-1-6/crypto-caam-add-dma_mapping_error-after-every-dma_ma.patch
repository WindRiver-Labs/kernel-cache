From 2a5b5f24e7c916a835d2a960f9874c9ca98e733d Mon Sep 17 00:00:00 2001
From: Yanjiang Jin <yanjiang.jin@windriver.com>
Date: Thu, 12 Feb 2015 15:44:57 +0800
Subject: [PATCH 04/10] crypto: caam: add dma_mapping_error after every
 dma_map_single

Add all missed dma_mapping_error() check for all dma_map_single().

Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 drivers/crypto/caam/caamalg.c    |   24 ++++++++++++++++
 drivers/crypto/caam/caamalg_qi.c |   16 +++++++++++
 drivers/crypto/caam/caamhash.c   |   56 ++++++++++++++++++++++++++++++++++++-
 drivers/crypto/caam/caamrng.c    |   16 +++++++++-
 4 files changed, 108 insertions(+), 4 deletions(-)

diff --git a/drivers/crypto/caam/caamalg.c b/drivers/crypto/caam/caamalg.c
index d3aa733..dfcf561 100644
--- a/drivers/crypto/caam/caamalg.c
+++ b/drivers/crypto/caam/caamalg.c
@@ -2206,6 +2206,10 @@ static struct aead_edesc *aead_edesc_alloc(struct aead_request *req,
 	 * All other - expected input sequence: AAD, IV, text
 	 */
 	iv_dma = dma_map_single(jrdev, req->iv, ivsize, DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, iv_dma)) {
+		dev_err(jrdev, "unable to map shared descriptor\n");
+		return ERR_PTR(-ENOMEM);
+	}
 	if (is_gcm)
 		all_contig = (!assoc_nents &&
 			      iv_dma + ivsize == sg_dma_address(req->assoc) &&
@@ -2245,6 +2249,10 @@ static struct aead_edesc *aead_edesc_alloc(struct aead_request *req,
 			 desc_bytes;
 	edesc->sec4_sg_dma = dma_map_single(jrdev, edesc->sec4_sg,
 					    sec4_sg_bytes, DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, edesc->sec4_sg_dma)) {
+		dev_err(jrdev, "unable to map shared descriptor\n");
+		return ERR_PTR(-ENOMEM);
+	}
 	*all_contig_ptr = all_contig;
 
 	sec4_sg_index = 0;
@@ -2406,6 +2414,10 @@ static struct aead_edesc *aead_giv_edesc_alloc(struct aead_givcrypt_request
 
 	/* Check if data are contiguous */
 	iv_dma = dma_map_single(jrdev, greq->giv, ivsize, DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, iv_dma)) {
+		dev_err(jrdev, "unable to map shared descriptor\n");
+		return ERR_PTR(-ENOMEM);
+	}
 	if (assoc_nents || sg_dma_address(req->assoc) + req->assoclen !=
 	    iv_dma || src_nents || iv_dma + ivsize != sg_dma_address(req->src))
 		contig &= ~GIV_SRC_CONTIG;
@@ -2446,6 +2458,10 @@ static struct aead_edesc *aead_giv_edesc_alloc(struct aead_givcrypt_request
 			 desc_bytes;
 	edesc->sec4_sg_dma = dma_map_single(jrdev, edesc->sec4_sg,
 					    sec4_sg_bytes, DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, edesc->sec4_sg_dma)) {
+		dev_err(jrdev, "unable to map shared descriptor\n");
+		return ERR_PTR(-ENOMEM);
+	}
 	*contig_ptr = contig;
 
 	sec4_sg_index = 0;
@@ -2663,6 +2679,10 @@ static struct ablkcipher_edesc *ablkcipher_edesc_alloc(struct ablkcipher_request
 	 * If so, include it. If not, create scatterlist.
 	 */
 	iv_dma = dma_map_single(jrdev, req->info, ivsize, DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, iv_dma)) {
+		dev_err(jrdev, "unable to map shared descriptor\n");
+		return ERR_PTR(-ENOMEM);
+	}
 	if (!src_nents && iv_dma + ivsize == sg_dma_address(req->src))
 		iv_contig = true;
 	else
@@ -2701,6 +2721,10 @@ static struct ablkcipher_edesc *ablkcipher_edesc_alloc(struct ablkcipher_request
 
 	edesc->sec4_sg_dma = dma_map_single(jrdev, edesc->sec4_sg,
 					    sec4_sg_bytes, DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, edesc->sec4_sg_dma)) {
+		dev_err(jrdev, "unable to map shared descriptor\n");
+		return ERR_PTR(-ENOMEM);
+	}
 	edesc->iv_dma = iv_dma;
 
 #ifdef DEBUG
diff --git a/drivers/crypto/caam/caamalg_qi.c b/drivers/crypto/caam/caamalg_qi.c
index aa0423a..40fe105 100644
--- a/drivers/crypto/caam/caamalg_qi.c
+++ b/drivers/crypto/caam/caamalg_qi.c
@@ -1147,6 +1147,10 @@ static struct aead_edesc *aead_edesc_alloc(struct aead_request *req,
 
 	/* Check if data are contiguous */
 	iv_dma = dma_map_single(qidev, req->iv, ivsize, DMA_TO_DEVICE);
+	if (dma_mapping_error(qidev, iv_dma)) {
+		dev_err(qidev, "unable to map shared descriptor\n");
+		return ERR_PTR(-ENOMEM);
+	}
 	if (assoc_nents ||
 	    sg_dma_address(req->assoc) + req->assoclen != iv_dma ||
 	    src_nents || iv_dma + ivsize != sg_dma_address(req->src)) {
@@ -1174,6 +1178,10 @@ static struct aead_edesc *aead_edesc_alloc(struct aead_request *req,
 
 	qm_sg_dma = dma_map_single(qidev, sg_table,
 				qm_sg_bytes, DMA_BIDIRECTIONAL);
+	if (dma_mapping_error(qidev, qm_sg_dma)) {
+		dev_err(qidev, "unable to map shared descriptor\n");
+		return ERR_PTR(-ENOMEM);
+	}
 
 	edesc->assoc_nents = assoc_nents;
 	edesc->assoc_chained = assoc_chained;
@@ -1520,6 +1528,10 @@ static struct aead_edesc *aead_giv_edesc_alloc(struct aead_givcrypt_request
 
 	/* Check if data are contiguous */
 	iv_dma = dma_map_single(qidev, greq->giv, ivsize, DMA_TO_DEVICE);
+	if (dma_mapping_error(qidev, iv_dma)) {
+		dev_err(qidev, "unable to map shared descriptor\n");
+		return ERR_PTR(-ENOMEM);
+	}
 
 	if (assoc_nents ||
 	    sg_dma_address(req->assoc) + req->assoclen != iv_dma ||
@@ -1560,6 +1572,10 @@ static struct aead_edesc *aead_giv_edesc_alloc(struct aead_givcrypt_request
 
 	qm_sg_dma = dma_map_single(qidev, sg_table,
 				qm_sg_bytes, DMA_BIDIRECTIONAL);
+	if (dma_mapping_error(qidev, qm_sg_dma)) {
+		dev_err(qidev, "unable to map shared descriptor\n");
+		return ERR_PTR(-ENOMEM);
+	}
 
 	edesc->assoc_nents = assoc_nents;
 	edesc->assoc_chained = assoc_chained;
diff --git a/drivers/crypto/caam/caamhash.c b/drivers/crypto/caam/caamhash.c
index 5592fec..a452509 100644
--- a/drivers/crypto/caam/caamhash.c
+++ b/drivers/crypto/caam/caamhash.c
@@ -137,13 +137,19 @@ struct caam_hash_state {
 /* Common job descriptor seq in/out ptr routines */
 
 /* Map state->caam_ctx, and append seq_out_ptr command that points to it */
-static inline void map_seq_out_ptr_ctx(u32 *desc, struct device *jrdev,
+static inline dma_addr_t map_seq_out_ptr_ctx(u32 *desc, struct device *jrdev,
 				       struct caam_hash_state *state,
 				       int ctx_len)
 {
 	state->ctx_dma = dma_map_single(jrdev, state->caam_ctx,
 					ctx_len, DMA_FROM_DEVICE);
+	if (dma_mapping_error(jrdev, state->ctx_dma)) {
+		dev_err(jrdev, "unable to map shared descriptor\n");
+		return -ENOMEM;
+	}
 	append_seq_out_ptr(desc, state->ctx_dma, ctx_len, 0);
+
+	return state->ctx_dma;
 }
 
 /* Map req->result, and append seq_out_ptr command that points to it */
@@ -153,6 +159,10 @@ static inline dma_addr_t map_seq_out_ptr_result(u32 *desc, struct device *jrdev,
 	dma_addr_t dst_dma;
 
 	dst_dma = dma_map_single(jrdev, result, digestsize, DMA_FROM_DEVICE);
+	if (dma_mapping_error(jrdev, dst_dma)) {
+		dev_err(jrdev, "unable to map shared descriptor\n");
+		return -ENOMEM;
+	}
 	append_seq_out_ptr(desc, dst_dma, digestsize, 0);
 
 	return dst_dma;
@@ -166,6 +176,10 @@ static inline dma_addr_t buf_map_to_sec4_sg(struct device *jrdev,
 	dma_addr_t buf_dma;
 
 	buf_dma = dma_map_single(jrdev, buf, buflen, DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, buf_dma)) {
+		dev_err(jrdev, "unable to map shared descriptor\n");
+		return 0;
+	}
 	dma_to_sec4_sg_one(sec4_sg, buf_dma, buflen, 0);
 
 	return buf_dma;
@@ -201,14 +215,20 @@ try_buf_map_to_sec4_sg(struct device *jrdev, struct sec4_sg_entry *sec4_sg,
 }
 
 /* Map state->caam_ctx, and add it to link table */
-static inline void ctx_map_to_sec4_sg(u32 *desc, struct device *jrdev,
+static inline dma_addr_t ctx_map_to_sec4_sg(u32 *desc, struct device *jrdev,
 				      struct caam_hash_state *state,
 				      int ctx_len,
 				      struct sec4_sg_entry *sec4_sg,
 				      u32 flag)
 {
 	state->ctx_dma = dma_map_single(jrdev, state->caam_ctx, ctx_len, flag);
+	if (dma_mapping_error(jrdev, state->ctx_dma)) {
+		dev_err(jrdev, "unable to map shared descriptor\n");
+		return -ENOMEM;
+	}
 	dma_to_sec4_sg_one(sec4_sg, state->ctx_dma, ctx_len, 0);
+
+	return state->ctx_dma;
 }
 
 /* Common shared descriptor commands */
@@ -821,6 +841,10 @@ static int ahash_update_ctx(struct ahash_request *req)
 		edesc->sec4_sg_dma = dma_map_single(jrdev, edesc->sec4_sg,
 						     sec4_sg_bytes,
 						     DMA_TO_DEVICE);
+		if (dma_mapping_error(jrdev, edesc->sec4_sg_dma)) {
+			dev_err(jrdev, "unable to map shared descriptor\n");
+			return -ENOMEM;
+		}
 
 		ctx_map_to_sec4_sg(desc, jrdev, state, ctx->ctx_len,
 				   edesc->sec4_sg, DMA_BIDIRECTIONAL);
@@ -923,6 +947,10 @@ static int ahash_final_ctx(struct ahash_request *req)
 			 DESC_JOB_IO_LEN;
 	edesc->sec4_sg_dma = dma_map_single(jrdev, edesc->sec4_sg,
 					    sec4_sg_bytes, DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, edesc->sec4_sg_dma)) {
+		dev_err(jrdev, "unable to map shared descriptor\n");
+		return -ENOMEM;
+	}
 	edesc->src_nents = 0;
 
 	ctx_map_to_sec4_sg(desc, jrdev, state, ctx->ctx_len, edesc->sec4_sg,
@@ -1001,6 +1029,10 @@ static int ahash_finup_ctx(struct ahash_request *req)
 			 DESC_JOB_IO_LEN;
 	edesc->sec4_sg_dma = dma_map_single(jrdev, edesc->sec4_sg,
 					    sec4_sg_bytes, DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, edesc->sec4_sg_dma)) {
+		dev_err(jrdev, "unable to map shared descriptor\n");
+		return -ENOMEM;
+	}
 
 	ctx_map_to_sec4_sg(desc, jrdev, state, ctx->ctx_len, edesc->sec4_sg,
 			   DMA_TO_DEVICE);
@@ -1068,6 +1100,10 @@ static int ahash_digest(struct ahash_request *req)
 			  DESC_JOB_IO_LEN;
 	edesc->sec4_sg_dma = dma_map_single(jrdev, edesc->sec4_sg,
 					    sec4_sg_bytes, DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, edesc->sec4_sg_dma)) {
+		dev_err(jrdev, "unable to map shared descriptor\n");
+		return -ENOMEM;
+	}
 	edesc->src_nents = src_nents;
 	edesc->chained = chained;
 
@@ -1135,6 +1171,10 @@ static int ahash_final_no_ctx(struct ahash_request *req)
 	init_job_desc_shared(desc, ptr, sh_len, HDR_SHARE_DEFER | HDR_REVERSE);
 
 	state->buf_dma = dma_map_single(jrdev, buf, buflen, DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, state->buf_dma)) {
+		dev_err(jrdev, "unable to map shared descriptor\n");
+		return -ENOMEM;
+	}
 
 	append_seq_in_ptr(desc, state->buf_dma, buflen, 0);
 
@@ -1210,6 +1250,10 @@ static int ahash_update_no_ctx(struct ahash_request *req)
 		edesc->sec4_sg_dma = dma_map_single(jrdev, edesc->sec4_sg,
 						    sec4_sg_bytes,
 						    DMA_TO_DEVICE);
+		if (dma_mapping_error(jrdev, edesc->sec4_sg_dma)) {
+			dev_err(jrdev, "unable to map shared descriptor\n");
+			return -ENOMEM;
+		}
 
 		state->buf_dma = buf_map_to_sec4_sg(jrdev, edesc->sec4_sg,
 						    buf, *buflen);
@@ -1309,6 +1353,10 @@ static int ahash_finup_no_ctx(struct ahash_request *req)
 			 DESC_JOB_IO_LEN;
 	edesc->sec4_sg_dma = dma_map_single(jrdev, edesc->sec4_sg,
 					    sec4_sg_bytes, DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, edesc->sec4_sg_dma)) {
+		dev_err(jrdev, "unable to map shared descriptor\n");
+		return -ENOMEM;
+	}
 
 	state->buf_dma = try_buf_map_to_sec4_sg(jrdev, edesc->sec4_sg, buf,
 						state->buf_dma, buflen,
@@ -1393,6 +1441,10 @@ static int ahash_update_first(struct ahash_request *req)
 		edesc->sec4_sg_dma = dma_map_single(jrdev, edesc->sec4_sg,
 						    sec4_sg_bytes,
 						    DMA_TO_DEVICE);
+		if (dma_mapping_error(jrdev, edesc->sec4_sg_dma)) {
+			dev_err(jrdev, "unable to map shared descriptor\n");
+			return -ENOMEM;
+		}
 
 		if (src_nents) {
 			sg_to_sec4_sg_last(req->src, src_nents,
diff --git a/drivers/crypto/caam/caamrng.c b/drivers/crypto/caam/caamrng.c
index 7626e65..5c6c0e2 100644
--- a/drivers/crypto/caam/caamrng.c
+++ b/drivers/crypto/caam/caamrng.c
@@ -188,7 +188,7 @@ static int caam_read(struct hwrng *rng, void *data, size_t max, bool wait)
 				      max - copied_idx, false);
 }
 
-static inline void rng_create_sh_desc(struct caam_rng_ctx *ctx)
+static inline dma_addr_t rng_create_sh_desc(struct caam_rng_ctx *ctx)
 {
 	struct device *jrdev = ctx->jrdev;
 	u32 *desc = ctx->sh_desc;
@@ -206,13 +206,19 @@ static inline void rng_create_sh_desc(struct caam_rng_ctx *ctx)
 
 	ctx->sh_desc_dma = dma_map_single(jrdev, desc, desc_bytes(desc),
 					  DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, ctx->sh_desc_dma)) {
+		dev_err(jrdev, "unable to map shared descriptor\n");
+		return -ENOMEM;
+	}
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR, "rng shdesc@: ", DUMP_PREFIX_ADDRESS, 16, 4,
 		       desc, desc_bytes(desc), 1);
 #endif
+	return ctx->sh_desc_dma;
 }
 
-static inline void rng_create_job_desc(struct caam_rng_ctx *ctx, int buf_id)
+static inline dma_addr_t rng_create_job_desc(struct caam_rng_ctx *ctx,
+						int buf_id)
 {
 	struct device *jrdev = ctx->jrdev;
 	struct buf_data *bd = &ctx->bufs[buf_id];
@@ -223,12 +229,18 @@ static inline void rng_create_job_desc(struct caam_rng_ctx *ctx, int buf_id)
 			     HDR_REVERSE);
 
 	bd->addr = dma_map_single(jrdev, bd->buf, RN_BUF_SIZE, DMA_FROM_DEVICE);
+	if (dma_mapping_error(jrdev, bd->addr)) {
+		dev_err(jrdev, "unable to map shared descriptor\n");
+		return -ENOMEM;
+	}
 
 	append_seq_out_ptr_intlen(desc, bd->addr, RN_BUF_SIZE, 0);
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR, "rng job desc@: ", DUMP_PREFIX_ADDRESS, 16, 4,
 		       desc, desc_bytes(desc), 1);
 #endif
+
+	return bd->addr;
 }
 
 static void caam_cleanup(struct hwrng *rng)
-- 
1.7.5.4

