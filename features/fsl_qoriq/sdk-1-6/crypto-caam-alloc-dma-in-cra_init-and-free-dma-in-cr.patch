From bb272ba9c7a4f837de053854d600d542378edb16 Mon Sep 17 00:00:00 2001
From: Yanjiang Jin <yanjiang.jin@windriver.com>
Date: Sun, 15 Feb 2015 16:29:11 +0800
Subject: [PATCH 06/10] crypto: caam: alloc dma in cra_init and free dma in
 cra_exit

Some DMA entries would be created many times in different functions,
such as .setkey, .setauthsize, but only be freed once in cra_exit.
So do this change to avoid the below calltrace:

caam_jr ffe301000.jr: Device is busy
platform ffe301000.jr: DMA-API: device driver has pending DMA allocations while released from device [count=5]
One of leaked entries details: [device address=0x00000000f764a000] [size=6144 bytes] [mapped with DMA_BIDIRECTIONAL] [mapped as coherent]
------------[ cut here ]------------
WARNING: at lib/dma-debug.c:760
Modules linked in:
CPU: 1 PID: 733 Comm: kexec Not tainted 3.10.62-ltsi-WR6.0.0.0_standard #3
task: c0000000f7723300 ti: c0000000f7ce4000 task.ti: c0000000f7ce4000
NIP: c0000000004f06ac LR: c0000000004f06a8 CTR: c0000000005f1fd0
REGS: c0000000f7ce7210 TRAP: 0700   Not tainted  (3.10.62-ltsi-WR6.0.0.0_standard)
MSR: 0000000080029000 <CE,EE,ME>  CR: 24082484  XER: 00000000
SOFTE: 1

GPR00: c0000000004f06a8 c0000000f7ce7490 c000000001273700 00000000000000f8
GPR04: 0000000024082484 00000000000000f9 c000000000ce6070 c000000015980660
GPR08: c000000000ce3700 0000000000000001 000000004e4785ac 0000000000000020
GPR12: 00000000000001b9 c000000001fff780 00000000100f0000 0000000000000001
GPR16: 0000000000000002 0000000000000000 0000000000000000 0000000000000000
GPR20: 0000000000000000 0000000000000000 ffffffffffffffff 0000000000000001
GPR24: 0000000000000001 c000000001349b80 c000000001349b40 c0000000f9426e40
GPR28: c000000001349b50 0000000000000005 c0000000f7639410 c000000001349b40
NIP [c0000000004f06ac] .dma_debug_device_change+0x1ec/0x260
LR [c0000000004f06a8] .dma_debug_device_change+0x1e8/0x260
Call Trace:
[c0000000f7ce7490] [c0000000004f06a8] .dma_debug_device_change+0x1e8/0x260 (unreliable)
[c0000000f7ce7540] [c000000000a2c52c] .notifier_call_chain+0x8c/0x100
[c0000000f7ce75e0] [c0000000000a2f1c] .__blocking_notifier_call_chain+0x6c/0xe0
[c0000000f7ce7680] [c000000000571d0c] .__device_release_driver+0xdc/0x110
[c0000000f7ce7700] [c000000000571d78] .device_release_driver+0x38/0x60
[c0000000f7ce7780] [c00000000057128c] .bus_remove_device+0x12c/0x1a0
[c0000000f7ce7810] [c00000000056c87c] .device_del+0x15c/0x250
[c0000000f7ce78a0] [c00000000056c994] .device_unregister+0x24/0x50
[c0000000f7ce7920] [c00000000084bc24] .of_device_unregister+0x24/0x40
[c0000000f7ce79a0] [c000000000812968] .caam_remove+0x88/0x410
[c0000000f7ce7a70] [c0000000005747ec] .platform_drv_shutdown+0x3c/0x60
[c0000000f7ce7af0] [c00000000056ed68] .device_shutdown+0x128/0x240
[c0000000f7ce7b90] [c0000000000880b4] .kernel_restart_prepare+0x54/0x70
[c0000000f7ce7c10] [c0000000000e5cac] .kernel_kexec+0x9c/0xd0
[c0000000f7ce7c90] [c000000000088404] .SyS_reboot+0x244/0x2d0
[c0000000f7ce7e30] [c000000000000718] syscall_exit+0x0/0x8c
Instruction dump:
394a2bb0 e91b0030 3c62ff91 79291f24 386378f8 7d2a4a14 78c61f24 7d4a302a
e9290020 7fa6eb78 48546b35 60000000 <0fe00000> 2fbb0000 41fe0024 3c62ff91
---[ end trace 21fa1e15cda9c0f3 ]---

Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 drivers/crypto/caam/caamalg.c    |  236 ++++++++++++++++----------------------
 drivers/crypto/caam/caamalg_qi.c |   31 ++---
 drivers/crypto/caam/caamhash.c   |  180 ++++++++++++++++++++---------
 3 files changed, 235 insertions(+), 212 deletions(-)

diff --git a/drivers/crypto/caam/caamalg.c b/drivers/crypto/caam/caamalg.c
index dfcf561..0a8fffc 100644
--- a/drivers/crypto/caam/caamalg.c
+++ b/drivers/crypto/caam/caamalg.c
@@ -215,11 +215,14 @@ static int aead_null_set_sh_desc(struct crypto_aead *aead)
 {
 	struct aead_tfm *tfm = &aead->base.crt_aead;
 	struct caam_ctx *ctx = crypto_aead_ctx(aead);
-	struct device *jrdev = ctx->jrdev;
 	bool keys_fit_inline;
 	u32 *key_jump_cmd, *jump_cmd, *read_move_cmd, *write_move_cmd;
 	u32 *desc;
 
+	/* All ctx dma should be created in caam_cra_init()*/
+	if (!ctx->sh_desc_enc_dma || !ctx->sh_desc_dec_dma
+		|| !ctx->sh_desc_givenc_dma || !ctx->key_dma)
+		return -ENOMEM;
 	/*
 	 * Job Descriptor and Shared Descriptors
 	 * must all fit into the 64-word Descriptor h/w Buffer
@@ -294,18 +297,11 @@ static int aead_null_set_sh_desc(struct crypto_aead *aead)
 	append_seq_store(desc, ctx->authsize, LDST_CLASS_2_CCB |
 			 LDST_SRCDST_BYTE_CONTEXT);
 
-	ctx->sh_desc_enc_dma = dma_map_single(jrdev, desc,
-					      desc_bytes(desc),
-					      DMA_TO_DEVICE);
-	if (dma_mapping_error(jrdev, ctx->sh_desc_enc_dma)) {
-		dev_err(jrdev, "unable to map shared descriptor\n");
-		return -ENOMEM;
-	}
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR,
 		       "aead null enc shdesc@"__stringify(__LINE__)": ",
 		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
-		       desc_bytes(desc), 1);
+		       sizeof(u32)*DESC_MAX_USED_LEN, 1);
 #endif
 
 	/*
@@ -388,18 +384,11 @@ static int aead_null_set_sh_desc(struct crypto_aead *aead)
 	append_seq_fifo_load(desc, ctx->authsize, FIFOLD_CLASS_CLASS2 |
 			     FIFOLD_TYPE_LAST2 | FIFOLD_TYPE_ICV);
 
-	ctx->sh_desc_dec_dma = dma_map_single(jrdev, desc,
-					      desc_bytes(desc),
-					      DMA_TO_DEVICE);
-	if (dma_mapping_error(jrdev, ctx->sh_desc_dec_dma)) {
-		dev_err(jrdev, "unable to map shared descriptor\n");
-		return -ENOMEM;
-	}
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR,
 		       "aead null dec shdesc@"__stringify(__LINE__)": ",
 		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
-		       desc_bytes(desc), 1);
+		       sizeof(u32)*DESC_MAX_USED_LEN, 1);
 #endif
 
 	return 0;
@@ -409,11 +398,15 @@ static int aead_set_sh_desc(struct crypto_aead *aead)
 {
 	struct aead_tfm *tfm = &aead->base.crt_aead;
 	struct caam_ctx *ctx = crypto_aead_ctx(aead);
-	struct device *jrdev = ctx->jrdev;
 	bool keys_fit_inline;
 	u32 geniv, moveiv;
 	u32 *desc;
 
+	/* All ctx dma should be created in caam_cra_init()*/
+	if (!ctx->sh_desc_enc_dma || !ctx->sh_desc_dec_dma
+		|| !ctx->sh_desc_givenc_dma || !ctx->key_dma)
+		return -ENOMEM;
+
 	if (!ctx->authsize)
 		return 0;
 
@@ -469,17 +462,10 @@ static int aead_set_sh_desc(struct crypto_aead *aead)
 	append_seq_store(desc, ctx->authsize, LDST_CLASS_2_CCB |
 			 LDST_SRCDST_BYTE_CONTEXT);
 
-	ctx->sh_desc_enc_dma = dma_map_single(jrdev, desc,
-					      desc_bytes(desc),
-					      DMA_TO_DEVICE);
-	if (dma_mapping_error(jrdev, ctx->sh_desc_enc_dma)) {
-		dev_err(jrdev, "unable to map shared descriptor\n");
-		return -ENOMEM;
-	}
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR, "aead enc shdesc@"__stringify(__LINE__)": ",
 		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
-		       desc_bytes(desc), 1);
+		       sizeof(u32)*DESC_MAX_USED_LEN, 1);
 #endif
 
 	/*
@@ -526,17 +512,10 @@ static int aead_set_sh_desc(struct crypto_aead *aead)
 	append_seq_fifo_load(desc, ctx->authsize, FIFOLD_CLASS_CLASS2 |
 			     FIFOLD_TYPE_LAST2 | FIFOLD_TYPE_ICV);
 
-	ctx->sh_desc_dec_dma = dma_map_single(jrdev, desc,
-					      desc_bytes(desc),
-					      DMA_TO_DEVICE);
-	if (dma_mapping_error(jrdev, ctx->sh_desc_dec_dma)) {
-		dev_err(jrdev, "unable to map shared descriptor\n");
-		return -ENOMEM;
-	}
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR, "aead dec shdesc@"__stringify(__LINE__)": ",
 		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
-		       desc_bytes(desc), 1);
+		       sizeof(u32)*DESC_MAX_USED_LEN, 1);
 #endif
 
 	/*
@@ -611,17 +590,10 @@ static int aead_set_sh_desc(struct crypto_aead *aead)
 	append_seq_store(desc, ctx->authsize, LDST_CLASS_2_CCB |
 			 LDST_SRCDST_BYTE_CONTEXT);
 
-	ctx->sh_desc_givenc_dma = dma_map_single(jrdev, desc,
-						 desc_bytes(desc),
-						 DMA_TO_DEVICE);
-	if (dma_mapping_error(jrdev, ctx->sh_desc_givenc_dma)) {
-		dev_err(jrdev, "unable to map shared descriptor\n");
-		return -ENOMEM;
-	}
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR, "aead givenc shdesc@"__stringify(__LINE__)": ",
 		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
-		       desc_bytes(desc), 1);
+		       sizeof(u32)*DESC_MAX_USED_LEN, 1);
 #endif
 
 	return 0;
@@ -642,7 +614,6 @@ static int tls_set_sh_desc(struct crypto_aead *aead)
 {
 	struct aead_tfm *tfm = &aead->base.crt_aead;
 	struct caam_ctx *ctx = crypto_aead_ctx(aead);
-	struct device *jrdev = ctx->jrdev;
 	bool keys_fit_inline;
 	u32 *key_jump_cmd, *zero_payload_jump_cmd, *skip_zero_jump_cmd;
 	u32 genpad, idx_ld_datasz, idx_ld_pad, jumpback, stidx;
@@ -651,6 +622,11 @@ static int tls_set_sh_desc(struct crypto_aead *aead)
 	/* Associated data length is always = 13 for TLS */
 	unsigned int assoclen = 13;
 
+	/* All ctx dma should be created in caam_cra_init()*/
+	if (!ctx->sh_desc_enc_dma || !ctx->sh_desc_dec_dma
+		|| !ctx->sh_desc_givenc_dma || !ctx->key_dma)
+		return -ENOMEM;
+
 	if (!ctx->enckeylen || !ctx->authsize)
 		return 0;
 
@@ -784,17 +760,10 @@ static int tls_set_sh_desc(struct crypto_aead *aead)
 	append_load_imm_u32(desc, genpad, LDST_CLASS_IND_CCB |
 			    LDST_SRCDST_WORD_INFO_FIFO | LDST_IMM);
 
-	ctx->sh_desc_enc_dma = dma_map_single(jrdev, desc,
-					      desc_bytes(desc),
-					      DMA_TO_DEVICE);
-	if (dma_mapping_error(jrdev, ctx->sh_desc_enc_dma)) {
-		dev_err(jrdev, "unable to map shared descriptor\n");
-		return -ENOMEM;
-	}
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR, "tls enc shdesc@"__stringify(__LINE__)": ",
 		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
-		       desc_bytes(desc), 1);
+		       sizeof(u32)*DESC_MAX_USED_LEN, 1);
 #endif
 
 	/*
@@ -943,17 +912,10 @@ static int tls_set_sh_desc(struct crypto_aead *aead)
 	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_CLASS2 | FIFOLD_TYPE_ICV |
 			     FIFOLD_TYPE_LAST2 | ctx->authsize);
 
-	ctx->sh_desc_dec_dma = dma_map_single(jrdev, desc,
-					      desc_bytes(desc),
-					      DMA_TO_DEVICE);
-	if (dma_mapping_error(jrdev, ctx->sh_desc_dec_dma)) {
-		dev_err(jrdev, "unable to map shared descriptor\n");
-		return -ENOMEM;
-	}
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR, "tls dec shdesc@"__stringify(__LINE__)": ",
 		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
-		       desc_bytes(desc), 1);
+		       sizeof(u32)*DESC_MAX_USED_LEN, 1);
 #endif
 
 	return 0;
@@ -972,12 +934,16 @@ static int gcm_set_sh_desc(struct crypto_aead *aead)
 {
 	struct aead_tfm *tfm = &aead->base.crt_aead;
 	struct caam_ctx *ctx = crypto_aead_ctx(aead);
-	struct device *jrdev = ctx->jrdev;
 	bool key_fits_inline = false;
 	u32 *key_jump_cmd, *zero_payload_jump_cmd,
 	    *zero_assoc_jump_cmd1, *zero_assoc_jump_cmd2;
 	u32 *desc;
 
+	/* All ctx dma should be created in caam_cra_init()*/
+	if (!ctx->sh_desc_enc_dma || !ctx->sh_desc_dec_dma
+		|| !ctx->sh_desc_givenc_dma || !ctx->key_dma)
+		return -ENOMEM;
+
 	if (!ctx->enckeylen || !ctx->authsize)
 		return 0;
 
@@ -1076,17 +1042,10 @@ static int gcm_set_sh_desc(struct crypto_aead *aead)
 	append_seq_store(desc, ctx->authsize, LDST_CLASS_1_CCB |
 			 LDST_SRCDST_BYTE_CONTEXT);
 
-	ctx->sh_desc_enc_dma = dma_map_single(jrdev, desc,
-					      desc_bytes(desc),
-					      DMA_TO_DEVICE);
-	if (dma_mapping_error(jrdev, ctx->sh_desc_enc_dma)) {
-		dev_err(jrdev, "unable to map shared descriptor\n");
-		return -ENOMEM;
-	}
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR, "gcm enc shdesc@"__stringify(__LINE__)": ",
 		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
-		       desc_bytes(desc), 1);
+		       sizeof(u32)*DESC_MAX_USED_LEN, 1);
 #endif
 
 	/*
@@ -1170,17 +1129,10 @@ static int gcm_set_sh_desc(struct crypto_aead *aead)
 	append_seq_fifo_load(desc, ctx->authsize, FIFOLD_CLASS_CLASS1 |
 			     FIFOLD_TYPE_ICV | FIFOLD_TYPE_LAST1);
 
-	ctx->sh_desc_dec_dma = dma_map_single(jrdev, desc,
-					      desc_bytes(desc),
-					      DMA_TO_DEVICE);
-	if (dma_mapping_error(jrdev, ctx->sh_desc_dec_dma)) {
-		dev_err(jrdev, "unable to map shared descriptor\n");
-		return -ENOMEM;
-	}
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR, "gcm dec shdesc@"__stringify(__LINE__)": ",
 		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
-		       desc_bytes(desc), 1);
+		       sizeof(u32)*DESC_MAX_USED_LEN, 1);
 #endif
 
 	return 0;
@@ -1211,13 +1163,17 @@ static int aead_setkey(struct crypto_aead *aead,
 	/* Sizes for MDHA pads (*not* keys): MD5, SHA1, 224, 256, 384, 512 */
 	static const u8 mdpadlen[] = { 16, 20, 32, 32, 64, 64 };
 	struct caam_ctx *ctx = crypto_aead_ctx(aead);
-	struct device *jrdev = ctx->jrdev;
 	struct rtattr *rta = (void *)key;
 	struct crypto_authenc_key_param *param;
 	unsigned int authkeylen;
 	unsigned int enckeylen;
 	int ret = 0;
 
+	/* All ctx dma should be created in caam_cra_init()*/
+	if (!ctx->sh_desc_enc_dma || !ctx->sh_desc_dec_dma
+		|| !ctx->sh_desc_givenc_dma || !ctx->key_dma)
+		return -ENOMEM;
+
 	param = RTA_DATA(rta);
 	enckeylen = be32_to_cpu(param->enckeylen);
 
@@ -1254,12 +1210,6 @@ static int aead_setkey(struct crypto_aead *aead,
 	/* postpend encryption key to auth split key */
 	memcpy(ctx->key + ctx->split_key_pad_len, key + authkeylen, enckeylen);
 
-	ctx->key_dma = dma_map_single(jrdev, ctx->key, ctx->split_key_pad_len +
-				       enckeylen, DMA_TO_DEVICE);
-	if (dma_mapping_error(jrdev, ctx->key_dma)) {
-		dev_err(jrdev, "unable to map key i/o memory\n");
-		return -ENOMEM;
-	}
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR, "ctx.key@"__stringify(__LINE__)": ",
 		       DUMP_PREFIX_ADDRESS, 16, 4, ctx->key,
@@ -1269,10 +1219,6 @@ static int aead_setkey(struct crypto_aead *aead,
 	ctx->enckeylen = enckeylen;
 
 	ret = aead_set_sh_desc(aead);
-	if (ret) {
-		dma_unmap_single(jrdev, ctx->key_dma, ctx->split_key_pad_len +
-				 enckeylen, DMA_TO_DEVICE);
-	}
 
 	return ret;
 badkey:
@@ -1286,13 +1232,17 @@ static int tls_setkey(struct crypto_aead *aead, const u8 *key,
 	/* Sizes for MDHA pads (*not* keys): MD5, SHA1, 224, 256, 384, 512 */
 	static const u8 mdpadlen[] = { 16, 20, 32, 32, 64, 64 };
 	struct caam_ctx *ctx = crypto_aead_ctx(aead);
-	struct device *jrdev = ctx->jrdev;
 	struct rtattr *rta = (void *)key;
 	struct crypto_authenc_key_param *param;
 	unsigned int authkeylen;
 	unsigned int enckeylen;
 	int ret = 0;
 
+	/* All ctx dma should be created in caam_cra_init()*/
+	if (!ctx->sh_desc_enc_dma || !ctx->sh_desc_dec_dma
+		|| !ctx->sh_desc_givenc_dma || !ctx->key_dma)
+		return -ENOMEM;
+
 	param = RTA_DATA(rta);
 	enckeylen = be32_to_cpu(param->enckeylen);
 
@@ -1328,12 +1278,6 @@ static int tls_setkey(struct crypto_aead *aead, const u8 *key,
 	/* postpend encryption key to auth split key */
 	memcpy(ctx->key + ctx->split_key_pad_len, key + authkeylen, enckeylen);
 
-	ctx->key_dma = dma_map_single(jrdev, ctx->key, ctx->split_key_pad_len +
-				       enckeylen, DMA_TO_DEVICE);
-	if (dma_mapping_error(jrdev, ctx->key_dma)) {
-		dev_err(jrdev, "unable to map key i/o memory\n");
-		return -ENOMEM;
-	}
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR, "ctx.key@"__stringify(__LINE__)": ",
 		       DUMP_PREFIX_ADDRESS, 16, 4, ctx->key,
@@ -1343,10 +1287,6 @@ static int tls_setkey(struct crypto_aead *aead, const u8 *key,
 	ctx->enckeylen = enckeylen;
 
 	ret = tls_set_sh_desc(aead);
-	if (ret) {
-		dma_unmap_single(jrdev, ctx->key_dma, ctx->split_key_pad_len +
-				 enckeylen, DMA_TO_DEVICE);
-	}
 
 	return ret;
 badkey:
@@ -1358,28 +1298,22 @@ static int gcm_setkey(struct crypto_aead *aead,
 		      const u8 *key, unsigned int keylen)
 {
 	struct caam_ctx *ctx = crypto_aead_ctx(aead);
-	struct device *jrdev = ctx->jrdev;
 	int ret = 0;
 
+	/* All ctx dma should be created in caam_cra_init()*/
+	if (!ctx->sh_desc_enc_dma || !ctx->sh_desc_dec_dma
+		|| !ctx->sh_desc_givenc_dma || !ctx->key_dma)
+		return -ENOMEM;
+
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR, "key in @"__stringify(__LINE__)": ",
 		       DUMP_PREFIX_ADDRESS, 16, 4, key, keylen, 1);
 #endif
 
 	memcpy(ctx->key, key, keylen);
-	ctx->key_dma = dma_map_single(jrdev, ctx->key, keylen,
-				      DMA_TO_DEVICE);
-	if (dma_mapping_error(jrdev, ctx->key_dma)) {
-		dev_err(jrdev, "unable to map key i/o memory\n");
-		return -ENOMEM;
-	}
 	ctx->enckeylen = keylen;
 
 	ret = gcm_set_sh_desc(aead);
-	if (ret) {
-		dma_unmap_single(jrdev, ctx->key_dma, ctx->enckeylen,
-				 DMA_TO_DEVICE);
-	}
 
 	return ret;
 }
@@ -1389,23 +1323,21 @@ static int ablkcipher_setkey(struct crypto_ablkcipher *ablkcipher,
 {
 	struct caam_ctx *ctx = crypto_ablkcipher_ctx(ablkcipher);
 	struct ablkcipher_tfm *tfm = &ablkcipher->base.crt_ablkcipher;
-	struct device *jrdev = ctx->jrdev;
 	int ret = 0;
 	u32 *key_jump_cmd;
 	u32 *desc;
 
+	/* All ctx dma should be created in caam_cra_init()*/
+	if (!ctx->sh_desc_enc_dma || !ctx->sh_desc_dec_dma
+		|| !ctx->sh_desc_givenc_dma || !ctx->key_dma)
+		return -ENOMEM;
+
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR, "key in @"__stringify(__LINE__)": ",
 		       DUMP_PREFIX_ADDRESS, 16, 4, key, keylen, 1);
 #endif
 
 	memcpy(ctx->key, key, keylen);
-	ctx->key_dma = dma_map_single(jrdev, ctx->key, keylen,
-				      DMA_TO_DEVICE);
-	if (dma_mapping_error(jrdev, ctx->key_dma)) {
-		dev_err(jrdev, "unable to map key i/o memory\n");
-		return -ENOMEM;
-	}
 	ctx->enckeylen = keylen;
 
 	/* ablkcipher_encrypt shared descriptor */
@@ -1433,18 +1365,11 @@ static int ablkcipher_setkey(struct crypto_ablkcipher *ablkcipher,
 	/* Perform operation */
 	ablkcipher_append_src_dst(desc);
 
-	ctx->sh_desc_enc_dma = dma_map_single(jrdev, desc,
-					      desc_bytes(desc),
-					      DMA_TO_DEVICE);
-	if (dma_mapping_error(jrdev, ctx->sh_desc_enc_dma)) {
-		dev_err(jrdev, "unable to map shared descriptor\n");
-		return -ENOMEM;
-	}
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR,
 		       "ablkcipher enc shdesc@"__stringify(__LINE__)": ",
 		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
-		       desc_bytes(desc), 1);
+		       sizeof(u32)*DESC_MAX_USED_LEN, 1);
 #endif
 	/* ablkcipher_decrypt shared descriptor */
 	desc = ctx->sh_desc_dec;
@@ -1471,19 +1396,11 @@ static int ablkcipher_setkey(struct crypto_ablkcipher *ablkcipher,
 	/* Perform operation */
 	ablkcipher_append_src_dst(desc);
 
-	ctx->sh_desc_dec_dma = dma_map_single(jrdev, desc,
-					      desc_bytes(desc),
-					      DMA_TO_DEVICE);
-	if (dma_mapping_error(jrdev, ctx->sh_desc_enc_dma)) {
-		dev_err(jrdev, "unable to map shared descriptor\n");
-		return -ENOMEM;
-	}
-
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR,
 		       "ablkcipher dec shdesc@"__stringify(__LINE__)": ",
 		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
-		       desc_bytes(desc), 1);
+		       sizeof(u32)*DESC_MAX_USED_LEN, 1);
 #endif
 
 	return ret;
@@ -3464,6 +3381,51 @@ static int caam_cra_init(struct crypto_tfm *tfm)
 	else
 		ctx->authsize = 0;
 
+	ctx->sh_desc_enc_dma = dma_map_single(ctx->jrdev, ctx->sh_desc_enc,
+					      sizeof(u32)*DESC_MAX_USED_LEN,
+					      DMA_TO_DEVICE);
+	if (dma_mapping_error(ctx->jrdev, ctx->sh_desc_enc_dma)) {
+		dev_err(ctx->jrdev, "unable to map shared descriptor\n");
+		return -ENOMEM;
+	}
+
+	ctx->sh_desc_dec_dma = dma_map_single(ctx->jrdev,  ctx->sh_desc_dec,
+					      sizeof(u32)*DESC_MAX_USED_LEN,
+					      DMA_TO_DEVICE);
+	if (dma_mapping_error(ctx->jrdev, ctx->sh_desc_dec_dma)) {
+		dev_err(ctx->jrdev, "unable to map shared descriptor\n");
+		dma_unmap_single(ctx->jrdev, ctx->sh_desc_enc_dma,
+				 sizeof(u32)*DESC_MAX_USED_LEN, DMA_TO_DEVICE);
+		return -ENOMEM;
+	}
+
+	ctx->sh_desc_givenc_dma = dma_map_single(ctx->jrdev,
+					ctx->sh_desc_givenc,
+					sizeof(u32)*DESC_MAX_USED_LEN,
+					DMA_TO_DEVICE);
+	if (dma_mapping_error(ctx->jrdev, ctx->sh_desc_givenc_dma)) {
+		dev_err(ctx->jrdev, "unable to map shared descriptor\n");
+		dma_unmap_single(ctx->jrdev, ctx->sh_desc_enc_dma,
+				 sizeof(u32)*DESC_MAX_USED_LEN, DMA_TO_DEVICE);
+		dma_unmap_single(ctx->jrdev, ctx->sh_desc_dec_dma,
+				 sizeof(u32)*DESC_MAX_USED_LEN, DMA_TO_DEVICE);
+		return -ENOMEM;
+	}
+
+	ctx->key_dma = dma_map_single(ctx->jrdev, ctx->key,
+						 sizeof(u8)*CAAM_MAX_KEY_SIZE,
+						 DMA_TO_DEVICE);
+	if (dma_mapping_error(ctx->jrdev, ctx->key_dma)) {
+		dev_err(ctx->jrdev, "unable to map shared descriptor\n");
+		dma_unmap_single(ctx->jrdev, ctx->sh_desc_enc_dma,
+				 sizeof(u32)*DESC_MAX_USED_LEN, DMA_TO_DEVICE);
+		dma_unmap_single(ctx->jrdev, ctx->sh_desc_dec_dma,
+				 sizeof(u32)*DESC_MAX_USED_LEN, DMA_TO_DEVICE);
+		dma_unmap_single(ctx->jrdev, ctx->sh_desc_givenc_dma,
+				 sizeof(u32)*DESC_MAX_USED_LEN, DMA_TO_DEVICE);
+		return -ENOMEM;
+	}
+
 	return 0;
 }
 
@@ -3474,20 +3436,20 @@ static void caam_cra_exit(struct crypto_tfm *tfm)
 	if (ctx->sh_desc_enc_dma &&
 	    !dma_mapping_error(ctx->jrdev, ctx->sh_desc_enc_dma))
 		dma_unmap_single(ctx->jrdev, ctx->sh_desc_enc_dma,
-				 desc_bytes(ctx->sh_desc_enc), DMA_TO_DEVICE);
+				 sizeof(u32)*DESC_MAX_USED_LEN, DMA_TO_DEVICE);
 	if (ctx->sh_desc_dec_dma &&
 	    !dma_mapping_error(ctx->jrdev, ctx->sh_desc_dec_dma))
 		dma_unmap_single(ctx->jrdev, ctx->sh_desc_dec_dma,
-				 desc_bytes(ctx->sh_desc_dec), DMA_TO_DEVICE);
+				 sizeof(u32)*DESC_MAX_USED_LEN, DMA_TO_DEVICE);
 	if (ctx->sh_desc_givenc_dma &&
 	    !dma_mapping_error(ctx->jrdev, ctx->sh_desc_givenc_dma))
 		dma_unmap_single(ctx->jrdev, ctx->sh_desc_givenc_dma,
-				 desc_bytes(ctx->sh_desc_givenc),
+				 sizeof(u32)*DESC_MAX_USED_LEN,
 				 DMA_TO_DEVICE);
 	if (ctx->key_dma &&
 	    !dma_mapping_error(ctx->jrdev, ctx->key_dma))
 		dma_unmap_single(ctx->jrdev, ctx->key_dma,
-				 ctx->enckeylen + ctx->split_key_pad_len,
+				sizeof(u8)*CAAM_MAX_KEY_SIZE,
 				 DMA_TO_DEVICE);
 
 	caam_jr_free(ctx->jrdev);
diff --git a/drivers/crypto/caam/caamalg_qi.c b/drivers/crypto/caam/caamalg_qi.c
index 40fe105..785b86d 100644
--- a/drivers/crypto/caam/caamalg_qi.c
+++ b/drivers/crypto/caam/caamalg_qi.c
@@ -758,12 +758,6 @@ static int aead_setkey(struct crypto_aead *aead,
 	/* postpend encryption key to auth split key */
 	memcpy(ctx->key + ctx->split_key_pad_len, key + authkeylen, enckeylen);
 
-	ctx->key_dma = dma_map_single(jrdev, ctx->key, ctx->split_key_pad_len +
-				       enckeylen, DMA_TO_DEVICE);
-	if (dma_mapping_error(jrdev, ctx->key_dma)) {
-		dev_err(jrdev, "unable to map key i/o memory\n");
-		return -ENOMEM;
-	}
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR, "ctx.key@"__stringify(__LINE__)": ",
 		       DUMP_PREFIX_ADDRESS, 16, 4, ctx->key,
@@ -774,8 +768,6 @@ static int aead_setkey(struct crypto_aead *aead,
 
 	ret = aead_set_sh_desc(aead);
 	if (ret) {
-		dma_unmap_single(jrdev, ctx->key_dma, ctx->split_key_pad_len +
-				 enckeylen, DMA_TO_DEVICE);
 		goto badkey;
 	}
 
@@ -862,12 +854,6 @@ static int tls_setkey(struct crypto_aead *aead, const u8 *key,
 	/* postpend encryption key to auth split key */
 	memcpy(ctx->key + ctx->split_key_pad_len, key + authkeylen, enckeylen);
 
-	ctx->key_dma = dma_map_single(jrdev, ctx->key, ctx->split_key_pad_len +
-				       enckeylen, DMA_TO_DEVICE);
-	if (dma_mapping_error(jrdev, ctx->key_dma)) {
-		dev_err(jrdev, "unable to map key i/o memory\n");
-		return -ENOMEM;
-	}
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR, "ctx.key@"__stringify(__LINE__)": ",
 		       DUMP_PREFIX_ADDRESS, 16, 4, ctx->key,
@@ -877,10 +863,6 @@ static int tls_setkey(struct crypto_aead *aead, const u8 *key,
 	ctx->enckeylen = enckeylen;
 
 	ret = tls_set_sh_desc(aead);
-	if (ret) {
-		dma_unmap_single(jrdev, ctx->key_dma, ctx->split_key_pad_len +
-				 enckeylen, DMA_TO_DEVICE);
-	}
 
 	/* Now update the driver contexts with the new shared descriptor */
 	if (ctx->drv_ctx[ENCRYPT]) {
@@ -2158,6 +2140,14 @@ static int caam_cra_init(struct crypto_tfm *tfm)
 	ctx->drv_ctx[DECRYPT] = NULL;
 	ctx->drv_ctx[GIVENCRYPT] = NULL;
 
+	ctx->key_dma = dma_map_single(ctx->jrdev, ctx->key,
+						 sizeof(u8)*CAAM_MAX_KEY_SIZE,
+						 DMA_TO_DEVICE);
+	if (dma_mapping_error(ctx->jrdev, ctx->key_dma)) {
+		dev_err(ctx->jrdev, "unable to map shared descriptor\n");
+		return -ENOMEM;
+	}
+
 	return 0;
 }
 
@@ -2169,6 +2159,11 @@ static void caam_cra_exit(struct crypto_tfm *tfm)
 	caam_drv_ctx_rel(ctx->drv_ctx[DECRYPT]);
 	caam_drv_ctx_rel(ctx->drv_ctx[GIVENCRYPT]);
 
+	if (ctx->key_dma && !dma_mapping_error(ctx->jrdev, ctx->key_dma))
+		dma_unmap_single(ctx->jrdev, ctx->key_dma,
+				sizeof(u8)*CAAM_MAX_KEY_SIZE,
+				 DMA_TO_DEVICE);
+
 	caam_jr_free(ctx->jrdev);
 }
 
diff --git a/drivers/crypto/caam/caamhash.c b/drivers/crypto/caam/caamhash.c
index a452509..934f1a9 100644
--- a/drivers/crypto/caam/caamhash.c
+++ b/drivers/crypto/caam/caamhash.c
@@ -320,10 +320,15 @@ static int ahash_set_sh_desc(struct crypto_ahash *ahash)
 {
 	struct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);
 	int digestsize = crypto_ahash_digestsize(ahash);
-	struct device *jrdev = ctx->jrdev;
 	u32 have_key = 0;
 	u32 *desc;
 
+	/* All ctx dma should be created in caam_hash_cra_init()*/
+	if (!ctx->sh_desc_update_dma || !ctx->sh_desc_update_first_dma
+		|| !ctx->sh_desc_fin_dma || !ctx->sh_desc_digest_dma
+		|| !ctx->sh_desc_finup_dma || !ctx->key_dma)
+		return -ENOMEM;
+
 	if (ctx->split_key_len)
 		have_key = OP_ALG_AAI_HMAC_PRECOMP;
 
@@ -343,12 +348,6 @@ static int ahash_set_sh_desc(struct crypto_ahash *ahash)
 	/* Load data and write to result or context */
 	ahash_append_load_str(desc, ctx->ctx_len);
 
-	ctx->sh_desc_update_dma = dma_map_single(jrdev, desc, desc_bytes(desc),
-						 DMA_TO_DEVICE);
-	if (dma_mapping_error(jrdev, ctx->sh_desc_update_dma)) {
-		dev_err(jrdev, "unable to map shared descriptor\n");
-		return -ENOMEM;
-	}
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR,
 		       "ahash update shdesc@"__stringify(__LINE__)": ",
@@ -361,13 +360,6 @@ static int ahash_set_sh_desc(struct crypto_ahash *ahash)
 	ahash_data_to_out(desc, have_key | ctx->alg_type, OP_ALG_AS_INIT,
 			  ctx->ctx_len, ctx);
 
-	ctx->sh_desc_update_first_dma = dma_map_single(jrdev, desc,
-						       desc_bytes(desc),
-						       DMA_TO_DEVICE);
-	if (dma_mapping_error(jrdev, ctx->sh_desc_update_first_dma)) {
-		dev_err(jrdev, "unable to map shared descriptor\n");
-		return -ENOMEM;
-	}
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR,
 		       "ahash update first shdesc@"__stringify(__LINE__)": ",
@@ -380,12 +372,6 @@ static int ahash_set_sh_desc(struct crypto_ahash *ahash)
 	ahash_ctx_data_to_out(desc, have_key | ctx->alg_type,
 			      OP_ALG_AS_FINALIZE, digestsize, ctx);
 
-	ctx->sh_desc_fin_dma = dma_map_single(jrdev, desc, desc_bytes(desc),
-					      DMA_TO_DEVICE);
-	if (dma_mapping_error(jrdev, ctx->sh_desc_fin_dma)) {
-		dev_err(jrdev, "unable to map shared descriptor\n");
-		return -ENOMEM;
-	}
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR, "ahash final shdesc@"__stringify(__LINE__)": ",
 		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
@@ -398,12 +384,6 @@ static int ahash_set_sh_desc(struct crypto_ahash *ahash)
 	ahash_ctx_data_to_out(desc, have_key | ctx->alg_type,
 			      OP_ALG_AS_FINALIZE, digestsize, ctx);
 
-	ctx->sh_desc_finup_dma = dma_map_single(jrdev, desc, desc_bytes(desc),
-						DMA_TO_DEVICE);
-	if (dma_mapping_error(jrdev, ctx->sh_desc_finup_dma)) {
-		dev_err(jrdev, "unable to map shared descriptor\n");
-		return -ENOMEM;
-	}
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR, "ahash finup shdesc@"__stringify(__LINE__)": ",
 		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
@@ -416,13 +396,6 @@ static int ahash_set_sh_desc(struct crypto_ahash *ahash)
 	ahash_data_to_out(desc, have_key | ctx->alg_type, OP_ALG_AS_INITFINAL,
 			  digestsize, ctx);
 
-	ctx->sh_desc_digest_dma = dma_map_single(jrdev, desc,
-						 desc_bytes(desc),
-						 DMA_TO_DEVICE);
-	if (dma_mapping_error(jrdev, ctx->sh_desc_digest_dma)) {
-		dev_err(jrdev, "unable to map shared descriptor\n");
-		return -ENOMEM;
-	}
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR,
 		       "ahash digest shdesc@"__stringify(__LINE__)": ",
@@ -523,7 +496,6 @@ static int ahash_setkey(struct crypto_ahash *ahash,
 	/* Sizes for MDHA pads (*not* keys): MD5, SHA1, 224, 256, 384, 512 */
 	static const u8 mdpadlen[] = { 16, 20, 32, 32, 64, 64 };
 	struct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);
-	struct device *jrdev = ctx->jrdev;
 	int blocksize = crypto_tfm_alg_blocksize(&ahash->base);
 	int digestsize = crypto_ahash_digestsize(ahash);
 	int ret = 0;
@@ -561,12 +533,6 @@ static int ahash_setkey(struct crypto_ahash *ahash,
 	if (ret)
 		goto badkey;
 
-	ctx->key_dma = dma_map_single(jrdev, ctx->key, ctx->split_key_pad_len,
-				      DMA_TO_DEVICE);
-	if (dma_mapping_error(jrdev, ctx->key_dma)) {
-		dev_err(jrdev, "unable to map key i/o memory\n");
-		return -ENOMEM;
-	}
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR, "ctx.key@"__stringify(__LINE__)": ",
 		       DUMP_PREFIX_ADDRESS, 16, 4, ctx->key,
@@ -574,10 +540,6 @@ static int ahash_setkey(struct crypto_ahash *ahash,
 #endif
 
 	ret = ahash_set_sh_desc(ahash);
-	if (ret) {
-		dma_unmap_single(jrdev, ctx->key_dma, ctx->split_key_pad_len,
-				 DMA_TO_DEVICE);
-	}
 
 	kfree(hashed_key);
 	return ret;
@@ -1752,6 +1714,103 @@ static int caam_hash_cra_init(struct crypto_tfm *tfm)
 	crypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),
 				 sizeof(struct caam_hash_state));
 
+	ctx->sh_desc_update_dma = dma_map_single(ctx->jrdev,
+					ctx->sh_desc_update,
+					sizeof(u32)*DESC_HASH_MAX_USED_LEN,
+					DMA_TO_DEVICE);
+	if (dma_mapping_error(ctx->jrdev, ctx->sh_desc_update_dma)) {
+		dev_err(ctx->jrdev, "unable to map shared descriptor\n");
+		return -ENOMEM;
+	}
+
+	ctx->sh_desc_update_first_dma = dma_map_single(ctx->jrdev,
+					ctx->sh_desc_update_first,
+					sizeof(u32)*DESC_HASH_MAX_USED_LEN,
+					DMA_TO_DEVICE);
+	if (dma_mapping_error(ctx->jrdev, ctx->sh_desc_update_first_dma)) {
+		dev_err(ctx->jrdev, "unable to map shared descriptor\n");
+		dma_unmap_single(ctx->jrdev, ctx->sh_desc_update_dma,
+				sizeof(u32)*DESC_HASH_MAX_USED_LEN,
+				DMA_TO_DEVICE);
+		return -ENOMEM;
+	}
+
+	ctx->sh_desc_fin_dma = dma_map_single(ctx->jrdev, ctx->sh_desc_fin,
+					sizeof(u32)*DESC_HASH_MAX_USED_LEN,
+					DMA_TO_DEVICE);
+	if (dma_mapping_error(ctx->jrdev, ctx->sh_desc_fin_dma)) {
+		dev_err(ctx->jrdev, "unable to map shared descriptor\n");
+		dma_unmap_single(ctx->jrdev, ctx->sh_desc_update_dma,
+				sizeof(u32)*DESC_HASH_MAX_USED_LEN,
+				DMA_TO_DEVICE);
+		dma_unmap_single(ctx->jrdev, ctx->sh_desc_update_first_dma,
+				sizeof(u32)*DESC_HASH_MAX_USED_LEN,
+				DMA_TO_DEVICE);
+		return -ENOMEM;
+	}
+
+	ctx->sh_desc_digest_dma = dma_map_single(ctx->jrdev,
+					ctx->sh_desc_digest,
+					sizeof(u32)*DESC_HASH_MAX_USED_LEN,
+					DMA_TO_DEVICE);
+	if (dma_mapping_error(ctx->jrdev, ctx->sh_desc_digest_dma)) {
+		dev_err(ctx->jrdev, "unable to map shared descriptor\n");
+		dma_unmap_single(ctx->jrdev, ctx->sh_desc_update_dma,
+				sizeof(u32)*DESC_HASH_MAX_USED_LEN,
+				DMA_TO_DEVICE);
+		dma_unmap_single(ctx->jrdev, ctx->sh_desc_update_first_dma,
+				sizeof(u32)*DESC_HASH_MAX_USED_LEN,
+				DMA_TO_DEVICE);
+		dma_unmap_single(ctx->jrdev, ctx->sh_desc_fin_dma,
+				sizeof(u32)*DESC_HASH_MAX_USED_LEN,
+				DMA_TO_DEVICE);
+		return -ENOMEM;
+	}
+
+	ctx->sh_desc_finup_dma = dma_map_single(ctx->jrdev, ctx->sh_desc_finup,
+					sizeof(u32)*DESC_HASH_MAX_USED_LEN,
+					DMA_TO_DEVICE);
+	if (dma_mapping_error(ctx->jrdev, ctx->sh_desc_finup_dma)) {
+		dev_err(ctx->jrdev, "unable to map shared descriptor\n");
+		dma_unmap_single(ctx->jrdev, ctx->sh_desc_update_dma,
+				sizeof(u32)*DESC_HASH_MAX_USED_LEN,
+				DMA_TO_DEVICE);
+		dma_unmap_single(ctx->jrdev, ctx->sh_desc_update_first_dma,
+				sizeof(u32)*DESC_HASH_MAX_USED_LEN,
+				DMA_TO_DEVICE);
+		dma_unmap_single(ctx->jrdev, ctx->sh_desc_fin_dma,
+				sizeof(u32)*DESC_HASH_MAX_USED_LEN,
+				DMA_TO_DEVICE);
+		dma_unmap_single(ctx->jrdev, ctx->sh_desc_digest_dma,
+				sizeof(u32)*DESC_HASH_MAX_USED_LEN,
+				DMA_TO_DEVICE);
+		return -ENOMEM;
+	}
+
+	ctx->key_dma = dma_map_single(ctx->jrdev, ctx->key,
+					sizeof(u8)*CAAM_MAX_HASH_KEY_SIZE,
+					DMA_TO_DEVICE);
+	if (dma_mapping_error(ctx->jrdev, ctx->key_dma)) {
+		dma_unmap_single(ctx->jrdev, ctx->sh_desc_update_dma,
+				sizeof(u32)*DESC_HASH_MAX_USED_LEN,
+				DMA_TO_DEVICE);
+		dma_unmap_single(ctx->jrdev, ctx->sh_desc_update_first_dma,
+				sizeof(u32)*DESC_HASH_MAX_USED_LEN,
+				DMA_TO_DEVICE);
+		dma_unmap_single(ctx->jrdev, ctx->sh_desc_fin_dma,
+				sizeof(u32)*DESC_HASH_MAX_USED_LEN,
+				DMA_TO_DEVICE);
+		dma_unmap_single(ctx->jrdev, ctx->sh_desc_digest_dma,
+				sizeof(u32)*DESC_HASH_MAX_USED_LEN,
+				DMA_TO_DEVICE);
+		dma_unmap_single(ctx->jrdev, ctx->sh_desc_finup_dma,
+				sizeof(u32)*DESC_HASH_MAX_USED_LEN,
+				DMA_TO_DEVICE);
+		dev_err(ctx->jrdev, "unable to map shared descriptor\n");
+		return -ENOMEM;
+	}
+
+
 	ret = ahash_set_sh_desc(ahash);
 
 	return ret;
@@ -1762,28 +1821,35 @@ static void caam_hash_cra_exit(struct crypto_tfm *tfm)
 	struct caam_hash_ctx *ctx = crypto_tfm_ctx(tfm);
 
 	if (ctx->sh_desc_update_dma &&
-	    !dma_mapping_error(ctx->jrdev, ctx->sh_desc_update_dma))
+		!dma_mapping_error(ctx->jrdev, ctx->sh_desc_update_dma))
 		dma_unmap_single(ctx->jrdev, ctx->sh_desc_update_dma,
-				 desc_bytes(ctx->sh_desc_update),
-				 DMA_TO_DEVICE);
+				sizeof(u32)*DESC_HASH_MAX_USED_LEN,
+				DMA_TO_DEVICE);
 	if (ctx->sh_desc_update_first_dma &&
-	    !dma_mapping_error(ctx->jrdev, ctx->sh_desc_update_first_dma))
+		!dma_mapping_error(ctx->jrdev, ctx->sh_desc_update_first_dma))
 		dma_unmap_single(ctx->jrdev, ctx->sh_desc_update_first_dma,
-				 desc_bytes(ctx->sh_desc_update_first),
-				 DMA_TO_DEVICE);
+				sizeof(u32)*DESC_HASH_MAX_USED_LEN,
+				DMA_TO_DEVICE);
 	if (ctx->sh_desc_fin_dma &&
-	    !dma_mapping_error(ctx->jrdev, ctx->sh_desc_fin_dma))
+		!dma_mapping_error(ctx->jrdev, ctx->sh_desc_fin_dma))
 		dma_unmap_single(ctx->jrdev, ctx->sh_desc_fin_dma,
-				 desc_bytes(ctx->sh_desc_fin), DMA_TO_DEVICE);
+				sizeof(u32)*DESC_HASH_MAX_USED_LEN,
+				DMA_TO_DEVICE);
 	if (ctx->sh_desc_digest_dma &&
-	    !dma_mapping_error(ctx->jrdev, ctx->sh_desc_digest_dma))
+		!dma_mapping_error(ctx->jrdev, ctx->sh_desc_digest_dma))
 		dma_unmap_single(ctx->jrdev, ctx->sh_desc_digest_dma,
-				 desc_bytes(ctx->sh_desc_digest),
-				 DMA_TO_DEVICE);
+				sizeof(u32)*DESC_HASH_MAX_USED_LEN,
+				DMA_TO_DEVICE);
 	if (ctx->sh_desc_finup_dma &&
-	    !dma_mapping_error(ctx->jrdev, ctx->sh_desc_finup_dma))
+		!dma_mapping_error(ctx->jrdev, ctx->sh_desc_finup_dma))
 		dma_unmap_single(ctx->jrdev, ctx->sh_desc_finup_dma,
-				 desc_bytes(ctx->sh_desc_finup), DMA_TO_DEVICE);
+				sizeof(u32)*DESC_HASH_MAX_USED_LEN,
+				DMA_TO_DEVICE);
+	if (ctx->key_dma &&
+		!dma_mapping_error(ctx->jrdev, ctx->key_dma))
+		dma_unmap_single(ctx->jrdev, ctx->key_dma,
+				sizeof(u8)*CAAM_MAX_HASH_KEY_SIZE,
+				DMA_TO_DEVICE);
 
 	caam_jr_free(ctx->jrdev);
 }
-- 
1.7.5.4

