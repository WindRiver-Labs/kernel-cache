From eaf4a2d478ee781e2d1d661780bb4e26ee7655be Mon Sep 17 00:00:00 2001
From: Claudiu Manoil <claudiu.manoil@freescale.com>
Date: Mon, 10 Mar 2014 14:37:06 +0200
Subject: [PATCH 30/50] gianfar: Add skb recycling

Add global per-CPU skb recycle lists to improve packet
forwarding throughput.  Having per-interface recycle
lists doesn't allow skb recycling when you're e.g.
unidirectionally routing from eth0 to eth1, as eth1 will
be producing a lot of recycled skbuffs but eth0 won't
have any skbuffs to allocate from its recycle list.
Reclaiming resp. recycling of skbs is done on the Rx resp.
Tx confirmation paths, in softirq context, and the access
to the driver's per-CPU skb lists is lockless and
preemption safe.

The skb recycling support was removed from the mainline
kernel (starting with v3.0).

[Original patch taken from QorIQ-SDK-V1.6-SOURCE-20140619-yocto.iso]

Signed-off-by: Claudiu Manoil <claudiu.manoil@freescale.com>
Change-Id: I40d47d1d4da337f4e9b0b18136848aa807fc24f7
Reviewed-on: http://git.am.freescale.net:8181/9707
Tested-by: Review Code-CDREVIEW <CDREVIEW@freescale.com>
Reviewed-by: Rajan Gupta <rajan.gupta@freescale.com>
Reviewed-by: Jose Rivera <German.Rivera@freescale.com>
Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 drivers/net/ethernet/freescale/gianfar.c |  126 +++++++++++++-----------------
 1 files changed, 53 insertions(+), 73 deletions(-)

diff --git a/drivers/net/ethernet/freescale/gianfar.c b/drivers/net/ethernet/freescale/gianfar.c
index c5c5b8f..fc84421 100644
--- a/drivers/net/ethernet/freescale/gianfar.c
+++ b/drivers/net/ethernet/freescale/gianfar.c
@@ -143,7 +143,6 @@ static int gfar_poll_rx(struct napi_struct *napi, int budget);
 static int gfar_poll_tx(struct napi_struct *napi, int budget);
 static int gfar_poll_rx_sq(struct napi_struct *napi, int budget);
 static int gfar_poll_tx_sq(struct napi_struct *napi, int budget);
-static void gfar_recycle_skb(struct gfar_private *priv, struct sk_buff *skb);
 
 #ifdef CONFIG_NET_POLL_CONTROLLER
 static void gfar_netpoll(struct net_device *dev);
@@ -186,6 +185,10 @@ MODULE_AUTHOR("Freescale Semiconductor, Inc");
 MODULE_DESCRIPTION("Gianfar Ethernet Driver");
 MODULE_LICENSE("GPL");
 
+static DEFINE_PER_CPU(struct sk_buff_head, skb_recycle_list);
+
+#define GFAR_RXB_REC_SZ (DEFAULT_RX_BUFFER_SIZE + RXBUF_ALIGNMENT)
+
 static void gfar_init_rxbdp(struct gfar_priv_rx_q *rx_queue, struct rxbd8 *bdp,
 			    dma_addr_t buf)
 {
@@ -2631,7 +2634,7 @@ static int gfar_start_xmit(struct sk_buff *skb, struct net_device *dev)
 		/* BD is free to be used by s/w */
 		/* Free skb for this BD if not recycled */
 		if (tx_queue->tx_skbuff[skb_curtx]) {
-			gfar_recycle_skb(priv, tx_queue->tx_skbuff[skb_curtx]);
+			gfar_recycle_skb(tx_queue->tx_skbuff[skb_curtx]);
 			tx_queue->tx_skbuff[skb_curtx] = NULL;
 		}
 
@@ -2924,52 +2927,26 @@ static void gfar_align_skb(struct sk_buff *skb)
 		    (((unsigned long) skb->data) & (RXBUF_ALIGNMENT - 1)));
 }
 
-static void gfar_recycle_skb(struct gfar_private *priv, struct sk_buff *skb)
+static void gfar_recycle_skb(struct sk_buff *skb)
 {
-	struct gfar_priv_recycle *rec_target = priv->recycle_target;
-	struct sk_buff_head *recycle_q;
-	struct gfar_priv_recycle_local *local;
-	int cpu;
-
-	if (unlikely(!rec_target))
-		goto free;
-
-	if (unlikely(!skb_is_recycleable(skb, rec_target->buff_size)))
-		goto free;
-
-	cpu = get_cpu();
-	local = per_cpu_ptr(rec_target->local, cpu);
+	struct sk_buff_head *h = &__get_cpu_var(skb_recycle_list);
+	int skb_size = SKB_DATA_ALIGN(GFAR_RXB_REC_SZ + NET_SKB_PAD);
 
-	if (likely(skb_queue_len(&local->recycle_q) < GFAR_RECYCLE_MAX)) {
-		local->recycle_cnt++;
+	if (skb_queue_len(h) < DEFAULT_RX_RING_SIZE &&
+	    !skb_cloned(skb) && !skb_is_nonlinear(skb) &&
+	    skb->fclone == SKB_FCLONE_UNAVAILABLE && !skb_shared(skb) &&
+	    skb_end_offset(skb) == skb_size) {
 
 		skb_recycle(skb);
 
 		gfar_align_skb(skb);
 
-		__skb_queue_head(&local->recycle_q, skb);
-		put_cpu();
+		__skb_queue_head(h, skb);
+
 		return;
 	}
-	put_cpu();
-
-	recycle_q = &rec_target->recycle_q;
-
-	if (unlikely(skb_queue_len(recycle_q) >= GFAR_RECYCLE_MAX))
-		goto free;
 
-	skb_recycle(skb);
-
-	gfar_align_skb(skb);
-
-	skb_queue_head(recycle_q, skb);
-
-	atomic_inc(&rec_target->recycle_cnt);
-
-	return;
-
-free:
-	dev_kfree_skb_irq(skb);
+	dev_kfree_skb_any(skb);
 }
 
 /* Interrupt Handler for Transmit complete */
@@ -3048,7 +3025,7 @@ static void gfar_clean_tx_ring(struct gfar_priv_tx_q *tx_queue)
 			bdp = next_txbd(bdp, base, tx_ring_size);
 		}
 
-		gfar_recycle_skb(priv, skb);
+		gfar_recycle_skb(skb);
 
 		tx_queue->tx_skbuff[skb_dirtytx] = NULL;
 
@@ -3134,42 +3111,20 @@ static struct sk_buff *gfar_alloc_skb(struct net_device *dev)
 	return skb;
 }
 
-static struct sk_buff *gfar_new_skb(struct gfar_private *priv)
+struct sk_buff *gfar_new_skb(struct net_device *dev)
 {
-	struct sk_buff *skb = NULL;
-	struct gfar_priv_recycle *rec = &priv->recycle;
-	struct gfar_priv_recycle_local *local;
-	struct sk_buff_head *recycle_q;
-	int cpu;
+	struct gfar_private *priv = netdev_priv(dev);
+	struct sk_buff *skb;
 
-	if (unlikely(!rec->local))
-		goto alloc;
+	if (likely(priv->rx_buffer_size <= DEFAULT_RX_BUFFER_SIZE)) {
+		struct sk_buff_head *h = &__get_cpu_var(skb_recycle_list);
 
-	cpu = get_cpu();
-	local = per_cpu_ptr(rec->local, cpu);
-	skb = __skb_dequeue(&local->recycle_q);
-	if (likely(skb)) {
-		local->reuse_cnt++;
-		put_cpu();
-		return skb;
+		skb = __skb_dequeue(h);
+		if (skb != NULL)
+			return skb;
 	}
-	put_cpu();
-
-	recycle_q = &rec->recycle_q;
 
-	skb = skb_dequeue(recycle_q);
-
-	if (unlikely(!skb))
-		goto alloc;
-
-	atomic_inc(&rec->reuse_cnt);
-
-	return skb;
-
-alloc:
-	skb = gfar_alloc_skb(priv->ndev);
-
-	return skb;
+	return gfar_alloc_skb(dev);
 }
 
 static inline void count_errors(unsigned short status, struct net_device *dev)
@@ -3345,7 +3300,7 @@ int gfar_clean_rx_ring(struct gfar_priv_rx_q *rx_queue, int rx_work_limit)
 
 #ifndef CONFIG_RX_TX_BUFF_XCHG
 		/* Add another skb for the future */
-		newskb = gfar_new_skb(priv);
+		newskb = gfar_new_skb(dev);
 #endif
 
 		skb = rx_queue->rx_skbuff[rx_queue->skb_currx];
@@ -3406,7 +3361,7 @@ int gfar_clean_rx_ring(struct gfar_priv_rx_q *rx_queue, int rx_work_limit)
 #ifdef CONFIG_RX_TX_BUFF_XCHG
 		if (!newskb) {
 			/* Allocate new skb for Rx ring */
-			newskb = gfar_new_skb(priv);
+			newskb = gfar_new_skb(dev);
 		}
 		if (!newskb)
 			/* All memory Exhausted,a BUG */
@@ -4062,4 +4017,29 @@ static struct platform_driver gfar_driver = {
 	.remove = gfar_remove,
 };
 
-module_platform_driver(gfar_driver);
+static int __init gfar_init(void)
+{
+	int i;
+
+	for_each_possible_cpu(i) {
+		struct sk_buff_head *h = &per_cpu(skb_recycle_list, i);
+		skb_queue_head_init(h);
+	}
+
+	return platform_driver_register(&gfar_driver);
+}
+
+static void __exit gfar_exit(void)
+{
+	int i;
+
+	for_each_possible_cpu(i) {
+		struct sk_buff_head *h = &per_cpu(skb_recycle_list, i);
+		skb_queue_purge(h);
+	}
+
+	platform_driver_unregister(&gfar_driver);
+}
+
+module_init(gfar_init);
+module_exit(gfar_exit);
-- 
1.7.5.4

