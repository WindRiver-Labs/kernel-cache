From 8b10955bdc45f6f744c599767e684ae2f66ae015 Mon Sep 17 00:00:00 2001
From: Laurentiu Tudor <Laurentiu.Tudor@freescale.com>
Date: Mon, 31 Mar 2014 16:48:53 +0300
Subject: [PATCH 325/466] powerpc/booke64: wrap tlb lock and search in htw
 miss with FTR_SMT

Virtualized environments expose a e6500 dual-threaded core
as two single-threaded e6500 cores. Take advantage of this
and get rid of the tlb lock and the trap-causing tlbsx in
the htw miss handler by guarding with CPU_FTR_SMT, as it's
already being done in the bolted tlb1 miss handler.

As results below show, lmbench random memory access
latency test shows an improvement of ~34%.

Memory latencies in nanoseconds - smaller is better
    (WARNING - may not be correct, check graphs)
----------------------------------------------------
Host       Mhz   L1 $   L2 $    Main mem    Rand mem
---------  ---   ----   ----    --------    --------
smt       1665 1.8020   13.2    83.0         1149.7
nosmt     1665 1.8020   13.2    83.0          758.1

Signed-off-by: Laurentiu Tudor <Laurentiu.Tudor@freescale.com>
Cc: Scott Wood <scottwood@freescale.com>
Change-Id: Ia6c028b8bb9c847d46d32f788a7257527cd6af09
Reviewed-on: http://git.am.freescale.net:8181/12089
Tested-by: Review Code-CDREVIEW <CDREVIEW@freescale.com>
Reviewed-by: Scott Wood <scottwood@freescale.com>
Reviewed-by: Richard Schmitt <richard.schmitt@freescale.com>
[Original patch taken from QorIQ-SDK-V1.6-SOURCE-20140619-yocto.iso]
Signed-off-by: Bin Jiang <bin.jiang@windriver.com>
---
 arch/powerpc/mm/tlb_low_64e.S |    4 ++++
 1 file changed, 4 insertions(+)

diff --git a/arch/powerpc/mm/tlb_low_64e.S b/arch/powerpc/mm/tlb_low_64e.S
index ef11b26..1373b18 100644
--- a/arch/powerpc/mm/tlb_low_64e.S
+++ b/arch/powerpc/mm/tlb_low_64e.S
@@ -348,6 +348,7 @@ END_FTR_SECTION_IFSET(CPU_FTR_SMT)
  * r10 = cpu number
  */
 tlb_miss_common_fsl_htw:
+BEGIN_FTR_SECTION
 	/*
 	 * Search if we already have an indirect entry for that virtual
 	 * address, and if we do, bail out.
@@ -388,6 +389,7 @@ tlb_miss_common_fsl_htw:
 //	ori	r10,r10,MAS1_IND
 	mtspr	SPRN_MAS1,r10
 	mtspr	SPRN_MAS2,r15
+END_FTR_SECTION_IFSET(CPU_FTR_SMT)
 
 	/* Now, we need to walk the page tables. First check if we are in
 	 * range.
@@ -449,6 +451,7 @@ tlb_miss_common_fsl_htw:
 
 tlb_miss_done_fsl_htw:
 	.macro	tlb_unlock_fsl_htw
+BEGIN_FTR_SECTION
 	beq	cr1,1f		/* no unlock if lock was recursively grabbed */
 	mtocrf	0x01,r11
 	addi	r10,r11,PACA_TLB_LOCK-1
@@ -457,6 +460,7 @@ tlb_miss_done_fsl_htw:
 	isync
 	stb	r15,0(r10)
 1:
+END_FTR_SECTION_IFSET(CPU_FTR_SMT)
 	.endm
 
 	tlb_unlock_fsl_htw
-- 
1.7.10.4

