From 1acc01351f9dba103754f73e007b9aeb744aed76 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Horia=20Geant=C4=83?= <horia.geanta@freescale.com>
Date: Tue, 17 Nov 2015 00:27:00 +0200
Subject: [PATCH 331/452] crypto: dpaa2-caam - fix DMA mapping dir for
 generated IV
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

In case IV is generated by the device (GIV), the DMA mapping direction
must reflect that, i.e. it must be DMA_FROM_DEVICE.

Signed-off-by: Horia GeantÄƒ <horia.geanta@freescale.com>
[Xulin: Original patch taken from FSL LS2085 SDK EAR6.0,
LS2085A-SDK-SOURCE-20160304-yocto.iso]
Signed-off-by: Xulin Sun <xulin.sun@windriver.com>
---
 drivers/crypto/dpaa2-caam/dpaa2-caam.c |   38 +++++++++++++++++--------------
 drivers/crypto/dpaa2-caam/dpaa2-caam.h |    9 +++++++
 2 files changed, 30 insertions(+), 17 deletions(-)

diff --git a/drivers/crypto/dpaa2-caam/dpaa2-caam.c b/drivers/crypto/dpaa2-caam/dpaa2-caam.c
index 1ef4d24..3a24677 100644
--- a/drivers/crypto/dpaa2-caam/dpaa2-caam.c
+++ b/drivers/crypto/dpaa2-caam/dpaa2-caam.c
@@ -71,13 +71,6 @@
 #define GIV_SRC_CONTIG		1
 #define GIV_DST_CONTIG		(1 << 1)
 
-enum optype {
-	ENCRYPT = 0,
-	DECRYPT,
-	GIVENCRYPT,
-	NUM_OP
-};
-
 /*
  * This is a a cache of buffers, from which the users of CAAM QI driver
  * can allocate short buffers. It's speedier than doing kmalloc on the hotpath.
@@ -816,7 +809,7 @@ static struct aead_edesc *aead_edesc_alloc(struct aead_request *req,
 		       CRYPTO_TFM_REQ_MAY_SLEEP)) ? GFP_KERNEL : GFP_ATOMIC;
 	int assoc_nents, src_nents, dst_nents = 0;
 	struct aead_edesc *edesc;
-	dma_addr_t iv_dma = 0;
+	dma_addr_t iv_dma;
 	int sgc;
 	bool all_contig;
 	bool assoc_chained = false, src_chained = false, dst_chained = false;
@@ -990,7 +983,7 @@ static struct aead_edesc *aead_giv_edesc_alloc(struct aead_givcrypt_request
 		       CRYPTO_TFM_REQ_MAY_SLEEP)) ? GFP_KERNEL : GFP_ATOMIC;
 	int assoc_nents, src_nents, dst_nents = 0;
 	struct aead_edesc *edesc;
-	dma_addr_t iv_dma = 0;
+	dma_addr_t iv_dma;
 	int sgc;
 	u32 contig = GIV_SRC_CONTIG | GIV_DST_CONTIG;
 	int ivsize = crypto_aead_ivsize(aead);
@@ -1018,7 +1011,7 @@ static struct aead_edesc *aead_giv_edesc_alloc(struct aead_givcrypt_request
 					 DMA_FROM_DEVICE, dst_chained);
 	}
 
-	iv_dma = dma_map_single(dev, areq->giv, ivsize, DMA_TO_DEVICE);
+	iv_dma = dma_map_single(dev, areq->giv, ivsize, DMA_FROM_DEVICE);
 	if (dma_mapping_error(dev, iv_dma)) {
 		dev_err(dev, "unable to map IV\n");
 		return ERR_PTR(-ENOMEM);
@@ -1709,7 +1702,7 @@ static struct ablkcipher_edesc *ablkcipher_edesc_alloc(struct ablkcipher_request
 		       GFP_KERNEL : GFP_ATOMIC;
 	int src_nents, dst_nents = 0, qm_sg_bytes;
 	struct ablkcipher_edesc *edesc;
-	dma_addr_t iv_dma = 0;
+	dma_addr_t iv_dma;
 	bool iv_contig = false;
 	int sgc;
 	int ivsize = crypto_ablkcipher_ivsize(ablkcipher);
@@ -1831,7 +1824,7 @@ static struct ablkcipher_edesc *ablkcipher_giv_edesc_alloc(
 		       GFP_KERNEL : GFP_ATOMIC;
 	int src_nents, dst_nents = 0, qm_sg_bytes;
 	struct ablkcipher_edesc *edesc;
-	dma_addr_t iv_dma = 0;
+	dma_addr_t iv_dma;
 	bool iv_contig = false;
 	int sgc;
 	int ivsize = crypto_ablkcipher_ivsize(ablkcipher);
@@ -1854,7 +1847,7 @@ static struct ablkcipher_edesc *ablkcipher_giv_edesc_alloc(
 					 DMA_FROM_DEVICE, dst_chained);
 	}
 
-	iv_dma = dma_map_single(dev, greq->giv, ivsize, DMA_TO_DEVICE);
+	iv_dma = dma_map_single(dev, greq->giv, ivsize, DMA_FROM_DEVICE);
 	if (dma_mapping_error(dev, iv_dma)) {
 		dev_err(dev, "unable to map IV\n");
 		return ERR_PTR(-ENOMEM);
@@ -1933,7 +1926,7 @@ static void caam_unmap(struct device *dev, struct scatterlist *src,
 		       struct scatterlist *dst, int src_nents,
 		       bool src_chained, int dst_nents, bool dst_chained,
 		       dma_addr_t iv_dma, int ivsize, dma_addr_t qm_sg_dma,
-		       int qm_sg_bytes)
+		       int qm_sg_bytes, enum optype op_type)
 {
 	if (dst != src) {
 		dma_unmap_sg_chained(dev, src, src_nents ? : 1, DMA_TO_DEVICE,
@@ -1946,7 +1939,10 @@ static void caam_unmap(struct device *dev, struct scatterlist *src,
 	}
 
 	if (iv_dma)
-		dma_unmap_single(dev, iv_dma, ivsize, DMA_TO_DEVICE);
+		dma_unmap_single(dev, iv_dma, ivsize,
+				 op_type == GIVENCRYPT ? DMA_FROM_DEVICE :
+							 DMA_TO_DEVICE);
+
 	if (qm_sg_bytes)
 		dma_unmap_single(dev, qm_sg_dma, qm_sg_bytes, DMA_TO_DEVICE);
 }
@@ -1956,6 +1952,7 @@ static void aead_unmap(struct device *dev, struct aead_edesc *edesc,
 {
 	struct crypto_aead *aead = crypto_aead_reqtfm(req);
 	int ivsize = crypto_aead_ivsize(aead);
+	struct caam_request *caam_req = aead_request_ctx(req);
 
 	dma_unmap_sg_chained(dev, req->assoc, edesc->assoc_nents,
 			     DMA_TO_DEVICE, edesc->assoc_chained);
@@ -1963,7 +1960,7 @@ static void aead_unmap(struct device *dev, struct aead_edesc *edesc,
 	caam_unmap(dev, req->src, req->dst,
 		   edesc->src_nents, edesc->src_chained, edesc->dst_nents,
 		   edesc->dst_chained, edesc->iv_dma, ivsize,
-		   edesc->qm_sg_dma, edesc->qm_sg_bytes);
+		   edesc->qm_sg_dma, edesc->qm_sg_bytes, caam_req->op_type);
 }
 
 static void ablkcipher_unmap(struct device *dev,
@@ -1972,11 +1969,12 @@ static void ablkcipher_unmap(struct device *dev,
 {
 	struct crypto_ablkcipher *ablkcipher = crypto_ablkcipher_reqtfm(req);
 	int ivsize = crypto_ablkcipher_ivsize(ablkcipher);
+	struct caam_request *caam_req = ablkcipher_request_ctx(req);
 
 	caam_unmap(dev, req->src, req->dst,
 		   edesc->src_nents, edesc->src_chained, edesc->dst_nents,
 		   edesc->dst_chained, edesc->iv_dma, ivsize,
-		   edesc->qm_sg_dma, edesc->qm_sg_bytes);
+		   edesc->qm_sg_dma, edesc->qm_sg_bytes, caam_req->op_type);
 }
 
 static void aead_encrypt_done(void *cbk_ctx, u32 err)
@@ -2087,6 +2085,7 @@ static int aead_encrypt(struct aead_request *req)
 			 req->cryptlen);
 	caam_req->flc = &ctx->flc[ENCRYPT];
 	caam_req->flc_dma = ctx->flc_dma[ENCRYPT];
+	caam_req->op_type = ENCRYPT;
 	caam_req->cbk = aead_encrypt_done;
 	caam_req->ctx = &req->base;
 	caam_req->edesc = edesc;
@@ -2121,6 +2120,7 @@ static int aead_givencrypt(struct aead_givcrypt_request *areq)
 			 req->cryptlen);
 	caam_req->flc = &ctx->flc[GIVENCRYPT];
 	caam_req->flc_dma = ctx->flc_dma[GIVENCRYPT];
+	caam_req->op_type = GIVENCRYPT;
 	caam_req->cbk = aead_encrypt_done;
 	caam_req->ctx = &req->base;
 	caam_req->edesc = edesc;
@@ -2153,6 +2153,7 @@ static int aead_decrypt(struct aead_request *req)
 			 req->cryptlen);
 	caam_req->flc = &ctx->flc[DECRYPT];
 	caam_req->flc_dma = ctx->flc_dma[DECRYPT];
+	caam_req->op_type = DECRYPT;
 	caam_req->cbk = aead_decrypt_done;
 	caam_req->ctx = &req->base;
 	caam_req->edesc = edesc;
@@ -2248,6 +2249,7 @@ static int ablkcipher_encrypt(struct ablkcipher_request *req)
 	dpaa2_fl_set_len(&caam_req->fd_flt[1], req->nbytes + ivsize);
 	caam_req->flc = &ctx->flc[ENCRYPT];
 	caam_req->flc_dma = ctx->flc_dma[ENCRYPT];
+	caam_req->op_type = ENCRYPT;
 	caam_req->cbk = ablkcipher_encrypt_done;
 	caam_req->ctx = &req->base;
 	caam_req->edesc = edesc;
@@ -2280,6 +2282,7 @@ static int ablkcipher_givencrypt(struct skcipher_givcrypt_request *greq)
 	dpaa2_fl_set_len(&caam_req->fd_flt[1], req->nbytes);
 	caam_req->flc = &ctx->flc[GIVENCRYPT];
 	caam_req->flc_dma = ctx->flc_dma[GIVENCRYPT];
+	caam_req->op_type = GIVENCRYPT;
 	caam_req->cbk = ablkcipher_encrypt_done;
 	caam_req->ctx = &req->base;
 	caam_req->edesc = edesc;
@@ -2311,6 +2314,7 @@ static int ablkcipher_decrypt(struct ablkcipher_request *req)
 	dpaa2_fl_set_len(&caam_req->fd_flt[1], req->nbytes + ivsize);
 	caam_req->flc = &ctx->flc[DECRYPT];
 	caam_req->flc_dma = ctx->flc_dma[DECRYPT];
+	caam_req->op_type = DECRYPT;
 	caam_req->cbk = ablkcipher_decrypt_done;
 	caam_req->ctx = &req->base;
 	caam_req->edesc = edesc;
diff --git a/drivers/crypto/dpaa2-caam/dpaa2-caam.h b/drivers/crypto/dpaa2-caam/dpaa2-caam.h
index b8ab040..4e2c5b3 100644
--- a/drivers/crypto/dpaa2-caam/dpaa2-caam.h
+++ b/drivers/crypto/dpaa2-caam/dpaa2-caam.h
@@ -198,6 +198,13 @@ struct caam_flc {
 	u32 sh_desc[MAX_SDLEN];
 } ____cacheline_aligned;
 
+enum optype {
+	ENCRYPT = 0,
+	DECRYPT,
+	GIVENCRYPT,
+	NUM_OP
+};
+
 /**
  * caam_request - the request structure the driver application should fill while
  *                submitting a job to driver.
@@ -207,6 +214,7 @@ struct caam_flc {
  * @fd_flt_dma: DMA address for the frame list table
  * @flc: Flow Context
  * @flc_dma: DMA address of Flow Context
+ * @op_type: operation type
  * @cbk: Callback function to invoke when job is completed
  * @ctx: arbit context attached with request by the application
  * @edesc: extended descriptor; points to one of {ablkcipher,ahash,aead}_edesc
@@ -216,6 +224,7 @@ struct caam_request {
 	dma_addr_t fd_flt_dma;
 	struct caam_flc *flc;
 	dma_addr_t flc_dma;
+	enum optype op_type;
 	void (*cbk)(void *ctx, u32 err);
 	void *ctx;
 	void *edesc;
-- 
1.7.5.4

