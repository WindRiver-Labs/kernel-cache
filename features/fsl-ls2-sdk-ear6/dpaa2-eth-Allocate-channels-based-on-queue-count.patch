From b1b50b603e00386598cc3ac7eb74b710f509a6d5 Mon Sep 17 00:00:00 2001
From: Ioana Radulescu <ruxandra.radulescu@freescale.com>
Date: Thu, 16 Jul 2015 19:50:15 +0300
Subject: [PATCH 240/452] dpaa2-eth: Allocate channels based on queue count

Limit the number of channels allocated per DPNI to the maximum
between the number of Rx queues per traffic class (distribution size)
and Tx confirmation queues (number of tx flows).
If this happens to be larger than the number of available cores, only
allocate one channel for each core and distribute the frame queues on
the cores/channels in a round robin fashion.

Signed-off-by: Ioana Radulescu <ruxandra.radulescu@freescale.com>
Signed-off-by: Bogdan Hamciuc <bogdan.hamciuc@freescale.com>
[Xulin: Original patch taken from FSL LS2085 SDK EAR6.0,
LS2085A-SDK-SOURCE-20160304-yocto.iso]
Signed-off-by: Xulin Sun <xulin.sun@windriver.com>
---
 .../staging/fsl-dpaa2/ethernet/dpaa2-eth-debugfs.c |    4 +-
 drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth.c     |   76 +++++++++++++++-----
 drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth.h     |   14 +++-
 3 files changed, 71 insertions(+), 23 deletions(-)

diff --git a/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth-debugfs.c b/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth-debugfs.c
index 693b50c..8b98f19 100644
--- a/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth-debugfs.c
+++ b/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth-debugfs.c
@@ -162,11 +162,11 @@ static int ldpaa_dbg_ch_show(struct seq_file *file, void *offset)
 		   "CHID", "CPU", "Deq busy", "Frames", "CDANs",
 		   "Avg frm/CDAN");
 
-	for_each_cpu(i, &priv->dpio_cpumask) {
+	for (i = 0; i < priv->num_channels; i++) {
 		ch = priv->channel[i];
 		seq_printf(file, "%4d%16d%16llu%16llu%16llu%16llu\n",
 			   ch->ch_id,
-			   i,
+			   ch->nctx.desired_cpu,
 			   ch->stats.dequeue_portal_busy,
 			   ch->stats.frames,
 			   ch->stats.cdan,
diff --git a/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth.c b/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth.c
index 2cb6d84..b016b17 100644
--- a/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth.c
+++ b/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth.c
@@ -832,7 +832,7 @@ static void ldpaa_eth_napi_enable(struct ldpaa_eth_priv *priv)
 	struct ldpaa_eth_channel *ch;
 	int i;
 
-	for_each_cpu(i, &priv->dpio_cpumask) {
+	for (i = 0; i < priv->num_channels; i++) {
 		ch = priv->channel[i];
 		napi_enable(&ch->napi);
 	}
@@ -843,7 +843,7 @@ static void ldpaa_eth_napi_disable(struct ldpaa_eth_priv *priv)
 	struct ldpaa_eth_channel *ch;
 	int i;
 
-	for_each_cpu(i, &priv->dpio_cpumask) {
+	for (i = 0; i < priv->num_channels; i++) {
 		ch = priv->channel[i];
 		napi_disable(&ch->napi);
 	}
@@ -1222,7 +1222,7 @@ static void ldpaa_eth_setup_fqs(struct ldpaa_eth_priv *priv)
 	/* We have one TxConf FQ per target CPU, although at the moment
 	 * we can't guarantee affinity.
 	 */
-	for_each_online_cpu(i) {
+	for_each_cpu(i, &priv->txconf_cpumask) {
 		priv->fq[priv->num_fqs].netdev_priv = priv;
 		priv->fq[priv->num_fqs].type = LDPAA_TX_CONF_FQ;
 		priv->fq[priv->num_fqs].consume = ldpaa_eth_tx_conf;
@@ -1344,17 +1344,22 @@ static int __cold ldpaa_dpio_setup(struct ldpaa_eth_priv *priv)
 	struct ldpaa_eth_channel *channel;
 	struct dpcon_notification_cfg dpcon_notif_cfg;
 	struct device *dev = priv->net_dev->dev.parent;
-	int err, i;
+	int i, err;
 
+	/* Don't allocate more channels than strictly necessary and assign
+	 * them to cores starting from the first one available in
+	 * cpu_online_mask.
+	 * If the number of channels is lower than the number of cores,
+	 * there will be no rx/tx conf processing on the last cores in the mask.
+	 */
 	cpumask_clear(&priv->dpio_cpumask);
-
 	for_each_online_cpu(i) {
-		/* Allocate a channel for each core */
+		/* Try to allocate a channel */
 		channel = ldpaa_alloc_channel(priv, i);
 		if (unlikely(!channel))
 			goto err_alloc_ch;
 
-		priv->channel[i] = channel;
+		priv->channel[priv->num_channels] = channel;
 
 		nctx = &channel->nctx;
 		nctx->is_cdan = 1;
@@ -1365,8 +1370,12 @@ static int __cold ldpaa_dpio_setup(struct ldpaa_eth_priv *priv)
 		/* Register the new context */
 		err = dpaa_io_service_register(NULL, nctx);
 		if (unlikely(err)) {
-			dev_err(dev, "Could not get affine DPIO\n");
-			goto err_service_reg;
+			dev_info(dev, "No affine DPIO for core %d\n", i);
+			/* This core doesn't have an affine DPIO, but there's
+			 * a chance another one does, so keep trying
+			 */
+			ldpaa_free_channel(priv, channel);
+			continue;
 		}
 
 		/* Register DPCON notification with MC */
@@ -1381,20 +1390,32 @@ static int __cold ldpaa_dpio_setup(struct ldpaa_eth_priv *priv)
 			goto err_set_cdan;
 		}
 
+		/* If we managed to allocate a channel and also found an affine
+		 * DPIO for this core, add it to the final mask
+		 */
 		cpumask_set_cpu(i, &priv->dpio_cpumask);
+		priv->num_channels++;
+
+		if (priv->num_channels == ldpaa_max_channels(priv))
+			break;
 	}
 
+	/* Tx confirmation queues can only be serviced by cpus
+	 * with an affine DPIO/channel
+	 */
+	cpumask_copy(&priv->txconf_cpumask, &priv->dpio_cpumask);
+
 	return 0;
 
 err_set_cdan:
 	dpaa_io_service_deregister(NULL, nctx);
-err_service_reg:
 	ldpaa_free_channel(priv, channel);
 err_alloc_ch:
 	if (unlikely(cpumask_empty(&priv->dpio_cpumask))) {
 		dev_err(dev, "No cpu with an affine DPIO/DPCON\n");
 		return -ENODEV;
 	}
+	cpumask_copy(&priv->txconf_cpumask, &priv->dpio_cpumask);
 
 	return 0;
 }
@@ -1405,13 +1426,31 @@ static void __cold ldpaa_dpio_free(struct ldpaa_eth_priv *priv)
 	struct ldpaa_eth_channel *ch;
 
 	/* deregister CDAN notifications and free channels */
-	for_each_cpu(i, &priv->dpio_cpumask) {
+	for (i = 0; i < priv->num_channels; i++) {
 		ch = priv->channel[i];
 		dpaa_io_service_deregister(NULL, &ch->nctx);
 		ldpaa_free_channel(priv, ch);
 	}
 }
 
+static struct ldpaa_eth_channel *
+ldpaa_get_channel_by_cpu(struct ldpaa_eth_priv *priv, int cpu)
+{
+	struct device *dev = priv->net_dev->dev.parent;
+	int i;
+
+	for (i = 0; i < priv->num_channels; i++)
+		if (priv->channel[i]->nctx.desired_cpu == cpu)
+			return priv->channel[i];
+
+	/* We should never get here. Issue a warning and return
+	 * the first channel, because it's still better than nothing
+	 */
+	dev_warn(dev, "No affine channel found for cpu %d\n", cpu);
+
+	return priv->channel[0];
+}
+
 static void ldpaa_set_fq_affinity(struct ldpaa_eth_priv *priv)
 {
 	struct ldpaa_eth_fq *fq;
@@ -1441,7 +1480,7 @@ static void ldpaa_set_fq_affinity(struct ldpaa_eth_priv *priv)
 			netdev_err(priv->net_dev, "Unknown FQ type: %d\n",
 				   fq->type);
 		}
-		fq->channel = priv->channel[fq->target_cpu];
+		fq->channel = ldpaa_get_channel_by_cpu(priv, fq->target_cpu);
 	}
 }
 
@@ -1934,7 +1973,7 @@ static int ldpaa_eth_alloc_rings(struct ldpaa_eth_priv *priv)
 	struct device *dev = net_dev->dev.parent;
 	int i;
 
-	for_each_cpu(i, &priv->dpio_cpumask) {
+	for (i = 0; i < priv->num_channels; i++) {
 		priv->channel[i]->store =
 			dpaa_io_store_create(LDPAA_ETH_STORE_SIZE, dev);
 		if (unlikely(!priv->channel[i]->store)) {
@@ -1946,7 +1985,7 @@ static int ldpaa_eth_alloc_rings(struct ldpaa_eth_priv *priv)
 	return 0;
 
 err_ring:
-	for_each_cpu(i, &priv->dpio_cpumask) {
+	for (i = 0; i < priv->num_channels; i++) {
 		if (!priv->channel[i]->store)
 			break;
 		dpaa_io_store_destroy(priv->channel[i]->store);
@@ -1959,7 +1998,7 @@ static void ldpaa_eth_free_rings(struct ldpaa_eth_priv *priv)
 {
 	int i;
 
-	for_each_cpu(i, &priv->dpio_cpumask)
+	for (i = 0; i < priv->num_channels; i++)
 		dpaa_io_store_destroy(priv->channel[i]->store);
 }
 
@@ -2156,7 +2195,7 @@ static void ldpaa_eth_napi_add(struct ldpaa_eth_priv *priv)
 	int i;
 	struct ldpaa_eth_channel *ch;
 
-	for_each_cpu(i, &priv->dpio_cpumask) {
+	for (i = 0; i < priv->num_channels; i++) {
 		ch = priv->channel[i];
 		netif_napi_add(priv->net_dev, &ch->napi, ldpaa_eth_poll,
 			       LDPAA_ETH_NAPI_WEIGHT);
@@ -2168,7 +2207,7 @@ static void ldpaa_eth_napi_del(struct ldpaa_eth_priv *priv)
 	int i;
 	struct ldpaa_eth_channel *ch;
 
-	for_each_cpu(i, &priv->dpio_cpumask) {
+	for (i = 0; i < priv->num_channels; i++) {
 		ch = priv->channel[i];
 		netif_napi_del(&ch->napi);
 	}
@@ -2376,8 +2415,7 @@ ldpaa_eth_probe(struct fsl_mc_device *dpni_dev)
 	if (err)
 		goto err_dpio_setup;
 
-	/* FQs and NAPI */
-	cpumask_copy(&priv->txconf_cpumask, &priv->dpio_cpumask);
+	/* FQs */
 	ldpaa_eth_setup_fqs(priv);
 	ldpaa_set_fq_affinity(priv);
 
diff --git a/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth.h b/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth.h
index 5f3360c..a3af9c30 100644
--- a/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth.h
+++ b/drivers/staging/fsl-dpaa2/ethernet/dpaa2-eth.h
@@ -210,8 +210,7 @@ struct ldpaa_eth_ch_stats {
 				LDPAA_ETH_MAX_TX_QUEUES + \
 				LDPAA_ETH_MAX_RX_ERR_QUEUES)
 
-/* FIXME */
-#define LDPAA_ETH_MAX_DPCONS		8
+#define LDPAA_ETH_MAX_DPCONS		NR_CPUS
 
 enum ldpaa_eth_fq_type {
 	LDPAA_RX_FQ = 0,
@@ -259,6 +258,7 @@ struct ldpaa_eth_priv {
 	/* First queue is tx conf, the rest are rx */
 	struct ldpaa_eth_fq fq[LDPAA_ETH_MAX_QUEUES];
 
+	uint8_t num_channels;
 	struct ldpaa_eth_channel *channel[LDPAA_ETH_MAX_DPCONS];
 
 	int dpni_id;
@@ -339,6 +339,16 @@ static inline int ldpaa_queue_count(struct ldpaa_eth_priv *priv)
 	return 1;
 }
 
+static inline int ldpaa_max_channels(struct ldpaa_eth_priv *priv)
+{
+	/* Ideally, we want a number of channels large enough
+	 * to accommodate both the Rx distribution size
+	 * and the max number of Tx confirmation queues
+	 */
+	return max_t(int, ldpaa_queue_count(priv),
+		     priv->dpni_attrs.max_senders);
+}
+
 void ldpaa_cls_check(struct net_device *);
 
 #endif	/* __LDPAA_H */
-- 
1.7.5.4

