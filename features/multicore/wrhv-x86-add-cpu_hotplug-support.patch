From df3f59404d3a98d84638f2ee17fdcfa9e298040e Mon Sep 17 00:00:00 2001
From: Liang Li <liang.li@windriver.com>
Date: Thu, 1 Jul 2010 19:58:53 +0800
Subject: [PATCH 3/7] wrhv/x86: add cpu_hotplug support

Enable cpu hotplug feature in Guest OS

Signed-off-by: Liang Li <liang.li@windriver.com>
---
 arch/x86/kernel/vbi/wrhv.c |   88 ++++++++++++++++++++++++++++++++++++++------
 1 files changed, 76 insertions(+), 12 deletions(-)

diff --git a/arch/x86/kernel/vbi/wrhv.c b/arch/x86/kernel/vbi/wrhv.c
index f64b0fe..7d4a308 100644
--- a/arch/x86/kernel/vbi/wrhv.c
+++ b/arch/x86/kernel/vbi/wrhv.c
@@ -75,7 +75,7 @@ static unsigned long is_vtlb_ops_cache_enabled[NR_CPUS];
 static cpumask_t flush_cpumask;
 static struct mm_struct *flush_mm;
 static unsigned long flush_va;
-static DEFINE_SPINLOCK(tlbstate_lock);
+static DEFINE_RAW_SPINLOCK(tlbstate_lock);
 static DEFINE_SPINLOCK(vioapic_lock);
 #endif
 
@@ -313,6 +313,7 @@ static int wrhv_set_next_event(unsigned long delta,
 
 static void wrhv_timer_broadcast(const struct cpumask *mask)
 {
+	cpus_and(*mask, cpu_online_map, *mask);
 	wrhv_send_IPI_mask(DUMMY_TIMER_INT, *mask);
 }
 
@@ -839,10 +840,12 @@ static void wrhv_flush_tlb_others(const cpumask_t *cpumaskp, struct mm_struct *m
 	 * MM hot path, but we'll see how contended it is.
 	 * AK: x86-64 has a faster method that could be ported.
 	 */
-	spin_lock(&tlbstate_lock);
+	raw_spin_lock(&tlbstate_lock);
 
 	flush_mm = mm;
 	flush_va = va;
+	cpus_and(cpumask, cpu_online_map, cpumask);
+	cpu_clear(smp_processor_id(), cpumask);
 	cpus_or(flush_cpumask, cpumask, flush_cpumask);
 	/*
 	 * We have to send the IPI only to
@@ -853,10 +856,9 @@ static void wrhv_flush_tlb_others(const cpumask_t *cpumaskp, struct mm_struct *m
 	while (!cpus_empty(flush_cpumask))
 		/* nothing. lockup detection does not belong here */
 		cpu_relax();
-
 	flush_mm = NULL;
 	flush_va = 0;
-	spin_unlock(&tlbstate_lock);
+	raw_spin_unlock(&tlbstate_lock);
 }
 #endif
 
@@ -970,12 +972,6 @@ static void wrhv_init_mm(void)
 #endif
 }
 
-int __init wrhv_late_init(void)
-{
-	return 0;
-}
-late_initcall(wrhv_late_init);
-
 static inline void wrhv_umask_timer_for_vcore(int irq)
 {
 	/*
@@ -1041,6 +1037,17 @@ void __init wrhv_calibrate_smp_cpus(void)
 	}
 }
 
+static void inline wrhv_mask_IPIs_for_vcore(void)
+{
+	/* mask ipi interrupt for vcore */
+	vbi_mask_vioapic_irq(WRHV_IPI_RESCHED);
+	vbi_mask_vioapic_irq(WRHV_IPI_INV_TLB);
+	vbi_mask_vioapic_irq(WRHV_IPI_FUNC_CALL);
+	vbi_mask_vioapic_irq(WRHV_IPI_FUNC_CALL_SINGLE);
+	if (enable_hrtimer)
+		vbi_mask_vioapic_irq(DUMMY_TIMER_INT);
+}
+
 static void inline wrhv_umask_IPIs_for_vcore(void)
 {
 	/* unmask ipi interrupt for vcore */
@@ -1342,6 +1349,7 @@ static int __cpuinit wrhv_cpu_up(unsigned int cpu)
 	int err;
 	unsigned long flags;
 
+	preempt_disable();
 	if (!physid_isset(cpu, phys_cpu_present_map)) {
 		printk(KERN_ERR "%s: bad cpu %d\n", __func__, cpu);
 		printk(KERN_ERR "phys_cpu_present_map: %x \n",
@@ -1381,6 +1389,7 @@ static int __cpuinit wrhv_cpu_up(unsigned int cpu)
 	check_tsc_sync_source(cpu);
 	local_irq_restore(flags);
 
+	preempt_enable();
 	while (!cpu_online(cpu))
 		cpu_relax();
 
@@ -1394,8 +1403,9 @@ static void __init wrhv_smp_cpus_done(unsigned int max_cpus)
 	return;
 }
 
-static void stop_me(void * t)
+static void wrhv_stop_me(void * t)
 {
+	printk(KERN_INFO "wrhv_stop_me.\n");
 	write_cr3(__pa(swapper_pg_dir));
 
 	/* Enter into infinite loop to stop self*/
@@ -1404,7 +1414,7 @@ static void stop_me(void * t)
 
 static void wrhv_smp_send_stop(void)
 {
-	smp_call_function(stop_me, NULL, 0);
+	smp_call_function(wrhv_stop_me, NULL, 0);
 	return;
 }
 
@@ -1423,10 +1433,64 @@ static void wrhv_smp_send_call_func_ipi(const cpumask_t *mask)
 	wrhv_send_IPI_mask(WRHV_IPI_FUNC_CALL, *mask);
 }
 
+static int wrhv_cpu_disable(void)
+{
+	unsigned int cpu = smp_processor_id();
+
+	printk(KERN_INFO "In wrhv_cpu_disable. cpu = %d\n", cpu);
+
+	if (cpu == 0)
+		return -EBUSY;
+
+	cpu_disable_common();
+
+	return 0;
+}
+
+static void wrhv_cpu_play_dead(void)
+{
+	int ret;
+
+	printk(KERN_INFO "wrhv_cpu_play_dead.\n");
+
+	play_dead_common();
+
+	load_cr3(swapper_pg_dir);
+
+	wrhv_mask_IPIs_for_vcore();
+	cpu_clear(smp_processor_id(), flush_cpumask);
+
+	/*
+	 * The below vbi call will actually reset the *self* core
+	 * so this vbi call is expected to not return as a normal
+	 * funtion call hence the rest codes after this vbi call
+	 * won't being executed in normal case
+	 */
+	ret = vbi_vb_reset(VBI_BOARD_ID_GET(), smp_processor_id(),
+		VBI_VBMGMT_RESET_DOWNLOAD |
+		VBI_VBMGMT_RESET_CLEAR
+		);
+
+	printk(KERN_ERR "wrhv_cpu_play_dead: reset self failed."
+			"ret = %d\n", ret);
+	/*
+	 * Normally we won't reach here, so lives a BUG()
+	 * here to produce verbose output to indicate that
+	 * something wrong happened
+	 */
+	BUG();
+	while (1);
+}
+
 static struct smp_ops wrhv_smp_ops __initdata = {
 	.smp_prepare_boot_cpu = wrhv_smp_prepare_boot_cpu,
 	.smp_prepare_cpus = wrhv_smp_prepare_cpus,
+
 	.cpu_up = wrhv_cpu_up,
+	.cpu_die = native_cpu_die,
+	.cpu_disable = wrhv_cpu_disable,
+	.play_dead = wrhv_cpu_play_dead,
+
 	.smp_cpus_done = wrhv_smp_cpus_done,
 
 	.smp_send_stop = wrhv_smp_send_stop,
-- 
1.6.5.2

