From d697405e0748f6b20d512fdb8cdb5b16580ce7d9 Mon Sep 17 00:00:00 2001
From: Kevin Hao <kexin.hao@windriver.com>
Date: Mon, 31 May 2010 16:28:08 +0800
Subject: [PATCH] wrhv/powerpc: enable SPE support for E500 core

Add SPE support for E500 core. Currently in hypervisor the
debug exception is defined by default. So we have to add a
dummy handler in linux kernel in order to create the correct
interrupt vector offset as the hypervisor.

When support SPE we introduce a dummy handler in linux kernel in order
to create the correct SPE interrupt vector offset as the hypervisor. But
actually that should be used for debug exception is defined by guest OS.

Now fix that to be dedicated to DEBUG exception.

Signed-off-by: Tiejun Chen <tiejun.chen@windriver.com>
---
 arch/powerpc/kernel/head_wrhv.S     |  110 +++++++++++++++++++++++++----------
 arch/powerpc/kernel/process.c       |    2 +-
 arch/powerpc/kernel/wrhv_entry_32.S |   17 +++++
 3 files changed, 98 insertions(+), 31 deletions(-)

diff --git a/arch/powerpc/kernel/head_wrhv.S b/arch/powerpc/kernel/head_wrhv.S
index 913e0ab..563dba5 100644
--- a/arch/powerpc/kernel/head_wrhv.S
+++ b/arch/powerpc/kernel/head_wrhv.S
@@ -428,11 +428,13 @@ interrupt_base:
         START_EXCEPTION(InstructionTLBError)
         b       InstructionStorage
 
+	/* Debug Interrupt */
+	DEBUG_DEBUG_EXCEPTION
 #ifdef CONFIG_SPE
 	/* SPE Unavailable */
 	START_EXCEPTION(SPEUnavailable)
 	NORMAL_EXCEPTION_PROLOG
-	bne	load_up_spe
+	beq	load_up_spe
 	addi	r3,r1,STACK_FRAME_OVERHEAD
 	EXC_XFER_EE_LITE(0x2010, KernelSPE)
 #else
@@ -442,7 +444,6 @@ interrupt_base:
 	/* SPE Floating Point Data */
 #ifdef CONFIG_SPE
 	EXCEPTION(0x2030, SPEFloatingPointData, SPEFloatingPointException, EXC_XFER_EE);
-
 	/* SPE Floating Point Round */
 	EXCEPTION(0x2050, SPEFloatingPointRound, SPEFloatingPointRoundException, EXC_XFER_EE)
 #else
@@ -457,8 +458,7 @@ interrupt_base:
 
 	CRITICAL_EXCEPTION(0x2080, CriticalDoorbell, unknown_exception)
 
-	/* Debug Interrupt */
-	DEBUG_DEBUG_EXCEPTION
+	/* Debug Crit Interrupt */
 	DEBUG_CRIT_EXCEPTION
 
 /*
@@ -562,10 +562,20 @@ load_up_spe:
  * On SMP we know the SPE units are free, since we give it up every
  * switch.  -- Kumar
  */
-	mfmsr	r5
-	oris	r5,r5,MSR_SPE@h
-	mtmsr	r5			/* enable use of SPE now */
-	isync
+	stw	r2,GPR2(r11)
+	stw	r12,_NIP(r11)
+	stw	r9,_MSR(r11)
+	mfctr	r12
+	mfspr	r2,SPRN_XER
+	stw	r12,_CTR(r11)
+	stw	r2,_XER(r11)
+
+	li r3, 1
+	li r4,(SPEFSCR_FINVE | SPEFSCR_FDBZE | SPEFSCR_FUNFE | SPEFSCR_FOVFE)
+	lis r0,VBI_SYS_spefscr_update@h
+	ori r0,r0,VBI_SYS_spefscr_update@l
+	sc
+
 /*
  * For SMP, we don't do lazy SPE switching because it just gets too
  * horrendously complex, especially when a task switches from one CPU
@@ -589,9 +599,7 @@ load_up_spe:
 	stw	r4,_MSR-STACK_FRAME_OVERHEAD(r5)
 1:
 #endif /* !CONFIG_SMP */
-	/* enable use of SPE after return */
-	oris	r9,r9,MSR_SPE@h
-	mfspr	r5,SPRN_SPRG_THREAD	/* current task's THREAD (phys) */
+	WRHV_MFSPRG3(r5)		/* current task's THREAD (phys) */
 	li	r4,1
 	li	r10,THREAD_ACC
 	stw	r4,THREAD_USED_SPE(r5)
@@ -602,20 +610,48 @@ load_up_spe:
 	subi	r4,r5,THREAD
 	stw	r4,last_task_used_spe@l(r3)
 #endif /* !CONFIG_SMP */
+
 	/* restore registers and return */
-2:	REST_4GPRS(3, r11)
-	lwz	r10,_CCR(r11)
-	REST_GPR(1, r11)
-	mtcr	r10
-	lwz	r10,_LINK(r11)
-	mtlr	r10
-	REST_GPR(10, r11)
-	mtspr	SPRN_SRR1,r9
-	mtspr	SPRN_SRR0,r12
-	REST_GPR(9, r11)
-	REST_GPR(12, r11)
-	lwz	r11,GPR11(r11)
-	rfi
+	lis	r4,wr_control@ha
+	lwz	r4,wr_control@l(r4)
+	lwz	r0,GPR0(r1)
+	stw	r0,VB_CONTROL_R0(r4)
+	lwz	r2,GPR2(r1)
+	lwz	r3,GPR3(r1)
+	lwz	r6,GPR6(r1)
+	lwz	r7,GPR7(r1)
+	lwz	r8,GPR8(r1)
+	lwz	r9,GPR9(r1)
+	lwz	r10,GPR10(r1)
+	lwz	r11,GPR11(r1)
+
+	lis	r12,wr_status@ha
+	lwz	r12,wr_status@l(r12)
+	lwz	r5,VB_STATUS_OLD_INT_DISABLE(r12)
+	stw	r5,VB_CONTROL_NEW_INT_DISABLE(r4)
+
+	lwz	r0,_CCR(r1)
+	stw	r0,VB_CONTROL_CR(r4)
+	lwz	r0,_NIP(r1)
+	stw	r0,VB_CONTROL_SRR0(r4)
+	lwz	r0,_LINK(r1)
+	mtlr	r0
+	lwz	r0,_XER(r1)
+	mtspr	SPRN_XER,r0
+	lwz	r0,_CTR(r1)
+	mtctr	r0
+	lwz	r0,_MSR(r1)
+	/* enable use of SPE after return */
+	oris	r0,r0,MSR_SPE@h
+	WRHV_LOAD_MSR(r0,r12,r5)
+	lwz	r12,GPR12(r1)
+	lwz	r5,GPR5(r1)
+
+	lwz	r4,GPR4(r1)
+	lwz	r1,GPR1(r1)
+	lis	r0,VBI_SYS_ctx_load@h
+	ori	r0,r0,VBI_SYS_ctx_load@l
+	sc
 
 /*
  * SPE unavailable trap from kernel - print a message, but let
@@ -652,16 +688,27 @@ KernelSPE:
 _GLOBAL(giveup_altivec)
 	blr
 
-#ifdef CONFIG_SPE
+#if defined(CONFIG_SPE) && defined(CONFIG_WRHV)
 /*
  * extern void giveup_spe(struct task_struct *prev)
  *
  */
 _GLOBAL(giveup_spe)
-	mfmsr	r5
-	oris	r5,r5,MSR_SPE@h
-	mtmsr	r5			/* enable use of SPE now */
-	isync
+	stwu	r1,-INT_FRAME_SIZE(r1)
+	mflr	r0
+	stw	r0,INT_FRAME_SIZE+4(r1)
+	SAVE_GPR(3,r1)
+
+	li	r3, 1			/* enable SPE */
+	li	r4,(SPEFSCR_FINVE | SPEFSCR_FDBZE | SPEFSCR_FUNFE | \
+			SPEFSCR_FOVFE)
+	lis	r0,VBI_SYS_spefscr_update@h
+	ori	r0,r0,VBI_SYS_spefscr_update@l
+	sc
+
+	REST_GPR(3,r1)
+	addi	r1,r1,INT_FRAME_SIZE
+
 	cmpi	0,r3,0
 	beqlr-				/* if no previous owner, done */
 	addi	r3,r3,THREAD		/* want THREAD of task */
@@ -672,7 +719,10 @@ _GLOBAL(giveup_spe)
 	evmwumiaa evr6, evr6, evr6	/* evr6 <- ACC = 0 * 0 + ACC */
 	li	r4,THREAD_ACC
 	evstddx	evr6, r4, r3		/* save off accumulator */
-	mfspr	r6,SPRN_SPEFSCR
+
+	lis	r6,wr_status@ha
+	ori	r6,r6,wr_status@l
+	lwz	r6,VB_STATUS_SPEFSCR(r6)
 	stw	r6,THREAD_SPEFSCR(r3)	/* save spefscr register value */
 	beq	1f
 	lwz	r4,_MSR-STACK_FRAME_OVERHEAD(r5)
diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 0d4c2f7..7944551 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -192,7 +192,7 @@ void flush_vsx_to_thread(struct task_struct *tsk)
 
 #ifdef CONFIG_SPE
 
-#ifdef CONFIG_PARAVIRT
+#if defined(CONFIG_PARAVIRT) && !defined(CONFIG_WRHV)
 /* refer to native implementation in
  * linux/arch/powerpc/kernel/head_fsl_booke.S
  */
diff --git a/arch/powerpc/kernel/wrhv_entry_32.S b/arch/powerpc/kernel/wrhv_entry_32.S
index 7a00e7d..71785ee 100644
--- a/arch/powerpc/kernel/wrhv_entry_32.S
+++ b/arch/powerpc/kernel/wrhv_entry_32.S
@@ -376,6 +376,14 @@ _GLOBAL(paravirt_switch)
 	stw	r0,_NIP(r1)	/* Return to switch caller */
 	LOAD_MSR_KERNEL(r11,MSR_KERNEL)
 	WRHV_FIX_MSR(r11,r10)
+#ifdef CONFIG_SPE
+BEGIN_FTR_SECTION
+	lis	r12,wr_status@ha
+	ori	r12,r12,wr_status@l
+	lwz	r12,VB_STATUS_SPEFSCR(r12)  /* save spefscr register value */
+	stw	r12,THREAD+THREAD_SPEFSCR(r2)
+END_FTR_SECTION_IFSET(CPU_FTR_SPE)
+#endif /* CONFIG_SPE */
 1:	stw	r11,_MSR(r1)
 	mfcr	r10
 	stw	r10,_CCR(r1)
@@ -398,6 +406,15 @@ _GLOBAL(paravirt_switch)
 	mr	r3,r2
 	addi	r2,r4,-THREAD	/* Update current */
 
+#ifdef CONFIG_SPE
+BEGIN_FTR_SECTION
+	lwz	r0,THREAD+THREAD_SPEFSCR(r2)
+	lis	r12,wr_control@h
+	ori	r12,r12,wr_control@l
+	stw	r0,VB_CONTROL_SPEFSCR(r12)	/* restore SPEFSCR reg */
+END_FTR_SECTION_IFSET(CPU_FTR_SPE)
+#endif /* CONFIG_SPE */
+
 	lwz	r0,_CCR(r1)
 	mtcrf	0xFF,r0
 	/* r3-r12 are destroyed -- Cort */
-- 
1.6.5.2

