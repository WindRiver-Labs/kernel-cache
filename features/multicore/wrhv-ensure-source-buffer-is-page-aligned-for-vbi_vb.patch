From 3103bda1dce443aebfa87ed202cefebd1dc7a38d Mon Sep 17 00:00:00 2001
From: Mark Asselstine <mark.asselstine@windriver.com>
Date: Fri, 8 Jul 2011 13:02:15 -0400
Subject: [PATCH] wrhv: ensure source buffer is page aligned for vbi_vb_write_mem

The hypervisor requires that the source buffer be at most
one page in size and must not span multiple pages, thus must
be page aligned.

Several other changes have been made to
streamline VBI mem writes as well. Splitting the source buffer
into page size chunks has been moved into the kernel, simplifying
userspace interactions with the syscall. This change also meant
allocating a static page size buffer was rather wasteful, so
it is allocated dynamically and cleaned up for each call.
Allocating on the stack using the 'aligned' attribute was not
done since this blows the frame size of 1024. Of course dynamic
allocation can result in an ENOMEM error for which the syscall
caller should handle this error condition.

Signed-off-by: Mark Asselstine <mark.asselstine@windriver.com>
---
 kernel/vbi/syscall_vbi.c |   33 +++++++++++++++++++++++----------
 1 files changed, 23 insertions(+), 10 deletions(-)

diff --git a/kernel/vbi/syscall_vbi.c b/kernel/vbi/syscall_vbi.c
index eba8282..0c435f8 100644
--- a/kernel/vbi/syscall_vbi.c
+++ b/kernel/vbi/syscall_vbi.c
@@ -113,16 +113,14 @@ asmlinkage long sys_vbi_activate_vb(uint32_t vb, uint32_t addr)
 	return retval;
 }
 
-#define VBI_MEM_BUF_LEN  4096
 asmlinkage long sys_vbi_mem(uint32_t vb, void *dest, void *src,
 				  uint32_t size, uint32_t flags)
 {
 	struct vbi_mem_ctl memCtl;
+	void *vbi_mem_buf;
 	uint32_t len;
 	long rv = OK;
 
-	static char vbi_mem_buf[VBI_MEM_BUF_LEN] = {};
-
 	if (!capable(CAP_SYS_ADMIN))
 		return -EPERM;
 
@@ -134,20 +132,33 @@ asmlinkage long sys_vbi_mem(uint32_t vb, void *dest, void *src,
 		return -EPERM;
 	}
 
-	if (size > VBI_MEM_BUF_LEN)
+	/* Must be one page in size and page aligned */
+	vbi_mem_buf = (void *)__get_free_page(GFP_KERNEL);
+	if (!vbi_mem_buf)
 		return -ENOMEM;
 
 	if (flags & VBI_MEM_WRITE) {
-		if ((copy_from_user(vbi_mem_buf, src, size) > 0))
-			rv = -EACCES;
-		else {
+		while (size > 0) {
+			len = size < PAGE_SIZE ? size : PAGE_SIZE;
+
+			if ((copy_from_user(vbi_mem_buf, src, len) > 0)) {
+				rv = -EACCES;
+				goto err;
+			}
+
 			memCtl.pBuffer = dest;
 			memCtl.pAddress = vbi_mem_buf;
-			memCtl.size_in = size;
+			memCtl.size_in = len;
 			memCtl.size_out = 0;
-			memCtl.flags = VBI_ICACHE_INV | VBI_DCACHE_FLUSH;
-			if ((vbi_vb_write_mem(&memCtl, vb) != OK))
+			memCtl.flags = VBI_ICACHE_INV|VBI_DCACHE_FLUSH;
+			if ((vbi_vb_write_mem(&memCtl, vb) != OK)) {
 				rv = -EACCES;
+				goto err;
+			}
+
+			size -= len;
+			src += len;
+			dest += len;
 		}
 	} else if (flags & VBI_MEM_READ) {
 		memCtl.pBuffer = src;
@@ -165,6 +176,8 @@ asmlinkage long sys_vbi_mem(uint32_t vb, void *dest, void *src,
 		rv = -EINVAL;
 	}
 
+err:
+	free_page((unsigned long)vbi_mem_buf);
 	return rv;
 }
 
-- 
1.7.4.1

