From 04b9dd26f3dae419ba6dd0cefec62b56a3b259d7 Mon Sep 17 00:00:00 2001
From: WRS Support <support@windriver.com>
Date: Fri, 2 Oct 2009 16:10:35 -0400
Subject: [PATCH] x86 wrhv: x86 specifics of the WR guest support

These x86 specific additions represent additions that are
not part of the reference VBI implementation.

Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
Signed-off-by: Bruce Ashfield <bruce.ashfield@windriver.com>
---
 arch/x86/kernel/vbi/wrhv.c        | 1281 +++++++++++++++++++++++++++++++++++++
 arch/x86/kernel/vbi/wrhv_initrd.S |    3 +
 include/asm-x86/wrhv.h            |   81 +++
 include/asm-x86/wrhv_serial.h     |   14 +
 4 files changed, 1379 insertions(+), 0 deletions(-)
 create mode 100644 arch/x86/kernel/vbi/wrhv.c
 create mode 100644 arch/x86/kernel/vbi/wrhv_initrd.S
 create mode 100644 include/asm-x86/wrhv.h
 create mode 100644 include/asm-x86/wrhv_serial.h

diff --git a/arch/x86/kernel/vbi/wrhv.c b/arch/x86/kernel/vbi/wrhv.c
new file mode 100644
index 0000000..432df57
--- /dev/null
+++ b/arch/x86/kernel/vbi/wrhv.c
@@ -0,0 +1,1281 @@
+/*
+ *  This program is free software; you can redistribute it and/or modify it
+ *  under the terms of the GNU General Public License as published by the
+ *  Free Software Foundation; either version 2, or (at your option) any
+ *  later version.
+ *
+ *  This program is distributed in the hope that it will be useful, but
+ *  WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ *  General Public License for more details.
+ *
+ *  Copyright (C) 2009 Wind River Systems, Inc.
+ */
+
+
+#include <linux/irq.h>
+#include <linux/pci.h>
+#include <linux/wrhv.h>
+#include <linux/kgdb.h>
+#include <vbi/vbi.h>
+#include <asm/setup.h>
+#include <asm/paravirt.h>
+#include <asm/processor.h>
+#include <asm/wrhv.h>
+#include <asm/pgtable.h>
+#include <asm/tlbflush.h>
+#include <do_timer.h>
+#include <asm/trampoline.h>
+#include <linux/percpu.h>
+#include <linux/smp.h>
+#include <asm/cpu.h>
+#include <asm/reboot.h>		/* for struct machine_ops */
+
+
+//#define WRHV_DEBUG_MSR	1
+#define WRHV_USE_XMLCONFIG	1
+#define WRHV_POLL_IRQ		7
+#define WRHV_RESERVED_PAGES	16
+#define WRHV_RESERVED_TOP	(WRHV_RESERVED_PAGES * PAGE_SIZE)
+
+/* Copied over during early bootstrap */
+struct vb_config __wr_config = { .pid = -1 };
+struct vb_config *_wr_config; /* Pointer passed from hypervisor */
+struct vb_config *wr_config = &__wr_config;
+struct vb_status *wr_status;
+struct vb_control *wr_control;
+
+#ifdef CONFIG_PCI
+extern struct pci_ops pci_root_ops;
+extern int (*pcibios_enable_irq)(struct pci_dev *dev);
+extern void (*pcibios_disable_irq)(struct pci_dev *dev);
+#endif
+
+static unsigned long cr3_val[NR_CPUS];
+static struct vbi_vtlb_control vtlb_ctrl[NR_CPUS];
+static unsigned long is_cr3_cache_enabled[NR_CPUS];
+static unsigned long is_vtlb_optim_enabled[NR_CPUS];
+static unsigned long is_vtlb_ops_cache_enabled[NR_CPUS];
+
+#ifdef	CONFIG_SMP
+#define	VTLB_GET_CPU_VAR(var)	var[smp_processor_id()]
+#else
+#define	VTLB_GET_CPU_VAR(var)	var[0]
+#endif
+
+static cpumask_t flush_cpumask;
+static struct mm_struct *flush_mm;
+static unsigned long flush_va;
+static DEFINE_SPINLOCK(tlbstate_lock);
+static int enable_hrtimer = 0;
+
+#define VBI_VTLB_OPTIM_OPTION (\
+			 VBI_VTLB_OPTIM_ENABLED |  \
+			 VBI_VTLB_CR3_CACHE_ENABLED |  \
+			 VBI_VTLB_OPS_CACHE_ENABLED |  \
+			 VBI_VTLB_DIRTY_BIT_SUPPORT_ENABLED)
+
+#define VBI_VTLB_OPTIM_OPTION_NOOPT (VBI_VTLB_DIRTY_BIT_SUPPORT_ENABLED)
+
+
+static void wrhv_pre_intr_init_hook(void)
+{
+	int i;
+
+	for (i = 0; i < NR_IRQS; i++) {
+		irq_desc[i].status = IRQ_DISABLED;
+		irq_desc[i].action = NULL;
+		irq_desc[i].depth = 1;
+		set_irq_chip_and_handler_name(i, &wrhv_irq_chip, handle_fasteoi_irq, "fasteoi");
+	}
+}
+
+void wrhv_setup_timer_irq(void);
+
+static void __wrhv_map_page(unsigned long vaddr, unsigned long paddr,
+				pgprot_t prot) {
+	pte_t pte = pfn_pte(paddr >> PAGE_SHIFT, prot);
+	set_pte_vaddr(vaddr, pte);
+}
+
+void __init wrhv_init_IRQ(void)
+{
+	int i;
+	unsigned long addr;
+
+	/* The following code maps in hypervisor config/status/control space.
+	   It has to be carefully crafted to be an identity mapping.  We ask
+	   for this space to be supplied to us from the hypervisor at
+	   address 0xffff0000 in the virtual board xml, and we essentially
+	   setup WRHV_RESERVED_PAGES to be 16, representing 16 4K pages from the
+	   end of address space.  This gives us the address 0xffff0000 in Linux
+	   which we need for a virt=phys aka identity mapping.  Why do we
+	   need this to be identity mapped?  Because this block of memory
+	   space is supplied by the hypervisor outside of Linux control, and
+	   it contains pointers to places within itself.  We really don't want
+	   to have to hunt down and modify all those pointers at run time to
+	   be a different (virtual) address.  And finally we tell Linux
+	   through the reservetop bootarg to actually move down the end of
+	   memory by the size in bytes represented by WRHV_RESERVED_PAGES
+	   so we don't interfere with Linux's fixmap facility.
+
+	   The numbers mentioned above are examples, but do reflect reality
+	   as of time of writing.  Please check your constants and do not
+	   rely on the numbers in the above paragraph.
+	*/
+	   
+	addr = (unsigned long)_wr_config;
+	for (i = 0; i < WRHV_RESERVED_PAGES; addr += PAGE_SIZE, i++)
+		__wrhv_map_page(addr, addr, PAGE_KERNEL);
+
+	/* We no longer need to use the vbconfig copy, map it straight in */
+	wr_config = _wr_config;
+		
+	/* Setup the global variables used by the vbi */
+	vbi_init(wr_config);
+
+	/* Now that critical hypervisor global regions are mapped in,
+	   proceed with doing the actual interrupt initialization work.
+	*/
+
+	wrhv_pre_intr_init_hook();
+
+	for (i = 0; i < (NR_VECTORS - FIRST_EXTERNAL_VECTOR); i++) {
+		int vector = FIRST_EXTERNAL_VECTOR + i;
+		if (i >= NR_IRQS)
+			break;
+		if (vector != SYSCALL_VECTOR)
+			set_intr_gate(vector, interrupt[i]);
+	}
+
+	irq_ctx_init(smp_processor_id());
+
+	wrhv_setup_timer_irq();
+}
+
+static void wrhv_init_timer(enum clock_event_mode mode,
+				struct clock_event_device *evt)
+{
+}
+
+static int wrhv_set_next_event(unsigned long delta,
+			    struct clock_event_device *evt)
+{
+	return 0;
+}
+
+static inline void wrhv_send_IPI_mask(int, cpumask_t);
+static void wrhv_timer_broadcast(cpumask_t mask)
+{
+	wrhv_send_IPI_mask(DUMMY_TIMER_INT, mask);
+}
+
+struct clock_event_device wrhv_clock_event = {
+	.name           = "wrhv",
+	.features       = CLOCK_EVT_FEAT_PERIODIC,
+	.set_mode       = wrhv_init_timer,
+	.set_next_event = wrhv_set_next_event,
+	.broadcast      = wrhv_timer_broadcast,
+	.max_delta_ns   = 0xffffffff,
+	.min_delta_ns   = 10000,
+	.shift          = 32,
+	.mult           = 1,
+	.irq            = 0,
+};
+
+DEFINE_PER_CPU(struct clock_event_device, wrhv_clock_events);
+
+irqreturn_t wrhv_dummy_timer_interrupt(int irq, void *dev_id)
+{
+	int cpu = smp_processor_id();
+	struct clock_event_device *evt = &per_cpu(wrhv_clock_events, cpu);
+	if (!evt->event_handler) {
+		printk(KERN_WARNING "wrhv timer handler \
+				     has not set yet %d\n", cpu);
+		return IRQ_NONE;
+	}
+
+	evt->event_handler(evt);
+
+	return IRQ_HANDLED;
+}
+
+irqreturn_t wrhv_timer_interrupt(int irq, void *dev_id)
+{
+	int cpu = smp_processor_id();
+	struct clock_event_device *evt = &per_cpu(wrhv_clock_events, cpu);
+
+	if (!evt->event_handler) {
+		printk(KERN_WARNING
+			   "Spurious Hyp timer interrupt on cpu %d\n", cpu);
+		return IRQ_NONE;
+	}
+
+	evt->event_handler(evt);
+
+	return IRQ_HANDLED;
+}
+
+static struct irqaction wrhv_timer_irq = {
+	.handler = wrhv_timer_interrupt,
+	.flags = IRQF_DISABLED | IRQF_NOBALANCING,
+	.mask = CPU_MASK_NONE,
+	.name = "timer",
+};
+
+static inline void wrhv_mask_timer_for_vcore(void);
+static inline void wrhv_umask_timer_for_vcore(void);
+void __devinit wrhv_setup_boot_clock(void)
+{
+#ifdef CONFIG_WRHV_X86_HRTIMERS
+	int ret;
+	struct clock_event_device *evt;
+
+	if (enable_hrtimer) {
+		wrhv_mask_timer_for_vcore();
+		evt = &per_cpu(wrhv_clock_events, 0);
+		memcpy(evt, &wrhv_clock_event, sizeof(*evt));
+		evt->cpumask = cpumask_of_cpu(0);
+		evt->features = CLOCK_EVT_FEAT_DUMMY | CLOCK_EVT_FEAT_ONESHOT;
+		evt->irq = DUMMY_TIMER_INT;
+		evt->rating = 1;
+		clockevents_register_device(evt);
+		wrhv_timer_irq.name = "dummy_ipi_timer";
+		wrhv_timer_irq.handler = wrhv_dummy_timer_interrupt;
+		ret = setup_irq(DUMMY_TIMER_INT, &wrhv_timer_irq);
+		if (ret)
+			printk(KERN_WARNING "setup dummy timer irq failed\n");
+	}
+#endif
+}
+
+void __devinit wrhv_setup_secondary_clock(void)
+{
+	int cpu;
+	struct clock_event_device *evt;
+	cpu = smp_processor_id();
+	printk(KERN_INFO "installing wrhv timer for CPU %d\n", cpu);
+
+	evt = &per_cpu(wrhv_clock_events, cpu);
+	memcpy(evt, &wrhv_clock_event, sizeof(*evt));
+	evt->cpumask = cpumask_of_cpu(cpu);
+
+#ifdef CONFIG_WRHV_X86_HRTIMERS
+	if (enable_hrtimer) {
+		evt->features = CLOCK_EVT_FEAT_DUMMY | CLOCK_EVT_FEAT_ONESHOT;
+		evt->irq = DUMMY_TIMER_INT;
+		evt->rating = 1;
+	}
+#endif
+	clockevents_register_device(evt);
+}
+
+static void __init wrhv_time_init(void)
+{
+	struct clock_event_device *evt;
+
+	evt = &per_cpu(wrhv_clock_events, 0);
+	memcpy(evt, &wrhv_clock_event, sizeof(*evt));
+	evt->cpumask = cpumask_of_cpu(0);
+
+	clockevents_register_device(evt);
+	wrhv_timer_irq.mask = cpumask_of_cpu(0);
+	setup_irq(0, &wrhv_timer_irq);
+}
+
+#ifdef CONFIG_PCI
+static int wrhv_pci_enable_irq(struct pci_dev *dev)
+{
+	return 0;
+}
+
+static void wrhv_pci_disable_irq(struct pci_dev *dev)
+{
+}
+
+static int wrhv_init_pci(void)
+{
+	pcibios_enable_irq = wrhv_pci_enable_irq;
+	pcibios_disable_irq = wrhv_pci_disable_irq;
+	return 0;
+}
+
+static int __devinitdata __wrhv_kgdboe_poll;
+static int __init wrhv_check_kgdboe(char *str)
+{
+	__wrhv_kgdboe_poll = 1;
+	return 0;
+}
+early_param("kgdboe", wrhv_check_kgdboe);
+
+static void __devinit pci_fixup_wrhv(struct pci_dev *dev)
+{
+	int irq;
+	char *devclass, devname[32] = { "Unknown" };
+	int skip_assign_irq = 0;
+
+	switch (dev->class >> 16) {
+	case PCI_BASE_CLASS_NETWORK:
+		devclass = "Ethernet";
+		if (__wrhv_kgdboe_poll) {
+			skip_assign_irq = 1;
+			irq = WRHV_POLL_IRQ;
+		}
+		break;
+
+	case PCI_BASE_CLASS_STORAGE:
+		devclass = "IDE";
+		break;
+
+	case PCI_BASE_CLASS_DISPLAY:
+		devclass = "VGA";
+		break;
+	default:
+		skip_assign_irq = 1;
+		irq = dev->irq;
+		break;
+	}
+
+	if (!skip_assign_irq) {
+		snprintf(devname, sizeof devname, "pci%s_%x:%x",
+			devclass, dev->bus->number, dev->devfn);
+		irq = vbi_find_irq(devname, 1);
+		if (irq == VBI_INVALID_IRQ)
+			irq = WRHV_POLL_IRQ;
+	}
+
+	dev->irq = irq;
+	printk(KERN_DEBUG "WRHV-PCI: %s CLASS:%x IRQ%d\n",
+		devname, dev->class, dev->irq);
+}
+
+DECLARE_PCI_FIXUP_HEADER(PCI_ANY_ID, PCI_ANY_ID, pci_fixup_wrhv);
+#endif /* CONFIG_PCI */
+
+static unsigned long long wrhv_read_msr(unsigned int msr, int *err)
+{
+#ifdef WRHV_DEBUG_MSR
+	printk("RDMSR from %p\n", __builtin_return_address(0));
+#endif
+	return native_read_msr(msr);
+}
+
+static int wrhv_write_msr(unsigned int msr, unsigned low, unsigned high)
+{
+#ifdef WRHV_DEBUG_MSR
+	printk("WRMSR from %p\n", __builtin_return_address(0));
+#endif
+	native_write_msr(msr, low, high);
+	return 0;
+}
+
+static unsigned long wrhv_get_debugreg(int regno)
+{
+	unsigned long val = ~0UL;
+
+	switch (regno) {
+	case 0 ... 3:
+		/* undefined state */
+		break;
+	case 6 ... 7:
+		val = 0;
+		break;
+	default:
+		BUG();
+	}
+	return val;
+}
+
+static void wrhv_set_debugreg(int regno, unsigned long value)
+{
+}
+
+void wrhv_cpu_workarounds(struct cpuinfo_x86 *c)
+{
+	/* Simics workaround */
+	c->hlt_works_ok = 0;
+
+	/* WP test fails currently */
+	c->wp_works_ok = 1;
+
+	clear_bit(X86_FEATURE_DE, (void *)boot_cpu_data.x86_capability);
+}
+
+void wrhv_boot_config(void)
+{
+	boot_params.hdr.type_of_loader = 0xff; /* Unknown */
+	if (__initrd_start != __initrd_end) {
+		boot_params.hdr.ramdisk_image = (unsigned long)&__initrd_start - PAGE_OFFSET;
+		boot_params.hdr.ramdisk_size = (unsigned long)&__initrd_end - (unsigned long)&__initrd_start;
+	}
+
+#ifndef WRHV_USE_XMLCONFIG
+	strlcpy(boot_command_line,
+		"pci=conf1 memmap=exactmap memmap=32M@0 mem=nopentium earlyprintk=vga,keep ramdisk_size=16384"
+		" serialnumber nolapic nomce nosep retain_initrd root=/dev/ram init=/bin/busybox console=ttyS0,9600",
+		COMMAND_LINE_SIZE);
+#else
+	/* Use the config space copy here, since we haven't mapped in the
+	   actual hypervisor config/status/control space yet */
+        snprintf(boot_command_line, COMMAND_LINE_SIZE,
+		"retain_initrd pci=conf1 idle=wrhv mem=nopentium serialnumber nolapic nomce nosep memmap=exactmap memmap=%dK@0 reservetop=%d %s",
+		wr_config->phys_mem_size / 1024, (int)WRHV_RESERVED_TOP,
+		wr_config->bootLine);
+#endif
+}
+
+#ifdef CONFIG_SMP
+irqreturn_t wrhv_ipi_func_call_single_handler(int irq, void *dev_id)
+{
+	irq_enter();
+	generic_smp_call_function_single_interrupt();
+	__get_cpu_var(irq_stat).irq_call_count++;
+	irq_exit();
+	return IRQ_HANDLED;
+}
+
+irqreturn_t wrhv_ipi_func_call_handler(int irq, void *dev_id)
+{
+	irq_enter();
+	generic_smp_call_function_interrupt();
+	__get_cpu_var(irq_stat).irq_call_count++;
+	irq_exit();
+	return IRQ_HANDLED;
+}
+
+irqreturn_t wrhv_ipi_inv_tlb_handler(int irq, void *dev_id)
+{
+	unsigned long cpu;
+
+	cpu = get_cpu();
+
+	if (!cpu_isset(cpu, flush_cpumask))
+		goto out;
+		/*
+		 * This was a BUG() but until someone can quote me the
+		 * line from the intel manual that guarantees an IPI to
+		 * multiple CPUs is retried _only_ on the erroring CPUs
+		 * its staying as a return
+		 *
+		 * BUG();
+		 */
+
+	if (flush_mm == per_cpu(cpu_tlbstate, cpu).active_mm) {
+		if (per_cpu(cpu_tlbstate, cpu).state == TLBSTATE_OK) {
+			if (flush_va == TLB_FLUSH_ALL)
+				local_flush_tlb();
+			else
+				__flush_tlb_one(flush_va);
+		} else {
+			leave_mm(cpu);
+		}
+	} else {
+	    if (flush_va == TLB_FLUSH_ALL)
+			wrhv_vtlb_op(VBI_VTLB_OP_DELETE_PMD,
+					__pa(flush_mm->pgd),
+					0, 0);
+	    else
+			wrhv_vtlb_op(VBI_VTLB_OP_UPDATE_PTE,
+					__pa(flush_mm->pgd),
+					(unsigned long)flush_va, 0);
+
+	}
+
+	smp_mb__before_clear_bit();
+	cpu_clear(cpu, flush_cpumask);
+	smp_mb__after_clear_bit();
+out:
+	put_cpu_no_resched();
+	__get_cpu_var(irq_stat).irq_tlb_count++;
+
+	return IRQ_HANDLED;
+}
+
+irqreturn_t wrhv_ipi_resched_handler(int irq, void *dev_id)
+{
+	__get_cpu_var(irq_stat).irq_resched_count++;
+	return IRQ_HANDLED;
+}
+#endif
+
+void wrhv_vtlb_op(unsigned int op, unsigned long arg1,
+		  unsigned long arg2, unsigned long arg3)
+{
+	unsigned long flags;
+	int i;
+
+	if (!VTLB_GET_CPU_VAR(is_vtlb_ops_cache_enabled))
+		vbi_vtlb_op(op, arg1, arg2, arg3);
+	else {
+		local_irq_save(flags);
+		i = VTLB_GET_CPU_VAR(vtlb_ctrl).vtlb_ops_ix;
+		VTLB_GET_CPU_VAR(vtlb_ctrl).vtlb_ops[i].op = op;
+		VTLB_GET_CPU_VAR(vtlb_ctrl).vtlb_ops[i].arg1 = arg1;
+		VTLB_GET_CPU_VAR(vtlb_ctrl).vtlb_ops[i].arg2 = arg2;
+		VTLB_GET_CPU_VAR(vtlb_ctrl).vtlb_ops[i].arg3 = arg3;
+		wmb();
+		/*
+		 * If the buffer is full, flush it. Index will be automatically
+		 * updated by the hypervisor.
+		 */
+
+		if (VTLB_GET_CPU_VAR(vtlb_ctrl).vtlb_ops_ix == (VBI_VTLB_OP_MAX_OPS - 1))
+			vbi_vtlb_op(VBI_VTLB_OP_FLUSH_OPS, 0, 0, 0);
+		else
+			VTLB_GET_CPU_VAR(vtlb_ctrl).vtlb_ops_ix += 1;
+
+		local_irq_restore(flags);
+	}
+}
+
+static void wrhv_write_cr3(unsigned long val)
+{
+	unsigned long cr3 = val;
+	int i;
+
+	if (VTLB_GET_CPU_VAR(is_cr3_cache_enabled) && VTLB_GET_CPU_VAR(vtlb_ctrl).vtlb_ops_ix == 0) {
+		for (i = 0; i < VBI_VTLB_OP_CR3_CACHE_ENTRIES; i++) {
+			if (VTLB_GET_CPU_VAR(vtlb_ctrl).cr3_cache[i].guest_cr3 == cr3) {
+				cr3 = VTLB_GET_CPU_VAR(vtlb_ctrl).cr3_cache[i].host_cr3;
+				VTLB_GET_CPU_VAR(vtlb_ctrl).cr3_cache_ix = i;
+				break;
+			}
+		}
+	} else
+		VTLB_GET_CPU_VAR(vtlb_ctrl).cr3_cache_ix = -1;
+
+	asm volatile ("mov %0,%%cr3": :"r" (cr3));
+	VTLB_GET_CPU_VAR(cr3_val) = val;
+}
+
+static unsigned long wrhv_read_cr3(void)
+{
+	/* Use cached value to avoid useless hypercall */
+	return VTLB_GET_CPU_VAR(cr3_val);
+}
+
+static void wrhv_set_pmd(pmd_t *pmdp, pmd_t pmdval)
+{
+	*pmdp = pmdval;
+	wrhv_vtlb_op(VBI_VTLB_OP_UPDATE_PMD,
+			__pa(((unsigned long) pmdp) & PAGE_MASK),
+			__pa(pmdp), 0);
+}
+
+#define is_current_as(mm) ((mm) == current->active_mm || ((mm) == &init_mm))
+
+static void wrhv_pte_update(struct mm_struct *mm, unsigned long addr, pte_t *ptep)
+{
+	if (!is_current_as(mm))
+		wrhv_vtlb_op(VBI_VTLB_OP_UPDATE_PTE, __pa(mm->pgd),
+						addr, __pa(ptep));
+}
+
+static void wrhv_pte_update_defer(struct mm_struct *mm, unsigned long addr, pte_t *ptep)
+{
+	if (!is_current_as(mm))
+		wrhv_pte_update (mm, addr, ptep);
+}
+
+static void wrhv_release_pd(u32 pfn)
+{
+	wrhv_vtlb_op(VBI_VTLB_OP_DELETE_PMD, pfn << PAGE_SHIFT, 0, 0);
+}
+
+static void wrhv_set_pte(pte_t *ptep, pte_t pte)
+{
+	*ptep = pte;
+}
+
+static void wrhv_flush_tlb_user(void)
+{
+	native_write_cr3(VTLB_GET_CPU_VAR(cr3_val));
+}
+
+static void wrhv_flush_tlb_kernel(void)
+{
+	unsigned long cr4, flags;
+
+	/* This routine is not optimized but since it is very rarely used
+	   let's not worry too much about this for now.
+	 */
+
+	local_irq_save(flags);
+	cr4 = native_read_cr4();
+	native_write_cr4(cr4 & ~X86_CR4_PGE);
+	native_write_cr3(VTLB_GET_CPU_VAR(cr3_val));
+	native_write_cr4(cr4);
+	local_irq_restore(flags);
+}
+
+static void wrhv_flush_tlb_single(unsigned long addr)
+{
+	__native_flush_tlb_single(addr);
+}
+
+
+static inline void wrhv_send_IPI_mask(int irq, cpumask_t mask)
+{
+	unsigned long coreset = cpus_addr(mask)[0];
+	unsigned long flags;
+
+	local_irq_save(flags);
+	WARN_ON(coreset & ~cpus_addr(cpu_online_map)[0]);
+	vbi_send_vcore_vioapic_irq(irq, coreset, 0);
+	local_irq_restore(flags);
+}
+
+static void wrhv_smp_send_invalidate_tlb_ipi(cpumask_t mask)
+{
+	wrhv_send_IPI_mask(WRHV_IPI_INV_TLB, mask);
+}
+
+static void wrhv_flush_tlb_others(const cpumask_t *cpumaskp, struct mm_struct *mm,
+			     unsigned long va)
+{
+	cpumask_t cpumask = *cpumaskp;
+
+	/*
+	 * A couple of (to be removed) sanity checks:
+	 *
+	 * - current CPU must not be in mask
+	 * - mask must exist :)
+	 */
+	BUG_ON(cpus_empty(cpumask));
+	BUG_ON(cpu_isset(smp_processor_id(), cpumask));
+	BUG_ON(!mm);
+
+	/*
+	 * i'm not happy about this global shared spinlock in the
+	 * MM hot path, but we'll see how contended it is.
+	 * AK: x86-64 has a faster method that could be ported.
+	 */
+	spin_lock(&tlbstate_lock);
+
+	flush_mm = mm;
+	flush_va = va;
+	cpus_or(flush_cpumask, cpumask, flush_cpumask);
+	/*
+	 * We have to send the IPI only to
+	 * CPUs affected.
+	 */
+	wrhv_smp_send_invalidate_tlb_ipi(cpumask);
+
+	while (!cpus_empty(flush_cpumask))
+		/* nothing. lockup detection does not belong here */
+		cpu_relax();
+
+	flush_mm = NULL;
+	flush_va = 0;
+	spin_unlock(&tlbstate_lock);
+}
+
+static void wrhv_set_pte_at(struct mm_struct *mm, unsigned long addr, pte_t *ptep, pte_t pte)
+{
+	if (!is_current_as(mm)) {
+		*ptep = pte;
+		wrhv_vtlb_op(VBI_VTLB_OP_SET_PTE_AT, __pa(mm->pgd),
+						addr, __pa(ptep));
+	} else
+		*ptep = pte;
+}
+
+static unsigned wrhv_patch(u8 type, u16 clobber, void *ibuf,
+				unsigned long addr, unsigned len)
+{
+	return paravirt_patch_default(type, clobber, ibuf, addr, len);
+}
+
+static void wrhv_exit_mmap (struct mm_struct *mm)
+{
+#ifdef CONFIG_SMP
+	/*
+	 * We are deleting the page directory. We need to delete it in
+	 * the current hypervisor cache but also in the cache of the
+	 * hypervisors managing the various virtual cores.
+	 */
+
+	cpumask_t cpumask;
+	int cpu = get_cpu ();
+	wrhv_vtlb_op(VBI_VTLB_OP_DELETE_PMD, __pa(mm->pgd), 0, 0);
+	cpumask = mm->cpu_vm_mask;
+	cpu_clear(cpu, cpumask);
+	if (!cpus_empty(cpumask))
+	    wrhv_flush_tlb_others (&cpumask, mm, TLB_FLUSH_ALL);
+	put_cpu ();
+#else
+	wrhv_vtlb_op(VBI_VTLB_OP_DELETE_PMD, __pa(mm->pgd), 0, 0);
+#endif
+}
+
+static void wrhv_init_vtlb_per_cpu(void)
+{
+	/* Initialize the cached copy of cr3 */
+	VTLB_GET_CPU_VAR(cr3_val) = native_read_cr3();
+
+	/*
+	 * set the size of the vtlb_ctrl structure in the structure provided
+	 * to the hypervisor; the hypervisor may be able to use this later
+	 * for backward compatibility.
+	 */
+
+	VTLB_GET_CPU_VAR(vtlb_ctrl).size = sizeof (VTLB_GET_CPU_VAR(vtlb_ctrl));
+
+	/* First set the options supported by the guest OS. The host will
+	   then update the mode field of vtlb_ctrl option to indicate which
+	   one will actually be in use.
+	*/
+
+	if (strstr(boot_command_line, "novtlbopt")) {
+		VTLB_GET_CPU_VAR(vtlb_ctrl).mode = VBI_VTLB_OPTIM_OPTION_NOOPT;
+		printk("WRHV:  CPU %d vtlb optimization disabled\n",
+			smp_processor_id());
+	}
+	else
+		VTLB_GET_CPU_VAR(vtlb_ctrl).mode = VBI_VTLB_OPTIM_OPTION;
+
+	vbi_vtlb_op(VBI_VTLB_OP_INIT, __pa_symbol(&VTLB_GET_CPU_VAR(vtlb_ctrl)),
+		 0, 0);
+
+	if (VTLB_GET_CPU_VAR(vtlb_ctrl).mode & VBI_VTLB_CR3_CACHE_ENABLED)
+		VTLB_GET_CPU_VAR(is_cr3_cache_enabled) = 1;
+	if (VTLB_GET_CPU_VAR(vtlb_ctrl).mode & VBI_VTLB_OPTIM_ENABLED)
+		VTLB_GET_CPU_VAR(is_vtlb_optim_enabled) = 1;
+	if (VTLB_GET_CPU_VAR(vtlb_ctrl).mode & VBI_VTLB_OPS_CACHE_ENABLED)
+		VTLB_GET_CPU_VAR(is_vtlb_ops_cache_enabled) = 1;
+
+}
+
+static void wrhv_init_mm(void)
+{
+	wrhv_init_vtlb_per_cpu();
+
+	pv_mmu_ops.read_cr3 = wrhv_read_cr3;
+	pv_mmu_ops.write_cr3 = wrhv_write_cr3;
+
+	if (VTLB_GET_CPU_VAR(is_vtlb_optim_enabled)) {
+		pv_mmu_ops.set_pte = wrhv_set_pte;
+		pv_mmu_ops.set_pte_at = wrhv_set_pte_at;
+
+		pv_mmu_ops.pte_update = wrhv_pte_update;
+		pv_mmu_ops.pte_update_defer = wrhv_pte_update_defer;
+		pv_mmu_ops.set_pmd = wrhv_set_pmd;
+
+		pv_mmu_ops.release_pmd = wrhv_release_pd;
+
+		pv_mmu_ops.exit_mmap = wrhv_exit_mmap,
+
+		pv_mmu_ops.flush_tlb_user = wrhv_flush_tlb_user;
+		pv_mmu_ops.flush_tlb_kernel = wrhv_flush_tlb_kernel;
+		pv_mmu_ops.flush_tlb_single = wrhv_flush_tlb_single;
+	}
+#ifdef CONFIG_SMP
+	pv_mmu_ops.flush_tlb_others = wrhv_flush_tlb_others;
+#endif
+}
+
+int __init wrhv_late_init(void)
+{
+	return 0;
+}
+late_initcall(wrhv_late_init);
+
+static inline void wrhv_umask_timer_for_vcore(void)
+{
+       /* unmask hypervisor-provided timer interrupt for vcore */
+       vbi_unmask_vioapic_irq(0);
+}
+
+static inline void wrhv_mask_timer_for_vcore(void)
+{
+       /* mask hypervisor-provided timer interrupt for vcore */
+       vbi_mask_vioapic_irq(0);
+}
+
+#ifdef CONFIG_SMP
+static void __init wrhv_smp_prepare_boot_cpu(void)
+{
+	BUG_ON(smp_processor_id() != 0);
+	native_smp_prepare_boot_cpu();
+	return;
+}
+
+void __init wrhv_calibrate_smp_cpus(void)
+{
+	/* Use the config space copy here, since we haven't mapped in the
+	   actual hypervisor config/status/control space yet */
+	int cpus = wr_config->numCores;
+	int cpuid = wr_config->coreId;
+	if (cpuid != 0)
+		return;
+	printk(KERN_INFO "WRHV: calibrate CPU information according to vbConfig \n");
+	physid_clear(16, phys_cpu_present_map);
+	if (cpus > 1) {
+		smp_found_config = 1;
+		alternatives_smp_switch(1);
+	}
+
+	while( --cpus >= 0) {
+		physid_set(cpus, phys_cpu_present_map);
+		cpu_set(cpus, cpu_present_map);
+		cpu_set(cpus, cpu_possible_map);
+	}
+	return;
+}
+EXPORT_SYMBOL(wrhv_calibrate_smp_cpus);
+
+static void inline wrhv_umask_IPIs_for_vcore(void)
+{
+	/* unmask ipi interrupt for vcore */
+	vbi_unmask_vioapic_irq(WRHV_IPI_RESCHED);
+	vbi_unmask_vioapic_irq(WRHV_IPI_INV_TLB);
+	vbi_unmask_vioapic_irq(WRHV_IPI_FUNC_CALL);
+	vbi_unmask_vioapic_irq(WRHV_IPI_FUNC_CALL_SINGLE);
+#ifdef CONFIG_WRHV_X86_HRTIMERS
+	if (enable_hrtimer)
+		vbi_unmask_vioapic_irq(DUMMY_TIMER_INT);
+#endif
+}
+
+void __init wrhv_smp_prepare_cpus(unsigned int max_cpus)
+{
+	int ret;
+	native_smp_prepare_cpus(max_cpus);
+
+	set_irq_chip_and_handler_name(WRHV_IPI_RESCHED,
+			&wrhv_ipi_irq_chip, handle_percpu_irq, "per_cpu");
+	set_irq_chip_and_handler_name(WRHV_IPI_INV_TLB,
+			&wrhv_ipi_irq_chip, handle_percpu_irq, "per_cpu");
+	set_irq_chip_and_handler_name(WRHV_IPI_FUNC_CALL,
+			&wrhv_ipi_irq_chip, handle_percpu_irq, "per_cpu");
+	set_irq_chip_and_handler_name(WRHV_IPI_FUNC_CALL_SINGLE,
+			&wrhv_ipi_irq_chip, handle_percpu_irq, "per_cpu");
+
+#ifdef CONFIG_WRHV_X86_HRTIMERS
+	if (enable_hrtimer)
+		set_irq_chip_and_handler_name(DUMMY_TIMER_INT,
+			&wrhv_ipi_irq_chip, handle_percpu_irq, "per_cpu");
+#endif
+
+	ret = request_irq(WRHV_IPI_RESCHED, wrhv_ipi_resched_handler,
+			IRQF_DISABLED|IRQF_NOBALANCING, "ipi_resched",
+			wrhv_ipi_resched_handler);
+	printk("request_irq ret for WRHV_IPI_RESCHED: %d \n", ret);
+
+	ret = request_irq(WRHV_IPI_INV_TLB, wrhv_ipi_inv_tlb_handler,
+			IRQF_DISABLED|IRQF_NOBALANCING, "ipi_inv_tlb",
+			wrhv_ipi_inv_tlb_handler);
+	printk("request_irq ret for WRHV_IPI_INV_TLB: %d \n", ret);
+
+	ret = request_irq(WRHV_IPI_FUNC_CALL, wrhv_ipi_func_call_handler,
+			IRQF_DISABLED|IRQF_NOBALANCING, "ipi_func_call",
+			wrhv_ipi_func_call_handler);
+	printk("request_irq ret for WRHV_IPI_FUNC_CALL: %d \n", ret);
+
+	ret = request_irq(WRHV_IPI_FUNC_CALL_SINGLE,
+			wrhv_ipi_func_call_single_handler,
+			IRQF_DISABLED|IRQF_NOBALANCING, "ipi_func_call_single",
+			wrhv_ipi_func_call_single_handler);
+	printk("request_irq ret for WRHV_IPI_FUNC_CALL_SINGLE: %d \n", ret);
+
+	wrhv_umask_IPIs_for_vcore();
+
+	return;
+}
+
+#ifdef CONFIG_HOTPLUG_CPU
+static DEFINE_PER_CPU(struct task_struct *, idle_thread_array);
+#define get_idle_for_cpu(x)      (per_cpu(idle_thread_array, x))
+#define set_idle_for_cpu(x, p)   (per_cpu(idle_thread_array, x) = (p))
+#else
+extern struct task_struct *idle_thread_array[NR_CPUS] __cpuinitdata ;
+#define get_idle_for_cpu(x)      (idle_thread_array[(x)])
+#define set_idle_for_cpu(x, p)   (idle_thread_array[(x)] = (p))
+#endif
+
+struct create_idle {
+	struct work_struct work;
+	struct task_struct *idle;
+	struct completion done;
+	int cpu;
+};
+
+static void __cpuinit do_fork_idle(struct work_struct *work)
+{
+	struct create_idle *c_idle =
+		container_of(work, struct create_idle, work);
+
+	c_idle->idle = fork_idle(c_idle->cpu);
+	complete(&c_idle->done);
+}
+
+static void __cpuinit wrhv_smp_callin(void)
+{
+	int cpuid;
+	unsigned long timeout;
+
+	cpuid = smp_processor_id();
+	if (cpu_isset(cpuid, cpu_callin_map)) {
+		panic("%s: CPU#%d already present??\n", __func__, cpuid);
+	}
+
+	/*
+	 * Waiting 2s total for startup (udelay is not yet working)
+	 */
+	timeout = jiffies + 2*HZ;
+	while (time_before(jiffies, timeout)) {
+		/*
+		 * Has the boot CPU finished it's STARTUP sequence?
+		 */
+		if (cpu_isset(cpuid, cpu_callout_map))
+			break;
+		cpu_relax();
+	}
+
+	if (!time_before(jiffies, timeout)) {
+		panic("%s: CPU%d started up but did not get a callout!\n",
+			__func__, cpuid);
+	}
+
+	local_irq_enable();
+	calibrate_delay();
+	local_irq_disable();
+
+	smp_store_cpu_info(cpuid);
+
+	/*
+	 * Allow the master to continue.
+	 */
+	cpu_set(cpuid, cpu_callin_map);
+}
+
+static int low_mappings;
+
+static void __cpuinit wrhv_smp_start_cpu(void)
+{
+	wrhv_mask_timer_for_vcore();
+
+	wrhv_init_vtlb_per_cpu();
+
+	cpu_init();
+	wrhv_umask_IPIs_for_vcore();
+	preempt_disable();
+	wrhv_smp_callin();
+
+	/* otherwise gcc will move up smp_processor_id before the cpu_init */
+	barrier();
+
+	/*
+	 * Check TSC synchronization with the BP:
+	 */
+	check_tsc_sync_target();
+
+#ifdef CONFIG_X86_32
+	while (low_mappings) {
+		cpu_relax();
+	}
+	__flush_tlb_all();
+#endif
+
+	set_cpu_sibling_map(raw_smp_processor_id());
+	wmb();
+
+	ipi_call_lock_irq();
+	cpu_set(smp_processor_id(), cpu_online_map);
+	ipi_call_unlock_irq();
+	per_cpu(cpu_state, smp_processor_id()) = CPU_ONLINE;
+
+	setup_secondary_clock();
+
+	if (!enable_hrtimer)
+		wrhv_umask_timer_for_vcore();
+
+	wmb();
+
+	local_irq_enable();
+	cpu_idle();
+
+}
+
+static int __cpuinit wrhv_wakeup_secondary_cpu(int core)
+{
+	return vbi_vb_resume(VBI_BOARD_ID_GET(), core);
+}
+
+static int __cpuinit wrhv_do_boot_cpu(int cpu)
+{
+	unsigned long boot_error = 0;
+	unsigned int timeout;
+	struct create_idle c_idle = {
+		.cpu = cpu,
+		.done = COMPLETION_INITIALIZER_ONSTACK(c_idle.done),
+	};
+
+	INIT_WORK(&c_idle.work, do_fork_idle);
+
+	alternatives_smp_switch(1);
+
+	c_idle.idle = get_idle_for_cpu(cpu);
+
+	if (c_idle.idle) {
+		c_idle.idle->thread.sp = (unsigned long) (((struct pt_regs *)
+			(THREAD_SIZE +  task_stack_page(c_idle.idle))) - 1);
+		init_idle(c_idle.idle, cpu);
+		goto do_rest;
+	}
+
+	if (!keventd_up() || current_is_keventd())
+		c_idle.work.func(&c_idle.work);
+	else {
+		schedule_work(&c_idle.work);
+		wait_for_completion(&c_idle.done);
+	}
+
+	if (IS_ERR(c_idle.idle)) {
+		printk("failed fork for CPU %d\n", cpu);
+		return PTR_ERR(c_idle.idle);
+	}
+
+	set_idle_for_cpu(cpu, c_idle.idle);
+
+do_rest:
+
+	per_cpu(current_task, cpu) = c_idle.idle;
+	init_gdt(cpu);
+	irq_ctx_init(cpu);
+
+	early_gdt_descr.address = (unsigned long)get_cpu_gdt_table(cpu);
+	initial_code = (unsigned long)wrhv_smp_start_cpu;
+	stack_start.sp = (void *) c_idle.idle->thread.sp;
+
+	printk(KERN_INFO "Booting processor %d\n", cpu);
+
+	boot_error = wrhv_wakeup_secondary_cpu(cpu);
+
+	if (!boot_error) {
+		/*
+		 * allow APs to start initializing.
+		 */
+		pr_debug("Before Callout %d.\n", cpu);
+		cpu_set(cpu, cpu_callout_map);
+		pr_debug("After Callout %d.\n", cpu);
+
+		/*
+		 * Wait 5s total for a response
+		 */
+		for (timeout = 0; timeout < 50000; timeout++) {
+			if (cpu_isset(cpu, cpu_callin_map))
+				break;	/* It has booted */
+			udelay(100);
+		}
+
+		if (cpu_isset(cpu, cpu_callin_map)) {
+			/* number CPUs logically, starting from 1 (BSP is 0) */
+			pr_debug("OK.\n");
+			printk(KERN_INFO "CPU%d: ", cpu);
+			print_cpu_info(&cpu_data(cpu));
+			pr_debug("CPU has booted.\n");
+		} else {
+			boot_error = 1;
+			printk(KERN_ERR "Not responding.\n");
+		}
+	}
+
+	if (boot_error) {
+		/* Try to put things back the way they were before ... */
+		cpu_clear(cpu, cpu_callout_map);
+		cpu_clear(cpu, cpu_initialized);
+		cpu_clear(cpu, cpu_present_map);
+	}
+
+	return boot_error;
+}
+
+static int __cpuinit wrhv_cpu_up(unsigned int cpu)
+{
+	int err;
+	unsigned long flags;
+
+	if ( !physid_isset(cpu, phys_cpu_present_map)) {
+		printk(KERN_ERR "%s: bad cpu %d\n", __func__, cpu);
+		printk("----------phys_cpu_present_map == %x \n", *(unsigned *)&phys_cpu_present_map);
+		return -EINVAL;
+	}
+
+	/*
+	 * Already booted CPU?
+	 */
+	if (cpu_isset(cpu, cpu_callin_map)) {
+		panic("wrhv_do_boot_cpu core%d Already started\n", cpu);
+		return -ENOSYS;
+	}
+
+	per_cpu(cpu_state, cpu) = CPU_UP_PREPARE;
+
+#ifdef CONFIG_X86_32
+	/* init low mem mapping */
+	clone_pgd_range(swapper_pg_dir, swapper_pg_dir + KERNEL_PGD_BOUNDARY,
+		min_t(unsigned long, KERNEL_PGD_PTRS, KERNEL_PGD_BOUNDARY));
+	flush_tlb_all();
+
+	low_mappings = 1;
+	err = wrhv_do_boot_cpu(cpu);
+	zap_low_mappings();
+	low_mappings = 0;
+#else
+	err = wrhv_do_boot_cpu(cpu);
+#endif
+	if (err) {
+		pr_debug("wrhv_do_boot_cpu failed %d\n", err);
+		return -EIO;
+	}
+
+
+	local_irq_save(flags);
+	check_tsc_sync_source(cpu);
+	local_irq_restore(flags);
+
+	while (!cpu_online(cpu)) {
+		cpu_relax();
+	}
+
+	return 0;
+}
+
+static void wrhv_smp_cpus_done(unsigned int max_cpus)
+{
+	printk(KERN_INFO "BP: smp_init done. \n");
+	native_smp_cpus_done(max_cpus);
+	return;
+}
+
+static void stop_me(void * t)
+{
+	write_cr3((unsigned long)swapper_pg_dir);
+
+	/* Enter into infinite loop to stop self*/
+	while(1);
+}
+
+static void wrhv_smp_send_stop(void)
+{
+	smp_call_function(stop_me, NULL, 0);
+	return;
+}
+
+static void wrhv_smp_send_reschedule(int cpu)
+{
+	wrhv_send_IPI_mask(WRHV_IPI_RESCHED, cpumask_of_cpu(cpu));
+}
+
+static void wrhv_smp_send_call_func_single_ipi(int cpu)
+{
+	wrhv_send_IPI_mask(WRHV_IPI_FUNC_CALL_SINGLE, cpumask_of_cpu(cpu));
+}
+
+static void wrhv_smp_send_call_func_ipi(cpumask_t mask)
+{
+	wrhv_send_IPI_mask(WRHV_IPI_FUNC_CALL, mask);
+}
+
+static const struct smp_ops wrhv_smp_ops __initdata = {
+	.smp_prepare_boot_cpu = wrhv_smp_prepare_boot_cpu,
+	.smp_prepare_cpus = wrhv_smp_prepare_cpus,
+	.cpu_up = wrhv_cpu_up,
+	.smp_cpus_done = wrhv_smp_cpus_done,
+
+	.smp_send_stop = wrhv_smp_send_stop,
+	.smp_send_reschedule = wrhv_smp_send_reschedule,
+
+	.send_call_func_ipi = wrhv_smp_send_call_func_ipi,
+	.send_call_func_single_ipi = wrhv_smp_send_call_func_single_ipi,
+};
+
+void __init wrhv_smp_init(void)
+{
+	smp_ops = wrhv_smp_ops;
+	return;
+}
+#endif
+
+void wrhv_restart(void)
+{
+	int ret;
+	printk(KERN_INFO "WRHV: rebooting \n");
+
+	ret = vbi_vb_reset(VBI_BOARD_ID_GET(), VBI_VB_CORES_ALL,
+		VBI_VBMGMT_RESET_AND_START_CORE0 |
+		VBI_VBMGMT_RESET_DOWNLOAD |
+		VBI_VBMGMT_RESET_CLEAR
+		);
+
+	if (unlikely(ret != 0))
+		printk(KERN_ERR "WRHV: reboot failed. \n");
+
+	while (1);
+}
+
+void __init wrhv_setup_timer_irq(void)
+{
+#ifdef CONFIG_WRHV_X86_HRTIMERS
+	int irq;
+	irq = vbi_find_irq(HRTIMER_IRQ_NAME, 1);
+	if (irq == VBI_INVALID_IRQ) {
+		enable_hrtimer = 0;
+		pv_time_ops.time_init = wrhv_time_init;
+		pv_time_ops.get_tsc_khz = wrhv_calculate_cpu_khz;
+		printk(KERN_INFO "WRHV: HRTIMER is NOT present.\n");
+	} else {
+		enable_hrtimer = 1;
+		set_irq_chip_and_handler_name(TIMER_INT_NUM, &wrhv_irq_chip,
+					      handle_edge_irq, "edge");
+		printk(KERN_INFO "WRHV: HRTIMER is present.\n");
+	}
+#else
+	enable_hrtimer = 0;
+	pv_time_ops.time_init = wrhv_time_init;
+	pv_time_ops.get_tsc_khz = wrhv_calculate_cpu_khz;
+#endif
+
+#ifdef CONFIG_X86_LOCAL_APIC
+	pv_apic_ops.setup_boot_clock = wrhv_setup_boot_clock;
+	pv_apic_ops.setup_secondary_clock = wrhv_setup_secondary_clock;
+#endif
+}
+
+void __init wrhv_init(void)
+{
+	pv_info.name = "wrhv";
+	pv_info.paravirt_enabled = 1;
+
+	wrhv_cpu_workarounds(&boot_cpu_data);
+
+	pv_init_ops.patch = wrhv_patch;
+
+	pv_cpu_ops.write_msr = wrhv_write_msr;
+	pv_cpu_ops.read_msr = wrhv_read_msr;
+
+	pv_irq_ops.init_IRQ = wrhv_init_IRQ;
+	pv_cpu_ops.get_debugreg = wrhv_get_debugreg;
+	pv_cpu_ops.set_debugreg = wrhv_set_debugreg;
+
+	machine_ops.emergency_restart = wrhv_restart;
+
+#ifdef CONFIG_KGDB
+	arch_kgdb_ops.flags &= ~KGDB_HW_BREAKPOINT,
+	arch_kgdb_ops.set_hw_breakpoint = NULL;
+	arch_kgdb_ops.remove_hw_breakpoint = NULL;
+	arch_kgdb_ops.remove_all_hw_break = NULL;
+	arch_kgdb_ops.correct_hw_break = NULL;
+#endif
+
+	wrhv_init_mm();
+
+#ifdef CONFIG_PCI
+	wrhv_init_pci();
+#endif
+
+#ifdef CONFIG_SMP
+	wrhv_smp_init();
+#endif
+}
diff --git a/arch/x86/kernel/vbi/wrhv_initrd.S b/arch/x86/kernel/vbi/wrhv_initrd.S
new file mode 100644
index 0000000..c5f40d6
--- /dev/null
+++ b/arch/x86/kernel/vbi/wrhv_initrd.S
@@ -0,0 +1,3 @@
+/*
+ *  This file simply exists to trigger the Makefile to include the INITRD
+ */
diff --git a/include/asm-x86/wrhv.h b/include/asm-x86/wrhv.h
new file mode 100644
index 0000000..7895736
--- /dev/null
+++ b/include/asm-x86/wrhv.h
@@ -0,0 +1,51 @@
+/*
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License as published by the
+ * Free Software Foundation; either version 2, or (at your option) any
+ * later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Copyright (C) 2009 Wind River Systems, Inc.
+ */
+
+
+#ifndef _ASM_WRHV_H
+#define _ASM_WRHV_H
+
+#define WRHV_IPI_RESCHED          0x17
+#define WRHV_IPI_INV_TLB          0x16
+#define WRHV_IPI_FUNC_CALL        0x15
+#define WRHV_IPI_FUNC_CALL_SINGLE 0x14
+
+#ifdef CONFIG_WRHV
+extern void wrhv_init(void);
+extern void wrhv_boot_config(void);
+extern void wrhv_cpu_workarounds(struct cpuinfo_x86 *);
+extern void wrhv_vtlb_op(unsigned int, unsigned long,
+			 unsigned long, unsigned long);
+extern int wrhv_pci_bus_scan(void);
+extern int wrhv_pci_probeonly;
+extern void wrhv_calibrate_smp_cpus(void);
+
+extern unsigned long __initrd_start, __initrd_end;
+
+DECLARE_PER_CPU(struct clock_event_device, wrhv_clock_events);
+
+/**
+ * For hyp-guest os, int 0 exclusively belongs to hypervisor-provided timer,
+ * If external HW timers are used as system timer(Now it only happens when
+ * hrtimer is enabled), we have to use other int number.
+ **/  
+#define HRTIMER_IRQ_NAME "PIT_Timer"
+#define TIMER_INT_NUM		10
+#define DUMMY_TIMER_INT	12
+#else	/* !CONFIG_WRHV */
+#define wrhv_pci_probeonly	0
+#define TIMER_INT_NUM		0
+#endif /* CONFIG_WRHV */
+
+#endif /* _ASM_WRHV_H */
diff --git a/include/asm-x86/wrhv_serial.h b/include/asm-x86/wrhv_serial.h
new file mode 100644
index 0000000..acb3bad
--- /dev/null
+++ b/include/asm-x86/wrhv_serial.h
@@ -0,0 +1,14 @@
+#ifndef _ASM_X86_WRHV_SERIAL_H
+#define _ASM_X86_WRHV_SERIAL_H
+
+#include <asm/serial.h>
+
+#undef SERIAL_PORT_DFNS
+#define SERIAL_PORT_DFNS			\
+	/* UART CLK   PORT IRQ     FLAGS        */			\
+	{ 0, BASE_BAUD, 0x3F8, 4,  STD_COM_FLAGS },	/* ttyS0 */	\
+	{ 0, BASE_BAUD, 0x2F8, 3,  STD_COM_FLAGS },	/* ttyS1 */	\
+	{ 0, BASE_BAUD, 0x220, 30, STD_COM_FLAGS },	/* ttyS2 */	\
+	{ 0, BASE_BAUD, 0x238, 31, STD_COM_FLAGS },	/* ttyS3 */
+
+#endif /* _ASM_X86_WRHV_SERIAL_H */
-- 
1.6.5.2

