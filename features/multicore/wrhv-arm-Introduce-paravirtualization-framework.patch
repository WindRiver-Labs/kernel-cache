From 1619be0b1120bb83613011a33b9dba59ce903006 Mon Sep 17 00:00:00 2001
From: Jim Somerville <Jim.Somerville@windriver.com>
Date: Wed, 3 Aug 2011 13:09:55 -0400
Subject: [PATCH] wrhv: arm: Introduce paravirtualization framework

These arm specific additions represent additions that are
not part of the reference VBI implementation. Introduce
core guest implementation for arm.

Signed-off-by: Jim Somerville <Jim.Somerville@windriver.com>
---
 Makefile                           |    9 +-
 arch/arm/Kconfig                   |   35 ++
 arch/arm/common/gic.c              |    8 +
 arch/arm/include/asm/arch_vbi.h    |   10 +
 arch/arm/include/asm/assembler.h   |   13 +
 arch/arm/include/asm/cpu-single.h  |   53 +++-
 arch/arm/include/asm/irqflags.h    |   19 ++
 arch/arm/include/asm/mmu.h         |    3 +
 arch/arm/include/asm/mmu_context.h |    8 +-
 arch/arm/include/asm/paravirt.h    |   68 ++++
 arch/arm/include/asm/proc-fns.h    |   13 +-
 arch/arm/include/asm/smp_plat.h    |    4 +
 arch/arm/include/asm/tlbflush.h    |   19 ++
 arch/arm/include/asm/unistd.h      |    6 +
 arch/arm/include/asm/wrhv.h        |   36 +++
 arch/arm/kernel/Makefile           |    6 +-
 arch/arm/kernel/asm-offsets.c      |   28 ++
 arch/arm/kernel/entry-armv.S       |  196 ++++++++++++-
 arch/arm/kernel/entry-common.S     |    2 +-
 arch/arm/kernel/head.S             |   54 +++-
 arch/arm/kernel/paravirt.c         |  130 ++++++++
 arch/arm/kernel/setup.c            |   62 ++++-
 arch/arm/kernel/smp.c              |   23 ++-
 arch/arm/kernel/traps.c            |   15 +-
 arch/arm/kernel/vbi/Makefile       |    2 +-
 arch/arm/kernel/vbi/syscalls.S     |   14 +-
 arch/arm/kernel/vbi/util.c         |   65 ++++
 arch/arm/kernel/vbi/wrhv.c         |  607 ++++++++++++++++++++++++++++++++++++
 arch/arm/kernel/vmlinux-wrhv.lds.S |  254 +++++++++++++++
 arch/arm/mm/Kconfig                |    5 +-
 arch/arm/mm/abort-ev7.S            |   11 +
 arch/arm/mm/context.c              |    7 +-
 arch/arm/mm/mmu.c                  |   96 ++++++
 arch/arm/mm/pabort-v7.S            |   12 +-
 arch/arm/mm/pgd.c                  |   40 +++
 arch/arm/mm/proc-macros.S          |    6 +
 arch/arm/mm/proc-v7.S              |   11 +
 arch/arm/mm/tlb-v7.S               |   16 +
 include/vbi/syscall.h              |    1 +
 39 files changed, 1930 insertions(+), 37 deletions(-)
 create mode 100644 arch/arm/include/asm/paravirt.h
 create mode 100644 arch/arm/include/asm/wrhv.h
 create mode 100644 arch/arm/kernel/paravirt.c
 create mode 100644 arch/arm/kernel/vbi/util.c
 create mode 100644 arch/arm/kernel/vbi/wrhv.c
 create mode 100644 arch/arm/kernel/vmlinux-wrhv.lds.S

diff --git a/Makefile b/Makefile
index 64b1dd6..f086ebd 100644
--- a/Makefile
+++ b/Makefile
@@ -523,6 +523,13 @@ else
 include/config/auto.conf: ;
 endif # $(dot-config)
 
+# Additional ARCH settings for arm
+ifeq ($(ARCH),arm)
+ifdef CONFIG_WRHV
+       export lds-filename-suffix := -wrhv
+endif
+endif
+
 # The all: target is the default when no target is given on the
 # command line.
 # This allow a user to issue only 'make' to build a kernel including modules
@@ -710,7 +717,7 @@ libs-y		:= $(libs-y1) $(libs-y2)
 vmlinux-init := $(head-y) $(init-y)
 vmlinux-main := $(core-y) $(libs-y) $(drivers-y) $(net-y)
 vmlinux-all  := $(vmlinux-init) $(vmlinux-main)
-vmlinux-lds  := arch/$(SRCARCH)/kernel/vmlinux.lds
+vmlinux-lds  := arch/$(SRCARCH)/kernel/vmlinux$(lds-filename-suffix).lds
 export KBUILD_VMLINUX_OBJS := $(vmlinux-all)
 
 # Rule to link vmlinux - also used during CONFIG_KALLSYMS
diff --git a/arch/arm/Kconfig b/arch/arm/Kconfig
index 3c4ff03..d9447bc 100644
--- a/arch/arm/Kconfig
+++ b/arch/arm/Kconfig
@@ -185,12 +185,47 @@ config ARM_L1_CACHE_SHIFT_6
 
 config VECTORS_BASE
 	hex
+	default 0xc0000000 if WRHV
 	default 0xffff0000 if MMU || CPU_HIGH_VECTOR
 	default DRAM_BASE if REMAP_VECTORS_TO_RAM
 	default 0x00000000
 	help
 	  The base address of exception vectors.
 
+menuconfig VIRTUALIZATION
+	bool "Virtualization"
+	---help---
+	  Say Y here to get to see options for enabling the kernel
+	  to run as a virtual machine (guest).
+	  This option alone does not add any kernel code.
+
+	  If you say N, all options in this submenu will be skipped and
+	  disabled.
+
+if VIRTUALIZATION
+
+config PARAVIRT
+        bool "Enable paravirtualization code"
+        default y
+        help
+          This changes the kernel so it can modify itself when it is run
+          under a hypervisor, potentially improving performance significantly
+          over full virtualization.  However, when run without a hypervisor
+          the kernel is theoretically slower and slightly larger.
+
+config PARAVIRT_CLOCK
+        bool
+        default n
+
+config PARAVIRT_DEBUG
+       bool "paravirt-ops debugging"
+       depends on PARAVIRT && DEBUG_KERNEL
+       help
+         Enable to debug paravirt_ops internals.  Specifically, BUG if
+         a paravirt_op is missing when it is called.
+
+endif # VIRTUALIZATION
+
 source "init/Kconfig"
 
 source "kernel/Kconfig.freezer"
diff --git a/arch/arm/common/gic.c b/arch/arm/common/gic.c
index 337741f..1240ba5 100644
--- a/arch/arm/common/gic.c
+++ b/arch/arm/common/gic.c
@@ -175,6 +175,10 @@ void __init gic_cascade_irq(unsigned int gic_nr, unsigned int irq)
 	set_irq_chained_handler(irq, gic_handle_cascade_irq);
 }
 
+#ifdef CONFIG_WRHV
+extern int wrhv_find_direct_interrupt(int);
+#endif
+
 void __init gic_dist_init(unsigned int gic_nr, void __iomem *base,
 			  unsigned int irq_start)
 {
@@ -234,6 +238,10 @@ void __init gic_dist_init(unsigned int gic_nr, void __iomem *base,
 	 * Setup the Linux IRQ subsystem.
 	 */
 	for (i = irq_start; i < gic_data[gic_nr].irq_offset + max_irq; i++) {
+#ifdef CONFIG_WRHV
+		if (wrhv_find_direct_interrupt(i))
+			continue;
+#endif
 		set_irq_chip(i, &gic_chip);
 		set_irq_chip_data(i, &gic_data[gic_nr]);
 		set_irq_handler(i, handle_level_irq);
diff --git a/arch/arm/include/asm/arch_vbi.h b/arch/arm/include/asm/arch_vbi.h
index 79b4cf3..fef1c8b 100644
--- a/arch/arm/include/asm/arch_vbi.h
+++ b/arch/arm/include/asm/arch_vbi.h
@@ -25,6 +25,14 @@
 
 #define __VBI_BYTE_ORDER __VBI_LITTLE_ENDIAN
 
+#define	VBI_MAX_CORES			    8
+
+/* exception defines */
+
+#define ARCH_MAX_INTERRUPT		    32
+#define VBI_ARCH_EXC_TABLE_SIZE		    32
+#define VBI_ARCH_IRQ_TABLE_SIZE		    256
+
 /* VIOAPIC number of entries */
 
 #define VB_VIOAPIC_ENTRIES_SIZE		    64 
@@ -296,6 +304,8 @@ typedef struct
 	uint32_t   dataVal;
 } VBI_BSP_MSG_REPLY;
 
+extern int32_t vbi_load_ctx(void);
+
 #else /*_ASMLANGUAGE */
 
 /*
diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index b5875e7..6c0ab8b 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -76,15 +76,28 @@
  */
 #if __LINUX_ARM_ARCH__ >= 6
 	.macro	disable_irq_notrace
+#ifndef CONFIG_WRHV
 	cpsid	i
+#else
+	cpsid	if
+#endif
 	.endm
 
 	.macro	enable_irq_notrace
+#ifndef CONFIG_WRHV
 	cpsie	i
+#else
+	cpsie	if
+#endif
 	.endm
 #else
 	.macro	disable_irq_notrace
+#ifndef CONFIG_WRHV
 	msr	cpsr_c, #PSR_I_BIT | SVC_MODE
+#else
+	msr	cpsr_c, #PSR_I_BIT | PSR_F_BIT | SVC_MODE
+#endif
+
 	.endm
 
 	.macro	enable_irq_notrace
diff --git a/arch/arm/include/asm/cpu-single.h b/arch/arm/include/asm/cpu-single.h
index f073a6d..a60e551 100644
--- a/arch/arm/include/asm/cpu-single.h
+++ b/arch/arm/include/asm/cpu-single.h
@@ -22,13 +22,49 @@
  * function pointers for this lot.  Otherwise, we can optimise the
  * table away.
  */
-#define cpu_proc_init			__cpu_fn(CPU_NAME,_proc_init)
-#define cpu_proc_fin			__cpu_fn(CPU_NAME,_proc_fin)
-#define cpu_reset			__cpu_fn(CPU_NAME,_reset)
-#define cpu_do_idle			__cpu_fn(CPU_NAME,_do_idle)
-#define cpu_dcache_clean_area		__cpu_fn(CPU_NAME,_dcache_clean_area)
-#define cpu_do_switch_mm		__cpu_fn(CPU_NAME,_switch_mm)
-#define cpu_set_pte_ext			__cpu_fn(CPU_NAME,_set_pte_ext)
+#define cpu_proc_init			__cpu_fn(CPU_NAME, _proc_init)
+#define cpu_proc_fin			__cpu_fn(CPU_NAME, _proc_fin)
+#define cpu_reset			__cpu_fn(CPU_NAME, _reset)
+#define native_cpu_do_idle		__cpu_fn(CPU_NAME, _do_idle)
+#define cpu_dcache_clean_area		__cpu_fn(CPU_NAME, _dcache_clean_area)
+#define native_cpu_do_switch_mm		__cpu_fn(CPU_NAME, _switch_mm)
+#define native_cpu_set_pte_ext		__cpu_fn(CPU_NAME, _set_pte_ext)
+
+extern void native_cpu_do_idle(void);
+extern void native_cpu_do_switch_mm(unsigned long pgd_phys,
+				struct mm_struct *mm);
+extern void native_cpu_set_pte_ext(pte_t *ptep, pte_t pte, unsigned int ext);
+extern void paravirt_do_idle(void);
+extern void paravirt_do_switch_mm(unsigned long pgd_phys, struct mm_struct *mm);
+extern void paravirt_set_pte_ext(pte_t *ptep, pte_t pte, unsigned int ext);
+
+static inline void cpu_do_idle(void)
+{
+#ifdef CONFIG_PARAVIRT
+	paravirt_do_idle();
+#else
+	native_cpu_do_idle();
+#endif
+}
+
+static inline void cpu_do_switch_mm(unsigned long pgd_phys,
+				struct mm_struct *mm)
+{
+#ifdef CONFIG_PARAVIRT
+	paravirt_do_switch_mm(pgd_phys, mm);
+#else
+	native_cpu_do_switch_mm(pgd_phys, mm);
+#endif
+}
+
+static inline void cpu_set_pte_ext(pte_t *ptep, pte_t pte, unsigned int ext)
+{
+#ifdef CONFIG_PARAVIRT
+	paravirt_set_pte_ext(ptep, pte, ext);
+#else
+	native_cpu_set_pte_ext(ptep, pte, ext);
+#endif
+}
 
 #include <asm/page.h>
 
@@ -37,8 +73,5 @@ struct mm_struct;
 /* declare all the functions as extern */
 extern void cpu_proc_init(void);
 extern void cpu_proc_fin(void);
-extern int cpu_do_idle(void);
 extern void cpu_dcache_clean_area(void *, int);
-extern void cpu_do_switch_mm(unsigned long pgd_phys, struct mm_struct *mm);
-extern void cpu_set_pte_ext(pte_t *ptep, pte_t pte, unsigned int ext);
 extern void cpu_reset(unsigned long addr) __attribute__((noreturn));
diff --git a/arch/arm/include/asm/irqflags.h b/arch/arm/include/asm/irqflags.h
index 6d09974..0006e64 100644
--- a/arch/arm/include/asm/irqflags.h
+++ b/arch/arm/include/asm/irqflags.h
@@ -10,6 +10,24 @@
  */
 #if __LINUX_ARM_ARCH__ >= 6
 
+#ifdef CONFIG_WRHV
+/* Fiq interrupts are used in wrhv for indirect interrupt delivery.
+ * Treat fiqs the same as regular direct interrupts with respect to
+ * enable/disable.
+ */
+#define raw_local_irq_save(x)					\
+	({							\
+	__asm__ __volatile__(					\
+	"mrs	%0, cpsr		@ local_irq_save\n"	\
+	"cpsid	if"						\
+	: "=r" (x) : : "memory", "cc");				\
+	})
+
+#define raw_local_irq_enable()  __asm__("cpsie if @ __sti" : : : "memory", "cc")
+#define raw_local_irq_disable() __asm__("cpsid if @ __cli" : : : "memory", "cc")
+#define local_fiq_enable()
+#define local_fiq_disable()
+#else
 #define raw_local_irq_save(x)					\
 	({							\
 	__asm__ __volatile__(					\
@@ -22,6 +40,7 @@
 #define raw_local_irq_disable() __asm__("cpsid i	@ __cli" : : : "memory", "cc")
 #define local_fiq_enable()  __asm__("cpsie f	@ __stf" : : : "memory", "cc")
 #define local_fiq_disable() __asm__("cpsid f	@ __clf" : : : "memory", "cc")
+#endif /* WRHV */
 
 #else
 
diff --git a/arch/arm/include/asm/mmu.h b/arch/arm/include/asm/mmu.h
index 68870c7..02d305d 100644
--- a/arch/arm/include/asm/mmu.h
+++ b/arch/arm/include/asm/mmu.h
@@ -9,6 +9,9 @@ typedef struct {
 	spinlock_t id_lock;
 #endif
 	unsigned int kvm_seq;
+#ifdef CONFIG_WRHV
+	unsigned long vmmu_handle;
+#endif
 } mm_context_t;
 
 #ifdef CONFIG_CPU_HAS_ASID
diff --git a/arch/arm/include/asm/mmu_context.h b/arch/arm/include/asm/mmu_context.h
index a0b3cac..b142846 100644
--- a/arch/arm/include/asm/mmu_context.h
+++ b/arch/arm/include/asm/mmu_context.h
@@ -20,6 +20,10 @@
 #include <asm/proc-fns.h>
 #include <asm-generic/mm_hooks.h>
 
+#ifdef CONFIG_SMP
+DECLARE_PER_CPU(struct mm_struct *, current_mm);
+#endif
+
 void __check_kvm_seq(struct mm_struct *mm);
 
 #ifdef CONFIG_CPU_HAS_ASID
@@ -43,10 +47,6 @@ void __check_kvm_seq(struct mm_struct *mm);
 #define ASID_FIRST_VERSION	(1 << ASID_BITS)
 
 extern unsigned int cpu_last_asid;
-#ifdef CONFIG_SMP
-DECLARE_PER_CPU(struct mm_struct *, current_mm);
-#endif
-
 void __init_new_context(struct task_struct *tsk, struct mm_struct *mm);
 void __new_context(struct mm_struct *mm);
 
diff --git a/arch/arm/include/asm/paravirt.h b/arch/arm/include/asm/paravirt.h
new file mode 100644
index 0000000..8c61687
--- /dev/null
+++ b/arch/arm/include/asm/paravirt.h
@@ -0,0 +1,68 @@
+/*
+ * arm paravirt.h - arm paravirtual operations structures
+ *
+ * Copyright (c) 2011 Wind River Systems, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ * See the GNU General Public License for more details.
+ *
+ */
+
+#ifndef __ARM_ASM_PARAVIRT_H
+#define __ARM_ASM_PARAVIRT_H
+
+#include <linux/kprobes.h>
+
+#ifdef CONFIG_PARAVIRT
+/*
+ * paravirtual operations structures
+ */
+
+struct pv_time_ops {
+	void (*percpu_timer_setup)(void);
+};
+
+struct pv_smp_ops {
+	void (*smp_init_cpus)(void);
+	void (*smp_prepare_cpus)(unsigned int max_cpus);
+	void (*smp_cross_call)(const struct cpumask *mask);
+	int (*boot_secondary)(unsigned int cpu, struct task_struct *idle);
+	void (*platform_secondary_init)(unsigned int cpu);
+};
+
+struct pv_cpu_ops {
+	void (*do_idle)(void);
+};
+
+/* general info */
+struct pv_info {
+	const char *name;
+	int paravirt_enabled;
+};
+
+struct pv_irq_ops {
+	void (*do_IRQ)(struct pt_regs *regs);
+};
+
+struct pv_mmu_ops {
+	void (*MMU_init)(void);
+	void (*do_switch_mm)(unsigned long pgd_phys, struct mm_struct *mm);
+	void (*set_pte_ext)(pte_t *ptep, pte_t pte, unsigned int ext);
+	pgd_t *(*cpu_get_pgd)(void);
+};
+
+extern struct pv_info pv_info;
+extern struct pv_time_ops pv_time_ops;
+extern struct pv_cpu_ops pv_cpu_ops;
+extern struct pv_irq_ops pv_irq_ops;
+extern struct pv_mmu_ops pv_mmu_ops;
+extern struct pv_smp_ops pv_smp_ops;
+
+#endif /* CONFIG_PARAVIRT */
+#endif	/* __ARM_ASM_PARAVIRT_H */
diff --git a/arch/arm/include/asm/proc-fns.h b/arch/arm/include/asm/proc-fns.h
index 8fdae9b..15ea310 100644
--- a/arch/arm/include/asm/proc-fns.h
+++ b/arch/arm/include/asm/proc-fns.h
@@ -259,11 +259,13 @@
 
 #include <asm/memory.h>
 
+extern pgd_t *paravirt_cpu_get_pgd(void);
+
 #ifdef CONFIG_MMU
 
 #define cpu_switch_mm(pgd,mm) cpu_do_switch_mm(virt_to_phys(pgd),mm)
 
-#define cpu_get_pgd()	\
+#define native_cpu_get_pgd()	\
 	({						\
 		unsigned long pg;			\
 		__asm__("mrc	p15, 0, %0, c2, c0, 0"	\
@@ -272,6 +274,15 @@
 		(pgd_t *)phys_to_virt(pg);		\
 	})
 
+static inline pgd_t *cpu_get_pgd(void)
+{
+#ifdef CONFIG_PARAVIRT
+	return paravirt_cpu_get_pgd();
+#else
+	return native_cpu_get_pgd();
+#endif
+}
+
 #endif
 
 #endif /* __ASSEMBLY__ */
diff --git a/arch/arm/include/asm/smp_plat.h b/arch/arm/include/asm/smp_plat.h
index e621530..39bc801 100644
--- a/arch/arm/include/asm/smp_plat.h
+++ b/arch/arm/include/asm/smp_plat.h
@@ -10,7 +10,11 @@
 /* all SMP configurations have the extended CPUID registers */
 static inline int tlb_ops_need_broadcast(void)
 {
+#ifdef CONFIG_WRHV
+	return 1;
+#else
 	return ((read_cpuid_ext(CPUID_EXT_MMFR3) >> 12) & 0xf) < 2;
+#endif
 }
 
 static inline int cache_ops_need_broadcast(void)
diff --git a/arch/arm/include/asm/tlbflush.h b/arch/arm/include/asm/tlbflush.h
index 33b546a..3e6b2d9 100644
--- a/arch/arm/include/asm/tlbflush.h
+++ b/arch/arm/include/asm/tlbflush.h
@@ -10,6 +10,9 @@
 #ifndef _ASMARM_TLBFLUSH_H
 #define _ASMARM_TLBFLUSH_H
 
+#ifdef CONFIG_WRHV
+#include <vbi/syscall.h>
+#endif
 
 #ifndef CONFIG_MMU
 
@@ -322,6 +325,10 @@ static inline void local_flush_tlb_all(void)
 	const int zero = 0;
 	const unsigned int __tlb_flag = __cpu_tlb_flags;
 
+#ifdef CONFIG_WRHV
+	vbi_flush_tlb(0, 0, -1); /* Flush everything */
+#endif
+
 	if (tlb_flag(TLB_WB))
 		dsb();
 
@@ -356,6 +363,10 @@ static inline void local_flush_tlb_mm(struct mm_struct *mm)
 	const int asid = ASID(mm);
 	const unsigned int __tlb_flag = __cpu_tlb_flags;
 
+#ifdef CONFIG_WRHV
+	vbi_flush_tlb(0, 0, -1); /* Flush everything */
+#endif
+
 	if (tlb_flag(TLB_WB))
 		dsb();
 
@@ -403,6 +414,10 @@ local_flush_tlb_page(struct vm_area_struct *vma, unsigned long uaddr)
 	const int zero = 0;
 	const unsigned int __tlb_flag = __cpu_tlb_flags;
 
+#ifdef CONFIG_WRHV
+	vbi_flush_tlb(0, 0, -1); /* Flush everything */
+#endif
+
 	uaddr = (uaddr & PAGE_MASK) | ASID(vma->vm_mm);
 
 	if (tlb_flag(TLB_WB))
@@ -452,6 +467,10 @@ static inline void local_flush_tlb_kernel_page(unsigned long kaddr)
 	const int zero = 0;
 	const unsigned int __tlb_flag = __cpu_tlb_flags;
 
+#ifdef CONFIG_WRHV
+	vbi_flush_tlb(0, 0, -1); /* Flush everything */
+#endif
+
 	kaddr &= PAGE_MASK;
 
 	if (tlb_flag(TLB_WB))
diff --git a/arch/arm/include/asm/unistd.h b/arch/arm/include/asm/unistd.h
index 4390398..61f3a56 100644
--- a/arch/arm/include/asm/unistd.h
+++ b/arch/arm/include/asm/unistd.h
@@ -405,6 +405,12 @@
 
 #define __NR_syscall_max 375
 
+#define VBI_MEM_READ			0x0010
+#define VBI_MEM_WRITE			0x0100
+#define SYS_VBI_VB_SUSPEND		0x10001
+#define SYS_VBI_VB_RESUME		0x20002
+#define SYS_VBI_VB_RESTART		0x40004
+
 /*
  * The following SWIs are ARM private.
  */
diff --git a/arch/arm/include/asm/wrhv.h b/arch/arm/include/asm/wrhv.h
new file mode 100644
index 0000000..10500fb
--- /dev/null
+++ b/arch/arm/include/asm/wrhv.h
@@ -0,0 +1,36 @@
+/*
+ *  This program is free software; you can redistribute it and/or modify it
+ *  under the terms of the GNU General Public License as published by the
+ *  Free Software Foundation; either version 2, or (at your option) any
+ *  later version.
+ *
+ *  This program is distributed in the hope that it will be useful, but
+ *  WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ *  General Public License for more details.
+ *
+ *  Copyright (C) 2011 Wind River Systems, Inc.
+ */
+
+#ifndef __ARM_ASM_WRHV_H
+#define __ARM_ASM_WRHV_H
+
+#ifdef CONFIG_WRHV
+
+#define WRHV_SUPER_EARLY_STACK_SIZE	512
+
+#define FAKE_READ_TLS_REG_UNDEF_INSTR	0xee1d0f72 /* mrc p15,0,r0,c13,c2,3 */
+
+#ifndef __ASSEMBLY__
+
+#include <vbi/vmmu.h>
+
+extern void wrhv_time_init(void);
+extern void wrhv_init_irq(void);
+extern void (*wrhv_machine_init_irq)(void);
+extern void (*wrhv_machine_init_timer)(void);
+extern spinlock_t vmmu_handle_lock;
+
+#endif /* __ASSEMBLY__ */
+#endif /* CONFIG_WRHV */
+#endif /* __ARM_ASM_WRHV_H */
diff --git a/arch/arm/kernel/Makefile b/arch/arm/kernel/Makefile
index 0ab3a46..390d16f 100644
--- a/arch/arm/kernel/Makefile
+++ b/arch/arm/kernel/Makefile
@@ -2,7 +2,7 @@
 # Makefile for the linux kernel.
 #
 
-CPPFLAGS_vmlinux.lds := -DTEXT_OFFSET=$(TEXT_OFFSET)
+CPPFLAGS_vmlinux$(lds-filename-suffix).lds := -DTEXT_OFFSET=$(TEXT_OFFSET)
 AFLAGS_head.o        := -DTEXT_OFFSET=$(TEXT_OFFSET)
 
 ifdef CONFIG_DYNAMIC_FTRACE
@@ -17,6 +17,8 @@ obj-y		:= compat.o elf.o entry-armv.o entry-common.o irq.o \
 		   process.o ptrace.o return_address.o setup.o signal.o \
 		   sys_arm.o stacktrace.o time.o traps.o
 
+obj-$(CONFIG_WRHV)		+= vbi/
+obj-$(CONFIG_PARAVIRT)		+= paravirt.o
 obj-$(CONFIG_LEDS)		+= leds.o
 obj-$(CONFIG_OC_ETM)		+= etm.o
 
@@ -60,7 +62,7 @@ head-y			:= head$(MMUEXT).o
 obj-$(CONFIG_DEBUG_LL)	+= debug.o
 obj-$(CONFIG_EARLY_PRINTK)	+= early_printk.o
 
-extra-y := $(head-y) init_task.o vmlinux.lds
+extra-y := $(head-y) init_task.o vmlinux$(lds-filename-suffix).lds
 
 obj-$(CONFIG_AUDIT)		+= audit.o
 compat-obj-$(CONFIG_AUDIT)	+= compat_audit.o
diff --git a/arch/arm/kernel/asm-offsets.c b/arch/arm/kernel/asm-offsets.c
index 8835115..0f21990 100644
--- a/arch/arm/kernel/asm-offsets.c
+++ b/arch/arm/kernel/asm-offsets.c
@@ -19,6 +19,13 @@
 #include <asm/procinfo.h>
 #include <linux/kbuild.h>
 
+#ifdef CONFIG_WRHV
+#include <vbi/types.h>
+#include <asm/arch_vbi.h>
+#include <vbi/interface.h>
+#include <vbi/vmmu.h>
+#endif
+
 /*
  * Make sure that the compiler and target are compatible.
  */
@@ -117,5 +124,26 @@ int main(void)
   DEFINE(DMA_BIDIRECTIONAL,	DMA_BIDIRECTIONAL);
   DEFINE(DMA_TO_DEVICE,		DMA_TO_DEVICE);
   DEFINE(DMA_FROM_DEVICE,	DMA_FROM_DEVICE);
+#ifdef CONFIG_WRHV
+  DEFINE(MM_VMMU_HANDLE,	offsetof(struct mm_struct,
+					context.vmmu_handle));
+  DEFINE(CTRL_SPACE_ASID,	offsetof(struct vb_arch_ctrl_regs, asid));
+  DEFINE(CTRL_SPACE_VMMUHANDLE,	offsetof(struct vb_arch_ctrl_regs,
+					vmmu_handle));
+  DEFINE(STAT_SPACE_IFAR,	offsetof(struct vb_arch_stat_regs, ifar));
+  DEFINE(STAT_SPACE_IFSR,	offsetof(struct vb_arch_stat_regs, ifsr));
+  DEFINE(STAT_SPACE_DFAR,	offsetof(struct vb_arch_stat_regs, dfar));
+  DEFINE(STAT_SPACE_DFSR,	offsetof(struct vb_arch_stat_regs, dfsr));
+  DEFINE(STAT_SPACE_ABT_LR,	offsetof(struct vb_arch_stat_regs,
+					modeSpecificReg[ABT_MODE & 0xf].lr));
+  DEFINE(STAT_SPACE_ABT_SPSR,	offsetof(struct vb_arch_stat_regs,
+					modeSpecificReg[ABT_MODE & 0xf].spsr));
+  DEFINE(CTRL_SPACE_PTR,	offsetof(struct vb_config, vb_control));
+  DEFINE(WRHV_COREID,		offsetof(struct vb_config, coreId));
+  DEFINE(VMMU_CFG_ADDR,		offsetof(VMMU_CONFIG, addr));
+  DEFINE(VMMU_CFG_FLUSH_TYPE,	offsetof(VMMU_CONFIG, flush_type));
+  DEFINE(VMMU_CFG_ASID,		offsetof(VMMU_CONFIG, asid));
+  DEFINE(VMMU_CFG_VMMU_HANDLE,	offsetof(VMMU_CONFIG, vmmu_handle));
+#endif
   return 0; 
 }
diff --git a/arch/arm/kernel/entry-armv.S b/arch/arm/kernel/entry-armv.S
index ba654fa..a288111 100644
--- a/arch/arm/kernel/entry-armv.S
+++ b/arch/arm/kernel/entry-armv.S
@@ -23,6 +23,10 @@
 #include <asm/unwind.h>
 #include <asm/unistd.h>
 
+#ifdef CONFIG_WRHV
+#include <asm/wrhv.h>
+#endif
+
 #include "entry-header.S"
 
 /*
@@ -176,6 +180,10 @@ __dabt_svc:
 	mrs	r9, cpsr
 	tst	r3, #PSR_I_BIT
 	biceq	r9, r9, #PSR_I_BIT
+#ifdef CONFIG_WRHV
+	tst	r3, #PSR_F_BIT
+	biceq	r9, r9, #PSR_F_BIT
+#endif
 
 	@
 	@ Call the processor-specific abort handler:
@@ -245,6 +253,43 @@ ENDPROC(__irq_svc)
 
 	.ltorg
 
+#ifdef CONFIG_WRHV
+	.align	5
+@ A clone of __irq_svc but with a different handler call out
+__fiq_svc:
+	svc_entry
+
+#ifdef CONFIG_PREEMPT
+	get_thread_info tsk
+	ldr	r8, [tsk, #TI_PREEMPT]		@ get preempt count
+	add	r7, r8, #1			@ increment it
+	str	r7, [tsk, #TI_PREEMPT]
+#endif
+	mov	r0, sp				@ pass parm *pt_regs
+	stmfd	sp!, {r7-r9, lr}
+	bl	paravirt_do_IRQ
+	ldmfd	sp!, {r7-r9, lr}
+
+#ifdef CONFIG_PREEMPT
+	str	r8, [tsk, #TI_PREEMPT]		@ restore preempt count
+	ldr	r0, [tsk, #TI_FLAGS]		@ get flags
+	teq	r8, #0				@ if preempt count != 0
+	movne	r0, #0				@ force flags to 0
+	tst	r0, #_TIF_NEED_RESCHED
+	blne	svc_preempt
+#endif
+	ldr	r4, [sp, #S_PSR]		@ irqs are already disabled
+#ifdef CONFIG_TRACE_IRQFLAGS
+	tst	r4, #PSR_I_BIT
+	bleq	trace_hardirqs_on
+#endif
+	svc_exit r4				@ return from exception
+ UNWIND(.fnend		)
+ENDPROC(__fiq_svc)
+
+	.ltorg
+#endif /* CONFIG_WRHV */
+
 #ifdef CONFIG_PREEMPT
 svc_preempt:
 	mov	r8, lr
@@ -311,7 +356,10 @@ __pabt_svc:
 	mrs	r9, cpsr
 	tst	r3, #PSR_I_BIT
 	biceq	r9, r9, #PSR_I_BIT
-
+#ifdef CONFIG_WRHV
+	tst	r3, #PSR_F_BIT
+	biceq	r9, r9, #PSR_F_BIT
+#endif
 	mov	r0, r2			@ pass address of aborted instruction.
 #ifdef MULTI_PABORT
 	ldr	r4, .LCprocfns
@@ -387,7 +435,11 @@ ENDPROC(__pabt_svc)
 	@
 	@ Enable the alignment trap while in kernel mode
 	@
+#ifndef CONFIG_WRHV
 	alignment_trap r0
+#else
+	@ Alignment is controlled by the hypervisor
+#endif
 
 	@
 	@ Clear FP to mark the first stack frame
@@ -476,6 +528,43 @@ ENDPROC(__irq_usr)
 
 	.ltorg
 
+#ifdef CONFIG_WRHV
+	.align	5
+__fiq_usr:
+	usr_entry
+	kuser_cmpxchg_check
+
+	get_thread_info tsk
+#ifdef CONFIG_PREEMPT
+	ldr	r8, [tsk, #TI_PREEMPT]		@ get preempt count
+	add	r7, r8, #1			@ increment it
+	str	r7, [tsk, #TI_PREEMPT]
+#endif
+	mov	r0, sp				@ pass parm *pt_regs
+	stmfd	sp!, {r7-r9, lr}
+	bl	paravirt_do_IRQ
+	ldmfd	sp!, {r7-r9, lr}
+
+#ifdef CONFIG_PREEMPT
+	ldr	r0, [tsk, #TI_PREEMPT]
+	str	r8, [tsk, #TI_PREEMPT]
+	teq	r0, r7
+ ARM(	strne	r0, [r0, -r0]	)
+ THUMB(	movne	r0, #0		)
+ THUMB(	strne	r0, [r0]	)
+#endif
+#ifdef CONFIG_TRACE_IRQFLAGS
+	bl	trace_hardirqs_on
+#endif
+
+	mov	why, #0
+	b	ret_to_user
+ UNWIND(.fnend		)
+ENDPROC(__irq_usr)
+
+	.ltorg
+#endif /* CONFIG_WRHV */
+
 	.align	5
 __und_usr:
 	usr_entry
@@ -742,7 +831,11 @@ ENTRY(__switch_to)
 #if defined(CONFIG_HAS_TLS_REG)
 	mcr	p15, 0, r3, c13, c0, 3		@ set TLS register
 #elif !defined(CONFIG_TLS_REG_EMUL)
+#ifdef CONFIG_WRHV
+	ldr	r4, =CONFIG_VECTORS_BASE+0xfff
+#else
 	mov	r4, #0xffff0fff
+#endif
 	str	r3, [r4, #-15]			@ TLS val at 0xffff0ff0
 #endif
 #ifdef CONFIG_CPU_USE_DOMAINS
@@ -1013,8 +1106,15 @@ __kuser_get_tls:				@ 0xffff0fe0
 #if !defined(CONFIG_HAS_TLS_REG) && !defined(CONFIG_TLS_REG_EMUL)
 	ldr	r0, [pc, #(16 - 8)]		@ TLS stored at 0xffff0ff0
 #else
+#ifdef CONFIG_WRHV
+	/* fake an undefined instruction, and let arm_mrc_hook in
+	 * arch/arm/kernel/traps.c handle it
+	 */
+	.word	FAKE_READ_TLS_REG_UNDEF_INSTR
+#else
 	mrc	p15, 0, r0, c13, c0, 3		@ read TLS register
 #endif
+#endif
 	usr_ret	lr
 
 	.rep	5
@@ -1094,6 +1194,56 @@ ENDPROC(vector_\name)
 1:
 	.endm
 
+#ifdef CONFIG_WRHV
+	.macro	wrhv_vector_abt_stub, name, mode, correction=0
+	.align	5
+
+vector_\name:
+
+	@
+	@ Save r0, lr_<exception> (parent PC) and spsr_<exception>
+	@ (parent CPSR)
+	@
+	str	r0, [sp, #0]
+	ldr	r0, LCstatptr		@ keep this var close as we can't
+					@ afford another trap here
+	ldr	r0, [r0, #STAT_SPACE_ABT_LR]
+	.if \correction
+	sub	r0, r0, #\correction
+	.endif
+	str	r0, [sp, #4]		@ save corrected lr
+
+	@
+	@ Prepare for SVC32 mode.  IRQs remain disabled.
+	@
+	mrs	r0, cpsr
+	eor	r0, r0, #(\mode ^ SVC_MODE | PSR_ISETSTATE)
+	msr	spsr_cxsf, r0
+
+	@ Save spsr
+	ldr	r0, LCstatptr
+	ldr	r0, [r0, #STAT_SPACE_ABT_SPSR]
+	str	r0, [sp, #8]		@ save spsr
+
+	@
+	@ the branch table must immediately follow this code
+	@
+	@ be careful how lr is used here, as a data or prefetch vtlb miss
+	@ in the hypervisor may trample it
+	and	lr, r0, #0x0f
+	mov	r0, sp
+ THUMB(	adr	r0, 1f			)
+ THUMB(	ldr	lr, [r0, lr, lsl #2]	)
+ ARM(	ldr	lr, [pc, lr, lsl #2]	)
+	movs	pc, lr			@ branch to handler in SVC mode
+ENDPROC(vector_\name)
+
+	.align	2
+	@ handler addresses follow this label
+1:
+	.endm
+#endif
+
 	.globl	__stubs_start
 __stubs_start:
 /*
@@ -1118,11 +1268,39 @@ __stubs_start:
 	.long	__irq_invalid			@  e
 	.long	__irq_invalid			@  f
 
+#ifdef CONFIG_WRHV
+/*
+ * FIQ Interrupt dispatcher
+ */
+	vector_stub	fiq, FIQ_MODE, 4
+
+	.long	__fiq_usr			@  0  (USR_26 / USR_32)
+	.long	__irq_invalid			@  1  (FIQ_26 / FIQ_32)
+	.long	__irq_invalid			@  1  (IRQ_26 / IRQ_32)
+	.long	__fiq_svc			@  3  (SVC_26 / SVC_32)
+	.long	__irq_invalid			@  4
+	.long	__irq_invalid			@  5
+	.long	__irq_invalid			@  6
+	.long	__irq_invalid			@  7
+	.long	__irq_invalid			@  8
+	.long	__irq_invalid			@  9
+	.long	__irq_invalid			@  a
+	.long	__irq_invalid			@  b
+	.long	__irq_invalid			@  c
+	.long	__irq_invalid			@  d
+	.long	__irq_invalid			@  e
+	.long	__irq_invalid			@  f
+#endif
+
 /*
  * Data abort dispatcher
  * Enter in ABT mode, spsr = USR CPSR, lr = USR PC
  */
-	vector_stub	dabt, ABT_MODE, 8
+#ifdef CONFIG_WRHV
+	wrhv_vector_abt_stub	dabt, ABT_MODE, 8
+#else
+	vector_stub		dabt, ABT_MODE, 8
+#endif
 
 	.long	__dabt_usr			@  0  (USR_26 / USR_32)
 	.long	__dabt_invalid			@  1  (FIQ_26 / FIQ_32)
@@ -1145,7 +1323,11 @@ __stubs_start:
  * Prefetch abort dispatcher
  * Enter in ABT mode, spsr = USR CPSR, lr = USR PC
  */
-	vector_stub	pabt, ABT_MODE, 4
+#ifdef CONFIG_WRHV
+	wrhv_vector_abt_stub	pabt, ABT_MODE, 4
+#else
+	vector_stub		pabt, ABT_MODE, 4
+#endif
 
 	.long	__pabt_usr			@  0 (USR_26 / USR_32)
 	.long	__pabt_invalid			@  1 (FIQ_26 / FIQ_32)
@@ -1189,6 +1371,7 @@ __stubs_start:
 
 	.align	5
 
+#ifndef CONFIG_WRHV
 /*=============================================================================
  * Undefined FIQs
  *-----------------------------------------------------------------------------
@@ -1202,6 +1385,7 @@ __stubs_start:
 vector_fiq:
 	disable_fiq
 	subs	pc, lr, #4
+#endif /* WRHV */
 
 /*=============================================================================
  * Address exception handler
@@ -1222,6 +1406,12 @@ vector_addrexcptn:
 .LCvswi:
 	.word	vector_swi
 
+#ifdef CONFIG_WRHV
+	.globl	LCstatptr
+LCstatptr:
+	.word	0x00000000	@ to be filled in from wr_status content
+#endif
+
 	.globl	__stubs_end
 __stubs_end:
 
diff --git a/arch/arm/kernel/entry-common.S b/arch/arm/kernel/entry-common.S
index 28a3ab8..7252517 100644
--- a/arch/arm/kernel/entry-common.S
+++ b/arch/arm/kernel/entry-common.S
@@ -256,7 +256,7 @@ ENTRY(vector_swi)
 
 #endif
 
-#ifdef CONFIG_ALIGNMENT_TRAP
+#if defined(CONFIG_ALIGNMENT_TRAP) && !defined(CONFIG_WRHV)
 	ldr	ip, __cr_alignment
 	ldr	ip, [ip]
 	mcr	p15, 0, ip, c1, c0		@ update control register
diff --git a/arch/arm/kernel/head.S b/arch/arm/kernel/head.S
index 82ea924..1d69a6b 100644
--- a/arch/arm/kernel/head.S
+++ b/arch/arm/kernel/head.S
@@ -22,6 +22,10 @@
 #include <asm/thread_info.h>
 #include <asm/system.h>
 
+#ifdef CONFIG_WRHV
+#include <asm/wrhv.h>
+#endif
+
 #if (PHYS_OFFSET & 0x001fffff)
 #error "PHYS_OFFSET must be at an even 2MiB boundary!"
 #endif
@@ -79,6 +83,22 @@ ENTRY(stext)
 	setmode	PSR_F_BIT | PSR_I_BIT | SVC_MODE, r9 @ ensure svc mode
 						@ and irqs disabled
 	mrc	p15, 0, r9, c0, c0		@ get processor id
+#ifdef CONFIG_WRHV
+#ifdef CONFIG_SMP
+	ldr	r1, [r0, #WRHV_COREID]		@ if we're core 0 then keep
+	cmp	r1, #0				@   doing bootup, else we are a
+	bne	secondary_startup		@   secondary processor
+#endif
+	ldr	r1, =wr_config
+	str	r0, [r1]			@ save pointer to config info
+
+	@ Force machine type until we find a better way to be told what
+	@ platform we are running on.  No U-boot here to tell us.  So just
+	@ assume we are running on the first (and likely only) platform that
+	@ has been linked in to our load.
+	ldr	r0, =__arch_info_begin
+	ldr	r1, [r0]
+#endif
 	bl	__lookup_processor_type		@ r5=procinfo r9=cpuid
 	movs	r10, r5				@ invalid processor (r5=0)?
 	beq	__error_p			@ yes, error 'p'
@@ -149,8 +169,12 @@ __secondary_data:
 	.long	secondary_data
 	.long	__secondary_switched
 #endif /* defined(CONFIG_SMP) */
-
-
+#ifdef CONFIG_WRHV
+        .align  2
+        .type   __wrhv_data, %object
+__wrhv_data:
+	.long   BSYM(wrhv_super_early_stack) + WRHV_SUPER_EARLY_STACK_SIZE @ sp
+#endif
 
 /*
  * Setup common bits before finally enabling the MMU.  Essentially
@@ -177,8 +201,22 @@ __enable_mmu:
 		      domain_val(DOMAIN_TABLE, DOMAIN_MANAGER) | \
 		      domain_val(DOMAIN_IO, DOMAIN_CLIENT))
 	mcr	p15, 0, r5, c3, c0, 0		@ load domain access register
+#ifndef CONFIG_WRHV
 	mcr	p15, 0, r4, c2, c0, 0		@ load page table pointer
 	b	__turn_mmu_on
+#else  /* WRHV - turn on the vmmu */
+	@ Must preserve registers r0, r1, r13
+	@ On entry, r4 contains the pointer to the page tables
+	@ r7 must be preserved for secondary processor startup
+	mov	r12, r13			@ r13 = sp, preserve it in reg
+	adr	r3, BSYM(__wrhv_data)
+	ldmia	r3, {sp}
+	stmfd	sp!, {r0, r1, r7, r12}
+	mov	r0, r4
+	bl	wrhv_load_initial_vmmu
+	ldmfd	sp!, {r0, r1, r7, r12}
+	mov	pc, r12
+#endif
 ENDPROC(__enable_mmu)
 
 /*
@@ -330,6 +368,18 @@ __create_page_tables:
 	add	r0, r4, #0xd8000000 >> 18
 	str	r3, [r0]
 #endif
+#ifdef CONFIG_WRHV
+	/*
+	 * Map in the config/control/status pages for early startup.
+	 * This should get overwritten in paging_init for proper size
+	 * and protection.
+	 */
+	ldr	r6, =wr_config
+	ldr	r6, [r6]			@ r6 now points to config
+	add	r0, r4, r6, lsr #18
+	orr	r3, r7, r6
+	str	r3, [r0]
+#endif
 #endif
 	mov	pc, lr
 ENDPROC(__create_page_tables)
diff --git a/arch/arm/kernel/paravirt.c b/arch/arm/kernel/paravirt.c
new file mode 100644
index 0000000..81f84ab
--- /dev/null
+++ b/arch/arm/kernel/paravirt.c
@@ -0,0 +1,130 @@
+/*  Paravirtualization interfaces
+
+    This program is free software; you can redistribute it and/or modify
+    it under the terms of the GNU General Public License as published by
+    the Free Software Foundation; either version 2 of the License, or
+    (at your option) any later version.
+
+    This program is distributed in the hope that it will be useful,
+    but WITHOUT ANY WARRANTY; without even the implied warranty of
+    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+    GNU General Public License for more details.
+
+    You should have received a copy of the GNU General Public License
+    along with this program; if not, write to the Free Software
+    Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
+
+    Copyright (C) 2011 Wind River Systems, Inc.
+
+*/
+
+#include <linux/module.h>
+#include <asm/setup.h>
+#include <asm/paravirt.h>
+
+#ifdef CONFIG_WRHV
+extern void wrhv_init(void);
+#endif
+
+/* paravirt init */
+void paravirt_init(void)
+{
+#ifdef CONFIG_WRHV
+	wrhv_init();
+#endif
+}
+
+struct pv_info pv_info = {
+	.name = "bare hardware",
+	.paravirt_enabled = 0,
+};
+
+/* default native operations */
+struct pv_time_ops pv_time_ops = {
+};
+
+struct pv_irq_ops pv_irq_ops = {
+};
+
+struct pv_cpu_ops pv_cpu_ops = {
+};
+
+struct pv_mmu_ops pv_mmu_ops = {
+};
+
+struct pv_smp_ops pv_smp_ops = {
+};
+
+
+/* pv_irq_ops */
+void paravirt_do_IRQ(struct pt_regs *regs)
+{
+	pv_irq_ops.do_IRQ(regs);
+}
+
+void paravirt_do_idle(void)
+{
+	pv_cpu_ops.do_idle();
+}
+
+void __init paravirt_MMU_init(void)
+{
+	pv_mmu_ops.MMU_init();
+}
+
+void paravirt_set_pte_ext(pte_t *ptep, pte_t pte, unsigned int ext)
+{
+	pv_mmu_ops.set_pte_ext(ptep, pte, ext);
+}
+
+void paravirt_do_switch_mm(unsigned long pgd_phys, struct mm_struct *mm)
+{
+	pv_mmu_ops.do_switch_mm(pgd_phys, mm);
+}
+
+pgd_t *paravirt_cpu_get_pgd(void)
+{
+	return pv_mmu_ops.cpu_get_pgd();
+}
+
+inline int paravirt_enabled(void)
+{
+	return pv_info.paravirt_enabled;
+}
+
+void paravirt_smp_init_cpus(void)
+{
+	pv_smp_ops.smp_init_cpus();
+}
+
+void paravirt_smp_prepare_cpus(unsigned int max_cpus)
+{
+	pv_smp_ops.smp_prepare_cpus(max_cpus);
+}
+
+int paravirt_boot_secondary(unsigned int cpu, struct task_struct *idle)
+{
+	return pv_smp_ops.boot_secondary(cpu, idle);
+}
+
+void paravirt_platform_secondary_init(unsigned int cpu)
+{
+	pv_smp_ops.platform_secondary_init(cpu);
+}
+
+void paravirt_smp_cross_call(const struct cpumask *mask)
+{
+	pv_smp_ops.smp_cross_call(mask);
+}
+
+void paravirt_percpu_timer_setup(void)
+{
+	pv_time_ops.percpu_timer_setup();
+}
+
+EXPORT_SYMBOL(pv_info);
+EXPORT_SYMBOL(pv_time_ops);
+EXPORT_SYMBOL(pv_cpu_ops);
+EXPORT_SYMBOL(pv_mmu_ops);
+EXPORT_SYMBOL(pv_irq_ops);
+EXPORT_SYMBOL(pv_smp_ops);
diff --git a/arch/arm/kernel/setup.c b/arch/arm/kernel/setup.c
index b218e8d..8e17d19 100644
--- a/arch/arm/kernel/setup.c
+++ b/arch/arm/kernel/setup.c
@@ -48,6 +48,12 @@
 #include "atags.h"
 #include "tcm.h"
 
+#ifdef CONFIG_WRHV
+#include <vbi/syscall.h>
+#include <vbi/vmmu.h>
+#include <asm/wrhv.h>
+#endif
+
 #ifndef MEM_SIZE
 #define MEM_SIZE	(16*1024*1024)
 #endif
@@ -115,6 +121,9 @@ struct stack {
 	u32 irq[3];
 	u32 abt[3];
 	u32 und[3];
+#ifdef CONFIG_WRHV
+	u32 fiq[3];
+#endif
 } ____cacheline_aligned;
 
 static struct stack stacks[NR_CPUS];
@@ -358,7 +367,14 @@ void cpu_init(void)
 	"msr	cpsr_c, %5\n\t"
 	"add	r14, %0, %6\n\t"
 	"mov	sp, r14\n\t"
+#ifndef CONFIG_WRHV
 	"msr	cpsr_c, %7"
+#else
+	"msr	cpsr_c, %7\n\t"
+	"add	r14, %0, %8\n\t"
+	"mov	sp, r14\n\t"
+	"msr	cpsr_c, %9"
+#endif
 	    :
 	    : "r" (stk),
 	      PLC (PSR_F_BIT | PSR_I_BIT | IRQ_MODE),
@@ -367,6 +383,10 @@ void cpu_init(void)
 	      "I" (offsetof(struct stack, abt[0])),
 	      PLC (PSR_F_BIT | PSR_I_BIT | UND_MODE),
 	      "I" (offsetof(struct stack, und[0])),
+#ifdef CONFIG_WRHV
+	      PLC (PSR_F_BIT | PSR_I_BIT | FIQ_MODE),
+	      "I" (offsetof(struct stack, fiq[0])),
+#endif
 	      PLC (PSR_F_BIT | PSR_I_BIT | SVC_MODE)
 	    : "r14");
 }
@@ -390,7 +410,11 @@ static struct machine_desc * __init setup_machine(unsigned int nr)
 	return list;
 }
 
+#ifndef CONFIG_WRHV
 static int __init arm_add_memory(unsigned long start, unsigned long size)
+#else
+int __init arm_add_memory(unsigned long start, unsigned long size)
+#endif
 {
 	struct membank *bank = &meminfo.bank[meminfo.nr_banks];
 
@@ -606,6 +630,7 @@ static int __init parse_tag_cmdline(const struct tag *tag)
 
 __tagtable(ATAG_CMDLINE, parse_tag_cmdline);
 
+#ifndef CONFIG_WRHV
 /*
  * Scan the tag table for this tag, and call its parse function.
  * The tag table is built by the linker from all the __tagtable
@@ -654,6 +679,7 @@ static struct init_tags {
 	{ MEM_SIZE, PHYS_OFFSET },
 	{ 0, ATAG_NONE }
 };
+#endif
 
 static void (*init_machine)(void) __initdata;
 
@@ -666,11 +692,25 @@ static int __init customize_machine(void)
 }
 arch_initcall(customize_machine);
 
+#ifdef CONFIG_PARAVIRT
+extern void paravirt_init(void);
+extern void paravirt_MMU_init(void);
+#endif
+
 void __init setup_arch(char **cmdline_p)
 {
+#ifndef CONFIG_WRHV
 	struct tag *tags = (struct tag *)&init_tags;
-	struct machine_desc *mdesc;
 	char *from = default_command_line;
+#endif
+	struct machine_desc *mdesc;
+
+	/*
+	 * initialize paravirtual operations
+	 */
+#ifdef CONFIG_PARAVIRT
+	paravirt_init();
+#endif
 
 	unwind_init();
 
@@ -681,6 +721,7 @@ void __init setup_arch(char **cmdline_p)
 	if (mdesc->soft_reboot)
 		reboot_setup("s");
 
+#ifndef CONFIG_WRHV
 	if (__atags_pointer)
 		tags = phys_to_virt(__atags_pointer);
 	else if (mdesc->boot_params)
@@ -704,14 +745,24 @@ void __init setup_arch(char **cmdline_p)
 		save_atags(tags);
 		parse_tags(tags);
 	}
+#else
+	wrhv_machine_init_timer = mdesc->timer->init;
+	mdesc->timer->init = wrhv_time_init;
+	mdesc->init_machine = NULL;
+	if (mdesc->fixup)
+		mdesc->fixup(mdesc, NULL, NULL, &meminfo);
+	paravirt_MMU_init();
+#endif
 
 	init_mm.start_code = (unsigned long) _text;
 	init_mm.end_code   = (unsigned long) _etext;
 	init_mm.end_data   = (unsigned long) _edata;
 	init_mm.brk	   = (unsigned long) _end;
 
+#ifndef CONFIG_WRHV
 	/* parse_early_param needs a boot_command_line */
 	strlcpy(boot_command_line, from, COMMAND_LINE_SIZE);
+#endif
 
 	/* populate cmd_line too for later use, preserving boot_command_line */
 	strlcpy(cmd_line, boot_command_line, COMMAND_LINE_SIZE);
@@ -732,7 +783,12 @@ void __init setup_arch(char **cmdline_p)
 	/*
 	 * Set up various architecture-specific pointers
 	 */
+#ifdef CONFIG_WRHV
+	wrhv_machine_init_irq = mdesc->init_irq;
+	init_arch_irq = wrhv_init_irq;
+#else
 	init_arch_irq = mdesc->init_irq;
+#endif
 	system_timer = mdesc->timer;
 	init_machine = mdesc->init_machine;
 
@@ -744,6 +800,10 @@ void __init setup_arch(char **cmdline_p)
 #endif
 #endif
 	early_trap_init();
+#ifdef CONFIG_WRHV
+	vbi_set_exc_base((void *)CONFIG_VECTORS_BASE);
+#endif
+
 }
 
 
diff --git a/arch/arm/kernel/smp.c b/arch/arm/kernel/smp.c
index e4722a2..f28d3b2 100644
--- a/arch/arm/kernel/smp.c
+++ b/arch/arm/kernel/smp.c
@@ -39,6 +39,8 @@
 #include <asm/localtimer.h>
 #include <asm/smp_plat.h>
 
+extern void paravirt_percpu_timer_setup(void);
+
 /*
  * as from 2.5, kernels no longer have an init_tasks structure
  * so we need some other way of telling a new secondary core
@@ -72,8 +74,10 @@ int __cpuinit __cpu_up(unsigned int cpu)
 {
 	struct cpuinfo_arm *ci = &per_cpu(cpu_data, cpu);
 	struct task_struct *idle = ci->idle;
+#ifndef CONFIG_WRHV
 	pgd_t *pgd;
 	pmd_t *pmd;
+#endif
 	int ret;
 
 	/*
@@ -95,6 +99,7 @@ int __cpuinit __cpu_up(unsigned int cpu)
 		init_idle(idle, cpu);
 	}
 
+#ifndef CONFIG_WRHV
 	/*
 	 * Allocate initial page tables to allow the new CPU to
 	 * enable the MMU safely.  This essentially means a set
@@ -107,13 +112,18 @@ int __cpuinit __cpu_up(unsigned int cpu)
 		     PMD_TYPE_SECT | PMD_SECT_AP_WRITE);
 	flush_pmd_entry(pmd);
 	outer_clean_range(__pa(pmd), __pa(pmd + 1));
+#endif
 
 	/*
 	 * We need to tell the secondary core where to find
 	 * its stack and the page tables.
 	 */
 	secondary_data.stack = task_stack_page(idle) + THREAD_START_SP;
+#ifdef CONFIG_WRHV
+	secondary_data.pgdir = init_mm.context.vmmu_handle;
+#else
 	secondary_data.pgdir = virt_to_phys(pgd);
+#endif
 	__cpuc_flush_dcache_area(&secondary_data, sizeof(secondary_data));
 	outer_clean_range(__pa(&secondary_data), __pa(&secondary_data + 1));
 
@@ -144,9 +154,11 @@ int __cpuinit __cpu_up(unsigned int cpu)
 	secondary_data.stack = NULL;
 	secondary_data.pgdir = 0;
 
+#ifndef CONFIG_WRHV
 	*pmd = __pmd(0);
 	clean_pmd_entry(pmd);
 	pgd_free(&init_mm, pgd);
+#endif
 
 	if (ret) {
 		printk(KERN_CRIT "CPU%u: processor failed to boot\n", cpu);
@@ -464,7 +476,7 @@ static void local_timer_setup(struct clock_event_device *evt)
 }
 #endif
 
-void __cpuinit percpu_timer_setup(void)
+void __cpuinit native_percpu_timer_setup(void)
 {
 	unsigned int cpu = smp_processor_id();
 	struct clock_event_device *evt = &per_cpu(percpu_clockevent, cpu);
@@ -474,6 +486,15 @@ void __cpuinit percpu_timer_setup(void)
 	local_timer_setup(evt);
 }
 
+void __cpuinit percpu_timer_setup(void)
+{
+#ifdef CONFIG_PARAVIRT
+	paravirt_percpu_timer_setup();
+#else
+	native_percpu_timer_setup();
+#endif
+}
+
 static DEFINE_SPINLOCK(stop_lock);
 
 /*
diff --git a/arch/arm/kernel/traps.c b/arch/arm/kernel/traps.c
index 682a178..ee7d248 100644
--- a/arch/arm/kernel/traps.c
+++ b/arch/arm/kernel/traps.c
@@ -32,6 +32,10 @@
 #include <asm/traps.h>
 #include <asm/unwind.h>
 
+#ifdef CONFIG_WRHV
+#include <asm/wrhv.h>
+#endif
+
 #include "ptrace.h"
 #include "signal.h"
 
@@ -539,8 +543,12 @@ asmlinkage int arm_syscall(int no, struct pt_regs *regs)
 		 * The user helper at 0xffff0fe0 must be used instead.
 		 * (see entry-armv.S for details)
 		 */
+#ifdef CONFIG_WRHV
+		*((unsigned int *)(CONFIG_VECTORS_BASE + 0xff0)) = regs->ARM_r0;
+#else
 		*((unsigned int *)0xffff0ff0) = regs->ARM_r0;
 #endif
+#endif
 		return 0;
 
 #ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG
@@ -650,8 +658,13 @@ static int get_tp_trap(struct pt_regs *regs, unsigned int instr)
 }
 
 static struct undef_hook arm_mrc_hook = {
+#ifdef CONFIG_WRHV
+	.instr_mask	= 0xffffffff,
+	.instr_val	= FAKE_READ_TLS_REG_UNDEF_INSTR,
+#else
 	.instr_mask	= 0x0fff0fff,
 	.instr_val	= 0x0e1d0f70,
+#endif
 	.cpsr_mask	= PSR_T_BIT,
 	.cpsr_val	= 0,
 	.fn		= get_tp_trap,
@@ -757,7 +770,7 @@ void __init trap_init(void)
 
 void __init early_trap_init(void)
 {
-#if defined(CONFIG_CPU_USE_DOMAINS)
+#if defined(CONFIG_CPU_USE_DOMAINS) || defined(CONFIG_WRHV)
 	unsigned long vectors = CONFIG_VECTORS_BASE;
 #else
 	unsigned long vectors = (unsigned long)vectors_page;
diff --git a/arch/arm/kernel/vbi/Makefile b/arch/arm/kernel/vbi/Makefile
index 6639318..ab54d86 100644
--- a/arch/arm/kernel/vbi/Makefile
+++ b/arch/arm/kernel/vbi/Makefile
@@ -2,4 +2,4 @@
 # Makefile for the vbi arm.
 #
 
-obj-y		:= syscalls.o wrhv.o show.o
+obj-y		:= syscalls.o wrhv.o show.o util.o
diff --git a/arch/arm/kernel/vbi/syscalls.S b/arch/arm/kernel/vbi/syscalls.S
index e973d6f..338a68c 100644
--- a/arch/arm/kernel/vbi/syscalls.S
+++ b/arch/arm/kernel/vbi/syscalls.S
@@ -530,7 +530,7 @@ FUNC_END(vbiVbMgmt)
 *
 */
 
-FUNC_LABEL(vbiKputs)
+FUNC_LABEL(vbi_kputs)
         /*
          * r0 - char * s
          */
@@ -539,7 +539,7 @@ FUNC_LABEL(vbiKputs)
         HCALL
         RESTOREREGS
         mov     pc, lr
-FUNC_END(vbiKputs)
+FUNC_END(vbi_kputs)
 
 /******************************************************************************
 *
@@ -1035,7 +1035,7 @@ FUNC_LABEL(vbi_flush_tlb)
         SAVEREGS
         ldr     r8, =VBI_SYS_tlb_flush
         HCALL
-	INVALIDATE_CURR_ASID(r8)
+        TLBIALL(r0)
         RESTOREREGS
         mov     pc, lr
 FUNC_END(vbi_flush_tlb)
@@ -1341,7 +1341,7 @@ FUNC_END(vbi_create_vmmu)
 *
 * vbi_delete_vmmu - delete the virtual MMU handle
 *
-* This system call enables a context's virtual MMU.
+* This system call deletes a context's virtual MMU.
 *
 */
 
@@ -1374,6 +1374,9 @@ FUNC_LABEL(vbi_tlb_load_vmmu)
         SAVEREGS
         ldr     r8, =VBI_SYS_vmmu_tlbload
         HCALL
+
+        INVALIDATE_CURR_ASID(r8)
+
         RESTOREREGS
         mov     pc, lr
 FUNC_END(vbi_tlb_load_vmmu)
@@ -1396,6 +1399,9 @@ FUNC_LABEL(vbi_tlb_flush_vmmu)
         SAVEREGS
         ldr     r8, =VBI_SYS_vmmu_tlbflush
         HCALL
+
+        INVALIDATE_CURR_ASID(r8)
+
         RESTOREREGS
         mov     pc, lr
 FUNC_END(vbi_tlb_flush_vmmu)
diff --git a/arch/arm/kernel/vbi/util.c b/arch/arm/kernel/vbi/util.c
new file mode 100644
index 0000000..9fe2b19
--- /dev/null
+++ b/arch/arm/kernel/vbi/util.c
@@ -0,0 +1,65 @@
+/*
+ * util.c - utilities routines for guest OS para-virtualization
+ *
+ * Copyright (c) 2011 Wind River Systems, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ * See the GNU General Public License for more details.
+ *
+ */
+
+#include <asm/page.h>
+#include <linux/module.h>
+#include <vbi/interface.h>
+#include <vbi/vmmu.h>
+#include <vbi/syscall.h>
+#include <vbi/vbi.h>
+
+extern struct vb_config *wr_config;
+extern struct vb_status *wr_status;
+extern struct vb_control *wr_control;
+
+/*
+ * vb_memsize_get should not be called before wr_config is initialized
+ */
+unsigned int vb_memsize_get(void)
+{
+	if (wr_config == (struct vb_config *)(-1))
+		return 0;
+	return VBI_MEM_SIZE_GET();
+}
+
+unsigned int vb_context_get(void)
+{
+	if (wr_config == (struct vb_config *)(-1))
+		return 0xbadc0de0;
+	return VBI_CONTEXT_ID_GET();
+}
+
+void vb__flush_dcache_icache(void *start)
+{
+	vbi_flush_icache(start, PAGE_SIZE);
+	vbi_flush_dcache(start, PAGE_SIZE);
+}
+
+void vb_flush_dcache_range(unsigned long start, unsigned long stop)
+{
+	vbi_flush_dcache((void *) start, (stop - start + 1));
+}
+
+void vb__flush_icache_range(unsigned long start, unsigned long stop)
+{
+	vbi_update_text_cache((void *) start, (stop - start + 1));
+}
+
+void vb__flush_dcache_icache_phys(unsigned long physaddr)
+{
+	vbi_flush_icache((void *) physaddr, PAGE_SIZE);
+	vbi_flush_dcache((void *) physaddr, PAGE_SIZE);
+}
diff --git a/arch/arm/kernel/vbi/wrhv.c b/arch/arm/kernel/vbi/wrhv.c
new file mode 100644
index 0000000..e07bc1b
--- /dev/null
+++ b/arch/arm/kernel/vbi/wrhv.c
@@ -0,0 +1,607 @@
+/*
+ *  This program is free software; you can redistribute it and/or modify it
+ *  under the terms of the GNU General Public License as published by the
+ *  Free Software Foundation; either version 2, or (at your option) any
+ *  later version.
+ *
+ *  This program is distributed in the hope that it will be useful, but
+ *  WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ *  General Public License for more details.
+ *
+ *  Copyright (C) 2011 Wind River Systems, Inc.
+ */
+
+#include <linux/module.h>
+#include <linux/irq.h>
+#include <linux/wrhv.h>
+#include <linux/interrupt.h>
+#include <linux/ptrace.h>
+#include <linux/delay.h>
+#include <linux/mm.h>
+#include <linux/stddef.h>
+#include <linux/clockchips.h>
+#include <linux/kernel_stat.h>
+
+#include <asm/setup.h>
+#include <asm/irq.h>
+#include <asm/paravirt.h>
+#include <asm/smp_scu.h>
+#include <asm/wrhv.h>
+
+#include <trace/irq.h>
+
+#include <vbi/vbi.h>
+#include <vbi/interface.h>
+#include <vbi/vmmu.h>
+
+#define WRHV_BOOTARG_BUF_SIZE	256
+
+#define IPI_IRQ_BASE_NAME	"ipi"
+
+#define TIMERTICK_IRQ		0
+
+DEFINE_PER_CPU(struct clock_event_device, wrhv_clock_events);
+spinlock_t vmmu_handle_lock;
+
+/* wr_config is set super early to a pointer passed from the hv */
+struct vb_config *wr_config = (void *)(-1); /* keep it out of the bss */
+EXPORT_SYMBOL(wr_config);
+
+/* control and status pointers are set from info in the config region */
+struct vb_control *wr_control;
+struct vb_status *wr_status;
+extern struct vb_status *LCstatptr;
+
+static char *direct_interrupts_list;
+static int __init wrhv_check_direct_interrupts(char *str)
+{
+	direct_interrupts_list = str;
+	return 0;
+}
+
+early_param("direct_interrupts", wrhv_check_direct_interrupts);
+
+int wrhv_find_direct_interrupt(int intr_candidate)
+{
+	char	direct_interrupts[WRHV_BOOTARG_BUF_SIZE];
+	char	*current_tok;
+	char	*position;
+	int	intr_num;
+
+	if (direct_interrupts_list == NULL)
+		return -ENOENT;
+
+	strncpy(direct_interrupts, direct_interrupts_list,
+		WRHV_BOOTARG_BUF_SIZE-1);
+	direct_interrupts[WRHV_BOOTARG_BUF_SIZE-1] = '\0'; /* be safe */
+	position = direct_interrupts;
+	/* crack by ',' first */
+	while ((current_tok = strsep(&position, ","))) {
+		if (sscanf(current_tok, "%d", &intr_num) != 1)
+			return -EINVAL; /* broken list */
+
+			if (intr_num == intr_candidate) {
+				/* We have a match! */
+				return 0;
+			}
+	}
+	/* Didn't find it */
+	return -ENOENT;
+}
+
+unsigned long __init wrhv_find_end_of_memory(void)
+{
+	return wr_config->phys_mem_size;
+}
+
+char wrhv_super_early_stack[WRHV_SUPER_EARLY_STACK_SIZE];
+
+void wrhv_load_initial_vmmu(uint32_t pgtbl)
+{
+	/* This is called super early.  We have a stack, and wr_config
+	 * has been set, but that's about it.  If the core we're starting
+	 * is anything other than the bp (core0), then pgtbl represents
+	 * the vmmu handle which was already created.
+	 */
+
+	VMMU_CONFIG	vmmu_cfg;
+
+	vbi_enable_vmmu(0);
+
+	if (wr_config->coreId == 0) {
+		/* Initial startup of the boot core */
+		vmmu_cfg.addr = pgtbl;
+		vmmu_cfg.flush_type = 0;
+		vmmu_cfg.asid = 0;
+		if (vbi_create_vmmu(&vmmu_cfg) != 0) {
+			printk(KERN_ERR "Could not create initial vmmu!\n");
+			while (1) {}; /* No point in continuing */
+		
+		} else {
+			wr_config->vb_control->vb_control_regs.vmmu_handle =
+				vmmu_cfg.vmmu_handle;
+			init_mm.context.vmmu_handle = vmmu_cfg.vmmu_handle;
+		}
+	} else
+		wr_config->vb_control->vb_control_regs.vmmu_handle = pgtbl;
+	wr_config->vb_control->vb_control_regs.asid = 0;
+	if (vbi_load_ctx() != 0) {
+		printk(KERN_ERR "Could not load initial context!\n");
+		while (1) {}; /* No point in continuing */
+	}
+}
+
+static void wrhv_do_restart(void *data)
+{
+	int ret;
+	int cpu = smp_processor_id();
+
+	if (!cpu) {
+		printk(KERN_INFO "WRHV: rebooting\n");
+
+		ret = vbi_vb_reset(VBI_BOARD_ID_GET(), VBI_VB_CORES_ALL,
+				VBI_VBMGMT_RESET_AND_START_CORE0 |
+				VBI_VBMGMT_RESET_DOWNLOAD
+				);
+
+		if (ret)
+			printk(KERN_ERR "WRHV: reboot failed. ret = %d\n", ret);
+	}
+}
+
+void wrhv_restart(char str, const char *cmd)
+{
+	int cpu = smp_processor_id();
+
+	if (!cpu)
+		wrhv_do_restart(NULL);
+	else
+		smp_call_function(wrhv_do_restart, NULL, 1);
+
+	while (1);
+}
+
+#ifdef CONFIG_SMP
+static irqreturn_t wrhv_ipi_interrupt(int irq, void *dev_id)
+{
+	do_IPI(get_irq_regs());
+	return IRQ_HANDLED;
+}
+
+int ipi_irq = VBI_INVALID_IRQ;	/* Make sure it gets set before use */
+
+void wrhv_unmask_IPIs_for_vcore(void)
+{
+	printk(KERN_INFO "CPU%d: Unmasking ipi %d\n", smp_processor_id(),
+		ipi_irq);
+	vbi_unmask_vioapic_irq(ipi_irq);
+}
+
+int wrhv_request_ipis(void)
+{
+	static char *ipi_names[] = {
+		"IPI (all ipi functions)",
+	};
+
+	int err;
+
+	ipi_irq = vbi_find_irq(IPI_IRQ_BASE_NAME, VB_INPUT_INT);
+	if (ipi_irq == VBI_INVALID_IRQ) {
+		printk(KERN_ERR "WRHV lookup of interrupt name '"
+				IPI_IRQ_BASE_NAME
+				"' failed!\n");
+		panic("WRHV resolve irq for IPI failed\n");
+	}
+
+	set_irq_chip_and_handler_name(ipi_irq, &wrhv_ipi_irq_chip,
+				      handle_percpu_irq, "per_cpu");
+	err = request_irq(ipi_irq, wrhv_ipi_interrupt,
+			  IRQF_DISABLED | IRQF_NOBALANCING,
+			  ipi_names[0], wrhv_ipi_interrupt);
+	if (err) {
+		printk(KERN_ERR "WRHV request of irq %d for IPI(%s) failed\n",
+		       ipi_irq, ipi_names[0]);
+	} else
+		wrhv_unmask_IPIs_for_vcore(); /* Allow the BP to receive them */
+	return err;
+}
+#endif
+
+void wrhv_smp_init_cpus(void)
+{
+	unsigned int i, ncores;
+
+	/* Ask the vbi how many cores we have */
+	ncores = VBI_VCORES_COUNT_GET();
+
+	/* Check if we have more available than configured to support */
+	if (ncores > NR_CPUS) {
+		printk(KERN_WARNING
+		       "wrhv: no. of cores (%d) greater than configured "
+		       "maximum of %d - clipping\n",
+		       ncores, NR_CPUS);
+		ncores = NR_CPUS;
+	}
+
+	for (i = 0; i < ncores; i++)
+		set_cpu_possible(i, true);
+}
+
+void wrhv_smp_prepare_cpus(unsigned int max_cpus)
+{
+	unsigned int ncores = num_possible_cpus();
+	unsigned int cpu = smp_processor_id();
+	int i;
+
+	smp_store_cpu_info(cpu);
+
+	/*
+	 * are we trying to boot more cores than exist?
+	 */
+	if (max_cpus > ncores)
+		max_cpus = ncores;
+
+	if (max_cpus > 1)
+		if (wrhv_request_ipis()) {
+			printk(KERN_ERR "IPI init issue, continuing in UP\n");
+			return;		/* something went wrong */
+		}
+
+	/*
+	 * Initialize the present map, which describes the set of CPUs
+	 * actually populated at the present time.
+	 */
+	for (i = 0; i < max_cpus; i++)
+		set_cpu_present(i, true);
+}
+
+static DEFINE_SPINLOCK(boot_lock);
+
+extern volatile int pen_release;
+int __cpuinit wrhv_boot_secondary(unsigned int cpu, struct task_struct *idle)
+{
+	int ret;
+	unsigned long timeout;
+
+	/*
+	 * Set synchronisation state between this boot processor
+	 * and the secondary one
+	 */
+	spin_lock(&boot_lock);
+
+	pen_release = cpu;
+	ret = vbi_vb_resume(VBI_BOARD_ID_GET(), cpu);
+
+	timeout = jiffies + (1 * HZ);
+	while (time_before(jiffies, timeout)) {
+		smp_rmb();
+		if (pen_release == -1)
+			break;
+
+		udelay(10);
+	}
+
+	/*
+	 * now the secondary core is starting up let it run its
+	 * calibrations, then wait for it to finish
+	 */
+	spin_unlock(&boot_lock);
+
+	if (pen_release != -1)
+		return -ENOSYS;
+
+
+	return ret;
+}
+
+extern void __iomem *gic_cpu_base_addr;
+void __cpuinit wrhv_platform_secondary_init(unsigned int cpu)
+{
+	trace_hardirqs_off();
+
+	vbi_set_exc_base((void *)CONFIG_VECTORS_BASE);
+
+	if (direct_interrupts_list)
+		gic_cpu_init(0, gic_cpu_base_addr);
+
+	wrhv_unmask_IPIs_for_vcore();
+
+	/*
+	 * let the primary processor know we're out of the
+	 * pen, then head off into the C entry point
+	 */
+	pen_release = -1;
+	smp_wmb();
+
+	/*
+	 * Synchronise with the boot thread.
+	 */
+	spin_lock(&boot_lock);
+	spin_unlock(&boot_lock);
+}
+
+irqreturn_t wrhv_timer_interrupt(int irq, void *dev_id)
+{
+	u64 ticks;
+	static DEFINE_PER_CPU(u64, mark_offset);
+	static DEFINE_PER_CPU(int, mark_first_time) = 1;
+	int cpu = smp_processor_id();
+	struct clock_event_device *evt = &per_cpu(wrhv_clock_events, cpu);
+
+	if (!evt->event_handler) {
+		printk(KERN_WARNING
+			   "Spurious Hyp timer interrupt on cpu %d\n", cpu);
+		return IRQ_NONE;
+	}
+
+	if (__get_cpu_var(mark_first_time) == 0) {
+		ticks = wr_vb_status->tick_count;
+		ticks -= __get_cpu_var(mark_offset);
+		__get_cpu_var(mark_offset) = wr_vb_status->tick_count;
+		if (ticks > (2*HZ)) {
+			printk(KERN_DEBUG "Time falling behind %lld jiffies\n",
+				ticks);
+			ticks = 1;
+		}
+	} else {
+		ticks = 1;
+		__get_cpu_var(mark_first_time) = 0;
+		__get_cpu_var(mark_offset) = wr_vb_status->tick_count;
+	}
+
+	if (ticks > 1)
+		account_steal_time(jiffies_to_cputime(ticks - 1));
+
+	while (ticks != 0) {
+		evt->event_handler(evt);
+		ticks--;
+	}
+
+	return IRQ_HANDLED;
+}
+static struct irqaction wrhv_timer_irq = {
+	.handler = wrhv_timer_interrupt,
+	.flags = IRQF_DISABLED | IRQF_NOBALANCING,
+	.name = "timer",
+};
+
+#ifdef CONFIG_SPARSE_IRQ
+#define WRHV_NR_IRQS	NR_IRQS_LEGACY
+#else
+#define WRHV_NR_IRQS	NR_IRQS
+#endif
+
+void (*wrhv_machine_init_irq)(void) __initdata = NULL;
+void (*wrhv_machine_init_timer)(void) __initdata = NULL;
+
+void __init wrhv_init_irq(void)
+{
+	int i;
+	struct irq_desc *desc;
+
+#ifdef CONFIG_SMP
+	/* By default all the irqs will be routed to core0 */
+	cpumask_copy(irq_default_affinity, cpumask_of(0));
+#endif
+
+	wrhv_irq_chip.typename = "WRHV-PIC";
+	for (i = 0; i < WRHV_NR_IRQS; i++) {
+		desc = irq_to_desc_alloc_node(i, 0);
+		desc->status = IRQ_DISABLED | IRQ_LEVEL;
+		desc->action = NULL;
+		desc->depth = 1;
+		set_irq_chip_and_handler(i, &wrhv_irq_chip, handle_fasteoi_irq);
+	}
+
+	/* Do any hardware specific init to support direct irqs */
+	if (wrhv_machine_init_irq && direct_interrupts_list)
+		wrhv_machine_init_irq();
+	else
+		printk(KERN_INFO "WRHV: No direct irq support\n");
+}
+
+void wrhv_do_IRQ(struct pt_regs *regs)
+{
+	struct pt_regs *old_regs = set_irq_regs(regs);
+	unsigned int irq;
+	int handled_at_least_one = 0;
+
+	trace_irq_entry(0, regs, NULL);
+
+	irq_enter();
+
+	/* To be implemented if there is a simple way...
+	check_stack_overflow();
+	*/
+
+check_again:
+	irq = vbi_get_pending_vioapic_irq();
+
+	if (irq != 0xffff) {
+		generic_handle_irq(irq);
+		handled_at_least_one = 1;
+		goto check_again;
+	} else if (!handled_at_least_one)
+		printk(KERN_WARNING "WRHV: Spurious interrupt!\n");
+
+	irq_exit();
+	set_irq_regs(old_regs);
+
+	trace_irq_exit(IRQ_HANDLED);
+}
+
+static void wrhv_set_mode(enum clock_event_mode mode,
+				 struct clock_event_device *dev)
+{
+	return;
+}
+
+static int wrhv_set_next_event(unsigned long evt,
+				      struct clock_event_device *dev)
+{
+	return 0;
+}
+
+static struct clock_event_device wrhv_clockevent = {
+	.name		= "wrhv",
+	.features	= CLOCK_EVT_FEAT_PERIODIC,
+	.set_mode	= wrhv_set_mode,
+	.set_next_event = wrhv_set_next_event,
+	.max_delta_ns	= 0xffffffff,
+	.min_delta_ns	= 10000,
+	.shift		= 32,   /* nanoseconds to cycles divisor 2^ */
+	.mult		= 1,     /* To be filled in */
+	.irq		= TIMERTICK_IRQ,
+	.rating		= 1,
+};
+
+void __init wrhv_time_init(void)
+{
+	struct clock_event_device *evt;
+
+	evt = &per_cpu(wrhv_clock_events, 0);
+	memcpy(evt, &wrhv_clockevent, sizeof(*evt));
+	evt->cpumask = cpumask_of(0);
+
+	clockevents_register_device(evt);
+	setup_irq(TIMERTICK_IRQ, &wrhv_timer_irq);
+	vbi_unmask_vioapic_irq(TIMERTICK_IRQ); /* Allow timer ints through */
+
+	if (wrhv_machine_init_timer)
+		wrhv_machine_init_timer();
+}
+
+void __devinit wrhv_setup_secondary_clock(void)
+{
+	int cpu;
+	struct clock_event_device *evt;
+	cpu = smp_processor_id();
+	printk(KERN_INFO "installing wrhv timer for CPU %d\n", cpu);
+
+	evt = &per_cpu(wrhv_clock_events, cpu);
+	memcpy(evt, &wrhv_clockevent, sizeof(*evt));
+	evt->cpumask = cpumask_of(cpu);
+
+	clockevents_register_device(evt);
+
+	vbi_unmask_vioapic_irq(TIMERTICK_IRQ); /* Allow timer ints through */
+}
+
+pgd_t *wrhv_cpu_get_pgd(void)
+{
+	return current->active_mm->pgd;
+}
+
+void wrhv_do_idle(void)
+{
+	vbi_idle(1);
+}
+
+extern int __init arm_add_memory(unsigned long start, unsigned long size);
+
+void __init wrhv_MMU_init(void)
+{
+
+	__u32 start;
+	__u32 size;
+
+	start = 0x00000000ul;
+	size = wrhv_find_end_of_memory();
+
+	printk(KERN_DEBUG "Total %dK memory added\n", size / 1024);
+
+	arm_add_memory(start, size);
+
+}
+
+void wrhv_set_pte_ext(pte_t *ptep, pte_t pte, unsigned int ext)
+{
+	native_cpu_set_pte_ext(ptep, pte, ext);
+
+	/* We really want to just flush the modified entry for obvious
+	 * efficiency reasons.  But hypervisor issues right now mean that
+	 * we must flush them all.  The third parameter, length, being set
+	 * to -1 will flush them all.  We will leave the second parameter set
+	 * to what we really just want to flush even though it's ignored.
+	 */
+	vbi_flush_tlb(0, (void *)(pte & (-PAGE_SIZE)), -1);
+}
+
+void wrhv_do_switch_mm(unsigned long pgd_phys, struct mm_struct *mm)
+{
+	wr_control->vb_control_regs.vmmu_handle = mm->context.vmmu_handle;
+	wr_control->vb_control_regs.asid = 0;
+	if (vbi_load_ctx() != 0)
+		printk(KERN_WARNING "Bad vmmu handle %lu\n",
+				mm->context.vmmu_handle);
+	/* Flush everything for now, until we support asids.
+	 */
+	vbi_flush_tlb(0, 0, -1);
+}
+
+void wrhv_smp_cross_call(const struct cpumask *mask)
+{
+	unsigned long coreset = cpus_addr(*mask)[0];
+	unsigned long flags;
+
+	local_irq_save(flags);
+	WARN_ON(coreset & ~cpus_addr(cpu_online_map)[0]);
+	vbi_send_vcore_vioapic_irq(ipi_irq, coreset, VBI_IOAPICSEND_VCORE_NONE);
+	local_irq_restore(flags);
+}
+
+void wrhv_calculate_clock_freq(void)
+{
+	u64 lpj;
+	/* Hypervisor doesn't fill in the stamp freq field yet
+	unsigned long cpu_khz = wrhv_calculate_cpu_khz();
+	*/
+	unsigned long cpu_khz = 400 * 1000; /* 400 MHz default */
+
+	lpj = ((u64)cpu_khz * 1000);
+	do_div(lpj, HZ);
+	preset_lpj = lpj;
+
+	printk(KERN_INFO "Detected %lu.%03lu MHz processor.\n", cpu_khz / 1000,
+		cpu_khz % 1000);
+}
+
+void __init wrhv_init(void)
+{
+	/* wr_config was already set, super early */
+	vbi_init(wr_config);
+
+	wr_control = wr_config->vb_control;
+	wr_status = LCstatptr = wr_config->vb_status;
+
+
+	pv_info.name = "wrhv";
+	pv_info.paravirt_enabled = 1;
+
+	pv_irq_ops.do_IRQ = wrhv_do_IRQ;
+
+	pv_cpu_ops.do_idle = wrhv_do_idle;
+
+	pv_smp_ops.smp_init_cpus = wrhv_smp_init_cpus;
+	pv_smp_ops.smp_prepare_cpus = wrhv_smp_prepare_cpus;
+	pv_smp_ops.smp_cross_call = wrhv_smp_cross_call;
+	pv_smp_ops.boot_secondary = wrhv_boot_secondary;
+	pv_smp_ops.platform_secondary_init = wrhv_platform_secondary_init;
+
+	pv_mmu_ops.MMU_init = wrhv_MMU_init;
+	pv_mmu_ops.do_switch_mm = wrhv_do_switch_mm;
+	pv_mmu_ops.set_pte_ext = wrhv_set_pte_ext;
+	pv_mmu_ops.cpu_get_pgd = wrhv_cpu_get_pgd;
+
+	pv_time_ops.percpu_timer_setup = wrhv_setup_secondary_clock;
+
+	snprintf(boot_command_line, COMMAND_LINE_SIZE,
+		"retain_initrd %s",
+	wr_config->bootLine);
+
+	wrhv_calculate_clock_freq();
+
+	arm_pm_restart = wrhv_restart;
+	spin_lock_init(&vmmu_handle_lock);
+}
diff --git a/arch/arm/kernel/vmlinux-wrhv.lds.S b/arch/arm/kernel/vmlinux-wrhv.lds.S
new file mode 100644
index 0000000..92b59fb
--- /dev/null
+++ b/arch/arm/kernel/vmlinux-wrhv.lds.S
@@ -0,0 +1,254 @@
+/*
+ * Copyright (c) 2011 Wind River Systems, Inc.
+ *
+ * The right to copy, distribute or otherwise make use of this software may be
+ * licensed only pursuant to the terms of an applicable Wind River license
+ * agreement.
+ */
+
+/* ld script to make ARM Linux guest OS kernel for Wind River Hypervisor.
+ *
+ * The main difference between this and the regular one is the movement
+ * of the initramfs region to avoid relocation limits when loading modules
+ * from rather large ramdisks.  This shuffling breaks XIP, but
+ * references to XIP have been removed as XIP is not supported with
+ * Wind River Hypervisor anyway.
+ *
+ * Derives from the regular arm version with some changes as suggested by
+ * Russell King on the linux kernel mailing list.
+ */
+
+#include <asm-generic/vmlinux.lds.h>
+#include <asm/thread_info.h>
+#include <asm/memory.h>
+#include <asm/page.h>
+
+#ifdef CONFIG_XIP_KERNEL
+#error XIP not supported with Wind River Hypervisor
+#endif
+
+OUTPUT_ARCH(arm)
+ENTRY(stext)
+
+#ifndef __ARMEB__
+jiffies = jiffies_64;
+#else
+jiffies = jiffies_64 + 4;
+#endif
+
+SECTIONS
+{
+	/*
+	 * unwind exit sections must be discarded before the rest of the
+	 * unwind sections get included.
+	 */
+	/DISCARD/ : {
+		*(.ARM.exidx.exit.text)
+		*(.ARM.extab.exit.text)
+#ifndef CONFIG_HOTPLUG_CPU
+		*(.ARM.exidx.cpuexit.text)
+		*(.ARM.extab.cpuexit.text)
+#endif
+#ifndef CONFIG_HOTPLUG
+		*(.ARM.exidx.devexit.text)
+		*(.ARM.extab.devexit.text)
+#endif
+#ifndef CONFIG_MMU
+		*(.fixup)
+		*(__ex_table)
+#endif
+	}
+
+	. = PAGE_OFFSET + TEXT_OFFSET;
+
+	_text = .;
+	_stext = .;
+	HEAD_TEXT_SECTION
+
+	.text : {	/* Real text segment	*/
+			__exception_text_start = .;
+			*(.exception.text)
+			__exception_text_end = .;
+			TEXT_TEXT
+			SCHED_TEXT
+			LOCK_TEXT
+			KPROBES_TEXT
+#ifdef CONFIG_MMU
+			*(.fixup)
+#endif
+			*(.gnu.warning)
+			*(.rodata)
+			*(.rodata.*)
+			*(.glue_7)
+			*(.glue_7t)
+		*(.got)			/* Global offset table		*/
+
+		_etext = .;		/* End of text and rodata section */
+	}
+
+		. = ALIGN(PAGE_SIZE);
+		__init_begin = .;
+
+		INIT_TEXT_SECTION(0)
+
+	.init.proc.info : {
+		__proc_info_begin = .;
+			*(.proc.info.init)
+		__proc_info_end = .;
+	}
+	.init.arch.info : {
+		__arch_info_begin = .;
+			*(.arch.info.init)
+		__arch_info_end = .;
+	}
+	.init.tagtable : {
+		__tagtable_begin = .;
+			*(.taglist.init)
+		__tagtable_end = .;
+	}
+
+	.init.data : {
+		INIT_DATA
+		INIT_SETUP(16)
+
+		INIT_CALLS
+		CON_INITCALL
+		SECURITY_INITCALL
+		INIT_RAM_FS
+
+	}
+
+	PERCPU(PAGE_SIZE)
+
+	. = ALIGN(PAGE_SIZE);
+	__init_end = .;
+
+	RO_DATA(PAGE_SIZE)
+
+#ifdef CONFIG_ARM_UNWIND
+	/*
+	 * Stack unwinding tables
+	 */
+	. = ALIGN(8);
+	.ARM.unwind_idx : {
+		__start_unwind_idx = .;
+		*(.ARM.exidx*)
+		__stop_unwind_idx = .;
+	}
+	.ARM.unwind_tab : {
+		__start_unwind_tab = .;
+		*(.ARM.extab*)
+		__stop_unwind_tab = .;
+	}
+#endif
+
+	. = ALIGN(THREAD_SIZE);
+	__data_loc = .;
+
+	.data : {
+		_data = .;		/* address in memory */
+		_sdata = .;
+
+		/*
+		 * first, the init task union, aligned
+		 * to an 8192 byte boundary.
+		 */
+		INIT_TASK_DATA(THREAD_SIZE)
+
+		NOSAVE_DATA
+		CACHELINE_ALIGNED_DATA(32)
+
+		/*
+		 * The exception fixup table (might need resorting at runtime)
+		 */
+		. = ALIGN(32);
+		__start___ex_table = .;
+#ifdef CONFIG_MMU
+		*(__ex_table)
+#endif
+		__stop___ex_table = .;
+
+		/*
+		 * and the usual data section
+		 */
+		DATA_DATA
+		CONSTRUCTORS
+
+		_edata = .;
+	}
+	_edata_loc = __data_loc + SIZEOF(.data);
+
+#ifdef CONFIG_HAVE_TCM
+        /*
+	 * We align everything to a page boundary so we can
+	 * free it after init has commenced and TCM contents have
+	 * been copied to its destination.
+	 */
+	.tcm_start : {
+		. = ALIGN(PAGE_SIZE);
+		__tcm_start = .;
+		__itcm_start = .;
+	}
+
+	/*
+	 * Link these to the ITCM RAM
+	 * Put VMA to the TCM address and LMA to the common RAM
+	 * and we'll upload the contents from RAM to TCM and free
+	 * the used RAM after that.
+	 */
+	.text_itcm ITCM_OFFSET : AT(__itcm_start)
+	{
+		__sitcm_text = .;
+		*(.tcm.text)
+		*(.tcm.rodata)
+		. = ALIGN(4);
+		__eitcm_text = .;
+	}
+
+	/*
+	 * Reset the dot pointer, this is needed to create the
+	 * relative __dtcm_start below (to be used as extern in code).
+	 */
+	. = ADDR(.tcm_start) + SIZEOF(.tcm_start) + SIZEOF(.text_itcm);
+
+	.dtcm_start : {
+		__dtcm_start = .;
+	}
+
+	/* TODO: add remainder of ITCM as well, that can be used for data! */
+	.data_dtcm DTCM_OFFSET : AT(__dtcm_start)
+	{
+		. = ALIGN(4);
+		__sdtcm_data = .;
+		*(.tcm.data)
+		. = ALIGN(4);
+		__edtcm_data = .;
+	}
+
+	/* Reset the dot pointer or the linker gets confused */
+	. = ADDR(.dtcm_start) + SIZEOF(.data_dtcm);
+
+	/* End marker for freeing TCM copy in linked object */
+	.tcm_end : AT(ADDR(.dtcm_start) + SIZEOF(.data_dtcm)){
+		. = ALIGN(PAGE_SIZE);
+		__tcm_end = .;
+	}
+#endif
+
+	BSS_SECTION(0, 0, 0)
+	_end = .;
+
+	STABS_DEBUG
+	.comment 0 : { *(.comment) }
+
+	/* Default discards */
+	DISCARDS
+}
+
+/*
+ * These must never be empty
+ * If you have to comment these two assert statements out, your
+ * binutils is too old (for other reasons as well)
+ */
+ASSERT((__proc_info_end - __proc_info_begin), "missing CPU support")
+ASSERT((__arch_info_end - __arch_info_begin), "no machine record defined")
diff --git a/arch/arm/mm/Kconfig b/arch/arm/mm/Kconfig
index 18e3d38..6a04fe1 100644
--- a/arch/arm/mm/Kconfig
+++ b/arch/arm/mm/Kconfig
@@ -417,7 +417,7 @@ config CPU_V7
 	select CPU_CACHE_V7
 	select CPU_CACHE_VIPT
 	select CPU_CP15_MMU
-	select CPU_HAS_ASID if MMU
+	select CPU_HAS_ASID if MMU && !WRHV
 	select CPU_COPY_V6 if MMU
 	select CPU_TLB_V7 if MMU
 
@@ -454,6 +454,7 @@ config CPU_32v6
 
 config CPU_32v7
 	bool
+	select TLS_REG_EMUL if SMP && WRHV
 
 # The abort model
 config CPU_ABRT_NOMMU
@@ -729,7 +730,7 @@ config TLS_REG_EMUL
 config HAS_TLS_REG
 	bool
 	depends on !TLS_REG_EMUL
-	default y if SMP || CPU_32v7
+	default y if (SMP || CPU_32v7) && !WRHV
 	help
 	  This selects support for the CP15 thread register.
 	  It is defined to be available on some ARMv6 processors (including
diff --git a/arch/arm/mm/abort-ev7.S b/arch/arm/mm/abort-ev7.S
index 2e6dc04..142a54d 100644
--- a/arch/arm/mm/abort-ev7.S
+++ b/arch/arm/mm/abort-ev7.S
@@ -1,5 +1,9 @@
 #include <linux/linkage.h>
 #include <asm/assembler.h>
+#ifdef CONFIG_WRHV
+#include <asm/asm-offsets.h>
+#endif
+
 /*
  * Function: v7_early_abort
  *
@@ -22,8 +26,15 @@ ENTRY(v7_early_abort)
 	 */
 	clrex
 
+#ifdef CONFIG_WRHV
+	ldr	r1, =wr_status			@ ptr to hv status region
+	ldr	r1, [r1]
+	ldr	r0, [r1, #STAT_SPACE_DFAR]	@ get dfar
+	ldr	r1, [r1, #STAT_SPACE_DFSR]	@ get dfsr
+#else
 	mrc	p15, 0, r1, c5, c0, 0		@ get FSR
 	mrc	p15, 0, r0, c6, c0, 0		@ get FAR
+#endif
 
 	/*
 	 * V6 code adjusts the returned DFSR.
diff --git a/arch/arm/mm/context.c b/arch/arm/mm/context.c
index b0ee9ba..bcf70c2 100644
--- a/arch/arm/mm/context.c
+++ b/arch/arm/mm/context.c
@@ -16,12 +16,14 @@
 #include <asm/mmu_context.h>
 #include <asm/tlbflush.h>
 
-static DEFINE_SPINLOCK(cpu_asid_lock);
-unsigned int cpu_last_asid = ASID_FIRST_VERSION;
 #ifdef CONFIG_SMP
 DEFINE_PER_CPU(struct mm_struct *, current_mm);
 #endif
 
+#ifdef CONFIG_CPU_HAS_ASID
+static DEFINE_SPINLOCK(cpu_asid_lock);
+unsigned int cpu_last_asid = ASID_FIRST_VERSION;
+
 /*
  * We fork()ed a process, and we need a new context for the child
  * to run in.  We reserve version 0 for initial tasks so we will
@@ -155,3 +157,4 @@ void __new_context(struct mm_struct *mm)
 	set_mm_context(mm, asid);
 	spin_unlock(&cpu_asid_lock);
 }
+#endif
diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 4b0f4b0..5296763 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -31,6 +31,10 @@
 
 #include "mm.h"
 
+#ifdef CONFIG_WRHV
+#include <vbi/interface.h>
+#endif
+
 DEFINE_PER_CPU(struct mmu_gather, mmu_gathers);
 
 /*
@@ -923,6 +927,92 @@ void __init reserve_node_zero(pg_data_t *pgdat)
 				BOOTMEM_DEFAULT);
 }
 
+#ifdef CONFIG_WRHV
+extern struct vb_config *wr_config;
+
+void __init wrhv_mapping(void)
+{
+	/*
+	 * Chicken and egg problem.  We need the config region to be
+	 * mapped in, just to see what we should map in.
+	 * Note also that the mapping will look like a cacheable device.
+	 */
+
+	uint32_t index, i;
+	unsigned long addr;
+	unsigned long end, pfn;
+	pmd_t *pmd;
+	struct config_page_map *pConfigPageMap;
+	pte_t *pte, *pte_start;
+
+	addr = (unsigned long)wr_config;
+	pmd = pmd_offset(pgd_offset_k(addr), addr);
+	alloc_init_pte(pmd, addr, addr + PAGE_SIZE, __phys_to_pfn(addr),
+		&mem_types[MT_DEVICE_CACHED]);
+	local_flush_tlb_all();
+	flush_cache_all();
+
+	/* In theory, we can now read config space.  Set up the rest. */
+	pConfigPageMap = &wr_config->configPageMap[0];
+	index = wr_config->configPageNum;
+	/* The zeroth would be the config page we just mapped so skip it */
+	for (i = 1; i < index; i++) {
+		addr = (unsigned long)pConfigPageMap[i].address;
+		pmd = pmd_offset(pgd_offset_k(addr), addr);
+		alloc_init_pte(pmd, addr, addr + PAGE_SIZE, __phys_to_pfn(addr),
+			&mem_types[MT_DEVICE_CACHED]);
+	}
+
+	/* Our architecture here relies upon our vector table being
+	 * placed right at the start of kernel space.  If you want to change
+	 * this for whatever reason, you'll have to work out your system
+	 * memory mappings carefully.
+	 */
+
+	if (CONFIG_PAGE_OFFSET != CONFIG_VECTORS_BASE) {
+		BUG();
+		/* no sense in continuing */
+		return;
+	}
+
+	/* Map in the first chunk of kernel space using conventional ptes
+	 * instead of just using a single l1 entry as is there already.
+	 */
+
+	pte = alloc_bootmem_low_pages(2 * PTRS_PER_PTE * sizeof(pte_t));
+	pte_start = pte;
+	addr = CONFIG_PAGE_OFFSET;
+	end = addr + PMD_SIZE;
+	pmd = pmd_offset(pgd_offset_k(addr), addr);
+	pfn = __phys_to_pfn(addr);
+
+	/* The first entry is our vector table, map it as user accessible.
+	 * Why?  Because the kernel user helper code resides at the far end
+	 * of it.  The hypervisor bounces code prefetches to 0xffff0fxx in
+	 * user mode to the same offset inside our vector table.
+	 */
+
+	set_pte_ext(pte, pfn_pte(pfn, PAGE_KERNEL_EXEC | L_PTE_MT_WRITEALLOC
+					| L_PTE_USER), 0);
+	pfn++;
+
+	/* Fill in the rest of the table */
+	while (pte++, addr += PAGE_SIZE, addr != end) {
+		set_pte_ext(pte, pfn_pte(pfn, PAGE_KERNEL_EXEC |
+						L_PTE_MT_WRITEALLOC), 0);
+		pfn++;
+	};
+
+	/* Overwrite the l1 entry in our live mm with an entry which references
+	 * the table we just built.
+	 */
+
+	pmd_populate_kernel(&init_mm, pmd, pte_start);
+	local_flush_tlb_all();
+	flush_cache_all();
+}
+#endif /* WRHV */
+
 /*
  * Set up device the mappings.  Since we clear out the page tables for all
  * mappings above VMALLOC_END, we will remove any debug device mappings.
@@ -932,7 +1022,9 @@ void __init reserve_node_zero(pg_data_t *pgdat)
  */
 static void __init devicemaps_init(struct machine_desc *mdesc)
 {
+#ifndef CONFIG_WRHV
 	struct map_desc map;
+#endif
 	unsigned long addr;
 
 	/*
@@ -973,6 +1065,7 @@ static void __init devicemaps_init(struct machine_desc *mdesc)
 	create_mapping(&map);
 #endif
 
+#ifndef CONFIG_WRHV
 	/*
 	 * Create a mapping for the machine vectors at the high-vectors
 	 * location (0xffff0000).  If we aren't using high-vectors, also
@@ -989,6 +1082,9 @@ static void __init devicemaps_init(struct machine_desc *mdesc)
 		map.type = MT_LOW_VECTORS;
 		create_mapping(&map);
 	}
+#else
+	wrhv_mapping();
+#endif
 
 	/*
 	 * Ask the machine support to map in the statically mapped devices.
diff --git a/arch/arm/mm/pabort-v7.S b/arch/arm/mm/pabort-v7.S
index a8b3b30..3e79a30 100644
--- a/arch/arm/mm/pabort-v7.S
+++ b/arch/arm/mm/pabort-v7.S
@@ -1,8 +1,11 @@
 #include <linux/linkage.h>
 #include <asm/assembler.h>
+#ifdef CONFIG_WRHV
+#include <asm/asm-offsets.h>
+#endif
 
 /*
- * Function: v6_pabort
+ * Function: v7_pabort
  *
  * Params  : r0 = address of aborted instruction
  *
@@ -14,7 +17,14 @@
 
 	.align	5
 ENTRY(v7_pabort)
+#ifdef CONFIG_WRHV
+	ldr	r1, =wr_status			@ ptr to hv status region
+	ldr	r1, [r1]
+	ldr	r0, [r1, #STAT_SPACE_IFAR]	@ get ifar
+	ldr	r1, [r1, #STAT_SPACE_IFSR]	@ get ifsr
+#else
 	mrc	p15, 0, r0, c6, c0, 2		@ get IFAR
 	mrc	p15, 0, r1, c5, c0, 1		@ get IFSR
+#endif
 	mov	pc, lr
 ENDPROC(v7_pabort)
diff --git a/arch/arm/mm/pgd.c b/arch/arm/mm/pgd.c
index be5f58e..a0fcf11 100644
--- a/arch/arm/mm/pgd.c
+++ b/arch/arm/mm/pgd.c
@@ -17,6 +17,12 @@
 
 #include "mm.h"
 
+#ifdef CONFIG_WRHV
+#include <asm/wrhv.h>
+#include <vbi/vmmu.h>
+#include <vbi/syscall.h>
+#endif
+
 #define FIRST_KERNEL_PGD_NR	(FIRST_USER_PGD_NR + USER_PTRS_PER_PGD)
 
 /*
@@ -25,8 +31,10 @@
 pgd_t *get_pgd_slow(struct mm_struct *mm)
 {
 	pgd_t *new_pgd, *init_pgd;
+#ifndef CONFIG_WRHV
 	pmd_t *new_pmd, *init_pmd;
 	pte_t *new_pte, *init_pte;
+#endif
 
 	new_pgd = (pgd_t *)__get_free_pages(GFP_KERNEL, 2);
 	if (!new_pgd)
@@ -43,6 +51,7 @@ pgd_t *get_pgd_slow(struct mm_struct *mm)
 
 	clean_dcache_area(new_pgd, PTRS_PER_PGD * sizeof(pgd_t));
 
+#ifndef CONFIG_WRHV
 	if (!vectors_high()) {
 		/*
 		 * On ARM, first page must always be allocated since it
@@ -62,11 +71,31 @@ pgd_t *get_pgd_slow(struct mm_struct *mm)
 		pte_unmap_nested(init_pte);
 		pte_unmap(new_pte);
 	}
+#endif
+
+#ifdef CONFIG_WRHV
+	{
+		VMMU_CONFIG vmmu_cfg;
+
+		vmmu_cfg.addr = (uint32_t)new_pgd;
+		vmmu_cfg.asid = 0;
+		vmmu_cfg.flush_type = 0;
+		spin_lock(&vmmu_handle_lock);
+		if (vbi_create_vmmu(&vmmu_cfg) != 0) {
+			printk(KERN_ERR "WRHV: Error creating vmmu!\n");
+			goto no_pmd;
+		} else
+			mm->context.vmmu_handle = vmmu_cfg.vmmu_handle;
+		spin_unlock(&vmmu_handle_lock);
+	}
+#endif
 
 	return new_pgd;
 
+#ifndef CONFIG_WRHV
 no_pte:
 	pmd_free(mm, new_pmd);
+#endif
 no_pmd:
 	free_pages((unsigned long)new_pgd, 2);
 no_pgd:
@@ -97,4 +126,15 @@ void free_pgd_slow(struct mm_struct *mm, pgd_t *pgd)
 	pmd_free(mm, pmd);
 free:
 	free_pages((unsigned long) pgd, 2);
+#ifdef CONFIG_WRHV
+	{
+		VMMU_CONFIG vmmu_cfg;
+
+		vmmu_cfg.vmmu_handle = mm->context.vmmu_handle;
+		spin_lock(&vmmu_handle_lock);
+		if (vbi_delete_vmmu(&vmmu_cfg) != 0)
+			printk(KERN_ERR "WRHV: Error deleting vmmu!\n");
+		spin_unlock(&vmmu_handle_lock);
+	}
+#endif
 }
diff --git a/arch/arm/mm/proc-macros.S b/arch/arm/mm/proc-macros.S
index 337f102..d5f32ec 100644
--- a/arch/arm/mm/proc-macros.S
+++ b/arch/arm/mm/proc-macros.S
@@ -39,9 +39,15 @@
 /*
  * mmid - get context id from mm pointer (mm->context.id)
  */
+#ifdef CONFIG_CPU_HAS_ASID
 	.macro	mmid, rd, rn
 	ldr	\rd, [\rn, #MM_CONTEXT_ID]
 	.endm
+#else
+	.macro	mmid, rd, rn
+	mov	\rd, #0
+	.endm
+#endif
 
 /*
  * mask_asid - mask the ASID from the context ID
diff --git a/arch/arm/mm/proc-v7.S b/arch/arm/mm/proc-v7.S
index c1c3fe0..6698385 100644
--- a/arch/arm/mm/proc-v7.S
+++ b/arch/arm/mm/proc-v7.S
@@ -107,7 +107,11 @@ ENDPROC(cpu_v7_dcache_clean_area)
 ENTRY(cpu_v7_switch_mm)
 #ifdef CONFIG_MMU
 	mov	r2, #0
+#ifdef CONFIG_CPU_HAS_ASID
 	ldr	r1, [r1, #MM_CONTEXT_ID]	@ get mm->context.id
+#else
+	mov	r1, #0
+#endif
 	orr	r0, r0, #TTB_FLAGS
 #ifdef CONFIG_ARM_ERRATA_430973
 	mcr	p15, 0, r2, c7, c5, 6		@ flush BTAC/BTB
@@ -193,6 +197,13 @@ cpu_v7_name:
  *	- cache type register is implemented
  */
 __v7_setup:
+#ifdef CONFIG_WRHV
+	@ The hypervisor controls most of processor initialization, so not
+	@ much to do here.
+	@ This routine needs the control register content returned.
+	mrc	p15, 0, r0, c1, c0, 0		@ read control register
+	mov	pc, lr				@ return to head.S:__ret
+#endif
 #ifdef CONFIG_SMP
 	mrc	p15, 0, r0, c1, c0, 1
 	tst	r0, #(1 << 6)			@ SMP/nAMP mode enabled?
diff --git a/arch/arm/mm/tlb-v7.S b/arch/arm/mm/tlb-v7.S
index f3f288a..c645d0f 100644
--- a/arch/arm/mm/tlb-v7.S
+++ b/arch/arm/mm/tlb-v7.S
@@ -56,6 +56,14 @@ ENTRY(v7wbi_flush_user_tlb_range)
 	mcr	p15, 0, ip, c7, c5, 6		@ flush BTAC/BTB
 #endif
 	dsb
+#ifdef CONFIG_WRHV
+	stmfd	sp!, {r0-r2, lr}
+	mov	r0, #0
+	mov	r1, #0
+	mov	r2, #-1
+	bl	vbi_flush_tlb
+	ldmfd	sp!, {r0-r2, lr}
+#endif
 	mov	pc, lr
 ENDPROC(v7wbi_flush_user_tlb_range)
 
@@ -90,6 +98,14 @@ ENTRY(v7wbi_flush_kern_tlb_range)
 #endif
 	dsb
 	isb
+#ifdef CONFIG_WRHV
+	stmfd	sp!, {r0-r2, lr}
+	mov	r0, #0
+	mov	r1, #0
+	mov	r2, #-1
+	bl	vbi_flush_tlb
+	ldmfd	sp!, {r0-r2, lr}
+#endif
 	mov	pc, lr
 ENDPROC(v7wbi_flush_kern_tlb_range)
 
diff --git a/include/vbi/syscall.h b/include/vbi/syscall.h
index 0b584c4..692a0c9 100644
--- a/include/vbi/syscall.h
+++ b/include/vbi/syscall.h
@@ -145,6 +145,7 @@ extern asmlinkage int32_t vbi_vb_reset(uint32_t id, int32_t core, uint32_t optio
 
 /* standard hypervisor and safety hypervisor stub functions */
 #if !defined(CONFIG_WRHV_SAFETY_PROFILE)
+extern int32_t vbi_flush_tlb(uint32_t asid, void *addr, size_t len);
 extern asmlinkage int vbi_ctx_ctl(unsigned operation, unsigned arg1,
 				unsigned arg2);
 extern asmlinkage int32_t vbi_kputs(const char *s);
-- 
1.7.0.4

