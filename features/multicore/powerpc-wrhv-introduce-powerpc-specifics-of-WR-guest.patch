From 4f9083e1716149e8129121236b15aeca6a0e8fce Mon Sep 17 00:00:00 2001
From: WRS Support <support@windriver.com>
Date: Tue, 27 Apr 2010 15:22:48 +0800
Subject: [PATCH] powerpc wrhv: introduce powerpc specifics of WR guest

These powerpc specific additons are the Linux specific
additions that were not a part of the reference VBI
implemenation.

Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
Signed-off-by: Bruce Ashfield <bruce.ashfield@windriver.com>
---
 arch/powerpc/boot/wrhv_boot.h               |   15 +
 arch/powerpc/include/asm/paravirt.h         |   96 +++
 arch/powerpc/include/asm/pv_hw_irq.h        |   65 ++
 arch/powerpc/include/asm/pv_pgtable-ppc32.h |   53 ++
 arch/powerpc/include/asm/reg_paravirt.h     |   22 +
 arch/powerpc/include/asm/reg_wrhv.h         |  142 ++++
 arch/powerpc/include/asm/wrhv.h             |   32 +
 arch/powerpc/kernel/dma.c                   |   36 +
 arch/powerpc/kernel/head_wrhv.h             |  146 ++++
 arch/powerpc/kernel/paravirt.c              |  271 +++++++
 arch/powerpc/kernel/vbi/util.c              |  142 ++++
 arch/powerpc/kernel/vbi/vmmu_display.c      |  140 ++++
 arch/powerpc/kernel/vbi/wrhv.c              | 1065 +++++++++++++++++++++++++++
 arch/powerpc/kernel/wrhv_entry_32.S         |  500 +++++++++++++
 arch/powerpc/kernel/wrhv_misc_32.S          |   72 ++
 15 files changed, 2797 insertions(+), 0 deletions(-)
 create mode 100644 arch/powerpc/boot/wrhv_boot.h
 create mode 100644 arch/powerpc/include/asm/paravirt.h
 create mode 100644 arch/powerpc/include/asm/pv_hw_irq.h
 create mode 100644 arch/powerpc/include/asm/pv_pgtable-ppc32.h
 create mode 100644 arch/powerpc/include/asm/reg_paravirt.h
 create mode 100644 arch/powerpc/include/asm/reg_wrhv.h
 create mode 100644 arch/powerpc/include/asm/wrhv.h
 create mode 100644 arch/powerpc/kernel/head_wrhv.h
 create mode 100644 arch/powerpc/kernel/paravirt.c
 create mode 100644 arch/powerpc/kernel/vbi/util.c
 create mode 100644 arch/powerpc/kernel/vbi/vmmu_display.c
 create mode 100644 arch/powerpc/kernel/vbi/wrhv.c
 create mode 100644 arch/powerpc/kernel/wrhv_entry_32.S
 create mode 100644 arch/powerpc/kernel/wrhv_misc_32.S

diff --git a/arch/powerpc/boot/wrhv_boot.h b/arch/powerpc/boot/wrhv_boot.h
new file mode 100644
index 0000000..6dc2d94
--- /dev/null
+++ b/arch/powerpc/boot/wrhv_boot.h
@@ -0,0 +1,15 @@
+#ifndef _WRHV_BOOT_H
+#define _WRHV_BOOT_H
+
+/*
+ * In current situation, there is no parameters protocol negotiated
+ * between hypervisor and guest Linux, so we still define the hard
+ * address for command line buffer like ENT_OFFSET as follows. Just
+ * for notes to remove them in the future.
+ * WRHV_CMDLINE_ADDR=offsetof(struct vb_config,bootLine)+ENT_OFFSET
+ */
+#define WRHV_CMDLINE_ADDR 0xF00000DC
+#define WRHV_CMDLINE_SIZE 256
+#define WRHV_EARLYCON_SIZE  14  /* sizeof("wrhv_earlycon=") */
+
+#endif /* _WRHV_BOOT_H */
diff --git a/arch/powerpc/include/asm/paravirt.h b/arch/powerpc/include/asm/paravirt.h
new file mode 100644
index 0000000..c2f4688
--- /dev/null
+++ b/arch/powerpc/include/asm/paravirt.h
@@ -0,0 +1,96 @@
+#ifndef __ASM_PARAVIRT_H
+#define __ASM_PARAVIRT_H
+
+#ifdef CONFIG_PARAVIRT
+/*
+ * native functions
+ */
+extern void native_do_IRQ(struct pt_regs *regs);
+extern unsigned int native_irq_of_parse_and_map(struct device_node *dev,
+						int index);
+extern unsigned int native_get_pvr(void);
+extern void native_timer_interrupt(struct pt_regs * regs);
+extern void __init native_time_init(void);
+extern void __init native_clocksource_init(void);
+extern void native_vmmu_restore (void);
+extern void __init native_MMU_init_hw(void);
+extern unsigned long __init native_mmu_mapin_ram(unsigned long top);
+extern void native_MMU_setup(void);
+extern void __init native_MMU_init(void);
+extern void native_flush_dcache_page(struct page *page);
+extern int native_map_page(unsigned long va, phys_addr_t pa, int flags);
+extern int native_kgdb_arch_handle_exception(int vector, int signo,
+				int err_code,
+				char *remcom_in_buffer,
+				char *remcom_out_buffer,
+				struct pt_regs *linux_regs);
+extern void __kprobes native_DebugException(struct pt_regs *regs,
+				unsigned long debug_status);
+extern int __init native_early_init_dt_scan_memory_ppc(unsigned long node, 
+			const char *uname, int depth, void *data);
+extern void __init native_time_init_cont(void);
+extern void __iomem* native___ioremap(phys_addr_t addr, unsigned long size, unsigned long flags);
+
+/*
+ * paravirtual operations structure
+ */
+struct pv_time_ops {
+	void (*time_init_cont)(void);
+	void (*timer_interrupt)(struct pt_regs *regs);
+	void (*clocksource_init)(void);
+};
+
+struct pv_cpu_ops {
+	unsigned int (*get_pvr)(void);
+	void (*DebugException)(struct pt_regs *regs, unsigned long debug_status);
+	int (*kgdb_arch_handle_exception)(int vector, int signo, int err_code,
+                               char *remcom_in_buffer, char *remcom_out_buffer,
+                               struct pt_regs *linux_regs);
+	int (*ppc_proc_freq)(void);
+};
+
+/* general info */
+struct pv_info {
+        const char *name;
+        int paravirt_enabled;
+};
+
+struct pv_irq_ops {
+	void (*do_IRQ)(struct pt_regs *regs);
+	unsigned int (*irq_of_parse_and_map)
+		(struct device_node *dev, int index);
+};
+
+struct pv_apic_ops {
+	unsigned int (*get_irq)(void);
+	void (*do_irq)(struct pt_regs *regs);
+	int (*get_ppc_spurious_interrupts)(void);
+	void (*set_ppc_spurious_interrupts)(int value);
+	unsigned int (*irq_of_parse_and_map)
+		(struct device_node *dev, int index);
+
+};
+
+struct pv_mmu_ops {
+	void (*vmmu_restore)(void);
+	void (*MMU_init_hw)(void);
+	unsigned long (*mmu_mapin_ram)(unsigned long top);
+	void (*MMU_setup)(void);
+	void (*MMU_init)(void);
+	void (*flush_dcache_page)(struct page *page);
+	int (*map_page)(unsigned long va, phys_addr_t pa, int flags);
+	int (*early_init_dt_scan_memory_ppc)(unsigned long node,
+			const char *uname, int depth, void *data);
+	void __iomem* (*__ioremap)(phys_addr_t addr, unsigned long size, unsigned long flags);
+	void (*__set_pte_at)(struct mm_struct *mm, unsigned long addr, 
+		pte_t *ptep, pte_t pte, int percpu);
+};
+
+extern struct pv_info pv_info;
+extern struct pv_time_ops pv_time_ops;
+extern struct pv_cpu_ops pv_cpu_ops;
+extern struct pv_irq_ops pv_irq_ops;
+extern struct pv_mmu_ops pv_mmu_ops;
+
+#endif /* CONFIG_PARAVIRT */
+#endif	/* __ASM_PARAVIRT_H */
diff --git a/arch/powerpc/include/asm/pv_hw_irq.h b/arch/powerpc/include/asm/pv_hw_irq.h
new file mode 100644
index 0000000..184631e
--- /dev/null
+++ b/arch/powerpc/include/asm/pv_hw_irq.h
@@ -0,0 +1,65 @@
+#ifndef PV_HW_IRQ_H
+#define PV_HW_IRQ_H
+
+/* set default definiation to native implemenation */
+#define local_irq_disable native_local_irq_disable
+#define local_irq_enable native_local_irq_enable
+#define local_irq_save_ptr native_local_irq_save_ptr
+#define irqs_disabled_flags native_irqs_disabled_flags
+
+#define local_save_flags native_local_save_flags
+#define local_irq_save native_local_irq_save
+#define irqs_disabled native_irqs_disabled
+
+#define hard_irq_enable native_hard_irq_enable
+#define hard_irq_disable native_hard_irq_disable
+
+
+/* Hypervisor specific irq implementation */
+#ifdef CONFIG_WRHV
+#include <vbi/interface.h>
+extern void wrhv_int_lock(void);
+extern void wrhv_int_unlock(int lvl);
+extern int wrhv_int_lvl_get (void);
+
+/* undefine native implementation */
+#undef local_irq_restore
+#undef local_save_flags
+#undef local_irq_save
+#undef irqs_disabled
+#undef local_irq_disable
+#undef local_irq_enable
+#undef local_irq_save_ptr
+#undef hard_irq_enable
+#undef hard_irq_disable
+
+/* WRHV specific static inline implementation */
+static inline void local_irq_disable(void)
+{
+        wrhv_int_lock();
+}
+
+static inline void local_irq_enable(void)
+{
+        wrhv_int_unlock(0);
+}
+
+static inline void local_irq_save_ptr(unsigned long *flags)
+{
+        *flags = wrhv_int_lvl_get();
+        wrhv_int_lock();
+}
+
+/* WRHV specific defination */
+#define local_irq_restore(flags) (flags == 0 ? local_irq_enable() : local_irq_disable ());
+#define local_save_flags(flags)	((flags) = wrhv_int_lvl_get())
+#define local_irq_save(flags) local_irq_save_ptr(&flags)
+#define irqs_disabled()	(wrhv_int_lvl_get() != 0)
+
+#define hard_irq_enable()      local_irq_enable()
+#define hard_irq_disable()     local_irq_disable()
+
+
+#endif /* CONFIG_WRHV */
+#endif /* PV_HW_IRQ_H */
+
diff --git a/arch/powerpc/include/asm/pv_pgtable-ppc32.h b/arch/powerpc/include/asm/pv_pgtable-ppc32.h
new file mode 100644
index 0000000..facc22f
--- /dev/null
+++ b/arch/powerpc/include/asm/pv_pgtable-ppc32.h
@@ -0,0 +1,53 @@
+#ifndef _ASM_PV_DEF_PGTABLE_PPC32_H
+#define _ASM_PV_DEF_PGTABLE_PPC32_H
+
+#include <asm-generic/pgtable-nopmd.h>
+
+#include <vbi/vmmu.h>
+#include <vbi/interface.h>
+
+/*
+ * refer to include/sys/vmmu.h on what format the hypervisor expects
+ * the guest OS software page table to be
+ */
+
+#define _PAGE_PRESENT		VMMU_PROT_SUPV_READ
+#define _PAGE_USER		(VMMU_PROT_USER_READ|VMMU_PROT_USER_EXECUTE)
+#define _PAGE_FILE      	_PAGE_USER
+#define _PAGE_ACCESSED	        VMMU_PROT_SUPV_WRITE
+#define _PAGE_HWWRITE   	VMMU_PROT_USER_WRITE
+#define	 _PAGE_RW		(VMMU_PROT_SUPV_EXECUTE|VMMU_PROT_USER_WRITE)
+#define _PAGE_HWEXEC    	VMMU_PROT_USER_EXECUTE
+
+#define _PAGE_ENDIAN		VMMU_CACHE_LE
+#define _PAGE_GUARDED		VMMU_CACHE_GUARDED
+#define _PAGE_COHERENT		VMMU_CACHE_COHERENT
+#define _PAGE_NO_CACHE		VMMU_CACHE_INHIBIT
+#define _PAGE_WRITETHRU		VMMU_CACHE_WRITETHROUGH
+
+#define _PAGE_DIRTY		VMMU_PTE_CHG_MASK
+
+#define _PAGE_EXEC		VMMU_CACHE_COHERENT
+
+#define _PTE_NONE_MASK		0xffffffff00000fffULL
+
+/* based on hypervisor VMMU_LEVEL_1_DESC definition */
+#define _PMD_PRESENT		0x00000001   /* big endian */
+#define _PMD_PRESENT_MASK	(_PMD_PRESENT)
+#define _PMD_BAD		(~PAGE_MASK & ~0x03)
+
+#define _PAGE_BASE		(_PAGE_PRESENT | _PAGE_ACCESSED | VMMU_PROT_USER_READ)
+
+#define PFN_SIZE		(1UL << PFN_SHIFT_OFFSET)
+#define PFN_MASK		(~(PFN_SIZE-1))
+#define pte_to_pa(x)		(pte_val(x) & PFN_MASK)
+#define pte_to_prot(x)		(pte_val(x) & (PFN_SIZE-1))
+
+/*
+ * Some bits are only used on some cpu families...
+ */
+#ifndef _PAGE_HASHPTE
+#define _PAGE_HASHPTE   0
+#endif
+
+#endif /* _ASM_PV_DEF_PGTABLE_PPC32_H */
diff --git a/arch/powerpc/include/asm/reg_paravirt.h b/arch/powerpc/include/asm/reg_paravirt.h
new file mode 100644
index 0000000..b765cce
--- /dev/null
+++ b/arch/powerpc/include/asm/reg_paravirt.h
@@ -0,0 +1,22 @@
+/*
+ * Contains the definition of registers common to all PowerPC variants.
+ * If a register definition has been changed in a different PowerPC
+ * variant, we will case it in #ifndef XXX ... #endif, and have the
+ * number used in the Programming Environments Manual For 32-Bit
+ * Implementations of the PowerPC Architecture (a.k.a. Green Book) here.
+ */
+
+#ifndef _ASM_POWERPC_REG_PARAVIRT_H
+#define _ASM_POWERPC_REG_PARAVIRT_H
+#ifdef __KERNEL__
+
+/* default native macros */
+#define PARAVIRT_MFSPR_SPRG3(a) mfspr a,SPRN_SPRG3 
+
+/* pickup individual hypervisor specific regs */
+#ifdef CONFIG_WRHV
+#include <asm/reg_wrhv.h>
+#endif
+
+#endif /* __KERNEL__ */
+#endif /* _ASM_POWERPC_REG_PARAVIRT_H */
diff --git a/arch/powerpc/include/asm/reg_wrhv.h b/arch/powerpc/include/asm/reg_wrhv.h
new file mode 100644
index 0000000..4d88f9e
--- /dev/null
+++ b/arch/powerpc/include/asm/reg_wrhv.h
@@ -0,0 +1,142 @@
+/*
+ *  This program is free software; you can redistribute it and/or modify it
+ *  under the terms of the GNU General Public License as published by the
+ *  Free Software Foundation; either version 2, or (at your option) any
+ *  later version.
+ *
+ *  This program is distributed in the hope that it will be useful, but
+ *  WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ *  General Public License for more details.
+ *
+ *  Copyright (C) 2009 Wind River Systems, Inc.
+ *
+ * Contains the definition of registers common to all PowerPC variants.
+ * If a register definition has been changed in a different PowerPC
+ * variant, we will case it in #ifndef XXX ... #endif, and have the
+ * number used in the Programming Environments Manual For 32-Bit
+ * Implementations of the PowerPC Architecture (a.k.a. Green Book) here.
+ */
+
+#ifndef _ASM_POWERPC_REG_WRHV_H
+#define _ASM_POWERPC_REG_WRHV_H
+#ifdef __KERNEL__
+
+#include <linux/stringify.h>
+#include <asm/cputable.h>
+#include <vbi/interface.h>
+#include <vbi/syscalls.h>
+
+/* macro used in on entry_32.S */
+#define PARAVIRT_ENABLE_MSR_EE      WRHV_INT_UNLOCK(r10,r11)
+#define PARAVIRT_DISABLE_MSR_EE     WRHV_INT_LOCK(r10,r3)
+
+/* macro used in misc.S */
+#undef PARAVIRT_MFSPR_SPRG3
+#define PARAVIRT_MFSPR_SPRG3(a)  WRHV_MFSPRG3(a)
+
+/* macro used in arch/powerpc/kernel/traps.c */
+#define PARAVIRT_DISABLE_INST_COMPLETION       do{ } while (0)
+#define PARAVIRT_CLEAR_INST_COMPLETION                 do{ } while (0)
+
+#ifdef __ASSEMBLY__
+.extern	var(wrhv_sprg3)
+.extern	var(wrhv_user)
+.extern var(wrhv_pir)
+
+#define WRHV_MFSPRG3(rd)                        \
+	lis	rd,wrhv_sprg3@ha;               \
+	lwz	rd,wrhv_sprg3@l(rd)
+
+#define WRHV_MTSPRG3(rs,tmpr)                   \
+	lis	tmpr,wrhv_sprg3@ha;             \
+	stw	rs,wrhv_sprg3@l(tmpr)
+
+#ifdef CONFIG_SMP
+#define WRHV_MFPIR(rd)				\
+	lis	rd,wrhv_pir@ha;		\
+	lwz	rd,wrhv_pir@l(rd);
+#endif
+
+#define WRHV_INT_LOCK(tmpr1,tmpr2)                      \
+	li	tmpr2,-1;                               \
+	lis	tmpr1,wr_control@ha;                   \
+	lwz	tmpr1,wr_control@l(tmpr1);             \
+	stw	tmpr2,VB_CONTROL_INT_DISABLE(tmpr1)
+
+#define WRHV_INT_UNLOCK(tmpr1,tmpr2)                    \
+	lis	tmpr1,wr_control@ha;                   \
+	lwz	tmpr1,wr_control@l(tmpr1);             \
+	li	tmpr2,0;                                \
+	stw	tmpr2,VB_CONTROL_INT_DISABLE(tmpr1);    \
+	lis	tmpr1,wr_status@ha;                    \
+	lwz	tmpr1,wr_status@l(tmpr1);              \
+	lwz	tmpr1,VB_STATUS_INT_PENDING(tmpr1);     \
+	cmpwi	0,tmpr1,0;                              \
+	beq	1f;                                     \
+	mr	tmpr1,r0;                               \
+	lis	r0,VBI_SYS_int_enable@h;                \
+	ori	r0,r0,VBI_SYS_int_enable@l;             \
+	sc;                                             \
+	mr	r0,tmpr1;                               \
+1:
+
+#define WRHV_INT_LVL_GET(rd)                            \
+	lis	rd,wr_control@ha;                      \
+	lwz	rd,wr_control@l(rd);                   \
+	lwz	rd,VB_CONTROL_INT_DISABLE(rd)
+
+#define WRHV_FIX_MSR(msr,tmpr)                                  \
+	rlwinm	msr,msr,0,18,15; /* Clear EE & PR bits */       \
+	WRHV_INT_LVL_GET(tmpr);                         \
+	cmpwi	0,tmpr,0;                                       \
+	bne	1f;                                             \
+	ori	msr,msr,MSR_EE;                                 \
+1:	lis	tmpr,wrhv_supervisor@ha;                        \
+	lwz	tmpr,wrhv_supervisor@l(tmpr);                   \
+	cmpwi	0,tmpr,0;                                       \
+	bne	2f;                                             \
+	ori	msr,msr,MSR_PR;                                 \
+2:
+
+#define WRHV_LOAD_MSR(msr,tmpr1,tmpr2)                          \
+	li	tmpr2,0;                                        \
+	rlwinm.	tmpr1,msr,0,16,16;      /* test EE bit */       \
+	bne	1f;                     /* IT unlocked? */      \
+	li	tmpr2,-1;                                       \
+1:	lis	tmpr1,wr_control@ha;                           \
+	lwz	tmpr1,wr_control@l(tmpr1);                     \
+	stw	tmpr2,VB_CONTROL_NEW_INT_DISABLE(tmpr1);        \
+	stw	msr,VB_CONTROL_SRR1(tmpr1);                     \
+	li	tmpr2,1;                                        \
+	rlwinm.	tmpr1,msr,0,17,17;      /* test PR bit */       \
+	beq	2f;                     /* priv. mode? */       \
+	li	tmpr2,0;                                        \
+2:	WRHV_SET_SUP_MODE(tmpr1,tmpr2)
+
+#define WRHV_FIX_MSR2(msr,tmpr)                         \
+	rlwinm	msr,msr,0,18,15; /* Clear EE & PR bits */       \
+	lis	tmpr,wr_status@ha;                             \
+	lwz	tmpr,wr_status@l(tmpr);                        \
+	lwz	tmpr,VB_STATUS_OLD_INT_DISABLE(tmpr);           \
+	cmpwi	0,tmpr,0;                                       \
+	bne	1f;                                             \
+	ori	msr,msr,MSR_EE;                                 \
+1:	lis	tmpr,wrhv_supervisor@ha;                        \
+	lwz	tmpr,wrhv_supervisor@l(tmpr);                   \
+	cmpwi	0,tmpr,0;                                       \
+	bne	2f;                                             \
+	ori	msr,msr,MSR_PR;                                 \
+2:
+
+#define WRHV_SET_SUP_MODE(tmpr,rs)                              \
+	lis	tmpr,wrhv_supervisor@ha;                        \
+	stw	rs,wrhv_supervisor@l(tmpr)
+
+#define WRHV_SUP_MODE_GET(rd)                                   \
+	lis	rd,wrhv_supervisor@ha;                          \
+	lwz	rd,wrhv_supervisor@l(rd)
+#endif /* __ASSEMBLY__ */
+
+#endif /* __KERNEL__ */
+#endif /* _ASM_POWERPC_REG_WRHV_H */
diff --git a/arch/powerpc/include/asm/wrhv.h b/arch/powerpc/include/asm/wrhv.h
new file mode 100644
index 0000000..275c203
--- /dev/null
+++ b/arch/powerpc/include/asm/wrhv.h
@@ -0,0 +1,32 @@
+/*
+ *  This program is free software; you can redistribute it and/or modify it
+ *  under the terms of the GNU General Public License as published by the
+ *  Free Software Foundation; either version 2, or (at your option) any
+ *  later version.
+ *
+ *  This program is distributed in the hope that it will be useful, but
+ *  WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ *  General Public License for more details.
+ *
+ *  Copyright (C) 2008 Wind River Systems, Inc.
+ */
+
+#ifndef __ASM_WRHV_H
+#define __ASM_WRHV_H
+
+#ifdef CONFIG_WRHV
+extern void wrhv_mapping(void);
+extern void wrhv_restart(char *cmd);
+extern unsigned long __init wrhv_find_end_of_memory(void);
+extern void wrhv_power_save(void);
+extern unsigned int wrhv_vioapic_get_irq(void);
+extern void wrhv_init_irq(void);
+extern void __init wrhv_calibrate_decr(void);
+extern void __init wrhv_time_init(void);
+extern int __init wrhv_earlycon_setup(void);
+
+extern unsigned long wrhv_cpu_freq;
+
+#endif /* CONFIG_WRHV */
+#endif /* __ASM_WRHV_H */
diff --git a/arch/powerpc/kernel/dma.c b/arch/powerpc/kernel/dma.c
index 6c1df57..c9e4c35 100644
--- a/arch/powerpc/kernel/dma.c
+++ b/arch/powerpc/kernel/dma.c
@@ -13,6 +13,10 @@
 #include <asm/bug.h>
 #include <asm/abs_addr.h>
 
+#ifdef CONFIG_WRHV
+#include <vbi/vbi.h>
+#endif
+
 /*
  * Generic direct DMA implementation
  *
@@ -27,6 +31,9 @@ void *dma_direct_alloc_coherent(struct device *dev, size_t size,
 				dma_addr_t *dma_handle, gfp_t flag)
 {
 	void *ret;
+#ifdef CONFIG_WRHV
+	u64 paddr;
+#endif
 #ifdef CONFIG_NOT_COHERENT_CACHE
 	ret = __dma_alloc_coherent(dev, size, dma_handle, flag);
 	if (ret == NULL)
@@ -45,7 +52,16 @@ void *dma_direct_alloc_coherent(struct device *dev, size_t size,
 		return NULL;
 	ret = page_address(page);
 	memset(ret, 0, size);
+#ifdef CONFIG_WRHV
+	if (vbi_get_guest_dma_addr((ret + get_dma_offset(dev)), &paddr) == 0) {
+		*dma_handle = (dma_addr_t)paddr;
+	} else {
+		free_pages((unsigned long)ret, get_order(size));
+		ret = NULL;
+	}
+#else
 	*dma_handle = virt_to_abs(ret) + get_dma_offset(dev);
+#endif
 
 	return ret;
 #endif
@@ -67,9 +83,19 @@ static int dma_direct_map_sg(struct device *dev, struct scatterlist *sgl,
 {
 	struct scatterlist *sg;
 	int i;
+#ifdef CONFIG_WRHV
+	u64 paddr;
+	u32 ptr;
+#endif
 
 	for_each_sg(sgl, sg, nents, i) {
+#ifdef CONFIG_WRHV
+		ptr = sg_phys(sg) + get_dma_offset(dev);
+		vbi_get_guest_dma_addr((void *)ptr, &paddr);
+		sg->dma_address = (dma_addr_t)paddr;
+#else
 		sg->dma_address = sg_phys(sg) + get_dma_offset(dev);
+#endif
 		sg->dma_length = sg->length;
 		__dma_sync_page(sg_page(sg), sg->offset, sg->length, direction);
 	}
@@ -102,9 +128,19 @@ static inline dma_addr_t dma_direct_map_page(struct device *dev,
 					     enum dma_data_direction dir,
 					     struct dma_attrs *attrs)
 {
+#ifdef CONFIG_WRHV
+	u64 paddr;
+	u32 ptr;
+#endif
 	BUG_ON(dir == DMA_NONE);
 	__dma_sync_page(page, offset, size, dir);
+#ifdef CONFIG_WRHV
+	ptr = (u32)page_to_phys(page) + offset + get_dma_offset(dev);
+	vbi_get_guest_dma_addr((void *)ptr, &paddr);
+	return (dma_addr_t)paddr;
+#else
 	return page_to_phys(page) + offset + get_dma_offset(dev);
+#endif
 }
 
 static inline void dma_direct_unmap_page(struct device *dev,
diff --git a/arch/powerpc/kernel/head_wrhv.h b/arch/powerpc/kernel/head_wrhv.h
new file mode 100644
index 0000000..cbb439f
--- /dev/null
+++ b/arch/powerpc/kernel/head_wrhv.h
@@ -0,0 +1,146 @@
+/*
+ *  This program is free software; you can redistribute it and/or modify it
+ *  under the terms of the GNU General Public License as published by the
+ *  Free Software Foundation; either version 2, or (at your option) any
+ *  later version.
+ *
+ *  This program is distributed in the hope that it will be useful, but
+ *  WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ *  General Public License for more details.
+ *
+ *  Copyright (C) 2008 Wind River Systems, Inc.
+ */
+
+#ifndef __HEAD_WRHV_H__
+#define __HEAD_WRHV_H__
+
+#include <asm/arch_vbi.h>
+
+	/* Interrupts are disabled by hypervisor at this entry point.
+	 * It puts the following registers into the status page:
+	 *   VB_STATUS_OLD_INT_DISABLE (the INT_DISABLE from Control)
+	 *   CR register
+	 *   SRR0 register - pc at time of interrupt
+	 *   SRR1 register - msr at time of interrupt
+	 *   LR register - link register at time of interrupt
+	 *   R3 register - R3 at time of interrupt
+	 *   R4 register - R4 at time of interrupt
+	 * When code in this macro has been executed, r9 contains MSR, r12 pc
+	 * r10 is trashed and r11 pointer on interrupt frame. All other
+	 * registers contain their value before the system call was executed.
+	 */
+#undef NORMAL_EXCEPTION_PROLOG
+#define NORMAL_EXCEPTION_PROLOG						     \
+        mr      r4,r1;                                                       \
+        WRHV_SUP_MODE_GET(r3);         /* check whether user or kernel */   \
+        cmpwi   0,r3,0;                                                 \
+        bne     1f;                                                          \
+        WRHV_MFSPRG3(r1);              /* if from user, start at top of   */\
+        lwz     r1,THREAD_INFO-THREAD(r1); /* this thread's kernel stack   */\
+        addi    r1,r1,THREAD_SIZE;                                           \
+1:      subi    r1,r1,INT_FRAME_SIZE;   /* Allocate an exception frame     */\
+        mr      r3,r1;                                                       \
+        stw     r0,GPR0(r3);                                                 \
+        stw     r4,GPR1(r3);                                                 \
+        stw     r4,0(r3);                                                    \
+        SAVE_4GPRS(5, r3);                                                   \
+        SAVE_4GPRS(9, r3);                                                   \
+        mr      r11,r3;                                                      \
+        lis     r4,wr_status@ha;                                           \
+        lwz     r4,wr_status@l(r4);                                        \
+        lwz     r12,VB_STATUS_LR(r4);                                        \
+        stw     r12,_LINK(r11);                                              \
+        lwz     r12,VB_STATUS_R3(r4);                                        \
+        stw     r12,GPR3(r11);                                               \
+        lwz     r12,VB_STATUS_R4(r4);                                        \
+        stw     r12,GPR4(r11);                                               \
+        lwz     r12,VB_STATUS_CR(r4);                                        \
+        stw     r12,_CCR(r11);                                               \
+        lwz     r9,VB_STATUS_SRR1(r4);                                       \
+	rlwinm  r9,r9,0,18,15; /* Clear EE & PR bits */                      \
+	mr	r12, r4;                                                     \
+        lwz     r12,VB_STATUS_OLD_INT_DISABLE(r12);                          \
+        cmpwi   0,r12,0;                                                     \
+        bne     2f;                                                          \
+        ori     r9,r9,MSR_EE;                                                \
+2:      lis     r12,wrhv_supervisor@ha;                                     \
+        lwz     r12,wrhv_supervisor@l(r12);                                \
+        cmpwi   0,r12,0;                                                     \
+        bne     3f;                                                          \
+        ori     r9,r9,MSR_PR;                                                \
+3:      li      r12,1;                                                       \
+        WRHV_SET_SUP_MODE(r3,r12);                                          \
+        lwz     r12,VB_STATUS_SRR0(r4);                                      \
+        lwz     r3,VB_STATUS_R3(r4);                                         \
+        mr      r10,r4;                                                      \
+        lwz     r4,VB_STATUS_R4(r4)
+
+
+/*
+ * Exception vectors.
+ */
+#if defined (CONFIG_WR_OCD_DEBUG) && defined (CONFIG_BOOKE)
+#define	START_EXCEPTION(label)						     \
+        .align 5;              						     \
+label:									     \
+	nop;								     \
+	nop;								     \
+	nop;								     \
+	isync;
+#else
+#ifdef CONFIG_WRHV
+#undef START_EXCEPTION
+#define        START_EXCEPTION(label)                                  \
+       .align 8;                                               \
+label:
+#else
+#define        START_EXCEPTION(label)                                  \
+       .align 5;                                               \
+label:
+#endif /* CONFIG_WRHV */
+#endif
+
+#undef DEBUG_DEBUG_EXCEPTION
+#define DEBUG_DEBUG_EXCEPTION						      \
+	START_EXCEPTION(DebugDebug);						\
+	NORMAL_EXCEPTION_PROLOG;					\
+	mr      r4,r12;                /* Pass SRR0 as arg2 */		\
+	lwz     r5,VB_STATUS_ESR(r10);					\
+	stw     r5,_ESR(r11);						\
+	addi    r3,r1,STACK_FRAME_OVERHEAD;				\
+	/* EXC_XFER_STD(0x1000, DebugException)	*/		\
+	EXC_XFER_TEMPLATE(DebugException, 0x2008, (MSR_KERNEL & ~(MSR_ME|MSR_DE|MSR_CE)), NOCOPY, transfer_to_handler_full, ret_from_except_full)
+
+#undef INSTRUCTION_STORAGE_EXCEPTION
+#define INSTRUCTION_STORAGE_EXCEPTION					      \
+	START_EXCEPTION(InstructionStorage)				      \
+	NORMAL_EXCEPTION_PROLOG;					      \
+	mr      r4,r12;                /* Pass SRR0 as arg2 */                \
+        lwz     r5,VB_STATUS_ESR(r10);                                        \
+        stw     r5,_ESR(r11);                                                 \
+        li      r5,0;                   /* Pass zero as arg3 */               \
+	EXC_XFER_EE_LITE(0x0400, handle_page_fault)
+
+#undef PROGRAM_EXCEPTION
+#define PROGRAM_EXCEPTION						      \
+	START_EXCEPTION(Program)					      \
+	NORMAL_EXCEPTION_PROLOG;					      \
+	mr      r4,r12;               /* Pass SRR0 as arg2 */                \
+        lwz     r5,VB_STATUS_ESR(r10);                                        \
+	stw	r5,_ESR(r11);						      \
+	addi	r3,r1,STACK_FRAME_OVERHEAD;				      \
+	EXC_XFER_STD(0x0700, program_check_exception)
+
+#undef DECREMENTER_EXCEPTION
+#define DECREMENTER_EXCEPTION						      \
+	START_EXCEPTION(Decrementer)					      \
+	NORMAL_EXCEPTION_PROLOG;					      \
+	addi    r3,r1,STACK_FRAME_OVERHEAD;				      \
+	EXC_XFER_LITE(0x0900, timer_interrupt)
+
+
+/* ensure this structure is always sized to a multiple of the stack alignment */
+#define STACK_EXC_LVL_FRAME_SIZE	_ALIGN_UP(sizeof (struct exception_regs), 16)
+
+#endif /* __HEAD_BOOKE_H__ */
diff --git a/arch/powerpc/kernel/paravirt.c b/arch/powerpc/kernel/paravirt.c
new file mode 100644
index 0000000..50939cc
--- /dev/null
+++ b/arch/powerpc/kernel/paravirt.c
@@ -0,0 +1,271 @@
+/*  Paravirtualization interfaces
+
+    This program is free software; you can redistribute it and/or modify
+    it under the terms of the GNU General Public License as published by
+    the Free Software Foundation; either version 2 of the License, or
+    (at your option) any later version.
+
+    This program is distributed in the hope that it will be useful,
+    but WITHOUT ANY WARRANTY; without even the implied warranty of
+    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+    GNU General Public License for more details.
+
+    You should have received a copy of the GNU General Public License
+    along with this program; if not, write to the Free Software
+    Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
+
+    Adaptation for powerpc based on x86 version, Copyright (C) 2009
+    Wind River Systems, Inc.
+
+*/
+
+#include <linux/errno.h>
+#include <linux/module.h>
+#include <linux/efi.h>
+#include <linux/bcd.h>
+#include <linux/highmem.h>
+
+#include <asm/bug.h>
+#include <asm/setup.h>
+#include <asm/pgtable.h>
+#include <asm/time.h>
+#include <asm/pgalloc.h>
+#include <asm/irq.h>
+#include <asm/delay.h>
+#include <asm/fixmap.h>
+#include <asm/tlbflush.h>
+#include <asm/machdep.h>
+#include <asm/fs_pd.h>
+
+#include <linux/kernel.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/mm.h>
+#include <linux/stddef.h>
+#include <linux/init.h>
+#include <linux/bootmem.h>
+#include <linux/initrd.h>
+#include <linux/pagemap.h>
+
+#include <linux/kprobes.h>
+#include <linux/kexec.h>
+#include <linux/backlight.h> 
+#include <linux/bug.h>
+#include <linux/kdebug.h>
+#include <linux/ltt-core.h>
+#include <trace/trap.h>
+#include <linux/kallsyms.h> 
+
+#include <mm/mmu_decl.h>
+#include <linux/lmb.h>
+
+#include <linux/major.h>
+#include <linux/device.h>
+#include <linux/platform_device.h>
+#include <linux/of_platform.h>
+#include <linux/phy.h>
+#include <linux/phy_fixed.h>
+#include <linux/spi/spi.h>
+#include <linux/fsl_devices.h>
+#include <linux/fs_enet_pd.h>
+#include <linux/fs_uart_pd.h>
+
+#include <asm/system.h>
+#include <asm/atomic.h>
+#include <asm/io.h>
+#include <asm/prom.h>
+#include <sysdev/fsl_soc.h>
+#include <asm/cpm2.h>
+
+#include <linux/kgdb.h>
+#include <linux/smp.h>
+#include <linux/signal.h>
+#include <linux/ptrace.h>
+#include <asm/current.h>
+#include <asm/processor.h>
+
+#include <asm/wrhv.h>
+#include <asm/paravirt.h>
+
+#ifdef CONFIG_WRHV
+extern void wrhv_init(void);
+#endif
+
+/* XXX fixme - use an existing implementation */
+#if 1
+#define DEBUGP printk
+#else
+#define DEBUGP(fmt , ...)       do { } while (0)
+#endif
+
+/* paravirt init */
+void paravirt_init(void)
+{
+#ifdef CONFIG_WRHV
+	wrhv_init();
+#endif
+}
+
+struct pv_info pv_info = {
+        .name = "bare hardware",
+        .paravirt_enabled = 0,
+};
+
+/* default native operations */
+struct pv_time_ops pv_time_ops = {
+	.time_init_cont = native_time_init_cont,
+	.timer_interrupt = native_timer_interrupt,
+	.clocksource_init = native_clocksource_init,
+};
+
+struct pv_irq_ops pv_irq_ops = {
+	.do_IRQ = native_do_IRQ,
+	.irq_of_parse_and_map = native_irq_of_parse_and_map,
+};
+
+struct pv_cpu_ops pv_cpu_ops = {
+	.get_pvr = native_get_pvr,
+	.DebugException = native_DebugException,
+	.kgdb_arch_handle_exception = native_kgdb_arch_handle_exception,
+	.ppc_proc_freq = native_ppc_proc_freq,
+};
+
+struct pv_mmu_ops pv_mmu_ops = {
+	.vmmu_restore = native_vmmu_restore,
+	.MMU_init_hw = native_MMU_init_hw,
+	.mmu_mapin_ram = native_mmu_mapin_ram,
+	.MMU_setup = native_MMU_setup,
+	.MMU_init = native_MMU_init,
+	.flush_dcache_page = native_flush_dcache_page,
+	.map_page = native_map_page,
+	.early_init_dt_scan_memory_ppc =
+		native_early_init_dt_scan_memory_ppc,
+	.__ioremap = native___ioremap,
+	.__set_pte_at = native__set_pte_at,
+};
+
+
+/* pv_time_ops */
+void __init paravirt_time_init_cont(void)
+{
+	pv_time_ops.time_init_cont();
+}
+
+void paravirt_timer_interrupt(struct pt_regs * regs)
+{	
+	pv_time_ops.timer_interrupt(regs);
+}
+
+void __init paravirt_clocksource_init(void)
+{
+	pv_time_ops.clocksource_init();
+}
+
+
+/* pv_irq_ops */
+void paravirt_do_IRQ(struct pt_regs *regs)
+{
+	pv_irq_ops.do_IRQ(regs);
+}
+
+
+unsigned int paravirt_irq_of_parse_and_map(struct device_node *dev, int index)
+{
+	return pv_irq_ops.irq_of_parse_and_map(dev, index);
+}
+
+/* pv_cpu_ops */
+unsigned int paravirt_get_pvr(void)
+{
+	return pv_cpu_ops.get_pvr();
+}
+
+void __kprobes paravirt_DebugException(struct pt_regs *regs, unsigned long debug_status)
+{
+	pv_cpu_ops.DebugException(regs, debug_status);
+
+}
+int paravirt_kgdb_arch_handle_exception(int vector, int signo, int err_code,
+			char *remcom_in_buffer, char *remcom_out_buffer,
+			struct pt_regs *linux_regs)
+{
+
+	return pv_cpu_ops.kgdb_arch_handle_exception(vector, signo, err_code,
+			remcom_in_buffer, remcom_out_buffer, linux_regs);
+}
+
+
+int paravirt_ppc_proc_freq(void)
+{
+	return pv_cpu_ops.ppc_proc_freq();
+}
+
+/* pv_mmu_ops */
+void paravirt_vmmu_restore (void)
+{
+	pv_mmu_ops.vmmu_restore();
+}
+
+void __init paravirt_MMU_init_hw(void)
+{
+	pv_mmu_ops.MMU_init_hw();
+}
+
+unsigned long __init paravirt_mmu_mapin_ram(unsigned long top)
+{
+	return pv_mmu_ops.mmu_mapin_ram(top);
+}
+
+void paravirt_MMU_setup(void)
+{
+	pv_mmu_ops.MMU_setup();
+}
+
+void __init paravirt_MMU_init(void)
+{
+	pv_mmu_ops.MMU_init();
+}
+
+void paravirt_flush_dcache_page(struct page *page)
+{
+	pv_mmu_ops.flush_dcache_page(page);
+}
+
+int paravirt_map_page(unsigned long va, phys_addr_t pa, int flags)
+{
+	return	pv_mmu_ops.map_page(va, pa, flags);
+}
+
+int paravirt_early_init_dt_scan_memory_ppc(unsigned long node,
+		const char *uname, int depth, void *data)
+{
+       return pv_mmu_ops.early_init_dt_scan_memory_ppc(node,
+					uname, depth, data);
+}
+
+void paravirt___ioremap(phys_addr_t addr, unsigned long size, unsigned long flags)
+{
+	pv_mmu_ops.__ioremap(addr, size, flags);
+}
+
+void paravirt__set_pte_at(struct mm_struct *mm, unsigned long addr, 
+					pte_t *ptep, pte_t pte, int percpu) 
+{
+	pv_mmu_ops.__set_pte_at(mm, addr, ptep, pte, percpu);
+}
+
+inline int paravirt_enabled(void)
+{
+        return pv_info.paravirt_enabled;
+}
+
+extern struct pv_time_ops pv_time_ops;
+extern struct pv_cpu_ops pv_cpu_ops;
+extern struct pv_irq_ops pv_irq_ops;
+extern struct pv_mmu_ops pv_mmu_ops; 
+
+EXPORT_SYMBOL    (pv_info);
+EXPORT_SYMBOL    (pv_time_ops);
+EXPORT_SYMBOL    (pv_cpu_ops);
+EXPORT_SYMBOL    (pv_mmu_ops);
+EXPORT_SYMBOL    (pv_irq_ops);
diff --git a/arch/powerpc/kernel/vbi/util.c b/arch/powerpc/kernel/vbi/util.c
new file mode 100644
index 0000000..62d84cb
--- /dev/null
+++ b/arch/powerpc/kernel/vbi/util.c
@@ -0,0 +1,142 @@
+/*
+ * util.c - utilities routines for guest OS para-virtualization
+ *
+ * Copyright (c) 2009 Wind River Systems, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ * See the GNU General Public License for more details.
+ *
+ */
+
+/*
+This module implements a library which is handy for para-virtualized
+guest os to use. The routines are developed based on the need while
+para-virtualize linux, therefore, may need some tweaks to be generic.
+*/
+
+#include <asm/page.h>
+#include <linux/module.h>
+#include <vbi/interface.h>
+#include <vbi/vmmu.h>
+#include <vbi/syscall.h>
+#include <vbi/vbi.h>
+
+
+/* defines */
+
+/* globals */
+
+/*
+ * wr_config is initialized as part of the guest os init, before os turns on
+ * MMU. For paravirualized linux, it is initialized in plaform_init().
+ */
+
+extern struct vb_config *wr_config;
+extern struct vb_status *wr_status;
+extern struct vb_control *wr_control;
+
+/* local */
+
+/* extern */
+extern void pteAttrSet(VMMU_PTE * pte, u_int attr);
+extern void vmmuPageTableDisplay(VMMU_LEVEL_1_DESC *l1, int vmmuon);
+
+/* forward declarations */
+
+/*
+ * vb_memsize_get should not be called before wr_config is initialized
+ */
+unsigned int vb_memsize_get(void)
+{
+	if (wr_config == (struct vb_config *)(-1)) 
+		return 0;
+	return VBI_MEM_SIZE_GET();
+}
+
+unsigned int vb_context_get(void)
+{
+	if (wr_config == (struct vb_config *)(-1))
+		return 0xdeadbee0;
+	return VBI_CONTEXT_ID_GET();
+}
+
+void vb_pte_set(void *pPte, unsigned long paddr, int protval)
+{
+
+	/* caller has guaranteed pPte != NULL */
+
+	*(uint *) pPte = (uint) VMMU_PTE_VALID_MASK;
+
+	/* linux uses more than the permission bits, in word1 of PTE */
+
+	*((uint *) ((uint *) pPte) + 1) = (((u_int) paddr & VMMU_PTE_RPN_MASK) | (protval & 0xfff));
+
+	return;
+}
+
+/*
+ * turn on mmu for the particular context
+ *
+ * note, caller must make sure, context switch inside the guest OS must
+ * not happen during this call.
+ */
+
+int vb_context_mmu_on(int pid,	/* context id */
+		      void *pgtable,	/* level 1 page table */
+		      int pagesize, int debug)
+{
+	static VMMU_CONFIG vmmu_cfg;
+
+	if (wr_config == (struct vb_config *)(- 1) || pgtable == NULL || pagesize <= 0)
+		return -1;
+
+	vmmu_cfg.addr = (VMMU_LEVEL_1_DESC *) pgtable;
+	vmmu_cfg.pageSize = pagesize;
+	vmmu_cfg.contextId = pid;
+	vmmu_cfg.vmmuNum = 0;	/* only vmmu 0 is support for the time being */
+
+	if ((vbi_config_vmmu(&vmmu_cfg)) != 0)
+		return -1;
+
+	if (debug) {
+		printk("L1 page table address %p\n", pgtable);
+		vmmuPageTableDisplay(pgtable, 0);
+		printk("End of page table display \n");
+	}
+
+	vbi_enable_vmmu(vmmu_cfg.vmmuNum);
+
+	return 0;
+}
+
+void vb__flush_dcache_icache(void *start)
+{
+	vbi_flush_icache(start, 4096);
+	vbi_flush_dcache(start, 4096);
+}
+
+void vb_flush_dcache_range(unsigned long start, unsigned long stop)
+{
+	vbi_flush_dcache((void *) start, (stop - start + 1));
+}
+
+void vb__flush_icache_range(unsigned long start, unsigned long stop)
+{
+	vbi_flush_icache((void *) start, (stop - start + 1));
+}
+
+void vb__flush_dcache_icache_phys(unsigned long physaddr)
+{
+	vbi_flush_icache((void *) physaddr, 4096);
+	vbi_flush_dcache((void *) physaddr, 4096);
+}
+
+EXPORT_SYMBOL(wrhv_int_lock);
+EXPORT_SYMBOL(wrhv_int_unlock);
+EXPORT_SYMBOL(wrhv_int_lvl_get);
diff --git a/arch/powerpc/kernel/vbi/vmmu_display.c b/arch/powerpc/kernel/vbi/vmmu_display.c
new file mode 100644
index 0000000..d1fc41a
--- /dev/null
+++ b/arch/powerpc/kernel/vbi/vmmu_display.c
@@ -0,0 +1,140 @@
+/*
+ * vmmu_display.c - hypervisor VMMU operations
+ *
+ * Copyright (c) 2009 Wind River Systems, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ * See the GNU General Public License for more details.
+ *
+ */
+
+#include <linux/kernel.h>
+#include <vbi/interface.h>
+#include <vbi/vmmu.h>
+
+#define __va(paddr) (((unsigned long )(paddr))+0xC0000000)
+#define __pa(vaddr) (((unsigned long )(vaddr))-0xC0000000)
+
+
+/*
+ *
+ * vmmuPageTableDisplay - display information about the specified page table
+ *
+ * This routine display all the VMMU PTE entries in the specified table
+ *
+ */
+
+void vmmuPageTableDisplay(VMMU_LEVEL_1_DESC *l1, int vmmuon)
+{
+	VMMU_LEVEL_2_DESC *l2;
+	VMMU_PTE *pte;
+	VMMU_EFFECTIVE_ADDR ea;
+	u_int l1_index;
+	u_int i,j;
+
+	l1_index = 0;
+	ea.addr = 0;
+
+	printk("Logical           Physical          R C U[01234567] WIMGE S[XWR] U[XWR]\n");
+	printk("----------------- ----------------- - -  ---------- -----  -----  -----\n");
+
+	/* run through all the entries */
+	for (i=0; i<VMMU_L1_ENTRIES; i++) {
+		if (l1->field.v) {
+			ea.field.l1index = l1_index;
+			l2 = (VMMU_LEVEL_2_DESC *)VMMU_LBA_TO_ADDR(l1->field.l2ba);
+			if (vmmuon)
+				l2 = (VMMU_LEVEL_2_DESC *)__va(l2);
+
+			pte = (VMMU_PTE *)l2;
+
+			for (j=0; j<VMMU_L2_ENTRIES; j++) {
+				if (pte->field.v) {
+					ea.field.l2index = j;
+					printk ("%08x-%08x %08x-%08x %d %d ",
+					(u_int)ea.addr, (u_int)ea.addr + 0xfff,
+					pte->field.rpn << VMMU_RPN_SHIFT,
+					(pte->field.rpn << VMMU_RPN_SHIFT) + 0xfff,
+					pte->field.r, pte->field.c);
+					printk ("  %d%d%d%d%d%d%d%d  %d%d%d%d%d   %c%c%c    %c%c%c\n",
+					pte->field.u0, pte->field.u1,
+					pte->field.u2, pte->field.u3,
+					pte->field.u4, pte->field.u5,
+					pte->field.u6, pte->field.u7,
+					pte->field.w, pte->field.i,
+					pte->field.m,
+					pte->field.g, pte->field.e,
+					pte->field.sx ? 'X' : ' ',
+					pte->field.sw ? 'W' : ' ',
+					pte->field.sr ? 'R' : ' ',
+					pte->field.ux ? 'X' : ' ',
+					pte->field.uw ? 'W' : ' ',
+					pte->field.ur ? 'R' : ' ');
+				} /* pte field.v */
+				pte++;
+			} /* j */
+		} /* l1 field.v */
+		l1++;
+		l1_index++;
+	} /* i */
+}
+
+/*
+ * vmmuPteDisplay - display a specific PTE entry
+ *
+ * This routine display the VMMU PTE entrie corresponding to the specified
+ * virtual address.
+ *
+ */
+unsigned int vmmuPteDisplay(VMMU_LEVEL_1_DESC *l1, void *vaddr)
+{
+	VMMU_LEVEL_2_DESC  *l2;
+	VMMU_PTE *pte;
+
+	/* find the level-1 page table descriptor for the virtual address */
+	l1 += VMMU_L1_INDEX(vaddr);
+
+	/* if no level-2 table exists abort and return error */
+	if (!l1->field.v)
+		return -1;
+
+	/* locate correct PTE entry in level-2 table */
+	l2  = (VMMU_LEVEL_2_DESC *)VMMU_LBA_TO_ADDR(l1->field.l2ba) +
+		VMMU_L2_INDEX(vaddr);
+
+	l2 = (VMMU_LEVEL_2_DESC *)__va(l2);
+
+	pte = &l2->pte;
+
+	if (!pte->field.v)
+		return  -1;
+
+	printk("PTE for virtual address 0x%p:\n", vaddr);
+	printk("  Page Number:  0x%08x\n", pte->field.rpn<<VMMU_RPN_SHIFT);
+	printk("  Referenced:   %d\n", pte->field.r);
+	printk("  Changed:      %d\n", pte->field.c);
+	printk("  User bits:    %d%d%d%d%d%d%d%d\n",
+		pte->field.u0, pte->field.u1,
+		pte->field.u2, pte->field.u3,
+		pte->field.u4, pte->field.u5,
+		pte->field.u6, pte->field.u7);
+	printk("  WIMGE:        %d%d%d%d%d\n",
+		pte->field.w, pte->field.i, pte->field.m,
+		pte->field.g, pte->field.e);
+	printk("  Supv Perms:   %c%c%c\n",
+		pte->field.sr ? 'R' : '-',
+		pte->field.sw ? 'W' : '-',
+		pte->field.sx ? 'X' : '-');
+	printk("  User Perms:   %c%c%c\n",
+		pte->field.ur ? 'R' : '-',
+		pte->field.uw ? 'W' : '-',
+		pte->field.ux ? 'X' : '-');
+
+	return 0;
+}
diff --git a/arch/powerpc/kernel/vbi/wrhv.c b/arch/powerpc/kernel/vbi/wrhv.c
new file mode 100644
index 0000000..e0d6679
--- /dev/null
+++ b/arch/powerpc/kernel/vbi/wrhv.c
@@ -0,0 +1,1065 @@
+/*
+ *  This program is free software; you can redistribute it and/or modify it
+ *  under the terms of the GNU General Public License as published by the
+ *  Free Software Foundation; either version 2, or (at your option) any
+ *  later version.
+ *
+ *  This program is distributed in the hope that it will be useful, but
+ *  WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ *  General Public License for more details.
+ *
+ *  Copyright (C) 2009 Wind River Systems, Inc.
+ */
+
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/irq.h>
+#include <linux/profile.h>
+#include <linux/wrhv.h>
+#include <linux/interrupt.h>
+#include <linux/vmalloc.h>
+#include <vbi/interface.h>
+#include <vbi/interrupt.h>
+#include <vbi/errors.h>
+
+#include <asm/page.h>
+#include <asm/pgtable.h>
+#include <asm/time.h>
+
+#include <linux/threads.h>
+#include <linux/kernel_stat.h>
+#include <linux/signal.h>
+#include <linux/sched.h>
+#include <linux/ptrace.h>
+#include <linux/ioport.h>
+#include <linux/timex.h>
+#include <linux/slab.h>
+#include <linux/delay.h>
+
+#include <linux/seq_file.h>
+#include <linux/cpumask.h>
+#include <linux/bitops.h>
+#include <linux/list.h>
+#include <linux/radix-tree.h>
+#include <linux/mutex.h>
+#include <linux/bootmem.h>
+#include <linux/pci.h>
+#include <linux/debugfs.h>
+
+#include <asm/uaccess.h>
+#include <asm/system.h>
+#include <asm/io.h>
+#include <asm/cache.h>
+#include <asm/prom.h>
+#include <asm/machdep.h>
+#include <asm/udbg.h>
+#include <asm/firmware.h>
+
+#include <asm/pgalloc.h>
+#include <asm/mmu_context.h>
+#include <asm/mmu.h>
+#include <asm/smp.h>
+#include <asm/btext.h>
+#include <asm/tlb.h>
+#include <asm/sections.h>
+#include <asm/pgtable.h>
+
+#include <linux/sched.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/mm.h>
+#include <linux/stddef.h>
+#include <linux/highmem.h>
+#include <linux/initrd.h>
+#include <linux/pagemap.h>
+
+#include <linux/kprobes.h>
+#include <linux/kexec.h>
+#include <linux/backlight.h>
+#include <linux/bug.h>
+#include <linux/kdebug.h>
+#include <linux/kallsyms.h>
+
+#include <mm/mmu_decl.h>
+#include <linux/lmb.h>
+
+#include <linux/major.h>
+#include <linux/device.h>
+#include <linux/platform_device.h>
+#include <linux/of_platform.h>
+#include <linux/phy.h>
+#include <linux/phy_fixed.h>
+#include <linux/spi/spi.h>
+#include <linux/fsl_devices.h>
+#include <linux/fs_enet_pd.h>
+#include <linux/fs_uart_pd.h>
+
+#include <asm/irq.h>
+#include <sysdev/fsl_soc.h>
+#include <asm/cpm2.h>
+
+#include <asm/current.h>
+#include <asm/processor.h>
+
+#include <asm/paravirt.h>
+
+#include <linux/perf_event.h>
+#include <asm/trace.h>
+#include <asm/ptrace.h>
+
+/* powerpc clocksource/clockevent code */
+
+#include <linux/clockchips.h>
+#include <linux/clocksource.h>
+#include <linux/kgdb.h>
+
+#include <asm/cputhreads.h>
+#include <linux/irq.h>
+#include <asm/tlb.h>
+#include <asm/arch_vbi.h>
+
+#include <vbi/vbi.h>
+#include <vbi/interface.h>
+
+static struct vb_config __wr_config;
+struct vb_config *wr_config;		/* TODO kernel relocation friendly ? */
+struct vb_control *wr_control;
+struct vb_status *wr_status;
+EXPORT_SYMBOL(wr_config);
+
+void wrhv_mapping(void);
+void mpc85xx_power_down(void);
+
+extern int map_page(unsigned long, phys_addr_t, int);
+
+extern int vb_context_mmu_on(int pid,  /* context id */
+			void *pgtable,    /* level 1 page table */
+			int pagesize, int debug);
+
+/* declared in linux/arch/powerpc/kernel/time.c */
+
+#define p_mapped_by_bats(x)     (0UL)
+#define p_mapped_by_tlbcam(x)   (0UL)
+
+unsigned long wrhv_cpu_freq = 0;
+
+int wrhv_earlycon = -1;
+int wrhv_pci_devfn = -1;
+char wrhv_macaddr[6];
+
+#define WRHV_EARLYCON_SIZE  14  /* sizeof("wrhv_earlycon=") */
+int __init wrhv_earlycon_setup(void)
+{
+	char *p = NULL;
+
+	if((p=strstr(cmd_line, "wrhv_earlycon=")) != NULL){
+	/* Since the maximal number supported of serial port is 8
+	 * in legacy serial, so here we just use this convention
+	 */
+		wrhv_earlycon =  p[WRHV_EARLYCON_SIZE] - 0x30;
+		printk(KERN_INFO "WRHV: early serial output port is at %d\n",
+			wrhv_earlycon);
+		return 1;
+	}
+
+	return 0;
+}
+
+static int __init wrhv_pci_devfn_setup(char *s)
+{
+	wrhv_pci_devfn = simple_strtoul(s, NULL, 0);
+	return 1;
+}
+
+__setup("wrhv_pci_devfn=", wrhv_pci_devfn_setup);
+
+static int __init wrhv_macaddr_setup(char *str)
+{
+	int i;
+
+	for (i = 0; i < 6; i++) {
+		int ret, oct;
+
+		ret = get_option(&str, &oct);
+		if (!ret)
+			break;
+
+		wrhv_macaddr[i] = oct;
+		if (ret != 2)
+			break;
+	}
+	return 1;
+}
+
+__setup("wrhv_macaddr=", wrhv_macaddr_setup);
+
+uint32_t service_handle;
+void get_hv_bsp_server_handle(void)
+{
+	int32_t rc;
+
+	rc = vbi_ns_lookup ("bspServer", 0, &service_handle);
+	if (rc)
+		printk ("bspServer lookup returned error code: %d\n", rc);
+}
+
+int bsp_service_handle(VBI_BSP_MSG *ask_msg, VBI_BSP_MSG_REPLY *reply_msg)
+{
+	int32_t rc = -1;
+
+	if (!service_handle) {
+		printk(KERN_ERR "Can't get bsp service handle!\n");
+		return rc;
+	}
+
+	rc = vbi_send (service_handle, ask_msg, sizeof(VBI_BSP_MSG),
+		reply_msg, sizeof(VBI_BSP_MSG_REPLY), NULL, NULL);
+
+	if (rc)
+		printk("vbi_send to the bspServer returned error code: %d\n", rc);
+
+        return rc;
+}
+
+uint32_t get_bsp_clock_freq(void)
+{
+	VBI_BSP_MSG clk_msg;
+	VBI_BSP_MSG_REPLY clk_reply;
+	uint16_t rc = -1;
+
+	clk_msg.request = VBI_BSP_CLK_FREQ;
+
+	rc = bsp_service_handle(&clk_msg, &clk_reply);
+	if (rc)
+		return rc;
+	return (clk_reply.dataVal);
+}
+
+void wrhv_mapping(void)
+{
+	/* map in vbConfig address */
+
+	/*
+	 * WRHV vb_config should really add a length field for each
+	 * of the objected we mapped.  As the sizes are WRHV
+	 * implementation AND worse, configuration dependent.
+	 *
+	 * For now, we just use 1 page which is fine for the time being.
+	 */
+
+	map_page((unsigned long)wr_config, (unsigned long)wr_config,
+		 PAGE_KERNEL_X);
+
+	map_page((unsigned long)wr_config->vb_status,
+		 (unsigned long)wr_config->vb_status, PAGE_KERNEL_X);
+	map_page((unsigned long)wr_config->vb_control,
+		 (unsigned long)wr_config->vb_control, PAGE_KERNEL_X);
+	map_page((unsigned long)wr_config->interruptConfiguration,
+		 (unsigned long)wr_config->interruptConfiguration, PAGE_KERNEL_X);
+	map_page((unsigned long)wr_config->vb_control->vIoapic,
+		 (unsigned long)wr_config->vb_control->vIoapic, PAGE_KERNEL_X);
+
+	memcpy(&__wr_config, wr_config, sizeof(__wr_config));
+	/* map any shared memory region info */
+
+	if (wr_config->sharedMemoryRegionsConfigAddress != 0)
+		map_page((unsigned long)wr_config->
+			 sharedMemoryRegionsConfigAddress,
+			 (unsigned long)wr_config->
+			 sharedMemoryRegionsConfigAddress, PAGE_KERNEL_X);
+
+	/* devices mapped by the hypervisor xml coqnfiguration */
+
+	return;
+}
+
+unsigned long __init wrhv_find_end_of_memory(void)
+{
+	return wr_config->phys_mem_size;
+}
+
+int wrhv_early_init_dt_scan_memory_ppc(unsigned long node,
+			const char *uname, int depth, void *data)
+{
+	/* instead of using the memory size from
+	 * device tree, we use RamSize from linux.xml
+	 */
+	u64 base, size;
+	/*
+	 * add the first memory region which is
+	 * from 0x00000000 to end of virtual board memory.
+	 */
+	base = 0x00000000ul;
+	size = wrhv_find_end_of_memory();
+	lmb_add(base, size);
+	memstart_addr = min((u64)memstart_addr, base);
+
+	return 0;
+}
+
+void wrhv_power_save(void)
+{
+	local_irq_enable();
+	vbi_idle(1);
+}
+
+void wrhv_restart(char *cmd)
+{
+	int ret;
+	printk(KERN_INFO "WRHV: rebooting \n");
+
+	ret = vbi_vb_reset(VBI_BOARD_ID_GET(), VBI_VB_CORES_ALL,
+		VBI_VBMGMT_RESET_AND_START_CORE0 |
+		VBI_VBMGMT_RESET_DOWNLOAD |
+		VBI_VBMGMT_RESET_CLEAR
+		);
+
+	if (unlikely(ret != 0))
+		printk(KERN_ERR "WRHV: reboot failed. \n");
+
+	while (1);
+}
+
+void __init wrhv_calibrate_decr(void)
+{
+	/* The timebase is updated every 8 bus clocks */
+	ppc_tb_freq = wrhv_cpu_freq / 8;
+	ppc_proc_freq = wrhv_cpu_freq;
+	printk(KERN_DEBUG "WRHV-TIME: wrhv_cpu_freq=%lu  ppc_tb_freq =%lu\n",
+			wrhv_cpu_freq, ppc_tb_freq);
+
+}
+
+void __init wrhv_time_init(void)
+{
+	return;
+}
+
+void __init wrhv_init_irq(void)
+{
+	int i;
+
+	wrhv_irq_chip.typename = "WRHV-PIC";
+	for (i = 0; i < NR_IRQS; i++) {
+		irq_desc[i].status = IRQ_DISABLED | IRQ_LEVEL;
+		irq_desc[i].action = NULL;
+		irq_desc[i].depth = 1;
+		set_irq_chip_and_handler(i, &wrhv_irq_chip, handle_fasteoi_irq);
+	}
+}
+
+#ifdef CONFIG_DEBUG_VIRTUAL_IRQS
+static irqreturn_t wrhv_vbint(int irq, void * dev_id)
+{
+	printk("[DEBUG VIRTUAL IRQS] Handling the DEBUG IRQ %d\n", irq);
+	return IRQ_HANDLED;
+}
+
+static int __init wrhv_late_init_irq(void)
+{
+	int dev_id = 1;
+	int i;
+
+	/* IRQ 0 is unknown IRQ number for Hypervisor */
+	for (i = 1; i < 32; i++) {
+		if(request_irq(i, wrhv_vbint, IRQF_SHARED, "vbint_single", &dev_id))
+			printk("Unable request IRQ for IRQ %d\n", i);
+	}
+
+	return 0;
+}
+subsys_initcall(wrhv_late_init_irq);
+#endif
+
+unsigned int wrhv_vioapic_get_irq(void)
+{
+	unsigned int irq;
+
+	irq = wr_control->irq_pend;
+
+#ifdef CONFIG_DEBUG_VIRTUAL_IRQS
+	/* Maybe this is useless for real external interrupt */
+	wr_status->irq_pend = 0;
+#endif
+
+	if (irq == 0xffff)
+		irq = NO_IRQ_IGNORE;
+	else
+		wr_control->irq_pend = 0xffff;
+
+	return irq;
+}
+
+/* refer to native implementation in arch/powerpc/kernel/irq.c */
+extern inline void check_stack_overflow(void);
+extern inline void handle_one_irq(unsigned int irq);
+
+void wrhv_do_IRQ(struct pt_regs *regs)
+{
+	struct pt_regs *old_regs = set_irq_regs(regs);
+	unsigned int irq;
+
+	trace_irq_entry(regs);
+
+	irq_enter();
+
+	check_stack_overflow();
+
+check_again:
+	irq = ppc_md.get_irq();
+
+	if (irq != NO_IRQ && irq != NO_IRQ_IGNORE) {
+		handle_one_irq(irq);
+		goto check_again;	
+	} else if (irq != NO_IRQ_IGNORE)
+		__get_cpu_var(irq_stat).spurious_irqs++;
+
+	irq_exit();
+	set_irq_regs(old_regs);
+
+#ifdef CONFIG_PPC_ISERIES
+	if (firmware_has_feature(FW_FEATURE_ISERIES) &&
+			get_lppaca()->int_dword.fields.decr_int) {
+		get_lppaca()->int_dword.fields.decr_int = 0;
+		/* Signal a fake decrementer interrupt */
+		timer_interrupt(regs);
+	}
+#endif
+
+	trace_irq_exit(regs);
+}
+
+unsigned int wrhv_irq_of_parse_and_map(struct device_node *dev, int index)
+{
+	int irq;
+
+	irq = vbi_find_irq(dev->full_name, VB_INPUT_INT);
+	if (irq == VBI_INVALID_IRQ)
+		return NO_IRQ;
+
+	return irq;
+}
+
+unsigned int wrhv_get_pvr(void)
+{
+       return 0x80200000;
+}
+
+
+
+
+static void wrhv_set_mode(enum clock_event_mode mode,
+				 struct clock_event_device *dev)
+{
+	return;
+}
+
+static int wrhv_set_next_event(unsigned long evt,
+				      struct clock_event_device *dev)
+{
+	return 0;
+}
+static struct clock_event_device wrhv_clockevent = {
+       .name	   = "wrhv",
+       .shift	  = 32,
+       .irq	    = 0,
+       .mult	   = 1,     /* To be filled in */
+       .set_mode       = wrhv_set_mode,
+       .set_next_event = wrhv_set_next_event,
+       .features       = CLOCK_EVT_FEAT_ONESHOT,
+};
+
+void wrhv_hw_timer_interrupt(struct pt_regs * regs)
+{
+	struct pt_regs *old_regs;
+
+	trace_timer_interrupt_entry(regs);
+
+	__get_cpu_var(irq_stat).timer_irqs++;
+
+#ifdef CONFIG_PPC32
+	if (test_perf_event_pending()) {
+		clear_perf_event_pending();
+		perf_event_do_pending();
+	}
+	if (atomic_read(&ppc_n_lost_interrupts) != 0)
+		do_IRQ(regs);
+#endif
+
+	old_regs = set_irq_regs(regs);
+	irq_enter();
+
+	calculate_steal_time();
+
+	wrhv_timer_interrupt(0, NULL);
+
+	irq_exit();
+	set_irq_regs(old_regs);
+
+	trace_timer_interrupt_exit(regs);
+}
+
+void __init wrhv_clocksource_init(void)
+{
+	return;
+}
+
+void __init wrhv_time_init_cont(void)
+{
+	wrhv_clockevent.cpumask = get_cpu_mask(0);
+	clockevents_register_device(&wrhv_clockevent);
+}
+
+
+/* arch/powerpc/mm/fault.c */
+void wrhv_vmmu_restore(void)
+{
+	/*
+	 * called by the end of page fault handling to reinstall the vmmu
+	 */
+	wr_control->vmmu0 = wr_status->vmmu0;
+	wr_control->vmmu1 = wr_status->vmmu1;
+	return;
+}
+
+/* arch/powerpc/mm/fsl_booke_mmu.c */
+void __init wrhv_MMU_init_hw(void)
+{
+	return;
+}
+
+unsigned long __init wrhv_mmu_mapin_ram(unsigned long top)
+{
+       return 0;
+}
+
+/* arch/powerpc/mm/init_32.c */
+void wrhv_MMU_setup(void)
+{
+	__map_without_bats = 1;
+
+#ifdef CONFIG_DEBUG_PAGEALLOC
+	__map_without_bats = 1;
+	__map_without_ltlbs = 1;
+#endif
+}
+
+void __init wrhv_MMU_init(void)
+{
+	if (ppc_md.progress)
+		ppc_md.progress("MMU:enter", 0x111);
+
+	/* parse args from command line */
+	wrhv_MMU_setup();
+
+	if (lmb.memory.cnt > 1) {
+		lmb.memory.cnt = 1;
+		lmb_analyze();
+		printk(KERN_WARNING "Only using first contiguous memory region");
+	}
+
+	total_lowmem = total_memory = lmb_end_of_DRAM() - memstart_addr;
+	lowmem_end_addr = memstart_addr + total_lowmem;
+
+	if (total_lowmem > __max_low_memory) {
+		total_lowmem = __max_low_memory;
+		lowmem_end_addr = memstart_addr + total_lowmem;
+#ifndef CONFIG_HIGHMEM
+		total_memory = total_lowmem;
+		lmb_enforce_memory_limit(lowmem_end_addr);
+		lmb_analyze();
+#endif /* CONFIG_HIGHMEM */
+	}
+
+	/* Initialize the MMU hardware */
+	if (ppc_md.progress)
+		ppc_md.progress("MMU:hw init", 0x300);
+	MMU_init_hw();
+
+	/* Map in all of RAM starting at KERNELBASE */
+	if (ppc_md.progress)
+		ppc_md.progress("MMU:mapin", 0x301);
+	mapin_ram();
+
+	/* Initialize early top-down ioremap allocator */
+	ioremap_bot = IOREMAP_TOP;
+
+	/* Map in I/O resources */
+	if (ppc_md.progress)
+		ppc_md.progress("MMU:setio", 0x302);
+
+	if (ppc_md.progress)
+		ppc_md.progress("MMU:exit", 0x211);
+
+	/* From now on, btext is no longer BAT mapped if it was at all */
+#ifdef CONFIG_BOOTX_TEXT
+	btext_unmap();
+#endif
+
+#ifndef CONFIG_PPC85xx_VT_MODE
+	/*
+	 * we enable the mmu here without having to do this from the caller
+	 * (which is in assembly world)
+	 */
+	vb_context_mmu_on(0, swapper_pg_dir, PAGE_SIZE, 0);
+#endif
+}
+
+/* arch/powerpc/mm/mem.c */
+extern void __flush_dcache_icache_phys(unsigned long physaddr);
+void wrhv_flush_dcache_page(struct page *page)
+{
+	if (cpu_has_feature(CPU_FTR_COHERENT_ICACHE))
+		return;
+	/* avoid an atomic op if possible */
+	if (test_bit(PG_arch_1, &page->flags))
+		clear_bit(PG_arch_1, &page->flags);
+	__flush_dcache_icache_phys(page_to_pfn(page) << PAGE_SHIFT);
+}
+
+void set_context(unsigned long contextid, pgd_t *pgd) 
+	__attribute__((weak, alias("wrhv_set_context")));
+
+/* arch/powerpc/mm/mmu_context_32.c */
+void wrhv_set_context(unsigned long contextId, pgd_t * pgd)
+{
+
+	pgd_t * kpdStart, *kpdEnd, *updStart;
+	/* we attach (copy) kernel page mapping to the user page table
+	 * Note, we only copy the L1 entrys to user L1 pageTable,
+	 * then letting L1 share the same L2 page table
+	 */
+
+	kpdStart = pgd_offset_k(KERNELBASE);
+	kpdEnd =   pgd_offset_k(0xffffffff);
+
+	updStart = pgd + pgd_index(KERNELBASE);
+
+	memcpy(updStart, kpdStart, (kpdEnd - kpdStart + 1) * sizeof (pgd_t));
+
+	/* in linux context, page table entry is not set up yet */
+	vb_context_mmu_on(contextId, pgd, PAGE_SIZE, 0);
+}
+
+/* arch/powerpc/mm/pgtable_32.c */
+int wrhv_map_page(unsigned long va, phys_addr_t pa, int flags)
+{
+	pmd_t *pd;
+	pte_t *pg;
+	int err = -ENOMEM;
+
+	/* Use upper 10 bits of VA to index the first level map */
+	pd = pmd_offset(pud_offset(pgd_offset_k(va), va), va);
+	/* Use middle 10 bits of VA to index the second-level map */
+	pg = pte_alloc_kernel(pd, va);
+	if (pg != 0) {
+		err = 0;
+		/* The PTE should never be already set nor present in the
+		 * hash table
+		 */
+		BUG_ON(pte_val(*pg) & (_PAGE_PRESENT | _PAGE_HASHPTE));
+		set_pte_at(&init_mm, va, pg, pfn_pte(pa >> PAGE_SHIFT,
+						     __pgprot(flags)));
+	}
+
+#ifndef CONFIG_PPC85xx_VT_MODE
+	if (mem_init_done)
+		flush_tlb_page(NULL, va);
+#endif
+	return err;
+}
+
+void __iomem *
+wrhv___ioremap(phys_addr_t addr, unsigned long size, unsigned long flags)
+{
+	unsigned long v, i;
+	phys_addr_t p;
+	int err;
+
+	/* writeable implies dirty for kernel addresses */
+	if (flags & _PAGE_RW)
+		flags |= _PAGE_DIRTY | _PAGE_HWWRITE;
+
+	/* we don't want to let _PAGE_USER and _PAGE_EXEC leak out */
+	flags &= ~(_PAGE_USER | _PAGE_EXEC);
+
+	/* Make sure we have the base flags */
+	if ((flags & _PAGE_PRESENT) == 0)
+		flags |= PAGE_KERNEL;
+
+	/* Non-cacheable page cannot be coherent */
+	if (flags & _PAGE_NO_CACHE)
+		flags &= ~_PAGE_COHERENT;
+
+	/*
+	 * Choose an address to map it to.
+	 * Once the vmalloc system is running, we use it.
+	 * Before then, we use space going down from ioremap_base
+	 * (ioremap_bot records where we're up to).
+	 */
+	p = addr & PAGE_MASK;
+	size = PAGE_ALIGN(addr + size) - p;
+
+	/*
+	 * If the address lies within the first 16 MB, assume it's in ISA
+	 * memory space
+	 */
+	if (p < 16*1024*1024)
+		p += _ISA_MEM_BASE;
+
+#ifndef CONFIG_CRASH_DUMP
+	/*
+	 * Don't allow anybody to remap normal RAM that we're using.
+	 * mem_init() sets high_memory so only do the check after that.
+	 */
+	if (mem_init_done && (p < virt_to_phys(high_memory))) {
+		printk("__ioremap(): phys addr 0x%llx is RAM lr %p\n",
+		       (unsigned long long)p, __builtin_return_address(0));
+		return NULL;
+	}
+#endif
+
+	if (size == 0)
+		return NULL;
+
+	/*
+	 * Is it already mapped?  Perhaps overlapped by a previous
+	 * BAT mapping.  If the whole area is mapped then we're done,
+	 * otherwise remap it since we want to keep the virt addrs for
+	 * each request contiguous.
+	 *
+	 * We make the assumption here that if the bottom and top
+	 * of the range we want are mapped then it's mapped to the
+	 * same virt address (and this is contiguous).
+	 *  -- Cort
+	 */
+	if ((v = p_mapped_by_bats(p)) /*&& p_mapped_by_bats(p+size-1)*/ )
+		goto out;
+
+	if ((v = p_mapped_by_tlbcam(p)))
+		goto out;
+
+	if (mem_init_done) {
+		struct vm_struct *area;
+		area = get_vm_area(size, VM_IOREMAP);
+		if (area == 0)
+			return NULL;
+		v = (unsigned long) area->addr;
+	} else {
+		v = (ioremap_bot -= size);
+	}
+
+	/*
+	 * Should check if it is a candidate for a BAT mapping
+	 */
+
+	err = 0;
+	for (i = 0; i < size && err == 0; i += PAGE_SIZE)
+		err = map_page(v+i, p+i, flags);
+	if (err) {
+		if (mem_init_done)
+			vunmap((void *)v);
+		return NULL;
+	}
+
+/* Just E500 Guest OS need copy kernel PTEs in ioremap.
+ * And, don't support 36 bit physical address now.
+ */
+#if !defined(CONFIG_PPC85xx_VT_MODE) && !defined(CONFIG_PHYS_64BIT)
+	{
+		pgd_t *kpd_start, *kpd_end, *upd_start, *pgd;
+		if (mem_init_done && (current->mm != NULL) && (current->mm != &init_mm)) {
+			pgd = current->mm->pgd;
+
+			/* we attach (copy) kernel page mapping to the user page table
+			 * Note, we only copy the L1 entrys to user L1 pageTable,
+			 * then letting L1 share the same L2 page table.
+			 */
+			kpd_start = pgd_offset_k(KERNELBASE);
+			kpd_end =   pgd_offset_k(0xffffffff);
+
+			upd_start = pgd + pgd_index(KERNELBASE);
+			memcpy(upd_start, kpd_start, (kpd_end - kpd_start + 1) * sizeof (pgd_t));
+		}
+	}
+#endif
+out:
+	return (void __iomem *) (v + ((unsigned long)addr & ~PAGE_MASK));
+}
+
+/* From arch/powerpc/include/asm/pgtable.h */
+static inline void wrhv__set_pte_at(struct mm_struct *mm, unsigned long addr,
+				pte_t *ptep, pte_t pte, int percpu)
+{
+#if defined(CONFIG_PPC_STD_MMU_32) && defined(CONFIG_SMP) && !defined(CONFIG_PTE_64BIT)
+	/* First case is 32-bit Hash MMU in SMP mode with 32-bit PTEs. We use the
+	 * helper pte_update() which does an atomic update. We need to do that
+	 * because a concurrent invalidation can clear _PAGE_HASHPTE. If it's a
+	 * per-CPU PTE such as a kmap_atomic, we do a simple update preserving
+	 * the hash bits instead (ie, same as the non-SMP case)
+	 */
+	if (percpu)
+		*ptep = __pte((pte_val(*ptep) & _PAGE_HASHPTE)
+			      | (pte_val(pte) & ~_PAGE_HASHPTE));
+	else
+		pte_update(ptep, ~_PAGE_HASHPTE, pte_val(pte));
+
+#elif defined(CONFIG_PPC32) && defined(CONFIG_PTE_64BIT)
+	/* Second case is 32-bit with 64-bit PTE.  In this case, we
+	 * can just store as long as we do the two halves in the right order
+	 * with a barrier in between. This is possible because we take care,
+	 * in the hash code, to pre-invalidate if the PTE was already hashed,
+	 * which synchronizes us with any concurrent invalidation.
+	 * In the percpu case, we also fallback to the simple update preserving
+	 * the hash bits
+	 */
+	if (percpu) {
+		*ptep = __pte((pte_val(*ptep) & _PAGE_HASHPTE)
+			      | (pte_val(pte) & ~_PAGE_HASHPTE));
+		return;
+	}
+#if _PAGE_HASHPTE != 0
+	if (pte_val(*ptep) & _PAGE_HASHPTE)
+		flush_hash_entry(mm, ptep, addr);
+#endif
+	__asm__ __volatile__("\
+		stw%U0%X0 %2,%0\n\
+		eieio\n\
+		stw%U0%X0 %L2,%1"
+	: "=m" (*ptep), "=m" (*((unsigned char *)ptep+4))
+	: "r" (pte) : "memory");
+
+#elif defined(CONFIG_PPC_STD_MMU_32)
+	/* Third case is 32-bit hash table in UP mode, we need to preserve
+	 * the _PAGE_HASHPTE bit since we may not have invalidated the previous
+	 * translation in the hash yet (done in a subsequent flush_tlb_xxx())
+	 * and see we need to keep track that this PTE needs invalidating
+	 */
+	*ptep = __pte((pte_val(*ptep) & _PAGE_HASHPTE)
+		      | (pte_val(pte) & ~_PAGE_HASHPTE));
+
+#else
+	/* Anything else just stores the PTE normally. That covers all 64-bit
+	 * cases, and 32-bit non-hash with 32-bit PTEs.
+	 */
+	*ptep = pte;
+
+#if defined(CONFIG_WRHV) && !defined(CONFIG_PPC85xx_VT_MODE)
+	/* linux does not use valid bit, hypervisor does, in word0 */
+	*(u_int *)ptep |= (u_int) VMMU_PTE_VALID_MASK;
+#endif /* CONFIG_WRHV */
+
+#endif
+}
+
+static void wrhv_handle_debug(struct pt_regs *regs, unsigned long debug_status)
+{
+	int changed = 0;
+	/*
+	 * Determine the cause of the debug event, clear the
+	 * event flags and send a trap to the handler. Torez
+	 */
+	if (debug_status & (DBSR_DAC1R | DBSR_DAC1W)) {
+		dbcr_dac(current) &= ~(DBCR_DAC1R | DBCR_DAC1W);
+#ifdef CONFIG_PPC_ADV_DEBUG_DAC_RANGE
+		current->thread.dbcr2 &= ~DBCR2_DAC12MODE;
+#endif
+		do_send_trap(regs, mfspr(SPRN_DAC1), debug_status, TRAP_HWBKPT,
+			     5);
+		changed |= 0x01;
+	}  else if (debug_status & (DBSR_DAC2R | DBSR_DAC2W)) {
+		dbcr_dac(current) &= ~(DBCR_DAC2R | DBCR_DAC2W);
+		do_send_trap(regs, mfspr(SPRN_DAC2), debug_status, TRAP_HWBKPT,
+			     6);
+		changed |= 0x01;
+	}  else if (debug_status & DBSR_IAC1) {
+		current->thread.dbcr0 &= ~DBCR0_IAC1;
+		dbcr_iac_range(current) &= ~DBCR_IAC12MODE;
+		do_send_trap(regs, mfspr(SPRN_IAC1), debug_status, TRAP_HWBKPT,
+			     1);
+		changed |= 0x01;
+	}  else if (debug_status & DBSR_IAC2) {
+		current->thread.dbcr0 &= ~DBCR0_IAC2;
+		do_send_trap(regs, mfspr(SPRN_IAC2), debug_status, TRAP_HWBKPT,
+			     2);
+		changed |= 0x01;
+	}  else if (debug_status & DBSR_IAC3) {
+		current->thread.dbcr0 &= ~DBCR0_IAC3;
+		dbcr_iac_range(current) &= ~DBCR_IAC34MODE;
+		do_send_trap(regs, mfspr(SPRN_IAC3), debug_status, TRAP_HWBKPT,
+			     3);
+		changed |= 0x01;
+	}  else if (debug_status & DBSR_IAC4) {
+		current->thread.dbcr0 &= ~DBCR0_IAC4;
+		do_send_trap(regs, mfspr(SPRN_IAC4), debug_status, TRAP_HWBKPT,
+			     4);
+		changed |= 0x01;
+	}
+	/*
+	 * At the point this routine was called, the MSR(DE) was turned off.
+	 * Check all other debug flags and see if that bit needs to be turned
+	 * back on or not.
+	 */
+	if (DBCR_ACTIVE_EVENTS(current->thread.dbcr0, current->thread.dbcr1))
+		regs->msr |= MSR_DE;
+	else
+		/* Make sure the IDM flag is off */
+		current->thread.dbcr0 &= ~DBCR0_IDM;
+
+	if (changed & 0x01)
+		mtspr(SPRN_DBCR0, current->thread.dbcr0);
+}
+
+/* arch/powerpc/kernel/traps.c */
+void __kprobes wrhv_DebugException(struct pt_regs *regs, unsigned long debug_status)
+{
+	debug_status = wr_control->vb_control_regs.dbsr;
+	wr_control->vb_control_regs.emsr &= ~MSR_DE;
+
+	current->thread.dbsr = debug_status;
+
+	/* Hack alert: On BookE, Branch Taken stops on the branch itself, while
+	 * on server, it stops on the target of the branch. In order to simulate
+	 * the server behaviour, we thus restart right away with a single step
+	 * instead of stopping here when hitting a BT
+	 */
+	if (debug_status & DBSR_BT) {
+		regs->msr &= ~MSR_DE;
+
+		/* Disable BT */
+		mtspr(SPRN_DBCR0, mfspr(SPRN_DBCR0) & ~DBCR0_BT);
+		/* Clear the BT event */
+		mtspr(SPRN_DBSR, DBSR_BT);
+
+		/* Do the single step trick only when coming from userspace */
+		if (user_mode(regs)) {
+			current->thread.dbcr0 &= ~DBCR0_BT;
+			current->thread.dbcr0 |= DBCR0_IDM | DBCR0_IC;
+			regs->msr |= MSR_DE;
+			return;
+		}
+
+		if (notify_die(DIE_SSTEP, "block_step", regs, 5,
+			       5, SIGTRAP) == NOTIFY_STOP) {
+			return;
+		}
+		if (debugger_sstep(regs))
+			return;
+	} else if (debug_status & DBSR_IC) { 	/* Instruction complete */
+		regs->msr &= ~MSR_DE;
+
+		if (notify_die(DIE_SSTEP, "single_step", regs, 5,
+			       5, SIGTRAP) == NOTIFY_STOP) {
+			return;
+		}
+
+		if (debugger_sstep(regs))
+			return;
+
+		if (user_mode(regs)) {
+			current->thread.dbcr0 &= ~DBCR0_IC;
+#ifdef CONFIG_PPC_ADV_DEBUG_REGS
+			if (DBCR_ACTIVE_EVENTS(current->thread.dbcr0,
+					       current->thread.dbcr1))
+				regs->msr |= MSR_DE;
+			else
+				/* Make sure the IDM bit is off */
+				current->thread.dbcr0 &= ~DBCR0_IDM;
+#endif
+		}
+
+		_exception(SIGTRAP, regs, TRAP_TRACE, regs->nip);
+	} else
+		wrhv_handle_debug(regs, debug_status);
+}
+
+/* arch/powerpc/kernel/kgdb.c */
+int wrhv_kgdb_arch_handle_exception(int vector, int signo, int err_code,
+			       char *remcom_in_buffer, char *remcom_out_buffer,
+			       struct pt_regs *linux_regs)
+{
+	char *ptr = &remcom_in_buffer[1];
+	unsigned long addr;
+
+	switch (remcom_in_buffer[0]) {
+		/*
+		 * sAA..AA   Step one instruction from AA..AA
+		 * This will return an error to gdb ..
+		 */
+	case 's':
+	case 'c':
+		/* handle the optional parameter */
+		if (kgdb_hex2long(&ptr, &addr))
+			linux_regs->nip = addr;
+
+		atomic_set(&kgdb_cpu_doing_single_step, -1);
+		/* set the trace bit if we're stepping */
+		if (remcom_in_buffer[0] == 's') {
+#ifdef CONFIG_PPC_ADV_DEBUG_REGS
+			wr_control->vb_control_regs.dbcr0 |= (DBCR0_IC | DBCR0_IDM);
+			wr_control->vb_control_regs.emsr |= MSR_DE;
+			linux_regs->msr |= MSR_DE;
+#else
+			linux_regs->msr |= MSR_SE;
+#endif
+			kgdb_single_step = 1;
+			atomic_set(&kgdb_cpu_doing_single_step,
+				   raw_smp_processor_id());
+		}
+		return 0;
+	}
+
+	return -1;
+}
+
+int wrhv_ppc_cpu_freq(void)
+{
+	return wrhv_cpu_freq;
+}
+
+void wrhv_init(void)
+{
+	/* initialize wr_config so that we can access
+	 * vbi configuration. The vbi configuration space
+	 * is defined in Hypervisor linux.xml
+	 */
+	wr_config = (struct vb_config *)0xF0000000;
+	wr_control = wr_config->vb_control;
+	wr_status = wr_config->vb_status;
+
+	pv_info.name = "wrhv";
+	pv_info.paravirt_enabled = 1;
+
+	pv_time_ops.time_init_cont = wrhv_time_init_cont;
+	pv_time_ops.timer_interrupt = wrhv_hw_timer_interrupt;
+	pv_time_ops.clocksource_init = wrhv_clocksource_init;
+
+	pv_irq_ops.do_IRQ = wrhv_do_IRQ;
+	pv_irq_ops.irq_of_parse_and_map =
+			wrhv_irq_of_parse_and_map;
+
+	pv_cpu_ops.get_pvr = wrhv_get_pvr;
+	pv_cpu_ops.DebugException = wrhv_DebugException;
+	pv_cpu_ops.kgdb_arch_handle_exception =
+		wrhv_kgdb_arch_handle_exception;
+	pv_cpu_ops.ppc_proc_freq =
+		wrhv_ppc_cpu_freq;
+
+#ifndef CONFIG_PPC85xx_VT_MODE
+	pv_mmu_ops.vmmu_restore = wrhv_vmmu_restore;
+#endif
+	pv_mmu_ops.MMU_init_hw = wrhv_MMU_init_hw;
+	pv_mmu_ops.mmu_mapin_ram = wrhv_mmu_mapin_ram;
+	pv_mmu_ops.MMU_setup = wrhv_MMU_setup;
+	pv_mmu_ops.MMU_init = wrhv_MMU_init;
+	pv_mmu_ops.flush_dcache_page = wrhv_flush_dcache_page;
+	pv_mmu_ops.map_page = wrhv_map_page;
+	pv_mmu_ops.early_init_dt_scan_memory_ppc =
+		wrhv_early_init_dt_scan_memory_ppc;
+	pv_mmu_ops.__ioremap = wrhv___ioremap;
+	pv_mmu_ops.__set_pte_at = wrhv__set_pte_at;
+
+}
diff --git a/arch/powerpc/kernel/wrhv_entry_32.S b/arch/powerpc/kernel/wrhv_entry_32.S
new file mode 100644
index 0000000..7a00e7d
--- /dev/null
+++ b/arch/powerpc/kernel/wrhv_entry_32.S
@@ -0,0 +1,500 @@
+/*
+ *  PowerPC version
+ *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)
+ *  Rewritten by Cort Dougan (cort@fsmlabs.com) for PReP
+ *    Copyright (C) 1996 Cort Dougan <cort@fsmlabs.com>
+ *  Adapted for Power Macintosh by Paul Mackerras.
+ *  Low-level exception handlers and MMU support
+ *  rewritten by Paul Mackerras.
+ *    Copyright (C) 1996 Paul Mackerras.
+ *  MPC8xx modifications Copyright (C) 1997 Dan Malek (dmalek@jlc.net).
+ *  
+ *  Fork from entry_32.S for Hypervisor/Guest, Copyright (C) 2009
+ *  Wind River Systems, Inc.
+ *
+ *  This file contains the system call entry code, context switch
+ *  code, and exception/interrupt return code for PowerPC.
+ *
+ *  This program is free software; you can redistribute it and/or
+ *  modify it under the terms of the GNU General Public License
+ *  as published by the Free Software Foundation; either version
+ *  2 of the License, or (at your option) any later version.
+ *
+ */
+
+#include <linux/errno.h>
+#include <linux/sys.h>
+#include <linux/threads.h>
+#include <asm/reg.h>
+#include <asm/page.h>
+#include <asm/mmu.h>
+#include <asm/cputable.h>
+#include <asm/thread_info.h>
+#include <asm/ppc_asm.h>
+#include <asm/asm-offsets.h>
+#include <asm/unistd.h>
+#include <asm/ftrace.h>
+#ifdef CONFIG_WRHV
+#include <vbi/interface.h>
+#include <asm/arch_vbi.h>
+#include <vbi/syscalls.h>
+#endif /* CONFIG_WRHV */
+
+#undef SHOW_SYSCALLS
+#undef SHOW_SYSCALLS_TASK
+#ifdef	CONFIG_WRHV
+#undef VMMU  /* just for debugging */
+#endif /* CONFIG_WRHV */
+
+#ifdef	CONFIG_WRHV
+	.data
+	.globl	wrhv_sprg3
+wrhv_sprg3:
+	.long	0
+	.globl	wrhv_supervisor
+wrhv_supervisor:
+	.long	1
+#ifdef CONFIG_SMP
+	.globl wrhv_pir
+wrhv_pir:
+	.long	0
+#endif
+	.text
+#endif	/* CONFIG_WRHV */
+
+/*
+ * MSR_KERNEL is > 0x10000 on 4xx/Book-E since it include MSR_CE.
+ */
+#if MSR_KERNEL >= 0x10000
+#define LOAD_MSR_KERNEL(r, x)	lis r,(x)@h; ori r,r,(x)@l
+#else
+#define LOAD_MSR_KERNEL(r, x)	li r,(x)
+#endif
+
+	.globl	paravirt_transfer_to_handler
+paravirt_transfer_to_handler:
+	stw	r2,GPR2(r11)
+	stw	r12,_NIP(r11)
+	stw	r9,_MSR(r11)
+	andi.	r2,r9,MSR_PR
+	mfctr	r12
+	mfspr	r2,SPRN_XER
+	stw	r12,_CTR(r11)
+	stw	r2,_XER(r11)
+	WRHV_MFSPRG3(r12)
+	addi	r2,r12,-THREAD
+	tovirt(r2,r2)			/* set r2 to current */
+	beq	2f			/* if from user, fix up THREAD.regs */
+	addi	r11,r1,STACK_FRAME_OVERHEAD
+	stw	r11,PT_REGS(r12)
+#if defined(CONFIG_40x) || defined(CONFIG_BOOKE) && !defined(CONFIG_WRHV)
+	/* Check to see if the dbcr0 register is set up to debug.  Use the
+	   internal debug mode bit to do this. */
+	lwz	r12,THREAD_DBCR0(r12)
+	andis.	r12,r12,DBCR0_IDM@h
+	beq+	3f
+	/* From user and task is ptraced - load up global dbcr0 */
+	li	r12,-1			/* clear all pending debug events */
+	mtspr	SPRN_DBSR,r12
+	lis	r11,global_dbcr0@ha
+	tophys(r11,r11)
+	addi	r11,r11,global_dbcr0@l
+#ifdef CONFIG_SMP
+	rlwinm	r9,r1,0,0,(31-THREAD_SHIFT)
+	lwz	r9,TI_CPU(r9)
+	slwi	r9,r9,3
+	add	r11,r11,r9
+#endif
+	lwz	r12,0(r11)
+	mtspr	SPRN_DBCR0,r12
+	lwz	r12,4(r11)
+	addi	r12,r12,-1
+	stw	r12,4(r11)
+#endif
+	b	3f
+
+2:	/* if from kernel, check interrupted DOZE/NAP mode and
+         * check for stack overflow
+         */
+	lwz	r9,KSP_LIMIT(r12)
+	cmplw	r1,r9			/* if r1 <= ksp_limit */
+	ble-	paravirt_stack_ovf		/* then the kernel stack overflowed */
+5:
+#if defined(CONFIG_6xx) || defined(CONFIG_E500)
+	rlwinm	r9,r1,0,0,31-THREAD_SHIFT
+	tophys(r9,r9)			/* check local flags */
+	lwz	r12,TI_LOCAL_FLAGS(r9)
+	mtcrf	0x01,r12
+	bt-	31-TLF_NAPPING,4f
+	bt-	31-TLF_SLEEPING,7f
+#endif /* CONFIG_6xx || CONFIG_E500 */
+	.globl paravirt_transfer_to_handler_cont
+paravirt_transfer_to_handler_cont:
+3:
+	mflr	r9
+	lwz	r11,0(r9)		/* virtual address of handler */
+	lwz	r9,4(r9)		/* where to go when done */
+	mtlr	r9
+	lis	r9,wr_control@ha
+	lwz	r9,wr_control@l(r9)
+	stw	r11,VB_CONTROL_SRR0(r9)
+	mfcr	r11
+	stw	r11,VB_CONTROL_CR(r9)
+	stw	r0,VB_CONTROL_R0(r9)
+
+	lis	r12,wr_status@ha
+	lwz	r12,wr_status@l(r12)
+
+	lwz	r11,VB_STATUS_OLD_INT_DISABLE(r12)
+	stw	r11,VB_CONTROL_NEW_INT_DISABLE(r9)
+
+/*
+	lwz	r11,VB_STATUS_CR(r12)
+	stw	r11,VB_CONTROL_CR(r9)
+*/
+
+#ifdef VMMU
+        /* restore vmmu from wr_status to wr_control */
+
+	lwz	r11,VB_STATUS_VMMU0(r12)
+	stw	r11,VB_CONTROL_VMMU0(r9)
+
+	lwz	r11,VB_STATUS_VMMU1(r12)
+	stw	r11,VB_CONTROL_VMMU1(r9)
+
+	lwz	r11,VB_STATUS_EMSR(r12)
+	stw	r11,VB_CONTROL_EMSR(r9)
+
+	stw	r1,VB_CONTROL_SP(r9)
+	stw	r2,VB_CONTROL_R2(r9)
+	stw	r3,VB_CONTROL_R3(r9)
+	stw	r4,VB_CONTROL_R4(r9)
+	stw	r5,VB_CONTROL_R5(r9)
+	stw	r6,VB_CONTROL_R6(r9)
+	stw	r7,VB_CONTROL_R7(r9)
+	stw	r8,VB_CONTROL_R8(r9)
+	stw	r10,VB_CONTROL_R10(r9)
+	mflr	r11
+#endif
+
+	WRHV_LOAD_MSR(r10,r9,r11)
+
+#ifdef VMMU
+        /* re-enable vmmu */
+
+	lis	r0,VBI_SYS_ctx_load_vmmu@h
+	ori	r0,r0,VBI_SYS_ctx_load_vmmu@l
+	sc
+
+#else
+	lis	r0,VBI_SYS_ctx_load@h
+	ori	r0,r0,VBI_SYS_ctx_load@l
+	sc
+#endif /* VMMU */
+
+#if defined (CONFIG_6xx) || defined(CONFIG_E500)
+4:	rlwinm	r12,r12,0,~_TLF_NAPPING
+	stw	r12,TI_LOCAL_FLAGS(r9)
+	b	power_save_ppc32_restore
+
+7:	rlwinm	r12,r12,0,~_TLF_SLEEPING
+	stw	r12,TI_LOCAL_FLAGS(r9)
+	lwz	r9,_MSR(r11)		/* if sleeping, clear MSR.EE */
+	rlwinm	r9,r9,0,~MSR_EE
+	lwz	r12,_LINK(r11)		/* and return to address in LR */
+	b	fast_exception_return
+#endif
+
+/*
+ * On kernel stack overflow, load up an initial stack pointer
+ * and call StackOverflow(regs), which should not return.
+ */
+paravirt_stack_ovf:
+	/* sometimes we use a statically-allocated stack, which is OK. */
+	lis	r12,_end@h
+	ori	r12,r12,_end@l
+	cmplw	r1,r12
+	ble	5b			/* r1 <= &_end is OK */
+	SAVE_NVGPRS(r11)
+	addi	r3,r1,STACK_FRAME_OVERHEAD
+	lis	r1,init_thread_union@ha
+	addi	r1,r1,init_thread_union@l
+	addi	r1,r1,THREAD_SIZE-STACK_FRAME_OVERHEAD
+	lis	r9,StackOverflow@ha
+	addi	r9,r9,StackOverflow@l
+	LOAD_MSR_KERNEL(r10,MSR_KERNEL)
+	FIX_SRR1(r10,r12)
+	mtspr	SPRN_SRR0,r9
+	mtspr	SPRN_SRR1,r10
+	SYNC
+	RFI
+
+
+	.globl	paravirt_ret_from_syscall
+paravirt_ret_from_syscall:
+#ifdef SHOW_SYSCALLS
+	bl	do_show_syscall_exit
+#endif
+	mr	r6,r3
+	rlwinm	r12,r1,0,0,(31-THREAD_SHIFT)	/* current_thread_info() */
+	/* disable interrupts so current_thread_info()->flags can't change */
+	WRHV_INT_LOCK(r10,r9)
+	lwz	r9,TI_FLAGS(r12)
+	li	r8,-_LAST_ERRNO
+	andi.	r0,r9,(_TIF_SYSCALL_T_OR_A|_TIF_SINGLESTEP|_TIF_USER_WORK_MASK|_TIF_PERSYSCALL_MASK)
+	bne-	syscall_exit_work
+	cmplw	0,r3,r8
+	blt+	paravirt_syscall_exit_cont
+	lwz	r11,_CCR(r1)			/* Load CR */
+	neg	r3,r3
+	oris	r11,r11,0x1000	/* Set SO bit in CR */
+	stw	r11,_CCR(r1)
+paravirt_syscall_exit_cont:
+#if defined(CONFIG_4xx) || defined(CONFIG_BOOKE) && !(CONFIG_WRHV)
+	/* If the process has its own DBCR0 value, load it up.  The internal
+	   debug mode bit tells us that dbcr0 should be loaded. */
+	lwz	r0,THREAD+THREAD_DBCR0(r2)
+	andis.	r10,r0,DBCR0_IDM@h
+	bnel-	load_dbcr0
+#endif
+#ifdef CONFIG_44x
+	lis	r4,icache_44x_need_flush@ha
+	lwz	r5,icache_44x_need_flush@l(r4)
+	cmplwi	cr0,r5,0
+	bne-	2f
+1:
+#endif /* CONFIG_44x */
+BEGIN_FTR_SECTION
+	lwarx	r7,0,r1
+END_FTR_SECTION_IFSET(CPU_FTR_NEED_PAIRED_STWCX)
+	stwcx.	r0,0,r1			/* to clear the reservation */
+	lwz	r4,_LINK(r1)
+	lwz	r5,_CCR(r1)
+	mtlr	r4
+	lis	r4,wr_control@ha
+	lwz	r4,wr_control@l(r4)
+	stw	r5,VB_CONTROL_CR(r4)
+	lwz	r5,GPR0(r1)
+	stw	r5,VB_CONTROL_R0(r4)
+	lwz	r5,_NIP(r1)
+	stw	r5,VB_CONTROL_SRR0(r4)
+
+	lis	r5,wr_status@ha
+	lwz	r5,wr_status@l(r5)
+	lwz	r5,VB_STATUS_OLD_INT_DISABLE(r5)
+	stw	r5,VB_CONTROL_NEW_INT_DISABLE(r4)
+ 
+	lwz	r5,_MSR(r1)
+	WRHV_LOAD_MSR(r5,r7,r8)
+	lwz	r2,GPR2(r1)
+	lwz	r1,GPR1(r1)
+	lis	r0,VBI_SYS_ctx_load@h
+	sc
+
+66:	li	r3,-ENOSYS
+	b	ret_from_syscall
+
+	.globl	paravirt_syscall_exit_work
+paravirt_syscall_exit_work:
+	andi.	r0,r9,_TIF_RESTOREALL
+	beq+	0f
+	REST_NVGPRS(r1)
+	b	2f
+0:	cmplw	0,r3,r8
+	blt+	1f
+	andi.	r0,r9,_TIF_NOERROR
+	bne-	1f
+	lwz	r11,_CCR(r1)			/* Load CR */
+	neg	r3,r3
+	oris	r11,r11,0x1000	/* Set SO bit in CR */
+	stw	r11,_CCR(r1)
+
+1:	stw	r6,RESULT(r1)	/* Save result */
+	stw	r3,GPR3(r1)	/* Update return value */
+2:	andi.	r0,r9,(_TIF_PERSYSCALL_MASK)
+	beq	4f
+
+	/* Clear per-syscall TIF flags if any are set.  */
+
+	li	r11,_TIF_PERSYSCALL_MASK
+	addi	r12,r12,TI_FLAGS
+3:	lwarx	r8,0,r12
+	andc	r8,r8,r11
+#ifdef CONFIG_IBM405_ERR77
+	dcbt	0,r12
+#endif
+	stwcx.	r8,0,r12
+	bne-	3b
+	subi	r12,r12,TI_FLAGS
+	
+4:	/* Anything which requires enabling interrupts? */
+	andi.	r0,r9,(_TIF_SYSCALL_T_OR_A|_TIF_SINGLESTEP)
+	beq	ret_from_except
+
+	/* Re-enable interrupts */
+	WRHV_INT_UNLOCK(r10,r4)
+
+	/* Save NVGPRS if they're not saved already */
+	lwz	r4,_TRAP(r1)
+	andi.	r4,r4,1
+	beq	5f
+	SAVE_NVGPRS(r1)
+	li	r4,0xc00
+	stw	r4,_TRAP(r1)
+5:
+	addi	r3,r1,STACK_FRAME_OVERHEAD
+	bl	do_syscall_trace_leave
+	b	ret_from_except_full
+
+
+/*
+ * This routine switches between two different tasks.  The process
+ * state of one is saved on its kernel stack.  Then the state
+ * of the other is restored from its kernel stack.  The memory
+ * management hardware is updated to the second process's state.
+ * Finally, we can return to the second process.
+ * On entry, r3 points to the THREAD for the current task, r4
+ * points to the THREAD for the new task.
+ *
+ * This routine is always called with interrupts disabled.
+ *
+ * Note: there are two ways to get to the "going out" portion
+ * of this code; either by coming in via the entry (_switch)
+ * or via "fork" which must set up an environment equivalent
+ * to the "_switch" path.  If you change this , you'll have to
+ * change the fork code also.
+ *
+ * The code which creates the new task context is in 'copy_thread'
+ * in arch/ppc/kernel/process.c
+ */
+_GLOBAL(paravirt_switch)
+	stwu	r1,-INT_FRAME_SIZE(r1)
+	mflr	r0
+	stw	r0,INT_FRAME_SIZE+4(r1)
+	/* r3-r12 are caller saved -- Cort */
+	SAVE_NVGPRS(r1)
+	stw	r0,_NIP(r1)	/* Return to switch caller */
+	LOAD_MSR_KERNEL(r11,MSR_KERNEL)
+	WRHV_FIX_MSR(r11,r10)
+1:	stw	r11,_MSR(r1)
+	mfcr	r10
+	stw	r10,_CCR(r1)
+	stw	r1,KSP(r3)	/* Set old stack pointer */
+
+#ifdef CONFIG_SMP
+	/* We need a sync somewhere here to make sure that if the
+	 * previous task gets rescheduled on another CPU, it sees all
+	 * stores it has performed on this one.
+	 */
+	sync
+#endif /* CONFIG_SMP */
+
+	tophys(r0,r4)
+	CLR_TOP32(r0)
+	WRHV_MTSPRG3(r0,r3)
+	lwz	r1,KSP(r4)	/* Load new stack pointer */
+
+	/* save the old current 'last' for return value */
+	mr	r3,r2
+	addi	r2,r4,-THREAD	/* Update current */
+
+	lwz	r0,_CCR(r1)
+	mtcrf	0xFF,r0
+	/* r3-r12 are destroyed -- Cort */
+	REST_NVGPRS(r1)
+
+	lwz	r4,_NIP(r1)	/* Return to _switch caller in new task */
+	mtlr	r4
+	addi	r1,r1,INT_FRAME_SIZE
+	blr
+
+
+	/* interrupts are hard-disabled at this point */
+	.globl	paravirt_restore
+paravirt_restore:
+#ifdef	CONFIG_WRHV
+	lis	r4,wr_control@ha
+	lwz	r4,wr_control@l(r4)
+	lwz	r0,GPR0(r1)
+	stw	r0,VB_CONTROL_R0(r4)
+	lwz	r2,GPR2(r1)
+	lwz	r3,GPR3(r1)
+	lwz	r6,GPR6(r1)
+	lwz	r7,GPR7(r1)
+	lwz	r8,GPR8(r1)
+	lwz	r9,GPR9(r1)
+	lwz	r10,GPR10(r1)
+	lwz	r11,GPR11(r1)
+
+	lis	r12,wr_status@ha
+	lwz	r12,wr_status@l(r12)
+	lwz	r5,VB_STATUS_OLD_INT_DISABLE(r12)
+	stw	r5,VB_CONTROL_NEW_INT_DISABLE(r4)
+
+#ifdef VMMU
+	stw	r2,VB_CONTROL_R2(r4)
+	stw	r3,VB_CONTROL_R3(r4)
+	stw	r6,VB_CONTROL_R6(r4)
+	stw	r7,VB_CONTROL_R7(r4)
+	stw	r8,VB_CONTROL_R8(r4)
+	stw	r9,VB_CONTROL_R9(r4)
+	stw	r10,VB_CONTROL_R10(r4)
+	stw	r11,VB_CONTROL_R11(r4)
+#endif /* VMMU */
+	lwz	r0,_CCR(r1)
+	stw	r0,VB_CONTROL_CR(r4)
+	lwz	r0,_NIP(r1)
+	stw	r0,VB_CONTROL_SRR0(r4)
+	lwz	r0,_LINK(r1)
+	mtlr	r0
+#if 0
+	stw	r0,VB_CONTROL_LR(r4)
+#endif
+	lwz	r0,_XER(r1)
+	mtspr	SPRN_XER,r0
+	lwz	r0,_CTR(r1)
+	mtctr	r0
+	lwz	r0,_MSR(r1)
+	WRHV_LOAD_MSR(r0,r12,r5)
+	lwz	r12,GPR12(r1)
+	lwz	r5,GPR5(r1)
+#ifdef VMMU
+	stw	r12,VB_CONTROL_R12(r4)
+	stw	r5,VB_CONTROL_R5(r4)
+
+	lwz	r5,GPR1(r1)
+	stw	r5,VB_CONTROL_R1(r4)
+
+	lwz	r5,GPR4(r1)
+	stw	r5,VB_CONTROL_R4(r4)
+
+	lis	r12,wr_status@ha
+	lwz	r12,wr_status@l(r12)
+
+	lwz	r5,VB_STATUS_EMSR(r12)
+	stw	r5,VB_CONTROL_EMSR(r4)
+
+#if 1
+        /* resume VMMU, since we always turn VMMU back on during the
+         * exception entrance, this is really not needed, but we
+         * make them here anyway for consistency
+         */
+	lwz	r5,VB_STATUS_VMMU0(r12)
+	stw	r5,VB_CONTROL_VMMU0(r4)
+
+	lwz	r5,VB_STATUS_VMMU1(r12)
+	stw	r5,VB_CONTROL_VMMU1(r4)
+#endif
+
+	lis	r0,VBI_SYS_ctx_load_vmmu@h
+	ori	r0,r0,VBI_SYS_ctx_load_vmmu@l
+#else
+	lwz	r4,GPR4(r1)
+	lwz	r1,GPR1(r1)
+	lis	r0,VBI_SYS_ctx_load@h
+#endif  /* VMMU */
+	sc
+
+	/* Never back from here */
+#endif /* CONFIG_WRHV */
+
diff --git a/arch/powerpc/kernel/wrhv_misc_32.S b/arch/powerpc/kernel/wrhv_misc_32.S
new file mode 100644
index 0000000..0bbec39
--- /dev/null
+++ b/arch/powerpc/kernel/wrhv_misc_32.S
@@ -0,0 +1,72 @@
+/*
+ * Low level asm functions for guest implementation on powerpc
+ * 
+ * Copyright (c) 2009 Wind River Systems, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ *
+ */
+
+#include <linux/sys.h>
+#include <asm/unistd.h>
+#include <asm/errno.h>
+#include <asm/reg.h>
+#include <asm/page.h>
+#include <asm/cache.h>
+#include <asm/cputable.h>
+#include <asm/mmu.h>
+#include <asm/ppc_asm.h>
+#include <asm/thread_info.h>
+#include <asm/asm-offsets.h>
+#include <asm/processor.h>
+#include <vbi/interface.h>
+#include <vbi/syscalls.h>
+
+	.text
+
+	.align	5
+_GLOBAL(wrhv_int_lock)
+	WRHV_INT_LOCK(r4,r5)
+	blr
+
+_GLOBAL(wrhv_int_lvl_get)
+	WRHV_INT_LVL_GET(r3)
+	blr
+
+_GLOBAL(wrhv_int_unlock)
+	WRHV_INT_UNLOCK(r3,r4)
+	blr
+
+/*
+ * Write any modified data cache blocks out to memory.
+ * Does not invalidate the corresponding cache lines (especially for
+ * any corresponding instruction cache).
+ *
+ * clean_dcache_range(unsigned long start, unsigned long stop)
+ */
+_GLOBAL(paravirt_clean_dcache_range)
+	/*
+	 * vbi_flush_dcache (void *start_addr, void *end_addr)
+	 */
+	#li	r5, 8
+	subf	r4,r3,r4
+	addi	r4, r4, 1
+	bl	vbi_flush_dcache
+	blr
+
+_GLOBAL(paravirt__flush_dcache_icache)
+	b	vb__flush_dcache_icache
+
+_GLOBAL(paravirt_flush_dcache_range)
+	b	vb_flush_dcache_range
+
+_GLOBAL(paravirt__flush_icache_range)
+	b	vb__flush_icache_range
+
+_GLOBAL(paravirt__flush_dcache_icache_phys)
+	b	vb__flush_dcache_icache_phys
+
+
-- 
1.6.5.2

