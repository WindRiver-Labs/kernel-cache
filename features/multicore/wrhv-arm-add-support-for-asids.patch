From 890ceb5710c7740d8249203245d0e8402c8edf6b Mon Sep 17 00:00:00 2001
From: Jim Somerville <Jim.Somerville@windriver.com>
Date: Tue, 10 Jan 2012 15:28:56 -0500
Subject: [PATCH] wrhv: arm: add support for asids

This still requires CONFIG_CPU_HAS_ASID to be enabled
once the hypervisor support is there.  Also some xml
changes will be needed to indicate the maximum number
of asids available to the VB.

Signed-off-by: Jim Somerville <Jim.Somerville@windriver.com>
---
 arch/arm/include/asm/mmu.h         |   13 +++++++-
 arch/arm/include/asm/mmu_context.h |    2 -
 arch/arm/include/asm/tlbflush.h    |   11 ++++--
 arch/arm/kernel/vbi/wrhv.c         |   59 ++++++++++++++++++++++++++++++++---
 arch/arm/mm/context.c              |   10 ++++++
 arch/arm/mm/tlb-v7.S               |   26 ++++++++++-----
 6 files changed, 99 insertions(+), 22 deletions(-)

diff --git a/arch/arm/include/asm/mmu.h b/arch/arm/include/asm/mmu.h
index 02d305d..26b8e59 100644
--- a/arch/arm/include/asm/mmu.h
+++ b/arch/arm/include/asm/mmu.h
@@ -15,10 +15,19 @@ typedef struct {
 } mm_context_t;
 
 #ifdef CONFIG_CPU_HAS_ASID
-#define ASID(mm)	((mm)->context.id & 255)
+#ifdef CONFIG_WRHV
+extern unsigned int		asid_bits;
+extern unsigned int		mask_asid;
+#define ASID_BITS		asid_bits
 #else
-#define ASID(mm)	(0)
+#define ASID_BITS		8
 #endif
+#define ASID_MASK		((~0) << ASID_BITS)
+
+#define ASID(mm)	((mm)->context.id & ~ASID_MASK)
+#else
+#define ASID(mm)	(0)
+#endif /* CONFIG_CPU_HAS_ASID */
 
 #else
 
diff --git a/arch/arm/include/asm/mmu_context.h b/arch/arm/include/asm/mmu_context.h
index b142846..043ac64 100644
--- a/arch/arm/include/asm/mmu_context.h
+++ b/arch/arm/include/asm/mmu_context.h
@@ -42,8 +42,6 @@ void __check_kvm_seq(struct mm_struct *mm);
  * The context ID is used by debuggers and trace logic, and
  * should be unique within all running processes.
  */
-#define ASID_BITS		8
-#define ASID_MASK		((~0) << ASID_BITS)
 #define ASID_FIRST_VERSION	(1 << ASID_BITS)
 
 extern unsigned int cpu_last_asid;
diff --git a/arch/arm/include/asm/tlbflush.h b/arch/arm/include/asm/tlbflush.h
index 3e6b2d9..4401de8 100644
--- a/arch/arm/include/asm/tlbflush.h
+++ b/arch/arm/include/asm/tlbflush.h
@@ -12,6 +12,9 @@
 
 #ifdef CONFIG_WRHV
 #include <vbi/syscall.h>
+
+#define ALL_VTLBS		-1
+#define FLUSH_EVERYTHING	-1
 #endif
 
 #ifndef CONFIG_MMU
@@ -326,7 +329,7 @@ static inline void local_flush_tlb_all(void)
 	const unsigned int __tlb_flag = __cpu_tlb_flags;
 
 #ifdef CONFIG_WRHV
-	vbi_flush_tlb(0, 0, -1); /* Flush everything */
+	vbi_flush_tlb(ALL_VTLBS, 0, FLUSH_EVERYTHING);
 #endif
 
 	if (tlb_flag(TLB_WB))
@@ -364,7 +367,7 @@ static inline void local_flush_tlb_mm(struct mm_struct *mm)
 	const unsigned int __tlb_flag = __cpu_tlb_flags;
 
 #ifdef CONFIG_WRHV
-	vbi_flush_tlb(0, 0, -1); /* Flush everything */
+	vbi_flush_tlb(asid, 0, FLUSH_EVERYTHING);
 #endif
 
 	if (tlb_flag(TLB_WB))
@@ -415,7 +418,7 @@ local_flush_tlb_page(struct vm_area_struct *vma, unsigned long uaddr)
 	const unsigned int __tlb_flag = __cpu_tlb_flags;
 
 #ifdef CONFIG_WRHV
-	vbi_flush_tlb(0, 0, -1); /* Flush everything */
+	vbi_flush_tlb(ASID(vma->vm_mm), 0, FLUSH_EVERYTHING);
 #endif
 
 	uaddr = (uaddr & PAGE_MASK) | ASID(vma->vm_mm);
@@ -468,7 +471,7 @@ static inline void local_flush_tlb_kernel_page(unsigned long kaddr)
 	const unsigned int __tlb_flag = __cpu_tlb_flags;
 
 #ifdef CONFIG_WRHV
-	vbi_flush_tlb(0, 0, -1); /* Flush everything */
+	vbi_flush_tlb(ALL_VTLBS, 0, FLUSH_EVERYTHING); /* Flush everything */
 #endif
 
 	kaddr &= PAGE_MASK;
diff --git a/arch/arm/kernel/vbi/wrhv.c b/arch/arm/kernel/vbi/wrhv.c
index e07bc1b..d217c3a 100644
--- a/arch/arm/kernel/vbi/wrhv.c
+++ b/arch/arm/kernel/vbi/wrhv.c
@@ -28,6 +28,8 @@
 #include <asm/paravirt.h>
 #include <asm/smp_scu.h>
 #include <asm/wrhv.h>
+#include <asm/mmu_context.h>
+#include <asm/tlbflush.h>
 
 #include <trace/irq.h>
 
@@ -525,19 +527,62 @@ void wrhv_set_pte_ext(pte_t *ptep, pte_t pte, unsigned int ext)
 	 * to -1 will flush them all.  We will leave the second parameter set
 	 * to what we really just want to flush even though it's ignored.
 	 */
-	vbi_flush_tlb(0, (void *)(pte & (-PAGE_SIZE)), -1);
+	vbi_flush_tlb(ASID(current->active_mm), (void *)(pte & (-PAGE_SIZE)),
+			FLUSH_EVERYTHING);
 }
 
+#ifdef CONFIG_CPU_HAS_ASID
+unsigned int asid_bits;
+unsigned int mask_asid;
+
+void wrhv_init_context_base(void)
+{
+	unsigned int number_of_asids;
+	int i;
+
+	/* Change to use the following once hypervisor support is there
+	number_of_asids = vbi_get_max_asid_vmmu();
+	*/
+	number_of_asids = 32;
+	if (number_of_asids < 2) {
+		printk(KERN_ERR "WRHV: asid support not available in hv\n");
+		panic("No extended asid support in hv\n");
+	}
+
+	/* Make sure it is a power of 2.  Round it down to such if need be. */
+	for (i = 1; number_of_asids & (number_of_asids - 1); i++)
+		number_of_asids &= (~0) << i;
+	printk(KERN_INFO "WRHV: Using %d asids\n", number_of_asids);
+
+	/* Determine the number of bits in use by counting the trailing 0s.
+	 * We start counting the number of asids at zero.  So for example, if
+	 * we have 32 asids, then 0 to 31 are in use meaning that 5 bits are
+	 * used to represent them.
+	 */
+	for (i = 1; (number_of_asids - 1) >> i; i++)
+		;
+	printk(KERN_INFO "WRHV: Using %d asid bits\n", i);
+
+	asid_bits = i;
+	cpu_last_asid = ASID_FIRST_VERSION;
+	mask_asid = ~ASID_MASK;
+
+	if (asid_bits < 4)
+		printk(KERN_WARNING
+		"WRHV: Too few asids, expect major performance degradation\n");
+}
+#endif
+
 void wrhv_do_switch_mm(unsigned long pgd_phys, struct mm_struct *mm)
 {
 	wr_control->vb_control_regs.vmmu_handle = mm->context.vmmu_handle;
-	wr_control->vb_control_regs.asid = 0;
+	wr_control->vb_control_regs.asid = ASID(mm);
 	if (vbi_load_ctx() != 0)
 		printk(KERN_WARNING "Bad vmmu handle %lu\n",
 				mm->context.vmmu_handle);
-	/* Flush everything for now, until we support asids.
-	 */
-	vbi_flush_tlb(0, 0, -1);
+#ifndef CONFIG_CPU_HAS_ASID
+	vbi_flush_tlb(0, 0, FLUSH_EVERYTHING);	/* Immediate flush required */
+#endif
 }
 
 void wrhv_smp_cross_call(const struct cpumask *mask)
@@ -604,4 +649,8 @@ void __init wrhv_init(void)
 
 	arm_pm_restart = wrhv_restart;
 	spin_lock_init(&vmmu_handle_lock);
+
+#ifdef CONFIG_CPU_HAS_ASID
+	wrhv_init_context_base();
+#endif
 }
diff --git a/arch/arm/mm/context.c b/arch/arm/mm/context.c
index bcf70c2..739a714 100644
--- a/arch/arm/mm/context.c
+++ b/arch/arm/mm/context.c
@@ -22,7 +22,11 @@ DEFINE_PER_CPU(struct mm_struct *, current_mm);
 
 #ifdef CONFIG_CPU_HAS_ASID
 static DEFINE_SPINLOCK(cpu_asid_lock);
+#ifdef CONFIG_WRHV
+unsigned int cpu_last_asid;
+#else
 unsigned int cpu_last_asid = ASID_FIRST_VERSION;
+#endif
 
 /*
  * We fork()ed a process, and we need a new context for the child
@@ -38,8 +42,10 @@ void __init_new_context(struct task_struct *tsk, struct mm_struct *mm)
 
 static void flush_context(void)
 {
+#ifndef CONFIG_WRHV
 	/* set the reserved ASID before flushing the TLB */
 	asm("mcr	p15, 0, %0, c13, c0, 1\n" : : "r" (0));
+#endif
 	isb();
 	local_flush_tlb_all();
 	if (icache_is_vivt_asid_tagged()) {
@@ -100,8 +106,12 @@ static void reset_context(void *info)
 	flush_context();
 	set_mm_context(mm, asid);
 
+#ifdef CONFIG_WRHV
+	cpu_switch_mm(0, mm);	/* stay in same vmmu but pick up new asid */
+#else
 	/* set the new ASID */
 	asm("mcr	p15, 0, %0, c13, c0, 1\n" : : "r" (mm->context.id));
+#endif
 	isb();
 }
 
diff --git a/arch/arm/mm/tlb-v7.S b/arch/arm/mm/tlb-v7.S
index c645d0f..a843ad9 100644
--- a/arch/arm/mm/tlb-v7.S
+++ b/arch/arm/mm/tlb-v7.S
@@ -37,6 +37,21 @@ ENTRY(v7wbi_flush_user_tlb_range)
 	dsb
 	mov	r0, r0, lsr #PAGE_SHIFT		@ align address
 	mov	r1, r1, lsr #PAGE_SHIFT
+#ifdef CONFIG_WRHV
+	stmfd	sp!, {r0-r2, lr}
+#ifdef CONFIG_CPU_HAS_ASID
+	ldr	r1, =mask_asid
+	ldr	r1, [r1]
+	and	r3, r3, r1
+	mov	r0, r3
+#else
+	mov	r0, #0
+#endif
+	mov	r1, #0
+	mov	r2, #FLUSH_EVERYTHING
+	bl	vbi_flush_tlb
+	ldmfd	sp!, {r0-r2, lr}
+#else
 	asid	r3, r3				@ mask ASID
 	orr	r0, r3, r0, lsl #PAGE_SHIFT	@ Create initial MVA
 	mov	r1, r1, lsl #PAGE_SHIFT
@@ -49,6 +64,7 @@ ENTRY(v7wbi_flush_user_tlb_range)
 	add	r0, r0, #PAGE_SZ
 	cmp	r0, r1
 	blo	1b
+#endif
 	mov	ip, #0
 #ifdef CONFIG_SMP
 	mcr	p15, 0, ip, c7, c1, 6		@ flush BTAC/BTB Inner Shareable
@@ -56,14 +72,6 @@ ENTRY(v7wbi_flush_user_tlb_range)
 	mcr	p15, 0, ip, c7, c5, 6		@ flush BTAC/BTB
 #endif
 	dsb
-#ifdef CONFIG_WRHV
-	stmfd	sp!, {r0-r2, lr}
-	mov	r0, #0
-	mov	r1, #0
-	mov	r2, #-1
-	bl	vbi_flush_tlb
-	ldmfd	sp!, {r0-r2, lr}
-#endif
 	mov	pc, lr
 ENDPROC(v7wbi_flush_user_tlb_range)
 
@@ -102,7 +110,7 @@ ENTRY(v7wbi_flush_kern_tlb_range)
 	stmfd	sp!, {r0-r2, lr}
 	mov	r0, #0
 	mov	r1, #0
-	mov	r2, #-1
+	mov	r2, #FLUSH_EVERYTHING
 	bl	vbi_flush_tlb
 	ldmfd	sp!, {r0-r2, lr}
 #endif
-- 
1.7.0.4

