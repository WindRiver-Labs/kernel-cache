From cee63b9f40de7d19c2fa18660fdda7d9db91552d Mon Sep 17 00:00:00 2001
From: Jeremy McNicoll <jeremy.mcnicoll@windriver.com>
Date: Fri, 18 Feb 2011 14:51:55 -0500
Subject: [PATCH 05/10] WRHV/ASID: paravirt context switch

Paravirtualization of context creation, destruction, switching
is needed such that within each the correct VBI calls are made.
As well the total number of contexts/ASID's must be limited to
the maximum available ASID's.

The kernel needs to limit the total number of ASID's available,
this cap is controlled through the use of a kernel config
option.  (CONFIG_WRHV_NUM_ASID)

When the kernel first boots it creates an ASID of 1.  Upon
successfully booting user processes need to start at context
ID of 2 to MAX_ASID-1.

Signed-off-by: Jeremy McNicoll <jeremy.mcnicoll@windriver.com>
---
 arch/powerpc/include/asm/paravirt.h  |   14 ++
 arch/powerpc/kernel/paravirt.c       |   30 ++++
 arch/powerpc/kernel/vbi/wrhv.c       |  297 ++++++++++++++++++++++++++++++++++
 arch/powerpc/mm/mmu_context_nohash.c |   33 ++++-
 4 files changed, 372 insertions(+), 2 deletions(-)

diff --git a/arch/powerpc/include/asm/paravirt.h b/arch/powerpc/include/asm/paravirt.h
index 13bbf39..6ba41f1 100644
--- a/arch/powerpc/include/asm/paravirt.h
+++ b/arch/powerpc/include/asm/paravirt.h
@@ -38,6 +38,11 @@ extern int native_fsl_pq_mdio_read(struct mii_bus *bus, int mii_id,
 					int devad, int regnum);
 extern void native_udbg_init_uart(void __iomem *comport, unsigned int speed,
 		unsigned int clock);
+extern int native_init_new_context(struct task_struct *t, struct mm_struct *mm);
+extern void native_destroy_context(struct mm_struct *mm);
+extern void native_switch_mmu_context(struct mm_struct *prev,
+		struct mm_struct *next);
+extern void __init native_mmu_context_init(void);
 
 /*
  * paravirtual operations structure
@@ -103,6 +108,14 @@ struct pv_mdio_ops {
 					int devad, int regnum);
 };
 
+struct pv_context_ops {
+	int (*init_new_context)(struct task_struct *t, struct mm_struct *mm);
+	void (*destroy_context)(struct mm_struct *mm);
+	void (*switch_mmu_context)(struct mm_struct *prev,
+		struct mm_struct *next);
+	void (*mmu_context_init)(void);
+};
+
 struct pv_serial_ops{
 	void (*udbg_init_uart)(void __iomem *comport, unsigned int speed,
 		unsigned int clock);
@@ -115,6 +128,7 @@ extern struct pv_irq_ops pv_irq_ops;
 extern struct pv_mmu_ops pv_mmu_ops;
 extern struct pv_mdio_ops pv_mdio_ops;
 extern struct pv_serial_ops pv_serial_ops;
+extern struct pv_context_ops pv_context_ops;
 
 #endif /* CONFIG_PARAVIRT */
 #endif	/* __ASM_PARAVIRT_H */
diff --git a/arch/powerpc/kernel/paravirt.c b/arch/powerpc/kernel/paravirt.c
index f0b9e01..583d690 100644
--- a/arch/powerpc/kernel/paravirt.c
+++ b/arch/powerpc/kernel/paravirt.c
@@ -151,6 +151,13 @@ struct pv_mdio_ops pv_mdio_ops = {
 	.fsl_pq_mdio_read	= native_fsl_pq_mdio_read,
 };
 
+struct pv_context_ops pv_context_ops = {
+	.init_new_context	= native_init_new_context,
+	.destroy_context	= native_destroy_context,
+	.switch_mmu_context	= native_switch_mmu_context,
+	.mmu_context_init	= native_mmu_context_init,
+};
+
 struct pv_serial_ops pv_serial_ops = {
 	.udbg_init_uart	= native_udbg_init_uart,
 };
@@ -172,6 +179,27 @@ void __init paravirt_clocksource_init(void)
 }
 
 
+/* pv_context_ops */
+void paravirt_init_new_context(struct task_struct *t, struct mm_struct *mm)
+{
+	pv_context_ops.init_new_context(t, mm);
+}
+
+void paravirt_destroy_context(struct mm_struct *mm)
+{
+	pv_context_ops.destroy_context(mm);
+}
+
+void paravirt_switch_mmu_context(struct mm_struct *prev, struct mm_struct *next)
+{
+	pv_context_ops.switch_mmu_context(prev, next);
+}
+
+void __init paravirt_mmu_context_init(void)
+{
+	pv_context_ops.mmu_context_init();
+}
+
 /* pv_irq_ops */
 void paravirt_do_IRQ(struct pt_regs *regs)
 {
@@ -310,10 +338,12 @@ extern struct pv_cpu_ops pv_cpu_ops;
 extern struct pv_irq_ops pv_irq_ops;
 extern struct pv_mmu_ops pv_mmu_ops; 
 extern struct pv_mdio_ops pv_mdio_ops;
+extern struct pv_context_ops pv_context_ops;
 
 EXPORT_SYMBOL    (pv_info);
 EXPORT_SYMBOL    (pv_time_ops);
 EXPORT_SYMBOL    (pv_cpu_ops);
+EXPORT_SYMBOL    (pv_context_ops);
 EXPORT_SYMBOL    (pv_mmu_ops);
 EXPORT_SYMBOL    (pv_irq_ops);
 EXPORT_SYMBOL    (pv_mdio_ops);
diff --git a/arch/powerpc/kernel/vbi/wrhv.c b/arch/powerpc/kernel/vbi/wrhv.c
index 4f76e5b..9c1b3ca 100644
--- a/arch/powerpc/kernel/vbi/wrhv.c
+++ b/arch/powerpc/kernel/vbi/wrhv.c
@@ -117,6 +117,10 @@
 #include <linux/clocksource.h>
 #include <linux/kgdb.h>
 
+/* Context switching code */
+#include <asm/mmu_context.h>
+#include <asm/tlbflush.h>
+
 #include <asm/cputhreads.h>
 #include <linux/irq.h>
 #include <asm/tlb.h>
@@ -154,6 +158,30 @@ int wrhv_earlycon = -1;
 int gfar_fn = -1;
 char wrhv_macaddr[6];
 
+unsigned int first_context, last_context;
+unsigned int next_context, nr_free_contexts;
+unsigned long *context_map;
+unsigned long *stale_map[NR_CPUS];
+struct mm_struct **context_mm;
+
+EXPORT_SYMBOL(context_mm);
+EXPORT_SYMBOL(stale_map);
+
+DEFINE_RAW_SPINLOCK(wrhv_context_lock);
+
+#define CTX_MAP_SIZE	\
+	(sizeof(unsigned long) * (last_context / BITS_PER_LONG + 1))
+
+extern unsigned int steal_context_up(unsigned int id);
+static void context_check_map(void) { }
+#define NO_CONTEXT	((unsigned long) -1)
+#define ASID_LAST_CONTEXT	CONFIG_WRHV_NUM_ASID
+#define ASID_FIRST_CONTEXT	2 /* ASID of 1 is reserved in the HV
+					for kernel context, 2 and > are
+					assumed to be userspace */
+#undef CONFIG_WRHV_DEBUG
+
+
 #define WRHV_EARLYCON_SIZE  14  /* sizeof("wrhv_earlycon=") */
 int __init wrhv_earlycon_setup(void)
 {
@@ -1855,6 +1883,269 @@ void wrhv_udbg_init_uart(void __iomem *comport, unsigned int speed,
 #endif
 }
 
+#ifdef CONFIG_WRHV_ASID_OPTIMIZATION
+/* Clone of: arch/powerpc/mm/mmu_context_nohash.c */
+
+unsigned int wrhv_steal_context_up(unsigned int id)
+{
+	struct mm_struct *mm;
+	int cpu = smp_processor_id();
+
+	static VMMU_CONFIG vmmu_cfg;
+
+	/* Pick up the victim mm */
+	mm = context_mm[id];
+#ifdef CONFIG_WRHV_DEBUG
+	printk(" | steal %d from 0x%p MM->ctxID %d \n", id, mm, mm->context.id);
+#endif
+
+	/* Flush the TLB for that context */
+	local_flush_tlb_mm(mm);
+
+	/* Mark this mm has having no context anymore */
+	mm->context.id = MMU_NO_CONTEXT;
+
+	/* XXX This clear should ultimately be part of local_flush_tlb_mm */
+	__clear_bit(id, stale_map[cpu]);
+
+	return id;
+}
+
+
+
+int wrhv_init_new_context(struct task_struct *t, struct mm_struct *mm)
+{
+	unsigned long ctx = next_context;
+	static VMMU_CONFIG vmmu_cfg;
+	unsigned int ret_code;
+	pgd_t *kpdStart, *kpdEnd, *updStart;
+
+
+	pgd_t *pgd = mm->pgd;
+
+	kpdStart = pgd_offset_k(KERNELBASE);
+	kpdEnd =   pgd_offset_k(0xffffffff);
+
+	updStart = pgd + pgd_index(KERNELBASE);
+
+	memcpy(updStart, kpdStart, (kpdEnd - kpdStart + 1) * sizeof (pgd_t));
+
+	vmmu_cfg.addr = (VMMU_LEVEL_1_DESC *) pgd;
+	vmmu_cfg.flush_type = VMMU_TLB_FLUSH_ASID;
+	vmmu_cfg.asid = ctx;
+	vmmu_cfg.vmmu_handle = ctx;
+
+	ret_code = vbi_create_vmmu(&vmmu_cfg);
+	if (ret_code) {
+		printk(" Error creating VMMU handles \n");
+	}
+	mm->context.vmmu_handle = vmmu_cfg.vmmu_handle;
+
+	mm->context.id = NO_CONTEXT;
+	mm->context.active = 0;
+
+	return 0;
+}
+
+
+void wrhv_destroy_context(struct mm_struct *mm)
+{
+	unsigned long flags;
+	unsigned int id;
+	static VMMU_CONFIG vmmu_cfg;
+
+	if (mm->context.id == NO_CONTEXT)
+		return;
+
+	vmmu_cfg.addr = -1;
+	vmmu_cfg.flush_type = VMMU_TLB_FLUSH_ASID;
+	vmmu_cfg.asid = mm->context.id;
+	vmmu_cfg.vmmu_handle = mm->context.vmmu_handle;
+
+	WARN_ON(mm->context.active != 0);
+
+	raw_spin_lock_irqsave(&wrhv_context_lock, flags);
+	id = mm->context.id;
+	if (id != NO_CONTEXT) {
+		__clear_bit(id, context_map);
+		mm->context.id = NO_CONTEXT;
+#ifdef DEBUG_MAP_CONSISTENCY
+		mm->context.active = 0;
+#endif
+		context_mm[id] = NULL;
+		nr_free_contexts++;
+	}
+	vbi_delete_vmmu(&vmmu_cfg);
+	raw_spin_unlock_irqrestore(&wrhv_context_lock, flags);
+}
+
+void wrhv_switch_mmu_context(struct mm_struct *prev, struct mm_struct *next)
+{
+	unsigned int i, id, cpu = smp_processor_id();
+	unsigned long *map;
+	static VMMU_CONFIG vmmu_cfg;
+
+	/* No lockless fast path .. yet */
+	raw_spin_lock(&wrhv_context_lock);
+#ifdef DEBUG_MAP_CONSISTENCY
+	printk("[%d] activating context for mm @%p, active=%d, id=%d",
+		cpu, next, next->context.active, next->context.id);
+#endif
+#ifdef CONFIG_SMP
+	/* Mark us active and the previous one not anymore */
+	next->context.active++;
+	if (prev) {
+		printk(" (old=0x%p a=%d)", prev, prev->context.active);
+		WARN_ON(prev->context.active < 1);
+		prev->context.active--;
+	}
+
+ again:
+#endif /* CONFIG_SMP */
+
+	/* If we already have a valid assigned context, skip all that */
+	id = next->context.id;
+	if (likely(id != NO_CONTEXT)) {
+#ifdef DEBUG_MAP_CONSISTENCY
+		if (context_mm[id] != next)
+			printk("MMU: mm 0x%p has id %d but context_mm[%d] says 0x%p\n",
+				next, id, id, context_mm[id]);
+#endif
+		goto ctxt_ok;
+	}
+
+	/* We really don't have a context, let's try to acquire one */
+	id = next_context;
+	if (id > last_context)
+		id = first_context;
+	map = context_map;
+
+	/* No more free contexts, let's try to steal one */
+	if (nr_free_contexts == 0) {
+#ifdef CONFIG_SMP
+		if (num_online_cpus() > 1) {
+			id = steal_context_smp(id);
+			if (id == NO_CONTEXT)
+				goto again;
+			goto stolen;
+		}
+#endif /* CONFIG_SMP */
+		id = wrhv_steal_context_up(id);
+		goto stolen;
+	}
+	nr_free_contexts--;
+
+	/* We know there's at least one free context, try to find it */
+	while (__test_and_set_bit(id, map)) {
+		id = find_next_zero_bit(map, last_context+1, id);
+		if (id > last_context)
+			id = first_context;
+	}
+ stolen:
+	if (id <= first_context)
+		id = first_context;
+	next_context = id + 1;
+	context_mm[id] = next;
+	next->context.id = id;
+#ifdef CONFIG_WRHV_DEBUG
+	printk(" | new id=%d,nrf=%d", id, nr_free_contexts);
+#endif
+
+	context_check_map();
+ ctxt_ok:
+
+	/* If that context got marked stale on this CPU, then flush the
+	 * local TLB for it and unmark it before we use it
+	 */
+	if (test_bit(id, stale_map[cpu])) {
+#ifdef CONFIG_WRHV_DEBUG
+		printk(" | stale flush %d [%d..%d]",
+				id, cpu_first_thread_in_core(cpu),
+				cpu_last_thread_in_core(cpu));
+#endif
+
+		local_flush_tlb_mm(next);
+
+		/* XXX This clear should ultimately be part of local_flush_tlb_mm */
+		for (i = cpu_first_thread_in_core(cpu);
+			i <= cpu_last_thread_in_core(cpu); i++) {
+			__clear_bit(id, stale_map[i]);
+		}
+	}
+
+	/* Flick the MMU and release lock */
+#ifdef CONFIG_WRHV_ASID_OPTIMIZATION
+#ifdef CONFIG_WRHV_DEBUG
+	printk(" -> %d\n", id);
+#endif
+	vb_context_mmu_on(id, next->pgd, PAGE_SIZE, next->context.id,
+		next->context.vmmu_handle, 0);
+#else
+	set_context(id, next->pgd);
+#endif
+	raw_spin_unlock(&wrhv_context_lock);
+}
+
+
+/*
+ * Initialize the context management stuff.
+ */
+void __init wrhv_mmu_context_init(void)
+{
+	/* Mark init_mm as being active on all possible CPUs since
+	 * we'll get called with prev == init_mm the first time
+	 * we schedule on a given CPU
+	 */
+	init_mm.context.active = NR_CPUS;
+
+	/*
+	 *   The MPC8xx has only 16 contexts.  We rotate through them on each
+	 * task switch.  A better way would be to keep track of tasks that
+	 * own contexts, and implement an LRU usage.  That way very active
+	 * tasks don't always have to pay the TLB reload overhead.  The
+	 * kernel pages are mapped shared, so the kernel can run on behalf
+	 * of any task that makes a kernel entry.  Shared does not mean they
+	 * are not protected, just that the ASID comparison is not performed.
+	 *      -- Dan
+	 *
+	 * The IBM4xx has 256 contexts, so we can just rotate through these
+	 * as a way of "switching" contexts.  If the TID of the TLB is zero,
+	 * the PID/TID comparison is disabled, so we can use a TID of zero
+	 * to represent all kernel pages as shared among all contexts.
+	 * 	-- Dan
+	 */
+	first_context = ASID_FIRST_CONTEXT;
+	last_context = ASID_LAST_CONTEXT;
+
+	/*
+	 * Allocate the maps used by context management
+	 */
+	context_map = alloc_bootmem(CTX_MAP_SIZE);
+	context_mm = alloc_bootmem(sizeof(void *) * (last_context + 1));
+	stale_map[0] = alloc_bootmem(CTX_MAP_SIZE);
+
+#ifdef CONFIG_SMP
+	register_cpu_notifier(&mmu_context_cpu_nb);
+#endif
+
+	printk(KERN_INFO
+		"MMU: Allocated %zu bytes of context maps for %d contexts\n",
+		2 * CTX_MAP_SIZE + (sizeof(void *) * (last_context + 1)),
+		last_context - first_context + 1);
+
+	/*
+	 * Some processors have too few contexts to reserve one for
+	 * init_mm, and require using context 0 for a normal task.
+	 * Other processors reserve the use of context zero for the kernel.
+	 * This code assumes first_context < 32.
+	 */
+	context_map[0] = (1 << first_context) - 1;
+	next_context = first_context;
+	nr_free_contexts = last_context - first_context + 1;
+}
+#endif
+
+
 void wrhv_init(void)
 {
 	/* initialize wr_config so that we can access
@@ -1890,6 +2181,12 @@ void wrhv_init(void)
 
 #ifndef CONFIG_PPC85xx_VT_MODE
 	pv_mmu_ops.vmmu_restore = wrhv_vmmu_restore;
+#ifdef CONFIG_WRHV_ASID_OPTIMIZATION
+	pv_context_ops.init_new_context = wrhv_init_new_context;
+	pv_context_ops.destroy_context = wrhv_destroy_context;
+	pv_context_ops.switch_mmu_context = wrhv_switch_mmu_context;
+	pv_context_ops.mmu_context_init = wrhv_mmu_context_init;
+#endif
 #endif
 	pv_mmu_ops.MMU_init_hw = wrhv_MMU_init_hw;
 	pv_mmu_ops.mmu_mapin_ram = wrhv_mmu_mapin_ram;
diff --git a/arch/powerpc/mm/mmu_context_nohash.c b/arch/powerpc/mm/mmu_context_nohash.c
index 1f2d9ff..1c02b4c 100644
--- a/arch/powerpc/mm/mmu_context_nohash.c
+++ b/arch/powerpc/mm/mmu_context_nohash.c
@@ -62,7 +62,6 @@ static DEFINE_RAW_SPINLOCK(context_lock);
 #define CTX_MAP_SIZE	\
 	(sizeof(unsigned long) * (last_context / BITS_PER_LONG + 1))
 
-
 /* Steal a context from a task that has one at the moment.
  *
  * This is used when we are running out of available PID numbers
@@ -136,7 +135,7 @@ static unsigned int steal_context_smp(unsigned int id)
  * this to work, we somewhat assume that CPUs that are onlined
  * come up with a fully clean TLB (or are cleaned when offlined)
  */
-static unsigned int steal_context_up(unsigned int id)
+unsigned int steal_context_up(unsigned int id)
 {
 	struct mm_struct *mm;
 	int cpu = smp_processor_id();
@@ -157,6 +156,7 @@ static unsigned int steal_context_up(unsigned int id)
 
 	return id;
 }
+EXPORT_SYMBOL(steal_context_up);
 
 #ifdef DEBUG_MAP_CONSISTENCY
 static void context_check_map(void)
@@ -189,8 +189,15 @@ static void context_check_map(void)
 static void context_check_map(void) { }
 #endif
 
+void paravirt_switch_mmu_context(struct mm_struct *prev, struct mm_struct *next)
+	__attribute__((weak, alias("native_switch_mmu_context")));
 void switch_mmu_context(struct mm_struct *prev, struct mm_struct *next)
 {
+	paravirt_switch_mmu_context(prev, next);
+}
+
+void native_switch_mmu_context(struct mm_struct *prev, struct mm_struct *next)
+{
 	unsigned int i, id, cpu = smp_processor_id();
 	unsigned long *map;
 
@@ -279,14 +286,22 @@ void switch_mmu_context(struct mm_struct *prev, struct mm_struct *next)
 	/* Flick the MMU and release lock */
 	pr_hardcont(" -> %d\n", id);
 	set_context(id, next->pgd);
+
 	raw_spin_unlock(&context_lock);
 }
 
 /*
  * Set up the context for a new address space.
  */
+int paravirt_init_new_context(struct task_struct *t, struct mm_struct *mm)
+	__attribute__((weak, alias("native_init_new_context")));
 int init_new_context(struct task_struct *t, struct mm_struct *mm)
 {
+	paravirt_init_new_context(t, mm);
+}
+
+int native_init_new_context(struct task_struct *t, struct mm_struct *mm)
+{
 	pr_hard("initing context for mm @%p\n", mm);
 
 	mm->context.id = MMU_NO_CONTEXT;
@@ -298,8 +313,15 @@ int init_new_context(struct task_struct *t, struct mm_struct *mm)
 /*
  * We're finished using the context for an address space.
  */
+void paravirt_destroy_context(struct mm_struct *mm)
+	__attribute__((weak, alias("native_destroy_context")));
 void destroy_context(struct mm_struct *mm)
 {
+	paravirt_destroy_context(mm);
+}
+
+void native_destroy_context(struct mm_struct *mm)
+{
 	unsigned long flags;
 	unsigned int id;
 
@@ -372,8 +394,15 @@ static struct notifier_block __cpuinitdata mmu_context_cpu_nb = {
 /*
  * Initialize the context management stuff.
  */
+void paravirt_mmu_context_init(void)
+	__attribute__((weak, alias("native_mmu_context_init")));
 void __init mmu_context_init(void)
 {
+	paravirt_mmu_context_init();
+}
+
+void __init native_mmu_context_init(void)
+{
 	/* Mark init_mm as being active on all possible CPUs since
 	 * we'll get called with prev == init_mm the first time
 	 * we schedule on a given CPU
-- 
1.6.5.2

