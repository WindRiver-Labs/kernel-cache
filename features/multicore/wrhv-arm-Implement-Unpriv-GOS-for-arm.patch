From 16a084ed86a309400ea41a3d55fee30715a49549 Mon Sep 17 00:00:00 2001
From: Jim Somerville <Jim.Somerville@windriver.com>
Date: Mon, 14 Jan 2013 17:12:52 -0500
Subject: [PATCH 3/3] wrhv: arm: Implement Unpriv GOS for arm

SMP and multiple VBs are supported, with associated support
in Hypervisor 2.1.

When running unprivileged, the hypervisor actually is always
keeping us in USR mode in terms of the processor.  All other
modes of the processor are emulated.  Thus we must always make
hv calls to read or change the psr since it is emulated.

Multiple ASIDs are not supported at this point.

Signed-off-by: Jim Somerville <Jim.Somerville@windriver.com>
---
 arch/arm/include/asm/arch_vbi.h   |   10 ++-
 arch/arm/include/asm/assembler.h  |   20 +++++
 arch/arm/include/asm/cacheflush.h |    2 +
 arch/arm/include/asm/domain.h     |    8 ++
 arch/arm/include/asm/irqflags.h   |   41 ++++++++++
 arch/arm/include/asm/system.h     |    2 +
 arch/arm/include/asm/tlbflush.h   |   10 +++
 arch/arm/kernel/asm-offsets.c     |   11 +++
 arch/arm/kernel/entry-armv.S      |  146 ++++++++++++++++++++++++++++++++++++-
 arch/arm/kernel/entry-common.S    |   19 +++++
 arch/arm/kernel/entry-header.S    |   80 ++++++++++++++++++++
 arch/arm/kernel/head.S            |   20 +++++
 arch/arm/kernel/process.c         |    2 +-
 arch/arm/kernel/time.c            |    2 +-
 arch/arm/kernel/vbi/syscalls.S    |   15 ++++
 arch/arm/kernel/vbi/wrhv.c        |   38 +++++++---
 arch/arm/mm/cache-v7.S            |   22 ++++++
 arch/arm/mm/proc-v7.S             |    6 ++
 arch/arm/mm/tlb-v7.S              |    4 +
 init/Kconfig.wrhv                 |   11 +++
 kernel/vbi/exports.c              |    3 +
 21 files changed, 456 insertions(+), 16 deletions(-)

diff --git a/arch/arm/include/asm/arch_vbi.h b/arch/arm/include/asm/arch_vbi.h
index fef1c8b..2b4f576 100644
--- a/arch/arm/include/asm/arch_vbi.h
+++ b/arch/arm/include/asm/arch_vbi.h
@@ -16,6 +16,12 @@
 #ifndef _ASM_ARCH_VBI_H
 #define _ASM_ARCH_VBI_H
 
+#include <linux/types.h>
+
+#ifndef CONFIG_WRHV_UNPRIV
+#define PRIVILEGED_GUEST
+#endif
+
 #if (CPU == ARMCA9)
 #define USE_TRUSTZONE
 #endif
@@ -304,7 +310,7 @@ typedef struct
 	uint32_t   dataVal;
 } VBI_BSP_MSG_REPLY;
 
-extern int32_t vbi_load_ctx(void);
+extern int32_t vbi_load_ctx(int32_t type, int32_t psr);
 
 #else /*_ASMLANGUAGE */
 
@@ -429,7 +435,7 @@ extern int32_t vbi_load_ctx(void);
 
 #define VBI_INT_VCORE_STATE_GET(REG0) \
 	mrs	REG0, cpsr;					\
-	and	REG0, REG0, ARM_IMM (CPSR_I | CPSR_F)
+	and	REG0, REG0, ARM_IMM (CPSR_I | CPSR_F);
 
 #else /* USE_TRUSTZONE */
 
diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 6c0ab8b..dafac86 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -20,6 +20,10 @@
 #include <asm/ptrace.h>
 #include <asm/domain.h>
 
+#ifdef CONFIG_WRHV
+#include <vbi/vbi.h>
+#endif
+
 /*
  * Endian independent macros for shifting bytes within registers.
  */
@@ -79,16 +83,32 @@
 #ifndef CONFIG_WRHV
 	cpsid	i
 #else
+#ifdef CONFIG_WRHV_UNPRIV
+	stmfd	sp!, {r0, r8}
+	mov	r0, #0  /* disable */
+	ldr	r8, = VBI_SYS_int_enable
+	smc	#0
+	ldmfd	sp!, {r0, r8}
+#else
 	cpsid	if
 #endif
+#endif
 	.endm
 
 	.macro	enable_irq_notrace
 #ifndef CONFIG_WRHV
 	cpsie	i
 #else
+#ifdef CONFIG_WRHV_UNPRIV
+	stmfd	sp!, {r0, r8}
+	mov	r0, #1  /* enable */
+	ldr	r8, = VBI_SYS_int_enable
+	smc	#0
+	ldmfd	sp!, {r0, r8}
+#else
 	cpsie	if
 #endif
+#endif
 	.endm
 #else
 	.macro	disable_irq_notrace
diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 4656a24..e2bbb02 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -372,9 +372,11 @@ static inline void __flush_icache_all(void)
 	extern void v6_icache_inval_all(void);
 	v6_icache_inval_all();
 #elif defined(CONFIG_SMP) && __LINUX_ARM_ARCH__ >= 7
+#ifndef CONFIG_WRHV_UNPRIV
 	asm("mcr	p15, 0, %0, c7, c1, 0	@ invalidate I-cache inner shareable\n"
 	    :
 	    : "r" (0));
+#endif
 #else
 	asm("mcr	p15, 0, %0, c7, c5, 0	@ invalidate I-cache\n"
 	    :
diff --git a/arch/arm/include/asm/domain.h b/arch/arm/include/asm/domain.h
index af18cea..5dd5c34 100644
--- a/arch/arm/include/asm/domain.h
+++ b/arch/arm/include/asm/domain.h
@@ -28,6 +28,7 @@
  * 36-bit addressing and supersections are only available on
  * CPUs based on ARMv6+ or the Intel XSC3 core.
  */
+#ifndef CONFIG_WRHV_UNPRIV
 #ifndef CONFIG_IO_36
 #define DOMAIN_KERNEL	0
 #define DOMAIN_TABLE	0
@@ -39,6 +40,13 @@
 #define DOMAIN_USER	1
 #define DOMAIN_IO	0
 #endif
+#else
+#define DOMAIN_KERNEL	0
+#define DOMAIN_TABLE	0
+#define DOMAIN_USER	0
+#define DOMAIN_IO	0
+#endif
+
 
 /*
  * Domain types
diff --git a/arch/arm/include/asm/irqflags.h b/arch/arm/include/asm/irqflags.h
index 0006e64..012fb51 100644
--- a/arch/arm/include/asm/irqflags.h
+++ b/arch/arm/include/asm/irqflags.h
@@ -15,6 +15,23 @@
  * Treat fiqs the same as regular direct interrupts with respect to
  * enable/disable.
  */
+#ifdef CONFIG_WRHV_UNPRIV
+#include <vbi/syscall.h>
+
+extern struct vb_status *wr_status;
+extern struct vb_control *wr_control;
+
+#define raw_local_irq_save(x)					\
+	({							\
+	__asm__ __volatile__(					\
+	"mrs	%0, cpsr		@ local_irq_save\n"	\
+	: "=r" (x) : : "memory", "cc");				\
+	vbi_vcore_irq_lock();					\
+	})
+
+#define raw_local_irq_enable	vbi_vcore_irq_unlock
+#define raw_local_irq_disable	vbi_vcore_irq_lock
+#else
 #define raw_local_irq_save(x)					\
 	({							\
 	__asm__ __volatile__(					\
@@ -25,6 +42,7 @@
 
 #define raw_local_irq_enable()  __asm__("cpsie if @ __sti" : : : "memory", "cc")
 #define raw_local_irq_disable() __asm__("cpsid if @ __cli" : : : "memory", "cc")
+#endif /* WRHV_UNPRIV */
 #define local_fiq_enable()
 #define local_fiq_disable()
 #else
@@ -122,6 +140,28 @@
 
 #endif
 
+#ifdef CONFIG_WRHV_UNPRIV
+/*
+ * Save the current interrupt enable state.
+ */
+#define raw_local_save_flags(x)					\
+	({							\
+	__asm__ __volatile__(					\
+	"mrs	%0, cpsr		@ local_save_flags"	\
+	: "=r" (x) : : "memory", "cc");				\
+	})
+
+/*
+ * restore saved IRQ & FIQ state
+ */
+#define raw_local_irq_restore(x)				\
+	({							\
+	if (x & PSR_I_BIT)					\
+		vbi_vcore_irq_lock();				\
+	else							\
+		vbi_vcore_irq_unlock();				\
+	})
+#else
 /*
  * Save the current interrupt enable state.
  */
@@ -141,6 +181,7 @@
 	:							\
 	: "r" (x)						\
 	: "memory", "cc")
+#endif
 
 #define raw_irqs_disabled_flags(flags)	\
 ({					\
diff --git a/arch/arm/include/asm/system.h b/arch/arm/include/asm/system.h
index 6155bdb..7e67b8c 100644
--- a/arch/arm/include/asm/system.h
+++ b/arch/arm/include/asm/system.h
@@ -186,9 +186,11 @@ static inline unsigned int get_cr(void)
 
 static inline void set_cr(unsigned int val)
 {
+#ifndef CONFIG_WRHV_UNPRIV
 	asm volatile("mcr p15, 0, %0, c1, c0, 0	@ set CR"
 	  : : "r" (val) : "cc");
 	isb();
+#endif
 }
 
 #ifndef CONFIG_SMP
diff --git a/arch/arm/include/asm/tlbflush.h b/arch/arm/include/asm/tlbflush.h
index 3e6b2d9..b5ad800 100644
--- a/arch/arm/include/asm/tlbflush.h
+++ b/arch/arm/include/asm/tlbflush.h
@@ -340,8 +340,10 @@ static inline void local_flush_tlb_all(void)
 		asm("mcr p15, 0, %0, c8, c6, 0" : : "r" (zero) : "cc");
 	if (tlb_flag(TLB_V4_I_FULL | TLB_V6_I_FULL))
 		asm("mcr p15, 0, %0, c8, c5, 0" : : "r" (zero) : "cc");
+#ifndef CONFIG_WRHV_UNPRIV
 	if (tlb_flag(TLB_V7_UIS_FULL))
 		asm("mcr p15, 0, %0, c8, c3, 0" : : "r" (zero) : "cc");
+#endif
 
 	if (tlb_flag(TLB_BTB)) {
 		/* flush the branch target cache */
@@ -349,12 +351,14 @@ static inline void local_flush_tlb_all(void)
 		dsb();
 		isb();
 	}
+#ifndef CONFIG_WRHV_UNPRIV
 	if (tlb_flag(TLB_V7_IS_BTB)) {
 		/* flush the branch target cache */
 		asm("mcr p15, 0, %0, c7, c1, 6" : : "r" (zero) : "cc");
 		dsb();
 		isb();
 	}
+#endif
 }
 
 static inline void local_flush_tlb_mm(struct mm_struct *mm)
@@ -392,8 +396,10 @@ static inline void local_flush_tlb_mm(struct mm_struct *mm)
 #ifdef CONFIG_ARM_ERRATA_720789
 		asm("mcr p15, 0, %0, c8, c3, 0" : : "r" (zero) : "cc");
 #else
+#ifndef CONFIG_WRHV_UNPRIV
 		asm("mcr p15, 0, %0, c8, c3, 2" : : "r" (asid) : "cc");
 #endif
+#endif
 
 	if (tlb_flag(TLB_BTB)) {
 		/* flush the branch target cache */
@@ -402,7 +408,9 @@ static inline void local_flush_tlb_mm(struct mm_struct *mm)
 	}
 	if (tlb_flag(TLB_V7_IS_BTB)) {
 		/* flush the branch target cache */
+#ifndef CONFIG_WRHV_UNPRIV
 		asm("mcr p15, 0, %0, c7, c1, 6" : : "r" (zero) : "cc");
+#endif
 		dsb();
 		isb();
 	}
@@ -442,6 +450,7 @@ local_flush_tlb_page(struct vm_area_struct *vma, unsigned long uaddr)
 		asm("mcr p15, 0, %0, c8, c6, 1" : : "r" (uaddr) : "cc");
 	if (tlb_flag(TLB_V6_I_PAGE))
 		asm("mcr p15, 0, %0, c8, c5, 1" : : "r" (uaddr) : "cc");
+#ifndef CONFIG_WRHV_UNPRIV
 	if (tlb_flag(TLB_V7_UIS_PAGE))
 #ifdef CONFIG_ARM_ERRATA_720789
 		asm("mcr p15, 0, %0, c8, c3, 3" : : "r" (uaddr & PAGE_MASK) : "cc");
@@ -460,6 +469,7 @@ local_flush_tlb_page(struct vm_area_struct *vma, unsigned long uaddr)
 		dsb();
 		isb();
 	}
+#endif
 }
 
 static inline void local_flush_tlb_kernel_page(unsigned long kaddr)
diff --git a/arch/arm/kernel/asm-offsets.c b/arch/arm/kernel/asm-offsets.c
index 0f21990..8c2e896 100644
--- a/arch/arm/kernel/asm-offsets.c
+++ b/arch/arm/kernel/asm-offsets.c
@@ -130,6 +130,10 @@ int main(void)
   DEFINE(CTRL_SPACE_ASID,	offsetof(struct vb_arch_ctrl_regs, asid));
   DEFINE(CTRL_SPACE_VMMUHANDLE,	offsetof(struct vb_arch_ctrl_regs,
 					vmmu_handle));
+  DEFINE(CTRL_SPACE_R0,		offsetof(struct vb_arch_ctrl_regs, r0));
+  DEFINE(CTRL_SPACE_SP,		offsetof(struct vb_arch_ctrl_regs, sp));
+  DEFINE(CTRL_SPACE_LR,		offsetof(struct vb_arch_ctrl_regs, lr));
+  DEFINE(CTRL_SPACE_PC,		offsetof(struct vb_arch_ctrl_regs, pc));
   DEFINE(STAT_SPACE_IFAR,	offsetof(struct vb_arch_stat_regs, ifar));
   DEFINE(STAT_SPACE_IFSR,	offsetof(struct vb_arch_stat_regs, ifsr));
   DEFINE(STAT_SPACE_DFAR,	offsetof(struct vb_arch_stat_regs, dfar));
@@ -138,6 +142,13 @@ int main(void)
 					modeSpecificReg[ABT_MODE & 0xf].lr));
   DEFINE(STAT_SPACE_ABT_SPSR,	offsetof(struct vb_arch_stat_regs,
 					modeSpecificReg[ABT_MODE & 0xf].spsr));
+  DEFINE(STAT_SPACE_USR_LR,	offsetof(struct vb_arch_stat_regs,
+					modeSpecificReg[USR_MODE & 0xf].lr));
+  DEFINE(STAT_SPACE_USR_SP,	offsetof(struct vb_arch_stat_regs,
+					modeSpecificReg[USR_MODE & 0xf].sp));
+  DEFINE(STAT_SPACE_MODE,	offsetof(struct vb_arch_stat_regs, mode));
+  DEFINE(STAT_SPACE_MODE_SPEC,	offsetof(struct vb_arch_stat_regs,
+					modeSpecificReg[0]));
   DEFINE(CTRL_SPACE_PTR,	offsetof(struct vb_config, vb_control));
   DEFINE(WRHV_COREID,		offsetof(struct vb_config, coreId));
   DEFINE(VMMU_CFG_ADDR,		offsetof(VMMU_CONFIG, addr));
diff --git a/arch/arm/kernel/entry-armv.S b/arch/arm/kernel/entry-armv.S
index 72ce65f..6224bf0 100644
--- a/arch/arm/kernel/entry-armv.S
+++ b/arch/arm/kernel/entry-armv.S
@@ -25,6 +25,7 @@
 
 #ifdef CONFIG_WRHV
 #include <asm/wrhv.h>
+#include <vbi/vbi.h>
 #endif
 
 #include "entry-header.S"
@@ -151,6 +152,12 @@ ENDPROC(__und_invalid)
  SPFIX(	tst	sp, #4		)
 #endif
  SPFIX(	subeq	sp, sp, #4	)
+#ifdef CONFIG_WRHV_UNPRIV
+	ldr	r1, [r0, #12]		@ restore r1
+	ldr	r2, [r0, #16]		@ restore scratch regs
+	ldr	r3, [r0, #20]
+	ldr	r8, [r0, #24]
+#endif
 	stmia	sp, {r1 - r12}
 
 	ldmia	r0, {r1 - r3}
@@ -212,7 +219,17 @@ __dabt_svc:
 	@
 	@ set desired IRQ state, then call main handler
 	@
+#ifdef CONFIG_WRHV_UNPRIV
+	mov	r12, r0				@ preserve r0
+	mov	r0, #0
+	tst	r9, #PSR_I_BIT
+	moveq	r0, #1				@ i bit not set, so enable ints
+	ldr	r8, =VBI_SYS_int_enable
+	smc	#0
+	mov	r0, r12
+#else
 	msr	cpsr_c, r9
+#endif
 	mov	r2, sp
 	bl	do_DataAbort
 
@@ -375,7 +392,17 @@ __pabt_svc:
 #else
 	bl	CPU_PABORT_HANDLER
 #endif
-	msr	cpsr_c, r9			@ Maybe enable interrupts
+#ifdef CONFIG_WRHV_UNPRIV
+	mov	r12, r0				@ preserve r0
+	mov	r0, #0
+	tst	r9, #PSR_I_BIT
+	moveq	r0, #1				@ i bit not set, so enable ints
+	ldr	r8, =VBI_SYS_int_enable
+	smc	#0
+	mov	r0, r12
+#else
+	msr	cpsr_c, r9
+#endif
 	mov	r2, sp				@ regs
 	bl	do_PrefetchAbort		@ call abort handler
 
@@ -416,6 +443,12 @@ ENDPROC(__pabt_svc)
  UNWIND(.fnstart	)
  UNWIND(.cantunwind	)	@ don't unwind the user space
 	sub	sp, sp, #S_FRAME_SIZE
+#ifdef CONFIG_WRHV_UNPRIV
+	ldr	r1, [r0, #12]		@ restore r1
+	ldr	r2, [r0, #16]		@ restore scratch regs
+	ldr	r3, [r0, #20]
+	ldr	r8, [r0, #24]
+#endif
  ARM(	stmib	sp, {r1 - r12}	)
  THUMB(	stmia	sp, {r0 - r12}	)
 
@@ -436,7 +469,15 @@ ENDPROC(__pabt_svc)
 	@ Also, separately save sp_usr and lr_usr
 	@
 	stmia	r0, {r2 - r4}
+#ifndef CONFIG_WRHV_UNPRIV
  ARM(	stmdb	r0, {sp, lr}^			)
+#else
+	ldr	r5, =wr_status
+	ldr	r5, [r5]
+	ldr	r6, [r5, #STAT_SPACE_USR_SP]
+	ldr	r7, [r5, #STAT_SPACE_USR_LR]
+	stmdb	r0, {r6, r7}
+#endif
  THUMB(	store_user_sp_lr r0, r1, S_SP - S_PC	)
 
 	@
@@ -567,7 +608,7 @@ __fiq_usr:
 	mov	why, #0
 	b	ret_to_user
  UNWIND(.fnend		)
-ENDPROC(__irq_usr)
+ENDPROC(__fiq_usr)
 
 	.ltorg
 #endif /* CONFIG_WRHV */
@@ -788,6 +829,43 @@ __und_usr_unknown:
 	b	do_undefinstr
 ENDPROC(__und_usr_unknown)
 
+#ifdef CONFIG_WRHV_UNPRIV
+	@ Returns with:
+	@ r0 = 0
+	@ r1 = ptr to status space
+	@ r2 = spsr equivalent
+	@ r3 = sp
+	@ r8 = current spsr
+__early_housekeeping:
+	str	r1, [sp, #12]		@ free up a few more scratch regs
+	str	r2, [sp, #16]
+	str	r3, [sp, #20]
+	str	r8, [sp, #24]
+	ldr	r1, =wr_status
+	ldr	r1, [r1]
+	ldr	r8, [r1, #STAT_SPACE_MODE]
+	and	r2, r8, #0xf
+
+	@ Get the spsr
+	mov	r3, #12			@ mode specific regs have 3 entries
+	mul	r3, r2, r3
+	add	r3, r3, r1
+	add	r3, r3, #STAT_SPACE_MODE_SPEC
+	ldr	r2, [r3, #0]		@ spsr is at offset 0
+	str	r2, [sp, #8]		@ save spsr
+
+	@ Set up for the jump to svc mode
+	mov	r0, #0
+	ldr	r3, =wr_control
+	ldr	r3, [r3]
+	str	r0, [r3, #CTRL_SPACE_SP]
+	str	r0, [r3, #CTRL_SPACE_LR]
+	str	r0, [r3, #CTRL_SPACE_PC]
+	mov	r3, sp
+	mov	pc, lr
+ENDPROC(__early_housekeeping)
+#endif
+
 	.align	5
 __pabt_usr:
 	usr_entry
@@ -1201,6 +1279,45 @@ ENDPROC(vector_\name)
 1:
 	.endm
 
+	.macro	wrhv_unpriv_vector_stub, name, mode, correction=0
+	.align	2
+
+vector_\name:
+	.if \correction
+	sub	lr, lr, #\correction
+	.endif
+
+	@
+	@ Save r0, lr_<exception> (parent PC) and spsr_<exception>
+	@ (parent CPSR)
+	@
+	stmia	sp, {r0, lr}		@ save r0, lr
+	ldr	r0, LCearly_housekeeping
+	mov	lr, pc
+	mov	pc, r0
+
+	@ Shift into emulated svc mode
+	@ Housekeeping already set up the rest of our parms
+	eor	r1, r8, #(\mode ^ SVC_MODE | PSR_ISETSTATE \
+					| PSR_I_BIT | PSR_F_BIT)
+	ldr	r8, =VBI_SYS_ctx_load
+	smc	#0
+
+	@
+	@ the branch table must immediately follow this code
+	@
+	ldr	r2, [r3, #8]		@ reload r2 - the ctx load destroyed it
+	and	r1, r2, #0x0f
+	mov	r0, r3			@ the old sp before the mode switch
+	ldr	r1, [pc, r1, lsl #2]
+	mov	pc, r1			@ branch to handler in SVC mode
+ENDPROC(vector_\name)
+
+	.align	2
+	@ handler addresses follow this label
+1:
+	.endm
+
 #ifdef CONFIG_WRHV
 	.macro	wrhv_vector_abt_stub, name, mode, correction=0
 	.align	5
@@ -1256,7 +1373,11 @@ __stubs_start:
 /*
  * Interrupt dispatcher
  */
+#ifdef CONFIG_WRHV_UNPRIV
+	wrhv_unpriv_vector_stub	irq, IRQ_MODE, 4
+#else
 	vector_stub	irq, IRQ_MODE, 4
+#endif
 
 	.long	__irq_usr			@  0  (USR_26 / USR_32)
 	.long	__irq_invalid			@  1  (FIQ_26 / FIQ_32)
@@ -1279,7 +1400,11 @@ __stubs_start:
 /*
  * FIQ Interrupt dispatcher
  */
+#ifdef CONFIG_WRHV_UNPRIV
+	wrhv_unpriv_vector_stub	fiq, FIQ_MODE, 4
+#else
 	vector_stub	fiq, FIQ_MODE, 4
+#endif
 
 	.long	__fiq_usr			@  0  (USR_26 / USR_32)
 	.long	__fiq_invalid			@  1  (FIQ_26 / FIQ_32)
@@ -1303,11 +1428,15 @@ __stubs_start:
  * Data abort dispatcher
  * Enter in ABT mode, spsr = USR CPSR, lr = USR PC
  */
+#ifdef CONFIG_WRHV_UNPRIV
+	wrhv_unpriv_vector_stub	dabt, ABT_MODE, 8
+#else
 #ifdef CONFIG_WRHV
 	wrhv_vector_abt_stub	dabt, ABT_MODE, 8
 #else
 	vector_stub		dabt, ABT_MODE, 8
 #endif
+#endif
 
 	.long	__dabt_usr			@  0  (USR_26 / USR_32)
 	.long	__dabt_invalid			@  1  (FIQ_26 / FIQ_32)
@@ -1330,11 +1459,15 @@ __stubs_start:
  * Prefetch abort dispatcher
  * Enter in ABT mode, spsr = USR CPSR, lr = USR PC
  */
+#ifdef CONFIG_WRHV_UNPRIV
+	wrhv_unpriv_vector_stub	pabt, ABT_MODE, 4
+#else
 #ifdef CONFIG_WRHV
 	wrhv_vector_abt_stub	pabt, ABT_MODE, 4
 #else
 	vector_stub		pabt, ABT_MODE, 4
 #endif
+#endif
 
 	.long	__pabt_usr			@  0 (USR_26 / USR_32)
 	.long	__pabt_invalid			@  1 (FIQ_26 / FIQ_32)
@@ -1357,7 +1490,11 @@ __stubs_start:
  * Undef instr entry dispatcher
  * Enter in UND mode, spsr = SVC/USR CPSR, lr = SVC/USR PC
  */
+#ifdef CONFIG_WRHV_UNPRIV
+	wrhv_unpriv_vector_stub	und, UND_MODE
+#else
 	vector_stub	und, UND_MODE
+#endif
 
 	.long	__und_usr			@  0 (USR_26 / USR_32)
 	.long	__und_invalid			@  1 (FIQ_26 / FIQ_32)
@@ -1418,6 +1555,11 @@ vector_addrexcptn:
 LCstatptr:
 	.word	0x00000000	@ to be filled in from wr_status content
 #endif
+#ifdef CONFIG_WRHV_UNPRIV
+	.globl	LCearly_housekeeping
+LCearly_housekeeping:
+	.word	__early_housekeeping
+#endif
 
 	.globl	__stubs_end
 __stubs_end:
diff --git a/arch/arm/kernel/entry-common.S b/arch/arm/kernel/entry-common.S
index 7252517..0b75959 100644
--- a/arch/arm/kernel/entry-common.S
+++ b/arch/arm/kernel/entry-common.S
@@ -196,10 +196,29 @@ ENTRY(vector_swi)
 	sub	sp, sp, #S_FRAME_SIZE
 	stmia	sp, {r0 - r12}			@ Calling r0 - r12
  ARM(	add	r8, sp, #S_PC		)
+#ifndef CONFIG_WRHV_UNPRIV
  ARM(	stmdb	r8, {sp, lr}^		)	@ Calling sp, lr
  THUMB(	mov	r8, sp			)
  THUMB(	store_user_sp_lr r8, r10, S_SP	)	@ calling sp, lr
 	mrs	r8, spsr			@ called from non-FIQ mode, so ok.
+#else
+	ldr	r9, =wr_status
+	ldr	r9, [r9]
+
+	@ Grab and store the usr sp and lr
+	ldr	r10, [r9, #STAT_SPACE_USR_SP]
+	ldr	r11, [r9, #STAT_SPACE_USR_LR]
+	stmdb	r8, {r10, r11}
+
+        @ Get the spsr
+	ldr	r10, [r9, #STAT_SPACE_MODE]
+	and	r10, r10, #0xf
+	mov	r11, #12		@ mode specific regs have 3 entries
+	mul	r11, r10, r11
+	add	r11, r11, r9
+	add	r11, r11, #STAT_SPACE_MODE_SPEC
+	ldr	r8, [r11, #0]		@ spsr is at offset 0
+#endif
 	str	lr, [sp, #S_PC]			@ Save calling PC
 	str	r8, [sp, #S_PSR]		@ Save CPSR
 	str	r0, [sp, #S_OLD_R0]		@ Save OLD_R0
diff --git a/arch/arm/kernel/entry-header.S b/arch/arm/kernel/entry-header.S
index 725da4b..19906ff 100644
--- a/arch/arm/kernel/entry-header.S
+++ b/arch/arm/kernel/entry-header.S
@@ -6,6 +6,10 @@
 #include <asm/errno.h>
 #include <asm/thread_info.h>
 
+#ifdef CONFIG_WRHV_UNPRIV
+#include <vbi/vbi.h>
+#endif
+
 @ Bad Abort numbers
 @ -----------------
 @
@@ -76,8 +80,58 @@
 	msr	cpsr_c, \rtemp			@ switch back to the SVC mode
 	.endm
 
+	.macro	copy_stack_regs_to_ctrl_space, fast = 0
+	@ destroys regs r0, r2-r10
+	@ On entry:
+	@ sp must point to just after the lr to be copied
+	@ On exit:
+	@ r2 points to control space
+
+	ldr	r2, =wr_control
+	ldr	r2, [r2]			@ ptr to control space
+	add	r3, r2, #CTRL_SPACE_R0
+
+	@ Point to the regs on the stack
+	.if	\fast
+	@ Skip copying r0
+	sub	r4, sp, #(14*4)
+	add	r3, r3, #4
+	.else
+	sub	r4, sp, #(15*4)
+	.endif
+
+	@ Do 12
+	ldmia	r4!, {r5-r10}
+	stmia	r3!, {r5-r10}
+	ldmia	r4!, {r5-r10}
+	stmia	r3!, {r5-r10}
+
+	.if	\fast
+	@ Do 2 more for a total of 14 registers
+	ldmia	r4!, {r5-r6}
+	stmia	r3!, {r5-r6}
+	.else
+	@ Do 3 more for a total of 15 registers
+	ldmia	r4!, {r5-r7}
+	stmia	r3!, {r5-r7}
+	.endif
+
+	.endm
+
 #ifndef CONFIG_THUMB2_KERNEL
 	.macro	svc_exit, rpsr
+#ifdef CONFIG_WRHV_UNPRIV
+	mov	r1, \rpsr
+	add	sp, sp, #(15*4)
+	copy_stack_regs_to_ctrl_space
+	ldmia	sp, {r12}			@ get the pc
+	sub	sp, sp, #(15*4)
+	str	r12, [r2, #CTRL_SPACE_PC]
+	mov	r0, #1
+	ldr	r8, =VBI_SYS_ctx_load
+	clrex
+	smc	#0
+#else
 	msr	spsr_cxsf, \rpsr
 #if defined(CONFIG_CPU_32v6K)
 	clrex					@ clear the exclusive monitor
@@ -89,11 +143,13 @@
 #else
 	ldmia	sp, {r0 - pc}^			@ load r0 - pc, cpsr
 #endif
+#endif
 	.endm
 
 	.macro	restore_user_regs, fast = 0, offset = 0
 	ldr	r1, [sp, #\offset + S_PSR]	@ get calling cpsr
 	ldr	lr, [sp, #\offset + S_PC]!	@ get pc
+#ifndef CONFIG_WRHV_UNPRIV
 	msr	spsr_cxsf, r1			@ save in spsr_svc
 #if defined(CONFIG_CPU_32v6K)
 	clrex					@ clear the exclusive monitor
@@ -109,6 +165,30 @@
 						@ after ldm {}^
 	add	sp, sp, #S_FRAME_SIZE - S_PC
 	movs	pc, lr				@ return & move spsr_svc into cpsr
+#else
+	clrex
+	mov	r11, r0				@ preserve r0
+	copy_stack_regs_to_ctrl_space fast=\fast
+	.if	\fast
+	str	r11, [r2, #CTRL_SPACE_R0]
+	.endif
+	str	lr, [r2, #CTRL_SPACE_PC]
+	ldr	r8, =wr_status
+	ldr	r8, [r8]
+	ldr	r9, [r2, #CTRL_SPACE_SP]
+	str	r9, [r8, #STAT_SPACE_USR_SP]
+	ldr	r9, [r2, #CTRL_SPACE_LR]
+	str	r9, [r8, #STAT_SPACE_USR_LR]
+	mov	r8, #0
+	str	r8, [r2, #CTRL_SPACE_SP]
+	str	r8, [r2, #CTRL_SPACE_LR]
+
+	@ Return sequence
+	add	sp, sp, #S_FRAME_SIZE - S_PC
+	mov	r0, #1
+	ldr	r8, =VBI_SYS_ctx_load
+	smc	#0
+#endif
 	.endm
 
 	.macro	get_thread_info, rd
diff --git a/arch/arm/kernel/head.S b/arch/arm/kernel/head.S
index ed00dec..a3c60e2 100644
--- a/arch/arm/kernel/head.S
+++ b/arch/arm/kernel/head.S
@@ -26,6 +26,9 @@
 #ifdef CONFIG_WRHV
 #include <asm/wrhv.h>
 #endif
+#ifdef CONFIG_WRHV_UNPRIV
+#include <vbi/syscall.h>
+#endif
 
 #if (PHYS_OFFSET & 0x001fffff)
 #error "PHYS_OFFSET must be at an even 2MiB boundary!"
@@ -81,8 +84,20 @@
  */
 	__HEAD
 ENTRY(stext)
+#ifdef CONFIG_WRHV_UNPRIV
+	@ Shift into emulated svc mode
+	@ Do it directly here since the lr will change on a mode shift and
+	@ so the function in the vbi won't return to us properly.
+	mov	r12, r0				@ preserve ptr to config area
+	mov	r0, #0
+	mov	r1, #(PSR_F_BIT | PSR_I_BIT | SVC_MODE)
+	ldr	r8, =VBI_SYS_ctx_load
+	smc	#0
+	mov	r0, r12
+#else
 	setmode	PSR_F_BIT | PSR_I_BIT | SVC_MODE, r9 @ ensure svc mode
 						@ and irqs disabled
+#endif
 	mrc	p15, 0, r9, c0, c0		@ get processor id
 #ifdef CONFIG_WRHV
 #ifdef CONFIG_SMP
@@ -133,7 +148,10 @@ ENTRY(secondary_startup)
 	 * the processor type - there is no need to check the machine type
 	 * as it has already been validated by the primary processor.
 	 */
+#ifndef CONFIG_WRHV
+	@ Already done as the hv releases processors at the stext entry pt.
 	setmode	PSR_F_BIT | PSR_I_BIT | SVC_MODE, r9
+#endif
 	mrc	p15, 0, r9, c0, c0		@ get processor id
 	bl	__lookup_processor_type
 	movs	r10, r5				@ invalid processor?
@@ -199,11 +217,13 @@ __enable_mmu:
 #ifdef CONFIG_CPU_ICACHE_DISABLE
 	bic	r0, r0, #CR_I
 #endif
+#ifndef CONFIG_WRHV_UNPRIV
 	mov	r5, #(domain_val(DOMAIN_USER, DOMAIN_MANAGER) | \
 		      domain_val(DOMAIN_KERNEL, DOMAIN_MANAGER) | \
 		      domain_val(DOMAIN_TABLE, DOMAIN_MANAGER) | \
 		      domain_val(DOMAIN_IO, DOMAIN_CLIENT))
 	mcr	p15, 0, r5, c3, c0, 0		@ load domain access register
+#endif
 #ifndef CONFIG_WRHV
 	mcr	p15, 0, r4, c2, c0, 0		@ load page table pointer
 	b	__turn_mmu_on
diff --git a/arch/arm/kernel/process.c b/arch/arm/kernel/process.c
index f8ccbd9..b2cb14c 100644
--- a/arch/arm/kernel/process.c
+++ b/arch/arm/kernel/process.c
@@ -252,7 +252,7 @@ void __show_regs(struct pt_regs *regs)
 		unsigned int ctrl;
 
 		buf[0] = '\0';
-#ifdef CONFIG_CPU_CP15_MMU
+#if defined(CONFIG_CPU_CP15_MMU) && !defined(CONFIG_WRHV_UNPRIV)
 		{
 			unsigned int transbase, dac;
 			asm("mrc p15, 0, %0, c2, c0\n\t"
diff --git a/arch/arm/kernel/time.c b/arch/arm/kernel/time.c
index 12382b4..8eb9c71 100644
--- a/arch/arm/kernel/time.c
+++ b/arch/arm/kernel/time.c
@@ -220,7 +220,7 @@ void __init time_init(void)
 		system_timer->offset = dummy_gettimeoffset;
 #endif
 	system_timer->init();
-#ifdef CONFIG_HAVE_SCHED_CLOCK
+#if defined(CONFIG_HAVE_SCHED_CLOCK) && !defined(CONFIG_WRHV_UNPRIV)
 	sched_clock_postinit();
 #endif
 }
diff --git a/arch/arm/kernel/vbi/syscalls.S b/arch/arm/kernel/vbi/syscalls.S
index 338a68c..9d86a5b 100644
--- a/arch/arm/kernel/vbi/syscalls.S
+++ b/arch/arm/kernel/vbi/syscalls.S
@@ -30,7 +30,9 @@ For this reason and others, THIS CODE WILL PROBABLY NOT WORK IN THUMB MODE.
 #include <vbi/vbi.h>
 #include <asm/vbi.h>
 
+#ifndef CONFIG_WRHV_UNPRIV
 #define PRIVILEGED_GUEST
+#endif
 
 #if (CPU == ARMCA9)
 #define USE_TRUSTZONE
@@ -1035,7 +1037,20 @@ FUNC_LABEL(vbi_flush_tlb)
         SAVEREGS
         ldr     r8, =VBI_SYS_tlb_flush
         HCALL
+#ifdef PRIVILEGED_GUEST
+        cmp     r0, #0          /* Flush all? */
+        beq     vbiTlbFlush_all
+
+        /* Flush only the returned hwAsid in r0 */
+        TLBIASID(r0)
+        mov     r0, #0
+        RESTOREREGS
+        mov     pc, lr
+
+vbiTlbFlush_all:
+        mov     r0, #0
         TLBIALL(r0)
+#endif
         RESTOREREGS
         mov     pc, lr
 FUNC_END(vbi_flush_tlb)
diff --git a/arch/arm/kernel/vbi/wrhv.c b/arch/arm/kernel/vbi/wrhv.c
index 33a86b8..d300e62 100644
--- a/arch/arm/kernel/vbi/wrhv.c
+++ b/arch/arm/kernel/vbi/wrhv.c
@@ -49,8 +49,8 @@ struct vb_config *wr_config = (void *)(-1); /* keep it out of the bss */
 EXPORT_SYMBOL(wr_config);
 
 /* control and status pointers are set from info in the config region */
-struct vb_control *wr_control;
-struct vb_status *wr_status;
+struct vb_control *wr_control = (void *)(-1); /* keep it out of the bss */
+struct vb_status *wr_status = (void *)(-1); /* keep it out of the bss */
 extern struct vb_status *LCstatptr;
 
 static char *direct_interrupts_list;
@@ -107,6 +107,8 @@ void wrhv_load_initial_vmmu(uint32_t pgtbl)
 
 	VMMU_CONFIG	vmmu_cfg;
 
+	wr_status = wr_config->vb_status;
+	wr_control = wr_config->vb_control;
 	vbi_enable_vmmu(0);
 
 	if (wr_config->coreId == 0) {
@@ -126,7 +128,11 @@ void wrhv_load_initial_vmmu(uint32_t pgtbl)
 	} else
 		wr_config->vb_control->vb_control_regs.vmmu_handle = pgtbl;
 	wr_config->vb_control->vb_control_regs.asid = 0;
-	if (vbi_load_ctx() != 0) {
+#ifdef CONFIG_WRHV_UNPRIV
+	if (vbi_load_ctx(3, 0) != 0) { /* 3 means change vm only */
+#else
+	if (vbi_load_ctx(0, 0) != 0) {
+#endif
 		printk(KERN_ERR "Could not load initial context!\n");
 		while (1) {}; /* No point in continuing */
 	}
@@ -185,12 +191,18 @@ int wrhv_request_ipis(void)
 	};
 
 	int err;
-
-	ipi_irq = vbi_find_irq(IPI_IRQ_BASE_NAME, VB_INPUT_INT);
+	char ipi_irq_name[64];
+	char board_id[1];
+
+	if ((unsigned int)wr_config->boardID > MAX_BOARD_ID)
+		panic("WRHV boardID exceeds limit\n");
+	board_id[0] = '0' + wr_config->boardID;
+	strncpy(ipi_irq_name, IPI_IRQ_BASE_NAME, sizeof(IPI_IRQ_BASE_NAME));
+	strncat(ipi_irq_name, board_id, 1);
+	ipi_irq = vbi_find_irq(ipi_irq_name, VB_INPUT_INT);
 	if (ipi_irq == VBI_INVALID_IRQ) {
-		printk(KERN_ERR "WRHV lookup of interrupt name '"
-				IPI_IRQ_BASE_NAME
-				"' failed!\n");
+		printk(KERN_ERR "WRHV lookup of interrupt name '%s' failed!\n",
+			ipi_irq_name);
 		panic("WRHV resolve irq for IPI failed\n");
 	}
 
@@ -479,8 +491,10 @@ void __init wrhv_time_init(void)
 	setup_irq(TIMERTICK_IRQ, &wrhv_timer_irq);
 	vbi_unmask_vioapic_irq(TIMERTICK_IRQ); /* Allow timer ints through */
 
+#ifndef CONFIG_WRHV_UNPRIV
 	if (wrhv_machine_init_timer)
 		wrhv_machine_init_timer();
+#endif
 }
 
 void __devinit wrhv_setup_secondary_clock(void)
@@ -542,8 +556,12 @@ void wrhv_set_pte_ext(pte_t *ptep, pte_t pte, unsigned int ext)
 void wrhv_do_switch_mm(unsigned long pgd_phys, struct mm_struct *mm)
 {
 	wr_control->vb_control_regs.vmmu_handle = mm->context.vmmu_handle;
-	wr_control->vb_control_regs.asid = 0;
-	if (vbi_load_ctx() != 0)
+	wr_control->vb_control_regs.asid = ASID(mm);
+#ifdef CONFIG_WRHV_UNPRIV
+	if (vbi_load_ctx(3, 0) != 0)
+#else
+	if (vbi_load_ctx(0, 0) != 0)
+#endif
 		printk(KERN_WARNING "Bad vmmu handle %lu\n",
 				mm->context.vmmu_handle);
 	/* Flush everything for now, until we support asids.
diff --git a/arch/arm/mm/cache-v7.S b/arch/arm/mm/cache-v7.S
index 37c8157..188bd18 100644
--- a/arch/arm/mm/cache-v7.S
+++ b/arch/arm/mm/cache-v7.S
@@ -28,6 +28,13 @@
  */
 ENTRY(v7_flush_dcache_all)
 	dmb					@ ensure ordering with previous memory accesses
+#ifdef CONFIG_WRHV_UNPRIV
+	stmfd	sp!, {r0-r1, lr}
+	mov	r0, #0				@ start
+	mov	r1, #-1				@ everything
+	bl	vbi_flush_dcache
+	ldmfd	sp!, {r0-r1, lr}
+#else
 	mrc	p15, 1, r0, c0, c0, 1		@ read clidr
 	ands	r3, r0, #0x7000000		@ extract loc from clidr
 	mov	r3, r3, lsr #23			@ left align loc bit field
@@ -70,6 +77,7 @@ skip:
 finished:
 	mov	r10, #0				@ swith back to cache level 0
 	mcr	p15, 2, r10, c0, c0, 0		@ select current cache level in cssr
+#endif
 	dsb
 	isb
 	mov	pc, lr
@@ -90,12 +98,14 @@ ENTRY(v7_flush_kern_cache_all)
  ARM(	stmfd	sp!, {r4-r5, r7, r9-r11, lr}	)
  THUMB(	stmfd	sp!, {r4-r7, r9-r11, lr}	)
 	bl	v7_flush_dcache_all
+#ifndef CONFIG_WRHV_UNPRIV
 	mov	r0, #0
 #ifdef CONFIG_SMP
 	mcr	p15, 0, r0, c7, c1, 0		@ invalidate I-cache inner shareable
 #else
 	mcr	p15, 0, r0, c7, c5, 0		@ I+BTB cache invalidate
 #endif
+#endif
  ARM(	ldmfd	sp!, {r4-r5, r7, r9-r11, lr}	)
  THUMB(	ldmfd	sp!, {r4-r7, r9-r11, lr}	)
 	mov	pc, lr
@@ -159,6 +169,11 @@ ENTRY(v7_coherent_kern_range)
  */
 ENTRY(v7_coherent_user_range)
  UNWIND(.fnstart		)
+#ifdef CONFIG_WRHV_UNPRIV
+	stmfd	sp!, {lr}
+	bl	vb__flush_icache_range
+	ldmfd	sp!, {lr}
+#else
 	dcache_line_size r2, r3
 	sub	r3, r2, #1
 	bic	r0, r0, r3
@@ -189,6 +204,7 @@ ENTRY(v7_coherent_user_range)
 	mov	r0, r0, lsl #12
 	add	r0, r0, #4096
 	b	2b
+#endif
  UNWIND(.fnend		)
 ENDPROC(v7_coherent_kern_range)
 ENDPROC(v7_coherent_user_range)
@@ -203,6 +219,11 @@ ENDPROC(v7_coherent_user_range)
  *	- size	- region size
  */
 ENTRY(v7_flush_kern_dcache_area)
+#ifdef CONFIG_WRHV_UNPRIV
+	stmfd	sp!, {lr}
+	bl	vbi_flush_dcache
+	ldmfd	sp!, {lr}
+#else
 	dcache_line_size r2, r3
 	add	r1, r0, r1
 1:
@@ -210,6 +231,7 @@ ENTRY(v7_flush_kern_dcache_area)
 	add	r0, r0, r2
 	cmp	r0, r1
 	blo	1b
+#endif
 	dsb
 	mov	pc, lr
 ENDPROC(v7_flush_kern_dcache_area)
diff --git a/arch/arm/mm/proc-v7.S b/arch/arm/mm/proc-v7.S
index 6698385..64f04be 100644
--- a/arch/arm/mm/proc-v7.S
+++ b/arch/arm/mm/proc-v7.S
@@ -46,12 +46,14 @@ ENDPROC(cpu_v7_proc_init)
 
 ENTRY(cpu_v7_proc_fin)
 	stmfd	sp!, {lr}
+#ifndef CONFIG_WRHV_UNPRIV
 	cpsid	if				@ disable interrupts
 	bl	v7_flush_kern_cache_all
 	mrc	p15, 0, r0, c1, c0, 0		@ ctrl register
 	bic	r0, r0, #0x1000			@ ...i............
 	bic	r0, r0, #0x0006			@ .............ca.
 	mcr	p15, 0, r0, c1, c0, 0		@ disable caches
+#endif
 	ldmfd	sp!, {pc}
 ENDPROC(cpu_v7_proc_fin)
 
@@ -82,6 +84,10 @@ ENTRY(cpu_v7_do_idle)
 	mov	pc, lr
 ENDPROC(cpu_v7_do_idle)
 
+#ifdef CONFIG_WRHV_UNPRIV
+#define TLB_CAN_READ_FROM_L1_CACHE
+#endif
+
 ENTRY(cpu_v7_dcache_clean_area)
 #ifndef TLB_CAN_READ_FROM_L1_CACHE
 	dcache_line_size r2, r3
diff --git a/arch/arm/mm/tlb-v7.S b/arch/arm/mm/tlb-v7.S
index c645d0f..f8787a8 100644
--- a/arch/arm/mm/tlb-v7.S
+++ b/arch/arm/mm/tlb-v7.S
@@ -41,20 +41,24 @@ ENTRY(v7wbi_flush_user_tlb_range)
 	orr	r0, r3, r0, lsl #PAGE_SHIFT	@ Create initial MVA
 	mov	r1, r1, lsl #PAGE_SHIFT
 1:
+#ifndef CONFIG_WRHV_UNPRIV
 #ifdef CONFIG_SMP
 	mcr	p15, 0, r0, c8, c3, 1		@ TLB invalidate U MVA (shareable) 
 #else
 	mcr	p15, 0, r0, c8, c7, 1		@ TLB invalidate U MVA
 #endif
+#endif
 	add	r0, r0, #PAGE_SZ
 	cmp	r0, r1
 	blo	1b
 	mov	ip, #0
+#ifndef CONFIG_WRHV_UNPRIV
 #ifdef CONFIG_SMP
 	mcr	p15, 0, ip, c7, c1, 6		@ flush BTAC/BTB Inner Shareable
 #else
 	mcr	p15, 0, ip, c7, c5, 6		@ flush BTAC/BTB
 #endif
+#endif
 	dsb
 #ifdef CONFIG_WRHV
 	stmfd	sp!, {r0-r2, lr}
diff --git a/init/Kconfig.wrhv b/init/Kconfig.wrhv
index 83947a9..7513264 100644
--- a/init/Kconfig.wrhv
+++ b/init/Kconfig.wrhv
@@ -8,6 +8,17 @@ config WRHV
 	select SERIAL_CORE
 	select SERIAL_CORE_CONSOLE
 
+config WRHV_UNPRIV
+	bool "Wind River Hypervisor Unprivileged"
+	default y
+	depends on WRHV && ARM
+	help
+	  This option is provided to avoid doing privileged operations with
+	  the Hypervisor.  In this mode, PSR operations are all emulated as
+	  the Linux Kernel will be really running in USR mode.  As such,
+	  accesses to most control registers are disallowed and have to be
+	  emulated through hypervisor syscalls.  Requires HV 2.1 or later.
+
 config WRHV_SAFETY_PROFILE
 	bool "Safety Profile Hypervisor"
 	default n
diff --git a/kernel/vbi/exports.c b/kernel/vbi/exports.c
index 7916a45..29666e4 100644
--- a/kernel/vbi/exports.c
+++ b/kernel/vbi/exports.c
@@ -38,3 +38,6 @@ EXPORT_SYMBOL(vbi_find_irq);
 EXPORT_SYMBOL(vbi_ns_lookup);
 
 EXPORT_SYMBOL(vbi_send);
+
+EXPORT_SYMBOL(vbi_vcore_irq_lock);
+EXPORT_SYMBOL(vbi_vcore_irq_unlock);
-- 
1.7.0

