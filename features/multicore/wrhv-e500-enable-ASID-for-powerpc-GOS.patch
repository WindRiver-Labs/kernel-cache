From bb4bf5ddb1d8c641270b61d3fcd84a85674fe8b2 Mon Sep 17 00:00:00 2001
From: Liang Li <liang.li@windriver.com>
Date: Mon, 8 Aug 2011 16:38:26 +0800
Subject: [PATCH 6/8] wrhv: e500: enable ASID for powerpc GOS

ASID - Address Space Identifier

The main purpose of ASID is to act as a MMU Assist register
thereby limiting the number of flushes and increasing the
overall guest performance.

o add kconfig option
[
Creation of user selectable and tunable ASID value. This
menu item enables and disables the ASID performance
feature.

Ideally we could have enabled and set the number of ASID's
through the bootline.  But the call to parse the bootline
value comes way too late for this.
]

o paravirt context switch
[
Paravirtualization of context creation, destruction, switching
is needed such that within each the correct VBI calls are made.
As well the total number of contexts/ASID's must be limited to
the maximum available ASID's.

The kernel needs to limit the total number of ASID's available,
this cap is controlled through the use of a kernel config
option.  (CONFIG_WRHV_NUM_ASID)

When the kernel first boots it creates an ASID of 1.  Upon
successfully booting user processes need to start at context
ID of 2 to MAX_ASID-1.
]

o Associate VMMU Handle and ASID to every MM
[
There needs to be a direct corelation between every
MM and VMMU Handle.  A VMMUHandle exists for the duration
of every allocated MM.  This association is used during
context creation and deletion.
]

o proper config and control setup before sys_ctx_load
[
The vbi_load_ctx stub that is included as part of the official
Hypervisor DVD is insufficient.  Prior to calling sys_ctx_load,
the config and control page need to be updated with the proper
values so that when the HV calls RFI Linux returns to the correct
location.

Currently the only caller of this function is the ASID performance
optimization.

This code is owned by the VBI and shortly will become part of the
official VBI release at which point this patch can be reverted.
]

o Update wr_config/control in and out of exceptions
[
Linux needs to push the virtual ASID and VMMU Handle when
returning from and going to exceptions so that they are
in sync with the Hypervisor.
]

o Paravirtualized set_context adjustments
[
The paravirtualized version of set_context is insuffifcient
for controlling the Hypervisor with ASID optimizations enabled.
Function signature updates are needed to distinguish between
non-asid enabled hypervisor and enabled, this provides backwards
compatibility.

The first ASID is treated differently within the hypervisor,
given that it is statically mapped within the hypervisor.
]

Signed-off-by: Jeremy McNicoll <jeremy.mcnicoll@windriver.com>
---
 arch/powerpc/include/asm/mmu-book3e.h |    4 +
 arch/powerpc/include/asm/paravirt.h   |   15 ++
 arch/powerpc/kernel/head_wrhv.h       |   15 ++
 arch/powerpc/kernel/paravirt.c        |   30 +++
 arch/powerpc/kernel/vbi/util.c        |   36 +++-
 arch/powerpc/kernel/vbi/wrhv.c        |  324 ++++++++++++++++++++++++++++++++-
 arch/powerpc/kernel/wrhv_entry_32.S   |   12 ++
 arch/powerpc/mm/mmu_context_nohash.c  |   33 +++-
 init/Kconfig.wrhv                     |   21 ++
 9 files changed, 477 insertions(+), 13 deletions(-)

diff --git a/arch/powerpc/include/asm/mmu-book3e.h b/arch/powerpc/include/asm/mmu-book3e.h
index 7469581..9e1119b 100644
--- a/arch/powerpc/include/asm/mmu-book3e.h
+++ b/arch/powerpc/include/asm/mmu-book3e.h
@@ -181,6 +181,10 @@ typedef struct {
 	unsigned int	id;
 	unsigned int	active;
 	unsigned long	vdso_base;
+#ifdef CONFIG_WRHV_ASID_OPTIMIZATION
+	unsigned int	asid;
+	unsigned long	vmmu_handle;
+#endif
 } mm_context_t;
 
 /* Page size definitions, common between 32 and 64-bit
diff --git a/arch/powerpc/include/asm/paravirt.h b/arch/powerpc/include/asm/paravirt.h
index 9e5d10c..cf0e9f8 100644
--- a/arch/powerpc/include/asm/paravirt.h
+++ b/arch/powerpc/include/asm/paravirt.h
@@ -37,6 +37,12 @@ extern int __attribute__((weak)) native_fsl_pq_mdio_write(struct mii_bus *bus, i
 extern int __attribute__((weak)) native_fsl_pq_mdio_read(struct mii_bus *bus, int mii_id,
 					int devad, int regnum);
 
+extern int native_init_new_context(struct task_struct *t, struct mm_struct *mm);
+extern void native_destroy_context(struct mm_struct *mm);
+extern void native_switch_mmu_context(struct mm_struct *prev,
+		struct mm_struct *next);
+extern void __init native_mmu_context_init(void);
+
 /*
  * paravirtual operations structure
  */
@@ -101,12 +107,21 @@ struct pv_mdio_ops {
 					int devad, int regnum);
 };
 
+struct pv_context_ops {
+	int (*init_new_context)(struct task_struct *t, struct mm_struct *mm);
+	void (*destroy_context)(struct mm_struct *mm);
+	void (*switch_mmu_context)(struct mm_struct *prev,
+		struct mm_struct *next);
+	void (*mmu_context_init)(void);
+};
+
 extern struct pv_info pv_info;
 extern struct pv_time_ops pv_time_ops;
 extern struct pv_cpu_ops pv_cpu_ops;
 extern struct pv_irq_ops pv_irq_ops;
 extern struct pv_mmu_ops pv_mmu_ops;
 extern struct pv_mdio_ops pv_mdio_ops;
+extern struct pv_context_ops pv_context_ops;
 
 #endif /* CONFIG_PARAVIRT */
 #endif	/* __ASM_PARAVIRT_H */
diff --git a/arch/powerpc/kernel/head_wrhv.h b/arch/powerpc/kernel/head_wrhv.h
index 1ea026e..2133805 100644
--- a/arch/powerpc/kernel/head_wrhv.h
+++ b/arch/powerpc/kernel/head_wrhv.h
@@ -38,6 +38,20 @@
 	 * registers contain their value before the system call was executed.
 	 */
 #ifndef	CONFIG_PPC85xx_VT_MODE
+#ifdef CONFIG_WRHV_ASID_OPTIMIZATION
+#define ASID_OPT							\
+	lis     r9,wr_control@ha;					\
+	lwz     r9,wr_control@l(r9);					\
+	lwz     r12,VB_STATUS_VMMU0(r4);				\
+	stw     r12,VB_CONTROL_VMMU0(r9);				\
+	lwz     r12,VB_STATUS_VMMU_HANDLE(r4);				\
+	stw     r12,VB_CONTROL_VMMU_HANDLE(r9);				\
+	lwz     r12,VB_STATUS_ASID(r4);					\
+	stw     r12,VB_CONTROL_ASID(r9)
+#else
+#define ASID_OPT							\
+	nop
+#endif
 #undef NORMAL_EXCEPTION_PROLOG
 #define NORMAL_EXCEPTION_PROLOG						     \
         mr      r4,r1;                                                       \
@@ -65,6 +79,7 @@
         stw     r12,GPR4(r11);                                               \
         lwz     r12,VB_STATUS_CR(r4);                                        \
         stw     r12,_CCR(r11);                                               \
+        ASID_OPT;                                                            \
         lwz     r9,VB_STATUS_SRR1(r4);                                       \
 	rlwinm  r9,r9,0,18,15; /* Clear EE & PR bits */                      \
 	mr	r12, r4;                                                     \
diff --git a/arch/powerpc/kernel/paravirt.c b/arch/powerpc/kernel/paravirt.c
index 080130d..5a43f54 100644
--- a/arch/powerpc/kernel/paravirt.c
+++ b/arch/powerpc/kernel/paravirt.c
@@ -151,6 +151,13 @@ struct pv_mdio_ops pv_mdio_ops = {
 	.fsl_pq_mdio_read	= native_fsl_pq_mdio_read,
 };
 
+struct pv_context_ops pv_context_ops = {
+	.init_new_context	= native_init_new_context,
+	.destroy_context	= native_destroy_context,
+	.switch_mmu_context	= native_switch_mmu_context,
+	.mmu_context_init	= native_mmu_context_init,
+};
+
 /* pv_time_ops */
 void __init paravirt_time_init_cont(void)
 {
@@ -168,6 +175,27 @@ void __init paravirt_clocksource_init(void)
 }
 
 
+/* pv_context_ops */
+void paravirt_init_new_context(struct task_struct *t, struct mm_struct *mm)
+{
+	pv_context_ops.init_new_context(t, mm);
+}
+
+void paravirt_destroy_context(struct mm_struct *mm)
+{
+	pv_context_ops.destroy_context(mm);
+}
+
+void paravirt_switch_mmu_context(struct mm_struct *prev, struct mm_struct *next)
+{
+	pv_context_ops.switch_mmu_context(prev, next);
+}
+
+void __init paravirt_mmu_context_init(void)
+{
+	pv_context_ops.mmu_context_init();
+}
+
 /* pv_irq_ops */
 void paravirt_do_IRQ(struct pt_regs *regs)
 {
@@ -300,10 +328,12 @@ extern struct pv_cpu_ops pv_cpu_ops;
 extern struct pv_irq_ops pv_irq_ops;
 extern struct pv_mmu_ops pv_mmu_ops; 
 extern struct pv_mdio_ops pv_mdio_ops;
+extern struct pv_context_ops pv_context_ops;
 
 EXPORT_SYMBOL    (pv_info);
 EXPORT_SYMBOL    (pv_time_ops);
 EXPORT_SYMBOL    (pv_cpu_ops);
+EXPORT_SYMBOL    (pv_context_ops);
 EXPORT_SYMBOL    (pv_mmu_ops);
 EXPORT_SYMBOL    (pv_irq_ops);
 EXPORT_SYMBOL    (pv_mdio_ops);
diff --git a/arch/powerpc/kernel/vbi/util.c b/arch/powerpc/kernel/vbi/util.c
index 62e3d6e..0fd41fd 100644
--- a/arch/powerpc/kernel/vbi/util.c
+++ b/arch/powerpc/kernel/vbi/util.c
@@ -88,18 +88,39 @@ void vb_pte_set(void *pPte, unsigned long paddr, int protval)
  */
 
 int vb_context_mmu_on(int pid,	/* context id */
-		      void *pgtable,	/* level 1 page table */
-		      int pagesize, int debug)
+	void *pgtable, int pagesize, int asid, int vmmu_handle,
+	int debug)
 {
 	static VMMU_CONFIG vmmu_cfg;
 
 	if (wr_config == (struct vb_config *)(- 1) || pgtable == NULL || pagesize <= 0)
 		return -1;
 
+#ifdef CONFIG_WRHV_ASID_OPTIMIZATION
+	/* Create needs to be called when dealing with kernel only,
+	   once we create a process, in this case we don't need to
+	   copy the kernel page tables since swapper_pg_dir is being
+	   passed in. */
+	if (asid == 1) { /* special case.  Kernel ASID needs to
+		be created here.  Once the first userspace process is
+		available init, create and destroy will do all the heavy
+		lifting. */
+		vmmu_cfg.addr = (VMMU_LEVEL_1_DESC *) pgtable;
+		vmmu_cfg.flush_type = VMMU_TLB_FLUSH_ASID;
+		vmmu_cfg.asid = asid;
+		vmmu_cfg.vmmu_handle = vmmu_handle;
+
+		if ((vbi_create_vmmu(&vmmu_cfg)) != 0)
+			return -1;
+	}
+	wr_control->vb_control_regs.asid = asid;
+	wr_control->vb_control_regs.vmmu_handle = vmmu_handle;
+	vbi_load_ctx();
+#else
 	vmmu_cfg.addr = (VMMU_LEVEL_1_DESC *) pgtable;
-	vmmu_cfg.pageSize = pagesize;
-	vmmu_cfg.contextId = pid;
-	vmmu_cfg.vmmuNum = 0;	/* only vmmu 0 is support for the time being */
+	vmmu_cfg.flush_type = pagesize;
+	vmmu_cfg.asid = pid;
+	vmmu_cfg.vmmu_handle = 0; /* only vmmu 0 is supported for now */
 
 	if ((vbi_config_vmmu(&vmmu_cfg)) != 0)
 		return -1;
@@ -110,10 +131,11 @@ int vb_context_mmu_on(int pid,	/* context id */
 		printk("End of page table display \n");
 	}
 
-	vbi_enable_vmmu(vmmu_cfg.vmmuNum);
-
+	vbi_enable_vmmu(vmmu_cfg.vmmu_handle);
+#endif
 	return 0;
 }
+EXPORT_SYMBOL(vb_context_mmu_on);
 
 void vb__flush_dcache_icache(void *start)
 {
diff --git a/arch/powerpc/kernel/vbi/wrhv.c b/arch/powerpc/kernel/vbi/wrhv.c
index c215c5b..074d055 100644
--- a/arch/powerpc/kernel/vbi/wrhv.c
+++ b/arch/powerpc/kernel/vbi/wrhv.c
@@ -117,6 +117,10 @@
 #include <linux/clocksource.h>
 #include <linux/kgdb.h>
 
+/* Context switching code */
+#include <asm/mmu_context.h>
+#include <asm/tlbflush.h>
+
 #include <asm/cputhreads.h>
 #include <linux/irq.h>
 #include <asm/tlb.h>
@@ -145,7 +149,7 @@ extern int map_page(unsigned long, phys_addr_t, int);
 
 extern int vb_context_mmu_on(int pid,  /* context id */
 			void *pgtable,    /* level 1 page table */
-			int pagesize, int debug);
+			int pagesize, int asid, int vmmu_handle, int debug);
 
 /* declared in linux/arch/powerpc/kernel/time.c */
 
@@ -161,6 +165,35 @@ char wrhv_net_name[15]; /* eth0, eth1, eth2... */
 int wrhv_nic_num = 0; /* start with eth0 as default */
 int wrhv_nic_start = 0;
 
+unsigned int first_context, last_context;
+unsigned int next_context, nr_free_contexts;
+unsigned long *context_map;
+unsigned long *stale_map[NR_CPUS];
+struct mm_struct **context_mm;
+
+EXPORT_SYMBOL(context_mm);
+EXPORT_SYMBOL(stale_map);
+
+DEFINE_RAW_SPINLOCK(wrhv_context_lock);
+
+#define CTX_MAP_SIZE	\
+	(sizeof(unsigned long) * (last_context / BITS_PER_LONG + 1))
+
+extern unsigned int steal_context_up(unsigned int id);
+static void context_check_map(void) { }
+#define NO_CONTEXT	((unsigned long) -1)
+#define ASID_LAST_CONTEXT	CONFIG_WRHV_NUM_ASID
+#define ASID_FIRST_CONTEXT	2 /* ASID of 1 is reserved in the HV
+					for kernel context, 2 and > are
+					assumed to be userspace */
+#undef CONFIG_WRHV_DEBUG
+
+#ifdef CONFIG_WRHV_ASID_OPTIMIZATION
+#define KERNEL_BASE_ASID 1
+#else
+#define KERNEL_BASE_ASID 0
+#endif
+
 #define WRHV_EARLYCON_SIZE  14  /* sizeof("wrhv_earlycon=") */
 int __init wrhv_earlycon_setup(void)
 {
@@ -1009,6 +1042,12 @@ void wrhv_vmmu_restore(void)
 	 */
 	wr_control->vmmu0 = wr_status->vmmu0;
 	wr_control->vmmu1 = wr_status->vmmu1;
+#ifdef CONFIG_WRHV_ASID_OPTIMIZATION
+	wr_control->vb_control_regs.vmmu_handle =
+		wr_status->vb_status_regs.vmmu_handle;
+	wr_control->vb_control_regs.asid =
+		wr_status->vb_status_regs.asid;
+#endif
 	return;
 }
 
@@ -1095,7 +1134,8 @@ void __init wrhv_MMU_init(void)
 	 * we enable the mmu here without having to do this from the caller
 	 * (which is in assembly world)
 	 */
-	vb_context_mmu_on(0, swapper_pg_dir, PAGE_SIZE, 0);
+	vb_context_mmu_on(0, swapper_pg_dir, PAGE_SIZE, KERNEL_BASE_ASID,
+		KERNEL_BASE_ASID, 0);
 #endif
 
 	/* Check if enable direct interrupt mode. */
@@ -1146,7 +1186,14 @@ void wrhv_set_context(unsigned long contextId, pgd_t * pgd)
 	memcpy(updStart, kpdStart, (kpdEnd - kpdStart + 1) * sizeof (pgd_t));
 
 	/* in linux context, page table entry is not set up yet */
-	vb_context_mmu_on(contextId, pgd, PAGE_SIZE, 0);
+#ifdef CONFIG_WRHV_ASID_OPTIMIZATION
+	vb_context_mmu_on(contextId, pgd, PAGE_SIZE,
+		wr_control->vb_control_regs.asid,
+		wr_control->vb_control_regs.vmmu_handle, 0);
+#else
+	vb_context_mmu_on(contextId, pgd, PAGE_SIZE, KERNEL_BASE_ASID,
+		KERNEL_BASE_ASID, 0);
+#endif
 }
 
 /* arch/powerpc/mm/pgtable_32.c */
@@ -1849,6 +1896,268 @@ int wrhv_mdio_read(struct mii_bus *bus, int mii_id, int devad, int regnum)
 	return mdio_reply.dataVal;
 }
 
+#ifdef CONFIG_WRHV_ASID_OPTIMIZATION
+/* Clone of: arch/powerpc/mm/mmu_context_nohash.c */
+
+unsigned int wrhv_steal_context_up(unsigned int id)
+{
+	struct mm_struct *mm;
+	int cpu = smp_processor_id();
+
+	static VMMU_CONFIG vmmu_cfg;
+
+	/* Pick up the victim mm */
+	mm = context_mm[id];
+#ifdef CONFIG_WRHV_DEBUG
+	printk(" | steal %d from 0x%p MM->ctxID %d \n", id, mm, mm->context.id);
+#endif
+
+	/* Flush the TLB for that context */
+	local_flush_tlb_mm(mm);
+
+	/* Mark this mm has having no context anymore */
+	mm->context.id = MMU_NO_CONTEXT;
+
+	/* XXX This clear should ultimately be part of local_flush_tlb_mm */
+	__clear_bit(id, stale_map[cpu]);
+
+	return id;
+}
+
+
+
+int wrhv_init_new_context(struct task_struct *t, struct mm_struct *mm)
+{
+	unsigned long ctx = next_context;
+	static VMMU_CONFIG vmmu_cfg;
+	unsigned int ret_code;
+	pgd_t *kpdStart, *kpdEnd, *updStart;
+
+
+	pgd_t *pgd = mm->pgd;
+
+	kpdStart = pgd_offset_k(KERNELBASE);
+	kpdEnd =   pgd_offset_k(0xffffffff);
+
+	updStart = pgd + pgd_index(KERNELBASE);
+
+	memcpy(updStart, kpdStart, (kpdEnd - kpdStart + 1) * sizeof (pgd_t));
+
+	vmmu_cfg.addr = (VMMU_LEVEL_1_DESC *) pgd;
+	vmmu_cfg.flush_type = VMMU_TLB_FLUSH_ASID;
+	vmmu_cfg.asid = ctx;
+	vmmu_cfg.vmmu_handle = ctx;
+
+	ret_code = vbi_create_vmmu(&vmmu_cfg);
+	if (ret_code) {
+		printk(" Error creating VMMU handles \n");
+	}
+	mm->context.vmmu_handle = vmmu_cfg.vmmu_handle;
+
+	mm->context.id = NO_CONTEXT;
+	mm->context.active = 0;
+
+	return 0;
+}
+
+
+void wrhv_destroy_context(struct mm_struct *mm)
+{
+	unsigned long flags;
+	unsigned int id;
+	static VMMU_CONFIG vmmu_cfg;
+
+	if (mm->context.id == NO_CONTEXT)
+		return;
+
+	vmmu_cfg.addr = -1;
+	vmmu_cfg.flush_type = VMMU_TLB_FLUSH_ASID;
+	vmmu_cfg.asid = mm->context.id;
+	vmmu_cfg.vmmu_handle = mm->context.vmmu_handle;
+
+	WARN_ON(mm->context.active != 0);
+
+	raw_spin_lock_irqsave(&wrhv_context_lock, flags);
+	id = mm->context.id;
+	if (id != NO_CONTEXT) {
+		__clear_bit(id, context_map);
+		mm->context.id = NO_CONTEXT;
+#ifdef DEBUG_MAP_CONSISTENCY
+		mm->context.active = 0;
+#endif
+		context_mm[id] = NULL;
+		nr_free_contexts++;
+	}
+	vbi_delete_vmmu(&vmmu_cfg);
+	raw_spin_unlock_irqrestore(&wrhv_context_lock, flags);
+}
+
+void wrhv_switch_mmu_context(struct mm_struct *prev, struct mm_struct *next)
+{
+	unsigned int i, id, cpu = smp_processor_id();
+	unsigned long *map;
+	static VMMU_CONFIG vmmu_cfg;
+
+	/* No lockless fast path .. yet */
+	raw_spin_lock(&wrhv_context_lock);
+#ifdef DEBUG_MAP_CONSISTENCY
+	printk("[%d] activating context for mm @%p, active=%d, id=%d",
+		cpu, next, next->context.active, next->context.id);
+#endif
+#ifdef CONFIG_SMP
+	/* Mark us active and the previous one not anymore */
+	next->context.active++;
+	if (prev) {
+		printk(" (old=0x%p a=%d)", prev, prev->context.active);
+		WARN_ON(prev->context.active < 1);
+		prev->context.active--;
+	}
+
+ again:
+#endif /* CONFIG_SMP */
+
+	/* If we already have a valid assigned context, skip all that */
+	id = next->context.id;
+	if (likely(id != NO_CONTEXT)) {
+#ifdef DEBUG_MAP_CONSISTENCY
+		if (context_mm[id] != next)
+			printk("MMU: mm 0x%p has id %d but context_mm[%d] says 0x%p\n",
+				next, id, id, context_mm[id]);
+#endif
+		goto ctxt_ok;
+	}
+
+	/* We really don't have a context, let's try to acquire one */
+	id = next_context;
+	if (id > last_context)
+		id = first_context;
+	map = context_map;
+
+	/* No more free contexts, let's try to steal one */
+	if (nr_free_contexts == 0) {
+#ifdef CONFIG_SMP
+		if (num_online_cpus() > 1) {
+			id = steal_context_smp(id);
+			if (id == NO_CONTEXT)
+				goto again;
+			goto stolen;
+		}
+#endif /* CONFIG_SMP */
+		id = wrhv_steal_context_up(id);
+		goto stolen;
+	}
+	nr_free_contexts--;
+
+	/* We know there's at least one free context, try to find it */
+	while (__test_and_set_bit(id, map)) {
+		id = find_next_zero_bit(map, last_context+1, id);
+		if (id > last_context)
+			id = first_context;
+	}
+ stolen:
+	if (id <= first_context)
+		id = first_context;
+	next_context = id + 1;
+	context_mm[id] = next;
+	next->context.id = id;
+#ifdef CONFIG_WRHV_DEBUG
+	printk(" | new id=%d,nrf=%d", id, nr_free_contexts);
+#endif
+
+	context_check_map();
+ ctxt_ok:
+
+	/* If that context got marked stale on this CPU, then flush the
+	 * local TLB for it and unmark it before we use it
+	 */
+	if (test_bit(id, stale_map[cpu])) {
+#ifdef CONFIG_WRHV_DEBUG
+		printk(" | stale flush %d [%d..%d]",
+				id, cpu_first_thread_in_core(cpu),
+				cpu_last_thread_in_core(cpu));
+#endif
+
+		local_flush_tlb_mm(next);
+
+		/* XXX This clear should ultimately be part of local_flush_tlb_mm */
+		for (i = cpu_first_thread_in_core(cpu);
+			i <= cpu_last_thread_in_core(cpu); i++) {
+			__clear_bit(id, stale_map[i]);
+		}
+	}
+
+	/* Flick the MMU and release lock */
+#ifdef CONFIG_WRHV_ASID_OPTIMIZATION
+#ifdef CONFIG_WRHV_DEBUG
+	printk(" -> %d\n", id);
+#endif
+	vb_context_mmu_on(id, next->pgd, PAGE_SIZE, next->context.id,
+		next->context.vmmu_handle, 0);
+#else
+	set_context(id, next->pgd);
+#endif
+	raw_spin_unlock(&wrhv_context_lock);
+}
+
+
+/*
+ * Initialize the context management stuff.
+ */
+void __init wrhv_mmu_context_init(void)
+{
+	/* Mark init_mm as being active on all possible CPUs since
+	 * we'll get called with prev == init_mm the first time
+	 * we schedule on a given CPU
+	 */
+	init_mm.context.active = NR_CPUS;
+
+	/*
+	 *   The MPC8xx has only 16 contexts.  We rotate through them on each
+	 * task switch.  A better way would be to keep track of tasks that
+	 * own contexts, and implement an LRU usage.  That way very active
+	 * tasks don't always have to pay the TLB reload overhead.  The
+	 * kernel pages are mapped shared, so the kernel can run on behalf
+	 * of any task that makes a kernel entry.  Shared does not mean they
+	 * are not protected, just that the ASID comparison is not performed.
+	 *      -- Dan
+	 *
+	 * The IBM4xx has 256 contexts, so we can just rotate through these
+	 * as a way of "switching" contexts.  If the TID of the TLB is zero,
+	 * the PID/TID comparison is disabled, so we can use a TID of zero
+	 * to represent all kernel pages as shared among all contexts.
+	 * 	-- Dan
+	 */
+	first_context = ASID_FIRST_CONTEXT;
+	last_context = ASID_LAST_CONTEXT;
+
+	/*
+	 * Allocate the maps used by context management
+	 */
+	context_map = alloc_bootmem(CTX_MAP_SIZE);
+	context_mm = alloc_bootmem(sizeof(void *) * (last_context + 1));
+	stale_map[0] = alloc_bootmem(CTX_MAP_SIZE);
+
+#ifdef CONFIG_SMP
+	register_cpu_notifier(&mmu_context_cpu_nb);
+#endif
+
+	printk(KERN_INFO
+		"MMU: Allocated %zu bytes of context maps for %d contexts\n",
+		2 * CTX_MAP_SIZE + (sizeof(void *) * (last_context + 1)),
+		last_context - first_context + 1);
+
+	/*
+	 * Some processors have too few contexts to reserve one for
+	 * init_mm, and require using context 0 for a normal task.
+	 * Other processors reserve the use of context zero for the kernel.
+	 * This code assumes first_context < 32.
+	 */
+	context_map[0] = (1 << first_context) - 1;
+	next_context = first_context;
+	nr_free_contexts = last_context - first_context + 1;
+}
+#endif
+
 void wrhv_init(void)
 {
 	/* initialize wr_config so that we can access
@@ -1884,6 +2193,12 @@ void wrhv_init(void)
 
 #ifndef CONFIG_PPC85xx_VT_MODE
 	pv_mmu_ops.vmmu_restore = wrhv_vmmu_restore;
+#ifdef CONFIG_WRHV_ASID_OPTIMIZATION
+	pv_context_ops.init_new_context = wrhv_init_new_context;
+	pv_context_ops.destroy_context = wrhv_destroy_context;
+	pv_context_ops.switch_mmu_context = wrhv_switch_mmu_context;
+	pv_context_ops.mmu_context_init = wrhv_mmu_context_init;
+#endif
 #endif
 	pv_mmu_ops.MMU_init_hw = wrhv_MMU_init_hw;
 	pv_mmu_ops.mmu_mapin_ram = wrhv_mmu_mapin_ram;
@@ -2201,7 +2516,8 @@ int __devinit wrhv_start_secondary(void *unused)
 
 	local_irq_disable();
 #ifndef CONFIG_PPC85xx_VT_MODE
-	vb_context_mmu_on(0, swapper_pg_dir, PAGE_SIZE, 0);
+	vb_context_mmu_on(0, swapper_pg_dir, PAGE_SIZE, KERNEL_BASE_ASID,
+			KERNEL_BASE_ASID, 0);
 #endif
 
 	wrhv_umask_IPIs_for_vcore();
diff --git a/arch/powerpc/kernel/wrhv_entry_32.S b/arch/powerpc/kernel/wrhv_entry_32.S
index 2d33f57..707e6cf 100644
--- a/arch/powerpc/kernel/wrhv_entry_32.S
+++ b/arch/powerpc/kernel/wrhv_entry_32.S
@@ -149,6 +149,18 @@ paravirt_transfer_to_handler_cont:
 	lwz	r11,VB_STATUS_OLD_INT_DISABLE(r12)
 	stw	r11,VB_CONTROL_NEW_INT_DISABLE(r9)
 
+#ifdef CONFIG_WRHV_ASID_OPTIMIZATION
+	/* Push the virtual ASID into the control struct */
+	lwz	r11,VB_STATUS_ASID(r12)
+	stw	r11,VB_CONTROL_ASID(r9)
+
+	/* Update the VMMU handle */
+	lwz	r11,VB_STATUS_VMMU_HANDLE(r12)
+	stw	r11,VB_CONTROL_VMMU_HANDLE(r9)
+
+	lwz	r11,VB_STATUS_VMMU0(r12)
+	stw	r11,VB_CONTROL_VMMU0(r9)
+#endif
 /*
 	lwz	r11,VB_STATUS_CR(r12)
 	stw	r11,VB_CONTROL_CR(r9)
diff --git a/arch/powerpc/mm/mmu_context_nohash.c b/arch/powerpc/mm/mmu_context_nohash.c
index 1f2d9ff..1c02b4c 100644
--- a/arch/powerpc/mm/mmu_context_nohash.c
+++ b/arch/powerpc/mm/mmu_context_nohash.c
@@ -62,7 +62,6 @@ static DEFINE_RAW_SPINLOCK(context_lock);
 #define CTX_MAP_SIZE	\
 	(sizeof(unsigned long) * (last_context / BITS_PER_LONG + 1))
 
-
 /* Steal a context from a task that has one at the moment.
  *
  * This is used when we are running out of available PID numbers
@@ -136,7 +135,7 @@ static unsigned int steal_context_smp(unsigned int id)
  * this to work, we somewhat assume that CPUs that are onlined
  * come up with a fully clean TLB (or are cleaned when offlined)
  */
-static unsigned int steal_context_up(unsigned int id)
+unsigned int steal_context_up(unsigned int id)
 {
 	struct mm_struct *mm;
 	int cpu = smp_processor_id();
@@ -157,6 +156,7 @@ static unsigned int steal_context_up(unsigned int id)
 
 	return id;
 }
+EXPORT_SYMBOL(steal_context_up);
 
 #ifdef DEBUG_MAP_CONSISTENCY
 static void context_check_map(void)
@@ -189,8 +189,15 @@ static void context_check_map(void)
 static void context_check_map(void) { }
 #endif
 
+void paravirt_switch_mmu_context(struct mm_struct *prev, struct mm_struct *next)
+	__attribute__((weak, alias("native_switch_mmu_context")));
 void switch_mmu_context(struct mm_struct *prev, struct mm_struct *next)
 {
+	paravirt_switch_mmu_context(prev, next);
+}
+
+void native_switch_mmu_context(struct mm_struct *prev, struct mm_struct *next)
+{
 	unsigned int i, id, cpu = smp_processor_id();
 	unsigned long *map;
 
@@ -279,14 +286,22 @@ void switch_mmu_context(struct mm_struct *prev, struct mm_struct *next)
 	/* Flick the MMU and release lock */
 	pr_hardcont(" -> %d\n", id);
 	set_context(id, next->pgd);
+
 	raw_spin_unlock(&context_lock);
 }
 
 /*
  * Set up the context for a new address space.
  */
+int paravirt_init_new_context(struct task_struct *t, struct mm_struct *mm)
+	__attribute__((weak, alias("native_init_new_context")));
 int init_new_context(struct task_struct *t, struct mm_struct *mm)
 {
+	paravirt_init_new_context(t, mm);
+}
+
+int native_init_new_context(struct task_struct *t, struct mm_struct *mm)
+{
 	pr_hard("initing context for mm @%p\n", mm);
 
 	mm->context.id = MMU_NO_CONTEXT;
@@ -298,8 +313,15 @@ int init_new_context(struct task_struct *t, struct mm_struct *mm)
 /*
  * We're finished using the context for an address space.
  */
+void paravirt_destroy_context(struct mm_struct *mm)
+	__attribute__((weak, alias("native_destroy_context")));
 void destroy_context(struct mm_struct *mm)
 {
+	paravirt_destroy_context(mm);
+}
+
+void native_destroy_context(struct mm_struct *mm)
+{
 	unsigned long flags;
 	unsigned int id;
 
@@ -372,8 +394,15 @@ static struct notifier_block __cpuinitdata mmu_context_cpu_nb = {
 /*
  * Initialize the context management stuff.
  */
+void paravirt_mmu_context_init(void)
+	__attribute__((weak, alias("native_mmu_context_init")));
 void __init mmu_context_init(void)
 {
+	paravirt_mmu_context_init();
+}
+
+void __init native_mmu_context_init(void)
+{
 	/* Mark init_mm as being active on all possible CPUs since
 	 * we'll get called with prev == init_mm the first time
 	 * we schedule on a given CPU
diff --git a/init/Kconfig.wrhv b/init/Kconfig.wrhv
index 72f60bd..3b37511 100644
--- a/init/Kconfig.wrhv
+++ b/init/Kconfig.wrhv
@@ -28,3 +28,24 @@ config DEBUG_VIRTUAL_IRQS
 	bool "Debug VIOAPIC with software IRQ"
 	default n
 	depends on WRHV
+
+config WRHV_ASID_OPTIMIZATION
+	bool "ASID performance optimization"
+	default n
+	depends on WRHV && E500
+	help
+	  ASID performance optimization.  The cost of invalidating and
+	  re-filling TLB cache's on behalf of the guest is an extremely
+	  expensive operation which results in a performance degradation.
+	  ASIDs enable TLB entries to remain resident during context switches,
+	  avoiding the penalty
+
+
+config WRHV_NUM_ASID
+	int "Number of ASID handles"
+	range 1 63
+	default "62"
+	depends on WRHV_ASID_OPTIMIZATION
+	help
+	   This defines the number of ASID handles used by this specific
+	   virtual board.
-- 
1.7.0.2

