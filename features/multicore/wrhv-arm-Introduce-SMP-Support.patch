From ebc0d505c7e60a8c0ce904444a48d8aeb0257674 Mon Sep 17 00:00:00 2001
From: Jim Somerville <Jim.Somerville@windriver.com>
Date: Tue, 6 Sep 2011 19:49:08 -0400
Subject: [PATCH] wrhv: arm: Introduce SMP Support

Change points:
- make local_fiq_enable/disable nops
- shut off CONFIG_CPU_HAS_ASID which cleans up a number
of the previous hooks.  Continue to supply zero as the asid.
- paravirt the smp routines
- use a separate early stack instead of hijacking an
existing stack in the task control block
- turn on CONFIG_TLS_REG_EMUL and supply it with an undefined
instruction which will trap, so we can emulate it.  The previous
solution here was not SMP friendly
- early loading of initial vmmu is now done in C code instead
of cryptic assembler
- remove global vmmu_cfg variable, use local var instead
- add ipi interrupts, generation and handling
- init the APs
- add per cpu timers.  Each cpu gets their own timer tick.
- set preset_lpj variable so APs don't have to calculate
- add spinlock for vmmu handles, because the hypervisor
is currently difficient with respect to race conditions on
certain internal data structures

Signed-off-by: Weiwei Wang <Weiwei.Wang@windriver.com>
Signed-off-by: Jim Somerville <Jim.Somerville@windriver.com>
---
 arch/arm/include/asm/irqflags.h    |    4 +-
 arch/arm/include/asm/mmu_context.h |    8 +-
 arch/arm/include/asm/paravirt.h    |   13 ++-
 arch/arm/include/asm/wrhv.h        |    8 +-
 arch/arm/kernel/asm-offsets.c      |    1 +
 arch/arm/kernel/entry-armv.S       |   11 ++
 arch/arm/kernel/head.S             |   48 +++----
 arch/arm/kernel/paravirt.c         |   30 ++++
 arch/arm/kernel/setup.c            |    2 -
 arch/arm/kernel/smp.c              |   23 +++-
 arch/arm/kernel/traps.c            |    9 +
 arch/arm/kernel/vbi/syscalls.S     |    4 +-
 arch/arm/kernel/vbi/util.c         |    2 -
 arch/arm/kernel/vbi/wrhv.c         |  292 +++++++++++++++++++++++++++++++++++-
 arch/arm/mm/Kconfig                |    3 +-
 arch/arm/mm/context.c              |   21 +--
 arch/arm/mm/pgd.c                  |    5 +
 arch/arm/mm/proc-macros.S          |    6 +
 arch/arm/mm/proc-v7.S              |    4 +
 arch/arm/mm/tlb-v7.S               |   16 ++
 20 files changed, 443 insertions(+), 67 deletions(-)

diff --git a/arch/arm/include/asm/irqflags.h b/arch/arm/include/asm/irqflags.h
index 852d828..0006e64 100644
--- a/arch/arm/include/asm/irqflags.h
+++ b/arch/arm/include/asm/irqflags.h
@@ -25,6 +25,8 @@
 
 #define raw_local_irq_enable()  __asm__("cpsie if @ __sti" : : : "memory", "cc")
 #define raw_local_irq_disable() __asm__("cpsid if @ __cli" : : : "memory", "cc")
+#define local_fiq_enable()
+#define local_fiq_disable()
 #else
 #define raw_local_irq_save(x)					\
 	({							\
@@ -36,9 +38,9 @@
 
 #define raw_local_irq_enable()  __asm__("cpsie i	@ __sti" : : : "memory", "cc")
 #define raw_local_irq_disable() __asm__("cpsid i	@ __cli" : : : "memory", "cc")
-#endif /* WRHV */
 #define local_fiq_enable()  __asm__("cpsie f	@ __stf" : : : "memory", "cc")
 #define local_fiq_disable() __asm__("cpsid f	@ __clf" : : : "memory", "cc")
+#endif /* WRHV */
 
 #else
 
diff --git a/arch/arm/include/asm/mmu_context.h b/arch/arm/include/asm/mmu_context.h
index a0b3cac..b142846 100644
--- a/arch/arm/include/asm/mmu_context.h
+++ b/arch/arm/include/asm/mmu_context.h
@@ -20,6 +20,10 @@
 #include <asm/proc-fns.h>
 #include <asm-generic/mm_hooks.h>
 
+#ifdef CONFIG_SMP
+DECLARE_PER_CPU(struct mm_struct *, current_mm);
+#endif
+
 void __check_kvm_seq(struct mm_struct *mm);
 
 #ifdef CONFIG_CPU_HAS_ASID
@@ -43,10 +47,6 @@ void __check_kvm_seq(struct mm_struct *mm);
 #define ASID_FIRST_VERSION	(1 << ASID_BITS)
 
 extern unsigned int cpu_last_asid;
-#ifdef CONFIG_SMP
-DECLARE_PER_CPU(struct mm_struct *, current_mm);
-#endif
-
 void __init_new_context(struct task_struct *tsk, struct mm_struct *mm);
 void __new_context(struct mm_struct *mm);
 
diff --git a/arch/arm/include/asm/paravirt.h b/arch/arm/include/asm/paravirt.h
index 85379b8..8c61687 100644
--- a/arch/arm/include/asm/paravirt.h
+++ b/arch/arm/include/asm/paravirt.h
@@ -25,6 +25,15 @@
  */
 
 struct pv_time_ops {
+	void (*percpu_timer_setup)(void);
+};
+
+struct pv_smp_ops {
+	void (*smp_init_cpus)(void);
+	void (*smp_prepare_cpus)(unsigned int max_cpus);
+	void (*smp_cross_call)(const struct cpumask *mask);
+	int (*boot_secondary)(unsigned int cpu, struct task_struct *idle);
+	void (*platform_secondary_init)(unsigned int cpu);
 };
 
 struct pv_cpu_ops {
@@ -41,9 +50,6 @@ struct pv_irq_ops {
 	void (*do_IRQ)(struct pt_regs *regs);
 };
 
-struct pv_apic_ops {
-};
-
 struct pv_mmu_ops {
 	void (*MMU_init)(void);
 	void (*do_switch_mm)(unsigned long pgd_phys, struct mm_struct *mm);
@@ -56,6 +62,7 @@ extern struct pv_time_ops pv_time_ops;
 extern struct pv_cpu_ops pv_cpu_ops;
 extern struct pv_irq_ops pv_irq_ops;
 extern struct pv_mmu_ops pv_mmu_ops;
+extern struct pv_smp_ops pv_smp_ops;
 
 #endif /* CONFIG_PARAVIRT */
 #endif	/* __ARM_ASM_PARAVIRT_H */
diff --git a/arch/arm/include/asm/wrhv.h b/arch/arm/include/asm/wrhv.h
index f3a1e1b..9dff1dc 100644
--- a/arch/arm/include/asm/wrhv.h
+++ b/arch/arm/include/asm/wrhv.h
@@ -17,14 +17,18 @@
 
 #ifdef CONFIG_WRHV
 
+#define WRHV_SUPER_EARLY_STACK_SIZE	512
+
+#define FAKE_READ_TLS_REG_UNDEF_INSTR	0xee1d0f72 /* mrc p15,0,r0,c13,c2,3 */
+
 #ifndef __ASSEMBLY__
 
 #include <vbi/vmmu.h>
 
 extern void wrhv_time_init(void);
-extern VMMU_CONFIG vmmu_cfg;
 extern void wrhv_init_irq(void);
-extern void *wrhv_machine_init_irq;
+extern void (*wrhv_machine_init_irq)(void);
+extern spinlock_t vmmu_handle_lock;
 
 #endif /* __ASSEMBLY__ */
 #endif /* CONFIG_WRHV */
diff --git a/arch/arm/kernel/asm-offsets.c b/arch/arm/kernel/asm-offsets.c
index eda76f3..0f21990 100644
--- a/arch/arm/kernel/asm-offsets.c
+++ b/arch/arm/kernel/asm-offsets.c
@@ -139,6 +139,7 @@ int main(void)
   DEFINE(STAT_SPACE_ABT_SPSR,	offsetof(struct vb_arch_stat_regs,
 					modeSpecificReg[ABT_MODE & 0xf].spsr));
   DEFINE(CTRL_SPACE_PTR,	offsetof(struct vb_config, vb_control));
+  DEFINE(WRHV_COREID,		offsetof(struct vb_config, coreId));
   DEFINE(VMMU_CFG_ADDR,		offsetof(VMMU_CONFIG, addr));
   DEFINE(VMMU_CFG_FLUSH_TYPE,	offsetof(VMMU_CONFIG, flush_type));
   DEFINE(VMMU_CFG_ASID,		offsetof(VMMU_CONFIG, asid));
diff --git a/arch/arm/kernel/entry-armv.S b/arch/arm/kernel/entry-armv.S
index 62da7cc..9161c91 100644
--- a/arch/arm/kernel/entry-armv.S
+++ b/arch/arm/kernel/entry-armv.S
@@ -23,6 +23,10 @@
 #include <asm/unwind.h>
 #include <asm/unistd.h>
 
+#ifdef CONFIG_WRHV
+#include <asm/wrhv.h>
+#endif
+
 #include "entry-header.S"
 
 /*
@@ -1102,8 +1106,15 @@ __kuser_get_tls:				@ 0xffff0fe0
 #if !defined(CONFIG_HAS_TLS_REG) && !defined(CONFIG_TLS_REG_EMUL)
 	ldr	r0, [pc, #(16 - 8)]		@ TLS stored at 0xffff0ff0
 #else
+#ifdef CONFIG_WRHV
+	/* fake an undefined instruction, and let arm_mrc_hook in
+	 * arch/arm/kernel/traps.c handle it
+	 */
+	.word	FAKE_READ_TLS_REG_UNDEF_INSTR
+#else
 	mrc	p15, 0, r0, c13, c0, 3		@ read TLS register
 #endif
+#endif
 	usr_ret	lr
 
 	.rep	5
diff --git a/arch/arm/kernel/head.S b/arch/arm/kernel/head.S
index 0e8a01b..1d69a6b 100644
--- a/arch/arm/kernel/head.S
+++ b/arch/arm/kernel/head.S
@@ -22,6 +22,10 @@
 #include <asm/thread_info.h>
 #include <asm/system.h>
 
+#ifdef CONFIG_WRHV
+#include <asm/wrhv.h>
+#endif
+
 #if (PHYS_OFFSET & 0x001fffff)
 #error "PHYS_OFFSET must be at an even 2MiB boundary!"
 #endif
@@ -80,6 +84,11 @@ ENTRY(stext)
 						@ and irqs disabled
 	mrc	p15, 0, r9, c0, c0		@ get processor id
 #ifdef CONFIG_WRHV
+#ifdef CONFIG_SMP
+	ldr	r1, [r0, #WRHV_COREID]		@ if we're core 0 then keep
+	cmp	r1, #0				@   doing bootup, else we are a
+	bne	secondary_startup		@   secondary processor
+#endif
 	ldr	r1, =wr_config
 	str	r0, [r1]			@ save pointer to config info
 
@@ -164,12 +173,7 @@ __secondary_data:
         .align  2
         .type   __wrhv_data, %object
 __wrhv_data:
-	.long	BSYM(vbi_enable_vmmu) @ r4
-	.long	BSYM(vbi_create_vmmu) @ r5
-	.long	BSYM(vbi_load_ctx)    @ r6
-	.long	BSYM(wr_config)	      @ r7
-	.long	BSYM(vmmu_cfg)	      @ r8
-	.long   BSYM(init_thread_union) + THREAD_START_SP @ sp
+	.long   BSYM(wrhv_super_early_stack) + WRHV_SUPER_EARLY_STACK_SIZE @ sp
 #endif
 
 /*
@@ -201,28 +205,16 @@ __enable_mmu:
 	mcr	p15, 0, r4, c2, c0, 0		@ load page table pointer
 	b	__turn_mmu_on
 #else  /* WRHV - turn on the vmmu */
-	mov	r12, r13
-	mov	r11, r1
-	mov	r10, r0
-	adr	r0, BSYM(__wrhv_data)
-	ldmia	r0, {r4-r8,sp}
-	mov	r0, #0
-	blx	r4		@ call vbi_enable_vmmu
-	ldr	r4, =swapper_pg_dir
-	str	r4, [r8, #VMMU_CFG_ADDR]	@ addr field
-	mov	r4, #0
-	str	r4, [r8, #VMMU_CFG_FLUSH_TYPE]	@ flushtype field
-	str	r4, [r8, #VMMU_CFG_ASID]	@ asid field
-	mov	r0, r8
-	blx	r5		@ call vbi_create_vmmu
-	ldr	r5, [r8, #VMMU_CFG_VMMU_HANDLE]	@ handle field
-	ldr	r7, [r7]	@ get pointer to config space
-	ldr	r7, [r7, #CTRL_SPACE_PTR]	@ get pointer to control space
-	str	r5, [r7, #CTRL_SPACE_VMMUHANDLE] @ handle field
-	str	r4, [r7, #CTRL_SPACE_ASID] @ asid field
-	blx	r6		@ call vbi_load_ctx
-	mov	r1, r11
-	mov	r0, r10
+	@ Must preserve registers r0, r1, r13
+	@ On entry, r4 contains the pointer to the page tables
+	@ r7 must be preserved for secondary processor startup
+	mov	r12, r13			@ r13 = sp, preserve it in reg
+	adr	r3, BSYM(__wrhv_data)
+	ldmia	r3, {sp}
+	stmfd	sp!, {r0, r1, r7, r12}
+	mov	r0, r4
+	bl	wrhv_load_initial_vmmu
+	ldmfd	sp!, {r0, r1, r7, r12}
 	mov	pc, r12
 #endif
 ENDPROC(__enable_mmu)
diff --git a/arch/arm/kernel/paravirt.c b/arch/arm/kernel/paravirt.c
index 7bc5684..81f84ab 100644
--- a/arch/arm/kernel/paravirt.c
+++ b/arch/arm/kernel/paravirt.c
@@ -52,6 +52,9 @@ struct pv_cpu_ops pv_cpu_ops = {
 struct pv_mmu_ops pv_mmu_ops = {
 };
 
+struct pv_smp_ops pv_smp_ops = {
+};
+
 
 /* pv_irq_ops */
 void paravirt_do_IRQ(struct pt_regs *regs)
@@ -91,6 +94,32 @@ inline int paravirt_enabled(void)
 
 void paravirt_smp_init_cpus(void)
 {
+	pv_smp_ops.smp_init_cpus();
+}
+
+void paravirt_smp_prepare_cpus(unsigned int max_cpus)
+{
+	pv_smp_ops.smp_prepare_cpus(max_cpus);
+}
+
+int paravirt_boot_secondary(unsigned int cpu, struct task_struct *idle)
+{
+	return pv_smp_ops.boot_secondary(cpu, idle);
+}
+
+void paravirt_platform_secondary_init(unsigned int cpu)
+{
+	pv_smp_ops.platform_secondary_init(cpu);
+}
+
+void paravirt_smp_cross_call(const struct cpumask *mask)
+{
+	pv_smp_ops.smp_cross_call(mask);
+}
+
+void paravirt_percpu_timer_setup(void)
+{
+	pv_time_ops.percpu_timer_setup();
 }
 
 EXPORT_SYMBOL(pv_info);
@@ -98,3 +127,4 @@ EXPORT_SYMBOL(pv_time_ops);
 EXPORT_SYMBOL(pv_cpu_ops);
 EXPORT_SYMBOL(pv_mmu_ops);
 EXPORT_SYMBOL(pv_irq_ops);
+EXPORT_SYMBOL(pv_smp_ops);
diff --git a/arch/arm/kernel/setup.c b/arch/arm/kernel/setup.c
index 4389de0..65d2b2e 100644
--- a/arch/arm/kernel/setup.c
+++ b/arch/arm/kernel/setup.c
@@ -751,8 +751,6 @@ void __init setup_arch(char **cmdline_p)
 	if (mdesc->fixup)
 		mdesc->fixup(mdesc, NULL, NULL, &meminfo);
 	paravirt_MMU_init();
-
-	init_mm.context.vmmu_handle = vmmu_cfg.vmmu_handle;
 #endif
 
 	init_mm.start_code = (unsigned long) _text;
diff --git a/arch/arm/kernel/smp.c b/arch/arm/kernel/smp.c
index e4722a2..f28d3b2 100644
--- a/arch/arm/kernel/smp.c
+++ b/arch/arm/kernel/smp.c
@@ -39,6 +39,8 @@
 #include <asm/localtimer.h>
 #include <asm/smp_plat.h>
 
+extern void paravirt_percpu_timer_setup(void);
+
 /*
  * as from 2.5, kernels no longer have an init_tasks structure
  * so we need some other way of telling a new secondary core
@@ -72,8 +74,10 @@ int __cpuinit __cpu_up(unsigned int cpu)
 {
 	struct cpuinfo_arm *ci = &per_cpu(cpu_data, cpu);
 	struct task_struct *idle = ci->idle;
+#ifndef CONFIG_WRHV
 	pgd_t *pgd;
 	pmd_t *pmd;
+#endif
 	int ret;
 
 	/*
@@ -95,6 +99,7 @@ int __cpuinit __cpu_up(unsigned int cpu)
 		init_idle(idle, cpu);
 	}
 
+#ifndef CONFIG_WRHV
 	/*
 	 * Allocate initial page tables to allow the new CPU to
 	 * enable the MMU safely.  This essentially means a set
@@ -107,13 +112,18 @@ int __cpuinit __cpu_up(unsigned int cpu)
 		     PMD_TYPE_SECT | PMD_SECT_AP_WRITE);
 	flush_pmd_entry(pmd);
 	outer_clean_range(__pa(pmd), __pa(pmd + 1));
+#endif
 
 	/*
 	 * We need to tell the secondary core where to find
 	 * its stack and the page tables.
 	 */
 	secondary_data.stack = task_stack_page(idle) + THREAD_START_SP;
+#ifdef CONFIG_WRHV
+	secondary_data.pgdir = init_mm.context.vmmu_handle;
+#else
 	secondary_data.pgdir = virt_to_phys(pgd);
+#endif
 	__cpuc_flush_dcache_area(&secondary_data, sizeof(secondary_data));
 	outer_clean_range(__pa(&secondary_data), __pa(&secondary_data + 1));
 
@@ -144,9 +154,11 @@ int __cpuinit __cpu_up(unsigned int cpu)
 	secondary_data.stack = NULL;
 	secondary_data.pgdir = 0;
 
+#ifndef CONFIG_WRHV
 	*pmd = __pmd(0);
 	clean_pmd_entry(pmd);
 	pgd_free(&init_mm, pgd);
+#endif
 
 	if (ret) {
 		printk(KERN_CRIT "CPU%u: processor failed to boot\n", cpu);
@@ -464,7 +476,7 @@ static void local_timer_setup(struct clock_event_device *evt)
 }
 #endif
 
-void __cpuinit percpu_timer_setup(void)
+void __cpuinit native_percpu_timer_setup(void)
 {
 	unsigned int cpu = smp_processor_id();
 	struct clock_event_device *evt = &per_cpu(percpu_clockevent, cpu);
@@ -474,6 +486,15 @@ void __cpuinit percpu_timer_setup(void)
 	local_timer_setup(evt);
 }
 
+void __cpuinit percpu_timer_setup(void)
+{
+#ifdef CONFIG_PARAVIRT
+	paravirt_percpu_timer_setup();
+#else
+	native_percpu_timer_setup();
+#endif
+}
+
 static DEFINE_SPINLOCK(stop_lock);
 
 /*
diff --git a/arch/arm/kernel/traps.c b/arch/arm/kernel/traps.c
index 561435e..b398b40 100644
--- a/arch/arm/kernel/traps.c
+++ b/arch/arm/kernel/traps.c
@@ -32,6 +32,10 @@
 #include <asm/traps.h>
 #include <asm/unwind.h>
 
+#ifdef CONFIG_WRHV
+#include <asm/wrhv.h>
+#endif
+
 #include "ptrace.h"
 #include "signal.h"
 
@@ -652,8 +656,13 @@ static int get_tp_trap(struct pt_regs *regs, unsigned int instr)
 }
 
 static struct undef_hook arm_mrc_hook = {
+#ifdef CONFIG_WRHV
+	.instr_mask	= 0xffffffff,
+	.instr_val	= FAKE_READ_TLS_REG_UNDEF_INSTR,
+#else
 	.instr_mask	= 0x0fff0fff,
 	.instr_val	= 0x0e1d0f70,
+#endif
 	.cpsr_mask	= PSR_T_BIT,
 	.cpsr_val	= 0,
 	.fn		= get_tp_trap,
diff --git a/arch/arm/kernel/vbi/syscalls.S b/arch/arm/kernel/vbi/syscalls.S
index 95d2fab..36200f0 100644
--- a/arch/arm/kernel/vbi/syscalls.S
+++ b/arch/arm/kernel/vbi/syscalls.S
@@ -491,7 +491,7 @@ FUNC_END(vbiVbMgmt)
 *
 */
 
-FUNC_LABEL(vbiKputs)
+FUNC_LABEL(vbi_kputs)
         /*
          * r0 - char * s
          */
@@ -500,7 +500,7 @@ FUNC_LABEL(vbiKputs)
         HCALL
         RESTOREREGS
         mov     pc, lr
-FUNC_END(vbiKputs)
+FUNC_END(vbi_kputs)
 
 /******************************************************************************
 *
diff --git a/arch/arm/kernel/vbi/util.c b/arch/arm/kernel/vbi/util.c
index c44f5f7..9fe2b19 100644
--- a/arch/arm/kernel/vbi/util.c
+++ b/arch/arm/kernel/vbi/util.c
@@ -25,8 +25,6 @@ extern struct vb_config *wr_config;
 extern struct vb_status *wr_status;
 extern struct vb_control *wr_control;
 
-VMMU_CONFIG vmmu_cfg;
-
 /*
  * vb_memsize_get should not be called before wr_config is initialized
  */
diff --git a/arch/arm/kernel/vbi/wrhv.c b/arch/arm/kernel/vbi/wrhv.c
index 570a122..2d5b4b1 100644
--- a/arch/arm/kernel/vbi/wrhv.c
+++ b/arch/arm/kernel/vbi/wrhv.c
@@ -21,19 +21,28 @@
 #include <linux/mm.h>
 #include <linux/stddef.h>
 #include <linux/clockchips.h>
+#include <linux/kernel_stat.h>
 
 #include <asm/setup.h>
 #include <asm/irq.h>
 #include <asm/paravirt.h>
+#include <asm/smp_scu.h>
+#include <asm/wrhv.h>
 
 #include <trace/irq.h>
 
 #include <vbi/vbi.h>
 #include <vbi/interface.h>
+#include <vbi/vmmu.h>
 
 #define WRHV_BOOTARG_BUF_SIZE	256
 
+#define IPI_IRQ_BASE_NAME	"ipi"
+
+#define TIMERTICK_IRQ		0
+
 DEFINE_PER_CPU(struct clock_event_device, wrhv_clock_events);
+spinlock_t vmmu_handle_lock;
 
 /* wr_config is set super early to a pointer passed from the hv */
 struct vb_config *wr_config = (void *)(-1); /* keep it out of the bss */
@@ -86,6 +95,43 @@ unsigned long __init wrhv_find_end_of_memory(void)
 	return wr_config->phys_mem_size;
 }
 
+char wrhv_super_early_stack[WRHV_SUPER_EARLY_STACK_SIZE];
+
+void wrhv_load_initial_vmmu(uint32_t pgtbl)
+{
+	/* This is called super early.  We have a stack, and wr_config
+	 * has been set, but that's about it.  If the core we're starting
+	 * is anything other than the bp (core0), then pgtbl represents
+	 * the vmmu handle which was already created.
+	 */
+
+	VMMU_CONFIG	vmmu_cfg;
+
+	vbi_enable_vmmu(0);
+
+	if (wr_config->coreId == 0) {
+		/* Initial startup of the boot core */
+		vmmu_cfg.addr = pgtbl;
+		vmmu_cfg.flush_type = 0;
+		vmmu_cfg.asid = 0;
+		if (vbi_create_vmmu(&vmmu_cfg) != 0) {
+			printk(KERN_ERR "Could not create initial vmmu!\n");
+			while (1) {}; /* No point in continuing */
+		
+		} else {
+			wr_config->vb_control->vb_control_regs.vmmu_handle =
+				vmmu_cfg.vmmu_handle;
+			init_mm.context.vmmu_handle = vmmu_cfg.vmmu_handle;
+		}
+	} else
+		wr_config->vb_control->vb_control_regs.vmmu_handle = pgtbl;
+	wr_config->vb_control->vb_control_regs.asid = 0;
+	if (vbi_load_ctx() != 0) {
+		printk(KERN_ERR "Could not load initial context!\n");
+		while (1) {}; /* No point in continuing */
+	}
+}
+
 static void wrhv_do_restart(void *data)
 {
 	int ret;
@@ -116,6 +162,203 @@ void wrhv_restart(char str, const char *cmd)
 	while (1);
 }
 
+#ifdef CONFIG_SMP
+static irqreturn_t wrhv_ipi_interrupt(int irq, void *dev_id)
+{
+	do_IPI(get_irq_regs());
+	return IRQ_HANDLED;
+}
+
+int ipi_irq = VBI_INVALID_IRQ;	/* Make sure it gets set before use */
+
+void wrhv_unmask_IPIs_for_vcore(void)
+{
+	printk(KERN_INFO "CPU%d: Unmasking ipi %d\n", smp_processor_id(),
+		ipi_irq);
+	vbi_unmask_vioapic_irq(ipi_irq);
+}
+
+int wrhv_request_ipis(void)
+{
+	static char *ipi_names[] = {
+		"IPI (all ipi functions)",
+	};
+
+	int err;
+
+	ipi_irq = vbi_find_irq(IPI_IRQ_BASE_NAME, VB_INPUT_INT);
+	if (ipi_irq == VBI_INVALID_IRQ) {
+		printk(KERN_ERR "WRHV lookup of interrupt name '"
+				IPI_IRQ_BASE_NAME
+				"' failed!\n");
+		panic("WRHV resolve irq for IPI failed\n");
+	}
+
+	set_irq_chip_and_handler_name(ipi_irq, &wrhv_ipi_irq_chip,
+				      handle_percpu_irq, "per_cpu");
+	err = request_irq(ipi_irq, wrhv_ipi_interrupt,
+			  IRQF_DISABLED | IRQF_NOBALANCING,
+			  ipi_names[0], wrhv_ipi_interrupt);
+	if (err) {
+		printk(KERN_ERR "WRHV request of irq %d for IPI(%s) failed\n",
+		       ipi_irq, ipi_names[0]);
+	} else
+		wrhv_unmask_IPIs_for_vcore(); /* Allow the BP to receive them */
+	return err;
+}
+#endif
+
+void wrhv_smp_init_cpus(void)
+{
+	unsigned int i, ncores;
+
+	/* Ask the vbi how many cores we have */
+	ncores = VBI_VCORES_COUNT_GET();
+
+	/* Check if we have more available than configured to support */
+	if (ncores > NR_CPUS) {
+		printk(KERN_WARNING
+		       "wrhv: no. of cores (%d) greater than configured "
+		       "maximum of %d - clipping\n",
+		       ncores, NR_CPUS);
+		ncores = NR_CPUS;
+	}
+
+	for (i = 0; i < ncores; i++)
+		set_cpu_possible(i, true);
+}
+
+void wrhv_smp_prepare_cpus(unsigned int max_cpus)
+{
+	unsigned int ncores = num_possible_cpus();
+	unsigned int cpu = smp_processor_id();
+	int i;
+
+	smp_store_cpu_info(cpu);
+
+	/*
+	 * are we trying to boot more cores than exist?
+	 */
+	if (max_cpus > ncores)
+		max_cpus = ncores;
+
+	if (max_cpus > 1)
+		if (wrhv_request_ipis()) {
+			printk(KERN_ERR "IPI init issue, continuing in UP\n");
+			return;		/* something went wrong */
+		}
+
+	/*
+	 * Initialize the present map, which describes the set of CPUs
+	 * actually populated at the present time.
+	 */
+	for (i = 0; i < max_cpus; i++)
+		set_cpu_present(i, true);
+}
+
+static DEFINE_SPINLOCK(boot_lock);
+
+extern volatile int pen_release;
+int __cpuinit wrhv_boot_secondary(unsigned int cpu, struct task_struct *idle)
+{
+	int ret;
+	unsigned long timeout;
+
+	/*
+	 * Set synchronisation state between this boot processor
+	 * and the secondary one
+	 */
+	spin_lock(&boot_lock);
+
+	pen_release = cpu;
+	ret = vbi_vb_resume(VBI_BOARD_ID_GET(), cpu);
+
+	timeout = jiffies + (1 * HZ);
+	while (time_before(jiffies, timeout)) {
+		smp_rmb();
+		if (pen_release == -1)
+			break;
+
+		udelay(10);
+	}
+
+	/*
+	 * now the secondary core is starting up let it run its
+	 * calibrations, then wait for it to finish
+	 */
+	spin_unlock(&boot_lock);
+
+	if (pen_release != -1)
+		return -ENOSYS;
+
+
+	return ret;
+}
+
+extern void __iomem *gic_cpu_base_addr;
+void __cpuinit wrhv_platform_secondary_init(unsigned int cpu)
+{
+	trace_hardirqs_off();
+
+	vbi_set_exc_base((void *)CONFIG_VECTORS_BASE);
+
+	gic_cpu_init(0, gic_cpu_base_addr);
+
+	wrhv_unmask_IPIs_for_vcore();
+
+	/*
+	 * let the primary processor know we're out of the
+	 * pen, then head off into the C entry point
+	 */
+	pen_release = -1;
+	smp_wmb();
+
+	/*
+	 * Synchronise with the boot thread.
+	 */
+	spin_lock(&boot_lock);
+	spin_unlock(&boot_lock);
+}
+
+irqreturn_t wrhv_timer_interrupt(int irq, void *dev_id)
+{
+	u64 ticks;
+	static DEFINE_PER_CPU(u64, mark_offset);
+	static DEFINE_PER_CPU(int, mark_first_time) = 1;
+	int cpu = smp_processor_id();
+	struct clock_event_device *evt = &per_cpu(wrhv_clock_events, cpu);
+
+	if (!evt->event_handler) {
+		printk(KERN_WARNING
+			   "Spurious Hyp timer interrupt on cpu %d\n", cpu);
+		return IRQ_NONE;
+	}
+
+	if (__get_cpu_var(mark_first_time) == 0) {
+		ticks = wr_vb_status->tick_count;
+		ticks -= __get_cpu_var(mark_offset);
+		__get_cpu_var(mark_offset) = wr_vb_status->tick_count;
+		if (ticks > (2*HZ)) {
+			printk(KERN_DEBUG "Time falling behind %lld jiffies\n",
+				ticks);
+			ticks = 1;
+		}
+	} else {
+		ticks = 1;
+		__get_cpu_var(mark_first_time) = 0;
+		__get_cpu_var(mark_offset) = wr_vb_status->tick_count;
+	}
+
+	if (ticks > 1)
+		account_steal_time(jiffies_to_cputime(ticks - 1));
+
+	while (ticks != 0) {
+		evt->event_handler(evt);
+		ticks--;
+	}
+
+	return IRQ_HANDLED;
+}
 static struct irqaction wrhv_timer_irq = {
 	.handler = wrhv_timer_interrupt,
 	.flags = IRQF_DISABLED | IRQF_NOBALANCING,
@@ -205,7 +448,7 @@ static struct clock_event_device wrhv_clockevent = {
 	.min_delta_ns	= 10000,
 	.shift		= 32,   /* nanoseconds to cycles divisor 2^ */
 	.mult		= 1,     /* To be filled in */
-	.irq		= 0,
+	.irq		= TIMERTICK_IRQ,
 	.rating		= 1,
 };
 
@@ -218,7 +461,24 @@ void __init wrhv_time_init(void)
 	evt->cpumask = cpumask_of(0);
 
 	clockevents_register_device(evt);
-	setup_irq(0, &wrhv_timer_irq);
+	setup_irq(TIMERTICK_IRQ, &wrhv_timer_irq);
+	vbi_unmask_vioapic_irq(TIMERTICK_IRQ); /* Allow timer ints through */
+}
+
+void __devinit wrhv_setup_secondary_clock(void)
+{
+	int cpu;
+	struct clock_event_device *evt;
+	cpu = smp_processor_id();
+	printk(KERN_INFO "installing wrhv timer for CPU %d\n", cpu);
+
+	evt = &per_cpu(wrhv_clock_events, cpu);
+	memcpy(evt, &wrhv_clockevent, sizeof(*evt));
+	evt->cpumask = cpumask_of(cpu);
+
+	clockevents_register_device(evt);
+
+	vbi_unmask_vioapic_irq(TIMERTICK_IRQ); /* Allow timer ints through */
 }
 
 pgd_t *wrhv_cpu_get_pgd(void)
@@ -265,12 +525,25 @@ void wrhv_do_switch_mm(unsigned long pgd_phys, struct mm_struct *mm)
 {
 	wr_control->vb_control_regs.vmmu_handle = mm->context.vmmu_handle;
 	wr_control->vb_control_regs.asid = 0;
-	vbi_load_ctx();
+	if (vbi_load_ctx() != 0)
+		printk(KERN_WARNING "Bad vmmu handle %lu\n",
+				mm->context.vmmu_handle);
 	/* Flush everything for now, until we support asids.
 	 */
 	vbi_flush_tlb(0, 0, -1);
 }
 
+void wrhv_smp_cross_call(const struct cpumask *mask)
+{
+	unsigned long coreset = cpus_addr(*mask)[0];
+	unsigned long flags;
+
+	local_irq_save(flags);
+	WARN_ON(coreset & ~cpus_addr(cpu_online_map)[0]);
+	vbi_send_vcore_vioapic_irq(ipi_irq, coreset, VBI_IOAPICSEND_VCORE_NONE);
+	local_irq_restore(flags);
+}
+
 void wrhv_calculate_clock_freq(void)
 {
 	u64 lpj;
@@ -281,13 +554,13 @@ void wrhv_calculate_clock_freq(void)
 
 	lpj = ((u64)cpu_khz * 1000);
 	do_div(lpj, HZ);
-	lpj_fine = lpj;
+	preset_lpj = lpj;
 
 	printk(KERN_INFO "Detected %lu.%03lu MHz processor.\n", cpu_khz / 1000,
 		cpu_khz % 1000);
 }
 
-void wrhv_init(void)
+void __init wrhv_init(void)
 {
 	/* wr_config was already set, super early */
 	vbi_init(wr_config);
@@ -303,11 +576,19 @@ void wrhv_init(void)
 
 	pv_cpu_ops.do_idle = wrhv_do_idle;
 
+	pv_smp_ops.smp_init_cpus = wrhv_smp_init_cpus;
+	pv_smp_ops.smp_prepare_cpus = wrhv_smp_prepare_cpus;
+	pv_smp_ops.smp_cross_call = wrhv_smp_cross_call;
+	pv_smp_ops.boot_secondary = wrhv_boot_secondary;
+	pv_smp_ops.platform_secondary_init = wrhv_platform_secondary_init;
+
 	pv_mmu_ops.MMU_init = wrhv_MMU_init;
 	pv_mmu_ops.do_switch_mm = wrhv_do_switch_mm;
 	pv_mmu_ops.set_pte_ext = wrhv_set_pte_ext;
 	pv_mmu_ops.cpu_get_pgd = wrhv_cpu_get_pgd;
 
+	pv_time_ops.percpu_timer_setup = wrhv_setup_secondary_clock;
+
 	snprintf(boot_command_line, COMMAND_LINE_SIZE,
 		"retain_initrd %s",
 	wr_config->bootLine);
@@ -315,4 +596,5 @@ void wrhv_init(void)
 	wrhv_calculate_clock_freq();
 
 	arm_pm_restart = wrhv_restart;
+	spin_lock_init(&vmmu_handle_lock);
 }
diff --git a/arch/arm/mm/Kconfig b/arch/arm/mm/Kconfig
index d64539e..3c93d22 100644
--- a/arch/arm/mm/Kconfig
+++ b/arch/arm/mm/Kconfig
@@ -417,7 +417,7 @@ config CPU_V7
 	select CPU_CACHE_V7
 	select CPU_CACHE_VIPT
 	select CPU_CP15_MMU
-	select CPU_HAS_ASID if MMU
+	select CPU_HAS_ASID if MMU && !WRHV
 	select CPU_COPY_V6 if MMU
 	select CPU_TLB_V7 if MMU
 
@@ -449,6 +449,7 @@ config CPU_32v6
 
 config CPU_32v7
 	bool
+	select TLS_REG_EMUL if SMP && WRHV
 
 # The abort model
 config CPU_ABRT_NOMMU
diff --git a/arch/arm/mm/context.c b/arch/arm/mm/context.c
index fc750c9..bcf70c2 100644
--- a/arch/arm/mm/context.c
+++ b/arch/arm/mm/context.c
@@ -16,12 +16,14 @@
 #include <asm/mmu_context.h>
 #include <asm/tlbflush.h>
 
-static DEFINE_SPINLOCK(cpu_asid_lock);
-unsigned int cpu_last_asid = ASID_FIRST_VERSION;
 #ifdef CONFIG_SMP
 DEFINE_PER_CPU(struct mm_struct *, current_mm);
 #endif
 
+#ifdef CONFIG_CPU_HAS_ASID
+static DEFINE_SPINLOCK(cpu_asid_lock);
+unsigned int cpu_last_asid = ASID_FIRST_VERSION;
+
 /*
  * We fork()ed a process, and we need a new context for the child
  * to run in.  We reserve version 0 for initial tasks so we will
@@ -34,7 +36,6 @@ void __init_new_context(struct task_struct *tsk, struct mm_struct *mm)
 	spin_lock_init(&mm->context.id_lock);
 }
 
-#ifndef CONFIG_WRHV
 static void flush_context(void)
 {
 	/* set the reserved ASID before flushing the TLB */
@@ -46,7 +47,6 @@ static void flush_context(void)
 		dsb();
 	}
 }
-#endif
 
 #ifdef CONFIG_SMP
 
@@ -54,9 +54,6 @@ static void set_mm_context(struct mm_struct *mm, unsigned int asid)
 {
 	unsigned long flags;
 
-#ifdef CONFIG_WRHV
-	asid = 0;
-#endif
 	/*
 	 * Locking needed for multi-threaded applications where the
 	 * same mm->context.id could be set from different CPUs during
@@ -80,7 +77,6 @@ static void set_mm_context(struct mm_struct *mm, unsigned int asid)
 	cpumask_set_cpu(smp_processor_id(), mm_cpumask(mm));
 }
 
-#ifndef CONFIG_WRHV
 /*
  * Reset the ASID on the current CPU. This function call is broadcast
  * from the CPU handling the ASID rollover and holding cpu_asid_lock.
@@ -108,15 +104,11 @@ static void reset_context(void *info)
 	asm("mcr	p15, 0, %0, c13, c0, 1\n" : : "r" (mm->context.id));
 	isb();
 }
-#endif
 
 #else
 
 static inline void set_mm_context(struct mm_struct *mm, unsigned int asid)
 {
-#ifdef CONFIG_WRHV
-	asid = 0;
-#endif
 	mm->context.id = asid;
 	cpumask_copy(mm_cpumask(mm), cpumask_of(smp_processor_id()));
 }
@@ -128,7 +120,6 @@ void __new_context(struct mm_struct *mm)
 	unsigned int asid;
 
 	spin_lock(&cpu_asid_lock);
-#ifndef CONFIG_WRHV
 #ifdef CONFIG_SMP
 	/*
 	 * Check the ASID again, in case the change was broadcast from
@@ -163,9 +154,7 @@ void __new_context(struct mm_struct *mm)
 		cpu_last_asid += NR_CPUS;
 	}
 
-#else
-	asid = 0;
-#endif
 	set_mm_context(mm, asid);
 	spin_unlock(&cpu_asid_lock);
 }
+#endif
diff --git a/arch/arm/mm/pgd.c b/arch/arm/mm/pgd.c
index 57ae249..a0fcf11 100644
--- a/arch/arm/mm/pgd.c
+++ b/arch/arm/mm/pgd.c
@@ -18,6 +18,7 @@
 #include "mm.h"
 
 #ifdef CONFIG_WRHV
+#include <asm/wrhv.h>
 #include <vbi/vmmu.h>
 #include <vbi/syscall.h>
 #endif
@@ -79,11 +80,13 @@ pgd_t *get_pgd_slow(struct mm_struct *mm)
 		vmmu_cfg.addr = (uint32_t)new_pgd;
 		vmmu_cfg.asid = 0;
 		vmmu_cfg.flush_type = 0;
+		spin_lock(&vmmu_handle_lock);
 		if (vbi_create_vmmu(&vmmu_cfg) != 0) {
 			printk(KERN_ERR "WRHV: Error creating vmmu!\n");
 			goto no_pmd;
 		} else
 			mm->context.vmmu_handle = vmmu_cfg.vmmu_handle;
+		spin_unlock(&vmmu_handle_lock);
 	}
 #endif
 
@@ -128,8 +131,10 @@ free:
 		VMMU_CONFIG vmmu_cfg;
 
 		vmmu_cfg.vmmu_handle = mm->context.vmmu_handle;
+		spin_lock(&vmmu_handle_lock);
 		if (vbi_delete_vmmu(&vmmu_cfg) != 0)
 			printk(KERN_ERR "WRHV: Error deleting vmmu!\n");
+		spin_unlock(&vmmu_handle_lock);
 	}
 #endif
 }
diff --git a/arch/arm/mm/proc-macros.S b/arch/arm/mm/proc-macros.S
index 7d63bea..92b1e5d 100644
--- a/arch/arm/mm/proc-macros.S
+++ b/arch/arm/mm/proc-macros.S
@@ -39,9 +39,15 @@
 /*
  * mmid - get context id from mm pointer (mm->context.id)
  */
+#ifdef CONFIG_CPU_HAS_ASID
 	.macro	mmid, rd, rn
 	ldr	\rd, [\rn, #MM_CONTEXT_ID]
 	.endm
+#else
+	.macro	mmid, rd, rn
+	mov	\rd, #0
+	.endm
+#endif
 
 /*
  * mask_asid - mask the ASID from the context ID
diff --git a/arch/arm/mm/proc-v7.S b/arch/arm/mm/proc-v7.S
index a5aff9d..1a5f13e 100644
--- a/arch/arm/mm/proc-v7.S
+++ b/arch/arm/mm/proc-v7.S
@@ -107,7 +107,11 @@ ENDPROC(cpu_v7_dcache_clean_area)
 ENTRY(cpu_v7_switch_mm)
 #ifdef CONFIG_MMU
 	mov	r2, #0
+#ifdef CONFIG_CPU_HAS_ASID
 	ldr	r1, [r1, #MM_CONTEXT_ID]	@ get mm->context.id
+#else
+	mov	r1, #0
+#endif
 	orr	r0, r0, #TTB_FLAGS
 #ifdef CONFIG_ARM_ERRATA_430973
 	mcr	p15, 0, r2, c7, c5, 6		@ flush BTAC/BTB
diff --git a/arch/arm/mm/tlb-v7.S b/arch/arm/mm/tlb-v7.S
index f3f288a..c645d0f 100644
--- a/arch/arm/mm/tlb-v7.S
+++ b/arch/arm/mm/tlb-v7.S
@@ -56,6 +56,14 @@ ENTRY(v7wbi_flush_user_tlb_range)
 	mcr	p15, 0, ip, c7, c5, 6		@ flush BTAC/BTB
 #endif
 	dsb
+#ifdef CONFIG_WRHV
+	stmfd	sp!, {r0-r2, lr}
+	mov	r0, #0
+	mov	r1, #0
+	mov	r2, #-1
+	bl	vbi_flush_tlb
+	ldmfd	sp!, {r0-r2, lr}
+#endif
 	mov	pc, lr
 ENDPROC(v7wbi_flush_user_tlb_range)
 
@@ -90,6 +98,14 @@ ENTRY(v7wbi_flush_kern_tlb_range)
 #endif
 	dsb
 	isb
+#ifdef CONFIG_WRHV
+	stmfd	sp!, {r0-r2, lr}
+	mov	r0, #0
+	mov	r1, #0
+	mov	r2, #-1
+	bl	vbi_flush_tlb
+	ldmfd	sp!, {r0-r2, lr}
+#endif
 	mov	pc, lr
 ENDPROC(v7wbi_flush_kern_tlb_range)
 
-- 
1.7.0.2

