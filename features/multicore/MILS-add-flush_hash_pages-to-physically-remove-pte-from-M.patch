From 64d2a5785044199c343da1ed37da9a77d3fea9e5 Mon Sep 17 00:00:00 2001
From: Thomas Tai <thomas.tai@windriver.com>
Date: Fri, 14 May 2010 10:45:19 -0400
Subject: [PATCH] add flush_hash_pages to physically remove pte from MMU

604 use flush_hash_pages to remove pte from MMU. Although
VMMU has E500 like pte format but it really a 604 MMU.
Correct pte flushing function to call flush_hash_pages
which in term call hypervisor's to physically remove
pte from the MMU.

All functions are #ifdef such that it only affect MILS
and 604 cpu.

Signed-off-by: Thomas Tai <thomas.tai@windriver.com>
---
 arch/powerpc/include/asm/tlb.h |    7 +++++++
 arch/powerpc/kernel/vbi/wrhv.c |   18 ++++++++++++++++++
 arch/powerpc/mm/hash_low_32.S  |    5 +++++
 arch/powerpc/mm/tlb_32.c       |   25 ++++++++++++++++++++++---
 4 files changed, 52 insertions(+), 3 deletions(-)

diff --git a/arch/powerpc/include/asm/tlb.h b/arch/powerpc/include/asm/tlb.h
index e20ff75..8cc57cb 100644
--- a/arch/powerpc/include/asm/tlb.h
+++ b/arch/powerpc/include/asm/tlb.h
@@ -72,8 +72,15 @@ extern void flush_hash_entry(struct mm_struct *mm, pte_t *ptep,
 static inline void __tlb_remove_tlb_entry(struct mmu_gather *tlb, pte_t *ptep,
 					unsigned long address)
 {
+#if ((defined CONFIG_WRHV_MILS) && (defined CONFIG_6xx))
+	/* hypervisor do not update _PAGE_HASHPTE field,
+	 * need to flush the entry unconditionally
+	 */
+	flush_hash_entry(tlb->mm, ptep, address);
+#else
 	if (pte_val(*ptep) & _PAGE_HASHPTE)
 		flush_hash_entry(tlb->mm, ptep, address);
+#endif
 }
 
 #endif
diff --git a/arch/powerpc/kernel/vbi/wrhv.c b/arch/powerpc/kernel/vbi/wrhv.c
index cdb9669..25e9e93 100644
--- a/arch/powerpc/kernel/vbi/wrhv.c
+++ b/arch/powerpc/kernel/vbi/wrhv.c
@@ -148,6 +148,24 @@ int wrhv_pci_devfn = -1;
 char wrhv_macaddr[6];
 
 #ifdef CONFIG_WRHV_MILS
+
+#ifdef CONFIG_6xx
+/* 604 use flush_hash_pages() to physically remove pte from the hardware MMU,
+ * for hypervisor, we need to use flush_HPTE() to do the same action.
+ */
+int flush_hash_pages(unsigned context, unsigned long va, unsigned long pmdval,
+			int count)
+{
+	while(count){
+		flush_HPTE(0, va, pmdval);
+		va+=0x1000;/* advance to next page */
+		count--;
+	}
+	return 0;
+
+}
+#endif
+
 static inline uint32_t get_emsr(void)
 {
 	return 0;
diff --git a/arch/powerpc/mm/hash_low_32.S b/arch/powerpc/mm/hash_low_32.S
index 2a23e27..3b0ca81 100644
--- a/arch/powerpc/mm/hash_low_32.S
+++ b/arch/powerpc/mm/hash_low_32.S
@@ -483,7 +483,12 @@ htab_hash_searches:
  *
  * We assume that there is a hash table in use (Hash != 0).
  */
+#if ((defined CONFIG_WRHV_MILS) && (defined CONFIG_6xx))
+/* refer to wrhv.c for paravirtualized version of flush_hash_pages() */
+_GLOBAL(native_flush_hash_pages)
+#else
 _GLOBAL(flush_hash_pages)
+#endif
 	tophys(r7,0)
 
 	/*
diff --git a/arch/powerpc/mm/tlb_32.c b/arch/powerpc/mm/tlb_32.c
index eb4b512..1e03ca4 100644
--- a/arch/powerpc/mm/tlb_32.c
+++ b/arch/powerpc/mm/tlb_32.c
@@ -39,11 +39,19 @@
 void flush_hash_entry(struct mm_struct *mm, pte_t *ptep, unsigned long addr)
 {
 	unsigned long ptephys;
-
+#if ((defined CONFIG_WRHV_MILS) && (defined CONFIG_6xx))
+	/* hypervisor VMMU is using hash table, so need to flush.
+	   For native linux, the variable Hash is initialized during mmu init.
+	   For hypervisor, the variable Hash is not initialized.
+	 */
+	ptephys = __pa(ptep) & PAGE_MASK;
+	flush_hash_pages(mm->context.id, addr, ptephys, 1);
+#else
 	if (Hash != 0) {
 		ptephys = __pa(ptep) & PAGE_MASK;
 		flush_hash_pages(mm->context.id, addr, ptephys, 1);
 	}
+#endif
 }
 
 /*
@@ -104,10 +112,15 @@ static void flush_range(struct mm_struct *mm, unsigned long start,
 	int count;
 	unsigned int ctx = mm->context.id;
 
+#if ((defined CONFIG_WRHV_MILS) && (defined CONFIG_6xx))
+	/* do not return here, need to physically flush pte */
+#else
 	if (Hash == 0) {
 		_tlbia();
 		return;
 	}
+#endif
+
 	start &= PAGE_MASK;
 	if (start >= end)
 		return;
@@ -143,11 +156,14 @@ void flush_tlb_kernel_range(unsigned long start, unsigned long end)
 void flush_tlb_mm(struct mm_struct *mm)
 {
 	struct vm_area_struct *mp;
-
+#if ((defined CONFIG_WRHV_MILS) && (defined CONFIG_6xx))
+	/* do not return here, need to physically flush pte */
+#else
 	if (Hash == 0) {
 		_tlbia();
 		return;
 	}
+#endif
 
 	/*
 	 * It is safe to go down the mm's list of vmas when called
@@ -164,11 +180,14 @@ void flush_tlb_page(struct vm_area_struct *vma, unsigned long vmaddr)
 {
 	struct mm_struct *mm;
 	pmd_t *pmd;
-
+#if ((defined CONFIG_WRHV_MILS) && (defined CONFIG_6xx))
+	/* do not return here, need to physically flush pte */
+#else
 	if (Hash == 0) {
 		_tlbie(vmaddr);
 		return;
 	}
+#endif
 	mm = (vmaddr < TASK_SIZE)? vma->vm_mm: &init_mm;
 	pmd = pmd_offset(pud_offset(pgd_offset(mm, vmaddr), vmaddr), vmaddr);
 	if (!pmd_none(*pmd))
-- 
1.7.1.rc2

