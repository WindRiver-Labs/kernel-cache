From 21dfde0db571d0562daa019cda55b1cd05192e10 Mon Sep 17 00:00:00 2001
From: Paul Gortmaker <paul.gortmaker@windriver.com>
Date: Fri, 13 Dec 2013 13:42:33 -0500
Subject: [PATCH 036/456] grsec: changes to arch_mips_include_asm from
 grsecurity-2.9.1-3.10.11-201309081953.patch

Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
---
 arch/mips/include/asm/atomic.h      | 728 +++++++++++++++++++++++++++++++++++-
 arch/mips/include/asm/cache.h       |   3 +-
 arch/mips/include/asm/elf.h         |  11 +-
 arch/mips/include/asm/exec.h        |   2 +-
 arch/mips/include/asm/local.h       |  57 +++
 arch/mips/include/asm/page.h        |   2 +-
 arch/mips/include/asm/pgalloc.h     |   5 +
 arch/mips/include/asm/thread_info.h |   9 +-
 8 files changed, 789 insertions(+), 28 deletions(-)

diff --git a/arch/mips/include/asm/atomic.h b/arch/mips/include/asm/atomic.h
index 08b6079..8b554d2 100644
--- a/arch/mips/include/asm/atomic.h
+++ b/arch/mips/include/asm/atomic.h
@@ -21,15 +21,39 @@
 #include <asm/cmpxchg.h>
 #include <asm/war.h>
 
+#ifdef CONFIG_GENERIC_ATOMIC64
+#include <asm-generic/atomic64.h>
+#endif
+
 #define ATOMIC_INIT(i)	  { (i) }
 
+#ifdef CONFIG_64BIT
+#define _ASM_EXTABLE(from, to)		\
+"	.section __ex_table,\"a\"\n"	\
+"	.dword	" #from ", " #to"\n"	\
+"	.previous\n"
+#else
+#define _ASM_EXTABLE(from, to)		\
+"	.section __ex_table,\"a\"\n"	\
+"	.word	" #from ", " #to"\n"	\
+"	.previous\n"
+#endif
+
 /*
  * atomic_read - read atomic variable
  * @v: pointer of type atomic_t
  *
  * Atomically reads the value of @v.
  */
-#define atomic_read(v)		(*(volatile int *)&(v)->counter)
+static inline int atomic_read(const atomic_t *v)
+{
+	return (*(volatile const int *) &v->counter);
+}
+
+static inline int atomic_read_unchecked(const atomic_unchecked_t *v)
+{
+	return (*(volatile const int *) &v->counter);
+}
 
 /*
  * atomic_set - set atomic variable
@@ -38,7 +62,15 @@
  *
  * Atomically sets the value of @v to @i.
  */
-#define atomic_set(v, i)		((v)->counter = (i))
+static inline void atomic_set(atomic_t *v, int i)
+{
+	v->counter = i;
+}
+
+static inline void atomic_set_unchecked(atomic_unchecked_t *v, int i)
+{
+	v->counter = i;
+}
 
 /*
  * atomic_add - add integer to atomic variable
@@ -47,7 +79,67 @@
  *
  * Atomically adds @i to @v.
  */
-static __inline__ void atomic_add(int i, atomic_t * v)
+static __inline__ void atomic_add(int i, atomic_t *v)
+{
+	int temp;
+
+	if (kernel_uses_llsc && R10000_LLSC_WAR) {
+		__asm__ __volatile__(
+		"	.set	mips3					\n"
+		"1:	ll	%0, %1		# atomic_add		\n"
+#ifdef CONFIG_PAX_REFCOUNT
+			/* Exception on overflow. */
+		"2:	add	%0, %2					\n"
+#else
+		"	addu	%0, %2					\n"
+#endif
+		"	sc	%0, %1					\n"
+		"	beqzl	%0, 1b					\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"3:							\n"
+		_ASM_EXTABLE(2b, 3b)
+#endif
+		"	.set	mips0					\n"
+		: "=&r" (temp), "+m" (v->counter)
+		: "Ir" (i));
+	} else if (kernel_uses_llsc) {
+		__asm__ __volatile__(
+		"	.set	mips3					\n"
+		"1:	ll	%0, %1		# atomic_add		\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		/* Exception on overflow. */
+		"2:	add	%0, %2					\n"
+#else
+		"	addu	%0, %2					\n"
+#endif
+		"	sc	%0, %1					\n"
+		"	beqz	%0, 1b					\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"3:							\n"
+		_ASM_EXTABLE(2b, 3b)
+#endif
+		"	.set	mips0					\n"
+		: "=&r" (temp), "+m" (v->counter)
+		: "Ir" (i));
+	} else {
+		unsigned long flags;
+
+		raw_local_irq_save(flags);
+		__asm__ __volatile__(
+#ifdef CONFIG_PAX_REFCOUNT
+			/* Exception on overflow. */
+		"1:	add	%0, %1					\n"
+		"2:							\n"
+		_ASM_EXTABLE(1b, 2b)
+#else
+		"	addu	%0, %1					\n"
+#endif
+		: "+r" (v->counter) : "Ir" (i));
+		raw_local_irq_restore(flags);
+	}
+}
+
+static __inline__ void atomic_add_unchecked(int i, atomic_unchecked_t *v)
 {
 	if (kernel_uses_llsc && R10000_LLSC_WAR) {
 		int temp;
@@ -90,7 +182,67 @@ static __inline__ void atomic_add(int i, atomic_t * v)
  *
  * Atomically subtracts @i from @v.
  */
-static __inline__ void atomic_sub(int i, atomic_t * v)
+static __inline__ void atomic_sub(int i, atomic_t *v)
+{
+	int temp;
+
+	if (kernel_uses_llsc && R10000_LLSC_WAR) {
+		__asm__ __volatile__(
+		"	.set	mips3					\n"
+		"1:	ll	%0, %1		# atomic64_sub		\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		/* Exception on overflow. */
+		"2:	sub	%0, %2					\n"
+#else
+		"	subu	%0, %2					\n"
+#endif
+		"	sc	%0, %1					\n"
+		"	beqzl	%0, 1b					\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"3:							\n"
+		_ASM_EXTABLE(2b, 3b)
+#endif
+		"	.set	mips0					\n"
+		: "=&r" (temp), "+m" (v->counter)
+		: "Ir" (i));
+	} else if (kernel_uses_llsc) {
+		__asm__ __volatile__(
+		"	.set	mips3					\n"
+		"1:	ll	%0, %1		# atomic64_sub		\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		/* Exception on overflow. */
+		"2:	sub	%0, %2					\n"
+#else
+		"	subu	%0, %2					\n"
+#endif
+		"	sc	%0, %1					\n"
+		"	beqz	%0, 1b					\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"3:							\n"
+		_ASM_EXTABLE(2b, 3b)
+#endif
+		"	.set	mips0					\n"
+		: "=&r" (temp), "+m" (v->counter)
+		: "Ir" (i));
+	} else {
+		unsigned long flags;
+
+		raw_local_irq_save(flags);
+		__asm__ __volatile__(
+#ifdef CONFIG_PAX_REFCOUNT
+			/* Exception on overflow. */
+		"1:	sub	%0, %1					\n"
+		"2:							\n"
+		_ASM_EXTABLE(1b, 2b)
+#else
+		"	subu	%0, %1					\n"
+#endif
+		: "+r" (v->counter) : "Ir" (i));
+		raw_local_irq_restore(flags);
+	}
+}
+
+static __inline__ void atomic_sub_unchecked(long i, atomic_unchecked_t *v)
 {
 	if (kernel_uses_llsc && R10000_LLSC_WAR) {
 		int temp;
@@ -129,7 +281,93 @@ static __inline__ void atomic_sub(int i, atomic_t * v)
 /*
  * Same as above, but return the result value
  */
-static __inline__ int atomic_add_return(int i, atomic_t * v)
+static __inline__ int atomic_add_return(int i, atomic_t *v)
+{
+	int result;
+	int temp;
+
+	smp_mb__before_llsc();
+
+	if (kernel_uses_llsc && R10000_LLSC_WAR) {
+		__asm__ __volatile__(
+		"	.set	mips3					\n"
+		"1:	ll	%1, %2		# atomic_add_return	\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"2:	add	%0, %1, %3				\n"
+#else
+		"	addu	%0, %1, %3				\n"
+#endif
+		"	sc	%0, %2					\n"
+		"	beqzl	%0, 1b					\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"	b	4f					\n"
+		"	.set	noreorder				\n"
+		"3:	b	5f					\n"
+		"	move	%0, %1					\n"
+		"	.set	reorder					\n"
+		_ASM_EXTABLE(2b, 3b)
+#endif
+		"4:	addu	%0, %1, %3				\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"5:							\n"
+#endif
+		"	.set	mips0					\n"
+		: "=&r" (result), "=&r" (temp), "+m" (v->counter)
+		: "Ir" (i));
+	} else if (kernel_uses_llsc) {
+		__asm__ __volatile__(
+		"	.set	mips3					\n"
+		"1:	ll	%1, %2	# atomic_add_return		\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"2:	add	%0, %1, %3				\n"
+#else
+		"	addu	%0, %1, %3				\n"
+#endif
+		"	sc	%0, %2					\n"
+		"	bnez	%0, 4f					\n"
+		"	b	1b					\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"	.set	noreorder				\n"
+		"3:	b	5f					\n"
+		"	move	%0, %1					\n"
+		"	.set	reorder					\n"
+		_ASM_EXTABLE(2b, 3b)
+#endif
+		"4:	addu	%0, %1, %3				\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"5:							\n"
+#endif
+		"	.set	mips0					\n"
+		: "=&r" (result), "=&r" (temp), "+m" (v->counter)
+		: "Ir" (i));
+	} else {
+		unsigned long flags;
+
+		raw_local_irq_save(flags);
+		__asm__ __volatile__(
+		"	lw	%0, %1					\n"
+#ifdef CONFIG_PAX_REFCOUNT
+			/* Exception on overflow. */
+		"1:	add	%0, %2					\n"
+#else
+		"	addu	%0, %2					\n"
+#endif
+		"	sw	%0, %1					\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		/* Note: Dest reg is not modified on overflow */
+		"2:							\n"
+		_ASM_EXTABLE(1b, 2b)
+#endif
+		: "=&r" (result), "+m" (v->counter) : "Ir" (i));
+		raw_local_irq_restore(flags);
+	}
+
+	smp_llsc_mb();
+
+	return result;
+}
+
+static __inline__ int atomic_add_return_unchecked(int i, atomic_unchecked_t *v)
 {
 	int result;
 
@@ -178,7 +416,93 @@ static __inline__ int atomic_add_return(int i, atomic_t * v)
 	return result;
 }
 
-static __inline__ int atomic_sub_return(int i, atomic_t * v)
+static __inline__ int atomic_sub_return(int i, atomic_t *v)
+{
+	int result;
+	int temp;
+
+	smp_mb__before_llsc();
+
+	if (kernel_uses_llsc && R10000_LLSC_WAR) {
+		__asm__ __volatile__(
+		"	.set	mips3					\n"
+		"1:	ll	%1, %2		# atomic_sub_return	\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"2:	sub	%0, %1, %3				\n"
+#else
+		"	subu	%0, %1, %3				\n"
+#endif
+		"	sc	%0, %2					\n"
+		"	beqzl	%0, 1b					\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"	b	4f					\n"
+		"	.set	noreorder				\n"
+		"3:	b	5f					\n"
+		"	move	%0, %1					\n"
+		"	.set	reorder					\n"
+		_ASM_EXTABLE(2b, 3b)
+#endif
+		"4:	subu	%0, %1, %3				\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"5:							\n"
+#endif
+		"	.set	mips0					\n"
+		: "=&r" (result), "=&r" (temp), "=m" (v->counter)
+		: "Ir" (i), "m" (v->counter)
+		: "memory");
+	} else if (kernel_uses_llsc) {
+		__asm__ __volatile__(
+		"	.set	mips3					\n"
+		"1:	ll	%1, %2	# atomic_sub_return		\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"2:	sub	%0, %1, %3				\n"
+#else
+		"	subu	%0, %1, %3				\n"
+#endif
+		"	sc	%0, %2					\n"
+		"	bnez	%0, 4f					\n"
+		"	b	1b					\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"	.set	noreorder				\n"
+		"3:	b	5f					\n"
+		"	move	%0, %1					\n"
+		"	.set	reorder					\n"
+		_ASM_EXTABLE(2b, 3b)
+#endif
+		"4:	subu	%0, %1, %3				\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"5:							\n"
+#endif
+		"	.set	mips0					\n"
+		: "=&r" (result), "=&r" (temp), "+m" (v->counter)
+		: "Ir" (i));
+	} else {
+		unsigned long flags;
+
+		raw_local_irq_save(flags);
+		__asm__ __volatile__(
+		"	lw	%0, %1					\n"
+#ifdef CONFIG_PAX_REFCOUNT
+			/* Exception on overflow. */
+		"1:	sub	%0, %2					\n"
+#else
+		"	subu	%0, %2					\n"
+#endif
+		"	sw	%0, %1					\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		/* Note: Dest reg is not modified on overflow */
+		"2:							\n"
+		_ASM_EXTABLE(1b, 2b)
+#endif
+		: "=&r" (result), "+m" (v->counter) : "Ir" (i));
+		raw_local_irq_restore(flags);
+	}
+
+	smp_llsc_mb();
+
+	return result;
+}
+static __inline__ int atomic_sub_return_unchecked(int i, atomic_unchecked_t *v)
 {
 	int result;
 
@@ -238,7 +562,7 @@ static __inline__ int atomic_sub_return(int i, atomic_t * v)
  * Atomically test @v and subtract @i if @v is greater or equal than @i.
  * The function returns the old value of @v minus @i.
  */
-static __inline__ int atomic_sub_if_positive(int i, atomic_t * v)
+static __inline__ int atomic_sub_if_positive(int i, atomic_t *v)
 {
 	int result;
 
@@ -295,8 +619,26 @@ static __inline__ int atomic_sub_if_positive(int i, atomic_t * v)
 	return result;
 }
 
-#define atomic_cmpxchg(v, o, n) (cmpxchg(&((v)->counter), (o), (n)))
-#define atomic_xchg(v, new) (xchg(&((v)->counter), (new)))
+static inline int atomic_cmpxchg(atomic_t *v, int old, int new)
+{
+	return cmpxchg(&v->counter, old, new);
+}
+
+static inline int atomic_cmpxchg_unchecked(atomic_unchecked_t *v, int old,
+					   int new)
+{
+	return cmpxchg(&(v->counter), old, new);
+}
+
+static inline int atomic_xchg(atomic_t *v, int new)
+{
+	return xchg(&v->counter, new);
+}
+
+static inline int atomic_xchg_unchecked(atomic_unchecked_t *v, int new)
+{
+	return xchg(&(v->counter), new);
+}
 
 /**
  * __atomic_add_unless - add unless the number is a given value
@@ -324,6 +666,10 @@ static __inline__ int __atomic_add_unless(atomic_t *v, int a, int u)
 
 #define atomic_dec_return(v) atomic_sub_return(1, (v))
 #define atomic_inc_return(v) atomic_add_return(1, (v))
+static __inline__ int atomic_inc_return_unchecked(atomic_unchecked_t *v)
+{
+	return atomic_add_return_unchecked(1, v);
+}
 
 /*
  * atomic_sub_and_test - subtract value from variable and test result
@@ -345,6 +691,10 @@ static __inline__ int __atomic_add_unless(atomic_t *v, int a, int u)
  * other cases.
  */
 #define atomic_inc_and_test(v) (atomic_inc_return(v) == 0)
+static __inline__ int atomic_inc_and_test_unchecked(atomic_unchecked_t *v)
+{
+	return atomic_add_return_unchecked(1, v) == 0;
+}
 
 /*
  * atomic_dec_and_test - decrement by 1 and test
@@ -369,6 +719,10 @@ static __inline__ int __atomic_add_unless(atomic_t *v, int a, int u)
  * Atomically increments @v by 1.
  */
 #define atomic_inc(v) atomic_add(1, (v))
+static __inline__ void atomic_inc_unchecked(atomic_unchecked_t *v)
+{
+	atomic_add_unchecked(1, v);
+}
 
 /*
  * atomic_dec - decrement and test
@@ -377,6 +731,10 @@ static __inline__ int __atomic_add_unless(atomic_t *v, int a, int u)
  * Atomically decrements @v by 1.
  */
 #define atomic_dec(v) atomic_sub(1, (v))
+static __inline__ void atomic_dec_unchecked(atomic_unchecked_t *v)
+{
+	atomic_sub_unchecked(1, v);
+}
 
 /*
  * atomic_add_negative - add and test if negative
@@ -398,14 +756,30 @@ static __inline__ int __atomic_add_unless(atomic_t *v, int a, int u)
  * @v: pointer of type atomic64_t
  *
  */
-#define atomic64_read(v)	(*(volatile long *)&(v)->counter)
+static inline long atomic64_read(const atomic64_t *v)
+{
+	return (*(volatile const long *) &v->counter);
+}
+
+static inline long atomic64_read_unchecked(const atomic64_unchecked_t *v)
+{
+	return (*(volatile const long *) &v->counter);
+}
 
 /*
  * atomic64_set - set atomic variable
  * @v: pointer of type atomic64_t
  * @i: required value
  */
-#define atomic64_set(v, i)	((v)->counter = (i))
+static inline void atomic64_set(atomic64_t *v, long i)
+{
+	v->counter = i;
+}
+
+static inline void atomic64_set_unchecked(atomic64_unchecked_t *v, long i)
+{
+	v->counter = i;
+}
 
 /*
  * atomic64_add - add integer to atomic variable
@@ -414,7 +788,66 @@ static __inline__ int __atomic_add_unless(atomic_t *v, int a, int u)
  *
  * Atomically adds @i to @v.
  */
-static __inline__ void atomic64_add(long i, atomic64_t * v)
+static __inline__ void atomic64_add(long i, atomic64_t *v)
+{
+	long temp;
+
+	if (kernel_uses_llsc && R10000_LLSC_WAR) {
+		__asm__ __volatile__(
+		"	.set	mips3					\n"
+		"1:	lld	%0, %1		# atomic64_add		\n"
+#ifdef CONFIG_PAX_REFCOUNT
+			/* Exception on overflow. */
+		"2:	dadd	%0, %2					\n"
+#else
+		"	daddu	%0, %2					\n"
+#endif
+		"	scd	%0, %1					\n"
+		"	beqzl	%0, 1b					\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"3:							\n"
+		_ASM_EXTABLE(2b, 3b)
+#endif
+		"	.set	mips0					\n"
+		: "=&r" (temp), "+m" (v->counter)
+		: "Ir" (i));
+	} else if (kernel_uses_llsc) {
+		__asm__ __volatile__(
+		"	.set	mips3					\n"
+		"1:	lld	%0, %1		# atomic64_add		\n"
+#ifdef CONFIG_PAX_REFCOUNT
+			/* Exception on overflow. */
+		"2:	dadd	%0, %2					\n"
+#else
+		"	daddu	%0, %2					\n"
+#endif
+		"	scd	%0, %1					\n"
+		"	beqz	%0, 1b					\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"3:							\n"
+		_ASM_EXTABLE(2b, 3b)
+#endif
+		"	.set	mips0					\n"
+		: "=&r" (temp), "+m" (v->counter)
+		: "Ir" (i));
+	} else {
+		unsigned long flags;
+
+		raw_local_irq_save(flags);
+		__asm__ __volatile__(
+#ifdef CONFIG_PAX_REFCOUNT
+			/* Exception on overflow. */
+		"1:	dadd	%0, %1					\n"
+		"2:							\n"
+		_ASM_EXTABLE(1b, 2b)
+#else
+		"	daddu	%0, %1					\n"
+#endif
+		: "+r" (v->counter) : "Ir" (i));
+		raw_local_irq_restore(flags);
+	}
+}
+static __inline__ void atomic64_add_unchecked(long i, atomic64_unchecked_t *v)
 {
 	if (kernel_uses_llsc && R10000_LLSC_WAR) {
 		long temp;
@@ -457,7 +890,67 @@ static __inline__ void atomic64_add(long i, atomic64_t * v)
  *
  * Atomically subtracts @i from @v.
  */
-static __inline__ void atomic64_sub(long i, atomic64_t * v)
+static __inline__ void atomic64_sub(long i, atomic64_t *v)
+{
+	long temp;
+
+	if (kernel_uses_llsc && R10000_LLSC_WAR) {
+		__asm__ __volatile__(
+		"	.set	mips3					\n"
+		"1:	lld	%0, %1		# atomic64_sub		\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		/* Exception on overflow. */
+		"2:	dsub	%0, %2					\n"
+#else
+		"	dsubu	%0, %2					\n"
+#endif
+		"	scd	%0, %1					\n"
+		"	beqzl	%0, 1b					\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"3:							\n"
+		_ASM_EXTABLE(2b, 3b)
+#endif
+		"	.set	mips0					\n"
+		: "=&r" (temp), "+m" (v->counter)
+		: "Ir" (i));
+	} else if (kernel_uses_llsc) {
+		__asm__ __volatile__(
+		"	.set	mips3					\n"
+		"1:	lld	%0, %1		# atomic64_sub		\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		/* Exception on overflow. */
+		"2:	dsub	%0, %2					\n"
+#else
+		"	dsubu	%0, %2					\n"
+#endif
+		"	scd	%0, %1					\n"
+		"	beqz	%0, 1b					\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"3:							\n"
+		_ASM_EXTABLE(2b, 3b)
+#endif
+		"	.set	mips0					\n"
+		: "=&r" (temp), "+m" (v->counter)
+		: "Ir" (i));
+	} else {
+		unsigned long flags;
+
+		raw_local_irq_save(flags);
+		__asm__ __volatile__(
+#ifdef CONFIG_PAX_REFCOUNT
+			/* Exception on overflow. */
+		"1:	dsub	%0, %1					\n"
+		"2:							\n"
+		_ASM_EXTABLE(1b, 2b)
+#else
+		"	dsubu	%0, %1					\n"
+#endif
+		: "+r" (v->counter) : "Ir" (i));
+		raw_local_irq_restore(flags);
+	}
+}
+
+static __inline__ void atomic64_sub_unchecked(long i, atomic64_unchecked_t *v)
 {
 	if (kernel_uses_llsc && R10000_LLSC_WAR) {
 		long temp;
@@ -496,7 +989,93 @@ static __inline__ void atomic64_sub(long i, atomic64_t * v)
 /*
  * Same as above, but return the result value
  */
-static __inline__ long atomic64_add_return(long i, atomic64_t * v)
+static __inline__ long atomic64_add_return(long i, atomic64_t *v)
+{
+	long result;
+	long temp;
+
+	smp_mb__before_llsc();
+
+	if (kernel_uses_llsc && R10000_LLSC_WAR) {
+		__asm__ __volatile__(
+		"	.set	mips3					\n"
+		"1:	lld	%1, %2		# atomic64_add_return	\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"2:	dadd	%0, %1, %3				\n"
+#else
+		"	daddu	%0, %1, %3				\n"
+#endif
+		"	scd	%0, %2					\n"
+		"	beqzl	%0, 1b					\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"	b	4f					\n"
+		"	.set	noreorder				\n"
+		"3:	b	5f					\n"
+		"	move	%0, %1					\n"
+		"	.set	reorder					\n"
+		_ASM_EXTABLE(2b, 3b)
+#endif
+		"4:	daddu	%0, %1, %3				\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"5:							\n"
+#endif
+		"	.set	mips0					\n"
+		: "=&r" (result), "=&r" (temp), "+m" (v->counter)
+		: "Ir" (i));
+	} else if (kernel_uses_llsc) {
+		__asm__ __volatile__(
+		"	.set	mips3					\n"
+		"1:	lld	%1, %2	# atomic64_add_return		\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"2:	dadd	%0, %1, %3				\n"
+#else
+		"	daddu	%0, %1, %3				\n"
+#endif
+		"	scd	%0, %2					\n"
+		"	bnez	%0, 4f					\n"
+		"	b	1b					\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"	.set	noreorder				\n"
+		"3:	b	5f					\n"
+		"	move	%0, %1					\n"
+		"	.set	reorder					\n"
+		_ASM_EXTABLE(2b, 3b)
+#endif
+		"4:	daddu	%0, %1, %3				\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"5:							\n"
+#endif
+		"	.set	mips0					\n"
+		: "=&r" (result), "=&r" (temp), "=m" (v->counter)
+		: "Ir" (i), "m" (v->counter)
+		: "memory");
+	} else {
+		unsigned long flags;
+
+		raw_local_irq_save(flags);
+		__asm__ __volatile__(
+		"	ld	%0, %1					\n"
+#ifdef CONFIG_PAX_REFCOUNT
+			/* Exception on overflow. */
+		"1:	dadd	%0, %2					\n"
+#else
+		"	daddu	%0, %2					\n"
+#endif
+		"	sd	%0, %1					\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		/* Note: Dest reg is not modified on overflow */
+		"2:							\n"
+		_ASM_EXTABLE(1b, 2b)
+#endif
+		: "=&r" (result), "+m" (v->counter) : "Ir" (i));
+		raw_local_irq_restore(flags);
+	}
+
+	smp_llsc_mb();
+
+	return result;
+}
+static __inline__ long atomic64_add_return_unchecked(long i, atomic64_unchecked_t *v)
 {
 	long result;
 
@@ -546,7 +1125,97 @@ static __inline__ long atomic64_add_return(long i, atomic64_t * v)
 	return result;
 }
 
-static __inline__ long atomic64_sub_return(long i, atomic64_t * v)
+static __inline__ long atomic64_sub_return(long i, atomic64_t *v)
+{
+	long result;
+	long temp;
+
+	smp_mb__before_llsc();
+
+	if (kernel_uses_llsc && R10000_LLSC_WAR) {
+		long temp;
+
+		__asm__ __volatile__(
+		"	.set	mips3					\n"
+		"1:	lld	%1, %2		# atomic64_sub_return	\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"2:	dsub	%0, %1, %3				\n"
+#else
+		"	dsubu	%0, %1, %3				\n"
+#endif
+		"	scd	%0, %2					\n"
+		"	beqzl	%0, 1b					\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"	b	4f					\n"
+		"	.set	noreorder				\n"
+		"3:	b	5f					\n"
+		"	move	%0, %1					\n"
+		"	.set	reorder					\n"
+		_ASM_EXTABLE(2b, 3b)
+#endif
+		"4:	dsubu	%0, %1, %3				\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"5:							\n"
+#endif
+		"	.set	mips0					\n"
+		: "=&r" (result), "=&r" (temp), "=m" (v->counter)
+		: "Ir" (i), "m" (v->counter)
+		: "memory");
+	} else if (kernel_uses_llsc) {
+		__asm__ __volatile__(
+		"	.set	mips3					\n"
+		"1:	lld	%1, %2	# atomic64_sub_return		\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"2:	dsub	%0, %1, %3				\n"
+#else
+		"	dsubu	%0, %1, %3				\n"
+#endif
+		"	scd	%0, %2					\n"
+		"	bnez	%0, 4f					\n"
+		"	b	1b					\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"	.set	noreorder				\n"
+		"3:	b	5f					\n"
+		"	move	%0, %1					\n"
+		"	.set	reorder					\n"
+		_ASM_EXTABLE(2b, 3b)
+#endif
+		"4:	dsubu	%0, %1, %3				\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		"5:							\n"
+#endif
+		"	.set	mips0					\n"
+		: "=&r" (result), "=&r" (temp), "=m" (v->counter)
+		: "Ir" (i), "m" (v->counter)
+		: "memory");
+	} else {
+		unsigned long flags;
+
+		raw_local_irq_save(flags);
+		__asm__ __volatile__(
+		"	ld	%0, %1					\n"
+#ifdef CONFIG_PAX_REFCOUNT
+			/* Exception on overflow. */
+		"1:	dsub	%0, %2					\n"
+#else
+		"	dsubu	%0, %2					\n"
+#endif
+		"	sd	%0, %1					\n"
+#ifdef CONFIG_PAX_REFCOUNT
+		/* Note: Dest reg is not modified on overflow */
+		"2:							\n"
+		_ASM_EXTABLE(1b, 2b)
+#endif
+		: "=&r" (result), "+m" (v->counter) : "Ir" (i));
+		raw_local_irq_restore(flags);
+	}
+
+	smp_llsc_mb();
+
+	return result;
+}
+
+static __inline__ long atomic64_sub_return_unchecked(long i, atomic64_unchecked_t *v)
 {
 	long result;
 
@@ -605,7 +1274,7 @@ static __inline__ long atomic64_sub_return(long i, atomic64_t * v)
  * Atomically test @v and subtract @i if @v is greater or equal than @i.
  * The function returns the old value of @v minus @i.
  */
-static __inline__ long atomic64_sub_if_positive(long i, atomic64_t * v)
+static __inline__ long atomic64_sub_if_positive(long i, atomic64_t *v)
 {
 	long result;
 
@@ -662,9 +1331,26 @@ static __inline__ long atomic64_sub_if_positive(long i, atomic64_t * v)
 	return result;
 }
 
-#define atomic64_cmpxchg(v, o, n) \
-	((__typeof__((v)->counter))cmpxchg(&((v)->counter), (o), (n)))
-#define atomic64_xchg(v, new) (xchg(&((v)->counter), (new)))
+static inline long atomic64_cmpxchg(atomic64_t *v, long old, long new)
+{
+	return cmpxchg(&v->counter, old, new);
+}
+
+static inline long atomic64_cmpxchg_unchecked(atomic64_unchecked_t *v, long old,
+					      long new)
+{
+	return cmpxchg(&(v->counter), old, new);
+}
+
+static inline long atomic64_xchg(atomic64_t *v, long new)
+{
+	return xchg(&v->counter, new);
+}
+
+static inline long atomic64_xchg_unchecked(atomic64_unchecked_t *v, long new)
+{
+	return xchg(&(v->counter), new);
+}
 
 /**
  * atomic64_add_unless - add unless the number is a given value
@@ -694,6 +1380,7 @@ static __inline__ int atomic64_add_unless(atomic64_t *v, long a, long u)
 
 #define atomic64_dec_return(v) atomic64_sub_return(1, (v))
 #define atomic64_inc_return(v) atomic64_add_return(1, (v))
+#define atomic64_inc_return_unchecked(v) atomic64_add_return_unchecked(1, (v))
 
 /*
  * atomic64_sub_and_test - subtract value from variable and test result
@@ -715,6 +1402,7 @@ static __inline__ int atomic64_add_unless(atomic64_t *v, long a, long u)
  * other cases.
  */
 #define atomic64_inc_and_test(v) (atomic64_inc_return(v) == 0)
+#define atomic64_inc_and_test_unchecked(v) atomic64_add_return_unchecked(1, (v)) == 0)
 
 /*
  * atomic64_dec_and_test - decrement by 1 and test
@@ -739,6 +1427,7 @@ static __inline__ int atomic64_add_unless(atomic64_t *v, long a, long u)
  * Atomically increments @v by 1.
  */
 #define atomic64_inc(v) atomic64_add(1, (v))
+#define atomic64_inc_unchecked(v) atomic64_add_unchecked(1, (v))
 
 /*
  * atomic64_dec - decrement and test
@@ -747,6 +1436,7 @@ static __inline__ int atomic64_add_unless(atomic64_t *v, long a, long u)
  * Atomically decrements @v by 1.
  */
 #define atomic64_dec(v) atomic64_sub(1, (v))
+#define atomic64_dec_unchecked(v) atomic64_sub_unchecked(1, (v))
 
 /*
  * atomic64_add_negative - add and test if negative
diff --git a/arch/mips/include/asm/cache.h b/arch/mips/include/asm/cache.h
index b4db69f..8f3b093 100644
--- a/arch/mips/include/asm/cache.h
+++ b/arch/mips/include/asm/cache.h
@@ -9,10 +9,11 @@
 #ifndef _ASM_CACHE_H
 #define _ASM_CACHE_H
 
+#include <linux/const.h>
 #include <kmalloc.h>
 
 #define L1_CACHE_SHIFT		CONFIG_MIPS_L1_CACHE_SHIFT
-#define L1_CACHE_BYTES		(1 << L1_CACHE_SHIFT)
+#define L1_CACHE_BYTES		(_AC(1,UL) << L1_CACHE_SHIFT)
 
 #define SMP_CACHE_SHIFT		L1_CACHE_SHIFT
 #define SMP_CACHE_BYTES		L1_CACHE_BYTES
diff --git a/arch/mips/include/asm/elf.h b/arch/mips/include/asm/elf.h
index cf3ae24..238d22f 100644
--- a/arch/mips/include/asm/elf.h
+++ b/arch/mips/include/asm/elf.h
@@ -372,13 +372,16 @@ extern const char *__elf_platform;
 #define ELF_ET_DYN_BASE		(TASK_SIZE / 3 * 2)
 #endif
 
+#ifdef CONFIG_PAX_ASLR
+#define PAX_ELF_ET_DYN_BASE	(TASK_IS_32BIT_ADDR ? 0x00400000UL : 0x00400000UL)
+
+#define PAX_DELTA_MMAP_LEN	(TASK_IS_32BIT_ADDR ? 27-PAGE_SHIFT : 36-PAGE_SHIFT)
+#define PAX_DELTA_STACK_LEN	(TASK_IS_32BIT_ADDR ? 27-PAGE_SHIFT : 36-PAGE_SHIFT)
+#endif
+
 #define ARCH_HAS_SETUP_ADDITIONAL_PAGES 1
 struct linux_binprm;
 extern int arch_setup_additional_pages(struct linux_binprm *bprm,
 				       int uses_interp);
 
-struct mm_struct;
-extern unsigned long arch_randomize_brk(struct mm_struct *mm);
-#define arch_randomize_brk arch_randomize_brk
-
 #endif /* _ASM_ELF_H */
diff --git a/arch/mips/include/asm/exec.h b/arch/mips/include/asm/exec.h
index c1f6afa..38cc6e9 100644
--- a/arch/mips/include/asm/exec.h
+++ b/arch/mips/include/asm/exec.h
@@ -12,6 +12,6 @@
 #ifndef _ASM_EXEC_H
 #define _ASM_EXEC_H
 
-extern unsigned long arch_align_stack(unsigned long sp);
+#define arch_align_stack(x) ((x) & ~0xfUL)
 
 #endif /* _ASM_EXEC_H */
diff --git a/arch/mips/include/asm/local.h b/arch/mips/include/asm/local.h
index d44622c..64990d2 100644
--- a/arch/mips/include/asm/local.h
+++ b/arch/mips/include/asm/local.h
@@ -12,15 +12,25 @@ typedef struct
 	atomic_long_t a;
 } local_t;
 
+typedef struct {
+	atomic_long_unchecked_t a;
+} local_unchecked_t;
+
 #define LOCAL_INIT(i)	{ ATOMIC_LONG_INIT(i) }
 
 #define local_read(l)	atomic_long_read(&(l)->a)
+#define local_read_unchecked(l)	atomic_long_read_unchecked(&(l)->a)
 #define local_set(l, i) atomic_long_set(&(l)->a, (i))
+#define local_set_unchecked(l, i)	atomic_long_set_unchecked(&(l)->a, (i))
 
 #define local_add(i, l) atomic_long_add((i), (&(l)->a))
+#define local_add_unchecked(i, l) atomic_long_add_unchecked((i), (&(l)->a))
 #define local_sub(i, l) atomic_long_sub((i), (&(l)->a))
+#define local_sub_unchecked(i, l) atomic_long_sub_unchecked((i), (&(l)->a))
 #define local_inc(l)	atomic_long_inc(&(l)->a)
+#define local_inc_unchecked(l)	atomic_long_inc_unchecked(&(l)->a)
 #define local_dec(l)	atomic_long_dec(&(l)->a)
+#define local_dec_unchecked(l)	atomic_long_dec_unchecked(&(l)->a)
 
 /*
  * Same as above, but return the result value
@@ -70,6 +80,51 @@ static __inline__ long local_add_return(long i, local_t * l)
 	return result;
 }
 
+static __inline__ long local_add_return_unchecked(long i, local_unchecked_t * l)
+{
+	unsigned long result;
+
+	if (kernel_uses_llsc && R10000_LLSC_WAR) {
+		unsigned long temp;
+
+		__asm__ __volatile__(
+		"	.set	mips3					\n"
+		"1:"	__LL	"%1, %2		# local_add_return	\n"
+		"	addu	%0, %1, %3				\n"
+			__SC	"%0, %2					\n"
+		"	beqzl	%0, 1b					\n"
+		"	addu	%0, %1, %3				\n"
+		"	.set	mips0					\n"
+		: "=&r" (result), "=&r" (temp), "=m" (l->a.counter)
+		: "Ir" (i), "m" (l->a.counter)
+		: "memory");
+	} else if (kernel_uses_llsc) {
+		unsigned long temp;
+
+		__asm__ __volatile__(
+		"	.set	mips3					\n"
+		"1:"	__LL	"%1, %2		# local_add_return	\n"
+		"	addu	%0, %1, %3				\n"
+			__SC	"%0, %2					\n"
+		"	beqz	%0, 1b					\n"
+		"	addu	%0, %1, %3				\n"
+		"	.set	mips0					\n"
+		: "=&r" (result), "=&r" (temp), "=m" (l->a.counter)
+		: "Ir" (i), "m" (l->a.counter)
+		: "memory");
+	} else {
+		unsigned long flags;
+
+		local_irq_save(flags);
+		result = l->a.counter;
+		result += i;
+		l->a.counter = result;
+		local_irq_restore(flags);
+	}
+
+	return result;
+}
+
 static __inline__ long local_sub_return(long i, local_t * l)
 {
 	unsigned long result;
@@ -117,6 +172,8 @@ static __inline__ long local_sub_return(long i, local_t * l)
 
 #define local_cmpxchg(l, o, n) \
 	((long)cmpxchg_local(&((l)->a.counter), (o), (n)))
+#define local_cmpxchg_unchecked(l, o, n) \
+	((long)cmpxchg_local(&((l)->a.counter), (o), (n)))
 #define local_xchg(l, n) (atomic_long_xchg((&(l)->a), (n)))
 
 /**
diff --git a/arch/mips/include/asm/page.h b/arch/mips/include/asm/page.h
index f59552f..3abe9b9 100644
--- a/arch/mips/include/asm/page.h
+++ b/arch/mips/include/asm/page.h
@@ -95,7 +95,7 @@ extern void copy_user_highpage(struct page *to, struct page *from,
   #ifdef CONFIG_CPU_MIPS32
     typedef struct { unsigned long pte_low, pte_high; } pte_t;
     #define pte_val(x)	  ((x).pte_low | ((unsigned long long)(x).pte_high << 32))
-    #define __pte(x)	  ({ pte_t __pte = {(x), ((unsigned long long)(x)) >> 32}; __pte; })
+    #define __pte(x)	  ({ pte_t __pte = {(x), (x) >> 32}; __pte; })
   #else
      typedef struct { unsigned long long pte; } pte_t;
      #define pte_val(x) ((x).pte)
diff --git a/arch/mips/include/asm/pgalloc.h b/arch/mips/include/asm/pgalloc.h
index 881d18b..cea38bc 100644
--- a/arch/mips/include/asm/pgalloc.h
+++ b/arch/mips/include/asm/pgalloc.h
@@ -37,6 +37,11 @@ static inline void pud_populate(struct mm_struct *mm, pud_t *pud, pmd_t *pmd)
 {
 	set_pud(pud, __pud((unsigned long)pmd));
 }
+
+static inline void pud_populate_kernel(struct mm_struct *mm, pud_t *pud, pmd_t *pmd)
+{
+	pud_populate(mm, pud, pmd);
+}
 #endif
 
 /*
diff --git a/arch/mips/include/asm/thread_info.h b/arch/mips/include/asm/thread_info.h
index 895320e..bf63e10 100644
--- a/arch/mips/include/asm/thread_info.h
+++ b/arch/mips/include/asm/thread_info.h
@@ -115,6 +115,8 @@ static inline struct thread_info *current_thread_info(void)
 #define TIF_32BIT_ADDR		23	/* 32-bit address space (o32/n32) */
 #define TIF_FPUBOUND		24	/* thread bound to FPU-full CPU set */
 #define TIF_LOAD_WATCH		25	/* If set, load watch registers */
+/* li takes a 32bit immediate */
+#define TIF_GRSEC_SETXID	29	/* update credentials on syscall entry/exit */
 #define TIF_SYSCALL_TRACE	31	/* syscall trace active */
 
 #define _TIF_SYSCALL_TRACE	(1<<TIF_SYSCALL_TRACE)
@@ -130,15 +132,18 @@ static inline struct thread_info *current_thread_info(void)
 #define _TIF_32BIT_ADDR		(1<<TIF_32BIT_ADDR)
 #define _TIF_FPUBOUND		(1<<TIF_FPUBOUND)
 #define _TIF_LOAD_WATCH		(1<<TIF_LOAD_WATCH)
+#define _TIF_GRSEC_SETXID	(1<<TIF_GRSEC_SETXID)
+
+#define _TIF_SYSCALL_WORK	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | _TIF_GRSEC_SETXID)
 
 /* work to do in syscall_trace_leave() */
-#define _TIF_WORK_SYSCALL_EXIT	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT)
+#define _TIF_WORK_SYSCALL_EXIT	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | _TIF_GRSEC_SETXID)
 
 /* work to do on interrupt/exception return */
 #define _TIF_WORK_MASK		\
 	(_TIF_SIGPENDING | _TIF_NEED_RESCHED | _TIF_NOTIFY_RESUME)
 /* work to do on any return to u-space */
-#define _TIF_ALLWORK_MASK	(_TIF_WORK_MASK | _TIF_WORK_SYSCALL_EXIT)
+#define _TIF_ALLWORK_MASK	(_TIF_WORK_MASK | _TIF_WORK_SYSCALL_EXIT | _TIF_GRSEC_SETXID)
 
 #endif /* __KERNEL__ */
 
-- 
1.8.5.1

