From 078018f022a23bcbcf7ea2044c3a185afaced343 Mon Sep 17 00:00:00 2001
From: Wu Zhangjin <zhangjin.wu@windriver.com>
Date: Mon, 20 Sep 2010 15:24:59 +0800
Subject: [PATCH 7/8] tracing: don't trace ftrace

We replace the callees of ftrace by the notrace variants to reduce more
garbage information from the tracing result.

FYI:

functions with "_notrace" suffix is the notrace variants of the
functions, will not be traced by the preemptoff tracer.

the "raw_" variants of local_irq_* will not be traced by the irqsoff
tracer.

Signed-off-by: Wu Zhangjin <zhangjin.wu@windriver.com>
---
 kernel/trace/blktrace.c              |   12 ++++++------
 kernel/trace/ftrace.c                |   16 ++++++++--------
 kernel/trace/ring_buffer.c           |   24 ++++++++++++------------
 kernel/trace/trace.c                 |   24 ++++++++++++------------
 kernel/trace/trace_boot.c            |    8 ++++----
 kernel/trace/trace_branch.c          |    4 ++--
 kernel/trace/trace_clock.c           |    4 ++--
 kernel/trace/trace_event_perf.c      |    4 ++--
 kernel/trace/trace_functions.c       |    8 ++++----
 kernel/trace/trace_functions_graph.c |    8 ++++----
 kernel/trace/trace_hw_branches.c     |    4 ++--
 kernel/trace/trace_mmiotrace.c       |    4 ++--
 kernel/trace/trace_sched_switch.c    |    8 ++++----
 kernel/trace/trace_sched_wakeup.c    |   12 ++++++------
 kernel/trace/trace_selftest.c        |    4 ++--
 kernel/trace/trace_stack.c           |   12 ++++++------
 16 files changed, 78 insertions(+), 78 deletions(-)

diff --git a/kernel/trace/blktrace.c b/kernel/trace/blktrace.c
index b3bc91a..53185b5 100644
--- a/kernel/trace/blktrace.c
+++ b/kernel/trace/blktrace.c
@@ -123,9 +123,9 @@ static void trace_note_time(struct blk_trace *bt)
 	words[0] = now.tv_sec;
 	words[1] = now.tv_nsec;
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	trace_note(bt, 0, BLK_TN_TIMESTAMP, words, sizeof(words));
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 }
 
 void __trace_note_message(struct blk_trace *bt, const char *fmt, ...)
@@ -139,14 +139,14 @@ void __trace_note_message(struct blk_trace *bt, const char *fmt, ...)
 		     !blk_tracer_enabled))
 		return;
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	buf = per_cpu_ptr(bt->msg_data, smp_processor_id());
 	va_start(args, fmt);
 	n = vscnprintf(buf, BLK_TN_MAX_MSG, fmt, args);
 	va_end(args);
 
 	trace_note(bt, 0, BLK_TN_MESSAGE, buf, n);
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 }
 EXPORT_SYMBOL_GPL(__trace_note_message);
 
@@ -224,7 +224,7 @@ static void __blk_add_trace(struct blk_trace *bt, sector_t sector, int bytes,
 	 * some space in the relay per-cpu buffer, to prevent an irq
 	 * from coming in and stepping on our toes.
 	 */
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 
 	if (unlikely(tsk->btrace_seq != blktrace_seq))
 		trace_note_tsk(bt, tsk);
@@ -262,7 +262,7 @@ record_it:
 		}
 	}
 
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 }
 
 static struct dentry *blk_tree_root;
diff --git a/kernel/trace/ftrace.c b/kernel/trace/ftrace.c
index f78744a..67f6192 100644
--- a/kernel/trace/ftrace.c
+++ b/kernel/trace/ftrace.c
@@ -613,7 +613,7 @@ function_profile_call(unsigned long ip, unsigned long parent_ip)
 	if (!ftrace_profile_enabled)
 		return;
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 
 	stat = &__get_cpu_var(ftrace_profile_stats);
 	if (!stat->hash || !ftrace_profile_enabled)
@@ -628,7 +628,7 @@ function_profile_call(unsigned long ip, unsigned long parent_ip)
 
 	rec->counter++;
  out:
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 }
 
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
@@ -645,7 +645,7 @@ static void profile_graph_return(struct ftrace_graph_ret *trace)
 	struct ftrace_profile *rec;
 	unsigned long flags;
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	stat = &__get_cpu_var(ftrace_profile_stats);
 	if (!stat->hash || !ftrace_profile_enabled)
 		goto out;
@@ -672,7 +672,7 @@ static void profile_graph_return(struct ftrace_graph_ret *trace)
 		rec->time += calltime;
 
  out:
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 }
 
 static int register_ftrace_profiler(void)
@@ -2668,9 +2668,9 @@ static int ftrace_process_locs(struct module *mod,
 		goto out;
 
 	/* disable interrupts to prevent kstop machine */
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	ftrace_update_code(mod);
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 
 	ret = ftrace_arch_module_modify_post_process();
 	FTRACE_WARN_ON(ret);
@@ -2752,9 +2752,9 @@ void __init ftrace_init(void)
 	/* Keep the ftrace pointer to the stub */
 	addr = (unsigned long)ftrace_stub;
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	ftrace_dyn_arch_init(&addr);
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 
 	/* ftrace_dyn_arch_init places the return code in addr */
 	if (addr)
diff --git a/kernel/trace/ring_buffer.c b/kernel/trace/ring_buffer.c
index 1b6197a..66ae005 100644
--- a/kernel/trace/ring_buffer.c
+++ b/kernel/trace/ring_buffer.c
@@ -2842,7 +2842,7 @@ rb_get_reader_page(struct ring_buffer_per_cpu *cpu_buffer)
 	int nr_loops = 0;
 	int ret;
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	arch_spin_lock(&cpu_buffer->lock);
 
  again:
@@ -2933,7 +2933,7 @@ rb_get_reader_page(struct ring_buffer_per_cpu *cpu_buffer)
 
  out:
 	arch_spin_unlock(&cpu_buffer->lock);
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 
 	return reader;
 }
@@ -3185,7 +3185,7 @@ ring_buffer_peek(struct ring_buffer *buffer, int cpu, u64 *ts)
 
 	dolock = rb_ok_to_lock();
  again:
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	if (dolock)
 		spin_lock(&cpu_buffer->reader_lock);
 	event = rb_buffer_peek(cpu_buffer, ts);
@@ -3193,7 +3193,7 @@ ring_buffer_peek(struct ring_buffer *buffer, int cpu, u64 *ts)
 		rb_advance_reader(cpu_buffer);
 	if (dolock)
 		spin_unlock(&cpu_buffer->reader_lock);
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 
 	if (event && event->type_len == RINGBUF_TYPE_PADDING)
 		goto again;
@@ -3247,13 +3247,13 @@ ring_buffer_consume(struct ring_buffer *buffer, int cpu, u64 *ts)
 
  again:
 	/* might be called in atomic */
-	preempt_disable();
+	preempt_disable_notrace();
 
 	if (!cpumask_test_cpu(cpu, buffer->cpumask))
 		goto out;
 
 	cpu_buffer = buffer->buffers[cpu];
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	if (dolock)
 		spin_lock(&cpu_buffer->reader_lock);
 
@@ -3263,10 +3263,10 @@ ring_buffer_consume(struct ring_buffer *buffer, int cpu, u64 *ts)
 
 	if (dolock)
 		spin_unlock(&cpu_buffer->reader_lock);
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 
  out:
-	preempt_enable();
+	preempt_enable_notrace();
 
 	if (event && event->type_len == RINGBUF_TYPE_PADDING)
 		goto again;
@@ -3474,13 +3474,13 @@ int ring_buffer_empty(struct ring_buffer *buffer)
 	/* yes this is racy, but if you don't like the race, lock the buffer */
 	for_each_buffer_cpu(buffer, cpu) {
 		cpu_buffer = buffer->buffers[cpu];
-		local_irq_save(flags);
+		raw_local_irq_save(flags);
 		if (dolock)
 			spin_lock(&cpu_buffer->reader_lock);
 		ret = rb_per_cpu_empty(cpu_buffer);
 		if (dolock)
 			spin_unlock(&cpu_buffer->reader_lock);
-		local_irq_restore(flags);
+		raw_local_irq_restore(flags);
 
 		if (!ret)
 			return 0;
@@ -3508,13 +3508,13 @@ int ring_buffer_empty_cpu(struct ring_buffer *buffer, int cpu)
 	dolock = rb_ok_to_lock();
 
 	cpu_buffer = buffer->buffers[cpu];
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	if (dolock)
 		spin_lock(&cpu_buffer->reader_lock);
 	ret = rb_per_cpu_empty(cpu_buffer);
 	if (dolock)
 		spin_unlock(&cpu_buffer->reader_lock);
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 
 	return ret;
 }
diff --git a/kernel/trace/trace.c b/kernel/trace/trace.c
index 1244f6b..375ac4a 100644
--- a/kernel/trace/trace.c
+++ b/kernel/trace/trace.c
@@ -91,14 +91,14 @@ DEFINE_PER_CPU(int, ftrace_cpu_disabled);
 
 static inline void ftrace_disable_cpu(void)
 {
-	preempt_disable();
+	preempt_disable_notrace();
 	__this_cpu_inc(ftrace_cpu_disabled);
 }
 
 static inline void ftrace_enable_cpu(void)
 {
 	__this_cpu_dec(ftrace_cpu_disabled);
-	preempt_enable();
+	preempt_enable_notrace();
 }
 
 cpumask_var_t __read_mostly	tracing_buffer_mask;
@@ -1060,7 +1060,7 @@ void trace_find_cmdline(int pid, char comm[])
 		return;
 	}
 
-	preempt_disable();
+	preempt_disable_notrace();
 	arch_spin_lock(&trace_cmdline_lock);
 	map = map_pid_to_cmdline[pid];
 	if (map != NO_CMDLINE_MAP)
@@ -1069,7 +1069,7 @@ void trace_find_cmdline(int pid, char comm[])
 		strcpy(comm, "<...>");
 
 	arch_spin_unlock(&trace_cmdline_lock);
-	preempt_enable();
+	preempt_enable_notrace();
 }
 
 void tracing_record_cmdline(struct task_struct *tsk)
@@ -1360,7 +1360,7 @@ ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3)
 		return;
 
 	pc = preempt_count();
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	cpu = raw_smp_processor_id();
 	data = tr->data[cpu];
 
@@ -1368,7 +1368,7 @@ ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3)
 		ftrace_trace_special(tr, arg1, arg2, arg3, pc);
 
 	atomic_dec(&data->disabled);
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 }
 
 /**
@@ -1408,7 +1408,7 @@ int trace_vbprintk(unsigned long ip, const char *fmt, va_list args)
 		goto out;
 
 	/* Lockdep uses trace_printk for lock tracing */
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	arch_spin_lock(&trace_buf_lock);
 	len = vbin_printf(trace_buf, TRACE_BUF_SIZE, fmt, args);
 
@@ -1433,7 +1433,7 @@ int trace_vbprintk(unsigned long ip, const char *fmt, va_list args)
 
 out_unlock:
 	arch_spin_unlock(&trace_buf_lock);
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 
 out:
 	atomic_dec_return(&data->disabled);
@@ -2402,7 +2402,7 @@ tracing_cpumask_write(struct file *filp, const char __user *ubuf,
 
 	mutex_lock(&tracing_cpumask_update_lock);
 
-	local_irq_disable();
+	raw_local_irq_disable();
 	arch_spin_lock(&ftrace_max_lock);
 	for_each_tracing_cpu(cpu) {
 		/*
@@ -2419,7 +2419,7 @@ tracing_cpumask_write(struct file *filp, const char __user *ubuf,
 		}
 	}
 	arch_spin_unlock(&ftrace_max_lock);
-	local_irq_enable();
+	raw_local_irq_enable();
 
 	cpumask_copy(tracing_cpumask, tracing_cpumask_new);
 
@@ -4402,7 +4402,7 @@ static void __ftrace_dump(bool disable_tracing)
 	int cnt = 0, cpu;
 
 	/* only one dump */
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	arch_spin_lock(&ftrace_dump_lock);
 	if (dump_ran)
 		goto out;
@@ -4476,7 +4476,7 @@ static void __ftrace_dump(bool disable_tracing)
 
  out:
 	arch_spin_unlock(&ftrace_dump_lock);
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 }
 
 /* By default: disable tracing after the dump */
diff --git a/kernel/trace/trace_boot.c b/kernel/trace/trace_boot.c
index c21d5f3..8ac4cc2 100644
--- a/kernel/trace/trace_boot.c
+++ b/kernel/trace/trace_boot.c
@@ -142,7 +142,7 @@ void trace_boot_call(struct boot_trace_call *bt, initcall_t fn)
 	 * disappear because it is in the .init section.
 	 */
 	sprint_symbol(bt->func, (unsigned long)fn);
-	preempt_disable();
+	preempt_disable_notrace();
 
 	buffer = tr->buffer;
 	event = trace_buffer_lock_reserve(buffer, TRACE_BOOT_CALL,
@@ -154,7 +154,7 @@ void trace_boot_call(struct boot_trace_call *bt, initcall_t fn)
 	if (!filter_check_discard(call, entry, buffer, event))
 		trace_buffer_unlock_commit(buffer, event, 0, 0);
  out:
-	preempt_enable();
+	preempt_enable_notrace();
 }
 
 void trace_boot_ret(struct boot_trace_ret *bt, initcall_t fn)
@@ -169,7 +169,7 @@ void trace_boot_ret(struct boot_trace_ret *bt, initcall_t fn)
 		return;
 
 	sprint_symbol(bt->func, (unsigned long)fn);
-	preempt_disable();
+	preempt_disable_notrace();
 
 	buffer = tr->buffer;
 	event = trace_buffer_lock_reserve(buffer, TRACE_BOOT_RET,
@@ -181,5 +181,5 @@ void trace_boot_ret(struct boot_trace_ret *bt, initcall_t fn)
 	if (!filter_check_discard(call, entry, buffer, event))
 		trace_buffer_unlock_commit(buffer, event, 0, 0);
  out:
-	preempt_enable();
+	preempt_enable_notrace();
 }
diff --git a/kernel/trace/trace_branch.c b/kernel/trace/trace_branch.c
index b9bc4d4..30bb21b 100644
--- a/kernel/trace/trace_branch.c
+++ b/kernel/trace/trace_branch.c
@@ -49,7 +49,7 @@ probe_likely_condition(struct ftrace_branch_data *f, int val, int expect)
 	if (unlikely(!tr))
 		return;
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	cpu = raw_smp_processor_id();
 	if (atomic_inc_return(&tr->data[cpu]->disabled) != 1)
 		goto out;
@@ -81,7 +81,7 @@ probe_likely_condition(struct ftrace_branch_data *f, int val, int expect)
 
  out:
 	atomic_dec(&tr->data[cpu]->disabled);
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 }
 
 static inline
diff --git a/kernel/trace/trace_clock.c b/kernel/trace/trace_clock.c
index 1fb5aaf..709aaa9 100644
--- a/kernel/trace/trace_clock.c
+++ b/kernel/trace/trace_clock.c
@@ -88,7 +88,7 @@ u64 notrace trace_clock_global(void)
 	int this_cpu;
 	u64 now;
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 
 	this_cpu = raw_smp_processor_id();
 	now = cpu_clock(this_cpu);
@@ -114,7 +114,7 @@ u64 notrace trace_clock_global(void)
 	arch_spin_unlock(&trace_clock_struct.lock);
 
  out:
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 
 	return now;
 }
diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 0565bb4..3395a3f 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -140,7 +140,7 @@ __kprobes void *perf_trace_buf_prepare(int size, unsigned short type,
 	pc = preempt_count();
 
 	/* Protect the per cpu buffer, begin the rcu read side */
-	local_irq_save(*irq_flags);
+	raw_local_irq_save(*irq_flags);
 
 	*rctxp = perf_swevent_get_recursion_context();
 	if (*rctxp < 0)
@@ -169,7 +169,7 @@ __kprobes void *perf_trace_buf_prepare(int size, unsigned short type,
 err:
 	perf_swevent_put_recursion_context(*rctxp);
 err_recursion:
-	local_irq_restore(*irq_flags);
+	raw_local_irq_restore(*irq_flags);
 	return NULL;
 }
 EXPORT_SYMBOL_GPL(perf_trace_buf_prepare);
diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index b3f3776..19af07d 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -91,7 +91,7 @@ function_trace_call(unsigned long ip, unsigned long parent_ip)
 	 * Need to use raw, since this must be called before the
 	 * recursive protection is performed.
 	 */
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	cpu = raw_smp_processor_id();
 	data = tr->data[cpu];
 	disabled = atomic_inc_return(&data->disabled);
@@ -102,7 +102,7 @@ function_trace_call(unsigned long ip, unsigned long parent_ip)
 	}
 
 	atomic_dec(&data->disabled);
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 }
 
 static void
@@ -122,7 +122,7 @@ function_stack_trace_call(unsigned long ip, unsigned long parent_ip)
 	 * Need to use raw, since this must be called before the
 	 * recursive protection is performed.
 	 */
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	cpu = raw_smp_processor_id();
 	data = tr->data[cpu];
 	disabled = atomic_inc_return(&data->disabled);
@@ -142,7 +142,7 @@ function_stack_trace_call(unsigned long ip, unsigned long parent_ip)
 	}
 
 	atomic_dec(&data->disabled);
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 }
 
 
diff --git a/kernel/trace/trace_functions_graph.c b/kernel/trace/trace_functions_graph.c
index 72a0d96..68075aa 100644
--- a/kernel/trace/trace_functions_graph.c
+++ b/kernel/trace/trace_functions_graph.c
@@ -221,7 +221,7 @@ int trace_graph_entry(struct ftrace_graph_ent *trace)
 	if (!(trace->depth || ftrace_graph_addr(trace->func)))
 		return 0;
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	cpu = raw_smp_processor_id();
 	data = tr->data[cpu];
 	disabled = atomic_inc_return(&data->disabled);
@@ -233,7 +233,7 @@ int trace_graph_entry(struct ftrace_graph_ent *trace)
 	}
 
 	atomic_dec(&data->disabled);
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 
 	return ret;
 }
@@ -278,7 +278,7 @@ void trace_graph_return(struct ftrace_graph_ret *trace)
 	int cpu;
 	int pc;
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	cpu = raw_smp_processor_id();
 	data = tr->data[cpu];
 	disabled = atomic_inc_return(&data->disabled);
@@ -287,7 +287,7 @@ void trace_graph_return(struct ftrace_graph_ret *trace)
 		__trace_graph_return(tr, trace, flags, pc);
 	}
 	atomic_dec(&data->disabled);
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 }
 
 void set_graph_array(struct trace_array *tr)
diff --git a/kernel/trace/trace_hw_branches.c b/kernel/trace/trace_hw_branches.c
index 7b97000..b40cb41 100644
--- a/kernel/trace/trace_hw_branches.c
+++ b/kernel/trace/trace_hw_branches.c
@@ -177,7 +177,7 @@ void trace_hw_branch(u64 from, u64 to)
 	if (unlikely(!trace_hw_branches_enabled))
 		return;
 
-	local_irq_save(irq1);
+	raw_local_irq_save(irq1);
 	cpu = raw_smp_processor_id();
 	if (atomic_inc_return(&tr->data[cpu]->disabled) != 1)
 		goto out;
@@ -197,7 +197,7 @@ void trace_hw_branch(u64 from, u64 to)
 
  out:
 	atomic_dec(&tr->data[cpu]->disabled);
-	local_irq_restore(irq1);
+	raw_local_irq_restore(irq1);
 }
 
 static void trace_bts_at(const struct bts_trace *trace, void *at)
diff --git a/kernel/trace/trace_mmiotrace.c b/kernel/trace/trace_mmiotrace.c
index 017fa37..d34f16f 100644
--- a/kernel/trace/trace_mmiotrace.c
+++ b/kernel/trace/trace_mmiotrace.c
@@ -362,10 +362,10 @@ void mmio_trace_mapping(struct mmiotrace_map *map)
 	struct trace_array *tr = mmio_trace_array;
 	struct trace_array_cpu *data;
 
-	preempt_disable();
+	preempt_disable_notrace();
 	data = tr->data[smp_processor_id()];
 	__trace_mmiotrace_map(tr, data, map);
-	preempt_enable();
+	preempt_enable_notrace();
 }
 
 int mmio_trace_printk(const char *fmt, va_list args)
diff --git a/kernel/trace/trace_sched_switch.c b/kernel/trace/trace_sched_switch.c
index 5fca0f5..2d8ef29 100644
--- a/kernel/trace/trace_sched_switch.c
+++ b/kernel/trace/trace_sched_switch.c
@@ -68,14 +68,14 @@ probe_sched_switch(struct rq *__rq, struct task_struct *prev,
 		return;
 
 	pc = preempt_count();
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	cpu = raw_smp_processor_id();
 	data = ctx_trace->data[cpu];
 
 	if (likely(!atomic_read(&data->disabled)))
 		tracing_sched_switch_trace(ctx_trace, prev, next, flags, pc);
 
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 }
 
 void
@@ -124,7 +124,7 @@ probe_sched_wakeup(struct rq *__rq, struct task_struct *wakee, int success)
 		return;
 
 	pc = preempt_count();
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	cpu = raw_smp_processor_id();
 	data = ctx_trace->data[cpu];
 
@@ -132,7 +132,7 @@ probe_sched_wakeup(struct rq *__rq, struct task_struct *wakee, int success)
 		tracing_sched_wakeup_trace(ctx_trace, wakee, current,
 					   flags, pc);
 
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 }
 
 static int tracing_sched_register(void)
diff --git a/kernel/trace/trace_sched_wakeup.c b/kernel/trace/trace_sched_wakeup.c
index 0271742..f4847ae 100644
--- a/kernel/trace/trace_sched_wakeup.c
+++ b/kernel/trace/trace_sched_wakeup.c
@@ -65,11 +65,11 @@ wakeup_tracer_call(unsigned long ip, unsigned long parent_ip)
 	if (unlikely(disabled != 1))
 		goto out;
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 
 	trace_function(tr, ip, parent_ip, flags, pc);
 
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 
  out:
 	atomic_dec(&data->disabled);
@@ -142,7 +142,7 @@ probe_wakeup_sched_switch(struct rq *rq, struct task_struct *prev,
 	if (likely(disabled != 1))
 		goto out;
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	arch_spin_lock(&wakeup_lock);
 
 	/* We could race with grabbing wakeup_lock */
@@ -170,7 +170,7 @@ probe_wakeup_sched_switch(struct rq *rq, struct task_struct *prev,
 out_unlock:
 	__wakeup_reset(wakeup_trace);
 	arch_spin_unlock(&wakeup_lock);
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 out:
 	atomic_dec(&wakeup_trace->data[cpu]->disabled);
 }
@@ -192,11 +192,11 @@ static void wakeup_reset(struct trace_array *tr)
 
 	tracing_reset_online_cpus(tr);
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	arch_spin_lock(&wakeup_lock);
 	__wakeup_reset(tr);
 	arch_spin_unlock(&wakeup_lock);
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 }
 
 static void
diff --git a/kernel/trace/trace_selftest.c b/kernel/trace/trace_selftest.c
index 81003b4..433241a 100644
--- a/kernel/trace/trace_selftest.c
+++ b/kernel/trace/trace_selftest.c
@@ -67,7 +67,7 @@ static int trace_test_buffer(struct trace_array *tr, unsigned long *count)
 	int cpu, ret = 0;
 
 	/* Don't allow flipping of max traces now */
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	arch_spin_lock(&ftrace_max_lock);
 
 	cnt = ring_buffer_entries(tr->buffer);
@@ -87,7 +87,7 @@ static int trace_test_buffer(struct trace_array *tr, unsigned long *count)
 	}
 	tracing_on();
 	arch_spin_unlock(&ftrace_max_lock);
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 
 	if (count)
 		*count = cnt;
diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index f4bc9b2..8844cb1 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -53,7 +53,7 @@ static inline void check_stack(void)
 	if (!object_is_on_stack(&this_size))
 		return;
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	arch_spin_lock(&max_stack_lock);
 
 	/* a race could have already updated it */
@@ -104,7 +104,7 @@ static inline void check_stack(void)
 
  out:
 	arch_spin_unlock(&max_stack_lock);
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 }
 
 static void
@@ -171,7 +171,7 @@ stack_max_size_write(struct file *filp, const char __user *ubuf,
 	if (ret < 0)
 		return ret;
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 
 	/*
 	 * In case we trace inside arch_spin_lock() or after (NMI),
@@ -186,7 +186,7 @@ stack_max_size_write(struct file *filp, const char __user *ubuf,
 	arch_spin_unlock(&max_stack_lock);
 
 	per_cpu(trace_active, cpu)--;
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 
 	return count;
 }
@@ -220,7 +220,7 @@ static void *t_start(struct seq_file *m, loff_t *pos)
 {
 	int cpu;
 
-	local_irq_disable();
+	raw_local_irq_disable();
 
 	cpu = smp_processor_id();
 	per_cpu(trace_active, cpu)++;
@@ -242,7 +242,7 @@ static void t_stop(struct seq_file *m, void *p)
 	cpu = smp_processor_id();
 	per_cpu(trace_active, cpu)--;
 
-	local_irq_enable();
+	raw_local_irq_enable();
 }
 
 static int trace_lookup_stack(struct seq_file *m, long i)
-- 
1.7.2.1

