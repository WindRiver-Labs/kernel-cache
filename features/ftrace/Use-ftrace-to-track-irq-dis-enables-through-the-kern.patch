From e8c11a2e9e092092b33005b8797e99256b6761c5 Mon Sep 17 00:00:00 2001
From: Matt Sherer <matt.sherer@windriver.com>
Date: Sat, 31 Jan 2009 07:41:06 -0800
Subject: [PATCH] Use ftrace to track irq dis/enables through the kernel.

This is done on top of ftrace/CONFIG_IRQSOFF_TRACER.
Track it in a normal ftrace buffer set, kept separately from the normal
CONFIG_IRQSOFF_TRACER buffer.  Also provide the ability to dump the stored
trace when an NMI occurs.

Usage:

Enable it with CONFIG_FTRACE and CONFIG_IRQSOFF_TRACER.
This adds a new tracer name, which can be activated with:

mount -t debugfs nodev /debug
echo 32 > /debug/tracing/trace_entries
echo irqtrack > /debug/tracing/current_tracer
echo 1 > /debug/tracing/tracing_enabled

cat /debug/tracing/trace # watch the current buffer sequence

The same contents of /debug/tracing/trace are dumped at the
beginning of an NMI event.
---
 arch/x86/kernel/traps_32.c   |    6 ++++
 kernel/trace/trace.c         |   22 ++++++++++++++
 kernel/trace/trace_irqsoff.c |   63 ++++++++++++++++++++++++++++++++++++++----
 3 files changed, 85 insertions(+), 6 deletions(-)

diff --git a/arch/x86/kernel/traps_32.c b/arch/x86/kernel/traps_32.c
index 5f585c1..b6d56e7 100644
--- a/arch/x86/kernel/traps_32.c
+++ b/arch/x86/kernel/traps_32.c
@@ -743,6 +743,12 @@ static DEFINE_SPINLOCK(nmi_print_lock);
 
 void notrace __kprobes die_nmi(char *str, struct pt_regs *regs, int do_panic)
 {
+
+#ifdef CONFIG_IRQSOFF_TRACER
+extern void dump_irq_trace(void);
+	dump_irq_trace();
+#endif
+
 	if (notify_die(DIE_NMIWATCHDOG, str, regs, 0, 2, SIGINT) == NOTIFY_STOP)
 		return;
 
diff --git a/kernel/trace/trace.c b/kernel/trace/trace.c
index 648bfce..321bd09 100644
--- a/kernel/trace/trace.c
+++ b/kernel/trace/trace.c
@@ -1302,6 +1302,28 @@ seq_print_sym_offset(struct trace_seq *s, const char *fmt,
 	return 1;
 }
 
+#ifdef CONFIG_IRQSOFF_TRACER
+extern struct trace_array *irqtrack_trace;
+static struct trace_iterator iter;
+void dump_irq_trace(void) {
+	int i = 0;
+	iter.tr = irqtrack_trace;
+	iter.trace = current_trace;
+	iter.pos = -1;
+	printk("#           TASK-PID    PPID CPU#    TIMESTAMP  FUNCTION\n");
+	printk("#              | |         |  |          |         |\n");
+	if (iter.trace && iter.trace->open) 
+		iter.trace->open(&iter); /* _really_ ensure tracing is off */
+	while (i < global_trace.entries) {
+		find_next_entry_inc(&iter);
+		print_trace_line(&iter);
+		printk("%s",iter.seq.buffer);
+		trace_seq_reset(&iter.seq);
+		i++;
+	}
+}
+#endif
+
 #ifndef CONFIG_64BIT
 # define IP_FMT "%08lx"
 #else
diff --git a/kernel/trace/trace_irqsoff.c b/kernel/trace/trace_irqsoff.c
index ece6cfb..1fb41cf 100644
--- a/kernel/trace/trace_irqsoff.c
+++ b/kernel/trace/trace_irqsoff.c
@@ -18,6 +18,7 @@
 
 #include "trace.h"
 
+struct trace_array			*irqtrack_trace __read_mostly;
 static struct trace_array		*irqsoff_trace __read_mostly;
 static int				tracer_enabled __read_mostly;
 
@@ -28,6 +29,7 @@ static DEFINE_SPINLOCK(max_trace_lock);
 enum {
 	TRACER_IRQS_OFF		= (1 << 1),
 	TRACER_PREEMPT_OFF	= (1 << 2),
+	TRACER_IRQ_TRACK	= (1 << 3),
 };
 
 static int trace_type __read_mostly;
@@ -46,7 +48,7 @@ preempt_trace(void)
 static inline int
 irq_trace(void)
 {
-	return ((trace_type & TRACER_IRQS_OFF) &&
+	return ((trace_type & TRACER_IRQS_OFF || trace_type & TRACER_IRQ_TRACK) &&
 		irqs_disabled());
 }
 #else
@@ -70,12 +72,17 @@ static __cacheline_aligned_in_smp	unsigned long max_sequence;
 static void
 irqsoff_tracer_call(unsigned long ip, unsigned long parent_ip)
 {
-	struct trace_array *tr = irqsoff_trace;
+	struct trace_array *tr;
 	struct trace_array_cpu *data;
 	unsigned long flags;
 	long disabled;
 	int cpu;
 
+	if (trace_type & TRACER_IRQ_TRACK) {
+		tr = irqtrack_trace;
+	} else { 
+		tr = irqsoff_trace;
+	}
 	/*
 	 * Does not matter if we preempt. We test the flags
 	 * afterward, to see if irqs are disabled or not.
@@ -173,21 +180,28 @@ out_unlock:
 out:
 	data->critical_sequence = max_sequence;
 	data->preempt_timestamp = ftrace_now(cpu);
-	tracing_reset(data);
+	if (!(trace_type & TRACER_IRQ_TRACK))
+		tracing_reset(data);
 	trace_function(tr, data, CALLER_ADDR0, parent_ip, flags);
 }
 
+extern void die_nmi(char *, struct pt_regs *, int);
 static inline void
 start_critical_timing(unsigned long ip, unsigned long parent_ip)
 {
 	int cpu;
-	struct trace_array *tr = irqsoff_trace;
+	struct trace_array *tr;
 	struct trace_array_cpu *data;
 	unsigned long flags;
 
 	if (likely(!tracer_enabled))
 		return;
 
+	if (trace_type & TRACER_IRQ_TRACK)
+		tr = irqtrack_trace;
+	else 
+		tr = irqsoff_trace;
+
 	cpu = raw_smp_processor_id();
 
 	if (per_cpu(tracing_cpu, cpu))
@@ -203,7 +217,8 @@ start_critical_timing(unsigned long ip, unsigned long parent_ip)
 	data->critical_sequence = max_sequence;
 	data->preempt_timestamp = ftrace_now(cpu);
 	data->critical_start = parent_ip ? : ip;
-	tracing_reset(data);
+	if (!(trace_type & TRACER_IRQ_TRACK))
+		tracing_reset(data);
 
 	local_save_flags(flags);
 
@@ -218,7 +233,7 @@ static inline void
 stop_critical_timing(unsigned long ip, unsigned long parent_ip)
 {
 	int cpu;
-	struct trace_array *tr = irqsoff_trace;
+	struct trace_array *tr;
 	struct trace_array_cpu *data;
 	unsigned long flags;
 
@@ -232,6 +247,11 @@ stop_critical_timing(unsigned long ip, unsigned long parent_ip)
 	if (!tracer_enabled)
 		return;
 
+	if (trace_type & TRACER_IRQ_TRACK)
+		tr = irqtrack_trace;
+	else 
+		tr = irqsoff_trace;
+
 	data = tr->data[cpu];
 
 	if (unlikely(!data) || unlikely(!head_page(data)) ||
@@ -362,6 +382,19 @@ static void stop_irqsoff_tracer(struct trace_array *tr)
 	unregister_ftrace_function(&trace_ops);
 }
 
+static void __irqtrack_tracer_init(struct trace_array *tr)
+{
+	int cpu;
+	irqtrack_trace = tr;
+	/* make sure that the tracer is visible */
+	smp_wmb();
+
+	for_each_online_cpu(cpu)
+		tracing_reset(tr->data[cpu]);
+	if (tr->ctrl)
+		start_irqsoff_tracer(tr);
+}
+
 static void __irqsoff_tracer_init(struct trace_array *tr)
 {
 	irqsoff_trace = tr;
@@ -406,6 +439,22 @@ static void irqsoff_tracer_init(struct trace_array *tr)
 
 	__irqsoff_tracer_init(tr);
 }
+static void irqtrack_tracer_init(struct trace_array *tr)
+{
+	trace_type = TRACER_IRQ_TRACK;
+
+	__irqtrack_tracer_init(tr);
+}
+static struct tracer irqtrack_tracer __read_mostly =
+{
+	.name		= "irqtrack",
+	.init		= irqtrack_tracer_init,
+	.reset		= irqsoff_tracer_reset,
+	.open		= irqsoff_tracer_open,
+	.close		= irqsoff_tracer_close,
+	.ctrl_update	= irqsoff_tracer_ctrl_update,
+};
+# define register_irqtrack(trace) register_tracer(&trace)
 static struct tracer irqsoff_tracer __read_mostly =
 {
 	.name		= "irqsoff",
@@ -422,6 +471,7 @@ static struct tracer irqsoff_tracer __read_mostly =
 # define register_irqsoff(trace) register_tracer(&trace)
 #else
 # define register_irqsoff(trace) do { } while (0)
+# define register_irqtrack(trace) do { } while (0)
 #endif
 
 #ifdef CONFIG_PREEMPT_TRACER
@@ -481,6 +531,7 @@ static struct tracer preemptirqsoff_tracer __read_mostly =
 
 __init static int init_irqsoff_tracer(void)
 {
+	register_irqsoff(irqtrack_tracer);
 	register_irqsoff(irqsoff_tracer);
 	register_preemptoff(preemptoff_tracer);
 	register_preemptirqsoff(preemptirqsoff_tracer);
-- 
1.6.0.3

