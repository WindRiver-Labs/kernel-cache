From 2727b85b56c027dec54095554a985ecd2bfe19b4 Mon Sep 17 00:00:00 2001
From: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
Date: Thu, 2 Oct 2008 14:36:35 -0400
Subject: [PATCH] lttng-instrumentation-i386

LTTng - x86_32 instrumentation

Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
CC: Thomas Gleixner <tglx@linutronix.de>
CC: Ingo Molnar <mingo@redhat.com>
CC: H. Peter Anvin <hpa@zytor.com>
---
 arch/x86/kernel/apic_32.c       |   21 +++++++++++++++++++++
 arch/x86/kernel/cpu/mcheck/p4.c |    7 +++++++
 arch/x86/kernel/process_32.c    |    6 +++++-
 arch/x86/kernel/ptrace.c        |    6 ++++++
 arch/x86/kernel/sys_i386_32.c   |    2 ++
 arch/x86/kernel/tlb_32.c        |    5 +++++
 arch/x86/kernel/traps_32.c      |   37 ++++++++++++++++++++++++++++++-------
 arch/x86/mm/fault.c             |    7 +++++++
 8 files changed, 83 insertions(+), 8 deletions(-)

diff --git a/arch/x86/kernel/apic_32.c b/arch/x86/kernel/apic_32.c
index f88bd0d..1b8cc33 100644
--- a/arch/x86/kernel/apic_32.c
+++ b/arch/x86/kernel/apic_32.c
@@ -43,6 +43,9 @@
 #include <mach_apicdef.h>
 #include <mach_ipi.h>
 
+#include <trace/irq.h>
+
+
 /*
  * Sanity check
  */
@@ -630,7 +633,13 @@ void smp_apic_timer_interrupt(struct pt_regs *regs)
 	 * interrupt lock, which is the WrongThing (tm) to do.
 	 */
 	irq_enter();
+
+	trace_irq_entry(LOCAL_TIMER_VECTOR, regs);
+
 	local_apic_timer_interrupt();
+
+	trace_irq_exit(IRQ_HANDLED);
+
 	irq_exit();
 
 	set_irq_regs(old_regs);
@@ -1275,6 +1284,9 @@ void smp_spurious_interrupt(struct pt_regs *regs)
 	unsigned long v;
 
 	irq_enter();
+
+	trace_irq_entry(SPURIOUS_APIC_VECTOR, regs);
+
 	/*
 	 * Check if this really is a spurious interrupt and ACK it
 	 * if it is a vectored one.  Just in case...
@@ -1288,6 +1300,9 @@ void smp_spurious_interrupt(struct pt_regs *regs)
 	printk(KERN_INFO "spurious APIC interrupt on CPU#%d, "
 	       "should never happen.\n", smp_processor_id());
 	__get_cpu_var(irq_stat).irq_spurious_count++;
+
+	trace_irq_exit(IRQ_HANDLED);
+
 	irq_exit();
 }
 
@@ -1299,6 +1314,9 @@ void smp_error_interrupt(struct pt_regs *regs)
 	unsigned long v, v1;
 
 	irq_enter();
+
+	trace_irq_entry(ERROR_APIC_VECTOR, regs);
+
 	/* First tickle the hardware, only then report what went on. -- REW */
 	v = apic_read(APIC_ESR);
 	apic_write(APIC_ESR, 0);
@@ -1318,6 +1336,9 @@ void smp_error_interrupt(struct pt_regs *regs)
 	*/
 	printk(KERN_DEBUG "APIC error on CPU%d: %02lx(%02lx)\n",
 		smp_processor_id(), v , v1);
+
+	trace_irq_exit(IRQ_HANDLED);
+
 	irq_exit();
 }
 
diff --git a/arch/x86/kernel/cpu/mcheck/p4.c b/arch/x86/kernel/cpu/mcheck/p4.c
index 9b60fce..80e4f08 100644
--- a/arch/x86/kernel/cpu/mcheck/p4.c
+++ b/arch/x86/kernel/cpu/mcheck/p4.c
@@ -7,6 +7,7 @@
 #include <linux/kernel.h>
 #include <linux/interrupt.h>
 #include <linux/smp.h>
+#include <trace/irq.h>
 
 #include <asm/processor.h>
 #include <asm/system.h>
@@ -60,8 +61,14 @@ static void (*vendor_thermal_interrupt)(struct pt_regs *regs) = unexpected_therm
 void smp_thermal_interrupt(struct pt_regs *regs)
 {
 	irq_enter();
+
+	trace_irq_entry(THERMAL_APIC_VECTOR, regs);
+
 	vendor_thermal_interrupt(regs);
 	__get_cpu_var(irq_stat).irq_thermal_count++;
+
+	trace_irq_exit(IRQ_HANDLED);
+
 	irq_exit();
 }
 
diff --git a/arch/x86/kernel/process_32.c b/arch/x86/kernel/process_32.c
index 31f40b2..5c5a134 100644
--- a/arch/x86/kernel/process_32.c
+++ b/arch/x86/kernel/process_32.c
@@ -233,6 +233,7 @@ extern void kernel_thread_helper(void);
 int kernel_thread(int (*fn)(void *), void * arg, unsigned long flags)
 {
 	struct pt_regs regs;
+	long pid;
 
 	memset(&regs, 0, sizeof(regs));
 
@@ -248,7 +249,10 @@ int kernel_thread(int (*fn)(void *), void * arg, unsigned long flags)
 	regs.flags = X86_EFLAGS_IF | X86_EFLAGS_SF | X86_EFLAGS_PF | 0x2;
 
 	/* Ok, create the new process.. */
-	return do_fork(flags | CLONE_VM | CLONE_UNTRACED, 0, &regs, 0, NULL, NULL);
+	pid = do_fork(flags | CLONE_VM | CLONE_UNTRACED,
+			0, &regs, 0, NULL, NULL);
+	trace_mark(kernel_arch_kthread_create, "pid %ld fn %p", pid, fn);
+	return pid;
 }
 EXPORT_SYMBOL(kernel_thread);
 
diff --git a/arch/x86/kernel/ptrace.c b/arch/x86/kernel/ptrace.c
index e37dccc..2176178 100644
--- a/arch/x86/kernel/ptrace.c
+++ b/arch/x86/kernel/ptrace.c
@@ -1426,6 +1426,12 @@ asmregparm long syscall_trace_enter(struct pt_regs *regs)
 	if (test_thread_flag(TIF_SINGLESTEP))
 		regs->flags |= X86_EFLAGS_TF;
 
+	if (!entryexit)
+		trace_mark(kernel_arch_syscall_entry, "syscall_id %d ip #p%ld",
+			(int)regs->orig_ax, instruction_pointer(regs));
+	else
+		trace_mark(kernel_arch_syscall_exit, "ret %ld", regs->ax);
+
 	/* do the secure computing check first */
 	secure_computing(regs->orig_ax);
 
diff --git a/arch/x86/kernel/sys_i386_32.c b/arch/x86/kernel/sys_i386_32.c
index 7066cb8..f505735 100644
--- a/arch/x86/kernel/sys_i386_32.c
+++ b/arch/x86/kernel/sys_i386_32.c
@@ -111,6 +111,8 @@ asmlinkage int sys_ipc(uint call, int first, int second,
 	version = call >> 16; /* hack for backward compatibility */
 	call &= 0xffff;
 
+	trace_mark(kernel_arch_ipc_call, "call %u first %d", call, first);
+
 	switch (call) {
 	case SEMOP:
 		return sys_semtimedop(first, (struct sembuf __user *)ptr, second, NULL);
diff --git a/arch/x86/kernel/tlb_32.c b/arch/x86/kernel/tlb_32.c
index fec1ece..592b9c0 100644
--- a/arch/x86/kernel/tlb_32.c
+++ b/arch/x86/kernel/tlb_32.c
@@ -1,9 +1,11 @@
 #include <linux/spinlock.h>
 #include <linux/cpu.h>
 #include <linux/interrupt.h>
+#include <trace/irq.h>
 
 #include <asm/tlbflush.h>
 
+
 DEFINE_PER_CPU(struct tlb_state, cpu_tlbstate)
 			____cacheline_aligned = { &init_mm, 0, };
 
@@ -104,6 +106,8 @@ void smp_invalidate_interrupt(struct pt_regs *regs)
 		 * BUG();
 		 */
 
+	trace_irq_entry(INVALIDATE_TLB_VECTOR, regs);
+
 	if (flush_mm == per_cpu(cpu_tlbstate, cpu).active_mm) {
 		if (per_cpu(cpu_tlbstate, cpu).state == TLBSTATE_OK) {
 			if (flush_va == TLB_FLUSH_ALL)
@@ -120,6 +124,7 @@ void smp_invalidate_interrupt(struct pt_regs *regs)
 out:
 	put_cpu_no_resched();
 	__get_cpu_var(irq_stat).irq_tlb_count++;
+	trace_irq_exit(IRQ_HANDLED);
 }
 
 void native_flush_tlb_others(const cpumask_t *cpumaskp, struct mm_struct *mm,
diff --git a/arch/x86/kernel/traps_32.c b/arch/x86/kernel/traps_32.c
index 3b07302..d5342a6 100644
--- a/arch/x86/kernel/traps_32.c
+++ b/arch/x86/kernel/traps_32.c
@@ -487,6 +487,9 @@ do_trap(int trapnr, int signr, char *str, int vm86, struct pt_regs *regs,
 {
 	struct task_struct *tsk = current;
 
+	trace_mark(kernel_arch_trap_entry, "trap_id %d ip #p%ld", trapnr,
+		instruction_pointer(regs));
+
 	if (regs->flags & X86_VM_MASK) {
 		if (vm86)
 			goto vm86_trap;
@@ -513,7 +516,7 @@ trap_signal:
 		force_sig_info(signr, info, tsk);
 	else
 		force_sig(signr, tsk);
-	return;
+	goto end;
 
 kernel_trap:
 	if (!fixup_exception(regs)) {
@@ -521,13 +524,14 @@ kernel_trap:
 		tsk->thread.trap_no = trapnr;
 		die(str, regs, error_code);
 	}
-	return;
+	goto end;
 
 vm86_trap:
 	if (handle_vm86_trap((struct kernel_vm86_regs *) regs,
 						error_code, trapnr))
 		goto trap_signal;
-	return;
+end:
+	trace_mark(kernel_arch_trap_exit, MARK_NOARGS);
 }
 
 #define DO_ERROR(trapnr, signr, str, name)				\
@@ -655,7 +659,10 @@ do_general_protection(struct pt_regs *regs, long error_code)
 		printk("\n");
 	}
 
+	trace_mark(kernel_arch_trap_entry, "trap_id %d ip #p%ld", 13,
+			instruction_pointer(regs));
 	force_sig(SIGSEGV, tsk);
+	trace_mark(kernel_arch_trap_exit, MARK_NOARGS);
 	return;
 
 gp_in_vm86:
@@ -793,27 +800,30 @@ static notrace __kprobes void default_do_nmi(struct pt_regs *regs)
 	if (!cpu)
 		reason = get_nmi_reason();
 
+	trace_mark(kernel_arch_trap_entry, "trap_id %d ip #p%ld", 2,
+		instruction_pointer(regs));
+
 	if (!(reason & 0xc0)) {
 		if (notify_die(DIE_NMI_IPI, "nmi_ipi", regs, reason, 2, SIGINT)
 								== NOTIFY_STOP)
-			return;
+			goto end;
 #ifdef CONFIG_X86_LOCAL_APIC
 		/*
 		 * Ok, so this is none of the documented NMI sources,
 		 * so it must be the NMI watchdog.
 		 */
 		if (nmi_watchdog_tick(regs, reason))
-			return;
+			goto end;
 		if (!do_nmi_callback(regs, cpu))
 			unknown_nmi_error(reason, regs);
 #else
 		unknown_nmi_error(reason, regs);
 #endif
 
-		return;
+		goto end;
 	}
 	if (notify_die(DIE_NMI, "nmi", regs, reason, 2, SIGINT) == NOTIFY_STOP)
-		return;
+		goto end;
 
 	/* AK: following checks seem to be broken on modern chipsets. FIXME */
 	if (reason & 0x80)
@@ -825,6 +835,8 @@ static notrace __kprobes void default_do_nmi(struct pt_regs *regs)
 	 * as it's edge-triggered:
 	 */
 	reassert_nmi();
+end:
+	trace_mark(kernel_arch_trap_exit, MARK_NOARGS);
 }
 
 notrace __kprobes void do_nmi(struct pt_regs *regs, long error_code)
@@ -944,7 +956,10 @@ void __kprobes do_debug(struct pt_regs *regs, long error_code)
 	}
 
 	/* Ok, finally something we can handle */
+	trace_mark(kernel_arch_trap_entry, "trap_id %d ip #p%ld",
+			1, instruction_pointer(regs));
 	send_sigtrap(tsk, regs, error_code);
+	trace_mark(kernel_arch_trap_exit, MARK_NOARGS);
 
 	/*
 	 * Disable additional traps. They'll be re-enabled when
@@ -955,7 +970,10 @@ clear_dr7:
 	return;
 
 debug_vm86:
+	trace_mark(kernel_arch_trap_entry, "trap_id %d ip #p%ld",
+			1, instruction_pointer(regs));
 	handle_vm86_trap((struct kernel_vm86_regs *) regs, error_code, 1);
+	trace_mark(kernel_arch_trap_exit, MARK_NOARGS);
 	return;
 
 clear_TF_reenable:
@@ -1106,10 +1124,13 @@ void do_simd_coprocessor_error(struct pt_regs *regs, long error_code)
 
 void do_spurious_interrupt_bug(struct pt_regs *regs, long error_code)
 {
+	trace_mark(kernel_arch_trap_entry, "trap_id %d ip #p%ld",
+		16, instruction_pointer(regs));
 #if 0
 	/* No need to warn about this any longer. */
 	printk(KERN_INFO "Ignoring P6 Local APIC Spurious Interrupt Bug...\n");
 #endif
+	trace_mark(kernel_arch_trap_exit, MARK_NOARGS);
 }
 
 unsigned long patch_espfix_desc(unsigned long uesp, unsigned long kesp)
@@ -1175,8 +1196,10 @@ asmlinkage void math_emulate(long arg)
 	printk(KERN_EMERG
 		"math-emulation not enabled and no coprocessor found.\n");
 	printk(KERN_EMERG "killing %s.\n", current->comm);
+	trace_mark(kernel_arch_trap_entry, "trap_id %d ip #p%ld", 7, 0L);
 	force_sig(SIGFPE, current);
 	schedule();
+	trace_mark(kernel_arch_trap_exit, MARK_NOARGS);
 }
 
 #endif /* CONFIG_MATH_EMULATION */
diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c
index ac4c918..b102d70 100644
--- a/arch/x86/mm/fault.c
+++ b/arch/x86/mm/fault.c
@@ -753,7 +753,10 @@ survive:
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
+	trace_mark(kernel_arch_trap_entry, "trap_id %d ip #p%ld",
+		14, instruction_pointer(regs));
 	fault = handle_mm_fault(mm, vma, address, write);
+	trace_mark(kernel_arch_trap_exit, MARK_NOARGS);
 	if (unlikely(fault & VM_FAULT_ERROR)) {
 		if (fault & VM_FAULT_OOM)
 			goto out_of_memory;
@@ -794,6 +797,9 @@ bad_area_nosemaphore:
 		 */
 		local_irq_enable();
 
+		trace_mark(kernel_arch_trap_entry, "trap_id %d ip #p%ld",
+			14, instruction_pointer(regs));
+
 		/*
 		 * Valid to do another page fault here because this one came
 		 * from user space.
@@ -820,6 +826,7 @@ bad_area_nosemaphore:
 		tsk->thread.error_code = error_code | (address >= TASK_SIZE);
 		tsk->thread.trap_no = 14;
 		force_sig_info_fault(SIGSEGV, si_code, address, tsk);
+		trace_mark(kernel_arch_trap_exit, MARK_NOARGS);
 		return;
 	}
 
-- 
1.5.5.1

