From 44011d9d7c541e8a6fad59113351118ae5acbe8f Mon Sep 17 00:00:00 2001
From: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
Date: Thu, 30 Oct 2008 23:26:55 -0400
Subject: [PATCH] lttng-relay-alloc-uninline

LTTng relay alloc uninline

ltt_relay_write, read.... are too big to be inlined. Uninline them.

Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
---
 include/linux/ltt-relay.h |  206 ++--------------------------------------
 ltt/ltt-relay-alloc.c     |  233 +++++++++++++++++++++++++++++++++++++++++++++
 2 files changed, 241 insertions(+), 198 deletions(-)

diff --git a/include/linux/ltt-relay.h b/include/linux/ltt-relay.h
index 221a8b4..1225a6a 100644
--- a/include/linux/ltt-relay.h
+++ b/include/linux/ltt-relay.h
@@ -140,192 +140,14 @@ struct rchan_callbacks {
 	int (*remove_buf_file)(struct dentry *dentry);
 };
 
-/*
- * Start iteration at the previous element. Skip the real list head.
- */
-static inline struct buf_page *ltt_relay_find_prev_page(struct rchan_buf *buf,
-	struct buf_page *page, size_t offset, ssize_t diff_offset)
-{
-	struct buf_page *iter;
-	size_t orig_iter_off;
-	unsigned int i = 0;
-
-	orig_iter_off = page->offset;
-	list_for_each_entry_reverse(iter, &page->list, list) {
-		/*
-		 * Skip the real list head.
-		 */
-		if (&iter->list == &buf->pages)
-			continue;
-		i++;
-		if (offset >= iter->offset
-			&& offset < iter->offset + PAGE_SIZE) {
-#ifdef CONFIG_LTT_RELAY_CHECK_RANDOM_ACCESS
-			if (i > 1) {
-				printk(KERN_WARNING
-					"Backward random access detected in "
-					"ltt_relay. Iterations %u, "
-					"offset %zu, orig iter->off %zu, "
-					"iter->off %zu diff_offset %zd.\n", i,
-					offset, orig_iter_off, iter->offset,
-					diff_offset);
-				WARN_ON(1);
-			}
-#endif
-			return iter;
-		}
-	}
-	return NULL;
-}
-
-/*
- * Start iteration at the next element. Skip the real list head.
- */
-static inline struct buf_page *ltt_relay_find_next_page(struct rchan_buf *buf,
-	struct buf_page *page, size_t offset, ssize_t diff_offset)
-{
-	struct buf_page *iter;
-	unsigned int i = 0;
-	size_t orig_iter_off;
-
-	orig_iter_off = page->offset;
-	list_for_each_entry(iter, &page->list, list) {
-		/*
-		 * Skip the real list head.
-		 */
-		if (&iter->list == &buf->pages)
-			continue;
-		i++;
-		if (offset >= iter->offset
-			&& offset < iter->offset + PAGE_SIZE) {
-#ifdef CONFIG_LTT_RELAY_CHECK_RANDOM_ACCESS
-			if (i > 1) {
-				printk(KERN_WARNING
-					"Forward random access detected in "
-					"ltt_relay. Iterations %u, "
-					"offset %zu, orig iter->off %zu, "
-					"iter->off %zu diff_offset %zd.\n", i,
-					offset, orig_iter_off, iter->offset,
-					diff_offset);
-				WARN_ON(1);
-			}
-#endif
-			return iter;
-		}
-	}
-	return NULL;
-}
-
-/*
- * Find the page containing "offset". Cache it if it is after the currently
- * cached page.
- */
-static inline struct buf_page *ltt_relay_cache_page(struct rchan_buf *buf,
-		struct buf_page **page_cache,
-		struct buf_page *page, size_t offset)
-{
-	ssize_t diff_offset;
-	ssize_t half_buf_size = buf->chan->alloc_size >> 1;
+extern int ltt_relay_write(struct rchan_buf *buf, size_t offset,
+	const void *src, size_t len);
 
-	/*
-	 * Make sure this is the page we want to write into. The current
-	 * page is changed concurrently by other writers. [wrh]page are
-	 * used as a cache remembering the last page written
-	 * to/read/looked up for header address. No synchronization;
-	 * could have to find the previous page is a nested write
-	 * occured. Finding the right page is done by comparing the
-	 * dest_offset with the buf_page offsets.
-	 * When at the exact opposite of the buffer, bias towards forward search
-	 * because it will be cached.
-	 */
-
-	diff_offset = (ssize_t)offset - (ssize_t)page->offset;
-	if (diff_offset <= -(ssize_t)half_buf_size)
-		diff_offset += buf->chan->alloc_size;
-	else if (diff_offset > half_buf_size)
-		diff_offset -= buf->chan->alloc_size;
-
-	if (unlikely(diff_offset >= (ssize_t)PAGE_SIZE)) {
-		page = ltt_relay_find_next_page(buf, page, offset, diff_offset);
-		WARN_ON(!page);
-		*page_cache = page;
-	} else if (unlikely(diff_offset < 0)) {
-		page = ltt_relay_find_prev_page(buf, page, offset, diff_offset);
-		WARN_ON(!page);
-	}
-	return page;
-}
-
-static inline int ltt_relay_write(struct rchan_buf *buf, size_t offset,
-	const void *src, size_t len)
-{
-	struct buf_page *page;
-	ssize_t pagecpy, orig_len;
+extern int ltt_relay_read(struct rchan_buf *buf, size_t offset,
+	void *dest, size_t len);
 
-	orig_len = len;
-	offset &= buf->chan->alloc_size - 1;
-	page = buf->wpage;
-	if (unlikely(!len))
-		return 0;
-	for (;;) {
-		page = ltt_relay_cache_page(buf, &buf->wpage, page, offset);
-		pagecpy = min_t(size_t, len, PAGE_SIZE - (offset & ~PAGE_MASK));
-		memcpy(page_address(page->page)
-			+ (offset & ~PAGE_MASK), src, pagecpy);
-		len -= pagecpy;
-		if (likely(!len))
-			break;
-		src += pagecpy;
-		offset += pagecpy;
-		/*
-		 * Underlying layer should never ask for writes across
-		 * subbuffers.
-		 */
-		WARN_ON(offset >= buf->chan->alloc_size);
-	}
-	return orig_len;
-}
-
-static inline int ltt_relay_read(struct rchan_buf *buf, size_t offset,
-	void *dest, size_t len)
-{
-	struct buf_page *page;
-	ssize_t pagecpy, orig_len;
-
-	orig_len = len;
-	offset &= buf->chan->alloc_size - 1;
-	page = buf->rpage;
-	if (unlikely(!len))
-		return 0;
-	for (;;) {
-		page = ltt_relay_cache_page(buf, &buf->rpage, page, offset);
-		pagecpy = min_t(size_t, len, PAGE_SIZE - (offset & ~PAGE_MASK));
-		memcpy(dest, page_address(page->page) + (offset & ~PAGE_MASK),
-			pagecpy);
-		len -= pagecpy;
-		if (likely(!len))
-			break;
-		dest += pagecpy;
-		offset += pagecpy;
-		/*
-		 * Underlying layer should never ask for reads across
-		 * subbuffers.
-		 */
-		WARN_ON(offset >= buf->chan->alloc_size);
-	}
-	return orig_len;
-}
-
-static inline struct buf_page *ltt_relay_read_get_page(struct rchan_buf *buf,
-	size_t offset)
-{
-	struct buf_page *page;
-
-	offset &= buf->chan->alloc_size - 1;
-	page = buf->rpage;
-	page = ltt_relay_cache_page(buf, &buf->rpage, page, offset);
-	return page;
-}
+extern struct buf_page *ltt_relay_read_get_page(struct rchan_buf *buf,
+	size_t offset);
 
 /*
  * Return the address where a given offset is located.
@@ -333,20 +155,8 @@ static inline struct buf_page *ltt_relay_read_get_page(struct rchan_buf *buf,
  * it's never on a page boundary, it's safe to write directly to this address,
  * as long as the write is never bigger than a page size.
  */
-static inline void *ltt_relay_offset_address(struct rchan_buf *buf,
-	size_t offset)
-{
-	struct buf_page *page;
-	unsigned int odd;
-
-	offset &= buf->chan->alloc_size - 1;
-	odd = !!(offset & buf->chan->subbuf_size);
-	page = buf->hpage[odd];
-	if (offset < page->offset || offset >= page->offset + PAGE_SIZE)
-		buf->hpage[odd] = page = buf->wpage;
-	page = ltt_relay_cache_page(buf, &buf->hpage[odd], page, offset);
-	return page_address(page->page) + (offset & ~PAGE_MASK);
-}
+extern void *ltt_relay_offset_address(struct rchan_buf *buf,
+	size_t offset);
 
 /*
  * CONFIG_LTT_RELAY kernel API, ltt/ltt-relay-alloc.c
diff --git a/ltt/ltt-relay-alloc.c b/ltt/ltt-relay-alloc.c
index 990ed11..0d35d5f 100644
--- a/ltt/ltt-relay-alloc.c
+++ b/ltt/ltt-relay-alloc.c
@@ -421,6 +421,239 @@ void ltt_relay_close(struct rchan *chan)
 }
 EXPORT_SYMBOL_GPL(ltt_relay_close);
 
+/*
+ * Start iteration at the previous element. Skip the real list head.
+ */
+static struct buf_page *ltt_relay_find_prev_page(struct rchan_buf *buf,
+	struct buf_page *page, size_t offset, ssize_t diff_offset)
+{
+	struct buf_page *iter;
+	size_t orig_iter_off;
+	unsigned int i = 0;
+
+	orig_iter_off = page->offset;
+	list_for_each_entry_reverse(iter, &page->list, list) {
+		/*
+		 * Skip the real list head.
+		 */
+		if (&iter->list == &buf->pages)
+			continue;
+		i++;
+		if (offset >= iter->offset
+			&& offset < iter->offset + PAGE_SIZE) {
+#ifdef CONFIG_LTT_RELAY_CHECK_RANDOM_ACCESS
+			if (i > 1) {
+				printk(KERN_WARNING
+					"Backward random access detected in "
+					"ltt_relay. Iterations %u, "
+					"offset %zu, orig iter->off %zu, "
+					"iter->off %zu diff_offset %zd.\n", i,
+					offset, orig_iter_off, iter->offset,
+					diff_offset);
+				WARN_ON(1);
+			}
+#endif
+			return iter;
+		}
+	}
+	return NULL;
+}
+
+/*
+ * Start iteration at the next element. Skip the real list head.
+ */
+static struct buf_page *ltt_relay_find_next_page(struct rchan_buf *buf,
+	struct buf_page *page, size_t offset, ssize_t diff_offset)
+{
+	struct buf_page *iter;
+	unsigned int i = 0;
+	size_t orig_iter_off;
+
+	orig_iter_off = page->offset;
+	list_for_each_entry(iter, &page->list, list) {
+		/*
+		 * Skip the real list head.
+		 */
+		if (&iter->list == &buf->pages)
+			continue;
+		i++;
+		if (offset >= iter->offset
+			&& offset < iter->offset + PAGE_SIZE) {
+#ifdef CONFIG_LTT_RELAY_CHECK_RANDOM_ACCESS
+			if (i > 1) {
+				printk(KERN_WARNING
+					"Forward random access detected in "
+					"ltt_relay. Iterations %u, "
+					"offset %zu, orig iter->off %zu, "
+					"iter->off %zu diff_offset %zd.\n", i,
+					offset, orig_iter_off, iter->offset,
+					diff_offset);
+				WARN_ON(1);
+			}
+#endif
+			return iter;
+		}
+	}
+	return NULL;
+}
+
+/*
+ * Find the page containing "offset". Cache it if it is after the currently
+ * cached page.
+ */
+static struct buf_page *ltt_relay_cache_page(struct rchan_buf *buf,
+		struct buf_page **page_cache,
+		struct buf_page *page, size_t offset)
+{
+	ssize_t diff_offset;
+	ssize_t half_buf_size = buf->chan->alloc_size >> 1;
+
+	/*
+	 * Make sure this is the page we want to write into. The current
+	 * page is changed concurrently by other writers. [wrh]page are
+	 * used as a cache remembering the last page written
+	 * to/read/looked up for header address. No synchronization;
+	 * could have to find the previous page is a nested write
+	 * occured. Finding the right page is done by comparing the
+	 * dest_offset with the buf_page offsets.
+	 * When at the exact opposite of the buffer, bias towards forward search
+	 * because it will be cached.
+	 */
+
+	diff_offset = (ssize_t)offset - (ssize_t)page->offset;
+	if (diff_offset <= -(ssize_t)half_buf_size)
+		diff_offset += buf->chan->alloc_size;
+	else if (diff_offset > half_buf_size)
+		diff_offset -= buf->chan->alloc_size;
+
+	if (unlikely(diff_offset >= (ssize_t)PAGE_SIZE)) {
+		page = ltt_relay_find_next_page(buf, page, offset, diff_offset);
+		WARN_ON(!page);
+		*page_cache = page;
+	} else if (unlikely(diff_offset < 0)) {
+		page = ltt_relay_find_prev_page(buf, page, offset, diff_offset);
+		WARN_ON(!page);
+	}
+	return page;
+}
+
+/**
+ * ltt_relay_write : write data to a ltt_relay buffer.
+ * @buf : buffer
+ * @offset : offset within the buffer
+ * @src : source address
+ * @len : length to write
+ */
+int ltt_relay_write(struct rchan_buf *buf, size_t offset,
+	const void *src, size_t len)
+{
+	struct buf_page *page;
+	ssize_t pagecpy, orig_len;
+
+	orig_len = len;
+	offset &= buf->chan->alloc_size - 1;
+	page = buf->wpage;
+	if (unlikely(!len))
+		return 0;
+	for (;;) {
+		page = ltt_relay_cache_page(buf, &buf->wpage, page, offset);
+		pagecpy = min_t(size_t, len, PAGE_SIZE - (offset & ~PAGE_MASK));
+		memcpy(page_address(page->page)
+			+ (offset & ~PAGE_MASK), src, pagecpy);
+		len -= pagecpy;
+		if (likely(!len))
+			break;
+		src += pagecpy;
+		offset += pagecpy;
+		/*
+		 * Underlying layer should never ask for writes across
+		 * subbuffers.
+		 */
+		WARN_ON(offset >= buf->chan->alloc_size);
+	}
+	return orig_len;
+}
+EXPORT_SYMBOL_GPL(ltt_relay_write);
+
+/**
+ * ltt_relay_read : read data from ltt_relay_buffer.
+ * @buf : buffer
+ * @offset : offset within the buffer
+ * @dest : destination address
+ * @len : length to write
+ */
+int ltt_relay_read(struct rchan_buf *buf, size_t offset,
+	void *dest, size_t len)
+{
+	struct buf_page *page;
+	ssize_t pagecpy, orig_len;
+
+	orig_len = len;
+	offset &= buf->chan->alloc_size - 1;
+	page = buf->rpage;
+	if (unlikely(!len))
+		return 0;
+	for (;;) {
+		page = ltt_relay_cache_page(buf, &buf->rpage, page, offset);
+		pagecpy = min_t(size_t, len, PAGE_SIZE - (offset & ~PAGE_MASK));
+		memcpy(dest, page_address(page->page) + (offset & ~PAGE_MASK),
+			pagecpy);
+		len -= pagecpy;
+		if (likely(!len))
+			break;
+		dest += pagecpy;
+		offset += pagecpy;
+		/*
+		 * Underlying layer should never ask for reads across
+		 * subbuffers.
+		 */
+		WARN_ON(offset >= buf->chan->alloc_size);
+	}
+	return orig_len;
+}
+EXPORT_SYMBOL_GPL(ltt_relay_read);
+
+/**
+ * ltt_relay_read_get_page : Get a whole page to read from
+ * @buf : buffer
+ * @offset : offset within the buffer
+ */
+struct buf_page *ltt_relay_read_get_page(struct rchan_buf *buf, size_t offset)
+{
+	struct buf_page *page;
+
+	offset &= buf->chan->alloc_size - 1;
+	page = buf->rpage;
+	page = ltt_relay_cache_page(buf, &buf->rpage, page, offset);
+	return page;
+}
+EXPORT_SYMBOL_GPL(ltt_relay_read_get_page);
+
+/**
+ * ltt_relay_offset_address : get address of a location within the buffer
+ * @buf : buffer
+ * @offset : offset within the buffer.
+ *
+ * Return the address where a given offset is located.
+ * Should be used to get the current subbuffer header pointer. Given we know
+ * it's never on a page boundary, it's safe to write directly to this address,
+ * as long as the write is never bigger than a page size.
+ */
+void *ltt_relay_offset_address(struct rchan_buf *buf, size_t offset)
+{
+	struct buf_page *page;
+	unsigned int odd;
+
+	offset &= buf->chan->alloc_size - 1;
+	odd = !!(offset & buf->chan->subbuf_size);
+	page = buf->hpage[odd];
+	if (offset < page->offset || offset >= page->offset + PAGE_SIZE)
+		buf->hpage[odd] = page = buf->wpage;
+	page = ltt_relay_cache_page(buf, &buf->hpage[odd], page, offset);
+	return page_address(page->page) + (offset & ~PAGE_MASK);
+}
+EXPORT_SYMBOL_GPL(ltt_relay_offset_address);
+
 /**
  *	relay_file_open - open file op for relay files
  *	@inode: the inode
-- 
1.5.5.1

