From 90108d2f54a45b128eb14cd11864c9b8f99fda5a Mon Sep 17 00:00:00 2001
From: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Date: Thu, 13 May 2010 19:27:23 -0400
Subject: [PATCH 238/390] lttng-relay-lockless-turn-add-return-into-simple-add

lttng lockless turn local_add_return into local_add

Remove unneeded heavyweight atomic instruction, which also means that we can
simply use a prefetchw rather than prefetch of the commit count.

On Xeon, probe time goes from 245 cycles down to 240 cycles.

Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
---
 ltt/ltt-relay-lockless.c |   64 ++++++++++++++--------------
 ltt/ltt-relay-lockless.h |  107 ++++++++++++++++++++++++++++++++++++++--------
 2 files changed, 121 insertions(+), 50 deletions(-)

diff --git a/ltt/ltt-relay-lockless.c b/ltt/ltt-relay-lockless.c
index 575cfc8..9089169 100644
--- a/ltt/ltt-relay-lockless.c
+++ b/ltt/ltt-relay-lockless.c
@@ -67,7 +67,7 @@
 struct ltt_reserve_switch_offsets {
 	long begin, end, old;
 	long begin_switch, end_switch_current, end_switch_old;
-	long commit_count, reserve_commit_diff;
+	long reserve_commit_diff;
 	size_t before_hdr_pad, size;
 };
 
@@ -816,7 +816,6 @@ static int ltt_relay_create_buffer(struct ltt_trace_struct *trace,
 		local_set(&ltt_buf->commit_count[j], 0);
 	init_waitqueue_head(&ltt_buf->write_wait);
 	init_waitqueue_head(&ltt_buf->read_wait);
-	atomic_set(&ltt_buf->wakeup_readers, 0);
 	spin_lock_init(&ltt_buf->full_lock);
 
 	ltt_buffer_begin(buf, trace->start_tsc, 0);
@@ -958,10 +957,9 @@ static void ltt_relay_async_wakeup_chan(struct ltt_channel_struct *ltt_channel)
 			continue;
 
 		ltt_buf = rchan->buf[i]->chan_private;
-		if (atomic_read(&ltt_buf->wakeup_readers) == 1) {
-			atomic_set(&ltt_buf->wakeup_readers, 0);
+		if (ltt_poll_deliver(ltt_channel, ltt_buf,
+				     rchan, rchan->buf[i]))
 			wake_up_interruptible(&ltt_buf->read_wait);
-		}
 	}
 }
 
@@ -1186,20 +1184,20 @@ static void ltt_reserve_switch_old_subbuf(
 		struct ltt_reserve_switch_offsets *offsets, u64 *tsc)
 {
 	long oldidx = SUBBUF_INDEX(offsets->old - 1, rchan);
+	long commit_count, padding_size;
 
+	padding_size = rchan->subbuf_size
+			- (SUBBUF_OFFSET(offsets->old - 1, rchan) + 1);
 	ltt_buffer_end(buf, *tsc, offsets->old, oldidx);
 	/* Must write buffer end before incrementing commit count */
 	smp_wmb();
-	offsets->commit_count =
-		local_add_return(rchan->subbuf_size
-				 - (SUBBUF_OFFSET(offsets->old - 1, rchan)
-				 + 1),
-				 &ltt_buf->commit_count[oldidx]);
-	if (likely((BUFFER_TRUNC(offsets->old - 1, rchan)
-			>> ltt_channel->n_subbufs_order)
-			- ((offsets->commit_count - rchan->subbuf_size)
-				& ltt_channel->commit_count_mask) == 0))
-		ltt_deliver(buf, oldidx, offsets->commit_count);
+	local_add(padding_size,
+		  &ltt_buf->commit_count[oldidx]);
+	commit_count = local_read(&ltt_buf->commit_count[oldidx]);
+	ltt_check_deliver(ltt_channel, ltt_buf, rchan, buf,
+		offsets->old - 1, commit_count, oldidx);
+	ltt_write_commit_counter(buf, ltt_buf, oldidx,
+		offsets->old, commit_count, padding_size);
 }
 
 /*
@@ -1216,18 +1214,19 @@ static void ltt_reserve_switch_new_subbuf(
 		struct ltt_reserve_switch_offsets *offsets, u64 *tsc)
 {
 	long beginidx = SUBBUF_INDEX(offsets->begin, rchan);
+	long commit_count;
 
 	ltt_buffer_begin(buf, *tsc, beginidx);
 	/* Must write buffer end before incrementing commit count */
 	smp_wmb();
-	offsets->commit_count = local_add_return(ltt_subbuffer_header_size(),
-			&ltt_buf->commit_count[beginidx]);
+	local_add(ltt_subbuffer_header_size(),
+		  &ltt_buf->commit_count[beginidx]);
+	commit_count = local_read(&ltt_buf->commit_count[beginidx]);
 	/* Check if the written buffer has to be delivered */
-	if (unlikely((BUFFER_TRUNC(offsets->begin, rchan)
-			>> ltt_channel->n_subbufs_order)
-			- ((offsets->commit_count - rchan->subbuf_size)
-				& ltt_channel->commit_count_mask) == 0))
-		ltt_deliver(buf, beginidx, offsets->commit_count);
+	ltt_check_deliver(ltt_channel, ltt_buf, rchan, buf,
+		offsets->begin, commit_count, beginidx);
+	ltt_write_commit_counter(buf, ltt_buf, beginidx,
+		offsets->begin, commit_count, ltt_subbuffer_header_size());
 }
 
 
@@ -1256,20 +1255,21 @@ static void ltt_reserve_end_switch_current(
 		struct ltt_reserve_switch_offsets *offsets, u64 *tsc)
 {
 	long endidx = SUBBUF_INDEX(offsets->end - 1, rchan);
+	long commit_count, padding_size;
+
+	padding_size = rchan->subbuf_size
+			- (SUBBUF_OFFSET(offsets->end - 1, rchan) + 1);
 
 	ltt_buffer_end(buf, *tsc, offsets->end, endidx);
 	/* Must write buffer begin before incrementing commit count */
 	smp_wmb();
-	offsets->commit_count =
-		local_add_return(rchan->subbuf_size
-				 - (SUBBUF_OFFSET(offsets->end - 1, rchan)
-				 + 1),
-				 &ltt_buf->commit_count[endidx]);
-	if (likely((BUFFER_TRUNC(offsets->end - 1, rchan)
-			>> ltt_channel->n_subbufs_order)
-			- ((offsets->commit_count - rchan->subbuf_size)
-				& ltt_channel->commit_count_mask) == 0))
-		ltt_deliver(buf, endidx, offsets->commit_count);
+	local_add(padding_size,
+		  &ltt_buf->commit_count[endidx]);
+	commit_count = local_read(&ltt_buf->commit_count[endidx]);
+	ltt_check_deliver(ltt_channel, ltt_buf, rchan, buf,
+		offsets->end - 1, commit_count, endidx);
+	ltt_write_commit_counter(buf, ltt_buf, endidx,
+		offsets->end, commit_count, padding_size);
 }
 
 /*
diff --git a/ltt/ltt-relay-lockless.h b/ltt/ltt-relay-lockless.h
index 1970dfa..6dff060 100644
--- a/ltt/ltt-relay-lockless.h
+++ b/ltt/ltt-relay-lockless.h
@@ -97,7 +97,6 @@ struct ltt_channel_buf_struct {
 					 * Wait queue for blocking user space
 					 * writers
 					 */
-	atomic_t wakeup_readers;	/* Boolean : wakeup readers waiting ? */
 	wait_queue_head_t read_wait;	/* reader wait queue */
 	unsigned int finalized;		/* buffer has been finalized */
 	struct timer_list switch_timer;	/* timer for periodical switch */
@@ -159,16 +158,72 @@ static __inline__ int last_tsc_overflow(struct ltt_channel_buf_struct *ltt_buf,
 }
 #endif
 
-static __inline__ void ltt_deliver(struct rchan_buf *buf,
-		unsigned int subbuf_idx,
-		long commit_count)
-{
-	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
-
 #ifdef CONFIG_LTT_VMCORE
-	local_set(&ltt_buf->commit_seq[subbuf_idx], commit_count);
+static __inline__ void ltt_check_deliver(struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf,
+		struct rchan *rchan,
+		struct rchan_buf *buf,
+		long offset, long commit_count, long idx)
+{
+	/* Check if all commits have been done */
+	if (unlikely((BUFFER_TRUNC(offset, rchan)
+			>> ltt_channel->n_subbufs_order)
+			- ((commit_count - rchan->subbuf_size)
+			   & ltt_channel->commit_count_mask) == 0)) {
+		local_set(&ltt_buf->commit_seq[idx], commit_count);
+	}
+}
+#else
+static __inline__ void ltt_check_deliver(struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf,
+		struct rchan *rchan,
+		struct rchan_buf *buf,
+		long offset, long commit_count, long idx)
+{
+}
 #endif
-	atomic_set(&ltt_buf->wakeup_readers, 1);
+
+static __inline__ int ltt_poll_deliver(struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf,
+		struct rchan *rchan,
+		struct rchan_buf *buf)
+{
+	long consumed_old, consumed_idx, commit_count, write_offset;
+
+	consumed_old = atomic_long_read(&ltt_buf->consumed);
+	consumed_idx = SUBBUF_INDEX(consumed_old, buf->chan);
+	commit_count = local_read(&ltt_buf->commit_count[consumed_idx]);
+	/*
+	 * No memory barrier here, since we are only interested
+	 * in a statistically correct polling result. The next poll will
+	 * get the data is we are racing. The mb() that ensures correct
+	 * memory order is in get_subbuf.
+	 */
+	write_offset = local_read(&ltt_buf->offset);
+
+	/*
+	 * Check that the subbuffer we are trying to consume has been
+	 * already fully committed.
+	 */
+
+	if (((commit_count - rchan->subbuf_size)
+	     & ltt_channel->commit_count_mask)
+	    - (BUFFER_TRUNC(consumed_old, buf->chan)
+	       >> ltt_channel->n_subbufs_order)
+	    != 0)
+		return 0;
+
+	/*
+	 * Check that we are not about to read the same subbuffer in
+	 * which the writer head is.
+	 */
+	if ((SUBBUF_TRUNC(write_offset, buf->chan)
+	   - SUBBUF_TRUNC(consumed_old, buf->chan))
+	   == 0)
+		return 0;
+
+	return 1;
+
 }
 
 /*
@@ -188,9 +243,11 @@ static __inline__ int ltt_relay_try_reserve(
 
 	*tsc = trace_clock_read64();
 
-	prefetch(&ltt_buf->commit_count[SUBBUF_INDEX(*o_begin, rchan)]);
 #ifdef CONFIG_LTT_VMCORE
+	prefetch(&ltt_buf->commit_count[SUBBUF_INDEX(*o_begin, rchan)]);
 	prefetch(&ltt_buf->commit_seq[SUBBUF_INDEX(*o_begin, rchan)]);
+#else
+	prefetchw(&ltt_buf->commit_count[SUBBUF_INDEX(*o_begin, rchan)]);
 #endif
 	if (last_tsc_overflow(ltt_buf, *tsc))
 		*rflags = LTT_RFLAG_ID_SIZE_TSC;
@@ -345,14 +402,28 @@ static __inline__ void ltt_commit_slot(
 
 	/* Must write slot data before incrementing commit count */
 	smp_wmb();
-	commit_count = local_add_return(slot_size,
-		&ltt_buf->commit_count[endidx]);
-	/* Check if all commits have been done */
-	if (unlikely((BUFFER_TRUNC(offset_end - 1, rchan)
-			>> ltt_channel->n_subbufs_order)
-			- ((commit_count - rchan->subbuf_size)
-			   & ltt_channel->commit_count_mask) == 0))
-		ltt_deliver(buf, endidx, commit_count);
+	local_add(slot_size, &ltt_buf->commit_count[endidx]);
+	/*
+	 * commit count read can race with concurrent OOO commit count updates.
+	 * This is only needed for ltt_check_deliver (for non-polling delivery
+	 * only) and for ltt_write_commit_counter. The race can only cause the
+	 * counter to be read with the same value more than once, which could
+	 * cause :
+	 * - Multiple delivery for the same sub-buffer (which is handled
+	 *   gracefully by the reader code) if the value is for a full
+	 *   sub-buffer. It's important that we can never miss a sub-buffer
+	 *   delivery. Re-reading the value after the local_add ensures this.
+	 * - Reading a commit_count with a higher value that what was actually
+	 *   added to it for the ltt_write_commit_counter call (again caused by
+	 *   a concurrent committer). It does not matter, because this function
+	 *   is interested in the fact that the commit count reaches back the
+	 *   reserve offset for a specific sub-buffer, which is completely
+	 *   independent of the order.
+	 */
+	commit_count = local_read(&ltt_buf->commit_count[endidx]);
+
+	ltt_check_deliver(ltt_channel, ltt_buf, rchan, buf,
+		offset_end - 1, commit_count, endidx);
 	/*
 	 * Update lost_size for each commit. It's needed only for extracting
 	 * ltt buffers from vmcore, after crash.
-- 
1.6.5.2

