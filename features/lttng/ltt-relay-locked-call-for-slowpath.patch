From 4b4f473e0fe6f72cc9aa450a7f441d58d72bb3bc Mon Sep 17 00:00:00 2001
From: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Date: Thu, 13 May 2010 19:27:10 -0400
Subject: [PATCH 213/390] ltt-relay-locked-call-for-slowpath

ltt relay locked: call separate functions for slowpath

- Diminishes stack usage, register pressure and instruction cache pressure.

Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
---
 ltt/ltt-relay-locked.c |  533 +++++++++++++++++++++++++++++++++++++++++++++++-
 ltt/ltt-relay-locked.h |  528 +++++------------------------------------------
 2 files changed, 583 insertions(+), 478 deletions(-)

diff --git a/ltt/ltt-relay-locked.c b/ltt/ltt-relay-locked.c
index 8690c0d..6c23e30 100644
--- a/ltt/ltt-relay-locked.c
+++ b/ltt/ltt-relay-locked.c
@@ -63,6 +63,13 @@
 #define printk_dbg(fmt, args...)
 #endif
 
+struct ltt_reserve_switch_offsets {
+	long begin, end, old;
+	long begin_switch, end_switch_current, end_switch_old;
+	long commit_count, reserve_commit_diff;
+	size_t before_hdr_pad, size;
+};
+
 static int ltt_relay_create_buffer(struct ltt_trace_struct *trace,
 		struct ltt_channel_struct *ltt_chan,
 		struct rchan_buf *buf,
@@ -77,11 +84,41 @@ static void ltt_force_switch(struct rchan_buf *buf,
 
 static const struct file_operations ltt_file_operations;
 
+static void ltt_buffer_begin(struct rchan_buf *buf,
+			u64 tsc, unsigned int subbuf_idx)
+{
+	struct ltt_channel_struct *channel =
+		(struct ltt_channel_struct *)buf->chan->private_data;
+	struct ltt_subbuffer_header *header =
+		(struct ltt_subbuffer_header *)
+			ltt_relay_offset_address(buf,
+				subbuf_idx * buf->chan->subbuf_size);
+
+	header->cycle_count_begin = tsc;
+	header->lost_size = 0xFFFFFFFF; /* for debugging */
+	header->buf_size = buf->chan->subbuf_size;
+	ltt_write_trace_header(channel->trace, header);
+}
+
 /*
- * A switch is done during tracing or as a final flush after tracing (so it
- * won't write in the new sub-buffer).
+ * offset is assumed to never be 0 here : never deliver a completely empty
+ * subbuffer. The lost size is between 0 and subbuf_size-1.
  */
-enum force_switch_mode { FORCE_ACTIVE, FORCE_FLUSH };
+static void ltt_buffer_end(struct rchan_buf *buf,
+		u64 tsc, unsigned int offset, unsigned int subbuf_idx)
+{
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	struct ltt_subbuffer_header *header =
+		(struct ltt_subbuffer_header *)
+			ltt_relay_offset_address(buf,
+				subbuf_idx * buf->chan->subbuf_size);
+
+	header->lost_size = SUBBUF_OFFSET((buf->chan->subbuf_size - offset),
+				buf->chan);
+	header->cycle_count_end = tsc;
+	header->events_lost = ltt_buf->events_lost;
+	header->subbuf_corrupt = ltt_buf->corrupted_subbuffers;
+}
 
 static struct dentry *ltt_create_buf_file_callback(const char *filename,
 		struct dentry *parent, int mode,
@@ -135,7 +172,7 @@ static void ltt_relay_wake_writers(struct ltt_channel_buf_struct *ltt_buf)
 /*
  * This function should not be called from NMI interrupt context
  */
-static notrace void ltt_buf_unfull(struct rchan_buf *buf,
+static void ltt_buf_unfull(struct rchan_buf *buf,
 		unsigned int subbuf_idx,
 		long offset)
 {
@@ -877,7 +914,7 @@ static int ltt_relay_create_dirs(struct ltt_trace_struct *new_trace)
  * Must be called when no tracing is active in the channel, because of
  * accesses across CPUs.
  */
-static notrace void ltt_relay_buffer_flush(struct rchan_buf *buf)
+static void ltt_relay_buffer_flush(struct rchan_buf *buf)
 {
 	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
 
@@ -1029,6 +1066,492 @@ static void ltt_relay_print_user_errors(struct ltt_trace_struct *trace,
 			dbg->write, dbg->read);
 }
 
+static void ltt_reserve_push_reader(
+		struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf,
+		struct rchan *rchan,
+		struct rchan_buf *buf,
+		struct ltt_reserve_switch_offsets *offsets)
+{
+	long consumed_old, consumed_new;
+
+	consumed_old = ltt_buf->consumed;
+	/*
+	 * If buffer is in overwrite mode, push the reader consumed
+	 * count if the write position has reached it and we are not
+	 * at the first iteration (don't push the reader farther than
+	 * the writer). This operation can be done concurrently by many
+	 * writers in the same buffer, the writer being at the farthest
+	 * write position sub-buffer index in the buffer being the one
+	 * which will win this loop.
+	 * If the buffer is not in overwrite mode, pushing the reader
+	 * only happens if a sub-buffer is corrupted.
+	 */
+	if (unlikely((SUBBUF_TRUNC(offsets->end-1, buf->chan)
+	   - SUBBUF_TRUNC(consumed_old, buf->chan))
+	   >= rchan->alloc_size)) {
+		consumed_new = SUBBUF_ALIGN(consumed_old, buf->chan);
+		ltt_buf->consumed = consumed_new;
+	} else
+		return;
+
+	if (unlikely(consumed_old != consumed_new)) {
+		/*
+		 * Reader pushed : we are the winner of the push, we can
+		 * therefore reequilibrate reserve and commit. Atomic increment
+		 * of the commit count permits other writers to play around
+		 * with this variable before us. We keep track of
+		 * corrupted_subbuffers even in overwrite mode :
+		 * we never want to write over a non completely committed
+		 * sub-buffer : possible causes : the buffer size is too low
+		 * compared to the unordered data input, or there is a writer
+		 * that died between the reserve and the commit.
+		 */
+		if (likely(offsets->reserve_commit_diff)) {
+			/*
+			 * We have to alter the sub-buffer commit count.
+			 * We do not deliver the previous subbuffer, given it
+			 * was either corrupted or not consumed (overwrite
+			 * mode).
+			 */
+			ltt_buf->commit_count[SUBBUF_INDEX(offsets->begin,
+							   buf->chan)] +=
+						offsets->reserve_commit_diff;
+			if (unlikely(!ltt_channel->overwrite
+			    || offsets->reserve_commit_diff
+			       != rchan->subbuf_size)) {
+				/*
+				 * The reserve commit diff was not subbuf_size :
+				 * it means the subbuffer was partly written to
+				 * and is therefore corrupted. If it is multiple
+				 * of subbuffer size and we are in flight
+				 * recorder mode, we are skipping over a whole
+				 * subbuffer.
+				 */
+				ltt_buf->corrupted_subbuffers++;
+			}
+		}
+	}
+}
+
+/*
+ * ltt_reserve_switch_old_subbuf: switch old subbuffer
+ *
+ * Concurrency safe because we are the last and only thread to alter this
+ * sub-buffer. As long as it is not delivered and read, no other thread can
+ * alter the offset, alter the reserve_count or call the
+ * client_buffer_end_callback on this sub-buffer.
+ *
+ * The only remaining threads could be the ones with pending commits. They will
+ * have to do the deliver themselves.  Not concurrency safe in overwrite mode.
+ * We detect corrupted subbuffers with commit and reserve counts. We keep a
+ * corrupted sub-buffers count and push the readers across these sub-buffers.
+ *
+ * Not concurrency safe if a writer is stalled in a subbuffer and another writer
+ * switches in, finding out it's corrupted.  The result will be than the old
+ * (uncommited) subbuffer will be declared corrupted, and that the new subbuffer
+ * will be declared corrupted too because of the commit count adjustment.
+ *
+ * Note : offset_old should never be 0 here.
+ */
+static void ltt_reserve_switch_old_subbuf(
+		struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
+		struct rchan_buf *buf,
+		struct ltt_reserve_switch_offsets *offsets, u64 *tsc)
+{
+	long oldidx = SUBBUF_INDEX(offsets->old - 1, rchan);
+
+	ltt_buffer_end(buf, *tsc, offsets->old, oldidx);
+	ltt_buf->commit_count[oldidx] +=
+		rchan->subbuf_size
+		- (SUBBUF_OFFSET(offsets->old - 1, rchan)
+		+ 1);
+	offsets->commit_count = ltt_buf->commit_count[oldidx];
+	if (likely((BUFFER_TRUNC(offsets->old - 1, rchan)
+			>> ltt_channel->n_subbufs_order)
+			- ((offsets->commit_count - rchan->subbuf_size)
+			   & ltt_channel->commit_count_mask) == 0))
+		ltt_deliver(buf, oldidx, NULL);
+}
+
+/*
+ * ltt_reserve_switch_new_subbuf: Populate new subbuffer.
+ *
+ * This code can be executed unordered : writers may already have written to the
+ * sub-buffer before this code gets executed, caution.  The commit makes sure
+ * that this code is executed before the deliver of this sub-buffer.
+ */
+static void ltt_reserve_switch_new_subbuf(
+		struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
+		struct rchan_buf *buf,
+		struct ltt_reserve_switch_offsets *offsets, u64 *tsc)
+{
+	long beginidx = SUBBUF_INDEX(offsets->begin, rchan);
+
+	ltt_buffer_begin(buf, *tsc, beginidx);
+	ltt_buf->commit_count[beginidx] += ltt_subbuffer_header_size();
+	offsets->commit_count = ltt_buf->commit_count[beginidx];
+	/* Check if the written buffer has to be delivered */
+	if (unlikely((BUFFER_TRUNC(offsets->end - 1, rchan)
+			>> ltt_channel->n_subbufs_order)
+			- ((offsets->commit_count - rchan->subbuf_size)
+			   & ltt_channel->commit_count_mask) == 0))
+		ltt_deliver(buf, beginidx, NULL);
+}
+
+/*
+ * ltt_reserve_end_switch_current: finish switching current subbuffer
+ *
+ * Concurrency safe because we are the last and only thread to alter this
+ * sub-buffer. As long as it is not delivered and read, no other thread can
+ * alter the offset, alter the reserve_count or call the
+ * client_buffer_end_callback on this sub-buffer.
+ *
+ * The only remaining threads could be the ones with pending commits. They will
+ * have to do the deliver themselves.  Not concurrency safe in overwrite mode.
+ * We detect corrupted subbuffers with commit and reserve counts. We keep a
+ * corrupted sub-buffers count and push the readers across these sub-buffers.
+ *
+ * Not concurrency safe if a writer is stalled in a subbuffer and another writer
+ * switches in, finding out it's corrupted.  The result will be than the old
+ * (uncommited) subbuffer will be declared corrupted, and that the new subbuffer
+ * will be declared corrupted too because of the commit count adjustment.
+ */
+static void ltt_reserve_end_switch_current(
+		struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
+		struct rchan_buf *buf,
+		struct ltt_reserve_switch_offsets *offsets, u64 *tsc)
+{
+	long endidx = SUBBUF_INDEX(offsets->end - 1, rchan);
+
+	ltt_buffer_end(buf, *tsc, offsets->end, endidx);
+	ltt_buf->commit_count[endidx] +=
+		rchan->subbuf_size
+		- (SUBBUF_OFFSET(offsets->end - 1, rchan)
+		+ 1);
+	offsets->commit_count = ltt_buf->commit_count[endidx];
+	if (likely((BUFFER_TRUNC(offsets->end - 1, rchan)
+			>> ltt_channel->n_subbufs_order)
+			- ((offsets->commit_count - rchan->subbuf_size)
+			   & ltt_channel->commit_count_mask) == 0))
+		ltt_deliver(buf, endidx, NULL);
+}
+
+/*
+ * Returns :
+ * 0 if ok
+ * !0 if execution must be aborted.
+ */
+static int ltt_relay_try_switch_slow(
+		enum force_switch_mode mode,
+		struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
+		struct rchan_buf *buf,
+		struct ltt_reserve_switch_offsets *offsets,
+		u64 *tsc)
+{
+	long subbuf_index;
+
+	offsets->begin = ltt_buf->offset;
+	offsets->old = offsets->begin;
+	offsets->begin_switch = 0;
+	offsets->end_switch_old = 0;
+
+	*tsc = trace_clock_read64();
+
+	if (SUBBUF_OFFSET(offsets->begin, buf->chan) != 0) {
+		offsets->begin = SUBBUF_ALIGN(offsets->begin, buf->chan);
+		offsets->end_switch_old = 1;
+	} else {
+		/* we do not have to switch : buffer is empty */
+		return -1;
+	}
+	if (mode == FORCE_ACTIVE)
+		offsets->begin += ltt_subbuffer_header_size();
+	/*
+	 * Always begin_switch in FORCE_ACTIVE mode.
+	 * Test new buffer integrity
+	 */
+	subbuf_index = SUBBUF_INDEX(offsets->begin, buf->chan);
+	offsets->reserve_commit_diff =
+		(BUFFER_TRUNC(offsets->begin, buf->chan)
+		 >> ltt_channel->n_subbufs_order)
+		- (ltt_buf->commit_count[subbuf_index]
+		   & ltt_channel->commit_count_mask);
+	if (offsets->reserve_commit_diff == 0) {
+		/* Next buffer not corrupted. */
+		if (mode == FORCE_ACTIVE
+		    && !ltt_channel->overwrite
+		    && offsets->begin - ltt_buf->consumed
+		       >= rchan->alloc_size) {
+			/*
+			 * We do not overwrite non consumed buffers and we are
+			 * full : ignore switch while tracing is active.
+			 */
+			return -1;
+		}
+	} else {
+		/*
+		 * Next subbuffer corrupted. Force pushing reader even in normal
+		 * mode
+		 */
+	}
+	offsets->end = offsets->begin;
+	return 0;
+}
+
+/*
+ * Force a sub-buffer switch for a per-cpu buffer. This operation is
+ * completely reentrant : can be called while tracing is active with
+ * absolutely no lock held.
+ */
+void ltt_force_switch_locked_slow(struct rchan_buf *buf,
+		enum force_switch_mode mode)
+{
+	struct ltt_channel_struct *ltt_channel =
+			(struct ltt_channel_struct *)buf->chan->private_data;
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	struct rchan *rchan = ltt_channel->trans_channel_data;
+	struct ltt_reserve_switch_offsets offsets;
+	unsigned long flags;
+	u64 tsc;
+
+	offsets.reserve_commit_diff = 0;
+	offsets.size = 0;
+
+	raw_local_irq_save(flags);
+	__raw_spin_lock(&ltt_buf->lock);
+
+	/*
+	 * Perform retryable operations.
+	 */
+	if (ltt_relay_try_switch_slow(mode, ltt_channel, ltt_buf,
+			rchan, buf, &offsets, &tsc)) {
+		__raw_spin_unlock(&ltt_buf->lock);
+		raw_local_irq_restore(flags);
+		return;
+	}
+	ltt_buf->offset = offsets.end;
+
+	save_last_tsc(ltt_buf, tsc);
+
+	/*
+	 * Push the reader if necessary
+	 */
+	if (mode == FORCE_ACTIVE)
+		ltt_reserve_push_reader(ltt_channel, ltt_buf, rchan,
+					buf, &offsets);
+
+	/*
+	 * Switch old subbuffer if needed.
+	 */
+	if (offsets.end_switch_old)
+		ltt_reserve_switch_old_subbuf(ltt_channel, ltt_buf, rchan, buf,
+			&offsets, &tsc);
+
+	/*
+	 * Populate new subbuffer.
+	 */
+	if (mode == FORCE_ACTIVE)
+		ltt_reserve_switch_new_subbuf(ltt_channel,
+			ltt_buf, rchan, buf, &offsets, &tsc);
+
+	__raw_spin_unlock(&ltt_buf->lock);
+	raw_local_irq_restore(flags);
+}
+EXPORT_SYMBOL_GPL(ltt_force_switch_locked_slow);
+
+/*
+ * Returns :
+ * 0 if ok
+ * !0 if execution must be aborted.
+ */
+static int ltt_relay_try_reserve_slow(
+		struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
+		struct rchan_buf *buf,
+		struct ltt_reserve_switch_offsets *offsets, size_t data_size,
+		u64 *tsc, unsigned int *rflags, int largest_align)
+{
+	offsets->begin = ltt_buf->offset;
+	offsets->old = offsets->begin;
+	offsets->begin_switch = 0;
+	offsets->end_switch_current = 0;
+	offsets->end_switch_old = 0;
+
+	*tsc = trace_clock_read64();
+	if (last_tsc_overflow(ltt_buf, *tsc))
+		*rflags = LTT_RFLAG_ID_SIZE_TSC;
+
+	if (unlikely(SUBBUF_OFFSET(offsets->begin, buf->chan) == 0)) {
+		offsets->begin_switch = 1;		/* For offsets->begin */
+	} else {
+		offsets->size = ltt_get_header_size(ltt_channel,
+					offsets->begin, data_size,
+					&offsets->before_hdr_pad, *rflags);
+		offsets->size += ltt_align(offsets->begin + offsets->size,
+					   largest_align)
+				 + data_size;
+		if (unlikely((SUBBUF_OFFSET(offsets->begin, buf->chan) +
+			      offsets->size) > buf->chan->subbuf_size)) {
+			offsets->end_switch_old = 1;	/* For offsets->old */
+			offsets->begin_switch = 1;	/* For offsets->begin */
+		}
+	}
+	if (unlikely(offsets->begin_switch)) {
+		long subbuf_index;
+
+		/*
+		 * We are typically not filling the previous buffer completely.
+		 */
+		if (likely(offsets->end_switch_old))
+			offsets->begin = SUBBUF_ALIGN(offsets->begin,
+						      buf->chan);
+		offsets->begin = offsets->begin + ltt_subbuffer_header_size();
+		/* Test new buffer integrity */
+		subbuf_index = SUBBUF_INDEX(offsets->begin, buf->chan);
+		offsets->reserve_commit_diff =
+			(BUFFER_TRUNC(offsets->begin, buf->chan)
+			 >> ltt_channel->n_subbufs_order)
+			- (ltt_buf->commit_count[subbuf_index]
+			   & ltt_channel->commit_count_mask);
+		if (likely(offsets->reserve_commit_diff == 0)) {
+			/* Next buffer not corrupted. */
+			if (unlikely(!ltt_channel->overwrite &&
+				(SUBBUF_TRUNC(offsets->begin, buf->chan)
+				- SUBBUF_TRUNC(ltt_buf->consumed, buf->chan))
+				>= rchan->alloc_size)) {
+				/*
+				 * We do not overwrite non consumed buffers
+				 * and we are full : event is lost.
+				 */
+				ltt_buf->events_lost++;
+				return -1;
+			} else {
+				/*
+				 * next buffer not corrupted, we are either in
+				 * overwrite mode or the buffer is not full.
+				 * It's safe to write in this new subbuffer.
+				 */
+			}
+		} else {
+			/*
+			 * Next subbuffer corrupted. Force pushing reader even
+			 * in normal mode. It's safe to write in this new
+			 * subbuffer.
+			 */
+		}
+		offsets->size = ltt_get_header_size(ltt_channel,
+					offsets->begin, data_size,
+					&offsets->before_hdr_pad, *rflags);
+		offsets->size += ltt_align(offsets->begin + offsets->size,
+					   largest_align)
+				 + data_size;
+		if (unlikely((SUBBUF_OFFSET(offsets->begin, buf->chan)
+			      + offsets->size) > buf->chan->subbuf_size)) {
+			/*
+			 * Event too big for subbuffers, report error, don't
+			 * complete the sub-buffer switch.
+			 */
+			ltt_buf->events_lost++;
+			return -1;
+		} else {
+			/*
+			 * We just made a successful buffer switch and the event
+			 * fits in the new subbuffer. Let's write.
+			 */
+		}
+	} else {
+		/*
+		 * Event fits in the current buffer and we are not on a switch
+		 * boundary. It's safe to write.
+		 */
+	}
+	offsets->end = offsets->begin + offsets->size;
+
+	if (unlikely((SUBBUF_OFFSET(offsets->end, buf->chan)) == 0)) {
+		/*
+		 * The offset_end will fall at the very beginning of the next
+		 * subbuffer.
+		 */
+		offsets->end_switch_current = 1;	/* For offsets->begin */
+	}
+	return 0;
+}
+
+/**
+ * ltt_relay_reserve_slot_locked_slow - Atomic slot reservation in a buffer.
+ * @trace : the trace structure to log to.
+ * @ltt_channel : channel structure
+ * @transport_data : data structure specific to ltt relay
+ * @data_size : size of the variable length data to log.
+ * @slot_size : pointer to total size of the slot (out)
+ * @buf_offset : pointer to reserved buffer offset (out)
+ * @tsc : pointer to the tsc at the slot reservation (out)
+ * @cpu : cpuid
+ *
+ * Return : -ENOSPC if not enough space, else returns 0.
+ *
+ * It will take care of sub-buffer switching.
+ */
+int ltt_reserve_slot_locked_slow(struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *ltt_channel, void **transport_data,
+		size_t data_size, size_t *slot_size, long *buf_offset, u64 *tsc,
+		unsigned int *rflags, int largest_align, int cpu,
+		unsigned long flags)
+{
+	struct rchan *rchan = ltt_channel->trans_channel_data;
+	struct rchan_buf *buf = *transport_data = rchan->buf[cpu];
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	struct ltt_reserve_switch_offsets offsets;
+
+	offsets.reserve_commit_diff = 0;
+	offsets.size = 0;
+
+	if (unlikely(ltt_relay_try_reserve_slow(ltt_channel, ltt_buf,
+			rchan, buf, &offsets, data_size, tsc, rflags,
+			largest_align))) {
+		__raw_spin_unlock(&ltt_buf->lock);
+		raw_local_irq_restore(flags);
+		return -ENOSPC;
+	}
+	ltt_buf->offset = offsets.end;
+
+	save_last_tsc(ltt_buf, *tsc);
+
+	/*
+	 * Push the reader if necessary
+	 */
+	ltt_reserve_push_reader(ltt_channel, ltt_buf, rchan, buf, &offsets);
+
+	/*
+	 * Switch old subbuffer if needed.
+	 */
+	if (unlikely(offsets.end_switch_old))
+		ltt_reserve_switch_old_subbuf(ltt_channel, ltt_buf, rchan, buf,
+			&offsets, tsc);
+
+	/*
+	 * Populate new subbuffer.
+	 */
+	if (unlikely(offsets.begin_switch))
+		ltt_reserve_switch_new_subbuf(ltt_channel, ltt_buf, rchan,
+			buf, &offsets, tsc);
+
+	if (unlikely(offsets.end_switch_current))
+		ltt_reserve_end_switch_current(ltt_channel, ltt_buf, rchan,
+			buf, &offsets, tsc);
+
+	ltt_buf->irqflags = flags;
+	*slot_size = offsets.size;
+	*buf_offset = offsets.begin + offsets.before_hdr_pad;
+	return 0;
+}
+EXPORT_SYMBOL_GPL(ltt_reserve_slot_locked_slow);
+
 static struct ltt_transport ltt_relay_transport = {
 	.name = "relay-locked",
 	.owner = THIS_MODULE,
diff --git a/ltt/ltt-relay-locked.h b/ltt/ltt-relay-locked.h
index 1cdc3a0..53c013c 100644
--- a/ltt/ltt-relay-locked.h
+++ b/ltt/ltt-relay-locked.h
@@ -64,7 +64,6 @@
 #define printk_dbg(fmt, args...)
 #endif
 
-
 /* LTTng locked logging buffer info */
 struct ltt_channel_buf_struct {
 	/* First 32 bytes cache-hot cacheline */
@@ -93,6 +92,21 @@ struct ltt_channel_buf_struct {
 } ____cacheline_aligned;
 
 /*
+ * A switch is done during tracing or as a final flush after tracing (so it
+ * won't write in the new sub-buffer).
+ */
+enum force_switch_mode { FORCE_ACTIVE, FORCE_FLUSH };
+
+extern int ltt_reserve_slot_locked_slow(struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *ltt_channel, void **transport_data,
+		size_t data_size, size_t *slot_size, long *buf_offset, u64 *tsc,
+		unsigned int *rflags, int largest_align, int cpu,
+		unsigned long flags);
+
+extern void ltt_force_switch_locked_slow(struct rchan_buf *buf,
+		enum force_switch_mode mode);
+
+/*
  * Last TSC comparison functions. Check if the current TSC overflows
  * LTT_TSC_BITS bits from the last TSC read. Reads and writes last_tsc
  * atomically.
@@ -132,48 +146,6 @@ static __inline__ int last_tsc_overflow(struct ltt_channel_buf_struct *ltt_buf,
 }
 #endif
 
-/*
- * A switch is done during tracing or as a final flush after tracing (so it
- * won't write in the new sub-buffer).
- */
-enum force_switch_mode { FORCE_ACTIVE, FORCE_FLUSH };
-
-static __inline__ void ltt_buffer_begin(struct rchan_buf *buf,
-			u64 tsc, unsigned int subbuf_idx)
-{
-	struct ltt_channel_struct *channel =
-		(struct ltt_channel_struct *)buf->chan->private_data;
-	struct ltt_subbuffer_header *header =
-		(struct ltt_subbuffer_header *)
-			ltt_relay_offset_address(buf,
-				subbuf_idx * buf->chan->subbuf_size);
-
-	header->cycle_count_begin = tsc;
-	header->lost_size = 0xFFFFFFFF; /* for debugging */
-	header->buf_size = buf->chan->subbuf_size;
-	ltt_write_trace_header(channel->trace, header);
-}
-
-/*
- * offset is assumed to never be 0 here : never deliver a completely empty
- * subbuffer. The lost size is between 0 and subbuf_size-1.
- */
-static __inline__ void ltt_buffer_end(struct rchan_buf *buf,
-		u64 tsc, unsigned int offset, unsigned int subbuf_idx)
-{
-	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
-	struct ltt_subbuffer_header *header =
-		(struct ltt_subbuffer_header *)
-			ltt_relay_offset_address(buf,
-				subbuf_idx * buf->chan->subbuf_size);
-
-	header->lost_size = SUBBUF_OFFSET((buf->chan->subbuf_size - offset),
-				buf->chan);
-	header->cycle_count_end = tsc;
-	header->events_lost = ltt_buf->events_lost;
-	header->subbuf_corrupt = ltt_buf->corrupted_subbuffers;
-}
-
 static __inline__ void ltt_deliver(struct rchan_buf *buf, unsigned int subbuf_idx,
 		void *subbuf)
 {
@@ -182,368 +154,50 @@ static __inline__ void ltt_deliver(struct rchan_buf *buf, unsigned int subbuf_id
 	ltt_buf->wakeup_readers = 1;
 }
 
-struct ltt_reserve_switch_offsets {
-	long begin, end, old;
-	long begin_switch, end_switch_current, end_switch_old;
-	long commit_count, reserve_commit_diff;
-	size_t before_hdr_pad, size;
-};
-
 /*
- * Returns :
- * 0 if ok
- * !0 if execution must be aborted.
+ * returns 0 if reserve ok, or 1 if the slow path must be taken.
  */
 static __inline__ int ltt_relay_try_reserve(
 		struct ltt_channel_struct *ltt_channel,
 		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
 		struct rchan_buf *buf,
-		struct ltt_reserve_switch_offsets *offsets, size_t data_size,
-		u64 *tsc, unsigned int *rflags, int largest_align)
+		size_t data_size,
+		u64 *tsc, unsigned int *rflags, int largest_align,
+		long *o_begin, long *o_end, long *o_old,
+		long *before_hdr_pad, long *size)
 {
-	offsets->begin = ltt_buf->offset;
-	offsets->old = offsets->begin;
-	offsets->begin_switch = 0;
-	offsets->end_switch_current = 0;
-	offsets->end_switch_old = 0;
+	*o_begin = ltt_buf->offset;
+	*o_old = *o_begin;
 
 	*tsc = trace_clock_read64();
 	if (last_tsc_overflow(ltt_buf, *tsc))
 		*rflags = LTT_RFLAG_ID_SIZE_TSC;
 
-	if (unlikely(SUBBUF_OFFSET(offsets->begin, buf->chan) == 0)) {
-		offsets->begin_switch = 1;		/* For offsets->begin */
-	} else {
-		offsets->size = ltt_get_header_size(ltt_channel,
-					offsets->begin, data_size,
-					&offsets->before_hdr_pad, *rflags);
-		offsets->size += ltt_align(offsets->begin + offsets->size,
-					   largest_align)
-				 + data_size;
-		if (unlikely((SUBBUF_OFFSET(offsets->begin, buf->chan) +
-			      offsets->size) > buf->chan->subbuf_size)) {
-			offsets->end_switch_old = 1;	/* For offsets->old */
-			offsets->begin_switch = 1;	/* For offsets->begin */
-		}
-	}
-	if (unlikely(offsets->begin_switch)) {
-		long subbuf_index;
-
-		/*
-		 * We are typically not filling the previous buffer completely.
-		 */
-		if (likely(offsets->end_switch_old))
-			offsets->begin = SUBBUF_ALIGN(offsets->begin,
-						      buf->chan);
-		offsets->begin = offsets->begin + ltt_subbuffer_header_size();
-		/* Test new buffer integrity */
-		subbuf_index = SUBBUF_INDEX(offsets->begin, buf->chan);
-		offsets->reserve_commit_diff =
-			(BUFFER_TRUNC(offsets->begin, buf->chan)
-			 >> ltt_channel->n_subbufs_order)
-			- (ltt_buf->commit_count[subbuf_index]
-			   & ltt_channel->commit_count_mask);
-		if (likely(offsets->reserve_commit_diff == 0)) {
-			/* Next buffer not corrupted. */
-			if (unlikely(!ltt_channel->overwrite &&
-				(SUBBUF_TRUNC(offsets->begin, buf->chan)
-				- SUBBUF_TRUNC(ltt_buf->consumed, buf->chan))
-				>= rchan->alloc_size)) {
-				/*
-				 * We do not overwrite non consumed buffers
-				 * and we are full : event is lost.
-				 */
-				ltt_buf->events_lost++;
-				return -1;
-			} else {
-				/*
-				 * next buffer not corrupted, we are either in
-				 * overwrite mode or the buffer is not full.
-				 * It's safe to write in this new subbuffer.
-				 */
-			}
-		} else {
-			/*
-			 * Next subbuffer corrupted. Force pushing reader even
-			 * in normal mode. It's safe to write in this new
-			 * subbuffer.
-			 */
-		}
-		offsets->size = ltt_get_header_size(ltt_channel,
-					offsets->begin, data_size,
-					&offsets->before_hdr_pad, *rflags);
-		offsets->size += ltt_align(offsets->begin + offsets->size,
-					   largest_align)
-				 + data_size;
-		if (unlikely((SUBBUF_OFFSET(offsets->begin, buf->chan)
-			      + offsets->size) > buf->chan->subbuf_size)) {
-			/*
-			 * Event too big for subbuffers, report error, don't
-			 * complete the sub-buffer switch.
-			 */
-			ltt_buf->events_lost++;
-			return -1;
-		} else {
-			/*
-			 * We just made a successful buffer switch and the event
-			 * fits in the new subbuffer. Let's write.
-			 */
-		}
-	} else {
-		/*
-		 * Event fits in the current buffer and we are not on a switch
-		 * boundary. It's safe to write.
-		 */
-	}
-	offsets->end = offsets->begin + offsets->size;
-
-	if (unlikely((SUBBUF_OFFSET(offsets->end, buf->chan)) == 0)) {
-		/*
-		 * The offset_end will fall at the very beginning of the next
-		 * subbuffer.
-		 */
-		offsets->end_switch_current = 1;	/* For offsets->begin */
-	}
-	return 0;
-}
-
-/*
- * Returns :
- * 0 if ok
- * !0 if execution must be aborted.
- */
-static __inline__ int ltt_relay_try_switch(
-		enum force_switch_mode mode,
-		struct ltt_channel_struct *ltt_channel,
-		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
-		struct rchan_buf *buf,
-		struct ltt_reserve_switch_offsets *offsets,
-		u64 *tsc)
-{
-	long subbuf_index;
-
-	offsets->begin = ltt_buf->offset;
-	offsets->old = offsets->begin;
-	offsets->begin_switch = 0;
-	offsets->end_switch_old = 0;
+	if (unlikely(SUBBUF_OFFSET(*o_begin, buf->chan) == 0))
+		return 1;
 
-	*tsc = trace_clock_read64();
+	*size = ltt_get_header_size(ltt_channel,
+				*o_begin, data_size,
+				before_hdr_pad, *rflags);
+	*size += ltt_align(*o_begin + *size, largest_align) + data_size;
+	if (unlikely((SUBBUF_OFFSET(*o_begin, buf->chan) + *size)
+		     > buf->chan->subbuf_size))
+		return 1;
 
-	if (SUBBUF_OFFSET(offsets->begin, buf->chan) != 0) {
-		offsets->begin = SUBBUF_ALIGN(offsets->begin, buf->chan);
-		offsets->end_switch_old = 1;
-	} else {
-		/* we do not have to switch : buffer is empty */
-		return -1;
-	}
-	if (mode == FORCE_ACTIVE)
-		offsets->begin += ltt_subbuffer_header_size();
 	/*
-	 * Always begin_switch in FORCE_ACTIVE mode.
-	 * Test new buffer integrity
+	 * Event fits in the current buffer and we are not on a switch
+	 * boundary. It's safe to write.
 	 */
-	subbuf_index = SUBBUF_INDEX(offsets->begin, buf->chan);
-	offsets->reserve_commit_diff =
-		(BUFFER_TRUNC(offsets->begin, buf->chan)
-		 >> ltt_channel->n_subbufs_order)
-		- (ltt_buf->commit_count[subbuf_index]
-		   & ltt_channel->commit_count_mask);
-	if (offsets->reserve_commit_diff == 0) {
-		/* Next buffer not corrupted. */
-		if (mode == FORCE_ACTIVE
-		    && !ltt_channel->overwrite
-		    && offsets->begin - ltt_buf->consumed
-		       >= rchan->alloc_size) {
-			/*
-			 * We do not overwrite non consumed buffers and we are
-			 * full : ignore switch while tracing is active.
-			 */
-			return -1;
-		}
-	} else {
-		/*
-		 * Next subbuffer corrupted. Force pushing reader even in normal
-		 * mode
-		 */
-	}
-	offsets->end = offsets->begin;
-	return 0;
-}
-
-static __inline__ void ltt_reserve_push_reader(
-		struct ltt_channel_struct *ltt_channel,
-		struct ltt_channel_buf_struct *ltt_buf,
-		struct rchan *rchan,
-		struct rchan_buf *buf,
-		struct ltt_reserve_switch_offsets *offsets)
-{
-	long consumed_old, consumed_new;
+	*o_end = *o_begin + *size;
 
-	consumed_old = ltt_buf->consumed;
-	/*
-	 * If buffer is in overwrite mode, push the reader consumed
-	 * count if the write position has reached it and we are not
-	 * at the first iteration (don't push the reader farther than
-	 * the writer). This operation can be done concurrently by many
-	 * writers in the same buffer, the writer being at the farthest
-	 * write position sub-buffer index in the buffer being the one
-	 * which will win this loop.
-	 * If the buffer is not in overwrite mode, pushing the reader
-	 * only happens if a sub-buffer is corrupted.
-	 */
-	if (unlikely((SUBBUF_TRUNC(offsets->end-1, buf->chan)
-	   - SUBBUF_TRUNC(consumed_old, buf->chan))
-	   >= rchan->alloc_size)) {
-		consumed_new = SUBBUF_ALIGN(consumed_old, buf->chan);
-		ltt_buf->consumed = consumed_new;
-	} else
-		return;
-
-	if (unlikely(consumed_old != consumed_new)) {
+	if (unlikely((SUBBUF_OFFSET(*o_end, buf->chan)) == 0))
 		/*
-		 * Reader pushed : we are the winner of the push, we can
-		 * therefore reequilibrate reserve and commit. Atomic increment
-		 * of the commit count permits other writers to play around
-		 * with this variable before us. We keep track of
-		 * corrupted_subbuffers even in overwrite mode :
-		 * we never want to write over a non completely committed
-		 * sub-buffer : possible causes : the buffer size is too low
-		 * compared to the unordered data input, or there is a writer
-		 * that died between the reserve and the commit.
+		 * The offset_end will fall at the very beginning of the next
+		 * subbuffer.
 		 */
-		if (likely(offsets->reserve_commit_diff)) {
-			/*
-			 * We have to alter the sub-buffer commit count.
-			 * We do not deliver the previous subbuffer, given it
-			 * was either corrupted or not consumed (overwrite
-			 * mode).
-			 */
-			ltt_buf->commit_count[SUBBUF_INDEX(offsets->begin,
-							   buf->chan)] +=
-						offsets->reserve_commit_diff;
-			if (unlikely(!ltt_channel->overwrite
-			    || offsets->reserve_commit_diff
-			       != rchan->subbuf_size)) {
-				/*
-				 * The reserve commit diff was not subbuf_size :
-				 * it means the subbuffer was partly written to
-				 * and is therefore corrupted. If it is multiple
-				 * of subbuffer size and we are in flight
-				 * recorder mode, we are skipping over a whole
-				 * subbuffer.
-				 */
-				ltt_buf->corrupted_subbuffers++;
-			}
-		}
-	}
-}
-
-
-/*
- * ltt_reserve_switch_old_subbuf: switch old subbuffer
- *
- * Concurrency safe because we are the last and only thread to alter this
- * sub-buffer. As long as it is not delivered and read, no other thread can
- * alter the offset, alter the reserve_count or call the
- * client_buffer_end_callback on this sub-buffer.
- *
- * The only remaining threads could be the ones with pending commits. They will
- * have to do the deliver themselves.  Not concurrency safe in overwrite mode.
- * We detect corrupted subbuffers with commit and reserve counts. We keep a
- * corrupted sub-buffers count and push the readers across these sub-buffers.
- *
- * Not concurrency safe if a writer is stalled in a subbuffer and another writer
- * switches in, finding out it's corrupted.  The result will be than the old
- * (uncommited) subbuffer will be declared corrupted, and that the new subbuffer
- * will be declared corrupted too because of the commit count adjustment.
- *
- * Note : offset_old should never be 0 here.
- */
-static __inline__ void ltt_reserve_switch_old_subbuf(
-		struct ltt_channel_struct *ltt_channel,
-		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
-		struct rchan_buf *buf,
-		struct ltt_reserve_switch_offsets *offsets, u64 *tsc)
-{
-	long oldidx = SUBBUF_INDEX(offsets->old - 1, rchan);
-
-	ltt_buffer_end(buf, *tsc, offsets->old, oldidx);
-	ltt_buf->commit_count[oldidx] +=
-		rchan->subbuf_size
-		- (SUBBUF_OFFSET(offsets->old - 1, rchan)
-		+ 1);
-	offsets->commit_count = ltt_buf->commit_count[oldidx];
-	if (likely((BUFFER_TRUNC(offsets->old - 1, rchan)
-			>> ltt_channel->n_subbufs_order)
-			- ((offsets->commit_count - rchan->subbuf_size)
-			   & ltt_channel->commit_count_mask) == 0))
-		ltt_deliver(buf, oldidx, NULL);
-}
-
-/*
- * ltt_reserve_switch_new_subbuf: Populate new subbuffer.
- *
- * This code can be executed unordered : writers may already have written to the
- * sub-buffer before this code gets executed, caution.  The commit makes sure
- * that this code is executed before the deliver of this sub-buffer.
- */
-static __inline__ void ltt_reserve_switch_new_subbuf(
-		struct ltt_channel_struct *ltt_channel,
-		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
-		struct rchan_buf *buf,
-		struct ltt_reserve_switch_offsets *offsets, u64 *tsc)
-{
-	long beginidx = SUBBUF_INDEX(offsets->begin, rchan);
-
-	ltt_buffer_begin(buf, *tsc, beginidx);
-	ltt_buf->commit_count[beginidx] += ltt_subbuffer_header_size();
-	offsets->commit_count = ltt_buf->commit_count[beginidx];
-	/* Check if the written buffer has to be delivered */
-	if (unlikely((BUFFER_TRUNC(offsets->end - 1, rchan)
-			>> ltt_channel->n_subbufs_order)
-			- ((offsets->commit_count - rchan->subbuf_size)
-			   & ltt_channel->commit_count_mask) == 0))
-		ltt_deliver(buf, beginidx, NULL);
-}
-
+		return 1;
 
-/*
- * ltt_reserve_end_switch_current: finish switching current subbuffer
- *
- * Concurrency safe because we are the last and only thread to alter this
- * sub-buffer. As long as it is not delivered and read, no other thread can
- * alter the offset, alter the reserve_count or call the
- * client_buffer_end_callback on this sub-buffer.
- *
- * The only remaining threads could be the ones with pending commits. They will
- * have to do the deliver themselves.  Not concurrency safe in overwrite mode.
- * We detect corrupted subbuffers with commit and reserve counts. We keep a
- * corrupted sub-buffers count and push the readers across these sub-buffers.
- *
- * Not concurrency safe if a writer is stalled in a subbuffer and another writer
- * switches in, finding out it's corrupted.  The result will be than the old
- * (uncommited) subbuffer will be declared corrupted, and that the new subbuffer
- * will be declared corrupted too because of the commit count adjustment.
- */
-static __inline__ void ltt_reserve_end_switch_current(
-		struct ltt_channel_struct *ltt_channel,
-		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
-		struct rchan_buf *buf,
-		struct ltt_reserve_switch_offsets *offsets, u64 *tsc)
-{
-	long endidx = SUBBUF_INDEX(offsets->end - 1, rchan);
-
-	ltt_buffer_end(buf, *tsc, offsets->end, endidx);
-	ltt_buf->commit_count[endidx] +=
-		rchan->subbuf_size
-		- (SUBBUF_OFFSET(offsets->end - 1, rchan)
-		+ 1);
-	offsets->commit_count = ltt_buf->commit_count[endidx];
-	if (likely((BUFFER_TRUNC(offsets->end - 1, rchan)
-			>> ltt_channel->n_subbufs_order)
-			- ((offsets->commit_count - rchan->subbuf_size)
-			   & ltt_channel->commit_count_mask) == 0))
-		ltt_deliver(buf, endidx, NULL);
+	return 0;
 }
 
 /**
@@ -561,7 +215,7 @@ static __inline__ void ltt_reserve_end_switch_current(
  *
  * It will take care of sub-buffer switching.
  */
-static __inline__ int ltt_relay_reserve_slot(struct ltt_trace_struct *trace,
+static __inline__ int ltt_reserve_slot(struct ltt_trace_struct *trace,
 		struct ltt_channel_struct *ltt_channel, void **transport_data,
 		size_t data_size, size_t *slot_size, long *buf_offset, u64 *tsc,
 		unsigned int *rflags, int largest_align, int cpu)
@@ -569,15 +223,13 @@ static __inline__ int ltt_relay_reserve_slot(struct ltt_trace_struct *trace,
 	struct rchan *rchan = ltt_channel->trans_channel_data;
 	struct rchan_buf *buf = *transport_data = rchan->buf[cpu];
 	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
-	struct ltt_reserve_switch_offsets offsets;
+	long o_begin, o_end, o_old;
+	size_t before_hdr_pad;
 	unsigned long flags;
 
 	raw_local_irq_save(flags);
 	__raw_spin_lock(&ltt_buf->lock);
 
-	offsets.reserve_commit_diff = 0;
-	offsets.size = 0;
-
 	/*
 	 * Perform retryable operations.
 	 */
@@ -589,43 +241,22 @@ static __inline__ int ltt_relay_reserve_slot(struct ltt_trace_struct *trace,
 	}
 
 	if (unlikely(ltt_relay_try_reserve(ltt_channel, ltt_buf,
-			rchan, buf, &offsets, data_size, tsc, rflags,
-			largest_align))) {
-		__raw_spin_unlock(&ltt_buf->lock);
-		raw_local_irq_restore(flags);
-		return -ENOSPC;
-	}
-	ltt_buf->offset = offsets.end;
-
-	save_last_tsc(ltt_buf, *tsc);
+			rchan, buf, data_size, tsc, rflags,
+			largest_align, &o_begin, &o_end, &o_old,
+			&before_hdr_pad, slot_size)))
+		goto slow_path;
 
-	/*
-	 * Push the reader if necessary
-	 */
-	ltt_reserve_push_reader(ltt_channel, ltt_buf, rchan, buf, &offsets);
-
-	/*
-	 * Switch old subbuffer if needed.
-	 */
-	if (unlikely(offsets.end_switch_old))
-		ltt_reserve_switch_old_subbuf(ltt_channel, ltt_buf, rchan, buf,
-			&offsets, tsc);
+	ltt_buf->offset = o_end;
 
-	/*
-	 * Populate new subbuffer.
-	 */
-	if (unlikely(offsets.begin_switch))
-		ltt_reserve_switch_new_subbuf(ltt_channel, ltt_buf, rchan,
-			buf, &offsets, tsc);
-
-	if (unlikely(offsets.end_switch_current))
-		ltt_reserve_end_switch_current(ltt_channel, ltt_buf, rchan,
-			buf, &offsets, tsc);
+	save_last_tsc(ltt_buf, *tsc);
 
 	ltt_buf->irqflags = flags;
-	*slot_size = offsets.size;
-	*buf_offset = offsets.begin + offsets.before_hdr_pad;
+	*buf_offset = o_begin + before_hdr_pad;
 	return 0;
+slow_path:
+	return ltt_reserve_slot_locked_slow(trace, ltt_channel,
+		transport_data, data_size, slot_size, buf_offset, tsc,
+		rflags, largest_align, cpu, flags);
 }
 
 /*
@@ -636,56 +267,7 @@ static __inline__ int ltt_relay_reserve_slot(struct ltt_trace_struct *trace,
 static __inline__ void ltt_force_switch(struct rchan_buf *buf,
 		enum force_switch_mode mode)
 {
-	struct ltt_channel_struct *ltt_channel =
-			(struct ltt_channel_struct *)buf->chan->private_data;
-	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
-	struct rchan *rchan = ltt_channel->trans_channel_data;
-	struct ltt_reserve_switch_offsets offsets;
-	unsigned long flags;
-	u64 tsc;
-
-	offsets.reserve_commit_diff = 0;
-	offsets.size = 0;
-
-	raw_local_irq_save(flags);
-	__raw_spin_lock(&ltt_buf->lock);
-
-	/*
-	 * Perform retryable operations.
-	 */
-	if (ltt_relay_try_switch(mode, ltt_channel, ltt_buf,
-			rchan, buf, &offsets, &tsc)) {
-		__raw_spin_unlock(&ltt_buf->lock);
-		raw_local_irq_restore(flags);
-		return;
-	}
-	ltt_buf->offset = offsets.end;
-
-	save_last_tsc(ltt_buf, tsc);
-
-	/*
-	 * Push the reader if necessary
-	 */
-	if (mode == FORCE_ACTIVE)
-		ltt_reserve_push_reader(ltt_channel, ltt_buf, rchan,
-					buf, &offsets);
-
-	/*
-	 * Switch old subbuffer if needed.
-	 */
-	if (offsets.end_switch_old)
-		ltt_reserve_switch_old_subbuf(ltt_channel, ltt_buf, rchan, buf,
-			&offsets, &tsc);
-
-	/*
-	 * Populate new subbuffer.
-	 */
-	if (mode == FORCE_ACTIVE)
-		ltt_reserve_switch_new_subbuf(ltt_channel,
-			ltt_buf, rchan, buf, &offsets, &tsc);
-
-	__raw_spin_unlock(&ltt_buf->lock);
-	raw_local_irq_restore(flags);
+	return ltt_force_switch_locked_slow(buf, mode);
 }
 
 /*
@@ -745,7 +327,7 @@ static __inline__ void ltt_write_commit_counter(struct rchan_buf *buf,
  * @buf_offset : offset following the event header.
  * @slot_size : size of the reserved slot.
  */
-static __inline__ void ltt_relay_commit_slot(
+static __inline__ void ltt_commit_slot(
 		struct ltt_channel_struct *ltt_channel,
 		void **transport_data, long buf_offset, size_t slot_size)
 {
-- 
1.6.5.2

