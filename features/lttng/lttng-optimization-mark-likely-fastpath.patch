From 17a1d37b73b2c486dc7e856266158d8d2cbd0f50 Mon Sep 17 00:00:00 2001
From: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Date: Thu, 13 May 2010 19:27:04 -0400
Subject: [PATCH 205/391] lttng-optimization-mark-likely-fastpath

LTTng optimizations relay lockless: mark likely fast path

Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
---
 ltt/ltt-relay-lockless.h |   83 +++++++++++++++++++++++----------------------
 1 files changed, 42 insertions(+), 41 deletions(-)

diff --git a/ltt/ltt-relay-lockless.h b/ltt/ltt-relay-lockless.h
index ce988dc..639c53a 100644
--- a/ltt/ltt-relay-lockless.h
+++ b/ltt/ltt-relay-lockless.h
@@ -220,7 +220,7 @@ static __inline__ int ltt_relay_try_reserve(
 	if (last_tsc_overflow(ltt_buf, *tsc))
 		*rflags = LTT_RFLAG_ID_SIZE_TSC;
 
-	if (SUBBUF_OFFSET(offsets->begin, buf->chan) == 0) {
+	if (unlikely(SUBBUF_OFFSET(offsets->begin, buf->chan) == 0)) {
 		offsets->begin_switch = 1;		/* For offsets->begin */
 	} else {
 		offsets->size = ltt_get_header_size(ltt_channel,
@@ -229,16 +229,19 @@ static __inline__ int ltt_relay_try_reserve(
 		offsets->size += ltt_align(offsets->begin + offsets->size,
 					   largest_align)
 				 + data_size;
-		if ((SUBBUF_OFFSET(offsets->begin, buf->chan) + offsets->size)
-				> buf->chan->subbuf_size) {
+		if (unlikely((SUBBUF_OFFSET(offsets->begin, buf->chan) +
+			     offsets->size) > buf->chan->subbuf_size)) {
 			offsets->end_switch_old = 1;	/* For offsets->old */
 			offsets->begin_switch = 1;	/* For offsets->begin */
 		}
 	}
-	if (offsets->begin_switch) {
+	if (unlikely(offsets->begin_switch)) {
 		long subbuf_index;
 
-		if (offsets->end_switch_old)
+		/*
+		 * We are typically not filling the previous buffer completely.
+		 */
+		if (likely(offsets->end_switch_old))
 			offsets->begin = SUBBUF_ALIGN(offsets->begin,
 						      buf->chan);
 		offsets->begin = offsets->begin + ltt_subbuffer_header_size();
@@ -249,14 +252,14 @@ static __inline__ int ltt_relay_try_reserve(
 			 >> ltt_channel->n_subbufs_order)
 			- (local_read(&ltt_buf->commit_count[subbuf_index])
 				& ltt_channel->commit_count_mask);
-		if (offsets->reserve_commit_diff == 0) {
+		if (likely(offsets->reserve_commit_diff == 0)) {
 			/* Next buffer not corrupted. */
-			if (!ltt_channel->overwrite &&
+			if (unlikely(!ltt_channel->overwrite &&
 				(SUBBUF_TRUNC(offsets->begin, buf->chan)
 				 - SUBBUF_TRUNC(atomic_long_read(
 							&ltt_buf->consumed),
 						buf->chan))
-				>= rchan->alloc_size) {
+				>= rchan->alloc_size)) {
 				/*
 				 * We do not overwrite non consumed buffers
 				 * and we are full : event is lost.
@@ -283,8 +286,8 @@ static __inline__ int ltt_relay_try_reserve(
 		offsets->size += ltt_align(offsets->begin + offsets->size,
 					   largest_align)
 				 + data_size;
-		if ((SUBBUF_OFFSET(offsets->begin, buf->chan) + offsets->size)
-				> buf->chan->subbuf_size) {
+		if (unlikely((SUBBUF_OFFSET(offsets->begin, buf->chan)
+			     + offsets->size) > buf->chan->subbuf_size)) {
 			/*
 			 * Event too big for subbuffers, report error, don't
 			 * complete the sub-buffer switch.
@@ -305,7 +308,7 @@ static __inline__ int ltt_relay_try_reserve(
 	}
 	offsets->end = offsets->begin + offsets->size;
 
-	if ((SUBBUF_OFFSET(offsets->end, buf->chan)) == 0) {
+	if (unlikely((SUBBUF_OFFSET(offsets->end, buf->chan)) == 0)) {
 		/*
 		 * The offset_end will fall at the very beginning of the next
 		 * subbuffer.
@@ -400,18 +403,16 @@ static __inline__ void ltt_reserve_push_reader(
 		 * If the buffer is not in overwrite mode, pushing the reader
 		 * only happens if a sub-buffer is corrupted.
 		 */
-		if ((SUBBUF_TRUNC(offsets->end-1, buf->chan)
+		if (unlikely((SUBBUF_TRUNC(offsets->end-1, buf->chan)
 		   - SUBBUF_TRUNC(consumed_old, buf->chan))
-		   >= rchan->alloc_size)
+		   >= rchan->alloc_size))
 			consumed_new = SUBBUF_ALIGN(consumed_old, buf->chan);
-		else {
-			consumed_new = consumed_old;
-			break;
-		}
-	} while (atomic_long_cmpxchg(&ltt_buf->consumed, consumed_old,
-			consumed_new) != consumed_old);
+		else
+			return;
+	} while (unlikely(atomic_long_cmpxchg(&ltt_buf->consumed, consumed_old,
+			consumed_new) != consumed_old));
 
-	if (consumed_old != consumed_new) {
+	if (unlikely(consumed_old != consumed_new)) {
 		/*
 		 * Reader pushed : we are the winner of the push, we can
 		 * therefore reequilibrate reserve and commit. Atomic increment
@@ -423,7 +424,7 @@ static __inline__ void ltt_reserve_push_reader(
 		 * compared to the unordered data input, or there is a writer
 		 * that died between the reserve and the commit.
 		 */
-		if (offsets->reserve_commit_diff) {
+		if (likely(offsets->reserve_commit_diff)) {
 			/*
 			 * We have to alter the sub-buffer commit count.
 			 * We do not deliver the previous subbuffer, given it
@@ -434,9 +435,9 @@ static __inline__ void ltt_reserve_push_reader(
 				  &ltt_buf->commit_count[
 					SUBBUF_INDEX(offsets->begin,
 						     buf->chan)]);
-			if (!ltt_channel->overwrite
+			if (unlikely(!ltt_channel->overwrite
 			    || offsets->reserve_commit_diff
-			       != rchan->subbuf_size) {
+			       != rchan->subbuf_size)) {
 				/*
 				 * The reserve commit diff was not subbuf_size :
 				 * it means the subbuffer was partly written to
@@ -488,10 +489,10 @@ static __inline__ void ltt_reserve_switch_old_subbuf(
 				 - (SUBBUF_OFFSET(offsets->old - 1, rchan)
 				 + 1),
 				 &ltt_buf->commit_count[oldidx]);
-	if ((BUFFER_TRUNC(offsets->old - 1, rchan)
+	if (likely((BUFFER_TRUNC(offsets->old - 1, rchan)
 			>> ltt_channel->n_subbufs_order)
 			- ((offsets->commit_count - rchan->subbuf_size)
-				& ltt_channel->commit_count_mask) == 0)
+				& ltt_channel->commit_count_mask) == 0))
 		ltt_deliver(buf, oldidx, NULL);
 }
 
@@ -516,10 +517,10 @@ static __inline__ void ltt_reserve_switch_new_subbuf(
 	offsets->commit_count = local_add_return(ltt_subbuffer_header_size(),
 			&ltt_buf->commit_count[beginidx]);
 	/* Check if the written buffer has to be delivered */
-	if ((BUFFER_TRUNC(offsets->begin, rchan)
+	if (unlikely((BUFFER_TRUNC(offsets->begin, rchan)
 			>> ltt_channel->n_subbufs_order)
 			- ((offsets->commit_count - rchan->subbuf_size)
-				& ltt_channel->commit_count_mask) == 0)
+				& ltt_channel->commit_count_mask) == 0))
 		ltt_deliver(buf, beginidx, NULL);
 }
 
@@ -558,10 +559,10 @@ static __inline__ void ltt_reserve_end_switch_current(
 				 - (SUBBUF_OFFSET(offsets->end - 1, rchan)
 				 + 1),
 				 &ltt_buf->commit_count[endidx]);
-	if ((BUFFER_TRUNC(offsets->end - 1, rchan)
+	if (likely((BUFFER_TRUNC(offsets->end - 1, rchan)
 			>> ltt_channel->n_subbufs_order)
 			- ((offsets->commit_count - rchan->subbuf_size)
-				& ltt_channel->commit_count_mask) == 0)
+				& ltt_channel->commit_count_mask) == 0))
 		ltt_deliver(buf, endidx, NULL);
 }
 
@@ -595,17 +596,17 @@ static __inline__ int ltt_reserve_slot(struct ltt_trace_struct *trace,
 	/*
 	 * Perform retryable operations.
 	 */
-	if (__get_cpu_var(ltt_nesting) > 4) {
+	if (unlikely(__get_cpu_var(ltt_nesting) > 4)) {
 		local_inc(&ltt_buf->events_lost);
 		return -EPERM;
 	}
 	do {
-		if (ltt_relay_try_reserve(ltt_channel, ltt_buf,
+		if (unlikely(ltt_relay_try_reserve(ltt_channel, ltt_buf,
 				rchan, buf, &offsets, data_size, tsc, rflags,
-				largest_align))
+				largest_align)))
 			return -ENOSPC;
-	} while (local_cmpxchg(&ltt_buf->offset, offsets.old,
-			offsets.end) != offsets.old);
+	} while (unlikely(local_cmpxchg(&ltt_buf->offset, offsets.old,
+			offsets.end) != offsets.old));
 
 	/*
 	 * Atomically update last_tsc. This update races against concurrent
@@ -623,18 +624,18 @@ static __inline__ int ltt_reserve_slot(struct ltt_trace_struct *trace,
 	/*
 	 * Switch old subbuffer if needed.
 	 */
-	if (offsets.end_switch_old)
+	if (unlikely(offsets.end_switch_old))
 		ltt_reserve_switch_old_subbuf(ltt_channel, ltt_buf, rchan, buf,
 			&offsets, tsc);
 
 	/*
 	 * Populate new subbuffer.
 	 */
-	if (offsets.begin_switch)
+	if (unlikely(offsets.begin_switch))
 		ltt_reserve_switch_new_subbuf(ltt_channel, ltt_buf, rchan,
 			buf, &offsets, tsc);
 
-	if (offsets.end_switch_current)
+	if (unlikely(offsets.end_switch_current))
 		ltt_reserve_end_switch_current(ltt_channel, ltt_buf, rchan,
 			buf, &offsets, tsc);
 
@@ -733,12 +734,12 @@ static __inline__ void ltt_write_commit_counter(struct rchan_buf *buf,
 		commit_count =
 			local_read(&ltt_buf->commit_count[subbuf_idx]);
 		/* SUBBUF_OFFSET includes commit_count_mask */
-		if (!SUBBUF_OFFSET(offset - commit_count, buf->chan)) {
+		if (likely(!SUBBUF_OFFSET(offset - commit_count, buf->chan))) {
 			lost_new = (uint32_t)buf->chan->subbuf_size
 				   - SUBBUF_OFFSET(commit_count, buf->chan);
 			lost_old = cmpxchg_local(&header->lost_size, lost_old,
 						lost_new);
-			if (lost_old <= lost_new)
+			if (likely(lost_old <= lost_new))
 				break;
 		} else {
 			break;
@@ -779,10 +780,10 @@ static __inline__ void ltt_commit_slot(
 	commit_count = local_add_return(slot_size,
 		&ltt_buf->commit_count[endidx]);
 	/* Check if all commits have been done */
-	if ((BUFFER_TRUNC(offset_end - 1, rchan)
+	if (unlikely((BUFFER_TRUNC(offset_end - 1, rchan)
 			>> ltt_channel->n_subbufs_order)
 			- ((commit_count - rchan->subbuf_size)
-			   & ltt_channel->commit_count_mask) == 0)
+			   & ltt_channel->commit_count_mask) == 0))
 		ltt_deliver(buf, endidx, NULL);
 	/*
 	 * Update lost_size for each commit. It's needed only for extracting
-- 
1.6.5.2

